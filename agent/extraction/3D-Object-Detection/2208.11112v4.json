{
    "meta_info": {
        "title": "DeepInteraction: 3D Object Detection via Modality Interaction",
        "abstract": "Existing top-performance 3D object detectors typically rely on the\nmulti-modal fusion strategy. This design is however fundamentally restricted\ndue to overlooking the modality-specific useful information and finally\nhampering the model performance. To address this limitation, in this work we\nintroduce a novel modality interaction strategy where individual per-modality\nrepresentations are learned and maintained throughout for enabling their unique\ncharacteristics to be exploited during object detection. To realize this\nproposed strategy, we design a DeepInteraction architecture characterized by a\nmulti-modal representational interaction encoder and a multi-modal predictive\ninteraction decoder. Experiments on the large-scale nuScenes dataset show that\nour proposed method surpasses all prior arts often by a large margin.\nCrucially, our method is ranked at the first position at the highly competitive\nnuScenes object detection leaderboard.",
        "author": "Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, Li Zhang",
        "link": "http://arxiv.org/abs/2208.11112v4",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "To appear at NeurIPS 2022. 16 pages, 7 figure"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n3D object detection is critical for autonomous driving by localizing and recognizing decision-sensitive objects in a 3D world.\nFor reliable object detection, LiDAR and camera sensors have been simultaneously deployed to provide {\\em point clouds} \nand \n{\\em RGB images} for more stronger perception.\nThe two modalities exhibit naturally strong complementary effects\ndue to their different perceiving characteristics.\nPoint clouds offer necessary localization and geometry information at low resolution,\nwhilst images give rich appearance information at high resolution.\nTherefore, {\\em information fusion} across modalities becomes particularly crucial for strong 3D object detection performance.\n\n\nExisting multi-modal 3D objection detection methods typically\nadopt a {\\bf\\em modality fusion} strategy (Figure~\\ref{fig:teaser}(a)) by combining individual per-modality representations into a {\\em single} hybrid representation.\nFor example, PointPainting~\\cite{vora2020pointpainting} and its variants~\\cite{wang2021pointaugmenting,Yin2021MVP,Xu2021FusionPaintingMF} aggregate category scores or semantic features from the image space into the 3D point cloud space. AutoAlign~\\cite{chen2022autoalign} and VFF~\\cite{li2022vff} similarly integrate image representations into the 3D grid space. Latest alternatives~\\cite{li2022deepfusion,liu2022bevfusion,liang2022bevfusion} merge the image and point cloud features into a joint bird's-eye view (BEV) representation. \nThis fusion approach is, however, structurally restricted due to its intrinsic limitation of potentially dropping off a large fraction of modality-specific representational strengths due to largely imperfect information fusion into a unified representation.\n\nTo overcome the aforementioned limitations, in this work \na novel {\\bf\\em modality interaction} strategy (Figure~\\ref{fig:teaser}(b)) for multi-modal 3D object detection is introduced.\nOur key idea is that, instead of deriving a fused single representation,\nwe learn and maintain two modality-specific representations throughout to enable  \ninter-modality interaction so that both information exchange and modality-specific strengths can be achieved spontaneously. \nOur strategy is implemented by formulating a \\methodname{} architecture.\nIt starts by mapping 3D point clouds and 2D multi-view images into LiDAR BEV feature and image perspective feature with two separate feature backbones in parallel.\nSubsequently, an encoder interacts the two features for progressive information exchange and representation learning in a {\\em bilateral} manner.\nTo fully exploit per-modality representations, \na decoder/head is further designed \nto conduct multi-modal predictive interaction \nin a cascaded manner.\n\nOur {\\bf contributions} are summarized as follows:\n{\\bf (i)} \nWe propose a novel {\\em modality interaction} strategy for \nmulti-modal 3D object detection, with the aim to resolve a fundamental limitation of previous {\\em modality fusion} strategy in dropping the unique perception strengths per modality.\n{\\bf (ii)} \nTo implement our proposed strategy, we formulate a {\\methodname{}} architecture with a multi-modal representational interaction encoder\nand a multi-modal predictive interaction decoder.\n{\\bf (iii)} \nExtensive experiments on the nuScenes dataset show that our \\methodname{} yields new state of the art for multi-modality 3D object detection and achieves the first position at the highly competitive nuScenes leaderboard."
            },
            "section 2": {
                "name": "Related work",
                "content": "\n\n\\paragraph{3D object detection with single modality}\nAutomated driving vehicles are generally equipped with both LiDAR and multiple surround-view cameras. But many previous methods perform 3D object detection by exploiting data captured from only a single form of sensor. For camera-based 3D object detection, since depth information is not directly accessible from RGB images, some previous works~\\cite{huang2021bevdet,wang2019pseudo,reading2021caddn} lift 2D features into a 3D space by conducting depth estimation, followed by performing object detection in the 3D space. Another line of works~\\cite{wang2022detr3d,liu2022petr,li2022bevformer,lu2022ego3rt,jiang2022polar} resort to the detection Transformer~\\cite{carion2020detr} architecture. They leverage 3D object queries and 3D-2D correspondence to incorporate 3D computation into the detection pipelines.\nDespite the rapid progress of camera-based approaches, the state-of-the-art of 3D object detection is still dominated by LiDAR-based methods. Most of LiDAR-based detectors quantify point clouds into regular grid structures such as voxels~\\cite{zhou2018voxelnet,yan2018second}, pillars~\\cite{pointpillars} or range images~\\cite{bewley2020range,fan2021rangedet,Chai2021ToTP} before processing them. Due to the sampling characteristics of LiDAR, these grids are naturally sparse and hence fit the Transformer design. So a number of approaches~\\cite{mao2021votr,fan2021sst} have applied the Transformer for point cloud feature extraction. Differently, several methods use the Transformer decoder or its variants as their detection head~\\cite{bai2022transfusion, wang2021objectdgcnn}. \n3DETR~\\cite{misra2021-3detr} adopts a complete Transformer encoder-decoder architecture with less priori in design.\nDue to intrinsic limitations with either sensor,\nthese methods are largely limited in performance.\n\n\\paragraph{Multi-modality fusion for 3D object detection}\nLeveraging the perception data from both camera and LiDAR sensors\nusually leads to improved performance.\nThis has emerged as a promising direction.\nExisting 3D detection methods typically perform multi-modal fusion\nat one of the three stages: raw input, intermediate feature, and object proposal.\nFor example, PointPainting~\\cite{vora2020pointpainting} is the pioneering input fusion method~\\cite{wang2021pointaugmenting,huang2020epnet, Yin2021MVP}. \nThe main idea is to decorate the 3D point clouds with the category scores or semantic features from the 2D instance segmentation network.\n%\nWhilst\n4D-Net~\\cite{piergiovanni20214d} placed the fusion module in the point cloud feature extractor for allowing the point cloud features to dynamically attend to the image features. ImVoteNet~\\cite{qi2020imvotenet} injects visual information into a set of 3D seed points abstracted from raw point clouds. \n%\nThe proposal based fusion methods~\\cite{ku2018joint,chen2022futr3d} keep the feature extraction of two modalities independently and aggregate multi-modal features via proposals or queries at the detection head. \nThe first two categories of methods take a unilateral fusion strategy \nwith bias to 3D LiDAR modality due to the superiority of point clouds in distance and spatial perception.\nInstead, the last category fully ignores the intrinsic association between the two modalities in representation.\nAs a result, all above previous methods fail to fully exploit both modalities, in particular their strong complementary nature.\nBesides, a couple of concurrent works have explored fusion of the two modalities in a shared representation space~\\cite{liu2022bevfusion,liang2022bevfusion}. \nThey conduct view transformation in the same way~\\cite{philion2020lift} as in the camera-only approach. This design is however less effective in exploiting\nthe spatial cues of point clouds during view transformation,\npotentially compromising the quality of camera BEV representation.\nThis gives rise to an extra need of calibrating such misalignment in network capacity.\n\nIn this work, we address the aforementioned limitations in all previous solutions with a novel multi-modal interaction strategy. \nThe key insight behind our approach is that we maintain two modality-specific feature representations and conduct {\\em representational} and {\\em predictive} interaction for \nmaximally exploring their complementary benefits whilst\npreserving their respective strengths."
            },
            "section 3": {
                "name": "Method",
                "content": "\n\\label{sec:method}\nWe present a novel modality interaction framework, dubbed \\methodname, for multi-modal (3D point clouds and 2D multi-camera images) 3D object detection.\nIn contrast to all prior arts, \n% we treat the 3D LiDAR and 2D camera modalities with \\textit{equally importance} and thus \nwe learn two representations specific for 3D LiDAR and 2D image modalities respectively, whilst\nconducting multi-modal interaction through both model encoding and decoding. An overview of \\methodname is shown in Figure~\\ref{fig:teaser}(b). It consists of two main components: An encoder with multi-modal representational interaction (Section \\ref{sec:encoder}), and a decoder with multi-modal predictive interaction (Section \\ref{sec:decoder}).\n\n",
                "subsection 3.1": {
                    "name": "Encoder: Multi-modal representational interaction",
                    "content": "\n\\label{sec:encoder}\nUnlike conventional modality fusion strategy that often aggregates multi-modal inputs into a hybrid feature map, \n{\\bf \\em individual per-modality representations} are learned and maintained via \n\\textit{multi-modal representational interaction} within our encoder.\nSpecifically, our encoder is formulated as a {\\em multi-input-multi-output} (MIMO) structure: Taking as input two modality-specific scene representations which are independently extracted by LiDAR and image backbones, and producing two refined representations as output.\nOverall, it is composed by stacking multiple encoder layers each \nwith {\\em (I) multi-modal representational interaction} (MMRI),\n{\\em (II) intra-modal representational learning} (IML),\nand {\\em (III) representational integration}.\n\n\n\\paragraph{(I) Multi-modal representational interaction (MMRI)}\nEach encoder layer takes the representations of two modalities, \\ie, the image perspective representation $\\boldsymbol{h}_c$ and the LiDAR BEV representation $\\boldsymbol{h}_p$, as input. Our multi-modal representational interaction aims to exchange the neighboring context in a bilateral cross-modal manner,\nas shown in Figure~\\ref{fig:encoder}.\nIt consists of two steps: \n\n\\paragraph{\\em (i) Cross-modal correspondence mapping and sampling}\nTo define cross-modality adjacency, we first need to build the pixel-to-pixel(s) correspondence between the representations $\\vh_p$ and $\\vh_c$.\nTo that end, we construct dense mappings between the image coordinate frame $c$ and the BEV coordinate frame $p$ ($\\mathcal{M}_{p\\rightarrow c}$ and $\\mathcal{M}_{c\\rightarrow p}$).\n\n{\\em From image to LiDAR BEV coordinate frame} $\\mathcal{M}_{c\\rightarrow p}:\\mathbb{R}^2 \\rightarrow 2^{\\mathbb{R}^2}$\n(Figure~\\ref{fig:encoder}(a)):\nWe first project each point $(x,y,z)$ in 3D point cloud to multi-camera images to form a sparse depth map $\\vd_{sparse}$, followed by depth completion~\\cite{ku2018defense} leading to a dense depth map $\\vd_{dense}$.\nWe further utilize $\\vd_{dense}$ to back-project each pixel in the image space into the 3D point space. This results in the corresponding 3D coordinate $(x,y,z)$, given an image pixel $(i,j)$ with depth $\\vd_{dense}^{[i,j]}$. Next, $(x,y)$ is used to locate the corresponding BEV coordinate $(i_p,j_p)$.\nWe denote the above mapping as $T(i,j)=(i_p,j_p)$. \nWe obtain this correspondence via $(2k+1) \\times (2k+1)$ sized neighbor sampling as\n$\\mathcal{M}_{c\\rightarrow p}(i,j)=\n\\left\\{ T(i+\\Delta i, j+\\Delta j) | \\Delta i , ~\\Delta j \n\\in [-k, +k]\n\\right\\}\n$.\n\n{\\em From LiDAR BEV to image coordinate frame} $\\mathcal{M}_{p\\rightarrow c}:\\mathbb{R}^2 \\rightarrow 2^{\\mathbb{R}^2}$ (Figure~\\ref{fig:encoder}(b)):\nGiven a coordinate $(i_p,j_p)$ in BEV, we first obtain the LiDAR points $\\left \\{(x,y,z)\\right \\}$ within the pillar corresponding to $(i_p,j_p)$.\nThen we project these 3D points into camera image coordinate frame $\\left \\{(i,j)\\right \\}$ according to the camera intrinsics and extrinsics. \nThis correspondence is obtained as:\n$\\mathcal{M}_{p\\rightarrow c}(i_p,j_p)=\\left\\{(i,j)\\right\\}$.\n\n\\paragraph{\\em (ii) Attention-based feature interaction}\nFor an image feature point as query $\\bm{q}=\\boldsymbol{h}_{c}^{[i_c,j_c]}$, \nits cross-modality neighbors $\\mathcal{N}_{q}$, denoted as $\\mathcal{N}_q = \\vh_p^{\\left[\\mathcal{M}_{c\\rightarrow p}(i_c,j_c)\\right]}$,\nare used as the \\texttt{key} $\\bm{k}$ and \\texttt{value} $\\bm{v}$ for cross-attention learning:\n\\begin{equation}\n    f_{{\\phi}_{c \\rightarrow p}}\n    \\left( \\vh_c, \\vh_p \\right)\n    ^{[i,j]} = \\sum_{\\bm{k},\\bm{v} \\in \\mathcal{N}_q}^{}\\text{softmax}\\left (  \\frac{\\bm{qk}}{\\sqrt{d} }\\right ) \\bm{v},\n\\label{eq:local_attention}\n\\end{equation}\nwhere $\\vh^{[i, j]}$ denotes indexing the element at location $(i,j)$ on the 2D representation $\\vh$. \nThis is {\\em image-to-LiDAR representational interaction}.\n\nThe other way around, given a LiDAR BEV feature point as query\n$\\bm{q}=\\boldsymbol{h}_{p}^{[i_p,j_p]}$, we similarly obtain its cross-modality neighbors as $\\mathcal{N}_q = \\vh_c^{\\left[\\mathcal{M}_{p\\rightarrow c}(i_p,j_p)\\right]}$.\nThe same process (Eq. (\\ref{eq:local_attention})) is applied\nfor realizing {\\em LiDAR-to-image representational interaction} \n$f_{{\\phi}_{p \\rightarrow c}} \\left( \\vh_c, \\vh_p \\right)$.\n\n\\paragraph{(II) Intra-modal representational learning (IML)}\nConcurrently, we conduct intra-modal representational learning\ncomplementary to multi-modal interaction.\nThe same local attention as defined in Eq.~(\\ref{eq:local_attention})\nis consistently applied.\nFor either modality, we use a $k \\times k$ grid neighborhood as the \\texttt{key} and \\texttt{value}.\nFormally, we denote $f_{{\\phi}_{c \\rightarrow c}} (\\bm{h}_c)$ for image representation and $f_{{\\phi}_{p \\rightarrow p}} (\\bm{h}_p)$ for LiDAR representation.\n\n\\paragraph{(III) Representational integration}\nEach layer ends up by integrating the two outputs per modality:\n\\begin{equation}\n\\begin{aligned}\n    \\boldsymbol{h}_p^\\prime & =  \\text{FFN}(\\text{Concat}(\\text{FFN}(\\text{Concat}(\\boldsymbol{h}_p^{p \\rightarrow p},~\\boldsymbol{h}_p^{c \\rightarrow p})),~\\boldsymbol{h}_p)), \n    \\\\\n    \\boldsymbol{h}_c^\\prime & =  \\text{FFN}(\\text{Concat}(\\text{FFN}(\\text{Concat}(\\boldsymbol{h}_c^{c \\rightarrow c},~\\boldsymbol{h}_c^{p \\rightarrow c})),~\\boldsymbol{h}_c)),\n    \\label{eq:ffn_fusion}\n\\end{aligned}\n\\end{equation}\nwhere FFN specifies a feed-forward network, and Concat denotes element-wise concatenation.\n\n"
                },
                "subsection 3.2": {
                    "name": "Decoder: Multi-modal predictive interaction",
                    "content": "\n\\label{sec:decoder}\nBeyond considering the multi-modal interaction at the representation-level, we further introduce a decoder with {\\em multi-modal predictive interaction }(MMPI) to maximize the complementary effects in prediction.\nAs depicted in Figure~\\ref{fig:decoder_layer}(a), our core idea is to enhance the 3D object detection of one modality conditioned on the other modality.\nIn particular, the decoder is built by stacking multiple {\\em multi-modal predictive interaction layers}, within which {\\em predictive interactions} are formulated in an alternative and progressive manner. \nSimilar to the decoder of DETR \\cite{carion2020detr}, we cast the 3D object detection as a set prediction problem. \nHere, we define a set of $N$ object queries $\\left \\{\\bm{Q}_{n}\\right \\}^N_{n=1}$ and the resulting $N$ object predictions $ \\left \\{ (\\vb_n, \\vc_n) \\right \\}^N_{n=1}$, where $\\vb_n$ and $\\vc_n$ denote the predicted bounding box and category for the $n$-th prediction.\n\n\\paragraph{Multi-modal predictive interaction layer (MMPI)}\nFor the $l$-th decoding layer, the set prediction is computed by taking the object queries $\\left \\{\\bm{Q}_{n}^{(l-1)}\\right \\}^N_{n=1}$ and the bounding box predictions $\\left \\{\\bm{b}_{n}^{(l-1)}\\right \\}^N_{n=1}$ from previous layer as inputs and \nenabling interaction with the intensified image $\\vh_p^{\\prime}$ or LiDAR $\\vh_c^\\prime$ representations ($\\vh_c^{\\prime}$ if $l$ is odd, $\\vh_p^{\\prime}$ if $l$ is even).\nWe formulate the multi-modal predictive interaction layer (Figure~\\ref{fig:decoder_layer}(b)) for specific modality as follows:\n\n\\paragraph{(I) Multi-modal predictive interaction on image representation (MMPI-image)}\nTaking as input 3D object proposals $\\left \\{\\bm{b}_{n}^{(l-1)}\\right \\}^N_{n=1}$ and object queries $\\left \\{\\bm{Q}_{n}^{(l-1)}\\right \\}^N_{n=1}$ generated by the previous layer, \nthis layer leverages the image representation $\\vh_c^{\\prime}$\nfor further prediction refinement. \nTo integrate the previous predictions $\\left \\{\\bm{b}_{n}^{(l-1)}\\right \\}^N_{n=1}$, \nwe first extract $N$ Region of Interest (RoI)~\\cite{he2017maskrcnn} features $\\left \\{ \\bm{R}_{n} \\right \\}^N_{n=1}$ from the image representation $\\vh_c^{\\prime}$, where $\\bm{R}_{n} \\in \\mathbb{R}^{S \\times S \\times C}$ is the extracted RoI feature for the $n$-th query, $(S \\times S)$ is RoI size, and $C$ is the number of channels of RoI features. \nSpecifically, for each 3D bounding box, we project it onto image representation $\\vh_c^{\\prime}$ to get the 2D convex polygon and take the minimum {\\em axis-aligned} circumscribed rectangle.\nWe then design a multi-modal predictive\ninteraction operator\nthat maps $\\left \\{\\bm{Q}_{n}^{(l-1)}\\right \\}^N_{n=1}$ into the parameters of a series of $1 \\times 1$ convolutions and then applies them consecutively to $\\left \\{ \\bm{R}_{n} \\right \\}^N_{n=1}$;\nThe resulted interactive representation is further used\nto obtain the updated object query $\\left \\{\\bm{Q}_{n}^{l}\\right \\}^N_{n=1}$.\n\n\\paragraph{(II) Multi-modal predictive interaction on LiDAR representation (MMPI-LiDAR)}\nThis layer shares the same design as the above \nexcept that it takes as input LiDAR representation instead.\nWith regards to the RoI for LiDAR representation, we project the 3D bounding boxes from previous layer to the LiDAR BEV representation $\\vh_p^{\\prime}$ and take the minimum {\\em axis-aligned} rectangle.\nIt is worth mentioning that due to the scale of objects in autonomous driving scenarios is usually tiny in the BEV coordinate frame, we enlarge the scale of the 3D bounding box by 2 times. \nThe shape of RoI features cropped from the LiDAR BEV representation $\\vh_p^{\\prime}$ is also set to be $S \\times S \\times C$. Here $C$ is the $C$ is the number of channels of RoI features as well as the height of BEV representation.\nThe multi-modal predictive interaction layer on LiDAR representation is stacked on the above image counterpart.\n\nFor object detection, a feed-forward network is appended on the $\\left \\{\\bm{Q}_{n}^{l}\\right \\}^N_{n=1}$ for each multi-modal predictive interaction layer to infer the locations, dimensions, orientations and velocities. \nDuring training, the matching cost and loss function as~\\cite{bai2022transfusion} are applied.\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:experiments}\n\n",
                "subsection 4.1": {
                    "name": "Experimental setup",
                    "content": "\n\n\\paragraph{Dataset}\nWe evaluate our approach on the nuScenes dataset~\\cite{caesar2020nuscenes}. \nIt provides point clouds from 32-beam LiDAR and images with a resolution of $1600 \\times 900$ from 6 surrounding cameras.\nThe total of 1000 scenes, where each sequence is roughly 20 seconds long and annotated every 0.5 second, is officially split into \\texttt{train/val/test} set with 700/150/150 scenes. \nFor the 3D object detection task, 1.4M objects in scenes are annotated with 3D bounding boxes and classified into 10 categories: car, truck, bus, trailer, construction vehicle, pedestrian, motorcycle, bicycle, barrier, and traffic cone.\n\n\\paragraph{Metric}\nMean average precision (mAP)~\\cite{everingham2010pascal} and nuScenes detection score (NDS)~\\cite{caesar2020nuscenes} are used as the evaluation metric of 3D detection performance. \nThe final mAP is computed by averaging over the distance thresholds of 0.5m, 1m, 2m, 4m across 10 classes.\nNDS is a weighted average of mAP and other attribute metrics, including translation, scale, orientation, velocity, and other box attributes. \n\n\n"
                },
                "subsection 4.2": {
                    "name": "Implementation details",
                    "content": "\n\\label{sec:details}\n\nOur implementation is based on the public code base \\textit{mmdetection3d}~\\cite{mmdet3d2020}. \nFor the image branch backbone, we use a simple \\textbf{{\\em ResNet-50}}~\\cite{he2016deep} and initialize it from the instance segmentation model {\\em Cascade Mask R-CNN}~\\cite{cai2019cascade} pretrained on COCO~\\cite{lin2014coco} and then nuImage~\\cite{caesar2020nuscenes}, which is same as Transfusion~\\cite{bai2022transfusion}.\nTo save the computation cost, we rescale the input image to 1/2 of its original size before feeding into the network, and {\\em freeze} the weights of image branch during training. \nThe voxel size is set to $(0.075m, 0.075m, 0.2m)$, and the detection range is set to $[-54m, 54m]$ for $X$ and $Y$ axis and $[-5m, 3m]$ for $Z$ axis. \nThe representational interaction encoder is composed by stacking two representational interaction layers. For the multi-modal predictive interaction decoder, we use 5 cascaded decoder layers. \nWe set the query number to 200 for training and testing and use the same query initialization method as Transfusion~\\cite{bai2022transfusion}.\nThe above configuration is termed \\texttt{DeepInteraction-base}.\n\nWe also adopt another two widely used settings for online submission, \\ie, test-time augmentation (TTA) and model ensemble. \nIn the following, we refer to the two settings as \\texttt{DeepInteraction-large} and \\texttt{DeepInteraction-e} respectively. In particular, \\texttt{DeepInteraction-large} uses Swin-Tiny~\\cite{liu2021Swin} as image backbone, and doubles the number of channel for each convolution block in LiDAR backbone. \nThe voxel size of \\texttt{DeepInteraction-large} is set to $[0.5m, 0.5m, 0.2m]$.\nFollowing the common practice, we use double flipping and rotation with yaw angles $[0^{\\circ}, \\pm 6.25^{\\circ}, \\pm 12.5^{\\circ}]$ for test-time augmentation. \\texttt{DeepInteraction-e} ensembles multiple DeepInteraction-large models with input LiDAR BEV grid size between $[0.5m, 0.5m]$ and $[1.5m, 1.5m]$.\n\nFor data augmentation, following TransFusion~\\cite{bai2022transfusion} we adopt random rotation with a range of $r \\in \\left[ -\\pi/4,\\pi/4 \\right]$, random scaling with a factor of  $r \\in \\left[ 0.9,1.1 \\right]$, random translation with standard deviation 0.5 in three axis, and random horizontal flipping. \nWe also use the class-balanced re-sampling in CBGS~\\cite{zhu2019class} to balance the class distribution for nuScenes.\nFollowing~\\cite{bai2022transfusion}, we adopt a two stage training recipe.\nWe take TransFusion-L~\\cite{bai2022transfusion} as our \\texttt{LiDAR-only baseline}.\nWe use Adam optimizer with one-cycle learning rate policy, with max learning rate $1 \\times 10^{-3}$, weight decay 0.01 and momentum 0.85 to 0.95, following CBGS~\\cite{zhu2019class}.\nOur LiDAR-only baseline is trained for 20 epochs and LiDAR-image fusion for 6 epochs with batch size of 16 using 8 NVIDIA V100 GPUs.\n\n"
                },
                "subsection 4.3": {
                    "name": "Comparison to the state of the art",
                    "content": "\n\\label{sec:sota}\n\n\\paragraph{Performance} We compare with state-of-the-art alternatives on the nuScenes test set. \nAs shown in Table~\\ref{tab:nuscene_test}, \\methodname achieves new state-of-the-art performance under all settings.\nThe base variant without TTA and model ensemble, \\texttt{DeepInteraction-base}, \\textbf{with a simple  ResNet-50 image backbone}, surpasses all the prior arts as well as the concurrent work BEVFusion~\\cite{liu2022bevfusion} even with a Swin-Tiny image backbone.\n\\texttt{DeepInteraction-large} beats the closest rival LargeKernel3D-F~\\cite{chen2022scaling} with the same TTA and test time augmentation (single model) by a considerable margin.\nOur ensemble version \\texttt{DeepInteraction-e} achieves the \\texttt{first} rank among all the solutions on the nuScenes leaderboard.\nThese results verify the performance advantages of our multi-modal interaction approach. \nPer-category results are shown in Appendix~\\ref{appendixa}.\nQualitative results are provided in Figure~\\ref{fig:quanlitative_result_nuscenes} and Appendix~\\ref{appendixb}.\n\n\n\\paragraph{Run time} \nWe compare inference speed tested on NVIDIA V100, A6000 GPUs and A100 separately.\n%\nAs shown in Table~\\ref{tab:efficiency}, our method achieves the best performance while running faster than alternative painting-based~\\cite{wang2021pointaugmenting} and query-based~\\cite{chen2022futr3d} fusion approaches.\nThis validates superior trade-off between detection performance and inference speed achieved by our method.\nAs found in~\\cite{wang2021pointaugmenting}, feature extraction for multi-view high resolution camera images contributes the most of overall latency in a multi-modal 3D detector. \nIndeed, from Table~\\ref{tab:tab3}(c) we observe that increasing the number of decoder layers only brings negligible extra latency, which concurs with the same conclusion.\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablations on the decoder",
                    "content": "\n\n\n{\\bf Multi-modal predictive interaction layer vs. DETR~\\cite{carion2020detr} decoder layer}\nIn Table~\\ref{tab:tab3}(a) we evaluate the design of decoder layer by comparing our \nmulti-modal predictive interaction (MMPI) with \nDETR~\\cite{carion2020detr} decoder layer.\nNote the DETR decoder layer means the conventional Transformer deocder layer is used to aggregate multi-modal information same as in Transfusion~\\cite{bai2022transfusion}.\nWe further test a mixing design:\nusing vanilla DETR decoder layer for aggregating features in LiDAR representation and our MMPI for aggregating features in image representation\n(second row).\nIt is evident that MMPI is significantly superior over DETR by improving 1.3\\% mAP and 1.0\\% NDS, \nwith combinational flexibility in design.\n\n\n{\\bf How many representations/modalities?}\nIn Table~\\ref{tab:tab3}(b), we evaluate the effect of using different numbers of representations/modalities in decoding.\nWe compare our MMPI using both representations in an alternating manner with a variant using LiDAR representation in all decoder layers.\n% under the same network structure.\nIt is observed that using both representations\nis beneficial, verifying our design consideration. \n\n\n\n{\\bf Number of decoder layers}\nAs shown in Table~\\ref{tab:tab3}(c), increasing the number of decoder layers up to 5 layers can consistently improve the performance\n% (except that 6 layers experience a little performance drop) \nwhilst introducing negligible latency.\nLiDAR-only denotes the Transfusion-L~\\cite{bai2022transfusion} baseline.\n\n{\\bf Number of queries}\nSince our query embeddings are initialized in a non-parametric and input-dependent manner as in ~\\cite{bai2022transfusion}, \nthe number of queries are adjustable during inference. \nIn Table~\\ref{tab:tab3}(d), we evaluate different combinations of query numbers for training and test. \nOverall, the performance is stable over different choices with 200/300 for training/test as the best setting.\n\n"
                },
                "subsection 4.5": {
                    "name": "Ablations on the encoder",
                    "content": "\n\n{\\bf Multi-modal representational interaction vs. fusion}\nTo precisely demonstrate the superiority of our multi-modal representational interaction, we compare a naive version of our \\methodname with conventional representational fusion strategy as presented in Transfusion~\\cite{bai2022transfusion}. We limit our \\methodname using the same number of encoder and decoder layers as~\\cite{bai2022transfusion} for a fair comparison. Table~\\ref{tab:tab4}(c) shows that our representational interaction is clearly more effective.\n\n\n{\\bf Encoder design}\nWe ablate the design of our encoder with focus on multi-modal representational interaction (MMRI) and intra-modal representational learning (IML).\nWe have a couple of observations from Table~\\ref{tab:tab4}(a):\n(1) Our MMRI can significantly improve the performance over IML;\n(2) MMRI and IML can work well together for further performance gain.\nAs seen from Table~\\ref{tab:tab4}(b),\nstacking our encoder layers for iterative MMRI\nis beneficial.\n\n{\\bf Qualitative results of representational interaction}\nTo gain more insight about the effect of our multi-modal representational interaction (MMRI), we visualize the heatmaps of challenging cases. \nWe observe from Figure~\\ref{fig:heatmap_vis_1} that \nwithout the assistance of MMRI, \nsome objects cannot be detected when \nusing LiDAR only (the middle column).\nThe locations of these objects are highlighted by red circles in the heatmap and white bounding boxes in the RGB image below.\n\nConcretely, the sample (a) suggests that camera information is helpful to recover partially obscured tiny objects with sparse observation in the point cloud.\nThe sample (b) shows a representative case where some distant objects can be recognized successfully due to the help of visual information.\nFrom the sample (c), we observe that the centers of some barriers yield a more distinct activation in the heatmap after representational interaction.\nThis is probably due to that it is too difficult to locate the boundaries of several consecutive barriers from LiDAR point clouds only.\n\n"
                },
                "subsection 4.6": {
                    "name": "Ablation on LiDAR backbones",
                    "content": "\n\nWe examine the generality of our framework with two different LiDAR backbones: PointPillars~\\cite{pointpillars} and VoxelNet~\\cite{zhou2018voxelnet}.\nFor PointPillars, we set the voxel size to (0.2m, 0.2m) while keeping the remaining settings same as DeepInteraction-base.\nFor fair comparison, we use the same number of queries as TransFusion~\\cite{bai2022transfusion}.\nAs shown in Table~\\ref{tab:abliation_for_backbone},\ndue to the proposed multi-modal interaction strategy, \\methodname{} exhibits consistent improvements over LiDAR-only baseline using either backbone (by 5.5\\% mAP for voxel-based backbone, and 4.4\\% mAP for pillar-based backbone). \nThis manifests the generality of our \\methodname~across varying point cloud encoder. \n\n"
                },
                "subsection 4.7": {
                    "name": "Performance breakdown",
                    "content": "\nTo demonstrate more fine-grained performance analysis, we compare our \\methodname and our LiDAR-only baseline Transfusion~\\cite{bai2022transfusion} at the category level in terms of mAP on nuScenes {\\tt val} set. We can see from  Table~\\ref{tab:nuscenes_val} that our fusion approach achieves remarkable improvements on all the categories, \nespecially on tiny or rare object categories (+11.8\\% mAP for bicycle, +6.9\\% mAP for motorcycle, and +5.9\\% mAP for traffic cone)."
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\nIn this work, we have presented a novel 3D object detection method \\methodname{} for exploring the intrinsic multi-modal complementary nature.\nThis key idea is to maintain two modality-specific representations and establish interactions between them for both representation learning and predictive decoding.\nThis strategy is designed particularly to resolve the fundamental limitation of existing unilateral fusion approaches that image representation are insufficiently exploited due to their auxiliary-source role treatment.\n%\nExtensive experiments demonstrate our proposed \\methodname yields new state of the art on the nuScenes benchmark dataset and achieves the first position at the highly competitive nuScenes 3D object detection leaderboard.\n\n\n\n\\paragraph{Acknowledgement}\nThis work was supported in part by \nNational Natural Science Foundation of China (Grant No. 6210020439),\nLingang Laboratory (Grant No. LG-QS-202202-07),\nNatural Science Foundation of Shanghai (Grant No. 22ZR1407500),\nScience and Technology Innovation 2030 - Brain Science and Brain-Inspired Intelligence Project (Grant No. 2021ZD0200204) and \nShanghai Municipal Science and Technology Major Project (Grant No. 2018SHZDZX01).\n\n% \\clearpage\n\n% \\section*{References}\n\\begin{thebibliography}{10}\n\n\\bibitem{bai2022transfusion}\nXuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and\n  Chiew-Lan Tai.\n\\newblock Transfusion: Robust lidar-camera fusion for 3d object detection with\n  transformers.\n\\newblock In {\\em CVPR}, 2022.\n\n\\bibitem{bewley2020range}\nAlex Bewley, Pei Sun, Thomas Mensink, Dragomir Anguelov, and Cristian\n  Sminchisescu.\n\\newblock Range conditioned dilated convolutions for scale invariant 3d object\n  detection.\n\\newblock {\\em arXiv preprint}, 2020.\n\n\\bibitem{caesar2020nuscenes}\nHolger Caesar, Varun Bankiti, Alex~H Lang, Sourabh Vora, Venice~Erin Liong,\n  Qiang Xu, Anush Krishnan, Yu~Pan, Giancarlo Baldan, and Oscar Beijbom.\n\\newblock nuscenes: A multimodal dataset for autonomous driving.\n\\newblock In {\\em CVPR}, 2020.\n\n\\bibitem{cai2019cascade}\nZhaowei Cai and Nuno Vasconcelos.\n\\newblock Cascade r-cnn: high quality object detection and instance\n  segmentation.\n\\newblock {\\em IEEE TPAMI}, 2019.\n\n\\bibitem{carion2020detr}\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander\n  Kirillov, and Sergey Zagoruyko.\n\\newblock End-to-end object detection with transformers.\n\\newblock In {\\em ECCV}, 2020.\n\n\\bibitem{Chai2021ToTP}\nYuning Chai, Pei Sun, Jiquan Ngiam, Weiyue Wang, Benjamin Caine, Vijay\n  Vasudevan, Xiao Zhang, and Drago Anguelov.\n\\newblock To the point: Efficient 3d object detection in the range image with\n  graph convolution kernels.\n\\newblock In {\\em CVPR}, 2021.\n\n\\bibitem{chen2022futr3d}\nXuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, and Hang Zhao.\n\\newblock Futr3d: A unified sensor fusion framework for 3d detection.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{chen2022focal}\nYukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia.\n\\newblock Focal sparse convolutional networks for 3d object detection.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{chen2022scaling}\nYukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang, Jian Sun, and Jiaya Jia.\n\\newblock Scaling up kernels in 3d cnns.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{chen2022autoalign}\nZehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinghong Jiang, Feng Zhao,\n  Bolei Zhou, and Hang Zhao.\n\\newblock Autoalign: Pixel-instance feature aggregation for multi-modal 3d\n  object detection.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{mmdet3d2020}\nMMDetection3D Contributors.\n\\newblock {MMDetection3D: OpenMMLab} next-generation platform for general {3D}\n  object detection.\n\\newblock \\url{https://github.com/open-mmlab/mmdetection3d}, 2020.\n\n\\bibitem{everingham2010pascal}\nMark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew\n  Zisserman.\n\\newblock The pascal visual object classes (voc) challenge.\n\\newblock {\\em IJCV}, 2010.\n\n\\bibitem{fan2021sst}\nLue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan\n  Wang, and Zhaoxiang Zhang.\n\\newblock Embracing single stride 3d object detector with sparse transformer.\n\\newblock {\\em arXiv preprint}, 2021.\n\n\\bibitem{fan2021rangedet}\nLue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, and ZhaoXiang Zhang.\n\\newblock Rangedet: In defense of range view for lidar-based 3d object\n  detection.\n\\newblock In {\\em ICCV}, 2021.\n\n\\bibitem{he2017maskrcnn}\nKaiming He, Georgia Gkioxari, Piotr Doll{\\'a}r, and Ross Girshick.\n\\newblock Mask r-cnn.\n\\newblock In {\\em ICCV}, 2017.\n\n\\bibitem{he2016deep}\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\\newblock Deep residual learning for image recognition.\n\\newblock In {\\em CVPR}, 2016.\n\n\\bibitem{huang2021bevdet}\nJunjie Huang, Guan Huang, Zheng Zhu, and Dalong Du.\n\\newblock Bevdet: High-performance multi-camera 3d object detection in\n  bird-eye-view.\n\\newblock {\\em arXiv preprint}, 2021.\n\n\\bibitem{huang2020epnet}\nTengteng Huang, Zhe Liu, Xiwu Chen, and Xiang Bai.\n\\newblock Epnet: Enhancing point features with image semantics for 3d object\n  detection.\n\\newblock In {\\em ECCV}, 2020.\n\n\\bibitem{jiang2022polar}\nYanqin Jiang, Li~Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, and\n  Yu-Gang Jiang.\n\\newblock Polarformer: Multi-camera 3d object detection with polar\n  transformers.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{ku2018defense}\nJason Ku, Ali Harakeh, and Steven~L Waslander.\n\\newblock In defense of classical image processing: Fast depth completion on\n  the cpu.\n\\newblock In {\\em CRV}, 2018.\n\n\\bibitem{ku2018joint}\nJason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven~L Waslander.\n\\newblock Joint 3d proposal generation and object detection from view\n  aggregation.\n\\newblock In {\\em IROS}, 2018.\n\n\\bibitem{pointpillars}\nAlex~H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar\n  Beijbom.\n\\newblock Pointpillars: Fast encoders for object detection from point clouds.\n\\newblock In {\\em CVPR}, 2019.\n\n\\bibitem{li2022vff}\nYanwei Li, Xiaojuan Qi, Yukang Chen, Liwei Wang, Zeming Li, Jian Sun, and Jiaya\n  Jia.\n\\newblock Voxel field fusion for 3d object detection.\n\\newblock In {\\em CVPR}, 2022.\n\n\\bibitem{li2022deepfusion}\nYingwei Li, Adams~Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng,\n  Junyang Shen, Bo~Wu, Yifeng Lu, Denny Zhou, et~al.\n\\newblock Deepfusion: Lidar-camera deep fusion for multi-modal 3d object\n  detection.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{li2022bevformer}\nZhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu,\n  and Jifeng Dai.\n\\newblock Bevformer: Learning bird's-eye-view representation from multi-camera\n  images via spatiotemporal transformers.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{liang2022bevfusion}\nTingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao\n  Wang, Tao Tang, Bing Wang, and Zhi Tang.\n\\newblock {BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework}.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{lin2014coco}\nT.~Y. Lin, M.~Maire, S.~Belongie, J.~Hays, and C.~L. Zitnick.\n\\newblock Microsoft coco: Common objects in context.\n\\newblock In {\\em ECCV}, 2014.\n\n\\bibitem{liu2022petr}\nYingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.\n\\newblock Petr: Position embedding transformation for multi-view 3d object\n  detection.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{liu2021Swin}\nZe~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\n  Baining Guo.\n\\newblock Swin transformer: Hierarchical vision transformer using shifted\n  windows.\n\\newblock In {\\em ICCV}, 2021.\n\n\\bibitem{liu2022bevfusion}\nZhijian Liu, Haotian Tang, Alexander Amini, Xingyu Yang, Huizi Mao, Daniela\n  Rus, and Song Han.\n\\newblock Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye\n  view representation.\n\\newblock {\\em arXiv preprint}, 2022.\n\n\\bibitem{lu2022ego3rt}\nJiachen Lu, Zheyuan Zhou, Xiatian Zhu, Hang Xu, and Li~Zhang.\n\\newblock Learning ego 3d representation as ray tracing.\n\\newblock In {\\em ECCV}, 2022.\n\n\\bibitem{mao2021votr}\nJiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang,\n  Hang Xu, and Chunjing Xu.\n\\newblock Voxel transformer for 3d object detection.\n\\newblock In {\\em CVPR}, 2021.\n\n\\bibitem{misra2021-3detr}\nIshan Misra, Rohit Girdhar, and Armand Joulin.\n\\newblock {An End-to-End Transformer Model for 3D Object Detection}.\n\\newblock In {\\em ICCV}, 2021.\n\n\\bibitem{philion2020lift}\nJonah Philion and Sanja Fidler.\n\\newblock Lift, splat, shoot: Encoding images from arbitrary camera rigs by\n  implicitly unprojecting to 3d.\n\\newblock In {\\em ECCV}, 2020.\n\n\\bibitem{piergiovanni20214d}\nAJ~Piergiovanni, Vincent Casser, Michael~S Ryoo, and Anelia Angelova.\n\\newblock 4d-net for learned multi-modal alignment.\n\\newblock In {\\em CVPR}, 2021.\n\n\\bibitem{qi2020imvotenet}\nCharles~R Qi, Xinlei Chen, Or~Litany, and Leonidas~J Guibas.\n\\newblock Imvotenet: Boosting 3d object detection in point clouds with image\n  votes.\n\\newblock In {\\em CVPR}, 2020.\n\n\\bibitem{reading2021caddn}\nCody Reading, Ali Harakeh, Julia Chae, and Steven~L. Waslander.\n\\newblock Categorical depth distributionnetwork for monocular 3d object\n  detection.\n\\newblock In {\\em CVPR}, 2021.\n\n\\bibitem{vora2020pointpainting}\nSourabh Vora, Alex~H Lang, Bassam Helou, and Oscar Beijbom.\n\\newblock Pointpainting: Sequential fusion for 3d object detection.\n\\newblock In {\\em CVPR}, 2020.\n\n\\bibitem{wang2021pointaugmenting}\nChunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang.\n\\newblock Pointaugmenting: Cross-modal augmentation for 3d object detection.\n\\newblock In {\\em CVPR}, 2021.\n\n\\bibitem{wang2019pseudo}\nYan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and\n  Kilian Weinberger.\n\\newblock Pseudo-lidar from visual depth estimation: Bridging the gap in 3d\n  object detection for autonomous driving.\n\\newblock In {\\em CVPR}, 2019.\n\n\\bibitem{wang2022detr3d}\nYue Wang, Vitor~Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao,\n  and Justin Solomon.\n\\newblock Detr3d: 3d object detection from multi-view images via 3d-to-2d\n  queries.\n\\newblock In {\\em CoRL}, 2022.\n\n\\bibitem{wang2021objectdgcnn}\nYue Wang and Justin~M. Solomon.\n\\newblock Object dgcnn: 3d object detection using dynamic graphs.\n\\newblock In {\\em NeurIPS}, 2021.\n\n\\bibitem{Xu2021FusionPaintingMF}\nShaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Bin Zhou, and Liangjun Zhang.\n\\newblock {FusionPainting}: Multimodal fusion with adaptive attention for 3d\n  object detection.\n\\newblock {\\em ITSC}, 2021.\n\n\\bibitem{yan2018second}\nYan Yan, Yuxing Mao, and Bo~Li.\n\\newblock Second: Sparsely embedded convolutional detection.\n\\newblock {\\em Sensors}, 2018.\n\n\\bibitem{yin2021center}\nTianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.\n\\newblock Center-based 3d object detection and tracking.\n\\newblock In {\\em CVPR}, 2021.\n\n\\bibitem{Yin2021MVP}\nTianwei Yin, Xingyi Zhou, and Philipp Kr{\\\"a}henb{\\\"u}hl.\n\\newblock Multimodal virtual point 3d detection.\n\\newblock {\\em NeurIPS}, 2021.\n\n\\bibitem{zhou2018voxelnet}\nYin Zhou and Oncel Tuzel.\n\\newblock Voxelnet: End-to-end learning for point cloud based 3d object\n  detection.\n\\newblock In {\\em CVPR}, 2018.\n\n\\bibitem{zhu2019class}\nBenjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu.\n\\newblock Class-balanced grouping and sampling for point cloud 3d object\n  detection.\n\\newblock {\\em arXiv preprint}, 2019.\n\n\\end{thebibliography}\n\n\\clearpage\n"
            },
            "section 6": {
                "name": "Checklist",
                "content": "\n\n\\begin{enumerate}\n\n\n\\item For all authors...\n\\begin{enumerate}\n  \\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n    \\answerYes{See Section~\\ref{sec:experiments}.}\n  \\item Did you describe the limitations of your work?\n    \\answerYes{See supplementary material.}\n  \\item Did you discuss any potential negative societal impacts of your work?\n    \\answerYes{See supplementary material.}\n  \\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\n    \\answerYes{}\n\\end{enumerate}\n\n\n\\item If you are including theoretical results...\n\\begin{enumerate}\n  \\item Did you state the full set of assumptions of all theoretical results?\n    \\answerNA{}\n        \\item Did you include complete proofs of all theoretical results?\n    \\answerNA{}\n\\end{enumerate}\n\n\n\\item If you ran experiments...\n\\begin{enumerate}\n  \\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\n    \\answerNA{}\n  \\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\n    \\answerYes{See Section~\\ref{sec:experiments}.}\n        \\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\n    \\answerNA{}\n        \\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\n    \\answerYes{See Section \\ref{sec:experiments}.}\n\\end{enumerate}\n\n\n\\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n\\begin{enumerate}\n  \\item If your work uses existing assets, did you cite the creators?\n    \\answerYes{}\n  \\item Did you mention the license of the assets?\n    \\answerNA{}\n  \\item Did you include any new assets either in the supplemental material or as a URL?\n    \\answerNo{}\n  \\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\n    \\answerNA{}\n  \\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\n    \\answerNA{}\n\\end{enumerate}\n\n\n\\item If you used crowdsourcing or conducted research with human subjects...\n\\begin{enumerate}\n  \\item Did you include the full text of instructions given to participants and screenshots, if applicable?\n    \\answerNA{}\n  \\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\n    \\answerNA{}\n  \\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\n    \\answerNA{}\n\\end{enumerate}\n\n\n\\end{enumerate}\n\\clearpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n",
                "subsection 7.1": {
                    "name": "Performance breakdown for categories",
                    "content": "\n\\label{appendixa}\n\nIn table~\\ref{tab:nuscene_test_ori}, we report the performance on each category.\n% of our \\methodname and prior state-of-the-art methods.\nIt is shown that our \\methodname performs the best among all the competitors across the most of object categories.\n\n"
                },
                "subsection 7.2": {
                    "name": "Discussions of potential societal impacts",
                    "content": "\nFusing multi-modal information allows to compensate for the shortcomings of single modality in 3D object detection,\nleading to more more accurate and robust performance.\nIn practice, a stronger 3D object detection method as our \\methodname model is expected to reduce the potential accidents of self-driving cars. \nThis improves the safety and reliability of autonomous driving.\nHowever, multi-modal algorithms often require more powerful computing devices and run at a higher cost.\nThis raises a need for improving the system efficiency to be resolved in the future.\n\n"
                },
                "subsection 7.3": {
                    "name": "Limitations",
                    "content": "\nAll the components for multi-modal fusion in our \\methodname have no preference to any per-modal representations. However, the initial queries are derived from LiDAR BEV, albeit fused with image features. We will explore how to generate initial queries from both modalities (\\ie, LiDAR’s bird-eyes-view and camera’s front-view).\n\nOur method involves explicit 2D-3D mapping, hence is conditioned on the calibration quality of the sensors. To relax this condition, a potential method is to exploit the attention mechanism to allow the network to automatically establish alignment between multi-modal features.\n\nFinally, our current model design does not take into account model efficiency.\nIn the future, we will develop a more advanced framework which can adaptively select more cost-effective combinations of interaction operators in order to optimize the trade-off between performance, efficiency and robustness.\n\n"
                },
                "subsection 7.4": {
                    "name": "More visualizations",
                    "content": "\n\\label{appendixb}\n\n"
                }
            }
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    f_{{\\phi}_{c \\rightarrow p}}\n    \\left( \\vh_c, \\vh_p \\right)\n    ^{[i,j]} = \\sum_{\\bm{k},\\bm{v} \\in \\mathcal{N}_q}^{}\\text{softmax}\\left (  \\frac{\\bm{qk}}{\\sqrt{d} }\\right ) \\bm{v},\n\\label{eq:local_attention}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\begin{aligned}\n    \\boldsymbol{h}_p^\\prime & =  \\text{FFN}(\\text{Concat}(\\text{FFN}(\\text{Concat}(\\boldsymbol{h}_p^{p \\rightarrow p},~\\boldsymbol{h}_p^{c \\rightarrow p})),~\\boldsymbol{h}_p)), \n    \\\\\n    \\boldsymbol{h}_c^\\prime & =  \\text{FFN}(\\text{Concat}(\\text{FFN}(\\text{Concat}(\\boldsymbol{h}_c^{c \\rightarrow c},~\\boldsymbol{h}_c^{p \\rightarrow c})),~\\boldsymbol{h}_c)),\n    \\label{eq:ffn_fusion}\n\\end{aligned}\n\\end{equation}"
        },
        "git_link": "https://github.com/open-mmlab/mmdetection3d"
    }
}