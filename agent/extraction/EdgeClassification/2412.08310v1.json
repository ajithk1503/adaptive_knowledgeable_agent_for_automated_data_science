{
    "meta_info": {
        "title": "Edge-Splitting MLP: Node Classification on Homophilic and Heterophilic  Graphs without Message Passing",
        "abstract": "Message Passing Neural Networks (MPNNs) have demonstrated remarkable success\nin node classification on homophilic graphs. It has been shown that they do not\nsolely rely on homophily but on neighborhood distributions of nodes, i.e.,\nconsistency of the neighborhood label distribution within the same class.\nMLP-based models do not use message passing, \\eg Graph-MLP incorporates the\nneighborhood in a separate loss function. These models are faster and more\nrobust to edge noise. Graph-MLP maps adjacent nodes closer in the embedding\nspace but is unaware of the neighborhood pattern of the labels, i.e., relies\nsolely on homophily. Edge Splitting GNN (ES-GNN) is a model specialized for\nheterophilic graphs and splits the edges into task-relevant and\ntask-irrelevant, respectively. To mitigate the limitations of Graph-MLP on\nheterophilic graphs, we propose ES-MLP that combines Graph-MLP with an\nedge-splitting mechanism from ES-GNN. It incorporates the edge splitting into\nthe loss of Graph-MLP to learn two separate adjacency matrices based on\nrelevant and irrelevant feature pairs. Our experiments on seven datasets with\nsix baselines show that ES-MLP is on par with homophilic and heterophilic\nmodels on all datasets without using edges during inference. We show that\nES-MLP is robust to multiple types of edge noise during inference and that its\ninference time is two to five times faster than that of commonly used MPNNs.\nThe source code is available at https://github.com/MatthiasKohn/ES-MLP.",
        "author": "Matthias Kohn, Marcel Hoffmann, Ansgar Scherp",
        "link": "http://arxiv.org/abs/2412.08310v1",
        "category": [
            "cs.LG",
            "stat.ML"
        ],
        "additionl_info": "Published at Learning on Graphs, 2024"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nIn social networks, people tend to be friends with people with similar interests, and in citation graphs, papers tend to cite papers of the same subject~\\cite{BeyondHomophilyInGNN}.\nThe property of such graphs that adjacent nodes are likely to share the same class is denoted as homophily.\nHomophily is the underlying assumption of many Message Passing Neural Networks (MPNNs)~\\cite{HeteroGNNSurvey}.\nHowever, in the real world, many graphs are heterophilic, \\ie adjacent nodes tend to have different labels. \nFor example, fraudsters in online transactions are more likely to build connections with customers instead of other fraudsters~\\cite{HeteroGNNSurvey} or different types of amino acids interact in protein structures~\\cite{BeyondHomophilyInGNN}.\nSeveral works claim that classical MPNNs have been designed under the homophily assumption and are unsuitable for heterophilic graphs~\\cite{BeyondHomophilyInGNN}.\nHence, new Graph Neural Networks (GNNs) have been developed, \\eg Edge Splitting GNN (ES-GNN)~\\cite{Edge-SplittingGNN}, that are more capable of handling heterophilic graphs through higher-order neighborhood aggregation or GNN architecture refinement methods~\\cite{HeteroGNNSurvey, ACM-GNN}.\nRecent work showed that MPNNs can perform well on some heterophilic graphs if the graph shows a uniform neighborhood pattern~\\cite{IsHomophilyANecessity}.\nHowever, these findings do not apply to multilayer perception (MLP) models for graphs as they do not rely on message passing.\nGraph-MLP~\\cite{GraphMLP:abs-2106-04051} is a popular GNN without messaging passing. \nIt is based on an MLP and a neighborhood contrastive loss to combine the edge and feature information in a single embedding during training.\nHence, Graph-MLP does not require edges during inference.\nThe neighborhood contrastive loss of Graph-MLP uses only single edges during training, \\ie considers only a pair of connected vertices at a time.\nThus, Graph-MLP is unaware of the neighborhood patterns that are required to perform well on heterophilic graphs~\\cite{IsHomophilyANecessity}.\n\nWe propose Edge-Splitting MLP (ES-MLP), which combines the strength of Graph-MLP and ES-GNN.\nES-GNN splits the original graph edges into two exclusive sets to mitigate the problem of MPNNs on heterophilic graphs.\nGraph-MLP does not use message passing, unlike conventional MPNNs.\nThis results in higher computational efficiency and robustness to edge noise during inference.\nWith ES-MLP, we propose a model for heterophilic graphs that does not need access to the adjacency matrix during inference time as well but is able to reduce the influence of harmful edges.\nIn contrast to ES-GNN, our ES-MLP is able to add new edges since the computations are based on powers of the adjacency matrix.\nOur model is robust against edge noise, \\ie differences in the neighborhood distribution of the training and test dataset, and has low computation time during inference.\nIn our experiments, we evaluate the performance of ES-MLP against five baseline GNN models on seven real-world datasets ranging from low homophily to high homophily and a synthetic dataset generated with the Contextual Stochastic Block Model (CSBM)~\\cite{GraphAttentionRetrospective(CSBM)}.\nIn summary, our contributions are:\n\\begin{itemize}\n    \\item We propose ES-MLP which combines the benefits of Graph-MLP and ES-GNN to handle both homophilic and heterophilic graphs without message passing.\n\n    \\item We evaluate the effectiveness of ES-MLP on seven benchmark datasets and one synthetic dataset and show that it is on par with all baselines in classification and inference time performance.\n\n    \\item We show that ES-MLP is robust to multiple types of edge noise during inference time.\n\n\\end{itemize}\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec:relatedwork}\n\n\\paragraph{GNNs under Homophily}\n~\\citet{GraphConvNet} proposed the Graph Convolutional Network (GCN) for node classification, which performs message passing by aggregating the neighborhood features to obtain the node embedding.\n\\citet{SymplifyingGCN} simplified it by removing the nonlinearities between GCN layers to collapse the resulting function into a single linear transformation.\n~\\citet{GraphAttentionNetwork} introduced Graph Attention Network (GAT), an attention-based architecture that computes the feature representation of every node, by weighting edges differently based on an attention mechanism.\nGraphSAGE~\\cite{GraphSage} separates the weight of a node from its neighbors, which makes it strong on heterophilic graphs.\n~\\citet{GraphMLP:abs-2106-04051} proposed Graph-MLP as a pure MLP-based framework. \nIt avoids explicit message passing by using the node features as input of an MLP and calculating a separate contrastive loss function based on the powers of the adjacency matrix.\n\n \\paragraph{GNNs under Heterophily}\nMost of the GNN models are designed under the assumption of homophily and perform low on heterophilic graphs~\\cite{BeyondHomophilyInGNN}.~\\citet{PerformanceDiscrepanciesLocalHomphily} and \\citet{PAC-Bayesian} demonstrated that node classification performance degrades when the local homophily of a node deviates from the global homophily.\nExisting GNNs for heterophilic graphs can mainly be categorized into two types~\\cite{HeteroGNNSurvey}, aggregating higher-order neighborhoods and propagating weighted messages.\nMixHop~\\cite{MixHop} aggregates messages from multi-hop neighbors.\nH$_2$GCN~\\cite{BeyondHomophilyInGNN} is designed with a propagation mechanism, which automatically changes the propagation and aggregation process according to homophily or heterophily between node pairs.\nThe second type is passing weighted messages between heterophilic neighbors~\\cite{GGCN, GPR-GNN, ACM-GNN, NCC_Metric_TA-GCN}.\nGGCN uses cosine similarity to send signed neighbor features under certain constraints of relative node degrees.\nGPR-GNN learns weights that can be positive or negative for feature propagation.\nTA-GCN~\\cite{NCC_Metric_TA-GCN} estimates the neighborhood patterns of the graph data and learns guided by this metric an augmented graph topology.\n\\citet{Label-WiseGraphConvolution} proposed Label-Wise GCN, which summarizes the neighborhood information of every node by label-wise aggregation to capture the useful heterophilic context.\n\\add{\\citet{GBK-GNN} proposed GBK-GNN, which employs a bi-kernel feature transformation and a selection gate mechanism, where one kernel captures homophilic node pairs and the other heterophilic node pairs.}\n~\\citet{EdgeDirectionalyForHeteroGraphs} introduced Directed GNN (Dir-GNN), which treats the graph as directed and performs separate aggregations of the incoming and outgoing edges. \nEdge Splitting GNN~\\cite{Edge-SplittingGNN} introduces an edge-splitting layer that disentangles edges into two exclusive sets, allowing the GNN to focus on the most relevant edges for the classification task. \nThese two channels are aggregated separately and the prediction is made only on the task-relevant feature channel.\nLINKX~\\cite{LINKX} is an MLP-based model that processes the adjacency matrix and node features separately and then concatenates them as a simple baseline for heterophilic graphs.\nIn contrast to ES-MLP, LINKX requires edges during inference time.\n\n\\paragraph{Homophily Measures}\nThere are several measures of homophily for a graph.\n~\\citet{BeyondHomophilyInGNN} proposed edge homophily ratio $h_{edge}$, which measures the proportion of edges in a graph that connects nodes within the same class.\nAnother measure is the node homophily $h_{node}$ \\cite{NodeHomoRatio}.\nFor each node, it measures the ratio of adjacent nodes with the same labels and all adjacent nodes. $h_{node}$ is the average over all nodes.\nBased on the edge homophily measure,~\\citet{ClassInsEdgeHomoRatio} proposed a class insensitive edge homophily ratio $h_{class}$.\nThis measure is normalized by the number and size of the classes, which makes graphs with different numbers and sizes of classes comparable. \nIt takes the edge homophily measure~\\cite{BeyondHomophilyInGNN} and subtracts the general proportion of nodes in the same class to all nodes to mitigate the class-imbalance problem.\nAdjusted homophily $h_{adj}$~\\cite{AdjustedHomophily} is the edge homophily adjusted for the expected number of edges connecting nodes with the same class, considering the number of classes and the distribution of node degrees among them.\nWe use the adjusted homophily as reference measure since it is comparable across different datasets with varying numbers of classes and class size balance.\nThe formulas of the homophily measures are provided in Appendix \\ref{appendix: homophily measures}.\n\n\n\n"
            },
            "section 3": {
                "name": "Edge-Splitting MLP",
                "content": "\n\\label{sec:methods}\nIn this section, we propose our new model Edge-Splitting MLP (ES-MLP), which combines elements of Graph-MLP~\\cite{GraphMLP:abs-2106-04051} and ES-GNN~\\cite{Edge-SplittingGNN}. \nWe assume a transductive setting for node classification, \\ie test nodes are already present during training.\n\nThe key feature of ES-GNN is an additional edge-splitting (ES) layer to partition the network topology to disentangle the task-relevant and irrelevant node features. \nWe denote task-relevant feature pairs that benefit from a connection through an edge.\nIn contrast to task-irrelevant features that are harmful to the task if connected by an edge.\nThe ES layer splits node features to distinguish the task-relevant and irrelevant connections among nodes.\nThe features are then aggregated separately along these connections to produce disentangled representations.\nES-GNN incorporates an Irrelevant Consistency Regularization (ICR) loss, which minimizes the distance of heterophilic neighbors.\n\nGraph-MLP is an MLP-based model without message passing. Instead, it uses a neighborhood contrastive loss to bridge the gap between GNN and MLP by utilizing the adjacency information during training. \nFor each node, the $r$-hop neighbors are regarded as positive samples and the other nodes as negative samples. \n\n\n\\paragraph{Edge-Splitting MLP}\n ES-MLP generalizes MLP to both, homophilic and heterophilic graph data.\n To this end, we integrate the neighborhood contrastive loss from Graph-MLP into ES-GNN to aggregate features on task-relevant edges without using the adjacency matrix during inference time. An overview of ES-MLP is given in Figure~\\ref{fig:method_pipeline}. \n\n\n\nLet $G = \\{V, E\\}$ denote a graph, where $V$ and $E$ are the sets of nodes and edges, respectively. \nWe project the feature matrix $X \\in \\mathbb{R} ^ {|V| \\times d}$ into two different subspaces $Z_s$ with $s \\in \\{R, IR\\}$ to obtain two separate representations, where $|V|$ is the number of nodes and $d$ the feature dimension. \n$Z_R$ represents task-relevant $(R)$ and $Z_{IR}$ task-irrelevant $(IR)$ embeddings.\nThe projection is defined by $Z_s^{(0)} = \\sigma(XW_s+ b_s)$, where $W_s$ and $b_s$ are learnable parameters for each channel $s \\in \\{R,IR\\}$. \n$R$ is the task-relevant channel and $IR$ is the task-irrelevant channel, and $\\sigma$ is a nonlinear activation function.\n\n\nThe edge-splitting mechanism separates the graph edges into two exclusive sets of edges, which are represented by $A_R, A_{IR} \\in \\mathbb{R}^{|V| \\times |V|}$, with splitting coefficients $A_{R(i,j)} \\in [0,1]$ and $ A_{IR(i,j)} \\in [0,1] $.\nThese matrices are then used for the neighborhood contrastive loss.\nWe process each channel $R$ and $IR$ seperately without using any message passing by $Z_s^{(k+1)} = \\epsilon_s Z_s^{(0)} + (1 - \\epsilon_s)Z_s^{(k)} W_s$,\nwith skip connections and weighting hyperparameter $\\epsilon_s$.\nWe use a linear layer for the final classification.\n\nThe splitting coefficients $A_{R(i,j)} $ and $ A_{IR(i,j)}$ are exclusive, \\ie $A_{R(i,j)} + A_{IR(i,j)} = A_{(i,j)} = 1$. Therefore, we parameterize the residual between $A_{R(i,j)}$ and $A_{IR(i,j)}$, and linear equations: \n\n\\begin{displaymath}\n    A_{R(i,j)} - A_{IR(i,j)} = \\alpha_{(i,j)}, \\quad\n    A_{R(i,j)} + A_{IR(i,j)} = 1\n\\end{displaymath}\n\nThis leads to two exclusive sets of edges, which are used for the neighborhood contrastive loss: \n\n\\begin{displaymath}\n    A_{R(i,j)} = \\frac{1 + \\alpha_{(i,j)}}{2}, \\hspace{0.2cm} A_{IR(i,j)} = \\frac{1 - \\alpha_{(i,j)}}{2},\n\\end{displaymath}\nwith $\\alpha_{(i,j)} \\in (-1,1)$.\nThe weights $\\alpha_{(i,j)}$ influence both channels\nand are computed by:\n\n\\begin{displaymath}\n    \\alpha_{(i,j)} = tanh(FF[Z_{R[i,:]}^{(K)} \\oplus Z_{IR[i,:]}^{(K)} \\oplus Z_{R[j,:]}^{(K)} \\oplus Z_{IR[j,:]}^{(K)}]^T),\n\\end{displaymath}\n\nwhere we concatenate the embeddings of both representations, $R$ and $IR$ of two connected nodes $v_i$ and $v_j$. \nThese are then passed into a linear layer $FF$, followed by a $tanh$ activation function, to obtain values within $(-1,1)$.\nIt ensures the exclusiveness of our splitting coefficients.\n\n \\paragraph{Neighborhood Contrastive Loss}\n\nTo incorporate the edge information, we use the Neighborhood Contrastive Loss from~\\citet{GraphMLP:abs-2106-04051}.\nThis loss enables our model to learn graph structure without explicit message passing.\nSince this loss is designed under the assumption that connected nodes are similar to each other, we calculate it on the task-relevant $R$ and task-irrelevant $IR$ embedding spaces separately to generalize the loss for heterophilic graphs.\nThe neighborhood contrastive loss for node $v_i$ can be formulated as:\n\\begin{displaymath}\n    l_{i_s} = - log \\frac{\\sum_{j=1}^{|V|} 1_{[j \\neq i]} \\hat{\\gamma}_{s(i,j)} exp(sim(Z_{s[i,:]}, Z_{s[j,:]})/\\tau)} {\\sum_{k=1}^{|V|} 1_{[k \\neq i]} exp(sim(Z_{s[i,:]}, Z_{s[k,:]} )/\\tau)} ,\n\\end{displaymath}\n\nwhere $sim$ denotes the cosine similarity, $\\tau$ a temperature parameter, $s \\in \\{R,IR\\}$ and $\\gamma_{s(i,j)} $ is calculated based on the $r^{th}$ power of the splitting adjacency matrices: $\\gamma_{s(i,j)} = A_{s(i,j)}^r$ with $s \\in \\{R,IR\\}$ and $\\gamma_{s(i,j)}$ if node $v_i$ is in the $r$-hop neighborhood of node $v_j$.\n\nInstead of simply minimizing the distance between nodes in the $r$-hop neighborhood, the loss minimizes the distance of node embeddings for edges that are relevant for the task according to feature embeddings.\nSimultaneously, the task-irrelevant edges are learned based on the task-irrelevant part of the feature vectors.\nSince taking the $r^{th}$ power of the splitting adjacency can lead to small splitting coefficients, we renormalize the $ \\gamma_{s(i,j)}$ to ensure that $\\gamma_{R(i,j)}  + \\gamma_{IR(i,j)} = 1$.\nThe values of $\\gamma_{s(i,j)} $ are non-zero if and only if node $j$ is in the $r$-hop neighborhood of node $v_i$~\\cite{GraphMLP:abs-2106-04051}:\n$$\n\\hat{\\gamma}_{s(i,j)}=\\begin{cases}\n     \\gamma_{s(i,j)}/ (\\gamma_{R(i,j)} + \\gamma_{IR(i,j)}), & \\text{$j$ is in the $r$-hop neighborhood of $v_i$}\\\\\n     0, & \\text{ $j$ is not in the $r$-hop neighborhood of $v_i$}\n\\end{cases}\n$$\n\nThe final neighborhood contrastive loss is computed by averaging over all nodes,    ${\\mathcal{L}_{NC} = \\frac{1}{|V|} \\sum_{i=1}^{|V|} (l_{i_R} + l_{i_{IR}})}$.\n\nGiven that $Z_{IR}$ is designed to represent the least relevant information for the node classification task, an Irrelevant Consistency Regularization (ICR) is added to improve the consistency of irrelevant information in $Z_{IR}$.\nThe ICR loss is formulated as: \n \\begin{displaymath}\n     \\mathcal{L}_{ICR} = \\sum_{(i, j) \\in E} (1- \\hat{y_i}^T \\hat{y_j}) \\lVert Z_{IR_i} - Z_{IR_j}\\rVert_2,\n \\end{displaymath}\n\n where $\\hat{y_i}^, \\hat{y_j}$ are the predicted probability vector of nodes $i$ and $ j$, and $Z_{IR_i}$, $ Z_{IR_j}$ denoting the irrelevant embedding of nodes $i$ and $j$.\n The ICR loss minimizes the distance of two adjacent nodes if they do not share the same label.\n\n Finally, the total loss is the sum of the ICR $\\mathcal{L}_{ICR}$, neighborhood contrastive loss $\\mathcal{L}_{NC}$, and the cross entropy loss $\\mathcal{L}_{CE}$ :\n\n\\begin{displaymath}\n \\mathcal{L}_{final} = \\mathcal{L}_{CE} + \\alpha_{NC} \\cdot \\mathcal{L}_{NC} + \\beta_{ICR} \\cdot \\mathcal{L}_{ICR}, \n\\end{displaymath}\n \nwhere $\\alpha$ is the neighborhood contrastive loss weight and $\\beta$ the ICR loss weight.\n\n\n\\paragraph{Complexity Analysis}\nThe computational complexity for one training step of ES-MLP is $\\mathcal{O}(dh^{l-2}|C|)$ for a forward pass through the network and another $\\mathcal{O}(r|V|^3))$ to compute the $r$-th power of the adjacency matrix for the loss function, where $h$ is the hidden dimension, $|C|$ the number of classes, the $d$ feature dimension, and $l$ the number of layers.\nThis is more than for regular MPNNs, \\eg GCN, which has a computational complexity of $\\mathcal{O}(|E|dh^{l-2}|C|)$~\\cite{GraphConvNet}.\nHowever, ES-MLP has a computational complexity of only $\\mathcal{O}(dh^{l-2}|C)|$ during inference, \\ie has lower computational complexity than MPNNs.\n\n\nIn this section, we proposed ES-MLP, which splits the original graph edges into two exclusive sets of edges to aggregate features on task-relevant and irrelevant edges. \nThis edge-splitting mechanism is incorporated in a neighborhood contrastive loss to bypass the use of message passing.\nModel predictions are performed on task-relevant embeddings and an irrelevant consistency loss is used to suppress information in the task-relevant embedding.\n\n"
            },
            "section 4": {
                "name": "Experimental Apparatus",
                "content": "\n\\label{sec:experimentalapparatus}\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\n\\label{sec:datasets}\n\\paragraph{Real World Datasets}\nFor our experiments, we use seven datasets, three with a homophily score over $0.5$ and four heterophilic datasets with homophily under $0.5$.\nWe measure homophily by the adjusted homophily ratio~\\cite{AdjustedHomophily} since it can compare datasets with different numbers of classes.\nTable~\\ref{tab:DatasetsStatistics} shows the properties of each dataset.\n\n\n\nThe homophilic datasets are the three standard citation graphs Cora, CiteSeer, and PubMed~\\cite{CitationNetworkDatasets}. \nIn these datasets, each node represents a document and edges correspond to citations between them. \nFor the heterophilic datasets, we use Actor~\\cite{NodeHomoRatio}, Amazon-ratings (Amazon), Roman-empire (Roman), and Minesweeper \\cite{NewHeteroData}. \nActor represents actors where the edges describe co-occurrences in the same Wikipedia article.\nIn the Roman dataset, each node corresponds to one (non-unique) word in the Roman Wikipedia article.\nThe Amazon dataset is a co-purchase graph.\nMinesweeper is based on the popular Minesweeper game and consists of a $100 \\times 100$ grid.\nEach node represents a cell.\nThe task is to classify each cell as a bomb or not a bomb.\nNode features are one-hot-encoded numbers of adjacent bombs.\nNote that in this dataset the features of a node are independent from its class.\nA detailed explanation of the datasets can be found in Appendix~\\ref{ExtendetDSDescription}.\n\n\n\\paragraph{Synthetic Dataset}\nWe use the Contextual Stochastic Block model (CSBM)~\\cite{GraphAttentionRetrospective(CSBM)} to generate random graphs with controlled levels of homophily. \nA CSBM is defined as $X, A \\sim CSBM(|V|, p, q, \\mu, \\sigma^2)$, where the edges are modeled by two random Bernoulli distributions $p$ and $q$.\nIf two nodes $v_i$, $v_j$ belong to the same class, the probability that an edge $e_{i,j}$ between these nodes is present is distributed with $e_{i,j}$ $\\sim Ber(p), p \\in [0,1]$. \nIf the nodes $v_i$, $v_j$ belong to different classes, the probability that an edge is present is distributed with $e_{i,j}$ $\\sim Ber(q), q \\in [0,1]$. \nFor each node $v_i$, we assign the $d$-dimensional feature vector $x_i \\sim \\mathcal{N}((2y_i - 1)\\mu, \\sigma^2\\mathbf{I})$, where $y_i \\in \\{0,1\\}$ is the class of node $v_i$, $\\mu \\in \\mathbb{R}^{d}$, $\\sigma \\in \\mathbb{R}$, and $\\mathbf{I} \\in \\{0,1\\}^{d \\times d}$ is the identity matrix.\nThis model allows us to control the ratios of the inter and intra-class edges, \\ie the homophily of the graph.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Procedure",
                    "content": "\n\\label{sec:procedure}\n\nWe compare our ES-MLP to six baseline models, MLP, GCN, Graph-MLP, ES-GNN, GraphSAGE, and \\add{LINKX}. \nWe train and evaluate the models on seven real-world and one synthetic datasets.\nIn addition, we perform a robustness analysis, where we add edge noise to the test graph.\nFor Minesweeper, we use AUROC since it is a binary classification task~\\cite{NewHeteroData}.\n\n\\paragraph{Real-world Benchmarks}\nFor the homophilic graphs Cora, CiteSeer, and PubMed, we create $10$ random splits with $20$ nodes per class following~\\citet{Pitfalls}.\nFor the heterophilic dataset Actor, we use the split from~\\citet{NodeHomoRatio}.\nIt consists of 10 random splits with $48\\%$ train nodes, $32\\%$ validation nodes, and $20\\%$ test nodes.\nFor Amazon, Roman, and Minesweeper, we adopt the setting from~\\citet{NewHeteroData}.\nWe repeat each experiment $10$ times and compute the test accuracy on the node classification task.\n\n\\paragraph{Synthetic Benchmarks}\nFor the synthetic data, we generate a graph using a CSBM for multiple combinations of the parameters $p$ and $q$ from $0$ to $1$ with steps of $0.2$.\nWe fix the number of nodes to $5,000$  and use two balanced classes.\nIn total, we have 36 experiments, each is run 5 times. \nFor the CSBM's number of features per node $d$, mean $\\mu$, and standard deviation $\\sigma$, we use a similar setting as \\citet{GraphAttentionRetrospective(CSBM)}, \\ie we set $d = n / \\log^2(n)$ features per node, $\\sigma = 0.20$ and $\\mu = 10\\sigma \\sqrt{\\log n^2}/2\\sqrt{2d}$. \nWe split the synthetic graphs into a $60\\%$ train, $20\\%$ validation, and $20\\%$ test nodes.\nThe overlap in node feature distribution between the classes leads to an inevitable error probability of $0.25$. \nThe actual error may be lower due to the additional information from the edges.\n\n\\paragraph{Robustness Analysis}\n\nWe investigate the robustness towards edge noise by deviating homophily levels during inference time.\nWe introduce two types of edge noise, which we increase gradually.\nWe adopt the edge noise addition algorithm provided by~\\citet{IsHomophilyANecessity}.\nThe algorithm adds a fixed number of edges to the graph according to a chosen distribution.\nIt uniformly samples a node $v_i$ with label $y_i$.\nThen a label $\\Tilde{c}$ is sampled from a neighborhood class distribution $D_{y_i}$.\nFrom the set of nodes $V_{\\Tilde{c}}$ with class $\\Tilde{c}$, we uniformly sample a node $v_j$.\nFinally, a new edge $(v_i,v_j)$ is added to the graph.\nWe set the distributions $D_{\\Tilde{c}}$ to be uniform to simulate uniform edge noise.\nWe also experiment with setting $D_{\\Tilde{c}}$ to a categorical distribution to simulate a shift in neighborhood classes.\nFor the categorical noise, we adopt a circulate matrix-like design to make any two classes from $C$ have different distributions, see Appendix~\\ref{appendix: distribution details} for the detailed distribution per class.\nWe perform this experiment for Cora and Amazon.\n\n\\paragraph{Inference Time}\nWe measure the inference time of the models in two settings, with the full graph as model input and only the test nodes as model input.\nWe aim to demonstrate that MLP-based models can be provided with only the test graph as input and do not decrease in performance.\nThis results in a performance gain in terms of inference time, an advantage, that is not possible with MPNNs, \\eg GCN, which is expected to lose performance, when only test nodes are given as model input.\n\n\n\n\n\\paragraph{Hyperparameter Optimization}\n\\label{sec:hyperparameteroptimization}\n\nWe optimized the learning rate, hidden dimension, weight decay, and dropout for GCN, GraphSAGE, LINKX, and MLP. \nFor GraphMLP, ES-GNN, and ES-MLP, we additionally tuned the loss weights $\\alpha$ for neighborhood contrastive loss and $\\beta$ for the irrelevant consistency loss and the used power of the adjacency matrix $r$.\nDetails on the search space and procedure, as well as the final hyperparameters, can be found in Appendix~\\ref{hyperparameter_optim}.\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Results",
                "content": "\n\\label{sec:results}\n\\paragraph{Real-world Datasets}\nTable~\\ref{tab:results_real_world} shows the results for the real-world datasets. \nES-MLP consistently shows competitive performance across all three homophilic datasets, where it keeps up with the best MPNN-based models.\nOn Cora and PubMed, ES-MLP outperforms the homophilic MLP-based model Graph-MLP.\nOn the heterophilic datasets Actor, Roman, and Amazon, ES-MLP outperforms all baselines.\nGraphSAGE is the best model on Minesweeper.\n\n\n\n\n\n\n\\paragraph{Synthetic Datasets}\n\nOur results for the CSBM datasets are provided in Figure \\ref{fig:csbm heatmap}.\nThe scores are averaged over $5$ runs.\nWe provide the adjusted homophily in Figure~\\ref{fig:adjusted_homophily}.\nES-MLP performed best on the dataset with $p=0.6$ and $q=0.2$ with $78\\%$.\nThe lowest score is $60\\%$ for $p=0.0$ and $q=0.0$ \\ie with no edges. \nFor $p=1.0$ and $q=1.0$ we get a similar score of $0.62$. \n\n\n\n\n\\paragraph{Inference Time}\nThe results for the inference times are presented in Table~\\ref{tab:InferenceTimes}.\nWe measure the times in two settings.\nEither the model is evaluated on the full graph (left) or the model is evaluated on the test nodes only (right).\nThis is an important difference since the MLP-based models only require the test nodes and do not need the edge information, unlike the MPNN models.\nThe scores are given in milliseconds and averaged over $10$ runs.\nAll MLP-based models achieve lower inference times than the MPNNs. \nES-MLP is two to five times faster in inference than the fastest MPNN.\nFrom MPNNs, GraphSAGE was at least two times faster than ES-GNN, which is the slowest.\nMPNNs lose up to $10\\%$ classification accuracy when given test nodes only, due to the missing edges to non-test nodes.\nMLP-based models are unaffected by this problem since they do not need the edges during inference.\nThe detailed test accuracies on test nodes only are in Appendix~\\ref{appendix: extendet accuracies}.\n\n\n\n\n\n    \n\n\n\n\n\\paragraph{Robustness Analysis}\nResults for the robustness analysis are shown in Figure~\\ref{fig:Robustness Analysis}.\nFor the robustness analysis, we added edge noise during test time to observe how models perform when neighborhood distribution changes on the test graph.\nOur results show that MPNNs lose performance if neighborhood distribution on the test graph differs from the training graph.\nIn the uniform edge-noise setting, GraphSAGE's performance decreases up to $8\\%$, GCN up to $12.7\\%$, and ES-GNN up to $10.3\\%$ on Cora.\nFor the heterophilic graph Amazon, all MPNNs lose between $2$ and $3\\%$, while the MLP-based models stay stable.\nWe observe for both datasets that the decrease is larger when adding noise with a categorical distribution. \nThe MLP-based models maintain performance for both edge-noise distributions, independent of the magnitude of the edge noise added to the test graph.\n\n\n"
            },
            "section 6": {
                "name": "Discussion",
                "content": "\n\\label{sec:discussion}\n\nThe results of our experiments show that ES-MLP is effective for both homophilic and heterophilic graphs.\nExtending Graph-MLP by a second channel to split the edges based on relevant versus irrelevant features enables the model to improve its performance on heterophilic graphs.\nNotably, this improvement comes without losing performance on homophilic graphs.\n\\add{An analysis of the learned adjacency matrices shows that the edge-splitting separates homophilic from heterophilic edges, see Appendix~\\ref{appendix:adj_vis} for details.}\n\n\n\\paragraph{Real-world Datasets}\nOn homophilic datasets, ES-MLP achieves competitive results to all baselines. \nThe difference to the best-performing models GCN and Graph-MLP is between $0.31$ and $2.07$~points.\nWe observe that ES-MLP effectively captures the relationship between node features and homophilic edges, maintaining performance on par with Graph-MLP. %\nOn heterophilic graphs, ES-MLP outperforms all baselines, except for Minesweeper.\nES-MLP shows strong improvements over Graph-MLP, for instance by up to $11.18$ points on Amazon, \\ie edge-splitting effectively improves the performance for heterophilic models.\nGraph-MLP cannot compete on the heterophilic dataset, since the original neighborhood contrastive loss is based on the homophily assumption, which is violated here.\nThis highlights the capacity of ES-MLP to generalize on heterophilic graphs.\nAll MLP-based models achieve on Minesweeper an AUROC score of about $50$ points.\nThis is due to the fact that the node features are independent of the class of a node.\nThe class of a node is solely determined by the features of the neighbors. \nSince MLP-based models do not have access to these features during inference time, they are unable to classify these nodes correctly.\n\\add{\nRecent studies demonstrated that node classification performance degrades when\nthe local homophily of a node deviates from the global homophily~\\cite{PerformanceDiscrepanciesLocalHomphily}.\nOur findings show similar behavior for ES-MLP, see Appendix~\\ref{appendix:local_vs_global_homophily} for details.}\n\n\\paragraph{Synthetic Graphs}\nThe experiments on the CSBM datasets show that the worst results are obtained for a graph with no edges ($p=0$, $q=0$).\nThe second worst results are obtained using a fully connected graph ($p=1$, $q=1$) since no information can be learned from the edges.\nThe results on the diagonal are all below $0.7$, since here the ratio of heterophilic and homophilic edges is equal, \\ie there is only a low amount of information in the graph structure.\nCompared to homophily ratios in Figure~\\ref{fig:adjusted_homophily}, we see no strong relationship between the test accuracies and the homophily levels of the graphs.\nNevertheless, ES-MLP achieves stronger results when $p$ and $q$ tend to be more asymmetric.\n\n\\paragraph{Robustness Analysis}\n\n\nOur results show that edge noise during test time leads to a performance decrease of MPNNs, while MLP-based models are unaffected.\nThe reason is that MLP-based models do not use edges during inference.\nAdding uniform edge-noise to Cora leads to a performance decrease of up to $12.7\\%$ and up to $20\\%$  with categorical edge-noise.\nCategorical noise is more challenging since the neighborhood distribution is shifted towards two specific classes, while for the uniform noise, the added neighbors are more likely to balance each other out. \nThe effect is less for the Amazon dataset, as it is already quite heterophilic, \\ie the relative change due to the noise is smaller than for the homophilic Cora dataset.\n\n\n\n\n\n\n\\paragraph{Ablation Study}\nWe perform an ablation study on the neighborhood contrastive loss $\\mathcal{L_{NC}}$ and the irrelevant consistency regularization loss $\\mathcal{L_{ICR}}$ to analyze the effect on the performance of ES-MLP.\nThe results of the ablation study are provided in Table \\ref{table:ablation study} by mean test accuracy.\nOn every dataset, the scores decrease when dropping the neighborhood contrastive loss.\nThe difference is highest for Roman with $4.52\\%$.\nThis implies that the loss is important for training ES-MLP.\nThe homophilic datasets CiteSeer and PubMed achieved their best scores with $\\beta_{ICR}$ of $0$, \\ie the accuracies remain the same without the ICR loss.\nTherefore, the ICR loss does not have a beneficial impact on the homophilic datasets, which was also the result of our hyperparameter search. \nOn heterophilic datasets, the performance decreases when the ICR loss is dropped. \nES-MLP loses $1.89$, $3.92$, and $3.73$ points on Amazon, Roman, and Actor, \\ie the ICR loss is important for heterophilic datasets.\nA sensitivity analysis for both loss parameters $\\alpha_{NC}$ and $\\beta_{ICR}$ can be found in Appendix~\\ref{appendix:hyper_sensitivity}.\n\n\n\n\n\\paragraph{Limitations}\n\\label{sec:threattovalidity}\nAlthough ES-MLP can learn a meaningful relationship between neighbors and features, it cannot be applied to tasks where the class depends on the neighborhood structure during inference time, as shown by the Minesweeper dataset.\nIn contrast to MPNNs, MLP-based methods like ES-MLP are unaware of neighborhood distributions~\\cite{IsHomophilyANecessity}.\n\n"
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\nWe proposed ES-MLP, a GNN for homophilic and heterophilic graphs. Our model combines the edge-splitting mechanism of ES-GNN with the neighborhood contrastive loss of Graph-MLP.\nWe found extending the neighborhood contrastive loss improves performance on heterophilic graphs, while not losing performance on homophilic graphs.\nFor future work, ES-MLP may be extended to model graph directionality like~\\cite{EdgeDirectionalyForHeteroGraphs}, to improve its performance on heterophilic datasets.\n\n\\textit{Acknowledgement.} The paper is the result of the first author's BSc thesis and Master's project.\nThe authors acknowledge support from the state of Baden-W\u00fcrttemberg through bwHPC.\n\n\\newpage\n\n\n\\bibliographystyle{unsrtnat}\n\\bibliography{reference}\n\n\\newpage\n\n\\appendix\n"
            },
            "section 8": {
                "name": "Appendix",
                "content": "\n\\label{appendix:supplementarymaterials}\n\n\n"
            },
            "section 9": {
                "name": "Hyperparameter Optimization",
                "content": "\n\\label{hyperparameter_optim}\nThe hyperparameter optimization consists of two steps, we first optimize the parameters for MLP, GCN, and GraphSAGE.\nFor the MLP-based models, Graph-MLP, and ES-MLP, we reuse the hyperparameters of MLP and tune the model-specific parameters.\nFor MPNN based model ES-GNN, we use the parameters of GCN and only tune the model-specific parameters that GCN does not have.\n\n",
                "subsection 9.1": {
                    "name": "MLP, GCN, and GraphSAGE",
                    "content": "\nWe adopt the hyperparameter tuning from~\\citet{IsHomophilyANecessity} and tune the learning rate, weight decay, dropout rate, and hidden dimension.\nWe conducted hyperparameter tuning for the baseline models GCN, MLP, GraphSAGE, and \\add{LINKX}. \nThe hyperparameter search space included the following ranges.\nThe learning rate was varied over $\\{0.002, 0.005, 0.01, 0.05\\}$, weight decay values were selected from $\\{5e-04, 5e-05, 5e-06, 5e-07, 5e-08, 1e-05, 0\\}$, dropout rates from $\\{0, 0.2, 0.5, 0.8\\}$, and hidden dimensions from $\\{64, 128, 256\\}$. \nThe best hyperparameters are reported in Table~\\ref{tab:HPMLP},~\\ref{tab:HPGCN} and~\\ref{tab:HPGS} for MLP, GCN, and GraphSAGE respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 9.2": {
                    "name": "ES-GNN, Graph-MLP, and ES-MLP",
                    "content": "\nWe tune the model-specific hyperparameters.\nFor the model-specific parameters, we optimize the weighting coefficient $\\alpha_{NC}$, irrelevant consistency coefficient $\\beta_{ICR}$, scaling parameter $\\epsilon_R$ and $\\epsilon_{IR}$, and powers of the adjacency matrix $r$.\nWe determine the hyperparameter the same way as it is done by \\citet{GraphMLP:abs-2106-04051} and \\citet{Edge-SplittingGNN} by using Grid-Search.\nWe determine $\\alpha_{NC}$  in a range of $\\{0, 1, 10, 100\\}$, $\\beta_{ICR}$ in $\\{1e-5, 1e-4, 1e-3, 1e-1\\}$, $\\epsilon_R$ and $\\epsilon_{IR}$, in $\\{0, 0.1, 0.3, 0.5, 0.7\\}$, and the powers of the adjacency matrix $r$ in $\\{1, 2, 3\\}$.\n\nFor each dataset, we tuned the hyperparameters as described in Section~\\ref{sec:hyperparameteroptimization}.\nThe Tables~\\ref{appendix:hyperparameteroptimization_esmlp},~\\ref{appendix:hyperparameteroptimization_graphmlp} and~\\ref{appendix:hyperparameteroptimization_esgnn}below provide the hyperparameter settings for ES-MLP, Graph-MLP, and ES-GNN respectively, we used for each dataset.\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 10": {
                "name": "Homophily",
                "content": "\nWe considered four homophily measures. One based on edge level, one on node level, a class insensitive homophily, and an adjusted homophily ratio.\nThe first three measures indicate the homophily level in a range from 0 to 1, where 1 denotes a completely homophilic graph.\n\n\nEdge Homophily Ratio, which is defined as follows:\n\\cite{BeyondHomophilyInGNN}\n \\begin{equation}\n    h = \\frac{|{(u,v): (u,v) \\in{E} \\wedge y_u = y_v}|}{|E|}\n\\end{equation}\n\nThis ratio indicates the proportion of edges in a graph that connects nodes with the same class (intra-edges). \n\nNode Homophily Ratio \\cite{NodeHomoRatio} is defined as:\n\n\\begin{equation}\n    \\beta = \\frac{1}{|V|} \\sum_{v \\in V} \\frac{|{(w,v) : w \\in N(v) \\wedge y_v = y_w}|}{|N(v)|}\n\\end{equation}\n\nIt measures the ratio of intra-edges of the respective nodes in its neighborhoods, normalized over the whole number of nodes.\n\nClass insensitive edge homophily Ratio\\cite{ClassInsEdgeHomoRatio}\n\n\\begin{equation}\n    \\hat{h} = \\frac{1}{C-1} \\sum_{k=0}^{C-1} \\max \\left(0, h_k - \\frac{|C_k|}{|V|}\\right).\n\\end{equation}\n\nWhere $|C_k|$ denotes the number of nodes with class $k$, and $h_k$ is the class-wise homophily metric, given with \n\\begin{displaymath}\n    h_k = \\frac{\\sum_{u \\in C_k d_u^{k_u}}}{\\sum_{u \\in C_k d_u}},\n\\end{displaymath}\n\nwhere $d_u$ is the number of neighbors of node $u$ and $d_u^{k_u}$ the number of neighbors of $u$ that have the same class label. \nIn this measure edge homophily is modified to be insensitive to the number of classes and size of each class. $\\hat{h}$ measures presence of homophily\n\nThe adjusted homophily ratio~\\cite{AdjustedHomophily} is formulated as:\n\n\\begin{displaymath}\n    h_{adj} = \\frac{h_{edge} - \\sum_{k=1}^C \\bar{p}(k)^2}{ 1 - \\sum_{k=1}^C \\bar{p}(k)^2},\n\\end{displaymath}\n\nwhere $\\bar{p}(k) = \\frac{D_k}{2|E|}$ is the degree-weighted distribution of class labels.\n\n\\label{appendix: homophily measures}\n\n"
            },
            "section 11": {
                "name": "Accuracies of Inference Time Experiment",
                "content": "\n\\label{appendix: extendet accuracies}\n\nWe report the accuracies of our inference time experiment. \nWe measured the accuracy for two settings.\nEither the model is evaluated on the full graph or the model is evaluated on the test nodes only.\nTherefore, the accuracies of the setting evaluated on the full graph correspond to the accuracies reported in Table~\\ref{tab:results_real_world}.\nThe respective inference times can be found in Table~\\ref{tab:InferenceTimes}. \nWe also added Chameleon and Squirrel datasets~\\cite{NodeHomoRatio} as additional experiments. \nThese datasets are not included in the main paper, since they contain duplicate nodes as shown by \\citet{NewHeteroData}.\nThe accuracies of the inference time experiment are reporter in Table~\\ref{tab:Extendet Results}.\n\nWe observe, that by just providing the model with the test graph only, MPNN performance decreases since adjacent vertices from training are not present anymore.\nMLP-based models are unaffected since they do not use edges during inference.\nTherefore, MLP-based models can only be provided with the test graph to improve needed inference time while maintaining accuracy.\n\\add{This does not apply to LINKX. \nAlthough LINKX is an MLP-based model, it still requires edges during inference. \nFor this reason, its performance degrades when applied only to the test graph.}\n\n\n\n\n"
            },
            "section 12": {
                "name": "Hardware",
                "content": "\n\\label{apppendix: hardware}\nAll experiments have been performed on a NVIDIA A100 GPU.\n\\label{ExtendetDSDescription}\n\n"
            },
            "section 13": {
                "name": "sec:procedure",
                "content": "\n\\label{appendix: distribution details}\nWe describe the details of how we added the edge-noise for the robustness analysis.\nSpecifically, we detail the distributions $D_{\\bar{c}}$.\n\n\\paragraph{Cora}\nCora has seven classes, which we denote as $\\{0, 1, 2, 3, 4, 5, 6\\}$. \n\n\\[\n\\mathcal{D}_0 : \\text{Categorical}([0, 0.5, 0.5, 0, 0, 0, 0, 0]),\n\\]\n\\[\n\\mathcal{D}_1 : \\text{Categorical}([0, 0, 0.5, 0.5, 0, 0, 0, 0]),\n\\]\n\\[\n\\mathcal{D}_2 : \\text{Categorical}([0, 0, 0, 0, 0.5, 0.5, 0, 0]),\n\\]\n\\[\n\\mathcal{D}_3 : \\text{Categorical}([0, 0, 0, 0, 0, 0.5, 0.5, 0]),\n\\]\n\\[\n\\mathcal{D}_4 : \\text{Categorical}([0, 0, 0, 0, 0, 0, 0.5, 0.5]),\n\\]\n\\[\n\\mathcal{D}_5 : \\text{Categorical}([0.5, 0, 0, 0, 0, 0, 0, 0.5]),\n\\]\n\\[\n\\mathcal{D}_6 : \\text{Categorical}([0.5, 0.5, 0, 0, 0, 0, 0, 0]).\n\\]\n\n\n\\paragraph{Amazon}\nAmazon has five classes, which we denote as $\\{0, 1, 2, 3, 4\\}$. \n\\[\n\\mathcal{D}_0 : \\text{Categorical}([0, 0.5, 0, 0, 0, 0, 0, 0.5]),\n\\]\n\\[\n\\mathcal{D}_1 : \\text{Categorical}([0.5, 0, 0.5, 0, 0, 0, 0, 0]),\n\\]\n\\[\n\\mathcal{D}_2 : \\text{Categorical}([0, 0.5, 0, 0.5, 0, 0, 0, 0]),\n\\]\n\\[\n\\mathcal{D}_3 : \\text{Categorical}([0, 0, 0.5, 0, 0.5, 0, 0, 0]),\n\\]\n\\[\n\\mathcal{D}_4 : \\text{Categorical}([0, 0, 0, 0.5, 0, 0.5, 0, 0]),\n\\]\n\n\n\n"
            },
            "section 14": {
                "name": "Extended Real-World Dataset Description",
                "content": "\nIn the following, we describe the datasets and their features in more detail since it may help to improve the understanding of some results.\nFor our experiments, we use nine datasets. Three datasets have a homophily\nscore above 0.5 and six datasets are heterophilic,i.e., homophily under 0.5. We measure homophily\nby the adjusted homophily ratio~\\cite{AdjustedHomophily}. This metric is suited to compare datasets with different numbers\nof classes.\nThe homophilic datasets are the three standard citation graphs Cora, CiteSeer, and PubMed~\\cite{CitationNetworkDatasets}. In\nthese datasets, each node represents a document and edges correspond to citations between them. In\nCora and CiteSeer, node features represent elements of a bag-of-words representation of a document\nand the label of a node indicates the topic. In PubMed, features are represented by a TF-IDF weighted\nvector. In these datasets, the nodes are more likely to be connected with nodes of the same class.\nFor the heterophilic datasets, where nodes are more likely to be connected with nodes from\nother classes, we use Chameleon, Squirrel~\\cite{NodeHomoRatio}, Actor~\\cite{NodeHomoRatio}, Amazon, Roman, and\nMinesweeper~\\cite{NewHeteroData}. Chameleon and Squirrel are page-to-page graphs in Wikipedia with low ho-\nmophily. Nodes represent Wikipedia articles and edges represent hyperlinks between pages. Node\nfeatures correspond to several informative nouns in the Wikipedia pages and the nodes are classified\ninto five categories, which denote the number of the average monthly traffic of the web page. In the\nActor dataset, the nodes correspond to an actor, and the edges between them denote co-occurrences\non the same Wikipedia page. Features correspond to some keywords in the Wikipedia pages. The\nRoman dataset is based on the Roman Empire Wikipedia article. Each node corresponds\nto one (non-unique) word in the text. Two nodes are connected with an edge, if either these words\nfollow each other in the text, or these words are connected in the dependency tree of the sentence. In\nthe Amazon dataset, nodes are products, and edges denote if products are frequently bought\ntogether. Minesweeper dataset is inspired by the Minesweeper game. The graph is a $100 \\times 100$ grid\nwhere each node denotes a cell. Each cell is connected to 8 adjacent cells (except the cells at the\nedge). Node features are one-hot-encoded numbers of adjacent mines. The task is to predict which\nnodes are mines\n\n\n\n"
            },
            "section 15": {
                "name": "Edge Noise on Train and Test Set",
                "content": "\n\nWe performed the edge-noise experiment on the whole dataset, \\ie the same noise during training and testing.\nWe added uniform noise and categorical noise to the existing edges on Cora. \nThe test accuracy of the models with respect to the homophily level can be seen in Figure~\\ref{fig:add_edge_noise_exp}. \nGCN loses up to $11.01\\%$ accuracy for uniform edge-noise and up to $11.9\\%$ with categorical noise, while ES-MLP only loses up to $3.24\\%$ and $5.73\\%$.\nWe observe that ES-MLP is more robust to edge-noise than GCN in both cases.\nNote that in this setting, the edge-noise is already present during training, \\ie the performance drop is not happening due to some distribution shift between train and test data.\nTherefore, ES-MLP is not only robust towards edge noise during inference time but also towards general noisy edges in the data.\n\n\n\n\n\n"
            },
            "section 16": {
                "name": "Hyperparameter Sensitivity Analysis",
                "content": "\n\\label{appendix:hyper_sensitivity}\n\\add{\nWe provide a hyperparameter sensitivity analysis for the neighborhood contrastive loss weight $\\alpha_{NC}$ and the irrelevant consistency loss $\\beta_{ICR}$ in Figures~\\ref{fig:LossWeightsAnalysisAlpha} and~\\ref{fig:LossWeightsAnalysisBeta}.\nFor the hyperparameter neighborhood contrastive loss weight, we observe that ES-MLP achieves the best performance when $\\alpha_{NC}$ is $1$. On all datasets, the accuracy increases when the neighborhood contrastive loss $\\alpha_{NC}$ is not $0$.\nFor higher values of $\\alpha_{NC}$, the performance decreases again, since the model neglects the node features too much.\nThe irrelevant consistency loss weight $\\beta_{ICR}$ is in general smaller than $\\alpha_{NC}$.\nEspecially for heterophilic datasets, ES-MLP achieves the best performance when the ICR loss is not $0$.\nSimilar to $\\alpha_{NC}$, the performance decreases on most datasets when $\\beta_{ICR}$ is large.\nBoth parameters are quite robust, \\ie by deviating the parameter values, the performance changes only slowly. \nThis shows that most of the performance gain for heterophilic graphs is achieved by the edge-splitting mechanism in the architecture itself.}\n\n\n\n\n\n\n"
            },
            "section 17": {
                "name": "Local vs Global Homophily",
                "content": "\n\\label{appendix:local_vs_global_homophily}\n\\add{\nWe analyze the performance discrepancies between local and global homophily ratios during the robustness analysis based on the work of \\citet{PerformanceDiscrepanciesLocalHomphily}.\nWe trained the model once and evaluated it for multiple levels of edge noise.\nWe investigated the local homophily of single nodes for the global homophily level of $\\{0.35, 0.43, 0.58, 0.76\\}$ on Cora and $\\{0.37, 0.36, 0.33, 0.31 \\}$ on Amazon.\nA bar plot for the accuracy of each quantile based on the nodes' homophily is shown in Figure~\\ref{fig:local_vs_global_homophily}.\nFor Cora, we observe that the accuracy for heterophilic nodes increases as the global homophily reduces, while the homophilic nodes disappear in the second step which is consistent with the work of \\citet{PerformanceDiscrepanciesLocalHomphily} and \\citet{PAC-Bayesian}.\n}\n\n\n\n\n\n"
            },
            "section 18": {
                "name": "Adjacency Matrix Visualization",
                "content": "\n\\label{appendix:adj_vis}\n\\add{\nWe aggregated the $r$-th power of the $A_R$ and $A_{IR}$ matrix into a $|C|x|C|$ matrix $\\mathcal{C}^s$, where $r$ is a hyperparameter used for the respective dataset. \n$\\mathcal{C}_{ij}^s$ is computed by the ratio of edges between class $c_i$ and class $c_j$ in the $r$-th power of $A_R$ and $A_{IR}$ versus $A$.\nThe results can be seen in Figures~\\ref{fig:edge_analysis_cora} and~\\ref{fig:edge_analysis_actor}.}\n\n\n\n\n\n\n\\add{\nWe observe that $A_r$ has a higher ratio of edges on the diagonal than $A_{IR}$, \\ie the learned adjacency matrix is more homophilic.\nFor Actor, the model learned to ignore most of the edges, \\ie the majority of the edges is in $A_{IR}$.\n}\n\n\n"
            },
            "section 19": {
                "name": "Extended CSBM Results",
                "content": "\nWe provide additional homophily measures for the CSBM datasets measured with the edge homophily, node homophily, and class-insensitive homophily measure in Figure~\\ref{fig:CSBM Heatmaps}.\nWe added a complete overview of CSBM results with different homophily measures in Table~\\ref{tab:csbm summary}.\n\n\n\n\n\n\n\n"
            }
        },
        "tables": {
            "tab:DatasetsStatistics": "\\begin{table}[!ht]\n        \\caption{Statistics of the real-world datasets. \n        $|V|$ denotes the number of nodes, $|E|$ the number of edges, $d$ the feature dimension, $|C|$ the number of classes, $h_{adj}$ the adjusted homophily measure, and $h_{edge}$ the edge homophily.}\n     \\centering\n    \\begin{tabular}{l  c  c  c  c  c c}\n    \\toprule\n      \\textbf{Dataset}   & $|V|$ & $|E|$ & $d$ & $|C|$ & $h_{adj}$ & $h_{edge}$\\\\\n         \\hline\n      Cora     & 2,708 & 5,278 & 1,433 & 7 & 0.77 & 0.81\\\\\n      CiteSeer & 3,327 & 4,552 & 3,703 & 6 & 0.67 & 0.74\\\\\n      PubMed   & 19,717 & 44,324 & 500 & 3 & 0.69 & 0.80\\\\\n      \\hline\n      Actor & 7,600 & 30,019 & 932 & 5 & 0.00 & 0.22\\\\\n      Roman & 22,662 & 32,927 & 300 & 18 & -0.05 & 0.05 \\\\\n      Amazon & 24,492 & 93,050 & 300 & 5 & 0.14 & 0.38\\\\\n      Minesweeper & 10,000 & 39,402 & 7 & 2 & 0.01 & 0.68\\\\\n      \\bottomrule\n    \\end{tabular}\n    \\label{tab:DatasetsStatistics}\n\\end{table}",
            "tab:results_real_world": "\\begin{table*}[!ht]\n    \\centering\n    \\adjustbox{max width=\\textwidth}{\n        \\begin{tabular}{  c  c  c  c  c  c  c|  c}\n        \\toprule\n        &  MLP & GCN & Graph-MLP &  ES-GNN & GraphSAGE & LINKX & ES-MLP (ours)  \\\\\n        \\hline\n        Cora     & $76.95_{1.00}$ & $\\mathbf{88.46_{0.83}}$  & $86.64_{1.14}$  & $87.30_{0.43}$ & $88.26_{0.50}$ & $83.15_{0.59}$ & $88.15_{1.85}$ \\\\\n        CiteSeer & $72.10_{1.12}$ & $77.41_{0.95}$ & $\\mathbf{77.79_{0.10}}$ & $74.27_{1.50} $& $76.54_{0.73}$ & $73.23_{0.85}$& $75.67_{0.92}$ \\\\\n        PubMed   & $87.49_{0.90}$ & $\\mathbf{89.63_{0.79}}$ & $87.06_2.41$   & $88.81_{0.49}$ & $89.60_{0.41}$ & $87.47_{0.29}$ & $87.56_{1.23}$\\\\\n        \\hline\n        Actor          & $35.81_{0.62}$ & $29.24_{0.47}$ & $36.03_{0.98}$  & $38.91_{0.45}$ & $32.24_{0.76}$ & $33.92_{1.11}$ & $\\mathbf{39.73_{0.37}}$\\\\\n        Roman   & $60.50_{0.88}$ & $41.40_{1.58}$ & $64.94_{0.25}$  & $60.41_{1.90}$ & $62.47_{1.90}$ & $65.40_{0.37}$& $\\mathbf{65.44_{0.92}}$\\\\\n        Amazon & $44.05_{0.54}$ & $46.27_{0.67}$ & $37.07_{0.80}$   & $46.53_{0.34}$ & $44.83_{1.16}$ & $39.25_{0.51}$ & $\\mathbf{47.8  5_{1.23}}$\\\\\n        Minesweeper    & $50.54_{0.49}$ & $71.44_{0.74}$ & $50.99_{0.35}$ & $68.23_{1.10}$ & $\\mathbf{88.90_{2.37}}$  & $51.61_{1.4}$&  $ 50.87_{2.03}$\\\\\n        \\bottomrule\n        \\end{tabular}}\n    \\caption{Test accuracies and standard deviation in percent (\\%) on the real-world datasets averaged over $10$ runs for each experiment. The best score for a dataset is marked in bold.}\n    \\label{tab:results_real_world}\n\\end{table*}",
            "tab:InferenceTimes": "\\begin{table*}[!ht]\n\\adjustbox{max width=\\textwidth}{\n    \\begin{tabular}{lrrrrrr|r|rrrrrr|r}\n    \\toprule\n    & \\multicolumn{6}{c|}{Inference time on test nodes using the full graph} & \\multicolumn{6}{c}{Inference time on test nodes using only the test graph} \\\\\n    \\midrule\n    & MLP & GCN & Graph- & ES- & Graph- & LINKX & ES- & MLP & GCN & Graph- & ES- & Graph- &LINKX& ES- \\\\ \n    &     &     & MLP    & GNN & SAGE   &       & MLP &     &     & MLP    & GNN & SAGE   &     & MLP \\\\ \n    \\midrule\n    Cora           & 0.150  & 0.963 & 0.206 & 1.389 &0.613 & 1.089 & 0.286 & 0.130 & 0.916 & 0.172 & 1.334  & 0.557 & 0.881 & 0.254 \\\\ \n    PubMed         & 0.150  & 1.763 & 0.219 & 1.367 &1.313 & 1.145 & 0.306 &  0.124 & 1.282 & 0.219 & 1.313 & 0.561 & 0.900 & 0.271 \\\\ \n    CiteSeer       & 0.144  & 1.230 & 0.222 & 1.381 &1.381 & 1.011 & 0.321 & 0.117 & 0.917 & 0.222 & 1.299 & 0.553 & 0.923 &0.264 \\\\ \n    \\midrule\n    Actor          & 0.166  & 0.996 & 0.243 & 1.395 &1.333 & 1.169 & 0.326 & 0.133 & 0.943 & 0.184 & 1.333 & 0.570 & 0.913 &0.290 \\\\ \n    Amazon          & 0.149  & 1.149 & 0.211 & 1.400 &1.322 & 1.194 & 0.314 & 0.128 & 0.918 & 0.179 & 1.322 & 0.556 & 0.918 & 0.273 \\\\ \n    Roman           & 0.141  & 1.554 & 0.204 & 1.392 &1.333 & 1.209 &0.301  & 0.127 & 0.897 & 0.174 & 1.333 & 0.556 & 0.915 & 0.261 \\\\ \n    Minesweeper    & 0.139  & 1.037 & 0.292 & 1.392 &1.289 & 1.253 & 0.294 & 0.109 & 0.913 & 0.292 & 1.289 & 0.624 & 0.911 &0.250 \\\\ \n    \\bottomrule\n\\end{tabular}}\n    \\caption{The inference times in milliseconds on the full graph (left) and test graph only (right). The times are averaged over 10 runs.}\n    \\label{tab:InferenceTimes}\n\\end{table*}",
            "table:ablation study": "\\begin{table}[!th]\n    \\small\n    \\centering\n    \\begin{tabular}{l | r | r | r | r | r | r}\n        \\toprule\n\n          & \\textbf{Cora} &\\textbf{CiteSeer} & \\textbf{PubMed} & \\textbf{Actor} & \\textbf{Amazon} & \\textbf{Roman} \\\\\n        \\hline  \n        ES-MLP w/o $\\mathcal{L_{NC}}$& $86.16_{0.95}$ & $72.84_{0.74}$ &  $86.62_{0.22}$ & $36.00_{0.09}$ & $45.22_{0.02}$ & $60.92_{0.15}$ \\\\\n        ES-MLP w/o $\\mathcal{L_{ICR}}$ & $87.15_{0.52}$ & $\\mathbf{75.67_{0.63}}$ & $\\mathbf{87.56_{0.14}}$ & $35.12_{0.33}$& $46.36_{0.14}$ & $61.52_{0.15}$ \\\\\n        \\hline\n        ES-MLP &$ \\mathbf{88.15_{1.85}}$ & $\\mathbf{75.67_{0.92}}$ & $\\mathbf{87.56_{1.23}} $ & $\\mathbf{39.73_{0.37}}$ & $\\mathbf{47.85_{1.23}}$ & $\\mathbf{65.44_{0.92}}$  \\\\\n        \\bottomrule\n        \n         \n    \\end{tabular}\n    \\caption{Ablation study on separate parts of the loss functions. Scores are given as test accuracies. The best results are marked in bold.}\n    \\label{table:ablation study}\n\\end{table}",
            "tab:HPMLP": "\\begin{table}[!ht]\n    \\small\n    \\centering\n    \\begin{tabular}{l r r r r}\n        \\toprule\n         \\textbf{Datasets}& Hidden & Learning rate & Dropout & Weight Decay \\\\\n         \\hline\n         Cora & 256&0.05&0.8&$5\\text{e-}04$\\\\\n         CiteSeer & 64&0.05&0.8&$5\\text{e-}07$\\\\\n         PubMed & 256&0.05&0.5&$1\\text{e-}05$\\\\\n         \\hline\n         Chameleon & 64&0.01&0.8&$5\\text{e-}07$\\\\\n         Squirrel & 256&0.05&0.8&$5\\text{e-}04$\\\\\n         Actor &256&0.05&0.8&$5\\text{e-}05$\\\\\n         Amazon &128&0.05&0.2&$5\\text{e-}07$\\\\\n         Roman &128&0.05&0.8&$5\\text{e-}05$\\\\\n         Minesweeper &256&0.01&0.5&$5\\text{e-}04$\\\\\n\n         \\bottomrule\n    \\end{tabular}\n    \\caption{Used Hyperparameters for MLP}\n    \\label{tab:HPMLP}\n\\end{table}",
            "tab:HPGCN": "\\begin{table}[!ht]\n    \\small\n    \\centering\n    \\begin{tabular}{l r r r r}\n    \\toprule\n         \\textbf{Datasets}& Hidden & Learning rate & Dropout & Weight Decay \\\\\n         \\hline\n         Cora & 128 & 0.05 & 0.8 & $5\\text{e-}04$ \\\\\n         CiteSeer& 32 & 0.005 & 0.2 & $5\\text{e-}04$ \\\\\n         PubMed & 256 & 0.05 &  0.5 & $5\\text{e-}07$\\\\\n         \\hline\n         Chameleon & 64 &0.05&0.5&  $5\\text{e-}07$\\\\\n         Squirrel & 256 &0.005&0.8&$1\\text{e-}05$\\\\\n         Actor & 64&0.05&0.8&$5\\text{e-}04$\\\\\n         Amazon &128&0.05&0.2&0.0\\\\\n         Roman & 256&0.05&0.5&$5\\text{e-}07$\\\\\n         Minesweeper & 128&0.005&0.8&$1\\text{e-}07$\\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{Used hyperparameters for GCN}\n    \\label{tab:HPGCN}\n\\end{table}",
            "tab:HPGS": "\\begin{table}[!ht]\n    \\small\n    \\centering\n    \\begin{tabular}{l r r r r}\n    \\toprule\n         \\textbf{Datasets}& Hidden & Learning rate & Dropout & Weight Decay \\\\\n         \\hline\n         Cora&256&0.005&0.5&$5\\text{e-}04$\\\\\n         CiteSeer&256&0.05&0.8&$5\\text{e-}04$\\\\\n         PubMed&128&0.05&0.5&$1\\text{e-}05$\\\\\n         \\hline\n         Chameleon&64&0.01&0.8&0.0\\\\\n         Squirrel&256&0.05&0.8&$5\\text{e-}04$\\\\\n         Actor&64&0.05&0.8&$5\\text{e-}06$\\\\\n         Amazon&256&0.01&0.2&0.0\\\\\n         Roman&256&0.05&0.2&$5\\text{e-}06$\\\\\n         Minesweeper&128&0.005&0.8&$5\\text{e-}07$\\\\ \n         \\bottomrule\n    \\end{tabular}\n    \\caption{Used hyperparameters for GraphSAGE}\n    \\label{tab:HPGS}\n\\end{table}",
            "tab:HPLINX": "\\begin{table}[!ht]\n    \\small\n    \\centering\n    \\begin{tabular}{l r r r r}\n    \\toprule\n         \\textbf{Datasets}& Hidden & Learning rate & Dropout & Weight Decay \\\\\n         \\hline\n         Cora          &256&0.05&0.2&$5\\text{e-}04$\\\\\n         CiteSeer      &128&0.5&0.0&$5\\text{e-}04$\\\\\n         PubMed        &64&0.01&0.2&$5\\text{e-}04$\\\\\n         \\hline\n         Chameleon     &256&0.002&0.5&$5\\text{e-}04$\\\\\n         Squirrel      &64&0.002&0.8&$5\\text{e-}04$\\\\\n         Actor         &256&0.05&0.2&$5\\text{e-}04$\\\\\n         Amazon        &128&0.01&0.5&$5\\text{e-}05$\\\\\n         Roman         &256&0.01&0.8&$1\\text{e-}05$\\\\\n         Minesweeper   &256&0.01&0.8&$5\\text{e-}04$\\\\ \n         \\bottomrule\n    \\end{tabular}\n    \\caption{Used hyperparameters for LINKX}\n    \\label{tab:HPLINX}\n\\end{table}",
            "appendix:hyperparameteroptimization_esmlp": "\\begin{table}[!ht]\n    \\small\n    \\centering\n    \\begin{tabular}{ l  r  r  r r  r }\n    \\toprule\n         \\textbf{Dataset} & adjacency power $r$ & \\textbf{$\\alpha_{NC}$}  & \\textbf{$\\beta_{ICR}$}  & \\textbf{$\\epsilon_R$} & \\textbf{$\\epsilon_{IR}$}\\\\\n         \\hline\n         Cora       & $4$  & $1$ & $0.01$ & $0.5$ & $0.5$ \\\\\n         CiteSeer   & $2 $ & $1$  & $0.0$   & $0.1$ & $0.3$ \\\\\n         PubMed     & $2$  &  $1$ &  $0.0$  &   $0.3$  & $0.1$ \\\\\n         \\hline\n         Chameleon &  $2$ & $10$ &  $0.001$    &  $0.3$  &   $0.5$   \\\\\n         Squirrel  &  $1$ & $1$ & $0.0001$ &  $0.7$ &    $0.3$   \\\\\n         Actor &3&1&0.0001&0&0.7\\\\\n         Amazon  &3&1&0.0001&0&0.5\\\\\n         Roman &1&1&0.00001&0.5&0.3\\\\\n         Minesweeper &4&100&0.01&0.7&0.7\\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{Hyperparameters for ES-MLP}\n    \\label{appendix:hyperparameteroptimization_esmlp}\n\\end{table}",
            "appendix:hyperparameteroptimization_graphmlp": "\\begin{table}[!th]\n    \\small\n    \\centering\n    \\begin{tabular}{l  r  r }\n        \\toprule\n         \\textbf{Dataset} & adjacency power $r$ & \\textbf{$\\alpha_{NC}$} \\\\\n         \\hline\n         Cora     & $2$ & $10$ \\\\\n         CiteSeer & $2$ & $1$ \\\\\n         PubMed   & $1$ & $10$ \\\\\n         \\hline\n         Chameleon & $2$ & $10$ \\\\\n         Squirrel & $1$ & $10$ \\\\\n         Actor & - & $0$ \\\\\n         Amazon &-& $0$\\\\\n         Roman &1&1\\\\\n         Minesweeper &2&10\\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{Hyperparameters for GraphMLP}\n    \\label{appendix:hyperparameteroptimization_graphmlp}\n\\end{table}",
            "appendix:hyperparameteroptimization_esgnn": "\\begin{table}[!ht]\n    \\small\n    \\centering\n    \\begin{tabular}{ l  r  r  r  }\n    \\toprule\n         \\textbf{Dataset} & \\textbf{$\\beta_{ICR}$}  & \\textbf{$\\epsilon_R$} & \\textbf{$\\epsilon_{IR}$}\\\\\n         \\hline\n         Cora       & $0.0001$ & $0.7$ & $0.1$ \\\\\n         CiteSeer   & $0.01$   & $0.7$ & $0.7$ \\\\\n         PubMed     &$0.01$ &   $0.7$  & $0.3$ \\\\\n         \\hline\n         Chameleon & $0.001$    &  $0.3$  &   $0.3$   \\\\\n         Squirrel  & $0.01$ &  $0.7$ &    $0.3$   \\\\\n         Actor & 0.0001 & 0.7 & 0.7\\\\\n         Amazon  & 0.01 &0.7 & 0.5.7\\\\\n         Roman & 0.00001 & 0.1 & 0.0\\\\\n         Minesweeper & 0.0001 & 0.7 & 0.0\\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{Hyperparameters for ES-GNN}\n    \\label{appendix:hyperparameteroptimization_esgnn}\n\\end{table}",
            "tab:Extendet Results": "\\begin{table*}[!ht]\n\\adjustbox{max width=\\textwidth}{\n    \\begin{tabular}{lrrrrrr|r|rrrrrr|r}\n    \\toprule\n    & \\multicolumn{6}{c|}{Accuracies on test nodes using the full graph} & \\multicolumn{6}{c}{Accuracies on test nodes using only the test graph} \\\\\n    \\midrule\n    & MLP & GCN & Graph- & ES- & Graph- & LINKX & ES- & MLP & GCN & Graph- & ES- & Graph- & LINKX & ES- \\\\ \n    &     &     & MLP    & GNN & SAGE   &       & MLP &     &     & MLP    & GNN & SAGE   &       & MLP \\\\ \n    \\midrule\n    Cora         & $76.95_{1.00}$  & $\\mathbf{88.46_{0.83}}$ & $86.64_{1.14}$ & $87.30_{0.43}$ & $88.26_{0.50}$ & $83.15_{0.59}$  & $88.15_{1.85}$ & $76.95_{1.00}$ &$80.03_{1.12}$ &  $86.64_{1.14}$ & $70.50_{6.71}$  & $79.49_{0.88}$ & $72.22_{1.19}$ &$\\mathbf{88.15_{1.85}}$ \\\\ \n    CiteSeer       & $72.10_{1.12}$  & $77.41_{0.95}$          &$\\mathbf{77.79_{0.10}}$ &  $74.27_{1.50} $ & $76.54_{0.73}$ &$73.23_{0.85}$& $75.67_{0.92}$ &   $72.10_{1.12}$ & $69.74_{0.64}$ & $\\mathbf{77.79_{0.10}}$  & $62.24_{3.83}$  & $62.94_{3.74}$ & $70.93_{1.06}$ &$75.67_{0.92}$ \\\\ \n    PubMed     & $87.49_{0.90}$  & $\\mathbf{89.63_{0.79}}$& $87.06_{2.41}$ & $88.81_{0.49}$ &$89.60_{0.41}$& $87.47_{0.29}$ &$87.56_{1.23}$ &$87.49_{0.90}$ & $85.52_{0.51}$ & $87.06_{2.41}$  &$82.56_{1.78}$ & $71.56_{1.78}$ & $87.54_{0.28}$ &$\\mathbf{87.56_{1.23}}$ \\\\ \n    \\midrule\n    Chameleon  &$50.54_{1.05}$  & $46.36_{1.45}$            & $51.22_{0.14}$ & $66.32_{2.10}$ & $53.46_{0.99}$ & $\\mathbf{68.10_{1.33}}$& $65.84_{2.19}$ & $50.54_{1.05}$ &$44.74_{1.23}$ & $51.22_{0.14}$ & $58.90_{1.25}$ & $26.90_{1.25}$ & $41.57_{1.65}$ &$\\mathbf{65.84_{2.19}}$ \\\\ \n    Squirrel   & $34.76_{0.90}$  &$28.79_{1.00}$           & $34.18_{0.16}$ & $60.20_{0.92}$ &$35.54_{0.91}$ &$\\mathbf{60.90_{0.81}} $& $55.01_{1.81}$ & $34.76_{0.90}$ & $28.62_{0.86}$ &  $34.18_{0.16}$  & $39.90_{0.94}$ & $21.90_{1.11}$ & $27.71_{1.56}$& $\\mathbf{55.01_{1.81}}$ \\\\ \n    Actor       & $35.81_{0.62}$  &  $29.24_{0.47}$          & $36.03_{0.98}$ & $38.91_{0.45}$ &$32.24_{0.76}$ & $33.92_{1.11}$ &$\\mathbf{39.73_{0.37}}$ &  $35.81_{0.62}$ &$30.76_{0.61}$ & $36.03_{0.98}$ & $31.29_{1.67}$ & $25.29_{1.68}$ &  $34.82_{1.22}$& $\\mathbf{39.73_{0.37}}$\\\\ \n    Amazon      &$44.05_{0.54}$  &  $46.27_{0.67}$              & $37.07_{0.80}$ & $46.53_{0.34}$ &$44.83_{1.16}$ & $39.25_{0.51}$ &$\\mathbf{47.8  5_{1.23}}$ &  $44.05_{0.54}$& $43.53_{0.78}$ &$37.07_{0.80}$  & $41.05_{0.68}$ & $41.05_{0.68}$ &  $39.26_{0.55}$& $\\mathbf{47.8  5_{1.23}}$ \\\\ \n    Roman       & $60.50_{0.88}$  & $41.40_{1.58}$          & $64.94_{0.25}$& $60.41_{1.90}$ &$62.47_{1.90}$ & $65.40_{0.37}$ &$\\mathbf{65.44_{0.92}}$ &$60.50_{0.88}$ & $42.10_{0.54}$ & $64.94_{0.25}$ & $52_.02_{0.09}$& $52.02_{0.09}$ &$64.61_{0.33}$& $\\mathbf{65.44_{0.92}}$ \\\\ \n    Minesweeper    & $50.54_{0.49}$  &$71.44_{0.74}$            & $50.99_{0.35}$ & $68.23_{1.10}$ &$\\mathbf{88.90_{2.37}}$ & $51.61_{1.4}$ &$ 50.87_{2.03}$ & $50.54_{0.49}$ & $\\mathbf{62.01_{0.29}}$ &$50.99_{0.35}$ &  $56.85_{0.89}$ & $56.68_{0.89}$ & $48.96_{1.01}$& $ 50.87_{2.03}$ \\\\ \n    \\bottomrule\n\\end{tabular}}\n    \\caption{Test accuracies and standard deviation in percent (\\%) on the real-world datasets. Full denoting given the full graph as model input and sub denoting given the test graph only as model input. The best results are marked in bolt for both settings respectively.}\n    \\label{tab:Extendet Results}\n\\end{table*}",
            "tab:csbm summary": "\\begin{table}[ht]\n\\centering\n\\begin{tabular}{cccccccc}\n\\hline\nIndex & $p$ & $q$ & $\\mathcal{H_E}$ & $\\mathcal{H_N}$ & $\\mathcal{H_{C}}$ & $h_{\\text{adj}}$ & Accuracy \\\\\n\\hline\n1  & 0.0 & 0.0 & 0.000 & 0.000 & 0.000 & 0.000 & 0.604 \\\\\n2  & 0.0 & 0.2 & 0.000 & 0.000 & 0.000 & -1.000 & 0.604 \\\\\n3  & 0.0 & 0.4 & 0.000 & 0.000 & 0.000 & -1.000 & 0.727 \\\\\n4  & 0.0 & 0.6 & 0.000 & 0.000 & 0.000 & -1.000 & 0.708 \\\\\n5  & 0.0 & 0.8 & 0.000 & 0.000 & 0.000 & -1.000 & 0.657 \\\\\n6  & 0.0 & 1.0 & 0.000 & 0.000 & 0.000 & -1.000 & 0.700 \\\\\n7  & 0.2 & 0.0 & 1.000 & 1.000 & 1.000 & 1.000 & 0.672 \\\\\n8  & 0.2 & 0.2 & 0.500 & 0.500 & 0.005 & 0.0001 & 0.672 \\\\\n9  & 0.2 & 0.4 & 0.333 & 0.349 & 0.000 & -0.333 & 0.693 \\\\\n10 & 0.2 & 0.6 & 0.250 & 0.298 & 0.000 & -0.499 & 0.690 \\\\\n11 & 0.2 & 0.8 & 0.200 & 0.298 & 0.000 & -0.600 & 0.654 \\\\\n12 & 0.2 & 1.0 & 0.167 & 0.305 & 0.005 & -0.666 & 0.685 \\\\\n13 & 0.4 & 0.0 & 1.000 & 1.000 & 1.000 & 1.000 & 0.721 \\\\\n14 & 0.4 & 0.2 & 0.667 & 0.670 & 0.346 & 0.333 & 0.678 \\\\\n15 & 0.4 & 0.4 & 0.500 & 0.500 & 0.001 & -0.000 & 0.699 \\\\\n16 & 0.4 & 0.6 & 0.400 & 0.406 & 0.000 & -0.200 & 0.695 \\\\\n17 & 0.4 & 0.8 & 0.333 & 0.367 & 0.000 & -0.333 & 0.691 \\\\\n18 & 0.4 & 1.0 & 0.286 & 0.341 & 0.000 & -0.428 & 0.622 \\\\\n19 & 0.6 & 0.0 & 1.000 & 1.000 & 1.000 & 1.000 & 0.720 \\\\\n20 & 0.6 & 0.2 & 0.750 & 0.750 & 0.505 & 0.5000 & 0.624 \\\\\n21 & 0.6 & 0.4 & 0.600 & 0.603 & 0.205 & 0.1999 & 0.779 \\\\\n22 & 0.6 & 0.6 & 0.500 & 0.500 & 0.001 & -0.0005 & 0.623 \\\\\n23 & 0.6 & 0.8 & 0.428 & 0.435 & 0.000 & -0.143 & 0.607 \\\\\n24 & 0.6 & 1.0 & 0.375 & 0.388 & 0.000 & -0.250 & 0.657 \\\\\n25 & 0.8 & 0.0 & 1.000 & 1.000 & 1.000 & 1.000 & 0.695 \\\\\n26 & 0.8 & 0.2 & 0.800 & 0.794 & 0.633 & 0.600 & 0.654 \\\\\n27 & 0.8 & 0.4 & 0.667 & 0.673 & 0.365 & 0.333 & 0.699 \\\\\n28 & 0.8 & 0.6 & 0.571 & 0.574 & 0.152 & 0.142 & 0.697 \\\\\n29 & 0.8 & 0.8 & 0.500 & 0.499 & 0.003 & -0.0005 & 0.630 \\\\\n30 & 0.8 & 1.0 & 0.444 & 0.447 & 0.000 & -0.111 & 0.664 \\\\\n31 & 1.0 & 0.0 & 1.000 & 1.000 & 1.000 & 1.000 & 0.694 \\\\\n32 & 1.0 & 0.2 & 0.833 & 0.821 & 0.696 & 0.666 & 0.701 \\\\\n33 & 1.0 & 0.4 & 0.714 & 0.718 & 0.472 & 0.427 & 0.754 \\\\\n34 & 1.0 & 0.6 & 0.625 & 0.632 & 0.278 & 0.249 & 0.670 \\\\\n35 & 1.0 & 0.8 & 0.556 & 0.557 & 0.120 & 0.110 & 0.665 \\\\\n36 & 1.0 & 1.0 & 0.500 & 0.500 & 0.002 & -0.0002 & 0.689 \\\\\n\\hline\n\\end{tabular}\n\\caption{The results for the CSBM datasets with homophily ratios, the respective $p$ and $q$ values, and accuracy.}\n\\label{tab:csbm summary}\n\\end{table}"
        },
        "figures": {
            "fig:method_pipeline": "\\begin{figure*}\n    \\centering\n    \\includegraphics[width=1.00\\textwidth]{Pipeline.pdf}\n    \\caption{The pipeline of ES-MLP. Node features are first projected into two different subspaces $R$ and $IR$. A separate forward pass is performed to obtain embeddings $Z_R$ and $Z_{IR}$. Both embeddings are then used for edge splitting to divide the original graph edges into two sets and for the neighborhood contrastive loss. The task-relevant representation $Z_R$ is then used for the prediction task and task-irrelevant representation $Z_{IR}$ is used to calculate the Irrelevant Consistency Loss.}\n    \\label{fig:method_pipeline}\n\\end{figure*}",
            "fig:CSBM_main_Heatmaps": "\\begin{figure}[htbp]\n    \\centering\n    \\subfigure{\n        \\includegraphics[width=0.42\\textwidth]{figures/csbm_heatmap.pdf}\n        \n        \\label{fig:csbm heatmap}\n    }\n    \\hspace{0.05\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.42\\textwidth]{figures/csbm_adj_homophily_hm.pdf}\n        \\label{fig:adjusted_homophily}\n    }\n    \\caption{Test accuracies for the CSBM datasets (left). The scores are averaged over 5 runs.\n            The probability for intra-class edges is on the x-axis and the probability for inter-class edges is on the y-axis. \n            The adjusted homophily ratios $\\mathcal{H}_{adj}$ for the CSBM datasets (right).}\n    \\label{fig:CSBM_main_Heatmaps}\n\\end{figure}",
            "fig:Robustness Analysis": "\\begin{figure*}[!ht]\n    \\includegraphics[width=\\textwidth]{figures/all_edge_noise_plots.pdf}\n        \\caption{Test accuracies of the robustness analysis on the Cora and Amazon datasets. \n        Left to right: Results on Cora with uniform edge-noise, Cora with categorical edge-noise, Amazon with Uniform edge-noise, and Amazon with categorical edge-noise. \n        Our ES-MLP, as it does not require edges at test time, remains constant on all datasets.\n        Graph-MLP, not shown for brevity, also remains constant. \n        }\n    \\label{fig:Robustness Analysis}\n\\end{figure*}",
            "fig:add_edge_noise_exp": "\\begin{figure}[htbp]\n    \\centering\n    \\subfigure{\n        \\includegraphics[width=0.4\\textwidth]{figures/edge_noise_appendix_cora_Uniform.pdf}\n        \n        \\label{fig:add_edge_noise_uniform}\n    }\n    \\hspace{0.01\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.4\\textwidth]{figures/edge_noise_appendix_cora_Custom.pdf}\n        \\label{fig:add_edge_noise_categorical}\n    }\n    \\caption{Results for the edge-noise experiment on Cora with uniform (left) and categorical (right) noise.}\n    \\label{fig:add_edge_noise_exp}\n\\end{figure}",
            "fig:LossWeightsAnalysisAlpha": "\\begin{figure}[htbp]\n    \\centering\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/Cora_alpha.pdf}\n        \n        \\label{fig:cora_alpha}\n    }\n    \\hspace{0\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/CiteSeer_alpha.pdf}\n        \\label{fig:citeseer_alpha}\n    }\n    \\hspace{0\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/actor_alpha.pdf}\n        \\label{fig:actor_alpha}\n    }\n    \\hspace{0\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/Amazon-ratings_alpha.pdf}\n        \\label{fig:amazon_alpha}\n    }\n    \\hspace{0\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/Roman-empire_alpha.pdf}\n        \\label{fig:roman_alpha}\n    }\n\n\n    \\caption{Hyperparameter sensitivity analysis on the neighborhood contrastive loss weight $\\alpha_{NC}$ hyperparameter}\n    \\label{fig:LossWeightsAnalysisAlpha}\n\\end{figure}",
            "fig:LossWeightsAnalysisBeta": "\\begin{figure}[htbp]\n    \\centering\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/Cora_beta.pdf}\n        \n        \\label{fig:cora_beta}\n    }\n    \\hspace{0\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/CiteSeer_beta.pdf}\n        \\label{fig:citeseeer_beta}\n    }\n    \\hspace{0\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/actor_beta.pdf}\n        \\label{fig:actor_beta}\n    }\n    \\hspace{0\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/Amazon-ratings_beta.pdf}\n        \\label{fig:amazon_beta}\n    }\n    \\hspace{0\\textwidth}\n    \\subfigure{\n        \\includegraphics[width=0.3\\textwidth]{figures/Roman-empire_beta.pdf}\n        \\label{fig:roman_beta}\n    }\n\n\n    \\caption{Hyperparameter sensitivity analysis on the hyperparameter ICR loss weight $\\beta_{ICR}$.}\n    \\label{fig:LossWeightsAnalysisBeta}\n\\end{figure}",
            "fig:local_vs_global_homophily": "\\begin{figure}[ht!]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/local_vs_gloabl_homophily.pdf}\n    \\caption{Performance for ES-MLP and baseline models on Cora with generated edge noise on the test nodes with global homophily ratios $\\{0.35, 0.43, 0.58, 0.76\\}$. Results are reported for different local homophily ranges.}\n    \\label{fig:local_vs_global_homophily}\n\\end{figure}",
            "fig:edge_analysis_cora": "\\begin{figure}[ht!]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/Edge_Analysis_Cora.pdf}\n    \\caption{Adjacency Matrices $A_r$ and $A_{ir}$ on Cora. The ratio of homophilic edges and all edges in $A_r$ is $77.53\\%$, and in $A_{ir}$ $66.82\\%$ }\n    \\label{fig:edge_analysis_cora}\n\\end{figure}",
            "fig:edge_analysis_actor": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/Edge_Analysis_Actor.pdf}\n    \\caption{Adjacency Matrices $A_r$ and $A_{ir}$ on Actor. The ratio of homophilic edges and all edges in $A_r$  is $32.33\\%5$, and in $A_{ir}$ $21.91\\%$}\n    \\label{fig:edge_analysis_actor}\n\\end{figure}",
            "fig:CSBM Heatmaps": "\\begin{figure}[htbp]\n    \\centering\n    \\subfigure{\n        \\includegraphics[width=0.4\\textwidth]{figures/csbm_heatmap_edge_homophily.pdf}\n        \n        \\label{fig:edge_homphily_heatmap}\n    }\n    \\quad\n    \\subfigure{\n        \\includegraphics[width=0.4\\textwidth]{figures/csbm_heatmap_node_homophily.pdf}\n        \\label{fig:node_homophily}\n    }\n    \\quad\n    \\subfigure{\n        \\includegraphics[width=0.4\\textwidth]{figures/csbm_heatmap_insensitive_homophily.pdf}\n        \\label{fig:class_homophily}\n    }\n    \\caption{The edge homophily (top-left), node homophily,(top-right), and class-insensitive homophily (bottom) ratios of the CSBM dataset.}\n    \\label{fig:CSBM Heatmaps}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    h = \\frac{|{(u,v): (u,v) \\in{E} \\wedge y_u = y_v}|}{|E|}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\beta = \\frac{1}{|V|} \\sum_{v \\in V} \\frac{|{(w,v) : w \\in N(v) \\wedge y_v = y_w}|}{|N(v)|}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\hat{h} = \\frac{1}{C-1} \\sum_{k=0}^{C-1} \\max \\left(0, h_k - \\frac{|C_k|}{|V|}\\right).\n\\end{equation}"
        },
        "git_link": "https://github.com/MatthiasKohn/ES-MLP"
    }
}