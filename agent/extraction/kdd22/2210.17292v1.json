{
    "meta_info": {
        "title": "M$^3$Care: Learning with Missing Modalities in Multimodal Healthcare  Data",
        "abstract": "Multimodal electronic health record (EHR) data are widely used in clinical\napplications. Conventional methods usually assume that each sample (patient) is\nassociated with the unified observed modalities, and all modalities are\navailable for each sample. However, missing modality caused by various clinical\nand social reasons is a common issue in real-world clinical scenarios. Existing\nmethods mostly rely on solving a generative model that learns a mapping from\nthe latent space to the original input space, which is an unstable ill-posed\ninverse problem. To relieve the underdetermined system, we propose a model\nsolving a direct problem, dubbed learning with Missing Modalities in Multimodal\nhealthcare data (M3Care). M3Care is an end-to-end model compensating the\nmissing information of the patients with missing modalities to perform clinical\nanalysis. Instead of generating raw missing data, M3Care imputes the\ntask-related information of the missing modalities in the latent space by the\nauxiliary information from each patient's similar neighbors, measured by a\ntask-guided modality-adaptive similarity metric, and thence conducts the\nclinical tasks. The task-guided modality-adaptive similarity metric utilizes\nthe uncensored modalities of the patient and the other patients who also have\nthe same uncensored modalities to find similar patients. Experiments on\nreal-world datasets show that M3Care outperforms the state-of-the-art\nbaselines. Moreover, the findings discovered by M3Care are consistent with\nexperts and medical knowledge, demonstrating the capability and the potential\nof providing useful insights and explanations.",
        "author": "Chaohe Zhang, Xu Chu, Liantao Ma, Yinghao Zhu, Yasha Wang, Jiangtao Wang, Junfeng Zhao",
        "link": "http://arxiv.org/abs/2210.17292v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "9 pages"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n\nMultimodal data can provide complementary information from various modalities that reveal the fundamental characteristics of real-world subjects~\\cite{xu2013survey,xu2018raim,hoang2021aid,baltruvsaitis2018multimodal,cai2018deep}.\nThus, many clinical applications, \nsuch as disease diagnosis and mortality prediction~\\cite{ma2020concare,xu2018raim,ni2019modeling,zhang2018integrative,hoang2021aid}, \nrequire multimodal electronic health record (EHR) data to achieve good diagnostic or prognostic results. \nConventional approaches usually assume that each sample is associated with the unified uncensored modalities, and all modalities are available for each sample~\\cite{ma2021smil,zhang2019cpm}.\nHowever, missing modality is a common issue in real-world clinical scenarios~\\cite{huang2020fusion}. \nFor example, different types of examinations are usually conducted for different patients~\\cite{zhang2019cpm}.\nAlso, patients may lack some specific modalities due to patient dropout~\\cite{pan2021disease}, sensor damage, data corruption~\\cite{chen2020hgmf}, safety considerations~\\cite{zhou2020hi,ramos2004mri} and high cost~\\cite{ford2000non}.\nFormally, we define modality-missing in EHR data as, for a sample, data for at least one modality is missing. \nThe absence of a modality means that all features in this modality are missing.\nMoreover, the modality-missing patterns (i.e., combinations of available modalities) make it more complex for the data with more modalities~\\cite{zhang2019cpm}.\n\n\n\n\n\n\nThus, some pioneering research works are proposed to handle the modality-missing issue.\nSome researchers drop the incomplete samples~\\cite{ni2019modeling,wang2020multimodal} and achieve some improvements.\nHowever, this approach cannot be applied in areas where data is scarce and contains rigid requirements, such as healthcare.\nAlso, it will escalate the small-sample-size issue and over-fitting~\\cite{chen2020hgmf,pan2021disease}.\n\nThe complementary way to dropping methods is the imputation-based method.\nAs shown in the left part of Figure~\\ref{fig:med_missing_modal}, some methods assume that the entries of the data matrix are missing at random (or some more specific assumption on the matrix space, e.g., incoherence, confer~\\cite{davis2006relationship} for a survey), and the missingness can be imputed via modeling correlations between the columns (features)~\\cite{lin2020missing}.\nWhereas, as illustrated in the right part of Figure~\\ref{fig:med_missing_modal}, missing modalities manifest themselves by column-wise consecutive missingness, where the most correlated information inside the same modalities is missing entirely. On the other hand, the features inside the same modality are naturally more correlated than thereof in different modalities, exhibiting a coherent behavior in the matrix space. Thus the traditional imputation methods do not work well~\\cite{wang2020multimodal}.\nAdditionally, block-wise missingness~\\cite{yi2016st}, such as image~\\cite{hong2019deep} or geosensory data~\\cite{yi2016st}, often assumes a sample realization is from the matrix space, such that the row vectors in a matrix are not permutable.\nDifferent from that, the rows in missing modalities refer to the samples, which are permutable. \nThis results in the prior spatial (or spatio-temporal) correlations required to complete the block-wise missingness are not present in the missing modalities.\nIn a word, conceptually, there is a gap between the missing modalities (column-wise consecutive missingness) and the existing imputation-based methods for random or block-wise missingness.\n\n\nIn methodology, some deep generative methods~\\cite{ngiam2011multimodal,li2020estimation,pan2021disease,cai2018deep} are proposed for missing modalities.\nEssentially, such methods are usually based on the manifold assumption: the probability mass of real-world objects is supported on low-dimensional manifolds~\\cite{agarwal2010learning}. \nIn terms of EHR data imputation, the manifold assumption can be interpreted as the low-rankness and stability of the feature covariance matrix. \nIn other words, there exist a set of low-dimensional basis vectors and a deterministic mapping $\\mathcal{T}$ subject to: \n(a) the basis vectors span the pre-image of the mapping $\\mathcal{T}$, \n(b) and the observed EHR feature vectors live on the image of the mapping $\\mathcal{T}$. \nExisting EHR generative-based completion methods tackle the problem by solving a generative model which learns a mapping from a latent space (spanned by basis vectors) to the original input space~\\cite{ngiam2011multimodal,li2020estimation,pan2021disease,cai2018deep}. \nSolving such mapping is essentially an ill-posed inverse problem (low- to high-dimensional underdetermined system~\\cite{datta2010numerical}), whose solution is often non-unique and unstable~\\cite{kabanikhin2008definitions}. \nOn the other hand, \nsuch complex auxiliary models may introduce extra noise, which has negative impacts~\\cite{chen2020hgmf,wang2020multimodal,enders2010applied}.\nTo this end, the problem of completing missing modalities requires a different way.\n\n\n\n\n\n\nIn fact, for missing modalities, solving the generative model is not necessary. \nBy assuming the low-rankness and the stability of the covariance matrix of EHR features, the locally similar row vectors (patients with similar features from some uncensored modalities) in a sub-matrix imply globally similar row vectors (those patients should have similar missing features) in the data matrix.\nMoreover, if a local row vector $X$ falls in a convex hull spanned by a set of local row vectors, then the global row vector corresponding to $X$ is likely to fall into the convex hull spanned by the particular set of global row vectors.\nThus, instead of solving the inverse problem of the low- to high-dimensional mapping, we can solve a less underdetermined problem: comparing similarities of local row vectors and impute the missing entries in a row by referring to the uncensored entries of locally similar rows. \nThe similarity comparison can be conducted in the original input space with a sophisticated metric on data manifolds, or in a learned latent space with a more straightforward metric.\nMore importantly, modeling the similarity relationship in the low-dimensional latent space is a direct (or forward, namely) problem of solving a mapping from a high-dimensional space to a low-dimensional space, which is less complicated than the inverse problem of solving a generative model.\n\nOn the other hand, this intuition is also in agreement with the real-world clinical practice, i.e., how doctors use the relationships between patients to assist the clinical analysis\\footnote{\\hyperref[sec:discovery]{We also substantiate this intuition by mining the real-world clinical datasets, please refer to the Intuition discovery experiments in Appendix.}}.\nIf two patients are similar in one clinical modality, they are more likely to be similar in another one~\\cite{bari2009randomized,kwiecinski2021native,gong2018learning,huang2018cross}.\nThus, as shown in Figure~\\ref{fig:insight}, for a patient with missing modalities, we can utilize the other complete modalities of the patient (local row vector) and other patients who also have these modalities (local uncensored row vectors), to find similar patients. \nAlthough seeming straightforward, applying this intuition to clinical tasks will face the following challenges:\n\n\n\n\\textbf{Challenge 1.\nWhich space can be used to perform imputation?}\nThere are at least two options: the original input space or a learned latent space.\nExisting methods usually estimate missing data in the original input space~\\cite{ngiam2011multimodal,cai2018deep,li2020estimation}.\nHowever, the probability distribution in the original input space contains task-relevant and task-irrelevant information. Imputation in the original input space treats task-irrelevant information equally, weakening the task-specific information conveyed in EHR data.\nThis results in an indiscriminate loss of task-relevant and task-irrelevant information, leading to inferior performance.\n\n\\textbf{Challenge 2.\nWhat metric(s) should be used to model the similarity relation?}\n% The raw multimodal EHR data are high-dimensional and difficult to measure similarity directly~\\cite{zhang2021grasp}.\nT-LSTM~\\cite{baytas2017patient} uses autoencoders to generate patient representations, based on which the similarity is obtained. \nSMIL~\\cite{ma2021smil} uses multivariate Gaussian to assign similarity weights.\nHowever, they did not associate the connotation of similarity with clinical tasks.\nIn different clinical tasks (e.g., mortality prediction and disease diagnosis), the patient characteristics that need attention are different, so two patients considered similar in one clinical task may be considered not so similar in another~\\cite{zhang2021grasp}.\nMore importantly, the similarity metric might vary for different modalities. \nThe manifolds in different modalities in the original input spaces are naturally equipped with different metrics. \nA learned deterministic mapping from the original input spaces to the learned latent space is not guaranteed to result in a unified metric for different mapped modalities in the latent space.\n\n\n\\textbf{Challenge 3.\nHow to infer the local-to-global similarities of patients?}\nMetrics computing the similarities between local row vectors is not sufficient to describe the similarities of global row vectors. \nThence modeling the intra- and inter-correlations of features from various modalities is challenging but indispensable to aggregate the information from different modalities. \n\n\n\nBy jointly considering the above issues, we propose a model learning with \\underline{M}issing \\underline{M}odalities in \\underline{M}ultimodal health\\underline{care} data (\\mname), an end-to-end inductive learning model to compensate for the missing modalities and perform clinical tasks.\nIn summary, our main contributions are summarized as follows:\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=2pt]\n\n    \\item We propose \\mname to compensate for %the missing information of  \n    the modality-missing patient in the latent space and perform clinical tasks with EHR data.\n    Since the latent representations are highly compressed and task-supervised, this results in less loss of task-relevant information and is thus more beneficial for subsequent tasks in an end-to-end learning schema (Response to Challenge 1). \n    \n    \\item Methodologically, (a) \\mname uses task-guided deep kernels in the latent space of each modality as the metric to compute patient similarities (Response to Challenge 2). (b)\n    \\mname captures intra-correlations within each modality and inter-correlations between modalities by a self-attentive multi-modal interaction module so that the local metrics are aggregated to calculate the similarities of global row vectors. (Response to Challenge 3).\n \n    \\item Extensive experiments show that \\mname outperforms all state-of-the-art models under multiple levels of incompleteness in different evaluation metrics. Besides, the findings discovered by \\mname are in accord with experts and medical knowledge, which shows it can provide useful insights and explanations.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n% 应该结构化一点，像第二部分看齐\n\\noindent\\textbf{Multimodal learning for healthcare.} \nWith the advancement of medical technology, comprehensive healthcare is burgeoning to meet the demands of patients.\nThis has allowed for multiple medical modalities (e.g., medical image, clinical notes, etc.) to be analyzed to offer patients with feedback, as well as physicians with insights on clinical applications~\\cite{xu2018raim,ni2019modeling,hoang2021aid,gao2020compose,ma2020adacare}.\n\n\nTo this end, multimodal learning for healthcare has attracted the interest of researchers.\nFor example, RAIM~\\cite{xu2018raim} is proposed for jointly analyzing continuous monitoring data (e.g., ECG, heart rate) and discrete clinical events (e.g., intervention, lab test) to predict patient decompensation.\n~\\citet{gao2020compose} utilize a multimodal inference model to jointly encode trial criteria text and patient EHR tabular data for patient-trial inference.\n~\\citet{huang2020multimodal} develop and compare different multimodal fusion architectures to classify Pulmonary Embolism (PE) cases.\n\\citet{ma2021distilling} and~\\citet{hoang2021aid} develop distillation frameworks to leverage the multimodal EHR data to enhance the prognosis.\n% LAVA~\\cite{an2019longitudinal} proposes a method for multiple types of healthcare data to proactively prevent high-risk attacks to their records.\nAlthough the methods above work well, one common drawback is that they can only handle samples with complete modalities.\nLimitations exist while modeling multimodal interactions with the presence of missing modalities.\n% However, these works cannot model multimodal interactions with the presence of missing \n\n\\noindent\\textbf{Methods for missing modalities.}\nCurrently, there have been research interests in handling missing modalities, which are mainly divided into two types: deleting incomplete samples or imputing missing modalities.\nFor the first type, FitRec~\\cite{ni2019modeling} performs workout profile forecasting based on multimodal user data, which discards samples with missing modalities.\n~\\citet{wang2020multimodal} propose a knowledge distillation framework on samples with complete modalities, while distilling the supplementary information from the incomplete ones.\nHowever, such methods can not handle the samples in need with missing modalities and have limitations to be applied in rigid demand domains like healthcare.\nBesides, such methods dramatically reduce training data and result in over-fitting of deep learning models~\\cite{chen2020hgmf,pan2021disease},\nespecially when there are many modalities and many different missing combination patterns\\footnote{e.g., five modalities can result in $2^5-1 = 31$ missing patterns.}.\n\nThe second type is generating the missing modalities at first~\\cite{pan2021disease,ma2021smil,cai2018deep}.\nHowever, the incompleteness of modalities leads to column-wise consecutive missingness of features, which makes traditional methods like matrix completion can not be used~\\cite{wang2020multimodal}.\nSome advanced generative methods such as autoencoders~\\cite{ngiam2011multimodal,li2020estimation} and generative adversarial networks (GAN)~\\cite{pan2021disease,cai2018deep} have been proposed. \nThese solutions, however, may introduce unwanted extra noise~\\cite{chen2020hgmf}.\nEspecially when the size of samples with complete modalities is small, yet the number of modalities is large, the modalities imputed by such methods may have a negative effect~\\cite{wang2020multimodal,enders2010applied}.\n% Moreover, while facing data with many modalities or missing patterns, the number of generators required is also large, which is difficult to train.\nMoreover, while facing data with many modalities or missing patterns, the number of generators required is also large, which is difficult to train.\n~\\citet{chen2020hgmf} propose a method to enable multimodal fusion of incomplete data and get good performance.\nHowever, the method is transductive and needs pre-training, indicating difficulty in applying it when a new sample comes.\n% Moreover, for patients with missing modalities, while finding the relationships between patients, it may be unstable because of the missingness.\nTherefore, in this paper, we propose a new inductive framework to tackle the above limitations in an end-to-end schema.\n\n\n"
            },
            "section 3": {
                "name": "Problem Formulation",
                "content": "\n\n%TOFIX: the0 ?\n\nIn this section, we define the input data and the modeling problem in this paper. \nBesides, the necessary notations used in the paper are listed in Table~\\ref{tab:notations} for ease of understanding.\n\n\\noindent\\textbf{Definition 1 (Patient multimodal EHR data).}\nIn multimodal EHR data, each patient can be represented as a collection of observations from multiple modalities (data sources), e.g., medical images, clinical notes, lab tests, etc. \nSuppose $M$ is the number of modalities, $N$ is the number of patients (samples), and let $n$ be the subscript referring to a specific patient, the patient multimodal EHR dataset can be denoted as: $\\mathbf{\\mathbb{X}} = \\{{\\mathbf{X}}_{n}\\}_{n=1}^N = \\{( \\mathbf{x}_{n}^{{1}}, \\mathbf{x}_{n}^{{2}}, ..., \\mathbf{x}_{n}^{{M}} )\\}_{n=1}^N$.\n\n\\noindent\\textbf{Definition 2 (Patient data with missing modalities).}\nFor a specific patient, as mentioned in Section~\\ref{sec:intro}, various clinical and social reasons cause the absence of some modalities.\nThus, the observed data of a patient are represented as: ${\\mathbf{X}}_{n} = \\{ \\mathbf{x}_{n}^{{1}}, \\mathbf{x}_{n}^{{2}}, ..., \\mathbf{x}_{n}^{{M^{'}}} \\}$, where $0 < M^{'} < M$.\nIt should be noted that, we used the most relaxed setting, i.e., the modality missingness is irregular across patients. \nIn all the training and test (validation) sets, each modality is potentially missing, but at least one modality is present for each patient.\n\n\n\n\\noindent\\textbf{Problem 1 (Disease diagnosis).}\nGiven a patient’s multimodal EHR data ${\\mathbf{X}}_{n}$ with some missing modalities, we formulate the disease diagnosis task as a \\textit{binary} or \\textit{multi-label} classification problem, which is diagnosing the disease $\\ \\mathbf{y} \\in \\{0,1\\}^{|C|}$ of the patient, where $|C|$ is the number of the unique number of disease categories. \n\n\n\nTable~\\ref{tab:notations} shows the notations used in the paper.\n \n"
            },
            "section 4": {
                "name": "Methodology",
                "content": "\n",
                "subsection 4.1": {
                    "name": "Overview",
                    "content": "\n\n\n\n\n\nFigure~\\ref{fig:framework} and Figure~\\ref{fig:framework2} show the architecture of \\mname. \nIt consists of two main sub-models.\nThe first one is used to compensate for the missing information in the latent space (Corresponding to Figure~\\ref{fig:framework}).\nThe other utilizes the processed representations to perform the clinical tasks (Corresponding to Figure~\\ref{fig:framework2}).\nSpecifically, \\mname includes the following detailed components:\n\n\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=2pt]\n    \\item The \\textit{Unimodal Representation Extraction} module maps the original input features of a patient in each modality to the latent space by encoders with various backbones.\n    The backbones are different due to different input modalities (Left part in Figure~\\ref{fig:framework}).\n    \n    \\item The \\textit{Similar Patients Discovery and Information Aggregation} module computes patient similarities %in the latent space \n    of each modality with learned task-guided deep kernels. The similarities induce patient graphs. With graph information propagation, the information from similar patients is aggregated (Middle and right parts in Figure~\\ref{fig:framework})\n  \n    \n    \\item The \\textit{Adaptive Modality Imputation} module imputes the missing modality in the latent space with the aggregated information, and fuses the existing modality and the auxiliary information to enhance the representation learning (Right part in Figure~\\ref{fig:framework}).\n    \n    \\item The \\textit{Multimodal Interaction Capture} module takes the intra- and inter-modality dynamics into consideration to perform the final clinical tasks  (Figure~\\ref{fig:framework2}).\n\\end{itemize}\n% The details are presented in the following subsections.\n\n"
                },
                "subsection 4.2": {
                    "name": "Unimodal Representation Extraction",
                    "content": "\nFor a specific patient $n$, it is hard to model the interactions among the raw data, since his/her data $\\mathbf{X}_{n}$ is high-dimensional and inconsistent with respect to different data structures in different modalities~\\cite{chen2020hgmf}.\nTherefore the unimodal representation extraction models are in need to extract the task-relevant feature latent representation in the latent space of each modality.\nHere, suppose $f_{m}\\left(\\cdot ; \\boldsymbol{\\Theta}_{m}\\right)$ be the modality $m$'s unimodal representation extraction model with learnable parameter $\\Theta_{m}$.\nFor modality $m$'s raw input, the corresponding latent representation can be obtained via:\n% \\useshortskip\n\\begin{equation}\n\\label{eq:hm}\n\\mathbf{h}^{m}_{n}=f_{m}\\left(\\mathbf{x}^{m}_{n} ; \\boldsymbol{\\Theta}_{m}\\right),\n\\end{equation}\nwhere $\\mathbf{h}^{m}_{n} \\in \\mathbb{R}^{N_{h}}$ and $N_{h}$ is the dimension of the representation for modality $m$.\nWe use the lowercase letter $\\mathbf{x}^{m}_{n}$ to denote a modality for a single patient $n$.\n% , thus omitting the subscript $n$ for simplicity.\n\nThree types of $f_{m}\\left(\\cdot ; \\boldsymbol{\\Theta}_{m}\\right)$ are taken into consideration in this paper: \n1) ResNet~\\cite{he2016deep} for embedding image modalities; \n2) Transformer Encoder~\\cite{vaswani2017attention} for embedding sequential modalities, such as time-series data like patient laboratory test, medication, and free texts like clinical notes; \nand \n3) multi-layer perceptron (MLP) for embedding vector-based modalities, such as demographic information.\nAs shown in the left part in Figure~\\ref{fig:framework}, the black boxes denote the missing information.\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Similar Patients Discovery and Information Aggregation",
                    "content": "\nThe above section describes how to deal with a single sample, and now we focus on a group of samples.\nGiven a batch of patients, the representations of modality $m$ for them are denoted as:\n% $H^{m} = [ h_{1}^{m}, h_{2}^{m}, \\,\\dots,  h_{B}^{m}]^{\\intercal} \\in\\mathbb{R}^{B \\times N_{h}}$,\n\\begin{equation}\\nonumber\n\\label{eq:batchH}\n\\mathbf{H}^{m} = [ \\mathbf{h}_{1}^{m}, \\mathbf{h}_{2}^{m}, \\,\\dots,  \\mathbf{h}_{B}^{m}]^{\\intercal} \\in\\mathbb{R}^{B \\times N_{h}},\n\\end{equation}\n\nwhere $B$ is the batch size. \n% Since the representations of vector-based modalities and image modalities are vectors, for a batch of patients, they can be easily aggregated as matrices $\\mathbf{H}^{\\text{vec}}$ and $\\mathbf{H}^{\\text{vis}}$.\nFor sequential modalities, we select the representation of the last timestamp as the one.\n\nFor a specific patient with missing modalities, as shown in Figure~\\ref{fig:med_missing_modal}, the representations from the missing modalities cannot be obtained, which results in a lack of modality information.\nAs discussed in Section~\\ref{sec:intro}, we can compare similarities of local patient data and impute the missing modalities of the patient by referring to the uncensored modalities of locally similar patients.\n% utilize the complete modalities of the patient and other patients who also have these modalities, to find similar patients in the latent spaces of these modalities.\n% In this way, we can estimate the information of missing modality via the information from the similar patients.\n% 对应challenge2？\nThere is thus a problem with similar patient discovery: What metric(s) should be used to model the similarity relation?\nIn practical applications, one often adopted strategy is to try different kinds of similarity measures on the learned representations in each modality space, such as Cosine, Euclidean distance, and so on, and then select the best similarity measure~\\cite{kang2016top}.\nHowever, this approach is time-consuming, and even if one tries different similarity metrics, it is found that those traditional similarity metrics often fail to consider the local environment of data points, and may learn incomplete and inaccurate relationships.\nIn this case, it is unlikely that the traditional similarity function will be adequate to capture the local manifold structure precisely~\\cite{kang2017kernel}. \nMoreover, complex relationships such as higher-order statistics are failed to capture in that way~\\cite{kang2017kernel}.\n\nTo this end, we extend this idea to kernel spaces and select the RBF kernel.\nGiven two samples $\\mathbf{h}_{i}^{m}$ and $\\mathbf{h}_{j}^{m}$, the similarity calculated from the RBF kernel is defined as:\n\\begin{equation}\n\\label{eq:GKernel}\n    k(\\mathbf{h}_{i}^{m}, \\mathbf{h}_{j}^{m})=\\exp (-\\frac{\\|\\mathbf{h}_{i}^{m}-\\mathbf{h}_{j}^{m}\\|_{2}^{2}}{2 \\sigma^{2}}),\n\\end{equation}\nwhere $\\sigma$ is the bandwidth to control the extent to which similarity of small distances is emphasized over large distances. \nFollowing~\\cite{chu2020distance}, we set $\\sigma$ as a fraction of the mean distance between examples.\nExpanding the exponential via Taylor series, we can see that the RBF kernel implies an infinite dimension mapping, capturing the higher-order statistics.\n\n\nFurthermore, as mentioned in Challenge 2, since the data are in multiple modalities, \\mname is required to calculate similarities in each modality space and associate the connotation of similarity with clinical tasks.\nThus, a task-guided modality-semantic-adaptive similarity metric is needed.\nWe extend the standard RBF kernel to deep kernel~\\cite{liu2020learning} to build an adaptive kernel with a learnable network to fit the representations in a modality data-driven way.\nSpecifically, the kernel is denoted as:\n\\begin{equation}\n\\label{eq:}\n    k_{\\omega_{m}}(\\mathbf{h}_{i}^{m}, \\mathbf{h}_{j}^{m})=[(1-\\delta_{m}) k(\\phi_{\\omega_{m}}(\\mathbf{h}_{i}^{m}), \\phi_{\\omega_{m}}(\\mathbf{h}_{j}^{m}))+\\delta_{m}] q(\\mathbf{h}_{i}^{m}, \\mathbf{h}_{j}^{m}),\n\\end{equation}\nwhere $\\phi_{\\omega_{m}}$ is a network with parameters $\\omega_{m}$ for modality $m$. $k$ and $q$ are different RBF kernels with different $\\sigma$.\nThe $\\delta_{m} \\in (0, 1)$ is a learnable \\textit{safeguard} to preventing the learned kernel from going extremely far-away from the right direction.\n\nNow, back to the batch of patient representations, the pairwise similarities with respect to each modality are calculated as (i.e., the No.1 arrow in Figure~\\ref{fig:framework}):\n\\begin{equation}\n\\label{eq:pim}\n\\begin{split}\n    \\Pi^{m }& =k_{\\omega_{m}}(\\mathbf{H}^{m} , {\\mathbf{H}^{m}}), \\\\\n    % \\Pi^{\\text{demo} }& =k_{\\omega}({\\mathcal{H^{\\text{demo}}} }, {\\mathcal{H^{\\text{demo}}}}), \\\\\n    % \\Pi^{\\text{vis} }& =k_{\\omega}({\\mathcal{H^{\\text{vis}}} }, {\\mathcal{H^{\\text{vis}}}}), \\\\\n    % \\Pi^{\\text{ts} }& =k_{\\omega}({\\mathcal{H^{\\text{ts}}} }, {\\mathcal{H^{\\text{ts}}}}),\n\\end{split}\n\\end{equation}\nwhere $\\Pi^{m} \\in\\mathbb{R}^{B\\times B}$ is the patient similarity matrix for modality $m$.\nMeanwhile, to ensure the stability of the similarity measure and prevent collapse, we restrict the norm of the difference between the deep representation and the original representation as the optimization objective:\n\\begin{equation}\n\\label{eq:norm_loss}\n    \\mathcal{L}_{\\text{stab}} = \n\\sum_{m=1}^{M} \\left|\\|\\phi_{\\omega_{m}}(\\mathbf{H}^{m})\\|_F - \\|\\mathbf{H}^{m}\\|_F \\right|,\n\\end{equation}\nwhere the outer $\\left|\\cdot \\right|$ means the absolute value, and the inner $\\|\\cdot\\|_F$ is the Frobenius norm.\n\nBecause of the characteristic of the kernel method, $\\Pi^{m}$ is a totally positive symmetric matrix, and each cell $\\Pi^{m}_{i,j}$ in $\\Pi^{m}$ ranges from 0 to 1, denoting the similarity between the $i$-th patient and $j$-th patient for modality $m$.\nHowever, in this way, all the patients are considered similar since the positive similarity. \nThere should be dissimilar ones to be filtered out.\nA straightforward way is setting a threshold, and the similarities below the threshold are considered dissimilar.\nNevertheless, in the early training phase, we notice that when the model is not convergent and the representations are not fully learned, all similarities are unstable.\nThe threshold may filter out some similar patients and lead to inferior performance.\nMoreover, the determination of the value of the threshold is not trivial.\nThus, we utilize a more flexible learnable threshold here.\nBy comprehensively considering similarity from each modality, the filtered similarity matrix can be obtained as:\n\\begin{equation}\n\\label{eq:pi}\n\\begin{split}\n   \\tilde{\\Pi} =  \\frac{\\sum_{1 }^{M} \\Pi^{m} \\cdot  \\text{mask}^{m}  }{\\sum_{1 }^{M} \\text{mask}^{m} + \\epsilon  } \n\\end{split}\n\\end{equation}\n\\begin{equation}\n\\label{eq:piij}\n\\begin{split}\n   \\tilde{\\Pi}_{i,j}  = \\begin{cases}\n  & \\tilde{\\Pi}_{i,j}   \\ \\  \\text{ if } \\tilde{\\Pi}_{i,j} >  \\Lambda   \\\\\n  & 0  \\ \\ \\ \\ \\ \\ \\text{ if  } \\tilde{\\Pi}_{i,j} \\le \\Lambda  \n\\end{cases}\n\\end{split}\n\\end{equation}\nwhere $\\Lambda $ is the learnable threshold to filter out dissimilar pairs and $\\epsilon$ is used to prevent unstable division by zero.\n$\\text{mask}^{m}  \\in\\mathbb{R}^{B\\times B}$ is the mask matrix of booleans that determines whether each element of the associated value is valid or not (i.e., similarity for missing modality is invalid).\nFor example, in modality $m$, if both the data of the $i$-th sample and $j$-th sample exist, $\\text{mask}^{m}_{i,j} = 1$, and otherwise $\\text{mask}^{m}_{i,j} = 0$, which masks the invalid similarity cell.\n\nOur aim is to impute the modality-missing sample by incorporating auxiliary information from the similar patients.\nThus, to aggregate the information from the similar ones, we formulate the batch of patients' representations as a graph in each modality, with the similarity matrix $\\tilde{\\Pi}$ as the graph adjacency matrix (i.e., the No.2 arrow in Figure~\\ref{fig:framework}).\nThen the graph convolutional layers (GCN)~\\cite{kipf2016semi} are applied to enhance the representation learning by leveraging the structural information:\n\\begin{equation}\n\\label{eq:graphconv}\n\\begin{split}\n   {\\hat{\\mathbf{H}}^{m}}  =  [ \\hat{\\mathbf{h}}_{1}^{m}, \\hat{\\mathbf{h}}_{2}^{m} & , \\,\\dots ,  \\hat{\\mathbf{h}}_{B}^{m}]^{\\intercal}  = \\operatorname{GCN}({\\mathbf{H}}^{m}, \\tilde{\\Pi}) \\\\\n   & = \\operatorname{ReLU}(\\tilde{\\Pi} \\  \\operatorname{ReLU}(\\tilde{\\Pi} {\\mathbf{H}}^{m} W^0)W^1 ) ,\n\\end{split}\n\\end{equation}\nwhere ${\\hat{\\mathbf{H}}^{m}}$ is the aggregated auxiliary information from similar patients in the space of modality $m$.\n$W^0$ and $W^1\\in\\mathbb{R}^{N_{h} \\times N_{h}}$ are the projection matrices.\nWe ignore the bias term here and after.\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Adaptive Modality Imputation",
                    "content": "\n\nNow we obtain two different representations for the batch of patients in each modality: $\\mathbf{H}^{m}$ and ${\\hat{\\mathbf{H}}^{m}}$. \nThe former focuses on the patients themselves for modality $m$, while the latter refers to the information aggregated from similar patients.\nFor a specific patient $i$, if the data of modality $m$ are missing, we can directly impute the representation with the auxiliary information aggregated from similar patients.\n% here 1.25\nWhile for a patient whose modality $m$ is complete, such auxiliary information can also be fused into the original representation, making the representation smoother to reduce noise, thus enhancing the representation learning.\n% can also enhance representation learning.\nHere, we use an attention fusion to adaptively extract the proper amount of information from them (i.e., ${\\mathbf{H}}^{m}$ and ${\\hat{{\\mathbf{H}}}^{m}}$).\nSpecifically, two weights $\\alpha^{m}, \\beta^{m} \\in\\mathbb{R}^{B \\times 1}$ are introduced to determine the importance of the above two representations, which are obtained by fully connected layers:\n\\begin{equation}\n\\label{eq:alpha}\n    \\alpha^{m}=\\operatorname{Sigmoid}(  {\\mathbf{H}}^{m} W_{o}), \\beta^{m}=\\operatorname{Sigmoid}({ \\hat{{\\mathbf{H}}}^{m}} W_{s} ),\n\\end{equation}\n% \\begin{equation}\n% \\label{eq:beta}\n%     \\beta^{m}=\\operatorname{Sigmoid}({ \\hat{{\\mathbf{H}}}^{m}} W_{s} ),\n% \\end{equation}\nwhere $ W_{o}, W_{s} \\in\\mathbb{R}^{ N_{h}  \\times 1}$ are the weight matrices.\n$\\alpha$ and $\\beta$ indicate the importances of self-information and information of similar patients. \nWe add a constraint $\\alpha + \\beta = 1$ by calculating $ \\alpha = \\frac{\\alpha}{\\alpha + \\beta},$ $\\beta = 1 - \\alpha.$\n% To keep $\\alpha + \\beta = 1$, a constraint is applied to them:\n% \\begin{equation}\n% \\label{eq:sum1}\n%     \\alpha = \\frac{\\alpha}{\\alpha + \\beta},\n% \\end{equation}\n% \\begin{equation}\n% \\label{eq:sum2}\n%     \\beta = 1 - \\alpha.\n% \\end{equation}\nThe final imputed and enhanced representations can be obtained as (i.e., the No.3 arrow in Figure~\\ref{fig:framework}):\n% \\begin{equation}\n% \\label{eq:finalrep}\n%     \\bm S = \\alpha \\cdot \\bm{{H}}  + \\beta \\cdot \\bm{\\hat{H}}.\n% \\end{equation}\n% imputation\n\\begin{equation}\n\\label{eq:impute}\n\\mathbf{h}_{i}^{m} = \n\\begin{cases}\n \\qquad\\quad\\ \\  \\hat{\\mathbf{h}}_{i}^{m}&  \\text{if modality } m \\text{ of sample }  i \\text{ is missing}  \\\\\n \\alpha^{m}_{i} \\cdot \\mathbf{h}_{i}^{m} + \\beta^{m}_{i} \\cdot  \\hat{\\mathbf{h}}_{i}^{m} & \\text{otherwise}\n\n\\end{cases}\n\\end{equation}\nwhere $\\mathbf{h}_{i}^{m} $ and  $\\hat{\\mathbf{h}}_{i}^{m} $ are the $i$-th sample of ${\\mathbf{H}}^{m}$ and ${ \\hat{{\\mathbf{H}}}^{m}}$, respectively.\n\n\n% MM transformer\n"
                },
                "subsection 4.5": {
                    "name": "Multimodal Interaction Capture",
                    "content": "\nBack to a specific patient, so far, the representations of the missing modalities have been imputed\n% and the representations of complete modalities have been enhanced \nthrough the above sections.\nThese representations are used to perform the clinical tasks.\nThus, we need to consider complex correlations among multimodal EHR, including intra-correlations within each modality and inter-correlations between modalities.\nInspired by~\\citet{kim2021vilt,akbari2021vatt}, a context-aware multimodal interaction capture is built.\nSpecifically, for sequential modalities, the internal positional encoding is added: $\\bar{\\mathbf{H}}^{\\text{seq}} = \\left[ \\mathbf{h}^{\\text{seq}}_{1}, \\mathbf{h}^{\\text{seq}}_{2}, \\,\\dots , \\mathbf{h}^{\\text{seq}}_{N_{\\text{seq}}}  \\right]  + \\text{PE}^{\\text{seq}},$\n% \\begin{equation}\n% \\bar{\\mathbf{H}}^{\\text{seq}} = \\left[ \\mathbf{h}^{\\text{seq}}_{1}, \\mathbf{h}^{\\text{seq}}_{2}, \\,\\dots , \\mathbf{h}^{\\text{seq}}_{N_{\\text{seq}}}  \\right]  + \\text{PE}^{\\text{seq}},\n% \\end{equation}\nwhere $\\text{PE}^{\\text{seq}}$ is the positional encoding for sequential modality $\\text{seq}$ and $N_{\\text{seq}}$ is the length of the sequence.\nNext, the representations are added with the corresponding modality type embeddings and concatenated to form the input:\n\\begin{equation}\n\\label{eq:mmbegin}\n\\mathbf{z}^{0} = [\\bar{\\mathbf{H}}^{1}+\\text{TE}^{1}; \\bar{\\mathbf{H}}^{2}+\\text{TE}^{2} ;... ; \\bar{\\mathbf{H}}^{M}+\\text{TE}^{M}],\n\\end{equation}\nwhere $\\text{TE}^{m}$ is the corresponding type embedding to identify each modality.\nAnd the multimodal interactions are captured through:\n\\begin{equation}\n\\label{eq:mmend}\n\\begin{split}\n% \\vspace*{-1mm}\n   \\tilde{\\mathbf{z}}^{l} & = \\operatorname{LayerNorm}({\\mathbf{z}}^{l-1} + \\operatorname{MHSA}({\\mathbf{z}}^{l-1})), \\\\\n%   \\vspace*{-1mm}\n   \\mathbf{z}^{l}  &= \\operatorname{LayerNorm}(\\tilde{\\mathbf{z}}^{l} + \\operatorname{FFN}(\\tilde{\\mathbf{z}}^{l})), \\\\\n%   \\vspace*{-1mm}\n\\end{split}\n\\end{equation}\nwhere $l = 1, \\ ..., L $ refers to the number of such stacked layers.\nMHSA refers to the Multi-Head Self-Attention~\\cite{vaswani2017attention}, FFN refers to a feed-forward network and LayerNorm is the layer normalization ~\\cite{ba2016layer}.\n% \\subsection{Final Prediction}\nThe predictor is built via:\n\\begin{equation}\n\\label{eq:predict1}\n% \\vspace*{-1mm}\n    \\hat{\\mathbf{y}}_i = \\operatorname{Sigmoid}( \\mathbf{z}^{L}_{0}  \\, W_{final}),\n    % \\vspace*{-1mm}\n\\end{equation}\nwhere $ W_{final} \\in\\mathbb{R}^{ N_{h} \\times 1}$ is the weight matrix.\nThe Sigmoid function is used to turn the output into the probability.\nIn this case, the cross-entropy loss is used as the prediction loss function:\n\\begin{equation}\n% \\label{eq:loss}\n% \\vspace*{-1mm}\n\\mathcal{L}_{\\text{pre}} = -\\frac{1}{B} \\sum_{i=1}^{B}(\\mathbf{y}_i^{\\intercal} \\operatorname{log}(\\hat{\\mathbf{y}}_i) + (1-\\mathbf{y}_i )^{\\intercal} \\operatorname{log}(1-\\hat{\\mathbf{y}}_i)),\n% \\vspace*{-1mm}\n\\end{equation}\nwhere $B$ is the batch size. \n$\\hat{\\mathbf{y}}_i \\in [0, 1]^{|C|} $ is the predicted probability, and ${\\mathbf{y}}_i \\in \\{0, 1\\}^{|C|} $ is the ground truth.\nThe overall loss function is:\n\\begin{equation}\n\\label{eq:loss}\n% \\vspace*{-1mm}\n\\mathcal{L} = \\mathcal{L}_{\\text{pre}} + \\lambda \\mathcal{L}_{\\text{stab}},\n% \\vspace*{-1mm}\n\\end{equation}\nwhere $\\lambda$ is the hyperparameter to control the loss.\n% In clinical practice, for a new case, it is input within a data batch with old cases.\nFor ease of understanding, we summarize \\mname in Algorithm~\\ref{alg} in Appendix.\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiment",
                "content": "\n\n\nWe evaluate \\mname on the following datasets: Ocular Disease Intelligent Recognition (ODIR) Dataset and Ophthalmic Vitrectomy (OV) Dataset. The model code is provided in~\\footnote{\\url{https://github.com/choczhang/M3Care}}.\n\n",
                "subsection 5.1": {
                    "name": "Data Description and Task Formulation",
                    "content": "\n\nWe use the following datasets and tasks to evaluate our model.\n\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=3pt]\n\n\\item\n\\textbf{Ocular Disease Intelligent Recognition (ODIR) Dataset} comes from an ophthalmic database, which is meant to represent real-life set of patients collected from hospitals~\\cite{li2021benchmark}.\n3,500 patients are extracted to construct this dataset to diagnose ocular diseases.\nThis dataset contains the following modalities (incomplete modalities exist): demographic information, clinical text for both eyes, and fundus images for both eyes.\nThe detailed statistics are presented in Table~\\ref{tab:dataset} in Appendix.\n\n\n\n\\textbf{Task.} The ocular diseases diagnosis task on this dataset is defined as a multi-label classification task.\nFollowing existing works~\\cite{bai2019improving,yu2020experimental}, we assess the performance using micro-averaged of the area under the receiver operating characteristic curve (i.e., micro-AUC), macro-AUC, and the average test loss value.\nWe divide the dataset into the training set, validation set, and test set with a proportion of 0.8\\,:\\,0.1:\\,0.1, and report the performance with the standard deviation of bootstrapping for 1,000 times.\n% ~\\footnote{Data are now publicly available at~\\url{https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k} }\n\\item\n\\textbf{Ophthalmic Vitrectomy (OV) Dataset} comes from an ophthalmic hospital\\footnote{This study was approved by the Research Ethical Committee.}.\nIn clinical practice, after vitrectomy, the intraocular pressure (IOP) may increase abnormally. \nThis symptom cannot be predicted by doctors. \nWe collect 832 patients to predict whether the IOP will increase abnormally. \n% During and after data collection and analysis, the authors could not identify individual participants and centers as patients’ names were replaced by ID.\nThis dataset contains six modalities (incomplete modalities exist): demographic,  clinical notes, medications, admission records, discharge records, and surgical consumables.\nThe detailed statistics are presented in Table~\\ref{tab:dataset}.\n\n\n\n\n\\textbf{Task.} The task on the dataset are %defined as \nbinary classification tasks.\nFollowing existing works~\\cite{zhang2021grasp,hoang2021aid}, we assess performance using the area under the precision-recall curve (AUPRC), the area under the ROC curve (AUROC), and accuracy (ACC).\n% Since the labels are balanced in MIMIC-III dataset, we replace ACC with F1-score while reporting the performance.\nAUPRC is the most informative and primary evaluation metric, especially while dealing with skewed real-world data~\\cite{davis2006relationship,choi2018mime,zhang2021grasp}.\nDue to the size of the dataset, we employ 10-fold cross-validation to assure the consistency of the performance and report the average performance with standard deviations.\n\n\n\n\n\n\n\n\n\\end{itemize}\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Experimental Setup and Baselines",
                    "content": "\n\n\nTo conduct the experiment, we use the Adam optimization algorithm in Pytorch 1.5.1. \nMore \\hyperref[sec:detailed]{details are in the Appendix}.\nWe include these state-of-the-art models as our baseline models:\n\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=1pt]\n\n\\item \\textbf{MFN}~\\cite{zadeh2018memory} captures view-specific and cross-view interactions, and summarizes them with a multi-view gated memory module.\n\n\\item \\textbf{MulT}~\\cite{tsai2019multimodal} utilizes directional pairwise cross-modal transformers to attend to interactions between multimodal data.\n\n\\item \\textbf{ViLT}~\\cite{kim2021vilt} commissions the transformer module to extract and process all the multimodal features simultaneously.\n\n% \\item \\textbf{VATT}~\\cite{akbari2021vatt} takes raw signals as inputs and extracts multimodal representations and we modified it to a supervised version.\n\n\\item \\textbf{CM-AEs}~\\cite{ngiam2011multimodal}: The cross-modal autoencoders, which generate missing modalities first and make predictions.\n\n% \\item \\textbf{CPM-Nets}~\\cite{zhang2019cpm} handles the missing modalities by learning versatile representations, while the classification schema in turn enhances the separability for latent representations.\n\n\\item \\textbf{SMIL}~\\cite{ma2021smil} approximates the missing modality using a weighted sum of manually defined modality priors learned from the dataset. \n\n\\item \\textbf{HGMF}~\\cite{chen2020hgmf} fuses incomplete multimodal data within a heterogeneous graph structure, and we modify it to an inductive version. \n\n% \\item \\textbf{Wiki2Prop}~\\cite{luggen2021wiki2prop} combines the multiple modalities with missing data to produce a confidence vector and make predictions.\n\n\n% Some of their embedding layers of raw multimodal data are a little bit out of date (e.g., GRU and Densenet). \n% Thus, to perform a fair comparison, we upgrade their embedding layers to the same ones as ours. \n\n\n\n\\end{itemize}\n% To fairly compare different approaches, our principle for hyper-parameter settings of each baseline model is as follows:\n% If the hyper-parameter setting is available in the original paper, we use the recommended setting. \n% Otherwise, the hyper-parameters of the baseline models are fine-tuned by the grid-searching strategy.\n\nThe following ablation studies are also conducted:\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=1pt]\n    \\item \\textbf{$\\mname_{1-}$} does not use the task-guided deep kernels of each modality. It directly calculates similarity via cosine similarity.\n    \\item \\textbf{$\\mname_{2-}$}  does not consist of the Information Aggregation and the Adaptive Modality Imputation module. \n    It directly computes the mean similarity from each modality and approximates the missing-modality representations via the similar patients.\n\\end{itemize}\n\nIt should be noted that some of the above models' embedding networks of raw data are a little bit weak. \nThus, to perform a fair comparison, we upgrade their embedding layers to the same ones as ours (e.g., Transformer Encoder~\\cite{vaswani2017attention} and ResNet18~\\cite{he2016deep}) and we do not include any pre-trained parameters. \n\n\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Experimental Results",
                    "content": "\n\n\n\n\n\n\n% \\begin{table}[]\n% % \\small \n%   \\centering\n%   \\caption{Results on the OV Dataset}\n%   \\label{tab:result_OV}\n\n\n% \\begin{tabular}{cccc}\n% \\hline\n\n%  &  \\multicolumn{3}{c}{\\textbf{OV Dataset} (Binary Classification) }\\\\\n\n%  \\textbf{Methods}& \\textbf{AUROC$\\ \\uparrow$}& \\textbf{AUPRC$\\ \\uparrow$}& \\textbf{ACC$\\ \\uparrow$}  \\\\ \n% \\hline\n\n% CM-AEs~\\cite{ngiam2011multimodal} & 0.0000\\,(0.000) & 0.0000\\,(0.000) & 0.0000\\,(0.000) \\\\ \n% MFN~\\cite{zadeh2018memory}    & 0.6426\\,(0.032) & 0.5729\\,(0.039) & 0.6766\\,(0.032) \\\\ \n% MulT~\\cite{tsai2019multimodal}  & 0.6502\\,(0.041) & 0.5920\\,(0.047) & 0.6834\\,(0.030) \\\\ \n% ViLT~\\cite{kim2021vilt}   & 0.7044\\,(0.046) & 0.6121\\,(0.058) & 0.6875\\,(0.033) \\\\ \n% CPM-Nets~\\cite{zhang2019cpm}   & 0.6580\\,(0.034) & 0.5828\\,(0.055) & 0.6658\\,(0.030) \\\\ \n% SMIL~\\cite{ma2021smil}    & 0.6694\\,(0.033) & 0.5808\\,(0.050) & 0.6719\\,(0.035) \\\\ \n% Wiki2Prop~\\cite{luggen2021wiki2prop}   & 0.6571\\,(0.035) & 0.5772\\,(0.051) & 0.6671\\,(0.030) \\\\ \n% \\hline\n%  $\\mname_{1-}$ & 0.7472\\,(0.052) & 0.6849\\,(0.074) & 0.7080\\,(0.064) \\\\ \n%   $\\mname_{2-}$  & 0.7562\\,(0.057) & 0.7110\\,(0.044) & 0.7234\\,(0.072) \\\\ \n% \\hline\n     \n%  \\textbf{ $\\mname$}   & \\textbf{0.8098}\\,(0.049) & \\textbf{0.7749}\\,(0.065) & \\textbf{0.7438}\\,(0.058) \\\\ % ok\n% \\hline\n% \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}[]\n% \\small \n% % \\footnotesize\n%   \\centering\n%   \\caption{Results on the MIMIC-III Dataset}\n%   \\label{tab:result_mimic}\n% % \\vspace*{-4mm}\n\n% \\begin{tabular}{cccc}\n% \\hline\n\n%  &  \\multicolumn{3}{c}{\\textbf{MIMIC-III Dataset} (Binary Classification) }\\\\\n\n%  \\textbf{Methods}& \\textbf{AUPRC$\\ \\uparrow$}& \\textbf{AUROC$\\ \\uparrow$}& \\textbf{F1-score$\\ \\uparrow$}  \\\\ \n% \\hline\n\n\n% MFN~\\cite{zadeh2018memory}    & 0.4583\\,(0.020) & 0.8364\\,(0.10) & 0.3989\\,(0.026) \\\\ \n% MulT~\\cite{tsai2019multimodal}  & 0.4748\\,(0.026) & 0.8387\\,(0.010) & 0.3860\\,(0.024) \\\\ \n% ViLT~\\cite{kim2021vilt}   & 0.4474\\,(0.027) & 0.8390\\,(0.010) & 0.4138\\,(0.025) \\\\\n% VATT~\\cite{akbari2021vatt}   & 0.4501\\,(0.027) & 0.8333\\,(0.011) & 0.4092\\,(0.025) \\\\\n% CM-AEs~\\cite{ngiam2011multimodal} & 0.4653\\,(0.027) & 0.8422\\,(0.010) & 0.4099\\,(0.025) \\\\ \n% % CPM-Nets~\\cite{zhang2019cpm}   & 0.4733\\,(0.026) & 0.8423\\,(0.010) & 0.4006\\,(0.025) \\\\ \n% SMIL~\\cite{ma2021smil}    & 0.4517\\,(0.026) & 0.8401\\,(0.009) & 0.4008\\,(0.025) \\\\ \n\n% HGMF~\\cite{chen2020hgmf} & 0.4651\\,(0.025) & 0.8413\\,(0.010) & 0.4119\\,(0.024) \\\\\n% \\hline\n%  $\\mname_{1-}$  & 0.4726\\,(0.026) & 0.8375\\,(0.010) & 0.4122\\,(0.025) \\\\ \n%  $\\mname_{2-}$   & 0.4753\\,(0.026) & 0.8383\\,(0.010) & 0.4214\\,(0.024) \\\\ \n% \\hline\n     \n%  \\textbf{ $\\mname$}   & \\textbf{0.4901}$^{**}$\\,(0.026) & \\textbf{0.8445}$^{*}$\\,(0.010) & \\textbf{0.4608}$^{**}$\\,(0.023) \\\\ % ok\n% \\hline\n% \\end{tabular}\n% \\end{table}\n\n\n% \\begin{figure*}[!htbp]\n%   \\centering\n%   \\subfigure[Concat] {\\includegraphics[width=0.25\\columnwidth]{figs/concat_cm_all.pdf}}\n%   \\subfigure[MFN~\\cite{zadeh2018memory}] {\\includegraphics[width=0.25\\columnwidth]{figs/icml_cm_all.pdf}}\n%   \\subfigure[MulT~\\cite{tsai2019multimodal}] {\\includegraphics[width=0.25\\columnwidth]{figs/mult_cm_all.pdf}}\n%   \\subfigure[ViLT~\\cite{kim2021vilt}] {\\includegraphics[width=0.25\\columnwidth]{figs/smil_cm_all.pdf}}\n%   \\subfigure[CPM-Nets~\\cite{zhang2019cpm} ] {\\includegraphics[width=0.25\\columnwidth]{figs/mfn_cm_all.pdf}}\n%   \\subfigure[SMIL~\\cite{ma2021smil} ] {\\includegraphics[width=0.25\\columnwidth]{figs/cpm_cm_all.pdf}}\n%   \\subfigure[Wiki2Prop~\\cite{luggen2021wiki2prop}] {\\includegraphics[width=0.25\\columnwidth]{figs/wiki_cm_all.pdf}}\n%   \\subfigure[\\mname] {\\includegraphics[width=0.25\\columnwidth]{figs/m4_cm_all.pdf}}\n% %   \\subfigure[\\mname] {\\includegraphics[width=0.48\\columnwidth]{figs/concat_cm_all.pdf}}\n  \n\n%   \\caption{Confusion matrices of all methods on the fixed test set on OV Dataset. }\n%   \\label{fig:conf_mat_comb}\n% \\end{figure*} \n\n% 1. 我们做了什么实验，展示在哪几个table/figure里。\n% 结果充分证明了我们方法，在不同的数据集上、处理不同类型的multivariate time series data、解决不同医疗预测任务时deal with / tackle with various clinical prediction tasks，相比已有sota均有显著提升。\n\n% 2. 首先在covid-19数据集上，模型处理icu数据，which is 74维的稀疏lab tests和static demographic information，我们的方法相比已有sota（即t-lstm，上面的baseline，不是变体），显著提升，这体现在mse ( relatively百分比, absolutely),kappa ( relatively, absolutely)。\n% 【如mse这种，提升巨大，而且甚至相比变体也提升巨大的，要把提升的具体值、相对百分比，都强调出来。但是比如mad这种，有时候还不如变体高。就不用说的过于详细了】。\n% besides，值得一提的是，covid-19数据集仅包含400位患者的时序数据。在一个小数据集上，concares学习效果良好。表明our proposed model能有效针对突发疫情，进行lengh of stay预测，将有助于在大量患者 overwhelm clinic机构时，帮助医生合理分配医疗资源，improve the clinical efficiency.\n\n% 3. 在physionet数据集上，模型处理34维lab tests，相比已有sota（即multichannel或者tlstm），在auprc上显著提升了（absolutely值 relatively百分比），minse显著提升。\n\n% 4. 在mimiciii数据集上，模型处理xx维one-hot编码的categorical 型vital signs，相比已有方法仍然有稳定提升。\n\n% 5. 强调变体，因为concares完全体比xx变体强，则证明了什么变体的有效性。\n\n% 6. 强调比其他可解释性方法，如retain的优势，我们有性能优势，而且还能带来可解释性。但是retain之类的 blabla 批评。\n\n% To verify the effectiveness of \\mname, we conduct the experiments on three real-world datasets: ocular disease intelligent recognition (ODIR) Dataset and the Ophthalmic Vitrectomy (OV) Dataset. \n% two multi-center online EHR datasets: the Ocular Disease Intelligent Recognition (ODIR) Dataset and the Ophthalmic Vitrectomy (OV) Dataset. \nAs shown in Table~\\ref{tab:result_ODIR} and \\ref{tab:result_OV}, we can see that \\mname can outperform all the baselines in terms of different evaluation metrics\\footnote{${}^{**}:p<0.01$, ${}^{*}:p<0.05$}.\n\n\n\n\n\n\n\n\nSpecifically, on the ODIR Dataset, the number in () denotes the standard deviation of bootstrapping for 1,000 times.\nThe results show that, compared with the best baseline method, \\mname achieves relative improvements of 4.9\\% in micro-AUC.\nOn the OV Dataset, the number in () denotes the standard deviation of 10-fold cross-validation.\nWe can see that, compared with the best baseline method, \\mname achieves relative improvements of 6.1\\% in AUPRC and 6.0\\% in AUROC.\nAmong these baseline methods, some ones like CM-AEs~\\cite{ngiam2011multimodal},  SMIL~\\cite{ma2021smil}, HGMF~\\cite{chen2020hgmf} use various mechanisms to handle the missing modalities, and thus they achieve relative higher performance.\nHowever, the performance boost demonstrates the effectiveness of \\mname.\nBesides, it is worth mentioning that the OV Dataset only contains the multimodal EHR data of 832 patients yet has six modalities, demonstrating that \\mname performs well on a small dataset while the number of modalities is large, which is suitable for the real-world scenario, where data has a large number of modalities or missing patterns.\n\nThe superior performance of \\mname than the $\\mname_{1-}$ (i.e., calculating similarity via cosine similarity) verifies the efficacy of the task-guided deep kernels. \nMoreover, \\mname outperforms $\\mname_{2-}$, which demonstrates the superiority of the Information Aggregation and the Adaptive Modality Imputation module.\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Further Analysis",
                    "content": "\n\nWe conduct several further experiments.\nDue to the limitation of pages, some of the experiments are in the Appendix.\n\n\n\n",
                    "subsubsection 5.4.1": {
                        "name": "Clinical implications",
                        "content": " \\label{sec:cli_imp}\n\n% \\begin{figure}[]\n%   \\centering\n%   \\includegraphics[width=0.8\\columnwidth]{figs/case_atten49.pdf}\n%   \\caption{Attention weight visualization for a patient with abnormally increased intraocular pressure in the test set of OV dataset.}\n%   \\label{fig:case}\n% \\end{figure}\n\n\n\nTo intuitively show the implication of \\mname, we visualize the attention weights of the prediction process.\nDue to the limitation of pages, we report two cases in the test set here.\nAs shown in Figure~\\ref{fig:case}, the intraocular pressure (IOP) of the two patients increase abnormally and \\mname successfully predicts the outcome.\nThe rows and columns show the Query and Key multimodal records, which are the abbreviations of each modality, i.e., demographic information, clinical notes, medications, admission records, discharge records, and surgical consumables, respectively.\n\nWe notice that \\mname gives strong focus on $\\text{Med}_0$ and $\\text{Med}_2$  (i.e., the first and third medications) of patient $a$, and $\\text{Med}_0$ of patient $b$.\nIn all these medications, the patients received the two drugs: \\textit{Tropicamide Phenylephrine Eye Drops (Mydrin-P)} and \\textit{Prednisolone Acetate Ophthalmic Suspension (Pred Forte)}.\nThese drugs are used for ophthalmologic examinations, prior to ocular surgery~\\cite{Mydrin} or treat eye swelling caused by allergy, infection, injury, or other conditions~\\cite{PRED_FORTE}.\nOur model discovers that these two drugs may have a strong relationship with the abnormally increased intraocular pressure.\nThis is highly consistent with medical literature~\\cite{kim2012changes,atalay2015change,matossian2020impact,rajendrababu2021incidence} and clinician experience, which confirm that the two drugs can lead to adverse reactions like elevation of IOP and should be used with caution for specific patients in clinical practice.\n\n% Adverse Reactions\n% Tropicamide Phenylephrine Eye Drops (Mydrin-P)\n% Indicated for ophthalmologic examinations like during ophthalmoscopy, slit-lamp examination, retinal photography, prior to ocular surgery & as an adjunct in the treatment of anterior uveitis.~\\cite{Mydrin}\n% 1. Changes in intraocular pressure after pharmacologic pupil dilation\n% 眼压升高 The elevation of IOP was significant \n% 2.The change in intraocular pressure after pupillary dilation in eyes with pseudoexfoliation glaucoma, primary open angle glaucoma, and eyes of normal subjects\n% 3.Intraocular pressure elevation after pupillary dilation in open angle glaucoma\n\n% Prednisolone Acetate Ophthalmic Suspension (Pred Forte)\n% Pred Forte (prednisolone acetate ophthalmic suspension) is a steroid medicine used to treat eye swelling caused by allergy, infection, injury, surgery, or other conditions~\\cite{PRED_FORTE}.\n% 1.Impact of dexamethasone intraocular suspension 9% on intraocular pressure after routine cataract surgery: post hoc analysis \n% increased risk for postoperative IOP elevation \n% 2. Incidence and risk factors for postoperative intraocular pressure response to topical prednisolone eye drops in patients undergoing phacoemulsification\n\n\n\n\n\n"
                    }
                }
            },
            "section 6": {
                "name": "conclusions",
                "content": "\nIn this paper, we propose \\mname, an end-to-end model to compensate for the missing information of the patients with missing modalities and perform clinical prediction as well as analysis.\nFor a patient with missing modalities, \\mname finds similar patients with a task-guided modality-adaptive similarity metric. \nInstead of generating raw missing data, \\mname imputes the hidden representations of the missing modalities in the latent space by the auxiliary information from these similar ones, and conducts the clinical tasks.\nExperiments show that \\mname outperforms all baseline models. \nBesides, the findings are in accord with experts and medical knowledge, which shows it can provide useful insights.\n\n\\begin{acks}\nThis work is supported by the National Natural Science Foundation of China (No.62172011).\nL. Ma is supported by the China Postdoctoral Science Foundation (2021TQ0011).\nJ. Wang is supported by EPSRC New Investigator Award under Grant No.EP/V043544/1.\n\\end{acks}\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{sample-sigconf}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\clearpage\n\n\\appendix\n\n\n"
            },
            "section 7": {
                "name": "Intuition discovery experiment",
                "content": "\n\\label{sec:discovery}\nIn each dataset, for each modality, we first take the samples containing the complete modality data and divide them into training, validation, and test sets. \nThen we use a unimodal classifier (e.g., multilayer perceptron, transformer encoder) to classify the data for each modality. \nWe take the best model from the validation set, apply it to the test set, and collect the representations in the latent space of each modality for each sample.\nNext, we compute pair-wise similarity matrix $\\Pi_m$ between samples in each modality, where $m$ is the corresponding modality.\n\nWe want to justify the intuition in our datasets: if two patients are similar in one modality, they are more likely to be similar in another modality with regard to the clinical task.\nWe call intuition the \\textit{cross-modal transfer of sample similarity}.\nGiven modalities $a$ and $b$, the intuition holds if their difference of the pair-wise similarity matrix $\\|\\Pi_a - \\Pi_b \\|_\\text{norm}$ is small enough, where $\\text{norm}$ is a type of matrix norm.\nIn this experiment, we try different similarity metrics such as normalized Euclidean distance, cosine similarity and RBF kernel.\nAnd we also try different norms as metrics of difference, such as Frobenius norm, 2-norm and mean value of all entries in $\\|\\Pi_a - \\Pi_b \\|$.\n\nFor comparison, we add noise to the representations in the latent space in one modality (we select modality $b$ here), and calculate the difference $\\|\\Pi_a - \\Pi_b^{'} \\|_\\text{norm}$.\nIf the intuition holds, the difference should be bigger than the above original $\\|\\Pi_a - \\Pi_b \\|_\\text{norm}$.\nWe perform this experiment 1,000 times and calculate the average difference to avoid chance.\n\nFurthermore, we also shuffle the representations in the latent space in one modality (we select modality $b$ here), and calculate the difference $\\|\\Pi_a - \\Pi_b^{''} \\|_\\text{norm}$.\nIf the intuition holds, the difference should also be bigger than the first original $\\|\\Pi_a - \\Pi_b \\|_\\text{norm}$.\nIn the same way, we perform this experiment 1,000 times and calculate the average difference to avoid chance.\nThe results are shown in Table\n% ~\\ref{tab:mimic_intuition},\n~\\ref{tab:ov_intuition1} and~\\ref{tab:ov_intuition2}.\n\n\n\n\n\nAs shown in Table~\\ref{tab:ov_intuition1}, the original difference of the pair-wise similarity matrices in the two modalities is smaller than both the Noise and Shuffle ones with regard to different similarity metrics and different norms.\nThis justifies that if two patients are similar in one modality, they are more likely to be similar in another modality in different view.\nTo this end, we come up with our intuition, i.e., the \\textit{cross-modal transfer of sample similarity}.\nIn another pair, as shown in Table~\\ref{tab:ov_intuition2}, the same conclusion can be drew.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
            },
            "section 8": {
                "name": "algorithm",
                "content": "\n\nAlgorithm~\\ref{alg} shows the algorithm of \\mname.\n\n\n\\begin{algorithm}[]\n% \\small\n\\SetAlgoLined\n\\KwInput{\\\\Multimodal EHR dataset $\\mathbf{\\mathbb{X}}$ }\n\\KwOutput{\\\\Prediction for the patient $\\hat{\\mathbf{y}}$}\n\\KwTraining{}\n Initialize weights\\;\n \\While{training is not convergence}{\n  \\For {each batch of patient }{\n Extract $\\mathbf{h}^{m}_{n}$ of each modality $m$ via Eq.~\\ref{eq:hm}\\;\n Form the batch-wise representation matrices $\\mathbf{H}^{m}$\\;\n Compute patient similarity matrix $\\Pi^{m }$ in each modality space via Eq.~\\ref{eq:pim}\\;\n Compute  comprehensive similarity $\\tilde{\\Pi} $ via Eq.~\\ref{eq:pi},~\\ref{eq:piij}\\;\n Form the similar patient graph with ${\\mathbf{H}}^{m}$ as nodes and $\\tilde{\\Pi} $ as adjacency matrix\\;\n Compute the aggregated information ${\\hat{\\mathbf{H}}^{m}}$  via Eq.~\\ref{eq:graphconv} in the space of each modality $m$\\;\n \\For {each patient in the batch}{\n \\If {missing modalities exist}{Impute the missing modalities via Eq.~\\ref{eq:impute}}\n \\Else{Enhance the representations via Eq.~\\ref{eq:alpha}-\\ref{eq:impute}\\;}\n\n Model the multimodal dynamics via Eq.~\\ref{eq:mmbegin}-\\ref{eq:mmend}\\;\n Make prediction via Eq.~\\ref{eq:predict1}\\;\n\n }\n Update the parameters by optimizing Eq.~\\ref{eq:loss}\\;\n }\n }\n \\caption{Algorithm of \\mname}\\label{alg}\n\\end{algorithm}\n\n\n\n\n"
            },
            "section 9": {
                "name": "Further Analysis",
                "content": "\n% \\subsection{Performance of each disease diagnosis in ODIR dataset}\n",
                "subsection 9.1": {
                    "name": "Multiple levels multimodal incompleteness",
                    "content": "\n%两个数据集 两张图\n\n\n\nTo consider more realistic various settings and verify the generalizability of \\mname, experiments on multimodal data with multiple levels of multimodal incompleteness are conducted.\nWe evaluate the influences of missing modalities by attaching additional synthesizing multimodal incompleteness rates on ODIR Dataset from 30\\% to 60\\% with an intermittent 10\\%. \n% Figure~\\ref{fig:data_amount_ana} shows the mean square error (MSE) for LOS prediction on validation sets under different training data amount.\nThe experiments are repeated test with bootstrapping for 1,000 times, and the results (micro-AUC) are in Table~\\ref{tab:result_missing_odir}.\n\n% \\begin{figure}[]\n%   \\centering\n%   \\includegraphics[width=\\columnwidth]{figs/missing_rate.pdf}\n%   \\caption{Performance on ODIR Dataset under different additional synthesizing multimodal incompleteness rates.}\n%   \\label{fig:missing_rate}\n% \\end{figure}\n\nWe can see that all the models' micro-AUCs decrease as the missing rate increases, and \\mname still outperforms all baselines.\n% As more modalities are missing, the performances of Concat drop dramatically.\n% It is because Concat does not explore much inter-modality interaction.\n% The performance gap between \\mname and the baselines tend to be more considerable under higher missing rates, which indicates the capability of \\mname to alleviate the modality-missing problem.\nWhen the missing rate is the biggest of all settings (60\\%), \\mname also demonstrated significantly better performance than the best baselines CM-AEs~\\cite{ngiam2011multimodal} and HGMF~\\cite{chen2020hgmf}.\nSpecifically, \\mname achieves a micro-AUC of 0.7715, while the baseline models CM-AEs~\\cite{ngiam2011multimodal} and HGMF~\\cite{chen2020hgmf} achieve 0.7477 and 0.7427, showing 3.2\\% and 3.8\\% relative improvement, respectively.\n\n"
                },
                "subsection 9.2": {
                    "name": "Clinical implications",
                    "content": "\n\n% Since the disease diagnosis task on ODIR Dataset is a multi-label classification task, we plot the AUROC of each label in ODIR Dataset in Figure~\\ref{fig:ROC4models} to analyze the effectiveness for different diseases.\n% We can see that the AMD (Age related Macular Degeneration) label is relatively easy for all models, where our model (\\mname) is the only one that exceeds AUROC of 95\\%.\n% For other labels, \\mname also achieves the best performance among all the methods.\n\n% \\begin{figure}[]\n% \\centering\n% \\includegraphics[scale=0.2]{figs/ROC4models.pdf}\n% \\caption{AUROC of each disease diagnosis in ODIR Dataset}\n% \\label{fig:ROC4models}\n% \\end{figure}\n% \\subsection{Performance of each disease diagnosis in ODIR dataset}\n\n\nTo intuitively show the implication of \\mname, similar to Section~\\ref{sec:cli_imp}, we further visualize the attention weights of the prediction process for another two cases.\n% Due to the limitation of pages, we report two cases in the test set of OV dataset here.\nAs shown in Figure~\\ref{fig:case2}, the first case has a positive label, and the second one is negative.\nFor the first one, \\mname gives a strong focus on $\\text{Med}_0$ and $\\text{Med}_1$.\nIn both these medications, the patients received not only the drug mentioned above in Section~\\ref{sec:cli_imp}: \\textit{Tropicamide Phenylephrine Eye Drops (Mydrin-P)}, which have been proved by medical literature~\\cite{kim2012changes,atalay2015change} and clinician experience that they can lead to adverse reactions like elevation of IOP.\nThe patients also received Tobramycin Dexamethasone Eye Drops (Tobradex), which is highly consistent with medical literature~\\cite{chen2016comparison}\n% Comparison of the anti-inflammatory effects of fluorometholone 0.1% combined with levofloxacin 0.5% and tobramycin/dexamethasone eye drops after cataract surgery\nand clinician.\nThe drugs can lead to adverse reactions and have the tendency to increase intraocular pressure~\\cite{chen2016comparison}.\nFor the second one, a healthy patient with a negative label, \\mname gives relatively even attention to each data of each modality of the patient, which indicates that \\mname does not discover significant signs of elevation of IOP and finally makes a right prediction.\n\n\n% \\subsection{Hyperparameter analysis}\n% The main hyperparameter in \\mname is the batch size, since the similar patients are found batch-wisely.\n% % The other is the layer of GCNs, which indicates the degree of aggregation of auxiliary information from similar patients.\n% Thus, we conduct experiments to perform the batch size analysis, and the results are shown in Table~\\ref{tab:hp}.\n% It is worth mentioning that the maximum batch size is 512 because of the limitation of GPU memory.\n\n% \\begin{table}[]\n% \\small\n%  \\centering\n%   \\caption{Batch size analysis on ODIR Dataset}\n% %   \\vspace*{-3mm}\n%   \\label{tab:hp}\n%   \\begin{tabular}{cccc}\n%     \\hline\n%      Batch Size   &\\textbf{micro-AUC$\\ \\uparrow$}&\\textbf{macro-AUC$\\ \\uparrow$}&\\textbf{test loss$\\ \\downarrow$}\\\\\n%     \\hline\n%     512 &   0.8490\\,(0.025) & 0.8245\\,(0.026) & 0.1543\\,(0.018) \\\\\n%     256 &   0.8426\\,(0.023) & 0.8189\\,(0.026) & 0.1672\\,(0.016) \\\\\n%     128 &   0.8325\\,(0.025) & 0.8099\\,(0.025) & 0.1655\\,(0.019) \\\\\n%     64 &   0.8195\\,(0.024) & 0.7807\\,(0.025) & 0.1632\\,(0.018) \\\\\n%   \\hline\n% \\end{tabular}\n% \\end{table}\n\n% As shown in Table~\\ref{tab:hp}, we can see that the performance increases as the batch size increases, which reflects the intuition that \\mname extracts auxiliary information from similar patients to complement the information of the modality-missing patient. \n% That is, the larger the batch size, the more comprehensive the auxiliary information extracted from similar patients, which leads to better performance on the task.\n\n"
                }
            },
            "section 10": {
                "name": "Details of Experimental Settings",
                "content": "\n\\label{sec:detailed}\n\n\n",
                "subsection 10.1": {
                    "name": "Statistics of the Datasets",
                    "content": "\n\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=3pt]\n\n\\item\n\\textbf{Ocular Disease Intelligent Recognition (ODIR) Dataset} contains the following modalities (incomplete modalities exist): demographic information, clinical text for both eyes, and fundus images for both eyes.\nThe detailed statistics are presented in Table~\\ref{tab:dataset}.\n\n\n\\item\n\\textbf{Ophthalmic Vitrectomy (OV) Dataset} contains six modalities (incomplete modalities exist): demographic,  clinical notes, medications, admission records, discharge records, and surgical consumables.\nThe detailed statistics are presented in Table~\\ref{tab:dataset}.\n\n\\end{itemize}\n\n\n\n\n\n"
                },
                "subsection 10.2": {
                    "name": "Model Implementation",
                    "content": "\n\nThe experiment environment is a machine equipped with CPU: Intel Xeon E5-2630, 256GB RAM, and GPU: Nvidia RTX8000. The code is implemented based on Pytorch 1.5.1. \nThe hyper-parameter setting of the proposed \\mname is as follows:\nWe set the embedding dimension and hidden dimension as 128/256 for Ocular Disease Intelligent Recognition (ODIR) / Ophthalmic Vitrectomy (OV) dataset, respectively.\nSince the clinical notes modality in OV dataset is too long per patient, and the number of samples is small (832 patients), we set the batch size as 32 while conducting experiments on OV dataset.\nFor ODIR dataset, we set the batch size as 512.\n\n\n\n"
                }
            }
        },
        "tables": {
            "tab:notations": "\\begin{table}[h]\n    \\centering\n    \n      \\caption{Notations for \\mname}\n    %   \\vspace*{-3mm}\n    \\label{tab:notations}\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{c|l}\n        \\hline\n        Notation & Definition \\\\\n         \\hline\n         $ \\mathbf{y} \\in \\{0,1\\}^{|C|} $ & Ground truth of the classification target\\\\\n         $\\hat{\\mathbf{y}} \\in [0,1]^{|C|}$ & Classification result\\\\\n         $\\mathbb{X}$ & The multimodal EHR dataset \\\\\n         $\\mathbf{{X}}_{n} \\in \\mathbb{X}$ & The $n$-th patient in the dataset \\\\\n        $\\mathbf{x}_{n}^{{m}}$ & Modality $m$'s raw data of the $n$-th patient \\\\\n\n         \\hline\n         $\\mathbf{h}^{m}_{n} \\in \\mathbb{R}^{N_{h}}$ & Learned representation of modality $m$ of patient $n$\\\\\n         ${\\mathbf{H}^{{m}}} \\in \\mathbb{R}^{B \\times \\bm{N}_{h}}$ & Learned representations of modality $m$ for the patient batch \\\\\n         ${\\hat{\\mathbf{H}}^{m}} \\in \\mathbb{R}^{B \\times \\bm{N}_{h}}$ & Modality $m$'s auxiliary information representation  \\\\\n         & aggregated from similar patients\\\\\n         $\\bar{\\mathbf{H}}^{\\text{seq}} \\in \\mathbb{R}^{N_{\\text{seq}} \\times \\bm{N}_{h}}$  & Representations of a sequential modality with positional \\\\\n         & encoding added\\\\\n        % 有可能是要补充的模态表示，也有可能是来自相似病人的模态补充表示 \\\\\n\n         \\hline\n         $\\delta\\in (0, 1)$ & Parameter to control learnable kernel \\\\\n         $ \\Pi^{{m} } \\in\\mathbb{R}^{B\\times B}$ & The patient similarity matrix for modality $m$\\\\\n         $\\text{mask}^{m}  \\in\\mathbb{R}^{B\\times B}$& Matrix of booleans that determines each element of the associated \\\\\n         &  value is valid or not (i.e., similarity for missing modality is invalid) \\\\\n         $\\Lambda \\in\\mathbb{R}$ & A learnable threshold to filter out dissimilar pairs \\\\\n        %  $\\epsilon$ = 1e-8 & A value to prevent unstable division by zero \\\\\n         $ \\tilde{\\Pi} \\in\\mathbb{R}^{B\\times B}$ & The comprehensive similarity matrix across all modalities\\\\\n         \n\n         $\\mathbf{z}^{l} $ & The output representations of the $l$-th layer in the model \\\\\n         $\\tilde{\\mathbf{z}}^{l} $ & The middle representations inside the $l$-th layer in the model \\\\\n        %  \\hline\n         $\\alpha_{m} \\in\\mathbb{R}^{B \\times 1}$ & The importance of self-information ${\\mathbf{H}^{{m}}} $\\\\\n         $\\beta_{m} \\in\\mathbb{R}^{B \\times 1} $ & The importance of similar patients information ${\\hat{\\mathbf{H}}^{m}} $\\\\\n        %  $\\bm{S} \\in \\mathbb{R}^{B \\times \\bm{N}_{h}}$ & Final health status representations for the target prediction\\\\\n\n         \\hline\n        %  \\hline\n    \\end{tabular}}\n\\end{table}",
            "tab:result_ODIR": "\\begin{table}[h]\n\\small \n% \\footnotesize\n  \\centering\n  \\caption{Results on the ODIR Dataset}\n  \\label{tab:result_ODIR}\n\n% \\vspace*{-4mm}\n\\begin{tabular}{cccc}\n\\hline\n\n & \\multicolumn{3}{c}{\\textbf{ODIR Dataset} (Multi-label Classification) }\\\\\n\n \\textbf{Methods}&\\textbf{micro-AUC$\\ \\uparrow$}&\\textbf{macro-AUC$\\ \\uparrow$}&\\textbf{test loss$\\ \\downarrow$}\\\\ \n\\hline\n\n\nMFN~\\cite{zadeh2018memory}   & 0.7877\\,(0.030) & 0.7766\\,(0.029) & 0.1772\\,(0.020) \\\\ \nMulT~\\cite{tsai2019multimodal}  & 0.7944\\,(0.028) & 0.8032\\,(0.026) & 0.2339\\,(0.019)  \\\\ \nViLT~\\cite{kim2021vilt}   & 0.7966\\,(0.031) & 0.7624\\,(0.029) & 0.1731\\,(0.016) \\\\\nCM-AEs~\\cite{ngiam2011multimodal} & 0.8028\\,(0.030) & 0.7672\\,(0.027) & 0.1878\\,(0.031) \\\\ \nSMIL~\\cite{ma2021smil}   & 0.8092\\,(0.032) & 0.7978\\,(0.025) & 0.2278\\,(0.032) \\\\ \nHGMF~\\cite{chen2020hgmf} & 0.8080\\,(0.030) & 0.8103\\,(0.031) & 0.1810\\,(0.022) \\\\\n\\hline\n $\\mname_{1-}$ & 0.8130\\,(0.031) & 0.8059\\,(0.032) & 0.1781\\,(0.020)  \\\\ \n   $\\mname_{2-}$  & 0.8030\\,(0.031) & 0.8138\\,(0.029) & 0.1631\\,(0.018)  \\\\ \n\\hline\n \\textbf{ $\\mname$} & \\textbf{0.8490}$^{**}$\\,(0.025) & \\textbf{0.8245}$^{**}$\\,(0.026) & \\textbf{0.1543}$^{**}$\\,(0.018)   \\\\ % ok\n\\hline\n\\end{tabular}\n\\end{table}",
            "tab:result_OV": "\\begin{table}[h]\n\\small\n% \\footnotesize\n  \\centering\n  \\caption{Results on the OV Dataset}\n  \\label{tab:result_OV}\n% \\vspace*{-4mm}\n\n\\begin{tabular}{cccc}\n\\hline\n\n &  \\multicolumn{3}{c}{\\textbf{OV Dataset} (Binary Classification) }\\\\\n\n \\textbf{Methods}& \\textbf{AUPRC$\\ \\uparrow$}& \\textbf{AUROC$\\ \\uparrow$}& \\textbf{ACC$\\ \\uparrow$}  \\\\ \n\\hline\n\n\nMFN~\\cite{zadeh2018memory}   & 0.6456\\,(0.038) & 0.6789\\,(0.032)  & 0.6627\\,(0.032) \\\\ \nMulT~\\cite{tsai2019multimodal}  & 0.6814\\,(0.047)& 0.6891\\,(0.043)  & 0.6988\\,(0.031) \\\\ \nViLT~\\cite{kim2021vilt}  & 0.6987\\,(0.051) & 0.7245\\,(0.048)  & 0.6627\\,(0.033) \\\\ \nCM-AEs~\\cite{ngiam2011multimodal} & 0.6891\\,(0.031) & 0.6927\\,(0.040) & 0.6747\\,(0.029) \\\\ \nSMIL~\\cite{ma2021smil}    & 0.7109\\,(0.045)& 0.7041\\,(0.033)  & 0.6867\\,(0.032) \\\\ \n\nHGMF~\\cite{chen2020hgmf} & 0.7037\\,(0.050) & 0.7544\\,(0.027) & 0.7100\\,(0.032) \\\\\n\\hline\n $\\mname_{1-}$ & 0.6849\\,(0.054) & 0.7472\\,(0.052)  & 0.7080\\,(0.064) \\\\ \n   $\\mname_{2-}$ & 0.7110\\,(0.044) & 0.7562\\,(0.057)  & 0.7234\\,(0.072) \\\\ \n\\hline\n     \n \\textbf{ $\\mname$}  & \\textbf{0.7549}$^{**}$\\,(0.065) & \\textbf{0.7998}$^{**}$\\,(0.049)  & \\textbf{0.7438}$^{**}$\\,(0.058) \\\\ % ok\n\\hline\n\\end{tabular}\n\\end{table}",
            "tab:ov_intuition1": "\\begin{table}[b]\n% \\small\n  \\centering\n  \\caption{Intuition observation results for two modalities: admission records and clinical notes, on the OV Dataset}\n%   \\vspace*{-3mm}\n  \\label{tab:ov_intuition1}\n\\begin{tabular}{ccccc}\n% \\label{tab:mimic_intuition}\n\\hline\n\nMetric                                       & Norm      & Original & Noise   & Shuffle \\\\ \\hline\n\\multicolumn{1}{c}{\\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Normalized \\\\ Euclidean \\\\ Distance\\end{tabular}}} & Frobenius & 183.35   & 403.23 & 207.27  \\\\\n\\multicolumn{1}{c}{}                         & 2         & 153.97   & 385.21 & 173.73  \\\\\n\\multicolumn{1}{c}{}                         & Mean      & 0.2334   & 0.4871 & 0.2724  \\\\ \\hline\n\\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Cosine \\\\ similarity\\end{tabular}} & Frobenius & 402.96   & 462.59 & 452.72  \\\\\n                                             & 2         & 336.31   & 365.02 & 376.69  \\\\\n                                             & Mean      & 0.5081   & 0.5821 & 0.5908  \\\\ \\hline\n\\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}RBF \\\\ kernel\\end{tabular}}   & Frobenius & 176.35   & 261.05 & 199.60  \\\\\n                                             & 2         & 147.51   & 196.10 & 166.46  \\\\\n                                             & Mean      & 0.2241   & 0.3024 & 0.2623  \\\\ \\hline\n\\end{tabular}\n\\end{table}",
            "tab:ov_intuition2": "\\begin{table}[b]\n% \\small\n  \\centering\n  \\caption{Intuition observation results for two modalities: medications and surgical consumables information, on the OV Dataset}\n%   \\vspace*{-3mm}\n  \\label{tab:ov_intuition2}\n\\begin{tabular}{ccccc}\n% \\label{tab:mimic_intuition}\n\\hline\n\nMetric                                       & Norm      & Original & Noise   & Shuffle \\\\ \\hline\n\\multicolumn{1}{c}{\\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Normalized \\\\ Euclidean \\\\ Distance\\end{tabular}}}  & Frobenius & 152.96   & 159.02 & 370.39  \\\\\n\\multicolumn{1}{c}{}                         & 2         & 131.37   & 145.77 & 360.48  \\\\\n\\multicolumn{1}{c}{}                         & Mean      & 0.2021   & 0.2289 & 0.5588  \\\\ \\hline\n\\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Cosine \\\\ similarity\\end{tabular}}& Frobenius & 384.57   & 406.01 & 404.89  \\\\\n                                             & 2         & 326.61   & 353.72 & 367.75  \\\\\n                                             & Mean      & 0.4984   & 0.5328 & 0.5671  \\\\ \\hline\n\\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}RBF \\\\ kernel\\end{tabular}} & Frobenius & 168.85   & 173.56 & 203.48  \\\\\n                                             & 2         & 144.43   & 157.96 & 188.48  \\\\\n                                             & Mean      & 0.2081   & 0.2260 & 0.2825  \\\\ \\hline\n\\end{tabular}\n\\end{table}",
            "tab:result_missing_odir": "\\begin{table}[]\n% \\footnotesize\n\\small \n  \\centering\n  \\caption{Micro-AUC on ODIR Dataset under different additional synthesizing multimodal missing rates.}\n  \\label{tab:result_missing_odir}\n% \\vspace*{-4mm}\n\\begin{tabular}{c|cccc}\n\\hline\n\n%  & \\multicolumn{3}{c}{\\textbf{micro-AUC}}\\\\\n\n \\textbf{Methods}&30\\%&40\\%&50\\%&60\\%\\\\ \n\\hline\n\n\nMFN~\\cite{zadeh2018memory}  & .7625\\,(.03) & .7334\\,(.03)  & .7260\\,(.03)  & .7166\\,(.03) \\\\ \nMulT~\\cite{tsai2019multimodal} & .7768\\,(.02) & .7637\\,(.03)  & .7480\\,(.03)  & .7348\\,(.03) \\\\ \nViLT~\\cite{kim2021vilt}  & .7601\\,(.03) & .7583\\,(.03)  & .7492\\,(.03)  & .7355\\,(.02) \\\\  \n% VATT~\\cite{akbari2021vatt}   & .7632\\,(.03) & .7556\\,(.03)  & .7465\\,(.04)  & .7284\\,(.03) \\\\ \nCM-AEs~\\cite{ngiam2011multimodal} & .7846\\,(.03) & .7707\\,(.00)  & .7648\\,(.03)  & .7477\\,(.03) \\\\ \n% CPM-Nets~\\cite{zhang2019cpm}  & .7655\\,(.03) & .7570\\,(.03)  & .7403\\,(.03)  & .7246\\,(.03) \\\\ \nSMIL~\\cite{ma2021smil}   & .7702\\,(.02) & .7595\\,(.02)  & .7485\\,(.03)  & .7396\\,(.03) \\\\  \nHGMF~\\cite{chen2020hgmf}  & .7831\\,(.03) & .7711\\,(.03)  & .7585\\,(.03)  & .7427\\,(.02) \\\\ \n\n\\hline\n \\textbf{ $\\mname$} & \\textbf{.8119}$^{**}$\\,(.02) & \\textbf{.7927}$^{**}$\\,(.03) & \\textbf{.7795}$^{**}$\\,(.03) & \\textbf{.7715}$^{**}$\\,(.02)  \\\\ % ok\n\\hline\n\\end{tabular}\n\\end{table}",
            "tab:dataset": "\\begin{table}[h]\n% \\footnotesize\n\\small\n\\caption{Statistics of the Datasets}\n% \\vspace*{-4mm}\n\\label{tab:dataset}\n\\begin{tabular}{llc}\n\\hline\nDataset                                                                                                                 & Statistic                    & Value                     \\\\ \\hline\n\n\\multirow{5}{*}{\\begin{tabular}[c]{@{}l@{}}Ocular Disease \\\\ Intelligent \\\\ Recognition \\\\ (ODIR) Dataset\\end{tabular}} & \\# patients                   & 3,500                      \\\\\n                                                                                                                        & \\# modalities                & 3                         \\\\\n                                                                                                                      & \\% missing per modality\n                                                                                                                      & {[}0\\%, 48.34\\%, 0\\%{]}   \\\\\n                                                                                                                        & \\% positive labels           & {[}0.061, 0.060, .0046{]} \\\\\n                                                                                                                        \n                                                                                                                        & \\% female                    & .461                     \\\\ \\hline\n\\multirow{6}{*}{\\begin{tabular}[c]{@{}l@{}}Ophthalmic \\\\ Vitrectomy \\\\ (OV) Dataset\\end{tabular}}                       & \\# patients                   & 832                       \\\\\n                                                                                                                        & \\#  modalities               & 6                         \\\\\n                                                                                                                        & \\% missing per modality\\tablefootnote{The order of the missing rates are\n                                                                                                                      corresponding to the above data description.}      &  {[}0\\%, 0.60\\%, 15.38\\%, \\\\\n                                                                                                                        &  &10.33\\%, 10.33\\%, 4.08\\%{]} \\\\\n                                                                                                                        & \\% positive labels           & 41.7\\%                      \\\\\n                                                                                                                        \n                                                                                                                        & \\% female                    & .456                     \\\\ \\hline\n \n                                                                                                                       \n% \\multirow{5}{*}{\\begin{tabular}[c]{@{}l@{}}MIMIC-III \\\\ Dataset\\end{tabular}}                                              & \\# patients                   & 21,139                       \\\\\n%                                                                                                                         & \\# modalities               & 3                         \\\\\n%                                                                                                                         & \\% missing per modality      &  {[}20.05\\%, 20.01\\%, 20.02\\%{]}    \\\\\n%                                                                                                                         & \\% positive labels           & 13.23\\%                    \\\\\n                                                                                                                        \n%                                                                                                                         & \\% female                    & .456                     \\\\ \\hline                                                                                                                        \n\n\\end{tabular}\n% \\vspace*{-3mm}\n\\end{table}"
        },
        "figures": {
            "fig:med_missing_modal": "\\begin{figure}[]\n  \\centering\n  \\includegraphics[width=\\columnwidth]{figs/fig_missing_modal2.pdf}\n\n  \\caption{Left: missing features; right: missing modalities. Each row refers to a patient. In the left figure, each column refers to a feature ($f_n$). In the right figure, each group of columns refers to a modality ($M_n$), meaning that a modality contains many features. These features have high correlations, since they belong to the same modality (e.g., medical images, medical notes, etc.). The boxes in gray indicate the features exist, and others in black represent missing ones. \n  }\n  \\label{fig:med_missing_modal}\n\n\\end{figure}",
            "fig:insight": "\\begin{figure}[]\n  \\centering\n  \\includegraphics[width=\\columnwidth]{figs/insight_figure22.pdf}\n\n  \\caption{Intuition: For a patient with missing modalities, we utilize the other uncensored modalities of the patient and the other patients who also have the same uncensored modalities to find similar patients and estimate the missing information. }\n%   other complete modalities of the patient and other patients who also have these modalities, to find similar patients \n  \\label{fig:insight}\n\n\\end{figure}",
            "fig:framework": "\\begin{figure*}[t]\n\\centering\n\\includegraphics[scale=0.48]{figs/KDDmodel1.pdf}\n% \\vspace*{-4mm}\n\\caption{Framework of \\mname, the black boxes denote missing.\n% \\tbd{5}%可以给两句描述带读者看一下图，例如framework分为几个模块。或者再解释一下和FIG3的关系\n}\n\\label{fig:framework}\n\\end{figure*}",
            "fig:framework2": "\\begin{figure}[t]\n\\centering\n\\includegraphics[scale=0.5]{figs/KDDmodel2.pdf}\n% \\vspace*{-4mm}\n\\caption{Multimodal Learning Model for Clinical Tasks. Continued from the bottom right corner in Figure~\\ref{fig:framework}. \n% \\tbd{6}%后半句应该在前面吧。\n     }\n\\label{fig:framework2}\n\\end{figure}",
            "fig:case": "\\begin{figure}\n \\centering \n \\subfigure[]{\\includegraphics[width=1.6in]{figs/case_atten49_.pdf}}\n \\subfigure[]{\\includegraphics[width=1.6in]{figs/case_atten464_.pdf}}\n%  \\vspace*{-5mm}\n \\caption{Attention weight visualization for two patients with abnormally increased intraocular pressure in the test set of the OV dataset.~\\textit{Best viewed in color.}}\n \\label{fig:case}\n\\end{figure}",
            "fig:case2": "\\begin{figure}[]\n \\centering\n \\subfigure[a positive case]{\\includegraphics[width=1.6in]{figs/case_atten444_.pdf}}\n \\subfigure[a negative case]{\\includegraphics[width=1.6in]{figs/case_atten667_.pdf}}\n%  \\vspace*{-3mm}\n \\caption{Attention weight visualization for two patients in the test set of OV dataset.}\n \n \\label{fig:case2}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{eq:hm}\n\\mathbf{h}^{m}_{n}=f_{m}\\left(\\mathbf{x}^{m}_{n} ; \\boldsymbol{\\Theta}_{m}\\right),\n\\end{equation}",
            "eq:2": "\\begin{equation}\\nonumber\n\\label{eq:batchH}\n\\mathbf{H}^{m} = [ \\mathbf{h}_{1}^{m}, \\mathbf{h}_{2}^{m}, \\,\\dots,  \\mathbf{h}_{B}^{m}]^{\\intercal} \\in\\mathbb{R}^{B \\times N_{h}},\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:GKernel}\n    k(\\mathbf{h}_{i}^{m}, \\mathbf{h}_{j}^{m})=\\exp (-\\frac{\\|\\mathbf{h}_{i}^{m}-\\mathbf{h}_{j}^{m}\\|_{2}^{2}}{2 \\sigma^{2}}),\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:}\n    k_{\\omega_{m}}(\\mathbf{h}_{i}^{m}, \\mathbf{h}_{j}^{m})=[(1-\\delta_{m}) k(\\phi_{\\omega_{m}}(\\mathbf{h}_{i}^{m}), \\phi_{\\omega_{m}}(\\mathbf{h}_{j}^{m}))+\\delta_{m}] q(\\mathbf{h}_{i}^{m}, \\mathbf{h}_{j}^{m}),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\label{eq:pim}\n\\begin{split}\n    \\Pi^{m }& =k_{\\omega_{m}}(\\mathbf{H}^{m} , {\\mathbf{H}^{m}}), \\\\\n    % \\Pi^{\\text{demo} }& =k_{\\omega}({\\mathcal{H^{\\text{demo}}} }, {\\mathcal{H^{\\text{demo}}}}), \\\\\n    % \\Pi^{\\text{vis} }& =k_{\\omega}({\\mathcal{H^{\\text{vis}}} }, {\\mathcal{H^{\\text{vis}}}}), \\\\\n    % \\Pi^{\\text{ts} }& =k_{\\omega}({\\mathcal{H^{\\text{ts}}} }, {\\mathcal{H^{\\text{ts}}}}),\n\\end{split}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\label{eq:norm_loss}\n    \\mathcal{L}_{\\text{stab}} = \n\\sum_{m=1}^{M} \\left|\\|\\phi_{\\omega_{m}}(\\mathbf{H}^{m})\\|_F - \\|\\mathbf{H}^{m}\\|_F \\right|,\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\label{eq:pi}\n\\begin{split}\n   \\tilde{\\Pi} =  \\frac{\\sum_{1 }^{M} \\Pi^{m} \\cdot  \\text{mask}^{m}  }{\\sum_{1 }^{M} \\text{mask}^{m} + \\epsilon  } \n\\end{split}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\label{eq:piij}\n\\begin{split}\n   \\tilde{\\Pi}_{i,j}  = \\begin{cases}\n  & \\tilde{\\Pi}_{i,j}   \\ \\  \\text{ if } \\tilde{\\Pi}_{i,j} >  \\Lambda   \\\\\n  & 0  \\ \\ \\ \\ \\ \\ \\text{ if  } \\tilde{\\Pi}_{i,j} \\le \\Lambda  \n\\end{cases}\n\\end{split}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\label{eq:graphconv}\n\\begin{split}\n   {\\hat{\\mathbf{H}}^{m}}  =  [ \\hat{\\mathbf{h}}_{1}^{m}, \\hat{\\mathbf{h}}_{2}^{m} & , \\,\\dots ,  \\hat{\\mathbf{h}}_{B}^{m}]^{\\intercal}  = \\operatorname{GCN}({\\mathbf{H}}^{m}, \\tilde{\\Pi}) \\\\\n   & = \\operatorname{ReLU}(\\tilde{\\Pi} \\  \\operatorname{ReLU}(\\tilde{\\Pi} {\\mathbf{H}}^{m} W^0)W^1 ) ,\n\\end{split}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\label{eq:alpha}\n    \\alpha^{m}=\\operatorname{Sigmoid}(  {\\mathbf{H}}^{m} W_{o}), \\beta^{m}=\\operatorname{Sigmoid}({ \\hat{{\\mathbf{H}}}^{m}} W_{s} ),\n\\end{equation}",
            "eq:11": "\\begin{equation}\n\\label{eq:impute}\n\\mathbf{h}_{i}^{m} = \n\\begin{cases}\n \\qquad\\quad\\ \\  \\hat{\\mathbf{h}}_{i}^{m}&  \\text{if modality } m \\text{ of sample }  i \\text{ is missing}  \\\\\n \\alpha^{m}_{i} \\cdot \\mathbf{h}_{i}^{m} + \\beta^{m}_{i} \\cdot  \\hat{\\mathbf{h}}_{i}^{m} & \\text{otherwise}\n\n\\end{cases}\n\\end{equation}",
            "eq:12": "\\begin{equation}\n\\label{eq:mmbegin}\n\\mathbf{z}^{0} = [\\bar{\\mathbf{H}}^{1}+\\text{TE}^{1}; \\bar{\\mathbf{H}}^{2}+\\text{TE}^{2} ;... ; \\bar{\\mathbf{H}}^{M}+\\text{TE}^{M}],\n\\end{equation}",
            "eq:13": "\\begin{equation}\n\\label{eq:mmend}\n\\begin{split}\n% \\vspace*{-1mm}\n   \\tilde{\\mathbf{z}}^{l} & = \\operatorname{LayerNorm}({\\mathbf{z}}^{l-1} + \\operatorname{MHSA}({\\mathbf{z}}^{l-1})), \\\\\n%   \\vspace*{-1mm}\n   \\mathbf{z}^{l}  &= \\operatorname{LayerNorm}(\\tilde{\\mathbf{z}}^{l} + \\operatorname{FFN}(\\tilde{\\mathbf{z}}^{l})), \\\\\n%   \\vspace*{-1mm}\n\\end{split}\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\label{eq:predict1}\n% \\vspace*{-1mm}\n    \\hat{\\mathbf{y}}_i = \\operatorname{Sigmoid}( \\mathbf{z}^{L}_{0}  \\, W_{final}),\n    % \\vspace*{-1mm}\n\\end{equation}",
            "eq:15": "\\begin{equation}\n% \\label{eq:loss}\n% \\vspace*{-1mm}\n\\mathcal{L}_{\\text{pre}} = -\\frac{1}{B} \\sum_{i=1}^{B}(\\mathbf{y}_i^{\\intercal} \\operatorname{log}(\\hat{\\mathbf{y}}_i) + (1-\\mathbf{y}_i )^{\\intercal} \\operatorname{log}(1-\\hat{\\mathbf{y}}_i)),\n% \\vspace*{-1mm}\n\\end{equation}",
            "eq:16": "\\begin{equation}\n\\label{eq:loss}\n% \\vspace*{-1mm}\n\\mathcal{L} = \\mathcal{L}_{\\text{pre}} + \\lambda \\mathcal{L}_{\\text{stab}},\n% \\vspace*{-1mm}\n\\end{equation}"
        },
        "git_link": "https://github.com/choczhang/M3Care"
    }
}