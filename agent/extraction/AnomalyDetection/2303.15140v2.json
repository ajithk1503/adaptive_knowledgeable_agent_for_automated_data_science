{
    "meta_info": {
        "title": "SimpleNet: A Simple Network for Image Anomaly Detection and Localization",
        "abstract": "We propose a simple and application-friendly network (called SimpleNet) for\ndetecting and localizing anomalies. SimpleNet consists of four components: (1)\na pre-trained Feature Extractor that generates local features, (2) a shallow\nFeature Adapter that transfers local features towards target domain, (3) a\nsimple Anomaly Feature Generator that counterfeits anomaly features by adding\nGaussian noise to normal features, and (4) a binary Anomaly Discriminator that\ndistinguishes anomaly features from normal features. During inference, the\nAnomaly Feature Generator would be discarded. Our approach is based on three\nintuitions. First, transforming pre-trained features to target-oriented\nfeatures helps avoid domain bias. Second, generating synthetic anomalies in\nfeature space is more effective, as defects may not have much commonality in\nthe image space. Third, a simple discriminator is much efficient and practical.\nIn spite of simplicity, SimpleNet outperforms previous methods quantitatively\nand qualitatively. On the MVTec AD benchmark, SimpleNet achieves an anomaly\ndetection AUROC of 99.6%, reducing the error by 55.5% compared to the next best\nperforming model. Furthermore, SimpleNet is faster than existing methods, with\na high frame rate of 77 FPS on a 3080ti GPU. Additionally, SimpleNet\ndemonstrates significant improvements in performance on the One-Class Novelty\nDetection task. Code: https://github.com/DonaldRR/SimpleNet.",
        "author": "Zhikang Liu, Yiming Zhou, Yuansheng Xu, Zilei Wang",
        "link": "http://arxiv.org/abs/2303.15140v2",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "Accepted to CVPR 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nImage anomaly detection and localization task aims to identify abnormal images and locate abnormal subregions. The technique to detect the various anomalies of interest has a broad set of applications in industrial inspection~\\cite{bergmann2019mvtec, defard2021padim}. In industrial scenarios, anomaly detection and localization is especially hard, as abnormal samples are scarce and anomalies can vary from subtle changes such as thin scratches to large structural defects, \\eg missing parts. Some examples from the MVTec AD benchmark~\\cite{bergmann2019mvtec} along with results from our proposed method are shown in Figure~\\ref{Fig1}. This situation prohibits the supervised methods from approaching. \n\n\nCurrent approaches address this problem in an unsupervised manner, where only normal samples are used during the training process. The reconstruction-based methods~\\cite{gong2019memorizing,zavrtanik2021reconstruction,ristea2022self}, synthesizing-based methods~\\cite{zavrtanik2021draem, li2021cutpaste}, and embedding-based methods~\\cite{defard2021padim, rudolph2022fully, roth2022towards} are three main trends for tackling this problem. The reconstruction-based methods such as~\\cite{zavrtanik2021reconstruction,ristea2022self} assume that a deep network trained with only normal data cannot accurately reconstruct anomalous regions. The pixel-wise reconstruction errors are taken as anomaly scores for anomaly localization. However, this assumption may not always hold, and sometimes a network can \"generalize\" so well that it can also reconstruct the abnormal inputs well, leading to misdetection~\\cite{gong2019memorizing, perera2019ocgan}.\nThe synthesizing-based methods~\\cite{zavrtanik2021draem, li2021cutpaste}  estimate the decision boundary between the normal and anomalous by training on synthetic anomalies generated on anomaly-free images. However, the synthesized images are not realistic enough. Features from synthetic data might stray far from the normal features, training with such negative samples could result in a loosely bounded normal feature space, meaning indistinct defects could be included in in-distribution feature space. \n\nRecently, the embedding-based methods~\\cite{defard2021padim, rudolph2022fully, deng2022anomaly,  roth2022towards} achieve state-of-the-art performance. These methods use ImageNet pre-trained convolutional neural networks (CNN) to extract generalized normal features. Then a statistical algorithm such as multivariate Gaussian distribution~\\cite{defard2021padim}, normalizing flow~\\cite{rudolph2022fully}, and memory bank~\\cite{roth2022towards} is adopted to embed normal feature distribution. Anomalies are detected by comparing the input features with the learned distribution or the memorized features. However, industrial images generally have a different distribution from ImageNet. Directly using these biased features may cause mismatch problems. Moreover, the statistical algorithms always suffer from high computational complexity or high memory consumption.\n\n\n\n\nTo mitigate the aforementioned issues, we propose a novel anomaly detection and localization network, called SimpleNet. SimpleNet takes advantage of the synthesizing-based and the embedding-based manners, and makes several improvements. First, instead of directly using pre-trained features, we propose to use a feature adaptor to produce target-oriented features which reduce domain bias. Second, instead of directly synthesizing anomalies on the images, we propose to generate anomalous features by posing noise to normal features in feature space. We argue that with a properly calibrated scale of the noise, a closely bounded normal feature space can be obtained. Third, we simplify the anomalous detection procedure by training a simple discriminator, which is much more computational efficient than the complex statistical algorithms adopted by the aforementioned embedding-based methods. Specifically, SimpleNet makes use of a pre-trained backbone for normal feature extraction followed by a feature adapter to transfer the feature into the target domain. Then, anomaly features are simply generated by adding Gaussian noise to the adapted normal features. A simple discriminator consisting of a few layers of MLP is trained on these features to discriminate anomalies.\n\nSimpleNet is easy to train and apply, with outstanding performance and inference speed. The proposed SimpleNet, based on a widely used WideResnet50 backbone, achieves 99.6 \\% AUROC on MVTec AD while running at 77 fps, surpassing the previous best-published anomaly detection methods on both accuracy and efficiency, see Figure~\\ref{Fig:speed}. We further introduce SimpleNet to the task of One-Class Novelty Detection to show its generality. These advantages make SimpleNet bridge the gap between academic research and industrial application. Code will be publicly available.\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\nAnomaly detection and localization methods can be mainly categorized into three types, \\ie, the reconstruction-based methods, the synthesizing-based methods, and the embedding-based methods. \n\n\\textbf{Reconstruction-based methods} hold the insight that anomalous image regions should not be able to be properly reconstructed since they do not exist in the training samples. Some methods~\\cite{gong2019memorizing} utilize generative models such as auto-encoders and generative adversarial networks~\\cite{goodfellow2014generative} to encode and reconstruct normal data. Other methods~\\cite{haselmann2018anomaly, zavrtanik2021reconstruction, ristea2022self} frame anomaly detection as an inpainting problem, where patches from images are masked randomly. Then, neural networks are utilized to predict the erased information. Integrating structural similarity index (SSIM)~\\cite{wang2004image} loss function is widely used in training. An anomaly map is generated as pixel-wise difference between the input image and its reconstructed image. However, if anomalies share common compositional patterns (e.g. local edges) with the normal training data or the decoder is \"too strong\" for decoding some abnormal encodings well, the anomalies in images are likely to be reconstructed well~\\cite{zavrtanik2021reconstruction}.\n\n\n\\textbf{Synthesizing-based methods} typically synthesize anomalies on anomaly-free images. DR\u00c6M~\\cite{zavrtanik2021draem} proposes a network that is discriminatively trained in an end-to-end manner on synthetically generated just-out-of-distribution patterns. CutPaste~\\cite{li2021cutpaste} proposes a simple strategy to generate synthetic anomalies for anomaly detection that cuts an image patch and pastes at a random location of a large image. A CNN is trained to distinguish images from normal and augmented data distributions. However, the appearance of the synthetic anomalies does not closely match the real anomalies'. In practice, as defects are various and unpredictable, generating an anomaly set that includes all outliers is impossible. Instead of synthesizing anomalies on images, with the proposed SimpleNet, negative samples are synthesized in the feature space. \n\n\n\\textbf{Embedding-based methods} achieve state-of-the-art performance recently. These methods embed normal features into a compressed space. The anomalous features are far from the normal clusters in the embedding space. Typical methods~\\cite{defard2021padim, rudolph2022fully, deng2022anomaly, roth2022towards} utilize networks that are pre-trained on ImageNet for feature extraction. With a pre-trained model, PaDiM~\\cite{defard2021padim} embeds the extracted anomaly patch features by multivariate Gaussian distribution. PatchCore~\\cite{roth2022towards} uses a maximally representative memory bank of nominal patch features. Mahalanobis distance or maximum feature distance is adopted to score the input features in testing. However, industrial images generally have a different distribution from ImageNet. Directly using pre-trained features may cause a mismatch problem. Moreover, either computing the inverse of covariance~\\cite{defard2021padim} or searching through the nearest neighbor in the memory bank~\\cite{roth2022towards} limits the real-time performance, especially for edge devices.\n\nCS-Flow~\\cite{rudolph2022fully}, CFLOW-AD~\\cite{gudovskiy2022cflow}, and DifferNet~\\cite{rudolph2021same} propose to transform the normal feature distribution into Gaussian distribution via normalizing flow (NF)~\\cite{rezende2015variational}. As normalizing flow can only process full-sized feature maps, i.e., down sample is not allowed and the coupling layer~\\cite{dinh2016density} consumes a few times of memory than the normal convolutional layer, these methods are memory consuming. Distillation methods~\\cite{bergmann2020uninformed, deng2022anomaly} train a student network to match the outputs of a fixed pre-trained teacher network with only normal samples. A discrepancy between student and teacher output should be detected given an anomalous query. \nThe computational complexity is doubled as an input image should pass through both the teacher and the student.\n\n\nSimpleNet overcomes the aforementioned problems.  SimpleNet uses a feature adaptor that performs transfer learning on the target dataset to alleviate the bias of pre-trained CNNs. SimpleNet proposes to synthesize anomalous in the feature space rather than directly on the images. SimpleNet follows a single-stream manner at inference and is totally constructed by conventional CNN blocks which facilitate fast training, inference, and industrial application. \n\n"
            },
            "section 3": {
                "name": "Method",
                "content": "\n\nThe proposed SimpleNet is elaborately introduced in this section. As illustrated in Figure~\\ref{Fig2}, SimpleNet consist of a \\textit{Feature Extractor}, a \\textit{Feature Adaptor}, an \\textit{Anomalous Feature Generator} and a \\textit{Discriminator}. \nThe \\textit{Anomalous Feature Generator} is only used during training, thus SimpleNet follows a single stream manner at inference. These modules will be described below in sequence.\n\n\n",
                "subsection 3.1": {
                    "name": "Feature Extractor",
                    "content": "\n\nFeature Extractor acquires local feature as in~\\cite{roth2022towards}. We reformulate the process as follows. We denote the training set and test set as $\\mathcal{X}_{train}$ and $\\mathcal{X}_{test}$. For any image $x_{i}\\in\\mathbb{R}^{H\\times W \\times 3}$ in $\\mathcal{X}_{train}\\bigcup\\mathcal{X}_{test}$ , the pre-trained network $\\phi$ extracts features from different hierarchies, as normally done with ResNet-like backbone. Since pre-trained network is biased towards the dataset in which it is trained, it is reasonable to choose only a subset of levels for the target dataset. Formally, we define  $L$ the subset including the indexes of hierarchies for use. The feature map from level $l \\in L$ is denoted as $\\phi^{l, i} \\sim  \\phi^{l}(x_{i})\\in\\mathbb{R}^{H_{l}\\times W_{l}\\times C_{l}}$, where $H_{l}$, $W_{l}$ and $C_{l}$ are the height, width and channel size of the feature map. For an entry $\\phi^{l,i}_{h,w}\\in\\mathbb{R}^{C_{l}}$ at location $(h, w)$, its neighborhood with patchsize $p$ is defined as \n\\begin{equation} \\label{eq1} \\small\n\t\\begin{split}\n\t\t\\mathcal{N}_{p}^{(h,w)} =  \\{ (h',y')| \n\t\t& h'\\in \\left [ h-\\left \\lfloor  p/2\\right \\rfloor,...,h+\\left \\lfloor p/2 \\right \\rfloor  \\right ], \\\\\n\t\t& y'\\in \\left [ w-\\left \\lfloor  p/2\\right \\rfloor,...,w+\\left \\lfloor p/2 \\right \\rfloor  \\right ] \\} \n\t\\end{split}\n\\end{equation}\nAggregating the features within the neighborhood $\\mathcal{N}_{p}^{h,w}$ with aggregation function $f_{agg}$ (use adaptive average pooling here) results in the local feature $z_{h,w}^{l,i}$, as\n\\begin{equation} \\label{eq2} \\small\n\tz^{l,i}_{h,w} = f_{agg}(\\{\\phi^{l,i}_{h',y'}|(h',y')\\in\\mathcal{N}_{p}^{h,w}\\})\n\\end{equation}\nTo combine features $z^{l,i}_{h,w}$ from different hierarchies, all feature maps are linearly resized to the same size $(H_{0}, W_{0})$, i.e. the size of the largest one. Simply concatenating the feature maps channel-wise gives the feature map $o^{i}\\in\\mathbb{R}^{H_{0}\\times H_{0}\\times C}$. The process is defined as\n\\begin{equation}\\label{eq3} \\small\n\to^{i} = f_{cat}({resize(z^{l', i}, (H_{0}, W_{0}))|l'\\in L}\n\\end{equation}\nwe define $o^{i}_{h, w} \\in \\mathbb{R}^{C}$ as the entry of $o^{i}$ at location $(h,w)$.\n\nWe simplify the above expressions as\n\\begin{equation} \\small\n\to^{i} = F_{\\phi}(x^{i})\n\\end{equation}\nwhere $F_{\\phi}$ is the Feature Extractor. \n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Feature Adaptor",
                    "content": "\n\nAs industrial images generally have a different distribution from the dataset used in backbone pre-training, we adopt a Feature Adaptor $G_{\\theta}$ to transfer the training features to the target domain. The Feature Adaptor $G_{\\theta}$ projects local feature $o_{h, w}$ to adapted feature $q_{h, w}$ as \n\\begin{equation} \\label{eq6} \\small\n\tq^{i}_{h, w}=G_{\\theta}(o^{i}_{h,w})\n\\end{equation} \n\n\n\nThe Feature Adaptor can be made up of simple neural blocks such as a fully-connected layer or multi-layer perceptron (MLP). We experimentally find that a single fully-connected layer yields good performance. \n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Anomalous Feature Generator",
                    "content": "\n\nTo train the Discriminator to estimate the likelihood of samples being normal, the easiest way is sampling negative samples, i.e. defect features, and optimizing it together with normal samples. The lack of defects makes the sampling distribution estimation intractable. While~\\cite{liznerskiexplainable,zavrtanik2021draem,li2021cutpaste} relying on extra data to synthesize defect images, we add simple noise on normal samples in the feature space, claiming that it outperforms those manipulated methods.\n\nThe anomalous features are generated by adding Gaussian noise on the normal features $q^{i}_{h, w}\\in\\mathbb{R}^{C}$. Formally, a noise vector $\\epsilon\\in\\mathbb{R}^{C}$ is sampled, with each entry following an i.i.d. Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^{2})$. The anomalous feature $q^{i-}_{h, w}$ is fused as\n\\begin{equation} \\small\n\tq^{i-}_{h, w} = q^{i}_{h, w} + \\epsilon\n\\end{equation}\n\nFigure~\\ref{Fig:FA} illustrates the influence of anomalous features on four classes of MVTec AD. We can see that the standard deviation along each dimension of the adapted features tends to be consistent. Thus, the feature space tends to be compact when distinguishing anomalous features from normal features.\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Discriminator",
                    "content": "\nThe Discriminator $D_{\\psi}$ works as a normality scorer, estimating the normality at each location $(h, w)$ directly.\nSince negative samples are generated along with normal features $\\{q^{i}|x^{i}\\in\\mathcal{X}_{train}\\}$, they are both fed to the Discriminator during training. The Discriminator expects positive output for normal features while negative for anomalous features. We simply use a 2-layer multi-layer perceptron (MLP) structure as common classifiers do, estimating normality as $D_{\\psi}(q_{h, w})\\in\\mathbb{R}$.\n\n\n"
                },
                "subsection 3.5": {
                    "name": "Loss function and Training",
                    "content": "\n\\label{loss_func}\nA simple truncated $l1$ loss is derived as \n\\begin{equation} \\label{eq7} \\small\n\tl^{i}_{h,w} = \\max(0, th^{+}-D_{\\psi}(q^{i}_{h,w})) + \\max(0, -th^{-}+D_{\\psi}(q^{i-}_{h,w}))\n\\end{equation}\n$th^{+}$ and $th^{-}$ are truncation terms preventing overfitting. They are set to 0.5 and -0.5 by default.\nThe training objective is \n\\begin{equation} \\label{eq9} \\small\n\t\\mathcal{L} = \\min\\limits_{\\theta, \\psi}\\sum_{x^{i}\\in\\mathcal{X}_{train}}\\sum_{h,w}\\frac{{}l^{i}_{h,w}}{H_{0}*W_{0}}\n\\end{equation}\nWe will experimentally evaluate the proposed truncated $l1$ loss function with the widely used cross-entropy loss in the experiments section. \nThe pseudo-code of the training procedure is shown in Algorithm~\\ref{algo1}.\n\n\n\n"
                },
                "subsection 3.6": {
                    "name": "Inference and Scoring function",
                    "content": "\nThe Anomalous Feature Generator is discarded at inference. Note that the remaining modules can be stacked into an end-to-end network. We feed each $x_i \\in \\mathcal{X}_{test}$ into the aforementioned Feature Extractor $F_{\\phi}$ and the Feature Adaptor $G_{\\theta}$ sequentially to get adapted features $q^i_{h,w}$ as in Equation~\\ref{eq6}. The anomaly score is provided by the Discriminator $D_{\\psi}$ as\n\\begin{equation} \\small\n\ts^{i}_{h, w} = -D_{\\psi}(q^{i}_{h,w})\n\\end{equation}\n\nThe anomaly map for anomaly localization during inference is defined as\n\\begin{equation}  \\label{eq18} \\small\n\tS_{AL}(x_i):= \\{s^{i}_{h, w}|(h,w)\\in W_0\\times H_0\\}\n\\end{equation}\n\nThen $S_{AL}(x_i)$ is interpolated to have the spatial resolution of the input sample and Gaussian filtered with $\\sigma = 4$ for smooth boundaries. As the most responsive point exists for any size of the anomalous region, the maximum score of the anomaly map is taken as the anomaly detection score of each image\n\\begin{equation}  \\label{eq19} \\small\n\tS_{AD}(x_i):=\\max_{(h,w)\\in W_0\\times H_0} s^i_{h,w}\n\\end{equation}\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\n",
                "subsection 4.1": {
                    "name": "Datasets.",
                    "content": " \nWe conduct most of the experiments on the MVTec Anomaly Detection benchmark~\\cite{bergmann2019mvtec}, that is, a famous dataset in the anomaly detection and localization field. MVTec AD contains 5 texture and 10 object categories stemming from manufacturing with a total of $5354$ images.\nThe dataset is composed of normal images for training and both normal and anomaly images with various types of defect for test. \nIt also provides pixel-level annotations for defective test images. Typical images are illustrated in Figure~\\ref{Fig1}. As in~\\cite{defard2021padim,roth2022towards}, images are resized and center cropped to $256 \\times 256$ and $224 \\times 224$, respectively. No data augmentation is applied. We follow the one-class classification protocol, also known as cold-start anomaly detection, where we train a one-class classifier for each category on its respective normal training samples.\n\nWe conduct one-class novelty detection on CIFAR10~\\cite{krizhevsky2009learning}, which contains 50K training images and 10K test images with scale of $32 \\times 32$ in 10 categories. Under the setting of one-class novelty detection, one category is regarded as normal data and other categories are used as novelty. \n\n\n\\definecolor{dkgreen}{rgb}{0,0.6,0}\n\\definecolor{gray}{rgb}{0.5,0,0}\n\\definecolor{mauve}{rgb}{0.58,0,0.82}\n\n\\lstset{\n\tlanguage=Python,\n\tbasicstyle=\\small\\ttfamily\\bfseries,\n\tnumberstyle=\\color{gray},\n\txleftmargin=5pt,\n\t%frame=,\n\tframexleftmargin=5pt,\n\tcommentstyle=\\color{dkgreen},\n\tkeywordstyle=\\color{blue},\n}\n\n\\begin{algorithm}[t]\n\\setstretch{0.85} \n\\caption{SimpleNet training pseudo-code, Pytorch-like}\n\\begin{lstlisting}  \n# F: Feature Extractor\n# G: Feature Adaptor\n# N: i.i.d Gaussian noise\n# D: Discriminator\npretrain_init(F)\nrandom_init(G, D)\nfor x in data_loader:\n    o = F(x)  # normal features\n    q = G(o)  # adapted features\n    q_ = q + random(N) # anomalous features\n\t\n    loss = loss_func(D(q), D(q_)).mean()\n    loss.backward() # back-propagate\n\n    F = F.detach() # stop gradient\n    update(G, D) # Adam\n\n# loss function    \ndef loss_func(s, s_):\n    th_ = -th = 0.5\n    return max(0, th-s) + max(0, th_+s_)\n\\end{lstlisting}\n\\label{algo1}\n\\end{algorithm}\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Evaluation Metrics.",
                    "content": " Image-level anomaly detection performance is measured via the standard Area Under the Receiver Operator Curve, which we denote as I-AUROC, using produced anomaly detection scores $S_{AD}$ (Equation~\\ref{eq19}). For anomaly localization, the anomaly map $S_{AL}$ (Equation~\\ref{eq18}) is used for an evaluation of pixel-wise AUROC (denoted as P-AUROC). In accordance with prior works~\\cite{defard2021padim,roth2022towards}, we compute on MVTec AD the class-average AUROC and mean AUROC overall categories for detection and localization. \nThe comparison baselines includes AE-SSIM~\\cite{bergmann2019mvtec}, RIAD~\\cite{zavrtanik2021reconstruction}, DR\u00c6M~\\cite{zavrtanik2021draem}, CutPaste~\\cite{li2021cutpaste}, CS-Flow~\\cite{rudolph2022fully}, PaDiM~\\cite{defard2021padim}, RevDist~\\cite{deng2022anomaly} and PatchCore~\\cite{roth2022towards}.\n\n\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Implementation Details",
                    "content": "\nThis section describes the configuration implementation details of the experiments in this paper. All backbones used in the experiments were pre-trained with ImageNet~\\cite{deng2009imagenet}. The 2nd and 3rd intermediate layers of the backbone e.g. $l' \\in [2,3]$ in Equation~\\ref{eq3} are used in the feature extractor as in~\\cite{roth2022towards} when the backbone is ResNet-like architecture. By default, our implementation uses WideResnet50 as backbone, and the feature dimension from the feature extractor is set to 1536. \nThe later feature adaptor is essentially a fully connected layer without bias. The dimensions of the input and output features for the FC layer in the adaptor are the same. The anomaly feature generator adds i.i.d. Gaussian noise $\\mathcal{N}(0, \\sigma^{2})$ to each entry of normal features. $\\sigma $ is set to 0.015 by default. \nThe subsequent discriminator composes of a linear layer, a batch normalization layer, a leaky relu(0.2 slope), and a linear layer. $th^{+}$ and $th^{-}$ are both set to $0.5$ in Equation~\\ref{eq7}. The Adam optimizer is used, setting the learning rate for the feature adaptor and discriminator to 0.0001 and 0.0002 respectively, and weight decay to 0.00001. Training epochs is set to 160 for each dataset and batchsize is 4.  \n\n\n"
                },
                "subsection 4.4": {
                    "name": "Anomaly detection on MVTec AD",
                    "content": "\n\n\nAnomaly detection results on MVTec AD are shown in Table~\\ref{table:1}. Image-level anomaly score is given by the maximum score of the anomaly map as in Equation~\\ref{eq19}. SimpleNet achieves the highest score for 9 out of 15 classes. For textures and objects, SimpleNet reaches new SOTA of $99.8\\%$ and $99.5\\%$ of I-AUROC, respectively. SimpleNet achieves significantly higher mean image anomaly detection performance i.e. I-AUROC score of $99.6\\%$. Please note that, a reduction from an error of $0.9\\%$ for PatchCore~\\cite{roth2022towards} (next best competitor, under the same WideResnet50 backbone) to $0.4\\%$ for SimpleNet means a reduction of the error by $55.5\\%$. In industrial inspection settings, this is a relevant and significant reduction.\n\n\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Anomaly localization on MVTec AD",
                    "content": "\nThe anomaly localization performance is measured by pixel-wise AUROC, which we note as P-AUROC. Comparisons with the state-of-the-art methods are shown in Table~\\ref{table:1}. SimpleNet achieves the best anomaly detection performance of $98.1\\%$ P-AUROC on MVTec AD as well as the new SOTA of $98.4\\%$ P-AUROC for objects. SimpleNet achieves the highest score for 4 out of 15 classes. We visualize representative samples for anomaly localization in Figure~\\ref{Fig8}.\n\n"
                },
                "subsection 4.6": {
                    "name": "Inference time",
                    "content": "\nAlongside the detection and localization performance, inference time is the most important concern for industrial model deployment. The comparison with the state-of-the-art methods on inference time is shown in Figure~\\ref{Fig:speed}. All the methods are measured on the same hardware containing a Nvidia GeForce GTX 3080ti GPU and an Intel(R) Xeon(R) CPU E5-2680 v3@2.5GHZ. It clearly shows that our method achieves the best performance as well as the fastest speed at the same time. SimpleNet is nearly 8$\\times$ faster than PatchCore~\\cite{roth2022towards}.\n\n\n"
                },
                "subsection 4.7": {
                    "name": "Ablation study",
                    "content": "\n\\label{Ablation}\n\n\n\n\\textbf{Neighborhood size and hierarchies.} We investigate the influence of neighborhood size $p$ in Equation~\\ref{eq1}. \nResults in Figure~\\ref{Fig9} show a clear optimum between locality and global context for anomaly predictions, thus motivating the neighborhood size $p =3$. \nWe design a group of experiments to test the influence of hierarchies subset $L$ on model performance and the results are shown in Table~\\ref{table:20}. We index the first three WideResNet50 blocks with $1-3$. As can be seen, features from hierarchy level 3 can already achieve state-of-the-art performance but benefit from additional hierarchy level 2. We chose $2+3$ as the default setting.\n\n\n\n\n\n\n\n\n\\textbf{Adaptor configuration.} Adaptor provides a transformation (projection) on the pre-trained features. Our default feature adaptor is a single FC layer without bias, with equal input and output channels. A comparison of different feature adaptors is shown in Table~\\ref{table:2}, the first row \u201dOurs\u201d implementation follows the same configuration as in Table~\\ref{table:1}. \u201cOurs-complex-FA\u201d replaces the simple feature adaptor with a nonlinear one (i.e. 1 layer MLPs with nonlinearity). The row \u201dOurs-w/o-FA\u201d drops the feature adaptor. The results indicate that a single FC layer yields the best performance. Intuitively, the feature adaptor finds a projection such that the faked abnormal features and projected pre-trained features are easily severed, meaning a simple solution to the discriminator. This is also indicated by the phenomenon that using a feature adaptor helps the network converge fast (Figure~\\ref{Fig6}). We observe a significant performance drop with a complex feature adaptor. One possible reason is that a complex adaptor may lead to overfitting, reducing the generalization ability for various defects in test. \nFigure~\\ref{Fig:FA} compares the histogram of standard deviation along each dimension of the features before and after the feature adaptor. We can see that, when training with anomalous features, adapted feature space becomes compact.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\textbf{Scale of noise.} The scale of noise in the anomaly feature generator controls how far away the synthesized abnormal features are from the normal ones. To be specific, high $\\sigma$ results in abnormal features keeping a high Euclidean distance towards normal features. Training on a large $\\sigma$ will result in a loose decision bound, leading to a high false negative. Conversely, the training procedure will become unstable if $\\sigma$ is tiny, and the discriminator cannot generalize to normal features well. Figure~\\ref{Fig4} details the effect of $\\sigma$ for each class in MVTec AD. As can be seen, $\\sigma = 0.015$ reaches the balance and yield the best performance. \n\n\\textbf{Loss function.} We compared the proposed loss function in Section~\\ref{loss_func} with the widely used cross-entropy loss (as show in row \"Ours-CE\" in Table~\\ref{table:2}). We found the improvements, $0.2 \\%$ I-AUROC and $0.3 \\%$ P-AUROC, over cross-entropy loss. \n\n\\textbf{Dependency on backbone.} We test SimpleNet with different backbones, the results are shown in Table~\\ref{table:backbone}. We find that results are mostly stable over the choice of different backbones. \nThe choice of WideResNet50 is made to be comparable with PaDiM~\\cite{defard2021padim} and PatchCore~\\cite{roth2022towards}.\n\n\n\\textbf{Qualitative Results} Figure~\\ref{Fig8} shows results of anomaly localization that indicate the abnormal areas. The threshold for segmentation results is obtained by calculating the F1-score for all anomaly scores of each sub-class. Experimental results prove that the proposed method can localize abnormal areas well even in rather difficult cases. In addition, we can find that the proposed method has consistent performance in both object and texture classes. \n\n\n"
                },
                "subsection 4.8": {
                    "name": "One-Class Novelty Detection",
                    "content": "\nTo evaluate the generality of the proposed SimpleNet, we conduct a one-class novelty detection experiment on CIFAR-10~\\cite{krizhevsky2009learning}. Following ~\\cite{perera2019ocgan}, we train the model with samples from a single class and detect novel samples from other categories. We train the corresponding model for each class respectively. Note that the novelty score is defined as the max score in the similarity map. Table~\\ref{one_class} reports the I-AUROC scores of our method and other methods. For fair comparison, all the methods are pre-trained on ImageNet. The baselines include VAE~\\cite{an2015variational}, LSA~\\cite{abati2019latent}, DSVDD~\\cite{ruff2018deep}, OCGAN~\\cite{perera2019ocgan}, HRN~\\cite{hu2020hrn}, AnoGAN~\\cite{schlegl2017unsupervised}, DAAD~\\cite{hou2021divide}, MKD~\\cite{salehi2021multiresolution}, DisAug CLR~\\cite{sohnlearning2021}, IGD~\\cite{chen2022deep}\nand RevDist~\\cite{deng2022anomaly}. Our method outperforms these comparison methods. Note that, IGD~\\cite{chen2022deep} and DisAug CLR~\\cite{sohnlearning2021} achieve $91.25 \\%$  and $92.4 \\%$ respectively when boosted by self-supervised learning. \n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\nIn this paper, we propose a simple but efficient approach named SimpleNet for unsupervised anomaly detection and localization. SimpleNet consists of several simple neural network modules which are easy to train and apply in industrial scenarios. Though simple, SimpleNet achieves the highest performance as well as the fastest inference speed compared to the previous state-of-the-art methods \non the MVtec AD benchmark. SimpleNet provides a new perspective to bridge the gap between academic research and industrial application in anomaly detection and localization.\n\n"
            },
            "section 6": {
                "name": "Acknowledgments",
                "content": "\nThis work is supported by the National Natural Science Foundation of China under Grant 62176246 and Grant 61836008. This work is also supported by Anhui Provincial Natural Science Foundation 2208085UD17 and the Fundamental Research Funds for the Central Universities (WK3490000006).\n\n\n%%%%%%%%% REFERENCES\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egbib}\n}\n\n"
            }
        },
        "tables": {
            "table:1": "\\begin{table*}[t]\n\t\\footnotesize\n%\t\\small\n\t\\caption{Comparison of SimpleNet with state-of-the-arts works on MVTec AD. Image-wise AUROC (I-AUROC) and pixel-wise AUROC (P-AUROC) are displayed in each entry as I-AUROC\\%/P-AUROC\\%. P-AUROC for CS-Flow is not recorded in \\cite{rudolph2022fully}}%, and we replace it with \"-\". }\n\t\n\t\\centering \n\t\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}\n\t\t\\hline\n\t\t\\multicolumn{1}{|l|}{Type} & \\multicolumn{2}{c|}{Reconstruction-based} & \\multicolumn{2}{c|}{Synthesizing-based}                    & \\multicolumn{4}{c|}{Embedding-based}                                                  & \\multicolumn{1}{c|}{Ours}   \\\\ \n\t\t\\hline\n\t\tModel                      & AE-SSIM & RIAD                            & DR\u00c6M                       & CutPaste                   & CS-Flow        & PaDiM              & RevDist                    & PatchCore          & SimpleNet                   \\\\\n\t\t\\hline\n\t\tCarpet     & 87/64.7 & 84.2/96.3                      & 97.0/95.5                  & 93.9/98.3                  & \\textbf{100}/-  & 99.8/99.1         & 98.9/98.9                  & 98.7/\\textbf{99.0} & 99.7/98.2                      \\\\\n\t\tGrid       & 94/84.9 & 99.6/98.8                      & 99.9/99.7                  & \\textbf{100}/97.5          & 99.0/-          & 96.7/97.3         & \\textbf{100}/\\textbf{99.3} & 98.2/98.7          & 99.7/98.8                      \\\\\n\t\tLeather    & 78/56.1 & \\textbf{100}/99.4              & \\textbf{100}/98.6          & \\textbf{100}/\\textbf{99.5} & \\textbf{100}/-  & \\textbf{100}/99.2 & \\textbf{100}/99.4          & \\textbf{100}/99.3  & \\textbf{100}/99.2              \\\\\n\t\tTile       & 59/17.5 & 98.7/89.1                      & 99.6/\\textbf{99.2}         & 94.6/90.5                  & \\textbf{100}/-  & 98.1/94.1         & 99.3/95.6                  & 98.7/95.6          & 99.8/97.0                      \\\\\n\t\tWood       & 73/60.3 & 93.0/85.8                      & 99.1/\\textbf{96.4}         & 99.1/95.5                  & \\textbf{100}/-  & 99.2/94.9         & 99.2/95.3                  & 99.2/95.0          & \\textbf{100}/94.5             \\\\\n\t\t\\hline\n\t\tAvg. Text. & 78/56.7 & 95.1/93.9                      & 99.1/\\textbf{97.9}         & 97.5/96.3                  & \\textbf{99.8}/- & 95.5/96.9         & 99.5/97.7                  & 99.0/97.5          & \\textbf{99.8}/97.5            \\\\\n\t\t\\hline\n\t\tBottle     & 93/83.4 & 99.9/98.4                      & 99.2/\\textbf{99.1}         & 98.2/97.6                  & 99.8/-          & 99.1/98.3         & \\textbf{100}/98.7          & \\textbf{100}/98.6  & \\textbf{100}/98.0              \\\\\n\t\tCable      & 82/47.8 & 81.9/84.2                      & 91.8/94.7                  & 81.2/90.0                  & 99.1/-          & 97.1/96.7         & 95.0/97.4                  & 99.5/\\textbf{98.4} & \\textbf{99.9}/97.6            \\\\\n\t\tCapsule    & 94/86.0 & 88.4/92.8                      & \\textbf{98.5}/94.3         & 98.2/97.4                  & 97.1/-          & 87.5/98.5         & 96.3/98.7                  & 98.1/98.8          & 97.7/\\textbf{98.9}            \\\\\n\t\tHazelhut   & 97/91.6 & 83.3/96.1                      & \\textbf{100}/\\textbf{99.7} & 98.3/97.3                  & 99.6/-          & 99.4/98.2         & 99.9/98.9                  & \\textbf{100}/98.7  & \\textbf{100}/97.9            \\\\\n\t\tMetal Nut  & 89/60.3 & 88.5/92.5                      & 98.7/\\textbf{99.5}         & 99.9/93.1                  & 99.1/-          & 96.2/97.2         & \\textbf{100}/97.3          & \\textbf{100}/98.4  & \\textbf{100}/98.8              \\\\\n\t\tPill       & 91/83.0 & 83.8/95.7                      & 98.9/97.6                  & 94.9/95.7                  & 98.6/-          & 90.1/95.7         & 96.6/98.2                  & 96.6/97.4          & \\textbf{99.0}/\\textbf{98.6}   \\\\\n\t\tScrew      & 96/88.7 & 84.5/98.8                      & 93.9/97.6                  & 88.7/96.7                  & 97.6/-          & 97.5/98.5         & 97.0/\\textbf{99.6}         & 98.1/99.4          & \\textbf{98.2}/99.3            \\\\\n\t\tToothbrush & 92/78.4 & \\textbf{100}/98.9              & \\textbf{100}/98.1          & 99.4/98.1                  & 91.9/-          & \\textbf{100}/98.8 & 99.5/\\textbf{99.1}         & \\textbf{100}/98.7  & 99.7/98.5                     \\\\\n\t\tTransistor & 90/72.5 & 90.9/87.7                      & 93.1/90.9                  & 96.1/93.0                  & 99.3/-          & 94.4/97.5         & 96.7/92.5                  & \\textbf{100}/96.3  & \\textbf{100}/\\textbf{97.6}    \\\\\n\t\tZipper     & 88/66.5 & 98.1/97.8                      & \\textbf{100}/98.8          & 99.9/99.3                  & 99.7/-          & 98.6/98.5         & 98.5/98.2                  & 99.4/98.8          & 99.9/\\textbf{98.9}             \\\\\n\t\t\\hline\n\t\tAvg. Obj.  & 91/75.8 & 89.9/94.3                      & 97.4/97.0                  & 95.5/95.8                  & 98.2/-          & 96.0/97.8         & 98/97.9                    & 99.2/\\textbf{98.4} & \\textbf{99.5}/\\textbf{98.4}    \\\\\n\t\t\\hline\n\t\tAverage    & 87/69.4 & 91.7/94.2                      & 98.0/97.3                  & 96.1/96.0                  & 98.7/-          & 95.8/97.5         & 98.5/97.8                  & 99.1/\\textbf{98.1} & \\textbf{99.6}/\\textbf{98.1}   \\\\ \\hline                   \n\t\\end{tabular}\n\t\\label{table:1}\n\\end{table*}",
            "table:20": "\\begin{table}\\caption{Performance on MVTec AD under different combinations of hierarchy levels of WideResNet50 to use.}\\centering\n\t\\footnotesize\n\t% \\setlength{\\tabcolsep}{3pt}\n\t\\begin{tabular}{|c|c|c|c|c|}\n\t\t\\hline\n\t\tlevel1     & level2    & level3    & I-AUROC\\%     & P-AUROC\\% \\\\\n\t\t\\hline\n\t\t\\checkmark &           &           & 93.0          & 94.2      \\\\\n\t\t& \\checkmark&           & 98.4          & 96.7      \\\\\n\t\t&           & \\checkmark& 99.2          & 97.5      \\\\\n\t\t\\checkmark & \\checkmark&           & 96.7          & 96.7      \\\\\n\t\t& \\checkmark&\\checkmark & \\textbf{99.6} & \\textbf{98.1}     \\\\\n\t\t\\checkmark & \\checkmark& \\checkmark& 99.1          & \\textbf{98.1}     \\\\\n\t\t\\hline \n\t\\end{tabular}\n\t\\label{table:20}\n\\end{table}",
            "table:2": "\\begin{table}\\footnotesize\\caption{Comparison of different feature adaptors. \"Ours\" implementation follows the same configuration as in Table \\ref{table:1}. \"Ours-complex-FA\" replaces the simple feature adaptor with a nonlinear one. \"Ours-w/o-FA\" drops the feature adaptor, equivalent to using an identity fully-connected layer. \"Ours-CE\" uses cross-entropy loss. I-AUROC\\% and P-AUROC\\% of MVTec AD are shown.}\\centering\n\t\n\t% \\setlength{\\tabcolsep}{3pt}\n\t\\begin{tabular}{|c|c|c|}\n\t\t\\hline\n\t\tModel & I-AUROC\\% & P-AUROC\\% \\\\\n\t\t\\hline\n\t\tOurs & \\textbf{99.6} & \\textbf{98.1} \\\\\n\t\t\\hline\n\t\tOurs-complex-FA & 98.3 & 97.2 \\\\\n\t\t\\hline\n\t\tOurs-w/o-FA & 99.2 & 97.9 \\\\\n\t\t\\hline\n\t\tOurs-CE & 99.4 & 97.8 \\\\ \n\t\t\\hline\n\t\\end{tabular}\n\t\\label{table:2}\n\\end{table}",
            "table:backbone": "\\begin{table}\n\t\\centering\n\t\\caption{Performance under different backbones on MVTec AD. }\\centering\n\t\\footnotesize\n\t\\begin{tabular}{|c|c|c|} \n\t\t\\hline\n\t\tModel           & I-AUROC\\% & P-AUROC\\%  \\\\ \n\t\t\\hline\n\t\tResNet18        & 98.3    & 95.7     \\\\ \n\t\t\\hline\n\t\tResNet50        & 99.6    & 98.0       \\\\ \n\t\t\\hline\n\t\tResNet101   & 99.2    & 97.6     \\\\ \n\t\t\\hline\n\t\tWideResNet50    & \\textbf{99.6}    & \\textbf{98.1}\\\\\n\t\t\\hline\n\t\\end{tabular}\n\t\\label{table:backbone}\n\\end{table}",
            "one_class": "\\begin{table} \n\t\\caption{One-Class Novelty Detection I-AUROC(\\%) results on CIFAR-10 dataset.}\n\t\\centering\n\t\\small\n\t\\resizebox{1\\columnwidth}{!}{\n\t\t\\begin{tabular}{|l|c|c|c|c|c|} \n\t\t\t\\hline\n\t\t\tMethod & LSA         & DSVDD    & OCGAN & HRN     & DAAD                   \\\\ \n\t\t\t\\hline\n\t\t\tAUROC  & 64.1        & 64.8     & 65.6  & 71.3    & 75.3      \\\\ \n\t\t\t\\hline\n\t\t\tMethod & DisAug CLR~ & IGD      & MKD   & RevDist & \\textbf{\\textbf{SimpleNet}} \\\\ \n\t\t\t\\hline\n\t\t\tAUROC  & 80.0        & 83.68 & 84.5  & 86.5    & \\textbf{86.5}     \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t}\n\t\\label{one_class}\n\\end{table}"
        },
        "figures": {
            "Fig1": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.46\\textwidth]{figure_show_class_results.pdf} \n\t\\caption{Visualization of samples in MVTec AD. The produced anomaly maps superimposed on the images. Anomaly region of high anomaly score is colored with \\textcolor{orange}{orange}. The \\textcolor{red}{red} boundary denotes contours of actual segmentation maps for anomalies.}\n\t\\label{Fig1}\n\\end{figure}",
            "Fig:speed": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.46\\textwidth]{figure_fps_i_auroc.pdf} \n\t\\caption{Inference speed (FPS) versus I-AUROC on MVTec AD benchmark. SimpleNet outperforms all previous methods on both accuracy and efficiency by a large margin.}\n\t\\label{Fig:speed}\n\\end{figure}",
            "Fig2": "\\begin{figure*}[t]\n\t\\centering\n\t\\includegraphics[width=0.8\\textwidth]{model_arch.pdf} % Reduce the figure size so that it is slightly narrower than the column.\n\t\\caption{Overview of the proposed SimpleNet. In the training phase, nominal samples are fed into a pre-trained \\textit{Feature Extractor} to get local features. Then, a \\textit{Feature Adaptor} is utilized to adapt pre-trained features into the target domain. Anomalous features are synthesized by adding Gaussian noise to the adapted features. The adapted features and the anomalous features and used as positive and negative samples respectively to train the final \\textit{Discriminator}. The \\textit{Anomalous Feature Generator} is removed at inference.}\n\t\\label{Fig2}\n\\end{figure*}",
            "Fig:FA": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth] {figure_features_stat.pdf}\n\t\\caption{Histogram of standard deviation along each dimension of local feature and adapted feature. The adapted feature space becomes more compact when training with anomalous features.}\n\t\\label{Fig:FA}\n\\end{figure}",
            "Fig4": "\\begin{figure*}[t]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{figure_std_i_auroc_classes.pdf}\n\t\\hspace{0.5in}\n\t\\includegraphics[width=0.4\\textwidth]{figure_std_p_auroc_classes.pdf}\n\t\\caption{I-AUROC\\%  and P-AUROC\\% for each class of MVTec AD dataset with varied $\\sigma$. (Best viewed in color.)}\n\t\\label{Fig4}\n\\end{figure*}",
            "Fig9": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{figure_auroc_patchsize.pdf} \n\t\\caption{Performance with varied patch sizes on MVTec AD.}\n\t\\label{Fig9}\n\\end{figure}",
            "Fig6": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{figure_loss.pdf}\n\t\\caption{Visualization of loss during training. The plotted lines show the mean loss for all classes in the MVTec AD dataset. The transparent color shows the range of loss fluctuation.}\n\t\\label{Fig6}\n\\end{figure}",
            "Fig8": "\\begin{figure*}[t]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{qualitative_results.pdf} \n\t\\caption{Qualitative results, where sampled image, ground truth, and anomaly map are shown for each class in MVTec AD.}\n\t\\label{Fig8}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation} \\label{eq1} \\small\n\t\\begin{split}\n\t\t\\mathcal{N}_{p}^{(h,w)} =  \\{ (h',y')| \n\t\t& h'\\in \\left [ h-\\left \\lfloor  p/2\\right \\rfloor,...,h+\\left \\lfloor p/2 \\right \\rfloor  \\right ], \\\\\n\t\t& y'\\in \\left [ w-\\left \\lfloor  p/2\\right \\rfloor,...,w+\\left \\lfloor p/2 \\right \\rfloor  \\right ] \\} \n\t\\end{split}\n\\end{equation}",
            "eq:2": "\\begin{equation} \\label{eq2} \\small\n\tz^{l,i}_{h,w} = f_{agg}(\\{\\phi^{l,i}_{h',y'}|(h',y')\\in\\mathcal{N}_{p}^{h,w}\\})\n\\end{equation}",
            "eq:eq3": "\\begin{equation}\\label{eq3} \\small\n\to^{i} = f_{cat}({resize(z^{l', i}, (H_{0}, W_{0}))|l'\\in L}\n\\end{equation}",
            "eq:3": "\\begin{equation} \\small\n\to^{i} = F_{\\phi}(x^{i})\n\\end{equation}",
            "eq:4": "\\begin{equation} \\label{eq6} \\small\n\tq^{i}_{h, w}=G_{\\theta}(o^{i}_{h,w})\n\\end{equation}",
            "eq:5": "\\begin{equation} \\small\n\tq^{i-}_{h, w} = q^{i}_{h, w} + \\epsilon\n\\end{equation}",
            "eq:6": "\\begin{equation} \\label{eq7} \\small\n\tl^{i}_{h,w} = \\max(0, th^{+}-D_{\\psi}(q^{i}_{h,w})) + \\max(0, -th^{-}+D_{\\psi}(q^{i-}_{h,w}))\n\\end{equation}",
            "eq:7": "\\begin{equation} \\label{eq9} \\small\n\t\\mathcal{L} = \\min\\limits_{\\theta, \\psi}\\sum_{x^{i}\\in\\mathcal{X}_{train}}\\sum_{h,w}\\frac{{}l^{i}_{h,w}}{H_{0}*W_{0}}\n\\end{equation}",
            "eq:8": "\\begin{equation} \\small\n\ts^{i}_{h, w} = -D_{\\psi}(q^{i}_{h,w})\n\\end{equation}",
            "eq:9": "\\begin{equation}  \\label{eq18} \\small\n\tS_{AL}(x_i):= \\{s^{i}_{h, w}|(h,w)\\in W_0\\times H_0\\}\n\\end{equation}",
            "eq:10": "\\begin{equation}  \\label{eq19} \\small\n\tS_{AD}(x_i):=\\max_{(h,w)\\in W_0\\times H_0} s^i_{h,w}\n\\end{equation}"
        },
        "git_link": "https://github.com/DonaldRR/SimpleNet"
    }
}