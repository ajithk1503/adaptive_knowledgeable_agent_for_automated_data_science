{
    "meta_info": {
        "title": "Serverless Federated AUPRC Optimization for Multi-Party Collaborative  Imbalanced Data Mining",
        "abstract": "Multi-party collaborative training, such as distributed learning and\nfederated learning, is used to address the big data challenges. However,\ntraditional multi-party collaborative training algorithms were mainly designed\nfor balanced data mining tasks and are intended to optimize accuracy\n(\\emph{e.g.}, cross-entropy). The data distribution in many real-world\napplications is skewed and classifiers, which are trained to improve accuracy,\nperform poorly when applied to imbalanced data tasks since models could be\nsignificantly biased toward the primary class. Therefore, the Area Under\nPrecision-Recall Curve (AUPRC) was introduced as an effective metric. Although\nsingle-machine AUPRC maximization methods have been designed, multi-party\ncollaborative algorithm has never been studied. The change from the\nsingle-machine to the multi-party setting poses critical challenges.\n  To address the above challenge, we study the serverless multi-party\ncollaborative AUPRC maximization problem since serverless multi-party\ncollaborative training can cut down the communications cost by avoiding the\nserver node bottleneck, and reformulate it as a conditional stochastic\noptimization problem in a serverless multi-party collaborative learning setting\nand propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to\ndirectly optimize the AUPRC. After that, we use the variance reduction\ntechnique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based\nvariance reduction (SLATE-M) algorithm to improve the convergence rate, which\nmatches the best theoretical convergence result reached by the single-machine\nonline method. To the best of our knowledge, this is the first work to solve\nthe multi-party collaborative AUPRC maximization problem.",
        "author": "Xidong Wu, Zhengmian Hu, Jian Pei, Heng Huang",
        "link": "http://arxiv.org/abs/2308.03035v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nMulti-party collaborative learning, such as distributed learning \\cite{dean2012large, li2014scaling, bao2022doubly} (typically focus on IID data and train learning model using the gradients from different parties) and federated learning \\cite{mcmahan2017communication} (focus on non-IID data and train model via periodically averaging model parameters from different parties coordinated by the server), have been actively studied at past decades to train large-scale deep learning models in a variety of real-world applications, such as computer vision \\cite{goyal2017accurate, wu2022retrievalguard}, natural language processing \\cite{devlin2018bert}, generative modeling \\cite{brock2018large} and other areas \\cite{li2023referring, ma2022traffic, li2022bridging, zhang2022many}. In literature, multi-party collaborative learning is also often called decentralized learning (compared to centralized learning in the single-machine setting). With different network topology, serverless algorithms could be converted into different multi-party collaborative algorithms (seen in \\ref{sec:3:1}). \nOn the other hand, although there are many ground-breaking studies with DNN in data classification \\cite{goyal2017accurate, wei2021direct, wu2022adversarial, sun2022demystify}, most works focus on balanced data sets, optimize the cross entropy, and use accuracy to measure model performance. From the viewpoint of optimization, the cross entropy between the estimated probability distribution based on the output of deep learning models and encoding ground-truth labels is a surrogate loss function of the misclassification rate/accuracy. However, in many real-world applications, such as healthcare and biomedicine \\cite{joachims2005support, davis2006relationship, zhang2022toward}, where patients make up a far smaller percentage of the population than healthy individuals, the data distribution is frequently skewed due to the scarce occurrence of positive samples. The data from the majority class essentially define the result, and the accuracy fails to be an appropriate metric to assess classifiers' performance. As a result,  areas under the curves (AUC), including  area under the receiver operating curve (AUROC) and area under precision-recall curves (AUPRC) are given much attention since it excels at discovering models with strong predictive power in imbalanced binary classification \\cite{cortes2003auc, ji2023prediction}. \n\nThe prediction performance of models, which are trained with cross entropy as the loss function for imbalanced binary classification, may be subpar because cross-entropy is not the surrogate function of AUC, which call for the study of AUC maximization. Recent works have achieved remarkable progress in directly optimizing AUROC with single-machine and multi-party training algorithms \\cite{zhao2011online, liu2019stochastic}. \\citet{liu2019stochastic} constructed deep AUC as a minimax problem and resolved the stochastic AUC maximization problem with a deep neural network as the classifier. Recently, \\citet{yuan2021federated} and \\citet{guo2020communication} extended the single-machine training to federated learning and proposed a PL-strongly-concave minimax optimization method to maximize AUROC. \n\nHowever, AUROC is not suitable for data with a much larger number of negative examples than positive examples, and AUPRC can address this issue because it doesnâ€™t rely on true negatives. Given that an algorithm that maximizes AUROC does not necessarily maximize AUPRC \\cite{davis2006relationship} and matching of loss and metric is important \\cite{dou2022learning, ma2020statistical, dou2022sampling}, the design of AUPRC maximization algorithms has attracted attention  \\cite{qi2021stochastic,wang2021momentum, wu2022fast, jiang2022multi,why2022finite}.\nNonetheless, the  multi-party algorithm for AUPRC maximization problems has not been studied. Existing AUPRC optimization methods cannot be directly applied to  multi-party collaborative training, since they mainly focus on the finite-sum problem and maintain an inner state for each positive data point, which is not permitted in a multi-party online environment. In addition, to improve communication efficiency, serverless multi-party collaborative learning algorithms are needed to avoid the server node bottleneck in model training. Thus, it is desired to develop efficient stochastic optimization algorithms for serverless multi-party AUPRC maximization for deep learning to meet the challenge of large-scale imbalanced data mining.\n\nThe challenges to design serverless multi-party collaborative AUPRC maximization algorithm are three-fold. The first difficulty lies in the complicated integral definition. \nTo overcome the problem of the continuous integral, we can use some point estimators. \n% Several estimators of AUPRC have been presented in previous works \\cite{brown2020smooth, boyd2013area}. \nThe average precision (AP) estimator is one of the most popularly used estimators. AP can be directly calculated based on the sample prediction scores and is not subject to sampling bias. It is ideally suited to be used in stochastic optimization problems due to these advantages. \n\nThe second difficulty lies in the nested structure and the non-differential ranking functions in the AP. Traditional gradient-based gradient descent techniques cannot directly be used with the original concept of AP. Most existing optimization works use the surrogate function to replace the ranking function in the AP function \\cite{liu2019stochastic, guo2020communication, qi2021stochastic,wang2021momentum, wu2022fast,jiang2022multi}. We can follow these works and substitute a surrogate loss for the ranking function in the AP function. \n\nThe third difficulty is that existing algorithms only focus on finite-sum settings and maintain inner estimators $u_t$ for each positive data point, which is not permitted in multi-party collaborative online learning.\n% due to the dependence on the specific local data point. \nTherefore, despite recent developments, it is still unclear if there is a strategy to optimize AUPRC for multi-party collaborative imbalanced data mining. It is natural to ask the following question: \\textbf{Can we design multi-party stochastic optimization algorithms to directly maximize AUPRC with guaranteed convergence?}\n% \\begin{center}\n% \\begin{tcolorbox}\n% [colback=green!5!white,colframe=lime!75!black]\n% \\vspace{-0.05in}\n% \\textbf{ Can we design multi-party stochastic optimization algorithms to directly maximize AUPRC with guaranteed convergence? }\n% \\vspace{-0.05in}\n% \\end{tcolorbox}\n% \\end{center}\n\nIn this paper, we provide an affirmative answer to the aforementioned question. We propose the new algorithms for multi-party collaborative AUPRC maximization and provide systematic analysis. Our main contributions can be summarized as follows:\n%\\textbf{Contributions}:\n\\begin{itemize}[leftmargin=5.5mm]\n\\item  We cast the AUPRC maximization problem into non-convex conditional stochastic optimization problem by substituting a surrogate loss for the indicator function in the definition of AP. Unlike existing methods that just focus on finite-sum settings, we consider the stochastic online setting.\n\n\\item We propose the first multi-party collaborative learning algorithm, ServerLess biAsed sTochastic gradiEnt (SLATE), to solve our new objective. It can be used in an online environment and has no reliance on specific local data points. In addition, with different network topologies, our algorithm can also be used for distributed learning and federated learning.\n\n\\item  Furthermore, we propose a stochastic method (\\emph{i.e.}, SLATE-M) based on the momentum-based variance-reduced technique to reduce the convergence complexity in multi-party collaborative  learning. Our method can reach iteration complexity of $O\\left(1 / \\epsilon^{5}\\right)$, which matches the lower bound proposed in the single-machine conditional stochastic optimization.\n\n\\item Extensive experiments on various datasets compared with baselines verify the effectiveness of our methods.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Related work",
                "content": "\n% This section briefly reviews related work on AUROC and AUPRC maximization, and stochastic serverless multi-party collaborative learning, respectively.\n\n",
                "subsection 2.1": {
                    "name": "AUROC Maximization",
                    "content": "\nThere is a long line of research that investigated the imbalanced data mining with AUROC metric \\cite{zhao2011online,ying2016stochastic,liu2019stochastic,yuan2021federated,guo2020communication}, which highlight the value of the AUC metric in imbalanced data mining.  Earlier works about AUROC focused on linear models with pairwise surrogate\nlosses \\cite{joachims2005support}. Furthermore, \\citet{ying2016stochastic} solved the AUC square surrogate loss using a stochastic gradient descent ascending approach and provided a minimax reformulation of the loss to address the scaling problem of AUC optimization. Later, \\citet{liu2019stochastic} studied the application of AUROC in deep learning and reconstructed deep AUC as a minimax problem, which offers a strategy to resolve the stochastic AUC maximization problem with a deep neural network as the predictive model. Furthermore, some methods were proposed for multi-party AUROC maximization. \\citet{yuan2021federated} and \\citet{guo2020communication} reformulated the federated deep AUROC maximization as non-convex-strongly-concave problem in the federated setting. However, the analyses of methods in \\cite{yuan2021federated} and \\cite{guo2020communication} rely on the assumption of PL condition on the deep models. Recently, \\cite{yuan2022compositional} developed the compositional deep\nAUROC maximization model and \\cite{zhang2023federated} extend it to federated learning.\n"
                },
                "subsection 2.2": {
                    "name": "AUPRC Maximization",
                    "content": "\nEarly works about AUPRC optimization mainly depend on traditional optimization techniques. Recently, \\citet{qi2021stochastic} analyzed AUPRC maximization with deep models in the finite-sum setting. They use a surrogate loss to replace the ranking function in the AP function and maintain biased estimators of the surrogate ranking functions for each positive data point. They proposed the algorithm to directly optimize AUPRC and show a guaranteed convergence. \nAfterward, \\citet{wang2021momentum} presented adaptive and non-adaptive methods (\\emph{i.e.} ADAP and MOAP) with a new strategy to update the biased estimators for each data point. The momentum average is applied to both the outer and inner estimators to track individual ranking scores. More recently, algorithms proposed in \\cite{why2022finite} reduce convergence complexity with the parallel speed-up and \\citet{wu2022fast, jiang2022multi} introduced the momentum-based variance-reduction technology into AUPRC maximization to reduce the convergence complexity. While we developed distributed AUPRC optimization concurrently with \\cite{guo2023fedx}, they pay attention to X-Risk Optimization in federated learning. Because X-Risk optimization is a sub-problem in conditional stochastic optimization and federated learning could be regarded as decentralized learning with a specific network topology (seen \\ref{sec:3:1}), our methods could also be applied to their problem.\n\nOverall, existing methods mainly focus on finite-sum single-machine setting \\cite{qi2021stochastic,wang2021momentum, wu2022fast, why2022finite, jiang2022multi}. To solve the biased stochastic gradient, they maintain an inner state for local each data point. However, this strategy limits methods to be applied to real-world big data applications because we cannot store an inner state for each data sample in the online environment. In addition, we cannot extend them directly from the single-machine setting to multi-party setting, because under non-IID assumption, the data point on each machine is different and this inner state can only contain the local data information and make it difficult to train a global model. \n\nIn the perspective of theoretical analysis, \\citet{hu2020biased} studied the general condition stochastic optimization and proposed two single-machine algorithms with and without using the variance-reduction technique (SpiderBoost) named BSGD and BSpiderboost, and established the lower bound at $\\varepsilon^{-5}$ in the online setting.\n\nAUPRC is widely utilized in binary classification tasks. It is simple to adapt it for multi-class classifications. If a task has multiple classes, we can assume that each class has a binary classification task and adopt the one vs. the rest classification strategy. We can then calculate average precision based on all classification results.\n\\vspace{-6pt}\n"
                },
                "subsection 2.3": {
                    "name": "Serverless Multi-Party Collaborative Learning",
                    "content": "\nMulti-party collaborative learning (i.e, distributed and federated learning) has wide applications in data mining and machine learning problems \\cite{wang2023applications, zhang2021low, mei2023mac, zhang2022learning, liu2022fast, luo2022multisource, he2023robust}. Multi-party collaborative learning in this paper has a more general definition that does not rely on the IID assumption of data to guarantee the convergence analysis. \nIn the last years, many serverless multi-party collaborative learning approaches have been put out because they avoid the communication bottlenecks or constrained bandwidth between each worker node and the central server, and also provide some level of data privacy \\cite{yuan2016convergence}. \\citet{lian2017can} offered the first theoretical backing for serverless multi-party collaborative training. Then serverless multi-party collaborative training attracts attention \\cite{tang2018d, lu2019gnsd, liu2020decentralized, xin2021hybrid} and the convergence rate has been improved using many different strategies, including variance extension \\cite{tang2018d}, variance reduction \\cite{pan2020d, zhang2021gt}, gradient tracking \\cite{lu2019gnsd}, and many more. \nIn addition, serverless multi-party collaborative learning has been applied to various applications, such as reinforcement learning \\cite{zhang2021taming, he2020data, }, robust training \\cite{xian2021faster}, generative adversarial nets (GAN) \\cite{liu2020decentralized},  robust\nprincipal component analysis \\cite{wu2023decentralized} and other optimization problems \\cite{liu2022interact, zhang2022net, zhang2019compressed,zhang2020private}. However, none of them focus on imbalanced data mining. \nIn application, the serverless multi-party collaborative learning setting in this paper is different from federated learning \\cite{mcmahan2017communication} which uses a central server with different communication mechanisms to periodically average the model parameters for indirectly aggregating data from numerous devices. \n%, such as in IoT applications, not for training large-scale deep learning models with big data.\nHowever, with a specific network topology (seen \\ref{sec:3:1}), federated learning could be regarded as multi-party collaborative learning. Thus, our algorithms could be regarded as a general federated AUPRC maximization.\n\n"
                }
            },
            "section 3": {
                "name": "Preliminary",
                "content": "\n",
                "subsection 3.1": {
                    "name": "Serverless Multi-Party Collaborative Learning",
                    "content": " \\label{sec:3:1}\n\\textbf{Notations}: \nWe use $\\mathbf{x}$ to denote a collection of all local model parameters $\\mathbf{x}_n$, where $n \\in [N]$, \\emph{i.e.}, $\\mathbf{x} = [x_{1}^{\\top}, x_{2}^{\\top}, \\dots, x_{N}^{ \\top}]^{\\top} \\in \\mathbb{R}^{N d}$. Similarly, we define $\\mathbf{u}, \\mathbf{v}$ as the concatenation of $\\mathbf{u}_n, \\mathbf{v}_n$ for $n \\in [N]$. In addition, $\\otimes$ denotes the Kronecker product, and\n% Define $\\bar{\\mathbf{x}} = \\mathbf{1} \\otimes \\bar{x}$, $\\bar{\\mathbf{u}} = \\mathbf{1} \\otimes \\bar{\\mathbf{u}}$, $\\bar{\\mathbf{v}} = \\mathbf{1} \\otimes \\bar{v}$.  \n$\\|\\cdot\\|$ denotes the $\\ell_2$ norm for vectors, respectively. \n$\\mathcal{D}_n^{+}$ denotes the positive dataset on the $N$ worker nodes and $\\mathcal{D}$ denotes the whole dataset on the $n$ worker nodes.\n% For two vectors $x$ and $y$, $\\langle x,y\\rangle$ denotes their inner product. \nIn multi-party collaborative\ntraining, the network system of N worker nodes $\\mathcal{G} = (\\mathcal{V},  \\mathcal{E}) $ is represented by double stochastic matrix $\\underline{\\mathbf{W}} = \\{\\underline{w}_{ij} \\}  \\in \\mathbb{R}^{N \\times N}$, which is defined as follows: (1) if there exists a link between node i and node j, then $\\underline{w}_{ij} > 0$, otherwise $\\underline{w}_{ij}$ = 0, (2) $\\underline{\\mathbf{W}} = \\underline{\\mathbf{W}}^{\\top}$ and (3) $\\underline{\\mathbf{W}} \\mathbf{1} = \\mathbf{1}$ and $\\mathbf{1}^{\\top} \\underline{\\mathbf{W}} = \\mathbf{1}^{\\top}$. We define the second-largest eigenvalue of $\\mathbf{W}$ as $\\lambda$ and $\\mathbf{W} := \\underline{\\mathbf{W}} \\otimes \\mathbf{I}_{d}$. We denote the exact averaging matrix as $\\mathbf{J} = \\frac{1}{N}(\\mathbf{1}_n \\mathbf{1}_n^{\\top}) \\otimes \\mathbf{I}_d  $ and $\\lambda = \\| \\mathbf{W} - \\mathbf{J} \\|$. Taking ring network topology as an example, where each node can only exchange information with its two neighbors. The corresponding W is in the form of\n\\begin{align}\n\\small\n\\underline{\\mathbf{W}} =\\left(\\begin{array}{cccccc}\n1 / 3 & 1 / 3 &  & &  & 1 / 3 \\\\\n1 / 3 & 1 / 3 & 1 / 3 & & & \\\\\n& 1 / 3 & 1 / 3 & \\ddots & & \\\\\n& & \\ddots & \\ddots & 1 / 3 & \\\\\n& & & 1 / 3 & 1 / 3 & 1 / 3 \\\\\n1 / 3 & & & & 1 / 3 & 1 / 3\n\\end{array}\\right) \\in \\mathbb{R}^{N \\times N} \\nonumber\n\\end{align}\nIf we change the network topology, multi-Party collaborative\nlearning could become different types of multi-party collaborative training. If $\\mathbf{W}$ is $\\frac{1}{N} \\mathbf{1} \\mathbf{1}^{\\top}$, it is converted to distributed learning with the average operation in each iteration. If we choose $\\mathbf{W}$  as the Identity matrix and change it to $\\frac{1}{N} \\mathbf{1} \\mathbf{1}^{\\top}$ every q iteration, it would be federated learning.\n\n"
                },
                "subsection 3.2": {
                    "name": "AUPRC",
                    "content": "\nAUPRC can be defined as the following integral problem \\cite{bamber1975area}:\n\\begin{align} \n\\mathrm{AUPRC} =\\int_{-\\infty}^{\\infty} \\operatorname{Pr}(y = 1 \\mid h(\\mathbf{x}; \\mathbf{z}) \\geq c) d \\operatorname{Pr}(h(\\mathbf{x}; \\mathbf{z}) \\leq c \\mid y=1) \\nonumber\n\\end{align}\nwhere $h(\\mathbf{x}; \\mathbf{z})$ is the prediction score function, $\\mathbf{x}$ is the model parameter, $\\mathbf{\\xi} = \\left(\\mathbf{z}, y\\right)$ is the data point, and $Pr(y = 1 | h(\\mathbf{x}; \\mathbf{z}) \\geq c)$ is the precision at the threshold value of c.\n\nTo overcome the problem of the continuous integral, we use AP as the estimator to approximate AUPRC, which is given by \\cite{boyd2013area}:\n\\begin{equation}  \\label{eq:1}\n\\mathrm{AP} = \\mathbb{E}_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} Precision \\left(h\\left(\\mathbf{x}; \\mathbf{z} \\right)\\right) \n= \\mathbb{E}_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} \\frac{\\mathrm{r}^{+} \\left(\\mathbf{x}; \\mathbf{z}\\right)}{\\mathrm{r}\\left(\\mathbf{x}; \\mathbf{z}\\right)} \n\\end{equation}\nwhere $\\mathcal{D}^{+}$ denotes the positive dataset, and samples $\\mathbf{\\xi} = \\left(\\mathbf{z}, y\\right)$ are drawn from positive dataset $\\mathcal{D}^{+}$ where $\\mathbf{z} \\in \\mathcal{Z}$ represents the data features and $y = +1$ is the positive label. $\\mathrm{r}^{+}$ denotes the positive data rank ratio of  prediction score (\\emph{i.e.}, the number of positive data points with no less prediction score than that of $\\mathbf{\\xi}$  including itself over total data number)  and $\\mathrm{r}$ denotes its prediction score rank ratio among all data points (\\emph{i.e.}, the number of data points with no less prediction score than that of $\\mathbf{\\xi}$  including itself  over total data number).  $\\mathcal{D}$ denotes the whole datasets and $\\mathbf{\\xi}^{\\prime} = (\\mathbf{z}^{\\prime}, y^{\\prime}) \\sim \\mathcal{D}$ denote a random data drawn from an unknown distribution $\\mathcal{D}$, where $\\mathbf{z}^{\\prime} \\in \\mathcal{Z}$ represents the data features and $y^{\\prime} \\in \\mathcal{Y} = \\{-1, +1\\}$. Therefore, \\eqref{eq:1} is the same as:\n\\begin{align} \n\\mathrm{AP} = \\mathbb{E}_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} \\frac{\\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim  \\mathcal{D}} \\mathbf{I} (h(\\mathbf{x}; \\mathbf{z}^{\\prime}) \\geq h(\\mathbf{x}; \\mathbf{z})) \\cdot \\mathbf{I}\\left(y^{\\prime} = 1\\right)}{\\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim \\mathcal{D}} \\mathbf{I} (h(\\mathbf{x}; \\mathbf{z}^{\\prime}) \\geq h(\\mathbf{x}; \\mathbf{z}))} \\nonumber\n\\end{align}\nWe employ the following squared hinge loss:\n\\begin{align} \\label{eq:2}\n\\ell\\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime} \\right) = (max\\{s - h(\\mathbf{x}; \\mathbf{z}) + h(\\mathbf{x}; \\mathbf{z}^{\\prime}), 0\\})^2\n\\end{align}\nas the surrogate for the indicator function $\\mathbf{I} (h(\\mathbf{x}; \\mathbf{z}^{\\prime}) \\geq h(\\mathbf{x}; \\mathbf{z}))$, where $s$ is a margin parameter, that is a common choice used by previous studies\n\\cite{qi2021stochastic, wang2021momentum, jiang2022multi}. As a result, the AUPRC maximization problem can be formulated as:\n\\begin{align} \nAP = \\mathbb{E}_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} \\frac{\\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim  \\mathcal{D}} \\mathbf{I}\\left(y^{\\prime}=1\\right)  \\ell \\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}{\\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim \\mathcal{D}} \\quad \\ell\\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}  \\nonumber\n\\end{align}\nIn the finite-sum setting, it is defined as :\n\\begin{align} \nAP = \\frac{1}{| \\mathcal{D}^{+}|} \\sum_{\\mathbf{x}_{i}, y_{i}=1} \\frac{\\mathrm{r}^{+}\\left(\\mathbf{x}_{i}\\right)}{\\mathrm{r}\\left(\\mathbf{x}_{i}\\right)} = \\frac{1}{| \\mathcal{D}^{+}|} \\sum_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} \\frac{ \n \\frac{1}{| \\mathcal{D}|} \\sum_{\\mathbf{\\xi} \\sim  \\mathcal{D}}\n\\mathbf{I}\\left(y^{\\prime}=1\\right)  \\ell \\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}{ \\frac{1}{| \\mathcal{D}|} \\sum_{\\mathbf{\\xi} \\sim  \\mathcal{D}} \\quad \\ell\\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}  \\nonumber\n\\end{align}\nFor convenience, we define the elements in $g(\\mathbf{x})$ as the surrogates of the two prediction score ranking function $\\mathrm{r}^{+}\\left(\\mathbf{x}\\right)$ and $\\mathrm{r} \\left(\\mathbf{x}\\right)$ respectively. Define the following equation: \n\\begin{align} \ng\\left(\\mathbf{x}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right)=\n\\begin{bmatrix}\ng^1\\left(\\mathbf{x}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\\\\ng^2\\left(\\mathbf{x}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right)\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\ell\\left(\\mathbf{x} ; \\mathbf{z}, \\mathbf{z}^{\\prime} \\right) \\mathbf{I}\\left(y^{\\prime}=1\\right) \\\\\n\\ell\\left(\\mathbf{x} ; \\mathbf{z}, \\mathbf{z}^{\\prime} \\right) \n\\end{bmatrix} \\nonumber\n\\end{align}\nand $g(\\mathbf{x}; \\mathbf{\\xi}) = \\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim \\mathcal{D}} g\\left(\\mathbf{x} ; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime}\\right) \\in \\mathbb{R}^{2} $, and assume $f(\\mathbf{u}) = -\\frac{u_1}{u_2}:\\mathbb{R}^{2} \\mapsto \\mathbb{R}$ for any $\\mathbf{u} = [u_1, u_2]^{\\top} \\in  \\mathbb{R}^{2} $. \nWe reformulate the optimization objective into the following stochastic optimization problem:\n\\begin{align} \\label{eq:3}\n\\min_{\\mathbf{x}} F (\\mathbf{x}) &=\\mathrm{E}_{\\mathbf{\\xi} \\sim \\mathcal{D}^{+}}\\left[f(g(\\mathbf{x}; \\mathbf{\\xi}) \\right] \\nonumber\\\\\n&=\\mathrm{E}_{\\mathbf{\\xi} \\sim \\mathcal{D}^{+}} \\left[f(\\mathrm{E}_{\\mathbf{\\xi}^{\\prime} \\sim \\mathcal{D}} g(\\mathbf{x}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime}))\\right]\n\\end{align}\nIt is similar to the two-level conditional stochastic optimization \\cite{hu2020biased}, where the inner layer function depends on the data points sampled from both inner and outer layer functions. Given that $f(\\cdot)$ is a nonconvex function, problem \\eqref{eq:3} is a noncvonex optimiztion problem. In this paper, we considers serverless multi-party collaborative non-convex optimization where N worker nodes cooperate to solve the following problem:\n\\begin{align} \\label{eq:4}\n\\min_{\\mathbf{x}} F (\\mathbf{x}) &= \\min_{\\mathbf{x}} \\frac{1}{N} \\sum_{n=1}^n F_{n}(\\mathbf{x})\n\\end{align}\nwhere $F_{n}(\\mathbf{x}) =\n\\mathbb{E}_{\\xi_n \\sim \\mathcal{D}_n^{+}} f(\\mathbb{E}_{\\xi^{\\prime}_n \\sim \\mathcal{D}_n} g_n(\\mathbf{x}; \\xi_n, \\xi^{\\prime}_n))$ where $\\mathbf{\\xi}_n^{\\prime} = (\\mathbf{z}_n^{\\prime}, y_n^{\\prime}) \\sim \\mathcal{D}_n$ and $\\mathbf{\\xi}_n = (\\mathbf{z}_n, y_n) \\sim \\mathcal{D}^{+}$  \nWe consider heterogeneous data setting in this paper, which refers to a situation where $\\mathcal{D}_{i}$ and $\\mathcal{D}_{j}$ are different ($i \\neq j$ ) on different worker nodes.\n\nIn order to design the method, we first consider how to compute the gradient of $F(\\mathbf{x})$.\n\\begin{equation} \\label{eq:8}\n\\begin{aligned}\n\\nabla F_n(\\mathbf{x})= & \\mathbb{E}_{\\xi_n \\sim \\mathcal{D}_n^{+}} \\nabla g_n(\\mathbf{x}; \\xi_n)^{\\top} \\nabla f\\left(g_n(\\mathbf{x}; \\xi_n)\\right) \\\\\n= & \\mathbb{E}_{\\xi_n \\sim \\mathcal{D}_n^{+}} \\nabla g_n(\\mathbf{x}; \\xi_n)^{\\top}\\left(\\frac{-1}{ g^{2}_n(\\mathbf{x}; \\xi_n)}, \\frac{ g^{1}_n(\\mathbf{x}; \\xi_n) }{\\left( g^{2}_n(\\mathbf{x}; \\xi_n)\\right)^{2}}\\right)^{\\top} \\nonumber\n\\end{aligned}\n\\end{equation}\nwhere \n\\begin{align} \n\\nabla  g_n(\\mathbf{x}; \\xi_n) &= \n\\begin{bmatrix}\n\\nabla  g^{1}_n(\\mathbf{x}; \\xi_n) \\\\\n\\nabla  g^{2}_n(\\mathbf{x}; \\xi_n)\n\\end{bmatrix}\n\\nonumber\\\\\n&= \\begin{bmatrix}\n\\mathbb{E}_{\\xi^{\\prime}_n \\sim \\mathcal{D}_n} \\mathbf{I}\\left(y_n^{\\prime}=1\\right) \\nabla \\ell\\left(\\mathbf{x} ; \\mathbf{z}_n, \\mathbf{z}_n^{\\prime}\\right) \\\\\n\\mathbb{E}_{\\xi^{\\prime}_n \\sim \\mathcal{D}_n} \\nabla \\ell\\left(\\mathbf{x}; \\mathbf{z}_n, \\mathbf{z}_n^{\\prime}\\right) \\nonumber\n\\end{bmatrix}\n\\end{align}\n\nWe can notice that it is different from the standard gradient since there are two levels of functions and the inner function also depends on the sample data from the outer layer. Therefore, the stochastic gradient estimator is not an unbiased estimation for the full gradient. Instead of constructing an unbiased stochastic estimator of the gradient \\cite{sun2023scheduling}, we consider a biased estimator of $\\nabla F_n(x)$ using one sample $\\xi$ from $\\mathcal{D}^{+}_n$ and $m$ sample $\\xi^{\\prime}$ from $\\mathcal{D}_n$ as $\\mathcal{B}_{n}$ in the following form:\n\\begin{align} \\label{eq:5}\n&\\nabla \\hat{F}_n\\left(\\mathbf{x}; \\xi_n, \\mathcal{B}_{n}\\right) \\\\\n=&(\\frac{1}{m} \\sum_{\\xi^{\\prime} \\in \\mathcal{B}_{n}} \\nabla g_n(\\mathbf{x}; \\xi_n, \\xi^{\\prime}_n))^{\\top} \\nabla f (\\frac{1}{m} \\sum_{\\xi^{\\prime}_n \\in \\mathcal{B}_{n}} g_n(\\mathbf{x}; \\xi_n, \\xi^{\\prime}_n))   \\nonumber \n\\end{align}\nwhere $\\mathcal{B}_{n} = \\left\\{\\xi^{\\prime  j}\\right\\}_{j=1}^m$.\nIt is observed that $\\nabla \\hat{F}_n\\left(\\mathbf{x}; \\xi_n, \\mathcal{B}_{n} \\right)$ is the gradient of an empirical objective such that\n\\begin{align}\n\\hat{F}_n\\left(\\mathbf{x}; \\xi_n, \\mathcal{B}_{n} \\right):=f_n \\left(\\frac{1}{m} \\sum_{\\xi^{\\prime}_n \\in \\mathcal{B}_{n}} g_n(\\mathbf{x}; \\xi_n, \\xi^{\\prime}_n)\\right) \\,.\\nonumber\n\\end{align}\n\n"
                }
            },
            "section 4": {
                "name": "Algorithms",
                "content": "\nIn this section, we propose the new serverless multi-party collaborative learning algorithms for\nsolving the problem \\eqref{eq:4}. Specifically, we use the gradient tracking technique ( which could be ignored in practice) and propose a ServerLess biAsed sTochastic gradiEnt (SLATE). We further propose an accelerated version of SLATE with momentum-based variance reduction \\citep{cutkosky2019momentum} technology (SLATE-M).\n\n",
                "subsection 4.1": {
                    "name": "Serverless Biased Stochastic Gradient (SLATE)",
                    "content": "\n\nBased on the above analysis, we design a serverless multi-party collaborative algorithms with biased stochastic gradient and is named SLATE. \\Cref{alg:1} shows the algorithmic framework of the SLATE. \\textbf{Step 8 could be ignored in practice}.\n\n\\setlength{\\textfloatsep}{0.15cm}\n\\begin{algorithm}[!t]\n\\caption{SLATE Algorithm }\n\\label{alg:1}\n\\begin{algorithmic}[1] %[1] enables line numbers\n\\STATE {\\bfseries Input:} $T$, step size $ \\eta$ inner batch size $m$ and mini-batch size $b$; \n$\\mathbf{u}_{n, 0} = 0$ and $\\mathbf{v}_{n, 0} = 0$ for $n \\in \\{1, \\cdots, N\\}$\n\\STATE {\\bfseries Initialize:} $x_{n, 0} = \\frac{1}{N} \\sum_{k=1}^{N} x_{n, 0}$. \\\\\n\\FOR{$t = 0, 1, \\ldots, T$}\n\\FOR{$n = 1, 2, \\ldots, N$}\n\\STATE Draw $b$ samples $\\mathcal{B}^{+}_{n, t} = \\{\\xi^i_{n, t}\\}_{i = 1}^{b}$ from $\\mathcal{D}^{+}_n$ \\\\\n\\STATE  Draw $m$ samples $\\mathcal{B}_{n, t} = \\left\\{\\xi^{\\prime  j}_{n, t}\\right\\}_{j=1}^m$ from $\\mathcal{D}_n$,\n\\\\\n\\STATE $\\mathbf{u}_{n, t} = \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n,t}, \\mathcal{B}_{n, t})$ as in \\eqref{eq:6} \\\\\n\\STATE $\\mathbf{v}_{n, t} = \\sum_{r=1}^{N} \\underline{w}_{nr} (\\mathbf{v}_{t - 1}^r + \\mathbf{u}_t^r - \\mathbf{u}_{t - 1}^r)$ \\\\\n\\STATE $\\mathbf{x}_{n, t+1} = \\sum_{r = 1}^{N} \\underline{w}_{nr} (\\mathbf{x}_{t}^{n} -  \\eta \\mathbf{v}_{n, t})$\\\\\n\\ENDFOR\n\\ENDFOR\n\\STATE {\\bfseries Output:} $x$ chosen uniformly random from $\\{\\bar{\\mathbf{x}}_t\\}_{t=1}^{T}$.\n\\end{algorithmic}\n\\end{algorithm}\n\nAt the beginning of Algorithm \\ref{alg:1}, one simply initializes local model parameters $\\mathbf{x}$ for all worker nodes. \nGiven the couple structure of problem \\eqref{eq:4}. We can assign the value of gradient estimator $\\mathbf{u}_{n, 0}$ and gradient tracker $\\mathbf{v}_{n, 0}$ as 0. \n\nAt the Lines 5-6 of Algorithm \\ref{alg:1}, we draw $b$ samples as $\\mathcal{B}^{+}_{n, t}$ from positive dataset $\\mathcal{D}_n^{+}$ and $m$ samples as $\\mathcal{B}_{n, t}$from full data sets $\\mathcal{D}_n$ on each node, respectively. We use a biased stochastic gradient to update the gradient estimator $\\mathbf{u}_{n,t}$ according to the \\eqref{eq:6}.\n\\begin{align} \\label{eq:6}\n&\\mathbf{u}_{n, t} = \\\\\n& \\sum_{\\mathbf{\\xi} \\in \\mathcal{B}^{+}_{n,t}} \\sum_{\\mathbf{\\xi}^{\\prime} \\in \\mathcal{B}_{n , t}} \\frac{\\left(g^1\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) - g^2\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\mathbf{I}\\left(\\mathbf{y}^{\\prime}=1\\right)\\right) \\nabla \\ell\\left(\\mathbf{x}_{n,t}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}{b m \\left( g^2\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\right)^2} \\nonumber\n\\end{align}\nwhere $\\mathbf{\\xi} = \\left(\\mathbf{z}, y\\right)$ and $\\mathbf{\\xi}^{\\prime} = \\left(\\mathbf{z}^{\\prime}, y^{\\prime}\\right)$\n\nAfterward, at the Line 8 of Algorithm \\ref{alg:1} (optional), we adopt the gradient tracking technique \\cite{lu2019gnsd} to reduce network consensus error, where we update the $\\mathbf{v}_{n, t}$ and then do the consensus step with double stochastic matrix $\\mathbf{W}$ as:\n\\begin{align}\n\\mathbf{v}_{n, t} = \\sum_{r=1}^{N} \\underline{w}_{nr} (\\mathbf{v}_{t - 1}^r + \\mathbf{u}_t^r - \\mathbf{u}_{t - 1}^r) \\nonumber\n\\end{align}\n\nFinally, at the Line 9 of Algorithm \\ref{alg:1}, we update the model with gradient tracker $\\mathbf{v}_{n, t}$, following the consensus step among worker nodes with double stochastic matrix $\\mathbf{W}$:\n\\begin{align}\n\\mathbf{x}_{n, t+1} = \\sum_{r = 1}^{N} \\underline{w}_{nr} (\\mathbf{x}_{r, t} -  \\eta \\mathbf{v}_{r, t}) \\nonumber\n\\end{align}\nThe output $\\bar{\\mathbf{x}}_t$ is defined as:\n$\n\\bar{\\mathbf{x}}_t = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n,t}\\,. \\nonumber\n$\n"
                },
                "subsection 4.2": {
                    "name": "SLATE-M",
                    "content": "\nFurthermore, we further propose an accelerated version of SLATE (SLATE-M) based on the momentum-based variance reduced technique, which has the better convergence complexity. The algorithm is shown in \\Cref{alg:2}. \\textbf{Step 11 could be ignored in practice}.\n\n\\begin{algorithm}[!t]\n\\caption{SLATE-M Algorithm }\n\\label{alg:2}\n\\begin{algorithmic}[1] %[1] enables line numbers\n\\STATE {\\bfseries Input:} $T$, step size $ \\eta$, momentum coefficient $\\alpha$, inner batch size $m$ and mini-batch size $b$, and initial batch size $B$; \n\\STATE {\\bfseries Initialize:} $\\mathbf{x}_{n, 0} = \\frac{1}{N} \\sum_{k=1}^{N} \\mathbf{x}_{n, 0}$ \n\\STATE  Draw $B$ samples of $\\{\\xi^i_{n, 0}\\}_{i=1}^B$ from $\\mathcal{D}^{+}_n$, and draw $m$ samples $\\mathcal{B}_{n, 0} = \\left\\{\\xi_{n,0}^{\\prime  j}\\right\\}_{j=1}^m$ from $\\mathcal{D}_n$, $\\mathbf{u}_{n, 0} = \\frac{1}{B} \\sum_{i=1}^{B} \\nabla \\hat{F}_n(\\mathbf{x}_{n, 0}; \\xi^i_{n, 0}, \\mathcal{B}_{n, 0}) \\forall n \\in [N]$\n\\\\\n\\STATE $\\mathbf{v}_{n, 0} = \\sum_{r=1}^{N} \\underline{w}_{nr} \\mathbf{u}_{r, 0} \\forall n \\in [N]$ \\\\\n\\STATE $\\mathbf{x}_{n, 1} = \\sum_{r = 1}^{N} \\underline{w}_{nr} (\\mathbf{x}_{n, 0} -  \\eta \\mathbf{v}_{n, 0}) \\forall n \\in [N]$\\\\\n\\FOR{$t = 1, 2, \\ldots, T$}\n\\FOR{$n = 1, 2, \\ldots, N$}\n\\STATE Draw $b$ samples  $\\mathcal{B}^{+}_{n, t} = \\{\\xi^0_{n, 1}, \\cdots, \\xi^{b}_{n, 1} \\}$ from $\\mathcal{D}^{+}_n$ \\\\\n\\STATE  Draw $m$ samples $\\mathcal{B}_{n, t} = \\left\\{\\xi_{n,t}^{\\prime  j}\\right\\}_{j=1}^m$ from $\\mathcal{D}_n$,\n\\\\\n\\STATE $\\mathbf{u}_{n, t} = \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}; \\xi^i_{n,t}, \\mathcal{B}_{n, t}) + (1 - \\alpha) (\\mathbf{u}_{n, t - 1}  - \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t - 1}; \\xi^i_{n,t}, \\mathcal{B}_{n, t}))$ \\\\\n\\STATE $\\mathbf{v}_{n, t} = \\sum_{r=1}^{N} \\underline{w}_{nr} (\\mathbf{v}_{t - 1}^r + \\mathbf{u}_t^r - \\mathbf{u}_{t - 1}^r)$ \\\\\n\\STATE $\\mathbf{x}_{n, t+1} = \\sum_{r = 1}^{N} \\underline{w}_{nr} (\\mathbf{x}_{t}^{n} -  \\eta \\mathbf{v}_{n, t}) $\\\\\n\\ENDFOR\n\\ENDFOR\n\\STATE {\\bfseries Output:} $x$ chosen uniformly random from $\\{\\bar{\\mathbf{x}}_t\\}_{t=1}^{T}$.\n\\end{algorithmic}\n\\end{algorithm}\n\n% \\begin{algorithm}[tb]\n% \\caption{DASCO Algorithm }\n% \\label{alg:2}\n% \\begin{algorithmic}[1] %[1] enables line numbers\n% \\STATE {\\bfseries Input:} $T$, Parameters: $c > 0, \\beta, \\eta_t, \\alpha_t$, the number of local updates $q$, inner batch size $m$ and mini-batch size $b, B$ ; u_{n, t} = 0 for $n \\in \\{1, \\cdots, N\\}$\\\\\n% \\STATE {\\bfseries initialize:} Initialize: $x_{0}_n = \\frac{1}{N} \\sum_{k=1}^{N} x_{0}_n$. \\\\\n% \\FOR{$t = 1, 2, \\ldots, T$}\n% \\FOR{$n = 1, 2, \\ldots, N$}\n% \\STATE Draw $b$ samples of $\\{\\xi_{t, 1}_n, \\cdots, \\xi_{t, b}_n \\}$ \\\\\n% \\STATE  Draw $m$ i.i.d samples $\\mathcal{B}^{n}_{t, n} = \\left\\{\\eta_n_{ij}\\right\\}_{j=1}^m$ from $P(\\eta_n\\mid \\xi_n_{t, i})$ for each $\\xi_n_{t, i} \\in \\{\\xi_{t, 1}_n, \\cdots, \\xi_{t, B}_n \\}$,\n% \\\\\n% \\STATE $u_{n, t} = \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(x_{n,t}; \\xi_{t, i}_n,\\mathcal{B}_n_{t, i}) + (1 - \\alpha) (u_{t - 1}_n - \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(x_{t - 1}_n; \\xi_{t, i}_n,\\mathcal{B}_n_{t, i}))$ \\\\\n% \\STATE $v_{t}_n = \\sum_{r=1}^{N} \\underline{w}_{nr} (v_{t - 1}^r + u_t^r - u_{t - 1}^r)$ \\\\\n% \\STATE $x_{t+1}_n = \\sum_{r = 1}^{N} \\underline{w}_{nr} (x_{t}^{n} -  \\eta v_{t}_n) $\\\\\n\n% \\ENDFOR\n% \\ENDFOR\n% \\STATE {\\bfseries Output:} $x$ chosen uniformly random from $\\{\\bar{x}_t\\}_{t=1}^{T}$.\n% \\end{algorithmic}\n% \\end{algorithm}\n\n% We write the udpate of gradient tracker and model parameter as   following matrix form: âˆ€k â‰¥ 0,\n% \\begin{align}\n% \\mathbf{v}_t & = \\mathbf{x}_t (\\mathbf{v}_t + \\mathbf{u}_t - \\mathbf{u}_{t - 1}) \\nonumber\\\\ \n% \\mathbf{x}_{t + 1} &= \\mathbf{x}_t (\\mathbf{x}_{t} - \\alpha \\mathbf{v}_{t}) \\nonumber\n% \\end{align}\n\n% where $\\mathbf{W} = \\mathbf{W} \\otimes   \\mathbf{I}_p$.\n\nAt the beginning, similar to the SLATE,  worker nodes initialize local model parameters $\\mathbf{x}$ as seen in Lines 1-2 in \\Cref{alg:2}. \n\nDifferent from SLATE, we initialize the $\\mathbf{u}_{n,0}$ with initial batch size $B$ and $\\mathbf{v}_{n,0} \\forall n \\in [N]$, which can be seen in Lines 3-4 in \\Cref{alg:2}. Then we do the consensus step to update the model parameters $\\mathbf{x}_n$. The definition of $\\hat{F}_n(\\mathbf{x}_{n, 0}; \\xi^i_{n, 0}, \\mathcal{B}_{n, 0}) $ is similar to \\eqref{eq:6} as below:\n\\begin{align} \\label{eq:7}\n&\\frac{1}{|\\mathcal{B}^{+}_{n, t} | } \\hat{F}_n(\\mathbf{x}_{n, t}; \\xi^i_{n, t}, \\mathcal{B}_{n, t})  =\\\\\n& \\sum_{\\mathbf{\\xi} \\in \\mathcal{B}^{+}_{n,t}} \\sum_{\\mathbf{\\xi}^{\\prime} \\in \\mathcal{B}_{n , t}} \\frac{\\left(g^1\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) - g^2\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\mathbf{I}\\left(\\mathbf{y}^{\\prime}=1\\right)\\right) \\nabla \\ell\\left(\\mathbf{x}_{n,t}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}{|\\mathcal{B}^{+}_{n, t} | m \\left( g^2\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\right)^2}  \\nonumber\n\\end{align}\nwhere $|\\mathcal{B}_{n, t} |$ denotes the size of batch $\\mathcal{B}_{n, t} $ and $\\mathbf{\\xi} = \\left(\\mathbf{z}, y\\right)$ and $\\mathbf{\\xi}^{\\prime} = \\left(\\mathbf{z}^{\\prime}, y^{\\prime}\\right)$.\n\nAfterwards, similar to SLATE, each iteration, we draw $b$ samples from positive dataset $\\mathcal{D}_n^{+}$ and $m$ samples from full data sets $\\mathcal{D}_n$ on each worker node, respectively to construct the biased stochastic gradient, seen in Line 8-9 of Algorithm \\ref{alg:2}.\n\nThe key different between SLATE and SLATE-M is that we update gradient estimator $\\mathbf{u}_{n, t}$ in SLATE-M with the following variance reduction method: \n\\begin{align}\n& \\mathbf{u}_{n, t} = \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}; \\xi^i_{n,t}, \\mathcal{B}_{n, t}) + (1 - \\alpha) (\\mathbf{u}_{n, t - 1} \\nonumber\\\\\n&- \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t - 1}; \\xi^i_{n,t}, \\mathcal{B}_{n, t})) \\nonumber\n\\end{align}\nwhere $\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}; \\xi^i_{n,t}, \\mathcal{B}_{n, t})$ and $\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t - 1}; \\xi^i_{n,t}, \\mathcal{B}_{n, t})$ are defined in \\eqref{eq:7}\n\nFinally, we update gradient tracker $\\mathbf{v}_{n,t}$ and model parameters $\\mathbf{x}_{n,t}$ as in Lines 11-12 in \\Cref{alg:2}.\n\n"
                }
            },
            "section 5": {
                "name": "Theoretical Analysis",
                "content": "\nWe will discuss some mild assumptions and present the convergence results of our algorithms (SLATE and SLATE-M).\n\n",
                "subsection 5.1": {
                    "name": "Assumptions",
                    "content": "\nIn this section, we introduce some basic assumptions used for theoretical analysis.\n\\begin{assumption} \\label{ass:1} $\\forall n \\in [N]$, we assume (i) there is $C ( > 0)$ that $\\ell(x; z_n, z_n) > C$; (ii) there is $M ( > 0)$ that $0 < \\ell(x; z_n, z_n^{\\prime}) < M$; (iii)  $\\ell(x; z_n, z_n^{\\prime})$ is Lipschitz continuous and smooth with respect to model $\\mathbf{x}$ for any $\\xi_n = (z_n, y_n) \\sim \\mathcal{D}^{+}, \\xi^{\\prime}_n = (z^{\\prime}_n, y^{\\prime}_n) \\sim \\mathcal{D}$. \\end{assumption}\n\\begin{assumption} \\label{ass:2}\n$\\forall n \\in [N]$, we assume there exists a positive constant $\\sigma$, such that $\\| \\nabla g(\\mathbf{x}; \\xi , \\xi^{\\prime})\\|^2 \\leq \\sigma^2,  \\forall \\xi \\sim \\mathcal{D}_n^{+}, \\xi^{\\prime} \\sim \\mathcal{D}_n$\n\\end{assumption}\nAssumptions \\ref{ass:1} and \\ref{ass:2} are a widely used assumption in optimization analysis of AUPRC maximization  \\cite{qi2021stochastic, wang2021momentum,jiang2022multi}. They can be easily satisfied when we choose a smooth surrogate loss function $\\ell(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime})$ and a bounded score function model $h(\\mathbf{x}; \\cdot)$.\n\nFurthermore, based on Assumptions \\ref{ass:1} and \\ref{ass:2}, we can build the smoothness and lipschitz continuity of objective function in the problem \\eqref{eq:4}.\n\n\\begin{lemma} \\label{lem:1}(Lemma 1 in \\cite{wang2021momentum}) Suppose Assumptions  \\ref{ass:1} and \\ref{ass:2} hold, then $\\forall \\mathbf{x}, \\| g_n(\\mathbf{x}; \\mathbf{\\xi}) \\|^2 \\leq \\sigma_g^2$, $g_n(\\mathbf{x}; \\mathbf{\\xi})$ is $L_g$-Lipschitz and $S_g$-smooth for $\\mathbf{xi} \\sim \\mathcal{D}_n^{+}$, and $\\forall u \\in \\Omega, f(u)$ is $L_f$-Lipschitz and $S_f$-smooth. $\\forall \\mathbf{x}, F_n(\\mathbf{x})$ is $L_F$-Lipschiz and $S_F$-smooth.\n\\end{lemma}\n\nFrom the \\Cref{lem:1}, we have $f_n$ and $g_n$ are $S_f$-smooth and $S_h$-smooth. This implies that for an samgle $\\mathbf{\\xi}_n \\sim \\mathcal{D}_n^{+}$ there exist $S_f > 0$  and $S_g > 0$ such that\n\\begin{align}\n\\mathbb{E}\\|\\nabla f_n(x_1) - \\nabla f_n(x_2) \\| \\leq S_{f} \\|x_1 - x_2\\| \\nonumber\\\\\n\\mathbb{E}\\|\\nabla g_n(y_1, \\xi_n) - \\nabla g_n(y_2, \\xi_n) \\| \\leq S_{g} \\|y_1 - y_2\\| \\nonumber\n\\end{align}\nAnd $f_n$ and $g_n$ are $L_f$-Lipchitz continuous and $L_g$-Lipchitz continuous. This implies that there exist $L_f > 0$  and $L_g > 0$ such that\n\\begin{align}\n\\mathbb{E}\\|\\nabla f_n(x)  \\|^2 \\leq L^2_{f}  \\nonumber\\\\\n\\mathbb{E}\\|\\nabla g_n(y_1, \\xi_n) \\|^2 \\leq S^2_{g} \\nonumber \n\\end{align}\nIn addition, we also have bounded variance of $g_n$. There exist $\\sigma_g > 0$  such that\n\\begin{align}\n\\mathbb{E}_{\\xi_n \\sim \\mathcal{D}_n^{+}} \\|g_n(\\mathbf{x}; \\mathbf{\\xi}_n, \\mathbf{\\xi}^{\\prime}_n) - \\mathbb{E}_{\\xi_n^{\\prime} \\sim \\mathcal{D}_n} g_n(\\mathbf{x}; \\mathbf{\\xi}_n, \\mathbf{\\xi}^{\\prime}_n) \\|^2  \\leq \\sigma_g^2  \\nonumber\n\\end{align}\nwhich indicates that the inner function $g_n$ has bounded variance. To control the estimation bias, we follow the analysis in single-machine conditional  stochastic optimization \\cite{hu2020biased}. \n\n\\begin{lemma} \\label{lem:A1}  (Proposition B.1 in \\cite{hu2020biased}) Under Assumptions \\ref{ass:1} and \\ref{ass:2}, on the $n$-th worker node, for a sample $\\mathbf{\\xi}_n \\sim \\mathcal{D}_n^{+}$ and $m$ samples $\\mathcal{B}_{n} $ from $\\mathcal{D}_n$, \\\\\n(a) $\\mathcal{B}_{n} = \\left\\{\\xi^{\\prime  j}\\right\\}_{j=1}^m $ and we have\n\\begin{align}\n\\|\\mathbb{E} \\nabla \\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n}) - \\nabla F_n (x) \\|^2 \\leq \\frac{L_g^2 S_f^2 \\sigma_g^2}{m}\n\\end{align} .\\\\\n(b) $\\mathbb{E} \\nabla \\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n})$ are $S_F$-Lipschitz smooth \\\\\n% (c) $\\mathbb{E}_{\\xi}\\left\\|\\nabla f(y)-\\nabla \\mathbb{E} f(y)\\right\\|_2^2 \\leq L_f^2, \\mathbb{E}_{\\xi, \\eta}\\left\\|\\nabla g(x, \\xi)-\\nabla \\mathbb{E} g(x, \\xi)\\right\\|_2^2 \\leq L_g^2, $\n(c) \\begin{align}\n\\left\\|\\nabla\\left(f(\\hat{g}_n(\\mathbf{x}, \\xi_n))\\right) - \\nabla \\hat{F}(x)\\right\\|_2^2 \\leq L_f^2 L_g^2\n\\end{align}\n\\end{lemma}\n\\Cref{lem:A1} (a) provide a bound of biased stochastic gradient, which will be used in the following theoretical analysis.\n\n\\begin{assumption} \\label{ass:4}\nThe function $F_n(x)$ is bounded below, \\emph{i.e.,} $\\inf_{x} F_n(x) > -\\infty$.\n\\end{assumption}\n\n\n"
                },
                "subsection 5.2": {
                    "name": "The Communication Mechanism in Serverless Multi-Party Collaborative Training",
                    "content": "\n% In this section, we discuss the communication mechanism used in serverless multi-party collaborative training. \nThe network system of N worker nodes $\\mathcal{G} = (\\mathcal{V},  \\mathcal{E}) $ is represented by double stochastic matrix $\\underline{\\mathbf{W}} = \\{\\underline{w}_{ij} \\}  \\in \\mathbb{R}^{N \\times N}$ in the analysis. \n\nFor the ease of exposition, we write the $\\mathbf{x}_t$ and $\\mathbf{v}_t$-update in \\Cref{alg:1} and \\Cref{alg:2} in the following equivalent matrix form: $\\forall t \\geq 0$,\n\\begin{equation}\n\\mathbf{v}_{t} = \\mathbf{W} (\\mathbf{v}_{t - 1} + \\mathbf{u}_{t} - \\mathbf{u}_{t - 1}), \\;\\;\n\\mathbf{x}_{t+1} = \\mathbf{W} (\\mathbf{x}_{t - 1} - \\eta \\mathbf{v}_{t}) \\nonumber\n\\end{equation}\nwhere $\\mathbf{W} := \\underline{\\mathbf{W}} \\otimes \\mathbf{I}_{d}$ and $\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{v}_t$ are random vectors in $\\mathbb{R}^{Nd}$ that respectively concatenate the local estimates $\\{\\mathbf{x}_{n, t} \\}_{n=1}^{N}$ of a stationary point of $F$ , gradient trackers $\\{\\mathbf{v}_{n, t} \\}_{n=1}^{N} $ , gradient estimators $\\{\\mathbf{u}_{n, t} \\}_{n=1}^{N} $. With the exact averaging matrix $\\mathbf{J}$, \n% is defined below: \n% \\begin{align}\n% \\mathbf{J} & :=\\left(\\frac{1}{n} \\mathbf{1}_n \\mathbf{1}_n^{\\top}\\right) \\otimes \\mathbf{I}_d \\nonumber\n% \\end{align} \n% Similarly, \nwe have following quantities:\n% \\begin{equation}\n% \\mathbf{\\bar{x}}_t  :=\\frac{1}{n}\\left(\\mathbf{1}_n^{\\top} \\otimes \\mathbf{I}_p\\right) \\mathbf{x}_t, \\;\n% \\bar{\\mathbf{u}}_t :=\\frac{1}{n}\\left(\\mathbf{1}_n^{\\top} \\otimes \\mathbf{I}_p\\right) \\mathbf{u}_t, \\;\n% \\overline{\\mathbf{v}}_t  :=\\frac{1}{n}\\left(\\mathbf{1}_n^{\\top} \\otimes \\mathbf{I}_p\\right) \\mathbf{v}_t\\, \\nonumber\n% \\end{equation}\n\\begin{equation}\n\\mathbf{1} \\otimes \\mathbf{\\bar{x}}_t  :=  \\mathbf{J} \\mathbf{x}_t, \\;\n\\mathbf{1} \\otimes  \\bar{\\mathbf{u}}_t := \\mathbf{J} \\mathbf{u}_t, \\;\n\\mathbf{1} \\otimes  \\bar{\\mathbf{v}}_t  := \\mathbf{J} \\mathbf{v}_t\\, \\nonumber\n\\end{equation}\n\nNext, we enlist some useful results of gradient tracking-based algorithms for serveress multi-party collaborative stochastic optimization\n\\begin{lemma} \\label{lem:A2} (Lemma 1 in  \\cite{xin2021hybrid})\nFor double stochastic\nmatrix, we have the following:\\\\\n(a) $\\|\\mathbf{W} \\mathbf{x} - \\mathbf{J} \\mathbf{x}\\| \\leq \\lambda\\|\\mathbf{x} - \\mathbf{J} \\mathbf{x}\\|, \\forall \\mathbf{x} \\in \\mathbb{R}^{n d}$.\\\\\n(b) $\\bar{\\mathbf{v}}_{t}=\\bar{\\mathbf{u}}_t, \\forall t \\geq 0$. \nAs the update step in \\ref{alg:1} and \\ref{alg:2}, we have \n\\begin{align}\n \\bar{\\mathbf{x}}_{t+1} = \\bar{\\mathbf{x}}_t - \\eta  \\bar{\\mathbf{v}}_{t} = \\bar{\\mathbf{x}}_t - \\eta \\bar{\\mathbf{u}}_t \\nonumber\n\\end{align}\n(c) According to the definition of network $\\mathbf{W}$, we have the following inequalities: $\\forall k \\geq 0$,\n\\begin{align}\n&\\left\\|\\mathbf{x}_{t + 1} - \\mathbf{J} \\mathbf{x}_{t + 1}\\right\\|^2 \\leq \\frac{1 + \\lambda^2}{2}\\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2 + \\frac{2 \\eta^2 \\lambda^2}{1 - \\lambda^2}\\left\\|\\mathbf{v}_{t} - \\mathbf{J v}_{t}\\right\\|^2 \\label{eq:16}\\\\\n&\\left\\|\\mathbf{x}_{t+1} - \\mathbf{J} \\mathbf{x}_{t+1} \\right\\|^2 \\leq 2 \\lambda^2\\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2 + 2 \\eta^2 \\lambda^2\\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right\\|^2  \\label{eq:17} \\\\\n&\\left\\|\\mathbf{x}_{t + 1} - \\mathbf{J} \\mathbf{x}_{t + 1}\\right\\| \\leq \\lambda \\left\\| \\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2 + \\eta \\lambda \\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right\\| \\label{eq:18}\n\\end{align}\n\\end{lemma}\n\nThen, we study the convergence properties of SLATE and SLATEM. We first discus the metric\nto measure convergence of our algorithms. Given that the loss function is nonconvex, we are unable to demonstrate convergence to an global minimum point. Instead, we establish convergence to an approximate stationary point,  defined below:\n\n\\begin{definition}\n A point $x$ is called $\\epsilon$-stationary point if $\\|\\nabla f(x)\\| \\leq \\epsilon$.  Generally, a stochastic algorithm is defined to achieve an $\\epsilon$-stationary point in $T$ iterations if  $\\mathbb{E}\\|\\nabla f(x_T)\\| \\leq \\epsilon$.\n\\end{definition}\n\n"
                },
                "subsection 5.3": {
                    "name": "Convergence Analysis of SLATE Algorithm",
                    "content": "\nFirst, we study the convergence properties of our SLATE algorithm. The detailed proofs are provided in the supplementary materials.\n\\begin{theorem} \\label{thm:1} Suppose the sequence $\\{\\mathbf{\\bar{x}}_t\\}_{t=1}^T$ be generated from Algorithm \\ref{alg:1} and  Assumptions \\ref{ass:1}, \\ref{ass:2}, and \\ref{ass:4} hold, $0 < \\eta \\leq  \\min \\{\\frac{1 - \\lambda^2}{24 \\lambda^2 S_F}, \\frac{1}{6 S_F}\\}$, SLATE in \\cref{alg:1} has the following\n\\begin{align}\n&\\frac{1}{T} \\sum_{t=0}^{T - 1} \\mathbb{E} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \\leq  \\frac{2  \\mathbb{E} [F(\\bar{\\mathbf{x}}_{0}) - F(\\bar{\\mathbf{x}}_{T})] }{\\eta T}\n\\nonumber\\\\\n&+ \\left(\\frac{1}{ \\lambda^2} + 5 N\\right) \\frac{32 \\lambda^2 \\eta^2 L_f^2 L_g^2 S_F^2}{(1 - \\lambda^2)^2 N}  + \\frac{2 L_g^2 S_f^2 \\sigma_g^2}{m} + \\frac{2 \\eta S_F L_f^2 L_g^2}{N} \\nonumber\n\\end{align}\n\\end{theorem}\n\\begin{corollary} \\label{corollary:1} Based on the analysis in \\Cref{thm:1}, by setting $\\eta = O(\\sqrt{\\frac{N}{T}})$ , SLATE in \\Cref{alg:1} has the\nfollowing\n\\begin{align}\n&\\frac{1}{T} \\sum_{t=0}^{T - 1} \\mathbb{E} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \\leq O(\\frac{\\mathbb{E} [F(\\bar{\\mathbf{x}}_{0}) - F(\\bar{\\mathbf{x}}_{T})] }{(N T)^{1/2}}) \\nonumber\\\\\n&+  \\left(\\frac{1}{ \\lambda^2} + 5 \\right) \\frac{24 \\lambda^2 L_f^2 L_g^2 S_F^2}{(1 - \\lambda^2)^2} O(\\frac{N}{T})  + \\frac{2 L_g^2 S_f^2 \\sigma_g^2}{m} + O(\\frac{2 S_F L_f^2 L_g^2}{(NT)^{1/2}}) \\nonumber\n\\end{align}\n\\end{corollary}\nBased on the result in \\Cref{thm:1} and \\Cref{corollary:1}, we can get the convergence result of SLATE. \n\\begin{remark}\nAccording to \\Cref{corollary:1}, without loss of generality, we let $m = O(\\varepsilon^{-2})$, $b = O(1)$ and $\\sqrt{T} > N$, we know to make $\\frac{1}{T} \\sum_{t=0}^{T - 1} \\mathbb{E} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \\leq \\varepsilon^{2}$, we have iterations $T$ should be as large as $ O( N^{-1} \\varepsilon^{-4} )$. \n\nIn \\Cref{alg:1}, we sample $b + m$ data points to build the biased stochastic gradients $\\mathbf{u}_{n,t}$, and need $T$ iterations. Thus, our SLATE algorithm has a sample complexity of   $m \\cdot T = O(N^{-1} \\varepsilon^{-6}) $, for finding an $\\epsilon$-stationary point. In addition, the result also indicates the linear speedup of our algorithm with respect to the number of worker nodes.\n\\end{remark}\n"
                },
                "subsection 5.4": {
                    "name": "Convergence Analysis of SLATE-M",
                    "content": "\nIn the subsection, we study the convergence properties of our SLATE-M algorithm. The details about proofs are provided in the supplementary materials.\n\n\\begin{theorem}  \\label{thm:2}\nSuppose the sequence $\\{\\mathbf{\\bar{x}}_t\\}_{t=1}^T$ be generated from Algorithm \\ref{alg:2} and Assumptions \\ref{ass:1}, \\ref{ass:2}, and \\ref{ass:4} hold, $0 < \\eta \\leq min \\{\\frac{1}{4}, \\\\ \\frac{\\left(1 - \\lambda^2\\right)^2}{90 \\lambda^2}, \\frac{\\sqrt{1 - \\lambda^2}}{12 \\sqrt{7} \\lambda} \\} \\frac{1}{S_F}$ and $\\alpha = \\frac{72 S^2_F \\eta^2 }{Nb}$ , SLATE-M in \\Cref{alg:2} has the following\n\\begin{align}\n& \\frac{1}{T}\\sum_{t = 0}^{T - 1} \\mathbb{E} \\| \\nabla \\mathbf{F} (\\bar{\\mathbf{x}}_{t})\\|^2 \\leq \\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{\\eta T}   + \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} + 3 \\frac{L_g^2 L_f^2 }{\\alpha N B T}  \\nonumber\\\\\n&+ \\frac{6 \\alpha L_g^2 L_f^2}{Nb} + \\frac{96 \\lambda^2 L_g^2 L_f^2}{\\left(1 - \\lambda^2 \\right)^3 B T} + \\frac{256 \\lambda^2 \\alpha^2 L_f^2 L_g^2}{(1 - \\lambda^2)^3} + \\frac{64 \\lambda^4 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{(1 - \\lambda^2)^3 N T}  \\nonumber\n\\end{align}\n\\end{theorem}\n\n\\begin{corollary} \\label{corollary:2}\nBased on the analysis in the \\cref{thm:2}, we choose $b = O(1), \\eta = O(\\frac{N^{2/3}}{T^{1/3}}), \\alpha = O(\\frac{N^{1/3}}{T^{2/3}}), B = O(\\frac{T^{1/3}}{N^{2/3}})$, SLATE-M in \\Cref{alg:2} has the\nfollowing\n\\begin{align}\n&\\frac{1}{T}\\sum_{t = 0}^{T - 1} \\mathbb{E} \\| \\nabla \\mathbf{F} (\\bar{\\mathbf{x}}_{t})\\|^2 \\leq O(\\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{(N T)^{2/3}} + \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} \\nonumber\\\\\n&+ O(\\frac{3 L_g^2 L_f^2 }{(NT)^{2/3}})  + O(\\frac{6 L_g^2 L_f^2}{(NT)^{2/3}}) + \\frac{352 \\lambda^2 L_f^2 L_g^2}{(1 - \\lambda^2)^3} O(\\frac{N^{2/3}}{T^{4/3}}) \\nonumber\\\\\n&+ \\frac{64 \\lambda^4 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{(1 - \\lambda^2)^3 N T}  \n\\end{align}\n\\end{corollary}\nBased on the result in \\cref{thm:2}, we can get the convergence result of SLATE-M. \n\\begin{remark}\nAccording to \\Cref{corollary:2}, without loss of generality,  Let $m = O(\\varepsilon^{-2})$, $b = O(1)$ $\\eta = O(\\frac{N^{2/3}}{T^{1/3}}), \\alpha = O(\\frac{N^{1/3}}{T^{2/3}})$, and $B = O(\\frac{T^{1/3}}{N^{2/3}})$, we know to make $\\frac{1}{T} \\sum_{t=0}^{T - 1} \\mathbb{E} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \\leq \\varepsilon^{2}$, we have iterations $T$ should be as large as $ O( N^{-1} \\varepsilon^{-3})$.\n\nIn \\Cref{alg:2}, in each iteration, we sample $b + m$ data points to build the biased stochastic gradients $\\mathbf{u}_{n,t}$, and need $T$ iterations. Thus, our SLATE-M algorithm has a sample complexity of   $m \\cdot T = O(N^{-1} \\varepsilon^{-5}) $, for finding an $\\epsilon$-stationary point, which also achieves the linear speedup of our algorithm with respect to the number of worker nodes. \n\\end{remark}\n\\begin{remark}\nThe sample complexity of  $O(N^{-1} \\varepsilon^{-5})$ in SLATE-M matches the best convergence complexity achieved by the single-machine stochastic method for conditional stochastic optimization in the online setting, and also match the lower bound for the online stochastic algorithms \\cite{hu2020biased}.\n\\end{remark}\n\n\n"
                }
            },
            "section 6": {
                "name": "Experiments",
                "content": "\nIn this section, we conduct extensive experiments on imbalanced benchmark datasets to show the efficiency of our algorithms. All experiments are run over a machine with AMD EPYC 7513 32-Core Processors and NVIDIA RTX A6000 GPU. The source code is available at \\textbf{https://github.com/xidongwu/D-AUPRC}.\n\nThe goal of our experiments is two-fold: (1) to verify that \\eqref{eq:4} is the surrogate function of AUPRC and illustrate that directly optimizing the AUPRC in the multi-party collaborative training would improve the model performance compared with traditional loss optimization, and (2) to show the efficiency of our methods for AUPRC maximization. \n\n\n\n\n\n\n\n\n\n",
                "subsection 6.1": {
                    "name": "Configurations",
                    "content": "\n\\textbf{Datasets}:\nWe conduct experiments on imbalanced benchmark datasets from LIBSVM data \\footnote{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}: w7a and w8a, and four typical image datasets: MNIST dataset, Fashion-MNIST dataset, CIFAR-10, and Tiny-ImageNet dataset (seen in \\Cref{tb1}). For w7a and w8a, we scale features to [0, 1]. For image datasets, following \\citep{qi2021stochastic, wang2021momentum}, we construct the imbalanced binary-class versions as follows: Firstly, the first half of the classes (0 - 4) in the original MNIST, Fashion-MNIST, and CIFAR-10, and (0 - 99) in Tiny-ImageNet datasets are converted to be the negative class, and another half of classes are considered to be the positive class. Because the original distributions of image datasets are balanced, we randomly drop 80\\% of the positive examples in the training set to make them imbalanced and keep test sets of image datasets unchanged. Finally, we evenly partition each datasets into disjoint sets and distribute datasets among worker nodes.\n\n\\textbf{Models}:\nFor w7a and w8a, we use two layers of neural networks with the dimension of the hidden layer as 28. The RELU is used as the activation function. For MNIST, Fashion MNIST and Cifar-10 data sets, we choose model architectures from \\cite{ wu2022faster} for our imbalanced binary image classification task. For Tiny-ImageNet, we choose ResNet-18 \\cite{he2016deep} as the classifier. In our algorithms, We modify the output of all models to 1 and the sigmoid function is followed since we consider binary classification tasks. \n\nIn the experiments, the number of worker nodes is set as N = 20 and we use the ring-based topology as the communication network structure. \\citep{lian2017can}.\n\n"
                },
                "subsection 6.2": {
                    "name": "Comparison with Existing Multi-Party Stochastic Methods",
                    "content": "\n\n\\textbf{Baselines}: We compare our algorithms with two baselines: 1) D-PSGD \\cite{lian2017can}, a SGD-like serverless multi-party collaborative algorithm with the Cross-Entropy loss as the optimization objective. D-PSGD runs SGD locally and then computes the neighborhood weighted average by fetching model parameters from neighbors; 2) CODA, a typical federated learning algorithm for optimizing minimax formulated AUROC loss \\cite{guo2020communication, yuan2021federated}. CODA runs local SGDA with the periodic model average in the federated learning setting. We convert it into the serverless multi-party setting and run local SGDA, following the consensus step to update the models. Gradient tracking steps are ignored. In the experiments, we ignore the gradient tracking steps to reduce computation and communication costs.\n\n\\noindent\\textbf{Parameter tuning}:  We perform a grid search to tune all methods carefully. The total batch size m drawn from $\\mathcal{D}$ is chosen in the set $\\{20, 20, 20, 20, 60, 200\\}$. For SLATE and SLATE-M, the positive batch size b in the total batch size is chosen in the set $\\{2, 2, 3, 5, 20, 35\\}$, and  m - b negative data points. The squared hinge loss is used as \\eqref{eq:2}, $\\alpha$ is chosen from $\\{0.1, 0.9\\}$ and the margin parameter s is selected from $\\{0.1, 0.3, 0.5, 0.7, 0.9\\}$. The step size is selected from the set $\\{0.01, 0.005, 0.001\\}$. For the D-PSGD, the step size is chosen in the set $\\{0.01, 0.005, 0.001\\}$. For the CODA, the \nstep size for minimum variable is chose from the set $\\{0.01, 0.005, 0.001\\}$ and that for the maximum variable is chosen from the set $\\{0.0001, 0.0005, 0.001\\}$. Moreover, we use Xavier normal to initialize  models.\n\n\n\n\n\n\\noindent\\textbf{Experimental results}:\n\\Cref{tb2} summarizes the final results on the test sets. In order to present the advantage of optimization of AUPRC, we plot the Precision-Recall curves of final models on testing sets of W7A, MNIST, and CIFAR-10 when training stop in \\Cref{fig:1}. Then we illustrate the convergence curve on test sets in \\Cref{fig:2}. Results show that our algorithms (\\emph{i.e.}, SLATE, SLATE-M) can outperform baselines in terms of AP with a great margin across each benchmark, regardless of model structure. The experiments verify that 1) the objective function \\eqref{eq:4} is a good surrogate loss function of AUPRC and directly optimizing the AUPRC in the multi-party collaborative training would improve the model performance compared with traditional loss optimization in the imbalanced data mining. 2) Although CODA, with minimax formulated AUROC loss, has a relatively better performance compared with D-PSGD in large-scale datasets (CIFAR-10 and Tiny-ImageNet), the results verify the previous results that an algorithm that maximizes AUROC does not necessarily maximize AUPRC. Therefore, designing the algorithms for AUPRC in multi-party collaborative training is necessary. 3) our algorithms can efficiently optimize the \\eqref{eq:4} and largely improve the performance in terms of AUPRC in multi-party collaborative imbalanced data mining. 4) In datasets CIFAR-10 and Tiny-ImageNet, SLATE-M has better performance compared with SLATE.\n\n\\noindent\\textbf{Ablation study}: In this part, we study the effect of margin parameters, positive batch size, and $\\alpha $ of SLATE-M. The results are listed in \\Cref{tb3} and \\Cref{tb4}.\n\n"
                }
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\nIn this paper, we systematically studied how to design serverless multi-party collaborative learning algorithms to directly maximize AUPRC and also provided the theoretical guarantee on algorithm convergence. To the best of our knowledge, this is the first work to optimize AUPRC in the multi-party collaborative training. We cast the AUPRC maximization problem into non-convex two-level stochastic optimization functions under the multi-party collaborative learning settings as the problem \\eqref{eq:4}, and proposed the first multi-party collaborative learning algorithm, ServerLess biAsed sTochastic gradiEnt (SLATE). Theoretical analysis shows that SLATE has a sample complexity of $O(\\varepsilon^{-6})$ and shows a linear speedup respective to the number of worker nodes. Furthermore, we proposed a stochastic method (\\emph{i.e.}, SLATE-M) based on the momentum-based variance-reduced technique to reduce the convergence complexity for maximizing AP in multi-party collaborative optimization. Our methods reach iteration complexity of $O\\left(1 / \\epsilon^{5}\\right)$, which matches the best convergence complexity achieved by the single-machine stochastic method for conditional stochastic optimization in the online setting, and also matches the lower bound for the online stochastic algorithms. Unlike existing single-machine methods that just focus on finite-sum settings and must keep an inner state for each positive data point, we consider the stochastic online setting. The extensive experiments on various data sets compared with previous stochastic multi-party collaborative optimization algorithms validate the effectiveness of our methods. Experimental results also demonstrate that directly optimizing the AUPRC in the multi-party collaborative training would largely improve the model performance compared with traditional loss optimization.\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\newpage\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{sample-base}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\n\\newpage\n\\appendix\n\\onecolumn\n"
            },
            "section 8": {
                "name": "Supplementary material",
                "content": "\n% \\subsection{Model Architecture}\n\n% \\begin{table}\n%   \\centering\n%   \\caption{ Model Architecture for the the MNIST dataset \\citep{wu2022faster}}\\label{tb5}\n% \\setlength{\\tabcolsep}{16pt} \n% \\begin{tabular}{ll}\n% \\hline Layer Type & Shape \\\\\n% \\hline Convolution + ReLU & $5 \\times 5 \\times 20 $ \\\\\n% Max Pooling & $2 \\times 2$ \\\\\n% Convolution + ReLU & $5 \\times 5 \\times 50$ \\\\\n% Max Pooling & $2 \\times 2$ \\\\\n% Fully Connected + ReLU & 500 \\\\\n% Fully Connected + ReLU & 1 \\\\\n% \\hline\n% \\end{tabular}\n% \\end{table}\n\n% \\begin{table} \n%   \\centering\n%   \\caption{ Model Architecture for the Fashion MNIST dataset \\citep{wu2022faster}}\\label{tb6}\n% \\setlength{\\tabcolsep}{16pt} \n% \\begin{tabular}{ll}\n% \\hline Layer Type & Shape \\\\\n% \\hline Convolution + ReLU & $3 \\times 3 \\times 5 $ \\\\\n% Max Pooling & $2 \\times 2$ \\\\\n% Convolution + ReLU & $3 \\times 3 \\times 10$ \\\\\n% Max Pooling & $2 \\times 2$ \\\\\n% Fully Connected + ReLU & 100 \\\\\n% Fully Connected + ReLU & 1 \\\\\n% \\hline\n% \\end{tabular}\n% \\vspace{10pt}\n% \\end{table}\n\n",
                "subsection 8.1": {
                    "name": "Basic Lemma",
                    "content": "\nWe draw one sample $\\xi_n$ from $\\mathcal{D}^{+}_n$ and $m$ sample $\\xi^{\\prime}_n$ from $\\mathcal{D}_n$ as $\\mathcal{B}_{n}$. We define \n\\begin{align}\ng_n(x; \\mathbf{\\xi}_n) = \\mathbb{E}_{\\mathbf{\\xi}_n^{\\prime} \\sim \\mathcal{D}_n} g_n \\left(x; \\mathbf{\\xi}_n, \\mathbf{\\xi}_n^{\\prime} \\right) \\nonumber\n\\end{align} \n\\begin{align}\n\\hat{g}_n(x, \\xi_n) = \\frac{1}{m} \\sum_{\\xi_n^{\\prime} \\in \\mathcal{B}_{n}} g_n(x, \\xi_n; \\xi_n^{\\prime}) \\nonumber\n\\end{align}\n\\begin{align}\n\\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n}) = f(\\hat{g}_n(x, \\xi_n)) \\nonumber\n\\end{align}\n\nWe also define \n\\begin{align}\n&\\nabla F_n (x) = \\nabla \\mathbb{E}_{\\xi_n} f(g_n(x, \\xi_n))  = \\mathbb{E}_{\\xi_n}\\left[\\nabla\\left(f(g_n(x, \\xi_n))\\right)\\right] \n= \\mathbb{E}_{\\xi_n}\\left[\\nabla f(g_n(x, \\xi_n)) \\cdot \\nabla g_n(x, \\xi_n)\\right] \\nonumber\n\\end{align}\n\\begin{align}\n\\hat{F}_n(x) = \\mathbb{E}\\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n}) = \\mathbb{E} \\left[f(\\hat{g}_n(x, \\xi_n))\\right] \\nonumber\n\\end{align}\n\\begin{align}\n\\nabla \\hat{F}_n(x) = \\mathbb{E}\\nabla\\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n})\n\\end{align}\n\nFor convenience, we denote \n\\begin{align} \\bar{\\mathbf{x}}_t = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_{n, t}, \\quad \\bar{\\mathbf{v}}_t = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{v}_{n,t},  \\quad \\bar{\\mathbf{u}}_t = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{u}_{n,t} \\quad F(\\bar{\\mathbf{x}}) = \\frac{1}{N} \\sum_{n = 1}^{N} F_n (\\bar{\\mathbf{x}}), \\nabla \\hat{F}(\\bar{\\mathbf{x}}) = \\frac{1}{N} \\sum_{n = 1}^{N} \\nabla \\hat{F}_n(\\bar{\\mathbf{x}}) \n\\end{align}\nand $\\nabla \\hat{\\mathbf{F}}_t = [\\nabla \\hat{F}_1(\\mathbf{x}_1)^{\\top}, \\nabla \\hat{F}_2(\\mathbf{x}_2)^{\\top}, \\dots, \\nabla \\hat{F}_N(\\mathbf{x}_N)^{\\top}]^{\\top} \\in \\mathbb{R}^{n d}$.\n\n% \\begin{lemma} \\label{lem:A1}  (Proposition B.1 in \\cite{hu2020biased}) Under Assumption \\ref{ass:1} \\ref{ass:2} , in the n-th device, for a sample $\\xi_n$ and $m$ samples $\\mathcal{B}_{n} $ from $\\mathcal{D}_n$, we have \\\\\n% (a)\n% \\begin{align}\n% \\|\\mathbb{E} \\nabla \\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n}) - \\nabla F_n (x) \\|^2 \\leq \\frac{L_g^2 S_f^2 \\sigma_g^2}{m}\n% \\end{align} \n% (b) $F_n(x)$ and $\\hat{F}_n(x)$ are $S_F$-Lipschitz smooth where $S_F=S_g L_f+S_f L_g^2$ \\\\\n% (c) $\\mathbb{E}_{\\xi}\\left\\|\\nabla f(y)-\\nabla \\mathbb{E} f(y)\\right\\|_2^2 \\leq L_f^2, \\mathbb{E}_{\\xi, \\eta}\\left\\|\\nabla g(x, \\xi)-\\nabla \\mathbb{E} g(x, \\xi)\\right\\|_2^2 \\leq L_g^2, \\left\\|\\nabla\\left(f(\\hat{g}_n(\\mathbf{x}, \\xi_n))\\right) - \\nabla \\hat{F}(x)\\right\\|_2^2 \\leq$ $L_f^2 L_g^2$ \n% \\end{lemma}\n% \\begin{proof}\n% (a) Recall the definition of  $\\hat{g}_n(x, \\xi_n)$, we have\n% \\begin{align}\n% &\\left\\|\\mathbb{E} \\nabla \\hat{F}_n\\left(x ; \\xi_n,\\mathcal{B}_{n}\\right) - \\nabla F_n(x)\\right\\|^2 \\nonumber\\\\\n% = &\\left\\|\\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}(\\nabla \\hat{g}_n(x, \\xi_n))^{\\top} \\nabla f(\\hat{g}_n(x, \\xi_n)) - \\mathbb{E}_{\\xi_n} \\nabla g_n(x, \\xi_n)^{\\top} \\nabla f(g_n(x; \\xi_n))\\right\\|^2 \\nonumber\\\\\n% \\leq &\\left\\|\\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}(\\nabla \\hat{g}_n(x; \\xi_n))^{\\top} \\nabla f(\\hat{g}_n(x; \\xi_n)) - \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}(\\nabla \\hat{g}_n(x; \\xi_n))^{\\top} \\nabla f\\left(g_n(x; \\xi_n)\\right)\\right\\|^2 \\nonumber\\\\\n% +&\\left\\|\\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}(\\nabla \\hat{g}_n(x; \\xi_n))^{\\top} \\nabla f\\left(g_n(x; \\xi_n)\\right) - \\mathbb{E}_{\\xi_n}\\left( \\nabla g_n(x; \\xi_n)\\right)^{\\top} \\nabla f\\left(g_n(x; \\xi_n)\\right)\\right\\|^2 \\nonumber\\\\\n% =& \\left\\|\\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}(\\nabla \\hat{g}_n(x; \\xi_n))^{\\top} \\nabla f(\\hat{g}_n(x; \\xi_n)) - \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}(\\nabla \\hat{g}_n(x; \\xi_n))^{\\top} \\nabla f\\left(g_n(x; \\xi_n) \\right)\\right\\|^2 \\nonumber\\\\\n% +&\\|\\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n} (\\nabla \\hat{g}_n(x; \\xi_n) -  \\nabla g_n(x; \\xi_n))^{\\top} \\nabla f\\left( g_n(x; \\xi_n)\\right)\\|^2 \\nonumber\\\\\n% =&\\left\\|\\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}(\\nabla \\hat{g}_n(x; \\xi_n))^{\\top} \\nabla f(\\hat{g}_n(x; \\xi_n)) - \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}(\\nabla \\hat{g}_n(x; \\xi_n))^{\\top} \\nabla f\\left( g_n(x; \\xi_n)\\right)\\right\\|^2 \\nonumber\\\\\n% \\leq & \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}\\|\\nabla \\hat{g}(x, \\xi_n)\\|^2\\left\\|\\nabla f(\\hat{g}_n(x; \\xi_n))-\\nabla f\\left( g_n(x; \\xi_n)\\right)\\right\\|^2 \\nonumber\\\\\n% \\leq & L_g^2 S_f^2 \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi^{\\prime}_n}\\left\\|\\hat{g}_n(x; \\xi_n) -  g_n(x; \\xi_n)\\right\\|^2 \\nonumber\\\\\n% \\leq & \\frac{L_g^2 S_f^2 \\sigma_g^2}{m}. \\nonumber\n% \\end{align}\n\n% (b) Given a fixed $\\xi_n$, we have\n% \\begin{align}\n% &\n% % \\|\\nabla F(x_1) - \\nabla F(x_2) \\| =\n% \\left\\|\\nabla(f(g_n(x_1, \\xi_n))) - \\nabla\\left(f(g_n(x_2, \\xi_n))\\right)\\right\\| \\nonumber\\\\\n% =& \\left\\| \\nabla g_n(x_1, \\xi_n)^{\\top} \\cdot \\nabla f(g_n(x_1, \\xi_n)) - \\nabla g_n(x_2, \\xi_n)^{\\top} \\cdot \\nabla f(g_n(x_2, \\xi_n)) \\right\\| \\nonumber \\\\\n% =& \\|\\nabla g_n(x_1, \\xi_n)^{\\top} \\cdot \\nabla f(g_n(x_1, \\xi_n)) -  \\nabla g_n(x_2, \\xi_n)^{\\top} \\cdot \\nabla f(g_n(x_1, \\xi_n))  \\nonumber\\\\\n% &+ \\nabla g_n(x_2, \\xi_n)^{\\top} \\cdot \\nabla f(g_n(x_1, \\xi_n)) - \\nabla g_n(x_2, \\xi_n)^{\\top} \\cdot  \\nabla f(g_n(x_2, \\xi_n)) \\| \\nonumber\\\\\n% \\leq & \\left\\| (\\nabla g_n(x_1, \\xi_n) - \\nabla g_n(x_2, \\xi_n))^{\\top} \\cdot \\nabla f(g_n(x_1, \\xi_n)) \\right\\| + \\left\\| \\nabla g(x_2, \\xi_n)^{\\top} \\cdot \\left(\\nabla f(g_n(x_1, \\xi_n)) - \\nabla f(g_n(x_2, \\xi))\\right) \\right\\| \\nonumber \\\\\n% \\leq &\\left(S_g L_f+S_f L_g^2\\right)\\|x_1 - x_2\\|\n% \\end{align}\n% where the last inequality holds since $g(\\cdot, \\xi)$ is $L_g$-Lipschitz continuity and $S_g$-Lipschitz smoothness.\n% Similarly,\n% \\begin{align}\n% &\\left\\|\\nabla\\left(f(\\hat{g}_n(x_1, \\xi_n))\\right) - \\nabla\\left(f(\\hat{g}_n(x_2, \\xi_n))\\right)\\right\\| \\nonumber\\\\\n% =&\\left\\|\\nabla \\hat{g}_n(x_1, \\xi_n)^{\\top} \\nabla f(\\hat{g}_n(x_1, \\xi_n)) - \\nabla \\hat{g}_n(x_2, \\xi_n)^{\\top} \\nabla f(\\hat{g}_n(x_2, \\xi_n))\\right\\| \\nonumber\\\\\n% =&\\|\\nabla \\hat{g}_n(x_1, \\xi_n)^{\\top} \\nabla f(\\hat{g}_n(x_1, \\xi_n)) - \\nabla \\hat{g}_n(x_2, \\xi_n)^{\\top} \\nabla f(\\hat{g}_n(x_1, \\xi_n)) \\nonumber\\\\\n% &+ \\nabla \\hat{g}_n(x_2, \\xi_n)^{\\top} \\nabla f(\\hat{g}_n(x_1, \\xi_n)) - \\nabla \\hat{g}_n(x_2, \\xi_n)^{\\top} \\nabla f(\\hat{g}_n(x_2, \\xi_n))\\| \\nonumber\\\\\n% \\leq&\\left\\|(\\nabla \\hat{g}_n(x_1, \\xi_n) - \\nabla \\hat{g}_n(x_2, \\xi_n))^{\\top} \\nabla f(\\hat{g}_n(x_1, \\xi_n))\\right\\| \\nonumber\\\\\n% &+ \\| \\nabla \\hat{g}_n(x_2, \\xi_n)^{\\top}(\\nabla f(\\hat{g}_n(x_1, \\xi_n)) - \\nabla f(\\hat{g}_n(x_2, \\xi_n))) \\|^2 \\nonumber\\\\\n% \\leq& L_f\\|\\nabla \\hat{g}_n(x_1, \\xi_n) - \\nabla \\hat{g}_n(x_2, \\xi_n)\\| + S_f\\|\\hat{g}_n(x_1, \\xi_n) - \\hat{g}_n(x_2, \\xi_n)\\| \\cdot\\|\\nabla \\hat{g}_n(x_2, \\xi_n)\\| \\nonumber\\\\\n% \\leq& (S_g L_f + S_f L_g^2)\\|x_1 - x_2\\| \\nonumber\\\\\n% \\end{align}\n% (c) Given the fact that $ \\mathbb{E}\\|X - \\mathbb{E}X \\|^2 \\leq \\mathbb{E}\\|X\\|^2$ we have\n% % \\begin{align}\n% % \\mathbb{E}_{\\xi}\\left\\|\\nabla f(x) - \\nabla \\mathbb{E} f(x)\\right\\|^2 \\leq \\mathbb{E}_{\\xi_n}\\left\\|\\nabla f(x)\\right\\|^2 \\leq L_f^2 \n% % \\end{align}\n% % \\begin{align}\n% % \\mathbb{E}_{\\xi_n^{\\prime}}\\left\\|\\nabla g_n(x; \\xi_n) - \\nabla \\mathbb{E} g_n(x; \\xi_n)\\right\\|^2 \\leq \\mathbb{E}_{\\xi_n^{\\prime}}\\left\\|\\nabla g_n(x; \\xi_n)\\right\\|^2 \\leq L_g^2  \n% % \\end{align}\n\n% % Similarly, we have\n% \\begin{align}\n% & \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi_n^{\\prime}} \\|\\nabla\\left(f(\\hat{g}_n(x; \\xi_n))\\right) - \\nabla \\hat{F}_n(x)\\|^2 \\nonumber\\\\\n% \\leq & \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi_n^{\\prime}}\\left\\|\\nabla\\left(f(\\hat{g}_n(x; \\xi_n))\\right)\\right\\|^2 = \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi_n^{\\prime}}\\left\\| \\nabla(\\hat{g}_n(x; \\xi_n))^{\\top} \\cdot \\nabla f(\\hat{g}_n(x; \\xi_n)) \\right\\|^2 \\nonumber \\\\\n% \\leq & \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\xi_n^{\\prime}}\\left\\|\\nabla f(\\hat{g}_n(x; \\xi_n))\\right\\|^2 \\cdot\\left\\|\\frac{1}{m} \\sum_{i=1}^m \\nabla g_n(x; \\xi_n)\\right\\|^2 \\nonumber\\\\\n% % \\leq & L_f^2 \\mathbb{E}_{\\xi_n} \\mathbb{E}_{\\eta_n}\\left[\\frac{1}{m} \\sum_i^m\\left\\|\\nabla g_n_{\\eta_n_i}(x, \\xi_n)\\right\\|^2\\right] \n% \\leq& L_f^2 L_g^2\n% \\end{align}\n\n% \\end{proof}\n\n\n\n\n\n\\begin{lemma} (Lemma 6 in \\cite{xin2021hybrid}) \\label{lem:A3}\nLet $\\{ V_t \\}_{t \\geq 0}, \\{ R_t \\}_{t\\geq 0}$ and $\\{Q_t \\}_{t\\geq 0}$ be non-negative sequences and $C \\geq 0$ be some constant such that $V_t \\leq q V_{t-1}+q R_{t-1}+Q_t+C, \\forall t \\geq 1$, where $q \\in(0,1)$ Then the following inequality holds: $\\forall T \\geq 1$,\n\\begin{align}\n\\sum_{t=0}^{T-1} V_t \\leq \\frac{V_0}{1-q}+\\frac{1}{1-q} \\sum_{t=0}^{T-2} R_t+\\frac{1}{1-q} \\sum_{t=1}^{T-1} Q_t + \\frac{C T}{1-q}  \n\\end{align}\n\\end{lemma}\n\n%%%%%%%%%%%%%%%%\n\n\n%%%%%%%%%%%%%%%%%%%%\n"
                }
            },
            "section 9": {
                "name": "SLATE",
                "content": "\n",
                "subsection 9.1": {
                    "name": "Proofs of the Intermediate Lemmas",
                    "content": "\n% %%%%%%%%%%%%%%%%%\n\\begin{lemma} \\label{lem:B1}\nLet Assumptions \\ref{ass:1}, \\ref{ass:2} hold, and $F$ is $S_F$-smooth, we have \n\\begin{align}\n\\mathbb{E} F(\\bar{\\mathbf{x}}_{t+1}) &  \\leq \\mathbb{E} F(\\bar{\\mathbf{x}}_{t}) - (\\frac{\\eta}{2} - \\eta^2 S_F ) \\mathbb{E} \\|\\nabla \\hat{F}(\\bar{\\mathbf{x}}_t) \\|^2 - \\frac{\\eta}{2} \\mathbb{E}\\| \\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \\nonumber\\\\\n&+ \\frac{\\eta S_F^2}{N} \\sum_{n=1}^{N}\\|x_{n,t} - \\bar{\\mathbf{x}}_t\\|^2 + \\frac{\\eta L_g^2 S_f^2 \\sigma_g^2}{m} + \\frac{\\eta^2 S_F L_f^2 L_g^2}{N}\n\\end{align}\n\\end{lemma}\n\\begin{proof}\nBased on the smoothness of F, we have \n\\begin{align}\n\\mathbb{E} F(\\bar{\\mathbf{x}}_{t+1}) & \\stackrel{(a)}{\\leq}  \\mathbb{E} F(\\bar{\\mathbf{x}}_{t}) + \\mathbb{E}\\langle\\nabla F(\\bar{\\mathbf{x}}_{t}), \\bar{\\mathbf{x}}_{t+1} - \\bar{\\mathbf{x}}_{t}\\rangle + \\frac{S_F}{2}\\mathbb{E}\\left\\|\\bar{\\mathbf{x}}_{t+1} - \\bar{\\mathbf{x}}_{t}\\right\\|^{2}  \\nonumber\\\\\n&\\stackrel{(b)}{\\leq} \\mathbb{E}F(\\bar{\\mathbf{x}}_{t}) - \\eta \\mathbb{E} \\langle\\nabla F(\\bar{\\mathbf{x}}_{t}), \\bar{\\mathbf{u}}_{t}\\rangle + \\frac{\\eta^2 S_F }{2} \\mathbb{E} \\|\\bar{\\mathbf{u}}_{t}\\|^{2} \\nonumber\\\\\n&\\stackrel{(c)}{\\leq} \\mathbb{E}F(\\bar{\\mathbf{x}}_{t}) - \\eta \\mathbb{E} \\langle\\nabla F(\\bar{\\mathbf{x}}_{t}), \\frac{1}{N} \\sum_{n=1}^{N} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\rangle + \\eta^2 S_F\\mathbb{E} [\\|\\bar{\\mathbf{u}}_{t} -  \\frac{1}{N} \\sum_{n=1}^{N} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^{2} + \\|\\frac{1}{N} \\sum_{n=1}^{N} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2] \\nonumber\\\\\n&\\stackrel{(d)}{=} \\mathbb{E} F(\\bar{\\mathbf{x}}_{t}) - (\\frac{\\eta}{2} - \\eta^2 S_F ) \\mathbb{E}\\| \\frac{1}{N} \\sum_{n=1}^{N} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 - \\frac{\\eta}{2} \\mathbb{E} \\| \\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 + \\frac{\\eta}{2} \\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{x}}_{t}) - \\frac{1}{N} \\sum_{n=1}^{N} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 + \\eta^2 S_F \\mathbb{E} \\|\\bar{\\mathbf{u}}_{t} -  \\frac{1}{N} \\sum_{n=1}^{N} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^{2} \\nonumber\\\\\n&\\leq  \\mathbb{E} F(\\bar{\\mathbf{x}}_{t}) - (\\frac{\\eta}{2} - \\eta^2 S_F ) \\| \\frac{1}{N} \\sum_{n=1}^{N} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) \\|^2 - \\frac{\\eta}{2} \\mathbb{E}\\| \\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 + \\eta \\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{x}}_{t})- \\frac{1}{N} \\sum_{n=1}^N \\nabla F_n(\\mathbf{x}_{n, t})\\|^2 \\nonumber\\\\\n&+ \\eta \\mathbb{E} \\|\\frac{1}{N} \\sum_{n=1}^N \\nabla F_n(\\mathbf{x}_{n, t}) - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 + \\eta^2 S_F \\mathbb{E} \\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) - \\bar{\\mathbf{u}}_{t}\\|^2\n\\end{align}\nwhere inequality (a) holds by the smoothness of $F$; equality (b) follows from update step in Step 9 of Algorithm \\ref{alg:1} and \\cref{lem:A1} (b); (c) uses the fact that $\\mathbb{E} u_{n,t} = \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})$ and $\\|a + b\\|^2 \\leq 2\\|a\\|^2 + 2\\|b\\|^2$; (d) holds since the inequality $ \\langle a, b\\rangle = \\frac{1}{2}[ \\|a\\|^2 +\\|a\\|^2 - \\|a - b\\|^2]$. Taking expectation on both sides and considering the last third term\n\\begin{align}\n\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{x}}_{t})- \\frac{1}{N} \\sum_{n=1}^N \\nabla F_n(\\mathbf{x}_{n, t})\\|^{2} &\\leq \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\|\\nabla F_n(\\bar{\\mathbf{x}}_{t}) - \\nabla F_n(\\mathbf{x}_{n, t})\\|^{2} \\nonumber\\\\\n &\\leq \\frac{S_F^{2}}{N} \\sum_{i=1}^{N} \\|x_{n,t} - \\bar{\\mathbf{x}}_{t}\\|^{2} \n\\end{align}\nConsidering the last second term and \\Cref{lem:A1} (a), we have\n\\begin{align}\n\\mathbb{E}\\|\\frac{1}{N} \\sum_{n=1}^N \\nabla F_n(\\mathbf{x}_{n, t}) - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 &\\leq \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{E} \\| \\nabla F_n(\\mathbf{x}_{n, t}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 \\nonumber\\\\\n&\\leq \\frac{L_g^2 S_f^2 \\sigma_g^2 }{m}\n\\end{align}\nGiven that $\\mathbb{E} u_{n,t} = \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})$ and \\Cref{lem:A1} (c), for the last term, we have \n\\begin{align} \\label{eq:21}\n\\mathbb{E} \\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) - \\bar{\\mathbf{u}}_{t}\\|^2 \\leq \\frac{1}{N^2} \\sum_{n=1}^N \\mathbb{E} \\| \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) - \\mathbf{u}_{n, t}\\|^2 \\leq  \\frac{ L_f^2 L_g^2}{N}\n\\end{align}\n\nTherefore, we obtain\n\\begin{align}\n\\mathbb{E} F(\\bar{\\mathbf{x}}_{t+1}) &  \\leq \\mathbb{E} F(\\bar{\\mathbf{x}}_{t}) - (\\frac{\\eta}{2} - \\eta^2 S_F ) \\mathbb{E} \\|\\nabla \\hat{F}(\\bar{\\mathbf{x}}_t) \\|^2 - \\frac{\\eta}{2} \\mathbb{E}\\| \\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 + \\frac{\\eta S_F^2}{N} \\sum_{n=1}^{N}\\|x_{n,t} - \\bar{\\mathbf{x}}_t\\|^2 \\nonumber\\\\\n&+ \\frac{\\eta L_g^2 S_f^2 \\sigma_g^2}{m} + \\frac{\\eta^2 S_F L_f^2 L_g^2}{N}\n\\end{align}\n\\end{proof}\n\n\n% %%%%%%%%%%%%\n\\begin{lemma} \\label{lem:7}\nLet Assumptions \\ref{ass:1}, \\ref{ass:2}  hold. We have: $\\forall t \\geq 0$,\n\\begin{align}\n& \\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t + 1} - \\mathbf{J} \\mathbf{v}_{t + 1}\\right\\|^2\\right] \n\\leq  \\frac{1 +\\lambda^2}{2} \\mathbb{E}\\left [\\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right\\|^2\\right] + \\frac{6 \\lambda^2 \\eta^2 S_F^2 N}{1 - \\lambda^2} \\mathbb{E} \\left\\|\\nabla \\hat{F}_t\\right\\|^2 \\nonumber\\\\\n&+ \\frac{24 \\lambda^2 S_F^2}{1 - \\lambda^2} \\mathbb{E}\\left[\\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2\\right] + \\left(\\frac{6 \\lambda^2 \\eta^2 S_F^2 N}{1 - \\lambda^2} + 3 N + 2\\right) L_f^2 L_g^2 \n\\end{align}\n\\end{lemma}\n\\begin{proof}\nUsing the gradient tracking update step in \\Cref{alg:1}, and the fact that $\\mathbf{W J}=\\mathbf{J} \\mathbf{W}=\\mathbf{J}$, we have: $\\forall t \\geq 0$,\n\\begin{align} \\label{eq:22}\n&\\mathbb{E}\\left\\|\\mathbf{v}_{t + 1} - \\mathbf{J} \\mathbf{v}_{t + 1}\\right\\|^2 \\nonumber\\\\\n=&\\mathbb{E}\\left\\|\\mathbf{W}\\left(\\mathbf{v}_{t} + \\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right) - \\mathbf{J} \\left(\\mathbf{v}_{t} + \\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right)\\right\\|^2  \\nonumber\\\\\n=&\\mathbb{E} \\left\\|\\mathbf{W} \\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t} + (\\mathbf{W} - \\mathbf{J})\\left(\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right)\\right\\|^2  \\nonumber\\\\\n=&\\mathbb{E}\\left\\|\\mathbf{W} \\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right\\|^2 + \\mathbb{E} \\left\\|(\\mathbf{W} - \\mathbf{J})\\left(\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right)\\right\\|^2  \\nonumber\\\\\n&+2\\mathbb{E}\\left\\langle(\\mathbf{W}-\\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J})\\left(\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right)\\right\\rangle  \\nonumber\\\\\n\\leq & \\lambda^2\\mathbb{E}\\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t} \\right\\|^2+\\lambda^2\\mathbb{E}\\left\\|\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right\\|^2  \\nonumber\\\\\n&+2 \\mathbb{E}\\left\\langle(\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J})\\left(\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right)\\right\\rangle\n\\end{align}\n\nwhere the last inequality is due to \\Cref{lem:A2} (a). since $u_{t+1}$ and $v_{t+1}$ are $\\mathcal{F}_{t+1}$ -measurable.\n\nFor the second term in \\eqref{eq:22}, we have \n%%%%%%%\n\\begin{align} \\label{eq:23}\n& \\mathbb{E} \\left\\|\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right\\|^2 \\nonumber\\\\\n\\stackrel{(a)}{=}& \\mathbb{E}[\\|\\mathbf{u}_{t + 1} - \\nabla \\hat{\\mathbf{F}}_{t + 1}\\|^2 + \\|\\nabla \\hat{\\mathbf{F}}_{t + 1} - \\mathbf{u}_{t}\\|^2]  \\nonumber\\\\\n=& \\mathbb{E}[\\|\\mathbf{u}_{t + 1} - \\nabla \\hat{\\mathbf{F}}_{t + 1}\\|^2 + 2\\|\\nabla \\hat{\\mathbf{F}}_{t + 1} - \\nabla \\hat{\\mathbf{F}}_{t}\\|^2 + 2\\|\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t}\\|^2]  \\nonumber\\\\\n\\leq & 3 N L_f^2 L_g^2  + 2 S_F^2 \\mathbb{E}\\left[\\left\\|\\mathbf{x}_{t + 1}-\\mathbf{x}_{t}\\right\\|^2\\right]\n\\end{align}\nwhere the equality (a) follows that $\\mathbb{E}[\\mathbf{u}_{t + 1} \\mid \\mathcal{F}_{t+1}] = \\nabla \\hat{\\mathbf{F}}_{t + 1}$ and the last inequality uses \\Cref{lem:A1} (b) and (c). $\\forall t \\geq 0$,\n\\begin{align} \\label{eq:24}\n\\mathbb{E}\\left\\|\\mathbf{x}_{t + 1} - \\mathbf{x}_{t}\\right\\|^2 =& \\mathbb{E}\\left\\|\\mathbf{x}_{t + 1} - \\mathbf{J} \\mathbf{x}_{t + 1} + \\mathbf{J} \\mathbf{x}_{t + 1} - \\mathbf{J} \\mathbf{x}_{t} + \\mathbf{J} \\mathbf{x}_{t} - \\mathbf{x}_{t}\\right\\|^2 \\nonumber\\\\\n\\stackrel{(a)}{\\leq} & 3 \\mathbb{E} \\left\\|\\mathbf{x}_{t + 1} - \\mathbf{J} \\mathbf{x}_{t + 1}\\right\\|^2 + 3 N \\eta^2 \\mathbb{E} \\left\\|\\bar{\\mathbf{u}}_{t}\\right\\|^2 + 3 \\mathbb{E} \\left\\|\\mathbf{x}_{t} - \\mathbf{J} \\mathbf{x}_{t}\\right\\|^2  \\nonumber \\\\\n\\stackrel{(b)}{\\leq} & 9 \\mathbb{E}\\left[\\left\\|\\mathbf{x}_{t} - \\mathbf{J} \\mathbf{x}_{t}\\right\\|^2\\right] + 3 N \\eta^2 \\mathbb{E}\\left[\\left\\|\\bar{\\mathbf{u}}_{t}\\right\\|^2\\right] + 6 \\eta^2 \\lambda^2 \\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right\\|^2\\right]\n\\end{align}\nwhere (b) holds due to \\Cref{lem:A2} (b), and (c) holds due to \\eqref{eq:17} and $\\lambda \\leq 1$.\nPutting the \\eqref{eq:24} into \\eqref{eq:23}, we have \n\\begin{align} \\label{eq:25}\n& \\mathbb{E} \\left\\|\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right\\|^2 \\leq 3 N L_f^2 L_g^2  + 18 S_F^2 \\mathbb{E} \\left\\|\\mathbf{x}_{t} - \\mathbf{J} \\mathbf{x}_{t}\\right\\|^2 + 6 N \\eta^2 S_F^2 \\mathbb{E} \\left\\|\\bar{\\mathbf{u}}_{t}\\right\\|^2 + 12 \\eta^2 \\lambda^2 S_F^2 \\mathbb{E} \\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right\\|^2\n\\end{align}\n\nFor the last term in \\eqref{eq:22}, we have,\n\\begin{align} \\label{eq:31}\n& \\mathbb{E}\\left[\\left\\langle(\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J})\\left(\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right)\\right\\rangle \\mid \\mathcal{F}_{t + 1}\\right]  \\nonumber\\\\\n=&\\mathbb{E}\\left\\langle(\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t + 1} - \\mathbf{u}_{t}\\right)\\right\\rangle  \\nonumber\\\\\n=&\\mathbb{E}\\left\\langle(\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t} - \\mathbf{u}_{t} \\right) \\right \\rangle + \\mathbb{E}\\left\\langle(\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J}) \\left( \\nabla \\hat{\\mathbf{F}}_{t + 1} -  \\nabla \\hat{\\mathbf{F}}_{t}\\right)\\right\\rangle\n\\end{align}\n\nFurthermore, we have \n\\begin{align} \\label{eq:32} &\\mathbb{E}\\left[\\left\\langle(\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t} - \\mathbf{u}_{t}\\right)\\right\\rangle \\mid \\mathcal{F}_t\\right] \\nonumber\\\\\n\\stackrel{(a)}{=}& \\mathbb{E}\\left[\\left\\langle\\mathbf{W} \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t} - \\mathbf{u}_{t}\\right)\\right\\rangle \\mid \\mathcal{F}_t\\right] \\nonumber\\\\\n\\stackrel{(b)}{=}& \\mathbb{E}\\left[\\left\\langle\\mathbf{W}^2 \\left(\\mathbf{v}_{t - 1} + \\mathbf{u}_{t} - \\mathbf{u}_{t-1}\\right), (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t} - \\mathbf{u}_{t}\\right)\\right\\rangle \\mid \\mathcal{F}_t\\right] \\nonumber\\\\\n\\stackrel{(c)}{=}& \\mathbb{E}\\left[\\left\\langle\\mathbf{W}^2 \\mathbf{u}_t, (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t} - \\mathbf{u}_{t}\\right) \\right\\rangle \\mid \\mathcal{F}_t\\right] \\nonumber\\\\\n=& \\mathbb{E}\\left[\\left\\langle\\mathbf{W}^2\\left(\\mathbf{u}_t - \\nabla \\hat{\\mathbf{F}}_t\\right), (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_t - \\mathbf{u}_t\\right)\\right\\rangle \\mid \\mathcal{F}_t \\right] \\nonumber\\\\\n=& \\mathbb{E}\\left[\\left(\\mathbf{u}_t - \\nabla \\hat{\\mathbf{F}}_t \\right)^{\\top}\\left(\\mathbf{J}-\\mathbf{W}^{\\top} \\mathbf{W}^2\\right)\\left(\\mathbf{u}_t - \\nabla \\hat{\\mathbf{F}}_t\\right) \\mid \\mathcal{F}_t\\right] \\nonumber\\\\\n=& \\mathbb{E}\\left[\\left(\\mathbf{u}_t - \\nabla \\hat{\\mathbf{F}}_t\\right)^{\\top} \\operatorname{diag}\\left(\\mathbf{J}-\\mathbf{W}^{\\top} \\mathbf{W}^2\\right)\\left(\\mathbf{u}_t - \\nabla \\hat{\\mathbf{F}}_t\\right) \\mid \\mathcal{F}_t\\right] \\nonumber\\\\\n\\leq &  \\mathbb{E}\\left[\\left\\|\\mathbf{u}_t - \\nabla \\hat{\\mathbf{F}}_t\\right\\|^2 \\mid \\mathcal{F}_t\\right] / N \\leq L_f^2 L_g^2\n\\end{align}\nwhere (a) holds since $\\mathbf{J} (\\mathbf{W} - \\mathbf{J}) = \\mathbf{O}_{np}$, (b) because the update step of $\\mathbf{v}_t$; (c) because $\\mathbb{E} \\mathbf{u}_t = \\nabla \\hat{F}_t$ and $\\mathbf{v}_{t - 1}$ and $\\mathbf{u}_{t - 1}$ are independently. \n\nFor the second term in \\eqref{eq:31}, we have\n\\begin{align} \\label{eq:31}\n&\\mathbb{E} \\left\\langle(\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W}-\\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t+1} - \\nabla \\hat{\\mathbf{F}}_t \\right)\\right\\rangle \\nonumber\\\\\n\\stackrel{(a)}{=}& \\mathbb{E} \\left\\langle(\\mathbf{W} - \\mathbf{J})\\left(\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right), (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_t - \\nabla \\hat{\\mathbf{F}}_{t - 1} \\right)\\right\\rangle \\nonumber\\\\\n\\stackrel{(b)}{\\leq} & \\lambda^2 S_F \\left\\|\\mathbf{v}_{t}-\\mathbf{J} \\mathbf{v}_{t}\\right\\|\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_t\\right\\|\n\\end{align}\nwhere (a) follows  $(\\mathbf{W}-\\mathbf{J}) \\mathbf{J}=\\mathbf{O}_{n p}$ and (b) the Cauchy-Schwarz inequality and smooth of $\\hat{F}(x) $, we have: $\\forall k \\geq 0$,\nwhere the last inequality uses $\\|\\mathbf{W}-\\mathbf{J}\\|=\\lambda$ and the $S_F$ smoothness of $\\hat{F}_{n,t}$. \n\nFurthermore, $\\forall t \\geq 0$,\n\\begin{align} \\label{eq:29}\n&\\left\\|\\mathbf{x}_{t+1} - \\mathbf{x}_t\\right\\| \\nonumber\\\\\n=&\\left\\|\\mathbf{x}_{t+1} - \\mathbf{J} \\mathbf{x}_{t+1} + \\mathbf{J} \\mathbf{x}_{t+1} - \\mathbf{J} \\mathbf{x}_t + \\mathbf{J} \\mathbf{x}_t - \\mathbf{x}_t\\right\\| \\nonumber\\\\\n\\leq &\\left\\|\\mathbf{x}_{t+1} - \\mathbf{J} \\mathbf{x}_{t+1}\\right\\| + \\eta \\sqrt{N}\\left\\|\\bar{\\mathbf{u}}_t\\right\\| + \\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\| \\nonumber\\\\\n\\leq & 2\\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t \\right\\| + \\eta \\sqrt{N} \\left\\|\\bar{\\mathbf{u}}_t\\right\\| + \\eta \\lambda\\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t} \\right\\|\n\\end{align}\n\nwhere the last inequality uses \\eqref{eq:18}. Combining the above two inequalities, we have\n\n\\begin{align} \\label{eq:31}\n&\\left\\langle(\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_{t}, (\\mathbf{W} - \\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t+1} - \\nabla \\hat{\\mathbf{F}}_{t}\\right)\\right\\rangle \\nonumber\\\\\n\\leq & \\lambda^3 \\eta S_F \\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t} \\right\\|^2 + \\left(\\lambda\\left\\|\\mathbf{v}_{t} - \\mathbf{J v}_{t}\\right\\|\\right)\\left(\\lambda \\eta S_F \\sqrt{N}\\left\\|\\bar{\\mathbf{u}}_t\\right\\|\\right) + 2\\left(\\lambda\\left\\|\\mathbf{v}_{t} - \\mathbf{J v}_{t}\\right\\|\\right)\\left(\\lambda S_F\\left\\|\\mathbf{x}_k-\\mathbf{J} \\mathbf{x}_k\\right\\|\\right) \\nonumber \\\\\n\\stackrel{(a)}{\\leq} & \\lambda^3 \\eta S_F \\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t} \\right\\|^2 +  \\frac{c_0 \\lambda^2}{2}\\left\\|\\mathbf{v}_{t} - \\mathbf{J v}_{t}\\right\\|^2 + \\frac{ \\lambda^2 \\eta^2 S_F^2 N}{2c_0}\\left\\|\\bar{\\mathbf{u}}_t\\right\\|^2 + c_1 \\lambda^2\\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right\\|^2 + \\frac{\\lambda^2 S_F^2}{c_1} \\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2 \\nonumber\\\\\n\\leq& (\\lambda^3 \\eta S_F + \\frac{c_0 \\lambda^2}{2} + c_1 \\lambda^2) \\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t} \\right\\|^2  + \\frac{ \\lambda^2 \\eta^2 S_F^2 N}{2c_0}\\left\\|\\bar{\\mathbf{u}}_t\\right\\|^2 +  \\frac{\\lambda^2 S_F^2}{c_1} \\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2\n\\end{align}\nwhere (a) holds due to the Youngâ€™s inequality and $c_0, c_1 (> 0)$ are arbitrary.\n\nThen putting \\eqref{eq:25}, \\eqref{eq:31}, \\eqref{eq:32} and \\eqref{eq:31} into \\eqref{eq:22}, we have\n\\begin{align}\n\\mathbb{E} \\|\\mathbf{v}_{t + 1} - \\mathbf{J v}_{t + 1} \\|^2  \\leq& \\lambda^2 (1 + 12 \\lambda^2 \\eta^2 S_F^2 + 2 \\lambda \\eta S_F + c_0 + 2 c_1) \\mathbb{E}  \\|\\mathbf{v}_{t} - \\mathbf{J v}_{t} \\|^2 + 3 \\lambda^2 N L_f^2L_g^2 \\nonumber\\\\\n+ 2 L_f^2 L_g^2  &+ (18 + \\frac{2}{c_1} ) \\lambda^2 S_F^2 \\mathbb{E} \\| \\mathbf{x}_{t} - \\mathbf{J x}_{t} \\|^2 + (6 + \\frac{1}{c_0}) \\lambda^2 \\eta^2 S_F^2 N \\mathbb{E} \\|\\bar{\\mathbf{u}}_t \\|^2.\n\\end{align}\nWe set $c_0 = \\frac{1 - \\lambda^2}{6 \\lambda^2}$ and $c_1 = \\frac{1 - \\lambda^2}{12 \\lambda^2}$. When $0 < \\eta \\leq \\min \\{\\frac{1 - \\lambda^2}{24 \\lambda^2 S_F}, \\frac{1}{6 S_F}\\}$, we have $\\lambda^2 (1 + 12 \\lambda^2 \\eta^2 S_F^2 + 2 \\lambda \\eta S_F + c_0 + 2 c_1) \\leq \\frac{1 + \\lambda^2}{2}$, and the fact \n\\begin{align}\n\\mathbb{E} \\|\\bar{\\mathbf{u}}_t\\| =&\\mathbb{E} \\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 +  \\mathbb{E} \\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) - \\bar{\\mathbf{u}}_{t}\\|^2 \\nonumber\\\\\n\\leq& 2\\mathbb{E} \\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{\\bar{x}}_{t})\\|^2 + 2\\mathbb{E} \\| \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{\\bar{x}}_{t})\\|^2 +  \\frac{ L_f^2 L_g^2}{N} \\nonumber\\\\\n\\leq& \\frac{2S_F^2}{N} \\sum_{n=1}^N\\mathbb{E} \\| \\mathbf{x}_{n, t} - \\mathbf{\\bar{x}}_{t}\\|^2 + 2\\mathbb{E} \\| \\nabla \\hat{F}(\\mathbf{\\bar{x}}_{t})\\|^2 +  \\frac{ L_f^2 L_g^2}{N} \\nonumber\n\\end{align}\nthen we have\n\\begin{align}\n\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t + 1} - \\mathbf{J} \\mathbf{v}_{t + 1}\\right\\|^2\\right] \n\\leq & \\frac{1 +\\lambda^2}{2} \\mathbb{E}\\left [\\left\\|\\mathbf{v}_{t} - \\mathbf{J} \\mathbf{v}_{t}\\right\\|^2\\right] + \\left(\\frac{6 \\lambda^2 \\eta^2 S_F^2 N}{1 - \\lambda^2} + 3 N + 2\\right) L_f^2 L_g^2 \\nonumber\\\\\n&+ \\frac{36 \\lambda^2 S_F^2}{1 - \\lambda^2} \\mathbb{E}\\left[\\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2\\right] + \\frac{12 \\lambda^2 \\eta^2 S_F^2 N}{1 - \\lambda^2} \\mathbb{E} \\left\\|\\nabla \\hat{F}(\\bar{\\mathbf{x}}_{ t})\\right\\|^2 \n\\end{align}\nwhere \n\n% he proof follows by $\\frac{6 \\lambda^2 \\alpha_k^2 L^2}{1-\\lambda^2} \\leq 1$ if $0<\\alpha_k \\leq \\frac{1-\\lambda^2}{24 \\lambda L}, \\forall k$\n\\end{proof}\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Proofs of Theorem ",
                    "content": "\nBased on previous lemmas, we start to prove the convergence of Theorem. \n\\begin{proof}\nRecall \\Cref{lem:A2},  we have \n\\begin{align} \\label{eq:38}\n\\left\\|\\mathbf{x}_{t+1}-\\mathbf{J} \\mathbf{x}_{t+1}\\right\\|^2 \\leq & \\frac{1+\\lambda^2}{2}\\left\\|\\mathbf{x}_t-\\mathbf{J} \\mathbf{x}_t\\right\\|^2 +\\frac{2 \\eta^2 \\lambda^2}{1-\\lambda^2}\\left\\|\\mathbf{v}_{t}-\\mathbf{J} \\mathbf{v}_{t}\\right\\|^2\n\\end{align}\n\nPutting \\eqref{eq:38} and \\cref{lem:7} into \\cref{lem:A3}, then we have \n\\begin{align} \\label{eq:39}\n\\sum_{t=0}^{T}\\|\\mathbf{x}_t - \\mathbf{J x}_{t} \\|^2 \\leq \\frac{4\\lambda^2 \\eta^2}{(1- \\lambda^2)^2} \\sum_{t=0}^{T}\\|\\mathbf{v}_t - \\mathbf{J v}_{t} \\|^2\n\\end{align}\n\n\\begin{align} \\label{eq:40}\n&\\sum_{t=0}^{T} \\|\\mathbf{v}_t - \\mathbf{J v}_{t} \\|^2 \\leq  \\frac{2}{1 - \\lambda^2} \\|\\mathbf{v}_0 - \\mathbf{J v}_{0} \\|^2 + \\left(\\frac{6 \\lambda^2 \\eta^2 S_F^2}{1 - \\lambda^2} + 5 N\\right) \\frac{2 L_f^2 L_g^2 T}{1 - \\lambda^2 } \\nonumber\\\\\n&+ \\frac{72 \\lambda^2 S_F^2}{(1 - \\lambda^2)^2} \\sum_{t=0}^{ T} \\mathbb{E}\\left[\\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2\\right] + \\frac{24 \\lambda^2 \\eta^2 S_F^2 N}{(1 - \\lambda^2)^2} \\sum_{t=0}^{t = T}\\mathbb{E}\\left\\| \\nabla \\hat{F}(\\bar{\\mathbf{x}}_{ t}) \\right\\|^2\n\\end{align}\n\nThen putting \\eqref{eq:39} into \\eqref{eq:40}, we get \n\\begin{align}\n&\\sum_{t=0}^{T}\\|\\mathbf{x}_t - \\mathbf{J x}_{t} \\|^2 \n\\leq \\frac{4\\lambda^2 \\eta^2}{(1- \\lambda^2)^2}\\frac{2}{1 - \\lambda^2} \\|\\mathbf{v}_0 - \\mathbf{J v}_{0} \\|^2 + \\frac{96 \\lambda^4 \\eta^4 S_F^2 N}{(1 - \\lambda^2)^4} \\sum_{t=0}^{t = T}\\mathbb{E}\\left\\|\\nabla \\hat{F}(\\bar{\\mathbf{x}}_{ t})\\right\\|^2  \\nonumber\\\\\n&+ \\frac{288 \\lambda^4 \\eta^2 S_F^2}{(1 - \\lambda^2)^4} \\sum_{t=0}^{ T} \\mathbb{E} \\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2 + \\left(\\frac{6 \\lambda^2 \\eta^2 S_F^2}{1 - \\lambda^2} + 5 N\\right) \\frac{8 \\lambda^2 \\eta^2 L_f^2 L_g^2 T}{(1 - \\lambda^2)^3} \\nonumber\n\\end{align}\n\nThen we have \n\\begin{align}\n&[1 - \\frac{288 \\lambda^4 \\eta^2 S_F^2}{(1 - \\lambda^2)^4} ]\\sum_{t=0}^{T}\\|\\mathbf{x}_t - \\mathbf{J x}_{t} \\|^2 \n\\leq  \\left(\\frac{6 \\lambda^2 \\eta^2 S_F^2}{1 - \\lambda^2} + 5 N\\right) \\frac{8 \\lambda^2 \\eta^2 L_f^2 L_g^2 T}{(1 - \\lambda^2)^3} + \\frac{96 \\lambda^4 \\eta^4 S_F^2 N}{(1 - \\lambda^2)^4} \\sum_{t=0}^{t = T}\\mathbb{E}\\left\\|\\nabla \\hat{F}(\\bar{\\mathbf{x}}_{ t}) \\right\\|^2\n\\end{align}\nWhen $0 < \\eta \\leq  \\min \\{\\frac{1 - \\lambda^2}{24 \\lambda^2 S_F}, \\frac{1}{6 S_F}\\}$, we have $[1 - \\frac{192 \\lambda^4 \\eta^2 S_F^2}{(1 - \\lambda^2)^4} ] \\geq \\frac{1}{2}$,\nTherefore, we have \n\\begin{align} \\label{eq:43}\n\\sum_{t=0}^{T}\\|\\mathbf{x}_t - \\mathbf{J x}_{t} \\|^2 \\leq  \n\\left(\\frac{1}{ \\lambda^2} + 5 N\\right) \\frac{16 \\lambda^2 \\eta^2 L_f^2 L_g^2 T}{(1 - \\lambda^2)^2}  + \\frac{192 \\lambda^4 \\eta^4 S_F^2 N}{(1 - \\lambda^2)^4} \\sum_{t=0}^{t = T}\\mathbb{E}\\left\\|\\nabla \\hat{F}_t\\right\\|^2 \n\\end{align}\nPutting \\eqref{eq:43} into \\cref{lem:B1}, we have\n\\begin{align}\n&\\frac{1}{T} \\sum_{t=0}^{T - 1} \\mathbb{E} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \n\\leq \\frac{2  \\mathbb{E} [F(\\bar{\\mathbf{x}}_{0}) - F(\\bar{\\mathbf{x}}_{T})] }{\\eta T}  + \\frac{2 \\eta S_F L_f^2 L_g^2}{N} \n\\nonumber\\\\\n&+ \\frac{2 S_F^2}{N T } \\sum_{t=0}^{T - 1} \\sum_{n=1}^{N}\\|x_{n,t} - \\bar{\\mathbf{x}}_t\\|^2 + \\frac{2 L_g^2 S_f^2 \\sigma_g^2}{m} - \\frac{(1 - 2 \\eta S_F )}{T} \\sum_{t=0}^{T - 1} \\mathbb{E}\\|\\nabla \\hat{F}(\\bar{\\mathbf{x}}_t) \\|^2 \\nonumber\\\\\n&\\leq \\frac{2  \\mathbb{E} [F(\\bar{\\mathbf{x}}_{0}) - F(\\bar{\\mathbf{x}}_{T})] }{\\eta T}\n- \\frac{1}{T}[1 - 2 \\eta S_F - \\frac{384 \\lambda^4 \\eta^4 S_F^4 }{(1 - \\lambda^2)^4}] \\sum_{t = 0}^{T - 1} \\|\\nabla \\hat{F}(\\bar{\\mathbf{x}}_t) \\|^2 \\nonumber\\\\\n&+  \\left(\\frac{1}{ \\lambda^2} + 5 N\\right) \\frac{32 \\lambda^2 \\eta^2 L_f^2 L_g^2 S_F^2}{(1 - \\lambda^2)^2 N}  + \\frac{2 L_g^2 S_f^2 \\sigma_g^2}{m} + \\frac{2 \\eta S_F L_f^2 L_g^2}{N}  \\nonumber\\\\\n&\\leq \\frac{2  \\mathbb{E} [F(\\bar{\\mathbf{x}}_{0}) - F(\\bar{\\mathbf{x}}_{T})] }{\\eta T}\n+  \\left(\\frac{1}{ \\lambda^2} + 5 N\\right) \\frac{32 \\lambda^2 \\eta^2 L_f^2 L_g^2 S_F^2}{(1 - \\lambda^2)^2 N}  + \\frac{2 L_g^2 S_f^2 \\sigma_g^2}{m} + \\frac{2 \\eta S_F L_f^2 L_g^2}{N} \\nonumber\n\\end{align}\n\\end{proof}\n% %%%%%%%%%%%%%%%%%\n\nThen we discuss the convergence rate of the \\cref{alg:1}. Setting $\\eta = O(\\sqrt{\\frac{N}{T}})$ , we have final\n\\begin{align}\n\\frac{1}{T} \\sum_{t=0}^{T - 1} \\mathbb{E} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \\leq O(\\frac{\\mathbb{E} [F(\\bar{\\mathbf{x}}_{0}) - F(\\bar{\\mathbf{x}}_{T})] }{(N T)^{1/2}})\n+  \\left(\\frac{1}{ \\lambda^2} + 5 N\\right) \\frac{32 \\lambda^2 L_f^2 L_g^2 S_F^2}{(1 - \\lambda^2)^2} O(\\frac{1}{T})  + \\frac{2 L_g^2 S_f^2 \\sigma_g^2}{m} + O(\\frac{2 S_F L_f^2 L_g^2}{(NT)^{1/2}})\n\\end{align}\n\nLet $m = O(\\varepsilon^{-2})$ and $\\sqrt{T} > N$, we know to make $\\frac{1}{T} \\sum_{t=0}^{T - 1} \\mathbb{E} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \\leq \\varepsilon^{2}$, we have $T \\leq N^{-1} \\varepsilon^{-4} $. \n% %%%%%%%%%%%%%%%%%%\n\n"
                }
            },
            "section 10": {
                "name": "Proof of SLATE-M Algorithm",
                "content": "\n\n% %%%%%%%%%%%%%%%%%\n",
                "subsection 10.1": {
                    "name": "Proofs of the Intermediate Lemmas",
                    "content": "\n\\begin{lemma} \\label{lem:C1}\nSuppose the sequence $\\{x_t\\}_0^{T}$ are generated from SLATE-M in \\cref{alg:2}, we have\n\\begin{align}\nF(\\bar{\\mathbf{x}}_{t+1}) &  \\leq F(\\bar{\\mathbf{x}}_{t}) - (\\frac{\\eta}{2} - \\frac{\\eta^2 S_F }{2} ) \\|\\bar{\\mathbf{u}}_{t} \\|^2 - \\frac{\\eta}{2} \\| \\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 + \\frac{3\\eta S_F^2}{2N} \\|\\mathbf{x}_{t} - \\bar{\\mathbf{x}}_t\\|^2 \\nonumber\\\\ \n&+ \\frac{3\\eta L_g^2 S_f^2 \\sigma_g^2}{2m} + \\frac{3\\eta}{2}\\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) - \\bar{\\mathbf{u}}_{t}\\|^2\n\\end{align}\n\\end{lemma}\n\\begin{proof}\n\\begin{align}\nF(\\bar{\\mathbf{x}}_{t+1}) &\\stackrel{(a)}{\\leq} F(\\bar{\\mathbf{x}}_{t}) + \\langle\\nabla F(\\bar{\\mathbf{x}}_{t}), \\bar{\\mathbf{x}}_{t+1}-\\bar{\\mathbf{x}}_{t}\\rangle + \\frac{S_F}{2}\\left\\|\\bar{\\mathbf{x}}_{t+1} - \\bar{\\mathbf{x}}_{t}\\right\\|^{2}  \\nonumber\\\\\n&\\stackrel{(b)}{=}F(\\bar{\\mathbf{x}}_{t}) - \\eta \\langle\\nabla F(\\bar{\\mathbf{x}}_{t}), \\bar{\\mathbf{u}}_{t}\\rangle + \\frac{\\eta^2 S_F }{2} \\|\\bar{\\mathbf{u}}_{t}\\|^{2} \\nonumber\\\\\n&\\stackrel{(c)}{=}F(\\bar{\\mathbf{x}}_{t}) - (\\frac{\\eta}{2} - \\frac{\\eta^2 S_F }{2} ) \\|\\bar{\\mathbf{u}}_{t} \\|^2 - \\frac{\\eta}{2} \\| \\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 + \\frac{\\eta}{2} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})- \\bar{\\mathbf{u}}_{t}\\|^2 \\nonumber\\\\\n&\\leq  F(\\bar{\\mathbf{x}}_{t}) - (\\frac{\\eta}{2} - \\frac{\\eta^2 S_F }{2} ) \\|\\bar{\\mathbf{u}}_{t} \\|^2 - \\frac{\\eta}{2} \\| \\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 + \\frac{3\\eta}{2} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})- \\frac{1}{N} \\sum_{n=1}^N \\nabla F_n(\\mathbf{x}_{n, t})\\|^2 \\nonumber\\\\ \n&+ \\frac{3\\eta}{2} \\|\\frac{1}{N} \\sum_{n=1}^N \\nabla F(\\mathbf{x}_{n, t}) - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 + \\frac{3\\eta}{2} \\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n (\\mathbf{x}_{n, t}) - \\bar{\\mathbf{u}}_{t}\\|^2\n\\end{align}\nwhere inequality (a) holds by the smoothness of F(x); equality (b) follows from update step in Step 9 of Algorithm \\ref{alg:1}; (c) uses the fact that $ \\langle a, b\\rangle = \\frac{1}{2}[ \\|a\\|^2 +\\|a\\|^2 - \\|a - b\\|^2]$.  Taking expectation on both sides and considering the last third term\n\\begin{align}\n\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{x}}_{t}) - \\frac{1}{N} \\sum_{n=1}^N \\nabla F_n(\\mathbf{x}_{n, t})\\|^{2} \\leq \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{E}\\|\\nabla F_n(\\bar{\\mathbf{x}}_{t}) - \\nabla F_n(\\mathbf{x}_{n, t})\\|^{2} \\leq \\frac{S_F^{2}}{N} \\sum_{n=1}^{N} \\|x_{n,t} - \\bar{\\mathbf{x}}_{t}\\|^{2} \n\\end{align}\nConsidering the last second term, we have\n\\begin{align}\n\\mathbb{E}\\|\\frac{1}{N} \\sum_{n=1}^N \\nabla F_n (\\mathbf{x}_{t}) - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{t})\\|^2 \\leq \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{E} \\| \\nabla F_n (\\mathbf{x}_{n, t}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 \\leq \\frac{L_g^2 S_f^2 \\sigma_g^2 }{m}\n\\end{align}\n\nTherefore, we obtain\n\\begin{align}\nF(\\bar{\\mathbf{x}}_{t+1}) &  \\leq F(\\bar{\\mathbf{x}}_{t}) - (\\frac{\\eta}{2} - \\frac{\\eta^2 S_F }{2} ) \\|\\bar{\\mathbf{u}}_{t} \\|^2 - \\frac{\\eta}{2} \\| \\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 + \\frac{3\\eta S_F^2}{2N} \\sum_{n=1}^{N} \\|x_{n,t} - \\bar{\\mathbf{x}}_{t}\\|^{2}  \\nonumber\\\\ \n&+ \\frac{3\\eta L_g^2 S_f^2 \\sigma_g^2}{2m} + \\frac{3\\eta}{2}\\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) - \\bar{\\mathbf{u}}_{t}\\|^2\n\\end{align}\n\\end{proof}\n\n\\begin{lemma} \\label{lem:A6}\nAssume that the stochastic partial derivatives $u_{t}$  be generated from SLATE-M in Algorithm \\ref{alg:2}, we have\n\\begin{align}\n&\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t + 1} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})\\|^2 \n\\leq (1 \\alpha)^{2}\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^{2} + \\frac{2(1 -  \\alpha)^2 S_F^2}{N^2b} \\mathbb{E}\\|\\mathbf{x}_{t + 1} - \\mathbf{x}_{t}\\|^2 + \\frac{2 \\alpha^2 L_g^2 L_f^2}{Nb}    \\nonumber\\\\\n&\\mathbb{E}\\|u_{n, t + 1} - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})\\|^2 \\leq (1-\\alpha)^{2}\\mathbb{E}\\|\\mathbf{u}_{n, t} - \\nabla \\hat{F}(\\mathbf{x}_{n, t})\\|^{2} + \\frac{2(1 -  \\alpha)^2 S_F^2}{b} \\mathbb{E}\\|x_{n, t + 1} - \\mathbf{x}_{n, t}\\|^2 + \\frac{2 \\alpha^2 L_g^2 L_f^2}{b} \n\\end{align}\n\\end{lemma}\n\\begin{proof}\nRecall that $ \\bar{\\mathbf{u}}_{t + 1} = \\frac{1}{N}\\sum_{n=1}^{N} [ \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t+1},\\mathcal{B}_{n, t + 1}) + (1 - \\alpha)(u_{n,t} -  \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1}, \\mathcal{B}_{n, t + 1}))]$,\\\\ and $\\nabla \\hat{F}_n(x_{n,t}) = \\mathbb{E}\\nabla\\hat{F}_n(x_{n,t}; \\xi_{n,t}, \\mathcal{B}_{n,t})$, we have\n\\begin{align}\n&\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t + 1} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})\\|^2 \\nonumber\\\\\n=& \\mathbb{E}\\|\\frac{1}{N}\\sum_{n=1}^{N}[\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) + (1- \\alpha)(u_{n,t} -  \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1})) - \\nabla \\hat{F}_{n}(\\mathbf{x}_{n, t+1}) ] \\|^{2} \\nonumber\\\\\n=& \\mathbb{E}\\|\\frac{1}{N}\\sum_{i=1}^{N} [(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1})) - (1 - \\alpha)(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n (\\mathbf{x}_{n, t})) + (1 - \\alpha)(\\bar{\\mathbf{u}}_{t} - \\nabla \\hat{F}(\\mathbf{x}_{n, t})]\\|^2 \\nonumber\\\\\n\\stackrel{(a)}{=}& (1 - \\alpha)^{2} \\mathbb{E}\\|\\bar{\\mathbf{u}}_{t} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^{2} + \\frac{1}{N^2} \\sum_{i=1}^{N}\\mathbb{E} \\|(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1})) \\nonumber\\\\\n&-(1- \\alpha)(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}))\\|^2 \\nonumber\\\\\n=& (1-\\alpha)^{2}\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t} -  \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) \\|^{2} + \\frac{1}{N^2} \\sum_{i=1}^{N} \\mathbb{E}\\|(1 - \\alpha)[(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1})) \\nonumber\\\\\n&- (\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}))] +  \\alpha(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})) \\|^2 \\nonumber\\\\\n\\leq& (1-\\alpha)^{2}\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) \\|^{2} + \\frac{2(1 -  \\alpha)^2}{N^2} \\sum_{i=1}^{N} \\mathbb{E}\\|(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})) \\nonumber\\\\\n&- (\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}))\\|^2 +  \\frac{2 \\alpha^2}{N^2}\\sum_{i=1}^{N} \\mathbb{E}\\|(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}))\\|^2 \\nonumber\\\\\n% &\\leq (1 - \\alpha)^{2} \\mathbb{E}\\|\\bar{\\mathbf{u}}_{t} - \\nabla \\hat{F} (x_{t})\\|^{2} + \\frac{2(1 -  \\alpha)^2}{N^2b^2} \\sum_{i=1}^{N}\\sum_{i = 1}^{b} \\mathbb{E}\\|\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1})  \\nonumber\\\\\n% &- \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1})\\|^2 + \\frac{2 \\alpha^2 }{N^2}\\sum_{i=1}^{N} \\mathbb{E}\\| \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}) \\|^2 \\nonumber\\\\\n\\stackrel{(b)}{\\leq}& (1-\\alpha)^{2}\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}))\\|^{2} +  \\frac{2(1 -  \\alpha)^2 S_F^2}{N^2b} \\mathbb{E}\\|\\mathbf{x}_{t + 1} - \\mathbf{x}_{t}\\|^2 + \\frac{2 \\alpha^2 L_g^2 L_f^2}{Nb} \n\\end{align}\nwhere (a) holds due to $\\mathbb{E} [(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1})) - (1 - \\alpha)(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n (\\mathbf{x}_{n, t}))] = 0$ and (b) is due to the  \\cref{lem:A1} (b).Similarly, we have \n\\begin{align}\n& \\mathbb{E}\\|u_{n, t + 1} - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})\\|^2 \\nonumber\\\\\n=& \\mathbb{E}\\|[\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})] \\nonumber\\\\\n&- (1 -  \\alpha)(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n (x_{n, t + 1})) + (1 - \\alpha)(u_{n, t} - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\|^2 \\nonumber\\\\\n=& (1 - \\alpha)^{2}\\mathbb{E}\\|u_{n, t} - \\nabla \\hat{F}_n (\\mathbf{x}_{n, t})\\|^{2} + \\mathbb{E} \\|(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})) \\nonumber\\\\\n& - (1 - \\alpha)(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}))\\|^2 \\nonumber\\\\\n\\leq& (1 - \\alpha)^{2}\\mathbb{E}\\|u_{n, t} - \\nabla \\hat{F}_n (\\mathbf{x}_{n, t})\\|^{2} + 2(1 - \\alpha)^2 \\mathbb{E}\\|(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})) \\nonumber\\\\\n&- (\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}))\\|^2 +  2 \\alpha^2 \\mathbb{E}\\|(\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1}))\\|^2 \\nonumber\\\\\n% &\\leq (1 - \\alpha)^{2}\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t} - \\nabla \\hat{F} (x_{t-1})\\|^{2} + \\frac{2(1 -  \\alpha)^2}{N^2b^2} \\sum_{i=1}^{N}\\sum_{i = 1}^{b} \\mathbb{E}\\|\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n},\\mathcal{B}_{n, t})  \\nonumber\\\\\n% &- \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{t-1}_n; \\xi^i_{n},\\mathcal{B}_{n, t})\\|^2 + \\frac{2 \\alpha^2 }{N^2}\\sum_{i=1}^{N} \\mathbb{E}\\|\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n},\\mathcal{B}_{n, t}) - \\nabla \\hat{F}_n(\\mathbf{x}_t) \\|^2 \\nonumber\\\\\n\\leq& (1-\\alpha)^{2}\\mathbb{E}\\|\\mathbf{u}_{n, t} - \\nabla \\hat{F}(\\mathbf{x}_{n, t})\\|^{2} + \\frac{2(1 -  \\alpha)^2 S_F^2}{b} \\mathbb{E}\\|x_{n, t + 1} - \\mathbf{x}_{n, t}\\|^2 + \\frac{2 \\alpha^2 L_g^2 L_f^2}{b} \n\\end{align}\n\\end{proof}\n% %%%%%%%%%%%\n\\begin{lemma}\nSuppose sequence $\\mathbf{v}_{t}$ are generated by \\Cref{alg:2} and if $0<\\eta \\leq \\frac{1 - \\lambda^2}{2 \\sqrt{24} \\lambda^2 S_F}$, we have\n% \\begin{align}\n% \\mathbb{E}\\left\\|\\mathbf{v}_{t+1} - \\mathbf{J} \\mathbf{v}_{t+1}\\right\\|^2\n% \\leq & \\frac{3 + \\lambda^2}{4} \\mathbb{E}\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2 + \\frac{21 \\lambda^2 N S_F^2 \\eta^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_{t}\\right\\|^2  \\nonumber\\\\\n% &+\\frac{63 \\lambda^2 S_F^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\mathbf{x}_{t} - \\mathbf{J} \\mathbf{x}_{t}\\right\\|^2 + \\frac{7 \\lambda^2 \\alpha^2}{1-\\lambda^2} \\mathbb{E}\\left\\|\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}(\\mathbf{x}_{t}) \\right\\|^2 + 3 \\lambda^2 N \\alpha^2 L_f^2 L_g^2\n% \\end{align}\n\\begin{align}\n&\\mathbb{E}\\left\\|\\mathbf{v}_{t+1} - \\mathbf{J} \\mathbf{v}_{t+1}\\right\\|^2\n\\leq  \\frac{3 + \\lambda^2}{4} \\mathbb{E}\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2 + \\frac{21 \\lambda^2 N S_F^2 \\eta^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_{t}\\right\\|^2  \\nonumber\\\\\n&+\\frac{63 \\lambda^2 S_F^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\mathbf{x}_{t} - \\mathbf{J} \\mathbf{x}_{t}\\right\\|^2 + \\frac{7 \\lambda^2 \\alpha^2}{1-\\lambda^2} \\mathbb{E}\\left\\|\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t} \\right\\|^2 + 3 \\lambda^2 N \\alpha^2 L_f^2 L_g^2 \\nonumber\n\\end{align}\n\\begin{align}\n\\mathbb{E}\\left\\|\\mathbf{v}_0 - \\mathbf{J v}_0\\right\\|^2\n\\leq \\lambda^2 \\mathbb{E} \\left\\|\\mathbf{u}_0 - \\nabla \\hat{\\mathbf{F}}_0 \\right\\|^2 + \\lambda^2 \\mathbb{E} \\left\\| \\nabla \\hat{\\mathbf{F}}_0 \\right\\|^2 \\nonumber\n\\end{align}\n\\end{lemma}\n\n\\begin{proof}\nSimilar to \\eqref{eq:22}, we have\n\\begin{align} \\label{eq:49}\n\\left\\|\\mathbf{v}_{t+1} - \\mathbf{J} \\mathbf{v}_{t+1}\\right\\|^2 \\leq \\lambda^2\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2 + \\lambda^2\\left\\|\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right\\|^2 +  2\\left\\langle (\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_t, (\\mathbf{W} - \\mathbf{J})\\left(\\mathbf{u}_{t + 1} - \\mathbf{u}_{t}\\right)\\right\\rangle\n\\end{align}\nTo bound the above terms, we recall the update of each local stochastic gradient estimator $\\mathbf{u}_{n,t}$ in Algorithm \\ref{alg:2}: $\\forall t \\geq 0$,\n$$\nu_{n, t + 1} = \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) + (1-\\alpha)(u_{n,t} -  \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}; \\xi^i_{n, t + 1}, \\mathcal{B}_{n, t + 1}))\n$$\nFirstly, we consider the second term in \\eqref{eq:49}, We have that $\\forall t \\geq 0$ and $\\forall n \\in [N]$\n\\begin{align} \\label{eq:50}\n\\mathbf{u}_{n, t + 1} - \\mathbf{u}_{n,t} &= \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\alpha u_{n,t} - (1-\\alpha) \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1})) \\nonumber\\\\ \n&= \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\alpha (\\mathbf{u}_{n,t} - \\nabla \\hat{F}_n(\\mathbf{x}_{n,t})) \\nonumber\\\\ \n&+ \\alpha (\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n (\\mathbf{x}_{n,t}))  \n\\end{align}\n\nWe take the expectation to obtain: $\\forall t \\geq 1$ and $\\forall i \\in [1, N]$,\n\\begin{align} \\label{eq:51}\n\\mathbb{E}\\left\\|\\mathbf{u}_{n, t + 1} - \\mathbf{u}_{n,t}\\right\\|^2 \\leq & 3 \\mathbb{E}\\|\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t+1}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) \\|^2 \\nonumber \\\\\n+ 3 \\alpha^2 \\mathbb{E}\\| u_{n,t} &- \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}) \\|^2 + 3 \\alpha^2 \\mathbb{E}\\left\\|\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}; \\xi^i_{n, t + 1},\\mathcal{B}_{n, t + 1}) - \\nabla \\hat{F}_n(\\mathbf{x}_{n,t})\\right\\|^2 \\nonumber\\\\\n\\leq & 3 S_F^2 \\mathbb{E} \\left\\|\\mathbf{x}_{n, t + 1} - \\mathbf{x}_{n,t}\\right\\|^2 + 3 \\alpha^2 \\mathbb{E}\\left\\|u_{n,t} - \\nabla \\hat{F}_n(\\mathbf{x}_{n,t}) \\right\\|^2 + 3 \\alpha^2 L_g^2 L_f^2\n\\end{align}\n\nNext, towards the last term in \\eqref{eq:49}. Then we take expectation on the both sides of \\eqref{eq:50}, we have that $\\forall t \\geq 1$,\n\\begin{align} \\label{eq:52}\n\\mathbb{E}\\left[\\mathbf{u}_{t + 1} - \\mathbf{u}_{t} \\mid \\mathcal{F}_{t + 1}\\right] = \\nabla \\hat{\\mathbf{F}}_{t + 1} - \\nabla \\hat{\\mathbf{F}}_t - \\alpha (\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t})\n\\end{align}\nThen we discuss the bound of the third term with $\\mathcal{F}_{t + 1}$-measurability of $\\mathbf{v}_{t + 1}$, we have: $\\forall t \\geq 1$,\n\\begin{align} \\label{eq:53}\n& 2\\left\\langle (\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_t, (\\mathbf{W}-\\mathbf{J}) \\mathbb{E}\\left[\\mathbf{u}_{t + 1} - \\mathbf{u}_{t} \\mid \\mathcal{F}_{t + 1}\\right]\\right\\rangle \\nonumber\\\\\n\\stackrel{(a)}{=}& 2 \\left\\langle (\\mathbf{W} - \\mathbf{J}) \\mathbf{v}_t, (\\mathbf{W}-\\mathbf{J}) \\left(\\nabla \\hat{\\mathbf{F}}_{t + 1} - \\nabla \\hat{\\mathbf{F}}_{t} - \\alpha (\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t})\\right)\\right\\rangle  \\nonumber\\\\\n\\stackrel{(b)}{\\leq}& 2 \\lambda\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\| \\cdot \\lambda\\left\\|\\nabla \\hat{\\mathbf{F}}_{t + 1} - \\nabla \\hat{\\mathbf{F}}_{t} - \\alpha (\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t})\\right\\| \\nonumber\\\\\n\\stackrel{(c)}{\\leq} & \\frac{1 - \\lambda^2}{2}\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2 + \\frac{2 \\lambda^4}{1-\\lambda^2}\\left\\|\\nabla \\hat{\\mathbf{F}}_{t + 1} - \\nabla \\hat{\\mathbf{F}}_{t} - \\alpha (\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t}) \\right\\|^2, \\nonumber\\\\\n\\stackrel{(d)}{\\leq} & \\frac{1 - \\lambda^2}{2}\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2 + \\frac{4 \\lambda^4 S_F^2}{1-\\lambda^2}\\|\\mathbf{x}_{t + 1} - \\mathbf{x}_{t} \\|^2 + \\frac{4 \\lambda^4 \\alpha^2}{1-\\lambda^2}\\| \\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t} \\|^2\n\\end{align}\n\nwhere (a) uses \\eqref{eq:52}, (b) is due to the Cauchy-Schwarz inequality and $\\|\\mathbf{W}-\\mathbf{J}\\|= \\lambda$, (c) uses the elementary inequality that $2 a b \\leq c_0 a^2 + b^2 / c_0$, with $c_0 = \\frac{1-\\lambda^2}{2 \\lambda^2}$ for any $a, b \\in \\mathbb{R}$,  and $(d)$ holds since each $\\hat{F}(x)$ is $L$-smooth. \n%  \n\nPutting \\eqref{eq:51} and \\eqref{eq:53} into to \\eqref{eq:49} obtain: $\\forall t \\geq 1$,\n\\begin{align}\n\\mathbb{E}\\left\\|\\mathbf{v}_{t+1} - \\mathbf{J} \\mathbf{v}_{t+1}\\right\\|^2 \\leq & \\frac{1 + \\lambda^2}{2} \\mathbb{E}\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2 + \\frac{7 \\lambda^2 S_F^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\mathbf{x}_{t + 1} - \\mathbf{x}_{t}\\right\\|^2 + \\frac{7 \\lambda^2 \\alpha^2}{1-\\lambda^2} \\mathbb{E}\\left\\|\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t} \\right\\|^2 + 3 \\lambda^2 N \\alpha^2 L_f^2 L_g^2 \\nonumber\\\\\n\\stackrel{(a)}{\\leq} &\\left(\\frac{1 + \\lambda^2}{2} + \\frac{42 \\lambda^4 S_F^2 \\eta^2}{1 - \\lambda^2}\\right) \\mathbb{E}\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2 + \\frac{21 \\lambda^2 N S_F^2 \\eta^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_{t}\\right\\|^2  \\nonumber\\\\\n&+\\frac{63 \\lambda^2 S_F^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\mathbf{x}_{t} - \\mathbf{J} \\mathbf{x}_{t}\\right\\|^2 + \\frac{7 \\lambda^2 \\alpha^2}{1-\\lambda^2} \\mathbb{E}\\left\\|\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t} \\right\\|^2 + 3 \\lambda^2 N \\alpha^2 L_f^2 L_g^2\\nonumber\\\\\n\\stackrel{(b)}{\\leq} & \\frac{3 + \\lambda^2}{4} \\mathbb{E}\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2 + \\frac{21 \\lambda^2 N S_F^2 \\eta^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_{t}\\right\\|^2  \\nonumber\\\\\n&+\\frac{63 \\lambda^2 S_F^2}{1 - \\lambda^2} \\mathbb{E}\\left\\|\\mathbf{x}_{t} - \\mathbf{J} \\mathbf{x}_{t}\\right\\|^2 + \\frac{7 \\lambda^2 \\alpha^2}{1-\\lambda^2} \\mathbb{E}\\left\\|\\mathbf{u}_{t} - \\nabla \\hat{\\mathbf{F}}_{t} \\right\\|^2 + 3 \\lambda^2 N \\alpha^2 L_f^2 L_g^2\n\\end{align}\nwhere (a) follows the \\eqref{eq:24} and (b) holds due to $\\frac{1 + \\lambda^2}{2} + \\frac{42 \\lambda^4 S_F^2 \\eta^2}{1 - \\lambda^2} \\leq \\frac{3 + \\lambda^2}{4}$ if $0 < \\alpha \\leq \\frac{1 - \\lambda^2}{2 \\sqrt{42} \\lambda^2 S_F}$. \n%%%%%%%%%%%%%%%%%%%%%\nIn addition, \n\\begin{align} \\mathbb{E}\\left\\|\\mathbf{v}_0 - \\mathbf{J v}_0\\right\\|^2\n% &=\\mathbb{E} \\left\\|\\mathbf{W}\\left(\\mathbf{v}_0 + \\mathbf{u}_1 - \\mathbf{u}_{0}\\right) - \\mathbf{J} \\mathbf{W}\\left(\\mathbf{v}_0 + \\mathbf{u}_1 - \\mathbf{u}_{0}\\right)\\right\\|^2  \\nonumber\\\\ \n&=\\mathbb{E} \\left\\|\\mathbf{W}\\left(\\mathbf{u}_{0}\\right) - \\mathbf{J} \\mathbf{W}\\left( \\mathbf{u}_{0} \\right)\\right\\|^2  = \\mathbb{E}\\left\\|(\\mathbf{W}-\\mathbf{J}) \\mathbf{u}_1\\right\\|^2 \\nonumber\\\\ \n& \\leq\\lambda^2 \\mathbb{E} \\left\\|\\mathbf{u}_0 - \\nabla \\hat{\\mathbf{F}}(\\mathbf{x}_0) + \\nabla \\hat{\\mathbf{F}}(\\mathbf{x}_0) \\right\\|^2 \\nonumber\\\\ \n& \\leq \\lambda^2 \\mathbb{E} \\left\\|\\mathbf{u}_0 - \\nabla \\hat{\\mathbf{F}}_0 \\right\\|^2 + \\lambda^2 \\mathbb{E} \\left\\| \\nabla \\hat{\\mathbf{F}}_0 \\right\\|^2 \\nonumber\n\\end{align}\n\\end{proof}\n% %%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 10.2": {
                    "name": "proof of Theorem ",
                    "content": "\nThen we start the proof of Theorem\n\\ref{thm:2}.\n\\begin{proof}\nRecall that \n\\begin{align}\n&\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t + 1} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}(x_{n, t + 1})\\|^2 \\leq (1-\\alpha)^{2}\\mathbb{E}\\|\\bar{\\mathbf{u}}_{t} - \\frac{1}{N} \\sum_{n=1}^N  \\nabla \\hat{F}(\\mathbf{x}_{n, t})\\|^{2} +  \\frac{2(1 -  \\alpha)^2 S_F^2}{N^2b} \\mathbb{E}\\|\\mathbf{x}_{t + 1} - \\mathbf{x}_{t}\\|^2 + \\frac{2 \\alpha^2 L_g^2 L_f^2}{Nb} \n\\end{align}\nWe know that $\\frac{1}{1-(1-\\alpha)^2} \\leq \\frac{1}{\\alpha}$ for $\\alpha \\in (0, 1)$. Based on \\Cref{lem:A3}, we have: $\\forall T \\geq 2$,\n\\begin{align} \\label{eq:61}\n&\\sum_{t=0}^{T-1} \\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_{t} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}(x_{n, 0})\\right\\|^2 \\\\\n\\leq& \\frac{\\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_{0} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}(x_{n, 0})\\right\\|^2}{\\alpha} + \\sum_{t = 0}^{T - 2} \\frac{2 S_F^2}{N^2 \\alpha b} \\mathbb{E}\\|\\mathbf{x}_{t + 1} - \\mathbf{x}_{t}\\|^2 + \\frac{2 \\alpha L_g^2 L_f^2}{Nb} T \\nonumber \\\\\n\\leq& \\frac{\\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_{0} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}(x_{n, 0})\\right\\|^2}{\\alpha} + \\frac{6 S_F^2}{N^2 \\alpha b} \\sum_{t = 0}^{T - 2} [ \\mathbb{E}\\|\\mathbf{x}_{t + 1} - \\mathbf{Jx}_{t + 1}\\|^2 + \\|\\mathbf{Jx}_{t + 1} - \\mathbf{Jx}_{t} \\|^2 + \\| \\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_{t}\\|^2 ] + \\frac{2 \\alpha L_g^2 L_f^2}{Nb} T \\nonumber \\\\\n\\leq& \\frac{\\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_{0} - \\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}(x_{n, 0})\\right\\|^2}{\\alpha} + \\frac{12 S_F^2}{N^2 \\alpha b} \\sum_{t=0}^{T - 1}  \\mathbb{E}\\|\\mathbf{x}_{t} - \\mathbf{Jx}_{t}\\|^2 +  \\frac{6 \\eta^2  S_F^2 }{N \\alpha b} \\sum_{t=0}^{T - 1}  \\| \\bar{\\mathbf{u}}_t \\|^2 + \\frac{2 \\alpha L_g^2 L_f^2}{Nb} T  \\nonumber\\\\\n\\leq& \\frac{L_g^2 L_f^2 }{\\alpha N B} + \\frac{12 S_F^2}{N^2 \\alpha b} \\sum_{t = 0}^{T - 1}  \\mathbb{E}\\|\\mathbf{x}_{t} - \\mathbf{Jx}_{t}\\|^2 +  \\frac{6 \\eta^2  S_F^2 }{N \\alpha b} \\sum_{t = 0}^{T - 1}  \\| \\bar{\\mathbf{u}}_t \\|^2 + \\frac{2 \\alpha L_g^2 L_f^2}{Nb} T \\nonumber\n\\end{align}\nwhere $B$ is the initial batch size. \nRecall that \n\\begin{align}\n\\mathbb{E}\\|u_{n, t + 1} - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t + 1})\\|^2 \\leq (1-\\alpha)^{2}\\mathbb{E}\\|\\mathbf{u}_{n, t} - \\nabla \\hat{F}(\\mathbf{x}_{n, t})\\|^{2} + \\frac{2(1 -  \\alpha)^2 S_F^2}{b} \\mathbb{E}\\|x_{n, t + 1} - \\mathbf{x}_{n, t}\\|^2 + \\frac{2 \\alpha^2 L_g^2 L_f^2}{b}  \n\\end{align}\nSimilarly, we have the following: $\\forall T \\geq 2$,\n\\begin{align} \\label{eq:63}\n&\\sum_{n=1}^{N} \\sum_{t=0}^{T - 1} \\mathbb{E}\\left\\|\\mathbf{u}_{n, t} - \\nabla \\hat{F}_n(\\mathbf{x}_{n, t})\\right\\|^2 \\\\\n\\leq & \\sum_{n=1}^{N}\\frac{\\mathbb{E}\\left\\|\\mathbf{u}_{n, 0} - \\nabla \\hat{F}_n(x_{n, 0 })\\right\\|^2}{\\alpha} + \\frac{2 S_F^2}{\\alpha b} \\sum_{t = 0}^{T - 2} \\mathbb{E}\\|x_{t + 1} - x_{t}\\|^2 + \\frac{2 \\alpha N L_g^2 L_f^2}{b } T \\nonumber\\\\\n\\leq & \\sum_{n=1}^{N} \\frac{\\mathbb{E}\\left\\|\\mathbf{u}_{n, 0} - \\nabla \\hat{F}_n (x_{n, 0})\\right\\|^2}{\\alpha} + \\frac{6 N S_F^2 \\eta^2}{\\alpha} \\sum_{t=1}^{T-1} \\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_t \\right\\|^2 + \\frac{12 S_F^2}{\\alpha b} \\sum_{t=1}^T \\mathbb{E} \\left\\|\\mathbf{x}_t-\\mathbf{J} \\mathbf{x}_t\\right\\|^2  + \\frac{2 \\alpha N L_g^2 L_f^2}{b } T \\nonumber\\\\\n\\leq & \\frac{N L_f^2 L_g^2}{\\alpha B} + \\frac{6 N S_F^2 \\eta^2}{\\alpha} \\sum_{t=0}^{T-2} \\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_t \\right\\|^2 + \\frac{12 S_F^2}{\\alpha b} \\sum_{t=0}^{T- 1} \\mathbb{E} \\left\\|\\mathbf{x}_t-\\mathbf{J} \\mathbf{x}_t\\right\\|^2  + \\frac{2 \\alpha N L_g^2 L_f^2}{b } T  \\nonumber\n\\end{align}\n\nwhere the last inequality follows that \n\\begin{align}\n\\mathbb{E}\\left\\|\\mathbf{u}_{n,0} - \\nabla \\hat{F}(\\mathbf{x}_{n,0})\\right\\|^2 &= \\mathbb{E} \\left\\|\\frac{1}{B} \\sum_{i=1}^{B} \\nabla \\hat{F}_n(\\mathbf{x}_{n, 0}; \\xi^i_{n, 0},\\mathcal{B}_{n, 0})) - \\nabla \\hat{F}_{n}(\\mathbf{x}_{n, 0})\\right\\|^2  \\\\\n& \\stackrel{(a)}{=} \\frac{1}{B^2} \\sum_{i=1}^{B} \\mathbb{E}\\left\\| \\nabla \\hat{F}_n(\\mathbf{x}_{n, 0}; \\xi^i_{n, 0},\\mathcal{B}_{n, 0})) - \\nabla \\hat{F}_{n}(\\mathbf{x}_{n, 0})\\right\\|^2  \\leq \\frac{L_g^2 L_f^2}{B},\n\\end{align}\n\nwhere $(a)$ follows from $\\mathbb{E}[\\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, 0}; \\xi^i_{n, 0},\\mathcal{B}_{n, 0})) - \\nabla \\hat{F}_{n}(\\mathbf{x}_{n, 0})] = 0$. \n\n%%%%%%%%%%%%%%%%%%%%\n\nTo further bound $\\sum_{t=0}^{T - 1}\\left\\|\\mathbf{v}_t - \\mathbf{J y}_t\\right\\|^2$, we obtain: if $0<\\alpha \\leq \\frac{1 - \\lambda^2}{2 \\sqrt{24} \\lambda^2 S_F}$, then $\\forall T \\geq 2$,\n\\begin{align}\n& \\sum_{t=0}^{T - 1} \\mathbb{E}\\left[\\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t\\right\\|^2\\right] \\nonumber\\\\\n\\leq & \\frac{4 \\mathbb{E} \\left\\|\\mathbf{v}_0 - \\mathbf{J} \\mathbf{v}_0 \\right\\|^2 }{1 - \\lambda^2} + \\frac{84 \\lambda^2 N S_F^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^2} \\sum_{t = 0}^{T - 2} \\mathbb{E}\\left\\|\\bar{\\mathbf{u}}_t\\right\\|^2 +\\frac{252 \\lambda^2 S_F^2}{\\left(1 - \\lambda^2\\right)^2} \\sum_{t = 0}^{T - 2} \\mathbb{E} \\left\\|\\mathbf{x}_t-\\mathbf{J} \\mathbf{x}_t\\right\\|^2 + \\frac{28 \\lambda^2 \\alpha^2}{\\left(1 - \\lambda^2\\right)^2} \\sum_{t = 1}^{T - 1} \\mathbb{E} \\left\\|\\mathbf{u}_t - \\nabla \\hat{\\mathbf{F}}\\left(\\mathbf{x}_t\\right)\\right\\|^2 + \\frac{12 \\lambda^2 N \\alpha^2 L_f^2 L_g^2 T}{1 - \\lambda^2} \\nonumber\\\\\n\\leq & \\frac{4 \\lambda^2 \\mathbb{E}\\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{1 - \\lambda^2} + \\frac{4 \\lambda^2 N L_f^2 L_g^2}{\\left(1 - \\lambda^2\\right) B} + \\frac{84 \\lambda^2 N S_F^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^2} \\sum_{t = 0}^{T - 1} \\mathbb{E} \\left\\|\\bar{\\mathbf{u}}_t\\right\\|^2 + \\frac{252 \\lambda^2 S_F^2}{\\left(1 - \\lambda^2\\right)^2} \\sum_{t = 0}^{T - 1} \\mathbb{E}\\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2  + \\frac{12 \\lambda^2 N \\alpha^2 L_f^2 L_g^2 T}{1 - \\lambda^2}  + \\frac{28 \\lambda^2 \\alpha^2}{\\left(1 - \\lambda^2\\right)^2} \\sum_{t = 0}^{T - 1} \\sum_{n=1}^{N} \\mathbb{E} \\left\\|\\mathbf{u}_{n, t} - \\nabla \\hat{\\mathbf{F}}_n\\left(\\mathbf{x}_{n, t}\\right)\\right\\|^2 \\nonumber\n\\end{align}\n\nFurthermore, we use \\eqref{eq:63} and if $0<\\eta \\leq \\frac{1-\\lambda^2}{2 \\sqrt{42} \\lambda^2 L}$ and $\\alpha \\in(0,1)$, then $\\forall T \\geq 2$,\n\\begin{align}\n\\sum_{t=0}^{T-1} \\mathbb{E} \\left\\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{v}_t \\right\\|^2 \\leq & \\frac{252 \\lambda^2 N S_F^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^2} \\sum_{t = 0}^{T - 1} \\mathbb{E} \\left\\|\\bar{\\mathbf{u}}_t\\right\\|^2 + \\frac{588 \\lambda^2 S_F^2}{\\left(1 - \\lambda^2\\right)^2} \\sum_{t = 0}^{T-1} \\mathbb{E}\\left[\\left\\|\\mathbf{x}_t-\\mathbf{J} \\mathbf{x}_t\\right\\|^2\\right] \\nonumber\\\\\n&+ \\frac{28 \\lambda^2 N \\alpha L_g^2 L_f^2}{\\left(1 - \\lambda^2\\right)^2 B} + \\frac{56 \\lambda^2 N \\alpha^3 L_f^2 L_F^2 T}{\\left(1 - \\lambda^2\\right)^2} + \\frac{12 \\lambda^2 N \\alpha^2 L_f^2 L_g^2 T}{1 - \\lambda^2} + \\frac{4 \\lambda^2 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{1 - \\lambda^2} + \\frac{4 \\lambda^2 N L_f^2 L_g^2}{\\left(1 - \\lambda^2\\right) B}\n\\end{align}\n\nFinally, we use \\Cref{lem:A3} in \\eqref{eq:16} to obtain: $\\forall T \\geq 2$,\n\\begin{align}\n\\sum_{t = 0}^{T-1} \\mathbb{E} \\left\\|\\mathbf{x}_t - \\mathbf{J} \\mathbf{x}_t\\right\\|^2 \\leq& \\frac{4 \\lambda^2 \\eta^2}{(1 - \\lambda^2)^2} \\sum_{t = 0}^{T-2} \\|\\mathbf{v}_t - \\mathbf{J} \\mathbf{y}_t\\|\\nonumber\\\\\n\\leq & \\frac{1008 \\lambda^4 N S_F^2 \\eta^4}{\\left(1 - \\lambda^2\\right)^4} \\sum_{t = 0}^{T - 1} \\mathbb{E} \\left\\|\\bar{\\mathbf{u}}_t\\right\\|^2 + \\frac{2352 \\lambda^4 S_F^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^4} \\sum_{t = 0}^{T-1} \\mathbb{E}\\left\\|\\mathbf{x}_t-\\mathbf{J} \\mathbf{x}_t\\right\\|^2 \\nonumber \\\\\n&+ (\\frac{7\\alpha}{1 - \\lambda^2} + 1)\\frac{16 \\lambda^4 N L_g^2 L_f^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^3 B} + (\\frac{14 \\alpha}{1 - \\lambda^2} + 3) \\frac{16 \\lambda^2 N \\alpha^2 L_f^2 L_g^2 T \\eta^2}{(1 - \\lambda^2)^3} + \\frac{16 \\lambda^4 \\eta^2 \\left\\|\\nabla \\hat{\\mathbf{F}}_0 \\right\\|^2}{(1 - \\lambda^2)^3}\n\\end{align}\n\nwhich may be written equivalently as\n\\begin{align} \\label{eq:68}\n\\left(1 - \\frac{2352 \\lambda^4 S_F^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^4}\\right) \\sum_{t = 1}^T \\mathbb{E}\\left[\\left\\|\\mathbf{x}_t-\\mathbf{J} \\mathbf{x}_t\\right\\|^2\\right] \\leq & \\frac{1008 \\lambda^4 N S_F^2 \\eta^4}{\\left(1 - \\lambda^2\\right)^4} \\sum_{t = 0}^{T - 1} \\mathbb{E} \\left\\|\\bar{\\mathbf{u}}_t\\right\\|^2 + (\\frac{7\\alpha}{1 - \\lambda^2} + 1)\\frac{16 \\lambda^4 N L_g^2 L_f^2 \\eta^2}{\\left(1 - \\lambda^2 \\right)^3 B} \\nonumber\\\\\n&+ (\\frac{14 \\alpha}{1 - \\lambda^2} + 3) \\frac{16 \\lambda^2 N \\alpha^2 L_f^2 L_g^2 T \\eta^2}{(1 - \\lambda^2)^3} + \\frac{16 \\lambda^4 \\eta^2 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}\\left(\\mathbf{x}_0\\right)\\right\\|^2}{(1 - \\lambda^2)^3}\n\\end{align}\n\nWe observe in \\eqref{eq:68} that $\\frac{2352 \\lambda^4 S_F^2 \\alpha^2}{\\left(1-\\lambda^2\\right)^4} \\leq \\frac{1}{2}$ if $0 < \\eta \\leq \\frac{\\left(1 - \\lambda^2\\right)^2}{90 \\lambda^2 S_F}$. \nBased on \\Cref{lem:C1}, we have\n%%%%%%%%%%%%%%%%\n\\begin{align}\n\\frac{1}{T}\\sum_{t = 0}^{T - 1} \\mathbb{E} \\| \\nabla \\mathbf{F} (\\bar{\\mathbf{x}}_{t})\\|^2 \n\\leq& \\frac{2(\\mathbf{F} (\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} (\\bar{\\mathbf{x}}_{T}))}{\\eta T} - (1 - \\eta S_F ) \\frac{1}{T} \\sum_{t = 0}^{T - 1}\\|\\bar{\\mathbf{u}}_{t} \\|^2  + \\frac{3 S_F^2}{N T} \\sum_{t = 0}^{T - 1} \\sum_{n=1}^N\\|\\mathbf{x}_{n, t} - \\bar{\\mathbf{x}}_t\\|^2 \\nonumber\\\\\n&+ \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} + 3 \\frac{1}{T} \\sum_{t = 0}^{T - 1}\\|\\frac{1}{N} \\sum_{n=1}^N \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}) - \\bar{\\mathbf{u}}_{t}\\|^2 \\nonumber\\\\\n&\\stackrel{(a)}{\\leq }\\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{\\eta T} - (1 - \\eta S_F) \\frac{1}{T} \\sum_{t = 0}^{T - 1}\\|\\bar{\\mathbf{u}}_{t} \\|^2  + \\frac{3 S_F^2}{NT} \\sum_{t = 0}^{T - 1} \\sum_{n=1}^N \\|\\mathbf{x}_{n, t} - \\bar{\\mathbf{x}}_t \\|^2 \\nonumber\\\\\n&+ \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} + 3 \\frac{L_g^2 L_f^2 }{\\alpha N B T} + \\frac{36 S_F^2}{N^2 \\alpha b T} \\sum_{t = 0}^{T - 1} \\mathbb{E}\\|\\mathbf{x}_{t} - \\mathbf{Jx}_{t}\\|^2 + \\frac{18 \\eta^2  S_F^2 }{N \\alpha b T} \\sum_{t = 0}^{T - 1}  \\| \\bar{\\mathbf{u}}_t \\|^2 + \\frac{6 \\alpha L_g^2 L_f^2}{Nb} \\nonumber\\\\\n\\stackrel{(b)}{\\leq}& \\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{\\eta T} - (1 - \\eta S_F - \\frac{18 \\eta^2  S_F^2 }{N \\alpha b} ) \\frac{1}{T}\\sum_{t = 0}^{T - 1}\\|\\bar{\\mathbf{u}}_{t} \\|^2  + \\frac{2}{N \\eta^2 T} \\sum_{t = 0}^{T - 1}  \\mathbb{E}\\|\\mathbf{x}_{t} - \\mathbf{Jx}_{t}\\|^2 \\nonumber\\\\\n&+ \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} + 3 \\frac{L_g^2 L_f^2 }{\\alpha N B T}  + \\frac{6 \\alpha L_g^2 L_f^2}{Nb} \\nonumber\\\\\n\\stackrel{(c)}{\\leq}& \\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{\\eta T} - (1 - \\eta S_F - \\frac{18 \\eta^2  S_F^2 }{N \\alpha b}) \\frac{1}{T} \\sum_{t = 0}^{T - 1}\\|\\bar{\\mathbf{u}}_{t} \\|^2   + \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} + 3 \\frac{L_g^2 L_f^2 }{\\alpha N B T}  + \\frac{6 \\alpha L_g^2 L_f^2}{Nb} \\nonumber\\\\\n&+ \\frac{4032 \\lambda^4 S_F^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^4 T} \\sum_{t = 0}^{T - 1} \\mathbb{E} \\left\\|\\bar{\\mathbf{u}}_t\\right\\|^2 + (\\frac{7\\alpha}{1 - \\lambda^2} + 1)\\frac{64 \\lambda^4 L_g^2 L_f^2}{\\left(1 - \\lambda^2 \\right)^3 B T} + (\\frac{14 \\alpha}{1 - \\lambda^2} + 3) \\frac{64 \\lambda^2 \\alpha^2 L_f^2 L_g^2}{(1 - \\lambda^2)^3} + \\frac{64 \\lambda^4 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{(1 - \\lambda^2)^3 N T} \\nonumber\\\\\n=& \\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{\\eta T} - (1 - \\eta S_F - \\frac{18 \\eta^2  S_F^2 }{N \\alpha b} - \\frac{4032 \\lambda^4 S_F^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^4} ) \\frac{1}{T} \\sum_{t = 0}^{T - 1}\\|\\bar{\\mathbf{u}}_{t} \\|^2   + \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} + 3 \\frac{L_g^2 L_f^2 }{\\alpha N B T}  + \\frac{6 \\alpha L_g^2 L_f^2}{Nb} \\nonumber\\\\\n&+ (\\frac{7\\alpha}{1 - \\lambda^2} + 1)\\frac{64 \\lambda^4 L_g^2 L_f^2}{\\left(1 - \\lambda^2 \\right)^3 B T} + (\\frac{14 \\alpha}{1 - \\lambda^2} + 3) \\frac{64 \\lambda^2 \\alpha^2 L_f^2 L_g^2}{(1 - \\lambda^2)^3} + \\frac{64 \\lambda^4 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{(1 - \\lambda^2)^3 N T} \\nonumber\\\\\n\\stackrel{(d)}{\\leq}&  \\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{\\eta T}   + \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} + 3 \\frac{L_g^2 L_f^2 }{\\alpha N B T}  + \\frac{6 \\alpha L_g^2 L_f^2}{Nb} + \\frac{96 \\lambda^2 L_g^2 L_f^2}{\\left(1 - \\lambda^2 \\right)^3 B T} + \\frac{256 \\lambda^2 \\alpha^2 L_f^2 L_g^2}{(1 - \\lambda^2)^3} + \\frac{64 \\lambda^4 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{(1 - \\lambda^2)^3 N T}  \\nonumber\n\\end{align}\nwhere (a) holds due to \\eqref{eq:61}; (b) uses the $\\alpha = \\frac{72 S^2_F \\eta^2 }{Nb}$ and $S_F \\eta \\leq \\frac{1}{4} \\leq \\frac{1}{2}$; (c) follows the \\eqref{eq:68} and (d) holds due to the fact that $1 - \\eta S_F - \\frac{18 \\eta^2  S_F^2 }{N \\alpha b} - \\frac{4032 \\lambda^4 S_F^2 \\eta^2}{\\left(1 - \\lambda^2\\right)^4} \\geq 0$ if $0 < \\eta \\leq min \\{\\frac{1}{4}, \\frac{\\left(1 - \\lambda^2\\right)^2}{90 \\lambda^2} \\} \\frac{1}{S_F}$, and $\\alpha = \\frac{72 S^2_F \\eta^2 }{Nb} \\leq \\frac{1 - \\lambda^2}{14 \\lambda^2}$ if $\\eta \\leq \\frac{\\sqrt{1 - \\lambda^2}}{12 \\sqrt{7} \\lambda S_F}$\n\\end{proof}\n\nThen, we choose $b = O(1), \\eta = O(\\frac{N^{2/3}}{T^{1/3}}), \\alpha = \\frac{N^{1/3}}{T^{2/3}}, B = \\frac{T^{1/3}}{N^{2/3}} $\n\\begin{align}\n&\\frac{1}{T}\\sum_{t = 0}^{T - 1} \\mathbb{E} \\| \\nabla \\mathbf{F} (\\bar{\\mathbf{x}}_{t})\\|^2 \\leq O(\\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{(N T)^{2/3}} + \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} \\nonumber\\\\\n&+ O(\\frac{3 L_g^2 L_f^2 }{(NT)^{2/3}})  + O(\\frac{6 L_g^2 L_f^2}{(NT)^{2/3}}) + \\frac{352 \\lambda^2 L_f^2 L_g^2}{(1 - \\lambda^2)^3} O(\\frac{N^{2/3}}{T^{4/3}}) + \\frac{64 \\lambda^4 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{(1 - \\lambda^2)^3 N T}  \\nonumber\n\\end{align}\n\n\n\n\n"
                }
            }
        },
        "tables": {
            "tb1": "\\begin{table*}[!t]\n  \\caption{Statistics of benchmark datasets } \\label{tb1}\n  \\setlength{\\tabcolsep}{12pt}\n%   \\label{tab:commands}\n  \\begin{tabular}{ccccc}\n    \\toprule\n    Data Set &Training examples & Testing examples & Feature Size & Proportion of positive data\\\\\n    \\midrule\nw7a & 24692 & 25057 & 300 & 2.99\\%  \\\\\nw8a & 49749 & 14951 & 300 & 2.97 \\% \\\\\nMNIST & 60000 & 10000 & $28\\times 28$ & 16.7\\% \\\\\nFashion MNIST  & 60000 & 10000 & $28\\times 28$ & 16.7 \\% \\\\\nCIFAR-10 & 50000 & 10000 & $3 \\times 32 \\times 32$ & 16.7 \\% \\\\\nTiny-ImageNet & 100000 & 10000 & $3 \\times 64 \\times 64$ & 16.7 \\% \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table*}",
            "tb2": "\\begin{table*}[!t]\n  \\caption{Final averaged AP scores on the testing data} \\label{tb2}\n  \\setlength{\\tabcolsep}{12pt}\n  \\centering\n  \\begin{tabular}{lcccccc}\n    \\hline\n    Method & w7a & w8a & MNIST & Fashion MNIST & CIFAR-10 & Tiny-ImageNet\\\\\n    \\hline\nD-PSGD & 0.6372 & 0.6585 & 0.9592 & 0.9497 & 0.5058 & 0.5906 \\\\\nCODA   & 0.5414 & 0.5786 & 0.9460 & 0.9474 & 0.5668 & 0.5971 \\\\\nSLATE  & 0.7788 & 0.8072 & 0.9911 & 0.9515 & 0.7279 & 0.6032  \\\\\nSLATEM & 0.7778 & 0.8063 & 0.9913 & 0.9515 & 0.7285 & 0.6131 \\\\\n    \\hline\n  \\end{tabular}\n\\end{table*}",
            "tb3": "\\begin{table}[!]\n  \\caption{SLATE test accuracy on CIFAR-10 with margin parameter $s$, and positive batch size b (B=60)} \\label{tb3}\n  % \\setlength{\\tabcolsep}{12pt}\n  \\centering\n  \\begin{tabular}{cccccc}\n    \\hline\n    Margin & 0.1 & 0.3 & 0.5 & 0.7 & 0.9\\\\\n    \\hline\nb = 5 &  0.6270 & 0.5971 & 0.5992 & 0.5928 & 0.5765 \\\\\nb = 10 & 0.6910 & 0.5887 & 0.5956 & 0.5986 & 0.5989 \\\\\nb = 15 & 0.7248 & 0.5895 & 0.5902 & 0.5952 & 0.5979 \\\\\nb = 20 & 0.7279 & 0.6001 & 0.5870 & 0.5929 & 0.5962\\\\\nb = 25 & 0.7216 & 0.6038 & 0.5876 & 0.5933 & 0.5962\\\\\n    \\hline\n  \\end{tabular}\n\\end{table}",
            "tb4": "\\begin{table}[!]\n  \\caption{SLATE-M test accuracy on CIFAR-10 with margin parameter $s$,  positive batch size b (B = 60), and  $\\alpha$} \\label{tb4}\n  % \\setlength{\\tabcolsep}{12pt}\n  \\centering\n  \\begin{tabular}{lcccc}\n    \\hline\n    Margin & 0.1 ($\\alpha$ = 0.1) & 0.1 ($\\alpha$ = 0.9) & 0.3 ($\\alpha$ = 0.1) & 0.3 ($\\alpha$ = 0.9) \\\\\n    \\hline\nb = 15 &  0.7273 & 0.7272 & 0.5853 & 0.5895 \\\\\nb = 20 &  0.7285 & 0.7289 & 0.5986 & 0.6001 \\\\\nb = 25 &  0.7167 & 0.7206 & 0.6024 & 0.6036 \\\\\n    \\hline\n  \\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:1": "\\begin{figure*}[!t]\n     \\centering\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/w7acurve.png}\n         \\caption{w7a dataset}\n         \\label{fig:1_1}\n     \\end{subfigure}\n     \\qquad\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/mnistcurve.png}\n         \\caption{MNIST dataset}\n         \\label{fig:1:3}\n     \\end{subfigure}\n     \\qquad\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/cifar10curve.png}\n         \\caption{CIFAR-10 dataset}\n         \\label{fig:1_2}\n     \\end{subfigure}\n     \n        \\caption{Precision-Recall curves of the models on the testing set}\n        \\label{fig:1}\n\\end{figure*}",
            "fig:2": "\\begin{figure*}[!t]\n     \\centering\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/w7a.png}\n         \\caption{w7a  dataset}\n         \\label{phishing_1}\n     \\end{subfigure}\n     \\qquad\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/w8a.png}\n         \\caption{w8a dataset}\n         \\label{a6a_1}\n     \\end{subfigure}\n     \\qquad\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/mnist.png}\n         \\caption{MNIST dataset}\n         \\label{a7a_1}\n     \\end{subfigure}\n     \n    \\medskip\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/fmnist.png}\n         \\caption{Fashion MNIST dataset}\n         \\label{w7a_1}\n     \\end{subfigure}\n     \\qquad\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/cifar10.png}\n         \\caption{CIFAR-10 dataset}\n         \\label{w8a_1}\n     \\end{subfigure}\n     \\qquad\n     \\begin{subfigure}[b]{0.26\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{images/tiny.png}\n         \\caption{Tiny-ImageNet dataset}\n         \\label{covtype_1}\n     \\end{subfigure}\n        \\caption{AP vs Iterations on the test set } \\label{fig:2}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\\small\n\\underline{\\mathbf{W}} =\\left(\\begin{array}{cccccc}\n1 / 3 & 1 / 3 &  & &  & 1 / 3 \\\\\n1 / 3 & 1 / 3 & 1 / 3 & & & \\\\\n& 1 / 3 & 1 / 3 & \\ddots & & \\\\\n& & \\ddots & \\ddots & 1 / 3 & \\\\\n& & & 1 / 3 & 1 / 3 & 1 / 3 \\\\\n1 / 3 & & & & 1 / 3 & 1 / 3\n\\end{array}\\right) \\in \\mathbb{R}^{N \\times N} \\nonumber\n\\end{align}",
            "eq:2": "\\begin{align} \n\\mathrm{AUPRC} =\\int_{-\\infty}^{\\infty} \\operatorname{Pr}(y = 1 \\mid h(\\mathbf{x}; \\mathbf{z}) \\geq c) d \\operatorname{Pr}(h(\\mathbf{x}; \\mathbf{z}) \\leq c \\mid y=1) \\nonumber\n\\end{align}",
            "eq:3": "\\begin{equation}  \\label{eq:1}\n\\mathrm{AP} = \\mathbb{E}_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} Precision \\left(h\\left(\\mathbf{x}; \\mathbf{z} \\right)\\right) \n= \\mathbb{E}_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} \\frac{\\mathrm{r}^{+} \\left(\\mathbf{x}; \\mathbf{z}\\right)}{\\mathrm{r}\\left(\\mathbf{x}; \\mathbf{z}\\right)} \n\\end{equation}",
            "eq:4": "\\begin{align} \n\\mathrm{AP} = \\mathbb{E}_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} \\frac{\\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim  \\mathcal{D}} \\mathbf{I} (h(\\mathbf{x}; \\mathbf{z}^{\\prime}) \\geq h(\\mathbf{x}; \\mathbf{z})) \\cdot \\mathbf{I}\\left(y^{\\prime} = 1\\right)}{\\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim \\mathcal{D}} \\mathbf{I} (h(\\mathbf{x}; \\mathbf{z}^{\\prime}) \\geq h(\\mathbf{x}; \\mathbf{z}))} \\nonumber\n\\end{align}",
            "eq:5": "\\begin{align} \\label{eq:2}\n\\ell\\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime} \\right) = (max\\{s - h(\\mathbf{x}; \\mathbf{z}) + h(\\mathbf{x}; \\mathbf{z}^{\\prime}), 0\\})^2\n\\end{align}",
            "eq:6": "\\begin{align} \nAP = \\mathbb{E}_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} \\frac{\\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim  \\mathcal{D}} \\mathbf{I}\\left(y^{\\prime}=1\\right)  \\ell \\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}{\\mathbb{E}_{\\mathbf{\\xi}^{\\prime} \\sim \\mathcal{D}} \\quad \\ell\\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}  \\nonumber\n\\end{align}",
            "eq:7": "\\begin{align} \nAP = \\frac{1}{| \\mathcal{D}^{+}|} \\sum_{\\mathbf{x}_{i}, y_{i}=1} \\frac{\\mathrm{r}^{+}\\left(\\mathbf{x}_{i}\\right)}{\\mathrm{r}\\left(\\mathbf{x}_{i}\\right)} = \\frac{1}{| \\mathcal{D}^{+}|} \\sum_{\\mathbf{\\xi} \\sim  \\mathcal{D}^{+}} \\frac{ \n \\frac{1}{| \\mathcal{D}|} \\sum_{\\mathbf{\\xi} \\sim  \\mathcal{D}}\n\\mathbf{I}\\left(y^{\\prime}=1\\right)  \\ell \\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}{ \\frac{1}{| \\mathcal{D}|} \\sum_{\\mathbf{\\xi} \\sim  \\mathcal{D}} \\quad \\ell\\left(\\mathbf{x}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}  \\nonumber\n\\end{align}",
            "eq:8": "\\begin{align} \ng\\left(\\mathbf{x}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right)=\n\\begin{bmatrix}\ng^1\\left(\\mathbf{x}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\\\\ng^2\\left(\\mathbf{x}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right)\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\ell\\left(\\mathbf{x} ; \\mathbf{z}, \\mathbf{z}^{\\prime} \\right) \\mathbf{I}\\left(y^{\\prime}=1\\right) \\\\\n\\ell\\left(\\mathbf{x} ; \\mathbf{z}, \\mathbf{z}^{\\prime} \\right) \n\\end{bmatrix} \\nonumber\n\\end{align}",
            "eq:9": "\\begin{align} \\label{eq:3}\n\\min_{\\mathbf{x}} F (\\mathbf{x}) &=\\mathrm{E}_{\\mathbf{\\xi} \\sim \\mathcal{D}^{+}}\\left[f(g(\\mathbf{x}; \\mathbf{\\xi}) \\right] \\nonumber\\\\\n&=\\mathrm{E}_{\\mathbf{\\xi} \\sim \\mathcal{D}^{+}} \\left[f(\\mathrm{E}_{\\mathbf{\\xi}^{\\prime} \\sim \\mathcal{D}} g(\\mathbf{x}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime}))\\right]\n\\end{align}",
            "eq:10": "\\begin{align} \\label{eq:4}\n\\min_{\\mathbf{x}} F (\\mathbf{x}) &= \\min_{\\mathbf{x}} \\frac{1}{N} \\sum_{n=1}^n F_{n}(\\mathbf{x})\n\\end{align}",
            "eq:11": "\\begin{equation} \\label{eq:8}\n\\begin{aligned}\n\\nabla F_n(\\mathbf{x})= & \\mathbb{E}_{\\xi_n \\sim \\mathcal{D}_n^{+}} \\nabla g_n(\\mathbf{x}; \\xi_n)^{\\top} \\nabla f\\left(g_n(\\mathbf{x}; \\xi_n)\\right) \\\\\n= & \\mathbb{E}_{\\xi_n \\sim \\mathcal{D}_n^{+}} \\nabla g_n(\\mathbf{x}; \\xi_n)^{\\top}\\left(\\frac{-1}{ g^{2}_n(\\mathbf{x}; \\xi_n)}, \\frac{ g^{1}_n(\\mathbf{x}; \\xi_n) }{\\left( g^{2}_n(\\mathbf{x}; \\xi_n)\\right)^{2}}\\right)^{\\top} \\nonumber\n\\end{aligned}\n\\end{equation}",
            "eq:12": "\\begin{align} \n\\nabla  g_n(\\mathbf{x}; \\xi_n) &= \n\\begin{bmatrix}\n\\nabla  g^{1}_n(\\mathbf{x}; \\xi_n) \\\\\n\\nabla  g^{2}_n(\\mathbf{x}; \\xi_n)\n\\end{bmatrix}\n\\nonumber\\\\\n&= \\begin{bmatrix}\n\\mathbb{E}_{\\xi^{\\prime}_n \\sim \\mathcal{D}_n} \\mathbf{I}\\left(y_n^{\\prime}=1\\right) \\nabla \\ell\\left(\\mathbf{x} ; \\mathbf{z}_n, \\mathbf{z}_n^{\\prime}\\right) \\\\\n\\mathbb{E}_{\\xi^{\\prime}_n \\sim \\mathcal{D}_n} \\nabla \\ell\\left(\\mathbf{x}; \\mathbf{z}_n, \\mathbf{z}_n^{\\prime}\\right) \\nonumber\n\\end{bmatrix}\n\\end{align}",
            "eq:13": "\\begin{align} \\label{eq:5}\n&\\nabla \\hat{F}_n\\left(\\mathbf{x}; \\xi_n, \\mathcal{B}_{n}\\right) \\\\\n=&(\\frac{1}{m} \\sum_{\\xi^{\\prime} \\in \\mathcal{B}_{n}} \\nabla g_n(\\mathbf{x}; \\xi_n, \\xi^{\\prime}_n))^{\\top} \\nabla f (\\frac{1}{m} \\sum_{\\xi^{\\prime}_n \\in \\mathcal{B}_{n}} g_n(\\mathbf{x}; \\xi_n, \\xi^{\\prime}_n))   \\nonumber \n\\end{align}",
            "eq:14": "\\begin{align}\n\\hat{F}_n\\left(\\mathbf{x}; \\xi_n, \\mathcal{B}_{n} \\right):=f_n \\left(\\frac{1}{m} \\sum_{\\xi^{\\prime}_n \\in \\mathcal{B}_{n}} g_n(\\mathbf{x}; \\xi_n, \\xi^{\\prime}_n)\\right) \\,.\\nonumber\n\\end{align}",
            "eq:15": "\\begin{align} \\label{eq:6}\n&\\mathbf{u}_{n, t} = \\\\\n& \\sum_{\\mathbf{\\xi} \\in \\mathcal{B}^{+}_{n,t}} \\sum_{\\mathbf{\\xi}^{\\prime} \\in \\mathcal{B}_{n , t}} \\frac{\\left(g^1\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) - g^2\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\mathbf{I}\\left(\\mathbf{y}^{\\prime}=1\\right)\\right) \\nabla \\ell\\left(\\mathbf{x}_{n,t}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}{b m \\left( g^2\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\right)^2} \\nonumber\n\\end{align}",
            "eq:16": "\\begin{align}\n\\mathbf{v}_{n, t} = \\sum_{r=1}^{N} \\underline{w}_{nr} (\\mathbf{v}_{t - 1}^r + \\mathbf{u}_t^r - \\mathbf{u}_{t - 1}^r) \\nonumber\n\\end{align}",
            "eq:17": "\\begin{align}\n\\mathbf{x}_{n, t+1} = \\sum_{r = 1}^{N} \\underline{w}_{nr} (\\mathbf{x}_{r, t} -  \\eta \\mathbf{v}_{r, t}) \\nonumber\n\\end{align}",
            "eq:18": "\\begin{align} \\label{eq:7}\n&\\frac{1}{|\\mathcal{B}^{+}_{n, t} | } \\hat{F}_n(\\mathbf{x}_{n, t}; \\xi^i_{n, t}, \\mathcal{B}_{n, t})  =\\\\\n& \\sum_{\\mathbf{\\xi} \\in \\mathcal{B}^{+}_{n,t}} \\sum_{\\mathbf{\\xi}^{\\prime} \\in \\mathcal{B}_{n , t}} \\frac{\\left(g^1\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) - g^2\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\mathbf{I}\\left(\\mathbf{y}^{\\prime}=1\\right)\\right) \\nabla \\ell\\left(\\mathbf{x}_{n,t}; \\mathbf{z}, \\mathbf{z}^{\\prime}\\right)}{|\\mathcal{B}^{+}_{n, t} | m \\left( g^2\\left(\\mathbf{x}_{n,t}; \\mathbf{\\xi}, \\mathbf{\\xi}^{\\prime} \\right) \\right)^2}  \\nonumber\n\\end{align}",
            "eq:19": "\\begin{align}\n& \\mathbf{u}_{n, t} = \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t}; \\xi^i_{n,t}, \\mathcal{B}_{n, t}) + (1 - \\alpha) (\\mathbf{u}_{n, t - 1} \\nonumber\\\\\n&- \\frac{1}{b} \\sum_{i=1}^{b} \\nabla \\hat{F}_n(\\mathbf{x}_{n, t - 1}; \\xi^i_{n,t}, \\mathcal{B}_{n, t})) \\nonumber\n\\end{align}",
            "eq:20": "\\begin{align}\n\\mathbb{E}\\|\\nabla f_n(x_1) - \\nabla f_n(x_2) \\| \\leq S_{f} \\|x_1 - x_2\\| \\nonumber\\\\\n\\mathbb{E}\\|\\nabla g_n(y_1, \\xi_n) - \\nabla g_n(y_2, \\xi_n) \\| \\leq S_{g} \\|y_1 - y_2\\| \\nonumber\n\\end{align}",
            "eq:21": "\\begin{align}\n\\mathbb{E}\\|\\nabla f_n(x)  \\|^2 \\leq L^2_{f}  \\nonumber\\\\\n\\mathbb{E}\\|\\nabla g_n(y_1, \\xi_n) \\|^2 \\leq S^2_{g} \\nonumber \n\\end{align}",
            "eq:22": "\\begin{align}\n\\mathbb{E}_{\\xi_n \\sim \\mathcal{D}_n^{+}} \\|g_n(\\mathbf{x}; \\mathbf{\\xi}_n, \\mathbf{\\xi}^{\\prime}_n) - \\mathbb{E}_{\\xi_n^{\\prime} \\sim \\mathcal{D}_n} g_n(\\mathbf{x}; \\mathbf{\\xi}_n, \\mathbf{\\xi}^{\\prime}_n) \\|^2  \\leq \\sigma_g^2  \\nonumber\n\\end{align}",
            "eq:23": "\\begin{equation}\n\\mathbf{v}_{t} = \\mathbf{W} (\\mathbf{v}_{t - 1} + \\mathbf{u}_{t} - \\mathbf{u}_{t - 1}), \\;\\;\n\\mathbf{x}_{t+1} = \\mathbf{W} (\\mathbf{x}_{t - 1} - \\eta \\mathbf{v}_{t}) \\nonumber\n\\end{equation}",
            "eq:24": "\\begin{equation}\n\\mathbf{1} \\otimes \\mathbf{\\bar{x}}_t  :=  \\mathbf{J} \\mathbf{x}_t, \\;\n\\mathbf{1} \\otimes  \\bar{\\mathbf{u}}_t := \\mathbf{J} \\mathbf{u}_t, \\;\n\\mathbf{1} \\otimes  \\bar{\\mathbf{v}}_t  := \\mathbf{J} \\mathbf{v}_t\\, \\nonumber\n\\end{equation}",
            "eq:25": "\\begin{align}\ng_n(x; \\mathbf{\\xi}_n) = \\mathbb{E}_{\\mathbf{\\xi}_n^{\\prime} \\sim \\mathcal{D}_n} g_n \\left(x; \\mathbf{\\xi}_n, \\mathbf{\\xi}_n^{\\prime} \\right) \\nonumber\n\\end{align}",
            "eq:26": "\\begin{align}\n\\hat{g}_n(x, \\xi_n) = \\frac{1}{m} \\sum_{\\xi_n^{\\prime} \\in \\mathcal{B}_{n}} g_n(x, \\xi_n; \\xi_n^{\\prime}) \\nonumber\n\\end{align}",
            "eq:27": "\\begin{align}\n\\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n}) = f(\\hat{g}_n(x, \\xi_n)) \\nonumber\n\\end{align}",
            "eq:28": "\\begin{align}\n&\\nabla F_n (x) = \\nabla \\mathbb{E}_{\\xi_n} f(g_n(x, \\xi_n))  = \\mathbb{E}_{\\xi_n}\\left[\\nabla\\left(f(g_n(x, \\xi_n))\\right)\\right] \n= \\mathbb{E}_{\\xi_n}\\left[\\nabla f(g_n(x, \\xi_n)) \\cdot \\nabla g_n(x, \\xi_n)\\right] \\nonumber\n\\end{align}",
            "eq:29": "\\begin{align}\n\\hat{F}_n(x) = \\mathbb{E}\\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n}) = \\mathbb{E} \\left[f(\\hat{g}_n(x, \\xi_n))\\right] \\nonumber\n\\end{align}",
            "eq:30": "\\begin{align}\n\\nabla \\hat{F}_n(x) = \\mathbb{E}\\nabla\\hat{F}_n(x; \\xi_n, \\mathcal{B}_{n})\n\\end{align}",
            "eq:31": "\\begin{align} \\bar{\\mathbf{x}}_t = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_{n, t}, \\quad \\bar{\\mathbf{v}}_t = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{v}_{n,t},  \\quad \\bar{\\mathbf{u}}_t = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{u}_{n,t} \\quad F(\\bar{\\mathbf{x}}) = \\frac{1}{N} \\sum_{n = 1}^{N} F_n (\\bar{\\mathbf{x}}), \\nabla \\hat{F}(\\bar{\\mathbf{x}}) = \\frac{1}{N} \\sum_{n = 1}^{N} \\nabla \\hat{F}_n(\\bar{\\mathbf{x}}) \n\\end{align}",
            "eq:32": "\\begin{align}\n\\frac{1}{T} \\sum_{t=0}^{T - 1} \\mathbb{E} \\|\\nabla F(\\bar{\\mathbf{x}}_{t})\\|^2 \\leq O(\\frac{\\mathbb{E} [F(\\bar{\\mathbf{x}}_{0}) - F(\\bar{\\mathbf{x}}_{T})] }{(N T)^{1/2}})\n+  \\left(\\frac{1}{ \\lambda^2} + 5 N\\right) \\frac{32 \\lambda^2 L_f^2 L_g^2 S_F^2}{(1 - \\lambda^2)^2} O(\\frac{1}{T})  + \\frac{2 L_g^2 S_f^2 \\sigma_g^2}{m} + O(\\frac{2 S_F L_f^2 L_g^2}{(NT)^{1/2}})\n\\end{align}",
            "eq:33": "\\begin{align}\n&\\frac{1}{T}\\sum_{t = 0}^{T - 1} \\mathbb{E} \\| \\nabla \\mathbf{F} (\\bar{\\mathbf{x}}_{t})\\|^2 \\leq O(\\frac{2(\\mathbf{F}(\\bar{\\mathbf{x}}_{0}) - \\mathbf{F} ( \\bar{\\mathbf{x}}_{T}))}{(N T)^{2/3}} + \\frac{3 L_g^2 S_f^2 \\sigma_g^2}{m} \\nonumber\\\\\n&+ O(\\frac{3 L_g^2 L_f^2 }{(NT)^{2/3}})  + O(\\frac{6 L_g^2 L_f^2}{(NT)^{2/3}}) + \\frac{352 \\lambda^2 L_f^2 L_g^2}{(1 - \\lambda^2)^3} O(\\frac{N^{2/3}}{T^{4/3}}) + \\frac{64 \\lambda^4 \\mathbb{E} \\left\\|\\nabla \\hat{\\mathbf{F}}_0\\right\\|^2}{(1 - \\lambda^2)^3 N T}  \\nonumber\n\\end{align}"
        },
        "git_link": "https://github.com/xidongwu/D-AUPRC"
    }
}