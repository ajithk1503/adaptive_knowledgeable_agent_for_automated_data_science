{
    "meta_info": {
        "title": "Partial-label Learning with Mixed Closed-set and Open-set  Out-of-candidate Examples",
        "abstract": "Partial-label learning (PLL) relies on a key assumption that the true label\nof each training example must be in the candidate label set. This restrictive\nassumption may be violated in complex real-world scenarios, and thus the true\nlabel of some collected examples could be unexpectedly outside the assigned\ncandidate label set. In this paper, we term the examples whose true label is\noutside the candidate label set OOC (out-of-candidate) examples, and pioneer a\nnew PLL study to learn with OOC examples. We consider two types of OOC examples\nin reality, i.e., the closed-set/open-set OOC examples whose true label is\ninside/outside the known label space. To solve this new PLL problem, we first\ncalculate the wooden cross-entropy loss from candidate and non-candidate labels\nrespectively, and dynamically differentiate the two types of OOC examples based\non specially designed criteria. Then, for closed-set OOC examples, we conduct\nreversed label disambiguation in the non-candidate label set; for open-set OOC\nexamples, we leverage them for training by utilizing an effective\nregularization strategy that dynamically assigns random candidate labels from\nthe candidate label set. In this way, the two types of OOC examples can be\ndifferentiated and further leveraged for model training. Extensive experiments\ndemonstrate that our proposed method outperforms state-of-the-art PLL methods.",
        "author": "Shuo He, Lei Feng, Guowu Yang",
        "link": "http://arxiv.org/abs/2307.00553v2",
        "category": [
            "cs.LG",
            "cs.CV"
        ],
        "additionl_info": "Revised"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Response to Reviewer 1",
                "content": "\nThank you so much for your insightful comments!\n%We have listed each of your questions and will answer them in turn. \n\n\\noindent \\textbf{Q1: Some notations are not clear. For example, in Eq, (1), what is Dâ€™?}\n\n\\noindent \\textbf{A:}\nWe are sorry for the confusion in Eq. (1). To provide a clearer expression, we rewrite the formulation as $\\widetilde{\\mathcal{D}}_{N}=\\mathrm{argmax}_{\\mathcal{D}^\\prime:|\\mathcal{D}^\\prime|\\geq(1-\\gamma_{1})|\\mathcal{D}|}(\\overline{\\mathcal{L}}(\\mathcal{D}^\\prime)-\\mathcal{L}(\\mathcal{D}^\\prime))$, where $D^\\prime$ is an argument symbol of the operator \"argmax\". \n\n\\noindent \\textbf{Q2: Selecting the OOC samples is very important for the proposed model, please identify the accuracy of the selected OOC samples by the proposed method.}\n\n\\noindent \\textbf{A:}\nWe agree with the importance of OOC selection. In the experiment, we reported the precision curves of selecting OOC samples at each iteration during the training. We refer the reviewer to Figure 5 on Page 7 of the submission. \n\n\n\\noindent \\textbf{Q3: What if we directly remove these OOC examples, and train the model without those samples?}\n\n\\noindent \\textbf{A:}\nThis is a good point, thank you! In fact, the experiment in hyper-parameter analysis, against $\\alpha$ and $\\beta$ in Figure 6 on page 9, had achieved this consideration indirectly. Specifically, when the parameter $\\alpha$ ($\\beta$) equals zero, the corresponding loss $\\mathcal{L}_C$ ($\\mathcal{L}_N$) is disabled to model updating, which is equivalent to training without selected OOC examples. To show this clearly, we further conduct the experiment on CIFAR-10 and SVHN datasets to show the result of removing selected either closed-set or open-set (or both) OOC examples. The comparison result is shown in the following table:\n\n\nIn the Table, the term \"w/o C (O, CO)\" means that we train the model without selected closed-set (open-set, both) OOC examples. From the table, we can see that the overall performance drops without selected OOC examples. In particular, removing closed-set OOC examples have a greater performance degradation than removing open-set OOC ones. This is reasonable since closed-set OOC examples contain useful information for generalization. Therefore, directly removing these selected OOC examples is sub-optimal, and our proposed reversed label disambiguation and random candidate generation can leverage them for model training in a more efficient way.        \n\n\\noindent \\textbf{Q4: In Eq. (6), the reversed label disambiguation is adopted. Can we directly find the correct label of the OOC samples by the OOC identification method?}\n\n\\noindent \\textbf{A:}\nThis may be difficult to directly find the true label of the OOC example in the OOC selection. This is because the selection criterion only implies the location of the true label is inside either the candidate or non-candidate label set. Based on this, we can use reversed label disambiguation to search for the true label only in the non-candidate label set. Otherwise, \n\n\n\\noindent \\textbf{Q5: Random candidate generation is proposed to solve the open-set OOC problem. Can other open-set related techniques be adopted here?}\n\n\\noindent \\textbf{A:}\nThe answer may be positive! Motivated by the literature \\cite{cao2022open}, we can form unseen novel classes of selected open-set examples and meanwhile learn from seen classes in the domain and discovered novel classes. In this case, the classifier is expected to predict seen and novel classes simultaneously. This idea might be a new task in our future work.   \n\n"
            },
            "section 2": {
                "name": "Response to Reviewer 2",
                "content": "\n\nThank you for your insightful and helpful comments. We summarize your concerns and respond in turn.\n\n\\noindent \\textbf{Q1: The confusion in the second line of Eq.(2)}\n\n\\noindent \\textbf{A:}\nWe are sorry for the confusion. We will clarify the second line of Eq.(2), i.e., the non-candidate loss $\\overline{l}_{c}$ with a non-candidate label set $\\overline{Y}$, based on the defined non-candidate label vector $\\overline{\\bm{y}}$, where $\\overline{y}^{j}=1 (0)$ means the $j-$th label is a non-candidate (candidate) one. The term $l_{ce}^{i}(f(x),\\overline{Y})=-\\overline{y}^{\\overline{Y}^{i}}log(f^{\\overline{Y}^{i}}(x))$, where $\\overline{y}^{m} (m=\\overline{Y}^{i})$ is the $m$-th entry of the non-candidate label vector $\\overline{\\bm{y}}$. We hope this definition is helpful to clear up your confusion. \n\n\\noindent \\textbf{Q2: The confusion in Eq.(1)}\n\n\\noindent \\textbf{A:}\nWe apologize for the confusion about Eq.(1). We will further clarify our motivation to design the selection criterion. Intuitively, a normal example (the true label inside (outside) the candidate (non-candidate) label set) tends to have a small (large) candidate $l$ (non-candidate $\\overline{l}$) loss. The principle is that, with smaller $l$ and larger $\\overline{l}$, the probability of being a normal example is greater. The value $(\\overline{l}\\uparrow-l\\downarrow)$ just corresponds to this relationship. Similarly, a closed-set OOC example (the true label inside (outside) the non-candidate (candidate) label set) tends to have a large (small) candidate (non-candidate) loss. The probability of being a closed-set OOC example is greater, as $l$ increased and $\\overline{l}$ decreased. This interplay can be formulated as the value $(l\\uparrow-\\overline{l}\\downarrow)$. Also, an open-set OOC example (the true label outside both the candidate and non-candidate label sets) tends to have both large candidate and non-candidate losses. With larger $l$ and $\\overline{l}$, the probability of being an open-set OOC example is greater. Analogously, we formulate this relationship as the value $(\\overline{l}\\uparrow+l\\uparrow)$. Therefore, the selection criterion in Eq.(1) is designed to select examples that are most likely three types of examples respectively. We hope this explanation will solve your concerns.   \n\n\\noindent \\textbf{Q3: The effectiveness of Random Candidate Generation.}\n\n\\noindent \\textbf{A:}\nThanks for your suggestion! We will provide more details about the random candidate label generation. Our motivation stems from the literature \\cite{wei2021open}. They proposed to generate one random label for each\nopen-set example over training epochs. The intuitive interpretation in the literature based on the \"insufficient capacity\" hypothesis is that fitting these random open-set noisy examples consumes extra capacity of networks, thereby reducing the memorization of inherent noisy labels. This motivates us to design an additional label disambiguation task for open-set examples to consume the extra capacity of the model. For this purpose, we propose to generate a random candidate label set based on a uniform probability of 0.5 for each selected open-set example and allow to disambiguate them by Eq.(4).  In our consideration, the model tends to make a big effort to disambiguate these open-set examples and thus consuming extra capacity of the model to prevent the memorization of incorrectly identified labels. To validate the effectiveness, in the ablation study (Table 2 on page 8), we perform the experiment without this module, which degrades the performance.  \n\n\\noindent \\textbf{Q4: The design of selection criterion against the closed-set OOC example.}\n\n\\noindent \\textbf{A:}\nIn fact, we have considered the characteristics of the closed-set examples (the distribution of candidate - and non-candidate loss)in OOC selection. Reversed label disambiguation is qualified to identify the true label once. \n\n\n\\noindent \\textbf{Q5: The PDF in Figure 3 about the open-set OOC example.}\n\n\\noindent \\textbf{A:}\nWe want to emphasize that overlapping strips in the third column pdf of Figure 3 do not imply an indistinguishable characteristic of the open-set OOC example. We should compare the loss distribution in pdf horizontally to see the difference between the three types of examples. Based on our selection criterion, the precision of selecting open-set OOC examples maintains a high level, e.g., near 97\\% on CIFAR10-SVHN (q=0.1, $\\tau_1=0.3$ and $\\tau_2=0.6$).        \n\n\n\\noindent \\textbf{Q6: Lacking the theoretical analysis against the wooden loss and the moving-average ensemble output.}\n\n\\noindent \\textbf{A:}\nWe mainly focus on a heuristic and empirically motivated method (selection criterion and the moving-average ensemble output) to select OOC examples, which is analogous to many selection-based seminal works (e.g., Co-teaching \\cite{han2018co} and DivideMix \\cite{li2020dividemix}) that also lacks theoretical analysis for selection criterion.\n\n\n\\noindent \\textbf{Q7: The hyper-parameter analysis against $\\tau_1$ and $\\tau_2$.}\n\n\\noindent \\textbf{A:}\nThis observation is true that the performance drops with the overlarge parameters $\\alpha$ and $\\beta$. We think this is not a weakness of the model but a point with noting where appropriate parameters $\\alpha$ and $\\beta$ are significant for our method to maintain the precision of selection.  \n\n\\noindent \\textbf{Q8: The simple total loss function.}\n\n\\noindent \\textbf{A:}\nThe key to our method is the selection criterion and corresponding treatments for the selected three types of examples. Cross-entropy loss with disambiguated label confidence is qualified to train the model.  \n\n\\noindent \\textbf{Q9: The selection criterion should be more complicated.}\n\n\\noindent \\textbf{A:}\nWe think that the designed simple yet effective selection criterion is not necessary to be complicated.\n\n\n\\noindent \\textbf{Q10: The visualization in Figure 7 for the open-set examples.}\n\n\\noindent \\textbf{A:}\nThe case where the disambiguated label confidence of candidates is close to a uniform value in open-set OOC examples is reasonable. We want to emphasize that our aim against open-set OOC examples is not to identify them but to use them as a regularizer to alleviate the effect of incorrectly identified labels (referred to Q3).      \n\n"
            },
            "section 3": {
                "name": "Response to Reviewer 3",
                "content": "\n\nThanks very much for your insightful reviews. We provide a more detailed explanation and responses as the following.\n\n\\noindent \\textbf{Q1: The clarity of some paragraphs is lacking. For example, on Page 3, the selection criterion of open-set examples requires more explanations.}\n\n\\noindent \\textbf{A:}   \nWe are sorry for the deficient clarity of the selection criterion. We will further clarify our motivation to design the selection criterion as the following. Intuitively, a normal example (the true label inside (outside) the candidate (non-candidate) label set) tends to have a small (large) candidate $l$ (non-candidate $\\overline{l}$) loss. The principle is that, with smaller $l$ and larger $\\overline{l}$, the probability of being a normal example is greater. The value $(\\overline{l}\\uparrow-l\\downarrow)$ just corresponds to this relationship. Similarly, a closed-set OOC example (the true label inside (outside) the non-candidate (candidate) label set) tends to have a large (small) candidate (non-candidate) loss. The probability of being a closed-set OOC example is greater, as $l$ increased and $\\overline{l}$ decreased. This interplay can be formulated as the value $(l\\uparrow-\\overline{l}\\downarrow)$. Also, an open-set OOC example (the true label outside both the candidate and non-candidate label sets) tends to have both large candidate and non-candidate losses. With larger $l$ and $\\overline{l}$, the probability of being an open-set OOC example is greater. Analogously, we formulate this relationship as the value $(\\overline{l}\\uparrow+l\\uparrow)$. Therefore, the selection criterion in Eq.(1) is designed to select examples that are most likely three types of examples respectively. We hope this explanation provides further clarity. \n\n\\noindent \\textbf{Q2: On Page 4, Line 433, the â€˜confidenceâ€™ is confusing. On Page 3, Line 262, the label confidence vector p is mentioned but not defined. Then On Page 5, the definition of p is given while is also given. Hence, what is the meaning of the \"confidence\" on Page 4? It is important to clarify it and the corresponding confidence in Figure 4.}\n\n\\noindent \\textbf{A:}\nTo improve the clarity, we make a formal definition in the section 3 problem setup for label confidence vector $\\bm{p}$ on page 3 line 262. We add the definition of $\\overline{\\bm{p}}$. Indeed, the \"confidence\" in Figure 4 refers to the \"label confidence vector\". We decouple per-sample confidence into two individual parts: candidate confidence $\\bm{p}$ (i.e., ) and non-candidate confidence $\\overline{\\bm{p}}$. This is calculated by Eq.(4) and Eq.(6) in the manuscript. \n\n\\noindent \\textbf{Q3: There may be something wrong in Eq.(1). The sign of \"\\\\sum\" in Eq.(1) on Page 3 is confusing.}\n\n\\noindent \\textbf{A: } \nWe are sorry for the confusion in Eq.(1). To be better clear, we rewrite the formulation, e.g., $\\widetilde{\\mathcal{D}}_{N}=\\mathrm{argmax}_{\\mathcal{D}^{'}:|\\mathcal{D}^{'}|\\geq(1-\\gamma_{1})|\\mathcal{D}|}(\\overline{\\mathcal{L}}(\\mathcal{D}^{'})-\\mathcal{L}(\\mathcal{D}^{'}))$.\n\n\\noindent \\textbf{Q4: In Algorithm 1, â€˜the standard trainingâ€™ is ambiguous on Page 5, Line 530. So is the â€˜ordinary label disambiguationâ€™ on Page 7, Line 747.}\n\n\\noindent \\textbf{A: } \nOur bad! the standard training indeed means the ordinary label disambiguation Eq.(4). We will unify them in the revised submission. \n\n\n\\noindent \\textbf{Q5: On Page 5, Line 481, it might be better to formulate the entropy of confidence. After all, it is utilized later in the ablation study.}\n\n\\noindent \\textbf{A: } \nWe will add the formulation into the revised submission: $\\mathrm{Entropy}(\\bm{p})=-\\sum_{j}p_{j}\\mathrm{log} p_{j} \\quad (j \\in Y)$. \n\n\n\n\\noindent \\textbf{Q6: As for the experiments, some important baselines [1,2,3] are missing.}\n\n\\noindent \\textbf{A: } \nThanks for your suggestion! We conduct additional experiments on these three baselines. We implement them by their open-source codes and use the same base training scheme (learning rate is 0.01, the used model is resnet18). Specially, the original methods CAVL [3] and VALEN [2] do not use data augmentation technologies. For a fair comparison, we use the same data augmentation in our work for them, while for the method [1], we keep its original data augmentation. The result is shown in the Table. \n\n"
            },
            "section 4": {
                "name": "Response to Reviewer 4",
                "content": "\n\nThanks very much for your insightful comments. We provide a more detailed explanation and responses as the following.\n\n\\noindent \\textbf{Q1: How to handle $\\tau_{1}$ and $\\tau_{2}$ in real applications?}\n\n\\noindent \\textbf{A:}\nThis is a good and necessary point for real-world applications, thank you! Indeed, in real-world applications, we have no prior information about the proportion of the three types of examples. To solve this issue, we provide a tuning scheme based on a clean validation set that is available in many tasks [10]. The key idea is to linearly ramp up the parameter with the training epoch: $\\tau = \\frac{i}{e} \\in [0,1]$ where $i$ is the current epoch and $e$ is the defined max epoch to control the increase rate. The procedure is as the following: \n\n(1) At first, we set $\\tau_{1}$ and $\\tau_{2}$ to 0, $\\tau_{3}$ to 0.5 (this value is initialized according to different datasets). We select $\\tau_{3}\\%$ normal examples to train the model based on the selection criterion ($\\widetilde{\\mathcal{D}}_{N}=\\mathrm{argmax}_{\\mathcal{D}^{'}:|\\mathcal{D}^{'}|\\geq\\tau_{3}|\\mathcal{D}|}(\\overline{\\mathcal{L}}(\\mathcal{D}^{'})-\\mathcal{L}(\\mathcal{D}^{'}))$). Then, we ramp up the parameter $\\tau_{3}$ with $\\tau_3 = \\frac{i}{e}$ where $i=25$ and $e=50$ (according to the initial value 0.5). As the parameter increases, we examine the accuracy of the validation set. When the drop in performance on the validation set exceeds a defined value $\\epsilon=2$, it indicates that selected normal examples contain too much noise. At this moment, we stop the increase and obtain an approximate estimate against the unknown true proportion. \n\n(2) After this process, we fix the parameter $\\tau_3$ ($\\tau_2=0$) and tune the parameter $\\tau_1$. The process of (1) is repeated for $\\tau_1=\\frac{i}{e}$ ($i=0$ and $e=50$) based on the selection criterion ($\\widetilde{\\mathcal{D}}_{C}=\\mathrm{argmax}_{\\mathcal{D}^{'}:|\\mathcal{D}^{'}|\\geq\\tau_{1}|\\mathcal{D}|}(\\mathcal{L}(\\mathcal{D}^{'})-\\overline{\\mathcal{L}}(\\mathcal{D}^{'}))$). \n\n(3) Finally, we fix the estimated parameters $\\tau_1$ and $\\tau_3$ and tune the parameter $\\tau_2$. The selection criterion is ($\\widetilde{\\mathcal{D}}_{O}=\\mathrm{argmax}_{\\mathcal{D}^{'}:|\\mathcal{D}^{'}|\\geq\\tau_{2}|\\mathcal{D}|}(\\mathcal{L}(\\mathcal{D}^{'})+\\overline{\\mathcal{L}}(\\mathcal{D}^{'}))$). In this way, our proposed method can be employed in real-world applications. We hope this scheme could solve your concerns.   \n\n\\noindent \\textbf{Q2: Can you explain the convergence analysis of the proposed method?}\n\n\\noindent \\textbf{A:}\nOur proposed method involves iterative labeling confidence updating, and thus we can assess the convergence using the difference in label confidence between two iterations $||\\bm{P}^{(t+1)}-\\bm{P}^{(t)}||_F$ where $||\\cdot||_F$ is the Frobenius norm. \n\n\n\\noindent \\textbf{Q3: The experiments.}\n\n\\noindent \\textbf{A:}\nActually, using artificially generated datasets is commonly accepted by the weakly supervised learning community, because many weakly supervised learning problems were rarely studied before (which means there is currently a lack of specific real-world datasets) but could be potentially involved in future real-world applications. These weakly supervised learning problems include but are not limited to unlabeled-unlabeled learning [1,2], complementary-label learning [3,4,5], similarity-unlabeled classification [6], similarity-confidence classification [7], pairwise-comparison classification [8], and our studied partial-label learning [9,10,11]. For these problems, people normally artificially generate training data based on commonly used datasets, such as MNIST, SVHN, and CIFAR-10. We agree that demonstrating the effectiveness of real-world applications would be better, while we believe that our experiments are quite extensive, compared with most of the previous studies in this area. \n\n[1] On the minimal supervision for training any binary classifier from only unlabeled data. ICLR 2020.\n\n[2] Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach. AISTATS 2020.\n\n[3] Learning from complementary labels. NeurIPS 2017.\n\n[4] Learning with Biased Complementary Labels. ECCV 2018.\n\n[5] Discriminative complementary-label learning with weighted loss. ICML 2021.\n\n[6] Classification from Pairwise Similarity and Unlabeled Data. ICML 2018.\n\n[7] Learning from similarity-confidence data. ICML 2021.\n\n[8] Pointwise binary classification with pairwise confidence comparisons. ICML 2021.\n\n[9] Learning from partial labels. JMLR 2011.\n\n[10] PiCO: Contrastive Label Disambiguation for Partial Label Learning. ICLR 2022.\n\n[11] Revisiting Consistency Regularization for Deep Partial Label Learning. ICML 2022.\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{reference}\n"
            }
        }
    }
}