{
    "meta_info": {
        "title": "RealNet: A Feature Selection Network with Realistic Synthetic Anomaly  for Anomaly Detection",
        "abstract": "Self-supervised feature reconstruction methods have shown promising advances\nin industrial image anomaly detection and localization. Despite this progress,\nthese methods still face challenges in synthesizing realistic and diverse\nanomaly samples, as well as addressing the feature redundancy and pre-training\nbias of pre-trained feature. In this work, we introduce RealNet, a feature\nreconstruction network with realistic synthetic anomaly and adaptive feature\nselection. It is incorporated with three key innovations: First, we propose\nStrength-controllable Diffusion Anomaly Synthesis (SDAS), a diffusion\nprocess-based synthesis strategy capable of generating samples with varying\nanomaly strengths that mimic the distribution of real anomalous samples.\nSecond, we develop Anomaly-aware Features Selection (AFS), a method for\nselecting representative and discriminative pre-trained feature subsets to\nimprove anomaly detection performance while controlling computational costs.\nThird, we introduce Reconstruction Residuals Selection (RRS), a strategy that\nadaptively selects discriminative residuals for comprehensive identification of\nanomalous regions across multiple levels of granularity. We assess RealNet on\nfour benchmark datasets, and our results demonstrate significant improvements\nin both Image AUROC and Pixel AUROC compared to the current state-o-the-art\nmethods. The code, data, and models are available at\nhttps://github.com/cnulab/RealNet.",
        "author": "Ximiao Zhang, Min Xu, Xiuzhuang Zhou",
        "link": "http://arxiv.org/abs/2403.05897v1",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "Accepted to CVPR 2024"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nImage anomaly detection is a critical task in industrial production, with wide-ranging applications in quality control and safety monitoring. While self-supervised methods \\cite{zavrtanik2021draem,li2021cutpaste,schluter2022natural,zhang2023destseg,zhang2023prototypical} have gained attention for training models using synthetic anomalies, they still face challenges in synthesizing realistic and diverse anomaly images, especially generating complex structural anomalies and unseen anomaly categories. Due to the lack of available anomaly images and prior knowledge about anomaly categories, existing methods rely on carefully crafted data augmentation strategies \\cite{li2021cutpaste,schluter2022natural} or external data \\cite{zavrtanik2021draem} for anomaly synthesis, leading to significant distribution discrepancy between synthetic anomalies and real anomalies, thereby limiting the generalization ability of anomaly detection models to real-world applications. To address these issues, we introduce Strength-controllable Diffusion Anomaly Synthesis (SDAS), a novel synthesis strategy that generates diverse samples more closely aligned with natural distributions, and offers flexibility in controlling anomaly strength. SDAS employs DDPM \\cite{ho2020denoising} to model the distribution of normal samples and introduces perturbation terms during the sampling process to generate samples in low probability density regions. These samples simulate various natural anomaly patterns, such as aging, structural changes, abnormal textures, and color changes, as shown in \\cref{fig:fig1}. \n\n\n\nParallel to this, feature reconstruction-based anomaly detection \\cite{yang2020dfr,zavrtanik2022dsr,you2022unified,deng2022anomaly,zhang2023destseg} is another promising research direction, which reconstructs the features of anomalous images as those of normal images and conducts anomaly detection and localization by reconstruction residuals. They have attracted considerable attention due to the simple paradigm. However, due to the high computational demands of feature reconstruction and the lack of effective feature selection strategies, existing methods either employ small-scale pre-trained CNNs \\cite{yang2020dfr,zavrtanik2022dsr,you2022unified} for anomaly detection or handpick layer-specific features from pre-trained network \\cite{deng2022anomaly,zhang2023destseg} for reconstruction. The latest work \\cite{heckler2023exploring} highlights the importance of feature selection, indicating that existing anomaly detection methods \\cite{roth2022towards, yu2021fastflow} are sensitive to feature selection. The optimal pre-trained feature subset for anomaly detection varies across different categories. Therefore, devising a unified feature selection approach has become a pressing need for advancing anomaly detection. In this paper, we propose RealNet, a feature reconstruction framework that incorporates Anomaly-aware Features Selection (AFS) and Reconstruction Residuals Selection (RRS). RealNet fully exploits the discriminative capabilities of large-scale pre-trained CNNs while reducing feature redundancy and pre-training bias, enhancing anomaly detection performance while effectively controlling computational demands. For different categories, RealNet selects different pre-trained feature subsets for anomaly detection, ensuring optimal anomaly detection performance while flexibly controlling the model size. Furthermore, RealNet effectively reduces missed detections by adaptively discarding reconstruction residuals lacking anomalous information, and significantly improves the recall of anomalous regions. In summary, our contributions are fourfold:\n\n\n\\begin{itemize}\n\\item We propose RealNet, a feature reconstruction network that effectively leverages multi-scale pre-trained features for anomaly detection by adaptively selecting pre-trained features and reconstruction residuals. RealNet achieves state-of-the-art performance while addressing the computational cost limitations suffered by previous methods.\n\\item We introduce Strength-controllable Diffusion Anomaly Synthesis (SDAS), a novel anomaly synthesis strategy that generates realistic and diverse anomalous samples closely aligned with natural distributions.\n\\item We evaluate RealNet on four datasets (MVTec-AD \\cite{bergmann2019mvtec}, MPDD \\cite{jezek2021deep}, BTAD \\cite{mishra2021vt}, and VisA \\cite{zou2022spot}), surpassing existing state-of-the-art methods using the same set of network architectures and hyperparameters across datasets. \n\\item We provide the Synthetic Industrial Anomaly Dataset (SIA). SIA is generated by SDAS and consists of a total of 360,000 anomalous images from 36 categories of industrial products. SIA can be conveniently utilized for anomaly synthesis to facilitate self-supervised anomaly detection methods.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Related work",
                "content": "\n\n% Unsupervised anomaly detection and localization approaches can be roughly classified into four main categories: \n\n\nUnsupervised anomaly detection and localization approaches use only normal images for model training, without any anomalous data. These methods can be roughly classified into four main categories: reconstruction-based methods \\cite{akccay2019skip,baur2019deep}, self-supervised learning-based methods \\cite{li2021cutpaste,zavrtanik2021draem}, deep feature embedding-based methods \\cite{defard2021padim,roth2022towards}, and one-class classification-based methods \\cite{yi2020patch,liznerski2021explainable}. In this paper, we focus on the reconstruction-based and self-supervised learning-based methods, which are of particular relevance to our proposed RealNet framework.\n\n\\textbf{Reconstruction-based methods} follow a relatively consistent paradigm, which entails training a reconstruction model on normal images. The inability to effectively reconstruct anomalous regions in input images facilitates anomaly detection and localization through comparison of the original and reconstructed images. In this context, a variety of reconstruction techniques are explored, such as Autoencoder \\cite{baur2019deep,youkachen2019defect}, GAN \\cite{akccay2019skip,schlegl2017unsupervised}, Transformer \\cite{mishra2021vt,pirnay2022inpainting}, and Diffusion model \\cite{wyatt2022anoddpm,Zhang2023ICCV,Lu2023ICCV}. However, managing the reconstruction capability of the network remains challenging. In cases of complex image structures or textures, the network may produce a simplistic copy instead of selective reconstruction. Furthermore, inherent stylistic discrepancies between original and reconstructed images can lead to false positives or undetected anomalies.\n\nRecent studies, as exemplified by \\cite{yang2020dfr,you2022unified,zavrtanik2022dsr,deng2022anomaly}, have been primarily focused on anomaly detection through the reconstruction of pre-trained image features. In contrast to image-level reconstruction, multi-scale features pre-trained on ImageNet \\cite{deng2009imagenet} demonstrate enhanced discriminative abilities to detect anomalies across a wide range of scales and diverse image patterns. However, due to the inherent feature redundancy in high-dimensional features and the pre-training bias introduced by classification tasks, the anomaly detection capability of large-scale pre-trained networks has not been fully utilized. Recent studies \\cite{yang2020dfr,zavrtanik2022dsr,you2022unified} use small-scale pre-trained networks to ensure controllable reconstruction costs, and other works \\cite{zhang2023destseg,tien2023revisiting,roth2022towards} manually select partial layer features from pre-trained networks for anomaly detection. However, the optimal feature subset for anomaly detection varies across different categories \\cite{heckler2023exploring}, thus, these manually selecting methods often prove to be dataset-specific and suboptimal, resulting in a significant performance drop. Different from previous solutions, our RealNet presents a novel combination of efficient feature selection strategies and an optimized reconstruction process, effectively enhancing anomaly detection performance while maintaining computational efficiency.\n\n\n\n\\textbf{Self-supervised learning-based methods} aim to bypass the need for labels of anomalous images by setting a suitable proxy task. Notable works in this domain include CutPaste \\cite{li2021cutpaste}, which generates anomalies by transplanting image patches from one location to another, albeit with suboptimal continuity in the anomalous regions. NSA \\cite{schluter2022natural} uses Poisson image editing \\cite{perez2003poisson} for seamless image pasting to synthesize more natural anomaly regions. DRAEM \\cite{zavrtanik2021draem} leverages the texture dataset DTD \\cite{cimpoi2014describing} to synthesize various texture anomalies and achieve advanced self-supervised anomaly detection performance, however, it falls short when faced with specific structural anomalies, such as partial missing or misplaced elements.\n\nThe performance of self-supervised anomaly detection methods hinges on how closely the proxy task aligns with the real anomaly detection task. Anomaly synthesis, as a fundamental study in anomaly detection, has not yet received widespread exploration. Recent work \\cite{duan2023few} use StyleGAN2 \\cite{karras2020analyzing} for image editing to generate anomalous images. However, the proposed method relies on real anomalous images and cannot generate unseen anomaly types. In contrast, SDAS operates in the probability space, free from constraints imposed by data augmentation rules or existing data, enabling effective control over anomaly strengths and the generation of realistic and diverse anomaly images using only normal images.\n\n\n"
            },
            "section 3": {
                "name": "Method",
                "content": "\n\nIn this section, we will introduce our proposed feature reconstruction framework, RealNet, which consists of three key components: Strength-controllable Diffusion Anomaly Synthesis (SDAS), Anomaly-aware Features Selection (AFS), and Reconstruction Residuals Selection (RRS). The pipeline of RealNet is illustrated in \\cref{fig:fig2}.\n\n",
                "subsection 3.1": {
                    "name": "Strength-controllable Diffusion Anomaly \\\\Synthesis",
                    "content": "\n\nDenoising Diffusion Probabilistic Models (DDPM) \\cite{ho2020denoising} employ a forward diffusion process to incrementally add noise $\\mathcal N(0,\\textbf{I})$ to the original data distribution $q(x_0)$. At time $t$, the conditional probability distribution of the noisy data $x_t$ is $q(x_t|x_{t-1})=\\mathcal N(x_t;\\sqrt{1-\\beta_t}x_{t-1},\\beta_t\\textbf{I})$, where $\\{\\beta_t\\}_{t=1}^T$ is a fixed variance schedule, and $\\{x_t\\}_{t=1}^T$ are the latent variables. The diffusion process is defined as a Markov chain, with joint probability distribution $q(x_{1:T}|x_0)=\\prod_{t=1}^{T}q(x_t|x_{t-1})$. Following the sum rule of Gaussian random variables, the conditional probability distribution of $x_t$ at time $t$ is $q(x_t|x_0)=\\mathcal N(x_t;\\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)\\textbf{I})$, where $\\alpha_t=1-\\beta_t$, and $\\bar{\\alpha}_t=\\prod_{i=1}^{t}\\alpha_i$.\n\nThe reverse process is described as another Markov chain, where the mean and variance of the reverse process are parameterized by $\\theta$, \\ie, $p_\\theta(x_{t-1}|x_t)=\\mathcal N(x_{t-1};\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t))$. There are various ways to model $\\mu_\\theta(x_t,t)$; typically, neural networks $\\epsilon_\\theta(x_t,t)$ are used to model the noise $\\epsilon$ in the diffusion process, resulting in $\\mu_\\theta(x_t,t)=\\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t,t))$. In the training phase, our goal is to minimize the variational upper bound of the negative log-likelihood, which leads to the simplified objective:\n\\begin{equation}\n\\label{eq:equ1}\n\\mathcal{L}_{simple}=\\mathbb{E}_{t,x_0,\\epsilon}[\\|\\epsilon-\\epsilon_\\theta(x_t,t)\\|^2]\n\\end{equation}\n\n\n\nTo generate realistic anomalous images, we first train a diffusion model to learn the distribution of normal images using \\cref{eq:equ1}. In reverse diffusion process characterized by $p_\\theta(x_{t-1}|x_{t})=\\mathcal N(x_{t-1};\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t))$, $x_{t-1}$ is the normal image obtained at time $t-1$. Due to the anomalous images being located in low-density regions near the normal images, we introduce an additional perturbation $s\\Sigma$ to sample anomalous images, yielding $p(x_{t-1}'|x_{t-1})=\\mathcal N(x_{t-1}';x_{t-1},s\\Sigma)$, where $\\Sigma$ is the additional introduced variance, scalar $s$ controls the anomaly strength $(s \\geq 0)$, and $x_{t-1}'$ is the anomalous image obtained at time $t-1$. To simplify the anomalous synthesis process, we set $\\Sigma=\\Sigma_\\theta(x_t,t)$, by which the conditional probability distribution of anomalous images $x_{t-1}'$ can be written as follows:\n\\begin{equation}\n\\label{eq:equ2}\np_\\theta(x_{t-1}'|x_{t})=\\mathcal N(x_{t-1}';\\mu_\\theta(x_t,t),(1+s)\\Sigma_\\theta(x_t,t))\n\\end{equation}\n\n\\begin{algorithm}[b]\n\\caption{Strength-controllable Diffusion Anomaly Synthesis (SDAS)}\n\\begin{algorithmic}\n\\STATE \\textbf{Input}: \\text{diffusion model $(\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t))$}\n\\STATE \\hspace{0.94cm} anomaly strength $s$\n\\STATE $x_T \\sim \\mathcal N(0,\\textbf{I})$\n\\STATE \\textbf{for all} $t$ \\text{from T to 1} \\textbf{do}\n\\STATE \\hspace{0.4cm}$\\mu,\\Sigma \\leftarrow \\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t)$\n\\STATE \\hspace{0.4cm}$x_{t-1} \\sim \\mathcal N(\\mu,(1+s)\\Sigma)$\n\\STATE \\textbf{end for}\n\\STATE \\textbf{return} $x_0$\n\\end{algorithmic}\n\\label{alg:algorithm1}\n\\end{algorithm}\n\nTo ensure that the generated anomalous images are close to the distribution of normal images, we set $s \\rightarrow 0$, resulting in $x_{t-1}' \\approx x_{t-1}$; then we use $x_{t-1}'$ for the next time step of the reverse diffusion process. The final form is $p_\\theta(x_{t-1}'|x_{t}')=\\mathcal N(x_{t-1}';\\mu_\\theta(x_t',t),(1+s)\\Sigma_\\theta(x_t',t))$. We term this process Strength-controllable Diffusion Anomaly Synthesis (SDAS), detailed in \\cref{alg:algorithm1}. Specifically, SDAS will generate normal images if $s$ is set to 0.\n\nTo incorporate these anomalous images during training of anomaly detection model, we follow the approach presented in \\cite{zavrtanik2021draem}, utilizing a Perlin noise generator \\cite{perlin1985image} to capture various anomalous shapes and binarize them into an anomaly mask $M$. We denote the normal image as $I$, the anomalous image generated by SDAS as $P$, and the image with local anomalies synthesized by image blending as $A$:\n\\begin{equation}\n\\label{eq:equ3}\nA=\\overline{M} \\odot I+(1-\\delta)(M \\odot I)+\\delta(M \\odot P)\n\\end{equation}\nwhere $\\overline{M}=1-M$, $\\odot$ denotes the element-wise multiplication operation, and $\\delta$ is the opacity in the image blending. To ensure that the generated anomalous regions are located in the foreground, we use an adaptive threshold-based binarization method for foreground segmentation, similar to methods used in \\cite{yang2023memseg,yao2023explicit,schluter2022natural}. \\cref{fig:fig3}a shows the images generated by SDAS under different anomaly strengths, while \\cref{fig:fig3}b compares the images with local anomaly regions synthesized by different methods. The larger the value of $s$, the greater the distribution difference between the generated image and the normal image, and the more obvious the abnormal region obtained after image blending. When $s$ is very small, imperceptible abnormal regions can be synthesized. Compared with alternative synthesis methods, the anomalies generated by SDAS are more continuous and can have very realistic structural anomalies. \n\n"
                },
                "subsection 3.2": {
                    "name": "Anomaly-aware Features Selection",
                    "content": "\n\nIn this section, we introduce the Anomaly-aware Features Selection (AFS) module within RealNet, a self-supervised method for pre-trained feature selection, reducing feature dimensionality and eliminating pre-training bias, as well as managing reconstruction costs. Firstly, we define a set of $N$ triplets $\\{A_n,I_n,M_n\\}_{n=1}^N$, where $A_n,I_n\\in R^{h\\times w\\times 3}$ represent anomaly images synthesized by SDAS and original normal images, and $M_n\\in R^{h\\times w}$ represents the corresponding anomaly mask. We denote the pre-trained network as $\\phi_k$, and $\\phi_k(A_n)\\in R^{h_k\\times w_k\\times c_k}$ represents the $k$th layer pre-trained feature extracted from $A_n$, where $c_k$ represents the number of channels. For the $i$th feature map, $\\phi_{k,i}(A_n)\\in R^{h_k\\times w_k}$, AFS selects $m_k$ feature maps for reconstruction ($m_k \\leq c_k$). Specifically, the feature maps indexed by $k$ are from ResNet-like architectures, such as ResNet50 \\cite{he2016deep} or WideResNet50 \\cite{zagoruyko2016wide}, where $k \\in \\{1,2,3,4\\}$ represent the last layer outputs of blocks with different spatial resolutions.\n\nFor the $k$th layer pre-trained features, we define the following AFS loss for evaluation of the $i$th feature map:\n\\begin{equation}\n\\label{eq:equ4}\n\\mathcal{L}_{AFS}(\\phi_{k,i})=\\frac{1}{N}\\sum_{n=1}^N\\|F([\\phi_{k,i}(A_n)-\\phi_{k,i}(I_n)]^2)-M_n\\|_2^2\n\\end{equation}\nwhere $F(\\cdot)$ is a function that performs normalization operation and aligns the resolution of $[\\phi_{k,i}(A_n)-\\phi_{k,i}(I_n)]^2$ to $M_n$. Given the feature reconstruction process for anomalous images, we train a reconstruction network to infer $\\phi_{k,i}(I_n)$ based on $\\phi_{k,i}(A_n)$, which enables the detection and localization of anomalies through $[\\phi_{k,i}(A_n)-\\phi_{k,i}(I_n)]^2$. Ideally, $[\\phi_{k,i}(A_n)-\\phi_{k,i}(I_n)]^2$ should closely approximate $M_n$. The $\\mathcal{L}_{AFS}(\\phi_{k,i})$ represents the capability of $\\phi_{k,i}$ in identifying anomalous regions. Due to the unavailability of real anomalous samples, we employ synthetic anomalies for feature selection. For the $k$th layer of pre-trained features, AFS selects $m_k$ feature maps with the smallest $\\mathcal{L}_{AFS}$ for reconstruction. We denote the AFS as $\\varphi_k(\\cdot)$, and $\\varphi_k(A_n) \\in R^{h_k\\times w_k\\times m_k}$, where $m_k \\leq c_k$. We perform AFS on each layer of pre-trained features separately, and finally obtain selected multi-scale features $\\{\\varphi_1(A_n),...,\\varphi_K(A_n)\\}$. In this process, each layer's feature dimension $\\{m_1,...,m_K\\}$ serves as a set of hyperparameters. Specifically, for RealNet, AFS operation is performed only once on the pre-trained features of each layer, and the index of the selected feature maps is cached for subsequent training and inference.\n\nAFS adaptively selects a subset of features from all available layers for anomaly detection, offering the following advantages compared to conventional methods \\cite{zhang2023destseg,tien2023revisiting,roth2022towards} that select all features from partial layers: 1) AFS reduces feature redundancy within layers and mitigates pre-training bias, enhancing both feature representativeness and discriminability to improve anomaly detection performance. 2) AFS broadens the receptive field to enhance multi-scale anomaly detection capabilities. 3) AFS distinguishes the dimensions of pre-trained features from those employed for anomaly detection, ensuring efficient control over computational costs and flexible customization of the model size.\n\nIn RealNet, a set of reconstruction networks $\\{G_1,...,G_K\\}$ are designed to reconstruct the selected synthetic anomalous features $\\{\\varphi_1(A_n),...,\\varphi_K(A_n)\\}$ into the original image features $\\{\\varphi_1(I_n),...,\\varphi_K(I_n)\\}$ at various resolutions. The loss function $\\mathcal{L}_{recon}$ is defined as:\n\n\\begin{equation}\n\\label{eq:equ5}\n\\mathcal{L}_{recon}(A,I)=\\frac{1}{N}\\sum_{n=1}^N\\sum_{k=1}^K\\|G_k(\\varphi_k(A_n))-\\varphi_k(I_n)\\|_2^2\n\\end{equation}\n\nDuring the reconstruction process, we intentionally forgo aligning multi-scale features \\cite{yang2020dfr,you2022unified,roth2022towards} to preserve optimal performance. This choice is motivated by the potential drawbacks associated with aligning low-resolution features through down-sampling, which could compromise the network's detection resolution and increase the risk of misidentifying anomalies. On the other hand, aligning high-resolution features using up-sampling may result in unnecessary feature redundancy, leading to elevated reconstruction costs. A detailed discussion on the reconstruction network architectures can be found in \\cref{sec: supC}.\n\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Reconstruction Residuals Selection",
                    "content": "\n\nIn this section, we present the Reconstruction Residuals Selection (RRS) module. Reconstruction residuals are denoted as $\\{E_1(A_n),...,E_K(A_n)\\}$, where $E_k(A_n)=[\\varphi_k(A_n)-G_k(\\varphi_k(A_n))]^2$. To obtain the global reconstruction residual $E(A_n)\\in R^{h'\\times w'\\times m'}$, we up-sample the low-resolution reconstruction residuals and concatenate them channel-wise, where $m'=\\sum_{k=1}^Km_k$, $h'=max(h_1,...,h_K)$, and $w'=max(w_1,...,w_K)$.\n\nThe reconstruction residuals in $E(A_n)$ is obtained from the pre-trained features of reconstructing corresponding layer, and the features of the same resolution only have good ability to capture anomalies within a certain range. For instance, subtle low-level texture anomalies can be effectively captured exclusively by reconstruction residuals derived from low-level features. Therefore, RRS selects only a subset of reconstruction residuals that contain the most anomalous information for the anomaly score generation, to achieve the highest possible recall of anomalous regions.\n\nFirstly, RRS performs GlobalMaxPooling (GMP) and GlobalAveragePooling (GAP) on $E(A_n)$ to obtain $E_{GMP}(A_n), E_{GAP}(A_n) \\in R^{m'}$ respectively. The $r$ largest elements in $E_{GMP}(A_n)$ and $E_{GAP}(A_n)$ are then used to index the positions of $E(A_n)$ and obtain $E_{max}(A_n,r), E_{avg}(A_n,r) \\in R^{h' \\times w' \\times r}$, which respectively represent the Top\\textit{K} reconstruction residuals with the highest maximum and average values. To avoid missed detections caused by inadequate resolution, reconstruction residuals with insufficient anomalous information are discarded in RRS.\n\nAs GMP and GAP respectively represent local and global properties spatially, $E_{max}$ is more effective in capturing local anomalies in small areas, while $E_{avg}$ focuses on selecting anomalies with large spans. Combining $E_{max}$ and $E_{avg}$ together can enhance the RRS's ability to capture anomalies of various scales. We define the RRS operator as $E_{RRS}(A_n,r) \\in R^{h'\\times w' \\times r}$. $E_{RRS}(A_n,r)$ concatenates $E_{max}(A_n,r/2)$ and $E_{avg}(A_n,r/2)$. Finally, we feed the $E_{RRS}(A_n,r)$ into a discriminator, which maps the reconstruction residual to the image-level resolution, obtaining the final anomaly scores. The maximum value in anomaly scores is used as the image-level anomaly score. We use cross entropy loss $\\mathcal{L}_{seg}(A,M)$ to supervise the training of discriminator. The overall loss of RealNet is:\n\\begin{equation}\n\\label{eq:equ6}\n\\mathcal{L}(A,I,M)=\\mathcal{L}_{recon}(A,I)+\\mathcal{L}_{seg}(A,M)\n\\end{equation}\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Synthetic Industrial Anomaly Dataset",
                    "content": "\n\nTo facilitate the reuse of generated anomaly images by SDAS, we constructed the Synthetic Industrial Anomaly Dataset (SIA). SIA comprises anomaly images for 36 categories from four industrial anomaly detection datasets, including MVTec-AD \\cite{bergmann2019mvtec}, MPDD \\cite{jezek2021deep}, BTAD \\cite{mishra2021vt}, and VisA \\cite{zou2022spot}. We generated 10,000 anomaly images with a resolution of $256 \\times 256$ for each category, with anomaly strength $s$ uniformly sampled between 0.1 and 0.2. SIA can be conveniently used for synthesizing anomaly images through image blending, as described in \\cref{eq:equ3}, and can serve as an effective alternative to the widely used DTD dataset \\cite{cimpoi2014describing}.\n\n"
                }
            },
            "section 4": {
                "name": "Experiment",
                "content": "\n\n",
                "subsection 4.1": {
                    "name": "Experimental setup",
                    "content": "\n\n\\label{setup}\n\n\\textbf{Datasets.} We conduct extensive evaluations on four datasets, including MVTec-AD \\cite{bergmann2019mvtec}, MPDD \\cite{jezek2021deep}, BTAD \\cite{mishra2021vt}, and VisA \\cite{zou2022spot}. MVTec-AD \\cite{bergmann2019mvtec} contains 5,354 images from 15 categories for industrial anomaly detection tasks, including 10 object categories and 5 texture categories. MPDD \\cite{jezek2021deep} contains 1,346 images from 6 types of industrial metal products with varying lighting conditions, non-uniform backgrounds, and multiple products in each image. Furthermore, the placement orientation, shooting distance, and position of the products are also varied. BTAD \\cite{mishra2021vt} contains images of 3 industrial products from the real world. VisA \\cite{zou2022spot} is comprised of 9,621 normal images and 1,200 anomaly images from 12 categories. Certain categories demonstrate intricate structures, as exemplified by PCBs, while others consist of multiple objects that require detection, such as Capsules, thus rendering detection and localization a challenging task.\n\n\\textbf{Metrics.} To evaluate the performance of image-level anomaly detection, we use the Area Under the Receiver Operator Curve (AUROC) metric, as in previous works \\cite{bergmann2019mvtec,jezek2021deep,mishra2021vt,zou2022spot}. For pixel-level anomaly location, we use Pixel AUROC and Per Region Overlap (PRO) \\cite{bergmann2020uninformed}.\n\n\n\n\n\n\n\n\n\\textbf{Implementation details.} We evaluate RealNet on four datasets with consistent network architectures and hyperparameters, without specific tuning for individual categories. We use a WideResNet50 \\cite{zagoruyko2016wide} pre-trained on ImageNet \\cite{deng2009imagenet} as the backbone. In AFS, we set the dimension of pre-trained feature of each layer to $\\{256,512,512,256\\}$ for reconstruction. For RRS, $1/3$ of the reconstruction residuals are reserved to generate the final anomaly scores. For SDAS, we train the diffusion model following \\cite{dhariwal2021diffusion} and use the SIA dataset for anomaly synthesis. Both SDAS and anomaly detection are performed at a resolution of $256\\times256$ without center cropping, with a batch size of 16, and we use 64 batches of synthetic anomaly images for AFS. More details can be found in \\cref{sec: supB}.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Anomaly detection on MVTec-AD",
                    "content": "\n\nWe train RealNet using SIA and alternative anomaly synthetic methods on the MVTec-AD dataset \\cite{bergmann2019mvtec}, to evaluate the model's performance in anomaly detection and localization. These methods include: 1) \\textbf{DTD} \\cite{cimpoi2014describing}: This method utilizes the DTD dataset \\cite{cimpoi2014describing} to blend images with generated anomalous textures, and the data augmentation strategy in \\cite{zavrtanik2021draem} is employed during the blending process. 2) \\textbf{NSA} \\cite{schluter2022natural}: This method employs Poisson image editing \\cite{perez2003poisson} to seamless image editing, following parameter setting in \\cite{schluter2022natural}. 3) \\textbf{CutPaste} \\cite{li2021cutpaste}: This method involves random image cropping and pasting to synthesize anomaly regions. \n\nThe experimental results are shown in \\cref{tab:table1}. SDAS demonstrates flexibility in controlling the anomaly strength and generates synthetic anomalies with multiple anomaly patterns, especially for the object categories, where it achieves the best detection and localization performance. Compared to other methods, SDAS is not constrained by data augmentation rules or external data, enabling the synthesis of more natural and rich functional anomalies, as shown in \\cref{fig:fig3}. RealNet trained using SIA achieves remarkable performance on the MVTec-AD dataset \\cite{bergmann2019mvtec}, with an Image AUROC of 99.65\\%, a Pixel AUROC of 99.03\\%, and a PRO score of 93.07\\%. \\cref{fig:fig4} presents the qualitative anomaly localization results of RealNet on the MVTec-AD dataset \\cite{bergmann2019mvtec}. The method exhibits remarkable pixel-level anomaly localization, proficiently identifying diverse anomaly patterns at various scales. Furthermore, RealNet can achieve a rapid inference speed of 31.93 FPS when using a single Nvidia GeForce RTX 3090, and it can perform inference using only 4GB of GPU memory. A detailed computational efficiency analysis can be found in \\cref{sec: supC}.\n\n\nWe also compare RealNet with several state-of-the-art anomaly detection methods, and the results are shown in \\cref{tab:table2}. Built on the same pre-trained network, RealNet outperforms the state-of-the-art alternatives, including Deep Feature Embedding-Based method (PatchCore \\cite{roth2022towards} and SimpleNet \\cite{liu2023simplenet}) and the NF-Based method (FastFlow \\cite{yu2021fastflow}). When compared to previous reconstruction-based methods, RealNet achieves significant performance improvement.\n\n\n%\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Anomaly detection on MPDD",
                    "content": "\n\nWe evaluate RealNet on MPDD dataset \\cite{jezek2021deep} with SIA, DTD \\cite{cimpoi2014describing}, and CutPaste \\cite{li2021cutpaste}, and the results are shown in \\cref{tab:table3}. Notably, RealNet trained with SIA achieves a significant improvement of 2.88\\% in Image AUROC over DTD \\cite{cimpoi2014describing}. \\cref{tab:table4} shows the Image AUROC and the Pixel AUROC of RealNet and other methods on the MPDD dataset \\cite{jezek2021deep}. RealNet achieves an Image AUROC of 96.3\\%, surpassing the current best performance (CFlow \\cite{gudovskiy2022cflow}) by 10.2\\%, even without any dataset-specific tuning.\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Anomaly detection on other benchmarks",
                    "content": "\n\nTo comprehensively assess the effectiveness of RealNet, we conduct experiments on the BTAD \\cite{mishra2021vt} and VisA \\cite{zou2022spot} datasets. On the VisA dataset \\cite{zou2022spot}, characterized by complex structures and multiple detection objects, RealNet employing SIA yields a significant performance improvement, achieving an Image AUROC of 97.8\\% and a Pixel AUROC of 98.8\\%. In the case of the BTAD dataset \\cite{mishra2021vt}, RealNet with SIA achieves competitive results, securing an Image AUROC of 96.1\\% and a Pixel AUROC of 97.9\\%. Detailed results can be found in \\cref{sec: supC}.\n\n"
                },
                "subsection 4.5": {
                    "name": "Ablation studies",
                    "content": "\n\nTo evaluate the effectiveness of each module of RealNet, we conduct comprehensive ablation studies on MVTec-AD dataset \\cite{bergmann2019mvtec}. First, we evaluate the impact of AFS and RRS on the performance of RealNet.\n\n\\textbf{W/O AFS:} We replace AFS with two alternative dimensionality reduction methods, namely Random Dimensionality Reduction (RDR) \\cite{defard2021padim} and Random Linear Projections Reduction (RLPR) \\cite{roth2022towards,xisoftpatch}. RDR randomly selects some dimensional features from high-dimensional features, while RLPR employs an untrained linear transformation layer for linear projection. We report the results of our RealNet with RDR and RLPR, respectively, as shown in the experiments 1 and 3 in \\cref{tab:table5-a}. \\textbf{W/O RRS:} We feed the global reconstruction residual $E(A_n)$ into the discriminator to generate anomaly scores, and the results are shown in the experiments 1 and 2 in \\cref{tab:table5-a}.\n\nAs indicated by the ablation results in \\cref{tab:table5-a}, RRS contributes significantly to performance improvement. Using all the reconstruction residuals to generate anomaly scores, reconstruction residuals lacking of anomaly information can lead to missed anomaly regions, resulting in a significant decrease in anomaly detection performance. Furthermore, AFS yields better anomaly detection results compared to RDR and RLPR. A straightforward visualization result about AFS is provided in \\cref{sec: supD}.\n\nWe further investigate the impact of anomaly strength $s$ in SDAS, and the results are shown in \\cref{tab:table5-b}. When $s$ equals 0, SDAS generates normal images in high probability density regions. Blending images may introduce false positive anomaly regions, which lowers the reconstruction difficulty and confuses the discriminator, leading to suboptimal performance. Conversely, when $s$ is too large, the synthetic anomaly images deviate from the true distribution of anomalous images, causing RealNet's performance to deteriorate. Our findings indicate that uniformly sampling $s$ within a specific range serves as a robust approach for generating anomalous images. This method enables RealNet to cover a wider range of anomalous patterns, ultimately improving the overall anomaly detection performance.\n\nIn \\cref{fig:fig5}, we report the impact of different RRS modes and retention ratios on the performance of RealNet. Since the setting of $r$ is related to $m_k$, we introduce the retention ratio $P$, defined as: $P=\\dfrac{r}{\\Sigma_{k=1}^Km_k}$. Compared to \\texttt{Max} and \\texttt{Avg} modes, \\texttt{Max}\\&\\texttt{Avg} mode demonstrates superior robustness in detecting anomalies across various scales. At equal retention rates, the \\texttt{Max}\\&\\texttt{Avg} mode discards more reconstruction residuals lacking anomalous information than the \\texttt{Max} and \\texttt{Avg} modes, mitigating performance degradation and further emphasizing the effectiveness of the \\texttt{Max}\\&\\texttt{Avg} mode in enhancing RealNet's anomaly detection capabilities. More ablation experiments and analysis can be found in \\cref{sec: supC}.\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\nIn this work, we introduce RealNet, an innovative self-supervised anomaly detection framework. Our approach integrates three core components: Strength-controllable Diffusion Anomaly Synthesis (SDAS),  Anomaly-aware Features Selection (AFS), and Reconstruction Residuals Selection (RRS). These components synergistically contribute to RealNet, enabling effective exploitation of large-scale pre-trained models in anomaly detection while keeping the computational overhead within a reasonably low and acceptable range. RealNet provides a flexible foundation for future research in anomaly detection utilizing pre-trained feature reconstruction techniques. Through extensive experiments, we illustrate RealNet's proficiency in addressing diverse real-world anomaly detection challenges.\n\n\n\\noindent \\textbf{Acknowledgements}. This work was supported in part by the National Natural Science Foundation of China under Grant 62177034 and Grant 61972046.\n\n{\n    \\small\n    \\bibliographystyle{ieeenat_fullname}\n    \\bibliography{main}\n}\n\n\\clearpage\n\n\\maketitlesupplementary\n\\renewcommand*{\\thefigure}{S\\arabic{figure}}\n\\renewcommand*{\\thetable}{S\\arabic{table}}\n\\renewcommand*{\\theequation}{S\\arabic{equation}}\n\\renewcommand*{\\thealgorithm}{S\\arabic{algorithm}}\n\\renewcommand\\thesection{\\Alph{section}}\n\\setcounter{table}{0}\n\\setcounter{figure}{0}\n\\setcounter{equation}{0}\n\\setcounter{section}{0}\n\\setcounter{algorithm}{0}\n\n\\appendix\n\n\n"
            },
            "section 6": {
                "name": "Overview",
                "content": "\n\\label{sec: supA}\n\nWe organize this supplementary material into the following sections: \\cref{sec: supB} provides additional implementation details for RealNet. \\cref{sec: supC} provides detailed results on the BTAD \\cite{mishra2021vt} and VisA \\cite{zou2022spot} datasets, supplementary ablation study results, an analysis of RealNet's computational efficiency, anomaly detection results in multi-class setting, as well as synthetic anomaly image quality assessment results. \\cref{sec: supD} offers additional visualization results, including qualitative results of RealNet in anomaly localization, images generated by SDAS, and a straightforward visualization result of AFS. \\cref{sec: supE} discusses the limitations of our method.\n\n"
            },
            "section 7": {
                "name": "More details",
                "content": "\n\\label{sec: supB}\nIn SDAS, we use the learnable reverse diffusion variance \\cite{nichol2021improved} as $\\Sigma_\\theta(x_t,t)$, given by:\n\n\\begin{equation}\n\\label{eq:equs1}\n\\Sigma_\\theta(x_t,t)=\\exp(v\\log\\beta_t+(1-v)\\log\\tilde{\\beta}_t)\n\\end{equation}\nHere, $\\beta_t$ represents the variance of the diffusion process, while $\\tilde{\\beta}_t$ represents the variance of the conditional posterior distribution $q(x_{t-1}|x_t,x_0)$, and $\\tilde{\\beta}_t=\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_t$. The vector $v$ is predicted by the model and weighted with $\\beta_t$ and $\\tilde{\\beta}_t$ in the $log$ space. We optimize $\\mu_\\theta(x_t,t)$ and $\\Sigma_\\theta(x_t,t)$ with the loss $\\mathcal{L}_{hybrid}$:\n\\begin{equation}\n\\label{eq:equs2}\n\\mathcal{L}_{hybrid}=\\mathcal{L}_{simple}+\\gamma\\mathcal{L}_{vlb}\n\\end{equation}\nwhere\n\\begin{gather}\n\\begin{align}\n\\mathcal{L}_{vlb}&=\\mathcal{L}_0+\\mathcal{L}_1+...+\\mathcal{L}_{T-1}+\\mathcal{L}_{T}\\notag \\\\\n\\mathcal{L}_0&=-\\log p_{\\theta}(x_0|x_1)\\notag \\\\\n\\mathcal{L}_{t-1}&=D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\\theta}(x_{t-1}|x_t))\\notag \\\\\n\\mathcal{L}_{T}&=D_{KL}(q(x_{T}|x_0)||p(x_{T}))\n\\end{align}\n\\end{gather}\n\nWe set $\\gamma$ to 0.001 in \\cref{eq:equs2}, and stop the gradient of $\\mu_\\theta(x_t,t)$ in $\\mathcal{L}_{vlb}$ during the training phase. To accelerate the convergence of the diffusion model, we initialize it with weights pre-trained on ImageNet \\cite{deng2009imagenet}. We set the reverse diffusion step $T$ of 20, and generating 10,000 images at a resolution of $256 \\times 256$ takes 6 hours using a single NVIDIA GeForce RTX 3090. \n\nThe SDAS with DDIM \\cite{song2020denoising} is described in \\cref{alg:algs1}, which provides three options for applying perturbation variance in the deterministic reverse diffusion process: $\\Sigma=\\beta_t$, $\\Sigma=\\tilde{\\beta}_{t}$, and $\\Sigma=\\Sigma_\\theta(x_t,t)$. Experimental observations show that the anomaly images obtained by IDDPM \\cite{nichol2021improved} are slightly better than those obtained by DDIM \\cite{song2020denoising}, and therefore, we use IDDPM \\cite{nichol2021improved} for SDAS. Some examples can be found in \\cref{sec: supD}.\n\n\n\n\\cref{fig:figs1} presents examples of images generated by SDAS with a broader range of anomaly strengths. As the anomaly strength increases, the generated anomalous images contain more noise, reducing their authenticity. In the experiments, we set the anomaly strength between 0.1 and 0.2, allowing SDAS to encompass a wider range of real-world anomalies.\n\nIn RRS, the global reconstruction residual $E(A_{n})$ originates from distinct reconstruction networks, leading to disparate distributions across its dimensions. We apply a \\texttt{BatchNorm} \\cite{ioffe2015batch} layer (without Affine) to $E(A_{n})$ and then perform reconstruction residuals selection to ensure a consistent distribution across the dimensions of $E(A_{n})$. \n\nThe discriminator is implemented using a basic MLP with upsampling layers to map anomaly scores from feature resolution to image resolution. During the training phase of RealNet, we do not use any data augmentation for the synthesis of anomalous images, and maintain an equal ratio between normal images and synthetic anomalous images. In the process of image blending, we uniformly sample the opacity $\\delta$ from 0.5 to 1.0 in Eq. (\\textcolor{red}{3}). The training of RealNet is performed on a single NVIDIA GeForce RTX 3090, with an approximate average training time of 2 hours. \n\n\\begin{algorithm*}[t]\n\\caption{SDAS with DDIM \\cite{song2020denoising}}\n\\begin{algorithmic}\n\\STATE \\textbf{Input}: \\text{diffusion model $\\epsilon_\\theta(x_t,t)$, perturbation variance $\\Sigma$, anomaly strength $s$}\n\\STATE $x_T \\sim \\mathcal N(0,\\textbf{I})$\n\\STATE \\textbf{for all} $t$ \\text{from T to 1} \\textbf{do}\n\\STATE \\hspace{0.4cm}$x_{t-1} \\sim \\mathcal N(\\sqrt{\\bar{\\alpha}_{t-1}}(\\frac{x_t-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_\\theta(x_t,t) }{\\sqrt{\\bar{\\alpha}_{t}}})+\\sqrt{1-\\bar{\\alpha}_{t-1}}\\epsilon_\\theta(x_t,t),s\\Sigma)$\n\\STATE \\textbf{end for}\n\\STATE \\textbf{return} $x_0$\n\\end{algorithmic}\n\\label{alg:algs1}\n\\end{algorithm*}\n\n\n\n\n\n"
            },
            "section 8": {
                "name": "More results",
                "content": "\n\\label{sec: supC}\n\n",
                "subsection 8.1": {
                    "name": "Experimental results on BTAD",
                    "content": "\n\nWe evaluate the anomaly detection and localization performance of RealNet and alternative methods on the BTAD dataset \\cite{mishra2021vt}, with the results shown in \\cref{tab:tables1}. Although SIA does not show a significant performance improvement compared to DTD \\cite{cimpoi2014describing} due to the absence of complex structural anomalies in the three industrial products of the BTAD dataset \\cite{mishra2021vt}, RealNet demonstrates state-of-the-art performance in anomaly detection and localization when compared to other methods, without any structural or hyperparameter tuning.\n\n"
                },
                "subsection 8.2": {
                    "name": "Experimental results on VisA",
                    "content": "\n\nWe present the performance of RealNet and alternative methods on the VisA dataset under the one-class protocol \\cite{zou2022spot} in \\cref{tab:tables2}. RealNet achieves the best performance in both anomaly detection and localization. Compared to DTD \\cite{cimpoi2014describing}, the RealNet trained using SIA shows an improvement of 1.5\\% in Image AUROC and 0.4\\% in Pixel AUROC.\n\n\n"
                },
                "subsection 8.3": {
                    "name": "Supplementary ablation studies",
                    "content": "\n\nTo further investigate RealNet's anomaly detection performance on the MVTec-AD dataset \\cite{bergmann2019mvtec}, we examine various backbones and reconstruction feature dimension settings. As shown in \\cref{tab:tables3}, when WideResNet50 \\cite{zagoruyko2016wide} is employed as the backbone and the reconstruction feature dimensions $\\{m_1,...,m_K\\}$ are reduced from \\{256, 512, 512, 256\\} to \\{128, 256, 256, 128\\}, there is a slight decrease of 0.16\\% in Image AUROC. Despite this reduction, RealNet maintains its competitive performance compared to other methods. Additionally, the adoption of EfficientNetB4 \\cite{tan2019efficientnet} and ResNet34 \\cite{he2016deep} as backbones also results in competitive performance, demonstrating the effectiveness of RealNet across various settings.\n\n\n\n"
                },
                "subsection 8.4": {
                    "name": "Computational efficiency analysis",
                    "content": "\n\nWe investigate the computational efficiency and detection performance of three different multi-scale feature reconstruction architectures on the MVTec-AD dataset \\cite{bergmann2019mvtec}, as illustrated in  \\cref{fig:figs2}. To provide a comprehensive analysis, \\cref{tab:tables4} presents the inference speed, model size (including backbone), and anomaly detection performance of these architectures. The inference is performed using a single Nvidia GeForce RTX 3090, with all other settings adhering to the specifications detailed in Sec. \\textcolor{red}{4.1}.\n\nWe utilize a consistent reconstruction network based on the U-Net model with skip connections across three distinct architectures. The employed U-Net model initiates with a stack of residual layers and down-sampling layers, gradually decreasing the spatial dimensions while increasing the number of channels. Subsequently, the model utilizes a stack of residual layers and up-sampling layers to inversely reconstruct features. Throughout this process, skip connections are incorporated at equivalent spatial resolutions to ensure a smooth and logical flow.\n\nSpecifically, architecture \\textbf{A} adopts separate reconstruction networks to reconstruct multi-scale features without the need for feature interpolation or alignment. This method ensures outstanding anomaly detection performance while maintaining high computational efficiency. With a resolution of $256 \\times 256$ and reconstruction feature dimensions of \\{256, 512, 512, 256\\}, architecture \\textbf{A} with model size of 2.2 GB achieves a rapid inference speed of 31.93 FPS. And it can perform inference using only 4GB of GPU memory. Concurrently, it attains an Image AUROC of 99.65\\% and a Pixel AUROC of 99.03\\%. By decreasing the reconstruction feature dimensions to \\{128, 256, 256, 128\\}, architecture \\textbf{A} reduces the model size to 0.74 GB and achieves a higher inference speed of 40.42 FPS, while preserving an Image AUROC of 99.49\\% and a Pixel AUROC of 99.07\\%. Furthermore, at a high resolution of $512 \\times 512$, it delivers an inference speed of 13.53 FPS, along with an Image AUROC of 99.40\\% and a Pixel AUROC of 98.71\\%. These inference speeds indicate that architecture \\textbf{A} satisfies the real-time requirements for industrial inspection applications.\n\nRegarding architecture \\textbf{B}, as referenced in \\cite{yang2020dfr,tao2022unsupervised,you2022unified}, it is used to align the multi-scale features of a small pre-trained network. As aligning down-sampled features will reduce the resolution of model detection and cause predictable performance loss, the experiment only discusses up-sampling alignment. Compared to architecture \\textbf{A}, architecture \\textbf{B} reconstructs the interpolated features, significantly reducing computational efficiency and increasing model size. Moreover, due to the limited number of normal images, the overly large reconstruction network in architecture \\textbf{B} is prone to overfitting, resulting in reduced detection performance. Consequently, for large-scale pre-trained networks with high-dimensional features, aligning and reconstructing all features is suboptimal.\n\nMoreover, we observe that utilizing multiple reconstruction networks for feature reconstruction in architecture \\textbf{A} causes minor deviations in localizing small-area anomalies, resulting in a reduced PRO. To address this, we propose architecture \\textbf{C}, which aligns and reconstructs features from two neighboring resolution, thereby reducing the number of reconstruction networks, controlling the model size, and striking a balance between computational efficiency and localization accuracy. At a $256 \\times 256$ resolution, with reconstruction feature dimensions of \\{256, 512, 512, 256\\}, architecture \\textbf{C} has a 3.75 GB model size and achieves an inference speed of 22.39 FPS, while attaining an Image AUROC of 99.62\\%, a Pixel AUROC of 98.90\\%, and a PRO of 94.71\\%.\n\nIn summary, the design of RealNet balances both anomaly detection performance and computational efficiency. The introduction of AFS allows us to flexibly customize models of various sizes to accommodate different usage scenarios. Furthermore, among our three key innovations, both AFS and RRS introduce no additional learnable parameters, ensuring strong interpretability. As for SDAS, it only introduces perturbation during the reverse diffusion process, without requiring any prior knowledge about the distribution of real anomaly images.\n\n\n%\n\n\n"
                },
                "subsection 8.5": {
                    "name": "Anomaly detection in multi-class setting",
                    "content": "\n\nIn the multi-class setting \\cite{you2022unified,zhao2023omnial}, anomaly detection is performed across multiple target classes concurrently, without access to sample class labels during both training and inference phases. Learning the data distributions of multiple classes jointly makes the reconstruction more complex. In such settings, previous reconstruction methods tend to output copies of the input images instead of performing selective reconstruction, which leads to a significant decrease in performance. We evaluate the performance of RealNet in multi-class anomaly detection on the MVTec-AD dataset \\cite{bergmann2019mvtec} and compare it with alternative state-of-the-art methods. We use DTD \\cite{cimpoi2014describing} for anomaly synthesis as class labels are unavailable during training. The remaining settings are consistent with Sec. \\textcolor{red}{4.1}.\n\nThe results are shown in Tab. \\ref{tab:tables5}. When detecting anomalies across 15 categories of the MVTec-AD dataset \\cite{bergmann2019mvtec} concurrently, RealNet achieves an Image AUROC of 97.3\\% and a Pixel AUROC of 98.4\\% using a ResNet50 \\cite{he2016deep} pre-trained on ImageNet \\cite{deng2009imagenet}, surpassing state-of-the-art multi-class anomaly detection methods \\cite{you2022unified,zhao2023omnial}. To ensure that normal regions can be reconstructed correctly, we do not explicitly constrain the generalization ability of the reconstructed network in RealNet. Instead, we implicitly constrain the reconstruction network to ensure that anomalous regions can be correctly detected by discarding a part of the reconstruction residuals.\n\n%\n\n\n%\n\n\n"
                },
                "subsection 8.6": {
                    "name": "Synthetic anomaly image quality assessment",
                    "content": "\n\nIn this section, we evaluate the quality of anomaly images generated by various anomaly synthesis methods on the MVTec-AD dataset \\cite{bergmann2019mvtec}. Specifically, we use the following evaluation metrics: \n\\begin{itemize}\n\\item FID (Fr{\\'e}chet Inception Distance) \\cite{heusel2017gans}: FID measures the distance between the distribution of synthetic anomaly images and real anomaly images, evaluating both the realism and diversity of the synthetic anomaly images. A lower value indicates better performance.\n\\item LPIPS (Learned Perceptual Image Patch Similarity) \\cite{zhang2018unreasonable}: We employ cluster-based LPIPS \\cite{duan2023few} to evaluate the diversity of synthetic anomaly images. Supposing a category contains $N$ real anomaly images, we partition the synthesized anomaly images into $N$ groups by finding the lowest LPIPS, then we compute the mean pairwise LPIPS within each group and compute the average of all groups. A higher cluster LPIPS indicates greater diversity.\n\\end{itemize}\n\nWe employ various anomaly synthesis methods to generate 1,000 anomaly images for evaluation, with each method independently assessed three times. The experimental results are shown in \\cref{tab:tables6}. In comparison to other anomaly synthesis methods, SIA achieves the best FID and LPIPS metrics, highlighting the outstanding performance of SDAS in generating both realistic and diverse anomaly images, and demonstrating the effectiveness of SDAS in improving anomaly detection performance.\n\n\n"
                }
            },
            "section 9": {
                "name": "Visualization",
                "content": "\n\n\\label{sec: supD}\nWe conduct a comprehensive visual analysis of RealNet on the four datasets. \\cref{fig:figs3} shows the qualitative results of RealNet in anomaly localization, showcasing its outstanding performance in pixel-level anomaly localization. Figs. \\ref{fig:figs4} and \\ref{fig:figs5} display the anomaly images and normal images generated by SDAS, respectively. \\cref{fig:figs6} illustrates images synthesized using SIA with localized anomalous regions. \\cref{fig:figs7} provides an intuitive explanation of pre-training bias, indicating that not all feature maps contribute equally to anomaly detection and localization, which validates the efficacy of AFS.\n\n"
            },
            "section 10": {
                "name": "Limitations",
                "content": "\n\\label{sec: supE}\n\nIn some categories with more texture anomalies, such as the texture categories in MVTec-AD dataset \\cite{bergmann2019mvtec}, SIA's performance may slightly underperform when compared to DTD \\cite{cimpoi2014describing}. Given that DTD dataset \\cite{cimpoi2014describing} includes a diverse range of real-world texture images, it effectively simulates common anomaly types in the textural category, such as color, oil, and glue. Nonetheless, SIA excels in the majority of scenarios, outperforming DTD \\cite{cimpoi2014describing} and offering superior capability in synthesizing anomalies in images with intricate structures.\n\nCompared to anomaly synthesis methods based on data augmentation \\cite{li2021cutpaste,schluter2022natural} or external data \\cite{zavrtanik2021draem}, SDAS increases additional offline training time. For instance, we generate 10,000 anomaly images at a resolution of $256 \\times 256$ for each category, and it will take 6 hours using a single NVIDIA GeForce RTX 3090. However, it is pivotal to clarify that RealNet omits SDAS without any additional computational cost during inference and real-world applications. Therefore, we believe that the slight increase in training time to enhance performance is necessary and worthwhile.\n\nIn order to achieve higher computational efficiency, we do not upsample multi-scale features. Instead, we employ multiple reconstruction networks for feature reconstruction, which reduce the resolution of anomaly detection. The lower feature reconstruction resolution may introduce minor deviations in localizing small anomalous areas, leading to a decrease in PRO. However, we found that increasing the resolution of anomaly detection by reducing the number of reconstruction networks can improve PRO. For instance, architecture \\textbf{C} in \\cref{fig:figs2} achieved a higher PRO score of 94.71\\%. Furthermore, increasing the resolution of images can also lead to an improvement in PRO, as detailed in \\cref{tab:tables4}.\n\n\\clearpage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
            }
        },
        "tables": {
            "tab:table1": "\\begin{table*}[t]\n  \\renewcommand\\arraystretch{1.0}\n  \\centering\n  \\caption{Comparison of SIA with alternative anomaly synthesis approaches on the MVTec-AD dataset \\cite{bergmann2019mvtec}, employing Image AUROC (\\%), Pixel AUROC (\\%), and PRO (\\%) as evaluation metrics. }\n    \\resizebox{0.91\\linewidth}{!}{\n    \\tiny\n    \\begin{tabular}{c|c|c|c|c|c}\n    \\bottomrule\n    \\multicolumn{2}{c|}{Category} & SIA   & DTD \\cite{cimpoi2014describing}   & NSA \\cite{schluter2022natural}   & CutPaste \\cite{li2021cutpaste} \\\\\n    \\hline\n    \\multicolumn{1}{c|}{\\multirow{6}{*}{Texture}} & \\multicolumn{1}{c|}{Carpet} & (99.84, 99.19, 96.41) & (\\textbf{100.0}, \\textbf{99.27}, \\textbf{96.96}) & (99.80, 98.60, 88.77) & (99.24, 98.42, 93.85) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Grid} & (\\textbf{100.0}, 99.51, \\textbf{97.28}) & (\\textbf{100.0}, \\textbf{99.57}, 97.14) & (\\textbf{100.0}, 99.32, 91.31) & (\\textbf{100.0}, 99.18, 92.53) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Leather} & (\\textbf{100.0}, 99.76, 96.22) & (\\textbf{100.0}, \\textbf{99.77}, 96.41) & (\\textbf{100.0}, 99.24, \\textbf{96.85}) & (\\textbf{100.0}, 99.41, 92.12) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Tile} & (99.96,\\textbf{99.44}, \\textbf{97.70}) & (\\textbf{100.0}, 99.35, 95.27) & (\\textbf{100.0}, 97.40, 86.45) & (99.86, 97.63, 84.39) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Wood} & (99.21, 98.22, 90.54) & (\\textbf{99.65}, \\textbf{98.28}, \\textbf{91.23}) & (97.63, 93.30, 87.20) & (98.95, 95.29, 81.47) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{\\textbf{AVG}} & (99.80, 99.22, \\textbf{95.63}) & (\\textbf{99.93}, \\textbf{99.25},95.40) & (99.49, 97.57, 90.11) & (99.61, 97.99, 88.87) \\\\\n    \\hline\n    \\multicolumn{1}{c|}{\\multirow{11}{*}{Object}} & \\multicolumn{1}{c|}{Bottle} & (\\textbf{100.0}, 99.30, \\textbf{95.62}) & (\\textbf{100.0}, 99.35, 95.57) & (\\textbf{100.0}, \\textbf{99.37}, 93.49) & (\\textbf{100.0}, 99.14, 91.41) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Cable} & (99.19, \\textbf{98.10}, \\textbf{93.38}) & (98.95, 97.84, 90.36) & (\\textbf{99.33}, 97.62, 93.26) & (96.35, 96.23, 86.05) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Capsule} & (\\textbf{99.56}, \\textbf{99.32}, 84.48) & (99.32, 99.19, 82.28) & (99.04, 99.27, \\textbf{85.77}) & (98.48, 99.10, 79.55) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{hazelnut} & (\\textbf{100.0}, \\textbf{99.68}, 93.14) & (\\textbf{100.0}, 99.46, 93.46) & (\\textbf{100.0}, 99.25, \\textbf{94.41}) & (\\textbf{100.0}, 99.03, 91.51) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Metal Nut} & (99.76, 98.58, 94.39) & (99.90, 98.58, \\textbf{96.49}) & (\\textbf{100.0}, \\textbf{99.11}, 93.27) & (99.90, 98.03, 89.69) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Pill} & (\\textbf{99.13}, \\textbf{99.02}, 91.04) & (98.36, 98.88, 84.44) & (97.19, 98.28, \\textbf{95.15}) & (97.22, 98.96, 86.48) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Screw} & (\\textbf{98.83}, 99.45, 87.90) & (97.72, 99.36, 85.22) & (98.79, \\textbf{99.62}, \\textbf{93.74}) & (92.74, 98.53, 79.63) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Toothbrush} & (99.44, 98.71, \\textbf{91.57}) & (99.44, 98.69, 90.87) & (\\textbf{100.0}, \\textbf{99.18}, 89.20) & (99.17, 98.85, 78.48) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Transistor} & (\\textbf{100.0}, \\textbf{98.00}, \\textbf{92.92}) & (99.71, 97.15, 86.56) & (98.54, 95.67, 79.09) & (99.38, 96.32, 76.52) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{zipper} & (99.82, \\textbf{99.17}, \\textbf{93.43}) & (99.68, 99.02, 88.77) & (\\textbf{99.90}, 98.91, 93.05) & (99.61, 98.03, 92.26) \\\\\n\\cline{2-6}    \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{\\textbf{AVG}} & (\\textbf{99.57}, \\textbf{98.93}, \\textbf{91.79}) & (99.31, 98.75, 89.40) & (99.28, 98.63, 91.04) & (98.29, 98.22, 85.16) \\\\\n    \\hline\n    \\multicolumn{2}{c|}{\\textbf{AVG}} & (\\textbf{99.65}, \\textbf{99.03}, \\textbf{93.07}) & (99.52, 98.92, 91.40) & (99.35, 98.28, 90.73) & (98.73, 98.14, 86.40) \\\\\n    \\toprule\n    \\end{tabular}\n    }\n  \\label{tab:table1}\n\\end{table*}",
            "tab:table2": "\\begin{table*}[t]\n  \\renewcommand\\arraystretch{1.1}\n  \\centering\n  \\caption{Comparison of RealNet with alternative anomaly detection methods on the MVTec-AD dataset \\cite{bergmann2019mvtec}.}\n   \\resizebox{0.91\\linewidth}{!}{\n    \\begin{tabular}{c|ccc|cccccc|c}\n    \\bottomrule\n    \\multicolumn{1}{c|}{Metric} & \\multicolumn{1}{c}{\\textit{PatchCore} \\cite{roth2022towards}} & \\multicolumn{1}{c}{\\textit{SimpleNet} \\cite{liu2023simplenet}} & \\multicolumn{1}{c|}{\\textit{FastFlow} \\cite{yu2021fastflow}} & \\multicolumn{1}{c}{DRAEM+SSPCAB \\cite{ristea2022self}} & DSR \\cite{zavrtanik2022dsr}   & \\multicolumn{1}{c}{UniAD \\cite{you2022unified}} & \\multicolumn{1}{c}{RD++ \\cite{tien2023revisiting}} & \\multicolumn{1}{c}{DeSTSeg \\cite{zhang2023destseg}} & \\multicolumn{1}{c|}{DiffAD \\cite{Zhang2023ICCV}} & \\multicolumn{1}{c}{RealNet} \\\\\n    \\hline\n    Image AUROC & 99.1 &\\textbf{99.6} & 99.3  & 98.9  & \\multicolumn{1}{c}{98.2} & 96.6  & 99.4  & 98.6  & 98.7 & \\textbf{99.6} \\\\\n    Pixel AUROC & 98.1 &98.1 & 98.1  & 97.2  & -     & 96.6  & 98.3  & 97.9  & 98.3 & \\textbf{99.0} \\\\\n    \\toprule\n    \\end{tabular}\n    }\n  \\label{tab:table2}\n\\end{table*}",
            "tab:table4": "\\begin{table*}[h]\n  \\centering\n  \\renewcommand\\arraystretch{1.0}\n    \\caption{Comparison of RealNet with alternative anomaly detection methods on the MPDD dataset \\cite{jezek2021deep}.}\n   \\resizebox{0.9\\linewidth}{!}{\n       \\tiny\n    \\begin{tabular}{c|cccc|cc|c}\n    \\bottomrule\n    \\multicolumn{1}{c|}{Metric} & \\multicolumn{1}{c}{\\textit{PatchCore} \\cite{roth2022towards}} & \\multicolumn{1}{c}{\\textit{CFlow} \\cite{gudovskiy2022cflow}} & \\multicolumn{1}{c}{\\textit{PaDiM} \\cite{defard2021padim}} & \\multicolumn{1}{c|}{\\textit{SPADE} \\cite{cohen2020sub}} & \\multicolumn{1}{c}{DAGAN \\cite{tang2020anomaly}} & \\multicolumn{1}{c|}{Skip-GANomaly \\cite{akccay2019skip}} & \\multicolumn{1}{c}{RealNet} \\\\\n    \\hline\n    Image AUROC & 82.1  & 86.1  & 74.8  & 77.1  & 72.5  & 64.8  & \\textbf{96.3} \\\\\n    Pixel AUROC & 95.7  & 97.7  & 96.7  & 95.9  & 83.3  & 82.2  & \\textbf{98.2} \\\\\n    \\toprule\n    \\end{tabular}\n  }\n  \\label{tab:table4}\n\\end{table*}",
            "tab:table5": "\\begin{table*}[h]\n\t\\centering\n    \\label{tab:table5}\n    \\caption{Ablation studies of RealNet on the MVTec-AD dataset \\cite{bergmann2019mvtec}.}\n    \\begin{subtable}{0.47\\textwidth}\n        \\renewcommand\\arraystretch{0.975}\n        \\caption{The impact of AFS and RRS on RealNet.}\n        \\label{tab:table5-a}\n        \\resizebox{\\linewidth}{!}{\n            \\tiny\n            \\begin{tabular}{c|cc|c|c|c}\n            \\bottomrule\n                  & AFS   & RRS   & \\multicolumn{1}{c|}{Image AUROC} & \\multicolumn{1}{c|}{Pixel AUROC} & \\multicolumn{1}{c}{PRO} \\\\\n            \\hline\n            1     & -     & -     & \\multicolumn{1}{c|}{94.46 / 95.67} & \\multicolumn{1}{c|}{93.38 / 95.84} & \\multicolumn{1}{c}{79.81 / 82.26} \\\\\n            2     & \\checkmark     & -     & 96.86 & 96.32 & 84.13 \\\\\n            3     & -     & \\checkmark     & \\multicolumn{1}{c|}{99.39 / 99.09} & \\multicolumn{1}{c|}{98.66 / 98.22} & \\multicolumn{1}{c}{92.01 / 88.38} \\\\\n            4     & \\checkmark     & \\checkmark     & \\textbf{99.65} & \\textbf{99.03} & \\textbf{93.07} \\\\\n            \\toprule\n            \\end{tabular}%\n            }\n\t\\end{subtable}\\quad\n\t\\begin{subtable}{0.50\\textwidth}\n        \\renewcommand\\arraystretch{1.02}\n        \\caption{The impact of anomaly strengths on RealNet.}\n        \\label{tab:table5-b}\n        \\resizebox{\\linewidth}{!}{\n        \\tiny\n    \t \\begin{tabular}{c|c|c|c|c}\n        \\bottomrule\n        \\multicolumn{1}{c|}{Metric} & $s$ = 0     & $s$ = 0.1  & $s$ = 0.2   & \\multicolumn{1}{c}{$s \\in$ [0.1, 0.2]} \\\\\n        \\hline\n        Image AUROC & 99.35 & \\textbf{99.65} & 99.61 & \\textbf{99.65} \\\\\n        Pixel AUROC & 98.85 & 98.96 & 98.95 & \\textbf{99.03} \\\\\n        PRO   & 91.80  & 92.16 & 89.36 & \\textbf{93.07} \\\\\n        \\toprule\n        \\end{tabular}%\n        }\n\t\\end{subtable}\\quad\n\\end{table*}",
            "tab:tables4": "\\begin{table}[t]\n  \\centering\n  \\renewcommand\\arraystretch{1.2}\n  \\caption{Performance evaluation of various reconstruction architectures on the MVTec-AD dataset \\cite{bergmann2019mvtec}. The metrics include Image AUROC (\\%), Pixel AUROC (\\%), and PRO (\\%).}\n  \\resizebox{\\linewidth}{!}{\n    \t  \\begin{tabular}{c|c|c|c}\n            \\bottomrule\n            \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Speed (FPS) $\\uparrow$} & \\multicolumn{1}{c|}{Model Size (GB) $\\downarrow$} & Metrics $\\uparrow$\\\\\n            \\toprule\n            \\multicolumn{3}{c}{\\{$m_1$,...,$m_{K}$\\} is \\{128, 256, 256, 128\\} and image size is $256 \\times 256$}&\\multicolumn{1}{c}{}  \\\\\n             \\bottomrule\n            A     & \\textbf{40.42} & \\textbf{0.74} & (99.49, \\textbf{99.07}, 91.17) \\\\\n            \\toprule\n            \\multicolumn{3}{c}{\\{$m_1$,...,$m_{K}$\\} is \\{256, 512, 512, 256\\} and image size is $256 \\times 256$}&\\multicolumn{1}{c}{}  \\\\\n            \\bottomrule\n            A     & 31.93 & 2.20 & (\\textbf{99.65}, 99.03, 93.07) \\\\\n            B     & 10.83 & 7.22  & (98.44, 98.17, 94.27) \\\\\n            C     & 22.39 & 3.75  & (99.62, 98.90, \\textbf{94.71}) \\\\\n            \\toprule\n            \\multicolumn{3}{c}{\\{$m_1$,...,$m_{K}$\\} is \\{256, 512, 512, 256\\} and image size is $512 \\times 512$}&\\multicolumn{1}{c}{}  \\\\\n            \\bottomrule\n            A     & 13.53 & 2.20 & (99.40, 98.71, 94.01) \\\\\n            \\toprule\n            \\end{tabular}%\n        }\n  \\label{tab:tables4}%\n\\end{table}",
            "tab:tables5": "\\begin{table}[t]\n  \\centering\n  \\renewcommand\\arraystretch{1.12}\n  \\caption{Comparison of RealNet with alternative methods in multi-class anomaly detection on the MVTec-AD dataset \\cite{bergmann2019mvtec}.}\n   \\resizebox{0.91\\linewidth}{!}{\n        \\tiny\n        \\begin{tabular}{c|cc}\n        \\bottomrule\n        Methods & \\multicolumn{1}{c}{Image AUROC} & \\multicolumn{1}{c}{Pixel AUROC} \\\\\n        \\hline\n        DRAEM \\cite{zavrtanik2021draem} & 88.1  & 87.2 \\\\\n        PaDiM \\cite{defard2021padim} & 84.2  & 89.5 \\\\\n        UniAD \\cite{you2022unified} & 96.5  & 96.8 \\\\\n        OmniAL \\cite{zhao2023omnial} & 97.2  & 98.3 \\\\\n        \\hline\n        RealNet & \\textbf{97.3} & \\textbf{98.4} \\\\\n        \\toprule\n        \\end{tabular}%\n    }\n  \\label{tab:tables5}%\n\\end{table}",
            "tab:tables6": "\\begin{table}[t]\n  \\centering\n  \\renewcommand\\arraystretch{1.12}\n  \\caption{Image quality comparison of SIA with alternative anomaly synthesis approaches on the MVTec-AD dataset \\cite{bergmann2019mvtec}.}\n   \\resizebox{0.91\\linewidth}{!}{\n        \\tiny\n        \\begin{tabular}{c|cc}\n        \\bottomrule\n        Methods & \\multicolumn{1}{c}{FID \\cite{heusel2017gans} $\\downarrow$ } & \\multicolumn{1}{c}{LPIPS \\cite{zhang2018unreasonable} $\\uparrow$ } \\\\\n        \\hline\n        DTD \\cite{cimpoi2014describing} & 120.52$\\pm$0.63  & 0.16$\\pm$0.00 \\\\\n        CutPaste \\cite{li2021cutpaste} & 77.34$\\pm$0.09  & 0.11$\\pm$0.00 \\\\\n        NSA \\cite{schluter2022natural} & 68.76$\\pm$0.16  & 0.09$\\pm$0.01 \\\\\n        \\hline\n        SIA & \\textbf{60.39$\\pm$1.26} & \\textbf{0.18$\\pm$0.01} \\\\\n        \\toprule\n        \\end{tabular}%\n    }\n  \\label{tab:tables6}%\n\\end{table}"
        },
        "figures": {
            "fig:fig1": "\\begin{figure}[t]\n  \\centering\n   \\includegraphics[width=0.95\\linewidth]{Fig1.pdf}\n   \\caption{SDAS generates anomaly images using only normal images. The example images are sourced from the MVTec-AD dataset \\cite{bergmann2019mvtec}.}\n   \\label{fig:fig1}\n\\end{figure}",
            "fig:fig2": "\\begin{figure*}[t]\n  \\centering\n   \\includegraphics[width=\\linewidth]{Fig2.pdf}\n   \\caption{The pipeline of our RealNet consists of three core components: Strength-controllable Diffusion Anomaly Synthesis (SDAS), Anomaly-aware Features Selection (AFS), and Reconstruction Residuals Selection (RRS). 1) SDAS enables the synthesis of diverse, near-natural distribution anomalous images. 2) AFS refines features extracted by large-scale pre-trained CNN for dimensionality reduction. Refined features are reconstructed into corresponding normal image features by a set of reconstruction networks. 3) RRS selects reconstruction residuals most likely to identify anomalies, which are then fed into a discriminator for anomaly detection and localization.}\n   \\label{fig:fig2}\n\\end{figure*}",
            "fig:fig3": "\\begin{figure*}[t]\n  \\centering\n   \\includegraphics[width=0.99\\linewidth]{Fig3.pdf}\n   \\caption{Anomaly image examples generated with different synthesis methods. (a) Examples generated using SDAS with different anomaly strengths $s$. (b) Examples featuring local anomaly regions generated by various anomaly synthesis methods.}\n   \\label{fig:fig3}\n\\end{figure*}",
            "fig:fig5": "\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.919\\textwidth]{Fig5.pdf}\n  \\caption{Performance of RealNet on MVTec-AD dataset \\cite{bergmann2019mvtec} under various modes of reconstruction residuals selection (\\texttt{Max}, \\texttt{Avg}, and \\texttt{Max}\\&\\texttt{Avg}) and varying retention ratio $P$ of reconstruction residuals.}\n  \\label{fig:fig5}\n\\end{figure*}",
            "fig:figs1": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{FigS1.pdf}\n  \\caption{Sample anomaly images generated by SDAS with different anomaly strengths $s$.}\n  \\label{fig:figs1}\n  \\vspace{-0.4cm}\n\\end{figure}",
            "fig:figs3": "\\begin{figure*}[htbp]\n  \\centering\n  \\includegraphics[width=\\textwidth]{FigS3.pdf}\n  \\caption{Qualitative results of RealNet. Within each group, from left to right, are the anomaly image, ground-truth, and predicted anomaly score. The examples are from the MVTec-AD \\cite{bergmann2019mvtec}, MPDD \\cite{jezek2021deep}, BTAD \\cite{mishra2021vt}, and VisA \\cite{zou2022spot} datasets.}\n  \\label{fig:figs3}\n\\end{figure*}",
            "fig:figs4": "\\begin{figure*}[htbp]\n  \\centering\n  \\includegraphics[width=\\textwidth]{FigS4.pdf}\n  \\caption{Anomaly images generated by SDAS. The examples are from the MVTec-AD \\cite{bergmann2019mvtec}, MPDD \\cite{jezek2021deep}, BTAD \\cite{mishra2021vt}, and VisA \\cite{zou2022spot} datasets. Within each group, from top to bottom, the anomaly strength gradually increases.}\n  \\label{fig:figs4}\n\\end{figure*}",
            "fig:figs5": "\\begin{figure*}[htbp]\n  \\centering\n  \\includegraphics[width=\\textwidth]{FigS5.pdf}\n  \\caption{Normal images generated by SDAS (when $s=0$). The examples are from the MVTec-AD \\cite{bergmann2019mvtec}, MPDD \\cite{jezek2021deep}, BTAD \\cite{mishra2021vt}, and VisA \\cite{zou2022spot} datasets.}\n  \\label{fig:figs5}\n\\end{figure*}",
            "fig:figs6": "\\begin{figure*}[htbp]\n  \\centering\n  \\includegraphics[width=\\textwidth]{FigS6.pdf}\n  \\caption{Local anomaly images synthesized by SIA. The examples are from the MVTec-AD \\cite{bergmann2019mvtec}, MPDD \\cite{jezek2021deep}, BTAD \\cite{mishra2021vt}, and VisA \\cite{zou2022spot} datasets. Within each group, from top to bottom, the anomaly strength gradually increases.}\n  \\label{fig:figs6}\n\\end{figure*}",
            "fig:figs7": "\\begin{figure*}[htbp]\n  \\centering\n  \\includegraphics[width=0.95\\textwidth]{FigS7.pdf}\n  \\caption{Visualization of AFS. For an original image and a synthetic anomaly image, we visualize the normalized difference between their corresponding feature maps across different layers of a pre-trained WideResNet50 \\cite{zagoruyko2016wide}. From top to bottom, the feature map respectively come from the first layer to the fourth layer. Each feature map is labelled with its index in the layer and the corresponding AFS loss. From left to right, the localization performance of the feature maps gradually decreases. Our visualization intuitively demonstrates the localization bias caused by pre-training, indicating that not all feature maps contribute equally to anomaly detection and localization, as well as emphasizing the effectiveness of AFS.}\n  \\label{fig:figs7}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{eq:equ1}\n\\mathcal{L}_{simple}=\\mathbb{E}_{t,x_0,\\epsilon}[\\|\\epsilon-\\epsilon_\\theta(x_t,t)\\|^2]\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{eq:equ2}\np_\\theta(x_{t-1}'|x_{t})=\\mathcal N(x_{t-1}';\\mu_\\theta(x_t,t),(1+s)\\Sigma_\\theta(x_t,t))\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:equ3}\nA=\\overline{M} \\odot I+(1-\\delta)(M \\odot I)+\\delta(M \\odot P)\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:equ4}\n\\mathcal{L}_{AFS}(\\phi_{k,i})=\\frac{1}{N}\\sum_{n=1}^N\\|F([\\phi_{k,i}(A_n)-\\phi_{k,i}(I_n)]^2)-M_n\\|_2^2\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\label{eq:equ5}\n\\mathcal{L}_{recon}(A,I)=\\frac{1}{N}\\sum_{n=1}^N\\sum_{k=1}^K\\|G_k(\\varphi_k(A_n))-\\varphi_k(I_n)\\|_2^2\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\label{eq:equ6}\n\\mathcal{L}(A,I,M)=\\mathcal{L}_{recon}(A,I)+\\mathcal{L}_{seg}(A,M)\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\label{eq:equs1}\n\\Sigma_\\theta(x_t,t)=\\exp(v\\log\\beta_t+(1-v)\\log\\tilde{\\beta}_t)\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\label{eq:equs2}\n\\mathcal{L}_{hybrid}=\\mathcal{L}_{simple}+\\gamma\\mathcal{L}_{vlb}\n\\end{equation}"
        },
        "git_link": "https://github.com/cnulab/RealNet"
    }
}