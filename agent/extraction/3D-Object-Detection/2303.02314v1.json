{
    "meta_info": {
        "title": "Virtual Sparse Convolution for Multimodal 3D Object Detection",
        "abstract": "Recently, virtual/pseudo-point-based 3D object detection that seamlessly\nfuses RGB images and LiDAR data by depth completion has gained great attention.\nHowever, virtual points generated from an image are very dense, introducing a\nhuge amount of redundant computation during detection. Meanwhile, noises\nbrought by inaccurate depth completion significantly degrade detection\nprecision. This paper proposes a fast yet effective backbone, termed\nVirConvNet, based on a new operator VirConv (Virtual Sparse Convolution), for\nvirtual-point-based 3D object detection. VirConv consists of two key designs:\n(1) StVD (Stochastic Voxel Discard) and (2) NRConv (Noise-Resistant Submanifold\nConvolution). StVD alleviates the computation problem by discarding large\namounts of nearby redundant voxels. NRConv tackles the noise problem by\nencoding voxel features in both 2D image and 3D LiDAR space. By integrating\nVirConv, we first develop an efficient pipeline VirConv-L based on an early\nfusion design. Then, we build a high-precision pipeline VirConv-T based on a\ntransformed refinement scheme. Finally, we develop a semi-supervised pipeline\nVirConv-S based on a pseudo-label framework. On the KITTI car 3D detection test\nleaderboard, our VirConv-L achieves 85% AP with a fast running speed of 56ms.\nOur VirConv-T and VirConv-S attains a high-precision of 86.3% and 87.2% AP, and\ncurrently rank 2nd and 1st, respectively. The code is available at\nhttps://github.com/hailanyi/VirConv.",
        "author": "Hai Wu, Chenglu Wen, Shaoshuai Shi, Xin Li, Cheng Wang",
        "link": "http://arxiv.org/abs/2303.02314v1",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "Accepted by CVPR 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n\n3D object detection plays a critical role in autonomous driving~\\cite{MPVN,MVP}. The LiDAR sensor measures the depth of scene~\\cite{MV3D} in the form of a point cloud and enables reliable localization of objects in various lighting environments. While LiDAR-based 3D object detection has made rapid progress in recent years~\\cite{PartA2,3DIoULoss,PointPillars,STD,3DSSD,PointRCNN,CT3D,Pyramid}, its performance drops significantly on distant objects, which inevitably have sparse sampling density in the scans. \nUnlike LiDAR scans, color image sensors provide high-resolution sampling and rich context data of the scene. \nThe RGB image and LiDAR data can complement each other and usually boost 3D detection performance~\\cite{F-PointNet,UberATG-MMF,BEVFusion,TransFusion,FConv}.\n\nEarly methods~\\cite{MVX-Net, Pointaugmenting, PointPainting} extended the features of LiDAR points with image features, such as semantic mask and 2D CNN features. They did not increase the number of points; thus, the distant points still remain sparse. In contrast, the methods based on virtual/pseudo points (for simplicity, both denoted as \\textbf{virtual points} in the following) enrich the sparse points by creating additional points around the LiDAR points. For example, MVP~\\cite{MVP} creates the virtual points by completing the depth of 2D instance points from the nearest 3D points. SFD~\\cite{SFD} creates the virtual points based on depth completion networks~\\cite{PENet}. The virtual points complete the geometry of distant objects, showing the great potential for high-performance 3D detection.\n\nHowever, virtual points generated from an image are generally very dense. Taking the KITTI~\\cite{KITTI} dataset as an example, an 1242$\\times$375 image generates 466k virtual points (\\textbf{$\\sim$27$\\times$} more than the LiDAR scan points). This brings a huge computational burden and causes a severe efficiency issue (see Fig.~\\ref{fig:motivation} (f)). Previous work addresses the density problem by using a larger voxel size~\\cite{PointPillars, CenterPoint} or by randomly down-sampling~\\cite{RandLA-Net} the points. However, applying such methods to virtual points will inevitably sacrifice useful shape cues from faraway points and result in decreased detection accuracy.\n\n\n\nAnother issue is that the depth completion can be inaccurate, and it brings a large amount of noise in the virtual points (see Fig.~\\ref{fig:motivation} (c)).\nSince it is very difficult to distinguish the noises from the background in 3D space, the localization precision of 3D detection is greatly degraded. \nIn addition, the noisy points are non-Gaussian distributed, and can not be filtered by conventional denoising algorithms~\\cite{BMD,Reviewfilter}. \nAlthough recent semantic segmentation network~\\cite{CNNDenoising} show promising results, they generally require extra annotations.\n\nTo address these issues, this paper proposes a VirConvNet pipeline based on a new Virtual Sparse Convolution (VirConv) operator. \nOur design builds on \\textbf{two main observations}.\n(1) First, geometries of nearby objects are often relatively complete in LiDAR scans. Hence, most virtual points of nearby objects only bring marginal performance gain (see Fig.~\\ref{fig:motivation} (e)(f)), but increase the computational cost significantly.  \n(2) Second, noisy points introduced by inaccurate depth completions are mostly distributed on the instance boundaries (see Fig.~\\ref{fig:motivation} (d)). \nThey can be recognized in 2D images after being projected onto the image plane. \n\nBased on these two observations, we design a \\textbf{StVD} (Stochastic Voxel Discard) scheme to retain those most important virtual points by a bin-based sampling, namely, discarding a huge number of nearby voxels while retaining faraway voxels. This can greatly speed up the network computation. \nWe also design a \\textbf{NRConv} (Noise-Resistant Submanifold Convolution) layer to encode geometry features of voxels in both 3D space and 2D image space. \nThe extended receptive field in 2D space allows our NRConv to distinguish the noise pattern on the instance boundaries in 2D image space. Consequently, the negative impact of noise can be suppressed. \n\nWe develop three multimodal detectors to demonstrate the superiority of our VirConv: (1) a lightweight \\textbf{VirConv-L} constructed from Voxel-RCNN~\\cite{Voxel-RCNN}; (2) a high-precision \\textbf{VirConv-T} based on multi-stage~\n\\cite{CasA} and multi-transformation~\\cite{TED} design; (3) a semi-supervised \\textbf{VirConv-S} based on a pseudo-label~\\cite{3DIoUMatch} framework. The effectiveness of our design is verified by extensive experiments on the widely used KITTI dataset~\\cite{KITTI} and nuScenes dataset~\\cite{nuScenes}.\nOur contributions are summarized as follows: \n\n\\begin{itemize}\n\\item We propose a \\textbf{VirConv} operator, which effectively encodes voxel features of virtual points by \\textbf{StVD} and \\textbf{NRConv}. The StVD discards a huge number of redundant voxels and substantially speeds up the 3D detection prominently. The NRConv extends the receptive field of 3D sparse convolution to the 2D image space and significantly reduces the impact of noisy points. \n\n\\item Built upon VirConv, we present three new multimodal detectors: a \\textbf{VirConv-L}, a \\textbf{VirConv-T}, and a semi-supervised \\textbf{VirConv-S} for efficient, high-precision, and semi-supervised 3D detection, respectively.\n\n\\item Extensive experiments demonstrated the effectiveness of our design (see Fig.~\\ref{fig:performance}). On the KITTI leaderboard, our VirConv-T and VirConv-S currently \\textbf{rank 2nd and 1st}, respectively. Our VirConv-L runs at \\textbf{56ms} with competitive precision.\n\\end{itemize}"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n \\textbf{LiDAR-based 3D object detection.}\nLiDAR-based 3D object detection has been widely studied in recent years. Early methods project the point clouds into a 2D Bird's eye view (BEV) or range view images~\\cite{MV3D,Birdnet} for 3D detection. Recently, voxel-based sparse convolution~\\cite{SECOND,PointPillars,Voxel-RCNN,SA-SSD} and point-based set abstraction~\\cite{PointRCNN,STD,3DSSD,PV-RCNN} have become popular in designing effective detection frameworks. \nHowever, the scanning resolution of LiDAR is generally very low for distant objects. The LiDAR-only detectors usually suffer from such sparsity. This paper addresses this problem by introducing RGB image data in a form of virtual points.\n\n \\textbf{Multimodal 3D object detection.}\nThe RGB image and LiDAR data can complement each other and usually boost 3D detection performance. Early methods extend the features of LiDAR points with image features~\\cite{MVX-Net, Pointaugmenting, PointPainting}. Some works encode the feature of two modalities independently and fuse the two features in the local Region of Interest (RoI)~\\cite{AVOD-FPN,FUTR3D} or BEV plane~\\cite{BEVFusion}. We follow the recent work that fuses the two data via virtual points~\\cite{MVP,SFD}. The virtual points explicitly complete the geometry of distant objects by depth estimation, showing the great potential for high-performance 3D detection. But virtual points are extremely dense and often noisy. This paper addresses these problems through two new schemes, StVD and NRConv, respectively.\n\n\\textbf{3D object detection with re-sampled point clouds.}\nThe points captured by LiDAR are generally dense and unevenly distributed. Previous work speeds up the network by using a larger voxel size~\\cite{PointPillars, CenterPoint} or by randomly down-sampling~\\cite{RandLA-Net} the point clouds. However, applying these methods to the virtual points will significantly decrease the useful geometry cues, especially for the faraway objects. Different from that, our StVD retains all the useful faraway voxels and speeds up the network by discarding nearby redundant voxels. \n\n\\textbf{Noise handling in 3D vision.}\nTraditional methods handle the noises by filtering algorithm~\\cite{BMD,Reviewfilter,GuidedFilter}. Recently, score-based~\\cite{SBDenoising} and semantic segmentation networks~\\cite{CNNDenoising} are developed for point cloud noise removal. Different from the traditional noises that are randomly distributed in 3D space, the noises brought by inaccurate depth completion are mostly distributed on 2D instance boundaries. Although the noise can be roughly removed by some 2D edge detection method~\\cite{BiEdge}, this will sacrifice the useful boundary\npoints of object. We design a new scheme, NRConv, that extends the receptive field of 3D sparse convolution to 2D image space, distinguishing the noise pattern without the loss of useful boundary points.\n\n\\textbf{Semi-supervised 3D object detection.}\nRecent semi-supervised methods boost 3D object detection by a large amount of unlabeled data. Inspired by the pseudo-label-based framework~\\cite{3DIoUMatch,SESS,SS3D}, we also constructed a VirConv-S pipeline to perform semi-supervised multimodal 3D object detection. \n"
            },
            "section 3": {
                "name": "VirConv for Multimodal 3D Detection",
                "content": "\n\\label{sec:virpoint}\n\nThis paper proposes VirConvNet, based on a new VirConv operator, for virtual-point-based multimodal 3D object detection. \nAs shown in Fig.~\\ref{fig:framework},  VirConvNet first converts points into voxels, and gradually encodes voxels into feature volumes by a series of VirConv block with $1\\times$, $2\\times$, $4\\times$ and $8\\times$ downsampling strides. The VirConv block consists of three parts (see Fig.~\\ref{fig:framework} (a)): (1) an StVD layer for speeding up the network and improving density robustness; (2) multiple NRConv layers for encoding features and decreasing the impact of noise; (3) a 3D SpConv layer for down-sampling the feature map. Based on the VirConv operator, we build three detectors for efficient, accurate, and semi-supervised multimodal 3D detection, respectively.\n\n",
                "subsection 3.1": {
                    "name": "Virtual Points for Data Fusion",
                    "content": "\n\\label{sec:vpdf}\n\nMany recent 3D detectors use virtual points~\\cite{MVP} (pseudo points~\\cite{SFD}) generated from an image by depth completion algorithms to fuse RGB and LiDAR data. \nWe denote the LiDAR points and virtual points as $\\mathbf{P}$ and $\\mathbf{V}$, respectively. \nRecently, two popular fusion schemes have been applied for 3D object detection: \n(1) early fusion~\\cite{MVP}, which fuses $\\mathbf{P}$ and $\\mathbf{V}$ into a single point cloud $\\mathbf{P}^*$ and performs 3D object detection using existing detectors, and \n(2) late fusion~\\cite{SFD}, which encodes the features of $\\mathbf{P}$ and $\\mathbf{V}$ by different backbone networks and fuses the two types of features in BEV plane or local RoI.\n%\\textcolor{red}{(``late fusion'' reads very awkward. Is this an commonly used term or a self-invented word?)}\nHowever, both fusion methods suffer from the dense and noisy nature of virtual points. \n\n\\textbf{(1) Density problem.}\nAs motioned in Section~\\ref{sec:intro}, the virtual points are usually very dense. They introduce a huge computational burden, which significantly decreases the detection speed (e.g., more than \\textbf{2$\\times$} in Fig.~\\ref{fig:motivation} (f)). \nExisting work tackles the density issue by using a larger voxel size~\\cite{PointPillars} or by randomly down-sampling~\\cite{RandLA-Net} the points. But these methods will inevitably sacrifice the shape cues from the virtual points, especially for the faraway object. Based on a pilot experiment on the KITTI dataset~\\cite{KITTI} using the Voxel-RCNN~\\cite{Voxel-RCNN} with a late fusion, we observed that a huge number of virtual points introduced for nearby objects are redundant. Specifically, \\textbf{97\\%} of virtual points from the nearby objects bring only a \\textbf{0.18\\%} performance improvement, while \\textbf{3\\%} of virtual points for the faraway objects bring a \\textbf{2.2\\%} performance improvement. \nThe reason is that the geometry of nearby objects\nis relatively complete for LiDAR points. \nSuch virtual points generally bring marginal performance gain but increase unnecessary computation. Motivated by this observation, we design an StVD (Stochastic Voxel Discard) scheme, which alleviates the computation problem by discarding nearby redundant voxels. In addition, the points of the distant object are much sparser than the nearby objects (see Fig.~\\ref{fig:motivation} (e)). The StVD can simulate sparser training samples to improve detection robustness. \n\n\n\n\\textbf{(2) Noise problem.}\nThe virtual points generated by the depth completion network are usually noisy. An example is shown in Fig.~\\ref{fig:motivation} (c). The noise is mostly introduced by the inaccurate depth completion, and is hardly distinguishable in  3D space. By using only virtual points, the detection performance drops $\\sim$9\\% AP compared with the LiDAR-only detector (see Fig.~\\ref{fig:motivation} (f)). In addition, the noisy points are non-Gaussian distributed, and cannot be filtered by traditional denoising algorithms~\\cite{BMD,Reviewfilter}. \nWe observed that noise is mainly distributed on the instance boundaries (see Fig.~\\ref{fig:motivation} (d)) and can be more easily recognized in 2D images. Although the edge detection~\\cite{BiEdge} could be applied here to roughly remove the noise, this will sacrifice the useful boundary points which are beneficial to the object's shape and position estimation. Our idea is to extend the receptive field of sparse convolution to the 2D image space, and distinguish the noise without the loss of shape cues. \n\n"
                },
                "subsection 3.2": {
                    "name": "Stochastic Voxel Discard",
                    "content": "\n\\label{sec:svd}\n\nTo alleviate the computation problem and improve the density robustness for the virtual-point-based detector, we develop the StVD. It consists of two parts: (1) input StVD, which speeds up the network by discarding input voxels of virtual points during both the training and inference process; (2) layer StVD, which improves the density robustness by discarding voxels of virtual points at every VirConv block during only the training process.\n\n\\textbf{Input StVD.}\nTwo naive methods can keep less input voxels: (1) random sampling or (2) farthest point sampling (FPS).\nHowever, the random sampling usually keeps unbalanced voxels at different distances and inevitably sacrifices some useful shape cues (in the red region at Fig.~\\ref{fig:method_sampling} (a) (b)). In addition, FPS needs huge extra computation when down-sampling the huge number of virtual points due to the high computational complexity ($O(n^2)$).\nTo tackle this problem, we introduce a bin-based sampling strategy to perform efficient and balanced sampling (see Fig.~\\ref{fig:method_sampling} (c)). \nSpecifically, We first divide the input voxels into $N^b$ bins (we adopt $N^b=10$ in this paper) according to different distances. For the nearby bins ($\\leq$30m based on the statistics in Fig.~\\ref{fig:motivation} (e)), we randomly keep a fixed number ($\\sim$ 1K) of voxels. For distant bins, we keep all of the inside voxels. After the bin-based sampling, we discard about \\textbf{90\\%} (which achieves the best precision-efficiency trade-off, see Fig.~\\ref{tab:ablation_svd_rate}) of redundant voxels and it speeds up the network by about \\textbf{2 times}. \n\n\n\\textbf{Layer StVD.}\nTo improve the robustness of detection from sparse points, we also develop a layer StVD which is applied to the training process.\nSpecifically, we discard voxels at each VirConv block to simulate sparser training samples. We adopt a discarding rate of 15\\% in this paper (the layer StVD rate is discussed in Fig.~\\ref{tab:ablation_svd_rate}). The layer StVD serves as a data augmentation strategy to help enhance the 3D detector's training.\n\n\n%\\input{figText/method_StVDrate}\n\n"
                },
                "subsection 3.3": {
                    "name": "Noise-Resistant Submanifold Convolution",
                    "content": "\nAs analyzed in Section~\\ref{sec:vpdf}, the noise introduced by the inaccurate depth completion can hardly be recognized from 3D space but can be easily recognized from 2D images. We develop an NRConv (see Fig.~\\ref{fig:framework} (b)) from the widely used submanifold sparse convolution~\\cite{SpConv} to address the noise problem. Specifically, given $N$ input voxels formulated by a 3D indices vector\n$\\mathbf{H}\\in \\mathbb{R}^{N\\times 3}$ and a features vector\n$\\mathbf{X}\\in \\mathbb{R}^{N\\times C^{in}}$, we encode the noise-resistant geometry features $\\mathbf{Y}\\in \\mathbb{R}^{N\\times C^{out}}$ in both 3D and 2D image space, where $C^{in}$ and $C^{out}$ denote the number of input and output feature channels respectively.\n\n\\textbf{Encoding geometry features in 3D space.}\nFor each voxel feature $ X_i$ in $\\mathbf{X}$, we first encode the geometry features by the 3D submanifold convolution kernel $\\mathcal{K}^{3D}(\\cdot)$. Specifically, the geometry features $\\hat{X}_i\\in\\mathbb{R}^{C^{out}/2}$ are calculated from the non-empty voxels within $3\\times3\\times3$ neighborhood based on the corresponding 3D indices as\n\n\\vspace{-4mm}\n\\begin{align} \n\\hat{X}_i = \\mathcal{R}\\left\\{\\mathcal{K}^{3D} \\left(  X_i, X_i^{(f_{1})}, ..., X_i^{(f_{j})} \\right)\\right\\}, %\\text{ for } i = 1, ..., N,\n\\end{align}\n\\vspace{-4mm}\n\n\\noindent where $X_i^{(f_{1})}, ..., X_i^{(f_{j})}$ denote neighbor features generated by $\\mathbf{H}$, and $\\mathcal{R}$ denotes the nonlinear activation function.\n\n \\textbf{Encoding noise-aware features in 2D image space.}\nThe noise brought by the inaccurate depth completion significantly degrade the detection performance. Since the noise is mostly distributed on the 2D instance boundaries, we extend the convolution receptive field to the 2D image space and encode the noise-aware features using the 2D neighbor voxels. Specifically, we first convert the 3D indices to a set of grid points based on the voxelization parameters (the conversion denoted as $\\mathcal{G}(\\cdot)$). Since state-of-the-art detectors~\\cite{SFD, Voxel-RCNN} also adopt the transformation augmentations (the augmentation denoted as $\\mathcal{T}(\\cdot)$) such as rotation and scaling, the grid points are generally misaligned with the corresponding image. Therefore, we transform the grid points backward into the original coordinate system based on the data augmentation parameters. Then we project the grid points into the 2D image plane based on the LiDAR-Camera calibration parameters (with the projection denoted as $\\mathcal{P}(\\cdot)$). The overall projection can be summarized by \n\n\\vspace{-4mm}\n\\begin{align}\n    \\mathbf{\\hat{H}} = \\mathcal{P} \\left(\\mathcal{T}^{-1}\\left(\\mathcal{G}\\left(\\mathbf{H}\\right)\\right)\\right),\n\\end{align}\n\\vspace{-4mm}\n\n\\noindent where $\\mathbf{\\hat{H}}\\in\\mathbb{R}^{N\\times2}$ denotes the 2D indices vector.\nFor each voxel feature $X_i\\in \\mathbb{R}^{C^{in}}$, we then calculate the noise-aware features $\\tilde{X}_i\\in\\mathbb{R}^{C^{out}/2}$ from the non-empty voxels within a $3\\times3$ neighborhood based on the corresponding 2D indices. \n\n\\vspace{-4mm}\n\\begin{align} \n\\tilde{X}_i = \\mathcal{R}\\left\\{\\mathcal{K}^{2D} \\left (  X_i, \\tilde{X}_i^{(f_{1})}, ..., \\tilde{X}_i^{(f_{k})} \\right) \\right\\}, %\\text{ for } i = 1, ..., N,\n\\end{align}\n\\vspace{-4mm}\n\n\\noindent where $\\tilde{X}_i^{(f_{1})}, ..., \\tilde{X}_i^{(f_{k})}$ denote the neighbor voxel features generated by $\\mathbf{\\hat{H}}$, and  $\\mathcal{K}^{2D}(\\cdot)$ denote the 2D submanifold convolution kernel. If there are multiple features in a single 2D neighbor voxel, we perform max-pooling and keep one feature in each voxel to perform the 2D convolution. \n\nAfter the 3D and 2D features encoding, we adopt a simple concatenation to implicitly learn a noise-resistant feature. \nSpecifically, we finally concatenate $\\hat{X}_i$ and $\\tilde{X}_i$ to obtain the noise-resistant feature vector $\\mathbf{Y}\\in\\mathbb{R}^{N\\times C^{out}}$ as\n\\begin{equation}\n   \\mathbf{Y} =\\left[\\left[\\hat{X}_i,\\tilde{X}_i\\right]^T,...,\\left[\\hat{X}_N,\\tilde{X}_N\\right]^T \\right]^T.\n\\end{equation}\nDifferent from related noise segmentation and removal~\\cite{CNNDenoising} methods, our NRConv implicitly distinguishes the noise pattern by extending the receptive field to 2D image space. Consequently, the impact of noise is suppressed without lose of shape cues.\n%-------------------------------------------------------------------------\n\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Detection Frameworks with VirConv",
                    "content": "\n\\label{sec:framework}\n\nTo demonstrate the superiority of our VirConv, we constructed VirConv-L, VirConv-T and VirConv-S from the widely used Voxel-RCNN~\\cite{Voxel-RCNN} for fast, accurate and semi-supervised 3D object detection, respectively.\n\n \\textbf{VirConv-L.}\nWe first construct the lightweight VirConv-L (Fig.~\\ref{fig:framework} (c)) for fast multimodal 3D detection. VirConv-L adopts an early fusion scheme and replaces the backbone of Voxel-RCNN with our VirConvNet. Specifically, we denote the LiDAR points as $\\mathbf{P}=\\{p\\}, p=[x,y,z,\\alpha]$, where $x,y,z$ denotes the coordinates and  $\\alpha$ refers intensity. We denote the virtual points as $\\mathbf{V}=\\{v\\}, v=[x,y,z]$. We fuse them into a single point cloud $\\mathbf{P}^*=\\{p^*\\}_k,p^*=[x,y,z,\\alpha,\\beta]$, where $\\beta$ is an indicator denoting where the point came from. The intensity of virtual points is padded by zero. The fused points are encoded into feature volumes by our VirConvNet for 3D detection.\n\n%\\input{figText/method_virconv_S}\n  \\textbf{VirConv-T.}\nWe then construct a high-precision VirConv-T based on a Transformed Refinement Scheme (TRS) and a late fusion scheme (see Fig.~\\ref{fig:virconvt}). CasA~\\cite{CasA} and TED~\\cite{TED} achieve high detection performance based on three-stage refinement and multiple transformation design, respectively. However, both of them require heavy computations. We fuse the two high computation detectors into a single efficient pipeline. Specifically, we first transform $\\mathbf{P}$ and $\\mathbf{V}$ with different rotations and reflections. Then we adopt the VoxelNet~\\cite{Voxel-RCNN} and VirConvNet to encode the features of $\\mathbf{P}$ and $\\mathbf{V}$, respectively. Similar to TED~\\cite{TED}, the convolutional weights between different transformations are shared. After that, the RoIs are generated by a Region Proposal Network (RPN)~\\cite{Voxel-RCNN} and refined by the backbone features (the RoI features of $\\mathbf{P}$ and $\\mathbf{V}$ fused by simple concatenation) under the first transformation. The refined RoIs are further refined by the backbone features under other transformations. Next, the refined RoIs from different refinement stages are fused by boxes voting, as is done by CasA~\\cite{CasA}. We finally perform a non-maximum-suppression (NMS) on the fused RoIs to obtain detection results. \n\n\\textbf{VirConv-S.}\nWe also design a semi-supervised pipeline, VirConv-S, using the widely used pseudo-label method~\\cite{ST3D,3DIoUMatch}. \nSpecifically, first, a model is pre-trained using the labeled training data. Then, pseudo labels are generated on a larger-scale unannotated dataset using this pre-trained model. \nA high-score threshold (empirically, 0.9) is adopted to filter out low-quality labels. \nFinally, the VirConv-T model is trained using both real and pseudo labels. "
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\n",
                "subsection 4.1": {
                    "name": "KITTI Datasets and Evaluation Metrics",
                    "content": "\nThe KITTI 3D object detection dataset~\\cite{KITTI} contains 7,481 and 7,518 LiDAR and image frames for training and testing, respectively. We divided the training data into a train split of 3712 frames and a validation split of 3769 frames following recent works~\\cite{Voxel-RCNN,CasA}. We also adopted the widely used evaluation metric: 3D Average Precision (AP) under 40 recall thresholds (R40). The IoU thresholds in this metric are 0.7, 0.5, and 0.5 for car, pedestrian, and cyclist, respectively. \nWe used the KITTI odometry dataset~\\cite{KITTI} as the large-scale unlabeled dataset. The KITTI odometry dataset contains 43,552 LiDAR and image frames. We uniformly sampled 10,888 frames (denoted as the \\textit{semi} dataset) and used them to train our VirConv-S. There is no overlap found between the KITTI 3D detection dataset and the KITTI odometry dataset after checking the mapping files released by KITTI. \n\n"
                },
                "subsection 4.2": {
                    "name": "Setup Details ",
                    "content": "\n\\textbf{Network details}. \nSimilar to SFD, our method uses the virtual points generated by PENet~\\cite{PENet}. \nVirConvNet adopts an architecture similar to the Voxel-RCNN backbone~\\cite{Voxel-RCNN}. \nVirConvNet includes four levels of VirConv blocks with feature dimensions 16, 32, 64, and 64, respectively. \nThe input StVD rate and layer StVD rate are set to 90\\% and 15\\% by default. \nOn the KITTI dataset, all the detectors use the same  detection range and voxel size as CasA~\\cite{CasA}.\n\n\\textbf{Losses and data augmentation}. \nVirConv-L uses the same training loss as in~\\cite{Voxel-RCNN}. \nVirConv-T and VirConv-S use the same training loss as CasA~\\cite{CasA}. \nIn all these three pipelines, we adopted the widely used local and global data augmentation~\\cite{SFD,CasA,PointRCNN}, including ground-truth sampling, local transformation (rotation and translation), and global transformation (rotation and flipping).\n\n\\textbf{Training and inference details}. \nAll three detectors were trained on 8 Tesla V100 GPUs with the ADAM optimizer. We used a learning rate of 0.01 with a one-cycle learning rate strategy. We trained the VirConv-L and VirConv-T for 60 epochs. The weights of VirConv-S are initialized by the trained VirConv-T. \nWe further trained the VirConv-S on the labeled and unlabeled dataset for 10 epochs. \nWe used an NMS threshold of 0.8 to generate 160 object proposals with 1:1 positive and negative samples during training. During testing, we used an NMS threshold of 0.1 to remove redundant boxes after proposal refinement. \n\n"
                },
                "subsection 4.3": {
                    "name": "Main Results",
                    "content": "\n\n\\textbf{Results on KITTI validation set.}\nWe report the car detection results on the KITTI validation set in Table~\\ref{tab:kitti_val_ap}. Compared with the baseline detector Voxel-RCNN~\\cite{Voxel-RCNN}, our VirConv-L, VirConv-T and VirConv-S show 3.42\\%, 5\\% and 5.68\\% 3D AP(R40) improvement in the moderate car class, respectively. We also reported the performance based on the 3D AP under 11 recall thresholds (R11). Our VirConv-L, VirConv-T and VirConv-S show 2.38\\%, 3.33\\% and 3.54\\% 3D AP(R11) improvement in the moderate car class, respectively. The performance gains are mostly derived from the VirConv design, which effectively addressed the density problem and noise problem brought by virtual points. Note that our VirConv-L also runs much faster than other multimodal detectors, thanks to our efficient StVD design.\n\n\n\\textbf{Results on KITTI test set.}\nThe experimental results on the KITTI test set are reported in Table~\\ref{tab:kitti_test_ap}. Our VirConv-L, VirConv-T, and VirConv-S outperform the baseline Voxel-RCNN~\\cite{Voxel-RCNN} by 3.43\\%, 4.63\\% and 5.58\\% 3D AP (R40) in the moderate car class, respectively. \nThe VirConv-L, VirConv-T, and VirConv-S also outperform the best previous 3D detector SFD~\\cite{SFD} by 0.29\\%, 1.49\\%, and 2.44\\%, respectively. \nAs of the date of the CVPR deadline (Nov.11, 2022), our VirConv-T and VirConv-S rank 2nd and 1st, respectively, on the KITTI 3D object detection leaderboard.\nThe results further demonstrate the effectiveness of our method. \n\n \n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study",
                    "content": "\nWe conducted experiments on the KITTI validation set to examine the hyper-parameters and validate each component/design of the proposed method.\n\n\n\\textbf{VirConv performance with different fusion schemes.}\nVirtual points only, early fusion, and late fusion are three potential choices for virtual points-based 3D object detection. To investigate the performance of VirConv under these three settings, we first constructed three baselines: Voxel-RCNN~\\cite{Voxel-RCNN} with only virtual points, Voxel-RCNN~\\cite{Voxel-RCNN} with early fusion, and Voxel-RCNN~\\cite{Voxel-RCNN} with late fusion. \nThen we replaced the backbone of Voxel-RCNN with our VirConvNet. The experimental results on the KITTI validation set are shown in Table~\\ref{tab:ablation_virconv}. \nWith our VirConv, the 3D AP has significantly improved by 3.43\\%, 2.93\\%, and 2.65\\%, under virtual points only, early fusion, and late fusion settings, respectively. Meanwhile, the efficiency significantly improves. This is because VirConv speeds up the network with the StVD design and decreases the noise impact with the NRConv design.\n\n\n\n\n\\textbf{Effectiveness of StVD.}\nWe next investigated the effectiveness of StVD. The results are shown in Table~\\ref{tab:ablation_svd_submconv}. With StVD, VirConv-T not only performs more accurate 3D detection but also runs faster by about 2$\\times$. The reason lies in that  StVD discards about 90\\% of redundant voxels to speed up the network, and it also improves the detection robustness by simulating more sparse training samples.\n\n\\textbf{Influence of StVD rate.} \nWe then conducted experiments to select the best input and layer StVD rate. The results are shown in Fig.~\\ref{tab:ablation_svd_rate}. We observe that using a higher input StVD rate, the detection performance will decrease dramatically due to the geometry feature loss. On the contrary, using a lower input StVD rate, the efficiency is degraded with poor AP improvement. We found that by randomly discarding 90\\% of nearby voxels, we achieve the best accuracy-efficiency trad-off. Therefore, this paper adopts an input StVD rate of 90\\%. Similarly, by using a 15\\% layer StVD rate, we achieved the best detection accuracy.\n%Thus, this paper adopts a layer StVD rate of 15\\%.\n\n\n\\textbf{Effectiveness of NRConv.}\nWe then investigated the effects of NRConv using VirConv-T. The results are shown in Table~\\ref{tab:ablation_svd_submconv}. With our NRConv, the car detection AP of VirConv-T improves from  88.32\\% to 90.29\\%. \nSince the NRConv encodes the voxel features in both 3D and 2D image space, reducing the noise impact brought by the inaccurate depth completion, the detection performance is significantly improved. \n\n\\textbf{Effectiveness of TRS.} We conducted experiments to examine the effect of TRS in VirConv-T. The results are shown in Table~\\ref{tab:ablation_virconv}. With our TRS, detectors show 1.36\\%, 0.25\\%, and 1.32\\% performance improvement under virtual points only, early fusion, and late fusion, respectively. The performance gain is derived from the two-transform and two-stage refinement, which improves the transformation robustness and leads to better detection performance.\n\n\n\n \\textbf{Multi-class performance.}\nWe also trained a multi-class VirConv-T to detect car, pedestrian and cyclist class instances using a single model. \nWe reported the multi-class 3D object detection performance in Table~\\ref{tab:ablation_multiclass}, where the baseline refers to the multi-class Voxel-RCNN~\\cite{Voxel-RCNN}. Compared with the baseline, the detection performance of VirConv-T in all classes has been significantly improved. The results demonstrate that our VirConv can be easily generalized to a multi-class model and boost the detection performance.    \n\n \\textbf{Performance breakdown.}\nTo investigate where our model improves the baseline most, we evaluate the detection performance based on the different distances. The results are shown in Fig.~\\ref{fig:ablation_distance}. Our three detectors have significant improvements for faraway objects because our VirConv models better geometry features of distant sparse objects from the virtual points.\n \n\n \\textbf{Evaluation on nuScenes test set.}\nTo demonstrate the universality of our method, we conducted an experiment on the nuScenes~\\cite{nuScenes} dataset. we compared our method with CenterPoint + VP (virtual point), TransFuison-L + VP and TransFusion. We adopted the same data augmentation strategy as TransFuison-L and trained the network for 30 epochs on 8 Tesla V100 GPUs. The results on the nuScenes test set are shown in Table~\\ref{tab:ablation_nus}. With VirConv, the detection performance of CenterPoint + VP and TransFuison-L + VP has been significantly improved. In addition, the TransFusion-L with VirConv even surpasses the TransFusion in terms of NDS, demonstrating that our model is able to boost the virtual point-based detector significantly. \n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\nThis paper presented a new VirConv operator for virtual-point-based multimodal 3D object detection. VirConv addressed the density and noise problems of virtual points through the newly designed Stochastic Voxel Discard and Noise-Resistant Submanifold Convolution mechanisms. Built upon VirConv, we presented VirConv-L, VirConv-T, and VirConv-S for efficient, accurate, and semi-supervised 3D detection, respectively. Our VirConvNet holds the leading entry on both KITTI car 3D object detection and BEV detection leaderboards, demonstrating the effectiveness of our method. \n\n\\textbf{Acknowledgements }\nThis work was supported in part by the National Natural Science Foundation of China (No.62171393), and the Fundamental Research Funds for the Central Universities (No.20720220064).\n%%%%%%%%% REFERENCES\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egbib}\n}\n\n"
            }
        },
        "equations": {
            "eq:1": "\\begin{align} \n\\hat{X}_i = \\mathcal{R}\\left\\{\\mathcal{K}^{3D} \\left(  X_i, X_i^{(f_{1})}, ..., X_i^{(f_{j})} \\right)\\right\\}, %\\text{ for } i = 1, ..., N,\n\\end{align}",
            "eq:2": "\\begin{align}\n    \\mathbf{\\hat{H}} = \\mathcal{P} \\left(\\mathcal{T}^{-1}\\left(\\mathcal{G}\\left(\\mathbf{H}\\right)\\right)\\right),\n\\end{align}",
            "eq:3": "\\begin{align} \n\\tilde{X}_i = \\mathcal{R}\\left\\{\\mathcal{K}^{2D} \\left (  X_i, \\tilde{X}_i^{(f_{1})}, ..., \\tilde{X}_i^{(f_{k})} \\right) \\right\\}, %\\text{ for } i = 1, ..., N,\n\\end{align}",
            "eq:4": "\\begin{equation}\n   \\mathbf{Y} =\\left[\\left[\\hat{X}_i,\\tilde{X}_i\\right]^T,...,\\left[\\hat{X}_N,\\tilde{X}_N\\right]^T \\right]^T.\n\\end{equation}"
        },
        "git_link": "https://github.com/hailanyi/VirConv"
    }
}