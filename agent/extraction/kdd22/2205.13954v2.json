{
    "meta_info": {
        "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype  Representation",
        "abstract": "With the tremendous expansion of graphs data, node classification shows its\ngreat importance in many real-world applications. Existing graph neural network\nbased methods mainly focus on classifying unlabeled nodes within fixed classes\nwith abundant labeling. However, in many practical scenarios, graph evolves\nwith emergence of new nodes and edges. Novel classes appear incrementally along\nwith few labeling due to its newly emergence or lack of exploration. In this\npaper, we focus on this challenging but practical graph few-shot\nclass-incremental learning (GFSCIL) problem and propose a novel method called\nGeometer. Instead of replacing and retraining the fully connected neural\nnetwork classifer, Geometer predicts the label of a node by finding the nearest\nclass prototype. Prototype is a vector representing a class in the metric\nspace. With the pop-up of novel classes, Geometer learns and adjusts the\nattention-based prototypes by observing the geometric proximity, uniformity and\nseparability. Teacher-student knowledge distillation and biased sampling are\nfurther introduced to mitigate catastrophic forgetting and unbalanced labeling\nproblem respectively. Experimental results on four public datasets demonstrate\nthat Geometer achieves a substantial improvement of 9.46% to 27.60% over\nstate-of-the-art methods.",
        "author": "Bin Lu, Xiaoying Gan, Lina Yang, Weinan Zhang, Luoyi Fu, Xinbing Wang",
        "link": "http://arxiv.org/abs/2205.13954v2",
        "category": [
            "cs.AI"
        ],
        "additionl_info": "Accepted to KDD2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nGraphs data are ubiquitously used to reveal the interactions among various entities, such as academic graphs, social networks, recommendation systems, etc. During the past several years, node classification~\\cite{wang2020gcn,xhonneux2020continuous,DBLP:conf/kdd/WangWGG21,DBLP:conf/kdd/QiuCDZYDWT20,DBLP:conf/aaai/YouGYL21} has received considerable interests and achieved remarkable progress with the rise of graph neural networks (GNNs). In contrast, real-world networks evolve with the emergence of new nodes and edges, thereby generating novel classes. For example, in academic networks, the publication of new research papers produces new interdisciplines; Industrial development brings about new types of commodities in online e-commerce; The addition of new users leads to the emergence of new social groups. Classes of nodes are expanding incrementally and usually accompanied by few labeling due to its newly emergence or lack of exploration.\n\n\n% Graphs are ubiquitously used to reveal the interactions among various entities, such as academic graphs, social networks, recommendation systems, etc. During the past several years, node classification has received considerable interests and achieved remarkable results with the rise of graph neural networks (GNNs). Prevailing approaches~\\cite{DBLP:conf/iclr/KipfW17,DBLP:conf/nips/Huang0RH18} mainly focus on static graphs with a set of fixed classes. The goal is to classify the unlabeled nodes within known labels.\n\n% \\subsubsection{Problem.} In contrast, graphs of real-world networks evolve continuously. New nodes and edges appear and gradually generate novel node categories. For example, the publication of new research papers makes new interdisciplines appear in the academic network; Industrial development brings about new types of commodities in online e-commerce; The addition of new users lead to the emergence of new social groups. Categories (labels) of nodes are expanding incrementally and usually accompanied by few samples of labeled nodes due to its newly emergence.\n\n% However, prevailing approaches~\\cite{DBLP:conf/iclr/KipfW17,DBLP:conf/nips/Huang0RH18} mainly focus on static graphs with a set of fixed classes. The goal is to classify the unlabeled nodes within known labels. It cannot solve the class-incremental node classification with few-shot samples.\nTake a toy academic graph in Figure \\ref{fig:toy_example} for further illustration. Originally, there are abundant labeled nodes for ``Transfer Learning'' and ``Multi-task Learning'' (i.e., \\emph{base classes}). With the knowledge evolution, new nodes appear and introduce additional citation relationships (edges). A new emerging research topic ``Incremental Learning'' (i.e., \\emph{novel class}) has also turned up with few labeled nodes. A critical problem to be solved is to classify the remaining unlabeled nodes into either a \\emph{base class} or a \\emph{novel class}, where \\emph{novel class} only have few labeled samples. We term this kind of node classification among all encountered classes (\\emph{base classes} and \\emph{novel classes} altogether) in dynamic graphs as \\emph{graph few-shot class-incremental learning} (GFSCIL).\n\n\n \n% all encountered classes\n% \\emph{Graph Few-Shot Class-Incremental Learning} for node classification is to classify the remaining unlabeled nodes into either a base class or a novel class.\n\n\n\\textbf{Prior works.} Classical GNN-based methods~\\cite{DBLP:conf/iclr/KipfW17,DBLP:conf/nips/Huang0RH18,DBLP:conf/aaai/BoWSS21} mainly focus on classifying the nodes within a set of fixed classes with abundant labelling. However, \ndue to the \\emph{few-shot} and \\emph{class-incremental} nature, these methods fail to solve GFSCIL problem. \nSome advanced methods aim at addressing part of the problem of GFSCIL.\nOn one hand, to tackle the \\emph{few-shot} node classification problems in graphs, MAML-based studies transfer the knowledge from \\emph{base classes} to never-before-seen classes with only a handful of labeled information. \nWhereas, these methods~\\cite{zhou2019meta,DBLP:conf/nips/HuangZ20,liu2021relative} all make a strong prior \\emph{N-way K-shot} assumption that the unlabeled nodes belong to a fixed set of \\emph{N novel classes}. \nMeanwhile, the classification of base classes and novel classes are separated into two models, \nwhich prevents from judging the results under a unified metric.\n% which cannot distinguish the results of two models under a unified metric.\n% which cannot satisfy the class-incremental characteristic. \nOn the other hand, although \\emph{class-incremental} learning has achieved significant progress in computer vision tasks~\\cite{li2017learning,rebuffi2017icarl,hou2019learning}, class-incremental node classification in graphs has not been fully explored. Existing methods are dedicated to independent and identically distributed data (e.g., images), which has no explicit interations.\nGraph data lies in non-Euclidean space and the network structure evolves dynamically.\nThe emergence of new edges changes and complicates the node correlations, thus bringing more challenges.\n% It is more challenging to achieve class-incremental learning on graphs\n% than on independent and identically distributed data (e.g., images).\n\n% In GFSCIL, the emergence of novel classes has few-shot labeling, resulting in a significant amount of noise or outliers. Directly grafting existing method makes the representation of nodes and classes become vulnerable.\n\n% Yet it is even more challenging to achieve few-shot learning on graphs than on independent and identically distributed data (e.g., images) which exisiting few shot learning algorithms focus on. \n\n% The emergence of new nodes and edges make the network structure evolves dynamically.\n% Directly grafting existing method makes the representation of nodes and classes become vulnerable due to the few-shot labeling of novel classes.\n\n% However, compared with images, graph data lies in non-Euclidean space and the network structure evolves dynamically. Directly grafting existing method makes the node representations less expressive. \n% However, graph data lies in non-Euclidean space and the network structure is evolving. Directly grafting existing method is infeasible to capture the dynamic structure feature and the node representations are less expressive. To the best of our knowledge, we are the first work to study the \\emph{graph few-shot class-incremental learning} (GFSCIL) problem.\n\n% Besides, dynamic graph representation~\\cite{kazemi2020representation} learns the time-independent embedding for the changes of structure and features. On the contray, the increase of node classes are not considered. Incremental\n\n\\textbf{Challenges.} A naive approach for GFSCIL is to finetune the base model on both \\emph{base classes} and \\emph{novel classes}. However, there are three main challenges that need to be addressed:\n(1) \\emph{How to find a way out of ``forgetting old''?} Catastrophic forgetting phenomenon~\\cite{DBLP:journals/corr/GoodfellowMDCB13,kirkpatrick2017overcoming,DBLP:conf/icml/YapRB21} describes the performance degradation on old classes when incrementally learning novel classes. \nIn GFSCIL, the growing number of novel classes makes the model suffer from forgetting base classes.\n(2) \\emph{How to overcome the unbalanced labeling between base classes and novel classes?} In GFSCIL, the labeling between large-scale base classes and few-shot novel classes is unbalanced. Directly training on few-shot samples may cause over-fitting problem. \n(3) \\emph{How do we capture the dynamic structure as the network evolves?} The structure of graphs are highly dynamic in GFSCIL. The arrival of new nodes and edges make more complex connections, which is a big challenge for expressive node representations. \n\n% simultaneously affecting the expressive representations of novel nodes as well as the origin nodes. \n\n% Simultaneously, the spatial correlations of original nodes change, thereby affecting the representation of nodes.\n\n% \\emph{\\underline{\\textbf{G}}raph f\\underline{\\textbf{E}}w-sh\\underline{\\textbf{O}}t class-incre\\underline{\\textbf{M}}ental l\\underline{\\textbf{E}}arning via pro\\underline{\\textbf{T}}otyp\\underline{\\textbf{E}} \\underline{\\textbf{R}}epresentation}\n\n\\textbf{Our Work.} To address the aforementioned problems, we leverage the concept of metric learning and propose a new method for \n\\underline{\\textbf{G}}raph f\\underline{\\textbf{E}}w-Sh\\underline{\\textbf{O}}t Class-Incre\\underline{\\textbf{M}}ental L\\underline{\\textbf{E}}arning via Pro\\underline{\\textbf{T}}otyp\\underline{\\textbf{E}} \\underline{\\textbf{R}}epresentation, named \\textsc{Geometer}. \nInstead of replacing and retraining the fully connected neural network classifer, \\textsc{Geometer} predicts the ever-expanding class of a node by finding the nearest prototype representation. Prototype is a vector representing a class in the metric space. \nWe propose class-level multi-head attention to learn the dynamic prototype representation of each class. When novel classes popping up, \\textsc{Geometer} learns and adjusts the representation based on the geometric relationships of intra-class proximity, inter-class uniformity and inter-class separability in the metric space. \nIn order to avoid \\emph{forgetting old}, \\textsc{Geometer} iteratively takes the previous model as the teacher, and guides the student model's representation of old classes with knowledge distillation. \\textsc{Geometer} adopts pretrain-finetune paradigm with well-designed biased sampling strategy to further alleviate the impact of unbalanced labeling. \n% Instead of replacing and retraining the classifier, we predict the label of a node by finding the nearest class prototype in a metric space. \n% To overcome unbalanced labeling, \n% \\textsc{Geometer} adopts the episode learning with biased sampling strategy in pretraining-finetuning paradigm. \n% As the network dynamically evovles, update of the metric space (both node embeddings and prototypes) to reflect this variation is desired. We propose node-level and class-level multi-head attention mechanism to learn the dynamic prototype representation of each class. \n% With the novel classes pop up, \n% \\textsc{Geometer} proposes geometric loss based on the intra-class proximity, inter-class uniformity and inter-class separability of metric space to adjust the representation of nodes and classes. In order to avoid \\emph{forgetting old}, \\textsc{Geometer} iteratively takes the model before the streaming arrival as the teacher model, and guides the student model (new model) by leveraging the knowledge distillation.\n\n% 还没写完\n\n% to mitigate forgetting, \\textsc{Geometer} utilizes knowledge distillation technique to maintain the network’s output logits corresponding to old classes.\n\n% in this paper, we address \\emph{\\underline{G}raph few-shot class-incremental learning} framework via \\emph{\\underline{P}rototype \\underline{R}epresentation} (\\textsc{Geometer}) from the feature geometric space perspective.\n\n% 无需引入新的参数\n% for node classification with GFSCIL setting\n\nTo summarize, the main contributions of our works are as follows:\n\\begin{itemize}[left=1em]\n    \\item We investigate a novel problem for node classification: \\emph{graph few-shot class-incremental learning} (GFSCIL). To the best of our knowledge, this is the first work to study this challenging yet practical problem.\n    \\item We propose a novel model \\textsc{Geometer} to solve GFSCIL problem. With the novel classes popping up, \\textsc{Geometer} learns and adjusts the attention-based prototypes based on the geometric relationships of proximity, uniformity and separability of representations.\n    \\item \\textsc{Geometer} proposes teacher-student knowledge distillation and biased sampling strategy to further mitigate the catastrophic forgetting and unbalanced labeling in GFSCIL.\n\\end{itemize}\nWe conduct extensive experiments on four real-world node classification datasets to corroborate the effectiveness of our approach. \\textsc{Geometer} achieves a substantial improvement of nearly 9.46\\% to 27.60\\% in multiple sessions of GFSCIL over state-of-the-art baselines.\n\n    \n% to classify the ever-expanding classes by finding the nearest prototype in metric space. \\textsc{Geometer} learns and adjusts the representation based on geometric loss with clear interpretations and teacher-student knowledge distillation.\n\n% We propose geometric loss based on clear interpretations in metric space to adjust the dynamic prototype representations for ever-expanding classes. To mitigate forgetting, teacher-student knowledge distillation is utilized for transfering knowledge on classifying old classes.\n% \\item \\textsc{Geometer} proposes attention-based approach for dynamic prototype representation and adopt episode learning paradigm with biased sampling strategy to mimic different situations in pretraining and finetuning process.\n\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n% 可能有几篇文章我已经引用了，就直接用对应的bibtex就好~ @lina\nIn this section, we briefly introduce the relevant research lines of our work, namely few-shot node classification and class-incremental learning.\n\n",
                "subsection 2.1": {
                    "name": "Few-Shot Node Classification",
                    "content": "\n\nIn recent years, few-shot node classification on graph has attracted increasing attention. These works can be categorized into two types: (1) optimization based approaches, and (2) metric based approaches. Optimization-based approaches leverage MAML~\\cite{finn2017model} to learn a better GNN initialization on base classes, and quickly adapt to novel classes with few-shot samples. Meta-GNN~\\cite{zhou2019meta} firstly incorporates the meta-learning paradigm into GNNs for few-shot node classification. G-Meta~\\cite{DBLP:conf/nips/HuangZ20} proposes to use local subgraphs to learn the transferable knowledge across tasks. Liu et al.~\\cite{liu2021relative} \nfurther design the relative and absolute embedding of nodes and achieves promising performance.\nHowever, these method divide the classification of novel classes and old classes into two seperate models, which cannot carry out a unified classification of the unknown nodes in GFSCIL.\nMetric-based methods propose to learn a transferable metric space, which is closely related to our work. Ding et al. propose GPN~\\cite{ding2020graph} for few-shot learning on attributed graphs by combining prototype network with GNNs. Yao et al.~\\cite{yao2020graph} incorporate prior knowledge learned from auxiliary graphs to further transfer the knowledge to a new target graph.\nWhereas, the growing of new labels in GFSCIL will make the prototypes overlapping, and the accuracy of the classification will decline sharply.\n\n% growing new labels will make the prototypes overlapping, and the accuracy of classification sharp decline.\n\n% However, they typically think that there are only novel nodes at the test stage, which may include both base and novel classes in real-world.\n\n% ~\\cite{zhou2019meta,yao2020graph,ding2020graph,liu2021relative} to tackle the few-shot node classification problem. Specifically, Meta-GNN~\\cite{zhou2019meta} leverages meta-learning mechanism to learn better parameter initialization of GNNs. GFL~\\cite{yao2020graph} learns a transferable metric space to improve the accuracy of node classificaion. GPN~\\cite{ding2020graph} learns representative prototypes for classes in a metric space. However, they think that there are only novel nodes at the test stage.\n\n"
                },
                "subsection 2.2": {
                    "name": "Class-Incremental Learning",
                    "content": "\n\nClass-incremental learning aims to learn a unified classifier \nto recognize the ever-expanding classes over time, which is extensively studied in the field of computer vision~\\cite{li2017learning,rebuffi2017icarl,castro2018end,hou2019learning}. \niCaRL~\\cite{rebuffi2017icarl} adopts an ``episodic memory'' of class exemplars and incrementally learns the nearest-neighbor classifier for novel classes. \nCastro et al.~\\cite{castro2018end} propose a distillation measure to retain the knowledge of old classes, and combines it with the cross-entropy loss for end-to-end training. \nHou et al.~\\cite{hou2019learning} propose the multi-class incremental setting and raises the imbalance challenge between old classes and novel classes.\nHowever, among these works, the training samples of novel classes are all large-scale. In many real-world scenarios, the novel classes often lack of labeling due to its newly emergence or lack of exploration. Recently, the FSCIL problem has just been put forward in image classification~\\cite{tao2020few,cheraghian2021semantic}. Tao et al.~\\cite{tao2020few} firstly propose this FSCIL problem and utilize a neural gas (NG) network to learn and maintain the topology of the feature manifold of various classes. Cheraghian et al.~\\cite{cheraghian2021semantic} further introduce a distillation algorithm with semantic information.\nExcept in the field of computer vision, few-shot class-incremental learning also shows practical significance in graphs and remains an under-explored problem. To the best of our knowledge, this is the first study of FSCIL for node classification in graphs, which we denoted as GFSCIL.\n\n% incrementally to recognize all the classes which appear over time. It is widely studied in the field of computer vision. Early works on CIL mainly use knowledge distillation~\\cite{rebuffi2017icarl,castro2018end,hou2019learning} to solve the problem of catastrophic forgetting. In these works, the incremental classes have a large number of data samples, while few-shot class-incremental learning is a more difficult but more practical task. Recently, the FSCIL problem has just been put forward in the field of CV~\\cite{tao2020few,dong2021few,cheraghian2021semantic}, and it remains an under-studied problem in graph so that we are the first to propose it.\n\n% For example, iCaRL~\\cite{rebuffi2017icarl} maintains an “episodic memory” to avoid forgetting and incrementally adapts the nearest-neighbor classifier for the novel classes. Because there is an imbalance between old and novel classes, the prediction results bias towards novel classes. To eliminate the bias, LUCIR~\\cite{hou2019learning} adopts a cosine distance metric. Recently, some works~\\cite{tao2020few,dong2021few,cheraghian2021semantic} focus on the more difficult FSCIL problem, where each novel class has few labeled nodes.\n \n"
                }
            },
            "section 3": {
                "name": "Problem Statement",
                "content": "\n\nIn this section, we provide problem statement and definitions. In the base stage, we have an initial graph $\\mathcal{G}^{base}$. In streaming sessions, suppose we have $T$ snapshots of evolving graph, denoting as $\\mathcal{G}^{stream} = \\{\\mathcal{G}^1, \\cdots, \\mathcal{G}^T\\}$. Take the $t$-th session as an example, its corresponding graph represents as $\\mathcal{G}^t = (\\mathcal{V}^t, \\mathcal{E}^t, \\mathbf{X}^t)$. Suppose we have $N_t$ nodes and $M_t$ edges. $\\mathcal{V}^t$ is the node set $\\{v_1, v_2, \\cdots, v_{N_t}\\}$, and $\\mathcal{E}^t$ is the edge set $\\{e_1, e_2, \\cdots, e_{M_t}\\}$. \nThe feature vector of node $v_i$ is represented as $\\bm{x}_{i} \\in \\mathbb{R}^{d}$, and $\\mathbf{X}^t = \\{\\bm{x}_1, \\cdots, \\bm{x}_{N_t}\\} \\in \\mathbb{R}^{{N_t}\\times d}$ denotes all the node features. We denote $\\{\\mathcal{C}^{base}, \\mathcal{C}^{1}, \\cdots, \\mathcal{C}^{T}\\}$ as sets of classes from base stage to the $T$-th streaming session. $\\mathcal{C}^{base}$ is the set of base classes with large training samples. In $t$-th streaming session, $\\Delta \\mathcal{C}^t$ novel classes are introduced with few-shot samples, where $\\forall i,j, \\Delta \\mathcal{C}^{i} \\cap \\Delta \\mathcal{C}^{j} = \\varnothing$ and $\\mathcal{C}^{i} = \\mathcal{C}^{i-1} + \\Delta \\mathcal{C}^{i}$.\nWe denote the totally encountered class in $t$-th session as $\\mathcal{C}^{t} = \\mathcal{C}^{base} + \\sum_{i=1}^{t} \\Delta \\mathcal{C}^i$.\n\n\n\\begin{myProb}\n\\textbf{Graph Few-Shot Class-Incremental Learning} In $t$-th streaming session, we denote $\\Delta \\mathcal{C}^t$ novel classes with $K$ labeled nodes as the $\\Delta \\mathcal{C}^t$-way $K$-shot GFSCIL problem. \nThe labeled training samples are denoted as support sets $\\mathcal{S}$. \nAnother batch of nodes to predict their corresponding label are denoted as query sets $\\mathcal{Q}$.\nAfter training on the support sets $\\mathcal{S}$ of $t$-th session, the GFSCIL problem is tested to classify unlabel nodes of query sets $\\mathcal{Q}$ into all encountered classes $\\mathcal{C}^{t}$.\n\\end{myProb}\n\n\\begin{myDef}\n\\textbf{Prototype Representation} A prototype representation is a representative embedding of one class. The node embeddings of one class tend to cluster around its prototype representation in the same metric space. Prototype representation is first proposed in {\\rm{~\\cite{snell2017prototypical}}}, which regards the mean of its support set as class’s prototypes.\n\\end{myDef}\n\n\n\n\n"
            },
            "section 4": {
                "name": "Methodology",
                "content": "\n% 整体介绍各个模块\n% Figure \\ref{fig:system_model}(a) shows the problem setting of GFSCIL. With the arrival of nodes, the network structure has become more complex and novel node classes have been introduced. Our task is to classify all the encountered classes within base classes with large-scale samples and novel classes with few-shot samples. \\textsc{Geometer} follows an \\emph{episode meta learning} paradigm, and Figure \\ref{fig:system_model}(b) and Figure \\ref{fig:system_model}(c) show the learning process with biased sampling strategy at base stage and streaming sessions.\n\nWe first give an overview of the proposed \\textsc{Geometer}, as illustrated in Figure \\ref{fig:system_model}.\n\\textsc{Geometer} intends to predict the node class by finding the nearest \\emph{attention-based prototype representation}. When novel classes emerging, we learn and adjust prototypes based on \\emph{geometric metric learning} and \\emph{teacher-student knowledge distillation}.\nOur approach follows the \\emph{episode meta learning} process, and different biased sampling are designed to overcome the unbalanced labeling among base classes and novel classes.\n\n% a series of \\emph{geometric metric learning} strategies are defined with clear interpretation of prototypes relationship in metric space. In order to avoid ``forgetting old\" problem, \\emph{teacher-student knowledge distillation} technique is utilized to migrate the knowledge of classifying old classes. \n\n% Finally, the episode meta learning method with unbiased sampling are given in Section 4.4.\n\n\n% The biased sampling strategy in training are utilized to generate class-imbalanced support sets by mimicking the circumstances encountered during fine-tuning. In streaming sessions, class-imbalanced query sets are sampled to calculate the loss function.\n\n\n% 数学建模方式，G 流式 小样本n-way k-shot\n\n",
                "subsection 4.1": {
                    "name": "Attention-based Prototype Representation",
                    "content": "\n\nThe evolution of the network makes the influence of nodes unequal and non-static.\nIn addition, weakly-labeled few-shot data usually contains a significant amount of noise.\nTherefore, direct average of support node features cannot be fully representative and is highly vulnerable to the noise or outliers. In order to learn the expressive prototype representation of each class, we propose a two-level attention-based prototype representation learning method as shown in Figure \\ref{fig:proto_repre}. \n\n% Compared with directly averaging of support node features, evolution of the network makes the influence of nodes unequal and non-static. In addition, weakly-labeled few-shot data usually contain a significant amount of noise, average of equal importance are highly vulnerable to the noise or outliers.\n\n% On one hand, \\emph{node-level graph attention network} ~\\cite{DBLP:conf/iclr/VelickovicCCRLB18} is utilized to extract the dynamic node features due to the insertion of new nodes and edges. On the other hand, we design the \\emph{class-level multi-head attention} to compute the prototype representation with the labeled nodes from the support set. \n\n",
                    "subsubsection 4.1.1": {
                        "name": "Node-level Graph Attention Network",
                        "content": " Graph neural network is typically expressed as a message-passing process in which information can be passed from one node to another along edges directly. \\emph{Node-level graph attention network} $f_{\\mathcal{G}}(\\cdot)$ computes a learned edge weight by performing masked attention mechanism~\\cite{DBLP:conf/iclr/VelickovicCCRLB18}. The attention score $\\alpha_{i j}$ between node $v_i$ and $v_j$ is normalized across all node $v_i$'s neighbors $\\mathcal{N}_i$ as \n\\begin{equation}\n    \\alpha_{i j}=\\frac{\\exp \\left(\\operatorname{LeakyReLU}\\left({\\mathbf{a}}^{T}\\left[\\mathbf{W} \\bm{h}^{l}_{i} \\| \\mathbf{W} \\bm{h}^{l}_{j}\\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(\\operatorname{LeakyReLU}\\left(\\mathbf{a}^{T}\\left[\\mathbf{W} \\bm{h}^{l}_{i} \\| \\mathbf{W} \\bm{h}^{l}_{k}\\right]\\right)\\right)},\n\\end{equation}\nwhere $\\bm{h}^{l}_{i}$ and $\\bm{h}^{l}_{j}$ represent the node features of $l$-th GNN layer,\n$\\mathbf{a} \\in \\mathbb{R}^{2d^{\\prime}}$ and $\\mathbf{W} \\in \\mathbb{R}^{d^{\\prime} \\times d}$ are weight matrices. $\\|$ denotes vector concatenation.\nThen, graph attention network computes a weighted average of the transformed features of the neighbor nodes as the new representation, followed by a nonlinear function $\\sigma$. The $(l+1)$-th layer hidden state of node $v_{i}$ is calculated via\n\\begin{equation}\n    \\bm{h}_{i}^{l+1}=\\sigma(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j} \\cdot \\mathbf{W} \\bm{h}_{j}^{l}).\n\\end{equation}\n\nWe denote the $L$-th layer hidden state output of node $v_i$ as $f_{\\mathcal{G}}(\\bm{x}_i) = \\bm{h}_{i}^{L}$.\nAs usual, we build a 2-layer graph attention network for feature extraction.\n\n% A scoring function $e: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ computes a dynamic edge weight indicating the importance of the feature between the neighbor $v_j$ to the node $v_i$:\n\n% \\begin{equation}\n%     e(i,j) = \\text{LeakyReLU}(\\bm{a}^T \\cdot [\\bm{W}\\bm{h}_{i} \\| \\bm{W}\\bm{h}_{j}]),\n% \\end{equation}\n\n% where $a \\in \\mathbb{R}^{2d^{\\prime}}$ and $\\mathbb{W} \\in \\mathbb{R}^{d^{\\prime} \\times d}$ are weight matrix. $\\|$ denotes vector concatenation. After that, the attention score is normalized across all neighbors of $j \\in \\mathcal{N}_i$ using the softmax function:\n\n% \\begin{equation}\n%     \\alpha_{i j}=\\text{softmax}\\left(e\\left(\\bm{h}_{i}, \\bm{h}_{j}\\right)\\right)=\\frac{\\exp \\left(e\\left(\\bm{h}_{i}, \\bm{h}_{j}\\right)\\right)}{\\sum_{j^{\\prime} \\in \\mathcal{N}_{i}} \\exp \\left(e\\left(\\bm{h}_{i}, \\bm{h}_{j^{\\prime}}\\right)\\right)}.\n% \\end{equation}\n\n% A prototype $p$ is a representation in the same metric space with nodes embeddings, which is usually formed by average of the support nodes embeddings.\n% A prediction of class is made for a query node based on its proximity to one prototype over the metric space. Due to the dynamic structure and stochastic label noise, \\textsc{Geometer} propose \\emph{class-level multi-head attention} to learn the prototype as follows.\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Class-level Multi-head Attention",
                        "content": "\nDue to the dynamic structure and stochastic label noise, \\textsc{Geometer} proposes \\emph{class-level multi-head attention} to learn the class prototype as follows. In streaming fashion of GFSCIL, \nthe importance of nodes changes dynamically as the network evolves.\nThe degree centrality is one of simplest way to measure the importance of nodes. \nThe large-degree nodes are often referred to as hubs, which has a stronger influence in the networks.\nTherefore, an initial prototype $\\bm{\\hat{p}}_{i}$ of class $i$ is calculated by degree-based weighted-sum of support node embeddings:\n\n\\begin{equation}\n    \\bm{\\hat{p}}_{i} = \\sum_{j \\in \\mathcal{S}_i} \\frac{\\text{degree}(v_j)}{\\sum_{j^{\\prime} \\in \\mathcal{S}_i} \\text{degree}(v_{j^{\\prime}})} \\cdot f_{\\mathcal{G}}(\\bm{x}_{j}),\n\\end{equation}\nwhere $\\mathcal{S}_{i}$ is the support set of class $i$, $f_\\mathcal{G}(\\bm{x}_j)$ is the node representation of $v_j$ obtained by \\emph{node-level graph attention network}, $\\text{degree}(v_j)$ is the degree centrality of node $v_j$. Apart from considering the structural information, i.e. degree centrality, different support node features plays an important role in learning a representative class prototype.\nIn order to fully characterize the relationship between node features and prototypes, \\emph{class-level multi-head attention} calculates the attention score between the initial prototype and support node representations to obtain an expressive prototype representation. To be specific, we take the linear transformation of initial prototype $\\bm{\\hat{p}}_{i}$ as the query $\\textbf{Q}$ and then concatenate the initial prototype and support node representations as $\\bm{h}_{i}^{spt}$:\n\\begin{equation}\n    \\bm{h}_{i}^{spt} = \\textsc{Concatenate}(\\bm{\\hat{p}}_i, \\|_{j\\in \\mathcal{N}_i} f_{\\mathcal{G}}(\\bm{x}_j)).\n\\end{equation}\nWe take the linear transformation of $\\bm{h}_{i}^{spt}$  as the key $\\textbf{K}$ and value $\\textbf{V}$.\n\\textsc{Geometer} adopts the scaled dot-product attention~\\cite{vaswani2017attention}, which is calculated via\n\\begin{equation}\n    \\textsc{Attention}(\\bm{Q},\\bm{K},\\bm{V}) = \\text{softmax}(\\frac{\\bm{Q}\\bm{K}^{T}}{\\sqrt{d_k}})\\bm{V}, \n\\end{equation}\nwhere $\\bm{Q} = \\mathbf{W}_Q \\bm{\\hat{p}}_{i}$, $\\bm{K} = \\mathbf{W}_K \\bm{h}_{i}^{spt}$, $\\bm{V} = \\mathbf{W}_V \\bm{h}_{i}^{spt}$, $\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V$ are three weight matrices, and $d_k$ is the dimension of $\\bm{Q}$ and $\\bm{K}$.\nFinally, the residual connection is adopted to obtain the final prototype representation $\\bm{p}_i$ of class $i$: \n\\begin{equation}\n    \\bm{p}_i = \\bm{\\hat{p}}_i + \\textsc{Attention}(\\bm{\\hat{p}}_i, \\bm{h}_{i}^{spt}, \\bm{h}_{i}^{spt}).\n\\end{equation}\n\n\n% the initial prototype is used as query vector to calculate the attention mechanism~\\cite{vaswani2017attention} with each node representation. Finally, the residual connection is adopted to \n% obtain the final prototype representation $\\bm{p}_i$ of class $i$. \n\n% % \\begin{equation}\n% %     p_i = p_i^0 + \\textsc{Attention}(p_i^0, [p_i^0, \\| _{j\\in \\mathcal{N}_i} f_{\\mathcal{G}}(\\bm{x}_j)),\n% % \\end{equation}\n\n% \\begin{equation}\n%     \\bm{p}_i = \\bm{p}_i^0 + \\textsc{Attention}(\\bm{p}_i^0, \\left[ \\bm{p}_i^0, \\|_{j\\in \\mathcal{N}_i} f_{\\mathcal{G}}(\\bm{x}_j^)\\right]),\n% \\end{equation}\n\n% where $\\bm{p}_i^0$ is the query $\\bm{Q}$, and the concatenation of initial prototype and support node embeddings act as the key $\\bm{K}$ and value $\\bm{V}$ in following vanilla attention mechansim.\n% \\begin{equation}\n%     \\textsc{Attention}(\\bm{Q},\\bm{K},\\bm{V}) = \\text{softmax}(\\frac{\\bm{Q}\\bm{K}^{T}}{\\sqrt{d_k}})\\bm{V}.\n% \\end{equation}\n\n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Geometric Metric Learning",
                    "content": "\n\nIn GFSCIL problem, as the graph evolves, novel node classes obtain new prototypes in the metric space. With the increase of growing classes, the performance of node classification is greatly reduced due to the overlapping of prototype representations.\nAs a consequence of few-shot samples of novel classes, parameter update only based on node classification results is prone to overfitting novel classes, \n% \\weinan{this term is confusing to me. what new? node or class?}, \nor is greatly affected by the support set sample distribution. \nTherefore, we propose to learn the prototype representation from geometric relationships. As shown in Figure \\ref{fig:system_model}(c), we propose geometric loss functions from three aspects: intra-class proximity, inter-class uniformity and inter-class separability. \n\n",
                    "subsubsection 4.2.1": {
                        "name": "Intra-Class Proximity",
                        "content": "\nIntra-class proximity indicates that the nodes of same classes should be closely clustered.\nTherefore, in the metric space, the distance between the node embedding and its corresponding class prototype representation should be relatively close.\nWe use squared Euclidean distance $d(\\cdot)$ to measure the distance between node features and class prototype, and define the intra-class proximity loss $\\mathcal{L}_{P}$ as follows:\n\\begin{equation}\n    \\mathcal{L}_{P}=\\sum_{k=1}^{\\| \\mathcal{C}^{k}\\|}  \\frac{\\alpha_{k}}{n_{k}} \\sum_{i=1}^{n_{k}}-\\log \\frac{\\exp \\left(-d\\left(f_{\\mathcal{G}}\\left(\\bm{x}_{i}\\right), \\bm{p}_{k}\\right)\\right)}{\\sum_{k^{\\prime}\\in \\mathcal{C}^{k}} \\exp \\left(-d\\left(f_{\\mathcal{G}}\\left(\\bm{x}_{i}\\right), \\bm{p}_{k^{\\prime}}\\right)\\right)},\n\\end{equation}\nwhere $\\| \\mathcal{C}^{k}\\|$ is the total number of encountered classes up to $k$-th streaming session, $n_k$ is the number of node samples of class $k$. $\\alpha_k \\in \\left[0,1\\right]$ is a weighting factor, which is used to adjust the impact of unbalanced labeling of base classes and novel classes in total loss function.\n    \n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "Inter-Class Uniformity",
                        "content": "\n\nInter-class uniformity describes the positional uniformity of different prototypes in metric space. Specifically, a prototype center $p_c$ is denoted by the mean of all prototypes:\n\\begin{equation}\n    \\bm{p}_{c}=\\frac{1}{\\left\\|\\mathcal{C}^{k}\\right\\|} \\sum_{i=1}^{\\left\\|\\mathcal{C}^{k}\\right\\|} \\bm{p}_{i},\n\\end{equation}\nGeometrically, taking the prototype center $p_c$ as the coordinate origin, each normalized prototype relative to the center $\\frac{\\bm{p}_{j}-\\bm{p}_{c}}{\\left\\|\\bm{p}_{j}-\\bm{p}_{c}\\right\\|}, \\forall j$\nis distributed on a unit sphere.\nAs the prototypes of the novel classes are increasingly projected onto the unit sphere, we propose that the distribution of prototypes should tend to be uniform. \nThe distribution of class prototypes are adjusted based on the division of sphere angle. \nTherefore, we define the following inter-class uniformity loss function $\\mathcal{L}_{U}$ based on cosine similarity distance as\n\\begin{equation}\n    \\mathcal{L}_{U}=\\frac{1}{\\left\\|\\mathcal{C}^{k}\\right\\|} \\sum_{i=1}^{\\left\\|\\mathcal{C}^{k}\\right\\|} \\left\\{ 1+\\max _{j \\in \\{\\mathcal{C}^k\\} \\backslash i}\\left[\\frac{\\left(\\bm{p}_{i}-\\bm{p}_{c}\\right)}{\\left\\|\\bm{p}_{i}-\\bm{p}_{c}\\right\\|} \\cdot \\frac{(\\bm{p}_{j}-\\bm{p}_{c})}{\\|\\bm{p}_{j}-\\bm{p}_{c}\\|}\\right] \\right\\},\n\\end{equation}\nwhere 1 is used as a bias to ensure that the value is always non-negative, and the purpose of taking the maximum value of cosine similarity here is to focus on the angular distribution of adjacent prototypes. By defining the inter-class uniformity, the unbalanced labeling class prototypes are inclined to be evenly distributed on the sphere. Especially for novel classes with few-shot labeled nodes, the inter-class geometric relations provide important guidance for the learning of prototype representations.\n\n"
                    },
                    "subsubsection 4.2.3": {
                        "name": "Inter-Class Separability",
                        "content": "\nBefore finetuning, the feature extractor is more suitable for old classes representation. The prototypes of novel classes are likely to overlap with the old class prototypes, which greatly affects the accuracy of node classification. \nTherefore, we propose inter-class separability in geometric metric learning, which describes that the prototypes of novel classes and old classes should keep a distance in the metric space. The inter-class separability loss $\\mathcal{L}_{S}$ is denoted as\n\\begin{equation}\n    \\mathcal{L}_{S}=\\frac{1}{\\Delta \\mathcal{C}^{k}} \\sum_{i \\in \\Delta \\mathcal{C}^{k}} \\min _{j \\in \\mathcal{C}^{k-1}} \\exp \\left(-d\\left(\\bm{p}_{i}, \\bm{p}_{j}\\right)\\right),\n\\end{equation}\nwhere $d(\\cdot)$ is the squared euclidean distance, $\\mathcal{C}^{k-1}$ is the set of labels of $(k-1)$-th session, and $\\Delta \\mathcal{C}^{k}$ is the novel classes of $k$-th\nsession. The definition of inter-class separability is a supplement to inter-class uniformity. With the addition of novel classes, in order to avoid the error diffusion caused by the inaccurate classification of old categories, expanding the distance between old and novel class prototypes in metric space helps to further enhance the classification accuracy in GFSCIL settings.\n\n"
                    }
                },
                "subsection 4.3": {
                    "name": "Teacher-Student Knowledge Distillation",
                    "content": "\nDue to the class-incremental nature of GFSCIL problem, \\textsc{Geometer} applies the idea of teacher-student knowledge distillation to further mitigate ``forgetting old'' during finetuning.\nIn contrast to the classic teacher-student knowledge distillation techniques~\\cite{hinton2015distilling,10.1145/3447548.3467319,10.1145/3394486.3403234} used to compress large model into lightweight model with better inference efficiency, we regard the model before streaming as the teacher model and new model as the student model. \nKnowledge distillation technique is used to transfer the classification ability of previous model, while preserving the interrelationships of the old classes in the metric space.\nTemperature-scaled softmax~\\cite{li2017learning} is utilized to soften the old classes logits of teacher model and student model. The modified logits $y^{\\prime(i)}$ of class $i$ by applying a temperature scaling function in the softmax are calculated as\n\\begin{equation}\n    y^{\\prime(i)} = \\frac{\\exp \\left(d\\left(f_{\\mathcal{G}}\\left(\\bm{x}^{(i)}\\right), \\bm{p}_{i}\\right) / \\tau\\right)}{\\sum_{j} \\exp \\left(d\\left(f_{\\mathcal{G}}\\left(x^{(i)}\\right), \\bm{p}_{j}\\right) / \\tau\\right)},\n\\end{equation}\nwhere $\\tau$ is the temperature factor. Generally, we set $\\tau > 1$ to increase the weight of smaller logit values and encourages the network to better reveal inter-class relationships learned by the teacher model. \\textsc{Geometer} proposes to calculate the KL-divergence of the softened logits to make the student model gain the experience of classifying old classes $\\mathcal{C}^{t-1}$ from teacher model. The \nteacher-student knowledge distillation loss $\\mathcal{L}_{K D}$ on $k$-th session is calculated as\n\\begin{equation}\n    \\mathcal{L}_{K D}=\\frac{1}{\\left\\|\\mathcal{C}^{k-1}\\right\\|} \\sum_{i=1}^{\\left\\| \\mathcal{C}^{k-1}\\right\\|} y_{S}^{\\prime(i)} \\cdot \\log \\left(\\frac{y_{S}^{\\prime(i)}}{y_{T}^{\\prime(i)}}\\right),\n\\end{equation}\nwhere $\\mathcal{C}^{k-1}$ is the set of old classes of $(k-1)$-th streaming session and $y_{S}^{\\prime(i)}$ and $y_{T}^{\\prime(i)}$ are the modified logits. \\textsc{Geometer} proposes the teacher-student knowledge distillation to avoid the catastrophic forgetting caused by the growing addition of novel classes. Meanwhile, it makes \\textsc{Geometer} a system of checks and balances related to geometric losses.\n\n% The logits are softened by applying a \"temperature\" scaling function in the softmax, effectively smoothing out the probability distribution and revealing inter-class relationships learned by the teacher.\n\n% \\begin{equation}\n%     \\mathcal{L}_{KD}=\\frac{1}{\\left\\|\\mathcal{C}^{k}\\right\\|} \\sum_{i=1}^{\\left\\|\\mathcal{C}^{k}\\right\\|} \\mathcal{Y}_{\\text{Stu}}^{\\mathcal{C}^{k}} \\cdot \\log (\\frac{\\mathcal{Y}_{\\text{Stu}}^{C^{k}}}{\\mathcal{Y}_{\\text{Tch}}^{\\mathcal{C}^{k}}})\n% \\end{equation}\n\n% \\begin{equation}\n%     \\mathcal{Y}_{\\text{S}}^{C^{k}} = \n% \\end{equation}\n\n"
                },
                "subsection 4.4": {
                    "name": "Episode Meta Learning",
                    "content": "\n\nIn this section, we discuss the learning process of \\textsc{Geometer}. We adopt the episode paradigm in learning process, which has shown great promise in few-shot learning. Instead of directly training or finetuning on batches of data, a set of\ntasks $\\mathcal{T}$ are generated by imitating the \\emph{N-way-K-shot} few-shot scenario.\nEach task $\\mathcal{T}_i \\in \\mathcal{T}$ includes support set $\\mathcal{S}_i$ and query set $\\mathcal{Q}_i$.\nIn GFSCIL setting, two different \\emph{bias sampling strategies} are adopted in both pretraining and finetuning stages. \n\nIn the base stage, all base classes have a large number of training samples. However, in streaming sessions, the novel classes follows few-shot labeling. \\textsc{Geometer} adopts biased sampling strategy in pretraining stage \nto generate class-imbalanced support sets by mimicking the circumstances encountered during finetuning. Specifically, the number of sampling number of each class in support set $\\mathcal{S}$ follow a uniform distribution $\\textbf{U}[1, K_{max}]$. The size of query set still maintains a fixed number $K_{qry}$. \nDuring meta training, intra-class proximity and inter-class uniformity loss are utilized. Therefore, the loss function $\\mathcal{L}_{\\text{train}}$ is as follows, with hyper-parameters $\\lambda_{P}$ and $\\lambda_{U}$:\n\\begin{equation}\n    \\mathcal{L}_{\\text{train}} = \\lambda_{P}\\mathcal{L}_{P} + \\lambda_{U}\\mathcal{L}_{U}.\n\\end{equation}\n\nIn the streaming sessions, the biased sampling adopts different strategies to obtain the class-imbalanced query set $\\mathcal{Q}$. In order to avoid ``forgetting old\" and ``overfitting new\" problem, a higher proportion of the old samples will be sampled when sampling the query set. \nThis helps to fully retain the classification accuracy on old classes during meta fine-tuning. During meta-finetuning, three geometric losses and the knowledge distillation losses are taken into account, and the loss function $\\mathcal{L}_{\\text{finetune}}$ is calculated as\n\\begin{equation}\n    \\mathcal{L}_{\\text{finetune}} = \\lambda_{P}\\mathcal{L}_{P} + \\lambda_{U}\\mathcal{L}_{U} + \\lambda_{S}\\mathcal{L}_{S} + \\lambda_{KD}\\mathcal{L}_{KD},\n\\end{equation}\nwhere $\\lambda_{P}$, $\\lambda_{U}$, $\\lambda_{S}$ and $\\lambda_{KD}$ are hyper-parameters.\n\n% \\begin{enumerate}\n%     \\item for $c_{i}$ in $\\mathcal{C}^{base}$ do\n%     \\item \\qquad $K_{spt} = U(1, K_{max})$\n%     \\item \\qquad $\\mathcal{S}_{i}$ $\\leftarrow$ \\textsc{BiasedSample}($\\mathcal{D}^{base}$, $K_{spt}$)\n%     \\item \\qquad $\\mathcal{Q}_{i}$ $\\leftarrow$ \\textsc{RandomSample}($\\mathcal{D}^{base}-\\mathcal{S}_{i}$, $K_qry$)\n% \\end{enumerate}\n\n% \\begin{algorithm}[tb]\n% \\caption{Biases Sampling in Meta Training}\n% \\label{alg:bias_train}\n% \\textbf{Input}: Base class training data $\\mathcal{D}^{base}$, base class set $\\mathcal{C}^{base}$, maximum biased sampling number $K_{max}$, query set sampling number $K_{qry}$\\\\\n% \\textbf{Output}: Meta training tasks $\\mathcal{T}_{train} = \\{\\mathcal{S}, \\mathcal{Q}\\}$\n% \\begin{algorithmic}[1] %[1] enables line numbers\n% \\FOR{$c_i$ in \\mathcal{C}^{base}}{\n% \\STATE $K_{spt} = U(1, K_{max})$\n% \\STATE $\\mathcal{S}_{i}$ $\\leftarrow$ \\textsc{BiasedSample}($\\mathcal{D}^{base}$, $K_{spt}$)\n% \\STATE $\\mathcal{Q}_{i}$ $\\leftarrow$ \\textsc{RandomSample}($\\mathcal{D}^{base}-\\mathcal{S}_{i}$, $K_qry$)\n% }\n% \\end{algorithmic}\n% \\end{algorithm}\n\n\n% the number of labels encountered in each stream is very uneven. Therefore, a support set is generated for biased sampling during the training phase. Specifically, the number of samples for each category obeys a uniform distribution. The query set still maintains a fixed number of samples.\n\n% The biased sampling strategy in training are utilized to generate class-imbalanced support sets by mimicking the circumstances encountered during fine-tuning. In streaming sessions, class-imbalanced query sets are sampled to calculate the loss function.\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiment",
                "content": "\n\n",
                "subsection 5.1": {
                    "name": "Experimental Setup",
                    "content": "\n\n",
                    "subsubsection 5.1.1": {
                        "name": "Datasets",
                        "content": "\nWe evaluate the proposed \\textsc{Geometer} on four real-world representative datasets: Cora-ML, Flickr, Amazon and Cora-Full. We summarize the statistics of these datasets in Table \\ref{tab:dataset}.\nA detailed description of these four datasets is provided in Appendix \\ref{appendix-dataset}.\n\n% We evaluate the proposed method on four real-world representative datasets: Cora-ML, Flickr, Amazon and Cora. (1) \\textbf{{Cora-ML}}~\\cite{bojchevski2018deep} is an acedemic network about machine learning. The dataset contains 7 different classes, in which each node represents a paper and each edge represents the citation relationship between two papers. It's a small but representative dataset in classification tasks.\n% (2) \\textbf{{Flickr}}~\\cite{graphsaint-iclr20} is a social network between users of Flickr. Each node is a user and each edge denotes a friend relationship between two users. Although there are only 9 classes, it has much more edges and a larger feature scale.\n% (3) \\textbf{{Amazon}}~\\cite{Hou2020Measuring} is an e-commerce network, in which each node is an item and each edge denotes the co-purchasing relationship by a common user. The dataset has many nodes and edges, and the network is dense.\n% (4) \\textbf{{Cora}}~\\cite{bojchevski2018deep} is an academic network, which has 70 different classes of papers. We summarize the statistics of these datasets in Table \\ref{tab:dataset}.\n\n\n% (1) \\textbf{\\textit{Cora-ML}}~\\cite{bojchevski2018deep} is an acedemic network about Machine Learning. The dataset contains 7 different classes, in which each node represents a paper and each edge represents the citation relationship between two papers. It's a small but representative dataset in classification tasks. (2) \\textbf{\\textit{Flickr}}~\\cite{graphsaint-iclr20} is a social network, in which each node is a user of Flickr and each edge denotes the friend relationship between two users. (3) \\textbf{\\textit{Amazon}}~\\cite{Hou2020Measuring} is an e-commerce network, in which each node is an item and each edge denotes the co-purchasing relationship by a common user. (4) \\textbf{\\textit{Cora}}~\\cite{bojchevski2018deep} is also an academic network, which is larger than Cora-ML. We summarize the statistics of these datasets in Table 1.\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{graphicx}\n\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Experiment Settings",
                        "content": "\n\nWe divide the dataset into base stage and several streaming sessions respectively. \nFor Cora-ML, Flickr and Amazon datasets, we select five classes as novel classes and the rest as base classes, and adopt the \\emph{1-way 5-shot} GFSCIL setting, which means we have 6 sessions (i.e., 1 base + 5 novel) in total. While for Cora-Full dataset, we adopt \\emph{5-way 5-shot} GFSCIL setting, by choosing 20 classes as base classes and splitting the remaining 50 classes into 10 streaming sessions. \nWe set 2-layer GNNs with 512 neurons of hidden layer. The learning rate of base class is 1e-3, and the learning rate during fine-tuning is 1e-4. The temperature factor $\\tau$ is 2.\n\n% 说一下如何产生streaming session\n\n% Since our work focuses on few-shot class-Incremental learning problem, we process datasets into streaming sessions. Specifically, we divide the dataset into base stage and several streamsing sessions respectively. In the streaming session stage, the novel classes appear in the form of N-way K-shot. That is, in each session, N classes of novel nodes are added and each novel class has K labeled nodes. Cora-ML, Flickr and Amazon dataset are divided into base stage and 5 streaming sessions, and there is one class of novel nodes added in each session. Cora has 70 classes of nodes, so we construct base stage and 10 streaming sessions with 5 classes of novel nodes increased each time. We implement all the frameworks and models based on \\emph{Pytorch} \\footnote{The implementation code of our model is available at https://github.com/xxxxx/xxxx.}. We use a two-layer graph neural networks with 512 neurons in each layer as the backbone to extract features. The learning rate of base class is 1e-3, and the learning rate during fine-tuning is 1e-4. All the evaluated models are implemented on a server with two CPUs (Intel Xeon E5-2630 $\\times$ 2) and four GPUs (NVIDIA GTX 2080 $\\times$ 4).\n\n\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Baseline Methods",
                        "content": "\n\nWe compare the proposed method with following baselines:\n\n\\begin{itemize}[left=1em]\n    \\item \\textbf{GAT (FT)}: Graph attention network (GAT)~\\cite{DBLP:conf/iclr/VelickovicCCRLB18} is one of the state-of-the-art methods for node classification. We first pretrain a 2-layer GAT model with a fully connected neural network classifer on the base classes. During streaming sessions, we replace and retrain the parameters of the fully connected neural network classifer on the support set of both base classes and novel classes.\n    \\item \\textbf{GAT+ (FT)}: It adopts the same architecture of GAT (FT). The difference is that we fine-tune all training parameters on support set on different streaming sessions.%全部都finetune\n    \\item \\textbf{GPN}~\\cite{ding2020graph}: GPN is a superior method for few-shot node classification. It exploits graph neural networks and meta-learning on attributed networks for metric-based few-shot learning.\n    \\item \\textbf{GFL}~\\cite{yao2020graph}: It is the first work that resorts to knowledge transfer to improve semi-supervised node classification in graphs. It integrates local node-level and global graph-level knowledge to learn a transferable metric space, which is shared between auxiliary graphs and the target. \n    \\item \\textbf{PN*}~\\cite{snell2017prototypical}: Prototype Network firstly proposed for few-shot image classification. We adopt the key idea and implement \\textbf{PN*} for node classification.\n    \\item \\textbf{PN* (FT)}: The training process is the same as PN*, but in the test process it fine-tunes all trained parameters before making prediction on the query set.\n    \\item \\textbf{iCaRL*}~\\cite{rebuffi2017icarl}: iCaRL is a class-incremental methods for image classification. We replace the feature extractor as a two-layer GAT network.\n\\end{itemize}\n\nWe only modify the dataset partition to satisfy the graph few-shot class-incremental settings for GAT, GPN and GFL, while other settings are the same as its original implementation. PN and iCaRL are two methods proposed in image classification tasks. To explore its performance on node classification, we utilize GNN-based backbone for feature extraction, and we marker with asterisk(*) for clarification.  \nThe above baselines can be can be summarized into four categories: (1) GNN methods with fully connected neural network classifer, which is a widely used architecture for node classification. (2) Representative graph few-shot learning models includes GPN and GFL. (3) Prototype network methods. (4) Class-incremental learning baselines.\n\n\n% The above baselines can be can be summarized into three categories: (1) GNN methods with fully connected neural network classifer is a widely used architecture for node classification. We adopt two different finetune strategy for GFSCIL problem.\n% (2) Graph few-shot learning models includes GPN and GFL. These two models are representative methods for few-shot node classification, and GFL achieves the state-of-the-art performance for semi-supervised node classification. \n% (3) Two prototype network methods are the basis of our works. The average of support node features are regarded as the class prototypes, which is trained and finetuned based on cross-entropy loss of base classes and novel classes.\n\n% \\begin{itemize}[left=1em]\n%     \\item GNNs: Graph attention network (GAT)~\\cite{DBLP:conf/iclr/VelickovicCCRLB18} is one of the state-of-the-art methods for node classification. We first pretrain a 2-layer GAT model with a fully connected neural network classifer on the base classes. During streaming sessions, \\textbf{GAT (fine-tuned)} only replace and retrain the parameters of the fully connected neural network classifer on the support set, and \\textbf{GAT+ (fine-tuned)} fine-tune all training parameters.\n%     \\item Graph Few-Shot Learning models: We compare with two superior methods for few-shot node classification \\textbf{GPN}~\\cite{ding2020graph} and \\textbf{GFL}~\\cite{yao2020graph}. We only modify the dataset partition to satisfy the GFSCIL setting, and other settings of each model are the same as its original implementation.\n%     \\item Prototype Network: Prototype Network~\\cite{snell2017prototypical} is firstly proposed for few-shot image classification. We adopt the key idea and implement \\textbf{PN*} for node classification. \\textbf{PN* (fine-tuned)} fine-tunes all model parameters given support sets in each streaming session.\n% \\end{itemize}\n\n%  Since we propose the GFSCIL problem for the first time, there do not exist any state-of-art methods. To evaluate the performance of the proposed method, we choose baseline from the technical solutions of model fine-tuning, few-shot learning and metric learning. Finally, we compare with the following representative baselines.\n\n% 说明一下因为我们是首次propose了这个问题的人，没有直接的算法解决这个问题。我们从模型微调、小样本学习、度量学习这些技术方案来选择baseline\n\n% \\begin{itemize}[left=1em]\n%     \\item \\textbf{GAT (fine-tuned)}~\\cite{DBLP:conf/iclr/VelickovicCCRLB18}: GAT is a GNN architecture. We pre-train a GNN model on the base classes. During testing, we only fine-tune the parameters of the classifier on the support set before predicting the classes of the query set. %只对全连接层的classifier进行finetune\n%     \\item \\textbf{GAT+ (fine-tuned)}: Its architecture is the same as GAT. The difference is that we fine-tune all trained parameters on the support set during testing.%全部都finetune\n%     \\item \\textbf{GPN}~\\cite{ding2020graph}: It learns representative class prototypes in a transferable metric space. By measuring the similarity  with prototypes, the label of each query node can be computed. \n%     \\item \\textbf{GFL}~\\cite{yao2020graph}: It is the first work that resorts to knowledge transfer to improve semi-supervised node classification in graphs. It integrates local node-level and global graph-level knowledge to learn a transferable metric space, which is shared between auxiliary graphs and the target. \n%     \\item \\textbf{PN*}~\\cite{snell2017prototypical}: Prototype Network is used for few-shot image classification by mapping the sampling data from each class to a metric space and extracting their mean to represent the prototype of the class.\n%     \\item \\textbf{PN* (fine-tuned)}: The training process is the same as PN*, but in the test process it fine-tunes all trained parameters before making prediction on the query set.\n% \\end{itemize}\n\n% We only modify the dataset partition to satisfy the graph few-shot class-incremental settings for GAT, GPN and GFL, and other settings of each model are the same as its original implementation. PN* and PN* (fine-tuned) are implemented based on the idea of Prototypical Network widely used for image classification, and we use GAT as the backbone of the feature extractor.\n%说明一下PN*和PN* (fine-tuned)基于Prototypical Network思想自己去实现的一个版本，并采用了GAT作为特征提取器的backbone\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n% \\usepackage{multirow}\n% \\usepackage{graphicx}\n% \\usepackage[table,xcdraw]{xcolor}\n% If you use beamer only pass \"xcolor=table\" option, i.e. \\documentclass[xcolor=table]{beamer}\n\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Performance Comparison",
                    "content": "\n\nWe run \\textsc{Geometer} and other baselines 10 times with different random seeds and report the average test accuracy over all encountered classes in Table \\ref{tab:cora_flickr} and Figure \\ref{fig:performance_two}. \nFrom the comprehensive views, we make the following observations:\n\n(1) Firstly, our proposed model \\textsc{Geometer} outperforms other baselines across Cora-ML, Amazon and Cora-Full datasets. For example, \\textsc{Geometer} achieves 13.49\\% to 24.15\\% performance improvement over the best baseline model in 10 streaming sessions on Cora-Full dataset. Meanwhile, as shown in Figure \\ref{fig:performance_two}, \\textsc{Geometer} does not suffer dramatic performance degradation as other baselines, strongly demonstrating the superiority of our approach. \n\n(2) Secondly, by integrating the idea of metric learning, GFL, as the state-of-the-art model of graph few-shot learning, shows competitive performance at the base stage and first session on Flickr dataset. However, it is worth noting that \\textsc{Geometer} achieves supreme results as more streaming session arrives and achieves substantial improvements. \n\n(3) GNN methods with fully connected neural network classifer largely fall behind other baselines. Those two methods need to replace and retrain the fully connected neural network classifer, which relies on sufficent training samples of each node classes. Therefore, with the increase of few-labeling novel classes, the performance of node classification deteriorates dramatically. PN* (FT) and iCaRL show better performance in several datasets, which shows metric-based methods with finetuning are more suitable for solving the GFSCIL problems.\n\n\n% , GFL, as the state-of-the-art model of graph few-shot learning, shows competitive performance at the base stage and first session. However, \\textsc{Geometer} achieves optimal results as more streaming session arrives. GFSCIL problem are more concerned with the performance changes as more few-labeling novel classes popping up.\n\n\n\n% Firstly, as shown in Figure \\ref{fig:performance_two}, our proposed model \\textsc{Geometer} outperforms other baselines across Cora-ML and Amazon datasets.\n% As the streaming sessions arrive, \\textsc{Geometer} does not suffer dramatic performance degradation as other baselines by adjusting the prototype representation of the metric space. Table \\ref{tab:performance_cora} shows that \\textsc{Geometer} achieves 13.66\\% to 24.15\\% performance improvement over the \n% best baseline model in 10 streaming sessions on Cora dataset, strongly demonstrating the superiority of our approach. \n% Secondly, as shown in Table \\ref{tab:performance_flickr}, GFL, as the state-of-the-art work of graph few-shot learning, shows competitive performance at the base stage and first session on Flickr dataset, but our method achieve optimal results as more streaming session arrives. Lastly, PN* and PN* (fine-tuned)  show better performance in several datasets, and GPN is also designed with the idea of prototype representation, which shows metric-based methods are more suitable for solving the GFSCIL problems.\n\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n% \\usepackage{graphicx}\n% \\begin{table}[]\n% \\centering\n% \\caption{Comparison results of node classification accuracy in GFSCIL settings on Flickr datasets}\n% \\label{tab:flickr}\n% \\renewcommand\\arraystretch{1.15}\n% \\resizebox{\\linewidth}{!}{%\n% \\begin{tabular}{lcccc}\n% \\hline\n% \\multicolumn{1}{c}{\\multirow{2}{*}{Methods}} & \\multicolumn{4}{c}{Flickr (1-way 5-shot GFSCIL setting)}                                      \\\\ \\cline{2-5} \n% \\multicolumn{1}{c}{}   & Base Classes & Session 1    & Session 3    & Session 5    \\\\ \\hline\n% GAT (fine-tuned)       & 62.16±1.30\\% & 24.24±3.91\\% & 16.13±5.55\\% & 9.04±5.46\\%  \\\\\n% GAT+ (fine-tuned)      & 61.41±1.55\\% & 26.13±7.62\\% & 8.55±0.71\\%  & 4.94±2.48\\%  \\\\\n% GPN {[}CIKM'2020{]}    & 72.80±1.13\\% & 51.02±1.70\\% & 32.79±2.65\\% & 22.01±1.28\\% \\\\\n% GFL {[}AAAI'2020{]}                          & \\textbf{84.82±3.13\\%} & \\textbf{61.09±1.41\\%} & 33.73±1.49\\%          & 28.15±1.17\\%          \\\\\n% PN* {[}NeurIPS'2017{]} & 59.86±1.81\\% & 34.29±1.97\\% & 23.37±3.63\\% & 20.23±4.00\\% \\\\\n% PN* (fine-tuned)       & 59.43±2.68\\% & 40.96±2.38\\% & 35.43±5.63\\% & 32.74±4.57\\% \\\\ \\hline\n% \\textbf{Geometer (Ours)}                     & 64.75±1.76\\%          & 57.57±2.80\\%          & \\textbf{45.21±1.04\\%} & \\textbf{36.26±2.79\\%} \\\\\n% impr.                  & -23.66\\%     & -5.76\\%      & +27.60\\%     & +10.75\\%     \\\\ \\hline\n% \\end{tabular}%\n% }\n% \\end{table}\n\n% Figure \\ref{fig:performance_two}, Table \\ref{tab:performance_cora} and Table \\ref{tab:performance_flickr} show the performance comparsion with baselines on four datasets. Firstly, \n\n% our proposed model \\textsc{Geometer} outperforms other baselines across Cora-ML and Amazon datasets. \n% As streaming sessions arrive, the performance of node classification does not suffer as dramatically as other methods. \n\n% Table \\ref{tab:performance} shows the performance comparison with baselines on Cora-ML, Flickr and Amazon datasets, testing on base classes and 5 streaming sessions. Figure \\ref{fig:Cora} shows the result on Cora dataset. We make the following observations.\n\n% Firstly, our proposed model \\textsc{Geometer} achieves the best performance across all four datasets in the streaming sessions. For example, on Cora-ML dataset, \\textsc{Geometer}'s accuracy at streaming session 5 is about $65\\%$, while the accuracy of the best performing baseline PN*(fine-tuned) is only about $50\\%$. Even at streaming session 10 of Cora database, \\textsc{Geometer} obviously outperforms the best performing baseline. It verifies that \\textsc{Geometer} is a powerfull model to solve the  GFSCIL problems.\n\n% Secondly, as the state-of-the-art of graph meta-learning, GFL is a competitive method especially at the base stage. We can see that GFL outperforms \\textsc{Geometer} at the base class on cora-ML and Flickr datasets. But in order to tackle the GFSCIL problems, we attach more attention to the result of the streaming sessions. It performs well in the base class, but the classification accuracy decreases rapidly with the occurrence of streaming session. It means that it cannot avoid catastrophic forgetting.\n\n% Thirdly, the overall performance of the metric-based methods such as GPN, GFL, PN* and \\textsc{Geometer} is much better than GAT and GAT+ so that they are more suitable for solving the GFSCIL problems. With the addition of novel classes  at streaming sessions, the classes of nodes are increasing. The metric-based methods can classify the nodes just by comparing each node and prototypes.\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n% \\usepackage{graphicx}\n% \\usepackage[table,xcdraw]{xcolor}\n% If you use beamer only pass \"xcolor=table\" option, i.e. \\documentclass[xcolor=table]{beamer}\n\n"
                },
                "subsection 5.3": {
                    "name": "Ablation Study",
                    "content": "\n\nIn this section, we analyze our \\textsc{Geometer} model with several degenerate models from four aspects. Due to space limitations, the ablation studies are conducted on two representative datasets: Cora-ML and Amazon.\n\n(1) \\textbf{GNN backbone}: Due to the dynamic evolution of graph, \\textsc{Geometer} utilizes graph attention network to capture the node features. In the ablation study, we replace it with two well-known GNN backbone GCN and GraphSage for comparison. As shown in Figure \\ref{fig:ablation_gnn}, the performance of GCN and GraphSage falls behind graph attention network especially in Amazon dataset, since these two model fails to capture the complex correlations in message-passing.\n\n\n\n\n\n(2) \\textbf{Prototype representation method}: \\textsc{Geometer} proposes class-level multi-head attention to learn the class prototype.\nThree degenerate prototype representation methods are considered in ablation study-i.e. means of support node embedding (\\text{Average}), weighted-sum of node embedding by node degree (\\text{Weighted-sum}), and average node embeddings as the initial prototype in multi-head attention mechanism (\\text{Attention (Average)}). Results on two datasets are presented in Figure \\ref{fig:ablation_proto}. Since the unequal and non-static node influence, attention-based methods helps to better characterize the relationship of nodes and classes. On Amazon dataset, \\textsc{Geometer} and \\text{Attention (Average)} show close performance, while the introduction of degree-based weighted-sum further improve accuracy of prototype representation on Cora-ML dataset.\n\n\n\n(3) \\textbf{Loss functions}: \\textsc{Geometer} proposes four different loss functions to finetune the prototype representation, and the effects of inter-class uniformity, inter-class separability and knowledge distillation are compared in Table \\ref{tab:loss_functions}. In general, with the pop-up of novel classes, the best classification results are obtained when the four loss functions are combined together. Different loss functions optimize the classification effect from different aspects, and the deletion of any loss function will have a significant impact on performance.\n\n(4) \\textbf{Biased sampling strategy}: In episode meta learning, due to the imbalanced labeling of base and novel classes, different biased sampling strategies are adopted in pretraining stage and finetuning stage. In ablation study, we compare the performance with only one strategy is adopted or without biased sampling strategy in Figure \\ref{fig:ablation_bias}. When we discard any biased sampling strategy, the learning process degenerates into a MAML-based learning strategy, and our method always shows better performance. If only one biased sampling strategy is adopted, partial sessions on Amazon datasets will have better results, but overall requires a combination of two biased sampling strategies.\n\n% the performance of only one strategy is compared with that of both.\n\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{graphicx}\n% \\usepackage[table,xcdraw]{xcolor}\n% If you use beamer only pass \"xcolor=table\" option, i.e. \\documentclass[xcolor=table]{beamer}\n% \\begin{table}[]\n% \\centering\n% \\caption{Ablation study of biased sampling strategy on Cora-ML and Amazon dataset}\n% \\label{tab:bias_sampling}\n% \\renewcommand\\arraystretch{1.1}\n% \\resizebox{\\linewidth}{!}{%\n% \\begin{tabular}{cccccc}\n% \\hline\n% \\multicolumn{2}{c}{\\textbf{Biased Sampling}} &\n%   \\multicolumn{4}{c}{\\textbf{Cora-ML (1-way 5-shot GFSCIL setting)}} \\\\ \\hline\n% \\textbf{Pretrain} &\n%   \\textbf{Finetune} &\n%   \\textbf{Base} &\n%   \\textbf{Session 1} &\n%   \\textbf{Session 3} &\n%   \\textbf{Session 5} \\\\ \\hline\n%  &\n%   &\n%   {\\color[HTML]{333333} 96.32±0.51\\%} &\n%   {\\color[HTML]{333333} 89.26±3.63\\%} &\n%   {\\color[HTML]{333333} 68.35±4.36\\%} &\n%   {\\color[HTML]{333333} 62.34±2.68\\%} \\\\\n% $\\checkmark$ &\n%   &\n%   \\cellcolor[HTML]{EFEFEF}{\\color[HTML]{333333} \\textbf{96.55±0.71\\%}} &\n%   {\\color[HTML]{333333} 75.77±2.09\\%} &\n%   {\\color[HTML]{333333} 65.35±1.53\\%} &\n%   {\\color[HTML]{333333} 61.22±4.01\\%} \\\\\n%  &\n%   $\\checkmark$ &\n%   {\\color[HTML]{333333} 96.27±0.41\\%} &\n%   \\cellcolor[HTML]{EFEFEF}{\\color[HTML]{333333} \\textbf{89.90±3.47\\%}} &\n%   {\\color[HTML]{333333} 57.19±6.71\\%} &\n%   {\\color[HTML]{333333} 50.56±5.22\\%} \\\\ \\hline\n% $\\checkmark$ &\n%   $\\checkmark$ &\n%   96.01±0.92\\% &\n%   89.89±3.97\\% &\n%   \\cellcolor[HTML]{EFEFEF}{\\color[HTML]{333333} \\textbf{72.45±4.01\\%}} &\n%   \\cellcolor[HTML]{EFEFEF}{\\color[HTML]{333333} \\textbf{64.25±3.60\\%}} \\\\ \\hline\n% \\multicolumn{1}{l}{} &\n%   \\multicolumn{1}{l}{} &\n%   {\\color[HTML]{333333} } &\n%   {\\color[HTML]{333333} } &\n%   {\\color[HTML]{333333} } &\n%   {\\color[HTML]{333333} } \\\\ \\hline\n% \\multicolumn{2}{c}{\\textbf{Biased Sampling}} &\n%   \\multicolumn{4}{c}{{\\color[HTML]{333333} \\textbf{Amazon (1-way 5-shot GFSCIL setting)}}} \\\\ \\hline\n% \\textbf{Pretrain} &\n%   \\textbf{Finetune} &\n%   \\textbf{Base} &\n%   \\textbf{Session 1} &\n%   \\textbf{Session 3} &\n%   \\textbf{Session 5} \\\\ \\hline\n%  &\n%   &\n%   95.43±0.29\\% &\n%   89.04±0.40\\% &\n%   72.26±2.77\\% &\n%   75.19±1.39\\% \\\\\n% $\\checkmark$ &\n%   &\n%   \\cellcolor[HTML]{EFEFEF}\\textbf{96.84±0.10\\%} &\n%   89.45±1.49\\% &\n%   72.28±2.08\\% &\n%   73.33±1.68\\% \\\\\n%  &\n%   $\\checkmark$ &\n%   95.07±0.52\\% &\n%   89.06±0.63\\% &\n%   75.30±4.08\\% &\n%   72.78±1.85\\% \\\\ \\hline\n% $\\checkmark$ &\n%   $\\checkmark$ &\n%   96.50±0.29\\% &\n%   \\cellcolor[HTML]{EFEFEF}\\textbf{91.44±0.46\\%} &\n%   \\cellcolor[HTML]{EFEFEF}\\textbf{76.74±1.89\\%} &\n%   \\cellcolor[HTML]{EFEFEF}\\textbf{77.66±1.58\\%} \\\\ \\hline\n% \\end{tabular}%\n% }\n% \\end{table}\n\n"
                },
                "subsection 5.4": {
                    "name": "Parameter Analysis",
                    "content": "\n\nIn addition, we investigate the effect of support set size $K_{spt}$ on two dataset. By changing the the value of shot number $K_{spt}\\in[1,3,5]$, we obtain different model performance. As shown in Figure \\ref{fig:hyper_spt}, we can clearly observe that the performance of \\textsc{Geometer} increases as the support set size $K_{spt}$, indicating that a larger support set helps to learn better class prototypes. At the same time, it shows that \\textsc{Geometer} is able to overcome the noise or outliers due to few-shot labeling and effectively learn the class representations.\n\n\n\n"
                },
                "subsection 5.5": {
                    "name": "Case Study",
                    "content": "\n\nIn order to explore the effects of geometric losses and knowledge distillation technique, we use $t$-SNE method to project the node embeddings and prototype representations of base stage and 5 streaming sessions on Amazon dataset, as shown in Figure \\ref{fig:tsne}. The visualization shows that as the novel classes arrival, most nodes are well clustered, and the prototypes are uniformly distributed around the prototype center. It is worth noting that a hard novel classes (colored in light purple) emerges in streaming session 2. Since \\textsc{Geometer} takes into account the geometric relationships in the metric space and adapt knowledge distillation, the following novel classes in the subsequent streaming sessions actively distance with the light purple class, thereby avoiding the dramatic drop in performance caused by prototype representations overlapping.\n\n\n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{images/tsne_gfl_case_study.pdf}\n%     \\caption{A $t$-SNE visualization of the query node embeddings and prototypes of GFL on Amazon dataset.}\n%     \\label{fig:tsne_gfl}\n% \\end{figure}\n\n\n\n% We analyze our model \\textsc{Geometer} in several aspects, including two ablation studies to show contribution from biased smpling stagy and different loss function, and an analysis of parameter sensitivity.\n\n\n% It is worth noting that a new category (colored in light purple) is more difficult when the second stream arrives. Since \\textsc{Geometer} takes into account dispersion and knowledge distillation during training, other novel classes actively distance themselves from this class during subsequent stream arrivals, avoiding the cliff drop in performance due to overlapping prototype representations.\n\n% Figure \\ref{fig:tsne} shows the spatial distribution of the query node embeddings and prototypes of Amazon dataset during base stage and 5 streaming sessions. The prototypes can well aggregate various nodes of each class in space and they are reasonably distributed around a center without being too close to each other. From the figure, we can observe that \\textsc{Geometer} can effectively solve the few-shot incremental classification task in the process of streaming arrival.\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\nIn this paper, we propose \\textsc{Geometer} for Graph Few-Shot Class-Incremental Learning (GFSCIL). As far as we known, this is the first work to deal with this challenging yet practical problem. The core idea of \\textsc{Geometer} is to adjust the prototype representation in metric space from the aspects of geometric relationship and knowledge distillation, so as to realize the classification of ever-expanding classes with few-shot samples. Extensive experiments on four public datasets show that \\textsc{Geometer} significantly outperforms the state-of-the-art baselines. In the future, we would like to extend our framework to address more challenging problem, like the open-set classification in graphs.\n\n"
            },
            "section 7": {
                "name": "Acknowledgement",
                "content": "\nThis work was supported by Natural Science Foundation of China under Grants No. 42050105; in part by NSF China (No. 62020106005, 62061146002, 61960206002, 61829201, 61832013), 2021 Tencent AI Lab RhinoBird Focused Research Program (No: JR202132), and the Program of Shanghai Academic/Technology Research Leader under Grant No. 18XD1401800.\n\n\n\\bibliographystyle{unsrt}\n\\bibliography{reference}\n\n\\appendix\n\n"
            },
            "section 8": {
                "name": "Appendix",
                "content": "\n\nTo support the reproducibility of the results in this paper, we have released our code and data. We implement the \\textsc{Geometer} model based on Pytorch framework.\\footnote{The implementation code and details of our model is available at https://github.com/RobinLu1209/Geometer.} \nAll the evaluated models are implemented on a server with two CPUs (Intel Xeon E5-2630 $\\times$ 2) and four GPUs (NVIDIA GTX 2080 $\\times$ 4). \n\n",
                "subsection 8.1": {
                    "name": "Dataset",
                    "content": "\n\\label{appendix-dataset}\nIn this paper, we evaluate the proposed \\textsc{Geometer} on four public datasets as follows:\n\\begin{itemize}[left=1em]\n    \\item \\textbf{{Cora-ML}}~\\cite{bojchevski2018deep} is an academic network about machine learning papers. The dataset contains 7 different classes, in which each node represents a paper and each edge represents the citation relationship between two papers.\n    \\item \\textbf{{Flickr}}~\\cite{graphsaint-iclr20} is a photo-sharing social network from Flickr.\n     Each node represents one picture uploaded to the Flickr website and the node feature contains information of low-level feature from NUS-WIDE Dataset. Flickr forms the edges between images from the same location, submitted to the same gallery, sharing common tags, taken by friends, etc. \n    \\item \\textbf{{Amazon}}~\\cite{Hou2020Measuring} is the segments of Amazon co-purchase e-commerce network, in which each node is an item and each edge denotes the co-purchasing relationship by a common user. The node features are bag-of-words encoded product reviews, and class labels are given by the product category.\n    \\item \\textbf{{Cora-Full}}~\\cite{bojchevski2018deep} is a well-known citation network labeled based on the paper topic, which has 70 different classes of papers. Among the academic networks we know, it has the largest network size and the largest number of categories.\n\\end{itemize}\n\n"
                }
            }
        },
        "tables": {
            "tab:dataset": "\\begin{table}[h]\n\\centering\n\\caption{Statistics of datasets used in the experiments}\n\\label{tab:dataset}\n\\resizebox{\\linewidth}{!}{%\n\\begin{tabular}{cccccc}\n\\toprule\n\\textbf{Dataset} & \\textbf{Data Field} & \\textbf{Nodes} & \\textbf{Edges} & \\textbf{Features} & \\textbf{Class} \\\\ \\midrule\nCora-ML & Academic       & 2,995  & 16,316  & 2,879  & 7  \\\\\nFlickr  & Social network & 7,575  & 479,476 & 12,047 & 9  \\\\\nAmazon  & E-commerce     & 13,752 & 491,722 & 767    & 10 \\\\\nCora-Full    & Academic       & 19,793 & 126,842 & 8,710  & 70 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\end{table}",
            "tab:cora_flickr": "\\begin{table*}[]\n\\centering\n\\caption{Comparison results of node classification accuracy in GFSCIL settings on Cora-Full and Flickr dataset. \\textsc{Geometer}’s improvement is calculated relative to the best baseline.}\n\\label{tab:cora_flickr}\n\\resizebox{\\linewidth}{!}{%\n\\begin{tabular}{@{}cccccccccc@{}}\n\\toprule\n &\n  \\multicolumn{8}{c}{\\textbf{Cora-Full (5-way 5-shot GFSCIL setting)}} &\n   \\\\ \\cmidrule(lr){2-9}\n\\multirow{-2}{*}{\\textbf{Session}} &\n  \\textbf{GAT (FT)} &\n  \\textbf{GAT+ (FT)} &\n  \\textbf{GPN} &\n  \\textbf{GFL} &\n  \\textbf{PN*} &\n  \\textbf{PN* (FT)} &\n  \\textbf{iCaRL*} &\n  \\multicolumn{1}{l}{\\cellcolor[HTML]{EFEFEF}\\textbf{\\textsc{Geometer} (Ours)}} &\n  \\multirow{-2}{*}{\\textbf{impr.}} \\\\ \\midrule\nBase &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{80.53±1.32\\%} &\n  81.11±0.79\\% &\n  73.82±1.94\\% &\n  76.02±0.94\\% &\n  74.88±0.89\\% &\n  74.18±0.72\\% &\n  73.92±1.06\\% &\n  79.88±0.96\\% &\n  -1.52\\% \\\\ \\midrule\nSession 1 &\n  33.13±2.51\\% &\n  37.10±1.51\\% &\n  55.95±1.52\\% &\n  60.50±0.74\\% &\n  56.60±1.11\\% &\n  58.07±0.92\\% &\n  59.33±1.79\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{69.48±1.66\\%} &\n  +14.84\\% \\\\\nSession 2 &\n  25.39±1.59\\% &\n  26.34±0.98\\% &\n  49.49±1.57\\% &\n  52.85±1.88\\% &\n  48.59±0.66\\% &\n  53.97±0.97\\% &\n  54.05±0.70\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{61.34±0.92\\%} &\n  +13.49\\% \\\\\nSession 3 &\n  17.48±1.59\\% &\n  17.41±1.43\\% &\n  43.41±1.66\\% &\n  43.88±2.84\\% &\n  39.70±1.25\\% &\n  43.76±0.92\\% &\n  44.65±0.55\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{53.61±0.81\\%} &\n  +20.07\\% \\\\\nSession 4 &\n  12.09±1.35\\% &\n  12.12±0.78\\% &\n  39.03±1.29\\% &\n  38.22±1.81\\% &\n  37.33±2.07\\% &\n  41.83±0.91\\% &\n  40.52±1.56\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{48.24±1.46\\%} &\n  +15.30\\% \\\\\nSession 5 &\n  10.04±1.56\\% &\n  8.54±0.39\\% &\n  35.12±1.98\\% &\n  38.69±2.50\\% &\n  32.66±2.01\\% &\n  37.35±0.73\\% &\n  36.25±1.06\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{44.97±1.03\\%} &\n  +16.23\\% \\\\\nSession 6 &\n  8.63±0.88\\% &\n  7.01±0.80\\% &\n  33.34±1.35\\% &\n  33.94±3.53\\% &\n  30.83±2.09\\% &\n  36.56±0.84\\% &\n  33.46±1.16\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{42.93±0.88\\%} &\n  +17.42\\% \\\\\nSession 7 &\n  7.76±0.62\\% &\n  5.79±0.42\\% &\n  31.98±1.03\\% &\n  32.60±1.65\\% &\n  29.52±1.92\\% &\n  34.70±0.20\\% &\n  32.68±1.44\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{42.82±1.14\\%} &\n  +23.40\\% \\\\\nSession 8 &\n  6.99±0.72\\% &\n  5.38±0.49\\% &\n  30.63±1.64\\% &\n  28.32±1.78\\% &\n  28.39±1.97\\% &\n  33.97±1.24\\% &\n  31.02±1.48\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{41.01±0.96\\%} &\n  +20.72\\% \\\\\nSession 9 &\n  5.95±0.75\\% &\n  4.49±0.40\\% &\n  30.53±1.80\\% &\n  21.95±1.71\\% &\n  27.65±2.19\\% &\n  33.71±0.75\\% &\n  30.37±1.76\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{40.49±0.97\\%} &\n  +20.11\\% \\\\\nSession 10 &\n  5.51±0.95\\% &\n  3.92±0.61\\% &\n  28.33±1.48\\% &\n  21.77±1.50\\% &\n  26.07±1.89\\% &\n  31.67±0.55\\% &\n  29.21±1.71\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{39.32±0.78\\%} &\n  +24.15\\% \\\\ \\midrule\n\\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n   &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} \\\\ \\midrule\n &\n  \\multicolumn{8}{c}{\\textbf{Flickr (1-way 5-shot GFSCIL setting)}} &\n   \\\\ \\cmidrule(lr){2-9}\n\\multirow{-2}{*}{\\textbf{Session}} &\n  \\textbf{GAT (FT)} &\n  \\textbf{GAT+ (FT)} &\n  \\textbf{GPN} &\n  \\textbf{GFL} &\n  \\textbf{PN*} &\n  \\textbf{PN* (FT)} &\n  \\textbf{iCaRL*} &\n  \\multicolumn{1}{l}{\\cellcolor[HTML]{EFEFEF}\\textbf{\\textsc{Geometer} (Ours)}} &\n  \\multirow{-2}{*}{\\textbf{impr.}} \\\\ \\midrule\nBase &\n  62.16±1.30\\% &\n  61.41±1.55\\% &\n  72.80±1.13\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{84.82±3.13\\%} &\n  59.86±1.81\\% &\n  59.43±2.68\\% &\n  60.81±1.87\\% &\n  64.75±1.76\\% &\n  -23.66\\% \\\\ \\midrule\nSession 1 &\n  24.24±3.91\\% &\n  26.13±7.62\\% &\n  51.02±1.70\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{61.09±1.41\\%} &\n  34.29±1.97\\% &\n  40.96±2.38\\% &\n  40.54±1.28\\% &\n  57.57±2.80\\% &\n  -5.76\\% \\\\\nSession 2 &\n  17.54±4.98\\% &\n  14.83±0.92\\% &\n  37.77±4.29\\% &\n  45.53±3.08\\% &\n  28.30±4.16\\% &\n  38.78±1.97\\% &\n  37.03±5.06\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{50.11±2.03\\%} &\n  +10.05\\% \\\\\nSession 3 &\n  16.13±5.55\\% &\n  8.55±0.71\\% &\n  32.79±2.65\\% &\n  33.73±1.49\\% &\n  23.37±3.63\\% &\n  35.43±5.63\\% &\n  32.93±7.96\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{45.21±1.04\\%} &\n  +27.60\\% \\\\\nSession 4 &\n  8.41±2.11\\% &\n  6.30±2.31\\% &\n  24.39±2.54\\% &\n  31.63±2.35\\% &\n  23.77±5.02\\% &\n  38.16±5.31\\% &\n  31.27±6.79\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{41.77±0.79\\%} &\n  +9.46\\% \\\\\nSession 5 &\n  9.04±5.46\\% &\n  4.94±2.48\\% &\n  22.01±1.28\\% &\n  28.15±1.17\\% &\n  20.23±4.00\\% &\n  32.74±4.57\\% &\n  26.57±5.83\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{36.26±2.79\\%} &\n  +10.75\\% \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\end{table*}",
            "tab:loss_functions": "\\begin{table*}\n\\centering\n\\caption{Ablation study of loss functions comparsion on Cora-ML and Amazon dataset.}\n\\label{tab:loss_functions}\n\\resizebox{\\linewidth}{!}{%\n\\begin{tabular}{@{}cccc|cccccccc@{}}\n\\toprule\n\\multicolumn{4}{c|}{\\textbf{Loss functions}} &\n  \\multicolumn{4}{c}{\\textbf{Cora-ML (1-way 5-shot GFSCIL setting)}} &\n  \\multicolumn{4}{c}{\\textbf{Amazon (1-way 5-shot GFSCIL setting)}} \\\\ \\midrule\n\\textbf{$\\mathcal{L}_{P}$} &\n  \\textbf{$\\mathcal{L}_{U}$} &\n  \\textbf{$\\mathcal{L}_{S}$} &\n  \\textbf{$\\mathcal{L}_{KD}$} &\n  \\textbf{Base Classes} &\n  \\textbf{Session 1} &\n  \\textbf{Session 3} &\n  \\textbf{Session 5} &\n  \\textbf{Base Classes} &\n  \\textbf{Session 1} &\n  \\textbf{Session 3} &\n  \\textbf{Session 5} \\\\ \\midrule\n$\\checkmark$ &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  $\\checkmark$ &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{96.21±0.67\\%} &\n  88.25±3.99\\% &\n  64.89±2.53\\% &\n  56.21±5.55\\% &\n  96.72±0.28\\% &\n  90.91±0.59\\% &\n  74.74±2.33\\% &\n  73.73±3.01\\% \\\\\n$\\checkmark$ &\n  $\\checkmark$ &\n  \\multicolumn{1}{l}{} &\n  $\\checkmark$ &\n  95.85±0.56\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{90.41±3.86\\%} &\n  68.26±3.65\\% &\n  58.72±4.66\\% &\n  96.72±0.22\\% &\n  91.39±0.56\\% &\n  76.55±1.94\\% &\n  73.97±1.90\\% \\\\\n$\\checkmark$ &\n  \\multicolumn{1}{l}{} &\n  $\\checkmark$ &\n  $\\checkmark$ &\n  95.71±0.55\\% &\n  89.21±2.88\\% &\n  69.57±2.71\\% &\n  54.28±3.90\\% &\n  96.83±0.32\\% &\n  91.15±0.35\\% &\n  75.08±2.61\\% &\n  73.92±2.60\\% \\\\\n$\\checkmark$ &\n  $\\checkmark$ &\n  $\\checkmark$ &\n   &\n  95.74±0.61\\% &\n  90.40±4.82\\% &\n  68.16±1.45\\% &\n  62.41±2.37\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{96.86±0.35\\%} &\n  91.17±0.38\\% &\n  75.36±1.28\\% &\n  74.51±2.53\\% \\\\ \\midrule\n$\\checkmark$ &\n  $\\checkmark$ &\n  $\\checkmark$ &\n  $\\checkmark$ &\n  96.01±0.92\\% &\n  89.89±3.97\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{72.45±4.01\\%} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{64.25±3.60\\%} &\n  96.50±0.29\\% &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{91.44±0.46\\%} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{76.74±1.89\\%} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{77.66±1.58\\%} \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\end{table*}"
        },
        "figures": {
            "fig:toy_example": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/toy_example_update.pdf}\n    \\caption{Illustration of GFSCIL problem on an academic graph. Nodes represent papers, edges represent citation relationships, and each paper belongs to a certain research field (node class).}\n    \\label{fig:toy_example}\n\\end{figure}",
            "fig:system_model": "\\begin{figure*}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/system_model.pdf}\n    \\caption{Overview of the proposed \\textsc{Geometer} for Graph Few-Shot Class-Incremental Learning. (a) Problem setting of GFSCIL. With the arrival of nodes, the network structure has become more complex and novel node classes have been introduced (shown by different colors). (b) and (c) show the episode meta learning process with biased sampling strategy at base stage and streaming sessions. Two different loss functions $\\mathcal{L}_{\\mathcal{G}}$ and $\\mathcal{L}_{KD}$ are utilized for the update of the metric space.}\n    \\label{fig:system_model}\n\\end{figure*}",
            "fig:proto_repre": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{images/proto_model.pdf}\n    \\caption{Attention-based Prototype Representation Model}\n    \\label{fig:proto_repre}\n\\end{figure}",
            "fig:performance_two": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/performance_two_dataset_seaborn.pdf}\n    \\caption{Comparison results of node classification accuracy in GFSCIL settings  on Cora-ML and Amazon datasets.}\n    \\label{fig:performance_two}\n\\end{figure}",
            "fig:ablation_gnn": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/ablation_study_gnn.pdf}\n    \\caption{Ablation study of different GNN backbone on Cora-ML and Amazon dataset.}\n    \\label{fig:ablation_gnn}\n\\end{figure}",
            "fig:ablation_proto": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/ablation_study_prototype.pdf}\n    \\caption{Ablation study of prototype representation methods on Cora-ML and Amazon dataset.}\n    \\label{fig:ablation_proto}\n\\end{figure}",
            "fig:ablation_bias": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/ablation_study_biased.pdf}\n    \\caption{Ablation study of different biased sampling strategy on Cora-ML and Amazon dataset.}\n    \\label{fig:ablation_bias}\n\\end{figure}",
            "fig:hyper_spt": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/hyper_spt_size.pdf}\n    \\caption{Parameter Analysis of support set size of novel classes on Cora-ML and Amazon dataset.}\n    \\label{fig:hyper_spt}\n\\end{figure}",
            "fig:tsne": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/tsne_case_study.pdf}\n    \\caption{A $t$-SNE visualization of the query node embeddings and prototypes of \\textsc{Geometer} (ours) on Amazon dataset.}\n    \\label{fig:tsne}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\alpha_{i j}=\\frac{\\exp \\left(\\operatorname{LeakyReLU}\\left({\\mathbf{a}}^{T}\\left[\\mathbf{W} \\bm{h}^{l}_{i} \\| \\mathbf{W} \\bm{h}^{l}_{j}\\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(\\operatorname{LeakyReLU}\\left(\\mathbf{a}^{T}\\left[\\mathbf{W} \\bm{h}^{l}_{i} \\| \\mathbf{W} \\bm{h}^{l}_{k}\\right]\\right)\\right)},\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\bm{h}_{i}^{l+1}=\\sigma(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j} \\cdot \\mathbf{W} \\bm{h}_{j}^{l}).\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\bm{\\hat{p}}_{i} = \\sum_{j \\in \\mathcal{S}_i} \\frac{\\text{degree}(v_j)}{\\sum_{j^{\\prime} \\in \\mathcal{S}_i} \\text{degree}(v_{j^{\\prime}})} \\cdot f_{\\mathcal{G}}(\\bm{x}_{j}),\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\bm{h}_{i}^{spt} = \\textsc{Concatenate}(\\bm{\\hat{p}}_i, \\|_{j\\in \\mathcal{N}_i} f_{\\mathcal{G}}(\\bm{x}_j)).\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\textsc{Attention}(\\bm{Q},\\bm{K},\\bm{V}) = \\text{softmax}(\\frac{\\bm{Q}\\bm{K}^{T}}{\\sqrt{d_k}})\\bm{V}, \n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\bm{p}_i = \\bm{\\hat{p}}_i + \\textsc{Attention}(\\bm{\\hat{p}}_i, \\bm{h}_{i}^{spt}, \\bm{h}_{i}^{spt}).\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mathcal{L}_{P}=\\sum_{k=1}^{\\| \\mathcal{C}^{k}\\|}  \\frac{\\alpha_{k}}{n_{k}} \\sum_{i=1}^{n_{k}}-\\log \\frac{\\exp \\left(-d\\left(f_{\\mathcal{G}}\\left(\\bm{x}_{i}\\right), \\bm{p}_{k}\\right)\\right)}{\\sum_{k^{\\prime}\\in \\mathcal{C}^{k}} \\exp \\left(-d\\left(f_{\\mathcal{G}}\\left(\\bm{x}_{i}\\right), \\bm{p}_{k^{\\prime}}\\right)\\right)},\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\bm{p}_{c}=\\frac{1}{\\left\\|\\mathcal{C}^{k}\\right\\|} \\sum_{i=1}^{\\left\\|\\mathcal{C}^{k}\\right\\|} \\bm{p}_{i},\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    \\mathcal{L}_{U}=\\frac{1}{\\left\\|\\mathcal{C}^{k}\\right\\|} \\sum_{i=1}^{\\left\\|\\mathcal{C}^{k}\\right\\|} \\left\\{ 1+\\max _{j \\in \\{\\mathcal{C}^k\\} \\backslash i}\\left[\\frac{\\left(\\bm{p}_{i}-\\bm{p}_{c}\\right)}{\\left\\|\\bm{p}_{i}-\\bm{p}_{c}\\right\\|} \\cdot \\frac{(\\bm{p}_{j}-\\bm{p}_{c})}{\\|\\bm{p}_{j}-\\bm{p}_{c}\\|}\\right] \\right\\},\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\mathcal{L}_{S}=\\frac{1}{\\Delta \\mathcal{C}^{k}} \\sum_{i \\in \\Delta \\mathcal{C}^{k}} \\min _{j \\in \\mathcal{C}^{k-1}} \\exp \\left(-d\\left(\\bm{p}_{i}, \\bm{p}_{j}\\right)\\right),\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    y^{\\prime(i)} = \\frac{\\exp \\left(d\\left(f_{\\mathcal{G}}\\left(\\bm{x}^{(i)}\\right), \\bm{p}_{i}\\right) / \\tau\\right)}{\\sum_{j} \\exp \\left(d\\left(f_{\\mathcal{G}}\\left(x^{(i)}\\right), \\bm{p}_{j}\\right) / \\tau\\right)},\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    \\mathcal{L}_{K D}=\\frac{1}{\\left\\|\\mathcal{C}^{k-1}\\right\\|} \\sum_{i=1}^{\\left\\| \\mathcal{C}^{k-1}\\right\\|} y_{S}^{\\prime(i)} \\cdot \\log \\left(\\frac{y_{S}^{\\prime(i)}}{y_{T}^{\\prime(i)}}\\right),\n\\end{equation}",
            "eq:13": "\\begin{equation}\n    \\mathcal{L}_{\\text{train}} = \\lambda_{P}\\mathcal{L}_{P} + \\lambda_{U}\\mathcal{L}_{U}.\n\\end{equation}",
            "eq:14": "\\begin{equation}\n    \\mathcal{L}_{\\text{finetune}} = \\lambda_{P}\\mathcal{L}_{P} + \\lambda_{U}\\mathcal{L}_{U} + \\lambda_{S}\\mathcal{L}_{S} + \\lambda_{KD}\\mathcal{L}_{KD},\n\\end{equation}"
        },
        "git_link": "https://github.com/RobinLu1209/Geometer."
    }
}