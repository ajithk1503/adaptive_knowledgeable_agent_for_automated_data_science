{
    "meta_info": {
        "title": "FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure  Graph Perspective",
        "abstract": "Multivariate time series (MTS) forecasting has shown great importance in\nnumerous industries. Current state-of-the-art graph neural network (GNN)-based\nforecasting methods usually require both graph networks (e.g., GCN) and\ntemporal networks (e.g., LSTM) to capture inter-series (spatial) dynamics and\nintra-series (temporal) dependencies, respectively. However, the uncertain\ncompatibility of the two networks puts an extra burden on handcrafted model\ndesigns. Moreover, the separate spatial and temporal modeling naturally\nviolates the unified spatiotemporal inter-dependencies in real world, which\nlargely hinders the forecasting performance. To overcome these problems, we\nexplore an interesting direction of directly applying graph networks and\nrethink MTS forecasting from a pure graph perspective. We first define a novel\ndata structure, hypervariate graph, which regards each series value (regardless\nof variates or timestamps) as a graph node, and represents sliding windows as\nspace-time fully-connected graphs. This perspective considers spatiotemporal\ndynamics unitedly and reformulates classic MTS forecasting into the predictions\non hypervariate graphs. Then, we propose a novel architecture Fourier Graph\nNeural Network (FourierGNN) by stacking our proposed Fourier Graph Operator\n(FGO) to perform matrix multiplications in Fourier space. FourierGNN\naccommodates adequate expressiveness and achieves much lower complexity, which\ncan effectively and efficiently accomplish the forecasting. Besides, our\ntheoretical analysis reveals FGO's equivalence to graph convolutions in the\ntime domain, which further verifies the validity of FourierGNN. Extensive\nexperiments on seven datasets have demonstrated our superior performance with\nhigher efficiency and fewer parameters compared with state-of-the-art methods.",
        "author": "Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Longbing Cao, Zhendong Niu",
        "link": "http://arxiv.org/abs/2311.06190v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "arXiv admin note: substantial text overlap with arXiv:2210.03093"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\\label{sec:intro}\nMultivariate time series (MTS) forecasting plays an important role in numerous real-world scenarios, such as traffic flow prediction in transportation systems \\cite{Yu2018,Bai2020nips,He_2023}, temperature estimation in weather forecasting \\cite{Zheng2015,autoformer21,WWWZF23}, and electricity consumption planning in the energy market \\cite{HeZBYN22,depts_2022}, etc.\nIn MTS forecasting, the core challenge is to model intra-series (temporal) dependencies and simultaneously capture inter-series (spatial) correlations. \nExisting literature has primarily focused on the temporal modeling and proposed several forecasting architectures, including Recurrent Neural Network (RNN)-based methods (e.g., DeepAR~\\cite{salinas2020deepar}), Convolution Neural Network (CNN)-based methods (e.g., Temporal Convolution Network \\cite{bai2018}) and more recent Transformer-based methods (e.g., Informer \\cite{Zhou2021} and Autoformer \\cite{autoformer21}). \nIn addition, another branch of MTS forecasting methods has been developed to not only model temporal dependencies but also places emphasis on spatial correlations. The most representative methods are the emerging Graph Neural Network (GNN)-based approaches \\cite{Cao2020,Bai2020nips,tampsgcnets2022} that have achieved state-of-the-art performance in the MTS forecasting task.\n\nPrevious GNN-based forecasting methods (e.g., STGCN~\\cite{YuYZ18} and TAMP-S2GCNets \\cite{tampsgcnets2022}) heavily rely on a pre-defined graph structure to specify the spatial correlations, which as a matter of fact cannot capture the \\textit{spatial dynamics}, i.e., the time-evolving spatial correlation patterns.\nLater advanced approaches (e.g., StemGNN \\cite{Cao2020}, MTGNN \\cite{wu2020connecting}, AGCRN~\\cite{Bai2020nips}) can automatically learn inter-series correlations and accordingly model spatial dynamics without pre-defined priors, but almost all of them are designed by stacking graph networks (e.g., GCN and GAT) to capture \\textit{spatial dynamics} and temporal networks (e.g., LSTM and GRU) to capture \\textit{temporal dependencies}. \nHowever, the uncertain compatibility of the graph networks and the temporal networks puts extra burden on handcrafted model designs, which hinders the forecasting performance. Moreover, the respective modeling for the two networks \\textit{separately} learn spatial/temporal correlations, which naturally violate the real-world unified spatiotemporal inter-dependencies.\nIn this paper, we explore an opposite direction of \\textit{directly applying graph networks for forecasting} and investigate an interesting question: \\textbf{\\textit{can pure graph networks capture spatial dynamics and temporal dependencies even without temporal networks?}}\n\n% To answer this question, we rethink the MTS forecasting task from a pure graph perspective. \n% We start with building a new data structure, \\textit{hypervariate graph}, to represent time series with an united view of spatial/temporal dynamics. The core idea of the \\textit{hypervariate graph} is to construct a space-time fully-connected structure. Specifically, given a multivariate time series window (say input window)  %$\\mathbf{x}^{(1..N)}_{t-L:t} \\in \\mathbb{R}^{N \\times L} $ \n% %$ \\{ \\bm{x}_{t-L:t}^{(n)} \\}_{n=1}^{N} \\in \\mathbb{R}^{N \\times L}$ \n% $X_t \\in \\mathbb{R}^{N \\times T}$ at timestamp $t$, where $N$ is the number of series (variates) and $T$ is the length of input window, we construct a corresponding \\textit{hypervariate graph} structure represented as $\\mathcal{G}_t^{T} = (X_t^T, A_t^T)$, which is initialized as a fully-connected graph of $NT$ nodes with adjacency matrix $A_t^T \\in \\mathbb{R}^{NT \\times NT}$  and node features $X_t^T \\in \\mathbb{R}^{NT \\times 1}$ by regarding \\textit{each value} $x_t^{(n)} \\in \\mathbb{R}^1$ (variate $n$ at step $t$) in the window as a distinct \\textit{node} of hypervariate graph.\n% Such a special structure design formulates both intra- and inter-series correlations of multivariate series as pure \\textit{node-node dependencies} in hypervariate graph. \n% Different from classic formulations that learn dynamics in a two-stage (spatial and temporal) process \\cite{wu2020connecting}, our perspective views spatiotemporal correlations as a whole. It abandons the uncertain compatibility of spatial/temporal modeling, constructs adaptive space-time inter-dependencies, and\n% brings up higher-resolution fusion across multiple variates and timestamps in multivariate time series forecasting.\n\nTo answer this question, we rethink the MTS forecasting task from a pure graph perspective. \nWe start with building a new data structure, \\textit{hypervariate graph}, to represent time series with a united view of spatial/temporal dynamics. The core idea of the {hypervariate graph} is to construct a space-time fully-connected structure. Specifically, given a multivariate time series window (say input window)  %$\\mathbf{x}^{(1..N)}_{t-L:t} \\in \\mathbb{R}^{N \\times L} $ \n%$ \\{ \\bm{x}_{t-L:t}^{(n)} \\}_{n=1}^{N} \\in \\mathbb{R}^{N \\times L}$ \n$X_t \\in \\mathbb{R}^{N \\times T}$ at timestamp $t$, \nwhere $N$ is the number of series (variates) and $T$ is the length of input window, we construct a corresponding \\textit{hypervariate graph} structure represented as $\\mathcal{G}_t^{T} = (X_t^T, A_t^T)$, which is initialized as a fully-connected graph of $NT$ nodes with adjacency matrix $A_t^T \\in \\mathbb{R}^{NT \\times NT}$  and node features $X_t^T \\in \\mathbb{R}^{NT \\times 1}$ by regarding \\textit{each value} $x_t^{(n)} \\in \\mathbb{R}^1$ (variate $n$ at step $t$) of input window as a distinct \\textit{node} of a hypervariate graph.\nSuch a special structure design formulates both intra- and inter-series correlations of multivariate series as pure \\textit{node-node dependencies} in the hypervariate graph. \nDifferent from classic formulations that make spatial-correlated graphs and learn dynamics in a two-stage (spatial and temporal) process \\cite{wu2020connecting}, our perspective views spatiotemporal correlations as a whole. It abandons the uncertain compatibility of spatial/temporal modeling, constructs adaptive space-time inter-dependencies, and\nbrings up higher-resolution fusion across multiple variates and timestamps in MTS forecasting.\n\nThen, with such a graph structure, the multivariate forecasting can be originally formulated into {the predictions on the hypervariate graph}. However, the node number of the hypervariate graph increase with the number of series ($N$) and the window length ($T$), leading to a graph of large order and size. This could make classic graph networks (e.g., GCN~\\cite{KipfW17}, GAT~\\cite{gat_2017}) computationally expensive (usually with quadratic complexity) and suffer from optimization difficulty in obtaining accurate node representations~\\cite{Smith-MilesL12}. \n%whose higher complexity brings up extra potential computational burden and optimization difficulties when adopting traditional graph neural networks. \nTo this end, we propose a novel architecture, \\textit{Fourier Graph Neural Network} (FourierGNN), for MTS forecasting from a pure graph perspective.\nSpecifically, FourierGNN is built upon our proposed \\emph{Fourier Graph Operator} (FGO), which as a replacement of classic graph operation units (e.g., convolutions), performs \\textit{matrix multiplications} in \\textit{Fourier space} of graphs. \nBy stacking FGO layers in Fourier space, FourierGNN can accommodate adequate learning expressiveness and in the mean time achieve much lower complexity (Log-linear complexity), which thus can effectively and efficiently accomplish MTS forecasting. \nBesides, we present theoretical analysis to demonstrate that the FGO is equivalent to graph convolutions in the time domain, which further explains the validity of FourierGNN.\n\nFinally, we perform extensive experiments on seven real-world benchmarks. Experimental results demonstrate that FourierGNN achieves an average of more than 10\\% improvement in accuracy compared with state-of-the-art methods. In addition, FourierGNN achieves higher forecasting efficiency, which has about 14.6\\% less costs in training time and 20\\% less parameter volumes, compared with most lightweight GNN-based forecasting methods.\n\\vspace{-2mm}\n% Finally, we perform extensive multivariate forecasting experiments on seven real-world MTS datasets. Experimental results have demonstrated that compared with existing state-of-the-art GNN-based forecasting methods, FourierGNN can achieve a more than 10\\% improvement on average. Besides, FourierGNN exhibits higher model efficiency, which has about 14.6\\% less costs in training time and 20\\% less parameter volumes, compared with most lightweight GNN-based forecasting methods.\n\n%average efficiency improvements of 14.6\\% and a reduction in parameters of 20\\% compared to the fast GNN-based baseline. Furthermore, we conduct multifaceted visualization experiments and the results interpret the effectiveness of the hypervariate graph and FourierGNN in modeling the spatiotemporal inter-dependencies for MTS forecasting.\n%The main contributions of this paper are summarized as follows:\n%\\begin{itemize}\n%    \\item a hypervariate graph, representing non-static correlations between any two variables at any two timestamps, to capture high-resolution spatiotemporal dependencies.\n%    \\item\n%    \\item\n%\\end{itemize}\n\n% By leveraging FGSO, FourierGNN reformulates graph convolutions (time domain) to matrix multiplication in the frequency domain, which naturally results into much lower-complexity.\n% Moreover, it largely enhances the expressiveness on non-static spatiotemporal correlations; thus perform better in capturing the spatiotemporal dependencies and achieve better results in MTS forecasting.\n\n\n\n\n% Obviously, the supra-graph will heavily increase the computational complexity of GNN-based model, then a high-efficiency learning method is required to reduce the cost for model training. Inspired by \\emph{Fourier Neural Operator} (FNO)~\\cite{LiKALBSA2021}, we reformulate the graph convolution (time domain) to much lower-complexity matrix multiplication in the frequency domain by leveraging an efficient and newly-defined \\emph{Fourier Graph Shift Operator} (FGSO). In addition, it is necessary to consider the multiple iterations (layers) of graph convolutions to expand receptive neighbors and mix the diffusion information in the graph. To capture time-varying diffusion over the supra-graph, we introduce the edge-varying graph filter to weight the graph edges differently from different iterations and reformulate the edge-varying graph filter with multiple frequency-invariant FGSOs in the frequency domain to reduce the computational cost of graph convolutions. Finally, a novel Edge-Varying Fourier Graph Networks (EV-FGN) is designed for MTS analysis, which is stacked with multiple FGSOs to perform high-efficient multi-layer graph convolutions in the Fourier space.\n%Accordingly, we construct a complex-valued feed forward network, dubbed as Edge-Varying Fourier Graph Networks (EV-FGN), stacked with multiple FGSOs to perform high-efficiency multi-layer graph convolutions in the Fourier space.\n%, expressed as a polynomial of multiple graph shift operators (GSOs), \n\n%Finally, we proposed a novel Edge-Varying Fourier Graph Networks (EV-FGN), a high-efficiency scale-free parameter learning scheme is derived for MTS analysis and forecasting \n\n%graph signal processing%\n% The main contributions of this paper are summarized as follows:\n\n% $\\bullet$ We adaptively learn a hypervariate graph, representing non-static correlations between any two variables at any two timestamps, to capture high-resolution spatiotemporal dependencies.\n\n% $\\bullet$ To efficiently compute graph convolutions over the hypervariate graph, we reformulate the graph convolutions in the Fourier space by leveraging FGSO. To the best of our knowledge, this work makes the first step to reformulate the graph convolutions in the Fourier space.\n\n%we define FGSO that has the capacity of scale-free learning parameters in the Fourier space.%on the fly infer graph structures adaptively learns the supra-graph\n\n%$\\bullet$ We design a novel network EV-FGN for MTS analysis, which is evolved from edge varying graph filters to capture the time-varying variable dependencies in the Fourier space. \\textcolor{blue}{This study makes the first attempt to design a complex-valued feed-forward network in the Fourier space for efficiently computing multi-layer graph convolutions}.\n% $\\bullet$ We design a novel network FourierGNN evolved from edge-varying graph filters for MTS analysis to capture the time-varying variable dependencies in the Fourier space. This study makes the first attempt to design a complex-valued feed-forward network in the Fourier space to efficiently compute multi-layer graph convolutions.\n\n% %where FGSOs are used to reduce the complexity of graph convolutions.\n\n% $\\bullet$ Extensive experimental results on seven MTS datasets demonstrate that FourierGNN achieves state-of-the-art performance with high efficiency and fewer parameters. Multifaceted visualizations further interpret the efficacy of FourierGNN in graph representation learning for MTS forecasting.\n% % \\fi\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\paragraph{Graph Neural Networks for Multivariate Time Series Forecasting}\nMultivariate time series (MTS) have embraced GNN due to their best capability of modeling structural dependencies between variates~\\cite{zonghanwu2019,Bai2020nips,wu2020connecting,YuYZ18,tampsgcnets2022,LiYS018,lian2022}. Most of these models, such as STGCN \\cite{YuYZ18}, DCRNN \\cite{LiYS018}, and TAMP-S2GCNets \\cite{tampsgcnets2022}, require a pre-defined graph structure which is usually unknown in most cases. For this limitation, some studies enable to automatically learn the graphs by the inter-series correlations, e.g., by node similarity \\cite{MateosSMR19,Bai2020nips,zonghanwu2019} or self-attention mechanism \\cite{Cao2020}.\nHowever, these methods always adopt a \\textit{graph network} for spatial correlations and a \\textit{temporal network} for temporal dependencies separately~\\cite{MateosSMR19,zonghanwu2019,Cao2020,Bai2020nips}. For example, AGCRN~\\cite{Bai2020nips} use a GCN~\\cite{KipfW17} and a GRU~\\cite{gru_2014}, GraphWaveNet~\\cite{zonghanwu2019} use a GCN and a TCN~\\cite{bai2018}, etc.\nIn this paper, we propose an unified spatiotemporal formulation with pure graph networks for MTS forecasting.\n\\vspace{-2mm}\n%In recent years, some GNN-based works \\cite{Kipf2018,Deng2021} account for the dynamic dependencies through introducing network design such as the time-varying attention \\cite{Deng2021}. In comparison, our proposed model captures the dynamic dependencies leveraging the high-resolution correlation in the hypervariate graph without introducing specific networks. \n\\paragraph{Multivariate Time Series Forecasting with Fourier Transform} \nRecently, many MTS forecasting models have integrated the Fourier theory into deep neural networks~\\cite{kunyi_2023,FiLM_2022}. For instance, SFM \\cite{ZhangAQ17} decomposes the hidden state of LSTM into multiple frequencies by Discrete Fourier Transform (DFT). mWDN \\cite{jyWang2018} decomposes the time series into multilevel sub-series by discrete wavelet decomposition (DWT) and feeds them to LSTM network. \nATFN \\cite{YangYHM22} proposes a Discrete Fourier Transform-based block to capture dynamic and complicated periodic patterns of time series data. \nFEDformer \\cite{fedformer22} proposes Discrete Fourier Transform-based attention mechanism with low-rank approximation in frequency. \nWhile these models only capture temporal dependencies with Fourier Transform,  \nStemGNN \\cite{Cao2020} takes the advantages of both spatial correlations and temporal dependencies in the spectral domain by utilizing Graph Fourier Transform (GFT) to perform graph convolutions and Discrete Fourier Transform (DFT) to calculate the series relationships. %, but, it captures the temporal and spatial dependencies separately. Unlike these efforts, our model is able to jointly encode spatiotemporal dependencies in Fourier space.\n\\vspace{-2mm}\n\\iffalse\n",
                "subsection 2.1": {
                    "name": "Multivariate Time Series Forecasting",
                    "content": "\n\nClassic time series forecasting methods are linear models, such as VAR \\cite{Vector1993}, ARIMA \\cite{Asteriou2011} and state space model (SSM) \\cite{Hyndman2008}. Recently, deep learning based methods \\cite{Lai2018,Sen2019,Zhou2021} have dominated MTS forecasting due to their capability of fitting any complex nonlinear correlations \\cite{Lim2020}. \n%For example, IVM \\cite{Guo2019} explore the structure of LSTM to infer variable-wise temporal importance, and TFT \\cite{Lim2019} adopts a neural attentive model for interpretable high-performance multi-horizon forecasting.\n\n\\textbf{MTS with GNN}. More recently, MTS have embraced GNN \\cite{zonghanwu2019,Bai2020nips,wu2020connecting,YuYZ18,tampsgcnets2022,LiYS018} due to their best capability of modeling structural dependencies between variates. Most of these models, such as STGCN \\cite{YuYZ18}, DCRNN \\cite{LiYS018} and TAMP-S2GCNets \\cite{tampsgcnets2022}, require a pre-defined graph structure which is usually unknown in most cases. In recent years, some GNN-based works \\cite{Kipf2018,Deng2021} account for the dynamic dependencies due to network design such as the time-varying attention \\cite{Deng2021}. In comparison, our proposed model captures the dynamic dependencies leveraging the high-resolution correlation in the supra-graph without introducing specific networks.\n\n\\textbf{MTS with Fourier transform}. Recently, increasing MTS forecasting models have introduced the Fourier theory into neural networks as high-efficiency convolution operators \\cite{John2021, ChiJM20}. SFM \\cite{ZhangAQ17} decomposes the hidden state of LSTM into multiple frequencies by discrete Fourier transform (DFT). mWDN \\cite{jyWang2018} decomposes the time series into multilevel sub-series by discrete wavelet decomposition and feeds them to LSTM network, respectively. ATFN \\cite{YangYHM22} utilizes a time-domain block to learn the trending feature of complicated non-stationary time series and a frequency-domain block to capture dynamic and complicated periodic patterns of time series data. FEDformer \\cite{fedformer22} proposes an attention mechanism with low-rank approximation in frequency and a mixture of expert decomposition to control the distribution shift. However, these models only capture temporal dependencies in the frequency domain. StemGNN \\cite{Cao2020} takes the advantages of both inter-series correlations and temporal dependencies by modeling them in the spectral domain, but, it captures the temporal and spatial dependencies separately. Unlike these efforts, our model is able to jointly encode spatial-temporal dependencies in Fourier space.\n\n"
                },
                "subsection 2.2": {
                    "name": "Graph Shift Operator",
                    "content": "\nGraph shift operators (GSOs) (e.g., the adjacency matrix and the Laplacian matrix) are a general set of linear operators which are used to encode neighbourhood topologies in the graph. \\cite{gasteiger_diffusion_2019} shows that applying the varying GSOs in the message passing step of GNNs can lead to significant improvement of performance. \\cite{Dasoulas2021} proposes a parameterized graph shift operator to automatically adapt to networks with varying sparsity. \\cite{isufi2021edgenets} allows different nodes to use different GSOs to weight the information of different neighbors. \\cite{Samar2021} introduces a linear composition of the graph shift operator and time-shift operator to design space-time filters for time-varying graph signals. Inspired by these works, in this paper we design a varying parameterized graph shift operator in Fourier space.\n\n"
                },
                "subsection 2.3": {
                    "name": "Fourier Neural Operator ",
                    "content": "\nDifferent from classical neural networks which learn mappings between finite-dimensional Euclidean spaces, neural operators learn mappings between infinite-dimensional function spaces \\cite{Kovachki2021}. Fourier neural operators (FNOs), currently the most promising one of the neural operators, are universal, in the sense that they can approximate any continuous operator to the desired accuracy \\cite{KovachkiLM21}. \\cite{LiKALBSA2021} formulates a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture for partial differential equations. \\cite{John2021} proposes an efficient token mixer that learns to mix in Fourier domain which is a principled architectural modification to FNO. In this paper, we learn a Fourier graph shift operator by leveraging the Fourier Neural operator.\n\\fi\n%$x_t^{(n)} \\in \\mathbb{R}^1$ --> $\\bm{x}_t \\in \\mathbb{R}^{N}$ --> $X_t \\in \\mathbb{R}^{N \\times T}$  -->  $\\mathcal{G}({X^\\mathcal{G}_t},{A_t^\\mathcal{G}})$ --> $\\mathcal{G}({\\mathbf{X}^\\mathcal{G}_t},{{A}_t^\\mathcal{G}})$ --> $\\mathcal{G}({\\mathcal{X}^\\mathcal{G}_t},{{A}_t^\\mathcal{G}})$ --> Shift Operator (AXW) --> $\\mathcal{G}({\\mathcal{Y}^\\mathcal{G}_t},{{A}_t^\\mathcal{G}})$ --> $\\mathcal{G}({\\mathbf{Y}^\\mathcal{G}_t},{{A}_t^\\mathcal{G}})$ --> \n%$ \\hat{Y}_t \\in \\mathbb{R}^{N \\times \\tau}$.\n"
                }
            },
            "section 3": {
                "name": "Problem Definition",
                "content": "\n%Let us denote an entire multivariate time series (MTS) dataset as $\\mathbb{X} \\in \\mathbb{R}^{N\\times L}$ where $N$ is the number of variables and $L$ is the number of timestamps of the whole dataset. \n%Under the rolling setting, the time-series inputs are defined as averagely sampled lookback windows of length $T$, i.e., $\\{X|X\\subset\\mathbb{X},X\\in\\mathbb{R}^{N\\times T}\\}$. \nGiven the multivariate time series input, i.e., the lookback window $X_t=[\\bm{x}_{t-T+1},...,\\bm{x}_t]\\in\\mathbb{R}^{N\\times T}$ at timestamps $t$ with the number of series (variates) $N$ and the lookback window size $T$, where $\\bm{x}_t \\in \\mathbb{R}^{N}$ denotes the multivariate values of $N$ series at timestamp $t$. Then, the \\textit{multivariate time series forecasting} task is to predict the next $\\tau$ timestamps $Y_t=[{\\bm{x}}_{t+1}, ..., {\\bm{x}}_{t+\\tau}]\\in \\mathbb{R}^{N \\times \\tau}$ based on the historical $T$ observations $X_t=[\\bm{x}_{t-T+1},...,\\bm{x}_t]$. The forecasting process can be given by:\n% \\begin{equation}\\label{eq:problem_define}\n%     \\hat{\\bm{x}}_{t+1},...,\\hat{\\bm{x}}_{t+\\tau} = {F_{\\theta}}(\\bm{x}_{t-T+1},...,\\bm{x}_t))\n% \\end{equation}\n\\begin{equation}\\label{eq:problem_define}\n   \\hat{Y}_t:= {F_{\\theta}}(X_t) = {F_{\\theta}}([\\bm{x}_{t-T+1},...,\\bm{x}_t])\n\\end{equation}\nwhere $\\hat{Y}_t$ are the predictions corresponding to the ground truth ${Y}_t$. The forecasting function is denoted as $F_\\theta$ parameterized by $\\theta$.\nIn practice, many MTS forecasting models usually leverage a \\textit{graph network} (assume parameterized by $\\theta_g$) to learn the spatial dynamics and a \\textit{temporal network} (assume parameterized by $\\theta_t$) to learn the temporal dependencies, respectively \\cite{zonghanwu2019, Cao2020,Bai2020nips,wu2020connecting,tampsgcnets2022}. Thus, the original definition of Equation (\\ref{eq:problem_define}) can be rewritten to:\n% \\begin{equation}\\label{eq:problem_define_gt}\n%     \\hat{\\bm{x}}_{t+1},...,\\hat{\\bm{x}}_{t+\\tau} = {F_{{\\theta_g},{\\theta_t}}}(\\bm{x}_{t-T+1},...,\\bm{x}_t)\n% \\end{equation}\n\\begin{equation}\\label{eq:problem_define_gt}\n    \\hat{Y}_t:= {F_{\\theta_g,\\theta_t}}(X_t) = F_{\\theta_g,\\theta_t}([\\bm{x}_{t-T+1},...,\\bm{x}_t])\n\\end{equation}\nwhere original parameters $\\theta$ are exposed to the parameters of the graph network ${\\theta_g}$ and the temporal network $ {\\theta_t}$ to make prediction based on the learned spatial-temporal dependencies.\n%{\n%\\color{red}\n%In the following section, we detnote the lookback window at timestamp $t$ as $X_t$, and the horizon window as $Y_t$ for brevity. \n%}\n\n% $F_{\\theta_g}$ with parameters $\\theta_g$ for spatial correlations and a temporal network $F_{\\theta_t}$ with parameters $\\theta_t$ for temporal dependencies. Subsequently, a forecasting function $F_{\\theta_f}$ with parameters $\\theta_f$ is applied to make predictions based on the learned spatial-temporal dependencies. As a result, the forecasting problem is formulated as follows:\n% \\begin{equation}\n%     \\hat{X}_{t+1},...,\\hat{X}_{t+\\tau} = {F_{\\theta_f}}((F_{\\theta_g},F_{\\theta_s})(X_{t-T},...,X_t))\n% \\end{equation}\n%where $\\operatorname{F}$ is the forecasting function and $\\Theta$ is the learnable parameters of the forecasting function.\n\n\n"
            },
            "section 4": {
                "name": "Methodology",
                "content": "\nIn this section, we elaborate on our proposed framework: First, we start with our pure graph formulation with a novel hypervariate graph structure for MTS forecasting\nin Section \\ref{sec:method_pure_graph_form}. Then, we illustrate the proposed neural architecture, Fourier Graph Neural Network (FourierGNN), for this formulation in Section \\ref{sec:method_fouriergnn}. Besides, we theoretically analyze FourierGNN to demonstrate its architecture validity, and also conduct complexity analysis to show its efficiency. Finally, we introduce certain inductive bias to instantiate FourierGNN for MTS forecasting in Section \\ref{sec:ovarc}.\n\n\n\n",
                "subsection 4.1": {
                    "name": "The Pure Graph Formulation",
                    "content": "\\label{sec:method_pure_graph_form}\nTo overcome the uncertain compatibility of the graph network and the temporal network as aforementioned in Section \\ref{sec:intro}, and learn the united spatiotemporal dynamics, we propose a \\textit{pure} graph formulation that refines Equation (\\ref{eq:problem_define_gt}) by a novel data structure, \\textit{hypervariate graph}, for time series.\n% In this paper, our goal is to learn an united view of spatiotemporal dynamics simultaneously and model the interdependencies between variables and their evolution over time, to this end, we build a hypervariate graph $\\mathcal{G}=(X,A)$ attributed to each $X$ and further formulate the problem of MTS forecasting on the hypervariate graph. \n \\begin{myDef}[\\textbf{Hypervariate Graph}]\n\\label{hypervariate_graph}\n    Given a multivariate time series window as input $X_t \\in \\mathbb{R}^{N \\times T}$ of $N$ variates at timestamp $t$, we construct a hypervariate graph of $NT$ nodes, $\\mathcal{G}_t=({X}^{\\mathcal{G}}_t, {A}^{\\mathcal{G}}_t)$, by regarding each element of $X_t$ \n    %(regardless of variables or timestamps)\n    as one node of $\\mathcal{G}_t$ such that ${X}^{\\mathcal{G}}_t \\in \\mathbb{R}^{NT \\times 1}$ stands for the node feature and ${A}^{\\mathcal{G}}_t \\in \\mathbb{R}^{NT \\times NT}$ is the adjacency matrix initialized to make $\\mathcal{G}_t$ as a fully-connected graph.\n    % variable at each timestamp is represented by a node in the graph, with $X\\in \\mathbb{R}^{NT}$ representing the nodes and $A \\in \\mathbb{R}^{(NT) \\times (NT)}$ being the adjacency matrix. We refer to the graph $\\mathcal{G}$ as a hypervariate graph.\n\\end{myDef}\n %The hypervariate graph $\\mathcal{G}_t$ contains $NT$ nodes that represent values of each variable at each timestamp in $X_t$. \n \nSince the prior graph structure is usually unknown in most multivariate time series scenarios~\\cite{Cao2020,Bai2020nips,wu2020connecting}, and the elements of $X_t$ are spatially or temporally correlated with each other because of time lag effect~\\cite{wei1989}, \n%Since the elements of $X_t$ are spatially or temporally correlated with each other because of time lag effect~\\cite{wei1989}, and this graph structure is usually unknown in most MTS scenarios,\n%~\\cite{Cao2020,Bai2020nips,wu2020connecting}, \nwe assume all nodes in the hypervariate graph $\\mathcal{G}_t$ are fully-connected.\nThe hypervariate graph $\\mathcal{G}_t$ contains $NT$ nodes representing the values of each variate at each timestamp in $X_t$, which can learn a high-resolution representation across timestamps and variates (more explanations of the hypervariate graph can be seen in Appendix \\ref{analysis_hypervariate_graph}). We present an example hypervariate graph of three time series in Figure \\ref{fig:hypervariate_graph}. \n Thus, with such a data structure, \n we can reformulate the \\textit{multivariate time series forecasting} task into the predictions on the hypervariate graphs, and accordingly rewrite Equation (\\ref{eq:problem_define_gt}) into:\n %on the hypervariate graph $\\mathcal{G}_t=({X}^{\\mathcal{G}}_t, {A}^{\\mathcal{G}}_t)$ and the problem formulation is modified as follows:\n\\begin{equation}\n    \\hat{Y}_t :=F_{\\theta_\\mathcal{G}}({X^\\mathcal{G}_t},{A_t^\\mathcal{G}})\n\\end{equation}\nwhere $\\theta_\\mathcal{G}$ stands for the network parameters for hypervariate graphs.\nWith such a formulation, we can view the spatial dynamics and the temporal dependencies from an united perspective, which benefits modeling the real-world spatiotemporal inter-dependencies.\n\n"
                },
                "subsection 4.2": {
                    "name": "FourierGNN",
                    "content": "\\label{sec:method_fouriergnn}\n\nThough the pure graph formulation can enhance spatiotemporal modeling, \nthe order and size of hypervariate graphs increase with the number of variates $N$ and the size of window $T$, which makes classic graph networks (e.g., GCN~\\cite{KipfW17} and GAT~\\cite{gat_2017}) computationally expensive (usually quadratic complexity) and suffer from optimization difficulty in obtaining accurate hidden node representations~\\cite{Smith-MilesL12}. \nIn this regard, we propose an efficient and effective method, FourierGNN, for the pure graph formulation. The main architecture of FourierGNN is built upon our proposed Fourier Graph Operator (FGO), a learnable network layer in Fourier space, which is detailed as follows.\n\n%\\begin{myDef}[\\textbf{Fourier Graph Operator}]\n%\\label{def:fgso}\n%Given a graph $G=({X}, A)$ with node features ${X}\\in\\mathbb{R}^{n\\times d}$ and the adjacency matrix $A \\in\\mathbb{R}^{n\\times n}$ where $n$ is %the node number and $d$ is the feature number, we can use a learnable weight matrix $W\\in\\mathbb{R}^{d\\times d}$ to acquire a matrix-valued kernel %$\\kappa: [n]\\times [n] \\xrightarrow{} \\mathbb{R}^{d\\times d}$ with $\\kappa[i,j]=A_{ij}\\circ {W}$  where $[n]=\\{1,2,\\cdots,n\\}$.\n%Then, we define $\\mathcal{S}(X, A):=\\mathcal{F}(\\kappa)\\in\\mathbb{C}^{d \\times d}$ as Fourier Graph Operator (FGO), where $\\mathcal{F}$ denotes %Discrete Fourier Transform (DFT).\n%\\end{myDef} \n\\begin{myDef}[\\textbf{Fourier Graph Operator}]\n\\label{def:fgso}\nGiven a graph $G=({X}, A)$ with node features ${X}\\in\\mathbb{R}^{n\\times d}$ and the adjacency matrix $A \\in\\mathbb{R}^{n\\times n}$, where $n$ is the number of nodes and $d$ is the number of features, we introduce a weight matrix $W\\in\\mathbb{R}^{d\\times d}$ to acquire a tailored Green\u2019s kernel $\\kappa: [n]\\times [n] \\xrightarrow{} \\mathbb{R}^{d\\times d}$ with $\\kappa[i,j]:=A_{ij}\\circ {W}$ and $\\kappa[i,j]=\\kappa[i-j]$. We define $\\mathcal{S}_{A, W}:=\\mathcal{F}(\\kappa)\\in\\mathbb{C}^{n\\times d \\times d}$ as a Fourier Graph Operator (FGO), where $\\mathcal{F}$ denotes Discrete Fourier Transform (DFT).\n\\end{myDef}\n% Consequently, according to the convolution theorem~\\cite{1970An} (see Appendix \\ref{convolution_theorem_appendix}), we can write the multiplications between $\\mathcal{F}({X})$ and the Fourier Graph Operator $\\mathcal{S}(X,A)$ in Fourier space as:\n% \\begin{equation}\\label{equ:fgso}\n%     (\\mathcal{F}({X})\\mathcal{S}(X, A))[i]= (\\mathcal{F}({X})\\mathcal{F}(\\kappa))[i] = \\mathcal{F}(\\sum_{j=1}^n {{X}}[j]\\kappa[i-j]), \\quad\\quad   \\forall i \\in [n].\n% \\end{equation}\n% In the special case of the Green's kernel $\\kappa[i,j]=\\kappa[i-j]$ \\cite{John2021}, we can rewrite the term inside $\\mathcal{F}$ in the right part of the above equation as follows:\n% \\begin{equation}\\label{equ:kernel_summation}\n%     \\sum_{j=1}^n {{X}}[j]\\kappa[i-j]=\\sum_{j=1}^n {{X}}[j]\\kappa[i,j],  \\quad\\quad   \\forall i \\in [n].\n% \\end{equation}\n% Then, we can replace $\\kappa[i,j]$ in the right part of Equation (\\ref{equ:kernel_summation}) with the previously defined $\\kappa[i,j]=A_{ij}\\circ {W}$, and obtain $\\sum_{j=1}^n {X}[j]\\kappa[i,j]=\\sum_{j=1}^n A_{ij}{X}[j]W=(AXW)[i]$. Finally, combining Equation (\\ref{equ:fgso}), we can get the following formula:\n% \\begin{equation}\\label{equ:gcn_unit}\n%     \\mathcal{F}(X)\\mathcal{S}(X,A) = \\mathcal{F}(AXW).\n% \\end{equation}\n% Consequently, according to the convolution theorem~\\cite{1970An} (see Appendix \\ref{convolution_theorem_appendix}), we can write the multiplications between $\\mathcal{F}({X})$ and the FGO $\\mathcal{S}(X,A)$ in Fourier space as:\n% \\begin{equation}\\label{equ:fgso}\n%     \\mathcal{F}({X})\\mathcal{F}(\\kappa)=\\mathcal{F}((X*\\kappa)[i])=\\mathcal{F}(\\sum_{j=1}^n {{X}}[j]\\kappa[i-j])=\\mathcal{F}(\\sum_{j=1}^n {{X}}[j]\\kappa[i,j]), \\quad\\quad   \\forall i \\in [n].\n% \\end{equation}\n% Since $\\kappa[i,j]=A_{ij}\\circ {W}$, it yields $\\sum_{j=1}^n {X}[j]\\kappa[i,j]=\\sum_{j=1}^n A_{ij}{X}[j]W=(AXW)$. Finally, combining Equation (\\ref{equ:fgso}), we can get the following formula:\n% \\begin{equation}\\label{equ:gcn_unit}\n%     \\mathcal{F}(X)\\mathcal{S}(A,X) = \\mathcal{F}(AXW).\n% \\end{equation}\n% In particular, turning to our case of the fully-connected hypervariate graph, we adopt a frequency-invariant FGO $\\mathcal{S}\\in\\mathbb{C}^{d \\times d}$ that has a computationally low cost compared to previous $\\mathbb{C}^{n \\times d\\times d}$. We provide more details and explanations of frequency-invariant FGO in Appendix \\ref{intp_FGO}.%For simplicity, we denote the frequency-invariant FGO as  in the following.\n\n% From the Equation (\\ref{equ:gcn_unit}), we can observe that performing a single multiplication between $\\mathcal{F}(X)$ and FGO $\\mathcal{S}(X,A)$ in Fourier space is equivalent to a graph shift operation (i.e., a graph convolution) in the time domain~\\cite{MateosSMR19}.\n% Since the multiplications in Fourier space ($\\mathcal{O}(n)$) have much lower complexity than above shift operations ($\\mathcal{O}(n^2)$) in the time domain (See \\textit{Complexity Analysis} below), it motivates us to develop a highly efficient graph neural network in Fourier space.\n\n% %The recursive composition has a nice property that $\\mathcal{S}_{0:k}=\\mathcal{S}_{0:k-1}\\mathcal{S}_k$, which inspires us to design a complex-valued feed forward network with $\\mathcal{S}_k$ being the complex weights for the $k$-th layer. However, both the FGSO and edge-variant graph filter are linear transformations, limiting the capability of modeling non-linear information diffusion on graphs. Following by the convention in GNNs, we introduce the non-linear activation and biases to reformulate the $k$-the layer as follows: \n% %\\begin{equation}\n% %    \\label{equ:layer}\n% %    \\mathcal{X}_k = \\sigma(\\mathcal{X}_{k-1}\\mathcal{A}_k+b_k)\n% %\\end{equation}\n% %with the complex weight $\\mathcal{A}_k \\in \\mathbb{C}^{d\\times d}$, biases $b_k \\in \\mathbb{C}^{d}$ and the activation function $\\sigma$.\n% To this end, we propose the \\textit{Fourier Graph Neural Networks} (FourierGNN) based on the Fourier Graph Operator.\n% %For the convenience of writing, we abbreviate $\\mathcal{S}(X, A)$ as $\\mathcal{S}$.\n% Specifically, by stacking multiple layers of FGOs, we can define the $K$-layer Fourier graph neural networks given a graph $G = (X,A)$ with node features $X\\in \\mathbb{R}^{n \\times d}$ and the adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$  as: \n% \\begin{equation} \\label{equ:fourier_gnn}\n% \\operatorname{FourierGNN}(X, A) =\n% %\\mathcal{F}(X)\\mathcal{S}_0+\n%     \\sum_{k=0}^{K}\\sigma(\\mathcal{F}(X)\\mathcal{S}_{0:k}+b_k),  \\quad s.t.\\ \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i\n%     %\\sum_{k=0}^K \\mathcal{X}_{k} \\quad s.t.\\ \\mathcal{X}_k=\\sigma(\\mathcal{X}_{k-1}\\mathcal{S}_k+b_k)\n% \\end{equation}\n% %\\TODO{ include A, add more explanation with many words, F, S, b, why sum, why K-order, why multiply $S_o:k$}\n% %where $\\mathcal{S}_k$ is the FGO in the $k$-th layer, which is the abbreviation for $\\mathcal{S}_k(X,A)$ and satisfies $\\mathcal{F}(X)\\mathcal{S}_k(X,A)=\\mathcal{F}(AXW_k)$ with $W_k \\in \\mathbb{R}^{d \\times d}$ as learnable weights; \n% %$\\mathcal{S}_{0:k}$ is recursive multiplications from $\\mathcal{S}_0$ to $\\mathcal{S}_k$;\n% %$\\mathcal{F}$ stands for Discrete Fourier Transform; $\\sigma$ is the activation function; %$\\mathcal{S}_k\\in\\mathbb{C}^{d\\times d}$ is the FGO in %the $k$-th layer satisfying $\\mathcal{F}(X)\\mathcal{S}_k=\\mathcal{F}(AXW_k)$ with $W_k \\in \\mathbb{R}^{d \\times d}$ being the weights ,\n% %$b_{k}\\in\\mathbb{C}^{d}$ is learnable complex-valued biases parameters. All operations in FourierGNN are performed in Fourier space, thus, all %involving parameters are complex numbers.\n% where $\\mathcal{S}_k$ is the parameterized FGO in the $k$-th layer, satisfying $\\mathcal{F}(X)\\mathcal{S}_k=\\mathcal{F}(AXW_k)$ with $W_k \\in \\mathbb{R}^{d \\times d}$ being the weights, and $b_{k}\\in\\mathbb{C}^{d}$ are the complex-valued biases parameters; $\\mathcal{F}$ stands for Discrete Fourier Transform; $\\sigma$ is the activation function. %$\\mathcal{S}_k\\in\\mathbb{C}^{d\\times d}$ is the FGO in the $k$-th layer satisfying $\\mathcal{F}(X)\\mathcal{S}_k=\\mathcal{F}(AXW_k)$ with $W_k \\in \\mathbb{R}^{d \\times d}$ being the weights ,\n% All operations in FourierGNN are performed in Fourier space, thus, all involving parameters, i.e., $\\{\\mathcal{S}_k,b_k\\}_{k=1}^K$, are complex numbers.\n% %The core operation of FourierGNN is the summation of recursive multiplications with non-linear activation functions. Specifically, \n% %the recursive multiplications between $\\mathcal{F}(X)$ and $\\mathcal{S}$, i.e., $\\mathcal{F}(X)\\mathcal{S}_{0:k}$, are equivalent to the %multi-order convolutions on the graph structure (see \\textit{Theoretical Analysis} below). Non-linear activation functions $\\sigma$ is introduced %to address the capability limitations of modeling non-linear information diffusion on graphs by the linear summation. \n\n% The core operation of FourierGNN is the summation of recursive multiplications with non-linear activation functions. Note that the recursive multiplications between $\\mathcal{F}(X)$ and $\\mathcal{S}$, i.e., $\\mathcal{F}(X)\\mathcal{S}_{0:k}$ with $\\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i$, are equivalent to the multi-order convolutions on the graph structure (see \\textit{Theoretical Analysis} below). Non-linear activation functions $\\sigma$ is introduced to address the capability limitations of modeling non-linear information diffusion on graphs by the linear summation. \n\n% \\paragraph{Theoretical Analysis} \\label{sec:theoretical_analysis}\n% %We theoretically analyze the effectiveness of our proposed FourierGNN and verify the validity of its architecture. \n% %For the convenience of analysis, we exclude the non-linear activation function $\\sigma$ and learnable biases parameters $b$ from Equation %(\\ref{equ:fourier_gnn}), and focus on the main operations $\\mathcal{F}(X)\\mathcal{S}_{0:k}$ of the right part in the equation.\n% We theoretically analyze the effectiveness and interpretability of FourierGNN and verify the validity of its architecture. For convenience, we exclude the non-linear activation function $\\sigma$ and learnable biases parameters $b$ from Equation (\\ref{equ:fourier_gnn}), and focus on $\\mathcal{F}(X)\\mathcal{S}_{0:k}$.% of the right part in the equation.\n% \\begin{myPro}%[\\textbf{Equivalence}]\n% \\label{pro:filter}\n% Given a graph $G=(X, A)$ with the node features $X \\in \\mathbb{R}^{n \\times d}$ and the adjacency matrix $A\\in \\mathbb{R}^{n \\times n}$, the recursive multiplication of FGOs in Fourier space is equivalent to multi-order convolutions in the time domain:\n% \\begin{equation} \\label{equ:fgso_filter}\n%     \\mathcal{F}^{-1}({\\mathcal{F}(X)\\mathcal{S}_{0:k}})= A_{k:0}{X}W_{0:k}, \\quad s.t.\\ \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i,  A_{k:0}=\\prod_{i=0}^k{A}_i,  W_{0:k}=\\prod_{i=0}^k{W}_i\n% \\end{equation}\n% where $A_0$ and $\\mathcal{S}_0$ are the identity matrix, $A_k\\in\\mathbb{R}^{n\\times n}$ corresponds to the $k$-th diffusion step sharing the same sparsity pattern of $A$, $W_{k} \\in \\mathbb{R}^{d \\times d}$ is the $k$-th weight matrix, $\\mathcal{S}_k\\in\\mathbb{C}^{d\\times d}$ is the $k$-th FGO satisfying $\\mathcal{F}(A_k {X} W_k)=\\mathcal{F}({X})\\mathcal{S}_k$, and $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denotes DFT and its inverse, respectively.\n% %where $A_0$ denotes the identity matrix, $A_k\\in\\mathbb{R}^{n\\times n}$ corresponds to the $k$-th diffusion step sharing the sparsity pattern of $A$, $W_{k} \\in \\mathbb{R}^{d \\times d}$ is the $k$-th weight matrix, $\\mathcal{S}_k\\in\\mathbb{C}^{d\\times d}$ is the $k$-th FGO satisfying $\\mathcal{F}(A_k {X} W_k)=\\mathcal{F}({X})\\mathcal{S}_k$, and $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denotes DFT and its inverse, respectively.\n% \\end{myPro}\n% \\begin{myProof}\n% We transform the right part in Equation (\\ref{equ:fgso_filter}) to Fourier space by $\\mathcal{F}$ as:\n% \\begin{equation}\n%     \\mathcal{F}(A_{k:0}{X}W_{0:k})=\\mathcal{F}(A_kA_{k-1}\\cdots XW_0 \\cdots W_{k-1}W_k). \\nonumber \n% \\end{equation}\n% Let $Z=A_{k-1}\\cdots XW_0\\cdots W_{k-1}$, then \n% \\begin{equation}\n%     \\mathcal{F}(A_{k:0}{X}W_{0:k})=\\mathcal{F}(A_kZW_k). \\nonumber\n% \\end{equation}\n% According to Equation (\\ref{equ:gcn_unit}), \n% \\begin{equation}\n%     \\mathcal{F}(A_kZW_k)=\\mathcal{F}(Z)\\mathcal{S}_k=\\mathcal{F}(A_{k-1}\\cdots XW_0\\cdots W_{k-1})\\mathcal{S}_k. \\nonumber\n% \\end{equation}\n% And so on,\n% \\begin{equation}\n% \\begin{aligned}\n%     \\mathcal{F}(A_{k-1}\\cdots XW_0 \\cdots W_{k-1})\\mathcal{S}_k \\nonumber &= \\mathcal{F}(A_{k-1}(A_{k-2}\\cdots XW_0\\ldots W_{k-2})W_{k-1})\\mathcal{S}_k\\nonumber\\\\\n%                                                                  &= \\mathcal{F}(A_{k-2}\\cdots XW_0\\ldots W_{k-2})\\mathcal{S}_{k-1}\\mathcal{S}_k \\nonumber\\\\\n%                                                                  &=\\cdots \\nonumber\\\\\n%                                                                  &=\\mathcal{F}(X)\\mathcal{S}_0\\cdots\\mathcal{S}_k.\\nonumber\n% \\end{aligned}\n% \\end{equation}\n% Accordingly, we have $\\mathcal{F}(A_{k:0}{X}W_{0:k})=\\mathcal{F}(X)\\mathcal{S}_0 \\cdots \\mathcal{S}_k=\\mathcal{F}(X)\\mathcal{S}_{0:k}$ proved.\n\n% \\end{myProof}\n% The operation $A_{k:0}{X}W_{0:k}$ adopts different weights $W_k\\in \\mathbb{R}^{d \\times d}$ to measure the information of different neighbors in each diffusion order, which is beneficial to capture the time-varying counterparts~\\cite{MateosSMR19,SegarraMMR17,isufi2021edgenets}. This shows that FourierGNN is effective in attending to time-varying correlations among the nodes in a graph. \n% %Furthermore, the diffusion can be a recursive  which \nAccording to the convolution theorem~\\cite{1970An} (see Appendix \\ref{convolution_theorem_appendix}), we can write the multiplication between $\\mathcal{F}({X})$ and FGO $\\mathcal{S}_{A,W}$ in Fourier space as:\n\\begin{equation}\\label{equ:fgso}\n    \\mathcal{F}({X})\\mathcal{F}(\\kappa)=\\mathcal{F}((X*\\kappa)[i])=\\mathcal{F}(\\sum_{j=1}^n {{X}}[j]\\kappa[i-j])=\\mathcal{F}(\\sum_{j=1}^n {{X}}[j]\\kappa[i,j]), \\quad\\quad   \\forall i \\in [n]\n\\end{equation}\nwhere $(X*\\kappa)[i]$ denotes the convolution of $X$ and $\\kappa$. As defined $\\kappa[i,j]=A_{ij}\\circ {W}$, it yields $\\sum_{j=1}^n {X}[j]\\kappa[i,j]=\\sum_{j=1}^n A_{ij}{X}[j]W=AXW$. Accordingly, we can get the convolution equation:\n\\begin{equation}\\label{equ:gcn_unit}\n    \\mathcal{F}(X)\\mathcal{S}_{A,W} = \\mathcal{F}(AXW).\n\\end{equation}\nIn particular, turning to our case of the fully-connected hypervariate graph, we can adopt a $n$-invariant FGO $\\mathcal{S}\\in\\mathbb{C}^{d \\times d}$ that has a computationally low cost compared to previous $\\mathbb{C}^{n \\times d\\times d}$. We provide more details and explanations  in Appendix \\ref{intp_FGO}.%For simplicity, we denote the frequency-invariant FGO as  in the following.\n\n%the Fourier transform of a convolution of two sequences equals to the pointwise product of their Fourier transforms.\n\nFrom Equation (\\ref{equ:gcn_unit}), we can observe that performing the multiplication between $\\mathcal{F}(X)$ and FGO $\\mathcal{S}$ in Fourier space corresponds to a graph shift operation (i.e., a graph convolution) in the time domain~\\cite{MateosSMR19}. Since the multiplications in Fourier space ($\\mathcal{O}(n)$) have much lower complexity than the above shift operations ($\\mathcal{O}(n^2)$) in the time domain (See \\textit{Complexity Analysis} below), it motivates us to develop a highly efficient graph neural network in Fourier space.\n\n%The recursive composition has a nice property that $\\mathcal{S}_{0:k}=\\mathcal{S}_{0:k-1}\\mathcal{S}_k$, which inspires us to design a complex-valued feed forward network with $\\mathcal{S}_k$ being the complex weights for the $k$-th layer. However, both the FGSO and edge-variant graph filter are linear transformations, limiting the capability of modeling non-linear information diffusion on graphs. Following by the convention in GNNs, we introduce the non-linear activation and biases to reformulate the $k$-the layer as follows: \n%\\begin{equation}\n%    \\label{equ:layer}\n%    \\mathcal{X}_k = \\sigma(\\mathcal{X}_{k-1}\\mathcal{A}_k+b_k)\n%\\end{equation}\n%with the complex weight $\\mathcal{A}_k \\in \\mathbb{C}^{d\\times d}$, biases $b_k \\in \\mathbb{C}^{d}$ and the activation function $\\sigma$.\nTo this end, we propose the \\textit{Fourier Graph Neural Networks} (FourierGNN) based on FGO. Specifically, by stacking multiple layers of FGOs, we can define the $K$-layer Fourier graph neural networks given a graph $G = (X,A)$ with node features $X\\in \\mathbb{R}^{n \\times d}$ and the adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$  as: \n\\begin{equation} \\label{equ:fourier_gnn}\n\\operatorname{FourierGNN}(X, A) :=\n%\\mathcal{F}(X)\\mathcal{S}_0+\n    \\sum_{k=0}^{K}\\sigma(\\mathcal{F}(X)\\mathcal{S}_{0:k}+b_k),  \\quad \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i.\n    %\\sum_{k=0}^K \\mathcal{X}_{k} \\quad s.t.\\ \\mathcal{X}_k=\\sigma(\\mathcal{X}_{k-1}\\mathcal{S}_k+b_k)\n\\end{equation}\n%\\TODO{ include A, add more explanation with many words, F, S, b, why sum, why K-order, why multiply $S_o:k$}\nHerein, $\\mathcal{S}_k$ is the FGO in the $k$-th layer, satisfying $\\mathcal{F}(X)\\mathcal{S}_k=\\mathcal{F}(A_k X W_k)$ with $W_k \\in \\mathbb{R}^{d \\times d}$ being the weights and  $A_k\\in\\mathbb{R}^{n\\times n}$ corresponding to the $k$-th adjacency matrix sharing the same sparsity pattern of $A$, and $b_{k}\\in\\mathbb{C}^{d}$ are the complex-valued biases parameters; $\\mathcal{F}$ stands for Discrete Fourier Transform; $\\sigma$ is the activation function. In particular, $\\mathcal{S}_0$, $W_0$, $A_0$ are the identity matrix, and we adopt identical activation at $k=0$ to obtain residual $\\mathcal{F}(X)$. All operations in FourierGNN are performed in Fourier space. Thus, all parameters, i.e., $\\{\\mathcal{S}_k,b_k\\}_{k=1}^K$, are complex numbers.\n\nThe core operation of FourierGNN is the summation of recursive multiplications with nonlinear activation functions. Specifically, \nthe recursive multiplications between $\\mathcal{F}(X)$ and $\\mathcal{S}$, i.e., $\\mathcal{F}(X)\\mathcal{S}_{0:k}$, are equivalent to the multi-order convolutions on the graph structure (see \\textit{Theoretical Analysis} below). Nonlinear activation functions $\\sigma$ are introduced to address the capability limitations of modeling nonlinear information diffusion on graphs in the summation.\n\n\n% The core operation of FourierGNN is the summation of recursive multiplications with non-linear activation functions. Note that the recursive multiplications between $\\mathcal{F}(X)$ and $\\mathcal{S}$, i.e., $\\mathcal{F}(X)\\mathcal{S}_{0:k}$ with $\\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i$, are equivalent to the multi-order convolutions on the graph structure (see \\textit{Theoretical Analysis} below). Non-linear activation functions $\\sigma$ is introduced to address the capability limitations of modeling non-linear information diffusion on graphs.% by the linear summation. \n\\paragraph{Theoretical Analysis} \\label{sec:theoretical_analysis}\nWe theoretically analyze the effectiveness and interpretability of FourierGNN and verify the validity of its architecture. For convenience, we exclude the non-linear activation function $\\sigma$ and learnable bias parameters $b$ from Equation (\\ref{equ:fourier_gnn}), and focus on $\\mathcal{F}(X)\\mathcal{S}_{0:k}$.% of the right part in the equation.\n\n\\begin{myPro}%[\\textbf{Equivalence}]\n\\label{pro:filter}\nGiven a graph $G=(X, A)$ with node features $X \\in \\mathbb{R}^{n \\times d}$ and adjacency matrix $A\\in \\mathbb{R}^{n \\times n}$, the recursive multiplication of FGOs in Fourier space is equivalent to multi-order convolutions in the time domain:\n\\begin{equation} \\label{equ:fgso_filter}\n    \\mathcal{F}^{-1}({\\mathcal{F}(X)\\mathcal{S}_{0:k}})= A_{k:0}{X}W_{0:k}, \\quad \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i,  A_{k:0}=\\prod_{i=k}^0 {A}_i,  W_{0:k}=\\prod_{i=0}^k{W}_i\n\\end{equation}\nwhere $A_0, \\mathcal{S}_0, W_0$ are the identity matrix, $A_k\\in\\mathbb{R}^{n\\times n}$ corresponds to the $k$-th diffusion step sharing the same sparsity pattern of $A$, $W_{k} \\in \\mathbb{R}^{d \\times d}$ is the $k$-th weight matrix, $\\mathcal{S}_k\\in\\mathbb{C}^{d\\times d}$ is the $k$-th FGO satisfying $\\mathcal{F}(A_k {X} W_k)=\\mathcal{F}({X})\\mathcal{S}_k$, and $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denote DFT and its inverse, respectively.\n\\end{myPro}\n% In the time domain, operation $A_{k:0}{X}W_{0:k}$ adopts different weights $W_k\\in \\mathbb{R}^{d \\times d}$ to weigh the information of different neighbors in different diffusion orders, beneficial to capture the time-varying counterparts on graphs~\\cite{MateosSMR19,SegarraMMR17,isufi2021edgenets}. \n% This indicates FourierGNN is effective in attending to time-varying correlations among graph nodes, e.g., spatiotemporal dependencies in hypervariate graphs. \n% The proof of Proposition 1 and more explanations of FourierGNN are provided in Appendix \\ref{sec:filter}.\nIn the time domain, operation $A_{k:0}{X}W_{0:k}$ adopts different weights $W_k\\in \\mathbb{R}^{d \\times d}$ to weigh the information of different neighbors in different diffusion orders, beneficial to capture the extensive dependencies on graphs~\\cite{MateosSMR19,SegarraMMR17,isufi2021edgenets}.\n%This enhances the expressive power of GNNs, enabling them to model complex structures in graphs more effectively.\nThis indicates FourierGNN is expressive in modeling the complex correlations among graph nodes, i.e., spatiotemporal dependencies in the hypervariate graph. \nThe proof of Proposition \\ref{pro:filter} and more explanations of FourierGNN are provided in Appendix \\ref{sec:filter}.\n\\paragraph{Complexity Analysis}\nThe time complexity of $\\mathcal{F}(X)\\mathcal{S}$ is $\\mathcal{O}(nd\\operatorname{log}n+nd^2)$, which includes the Discrete Fourier Transform (DFT), the Inverse Discrete Fourier Transform (IDFT), and the matrix multiplication in the Fourier space. Comparatively, \nthe time complexity of the equivalent operations of $\\mathcal{F}(X)\\mathcal{S}$ in the time domain, i.e., $AXW$, is $\\mathcal{O}(n^2d+nd^2)$. \nThen, as a $K$-order summation of a recursive multiplication of $\\mathcal{F}(X)\\mathcal{S}$, FourierGNN, achieves the time complexity of $\\mathcal{O}(nd\\operatorname{log}n+Knd^2)$, including DFT and IDFT, and the recursive multiplication of FGOs. \nOverall, the Log-linear $\\mathcal{O}(n\\operatorname{log}n)$ complexity makes FourierGNN much more efficient.\n%especially in our hypervariate graph with $n=NT$ nodes. \n\n\\paragraph{FourierGNN vs Other Graph Networks}\nWe analyze the connection and difference between our FourierGNN with GCN~\\cite{KipfW17} and GAT~\\cite{gat_2017}. From the complexity perspective, FourierGNN with log-linear complexity shows much higher efficiency than GCN and GAT. \nRegarding the network architecture, we analyze them from two main perspectives:\n(1) Domain. GAT implements operations in the time domain, while GCN and FourierGNN are in Fourier space. However, GCN achieves the transformation through the Graph Fourier Transform (GFT), whereas FourierGNN utilizes the Discrete Fourier Transform (DFT).\n%GAT calculates attention weights, and GCN performs Laplacian eigendecompositions. Both GAT and GCN perform operations in the time domain, while FourierGNN based on the Discrete Fourier Transform (DFT) performs operations in the Fourier domain.\n(2) Information diffusion: GAT aggregates neighbor nodes with varying weights to via attention mechanisms. FourierGNN and GCN update node information via convoluting neighbor nodes. Different from GCN, FourierGNN assigns varying importance to neighbor nodes in different diffusion steps. We provide a detailed comparison in Appendix \\ref{compares}.\n\n% We analyze the connection and difference between our FourierGNN with other graph neural networks, including GCN~\\cite{KipfW17} and GAT~\\cite{gat_2017}. From the complexity perspective, FourierGNN with log-linear complexity shows much higher efficiency than GCN and GAT. \n% Regarding the network architecture, we analyze them from two main perspectives:\n% (1) Domain. GAT implements operations in the spatial domain, while GCN and FourierGNN conduct operations in Fourier space. However, GCN achieves the transformation through the Graph Fourier Transform (GFT), whereas FourierGNN utilizes the Discrete Fourier Transform (DFT).\n% (2) Information diffusion: GAT relies on attention mechanisms to adaptively assign weights to neighboring nodes.\n% FourierGNN and GCN update node information via convoluting neighboring nodes. Different from GCN, FourierGNN assigns varying importance to nodes of the same neighborhood in different diffusion steps.\n% We include a more detailed comparison in Appendix \\ref{compares}.\n\n% to connect all observed values $\\{x_{it}\\}$ among variables and timestamps\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Multivariate Time Series Forecasting with FourierGNN",
                    "content": " \\label{sec:ovarc}\nIn this section, we instantiate FourierGNN for MTS forecasting.\nThe overall architecture of our model is illustrated in Figure \\ref{fig:model_fig}. Given the MTS input data $X_t \\in \\mathbb{R}^{N\\times T}$, first we construct a fully-connected \\textit{hypervariate graph} $\\mathcal{G}_t=(X^{\\mathcal{G}}_t, A^{\\mathcal{G}}_t)$ with $X^{\\mathcal{G}}_t \\in \\mathbb{R}^{NT \\times 1}$ and $A^\\mathcal{G}_t \\in \\{1\\}^{n\\times n}$. Then, we project $X^\\mathcal{G}_t$ into node embeddings $\\mathbf{X}^{\\mathcal{G}}_t \\in \\mathbb{R}^{NT\\times d}$ by assigning a $d$-dimension vector for each node using an embedding matrix $E_\\phi\\in\\mathbb{R}^{1\\times d}$, i.e., $\\mathbf{X}^{\\mathcal{G}}_t=X^{\\mathcal{G}}_t \\times E_\\phi$. \n\nSubsequently, to capture the spatiotemporal dependencies simultaneously, we aim to feed multiple embeded hypervariate graphs with $\\mathbf{X}^{\\mathcal{G}}_t$ to {FourierGNN}. First, we perform Discrete Fourier Transform (DFT) $\\mathcal{F}$ on each discrete spatio-temporal dimension of the embeddings $\\mathbf{X}^{\\mathcal{G}}_t$ and obtain the frequency output $\\mathcal{X}^{\\mathcal{G}}_t:=\\mathcal{F}(\\mathbf{X}^{\\mathcal{G}}_t) \\in \\mathbb{C}^{NT\\times d}$. Then, we perform a recursive multiplication between $\\mathcal{X}_t^{\\mathcal{G}}$ and FGOs $\\mathcal{S}_{0:k}$ in Fourier space and output the resulting representations $\\mathcal{Y}^{\\mathcal{G}}_t$ as:\n\\begin{equation} \n    \\mathcal{Y}^{\\mathcal{G}}_t = \\operatorname{FourierGNN}(\\textbf{X}_t^{\\mathcal{G}},A_t^{\\mathcal{G}}) =\n    \\sum_{k=0}^{K}\\sigma(\\mathcal{F}(\\mathbf{X}_t^{\\mathcal{G}})\\mathcal{S}_{0:k}+b_k), \\quad \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i.\n\\end{equation}\nThen $\\mathcal{Y}^{\\mathcal{G}}_t$ are transformed back to the time domain using Inverse Discrete Fourier Transform (IDFT) $\\mathcal{F}^{-1}$, which yields $\\mathbf{Y}^{\\mathcal{G}}_t:=\\mathcal{F}^{-1}(\\mathcal{Y}^{\\mathcal{G}}_t)\\in \\mathbb{R}^{NT\\times d}$.\n\nFinally, according to the FourierGNN output $\\mathbf{Y}^{\\mathcal{G}}_t$ which encodes spatiotemporal inter-dependencies, we use two layer feed-forward networks (FFN) (see Appendix \\ref{subsec:es} for more details) to project it onto $\\tau$ future steps, resulting in $\\hat{Y}_t = \\operatorname{FFN}(\\mathbf{Y}^{\\mathcal{G}}_t) \\in \\mathbb{R}^{N\\times \\tau}$.\n\\vspace{-2mm}\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\nTo evaluate the performance of FourierGNN, we conduct extensive experiments on seven real-world time series benchmarks to compare with state-of-the-art graph neural network-based methods. \n\\vspace{-2mm}\n",
                "subsection 5.1": {
                    "name": "Experimental Setup",
                    "content": "\n\\label{subsec:setup}\n\\textbf{Datasets}. We evaluate our proposed method on seven representative datasets from various application scenarios, including traffic, energy, web traffic, electrocardiogram, and COVID-19. All datasets are normalized using the min-max normalization. Except the COVID-19 dataset, we split the other datasets into training, validation, and test sets with the ratio of 7:2:1 in a chronological order. For the COVID-19 dataset, the ratio is 6:2:2. More detailed information about datasets are in Appendix \\ref{appendix_datasets}.\n\n\n\n\n\n% \\begin{table}[ht]\n%     \\centering\n%     \\caption{Summary of datasets.}\n%     \\resizebox{\\textwidth}{12mm}{\n%     \\begin{tabular}{c c c c c c c c}\n%     \\toprule\n%     Datasets & Solar & Wiki & Traffic & ECG & Electricity & COVID-19 & METR-LA\\\\\n%     \\midrule\n%       Samples & 3650 & 803& 10560& 5000 & 140211 & 335 & 34272\\\\\n%       Variables & 592 & 2000& 963& 140 & 370 & 55 & 207 \\\\\n%       Granularity & 1hour & 1day& 1hour& - & 15min& 1day & 5min\\\\\n%       Start time & 01/01/2006 & 01/07/2015&01/01/2015 & - &01/01/2011 & 01/02/2020 & 01/03/2012\\\\\n%     \\bottomrule\n%     \\end{tabular}}\n%     \\label{tab:datasets}\n% \\end{table}\n\n\\textbf{Baselines}. We conduct a comprehensive comparison of the forecasting performance between our FourierGNN and several representative and state-of-the-art (SOTA) models on the seven datasets, including classic method VAR \\cite{Vector1993}, deep learning-based models such as SFM \\cite{ZhangAQ17}, LSTNet \\cite{Lai2018}, TCN \\cite{bai2018}, DeepGLO \\cite{Sen2019}, and CoST~\\cite{cost22}. We also compare FourierGNN against GNN-based models like GraphWaveNet \\cite{zonghanwu2019}, StemGNN \\cite{Cao2020}, MTGNN \\cite{wu2020connecting}, and AGCRN \\cite{Bai2020nips}, and two representative Transformer-based models like Reformer \\cite{reformer20} and Informer \\cite{Zhou2021}, as well as two frequency enhanced Transformer-based models including Autoformer \\cite{autoformer21} and FEDformer \\cite{fedformer22}. In addition, we compare FourierGNN with SOTA models such as TAMP-S2GCNets \\cite{tampsgcnets2022}, DCRNN \\cite{LiYS018}, and STGCN \\cite{Yu2018}, which require pre-defined graph structures. %For further implementation details of the adopted baselines, please refer to Appendix \\ref{subsec:baselines}.\nPlease refer to Appendix \\ref{subsec:baselines} for more implementation details of the adopted baselines.\n\n\\textbf{Experimental Settings}. All experiments are conducted in Python using Pytorch 1.8~\\cite{PYTORCH19} (except for SFM~\\cite{ZhangAQ17} which uses Keras) and performed on single NVIDIA RTX 3080 10G GPU. Our model is trained using RMSProp with a learning rate of $10^{-5}$ and MSE (Mean Squared Error) as the loss function. The best parameters for all comparative models are chosen through careful parameter tuning on the validation set. We use Mean Absolute Errors (MAE), Root Mean Squared Errors (RMSE), and Mean Absolute Percentage Error (MAPE) to measure the performance. The evaluation details are in Appendix \\ref{appendix_metrics} and more experimental settings are in Appendix \\ref{subsec:es}.\n\\vspace{-3mm}\n%More details about the datasets, baselines and experimental settings can be found in Appendix \\ref{Experiment_detail}.\n\n"
                },
                "subsection 5.2": {
                    "name": "Main Results",
                    "content": "\nWe present the evaluation results with an input length of 12 and a prediction length of 12 in Table \\ref{tab:result}. Overall, FourierGNN achieves a new state-of-the-art on all datasets. On average, FourierGNN makes an improvement of 9.4\\% in MAE and 10.9\\% in RMSE compared to the best-performing across all datasets. \nAmong these baselines, Reformer, Informer, Autoformer, and FEDformer are Transformer-based models that demonstrate competitive performance on Electricity and COVID-19 datasets, as they excel at capturing temporal dependencies. \n% \\begin{wrapfigure}{r}{0.55\\linewidth}\n% \\centering\n% \\vspace{-5mm}\n%     \\centering\n%     \\subfigure[MAE]\n%     {\n%         \\centering\n%         \\includegraphics[width=0.47\\linewidth]{figures/ecg_mae_multistep.png}\n%     }\n%     \\hspace{-4mm}\n%     \\subfigure[RMSE]\n%     {\n%         \\centering\n%         \\includegraphics[width=0.47\\linewidth]{figures/ecg_rmse_multistep.png}\n%     }\n%  \\caption{Performance comparison in different prediction lengths on the ECG dataset.}\n% \\label{fig:ecg_multistep}\n%  \\vspace{-2mm}\n% \\end{wrapfigure} \nHowever, they have limitations in capturing the spatial dependencies explicitly.\nGraphWaveNet, MTGNN, StemGNN, and AGCRN are GNN-based models that show promising results on Wiki, Traffic, Solar, and ECG datasets, primarily due to their capability to handle spatial dependencies among variates. \nHowever, they are limited in their capacity to simultaneously capture spatiotemporal\ndependencies. FourierGNN outperforms the baseline models since it can learn comprehensive spatiotemporal dependencies simultaneously and attends to time-varying dependencies among variates. \n\\vspace{-2mm}\n%In addition, we compare FourierGNN with the baseline models under the different prediction lengths on the ECG dataset, as shown in Figure  \\ref{fig:ecg_multistep}. It reports that FourierGNN achieves the best performances (MAE and RMSE) for all prediction lengths. \n\n% To further evaluate the performance of our model FourierGNN in multi-step forecasting, we conduct more experiments on the COVID-19. We compare FourierGNN with other GNN-based MTS models (including StemGNN, AGCRN, GraphWaveNet, MTGNN and TAMP-S2GCNets) and representation learning model (CoST) on the COVID-19 dataset under different prediction lengths, and the results are shown in Table \\ref{tab:covid_result}. From the table, we can find that FourierGNN achieves the best MAE and RMSE on all the prediction lengths. On average, FourierGNN has 30.0\\% and 27.9\\% improvement on MAE and RMSE respectively over the best baseline, i.e., TAMP-S2GCNets. Among these models, TAMP-S2GCNets requiring a pre-defined graph topology achieves competitive performance since it enhances the resultant graph learning mechanisms with a multi-persistence. However, it constructs the graph in the spatial dimension, while our model FourierGNN adaptively learns a supra-graph connecting any two variables at any two timestamps, which is effective and more powerful to capture high-resolution spatial-temporal dependencies.\n\n\n\\paragraph{Multi-Step Forecasting} \nTo further evaluate the performance in multi-step forecasting, we compare FourierGNN with other GNN-based MTS models (including StemGNN~\\cite{Cao2020}, AGCRN~\\cite{Bai2020nips}, GraphWaveNet~\\cite{zonghanwu2019}, MTGNN~\\cite{wu2020connecting}, and TAMP-S2GCNets~\\cite{tampsgcnets2022}) and a representation learning model (CoST~\\cite{cost22}) on COVID-19 dataset under different prediction lengths, and the results are shown in Table \\ref{tab:covid_result}. It shows that FourierGNN achieves an average 30.1\\% and 30.2\\% improvement on MAE and RMSE respectively over the best baseline. In Appendix \\ref{more_results}, we include more experiments and analysis under different prediction lengths, and further compare FourierGNN with models that require pre-defined graph structures.\n\\vspace{-2mm}\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Model Analysis",
                    "content": "\n\\label{sec:analysis}\n%Table \\ref{tab:time_complexity} gives the theoretical analysis of the time complexity and memory usage of our model. \n\n\\paragraph{Efficiency Analysis} We investigate the parameter volumes and training time costs of FourierGNN, StemGNN \\cite{Cao2020}, AGCRN \\cite{Bai2020nips}, GraphWaveNet \\cite{zonghanwu2019}, and MTGNN \\cite{wu2020connecting} on two representative datasets, including the Wiki dataset and the Traffic dataset. The results are reported in Table \\ref{tab:efficiency}, showing the comparison of parameter volumes and average time costs over five rounds of experiments. In terms of parameters, FourierGNN exhibits the lowest volume of parameters among the comparative models. Specifically, it achieves a reduction of 32.2\\% and 9.5\\% in parameters  compared to GraphWaveNet on Traffic and Wiki datasets, respectively. This reduction is mainly attributed that FourierGNN has shared scale-free parameters for each node. Regarding training time, FourierGNN runs much faster than all baseline models, and it demonstrates efficiency improvements of 5.8\\% and 23.3\\% over the fast baseline GraphWaveNet on Traffic and Wiki datasets, respectively. Considering variate number of Wiki dataset is about twice larger than that of Traffic dataset, FourierGNN exhibits larger efficiency superiority with the baselines. These findings highlight the high efficiency of FourierGNN in computing graph operations and its scalability to large datasets with extensive graphs, which is important for the pure graph formulation due to the larger size of hypervariate graphs with $NT$ nodes. \n%{Importantly}, it should be noted that the hypervariate graph in FourierGNN has $NT$ nodes, which is significantly larger than the graphs in the baseline models with only $N$ nodes.\n\n\n\n\\paragraph{Ablation Study} We perform an ablation study on the METR-LA dataset to assess the individual contributions of different components in FourierGNN. The results, presented in Table \\ref{tab:metr_ablation}, validate the effectiveness of each component. Specifically, \\textbf{w/o Embedding} emphasizes the significance of performing node embedding to improve model generalization. \\textbf{w/o Dynamic FGO} using the same FGO verifies the effectiveness of applying different FGOs in capturing time-varying dependencies. In addition, \\textbf{w/o Residual} represents FourierGNN without the $K=0$ layer, while \\textbf{w/o Summation} adopts the last order (layer) output, i.e., $\\mathcal{X}S_{0:K}$, as the output of FourierGNN. These results demonstrate the importance of high-order diffusion and the contribution of multi-order diffusion. More results and analysis of the ablation study are provided in Appendix \\ref{appendix_ablation_study}.\n\\vspace{-2mm}\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Visualization",
                    "content": "\n To gain a better understanding of the hypervariate graph and FourierGNN in spatiotemporal modeling for MTS forecasting, \n We conduct visualization experiments on the METR-LA and COVID-19 datasets. Please refer to Appendix \\ref{sec:visual_details} for more information on the visualization techniques used.\n\n\n\n\\textbf{Visualization of temporal representations learned by FourierGNN}\\quad In order to showcase the temporal dependencies learning capability of FourierGNN, we visualize the temporal adjacency matrix of different variates. \nSpecifically, we randomly select $8$ counties from the COVID-19 dataset and calculate the relations of $12$ consecutive time steps for each county. Then, we visualize the adjacency matrix by a heatmap, and the results are illustrated in Figure \\ref{fig:TIME_ADJ}, where $N$ denotes the index of the country (variate). It shows that FourierGNN learns distinct temporal patterns for each county, indicating that the hypervariate graph can encode rich and discriminative temporal dependencies. \n\n\\textbf{Visualization of spatial representations learned by FourierGNN} \\quad To investigate the spatial correlations learning capability of FourierGNN, we visualize the generated adjacency matrix based on the representations learned by FourierGNN on the METR-LA dataset. Specifically, we randomly select 20 detectors and visualize their corresponding adjacency matrix via a heatmap, as depicted in Figure \n\\ref{fig:visual_metr}. \n\nBy examining the adjacency matrix in conjunction with the actual road map, we observe: 1) the detectors (7, 8, 9, 11, 13, 18) are very close w.r.t. the physical distance, corresponding to the high values of their correlations with each other in the heatmap; 2) the detectors 4, 14 and 16 have small overall correlation values since they are far from other detectors; 3) however, compared with detectors 14 and 16, the detector 4 has slightly higher correlation values to other detectors, e.g., 7, 8, 9, which is because although they are far apart, the detectors 4, 7, 8, 9 are on the same road. The results verify that the hypervariate graph structure can represent highly interpretative correlations.\n\nMoreover, to gain a understanding of how FGO works, we visualize the output of each layer of FourierGNN, and the visualization results demonstrate that FGO can adaptively and effectively capture important patterns while removing noises to a learn discriminative model. Further details can be found in  Appendix \\ref{FGO_visualization}.\nAdditionally, to investigate the ability of FourierGNN to capture time-varying dependencies among variates, we further visualize the spatial correlations at different timestamps. The results illustrate that FourierGNN can effectively attend to the temporal variability in the data. For more information, please refer to Appendix \\ref{time_varying_visualization}.\n\n% Based on the insights gained from these visualization results, we can conclude that the hypervariate graph structure exhibits strong capabilities to encode spatiotemporal dependencies. By incorporating FGOs, FourierGNN can effectively attend to and exploit the time-varying dependencies among variates. The synergy between the hypervariate graph structure and FGOs empowers FourierGNN to capture and model intricate spatiotemporal relationships with remarkable effectiveness.\n\\vspace{-2mm}\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion Remarks",
                "content": "\n\\label{sec:conc}\n\\vspace{-3mm}\nIn this paper, we explore an interesting direction of directly applying graph networks for MTS forecasting from a pure graph perspective. To overcome the previous separate spatial and temporal modeling problem, we build a hypervariate graph, regarding each series value as a graph node, which considers spatiotemporal dynamics unitedly. Then, we formulate time series forecasting on the hypervariate graph and propose FourierGNN by stacking Fourier Graph Operator (FGO) to perform matrix multiplications in Fourier space, which can accommodate adequate learning expressiveness with much lower complexity.\nExtensive experiments demonstrate that FourierGNN achieves state-of-the-art performances with higher efficiency and fewer parameters, and the hypervariate graph structure exhibits strong capabilities to encode spatiotemporal inter-dependencies.\n\n%In this paper, we define a Fourier Graph Operator (FGO) and construct the efficient Fourier graph neural networks (FourierGNN) for MTS forecasting. FourierGNN is adopted to \n%learn the spatial-temporal dependencies simultaneously on a supra-graph for MTS forecasting, \n% simultaneously capture high-resolution spatiotemporal dependencies \n\n%and account for time-varying variate dependencies. This study makes the first attempt to design a complex-valued feed-forward network in the Fourier space for efficiently computing multi-layer graph convolutions. Extensive experiments demonstrate that FourierGNN achieves state-of-the-art performances with higher efficiency and fewer parameters and shows high interpretability in graph representation learning. This study sheds light on efficiently calculating graph operations in Fourier space by learning a Fourier graph shift operator.\n %and constructing Fourier graph networks (FGN).\n\n\n%\\section{Case Study}\n%To further investigate\n\n\\begin{ack}\nThe work was supported in part by the National Key Research and Development Program of China under Grant 2020AAA0104903 and 2019YFB1406300,\nand National Natural Science Foundation of China under Grant 62072039 and 62272048.\n\\end{ack}\n\n\\bibliography{ref}\n\\bibliographystyle{unsrt}\n\n\\newpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Notation",
                "content": "\n\n\n"
            },
            "section 8": {
                "name": "Convolution Theorem",
                "content": " \\label{convolution_theorem_appendix}\n\nThe convolution theorem \\cite{1970An} is one of the most important properties of the Fourier transform. It states the Fourier transform of a convolution of two signals equals the pointwise product of their Fourier transforms in the frequency domain. Given a signal $x[n]$ and a filter $h[n]$, the convolution theorem can be defined as follows:\n\\begin{equation}\\label{convolution_theorem}\n    \\mathcal{F}((x*h)[n])=\\mathcal{F}(x)\\mathcal{F}(h)\n\\end{equation}\nwhere $(x*h)[n]=\\sum_{m=0}^{N-1} h[m]x[(n-m)_N]$ denotes the convolution of $x$ and $h$, $(n-m)_N$ denotes $(n-m)$ modulo N, and $\\mathcal{F}(x)$ and $\\mathcal{F}(h)$ denote discrete Fourier transform of $x[n]$ and $h[n]$, respectively.\n\n"
            },
            "section 9": {
                "name": "Explanations and Proofs",
                "content": " \\label{explanation}\n",
                "subsection 9.1": {
                    "name": "The Explanations of the Hypervariate Graph Structure",
                    "content": " \\label{analysis_hypervariate_graph}\nNote that the time lag effect between time-series variables is a common phenomenon in real-world multivariate time series scenarios, for example, the time lag influence between two financial assets (e.g. dollar and gold) of a portfolio. It is beneficial but challenging to consider dependencies between different variables under different timestamps.\n\nThe hypervariate graph connecting any two variables at any two timestamps aims to encode high-resolution spatiotemporal dependencies. It embodies not only the intra-series temporal dependencies (node connections of each individual variable), inter-series spatial dependencies (node connections under each single time step), and also the time-varying spatiotemporal dependencies (node connections between different variables at different time steps).\nBy leveraging the hypervariate graph structure, we can effectively learn the spatial and temporal dependencies. This approach is distinct from previous methods that represent the spatial and temporal dependencies separately using two network structures.\n\n%Although the hypervariate graph can represent high-resolution spatiotemporal dependencies, fully connecting all space-time nodes may also introduce noise. However, in this paper, we conduct graph convolutions in Fourier space. \n%The Fourier transform possesses a property called energy compaction, which enables the concentration of most of the signal's energy into a small number of coefficients. This property allows for reducing the impact of noise on the signal.\n\n%Therefore, combining the hypervariate graph structure and Fourier transform, we can effectively \n\n% \\begin{myDef}[\\textbf{Fourier Graph Operator}]\n% \\label{def:fgso}\n% Given a graph $G=({X}, A)$ with node features ${X}\\in\\mathbb{R}^{n\\times d}$ and the adjacency matrix $A \\in\\mathbb{R}^{n\\times n}$, where $n$ is the number of nodes and $d$ is the number of features, we introduce a weight matrix $W\\in\\mathbb{R}^{d\\times d}$ to acquire a tailored Green\u2019s kernel $\\kappa: [n]\\times [n] \\xrightarrow{} \\mathbb{R}^{d\\times d}$ with $\\kappa[i,j]:=A_{ij}\\circ {W}$ and $\\kappa[i,j]=\\kappa[i-j]$. We define $\\mathcal{S}_{A, W}:=\\mathcal{F}(\\kappa)\\in\\mathbb{C}^{n\\times d \\times d}$ as a Fourier Graph Operator (FGO), where $\\mathcal{F}$ denotes Discrete Fourier Transform (DFT).\n% \\end{myDef}\n\n"
                },
                "subsection 9.2": {
                    "name": "The Interpretation of $n$-invariant FGO",
                    "content": "\n\\label{intp_FGO}\n\\textbf{Why $\\mathcal{F}(\\kappa)\\in\\mathbb{C}^{n\\times d\\times d}$?} From Definition \\ref{def:fgso}, we know that the kernel $\\kappa$ is defined as a matrix-valued projection, i.e., $\\kappa: [n]\\times [n] \\xrightarrow{} \\mathbb{R}^{d\\times d}$. Note that we assume $\\kappa$ is in the special case of the Green's kernel, i.e., a  translation-invariant kernel $\\kappa[i,j]=\\kappa[i-j]$. Accordingly, $\\kappa$ can be reduced: $\\kappa: [n] \\xrightarrow{} \\mathbb{R}^{d\\times d}$ where we can parameterize $\\mathcal{F}(\\kappa)$ with a complex-valued matrix $\\mathbb{C}^{n\\times d\\times d}$.\n%From Definition \\ref{def:fgso}, we know that the kernel $\\kappa$ is defined as a matrix-valued projection, i.e., $\\kappa: [n]\\times [n] \\xrightarrow{} \\mathbb{R}^{d\\times d}$. Accordingly, we can parameterize $\\mathcal{F}(\\kappa)$ with a complex-valued matrix $\\mathbb{C}^{n\\times n\\times d\\times d}$. Note that we assume $\\kappa$ is in the special case of the Green's kernel, i.e., $\\kappa[i,j]=\\kappa[i-j]$, indicating that $\\kappa$ is shift-invariant either dimension of discrete \n\n\\textbf{What is $n$-invariant FGO?} Turning to our case of the fully-connected hypervariate graph, we can consider a special case of $\\kappa$, i.e., a space-invariant kernel $\\kappa[i,j]=\\kappa[\\varrho]$ with $\\varrho$ being a constant scalar. Accordingly, we can parameterize FGO $\\mathcal{S}$ with a $n$-invariant complex-valued matrix $\\mathbb{C}^{d \\times d}$.\n\n\\textbf{The interpretation of $n$-invariant FGO.} An $n$-invariant FGO is similar to a shared-weight convolution kernel or filter of CNNs that slide along ($[n]\\times [n]$) input features, which effectively reduces parameter volumes and saves computation costs. Note that although we adopt the same transformation (i.e., the $n$-invariant FGO) over $NT$ frequency points, we embed the raw MTS inputs in the $d$-dimension distributive space beforehand and then perform FourierGNN over MTS embeddings, which can be analogized as $d$ convolution kernels/filters in each convolutional layer in CNNs. This can ensure FourierGNN is able to learn informative features/patterns to improve its model capacity (See the following analysis of the effectiveness of $n$-invariant FGO).\n\n% the model learning capability of FourierGNN with the $n$-invariant FGOs.\n\n\\textbf{The effectiveness of $n$-invariant FGO.} In addition, the $n$-invariant parameterized FGO is empirically proven effective to improve model generalization and achieve superior forecasting performance (See the ablation study in Section \\ref{sec:analysis} for more details). Although parameterizing $\\mathcal{F}(\\kappa)\\in\\mathbb{C}^{n\\times d \\times d}$ (i.e., an $n$-variant FGO) may be more powerful and flexible than the $n$-invariant FGO in terms of forecasting performance, it introduces much more parameters and training time costs, especially in case of multi-layer FourierGNN, and may obtain inferior performance due to inadequate training or overfitting. As indicated in Table \\ref{tab:n_invariant}, the FourierGNN with the $n$-invariant FGO achieves slightly better performance than that with the $n$-variant FGO on ECG and COVID-19, respectively. Notably, the FourierGNN with the $n$-variant FGO introduces a much larger parameter volume proportional to $n$ and requires significantly more training time. In contrast, $n$-invariant FGO is $n$-agnostic and lightweight, which is a more wise and efficient alternative. These results confirm our design and verify the effectiveness and applicability of $n$-invariant FGO.\n\n%In the time domain, we embed the raw MTS inputs to improve the model learning capability, while we learn the same transformation (FGO) for all $NT$ frequency points in Fourier space (similar to CNN with shared-weight convolution kernels or filters that slide along input features). \n%It may not obtain inferior performance due to inadequate training or overfitting. As shown in Table \\ref{tab:n_invariant}, ..... \n\n\n\n% Note that the frequency spectrum in Fourier space has a global view of which each frequency point attends to all variables or timestamps. This treatment in FourierGNN guarantees the model capacity and is empirically proved superior over the treatment without embeddings and with $n$-variant FGO (please refer to the ablation study in Section \\ref{sec:analysis} for detailed results).\n\n\n%${\\mathcal{G}}_t$ with an all-one adjacency matrix $A\\in\\{1\\}^{n \\times n}$, \n\n% Since the Green's kernel $\\kappa$, satisfying $\\kappa[i,j]=\\kappa[i-j]$, is space invariant. \n\n% $\\mathcal{S}=\\mathcal{F}(\\kappa)$\n\n\n%As we mentioned above, we perform FourierGNN over the input embeddings and adopt the $n$-invariant FGOs. Relatively to directly adopting the variant FGOs ($\\mathbb{C}^{n\\times d\\times d}$), we subtly \u201cfactorize\u201d the variant parameterization to the time domain (i.e., the embedding matrix $\\Phi\\in\\mathbb{R}^{n\\times d}$) and the frequency domain (i.e., $n$-invariant FGO $\\mathbb{C}^{d\\times d}$). \n\n% In the time domain, we embed the raw MTS inputs to improve the model learning capability, while we learn the same transformation (FGO) for all $NT$ frequency points in Fourier space (similar to CNN with shared-weight convolution kernels or filters that slide along input features). \n\n% Note that the frequency spectrum in Fourier space has a global view of which each frequency point attends to all variables or timestamps. This treatment in FourierGNN guarantees the model capacity and is empirically proved superior over the treatment without embeddings and with $n$-variant FGO (please refer to the ablation study in Section \\ref{sec:analysis} for detailed results).\n\n% Although the variant parameterization may be more powerful and flexible than the $n$-invariant one, it introduces more parameters in Fourier space, especially for multi-layer FourierGNN, and may not obtain superior performance due to inadequate training or overfitting.\n\n"
                },
                "subsection 9.3": {
                    "name": "pro:filter",
                    "content": " \n\\label{sec:filter}\n\n\\begin{appendixPro}%[\\textbf{Equivalence}]\nGiven a graph $G=(X, A)$ with node features $X \\in \\mathbb{R}^{n \\times d}$ and adjacency matrix $A\\in \\mathbb{R}^{n \\times n}$, the recursive multiplication of FGOs in Fourier space is equivalent to multi-order convolutions in the time domain:\n\\begin{equation}\n    \\mathcal{F}^{-1}({\\mathcal{F}(X)\\mathcal{S}_{0:k}})= A_{k:0}{X}W_{0:k}, \\quad \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i,  A_{k:0}=\\prod_{i=k}^0 {A}_i,  W_{0:k}=\\prod_{i=0}^k{W}_i\\nonumber\n\\end{equation}\nwhere $A_0, \\mathcal{S}_0, W_0$ are the identity matrix, $A_k\\in\\mathbb{R}^{n\\times n}$ corresponds to the $k$-th diffusion step sharing the same sparsity pattern of $A$, $W_{k} \\in \\mathbb{R}^{d \\times d}$ is the $k$-th weight matrix, $\\mathcal{S}_k\\in\\mathbb{C}^{d\\times d}$ is the $k$-th FGO satisfying $\\mathcal{F}(A_k {X} W_k)=\\mathcal{F}({X})\\mathcal{S}_k$, and $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denote DFT and its inverse, respectively.\n\\end{appendixPro}\n\n\\begin{proof}\nThe proof aims to demonstrate the equivalence between the recursive multiplication of FGOs in Fourier space and multi-order convolutions in the time domain. According to $\\mathcal{F}(A_k {X} W_k)=\\mathcal{F}({X})\\mathcal{S}_k$, we expand the multi-order convolutions $A_{0:K}XW_{0:K}$ in the time domain using a set of FGOs in Fourier space:\n\n%According to the concise form of Equation \\ref{equ:gcn_unit}, i.e.,\n% \\begin{equation}\n%     \\mathcal{F}(AXW)=\\mathcal{F}(X) \\mathcal{S}\n% \\end{equation}\n% where $\\mathcal{F}$ denotes the Discrete Fourier Transform (DFT), \n%we omit the weight matrix $W$ for convenience (we can treat $W$ as an identity matrix), \n% we expand the multi-order convolutions $A_{0:K}XW_{0:K}$ in the time domain using a set of FGOs in Fourier space,\n% it yields:\n\\begin{equation}\n    \\begin{aligned}\n        \\mathcal{F}(A_KA_{K-1}\\cdots A_0XW_0\\cdots W_{K-1} W_K)&=\\mathcal{F}(A_K(A_{K-1}...A_0XW_0\\cdots W_{K-1})W_K)\\\\\n        &=\\mathcal{F}(A_{K-1}...A_0XW_0\\cdots W_{K-1}) \\mathcal{S}_K\\\\\n        &=\\mathcal{F}(A_{K-1}(A_{K-2}...A_0XW_0\\cdots W_{K-2})W_{K-1}) \\mathcal{S}_K\\\\\n        &= \\mathcal{F}(A_{K-2}...A_0XW_0\\cdots W_{K-2}) \\mathcal{S}_{K-1}\\mathcal{S}_K\\\\\n        &= \\cdots \\\\\n        &=\\mathcal{F}({X})  \\mathcal{S}_0\\cdots\\mathcal{S}_{K-1} \\mathcal{S}_{K}\\\\\n        &=\\mathcal{F}({X})\\mathcal{S}_{0:K}\n    \\end{aligned}\n\\end{equation}\n%where $\\{A_i\\}_{i=0}^K$ is a set of GSOs, and $\\{\\mathcal{A}_i\\}_{i=0}^K$ is a set of FGOs corresponding to $\\{A_i\\}_{i=0}^K$ individually.\n%and $S_2$ are GSOs and $\\mathbf{S}_1$ and $\\mathbf{S}_2$ are FGOs. Extending the above equation to a high-order case, we can easily obtain:\n% \\begin{equation}\n%     \\mathcal{F}(S_KS_{K-1}...S_0X)=\\mathcal{F}(\\mathcal{X})\\mathcal{S}_0\\cdots\\mathcal{S}_{K-1}\\mathcal{S}_{K}\n% \\end{equation} \\prod_{k=0}^KW_k\nwhere it yields $\\mathcal{F}^{-1}({\\mathcal{F}(X)\\mathcal{S}_{0:K}})= A_{K:0}{X}W_{0:K}$ with $\\mathcal{S}_{0:K}=\\prod_{i=0}^K\\mathcal{S}_i$,$A_{K:0}=\\prod_{i=K}^0 {A}_i$ and $W_{0:K}=\\prod_{i=0}^K{W}_i$. Proved.\n\\end{proof}\n\nThus, the FourierGNN can be rewritten as (for convenience, we exclude the non-linear activation function $\\sigma$ and learnable bias parameters $b$):\n\\begin{equation}\n\\begin{aligned}\n    \\mathcal{F}^{-1}(\\sum_{k=0}^{K}\\mathcal{F}(X)\\mathcal{S}_{0:K})\n    %&=\\mathcal{F}(A_0XW_0+A_1(A_0XW_0)W_1+...+A_KA_{K-1}...(A_0XW_0)...W_{K-1}W_K)\\\\\n    =A_0XW_0+A_1(A_0XW_0)W_1+...+A_{K:0}{X}W_{0:K}\n    % &+A_KA_{K-1}...(A_0XW_0)...W_{K-1}W_K\n    %\\\\&=\\sum_{k=0}^{K}\\mathcal{F}(X)\\mathcal{S}_{0:k} \\quad s.t.\\  \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i\n\\end{aligned}\n\\end{equation}\nFrom the right part of the above equation, we can observe that it assigns different weights to weigh the information of different neighbors in each diffusion order. This property enable FourierGNN to capture the complex correlations (i.e., spatiotemporal dependencies) in the hypervariate graph, which is empirically verified in our visualization experiments.\n%This property enable FourierGNN to capture time-varying correlations, which is empirically verified in our experiments (See Appendix \\ref{time_varying_visualization} for more details).\n% This indicates that FourierGNN is effective in capturing time-varying correlations. We also conducted experiments to verify this, and the details can be found in Appendix \\ref{time_varying_visualization}.\n\n\\iffalse\n"
                },
                "subsection 9.4": {
                    "name": "Explanation to the extension of Definition 2 to 2D domain",
                    "content": "\nRecall Eqs. 4 and 5, \\begin{center}Eq. 4 (kernel summation): $O({X})[i]=\\sum_{j=1}^n {X}[j]\\kappa[i,j]  \\quad\\quad   \\forall i \\in [n]$\\\\\nEq. 5 (kernel summation): $O({X})[i]=\\sum_{j=1}^n {X}[j]\\kappa[i-j]=(X*\\kappa)[i]  \\quad\\quad   \\forall i \\in [n]$\\end{center}\nWhen we extend the equations to 2D domain, i.e., from $[n]$ to $[N]\\times[T]$, it means performing a kernel summation/graph convolution over the discrete spatial-temporal space corresponding to all nodes in the supra-graph. Obviously, these computations of the kernel summation can be easily extended to 2D domain. Similarly, according to the convolution theorem, we can obtain a 2D-version of Eq. 6:\\begin{center}\nEq. 6 (graph convolution): $O(X)(i) = \\mathcal{F}^{-1}\\left(\\mathcal{F}(X)\\mathcal{F}(\\kappa)\\right)(i)\\quad\\quad  \\forall i \\in [N]\\times[T]$    \n\\end{center}\nwith $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denote the 2D discrete Fourier transform and its inverse, respectively. Accordingly, when extending Definition 2 to 2D domain, $\\mathcal{F}$ denotes the 2D discrete Fourier transform. Therefore, given input embeddings $\\mathbf{X}\\in\\mathbb{R}^{N\\times T\\times d}$, we perform 2D discrete Fourier transform on each discrete $N\\times T$ spatial-temporal plane of the embeddings to obtain the frequency spectrum, and then feed the frequency input into K-layer FourierGNN followed by two-layer (real-valued) feed-forward networks to generate multi-step forecasting (see Section 3.1 for more details). Note that we adopt the frequency-invariant FGSO ($d\\times d$) in the FourierGNN where the feed-forward computations act on the embedding dimension, i.e., $d$.\nIn addition, when extending Definition 2 to 2D domain, the frequency-invariant FGSO is invariant to both frequency components derived from the spatial dimension ($N$) and time dimension ($T$) respectively.\n\\fi\n\\iffalse\n"
                },
                "subsection 9.5": {
                    "name": "pro:filter",
                    "content": "\nThis treatment of omitting the weight matrix $W$ in Eq. \\ref{equ:fgso_filter} is feasible since the weight matrix $W$ can be absorbed in the embedding input, precisely the embedding matrix parameters. Note that we feed the input embeddings into FourierGNN (refer to Figure \\ref{fig:model_fig} and Section \\ref{sec:ovarc}). In addition, this treatment will not reduce the capability of FourierGNN intuitively since we adopt edge-varying filters in FourierGNN. Note that traditional GCNs adopt different weight matrices (regarding the model capability) but the same GSO (e.g., adjacency and Laplacian matrices regarding the given graph structure) in different diffusion orders. Differently, there is no pre-given graph structure in MTS forecasting scenarios, therefore we adopt the edge-varying filters, i.e., varying GSOs in FourierGNN, which does not reduce the model capability and achieves desirable performance empirically.\n\\fi\n\\iffalse\n"
                }
            },
            "section 10": {
                "name": "Proofs",
                "content": " \\label{proof}\n\n",
                "subsection 10.1": {
                    "name": "pro:filter",
                    "content": " \n\\label{sec:filter}\nThe proof aims to expand the graph convolution corresponding to $H_{EV}$ using a set of FGSOs in the Fourier space. According to the concise form of Equation \\ref{equ:fgso}, i.e.,\n\\begin{equation}\n    \\mathcal{F}(AXW)=\\mathcal{F}(X)\\times_n \\mathcal{A}\n\\end{equation}\nwhere $\\mathcal{F}$ denotes the discrete Fourier transform (DFT) and we omit the weight matrix $W$ for convenience (we can treat $W$ as an identity matrix), it yields:\n\\begin{equation}\n    \\begin{aligned}\n        \\mathcal{F}(A_KA_{K-1}\\cdots A_0X)&=\\mathcal{F}(A_K(A_{K-1}...A_0X))\\\\\n        &=\\mathcal{F}(A_{K-1}...A_0X)\\times_n \\mathcal{A}_K\\\\\n        &=\\mathcal{F}(\\mathcal{X})\\times_n  \\mathcal{A}_0\\cdots\\mathcal{A}_{K-1}\\times_n \\mathcal{A}_{K}\n    \\end{aligned}\n\\end{equation}\nwhere $\\{A_i\\}_{i=0}^K$ is a set of GSOs, and $\\{\\mathcal{A}_i\\}_{i=0}^K$ is a set of FGSOs corresponding to $\\{A_i\\}_{i=0}^K$ individually.\n%and $S_2$ are GSOs and $\\mathbf{S}_1$ and $\\mathbf{S}_2$ are FGSOs. Extending the above equation to a high-order case, we can easily obtain:\n% \\begin{equation}\n%     \\mathcal{F}(S_KS_{K-1}...S_0X)=\\mathcal{F}(\\mathcal{X})\\mathcal{S}_0\\cdots\\mathcal{S}_{K-1}\\mathcal{S}_{K}\n% \\end{equation} \\prod_{k=0}^KW_k\nThus, the edge-varying graph filter $H^{EV}_K$ can be rewritten as\n\\begin{equation}\n\\begin{aligned}\n    \\mathcal{F}(H^{EV}_K X)&= \\mathcal{F}(A_0XW_0+A_1(A_0XW_0)W_1+...+A_KA_{K-1}...(A_0XW_0)...W_{K-1}W_K)\\\\\n    &=\\mathcal{F}(A_0XW_0)+\\mathcal{F}(A_1(A_0XW_0)W_1)+...+\\mathcal{F}(A_KA_{K-1}...(A_0XW_0)...W_{K-1}W_K)\\\\\n    &=\\mathcal{F}(X)\\times_n\\mathcal{A}_0+\\mathcal{F}(X)\\times_n\\mathcal{A}_0\\times_n\\mathcal{A}_1+...+\\mathcal{F}(X)\\mathcal{A}_0\\times_n\\cdots\\mathcal{A}_{K-1}\\times_n\\mathcal{A}_{K}\n    %\\\\&=\\sum_{k=0}^{K}\\mathcal{F}(X)\\mathcal{S}_{0:k} \\quad s.t.\\  \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i\n\\end{aligned}\n\\end{equation}\nAccordingly, we have\n%Let denote $\\mathcal{X}=\\mathcal{F}(X)$, we have\n    \\begin{equation}\n        H^{EV}_KX=\\mathcal{F}^{-1}\\left(\\sum_{k=0}^{K}\\mathcal{F}(X)\\times_n\\mathcal{A}_{0:k}\\right)\\quad s.t.\\  \\mathcal{A}_{0:k}=\\mathcal{A}_0\\times_n\\cdots\\mathcal{A}_{K-1}\\times_n\\mathcal{A}_{K}\n    \\end{equation}\nProved.\n\\fi\n%\\subsection{Extension to Supra-Graph}\n%\\label{pro:extension}\n%We have that \n\n%$\\mathcal{F}(SXW)=\\mathcal{F}(X)$\n\n% \\subsection{Analysis of the rationality of the kernel invariant to the spatial-temporal space}\n% \\label{proof_proposition}\n% Given a graph $G$ attributed to MTS input $X\\in\\mathbb{R}^{N\\times T}$ and its a GSO $S\\in\\mathbb{R}^{(N*T)\\times(N*T)}$, we obtain its embedding $\\mathbf{X}\\in\\mathbb{R}^{N\\times T\\times d}$ of the input via an embedding matrix $\\Phi\\in\\mathbb{R}^{N\\times T\\times d}$. We flatten the input, its embedding, i.e., $X\\in\\mathbb{R}^{n}$ and $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ with $n:=N*T$, for better formulating the following calculation. According to Equation \\ref{equ:graph_covolution}, we have:\n% \\begin{equation}\n%     O(S\\mathbf{X})=S^0S\\mathbf{X}W\n% \\end{equation}\n% where $S^0$ is GSO and $W\\in\\mathbb{R}^{d\\times d}$ is the parameter matrix. For the scenarios without a predefined graph, we aim to parameterize and learn $S$ to learn the graph representations. Thus, we can assume $\\mathbf{X}:=S\\mathbf{X}$ where the parameters in $S$ is absorbed in the parameters in the embedding matrix $\\Phi$. Assume that $S^0$ is an all-one matrix, i.e., $S^0\\in\\{1\\}^{n\\times n}$ a complete graph with all edges equally weighted, we can define the corresponding matrix-valued kernel $\\kappa:[n]\\times[n]\\xrightarrow{}\\mathbb{R}^{d\\times d}$ with $\\kappa[i,j]=S^0_{ij}\\circ W=W$. Note that $S^0\\in\\{1\\}^{n\\times n}$ is also an instance of GSO. It results a kernel invariant to the spatial-temporal space. Accordingly, in the Fourier space, we can parameterize the invariant kernel with a complex-valued matrix $\\mathcal{F}(\\kappa)\\in\\mathbb{C}^{d\\times d}$.\n\n% In summary, when a graph structure is not predefined, we can assume a fully-connected graph and use an invariant kernel to learn the graph representations. Under this treatment, the GSO $S$ is parametrically \"factorized\" into the FGSO and the embeddings, and the invariant kernel shares parameters for spatial-temporal space and reduces computation costs. According to Equation \\ref{equ:fno}, when adopting an invariant kernel, we can omit the node index $i$ and obtain a concise form, i.e., Equation \\ref{equ:fgso}.\n\n"
                }
            },
            "section 11": {
                "name": "Compared with Other Graph Neural Networks",
                "content": " \n\\label{compares}\n% \\subsection{GNN}\n\\textbf{Graph Convolutional Networks}. Graph convolutional networks (GCNs) depend on the Laplacian eigenbasis to perform the multi-order graph convolutions over a given graph structure. Compared with GCNs, FourierGNN as an efficient alternative to multi-order graph convolutions has three main differences: 1) No eigendecompositions or similar costly matrix operations are required. FourierGNN transforms the input into Fourier domain by discrete Fourier transform (DFT) instead of graph Fourier transform (GFT); 2) Explicitly assigning various importance to nodes of the same neighborhood with different diffusion steps. FourierGNN adopts different Fourier Graph Operators $\\mathcal{S}$ in different diffusion steps corresponding to different dependencies among nodes; 3) FourierGNN is invariant to the discretization $N$, $T$. It parameterizes the graph convolution via Fourier Graph Operators which are invariant to the graph structure and graph scale.%; 4) FourierGNN does not require a predefined graph structure.% and has been empirically demonstrated with ability to transfer solution between different meshes by FNO~\\cite{LiKALBSA2021}\n%That is to say, $AX=U\\Lambda U^TX$ in GCNs where $U$ is the matrix of eigenvectors of $A$ while $AX=\\mathcal{F}^{-1}(\\mathcal{S}\\mathcal{X})$ in our model where $\\mathcal{S}$ is the Fourier graph shift operator.\n\n\\textbf{Graph Attention Networks}. Graph attention networks (GATs) are non-spectral attention-based graph neural networks. GATs use node representations to calculate the attention weights (i.e., edge weights) varying with different graph attention layers. Accordingly, both GATs and FourierGNN do not depend on eigendecompositions and adopt varying edge weights with different diffusion steps (layers). However, FourierGNN can efficiently perform graph convolutions in Fourier space. For a complete graph, the time complexity of the attention calculation of $K$ layers is proportional to $Kn^2$ where $n$ is the number of nodes, while a $K$-layer FourierGNN infers the graph structure in Fourier space with the time complexity proportional to $n\\operatorname{log}n$. In addition, compared with GATs that implicitly achieve edge-varying weights with different layers, FourierGNN adopts different FGOs in different diffusion steps explicitly.\n\n%GAT uses node features for similarity computations and calculate the attention scores in time domain. For a fully-connected graph, the time complexity of the attention scores calculation is $\\mathcal{O}(N^2)$ where $N$ is the number of nodes. Compared to GAT, our model infers the graph structure in Fourier space with the time complexity of $\\mathcal{O}(NlogN)$. Moreover, although GAT applies different weights to nodes of a same neighborhood, the weights to nodes are the same in different layers. However, our model assigns different FGSOs in different diffusion steps explicitly.\n\n% \\subsection{FNO}\n% Fourier neural operator (FNO)~\\cite{LiKALBSA2021} computes the global convolutions in Fourier space while we elaborately design FourierGNN to compute the multi-order graph convolutions in Fourier space. Both FNO and FourierGNN replace the time-consuming graph/global convolutions in time domain with the efficient spectral convolution in Fourier space according to the convolution theorem.\n\n% However, FNO and FourierGNN are quite different in the network architecture. As shown in Figure \\ref{fig:fno_fig}, FNO consists of a stack of Fourier layers where each Fourier layer serially performs 1) DFT to obtain the spectrum of the input, 2) then the spectral convolution in Fourier space, 3) and finally IDFT to transform the output to the time domain. Accordingly, FNO needs $K$ pairs of DFT and IDFT for $K$ Fourier layers in FNO. In contrast, FourierGNN with just a pair of DFT and IDFT performs multi-order (multi-layer) graph convolutions on one fly via stacking multiple FGOs in Fourier space, as shown in Figure \\ref{fig:model_fig}. \n\n%In addition, adaptive Fourier neural operator (AFNO)~\\cite{John2021} is a variant of FNO. It is a neural unit and used in a plug-and-play fashion as an alternative to self-attention to reduce the quadratic complexity. Similarly, it requires a pair of DFT and IDFT to accomplish the self-attention computation. In contrast, our proposed FourierGNN is a neural network with a set of FGSOs in a well-designed connection.\n\n% \\begin{figure}[ht]\n%     \\centering\n%     \\includegraphics[width=0.75\\linewidth]{figures/fno_fig.png}\n%     \\caption{The simplified structure of FNO derived from~\\cite{LiKALBSA2021}. $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denote Fourier transform and its reverse respectively.}\n%     \\label{fig:fno_fig}\n% \\end{figure}\n\n"
            },
            "section 12": {
                "name": "Experiment Details",
                "content": " \\label{Experiment_detail}\n",
                "subsection 12.1": {
                    "name": "Datasets",
                    "content": "\\label{appendix_datasets}\nWe use seven public multivariate benchmarks for multivariate time series forecasting and these benchmark datasets are summarized in Table \\ref{tab:datasets}.\n\n\n\n\\textbf{Solar}\\footnote{\\url{https://www.nrel.gov/grid/solar-power-data.html}}: This dataset is about solar power collected by National Renewable Energy Laboratory. We choose the power plant data points in Florida as the data set which contains 593 points. The data is collected from 2006/01/01 to 2016/12/31 with the sampling interval of every 1 hour.%synthetic solar photovoltaic (PV) power plant data points\n\n\\textbf{Wiki~\\cite{Sen2019}}: This dataset contains a number of daily views of different Wikipedia articles and is collected from 2015/7/1 to 2016/12/31. It consists of approximately $145k$ time series and we randomly choose $2k$ from them as our experimental data set.\n\n\\textbf{Traffic~\\cite{Sen2019}}: This dataset contains hourly traffic data from $963$ San Francisco freeway car lanes. The traffic data are collected since 2015/01/01 with the sampling interval of every 1 hour.\n\n\\textbf{ECG\\footnote{\\url{http://www.timeseriesclassification.com/description.php?Dataset=ECG5000}}}: This dataset is about Electrocardiogram(ECG) from the UCR time-series classification\narchive \\cite{dau2018}. It contains 140 nodes and each node has a length of 5000.\n\n\\textbf{Electricity\\footnote{\\url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}}}: This dataset contains the electricity consumption of 370 clients and is collected since 2011/01/01. The data sampling interval is every 15 minutes.\n\n\\textbf{COVID-19\\footnote{\\url{https://github.com/CSSEGISandData/COVID-19}}}: This dataset is about COVID-19 hospitalization in the U.S. states of California (CA) from 01/02/2020 to 31/12/2020 provided by the Johns Hopkins University with the sampling interval of every one day.\n\n\\textbf{METR-LA\\footnote{\\url{https://github.com/liyaguang/DCRNN}}}: This dataset contains traffic information collected from loop detectors in the highway of Los Angeles County from 01/03/2012 to 30/06/2012. It contains 207 sensors and the data sampling interval is every 5 minutes.\n\n"
                },
                "subsection 12.2": {
                    "name": "Baselines",
                    "content": "\n\\label{subsec:baselines}\nIn experiments, we conduct a comprehensive comparison of the forecasting performance between our FourierGNN and representative and state-of-the-art (SOTA) models as follows.\n\n\\textbf{VAR} \\cite{Vector1993}: VAR is a classic linear autoregressive model. We use the Statsmodels library (\\url{https://www.statsmodels.org}) which is a Python package that provides statistical computations to realize the VAR. \n\n\\textbf{DeepGLO} \\cite{Sen2019}: DeepGLO models the relationships among variables by matrix factorization and employs a temporal convolution neural network to introduce non-linear relationships. We download the source code from: \\url{https://github.com/rajatsen91/deepglo}. We follow the recommended configuration as our experimental settings for wiki, electricity, and traffic datasets. For covid datasets, the vertical and horizontal batch size is set to 64, the rank of the global model is set to 64, the number of channels is set to [32, 32, 32, 1], and the period is set to 7.\n\n\\textbf{LSTNet} \\cite{Lai2018}: LSTNet uses a CNN to capture inter-variable relationships and an RNN to discover long-term patterns. We download the source code from: \\url{https://github.com/laiguokun/LSTNet}. In our experiment, we use the recommended configuration where the number of CNN hidden units is 100, the kernel size of the CNN layers is 4, the dropout is 0.2, the RNN hidden units is 100, the number of RNN hidden layers is 1, the learning rate is 0.001 and the optimizer is Adam.\n\n\\textbf{TCN} \\cite{bai2018}: TCN is a causal convolution model for regression prediction. We download the source code from: \\url{https://github.com/locuslab/TCN}. We utilize the same configuration as the polyphonic music task exampled in the open source code where the dropout is 0.25, the kernel size is 5, the number of hidden units is 150, the number of levels is 4 and the optimizer is Adam.\n\n\\textbf{Reformer} \\cite{reformer20}: Reformer combines the modeling capacity of a Transformer with an architecture that can be executed\nefficiently on long sequences and with small memory use. We download the source code from: \\url{https://github.com/thuml/Autoformer}. We follow the recommended configuration as the experimental settings.\n\n\\textbf{Informer} \\cite{Zhou2021}: Informer leverages an efficient self-attention mechanism to encode the dependencies among variables. We download the source code from: \\url{https://github.com/zhouhaoyi/Informer2020}. We follow the recommended configuration as our experimental settings where the dropout is 0.05, the number of encoder layers is 2, the number of decoder layers is 1, the learning rate is 0.0001, and the optimizer is Adam.\n\n\\textbf{Autoformer} \\cite{autoformer21}: Autoformer proposes a decomposition architecture by embedding the series decomposition block as an inner operator, which can progressively aggregate the long-term trend part from intermediate prediction. We download the source code from: \\url{https://github.com/thuml/Autoformer}. We follow the recommended configuration as our experimental settings with 2 encoder layers and 1 decoder layer.\n\n\\textbf{FEDformer} \\cite{fedformer22}: FEDformer proposes an attention mechanism with low-rank approximation in frequency and a mixture of expert decomposition to control the distribution shifting. We download the source code from: \\url{https://github.com/MAZiqing/FEDformer}. We use FEB-f as the Frequency Enhanced Block and select the random mode with 64 as the experimental mode.\n\n\\textbf{SFM} \\cite{ZhangAQ17}: On the basis of the LSTM model, SFM introduces a series of different frequency components in the cell states. We download the source code from: \\url{https://github.com/z331565360/State-Frequency-Memory-stock-prediction}. We follow the recommended settings where the learning rate is 0.01, the frequency dimension is 10, the hidden dimension is 10 and the optimizer is RMSProp.\n\n\\textbf{StemGNN} \\cite{Cao2020}: StemGNN leverages GFT and DFT to capture dependencies among variables in the frequency domain. We download the source code from: \\url{https://github.com/microsoft/StemGNN}. We use the recommended configuration of stemGNN as our experiment setting where the optimizer is RMSProp, the learning rate is 0.0001, the number of stacked layers is 5, and the dropout rate is 0.5.\n\n\\textbf{MTGNN} \\cite{wu2020connecting}: MTGNN proposes an effective method to exploit the inherent dependency relationships among multiple time series. We download the source code from: \\url{https://github.com/nnzhan/MTGNN}. Because the experimental datasets have no static features, we set the parameter load\\_static\\_feature to false. We construct the graph by the adaptive adjacency matrix and add the graph convolution layer. Regarding other parameters, we adopt the recommended settings.\n\n\\textbf{GraphWaveNet} \\cite{zonghanwu2019}: GraphWaveNet introduces an adaptive dependency matrix learning to capture the hidden spatial dependency. We download the source code from: \\url{https://github.com/nnzhan/Graph-WaveNet}. Since our datasets have no prior defined graph structures, we use only adaptive adjacent matrix. We add a graph convolution layer and randomly initialize the adjacent matrix. We adopt the recommended configuration as our experimental settings where the learning rate is 0.001, the dropout is 0.3, the number of epochs is 50, and the optimizer is Adam.\n\n\\textbf{AGCRN} \\cite{Bai2020nips}: AGCRN proposes a data-adaptive graph generation module for discovering spatial correlations from data. We download the source code from: \\url{https://github.com/LeiBAI/AGCRN}. We follow the recommended configuration as our experimental settings where the embedding dimension is 10, the learning rate is 0.003, and the optimizer is Adam.\n\n\\textbf{TAMP-S2GCNets} \\cite{tampsgcnets2022}: TAMP-S2GCNets explores the utility of MP to enhance knowledge representation mechanisms within the time-aware DL paradigm. We download the source code from: \\url{https://www.dropbox.com/sh/n0ajd5l0tdeyb80/AABGn-ejfV1YtRwjf_L0AOsNa?dl=0}. TAMP-S2GCNets requires predefined graph topology and we use the California State topology provided by the source code as input. We adopt the recommended configuration as our experimental settings on COVID-19.\n\n\\textbf{DCRNN} \\cite{LiYS018}: DCRNN uses bidirectional graph random walk to model spatial dependency and recurrent neural network to capture the temporal dynamics. We download the source code from: \\url{https://github.com/liyaguang/DCRNN}. We follow the recommended configuration as our experimental settings with the batch size is 64, the learning rate is $0.01$, the input dimension is 2 and the optimizer is Adam. DCRNN requires a pre-defined graph structure and we use the adjacency matrix as the pre-defined structure provided by the METR-LA dataset.\n\n\\textbf{STGCN} \\cite{Yu2018}: STGCN integrates graph convolution and gated temporal convolution through spatial-temporal convolutional blocks. We download the source code from:\\url{https://github.com/VeritasYin/STGCN_IJCAI-18}. We use the recommended configuration as our experimental settings where the batch size is 50, the learning rate is $0.001$ and the optimizer is Adam. STGCN requires a pre-defined graph structure and we leverage the adjacency matrix as the pre-defined structures provided by the METR-LA dataset.\n\n\\textbf{CoST} \\cite{cost22}: CoST separates the representation learning and downstream forecasting task and proposes a contrastive learning framework that learns disentangled season-trend representations for time series forecasting tasks. We download the source code from: \\url{https://github.com/salesforce/CoST}. We set the representation dimension to 320, the learning rate to 0.001, and the batch size to 32. Inputs are min-max normalization, we perform a 70/20/10 train/validation/test split for the METR-LA dataset and 60/20/20 for the COVID-19 dataset.\n\n"
                },
                "subsection 12.3": {
                    "name": "Evaluation Metrics",
                    "content": " \\label{appendix_metrics}\nWe use MAE (Mean Absolute Error), RMSE (Root Mean Square Error), and MAPE (Mean Absolute Percentage Error) as the evaluation metrics in the experiments.\n\nSpecifically, given the groudtruth at timestamps $t$, $Y_t=[{\\bm{x}}_{t+1}, ..., {\\bm{x}}_{t+\\tau}]\\in\\mathbb{R}^{N\\times\\tau}$, and the predictions $\\hat{Y}_t=[{\\hat{\\bm{x}}}_{t+1}, ..., \\hat{{\\bm{x}}}_{t+\\tau}]\\in\\mathbb{R}^{N\\times\\tau}$ for future $\\tau$ steps at timestamp $t$, the metrics are defined as follows:\n\\begin{equation}\n    MAE=\\frac{1}{\\tau N}\\sum_{i=1}^{N}\\sum_{j=1}^{\\tau}\\left | x_{ij}-\\hat{x}_{ij} \\right |\n\\end{equation}\n\\begin{equation}\n   RMSE=\\sqrt{\\frac{1}{\\tau N}\\sum_{i=1}^{N}\\sum_{j=1}^{\\tau}\\left (x_{ij}-\\hat{x}_{ij}\\right )^2}\n\\end{equation}\n\\begin{equation}\n    MAPE=\\frac{1}{\\tau N}\\sum_{i=1}^{N}\\sum_{j=1}^{\\tau}\\left | \\frac{x_{ij}-\\hat{x}_{ij}}{x_{ij}} \\right | \\times 100\\%\n\\end{equation}\nwith $x_{ij}\\in Y_t$ and $\\hat{x}_{ij}\\in\\hat{Y}_t$.\n% \\begin{equation}\n%     MAE=\\frac{1}{T}\\sum_{t=1}^{T}\\left | X_t-\\hat{X_t} \\right |,\n% \\end{equation}\n% \\begin{equation}\n%   RMSE=\\sqrt{\\frac{1}{T}\\sum_{t=1}^{T}\\left ( X_t-\\hat{X_t} \\right )^2},\n% \\end{equation}\n% \\begin{equation}\n%     MAPE=\\frac{1}{T}\\sum_{t=1}^{T}\\left | \\frac{X_t-\\hat{X_t}}{X_t} \\right | \\times 100\\%.\n% \\end{equation}\n\n"
                },
                "subsection 12.4": {
                    "name": "Experimental Settings",
                    "content": "\n\\label{subsec:es}\n% tuning process\n% Network parameter of FFN (figure)\n% The settings for baselines.\n%Our experimental hardware environment is with one NVIDIA RTX 3080 GPU card. Our code is implemented in Python with PyTorch 1.8. We separate the COVID-19 dataset with a ratio of 6:2:2 and for other datasets the ratio is 7:2:1. For all datasets, the optimizer is RMSProp, the loss function is MSELoss, the learning rate is initialized as $0.00001$, the maximum of training epochs is set to 100 and the activation function of FourierGNN is $ReLU$. \nWe summarize the implementation details of the proposed FourierGNN as follows. Note that the details of the baselines are introduced in their corresponding descriptions (see Section \\ref{subsec:baselines}).\n\n\\textbf{Network details.} The fully connected feed-forward network (FFN) consists of three linear transformations with $LeakyReLU$ activations in between. The FFN is formulated as follows:% $\\mathcal{X}_\\Psi\\in\\mathbb{R}^{}$\n\\begin{equation}\n    \\begin{aligned}\n        \\mathbf{X}_1&=\\operatorname{LeakyReLU}(\\mathbf{Y}_t^{\\mathcal{G}}\\mathbf{W}_1+\\mathbf{b}_1)\\\\\n        \\mathbf{X}_2&=\\operatorname{LeakyReLU}(\\mathbf{X}_1\\mathbf{W}_2+\\mathbf{b}_2)\\\\\n        \\hat{Y}&=\\mathbf{X}_2\\mathbf{W}_3+\\mathbf{b}_3\n    \\end{aligned}\n\\end{equation}\nwhere $\\mathbf{W}_1\\in\\mathbb{R}^{(Td)\\times d^{ffn}_1}$, $\\mathbf{W}_2\\in\\mathbb{R}^{d^{ffn}_1\\times {d^{ffn}_2}}$ and $\\mathbf{W}_3\\in\\mathbb{R}^{d^{ffn}_2\\times \\tau}$ are the weights of the three layers respectively, and $\\mathbf{b}_1\\in\\mathbb{R}^{d^{ffn}_1}$, $\\mathbf{b}_2\\in\\mathbb{R}^{d^{ffn}_2}$ and $\\mathbf{b}_3\\in\\mathbb{R}^{\\tau}$ are the biases of the three layers respectively. Here, $d^{ffn}_1$ and $d^{ffn}_2$ are the dimensions of the three layers. In addition, we adopt a $ReLU$ activation function in Equation \\ref{equ:fourier_gnn}.\n\n\\textbf{Training details.} We carefully tune the hyperparameters, including the embedding size, batch size, $d^{ffn}_1$ and $d^{ffn}_2$, on the validation set and choose the settings with the best performance for FourierGNN on different datasets. Specifically, the embedding size and batch size are tuned over $\\{32,64,128,256,512\\}$ and $\\{2,4,8,16,32,64,128\\}$ respectively. For the COVID-19 dataset, the embedding size is 256, and the batch size is set to 4. For the Traffic, Solar, and Wiki datasets, the embedding size is 128, and the batch size is set to 2. For the METR-LA, ECG, and Electricity datasets, the embedding size is 128, and the batch size is set to 32. \n%Note that the hypervariate graph connecting all nodes is a fully-connected graph, indicating that any spatial order is feasible. Therefore, although we perform DFT in FourierGNN, none spatial order in the spatial space is necessary for performing FourierGNN, and we directly adopt the raw dataset for experiments.\n\nTo reduce the number of parameters, we adopt a linear transform to reshape the original time domain representation $\\mathbf{Y}_t^{\\mathcal{G}}  \\in\\mathbb{R}^{NT\\times d}$ to $\\mathbf{Y}_t \\in\\mathbb{R}^{N\\times T\\times d}$, and map  $\\mathbf{Y}_t$ to a low-dimensional tensor $\\mathbf{Y}_t \\in\\mathbb{R}^{N\\times l\\times d}$ with $l<T$. We then reshape $\\mathbf{Y}_t \\in\\mathbb{R}^{N\\times (ld)}$ and feed it to FFN. We perform a grid search on the dimensions of FFN, i.e., $d^{ffn}_1$ and $d^{ffn}_2$, over $\\{32, 64, 128, 256, 512\\}$ and tune the intermediate dimension $l$ over $\\{2,4,6,8,12\\}$. The settings of the three hyperparameters over all datasets are shown in Table \\ref{tab:settings}. \nBy default, we set the diffusion step (layers) $K=3$ for all datasets.\n\n\n\n\n"
                },
                "subsection 12.5": {
                    "name": "Details for Visualization Experiments",
                    "content": "\n\\label{sec:visual_details}\nTo verify the effectiveness of FourierGNN in learning the spatiotemporal dependencies on the hypervariate graph, we obtain the output of FourierGNN as the node representation, denoted as $\\mathbf{Y}_t^{\\mathcal{G}}=\\operatorname{IDFT}(\\operatorname{FourierGNN}(\\mathbf{X}_t^{\\mathcal{G}}))\\in\\mathbb{R}^{NT\\times d}$ with Inverse Discrete Fourier Transform ($\\operatorname{IDFT}$). Then, we visualize the adjacency matrix $\\mathbf{A}$ calculated based the flatten node representation $\\mathbf{Y}_t^{\\mathcal{G}} \\in\\mathbb{R}^{NT\\times d}$, formulated as $\\mathbf{A}=\\mathbf{Y}_t^{\\mathcal{G}}(\\mathbf{Y}_t^{\\mathcal{G}})^T\\in\\mathbb{R}^{NT\\times NT}$, to show the variable correlations. Note that $\\mathbf{A}$ is normalized via $\\mathbf{A}/\\operatorname{max}(\\mathbf{A})$. Since it is not feasible to directly visualize the huge adjacency matrix $\\mathbf{A}$ of the hypervariate graph, we visualize its different subgraphs in Figures \\ref{fig:TIME_ADJ}, \\ref{fig:visual_metr}, \\ref{fig:filters}, and \\ref{fig:space_adj} to better verify the learned spatiotemporal information on the hypervariate graph from different perspectives.\n\nFigure \\ref{fig:TIME_ADJ}. We select $8$ counties and visualize the correlations between $12$ consecutive time steps for each selected county respectively. Figure \\ref{fig:TIME_ADJ} reflects the temporal correlations within each variable.\n\nFigure \\ref{fig:visual_metr}: On the METR-LA dataset, we average its adjacency matrix $\\mathbf{A}$ over the temporal dimension (i.e., marginalizing $T$) to $\\mathbf{A}'\\in\\mathbb{R}^{N\\times N}$. Then, we randomly select $20$ detectors out of all $N=207$ detectors and obtain their corresponding sub adjacency matrix ($\\mathbb{R}^{20\\times 20}$) from $\\mathbf{A}'$ for visualization. We further compare the sub-adjacency with the real road map (generated by the Google map tool) to verify the learned dependencies between different detectors.\n\nFigure \\ref{fig:filters}. Since we adopt a $3$-layer FourierGNN, we can calculate four adjacency matrices based on the spectrum input $\\mathcal{X}_t^{\\mathcal{G}}$ of FourierGNN and the outputs of each layer in FourierGNN. Following the way of visualization in Figure \\ref{fig:visual_metr}, we select $10$ counties and two timestamps on the four adjacency matrices for visualization. Figure \\ref{fig:filters} shows the effects of each layer of FourierGNN in filtering or enhancing variable correlations.\n\nFigure \\ref{fig:space_adj}. On the COVID-19 dataset, we randomly select $10$ counties out of $N=55$ counties and obtain their four sub-adjacency matrices of four consecutive days for visualization. Each of the four sub adjacency matrices $\\mathbb{R}^{10\\times 10}$ embodies the dependencies between counties in one day. Figure \\ref{fig:space_adj} reflects the time-varying dependencies between counties (i.e., variables).\n\n"
                }
            },
            "section 13": {
                "name": "Additional Results",
                "content": " \\label{more_results}\n% \\begin{table*}[ht]\n%     \\centering\n%     \\caption{Performance comparison under different prediction lengths on the COVID-19 dataset.% We compare FourierGNN with other five GNN-based baseline models on the COVID-19 dataset when prediction length is 3, 6, 9, and 12, respectively.\n%     }\n%     \\resizebox{\\textwidth}{18mm}{\n%     \\begin{tabular}{l|c c c|c c c|c c c|c c c}\n%     \\toprule\n%       Length & \\multicolumn{3}{c|}{3} &  \\multicolumn{3}{c|}{6} & \\multicolumn{3}{c|}{9}  & \\multicolumn{3}{c}{12}\\\\\n%       Metrics & MAE &RMSE&MAPE(\\%)&  MAE& RMSE&MAPE(\\%)& MAE &RMSE&MAPE(\\%) & MAE& RMSE&MAPE(\\%)\\\\\n%       \\midrule\n%          GraphWaveNet \\cite{zonghanwu2019} & \\underline{0.092} & \\underline{0.129} & 53.00& \\underline{0.133} & \\underline{0.179} & 65.11& 0.171 &0.225 & 80.91& 0.201 &0.255&100.83\\\\\n%          StemGNN \\cite{Cao2020} & 0.247 &0.318& 99.98& 0.344 &0.429& 125.81& 0.359 &0.442&131.14 & 0.421 &0.508&141.01\\\\\n%          AGCRN \\cite{Bai2020nips} & 0.130 &0.172 &68.64 & 0.171 &0.218& 79.29& 0.224 &0.277 & 113.42& 0.254 & 0.309 & 125.43\\\\\n%          MTGNN \\cite{wu2020connecting}& 0.276 &0.379&91.42 & 0.446 &0.513& 133.49& 0.484 &0.548& 139.52& 0.394 &0.488&88.13 \\\\\n%          TAMP-S2GCNets \\cite{tampsgcnets2022} & 0.140 & 0.190 & \\textbf{50.01} & 0.150 & 0.200 & \\textbf{55.72}& \\underline{0.170} & \\underline{0.230} & \\underline{71.78}& \\underline{0.180} & \\underline{0.230} & \\textbf{65.76}\\\\\n%          CoST \\cite{cost22} & 0.122 & 0.246 & 68.74 & 0.157 & 0.318 & 72.84 & 0.183 & 0.364 & 77.04 & 0.202 & 0.377 & 80.81\\\\\n%          \\midrule\n%          \\textbf{FourierGNN(ours)} & \\textbf{0.071}  &\\textbf{0.103} & 61.02 & \\textbf{0.093} &  \\textbf{0.131} & 65.72& \\textbf{0.109}  &\\textbf{0.148} & \\textbf{69.59} & \\textbf{0.124} &\\textbf{0.164} & 72.57\\\\\n%          Improvement &22.8\\% &20.2\\% & -& 30.1\\% & 26.8\\% &- & 35.9\\% &35.7\\% & 3.1 \\%& 31.1\\% &28.7\\% &- \\\\ \n%          \\bottomrule\n%     \\end{tabular}\n%     }\n%     \\label{tab:covid_result}\n% \\end{table*}\nTo further evaluate the performance of our model FourierGNN in multi-step forecasting, we conduct more experiments on the Wiki, METR-LA, and ECG datasets, respectively. We compare our model FourierGNN with five models (including StemGNN~\\cite{Cao2020}, AGCRN~\\cite{Bai2020nips}, GraphWaveNet~\\cite{zonghanwu2019}, MTGNN~\\cite{wu2020connecting}, and Informer~\\cite{Zhou2021}) on the Wiki dataset under different prediction lengths, and the results are shown in Table \\ref{tab:wiki_result}. From the table, we observe that FourierGNN outperforms other models on MAE, RMSE, and MAPE metrics for all the prediction lengths. On average, FourierGNN improves MAE, RMSE, and MAPE by 7.4\\%, 3.5\\%, and 22.3\\%, respectively. Among these models, AGCRN shows promising performances since it captures the spatial and temporal correlations adaptively. However, it fails to simultaneously capture spatiotemporal dependencies, limiting its forecasting performance. In contrast, our model captures comprehensive spatiotemporal dependencies simultaneously on a hypervariate graph for multivariate time series forecasting. \n%We compare FourierGNN with other GNN-based MTS models (including StemGNN, AGCRN, GraphWaveNet, MTGNN and TAMP-S2GCNets) and representation learning model (CoST) on the COVID-19 dataset under different prediction lengths, and the results are shown in Table \\ref{tab:covid_result}. From the table, we can find that FourierGNN achieves the best MAE and RMSE on all the prediction lengths. On average, FourierGNN has 30.0\\% and 27.9\\% improvement on MAE and RMSE respectively over the best baseline, i.e., TAMP-S2GCNets. Among these models, TAMP-S2GCNets requiring a pre-defined graph topology achieves competitive performance since it enhances the resultant graph learning mechanisms with a multi-persistence. However, it constructs the graph in the spatial dimension, while our model FourierGNN adaptively learns a supra-graph connecting any two variables at any two timestamps, which is effective and more powerful to capture high-resolution spatial-temporal dependencies.\n\n\n\n% In addition, we compare our model FourierGNN with five neural MTS models (including StemGNN, AGCRN, GraphWaveNet, MTGNN and Informer) on Wiki dataset under different prediction lengths, and the results are shown in Table \\ref{tab:wiki_result}. From the table, we observe that FourierGNN outperforms other models on MAE, RMSE and MAPE metrics for all the prediction lengths. On average, FourierGNN improves MAE, RMSE and MAPE by 6.8\\%, 3.2\\% and 22.9\\%, respectively. Among these models, AGCRN shows promising performances since it captures the spatial and temporal correlations adaptively. However, it fails to simultaneously capture spatial-temporal dependencies, limiting its forecasting performance. In contrast, our model learns a supra-graph to capture comprehensive spatial-temporal dependencies simultaneously for multivariate time series forecasting. \n\n\n\nFurthermore, we compare our model FourierGNN with seven MTS models (including STGCN~\\cite{Yu2018}, DCRNN~\\cite{LiYS018}, StemGNN~\\cite{Cao2020}, AGCRN~\\cite{Bai2020nips}, GraphWaveNet~\\cite{zonghanwu2019}, MTGNN~\\cite{wu2020connecting}, Informer~\\cite{Zhou2021}, and CoST~\\cite{cost22}) on the METR-LA dataset which has a predefined graph topology in the data, and the results are shown in Table \\ref{tab:metr_result}. On average, we improve $5.7\\%$ on MAE and $1.5\\%$ on RMSE. Among these models, StemGNN achieves competitive performance because it combines GFT to capture the spatial dependencies and DFT to capture the temporal dependencies. However, it is also limited to simultaneously capturing spatiotemporal dependencies. CoST learns disentangled seasonal-trend representations for time series forecasting via contrastive learning and obtains competitive results. But, our model still outperforms CoST. Because, compared with CoST, our model not only can learn the dynamic temporal representations, but also capture the discriminative spatial representations. Besides, STGCN and DCRNN require pre-defined graph structures. But StemGNN and our model outperform them for all steps, and AGCRN outperforms them when the prediction lengths are 9 and 12. This also shows that a novel adaptive graph learning can precisely capture the hidden spatial dependency. \nIn addition, we compare FourierGNN with the baseline models under the different prediction lengths on the ECG dataset, as shown in Figure \\ref{fig:ecg}. It reports that FourierGNN achieves the best performances (MAE, RMSE, and MAPE) for all prediction lengths.\n\n\n\n"
            },
            "section 14": {
                "name": "Further Analyses",
                "content": " \\label{more_analysis}\n",
                "subsection 14.1": {
                    "name": "Scalability Analysis",
                    "content": "\n\\label{more_oarameter_analysis}\nWe further conduct experiments on the Wiki dataset to investigate the performance of FourierGNN under different graph sizes ($N\\times T$). The results are shown in Figure \\ref{fig:scale-invariant}, where Figure \\ref{scale_b}, Figure \\ref{scale_c} and Figure \\ref{scale_d} show MAE, RMSE, and MAPE at the different number of nodes, respectively. From these figures, we observe that FourierGNN keeps a leading edge over the other state-of-the-art MTS models as the number of nodes increases. The results demonstrate the superiority and scalability of FourierGNN on large-scale datasets.\n%More parameter sensitive analysis can be found in Appendix \\ref{more_oarameter_analysis}.\n\n\n% \\begin{wrapfigure}{l}{7cm}\n% \\vspace{-7mm}\n% \\centering\n%     \\includegraphics[width=6cm]{figures/parameter_embedding.png}\n%     \\caption{Influence of embedding size on the ECG datasets.}\n%     \\label{parameter_embedding}\n%     \\vspace{-5mm}\n% \\end{wrapfigure}\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=6cm]{figures/parameter_embedding.png}\n%     \\caption{Influence of embedding size on the ECG dataset.}\n%     \\label{parameter_embedding}\n% \\end{figure}\n\n"
                },
                "subsection 14.2": {
                    "name": "Parameter Analysis",
                    "content": " \\label{appendix_parameter_analysis}\n\nWe evaluate the forecasting performance of our model FourierGNN under different diffusion steps (layers) on the COVID-19 dataset, as illustrated in Table \\ref{diffusion-step}. The table shows that FourierGNN achieves increasingly better performance from $K=1$ to $K=4$ and achieves the best results when $K=3$. With the further increase of $K$, FourierGNN obtains inferior performance. The results indicate that high-order diffusion information is beneficial for improving forecasting accuracy, but the diffusion information may gradually weaken the effect or even bring noises to forecasting with the increase of the order.\n\n\n\nIn addition, we conduct additional experiments on the ECG dataset to analyze the effect of the input lookback window length $T$ and the embedding dimension $d$, as shown in Figure \\ref{parameter_input} and Figure \\ref{parameter_output}, respectively. Figure \\ref{parameter_input} shows that the performance (including RMSE and MAPE) of FourierGNN gets better as the input lookback window length increases, indicating that FourierGNN can learn a comprehensive hypervariate graph from long MTS inputs to capture the spatial and temporal dependencies. Moreover, Figure \\ref{parameter_output} shows that the performance (RMSE and MAPE) first increases and then decreases with the increase of the embedding size, which is attributed that a large embedding size improves the fitting ability of FourierGNN but it may easily lead to the overfitting issue especially when the embedding size is too large.\n%under different frequency bands and different diffusion steps.\n\n"
                },
                "subsection 14.3": {
                    "name": "Ablation Study",
                    "content": "\\label{appendix_ablation_study}\nWe provide more details about each variant used in this section and Section \\ref{sec:analysis}.\n\\begin{itemize}\n    \\item \\textbf{w/o Embedding}. A variant of FourierGNN feeds the raw MTS input instead of its embeddings into the graph convolution in Fourier space.\n    \\item \\textbf{w/o Dynamic FGO}. A variant of FourierGNN uses the same FGO for all diffusion steps instead of applying different FGOs in different diffusion steps. It corresponds to a vanilla graph filter.\n    \\item \\textbf{w/o Residual}. A variant of FourierGNN does not have the $K=0$ layer output, i.e., $\\mathcal{X}_t^{\\mathcal{G}}$, in the summation.\n    \\item \\textbf{w/o Summation}. A variant of FourierGNN adopts the last order (layer) output as the final frequency output of the FourierGNN.\n\\end{itemize}\n\nWe conduct another ablation study on the COVID-19 dataset to further investigate the effects of the different components of our FourierGNN. The results are shown in Table \\ref{tab:ablation_covid}, which confirms the results in Table \\ref{tab:metr_ablation} and further verifies the effectiveness of each component in FourierGNN. Both Table \\ref{tab:ablation_covid} and Table \\ref{tab:metr_ablation} report that the embedding and dynamic FGOs in FourierGNN contribute more than the design of residual and summation to the state-of-the-art performance of FourierGNN.\n\n\n%The baseline is our model FourierGNN which composes of all components. \n\n%\\textbf{Ordering of the time series}. Note that the supra-graph connecting all nodes is a fully-connected graph, indicating that any spatial order is feasible. Then how could we perform 2D DFT to achieve the graph convolution? First, the graph convolution on the supra-graph can be viewed as a kernel summation (cf. Eq.4, i.e., $O({X})[i]=\\sum_{j=1}^n {X}[j]\\kappa[i,j],\\forall j \\in [n]$) and does not depend on a specific spatial order. From Eq.4 to Eq.5, we extend the kernel summation to a kernel integral in the continuous spatial-temporal space and introduce a special kernel, i.e., the shift-invariant Green's kernel $\\kappa(i,j)=\\kappa(i-j)$. According to the convolution theorem, we can reformulate the graph convolution with continuous Fourier transform (i.e., Eq.6 $\\mathcal{O}(X)(i) = \\mathcal{F}^{-1}\\left(\\mathcal{F}(X)\\mathcal{F}(\\kappa)\\right)(i), \\forall i \\in \\mathcal{D}$) and then apply Eq.6 to the finite discrete spatial-temporal space (i.e., the supra-graph). Accordingly, we can obtain the Definition 2 and Eq.7 reformulating the graph convolution with 2D DFT.\n\n%To understand the learning paradigm of FourierGNN, we can recall the self-attention mechanism as an analogy that does not need any assumption on datapoint order and introduce extra position embeddings for capturing temporal patterns. FourierGNN performs 2D DFT on each discrete $N\\times T$ spatial-temporal plane of the embeddings $\\mathbf{X}$ instead of the raw data $X$. A key insight underpinning FourierGNN is to introduce variable embeddings and temporal position embeddings to equip FourierGNN with a sense of variable correlations (spatial patterns and temporal patterns).\n\n%To verify the claim, we conducted experiments on the dataset ECG. Specifically, we randomly shuffle the order of time-series variables of the raw data five times and evaluate our FourierGNN on each shuffled data. The results are reported in the following table. Note that we can also conduct shuffling experiments on temporal order via performing the same shuffling scheme over each window-sized time-series input, which will get the same conclusion. %We will provide discussions about this issue in the paper.\n\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Five-round results on randomly shuffled data on the dataset ECG.}\n%     \\begin{tabular}{l|ccccc|c}\n%     \\toprule\n%     Metric & R1    & R2    & R3    & R4    & R5    & Raw \\\\\n%     \\midrule\n%     MAE   & 0.052 & 0.053 & 0.053 & 0.053 & 0.052 & 0.052 \\\\\n%     RMSE  & 0.078 & 0.078 & 0.078 & 0.078 & 0.078 & 0.078 \\\\\n%     MAPE(\\%) & 10.95 & 10.98 & 11.02 & 10.99 & 10.99 & 11.05 \\\\\n%     \\bottomrule\n%     \\end{tabular}%\n%   \\label{tab:addlabel}%\n% \\end{table}%\n\n"
                }
            },
            "section 15": {
                "name": "Visualizations",
                "content": " \\label{visualization_filter}\n%To understand how FourierGNN works, we visualize the diffusion process of the Fourier graph network. We choose 10 counties from COVID-19 datasets and calculate the adjacency matrix between them. Then we produce the heat map of these adjacency matrix and the results are shown in Figure\\ref{fig:filters}. From left to right, they are the results of the original Fourier input $\\mathcal{X}$, the first filter, the second filter and the third filter, respectively. From  Figure \\ref{filter_a}, we can find that as the number of layers increases, some information is filtered out. In contrast, Figure \\ref{filter_b} illustrates some information is enhanced as the number of layers increases. Combining Figure\\ref{filter_a} and Figure\\ref{filter_b}, we can find that FourierGNN is adaptively and can capture the pattern while removing impurities.\n\n%To demonstrate the ability of our FourierGNN in jointly learning spatial-temporal dependencies, we visualize the temporal adjacency matrix of different variables. Note that the spatial adjacency matrices of different days are reported in Figure \\ref{fig:space_adj}. Specifically, we randomly select $8$ counties from the COVID-19 dataset and calculate the correlations of $12$ consecutive time steps for each county. Then we visualize the adjacency matrix via heat map, and the results are shown in Figure \\ref{fig:TIME_ADJ} where $N$ denotes the index of the country (variable). From the figure, we observe that FourierGNN learns clear and specific temporal patterns for each county. These results show that our FourierGNN can not only learn highly interpretable spatial correlations (see Figure \\ref{fig:visual_metr} and Figure \\ref{fig:space_adj}), but also capture discriminative temporal patterns.\n\n",
                "subsection 15.1": {
                    "name": "Visualization of the Diffusion Process in FourierGNN",
                    "content": "\\label{FGO_visualization}\nTo gain insight into the operation of the FGO, we visualize the frequency output of each layer in our FourierGNN. We select 10 counties from the COVID-19 dataset and visualize their adjacency matrices at two different timestamps, as shown in Figure \\ref{fig:filters}. From left to right, the results correspond to the original spectrum of the input, as well as the outputs of the first, second, and third layers of the FourierGNN. From the top, we can find that as the number of layers increases, some correlation values are reduced, indicating that some correlations are filtered out. In contrast, the bottom case illustrates some correlations are enhanced as the number of layers increases. These results show that FGO can adaptively and effectively capture important patterns while removing noises, enabling the learning of a discriminative model. \n%Additional visualizations can be found in Appendix \\ref{visualization_filter}.\n\n\n\n"
                },
                "subsection 15.2": {
                    "name": "Visualization of Time-Varying Dependencies Learned by FourierGNN",
                    "content": "\\label{time_varying_visualization}\nFurthermore, we explore the capability of FourierGNN in capturing time-varying dependencies among variables. To investigate this, we perform additional experiments to visualize the adjacency matrix of $10$ randomly-selected counties over four consecutive days on the COVID-19 dataset. The visualization results, displayed as a heatmap in Figure \\ref{fig:space_adj}, reveal clear spatial patterns that exhibit continuous evolution in the temporal dimension. This is because FourierGNN can attend to the time-varying variability of the spatiotemporal dependencies. These results verify that our model enjoys the feasibility of exploiting the time-varying dependencies among variables. \n\nBased on the insights gained from these visualization results, we can conclude that the hypervariate graph structure exhibits strong capabilities to encode spatiotemporal dependencies. By incorporating FGOs, FourierGNN can effectively attend to and exploit the time-varying dependencies among variates. The synergy between the hypervariate graph structure and FGOs empowers FourierGNN to capture and model intricate spatiotemporal relationships with remarkable effectiveness.\n\n\n\n\n\n\n\n\\iffalse\n"
                }
            },
            "section 16": {
                "name": "Submission of papers to NeurIPS 2022",
                "content": "\n\n\nPlease read the instructions below carefully and follow them faithfully.\n\n\n",
                "subsection 16.1": {
                    "name": "Style",
                    "content": "\n\n\nPapers to be submitted to NeurIPS 2022 must be prepared according to the\ninstructions presented here. Papers may only be up to {\\bf nine} pages long,\nincluding figures. Additional pages \\emph{containing only acknowledgments and\nreferences} are allowed. Papers that exceed the page limit will not be\nreviewed, or in any other way considered for presentation at the conference.\n\n\nThe margins in 2022 are the same as those in 2007, which allow for $\\sim$$15\\%$\nmore words in the paper compared to earlier years.\n\n\nAuthors are required to use the NeurIPS \\LaTeX{} style files obtainable at the\nNeurIPS website as indicated below. Please make sure you use the current files\nand not previous versions. Tweaking the style files may be grounds for\nrejection.\n\n\n"
                },
                "subsection 16.2": {
                    "name": "Retrieval of style files",
                    "content": "\n\n\nThe style files for NeurIPS and other conference information are available on\nthe World Wide Web at\n\\begin{center}\n  \\url{http://www.neurips.cc/}\n\\end{center}\nThe file \\verb+neurips_2022.pdf+ contains these instructions and illustrates the\nvarious formatting requirements your NeurIPS paper must satisfy.\n\n\nThe only supported style file for NeurIPS 2022 is \\verb+neurips_2022.sty+,\nrewritten for \\LaTeXe{}.  \\textbf{Previous style files for \\LaTeX{} 2.09,\n  Microsoft Word, and RTF are no longer supported!}\n\n\nThe \\LaTeX{} style file contains three optional arguments: \\verb+final+, which\ncreates a camera-ready copy, \\verb+preprint+, which creates a preprint for\nsubmission to, e.g., arXiv, and \\verb+nonatbib+, which will not load the\n\\verb+natbib+ package for you in case of package clash.\n\n\n\\paragraph{Preprint option}\nIf you wish to post a preprint of your work online, e.g., on arXiv, using the\nNeurIPS style, please use the \\verb+preprint+ option. This will create a\nnonanonymized version of your work with the text ``Preprint. Work in progress.''\nin the footer. This version may be distributed as you see fit. Please \\textbf{do\n  not} use the \\verb+final+ option, which should \\textbf{only} be used for\npapers accepted to NeurIPS.\n\n\nAt submission time, please omit the \\verb+final+ and \\verb+preprint+\noptions. This will anonymize your submission and add line numbers to aid\nreview. Please do \\emph{not} refer to these line numbers in your paper as they\nwill be removed during generation of camera-ready copies.\n\n\nThe file \\verb+neurips_2022.tex+ may be used as a ``shell'' for writing your\npaper. All you have to do is replace the author, title, abstract, and text of\nthe paper with your own.\n\n\nThe formatting instructions contained in these style files are summarized in\nSections \\ref{gen_inst}, \\ref{headings}, and \\ref{others} below.\n\n\n"
                }
            },
            "section 17": {
                "name": "General formatting instructions",
                "content": "\n\\label{gen_inst}\n\n\nThe text must be confined within a rectangle 5.5~inches (33~picas) wide and\n9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point\ntype with a vertical spacing (leading) of 11~points.  Times New Roman is the\npreferred typeface throughout, and will be selected for you by default.\nParagraphs are separated by \\nicefrac{1}{2}~line space (5.5 points), with no\nindentation.\n\n\nThe paper title should be 17~point, initial caps/lower case, bold, centered\nbetween two horizontal rules. The top rule should be 4~points thick and the\nbottom rule should be 1~point thick. Allow \\nicefrac{1}{4}~inch space above and\nbelow the title to rules. All pages should start at 1~inch (6~picas) from the\ntop of the page.\n\n\nFor the final version, authors' names are set in boldface, and each name is\ncentered above the corresponding address. The lead author's name is to be listed\nfirst (left-most), and the co-authors' names (if different address) are set to\nfollow. If there is only one co-author, list both author and co-author side by\nside.\n\n\nPlease pay special attention to the instructions in Section \\ref{others}\nregarding figures, tables, acknowledgments, and references.\n\n\n"
            },
            "section 18": {
                "name": "Headings: first level",
                "content": "\n\\label{headings}\n\n\nAll headings should be lower case (except for first word and proper nouns),\nflush left, and bold.\n\n\nFirst-level headings should be in 12-point type.\n\n\n",
                "subsection 18.1": {
                    "name": "Headings: second level",
                    "content": "\n\n\nSecond-level headings should be in 10-point type.\n\n\n",
                    "subsubsection 18.1.1": {
                        "name": "Headings: third level",
                        "content": "\n\n\nThird-level headings should be in 10-point type.\n\n\n\\paragraph{Paragraphs}\n\n\nThere is also a \\verb+\\paragraph+ command available, which sets the heading in\nbold, flush left, and inline with the text, with the heading followed by 1\\,em\nof space.\n\n\n"
                    }
                }
            },
            "section 19": {
                "name": "Citations, figures, tables, references",
                "content": "\n\\label{others}\n\n\nThese instructions apply to everyone.\n\n\n",
                "subsection 19.1": {
                    "name": "Citations within the text",
                    "content": "\n\n\nThe \\verb+natbib+ package will be loaded for you by default.  Citations may be\nauthor/year or numeric, as long as you maintain internal consistency.  As to the\nformat of the references themselves, any style is acceptable as long as it is\nused consistently.\n\n\nThe documentation for \\verb+natbib+ may be found at\n\\begin{center}\n  \\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}\n\\end{center}\nOf note is the command \\verb+\\citet+, which produces citations appropriate for\nuse in inline text.  For example,\n\\begin{verbatim}\n   \\citet{hasselmo} investigated\\dots\n\\end{verbatim}\nproduces\n\\begin{quote}\n  Hasselmo, et al.\\ (1995) investigated\\dots\n\\end{quote}\n\n\nIf you wish to load the \\verb+natbib+ package with options, you may add the\nfollowing before loading the \\verb+neurips_2022+ package:\n\\begin{verbatim}\n   \\PassOptionsToPackage{options}{natbib}\n\\end{verbatim}\n\n\nIf \\verb+natbib+ clashes with another package you load, you can add the optional\nargument \\verb+nonatbib+ when loading the style file:\n\\begin{verbatim}\n   \\usepackage[nonatbib]{neurips_2022}\n\\end{verbatim}\n\n\nAs submission is double blind, refer to your own published work in the third\nperson. That is, use ``In the previous work of Jones et al.\\ [4],'' not ``In our\nprevious work [4].'' If you cite your other papers that are not widely available\n(e.g., a journal paper under review), use anonymous author names in the\ncitation, e.g., an author of the form ``A.\\ Anonymous.''\n\n\n"
                },
                "subsection 19.2": {
                    "name": "Footnotes",
                    "content": "\n\n\nFootnotes should be used sparingly.  If you do require a footnote, indicate\nfootnotes with a number\\footnote{Sample of the first footnote.} in the\ntext. Place the footnotes at the bottom of the page on which they appear.\nPrecede the footnote with a horizontal rule of 2~inches (12~picas).\n\n\nNote that footnotes are properly typeset \\emph{after} punctuation\nmarks.\\footnote{As in this example.}\n\n\n"
                },
                "subsection 19.3": {
                    "name": "Figures",
                    "content": "\n\n\n\n\n\nAll artwork must be neat, clean, and legible. Lines should be dark enough for\npurposes of reproduction. The figure number and caption always appear after the\nfigure. Place one line space before the figure caption and one line space after\nthe figure. The figure caption should be lower case (except for first word and\nproper nouns); figures are numbered consecutively.\n\n\nYou may use color figures.  However, it is best for the figure captions and the\npaper body to be legible if the paper is printed in either black/white or in\ncolor.\n\n\n"
                },
                "subsection 19.4": {
                    "name": "Tables",
                    "content": "\n\n\nAll tables must be centered, neat, clean and legible.  The table number and\ntitle always appear before the table.  See Table~\\ref{sample-table}.\n\n\nPlace one line space before the table title, one line space after the\ntable title, and one line space after the table. The table title must\nbe lower case (except for first word and proper nouns); tables are\nnumbered consecutively.\n\n\nNote that publication-quality tables \\emph{do not contain vertical rules.} We\nstrongly suggest the use of the \\verb+booktabs+ package, which allows for\ntypesetting high-quality, professional tables:\n\\begin{center}\n  \\url{https://www.ctan.org/pkg/booktabs}\n\\end{center}\nThis package was used to typeset Table~\\ref{sample-table}.\n\n\n\n\n\n"
                }
            },
            "section 20": {
                "name": "Final instructions",
                "content": "\n\n\nDo not change any aspects of the formatting parameters in the style files.  In\nparticular, do not modify the width or length of the rectangle the text should\nfit into, and do not change font sizes (except perhaps in the\n\\textbf{References} section; see below). Please note that pages should be\nnumbered.\n\n\n"
            },
            "section 21": {
                "name": "Preparing PDF files",
                "content": "\n\n\nPlease prepare submission files with paper size ``US Letter,'' and not, for\nexample, ``A4.''\n\n\nFonts were the main cause of problems in the past years. Your PDF file must only\ncontain Type 1 or Embedded TrueType fonts. Here are a few instructions to\nachieve this.\n\n\n\\begin{itemize}\n\n\n\\item You should directly generate PDF files using \\verb+pdflatex+.\n\n\n\\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the\n  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can\n  also use the program \\verb+pdffonts+ which comes with \\verb+xpdf+ and is\n  available out-of-the-box on most Linux machines.\n\n\n\\item The IEEE has recommendations for generating PDF files whose fonts are also\n  acceptable for NeurIPS. Please see\n  \\url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}\n\n\n\\item \\verb+xfig+ \"patterned\" shapes are implemented with bitmap fonts.  Use\n  \"solid\" shapes instead.\n\n\n\\item The \\verb+\\bbold+ package almost always uses bitmap fonts.  You should use\n  the equivalent AMS Fonts:\n\\begin{verbatim}\n   \\usepackage{amsfonts}\n\\end{verbatim}\nfollowed by, e.g., \\verb+\\mathbb{R}+, \\verb+\\mathbb{N}+, or \\verb+\\mathbb{C}+\nfor $\\mathbb{R}$, $\\mathbb{N}$ or $\\mathbb{C}$.  You can also use the following\nworkaround for reals, natural and complex:\n\\begin{verbatim}\n   \\newcommand{\\RR}{I\\!\\!R} %real numbers\n   \\newcommand{\\Nat}{I\\!\\!N} %natural numbers\n   \\newcommand{\\CC}{I\\!\\!\\!\\!C} %complex numbers\n\\end{verbatim}\nNote that \\verb+amsfonts+ is automatically loaded by the \\verb+amssymb+ package.\n\n\n\\end{itemize}\n\n\nIf your file contains type 3 fonts or non embedded TrueType fonts, we will ask\nyou to fix it.\n\n\n",
                "subsection 21.1": {
                    "content": "\n\n\nMost of the margin problems come from figures positioned by hand using\n\\verb+\\special+ or other commands. We suggest using the command\n\\verb+\\includegraphics+ from the \\verb+graphicx+ package. Always specify the\nfigure width as a multiple of the line width as in the example below:\n\\begin{verbatim}\n   \\usepackage[pdftex]{graphicx} ...\n   \\includegraphics[width=0.8\\linewidth]{myfile.pdf}\n\\end{verbatim}\nSee Section 4.4 in the graphics bundle documentation\n(\\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})\n\n\nA number of width problems arise when \\LaTeX{} cannot properly hyphenate a\nline. Please give LaTeX hyphenation hints using the \\verb+\\-+ command when\nnecessary.\n\n\n\\begin{ack}\nUse unnumbered first level headings for the acknowledgments. All acknowledgments\ngo at the end of the paper before the list of references. Moreover, you are required to declare\nfunding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).\nMore information about this disclosure can be found at: \\url{https://neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure}.\n\n\nDo {\\bf not} include this section in the anonymized submission, only in the final paper. You can use the \\texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.\n\\end{ack}\n\n\n"
                }
            },
            "section 22": {
                "name": "References",
                "content": "\n\n\nReferences follow the acknowledgments. Use unnumbered first-level heading for\nthe references. Any choice of citation style is acceptable as long as you are\nconsistent. It is permissible to reduce the font size to \\verb+small+ (9 point)\nwhen listing the references.\nNote that the Reference section does not count towards the page limit.\n\\medskip\n\n\n{\n\\small\n\n\n[1] Alexander, J.A.\\ \\& Mozer, M.C.\\ (1995) Template-based algorithms for\nconnectionist rule extraction. In G.\\ Tesauro, D.S.\\ Touretzky and T.K.\\ Leen\n(eds.), {\\it Advances in Neural Information Processing Systems 7},\npp.\\ 609--616. Cambridge, MA: MIT Press.\n\n\n[2] Bower, J.M.\\ \\& Beeman, D.\\ (1995) {\\it The Book of GENESIS: Exploring\n  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:\nTELOS/Springer--Verlag.\n\n\n[3] Hasselmo, M.E., Schnell, E.\\ \\& Barkai, E.\\ (1995) Dynamics of learning and\nrecall at excitatory recurrent synapses and cholinergic modulation in rat\nhippocampal region CA3. {\\it Journal of Neuroscience} {\\bf 15}(7):5249-5262.\n}\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
            },
            "section 23": {
                "name": "Checklist",
                "content": "\n\n\n%%% BEGIN INSTRUCTIONS %%%\nThe checklist follows the references.  Please\nread the checklist guidelines carefully for information on how to answer these\nquestions.  For each question, change the default \\answerTODO{} to \\answerYes{},\n\\answerNo{}, or \\answerNA{}.  You are strongly encouraged to include a {\\bf\njustification to your answer}, either by referencing the appropriate section of\nyour paper or providing a brief inline description.  For example:\n\\begin{itemize}\n  \\item Did you include the license to the code and datasets? \\answerYes{See Section~\\ref{gen_inst}.}\n  \\item Did you include the license to the code and datasets? \\answerNo{The code and the data are proprietary.}\n  \\item Did you include the license to the code and datasets? \\answerNA{}\n\\end{itemize}\nPlease do not modify the questions and only use the provided macros for your\nanswers.  Note that the Checklist section does not count towards the page\nlimit.  In your paper, please delete this instructions block and only keep the\nChecklist section heading above along with the questions/answers below.\n%%% END INSTRUCTIONS %%%\n\n\n\\begin{enumerate}\n\n\n\\item For all authors...\n\\begin{enumerate}\n  \\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n    \\answerTODO{}\n  \\item Did you describe the limitations of your work?\n    \\answerTODO{}\n  \\item Did you discuss any potential negative societal impacts of your work?\n    \\answerTODO{}\n  \\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\n    \\answerTODO{}\n\\end{enumerate}\n\n\n\\item If you are including theoretical results...\n\\begin{enumerate}\n  \\item Did you state the full set of assumptions of all theoretical results?\n    \\answerTODO{}\n        \\item Did you include complete proofs of all theoretical results?\n    \\answerTODO{}\n\\end{enumerate}\n\n\n\\item If you ran experiments...\n\\begin{enumerate}\n  \\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\n    \\answerTODO{}\n  \\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\n    \\answerTODO{}\n        \\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\n    \\answerTODO{}\n        \\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\n    \\answerTODO{}\n\\end{enumerate}\n\n\n\\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n\\begin{enumerate}\n  \\item If your work uses existing assets, did you cite the creators?\n    \\answerTODO{}\n  \\item Did you mention the license of the assets?\n    \\answerTODO{}\n  \\item Did you include any new assets either in the supplemental material or as a URL?\n    \\answerTODO{}\n  \\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\n    \\answerTODO{}\n  \\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\n    \\answerTODO{}\n\\end{enumerate}\n\n\n\\item If you used crowdsourcing or conducted research with human subjects...\n\\begin{enumerate}\n  \\item Did you include the full text of instructions given to participants and screenshots, if applicable?\n    \\answerTODO{}\n  \\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\n    \\answerTODO{}\n  \\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\n    \\answerTODO{}\n\\end{enumerate}\n\n\n\\end{enumerate}\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\n\\appendix\n\n\n"
            },
            "section 24": {
                "name": "Appendix",
                "content": "\n\n\nOptionally include extra information (complete proofs, additional experiments and plots) in the appendix.\nThis section will often be part of the supplemental material.\n\\fi\n\n"
            }
        },
        "tables": {
            "tab:result": "\\begin{table*}[ht]\n    \\centering\n    \\caption{Overall performance of forecasting models on the six datasets.}\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l|c c c|c c c|c c c}\n    \\toprule\n         \\multirow{2}{*}{\\diagbox{{Models}}{{Datasets}}} & \\multicolumn{3}{c|}{Solar} &  \\multicolumn{3}{c|}{Wiki} & \\multicolumn{3}{c}{Traffic}\\\\\n         & MAE & RMSE & MAPE(\\%) &  MAE & RMSE & MAPE(\\%) & MAE & RMSE & MAPE(\\%) \\\\\n         \\midrule\n         VAR \\cite{Vector1993} & 0.184& 0.234 & 577.10 & 0.057& 0.094 & 96.58 & 0.535 &1.133 & 550.12 \\\\\n         SFM \\cite{ZhangAQ17}& 0.161 & 0.283 & 362.89 & 0.081 & 0.156 &104.47 & 0.029 & 0.044 &59.33 \\\\\n         LSTNet \\cite{Lai2018} & 0.148 &0.200 & 132.95 & 0.054 &0.090 &118.24 & 0.026 &0.057 & \\textbf{25.77}\\\\\n         TCN \\cite{bai2018} & 0.176 & 0.222 &142.23 & 0.094 & 0.142 & 99.66 & 0.052 & 0.067 & -\\\\\n         DeepGLO \\cite{Sen2019} & 0.178 &0.400 & 346.78 & 0.110 & 0.113 &119.60 & 0.025 & 0.037 & 33.32 \\\\\n         Reformer \\cite{reformer20} & 0.234 & 0.292 & 128.58&0.048 & 0.085 & \\underline{73.61} & 0.029&0.042 & 112.58\\\\\n         Informer \\cite{Zhou2021} & 0.151 & {0.199} &128.45 & 0.051 & 0.086 & 80.50 & 0.020 &0.033 & 59.34\\\\\n         Autoformer \\cite{autoformer21} & 0.150& 0.193 &103.79 &0.069 & 0.103& 121.90& 0.029 & 0.043 & 100.02\\\\\n         FEDformer \\cite{fedformer22} & 0.139&\\underline{0.182} & \\textbf{100.92} &0.068&0.098 & 123.10&0.025&0.038 & 85.12\\\\\n         GraphWaveNet \\cite{zonghanwu2019}& 0.183 & 0.238 & 603 & 0.061 & 0.105 &136.12 & \\underline{0.013} & 0.034 & 33.78\\\\\n         StemGNN \\cite{Cao2020}& 0.176 &0.222 & {128.39} & 0.190 &0.255 & 117.92 & 0.080 &0.135 & 64.51 \\\\\n         MTGNN \\cite{wu2020connecting}& 0.151 & 0.207 &507.91 & 0.101 & 0.140 & 122.96 & 0.013 & \\underline{0.030} & 29.53\\\\\n         AGCRN \\cite{Bai2020nips}& \\underline{0.123} &0.214 & 353.03 & \\underline{0.044} & \\underline{0.079} & {78.52} & 0.084 & 0.166 & 31.73\\\\\n         \\midrule\n         \\textbf{FourierGNN} & \\textbf{0.120} &\\textbf{0.162} & {116.48} & \\textbf{0.041} &\\textbf{0.076} & \\textbf{64.50} & \\textbf{0.011} &\\textbf{0.023} & 28.71\\\\\n         %Improvement &2.4\\% &11.0\\% & -& 6.8\\% &3.8\\% & 12.4\\%& 15.4\\% &23.3\\% & -\\\\\n         \\bottomrule\n          \\multirow{2}{*}{\\diagbox{{Models}}{{Datasets}}} & \\multicolumn{3}{c|}{ECG} &  \\multicolumn{3}{c|}{Electricity} & \\multicolumn{3}{c}{COVID-19} \\\\\n         & MAE & RMSE & MAPE(\\%) &  MAE & RMSE & MAPE(\\%) & MAE & RMSE & MAPE(\\%)\\\\\n         \\midrule\n         VAR \\cite{Vector1993} & 0.120& 0.170 & 22.56 & 0.101& 0.163 & 43.11 & 0.226 & 0.326&191.95 \\\\\n         SFM \\cite{ZhangAQ17} & 0.095 & 0.135 & 24.20 &0.086 &0.129 & 33.71 &0.205 &0.308 &76.08  \\\\\n         LSTNet \\cite{Lai2018} & 0.079 &0.115 & 18.68 & 0.075 &0.138 & 29.95 & 0.248  & 0.305 & 89.04\\\\\n         TCN \\cite{bai2018} & 0.078 & 0.107 & 17.59 & 0.057 & 0.083 & 26.64 & 0.317 & 0.354 & 151.78 \\\\\n         DeepGLO \\cite{Sen2019} & 0.110 &0.163 & 43.90 & 0.090 & 0.131 & 29.40& 0.169 & 0.253 & \\underline{75.19}\\\\\n         Reformer \\cite{reformer20} & 0.062& 0.090 & 13.58 &0.078 & 0.129 &33.37 &\\underline{0.152} & \\underline{0.209} &132.78\\\\\n         Informer \\cite{Zhou2021} & 0.056 &0.085 & 11.99 & 0.070 & 0.119 &32.66 & 0.200 & 0.259 &155.55\\\\\n         Autoformer \\cite{autoformer21} & 0.055 &0.081 & 11.37 &{0.056} & {0.083} & 25.94&0.159 & 0.211&136.24 \\\\\n         FEDformer \\cite{fedformer22} & 0.055 & 0.080 & \\underline{11.16} & \\underline{0.055}& \\underline{0.081} & \\underline{25.84}& 0.160&0.219 &134.45\\\\\n         GraphWaveNet \\cite{zonghanwu2019} & 0.093 & 0.142 &40.19 & 0.094 & 0.140 & 37.01  & 0.201 & 0.255 & 100.83\\\\\n         StemGNN \\cite{Cao2020} & 0.100 &0.130 & 29.62 & 0.070 & 0.101 & - & 0.421 & 0.508 & 141.01\\\\\n         MTGNN \\cite{wu2020connecting} & 0.090 & 0.139 & 35.04 & 0.077 & 0.113 & 29.77 & 0.394 & 0.488 & 88.13\\\\\n         AGCRN \\cite{Bai2020nips} & \\underline{0.055} & \\underline{0.080} & {11.75} & 0.074 & 0.116 & {26.08} & 0.254 & 0.309 & {83.37}\\\\\n         %TAMP-S2GNets  \\cite{tampsgcnets2022} &- & -&- &  -& -& -& 0.180 & \\underline{0.230} & \\textbf{65.76}\\\\\n         \\midrule\n         \\textbf{FourierGNN} & \\textbf{0.052} &\\textbf{0.078} & \\textbf{10.97} & \\textbf{0.051} &\\textbf{0.077} & \\textbf{24.28} & \\textbf{0.123}  & \\textbf{0.168} & \\textbf{71.52}\\\\\n         %Improvement &5.5\\% &2.5\\% & 1.7\\%& 7.3\\% &4.9\\% & 6.0\\%& 19.1\\% &19.6\\% & - \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\label{tab:result}\n    \\vspace{-3mm}\n\\end{table*}",
            "tab:covid_result": "\\begin{table*}[!t]\n    \\centering\n    \\caption{Performance comparison under different prediction lengths on the COVID-19 dataset.% We compare FourierGNN with other five GNN-based baseline models on the COVID-19 dataset when prediction length is 3, 6, 9, and 12, respectively.\n    }\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l|c c c|c c c|c c c|c c c}\n    \\toprule\n       Length & \\multicolumn{3}{c|}{3} &  \\multicolumn{3}{c|}{6} & \\multicolumn{3}{c|}{9}  & \\multicolumn{3}{c}{12}\\\\\n       Metrics & MAE &RMSE&MAPE(\\%)&  MAE& RMSE&MAPE(\\%)& MAE &RMSE&MAPE(\\%) & MAE& RMSE&MAPE(\\%)\\\\\n       \\midrule\n         GraphWaveNet \\cite{zonghanwu2019} & \\underline{0.092} & \\underline{0.129} & 53.00& \\underline{0.133} & \\underline{0.179} & 65.11& 0.171 &0.225 & 80.91& 0.201 &0.255&100.83\\\\\n         StemGNN \\cite{Cao2020} & 0.247 &0.318& 99.98& 0.344 &0.429& 125.81& 0.359 &0.442&131.14 & 0.421 &0.508&141.01\\\\\n         AGCRN \\cite{Bai2020nips} & 0.130 &0.172 &76.73 & 0.171 &0.218& 79.07& 0.224 &0.277 & 82.90& 0.254 & 0.309 & {83.37}\\\\\n         MTGNN \\cite{wu2020connecting}& 0.276 &0.379&91.42 & 0.446 &0.513& 133.49& 0.484 &0.548& 139.52& 0.394 &0.488&88.13 \\\\\n         TAMP-S2GCNets \\cite{tampsgcnets2022} & 0.140 & 0.190 & \\textbf{50.01} & 0.150 & 0.200 & \\textbf{55.72}& \\underline{0.170} & \\underline{0.230} & \\underline{71.78}& \\underline{0.180} & \\underline{0.230} & \\textbf{65.76}\\\\\n         CoST \\cite{cost22} & 0.122 & 0.246 & 68.74 & 0.157 & 0.318 & 72.84 & 0.183 & 0.364 & 77.04 & 0.202 & 0.377 & 80.81\\\\\n         \\midrule\n         \\textbf{FourierGNN(ours)} & \\textbf{0.071}  &\\textbf{0.103} & 61.02 & \\textbf{0.093} &  \\textbf{0.131} & 65.72& \\textbf{0.109}  &\\textbf{0.148} & \\textbf{69.59} & \\textbf{0.123} &\\textbf{0.168} & 71.52\\\\\n         \\bottomrule\n    \\end{tabular}\n    }\n    \\label{tab:covid_result}\n    \\vspace{-3mm}\n\\end{table*}",
            "tab:efficiency": "\\begin{table}[ht]\n    \\vspace{-2mm}\n    \\centering\n    \\small\n    \\caption{Comparisons of parameter volumes and training time costs on datasets Traffic and Wiki.}\n    \\begin{tabular}{c c c | c c c}\n    \\toprule\n        & \\multicolumn{2}{c}{Traffic} & \\multicolumn{2}{c}{Wiki} \\\\\n        Models & Parameters & Training (s/epoch) & Parameters & Training (s/epoch)\\\\\n    \\midrule\n        StemGNN & $1,606,140$ & 185.86$\\pm$2.22 & $4,102,406$ & 92.95$\\pm$1.39\\\\\n        MTGNN & $707,516$ & 169.34$\\pm$1.56 & $1,533,436 $& 28.69$\\pm$0.83\\\\\n        AGCRN & $749,940$ & 113.46$\\pm$1.91 & $755,740$ & 22.48$\\pm$1.01\\\\\n        GraphWaveNet & $280,860$ & 105.38$\\pm$1.24 & $292,460$ & 21.23$\\pm$0.76\\\\\n        \\midrule\n        FourierGNN & $190,564$ & 99.25$\\pm$1.07 & $264, 804$ & 16.28$\\pm$0.48\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:efficiency}\n    \\vspace{-3mm}\n\\end{table}",
            "tab:metr_ablation": "\\begin{table}[ht]\n    \\centering\n    \\caption{Ablation study on METR-LA dataset.}\n    \\scalebox{0.90}{\n    \\begin{tabular}{c c c c c c}\n    \\toprule\n     metrics & w/o Embedding & w/o Dynamic FGO & w/o Residual & w/o Summation& FourierGNN\\\\ \n    \\midrule\n       MAE  & 0.053 & 0.055 & 0.054 & 0.054 & 0.050\\\\\n       RMSE & 0.116 & 0.114& 0.115 &  0.114 & 0.113\\\\\n       MAPE(\\%) & 86.73 & 86.69 & 86.75 & 86.62 & 86.30\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\label{tab:metr_ablation}\n    \\vspace{-3mm}\n\\end{table}",
            "notion": "\\begin{table}[!h]\n    \\centering\n    \\renewcommand\\arraystretch{1.5}\n    \\caption{Notations}\n    \\begin{tabular}{l | l }\n    \\toprule[1pt]\n    %$\\mathbb{X}$ & entire multivariate time series data, $\\mathbb{X} \\in \\mathbb{R}^{N \\times L}$\\\\\n    $X_t$ & multivariate time series input at timestamps $t$, $X \\in \\mathbb{R}^{N \\times T}$\\\\\n    $x_t$ & the multivariate values of $N$ series at timestamp t, $x_t \\in \\mathbb{R}^{N}$\\\\\n    $Y_t$ & the next $\\tau$ timestamps of multivariate time series, $Y_t \\in \\mathbb{R}^{N \\times \\tau}$\\\\\n    $\\hat{Y}_t$ & the prediction values of multivariate time series for next $\\tau$ timestamps, $\\hat{Y}_t \\in \\mathbb{R}^{N \\times \\tau}$\\\\\n    $N$ & the number of series\\\\\n    %$L$ & the number of timestamps of $\\mathbb{X}$\\\\\n    $T$ & the lookback window size\\\\\n    $\\tau$ & the prediction length of multivariate time series forecasting\\\\\n    %$x_{it}$ & the value (node) of the $i$-th variable at $t$ timestamp in $X$ \\\\\n    %$\\mathcal{E}$ & the sets of edges attributed to $X$\\\\\n    % $\\mathcal{V}$ & the sets of nodes attributed to $X$\\\\\n    $\\mathcal{G}_t$ & the hypervariate graph, $\\mathcal{G}_t=\\{X_t^{\\mathcal{G}},A_t^{\\mathcal{G}}\\}$ attributed to $X_t^{\\mathcal{G}}$\\\\\n    $X_t^{\\mathcal{G}}$ & the nodes of the hypervariate graph, $X_t^{\\mathcal{G}} \\in \\mathbb{R}^{NT \\times 1}$\\\\\n    $A_t^{\\mathcal{G}}$ & the adjacency matrix of $\\mathcal{G}_t$, $A_t^{\\mathcal{G}} \\in \\mathbb{R}^{NT \\times NT}$\\\\\n    $\\mathcal{S}$ & the Fourier Graph Operator \\\\\n    $d$ & the embedding dimension\\\\\n    $\\mathbf{X}_t^{\\mathcal{G}}$ & the embedding of $X_t^{\\mathcal{G}}$, $\\mathbf{X}_t^{\\mathcal{G}} \\in \\mathbb{R}^{NT \\times d}$ \\\\\n    $\\mathcal{X}_t^{\\mathcal{G}}$ & the spectrum of $\\mathbf{X}_t^{\\mathcal{G}}$, $\\mathcal{X}_t^{\\mathcal{G}} \\in \\mathbb{C}^{NT \\times d}$\\\\\n    $\\mathcal{Y}_t^{\\mathcal{G}}$ & the output of FourierGNN, $\\mathcal{Y}_t^{\\mathcal{G}} \\in \\mathbb{C}^{NT \\times d}$\\\\\n    $\\theta_g$ & the parameters of the graph network \\\\\n    $\\theta_t$ & the parameters of the temporal network \\\\\n    $\\theta_{\\mathcal{G}}$ &  the network parameters for hypervariate graphs\\\\\n    $E_\\phi$ & the embedding matrix, $E_\\phi \\in \\mathbb{R}^{1 \\times d}$\\\\\n    $\\kappa$ & the kernel function\\\\\n    $W$ & the weight matrix\\\\\n    $b$ & the complex bias weights\\\\\n    $\\mathcal{F}$ & Discrete Fourier Transform\\\\\n    $\\mathcal{F}^{-1}$ & Inverse Discrete Fourier Transform \\\\\n    $\\operatorname{F}$ & the forecasting model\\\\\n    %$\\tau$ & the prediction length\\\\\n    \\bottomrule[1pt]\n    \\end{tabular}\n    \\label{notion}\n\\end{table}",
            "tab:n_invariant": "\\begin{table}[!h]\\small\n    \\centering\n    \\caption{Comparison between FourierGNN models with $n$-invariant FGO and $n$-variant FGO on the ECG and COVID-19 datasets.}\n    \\resizebox{\\textwidth}{14mm}{\n    \\begin{tabular}{c | c c c c c c}\n    \\toprule\n    Datasets &  Models   &  Parameters (M) & Training (s/epoch) & MAE & RMSE & MAPE (\\%)\\\\\n         \\midrule\n    \\multirow{2}*{\\rotatebox{0}{ECG}}  & $n$-invariant &  \\textbf{0.18} & \\textbf{12.45} & \\textbf{0.052} & 0.078 & \\textbf{10.97}  \\\\\n      & $n$-variant & 82.96 & 104.06 & 0.053 & 0.078 & 11.05 \\\\\n      \\midrule\n    \\multirow{2}*{\\rotatebox{0}{COVID-19}}  & $n$-invariant &  \\textbf{1.06} & \\textbf{0.62} & \\textbf{0.123} & \\textbf{0.168} & \\textbf{71.52}  \\\\\n      & $n$-variant & 130.99 & 7.46 & 0.129 & 0.174 & 72.12 \\\\\n      \\bottomrule\n    \\end{tabular}}\n    \\label{tab:n_invariant}\n\\end{table}",
            "tab:datasets": "\\begin{table}[ht]\\small\n    \\centering\n    \\caption{Summary of datasets.}\n    % \\resizebox{\\textwidth}{12mm}{\n    \\begin{tabular}{c | c c c c c c c}\n    \\toprule\n    Datasets & Solar & Wiki & Traffic & ECG & Electricity & COVID-19 & METR-LA\\\\\n    \\midrule\n      Samples & 3650 & 803& 10560& 5000 & 140211 & 335 & 34272\\\\\n      Variables & 592 & 2000& 963& 140 & 370 & 55 & 207 \\\\\n      Granularity & 1hour & 1day& 1hour& - & 15min& 1day & 5min\\\\\n      Start time & 01/01/2006 & 01/07/2015&01/01/2015 & - &01/01/2011 & 01/02/2020 & 01/03/2012\\\\\n    \\bottomrule\n    \\end{tabular}\n    % }\n    \\label{tab:datasets}\n\\end{table}",
            "tab:settings": "\\begin{table}[ht]\n    \\small\n    \\centering\n    \\caption{Dimension settings of FFN on different datasets. $*$ denotes that we feed the original time domain representation to FFN without the dimension reduction.}\n    \\renewcommand\\arraystretch{1.5}\n    \\begin{tabular}{c | c c c c c c c}\n    \\toprule\n    Datasets & Solar & Wiki & Traffic & ECG & Electricity & COVID-19 & META-LR\\\\\n    \\midrule\n      $l$ & 6 & 2& 2& $*$ & 4 & 8 & 4\\\\\n      $d^{ffn}_1$ & 64 & 64& 64& 64 & 64 & 256 & 64 \\\\\n      $d^{ffn}_2$ & 256 & 256& 256& 256 & 256& 512 & 256\\\\\n    \\bottomrule\n    \\end{tabular}\n    % \\begin{threeparttable}\n    % % \\resizebox{\\textwidth}{12mm}\n    % {\n    % }\n    % \\footnotesize\n    % \\end{threeparttable}\n    \\centering\n    \\label{tab:settings}\n\\end{table}",
            "tab:wiki_result": "\\begin{table*}[ht]\n    \\centering\n    \\caption{Accuracy comparison under different prediction lengths on the Wiki dataset.}\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l|c c c|c c c|c c c|c c c}\n    \\toprule\n       Length & \\multicolumn{3}{c|}{3} &  \\multicolumn{3}{c|}{6} & \\multicolumn{3}{c|}{9}  & \\multicolumn{3}{c}{12}\\\\\n       Metrics & MAE &RMSE& MAPE(\\%)&  MAE& RMSE& MAPE(\\%)&MAE &RMSE &MAPE(\\%)& MAE& RMSE&MAPE(\\%)\\\\\n       \\midrule\n         GraphWaveNet \\cite{zonghanwu2019}& 0.061 &0.105 & 138.60 & 0.061 &0.105 &135.32 & 0.061 & 0.105 & 132.52& 0.061 &0.104 &136.12\\\\\n         StemGNN \\cite{Cao2020}& 0.157 &0.236 &89.00 & 0.159 &0.233 & 98.01 & 0.232 &0.311 & 142.14& 0.220 &0.306 &125.40\\\\\n         AGCRN \\cite{Bai2020nips}& \\underline{0.043} & \\underline{0.077}  & \\underline{73.49} & \\underline{0.044} & \\underline{0.078} & \\underline{80.44} & \\underline{0.045} & \\underline{0.079} & \\underline{81.89} & \\underline{0.044} & \\underline{0.079} &\\underline{78.52}\\\\\n         MTGNN \\cite{wu2020connecting}& 0.102 & 0.141 & 123.15 & 0.091 &0.133 & 91.75 & 0.074 &0.120 & 85.44& 0.101 & 0.140 &122.96 \\\\\n         Informer \\cite{Zhou2021}& 0.053 &0.089 & 85.31 & 0.054 &0.090 & 84.46 & 0.059 &0.095  & 93.80& 0.059 &0.095 &95.09\\\\\n         \\midrule\n         \\textbf{FourierGNN} & \\textbf{0.040} &\\textbf{0.075} & \\textbf{58.18}& \\textbf{0.041} &\\textbf{0.075} & \\textbf{60.43} & \\textbf{0.041} &\\textbf{0.076}  & \\textbf{60.95}& \\textbf{0.041} &\\textbf{0.076} & \\textbf{64.50}\\\\\n         %Improvement &7.0\\% &2.6\\% & 20.83\\%& 6.8\\% &3.8\\% & 24.9\\%& 8.9\\% &3.8\\% & 25.6\\%& 4.5\\% &2.5\\% &20.3\\%\\\\ \n         \\bottomrule\n    \\end{tabular}}\n    \\label{tab:wiki_result}\n\\end{table*}",
            "tab:metr_result": "\\begin{table*}[ht]\n    \\centering\n    \\caption{Accuracy comparison under different prediction lengths on the METR-LA dataset.}\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l|c c c|c c c|c c c|c c c}\n    \\toprule\n       Horizon & \\multicolumn{3}{c|}{3} &  \\multicolumn{3}{c|}{6} & \\multicolumn{3}{c|}{9}  & \\multicolumn{3}{c}{12}\\\\\n       Metrics & MAE &RMSE& MAPE(\\%)&  MAE& RMSE& MAPE(\\%)&MAE &RMSE &MAPE(\\%)& MAE& RMSE&MAPE(\\%)\\\\\n       \\midrule\n         DCRNN \\cite{LiYS018}& 0.160 & 0.204 & 80.00 & 0.191 & 0.243 & 83.15 & 0.216 & 0.269 & 85.72 & 0.241 & 0.291 & 88.25\\\\\n         STGCN \\cite{Yu2018}& 0.058 & 0.133& 59.02 &0.080 &0.177 &60.67 &0.102 & 0.209& 62.08 &0.128 &0.238 &63.81 \\\\\n         GraphWaveNet \\cite{zonghanwu2019}& 0.180 &0.366 & 21.90 & 0.184 &0.375 &22.95 & 0.196 & 0.382 & 23.61& 0.202 &0.386 &24.14\\\\\n         MTGNN \\cite{wu2020connecting}& 0.135 & 0.294 & \\textbf{17.99} & 0.144 &0.307 & \\textbf{18.82} & 0.149 &0.328 & \\textbf{19.38}& 0.153 & 0.316 &\\textbf{19.92} \\\\\n         StemGNN \\cite{Cao2020}& \\underline{0.052} & \\underline{0.115} &86.39 & \\underline{0.069} & \\underline{0.141} & 87.71 & \\underline{0.080} & 0.162 & 89.00& \\underline{0.093} & 0.175 &90.25\\\\\n         AGCRN \\cite{Bai2020nips}& 0.062 & 0.131  & 24.96 & 0.086 & 0.165 & 27.62 & 0.099 & 0.188 & 29.72 & 0.109 & 0.204 & 31.73\\\\\n         Informer \\cite{Zhou2021} & 0.076 &0.141 & 69.96 & 0.088 &0.163 & 70.94 & 0.096 &0.178  & 72.26& 0.100 &0.190 &72.54\\\\\n         %LSTNet &0.016 &0.023 & 2.55&0.051 & 0.103 & 8.25& 0.074 &0.139 & 11.94 &0.083 &0.151 &13.40 \\\\\n         CoST \\cite{cost22} &  0.064 &0.118 & 88.44 & 0.077 & \\underline{0.141} & 89.63 & 0.088 &\\textbf{0.159}  & 90.56& 0.097 & \\underline{0.171} &91.42\\\\\n         \\midrule\n         \\textbf{FourierGNN} & \\textbf{0.050} &\\textbf{0.113} & 86.30& \\textbf{0.066} &\\textbf{0.140} & 87.97 & \\textbf{0.076} &\\textbf{0.159}  & 88.99& \\textbf{0.084} &\\textbf{0.165} & 89.69\\\\\n         %Improvement &3.8\\% &1.7\\% & -& 4.3\\% &0.7\\% & -& 5.0\\% &- &-& 9.7\\% &3.5\\% &-\\\\ \n         \\bottomrule\n    \\end{tabular}}\n    \\label{tab:metr_result}\n\\end{table*}",
            "diffusion-step": "\\begin{wraptable}{r}{7.5cm}\\small\n    \\centering\n    \\caption{Performance at different diffusion steps (layers) on the COVID-19 dataset.}\n    \\begin{tabular}{c | c c c c}\n    \\toprule\n     & K=1  & K=2 & K=3 & K=4\\\\\n    \\midrule\n       MAE & 0.136 &  0.133 & \\underline{0.123} & 0.132 \\\\\n       RMSE  & 0.181 & 0.177 & \\underline{0.168} & 0.176\\\\\n       MAPE(\\%) & 72.30 & 71.80  & \\underline{71.52} & 72.59\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{diffusion-step}\n\\end{wraptable}",
            "tab:ablation_covid": "\\begin{table}[!h]\n    \\centering\n    \\caption{Ablation studies on the COVID-19 dataset.}\n    \\scalebox{0.9}{\n    \\begin{tabular}{c | c c c c c}\n    \\toprule[1pt]\n    Metric & w/o Embedding & w/o Dynamic FGO & w/o Residual & w/o Summation & FourierGNN\\\\ \n    \\midrule\n     MAE   &0.157&0.138&0.131&0.134&\\underline{0.123}\\\\\n     RMSE    &0.203&0.180&0.174&0.177&\\underline{0.168}\\\\\n     MAPE(\\%) &76.91&74.01&72.25&72.57&\\underline{71.52}\\\\\n     \\bottomrule[1pt]\n    \\end{tabular}\n    }\n    \\label{tab:ablation_covid}\n\\end{table}",
            "sample-table": "\\begin{table}\n  \\caption{Sample table title}\n  \\label{sample-table}\n  \\centering\n  \\begin{tabular}{lll}\n    \\toprule\n    \\multicolumn{2}{c}{Part}                   \\\\\n    \\cmidrule(r){1-2}\n    Name     & Description     & Size ($\\mu$m) \\\\\n    \\midrule\n    Dendrite & Input terminal  & $\\sim$100     \\\\\n    Axon     & Output terminal & $\\sim$10      \\\\\n    Soma     & Cell body       & up to $10^6$  \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:hypervariate_graph": "\\begin{wrapfigure}{r}{0cm}\n\\centering\n\\vspace{-0mm}\n \\includegraphics[width=0.38\\textwidth]{figures/hypergraph.pdf}\n \\vspace{-1mm}\n \\caption{Illustration of a hypervariate graph with three time series. Each value in the input window is considered as a node of the graph.}\n\\label{fig:hypervariate_graph}\n\\vspace{-3mm}\n\\end{wrapfigure}",
            "fig:model_fig": "\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/FourierGNN_model_new_2.pdf}\n    \\vspace{-3mm}\n    \\caption{The network architecture of MTS forecasting with FourierGNN (\\textcolor{cyan}{blue characters} denote complex values, such as \\textcolor{cyan}{$\\mathcal{X}_t^{\\mathcal{G}}$ $\\mathcal{S}_i$}). Given the hypervariate graph $\\mathcal{G}=(X^\\mathcal{G}_t, A^\\mathcal{G}_t)$, we 1) embed nodes of $X^\\mathcal{G}_t\\in \\mathbb{R}^{NT \\times 1}$ to obtain node embeddings $\\mathbf{X}^\\mathcal{G}_t \\in \\mathbb{R}^{NT \\times d}$; 2) feed embedded hypervariate graphs to $\\operatorname{FourierGNN}$: \n    (i) transform $\\mathbf{X}^\\mathcal{G}_t$ with DFT to $\\mathcal{X}^\\mathcal{G}_t \\in \\mathbb{C}^{NT \\times d}$; \n    (ii) conduct recursive multiplications and make summation to output $\\mathcal{Y}^\\mathcal{G}_t$; \n    (iii) transform $\\mathcal{Y}^\\mathcal{G}_t$ back to time domain by IDFT, resulting in $\\mathbf{Y}^\\mathcal{G}_t \\in \\mathbb{R}^{NT \\times d}$; \n    3) generate $\\tau$-step predictions $\\hat{Y}_t \\in \\mathbb{R}^{N \\times \\tau}$ via feeding $\\mathbf{Y}^\\mathcal{G}_t$ to fully-connected layers.}\n    \\label{fig:model_fig}\n    \\vspace{-3mm}\n\\end{figure}",
            "fig:TIME_ADJ": "\\begin{figure}[ht]\n    \\vspace{-3mm}\n    \\centering\n    \\subfigure[N=7]\n    {\n        \\centering\n        \\includegraphics[width=0.23\\linewidth]{figures/N=7_T.png}\n         \\label{N=7}\n    }\n    \\subfigure[N=18]\n    {\n        \\centering\n        \\includegraphics[width=0.23\\linewidth]{figures/N=18_T.png}\n        \\label{N=18}\n    }\n    \\subfigure[N=24]\n    {\n        \\centering\n        \\includegraphics[width=0.23\\linewidth]{figures/N=24_T.png}\n        \\label{N=24}\n    }\n    \\subfigure[N=29]\n    {\n        \\centering\n        \\includegraphics[width=0.23\\linewidth]{figures/N=29_T.png}\n        \\label{N=29}\n    }\n    \\vspace{-3mm}\n    \n    \\subfigure[N=38]\n    {\n        \\centering\n        \\includegraphics[width=0.23\\linewidth]{figures/N=38_T.png}\n         \\label{N=38}\n    }\n    \\subfigure[N=45]\n    {\n        \\centering\n        \\includegraphics[width=0.22\\linewidth]{figures/N=45_T.png}\n        \\label{N=45}\n    }\n    \\subfigure[N=46]\n    {\n        \\centering\n        \\includegraphics[width=0.23\\linewidth]{figures/N=46_T.png}\n        \\label{N=46}\n    }\n    \\subfigure[N=55]\n    {\n        \\centering\n        \\includegraphics[width=0.23\\linewidth]{figures/N=55_T.png}\n        \\label{N=55}\n    }\n    \\caption{The temporal adjacency matrix of eight variates on COVID-19 dataset.}\n    \\label{fig:TIME_ADJ}\n    \\vspace{-5mm}\n\\end{figure}",
            "fig:visual_metr": "\\begin{wrapfigure}{r}{0cm}\n\\centering\n\\vspace{-7mm}\n \\includegraphics[width=0.55\\textwidth]{figures/fourier_gnn_vis_v2.png}\n \\caption{The adjacency matrix (right) learned by FourierGNN and the corresponding road map (left).}\n\\label{fig:visual_metr}\n\\vspace{-4mm}\n\\end{wrapfigure}",
            "fig:ecg": "\\begin{figure*}[ht]\n    \\vspace{-5mm}\n    \\centering\n    \\subfigure[MAE]\n    {\n        \\centering\n        \\includegraphics[width=0.3\\linewidth]{figures/ecg_mae_multistep_new.png}\n    }\n    \\subfigure[RMSE]\n    {\n        \\centering\n        \\includegraphics[width=0.3\\linewidth]{figures/ecg_rmse_multistep_new.png}\n    }\n    \\subfigure[MAPE]\n    {\n        \\centering\n        \\includegraphics[width=0.3\\linewidth]{figures/ecg_mape_multistep_new.png}\n    }\n\n    \\caption{Performance comparison in multi-step prediction on the ECG dataset.}\n    \\label{fig:ecg}\n\\end{figure*}",
            "fig:scale-invariant": "\\begin{figure}[ht]\n    \\centering\n    \\subfigure[MAE]\n    {\n        \\centering\n        \\includegraphics[width=0.3\\linewidth]{figures/mae_wiki_new.png}\n        \\label{scale_b}\n    }\n    \\subfigure[RMSE]\n    {\n        \\centering\n        \\includegraphics[width=0.3\\linewidth]{figures/rmse_wiki_new.png}\n        \\label{scale_c}\n    }\n    \\subfigure[MAPE]\n    {\n        \\centering\n        \\includegraphics[width=0.3\\linewidth]{figures/mape_wiki_new.png}\n        \\label{scale_d}\n    }\n    \\caption{Scalability analyses in terms of MAE, RMSE, and MAPE under different number of nodes on the Wiki dataset.}\n    \\label{fig:scale-invariant}\n\\end{figure}",
            "fig:filters": "\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/filters-1.png}\n    \\caption{The diffusion process of FourierGNN at two timestamps (top and bottom) on COVID-19.}\n    \\label{fig:filters}\n\\end{figure}",
            "fig:space_adj": "\\begin{figure}[ht]\n    \\centering    \\includegraphics[width=1\\linewidth]{figures/adj_at_time-1.png}\n    \\caption{The adjacency matrix for four consecutive days on the COVID-19 dataset.}\n    \\label{fig:space_adj}\n\\end{figure}"
        },
        "equations": {
            "eq:eq:problem_define": "\\begin{equation}\\label{eq:problem_define}\n   \\hat{Y}_t:= {F_{\\theta}}(X_t) = {F_{\\theta}}([\\bm{x}_{t-T+1},...,\\bm{x}_t])\n\\end{equation}",
            "eq:eq:problem_define_gt": "\\begin{equation}\\label{eq:problem_define_gt}\n    \\hat{Y}_t:= {F_{\\theta_g,\\theta_t}}(X_t) = F_{\\theta_g,\\theta_t}([\\bm{x}_{t-T+1},...,\\bm{x}_t])\n\\end{equation}",
            "eq:1": "\\begin{equation}\n    \\hat{Y}_t :=F_{\\theta_\\mathcal{G}}({X^\\mathcal{G}_t},{A_t^\\mathcal{G}})\n\\end{equation}",
            "eq:equ:fgso": "\\begin{equation}\\label{equ:fgso}\n    \\mathcal{F}({X})\\mathcal{F}(\\kappa)=\\mathcal{F}((X*\\kappa)[i])=\\mathcal{F}(\\sum_{j=1}^n {{X}}[j]\\kappa[i-j])=\\mathcal{F}(\\sum_{j=1}^n {{X}}[j]\\kappa[i,j]), \\quad\\quad   \\forall i \\in [n]\n\\end{equation}",
            "eq:equ:gcn_unit": "\\begin{equation}\\label{equ:gcn_unit}\n    \\mathcal{F}(X)\\mathcal{S}_{A,W} = \\mathcal{F}(AXW).\n\\end{equation}",
            "eq:2": "\\begin{equation} \\label{equ:fourier_gnn}\n\\operatorname{FourierGNN}(X, A) :=\n%\\mathcal{F}(X)\\mathcal{S}_0+\n    \\sum_{k=0}^{K}\\sigma(\\mathcal{F}(X)\\mathcal{S}_{0:k}+b_k),  \\quad \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i.\n    %\\sum_{k=0}^K \\mathcal{X}_{k} \\quad s.t.\\ \\mathcal{X}_k=\\sigma(\\mathcal{X}_{k-1}\\mathcal{S}_k+b_k)\n\\end{equation}",
            "eq:3": "\\begin{equation} \n    \\mathcal{Y}^{\\mathcal{G}}_t = \\operatorname{FourierGNN}(\\textbf{X}_t^{\\mathcal{G}},A_t^{\\mathcal{G}}) =\n    \\sum_{k=0}^{K}\\sigma(\\mathcal{F}(\\mathbf{X}_t^{\\mathcal{G}})\\mathcal{S}_{0:k}+b_k), \\quad \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i.\n\\end{equation}",
            "eq:convolution_theorem": "\\begin{equation}\\label{convolution_theorem}\n    \\mathcal{F}((x*h)[n])=\\mathcal{F}(x)\\mathcal{F}(h)\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\begin{aligned}\n    \\mathcal{F}^{-1}(\\sum_{k=0}^{K}\\mathcal{F}(X)\\mathcal{S}_{0:K})\n    %&=\\mathcal{F}(A_0XW_0+A_1(A_0XW_0)W_1+...+A_KA_{K-1}...(A_0XW_0)...W_{K-1}W_K)\\\\\n    =A_0XW_0+A_1(A_0XW_0)W_1+...+A_{K:0}{X}W_{0:K}\n    % &+A_KA_{K-1}...(A_0XW_0)...W_{K-1}W_K\n    %\\\\&=\\sum_{k=0}^{K}\\mathcal{F}(X)\\mathcal{S}_{0:k} \\quad s.t.\\  \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i\n\\end{aligned}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\mathcal{F}(AXW)=\\mathcal{F}(X)\\times_n \\mathcal{A}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\begin{aligned}\n        \\mathcal{F}(A_KA_{K-1}\\cdots A_0X)&=\\mathcal{F}(A_K(A_{K-1}...A_0X))\\\\\n        &=\\mathcal{F}(A_{K-1}...A_0X)\\times_n \\mathcal{A}_K\\\\\n        &=\\mathcal{F}(\\mathcal{X})\\times_n  \\mathcal{A}_0\\cdots\\mathcal{A}_{K-1}\\times_n \\mathcal{A}_{K}\n    \\end{aligned}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\begin{aligned}\n    \\mathcal{F}(H^{EV}_K X)&= \\mathcal{F}(A_0XW_0+A_1(A_0XW_0)W_1+...+A_KA_{K-1}...(A_0XW_0)...W_{K-1}W_K)\\\\\n    &=\\mathcal{F}(A_0XW_0)+\\mathcal{F}(A_1(A_0XW_0)W_1)+...+\\mathcal{F}(A_KA_{K-1}...(A_0XW_0)...W_{K-1}W_K)\\\\\n    &=\\mathcal{F}(X)\\times_n\\mathcal{A}_0+\\mathcal{F}(X)\\times_n\\mathcal{A}_0\\times_n\\mathcal{A}_1+...+\\mathcal{F}(X)\\mathcal{A}_0\\times_n\\cdots\\mathcal{A}_{K-1}\\times_n\\mathcal{A}_{K}\n    %\\\\&=\\sum_{k=0}^{K}\\mathcal{F}(X)\\mathcal{S}_{0:k} \\quad s.t.\\  \\mathcal{S}_{0:k}=\\prod_{i=0}^k\\mathcal{S}_i\n\\end{aligned}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n        H^{EV}_KX=\\mathcal{F}^{-1}\\left(\\sum_{k=0}^{K}\\mathcal{F}(X)\\times_n\\mathcal{A}_{0:k}\\right)\\quad s.t.\\  \\mathcal{A}_{0:k}=\\mathcal{A}_0\\times_n\\cdots\\mathcal{A}_{K-1}\\times_n\\mathcal{A}_{K}\n    \\end{equation}",
            "eq:9": "\\begin{equation}\n    MAE=\\frac{1}{\\tau N}\\sum_{i=1}^{N}\\sum_{j=1}^{\\tau}\\left | x_{ij}-\\hat{x}_{ij} \\right |\n\\end{equation}",
            "eq:10": "\\begin{equation}\n   RMSE=\\sqrt{\\frac{1}{\\tau N}\\sum_{i=1}^{N}\\sum_{j=1}^{\\tau}\\left (x_{ij}-\\hat{x}_{ij}\\right )^2}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    MAPE=\\frac{1}{\\tau N}\\sum_{i=1}^{N}\\sum_{j=1}^{\\tau}\\left | \\frac{x_{ij}-\\hat{x}_{ij}}{x_{ij}} \\right | \\times 100\\%\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    \\begin{aligned}\n        \\mathbf{X}_1&=\\operatorname{LeakyReLU}(\\mathbf{Y}_t^{\\mathcal{G}}\\mathbf{W}_1+\\mathbf{b}_1)\\\\\n        \\mathbf{X}_2&=\\operatorname{LeakyReLU}(\\mathbf{X}_1\\mathbf{W}_2+\\mathbf{b}_2)\\\\\n        \\hat{Y}&=\\mathbf{X}_2\\mathbf{W}_3+\\mathbf{b}_3\n    \\end{aligned}\n\\end{equation}"
        },
        "git_link": "https://github.com/aikunyi/FourierGNN"
    }
}