{
    "meta_info": {
        "title": "TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series  Forecasting",
        "abstract": "Transformers have gained popularity in time series forecasting for their\nability to capture long-sequence interactions. However, their high memory and\ncomputing requirements pose a critical bottleneck for long-term forecasting. To\naddress this, we propose TSMixer, a lightweight neural architecture exclusively\ncomposed of multi-layer perceptron (MLP) modules for multivariate forecasting\nand representation learning on patched time series. Inspired by MLP-Mixer's\nsuccess in computer vision, we adapt it for time series, addressing challenges\nand introducing validated components for enhanced accuracy. This includes a\nnovel design paradigm of attaching online reconciliation heads to the MLP-Mixer\nbackbone, for explicitly modeling the time-series properties such as hierarchy\nand channel-correlations. We also propose a novel Hybrid channel modeling and\ninfusion of a simple gating approach to effectively handle noisy channel\ninteractions and generalization across diverse datasets. By incorporating these\nlightweight components, we significantly enhance the learning capability of\nsimple MLP structures, outperforming complex Transformer models with minimal\ncomputing usage. Moreover, TSMixer's modular design enables compatibility with\nboth supervised and masked self-supervised learning methods, making it a\npromising building block for time-series Foundation Models. TSMixer outperforms\nstate-of-the-art MLP and Transformer models in forecasting by a considerable\nmargin of 8-60%. It also outperforms the latest strong benchmarks of\nPatch-Transformer models (by 1-2%) with a significant reduction in memory and\nruntime (2-3X). The source code of our model is officially released as\nPatchTSMixer in the HuggingFace. Model:\nhttps://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer\nExamples: https://github.com/ibm/tsfm/#notebooks-links",
        "author": "Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam",
        "link": "http://arxiv.org/abs/2306.09364v4",
        "category": [
            "cs.LG",
            "cs.AI",
            "I.2"
        ],
        "additionl_info": "Accepted in the Proceedings of the 29th ACM SIGKDD Conference on  Knowledge Discovery and Data Mining (KDD 23), Research Track. Delayed release  in arXiv to comply with the conference policies on the double-blind review  process. This paper has been submitted to the KDD peer-review process on Feb  02, 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n% \\textcolor{red}{QUESTION: Are we posing this as a foundation model paper for multiple applications, or, forecasting only? Based on that we should organize the story.} \\\\\n\n% \\nam{To pose it as the foundation model paper we will need to benchmark on several downstream tasks. Since the experiments are all for forecasting + rep. learning so far and several proposed components are specific for forecasting, perhaps we should pose it as a MLP Mixer architecture for time series forecasting and rep. learning}\n\n% \\textcolor{red}{QUESTION: Should we use the keyword ``foundation model'' and make the flow in the form of an FM paper?} \\\\\n\n% \\textbf{Applications of multivariate time series forecasting}\\\\\nMultivariate time series forecasting is the task of predicting the future values of multiple (possibly) related time series at future time points given the historical values of those time series.\nIt has widespread applications in weather forecasting, traffic prediction, industrial process controls, \\textit{etc.}\nThis decade-long problem has been well studied in the past by several statistical and ML methods~\\cite{al2018short},~\\cite{hyndman_book}.\n% This decade-long problem has been well studied in the past by traditional statistical methods such as SARIMA, vector autoregressive model~(VARs)~\\cite{hyndman_book} or machine learning techniques like SVR~\\cite{al2018short} that are designed to capture the temporal  information of the data.\n% \\arindam{Sudden break, need a filler sentence like traditional methods for multivariate forecasting.}\n% \\textbf{Recent boom of transformer-based models (in forecasting)}\\\\\nRecently, Transformer~\\cite{transformer}-based models are becoming popular for long-term multivariate forecasting due to their powerful capability to capture long-sequence dependencies.\n% because of their superior performance over the traditional models~\\cite{survey1, survey2}.\nSeveral Transformer architectures were suitably designed for this task in last few years including Informer~\\cite{informer}, Autoformer~\\cite{autoformer}, FEDformer~\\cite{autoformer}, and Pyraformer~\\cite{pyraformer}.\nHowever, the success of the Transformer in the semantically rich NLP domain has not been well-transferred to the time series domain.\n%One of the possible reasons is that the positional embeddings employed in the Transformer might not fully preserve the temporal ordering of the time series~\\cite{dlinear}. \n% because of the application of permutation-invariant attention layers on top of them.\n%It might be sufficient in NLP domain because a slight reordering of some words in a sentence largely preserves its semantic meaning. However, this can cause more information loss in the case of time series. This hypothesis has been empirically validated in~\\cite{dlinear}, where an embarrassingly simple linear~(DLinear) model is able to outperform most of the above-mentioned transformer-based forecasting models. \nOne of the possible reasons is that, though positional embedding in Transformers preserves some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. This hypothesis has been empirically validated in~\\cite{dlinear}, where an embarrassingly simple linear~(DLinear) model is able to outperform most of the above-mentioned Transformer-based forecasting models. \n% One of the possible reasons is that the positional embeddings employed in the Transformer might not fully preserve the temporal ordering of the time series~\\cite{dlinear}. This embedding can be sufficient in NLP since a slight reordering of some words in a sequence can drastically change its semantic meaning. In contrast, the temporal information can be still conserved even with sufficient shuffling. This hypothesis has been empirically validated in~\\cite{dlinear}, where the performance of the Transformer does not suffer with randomly shuffled input sequence while a simple linear (DLinear) model observes a significant prediction loss.\n\nFurthermore, unlike words in a sentence, individual time points in a time series lack significant semantic information and can be easily inferred from neighboring points. Consequently, a considerable amount of modeling capacity is wasted on learning point-wise details. PatchTST~\\cite{patchtst} has addressed this issue by dividing the input time series into patches and applying a transformer model, resulting in superior performance compared to existing models.\n% Moreover, unlike words in a sentence, an individual time point in a time series does not contain much semantic information and often can be easily interpolated from its neighbors.\n% Hence, a significant amount of modeling capability is wasted in learning point-wise information. % in the time series when a Transformer is trained with point-wise input tokens.\n% Recently, PatchTST~\\cite{patchtst}, has overcome this bottleneck by segregating the input time series into a finite number of patches and then feeding into the transformer model.\n% It has been able to achieve superior performance than state-of-the-art models on several benchmarks. \nHowever, PatchTST employs a pure channel\\footnote{Throughout the text, we will use ``channel'' to denote the individual time series in a multivariate data (i.e. a multivariate time series is a multi-channel signal).} independence approach which does not explicitly capture the cross-channel correlations. \n%An ablation study in PatchTST shows a significant drop in accuracy when channel independence is not employed in datasets with a very high number of channels that can introduce a lot of noisy interactions \n% It has been shown in the paper that channel independence can improve performance compared to channel mixing via simple concatenation of time points or patches before embedding. \nPatchTST has demonstrated that channel independence can enhance performance compared to channel mixing, where channels are simply concatenated before being fed into the model. This simple mixing approach can lead to noisy interactions between channels in the initial layer of the Transformer, making it challenging to disentangle them at the output. CrossFormer~\\cite{crossformer}, another recent effort on Patch Transformers with an improved channel mixing technique, also faces this issue (see Appendix Table 9). Therefore, there is a necessity to explicitly model channel interactions to seize opportunistic accuracy improvements while effectively reducing the high volume of noisy interactions across channels. In addition, though PatchTST reduces the timing and memory overhead via patching, it still uses the multi-head self-attention under the hood, which is computationally expensive even when it is applied at the patch level. \n\n\n% It has been shown in~\\cite{patchtst} that channel independence can improve performance compared to channel mixing (via simple concatenation of channels before feeding into the model). \n% This simple mixing approach can produce noisy interaction between channels at the very first layer of the Transformer which will be difficult to decouple at output. CrossFormer~\\cite{crossformer}, another recent effort on patch Transformers with an improved channel mixing approach, also suffers from the same problem. (Appendix Table~\\ref{tab:crossformer}).\n% Thus, there is a need to explicitly model the channel interactions for opportunistic accuracy improvements, while smartly eliminating the high volume of noisy interactions across channels. In addition, though PatchTST reduces the timing and memory overhead via patching, it still uses the multi-head self-attention under the hood, which is computationally expensive even when it is applied at the patch level. \n\n\n% However, there are a few downsides of using the patch-based transformer-based models as well.\n% First, self-attention networks in transformers have quadratic timing requirement which makes it practically difficult to apply in long term forecasting (with longer history and forecast horizon).\n% Informer tried to address this issue with ProbSparse attention mechanism~\\cite{informer}, but it results in sub-optimal performance as shown in~\\cite{dlinear, patchtst}.\n% Second, transformer-based models also have higher memory requirement which makes it harder to train on large datasets, and deploy in resource-constrained scenarios.\n\n% \\textbf{How to overcome, adoption of MLP Mixer is not straightforward}\nRecently, a series of multi-layer perceptron~(MLP) models under the umbrella of ``MLP-Mixers\" have been proposed in the computer vision domain~\\cite{mlpmixer, payattention, resmlp}.\nThese models are lightweight and fast, and achieve comparable or superior performance to vision Transformer models while completely eliminating the need for computing intense multi-head self-attention~\\cite{mlpmixer}. Moreover, MLP-Mixer by its default architecture does not disturb the temporal ordering of the inputs \n% (i.e. no need to have positional encoding) \nwhich makes it a natural fit for time-series problems and resolves the concerns raised in DLinear~\\cite{dlinear}.\n% At this point, we ask the following questions: (i) Can we adapt Vision MLP-Mixers for long sequence multivariate time series forecasting and representation learning, and achieve superior performance compared to the state-of-the-art Transformer/non-Transformer based architectures? (ii) What are the customizations required in MLP mixers which can exploit various inherent properties of time series (such as hierarchy, historical lags, etc.) and achieve superior performance? (iii) Unlike PatchTST, can we explicitly model the channel interactions while efficiently avoiding the high volume of noisy interactions across channels?\nAt this point, we ask the following important question - \\textit{Can MLP-Mixer models yield superior performance for multivariate time series forecasting?  If so, what are the required time series customizations?}\n% \\textbf{Main contributions of the paper}\\\\\n\nTowards this, we show that the adoption of \\mixer for time series is \\textit{not} trivial, \\ie just applying the vanilla \\mixer with some input and output shape modification will not make it a powerful model, and will have suboptimal performance compared to the state-of-the-arts. Hence, we propose \\textbf{~\\tsm}, a novel MLP-Mixer architecture for accurate multivariate time series forecasting. Like PatchTST, \\tsm~ is also patching-based and follows a modular architecture of learning a common \\textit{``backbone''} to capture the temporal dynamics of the data as a patch representation, and different \\textit{``heads''} are attached and finetuned based on various downstream tasks (Ex. forecasting).  Backbone is considered task-independent and can learn across multiple datasets with a masked reconstruction loss while the heads are task and data-specific. \\\\\n% \\tsm~propose a new design paradigm of attaching online reconciliation heads to the mixer backbone that significantly empowers the learning capability of simple MLP structures to outperform complex transformer models with very less compute usage. Specifically, it contains the following novel architectural enhancements: \\textbf{(i) Channel Independent backbone augmented with cross-channel reconciliation heads:} ~\\tsm~ adopts the channel-independent backbone (from PatchTST), but attaches a \\textit{Forecast Cross-Channel Reconciliation head} on top of it to explicitly capture the channel interactions while keeping the backbone generalizable to multiple datasets with varying channels. This approach is highly effective in eliminating the high volume of noisy interactions across channels, which existing patch channel-mixing techniques suffer from. \\textbf{(ii) Hierarchical Patch Reconciliation head:} ~\\tsm~exploits the patch-time aggregation as implicit hierarchy signals and enables an online hierarchical patch reconciliation in the prediction head to boost the performance. This approach  improves the granular forecasts when patch aggregated forecasts show better predictability. \\textbf{(iii) Gated Attention~(GA):} ~\\tsm~ introduce a simple linear gated attention in the backbone which, when augmented with standard MLP-mixing operations, enables fast and effective capture of long sequence interactions, without requiring the need for complex multi-head self-attention blocks. \n\n\\textbf{Key highlights of ~\\tsm~}are as follows: \n\\begin{enumerate}\n    \\item TSMixer is a patching-based, lightweight neural architecture designed solely with MLP modules that exploits various inherent time-series characteristics for accurate \\textit{multivariate forecasting and representation learning}.\n    \\item TSMixer proposes a novel design paradigm of attaching \\& tuning online reconciliation\\footnote{\\label{reconcile_tag}This ``reconciliation'' is different from the standard reconciliation in hierarchical forecasting~\\cite{hyndman_book}. Here, reconciliation targets patch-aggregation and  cross-channel correlation and it is done online during training. We call it ``online\" as it is learned as part of the overall loss computation and not as a separate offline process.} heads to the MLP-Mixer backbone that significantly empowers the learning capability of simple MLP structures to outperform complex Transformer models while using less computing resources. This study is the first of its kind to highlight the benefits of infusing online reconciliation approaches in the prediction head of Mixer style backbone architectures for time-series modeling. \n    % \\item Specifically, ~\\tsm~ proposes two novel online reconciliation heads to tune \\& improve the forecasts by leveraging the \\textit{hierarchical patch-aggregation} and \\textit{cross-channel correlation} signals in the time series.\n    \\item Specifically, ~\\tsm~ proposes two novel online reconciliation heads to tune \\& improve the forecasts by leveraging the following intrinsic time series properties:  \\textit{hierarchical patch-aggregation} and \\textit{cross-channel correlation}.\n    \\item  For effective \\textit{cross-channel modeling}, ~\\tsm~ follows a novel ``Hybrid\" channel modeling by \\textit{augmenting a channel independent backbone with a cross-channel reconciliation head}. This hybrid architecture allows the backbone to generalize across diverse datasets with different channels, while the reconciliation head effectively learns the channel interactions specific to the task and data. This approach effectively handles noisy interactions across channels, which is a challenge in existing patch channel-mixing methods.   \n    % This approach also reduces the high volume of noisy interactions across channels, which existing patch channel-mixing techniques suffer from.\n    % designing a channel-independent backbone (like PatchTST) but attaching a \\textit{Forecast Cross-Channel Reconciliation head} on top of it to explicitly capture the channel interactions while keeping the backbone generalizable to multiple datasets with varying channels. This hybrid approach effectively eliminates the high volume of noisy interactions across channels, which existing patch channel-mixing techniques suffer from. \n    \\item For effective \\textit{long sequence modeling}, ~\\tsm~ introduces a simple Gated Attention\\footnote{\\label{ga_note}Gated Attention proposed in this paper is different from the Spatial Gating Unit proposed in gMLP~\\cite{payattention}} that guides the model to focus on the important features. This, when augmented with Hierarchical Patch Reconciliation Head and standard MLP-Mixer operations enables effective modeling of long-sequence interaction and eliminates the need for complex multi-head self-attention blocks.\n    \\item Finally, the modular design of ~\\tsm~ enables it to work with both supervised and masked self-supervised learning methodologies which makes it a potential building block for time-series foundation models~\\cite{foundation}.\n\\end{enumerate}\nWe conduct a detailed empirical analysis on 7 popular public datasets, wherein. ~\\tsm~ outperforms all existing benchmarks with \\textit{extremely reduced training time and memory usage}. Snapshot (relative MSE improvements) of the primary benchmarks are as follows:\n\\begin{itemize}\n    \\item \\tsm~ outperforms DLinear by a considerable margin of 8\\% (Table~\\ref{tab:supervised}).\n    \\item \\tsm~ marginally outperforms PatchTST by 1-2\\% but with a significant 2-3X reduction in training time and memory usage. (Table~\\ref{tab:supervised},\\ref{tab:speedup},\\ref{tab:fm_1},\\ref{tab:fm-2-all}).\n    % \\item ~\\tsm~ outperforms all the other standard benchmarks(i.e. CrossFormer, FEDformer, Autoformer, Informer, S4, LightTS, BTSF, TNC, TS-TCC, CPC, and TS2Vec) by a significant margin of 20-60\\% \n    \\item \\tsm~ outperforms all other state-of-the-arts by a significant margin of 20-60\\%. \n\\end{itemize}\n\n\n% For effective cross-channel modeling, ~\\tsm~ adopts the channel-independent backbone (from PatchTST), but attaches a \\textit{Forecast Cross-Channel Reconciliation head} on top of it to explicitly capture the channel interactions while keeping the backbone generalizable to multiple datasets with varying channels. This approach is highly effective in eliminating the high volume of noisy interactions across channels, which existing patch channel-mixing techniques suffer from.\n\n% For effective \n\n% the \\textbf{main contributions of the paper} are as follows: \n% \\begin{enumerate}\n    \n%     \\item We show that the adoption of \\mixer for time series is \\textit{not} trivial, \\ie just applying the vanilla \\mixer with some input and output shape modification will not make it a powerful model, and will have suboptimal performance compared to the state-of-the-arts.\n\n%     \\item Hence, we propose \\textbf{~\\tsm}, a novel MLP-Mixer based lightweight architecture that exploits various inherent time-series characteristics for accurate \\textbf{multivariate forecasting and representation learning}.\n\n%     \\item \\tsm~ follows a modular architecture of having a common \\textit{backbone} to capture the temporal dynamics of the data as a learnable representation, and different \\textit{heads} are attached and finetuned on top of it based on various downstream tasks (ex. forecasting). This enables ~\\tsm~ to work with both supervised and self-supervised settings which makes it a potential building block for time-series foundation models~\\cite{foundation}.\n\n%     \\item ~\\tsm~propose a new design paradigm of attaching online reconciliation heads to the mixer backbone that significantly empowers the learning capability of simple MLP structures to outperform complex transformer models with very less compute usage. Specifically, we propose two reconciliation heads that exploit the inherent properties of time-series to model the hierarchical patch-aggregation and inter-channel cross-correlation.\n\n%     \\item For effective cross-channel modeling, ~\\tsm~ adopts the channel-independent backbone (from PatchTST), but attaches a \\textit{Forecast Cross-Channel Reconciliation head} on top of it to explicitly capture the channel interactions while keeping the backbone generalizable to multiple datasets with varying channels. This approach is highly effective in eliminating the high volume of noisy interactions across channels, which existing patch channel-mixing techniques suffer from.\n\n%     \\item For effective long-sequence modeling, \n    \n\n    \n\n%     % \\item \\textbf{Highlights of ~\\tsm}: \\\\\n%     % (i) \\tsm~ follows a modular architecture of having a common \\textit{backbone} to capture the temporal dynamics of the data as a learnable representation, and different \\textit{heads} are attached and finetuned on top of it based on various downstream tasks (ex. forecasting). This enables ~\\tsm~ to work with both supervised and self-supervised settings which makes it a potential building block for time-series foundation models~\\cite{foundation}. \\\\\n%     % (ii) \\tsm~ introduces a novel design paradigm of attaching \\textit{reconciliation}\\footnote{\\label{reconcile_tag}This ``reconciliation'' is different from the standard reconciliation in hierarchical forecasting~\\cite{hyndman_book}. Here, reconciliation targets patch-aggregation and  cross-channel correlation and it is done online during training.}\n%     % \\textit{heads to the channel-independent mixer backbone }that significantly empowers the learning capability of simple MLP structures to outperform complex transformer models with very less compute usage. \\\\\n%     % (iii) For effective cross-channel modeling, ~\\tsm~ adopts the channel-independent backbone (from PatchTST), but attaches a \\textit{Forecast Cross-Channel Reconciliation head} on top of it to explicitly capture the channel interactions while keeping the backbone generalizable to multiple datasets with varying channels. This approach is highly effective in eliminating the high volume of noisy interactions across channels, which existing patch channel-mixing techniques suffer from. \\\\\n    \n%     % (iv) For effective temporal modeling, \n    \n%     % \\item \\tsm~ decouples its functionality into 2 components: (i) \\textbf{Backbone} - which is pre-trained to capture the generic temporal dynamics of the data as a learnable representations, (ii) \\textbf{Heads} - which are added on top of the pretrained back\n%     % \\item Hence, we propose \\textbf{~\\tsm}, a novel MLP-Mixer based architecture for time-series that exploits various inherent time-series characteristics to boost the overall accuracy. \n\n%     \\item \\tsm~ is architected with a common \\textit{backbone} that is pre-trained with masked reconstruction loss to capture the generic temporal dynamics of the data as a learnable representation, and different \\textit{heads} are attached and finetuned on top of it based on various downstream tasks (ex. forecasting). This enables ~\\tsm~ to work with both supervised and masked self-supervised settings which makes it a potential building block for time-series foundation models~\\cite{foundation}.\n    \n%     % \\item \\tsm~ is architected with a common backbone that is pre-trained with multivariate time series data to capture the generic temporal dynamics of the data as a learnable representation, and different heads are finetuned on top of it based on various downstream tasks (ex. forecasting). This enables ~\\tsm~ to work with both supervised and masked self-supervised settings which makes it a potential building block for time-series foundation models~\\cite{foundation}.\n\n%     % \\item \\tsm~ is architected with a common backbone that captures the temporal dynamics of one or multiple time series data and different heads are added to it based on various downstream tasks (ex. forecasting). This enables ~\\tsm~ to work with both supervised and semi-supervised settings. Thus, the proposed \\tsm~can be trained with a forecasting objective directly, or pre-trained with a masked self-supervised loss to learn distributed representations which makes it appropriate a foundational model~\\cite{foundation}\n    \n%     \\item The proposed \\tsm~ introduces a novel design paradigm of attaching reconciliation\\footnote{\\label{reconcile_tag}This ``reconciliation'' is different from the standard reconciliation in hierarchical forecasting~\\cite{hyndman_book}. Here, reconciliation targets patch-aggregation and  cross-channel correlation and it is done online during training.}\n%     heads to the channel-independent mixer backbone that significantly empowers the learning capability of simple MLP structures to outperform complex transformer models. Specifically, it contains the following novel architectural enhancements: \\\\\n%     \\textbf{(i) Channel Independent backbone augmented with cross-channel reconciliation heads:} ~\\tsm~ adopts the channel independent backbone (from PatchTST), but adds a forecast cross-channel reconciliation module in the prediction head that explicitly captures the cross-channel interactions while keeping the backbone generalizable to multiple datasets (with a varying number of channels). This approach is highly effective in eliminating the high volume of noisy interactions across channels, which existing patch channel-mixing techniques suffer from.\\\\\n%     \\textbf{(ii) Hierarchical Patch Reconciliation head:} ~\\tsm~exploits the patch-time aggregation as implicit hierarchy signals and enables an online hierarchical patch reconciliation in the prediction head to boost the performance. This approach greatly improves the granular forecasts when patch aggregated forecasts show better predictability. \\\\\n%     \\textbf{(iii) Gated Attention~(GA):} ~\\tsm~ apply a simple lightweight gated attention\\footnote{Gated Attention proposed in this paper is different from the Spatial Gating Unit proposed in gMLP~\\cite{payattention}} (in the backbone) to relatively weigh the input features based on their local importance. This approach effectively filters the noise in the data, which when augmented with \\tsm~ leads to improved short and long-term interaction modeling without requiring the need for complex multi-head self-attention blocks.    \n    \n%     % \\begin{itemize}\n%     %     \\item \\textbf{Channel Independent backbone augmented with cross-channel reconciliation heads:} ~\\tsm~ adopts the channel independent backbone as inspired by the PatchTST, but adds a forecast cross-channel reconciliation module in the prediction head that explicitly captures the cross-channel interactions while keeping the backbone generalizable to multiple datasets (with a varying number of channels).\n%     %     \\item \\textbf{Hierarchical Patch Reconciliation head:} ~\\tsm~exploits the patch-time aggregation as implicit hierarchy signals and enables an online hierarchical patch reconciliation in the prediction head to boost the performance.\n%     %     \\item \\textbf{Gated Attention~(GA):} ~\\tsm~ apply a simple lightweight gated attention to relatively weigh the input features based on their local importance. This approach effectively filters the noise in the data, which when augmented with \\tsm~ leads to improved short and long-term interaction modeling without requiring the need for complex multi-head self-attention blocks.    \n%     % \\end{itemize}\n    \n%     % \\item While several standard reconciliation approaches exist in the literature, \n%     \\item This study is the first of its kind to highlight the benefits of infusing online reconciliation\\footref{reconcile_tag} approaches in the prediction head of Transformer/Mixer style backbone architectures for time-series modeling. We call it ``online\" as these are learned as part of the overall loss computation and not as a separate offline process.\n%     % as part of the standard pretraining/finetuning loss computation as compared to offline standard reconciliation approaches.\n%     \\item We conduct a detailed empirical analysis on 7 popular public datasets, wherein. ~\\tsm~ outperforms all existing benchmarks with extremely reduced training time and memory usage. Snapshot (relative MSE improvements) of the primary benchmarks are as follows:\n%         \\begin{itemize}\n%             \\item ~\\tsm~ outperforms DLinear by a good margin of 8\\% (Table~\\ref{tab:supervised}).\n%             \\item ~\\tsm~ outperforms PatchTST by a margin of 1-2\\% but with a significant 2-3X reduction in training time and memory usage. (Table~\\ref{tab:supervised},\\ref{tab:speedup},\\ref{tab:fm_1},\\ref{tab:fm-2-all}).\n%             % \\item ~\\tsm~ outperforms all the other standard benchmarks(i.e. CrossFormer, FEDformer, Autoformer, Informer, S4, LightTS, BTSF, TNC, TS-TCC, CPC, and TS2Vec) by a significant margin of 20-60\\% \n%             \\item ~\\tsm~ outperforms all other state-of-the-arts (i.e. CrossFormer, FEDformer, Autoformer, Informer, S4, LightTS, BTSF, TNC, TS-TCC, CPC, and TS2Vec) by a significant margin of 20-60\\%. \n%         \\end{itemize}\n        \n%     % \\item We conduct a detailed empirical analysis on 7 popular public datasets, wherein. ~\\tsm~ outperforms all existing benchmarks with the following MSE relative improvements:\n%     %     \\begin{itemize}\n%     %         \\item {8-60\\%} \\wrt {MLP and non-transformer based forecasting benchmarks} (i.e. DLinear, S4 and LightTS)\n%     %         \\item {50-70\\% } \\wrt {self-supervised forecasting benchmarks} (i.e. BTSF, TNC, TS-TCC~, CPC, and TS2Vec)\n%     %         \\item {20-60\\% } \\wrt {standard transformer-based forecasting benchmarks} (i.e. FEDformer, Autoformer, and Informer).\n%     %         \\item {1-2\\%} marginal improvement \\wrt {Patch Transformer based forecasting benchmark} (i.e. PatchTST) but with a significant 2-3X reduction in training time and memory usage.\n%     %     \\end{itemize}\n    \n%     % \\item Finally, this study compares and contrasts different channel-mixing techniques for improved time-series MLP Mixer adoption.\n%     \\item Finally, we compare and contrast different channel-mixing techniques for improved time-series MLP Mixer adoption.\n% \\end{enumerate}\n\n% Towards this, the main contributions of the paper are as follows:\n% \\begin{enumerate}\n%     \\item We show that the adoption of \\mixer for time series is \\textit{not} trivial, \\ie just applying the vanilla \\mixer with some input and output shape modification will not make it a powerful model, and will have suboptimal performance compared to the state-of-the-arts.\n\n%     \\item Hence, we propose \\textbf{~\\tsm}, a novel MLP-Mixer based lightweight architecture that exploits various inherent time-series characteristics for accurate multivariate forecasting and representation learning.\n\n%     % \\item \\tsm~ decouples its functionality into 2 components: (i) \\textbf{Backbone} - which is pre-trained to capture the generic temporal dynamics of the data as a learnable representations, (ii) \\textbf{Heads} - which are added on top of the pretrained back\n%     % \\item Hence, we propose \\textbf{~\\tsm}, a novel MLP-Mixer based architecture for time-series that exploits various inherent time-series characteristics to boost the overall accuracy. \n\n%     \\item \\tsm~ is architected with a common \\textit{backbone} that is pre-trained with masked reconstruction loss to capture the generic temporal dynamics of the data as a learnable representation, and different \\textit{heads} are attached and finetuned on top of it based on various downstream tasks (ex. forecasting). This enables ~\\tsm~ to work with both supervised and masked self-supervised settings which makes it a potential building block for time-series foundation models~\\cite{foundation}.\n    \n%     % \\item \\tsm~ is architected with a common backbone that is pre-trained with multivariate time series data to capture the generic temporal dynamics of the data as a learnable representation, and different heads are finetuned on top of it based on various downstream tasks (ex. forecasting). This enables ~\\tsm~ to work with both supervised and masked self-supervised settings which makes it a potential building block for time-series foundation models~\\cite{foundation}.\n\n%     % \\item \\tsm~ is architected with a common backbone that captures the temporal dynamics of one or multiple time series data and different heads are added to it based on various downstream tasks (ex. forecasting). This enables ~\\tsm~ to work with both supervised and semi-supervised settings. Thus, the proposed \\tsm~can be trained with a forecasting objective directly, or pre-trained with a masked self-supervised loss to learn distributed representations which makes it appropriate a foundational model~\\cite{foundation}\n    \n%     \\item The proposed \\tsm~ introduces a novel design paradigm of attaching reconciliation\\footnote{\\label{reconcile_tag}This ``reconciliation'' is different from the standard reconciliation in hierarchical forecasting~\\cite{hyndman_book}. Here, reconciliation targets patch-aggregation and  cross-channel correlation and it is done online during training.}\n%     heads to the channel-independent mixer backbone that significantly empowers the learning capability of simple MLP structures to outperform complex transformer models. Specifically, it contains the following novel architectural enhancements: \\\\\n%     \\textbf{(i) Channel Independent backbone augmented with cross-channel reconciliation heads:} ~\\tsm~ adopts the channel independent backbone (from PatchTST), but adds a forecast cross-channel reconciliation module in the prediction head that explicitly captures the cross-channel interactions while keeping the backbone generalizable to multiple datasets (with a varying number of channels). This approach is highly effective in eliminating the high volume of noisy interactions across channels, which existing patch channel-mixing techniques suffer from.\\\\\n%     \\textbf{(ii) Hierarchical Patch Reconciliation head:} ~\\tsm~exploits the patch-time aggregation as implicit hierarchy signals and enables an online hierarchical patch reconciliation in the prediction head to boost the performance. This approach greatly improves the granular forecasts when patch aggregated forecasts show better predictability. \\\\\n%     \\textbf{(iii) Gated Attention~(GA):} ~\\tsm~ apply a simple lightweight gated attention\\footnote{Gated Attention proposed in this paper is different from the Spatial Gating Unit proposed in gMLP~\\cite{payattention}} (in the backbone) to relatively weigh the input features based on their local importance. This approach effectively filters the noise in the data, which when augmented with \\tsm~ leads to improved short and long-term interaction modeling without requiring the need for complex multi-head self-attention blocks.    \n    \n%     % \\begin{itemize}\n%     %     \\item \\textbf{Channel Independent backbone augmented with cross-channel reconciliation heads:} ~\\tsm~ adopts the channel independent backbone as inspired by the PatchTST, but adds a forecast cross-channel reconciliation module in the prediction head that explicitly captures the cross-channel interactions while keeping the backbone generalizable to multiple datasets (with a varying number of channels).\n%     %     \\item \\textbf{Hierarchical Patch Reconciliation head:} ~\\tsm~exploits the patch-time aggregation as implicit hierarchy signals and enables an online hierarchical patch reconciliation in the prediction head to boost the performance.\n%     %     \\item \\textbf{Gated Attention~(GA):} ~\\tsm~ apply a simple lightweight gated attention to relatively weigh the input features based on their local importance. This approach effectively filters the noise in the data, which when augmented with \\tsm~ leads to improved short and long-term interaction modeling without requiring the need for complex multi-head self-attention blocks.    \n%     % \\end{itemize}\n    \n%     % \\item While several standard reconciliation approaches exist in the literature, \n%     \\item This study is the first of its kind to highlight the benefits of infusing online reconciliation\\footref{reconcile_tag} approaches in the prediction head of Transformer/Mixer style backbone architectures for time-series modeling. We call it ``online\" as these are learned as part of the overall loss computation and not as a separate offline process.\n%     % as part of the standard pretraining/finetuning loss computation as compared to offline standard reconciliation approaches.\n%     \\item We conduct a detailed empirical analysis on 7 popular public datasets, wherein. ~\\tsm~ outperforms all existing benchmarks with extremely reduced training time and memory usage. Snapshot (relative MSE improvements) of the primary benchmarks are as follows:\n%         \\begin{itemize}\n%             \\item ~\\tsm~ outperforms DLinear by a good margin of 8\\% (Table~\\ref{tab:supervised}).\n%             \\item ~\\tsm~ outperforms PatchTST by a margin of 1-2\\% but with a significant 2-3X reduction in training time and memory usage. (Table~\\ref{tab:supervised},\\ref{tab:speedup},\\ref{tab:fm_1},\\ref{tab:fm-2-all}).\n%             % \\item ~\\tsm~ outperforms all the other standard benchmarks(i.e. CrossFormer, FEDformer, Autoformer, Informer, S4, LightTS, BTSF, TNC, TS-TCC, CPC, and TS2Vec) by a significant margin of 20-60\\% \n%             \\item ~\\tsm~ outperforms all other state-of-the-arts (i.e. CrossFormer, FEDformer, Autoformer, Informer, S4, LightTS, BTSF, TNC, TS-TCC, CPC, and TS2Vec) by a significant margin of 20-60\\%. \n%         \\end{itemize}\n        \n%     % \\item We conduct a detailed empirical analysis on 7 popular public datasets, wherein. ~\\tsm~ outperforms all existing benchmarks with the following MSE relative improvements:\n%     %     \\begin{itemize}\n%     %         \\item {8-60\\%} \\wrt {MLP and non-transformer based forecasting benchmarks} (i.e. DLinear, S4 and LightTS)\n%     %         \\item {50-70\\% } \\wrt {self-supervised forecasting benchmarks} (i.e. BTSF, TNC, TS-TCC~, CPC, and TS2Vec)\n%     %         \\item {20-60\\% } \\wrt {standard transformer-based forecasting benchmarks} (i.e. FEDformer, Autoformer, and Informer).\n%     %         \\item {1-2\\%} marginal improvement \\wrt {Patch Transformer based forecasting benchmark} (i.e. PatchTST) but with a significant 2-3X reduction in training time and memory usage.\n%     %     \\end{itemize}\n    \n%     % \\item Finally, this study compares and contrasts different channel-mixing techniques for improved time-series MLP Mixer adoption.\n%     \\item Finally, we compare and contrast different channel-mixing techniques for improved time-series MLP Mixer adoption.\n    \n\n\n\n    \n    %     Time-series often possess an inherent hierarchical structure and hierarchically aggregated time-series often \n    % \\end{itemize}\n    \n    % % with several architectural changes to exploit the time-series characteristics which are proved to be useful to boost the performance, and propose \\tsm~model suitable for time series modeling. The proposed modeling components are below.\n\n    % \\begin{itemize}\n    %     \\item \\textbf{Channel Independent backbone:} The backbone of the proposed architecture shares model weights across different channels (a channel refers to a particular time series in the multi-variate data). This enables better generalization and eases transfer learning across datasets with different number of channels.\n        \n    %     \\item \\textbf{Gated Attention~(GA):} While self-attention involves higher complexity, we apply a simple light-weight gated attention to relatively weigh the input features used in the \\tsm. This approach is relatively very simple, and when augmented with \\tsm~architecture, enables effective capture of short- and long-term dependencies without requiring the complex self-attention.\n        \n    %     \\item \\textbf{Forecast Channel Finetune~(CF) head:} While the backbone is channel independent, in order to explicitly capture the cross-correlations between channels, we add the CF head which does online reconciliation of every forecast point with its pre- and post-surrounding context in a channel mixed way. Through empirical ablation study, we show that this approach when combined with channel independent backbone performs better as compared to standard channel flattening or inter-channel mixer backbone.\n    \n    %     \\item \\textbf{Hierarchical Patch FineTuning~(HF) head:} This enables online reconciliation of forecasts with respect to patch-time aggregation.\n    % \\end{itemize}\n\n    % \\item We propose different channel mixing techniques which are showed to improve the performance of \\tsm~over vanilla \\mixer model applied to time series domain.\n\n    % \\item We demonstrate reduced run-time complexity of the proposed model compared to transformer-based models through theoretical (number of flops) and empirical evidence.   \n\n    % \\item The proposed \\tsm~can be trained with a forecasting objective directly, or pre-trained with a self-supervised loss which makes it appropriate a foundational model~\\cite{foundation}. \n% \\end{enumerate}\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n% \\subsection{Transformer-based time series models}\n\\textbf{Transformer-based time series models: }\nThe success of Transformers~\\cite{transformer} in the NLP domain has inspired time series researchers to come up with TS Transformer-based models.\nDue to the quadratic time and memory complexity of Transformer architectures, one major difficulty in adopting NLP Transformers for time series tasks lies in processing much longer input sequence lengths ($L$). \n% To address this, several compute-aware time series transformer models such as Informer~\\cite{informer}, Autoformer~\\cite{autoformer}, Pyraformer\\cite{pyraformer} and FEDformer~\\cite{fedformer} have been proposed in the literature which modifies the self-attention mechanisms to achieve $O(L)$ or $O(L\\log(L))$ complexity.\nTo address this, several compute-aware time series Transformer models such as Informer~\\cite{informer}, Autoformer~\\cite{autoformer}, Pyraformer\\cite{pyraformer} and FEDformer~\\cite{fedformer} have been proposed, which modifies the self-attention mechanisms to achieve $O(L)$ or $O(L\\log(L))$ complexity.\n\n\n% Informer addressed this issue with a ProbSparse attention mechanism which attains $L\\log(L)$ complexity for an input sequence length of $L$~\\cite{informer}.\n% Autoformer proposed a auto-correlation and decomposition-based transformer architecture for long-term forecasting~\\cite{autoformer}.\n% Pyraformer proposed a multi-resolution pyramidal attention mechanism that captures hierarchical temporal dependencies~\\cite{pyraformer}.\n% FEDformer introduced Fourier and Wavelet transform-enhanced blocks to achieve $O(L)$ complexity~\\cite{fedformer}.\n% More details about these models can be found in~\\cite{dlinear}.\n\n% \\subsection{Patch-based time series models}\n\\textbf{Patch-based time series models:}\nThe above-mentioned works mostly feed granular-level time series into the Transformer model, hence, they focus on learning attention over every time point. In contrast, PatchTST~\\cite{patchtst} and CrossFormer~\\cite{crossformer} enable patching before feeding it to the Transformers for learning representation across patches. PatchTST follows the channel independence approach and in contrast, CrossFormer follows the channel-mixing approach.\n% which can result in constraining learning short sequences and missing global information.\n% Some of these models attempt to address this issue with handcrafted designs.\n% For example, Autoformer's auto-correlation structure~\\cite{autoformer}, Triformer's pseudo time stamps~\\cite{triformer} etc. These methods designs domain specific biase to the model to improve performance.\n% PatchTST~\\cite{patchtst}, on the other hand, proposed an end-to-end patch based Transformer model with channel independence and demonstrated the benefit of employing patches instead of granular-level time series to the Transformer model. Likewise, CrossFormer~\\cite{crossformer} also enables patching with cross-channel correlation. However, \n% However, the usage of Transformer can still be a performance bottleneck because of the presence of several matrix multiplications over multiple self attention heads.\n\n% \\subsection{\\mixer in vision domain}\n\\textbf{\\mixer in vision domain:}\nRecently, multi-layer perceptron's (MLP) strength has been reinvented in the vision domain~\\cite{mlpmixer, payattention, resmlp}.\nThe idea of~\\cite{mlpmixer} is to transform the input image through a series of permutation operations, which inherently enforces mixing of features within and across patches to capture short and long-term dependencies without the need for self-attention blocks.\n\\mixer attains similar performance as CNN and Transformer. To extend upon, in gMLP~\\cite{payattention}, authors propose to use MLP-Mixer with Spatial Gating Unit (SGU)\\footref{ga_note} and ResMLP~\\cite{resmlp} proposes to use MLP-Mixer with residual connections. \n% Please note that the SGU component in gMLP~\\cite{payattention} is different from the gated attention block proposed in our paper.\n% Liu \\etal~\\cite{payattention} proposed gMLP (MLP with Spatial Gating Unit (SGU)) which achieved similar performance as Transformers in vision and NLP tasks. Please note that SGU is different from the gated attention proposed in our paper. The ResMLP~\\cite{resmlp} is built with MLP and residual connections, and it also learns cross-patch information similar to \\mixer.\n\n% \\subsection{MLP based sequence / time series models}\n\\textbf{MLP based sequence / time series models:}\\footnote{Comparison with \\cite{chen2023} is not included, as it was released in arXiv one month after the start of the KDD 2023 peer-review process~\\cite{kdd23}, where our current paper has been submitted and got accepted.}\nSimilar efforts have recently been put in the time series domain.\nZhang \\etal~\\cite{lightts} proposed LightTS that is built upon MLP and two sophisticated downsampling strategies for improved forecasting.\n% LightTS achieved similar or better performance than deeper and complex DNN models like RNNs, GNNs, and transformers.\nThe DLinear model~\\cite{dlinear} also employs a simple linear model after decomposing the time series into trend and seasonality components. DLinear questions the effectiveness of Transformers as it easily beats Transformer-based SOTAs.\n% DLinear questions the effectiveness of transformers for long-term forecasting and also beats LightTS.\nLi \\etal~\\cite{MLP4Rec} proposed MLP4Rec, a pure MLP-based architecture for sequential recommendation task, to capture correlations across time, channel, and feature dimensions.\n\n% \\subsection{Background on vision \\mixer}\n% The basic intuition behind \\mixer is to capture cross-channel and cross-patch correlations effectively by a series of permutation and MLP operations.\n% It works on images of shape ``patches $\\times$ channels'', where the image patches act as tokens.\n% \\mixer employs two types of mixing: channel mixing and token mixing.\n% The MLP model that does channel mixing executes on each token independently and thus, finds correlations across different channels.\n% The token mixer MLP model operates on each channel independently and explore relationships across spatial locations \\ie the patch tokens.\n% These two mixer layers are interleaved to extract information across both dimensions.\n% Skip connections are incorporated in the model to enable residual learning.\n\n"
            },
            "section 3": {
                "name": "Methodology",
                "content": "\n\n\n",
                "subsection 3.1": {
                    "name": "Notations",
                    "content": "\nThroughout the paper, we use the following variable names: \n$\\mX_{c \\times L}$: a multivariate time series of length $L$ and $c$ channels or $c$ time series, $sl \\le L$: input sequence length, $fl$: forecast sequence length (a.k.a. horizon), \n%$c$: number of channels,\n% \\footnote{Throughout the text, we will use ``channel'' to denote the individual times series in a multivariate data.}, \n$b$: batch size, $n$: number of patches, $pl$: patch length, $hf$: hidden feature dimension, $ef$: expansion feature dimension, $nl$: number of \\mixer~ layers, $\\gM$: learned DNN model, $op$: number of output forecast patches, $cl$: context length, $spl$: patch length for cross-channel forecast reconciliation head, $\\hat{H}$: patch-aggregated prediction, $H$: patch-aggregated ground truth, $\\hat{Y}_\\text{rec}$: actual base prediction, $Y_\\text{rec}$: base ground truth, $sf$: scale factor.\n% Some additional notations are used in Figure~\\ref{fig:FCR} \\& ~\\ref{fig:HPR}: $op$: number of output forecast patches, $cl$: context length, $spl$: patch length for cross-channel forecast reconciliation head, $\\hat{H}$: patch-aggregated prediction, $H$: patch-aggregated ground truth, $\\hat{Y}_\\text{rec}$: actual base prediction, $Y_\\text{rec}$: base ground truth, $sf$: scale factor.\n% One additional notation is used in Figure~\\ref{fig:HPR}: $sf$: scale factor.\nTo ensure better understanding, we provide the shape of a tensor as its subscript in the text, and in square brackets in the architectural diagrams.\nWe denote a linear layer in the neural network by $\\gA(\\cdot)$ for compactness. \n% \\begin{equation}\n%     \\vy_{* \\times d_{\\text{out}}} = \\gA\\left( \\vx_{* \\times d_{\\text{in}}} \\right) = \\vx_{* \\times d_{\\text{in}}} \\times \\mW_{d_{\\text{in}} \\times d_{\\text{out}}} + \\vb_{d_{\\text{out}}}\n% \\end{equation}\n\nThe multivariate forecasting task is defined as predicting future values of the time series given some history:\n\\begin{equation}\n    \\hat{\\mY}_{fl \\times c} = \\gM \\left( \\mX_{sl \\times c} \\right).\n\\end{equation}\nThe ground truth future values will be denoted by ${\\mY}_{fl \\times c}$.\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Training methodologies",
                    "content": "\\label{subsec:Training methodologies}\nTSMixer has two training methodologies: supervised and self supervised.\nThe supervised training follows the ``prediction'' workflow as shown in the right part of Figure~\\ref{fig:high-level}.\nFirst, the input history time series goes through a sequence of transformation (normalization, patching, and permutation).\nThen, it enters the TSMixer backbone which is responsible for the main learning process.\nThe prediction head converts the output embeddings of the backbone to the base forecasts, $\\hat{\\mY}$.\nThe model can be trained to minimize the mean squared error~(MSE) of the base forecasts: \n$\\gL\\left( \\mY, \\hat{\\mY} \\right) = \\left|\\left| \\mY - \\hat{\\mY} \\right|\\right|_2^2$.\nWe introduce two extra \\textit{online} forecast reconciliation heads which, if activated, can tune the base forecasts and produce more accurate forecasts by leveraging cross-channel and patch-aggregation information. When any or both of these reconciliation heads are activated, a customized MSE-based objective function is employed on the tuned forecasts. A detailed discussion is provided in Section~\\ref{subsubsec:Forecast online reconciliation}. \n\nThe self-supervised training is performed in two stages.\nFirst, the model is pretrained (see the ``pretrain'' workflow in Figure~\\ref{fig:high-level}) with a self-supervised objective. Then, the pretrained model is finetuned through the ``prediction'' workflow for a supervised downstream task.\nSelf-supervised pretraining has been found to be useful for a variety of NLP~\\cite{bert}, vision~\\cite{beit}, and time series tasks~\\cite{patchtst}.\nSimilar to BERT's~\\cite{bert} masked language modeling~(MLM) in the NLP domain, we employ a masked time series modeling~(MTSM) task.\nThe MTSM task randomly applies masks on a fraction of input patches, and the model is trained to recover the masked patches from the unmasked input patches.\nOther input transformations in the pretrain workflow are the same as in the prediction workflow.\nThe MTSM task minimizes the MSE reconstruction error on the masked patches. The modular design of TSMixer enables it for either supervised or self-supervised training by only changing the model head (and keeping backbone the same).\n% If $\\mX^M = \\left[ \\mX^i \\right]_{i=1}^{n}$ and $\\hat{\\mX^M} = \\left[ \\hat{\\mX}^i \\right]_{i=1}^{n}$ denote the masked input time series and masked reconstructed time series respectively, then the MTSM objective can be written as:\n% \\begin{equation}\n%     \\gL_{\\text{MTSM}} = \\left|\\left| \\mX^M - \\hat{\\mX^M} \\right|\\right|_2^2$.\n% \\end{equation}\n\n"
                },
                "subsection 3.3": {
                    "name": "Model components",
                    "content": " \nHere we discuss the modeling components that are introduced to a vanilla \\mixer to have improved performance.\n% \\subsubsection{High-level structure}\nThe high-level architecture is shown in Figure~\\ref{fig:high-level}.\nFor stochastic gradient descent~(SGD), each minibatch, $\\mX^{B}_{b \\times sl \\times c}$, is populated from $\\mX$ by a moving window technique.\n% A minibatch is denoted by $\\mX^b_{b \\times sl \\times c}$, where $b$ denotes batch size, and $sl$ is the input sequence (window) length.\nThe forward pass of a minibatch along with its shape is shown in Figure~\\ref{fig:high-level}\n%as it undergoes through the forward pass.\n\n",
                    "subsubsection 3.3.1": {
                        "name": "Instance normalization",
                        "content": "\nThe input time series segment goes through reversible instance normalization~(RevIN)~\\cite{revin}.\nRevIN standardizes the data distribution (\\ie removes mean and divides by the standard deviation) to tackle data shifts in the time series.\n% After that it scales the distribution with trainable affine parameters $\\gamma$ and $\\beta$. The affine parameters are learned from the input data, and then, utilized again on the model output to denormalize the data.\n% which helps the model tackle data drift that is often observed between train and test periods in a time series.\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Patching",
                        "content": "\n\\label{sec:patching}\nEvery univariate time series is segregated into overlapping / non-overlapping patches with a stride of $s$. For self-supervised training flow, patches have to be strictly non-overlapping.\n% The minibatch $\\mX^B_{b \\times sl \\times c}$ is reshaped into $\\mX^P_{b \\times n \\times pl \\times c}$, where $pl$ denotes the patch length, and $n$ is the number of patches (hence, $n \\times pl = sl$).\nThe minibatch $\\mX^B_{b \\times sl \\times c}$ is reshaped into $\\mX^P_{b \\times n \\times pl \\times c}$, where $pl$ denotes the patch length, and $n$ is the number of patches (hence, $n = \\lfloor (sl - pl)/s \\rfloor +1$).\nThe patched data is then permuted to $\\mX^{P'}_{b \\times c \\times n \\times pl}$ and fed to the TSMixer backbone model.\nPatching reduces the number of input tokens to the model by a factor of $s$, and hence, increases the model runtime performance significantly as compared to standard point-wise Transformer approaches~\\cite{patchtst}.\n% \\subsubsection{Masking (only for pretrain workflow)}\n\n"
                    },
                    "subsubsection 3.3.3": {
                        "name": "TSMixer backbone",
                        "content": "\nThree possible backbones are shown in Figure~\\ref{fig:backbone:backbones}.\nThe vanilla backbone (\\vtsm) flattens the channel and patch dimensions $(c \\times pl)$ on the input before passing to the next layer. This approach is commonly followed in Vision MLP Mixing techniques and hence acts as a vanilla baseline.\nWe propose two new types of backbone: channel independent backbone (\\citsm) and inter-channel backbone (\\ictsm). They differ in their MLP mixer layer architectures.\n\\citsm~backbone is inspired from the PatchTST~\\cite{patchtst} model, in which the MLP Mixer layer is shared across channels which forces the model to share learnable weights across the channels. This results in a reduction of model parameters. Moreover, \\citsm~ enables us to employ TSMixer for self-supervised modeling over multiple datasets each having a different number of channels, which ~\\vtsm~ cannot.\nIn ~\\ictsm, an extra inter-channel mixer module is activated in the backbone to explicitly capture inter-channel dependencies. All these backbones will be compared with extensive experimentation.\n\nNote that all the TSMixer backbones start with a linear patch embedding layer (see Figure~\\ref{fig:backbone:backbones}).\nIt transforms every patch independently into an embedding: $\\mX^E_{b \\times c \\times n \\times hf} = \\gA \\left( \\mX^{P'} \\right)$.\nThe weight and bias of $\\gA(\\cdot)$ are shared across channels for ~\\citsm~ and ~\\ictsm ~backbones, but \\textit{not} for ~\\vtsm. Since channels are completely flattened in ~\\vtsm, $\\gA(\\cdot)$ in ~\\vtsm~ does not have any notion of multiple channels (i.e. c = 1).\n\n\n"
                    },
                    "subsubsection 3.3.4": {
                        "name": "MLP Mixer layers",
                        "content": "\n% TSMixer's MLP mixing technology has been inspired from and improved upon the vision \\mixer by incorporating enhancements that are relevant for multivariate time series datasets.\n\\tsm~ backbone stacks a set of mixer layers like encoder stacking in Transformers. Intuitively, each mixer layer (Figure~\\ref{fig:backbone:mixer-layers}) tries to learn correlations across three different directions: (i) between different patches, (ii) between the hidden feature inside a patch, and (iii) between different channels.\nThe former two mixing methods are adopted from the vision \\mixer, while the last one is proposed particularly for multivariate time series data.\n% Note that mixing of features is also performed in the hindsight by convolutional and transformer models at different spatial contexts~\\cite{mlpmixer}. \n% The novelty of TSMixer model lies in separating the feature-mixing in three distinct stages (see Figure~\\ref{fig:backbone:mixer-layers}).\nThe \\textbf{inter patch mixer} module employs a shared MLP (weight dimension $=n\\times n$) to learn correlation between different patches. The \\textbf{intra patch mixer} block's shared MLP layer mixes the dimensions of the hidden features, and hence the weight matrix has a dimension of $hf \\times hf$. The proposed \\textbf{inter channel mixer} (weight matrix size $=c \\times c$) mixes the input channel dimensions, and tries to capture correlations between multiple channels in a multivariate context. This inter-channel mixer has been proposed in MLP4Rec~\\cite{MLP4Rec} for the event prediction problem and we investigate its applicability for the time-series domain. Please note that inter-channel mixer block is included only in the ~\\ictsm~ backbone and not with the \\citsm~ and \\vtsm~ backbones (Figure~\\ref{fig:backbone:mixer-layers}). \nThe input and output of the mixer layers and mixer blocks are denoted by $\\mX^M_{b \\times c \\times n \\times hf}$. Based on the dimension under focus in each mixer block, the input gets reshaped accordingly to learn correlation along the focussed dimension. The reshape gets reverted in the end to retain the original input shape \n% (i.e. $b \\times c \\times n \\times hf$) \nacross the blocks and layers. \nAll of the three mixer modules are equipped with an MLP block (Appendix Figure.~\\ref{fig:mlp}), layer normalization~\\cite{layernorm}, residual connection~\\cite{resnet}, and gated attention. The former three components are standard in \\mixer while the gated attention block is described below. \n\n% TSMixer supports three types of backbone architectures.\n% \\textbf{Vanilla-TSMixer (V-TSMixer)} incorporates the original vision \\mixer and thus, flattens the channel dimension.\n% \\textbf{Channel Independent-TSMixer (CI-TSMixer)} employs channel independence as stated above, hence, the backbone weights are shared across channels. \n% \\textbf{InterChannel-TSMixer (IC-TSMixer)} employs a novel inter-channel mixing technique along with the inter-patch and intra-patch mixing of vanilla \\mixer.\n\n\n"
                    },
                    "subsubsection 3.3.5": {
                        "name": "Gated attention~(GA) block",
                        "content": "\n\nTime series data often has a lot of unimportant features that confuse the model. In order to effectively filter out these features, we add a simple Gated Attention ~\\cite{bahdanauattention} after the MLP block in each mixer component. GA acts like a simple gating function that probabilistically upscales the dominant features and downscales the unimportant features based on its feature values. The attention weights are derived by: $\\mW^A_{b \\times c \\times n \\times hf} = \\text{softmax}\\left( \\gA \\left( \\mX^M \\right) \\right)$.\nThe output of the gated attention module is obtained by performing a dot product between the attention weights and the hidden tensor coming out of the mixer modules: $\\mX^G = \\mW^A \\cdot \\mX^M$ (Appendix Figure.~\\ref{fig:ga}). Augmenting GA with standard mixer operations effectively guides the model to focus on the important features leading to improved long-term interaction modeling, without requiring the need for complex multi-head self-attention.\n% Through empirical analysis, we observe that GA effectively filters the noise in the data, which when\n% augmented with TSMixer leads to improved short and long-term interaction modeling without requiring the need for complex multi-head self-attention blocks.\n\n\n% in time-series, adding simple gating attention effectively reduces noise in the hidden features allowing only the important features to pass through the network. \n% The vanilla \\mixer employs an MLP after the mixer layers (see Figure~\\ref{fig:gated-attention:vanilla}).\n% However, MLP after the mixer layers mixes all the hidden features and does not weigh the features.\n% We incorporated gated attention (Figure~\\ref{fig:gated-attention:GA}), traditionally used in sequence to sequence models~\\cite{bahdanauattention}, to select a weighted transformation of the hidden features.\n% The attention weights are derived by: $\\mW^A_{b \\times c \\times n \\times hf} = \\text{softmax}\\left( \\gA \\left( \\mX^M \\right) \\right)$.\n% The output of the gated attention module is obtained by performing a dot product between the attention weights and the hidden tensor coming out of the mixer modules: $\\mX^G = \\mW^A \\cdot \\mX^M$.\n\n"
                    },
                    "subsubsection 3.3.6": {
                        "name": "Model heads",
                        "content": "\n\nBased on the training methodology (i.e. supervised or self-supervised), we either add prediction or pretrain head to the backbone. Both heads employ a simple Linear layer with dropout after flattening the hidden features across all patches (Appendix Figure.~\\ref{fig:model-heads}). By default, heads share the same weights across channels. The output of the prediction head is the predicted multivariate time series ($\\hat{\\mY}_{ b \\times fl \\times c }$), while the pretrain head emits a multivariate series of the same dimension as the input ($\\hat{\\mX}_{ b \\times sl \\times c}$).\n\n% Based on the training workflow (i.e. supervised or self-supervised), we either add prediction or pretrain head to the backbone. Both the heads Both the pretrain and prediction workflows employ MLP with dropout after flattening the hidden features across all patches (Figure~\\ref{fig:model-heads}).\n% The output of the prediction head is the predicted multivariate time series ($\\hat{\\mY}_{ b \\times c \\times fl}$), while the pretrain head emits a multivariate series of the same dimension as the input ($\\hat{\\mX}_{ b \\times c \\times sl}$).\n\n"
                    },
                    "subsubsection 3.3.7": {
                        "name": "Forecast online reconciliation",
                        "content": "\\label{subsubsec:Forecast online reconciliation}\nHere we propose two novel methods (in the prediction workflow, see Figure~\\ref{fig:high-level}) to tune the original forecasts, $\\hat{\\mY}$, based on two important characteristics of time series data: inherent temporal hierarchical structure and cross-channel dependency. Any or both of them can be activated in our TSMixer model to obtain reconciled forecasts.\n\n\\textbf{Cross-channel forecast reconciliation head:}\n\n% The CI-TSM backbone has been designed to be channel independent, to make it trainable over multiple datasets with different number of channels.\nIn many scenarios, the forecast for a channel at a certain time might be dependent on the forecast of another channel at a different point in time on the horizon.\nFor example, in retail domain, sales at a future time point might be dependent on the discount patterns around that time.\nHence, we introduce a cross-channel forecast reconciliation head which derives an objective that attempts to learn cross-dependency across channels within a local surrounding context \\textit{in the forecast horizon}.\nFigure~\\ref{fig:FCR} demonstrates its architecture.\n\nFirst, every forecast point is converted into a patch (of length $spl$) by appending its pre and post-surrounding forecasts based on the context length ($cl$). \n% First, the original forecasts are patched with a specific surrounding context length (3 in Figure~\\ref{fig:FCR}).\nThen, each patch is flattened across channels and passed through a gated attention and linear layer to obtain a revised forecast point for that patch. \nThus, all channels of a forecast point reconcile its values based on the forecast channel values in the surrounding context leading to effective cross-channel modeling. Residual connections ensure that reconciliation does not lead to accuracy drops in scenarios when the channel correlations are very noisy.\n% Then the patches are flattened, and gated attention (optional) and linear transformation are applied over the flattened vectors to obtain revised forecasts.\n% Since, the flattened vectors have forecasts from all the channels within a surrounding local context, the linear projection can effectively learn any inter-dependency that exists between them.\nSince the revised forecasts have the same dimension as the original forecasts, no change to the loss function is required. From experiments, we observe that the ``hybrid\" approach of having a channel-independent backbone augmented with a cross-channel reconciliation head provides stable improvements as compared to other channel-mixing approaches. Also, this architecture helps in better backbone generalization as it can be trained with multiple datasets with a varying number of channels while offloading the channel-correlation modeling to the prediction head (which is task and data-dependent).\n\n\\textbf{Online hierarchical patch reconciliation head:}\\footnote{This ``reconciliation'' is different from the reconciliation in hierarchical forecasting~\\cite{hyndman_book}. Here, hierarchy is defined as an aggregation over a patch, and the reconciliation is done online during training.}.\n\nTime series data often possess an inherent hierarchical structure, either explicitly known (\\eg hierarchical forecasting datasets~\\cite{hyndman_book}), or as an implicit characteristic (\\eg an aggregation of weather forecasts for seven days denotes an estimate of weekly forecast, an aggregation of sales forecast over all stores in a state denotes state-level sales, and so on).\nIn general, aggregated time series have better predictability and a good forecaster aims at achieving low forecast error in all levels of the hierarchy (\\cite{hyndman_book, makridakis2022m5, proxyhierarchical}).\nHere, we propose a novel method to automatically derive a hierarchical patch aggregation loss (online during training) that is minimized along with the granular-level forecast error.\nFigure~\\ref{fig:HPR} shows the architecture.\nThe original forecast $\\hat{\\mY}$ is segregated into $op$ number of patches, each of length $pl$.\nWe denote this as $\\hat{\\mY}^P$.\nNow, $\\hat{\\mY}$ is also passed through a linear layer to predict the hierarchically aggregated forecasts at the patch-level: $\\hat{\\mH}_{b \\times c \\times op}= \\gA \\left( \\hat{\\mY}_{b \\times c \\times fl} \\right)$.\nThen, we concatenate $\\hat{\\mY}^P$ and $\\hat{\\mH}$ at the patch level and pass it through another linear transformation to obtain reconciled granular-level forecast: $\\hat{\\mY}_{\\text{rec}}$. Thus, the granular-level forecasts get reconciled at a patch level based on the patch-aggregated forecasts leading to improved granular-level forecasts. Residual connections ensure that the reconciliation does not lead to accuracy drops in scenarios when predicting aggregated signals become challenging.\nNow, the hierarchical patch aggregation loss is calculated as follows:\n\\begin{equation}\n    \\gL_{\\text{hier}} = \\frac{1}{sf} \\gL\\left( \\hat{\\mH}, \\mH \\right) +\n    \\gL \\left( \\mY, \\hat{\\mY}_{\\text{rec}} \\right) +\n    \\frac{1}{sf} \\gL \\left( \\text{BU} \\left(  \\hat{\\mY}_{\\text{rec}} \\right), \\hat{\\mH} \\right)\n\\end{equation}\nwhere, $\\mY$ is ground-truth future time series, $H$ is the aggregated ground-truth at patch-level, $\\text{BU}$ refers to bottom-up aggregation of the granular-level forecasts to obtain the aggregated patch-level forecasts~\\cite{hyndman_book}, and $sf$ is scale factor.\nFor MSE loss, $sf = (pl)^2$.\nMore intuitively, this loss tries to tune the base forecasts in a way such that they are not only accurate at the granular-level, but also accurate at the aggregated patch-level.\nNote that a pre-defined dataset-specific hierarchical structure can be enforced here, but it is left for future studies.\n\n\n\n\n\n\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\n",
                "subsection 4.1": {
                    "name": "Experimental settings",
                    "content": "\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Datasets",
                        "content": "\nWe evaluate the performance of the proposed \\tsm~model on 7 popular multivariate datasets as depicted in Table~\\ref{table:dataset}. These datasets have been extensively used in the literature~\\cite{patchtst}\\cite{dlinear}\\cite{autoformer} for benchmarking multivariate forecasting models and are publically available in ~\\citep{dlinearrepo}. We follow the same data loading parameters (Ex. train/val/test split ratio) as followed in ~\\cite{patchtst}.\n% We use the following naming conventions to represent a model configuration. ~\\tsm can operate with 3 different backbone architectures: (i) ~\\citsm: Channel Independent ~\\tsm which only has inter-patch and intra-patch mixer in the MLP Mixer Layer and share weights across channels, (ii) ~\\ictsm: Inter Channel ~\\tsm which has inter-patch and intra-patch mixer in the MLP mixer layer (which shares weights across channels) and an explicit inter-channel mixer to capture the channel correlation. (iii) ~\\vtsm \n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Model Variants",
                        "content": "\n\nA ~\\tsm~ variant is represented using the naming convention ${``BackboneType\"}$-\\tsm(${``Enhancements\"}$).\\\\\n${``BackboneType\"}$ can either be Vanilla (\\van), or Channel Independent (\\ci), or Inter Channel (\\ic).\n${``Enhancements\"}$ can be a combination of Gated Attention (\\ga), Hierarchical Patch Reconciliation head (\\hr\n), and/or Cross-channel Reconciliation head (\\cc). Common variants are:\n\\begin{itemize}\n    \\item \\textbf{~\\ci-\\tsm}: Channel Independent ~\\tsm~ with no enhancements\n    \\item \\textbf{~\\tsmgh}: Channel Independent ~\\tsm~ with Gated Attention and Hierarchy Reconciliation head\n    \\item \\textbf{~\\tsmghc}: Channel Independent ~\\tsm~ with Gated Attention, Hierarchy and Cross-Channel Reconciliation head\n    \\item \\textbf{~\\tsmb}: Best of top performing ~\\tsm~variants [i.e. ~\\tsmgh~ and ~\\tsmghc]\n    \\item \\textbf{\\vtsm}: Vanilla ~\\tsm\n    \\item \\textbf{\\ictsm}: Inter-Channel ~\\tsm\n\\end{itemize}\nOther model variants can be formed using the same naming convention. Unless stated explicitly, the cross-channel reconciliation head uses the default context length of 1.\n\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Data \\& Model Configuration",
                        "content": "\n\nBy default, the following data and model configuration is used: Input Sequence length $sl$ = 512, Patch length $pl$ = 16, Stride $s$ = 8, Batch size $b$ = 8, Forecast sequence length $fl \\in \\{96, 192, 336, 720\\}$, Number of Mixer layers $nl$ = 8, feature scaler $fs$ = 2, Hidden feature size $hf$ = $fs*pl$ (32), Expansion feature size $ef$ = $fs*hf$ (64) and Dropout $do$ = 0.1. Training is performed in a distributed fashion with 8 GPUs, 10 CPUs and 1000 GB of memory. For ETT datasets, we use a lower hardware and model configuration with high dropout to avoid over-fitting, as the dataset is relatively small ($nl$ = 3, $do$ = 0.7, $ngpus$ = 1). Supervised training is performed with 100 epochs. \nIn self-supervised training, we first pretrain the backbone with 100 epochs. After that, in the finetuning phase, we freeze the backbone weights for the first 20 epochs to train/bootstrap the head (also known as \\textit{linear probing}), and then, we finetune the entire network (backbone + head) for the next 100 epochs. \nWe choose the final model based on the best validation score. Since overlapping patches have to be avoided in self-supervised methodology, we use reduced patch length and stride with the same size there (i.e. $pl$ = 8, $s$ = 8). This further updates the hidden feature and expansion feature size by $fs$ (i.e. $hf$ = 16, $ef$ = 32 ) for self-supervised methodology.\n% reduces the hidden feature and expansion feature size by 2 (i.e. $hf$ = 16 [i.e. $sf*pl$], $ef$ = 32 [i.e. $sf*hf$]) for self-supervised methodology. \nEvery experiment is executed with 5 random seeds (from 42-46) and the mean scores are reported. Standard deviation is also reported for the primary results. We use mean squared error (MSE) and mean absolute error (MAE) as the standard error metrics.\n\n\n% \\begin{table}[!ht]\n%     \\centering\n%     \\begin{tabular}{|l|l|l|l|l|l|}\n%     \\hline\n%         Metric & Data & Patch TST & CI-TSMixer (Imp) & CI-TSMixer(G,H) (Imp) & CI-TSMixer(G,H,CC) (Imp) \\\\ \\hline\n%         MACs(T) & Electricity & 147.879 & 37.062 (4X) & 46.44 (3.2X) & 48.566 (3X) \\\\ \\hline\n%         MACs(T) & Traffic & 260.367 & 65.25 (4X) & 81.77 (3.2X) & 91.78 (2.8X) \\\\ \\hline\n%         MACs(T) & Weather & 19.709 & 4.94 (4X) & 6.19 (3.2X) & 6.21 (3.2X) \\\\ \\hline\n%         PARAMETERS (M) & Electricity & 1.174 & 0.348 (3.4X) & 0.4 (2.9X) & 1.648 (0.7X) \\\\ \\hline\n%         PARAMETERS (M) & Traffic & 1.174 & 0.348 (3.4X) & 0.4 (2.9X) & 9.33 (0.1X) \\\\ \\hline\n%         PARAMETERS (M) & Weather & 1.174 & 0.348 (3.4X) & 0.4 (2.9X) & 0.405 (2.9X) \\\\ \\hline\n%         EPOCH TIME (min) & Electricity & 36.22 & 15.56 (2.3X) & 20.43 (1.8X) & 20.5 (1.8X) \\\\ \\hline\n%         EPOCH TIME (min) & Traffic & 64.04 & 27.49 (2.3X) & 36.08 (1.8X) & 36.51 (1.8X) \\\\ \\hline\n%         EPOCH TIME (min) & Weather & 5.2 & 3.08 (1.7X) & 4.13 (1.3X) & 4.17 (1.2X) \\\\ \\hline\n%         MAX MEMORY (GB) & Electricity & 6.14 & 2.25 (2.7X) & 2.9 (2.1X) & 2.94 (2.1X) \\\\ \\hline\n%         MAX MEMORY (GB) & Traffic & 8.24 & 3.03 (2.7X) & 3.89 (2.1X) & 4.15 (2X) \\\\ \\hline\n%         MAX MEMORY (GB) & Weather & 0.451 & 0.165 (2.7X) & 0.21 (2.1X) & 0.211 (2.1X) \\\\ \\hline\n%     \\end{tabular}\n% \\end{table}\n\n"
                    },
                    "subsubsection 4.1.4": {
                        "name": "SOTA Benchmarks",
                        "content": "\n% We categorize SOTA forecasting benchmarks into the following categories: \n% \\begin{itemize}\n%     \\item \\textbf{Standard Transformers:} FEDformer~\\cite{fedformer}, Autoformer\\cite{autoformer} and Informer ~\\cite{informer}. \n%     % We exclude Pyraformer~\\cite{pyraformer} and LogTrans~\\cite{logtrans} considering their inferior performance compared to FEDformer ~\\cite{fedformer}.\n%     \\item \\textbf{Patch Transformers:} PatchTST~\\cite{patchtst} and CrossFormer~\\cite{crossformer}. \n%     \\item \\textbf{MLPs and Non-Transformers:} DLinear~\\cite{dlinear}, LightTS~\\cite{lightts} and S4~\\cite{s4}.\n%     \\item \\textbf{Self-supervised models:} BTSF~\\cite{btsf}, TNC~\\cite{tnc}, TS-TCC~\\cite{ts-tcc}, CPC~\\cite{cpc}, and TS2Vec~\\cite{ts2vec}.\n% \\end{itemize}\n\nWe categorize SOTA forecasting benchmarks into the following categories: \\textbf{ (i) Standard Transformers:} FEDformer~\\cite{fedformer}, Autoformer\\cite{autoformer} and Informer ~\\cite{informer}, \\textbf{(ii) Patch Transformers:} PatchTST~\\cite{patchtst} and CrossFormer~\\cite{crossformer}, \\textbf{(iii) MLPs and Non-Transformers:} DLinear~\\cite{dlinear}, LightTS~\\cite{lightts} and S4~\\cite{s4}, \\textbf{ (iv) Self-supervised models:} BTSF~\\cite{btsf}, TNC~\\cite{tnc}, TS-TCC~\\cite{ts-tcc}, CPC~\\cite{cpc}, and TS2Vec~\\cite{ts2vec}.\n\n\n% We categorize SOTA benchmarks into the following categories: \n% \\begin{itemize}\n%     \\item \\textbf{Standard Transformer forecasting benchmarks}: FEDformer~\\cite{fedformer}, Autoformer\\cite{autoformer} and Informer ~\\cite{informer}. \n%     % We exclude Pyraformer~\\cite{pyraformer} and LogTrans~\\cite{logtrans} considering their inferior performance compared to FEDformer ~\\cite{fedformer}.\n%     \\item \\textbf{Patch Transformer forecasting benchmark:} \\\\ PatchTST~\\cite{patchtst} and CrossFormer~\\cite{crossformer}. \n%     \\item \\textbf{MLP and Non-Transformer forecasting benchmarks: }DLinear~\\cite{dlinear}, LightTS~\\cite{lightts} and S4~\\cite{s4}.\n%     \\item \\textbf{Self-supervised forecasting benchmarks:} BTSF~\\cite{btsf}, TNC~\\cite{tnc}, TS-TCC~\\cite{ts-tcc}, CPC~\\cite{cpc}, and TS2Vec~\\cite{ts2vec}.\n% \\end{itemize}\n\n\n% Unless explicitly stated,We report the results of all the benchmarks from ~\\cite{patchtst} \\& ~\\cite{btsf}.\n\n \n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Supervised Multivariate Forecasting",
                    "content": "\nIn this section, we compare the accuracy and computational improvements of TSMixer with the popular benchmarks in supervised multivariate forecasting.\n\n",
                    "subsubsection 4.2.1": {
                        "name": "Accuracy Improvements",
                        "content": "\nIn Table~\\ref{tab:supervised}, we compare the accuracy of \\tsm~ Best variant (i.e. ~\\tsmb) with SOTA benchmarks.\n% on the supervised multivariate forecasting problem (\\ie the model follows the supervised methodology as described in Section~\\ref{subsec:Training methodologies}). \n% Since MSE and MAE report similar relative characteristics \\wrt benchmarks, we explain all our results using the MSE metric. \nSince we observe similar relative patterns in MSE and MAE, we explain all the results in this paper using the MSE metric. \n~\\tsm~ outperforms standard Transformer and MLP benchmarks by a significant margin (DLinear: 8\\%, FEDformer:  23\\%, Autoformer: 30\\% and Informer: 64\\%). \n% Since LightTS and S4 report results in a different experiment setting, we highlight their results in the Appendix (Table~\\ref{tab:lightts}), wherein, \\tsm~ outperforms LightTS and S4 by 33\\% and 66\\%. \n% PatchTST (which refers to PatchTST/64 in ~\\cite{patchtst}) is one of the strongest baseline, which \\tsm~ marginally beats by 1\\%. \nPatchTST (refers to PatchTST/64 in ~\\cite{patchtst}) is one of the strongest baselines, and \\tsm~ marginally outperforms it by 1\\%. \nHowever, \\tsm~ achieves this improvement over PatchTST with a significant performance improvement \\wrt training time and memory (Section.~\\ref{sec:speedup}). \nFor exhaustive results with the individual best~\\tsm~ variants, refer to Appendix Table~\\ref{tab:supervised-ab1}. \nAlso, Appendix~\\ref{appendix_1} highlights the superior performance of TSMixer with other secondary benchmarks (LightTS, S4 and CrossFormer).\n\n\n\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "Computational Improvements",
                        "content": "\n\\label{sec:speedup}\nPatchTST is considered the most compute-effective model across all time series Transformers, as patching drastically reduces the input size by a factor of s (stride)~\\cite{patchtst}. In contrast, TSMixer not only enables patching but also completely eliminates the self-attention blocks.  Hence, TSMixer shows significant computation improvement over PatchTST and other Transformer models.  In  Table~\\ref{tab:speedup}, we highlight the speed-up and memory comparison of ~\\tsm~ over PatchTST. For analysis, we capture the following metrics: (i) Multiply-Add cumulative operations on the entire data per epoch (MACs), (ii) Number of model parameters (NPARAMS), (iii) Single EPOCH TIME and (iv) Peak GPU memory reached during a training run (MAX MEMORY). For this experiment, we trained TSMixer and PatchTST models in a single GPU node with the same hardware configuration in a non-distributed manner to report the results. To ensure fairness in comparison, we use the exact model parameters of PatchTST and ~\\tsm~ which were used in the error metric comparison reported in Table~\\ref{tab:supervised}. In Table~\\ref{tab:speedup}, we capture the average improvement of ~\\tsm~ over PatchTST for each performance metric across the three larger datasets (Electricity, Weather, Traffic) with $fl$=96. Since ~\\citsm~ is purely MLP based, it significantly reduces the average MACs (by $\\sim$4X), NPARAMS \\& MAX MEMORY (by $\\sim$3X), and EPOCH TIME \\footnote{MACs and EPOCH TIME are highly correlated and in general create a similar relative impact across models. However, since PatchTST involves high parallelism across the attention heads, we observe a different relative impact \\wrt MACs and EPOCH TIME.} (by $\\sim$2X).  Even after enabling gated attention and hierarchy reconciliation, ~\\tsmgh~ still shows a good reduction in MACs \\& NPARAMS (by $\\sim$3X), and EPOCH TIME \\& MAX MEMORY (by $\\sim$2X). It is important to note that, when cross-channel reconciliation is enabled [i.e ~\\tsmghc], the number of parameters becomes very high in ~\\tsm~ as compared to PatchTST. The reason is that the number of parameters in the cross-channel reconciliation head is dependent on the number of channels in the dataset that leads to this scaling effect. For example, since Electricity and Traffic datasets have a very high number of channels (i.e. 321 and 862 respectively), the number of parameters of the model also scales up, whereas weather (with only 21 channels) did not encounter any such scaling effect. Even PatchTST should show a similar scaling effect if the channel correlation is enabled in it. However, even with increased parameters, ~\\tsmghc~ still shows a notable reduction in MACs (by $\\sim$3X) and EPOCH TIME \\& MAX MEMORY (by $\\sim$2X).  The reason is that parameter scaling affects only the reconciliation head and not the backbone which primarily constitutes the total training time and memory. Thus, ~\\tsm~ and its variants can easily produce improved results over PatchTST in significantly less training time and memory utilization.\n\n\n"
                    }
                },
                "subsection 4.3": {
                    "name": "Component \\& Design Choice Analysis",
                    "content": "\nIn this section, we study the impact of various key components \\& design choices followed in the TSMixer.  \n\n",
                    "subsubsection 4.3.1": {
                        "name": "Effect of CI, Gated Attention \\& Hierarchy Patch Reconciliation",
                        "content": "\nTable~\\ref{tab:ci_ga_hr} depicts the improvements of various enhancement components in ~\\tsm~ over ~\\vtsm~ in three datasets: ETTH1, ETTM1, and Weather (for space constraint). \n% For results on all datasets, refer to the Appendix (Table~\\ref{tab:ga_hc-ab1}). \n~\\vtsm~ represents the vanilla model where all channels are flattened and processed together (similar to vision \\mixer). ~\\citsm~ outperforms ~\\vtsm~ by 9.5\\% by introducing channel independence (CI) in the backbone instead of channel flattening. By further adding gated attention (G) and hierarchy reconciliation (H) together [i.e. ~\\tsmgh], we observe an additional 2\\% improvement leading to a total of 11.5\\% improvement \\wrt ~\\vtsm. \nOn analysis with all datasets (Appendix Table~\\ref{tab:ga_hc-ab1}), we observe that ~\\tsmgh~ outperforms ~\\vtsm~ by an avg. of 19.3\\%. \nIn general, adding `G' and `H' together leads to more stable improvements in ~\\citsm~ as compared to just adding `G' or `H'.  \n\n\n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 4.3.2": {
                        "name": "Hybrid Channel Modelling Approach.",
                        "content": "\nIn Table~\\ref{tab:channel_table}, we compare various channel-mixing techniques and highlight the benefits of ``hybrid'' channel modeling followed in TSMixer. In summary, \\citsm~ outperforms \\vtsm~ by 11.5\\%, and by adding cross-channel reconciliation head (CC), the accuracy further improves by 2\\% leading to an overall improvement of 13.5\\% for \\citsm(\\ga,\\cc-Best) (i.e. channel independent backbone with CC head)\n% \\arindam{The gated attention in the cross-channel correlation head considerably reduces the noisy interactions across the channels. -- Any particular logic to come to this conclusion?}\nContext Length ($cl$) is an important hyperparameter in the CC head, which decides the surrounding context space across channels to enable reconciliation, and this parameter has to be decided based on the underlying data characteristics. For this experiment, we varied $cl$ from 1 to 5 and selected the best which is depicted as \\citsm(\\ga,\\cc-Best) in Table~\\ref{tab:channel_table}. \nFor more exhaustive results on various $cl$ and all datasets, refer to the Appendix Table.~\\ref{tab:cc-detail}. \n% Moreover, we observe from Table~\\ref{tab:channel_table} that channel independent backbone with cross-channel reconciliation head (i.e. \\citsm(\\ga,\\cc-Best) performs better as compared to the inter-channel mixer (i.e. ~\\ictsm). \nMoreover, we observe from Table~\\ref{tab:channel_table} that \\citsm(\\ga,\\cc-Best) performs better compared to the ~\\ictsm~ which applies cross-channel correlation inside the backbone. In addition, CrossFormer~\\cite{crossformer} proposes an alternative patch cross-channel correlation approach which ~\\tsm~ significantly outperforms by 30\\% (Appendix Table.~\\ref{tab:crossformer}). Thus, the ``hybrid\" channel modeling approach of having a \\textit{channel-independent backbone augmented with a cross-channel reconciliation head} is relatively robust to the noisy interactions across the channels.\n% and capturing the important ones for opportunistic improvements. \nAlso, from the architectural perspective - this approach helps in pretraining the backbone on multiple datasets with a varying number of channels. This cannot be achieved trivially with channel-mixing backbones. \n% From architectural perspective, another advantage of having channel independence in the backbone is that, it helps in pretraining the models on multiple datasets with varying number of channels (explained later). This cannot be achieved trivially with an inter-channel or vanilla backbone. \n% % Hence, having a channel-independent backbone with a cross-channel reconciliation head enables effective capture of cross-channel correlations while giving the flexibility to the backbone to pretrain with multiple datasets with varying channel counts. \n% Hence, having a CI backbone with a CC head enables effective capture of cross-channel correlations while providing a flexibility to pretrain the backbone on multiple datasets with varying number of channel. \n\n\n"
                    }
                },
                "subsection 4.4": {
                    "name": "Forecasting via Representation Learning",
                    "content": "\nIn this section, we compare TSMixer with popular benchmarks on multivariate forecasting via self-supervised representation learning.\n\n",
                    "subsubsection 4.4.1": {
                        "name": "Accuracy Improvements",
                        "content": "\nIn Table~\\ref{tab:fm_1}, we compare the forecasting results of ~\\tsm~ with self-supervised benchmarks. For BTSF, TNC, TS-TCC and CPC - we report the results from ~\\cite{btsf}. For self-supervised PatchTST, we report ETTH1 from ~\\cite{patchtst} and calculated it for Weather as it was unavailable. We use the $fl$ space commonly reported across~\\cite{btsf}~and~\\cite{patchtst}.\n% achieved via masked self-supervised pretrain and finetune workflows on two datasets (ETTH1 and Weather). \nIn the self-supervised workflow, the TSMixer backbone is first pre-trained to learn a generic patch representation, and then the entire network (backbone + head) is finetuned for the forecasting task. By training ~\\tsm~ with this approach, we observe from Table.~\\ref{tab:fm_1} that ~\\tsmb~ achieves a significant improvement (50-70\\% margin) from existing forecasting benchmarks learned via self-supervision (such as BFSF, TNC, TS-TCC, CPC). Similar trends were also observed with TS2Vec~\\cite{ts2vec} (Appendix Table~\\ref{tab:ab_ts2vec}). Also, ~\\tsmb~ beats self-supervised PatchTST by a  margin of 2\\%. However, as explained in Section.~\\ref{sec:speedup}, we achieve this improvement over PatchTST with a significant reduction in time and memory. \nFor a drill-down view of ~\\tsmb, refer to Appendix Table.~\\ref{tab:ab_fm_1}\n\n\n% \\subsection{Masked Self-Supervised Multivariate forecasting}\n% In Table~\\ref{tab:fm_1}, we compare and contrast the forecasting results achieved via self-supervised pretrain and finetune workflows on 2 datasets (ETTH1 and Weather). In this workflow, the TSMixer backbone is first pretrained to learn a generic patch representation, and then the entire network (backbone + head) is finetuned for the forecasting task. By training ~\\tsm~ with this approach, we observe from Table~\\ref{tab:fm_1} that ~\\tsmb~ achieves a significant improvement (50-70\\% margin) with existing forecasting benchmarks learned via self-supervision (or representation learning) techniques (such as BFSF, TNC, TS-TCC, CPC). Similar trends were also observed with TS2Vec which is depicted separately in Appendix(Table~\\ref{tab:ab_ts2vec}) due to different experimental settings. ~\\tsmb~ also beats self-supervised PatchTST by a notable margin of 2\\%. However, as explained in Section.~\\ref{sec:speedup}, we achieve this improvement over PatchTST with a very significant reduction in time and memory. \n\n\n"
                    },
                    "subsubsection 4.4.2": {
                        "name": "Pretrain strategies in Self Supervision",
                        "content": "\n% In Table~\\ref{tab:fm-2-all}, we do a detailed analysis of masked self-supervised approaches by varying the pretrain data creation strategy (on the three larger datasets: Electricity, Traffic, and Weather). We consider three pretraining data strategies: (i) SAME (same primary data for pretrain and finetune), (ii) ALL (pretrain with all data [ETT, Electricity, Traffic and Weather] and finetune with the primary data), (iii) TL (transfer learning: pretrain with all data except primary data and finetune with the primary data). \nIn Table.~\\ref{tab:fm-2-all}, we do a detailed analysis of the self-supervised approach on 3 large datasets (Electricity, Traffic, and Weather) with 3 different pretrain data creation strategies as follows: (i) SAME (same primary data for pretrain and finetune), (ii) ALL (pretrain with all data [ETT, Electricity, Traffic and Weather] and finetune with the primary data), (iii) TL (transfer learning: pretrain with all data except primary data and finetune with the primary data). \nSince we learn across multiple datasets, we use a bigger model size for this experiment (i.e. $nl$ = 12, $fs$ = 3) for better modeling capacity. From Table.~\\ref{tab:fm-2-all}, we observe that all three considered data strategies work equally well with marginal variations across them.\nHowever, the benefits of transfer learning across multiple datasets are not very notable, as we observe in other domains (like vision and text).\nOn average, ~\\citsm-Overall-Best(SS) [which indicates the best result across the considered data strategy variants] shows improved performance \\wrt self-supervised PatchTST (reported from ~\\cite{patchtst}) and supervised ~\\tsm~ by 1.5\\%. Thus, enabling self-supervision before the forecasting task helps in improving forecast accuracy. Furthermore, it helps in faster convergence for downstream tasks. \n% However, in future work, we plan to investigate other variants of ~\\tsm~ that can effectively benefit from transfer learning across multiple datasets for significant improvements, as we observe in other domains (like vision and text).\n% arindam {it is better to put it in conclusion ?}\n% However, in future work - we plan to investigate other novel architectures that can effectively benefit from different pretrain data strategies and show significant improvements \\wrt supervised approaches as we observe in other domains (like text and vision).\n\n% For this experiment, we consider 2 model sizes: (i) Default ($nl$ = 8, $sf$ = 2), (ii) Big ($nl$ = 12, $sf$ = 3) and 3 pretrain data strategies: (i) SAME (same primary data for pretrain and finetune), (ii) ALL (pretrain with all data[ETT, Electricity, Traffic and Weather] and finetune with the primary data), (iii) TL (transfer learning - pretrain with all data except primary data and finetune with the primary data). since we are training with mutliple sets of data together,  From Table~\\ref{tab:fm-2-all}, we observe that increasing model size generally improves the accuracy in self-supervision. However, there is no significant difference in accuracy across the 3 pretrain data strategies. On average, ~\\citsm-Overall-Best(SS) [which indicates the best result across the considered model-size and data strategy variants] shows improved performance \\wrt self-supervised PatchTST and supervised ~\\tsm~ by 1.5\\%. Thus, in general - enabling self-supervision before applying the forecasting task, helps in accuracy improvements. However, in future work - we plan to investigate new novel architectures that can effectively benefit from different pretrain data strategies and show significant improvements \\wrt supervised approaches as we observe in other domains (like text and vision).\n\n\n\n\n\n% \\begin{table*}[t]\n%     % \\setlength{\\tabcolsep}{5pt}\n%     \\centering\n%     \\scalebox{0.9}{\n%     % \\resizebox{\\linewidth}{!}{\n%     \\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}\n%         \\toprule\n        \n%         & FL  &  \\multicolumn{3}{c|}{\\makecell{\\citsm-Best (SS) \\\\ Model Size: Default}}& \\multicolumn{3}{c|}{\\makecell{\\citsm-Best (SS) \\\\ Model Size: Big}}& \\makecell{\\textbf{\\citsm-}\\\\\\textbf{Overall-Best (SS)}} & \\makecell{PatchTST \\\\ (SS)}& \\makecell{\\citsm-\\\\Best} \\\\ \\hline\n\n%         & & SAME & ALL & TL & SAME & ALL & TL & & & Supervised\\\\ \\hline\n%         \\multirow{4}*{\\rotatebox{90}{Electricity}} &96& 0.129 & 0.131 & 0.13 & \\uline{0.127} & 0.128 & 0.129 & \\uline{0.127$\\pm$0.001} & \\textbf{0.126} & 0.129$\\pm$0 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & 0.149 & 0.149 & 0.148 & \\textbf{0.145} & \\uline{0.146} & 0.147 & \\textbf{0.145$\\pm$1e-4} & \\textbf{0.145} & \\uline{0.146$\\pm$0.001} \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & 0.16 & 0.161 & 0.161 & \\textbf{0.156} & \\textbf{0.156} & 0.159 & \\textbf{0.156$\\pm$0.001} & 0.164 & \\uline{0.158$\\pm$0.001} \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & 0.187 & 0.189 & \\uline{0.186} & 0.187 & \\textbf{0.185} & 0.189 & \\textbf{0.185$\\pm$0.002} & 0.193 & \\uline{0.186$\\pm$0.001} \\\\ \n%         \\hline \n%         \\multirow{4}*{\\rotatebox{90}{Traffic}} &96& 0.359 & 0.368 & 0.367 & \\uline{0.35} & \\textbf{0.348} & \\uline{0.35} & \\textbf{0.348$\\pm$0.001} & 0.352 & 0.356$\\pm$0.002 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & 0.38 & 0.387 & 0.384 & 0.372 & \\textbf{0.37} & 0.372 & \\textbf{0.37$\\pm$0.001} & \\uline{0.371} & 0.377$\\pm$0.003 \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & 0.392 & 0.395 & 0.394 & \\textbf{0.379} & \\uline{0.38} & \\textbf{0.379} & \\textbf{0.379$\\pm$0.003} & 0.381 & 0.385$\\pm$0.002 \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & 0.428 & 0.43 & 0.431 & \\textbf{0.42} & \\uline{0.421} & 0.422 & \\textbf{0.42$\\pm$0.001} & 0.425 & 0.424$\\pm$0.001 \\\\ \n%         \\hline \n%         \\multirow{4}*{\\rotatebox{90}{Weather}} &96& \\uline{0.144} & 0.145 & 0.147 & \\uline{0.144} & \\textbf{0.143} & \\uline{0.144} & \\textbf{0.143$\\pm$1e-4} & \\uline{0.144} & 0.146$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & \\uline{0.19} & 0.191 & 0.194 & \\textbf{0.189} & \\textbf{0.189} & \\uline{0.19} & \\textbf{0.189$\\pm$1e-4} & \\uline{0.19} & 0.191$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & 0.241 & 0.242 & 0.266 & \\uline{0.24} & \\textbf{0.239} & 0.241 & \\textbf{0.239$\\pm$0.001} & 0.244 & 0.243$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & 0.319 & 0.328 & 0.321 & \\textbf{0.311} & \\uline{0.316} & 0.319 & \\textbf{0.311$\\pm$0.001} & 0.32 & \\uline{0.316$\\pm$0.001} \\\\ \n%         \\hline \n%         \\multicolumn{9}{c|}{\\textbf{\\citsm-Overall-Best (SS) \\% Improvement}} & \\textbf{1.45\\%}  & \\textbf{1.41\\%}  \\\\\n%         \\hline \n%     \\end{tabular}\n%     }\n%     \\caption{Masked Self-Supervised(SS) MSE benchmarking with varying pretrain data strategies and model sizes}\n%     \\label{tab:fm-2-all-dd}\n% \\end{table*}\n\n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 4.4.3": {
                        "name": "Patch Representations",
                        "content": "\nTo understand the semantic meaning of the learned patch representations, we randomly choose 5 patch embeddings (i.e. output of ~\\tsm~ backbone after pretraining) from ETTH1 and fetched its nearest 50 embeddings to form 5 clusters in the patch embedding space (Figure.~\\ref{fig:patch_emb}(b)). We then fetch their associated patch time series and plot in Figure.~\\ref{fig:patch_emb}(a). From the figure, we observe that nearby patch representations highly correlate to the patch time series of similar shapes and patterns, thereby learning meaningful patch representations that can effectively help in the finetuning process for various downstream tasks. \nPlease refer to Appendix Figure.~\\ref{fig:cluster_overall} for visualizations on more datasets.\n\n% \\begin{table*}[t]\n%     % \\setlength{\\tabcolsep}{5pt}\n%     \\centering\n%     \\resizebox{\\linewidth}{!}{\n%     \\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}\n%         \\toprule\n        \n%         & FL & \\makecell{\\citsm-\\\\Overall-Best (SS)} &  \\multicolumn{3}{c|}{\\makecell{\\citsm-Best (SS) \\\\ Model Size: Default}}& \\multicolumn{3}{c|}{\\makecell{\\citsm-Best (SS) \\\\ Model Size: Big}} & \\makecell{PatchTST \\\\ (SS)}& \\makecell{\\citsm-\\\\Best} \\\\ \\hline\n\n%         & & & SAME & ALL & TL & SAME & ALL & TL &  & Supervised\\\\ \\hline\n%         \\multirow{4}*{\\rotatebox{90}{electricity}} &96& \\uline{0.127$\\pm$0.001} & 0.129 & 0.131 & 0.13 & \\uline{0.127} & 0.128 & 0.129 & \\textbf{0.126} & 0.129$\\pm$0 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & \\textbf{0.145$\\pm$0} & 0.149 & 0.149 & 0.148 & \\textbf{0.145} & \\uline{0.146} & 0.147 & \\textbf{0.145} & \\uline{0.146$\\pm$0.001} \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & \\textbf{0.156$\\pm$0.001} & 0.16 & 0.161 & 0.161 & \\textbf{0.156} & \\textbf{0.156} & 0.159 & 0.164 & \\uline{0.158$\\pm$0.001} \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & \\textbf{0.185$\\pm$0.002} & 0.187 & 0.189 & \\uline{0.186} & 0.187 & \\textbf{0.185} & 0.189 & 0.193 & \\uline{0.186$\\pm$0.001} \\\\ \n%         \\hline \n%         \\multirow{4}*{\\rotatebox{90}{traffic}} &96& \\textbf{0.348$\\pm$0.001} & 0.359 & 0.368 & 0.367 & \\uline{0.35} & \\textbf{0.348} & \\uline{0.35} & 0.352 & 0.356$\\pm$0.002 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & \\textbf{0.37$\\pm$0.001} & 0.38 & 0.387 & 0.384 & 0.372 & \\textbf{0.37} & 0.372 & \\uline{0.371} & 0.377$\\pm$0.003 \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & \\textbf{0.379$\\pm$0.003} & 0.392 & 0.395 & 0.394 & \\textbf{0.379} & \\uline{0.38} & \\textbf{0.379} & 0.381 & 0.385$\\pm$0.002 \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & \\textbf{0.42$\\pm$0.001} & 0.428 & 0.43 & 0.431 & \\textbf{0.42} & \\uline{0.421} & 0.422 & 0.425 & 0.424$\\pm$0.001 \\\\ \n%         \\hline \n%         \\multirow{4}*{\\rotatebox{90}{weather}} &96& \\textbf{0.143$\\pm$0} & \\uline{0.144} & 0.145 & 0.147 & \\uline{0.144} & \\textbf{0.143} & \\uline{0.144} & \\uline{0.144} & 0.146$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & \\textbf{0.189$\\pm$0} & \\uline{0.19} & 0.191 & 0.194 & \\textbf{0.189} & \\textbf{0.189} & \\uline{0.19} & \\uline{0.19} & 0.191$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & \\textbf{0.239$\\pm$0.001} & 0.241 & 0.242 & 0.266 & \\uline{0.24} & \\textbf{0.239} & 0.241 & 0.244 & 0.243$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & \\textbf{0.311$\\pm$0.001} & 0.319 & 0.328 & 0.321 & \\textbf{0.311} & \\uline{0.316} & 0.319 & 0.32 & \\uline{0.316$\\pm$0.001} \\\\ \n%         \\hline \n%         \\multicolumn{9}{c|}{\\textbf{\\citsm-Overall-Best (SS) \\% Improvement}} & \\textbf{1.45\\%}  & \\textbf{1.41\\%}  \\\\\n%         \\hline \n%     \\end{tabular}\n%     }\n%     \\caption{Self-Supervised(SS) MSE benchmarking}\n%     \\label{tab:fm-2-all}\n% \\end{table*}\n\n\n\n% \\begin{table}[htbp]\n% \\centering\n% \\setlength{\\tabcolsep}{2pt}\n% \\scalebox{0.9}{\n% \\begin{tabular}{c|c|c|c|c|}\n% \\hline  \n% Data & Forecast Length & LightTS & S4 & \\textbf{\\citsm-Best}  \\\\\n% \\hline\n% ETTH1 & 336 & 0.466 & 1.407 & \\textbf{0.421} \\\\ \\hline\n% ETTH1 & 720 & 0.542 & 1.162 & \\textbf{0.444} \\\\ \\hline\n% ETTH2 & 336 & 0.497 & 1.98 & \\textbf{0.357} \\\\ \\hline\n% ETTH2 & 720 & 0.739 & 2.65 & \\textbf{0.395} \\\\ \\hline\n% Weather & 336 & 0.527 & 0.531 & \\textbf{0.243} \\\\ \\hline\n% Weather & 720 & 0.554 & 0.578 & \\textbf{0.316} \\\\ \\hline\n% \\multicolumn{2}{c|}{\\textbf{Relative \\% improvement}}& \\textbf{33.2} & \\textbf{66.4} & \\\\ \\hline\n% \\end{tabular}}\n% \\caption{LightTS and S4 MSE Benchmarking}\n% \\label{tab:lightts2}\n% \\end{table}\n\n\n\n\n% % \\linespread{1.2}\n% \\begin{table*}[t]\n%         % \\small\n% \t\\centering\n% \t\\resizebox{\\linewidth}{!}{\n% \t\t\\begin{tabular}{cc|c|cc|cc|cc|cc|cc|ccc}\n% \t\t\t\\cline{2-15}\n% \t\t\t&\\multicolumn{2}{c|}{Models}& \\multicolumn{2}{c|}{PatchTST}& \\multicolumn{2}{c|}{FEDformer}& \\multicolumn{2}{c|}{Autoformer}& \\multicolumn{2}{c|}{Informer}& \\multicolumn{2}{c|}{DLinear}& \\multicolumn{2}{c}{\\citsm-Best} \\\\\n% \t\t\t\\cline{2-15}\n% \t\t\t&\\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\\\\n% \t\t\t\\cline{2-15}\n% \t\t\t&\\multirow{4}*{\\rotatebox{90}{Weather}}& 96    & \\textbf{0.149} & \\textbf{0.198} & \\uline{0.152} & \\uline{0.199} & 0.176 & 0.237 & 0.238 & 0.314 & 0.249 & 0.329 & 0.354$\\pm$0.001 & 0.405$\\pm$0.001 \\\\\n%             &\\multicolumn{1}{c|}{}& 192   & \\textbf{0.194} & \\textbf{0.241} & \\uline{0.197} & \\uline{0.243} & 0.220 & 0.282 & 0.275 & 0.329 & 0.325 & 0.370 & 0.419$\\pm$0.001 & 0.434$\\pm$0.001  \\\\\n% \t\t\t\\cline{2-15}\n                \n% \t\t\\end{tabular}\n% \t}\n% \t\\caption{result}\n% \t\\label{tab:supervised}\n% \\end{table*}\n% % \\linespread{1}\n\n\n\n\n\n% \\input{experiment}\n\n\n"
                    }
                }
            },
            "section 5": {
                "name": "Conclusions and future directions",
                "content": "\nInspired by the success of MLP-Mixers in the vision domain, this paper proposes \\textbf{~\\tsm}, a purely designed MLP architecture with empirically validated time-series specific enhancements for multivariate forecasting and representation learning. Especially, we introduce a new hybrid architecture of augmenting various reconciliation heads and gated attention to the channel-independent backbone that significantly empowers the learning capability of simple MLP structures to outperform complex Transformer models. Through extensive experimentation, we show that ~\\tsm~ outperforms all popular benchmarks with a significant reduction in compute resources. In future work, we plan to extend ~\\tsm~ to other downstream tasks (such as classification, anomaly detection, etc.) and also improve the transfer learning capabilities across datasets. We also plan to investigate Swin\\cite{hiremlp}, Shift\\cite{shiftmlp}, and other newer Mixer variants\\cite{cyclemlp}\\cite{wavemlp} for its applicability in time-series.\n% and see how it adapts to the time-series domain.\n\n\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{main}\n\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\appendix\n\n"
            },
            "section 6": {
                "name": "Appendix",
                "content": "\n\n\n",
                "subsection 6.1": {
                    "name": "Datasets",
                    "content": "\n\nWe use $7$ popular multivariate datasets provided in \\citep{patchtst} for forecasting and representation learning. \\textit{Weather}\\footnote{https://www.bgc-jena.mpg.de/wetter/} dataset collects 21 meteorological indicators such as humidity and air temperature. \\textit{Traffic}\\footnote{https://pems.dot.ca.gov/} dataset reports the road occupancy rates from different sensors on San Francisco freeways. \\textit{Electricity}\\footnote{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014} captures the hourly electricity consumption of 321 customers. \\textit{ETT}\\footnote{https://github.com/zhouhaoyi/ETDataset} (Electricity Transformer Temperature) datasets report sensor details from two electric Transformers in different resolutions (15 minutes and 1 hour). Thus, in total we have 4 ETT datasets: \\textit{ETTM1}, \\textit{ETTM2}, \\textit{ETTH1}, and \\textit{ETTH2}. \n\n"
                },
                "subsection 6.2": {
                    "name": "Supplementary Details",
                    "content": "\nTSMixer follows ~\\cite{lr_sched} to determine the optimal learning rate during training for better convergence. Early stopping with patience 10 is applied during training. TSMixer library is built using PyTorch and multi-GPU node training is enabled via Pytorch DDP\\footnote{$https://pytorch.org/tutorials/beginner/dist\\_overview.html$}. TSMixer can easily scale and cater to the needs of large-scale forecasting and can be deployed across multiple nodes in the Kubernetes clusters.\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Supplementary Figures",
                    "content": "\nThis section covers the supplementary figures that are referred in the paper for better understanding. Figure.~\\ref{fig:gated-attention}(a) depicts the standard MLP block used in the mixer layer. Figure.~\\ref{fig:gated-attention}(b) explains the gated attention block used in the mixer layer to downscale the noisy features. Figure.~\\ref{fig:model-heads} highlights the architecture followed in the prediction and pretrain head. In the self-supervised pre-training workflow, pre-train head is attached to the backbone. In other workflows (such as supervised, or finetuning), the prediction head is attached. \n\nFigure.~\\ref{fig:cluster_overall} shows the correlation between patch time series and its associated embeddings in different datasets.\n% \\begin{figure}[!htb]\n%      \\centering\n%      \\begin{subfigure}[b]{0.49\\columnwidth}\n%          \\centering\n%          \\includegraphics[width=0.8\\columnwidth]{figs/GA-1.pdf}\n%          \\caption{MLP block in \\mixer}\n%          \\label{fig:mlp}\n%      \\end{subfigure}\n%      \\hfill\n%      \\begin{subfigure}[b]{0.49\\columnwidth}\n%          \\centering\n%          \\includegraphics[width=0.8\\columnwidth]{figs/GA-2.pdf}\n%          \\caption{Gated attention in ours}\n%          \\label{fig:ga}\n%      \\end{subfigure}\n%     \\caption{\n%     MLP and Gated Attention \n%     }\n%     \\label{fig:gated-attention}\n% \\end{figure}\n\n\n\n\n\n% \\begin{figure}[!htb]\n%      \\centering\n%      \\begin{subfigure}[b]{0.49\\columnwidth}\n%          \\centering\n%          \\includegraphics[width=0.8\\columnwidth]{figs/MH-1.pdf}\n%          \\caption{Prediction head}\n%          \\label{fig:model-heads:prediction}\n%      \\end{subfigure}\n%      \\hfill\n%      \\begin{subfigure}[b]{0.49\\columnwidth}\n%          \\centering\n%          \\includegraphics[width=0.8\\columnwidth]{figs/MH-2.pdf}\n%          \\caption{Pretrain head}\n%          \\label{fig:model-heads:pretrain}\n%      \\end{subfigure}\n%     \\caption{\n%     Model heads for the two workflows.\n%     }\n%     \\label{fig:model-heads}\n% \\end{figure}\n\n\n\n\n\n% \\begin{figure*}[!htb]\n%      \\centering\n%      \\begin{subfigure}[b]{0.4\\textwidth}\n%          \\centering\n%          \\includegraphics[scale = 0.5]{figs/MH-1.pdf}\n%          \\caption{Prediction head}\n%          \\label{fig:model-heads:prediction}\n%      \\end{subfigure}\n%      \\hfill\n%      \\begin{subfigure}[b]{0.4\\textwidth}\n%          \\centering\n%          \\includegraphics[scale = 0.5]{figs/MH-2.pdf}\n%          \\caption{Pretrain head}\n%          \\label{fig:model-heads:pretrain}\n%      \\end{subfigure}\n%     \\caption{\n%     Model heads for the two workflows.\n%     }\n%     \\label{fig:model-heads}\n% \\end{figure*}\n\n% \\subsection{Implementation Details}\n% ~\\tsm~ library is built using pytorch in a modular fashion. \n\n"
                },
                "subsection 6.4": {
                    "name": "Supplementary results",
                    "content": "\n\nThis section explains the supplementary results which are not reported in the main paper due to space constraints.\n\n",
                    "subsubsection 6.4.1": {
                        "name": "Benchmarking LightTS, S4, CrossFormer and TS2Vec",
                        "content": "\n\\label{appendix_1}\nThis section compares and contrasts TSMixer with LightTS~\\cite{lightts}, S4~\\cite{s4}, CrossFormer~\\cite{crossformer} and TS2Vec~\\cite{ts2vec}. Considering the space constraints and the lower performance of these benchmarks as compared to our reported primary benchmarks (like PatchTST, DLinear), we mention these in the appendix instead of the main paper.\n% We highlight these benchmarks in the appendix, considering their inferior performance to the existing primary benchmarks (like PatchTST, DLinear) reported in our paper. \nTable.~\\ref{tab:lightts} and Table.~\\ref{tab:crossformer} compare and contrast TSMixer with LightTS, S4 and CrossFormer in a supervised workflow. Table.~\\ref{tab:ab_ts2vec} compares and contrasts TSMixer with TS2Vec in the self-supervised workflow. Since the baseline papers reported the results in a different forecast horizon ($fl$) space, we report their comparison in separate tables as per the commonly available forecast horizons.\n\n\n"
                    },
                    "subsubsection 6.4.2": {
                        "name": "Detailed Analysis",
                        "content": "\nIn this section, we provide more detailed benchmarking results as follows:\n\\begin{itemize}\n    \\item Table.~\\ref{tab:supervised-ab1} shows the drill-down view of ~\\tsmb~ on supervised multivariate forecasting.\n    \\item Table.~\\ref{tab:ga_hc-ab1} highlights the effect of CI, Gated Attention and Hierarchy Reconciliation over Vanilla TSMixer on all datasets.\n    \\item Table.~\\ref{tab:cc-detail} depicts the MSE analysis of various channel mixing techniques with different context lengths.\n    \\item Table.~\\ref{tab:ab_fm_1} highlights the MSE drill-down view of ~\\tsmb~ for self-supervised multivariate forecasting.\n\\end{itemize}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% % \\begin{table*}[t]\n% %         \\small\n% % \t\\centering\n% % \t\\resizebox{\\linewidth}{!}{\n% % \t\t\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}\n% % \t\t\t\\hline\n% % \t\t\t&\\multicolumn{2}{c|}{Models}& \\multicolumn{1}{c|}{V-\\tsm}& \\multicolumn{1}{c|}{CI-\\tsm}& \\multicolumn{1}{c|}{IC-\\tsm}& \\multicolumn{6}{c|}{\\citsm(\\ga,\\cc}\\\\\n% % \t\t\t\\hline\n% % \t\t\t&\\multicolumn{2}{c|}{Context Length}&&&&(1)&(2)&(3)&(4)&(5)&(Best)\\\\\n% % \t\t\t\\hline\n        \n% % \t\t\\end{tabular}\n% % \t}\n% % \t\\caption{Detail Analysis of various Channel Mixing techniques with different context lengths.}\n% % \t\\label{tab:ga_hc-ab1}\n% % \\end{table*}\n\n\n\n\n\n\n\n\n\n% \\begin{table*}[t]\n%     % \\setlength{\\tabcolsep}{5pt}\n%     \\centering\n%     \\scalebox{0.9}{\n%     % \\resizebox{\\linewidth}{!}{\n%     \\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}\n%         \\toprule\n        \n%         & FL  &  \\multicolumn{3}{c|}{\\makecell{\\citsm-Best (SS) \\\\ Model Size: Default}}& \\multicolumn{3}{c|}{\\makecell{\\citsm-Best (SS) \\\\ Model Size: Big}}& \\makecell{\\textbf{\\citsm-}\\\\\\textbf{Overall-Best (SS)}} & \\makecell{PatchTST \\\\ (SS)}& \\makecell{\\citsm-\\\\Best} \\\\ \\hline\n\n%         & & SAME & ALL & TL & SAME & ALL & TL & & & Supervised\\\\ \\hline\n%         \\multirow{4}*{\\rotatebox{90}{Electricity}} &96& 0.129 & 0.131 & 0.13 & \\uline{0.127} & 0.128 & 0.129 & \\uline{0.127$\\pm$0.001} & \\textbf{0.126} & 0.129$\\pm$0 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & 0.149 & 0.149 & 0.148 & \\textbf{0.145} & \\uline{0.146} & 0.147 & \\textbf{0.145$\\pm$1e-4} & \\textbf{0.145} & \\uline{0.146$\\pm$0.001} \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & 0.16 & 0.161 & 0.161 & \\textbf{0.156} & \\textbf{0.156} & 0.159 & \\textbf{0.156$\\pm$0.001} & 0.164 & \\uline{0.158$\\pm$0.001} \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & 0.187 & 0.189 & \\uline{0.186} & 0.187 & \\textbf{0.185} & 0.189 & \\textbf{0.185$\\pm$0.002} & 0.193 & \\uline{0.186$\\pm$0.001} \\\\ \n%         \\hline \n%         \\multirow{4}*{\\rotatebox{90}{Traffic}} &96& 0.359 & 0.368 & 0.367 & \\uline{0.35} & \\textbf{0.348} & \\uline{0.35} & \\textbf{0.348$\\pm$0.001} & 0.352 & 0.356$\\pm$0.002 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & 0.38 & 0.387 & 0.384 & 0.372 & \\textbf{0.37} & 0.372 & \\textbf{0.37$\\pm$0.001} & \\uline{0.371} & 0.377$\\pm$0.003 \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & 0.392 & 0.395 & 0.394 & \\textbf{0.379} & \\uline{0.38} & \\textbf{0.379} & \\textbf{0.379$\\pm$0.003} & 0.381 & 0.385$\\pm$0.002 \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & 0.428 & 0.43 & 0.431 & \\textbf{0.42} & \\uline{0.421} & 0.422 & \\textbf{0.42$\\pm$0.001} & 0.425 & 0.424$\\pm$0.001 \\\\ \n%         \\hline \n%         \\multirow{4}*{\\rotatebox{90}{Weather}} &96& \\uline{0.144} & 0.145 & 0.147 & \\uline{0.144} & \\textbf{0.143} & \\uline{0.144} & \\textbf{0.143$\\pm$1e-4} & \\uline{0.144} & 0.146$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 192 & \\uline{0.19} & 0.191 & 0.194 & \\textbf{0.189} & \\textbf{0.189} & \\uline{0.19} & \\textbf{0.189$\\pm$1e-4} & \\uline{0.19} & 0.191$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 336 & 0.241 & 0.242 & 0.266 & \\uline{0.24} & \\textbf{0.239} & 0.241 & \\textbf{0.239$\\pm$0.001} & 0.244 & 0.243$\\pm$0.001 \\\\ \n%         \\multicolumn{1}{c|}{} & 720 & 0.319 & 0.328 & 0.321 & \\textbf{0.311} & \\uline{0.316} & 0.319 & \\textbf{0.311$\\pm$0.001} & 0.32 & \\uline{0.316$\\pm$0.001} \\\\ \n%         \\hline \n%         \\multicolumn{9}{c|}{\\textbf{\\citsm-Overall-Best (SS) \\% Improvement}} & \\textbf{1.45\\%}  & \\textbf{1.41\\%}  \\\\\n%         \\hline \n%     \\end{tabular}\n%     }\n%     \\caption{Masked Self-Supervised(SS) MSE benchmarking with varying pretrain data strategies and model sizes}\n%     \\label{tab:fm-2-all-ablation}\n% \\end{table*}\n% \\section{Online Resources}\n% \\input{appendix}\n\n\n\n"
                    }
                }
            }
        },
        "tables": {
            "table:dataset": "\\begin{table}[htbp]\n\\centering\n\\setlength{\\tabcolsep}{2.5pt}\n\\scalebox{0.85}{\n\\begin{tabular}{c|ccccccc}\n\\toprule  \nDatasets & Weather  & Traffic & Electricity & ETTH1 & ETTH2 & ETTM1 & ETTM2 \\\\\n\\midrule  \nFeatures & 21 & 862 & 321 & 7 & 7 & 7 & 7 \\\\\nTimesteps & 52696 & 17544 & 26304 & 17420 & 17420 & 69680 & 69680 \\\\\n\\bottomrule \n\\end{tabular}}\n\\caption{Statistics of popular datasets for benchmark.}\n\\label{table:dataset}\n\\end{table}",
            "tab:supervised": "\\begin{table*}[htb]\n        % \\small\n\t\\centering\n        % \\captionsetup{width=.9\\linewidth}\n\t% \\resizebox{\\linewidth}{!}{\n        \\scalebox{0.9}{\n\t\t  \\begin{tabular}{cc|c|cc|cc|cc|cc|cc|ccc}\n            \\cline{2-15}\n\t\t\t&\\multicolumn{2}{c|}{Models} & \\multicolumn{2}{c}{\\textbf{\\citsm-Best}} & \\multicolumn{2}{c|}{DLinear} & \\multicolumn{2}{c|}{PatchTST}& \\multicolumn{2}{c|}{FEDformer}& \\multicolumn{2}{c|}{Autoformer}& \\multicolumn{2}{c}{Informer} \\\\\n\t\t\t\\cline{2-15}\n\t\t\t&\\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\\\\n\t\t\t\\cline{2-15}\n            &\\multirow{4}*{\\rotatebox{90}{ETTH1}} & 96 & \\textbf{0.368$\\pm$0.001} & \\textbf{0.398$\\pm$0.001} & 0.375 & \\uline{0.399} & \\uline{0.370} & 0.400 & 0.376 & 0.419 & 0.449 & 0.459 & 0.865 & 0.713 \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\textbf{0.399$\\pm$0.002} & \\uline{0.418$\\pm$0.001} & \\uline{0.405} & \\textbf{0.416} & 0.413 & 0.429 & 0.420 & 0.448 & 0.500 & 0.482 & 1.008 & 0.792 \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\textbf{0.421$\\pm$0.004} & \\textbf{0.436$\\pm$0.003} & 0.439 & 0.443 & \\uline{0.422} & \\uline{0.440} & 0.459 & 0.465 & 0.521 & 0.496 & 1.107 & 0.809 \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\textbf{0.444$\\pm$0.003} & \\textbf{0.467$\\pm$0.002} & 0.472 & 0.490 & \\uline{0.447} & \\uline{0.468} & 0.506 & 0.507 & 0.514 & 0.512 & 1.181 & 0.865 \\\\ \n            \\cline{2-15} \n            &\\multirow{4}*{\\rotatebox{90}{ETTH2}} & 96 & \\uline{0.276$\\pm$0.006} & \\textbf{0.337$\\pm$0.003} & 0.289 & \\uline{0.353} & \\textbf{0.274} & \\textbf{0.337} & 0.346 & 0.388 & 0.358 & 0.397 & 3.755 & 1.525 \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\textbf{0.330$\\pm$0.003} & \\textbf{0.374$\\pm$0.001} & 0.383 & 0.418 & \\uline{0.341} & \\uline{0.382} & 0.429 & 0.439 & 0.456 & 0.452 & 5.602 & 1.931 \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\uline{0.357$\\pm$0.001} & \\uline{0.401$\\pm$0.002} & 0.448 & 0.465 & \\textbf{0.329} & \\textbf{0.384} & 0.496 & 0.487 & 0.482 & 0.486 & 4.721 & 1.835 \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\uline{0.395$\\pm$0.003} & \\uline{0.436$\\pm$0.003} & 0.605 & 0.551 & \\textbf{0.379} & \\textbf{0.422} & 0.463 & 0.474 & 0.515 & 0.511 & 3.647 & 1.625 \\\\ \n            \\cline{2-15} \n            &\\multirow{4}*{\\rotatebox{90}{ETTM1}} & 96 & \\textbf{0.291$\\pm$0.002} & \\uline{0.346$\\pm$0.002} & 0.299 & \\textbf{0.343} & \\uline{0.293} & \\uline{0.346} & 0.379 & 0.419 & 0.505 & 0.475 & 0.672 & 0.571 \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\textbf{0.333$\\pm$0.002} & \\uline{0.369$\\pm$0.002} & \\uline{0.335} & \\textbf{0.365} & \\textbf{0.333} & 0.370 & 0.426 & 0.441 & 0.553 & 0.496 & 0.795 & 0.669 \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\textbf{0.365$\\pm$0.005} & \\textbf{0.385$\\pm$0.004} & \\uline{0.369} & \\uline{0.386} & \\uline{0.369} & 0.392 & 0.445 & 0.459 & 0.621 & 0.537 & 1.212 & 0.871 \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\textbf{0.416$\\pm$0.002} & \\textbf{0.413$\\pm$0.001} & \\uline{0.425} & 0.421 & \\textbf{0.416} & \\uline{0.420} & 0.543 & 0.490 & 0.671 & 0.561 & 1.166 & 0.823 \\\\ \n            \\cline{2-15} \n            &\\multirow{4}*{\\rotatebox{90}{ETTM2}} & 96 & \\textbf{0.164$\\pm$0.002} & \\textbf{0.255$\\pm$0.002} & 0.167 & 0.260 & \\uline{0.166} & \\uline{0.256} & 0.203 & 0.287 & 0.255 & 0.339 & 0.365 & 0.453 \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\textbf{0.219$\\pm$0.002} & \\textbf{0.293$\\pm$0.002} & 0.224 & 0.303 & \\uline{0.223} & \\uline{0.296} & 0.269 & 0.328 & 0.281 & 0.340 & 0.533 & 0.563 \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\textbf{0.273$\\pm$0.003} & \\textbf{0.329$\\pm$0.003} & 0.281 & \\uline{0.342} & \\uline{0.274} & \\textbf{0.329} & 0.325 & 0.366 & 0.339 & 0.372 & 1.363 & 0.887 \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\textbf{0.358$\\pm$0.002} & \\textbf{0.380$\\pm$0.001} & 0.397 & 0.421 & \\uline{0.362} & \\uline{0.385} & 0.421 & 0.415 & 0.433 & 0.432 & 3.379 & 1.338 \\\\ \n            \\cline{2-15} \n            &\\multirow{4}*{\\rotatebox{90}{Electricity}} & 96 & \\textbf{0.129$\\pm$1e-4} & \\uline{0.224$\\pm$0.001} & \\uline{0.140} & 0.237 & \\textbf{0.129} & \\textbf{0.222} & 0.193 & 0.308 & 0.201 & 0.317 & 0.274 & 0.368 \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\textbf{0.146$\\pm$0.001} & \\uline{0.242$\\pm$1e-4} & 0.153 & 0.249 & \\uline{0.147} & \\textbf{0.240} & 0.201 & 0.315 & 0.222 & 0.334 & 0.296 & 0.386 \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\textbf{0.158$\\pm$0.001} & \\textbf{0.256$\\pm$0.001} & 0.169 & 0.267 & \\uline{0.163} & \\uline{0.259} & 0.214 & 0.329 & 0.231 & 0.338 & 0.300 & 0.394 \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\textbf{0.186$\\pm$0.001} & \\textbf{0.282$\\pm$0.001} & 0.203 & 0.301 & \\uline{0.197} & \\uline{0.290} & 0.246 & 0.355 & 0.254 & 0.361 & 0.373 & 0.439 \\\\ \n            \\cline{2-15} \n            &\\multirow{4}*{\\rotatebox{90}{Traffic}} & 96 & \\textbf{0.356$\\pm$0.002} & \\textbf{0.248$\\pm$0.002} & 0.410 & 0.282 & \\uline{0.360} & \\uline{0.249} & 0.587 & 0.366 & 0.613 & 0.388 & 0.719 & 0.391 \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\textbf{0.377$\\pm$0.003} & \\uline{0.257$\\pm$0.002} & 0.423 & 0.287 & \\uline{0.379} & \\textbf{0.256} & 0.604 & 0.373 & 0.616 & 0.382 & 0.696 & 0.379 \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\textbf{0.385$\\pm$0.002} & \\textbf{0.262$\\pm$0.001} & 0.436 & 0.296 & \\uline{0.392} & \\uline{0.264} & 0.621 & 0.383 & 0.622 & 0.337 & 0.777 & 0.420 \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\textbf{0.424$\\pm$0.001} & \\textbf{0.283$\\pm$0.001} & 0.466 & 0.315 & \\uline{0.432} & \\uline{0.286} & 0.626 & 0.382 & 0.660 & 0.408 & 0.864 & 0.472 \\\\ \n            \\cline{2-15} \n            &\\multirow{4}*{\\rotatebox{90}{Weather}} & 96 & \\textbf{0.146$\\pm$0.001} & \\textbf{0.197$\\pm$0.002} & 0.176 & 0.237 & \\uline{0.149} & \\uline{0.198} & 0.217 & 0.296 & 0.266 & 0.336 & 0.300 & 0.384 \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\textbf{0.191$\\pm$0.001} & \\textbf{0.240$\\pm$0.001} & 0.220 & 0.282 & \\uline{0.194} & \\uline{0.241} & 0.276 & 0.336 & 0.307 & 0.367 & 0.598 & 0.544 \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\textbf{0.243$\\pm$0.001} & \\textbf{0.279$\\pm$0.002} & 0.265 & 0.319 & \\uline{0.245} & \\uline{0.282} & 0.339 & 0.380 & 0.359 & 0.395 & 0.578 & 0.523 \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\uline{0.316$\\pm$0.001} & \\textbf{0.333$\\pm$0.002} & 0.323 & 0.362 & \\textbf{0.314} & \\uline{0.334} & 0.403 & 0.428 & 0.419 & 0.428 & 1.059 & 0.741 \\\\ \n            \\cline{2-15} \n            % &\\multicolumn{4}{c|}{\\makecell{\\textbf{\\citsm-Best} \\textbf{\\% improvement}}}& \\textbf{8\\%} & \\textbf{6.8\\%}& \\textbf{0.7\\%} & \\textbf{0.4\\%} & \\textbf{22.9\\%} & \\textbf{18.2\\%} & \\textbf{30.1\\%} & \\textbf{22.7\\%} & \\textbf{64\\%} & \\textbf{50.3\\%} \\\\\n            % &\\multicolumn{4}{c|}{\\makecell{\\textbf{\\citsm-Best} \\textbf{\\% improvement (MSE)}}}& \\multicolumn{2}{c}{\\textbf{8\\%}} & \\multicolumn{2}{c}{ \\textbf{0.7\\%}}  & \\multicolumn{2}{c}{\\textbf{22.9\\%}}  & \\multicolumn{2}{c}{\\textbf{30.1\\%}}  & \\multicolumn{2}{c}{\\textbf{64\\%}}  \\\\\n            &\\multicolumn{4}{c|}{\\makecell{\\textbf{\\citsm-Best} \\textbf{\\% improvement (MSE)}}}& \\multicolumn{2}{c}{\\textbf{8\\%}} & \\multicolumn{2}{c}{ \\textbf{1\\%}}  & \\multicolumn{2}{c}{\\textbf{23\\%}}  & \\multicolumn{2}{c}{\\textbf{30\\%}}  & \\multicolumn{2}{c}{\\textbf{64\\%}}  \\\\\n            \\cline{2-15} \n\t\t\\end{tabular}\n\t}\n\t\\caption{Comparing TSMixer with popular benchmarks in supervised long-term multivariate forecasting. The best results are in bold and the second best is underlined. PatchTST results are reported from ~\\cite{patchtst}. All other benchmarks are reported from ~\\cite{dlinear}}\n\t\\label{tab:supervised}\n\\end{table*}",
            "tab:speedup": "\\begin{table}\n\\centering\n\\setlength{\\tabcolsep}{1.5pt}\n\\scalebox{0.9}{\n\\begin{tabular}{c|c|c|c|c|c|c|c|c}\n\\hline  \nMetric & Data & \\makecell{Patch\\\\TST} & \\multicolumn{2}{c|}{\\makecell{\\citsm\\\\(Avg. Imp)}} & \\multicolumn{2}{c|}{\\makecell{\\citsm\\\\(\\ga,\\hr)\\\\(Avg. Imp)}} & \\multicolumn{2}{c}{\\makecell{\\citsm\\\\(\\ga,\\hr,\\cc)\\\\(Avg. Imp)}}  \\\\\n\\hline\n\\multirow{3}{*}{\\makecell{MACs\\\\(T)}}  & Electricity & 147.879 & 37.062 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(4X)}}} & 46.44 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(3.2X)}}} & 48.566 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(3X)}}} \\\\ \n& Traffic & 260.367 & 65.25 &\\multicolumn{1}{c|}{} & 81.77 &\\multicolumn{1}{c|}{} & 91.78 &\\multicolumn{1}{c}{} \\\\ \n& Weather & 19.709 & 4.94 &\\multicolumn{1}{c|}{} & 6.19 &\\multicolumn{1}{c|}{} & 6.21 &\\multicolumn{1}{c}{} \\\\ \\hline\n\n\\multirow{3}{*}{\\makecell{NPARAMS \\\\ (M)}}   & Electricity & 1.174 & 0.348 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(3.4X)}}} & 0.4 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(2.9X)}}} & 1.648 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(1.2X)}}} \\\\ \n & Traffic & 1.174 & 0.348 &\\multicolumn{1}{c|}{} & 0.4 &\\multicolumn{1}{c|}{} & 9.33 &\\multicolumn{1}{c}{} \\\\ \n & Weather & 1.174 & 0.348 &\\multicolumn{1}{c|}{} & 0.4 &\\multicolumn{1}{c|}{} & 0.405 &\\multicolumn{1}{c}{} \\\\ \\hline\n\n\\multirow{3}{*}{\\makecell{EPOCH \\\\ TIME \\\\(min)}}  & Electricity & 36.22 & 15.56  &\\multirow{3}*{\\rotatebox{90}{\\textbf{(2X)}}} & 20.43 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(1.6X)}}} & 20.5 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(1.5X)}}} \\\\ \n & Traffic & 64.04 & 27.49 &\\multicolumn{1}{c|}{} & 36.08 &\\multicolumn{1}{c|}{} & 36.51 &\\multicolumn{1}{c}{} \\\\ \n & Weather & 5.2 & 3.08 &\\multicolumn{1}{c|}{} & 4.13 &\\multicolumn{1}{c|}{} & 4.17 &\\multicolumn{1}{c}{} \\\\ \\hline\n \n\\multirow{3}{*}{\\makecell{MAX  \\\\ MEMORY \\\\ (GB)}}  & Electricity & 6.14 & 2.25 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(2.7X)}}} & 2.9 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(2.1X)}}} & 2.94 &\\multirow{3}*{\\rotatebox{90}{\\textbf{(2X)}}} \\\\ \n& Traffic & 8.24 & 3.03 &\\multicolumn{1}{c|}{} & 3.89 &\\multicolumn{1}{c|}{} & 4.15 &\\multicolumn{1}{c}{} \\\\ \n& Weather & 0.451 & 0.165 &\\multicolumn{1}{c|}{} & 0.21 &\\multicolumn{1}{c|}{} & 0.211 &\\multicolumn{1}{c}{} \\\\ \\hline\n\\end{tabular}}\n\\caption{Computational Improvement. \\textbf{nX} denotes \\textbf{n} times average improvement across datasets (Avg. Imp).}\n\\label{tab:speedup}\n\\end{table}",
            "tab:ci_ga_hr": "\\begin{table}\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\scalebox{0.9}{\n\\begin{tabular}{c|c|c|c|c|c|c}\n\\hline  \n& \\makecell{FL} & \\makecell{V-\\\\\\tsm} & \\makecell{CI-\\\\\\tsm} & \\makecell{\\citsm\\\\(\\ga)} & \\makecell{\\citsm\\\\(\\hr)} & \\makecell{\\citsm\\\\(\\ga,\\hr)}  \\\\\n\\hline\n\\multirow{4}{*}{\\rotatebox{90}{ETTH1}} & 96 & 0.449 & 0.375 & 0.375 & 0.377 & \\textbf{0.368} \\\\ \n& 192 & 0.485 & 0.411 & 0.408 & 0.410 & \\textbf{0.399} \\\\ \n& 336 & 0.504 & 0.437 & 0.433 & 0.431 & \\textbf{0.421} \\\\ \n& 720 & 0.573 & 0.465 & 0.454 & 0.457 & \\textbf{0.444} \\\\ \\hline\n\n\\multirow{4}{*}{\\rotatebox{90}{ETTM1}} & 96 & 0.328 & 0.305 & \\textbf{0.296} & 0.301 & 0.297 \\\\ \n& 192 & 0.372 & 0.336 & 0.334 & 0.338 & \\textbf{0.333} \\\\ \n& 336 & 0.405 & 0.375 & 0.366 & 0.376 & \\textbf{0.365} \\\\ \n& 720 & 0.457 & 0.425 & 0.419 & 0.422 & \\textbf{0.416} \\\\ \\hline\n\n\\multirow{4}{*}{\\rotatebox{90}{Weather}}  & 96 & 0.159 & 0.150 & 0.149 & 0.151 & \\textbf{0.148} \\\\ \n& 192 & 0.207 & 0.195 & 0.195 & 0.195 & \\textbf{0.193} \\\\ \n& 336 & 0.256 & 0.246 & 0.246 & 0.246 & \\textbf{0.243} \\\\ \n& 720 & 0.330 & 0.323 & 0.317 & 0.321 & \\textbf{0.317} \\\\ \\hline\n\\multicolumn{3}{c|}{ \\makecell{\\textbf{\\% improvement} \\\\ \\textbf{over \\vtsm}}} & \\textbf{9.5\\%}\t& \\textbf{10.5\\%} & \\textbf{10\\%} & \\textbf{11.5\\%} \\\\ \\hline\n% &\\multicolumn{3}{c|}{\\makecell{\\textbf{\\citsm-Best} & \\textbf{9.372}}\t& \\textbf{10.421} & \\textbf{9.656} & \\textbf{11.352} \\\\ \\hline\n\\end{tabular}}\n\\caption{Effect of CI, Gated Attention and Hierarchy Reconciliation over Vanilla~\\tsm~(MSE).}\n\\label{tab:ci_ga_hr}\n\\end{table}",
            "tab:channel_table": "\\begin{table}\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\scalebox{.9}{\n\\begin{tabular}{c|c|c|c|c|c}\n\\hline  \n& \\makecell{FL} & \\makecell{V-\\tsm} & \\makecell{CI-\\tsm} & \\makecell{IC-\\tsm} & \\makecell{\\citsm\\\\(\\ga,\\cc-Best)}  \\\\\n\\hline\n\\multirow{4}{*}{\\rotatebox{90}{ETTH1}} & 96 & 0.449 & 0.375 & 0.379 & \\textbf{0.373} \\\\ \n    & 192 & 0.485 & 0.411 & 0.416  & \\textbf{0.407} \\\\ \n    & 336 & 0.504 & 0.437 & 0.437  & \\textbf{0.43} \\\\ \n     & 720 & 0.573 & 0.465 & 0.471  & \\textbf{0.454} \\\\  \\hline\n\\multirow{4}{*}{\\rotatebox{90}{ETTH2}} & 96 & 0.369 & 0.284 & 0.291  & \\textbf{0.269} \\\\\n     & 192 & 0.391 & 0.353 & 0.345  & \\textbf{0.330} \\\\ \n     & 336 & 0.403 & 0.365 & 0.361  & \\textbf{0.359} \\\\ \n     & 720 & 0.475 & 0.406 & 0.419  & \\textbf{0.393 }\\\\  \\hline\n\\multirow{4}{*}{\\rotatebox{90}{Weather}} & 96 & 0.159 & 0.150 & 0.150  & \\textbf{0.146} \\\\\n     & 192 & 0.207 & 0.195 & 0.196  & \\textbf{0.194} \\\\ \n     & 336 & 0.256 & \\textbf{0.246} & 0.248  & \\textbf{0.246} \\\\ \n     & 720 & 0.330 & 0.323 & 0.338 & \\textbf{0.317} \\\\  \\hline\n\n\n\\multicolumn{3}{c|}{ \\makecell{\\textbf{\\% improvement} \\\\ \\textbf{over \\vtsm}}} & \\textbf{11.5\\%}\t& \\textbf{10.7\\%}  & \\textbf{13.5\\%} \\\\ \\hline\n% &\\multicolumn{3}{c|}{\\makecell{\\textbf{\\citsm-Best} & \\textbf{9.372}}\t& \\textbf{10.421} & \\textbf{9.656} & \\textbf{11.352} \\\\ \\hline\n\\end{tabular}}\n\\caption{Channel mixing technique comparison (MSE).}\n\\label{tab:channel_table}\n\\end{table}",
            "tab:fm_1": "\\begin{table}\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\scalebox{0.9}{\n\\begin{tabular}{c|c|c|c|c|c|c|c}\n\\hline  \n& \\makecell{FL} & \\makecell{CI-\\tsm\\\\-Best} & \\makecell{PatchTST} & \\makecell{BTSF} & \\makecell{TNC} & \\makecell{TS-TCC} & \\makecell{CPC}  \\\\\n\\hline\n\\multirow{5}*{\\rotatebox{90}{ETTH1}} & 24 & \\textbf{0.314} & \\uline{0.322} & 0.541 & 0.632 & 0.653 & 0.687 \\\\ \n\\multicolumn{1}{c|}{} & 48 & \\textbf{0.343} & \\uline{0.354} & 0.613 & 0.705 & 0.720 & 0.779 \\\\ \n\\multicolumn{1}{c|}{} & 168& \\textbf{0.397} & \\uline{0.419} & 0.640 & 1.097 & 1.129 & 1.282 \\\\ \n\\multicolumn{1}{c|}{} & 336 & \\textbf{0.424} & \\uline{0.445} & 0.864 & 1.454 & 1.492 & 1.641 \\\\ \n\\multicolumn{1}{c|}{} & 720 & \\textbf{0.453} & \\uline{0.487} & 0.993 & 1.604 & 1.603 & 1.803 \\\\ \n\\hline \n\n\\multirow{5}*{\\rotatebox{90}{Weather}} & 24 & \\uline{0.088} & \\textbf{0.087} & 0.324 & 0.484 & 0.572 & 0.647 \\\\ \n\\multicolumn{1}{c|}{} & 48 & \\uline{0.114} & \\textbf{0.113} & 0.366 & 0.608 & 0.647 & 0.720 \\\\ \n\\multicolumn{1}{c|}{} & 168& \\textbf{0.177} & \\uline{0.178} & 0.543 & 1.081 & 1.117 & 1.351 \\\\ \n\\multicolumn{1}{c|}{} & 336 & \\textbf{0.241} & \\uline{0.244} & 0.568 & 1.654 & 1.783 & 2.019 \\\\ \n\\multicolumn{1}{c|}{} & 720 & \\textbf{0.319} & \\uline{0.321} & 0.601 & 1.401 & 1.850 & 2.109 \\\\ \n\\hline \n\n\\multicolumn{3}{c|}{ \\makecell{\\textbf{\\citsm-Best} \\\\ \\textbf{\\% improvement}}} & \\textbf{2.3\\%}\t& \\textbf{54.2\\%} & \\textbf{71.7\\%} & \\textbf{73.3\\%} & \\textbf{75.8\\%} \\\\ \\hline\n% &\\multicolumn{3}{c|}{\\makecell{\\textbf{\\citsm-Best} & \\textbf{9.372}}\t& \\textbf{10.421} & \\textbf{9.656} & \\textbf{11.352} \\\\ \\hline\n\\end{tabular}}\n\\caption{Forecasting via Representation Learning (MSE)}\n\\label{tab:fm_1}\n\\end{table}",
            "tab:fm-2-all": "\\begin{table}[t]\n    % \\setlength{\\tabcolsep}{5pt}\n    \\setlength{\\tabcolsep}{2pt}\n    \\centering\n    \\scalebox{0.9}{\n    % \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n        \\toprule\n        \n        & FL  & \\multicolumn{3}{c|}{\\makecell{\\citsm-Best (SS) \\\\ Model Size: Big}}& \\makecell{\\textbf{\\citsm-}\\\\\\textbf{Overall}\\\\\\textbf{-Best (SS)}} & \\makecell{PatchTST \\\\ (SS)}& \\makecell{\\citsm-\\\\Best} \\\\ \\hline\n\n        & & SAME & ALL & TL & & & Supervised\\\\ \\hline\n        \\multirow{4}*{\\rotatebox{90}{Electricity}} &96 & \\uline{0.127} & 0.128 & 0.129 & \\uline{0.127} & \\textbf{0.126} & 0.129 \\\\ \n        \\multicolumn{1}{c|}{} & 192  & \\textbf{0.145} & \\uline{0.146} & 0.147 & \\textbf{0.145} & \\textbf{0.145} & \\uline{0.146} \\\\ \n        \\multicolumn{1}{c|}{} & 336 & \\textbf{0.156} & \\textbf{0.156} & 0.159 & \\textbf{0.156} & 0.164 & \\uline{0.158} \\\\ \n        \\multicolumn{1}{c|}{} & 720 & 0.187 & \\textbf{0.185} & 0.189 & \\textbf{0.185} & 0.193 & \\uline{0.186} \\\\ \n        \\hline \n        \\multirow{4}*{\\rotatebox{90}{Traffic}} &96 & \\uline{0.350} & \\textbf{0.348} & \\uline{0.350} & \\textbf{0.348} & 0.352 & 0.356 \\\\ \n        \\multicolumn{1}{c|}{} & 192  & 0.372 & \\textbf{0.370} & 0.372 & \\textbf{0.370} & \\uline{0.371} & 0.377 \\\\ \n        \\multicolumn{1}{c|}{} & 336  & \\textbf{0.379} & \\uline{0.380} & \\textbf{0.379} & \\textbf{0.379} & 0.381 & 0.385 \\\\ \n        \\multicolumn{1}{c|}{} & 720 & \\textbf{0.420} & \\uline{0.421} & 0.422 & \\textbf{0.420} & 0.425 & 0.424 \\\\ \n        \\hline \n        \\multirow{4}*{\\rotatebox{90}{Weather}} &96 & \\uline{0.144} & \\textbf{0.143} & \\uline{0.144} & \\textbf{0.143} & \\uline{0.144} & 0.146 \\\\ \n        \\multicolumn{1}{c|}{} & 192 & \\textbf{0.189} & \\textbf{0.189} & \\uline{0.190} & \\textbf{0.189} & \\uline{0.190} & 0.191 \\\\ \n        \\multicolumn{1}{c|}{} & 336 & \\uline{0.240} & \\textbf{0.239} & 0.241 & \\textbf{0.239} & 0.244 & 0.243 \\\\ \n        \\multicolumn{1}{c|}{} & 720 & \\textbf{0.311} & \\uline{0.316} & 0.319 & \\textbf{0.311} & 0.321 & \\uline{0.316} \\\\ \n        \\hline \n        \\multicolumn{6}{c|}{\\makecell{\\textbf{\\citsm-Overall-Best (SS)} \\\\\\textbf{\\% Improvement (MSE)}}} & \\textbf{1.45\\%}  & \\textbf{1.41\\%}  \\\\\n        \\hline \n    \\end{tabular}\n    }\n    \\caption{Self-Supervision(SS) with varying pretrain strategies (MSE). PatchTST (SS) results are reported from ~\\cite{patchtst}(SS finetuning mode)}\n    \\label{tab:fm-2-all}\n\\end{table}",
            "tab:lightts": "\\begin{table}\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\scalebox{0.9}{\n\\begin{tabular}{c|c|c|c|c}\n\\hline  \nData & \\makecell{FL} & \\makecell{\\textbf{\\citsm-Best}}  & LightTS & S4  \\\\\n\\hline\n\\multirow{2}{*}{ETTH1} & 336 & \\textbf{0.421} & 0.466 & 1.407  \\\\ \n& 720 & \\textbf{0.444}  & 0.542 & 1.162 \\\\ \\hline\n\\multirow{2}{*}{ETTH2} & 336 & \\textbf{0.357} & 0.497 & 1.98  \\\\ \n& 720 & \\textbf{0.395} & 0.739 & 2.65  \\\\ \\hline\n\\multirow{2}{*}{Weather} & 336 & \\textbf{0.243} & 0.527 & 0.531  \\\\ \n& 720 & \\textbf{0.316} & 0.554 & 0.578  \\\\ \\hline\n\\multicolumn{3}{c|}{\\makecell{\\textbf{\\citsm-Best \\% improvement }}}& \\textbf{33.2\\%} & \\textbf{66.4\\%} \\\\ \\hline\n\\end{tabular}}\n\\caption{LightTS and S4 MSE Benchmarking - Baseline results reported from ~\\cite{lightts} and ~\\cite{s4}} \n\\label{tab:lightts}\n\\end{table}",
            "tab:crossformer": "\\begin{table}\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\scalebox{0.9}{\n\\begin{tabular}{c|c|c|c|c}\n\\hline  \nData & \\makecell{FL} & CrossFormer & \\makecell{\\citsm} & \\makecell{\\textbf{\\citsm-Best}}  \\\\\n\\hline\n\\multirow{2}*{{ETTH1}} & 336 & 0.44 & \\uline{0.437} & \\textbf{0.421} \\\\ \n\\multicolumn{1}{c|}{} & 720 & 0.519 & \\uline{0.465} & \\textbf{0.444} \\\\ \\hline\n\\multirow{2}*{{Electricity}} & 336 & 0.323 & \\uline{0.165} & \\textbf{0.158} \\\\ \n\\multicolumn{1}{c|}{} & 720 & 0.404 & \\uline{0.204} & \\textbf{0.186} \\\\ \n\\hline \n\\multirow{2}*{{Traffic}} & 336 & 0.53 & \\uline{0.388} & \\textbf{0.385} \\\\ \n\\multicolumn{1}{c|}{} & 720 & 0.573 & \\uline{0.426} & \\textbf{0.424} \\\\ \\hline\n\\multirow{2}*{{Weather}} & 336 & 0.495 & \\uline{0.246} & \\textbf{0.243} \\\\ \n\\multicolumn{1}{c|}{} & 720 & 0.526 & \\uline{0.323} & \\textbf{0.316} \\\\ \n\\hline \n\\multicolumn{3}{c|}{\\makecell{\\textbf{\\% Improvement} \\\\ \\textbf{over CrossFormer}}}& \\textbf{31.35\\%} & \\textbf{33.50\\%} \\\\ \\hline\n\\end{tabular}}\n\\caption{CrossFormer MSE Benchmarking - Baseline results reported from ~\\cite{crossformer}}\n\\label{tab:crossformer}\n\\end{table}",
            "tab:ab_ts2vec": "\\begin{table*}\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\scalebox{0.9}{\n\\begin{tabular}{c|c|c|c}\n\\hline  \n& \\makecell{FL} & \\makecell{CI-\\tsm\\\\-Best} & \\makecell{TS2Vec} \\\\\n\\hline\n\\multirow{5}*{ETTH1} & 24 & \\textbf{0.314} & {0.599} \\\\ \n\\multicolumn{1}{c|}{} & 48 & \\textbf{0.343} & {0.629} \\\\ \n\\multicolumn{1}{c|}{} & 168& \\textbf{0.397} & {0.755} \\\\ \n\\multicolumn{1}{c|}{} & 336 & \\textbf{0.424} & {0.907} \\\\ \n\\multicolumn{1}{c|}{} & 720 & \\textbf{0.453} & {1.048} \\\\ \n\\hline \n\\multirow{2}*{Electricity} & 336 & \\textbf{0.16} & {0.349} \\\\ \n\\multicolumn{1}{c|}{} & 720 & \\textbf{0.187} & {0.375} \\\\ \\hline\n&\\multicolumn{2}{c|}{\\makecell{\\textbf{\\% Improvement}}}\t& \\textbf{50.7\\%} \\\\ \\hline\n\n\\end{tabular}}\n\\caption{TS2Vec MSE Benchmarking - Baseline results reported from ~\\cite{ts2vec}}\n\\label{tab:ab_ts2vec}\n\\end{table*}",
            "tab:supervised-ab1": "\\begin{table*}[h!]\n        % \\small\n\t\\centering\n        \\captionsetup{width=.8\\linewidth}\n\t% \\resizebox{\\linewidth}{!}{\n        \\scalebox{0.9}{\n\t\t\\begin{tabular}{cc|c|cc|cc|cc|cc|ccc}\n\t\t\t\\cline{2-13}\n\t\t\t&\\multicolumn{2}{c|}{Models}& \\multicolumn{2}{c|}{PatchTST}& \\multicolumn{2}{c|}{DLinear}& \\multicolumn{2}{c|}{\\citsm(\\ga,\\hr)}& \\multicolumn{2}{c|}{\\citsm(\\ga,\\hr,\\cc)}& \\multicolumn{2}{c}{\\citsm-Best} \\\\\n\t\t\t\\cline{2-13}\n\t\t\t&\\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\\\\n\t\t\t\\cline{2-13}\n        \n            &\\multirow{4}*{\\rotatebox{90}{ETTH1}} & 96 & \\uline{0.37} & 0.4 & 0.375 & \\uline{0.399} & \\textbf{0.368} & \\textbf{0.398} & \\textbf{0.368} & \\textbf{0.398} & \\textbf{0.368} & \\textbf{0.398} \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & 0.413 & 0.429 & 0.405 & \\textbf{0.416} & \\textbf{0.399} & \\uline{0.418} & \\uline{0.4} & \\uline{0.418} & \\textbf{0.399} & \\uline{0.418} \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\uline{0.422} & 0.44 & 0.439 & 0.443 & \\textbf{0.421} & \\textbf{0.436} & \\uline{0.422} & \\uline{0.437} & \\textbf{0.421} & \\textbf{0.436} \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\uline{0.447} & \\uline{0.468} & 0.472 & 0.49 & \\textbf{0.444} & \\textbf{0.467} & 0.45 & \\textbf{0.467} & \\textbf{0.444} & \\textbf{0.467} \\\\ \n            \\cline{2-13} \n            &\\multirow{4}*{\\rotatebox{90}{ETTH2}} & 96 & \\textbf{0.274} & \\textbf{0.337} & 0.289 & 0.353 & \\uline{0.276} & \\textbf{0.337} & \\uline{0.276} & \\uline{0.339} & \\uline{0.276} & \\textbf{0.337} \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & 0.341 & 0.382 & 0.383 & 0.418 & \\uline{0.335} & \\uline{0.377} & \\textbf{0.33} & \\textbf{0.374} & \\textbf{0.33} & \\textbf{0.374} \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\textbf{0.329} & \\textbf{0.384} & 0.448 & 0.465 & 0.369 & 0.406 & \\uline{0.357} & \\uline{0.401} & \\uline{0.357} & \\uline{0.401} \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\textbf{0.379} & \\textbf{0.422} & 0.605 & 0.551 & 0.409 & 0.447 & \\uline{0.395} & \\uline{0.436} & \\uline{0.395} & \\uline{0.436} \\\\ \n            \\cline{2-13} \n            &\\multirow{4}*{\\rotatebox{90}{ETTM1}} & 96 & \\uline{0.293} & \\uline{0.346} & 0.299 & \\textbf{0.343} & 0.297 & 0.348 & \\textbf{0.291} & \\uline{0.346} & \\textbf{0.291} & \\uline{0.346} \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\textbf{0.333} & 0.37 & 0.335 & \\textbf{0.365} & \\textbf{0.333} & \\uline{0.369} & \\uline{0.334} & \\uline{0.369} & \\textbf{0.333} & \\uline{0.369} \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & 0.369 & 0.392 & 0.369 & \\uline{0.386} & \\textbf{0.365} & \\textbf{0.385} & \\uline{0.367} & 0.387 & \\textbf{0.365} & \\textbf{0.385} \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\textbf{0.416} & 0.42 & 0.425 & 0.421 & \\textbf{0.416} & \\textbf{0.413} & \\uline{0.421} & \\uline{0.415} & \\textbf{0.416} & \\textbf{0.413} \\\\ \n            \\cline{2-13} \n            &\\multirow{4}*{\\rotatebox{90}{ETTM2}} & 96 & 0.166 & \\uline{0.256} & 0.167 & 0.26 & \\textbf{0.164} & \\textbf{0.255} & \\uline{0.165} & \\textbf{0.255} & \\textbf{0.164} & \\textbf{0.255} \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\uline{0.223} & \\uline{0.296} & 0.224 & 0.303 & \\textbf{0.219} & \\textbf{0.293} & 0.225 & 0.299 & \\textbf{0.219} & \\textbf{0.293} \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\uline{0.274} & \\textbf{0.329} & 0.281 & 0.342 & \\textbf{0.273} & \\uline{0.33} & \\textbf{0.273} & \\textbf{0.329} & \\textbf{0.273} & \\textbf{0.329} \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & 0.362 & 0.385 & 0.397 & 0.421 & \\textbf{0.358} & \\textbf{0.38} & \\uline{0.361} & \\uline{0.383} & \\textbf{0.358} & \\textbf{0.38} \\\\ \n            \\cline{2-13} \n            &\\multirow{4}*{\\rotatebox{90}{Electricity}} & 96 & \\textbf{0.129} & \\textbf{0.222} & \\uline{0.14} & 0.237 & \\textbf{0.129} & \\uline{0.224} & \\textbf{0.129} & 0.225 & \\textbf{0.129} & \\uline{0.224} \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\uline{0.147} & \\textbf{0.24} & 0.153 & 0.249 & 0.148 & \\uline{0.242} & \\textbf{0.146} & \\uline{0.242} & \\textbf{0.146} & \\uline{0.242} \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\uline{0.163} & \\uline{0.259} & 0.169 & 0.267 & 0.164 & \\uline{0.259} & \\textbf{0.158} & \\textbf{0.256} & \\textbf{0.158} & \\textbf{0.256} \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\uline{0.197} & \\uline{0.29} & 0.203 & 0.301 & 0.201 & 0.292 & \\textbf{0.186} & \\textbf{0.282} & \\textbf{0.186} & \\textbf{0.282} \\\\ \n            \\cline{2-13} \n            &\\multirow{4}*{\\rotatebox{90}{Traffic}} & 96 & \\uline{0.36} & \\uline{0.249} & 0.41 & 0.282 & \\textbf{0.356} & \\textbf{0.248} & 0.369 & 0.264 & \\textbf{0.356} & \\textbf{0.248} \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & \\uline{0.379} & \\textbf{0.256} & 0.423 & 0.287 & \\textbf{0.377} & \\uline{0.257} & 0.393 & 0.278 & \\textbf{0.377} & \\uline{0.257} \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & \\uline{0.392} & \\uline{0.264} & 0.436 & 0.296 & \\textbf{0.385} & \\textbf{0.262} & 0.406 & 0.285 & \\textbf{0.385} & \\textbf{0.262} \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\uline{0.432} & \\uline{0.286} & 0.466 & 0.315 & \\textbf{0.424} & \\textbf{0.283} & 0.445 & 0.304 & \\textbf{0.424} & \\textbf{0.283} \\\\ \n            \\cline{2-13} \n            &\\multirow{4}*{\\rotatebox{90}{Weather}} & 96 & 0.149 & \\uline{0.198} & 0.176 & 0.237 & \\uline{0.148} & \\uline{0.198} & \\textbf{0.146} & \\textbf{0.197} & \\textbf{0.146} & \\textbf{0.197} \\\\ \n            &\\multicolumn{1}{c|}{} & 192 & 0.194 & \\uline{0.241} & 0.22 & 0.282 & \\uline{0.193} & \\textbf{0.24} & \\textbf{0.191} & \\textbf{0.24} & \\textbf{0.191} & \\textbf{0.24} \\\\ \n            &\\multicolumn{1}{c|}{} & 336 & 0.245 & 0.282 & 0.265 & 0.319 & \\textbf{0.243} & \\textbf{0.279} & \\uline{0.244} & \\uline{0.28} & \\textbf{0.243} & \\textbf{0.279} \\\\ \n            &\\multicolumn{1}{c|}{} & 720 & \\textbf{0.314} & \\uline{0.334} & 0.323 & 0.362 & 0.317 & \\textbf{0.333} & \\uline{0.316} & \\textbf{0.333} & \\uline{0.316} & \\textbf{0.333} \\\\ \n            \\cline{2-13} \n\t\t\\end{tabular}\n\t}\n\t\\caption{Drill-down view of ~\\tsmb~ for supervised multivariate forecasting.~\\tsmb~is defined as the best of ~\\tsmgh~and~\\tsmghc, wherein. based on the considered dataset - either ~\\tsmgh~ or ~\\tsmghc~ outperforms the existing benchmarks. ~\\tsmghc~ uses context length = 1 for this experiment. PatchTST results are reported from ~\\cite{patchtst} and DLinear from ~\\cite{dlinear}}\n\t\\label{tab:supervised-ab1}\n\\end{table*}",
            "tab:ga_hc-ab1": "\\begin{table*}[t]\n        % \\small\n\t\\centering\n        \\captionsetup{width=.8\\linewidth}\n\t% \\resizebox{\\linewidth}{!}{\n        \\scalebox{0.9}{\n\t\t\\begin{tabular}{cc|c|cc|cc|cc|cc|ccc}\n\t\t\t\\cline{2-13}\n\t\t\t&\\multicolumn{2}{c|}{Models}& \\multicolumn{2}{c|}{V-\\tsm}& \\multicolumn{2}{c|}{CI-\\tsm}& \\multicolumn{2}{c|}{\\citsm(\\ga)}& \\multicolumn{2}{c|}{\\citsm(\\hr)}& \\multicolumn{2}{c}{\\citsm(\\ga,\\hr)} \\\\\n\t\t\t\\cline{2-13}\n\t\t\t&\\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\\\\n\t\t\t\\cline{2-13}\n        &\\multirow{4}*{\\rotatebox{90}{ETTH1}} &96& 0.449 & 0.462 & \\uline{0.375} & 0.401 & \\uline{0.375} & \\uline{0.4} & 0.377 & 0.405 & \\textbf{0.368} & \\textbf{0.398} \\\\ \n        &\\multicolumn{1}{c|}{} & 192 & 0.485 & 0.484 & 0.411 & 0.426 & \\uline{0.408} & \\uline{0.422} & 0.41 & 0.428 & \\textbf{0.399} & \\textbf{0.418} \\\\ \n        &\\multicolumn{1}{c|}{} & 336 & 0.504 & 0.497 & 0.437 & 0.445 & 0.433 & \\uline{0.439} & \\uline{0.431} & 0.445 & \\textbf{0.421} & \\textbf{0.436} \\\\ \n        &\\multicolumn{1}{c|}{} & 720 & 0.573 & 0.534 & 0.465 & 0.478 & \\uline{0.454} & \\uline{0.47} & 0.457 & 0.477 & \\textbf{0.444} & \\textbf{0.467} \\\\ \n        \\cline{2-13} \n        &\\multirow{4}*{\\rotatebox{90}{ETTH2}} &96& 0.369 & 0.412 & 0.284 & 0.342 & \\textbf{0.276} & \\uline{0.339} & \\uline{0.283} & 0.341 & \\textbf{0.276} & \\textbf{0.337} \\\\ \n        &\\multicolumn{1}{c|}{} & 192 & 0.391 & 0.428 & 0.353 & 0.384 & \\uline{0.338} & \\uline{0.379} & 0.344 & 0.38 & \\textbf{0.335} & \\textbf{0.377} \\\\ \n        &\\multicolumn{1}{c|}{} & 336 & 0.403 & 0.44 & \\uline{0.365} & \\textbf{0.403} & \\textbf{0.362} & \\uline{0.404} & 0.376 & 0.408 & 0.369 & 0.406 \\\\ \n        &\\multicolumn{1}{c|}{} & 720 & 0.475 & 0.481 & \\uline{0.406} & \\uline{0.442} & 0.415 & 0.451 & \\textbf{0.396} & \\textbf{0.434} & 0.409 & 0.447 \\\\ \n        \\cline{2-13} \n        &\\multirow{4}*{\\rotatebox{90}{ETTM1}} &96& 0.328 & 0.372 & 0.305 & 0.354 & \\textbf{0.296} & \\uline{0.349} & 0.301 & 0.351 & \\uline{0.297} & \\textbf{0.348} \\\\ \n        &\\multicolumn{1}{c|}{} & 192 & 0.372 & 0.397 & 0.336 & 0.374 & \\uline{0.334} & \\uline{0.37} & 0.338 & 0.372 & \\textbf{0.333} & \\textbf{0.369} \\\\ \n        &\\multicolumn{1}{c|}{} & 336 & 0.405 & 0.416 & 0.375 & 0.398 & \\uline{0.366} & \\uline{0.387} & 0.376 & 0.394 & \\textbf{0.365} & \\textbf{0.385} \\\\ \n        &\\multicolumn{1}{c|}{} & 720 & 0.457 & 0.445 & 0.425 & 0.417 & \\uline{0.419} & \\uline{0.415} & 0.422 & 0.417 & \\textbf{0.416} & \\textbf{0.413} \\\\ \n        \\cline{2-13} \n        &\\multirow{4}*{\\rotatebox{90}{ETTM2}} &96& 0.195 & 0.282 & 0.172 & 0.263 & 0.175 & 0.264 & \\uline{0.168} & \\uline{0.259} & \\textbf{0.164} & \\textbf{0.255} \\\\ \n        &\\multicolumn{1}{c|}{} & 192 & 0.26 & 0.326 & \\uline{0.221} & \\uline{0.294} & \\uline{0.221} & \\uline{0.294} & 0.224 & 0.296 & \\textbf{0.219} & \\textbf{0.293} \\\\ \n        &\\multicolumn{1}{c|}{} & 336 & 0.341 & 0.373 & \\textbf{0.273} & \\textbf{0.328} & 0.277 & 0.333 & \\uline{0.276} & 0.331 & \\textbf{0.273} & \\uline{0.33} \\\\ \n        &\\multicolumn{1}{c|}{} & 720 & 0.437 & 0.432 & \\textbf{0.357} & 0.384 & 0.365 & 0.385 & 0.36 & \\uline{0.383} & \\uline{0.358} & \\textbf{0.38} \\\\ \n        \\cline{2-13} \n        &\\multirow{4}*{\\rotatebox{90}{Electricity}} &96& 0.212 & 0.323 & \\uline{0.13} & \\uline{0.224} & \\textbf{0.129} & \\textbf{0.223} & \\uline{0.13} & 0.226 & \\textbf{0.129} & \\uline{0.224} \\\\ \n        &\\multicolumn{1}{c|}{} & 192 & \\uline{0.21} & 0.319 & \\textbf{0.148} & \\uline{0.242} & \\textbf{0.148} & \\textbf{0.241} & \\textbf{0.148} & 0.243 & \\textbf{0.148} & \\uline{0.242} \\\\ \n        &\\multicolumn{1}{c|}{} & 336 & 0.224 & 0.332 & \\uline{0.165} & \\textbf{0.259} & \\uline{0.165} & \\textbf{0.259} & \\uline{0.165} & \\uline{0.26} & \\textbf{0.164} & \\textbf{0.259} \\\\ \n        &\\multicolumn{1}{c|}{} & 720 & 0.251 & 0.352 & \\uline{0.204} & 0.293 & \\textbf{0.201} & \\textbf{0.291} & \\uline{0.204} & 0.295 & \\textbf{0.201} & \\uline{0.292} \\\\ \n        \\cline{2-13} \n        &\\multirow{4}*{\\rotatebox{90}{Traffic}} &96& 0.618 & 0.386 & 0.358 & 0.251 & \\textbf{0.355} & \\textbf{0.246} & 0.357 & 0.251 & \\uline{0.356} & \\uline{0.248} \\\\ \n        &\\multicolumn{1}{c|}{} & 192 & 0.624 & 0.385 & 0.379 & \\uline{0.26} & \\textbf{0.377} & \\textbf{0.257} & \\uline{0.378} & \\uline{0.26} & \\textbf{0.377} & \\textbf{0.257} \\\\ \n        &\\multicolumn{1}{c|}{} & 336 & 0.63 & 0.38 & 0.388 & 0.265 & \\uline{0.387} & \\uline{0.264} & \\uline{0.387} & 0.265 & \\textbf{0.385} & \\textbf{0.262} \\\\ \n        &\\multicolumn{1}{c|}{} & 720 & 0.66 & 0.389 & 0.426 & 0.286 & 0.427 & \\uline{0.285} & \\uline{0.425} & 0.286 & \\textbf{0.424} & \\textbf{0.283} \\\\ \n        \\cline{2-13} \n        &\\multirow{4}*{\\rotatebox{90}{Weather}} &96& 0.159 & 0.216 & 0.15 & 0.202 & \\uline{0.149} & \\uline{0.199} & 0.151 & 0.202 & \\textbf{0.148} & \\textbf{0.198} \\\\ \n        &\\multicolumn{1}{c|}{} & 192 & 0.207 & 0.257 & \\uline{0.195} & 0.244 & \\uline{0.195} & \\uline{0.242} & \\uline{0.195} & 0.243 & \\textbf{0.193} & \\textbf{0.24} \\\\ \n        &\\multicolumn{1}{c|}{} & 336 & 0.256 & 0.294 & \\uline{0.246} & 0.284 & \\uline{0.246} & \\uline{0.281} & \\uline{0.246} & 0.283 & \\textbf{0.243} & \\textbf{0.279} \\\\ \n        &\\multicolumn{1}{c|}{} & 720 & 0.33 & 0.344 & 0.323 & 0.338 & \\textbf{0.317} & \\textbf{0.333} & \\uline{0.321} & \\uline{0.337} & \\textbf{0.317} & \\textbf{0.333} \\\\ \n        \\cline{2-13} \n        \\multicolumn{5}{c|}{ \\makecell{\\textbf{\\% MSE improvement}  \\textbf{over \\vtsm}}} &\\multicolumn{2}{c|}{\\textbf{18\\%}} &\\multicolumn{2}{c|}{\\textbf{18.5\\%}}  &\\multicolumn{2}{c|}{\\textbf{18.1\\%}} &\\multicolumn{2}{c}{\\textbf{19.3\\%}} \\\\ \n            \\cline{2-13} \n\t\t\\end{tabular}\n\t}\n\t\\caption{Effect of CI, Gated Attention and Hierarchy Reconciliation over Vanilla TSMixer on all datasets. ~\\citsm~outperforms ~\\vtsm~ by 18\\% and by adding gated attention (G) and Hierarchy Reconciliation head (H), the accuracy further improves by 1.3\\%, leading to a total of 19.3\\% improvement. It is important to note that, we observe stable improvements when  (G) and (H) are used together instead of just (G) or (H).}\n\t\\label{tab:ga_hc-ab1}\n\\end{table*}",
            "tab:cc-detail": "\\begin{table*}[t]\n    % \\setlength{\\tabcolsep}{5pt}\n    \\captionsetup{width=.8\\linewidth}\n    \\centering\n    % \\resizebox{\\linewidth}{!}{\n    \\scalebox{0.9}{\n    \\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}\n        \\toprule\n        \n        \\multicolumn{2}{c|}{Models}& V-\\tsm & CI-\\tsm & IC-\\tsm & \\multicolumn{6}{c}{\\citsm(\\ga,\\cc)} \\\\ \\hline\n        \\multicolumn{2}{c|}{Channel Context Length}& \\multicolumn{3}{c|}{} & (1) & (2) & (3) & (4) & (5) & (Best) \\\\ \\hline\n        \\multirow{4}*{\\rotatebox{90}{ETTH1}} &96& 0.449 & 0.375 & 0.379 & \\textbf{0.373} & \\textbf{0.373} & 0.377 & \\textbf{0.373} & \\uline{0.374} & \\textbf{0.373} \\\\ \n        \\multicolumn{1}{c|}{} & 192 & 0.485 & 0.411 & 0.416 & 0.416 & \\textbf{0.407} & \\textbf{0.407} & 0.409 & \\uline{0.408} & \\textbf{0.407} \\\\ \n        \\multicolumn{1}{c|}{} & 336 & 0.504 & 0.437 & 0.437 & 0.436 & \\uline{0.435} & 0.437 & 0.436 & \\textbf{0.43} & \\textbf{0.43} \\\\ \n        \\multicolumn{1}{c|}{} & 720 & 0.573 & \\uline{0.465} & 0.471 & 0.482 & 0.493 & 0.472 & \\uline{0.465} & \\textbf{0.454} & \\textbf{0.454} \\\\ \n        \\hline \n        \\multirow{4}*{\\rotatebox{90}{ETTH2}} &96& 0.369 & 0.284 & 0.291 & \\uline{0.27} & 0.274 & 0.277 & 0.275 & \\textbf{0.269} & \\textbf{0.269} \\\\ \n        \\multicolumn{1}{c|}{} & 192 & 0.391 & 0.353 & 0.345 & 0.336 & 0.347 & \\uline{0.333} & \\textbf{0.33} & 0.339 & \\textbf{0.33} \\\\ \n        \\multicolumn{1}{c|}{} & 336 & 0.403 & 0.365 & 0.361 & 0.364 & 0.363 & \\uline{0.36} & \\textbf{0.359} & 0.363 & \\textbf{0.359} \\\\ \n        \\multicolumn{1}{c|}{} & 720 & 0.475 & 0.406 & 0.419 & 0.4 & \\uline{0.398} & 0.41 & \\textbf{0.393} & \\textbf{0.393} & \\textbf{0.393} \\\\ \n        \\hline \n        \\multirow{4}*{\\rotatebox{90}{ETTM1}} &96& 0.328 & 0.305 & 0.307 & \\textbf{0.3} & \\uline{0.302} & 0.304 & 0.309 & 0.305 & \\textbf{0.3} \\\\ \n        \\multicolumn{1}{c|}{} & 192 & 0.372 & \\textbf{0.336} & 0.34 & 0.345 & 0.345 & 0.341 & 0.345 & \\uline{0.338} & \\uline{0.338} \\\\ \n        \\multicolumn{1}{c|}{} & 336 & 0.405 & \\uline{0.375} & \\textbf{0.372} & 0.379 & \\uline{0.375} & \\uline{0.375} & 0.377 & 0.383 & \\uline{0.375} \\\\ \n        \\multicolumn{1}{c|}{} & 720 & 0.457 & \\uline{0.425} & 0.428 & \\textbf{0.422} & 0.426 & 0.429 & 0.426 & 0.44 & \\textbf{0.422} \\\\ \n        \\hline \n        \\multirow{4}*{\\rotatebox{90}{ETTM2}} &96& 0.195 & 0.172 & 0.169 & \\uline{0.166} & 0.173 & \\textbf{0.165} & \\uline{0.166} & 0.168 & \\textbf{0.165} \\\\ \n        \\multicolumn{1}{c|}{} & 192 & 0.26 & 0.221 & 0.221 & 0.225 & \\textbf{0.218} & \\uline{0.22} & \\uline{0.22} & 0.221 & \\textbf{0.218} \\\\ \n        \\multicolumn{1}{c|}{} & 336 & 0.341 & \\uline{0.273} & 0.278 & \\textbf{0.272} & 0.284 & 0.277 & 0.285 & 0.276 & \\textbf{0.272} \\\\ \n        \\multicolumn{1}{c|}{} & 720 & 0.437 & \\textbf{0.357} & 0.367 & 0.359 & \\uline{0.358} & 0.385 & 0.362 & 0.362 & \\uline{0.358} \\\\ \n        \\hline \n        \\multirow{4}*{\\rotatebox{90}{Electricity}} &96& 0.212 & \\textbf{0.13} & 0.163 & 0.139 & \\uline{0.133} & \\uline{0.133} & \\uline{0.133} & 0.134 & \\uline{0.133} \\\\ \n        \\multicolumn{1}{c|}{} & 192 & 0.21 & \\textbf{0.148} & 0.181 & \\uline{0.15} & \\uline{0.15} & \\uline{0.15} & 0.151 & \\uline{0.15} & \\uline{0.15} \\\\ \n        \\multicolumn{1}{c|}{} & 336 & 0.224 & 0.165 & 0.196 & 0.162 & \\textbf{0.159} & \\uline{0.161} & \\uline{0.161} & 0.163 & \\textbf{0.159} \\\\ \n        \\multicolumn{1}{c|}{} & 720 & 0.251 & 0.204 & 0.224 & \\uline{0.197} & \\uline{0.197} & \\textbf{0.194} & \\textbf{0.194} & \\textbf{0.194} & \\textbf{0.194} \\\\ \n        \\hline \n        \\multirow{4}*{\\rotatebox{90}{Traffic}} &96& 0.618 & \\textbf{0.358} & 0.468 & 0.382 & 0.384 & 0.384 & \\uline{0.374} & 0.387 & \\uline{0.374} \\\\ \n        \\multicolumn{1}{c|}{} & 192 & 0.624 & \\textbf{0.379} & 0.483 & 0.401 & 0.403 & 0.401 & 0.404 & \\uline{0.4} & \\uline{0.4} \\\\ \n        \\multicolumn{1}{c|}{} & 336 & 0.63 & \\textbf{0.388} & 0.493 & \\uline{0.411} & 0.412 & 0.413 & 0.413 & 0.412 & \\uline{0.411} \\\\ \n        \\multicolumn{1}{c|}{} & 720 & 0.66 & \\textbf{0.426} & 0.524 & 0.448 & 0.457 & \\uline{0.445} & 0.454 & 0.451 & \\uline{0.445} \\\\ \n        \\hline \n        \\multirow{4}*{\\rotatebox{90}{Weather}} &96& 0.159 & 0.15 & 0.15 & 0.149 & \\textbf{0.146} & 0.148 & \\uline{0.147} & 0.15 & \\textbf{0.146} \\\\ \n        \\multicolumn{1}{c|}{} & 192 & 0.207 & \\uline{0.195} & 0.196 & 0.196 & 0.196 & \\textbf{0.194} & 0.197 & 0.197 & \\textbf{0.194} \\\\ \n        \\multicolumn{1}{c|}{} & 336 & 0.256 & \\textbf{0.246} & \\uline{0.248} & \\textbf{0.246} & 0.251 & 0.249 & 0.249 & \\textbf{0.246} & \\textbf{0.246} \\\\ \n        \\multicolumn{1}{c|}{} & 720 & 0.33 & 0.323 & 0.338 & \\textbf{0.317} & \\uline{0.318} & 0.319 & 0.321 & 0.319 & \\textbf{0.317} \\\\ \n        \\hline \n        \\multicolumn{3}{c|}{\\textbf{\\% improvement over \\vtsm}}& \\textbf{18\\% }&  \\textbf{13.2\\%} & \\multicolumn{5}{c}{} & \\textbf{19\\%} \\\\ \n        \\hline \n    \\end{tabular}\n    }\n    \\caption{Detailed MSE Analysis of various Channel Mixing techniques with different context lengths. Context length is varied from (1) to (5) and the minimum is selected as (Best) in this table. From the complete data analysis, we observe that ~\\citsm~ outperforms~\\vtsm~ by 18\\%, and by adding a cross-channel reconciliation head (CC), the accuracy further improves by 1\\% leading to a total improvement of 19\\%. In contrast - ~\\ictsm~ outperforms ~\\vtsm~ but not ~\\citsm~.}\n    \\label{tab:cc-detail}\n\\end{table*}",
            "tab:ab_fm_1": "\\begin{table*}\n\\centering\n\\captionsetup{width=.8\\linewidth}\n\\setlength{\\tabcolsep}{2pt}\n\\scalebox{0.9}{\n\\begin{tabular}{c|c|c|c|c|c|c}\n\\hline  \n& \\makecell{FL} & \\makecell{CI-\\tsm\\\\-Best} & \\makecell{CI-\\tsm\\\\(\\ga,\\hr))} & \\makecell{CI-\\tsm\\\\(\\ga,\\hr,\\cc))} & \\makecell{PatchTST} & \\makecell{BTSF}  \\\\\n\\hline\n\\multirow{5}*{\\rotatebox{90}{ETTH1}} & 24 & \\textbf{0.314} & \\uline{0.319} & \\textbf{0.314} & 0.322 & 0.541 \\\\ \n\\multicolumn{1}{c|}{} & 48 & \\textbf{0.343} & \\uline{0.344} & \\textbf{0.343} & 0.354 & 0.613 \\\\ \n\\multicolumn{1}{c|}{} & 168 & \\textbf{0.397} & \\textbf{0.397} & \\uline{0.402} & 0.419 & 0.64 \\\\ \n\\multicolumn{1}{c|}{} & 336 & \\textbf{0.424} & \\textbf{0.424} & \\uline{0.43} & 0.445 & 0.864 \\\\ \n\\multicolumn{1}{c|}{} & 720 & \\textbf{0.453} & \\textbf{0.453} & \\uline{0.457} & 0.487 & 0.993 \\\\ \n\\hline\n\\multirow{5}*{\\rotatebox{90}{Weather}} & 24 & \\uline{0.088} & 0.09 & \\uline{0.088} & \\textbf{0.087} & 0.324 \\\\ \n\\multicolumn{1}{c|}{} & 48 & \\uline{0.114} & \\uline{0.114} & 0.141 & \\textbf{0.113} & 0.366 \\\\ \n\\multicolumn{1}{c|}{} & 168 & \\textbf{0.177} & \\textbf{0.177} & \\uline{0.178} & \\uline{0.178} & 0.543 \\\\ \n\\multicolumn{1}{c|}{} & 336 & \\textbf{0.241} & \\textbf{0.241} & \\uline{0.244} & \\uline{0.244} & 0.568 \\\\ \n\\multicolumn{1}{c|}{} & 720 & \\textbf{0.319} & \\textbf{0.319} & 0.322 & \\uline{0.321} & 0.601 \\\\ \n\\hline\n% &\\multicolumn{3}{c|}{\\makecell{\\textbf{\\citsm-Best} & \\textbf{9.372}}\t& \\textbf{10.421} & \\textbf{9.656} & \\textbf{11.352} \\\\ \\hline\n\\end{tabular}}\n\\caption{MSE Drill-down view of ~\\tsmb~ for self-supervised multivariate forecasting. Min of ~\\tsmgh~and~\\tsmghc~is depicted as ~\\tsmb, wherein. based on the considered dataset - either ~\\tsmgh~ or ~\\tsmghc~ outperforms the existing benchmarks.}\n\\label{tab:ab_fm_1}\n\\end{table*}"
        },
        "figures": {
            "fig:high-level": "\\begin{figure}\n    \\centering\n    % \\includegraphics[width=0.8\\columnwidth]{figs/high-level.pdf}\n    \\includegraphics[width=0.8\\columnwidth]{figs/vj_flow_1.jpg}\n    \\caption{High-level model architecture.}\n    \\label{fig:high-level}\n\\end{figure}",
            "fig:backbone": "\\begin{figure*}[t]\n     \\centering\n     \\begin{subfigure}[b]{0.85\\columnwidth}\n         \\centering\n         % \\includegraphics[width=\\columnwidth]{figs/backbones.pdf}\n         \\includegraphics[width=\\columnwidth]{figs/vj_backbone.jpg}\n         \\caption{Different backbones in TSMixer}\n         \\label{fig:backbone:backbones}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{1.1\\columnwidth}\n         \\centering\n         % \\includegraphics[width=\\columnwidth]{figs/mixer-layers.pdf}\n         \\includegraphics[width=\\columnwidth]{figs/vj_mix.jpg}\n         \\caption{MLP Mixer layer architecture}\n         \\label{fig:backbone:mixer-layers}\n     \\end{subfigure}\n    \\caption{\n    Different backbones in TSMixer and the architecture of the mixer layers.\n    }\n    \\label{fig:backbone}\n\\end{figure*}",
            "fig:FCR": "\\begin{figure}\n    \\centering\n    % \\includegraphics[width=0.8\\columnwidth]{figs/FCR.pdf}\n    \\includegraphics[width=0.85\\columnwidth]{figs/vj_cc.jpg}\n    \\caption{Cross-channel forecast reconciliation head.}\n    \\label{fig:FCR}\n\\end{figure}",
            "fig:HPR": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.73\\columnwidth]{figs/vj_hier.jpg}\n    \\caption{Online hierarchical patch reconciliation head.}\n    \\label{fig:HPR}\n\\end{figure}",
            "fig:patch_emb": "\\begin{figure}[t]\n     \\centering\n     \\includegraphics[width=1\\columnwidth]{figs/patch_embedd_fig.jpg}\n    \\caption{Correlation between Patch time-series and its associated embeddings.}\n    \\label{fig:patch_emb}\n         \n\\end{figure}",
            "fig:gated-attention": "\\begin{figure}[!h]\n     \\centering\n     \\begin{subfigure}[b]{0.49\\columnwidth}\n         \\centering\n         \\includegraphics[width=0.8\\columnwidth]{figs/GA-1.pdf}\n         \\caption{MLP block in \\mixer}\n         \\label{fig:mlp}\n     \\end{subfigure}\n     % \\hfill\n     \\begin{subfigure}[b]{0.49\\columnwidth}\n         \\centering\n         \\includegraphics[width=0.8\\columnwidth]{figs/GA-2.pdf}\n         \\caption{Gated attention in ours}\n         \\label{fig:ga}\n     \\end{subfigure}\n    \\caption{\n    MLP and Gated Attention \n    }\n    \\label{fig:gated-attention}\n\\end{figure}",
            "fig:model-heads": "\\begin{figure}[!h]\n     \\centering\n     \\begin{subfigure}[b]{0.49\\columnwidth}\n         \\centering\n         \\includegraphics[width=0.8\\columnwidth]{figs/pred_head.jpg}\n         \\caption{Prediction head}\n         \\label{fig:model-heads:prediction}\n     \\end{subfigure}\n     % \\hfill\n     \\begin{subfigure}[b]{0.49\\columnwidth}\n         \\centering\n         \\includegraphics[width=0.8\\columnwidth]{figs/pretrain_head.jpg}\n         \\caption{Pretrain head}\n         \\label{fig:model-heads:pretrain}\n     \\end{subfigure}\n    \\caption{\n    Model heads for the two workflows.\n    }\n    \\label{fig:model-heads}\n\\end{figure}",
            "fig:cluster_overall": "\\begin{figure*}[!t]\n    \\centering\n    \\captionsetup{width=.8\\linewidth}\n    \\includegraphics[scale=0.5]{figs/cluster_overall.jpg}\n    \\caption{Correlation between Patch time-series and its associated embeddings in multiple datasets. Nearby patch representations highly correlate to the patch time series of similar shapes and patterns, thereby learning meaningful patch representations.}\n    \\label{fig:cluster_overall}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\hat{\\mY}_{fl \\times c} = \\gM \\left( \\mX_{sl \\times c} \\right).\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\gL_{\\text{hier}} = \\frac{1}{sf} \\gL\\left( \\hat{\\mH}, \\mH \\right) +\n    \\gL \\left( \\mY, \\hat{\\mY}_{\\text{rec}} \\right) +\n    \\frac{1}{sf} \\gL \\left( \\text{BU} \\left(  \\hat{\\mY}_{\\text{rec}} \\right), \\hat{\\mH} \\right)\n\\end{equation}"
        },
        "git_link": "https://github.com/ibm/tsfm"
    }
}