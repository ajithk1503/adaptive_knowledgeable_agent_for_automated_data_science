{
    "meta_info": {
        "title": "Spatial Heterophily Aware Graph Neural Networks",
        "abstract": "Graph Neural Networks (GNNs) have been broadly applied in many urban\napplications upon formulating a city as an urban graph whose nodes are urban\nobjects like regions or points of interest. Recently, a few enhanced GNN\narchitectures have been developed to tackle heterophily graphs where connected\nnodes are dissimilar. However, urban graphs usually can be observed to possess\na unique spatial heterophily property; that is, the dissimilarity of neighbors\nat different spatial distances can exhibit great diversity. This property has\nnot been explored, while it often exists. To this end, in this paper, we\npropose a metric, named Spatial Diversity Score, to quantitatively measure the\nspatial heterophily and show how it can influence the performance of GNNs.\nIndeed, our experimental investigation clearly shows that existing heterophilic\nGNNs are still deficient in handling the urban graph with high spatial\ndiversity score. This, in turn, may degrade their effectiveness in urban\napplications. Along this line, we propose a Spatial Heterophily Aware Graph\nNeural Network (SHGNN), to tackle the spatial diversity of heterophily of urban\ngraphs. Based on the key observation that spatially close neighbors on the\nurban graph present a more similar mode of difference to the central node, we\nfirst design a rotation-scaling spatial aggregation module, whose core idea is\nto properly group the spatially close neighbors and separately process each\ngroup with less diversity inside. Then, a heterophily-sensitive spatial\ninteraction module is designed to adaptively capture the commonality and\ndiverse dissimilarity in different spatial groups. Extensive experiments on\nthree real-world urban datasets demonstrate the superiority of our SHGNN over\nseveral its competitors.",
        "author": "Congxi Xiao, Jingbo Zhou, Jizhou Huang, Tong Xu, Hui Xiong",
        "link": "http://arxiv.org/abs/2306.12139v1",
        "category": [
            "cs.LG",
            "cs.SI"
        ],
        "additionl_info": "Accepted by KDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nApplying Graph Neural Networks (GNNs) in different urban applications has attracted much research attention in the past few years \\cite{song2020spatial, zhou2021competitive, fang2021spatial,xia20213dgcn, rao2022fogs,han2022semi,wu2020learning}.\nThese studies usually model the city as an urban graph whose nodes are urban objects (e.g., regions or Points of Interest (POIs)) and whose edges are  physical or social dependencies in the  urban area (e.g., human mobility and road connection \\cite{wu2022multi,wu2020learning}). Upon urban graphs, GNNs with variant architectures are proposed to achieve the classification or regression tasks. \n\n\n\nNevertheless, there is a serious limitation of GNNs which has been largely overlooked in previous studies but have attracted increasing research attention recently: GNNs have an implicit homophily assumption that nodes only with similar features\nor same labels are connected together on the graph \\cite{zhu2021graph,zheng2022graph}. Meanwhile, the opposite assumption is heterophily that connected nodes have dissimilar features or labels. \nIn fact, heterophily usually exists in an urban graph as it describes the complex urban system where both similar and dissimilar urban objects\n(e.g., regions with different functionalities) \ncan correlate with each other in complex manners.\nTaking the urban graph constructed with human mobility as an example, the start node and end node of an edge could be home and workplace, respectively, which are definitely heterophilic.\nSuch difference information on the heterophilic urban graph may not be modeled well by many traditional homophilic GNNs, who tend to generate similar representations for connected nodes \\cite{zhu2021graph, bo2021beyond}. In this way, the performance of these homophilic GNN methods on urban graphs may be largely hindered.\n\nOur further observation is that urban graphs have a unique \\emph{Spatial Heterophily} property. To be specific, we find that the heterophily on the urban graph often presents a characteristic of \\emph{spatial diversity}. In other words, the difference (or dissimilarity) between the central node and its neighbors \\hide{located }at different distances or directions exhibits evident discrepancy, rather than distributed uniformly. \nAware of such a characteristic, an attendant question is how to measure the spatial heterophily of the urban graph.\nThere have been various studies put forward to investigate the graph homophily and heterophily from different perspectives, including node homophily \\cite{pei2019geom}, edge homophily \\cite{zhu2020beyond} and class homophily \\cite{lim2021large}. But without considering the spatial position of linking nodes, these metrics cannot describe the spatial heterophily on urban graphs.\n\nTherefore, in this work, we propose a metric named \\textbf{spatial diversity score} to analyze the spatial heterophily, and investigate its influence on the performance of existing GNN\nmethods on urban graphs. Firstly, we divide the neighbors on an urban graph into different spatial groups according to their locations (including direction and distance), and then the spatial diversity score measures the discrepancy\nbetween different spatial groups, in terms of their label dissimilarity to the central node. A higher score (close to one) indicates a larger discrepancy between different spatial groups, and thus a higher spatial diversity of heterophily on the urban graph.\n\nYet, it still remains an outstanding challenge for designing powerful heterophilic GNN models over an urban graph if its spatial diversity score is high, where there is a diversity of dissimilarity distributions between the central node and its neighbors at different spatial locations.\nThere are some recent studies to improve the GNN architectures to handle the graph heterophily \\cite{jia2020residual,lim2021new, yang2021graph, kim2020find, du2022gbk}.\nMost of these methods can only work on a heterophilic graph when there is limited difference between nodes. For example, GBKGNN \\cite{du2022gbk} assumes that there are only two different kinds of nodes, and FAGCN \\cite{bo2021beyond} assumes that node features have only two different levels of frequencies (Note that this limitation is discussed in their papers). Following this line, it is hard to model such diverse distributions of spatial heterophily on urban graphs.\nTo provide more evidence, we conduct an experiment on synthetic urban graphs with varying levels of spatial diversity score (details are in Section \\ref{toy}). As shown in Figure \\ref{fig_motivation}(d), when the graph presents higher spatial diversity score,\\hide{of heterophily,} the performance of these two state-of-the-art heterophilic GNNs, GBKGNN and FAGCN, is far from optimal.\nWe also apply the proposed spatial diversity score to analyze three real-world urban graphs under different target tasks in our experiments. As we can see from Figure \\ref{fig_motivation}(c), three urban graphs present different levels of spatial heterophily, where one of them can get a very high score (0.99 on the urban graph in crime prediction task). Thus, it is valuable to develop an effective GNN model that can handle the diverse spatial heterophily of the urban graph.\n\nThrough our in-depth analysis of spatial heterophily, we observe that the heterophily further exhibits a spatial tendency on urban graphs, which reveals a promising opportunity for us to tackle such diverse heterophily in a divide-and-conquer way. \nDifferent from ordinary graphs, nodes on urban graphs should follow Tobler's First Law of Geography (TFL) \\cite{tobler1970computer}. As the fundamental assumption used in almost all urban analysis, TFL means \\emph{everything is related to everything else, but near things are more related than distant things.} \nObeying TFL, spatially close neighbors on the urban graph present a more similar mode of difference to the central node, compared to the distant ones.\nWe also analyze the real-world urban graph in commercial activeness prediction task to visualize such a tendency (details are in Section \\ref{spatial_tendency_method}). Figure \\ref{data_analysis} clearly shows that spatially close neighbors present less discrepancy from both the direction and distance view.\nThus, if we can properly\ngroup the spatially close neighbors together, it is possible to alleviate the diversity of heterophily inside groups on the urban graph.\n\nTo this end, we propose a novel \\underline{S}patial \\underline{H}eterophily Aware \\underline{G}raph \\underline{N}eural \\underline{N}etwork (\\mymodel), to tackle the spatial heterophily on urban graphs, with two specially designed modules. \nFirst, we devise a Rotation-Scaling Spatial Aggregation module. Its core idea is to properly divide the neighbors into different spatial groups according to their direction and distance to the central node, and perform a spatial-aware feature aggregation for each group, which serves as the basis of handling diverse heterophily distributions separately.\nThen, a Heterophily-Sensitive Spatial Interaction module with two learnable kernel functions is designed to capture the commonality and discrepancy in the neighborhood, and adaptively determine what and how much difference information the central node needs.\nIt acts between the central node and neighbors in different groups to manage the spatial diversity of heterophily on the urban graph.\n\nThe contribution of this paper is summarized as follows:\n\\begin{itemize}[leftmargin=20pt, topsep=2pt]\n    \\setlength{\\itemsep}{0pt}\n    \\setlength{\\parsep}{0pt}\n    \\setlength{\\parskip}{0pt}\n    \\item To the best of our knowledge, we are the first to investigate the spatial heterophily of urban graphs. \n    We design a metric named spatial diversity score, to analyze the spatial heterophily property, and identify the limitation of existing GNNs in handling the diverse spatial heterophily on the urban graph.\n    \\item We propose a novel spatial heterophily aware graph neural network named \\mymodel, in which two techniques: rotation-scaling spatial aggregation and heterophily-sensitive spatial interaction are devised to tackle the spatial heterophily of the urban graph in a divide-and-conquer way. \n    \\item We conducted extensive experiments to verify the effectiveness of \\mymodel on three real-world datasets.\n    \n    \\hide{on three real-world datasets to demonstrate the effectiveness of our \\mymodel across broad urban applications.}\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\label{pre}\nIn this section, we first introduce the basic concepts of the urban graph, then clarify the goal of our work. The frequently used notations are summarized in Table \\ref{table-symbol} in Appendix.\n\\vspace{-1.5mm}\n\\paragraph{Urban Graphs.} \nLet $\\mathcal{G}(\\mathcal{V},\\mathcal{E},\\bm{X})$ denote an urban graph, where $\\mathcal{V}=\\{v_1, ..., v_N\\}$ denotes a set of nodes representing a kind of urban entity, $\\mathcal{E}$ denotes the edge set indicating one type of relation among nodes in the urban scenario, and  $\\mathcal{N}(v_i) = \\{v_j|(v_i, v_j)\\in \\mathcal{E}\\}$ is the neighborhood of node $v_i$.\n$\\bm{X} \\in \\mathbb{R}^{N \\times d}$ denotes the feature matrix, in which the $i$-th row is the $d$-dimensional node features of $v_i$ obtained from the urban data.\nDifferent instantiations of node set and edge set will form different urban graphs, such as:\n(1) \\textbf{Mobility Graph} with regions as nodes and human flows as edges. The node features can be some region attributes such as the distribution of POI inside the region;\n(2) \\textbf{Road Network} where the node set is formed by road sections, and the edges denote the connectivity between them. The node features can be the structural information of a section, such as the number of branches and lanes.\n\n\\vspace{-1.5mm}\n\\paragraph{Problem Formulation.}\nGiven an urban graph, our goal is to design a GNN model, that considers and alleviates the spatial heterophily, to learn the node representation $f\\!:(v_i | \\mathcal{G}) \\!\\rightarrow\\! \\hat{\\bm{h}}_i$,\nwhere $\\hat{\\bm{h}}_i$ denotes the representation vector of node $v_i$. The model $f$ will be trained in an end-to-end manner in different downstream tasks.\n\n"
            },
            "section 3": {
                "name": "Spatial Heterophily",
                "content": "\nIn this section, an analysis of spatial heterophily on urban graphs is provided. \nWe first introduce our metric to measure the spatial heterophily (Section \\ref{discrepancy_metric}).\nThen, Section \\ref{toy} gives an experimental investigation on synthetic graphs. It not only demonstrates the importance for GNNs to consider the spatial heterophily on urban graphs, but also suggests a promising way to tackle this challenge.\n\n\\vspace{-1mm}\n",
                "subsection 3.1": {
                    "name": "Spatial Diversity of Heterophily",
                    "content": "\n\\label{discrepancy_metric}\n\nTo describe the spatial diversity of heterophily on urban graphs, we design a metric named \\textbf{spatial diversity score}.\nTypically, the graph heterophily is measured by the label dissimilarity between the central node and its neighbors (e.g., \\cite{pei2019geom,du2022gbk}). Following this line, our spatial diversity score aims to further assess the discrepancy between neighbors at different spatial locations, in terms of the distributions of their label dissimilarity to the central node.\nBriefly, we first divide the neighborhood into different spatial groups according to their spatial locations. Then, we can measure spatial groups' discrepancy by calculating the Wasserstein distance between their label dissimilarity distributions, and the metric is further defined as the ratio of nodes with high discrepancy.\n\n\\vspace{-1mm}\n",
                    "subsubsection 3.1.1": {
                        "name": "Dual-View Space Partition",
                        "content": "\n\\label{metric_space_partition}\nTo distinguish the spatial location of neighbors and form different spatial groups, we first partition the geographic space into several non-overlap subspaces,\n% from direction and distance view, \nand each neighbor of the central node can be assigned to the group corresponding to the subspace it locates in. \nNote that the spatial heterophily can both present in different directions and distances, thus we propose to perform space partition from both two views.\n\n\\textbf{Direction-Aware Partition.}\nGiven the central node $v_i$ on an urban graph, we evenly partition the geographic space centered by it into ten direction sectors $\\mathcal{S} = \\{s_k \\, | \\, k=0,1,...,9\\}$.\nCorrespondingly, nodes in the neighborhood $\\mathcal{N}(v_i)$ will be divided into the sector they locate in.\nNeighbors belonging to the same sector are redefined as the direction-aware neighborhood $\\{\\mathcal{N}_{s_k}(v_i) \\,|\\, k=0,1,...,9\\}$, where $\\bigcup_{k=0}^{9} \\mathcal{N}_{s_k}(v_i) \\!=\\! \\mathcal{N}(v_i)$.\nIn this way, the direction-aware neighborhoods can be regarded as different spatial groups associated with different spatial relations to the central node. We will then calculate the discrepancy between different spatial groups. Figure \\ref{fig_motivation}(a) illustrates such a sector partition.\n\n\\textbf{Distance-Aware Partition.}\nAs illustrated in Figure \\ref{fig_motivation}(b), we also divide the neighbors based on their distance to the central node. \nTo be specific, we first determine the distance range of the neighborhood on an urban graph, by making a statistic of the distance distribution between connected nodes. Note that we consider the $90\\%$ percentile of this distribution as the maximum distance of the neighborhood on the graph. This is based on the observation that the distance distribution often presents a long tail property, and such a distance cut-off can avoid interference from extremely distant outliers.\nAnd then, the distance range is evenly split into ten buckets, which results in distance rings $\\mathcal{R} = \\{r_k \\, | \\, k=0,1,...,9\\}$. Similarly, the original neighborhood $\\mathcal{N}(v_i)$ can be divided into these ten distance-aware neighborhoods $\\{\\mathcal{N}_{r_k}(v_i) \\, | \\, k=0,1,...,9\\}$ as another view of spatial groups, where we also have $\\bigcup_{k=0}^{9} \\mathcal{N}_{r_k}(v_i) = \\mathcal{N}(v_i)$.\n\n\n\n"
                    },
                    "subsubsection 3.1.2": {
                        "name": "Spatial Diversity Score",
                        "content": "\nAfter the neighborhood partition, our goal is to further define the spatial diversity score by measuring the discrepancy between different spatial groups, based on their distribution distance of label dissimilarity to the central node.\n\nFirst of all, we define spatial group's label dissimilarity distribution to the central node. \nFor a node classification task with $\\mathcal{C}$ classes, within a spatial group, this distribution is calculated as the ratio of neighbors belonging to each class (different to the central node's).\nTaking the direction view as an example, for the central node $v_i$, the label dissimilarity distribution of $s_k$ is formally defined as $P_i^{s_k} = [\\, P_{i,0}^{s_k} \\,,\\, P_{i,1}^{s_k} \\,,\\, ... \\,,\\, P_{i,|\\mathcal{C}-1|}^{s_k}]$ with:\n\\begin{equation}\n    P_{i,c}^{s_k} = \\sum_{v_j \\in \\mathcal{N}_{s_k}(v_i) \\land y_j \\neq y_i } \\!\\!\\! \\mathbbm{1}(y_{j,c} = 1) \\cdot |\\mathcal{N}_{s_k}(v_i)|^{-1},\n\\vspace{-1mm}\n\\end{equation}\nwhere $c=0,1,...,|\\mathcal{C}-1|$, $\\mathbbm{1}(\\cdot)$ is the indicator function, and $y_i$ is $v_i$'s one-hot label vector whose $c$-th value is denoted by $y_{i,c}$. \n\nIn addition, to improve the generality of this metric to more urban applications (e.g., regression tasks), we also extend the definition of label dissimilarity distribution above to the node regression task.\nTo be specific, we first make a statistic of the label difference between connected nodes on the whole graph $\\hat{\\mathcal{Y}} = \\{ (y_j-y_i) \\,|\\, (v_i, v_j) \\in \\mathcal{E} \\}$, and calculate the deciles $\\{D_1, D_2, ,..., D_9 \\}$ of this distribution. These nine deciles can determine ten buckets (intervals), which will be used for the discretization of continuous label difference value, to obtain a similar form of label dissimilarity as the node classification task.\nHence, still for the spatial group $\\mathcal{N}_{s_k}(v_i)$ in sector $s_k$, its label dissimilarity distribution will be calculated as the ratio of neighbors mapped into different buckets, according to their discretized label difference to the central node, which can be also formulized as:\n$P_i^{s_k} = [\\, P_{i,0}^{s_k} \\,,\\, P_{i,1}^{s_k} \\,,\\, ... \\,,\\, P_{i,9}^{s_k}]$, with the $c$-th element computed by:\n\\begin{equation}\n    P_{i,c}^{s_k} = \\sum_{v_j \\in \\mathcal{N}_{s_k}(v_i)} \\!\\!\\! \\mathbbm{1}(D_c < y_j - y_i \\leq D_{c+1}) \\cdot |\\mathcal{N}_{s_k}(v_i)|^{-1},\n\\vspace{-1mm}\n\\end{equation}\nwhere $c=0,1,...,9$. The $D_0$ and $D_{10}$ are used to denote the minimum and maximum of the label difference distribution on the whole graph, respectively (i.e., $D_0=\\min(\\hat{\\mathcal{Y}})$ and $D_{10}=\\max(\\hat{\\mathcal{Y}})$).\n\nNext, the discrepancy between different spatial groups can be defined by measuring the distance between their label dissimilarity distributions. Following a recent study on the graph heterophily \\cite{zhao2022neighborhood}, we adopt Wasserstein distance (WD) to measure the distribution distance between two spatial groups. \nFormally, consider two spatial groups $\\mathcal{N}_{s_p}(v_i)$ and $\\mathcal{N}_{s_q}(v_i)$ in sector $s_p$ and $s_q$ of node $v_i$, the discrepancy between them is defined as:\n\\begin{equation}\n\\label{disc_eq}\n    Disc(v_i, s_p, s_q) = WD(P_i^{s_p}, P_i^{s_q}) ,\n\\end{equation}\nwhere $WD(\\cdot, \\cdot)$ denotes the Wasserstein distance between two distributions, which can be approximately calculated by the Sinkhorn iteration algorithm \\cite{cuturi2013sinkhorn}.\n\nWith such a measurement, we can finally define the spatial diversity score to describe the diverse spatial heterophily of an urban graph. This metric can be computed based on the ratio of nodes with high discrepancy among different spatial groups:\n\\begin{equation}\n    \\lambda_d^{s} = |\\mathcal{V}|^{-1} \\cdot \\!\\! \\sum_{v_i \\in \\mathcal{V}} \\mathbbm{1}(\\max_{p \\neq q}Disc(v_i, s_p, s_q) \\geq 1) ,\n\\vspace{-0.5mm}\n\\end{equation}\nwhere $\\max_{p \\neq q}Disc(v_i, s_p, s_q) \\geq 1$ with $p, q=0,1,...,9$ indicates that there are at least two sectors being discrepant in terms of their label dissimilarity distributions to the central node $v_i$.\nIn this way, the score $\\lambda_d^{s}$ will get higher if there are more nodes whose spatial groups present high discrepancy on the urban graph.\nSimilarly, we can also define the score $\\lambda_d^{r}$ in the distance view, which measures the discrepancy between spatial groups formed by different rings in $\\mathcal{R}$, with different distances to the central node:\n\\begin{equation}\n    \\lambda_d^{r} = |\\mathcal{V}|^{-1} \\cdot \\!\\! \\sum_{v_i \\in \\mathcal{V}} \\mathbbm{1}(\\max_{p \\neq q}Disc(v_i, r_p, r_q) \\geq 1) ,\n\\vspace{-0.5mm}\n\\end{equation}\n\nIn practice, when only a part of nodes are labeled on an urban graph, it's often sufficient to use the labeled data to estimate $\\lambda_d^{s}$ and $\\lambda_d^{r}$.\nFigure \\ref{fig_motivation}(c) shows the scores of three real-world urban graphs.\nAs we can see, from both the direction and distance view, \nurban graphs can present very high spatial diversity scores (e.g., 0.99 in crime prediction), which reveals the diverse heterophily in different spatial groups, with discrepant label dissimilarity distributions.\n\n\\vspace{-1mm}\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Experimental Investigation",
                    "content": "\n\\label{toy}\nNext, we conduct an experimental investigation on synthetic urban graphs to illustrate the importance of considering the diverse spatial heterophily on the urban graph.\n% and examine the effectiveness of neighborhood spatial grouping inspired by the property of spatial tendency on urban graphs.\nSpecifically, we test the performance of several GNN models on a series of synthetic urban graphs with increasing spatial diversity of the heterophily.\nWe generate 10 graphs containing 5000 nodes with 10-dimensional randomly generated feature vectors.\nFor each node, we build 50 edges and assume that these neighbors locate at 10 distance rings from near to far around the central node.\nTo increase the spatial diversity of heterophily from $\\mathcal{G}_1$ to $\\mathcal{G}_{10}$, we gradually enlarge the discrepancy of label dissimilarity distributions between neighbors in different distance rings.\nSpecifically, in graph $\\mathcal{G}_i$, we evenly divide the node set into $i$ subsets $\\mathcal{V}_i = \\bigcup_{j=1}^{i} \\mathcal{V}_{i,j}$, where the labels of nodes in $\\mathcal{V}_{i,j}$ are sampled from the Gaussian distribution $\\mathcal{N}(10j,1)$. \nThen, the 50 neighbors connected to the node in $\\mathcal{V}_{i,j}$ are randomly selected from one of subsets $\\{ \\mathcal{V}_{i,k} \\} |_{k=j}^{i}$ by an equal probability $1/(i-j+1)$. Besides, we let neighbors' spatial distance be consistent with their label difference to the central node (i.e., neighbors with smaller differences locates in the closer distance ring).\nIn this way, these 10 graphs will have increasing spatial diversity scores $\\lambda_d^r$.\nMeanwhile, neighbors in the same distance ring have less discrepancy.\n\nFigure \\ref{fig_motivation}(d) shows the node regression error of GCN \\cite{kipf2016semi} and two state-of-the-art heterophic GNNs (FAGCN \\cite{bo2021beyond} and GBKGNN \\cite{du2022gbk}) on 10 synthetic graphs. As expected, GCN gets the worst performance, since it proved unsuitable for heterophily graphs in most cases \\cite{lim2021new}. For two heterophic GNNs, despite less error than GCN, their performance is still far from optimal when the spatial diversity score increases. This is because they only model limited dissimilarity between nodes (e.g., two levels of frequencies \\cite{bo2021beyond} or two classes of nodes \\cite{du2022gbk}), without considering diverse dissimilarity distributions.\nHowever, if the model separately processes neighbors in each distance ring whose differences to the central node are more similar, (as our \\mymodel did), it can always achieve satisfactory performance.\nThus, it's reasonable to consider whether we can tackle the diverse spatial heterophily, by properly grouping together the neighbors with less discrepancy on the urban graph.\n\\vspace{-1mm}\n"
                }
            },
            "section 4": {
                "name": "Methodology",
                "content": "\nIn this section, we first present that the heterophily on urban graphs often exhibits a spatial tendency: spatially close neighbors have more similar heterophily distributions than distant ones, which obeys TFL. This characteristic gives rise to the solution that we can properly divide neighbors according to their spatial locations, to achieve grouping neighbors with less discrepancy together.\nThen, we introduce our proposed \\mymodel that leverages such a spatial tendency to tackle the spatial heterophily of urban graphs.\n\n\\textbf{Spatial Tendency.}\n\\label{spatial_tendency_method}\nIn addition to the presence of a discrepancy between different spatial groups (discussed in Section \\ref{discrepancy_metric}), our in-depth investigation of spatial heterophily further reveals that such a discrepancy shows a spatial tendency on urban graphs. \nSpecifically, we observe that the discrepancy\nbetween two spatially close groups is smaller, compared to two distant groups. \nWe conduct a data analysis to visually present such a tendency.\nFigure \\ref{data_analysis} shows the pair-wise discrepancy of label dissimilarity distributions between any two spatial groups, which is computed based on real-world human mobility and regional commercial activeness data.\nIn Figure \\ref{data_analysis}(a), for the spatial group formed by \\hide{neighbors in }distance ring $r_p$, we can find that its discrepancy with other rings $r_q$ is highly correlated to their spatial distance (i.e., the discrepancy gets higher when $r_q$ is more distant to $r_p$). For example, \\hide{the label dissimilarity distributions of }the close ring pair $(r_0,r_1)$ are less discrepant than the distant pair $(r_0,r_9)$.\nA similar spatial tendency can also be observed from the direction view. \nAs shown in Figure \\ref{data_analysis}(b), in most cases, the discrepancy will increase along with the included angle between sectors. Taking sector $s_0$ as an example, its discrepancy with $s_1\\sim s_9$ increases first and then decreases, which is roughly in sync with the change of their included angles.\nIn other words, a sector is more likely to be discrepant to another distant sector (e.g., $(s_1, s_6)$) than a nearby one (e.g., $(s_1, s_2)$).\nThis characteristic motivates us to address the diverse spatial heterophily on urban graphs, by properly grouping spatially close neighbors and separately processing each group with less discrepancy inside.\n\nTo this end, we propose a novel GNN architecture named \\mymodel, which is illustrated in Figure \\ref{fig_multi_head} and \\ref{fig_framework}. Our model consists of two components: Rotation-Scaling Spatial Aggregation (see Section \\ref{rot_sca_agg}) and Heterophily-Sensitive Spatial Interaction (see Section \\ref{heter_aware}). \n\n\\vspace{-1mm}\n",
                "subsection 4.1": {
                    "name": "Rotation-Scaling Spatial Aggregation",
                    "content": "\n\\label{rot_sca_agg}\n\nThis component aims to properly group spatially close neighbors and alleviate the diversity of heterophily inside groups in the message passing process. In general, we first divide neighbors according to their relative positions to the central node. Then, the feature aggregation is performed in each spatial group separately. \n\n",
                    "subsubsection 4.1.1": {
                        "name": "Rotation-Scaling Dual-View Partition.",
                        "content": "\nFollowing the neighborhood partition in Section \\ref{metric_space_partition}, we also partition the geographic space into non-overlap subspaces, and neighbors located at the same subspace are then grouped together. Note that there are two major differences between the space partition performed in this component and that in Section \\ref{metric_space_partition}. First, we apply a more general partition with a variable number of subspaces. Second, we introduce a rotation-scaling multi-head partition strategy to model the neighbor's spatial location in a more comprehensive way.\n\nTo be specific, in the direction view, we evenly partition the space into a set of sectors $\\mathcal{S} = \\{s_k \\, | \\, k=0,1,...,n_s-1\\}$, where $n_s$ denotes the number of partitioned sectors, which can be appropriately set for different datasets. Nodes in each direction-aware neighborhood $\\mathcal{N}_{s_k}\\!(v_i)$ of sector $s_k$ are grouped together, which we still call spatial group.\nIn the distance view, the space will be partitioned into $n_r$ distance rings $\\mathcal{R} = \\{r_k \\, | \\, k=0,1,...,n_r-1\\}$, which is resulted from a predefined distance bucket (e.g., $<1km$, $1-2km$, and $>2km$).\nNote that the central node $v_i$ itself does not belong to any sector or ring, we regard it as an additional group $\\mathcal{N}_{s_{n_s}}\\!(v_i) = \\mathcal{N}_{r_{n_r}}\\!(v_i) = \\{v_i\\}$.\n\n\\vspace{-1mm}\n\\paragraph{Rotation-Scaling Multi-Head Partition.}\nIn view of the special case that a part of neighbors may locate at the boundary between two subspaces, we further propose a multi-head partition strategy to simultaneously perform multiple partitions at each view, where different heads can complement each other. \nFor example, as shown in Figure \\ref{fig_multi_head}(a), the orange node $v_4$ locates at the boundary between sectors $s_0$ and $s_1$, which indicates that the spatial relation of the neighborhood is still inadequately excavated by the single direction-based partition. A similar situation can be found in the partition of distance rings, such as the node $v_4$ in Figure \\ref{fig_multi_head}(b).\n\nTo overcome this limitation, we extend our partition strategy by devising two operations, which are \\textit{sector rotation} and \\textit{ring scaling}, to achieve multiple space partitions for capturing the diverse spatial relations comprehensively.\nSpecifically, as illustrated in Figure \\ref{fig_multi_head}(a) and (c), for the originally partitioned direction sectors, we turn the sector boundary a certain angle (e.g., 45 degrees) to derive another set of sectors, then the neighbors can be correspondingly reassigned in these new sectors and form a different set of direction-aware neighborhoods. Thus, we update the denotation of sectors as $\\mathcal{S}^m \\!=\\! \\{s^m_k \\, | \\, k=0,1,...,n_s\\}$, and that of direction-aware neighborhoods as $\\{\\mathcal{N}_{s^m_k}(v_i) \\, | \\, k=0,1,...,n_s\\}$, where $m=1,2,...,M_s$ denotes the $m$-th head partition among total $M_s$ heads. \nSimilarly, from the distance view, we scale the boundary of the original distance rings to obtain the supplemental partition, which is shown in Figure \\ref{fig_multi_head}(b) and (d). The denotations are updated as $\\mathcal{R}^m \\!=\\! \\{r^m_k \\, | \\, k=0,1,...,n_r\\}$ and $\\{\\mathcal{N}_{r^m_k}(v_i) \\, | \\, k=0,1,...,n_r\\}$ for rings and distance-aware neighborhoods, where $m=1,2,...,M_r$.\nIn this way, different heads of partitions model the spatial relation between neighbors and the central node complementarily, and thus we can avoid the improper grouping under a single partition.\n\n\n\n\\vspace{-1mm}\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Spatial-Aware Aggregation.",
                        "content": "\nAfter the multi-head partition from two spatial perspectives, we collect messages from neighbors to the central node.\nRather than mixing the messages (e.g., through averaging) that most of the GNNs follow \\cite{yang2021diverse}, our model performs a group-wise aggregation to handle different spatial groups with diverse heterophily. An illustration is shown in Figure \\ref{fig_framework}(a).\n\n\n\nFormally, taking the direction view as an example, with the set of direction-aware neighborhoods (spatial groups) under the $m$-head partition $\\{\\mathcal{N}_{s^m_k}(v_i) \\, | \\, k=0,1,...,n_s\\}$, we use graph convolution \\cite{kipf2016semi} to respectively aggregate the features of nodes in each neighborhood $\\mathcal{N}_{s^m_k}(v_i)$ with the normalization by degree:\n\\begin{equation}\n    \\bm{z}_{i,s_k^m}(l+1) = \\sum_{j\\in \\mathcal{N}_{s^m_k}(v_i)} (|\\mathcal{N}(v_i)| \\! \\cdot \\! |\\mathcal{N}(v_j)|)^{-\\frac{1}{2}} \\, \\bm{h}_{j}(l) \\, \\bm{W}_{s_k^m}(l),\n\\vspace{-1mm}\n\\end{equation}\nwhere $\\bm{z}_{i,s_k^m}(l)$ denotes the $l$-layer aggregated message from direction sector $s_k^m$, $\\bm{h}_j(l)$ denotes the $l$-layer features of neighbor $v_j$ with $\\bm{h}_j(0)=\\bm{x}_j$, and $\\bm{W}_{s_k^m}(l)$ is a trainable transformation extracting useful information from neighbors' features.\n\nSimilarly, at the distance view, we also perform the ring-wise aggregation \\hide{with graph convolution }in each distance ring separately by:\n\\begin{equation}\n    \\bm{z}_{i,r_k^m}(l+1) = \\sum_{j\\in \\mathcal{N}_{r^m_k}(v_i)} (|\\mathcal{N}(v_i)| \\! \\cdot \\! |\\mathcal{N}(v_j)|)^{-\\frac{1}{2}} \\, \\bm{h}_{j}(l) \\, \\bm{W}_{r_k^m}(l),\n\\vspace{-1mm}\n\\end{equation}\nwhere $\\bm{W}_{r_k^m}(l)$\n% $\\bm{W}_{r_k^m}^{(l)}$\nis another feature transformation for neighbors at different distances. \nIn this way, the aggregated messages can not only capture the structure information on the graph, but also discriminate their different spatial groups. It avoids losing different distributions of spatial heterophily on the urban graph.\n\n\\vspace{-1mm}\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Heterophily-Sensitive Spatial Interaction",
                    "content": "\n\\label{heter_aware}\nAfter the group-wise feature aggregation, \\mymodel further captures the diverse spatial heterophily in different spatial groups on the urban graph.\nIn detail, we devise two learnable kernel functions to first respectively capture the commonality and discrepancy between the central node and each spatial group. Then, an attentive gate is jointly learned to adaptively determine the ratio of two components that should be propagated to the central node. Additionally, as indicated by the analysis of spatial tendency in Figure \\ref{data_analysis}, the discrepancy of heterophily distributions varies along with the distance between two spatial groups, we further consider such characteristics by allowing the kernel functions to act between groups, which encourages the propagation of common and discrepant information among them. Since we view the central node as an additional group, this process can be regarded as an interaction among every two groups.\nFor simplicity, we omit the index $l$ and $m$ of layer and head in the following discussion.\n\n\\vspace{-1mm}\n\\paragraph{Commonality Kernel Function.}\nGiven the fact that different sectors / rings all belong to the neighborhood of the central node, they may share some common knowledge that can probably enhance the representation of each other. Thus, we first design a commonality kernel function to capture such information among them.\n\nFormally, taking the direction view as an example, with the representation $\\{\\bm{z}_{i,s_k} \\,|\\, k=0,1,...,n_s\\}$ of $n_s$ sectors centered by $v_i$, the kernel function $\\mathcal{K}_C^s(\\cdot, \\cdot)$ that models the commonality degree between sector $s_p$ and $s_q$ (including $v_i$ itself) is defined as:\n\\begin{equation}\n    \\mathcal{K}_C^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}) \\,=\\,\\,<\\!\\hat{\\bm{z}}_{i,s_p}, \\hat{\\bm{z}}_{i,s_q} \\!\\!>, \\,\\,\\, \\hat{\\bm{z}}_{i, s_k} \\!= \\bm{z}_{i, s_k} \\bm{W}_C^s,\n\\end{equation}\nwhere $<\\!\\!\\!\\cdot, \\cdot\\!\\!\\!>$ denotes the inner product and $\\bm{W}_C^s$ is the learnable matrix for common knowledge extraction. The larger output value suggests a higher similarity (i.e., commonality) between inputs.\nBased on this measurement, we enhance the sector representation with the extracted useful information from other sectors:\n\\vspace{-0.5mm}\n\\begin{align}\n    \\bm{z}_{i, s_p}^C &= \\sum_{q=0}^{n_s} \\, \\alpha_{pq}^{C,s} \\, \\bm{z}_{i, s_q} \\bm{W}_C^s, \\\\\n    \\alpha_{pq}^{C,s} &= \\frac{exp(\\,\\mathcal{K}_{C}^{s}(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}))}\n    {\\,\\sum_{k=0}^{n_s} exp(\\mathcal{K}_{C}^{s}(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_k}))},\n\\end{align}\nwhere the coefficient $\\alpha_{pq}^{C,s}$ is the level of commonality normalized by the softmax function. \nIn the same way, we can also obtain the representation of each distance ring $\\bm{z}_{i,r_p}^C$ with the enhancement of common knowledge from other rings using a similar kernel function $\\mathcal{K}_C^r(\\cdot, \\cdot)$ parametrized by $\\bm{W}_C^r$.\n\n\\vspace{-1mm}\n\\paragraph{Discrepancy Kernel Function.}\nIn addition to the common knowledge, modeling the difference information is critical on heterophilic urban graphs. Thus, we devise another kernel function to capture diverse dissimilarity between the central node and every group, as well as between any two groups. We introduce \\hide{elaborate on }it from the direction view.\nSpecifically, taking the original representation $\\{\\bm{z}_{i,s_k} | k=0,1,...,n_s\\}$ as inputs, the discrepancy kernel is defined as:\n\\begin{equation}\n    \\mathcal{K}_D^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}) \\,=\\,\\, <\\! \\bm{z}_{i, s_p} \\bm{W}_D^{s,a}, ( \\bm{z}_{i, s_p} \\bm{W}_D^{s,a} - \\bm{z}_{i, s_q} \\bm{W}_D^{s,b})\\!>,\n\\label{eq_discrepancy_kernel}\n\\end{equation}\nwhere $\\bm{W}_D^{s,a}$ and $\\bm{W}_D^{s,b}$ denote two transformations that learn to extract the difference of sector $s_q$ compared to sector $s_p$.\nAccording to \\cite{hou2020measuring}, the kernel function $\\mathcal{K}_D^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}) $ can be regarded as calculating the dissimilarity degree between two inputs, which tends to be higher when $s_p$ and $s_q$ are more dissimilar.\nSubsequently, our model facilitates each sector to be aware of the helpful difference information from others with this measurement of discrepancy:\n\\vspace{-1mm}\n\\begin{align}\n    \\bm{z}_{i,s_p}^D &= \\sum_{q=0}^{n_s} \\, \\alpha_{pq}^{D,s} \\, (\\bm{z}_{i,s_p} \\bm{W}_D^{s,a} \\!- \\bm{z}_{i,s_q} \\bm{W}_D^{s,b}), \\\\\n    \\alpha_{pq}^{D,s} &= \\frac{exp(\\,\\mathcal{K}_D^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}))}\n    {\\,\\sum_{k=0}^{n_s} exp(\\mathcal{K}_D^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_k}))}.\n\\end{align}\nSimilarly, we utilize an analogous kernel function, which is denoted as $\\mathcal{K}_D^r(\\cdot, \\!\\cdot)$ with parameters $\\bm{W}_D^{r,a}$ and $\\bm{W}_D^{r,b}$ to capture such discrepancy at the distance view and derive the ring representation $ \\bm{z}_{i,r_p}^D$ aware of the diverse distributions of spatial heterophily.\n\n\\vspace{-1mm}\n\\paragraph{Attentive Component Selection.}\nWith these two kernel functions, we can exploit both the common knowledge and diverse discrepancy information between the central node and neighbors in different groups. However, different nodes may possess varying levels of spatial heterophily in various applications. Thus, \\mymodel learns to derive a gate that adaptively determines the ratio of common and difference information \\hide{the central node needs, }in an end-to-end manner.\n\nSpecifically, for each central node $v_i$, we concatenate both the commonality and discrepancy components of all sectors, to derive a scalar with a transformation:\n\\begin{equation}\n    \\beta_i^{s} = \\sigma(\\parallel_{j\\in \\{C, D\\}}\\parallel_{k=0}^{n_s}\\bm{z}_{i,s_k}^j \\bm{W}_t^s) ,\n\\end{equation}\nwhere $\\bm{W}_t^s$ denotes the trainable transformation mapping the input to a scalar and $\\sigma$ denotes the \\textit{Sigmoid} function restricting the output value into $(0,1)$. \nThen, $\\beta_i^s$ serves as a gate controlling the ratio of the commonality and discrepancy components in the final representation of each sector:\n\\begin{equation}\n    \\bm{\\tilde{z}}_{i,s_k} = \\beta_i^{s} \\cdot \\bm{z}_{i,s_k}^C + (1 - \\beta_i^{s}) \\cdot \\bm{z}_{i,s_k}^D.\n\\end{equation}\nIn the same way, we learn to derive the gate $\\beta_{i}^r$ that determines the ratio in each ring's final representation $\\bm{\\tilde{z}}_{i,r_k}$.\n\nAfter the propagation among spatial groups (including the central node), different groups can contain diverse discrepancy information in the neighborhood, which is vital to the heterophilic urban graph. We then integrate this group-wise representation by concatenation (instead of summation, to avoid the mixing of diverse distributions of spatial heterophily) to obtain the global representation of two views. Since we adopt the multi-head partition strategy, a following concatenation is used to combine different heads at each view. The above two processes can be jointly expressed as:\n\\begin{equation}\n    \\bm{h}_{i,s} = \\, \\parallel_{m=1}^{M_s} \\, (\\parallel_{k=0}^{n_s} \\, \\bm{\\tilde{z}}_{i,s_k^m}), \\,\\,\\,\n    \\bm{h}_{i,r} = \\, \\parallel_{m=1}^{M_r} \\, (\\parallel_{k=0}^{n_r} \\, \\bm{\\tilde{z}}_{i,r_k^m}).\n\\end{equation}\n\n\\paragraph{Fusion of Two Spatial Views.}\nFinally, we fuse two spatial views with a learnable weighted summation to update the central node's representation as follows:\n\\begin{equation}\n    \\bm{h}_i = \\gamma \\, \\bm{h}_{i,s} \\bm{W}_f^s + (1-\\gamma) \\, \\bm{h}_{i,r} \\bm{W}_f^r,\n\\end{equation}\nwhere $\\bm{W}_f^s$ and $\\bm{W}_f^r$ are two weight matrices transforming the representation vectors of two views into the same space, and $\\gamma$ is a trainable trade-off parameter activated by \\textit{Sigmoid} function, which learns to assign different importance to the direction and distance view according to the target task. \n\n"
                },
                "subsection 4.3": {
                    "name": "Prediction and Optimization",
                    "content": "\nConsistent with general GNNs, we use the $L$-layer output activated by \\textit{ReLU} function $\\hat{\\bm{h}_i} \\!= \\!\\sigma(\\bm{h}_i^{(L)})$ as the node representation to make a prediction in different downstream tasks, and optimize the model by the appropriate loss function: $\\mathcal{L}(\\,LR(\\hat{\\bm{h}_i})\\,,\\, y_i\\,)$.\nIn node regression tasks, $LR(\\cdot)$ is the linear regressor, $y_i\\!\\in \\!\\mathbb{R}$ denotes the ground truth of labeled nodes, and $\\mathcal{L}$ can be L2 loss.\nWhile for node classification tasks, $LR(\\cdot)$ performs the logistic regression, $y_i\\! \\in \\!\\{0,1\\}^{\\mathcal{C}}$ is a one-hot label vector of $\\mathcal{C}$ classes, and $\\mathcal{L}$ can be cross entropy loss.\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\\label{section_exp}\nIn this section, we conduct extensive experiments on real-world datasets in three different tasks upon two types of urban graphs to evaluate the effectiveness of our model.\nThe code of \\mymodel is available at \n\\url{https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/SHGNN}.\n\n% \\vspace{-1.5mm}\n",
                "subsection 5.1": {
                    "name": "Experiment Settings",
                    "content": "\n",
                    "subsubsection 5.1.1": {
                        "name": "Tasks and Data Description.",
                        "content": "\n\\label{exp_task_description}\nWe first briefly introduce the three datasets corresponding to three different tasks, and details of how to build each dataset are described in Appendix \\ref{apdx_dataset}. Table \\ref{exp-dataset} summarizes the statistical information of the three datasets. \n\n\n\\textbf{Commercial Activeness Prediction (CAP)}\nis a node regression task on a mobility graph. Similar to \\cite{xi2022beyond}, we use the number of comments to POIs in each region as the indicator of regional commercial activeness.\nTo form the dataset of this task, we collect the following urban data in \\textbf{\\textit{Shenzhen}} city in China from Baidu Maps, including POI data and satellite images in September 2019 to construct region features, the daily human flow data from July 2019 to September 2019 for building the edge set of the urban graph, and the number of regional POI comments from June 2019 to April 2020 which is regarded as the ground truth. \n\n\n\n\n\\textbf{Crime Prediction (CP)}\nis also a node regression task on the mobility graph. We collect the real-world dataset of \\textbf{\\textit{New York City}} from NYC open data website\\footnote{opendata.cityofnewyork.us} for this task. The dataset contains 180 regions in Manhattan, with the POI data and crime number in each region, as well as taxi trips between regions \\cite{zhang2021multi,wu2022multi}. We construct node features from POIs. Taxi trips are used to build the urban graph, where we only keep the 20 most important edges for each region w.r.t. the number of trip records.\n\n\\textbf{Dangerous Road Section Detection (DRSD)}\nis a node classification task performed on the road network. In this work, the dangerous section is defined as the road section with a high incidence of traffic accidents. We build a real-world dataset in \\textbf{\\textit{Los Angeles}} based on the road network data from OSMnx Street Networks in Harvard Dataverse\\footnote{https://dataverse.harvard.edu/dataverse/osmnx-street-networks} \nand the traffic accident records in December 2021 from Kaggle dataset website\\footnote{https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents}.\nFirst, we make statistics of the number of accident records on each road section. Then, the sections containing more than 3 accident records in a month will be considered as dangerous road sections in our experiments.\n\nTo select the best hyper-parameters for all the comparing methods, we randomly split each dataset into three parts with $60\\%$ for training, $20\\%$ for validation and $20\\%$ for test. \n\n\\vspace{-1mm}\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Baselines.",
                        "content": "\nWe compare \\mymodel with a variety of state-of-the-art GNN models, including two classical message passing neural networks (\\textbf{GCN} \\cite{kipf2016semi} and \\textbf{GAT} \\cite{velivckovic2018graph}), five representative methods for heterophilic graphs (\\textbf{Mixhop} \\cite{abu2019mixhop}, \\textbf{FAGCN} \\cite{bo2021beyond}, \\textbf{NLGCN} \\cite{liu2021non}, \\textbf{GPRGNN} \\cite{chien2021adaptive} and \\textbf{GBKGNN} \\cite{du2022gbk}), two spatial GNN models\n(\\textbf{SAGNN} \\cite{li2020competitive} and \\textbf{PRIM} \\cite{chen2021points}),\nas well as three task-specific baselines (\\textbf{KnowCL} \\cite{liu2023knowledge} for CAP, \\textbf{NNCCRF} \\cite{yi2019neural} for CP and \\textbf{RFN} \\cite{jepsen2020relational} for DRSD).\nDetailed descriptions are introduced in Appendix \\ref{baseline_describe}.\n\n\\vspace{-1mm}\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Evaluation Metrics.",
                        "content": "\nFor the two regression tasks, we evaluate all methods with Root Mean Square Error (RMSE), Mean Absolute Error (MAE) and the coefficient of determination (R$^2$). For the node classification task, we use Area Under Curve (AUC) and F1-score.\n\n\\vspace{-1mm}\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Performance Evaluation",
                    "content": "\n",
                    "subsubsection 5.2.1": {
                        "name": "Overall Comparison.",
                        "content": "\nThe performance comparison of our \\mymodel and baselines is presented in Table \\ref{table-exp-main}, in which the mean and standard deviation of all metrics are obtained through five random runs.\nAs we can see, \\mymodel consistently achieves the best performance in three tasks on two kinds of urban graphs, with $6.6\\%$ and $11.2\\%$ reductions of RMSE in commercial activeness prediction (CAP) and crime prediction (CP), as well as $7.2\\%$ improvements of AUC in dangerous road section detection (DRSD) over the most competitive baseline of each task. We also conduct a pairwise t-test between \\mymodel and each baseline to demonstrate that our model outperforms all of them significantly.\nNote that although the spatial diversity of heterophily on the road network in DRSD task is not so strong ($\\lambda_d^s=0.23$ and $\\lambda_d^r=0.14$) as another two urban graphs, our model can still improve the accuracy in a large margin. It indicates the effectiveness of \\mymodel can be general but not just limited to urban graphs with strong spatial heterophily.\n\nSpecifically, the ordinary GNN models (GCN and GAT) generally have the worst overall performance.\nBy contrast, approaches designed to deal with graph heterophily (Mixhop, FAGCN, NLGCN, GBKGNN and GPRGNN) evidently perform better. \nIt indicates the inappropriateness of simply treating an urban graph as a general homophilic graph. However, as a specially designed model to handle spatial heterophily, our \\mymodel remarkably outperforms these general heterophilic GNNs.\nThe spatial GNN methods (SAGNN and PRIM) perform better than GCN and GAT sometimes, but perform worse than the methods for heterophilic graphs in many cases. \nAnd for task-specific baselines, it can also be found that some heterophilic GNNs are level pegging with them (such as Mixhop and GPRGNN vs. KnowCL in CAP task, GBKGNN vs. NNCCRF in CP task and Mixhop vs. RFN in DRSD task). \nThese results also demonstrate the importance to take the spatial heterophily into consideration when using GNNs over an urban graph.\nTo sum up, our \\mymodel is much more effective in considering and alleviating the spatial heterophily on urban graphs in all the tasks.\n\n\\vspace{-1.5mm}\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "Ablation Study.",
                        "content": "\nTo verify the effectiveness of each design in our model, we further compare \\mymodel with its five variants:\n\\begin{itemize}[leftmargin=*, topsep=2pt]\n    % \\setlength{\\itemsep}{0pt}\n    % \\setlength{\\parsep}{0pt}\n    % \\setlength{\\parskip}{0pt}\n    \\item \\textbf{\\mymodel-S} removes the direction section partition, which only models spatial heterophily from the distance view.\n    \\item \\textbf{\\mymodel-R} removes the distance ring partition,  which only models spatial heterophily from the direction view.\n    \\item \\textbf{\\mymodel-M} removes the multi-head partition strategy.\n    \\item \\textbf{\\mymodel-C} removes the commonality kernel function, without sharing common knowledge among spatial groups.\n    \\item \\textbf{\\mymodel-D} removes the discrepancy kernel function. It cannot capture the difference among spatial groups.\n\\end{itemize}\nAs shown in Figure \\ref{fig_ablation}, \\mymodel outperforms all the variants, proving the significance of our designs in tackling spatial heterophily. Specifically, the performances get worse if we remove either the partition of direction sections or distance rings (\\mymodel-S and \\mymodel-R), which indicates the necessity of considering the spatial heterophily from both views.\nBesides, the rotation-scaling multi-head partition strategy can evidently help to model diverse spatial relations (\\mymodel-M).\nIn addition, the performance degrades if the commonality kernel function is not used, suggesting the effectiveness of sharing knowledge among neighbors. More importantly, removing the discrepancy kernel function results in a notable performance decline, which verifies the importance of further capturing and exploiting the difference information on heterophilic urban graphs.\n\n\n\n\\vspace{-1mm}\n"
                    },
                    "subsubsection 5.2.3": {
                        "name": "Parameters Analysis.",
                        "content": "\nWe further investigate the influence of several important hyper-parameters on the performance of \\mymodel while keeping other parameters fixed. Figure \\ref{fig_exp_param_nyc} presents the results in CP task, and other results are in Appendix \\ref{addtional_results}.\n\n\\textbf{Number of spatial groups $n_s/n_r$.}\nWe first analyze the effects made by the number of partitioned sectors $n_s$ and rings $n_r$. By increasing $n_s/n_r$ to partition more subspaces, \\mymodel can model more diverse spatial relations and further capture the diversity of spatial heterophily in a more fine-grained manner. However, over-dense partitions bring no further improvements but even slight performance declines. A possible explanation is that some subspaces contain too few neighbors to support its representation learning.\n\n\\textbf{Partition head number $M_s/M_r$.}\nWe also study the impact of head number $M_s/M_r$ in the multi-head partition strategy. It can be observed that, compared to the single partition ($M_s,M_r=1$), \\mymodel using such a strategy ($M_s,M_r=2$) can evidently perform better,\nthanks to the complementing role between two heads. When $M_s$ and $M_r$ continue to increase, the model generally gets fewer further improvements, and too many heads with additional redundancy may sometimes result in performance degradation. Thus, we recommend to set a small head number but larger than 1(e.g., 2), which is good enough while keeping efficiency. \n% \\vspace{-0.5mm}\n"
                    }
                }
            },
            "section 6": {
                "name": "Related Work",
                "content": "\nHere we briefly review two related topics: GNNs for urban applications and  GNNs with heterophily.\n\n% \\textbf{GNNs for Urban Applications.}\n% \\vspace{-1mm}\n% \\paragraph{GNNs for Urban Applications.}\n\\textbf{GNNs for Urban Applications.}\nAs a powerful approach to representing relational data, GNN models \\hide{like GCN \\cite{kipf2016semi} and GAT \\cite{velivckovic2017graph} }are widely adapted in recent studies to learn on urban graphs, and achieve remarkable performance in various \\hide{urban }applications, including traffic \\hide{flow }forecasting \\cite{ xia20213dgcn, song2020spatial, fang2021spatial,yuan2021effective, rao2022fogs, wang2022event, NEURIPS2022_fanliu}, bike demand prediction \\cite{li2022data}, \\hide{road network representation \\cite{wu2020learning},} region embedding \\cite{wu2022multi, zhang2021multi}, regional economy prediction \\cite{xu2020attentional} and special region discovery \\cite{xiao2022contextual}. There are a few works that tend to encode the location information in GNNs' message passing process \\cite{li2020competitive, chen2021points} in a special domain (POI relation prediction). But these methods fail to generalize to urban graphs with heterophily, which may significantly degrade their performance in other urban applications.\n\n% \\vspace{-1mm}\n% \\paragraph{GNNs with Heterophily.}\n\\textbf{GNNs with Heterophily.}\nOur research is also related to the studies of graph heterophily. Here we only make a brief introduction to heterophilic GNNs and refer readers to a recent comprehensive survey \\cite{zheng2022graph}.\nSuch approaches solve the heterophily problem basically in the following two ways.\nThe first branch is to reconstruct the homophilic neighborhood with similar nodes on the graph measured by different criteria. The used criteria \\hide{for node similarity measuring }include the structural similarity defined by the distance in latent space \\cite{pei2019geom} or degree sequence \\cite{suresh2021breaking}, the difference of attention scores \\cite{liu2021non}, cosine similarity of node attributes \\cite{jin2021universal,jin2021node}, nodes' ability to mutually represent each other \\cite{li2022finding} and so on.\nHowever, as pointed out by \\citet{he2022block}, these methods will damage the network topology and tamper with the original real-world dependencies on the urban graph.\n\nAnother branch tends to modify the GNN architecture to handle the difference information on heterophilic graphs, in contrast to the Laplacian smoothing \\cite{wu2019simplifying} of typical GNNs, such as processing neighbors in different classes separately \\cite{du2022gbk,dai2022label}, explicitly aggregating features from higher-order neighbors in each layer \\cite{abu2019mixhop,zhu2020beyond}, \nallowing the high-frequency information by passing signed messages \\cite{yan2021two, bo2021beyond, yang2021diverse, wu2022beyond, luan2021heterophily}, and combining outputs of each layer (including ego features) to also empower the GNNs with a high-pass ability \\cite{chien2021adaptive, zhu2020beyond, chen2020simple}. \nHowever, most of these methods do not consider the diversity of dissimilarity distribution between the central node and different neighbors, especially with different spatial relations.\n\\vspace{-1mm}\n"
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\nIn this paper, we studied the unique spatial heterophily of the urban graph and developed a spatial heterophily-aware graph neural network. We designed a spatial diversity score to uncover the diversity of heterophily at different spatial locations in the neighborhood, and showed the limitation of existing GNNs for handling diverse heterophily distributions on urban graphs. Further, motivated by the analysis that spatially close neighbors present a more similar mode of heterophily, we proposed a novel method, named \\mymodel, which can group spatially close neighbors together and separately process each group with less diversity inside, to tackle the spatial heterophily in a divide-and-conquer way. Finally, extensive evaluations demonstrate the effectiveness of our approach.\n\n\\vspace{-1mm}\n\\begin{acks} \nThis work is supported in part by Foshan HKUST Projects (FSUST21-FYTRI01A, FSUST21-FYTRI02A).\n\\end{acks}\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{reference_abb}\n\n\\clearpage\n\\appendix\n\\nobalance\n"
            },
            "section 8": {
                "name": "Appendix",
                "content": "\n\\label{apdx}\n\\vspace{-2mm}\n\\vspace{-2mm}\n% \\vspace{-2mm}\n% \\vspace{-2mm}\n\n\n\n\n\n\n"
            }
        },
        "figures": {
            "fig_motivation": "\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.91\\textwidth]{figure/partition_and_score_3g.pdf}\n\\vspace{-3.5mm}\n\\caption{Analysis of spatial heterophily. (a)-(b) illustrate the space partition from two spatial views. (c) shows the spatial diversity scores calculated on three real-world urban graphs. (d) presents the results of experimental investigation.}\n\\label{fig_motivation}\n\\vspace{-4mm}\n\\end{figure*}",
            "data_analysis": "\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.92\\textwidth]{figure/data_analysis.pdf}\n \\vspace{-4mm}\n\\caption{Illustration of spatial tendency in the distance view (a) and direction view (b). \n}\n\\label{data_analysis}\n\\vspace{-4mm}\n\\end{figure*}",
            "fig_multi_head": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\columnwidth]{figure/multi_head.pdf}\n \\vspace{-3mm}\n\\caption{Illustration of Rotation-scaling Partition.}\n\\label{fig_multi_head}\n\\vspace{-4mm}\n\\end{figure}",
            "fig_framework": "\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.85\\textwidth]{figure/framework.pdf}\n \\vspace{-2mm}\n\\caption{The architecture of \\mymodel. We only detailedly present the operation under sector partition in the illustration of two kernel functions and the attentive component selection. The same operation is also performed under ring partition.}\n \\vspace{-4mm}\n\\label{fig_framework}\n\\end{figure*}",
            "fig_ablation": "\\begin{figure*}[t]\n\\centering\n    \\subfigure{\n    \\includegraphics[width=0.345\\columnwidth]{figure/ablation_cmt_RMSE.pdf}}\n    \\hspace{-2.5mm}\n    \\subfigure{\n    \\includegraphics[width=0.345\\columnwidth]{figure/ablation_cmt_MAE.pdf}}\n    \\hspace{-2.5mm}\n    \\subfigure{\n    \\includegraphics[width=0.35\\columnwidth]{figure/ablation_nyc_RMSE.pdf}}\n    \\hspace{-2.5mm}\n    \\subfigure{\n    \\includegraphics[width=0.35\\columnwidth]{figure/ablation_nyc_MAE.pdf}}\n    \\hspace{-2.5mm}\n    \\subfigure{\n    \\includegraphics[width=0.35\\columnwidth]{figure/ablation_us_dw_rev_AUC.pdf}}\n    \\hspace{-2.5mm}\n    \\subfigure{\n    \\includegraphics[width=0.35\\columnwidth]{figure/ablation_us_dw_rev_F1.pdf}}\n   \\vspace{-6mm}\n  \\caption{Performance comparison between \\mymodel and its variants on three tasks.}\n\\vspace{-5.5mm}\n\\label{fig_ablation}\n\\end{figure*}",
            "fig_exp_param_nyc": "\\begin{figure}[t]\n\\centering\n\\vspace{-1.5mm}\n\\subfigure{\n    \\includegraphics[width=0.45\\columnwidth]{figure/param_nyc_secnum.pdf}}\n\\subfigure{\n    \\includegraphics[width=0.45\\columnwidth]{figure/param_nyc_bktnum.pdf}} \\\\\n    \\vspace{-4.5mm}\n\\subfigure{\n    \\includegraphics[width=0.45\\columnwidth]{figure/param_nyc_sechead.pdf}}\n\\subfigure{\n    \\includegraphics[width=0.45\\columnwidth]{figure/param_nyc_bkthead.pdf}}\n\\vspace{-5mm}\n\\caption{Parameter analysis in CP task.}\n\\vspace{-6mm}\n\\label{fig_exp_param_nyc}\n\\end{figure}",
            "fig_exp_param_cmt": "\\begin{figure}[t]\n\\centering\n\\vspace{-1mm}\n\\subfigure{\n    \\includegraphics[width=0.43\\columnwidth]{figure/param_cmt_secnum.pdf}}\n\\subfigure{\n    \\includegraphics[width=0.43\\columnwidth]{figure/param_cmt_bktnum.pdf}} \\\\\n    \\vspace{-4mm}\n\\subfigure{\n    \\includegraphics[width=0.43\\columnwidth]{figure/param_cmt_sechead.pdf}}\n\\subfigure{\n    \\includegraphics[width=0.43\\columnwidth]{figure/param_cmt_bkthead.pdf}}\n\\vspace{-5.5mm}\n\\caption{Parameter analysis in CAP task.}\n\\vspace{-4mm}\n\\label{fig_exp_param_cmt}\n\\end{figure}",
            "fig_exp_param_drsd": "\\begin{figure}[t]\n\\centering\n% \\vspace{-2mm}\n\\subfigure{\n    \\includegraphics[width=0.45\\columnwidth]{figure/param_us_dw_rev_secnum.pdf}}\n\\subfigure{\n    \\includegraphics[width=0.45\\columnwidth]{figure/param_us_dw_rev_bktnum.pdf}} \\\\\n    \\vspace{-4mm}\n\\subfigure{\n    \\includegraphics[width=0.45\\columnwidth]{figure/param_us_dw_rev_sechead.pdf}}\n\\subfigure{\n    \\includegraphics[width=0.45\\columnwidth]{figure/param_us_dw_rev_bkthead.pdf}}\n\\vspace{-5.5mm}\n\\caption{Parameter analysis in DRSD task.}\n\\vspace{-1mm}\n\\label{fig_exp_param_drsd}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    P_{i,c}^{s_k} = \\sum_{v_j \\in \\mathcal{N}_{s_k}(v_i) \\land y_j \\neq y_i } \\!\\!\\! \\mathbbm{1}(y_{j,c} = 1) \\cdot |\\mathcal{N}_{s_k}(v_i)|^{-1},\n\\vspace{-1mm}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    P_{i,c}^{s_k} = \\sum_{v_j \\in \\mathcal{N}_{s_k}(v_i)} \\!\\!\\! \\mathbbm{1}(D_c < y_j - y_i \\leq D_{c+1}) \\cdot |\\mathcal{N}_{s_k}(v_i)|^{-1},\n\\vspace{-1mm}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{disc_eq}\n    Disc(v_i, s_p, s_q) = WD(P_i^{s_p}, P_i^{s_q}) ,\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\lambda_d^{s} = |\\mathcal{V}|^{-1} \\cdot \\!\\! \\sum_{v_i \\in \\mathcal{V}} \\mathbbm{1}(\\max_{p \\neq q}Disc(v_i, s_p, s_q) \\geq 1) ,\n\\vspace{-0.5mm}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\lambda_d^{r} = |\\mathcal{V}|^{-1} \\cdot \\!\\! \\sum_{v_i \\in \\mathcal{V}} \\mathbbm{1}(\\max_{p \\neq q}Disc(v_i, r_p, r_q) \\geq 1) ,\n\\vspace{-0.5mm}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\bm{z}_{i,s_k^m}(l+1) = \\sum_{j\\in \\mathcal{N}_{s^m_k}(v_i)} (|\\mathcal{N}(v_i)| \\! \\cdot \\! |\\mathcal{N}(v_j)|)^{-\\frac{1}{2}} \\, \\bm{h}_{j}(l) \\, \\bm{W}_{s_k^m}(l),\n\\vspace{-1mm}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\bm{z}_{i,r_k^m}(l+1) = \\sum_{j\\in \\mathcal{N}_{r^m_k}(v_i)} (|\\mathcal{N}(v_i)| \\! \\cdot \\! |\\mathcal{N}(v_j)|)^{-\\frac{1}{2}} \\, \\bm{h}_{j}(l) \\, \\bm{W}_{r_k^m}(l),\n\\vspace{-1mm}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\mathcal{K}_C^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}) \\,=\\,\\,<\\!\\hat{\\bm{z}}_{i,s_p}, \\hat{\\bm{z}}_{i,s_q} \\!\\!>, \\,\\,\\, \\hat{\\bm{z}}_{i, s_k} \\!= \\bm{z}_{i, s_k} \\bm{W}_C^s,\n\\end{equation}",
            "eq:9": "\\begin{align}\n    \\bm{z}_{i, s_p}^C &= \\sum_{q=0}^{n_s} \\, \\alpha_{pq}^{C,s} \\, \\bm{z}_{i, s_q} \\bm{W}_C^s, \\\\\n    \\alpha_{pq}^{C,s} &= \\frac{exp(\\,\\mathcal{K}_{C}^{s}(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}))}\n    {\\,\\sum_{k=0}^{n_s} exp(\\mathcal{K}_{C}^{s}(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_k}))},\n\\end{align}",
            "eq:10": "\\begin{equation}\n    \\mathcal{K}_D^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}) \\,=\\,\\, <\\! \\bm{z}_{i, s_p} \\bm{W}_D^{s,a}, ( \\bm{z}_{i, s_p} \\bm{W}_D^{s,a} - \\bm{z}_{i, s_q} \\bm{W}_D^{s,b})\\!>,\n\\label{eq_discrepancy_kernel}\n\\end{equation}",
            "eq:11": "\\begin{align}\n    \\bm{z}_{i,s_p}^D &= \\sum_{q=0}^{n_s} \\, \\alpha_{pq}^{D,s} \\, (\\bm{z}_{i,s_p} \\bm{W}_D^{s,a} \\!- \\bm{z}_{i,s_q} \\bm{W}_D^{s,b}), \\\\\n    \\alpha_{pq}^{D,s} &= \\frac{exp(\\,\\mathcal{K}_D^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_q}))}\n    {\\,\\sum_{k=0}^{n_s} exp(\\mathcal{K}_D^s(\\bm{z}_{i,s_p}, \\bm{z}_{i,s_k}))}.\n\\end{align}",
            "eq:12": "\\begin{equation}\n    \\beta_i^{s} = \\sigma(\\parallel_{j\\in \\{C, D\\}}\\parallel_{k=0}^{n_s}\\bm{z}_{i,s_k}^j \\bm{W}_t^s) ,\n\\end{equation}",
            "eq:13": "\\begin{equation}\n    \\bm{\\tilde{z}}_{i,s_k} = \\beta_i^{s} \\cdot \\bm{z}_{i,s_k}^C + (1 - \\beta_i^{s}) \\cdot \\bm{z}_{i,s_k}^D.\n\\end{equation}",
            "eq:14": "\\begin{equation}\n    \\bm{h}_{i,s} = \\, \\parallel_{m=1}^{M_s} \\, (\\parallel_{k=0}^{n_s} \\, \\bm{\\tilde{z}}_{i,s_k^m}), \\,\\,\\,\n    \\bm{h}_{i,r} = \\, \\parallel_{m=1}^{M_r} \\, (\\parallel_{k=0}^{n_r} \\, \\bm{\\tilde{z}}_{i,r_k^m}).\n\\end{equation}",
            "eq:15": "\\begin{equation}\n    \\bm{h}_i = \\gamma \\, \\bm{h}_{i,s} \\bm{W}_f^s + (1-\\gamma) \\, \\bm{h}_{i,r} \\bm{W}_f^r,\n\\end{equation}"
        },
        "git_link": "https://github.com/PaddlePaddle/PaddleSpatial/"
    }
}