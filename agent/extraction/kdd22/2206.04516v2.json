{
    "meta_info": {
        "title": "Accurate Node Feature Estimation with Structured Variational Graph  Autoencoder",
        "abstract": "Given a graph with partial observations of node features, how can we estimate\nthe missing features accurately? Feature estimation is a crucial problem for\nanalyzing real-world graphs whose features are commonly missing during the data\ncollection process. Accurate estimation not only provides diverse information\nof nodes but also supports the inference of graph neural networks that require\nthe full observation of node features. However, designing an effective approach\nfor estimating high-dimensional features is challenging, since it requires an\nestimator to have large representation power, increasing the risk of\noverfitting. In this work, we propose SVGA (Structured Variational Graph\nAutoencoder), an accurate method for feature estimation. SVGA applies strong\nregularization to the distribution of latent variables by structured\nvariational inference, which models the prior of variables as Gaussian Markov\nrandom field based on the graph structure. As a result, SVGA combines the\nadvantages of probabilistic inference and graph neural networks, achieving\nstate-of-the-art performance in real datasets.",
        "author": "Jaemin Yoo, Hyunsik Jeon, Jinhong Jung, U Kang",
        "link": "http://arxiv.org/abs/2206.04516v2",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "Fixed Table 4 from the KDD 2022 version"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n\\emph{Given a graph with partial observations of node features, how can we estimate the missing features accurately?}\nMany real-world data are represented as graphs to model the relationships between entities.\nSocial networks, seller-item graphs in electronic commerce, and user-movie graphs in a streaming service are all examples of graph data that have been studied widely in literature \\cite{Yoo2017, Kipf2017, Velickovic2018, Shchur2018, Hu2020}.\nSuch graphs become more powerful when combined with feature vectors that describe the diverse properties of nodes \\cite{Duong2019, Yang2020}.\n\nHowever, node features are commonly missing in a real-world graph.\nUsers in an online social network set their profiles private, and sellers in electronic commerce often register items without an informative description.\nIn such cases, even the observed features cannot be used properly due to the missing ones, since many graph algorithms assume the full observation of node features.\nFigure~\\ref{fig:motivation} illustrates the feature estimation problem in an example graph.\nAn accurate estimation of missing features not only provides diverse information of node properties but also improves the performance of essential tasks such as node classification or link prediction by providing important evidence for training a classifier.\n\n\n\nHowever, accurate estimation of missing features is challenging due to the following reasons.\nFirst, target nodes have no specific information that describes their properties.\nThe main evidence for estimation is the graph structure, which gives only partial information of nodes based on the relationships with the other nodes.\nSecond, the target variables are high-dimensional vectors containing up to thousands of elements.\nThis requires large representation power for accurate estimation, involving a high risk of overfitting as a consequence.\nExisting approaches \\cite{Huang2019, Chen2019, Chen2020} failed to address such challenges effectively, resulting in limited performance.\n\nWe propose \\method (\\methodlong), an accurate method for missing feature estimation.\nThe main idea for addressing the challenges is to run \\emph{structured} variational inference to effectively regularize the distribution of latent variables by modeling their correlations from the structure of a graph.\nWe first propose stochastic inference, which models the prior of latent variables as Gaussian Markov random field (GMRF).\n%The main idea for addressing the challenges of problem is to run \\emph{structured} variational inference without ignoring the correlations between target variables.\n%We first propose stochastic inference with modeling the prior of latent variables as Gaussian Markov random field (GMRF), using the graph structure as an effective regularizer for the distribution of node representations.\nThen, we improve the stability of inference with our proposed deterministic modeling, which results in a new graph-based regularizer.\nThese allow us to avoid the overfitting without degrading the representation power, achieving state-of-the-art performance in real-world datasets.\n\nOur contributions are summarized as follows:\n\\begin{itemize}\n\t\\item \\textbf{Method.}\n\t\tWe propose \\method, an accurate method for missing feature estimation.\n\t\t\\method introduces a new way to run variational inference on graph-structured data with modeling the correlations between target variables as GMRF.\n%\n\t\\item \\textbf{Theory.}\n\t\tWe analyze the theoretical properties of structured variational inference with the stochastic and deterministic modeling.\n\t\tWe also analyze the time and space complexities of our \\method, which are both linear with the number of nodes and edges of a given graph, showing its scalability.\n%\n\t\\item \\textbf{Experiments.}\n\t\tExtensive experiments on eight real-world datasets show that \\method provides state-of-the-art performance with up to 16.3\\% higher recall and 14.0\\% higher nDCG scores in feature estimation, and up to 14.2\\% higher accuracy in node classification compared to the best competitors.\n\\end{itemize}\n\nThe rest of this paper is organized as follows.\nIn Section \\ref{sec:preliminaries}, we introduce the problem definition and preliminaries of \\method.\nIn Section \\ref{sec:proposed-approach}, we propose \\method and discuss its theoretical properties.\nWe present experimental results in Section \\ref{sec:experiments} and describe related works in Section \\ref{sec:related-works}.\nWe conclude in Section \\ref{sec:conclusion}.\nThe code and datasets are available at \\underline{\\smash{\\url{https://github.com/snudatalab/SVGA}}}. \n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\label{sec:preliminaries}\n\nWe introduce the problem definition and preliminaries, including Gaussian Markov random field and variational inference.\n%Symbols used frequently in this paper are summarized in Appendix \\ref{appendix:symbols}.\n\n",
                "subsection 2.1": {
                    "name": "Missing Feature Estimation",
                    "content": "\n\\label{subsec:problem-definition}\n\nThe feature estimation problem is defined as follows.\nWe have an undirected graph $G = (\\mathcal{V}, \\mathcal{E})$, where $\\mathcal{V}$ and $\\mathcal{E}$ represent the sets of nodes and edges, respectively.\nA feature vector $\\mathbf{x}_i$ exists for every node $i$, but is observable only for a subset $\\mathcal{V}_x \\subset \\mathcal{V}$ of nodes.\nOur goal is to predict the missing features of \\emph{test} nodes $\\mathcal{V} \\setminus \\mathcal{V}_x$ using the structure of $G$ and the observations for $\\mathcal{V}_x$.\nThe problem differs from generative learning \\cite{Kingma2014} in that there exist correct answers; generative learning is typically an unsupervised problem.\n\nWe also assume that the label $y_i$ of each node $i$ can be given as an additional input for a set $\\mathcal{V}_y$ of nodes such that $\\mathcal{V}_y \\subseteq \\mathcal{V}$.\nSuch labels improve the accuracy of feature estimation, especially when they provide information for the test nodes: $\\mathcal{V}_y \\cap (\\mathcal{V} \\setminus \\mathcal{V}_x) \\neq \\emptyset$.\nThis is based on the idea that categorical labels are often easier to acquire than high-dimensional features, and knowing the labels of target nodes gives a meaningful advantage for estimation.\nThus, we design our framework to be able to work with $\\mathcal{V}_y \\neq \\emptyset$, although we consider $\\mathcal{V}_y = \\emptyset$ as a base setup of experiments for the consistency with previous approaches that take only the observed features.\n\n%\\textbf{Evaluation.}\n%We evaluate the performance of feature estimation in two ways.\n%First, we directly compare predictions with the true features that are unknown at the training time.\n%Second, we solve the node classification problem utilizing the generated features to quantify how well they model the relationships between nodes in terms of labels.\n%This is based on the idea that generated features can be informative for solving other tasks regardless of the error from the true features.\n%Detailed information is in Section \\ref{ssec:exp-setup}.\n\n"
                },
                "subsection 2.2": {
                    "name": "Gaussian Markov Random Field",
                    "content": "\n\n\n\n\n\nGaussian Markov random field (GMRF) \\cite{Koller2009} is a graphical model that represents a multivariate Gaussian distribution.\nGiven a graph $G = (\\mathcal{V}, \\mathcal{E})$ whose nodes have continuous signals that are correlated by the graph structure, GMRF represents the distribution of signals with two kinds of potential functions $\\psi_i$ and $\\psi_{ij}$ for every node $i$ and edge $(i, j)$, respectively.\nWe assume the signal of each node $i$ as a random variable $Z_i$ with a possible value $z_i$.\n\nSpecifically, the node potential $\\psi_i$ for each node $i$ and the edge potential $\\psi_{ij}$ for each edge $(i, j)$ are defined as follows:\n\\begin{align}\n\t& \\psi_i(z_i) = \\exp(-0.5K_{ii}z_i^2 + h_i z_i) \\label{eq:mrf-np} \\\\\n\t& \\psi_{ij}(z_i, z_j) = \\exp(-K_{ij}z_iz_j), \\label{eq:mrf-ep}\n\\end{align}\nwhere $\\mathbf{h} \\in \\mathbb{R}^n$ and $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ are the parameters of the GMRF, and $n$ is the number of nodes.\nThe nonzero elements of $\\mathbf{K}$ correspond to the edges of the graph as depicted in Figure \\ref{fig:gmrf}.\n\nThen, the joint probability $p(\\mathbf{z})$ is given as the multiplication of all potential functions:\n\\begin{equation}\n\tp(\\mathbf{z}) = \\frac{1}{C} \\prod_{i \\in \\mathcal{V}} \\psi_i(z_i) \\prod_{(i, j) \\in \\mathcal{E}} \\psi_{ij}(z_i, z_j),\n\t\\label{eq:mrf-joint}\n\\end{equation}\nwhere $C$ is a normalization constant.\nEach potential measures how likely $z_i$ or $(z_i, z_j)$ appears with the current probabilistic assumption with the parameters $\\mathbf{h}$ and $\\mathbf{K}$, and the joint probability is computed by multiplying the potentials for all nodes and edges.\n\nThe roles of parameters $\\mathbf{K}$ and $\\mathbf{h}$ can be understood with respect to the distribution that GMRF represents.\nLemma \\ref{lemma:gmrf} shows that GMRF is equivalent to a multivariate Gaussian distribution whose mean and covariance are determined by $\\mathbf{K}$ and $\\mathbf{h}$.\n$\\mathbf{K}$ is the inverse of the covariance $\\Sigma$, and a pair of signals $z_i$ and $z_j$ is more likely to be observed if $K_{ij}$ is small.\n$\\mathbf{h}$ determines the mean of the signals if $\\mathbf{K}$ is fixed, and is typically set to zero as we assume no initial bias of signals for the simplicity of computation.\n\n\\begin{lemma}\n\tThe joint probability of Equation \\eqref{eq:mrf-joint} is the same as the probability density function of a multivariate Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$, where $\\mu = \\mathbf{K}^{-1}\\mathbf{h}$ and $\\Sigma = \\mathbf{K}^{-1}$.\n\t\\label{lemma:gmrf}\n\\end{lemma}\n\n\\begin{proof}\n\t\\vspace{-1mm}\n\tSee Appendix \\ref{appendix:proof-gmrf}.\n\t\\vspace{-1mm}\n\\end{proof}\n\nWe utilize GMRF to incorporate a real-world graph in a probabilistic framework.\nSpecifically, we generate a multivariate Gaussian distribution that models the probabilistic relationships between nodes by designing $\\mathbf{K}$ and $\\mathbf{h}$ from the adjacency matrix $\\mathbf{A}$ of the given graph.\nGMRF plays a crucial role in our proposed approach, which aims to run variational inference in graph-structured data without ignoring the correlations between target variables.\n\n"
                },
                "subsection 2.3": {
                    "name": "Variational Inference for Joint Learning",
                    "content": "\n\nVariational inference \\cite{Kingma2014, Kipf2016, Tomczak2020} is a technique for approximating intractable posterior distributions, which has been used widely for generative learning.\nGiven the adjacency matrix $\\adj$ of a graph, our goal is to find optimal parameters $\\Theta$ that maximize the likelihood $p_\\Theta(\\xall, \\yall \\mid \\adj)$ of observed features $\\xall$ and labels $\\yall$.\nWe introduce a latent variable $\\mathbf{z}_i \\in \\mathbb{R}^d$ for each node $i$ and denote the realization of all latent variables by $\\mathbf{Z} \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of nodes and $d$ is the size of variables.\nThe latent variable $\\mathbf{z}_i$ represents the characteristic of each node $i$ for estimating its feature $\\mathbf{x}_i$.\n\nWith variational inference, we change the problem into maximizing the evidence lower bound (ELBO):\n\\begin{equation}\n\\begin{split}\n\t&\\log p_\\Theta(\\xall, \\yall \\mid \\adj) \\geq \\elbo \\\\\n\t&\\quad \\quad = \\mathbb{E}_{\\zvar \\sim \\zdist} [\\log p_{\\theta, \\rho} (\\xall, \\yall \\mid \\zvar, \\adj)] \\\\\n\t&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad - \\kld{\\zdist}{p(\\zvar \\mid \\adj)},\n\\end{split}\n\\label{eq:elbo-1}\n\\end{equation}\nwhere $\\elbo$ is the ELBO, $q_\\phi$ is a parameterized distribution of $\\zvar$, and $p_{\\theta, \\rho}$ is a parameterized distribution of $\\xall$ and $\\yall$.\nThe first term of $\\elbo$ is the likelihood of observed variables given $\\zvar$, while the second term measures the difference between $\\zdist$ and the prior distribution $p(\\zvar \\mid \\adj)$ by the KL divergence.\n\nWe assume the conditional independence between $\\xall$, $\\yall$, and $\\adj$ given $\\zvar$, expecting that each variable $\\mathbf{z}_i$ has sufficient information of node $i$ to generate its feature $\\mathbf{x}_i$ and label $y_i$.\nThen, the first term of $\\elbo$ in Equation \\eqref{eq:elbo-1} is rewritten as follows:\n\\begin{multline}\n\t\\mathbb{E}_{\\zvar \\sim q_\\phi (\\zvar \\mid \\xall, \\yall, \\adj)} [\\log p_{\\theta, \\rho} (\\xall, \\yall \\mid \\zvar, \\adj)] \\\\\n\t= \\mathbb{E}_{\\zvar \\sim q_\\phi (\\zvar \\mid \\cdot)} \\Bigl[\\sum_{i \\in \\xset} \\log p_\\theta (\\mathbf{x}_i \\mid \\mathbf{z}_i) + \\sum_{i \\in \\yset} \\log p_\\rho (y_i \\mid \\mathbf{z}_i) \\Bigr],\n\\label{eq:elbo-3}\n\\end{multline}\nwhere $\\xset$ and $\\yset$ are the sets of nodes whose features and labels are observed, respectively, and $q_\\phi (\\zvar \\mid \\cdot)$ denotes $q_\\phi (\\zvar \\mid \\xall, \\yall, \\adj)$.\n\nEquation \\eqref{eq:elbo-3} represents the conditional likelihood of observed features and labels given $\\mathbf{Z}$.\nThus, maximizing Equation \\eqref{eq:elbo-3} is the same as minimizing the reconstruction error of observed variables in typical autoencoders.\nOn the other hand, the KL divergence term in Equation \\eqref{eq:elbo-1} works as a regularizer that forces the distribution $\\zdist$ of latent variables to be close to the prior $p(\\zvar \\mid \\adj)$.\nThe characteristic of regularization depends on how we model the prior $p(\\zvar \\mid \\adj)$, which plays an essential role in our framework.\n\nNote that the objective function of Equation \\eqref{eq:elbo-1} works whether the observed labels $\\yall$ are given or not, due to our assumption on the conditional independence between $\\xall$ and $\\yall$.\nOnly the first term of Equation \\eqref{eq:elbo-3} is used if there are no observed labels.\n\n"
                }
            },
            "section 3": {
                "name": "Proposed Method",
                "content": "\n\\label{sec:proposed-approach}\n\nWe propose \\method (\\methodlong), an accurate method for missing feature estimation.\nThe main ideas of \\method are summarized as follows:\n\\begin{itemize}\n\t\\item \\textbf{GNN with identity node features (Sec. \\ref{ssec:method-gnn-modeling}).}\n\t\tWe address the deficiency of input features by utilizing a graph neural network (GNN) with identity node features as an encoder function, which allows us to learn an independent embedding vector for each node during the training.\n%\t\t\n\t\\item \\textbf{Structured variational inference (Sec. \\ref{ssec:method-variational-inference}).}\n\t\tWe propose a new way to run variational inference on graph-structured data without ignoring the correlations between target examples.\n\t\tThis is done by modeling the prior distribution of latent variables with Gaussian Markov random field (GMRF).\n%\t\t\n\t\\item \\textbf{Unified deterministic modeling (Sec. \\ref{ssec:deterministic-modeling}).}\n\t\tWe improve the stability of inference by changing the stochastic sampling of latent variables into a deterministic process.\n\t\tThis makes the KL divergence term of ELBO as a general regularizer that controls the space of node representations.\n\\end{itemize}\n\nIn Section \\ref{ssec:method-gnn-modeling}, we introduce the overall structure of \\method and the objective function for its training.\nThen in Sections \\ref{ssec:method-variational-inference} and \\ref{ssec:deterministic-modeling}, we induce our graph-based regularizer from structured variational inference.\nSpecifically, we propose the basic parameterization of structured inference in Section \\ref{ssec:method-variational-inference} and improve its stability with the deterministic modeling of latent variables in Section \\ref{ssec:deterministic-modeling}.\n\n",
                "subsection 3.1": {
                    "name": "Overall Structure of \\method",
                    "content": "\n\\label{ssec:method-gnn-modeling}\n\nFigure \\ref{fig:overview} shows the overall structure of \\method, which consists of an encoder $f$ and two decoder networks $g_x$ and $g_y$.\nThe networks $f$, $g_x$ and $g_y$ are designed to estimate the target distributions of the ELBO of Equation \\eqref{eq:elbo-1}: $\\zdist$, $p_\\theta(\\mathbf{x}_i \\mid \\mathbf{z}_i)$ and $p_\\rho (y_i \\mid \\mathbf{z}_i)$, respectively, where $\\phi$, $\\theta$, and $\\rho$ are their parameters.\nThe encoder $f$ generates latent representations of nodes, and the decoders $g_x$ and $g_y$ use the generated representations to estimate the features and labels of nodes.\nThe feature decoder $g_x$ makes the final estimation of missing features, while the label decoder $g_y$ helps the training of $g_x$ and is not used if no labels are observed.\n\n",
                    "subsubsection 3.1.1": {
                        "name": "Encoder Network",
                        "content": "\n\nThe encoder network $f$ aims to model the latent distribution $\\zdist$ with parameters $\\phi$.\nWe propose to use a graph neural network (GNN) as $f$, because the main functionality required for $f$ is to generate an embedding vector for each node following the graphical structure.\nIn experiments, we adopt a simple graph convolutional network (GCN) \\cite{Kipf2017} as $f$, which works well even when the amount of training data is insufficient.\n\nStill, it is required that every node contains a feature vector to run the GNN encoder on the given graph.\nOnly a few nodes have observed features in our case, and it makes an imbalance between nodes with and without observed features.\nThus, we use the identity matrix $\\mathbf{I} \\in \\mathbb{R}^{n \\times n}$ as the input of $f$, using the observed features only as the answer for the training of \\method.\nThis allows $f$ to learn an independent embedding for each node at its first layer and to have sufficient capacity to generate diverse node representations.\n%\\footnote{\\blue{We tried various types of initial features such as random normal vectors or structural features made from the adjacency matrix, but the identity matrix worked the best.}}\n\nIf we use a GCN with two layers as in previous work \\cite{Kipf2017}, the encoder function $f$ is defined as $f(\\adj; \\phi) = \\hat{\\adj} (\\sigma(\\hat{\\adj} \\mathbf{I} \\mathbf{W}_1)) \\mathbf{W}_2$, where {\\small $\\hat{\\mathbf{A}} = \\tilde{\\mathbf{D}}^{-1/2} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-1/2}$} is the normalized adjacency matrix, $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ is the adjacency matrix with self-loops, $\\tilde{\\mathbf{D}}$ is the degree matrix such that {\\small $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$, and $\\sigma$} is the ReLU function.\n{\\small $\\mathbf{W}_1 \\in \\mathbb{R}^{n \\times d}$} and {\\small $\\mathbf{W}_2 \\in \\mathbb{R}^{d \\times d}$} are the weight matrices of layers 1 and 2, respectively, where $n$ is the number of nodes, and $d$ is the size of latent variables.\nWe do not represent the bias terms for brevity.\nNote that the node feature matrix of the original formulation of GCN \\cite{Kipf2017} is replaced with the identity matrix $\\mathbf{I}$ based on our idea of identity node features.\n\n\\begin{algorithm}[t]\n\\caption{\n\tTraining of \\method with deterministic inference.\n}\n\\begin{algorithmic}[1]\n\t\\Require Adjacency matrix $\\mathbf{A}$, diagonal adjacency $\\mathbf{D}$, feature $\\mathbf{X}$, (optional) one-hot label $\\mathbf{Y}$, hyperparameters $\\alpha$, $\\beta$ and $\\lambda$, networks $f$, $g_x$, and $g_y$, and their parameters $\\phi$, $\\theta$, and $\\rho$, respectively\n\t\\Ensure Updated parameters $\\phi'$, $\\theta'$, and $\\rho'$\n\t\\State $\\mathbf{Z} \\leftarrow \\mathbf{E} \\leftarrow f(\\mathbf{A}; \\phi)$ \\Comment Run the unified encoder\n\t\\State $\\hat{\\mathbf{X}}, \\hat{\\mathbf{Y}} \\leftarrow g_x (\\mathbf{Z}, \\mathbf{A}; \\theta), g_y (\\mathbf{Z}, \\mathbf{A}; \\rho)$ \\Comment Make predictions\n\t\\State $l_{xy} \\leftarrow \\sum_i l_x(\\hat{\\mathbf{x}}_i, \\mathbf{x}_i) + \\sum_j l_y(\\hat{\\mathbf{y}}_j, \\mathbf{y}_j)$ \\Comment Equation \\eqref{eq:loss-x} to \\eqref{eq:loss-x3}\n\t\\State $\\mathbf{K} \\leftarrow \\mathbf{I} - \\mathbf{D}^{-1/2} \\adj \\mathbf{D}^{-1/2}$ \\Comment Equation \\eqref{eq:info-matrix}\n\t\\State $l_\\mathrm{GMRF} \\leftarrow \\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E}) - \\alpha \\log|\\mathbf{I} + \\beta^{-1}\\mathbf{E}^\\top \\mathbf{E}|$  \\Comment Equation \\eqref{eq:kl-divergence-2}\n\t\\State$\\phi', \\theta', \\rho' \\leftarrow$ Update $\\phi, \\theta, \\rho$ to minimize $l_{xy} + \\lambda l_\\mathrm{GMRF}$\n\\end{algorithmic}\n\\label{alg:overview}\n\\end{algorithm}\n\n\\textbf{Unit normalization.}\nA possible limitation of introducing the identity feature matrix is the large size of $\\mathbf{W}_1$, which can make the training process unstable.\nThus, we project the latent representations $\\mathbf{Z}$ generated from the encoder $f$ into a unit hypersphere by normalizing each vector of node $i$ as $\\mathbf{z}_i / \\|\\mathbf{z}_i \\|_2$.\nThis does not alter the main functionality of making diverse representations of nodes for making high-dimensional features, but improves the stability of training by restricting the output space \\cite{Ying2018}.\n\n"
                    },
                    "subsubsection 3.1.2": {
                        "name": "Decoder Networks",
                        "content": "\nWe propose two decoder networks $g_x$ and $g_y$ to model $p_\\theta(\\mathbf{x}_i \\mid \\mathbf{z}_i)$ and $p_\\rho (y_i \\mid \\mathbf{z}_i)$, respectively.\nWe assume that latent variables $\\mathbf{Z}$ have sufficient information to construct the observed features and labels.\nThus, we minimize the complexity of decoder networks by adopting the simplest linear transformation as $g_x(\\mathbf{z}_i) = \\mathbf{W}_x \\mathbf{z}_i + \\mathbf{b}_x$ and $g_y(\\mathbf{z}_i) = \\mathbf{W}_y \\mathbf{z}_i + \\mathbf{b}_y$, where {\\small $\\mathbf{W}_x \\in \\mathbb{R}^{m \\times d}$}, {\\small $\\mathbf{W}_y \\in \\mathbb{R}^{c \\times d}$}, $\\mathbf{b}_x \\in \\mathbb{R}^m$ and $\\mathbf{b}_y \\in \\mathbb{R}^c$ are learnable weights and biases, $m$ is the number of features, and $c$ is the number of classes.\n\n"
                    },
                    "subsubsection 3.1.3": {
                        "name": "Optimization",
                        "content": "\n\nWe update the parameters of all the networks $f$, $g_x$, and $g_y$ in an end-to-end way.\nWe rewrite the ELBO of Equation \\eqref{eq:elbo-1} as the following objective function to be minimized:\n\\begin{equation}\n\tl(\\Theta) = \\sum_{i \\in \\mathcal{V}_x} l_x(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) + \\sum_{i \\in \\mathcal{V}_y} l_y(\\hat{\\mathbf{y}}_i, \\mathbf{y}_i) + \\lambda l_\\mathrm{GMRF}(\\mathbf{Z}, \\adj),\n\\label{eq:obj-function}\n\\end{equation}\nwhere $l_x$ and $l_y$ are loss terms for features and labels, respectively.\n$l_\\mathrm{GMRF}$ is our proposed regularizer, whose details are described in Sections \\ref{ssec:method-variational-inference} and \\ref{ssec:deterministic-modeling} through the process of structured inference.\nWe use a hyperparameter $\\lambda$ for the amount of regularization.\n\nThe loss terms $l_x$ and $l_y$ are determined by how we model the distributions $p_\\theta(\\mathbf{x}_i \\mid \\mathbf{z}_i)$ and $p_\\rho (y_i \\mid \\mathbf{z}_i)$ following the distribution of true data.\nCommon distributions for features include Gaussian, Bernoulli, and categorical (or one-hot) distributions:\n\\begin{equation}\n\tl_x(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) = \\begin{cases}\n\t\tl_{\\mathrm{gau}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) & \\textrm{if $\\mathbf{x}_i$ is continuous} \\\\\n\t\tl_{\\mathrm{ber}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) & \\textrm{if $\\mathbf{x}_i$ is binary} \\\\\n\t\tl_{\\mathrm{cat}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) & \\textrm{if $\\mathbf{x}_i$ is categorical},\n\t\\end{cases}\n\t\\label{eq:loss-x}\n\\end{equation}\nwhere the specific loss terms are defined as follows:\n\\begin{align}\n\t& l_{\\mathrm{gau}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i)\n\t\t= - {\\textstyle \\sum_k} ( x_{ik} - \\hat{x}_{ik} )^2 \\label{eq:loss-x1} \\\\\n\t& l_{\\mathrm{ber}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i)\n\t\t= - {\\textstyle \\sum_k} ( \\alpha x_{ik} \\log \\sigma(\\hat{x}_{ik}) \\notag \\\\\n\t& \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad + (1 - \\alpha) (1 - x_{ik}) \\log (1 - \\sigma(\\hat{x}_{ik})) ) \\label{eq:loss-x2} \\\\\n\t& l_\\mathrm{cat}(\\hat{\\mathbf{x}}_i, \\mathbf{x}_i)\n\t\t= - {\\textstyle \\sum_k} x_{ik} \\log \\mathrm{softmax} (\\hat{x}_{ik}). \\label{eq:loss-x3}\n\\end{align}\n$\\hat{\\mathbf{x}}_i = g_x(\\mathbf{z}_i)$ is the output of the feature decoder, and $\\sigma$ is the logistic sigmoid function.\nWe introduce $\\alpha$ in Equation \\eqref{eq:loss-x2} to balance the effects of zero and nonzero entries of true features based on their occurrences \\cite{Chen2020}; $\\alpha$ is the ratio of zero entries in the observed feature matrix.\nFor the output $\\mathbf{y}_i = g_y(\\mathbf{z}_i)$ of the label decoder, we use the categorical loss, i.e., $l_y = l_\\mathrm{cat}$, due to the property of labels.\n\nAlgorithm \\ref{alg:overview} summarizes the training process of \\method.\nIt makes latent variables and predictions in lines 1 and 2, respectively, and computes the error between predictions and observations in line 3.\nThen, it computes our regularizer function in lines 4 and 5, whose information is described in the following subsections, to update the parameters of all three networks in an end-to-end way.\n\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Structured Variational Inference",
                    "content": "\n\\label{ssec:method-variational-inference}\n\nPrevious works utilizing variational inference \\cite{Kingma2014, Kipf2016} assume the prior of latent variables as a multivariate Gaussian distribution with identity covariance matrices, and run inference independently for each variable.\n%This assumption is inappropriate in our case due to the following reasons.\n%First, each variable $Z_i$ should be identifiable in our case, since our objective is to estimate specific $\\mathbf{x}_i$ instead of generating any plausible samples.\nThis assumption is inappropriate in our case, since the correlations between variables, represented as a graph, are the main evidence in our graph-based learning.\n\nWe thus model the prior distribution $p(\\zvar \\mid \\adj)$ of Equation \\eqref{eq:elbo-1} as Gaussian Markov random field (GMRF) to incorporate the graph structure in the probabilistic modeling of variables.\nSpecifically, we model $p(\\zvar \\mid \\adj)$ as GMRF $\\mathcal{N}(\\mathbf{0}, \\mathbf{K}^{-1})$ with parameters $\\mathbf{h} = \\mathbf{0}$ and $\\mathbf{K}$.\nWe make the information matrix $\\mathbf{K}$ from $\\adj$ as a graph Laplacian matrix with symmetric normalization \\cite{Zhang2015}:\n\\begin{equation}\n\t\\mathbf{K} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\adj \\mathbf{D}^{-1/2},\n\\label{eq:info-matrix}\t\n\\end{equation}\nwhere $\\mathbf{I}$ is the identity matrix, and $\\mathbf{D}$ is the degree matrix such that $D_{ii} = \\sum_j A_{ij}$.\nThe resulting $\\mathbf{K}$ preserves the structural information of the graph $G$ as a positive-semidefinite matrix that satisfies the constraint of GMRF; the nonzero entries of $\\mathbf{K}$ except the diagonal ones correspond to those of $\\adj$.\nNote that $\\mathbf{K}$ is a constant, since it represents the fixed prior distribution of variables.\n\nWe also model our target distribution $\\zdist$ as a multivariate Gaussian distribution $\\mathcal{N}(\\mathbf{U}, \\Sigma)$, where $\\mathbf{U}$ and $\\Sigma$ are the mean and covariance matrices of size $n \\times d$ and $n \\times n$, respectively.\nWe assume that all $d$ elements at each node share the same covariance matrix.\n$\\mathbf{U}$ and $\\Sigma$ are generated from encoder functions $f_\\mu$ and $f_\\sigma$, respectively, which contain the set $\\phi$ of learnable parameters.\n\nGiven the Gaussian modelings of $\\zdist$ and $\\prior$, the KL divergence is formulated as follows:\n\\begin{multline}\n\t\\kld{q_\\phi (\\zvar \\mid \\xall, \\yall, \\adj)}{p(\\zvar \\mid \\adj)} \\\\\n\t= 0.5 ( \\mathrm{tr}(\\mathbf{U}^\\top \\mathbf{K} \\mathbf{U}) + d(\\mathrm{tr}(\\mathbf{K}\\Sigma) - \\log|\\Sigma|)) + C,\n\t\\label{eq:kl-divergence}\n\\end{multline}\nwhere $C$ is a constant related to $\\mathbf{K}$ and $|\\mathcal{V}|$.\nThe goal of minimizing the KL divergence is to update $\\phi$ of encoder functions to make $q_\\phi$ similar to $p(\\zvar \\mid \\adj)$ as a regularizer of latent variables.\n\nThe computational bottleneck of Equation \\eqref{eq:kl-divergence} is $\\log |\\Sigma|$, whose computation is $O(n^3)$ \\cite{Han2015}.\nThus, we decompose the covariance as $\\Sigma = \\beta \\mathbf{I} + \\mathbf{V} \\mathbf{V}^\\top$ with a rectangular matrix $\\mathbf{V} \\in \\mathbb{R}^{n \\times r}$, where $\\beta$ and $r$ are hyperparameters such that $r \\ll n$ \\cite{Tomczak2020}.\nAs a result, $\\log |\\Sigma|$ is computed efficiently by the matrix determinant lemma \\cite{Harville1998}:\n\\begin{equation}\n\t\\log |\\Sigma| = \\log|\\mathbf{I}_r + \\beta^{-1} \\mathbf{V}^\\top \\mathbf{V}| + \\log |\\beta\\mathbf{I}_n|,\n\\label{eq:det-lemma}\n\\end{equation}\nwhere $\\mathbf{I}_r$ and $\\mathbf{I}_n$ are the identity matrices of sizes $r \\times r$ and $n \\times n$, respectively.\nThe computation of Equation \\eqref{eq:det-lemma} is $O(r^2n + r^3)$, which is tractable even in graphs with a large number of nodes.\n\nFor each inference, we sample $\\mathbf{Z}$ randomly from $q_\\phi$ based on $\\mathbf{U}$ and $\\mathbf{V}$ generated from $f_\\mu$ and $f_\\sigma$, respectively.\nSince the gradient-based update is not possible with the direct sampling of $\\zvar$, we use the reparametrization trick of variational autoencoders \\cite{Kingma2014, Tomczak2020}:\n\\begin{equation}\n\t\\mathbf{Z} = \\mathbf{U} + \\sqrt{\\beta} \\mathbf{M}_1 + \\mathbf{V}\\mathbf{M}_2,\n\t\\label{eq:var-sampling}\n\\end{equation}\nwhere $\\mathbf{M}_1 \\in \\mathbb{R}^{n \\times d}$ and $\\mathbf{M}_2 \\in \\mathbb{R}^{r \\times d}$ are matrices of standard normal variables, which are sampled randomly at each time to simulate the sampling of $\\zvar$ while supporting the backpropagation.\nThe detailed process of inference is described in Appendix \\ref{appendix:algorithm}.\n\nWe verify that the variables $\\mathbf{Z}$ sampled from Equation \\eqref{eq:var-sampling} follow the target distribution $\\mathcal{N}(\\mathbf{U}, \\Sigma)$ by Lemmas \\ref{lemma:vae-1} and \\ref{lemma:vae-2}.\n\n\\begin{lemma}\n\tLet $\\mathbf{z}_i$ be a latent variable sampled from Equation \\eqref{eq:var-sampling} for node $i$, and $\\mathbf{u}_i$ be the $i$-th row of $\\mathbf{U}$.\n\tThen, $\\mathbb{E}[\\mathbf{z}_i] = \\mathbf{u}_i$.\n\t\\label{lemma:vae-1}\n\\end{lemma}\n\n\\begin{proof}\n\t\\vspace{-1mm}\n\tSee Appendix \\ref{appendix:proof-vae-1}.\n\t\\vspace{-1mm}\n\\end{proof}\n\n\\begin{lemma}\n\tAssume that the size $d$ of latent variables is one.\n\tLet $z_i$ and $z_j$ be latent variables sampled from Equation \\eqref{eq:var-sampling} for nodes $i$ and $j$, respectively.\n\tThen, $\\mathbb{E}[(z_i - \\mathbb{E}[z_i])(z_j - \\mathbb{E}[z_j])] = \\Sigma_{ij}$.\n\t\\label{lemma:vae-2}\n\\end{lemma}\n\n\\begin{proof}\n\t\\vspace{-1mm}\n\tSee Appendix \\ref{appendix:proof-vae-2}.\n\t\\vspace{-1mm}\n\\end{proof}\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Unified Deterministic Modeling",
                    "content": "\n\\label{ssec:deterministic-modeling}\n\nThe reparametrization trick of variational inference requires us to sample different $\\mathbf{Z}$ at each inference to approximate the expectation term in Equation~\\eqref{eq:elbo-3}.\nHowever, this sampling process makes the training unstable, considering the characteristics of our feature estimation problem where 1) the inference is done for all nodes at once, not for each node independently, and 2) only a part of target variables have meaningful observations.\nEven a small perturbation for each node can result in a catastrophic change of the prediction, since we consider the correlations between nodes in $\\mathcal{N}(\\mathbf{U}, \\Sigma)$.\n\nWe propose two ideas for improving the basic parameterization.\nFirst, we unify the parameter matrices $\\mathbf{U}$ and $\\mathbf{V}$ as a single matrix $\\mathbf{E}$, and generate it from an encoder function $f$.\nThis is based on the observation that $\\mathbf{U}$ and $\\mathbf{V}$ have similar roles of representing target nodes as low-dimensional vectors based on the graphical structure.\nSecond, we change the stochastic sampling of $\\mathbf{Z}$ from $\\mathcal{N}(\\mathbf{E}, \\Sigma)$ into a deterministic process that returns $\\mathbf{E}$ at every inference, which has the largest probability in the distribution of $\\mathbf{Z}$.\nThis improves the stability of inference, while still allowing us to regularize the distribution of $\\mathbf{Z}$ with the KL divergence.\nFigure \\ref{fig:unification} depicts the difference between the basic parameterization and the unified modeling.\n\nThis unified modeling makes the KL divergence of Equation~\\eqref{eq:kl-divergence} into a general regularizer function that works with deterministic inference of node representations.\nFirst, we show in Lemma \\ref{lemma:equivalence} that the first two terms of the right hand side of Equation~\\eqref{eq:kl-divergence} become equivalent as we assume $\\mathbf{E} = \\mathbf{U} = \\mathbf{V}$ by the unified modeling.\n\n\\begin{lemma}\n\tLet $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{E} \\in \\mathbb{R}^{n \\times d}$, and $\\Sigma = \\beta \\mathbf{I} + \\mathbf{E} \\mathbf{E}^\\top$.\n\tThen, $\\mathrm{tr}(\\mathbf{K} \\Sigma) = \\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E}) + C$, where $C$ is a constant unrelated to $E$.\n\\label{lemma:equivalence}\n\\end{lemma}\n\n\\begin{proof}\n\t\\vspace{-1mm}\n\tSee Appendix \\ref{appendix:proof-equivalence}.\n\t\\vspace{-1mm}\n\\end{proof}\n\nThen, we propose our regularizer function used in Algorithm \\ref{alg:overview} by rewriting the KL divergence of Equation \\eqref{eq:kl-divergence} as follows:\n\\begin{equation}\n\tl_\\mathrm{GMRF}(\\mathbf{E}, \\mathbf{A}) = \\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E}) - \\alpha \\log|\\mathbf{I} + \\beta^{-1}\\mathbf{E}^\\top \\mathbf{E}|,\n\\label{eq:kl-divergence-2}\n\\end{equation}\nwhere $\\alpha > 0$ is a hyperparameter that controls the effect of the log determinant term.\nWe set $\\alpha = 1/2$ is all of our experiments. \n%Then, the KL divergence of Equation \\eqref{eq:kl-divergence} is rewritten as\n%\\begin{equation}\n%\tl_\\mathrm{GMRF}(\\mathbf{E}, \\mathbf{A}) = \\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E}) - 0.5 \\log|\\mathbf{I} + \\beta^{-1}\\mathbf{E}^\\top \\mathbf{E}| + C,\n%\\label{eq:kl-divergence-2}\n%\\end{equation}\n%where $C$ is the same constant as in Equation \\eqref{eq:kl-divergence}.\n%This is our proposed regularizer to train \\method in Algorithm \\ref{alg:overview}.\n\nThe first term of $l_\\mathrm{GMRF}$ is called the graph Laplacian regularizer and has been widely used in graph learning \\cite{Ando06, Pang2017}.\nIts minimization makes adjacent nodes have similar representations in $\\mathbf{E}$, and the symmetric normalization of $\\mathbf{K}$ alleviates the effect of node degrees in the regularization.\n%\\begin{equation}\n%\t\\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E})\n%\t= \\sum_{(i, j) \\in \\mathcal{E}} \\Bigl\\| \\frac{\\mathbf{e}_i}{\\sqrt{d_i}} - \\frac{\\mathbf{e}_j}{\\sqrt{d_j}}\\Bigr\\|^2_2,\n%\\label{eq:dot-product-2}\n%\\end{equation}\n%where $\\mathbf{e}_i$ and $\\mathbf{e}_j$ are the $i$-th and the $j$-th row of $\\mathbf{E}$, and $d_i$ and $d_j$ are the degrees of nodes $i$ and $j$, respectively.\n%The minimization of Equation \\eqref{eq:dot-product-2} makes adjacent nodes have similar representations in $\\mathbf{E}$, and the symmetric normalization of $\\mathbf{K}$ alleviates the effect of different node degrees in the regularization.\nThe second term of $l_\\mathrm{GMRF}$ can be considered as measuring the amount of space occupied by $\\mathbf{E}$.\nIn other words, its maximization makes $\\mathbf{e}_1, \\cdots, \\mathbf{e}_n$ distributed sparsely, alleviating the effect of $\\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E})$ that squeezes the embeddings into a small space.\nThe hyperparameter $\\beta$ controls the balance between the two terms having opposite goals; the second term is ignored if $\\beta = \\infty$, which means that the target nodes have no correlations.\n\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Complexity Analysis",
                    "content": "\n\\label{subsec:analysis}\n\nWe analyze the time and space complexities of \\method, assuming a graph convolutional network having two layers as the encoder function $f$.\nWe define a space complexity as the amount of space required to store intermediate data during each inference.\nLet $d$, $m$, and $c$ be the size of latent variables, the number of node features, and the number of labels, respectively.\n\n\\begin{lemma}\n\tGiven a graph $G = (\\mathcal{V}, \\mathcal{E})$, the time complexity of \\method is $O((d^2 + md + cd)|\\mathcal{V}| + d|\\mathcal{E}|)$ for each inference.\n\\label{lemma:time-complexity}\n\\end{lemma}\n\n\\begin{proof}\n\t\\vspace{-1mm}\n\tSee Appendix \\ref{appendix:proof-time-complexity}.\n\t\\vspace{-1mm}\n\\end{proof}\n\n\\begin{lemma}\n\tGiven a graph $G = (\\mathcal{V}, \\mathcal{E})$, the space complexity of \\method is $O((d + m + c)|\\mathcal{V}| + |\\mathcal{E}| + d^2 + md + cd)$ for each inference.\n\\label{lemma:space-complexity}\n\\end{lemma}\n\n\\begin{proof}\n\t\\vspace{-1mm}\n\tSee Appendix \\ref{appendix:proof-space-complexity}.\n\t\\vspace{-1mm}\n\\end{proof}\n\nLemmas \\ref{lemma:time-complexity} and \\ref{lemma:space-complexity} show that \\method is an efficient method whose complexity is linear with both the numbers of nodes and edges of the graph.\nThe GMRF regularizer does not affect the inference of \\method, because it is used only at the training time.\nStill, the time and space complexities of the GMRF loss $l_\\mathrm{GMRF}$ of Equation \\eqref{eq:kl-divergence-2} are $O(d^2|\\mathcal{V}| + d|\\mathcal{E}| + d^3)$ and $O(d|\\mathcal{V}| + |\\mathcal{E}| + d^2)$, respectively, which are linear with both the numbers of nodes and edges.\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:experiments}\n\nWe perform experiments to answer the following questions:\n\\begin{itemize}\n\t\\item[Q1.] \\textbf{Feature estimation (Section \\ref{subsec:feature-estimation}).}\n\t\tDoes \\method show higher accuracy in feature estimation than those of baselines?\n\t\\item[Q2.] \\textbf{Node classification (Section \\ref{subsec:node-classification}).}\n\t\tAre the features generated by \\method meaningful for node classification?\n\t\\item[Q3.] \\textbf{Effect of observed labels (Section \\ref{subsec:with-labels}).}\n\t\tDoes the observation of labels help generating more accurate features?\n\t\\item[Q4.] \\textbf{Scalability (Section \\ref{ssec:scalability}).}\n\t\tHow does the computational time of \\method increase with the number of edges?\n\t\\item[Q5.] \\textbf{Ablation study (Section \\ref{subsec:ablation-study}).}\n\t\tHow does the performance of \\method for feature estimation change by the GMRF regularizer and the deterministic modeling of inference?\n\\end{itemize}\n\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "Experimental Setup",
                    "content": "\n\\label{ssec:exp-setup}\n\nWe introduce our experimental setup including datasets, baseline methods, evaluation metrics, and training processes.\n\n\\textbf{Datasets.}\nWe use graph datasets summarized in Table \\ref{table:datasets}, which were used in previous works \\cite{McAuley2015, Yang2016, Shchur2018, Chen2020}.\nNode features in Cora, Citeseer, Photo, Computers, and Steam are zero-one binary vectors, while those in Pubmed, Coauthor, and ArXiv are continuous.\nEach node has a single discrete label.\nAll nodes in Steam have the same class, and thus the dataset is not used for node classification.\n\n\\textbf{Baselines.}\nWe compare \\method with existing models for feature estimation.\nNeighAgg \\cite{Simsek2008} is a simple approach that aggregates the features of neighboring nodes through mean pooling.\nVAE \\cite{Kingma2014} is a generative model that learns latent representations of examples.\n%We make latent representations of test nodes that contain no initial features by applying NeighAgg to the latent space.\nGCN \\cite{Kipf2017}, GraphSAGE \\cite{Hamilton2017}, and GAT \\cite{Velickovic2018} are popular graph neural networks that have been used in various domains.\n%We train the models by giving the graph structure as an input, represented as an adjacency matrix, and using the observed features as a target.\nWe report the best performance among the three models as GNN* for brevity.\n\nGraphRNA \\cite{Huang2019} and ARWMF \\cite{Chen2019} are recent methods for representation learning, which can be applied for generating features.\nSAT \\cite{Chen2020} is the state-of-the-art model for missing feature estimation, which trains separate autoencoders with a shared latent space for the features and graphical structure, respectively.\nWe use GAT and GCN as the backbone network of SAT in datasets with discrete and continuous features, respectively, which are the settings that show the best performance in the original paper \\cite{Chen2020}.\n\n\n\n\\textbf{Evaluation metrics.}\nWe evaluate the performance of feature estimation with four evaluation metrics.\nFor binary features, we treat each nonzero entry as a target item, considering the task as a ranking problem to find all nonzero entries.\nRecall at $k$ measures the ratio of true entries contained in the top $k$ predictions for each node, while nDCG at $k$ measures the overall quality of predicted scores in terms of information retrieval.\nWe vary $k$ over $\\{3, 5, 10\\}$ in the Steam dataset and $\\{10, 20, 50\\}$ in the other datasets, because Steam has fewer features and thus a prediction is generally easier.\nFor continuous features, we compare the predictions and the true features in an elementwise way with the root mean squared error (RMSE) and the square of the correlation coefficient (CORR).\nThe definitions of evaluation metrics are in Appendix \\ref{appendix:evaluation}.\n\n\\textbf{Experimental process.}\nWe take different processes of experiments for feature estimation and node classification.\nFor feature estimation, we split all nodes at each dataset into the training, validation, and test sets by the 4:1:5 ratio as in previous work \\cite{Chen2020}.\nWe train each model based on the observed features of training nodes and find the parameters that maximize the validation performance.\nWe run each experiment ten times and report the average.\n\nFor node classification, we take only the test nodes of feature estimation, whose features are generated by our \\method or baseline models.\nThen, we perform the 5-fold cross-validation in the target nodes, evaluating the quality of generated features with respect to the accuracy of node classification.\nWe use a multilayer perceptron (MLP) and GCN as classifiers.\nFor the training and evaluation of GCN, we use the induced subgraph of target nodes.\n\nEven though our \\method can utilize observed labels as additional evidence, we do not assume the observation of labels unless otherwise noted.\nThis is to make a fair comparison between \\method and baseline models that assume only the observation of features.\nWe perform experiments in Section \\ref{subsec:with-labels} with observed labels.\n\n\\textbf{Hyperparameters.}\nThe hyperparameter setting of our \\method is described in Appendix \\ref{appendix:parameters}.\nFor baselines, we take the experimental results from a previous work \\cite{Chen2020} that optimized the hyperparameters for the feature estimation problem on our datasets.\n\n"
                },
                "subsection 4.2": {
                    "name": "Performance on Feature Estimation (Q1)",
                    "content": "\n\\label{subsec:feature-estimation}\n\nTable \\ref{table:feature-estimation} compares \\method and baseline models for feature estimation.\n\\method outperforms all baselines with a significant margin in most cases; \\method shows up to 16.3\\% and 14.0\\% higher recall and nDCG, respectively, compared with the best competitors.\nThe amount of improvement over baselines is the largest in Cora and Citeseer, which are similar citation graphs, and the smallest in Steam.\nThis is because the citation graphs have high-dimensional features with sparse graph structures, increasing the difficulty of estimation for the baseline methods.\nOn the other hand, Steam has the smallest number of features, while having the densest structure.\n\nTable \\ref{table:continuous-feature-errors} presents the result of feature estimation for continuous features.\n\\method still outperforms all baselines, and the amount of improvement is similar in all three datasets.\nThe combination of Tables \\ref{table:feature-estimation} and \\ref{table:continuous-feature-errors} shows that \\method works well with various types of node features, providing stable performance.\nARWMF causes an out-of-memory error in 256GB memory, due to the computation of $\\mathbf{A}^n$ of the adjacency matrix $\\mathbf{A}$ with large $n\\geq5$.\n\n"
                },
                "subsection 4.3": {
                    "name": "Performance on Node Classification (Q2)",
                    "content": "\n\\label{subsec:node-classification}\n\nTable \\ref{table:classification-accuracy} shows the accuracy of node classification with two types of classifiers: MLP and GCN.\nMLP relies on the generated features for prediction, while GCN utilizes also the graph structure.\n\\method outperforms all baselines in most cases, making a consistency with the results of feature estimation in Tables~\\ref{table:feature-estimation} and \\ref{table:continuous-feature-errors};\n\\method achieves up to 14.2\\% and 1.9\\% higher accuracy in MLP and GCN, respectively, compared to the best competitors.\nThe Steam dataset is excluded from Table \\ref{table:classification-accuracy}, since it has the same label for all nodes.\n\n"
                },
                "subsection 4.4": {
                    "name": "Effect of Observed Labels (Q3)",
                    "content": "\n\\label{subsec:with-labels}\n\nFigure \\ref{fig:more-labels} shows the performance of \\method for feature estimation with different ratios of observed labels.\nFor instance, if the ratio is 0.5, half of all nodes have observed labels: $|\\mathcal{V}_y| = 0.5|\\mathcal{V}|$.\nNote that the experiments for Tables \\ref{table:feature-estimation}, \\ref{table:continuous-feature-errors}, and \\ref{table:classification-accuracy} are done with no labels for a fair comparison with the baseline models; the results of these experiments correspond to the leftmost points in Figure~\\ref{fig:more-labels}.\nWe also report the performance of SAT for comparison.\n\n\\method shows higher accuracy with more observations of labels in both datasets, demonstrating its ability to use labels to improve the performance of feature estimation.\nSince the parameters need to be optimized to predict both features and labels accurately, the observed labels work as an additional regularizer that guides the training of latent variables to avoid the overfitting.\n\n"
                },
                "subsection 4.5": {
                    "name": "Scalability (Q4)",
                    "content": "\n\\label{ssec:scalability}\n\nFigure \\ref{fig:scalability} shows the scalability of \\method with respect to the number of edges on the five largest datasets in Table \\ref{table:datasets}.\nFor each dataset, we sample nine random subgraphs of different sizes from $0.1|\\mathcal{E}|$ to $0.9|\\mathcal{E}|$, where $|\\mathcal{E}|$ denotes the number of original edges.\nWe measure the inference time of \\method in each graph ten times and report the average.\nThe figure shows the linear scalability of \\method with the number of edges in all datasets, supporting Lemma~\\ref{lemma:time-complexity}.\nArxiv and Coauthor take the longest inference times, as Arxiv and Coauthor have the largest numbers of edges and features, respectively.\n\n\n\n\n"
                },
                "subsection 4.6": {
                    "name": "Ablation Study (Q5)",
                    "content": "\n\\label{subsec:ablation-study}\n\nFigure \\ref{fig:regularization} shows an ablation study that compares \\method with its variants \\method-U and \\method-R.\n\\method-U runs stochastic inference described in Section \\ref{ssec:method-variational-inference}, without our idea of unified deterministic modeling.\nThe detailed process of stochastic inference is described also in Algorithm \\ref{alg:stochastic} of Appendix \\ref{appendix:algorithm}.\n\\method-R runs the deterministic inference but removes the regularizer term $l_\\mathrm{GMRF}$ of Equation~\\eqref{eq:kl-divergence-2}; it follows Algorithm \\ref{alg:overview} as in \\method except for lines 4 and 5.\n\n\\method shows the best test accuracy during the training with a stable curve.\nThe training accuracy is the best with \\method-R, since it overfits to training nodes without the regularizer term.\nOn the other hand, the training accuracy of \\method-U is the lowest among the three methods, while its test accuracy becomes similar to that of \\method-R at the later epochs.\nThis is because \\method-U fails even at maximizing the training accuracy due to the unstable training.\n%The low stability of \\method-U is shown also by the large standard deviation of its test accuracy, which increases as training proceeds.\nThe standard deviation of training accuracy is very small with all three methods, despite their different modelings.\n\n"
                }
            },
            "section 5": {
                "name": "Related Works",
                "content": "\n\\label{sec:related-works}\n\n\n\n%We review related works on \\method, which are categorized to graph neural networks, node representation learning, feature estimation, and graph probabilistic modeling.\n\n\\textbf{Graph neural networks.}\nGraph neural networks (GNN) refer to deep neural networks designed for graph-structured data \\cite{Hamilton2017, Velickovic2018, Velickovic2019, Kipf2017, You2020, You2021}.\n% Wang2021, Xu2019\nSince GNNs require the features of all nodes, one needs to generate artificial features to apply a GNN to a graph with missing features.\n\\citet{Derr2018} and \\citet{Cui2021} generate features from the graph structure.\n\\citet{Kipf2017} model the missing features as one-hot vectors, while \\citet{Zhao2020} leave them as zero vectors and propose a new regularizer function.\n\nOur \\method enables a GNN to be applied to graphs with partial observations by estimating missing features.\nThe main advantage of feature estimation is that the modification of a GNN classifier is not required, regardless of the number of observations given in the original graph.\nPrevious works that directly deal with partially observed graphs require finding new hyperparameters \\cite{Zhao2020} or even making a new weight matrix \\cite{Kipf2017} when the number of observations changes, making it difficult to reuse a trained model.\n\n\\textbf{Missing feature estimation.}\nThere are recent works that can be used directly for our feature estimation problem \\cite{Huang2019, Chen2019, Chen2020}.\nSuch methods are adopted as the main competitors in our experiments.\nThe main advantage of \\method over the previous approaches is the strong regularizer that allows us to effectively propagate the partial observations to the entire graph, avoiding the overfitting problem even with large representation power for feature estimation.\n\nGRAPE \\cite{You2020} estimates missing features in tabular data by learning a graph between examples and features.\nThe main difference from our work is that GRAPE assumes partial observations of feature elements, not feature vectors.\nIn other words, GRAPE cannot be used to estimate the features of nodes that have no partial observations, which is the scenario assumed in our experiments.\n\n\\textbf{Node representation learning.}\nUnsupervised node representation learning \\cite{Hamilton2017b} is to represent each node as a low-dimensional vector that summarizes its properties embedded in the structure and node features \\cite{Perozzi2014, Wang2016, Grover2016, Hamilton2017b, Velickovic2019}.\n%Traditional methods \\cite{Perozzi2014, Wang2016, Grover2016} use only the structural information, while recent ones utilize both the node features and graph structure to maximize performance \\cite{Hamilton2017b, Velickovic2019}.\nSuch methods make embeddings in a latent space, while we aim to learn the representations of nodes in a high-dimensional feature space; the node features generated from our \\method are interpretable in the feature domain. %, unlike the embedding vectors.\n\n\\textbf{Probabilistic modeling of graphs.}\nPrevious works model real-world graphs as pairwise Markov random fields with discrete variables and run graphical inference for node classification \\cite{Yoo2017, conf/wsdm/JoYK18, Yoo2019b, Yoo2020, Yoo2021}.\nOur work can be considered as a generalization of such works into the challenging task of missing feature estimation, which requires us to estimate high-dimensional continuous variables.\n%We introduce latent variables to assume the conditional independence between nodes, instead of running graphical inference for marginalization, and propose our deterministic structured variational inference for effective estimation.\n\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\n\nWe propose \\method (\\methodlong), an accurate method for missing feature estimation.\n\\method estimates high-dimensional features of nodes from a graph with partial observations, and its framework is carefully designed to model the target distributions of structured variational inference.\nThe main idea of \\method is the structural regularizer that assumes the prior of latent variables as Gaussian Markov random field, which considers the graph structure as the main evidence for modeling the correlations between nodes in variational inference.\n\\method outperforms previous methods for feature estimation and node classification, achieving the state-of-the-art accuracy in benchmark datasets.\nFuture works include extending the domain of \\method into directed or heterogeneous graphs that are common in real-world datasets.\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\\begin{acks}\n{ \\small\nThis work was supported by Institute of Information \\& communications Technology Planning \\& Evaluation(IITP) grant funded by the Korea government(MSIT) [No.2020-0-00894, Flexible and Efficient Model Compression Method for Various Applications and Environments],\n[No.2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)],\nand\n[NO.2021-0-0268, Artificial Intelligence Innovation Hub (Artificial Intelligence Institute, Seoul National University)].\n%\nThe Institute of Engineering Research at Seoul National University provided research facilities for this work.\nThe ICT at Seoul National University provides research facilities for this study.\nThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2021R1C1C1008526).\nU Kang is the corresponding author.\n}\n\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{paper.bib}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\clearpage\n\\appendix\n%\\section{Table of Symbols}\n%\\label{appendix:symbols}\n%\n%\\blue{Table \\ref{table:symbols} summarizes the symbols used frequently in this paper.}\n%\n%\\begin{table}\n%\\centering\n%\\caption{Table of symbols.}\n%\\small\n%\\begin{tabular}{c|l}\n%\t\\toprule\n%\t\\textbf{Symbol} & \\textbf{Description} \\\\\n%\t\\midrule\n%\t$G$ & Target undirected graph, consisting of $\\mathcal{V}$ and $\\mathcal{E}$ \\\\\n%\t$\\mathcal{V}, \\mathcal{E}$ & Sets of nodes and undirected edges, respectively \\\\\n%\t$\\mathcal{V}_x$ & Subset of nodes having observed features \\\\\n%\t$\\mathcal{V}_y$ & Subset of nodes having observed labels \\\\\n%%\t\\midrule\n%%\t$n$ & The number of nodes: $n = |\\mathcal{V}|$ \\\\\n%%\t$\\mathbf{x}_i, y_i$ & Feature vector and label of node $i \\in \\mathcal{V}$, respectively \\\\\n%%\t$\\mathbf{z}_i$ & Latent representation of node $i \\in \\mathcal{V}$ \\\\\n%%\t$y_i$ & Discrete label of node $i \\in \\mathcal{V}$ \\\\\n%\t\\midrule\n%\t$\\mathbf{h}, \\mathbf{K}$ & Mean vector and precision matrix of GMRF, respectively \\\\\n%\t$f_\\mu$, $f_\\sigma$, $f$ & Encoder networks for latent representations \\\\\n%\t$g_x, g_y$ & Decoder networks for features and labels, respectively \\\\\n%\t$\\mathbf{U}, \\mathbf{V}, \\mathbf{E}$ & Node embedding matrices for structured inference \\\\\n%\t$\\mathbf{M}_1, \\mathbf{M}_2$ & Standard random matrices for sampling-based inference \\\\\n%\t\\bottomrule\n%\\end{tabular}\n%\\label{table:symbols}\n%\\end{table}\n\n"
            },
            "section 7": {
                "name": "Proofs of Lemmas",
                "content": "\n\\label{appendix:proofs-vae}\n\n",
                "subsection 7.1": {
                    "name": "lemma:gmrf",
                    "content": "\n\\label{appendix:proof-gmrf}\n\n\\begin{proof}\n\tThe probability density function of $\\mathcal{N}(\\mu, \\Sigma)$ is\n\t\\begin{equation*}\n\t\tf(\\mathbf{z}) = C' \\exp (-(\\mathbf{z} - \\mu)^\\top \\Sigma^{-1} (\\mathbf{z} - \\mu)),\n\t\\end{equation*}\n\twhere $C' = (2\\pi)^{-d/2} |\\Sigma|^{-1/2}$ is a constant.\n\t\n\tWe rewrite $f$ as follows with $\\mathbf{K} = \\Sigma^{-1}$ and $\\mathbf{h} = \\mathbf{K} \\mu$:\n\t\\begin{align*}\n\t\tf(\\mathbf{z})\n\t\t\t&= C' \\exp (-\\mathbf{z}^\\top \\mathbf{K} \\mathbf{z} + 2 \\mu^\\top \\mathbf{K} \\mathbf{z} - \\mu^\\top \\mathbf{K} \\mu) \\\\\n\t\t\t&= C'' \\exp (-\\mathbf{z}^\\top \\mathbf{K} \\mathbf{z} + 2 \\mu^\\top \\mathbf{K} \\mathbf{z}) \\\\\n\t\t\t&= C'' \\exp \\Bigl( -\\sum_i \\sum_j z_i K_{ij} z_j + 2 \\sum_i h_i z_i \\Bigr).\n\t\\end{align*}\n\twhere $C'' = \\exp(\\mu^\\top \\mathbf{K} \\mu) \\cdot C'$ is also a constant.\n\t\n\tBy the definition of GMRF, $K_{ij} \\neq 0$ only if edge $(i, j)$ exists in the given graph $G = (\\mathcal{V}, \\mathcal{E})$.\n\tThen, we rewrite $f(\\mathbf{z})$ as\n\t\\begin{align*}\n\t\tf(\\mathbf{z}) = C \\exp \\Bigl( \\sum_{(i, j) \\in \\mathcal{E}} (- z_i K_{ij} z_j) + \\sum_{i \\in \\mathcal{V}} (-0.5 K_{ii} z_i^2 + h_iz_i) \\Bigr),\n\t\\end{align*}\n\twhere $C = \\exp(2) \\cdot C''$.\n\tWe prove the lemma by substituting $\\psi_{ij}(z_{ij})$ and $\\psi_i(z_i)$ for the first and second terms, respectively.\n\\end{proof}\n\n"
                },
                "subsection 7.2": {
                    "name": "lemma:vae-1",
                    "content": "\n\\label{appendix:proof-vae-1}\n\n\\begin{proof}\n\tThe random variables included in Equation \\eqref{eq:var-sampling} are $\\mathbf{M}_1$ and $\\mathbf{M}_2$.\n\tSince $\\mathbf{M}_1$ and $\\mathbf{M}_2$ are filled with standard normal values, it is satisfied that $\\mathbb{E}[\\sqrt{\\beta}\\mathbf{M}_1] = \\mathbf{0}$ and $\\mathbb{E}[\\mathbf{V} \\mathbf{M}_2] = \\mathbf{0}$, regardless of the actual values of $\\beta$ and $\\mathbf{V}$.\n\tThus, $\\mathbb{E}(\\mathbf{Z}) = \\mathbb{E}(\\mathbf{U}) = \\mathbf{U}$.\n\\end{proof}\n\n\n"
                },
                "subsection 7.3": {
                    "name": "lemma:vae-2",
                    "content": "\n\\label{appendix:proof-vae-2}\n\n\\begin{proof}\n\tThe following is satisfied for both $k=i$ and $k=j$ based on Lemma \\ref{lemma:vae-1}:\n\t\\begin{equation*}\n\t\tz_k - \\mathbb{E}[z_k] = \\sqrt{\\beta} m_{1k} + \\mathbf{v}_k^\\top \\mathbf{m}_2,\t\n\t\\end{equation*}\n\twhere $\\mathbf{v}_k$ and $\\mathbf{m}_2$ are $r$-dimensional vectors.\n\n\tThen, the covariance between $z_i$ and $z_j$ is given as\n\t\\begin{multline*}\n\t\t\\mathbb{E}[(z_i - \\mathbb{E}[z_i])(z_j - \\mathbb{E}[z_j])]\n\t\t= \\beta \\mathbb{E}[m_{1i} m_{1j}] \\\\\n\t\t+ \\sqrt{\\beta}\\mathbf{v}_j^\\top \\mathbb{E}[m_{1i} \\mathbf{m}_2]\n\t\t+ \\sqrt{\\beta} \\mathbf{v}_i^\\top \\mathbb{E}[m_{1j} \\mathbf{m}_2]\n\t\t+ \\mathbb{E}[\\mathbf{v}_i^\\top \\mathbf{m}_2 \\mathbf{v}_j^\\top \\mathbf{m}_2].\n\t\\end{multline*}\n\n\tSince every element of $\\mathbf{M}_1$ and $\\mathbf{M}_2$ follows the standard normal distribution, the following are satisfied.\n\tFirst, $\\mathbb{E}[m_{1i}m_{ij}] = 1$ if $i = j$ and zero otherwise.\n\tSecond, the second and third elements of the right hand side are zero.\n\tThird, $\\mathbb{E}[\\mathbf{v}_i^\\top \\mathbf{m}_2 \\mathbf{v}_j^\\top \\mathbf{m}_2] = \\mathbf{v}_i^\\top \\mathbf{v}_j$.\n\tAs a result, we have the following equality:\n\t\\begin{equation*}\n\t\t\\mathbb{E}[(z_i - \\mathbb{E}[z_i])(z_j - \\mathbb{E}[z_j])] = \\beta \\mathbb{I}[i = j] + \\mathbf{v}_i^\\top \\mathbf{v}_j,\n\t\\end{equation*}\n\twhere $\\mathbb{I}$ is an indicator function that returns one if the condition holds, and zero otherwise.\n\tThis equation is the same as the definition of $\\Sigma = \\beta \\mathbf{I} + \\mathbf{V} \\mathbf{V}^\\top$ in the matrix form.\n\\end{proof}\n\n"
                },
                "subsection 7.4": {
                    "name": "lemma:equivalence",
                    "content": "\n\\label{appendix:proof-equivalence}\n\n\\begin{proof}\n\t$\\mathrm{tr}(\\mathbf{K} \\Sigma) = \\beta \\mathrm{tr}(\\mathbf{K}) + \\mathrm{tr}(\\mathbf{K}\\mathbf{E}\\mathbf{E}^\\top)$ due to the definition of $\\Sigma$.\n\tThe cyclic property of a trace makes $\\mathrm{tr}(\\mathbf{K}\\mathbf{E}\\mathbf{E}^\\top) = \\mathrm{tr}(\\mathbf{E}^\\top\\mathbf{K}\\mathbf{E})$.\n\tThus, $\\mathrm{tr}(\\mathbf{K} \\Sigma) = \\beta \\mathrm{tr}(\\mathbf{K}) + \\mathrm{tr}(\\mathbf{E}^\\top\\mathbf{K}\\mathbf{E})$, and $\\mathrm{tr}(\\mathbf{K})$ is a constant.\n\\end{proof}\n\n"
                },
                "subsection 7.5": {
                    "name": "lemma:time-complexity",
                    "content": "\n\\label{appendix:proof-time-complexity}\n\n\\begin{proof}\n\t\\method consists of an encoder $f$ and two decoders $g_x$ and $g_y$.\n\tThe complexity of $f$ is $O(d^2|\\mathcal{V}| + d|\\mathcal{E}|)$ assuming the identity feature matrix.\n\tThe complexities of $g_x$ and $g_y$ are $O(md|\\mathcal{V}|)$ and $O(cd|\\mathcal{V}|)$, respectively.\n%\tThe complexity of the GMRF regularizer is $O(d^2|\\mathcal{V}| + d^2|\\mathcal{E}| + d^3)$, because $\\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E})$ is $O(d^2|\\mathcal{V}| + d^2|\\mathcal{E}|)$ and $\\log|\\mathbf{I} + \\beta^{-1}\\mathbf{E}^\\top \\mathbf{E}|$ is $O(d^2|\\mathcal{V}| + d^3)$.\n\\end{proof}\n\n"
                },
                "subsection 7.6": {
                    "name": "lemma:space-complexity",
                    "content": "\n\\label{appendix:proof-space-complexity}\n\n\\begin{proof}\n\t\\method consists of an encoder $f$ and two decoders $g_x$ and $g_y$.\n\tThe complexity of $f$ is $O(d|\\mathcal{V}| + |\\mathcal{E}| + d^2)$ assuming the identity feature matrix.\n\tThe complexities of $g_x$ and $g_y$ are $O(m|\\mathcal{V}| + d|\\mathcal{V}| + md)$ and $O(c|\\mathcal{V}| + d|\\mathcal{V}| + cd)$, respectively.\n%\tThe complexity of the GMRF regularizer is $O(d|\\mathcal{V}| + d|\\mathcal{E}| + d^2)$, as $\\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E})$ is $O(d|\\mathcal{V}| + d|\\mathcal{E}|)$ and the $\\log \\det$ is $O(d|\\mathcal{V}| + d^2)$.\n\\end{proof}\n\n"
                }
            },
            "section 8": {
                "name": "Details of Stochastic Inference",
                "content": "\n\\label{appendix:algorithm}\n\n\\begin{algorithm}[t]\n\\caption{Basic version of structured variational inference.}\n\\begin{algorithmic}[1]\n\t\\Require Adjacency matrix $\\mathbf{A}$, diagonal adjacency $\\mathbf{D}$, feature $\\mathbf{X}$, (optional) one-hot label $\\mathbf{Y}$, hyperparameter $\\beta$, networks $f_\\mu$, $f_\\sigma$, $g_x$, and $g_y$, and their parameters $\\phi_\\mu$, $\\phi_\\sigma$, $\\theta$, and $\\rho$, resp.\n\t\\Ensure Updated parameters $\\phi_\\mu'$, $\\phi_\\sigma'$, $\\theta'$, and $\\rho'$\n\t\\State $\\mathbf{U}, \\mathbf{V} \\leftarrow f_\\mu(\\mathbf{A}; \\phi_\\mu), f_\\sigma(\\mathbf{A}; \\phi_\\sigma)$ \\Comment Run encoder functions\n\t\\State $\\Sigma \\leftarrow \\beta \\mathbf{I} + \\mathbf{V} \\mathbf{V}^\\top$ \\Comment Make the covariance matrix\n\t\\State $\\mathbf{M}_1, \\mathbf{M}_2 \\leftarrow \\mathrm{StandardNormal}()$ \\Comment Sample random matrices\n\t\\State $\\mathbf{Z} \\leftarrow \\mathbf{U} + \\sqrt{\\beta} \\mathbf{M}_1 + \\mathbf{V} \\mathbf{M}_2$ \\Comment Make latent variables\n\t\\State $\\hat{\\mathbf{X}}, \\hat{\\mathbf{Y}} \\leftarrow g_x (\\mathbf{Z}, \\mathbf{A}; \\theta), g_y (\\mathbf{Z}, \\mathbf{A}; \\rho)$ \\Comment Make predictions\n\t\\State $l_{xy} \\leftarrow \\sum_i l_x(\\hat{\\mathbf{x}}_i, \\mathbf{x}_i) + \\sum_j l_y(\\hat{\\mathbf{y}}_j, \\mathbf{y}_j)$ \\Comment Equation \\eqref{eq:loss-x} to \\eqref{eq:loss-x3}\n\t\\State $\\mathbf{K} \\leftarrow \\mathbf{I} - \\mathbf{D}^{-1/2} \\adj \\mathbf{D}^{-1/2}$ \\Comment Equation \\eqref{eq:info-matrix}\n\t\\State $l_\\mathrm{KLD} \\leftarrow 0.5 ( \\mathrm{tr}(\\mathbf{U}^\\top \\mathbf{K} \\mathbf{U}) + d(\\mathrm{tr}(\\mathbf{K}\\Sigma) - \\log|\\Sigma|))$  \\Comment Equation \\eqref{eq:kl-divergence}\n\t\\State$\\phi'_\\mu, \\phi'_\\sigma, \\theta', \\rho' \\leftarrow$ Update $\\phi_\\mu, \\phi_\\sigma, \\theta, \\rho$ to minimize $l_{xy} + l_\\mathrm{KLD}$\n\\end{algorithmic}\n\\label{alg:stochastic}\n\\end{algorithm}\n\nWe present a detailed algorithm of structured variational inference in Algorithm \\ref{alg:stochastic}, which performs stochastic sampling described in Section \\ref{ssec:method-variational-inference}.\nIt uses two encoder functions for generating embedding matrices $\\mathbf{U}$ and $\\mathbf{V}$, respectively, in line 1.\nThen, it samples $\\mathbf{Z}$ from the Gaussian distribution with the reparametrization trick in lines 2 to 4.\nThe prediction is done as in the deterministic inference, but the regularizer term works differently in line 8, taking $\\mathbf{U}$ and $\\Sigma$ as its inputs.\nThe parameters of all four networks are updated.\n\n"
            },
            "section 9": {
                "name": "Evaluation Metrics",
                "content": "\n\\label{appendix:evaluation}\n\nWe use four metrics for the evaluation of estimated features: recall at $k$ and nDCG at $k$ for binary features, and RMSE and CORR for continuous features.\nCategorical features are not included in our datasets in Table \\ref{table:datasets}, but we can use classification accuracy as done for evaluating labels.\nThe symbols used in this section are defined as follows: $n$ is the number of nodes, $d$ is the number of features, $\\mathbf{x}_i$ is the true feature vector of node $i$, $\\hat{\\mathbf{x}}_i$ is the prediction for $\\mathbf{x}_i$, $x_{ij}$ is the $j$-th element of $\\mathbf{x}_i$, and $\\mathbb{I}(\\cdot)$ is a binary function that returns one if the condition holds and zero otherwise.\n\n\\textbf{Evaluation of binary features.}\nWe consider the feature estimation for binary features as a ranking problem, which is to find the nonzero elements at each feature vector $\\mathbf{x}_i$ of node $i$.\nLet $\\hat{r}_{ij}$ be the index having the $j$-th largest score in $\\hat{\\mathbf{x}}_i$.\nThen, we use the top $k$ predictions with the largest scores, i.e., $\\{ \\hat{x}_{il} \\mid l = \\hat{r}_{i1}, \\cdots, \\hat{r}_{ik} \\}$, at each node $i$, where $k$ is chosen in $\\{3, 5, 10, 20, 50\\}$.\nThis is done also in previous work for binary feature estimation \\cite{Chen2020} to focus more on the predictive performance of the top predictions.\n\nRecall at $k$ measures the ratio of true entries contained in the top $k$ predictions for each node:\n\\begin{equation}\n\t\\mathrm{REC}_k(\\hat{\\mathbf{X}}, \\mathbf{X}) =\n\t\t\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^k \\frac{\\mathbb{I}[x_{i, \\hat{r}_{ij}} = 1]}{\\|\\mathbf{x}_i\\|_0},\n\\end{equation}\nwhere $\\|\\mathbf{x}_i\\|_0$ is the number of nonzero entries in $\\mathbf{x}_i$.\nFor instance, recall @ 3 is computed as $2/3$ in the following example:\n\\begin{align*}\n\t&\\mathbf{x}_i = (0, 0, 1, 1, 1) \\\\\n\t&\\hat{\\mathbf{x}}_i = (0.1, 0.7, 0.2, 0.8, 0.9),\n\\end{align*}\nsince $\\hat{r}_{i1} = 5$, $\\hat{r}_{i2} = 4$, and $\\hat{r}_{i3} = 2$, and two of the nonzero entries of $\\mathbf{x}_i$ are included in the top 3 predictions with the largest scores.\n\nnDCG at $k$ measures also the quality of order in the top $k$ predictions with respect to information retrieval.\nnDCG is computed by normalizing DCG at $k$, which is defined as\n\\begin{equation}\n       \\mathrm{DCG}_k(\\hat{\\mathbf{X}}, \\mathbf{X}) = \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^k \\frac{\\mathbb{I}(x_{i, \\hat{r}_{ij}} = 1)}{\\log_2(j + 1)}.\n\\end{equation}\n\n\\textbf{Evaluation of continuous features.}\nWe evaluate predictions for continuous features with simple metrics of RMSE and CORR.\nRMSE measures an error between predictions and true features:\n\\begin{equation}\n\t\\mathrm{RMSE}(\\hat{\\mathbf{X}}, \\mathbf{X}) =\n\t\t\\frac{1}{n} \\sum_{i=1}^n \\sqrt{\\frac{1}{d} \\sum_{j=1}^d (\\hat{x}_{ij} - x_{ij})^2}.\n\\end{equation}\n\nCORR measures how much predictions and true features are correlated, and is defined as follows:\n\\begin{equation}\n\t\\mathrm{CORR}(\\hat{\\mathbf{X}}, \\mathbf{X}) =\n\t\t\\frac{1}{d} \\sum_{j=1}^d \\left( 1 - \\frac{\\sum_{i=1}^n (\\hat{x}_{ij} - x_{ij})^2}{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\right),\n\\end{equation}\nwhere $\\bar{x}_j = \\sum_{i=1}^n x_{ij} / n$ is the mean of the $j$-th feature of all nodes.\nCORR is higher the better, while RMSE is lower the better.\n\n%\\begin{table}\n%\\caption{%\n%\tThe performance of our \\method with different values of $\\lambda$ and $\\beta$.\n%\tWe report the recall @ $k$ scores with $k=10$.\n%}\n%\\begin{tabular}{cc|ccccc}\n%\t\\toprule\n%\t$\\lambda$ & $\\beta$ & Cora & Cite. & Comp. & Photo & Steam \\\\\n%\t\\midrule\n%\t0.1 & 0.1 & .1635 & \\textbf{.0943} & .0431 & .0438 & .2524 \\\\\n%\t0.1 & 1.0 & .1622 & .0932 & .0430 & .0437 & .2528 \\\\\n%\t1.0 & 0.1 & \\textbf{.1718} & .0925 & \\textbf{.0437} & \\textbf{.0446} & \\textbf{.2565} \\\\\n%\t1.0 & 1.0 & .1715 & .0917 & .0436 & \\textbf{.0446} & .2557 \\\\\n%\t\\bottomrule\n%\\end{tabular}\n%\\label{table:sensitivity}\n%\\end{table}\n\n"
            },
            "section 10": {
                "name": "Hyperparameter Settings",
                "content": "\n\\label{appendix:parameters}\n\nWe search the hyperparameters of our \\method as follows: the size $d$ of latent variables in $\\{256, 512\\}$, the dropout probability in $\\{0.0, 0.5\\}$, the regularization parameters $\\lambda$ and $\\beta$ in $\\{0.01, 0.1, 1.0\\}$, and the unit normalization of latent variables in $\\{\\mathrm{true}, \\mathrm{false}\\}$.\nWe also use the Adam \\cite{Kingma2015} optimizer with the learning rate $r=0.005$ in Steam and $r=0.001$ in all other datasets.\nThe early stopping is used with the validation performance, and all of our experiments were done at a workstation with RTX 2080 based on PyTorch.\nMore detailed information can be found in our official implementation.\\footnote{\\url{https://github.com/snudatalab/SVGA}}\n\n%Table \\ref{table:sensitivity} shows the parameter sensitivity of our \\method with respect to $\\lambda$ and $\\beta$.\n%We report only the recall @ $k$ scores with $k=10$ since other evaluation metrics present similar trends.\n%\\method works well in all settings of $\\lambda$ and $\\beta$, compared with the baseline approaches in Table \\ref{table:feature-estimation}, demonstrating its robustness for the choice of hyperparameters.\n%The setting with $\\lambda=1.0$ and $\\beta=0.1$ performs the best in four of the five datasets.\n%We report the test performance in Table \\ref{table:sensitivity}, but the trend is the same in the validation data, allowing us to find the best hyperparameters during the training.\n\n"
            }
        },
        "tables": {
            "table:feature-estimation": "\\begin{table*}\n\t\\setlength\\tabcolsep{4.9pt}\n\t\\centering\n\t\\caption{%\n\t\tEvaluation of \\method and baseline approaches for missing feature estimation with respect to (top) recall and (bottom) nDCG.\n\t\tThe best is in bold, and the second best is underlined.\n\t\tOur \\method outperforms all baselines in most cases.\n\t}\n\t\\vspace{-1mm}\n\t\\small\n\t\\begin{tabular}{c|l|ccc|ccc|ccc|ccc|ccc}\n\t\t\\toprule\n\t\t\\multirow{2}{*}{\\textbf{Metric}}\n\t\t\t& \\multirow{2}{*}{\\textbf{Model}}\n\t\t\t& \\multicolumn{3}{c|}{\\textbf{Cora}}\n\t\t\t& \\multicolumn{3}{c|}{\\textbf{Citeseer}}\n\t\t\t& \\multicolumn{3}{c|}{\\textbf{Computers}}\n\t\t\t& \\multicolumn{3}{c|}{\\textbf{Photo}}\n\t\t\t& \\multicolumn{3}{c}{\\textbf{Steam}} \\\\\n\t\t& & $@10$ & $@20$ & $@50$\n\t\t\t& $@10$ & $@20$ & $@50$\n\t\t\t& $@10$ & $@20$ & $@50$\n\t\t\t& $@10$ & $@20$ & $@50$\n\t\t\t& $@3$ & $@5$ & $@10$ \\\\\n\t\t\\midrule\n\t\t\\midrule\n\t\t\\multirow{8}{*}{Recall} & NeighAgg\n\t\t\t& .0906 & .1413 & .1961\n\t\t\t& .0511 & .0908 & .1501\n\t\t\t& .0321 & .0593 \t& .1306\n\t\t\t& .0329 & .0616 & .1361\n\t\t\t& .0603 & .0881 & .1446 \\\\\n\t\t& VAE\n\t\t\t& .0887 & .1228 & .2116\n\t\t\t& .0382 & .0668 & .1296\n\t\t\t& .0255 & .0502 \t& .1196\n\t\t\t& .0276 & .0538 & .1279\n\t\t\t& .0564 & .0820 & .1251 \\\\\n\t\t& GNN*\n\t\t\t& .1350 & .1812 & .2972\n\t\t\t& .0620 & .1097 & .2058\n\t\t\t& .0273 & .0533 \t& .1278\n\t\t\t& .0295 & .0573 & .1324\n\t\t\t& .2395 & .3431 & .4575 \\\\\n\t\t& GraphRNA\n\t\t\t& .1395 & .2043 & .3142\n\t\t\t& .0777 & .1272 & .2271\n\t\t\t& .0386 & .0690 \t& .1465\n\t\t\t& .0390 & .0703 & .1508\n\t\t\t& .2490 & .3208 & .4372 \\\\\n\t\t& ARWMF\n\t\t\t& .1291 & .1813 & .2960\n\t\t\t& .0552 & .1015 & .1952\n\t\t\t& .0280 & .0544 \t& .1289\n\t\t\t& .0294 & .0568 & .1327\n\t\t\t& .2104 & .3201 & .4512 \\\\\n%\t\t& SAT-SAGE\n%\t\t\t& .1356 & .1981 & .3165\n%\t\t\t& .0704 & .1163 & .2174\n%\t\t\t& .0419 & .0738 \t& .1562\n%\t\t\t& \\textbf{.0483} & \\underline{.0766} & .1601\n%\t\t\t& .2518 & .3470 & .4845 \\\\\n\t\t& SAT\n\t\t\t& \\underline{.1653} & \\underline{.2345} & \\underline{.3612}\n\t\t\t& \\underline{.0811} & \\underline{.1349} & \\underline{.2431}\n\t\t\t& \\underline{.0421} & \\underline{.0746} & \\underline{.1577}\n\t\t\t& \\underline{.0427} & \\underline{.0765} & \\underline{.1635}\n\t\t\t& \\underline{.2536} & \\textbf{.3620} & \\underline{.4965} \\\\\n\t\t\\cmidrule{2-17}\n\t\t& \\textbf{\\method}\n\t\t\t& \\textbf{.1718} & \\textbf{.2486} & \\textbf{.3814}\n\t\t\t& \\textbf{.0943} & \\textbf{.1539} & \\textbf{.2782}\n\t\t\t& \\textbf{.0437} & \\textbf{.0769} & \\textbf{.1602}\n\t\t\t& \\textbf{.0446} & \\textbf{.0798} & \\textbf{.1670}\n\t\t\t& \\textbf{.2565} & \\textbf{.3620} & \\textbf{.4996} \\\\\n\t\t\\midrule\n\t\t\\midrule\n\t\t\\multirow{8}{*}{nDCG} & NeighAgg\n\t\t\t& .1217 & .1548 & .1850\n\t\t\t& .0823 & .1155 & .1560\n\t\t\t& .0788 & .1156 \t& .1923\n\t\t\t& .0813 & .1196 & .1998\n\t\t\t& .0955 & .1204 & .1620 \\\\\n\t\t& VAE\n\t\t\t& .1224 & .1452 & .1924\n\t\t\t& .0601 & .0839 & .1251\n\t\t\t& .0632 & .0970 \t& .1721\n\t\t\t& .0675 & .1031 & .1830\n\t\t\t& .0902 & .1133 & .1437 \\\\\n\t\t& GNN*\n\t\t\t& .1791 & .2099 & .2711\n\t\t\t& .1026 & .1423 & .2049\n\t\t\t& .0673 & .1028 \t& .1830\n\t\t\t& .0712 & .1083 & .1896\n\t\t\t& .3366 & .4138 & .4912 \\\\\n\t\t& GraphRNA\n\t\t\t& .1934 & .2362 & .2938\n\t\t\t& .1291 & .1703 & .2358\n\t\t\t& .0931 & .1333 \t& .2155\n\t\t\t& .0959 & .1377 & .2232\n\t\t\t& .3437 & .4023 & .4755 \\\\\n\t\t& ARWMF\n\t\t\t& .1824 & .2182 & .2776\n\t\t\t& .0859 & .1245 & .1858\n\t\t\t& .0694 & .1053 \t& .1851\n\t\t\t& .0727 & .1098 & .1915\n\t\t\t& .3066 & .3877 & .4704 \\\\\n%\t\t& SAT-SAGE\n%\t\t\t& .1905 & .2320 & .2947\n%\t\t\t& .1179 & .1563 & .2227\n%\t\t\t& \\underline{.1030} & .1457 \t& .2333\n%\t\t\t& \\underline{.1082} & .1475 & .2402\n%\t\t\t& .3529 & .4271 & .5133 \\\\\n\t\t& SAT\n\t\t\t& \\underline{.2250} & \\underline{.2723} & \\underline{.3394}\n\t\t\t& \\underline{.1385} & \\underline{.1834} & \\underline{.2545}\n\t\t\t& \\underline{.1030} & \\underline{.1463} & \\underline{.2346}\n\t\t\t& \\underline{.1047} & \\underline{.1498} & \\underline{.2421}\n\t\t\t& \\textbf{.3585} & \\textbf{.4400} & \\underline{.5272} \\\\\n\t\t\\cmidrule{2-17}\n\t\t& \\textbf{\\method}\n\t\t\t& \\textbf{.2381} & \\textbf{.2894} & \\textbf{.3601}\n\t\t\t& \\textbf{.1579} & \\textbf{.2076} & \\textbf{.2892}\n\t\t\t& \\textbf{.1068} & \\textbf{.1509} & \\textbf{.2397}\n\t\t\t& \\textbf{.1084} & \\textbf{.1549} & \\textbf{.2472}\n\t\t\t& \\underline{.3567} & \\underline{.4391} & \\textbf{.5299} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\vspace{-2mm}\n\n\t\\label{table:feature-estimation}\n\\end{table*}",
            "table:datasets": "\\begin{table}\n\t\\centering\n\t\\caption{%\n\t\tSummary of datasets.\n\t}\n\t\\vspace{-1mm}\n\n\t\\small\n\t\\begin{tabular}{l|c|rrrr}\n\t\t\\toprule\n\t\t\\textbf{Dataset}\n\t\t\t& \\textbf{Type}\n\t\t\t& \\textbf{Nodes}\n\t\t\t& \\textbf{Edges}\n\t\t\t& \\textbf{Feat.}\n\t\t\t& \\textbf{Classes} \\\\\n\t\t\\midrule\n\t\tCora\\textsuperscript{1}\n\t\t\t& Binary & 2,708 & 5,429 & 1,433 & 7 \\\\\n\t\tCiteseer\\textsuperscript{1}\n\t\t\t& Binary & 3,327 & 4,732 & 3,703 & 6 \\\\\n\t\tPhoto\\textsuperscript{2}\n\t\t\t& Binary & 7,650 & 119,081 & 745 & 8 \\\\\n\t\tComputers\\textsuperscript{2}\n\t\t\t& Binary & 13,752 & 245,861 & 767 & 10 \\\\\n\t\tSteam\\textsuperscript{3}\n\t\t\t& Binary & 9,944 & 266,981 & 352 & 1 \\\\\n\t\t\\midrule\n\t\tPubmed\\textsuperscript{1}\n\t\t\t& Continuous & 19,717 & 44,324 & 500 & 3 \\\\\n\t\tCoauthor\\textsuperscript{2}\n\t\t\t& Continuous & 18,333 & 81,894 & 6,805 & 15 \\\\\n\t\tArxiv\\textsuperscript{4}\n\t\t\t& Continuous & 169,343 & 1,157,799 & 128 & 40 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\n\t\\begin{flushleft} \\footnotesize\n\t\t\\ \\ \\ \\ \\textsuperscript{1} \\url{https://github.com/kimiyoung/planetoid} \\\\\n\t\t\\ \\ \\ \\ \\textsuperscript{2} \\url{https://github.com/shchur/gnn-benchmark} \\\\\n\t\t\\ \\ \\ \\ \\textsuperscript{3} \\url{https://github.com/xuChenSJTU/SAT-master-online} \\\\\n\t\t\\ \\ \\ \\ \\textsuperscript{4} \\url{https://ogb.stanford.edu/docs/nodeprop/}\n\t\\end{flushleft}\n\n%\t\\vspace{-2mm}\n\t\\label{table:datasets}\n\\end{table}",
            "table:continuous-feature-errors": "\\begin{table}\n\t\\setlength\\tabcolsep{4pt}\n\n\t\\centering\n\t\\caption{\n\t\tEvaluation for missing feature estimation on continuous features.\n\t\tThe best is in bold, and the second best is underlined.\n\t\tRMSE is lower the better, while CORR is higher the better.\n\t\t``o.o.m.'' refers to an out-of-memory error.\n\t}\n\t\\vspace{-1mm}\n\n\t\\small\n\t\\begin{tabular}{l|rr|rr|rr}\n\t\t\\toprule\n\t\t\\multirow{2}{*}{\\textbf{Model}} &\n\t\t\t\\multicolumn{2}{c|}{\\textbf{Pubmed}} &\n\t\t\t\\multicolumn{2}{c|}{\\textbf{Coauthor}} &\n\t\t\t\\multicolumn{2}{c}{\\textbf{Arxiv}} \\\\\n\t\t& \\textbf{RMSE} & \\textbf{CORR} &\n\t\t\t\\textbf{RMSE} & \\textbf{CORR} &\n\t\t\t\\textbf{RMSE} & \\textbf{CORR} \\\\\n\t\t\\midrule\n\t\tNeighAgg\n\t\t\t& 0.0186\n\t\t\t& -0.2133\n\t\t\t& 0.0952\n\t\t\t& -0.2279\n\t\t\t& 0.1291\n\t\t\t& -0.4943 \\\\\n\t\tVAE\n\t\t\t& 0.0170\n\t\t\t& -0.0236\n\t\t\t& 0.0863\n\t\t\t& -0.0237\n\t\t\t& 0.1091\n\t\t\t& -0.4773 \\\\\n\t\tGNN*\n\t\t\t& 0.0168\n\t\t\t& -0.0010\n\t\t\t& 0.0850\n\t\t\t& 0.0179\n\t\t\t& 0.1091\n\t\t\t& 0.0283 \\\\\n\t\tGraphRNA\n\t\t\t& 0.0172\n\t\t\t& -0.0352\n\t\t\t& 0.0897\n\t\t\t& -0.1052\n\t\t\t& 0.1131\n\t\t\t& -0.0419 \\\\\n\t\tARWMF\n\t\t\t& \\underline{0.0165}\n\t\t\t& \\underline{0.0434}\n\t\t\t& 0.0827\n\t\t\t& 0.0710\n\t\t\t& o.o.m.\n\t\t\t& o.o.m. \\\\\n\t\tSAT\n\t\t\t& \\underline{0.0165}\n\t\t\t& 0.0378\n\t\t\t& \\underline{0.0820}\n\t\t\t& \\underline{0.0958}\n\t\t\t& \\underline{0.1055}\n\t\t\t& \\underline{0.0868} \\\\\n\t\t\\midrule\n\t\t\\textbf{\\method}\n\t\t\t& \\textbf{0.0158}\n\t\t\t& \\textbf{0.1169}\n\t\t\t& \\textbf{0.0798}\n\t\t\t& \\textbf{0.1488}\n\t\t\t& \\textbf{0.1005}\n\t\t\t& \\textbf{0.1666} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{table:continuous-feature-errors}\n\\end{table}",
            "table:classification-accuracy": "\\begin{table*}\n\t\\setlength\\tabcolsep{6pt}\n\t\\centering\n\t\\caption{\n\t\tComparison between \\method and baselines by node classification accuracy, where each classifier is trained with the generated features.\n\t\t\\method outperforms all baseline methods in most cases.\n\t\t``o.o.m.'' refers to an out-of-memory error.\n\t}\n\t\\vspace{-1mm}\n\n\t\\small\n\t\\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc}\n\t\t\\toprule\n\t\t\\multirow{2}{*}{\\textbf{Model}} &\n\t\t\t\\multicolumn{2}{c|}{\\textbf{Cora}} &\n\t\t\t\\multicolumn{2}{c|}{\\textbf{Citeseer}} &\n\t\t\t\\multicolumn{2}{c|}{\\textbf{Computers}} &\n\t\t\t\\multicolumn{2}{c|}{\\textbf{Photo}} &\n\t\t\t\\multicolumn{2}{c|}{\\textbf{Pubmed}} &\n\t\t\t\\multicolumn{2}{c|}{\\textbf{Coauthor}} &\n\t\t\t\\multicolumn{2}{c}{\\textbf{Arxiv}} \\\\\n\t\t& \\textbf{MLP} & \\textbf{GCN} &\n\t\t\t\\textbf{MLP} & \\textbf{GCN} &\n\t\t\t\\textbf{MLP} & \\textbf{GCN} &\n\t\t\t\\textbf{MLP} & \\textbf{GCN} &\n\t\t\t\\textbf{MLP} & \\textbf{GCN} &\n\t\t\t\\textbf{MLP} & \\textbf{GCN} &\n\t\t\t\\textbf{MLP} & \\textbf{GCN} \\\\\n\t\t\\midrule\n\t\tNeighAgg\n\t\t\t& .6248\n\t\t\t& .6494\n\t\t\t& .5539\n\t\t\t& .5413\n\t\t\t& \\underline{.8365}\n\t\t\t& .8715\n\t\t\t& .8846\n\t\t\t& .9010\n\t\t\t& .5150\n\t\t\t& .6564\n\t\t\t& .7562\n\t\t\t& .8031\n\t\t\t& \\underline{.3979}\n\t\t\t& \\underline{.6493} \\\\\n\t\tVAE\n\t\t\t& .2826\n\t\t\t& .3011\n\t\t\t& .2551\n\t\t\t& .2663\n\t\t\t& .3747\n\t\t\t& .4023\n\t\t\t& .2598\n\t\t\t& .3781\n\t\t\t& .4008\n\t\t\t& .4007\n\t\t\t& .2317\n\t\t\t& .2335\n\t\t\t& .1633\n\t\t\t& .1965 \\\\\n\t\tGNN*\n\t\t\t& .4852\n\t\t\t& .5779\n\t\t\t& .3933\n\t\t\t& .4278\n\t\t\t& .3747\n\t\t\t& .4034\n\t\t\t& .2683\n\t\t\t& .3789\n\t\t\t& .4013\n\t\t\t& .4203\n\t\t\t& .2317\n\t\t\t& .2335\n\t\t\t& .2607\n\t\t\t& .4721 \\\\\n\t\tGraphRNA\n\t\t\t& .7581\n\t\t\t& .8198\n\t\t\t& .6320\n\t\t\t& .6394\n\t\t\t& .6968\n\t\t\t& .8650\n\t\t\t& .8407\n\t\t\t& .9207\n\t\t\t& .6035\n\t\t\t& \\underline{.8172}\n\t\t\t& \\underline{.7710}\n\t\t\t& \\underline{.8851}\n\t\t\t& .1609\n\t\t\t& .1859 \\\\\n\t\tARWMF\n\t\t\t& .7769\n\t\t\t& .8205\n\t\t\t& .2267\n\t\t\t& .2764\n\t\t\t& .5608\n\t\t\t& .7400\n\t\t\t& .4675\n\t\t\t& .6146\n\t\t\t& \\textbf{.6180}\n\t\t\t& .8089\n\t\t\t& .2320\n\t\t\t& .8347\n\t\t\t& o.o.m.\n\t\t\t& o.o.m. \\\\\n\t\tSAT\n\t\t\t& \\underline{.7937}\n\t\t\t& \\textbf{.8579}\n\t\t\t& \\underline{.6475}\n\t\t\t& \\underline{.6767}\n\t\t\t& .8201\n\t\t\t& \\underline{.8766}\n\t\t\t& \\underline{.8976}\n\t\t\t& \\textbf{.9260}\n\t\t\t& .4618\n\t\t\t& .7439\n\t\t\t& .7672\n\t\t\t& .8402\n\t\t\t& .3144\n\t\t\t& .5677 \\\\\n\t\t\\midrule\n\t\t\\textbf{\\method (proposed)}\n\t\t\t& \\textbf{.8431}\n\t\t\t& \\underline{.8490}\n\t\t\t& \\textbf{.6774}\n\t\t\t& \\textbf{.6844}\n\t\t\t& \\textbf{.8450}\n\t\t\t& \\textbf{.8889}\n\t\t\t& \\textbf{.9021}\n\t\t\t& \\underline{.9253}\n\t\t\t& \\underline{.6178}\n\t\t\t& \\textbf{.8315}\n\t\t\t& \\textbf{.8805}\n\t\t\t& \\textbf{.9023}\n\t\t\t& \\textbf{.4394}\n\t\t\t& \\textbf{.6644} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\vspace{-2mm}\n\n\t\\label{table:classification-accuracy}\n\\end{table*}"
        },
        "figures": {
            "fig:motivation": "\\begin{figure}\n\t\\centering\n\t\\vspace{2mm}\n\t\\includegraphics[width=0.47\\textwidth]{figures/motivation}\n\t\\caption{\n\t\tAn illustration of the feature estimation problem.\n\t\tThe generated features not only provide direct information of node properties but also help other graph-related tasks.\n\t}\n\\label{fig:motivation}\n\\end{figure}",
            "fig:gmrf": "\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.425\\textwidth]{figures/gmrf}\n\t\\caption{\n\t\tGaussian Markov random field (GMRF) describing a Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$ by parameters $\\mathbf{h}$ and $\\mathbf{K}$.\n\t\tThe nonzero entries in $\\mathbf{K}$ correspond to the edges in $G$.\n\t}\n\t\\label{fig:gmrf}\n\\end{figure}",
            "fig:overview": "\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.9\\textwidth]{figures/overview}\n\t\\caption{%\n\t\tThe structure of our \\method, which consists of an encoder network $f$ and two decoder networks $g_x$ and $g_y$ for features and labels, respectively.\n\t\tWe model the distribution of latent variables with GMRF, exploiting the graph structure for modeling the correlations between target variables.\n\t\tThe label decoder $g_y$ works as an auxiliary module that helps $g_x$.\n\t}\n\t\\vspace{-3mm}\n\t\\label{fig:overview}\n\\end{figure*}",
            "fig:unification": "\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.44\\textwidth]{figures/encoder}\n\t\\caption{%\n\t\tComparison between the encoder structures of the (a) basic parameterization and (b) unified modeling.\n\t\tWe use a single encoder $f$ to deterministically generate $\\zvar$ while utilizing the strong regularization of the KL divergence.\n\t}\n\t\\label{fig:unification}\n\\end{figure}",
            "fig:more-labels": "\\begin{figure}\n\t\\centering\n\t\\includegraphics[trim=0 0 0 0, width=0.29\\textwidth]{figures/experiments/labels-legend}\n\t\\vspace{-1mm}\n\n\t\\begin{subfigure}{0.232\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{figures/experiments/labels-cora}\n        \\vspace{-1.5\\baselineskip}\n\t\t\\caption{Cora}\n\t\\end{subfigure} \\hfill\n\t\\begin{subfigure}{0.239\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{figures/experiments/labels-computers}\n        \\vspace{-1.5\\baselineskip}\n\t\t\\caption{Computers}\n\t\\end{subfigure}\n\t\t\n\t\\caption{%\n\t\tThe accuracy of \\method for feature estimation with additional observations of labels.\n\t\tWe show the average and standard deviation of ten runs.\n\t\t\\method effectively utilizes the given labels, making more accurate predictions.\n\t}\n\t\\label{fig:more-labels}\n\\end{figure}",
            "fig:scalability": "\\begin{figure}\n\t\\centering\n\n\t\\begin{minipage}{0.24\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figures/scalability/fig-annotated}\n\t\\end{minipage} \\hspace{-2mm}\n\t\\begin{minipage}{0.125\\textwidth}\n\t\t\\centering\n\t\t\\vspace{-5mm}\n\t\t\\includegraphics[width=\\textwidth]{figures/scalability/legend}\n\t\\end{minipage}\n\t\n\t\\vspace{-1mm}\n\t\\caption{\n\t\tThe inference time of \\method in graphs of different sizes.\n\t\tWe randomly sample nine subgraphs for each dataset.\n\t\t\\method shows the linear scalability in all datasets.\n\t}\n\t\\label{fig:scalability}\n\\end{figure}",
            "fig:regularization": "\\begin{figure}\n\t\\centering\n\t\\includegraphics[trim=0 0 0 0, width=0.44\\textwidth]{figures/experiments/epochs-legend}\n\t\\vspace{-1mm}\n\t\n\t\\begin{subfigure}{0.232\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{figures/experiments/epochs-cora-trn}\n        \\vspace{-1.5\\baselineskip}\n\t\t\\caption{Training}\n\t\\end{subfigure} \\hfill\n\t\\begin{subfigure}{0.239\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{figures/experiments/epochs-cora-test}\n        \\vspace{-1.5\\baselineskip}\n\t\t\\caption{Test}\n\t\\end{subfigure}\n\n\t\\caption{\n\t\tAn ablation study of \\method on Cora compared with its variants \\method-R and \\method-U (details in Section \\ref{subsec:ablation-study}).\n\t\tWe show the average and standard deviation of ten runs.\n\t\t\\method makes the best test accuracy based on our proposed ideas.\n\t}\n\t\\label{fig:regularization}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\t& \\psi_i(z_i) = \\exp(-0.5K_{ii}z_i^2 + h_i z_i) \\label{eq:mrf-np} \\\\\n\t& \\psi_{ij}(z_i, z_j) = \\exp(-K_{ij}z_iz_j), \\label{eq:mrf-ep}\n\\end{align}",
            "eq:2": "\\begin{equation}\n\tp(\\mathbf{z}) = \\frac{1}{C} \\prod_{i \\in \\mathcal{V}} \\psi_i(z_i) \\prod_{(i, j) \\in \\mathcal{E}} \\psi_{ij}(z_i, z_j),\n\t\\label{eq:mrf-joint}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\begin{split}\n\t&\\log p_\\Theta(\\xall, \\yall \\mid \\adj) \\geq \\elbo \\\\\n\t&\\quad \\quad = \\mathbb{E}_{\\zvar \\sim \\zdist} [\\log p_{\\theta, \\rho} (\\xall, \\yall \\mid \\zvar, \\adj)] \\\\\n\t&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad - \\kld{\\zdist}{p(\\zvar \\mid \\adj)},\n\\end{split}\n\\label{eq:elbo-1}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\tl(\\Theta) = \\sum_{i \\in \\mathcal{V}_x} l_x(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) + \\sum_{i \\in \\mathcal{V}_y} l_y(\\hat{\\mathbf{y}}_i, \\mathbf{y}_i) + \\lambda l_\\mathrm{GMRF}(\\mathbf{Z}, \\adj),\n\\label{eq:obj-function}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\tl_x(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) = \\begin{cases}\n\t\tl_{\\mathrm{gau}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) & \\textrm{if $\\mathbf{x}_i$ is continuous} \\\\\n\t\tl_{\\mathrm{ber}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) & \\textrm{if $\\mathbf{x}_i$ is binary} \\\\\n\t\tl_{\\mathrm{cat}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i) & \\textrm{if $\\mathbf{x}_i$ is categorical},\n\t\\end{cases}\n\t\\label{eq:loss-x}\n\\end{equation}",
            "eq:6": "\\begin{align}\n\t& l_{\\mathrm{gau}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i)\n\t\t= - {\\textstyle \\sum_k} ( x_{ik} - \\hat{x}_{ik} )^2 \\label{eq:loss-x1} \\\\\n\t& l_{\\mathrm{ber}}(\\mathbf{\\hat{x}}_i, \\mathbf{x}_i)\n\t\t= - {\\textstyle \\sum_k} ( \\alpha x_{ik} \\log \\sigma(\\hat{x}_{ik}) \\notag \\\\\n\t& \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad + (1 - \\alpha) (1 - x_{ik}) \\log (1 - \\sigma(\\hat{x}_{ik})) ) \\label{eq:loss-x2} \\\\\n\t& l_\\mathrm{cat}(\\hat{\\mathbf{x}}_i, \\mathbf{x}_i)\n\t\t= - {\\textstyle \\sum_k} x_{ik} \\log \\mathrm{softmax} (\\hat{x}_{ik}). \\label{eq:loss-x3}\n\\end{align}",
            "eq:7": "\\begin{equation}\n\t\\mathbf{K} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\adj \\mathbf{D}^{-1/2},\n\\label{eq:info-matrix}\t\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\t\\log |\\Sigma| = \\log|\\mathbf{I}_r + \\beta^{-1} \\mathbf{V}^\\top \\mathbf{V}| + \\log |\\beta\\mathbf{I}_n|,\n\\label{eq:det-lemma}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\t\\mathbf{Z} = \\mathbf{U} + \\sqrt{\\beta} \\mathbf{M}_1 + \\mathbf{V}\\mathbf{M}_2,\n\t\\label{eq:var-sampling}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\tl_\\mathrm{GMRF}(\\mathbf{E}, \\mathbf{A}) = \\mathrm{tr}(\\mathbf{E}^\\top \\mathbf{K} \\mathbf{E}) - \\alpha \\log|\\mathbf{I} + \\beta^{-1}\\mathbf{E}^\\top \\mathbf{E}|,\n\\label{eq:kl-divergence-2}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n\t\\mathrm{REC}_k(\\hat{\\mathbf{X}}, \\mathbf{X}) =\n\t\t\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^k \\frac{\\mathbb{I}[x_{i, \\hat{r}_{ij}} = 1]}{\\|\\mathbf{x}_i\\|_0},\n\\end{equation}",
            "eq:12": "\\begin{align*}\n\t&\\mathbf{x}_i = (0, 0, 1, 1, 1) \\\\\n\t&\\hat{\\mathbf{x}}_i = (0.1, 0.7, 0.2, 0.8, 0.9),\n\\end{align*}",
            "eq:13": "\\begin{equation}\n       \\mathrm{DCG}_k(\\hat{\\mathbf{X}}, \\mathbf{X}) = \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^k \\frac{\\mathbb{I}(x_{i, \\hat{r}_{ij}} = 1)}{\\log_2(j + 1)}.\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\t\\mathrm{RMSE}(\\hat{\\mathbf{X}}, \\mathbf{X}) =\n\t\t\\frac{1}{n} \\sum_{i=1}^n \\sqrt{\\frac{1}{d} \\sum_{j=1}^d (\\hat{x}_{ij} - x_{ij})^2}.\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\t\\mathrm{CORR}(\\hat{\\mathbf{X}}, \\mathbf{X}) =\n\t\t\\frac{1}{d} \\sum_{j=1}^d \\left( 1 - \\frac{\\sum_{i=1}^n (\\hat{x}_{ij} - x_{ij})^2}{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\right),\n\\end{equation}"
        },
        "git_link": "https://github.com/snudatalab/SVGA"
    }
}