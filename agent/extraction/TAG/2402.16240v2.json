{
    "meta_info": {
        "title": "High-Frequency-aware Hierarchical Contrastive Selective Coding for  Representation Learning on Text-attributed Graphs",
        "abstract": "We investigate node representation learning on text-attributed graphs (TAGs),\nwhere nodes are associated with text information. Although recent studies on\ngraph neural networks (GNNs) and pretrained language models (PLMs) have\nexhibited their power in encoding network and text signals, respectively, less\nattention has been paid to delicately coupling these two types of models on\nTAGs. Specifically, existing GNNs rarely model text in each node in a\ncontextualized way; existing PLMs can hardly be applied to characterize graph\nstructures due to their sequence architecture. To address these challenges, we\npropose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive\nSelective Coding method that integrates GNNs and PLMs into a unified model.\nDifferent from previous \"cascaded architectures\" that directly add GNN layers\nupon a PLM, our HASH-CODE relies on five self-supervised optimization\nobjectives to facilitate thorough mutual enhancement between network and text\nsignals in diverse granularities. Moreover, we show that existing contrastive\nobjective learns the low-frequency component of the augmentation graph and\npropose a high-frequency component (HFC)-aware contrastive learning objective\nthat makes the learned embeddings more distinctive. Extensive experiments on\nsix real-world benchmarks substantiate the efficacy of our proposed approach.\nIn addition, theoretical analysis and item embedding visualization provide\ninsights into our model interoperability.",
        "author": "Peiyan Zhang, Chaozhuo Li, Liying Kang, Feiran Huang, Senzhang Wang, Xing Xie, Sunghun Kim",
        "link": "http://arxiv.org/abs/2402.16240v2",
        "category": [
            "cs.IR",
            "cs.SI",
            "H.3.3"
        ],
        "additionl_info": "Accepted by WWW 2024"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nGraphs are pervasive in the real world, and it is common for nodes within these graphs to be enriched with textual attributes, thereby giving rise to text-attributed graphs (TAGs)~\\citep{zhao2022learning}. For instance, academic graphs~\\citep{tang2008arnetminer} incorporate  papers replete with their titles and abstracts, whereas social media networks~\\citep{zhang2016geoburst} encompass tweets accompanied by their textual content. Consequently, the pursuit of learning within the realm of TAGs has assumed significant prominence as a research topic spanning various domains, \\textit{e.g.,} network analysis~\\citep{wang2019heterogeneous}, recommender systems~\\citep{zhang2019heterogeneous}, and anomaly detection~\\citep{liu2019fine}.\n\n% Graphs are pervasively employed to model real-world data, encompassing academic graphs~\\citep{tang2008arnetminer}, product graphs~\\citep{dong2020autoknow}, and social media networks~\\citep{zhang2016geoburst}. The -attributed graph constitutes a category of graph that incorporates both nodes and edges, in conjunction with affiliated attributes, which may signify diverse aspects of the node, such as properties and semantics~\\citep{zhao2022learning}. As a prominent constituent of the -attributed graph family, text-attributed graphs (TAGs) represent graphs wherein nodes are conjoined with abundant textual information. For example, academic graphs~\\citep{tang2008arnetminer} feature papers with their respective titles and abstracts, while social media networks~\\citep{zhang2016geoburst} contain tweets accompanied by their content. Owing to their capacity to represent and analyze textual data, TAGs have gained considerable significance in a myriad of applications, including link prediction~\\citep{sun2011pathsim}, node classification~\\citep{wang2019heterogeneous}, and recommendation~\\citep{zhang2019heterogeneous}.\n\n\n% Graphs are ubiquitously utilized to model real-world data such as academic graphs~\\citep{tang2008arnetminer}, product graphs~\\citep{dong2020autoknow}, and social media networks~\\citep{zhang2016geoburst}. The -attributed graph is a type of graph that includes both nodes and edges, along with associated attributes which can represent various characteristics of the node such as the node properties and semantics~\\citep{zhao2022learning}. As an important member of the -attributed graph family, text-attributed graphs (TAGs) are the graphs where nodes are associated with rich text information. For instance, papers in academic graphs~\\citep{tang2008arnetminer} have their titles and abstracts; tweets in social media networks~\\citep{zhang2016geoburst} have their tweet contents. Being capable of representing and analyzing textual data, TAGs play an increasingly important role  in various applications, including link prediction~\\citep{sun2011pathsim}, node classification~\\citep{wang2019heterogeneous}, and recommendation~\\citep{zhang2019heterogeneous}.\n\n% Graphs are ubiquitously utilized to model real-world data such as academic graphs~\\citep{tang2008arnetminer}, product graphs~\\citep{dong2020autoknow}, and social media networks~\\citep{zhang2016geoburst}. Such networks are called text -attributed graphs (TAGs), where nodes are associated with text information. For instance, papers in academic graphs~\\citep{tang2008arnetminer} have their titles and abstracts; tweets in social media networks~\\citep{zhang2016geoburst} have their tweet contents. In such text -attributed networks, to achieve satisfying performance in tasks such as link prediction~\\citep{sun2011pathsim}, node classification~\\citep{wang2019heterogeneous}, and recommendation~\\citep{zhang2019heterogeneous}, models need to jointly consider network structures and text semantics.\n\n\n % *******************************************\n% \\begin{figure}[t]\n%     \\centering\n%     \\includegraphics[width=.8\\linewidth]{figures/example.png}\n%     \\caption{Examples of the textual-graphic correlations on TAGs. \\textbf{Left:} graphic neighbor structures help identify the textual semantics of the center node. \\textbf{Right:} The textual features of the node can be a strong indicates of the neighborhood information.}\n%     \\label{fig:example}\n%     % \\vspace{-0.3cm}\n% \\end{figure}\n % *******************************************\n\nIn essence, graph topology and node attributes comprise two integral components of TAGs. \n% Graph topology encapsulates the local structures of nodes by delineating their interconnections within a graph, while node attributes convey the semantics of nodes by endowing them with textual features. \nConsequently, the crux of representation learning on TAGs lies in the amalgamation of graph topology and node attributes. Previous works mainly adopt a cascaded architecture~\\citep{jin2021bite,li2021adsgnn,zhang2019shne,zhu2021textgnn} (Figure~\\ref{fig:example}(a)), which entails encoding the textual attributes of each node with Pre-trained Language Models (PLMs), subsequently utilizing the PLM embeddings as features to train a Graph Neural Network (GNN) for message propagation~\\citep{gururangan2020don,chien2021node,yasunaga2022linkbert}. However, as the modeling of node attributes and graph topology are segregated, this learning paradigm harbors conspicuous limitations. Firstly, the link connecting two nodes is not\nutilized when generating their text representations. In fact, linked\nnodes can benefit each other regarding text semantics understanding. For example, given a paper on \"LDA\" and its citation nodes which are related to topic modeling, the \"LDA\" can be more likely interpreted as \"Latent Dirichlet Allocation\" rather than \"Linear Discriminant Analysis\". In addition, this paradigm may yield textual embeddings that are not pertinent to downstream tasks, thereby impeding the model's ability to learn node representations suitable for such tasks. Moreover, given that the formation of the graph's topological structure is intrinsically driven by the node attribute~\\citep{zhao2022learning}, this paradigm may adversely affect the comprehension of the graph topology.\n\n% In general, graph topology and node attributes constitute two indispensable components of TAGs. Graph topology encapsulates the local structures of nodes by delineating their interconnections in a graph; node attributes convey the semantics of nodes by endowing them with textual features. Hence, the nucleus of representation learning on TAGs resides in the merging of graph topology and node attributes. \n% One prevalent approach is to encode the texts of nodes with a fixed LM, and subsequently utilize the LM embeddings as features to train a GNN for message propagation~\\citep{gururangan2020don,chien2021node,yasunaga2022linkbert}.\n% % Existing methods usually adopt a \u201ccascaded architecture\u201d~\\citep{jin2021bite,li2021adsgnn,zhang2019shne,zhu2021textgnn}, where the text information of each node is first encoded via pretrained language models (PLMs), then the node representations are aggregated via graph neural networks (GNNs). \n% Nevertheless, such learning paradigm suffers numerous drawbacks. First of all, this is not an end-to-end process. The modeling of node attributes and graph topology are segregated, leading to limited modeling capacity. Next, this paradigm results in the textual embeddings that are not relevant to downstream tasks, hindering the model's ability to learn node representations that are suitable for downstream tasks. Lastly, as the formation of the graph's topological structure is intrinsically driven by the node attribute~\\citep{zhao2022learning}, this paradigm may negatively impact the understanding of the graph topology. \n\n% To overcome these limitations, endeavors have been made~\\citep{zhu2021textgnn,li2021adsgnn,yang2021graphformers,bi2021leveraging,pang2022improving,jin2022heterformer} to co-train GNNs and LM under a joint learning framework. Their motivation stems from the \n\n%  **************************************\n% In accordance with the principle of network homophily~\\citep{mcpherson2001birds}, linked nodes exhibit correlated semantics that can provide mutual cues, thereby mitigating the aforementioned issues to a certain degree. On one hand, graphic patterns can serve as potent semantic indicators for nodes possessing limited textual features. For instance, as depicted in Figure~\\ref{fig:example}, given a paper on \"LDA\" and its citation nodes related to topic modeling, the neighborhood information renders \"LDA\" more likely to be interpreted as \"Latent Dirichlet Allocation\" rather than \"Linear Discriminant Analysis.\" On the other hand, textual features can signify semantic relationships within the network, thereby complementing graphic patterns. As illustrated in Figure~\\ref{fig:example}, given a paper node \"Roberta,\" its neighboring papers are likely associated with \"natural language processing.\" Consequently, rather than merely stacking GNNs and Pre-trained Language Models (PLMs), exploring the co-training paradigm is essential for effectively synergizing the respective strengths of GNNs and PLMs in representing network structures and text semantics, ultimately enhancing representation learning on TAGs.\n%  **************************************\n\n\n% According to the network homophily~\\citep{mcpherson2001birds}, linked nodes have correlated semantics that can mutually clue each other, which helps alleviate the aforementioned problems to some extent. On the one hand, graphic patterns can be strong semantic indicators for the nodes with fewer textual features. As in Figure~\\ref{fig:example}, given a paper on \u201cLDA\u201d and its citation nodes which are related to topic modeling. Based on the neighborhood information, \u201cLDA\u201d can be more likely interpreted as \u201cLatent Dirichlet Allocation\u201d rather than \u201cLinear Discriminant Analysis\u201d. On the other hand, textual features can indicate semantic relationships in the network, which serves as complementary of graphic patterns. In Figure~\\ref{fig:example}, given a paper node \u201cRoberta\u201d, its paper neighbors are likely related to \u201c\n% natural language processing\u201d. Therefore, instead of simply stacking GNNs and PLMs, investigating the co-training paradigm is crucial for effectively collaborating the respective merits of GNNs and PLMs in representing network structures and text semantics, and ultimately improving the representation learning on TAGs.\n\n\n\nFortunately, recent efforts have been undertaken~\\citep{li2017ppne,yang2021graphformers,bi2021leveraging,pang2022improving,jin2022heterformer,yan2024comprehensive} to co-train GNNs and LMs within a unified learning framework. For example, GraphFormers~\\citep{yang2021graphformers} introduces GNN-nested transformers, facilitating the joint encoding of text and node features. Heterformer~\\citep{jin2022heterformer} alternately stacks the graph aggregation module and a transformer-based text encoding module into a cohesive model to capture network heterogeneity. Despite the demonstrated efficacy of existing methods, they are encumbered by two primary drawbacks that may undermine the quality of representation learning. Firstly, these methods typically employ supervised training, necessitating a substantial volume of labeled data. However, in numerous scientific domains, labeled data are scarce and expensive to obtain~\\citep{hu2019strategies,wang2021self}. \n% For instance, accurately labeling an unknown gene often demands an extensive understanding of molecular biology, presenting a formidable challenge even for seasoned researchers~\\citep{hu2019strategies}. \nSecondly, these methods rely exclusively on limited optimization objectives to learn the entire model. When GNNs and LMs are jointly trained, the associated parameters are also learned through the constrained optimization objectives. It has been observed that such an optimization approach fails to capture the fine-grained correlations between textual features and graphic patterns~\\citep{yang2021graphformers, zhou2020s3}. Consequently, the importance of learning graph representations in an unsupervised or self-supervised manner is becoming increasingly paramount.\n\n\n% Fortunately, recent endeavors have been made~\\citep{zhu2021textgnn,li2021adsgnn,yang2021graphformers,bi2021leveraging,pang2022improving,jin2022heterformer} to co-train GNNs and LMs under a joint learning framework. GraphFormers~\\citep{yang2021graphformers} introduces GNN-nested transformers, which enables the joint encoding of text and node features. Heterformer~\\citep{jin2022heterformer} alternately stacks the graph aggregation module and a transformer-based text encoding module into a unified model to characterize the network heterogeneity. \n% % some recent studies have aimed to embrace this challenge. GraphFormers~\\citep{yang2021graphformers} introduces GNN-nested transformers, which enables the joint encoding of text and node features. Heterformer~\\citep{jin2022heterformer} alternately stacks the graph aggregation module and a transformer-based text encoding module into a unified model to characterize the network heterogeneity. \n% Despite the demonstrated efficacy of existing methods to a certain degree, they suffer from two major drawbacks that may compromise the quality of representation learning. First, these methods are usually trained in a supervised fashion, which requires a large number of labeled data. Nevertheless, in many scientific domains, labeled data are scarce and costly to acquire. For example, labeling an unknown gene accurately often requires extensive knowledge of molecular biology, which poses a great challenge even for experienced researchers~\\citep{hu2019strategies}. Second, these methods solely depend on one optimization objective to learn the entire model. When GNNs and LMs are jointlt trained, the involved parameters are also learned through the limited optimization objective. \n% It has been observed that such an optimization way fails to capture the fine-grained correlations between textual features and graphic patterns~\\citep{yang2021graphformers, zhou2020s3}. Therefore, it is becoming increasingly important to learn the representations of graphs in an unsupervised or self-supervised fashion.\n\n% However, these methods only rely on one optimization objective to learn the entire model. It has been found that such an optimization way is easy to suffer from issues such as data sparsity~\\citep{yang2021graphformers} and thus fails to capture the fine-grained correlations between textual features and graphic patterns.\n\n% To encode network structures, representation learning on graphs~\\citep{dong2020heterogeneous,yang2020heterogeneous} has been extensively studied. \n% % However, early studies on \u201cshallow\u201d node embedding~\\citep{dong2017metapath2vec,perozzi2014deepwalk} do not consider node attributes, thus cannot utilize text in each node; \n% -attributed node embedding methods~\\citep{cen2019representation} and graph neural networks (GNNs)~\\citep{hamilton2017inductive,kipf2016semi,velivckovic2017graph,wang2019heterogeneous} assign an attribute vector (e.g., bag-of-words or context-free text embeddings~\\citep{mikolov2013distributed}) to each node as its initial feature, but such context-free features are not strong enough to capture text semantics from the modern view of contextualized language models. For example, \u201ctransformer\u201d in electrical engineering papers and \u201ctransformer\u201d in machine learning papers should have different meanings given their context, but they are reflected in the same entry in bag-of-words and have the same context-free embedding.\n\n\n\n\n% To encode text semantics, transformer~\\citep{vaswani2017attention} is a powerful architecture featured by its fully connected attention mechanism. Taking transformer as the backbone, pretrained language models (PLMs)~\\citep{clark2020electra,devlin2018bert,liu2019roberta} learned from web-scale corpora can obtain contextualized semantics of words and documents. However, such models can hardly be adapted to encode network structure data due to their sequential architectures.\n\n% Due to the aforementioned limitations of GNNs and PLMs, it is imperative to explore a new model architecture that can collaborate their respective merits in representing network structures and text semantics, and can finally contribute to representation learning on TAGs. Existing methods usually adopt a \u201ccascaded architecture\u201d~\\citep{jin2021bite,li2021adsgnn,zhang2019shne,zhu2021textgnn}, where the text information of each node is first encoded via PLMs, then the node representations are aggregated via GNNs. (Figure~\\ref{fig:example} depicts this architecture.) In this way, text encoding and graph aggregation are operated consecutively, so network structures cannot provide hints to text understanding. Moreover, they rely on the node classification loss to learn the entire model. When context data is incorporated, the involved parameters are also learned through the only\n% optimization objective. It has been found that such an optimization way is easy to suffer from issues such as data sparsity~\\citep{yang2021graphformers}. \n\n% However, according to network homophily~\\citep{mcpherson2001birds}, linked nodes have correlated semantics that can mutually clue each other, which helps alleviate the aforementioned problems to some extent. On the one hand, graphic patterns can be strong semantic indicators for the nodes with fewer textual features. For example, given a paper on \u201cLDA\u201d and its citation nodes which are related to topic modeling. Based on the neighborhood information, \u201cLDA\u201d can be more likely interpreted as \u201cLatent Dirichlet Allocation\u201d rather than \u201cLinear Discriminant Analysis\u201d. On the other hand, textual features can indicate semantic relationships in the network, which serves as complementary of graphic patterns. For example, given a paper node \u201cRoberta\u201d, its paper neighbors are likely related to \u201c\n% natural language processing\u201d. Therefore, instead of simply stacking GNNs and PLMs, such inner correlations between textual features and graphic patterns are required to be emphasized and mutually reinforced in the model design. Therefore, there is a need to rethink the learning paradigm to develop more effective representation learning methods on TAGs.\n\n\n\nIn order to tackle the aforementioned challenges, we draw inspiration from the concept of contrastive learning to enhance representation learning on TAGs. Contrastive learning~\\citep{chen2020simple,hassani2020contrastive,he2020momentum,velickovic2019deep} refines representations by drawing positive pairs closer while maintaining a distance between negative pairs. As data sparsity and limited supervision signals constitute the two primary learning obstacles associated with existing co-training methods, contrastive learning appears to offer a promising solution to both issues: it capitalizes on intrinsic data correlations to devise auxiliary training objectives and bolsters data representations with an abundance of self-supervised signals.\n\n\n% To address the above challenges, we borrow the idea of contrastive learning for improving the representation learning on TAGs. Contrastive learning~\\citep{chen2020simple,hassani2020contrastive,he2020momentum,velickovic2019deep} is a newly emerging paradigm, which learns representations by pushing positive pairs closer while keeping negative pairs far apart. As previously discussed, data sparsity and limited supervision signals are the two major learning issues with the existing co-training methods. Fortunately, contrastive learning seems to provide a promising solution to both problems: it utilizes the intrinsic data correlations to devise auxiliary training objectives and enhances the data representations with rich self-supervised signals. \n\nIn practice, representation learning on TAGs with contrastive learning is non-trivial, primarily encountering the following three challenges: (1)~\\textit{How to devise a learning framework that capitalizes on the distinctive data properties of TAGs?} The contextual information within TAGs is manifested in a multitude of forms or varying intrinsic characteristics, such as tokens, nodes, or sub-graphs, which inherently exhibit complex hierarchical structures. Moreover, these hierarchies are interdependent and exert influence upon one another. How to capitalize these unique properties of TAGs remains an open question. (2)~\\textit{How to design effective contrastive tasks?} To obtain an effective node embedding that fully encapsulates the semantics, relying solely on the hierarchical topological views of TAGs remains insufficient. Within TAGs, graph topological views and textual semantic views possess the capacity to mutually reinforce one another, indicating the importance of exploring the cross-view contrastive mechanism. Moreover, the hierarchies in TAGs can offer valuable guidance in selecting positive pairs with analogous semantics and negative pairs with divergent semantics, an aspect that has received limited attention in existing research~\\citep{xu2021self,kim2021self}. (3)~\\textit{How to learn distinctive representations?} In developing the contrastive learning framework, we draw inspiration from the recently proposed spectral contrastive learning method~\\citep{haochen2021provable}, which outperforms several contrastive baselines with solid theoretical guarantees. However, we demonstrate that, from a spectral perspective, the spectral contrastive loss primarily learns the low-frequency component (LFC) of the graph, significantly attenuating the effects of high-frequency components (HFC). Recent studies suggest that the LFC does not necessarily encompass the most vital information~\\citep{bo2021beyond,chen2019drop}, and would ultimately contribute to the over-smoothing problem~\\citep{cai2020note,chen2020measuring,li2018deeper,liu2020towards}, causing node representations to converge to similar values and impeding their differentiation. Consequently, more explorations are needed to determine how to incorporate the HFC to learn more discriminative embeddings.\n\n\n\n\n% \\begin{itemize}\n%     \\item \\textbf{How to devise a hierarchical contrastive learning framework that capitalizes on the distinctive data properties of TAGs?} \n%     % The contextual information of TAGs exists in different forms or with varying intrinsics, including token, node, or sub-graph, which naturally possesses complex hierarchical structures. Moreover, different hierarchies are dependent and will influence each other. Learning node representations that embrace such hierarchical semantic structures can greatly benefit the semantic understanding on various downstream tasks. How to achieve this by contrastive learning is still an open problem.\n%     Contextual information within TAGs is manifested in a multitude of forms or varying intrinsic characteristics, such as tokens, nodes, or sub-graphs, which inherently exhibit complex hierarchical structures. Moreover, these hierarchies are interdependent and exert influence upon one another. The acquisition of node representations that encapsulate these hierarchical semantic structures can significantly enhance semantic comprehension across a range of downstream tasks. The means by which this can be achieved through contrastive learning remains an open question.\n%     % The contextual information of TAGs manifests in diverse forms or with varying intrinsics, such as token, node, or sub-graph, which inherently possess intricate hierarchical structures. Furthermore, different hierarchies are interdependent and will affect each other. Learning node representations that encompass such hierarchical semantic structures can substantially facilitate the semantic understanding on various downstream tasks. How to accomplish this by contrastive learning remains an unresolved problem.\n%     \\item \\textbf{How can cross-views be devised for more challenging contrastive tasks? } \n%  To obtain an effective node embedding that fully encapsulates the semantics, relying solely on hierarchical topological views for contrastive learning proves to be insufficient. Within TAGs, graph topological views and textual semantic views possess the capacity to mutually reinforce one another. As a result, exploring the cross-view contrastive mechanism is of particular importance for Hierarchical Graph Neural Networks (HGNNs). Furthermore, conventional contrastive methods typically generate negative pairs through exhaustive sampling from a given noise distribution, utilizing all sampled negative pairs without discrimination~\\citep{}. This approach offers no guarantee that the negative pairs acquired in this manner exhibit genuinely distinct semantics. Consequently, samples with similar semantics may be erroneously positioned far apart by these methods, thereby compromising the quality of the learned node representations. Due to the semantic hierarchies that naturally arise in TAGs, these structures can offer valuable guidance in selecting positive pairs with analogous semantics and negative pairs with divergent semantics, an aspect that has received limited attention in existing research~\\citep{}.\n%     % To learn an effective node embedding which can fully encode the semantics, performing contrastive learning only on the hierarchical topological views is actually distant from sufficient. \n%     % In TAGs, the graph topological views and textual semantic views can mutually reinforce each other.\n%     % Therefore, investigating the cross-view contrastive mechanism is especially important for HGNNs.\n%     %  Moreover, existing contrastive methods typically construct negative pairs by exhaustive sampling from some noise distribution, and all the sampled negative pairs are utilized without selection. There is no assurance that the negative pairs obtained in this manner possess truly distinct semantics. Consequently, some samples with similar semantics may be erroneously embedded far apart by these methods, which impairs the quality of learned node representations. Owing to the semantic hierarchies that naturally occur in TAGs, these semantic structures can provide useful guidance to select positive pairs with similar semantics and negative pairs with distinct semantics, which is also less investigated by existing works.\n%     \\item \\textbf{How to learn more distinctive representations?} \n%     In developing the contrastive learning framework, we draw inspiration from the recently proposed spectral contrastive learning method~\\citep{haochen2021provable}, which outperforms several robust contrastive baselines with solid theoretical guarantees. However, devising contrastive objectives based on spectral contrastive loss remains insufficient for our task. We demonstrate in this paper that, from a spectral perspective, the spectral contrastive loss primarily learns the low-frequency component (LFC) of the graph, significantly attenuating the effects of high-frequency components (HFC). Recent studies suggest that the LFC does not necessarily encompass the most vital information, while the HFC may encode valuable information that enhances performance~\\citep{bo2021beyond,chen2019drop}. Furthermore, the LFC ultimately contributes to the over-smoothing problem~\\citep{cai2020note,chen2020measuring,li2018deeper,liu2020towards}, particularly when the network exhibits heterogeneous characteristics, causing node representations to converge to similar values and impeding their differentiation. In this context, relying solely on spectral contrastive loss fails to adequately capture the varying significance of different frequency components, thereby limiting the expressiveness of learned representations and yielding suboptimal learning performance. Further exploration is needed to determine how to incorporate the HFC to learn more discriminative embeddings.\n%     % To devise the contrastive learning framework, we are motivated by the recently proposed spectral contrastive learning method~\\citep{haochen2021provable}, which surpasses several strong contrastive baselines with solid theoretical guarantees. However, designing contrastive objectives based on the spectral contrastive loss is still inadequate for our task. We demonstrate in this paper that the spectral contrastive loss only learns the low-frequency component (LFC) of the graph from a spectral perspective, where the effects of high-frequency components (HFC) are much more attenuated. Recent studies have indicated that the LFC does not necessarily contain the most crucial information; while HFC may also encode useful information that is beneficial for the performance~\\citep{bo2021beyond,chen2019drop}. Moreover, LFC eventually leads to the over-smoothing problem~\\citep{cai2020note,chen2020measuring,li2018deeper,liu2020towards} especially when the network exhibits a heterogeneous characteristic, where node representations converge to similar values, thus nodes cannot be easily distinguished.\n%     % In this regard, merely using the spectral contrastive loss cannot adequately capture the varying significance of different frequency components, thus constraining the expressiveness of learned representations and producing suboptimal learning performance. How to incorporate the HFC to learn a more discriminative embedding still requires explorations.\n% \\end{itemize}\n\n\n\n% aims to let the model learn from the intrinsic structure of the raw data. Contrastive learning, as one typical technique of self-supervised learning, has attracted considerable attentions~\\citep{chen2020simple,hassani2020contrastive,he2020momentum,velickovic2019deep}. \n% % By extracting positive and negative samples in data, contrastive learning aims at maximizing the similarity between positive samples while minimizing the similarity between negative samples. \n% As previously discussed, data sparsity and limited supervision signals are the two major learning issues with existing pretrained methods. Fortunately, contrastive learning seems to provide a promising solution to both problems: it utilizes the intrinsic data correlation to devise auxiliary training objectives and enhances the data representations with rich self-supervised signals. \n\n\n% To address the above issues, we borrow the idea of self-supervised learning for improving the representation learning on TAGs. Self-supervised learning~\\citep{devlin2018bert,mikolov2013distributed} is a newly emerging paradigm, which aims to let the model learn from the intrinsic structure of the raw data. Contrastive learning, as one typical technique of self-supervised learning, has attracted considerable attentions~\\citep{chen2020simple,hassani2020contrastive,he2020momentum,velickovic2019deep}. \n% % By extracting positive and negative samples in data, contrastive learning aims at maximizing the similarity between positive samples while minimizing the similarity between negative samples. \n% As previously discussed, data sparsity and limited supervision signals are the two major learning issues with existing pretrained methods. Fortunately, contrastive learning seems to provide a promising solution to both problems: it utilizes the intrinsic data correlation to devise auxiliary training objectives and enhances the data representations with rich self-supervised signals. \n\n\n% In particular, human praxeology lays the foundation of homophily in social networks~\\citep{}. Capturing the inner correlations across the network can help understand the underlying reasons behind human behaviors and provide abundant unsupervised signals for downstream tasks. For example, edges in academic graphs represent human citation behaviors. Given a paper on \"Bert\" and the neighbor nodes on \"recommender system\", the model can understand the authors are citing papers related to Bert's application in recommender system. Such an understanding is beneficial for downstream tasks, \\textit{e.g.}, node classification and link prediction. In this paper, we propose to utilize contrastive learning to capture the inner correlations between textual features and graphic patterns in TAGs. \n\n\n\n\n% edges in this graph connect augmentations of the same datapoint, and ground-truth classes naturally form connected sub-graphs~\\citep{haochen2021provable} (see Section~\\ref{} for more details). LFC helps explain the generalization of contrastive learning methods. However, too much LFC will lead to over-smoothing problem where the representations become too similar with each other and lose the discrimination. How to introduce the high-frequency component (HFC) to learn a more discriminative embedding still remains exploration.\n\n% This requires us to address the following three fundamental problems:\n\n% \\textit{(1) How to design a TAG contrastive mechanism?} For a TAG, the contextual information exists in different forms or with varying intrinsics, including token, sequence, node, or sub-graph, which naturally possesses complex hierarchical structures. Moreover, different hierarchies are dependent and will influence each other. For example, token-level representation influence the semantics of nodes and sub-graph properties. Therefore, it is not easy to develop a unified approach to characterizing such data correlations. For this problem, we are inspired by the recently proposed spectral contrastive learning method~\\citep{haochen2021provable}, which \n% % performs spectral decomposition on the implicit population augmentation graph~\\citep{haochen2021provable} and can be succinctly written as a contrastive learning objective on neural net representations. It has been shown to \n% outperforms several strong contrastive baselines with solid theoretical guarantees. \n\n% % \\textit{(2) How to select proper views in a TAG?} Proper view selection is critical for the success of contrastive learning~\\citep{tian2020makes,wang2022rethinking}. When selecting positive views, considering the difference between textual features and graphs, we need to design specific view selection methods to capture their unique properties. \n% % % Specifically, for textual representations, we should model tokens in a contextualized way to alleviate the ambiguity problems (\\textit{e.g.,} Apple means a company not a fruit). For node representations, textual features indicate its intrisic semantics while neighborhood information reflects the local structure. Both views should be carefully considered. \n% % When selecting negative views, existing contrastive methods commonly construct negative pairs by exhaustive sampling from some noise distribution, where all the sampled negative pairs are used without selection. However, false negatives will break the semantic structure in latent space. To select proper positive and negative views in a TAG, one strategy is to enhance the information diversity in two views, and the other is to generate harder negative samples of high quality. In short, selecting proper contrastive views is very crucial for TAGs.\n\n% \\textit{(2) How to design a proper contrastive loss?} As mentioned above, using a novel concept of the augmentation graph, where edges in this graph connect augmentations of the same datapoint, and ground-truth classes naturally form connected sub-graphs~\\citep{haochen2021provable}, spectral contrastive loss possesses provable performance guarantees on realistic empirical settings. We will prove that spectral contrastive loss learns the low-frequency component (LFC) of the augmentation graph (see Section~\\ref{}). LFC helps explain the generalization of contrastive learning methods. However, too much LFC will lead to over-smoothing problem where the representations become too similar with each other and lose the discrimination. How to introduce the high-frequency component (HFC) to learn a more discriminative embedding still remains exploration.\n\n% % It is well known that a proper contrastive loss will further promote to learn a more discriminative embedding~\\citep{chen2020simple,tian2020contrastive}. Recent theoretical works analyze contrastive learning using a novel concept of the augmentation graph, where edges in this graph connect augmentations of the same datapoint, and ground-truth classes naturally form connected sub-graphs~\\citep{haochen2021provable}. Based on the augmentation graph, they propose spectral contrastive loss that possesses provable performance guarantees on realistic empirical settings. We will prove that spectral contrastive loss learns the low-frequency component (LFC) of the augmentation graph. LFC helps explain the generalization of contrastive learning methods. However, too much LFC will lead to over-smoothing problem where the representations become too similar with each other and lose the discrimination. How to introduce the high-frequency component (HFC) to learn a more discriminative embedding still remains exploration.\n\nTo this end, we present a novel \\textbf{H}igh-frequency \\textbf{A}ware \\textbf{S}pectral \\textbf{H}ierarchical \\textbf{Co}ntrastive Selective Co\\textbf{d}ing framework (\\textbf{HASH-CODE}) to enhance TAG representation learning. Building upon a GNN and Transformer architecture~\\citep{yang2021graphformers,zhu2021textgnn}, we propose to jointly train the GNN and Transformer with self-supervised signals (Figure~\\ref{fig:example}(b) depicts this architecture). The primary innovation lies in the contrastive joint-training stage. Specifically, we devise five self-supervised optimization objectives to capture hierarchical intrinsic data correlations within TAGs. These optimization objectives are developed within a unified framework of contrastive learning. Moreover, we propose a loss that can be succinctly expressed as a contrastive learning objective, accompanied by robust theoretical guarantees. Minimizing this objective results in more distinctive embeddings that strike a balance between LFC and HFC. Consequently, the proposed method is capable of characterizing correlations across varying levels of granularity or between different forms in a general manner. \n\nOur main contributions are summarized as follows:\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]\n    \\item We propose five self-supervised optimization\nobjectives to maximize the mutual information of context information in different forms or granularities.\n    \\item We systematically examine the fundamental limitations of spectral contrastive loss from the perspective of spectral domain. We prove that it learns the LFC and propose an HFC-aware contrastive learning objective that makes the learned embeddings more discriminative.\n    \\item Extensive experiments conducted on three million-scale text-attributed graph datasets demonstrate the effectiveness of our proposed approach.\n\\end{itemize}\n\n% The rest of this article is structured in the following manner: Section~\\ref{sec:related} delineates the current methodologies employed for representation learning on TAGs, as well as recent advancements in the realm of contrastive learning. Section~\\ref{sec:bg} furnishes the necessary background information and a comprehensive theoretical examination of the contrastive learning loss. Subsequently, the proposed approach is expounded upon in Section~\\ref{sec:method}. In Section~\\ref{sec:exp}, comparative experiments and ablation analyses are executed to substantiate the efficacy of our method. Lastly, Section~\\ref{sec:con} encapsulates the conclusions drawn from this research.\n\n% The rest of this article is organized as follows: Section~\\ref{sec:related} introduces the existing methods for the representation learning on TAGs and recent advances in contrastive learning. In Section~\\ref{sec:bg}, We provide the background and our theoretical analysis on the contrastive learning loss. Then the proposed method is presented in Section~\\ref{sec:method}. In Section~\\ref{sec:exp}, we conduct comparison experiments and ablation analysis to verify our method. Finally, Section~\\ref{sec:con} provides our conclusions.\n% \\vspace{-0.6cm}\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec:related}\n",
                "subsection 2.1": {
                    "name": "Representation Learning on TAGs",
                    "content": "\n\nRepresentation learning on TAGs constitutes a significant research area across multiple domains, including natural language processing~\\cite{wang2016linked,wang2016text,yang2022reinforcement,jin2022towards}, information retrieval~\\cite{wang2019improving,xu2019deep,jin2023predicting,jin2022code}, and graph learning~\\cite{yang2015network,yasunaga2017graph,long2021hgk,long2020graph}. In order to attain high-quality representations for TAGs, it is imperative to concurrently harness techniques from both natural language understanding and graph representation. The recent advancements in pretrained language models (PLM) and graph neural networks (GNN) have catalyzed the progression of pertinent methodologies.\n\n% Representation learning on TAGs is an important research topic in multiple areas, such as natural language processing, information retrieval and graph learning~\\cite{yang2015network,wang2016linked,yasunaga2017graph,wang2019improving,xu2019deep}, where a key challenge is node classification. This challenge can be directly cast as a text representation learning problem, which aims to leverage the textual feature of each node for learning. To learn high-quality representation for TAGs, techniques on natural language understanding and graph representation need to be jointly leveraged. Recent advances in pretrained language models (PLM) and graph neural networks (GNN) have spurred the progress of the related techniques.\n\n% **************************************\n% \\textbf{PLM \\& GNN.} \n% PLMs endeavor to derive universal language representations from extensive corpora, thereby enhancing downstream text-related tasks. Previous research on PLMs predominantly focused on context-free text embeddings, such as word2vec~\\cite{} and GloVe~\\cite{}. Nevertheless, considering that identical words may convey disparate meanings contingent upon varying contexts, sophisticated language models like ELMo~\\cite{}, BERT~\\cite{}, RoBERTa~\\cite{}, XLNet~\\cite{}, ELECTRA~\\cite{}, and GPT~\\cite{} have been developed to ascertain contextualized word representations. These models employ the transformer architecture to capture long-range and high-order semantic dependencies, resulting in significant advancements across a multitude of downstream NLP tasks. Concurrently, the task of representation learning on TAGs can be approached as a graph learning task, which has been extensively investigated by Graph Neural Networks (GNNs)~\\cite{}. Methods such as GCN~\\cite{}, GAT~\\cite{}, and GraphSage~\\cite{} devise efficacious message-passing mechanisms that facilitate information aggregation between nodes, yielding expressive graph representations. By integrating both node attributes and graph structures, GNNs have exhibited remarkable performance in various applications, including node classification and link prediction.\n\n\n% % PLMs aim to learn universal language representations from large-scale corpora, which can facilitate downstream text-related tasks. Prior studies on PLMs mainly concentrate on context-free text embeddings such as word2vec and GloVe. However, in view of the fact that the same word can convey different meanings depending on different contexts, deep language models such as ELMo, BERT, RoBERTa, XLNet, ELECTRA, and GPT are devised to learn contextualized word representations. These models adopt the transformer architecture to capture long-range and high-order semantic dependency and achieve remarkable improvement on various downstream NLP tasks. Meanwhile, the task of representation learning on TAGs can also be tackled as a graph learning task that has been extensively explored by GNNs. Such methods (e.g., GCN, GAT , GraphSage) devise effective message passing mechanisms that enable information aggregation between the nodes for expressive graph representations. By incorporating both node attributes and graph structures, GNNs have demonstrated impressive performance in various applications, such as node classification and link prediction.\n\n% However, both PLMs and GNNs solely exploit a portion of the observed information (i.e., textual or structural) for representation learning, indicating potential avenues for further enhancement.\n% ****************************************\n\n\n\n% Nonetheless, both PLMs and GNNs only exploit partial observed information (i.e., textual or structural) for representation learning, leaving room for future improvement.\n\n\\textbf{Seperated Training.} \nA number of recent efforts strive to amalgamate GNNs and LMs, thereby capitalizing on the strengths inherent in both models. The majority of prior investigations on TAGs employ a \"cascaded architecture\"~\\citep{jin2021bite,li2021adsgnn,zhang2019shne,zhu2021textgnn}, in which the text information of each node is initially encoded through transformers, followed by the aggregation of node representations via GNNs. Nevertheless, these PLM embeddings remain non-trainable during the GNN training phase. Consequently, the model performance is adversely impacted by the semantic modeling process, which bears no relevance to the task and topology at hand.\n\n% Some recent endeavors aim at the integration of GNNs and LMs, which enables one to benefit from the strengths of both models. Most previous studies on TAGs adopt a \u201ccascaded architecture\"~\\citep{jin2021bite,li2021adsgnn,zhang2019shne,zhu2021textgnn}, where the text information of each node is first encoded via transformers, then the node representations are aggregated via GNNs. However, these PLM embeddings are still non-trainable during the GNN training phase. Therefore, the model performance suffers from the semantic modeling process that is irrelevant to the task and topology. \n\n\\textbf{Co-training.} \nIn an attempt to surmount these challenges, concerted efforts have been directed towards the co-training of GNNs and PLMs within a unified learning framework. GraphFormers~\\cite{yang2021graphformers} presents GNN-nested transformers, facilitating the concurrent encoding of text and node features. Heterformer~\\cite{jin2022heterformer} alternates between stacking the graph aggregation module and a transformer-based text encoding module within a unified model, thereby capturing network heterogeneity. However, these approaches solely depend on a single optimization objective for learning the entire model, which considerably constrains their capacity to discern the fine-grained correlations between textual and graphical patterns. \n% In contrast, HASH-CODE expands upon the joint learning framework of GraphFormers, enabling the characterization of text-graph correlations across varying levels of granularity and between distinct forms in a more generalized manner.\n\n% To address these challenges, efforts have been devoted to co-training GNNs and PLM within a unified learning framework. GraphFormers~\\cite{yang2021graphformers} introduces GNN-nested transformers, which enables the joint encoding of text and node features. Heterformer~\\cite{jin2022heterformer} alternately stacks the graph aggregation module and a transformer-based text encoding module into a unified model to characterize the network heterogeneity. However, these methods only rely on one optimization objective to learn the entire model. Therefore, the ability of capturing the fine-grained  correlations between textual and graphic patterns is significantly limited. In comparison, HASH-CODE extends the joint learning framework of GraphFormers and is able to characterize the text-graph correlations in varying levels of granularity and between different forms in a general way.\n\n\n% Most previous studies on TAGs adopt a \u201ccascaded architecture\"~\\citep{jin2021bite,li2021adsgnn,zhang2019shne,zhu2021textgnn}, where the text information of each node is first encoded via transformers, then the node representations are aggregated via GNNs. One drawback of such models is that text and network signals are processed consecutively, so the network information cannot benefit text encoding. To overcome this drawback, GraphFormers~\\citep{yang2021graphformers} introduces GNN-nested transformers so that text and node features can be encoded jointly. However, the message passing in GraphFormers is restricted within first-order neighbors and thus it is unaware of high-order information. Therefore, the ability of capturing correlations between textual and graphic patterns is significantly limited. In comparison, HASH-CODE extends the joint learning framework of GraphFormers and is able to characterize the text-graph correlations in varying levels of granularity and between different forms in a general way.\n\n\n\n% In recent years, breakthroughs in pretrained language models (PLM) and graph neural networks (GNN) significantly advance the development of corresponding techniques. \n\n% The PLMs are proposed to learn universal language models with neural networks trained on a large-scale corpus. The early works were based on shallow networks, e.g, word embeddings learned by Skip-Gram~\\cite{mikolov2013distributed} and GloVe~\\cite{pennington2014glove}. In recent years, the backbone networks are being quickly scaled up: from EMLo~\\cite{sarzynska2021detecting}, GPT~\\cite{radford2018improving}, to BERT~\\cite{devlin2018bert}, XLNet~\\cite{yang2019xlnet}, T5~\\cite{raffel2020exploring}, GPT-3~\\cite{brown2020language}. The large-scale models, which get fully trained with massive data, demonstrate superior performances on general NLP tasks. However, these models only utilize text information in the corpora. \n% % One of the most critical usages of PLMs is text representation, where the underlying semantics of texts are captured by low-dimensional embeddings. Such embeddings achieve competitive results on downstream tasks, like text retrieval and classification~\\cite{reimers2019sentence,luan2021sparse,gao2021simcse,su2021whitening}.\n\n% The GNNs are recognized as powerful tools for modeling graph data~\\cite{hamilton2017representation,zhou2020graph}. Such methods (e.g., GCN~\\cite{kipf2016semi}, GAT~\\cite{velivckovic2017graph}, GraphSage~\\cite{hamilton2017inductive}) learn effective message passing mechanisms such that information between the nodes can get aggregated for expressive graph representations. Despite the success of these models, when some nodes carry text information, they lack the power of handling textual signals in a contextualized way.\n\n"
                },
                "subsection 2.2": {
                    "name": "Contrastive Learning",
                    "content": "\n\\textbf{Empirical Works on Contrastive learning.} \nContrastive methods~\\citep{chen2020simple,chen2020improved,he2020momentum} derive representations from disparate views or augmentations of inputs and minimize the InfoNCE loss~\\citep{oord2018representation}, wherein two views of identical data are drawn together, while views from distinct data are repelled. The acquired representation can be utilized to address a wide array of downstream tasks with exceptional performance. In the context of node representation learning on graphs, DGI~\\cite{velickovic2019deep} constructs local patches and global summaries as positive pairs. GMI~\\cite{peng2020graph} is designed to establish a contrast between the central node and its local patch, derived from both node features and topological structure. MVGRL~\\cite{hassani2020contrastive} employs contrast across views and explores composition between varying views.\n\n% Contrastive methods~\\citep{chen2020simple,chen2020improved,he2020momentum} learn representations from different views or augmentations of inputs and minimize the InfoNCE loss~\\citep{oord2018representation}, where two views of the same data are attracted while views from different data are repulsed. The learned representation can be applied to address a broad spectrum of downstream tasks with high performance. For the node representation learning on graphs, DGI~\\cite{velickovic2019deep} builds local patches and global summary as positive pairs. GMI~\\cite{peng2020graph} is proposed to contrast between the center node and its local patch from node features and topological structure. MVGRL~\\cite{hassani2020contrastive} employs contrast across views and experiments composition between different views. \n\n\\textbf{Theoretical works on Contrastive Learning.}\nThe exceptional performance exhibited by contrastive learning has spurred a series of theoretical investigations into the contrastive loss. The majority of these studies treat the model class as a black box, with notable exceptions being the work of~\\cite{lee2021predicting}, which scrutinizes the learned representation with linear models, and the research conducted by~\\cite{tian2022deep} and~\\cite{wen2021toward}, which examine the training dynamics of contrastive learning for linear and 2-layer ReLU networks. Most relevant to our research is the study by~\\cite{saunshi2022understanding}, which adopts a spectral graph perspective to analyze contrastive learning methods and introduces the spectral contrastive loss. We ascertain that the spectral contrastive loss solely learns the LFC of the graph.\n\n% The remarkable performance of contrastive learning has stimulated a series of theoretical works that investigate the contrastive loss, most of which regard the model class as a black box except for the work of Lee et al. (2020) which examines the learned representation with linear models, and the works of Tian (2022) and Wen and Li (2021) which study the training dynamics of contrastive learning for linear and 2-layer ReLU networks. Most pertinent to our work is Saunshi et al. (2022) where a spectral graph perspective is adopted to analyze the contrastive learning methods and proposes the spectral contrastive loss. We discover that the spectral contrastive loss only learns the LFC of the graph.\n% A spectral graph point of view is also taken in~\\citep{haochen2021provable} to analyze contrastive learning methods and proposes the spectral contrastive loss. However, spectral contrastive loss only learns the LFC of the graph.\n\nDifferent from the existing works, our research represents the first attempt to contemplate the correlations inherent within the contextual information as self-supervised signals in TAGs. We endeavor to maximize the mutual information among the views of the token, node, and subgraph, which encompass varying levels of granularity within the contextual information. Our HFC-aware loss facilitates the learning of more discriminative data representations, thereby enhancing the performance of downstream tasks.\n\n% Different from the above approaches, our work is the first to consider the correlations within the contextual information as the self-supervised signals in TAGs. We maximize the mutual information among the views of the token, node, and subgraph, which are in different levels of granularity of the contextual information. Our HFC-aware loss can learn more discriminative data representations that improve downstream tasks' performance.\n\n\n\n% Here we mainly focus on reviewing the graph-related contrastive learning methods. Specifically, DGI~\\cite{velickovic2019deep} builds local patches and global summary as positive pairs, and utilizes Infomax~\\cite{linsker1988self} theory to contrast. Along this line, GMI~\\cite{peng2020graph} is proposed to contrast between the center node and its local patch from node features and topological structure. MVGRL~\\cite{hassani2020contrastive} employs contrast across views and experiments composition between different views. \\section{Preliminaries}\n\\label{sec:bg}\nIn this section, we first give the definition of the text-attributed graphs (TAGs) and formulate the node representation learning problem on TAGs. Then, we introduce our proposed HFC-aware spectral contrastive loss.\n\n% In this section, we first give the definition of the text -attributed graphs (TAGs) and formulate the node representation learning problem on TAGs. Next, we revisit the spectral clustering and spectral contrastive loss. Finally, we introduce our proposed HFC-aware loss.\n\n"
                },
                "subsection 2.3": {
                    "name": "Definition (Text-attributed Graphs)",
                    "content": "\n% \\textbf{Definition (Text -attributed Graphs).} \nA text-attributed graph is defined as $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$, where $\\mathcal{V}=\\{v_{1},...,v_{N}\\}$ and $\\mathcal{E}$ denote the set of nodes and edges, respectively. Let $A\\in \\mathbb{R}^{N\\times N}$ be the adjacency matrix of the graph such that $A_{i,j}=1$ if $v_{j}\\in \\mathcal{N}(v_{i})$, otherwise $A_{i,j}=0$. Here $\\mathcal{N}(.)$ denotes the one-hop neighbor set of a node. Besides, each node $v_{i}$ is associated with text information. \n\n% $\\mathcal{V}$, $\\mathcal{E}$, $\\mathcal{X}$ represent the sets of nodes, edges and textual attributes, respectively. Each node $v\\in \\mathcal{V}$ is associated with text information $x\\in \\mathcal{X}$. \n\n% \\begin{table}[t]\n%     \\centering\n%     \\caption{Main Notations Used in This Article.}\n%     {\n%     \\begin{tabular}{cc}\n%         \\toprule\n%            Notation      & Description \\\\\n%         \\midrule\n%         $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$  & 13,647,591\\\\\n%         \\#Items & 5,643,688\\\\\n%         \\#N   & 4.71 \\\\\n%         \\#Train  & 22,146,934 \\\\\n%         \\#Valid  & 30,000 \\\\\n%         \\#Test   & 306,742 \\\\\n%         \\bottomrule\n%     \\end{tabular}}\n%     \\label{tab:notation}\n%     % \\vspace{-0.4cm}\n% \\end{table}\n\n"
                },
                "subsection 2.4": {
                    "name": "Problem Statement",
                    "content": "\nGiven a textual attibuted graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$, the task is to build a model $f_{\\theta}: \\mathcal{V}\\rightarrow \\mathbb{R}^{K}$ with parameters $\\theta$ to learn the node embedding matrix $F\\in \\mathbb{R}^{N\\times K}$, taking network structures and text semantics into consideration, where $K$ denotes the number of feature channels. The learned embedding matrix $F$ can be further utilized in downstream tasks, \\textit{e.g.,} link prediction, node classification, \\textit{etc.}\n\n"
                },
                "subsection 2.5": {
                    "name": "HFC-aware Spectral Contrastive Loss",
                    "content": "\n\n\\par An important technique in our approach is the high-frequency aware spectral contrastive loss. It is developed based on the analysis of the conventional spectral contrastive loss~\\citep{haochen2021provable}. Given a node $v$, the conventional spectral contrastive loss is defined as:\n\\begin{equation}\n\\begin{aligned}\n    \\mathcal{L}_{Spectral}(v,v^{+},v^{-}) &= -2\\cdot \\mathbb{E}_{v,v^{+}}[f_{\\theta}(v)^{T}f_{\\theta}(v^{+})]\\\\\n    &\\ +\\mathbb{E}_{v,v^{-}}[(f_{\\theta}(v)^{T}f_{\\theta}(v^{-}))^{2}],\n\\end{aligned}\n\\end{equation}\nwhere $(v,v^{+})$ is a pair of positive views of node $v$, $(v,v^{-})$ is a pair of negative views, and $f_{\\theta}$ is a parameterized function from the node to $\\mathbb{R}^{K}$. Minimizing $\\mathcal{L}_{Spectral}$ is equivalent to spectral clustering on the population view graph~\\citep{haochen2021provable}, where the top smallest eigenvectors of the Laplacian matrix are preserved as the columns of the final embedding matrix $F$. \n\n\\par In Appendix~\\ref{sec:spectral}, we demonstrate that, from a spectral perspective, $\\mathcal{L}_{Spectral}$ primarily learns the low-frequency component (LFC) of the graph, significantly attenuating the effects of high-frequency components (HFC). Recent studies suggest that the LFC does not necessarily encompass the most vital information~\\citep{bo2021beyond,chen2019drop}, and would ultimately contribute to the over-smoothing problem~\\citep{cai2020note,chen2020measuring,li2018deeper,liu2020towards}.\n\nAs an alternative of such low-pass filter, to introduce HFC, we propose our HFC-aware spectral contrastive loss as follows: \n\\begin{equation}\n\\label{eq:hfc}\n\\begin{aligned}\n    \\mathcal{L}_{HFC}(v,v^{+},v^{-}) &= -2\\alpha\\cdot \\mathbb{E}_{v,v^{+}}[f_{\\theta}(v)^{T}f_{\\theta}(v^{+})]\\\\\n    &\\ +\\mathbb{E}_{v,v^{-}}[(f_{\\theta}(v)^{T}f_{\\theta}(v^{-}))^{2}],\n    \\end{aligned}\n\\end{equation} \nwhere $\\alpha$ is used to control the rate of HFC within the graph.\n\nUpon initial examination, one might observe that our $\\mathcal{L}_{HFC}$ formulation closely aligns with $\\mathcal{L}_{Spectral}$. Remarkably, the primary distinction lies in the introduction of the parameter $\\alpha$. However, this is not a mere trivial addition; it emerges from intricate mathematical deliberation and is surprisingly consistent with $\\mathcal{L}_{Spectral}$ that offers a nuanced control of the HFC rate within the graph. Minimizing our $\\mathcal{L}_{HFC}$ results in more distinctive embeddings that strike a balance between LFC and HFC. Please kindly refer to Appendix~\\ref{sec:spectral} for detailed discussions and proof.\n\n\n\n% \\subsection{Background: Spectral Clustering}\n% \\label{sec:spectral}\n% Given a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$, with adjacency matrix $A$, the Laplacian matrix of the graph is defined as $L=D - A$, where $D = diag(d_{1},...,d_{N})$ is the diagonal degree matrix ($d_{i}=\\Sigma_{j}A_{i,j}$). Then the symmetric normalized Laplacian matrix is defined as $L_{sym}=D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}$. As $L_{sym}$ is real symmetric and positive semidefinite, therefore it can be diagonalized as $L=U\\Lambda U^{T}$~\\citep{chung1997spectral}. Here $U\\in \\mathbb{R}^{N\\times N}=[u_{1},...,u_{N}]$, where $u_{i}\\in \\mathbb{R}^{N}$ denotes the $i$-th eigenvector of $L_{sym}$ and $\\Lambda=diag(\\lambda_{1},...,\\lambda_{N})$ is the corresponding eigenvalue matrix. To partition the graph, spectral clustering~\\citep{hastie2009elements,von2007tutorial} computes the first K eigenvectors and creates a feature vector $f_{K,v}\\in \\mathbb{R}^{K}$ for each node $v: \\forall k \\in [1,K], f_{K,v}(k)=u_{k}(v)$, which is in turn used to obtain K clusters by K-means or hierarchical clustering, etc.\n\n% An analogy between signals on graphs and usual signals~\\citep{shuman2013emerging} suggests to interpret the spectrum of $L_{sym}$ as a Fourier domain for graphs, hence defining filters on graphs as diagonal operators after change of basis with $U^{-1}$. It turns out that the features $f_{K,v}$ can be\n% obtained by ideal low-pass filtering of the Delta function $\\delta_{a}$ (localized at node a). Indeed, let $l_{K}$ be the step function where $l_{K}(\\lambda)=1$ if $\\lambda <\\lambda_{K}$ and 0 otherwise. We define $L_{K}$ the diagonal matrix for which $L_{K}(i,i)=l_{K}(\\lambda_{i})$. Then we have:\n% $f_{K,v}=L_{K}U^{-1}\\delta_{v}\\in \\mathbb{R}^{K}$, where we fill the last $N-K$ values with 0\u2019s. Therefore, spectral clustering is equivalent to clustering using low-pass filtering of the local descriptors $\\delta_{v}$ of each node $v$ of the graph $\\mathcal{G}$.\n\n% \\subsection{Spectral Contrastive Loss Revisited}\n% \\label{sec:scon}\n% To introduce spectral contrastive loss~\\citep{haochen2021provable}, we give the definition of population view graph~\\citep{haochen2021provable} first.\n\n% \\noindent \\textbf{Population View Graph.} A population view graph is defined as $\\mathcal{G}=(\\mathcal{X},\\mathcal{W})$, where the set of nodes comprises all augmented views $\\mathcal{X}$ of the population distribution, with $w_{xx^{'}}\\in \\mathcal{W}$ the edge weights of the edges connecting nodes $x,x^{'}$ that correspond to different views of the same input datapoint. The core assumption made is that this graph cannot be split into a large number of disconnected subgraphs. This set-up aligns well with the intuition that in order to generalize, the contrastive notion of \u201csimilarity\u201d must extent beyond the purely single-instance-level, and must somehow connect distinct inputs points.\n\n% \\noindent \\textbf{Spectral Contrastive Loss.} Using the concept of population view graph, spectral contrastive loss is defined as:\n% \\begin{equation}\n% \\begin{aligned}\n%     \\mathcal{L}(x,x^{+},x^{-},f_{\\theta}) &= -2\\cdot \\mathbb{E}_{x,x^{+}}[f_{\\theta}(x)^{T}f_{\\theta}(x^{+})]\\\\\n%     &\\ +\\mathbb{E}_{x,x^{-}}[(f_{\\theta}(x)^{T}f_{\\theta}(x^{-}))^{2}],\n% \\end{aligned}\n% \\end{equation}\n% where $(x,x^{+})$ is a pair of views of the same datapoint, $(x,x^{-})$ is a pair of independently random views, and $f_{\\theta}$ is a parameterized function from the data to $\\mathbb{R}^{k}$. Minimizing spectral contrastive loss is equivalent to spectral clustering on the population view graph, where the top smallest eigenvectors of the Laplacian matrix are preserved as the columns of the final embedding matrix $F$. \n\n% \\subsection{Adaptive Frequency Response Spectral Contrastive Loss}\n% Let $x\\in \\mathbb{R}^{N}$ be an one-channeled input signal of all nodes, then the Graph Fourier transform and inverse Fourier transform can be defined as $\\hat{x}=\\mathcal{F}(x)=U^{T}x$ and $x=\\mathcal{F}^{-1}(\\hat{x})=U\\hat{x}$,respectively. Here, (1) $\\hat{x}$ is the Fourier transformed graph signal; (2) the columns $u_{i}$ of $U$ are considered as the graph\u2019s Fourier modes; and (3) $\\sqrt{\\lambda_{i}}$ as its set of associated \u201cfrequencies\u201d~\\citep{shuman2013emerging}.\n\n\n\n\n\n\n\n\n% \\begin{lemma}\n% \\label{lm:}\n% By spectral clustering on the population view graph, the embedding matrix $F$ learned by spectral contrastive loss is the LFC of population view graph.\n% \\end{lemma}\n\n% \\begin{proof}\n% % The Laplacian matrix $L$ of the population view graphs is real symmetric and positive semidefinite, therefore can be diagonalized as $L=U\\Lambda U^{T}$~\\citep{chung1997spectral}, where $U=(u_{1}|u_{2}|...|u_{N})\\in \\mathbb{R}^{N\\times N}$, with column $u_{i}$ as the orthonormal basis of eigenvectors and $\\Lambda\\in \\mathbb{R}^{N\\times N}$ is the diagonal matrix containing the sorted eigenvalues $0\\le \\lambda_{1}\\le \\lambda_{2}...\\le \\lambda_{N}$.\n\n% % According to spectral graph theory~\\citep{chung1997spectral}, we have: (1) the columns $u_{i}$ of $U$ are considered as the graph\u2019s Fourier modes, and (2) $\\sqrt{\\lambda_{i}}$ as its set of associated \u201cfrequencies\u201d~\\citep{shuman2013emerging}. \n\n% Given a population view graph, suppose we collect the eigenvectors of its Laplacian matrix in a matrix $U=(u_{1},u_{2},...,u_{N})$, spectral clustering computes the first K eigenvectors and creates a feature vector $f_{K,v}\\in \\mathbb{R}^{K}$ for each node $v: \\forall k \\in [1,K], f_{K,v}(k)=u_{k}(v)$, which is in turn used to obtain K clusters by K-means or hierarchical clustering, etc.\n\n% Next, we will show that the features $f_{K,v}$ can also be obtained by ideal low-pass filtering of the Delta function $\\delta_{v}$ (localized at node $v$).\n\n% Let $l_{K}$ be the step function where $l_{K}(\\lambda)=1$ if $\\lambda <\\lambda_{K}$ and 0 otherwise.\n\n% Define $L_{K}$ the diagonal matrix for which $L_{K}(i,i)=l_{K}(\\lambda_{i})$. Then we have:\n% $f_{K,v}=L_{K}U^{-1}\\delta_{v}\\in \\mathbb{R}^{K}$, where we fill the last $N-K$ values with 0\u2019s.\n\n% Therefore, spectral clustering is equivalent to clustering using low-pass filtering of local descriptors $\\delta_{v}$ of each node $v$ of the graph. The feature representation learned by spectral contrastive loss is the \\textit{low-frequency component} (LFC) of the population view graph.\n% \\end{proof}\n\n% \\py{motivation for introducing HFC}\n\n% In image signal processing, the Laplacian kernel is widely used to capture high-frequency edge information for various tasks such as image sharpening and blurring~\\citep{he2012guided}. As its counterpart in\n% Graph Signal Processing (GSP)~\\citep{shuman2013emerging}, we can multiply the graph Laplacian matrix $L$ with the input graph signal $x\\in \\mathbb{R}^{N}$, (\\textit{i.e.,} $h=Lx$) to characterize its high-frequency components \u2013 the frequencies that carry sharply varying signal information across edges of graph. On the contrary, when highlighting the LFC, we would subtract the term $Lx$ which emphasizes more on HFC from the input signal $x$, \\textit{i.e.,} $z=x - Lx$. \n\n% It should be noted that the above operation corresponds to a fixed low-pass filter in the spectral domain, where higher weights are specified for LFC. However, in practice, LFC may not always be useful, and HFC can also provide complementary insights for learning~\\citep{bo2021beyond,chen2019drop}, especially when the label information is not smooth across edges. Additionally, the HFC of the input graph signal would be unavoidably too much weakened compared with the lower ones with fixed filters, leading to the well-known over-smoothing problem~\\citep{li2018deeper}. As discussed in Section~\\ref{sec:spectral}, spectral clustering is equivalent to clustering using a low-pass filter on each node of the graph. Henceforth, the feature vectors learned by the spectral contrastive loss is LFC of the population view graph. In this regard, the fixed low-pass filters largely limit the fitting capability of contrastive learning and its variants for learning discriminative node representations. As a consequence, it is vital to capture the varying importance of frequencies in the filter to preserve more useful information and alleviate over-smoothing issues. \n\n% As an alternative of the traditional low-pass filter, a simple and elegant solution to introduce HFC is to assign a single parameter to control the rate of high-frequency substraction. \n% \\begin{equation}\n%     z=x-\\alpha Lx=(I-\\alpha L)x\\nonumber,\n% \\end{equation}\n% where $I$ is the identity matrix. We thus obtain the kernel $I-\\alpha L$ that contains HFC.\n\n% Following~\\citep{haochen2021provable}, we consider the following matrix factorization based objective for eigenvectors:\n% \\begin{equation}\n% \\begin{aligned}\n%     \\min_{F\\in \\mathbb{R}^{N\\times K}}\\mathcal{L}_{mf}(F) &=||(I-\\alpha L)-FF^{T}||_{F}^{2}\\\\\n%     &=((1-\\alpha)I+\\Sigma_{i,j}(\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}-f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})))^{2},\n%     \\end{aligned}\n% \\end{equation}\n% where $w_{x}=\\Sigma_{x^{'}\\in \\mathcal{X}}w_{xx^{'}}$ is the total weights associated to view $x$. By the classical low-rank approximation theory (Eckart-Young-Mirsky theorem~\\citep{eckart1936approximation}), minimizer $F$ possesses eigenvectors of HFC-aware kernel $I-\\alpha L$ as columns and thus contains both the LFC and HFC of the population view graph.\n\n% \\begin{lemma}\n% (HFC-aware spectral contrastive loss.) Denote $p_{x}$ is the $x$-th row of $F$. Let $p_{x}=w_{x}^{1/2}f_{\\theta}(x)$. Then, the loss function $\\mathcal{L}_{mf}(F)$ is equivalent to the following loss function for $f_{\\theta}$, called HFC-aware spectral contrastive loss, up to an additive constant:\n% \\begin{equation}\n%      \\mathcal{L}_{mf}(F)=\\mathcal{L}_{HFC}(f_{\\theta})+const\\nonumber, \n% \\end{equation}\n% where \n% \\begin{equation}\n% \\label{eq:hfc}\n% \\begin{aligned}\n%     \\mathcal{L}_{HFC}(f_{\\theta}) &= -2\\alpha \\mathbb{E}_{x,x^{+}}[f_{\\theta}(x)^{T}f_{\\theta}(x^{+})]\\\\\n%     &\\ \\ \\ \\ +\\mathbb{E}_{x,x^{-}}[(f_{\\theta}(x)^{T}f_{\\theta}(x^{-}))^{2}]\n%     \\end{aligned}\n% \\end{equation}\n\n% \\end{lemma}\n\n% \\begin{proof}\n% We expand $\\mathcal{L}_{mf}(F)$ and obtain\n\n% \\begin{equation}\n% \\label{eq:all}\n% \\begin{aligned}\n%     \\mathcal{L}_{mf}(F)\n%     &=((1-\\alpha)I+\\Sigma_{i,j}(\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}-f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})))^{2}   \\\\\n%     &=const - 2\\Sigma_{i,j}[(1-\\alpha)I+\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}]f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})\\\\\n%     &\\ \\ \\ \\ +\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2} \\\\\n%     &= \n%     \\left\\{  \n% \\begin{array}{l}\n%     const - 2\\Sigma_{i,j}1-\\alpha+\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})\\\\\n%     \\ \\ +\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2}, i=j \\\\\n%     const - 2\\Sigma_{i,j}\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})\\\\\n%     \\ \\ +\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2}, i\\ne j \n% \\end{array}\n% \\right.  \n% \\end{aligned}\n% \\end{equation}\n% In our case two views $x_{i}$ and $x_{j}$ are not the same. We thus only focus on the $i\\ne j$ case. Ignoring the scaling factor which doesn\u2019t affect linear probe error, we can hence rewrite\n% the sum of last two terms of in Equation~\\ref{eq:all} as Equation~\\ref{eq:hfc}.\n% \\end{proof}\n\n\n\n\n% We rewrite the loss function as follows:\n% \\begin{equation}\n% \\begin{aligned}\n%     \\min_{F}\\mathcal{L}(F) \n    \n%     &=((1-\\alpha)I+\\Sigma_{i,j}(\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}-f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})))^{2} \\\\\n    \n%     &=const - 2\\Sigma_{i,j}[(1-\\alpha)I+\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}]f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})+\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2} \\\\\n%     &= \n%     \\left\\{  \n% \\begin{array}{l}\n%     const - 2\\Sigma_{i,j}1-\\alpha+\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})+\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2}, i=j \\\\\n    \n%     const - 2\\Sigma_{i,j}\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})+\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2}, i\\ne j\n% \\end{array}\n% \\right.  \n% \\end{aligned}\n    \n% \\end{equation}\n\n\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Methodology",
                "content": "\n\\label{sec:method}\n",
                "subsection 3.1": {
                    "name": "Overview",
                    "content": "\n\n\n\nExisting studies~\\cite{jin2021bite,li2021adsgnn,zhang2019shne,zhu2021textgnn} mainly emphasize the effect of sequential and graphic characteristics using the supervised optimization objective alone.\nInspired by recent progress with contrastive learning~\\cite{chen2020simple,he2020momentum}, we take a different perspective to characterize the data correlations by contrasting different views of the raw data.\n\nThe basic idea of our approach is to incorporate several elaborately designed self-supervised learning objectives for enhancing the original GNN and PLM. To develop such objectives, we leverage effective correlation signals reflected in the intrinsic characteristics of the input. As shown in Figure~\\ref{fig:main}, for our task, we consider the information in different\nlevels of granularity, including token, node and sub-graph, which are considered as different views of\nthe input. By capturing the multi-view correlation, we unify these self-supervised learning objectives with the typical joint learning training scheme in language modeling and graph mining~\\citep{yang2021graphformers}.\n\n% The overview of our proposed method is presented in . We take the unidirectional-simplified GraphFormers~\\citep{yang2021graphformers} trained with the two-stage progressive learning as our base model. In the following sections, we will\n% describe how we utilize the correlation signals among tokens, nodes and sub-graphs to enhance the data representations based on our proposed HFC method. Finally, we present the discussions on our approach.\n\n"
                },
                "subsection 3.2": {
                    "name": "Hierarchical Contrastive Learning with TAGs",
                    "content": "\nTAGs naturally possess 3 levels in the hierarchy: token-level, node-level and subgraph-level.\nBased on the above GNN and PLM model, we further incorporate additional self-supervised signals with contrastive learning to enhance the representations of input data. We adopt a joint-training way to construct\ndifferent loss functions based on the multi-view correlation.\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Intra-hierarchy contrastive learning",
                        "content": "\\ \n\n\n\\noindent \\textbf{Modeling Token-level Correlations.}  We first begin with modeling the bidirectional information in the token sequence. Inspired by the masked language model like BERT~\\citep{devlin2018bert}, we propose to use the contrastive learning framework to design a task that maximizes the mutual information between the masked sequence representation and its contextual representation vector. Specifically, for a node $v$, given its textual attribute sequence $x_{v} = \\{x_{v,1}, x_{v,2},..., x_{v,T} \\}$, we consider $x_{v,i:j}$ and $\\hat{x}_{v,i:j}$ as a positive pair, where $x_{v,i:j}$ is an \\textit{n}-grams spanning from i to j and $\\hat{x}_{v,i:j}$ is the corresponding sequence masked at position i to j. We may omit the subscript $v$ for notation simplification when it is not important to differentiate the affiliation between node and textual sequence.\n\n% The gist of token-token contrastive learning is to embed similar tokens nearby in the latent space while embedding those dissimilar ones far apart. However, the definition of dissimilar (\\textit{i.e.,} negative) token pairs is non-trivial. Previous methods usually derive negative samples by sampling uniformly over the dataset~\\cite{kong2019mutual}. However, they cannot guarantee that the produced negative samples own exactly distinct semantics relative to the query sample. Such a defect hampers token-token contrastive learning, in which those semantically relevant positive candidates could be wrongly expelled from the query sample in the latent space, and the semantic structure is thus broken to some extent. To overcome this drawback, we aim to select more precise negative samples that own truly irrelevant semantics.\n\nFor a specific query \\textit{n}-gram $x_{i:j}$, instead of contrasting it indiscriminately with all negative candidates $\\mathcal{N}$ in a batch~\\cite{zhao2023beyond,wang2022adaptive,li2019adversarial,kong2019mutual}, we select truly negative samples for contrasting based on the supervision signals provided by the hierarchical structure in TAGs, as shown in Figure~\\ref{fig:tt}. Intuitively, we would like to eliminate those candidates sharing highly similar semantics with the query, while keeping the ones that are less semantically relevant to the query. To achieve this goal, we first define a similarity measure between an \\textit{n}-gram and a node. Inspired by~\\cite{li2020prototypical}, for a node $v$, we define the semantic similarity between \\textit{n}-gram's hidden state $h_{x_{i:j}}$ and this node's hidden state $h_{v}$ using a node-specific dot product:\n\\begin{equation}\n    s(h_{x_{i:j}},h_{v}) = \\frac{h_{x_{i:j}}\\cdot h_{v}}{\\tau_{h_{v}}},\n    \\tau_{h_{v}} = \\frac{\\Sigma_{h_{x_{i}}\\in H_{v}}||h_{x_{i}}-h_{v}||_{2}}{|H_{v}|log(|H_{v}|+\\epsilon)}\\nonumber,\n\\end{equation}\nwhere $h_{x_{i}}$ is the hidden representation of the token $x_{i}$, $H_{v}$ consists of the hidden representations of the tokens assigned to node $v$, and $\\epsilon$ is a smooth parameter balancing the scale of temperature $\\tau_{h_{v}}$ among different nodes.\n\n% *****************************************\n\n% ******************************************\n\nOn such a basis, we conduct negative sampling selection considering both the token and node hierarchies. Given the query \\textit{n}-gram $x_{i:j}$, we denote its corresponding node $v$'s representation as $h_{v}$. For a negative candidate, we are more likely to select it if its similarity with $h_{v}$ is less prominent compared with other negative candidates' similarities with $h_{v}$. Based on such an intuition, the least dissimilar negative samples $\\mathcal{N}_{select}(h_{x_{i:j}})$ are produced for the specific query.\n\nBy using these refined negative samples, we define the objective function of token-level contrastive (TC) loss as below:\n\\begin{equation}\n    % \\mathcal{L}_{TC}=E_{p(x_{i:j},\\hat{x}_{i:j})}[L_{contrast}(g_{\\omega}(x_{i:j}),g_{\\omega}(\\hat{x}_{i:j}),N_{select}(z),\\tau)],\n    \\mathcal{L}_{TC}=\\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(x_{m,i:j},\\hat{x}_{m,i:j},\\mathcal{N}_{select}(h_{x_{m,i:j}})),\n\\end{equation}\nwhere $M$ is the size of the representation set and $\\mathcal{L}_{HFC}$ is our proposed HFC-aware spectral contrastive loss.\n% where $g_{\\omega}$ is our base PLM.\n\n\n% Therefore, we replace the spectral contrastive loss with our proposed HFC-aware contrastive loss to incorporate more HFC for learning discriminative representations. The objective function of token-level contrastive (TC) loss becomes:\n% \\begin{equation}\n%     % \\mathcal{L}_{TC}=E_{p(x_{i:j},\\hat{x}_{i:j})}[L_{contrast}(g_{\\omega}(x_{i:j}),g_{\\omega}(\\hat{x}_{i:j}),N_{select}(z),\\tau)],\n%     \\mathcal{L}_{TC}=\\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(x_{m,i:j},\\hat{x}_{m,i:j},N_{select}(z)),\n% \\end{equation}\n% where $M$ is the size of the representation set and $\\mathcal{L}_{HFC}$ is the HFC-aware contrastive loss. Note that in the follow-up contrastive objective design, we use the proposed HFC-aware contrastive loss as the underlying contrastive loss.\n\n% ****************************************\n\n% *****************************************\n\n\\noindent \\textbf{Modeling Node-level Correlations.}\nInvestigating the cross-view contrastive mechanism is especially important for node representation learning~\\citep{wang2021self}. As mentioned before, nodes in TAGs possess textual attributes that can indicate semantic relationships in the network and serve as complementary to structural patterns. As shown in Figure~\\ref{fig:nn}, given a node $v$, we treat its textual attribute sequence $x_{v}$ and its direct connected neighbors $u, \\textnormal{for}~u\\in N_{v}$ as two different views.\n\nThe negative selective encoding strategy in token-level correlation modeling tends to select less challenging negative samples, reducing their contribution over time. Inspired by~\\citep{xia2022progcl}, we adopt the ProGCL~\\citep{xia2022progcl} method to adversarially reweight and generate harder negative samples $\\widetilde{v}$ using the mixup operation~\\citep{zhang2017mixup}. Therefore, we minimize the following Node-level Contrastive (NC) loss:\n\n\n% The negative selective encoding strategy used in token-level correlation modeling may select those easy negative samples that contribute less and less during the training process. \n% Inspired by~\\citep{xia2022progcl}, we propose to adversarially generate the negative samples $\\widetilde{v}$ in the node-level contrastive learning process. Specifically, we adopt ProGCL~\\citep{xia2022progcl} method to reweight the negative node samples and performing mixup operation~\\citep{zhang2017mixup} to generate hard negative samples $\\widetilde{v}$. Therefore, we minimize the following Node-level Contrastive (NC) loss:\n\\begin{equation}\n    \\mathcal{L}_{NC}=\\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(x_{m,v},N_{m,v},\\widetilde{v_{m}})\n\\end{equation}\n\n\\noindent \\textbf{Modeling Subgraph-level Correlations.}\nHaving analyzed the correlations between a node\u2019s local neighborhood and its textual attributes, we extend our investigation to encompass the correlations among subgraphs. This approach facilitates the representation of both local and higher-order structures associated with the nodes. It is reasonable that nodes are more strongly correlated with their immediate neighborhoods than with distant nodes, which exert minimal influence on them. Consequently, local communities are likely to emerge within the graph. Therefore, to facilitate our analysis, we select a series of subgraphs that include regional neighborhoods from the original graph to serve as our training data.\n\n% Having modeled correlations between a node's local neighborhood and its textual features, we further consider modeling the correlations between subgraphs to cover both of the local and high-order structures of the nodes. Intuitively, nodes and their regional neighborhoods\n% are more correlated while long-distance nodes hardly influence them. Therefore, local communities may form with the graph. This assumption is more reasonable as the size of graphs increases. Therefore, we sample a series of subgraphs including regional neighborhoods from the original graph as training data.\n\nThe paramount challenge currently lies in sampling a context subgraph that can furnish adequate structural information essential for the derivation of a high-quality representation of the central node. In this context, we adopt the subgraph sampling methodology based on the personalized PageRank algorithm (PPR)~\\citep{jeh2003scaling} as introduced in~\\citep{zhang2020graph,jiao2020sub}. Given the variability in the significance of different neighbors, for a specific node $i$, the subgraph sampler $S$ initially computes the importance scores of neighboring nodes utilizing PPR. Considering the relational data among all nodes represented by an adjacency matrix $A\\in \\mathbb{R}^{N\\times N}$, the resulting matrix $S$ of importance scores is designated as \n\n% The most critical issue now is to sample a context subgraph, which can provide sufficient structure information for learning a high-quality representation for the central node. Here we follow the subgraph sampling based on personalized PageRank  algorithm (PPR)~\\citep{jeh2003scaling} as introduced in~\\citep{zhang2020graph,jiao2020sub}. Considering the\n% importance of different neighbors varies, for a specific node $i$, the subgraph sampler $S$ first measures the importance scores of other neighbor nodes by PPR. Given the relational information between all nodes in the form of an adjacency matrix, $A\\in \\mathbb{R}^{N\\times N}$, the importance score\n% matrix $S$ can be denoted as\n\\begin{equation}\n    S=\\alpha\\cdot(I-(1-\\alpha)\\cdot\\overline{A})\\nonumber,\n\\end{equation}\nwhere $I$ represents the identity matrix and $\\alpha$ is a parameter within the range $[0,1]$. The term $\\overline{A}=AD^{-1}$ is the column-normalized adjacency matrix, where $D$ is the corresponding diagonal matrix with entries $D(i,i)=\\Sigma_{j}A(i,j)$ along its diagonal. The vector $S(i,:)$ enumerates the importance scores for node $i$.\n\n% where $I$ is the identity matrix and $\\alpha\\in [0,1]$ is a parameter that is always set as 0.15. $\\overline{A}=AD^{-1}$ denotes the colum-normalized adjacency matrix, where $D$ denotes the corresponding diagonal matrix with $D(i,i)=\\Sigma_{j}A(i,j)$ on its diagonal. $S(i,:)$ is the importance scores vector for node $i$, indicating its correlation with other nodes.\n\n% It is noted that the importance score matrix S can be\n% precomputed before model training starts. And we implement node-wise PPR to calculate importance scores to reduce computation memory, which makes our method more suitable to work on large-scale graphs.\n\nFor a specific node $i$, the subgraph sampler $S$ selects the top-k most significant neighbors to form the subgraph $G_{i}$. The indices of the selected nodes are\n% For a specific node $i$, the subgraph sampler $S$ chooses top-k important neighbors to constitute the subgraph $G_{i}$. The index of chosen nodes can be denoted as\n\\begin{equation}\n    idx = top\\_rank(S(i,:), k)\\nonumber,\n\\end{equation}\nwhere $top\\_rank$ is the function that returns the indices corresponding to the top-k values, where k specifies the size of the context graphs.\n\nThe subgraph sampler $S$ processes the original graph along with the node index to derive the context subgraph $G_{i}$ for node $i$. The adjacency matrix $A_{i}$ and feature matrix $X_{i}$ of this subgraph are defined as follows:\n\\begin{equation}\n    A_{i}=A_{idx,idx,} X_{i}=X_{idx,:,}\\nonumber\n\\end{equation}\nwhere $.idx$ denotes an indexing operation. $A_{idx,idx}$ refers to the adjacency matrix, row-wise and column-wise indexed to correspond to the induced subgraph.  $X_{idx,:}$ is the feature matrix indexed row-wise. \n\n\\noindent \\textbf{Encoding subgraph.}\nUpon acquiring the context subgraph $G_{i}=(A_{i},X_{i})$ of a central node $i$, the encoder $\\mathcal{E}:\\mathbb{R}^{N\\times N}\\times \\mathbb{R}^{N\\times F}\\rightarrow \\mathbb{R}^{N\\times F}$ encodes it to derive the latent representations matrix $H_{i}$, which is denoted as\n\\begin{equation}\n    H_{i} = \\mathcal{E}(A_{i},X_{i})\\nonumber\n\\end{equation}\n\nThe subgraph-level representation $s_{i}$ is summarized using a readout function, $\\mathcal{R}:\\mathbb{R}^{N\\times F}\\rightarrow\\mathbb{R}^{F}$:\n\\begin{equation}\n    s_{i}=\\mathcal{R}(H_{i})\\nonumber\n\\end{equation}.\n\n% ******************************************\n\n% ******************************************\n\nSo far, the representations of subgraphs have been produced. As shown in Figure~\\ref{fig:ss}, to model the correlations in subgraph level, we treat two subgraphs $s_{i}$ and $\\hat{s}_{i}$ that sampled from the node $h_{i}$ and its most important neighbor node $\\hat{h}_{i}$ respectively as positive pairs while the rest of subgraphs $\\widetilde{s}$ are negative pairs. We minimize the following Subgraph-level Contrastive (SC) loss:\n\\begin{equation}\n    \\mathcal{L}_{SC} = \\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(s_{m}, \\hat{s}_{m},\\widetilde{s_{m}})\n\\end{equation}\n\n% ***********************************\n% \\begin{figure}[t]\n%     \\centering\n%     \\includegraphics[width=.8\\linewidth]{figures/Token-Node.png}\n%     \\caption{Modeling token-node correlations.}\n%     \\label{fig:tn}\n%     % \\vspace{-0.3cm}\n% \\end{figure}\n% **************************************\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Inter-hierarchy contrastive learning",
                        "content": "\\\n\n\n\n\\noindent Having modeled the intra-hierarchy correlations, we further consider modeling the intra-hierarchy correlations as different hierarchies are dependent and will influence each other. \n\n\n\\noindent \\textbf{Modeling Token-Node Correlations.}\nTo model the token-node correlation, our intuition is to train the language model to refine the understanding of the text by GNN produced embeddings. Therefore, the language model is pushed to learn fine-grained task-aware context information. Specifically, \n% as shown in Figure~\\ref{fig:tn}, \ngiven a sequence $x_{v} = \\{x_{v,1}, x_{v,2},..., x_{v,T} \\}$, we consider $x_{v}$ and its corresponding node representation $h_{v}$ as a positive pair. On the other hand, for a set of node representations, we employ a function, $\\mathcal{P}$, to corrupt them to generate negative samples, denoted as\n\\begin{equation}\n    \\{\\widetilde{h_{1}}, \\widetilde{h_{2}},...,\\widetilde{h_{M}}\\} = \\mathcal{P}\\{h_{1}, h_{2},...,h_{M}\\}\\nonumber,\n\\end{equation}\nwhere $M$ is the size of the representation set. $\\mathcal{P}$ is the random shuffle function in our experiment. This corruption strategy determines the differentiation of tokens with different context nodes, which is crucial for some downstream tasks, such as node classification.\nWe develop the following Token-Node Contrastive (TNC) loss:\n\\begin{equation}\n    % \\mathcal{L}_{TNC} = \\frac{1}{M}\\Sigma_{k=1}^{M}\\mathbb{E}_{}[\\mathcal{L}_{HFC}(x_{i:j},h_{k},\\widetilde{h_{k}})]\n    \\mathcal{L}_{TNC} = \\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(x_{m,v},h_{m,v},\\mathcal{P}\\{h_{1}, h_{2},...,h_{M}\\})\n\\end{equation}\n\n% % ****************************************\n% \\begin{figure}[h]\n%     \\centering\n%     \\includegraphics[width=.8\\linewidth]{figures/Node-Sub.png}\n%     \\caption{Modeling node-subgraph correlations.}\n%     \\label{fig:ns}\n%     \\vspace{-0.3cm}\n% \\end{figure}\n% % ****************************************\n\n\\noindent \\textbf{Modeling Node-Subgraph Correlations.} Intuitively, nodes are dependent on their regional neighborhoods and different nodes have different context subgraphs. Therefore, we consider the strong correlation between central nodes and their context subgraphs to design a self-supervision pretext task to contrast the real context subgraph with a fake one. Specifically, \n% as shown in Figure~\\ref{fig:ns}, \nfor the node representation, $h_{v}$, that captures the regional information in the context subgraph, we regard the context subgraph representation $s_{v}$ as the positive sample. Similar to the calculation of $\\mathcal{L}_{TNC}$, we employ the random shuffle function $\\mathcal{P}$ to corrupt other subgraph representations to generate negative samples, denoted as\n\\begin{equation}\n    \\{\\widetilde{s_{1}}, \\widetilde{s_{2}},...,\\widetilde{s_{M}}\\} = \\mathcal{P}\\{s_{1}, s_{2},...,s_{M}\\}\\nonumber\n\\end{equation}\nWe minimize the following Node-Subgraph Contrastive (NSC) loss:\n\\begin{equation}\n    \\mathcal{L}_{NSC} =\\frac{1}{M}\\Sigma_{m=1}^{M} \\mathcal{L}_{HFC}(h_{m,v},s_{m,v},\\mathcal{P}\\{s_{1}, s_{2},...,s_{M}\\})\n\\end{equation}\n\n\\noindent \\textbf{Overall Objective Loss.} Our overall objective function is a weighted combination of the five terms above:\n\\begin{equation}\n\\begin{aligned}\n    \\mathcal{L}_{HASH-CODE}&=\\lambda_{TC}\\mathcal{L}_{TC}+\\lambda_{NC}\\mathcal{L}_{NC}+\\lambda_{SC}\\mathcal{L}_{SC}\\\\\n    &+\\lambda_{TNC}\\mathcal{L}_{TNC}+\\lambda_{NSC}\\mathcal{L}_{NSC},\n\\end{aligned}\n\\end{equation}\nwhere $\\lambda_{TC},\\lambda_{NC},\\lambda_{SC},\\lambda_{TNC}$ and $\\lambda_{NSC}$ are hyper-parameters that balance the contribution of each term. \n% We summarize the workflow of our proposed HASH-CODE in Appendix~\\ref{sec:workflow}.\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:exp}\n",
                "subsection 4.1": {
                    "name": "Experimental Setup",
                    "content": "\n% In this section, we have conducted extensive experiments, and analyzed the performance of the proposed HASH-CODE method.\n% by addressing the following key research questions as follows:\n% \\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]\n%     \\item \\textbf{RQ1:} How does our method perform compared with baseline methods?\n%     \\item \\textbf{RQ2:} How does each component of our method contribute to the performance?\n%     \\item \\textbf{RQ3:} How about the efficiency of our proposed model compared with other baselines?\n%     \\item \\textbf{RQ4:} How does our method perform when facing the issue of data sparsity?\n%     \\item \\textbf{RQ5:} How do different hyper-parameters affect our method?\n% \\end{itemize}\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Datasets",
                        "content": "\nWe conduct experiments on six datasets (\\textit{i.e.,} DBLP\\footnote{\\noindent  \\url{https://originalstatic.aminer.cn/misc/dblp.v12.7z}}, Wikidata5M\\footnote{\\noindent   \\url{https://deepgraphlearning.github.io/project/wikidata5m}}~\\citep{wang2021kepler}, Beauty, Sports and Toys from Amazon dataset\\footnote{\\noindent  \\url{ http://snap.stanford.edu/data/amazon/}}~\\citep{mcauley2015image} and Product Graph) from three different domains (i.e., academic papers, social media posts, and e-commerce). We\nleverage three common metrics to measure the prediction accuracy: Precision@1 (P@1), NDCG, and MRR. \n% Detailed information about the datasets can be found in Appendix~\\ref{sec:data}. \nThe statistics of the six datasets are summarized in Table~\\ref{tab:dataset}.\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Baselines",
                        "content": "\nWe compare HASH-CODE with three types of baselines: (1) GNN-cascaded transformers, which includes BERT+Ma-xSAGE~\\citep{hamilton2017inductive}, BERT+MeanSAGE~\\citep{hamilton2017inductive},  BERT+GAT~\\citep{velivckovic2017graph}, TextGNN~\\citep{zhu2021textgnn}, and AdsGNN~\\citep{li2021adsgnn}. (2) GNN-nested transformers, which includes GraphFormers~\\citep{yang2021graphformers}, and Heterformer~\\citep{jin2022heterformer}. (3)\nVanilla GraphSAGE~\\citep{hamilton2017inductive}, Vanilla GAT~\\citep{velivckovic2017graph}, Vanilla BERT~\\citep{devlin2018bert} and Twin-Bert~\\citep{lu2020twinbert}. \n% Detailed information about the baselines can be found in Appendix~\\ref{sec:baseline}.\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Reproducibility.",
                        "content": " \nFor all compared models, we adopt the 12-layer BERT-base-uncased~\\citep{devlin2018bert} in the\nhuggingface as the backbone PLM for a fair comparison. The models are\ntrained for at most 100 epochs on all datasets. We use an early\nstopping strategy on P@1 with a patience of 2 epochs. The size of minimal training batch is\n64, learning rate is set to $1e-5$. We pad the sequence length to 32 for Product, DBLP and Amazon datasets, 64 for Wiki, depending on different text length of each dataset. Adam optimizer~\\citep{kingma2014adam} is employed to minimize the training loss. Other parameters are tuned on the validation dataset and we save the checkpoint with the best validation performance as the final model. Parameters in baselines are carefully tuned on the validation set to select the most desirable parameter setting. \n\n\n\n% In this section, we have conducted extensive experiments, and analyzed the performance of the proposed HASH-CODE method by addressing the\n% following key research questions as follows:\n% \\begin{itemize}\n%     \\item \\textbf{RQ1:} Can our proposed HASH-CODE outperform the state-of-the-art baselines in downstream tasks, \\textit{e.g.,} link prediction, node classification, \\textit{etc.} ?\n%     \\item \\textbf{RQ2:} How do different components (\\textit{e.g.}, different self-supervised optimization objectives, HFC-aware loss, \\textit{etc.}) affect the performance of HASH-CODE?\n%     \\item \\textbf{RQ3:} How is the scalability of HASH-CODE?\n%     \\item \\textbf{RQ4:} How do different parameter settings (\\textit{e.g.}, different neighbor sizes, \\textit{etc.}) affect the performance of HASH-CODE?\n% \\end{itemize}\n% \\subsection{Datasets and Preprocessing}\n% We evaluate HASH-CODE and the baselines on the following three real-world datasets:\n% \\begin{itemize}\n%     \\item \\textbf{DBLP}\\footnote{\\noindent  \\url{https://originalstatic.aminer.cn/misc/dblp.v12.7z}}, which contains the paper citation graph from DBLP up to 2020-04-09. Two papers are linked if one is cited by the other one. The paper\u2019s title is used as the textual feature.\n%     \\item \\textbf{Wikidata5M}\\footnote{\\noindent   \\url{https://deepgraphlearning.github.io/project/wikidata5m}} (Wiki)~\\citep{wang2021kepler}, which contains the entity graph from Wikipedia. The first sentence in each entity\u2019s introduction is taken as its textual feature.\n%     \\item \\textbf{Product Graph} (Product), an even larger dataset of online products collected by a world-wide search engine. In this dataset, the users\u2019 web browsing behaviors are tracked for the targeted product webpages (e.g., Amazon webpages of Nike shoes). The user\u2019s continuously browsed webpages within a short period of time (e.g., 30 minutes) is called a \u201csession\u201d. The products within a common session are connected in the graph (which is a common way of graph construction in e-commerce scenarios~\\citep{ying2018graph,wang2018billion}). Each product has its unique textual description, which specifies information like the product name, brand, and saler, etc.\n% \\end{itemize}\n\n% The textual features of all the datasets are in English. We make use of uncased WordPiece~\\citep{wu2016google} to tokenize the input text. We summarize the specifications of all the datasets with Table~\\ref{tab:dataset}. \n% Without specifications, we will take the unidirectional-simplified GraphFormers~\\citep{yang2021graphformers} trained with the two-stage progressive learning as our base model.\n\n% \\begin{table}[t]\n%     \\centering\n%     \\caption{Specifications of the experimental datasets: the number of items, the number of neighbour\n% nodes on average, and the number of training, validation, testing cases.}\n%     {\n%     \\begin{tabular}{cccc}\n%         \\toprule\n%                 & Product & DBLP & Wiki \\\\\n%         \\midrule\n%         \\#Item & 5,643,688 & 4,894,081 & 4,818,679  \\\\\n%         \\#N   & 4.71 & 9.31 & 8.86  \\\\\n%         \\#Train  & 22,146,934 & 3,009,506 & 7,145,834  \\\\\n%         \\#Valid  & 30,000 &  60,000 & 66,167  \\\\\n%         \\#Test   & 306,742 & 100,000  & 100,000  \\\\\n%         \\bottomrule\n%     \\end{tabular}}\n%     \\label{tab:dataset}\n%     \\vspace{-0.4cm}\n% \\end{table}\n\n% \\subsection{Baseline Models} \n% We compare HASH-CODE with two groups of baselines: GNN-cascaded transformers and GNN-nested transformers. The former group includes BERT+MeanSAGE~\\citep{hamilton2017inductive}, BERT+MaxSAGE~\\citep{hamilton2017inductive} and BERT+GAT~\\citep{velivckovic2017graph}. The latter one includes GraphFormers~\\citep{yang2021graphformers} model. To verify the importance of both text and network information, we also include vanilla GraphSAGE~\\citep{hamilton2017inductive} and vanilla BERT~\\citep{devlin2018bert} in comparison.\n\n% We consider 7 baselines including vanilla text/graph encoding models, GNN-cascaded transformers, and GNN-nested transformers to evaluate the performance of the proposed model. \n\n% \\textbf{Vanilla text/graph models:}\n% \\begin{itemize}\n%     \\item \\textbf{MeanSAGE}~\\citep{hamilton2017inductive}: This is a GNN method utilizing the mean function to aggregate information from neighbors for center node embedding learning. The initial node feature vector is bag-of-words weighted by TF-IDF.\n%     \\item \\textbf{BERT}~\\citep{devlin2018bert}: This is a benchmark PLM pretrained on two tasks: next sentence prediction and mask token prediction. For each text-rich node, we use BERT to encode its text and take the output of the [CLS] token as node representation.\n% \\end{itemize}\n\n% \\textbf{GNN-cascaded transformers:}\n% \\begin{itemize}\n%     \\item \\textbf{Bert+GAT}~\\citep{velivckovic2017graph}: GAT is a GNN method with attention-based neighbor importance calculation, and the weight of each neighbor during aggregation is based on its importance score.\n%     \\item \\textbf{Bert+MeanSAGE}~\\citep{hamilton2017inductive}: We stack BERT with MeanSAGE (i.e., using the output text representation of BERT as the input node attribute vector of MeanSAGE). The BERT+MeanSAGE model is trained in an end-to-end way. Other BERT+GNN baselines below have the same cascaded architecture.\n%     \\item \\textbf{Bert+MaxSAGE}~\\citep{hamilton2017inductive}: MaxSAGE is a GNN method utilizing the max function during neighbor aggregation for center node representation learning.\n% \\end{itemize}\n\n% \\textbf{GNN-nested transformers:}\n% \\begin{itemize}\n%     \\item \\textbf{GraphFormers}~\\citep{yang2021graphformers}: This is the state-of-the-art GNN-nested transformer model, which has graph-based propagation and aggregation in each transformer layer.\n% \\end{itemize}\n\n\n\n\n% \\begin{table*}[h]\n% \\Large\n%   \\caption{Experiment results of link prediction. (HASH-CODE marked in bold, the best baseline underlined). HASH-CODE outperforms all baselines, especially the ones based on GNN-nested transformers.}\n%   \\label{tab:main}\n%   {\n%   \\begin{tabular}{c|ccc|ccc|ccc}\n%     \\toprule\n%     \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{Product} & \\multicolumn{3}{c}{DBLP} & \\multicolumn{3}{c}{Wiki} \\\\\n%                ~&P@1 & NDCG & MRR &P@1 & NDCG & MRR &P@1 & NDCG & MRR \\\\\n%     \\midrule\n    \n%     MeanSAGE   & $0.6071$ & $0.7384$ & $0.6619$ & $0.4963$ & $0.6997$ & $0.6314$ & $0.2850$ & $0.5389$ &  $0.4411$  \\\\\n%     GAT   & $0.6135$ & $0.7384$ & $0.6619$ & $0.4963$ & $0.6997$ & $0.6314$ & $0.2850$ & $0.5389$ &  $0.4411$  \\\\\n%     Bert & $0.6563$ & $0.7911$ & $0.7344$ & $0.5673$ & $0.7484$ & $0.6777$ & $0.3466$ & $0.5799$ & $0.4712$  \\\\\n%     Twin-Bert & $0.6563$ & $0.7911$ & $0.7344$ & $0.5673$ & $0.7484$ & $0.6777$ & $0.3466$ & $0.5799$ & $0.4712$  \\\\\n%     \\midrule\n%     Bert+MeanSAGE & $0.7550$ & $0.8671$ & $0.8271$ & $0.6896$ & $0.8359$ & $0.7866$ & $0.3664$ & $0.6037$ &  $0.4980$  \\\\\n%     Bert+MaxSAGE & $0.7570$ & $0.8678$ & $0.8280$ & $0.6934$ & $0.8386$ & $0.7900$ & $0.3712$ & $0.6071$ & $0.5022$  \\\\\n%     Bert+GAT & $0.7540$ & $0.8637$ & $0.8232$ & $0.6633$ & $0.8204$ & $0.7667$ & $0.3006$ & $0.5430$  & $0.4270$   \\\\\n%     \\midrule\n%     TextGNN & $\\underline{0.7786}$ & $\\underline{0.8793}$ & $\\underline{0.8430}$ & $\\underline{0.7267}$ & $\\underline{0.8565}$ & $\\underline{0.8133}$ & $\\underline{0.3952}$ & $\\underline{0.6230}$ & $\\underline{0.5220}$  \\\\\n%     AdsGNN & $\\underline{0.7786}$ & $\\underline{0.8793}$ & $\\underline{0.8430}$ & $\\underline{0.7267}$ & $\\underline{0.8565}$ & $\\underline{0.8133}$ & $\\underline{0.3952}$ & $\\underline{0.6230}$ & $\\underline{0.5220}$  \\\\\n%     GraphFormers & $\\underline{0.7786}$ & $\\underline{0.8793}$ & $\\underline{0.8430}$ & $\\underline{0.7267}$ & $\\underline{0.8565}$ & $\\underline{0.8133}$ & $\\underline{0.3952}$ & $\\underline{0.6230}$ & $\\underline{0.5220}$  \\\\\n%     Heterformer & $\\underline{0.7786}$ & $\\underline{0.8793}$ & $\\underline{0.8430}$ & $\\underline{0.7267}$ & $\\underline{0.8565}$ & $\\underline{0.8133}$ & $\\underline{0.3952}$ & $\\underline{0.6230}$ & $\\underline{0.5220}$  \\\\\n%     \\midrule\n%     HASH-CODE & $\\textbf{0.7967}$ & $\\textbf{0.9039}$ & $\\textbf{0.8706}$ &  $\\textbf{0.7446}$  & $\\textbf{0.8823}$ & $\\textbf{0.8428}$ & $\\textbf{0.4104}$ & $\\textbf{0.6402}$ & $\\textbf{0.5356}$  \\\\\n%     \\midrule\n%     \\textit{Improv.} & $2.32\\%$ & $2.80\\%$ & $3.27\\%$ &  $2.67\\%$  & $3.01\\%$ & $3.63\\%$ & $3.84\\%$ & $2.76\\%$ & $2.61\\%$  \\\\\n%     \\bottomrule\n%   \\end{tabular}\n%   }\n%   % \\vspace{-0.3cm}\n% \\end{table*}\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Overall Comparison",
                    "content": "\nFollowing previous studies~\\citep{yang2021graphformers,jin2022heterformer} on network representation learning,\nwe consider two fundamental task:\nlink prediction and node classification. \nTo save space, we will mainly present the results on link prediction here and save the node classification part to Appendix~\\ref{sec:node_classification}. The overall evaluation results are reported in Table~\\ref{tab:main}. We have the following observations: \n\n\n\n% \\noindent \\textbf{Settings.} \n% The link prediction experiments are evaluated in terms of link prediction accuracy, i.e., to predict whether a query node and key node are connected given the textual features of themselves and their neighbours. For Product, DBLP and Wiki datasets, in each testing instance, one query is provided with 300 keys: 1 positive plus 299 randomly sampled negative cases.\n\n% We leverage three common metrics to measure the prediction accuracy: Precision@1, MRR, and NDCG. Given a query node $u$, Precision@1 measures whether the key node $v$ linked with $u$ is ranked the highest in the batch; MRR calculates the average of the reciprocal ranks of $v$; NDCG further takes the order and relative importance of $v$ into account and here we calculate on the full candidate list, the length of which equals to test batch size.\n\n% \\noindent \\textbf{Results.} \n\n\nIn comparing vanilla textual and graph models across various datasets, we find a consistent performance ranking: BERT outperforms Twin-BERT, which in turn exceeds GAT and GraphSAGE. This hierarchy reveals GNN models' limitations in capturing rich textual semantics due to their focus on node proximity and global structural information. Specifically, the superior performance of the one-tower BERT model over the two-tower Twin-BERT model underscores the advantage of integrating information from both sides, despite BERT's potential inefficiency in low-latency scenarios due to one-by-one similarity computations.\n\n% For four vanilla textual/graph baselines, the performance order is consistent across all datasets, \\textit{i.e.,} $\\textnormal{Bert} > \\textnormal{Twin-Bert} > \\textnormal{GAT} \\approx \\textnormal{GraphSAGE}$. GNN models obtain the worst performance, as they can only model the node proximity that preserved by the global structural information, but fail to encode the textual information that presents rich semantics to characterize the property of each node. This demonstrates the importance of leveraging the local textual information of individual nodes. As for the vanilla textual baselines, the one-tower textual model (BERT) outperforms the two-tower model (Twin-BERT) as it can incorporate the information from both sides, while two-tower models can only exploit the data from a single side. However, one-tower structure has to compute the similarity between a search query and each ad one-by-one, which is not suitable for low-latency online scenario. In general, vanilla textual/graph models  perform worse than GNN-cascaded transformers, which demonstrates the importance of encoding both text and network signals in text-attributed graphs.\n\nAs for GNN-cascaded transformers, BERT+GAT generally surpasses BERT+MeanSAGE and BERT+MaxSAGE in modeling attributes on the Product, DBLP, and Wiki datasets, attributed to its multi-head self-attention mechanism. However, its performance dips on the Beauty, Sports, and Toys datasets, likely due to noise from keyword-based attributes in Amazon Reviews. Despite these variations, GNN-cascaded transformers fall short of co-training methods, largely because of the static nature of node textual features during training. Among the models, AdsGNN consistently leads over TextGNN across all datasets. This highlights the effectiveness of AdsGNN's node-level aggregation model in capturing the nuanced roles of queries and keys, proving a tightly-coupled structure's superiority in integrating graph and textual data.\n\n\n% As for GNN-cascaded transformers, Bert+GAT performs better than Bert+MeanSAGE and Bert+MaxSAGE on Product, DBLP and Wiki datasets, because the multi-head self-attention mechanism has a stronger capacity to model attributes. However, the performance of GAT is worse than that of MeanSAGE on Beauty, Sports and Toys datasets. A potential reason is that the multi-head self-attention may incorporate more noise from the attributes since they are keywords extracted from the reviews on Amazon Reviews. In general, GNN-cascaded transformers perform worse than co-training-based methods, which may be due to the node textual features are pre-existed and fixed in the training phase, leading to the limited expression capacity. AdsGNN consistently outperforms TextGNN on all datasets. This is because compared with TextGNN, the node-level aggregation model AdsGNN can capture the different roles of queries and keys, demonstrating that the tightly-coupled structure is more powerful than the loosely-coupled framework in deeply fusing the graph and textual information.\n\nFor GNN-nested transformers, Heterformer outperforms Graphformers on denser networks like those of the Product and DBLP datasets compared to the Amazon datasets. Our HASH-CODE consistently outshines all baselines, achieving 2\\%$\\sim$4\\% relative improvements on six datasets against the most competitive ones.  These findings affirm the efficacy of contrastive learning in enhancing co-training architectures for representation learning tasks.\n\n% For GNN-nested transformers, Heterformer outperforms Graphformers on denser networks like those of the Product and DBLP datasets compared to the Amazon datasets. Our HASH-CODE performs consistently better than other baselines, achieving over 2\\%$\\sim$4\\% relative improvements over the most competitive baselines (underlined) on each of the experimental datasets. \n% These findings affirm the efficacy of contrastive learning in enhancing co-training architectures for representation learning tasks.\n\n% Different from these baselines, we adopt the contrastive learning to enhance the representations of the attribute, and nodes for the representation learning task, which incorporates five pre-training objectives to model multiple data correlations by our proposed HFC-aware contrastive objectives. This result also shows that the contrastive learning approach is effective to improve the performance of the co-training architecture for representation learning.\n\n\n\n\n% \\vspace{-0.3cm}\n\n% \\begin{table}[h]\n% % \\Large\n%   \\caption{Experiment results of transductive and inductive node classification on DBLP dataset. (HASH-CODE marked in bold, the best baseline underlined). HASH-CODE outperforms all baselines, especially the ones based on GNN-nested transformers.}\n%   \\label{tab:classification}\n%   {\n%   \\begin{tabular}{c|cc|cc}\n%     \\toprule\n%     \\multirow{2}{*}{Model} & \\multicolumn{2}{c}{Transductive} & \\multicolumn{2}{c}{Inductive}  \\\\\n%                ~&P@1 & NDCG  &P@1 & NDCG \\\\\n%     \\midrule\n%     MeanSAGE & $0.5186$ & $0.7231$ & $0.5152$ & $0.7197$  \\\\\n%     GAT & $0.5208$ & $0.7196$ & $0.5126$ & $0.7146$  \\\\\n%     Bert & $0.5493$ & $0.7506$ & $0.5310$ & $0.7485$  \\\\\n%     Twin-Bert & $0.5291$ & $0.7440$ & $0.5248$ & $0.7431$  \\\\\n%     \\midrule\n%     Bert+MeanSAGE & $0.6731$ & $0.7637$ & $0.6413$ & $0.7494$   \\\\\n%     Bert+MaxSAGE & $0.6705$ & $0.7752$ & $0.6587$ & $0.7599$   \\\\\n%     Bert+GAT & $0.6849$ & $0.0.7801$ & $0.6689$ & $0.0.7619$   \\\\\n%     \\midrule\n%     TextGNN & $0.6820$ & $0.7753$ & $0.6380$ & $0.7716$   \\\\\n%     AdsGNN & $0.6882$ & $0.7790$ & $0.6624$ & $0.7737$   \\\\\n%     GraphFormers & $0.6919$ & $0.7929$ & $\\underline{0.6791}$ & $0.7993$   \\\\\n%     Heterformer & $\\underline{0.6924}$ & $\\underline{0.7957}$ & $0.6746$ & $\\underline{0.8079}$   \\\\\n%     \\midrule\n%     HASH-CODE & $\\textbf{0.7116}$ & $\\textbf{0.8198}$ & $\\textbf{0.6961}$ &  $\\textbf{0.8170}$    \\\\\n%     \\midrule\n%     \\textit{Improv.} & $2.77\\%$ & $3.03\\%$ & $2.50\\%$ &  $1.13\\%$    \\\\\n%     \\bottomrule\n%   \\end{tabular}\n%   }\n%   % \\vspace{-0.3cm}\n% \\end{table}\n\n% \\subsection{Node Classification}\n% \\textbf{Settings.} In node classification, we train a 2-layer MLP classifier to classify nodes based on the output node representation embeddings of each method. The experiment is conducted on DBLP. We select the most frequent 30 classes in DBLP. Also, we study both transductive and inductive node classification to understand the capability of our model comprehensively. For transductive node classification, the model has seen the classified nodes during representation learning (using the link prediction objective), while for inductive node classification, the model needs to predict the label of nodes not seen before. We separate the whole dataset into train set, validation set, and test set in 7:1:2 in all cases and each experiment is repeated 5 times in this section with the average performance reported.\n\n\n\n% \\noindent \\textbf{Results.} Table~\\ref{tab:classification} demonstrates the results of different methods in transductive and inductive node classification. We observe that: (a) our HASH-CODE outperforms all the baseline methods significantly on both tasks, showing that HASH-CODE can learn more effective node representations for these tasks; (b) GNN-nested transformers generally achieve better results than GNN-cascaded transformers, which demonstrates the necessity of introducing graphic patterns in modeling textual representations; (c) HASH-CODE generalizes quite well on unseen nodes as its performance on inductive node classification is quite close to that on transductive node classification. Moreover, HASH-CODE even achieves higher performance in inductive settings than the baselines do in transductive settings.\n\n% \\vspace{-0.3cm}\n\n% \\vspace{-0.2cm}\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Study",
                    "content": "\nOur proposed HASH-CODE designs five pre-training objectives based on the HFC-aware contrastive objective. In this section, we conduct the ablation study\non Product and DBLP datasets to analyze the contribution of each objective. \nWe evaluate the performance of several HASH-CODE variants: (a) No-TT removes the $\\mathcal{L}_{TC}$; (b) No-TN removes the $\\mathcal{L}_{TNC}$; (c) No-NN removes the $\\mathcal{L}_{NC}$; (d) No-NS removes the $\\mathcal{L}_{NSC}$; (e) No-SS removes the $\\mathcal{L}_{SC}$; (f) No-HFC replaces the HFC-aware loss with spectral contrastive loss. The results from GraphFormers are also provided for comparison. P@1 and NDCG@10 are adopted for this evaluation.\n\nFrom Figure~\\ref{fig:ablation}, we can observe that removing any contrastive learning objective would lead to the performance decrease, indicating all the objectives are useful to capture the correlations in varying levels of granularity in TAGs. Besides, the importance of these objectives is varying on different datasets. Overall, $\\mathcal{L}_{TC}$ is more important than others. Removing it yields a larger drop of performance on all datasets, indicating that natural language understanding is more important on these datasets. In addition, No-HFC performs worse than the other variants, indicating the importance of learning more discriminative embeddings with HFC. \n\n% It is clearly seen that all model variants are better than GraphFormers, which is trained only with link predication loss.\n\n% We can find that: (a) HASH-CODE outperforms all model variants, which demonstrates that the correlation in varying levels of granularity in TAGs are essential for node encoding; (b) The performances between different simplified variants are comparable. Generally speaking, No-TT performs worse than the others, indicating that natural language understanding is more important on these datasets; (c) No-HFC performs worse than the other variants, indicating the importance of learning more discriminative embeddings with HFC. \n\n% \\subsubsection{HFC-aware Embedding Visualization.}\n% To intuitively study the impact of our HFC-loss, we visualize the input node embeddings for different target classes by t-SNE ~\\citep{van2008visualizing}. We conduct the visualization on DBLP with four different target classes, and each target class has more than 1000 node embeddings. Figure~\\ref{fig:visulize} shows that compared with HFC-aware loss, the spectral contrastive loss cannot effectively distinguish different types of sample nodes. Especially in the central part of Figure~\\ref{fig:nohfc}, sample points are almost completely overlapping. It is clear that the HFC-aware loss learns more discriminative node embeddings. \n\n\n% \\begin{figure}[h]\n% \\centering\n% \\subfigure[HCL-NoHFC]{\n% \\begin{minipage}[t]{0.5\\linewidth}\n% \\centering\n% \\includegraphics[width=\\linewidth]{figures/HCL-NoHFC.png}\n% \\label{fig:nohfc}\n% %\\caption{fig1}\n% \\end{minipage}%\n% }%\n% \\subfigure[HCL-HFC]{\n% \\begin{minipage}[t]{0.5\\linewidth}\n% \\centering\n% \\includegraphics[width=\\linewidth]{figures/HCL-HFC.png}\n% \\label{fig:hfc}\n% %\\caption{fig1}\n% \\end{minipage}%\n% }%\n                \n% \\centering\n% \\caption{Embedding visulization of input nodes belonging to different target classes. Points with the same color denote input nodes belonging to the same target class. HFC-aware loss learns more discriminative embeddings than spectral contrastive loss.}\n% \\label{fig:visulize}\n% % \\vspace{-0.3cm}\n% \\end{figure}\n\n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Efficiency Analysis",
                    "content": "\nWe compare the time efficiency between HASH-CODE, and GNN-nested Transformers (GraphFormers). The evaluation is conducted utilizing an Nvidia 3090 GPU. We follow the same setting with~\\citep{yang2021graphformers}, where each mini-batch contains 32 encoding instances; each instance contains one center and \\#N neighbour nodes; the token length of each node is 16. We report the average time and memory (GPU RAM) costs per mini-batch in Table~\\ref{tab:efficiency}. \n\nWe find that the time and memory costs associated with these methods exhibit a linear escalation in tandem with the augmentation of neighboring elements. Meanwhile, the overall time and memory costs of HASH-CODE exhibit a remarkable proximity to GraphFormers, especially when the number of neighbor nodes is small. In light of the above observations, it is reasonable to deduce that HASH-CODE exhibits superior accuracy while concurrently maintaining comparable levels of efficiency and scalability when juxtaposed with GNN-nested transformers.\n\n\n\n% Firstly, the time and memory costs of these methods grow linearly with the increment of neighbours. (There are overheads of time and memory costs. The time cost overhead may come from CPU processing; while the memory cost overhead is mainly due to the model parameters~\\citep{rajbhandari2020zero}). We may approximately remove the overheads by deducting the time and memory costs where \\#N=3). \n\n% Secondly, the overall time and memory costs of HASH-CODE are quite close to GraphFormers. When the number of neighbour nodes is small, the differences between both methods are almost ignorable. The differences become slightly larger when more neighbour nodes are included. However, the differences are still relatively small: merely around 5.8\\% of the overall running costs when \\#N is increased to 200.\n\n\n% Based on the above observations, we may conclude that HASH-CODE are more accurate, meanwhile equally efficient and scalable as the GNN-nested tranformers.\n\n\n\n% \\begin{figure}[h]\n% \\subfigure[DBLP]{\\centering\n%     \\includegraphics[width=0.48\\linewidth]{figures/DBLP_Data_Amount.pdf}\n%     }\n% \\subfigure[Product]{\\centering\n%     \\includegraphics[width=0.48\\linewidth]{figures/Product_Data_Amount.pdf}\n%     }\n%     \\caption{Performance (P@1) comparison w.r.t. different sparsity levels on DBLP and Product datasets. The performance substantially drops when less training data is used, while  HASH-CODE is consistently better than baselines in all\n% cases, especially in an extreme sparsity level (20\\%).}\n%     \\label{fig:sparsity}\n%     % \\vspace{-0.3cm}\n% \\end{figure}\n\n"
                },
                "subsection 4.5": {
                    "name": "In-depth Analysis",
                    "content": "\nWe continue to investigate several properties of the models \nin the next couple sections. To save space, we will mainly present the results here and save the details to the appendix:\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]\n    \\item In Appendix~\\ref{sec:sparsity}, we\nsimulate the data sparsity scenarios by using different proportions of the full dataset. We find that HASH-CODE is consistently better than baselines in all cases, especially in an extreme sparsity level (20\\%). This observation implies that HASH-CODE is able to make better use of the data with the contrastive learning method, which alleviates the influence of data sparsity problem for representation learning to some extent.\n    \\item In Appendix~\\ref{sec:epochs}, we investigate the influence of the number of training epochs on our performance. The results show that our model benefits mostly from the first 20 training epochs. And after that, the performance improves slightly. Based on this observation, we can conclude that the correlations among different views on TAGs can be well-captured by our contrastive learning\napproach through training within a small number of epochs. So that the enhanced data representations can improve the performance of the downstream tasks.\n    \\item In Appendix~\\ref{sec:neighbor_size}, we analyze the impact of neighbourhood size with a fraction of neighbour nodes randomly sampled for each center node. We can observe that with the increasing number of neighbour nodes, both HASH-CODE and Graphformers achieve higher prediction accuracies. However, the marginal gain is varnishing, as the relative improvement becomes smaller when more neighbours are included. In all the testing cases, HASH-CODE maintains consistent advantages over GraphFormers, which demonstrates the effectiveness of our proposed method.\n    % \\item In Appendix~\\ref{sec:visualization}, we visualize the input node embeddings for different target classes by t-SNE ~\\citep{van2008visualizing} to intuitively study the impact of our HFC-loss. We find that our $\\mathcal{L}_{HFC}$ helps the model learn more discriminative node embeddings compared with $\\mathcal{L}_{Spectral}$. \n\\end{itemize}\n\n% \\subsubsection{Performance Comparison w.r.t.  the Amount of Training Data.}\n% Conventional representation learning methods require a considerable amount of training data, thus they are likely to suffer from the data sparsity issues in real-world applications. This problem can be alleviated by our method because the proposed contrastive learning approach can better utilize the data correlation from input. We\n% simulate the data sparsity scenarios by using different proportions of the full dataset, i.e., 20\\%, 40\\%, 60\\%, 80\\%, and 100\\%.\n\n% Figure~\\ref{fig:sparsity} shows the evaluation results on Product and Sports datasets. As\n% we can see, the performance substantially drops when less training data is used. While, HASH-CODE is consistently better than baselines in all cases, especially in an extreme sparsity level (20\\%). This observation implies that HASH-CODE is able to make better use of the data with the contrastive learning method, which alleviates the influence of data\n% sparsity problem for representation learning to some extent.\n\n% \\subsubsection{Performance Comparison w.r.t. the Number of Training Epochs.}\n% Our approach consists of co-training with GNNs and Transformers. During the training stage, our model can learn the enhanced representations of the attribute and node for the representation learning task. The number of training epochs will affect the performance of the downstream task. To\n% investigate this, we train our model with a varying number of epochs and fine-tune it on the downstream task.\n\n% Figure~\\ref{fig:epoch} presents the results on Product and Sports datasets. We can see that our model benefits mostly from the first 20 training epochs. And after that, the performance improves slightly. Based on this observation, we can conclude that the correlations among different views (i.e., the graph topology and textual attributes) can be well-captured by our contrastive learning\n% approach through training within a small number of epochs. So that the enhanced data representations can improve the performance of the downstream tasks.\n\n\n\n\n\n\n% \\subsubsection{Performance Comparison w.r.t. the Neighbor Size.}\n% We analyze the impact of neighbourhood size with a fraction of neighbour nodes randomly sampled for each center node (using DBLP for illustration). The link prediction results are shown in Figure~\\ref{fig:neibor}. We can observe that with the increasing number of neighbour nodes, both HASH-CODE and Graphformers achieve higher prediction accuracies. However, the marginal gain is varnishing, as the relative improvement becomes smaller when more neighbours are included. In all the testing cases, HASH-CODE maintains consistent advantages over GraphFormers, which demonstrates the effectiveness of our proposed method.\n\n% \\begin{figure}[h]\n% \\subfigure[DBLP]{\\centering\n%     \\includegraphics[width=0.48\\linewidth]{figures/Emb_Size_DBLP.pdf}\n%     }\n% \\subfigure[Product]{\\centering\n%     \\includegraphics[width=0.48\\linewidth]{figures/Emb_Size_Products.pdf}\n%     }\n%     \\caption{Performance (P@1, NDCG) comparison w.r.t. different embedding sizes on DBLP and Product datasets.}\n%     \\label{fig:emb_size}\n%     % \\vspace{-0.5cm}\n% \\end{figure}\n\n% \\subsubsection{Performance Comparison w.r.t. the Embedding Size.}\n% We analyze the impact of embedding size with a fraction of neighbour nodes randomly sampled for each center node (using DBLP for illustration). The link prediction results are shown in Figure~\\ref{fig:emb_size}. We can observe that with the increasing number of neighbour nodes, both HASH-CODE and Graphformers achieve higher prediction accuracies. However, the marginal gain is varnishing, as the relative improvement becomes smaller when more neighbours are included. In all the testing cases, HASH-CODE maintains consistent advantages over GraphFormers, which demonstrates the effectiveness of our proposed methods.\n\n\n\n\n% \\begin{figure}[tbp]\n% \\subfigure[DBLP]{\\centering\n%     \\includegraphics[width=0.47\\linewidth]{figures/Emb_Size_DBLP.pdf}\n%     }\n% \\subfigure[Products]{\\centering\n%     \\includegraphics[width=0.47\\linewidth]{figures/Emb_Size_Products.pdf}\n%     }\n%     \\caption{Effect of node embedding dimensions on DBLP and Products datasets. The performance of HASH-CODE increases (before overfitting) as the embedding dimension becomes larger.}\n%     \\label{fig:emb_size}\n%     \\vspace{-0.2cm}\n% \\end{figure}\n\n\n\n% \\subsubsection{Dimension of Node Embedding.} To understand the effect of node embedding dimension, we test the performance of HASH-CODE in link prediction with the embedding dimension varying in 48, 96, 192, 384, and 768. The result is shown in Figure~\\ref{fig:emb_size}. It can be seen that the performance of HASH-CODE generally increases as the embedding dimension becomes larger. This is intuitive since the more parameters the node embeddinghas (before overfitting), the more information it can represent.\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\\label{sec:con}\nIn this paper, we introduce the problem of node representation learning on TAGs and propose HASH-CODE, a hierarchical contrastive learning architecture to address the problem. Different from previous \u201ccascaded architectures\u201d, HASH-CODE utilizes five self-supervised optimization objectives to facilitate thorough mutual enhancement between network and text signals in different granularities. We also propose a HFC-aware spectral contrastive loss to learn more discriminative node embeddings. Experimental results on various graph mining tasks, including link prediction and node classification demonstrate the superiority of HASH-CODE. Moreover, the proposed framework can serve as a building block with different task-specific inductive biases. It would be interesting to see its future applications on real-world TAGs such as recommendation, abuse detection and tweet-based network analysis.% \\clearpage\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{citation}\n% \\clearpage\n\\appendix\n"
            },
            "section 6": {
                "name": "Theoretical Analysis of HFC",
                "content": "\n",
                "subsection 6.1": {
                    "name": "Background: Spectral Clustering",
                    "content": "\n\\label{sec:spectral}\nGiven a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$, with adjacency matrix $A$, the Laplacian matrix of the graph is defined as $L=D - A$, where $D = diag(d_{1},...,d_{N})$ is the diagonal degree matrix ($d_{i}=\\Sigma_{j}A_{i,j}$). Then the symmetric normalized Laplacian matrix is defined as $L_{sym}=D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}$. As $L_{sym}$ is real symmetric and positive semidefinite, therefore it can be diagonalized as $L=U\\Lambda U^{T}$~\\citep{chung1997spectral}. Here $U\\in \\mathbb{R}^{N\\times N}=[u_{1},...,u_{N}]$, where $u_{i}\\in \\mathbb{R}^{N}$ denotes the $i$-th eigenvector of $L_{sym}$ and $\\Lambda=diag(\\lambda_{1},...,\\lambda_{N})$ is the corresponding eigenvalue matrix. To partition the graph, spectral clustering~\\citep{hastie2009elements,von2007tutorial} computes the first K eigenvectors and creates a feature vector $f_{K,v}\\in \\mathbb{R}^{K}$ for each node $v: \\forall k \\in [1,K], f_{K,v}(k)=u_{k}(v)$, which is in turn used to obtain K clusters by K-means or hierarchical clustering, etc.\n\nAn analogy between signals on graphs and usual signals~\\citep{shuman2013emerging} suggests to interpret the spectrum of $L_{sym}$ as a Fourier domain for graphs, hence defining filters on graphs as diagonal operators after change of basis with $U^{-1}$. It turns out that the features $f_{K,v}$ can be\nobtained by ideal low-pass filtering of the Delta function $\\delta_{a}$ (localized at node a). Indeed, let $l_{K}$ be the step function where $l_{K}(\\lambda)=1$ if $\\lambda <\\lambda_{K}$ and 0 otherwise. We define $L_{K}$ the diagonal matrix for which $L_{K}(i,i)=l_{K}(\\lambda_{i})$. Then we have:\n$f_{K,v}=L_{K}U^{-1}\\delta_{v}\\in \\mathbb{R}^{K}$, where we fill the last $N-K$ values with 0\u2019s. Therefore, spectral clustering is equivalent to clustering using low-pass filtering of the local descriptors $\\delta_{v}$ of each node $v$ of the graph $\\mathcal{G}$.\n\n"
                },
                "subsection 6.2": {
                    "name": "Spectral Contrastive Loss Revisited",
                    "content": "\n\\label{sec:scon}\nTo introduce spectral contrastive loss~\\citep{haochen2021provable}, we give the definition of population view graph~\\citep{haochen2021provable} first.\n\n\\noindent \\textbf{Population View Graph.} A population view graph is defined as $\\mathcal{G}=(\\mathcal{X},\\mathcal{W})$, where the set of nodes comprises all augmented views $\\mathcal{X}$ of the population distribution, with $w_{xx^{'}}\\in \\mathcal{W}$ the edge weights of the edges connecting nodes $x,x^{'}$ that correspond to different views of the same input datapoint. The core assumption made is that this graph cannot be split into a large number of disconnected subgraphs. This set-up aligns well with the intuition that in order to generalize, the contrastive notion of \u201csimilarity\u201d must extent beyond the purely single-instance-level, and must somehow connect distinct inputs points.\n\n\\noindent \\textbf{Spectral Contrastive Loss.} Using the concept of population view graph, spectral contrastive loss is defined as:\n\\begin{equation}\n\\begin{aligned}\n    \\mathcal{L}(x,x^{+},x^{-},f_{\\theta}) &= -2\\cdot \\mathbb{E}_{x,x^{+}}[f_{\\theta}(x)^{T}f_{\\theta}(x^{+})]\\\\\n    &\\ +\\mathbb{E}_{x,x^{-}}[(f_{\\theta}(x)^{T}f_{\\theta}(x^{-}))^{2}],\n\\end{aligned}\n\\end{equation}\nwhere $(x,x^{+})$ is a pair of views of the same datapoint, $(x,x^{-})$ is a pair of independently random views, and $f_{\\theta}$ is a parameterized function from the data to $\\mathbb{R}^{k}$. Minimizing spectral contrastive loss is equivalent to spectral clustering on the population view graph, where the top smallest eigenvectors of the Laplacian matrix are preserved as the columns of the final embedding matrix $F$. \n\n"
                },
                "subsection 6.3": {
                    "name": "HFC-aware Spectral Contrastive Loss",
                    "content": "\nAs discussed in Appendix~\\ref{sec:scon}, the spectral contrastive loss only learns the low-frequency component (LFC) of the graph from a spectral perspective, where the effects of high-frequency components (HFC) are much more attenuated. Recent studies have indicated that the LFC does not necessarily contain the most crucial information; while HFC may also encode useful information that is beneficial for the performance~\\citep{bo2021beyond,chen2019drop}. \n% Moreover, LFC eventually leads to the over-smoothing problem~\\citep{cai2020note,chen2020measuring,li2018deeper,liu2020towards} especially when the network exhibits a heterogeneous characteristic, where node representations converge to similar values, thus nodes cannot be easily distinguished. \nIn this regard, merely using the spectral contrastive loss cannot adequately capture the varying significance of different frequency components, thus constraining the expressiveness of learned representations and producing suboptimal learning performance. How to incorporate the HFC to learn a more discriminative embedding still requires explorations.\n\nIn image signal processing, the Laplacian kernel is widely used to capture high-frequency edge information~\\citep{he2012guided}. As its counterpart in\nGraph Signal Processing (GSP)~\\citep{shuman2013emerging}, we can multiply the graph Laplacian matrix $L$ with the input graph signal $x\\in \\mathbb{R}^{N}$, (\\textit{i.e.,} $h=Lx$) to characterize its high-frequency components, which carry sharply varying signal information across edges of graph. On the contrary, when highlighting the LFC, we would subtract the term $Lx$ which emphasizes more on HFC from the input signal $x$, \\textit{i.e.,} $z=x - Lx$. \n\n% It should be noted that the above operation corresponds to a fixed low-pass filter in the spectral domain, where higher weights are specified for LFC. \n% % However, in practice, HFC can also provide complementary insights for learning~\\citep{bo2021beyond,chen2019drop}, especially when the label information is not smooth across edges. Additionally, the HFC of the input graph signal would be unavoidably too much weakened with fixed filters, leading to the well-known over-smoothing problem~\\citep{li2018deeper}. \n% As discussed in Appendix~\\ref{sec:spectral}, spectral clustering is equivalent to clustering using a low-pass filter on each node of the graph. Henceforth, the feature vectors learned by the spectral contrastive loss is LFC of the population view graph. In this regard, the fixed low-pass filters largely limit the fitting capability of contrastive learning and its variants for learning discriminative node representations. As a consequence, it is vital to capture the varying importance of frequencies in the filter to preserve more useful information and alleviate over-smoothing issues. \n\nAs an alternative of the traditional low-pass filter, a simple and elegant solution to introduce HFC is to assign a single parameter to control the rate of high-frequency substraction. \n\\begin{equation}\n    z=x-\\alpha Lx=(I-\\alpha L)x\\nonumber,\n\\end{equation}\nwhere $I$ is the identity matrix. We thus obtain the kernel $I-\\alpha L$ that contains HFC.\n\nFollowing~\\citep{haochen2021provable}, we consider the following matrix factorization based objective for eigenvectors:\n\\begin{equation}\n\\begin{aligned}\n    \\min_{F\\in \\mathbb{R}^{N\\times K}}\\mathcal{L}_{mf}(F) &=||(I-\\alpha L)-FF^{T}||_{F}^{2}\\\\\n    &=((1-\\alpha)I+\\Sigma_{i,j}(\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}-f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})))^{2},\n    \\end{aligned}\n\\end{equation}\nwhere $w_{x}=\\Sigma_{x^{'}\\in \\mathcal{X}}w_{xx^{'}}$ is the total weights associated to view $x$. By the classical low-rank approximation theory (Eckart-Young-Mirsky theorem~\\citep{eckart1936approximation}), minimizer $F$ possesses eigenvectors of HFC-aware kernel $I-\\alpha L$ as columns and thus contains both the LFC and HFC of the population view graph.\n\n\\begin{lemma}\n(HFC-aware spectral contrastive loss.) Denote $p_{x}$ is the $x$-th row of $F$. Let $p_{x}=w_{x}^{1/2}f_{\\theta}(x)$. Then, the loss function $\\mathcal{L}_{mf}(F)$ is equivalent to the following loss function for $f_{\\theta}$, called HFC-aware spectral contrastive loss, up to an additive constant:\n\\begin{equation}\n     \\mathcal{L}_{mf}(F)=\\mathcal{L}_{HFC}(f_{\\theta})+const\\nonumber, \n\\end{equation}\nwhere \n\\begin{equation}\n\\begin{aligned}\n    \\mathcal{L}_{HFC}(f_{\\theta}) &= -2\\alpha \\mathbb{E}_{x,x^{+}}[f_{\\theta}(x)^{T}f_{\\theta}(x^{+})]\\\\\n    &\\ \\ \\ \\ +\\mathbb{E}_{x,x^{-}}[(f_{\\theta}(x)^{T}f_{\\theta}(x^{-}))^{2}]\n    \\end{aligned}\n\\end{equation}\n\n\\end{lemma}\n\n\\begin{proof}\nWe expand $\\mathcal{L}_{mf}(F)$ and obtain\n\n\\begin{equation}\n\\label{eq:all}\n\\begin{aligned}\n    \\mathcal{L}_{mf}(F)\n    &=((1-\\alpha)I+\\Sigma_{i,j}(\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}-f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})))^{2}   \\\\\n    &=const - 2\\Sigma_{i,j}[(1-\\alpha)I+\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}]f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})\\\\\n    &\\ \\ \\ \\ +\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2} \\\\\n    &= \n    \\left\\{  \n\\begin{array}{l}\n    const - 2\\Sigma_{i,j}1-\\alpha+\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})\\\\\n    \\ \\ +\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2}, i=j \\\\\n    const - 2\\Sigma_{i,j}\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})\\\\\n    \\ \\ +\\Sigma_{i,j}(f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j}))^{2}, i\\ne j \n\\end{array}\n\\right.  \n\\end{aligned}\n\\end{equation}\nIn our case two views $x_{i}$ and $x_{j}$ are not the same. We thus only focus on the $i\\ne j$ case. Ignoring the scaling factor which doesn\u2019t affect linear probe error, we can hence rewrite\nthe sum of last two terms of in Equation~\\ref{eq:all} as Equation~\\ref{eq:hfc}.\n\\end{proof}\n\n\n% \\section{Notes on the Experimental Setup}\n% \\subsection{Details of Datasets}\n% \\label{sec:data}\n% We conduct experiments on six datasets (\\textit{i.e.,} DBLP\\footnote{\\noindent  \\url{https://originalstatic.aminer.cn/misc/dblp.v12.7z}}, Wikidata5M\\footnote{\\noindent   \\url{https://deepgraphlearning.github.io/project/wikidata5m}}~\\citep{wang2021kepler}, Beauty, Sports and Toys from Amazon dataset\\footnote{\\noindent  \\url{ http://snap.stanford.edu/data/amazon/}}~\\citep{mcauley2015image} and Product Graph) from three different domains (i.e., academic papers, social media posts, and e-commerce):\n\n% \\noindent \\textbf{DBLP}: is a real-world academic citation graph dataset that contains the paper citation graph from DBLP up to 2020-04-09. Two papers are linked if one is cited by the other one. The paper\u2019s title is used as the textual feature.\n\n% \\noindent \\textbf{Wikidata5M} (Wiki): is a public entity graph dataset which contains the entity graph from Wikipedia. The first sentence in each entity\u2019s introduction is taken as its textual feature.\n\n% \\noindent \\textbf{Amazon Beauty, Sports and Toys}: are\n% obtained from Amazon review datasets in~\\citep{mcauley2015image}, which contain product ratings and reviews in 29\n% categories on Amazon.com and rich textual metadata such as title, brand, description, etc. We use the version released in the year 2018.\n% Specifically, we select three subcategories: \u201cBeauty\u201d, \u201cSports and Outdoors\u201d, and\n% \u201cToys and Games\u201d, and utilize the brands and the\n% descriptions of the items as attributes. We treat all the user-item rating records as implicit feedback and sort them according to the timestamps to form sequences. Following the common settings~\\citep{kang2018self}, we filter out users and items with less than five interaction records. For each user, we use the last clicked item for testing, the penultimate one for validation, and the remaining clicked items for training.\n\n% \\textbf{Product Graph} (Product): is an even larger dataset of online products collected by a world-wide search engine. In this dataset, the users\u2019 web browsing behaviors are tracked for the targeted product webpages (e.g., Amazon webpages of Nike shoes). The user\u2019s continuously browsed webpages within a short period of time (e.g., 30 minutes) is called a \u201csession\u201d. The products within a common session are connected in the graph (which is a common way of graph construction in e-commerce scenarios~\\citep{ying2018graph,wang2018billion}). Each product has its unique textual description, which specifies information like the product name, brand, and saler, etc.\n\n% The textual features of all the datasets are in English. We make use of uncased WordPiece~\\citep{wu2016google} to tokenize the input text. \n\n% \\subsection{Details of Baselines}\n% \\label{sec:baseline}\n% To thoroughly examine the effectiveness of our proposed method and substantiate its validity, we contrast three types of competitive methods:\n\n% \\textbf{First}, to verify the importance of both text and network information, we consider the vanilla textual/graph models that only exploit partial observed information (i.e., textual or structural) for node representation learning.\n% \\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]\n%     \\item Vanilla GraphSAGE~\\citep{hamilton2017inductive}: This is a GNN method that employs the mean function to aggregate information from neighbors for center node embedding learning. The initial node feature vector is bag-of-words weighted by TF-IDF. The number of entries in each attribute vector corresponds to the vocabulary size of the respective dataset, where we retain the most representative 10000, 2000, and 5000 words for DBLP, Wiki, and Product, respectively, in accordance with the corpus size.\n%     \\item Vanilla GAT~\\citep{velivckovic2017graph}: Simillar with the vanilla GraphSAGE, we employ the graph attention networks to aggregate the information from neighbors for center node embedding learning. \n%     \\item Vanilla BERT~\\citep{devlin2018bert}: This is a standard PLM pretrained on two tasks: next sentence prediction and mask token prediction. For each text-rich node, we use BERT to encode its text and extract the output of the [CLS] token as node representation.\n%     \\item Twin-Bert~\\cite{lu2020twinbert}: This is a two-tower BERT-based structure model, which serves for the efficient retrieval.\n% \\end{itemize}\n\n% \\textbf{Second}, the GNN-cascaded transformers which combines the GNN and PLM in a \"cascaded architectute\" that learns the node representation with fixed textual embeddings.\n\n% \\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]\n%     \\item BERT+MaxSAGE~\\citep{hamilton2017inductive}: We combine BERT with MaxSAGE (i.e., using the output text representation of BERT as the input node attribute vector of MaxSAGE). The BERT+MaxSAGE model is trained in an end-to-end manner. Other BERT+GNN baselines below have the same cascaded architecture.\n%     \\item BERT+MeanSAGE~\\citep{hamilton2017inductive}: MeanSAGE is a GNN method that applies the mean function during neighbor aggregation for center node representation learning.\n%     \\item BERT+GAT~\\citep{velivckovic2017graph}: GAT is a GNN method with attention-based neighbor importance calculation, and the weight of each neighbor during aggregation depends on its importance score.\n%     \\item TextGNN~\\citep{zhu2021textgnn}:  This model incorporates the text and graph information\n% with a node-level aggregator, in which the query encoders share the same parameters.\n% \\item AdsGNN~\\citep{li2021adsgnn}: This model also utilizes a node-level aggregator to aggregate the graph information at different levels, and introduces domain-specific pre-training and knowledge-distillation techniques to improve model performance.\n% \\end{itemize}\n\n% \\textbf{Third}, the state-of-the-art co-training-based methods that enables the joint encoding of text and node features for the node representation learning on TAGs.\n% \\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]\n%     \\item GraphFormers~\\citep{yang2021graphformers}: This is the state-of-the-art GNN-nested transformer model, which has graph-based propagation and aggregation in each transformer layer.\n%     \\item Heterformer~\\citep{jin2022heterformer}: This model alternately stacks the graph-attention-based neighbor aggregation module and the transformer-based text and neighbor joint encoding module to facilitate thorough mutual enhancement between network and text signals.\n% \\end{itemize}\n\n% \\subsection{Summary of HASH-CODE\u2019s workflow}\n% \\label{sec:workflow}\n% \\begin{algorithm}[h]\n% \\begin{algorithmic}\n% \\small\n% \\STATE {\\bfseries Input:} The input graphs $G$ (consist of the center node $v$ and its neighbours).\n% \\STATE {\\bfseries Output:} The embedding for the center node \\textbf{$h_v$}.\n% \\FOR {each text $g\\in G$}\n% \\STATE $H_{g}^{1}\\leftarrow \\textnormal{TRM}^{0}(H_{g}^{0});$ // Get the initial token-level embeddings.\n% \\ENDFOR\n% \\FOR {$l=1,...,L-1$}\n% \\STATE $Z_{g}^{l}\\leftarrow \\{z_{g}^{l}|g\\in G\\};$  // Gather node-level embeddings to GNN\n% \\STATE $\\widetilde{Z}_{g}^{l}\\leftarrow \\textnormal{GNN}(Z_{g}^{l});$ // Graph aggregation in GNN\n% \\FOR {$i=1,...,5$}\n% \\STATE $\\hat{Z}_{g}^{l}\\leftarrow \\textnormal{Contrastive}_{i}(\\widetilde{Z}_{g}^{l},H_{g}^{l});$ // Hierarchical contrastive learning for mutually reinforce the textual and graphic patterns\n% \\ENDFOR\n% \\FOR {each text $g\\in G$}\n% \\STATE $\\widetilde{H}_{g}^{l}\\leftarrow \\textnormal{Concat}(\\hat{z}_{g}^{l},H_{g}^{l});$ // Get contrastive graph-augmented token-level embeddings\n% \\STATE $H_{g}^{l+1}\\leftarrow \\textnormal{TRM}^{l}(\\widetilde{H}_{g}^{l});$ // Text encoding in Transformer\n% \\ENDFOR\n% \\ENDFOR\n%  \\RETURN $h_{v}\\leftarrow \\hat{z}_{v}^{L}$\n%  \\end{algorithmic}\n%  \\caption{HCL-TAG's Workflow}\n%  \\label{alg:main}\n% \\end{algorithm}\n\n"
                }
            },
            "section 7": {
                "name": "Node Classification",
                "content": "\n\\label{sec:node_classification}\n\\textbf{Settings.} In node classification, we train a 2-layer MLP classifier to classify nodes based on the output node representation embeddings of each method. The experiment is conducted on DBLP. Following~\\citep{jin2022heterformer}, we select the most frequent 30 classes in DBLP. Also, we study both transductive and inductive node classification to understand the capability of our model comprehensively. For transductive node classification, the model has seen the classified nodes during representation learning (using the link prediction objective), while for inductive node classification, the model needs to predict the label of nodes not seen before. We separate the whole dataset into train set, validation set, and test set in 7:1:2 in all cases and each experiment is repeated 5 times in this section with the average performance reported.\n\n\n\n\\noindent \\textbf{Results.} Table~\\ref{tab:classification} demonstrates the results of different methods in transductive and inductive node classification. We observe that: (a) our HASH-CODE outperforms all the baseline methods significantly on both tasks, showing that HASH-CODE can learn more effective node representations for these tasks; (b) GNN-nested transformers generally achieve better results than GNN-cascaded transformers, which demonstrates the necessity of introducing graphic patterns in modeling textual representations; (c) HASH-CODE generalizes quite well on unseen nodes as its performance on inductive node classification is quite close to that on transductive node classification. Moreover, HASH-CODE even achieves higher performance in inductive settings than the baselines do in transductive settings.\n\n\n\n"
            },
            "section 8": {
                "name": "In-depth Analysis",
                "content": "\n",
                "subsection 8.1": {
                    "name": "Data Sparsity Analysis",
                    "content": "\n\\label{sec:sparsity}\n% Conventional representation learning methods require a considerable amount of training data, thus they are likely to suffer from the data sparsity issues in real-world applications. This problem can be alleviated by our method because the proposed contrastive learning approach can better utilize the data correlation from input. \nWe\nsimulate the data sparsity scenarios by using different proportions of the full dataset.\nFigure~\\ref{fig:sparsity} shows the evaluation results on Product and Sports datasets. As\nwe can see, the performance substantially drops when less training data is used. While, HASH-CODE is consistently better than baselines in all cases, especially in an extreme sparsity level (20\\%). This observation implies that HASH-CODE is able to make better use of the data with the contrastive learning method, which alleviates the influence of data\nsparsity problem for representation learning to some extent.\n\n\n\n\n\n% \\begin{figure*}[h]\n% \\centering\n% \\subfigure[Product]{\n% \\includegraphics[width=0.25\\textwidth]{figures/Product_Data_Amount.pdf}\n% \\label{figure:prodcut_sparsity}\n% }\n% \\subfigure[Beauty]{\n% \\includegraphics[width=0.25\\textwidth]{figures/Beauty_Data_Amount.pdf}\n% \\label{figure:beauty_sparsity}\n% }\n% \\subfigure[Sports]{\n% \\includegraphics[width=0.25\\textwidth]{figures/Sports_Data_Amount.pdf}\n% \\label{figure:sports_sparsity}\n% }\n\n% \\subfigure[Toys]{\n% \\includegraphics[width=0.25\\textwidth]{figures/Toys_Data_Amount.pdf}\n% \\label{figure:toys_sparsity}\n% }\n% \\subfigure[DBLP]{\n% \\includegraphics[width=0.25\\textwidth]{figures/DBLP_Data_Amount.pdf}\n% \\label{figure:dblp_sparsity}\n% }\n% \\subfigure[Wiki]{\n% \\includegraphics[width=0.25\\textwidth]{figures/Wiki_Data_Amount.pdf}\n% \\label{figure:wiki_sparsity}\n% }\n% \\caption{Performance (P@1) comparison w.r.t. different sparsity levels on DBLP and Product datasets. The performance substantially drops when less training data is used, while  HASH-CODE is consistently better than baselines in all\n% cases, especially in an extreme sparsity level (20\\%).}\n% \\label{fig:sparsity}\n% \\end{figure*}\n\n\n\n"
                },
                "subsection 8.2": {
                    "name": "Influence of Training Epochs Number",
                    "content": "\n\\label{sec:epochs}\n% Our approach consists of co-training with GNNs and Transformers. During the training stage, our model can learn the enhanced representations of the attribute and node for the representation learning task. The number of training epochs will affect the performance of the downstream task. To\n% investigate this, \n\nWe train our model with a varying number of epochs and fine-tune it on the downstream task.\nFigure~\\ref{fig:epoch} presents the results on Product and Sports datasets. We can see that our model benefits mostly from the first 20 training epochs. And after that, the performance improves slightly. Based on this observation, we can conclude that the correlations among different views (i.e., the graph topology and textual attributes) can be well-captured by our contrastive learning\napproach through training within a small number of epochs. So that the enhanced data representations can improve the performance of the downstream tasks.\n\n\n\n\n\n% \\begin{figure}[h]\n% \\centering\n% \\subfigure[Product]{\n% \\includegraphics[width=0.45\\linewidth]{figures/Product_Size_P1.pdf}\n% \\label{figure:prodcut_size}\n% }\n% % \\subfigure[Beauty]{\n% % \\includegraphics[width=0.3\\textwidth]{figures/Beauty_Size_P1.pdf}\n% % \\label{figure:beauty_size}\n% % }\n% % \\subfigure[Sports]{\n% % \\includegraphics[width=0.3\\textwidth]{figures/Sports_Size_P1.pdf}\n% % \\label{figure:sports_size}\n% % }\n\n% % \\subfigure[Toys]{\n% % \\includegraphics[width=0.3\\textwidth]{figures/Toys_Size_P1.pdf}\n% % \\label{figure:toys_size}\n% % }\n% \\subfigure[DBLP]{\n% \\includegraphics[width=0.45\\linewidth]{figures/Neighbor_Size_P1.pdf}\n% \\label{figure:dblp_size}\n% }\n% % \\subfigure[Wiki]{\n% % \\includegraphics[width=0.3\\textwidth]{figures/Wiki_Size_P1.pdf}\n% % \\label{figure:wiki_size}\n% % }\n% \\caption{Impact of neighbor size on DBLP dataset. Enlarging the number of neighbour nodes brings performance improvement to both models. HASH-CODE maintains consistent advantages over GraphFormers over all test cases.}\n% \\label{fig:neibor}\n% \\end{figure}\n\n\n"
                },
                "subsection 8.3": {
                    "name": "Influence of Neighbor Size",
                    "content": "\n\\label{sec:neighbor_size}\nWe analyze the impact of neighbourhood size with a fraction of neighbour nodes randomly sampled for each center node. From Figure~\\ref{fig:neibor}, we can observe that with the increasing number of neighbour nodes, both HASH-CODE and Graphformers achieve higher prediction accuracies. However, the marginal gain is varnishing, as the relative improvement becomes smaller when more neighbours are included. In all the testing cases, HASH-CODE maintains consistent advantages over GraphFormers, which demonstrates the effectiveness of our proposed method.\n\n% \\subsection{HFC-aware Embedding Visualization.}\n% \\label{sec:visualization}\n% To intuitively study the impact of our HFC-loss, we visualize the input node embeddings for different target classes by t-SNE ~\\citep{van2008visualizing}. We conduct the visualization on DBLP with four different target classes, and each target class has more than 1000 node embeddings. Figure~\\ref{fig:visulize} shows that compared with HFC-aware loss, the spectral contrastive loss cannot effectively distinguish different types of sample nodes. Especially in the central part of Figure~\\ref{fig:nohfc}, sample points are almost completely overlapping. It is clear that the HFC-aware loss learns more discriminative node embeddings. \n\n\n% \\begin{figure}[h]\n% \\centering\n% \\subfigure[HASH-CODE-NoHFC]{\n% \\begin{minipage}[t]{0.5\\linewidth}\n% \\centering\n% \\includegraphics[width=\\linewidth]{figures/HCL-NoHFC.png}\n% \\label{fig:nohfc}\n% %\\caption{fig1}\n% \\end{minipage}%\n% }%\n% \\subfigure[HASH-CODE-HFC]{\n% \\begin{minipage}[t]{0.5\\linewidth}\n% \\centering\n% \\includegraphics[width=\\linewidth]{figures/HCL-HFC.png}\n% \\label{fig:hfc}\n% %\\caption{fig1}\n% \\end{minipage}%\n% }%               \n% \\centering\n% \\caption{Embedding visulization of input nodes belonging to different target classes. Points with the same color denote input nodes belonging to the same target class. HFC-aware loss learns more discriminative embeddings than spectral contrastive loss.}\n% \\label{fig:visulize}\n% % \\vspace{-0.3cm}\n% \\end{figure}\n\n\n\n%% The file named.bst is a bibliography style file for BibTeX 0.99c\n\n"
                }
            }
        },
        "tables": {
            "tab:dataset": "\\begin{table}[t]\n    \\centering\n    \\caption{Statistics of datasets after preprocessing.}\n    \\resizebox{0.5\\textwidth}{!}{\n    \\begin{tabular}{ccccccc}\n        \\toprule\n           Dataset      & Product   & Beauty & Sports & Toys & DBLP & Wiki\\\\\n        \\midrule\n        \\#Users  & 13,647,591 & 22,363 & 25,598 & 19,412 & N/A & N/A \\\\\n        \\#Items & 5,643,688 & 12,101 & 18,357 & 11,924 & 4,894,081 & 4,818,679  \\\\\n        \\#N   & 4.71 & 8.91 & 8.28 & 8.60 & 9.31 & 8.86  \\\\\n        \\#Train  & 22,146,934 & 188,451 & 281,332 & 159,111 & 3,009,506 & 7,145,834  \\\\\n        \\#Valid  & 30,000 & 3,770 & 5,627 & 3,182&  60,000 & 66,167  \\\\\n        \\#Test   & 306,742 & 6,280 & 9,377 & 5,304 & 100,000  & 100,000  \\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\label{tab:dataset}\n    % \\vspace{-0.4cm}\n\\end{table}",
            "tab:main": "\\begin{table*}[h]\n\\huge\n\\renewcommand{\\arraystretch}{1.5}\n\\centering\n  \\caption{Experiment results of link prediction. The results of the best performing baseline are underlined.\n  % (HASH-CODE marked in bold, the best baseline underlined). \n  The numbers in bold indicate statistically significant improvement (p < .01) by the pairwise t-test comparisons over the other baselines.}\n  \\label{tab:main}\n  \\resizebox{1.0\\textwidth}{!}{\n  \\begin{tabular}{ccccccccccccccc}\n    \\toprule\n    Datasets & Metric & MeanSAGE & GAT & Bert & Twin-Bert & Bert+MeanSAGE & Bert+MaxSAGE & Bert+GAT & TextGNN &  AdsGNN & GraphFormers & Heterformer & HASH-CODE & Improv. \\\\ \n    \\midrule\n    \\multirow{3}{*}{Product} & P@1 & 0.6071 & 0.6049 & 0.6563 & 0.6492 & 0.7240 & 0.7250 & 0.7270 & 0.7431 & 0.7623 & 0.7786 & \\underline{0.7820} & $\\textbf{0.7967}^{*}$ & 1.88\\%  \\\\ & NDCG & 0.7384 & 0.7401 & 0.7911 & 0.7907 & 0.8337 & 0.8371 & 0.8378 & 0.8494 & 0.8605 & 0.8793 & \\underline{0.8861} & $\\textbf{0.9039}^{*}$ & 2.01\\% \\\\ & MRR & 0.6619 & 0.6627 & 0.7344 & 0.7285 & 0.7871 & 0.7832 & 0.7880 & 0.8107 & 0.8361 & 0.8430 & \\underline{0.8492} & $\\textbf{0.8706}^{*}$ & 2.52\\% \\\\\n    \\midrule\n    \\multirow{3}{*}{Beauty} & P@1 & 0.1376 & 0.1367 & 0.1528 & 0.1492 & 0.1593 & 0.1586 & 0.1544 & 0.1625 & 0.1669 & \\underline{0.1774} & 0.1739 & $\\textbf{0.1862}^{*}$ & 4.96\\% \\\\ & NDCG & 0.2417 & 0.2469 & 0.2702 & 0.2683 & 0.2741 & 0.2756 & 0.2726 & 0.2863 & 0.2891 & \\underline{0.2919} & 0.2911 & $\\textbf{0.3061}^{*}$ & 4.86\\% \\\\ & MRR & 0.2558 & 0.2549 & 0.2680 & 0.2638 & 0.2712 & 0.2759 & 0.2720 & 0.2802 & 0.2821 & \\underline{0.2893} & 0.2841 & $\\textbf{0.3057}^{*}$ & 5.67\\% \\\\\n    \\midrule\n    \\multirow{3}{*}{Sports} & P@1 & 0.1102 & 0.1088  & 0.1275 & 0.1237 & 0.1330 & 0.1311 & 0.1302 & 0.1421 & 0.1466 & \\underline{0.1548} & 0.1534 & $\\textbf{0.1623}^{*}$ & 4.84\\% \\\\ & NDCG & 0.2091 & 0.2116 & 0.2375 & 0.2297 & 0.2432 & 0.2478 & 0.2419 & 0.2537 & 0.2582 & 0.2674 & \\underline{0.2692} & $\\textbf{0.2775}^{*}$ & 3.08\\% \\\\ & MRR & 0.2171 & 0.2168 & 0.2319 & 0.2296 & 0.2434 & 0.2471 & 0.2397 & 0.2612 & 0.2653 & \\underline{0.2679} & 0.2640 & $\\textbf{0.2754}^{*}$ & 2.80\\% \\\\\n    \\midrule\n    \\multirow{3}{*}{Toys} & P@1 & 0.1342 & 0.1330 & 0.1498 & 0.1427 & 0.1520 & 0.1536 & 0.1514 & 0.1658 & 0.1674 & \\underline{0.1703} & 0.1685 & $\\textbf{0.1767}^{*}$ & 3.76\\% \\\\ & NDCG & 0.2015 & 0.2028 & 0.2249 & 0.2206 & 0.2451 & 0.2486 & 0.2413 & 0.2692 & 0.2734 & \\underline{0.2859} & 0.2823 & $\\textbf{0.2946}^{*}$ & 3.04\\% \\\\ & MRR & 0.2173 & 0.2149 & 0.2311 & 0.2276 & 0.2509 & 0.2527 & 0.2476 & 0.2648 & 0.2715 & \\underline{0.2803} & 0.2778 & $\\textbf{0.2919}^{*}$ & 4.14\\% \\\\\n    \\midrule\n    %\u7ec6\u6570\u6570\u636e\u96c6\u4e0a\u7684improv.\u66f4\u5927\n    \\multirow{3}{*}{DBLP} & P@1 & 0.4963 & 0.4931 & 0.5673 & 0.5590 & 0.6533 & 0.6596 & 0.6634 & 0.6913 & 0.7102 & 0.7267 & \\underline{0.7288} & $\\textbf{0.7446}^{*}$ & 2.17\\% \\\\ & NDCG & 0.6997 & 0.6981 & 0.7484 & 0.7417 & 0.8004 & 0.8059 & 0.8086 & 0.8331 & 0.8507 & 0.8565 & \\underline{0.8576} & $\\textbf{0.8823}^{*}$ & 2.88\\% \\\\ & MRR & 0.6314 & 0.6309 & 0.6777 & 0.6643 & 0.7266 & 0.7067 & 0.7300 & 0.7792 & 0.7805 & 0.8133 & \\underline{0.8148} & $\\textbf{0.8428}^{*}$ & 3.44\\% \\\\\n    \\midrule\n    \\multirow{3}{*}{Wiki} & P@1 & 0.2850 & 0.2862 & 0.3066 & 0.3015 & 0.3306 & 0.3264 & 0.3412 & 0.3693 & 0.3820 & \\underline{0.3952} & 0.3947 & $\\textbf{0.4104}^{*}$ & 3.85\\% \\\\ & NDCG & 0.5389 & 0.5357 & 0.5699 & 0.5613 & 0.5730 & 0.5737 & 0.6071 & 0.6098 & 0.6155 & 0.6230 & \\underline{0.6233} & $\\textbf{0.6402}^{*}$ & 2.71\\% \\\\ & MRR & 0.4411 & 0.4436 & 0.4712 & 0.4602 & 0.4980 & 0.4970 & 0.5022 & 0.5097 & 0.5134 & \\underline{0.5220} & 0.5216 & $\\textbf{0.5356}^{*}$ & 2.61\\% \\\\ \\bottomrule\n  \\end{tabular} }\n  % \\vspace{-0.3cm}\n\\end{table*}",
            "tab:efficiency": "\\begin{table*}[t]\n% \\Large\n  \\caption{Time and memory costs per mini-batch for GraphFormers and HASH-CODE, with neighbour size increased from 3 to 200. HASH-CODE achieve similar efficiency and scalability as GraphFormers.}\n  \\label{tab:efficiency}\n  {\n  \\begin{tabular}{cccccccc}\n    \\toprule\n    \\#N & 3 & 5 & 10 & 20 & 50 & 100 & 200 \\\\\n    \\midrule\n    Time: GraphFormers  & $63.95$ms  & $97.19$ms & $170.16$ms & $306.12$ms & $714.32$ms & $1411.09$ms & $2801.67$ms   \\\\\n    Time: HASH-CODE & $67.68$ms & $105.35$ms & $180.03$ms &  $324.11$ms& $754.97$ms & $1573.29$ms & $2962.86$ms  \\\\\n    \\midrule\n    Mem: GraphFormers & $1.33$GiB & $1.39$GiB & $1.55$GiB & $1.83$GiB & $2.70$GiB & $4.28$GiB & $7.33$GiB   \\\\\n    Mem: HASH-CODE & $1.33$GiB & $1.39$GiB & $1.55$GiB & $1.84$GiB & $2.72$GiB & $4.43$GiB & $7.72$GiB   \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n\\end{table*}",
            "tab:classification": "\\begin{table}[h]\n% \\Large\n  \\caption{Experiment results of transductive and inductive node classification on DBLP dataset. (HASH-CODE marked in bold, the best baseline underlined). HASH-CODE outperforms all baselines, especially the ones based on GNN-nested transformers.}\n  \\label{tab:classification}\n  {\n  \\begin{tabular}{c|cc|cc}\n    \\toprule\n    \\multirow{2}{*}{Model} & \\multicolumn{2}{c}{Transductive} & \\multicolumn{2}{c}{Inductive}  \\\\\n               ~&P@1 & NDCG  &P@1 & NDCG \\\\\n    \\midrule\n    MeanSAGE & $0.5186$ & $0.7231$ & $0.5152$ & $0.7197$  \\\\\n    GAT & $0.5208$ & $0.7196$ & $0.5126$ & $0.7146$  \\\\\n    Bert & $0.5493$ & $0.7506$ & $0.5310$ & $0.7485$  \\\\\n    Twin-Bert & $0.5291$ & $0.7440$ & $0.5248$ & $0.7431$  \\\\\n    \\midrule\n    Bert+MeanSAGE & $0.6731$ & $0.7637$ & $0.6413$ & $0.7494$   \\\\\n    Bert+MaxSAGE & $0.6705$ & $0.7752$ & $0.6587$ & $0.7599$   \\\\\n    Bert+GAT & $0.6849$ & $0.0.7801$ & $0.6689$ & $0.0.7619$   \\\\\n    \\midrule\n    TextGNN & $0.6820$ & $0.7753$ & $0.6380$ & $0.7716$   \\\\\n    AdsGNN & $0.6882$ & $0.7790$ & $0.6624$ & $0.7737$   \\\\\n    GraphFormers & $0.6919$ & $0.7929$ & $\\underline{0.6791}$ & $0.7993$   \\\\\n    Heterformer & $\\underline{0.6924}$ & $\\underline{0.7957}$ & $0.6746$ & $\\underline{0.8079}$   \\\\\n    \\midrule\n    HASH-CODE & $\\textbf{0.7116}$ & $\\textbf{0.8198}$ & $\\textbf{0.6961}$ &  $\\textbf{0.8170}$    \\\\\n    \\midrule\n    \\textit{Improv.} & $2.77\\%$ & $3.03\\%$ & $2.50\\%$ &  $1.13\\%$    \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n  % \\vspace{-0.3cm}\n\\end{table}"
        },
        "figures": {
            "fig:example": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/illustrate.png}\n    \\caption{(a) An illustration of GNN-cascaded transformer. (b) An illustration of our proposed contrastive learning-empowered GNN-nested transformer. The red and green twines denote the original graph signals and the mixed LFC and HFC signals from the spectral perspective.}\n    \\label{fig:example}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:main": "\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{figures/framework.pdf}\n    \\caption{The overall architecture of HASH-CODE. \n    % We take the unidirectional-simplified GraphFormers trained with the two-stage progressive learning as our base model. \n    With GraphFormers as our base model, we incorporate five self-supervised learning objectives based on the HFC-aware contrastive loss to capture the text-graph correlations in different granularities. Spectral contrastive loss learns the LFC while our HFC-aware loss achieves the balance between HFC and LFC.}\n    \\label{fig:main}\n\\end{figure*}",
            "fig:tt": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.8\\linewidth]{figures/Token-Token.png}\n    \\caption{Token-level contrastive selective coding.}\n    \\label{fig:tt}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:nn": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.8\\linewidth]{figures/Node-Node.png}\n    \\caption{Modeling node-level correlations.}\n    \\label{fig:nn}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:ss": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.8\\linewidth]{figures/Sub-Sub.png}\n    \\caption{Modeling subgraph-level correlations.}\n    \\label{fig:ss}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:ablation": "\\begin{figure}[h]\n\\subfigure[P@1]{\\centering\n    \\includegraphics[width=0.48\\linewidth]{figures/ablation_p1.png}\n    }\n\\subfigure[NDCG]{\\centering\n    \\includegraphics[width=0.48\\linewidth]{figures/ablation_ndcg.png}\n    }\n    \\caption{Ablation studies of different components on DBLP and Products datasets.}\n    \\label{fig:ablation}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:sparsity": "\\begin{figure}[h]\n\\subfigure[DBLP]{\\centering\n    \\includegraphics[width=0.48\\linewidth]{figures/DBLP_Data_Amount.pdf}\n    }\n\\subfigure[Product]{\\centering\n    \\includegraphics[width=0.48\\linewidth]{figures/Product_Data_Amount.pdf}\n    }\n    \\caption{Performance (P@1) comparison w.r.t. different sparsity levels on DBLP and Product datasets. The performance substantially drops when less training data is used, while  HASH-CODE is consistently better than baselines in all\ncases, especially in an extreme sparsity level (20\\%).}\n    \\label{fig:sparsity}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:epoch": "\\begin{figure}[h]\n\\subfigure[DBLP]{\\centering\n    \\includegraphics[width=0.48\\linewidth]{figures/DBLP_Epoch.pdf}\n    }\n\\subfigure[Product]{\\centering\n    \\includegraphics[width=0.48\\linewidth]{figures/Product_Epoch.pdf}\n    }\n    \\caption{Performance (P@1) comparison w.r.t. different numbers of training epochs on DBLP and Product datasets. HASH-CODE benefits mostly from the first 20 training epochs, thus  the correlations among different views can be well-captured by our approach through training within a small number of epochs.}\n    \\label{fig:epoch}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:neibor": "\\begin{figure}[h]\n\\subfigure[DBLP]{\\centering\n    \\includegraphics[width=0.48\\linewidth]{figures/Neighbor_Size_P1.pdf}\n    }\n\\subfigure[Product]{\\centering\n    \\includegraphics[width=0.48\\linewidth]{figures/Product_Size_P1.pdf}\n    }\n    \\caption{Impact of neighbor size on DBLP and Product dataset. Enlarging the number of neighbour nodes brings performance improvement to both models. HASH-CODE maintains consistent advantages over GraphFormers over all test cases.}\n    \\label{fig:neibor}\n    % \\vspace{-0.3cm}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\begin{aligned}\n    \\mathcal{L}_{Spectral}(v,v^{+},v^{-}) &= -2\\cdot \\mathbb{E}_{v,v^{+}}[f_{\\theta}(v)^{T}f_{\\theta}(v^{+})]\\\\\n    &\\ +\\mathbb{E}_{v,v^{-}}[(f_{\\theta}(v)^{T}f_{\\theta}(v^{-}))^{2}],\n\\end{aligned}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{eq:hfc}\n\\begin{aligned}\n    \\mathcal{L}_{HFC}(v,v^{+},v^{-}) &= -2\\alpha\\cdot \\mathbb{E}_{v,v^{+}}[f_{\\theta}(v)^{T}f_{\\theta}(v^{+})]\\\\\n    &\\ +\\mathbb{E}_{v,v^{-}}[(f_{\\theta}(v)^{T}f_{\\theta}(v^{-}))^{2}],\n    \\end{aligned}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    s(h_{x_{i:j}},h_{v}) = \\frac{h_{x_{i:j}}\\cdot h_{v}}{\\tau_{h_{v}}},\n    \\tau_{h_{v}} = \\frac{\\Sigma_{h_{x_{i}}\\in H_{v}}||h_{x_{i}}-h_{v}||_{2}}{|H_{v}|log(|H_{v}|+\\epsilon)}\\nonumber,\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    % \\mathcal{L}_{TC}=E_{p(x_{i:j},\\hat{x}_{i:j})}[L_{contrast}(g_{\\omega}(x_{i:j}),g_{\\omega}(\\hat{x}_{i:j}),N_{select}(z),\\tau)],\n    \\mathcal{L}_{TC}=\\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(x_{m,i:j},\\hat{x}_{m,i:j},\\mathcal{N}_{select}(h_{x_{m,i:j}})),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\mathcal{L}_{NC}=\\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(x_{m,v},N_{m,v},\\widetilde{v_{m}})\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    S=\\alpha\\cdot(I-(1-\\alpha)\\cdot\\overline{A})\\nonumber,\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    idx = top\\_rank(S(i,:), k)\\nonumber,\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    A_{i}=A_{idx,idx,} X_{i}=X_{idx,:,}\\nonumber\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    H_{i} = \\mathcal{E}(A_{i},X_{i})\\nonumber\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    s_{i}=\\mathcal{R}(H_{i})\\nonumber\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    \\mathcal{L}_{SC} = \\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(s_{m}, \\hat{s}_{m},\\widetilde{s_{m}})\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    \\{\\widetilde{h_{1}}, \\widetilde{h_{2}},...,\\widetilde{h_{M}}\\} = \\mathcal{P}\\{h_{1}, h_{2},...,h_{M}\\}\\nonumber,\n\\end{equation}",
            "eq:13": "\\begin{equation}\n    % \\mathcal{L}_{TNC} = \\frac{1}{M}\\Sigma_{k=1}^{M}\\mathbb{E}_{}[\\mathcal{L}_{HFC}(x_{i:j},h_{k},\\widetilde{h_{k}})]\n    \\mathcal{L}_{TNC} = \\frac{1}{M}\\Sigma_{m=1}^{M}\\mathcal{L}_{HFC}(x_{m,v},h_{m,v},\\mathcal{P}\\{h_{1}, h_{2},...,h_{M}\\})\n\\end{equation}",
            "eq:14": "\\begin{equation}\n    \\{\\widetilde{s_{1}}, \\widetilde{s_{2}},...,\\widetilde{s_{M}}\\} = \\mathcal{P}\\{s_{1}, s_{2},...,s_{M}\\}\\nonumber\n\\end{equation}",
            "eq:15": "\\begin{equation}\n    \\mathcal{L}_{NSC} =\\frac{1}{M}\\Sigma_{m=1}^{M} \\mathcal{L}_{HFC}(h_{m,v},s_{m,v},\\mathcal{P}\\{s_{1}, s_{2},...,s_{M}\\})\n\\end{equation}",
            "eq:16": "\\begin{equation}\n\\begin{aligned}\n    \\mathcal{L}_{HASH-CODE}&=\\lambda_{TC}\\mathcal{L}_{TC}+\\lambda_{NC}\\mathcal{L}_{NC}+\\lambda_{SC}\\mathcal{L}_{SC}\\\\\n    &+\\lambda_{TNC}\\mathcal{L}_{TNC}+\\lambda_{NSC}\\mathcal{L}_{NSC},\n\\end{aligned}\n\\end{equation}",
            "eq:17": "\\begin{equation}\n\\begin{aligned}\n    \\mathcal{L}(x,x^{+},x^{-},f_{\\theta}) &= -2\\cdot \\mathbb{E}_{x,x^{+}}[f_{\\theta}(x)^{T}f_{\\theta}(x^{+})]\\\\\n    &\\ +\\mathbb{E}_{x,x^{-}}[(f_{\\theta}(x)^{T}f_{\\theta}(x^{-}))^{2}],\n\\end{aligned}\n\\end{equation}",
            "eq:18": "\\begin{equation}\n    z=x-\\alpha Lx=(I-\\alpha L)x\\nonumber,\n\\end{equation}",
            "eq:19": "\\begin{equation}\n\\begin{aligned}\n    \\min_{F\\in \\mathbb{R}^{N\\times K}}\\mathcal{L}_{mf}(F) &=||(I-\\alpha L)-FF^{T}||_{F}^{2}\\\\\n    &=((1-\\alpha)I+\\Sigma_{i,j}(\\frac{\\alpha w_{x_{i},x_{j}}}{\\sqrt{w_{x_{i}}}\\sqrt{w_{x_{j}}}}-f_{\\theta}(x_{i})^{T}f_{\\theta}(x_{j})))^{2},\n    \\end{aligned}\n\\end{equation}"
        }
    }
}