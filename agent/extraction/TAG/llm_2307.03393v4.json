{
    "meta_info": {
        "title": "Exploring the Potential of Large Language Models (LLMs) in Learning on  Graphs",
        "abstract": "Learning on Graphs has attracted immense attention due to its wide real-world\napplications. The most popular pipeline for learning on graphs with textual\nnode attributes primarily relies on Graph Neural Networks (GNNs), and utilizes\nshallow text embedding as initial node representations, which has limitations\nin general knowledge and profound semantic understanding. In recent years,\nLarge Language Models (LLMs) have been proven to possess extensive common\nknowledge and powerful semantic comprehension abilities that have\nrevolutionized existing workflows to handle text data. In this paper, we aim to\nexplore the potential of LLMs in graph machine learning, especially the node\nclassification task, and investigate two possible pipelines: LLMs-as-Enhancers\nand LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text\nattributes with their massive knowledge and then generate predictions through\nGNNs. The latter attempts to directly employ LLMs as standalone predictors. We\nconduct comprehensive and systematical studies on these two pipelines under\nvarious settings. From comprehensive empirical results, we make original\nobservations and find new insights that open new possibilities and suggest\npromising directions to leverage LLMs for learning on graphs. Our codes and\ndatasets are available at https://github.com/CurryTang/Graph-LLM.",
        "author": "Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang",
        "link": "http://arxiv.org/abs/2307.03393v4",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "To be appear on SIGKDD Explorations"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{intro}\n\n\n% 1. briefly introduce the current graph domain, why textual graph is important\n% 2. briefly introduce the current methods of applying LM + GNN\n% 3. drawbacks of the current method\n% 4. introduce llm, what can it bring to current pipeline\n% 5. drawback of llm, find best practices of applying lm + gnn\n% 6. combine llm + gun, two possible approaches\n\n\n\\vspace{1em}\nGraphs are ubiquitous in various disciplines \nand applications, encompassing a wide range of \nreal-world scenarios~\\cite{Xia2021GraphLA}. Many of \nthese graphs have nodes that are associated \nwith text attributes, resulting in the \nemergence of text-attributed graphs, such as citation graphs~\\cite{hu2020open, Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008} and product graphs~\\cite{Chiang2019ClusterGCNAE}. For example, in the \\products dataset~\\cite{hu2020open}, each node \nrepresents a product, and its corresponding \ntextual description is treated as the node's attribute. \nThese graphs have seen widespread use across \na myriad of domains, from social network analysis~\\cite{social_network}, information retrieval~\\cite{Zhu2021TextGNNIT}, \nto a diverse range of natural language processing tasks~\\cite{liu-etal-2020-fine, Yao2018GraphCN}. \n%The widespread presence and significant utility of such graphs are evident in their diverse applications, including\n\n\\iffalse\nGraphs are omnipresent in reality, transcending various disciplines and applications. Often, each node within these graphs is associated with distinctive text attributes, thus giving rise to text-attributed graphs. The prevalence and significant utility of such graphs are evident in their diverse applications across \nsocial network analysis~\\cite{social_network}, information retrieval~\\cite{Zhu2021TextGNNIT}, a multitude of natural language processing tasks~\\cite{liu-etal-2020-fine, Yao2018GraphCN}, and so on. Many popular real-world graph datasets also possess raw text attributes, such as citation graphs~\\cite{hu2020open, Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008} and product graphs~\\cite{Chiang2019ClusterGCNAE}. For instance, each node in the \\products dataset~\\cite{hu2020open} represents a product and its corresponding description acts as the node's attributes.\n\\fi\n\n\n\n\nGiven the prevalence of text-attributed graphs (TAGs), we aim to explore how to effectively handle these graphs, with a focus on the node classification task. Intuitively, TAGs provide both node attribute and graph structural information. Thus, it is important to effectively capture both while modeling their interrelated correlation. Graph Neural Networks (GNNs)~\\cite{ma2021deep} have emerged as the de facto technique for handling graph-structured data, often leveraging a message-passing paradigm to effectively capture the graph structure. To encode textual information, conventional pipelines typically make use of non-contextualized shallow embeddings e.g., Bag-of-Words~\\cite{harris1954distributional} and Word2Vec~\\cite{mikolov2013efficient} embeddings, as seen in the common graph benchmark datasets~\\cite{hu2020open, Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008}, where GNNs are subsequently employed to process these embeddings. Recent studies demonstrate that these non-contextualized shallow embeddings suffer from some limitations, such as the inability to capture polysemous words~\\cite{Qiu2020PretrainedMF} and deficiency in semantic information~\\cite{miaschi-dellorletta-2020-contextual, ethayarajh-2019-contextual}, which may lead to sub-optimal performance on downstream tasks. \n\n\n\nCompared to these non-contextualized shallow textual embeddings, large language models (LLMs) present massive context-aware knowledge and superior semantic comprehension capability through the process of pre-training on large-scale text corpora~\\cite{Petroni2019LanguageMA, ethayarajh-2019-contextual}. \nThis knowledge achieved from pre-training has led to a surge of revolutions for downstream NLP tasks~\\cite{Zhao2023ASO}.\nExemplars such as ChatGPT and GPT4~\\cite{OpenAI2023GPT4TR}, equipped with hundreds of billions of parameters, exhibit superior performance~\\cite{Bubeck2023SparksOA} on numerous text-related tasks from various domains. Considering the exceptional ability of these LLMs to process and understand textual data, a pertinent question arises: (1) \\textit{Can we leverage the knowledge of LLMs to compensate for the deficiency of contextualized knowledge and semantic comprehension inherent in the conventional GNN pipelines?}\n% \\hz{\"the deficiency of commonsense knowledge inherent in the conventional GNN pipelines\" is too sudden here. You need to explicitly point out this issue in the previous section}\n% \\haitao{Despite the common knowledge learned via pre-training, recent studies find reasoning abilit xxxx, for instance, KBQA, the reasoning ability, especially on graph, is essential. We then ask second question, whether LLM can directly predict on graph structure?} \nIn addition to the knowledge learned via pre-training, recent studies suggest that LLMs present preliminary success on tasks with implicit graph structures such as recommendation \\cite{liu2023chatgpt_rec, Gao2023ChatRECTI}, ranking \\cite{Ji2023ExploringCA}, and multi-hop reasoning \\cite{creswell2023selectioninference}, in which LLMs are adopted to make the final predictions. Given such success, we further question: (2) \\textit{Can LLMs, beyond merely integrating with GNNs, independently perform predictive tasks with explicit graph structures? } In this paper, we aim to embark upon a preliminary investigation of these two  questions by undertaking a series of extensive empirical analyses. Particularly, the key challenge is how to design an LLM-compatible pipeline for graph learning tasks. Consequently, we explore two potential pipelines to incorporate LLMs: (1) \\textit{LLMs-as-Enhancers}: LLMs are adopted to enhance the textual information; subsequently, GNNs utilize refined textual data to generate predictions. (2) \\textit{LLMs-as-Predictors}: LLMs are adapted to generate the final predictions, where structural and attribute information is present completely through natural languages.  \n\n\n\nIn this work, we embrace the challenges and opportunities to study the utilization of LLMs in graph-related problems and aim to deepen our understanding of \\textit{the potential of LLMs on graph machine learning}, with a focus on the node classification task. \\textbf{First}, we aim to investigate how LLMs can enhance GNNs by leveraging their extensive knowledge and semantic comprehension capability. It is evident that different types of LLMs possess varying levels of capability, and more powerful models often come with more usage restrictions~\\cite{sun2022black, Zhao2023ASO, Qiu2020PretrainedMF}. Therefore, we strive to design different strategies tailored to different types of models, and better leverage their capabilities within the constraints of these usage limitations. \\textbf{Second}, we want to explore how LLMs can be adapted to explicit graph structures as a predictor. A principal challenge lies in crafting a prompt that enables the LLMs to effectively use structural and attribute information. To address this challenge, we attempt to explore what information can assist LLMs in better understanding and utilizing graph structures. Through these investigations, we make some insightful observations and gain a better understanding of the capabilities of LLMs in graph machine learning.   \n\n\n\n\n\n\\textbf{Contributions.} Our contributions are summarized as follows:\n% \\haitao{the logic among contribution is not so clear for me. }\n\\begin{compactenum}[1.]\n\\item We explore two pipelines that incorporate LLMs to handle TAGs: \\textit{LLMs-as-Enhancers} and \\textit{LLMs-as-Predictors}. The first pipeline treats the LLMs as attribute enhancers, seamlessly integrating them with GNNs. The second pipeline directly employs the LLMs to generate predictions.\n% \\item We introduce two pipelines for incorporating LLMs into graph learning problems, i.e., \\textit{LLM-as-Enhancer} and \\textit{LLM-as-Predictor}. The first pipeline views LLM as an attribute enhancer to integrate with GNN, while the second pipeline directly utilize LLM to generate predictions.\n\n% These pipelines provide guidelines for integrating LLMs into the graph domain. \n% \\haitao{Maybe add a small description?}\n% \\item We conduct a series of experiments to compare different combinations of GNNs and text embeddings \\haitao{only text embedding? it seems to be a subset of the LLM-as-Enhancer} generated by LLMs. These comparisons yield insights into the most effective approaches for augmenting text attributes with LLMs. \n% \\item In terms of \\textit{LLMs-as-Enhancers}, we present \\jt{feature or text-level are not defined, we may just say two strategies for two types of LLMs}feature-level and text-level enhancements for embedding-visible and embedding-invisible LLMs. We further conduct a series of experiments to compare the effectiveness of these enhancements. \n\\item For \\textit{LLMs-as-Enhancers}, we introduce two strategies to enhance text attributes via LLMs. We further conduct a series of experiments to compare the effectiveness of these enhancements. \n\\item For \\textit{LLMs-as-Predictors}, we design a series of experiments to explore LLMs' capability in utilizing structural and attribute information. From empirical results, we summarize some original observations and provide new insights.  \n\n\\end{compactenum} \n\n\n% \\hz{Remove this paragraph. The intro is too long.}\n% \\jt{please update the organization based on our new structure.}\n\n\\textbf{Key Insights.} Through comprehensive empirical evaluations, we find the following key insights: \n\\begin{compactenum}[1.]\n\\item For \\textit{LLMs-as-Enhancers}, using deep sentence embedding \nmodels to generate embeddings for node \nattributes show both effectiveness and efficiency.\n\n\n\\item  For \\textit{LLMs-as-Enhancers}, utilizing LLMs to augment node attributes at the text level also leads to improvements in downstream performance. \n\n\n\\item  For \\textit{LLMs-as-Predictors}, LLMs present preliminary effectiveness but we should be careful about their inaccurate predictions and the potential test data leakage problem. \n\n\\item LLMs demonstrate the potential to serve as good annotators for labeling nodes, as a decent portion of their annotations is accurate.  \n\\end{compactenum}\n\n\n\n\\textbf{Organization.} The remaining of this paper is organized as follows.  Section~\\ref{sec:prl} introduces necessary preliminary knowledge and notations used in this paper. Section~\\ref{sec: pipeline} introduces two pipelines to leverage LLMs under the task of node classification.  Section~\\ref{sec:enh} explores the first pipeline, \\textit{LLMs-as-Enhancers}, which adopts LLMs to enhance text attributes. Section~\\ref{sec:pred} details the second pipeline, \\textit{LLMs-as-Predictors}, exploring the potential for directly applying LLMs to solve graph learning problems as a predictor. Section~\\ref{sec:rw} discusses works relevant to the applications of LLMs in the graph domain.  Section~\\ref{sec:fut} summarizes our insights and discusses the limitations of our study and the potential directions of LLMs in the graph domain. \n% Finally, Section~\\ref{sec:conclusion} offers a conclusion to this paper.\n\n\n\n\n\n\n\n\n% \\input{chapters/introduction_llm}\n\n",
                "category": [
                    "introduction"
                ]
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\label{sec:prl}\n% \\jt{put this section before related work. It leaves flexibility for us to present notations, settings, notations, definitions before any discussions?? we have sufficient space, so please use \"definition\" macro to define all terms we will use in the work. it is also a good place to present the figure to show our settings.}\n% \\haitao{Do not know whether here or somewhere else, we need to add the description on the dataset, typically, what does the label means, also the textural feature?}\n\\vspace{1em}\nIn this section, we present concepts, notations and problem settings used in the work. We primarily delve into the node classification task on the text-attributed graphs, which is one of the most important downstream tasks in the graph learning domain. Next, we first give the definition of text-attributed graphs. \n% \\haitao{the following sentence may not be neccerary?}\n% Moreover, we only consider \\textbf{undirected graph} in this paper.\n\n% \\jt{let us following the tradition for notations like we use bold upper case for matrix....., node uses $v$...index uses $i,j,k...$}\n\n\\noindent{}\\textbf{Text-Attributed Graphs}\nA text-attributed graph (TAG) $\\mathcal{G}_{S}$ is defined as a structure consisting of nodes $\\mathcal{V}$ and their corresponding adjacency matrix $\\mathbf{A} \\in \\mathbb{R}^{|V|\\times |V|}$. For each node $v_{i} \\in \\mathcal{V}$, it is associated with a text attribute, denoted as $\\mathbf{s}_i$. \n\n\nIn this study, we focus on node classification, which is one of the most commonly adopted graph-related tasks. \n\n\\noindent{}\\textbf{Node Classification on TAGs}\nGiven a set of labeled nodes $\\mathcal{L} \\subset \\mathcal{V}$ with their labels $y_{\\mathcal{L}}$, we aim to predict the labels $\\mathbf{y}_\\mathcal{U}$ for the remaining unlabeled nodes $\\mathcal{U} = \\mathcal{V} \\: \\backslash \\: \\mathcal{L}$.  \n\n\nWe use the citation network dataset \\arxiv~\\cite{hu2020open} as an illustrative example. In such a graph, each node represents an individual paper from the computer science subcategory, with the attribute of the node embodying the paper's title and abstracts. The edges denote the citation relationships. The task is to classify the papers into their corresponding categories, for example, ``cs.cv'' (i.e., computer vision). Next, we introduce the models adopted in this study, including graph neural networks and large language models. \n\n% \\noident\\textbf[Graph Neural Networks]\n\\noindent{}\\textbf{Graph Neural Networks.}\n    When applied to TAGs for node classification, Graph Neural Networks (GNNs) leverage the structural interactions between nodes. Given initial node features $h_{i}^{0}$, GNNs update the representation of each node by aggregating the information from neighboring nodes in a message-passing manner~\\cite{Gilmer2017NeuralMP}. The $l$-th layer can be formulated as:\n\\begin{equation}\n    h_i^{l}=\\operatorname{UPD}^{l}\\left(h_i^{l-1}, \\operatorname{AGG}_{j \\in \\mathcal{N}(i)} \\operatorname{MSG}^{l}\\left(h_i^{l-1}, h_j^{l-1}\\right)\\right), \n\\end{equation}\n% \\cancel{where $\\bigoplus$ denotes a differentiable, permutation-invariant function such as summation. $\\gamma$ and $\\phi$ represent some differentiable functions such as MLPs.} \nwhere $\\operatorname{AGG}$ is often an aggregation function such as summation, or maximum. $\\operatorname{UPD}$ and $\\operatorname{MSG}$ are usually some differentiable functions, such as MLP.  The final hidden representations can be passed through a fully connected layer to make classification predictions. \n% \n\n\n\n\\noindent\\textbf{Large Language Models.}\n% \\haitao{hard to follow the first sentence}\n% A Language Model (LM) is a form of probabilistic model that forecasts the succeeding token in a sequence, given the preceding tokens. \nIn this work, we primarily utilize the term \"large language models\" (LLMs) to denote language models that have been pre-trained on extensive text corpora. Despite the diversity of pre-training objectives~\\cite{bert, radford2019language, 2020t5}, the shared goal of these LLMs is to harness the knowledge acquired during the pre-training phase and repurpose it for a range of downstream tasks. Based on their interfaces, specifically considering whether their embeddings are accessible to users or not, in this work we roughly classify LLMs as below:\n\n\\noindent{}\\textbf{Embedding-visible LLMs}\n    Embedding-visible LLMs provide access to their embeddings, allowing users to interact with and manipulate the underlying language representations. Embedding-visible LLMs enable users to extract embeddings for specific words, sentences, or documents, and perform various natural language processing tasks using those embeddings. Examples of embedding-visible LLMs include BERT~\\cite{bert}, Sentence-BERT~\\cite{sbert}, and Deberta~\\cite{he2020deberta}.\n\n\n\\noindent{}\\textbf{Embedding-invisible LLMs}\n    Embedding-invisible LLMs do not provide direct access to their embeddings or allow users to manipulate the underlying language representations. Instead, they are typically deployed as web services~\\cite{sun2022black} and offer restricted interfaces. For instance, ChatGPT~\\cite{OpenAI2022}, along with its API, solely provides a text-based interface. Users can only engage with these LLMs through text interactions.\n\n\nIn addition to the interfaces, the size, capability, and model structure are crucial factors in determining how LLMs can be leveraged for graphs. Consequently, we take into account the following four types of LLMs: \n\n\\begin{compactenum}[1.]\n    \\item \\textbf{\\textit{Pre-trained Language Models:}} We use the term \"pre-trained language models\" (PLMs) to refer to those relatively small large language models, such as Bert~\\cite{bert} and Deberta~\\cite{he2020deberta}, which can be fine-tuned for downstream tasks. It should be noted that strictly speaking, all LLMs can be viewed as PLMs. Here we adopt the commonly used terminology for models like BERT~\\cite{Qiu2020PretrainedMF} to distinguish them from other LLMs follwing the convention in a recent paper~\\cite{Zhao2023ASO}. \n    \\item \\textbf{\\textit{Deep Sentence Embedding Models:}} These models typically use PLMs as the base encoders and adopt the bi-encoder structure~\\cite{sbert, wang2022text, Neelakantan2022TextAC}. They further pre-train the models in a supervised~\\cite{sbert} or contrastive manner~\\cite{wang2022text, Neelakantan2022TextAC}. In most cases, there is no need for these models to conduct additional fine-tuning for downstream tasks. These models can be further categorized into \\textit{local sentence embedding models} and \\textit{online sentence embedding models}. \\textit{Local sentence embedding models} are open-source and can be accessed locally, with Sentence-BERT (SBERT) being an example. On the other hand, \\textit{online sentence embedding models} are closed-source and deployed as services, with OpenAI's text-ada-embedding-002~\\cite{Neelakantan2022TextAC} being an example. \n    \\item \\textbf{\\textit{Large Language Models:}} Compared to PLMs, Large Language Models (LLMs) exhibit significantly enhanced capabilities with orders of magnitude more parameters. LLMs can be categorized into two types. The first type consists of open-source LLMs, which can be deployed locally, providing users with transparent access to the models' parameters and embeddings. However, the substantial size of these models poses a challenge, as fine-tuning them can be quite cumbersome. One representative example of an open-source LLM is LLaMA~\\cite{llama}.\n    The second type of LLMs is typically deployed as services~\\cite{sun2022black}, with  restrictions placed on user interfaces. In this case, users are unable to access the  model parameters, embeddings, or logits directly.  The most powerful LLMs such as  ChatGPT~\\cite{OpenAI2022} and GPT4~\\cite{OpenAI2023GPT4TR} belong to this kind.\n    \n   \n\\end{compactenum}\n\n\n \nAmong the four types of LLMs,  PLMs, deep sentence embedding models, and open-source LLMs are often embedding-visible LLMs. Closed-source LLMs are embedding-invisible LLMs. \n\n\n\n\n\n\n% \\haitao{I think we can have a figure here}\n",
                "category": [
                    "preliminary"
                ]
            },
            "section 3": {
                "name": "Pipelines for LLMs in Graphs",
                "content": " \n\\label{sec: pipeline}\n\\vspace{1em}\n\n\n\n\n\n% which try to leverage the knowledge of LLMs to enhance text attributes\n\n\nGiven the superior power of LLMs in understanding textual information, we now investigate different strategies to leverage LLMs for node classification in textual graphs. Specifically,  we present two distinct pipelines:  \\textit{LLMs-as-Enhancers} and \\textit{LLMs-as-Predictors}. Figure~\\ref{pipeline} provides figurative illustrations of these two pipelines, and we elaborate on their details as follows.\n% \\begin{compactenum}[1.]\n%     \\item \\textit{LLMs-as-Enhancers:} In this pipeline, \n% \\end{compactenum}\n\n\n\n\n\\noindent{}{\\textbf{\\textit{LLMs-as-Enhancers}}} In this pipeline, LLMs are leveraged to enhance the text attributes. As shown in Figure~\\ref{pipeline}, for \\textit{LLMs-as-Enhancers}, LLMs are adopted to pre-process the text attributes, and then GNNs are trained on the enhanced attributes as the predictors. Considering different structures of LLMs, we conduct enhancements either at the \\textbf{feature level} or at the \\textbf{text level} as shown in Figure~\\ref{llm_pipeline}. \n\\begin{compactenum}[1.]\n    \\item \\textit{\\textbf{Feature-level enhancement:}} For feature-level enhancement, embedding-visible LLMs inject their knowledge by simply encoding the text attribute $s_{i}$ into text embeddings $h_{i} \\in R^{d}$. We investigate two feasible \\textbf{integrating structures} for feature-level enhancement. \\textbf{(1) Cascading structure:} Embedding-visible LLMs and GNNs are combined sequentially. Embedding-visible LLMs first encode text attributes into text features, which are then adopted as the initial node features for GNNs. \\textbf{(2) Iterative structure~\\cite{GLEM}:} PLMs and GNNs are co-trained together by generating pseudo labels for each other. Only PLMs are suitable for this structure since it involves fine-tuning. \n    \\item \\textit{\\textbf{Text-level enhancement:}} For text-level enhancement, given the text attribute $s_{i}$, LLMs will first transform the text attribute into augmented attribute $s_{i}^{Aug}$. Enhanced attributes will then be encoded into enhanced node features $h_{i}^{Aug} \\in R^{d}$ through embedding-visible LLMs. GNNs will make predictions by ensembling the original node features and augmented node features.\n\\end{compactenum}\n\n\n\n\\noindent{}{\\textbf{\\textit{LLMs-as-Predictors}}} In this pipeline, LLMs are leveraged to directly make predictions for the node classification task. As shown in Figure~\\ref{llm_single}, for \\textit{LLMs-as-Predictors}, the first step is to  design prompts to represent graph structural information, text attributes, and label information with texts. Then, embedding-invisible LLMs make predictions based on the information embedded in the prompts. \n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n",
                "category": [
                    "method"
                ]
            },
            "section 4": {
                "name": "LLMs as the Enhancers",
                "content": "\n\\label{sec:enh}\n\n\n\\vspace{1em}\nIn this section, we investigate the potential of employing LLMs to enrich the text attributes of nodes. As presented in Section~\\ref{sec: pipeline}, we consider \\textbf{\\textit{feature-level enhancement}}, which injects LLMs' knowledge by encoding text attributes into features. Moreover, we consider \\textbf{\\textit{text-level enhancement}}, which inject LLMs' knowledge by augmenting the text attributes at the text level. We first study \\textbf{\\textit{feature-level enhancement}}. \n\n\n\n",
                "subsection 4.1": {
                    "name": "Feature-level Enhancement",
                    "content": " \n\\vspace{1.5em}\nIn \\textit{feature-level enhancement}, we mainly study how to combine embedding-visible LLMs with GNNs at the feature level. The embedding generated by LLMs will be adopted as the initial features of GNNs. We first briefly introduce the dataset and dataset split settings we use. \n\n\n\\noindent{}\\textbf{Datasets.} In this study, we adopt \\cora~\\cite{McCallum2000AutomatingTC}, \\pubmed~\\cite{Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008}, \\arxiv, and \\products~\\cite{hu2020open}, four popular benchmarks for node classification. We present their detailed statistics and descriptions in Appendix~\\ref{app: real-world}. Specifically, we examine two classification dataset split settings, specifically tailored for the \\cora and \\pubmed datasets. Meanwhile, for \\arxiv and \\products, we adopt the official dataset splits.\n(1) For \\cora and \\pubmed, the first splitting setting addresses \\textbf{low-labeling-rate} conditions, which is a commonly adopted setting~\\cite{Yang2016RevisitingSL}. To elaborate, we randomly select 20 nodes from each class to form the training set. Then, 500 nodes are chosen for the validation set, while 1000 additional random nodes from the remaining pool are used for the test set.  \n(2) The second splitting setting caters to \\textbf{high-labeling-rate} scenarios, which is also a commonly used setting, and also adopted by TAPE~\\cite{he2023explanations}. In this setting, 60\\% of the nodes are designated for the training set, 20\\% for the validation set, and the remaining 20\\% are set aside for the test set. We take the output of GNNs and compare it with the ground truth of the dataset. We conduct all the experiments on 10 different seeds and report both average accuracy and variance. \n\n\\noindent{}\\textbf{Baseline Models.} In our exploration of how LLMs augment node attributes at the feature level, we consider three main components: (1) \\textit{Selection of GNNs}, (2) \\textit{Selection of LLMs}, and (3) \\textit{Intergrating structures for LLMs and GNNs}. In this study, we choose the most representative models for each component, and the details are listed below. \n\n\\begin{compactenum}[1.]\n    \\item \\textit{Selection of GNNs:} For GNNs on \\cora and \\pubmed, we consider Graph Convolutional Network (GCN)~\\cite{kipf2017semisupervised} and Graph Attention Network~(GAT)~\\cite{veli\u010dkovi\u01072018graph}. We also include the performance of MLP to \\textbf{evaluate the quality of text embeddings without aggregations}. For \\arxiv, we consider GCN, MLP, and a better-performed GNN model RevGAT~\\cite{li2021training}. For \\products, we consider GraphSAGE~\\cite{hamilton2017inductive} which supports neighborhood sampling for large graphs, MLP, and a state-of-the-art model SAGN~\\cite{sun2021scalable}. For RevGAT and SAGN, we adopt all tricks utilized in the OGB leaderboard~\\cite{hu2020open}\\footnote{\\url{https://ogb.stanford.edu/docs/leader_nodeprop/}}.\n\n    \\item \\textit{Selection of LLMs:} \n    To enhance the text attributes at the feature level, we specifically require embedding-visible LLMs. Specifically, we select (1) \\textbf{Fixed PLM/LLMs without fine-tuning:} We consider Deberta~\\cite{he2020deberta} and LLaMA~\\cite{llama}. The first one is adapted from  GLEM~\\cite{GLEM} and we follow the setting of GLEM~\\cite{GLEM} to adopt the [CLS] token of PLMs as the text embeddings. LLaMA is a widely adopted open-source LLM, which has also been included in Langchain\\footnote{\\url{https://python.langchain.com/}}. We adopt LLaMA-cpp\\footnote{\\url{https://github.com/ggerganov/llama.cpp}}, which adopt the [EOS] token as text embeddings in our experiments. (2) \\textbf{Local sentence embedding models}: We adopt Sentence-BERT ~\\cite{sbert} and e5-large~\\cite{wang2022text}. The former is one of the most popular lightweight deep text embedding models while the latter is the state-of-the-art model on the MTEB leaderboard~\\cite{muennighoff-etal-2023-mteb}. (3) \\textbf{Online sentence embedding models}: We consider two online sentence embedding models, i.e., text-ada-embedding-002~\\cite{Neelakantan2022TextAC} from OpenAI, and Palm-Cortex-001~\\cite{anil2023palm} from Google. Although the strategy to train these models has been discussed~\\cite{anil2023palm, Neelakantan2022TextAC}, their detailed parameters are not known to the public, together with their capability on node classification tasks. (4) \\textbf{Fine-tuned PLMs}: We consider fine-tuning Deberta on the downstream dataset, and also adopt the last hidden states of PLMs as the text embeddings.  For fine-tuning, we consider two integrating structures below. \n\n    \\item \\textit{Integration structures:} We consider \\textbf{cascading structure} and \\textbf{iterative structure}. (1) \\textbf{Cascading structure:} we first fine-tune the PLMs on the downstream dataset. Subsequently, the text embeddings engendered by the fine-tuned PLM are employed as the initial node features for GNNs. (2) \\textbf{Iterative structure:}  PLMs and GNNs are first trained separately and  further co-trained in an iterative manner by generating pseudo labels for each other. This grants us the flexibility to choose either the final iteration of PLMs or GNNs as the predictive models, which are denoted as ``GLEM-LM'' and ``GLEM-GNN'', respectively.\n\\end{compactenum}\n\nWe also consider non-contextualized shallow embeddings~\\cite{miaschi-dellorletta-2020-contextual} including TF-IDF and Word2vec~\\cite{hu2020open} as a comparison. TF-IDF is adopted to process the original text attributes for \\pubmed~\\cite{Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008}, and Word2vec is utilized to encode the original text attributes for \\arxiv~\\cite{hu2020open}. For \\arxiv and \\products, we also consider the GIANT features~\\cite{GIANT}, which can not be directly applied to \\cora and \\pubmed because of its special pre-training strategy. Furthermore, we don't include LLaMA for \\arxiv and \\products because it imposes an excessive computational burden when dealing with large-scale datasets.\n\nThe results are shown in Table~\\ref{exp:small1}, Table~\\ref{exp:small2}, and Table~\\ref{exp: ogb}. In these tables, we demonstrate the performance of different combinations of text encoders and GNNs. We also include the performance of MLPs which can suggest the original quality of the textual embeddings before the aggregation. Moreover, We use colors to show the top 3 best LLMs under each GNN (or MLP) model. Specifically, We use \\textcolor{yellow}{yellow} to denote the best one under a specific GNN/MLP model, \\textcolor{green}{green} the second best one, and \\textcolor{pink}{pink} the third best one.\n\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage[normalem]{ulem}\n% \\useunder{\\uline}{\\ul}{}\n\n\n\n\n\n\n\n\n\n\n\n\n%\\czk{!!!Some setting error on seed for fine-tuned PLM here, will solve asap}\n\n\n\n\n\n\n\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Node Classification Performance Comparison",
                        "content": "\n% We make the following observations from results in \\jt{ Table xxx}:\n\n\\vspace{1em}\n\\textbf{\\uline{Observation 1.} Combined with different types of text embeddings, GNNs demonstrate distinct effectiveness.}\n\nFrom Table~\\ref{exp: ogb}, if we compare the performance of TF-IDF and fine-tuned PLM embeddings when MLP is the predictor, we can see that the latter usually achieves much better performance. However, when a GNN model is adopted as the predictor, the performance of TF-IDF embedding is close to and even surpasses the PLM embedding. This result is consistent with the findings in~\\cite{Purchase2022RevisitingEF}, which suggests that GNNs present distinct effectiveness for different types of text embeddings. However, we don't find a simple metric to determine the effectiveness of GNNs on different text embeddings. We will further discuss this limitation in Section~\\ref{sec: limit}.\n\n\\textbf{\\uline{Observation 2.} Fine-tune-based LLMs may fail at low labeling rate settings.}\n\nFrom Table~\\ref{exp:small1}, we note that no matter the cascading structure or the iterative structure, fine-tune-based LLMs' embeddings perform poorly for low labeling rate settings. Both fine-tuned PLM and GLEM present a large gap against deep sentence embedding models and TF-IDF, which do not involve fine-tuning. When training samples are limited, fine-tuning may fail to transfer sufficient knowledge for the downstream tasks. \n\n\n\n\\textbf{\\uline{Observation 3.} With a simple cascading structure, the combination of deep sentence embedding with GNNs makes a strong baseline.}\n\nFrom Table~\\ref{exp:small1}, Table~\\ref{exp:small2}, Table~\\ref{exp: ogb}, we can see that with a simple cascading structure, the combination of deep sentence embedding models (including both local sentence embedding models and online sentence embedding models) with GNNs show competitive performance,  under all dataset split settings. The intriguing aspect is that, during the pre-training stage of these deep sentence embedding models, no structural information is incorporated. Therefore, it is astonishing that these structure-unaware models can outperform GIANT on \\arxiv, which entails a structure-aware self-supervised learning stage. \n\n\n\n\n\n\\textbf{\\uline{Observation 4.} Simply enlarging the model size of LLMs may not help with the node classification performance.}\n\nFrom Table~\\ref{exp:small1} and Table~\\ref{exp:small2}, we can see that although the performance of the embeddings generated by LLaMA outperforms the Deberta-base without fine-tuning by a large margin, there is still a large performance gap between the performance of embeddings generated by deep sentence embedding models in the low labeling rate setting. This result indicates that simply increasing the model size may not be sufficient to generate high-quality embeddings for node classification. The pre-training objective may be an important factor. \n\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Scalability Investigation",
                        "content": " \n\\vspace{1em}\nIn the aforementioned experimental process, we empirically find that in larger datasets like \\arxiv, methods like GLEM that require fine-tuning of the PLMs will take several orders of magnitude more time in the training stage than these that do not require fine-tuning. It presents a hurdle for these approaches to be applied to even larger datasets or scenarios with limited computing resources. To gain a more comprehensive understanding of the efficiency and scalability of different LLMs and integrating structures, we conduct an experiment to measure the running time and memory usage of different approaches. It should be noted that we mainly consider the scalability problem in the training stage,  which is different from the efficiency problem in the inference stage.\n\n\n\nIn this study, we choose representative models from each type of LLMs, and each kind of integrating structure.  For TF-IDF, it's a shallow embedding that doesn't involve either training or inference, so the time and memory complexity of the LM phase can be neglected. In terms of Sentence-BERT, for the LM phase, this kind of local sentence embedding model does not involve a fine-tuning stage, and they only need to generate the initial embeddings. For text-ada-embedding-002, which is offered as an API service, we make API calls to generate embeddings. In this part, we set the batch size of Ada to 1,024 and call the API asynchronously, then we measure the time consumption to generate embeddings as the LM phase running time. For Deberta-base, we record the time used to fine-tune the model and generate the text embeddings as the LM phase running time. For GLEM, since it co-trains the PLM and GNNs, we consider LM phase running time and GNN phase running time together (and show the total training time in the ``LM phase'' column). The efficiency results are shown in Table~\\ref{exp: eff}. We also report the peak memory usage in the table. We adopt the default output dimension of each text encoder, which is shown in the brackets. \n\n\n\n\n\\textbf{\\uline{Observation 5.} For integrating structures,  iterative structure introduces massive computation overhead in the training stage.} \n\n% \\hz{Can we use fine-tunining-based pipeline or something like this to make the observation more general?}\n\nFrom Table~\\ref{exp:small2} and Table~\\ref{exp: ogb}, GLEM presents a superior performance in datasets with an adequate number of labeled training samples, especially in large-scale datasets like \\arxiv and \\products. However, from Table~\\ref{exp: eff}, we can see that it introduces massive computation overhead in the training stage compared to Deberta-base with a cascading structure, which indicates the potential efficiency problem of the iterative structures. \n\n% Moreover, from Table~\\ref{exp: eff}, \\jt{we do not show the inference stage cost, how do we get the following???} we note that \\textbf{(1)} the inference stage of the embedding-visible LLMs, as well as the training and inference stages of the GNNs, require much less time compared to the fine-tuning stage of PLMs. Therefore, the main factor determining the efficiency is whether we perform fine-tuning on downstream datasets. \\textbf{(2)} For the GNN phase, the dimension of node features mainly determines memory usage and time cost.  \n\nMoreover, from Table~\\ref{exp: eff}, we note that for the GNN phase, the dimension of initial node features, which is the default output dimension of text encoders mainly determines memory usage and time cost.  \n\n\\textbf{\\uline{Observation 6.} In terms of different LLM types, deep sentence embedding models present better efficiency in the training stage.}\n\nIn Table~\\ref{exp: eff}, we analyze the efficiency of different types of LLMs by selecting representative models from each category. Comparing fine-tune-based PLMs with deep sentence embedding models, we observe that the latter demonstrates significantly better time efficiency as they do not require a fine-tuning stage. Additionally, deep sentence embedding models exhibit improved memory efficiency as they solely involve the inference stage without the need to store additional information such as gradients.\n\n\n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Text-level Enhancement",
                    "content": "\n\\label{sec: tle}\n\\vspace{1em}\nFor feature-level enhancement, LLMs in the pipeline must be embedding-visible. However, the most powerful LLMs such as ChatGPT~\\cite{OpenAI2022}, PaLM~\\cite{anil2023palm}, and GPT4~\\cite{OpenAI2023GPT4TR} are all deployed as online services~\\cite{sun2022black}, which put strict restrictions so that users can not get access to model parameters and embeddings. Users can only interact with these embedding-invisible LLMs through texts, which means that user inputs must be formatted as texts and LLMs will only yield text outputs.\nIn this section, we explore the potential for these embedding-invisible LLMs to do text-level enhancement. To enhance the text attribute at the text level, the key is to expand more information that is not contained in the original text attributes. Based on this motivation and a recent paper~\\cite{he2023explanations}, we study the following two potential text-level enhancements, and illustrative examples of these two augmentations are shown in Figure~\\ref{fig:textlevel}. \n\n\\begin{compactenum}[1.] \n    \\item \\textbf{TAPE}~\\cite{he2023explanations}: The motivation of TAPE is to leverage the knowledge of LLMs to generate high-quality node features. Specifically, it uses LLMs to generate pseudo labels and explanations. These explanations aim to make the logical relationship between the text features and corresponding labels more clear. For example, given the original attributes ``mean-field approximation\" and the ground truth label ``probabilistic methods'', it will generate a description such as ``mean-field approximation is a widely adopted simplification technique for probabilistic models'', which makes the connection of these two attributes much more clear.  After generating pseudo labels and explanations, they further adopt PLMs to be fine-tuned on both the original text attributes and the explanations generated by LLMs, separately. Next, they generate the corresponding text features and augmented text features based on the original text attributes and augmented text attributes respectively, and finally ensemble them together as the initial node features for GNNs.\n    \\item \\textbf{Knowledge-Enhanced Augmentation}: The motivation behind Knowledge-Enhanced Augmentation (KEA) is to enrich the text attributes by providing additional information. KEA is inspired by knowledge-enhanced PLMs such as ERNIE~\\cite{Sun2019ERNIEER} and K-BERT~\\cite{Liu2019KBERTEL} and aims to explicitly incorporate external knowledge. \n    In KEA, we prompt the LLMs to generate a list of knowledge entities along with their text descriptions. For example, we can generate a description for the abstract term ``Hopf-Rinow theorem'' as follows: ``The Hopf-Rinow theorem establishes that a Riemannian manifold, which is both complete and connected, is geodesically complete if and only if it is simply connected.'' By providing such descriptions, we establish a clearer connection between the theorem and the category ``Riemannian geometry''. Once we obtain the entity list, we encode it either together with the original text attribute or separately. We try encoding text attributes with fine-tuned PLMs and deep sentence embedding models. We also employ ensemble methods to combine these embeddings. One potential advantage of KEA is that it is loosely coupled with the prediction performance of LLMs. In cases where LLMs generate incorrect predictions, TAPE may potentially generate low-quality node features because the explanations provided by PLMs may also be incorrect. However, with KEA, the augmented features may exhibit better stability since we do not rely on explicit predictions from LLMs.\n\\end{compactenum}\n\n\n\n\n\n\n\n",
                    "subsubsection 4.2.1": {
                        "name": "Experimental Setups",
                        "content": " \n\\vspace{1em}\n% \\haitao{Remember to add the API version}\nTo evaluate these two strategies, we conduct experiments on two small datasets \\cora and \\pubmed considering the cost to use the LLMs. For low labeling ratio and high labeling ratio, we adopt the same setting as that in Table~\\ref{exp:small1} and Table~\\ref{exp:small2}. For predictors, we adopt GCN, GAT, and MLP to study both the quality of textual embeddings before and after aggregations. For LLMs, we adopt ChatGPT with the latest version (gpt-3.5-turbo-0613). To better understand the effectiveness of TAPE, we separate it into TA, P, and E, where ``TA'' refers to ``text attributes'', ``P'' refers to ``pseudo labels'', and ``E'' refers to ``explanations''. For KEA, we try two approaches to inject the augmented textual attributes. The first approach is appending the augmented textual attributes into the original attribute, which is denoted as ``KEA-I''. Then the combined attributes are encoded into features. The second approach is to encode the augmented attributes and original attributes separately, which is denoted as ``KEA-S''. We report the results for original, augmented, and ensembling features. Both TAPE and KEA adopt the cascading structures. After encoding the text attributes with LLMs, the generated embeddings are adopted as the initial features for GNNs. We try two approaches to encode the attributes, which are fine-tuned PLMs and local sentence embedding models. Specifically, we adopt Deberta-base and e5-large. To conduct a fair comparison, we first determine the better text encoder by evaluating their overall performance. Once the text encoder is selected, we proceed to compare the performance of the augmented attributes against the original attributes.\n\n% For KEA, we show the results for \\jt{orignal features, contextual features and their ensembling features???} ensembling features and single attributes. Both TAPE and KEA adopt the cascading structures. After encoding the text attributes with LLMs, the generated embeddings are adopted as the initial features for GNN.\\jt{we use different text encoders, is their performance comparable??} For TAPE, fine-tuned PLM is adopted to encode the attribute. For KEA, we adopt sentence transformers to encode the attribute.  The results are shown in Table~\\ref{tab: tape}.\n\n% \\czk{TODO: add more contents to this section to make the presentation better. 1. different text encoders: PLMs / deep sentence embedding models; 2. whether these two can be combined together; 3. change the table look} \n\n\\vspace{0.5em}\n\\noindent{}\\textbf{A comprehensive evaluation of TAPE.} We first gain a deeper understanding of TAPE through a comprehensive ablation study. The experimental results are shown in Table~\\ref{tab:TAPEabla} and Table~\\ref{tab:TAPEabla2}. We show the approach we adopt to encode the text attributes in the bracket. In particular, we mainly consider fine-tuned Deberta-base, which is denoted as PLM, and e5-large, which is denoted as e5. \n\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n% \\usepackage{multirow}\n% \\usepackage[table,xcdraw]{xcolor}\n% If you use beamer only pass \"xcolor=table\" option, i.e. \\documentclass[xcolor=table]{beamer}\n\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n% \\usepackage[normalem]{ulem}\n% \\useunder{\\uline}{\\ul}{}\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n% \\usepackage{multirow}\n% \\usepackage[normalem]{ulem}\n% \\useunder{\\uline}{\\ul}{}\n\n\n\n\\textbf{\\uline{Observation 7.} The effectiveness of TAPE is mainly from the explanations \\textbf{E} generated by LLMs.}  \n\nFrom the ablation study, we can see that compared to pseudo labels \\textbf{P}, the explanations present better stability across different datasets. One main advantage of adopting explanations generated by LLMs  is that these augmented attributes present better performance in the low-labeling rate setting. From Table~\\ref{tab:TAPEabla}, we note that when choosing PLM as the encoders, \\textbf{E} performs much better than \\textbf{TA} in the low labeling rate setting. Compared to explanations, we find that the effectiveness of the P mainly depends on the zero-shot performance of LLMs, which may present large variances across different datasets. In the following analysis, we use \\textbf{TA + E} and neglect the pseudo labels generated by LLMs. \n\n\n\\textbf{\\uline{Observation 8.} Replacing fine-tuned PLMs with deep sentence embedding models can further improve the overall performance of TAPE.}\n\nFrom Table~\\ref{tab:TAPEabla} and Table~\\ref{tab:TAPEabla2}, we observe that adopting e5-large as the LLMs to encode the text attributes can achieve good performance across different datasets and different data splits. Specifically, the \\textbf{TA + E} encoded with e5 can achieve top 3 performance in almost all settings. In the following analysis, we adopt e5 to encode the original and enhanced attributes \\textbf{TA + E}. \n\n\n\n\\paragraph{Effectiveness of \\textbf{KEA}} We then show the results of \\textbf{KEA} in Table~\\ref{tab:kea1} and Table~\\ref{tab:kea2}. For \\textbf{KEA-I}, we inject the description of each technical term directly into the original attribute. For \\textbf{KEA-S}, we encode the generated description and original attribute separately.\n\n\\textbf{\\uline{Observation 9.} The proposed knowledge enhancement attributes \\textbf{KEA} can enhance the performance of the original attribute \\textbf{TA}.} \n\nFrom Table~\\ref{tab:kea1} and Table~\\ref{tab:kea2}, we first compare the performance of features encoded by e5 and PLM. We see that the proposed \\textbf{KEA} is more fitted to the e5 encoder, and fine-tuned PLM embeddings present poor performance on the low labeling rate, thus we also select e5 as the encoder to further compare the quality of attributes. From Table~\\ref{tab: kea4} we can see that the proposed \\textbf{KEA-I + TA} and \\textbf{KEA-S + TA} attributes can consistently outperform the original attributes \\textbf{TA}. \n\n\\textbf{\\uline{Observation 10.} For different datasets, the most effective enhancement methods may vary.}\n\nMoreover, we compare the performance of our proposed \\textbf{KEA} with \\textbf{TA + E}, and the results are shown in Table~\\ref{tab: kea4}. We can see that on \\cora, our methods can achieve better performance while \\textbf{TA + E} can achieve better performance on \\pubmed. One potential explanation for this phenomenon is that \\textbf{TA + E} relies more on the capability of LLMs. Although we have removed the pseudo labels \\textbf{P}, we find that the explanations still contain LLMs' predictions. As a result, the effectiveness of $\\textbf{TA + E}$ will be influenced by LLMs' performance on the dataset. As shown in \\cite{he2023explanations}, the LLMs can achieve superior performance on the \\pubmed dataset but perform poorly on the \\cora dataset. Compared to \\textbf{TA + E}, our proposed \\textbf{KEA} only utilizes the commonsense knowledge of the LLMs, which may have better stability across different datasets.\n\n\n\n% we can see that ensembling the augmented feature KEA-I with the original attribute TA can achieve better performance than the original attribute on datasets and all data splits, using e5 as the encoder (it's only meaningful to compare them under the same encoder, since PLM and e5 present different capabilities). We can also see that using e5 as the encoder, the KEA-I and KEA-S can achieve better overall performance. Moreover, we find that in the low-labeling rate, KEA-S presents better effectiveness, while for the high labeling rate, it's surpassed by KEA-I. \n\n% \\textbf{\\uline{Observation 10.} We can further improve the performance by ensembling TAPE and KEA} \n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n% \\usepackage{multirow}\n% \\usepackage[normalem]{ulem}\n% \\useunder{\\uline}{\\ul}{}\n\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n% \\usepackage{multirow}\n\n\n\n\n\n\n\n\n\n\n% \\haitao{I think this paragraph should be all the following sections as an outline, and discuss the logic between them. either here or in the introduction}\n"
                    }
                },
                "category": [
                    "method",
                    "experiments"
                ]
            },
            "section 5": {
                "name": "LLMs as the Predictors",
                "content": " \n\\label{sec:pred}\n\n\\vspace{1em}\nIn the \\textit{LLMs-as-Enhancers} pipeline, the role of the LLMs remains somewhat limited since we only utilize their pre-trained knowledge but overlook their reasoning capability. Drawing inspiration from the LLMs' proficiency in handling complex tasks with implicit structures, such as logical reasoning~\\cite{creswell2023selectioninference} and recommendation~\\cite{Gao2023ChatRECTI}, we question: \\textbf{Is it possible for the LLM to independently perform predictive tasks on graph structures?} By shifting our focus to node attributes and overlooking the graph structures, we can perceive node classification as a text classification problem. In~\\cite{Sun2023TextCV}, the LLMs demonstrate significant promise, suggesting that they can proficiently process text attributes. However, one key problem is that LLMs are not originally designed to process graph structures. Therefore, it can not directly process structural information like GNNs. \n\nIn this section, we aim to explore the potential of LLMs as a predictor. In particular, we first check whether LLM can perform well without any structural information. Then, we further explore some prompts to incorporate structural information with natural languages. Finally, we show a case study in Section~\\ref{sec: anno} to explore its potential usage as an annotator for graphs. \n\n\n\n",
                "subsection 5.1": {
                    "name": "How Can LLM Perform on Popular Graph Benchmarks without Structural Information?",
                    "content": "\n\\label{sec: tc}\n\\vspace{1em}\n% \\haitao{Better to illustrate that node classification is a structure-enhanced text classification task (structure is auxiliary information)}\n% According to the current prompt design, we find that LLMs struggle to use explicit graph structure relationships to infer node categories. \n% Concurrent work has found that by designing certain prompts, LLMs can achieve good performance in text classification tasks~\\cite{wang2023chatgpt}. Moreover, node classification is inherently a structure-enhanced text classification task, where the structure is auxiliary information to node attributes. \n% Therefore, in this section, we further test \\textit{whether LLMs can be good predictors based on node attributes.} \n% That's not to say we totally ignore the structural information. Since we find that with naive prompt design, LLMs can not effectively process the structures present in natural language, we try to inject \"soft\" structures such as the attribute of neighboring nodes. \nIn this subsection, we treat the node classification problem as a text classification problem by ignoring the structural information. We adopt ChatGPT (gpt-3.5-turbo-0613) as the LLMs to conduct all the experiments. \nWe choose five popular textual graph datasets with raw text attributes: \\cora~\\cite{McCallum2000AutomatingTC}, \\citeseer~\\cite{giles1998citeseer}, \\pubmed~\\cite{Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008}, \\arxiv, and \\products~\\cite{hu2020open}. The details of these datasets can be found in Appendix~\\ref{app: real-world}. Considering the costs to query LLMs' APIs, it's not possible for us to test the whole dataset for these graphs. Considering the rate limit imposed by OpenAI\\footnote{\\url{https://platform.openai.com/docs/guides/rate-limits/overview}}, we randomly select 200 nodes from the test sets as our test data. In order to ensure that these 200 nodes better represent the performance of the entire set, we repeat all experiments twice. Additionally, we employ zero-shot performance as a sanity check, comparing it with the results in TAPE~\\cite{he2023explanations} to ensure minimal discrepancies.\n\nWe explore the following strategies:\n\n\\begin{compactenum}[1.]\n    \\item \\textbf{Zero-shot prompts}: This approach solely involves the attribute of a given node.\n    \\item \\textbf{Few-shot prompts}: On the basis of zero-shot prompts, few-shot prompts provide in-context learning samples together with their labels for LLMs to better understand the task. In addition to the node's content, this approach integrates the content and labels of randomly selected in-context samples from the training set. In the section, we adopt random sampling to select few-shot prompts. \n    \\item \\textbf{Zero-shot prompts with Chain-of-Thoughts (CoT)}: CoT~\\cite{wei2022chain} presents its effectiveness in various reasoning tasks, which can greatly improve LLMs' reasoning abilities. In this study, we test whether CoT can improve LLMs' capability on node classification tasks. On the basis of zero-shot prompts, we guide the LLMs to generate the thought process by using the prompt \"think it step by step\".\n    \\item \\textbf{Few-shot prompts with CoT}: Inspired by~\\cite{Zhang2022AutomaticCO}, which demonstrates that incorporating the CoT process generated by LLMs can further improve LLMs' reasoning capabilities. Building upon the few-shot prompts, this approach enables the LLMs to generate a step-by-step thought process for the in-context samples. Subsequently, the generated CoT processes are inserted into the prompt as auxiliary information. \n\\end{compactenum}\n\n% \\begin{table}[!ht]\n% \\footnotesize\n%     \\centering\n%     \\caption{An illustration of prompts we use for \\textit{LLMs-as-Predictors} without structural information where we use citation data as an example.}\n%     \\label{tab: prompt_pred_nos}\n%     \\begin{tabularx}{\\textwidth}{lXc}\n%     \\toprule\n%         Prompt Name & Prompt Content \\\\  \\midrule\n%         Zero-shot Prompts & \\textbf{Paper:} \\newl \\promptfield{paper content} \\newl \\textbf{Task:} \\newl There are following categories: \\newl \\promptfield{list of categories} \\newl Which category does this paper belong to? \\newl Output the most 1 possible category of this paper as a python list, like ['XX'] \\\\ \\midrule\n%         Few-shot Prompts & \\# Information for the first few-shot samples \\newline \\textbf{Paper:} ... as a python list, like ['XX'] \\newl [\\promptfield{Ground truth 1}] \\newl \u2026(more few shot samples)\u2026 \\newline \\# Information for the current paper \\newline \\textbf{Paper:} ... category of this paper as a python list, like ['XX'] \\\\ \\midrule\n%         Zero-shot prompts with CoT & \\textbf{Paper:} ... category of this paper as a python list, like ['XX'] \\newl Think it step by step and output your reason in one sentence. \\\\ \\midrule\n%         Few-shot prompts with CoT & \\# first use zero-shot cot to generate the reasoning process and get CoT process for each few-shot samples \\newline\n% \\# Information for the first few-shot samples \\newline\n% \\textbf{Paper:} ... \\newl [\\promptfield{Ground truth 1}] \\newl \\promptfield{CoT process 1} \u2026(more few shot samples)\u2026   \\newline\n% \\# Information for this paper  \\newline \\textbf{Paper:} ...Think it step by step and output your reason in one sentence. \\\\ \\bottomrule\n%     \\end{tabularx}\n% \\end{table}\n\n\n\\noindent{}\\textbf{Output Parsing.}\nIn addition, we need a parser to extract the output from LLMs. \nWe devise a straightforward approach to retrieve the predictions from the outputs. \nInitially, we instruct the LLMs to generate the results in a formatted output like ``a python list''.  \nThen, we can use the symbols ``['' and ``]'' to locate the expected outputs. \nIt should be noted that this design aims to extract the information more easily but has little influence on the performance. \nWe observe that sometimes LLMs will output contents that are slightly different from the expected format, for example, output the expected format ``Information Retrieval'' to ``Information Extraction''. \nIn such cases, we compute the edit distance between the extracted output and the category names and select the one with the smallest distance. \nThis method proves effective when the input context is relatively short. \nIf this strategy encounters errors, we resort to extracting the first mentioned categories in the output texts as the predictions. \nIf there's no match, then the model's prediction for the node is incorrect.\n\nTo reduce the variance of LLMs' predictions, we set the temperature to 0. \nFor few-shot cases, we find that providing too much context will cause LLMs to generate outputs that are not compatible with the expected formats. \nTherefore, we set a maximum number of samples to ensure that LLMs generate outputs with valid formats. \nIn this study, we choose this number to $2$ and adopt accuracy as the performance metric. \n\n\n\n\n\n",
                    "subsubsection 5.1.1": {
                        "name": "Observations",
                        "content": "\n\\vspace{1em}\n\n\\textbf{\\uline{Observation 11.} LLMs present preliminary effectiveness on some datasets.}\n\n% \\czk{TODO: compare this with MLP/GCN's performance on the same set of nodes}\n\nAccording to the results in Table~\\ref{tab: llmres1}, it is evident that LLMs demonstrate remarkable zero-shot performance on \\pubmed. When it comes to \\products, LLMs can achieve performance levels comparable to fine-tuned PLMs. However, there is a noticeable performance gap between LLMs and GNNs on \\cora and \\pubmed datasets. To gain a deeper understanding of this observation, it is essential to analyze the output of LLMs. \n\n\n\n\n\\textbf{\\uline{Observation 12.} Wrong predictions made by LLMs are sometimes also reasonable.}\n\nAfter investigating the output of LLMs, we find that a part of the wrong predictions made by LLMs are very reasonable. An example is shown in Table~\\ref{table:reason}. In this example, we can see that besides the ground truth label \"Reinforcement Learning\", \"Neural Networks\" is also a reasonable label, which also appears in the texts. We find that this is a common problem for \\cora, \\citeseer, and \\arxiv. For \\arxiv, there are usually multiple labels for one paper on the website. However, in the \\arxiv dataset, only one of them is chosen as the ground truth. This leads to a misalignment between LLMs' commonsense knowledge and the annotation bias inherent in these datasets. Moreover, we find that introducing few-shot samples presents little help to mitigate the annotation bias. \n\n\n\n\n\n\\textbf{\\uline{Observation 13.} Chain-of-thoughts do not bring in performance gain.}\n\nFor reasoning tasks in the general domain, chain-of-thoughts is believed to be an effective approach to increase LLM's reasoning capability~\\cite{wei2022chain}. However, we find that it's not effective for the node classification task. This phenomenon can be potentially explained by \\textbf{Observation 12}. In contrast to mathematical reasoning, where a single answer is typically expected, multiple reasonable chains of thought can exist for node classification. An example is shown in Table~\\ref{table:cot_wrong}. This phenomenon poses a challenge for LLMs as they may struggle to match the ground truth labels due to the presence of multiple reasonable labels.\n\n\n\n\n\n\n\n\n\n% \\textbf{Observation 3. Chain-of-thought enables better in-context learning}\n\n% From Table~\\ref{tab: real-world}, we can see that chain-of-thought prompts which ask LLMs to generate explanations for its \n\n\n\n\\textbf{\\uline{Observation 14.} For prompts that are very similar in semantics, there may be huge differences in their effects.}\n\nIn addition, we observe that TAPE \\cite{he2023explanations} implements a unique prompt on the \\arxiv dataset, yielding impressive results via zero-shot prompts. The primary distinction between their prompts and ours lies in the label design. Given that all papers originate from the computer science subcategory of Arxiv, they employ the brief term \"arxiv cs subcategories\" as a substitute for these 40 categories. Remarkably, this minor alteration contributes to a substantial enhancement in performance. To delve deeper into this phenomenon, we experiment with three disparate label designs: (1) Strategy 1: the original Arxiv identifier, such as \"arxiv cs.CV\"; (2) Strategy 2: natural language descriptors, like \"computer vision\"; and (3) Strategy 3: the specialized prompt, utilizing \"arxiv cs subcategory\" to denote all categories. Unexpectedly, we discover that Strategy 3 significantly outperforms the other two (refer to Table~\\ref{tab: arxiv}). \n\n\n\nGiven that LLMs undergo pre-training on extensive text corpora, it's likely that these corpora include papers from the Arxiv database. That specific prompt could potentially enhance the ``activation'' of these models' corresponding memory. However, the reason for the excellent results achieved by this kind of prompt might not stem from the simple data memorization of the LLM~\\cite{huang2023can}. When applying to papers after 2023 that are not included in the pre-training corpus of the LLMs, this prompt also achieves similar effectiveness. This phenomenon reminds us that when using ChatGPT, sometimes providing more information in the prompt (such as category information from the \\arxiv dataset) may actually lead to a decrease in performance.\n\n% These observations suggest that the utilization of current textual graph datasets to assess LLMs may inadvertently lead to data leakage issues. Hence, it's crucial to reconsider the methods employed to accurately evaluate the performance of these LLMs on such tasks.\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Incorporating Structural Information in the Prompts",
                    "content": "\n\\label{sec: saware}\n\\vspace{1.5em}\nAs we note, LLMs can already present superior zero-shot performance on some datasets without providing any structural information. However, there is still a large performance gap between LLMs and GNNs in \\cora, \\citeseer, and \\arxiv. \nThen a question naturally raises that \\textit{whether we can further increase LLMs' performance by incorporating structural information?}\nTo answer this problem, we first need to identify how to denote the structural information in the prompt. LLMs such as ChatGPT are not originally designed for graph structures, so they can not process adjacency matrices like GNNs. In this part, we study several ways to convey structural information and test their effectiveness on the \\cora dataset. \n\nSpecifically, we first consider inputting the whole graph into the LLMs. \nUsing \\cora dataset as an example, we try to use prompts like ``node 1: \\prompt{paper content}'' to represent attributes, and prompts like ``node 1 cites node 2'' to represent the edge. \nHowever, we find that this approach is not feasible since LLMs usually present a small input context length restriction. \nAs a result, we consider an ``ego-graph'' view, which refers to the subgraphs induced from the center nodes. In this way, we can narrow the number of nodes to be considered. \n% Considering the cost to use LLMs, to find the best prompt representing graph structures, we try several prompts and compare them on $50$ nodes sampled from the \\cora dataset in Table~\\ref{tab:prompt_comparison}. The motivation of the first two prompts is similar where both aim to represent the edge relationship in an explicit way using the word ``cite''. The difference is that the first prompt uses indexes to refer to different papers while the second prompt uses the title of papers. For prompt $3$, one key part is the generation of neighbor summary. Specifically, we use the prompt in Table~\\ref{table:nsumm} to let LLMs generate a summarization of the current's neighbor attribute and neighbor labels. The motivation of this prompt is to simulate the behavior of GNNs, which also adopt an aggregation function to summarize the neighborhood information. \n\n% \\begin{table}[h!]\n% \\caption {Prompts used to generate a neighbor summary.}\n% \\label{table:nsumm}\n% \\centering\n% \\rule{\\linewidth}{2pt}\n% \\parbox{\\linewidth}{\n% \\vspace{5pt}\n% \\textbf{Prompts used to summarize the neighboring information} \\\\\n% The following list records some papers related to the current one. \\\\\n% \\# Lists of samples neighboring nodes \\\\\n% \\text{[}\\{ \"content\": \"Cadabra a field theory motivated ...\", \"category\": \"computer vision\"... \\}, ...\\text{]}   \\\\\n% \\# \\textbf{Instruction} \\\\ \n% Please summarize the information above with a short paragraph, find some common points which can reflect the category of this paper \n% \\vspace{2pt}\n% }\n% \\rule{\\linewidth}{2pt}\n% \\vspace{5pt}\n% \\end{table}\n\n% \\begin{table}[!ht]\n% \\centering\n% \\caption{Comparison of several different strategies to incorporate structural information in the prompts.}\n% \\label{tab:prompt_comparison}\n% \\resizebox{\\linewidth}{!}{\n% \\begin{tabular}{lc}\n% \\toprule\n% \\textbf{Prompt}                                                                                                                                                                                                                                                                                                                 & \\textbf{Accuracy} \\\\ \\midrule\n% (Baseline zero-shot without structural information)  \\\\ Paper: \\promptfield{paper content} \\\\ Instruction: \\promptfield{Task instruction}                                                                                                                                                                                           & 0.64              \\\\ \\midrule\n% (Prompt $1$) \\\\\n% Instruction: \\promptfield{Task instruction} \\\\ Paper 1: \\promptfield{paper 1 content} \\\\ Paper 2: \\promptfield{paper 2 content} ... \\\\ Paper 1 cites Paper 2, Paper 1 cites Paper 3.... \\\\ The category of Paper 2 is Machine Learning ... The category of Paper 1 is                                                                   & 0.64              \\\\ \\midrule\n% (Prompt $2$) \\\\\n% Instruction: \\promptfield{Task instruction} \\\\ \\promptfield{Paper 1 title}: {paper 1 content} \\\\ \\promptfield{Paper 2 title}: {paper 2 content} ... \\\\ \\promptfield{Paper 2 title} cites \\promptfield{Paper 1 title}, \\promptfield{Paper 1 title} cites \\promptfield{Paper 3 title}.... \\\\ The category of \\promptfield{Paper 2 title} is Machine Learning ... The category of \\promptfield{Paper 1 title} is & 0.64              \\\\ \\midrule\n% (Prompt $3$) \\\\\n% Paper: \\promptfield{paper content} \\\\ Neighbor Summary: \\promptfield{Neighbor summary} \\\\ Instruction: \\promptfield{Task instruction}                                                                                                                                                                                                    & 0.78              \\\\ \\hline\n% \\end{tabular}}\n% \\end{table}\n\n\n\n\n\n\n\n\nSpecifically, we first organize the neighbors of the current nodes as a list of dictionaries consisting of attributes and labels of the neighboring nodes for training nodes. Then, the LLMs summarize the neighborhood information. It should be noted that we only consider 2-hop neighbors because GNNs typically have 2 layers, indicating that the 2-hop neighbor information is the most useful in most cases. Considering the input context limit of LLMs, we empirically find that each time we can summarize the attribute information of 5 neighbors. In this paper, we sample neighbors once and only summarize those selected neighbors. In practice, we can sample multiple times and summarize each of them to obtain more fine-grained neighborhood information. \n\n\n\n\n\n\n\n\n\\textbf{\\uline{Observation 15.} Neighborhood summarization is likely to achieve performance gain.}\n\nFrom Table~\\ref{tab: llmres2}, we note that incorporating neighborhood information in either zero-shot or few-shot approaches yields performance gains compared to the zero-shot prompt without structural information except on the \\pubmed dataset. By following the \"homophily\" assumption~\\cite{homo, mao2023demystifying}, which suggests that neighboring nodes tend to share the same labels, the inclusion of neighboring information can potentially alleviate annotation bias. For instance, let's consider a paper from Arxiv covering general topics like transformers. Merely analyzing the content of this paper makes it difficult to determine which category the author would choose, as categories such as \"Artificial Intelligence,\" \"Machine Learning,\" and \"Computer Vision\" are all plausible options. However, by examining its citation relationships, we can better infer the author's bias. If the paper cites numerous sources from the \"Computer Vision\" domain, it is likely that the author is also a researcher in that field, thereby favoring the selection of this category. Consequently, structural information provides implicit supervision to assist LLMs in capturing the inherent annotation bias in the dataset. However, from the \\pubmed dataset, we observe that incorporating neighborhood information results in clear performance drop, which necessitates a deep analysis below.\n\n\n\\textbf{\\uline{Observation 16.} LLMs with structure prompts may suffer from heterophilous neighboring nodes.}\n\n% \\haitao{More explanation? it seems not so persuasive}\nFrom Table~\\ref{tab: llmres2}, we observe that LLMs perform worse on \\pubmed after incorporating the structural information. To gain a deeper understanding, we focus on those nodes where zero-shot prompts without structural information can lead to correct prediction but prompts with 2-hop information can't.\n\nAn example of this kind of node is shown in Table~\\ref{tab:pubcase1}. \nAfter analyzing the 2-hop neighbors of this node, we find that 15 out of 19 2-hop neighboring nodes have different labels against this node. This case is usually denoted as \"heterophily\"~\\cite{homo}, which is a phenomenon in graph theory where nodes in a graph tend to connect with nodes that are dissimilar to them. In this case, we find that both GNNs and LLMs with a structure-aware prompt make wrong predictions. However, LLMs ignoring structural information get correct predictions, which indicates that LLMs with a structure-aware prompt may also suffer from the \"heterophily\" problem. \n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Case Study: LLMs as the Pseudo Annotators",
                    "content": "\n\\label{sec: anno}\n\\vspace{1.5em}\n\nFrom Table~\\ref{tab: llmres1}, we show that LLMs can be good \\textbf{zero-shot predictors} on several real-world graphs, which provides the possibility to conduct zero-shot inference on datasets without labels. Despite the effectiveness of LLMs, it still presents two problems: (1) The price of using LLMs' API is not cheap, and conducting inference on all testing nodes for large graphs incurs high costs; (2) Whether it is a locally deployed open-source LLM or a closed source LLM accessed through an API, the inference with these LLMs are much slower than GNNs, since the former has high computational resource requirements, while the latter has rate limits. One potential solution to these challenges is leveraging the knowledge of LLMs to train smaller models like GNNs, which inspires a potential application of LLMs to be used as annotators. \n\nBased on the preliminary experimental outcomes, LLMs display encouraging results on certain datasets, thus highlighting their potential for generating high-quality pseudo-labels. However, the use of LLMs as an annotator introduces a new challenge. A key consideration lies in deciding the nodes that should be annotated. Unlike the self-labeling in GNNs\\cite{noisy, DRGST, li2023informative}, where confidence-based or information-based metrics are employed to estimate the quality of pseudo-labels. It remains a difficult task to determine the confidence of pseudo-labels generated by LLMs. Additionally, different nodes within a graph have distinct impacts on other nodes \\cite{Wu2019ActiveLF}. Annotating certain nodes can result in a more significant performance improvement compared to others. Consequently, the primary challenge can be summarized as follows: how can we effectively select both the critical nodes within the graph and the reliable nodes in the context of LLMs?\n\n\\iffalse % \\wei{I am commenting this paragraph to remove strategy 2.}\nTaking into account the complexity of these two challenges, we don't intend to comprehensively address them in this paper. Instead, we present a preliminary study to evaluate the performance of a simple strategy: randomly selecting a subset of nodes for annotation. We explore the effectiveness of two different strategies: (1) Strategy 1: Random selection of unlabeled nodes, and (2) Strategy 2: Equal selection of unlabeled nodes from each category. It should be noted that strategy 2 utilizes information which can't be known in the training stage. We use strategy 2 mainly to test whether class imbalance may influence the final performance.\nRegarding the annotation budget, we adopt a \"low labeling rate\" setting, wherein we either select 20 nodes for each class to be labeled or choose an equivalent number of nodes randomly. For the selected nodes, we adopt 75\\% of them as training nodes and the rest as validation nodes. Consequently, we annotate a total of 140 nodes in the \\cora dataset and 60 nodes in the \\pubmed dataset. In this part, we use GCN as the GNN model and adopt the embeddings generated by the Sentence-BERT model. As a reference, around 67\\% of the pseudo labels for \\cora can match ground truth labels, while around 93\\% of the pseudo labels for \\pubmed are ground truth labels. \n\\fi\n\nTaking into account the complexity of these two challenges, we don't intend to comprehensively address them in this paper. Instead, we present a preliminary study to evaluate the performance of a simple strategy: randomly selecting a subset of nodes for annotation. It is worth noting  that advanced selection strategies such  as active learning~\\cite{Wu2019ActiveLF} could be adopted to improve the final performance. We leave such exploration as future work.\nRegarding the annotation budget, we adopt a \"low labeling rate\" setting, wherein we randomly select a total of 20 nodes multiplied by the number of classes. For the selected nodes, we adopt 75\\% of them as training nodes and the rest as validation nodes. Consequently, we annotate a total of 140 nodes in the \\cora dataset and 60 nodes in the \\pubmed dataset. In this part, we use GCN as the GNN model and adopt the embeddings generated by the Sentence-BERT model. The results are shown in Table~\\ref{tab:anno}. We can observe that training GCN on the pseudo labels can lead to satisfying performance. Particularly, it can match the performance of GCN trained on ground truth labels with 10 shots per class.\nAs a reference, around 67\\% of the pseudo labels for \\cora can match ground truth labels, while around 93\\% of the pseudo labels for \\pubmed are ground truth labels. \n\n\n\n\n\n\n\\textbf{\\uline{Observation 17.} The quality of pseudo labels is key to downstream performance.}\n\nAlthough we don't place significant emphasis on the selection of nodes to be labeled, the preliminary results show that there is relatively little variance among different random selections. Comparing this to the impact of pseudo labels, we observe that the quality of pseudo labels can make a significant difference. When higher quality pseudo labels are used, GNNs perform much better on \\pubmed compared to \\cora. This result highlights the importance of developing an approach to select confident nodes for LLMs.\n\n\\textbf{\\uline{Observation 18.} Getting the confidence by simply prompting the LLMs may not work since they are too ``confident\".}\n\nBased on previous observations, we check some simple strategies to achieve the confidence level of LLMs' outputs.  Initially, we attempt to prompt the LLMs directly for their confidence level. However, we discover that most of the time, LLMs simply output a value of $1$, rendering it meaningless. Examples are shown in Table~\\ref{table:conftry}.\n\n\n\n\n\n\nAnother potential solution is to utilize LLMs that support prediction logits, such as text-davinci-003. However, we observe that the probability of the outputs from these models is consistently close to 1, rendering the output not helpful.\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Case Study: Applying LLMs to handle out-of-distribution data",
                    "content": " \n\\vspace{1.5em}\n\nOut-of-distribution (OOD) learning addresses scenarios where training and test data are drawn from different distributions. Given the ubiquity of distribution shifts in graph data~\\cite{li2022out}, OOD generalization on graphs has emerged as a crucial research direction in recent years. A recent benchmark, GOOD~\\cite{gui2022good}, reveals that existing GNN-based models struggle with robustness when confronted with distributional shifts. In contrast, LLMs have demonstrated commendable robustness on textual data in the presence of OOD scenarios~\\cite{wang2023robustness}. Node classification on the TAG, when disregarding graph structures, can also be considered as a text classification task. Therefore, in this section, we initiate a preliminary exploration into the application of LLMs for OOD scenarios on graphs.\n\n\\textbf{Experimental Setups.} We adopt the GOOD-Arxiv dataset from the GOOD benchmark~\\cite{gui2022good} considering its text attribute availability. Specifically, we adopt all four types of the OOD shift: ``Concept-degree'', ``Covariate-degree'', ``Concept-time'', and ``Covariate-time'' from the GOOD. The final results are shown in Table~\\ref{tab:ood}. We adopt the prompt from TAPE~\\cite{he2023explanations} since it achieves better performance on the \\arxiv dataset. For comparison, we take the best baseline models from the GOOD benchmark. \n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n\n\n\\textbf{\\uline{Observation 19.} LLMs-as-Predictors demonstrate robustness when facing OOD data.}\n\nFrom Table~\\ref{tab:ood}, we find that LLMs-as-Predictors present promising robustness against OOD data. It should be noted that we only try a simple structure-ignorant prompt, and we may further improve the OOD performance of LLMs by selecting proper in-context samples and incorporating structural information. In a nutshell, LLMs present great potential to enhance the OOD generalization capability of graph models. \n"
                },
                "category": [
                    "method",
                    "experiments"
                ]
            },
            "section 6": {
                "name": "Related Work",
                "content": "\n\\label{sec:rw}\n\n\\vspace{1em}\nFollowing our proposed two pipelines, i.e., LLMs as the Enhancers and LLMs as the Predictors, we review existing works in this section. \n\n\n",
                "subsection 6.1": {
                    "name": "LLMs as the Enhancers",
                    "content": "\n\\vspace{1em}\n% \\jt{let us clearly define LLMs as  attribute enhancer, it is the same as LLMs as enhancer in the intro? if we define it in the intro, we can directly use it??}\n% \\haitao{I think you can mention the pipeline correspondingly}\n% \\haitao{cascading structure, this term is very unclear.}\nIn the recent surge of research, increasing attention has been paid on the intersection of LLMs and GNNs in the realm of TAGs~\\cite{GLEM, GIANT, yasunaga2022linkbert, yasunaga2022dragon, Purchase2022RevisitingEF, he2023explanations, Zhu2021TextGNNIT, Hu2020GPTGNNGP, liu2023one, duan2023simteg}. \n Compared to shallow embeddings, LLMs can provide a richer repository of commonsense knowledge, which could potentially enhance the performance of downstream tasks~\\cite{Qiu2020PretrainedMF}.   \n \nSeveral studies employ PLMs as text encoders, transforming text attributes into node features, which can thus be classified as \\fen. The integration structures vary among these works: some adopt a simple cascading structure~\\cite{Purchase2022RevisitingEF, GIANT, yasunaga2022linkbert, liu-etal-2020-fine}, while others opt for an iterative structure~\\cite{GLEM, graphformers, yasunaga2022dragon}. \nFor those utilizing the cascading structure, preliminary investigations have been conducted to determine how the quality of text embeddings affects downstream classification performance~\\cite{Purchase2022RevisitingEF}. \nGIANT~\\cite{GIANT} attempts to incorporate structural information into the pre-training stage of PLMs, achieving improved performance albeit with additional training overhead. \nSimTEG~\\cite{duan2023simteg} suggests that using embeddings obtained through efficiently fine-tuned parameters to replace the original embeddings of pre-trained language models can solve the problem of overfitting during fine-tuning, thereby further enhancing the performance of the cascading structure. \nOneForAll~\\cite{liu2023one} further adopts sentence embedding model to unify the feature space, and propose a unified model for diverse tasks across multiple datasets. \nThis cascading structure has also been successfully applied to tasks such as fact verification~\\cite{liu-etal-2020-fine} and question answering~\\cite{yasunaga2022linkbert}. \nHowever, despite its simplicity, recent studies~\\cite{GLEM} have identified potential drawbacks of the cascading structure. Specifically, it establishes a tenuous connection between the text attribute and the graph. The embeddings generated by the PLMs do not take graph structures into account, and the parameters of the PLMs remain constant during the GNN training process. \nAlternatively, in the iterative structure, Graphformers~\\cite{graphformers} facilitates the co-training of PLMs and GNNs using each other's generated embeddings. GLEM~\\cite{GLEM} takes this a step further by considering pseudo labels generated by both PLMs and GNNs and incorporating them into the optimization process. DRAGON~\\cite{yasunaga2022dragon} successfully extends the iterative structure to the knowledge graph domain.\n\nCompared to these studies focusing on PLMs, a recent stud\\-y~\\cite{he2023explanations} considers the usage of embedding-invisible LLMs such as ChatGPT~\\cite{OpenAI2022} for representation learning on TAGs, which aims to adopt LLMs to enhance the text attributes and thus can be categorized into \\ten. \nThis work introduces a prompt designed to generate explanations for the predictions made by LLMs. These generated explanations are subsequently encoded into augmented features by PLMs. Through the ensemble of these augmented features with the original features, the proposed methodology demonstrates its efficacy and accomplishes state-of-the-art performance on the \\arxiv leaderboard~\\cite{hu2020open}. Nevertheless, the study offers limited analytical insights into the underlying reasons for the success of this approach. Additionally, we have identified a potential concern regarding the prompts utilized in the referenced study.\n\nAnother work pertaining to the integration of LLMs and GNNs is the Graph-Toolformer~\\cite{graph_toolformer}. Drawing inspirations from Toolformer~\\cite{schick2023toolformer}, this study utilizes LLMs as an interface to bridge the natural language commands and GNNs. This approach doesn't change the features and training of GNNs, which is out of our scope.\n\n \n\n\n\n\n\n\n\n"
                },
                "subsection 6.2": {
                    "name": "LLMs as the Predictors",
                    "content": "\n\\vspace{1em}\n\nWhile \\textit{LLMs-as-Enhancers} have proven to be effective, the pipeline still requires GNNs for final predictions. \nIn a significant shift from this approach, recent studies~\\cite{guo2023gpt4graph, graph_natural_language} have begun exploring a unique pipeline that solely relies on LLMs for final predictions. \nThese works fall under the category of \\textit{LLMs-as-Predictors}. \nThe first series of work focus on applying closed-source LLMs without tuning the parameters. \nGPT4Graph~\\cite{guo2023gpt4graph} evaluates the potential of LLMs in executing knowledge graph (KG) reasoning and node classification tasks. Their findings indicate that these models can deliver competitive results for short-range KG reasoning but struggle with long-range KG reasoning and node classification tasks. However, its presentation is pretty vague and they don't give the detailed format of the prompt they use. Considering the publicity of the Arxiv data, the data leakage problem in evaluation is further studied in~\\cite{huang2023can}. \nNLGraph~\\cite{graph_natural_language} introduces a synthetic benchmark to assess graph structure reasoning capabilities. The study primarily concentrates on traditional graph reasoning tasks such as shortest path, maximum flow, and bipartite matching, while only offering limited analysis on node classification tasks. This does not align with our central focus, primarily on graph learning, with a specific emphasis on node classification tasks.\nGraphText~\\cite{zhao2023graphtext} further tries to apply LLMs to a broader range of non-text-attributed graphs by converting the original features into clustering centers or pseudo labels.\nLLM4Dyg~\\cite{zhang2023llm4dyg} further evaluates LLMs' capability for temporal graph-related tasks. LLMGNN~\\cite{chen2023label} and GPT4GNAS~\\cite{wang2023graph} apply LLMs-as-predictors as annotators and agents for neural architecture search, respectively.\n\n\nAs these closed-source LLMs only accept text-type inputs, the first type of methods requires transforming graphs into certain form of natural language, either directly using node attributes or describing the graph structure using natural language.  \nMeanwhile, due to the input length limitations of LLMs, this transformation process often results in the loss of a considerable amount of information from the graph. \nTherefore, the second type of work involves fine-tuning LLMs to enable them to understand graph information represented as embeddings. \nInstructGLM~\\cite{ye2023natural} combines textual instructions with node features in embedding form, enabling LLMs to understand node features through instruction tuning. \nSubsequently, it predicts the type of nodes based on the given instructions. GraphGPT~\\cite{tang2023graphgpt} further introduces cross-modal contrastive learning to align the graph and text feature spaces. \nIt also introduces dual-stage instruction tuning, where the first stage adopts self-supervised instruction tuning to make LLMs better understand graph-structured information. \nThe second stage adopts task-specific fine-tuning to allow LLMs achieve task-specific knowledge and then make predictions. \nGraphLLM~\\cite{chai2023graphllm} and DGTL~\\cite{qin2023disentangled} apply this pipeline to graph reasoning tasks and graph representation learning. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "category": [
                    "related work"
                ]
            },
            "section 7": {
                "name": "Conclusions, Limitations, and Future Directions",
                "content": "\n\\label{sec:fut}\n\\vspace{1em}\n\nIn this section, we summarize our key findings, present the limitations of this study and discuss the potential directions of leveraging LLMs in graph machine learning. \n\n",
                "subsection 7.1": {
                    "name": "Key Findings",
                    "content": " \n\\vspace{1em}\n% \\hz{I think we should add more references (e.g., \"as we discussed in section x.x\" and make the description easier to understand. We should assume that readers will first read the key findings before delving deeper into the content they are interested in.}\nIn this paper, we propose two potential pipelines: \\textit{LLMs-as-Enhancers} and \\textit{LLMs-as-Predictors} that incorporate LLMs to handle the text-attributed graphs. Our rigorous empirical studies reveal several interesting findings which provide new insights for future studies. We highlight some key findings below and more can be found from Observation 1 to Observation 19.\n\n\\textbf{\\uline{Finding 1.} For \\textit{LLMs-as-Enhancers}, deep sentence embedding models present effectiveness in terms of performance and efficiency.} We empirically find that when we adopt deep sentence embedding models as enhancers at the feature level, they present good performance under different dataset split settings, and also scalability. This indicates that they are good candidates to enhance text attributes at the feature level. \n\n% \\haitao{How about other related findings}\n\n\\textbf{\\uline{Finding 2.}  For \\textit{LLMs-as-Enhancers}, the combination of LLMs' augmentations and ensembling demonst\\-rates its effectiveness.} As demonstrated in Section~\\ref{sec: tle}, when LLMs are utilized as enhancers at the text level, we observe performance improvements by ensembling the augmented attributes with the original attributes across datasets and data splits. This suggests a promising approach to enhance the performance of attribute-related tasks. The proposed pipeline involves augmenting the attributes with LLMs and subsequently ensembling the original attributes with the augmented ones. \n\n\\textbf{\\uline{Finding 3.} For \\textit{LLMs-as-Predictors}, LLMs present preliminary effectiveness but also indicate potential evaluation problem.} In Section~\\ref{sec:pred}, we conduct preliminary experiments on applying LLMs as predictors, utilizing both textual attributes and edge relationships. The results demonstrate that LLMs present effectiveness in processing textual attributes and achieving good zero-shot performance on certain datasets. Moreover, our analysis reveals two potential problems within the existing evaluation framework: (1) There are instances where LLMs' inaccurate predictions can also be considered reasonable, particularly in the case of citation datasets where multiple labels may be appropriate. (2) We find a potential test data leakage problem on \\arxiv, which underscores the need for a careful reconsideration of how to appropriately evaluate the performance of LLMs on real-world datasets.\n\n% \\textbf{\\uline{Finding 4.} LLMs also show the potential to serve as good annotators for labeling nodes, which we denote as \\textit{LLMs-as-Annotators}.} \n\n\n"
                },
                "subsection 7.2": {
                    "name": "Limitations",
                    "content": " \n\\label{sec: limit}\n\n\n\\vspace{1em}\n\\noindent{}{\\textbf{A deeper understanding of the effectiveness of text embeddings.}} Despite the effectiveness of deep sentence embedding models, our understanding of why their embeddings outperform PLMs' on node classification tasks remains limited. Furthermore, we observe a performance gap between deep sentence embedding models and GLEM on the \\products dataset, which may be related to the domains of the dataset. Moreover, as shown in Observation 4, GNNs demonstrate different levels of effectiveness on different text embeddings. However, we give limited explanations for this phenomenon. To gain a deeper understanding, we need to have a look at the original feature space and the feature space after aggregation. This phenomenon may potentially be related to the anistrophy in language model embeddings~\\cite{ethayarajh-2019-contextual}. More in-depth analysis is required to better understand these phenomena. \n\n\n\n\\noindent{}{\\textbf{Costs of LLM augmentations.}}\nIn the work, we study TAPE and KEA to enhance the textual attributes at the text level. Although these methods have proven to be effective, they require querying LLMs' APIs at least N times for a graph with N nodes. Given the cost associated with LLMs, this poses a significant expense when dealing with large-scale datasets. Consequently, we have not presented results for the \\arxiv and \\products datasets.\n\n\n\n\n\n\\noindent{}{\\textbf{Text-formatted hand-crafted prompts to represent graphs.}} \nIn Section~\\ref{sec:pred}, we limit our study to the use of ``natural language'' prompts for graph representation. However, various other formats exist for representing graphs in natural language such as XML, YAML, GML, and more~\\cite{Roughan2015UnravellingGF}. Moreover, we mainly design these prompts in a hand-crafted way, which is mainly based on trial and error. It's thus worthwhile to consider exploring more prompt formats and how to come up with automatic prompts. \n\n\n\n\n\n"
                },
                "subsection 7.3": {
                    "name": "Future Directions",
                    "content": "\n\\vspace{1em}\n\\noindent{}{\\textbf{Extending the current pipelines to more tasks and more types of graphs.}} \nIn this study, our primary focus is on investigating the node classification task for text-attributed graphs. Nevertheless, it remains unexplored whe\\-ther these two pipelines can be extended to other graph-learning tasks or other types of graphs. Certain tasks necessitate the utilization of long-range information~\\cite{dwivedi2022long}, and representing such information within LLMs' limited input context poses a significant challenge. Furthermore, we demonstrate that LLMs exhibit promising initial results in graphs containing abundant textual information, particularly in natural language. However, the exploration of their effective extension to other types of graphs with non-natural language information, such as molecular graph~\\cite{Fey2019FastGR, Li2023EmpoweringMD}, still needs further exploration. \n\n% \\noindent{}{LLMs for the graph domain}\n% In this paper, we focus on how to adapt LLMs to graph machine learning tasks through in-context learning. However, the extent to which in-context learning can help LLMs achieve task-specific information is restricted~\\cite{Dong2022ASF}, since the model parameters have not been updated. Recently, some research has begun to explore the use of instruction tuning-based method~\\cite{Wei2021FinetunedLM} to design domain-specific models such as recommendation systems~\\cite{Zhang2023RecommendationAI, Fan2023RecommenderSI}, multi-modality~\\cite{Li2023OtterAM}, and tabular data~\\cite{Slack2023TABLETLF}. These domain-specific models are built upon open-source large models like LLaMA~\\cite{llama} and Flan-T5~\\cite{flant5}. However, as far as we know, there are still no LLMs specifically tuned for the graph domain. How to adapt these tuning-based methods and apply them to the graph domain is thus a promising future direction. \n\n\\noindent{}{\\textbf{Using LLMs more efficiently.}}\nDespite the effectiveness of LLMs, the inherent operational efficiency and operational cost of these models still pose significant challenges. Taking ChatGPT, which is accessed through an API, as an example, the current billing model incurs high costs for processing large-scale graphs. As for locally deployed open-source large models, even just using them for inference requires substantial hardware resources, not to mention training the models with parameter updates. Therefore, developing more efficient strategies to utilize LLMs is currently a challenge.\n\n\\noindent{}{\\textbf{Evaluating LLMs' capability for graph learning tasks.}}\nIn this paper, we briefly talk about the potential pitfalls of the current evaluation framework. There are mainly two problems: (1) the test data may already appear in the training corpus of LLMs, which is referred to as \"contamination\"~\\footnote{\\url{https://hitz-zentroa.github.io/lm-contamination/}} (2) the ground truth labels may present ambiguity, and the performance calculated based on them may not reflect LLMs' genuine capability. For the first problem, one possible mitigation is to use the latest dataset which is not included in the training corpus of LLMs. However, that means we need to keep collecting data and annotating them, which seems not an effective solution. For the second problem, one possible solution is to reconsider the ground truth design. For instance, for the categorization of academic papers, we may adopt a multi-label setting and select all applicable categories as the ground truth. However, for more general tasks, it remains a challenge to design more reasonable ground truths. Generally speaking, it's a valuable future direction to rethink how to properly evaluate LLMs.  \n\n\n\\noindent{}{\\textbf{Aligning the feature space of graph models and LLMs.}}\nCurrently, a major obstacle hindering the wider application of LLMs in the field of graph learning is the discrepancy between the feature space of LLMs and that of graphs. This discrepancy makes it difficult for LLMs to effectively understand information in the graph domain. There are mainly two approaches to address this issue in current work. The first approach is to translate the information on the graph into natural language that LLMs can understand. The second approach involves directly inputting the graph information in the form of embeddings and then using instruction tuning to enable LLMs to understand this information. However, both methods have their evident limitations. For the first method, the translation process can result in information loss, and the inherent input length limitation of LLMs also prevents users from inputting large-scale graphs. For the second method, the introduction of tuning significantly increases computational overhead. Is there a better way to align LLMs with graphs? A recent work targeting multimodality~\\cite{pang2023frozen} has shown new possibilities. It demonstrates that with fixed LLM parameters, only a linear transformation layer is needed to convert information from the visual domain into content that can be effectively processed by LLMs, and such an architecture also holds great potential in the field of graph machine learning.\n\n\n\n\n\n% \\noindent{}{LLMs as annotators for learning on graphs}\n% In this paper, we conduct preliminary experiments on adopting LLMs as annotators. We find that the first challenge lies in how to select high-quality pseudo labels. Recently, some work conducted preliminary research~\\cite{Lin2023GeneratingWC, Xiong2023CanLE} on how to evaluate the uncertainty of ``black-box LLMs''. When applying those methods to the graph domain, we also need to consider the role of nodes in the graph. Specifically, different nodes present different importance in the graph, which means annotating some of them may be more beneficial to the overall performance~\\cite{Wu2019ActiveLF}. It's thus important to study how to find confident nodes of LLMs and important nodes of the graph simultaneously. \n%\n% The following two commands are all you need in the\n% initial runs of your .tex file to\n% produce the bibliography for the citations in your paper.\n\\bibliographystyle{abbrv}\n\\bibliography{references}  % sigproc.bib is the name of the Bibliography in this case\n% You must have a proper \".bib\" file\n%  and remember to run:\n% latex bibtex latex latex\n% to resolve all references\n%\n% ACM needs 'a single self-contained file'!\n%\n%APPENDICES are optional\n% SIGKDD: balancing columns messes up the footers: Sunita Sarawagi, Jan 2000.\n% \\balancecolumns\n% \\appendix\n%Appendix A\n\\appendix\n"
                },
                "category": [
                    "conclusion"
                ]
            },
            "section 8": {
                "name": "Datasets",
                "content": " \\label{app: real-world}\n\n\n\\vspace{1em}\nIn this work, we mainly use the following five real-world graph datasets. Their statistics are shown in Table~\\ref{tab: alldata}.\n\n\n",
                "subsection 8.1": {
                    "name": "Dataset Description",
                    "content": "\n\\vspace{1em}\n\\label{app: datades}\nIn this part, we give a brief introduction to each graph dataset. It should be noted that it's cumbersome to get the raw text attributes for some datasets, and we will elaborate them below. The structural information and label information of these datasets can be achieved from Pyg~\\footnote{\\url{https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html}}.  We will also release the pre-processed versions of these datasets to assist future related studies.\n\n\\noindent{}\\textbf{\\cora~\\cite{McCallum2000AutomatingTC}} \n\\cora is a paper citation dataset with the following seven categories: ['Rule Learning',\n'Neural Networks', 'Case Based', 'Genetic Algorithms', 'Theory', 'Reinforcement Learning', 'Probabilistic Methods']. The raw text attributes can be obtained from \\url{https://people.cs.umass.edu/~mccallum/data.html}. \n\n\\noindent{}\\textbf{\\citeseer~\\cite{giles1998citeseer}}\n\\citeseer is a paper citation dataset with the following seven categories: [\"Agents\", \"ML\", \"IR\", \"DB\", \"HCI\", \"AI\"]. Note that we find that the TAG versopm only contains the text attributes for 3186 nodes. As a result, we take the graph consisted of these 3186 nodes with 4277 edges. \n\n\\noindent{}\\textbf{\\pubmed~\\cite{Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008}}\n\\pubmed is a paper citation dataset consisting scientific journals collected from the PubMed database with the following three categories: ['Diabetes Mellitus, Experimental',\n'Diabetes Mellitus Type 1', 'Diabetes Mellitus Type 2']. \n\n\\noindent{}\\textbf{\\arxiv and \\products~\\cite{hu2020open}}\nThese dataset are selected from the popular OGB benchmark~\\cite{hu2020open}, and descriptions for these datasets can be found in \\url{https://ogb.stanford.edu/docs/nodeprop}. \n\n\n"
                },
                "category": [
                    "experiments"
                ]
            },
            "section 9": {
                "name": "Experiment Setups",
                "content": "\n\\vspace{0.5em}\n",
                "subsection 9.1": {
                    "name": "Computing Environment",
                    "content": "\n\\vspace{1em}\nWe implement all the baseline models with PyG~\\cite{Fey2019FastGR}, DGL~\\cite{Wang2019DeepGL}, and transformers~\\cite{Wolf2019HuggingFacesTS} modules. The experiments were conducted in a GPU server with eight NVIDIA RTX A5000 GPUs, each with 24GB VRAM. \n\n"
                },
                "subsection 9.2": {
                    "name": "Hyperparameters",
                    "content": "\n\\vspace{1em}\nFor RevGAT, GraphSage, and SAGN models, we directly adopt the best hyperparameters from the OGB leaderboard~\\footnote{\\url{https://github.com/snap-stanford/ogb}}. For Deberta-base on \\cora and \\pubmed, we follow the hyperparameter setting of TAPE~\\cite{he2023explanations}. In terms of GLEM, for the LM part, we follow the hyperparameter setting in their reporsitory~\\footnote{\\url{https://github.com/AndyJZhao/GLEM}}. For GCN, GAT, MLP, we use the following hyperparameter search range. \n\n\\begin{compactenum}[(a)]\n\\item \\textbf{Hidden dimension:} \\{8, 16, 32, 64, 128, 256\\}.\n\\item \\textbf{Number of layers:} \\{1, 2, 3\\}\n\\item \\textbf{Normalization:} \\{None, BatchNorm\\};\n\\item \\textbf{Learning rate:} \\{1e-2, 5e-2, 5e-3, 1e-3\\}\n\\item \\textbf{Weight Decay:} \\{1e-5, 5e-5, 5e-4, 0\\}\n\\item \\textbf{Dropout:} \\{0., 0.1, 0.5, 0.8\\}\n\\item \\textbf{Number of heads for GAT:} \\{1, 4, 8\\}\n\\end{compactenum}\n\n"
                },
                "category": [
                    "experiments"
                ]
            },
            "section 10": {
                "name": "Demonstrations of TAPE",
                "content": "\n\\label{app: demo_tk}\n\n\\vspace{1em}\n% In this section, we showcase the detailed prompts used to generate the enhancements for both TAPE~\\cite{he2023explanations} and KEA in Table~\\ref{table:tape_example} and Table~\\ref{table:kea_example}, respectively. For TAPE, after generating the augmented features, the pseudo labels will be extracted into features \\textbf{P}, and the whole output (including the pseudo labels in the first line) will be used as the augmented attributes, and further encoded into features \\textbf{E}. For KEA, after extracting those technical terms, we consider two options: (1) \\textbf{KEA-I}: we insert the description directly into the original attribute after locating these terms. (2) \\textbf{KEA-S}: we encode the original attributes and the augmented attributes separately. For outputs that exceed the maximum length of PLMs, we employ two strategies: truncating the outputs or encoding each subpart and then averaging them. The results indicate that both approaches yield similar performance, with truncation slightly outperforming the encoding method. Consequently, we opt to simply truncate the outputs.\n\n% \\begin{table}[!ht]\n% \\caption {An illustrative example for TAPE on the \\cora dataset where the input and output are directly from the original paper.}\n% \\label{table:tape_example}\n% \\centering\n% \\rule{\\linewidth}{2pt}\n% \\parbox{\\linewidth}{\n% \\vspace{5pt}\n% \\textbf{Input:} \\newline Abstract: \\promptfield{abstract text} \\newline Title: \\promptfield{title text} \\newline Question: Which of the following sub-categories of AI does this paper belong to: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory? If multiple options apply, provide a comma-separated list ordered from most to least related, then for each choice you gave, explain how it is present in the text. \\newline Answer:\n% \\textbf{An example Output:} \\newline\n% Rule Learning, Probabilistic Methods. \\newline Rule Learning is present in the text as the paper discusses an algorithm that searches for features defined by substructures, where each clause represents a binary feature. The algorithm conducts a top-down search for first-order clauses, which is a common approach in rule learning. \\newline Probabilistic Methods are present in the text as the algorithm described in the paper is stochastic, meaning it involves randomness and probability. The paper also mentions that preliminary experiments are favorable, which suggests that the algorithm's performance is evaluated using probabilistic methods.\"\n% \\vspace{2pt}\n% }\n% \\rule{\\linewidth}{2pt}\n% \\vspace{5pt}\n% \\end{table}\n\n\n\n\n\n\n\\noindent{}\\textbf{Examples for \\pubmed} After analyzing the \\pubmed data\\-set, we find an interesting phenomenon that sometimes the label of the paper just appears in the raw text attributes. An example is shown in Table~\\ref{table:pubmed_example}. This property of \\pubmed may be related to the superior zero-shot performance of LLMs on this dataset. This can also potentially explain why GCN and GAT are outperformed by MLP in the high labeling ratio. When the link between node attributes and node labels can be easily found and adequate to determine the categories, incorporating neighbors coming from other categories will introduce noise.  \n\n% \\begin{table}[H]\n% \\caption {An illustrative example for KEA on the \\cora dataset where the input and output are directly from the original paper}\n% \\label{table:kea_example}\n% \\centering\n% \\rule{\\linewidth}{2pt}\n% \\parbox{\\linewidth}{\n% \\vspace{5pt}\n% \\textbf{Input:} You should work like a named entity recognizer. \\newline Paper: \\promptfield{paper content}. Extract the technical terms from this paper and output a description for each terms in the format of a python dict, with the format \\{'XX': 'XXX', 'YY': 'YYY'\\}.  The extracted terms should be relevant to artificial intelligence, machine learning \\newline\n% \\textbf{An example Output:} \\newline\n% \\{'propositionalization': 'A process of converting relational data into propositional data where each instance in the dataset is converted into a set of feature/value pairs.',\\newline 'inductive learning': 'A type of machine learning where a model is trained on a subset of the data and then generalized to new data.' ... \\}\n% \\vspace{2pt}\n% }\n% \\rule{\\linewidth}{2pt}\n% \\vspace{5pt}\n% \\end{table}\n\n% \\jt{for this example, we can add a row to show the label of this node??}\n\n\n\n\n\n\n% \\section{Exploring Prompts from the Whole-graph View}\n% \\label{app: synexp}\n% In Section~\\ref{sec:pred}, we design prompts to represent graphs from the ego-graph view. However, we don't come up with this kind of prompt in the very beginning. Initially, we try to represent the \\textbf{whole graph} instead of the \\textbf{ego graph} with prompts. Since real-world datasets usually contain \n\n% To assess the capability of LLMs in utilizing graph structures, we examine two essential problems. First, is it possible to construct prompts that enable LLMs to internalize and comprehend structural information? Second, once LLMs have internalized the structural information, can they extract valuable patterns and make predictions? To address the first question, we conduct experiments using several prompts to depict the graph structure. It should be noted that in NLGraph~\\cite{graph_natural_language}, the authors also design synthetic experiments to test whether LLMs can process graphs represented with natural languages. However, they mainly consider whether LLMs can simulate the behavior of GNNs on synthetic graphs with attributes. Here, we test whether LLMs can utilize some assumptions and do node classification on synthetic graphs. \n% % \\jt{I remembered in the related work section, people alos use natural language to denote structure, are we the same? if not, why we introduce a new one??}\n% \\begin{compactenum}[1.]\n%     \\item \\textbf{Natural language:} We describe the graph structures using natural language. For instance, ``node 1 has label 1'' signifies the label information, while \\jt{is the graph a directed graph?? if undirected, should we specify in the prompt??}``node 1 is connected to node 2'' indicates the edge relationship. We also try giving a ``name'' to each node or giving edge relationships some real-world semantics, such as friendship. Empirically,  we find that the ``name'' of each node does not affect the performance of the prompt.  \\czk{we focus on undirected graph, TODO: design some explanations for direction}\n%     \\item \\textbf{Code:} Employing code as a representation of information proves effective for the LLMs, enhancing its reasoning capacity~\\cite{wei2022chain}. We utilize Python code to portray the structures. For example, the variable \\textit{y} represents label information, and the variable \\textit{edges} signifies the edge list of the graph. \\\\\n% \\end{compactenum}\n\n% Regarding the second problem, the crux lies in identifying how to instruct the LLMs to leverage the structural information present in the prompt. Typically, graph models are constructed on the basis of specific assumptions, such as \"homophily,\" which postulates that neighboring nodes are likely to have similar labels~\\cite{ma2022is, mao2023demystifying}. Label propagation, a standard graph model designed based on the homophily assumption, assigns labels to unlabeled nodes in a graph by iteratively propagating and harmonizing the labels from their labeled neighbors \\cite{Zhu2002LearningFL}. To gauge the LLMs' proficiency in node classification, we initially verify whether they can utilize the homophily assumption. We put a formal definition of \"homophily\" in Appendix~\\ref{app: homo}. \n\n% % \\haitao{This is confused, a series of synthetic graphs separated into $K$ connected components? or a series of synthetic graphs where each synthetic graph is separated into $K$ connected components}\n\n% \\noindent{}\\textbf{\\textit{Completely Homophilous Graph}}Specifically, we first generate a series of synthetic graphs. Each graph consists of $K$ connected components, and nodes within each connected component share the same label. \n% Given the label of some training nodes, LLMs need to determine the label of test nodes. Within each connected component, we consider either a sparse ring graph or a dense random graph with high clustering generated from the Watts-Strogatz model \\cite{watts1998collective}. It's obvious that by simply assigning the labels of neighboring nodes, we can achieve 100\\% accuracy on these simple graphs. Therefore, this experiment aims to test LLMs' capability to memorize and utilize the simplest patterns. \n% Detailed generation of these synthetic graphs can be found in Appendix~\\ref{app: synthetic}. \n% % \\haitao{It this natural language is often utilized, it could be better that put the general graph description at the front. }\n% % To encode the graph structure as natural language, we designate each node using its index, such as \"node 1\" for the first node. \n% % The term \"connected with\" succinctly signifies edge relations. \n% % For instance, an edge between the first and second nodes would be stated as \"node 1 is connected with node 2\".\n% % To integrate the label information, we use the format \"node X has label Y\", where X denotes the node index and Y refers to the label.\n% % For instance, if node 1 has a label of 1, we express it as \"node 1 has label 1\". As our labeling system only utilizes numeric values, the language model is unable to leverage its inherent common sense understanding. \n% We evaluate the classification accuracy on synthetic graphs of varied sizes and present the results in Table~\\ref{tab: synthetic1}. \n% We only show the results for \"natural language\" prompts since we find that the \"code\" prompt fails in the simplest case.\n% We set the temperature of LLMs to 0 to minimize the variance.\n% Each experiment is repeated for $2$ times. \n% We select nodes with an index that is a multiple of 5 as the test nodes.\n% The detailed input prompt and outputs of LLMs can be found in Appendix~\\ref{app: prompt}. \n% It should be noted that label propagation can achieve $100\\%$ accuracy in all cases.\n\n% % Please add the following required packages to your document preamble:\n% % \\usepackage{booktabs}\n% % \\usepackage{multirow}\n% \\begin{table}[!ht]\n% \\centering\n% \\caption{Performance of LLMs on structure-only node classification, with completely homophilous graphs. We show the correct cases, variances, and total cases.}\n% \\label{tab: synthetic1}\n% \\begin{tabular}{@{}ccccc@{}}\n% \\toprule\n% \\multirow{2}{*}{\\textbf{Random Graph Model}} & \\multirow{2}{*}{\\textbf{Classes}} & \\multicolumn{3}{c}{\\textbf{Size of graphs}}           \\\\ \\cmidrule(l){3-5} \n%                                     &                          & \\textbf{30}        & \\textbf{60}             & \\textbf{120}             \\\\ \\midrule\n% \\multirow{3}{*}{\\textbf{Ring Graph}}         & 2                        & 5 \u00b1 0 / 5 & 11 \u00b1 0 / 11    & 24 \u00b1 0 / 24     \\\\\n%                                     & 3                        & 5 \u00b1 0 / 5 & 11 \u00b1 0 / 11    & 23.5 \u00b1 0.5 / 24 \\\\\n%                                     & 6                        & 5 \u00b1 0 / 5 & 11 \u00b1 0 / 11    & 24 \u00b1 0 / 24     \\\\ \\cmidrule(l){2-5} \n% \\multirow{3}{*}{\\textbf{Watts\u2013Strogatz}}     & 2                        & 5 \u00b1 0 / 5 & 10.5 \u00b1 0.5/ 11 & 24 \u00b1 0 / 24     \\\\\n%                                     & 3                        & 5 \u00b1 0 / 5 & 11 \u00b1 0 / 11    & 23.5 \u00b1 0.5 / 24 \\\\\n%                                     & 6                        & 5 \u00b1 0 / 5 & 11 \u00b1 0 / 11    & 24 \u00b1 0 / 24     \\\\ \\bottomrule\n% \\end{tabular}\n% \\end{table}\n\n\n% % \\begin{table}[htbp]\n% % \\centering\n% % \\footnotesize\n% % \\caption{Performance of LLMs on structure-only node classification, with completely homophilous graphs. We show the correct cases, variances, and total cases.}\n% % \\label{tab: synthetic_graph}\n% % \\begin{tabular}{cc|ccc|}\n% % \\toprule\n% % \\multirow{2}{*}{Random Graph Model}     & \\multirow{2}{*}{Classes} & \\multicolumn{3}{c}{Size of graphs}             \\\\ \\cmidrule{3-5} \n% %                                         &                                    & 30        & 60         & 120          \\\\ \\midrule\n% % \\multirow{3}{*}{Ring Graph}             & 2                                  & 5 \u00b1 0 / 5 & 11 \u00b1 0 / 11 & 24 \u00b1 0 / 24  \\\\ \n% %                                      & 3                                  & 5 \u00b1 0 / 5          & 11 \u00b1 0 / 11          & 23.5 \u00b1 0.5 / 24           \\\\ \n% %                                         & 6                                  & 5 \u00b1 0 / 5          & 11 \u00b1 0 / 11          & 24 \u00b1 0 / 24         \\\\ \\midrule\n% % \\multirow{3}{*}{Watts\u2013Strogatz}         & 2                                  & 5 \u00b1 0 / 5 & 10.5 \u00b1 0.5/ 11  & 24 \u00b1 0 / 24  \\\\  \n% %                                         & 3                                  & 5 \u00b1 0 / 5          &  11 \u00b1 0 / 11        & 23.5 \u00b1 0.5 / 24          \\\\  \n% %                                         & 6                                  & 5 \u00b1 0 / 5          & 11 \u00b1 0 / 11          & 24 \u00b1 0 / 24          \\\\ \\bottomrule\n% % \\end{tabular}\n% % \\end{table}\n\n% \\textbf{\\uline{Observation 17.} LLMs present preliminary capabilities to  utilize simple graph structures present in texts }\n\n\n\n% From Table~\\ref{tab: synthetic1}, we can see that in straightforward cases where all neighboring nodes share identical labels, LLMs can achieve promising performance. That indicates that LLMs present some preliminary capabilities to memorize and utilize the graph structures. \n% % \\jt{From Table xxx}we can see that in straightforward cases where all neighboring nodes share identical labels, LLMs can achieve promising performance. That indicates that LLMs present some preliminary capabilities to memorize and utilize the graph structures. \n\n\n% % \\jt{which observation. I think when we number the observations, we do not need to restart for each study and we can continue to use the same in the whole paper from observation 1, 2, 3, ......10, 11,.... in such way, we can refer to them in the whole paper when we need.}\n\n% \\noindent{}\\textbf{\\textit{Highly Homophilous Graph}}Based on Observation $17$, we further probe the capabilities of the LLMs in handling more complex graph structures. \n% One such example is the Stochastic Block Model (SBM) \\cite{sbm}, a random graph generator, renowned for its flexibility in adjusting the homophily ratio within a graph through the tuning of two parameters - the intra-community probability ($p$) and the inter-community probability ($q$). For our experiment, we generate a series of random graphs each featuring $3$ distinct communities. The intra-community probability ($p$) is held constant at $0.4$, while the inter-community probability ($q$) is varied within the range of $0.025$ to $0.1$. The results of this experiment are demonstrated in Table~\\ref{tab: sbm}. It should be noted that label propagation can achieve $100\\%$ accuracy in all cases.\n\n\n% \\begin{table}[!ht]\n% \\footnotesize\n% \\caption{Performance of LLMs on structure-only node classification, with highly homophilous graphs. We show the correct cases, variances, and total cases.}\n% \\label{tab: sbm}\n% \\centering\n% \\begin{tabular}{@{}ccccc@{}}\n% \\toprule\n% \\multirow{2}{*}{\\textbf{Random Graph Model}}     & \\multirow{2}{*}{\\textbf{$q$}} & \\multicolumn{3}{c}{\\textbf{Size of graphs}} \\\\ \\cmidrule{3-5} \n%                                         &                              & \\textbf{30}     & \\textbf{45}     & \\textbf{60}     \\\\ \\midrule\n% \\multirow{3}{*}{\\textbf{Stochastic Block Model}} & 0.025                           & 8 \u00b1 1/9       & 13 \u00b1 0/14      & 19 \u00b1 0/19     \\\\ \n%                                         & 0.05                            & 7.5 \u00b1 0.5/9       & 13 \u00b1 0/14      & 12 \u00b1 6/19     \\\\ \n%                                         & 0.1                            & 8 \u00b1 0/9       & 9.5 \u00b1 0.5/14       & 9 \u00b1 3/19      \\\\ \\midrule\n% \\end{tabular}\n% \\end{table}\n\n% Via examining the experimental results, we observe that as the inter-community probability increases in larger graphs, LLMs' performance significantly deteriorates. To gain a deeper understanding of this observation, we prompt the LLMs to display its intermediate reasoning process. \n\n% % \\begin{tcolorbox}% [width=10cm]\n\n% % \\end{tcolorbox}\n\n\n% \\begin{table}[h!]\n% \\caption{An example demonstrating the failure cases of LLMs on synthetic graphs}\n% \\label{table:synfail}\n% \\centering\n% \\rule{\\linewidth}{2pt}\n% \\parbox{\\linewidth}{\n% \\vspace{5pt}\n% \\textbf{Input: } (Task Description) Given a graph and a set of nodes, you need to determine the label of some nodes.  The assumption is that the neighboring nodes are more likely to share the same labels. \\\\\n% (Labels of nodes) node 1 has label 0. node 2 has label 0 $\\cdots$ \\\\\n% (Edge Information) node 0 is connected with node 1. node 0 is connected with node 2. $\\cdots$ \\\\\n% (Instruction) What is the label of node 0, node 3, node 6, node 9, node 12, node 15, node 18, node 21, node 24, and node 27? \\underline{Output your reasoning process.} The label for node 0 is 0. \\\\\n% \\textbf{Output:} \n% (Thinking of methodology: ) To determine the label of each node, we can use the assumption that neighboring nodes are more likely to share the same labels. \\\\\n% We'll start with the given labels for some nodes: Node 1 has label 0 $\\cdots$ \\\\\n% (Reasoning process:) Now let's analyze the graph connections: Node 0 is connected with nodes 1, 2, 4, and 7. Since nodes 1, 2, 4, and 7 all have label 0, we can infer that node 0 is also likely to have label 0. $\\cdots$ \\\\\n% \\textcolor{red}{(Wrong Cases:) Node 6 is connected with node 14.\n% Since node 14 has label 1, we can infer that node 6 is also likely to have label 1. (Node 6 is also connected to nodes 1, 3, 4, 5. Its label is 0.)} $\\cdots$ \\\\\n% \\vspace{2pt}\n% }\n% \\rule{\\linewidth}{2pt}\n% \\vspace{5pt}\n% \\end{table}\n\n\n% % \\haitao{Think about the below sentences, I think it does not provide much useful information for the reader.}\n% \\textbf{\\uline{Observation 2.} Memorization is one main reason for LLMs' errors}\n\n% Upon analysis of the reasoning process, we discover that LLMs fail mainly due to their inability to precisely memorize structural information. In Table~\\ref{table:synfail}, the instance demonstrates that they overlook certain edges that are present in the prompt. With a lower inter-community probability, this may not necessarily yield incorrect predictions since nearly all edges link two nodes within the same class. Simply memorizing a subset of the neighboring edges could yield the correct results. However, as the inter-community probability increases, the omission of any edges could potentially lead to divergent reasoning outcomes.\n\n\n% \\section{Homophily} \\label{app: homo}\n% Homophily is an important assumption utilized by various kinds of graph learning models, such as GNNs and label propagation~\\cite{homo}, it mainly states a phenomenon that \"similar nodes\" tend to be neighoring to each other. To quantitatively measure the ratio of homophily, we adopt the edge homophily~\\cite{homo}.\n% \\begin{definition}\n% \\vspace{-0.15cm}\n% Let $\\mathcal{E}$ denote the set of edges in a given graph, and $y_u, y_v$ represent the class labels of nodes $u$ and $v$, respectively. Then, the Edge Homophily Ratio (EHR), denoted by $h$, is defined as the fraction of edges connecting nodes with identical class labels. Mathematically, Homophily can be expressed as $h = \\frac{|{(u,v): (u,v) \\in \\mathcal{E} \\land y_u = y_v}|}{|\\mathcal{E}|}$, wherein the numerator represents the number of intra-class edges.\n% \\label{dfn:homophily}\n% \\end{definition}\n\n% It's obvious that for a node classification task, if $h=1$ then we can simply get perfect performance by assigning neighboring nodes' labels. \n\n% \\section{Synthetic Graphs} \\label{app: synthetic}\n% In this section, we introduce the random graph models to generate the synthetic graphs. We use NetworkX~\\cite{networkx} to generate all the following synthetic graphs. Examples of generated graphs are shown in Figure~\\ref{fig: syn_exp}.\n% \\begin{compactenum}[1.]\n% \\item \\textbf{Ring Graph}: the most sparse fully-connected graph, with exactly one edge between every two nodes. \n% \\item \\textbf{Watts-Strogatz Model~\\cite{watts1998collective}}: we use this random graph generator to generate dense graphs presenting \"small-world\" properties, with \"short average path lengths and high clustering\". We adopt this model mainly to test how the sparsity of the graph influences the LLMs' predicting capability. \n% \\item \\textbf{Stochastic Block Model~\\cite{sbm}}: It is a generative probabilistic model widely used in network science to generate random graphs with community structures. It partitions the nodes of a graph into a fixed number of blocks or groups and then places edges between pairs of nodes with a probability that depends solely on the blocks that the nodes belong to. We choose this model since we can tune the parameters $p$ (intra-community probability) and $q$ (inter-community probability) to adjust the homophily ratio of the graph. We would like to test whether LLMs can get rid of the influence of a small number of inter-community edges. \n% \\end{compactenum}\n\n% \\begin{figure*}[!htb]\n%     \\centering\n%     \\begin{subfigure}[b]{0.32\\linewidth}\n%         \\centering\n%         \\includegraphics[width=\\linewidth]{figures/ring.pdf}\n%         \\caption{Examples of generated ring graphs}\n%     \\end{subfigure}\n%     \\hfill\n%     \\begin{subfigure}[b]{0.32\\linewidth}\n%         \\centering\n%         \\includegraphics[width=\\linewidth]{figures/wsg.pdf}\n%         \\caption{Examples of graphs generated from the Watts-Strogatz model}\n%     \\end{subfigure}\n%     \\hfill\n%     \\begin{subfigure}[b]{0.32\\linewidth}\n%         \\centering\n%         \\includegraphics[width=\\linewidth]{figures/sbm.pdf}\n%         \\caption{Examples of graphs generated from the Stochastic Block model}\n%     \\end{subfigure}\n%     \\caption{Examples of generated synthetic graphs}\n%     \\label{fig: syn_exp}\n% \\end{figure*}\n\n% \\section{Detailed Prompts for Section~\\ref{sec:syn}} \\label{app: prompt}\n% The following are the detailed inputs and outputs for the experiments of Table~\\ref{tab: synthetic_graph}. \n\n% We don't further try the \"code\" based prompt since it can not handle the simplest case on the smallest graph. We find that representing the graph with its adjacency matrix shows effectiveness in some small graphs. However, considering the tokenizer of LLMs, this representation is very token-ineffective, which can not work for larger graphs. As a comparison, \"natural language\" prompts can handle all simple cases perfectly.\n% \\begin{tcolorbox}\n% \\textbf{Input prompt and outputs using a \"code\" prompt} \\\\\n% \\textbf{Input:} \\\\\n% Given a graph and a set of nodes, you need to determine the label of some nodes.  The assumption is that the neighboring nodes are more likely to share the same labels. \\\\\n% edge\\_list=[(0, 1) ... ] \\\\ \n% train\\_nodes=[1, 2, 3, 4, 6 ... ]\\\\ \n% y\\_train=[0.0, 0.0, 0.0 ... ] \\\\\n% Output the labels of nodes [0, 5, 10, 15, 20, 25]. \\\\\n% test\\_nodes=[0, 5, 10, 15, 20, 25] \\\\\n% y\\_test=[0, \\\\\n% \\textbf{Output:} \\\\\n% 1, 1, 0, 1, 1] \\\\\n% \\end{tcolorbox}\n\n% The example input/output for the \"natural language\" prompt on the ring graph.\n% \\begin{tcolorbox}\n% \\textbf{Input prompt and outputs using a \"natural language\" prompt} \\\\\n% \\textbf{Input:} \\\\\n% Given a graph and a set of nodes, you need to determine the label of some nodes.  The assumption is that the neighboring nodes are more likely to share the same labels. \\\\\n% node 1 has label 0.  ... \\\\ \n% node 0 is connected with node 1 ... \\\\\n% What is the label of nodes node 0,node 5,node 10,node 15,node 20,node 25? \\\\\n% Output: \n% The label of node 0 is 0. \\\\\n% \\textbf{Output:} \\\\\n% The label of node 5 is 0. The label of node 10 is 0. The label of node 15 is 1. The label of node 20 is 1. The label of node 25 is 1.  \\\\\n% \\end{tcolorbox}\n\n% That's all folks!\n",
                "category": [
                    "method",
                    "experiments"
                ]
            }
        },
        "tables": {
            "exp:small1": "\\begin{table*}[!ht]\n\\caption{Experimental results for feature-level \\textit{LLMs-as-Enhancer} on \\cora and \\pubmed with a low labeling ratio. Since MLPs do not provide structural information, it is meaningless to co-train it with PLM (with their performance shown as N/A). We use \\textcolor{yellow}{yellow} to denote the best performance under a specific GNN/MLP model, \\textcolor{green}{green} the second best one, and \\textcolor{pink}{pink} the third best one.}\n\\label{exp:small1}\n\\centering\n\\resizebox{0.75\\linewidth}{!}{\n\\begin{tabular}{lcccccc}\n\\toprule\n& \\multicolumn{3}{c}{\\textsc{cora}} & \\multicolumn{3}{c}{\\textsc{pubmed}} \\\\ \n\\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n& GCN & GAT & MLP & GCN & GAT & MLP \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Non-contextualized Shallow Embeddings}} \\\\ \nTF-IDF & 81.99 \u00b1 0.63 & 82.30 \u00b1 0.65 & 67.18 \u00b1 1.01 & 78.86 \u00b1 2.00 & 77.65 \u00b1 0.91 & 71.07 \u00b1 0.78 \\\\ \nWord2Vec & 74.01 \u00b1 1.24 & 72.32 \u00b1 0.17 & 55.34 \u00b1 1.31 & 70.10 \u00b1 1.80 & 69.30 \u00b1 0.66 & 63.48 \u00b1 0.54 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{PLM/LLM Embeddings without Fine-tuning}} \\\\ \nDeberta-base & 48.49 \u00b1 1.86 & 51.02 \u00b1 1.22 & 30.40 \u00b1 0.57 & 62.08 \u00b1 0.06 & 62.63 \u00b1 0.27 & 53.50 \u00b1 0.43 \\\\ \nLLama 7B & 66.80 \u00b1 2.20 & 59.74 \u00b1 1.53 & 52.88 \u00b1 1.96 & 73.53 \u00b1 0.06 & 67.52 \u00b1 0.07 & 66.07 \u00b1 0.56 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Local Sentence Embedding Models}} \\\\ \nSentence-BERT(MiniLM) & \\cellcolor{pink}82.20 \u00b1 0.49 & \\cellcolor{green}82.77 \u00b1 0.59 & \\cellcolor{green}74.26 \u00b1 1.44 & \\cellcolor{green}81.01 \u00b1 1.32 & 79.08 \u00b1 0.07 & 76.66 \u00b1 0.50 \\\\ \ne5-large & \\cellcolor{green}82.56 \u00b1 0.73 & 81.62 \u00b1 1.09 & \\cellcolor{yellow}74.26 \u00b1 0.93 & \\cellcolor{yellow}82.63 \u00b1 1.13 & \\cellcolor{pink}79.67 \u00b1 0.80 & \\cellcolor{yellow}80.38 \u00b1 1.94 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Online Sentence Embedding Models}} \\\\ \ntext-ada-embedding-002 & \\cellcolor{yellow}82.72 \u00b1 0.69 & \\cellcolor{pink}82.51 \u00b1 0.86 & \\cellcolor{pink}73.15 \u00b1 0.89 & 79.09 \u00b1 1.51 & \\cellcolor{green}80.27 \u00b1 0.41 & \\cellcolor{pink}78.03 \u00b1 1.02 \\\\ \nGoogle Palm Cortex 001 & 81.15 \u00b1 1.01 & \\cellcolor{yellow}82.79 \u00b1 0.41 & 69.51 \u00b1 0.83 & \\cellcolor{pink}80.91 \u00b1 0.19 & \\cellcolor{yellow}80.72 \u00b1 0.33 & \\cellcolor{green}78.93 \u00b1 0.90 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Fine-tuned PLM Embeddings}} \\\\ \nFine-tuned Deberta-base & 59.23 \u00b1 1.16 & 57.38 \u00b1 2.01 & 30.98 \u00b1 0.68 & 62.12 \u00b1 0.07 & 61.57 \u00b1 0.07 & 53.65 \u00b1 0.26 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Iterative Structure}} \\\\ \nGLEM-GNN & 48.49 \u00b1 1.86 & 51.02 \u00b1 1.22 & N/A & 62.08 \u00b1 0.06 & 62.63 \u00b1 0.27 & N/A \\\\ \nGLEM-LM & 59.23 \u00b1 1.16 & 57.38 \u00b1 2.01 & N/A & 62.12 \u00b1 0.07 & 61.57 \u00b1 0.07 & N/A \\\\ \n\\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "exp:small2": "\\begin{table*}[!ht]\n\\caption{Experimental results for feature-level \\textit{LLMs-as-Enhancers} on \\cora and \\pubmed with a high labeling ratio.  We use \\textcolor{yellow}{yellow} to denote the best performance under a specific GNN/MLP model, \\textcolor{green}{green} the second best one, and \\textcolor{pink}{pink} the third best one.}\n\\label{exp:small2}\n\\centering\n\\resizebox{0.75\\linewidth}{!}{\n\n\\begin{tabular}{lcccccc}\n\\toprule\n& \\multicolumn{3}{c}{\\textsc{cora}} & \\multicolumn{3}{c}{\\textsc{pubmed}} \\\\ \n\\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n& GCN & GAT & MLP & GCN & GAT & MLP \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Non-contextualized Shallow Embeddings}} \\\\ \nTF-IDF & \\cellcolor{yellow}90.90 \u00b1 2.74 & \\cellcolor{green}90.64 \u00b1 3.08 & 83.98 \u00b1 5.91 & 89.16 \u00b1 1.25 & 89.00 \u00b1 1.67 & 89.72 \u00b1 3.57 \\\\ \nWord2Vec & 88.40 \u00b1 2.25 & 87.62 \u00b1 3.83 & 78.71 \u00b1 6.32 & 85.50 \u00b1 0.77 & 85.63 \u00b1 0.93 & 83.80 \u00b1 1.33 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{PLM/LLM Embeddings without Fine-tuning}} \\\\ \nDeberta-base & 65.86 \u00b1 1.96 & 79.67 \u00b1 3.19 & 45.64 \u00b1 4.41 & 67.33 \u00b1 0.69 & 67.81 \u00b1 1.05 & 65.07 \u00b1 0.57 \\\\ \nLLama 7B & 89.69 \u00b1 1.86 & 87.66 \u00b1 4.84 & 80.66 \u00b1 7.72 & 88.26 \u00b1 0.78 & 88.31 \u00b1 2.01 & 89.39 \u00b1 1.09 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Local Sentence Embedding Models}} \\\\ \nSentence-BERT(MiniLM) & 89.61 \u00b1 3.23 & \\cellcolor{yellow}90.68 \u00b1 2.22 & \\cellcolor{yellow}86.45 \u00b1 5.56 & 90.32 \u00b1 0.91 & 90.80 \u00b1 2.02 & 90.59 \u00b1 1.23 \\\\ \ne5-large & \\cellcolor{green}90.53 \u00b1 2.33 & 89.10 \u00b1 3.22 & \\cellcolor{green}86.19 \u00b1 4.38 & 89.65 \u00b1 0.85 & 89.55 \u00b1 1.16 & 91.39 \u00b1 0.47 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Online Sentence Embedding Models}} \\\\ \ntext-ada-embedding-002 & 89.13 \u00b1 2.00 & \\cellcolor{pink}90.42 \u00b1 2.50 & \\cellcolor{pink}85.97 \u00b1 5.58 & 89.81 \u00b1 0.85 & \\cellcolor{pink}91.48 \u00b1 1.94 & \\cellcolor{green}92.63 \u00b1 1.14 \\\\ \nGoogle Palm Cortex 001 & \\cellcolor{pink}90.02 \u00b1 1.86 & 90.31 \u00b1 2.82 & 81.03 \u00b1 2.60 & 89.78 \u00b1 0.95 & 90.52 \u00b1 1.35 & \\cellcolor{pink}91.87 \u00b1 0.84 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Fine-tuned PLM Embeddings}} \\\\\nFine-tuned Deberta-base & 85.86 \u00b1 2.28 & 86.52 \u00b1 1.87 & 78.20 \u00b1 2.25 & \\cellcolor{pink}91.49 \u00b1 1.92 & 89.88 \u00b1 4.63 & \\cellcolor{yellow}94.65 \u00b1 0.13 \\\\ \n\\midrule\n\\multicolumn{7}{l}{\\textbf{Iterative Structure}} \\\\ \nGLEM-GNN & 89.13 \u00b1 0.73 & 88.95 \u00b1 0.64 & N/A & \\cellcolor{green}92.57 \u00b1 0.25 & \\cellcolor{green}92.78 \u00b1 0.21 & N/A \\\\ \nGLEM-LM & 82.71 \u00b1 1.08 & 83.54 \u00b1 0.99 & N/A & \\cellcolor{yellow}94.36 \u00b1 0.21 & \\cellcolor{yellow}94.62 \u00b1 0.14 & N/A \\\\ \n\\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "exp: ogb": "\\begin{table*}[ht]\n\\caption{Experimental results for feature-level \\textit{LLMs-as-Enhancers} on \\arxiv and \\products dataset. MLPs do not provide structural information so it's meaningless to co-train it with PLM, thus we don't show the performance. We use \\textcolor{yellow}{yellow} to denote the best performance under a specific GNN/MLP model, \\textcolor{green}{green} the second best one, and \\textcolor{pink}{pink} the third best one.}\n\\label{exp: ogb}\n\\centering\n\\resizebox{0.75\\linewidth}{!}{\n\\begin{tabular}{lcccccc}\n\\toprule\n& \\multicolumn{3}{c}{\\textsc{\\arxiv}} & \\multicolumn{3}{c}{\\textsc{\\products}} \\\\ \\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n& GCN & MLP & RevGAT & SAGE & SAGN & MLP \\\\ \\midrule\n\\multicolumn{7}{l}{\\textbf{Non-contextualized Shallow Embeddings}} \\\\\nTF-IDF & 72.23 \u00b1 0.21 & 66.60 \u00b1 0.25 & 75.16 \u00b1 0.14 & 79.73 \u00b1 0.48 & 84.40 \u00b1 0.07 & 64.42 \u00b1 0.18 \\\\ \nWord2Vec & 71.74 \u00b1 0.29 & 55.50 \u00b1 0.23 & 73.78 \u00b1 0.19 & 81.33 \u00b1 0.79 & 84.12 \u00b1 0.18 & 69.27 \u00b1 0.54 \\\\ \\midrule\n\\multicolumn{7}{l}{\\textbf{PLM/LLM Embeddings without Fine-tuning}} \\\\\nDeberta-base & 45.70 \u00b1 5.59 & 40.33 \u00b1 4.53 & 71.20 \u00b1 0.48 & 62.03 \u00b1 8.82 & 74.90 \u00b1 0.48 & 7.18 \u00b1 1.09 \\\\ \\midrule\n\\multicolumn{7}{l}{\\textbf{Local Sentence Embedding Models}} \\\\\nSentence-BERT(MiniLM) & 73.10 \u00b1 0.25 & 71.62 \u00b1 0.10 & \\cellcolor{green}76.94 \u00b1 0.11 & 82.51 \u00b1 0.53 & 84.79 \u00b1 0.23 & 72.73 \u00b1 0.34 \\\\ \ne5-large & 73.74 \u00b1 0.12 & \\cellcolor{pink}72.75 \u00b1 0.00 & 76.59 \u00b1 0.44 & 82.46 \u00b1 0.91 & \\cellcolor{pink}85.47 \u00b1 0.21 & \\cellcolor{pink}77.49 \u00b1 0.29 \\\\ \\midrule\n\\multicolumn{7}{l}{\\textbf{Online Sentence Embedding Models}} \\\\\ntext-ada-embedding-002 & 72.76 \u00b1 0.23 & 72.17 \u00b1 0.00 & \\cellcolor{pink}76.64 \u00b1 0.20 & \\cellcolor{pink}82.90 \u00b1 0.42 & 85.20 \u00b1 0.19 & 76.42 \u00b1 0.31 \\\\ \\midrule\n\\multicolumn{7}{l}{\\textbf{Fine-tuned PLM Embeddings}} \\\\\nFine-tuned Deberta-base & \\cellcolor{pink}74.65 \u00b1 0.12 & \\cellcolor{green}72.90 \u00b1 0.11 & 75.80 \u00b1 0.39 & 82.15 \u00b1 0.16 & 84.01 \u00b1 0.05 & \\cellcolor{green}79.08 \u00b1 0.23 \\\\ \\midrule\n\\textbf{Others} \\\\\nGIANT & 73.29 \u00b1 0.10 & \\cellcolor{yellow}73.06 \u00b1 0.11 & 75.90 \u00b1 0.19 & \\cellcolor{green}83.16 \u00b1 0.19 & \\cellcolor{green}86.67 \u00b1 0.09 & \\cellcolor{yellow}79.82 \u00b1 0.07 \\\\ \\midrule\n\\multicolumn{7}{l}{\\textbf{Iterative Structure}} \\\\ \nGLEM-GNN & \\cellcolor{yellow}75.93 \u00b1 0.19 & N/A & \\cellcolor{yellow}76.97 \u00b1 0.19 & \\cellcolor{yellow}83.16 \u00b1 0.09 & \\cellcolor{yellow}87.36 \u00b1 0.07 & N/A \\\\ \nGLEM-LM & \\cellcolor{green}75.71 \u00b1 0.24 & N/A & 75.45 \u00b1 0.12 & 81.25 \u00b1 0.15 & 84.83 \u00b1 0.04 & N/A \\\\ \n\\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "exp: eff": "\\begin{table*}[!ht]\n\\caption{Efficiency analysis on \\arxiv. Note that we show the dimension of generated embeddings in the brackets. For GIANT, it adopts a special pre-training stage, which will introduce computation overhead with orders of magnitude larger than that of fine-tuning. The specific time was not discussed in the original paper, therefore its cost in LM-phase is not shown in the table. } \n\\label{exp: eff}\n\\centering\n \\resizebox{0.85\\linewidth}{!}{\n\\begin{tabular}{clcccc}\n\\toprule\n\\textbf{Input features}                       & \\textbf{Backbone} & \\textbf{\\begin{tabular}[c]{@{}c@{}} LM-phase \\\\ Running time(s)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}} LM-phase \\\\ Memory (GB)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}} GNN-phase \\\\ Running time(s)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}} GNN-phase \\\\ Memory (GB)\\end{tabular}} \\\\ \\midrule\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}} \\textbf{TF-IDF} \\\\ {(1024)}\\end{tabular}}              & GCN               & N/A                                 & N/A                            & 53                                 & 9.81                          \\\\ \n                                              & RevGAT            & N/A                                 & N/A                            & 873                                & 7.32                          \\\\ \\midrule\n\\multirow{2}{*}{{\\begin{tabular}[c]{@{}c@{}} \\textbf{Sentence-BERT} \\\\ {(384)}\\end{tabular}}}               & GCN               & 239                               & 1.30                         & 48                                 & 7.11                          \\\\ \n                                              & RevGAT            & 239                               & 1.30                         & 674                                & 4.37                          \\\\ \\midrule\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}} \\textbf{text-ada-embedding-002} \\\\ {(1536)}\\end{tabular}}                 & GCN               & 165                               & N/A                            & 73                                 & 11.00                         \\\\ \n                                              & RevGAT            & 165                               & N/A                            & 1038                               & 8.33                          \\\\ \\midrule\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}} \\textbf{Deberta-base} \\\\ {(768)}\\end{tabular}} & GCN               & 13560                             & 12.53                        & 50                                 & 9.60                          \\\\ \n                                              & RevGAT            & 13560                             & 12.53                        & 122                                & 6.82                          \\\\ \\midrule\n\\multirow{2}{*}{{\\begin{tabular}[c]{@{}c@{}} \\textbf{GLEM-GNN} \\\\ {(768)}\\end{tabular}}}            & GCN               & 68071                             & 18.22                        & N/A                              & N/A                         \\\\ \n                                              & RevGAT            & 68294                             & 18.22                        & N/A                              & N/A                         \\\\ \\midrule\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}} \\textbf{GIANT} \\\\ {(768)}\\end{tabular}} & GCN               &  N/A                           & N/A                        & 50                                 & 9.60                          \\\\ \n                                              & RevGAT            & N/A                             & N/A                       & 122                                & 6.82                          \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "tab:TAPEabla": "\\begin{table*}[!ht]\n\\centering\n\\caption{A detailed ablation study of TAPE on \\cora and \\pubmed dataset in low labeling rate setting. For each combination of features and models, we use \\textcolor{yellow}{yellow} to denote the best performance under a specific GNN/MLP model, \\textcolor{green}{green} the second best one, and \\textcolor{pink}{pink} the third best one. }\n\\label{tab:TAPEabla}\n\\centering\n\\resizebox{0.75\\linewidth}{!}{\n\\begin{tabular}{@{}llcccccc@{}}\n\\toprule\n &\n   & \n  \\multicolumn{3}{c}{\\cora} &\n  \\multicolumn{3}{c}{\\pubmed} \\\\  \\cmidrule(lr){3-5} \\cmidrule(lr){6-8} \n\\multirow{-2}{*}{} &\n  \\multirow{-2}{*}{} &\n  \\textbf{GCN} &\n  \\textbf{GAT} &\n  \\textbf{MLP} &\n  \\textbf{GCN} &\n  \\textbf{GAT} &\n  \\textbf{MLP} \\\\ \\midrule\n &\n  \\textbf{TAPE}  &\n  74.56 \u00b1 2.03 &\n  75.27 \u00b1 2.10 &\n  64.44 \u00b1 0.60 &\n  \\cellcolor{green}85.97 \u00b1 0.31 &\n  \\cellcolor{green}86.97 \u00b1 0.33 &\n  \\cellcolor{green}93.18 \u00b1 0.28 \\\\\n &\n  \\textbf{P} &\n  52.79 \u00b1 1.47 &\n  62.13 \u00b1 1.50 &\n  63.56 \u00b1 0.52 &\n  81.92 \u00b1 1.89 &\n  \\cellcolor{yellow}88.27 \u00b1 0.01 &\n  \\cellcolor{yellow}93.27 \u00b1 0.15 \\\\\n &\n  \\textbf{TA + E} (e5) &\n  \\cellcolor{green}83.38 \u00b1 0.42 &\n  \\cellcolor{yellow}84.00 \u00b1 0.09 &\n  \\cellcolor{yellow}75.73 \u00b1 0.53 &\n  \\cellcolor{yellow}87.44 \u00b1 0.49 &\n  \\cellcolor{pink}86.71 \u00b1 0.92 &\n  \\cellcolor{pink}90.25 \u00b1 1.56 \\\\\n &\n  \\textbf{TA + E} (PLM) &\n  78.02 \u00b1 0.56 &\n  64.08 \u00b1 12.36 &\n  55.72 \u00b1 11.98 &\n  80.70 \u00b1 1.73 &\n  79.66 \u00b1 3.08 &\n  76.42 \u00b1 2.18 \\\\\n &\n  \\textbf{E} (PLM) &\n  79.46 \u00b1 1.10 &\n  74.82 \u00b1 1.19 &\n  63.04 \u00b1 0.88 &\n  81.88 \u00b1 0.05 &\n  81.56 \u00b1 0.07 &\n  76.90 \u00b1 1.60 \\\\\n\\multirow{-6}{*}{\\begin{tabular}[c]{@{}l@{}}\\uline{\\textbf{TAPE}}\\end{tabular}} &\n  \\textbf{E} (e5) &\n  \\cellcolor{yellow}84.38 \u00b1 0.36 &\n  \\cellcolor{green}83.01 \u00b1 0.60 &\n  \\cellcolor{pink}70.64 \u00b1 1.10 &\n  82.23 \u00b1 0.78 &\n  80.30 \u00b1 0.77 &\n  77.23 \u00b1 0.48 \\\\ \\midrule\n &\n  \\textbf{TA} (PLM) &\n  59.23 \u00b1 1.16 &\n  57.38 \u00b1 2.01 &\n  30.98 \u00b1 0.68 &\n  62.12 \u00b1 0.07 &\n  61.57 \u00b1 0.07 &\n  53.65 \u00b1 0.26 \\\\ \n\\multirow{-2}{*}{\\begin{tabular}[c]{@{}l@{}}\\uline{\\textbf{Original}} \\\\ \\uline{\\textbf{attributes}}\\end{tabular}} &\n  \\textbf{TA} (e5) &\n  \\cellcolor{pink}82.56 \u00b1 0.73 &\n  \\cellcolor{pink}81.62 \u00b1 1.09 &\n  \\cellcolor{green}74.26 \u00b1 0.93 &\n  \\cellcolor{pink}82.63 \u00b1 1.13 &\n  79.67 \u00b1 0.80 &\n  80.38 \u00b1 1.94 \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "tab:TAPEabla2": "\\begin{table*}[!ht]\n\\centering\n\\caption{A detailed ablation study of TAPE on \\cora and \\pubmed dataset in the high labeling rate setting. For each combination of features and models, we use \\textcolor{yellow}{yellow} to denote the best performance under a specific GNN/MLP model, \\textcolor{green}{green} the second best one, and \\textcolor{pink}{pink} the third best one. }\n\\label{tab:TAPEabla2}\n\\resizebox{0.75\\linewidth}{!}{\n\\centering\n\\begin{tabular}{@{}llcccccc@{}}\n\\toprule\n &\n   &\n  \\multicolumn{3}{c}{\\cora} &\n  \\multicolumn{3}{c}{\\pubmed} \\\\ \\cmidrule(l){3-8} \n &\n   &\n  \\textbf{GCN} &\n  \\textbf{GAT} &\n  \\textbf{MLP} &\n  \\textbf{GCN} &\n  \\textbf{GAT} &\n  \\textbf{MLP} \\\\ \\midrule\n\\multirow{6}{*}{{\\ul \\textbf{TAPE}}} &\n  \\textbf{TAPE} &\n  87.88 \u00b1 0.98 &\n  88.69 \u00b1 1.13 &\n  83.09 \u00b1 0.91 &\n  \\cellcolor{green}92.22 \u00b1 1.30 &\n  \\cellcolor{green}93.35 \u00b1 1.50 &\n  \\cellcolor{green}95.05 \u00b1 0.27 \\\\\n &\n  \\textbf{P} &\n  64.90 \u00b1 1.39 &\n  80.11 \u00b1 4.01 &\n  70.31 \u00b1 1.91 &\n  85.73 \u00b1 0.59 &\n  91.60 \u00b1 0.62 &\n  93.65 \u00b1 0.35 \\\\\n &\n  \\textbf{TA + E} (e5) &\n  \\cellcolor{yellow}90.68 \u00b1 2.12 &\n  \\cellcolor{yellow}91.86 \u00b1 1.36 &\n  \\cellcolor{yellow}87.00 \u00b1 4.83 &\n  \\cellcolor{yellow}92.64 \u00b1 1.00 &\n  \\cellcolor{yellow}93.35 \u00b1 1.24 &\n  94.34 \u00b1 0.86 \\\\\n &\n  \\textbf{TA + E} (PLM) &\n  87.44 \u00b1 1.74 &\n  88.40 \u00b1 1.60 &\n  82.80 \u00b1 1.00 &\n  90.23 \u00b1 0.71 &\n  \\cellcolor{pink}91.73 \u00b1 1.58 &\n  \\cellcolor{yellow}95.40 \u00b1 0.32 \\\\\n &\n  \\textbf{E} (PLM) &\n  83.28 \u00b1 4.53 &\n  82.47 \u00b1 6.06 &\n  80.41 \u00b1 3.35 &\n  88.90 \u00b1 2.94 &\n  83.00 \u00b1 14.07 &\n  87.75 \u00b1 14.83 \\\\\n &\n  \\textbf{E} (e5) &\n  \\cellcolor{pink}89.39 \u00b1 2.69 &\n  \\cellcolor{green}90.13 \u00b1 2.52 &\n  \\cellcolor{pink}84.05 \u00b1 4.03 &\n  89.68 \u00b1 0.78 &\n  90.61 \u00b1 1.61 &\n  91.09 \u00b1 0.85 \\\\ \\midrule\n\\multirow{2}{*}{\\textbf{\\begin{tabular}[c]{@{}l@{}}\\uline{Original}\\\\ \\uline{attributes}\\end{tabular}}} &\n  \\textbf{TA} (PLM) &\n  85.86 \u00b1 2.28 &\n  86.52 \u00b1 1.87 &\n  78.20 \u00b1 2.25 &\n  \\cellcolor{pink}91.49 \u00b1 1.92 &\n  89.88 \u00b1 4.63 &\n  \\cellcolor{pink}94.65 \u00b1 0.13 \\\\\n &\n  \\textbf{TA} (e5) &\n  \\cellcolor{green}90.53 \u00b1 2.33 &\n  \\cellcolor{pink}89.10 \u00b1 3.22 &\n  \\cellcolor{green}86.19 \u00b1 4.38 &\n  89.65 \u00b1 0.85 &\n  89.55 \u00b1 1.16 &\n  91.39 \u00b1 0.47 \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "tab:kea1": "\\begin{table*}[!ht]\n\\centering\n\\caption{A detailed ablation study of KEA on \\cora and \\pubmed dataset in the low labeling rate setting. For each combination of features and models, we use \\textcolor{yellow}{yellow} to denote the best performance under a specific GNN/MLP model, \\textcolor{green}{green} the second best one, and \\textcolor{pink}{pink} the third best one.}\n\\label{tab:kea1}\n\\centering\n\\resizebox{0.75\\linewidth}{!}{\n\\begin{tabular}{@{}llcccccc@{}}\n\\toprule\n &\n   &\n  \\multicolumn{3}{c}{\\cora} &\n  \\multicolumn{3}{c}{\\pubmed} \\\\ \\cmidrule(l){3-8} \n &\n   &\n  \\textbf{GCN} &\n  \\textbf{GAT} &\n  \\textbf{MLP} &\n  \\textbf{GCN} &\n  \\textbf{GAT} &\n  \\textbf{MLP} \\\\ \\midrule\n\\multirow{2}{*}{\\textbf{\\begin{tabular}[c]{@{}l@{}}\\uline{Original}\\\\ \\uline{attributes}\\end{tabular}}} &\n  TA (PLM) &\n  59.23 \u00b1 1.16 &\n  57.38 \u00b1 2.01 &\n  30.98 \u00b1 0.68 &\n  62.12 \u00b1 0.07 &\n  61.57 \u00b1 0.07 &\n  53.65 \u00b1 0.26 \\\\\n &\n  TA (e5) &\n  82.56 \u00b1 0.73 &\n  81.62 \u00b1 1.09 &\n  \\cellcolor{pink}74.26 \u00b1 0.93 &\n  \\cellcolor{pink}82.63 \u00b1 1.13 &\n  79.67 \u00b1 0.80 &\n  \\cellcolor{pink}80.38 \u00b1 1.94 \\\\ \\midrule\n\\multirow{8}{*}{{\\ul \\textbf{KEA}}} &\n  KEA-I + TA (e5) &\n  \\cellcolor{pink}83.20 \u00b1 0.56 &\n  \\cellcolor{green}83.38 \u00b1 0.63 &\n  \\cellcolor{green}74.34 \u00b1 0.97 &\n  \\cellcolor{yellow}83.30 \u00b1 1.75 &\n  \\cellcolor{green}81.16 \u00b1 0.87 &\n  \\cellcolor{green}80.74 \u00b1 2.44 \\\\\n &\n  KEA-I + TA (PLM) &\n  53.21 \u00b1 11.54 &\n  55.38 \u00b1 4.64 &\n  31.80 \u00b1 3.63 &\n  57.13 \u00b1 8.20 &\n  58.66 \u00b1 4.27 &\n  52.28 \u00b1 4.47 \\\\\n &\n  KEA-I (e5) &\n  81.35 \u00b1 0.77 &\n  82.04 \u00b1 0.72 &\n  70.64 \u00b1 1.10 &\n  81.98 \u00b1 0.91 &\n  \\cellcolor{pink}81.04 \u00b1 1.39 &\n  79.73 \u00b1 1.63 \\\\\n &\n  KEA-I (PLM) &\n  36.68 \u00b1 18.63 &\n  37.69 \u00b1 12.79 &\n  30.46 \u00b1 0.60 &\n  56.22 \u00b1 7.17 &\n  59.33 \u00b1 1.69 &\n  52.79 \u00b1 0.51 \\\\\n &\n  KEA-S + TA (e5) &\n  \\cellcolor{yellow}84.63 \u00b1 0.58 &\n  \\cellcolor{yellow}85.02 \u00b1 0.40 &\n  \\cellcolor{yellow}76.11 \u00b1 2.66 &\n  \\cellcolor{green}82.93 \u00b1 2.38 &\n  \\cellcolor{yellow}81.34 \u00b1 1.51 &\n  \\cellcolor{yellow}80.74 \u00b1 2.44 \\\\\n &\n  KEA-S + TA (PLM) &\n  51.36 \u00b1 16.13 &\n  52.85 \u00b1 7.00 &\n  34.56 \u00b1 5.09 &\n  59.47 \u00b1 6.09 &\n  51.93 \u00b1 3.27 &\n  51.11 \u00b1 2.63 \\\\\n &\n  KEA-S (e5) &\n  \\cellcolor{green}84.38 \u00b1 0.36 &\n  \\cellcolor{pink}83.01 \u00b1 0.60 &\n  70.64 \u00b1 1.10 &\n  82.23 \u00b1 0.78 &\n  80.30 \u00b1 0.77 &\n  77.23 \u00b1 0.48 \\\\\n &\n  KEA-S (PLM) &\n  28.97 \u00b1 18.24 &\n  43.88 \u00b1 10.31 &\n  30.36 \u00b1 0.58 &\n  61.22 \u00b1 0.94 &\n  54.93 \u00b1 1.55 &\n  47.94 \u00b1 0.89 \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "tab:kea2": "\\begin{table*}[!ht]\n\\centering\n\\caption{A detailed ablation study of KEA on \\cora and \\pubmed dataset in the high labeling rate setting. For each combination of features and models, we use \\textcolor{yellow}{yellow} to denote the best performance under a specific GNN/MLP model, \\textcolor{green}{green} the second best one, and \\textcolor{pink}{pink} the third best one.}\n\\label{tab:kea2}\n\\centering\n\\resizebox{0.75\\linewidth}{!}{\n\\begin{tabular}{@{}llcccccc@{}}\n\\toprule\n &\n   &\n  \\multicolumn{3}{c}{\\cora} &\n  \\multicolumn{3}{c}{\\pubmed} \\\\ \\cmidrule(l){3-8} \n &\n   &\n  \\textbf{GCN} &\n  \\textbf{GAT} &\n  \\textbf{MLP} &\n  \\textbf{GCN} &\n  \\textbf{GAT} &\n  \\textbf{MLP} \\\\ \\midrule\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}l@{}}\\uline{\\textbf{Original}}\\\\ \\uline{\\textbf{Attributes}}\\end{tabular}} &\n  \\textbf{TA} (PLM) &\n  85.86 \u00b1 2.28 &\n  86.52 \u00b1 1.87 &\n  78.20 \u00b1 2.25 &\n  \\cellcolor{pink}91.49 \u00b1 1.92 &\n  89.88 \u00b1 4.63 &\n  \\cellcolor{pink}94.65 \u00b1 0.13 \\\\\n &\n  \\textbf{TA} (e5) &\n  90.53 \u00b1 2.33 &\n  89.10 \u00b1 3.22 &\n  86.19 \u00b1 4.38 &\n  89.65 \u00b1 0.85 &\n  89.55 \u00b1 1.16 &\n  91.39 \u00b1 0.47 \\\\ \\midrule\n\\multirow{8}{*}{\\uline{\\textbf{KEA}}} &\n\\textbf{KEA-I + TA} (e5) &\n  \\cellcolor{yellow}91.12 \u00b1 1.76 &\n  \\cellcolor{green}90.24 \u00b1 2.93 &\n  \\cellcolor{green}87.88 \u00b1 4.44 &\n  90.19 \u00b1 0.83 &\n  90.60 \u00b1 1.22 &\n  92.12 \u00b1 0.74 \\\\\n &\n \\textbf{KEA-I + TA} (PLM) &\n  87.07 \u00b1 1.04 &\n  87.66 \u00b1 0.86 &\n  79.12 \u00b1 2.77 &\n  \\cellcolor{yellow}92.32 \u00b1 0.64 &\n  \\cellcolor{yellow}92.29 \u00b1 1.43 &\n  \\cellcolor{yellow}94.85 \u00b1 0.20 \\\\\n &\n  \\textbf{KEA-I} (e5) &\n  \\cellcolor{green}91.09 \u00b1 1.78 &\n  90.13 \u00b1 2.76 &\n  \\cellcolor{pink}86.78 \u00b1 4.12 &\n  89.56 \u00b1 0.82 &\n  90.25 \u00b1 1.34 &\n  91.92 \u00b1 0.80 \\\\\n &\n \\textbf{KEA-I} (PLM) &\n  86.08 \u00b1 2.35 &\n  85.23 \u00b1 3.15 &\n  77.97 \u00b1 2.87 &\n  \\cellcolor{green}91.73 \u00b1 0.58 &\n  \\cellcolor{green}91.93 \u00b1 1.76 &\n  \\cellcolor{green}94.76 \u00b1 0.33 \\\\\n &\n  \n  \\textbf{KEA-S + TA} (e5) &\n  \\cellcolor{pink}91.09 \u00b1 1.78 &\n  \\cellcolor{yellow}92.30 \u00b1 1.69 &\n  \\cellcolor{yellow}88.95 \u00b1 4.96 &\n  90.40 \u00b1 0.92 &\n  \\cellcolor{pink}90.82 \u00b1 1.30 &\n  91.78 \u00b1 0.56 \\\\\n &\n  \\textbf{KEA-S + TA} (PLM) &\n  83.98 \u00b1 5.13 &\n  87.33 \u00b1 1.68 &\n  80.04 \u00b1 1.32 &\n  86.11 \u00b1 5.68 &\n  89.04 \u00b1 5.82 &\n  94.35 \u00b1 0.48 \\\\ &\n  \\textbf{KEA-S} (e5) &\n  89.39 \u00b1 2.69 &\n  \\cellcolor{pink}90.13 \u00b1 2.52 &\n  84.05 \u00b1 4.03 &\n  89.68 \u00b1 0.78 &\n  90.61 \u00b1 1.61 &\n  91.09 \u00b1 0.85 \\\\\n &\n  \n  \\textbf{KEA-S} (PLM) &\n  83.35 \u00b1 7.30 &\n  85.67 \u00b1 2.00 &\n  76.76 \u00b1 1.82 &\n  79.68 \u00b1 19.57 &\n  69.90 \u00b1 19.75 &\n  85.91 \u00b1 6.47 \\\\\n  \\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "tab: kea4": "\\begin{table*}[!ht]\n\\centering\n\\caption{Comparison of the performance of TA, KEA-I, and KEA-S, and TA + E. The best performance is shown with an underline. \\cora (low) means a low labeling rate setting, and \\cora (high) denotes a high labeling rate setting.}\n\\label{tab: kea4}\n\\centering\n\\resizebox{0.75\\linewidth}{!}{\n\\begin{tabular}{@{}lllllll@{}}\n\\toprule\n &\n  \\multicolumn{3}{c}{\\cora (low)} &\n  \\multicolumn{3}{c}{\\pubmed (low)} \\\\ \\cmidrule(l){2-7} \n &\n  \\multicolumn{1}{c}{\\textbf{GCN}} &\n  \\multicolumn{1}{c}{\\textbf{GAT}} &\n  \\multicolumn{1}{c}{\\textbf{MLP}} &\n  \\multicolumn{1}{c}{\\textbf{GCN}} &\n  \\multicolumn{1}{c}{\\textbf{GAT}} &\n  \\multicolumn{1}{c}{\\textbf{MLP}} \\\\ \\midrule\n\\textbf{TA} &\n  82.56 \u00b1 0.73 &\n  81.62 \u00b1 1.09 &\n  74.26 \u00b1 0.93 &\n  82.63 \u00b1 1.13 &\n  79.67 \u00b1 0.80 &\n  80.38 \u00b1 1.94 \\\\\n\\textbf{KEA-I + TA} &\n  83.20 \u00b1 0.56 &\n  83.38 \u00b1 0.63 &\n  74.34 \u00b1 0.97 &\n  83.30 \u00b1 1.75 &\n  81.16 \u00b1 0.87 &\n  80.74 \u00b1 2.44 \\\\\n\\textbf{KEA-S + TA} &\n  {\\ul 84.63 \u00b1 0.58} &\n  {\\ul 85.02 \u00b1 0.40} &\n  {\\ul 76.11 \u00b1 2.66} &\n  82.93 \u00b1 2.38 &\n  81.34 \u00b1 1.51 &\n  80.74 \u00b1 2.44 \\\\\n\\textbf{TA+E} &\n  83.38 \u00b1 0.42 &\n  84.00 \u00b1 0.09 &\n  75.73 \u00b1 0.53 &\n  {\\ul 87.44 \u00b1 0.49} &\n  {\\ul 86.71 \u00b1 0.92} &\n  {\\ul 90.25 \u00b1 1.56} \\\\ \\midrule\n &\n  \\multicolumn{3}{c}{\\cora (high)} &\n  \\multicolumn{3}{c}{\\pubmed (high)} \\\\ \\cmidrule(l){2-7} \n &\n  \\multicolumn{1}{c}{\\textbf{GCN}} &\n  \\multicolumn{1}{c}{\\textbf{GAT}} &\n  \\multicolumn{1}{c}{\\textbf{MLP}} &\n  \\multicolumn{1}{c}{\\textbf{GCN}} &\n  \\multicolumn{1}{c}{\\textbf{GAT}} &\n  \\multicolumn{1}{c}{\\textbf{MLP}} \\\\ \\midrule\n\\textbf{TA} &\n  90.53 \u00b1 2.33 &\n  89.10 \u00b1 3.22 &\n  86.19 \u00b1 4.38 &\n  89.65 \u00b1 0.85 &\n  89.55 \u00b1 1.16 &\n  91.39 \u00b1 0.47 \\\\\n\\textbf{KEA-I + TA} &\n  {\\ul 91.12 \u00b1 1.76} &\n  90.24 \u00b1 2.93 &\n  87.88 \u00b1 4.44 &\n  90.19 \u00b1 0.83 &\n  90.60 \u00b1 1.22 &\n  92.12 \u00b1 0.74 \\\\\n\\textbf{KEA-S + TA} &\n  91.09 \u00b1 1.78 &\n  {\\ul 92.30 \u00b1 1.69} &\n  {\\ul 88.95 \u00b1 4.96} &\n  90.40 \u00b1 0.92 &\n  90.82 \u00b1 1.30 &\n  91.78 \u00b1 0.56 \\\\\n\\textbf{TA+E} &\n  90.68 \u00b1 2.12 &\n  91.86 \u00b1 1.36 &\n  87.00 \u00b1 4.83 &\n  {\\ul 92.64 \u00b1 1.00} &\n  {\\ul 93.35 \u00b1 1.24} &\n  {\\ul 94.34 \u00b1 0.86} \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "tab: llmres1": "\\begin{table*}[!ht]\n\\footnotesize\n\\caption{Performance of LLMs on real-world text attributed graphs without structural information, we also include the result of GCN (or SAGE for \\products) together with Sentence-BERT features. For \\cora, \\citeseer, \\pubmed, we show the results of the low labeling rate setting. }\n\\label{tab: llmres1}\n    \\centering\n    \\resizebox{0.75\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{cccccc}\n    \\toprule\n        \\textbf{} & \\textsc{\\cora} & \\textsc{\\citeseer} & \\textsc{\\pubmed} & \\textsc{\\arxiv} & \\textsc{\\products} \\\\ \\midrule\n        \\textbf{Zero-shot} & 67.00 \u00b1 1.41 & 65.50 \u00b1 3.53   & 90.75 \u00b1 5.30 & 51.75 \u00b1 3.89  & 70.75 \u00b1 2.48 \\\\ \n        \\textbf{Few-shot} & 67.75 \u00b1 3.53 & 66.00 \u00b1 5.66 & 85.50 \u00b1 2.80  & 50.25 \u00b1 1.06 & 77.75 \u00b1 1.06 \\\\ \n        \\textbf{Zero-shot with COT} & 64.00 \u00b1 0.71 & 66.50 \u00b1 2.82  & 86.25 \u00b1 3.29  & 50.50 \u00b1 1.41 & 71.25 \u00b1 1.06 \\\\ \n        \\textbf{Few-shot with COT} & 64.00 \u00b1 1.41  & 60.50 \u00b1 4.94  & 85.50 \u00b1 4.94 & 47.25 \u00b1 2.47 & 73.25 \u00b1 1.77 \\\\ \n        \\textbf{GCN/SAGE} & 82.20 \u00b1 0.49  & 71.19 \u00b1 1.10 & 81.01 \u00b1 1.32 & 73.10 \u00b1 0.25 &  82.51 \u00b1 0.53\\\\ \\bottomrule\n    \\end{tabular}}\n\\end{table*}",
            "table:reason": "\\begin{table}[h!]\n\\caption {A wrong but reasonable prediction made by LLMs }\n\\label{table:reason}\n\\centering\n\\rule{\\linewidth}{2pt}\n\\parbox{\\linewidth}{\n\\vspace{5pt}\n         \\textbf{Paper:} The Neural Network House: An overview; Typical home comfort systems utilize only rudimentary forms of energy management and conservation. The most sophisticated technology in common use today is an automatic setback thermostat. Tremendous potential remains for improving the efficiency of electric and gas usage... \\newline\n    \\textbf{Ground Truth: } Reinforcement Learning \\newline\n    \\textbf{LLM's Prediction: } Neural Networks \\newline\n\\vspace{2pt}\n}\n\\rule{\\linewidth}{2pt}\n\\vspace{5pt}\n\\end{table}",
            "table:cot_wrong": "\\begin{table}[h!]\n\\caption {An example that LLMs generate CoT processes not matching with ground truth labels}\n\\label{table:cot_wrong}\n\\centering\n\\rule{\\linewidth}{2pt}\n\\parbox{\\linewidth}{\n\\vspace{5pt}\n         \\textbf{Paper:} The Neural Network House: An overview.: Typical home comfort systems utilize only rudimentary forms of energy management and conservation. The most sophisticated technology in common use today is an automatic setback thermostat. Tremendous potential remains for improving the efficiency of electric and gas usage... \\newline\n    \\textbf{Generated Chain-of-thoughts:} The paper discusses the use of neural networks for intelligent control and mentions the utilization of neural network reinforcement learning and prediction techniques. Therefore, the most likely category for this paper is 'Neural Networks'. \\newline\n    \\textbf{Ground Truth: } Reinforcement Learning \\newline\n    \\textbf{LLM's Prediction: } Neural Networks \\newline\n\\vspace{2pt}\n}\n\\rule{\\linewidth}{2pt}\n\\vspace{5pt}\n\\end{table}",
            "tab: arxiv": "\\begin{table}[!ht]\n\\footnotesize\n\\centering\n\\caption{Performance of LLMs on OGB-Arxiv dataset, with three different label designs. }\n\\label{tab: arxiv}\n\\begin{tabular}{cccc}\n\\toprule\n      & \\textbf{Strategy 1} & \\textbf{Strategy 2} & \\textbf{Strategy 3} \\\\ \\cmidrule{2-4}\n\\textsc{\\arxiv} & 48.5       &   51.8    &  74.5        \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab: llmres2": "\\begin{table*}[!ht]\n\\footnotesize\n\\caption{Performance of LLMs on real-world text attributed graphs with summarized neighborhood information. For \\cora, \\citeseer, \\pubmed, we show the results of the low labeling rate setting. We also include the result of GCN (or SAGE for \\products) together with Sentence-BERT features.}\n\\label{tab: llmres2}\n    \\centering\n    \\resizebox{0.75\\linewidth}{!}{\n    \\begin{tabular}{cccccc}\n    \\toprule\n        \\textbf{} & \\textsc{\\cora} & \\textsc{\\citeseer} & \\textsc{\\pubmed} & \\textsc{\\arxiv} & \\textsc{\\products} \\\\ \\midrule\n        \\textbf{Zero-shot} & 67.00 \u00b1 1.41 & 65.50 \u00b1 3.53   & 90.75 \u00b1 5.30 & 51.75 \u00b1 3.89  & 70.75 \u00b1 2.48 \\\\ \n        \\textbf{Few-shot} & 67.75 \u00b1 3.53 & 66.00 \u00b1 5.66 & 85.50 \u00b1 2.80  & 50.25 \u00b1 1.06 & 77.75 \u00b1 1.06 \\\\ \n        \\textbf{Zero-Shot with 2-hop info} & 71.75 \u00b1 0.35 & 62.00 \u00b1 1.41 & 88.00 \u00b1 1.41 & 55.00 \u00b1 2.83 & 75.25 \u00b1 3.53 \\\\ \n        \\textbf{Few-Shot with 2-hop info} & 74.00 \u00b1 4.24 & 67.00 \u00b1 4.94 & 79.25 \u00b1 6.71 & 52.25 \u00b1 3.18 & 76.00 \u00b1 2.82  \\\\ \n        \\textbf{GCN/SAGE} & 82.20 \u00b1 0.49  & 71.19 \u00b1 1.10 & 81.01 \u00b1 1.32 & 73.10 \u00b1 0.25 &  82.51 \u00b1 0.53\\\\ \\bottomrule\n    \\end{tabular}}\n\\end{table*}",
            "tab:pubcase1": "\\begin{table}[h!]\n\\caption {GNNs and LLMs with structure-aware prompts are both wrong}\n\\label{tab:pubcase1}\n\\centering\n\\rule{\\linewidth}{2pt}\n\\parbox{\\linewidth}{\n\\vspace{5pt}\nPaper:\n Title: C-reactive protein and incident cardiovascular events among men with diabetes. \\\\\nAbstract: OBJECTIVE: Several large prospective studies have shown that baseline levels of C-reactive protein (CRP)  ... \\\\ \nNeighbor Summary: \n This paper focuses on different aspects of \\textbf{type 2 diabetes} mellitus. It explores the levels of various markers such as tumor necrosis factor-alpha, interleukin-2 ... \\\\\n \\textbf{Ground truth: \"Diabetes Mellitus Type 1\"} \\\\\n \\textbf{Structure-ignorant prompts: \"Diabetes Mellitus Type 1\"} \\\\\n \\textbf{Structure-aware prompt: \"Diabetes Mellitus Type 2\"} \\\\\n \\textbf{GNN: \"Diabetes Mellitus Type 2\"} \\\\\n\\vspace{2pt}\n}\n\\rule{\\linewidth}{2pt}\n\\vspace{5pt}\n\\end{table}",
            "tab:anno": "\\begin{table}[!ht]\n\\centering\n\\caption{Performance of GCN trained on either pseudo labels generated by LLMs, or ground truth labels}\n\\begin{tabular}{lcc}\n\\toprule\n                    & \\textbf{\\cora} & \\textbf{\\pubmed} \\\\ \\hline\n                    \\multicolumn{3}{l}{\\textit{Using pseudo labels}}                \\\\ \n\\textbf{20 shots $\\times$ \\#class} & 64.95 \u00b1 0.98  & 71.70 \u00b1 1.06    \\\\ \n% \\textbf{Strategy 2} & 63.45 \u00b1 0.22  & 73.45 \u00b1 0.48    \\\\ \n\\multicolumn{3}{l}{\\textit{Using ground truth}}                \\\\ \n\\textbf{3 shots per class}   & 52.63 \u00b1 1.46         & 59.35 \u00b1 2.67    \\\\ \n\\textbf{5 shots per class}   & 58.97 \u00b1 1.41         & 65.98 \u00b1 0.74           \\\\ \n\\textbf{10 shots per class}  & 69.87 \u00b1 2.27         & 71.51 \u00b1 0.77           \\\\ \\bottomrule\n\\end{tabular}\n\\label{tab:anno}\n\\end{table}",
            "table:conftry": "\\begin{table}[h!]\n\\caption{Prompts used to generate neighbor summary}\n\\label{table:conftry}\n\\centering\n\\rule{\\linewidth}{2pt}\n\\parbox{\\linewidth}{\n\\vspace{5pt}\n\\textbf{Instruction} \\\\\n    Output the confidence level in the range of 0 to 1 and  the most 1 possible category of this paper as a python dict, like {''prediction\": \"XX\", \"confidence\": \"XX\"}\n\\vspace{2pt}\n}\n\\rule{\\linewidth}{2pt}\n\\vspace{5pt}\n\\end{table}",
            "tab:ood": "\\begin{table}[!ht]\n\\centering\n\\caption{OOD performance comparison. ``Val'' means the results on the IID validation sets. ``Test'' indicates the results of the OOD test sets. We can see that LLMs-as-Predictors consistently outperform the best GNN-based OOD baselines. Moreover, the gap between IID performance and OOD performance is small.}\n\\label{tab:ood}\n\\begin{tabular}{@{}lccc@{}}\n\\toprule\n                          & Val   & Test  & Best baseline (test) \\\\ \\midrule\n\\textbf{concept degree}   & 73.01 & 72.79 & 63.00                \\\\\n\\textbf{covariate degree} & 70.23 & 68.21 & 59.08                \\\\\n\\textbf{concept time}     & 72.66 & 71.98 & 67.45                \\\\\n\\textbf{covariate time}   & 74.28 & 74.37 & 71.34                \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab: alldata": "\\begin{table}[!ht]\n    \\centering\n    \\caption{Statistics of the graph datasets.}\n    \\label{tab: alldata}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{lcccc}\n    \\toprule\n         Dataset &  \\#Nodes &\\#Edges & Task  & Metric\\\\\n         \\midrule\n         \\cora~\\cite{McCallum2000AutomatingTC}& 2,708 &5,429 & 7-class classif. & Accuracy\\\\\n         \\citeseer\\textsuperscript{*}~\\cite{giles1998citeseer} & 3,186 & 4,277 & 6-class classif.& Accuracy \\\\\n         % \\\\\n         \\pubmed~\\cite{Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008}& 19,717 & 44,338 & 3-class classif.& Accuracy\n         \\\\\n         \\arxiv~\\cite{hu2020open} & 169,343 & 1,166,243 & 40-class classif.& Accuracy\n         \\\\\n         \\products~\\cite{hu2020open} & 2,449,029 & 61,859,140 & 47-class classif. & Accuracy\t\\\\\n         \\bottomrule\n    \\end{tabular}}\n\\end{table}",
            "table:pubmed_example": "\\begin{table}[H]\n\\caption {An illustrative example for \\pubmed}\n\\label{table:pubmed_example}\n\\centering\n\\rule{\\linewidth}{2pt}\n\\parbox{\\linewidth}{\n\\vspace{5pt}\n          Title: Predictive power of sequential measures of albuminuria for progression to ESRD or death in Pima Indians with \\textbf{type 2 diabetes}. \\newline\n          ... (content omitted here) \\newline \n          \\textbf{Ground truth label:} Diabetes Mellitus Type 2\n\\vspace{2pt}\n}\n\\rule{\\linewidth}{2pt}\n\\vspace{5pt}\n\\end{table}"
        },
        "figures": {
            "pipeline": "\\begin{figure*}[!htb]\n    \\centering\n    \\begin{subfigure}[b]{0.48\\textwidth}\n        \\centering\n        \\includegraphics[width=0.8\\textwidth]{figures/LLM_feature.pdf}\n        \\caption{\\textit{An illustration of LLMs-as-Enhancers}, where LLMs pre-process the text attributes, and GNNs eventually make the predictions. Three different structures for this pipeline are demonstrated in Figure~\\ref{llm_pipeline}.}\n        \\label{llm_enhancer}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.50\\textwidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figures/llm_single.pdf}\n        \\caption{\\textit{An illustration of LLMs-as-Predictors}, where LLMs directly make the predictions. The key component for this pipeline is how to design an effective prompt to incorporate structural and attribute information. }\n        \\label{llm_single}\n    \\end{subfigure}\n    \n    %\\vspace{1em}  % Add some space between the two rows of figures\n    \\caption{Pipelines for integrating LLMs into graph learning. In all figures, we use ``PLM'' to denote small-scale PLMs that can be fine-tuned on downstream datasets,  ``LLM\\textsuperscript{*}'' to denote embedding-visible LLMs, and ``LLM'' to denote embedding-invisible LLMs. } \n    %\\haitao{Make sure PLM and LLM, this expression is generally used in NLP} \n    % \\haitao{ALL caption needs more details, We hope the reviewer can understand the paper with just figure and caption}}\n    \\label{pipeline}\n\\end{figure*}",
            "llm_pipeline": "\\begin{figure*}[h]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{figures/LLM_feature_enhance.pdf}\n    \\caption{Three strategies to adopt LLMs as enhancers. The first two integrating structures are designed for feature-level enhancement, while the last structure is designed for text-level enhancement. From left to right: (1) Cascading Structure: Embedding-visible LLMs enhance text attributes directly by encoding them into initial node features for GNNs. (2) Iterative Structure: GNNs and PLMs are co-trained in an iterative manner. (3) Text-level enhancement structure: Embedding-invisible LLMs are initially adopted to enhance the text attributes by generating augmented attributes. The augmented attributes and original attributes are encoded and then ensembled together. } \n\\label{llm_pipeline}\n\\end{figure*}",
            "fig:textlevel": "\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/textlevel3.pdf}\n    \\caption{Illustrations for TAPE and KEA. TAPE leverages the knowledge of LLMs to generate explanations for their predictions. For KEA, we prompt the LLMs to generate a list of technical terms with their descriptions. The main motivation is to augment the attribute information.}\n    \\label{fig:textlevel}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    h_i^{l}=\\operatorname{UPD}^{l}\\left(h_i^{l-1}, \\operatorname{AGG}_{j \\in \\mathcal{N}(i)} \\operatorname{MSG}^{l}\\left(h_i^{l-1}, h_j^{l-1}\\right)\\right), \n\\end{equation}"
        },
        "git_link": "https://github.com/CurryTang/Graph-LLM"
    },
    "llm_extraction": {
        "dataset": [
            {
                "name": "Cora",
                "description": "Cora is a paper citation dataset with seven categories: ['Rule Learning', 'Neural Networks', 'Case Based', 'Genetic Algorithms', 'Theory', 'Reinforcement Learning', 'Probabilistic Methods'].",
                "proposed_in_paper": false,
                "url": "https://people.cs.umass.edu/~mccallum/data.html",
                "size": "2,708 nodes, 5,429 edges",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Split into two scenarios: low-labeling-rate (20 nodes per class for training) and high-labeling-rate (60% training, 20% validation, 20% testing).",
                "sampling": "Randomly selected 20 nodes from each class for low-labeling-rate; 60% for training in high-labeling-rate.",
                "preprocessing": "Raw text attributes processed as needed."
            },
            {
                "name": "Citeseer",
                "description": "Citeseer is a paper citation dataset with six categories: ['Agents', 'ML', 'IR', 'DB', 'HCI', 'AI']. The dataset consists of 3186 nodes and the text attributes for these nodes must be used for classification tasks.",
                "proposed_in_paper": false,
                "url": null,
                "size": "3,186 nodes, 4,277 edges",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Split similarly to Cora with low and high labeling rate scenarios.",
                "sampling": "Sampling conducted similar to Cora; issues noted regarding missing text attributes for unused nodes.",
                "preprocessing": null
            },
            {
                "name": "Pubmed",
                "description": "Pubmed is a paper citation dataset consisting of scientific journals collected from the PubMed database with three categories: ['Diabetes Mellitus, Experimental', 'Diabetes Mellitus Type 1', 'Diabetes Mellitus Type 2'].",
                "proposed_in_paper": false,
                "url": null,
                "size": "19,717 nodes, 44,338 edges",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Follow similar split settings as used in Cora.",
                "sampling": "Consistently applies sampling methods as with Cora.",
                "preprocessing": null
            },
            {
                "name": "Arxiv",
                "description": "Arxiv is selected from the popular Open Graph Benchmark (OGB) with specific citation information available. It documents various classifications of academic publications.",
                "proposed_in_paper": false,
                "url": "https://ogb.stanford.edu/docs/nodeprop",
                "size": "169,343 nodes, 1,166,243 edges",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Uses official dataset splits as defined in OGB protocols.",
                "sampling": null,
                "preprocessing": null
            },
            {
                "name": "Products",
                "description": "Products is also selected from OGB, providing relevant citation data for typical graph learning tasks. This dataset is very large compared to the others, suitable for inductive tasks.",
                "proposed_in_paper": false,
                "url": "https://ogb.stanford.edu/docs/nodeprop",
                "size": "2,449,029 nodes, 61,859,140 edges",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Official splits from OGB utilized in experiments.",
                "sampling": null,
                "preprocessing": null
            }
        ]
    }
}