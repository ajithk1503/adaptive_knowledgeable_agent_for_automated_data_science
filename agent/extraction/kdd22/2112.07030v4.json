{
    "meta_info": {
        "title": "Clustering with fair-center representation: parameterized approximation  algorithms and heuristics",
        "abstract": "We study a variant of classical clustering formulations in the context of\nalgorithmic fairness, known as diversity-aware clustering. In this variant we\nare given a collection of facility subsets, and a solution must contain at\nleast a specified number of facilities from each subset while simultaneously\nminimizing the clustering objective ($k$-median or $k$-means). We investigate\nthe fixed-parameter tractability of these problems and show several negative\nhardness and inapproximability results, even when we afford exponential running\ntime with respect to some parameters.\n  Motivated by these results we identify natural parameters of the problem, and\npresent fixed-parameter approximation algorithms with approximation ratios\n$\\big(1 + \\frac{2}{e} +\\epsilon \\big)$ and $\\big(1 + \\frac{8}{e}+ \\epsilon\n\\big)$ for diversity-aware $k$-median and diversity-aware $k$-means\nrespectively, and argue that these ratios are essentially tight assuming the\ngap-exponential time hypothesis. We also present a simple and more practical\nbicriteria approximation algorithm with better running time bounds. We finally\npropose efficient and practical heuristics. We evaluate the scalability and\neffectiveness of our methods in a wide variety of rigorously conducted\nexperiments, on both real and synthetic data.",
        "author": "Suhas Thejaswi, Ameet Gadekar, Bruno Ordozgoiti, Michal Osadnik",
        "link": "http://arxiv.org/abs/2112.07030v4",
        "category": [
            "cs.DS",
            "math.CO",
            "G.2.1; F.2.0; F.1.3"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nConsider the problem of forming a representative committee. In\nessence, the task amounts to finding a group of people among a given\nset of candidates, to represent the will of a (usually) larger body of\nconstituents. In computational terms, this can be modeled as a\nclustering problem like \\kmedian: each cluster center is a chosen\ncandidate, and the points in the corresponding cluster are the\nconstituents it best represents. \n\nIn certain scenarios, it may be adequate to consider additional requirements.\nFor instance, it may be necessary that at least a number of the chosen committee\nmembers belong to a certain minority-ethnic background, to ensure that all\ngroups in a society are represented in the decision-making process. \n%\nThis problem was recently formalized as the \\textit{diversity-aware $k$-median}\nproblem (\\divkmedian)~\\cite{thejaswi2021diversity}. As in conventional \\kmedian,\nthe goal is to pick $k$ facilities to minimize the sum of distances from clients\nto their closest facility~\\cite{arya2001local}. In \\divkmedian, however, each\nfacility is associated to an arbitrary number of attributes from a given finite\nset. The solution is required to contain at least a certain number of facilities\nhaving each attribute (the requirement for each attribute is given to us as part\nof the input).\n\nThejaswi et al. showed that this additional constraint makes \\kmedian harder to\nsolve~\\cite{thejaswi2021diversity}, in the following sense. While \\kmedian is\n\\np-hard to solve exactly, it is \\np-complete to even decide whether a\n\\divkmedian instance has a feasible solution. The rest of their work, thus,\nfocuses on tractable cases and practical heuristics.\n%\nThis work follows a recent line of interest in \\textit{algorithmic fairness},\nwhich has attracted significant attention in recent years. In the design of fair\nalgorithms, additional constraints are imposed on the objective function to\nensure equitable ---or otherwise desirable--- outcomes for the different groups\npresent in the\ndata~\\cite{zafar2017fairness,dwork2018decoupled,chierichetti2017fair,schmidt2019fair,huang2019coresets,backurs2019scalable,bercea2019cost}. \n\n\n\n\\mpara{Contributions.} In this paper we provide a much more comprehensive analysis of\n\\divkmedian (\\divkmeans resp.), addressing computational complexity,\napproximation algorithms, and practical heuristics for the problems. In particular, we\ngive the first known and tight approximation results for the problems.\n\nSince we know that \\divkmedian does not admit polynomial-time\napproximation algorithms~\\cite{thejaswi2021diversity},\nwe first focus on \\textit{fixed-parameter\n  tractability}~\\cite{cygan2015parameterized}. That is, we seek to\nanswer the following question: can we hope to find\nalgorithms with approximation guarantees by allowing their \nrunning time to be exponential in some input parameter? In other\nwords, is approximating \\divkmedian (\\divkmeans resp.) fixed-parameter tractable (\\fpt)?\n\nOur main\nresult in this paper is a positive answer to this question. We give a constant-factor\napproximation algorithm with running time parameterized by $k$ and $t$, the\nnumber of clusters and the number of candidate attributes\nrespectively. We further develop our understanding of \\divkmedian (\\divkmeans resp.) by\ncharacterizing the problem in terms of parameterized complexity. Finally, we consider practical aspects of the problem\nand design effective, practical heuristics which we evaluate through a\nvariety of experiments.\n%\nOur contributions are summarized below:\n\n\\spara{Computational complexity.} We strengthen the known complexity results for\n\\divkmedian (\\divkmeans resp.) by analyzing its parameterized complexity and\ninapproximibility. In particular, for these problems,\n\\squishlisttight\n\\item we give a lower bound for the running time of optimal and approximation\nalgorithms (Corollary~\\ref{corollary:introduction:1});\n%\n\\item we show that finding bicriteria approximation algorithms  is\nfixed-parameter intractable with respect to the number of clusters\n(Proposition~\\ref{proposition:introduction:2});\n%\n\\item we show that they are fixed-parameter intractable with\nrespect to various choices of parameters (Proposition~\\ref{proposition:introduction:3}).\n\\squishend\n%\n\\spara{Approximation algorithms.}\n\\squishlisttight\n\\item We give the first and tight fixed-parameter\ntractable constant-factor approximation algorithm for\n\\divkmedian (\\divkmeans resp.) w.r.t. $k$ and $t$ (Theorem~\\ref{theorem:mainfptapx}).\n%\n\\item We give a faster and tight dynamic programming algorithm for deciding the\nfeasibility (Theorem~\\ref{thm:dpfeasibility}). This yields a faster bicriteria\napproximation algorithm (Theorem~\\ref{theorem:localsearch}).\n\\squishend\n%\n\\spara{Practical heuristics and empirical evaluation.}\n\\squishlisttight\n\\item Despite their theoretical guarantees, the methods discussed above are\nimpractical. We propose a practical approach to find feasible solutions based on\nlinear programming. Despite its lack of guarantees, we show how it can be used\nas a building block in the design of effective heuristics.\n%\n\\item We evaluate the proposed methods in a wide variety of experimental\nresults, rigorously conducted on real an synthetic datasets.  In particular, we\nshow that the proposed heuristics are able to reliably and efficiently\nfind feasible solutions of good quality on a variety of real data sets.\n\\squishend\n\nThe rest of the paper is organized as follows. In\nSection \\ref{sec:preliminaries} we introduce notation and basic\nnotions. In Section \\ref{sec:hardness} we present our\n computational complexity analysis, and Section \\ref{sec:results} gives an overview of our main results. Our\nalgorithms are described in Sections \\ref{sec:algorithm} and\n~\\ref{sec:bicri}, and our experimental results in Section\n\\ref{sec:experiments}. An overview of related work is given in Section\n\\ref{sec:related}, while Section \\ref{sec:conclusions} provides\nconcluding remarks.\nSome proofs are deferred to the\nSupplementary.\n%For\n%more details of parameterized complexity theory we suggest the interested reader\n%to check Cygan et al~.\\cite[Chapter~X]{cygan2015parameterized}.\n\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\label{sec:preliminaries}\nIn this section we introduce notation and problem definitions.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\mpara{Notation.}\n\\label{sec:preliminaries:notation}\nGiven a metric space $(U,d)$, a set $C \\subseteq U$ of clients, a set $F\n\\subseteq U$ of facilities and a subset $S \\subseteq F$ of facilities, we denote\nby $\\cost(S)=\\sum_{c \\in C} d(c,S)$ the clustering cost of $S$, where\n$d(c,S)=\\min_{s \\in S} d(c,s)$. \n%\nWe say that $C$ is weighted when every $c \\in C$ is associated to a weight $w_c\n\\in \\mathbb{R}$, and the clustering cost becomes\n$\\cost(S)= \\sum_{c \\in C} w_c \\cdot d(c,S)$.\nSimilarly, for $C' \\subseteq C$ and $S \\subseteq F$, we write \n$\\cost(C',S)=\\sum_{c \\in C'} w_c \\cdot d(c,S)$.  \n%\nFurther, given a collection $\\mathcal{G}=\\{G_1,\\dots,G_t\\}$ of facility groups\nsuch that $G_i \\subseteq F$, for each facility $f \\in F$ we denote by\n$\\vec{\\chi}_f \\in \\{0,1\\}^t$ the \\emph {characteristic vector} of $f$ with\nrespect to $\\mathcal{G}$, and is defined as\n$\\vec{\\chi}_f[i] = 1$ if $f \\in G_i$, $0$ otherwise, for all $i \\in [t]$. For\n$\\eta >0$ and $a \\in \\mathbb{Z}_{\\ge 0}$, we denote $[a]_\\eta \\in \\mathbb{Z}$ as\nthe smallest integer such that $(1+\\eta)^{[a]_\\eta} \\ge a$.\n%\nFor a metric space $(U,d)$, the aspect ratio is defined as $\\Delta =\n\\frac{\\max_{x,y\\in U} d(x,y)}{\\min_{x,y\\in U} d(x,y)}$.\n\nIn this paper we use standard parameterized complexity terminology from \nCygan et al.~\\cite{cygan2015parameterized}.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%\\subsection{Problem definitions} \n%\\label{sec:preliminaries:problem}\n%We start by formally defining the diversity-aware $k$-median problem.\n\n\\begin{definition}[\\bf Diversity-aware $k$-median (\\divkmedian)]\n\\label{def:divkmedian}\nGiven a metric space $(U,d)$, a set $C \\subseteq U$ of clients, \na set $F \\subseteq U$ of facilities,\na collection, called \\textbf{groups}, $\\mathcal{G}=\\{G_1,\\dots,G_t\\}$% \\subseteq 2^F$ \nof facility sets $G_i\n\\subseteq F$, a budget $k \\leq |F|$, and a vector of\nrequirements $\\vec{r}=(r[1],\\dots,r[t])$.\n%, that is, one threshold $r[i] \\leq k$ for each group $G_i$.\n%\nThe problem asks to find a subset of facilities $S \\subseteq F$ of size $k$,\nsatisfying $|S \\cap G_i| \\geq r[i]$ for all $i \\in [t]$, such that the clustering\ncost of $S$, $\\cost(S) = \\sum_{c \\in C} d(c, S)$ is minimized. An instance of\n\\divkmedian is denoted as $I=\\divkins$.\n%\n\\end{definition}\n\nIn {\\bf diversity-aware $k$-means} problem (\\divkmeans), the clustering cost of\n$S \\subseteq F$ is $\\cost(S) = \\sum_{c \\in C} d(c,S)^2$.\nWe denote $r= \\max_{i \\in [t]} r[i]$ and we assume $\\Delta$ is polynomially bounded \\wrt $|U|$~\\cite{cohen2019tight}.\n\n\n\\iffalse\nAnother problem of our interest is a variant of \\kmedian with\n$p$-partition matroid constraints, which is formally define as\nfollows.\n\n\\begin{definition}[\\kmedian with $p$-Partition Matroid (\\kmedianppm)]\n\\label{def:kmedianpm}\nGiven a metric space $(U,d)$, a set of clients $C \\subseteq U$, a set of\nfacilities $F \\subseteq U$ and a collection $\\mathcal{E}=\\{E_1,\\dots,E_p\\}$ of\ndisjoint facility groups called a $p$-{\\em partition matroid}.\n%\nThe problem asks to find a subset of facilities $S \\subseteq F$ of size $k$,\ncontaining at most one facility from each group $E_i$, so that the clustering\ncost of $S$, $\\cost(S)=\\sum_{c\\in C}d(c,S)$ is minimized. An instance of\n\\kmedianppm is specified as $J=((U,d),F,C,\\mathcal{E},k)$.\n\\end{definition}\n\\fi\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
            },
            "section 3": {
                "name": "Hardness",
                "content": "\n\\label{sec:hardness}\nTo motivate the choice of parameters for designing \\fpt algorithms,\nwe characterize the hardness of \\divkmedian (\\divkmeans resp.) based on standard\ncomplexity theory assumptions.\n%\nObserve that \\divkmedian (\\divkmeans resp.) problems are an amalgamation of two independent\nproblems: ($i$) finding a subset of facilities $S \\subseteq F$ of size $|S|=k$\nthat satisfies the requirements $|S \\cap G_i| \\geq \\req{i}$ for all $i \\in [t]$, and\n($ii$) minimizing the \\kmedian (\\kmeans resp.) clustering cost.  To remain consistent with the\nproblem statement of Thejaswi et al.~\\cite{thejaswi2021diversity}, we refer to\nsubproblem ($i$) as the requirement satisfiability problem (\\reqsat), where the cost of clustering is\nignored. If we ignore the requirements in ($i$) we obtain the classical \\kmedian\n(\\kmeans resp.)\nformulation, which immediately establishes the \\np-hardness of \\divkmedian (\\divkmeans resp.).\n%Furthermore, a parameterized reduction from\n%$k$-dominating set problem (\\dominatingset) to \\divkmediannif problem preserving\n%parameter $k$, establishes that \\divkmediannif captures\n%\\dominatingset~\\cite[Lemma~1]{thejaswi2021diversity}.\n\n\n\n\nA reduction of the vertex cover problem to \\reqsat is sufficient to show\nthat \\divkmedian (\\divkmeans resp.) are inapproximable to any multiplicative factor in\npolynomial-time even if all the subsets are of size\ntwo~\\cite[Theorem~3]{thejaswi2021diversity-arxiv}.\n%\nThe \\wtwo-hardness of \\divkmedian (\\divkmeans resp.) with respect to parameter $k$ is a consequence\nof the fact that \\kmedian (\\kmeans resp.) are \\wtwo-hard, which follow from a reduction by Guha\nand Kuller~\\cite{guha1998greedy}.\n%\nMore strongly, combining the result of \\cite[Lemma~1]{thejaswi2021diversity}\nwith the strong exponential time hypothesis (\\seth)~\\cite{impagliazzo2001on}, we conclude the following: if we only consider the parameter $k$, a\ntrivial exhaustive-search algorithm is our best hope for finding an optimal, or\neven an approximate, solution to \\divkmedian (\\divkmeans resp.). The proof is\nin Supplementary~\\ref{app:otherproofs}.\n%\n%\n\\begin{corollary}\n\\label{corollary:introduction:1}\nAssume \\seth. For all $k \\geq 3$ and $\\epsilon >0$, there exists no \n$\\bigO(|{F}|^{k - \\epsilon})$\nalgorithm to solve \\divkmedian (\\divkmeans resp.).\nFurthermore, there exists no\n$\\bigO(|{F}|^{k - \\epsilon})$\nalgorithm to approximate \\divkmedian (\\divkmeans resp.) to any multiplicative factor.\n\\end{corollary}\n\n\nGiven the $\\wtwo$-hardness of \\divkmedian with respect to parameter $k$, it is\nnatural to consider relaxations of the problem. An obvious question is\nwhether we can approximate \\divkmedian  in \\fpt time \\wrt $k$, if we are allowed\nto open, say, $f(k)$ facilities instead of $k$, for some function $f$.\nUnfortunately, this is also unlikely as \\divkmedian captures\n\\dominatingset~\\cite[Lemma~1]{thejaswi2021diversity}, and even finding a\ndominating set of size $f(k)$ is \\wone-hard~\\cite{karthik2019on}. The proof is\nin Supplementary~\\ref{app:otherproofs}.\n%\n\\begin{proposition}\n\\label{proposition:introduction:2}\nFor any function $f(k)$, finding $f(k)$ facilities that approximate the\n\\divkmedian (\\divkmeans resp.) cost to any multiplicative factor in \\fpt time with respect to\nparameter $k$ is $\\wone$-hard.\n\\end{proposition}\n\n\n\n\nA possible way forward is to identify other parameters of the problem, to design\n\\fpt algorithms to solve the problem optimally. As established earlier,\n\\kmedian is a special case of \\divkmedian when $t=1$. This immediately rules out\nan exact \\fpt algorithm for \\divkmedian with respect to parameters $(k,t)$.\nFurthermore, we caution the reader against entertaining the prospects of other,\narguably natural, parameters, such as the maximum lower bound $r =\n\\max_{i\\in[t]} r[i]$ (ruled out by the relation $r[i]\\leq k$) and the maximum\nnumber of groups a facility can belong to \n$\\mu =\\max_{f \\in F}(|{G_i}: f \\in G_i, i \\in [t]|)$ \n(ruled out by the relaxation $\\mu \\leq t$).\n%\n\\begin{proposition}\n\\label{proposition:introduction:3}\nFinding an optimal solution for \\divkmedian  (\\divkmeans resp.) is $\\wtwo$-hard with respect to parameters\n$(k,t)$, $(r,t)$ and $(\\mu, t)$.\n\\end{proposition}\n\nThe above intractability results thwart our hopes of solving the \\divkmedian\nproblem optimally in \\fpt time. We then ask, are there any parameters of the\nproblem that allow us to find an approximate solution in \\fpt time? We answer\nthis question positively, and present a tight \\fpt-approximation algorithm \\wrt\n$(k,t)$, the number of chosen facilities and the number\nof facility groups.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
            },
            "section 4": {
                "name": "Our results",
                "content": "\n\\label{sec:results}\nOur main result, stated below, shows that a constant-factor approximation of\n\\divkmedian (\\kmeans resp.) can be achieved in \\fpt time with respect to parameters $(k,t)$. In\nfact, somewhat surprisingly, the factor is the same as the one achievable for\n\\kmedian (\\kmeans resp.). So despite the stark contrast in their polynomial-time\napproximability, the \\fpt landscape is rather similar for these two problems. We\nalso note that under the gap-exponential time hypothesis (\\gapeth), the\napproximation ratio achieved in Theorem~\\ref{theorem:mainfptapx} is essentially\ntight for any \\fpt algorithm \\wrt $(k,t)$. This follows from combining the fact\nthat the case $t=1$ is essentially \\kmedian (\\kmeans resp.) with the results of Cohen-Addad et\nal.~\\cite{cohen2019tight}, which assuming \\gapeth gives a lower bound for any\n\\fpt algorithm \\wrt $k$. This bound matches their ---and our--- approximation\nguarantee.\n%\n\\begin{theorem} \n\\label{theorem:mainfptapx}\nFor every $\\epsilon>0$, there exists a randomized $(1 + \\frac{2}{e}\n+\\epsilon)$-approximation algorithm for \\divkmedian with time\ntime $f(k,t,\\epsilon) \\cdot \\poly(|U|)$, where $f(k,t,\\epsilon) =\n\\bigO\\left(\\left( \\frac{2^t k^3 \\log^2 k}{\\epsilon^2 \\log(1+\\epsilon)}\\right)^k \\right)$. \nFurthermore, the approximation ratio is tight for any \\fpt algorithm\n\\wrt $(k,t)$, assuming \\gapeth.\nFor \\divkmeans, with the same running time, we obtain $(1 + \\frac{8}{e} +\\epsilon)$-approximation, which is tight assuming \\gapeth.\n\\end{theorem}\n\n%Our algorithmic result is motivated by the \\fpt inapproximability\n%results of \\divkmedian with respect to various natural parameters. For\n%a precise exposition of these results, please refer to Section~\\ref{sec:hard}.\nFinally, in Section~\\ref{sec:bicri} we will point out a simple observation: by\nrelaxing the upper bound on the number of facilities to at most $2k$, we can use\na practical local-search heuristic and obtain a slightly weaker quality\nguarantee with better running time bounds.\n%\nTo achieve this we make use of Theorem~\\ref{thm:dpfeasibility}.\n%\n\\begin{theorem}\n\\label{theorem:localsearch} \nFor every $\\epsilon > 0$, there exists a\nrandomized $(3+\\epsilon)$-approximation algorithm that outputs at most $2k$\nfacilities for the \\divkmedian problem in time $\\bigO( 2^t(r+1)^t \\cdot \\poly(|U|,\n1/\\epsilon))$.\n\\end{theorem}\n\n  \n%Finally, we note that the enumeration strategy we develop for \\divkmedian implies\n%an exact \\fpt algorithm for the set multicover problem without repeated sets\n%(\\setmc). We believe that this is the first such algorithm for this\n%variant of \\setmc.\n%\n%\\begin{theorem}\n%\tThere exists a $f(k,n)$-time deterministic\n%        algorithm for the \\setmcwr problem without repeated sets,\n%        where $f(k,n)=\\bigO(2^{kn})$.\n%\\end{theorem}\n\n%In addition to algorithmic results we also present \\fpt inapproximability\n%results with respect to various natural parameters of the \\divkmedian problem.\n\n\\iffalse\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                "subsection 4.1": {
                    "name": "Our techniques",
                    "content": "\nTo achieve the result stated in Theorem~\\ref{theorem:mainfptapx}, first we\npartition the facility set $F$ into $2^t$ disjoint subsets. The facilities in\neach subset share the same characteristic vector, which encodes the groups they\nbelong to. More precisely, the {\\em characteristic vector} of a facility $f$ is\na vector of length $t$ such that its $i$-th index is set to $1$ if $f\\in G_i$,\n$0$ otherwise.  Next, we enumerate over all $k$-multisets of the\npartition, but retain only those corresponding to a\nfeasible solution - meaning, picking one facility from each subset of such\n$k$-multiset satisfies the lower-bound requirements. To this end, we leverage a combinatorial structure of the partition to find such $k$-multisets in time $\\bigO(2^{kt} \\poly(|U|))$.\nFinally, we show that at least one such $k$-multiset is guaranteed\nto contain an optimal set of facilities, provided a feasible solution exists.\nIn order to achieve this, for each $k$-multiset containing a feasible solution\nwe create an instance of \\kmedianpm\\footnote{Roughly, \\kmedianpm is \\kmedian with $k$-partition matroid constraint. Refer  Section~\\ref{sec:preliminaries:problem} for formal definition.} , where the $k$-multiset\ndetermines the partition matroid constraint. At this point, the stage will be\nset to use the \\fpt algorithm of Cohen-Addad et al.~\\cite{cohen2019tight}, which\napproximately solves \\kmedian, to achieve the approximation factor of\nTheorem~\\ref{theorem:mainfptapx}.\n\n\nOn the other hand, Theorem~\\ref{theorem:localsearch} follows from the\nobservation that, if we pad our facility set to satisfy the\nlower-bound requirements, we can employ the classic result of Arya et\nal.~\\cite{arya2004local} to bound the local-search approximation factor. Note\nthat as observed by Thejaswi et al.~\\cite[Section~5.1]{thejaswi2021diversity},\nthe local-search algorithm has an unbounded locality gap in \\divkmedian, so this\ntrick only applies to \\fpt algorithms. Bounding the approximation factor of\nlocal search restricted to $k$-facilities is left as an open problem.\n\n\nThe rest of the paper is structured as follows: in Section~\\ref{sec:related} we\nreview related work, and continue to introduce notation and formal definition of\nthe problems in Section~\\ref{sec:preliminaries}. In Section~\\ref{sec:algorithm},\nwe present our main algorithmic results.\n\n\\note{}\n\\fi\n\n"
                }
            },
            "section 5": {
                "name": "Algorithms",
                "content": "\n\\label{sec:algorithm}\nIn this section we present an \\fpt approximation algorithm for \\divkmedian. \nFor \\divkmeans, the ideas are similar.\nThroughout the section, by \\fpt we imply \\fpt \\wrt $(k,t)$.\n\n\n\nA birds-eye view of our algorithm (see Algorithm~\\ref{algo:divkmed}) is as follows: Given a feasible instance\nof \\divkmedian, we first carefully enumerate collections of\nfacility subsets that satisfy the\nlower-bound requirements (Section~\\ref{sec:algorithm:finding}). \n%\nFor each such collection, we obtain a constant-factor approximation of the\noptimal cost of the collection (Section~\\ref{sec:algoritheorem:fpt}). \n%\n%Since the instance is feasible, our algorithm will enumerate some collection\n%containing an optimal solution.\n%Hence the approximate solution of this collection will also be an approximate solution of $I$.\nSince at least one of these feasible \nsolutions is optimal, \n%---provided the instance is feasible in the first place---, \nthe corresponding approximate solution will be an approximate\nsolution for the \\divkmedian problem. \n%\nA key ingredient for obtaining a constant factor approximation in \\fpt\ntime is to shrink the set of clients. For\nthis we rely on the notion of coresets (Section ~\\ref{sec:algoritheorem:coresets}).\n\n\n%\\todo{Ameet: we can skip the reference to section 6.}\n%Additionally, we observe (Section~\\ref{sec:bicri}) that by padding the solution\n%of a local-search heuristic or any other approximation algorithm for the\n%classical \\kmedian with a facility set satisfying the lower-bound requirements,\n%we obtain a bicriteria \\fpt approximation algorithm for \\divkmedian.\n%the facility set to satisfy the\n%lower-bound requirements, we can employ a local-search heuristic ---or\n%any other approximation algorithm--- to obtain a\n%bicriteria approximation algorithm. \n%In particular, this approach yields $(3 + \\epsilon)$-approximate solution with\n%at most $2k$ facilities. \n\nIn the exposition to follow, we will refer to the problem of \\kmedian with \n$p$-partition matroid constraints:\n\n\\begin{definition}[\\kmedian with $p$-Partition Matroid (\\kmedianppm)]\n\\label{def:kmedianpm}\nGiven a metric space $(U,d)$, a set of clients $C \\subseteq U$, a set of\nfacilities $F \\subseteq U$ and a collection $\\mathcal{E}=\\{E_1,\\dots,E_p\\}$ of\ndisjoint facility groups called a $p$-{\\em partition matroid}.\n%\nThe problem asks to find a subset of facilities $S \\subseteq F$ of size $k$,\ncontaining at most one facility from each group $E_i$, so that the clustering\ncost of $S$, $\\cost(S)=\\sum_{c\\in C}d(c,S)$ is minimized. An instance of\n\\kmedianppm is specified as $J=((U,d),F,C,\\mathcal{E},k)$.\n\\end{definition}\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                "subsection 5.1": {
                    "name": "\\pattern",
                    "content": "\n\\label{sec:algorithm:finding}\nWe start by defining the concept of {\\pattern}.\n%To begin, let us clarify what a \\pattern is.  Recall that in the \\divkmedian\n%problem we are given a collection of possibly intersecting facility groups\nGiven an instance $I=\\divkins$ of \\divkmedian, where\n$\\mathcal{G}=\\{G_1,\\dots,G_t\\}$, consider the set  $\\{\\charvec_f\\}_{f\\in F}$ of the characteristic vectors of $F$.\n%We associate each facility $f \\in F$ with a\n%{\\em characteristic vector} $\\charvec_f \\in \\{0,1\\}^t$, such that, for $i \\in [t]$, the index\n%$\\charvec_f[i]$ is set to $1$ if $f \\in G_i$, $0$ otherwise. \nFor each $\\vec{\\gamma} \\in \\{0,1\\}^t$, let \n$\\patternset{\\vec{\\gamma}} = \\{f \\in F: {\\charvec}_f=\\vec{\\gamma}\\}$ \ndenote the set of all facilities with\ncharacteristic vector $\\vec{\\gamma}$.  Finally, let $\\charpart=\\{E(\\vec{\\gamma}):\n\\vec{\\gamma} \\in \\{0,1\\}^t\\}$.\n%be the set of mutually disjoint facility groups.\nNote that $\\charpart$ induces a partition on $F$.\n\nGiven a $k$-multiset\n$\\pazocal{E}=\\{E(\\vec{\\gamma}_{i_1}),\\dots,E(\\vec{\\gamma}_{i_k})\\}$, where each $E(\\vec{\\gamma}_{i_j})\n\\in \\charpart$,\nthe \\textit{\\pattern} associated with $\\pazocal{E}$ is the vector obtained by the\nelement-wise sum of the characteristic vectors \n$\\{\\vec{\\gamma}_{i_1},\\dots,\\vec{\\gamma}_{i_k}\\}$, that is, \n$\\sum_{j \\in [k]} \\vec{\\gamma}_{i_j}$.\n%\nA \\pattern is said to be \\textit{feasible} if \n$\\sum_{j \\in [k]} \\vec{\\gamma}_{i_j} \\geq \\vec r$, \nwhere the inequality is taken element-wise.\n\n\n\n\\begin{lemma} \n\\label{lemma:feasiblecp}\nGiven an instance $I=\\divkins$ of \\divkmedian, we can enumerate all the $k$-multisets with feasible constraint pattern in time $\\bigO(2^{tk}t |U|)$. \n\\end{lemma}\n\\begin{proof}\nThere are $|\\charpart|+k-1 \\choose k$\npossible $k$-multisets of $\\charpart$, so enumerating all feasible {\\pattern}s\ncan be done in $\\bigO(|\\charpart|^k t)$ time.\nFurther, the enumeration of $\\charpart$ itself can be done in time $\\bigO(2^t|F|)$, since $|\\charpart| \\leq 2^t$. Hence,\n%The size of the partition $|\\charpart|$ is bounded by $\\min\\{n,2^t\\}$, \n%since there can be at most $\\min\\{n,2^t\\}$ unique characteristic vectors. So the\ntime complexity of enumerating all feasible {\\pattern}s\n%\\footnote{Let \n%$\\mu= \\max{\\big(\\sum_{j \\in [t]} \\charvec_f[i]: f \\in F\\big)}$\n%is the maximum number of groups a facility belongs to, then the size of the\n%partition is bounded by $|\\charpart| = \\sum_{i \\in [\\mu]} {{t}\\choose{\\mu}}$,\n%which is $\\bigO(t^\\mu)$. So the time complexity of enumerating feasible\n%{\\pattern}s with respect to parameters $(t,k,\\mu)$ is $\\bigO(t^{\\mu k})$.}\nis $\\bigO(2^{tk} t |U|)$.\n%\\todo{Ameet: I do not find where we defined n. So, changed.}\n\\end{proof}\nObserve that for every $k$ multiset\n$\\pazocal{E}=\\{E(\\vec{\\gamma}_{i_1}),\\dots,E(\\vec{\\gamma}_{i_k})\\}$ with a\nfeasible  \\pattern, picking an arbitrary facility from each\n$E(\\vec{\\gamma}_{i_j})$ yields a feasible solution to the \\divkmedian instance $I$.\n%Hence, every $k$-multiset feasible constraint corresponds to a feasible\n\n%Next, using the partition set $\\mathcal{E}$, we show a faster algorithm if we are only concerned about the feasibility of a given instance \\divkmedian.\n%\n%\\begin{lemma} \n%\t\\label{lemma:divkmedf}\n%\t\\divkmediannif can be solved in time $\\bigO(2^{t (1 + \\log (k+1))} \\poly(|U|))$.\n%\\end{lemma}\n%\\begin{proof}\n%IDEA: construct a DAG $G$ on the vertices of $\\{0,1,\\cdots,r\\}^t$. Label edges by sets of $\\mathcal{E}$ that are responsible for the edge.\n% Note $|G|= \\bigO((2(r+1))^t)$. Create a sink vertex $v$ and connect it to all the vertices corresponding to the points at least lower bound requirement. Let $u$ be the vertex corresponding to $\\vec{0}$. Then, $I$ is feasible if and only if there is $(k+1)$-path $P$ from $u$ to $v$ in $G$ such that each edge of $P$ has a different label. To find such a path, we can use dynamic program that runs in time $\\bigO(|G| \\poly(|U|)) = \\bigO((2(r+1))^t \\poly(|U|)) = \\bigO(2^{t (1 + \\log (k+1))} \\poly(|U|))$.\n%\\end{proof}\n%Before moving to \\fpt-approximation of the \\divkmedian problem, it is convenient to introduce the concept of coresets.\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 5.2": {
                    "name": "Coresets",
                    "content": "\n\\label{sec:algoritheorem:coresets}\nOur algorithm relies on the notion of coresets. The high-level idea\nis to reduce the number of clients  such that\nthe distortion in the sum of distances is bounded to a multiplicative factor \n$(1 \\pm \\nu)$, for\nsome $\\nu > 0$. Given an instance $((U,d),C,F,k)$ of\nthe \\kmedianpm problem, for every $\\nu > 0$ we can reduce the number of\nclients in $C$ to a weighted set $C'$ of size $|C'|= \\bigO({\\nu}^{-2}k \\log|U|)$.\nWe make use of the coreset construction for \\kmedian by Feldman and\nLangberg~\\cite{feldman2011unified} and extend the approach to \\kmedianpm. To our knowledge, this is the best-known framework for constructing\ncoresets.\n%\n%\n\\begin{definition}[Coreset]\nGiven an instance $I=((U,d),C,F,k)$ of \\kmedian and a constant $\\nu >0$, a\n(strong) {coreset} is a subset of clients $C' \\subseteq C$ with associated weights\n$\\{w_c: c \\in C'\\}$ such that for any subset of facilities $S \\subseteq F$ of size $|S|=k$\nit holds that\n$$\n(1-\\nu) \\cdot \\sum_{c \\in C} d(c,S) \\le \\sum_{c \\in C'} w_c \\cdot d(c,S) \\le\n(1+\\nu)\\cdot \\sum_{c \\in C} d(c,S)\n$$\n\\end{definition}\n\n%\n\\begin{theorem}[\\cite{feldman2011unified}, Theorem~4.9]\n\\label{theorem:coreset}\nGiven a metric instance  $I=((U,d),C,F,k)$ of the \\kmedian problem,\nfor each $\\nu > 0$, $\\delta<\\frac{1}{2}$,\nthere exists a randomized algorithm that, with probability at least $1-\\delta$,\ncomputes\na coreset $C' \\subseteq C$ of\nsize $|C'| = \\bigO({\\nu}^{-2}(k \\log|U| + \\log \\frac{1}{\\delta}))$ in time\n$\\bigO(k(|U| + k)+\\log^2\\frac{1}{\\delta}\\log^2|U|)$. For \\kmeans, with the same runtime, it yields a coreset of size $|C'| = \\bigO({\\nu}^{-4}(k \\log|U| + \\log \\frac{1}{\\delta}))$.\n\\end{theorem}\n%\n%\nObserve that the coresets obtained from the above theorem are also  coresets for \\divkmedian and \\divkmeans resp., as the corresponding objective remain same.\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 5.3": {
                    "name": "\\fpt approximation algorithms",
                    "content": "\n\\label{sec:algoritheorem:fpt}\nIn this section we present our main result. We will first give an\nintuitive overview of our algorithm. As a warm-up, we will describe a\nsimple $(3+\\epsilon)$-\\fpt-approximation algorithm. Then, we will show how\nto obtain a better guarantee, leveraging the recent \\fpt-approximation techniques of \\kmedian.\n\\paragraph*{Intuition}\nGiven an instance $\\divkins$ of \\divkmedian, we\nfirst partition the facility set $F$ into at most $2^t$ subsets $\\mathcal{E} =\n\\{E(\\vec{\\gamma}): \\vec{\\gamma} \\in \\{0,1\\}^t\\}$, such that each\nsubset $E(\\vec{\\gamma})$ corresponds to the facilities with\ncharacteristic vector same as $\\vec\\gamma \\in \\{0,1\\}^t$. Then, using\nLemma~\\ref{lemma:feasiblecp}, we enumerate all $k$-multisets of $\\mathcal{E}$ with\nfeasible \\pattern. For each such $k$-multiset $\\pazocal{E}=\\{E(\\vec{\\gamma}_{1}),\\dots,E(\\vec{\\gamma}_{k})\\}$, we generate an instance $J_\\pazocal{E}=((U,d), \\{E(\\vec{\\gamma}_{1}),\\dots,E(\\vec{\\gamma}_{k})\\},C',k)$ of \\kmedianpm,\n%, whose solutions are feasible to $I$. \nresulting in at most $\\bigO(2^{tk}t|U|)$ instances.\n%\nNext, using Theorem~\\ref{theorem:coreset}, we build a coreset $C'\\subseteq C$ of clients. \nIn our final step, we obtain\nan approximate solution to each \\kmedianpm instance by adapting the techniques\nfrom~\\cite{cohen2019tight}, which we discuss next.\n\n\n\nLet $\\pazocal{E}=\\{E(\\vec{\\gamma}_{1}),\\dots,E(\\vec{\\gamma}_{k})\\}$  be a $k$-multiset of $\\charpart$ with a feasible constraint pattern,\nand let $J_\\pazocal{E}$ be the corresponding feasible \\kmedianpm\ninstance. \nLet $\\tilde{F}^* = \\{\\tilde{f}_i^* \\in E(\\vec{\\gamma}_{i})\\}_{i \\in [k]}$ be an\noptimal solution of $J_\\pazocal{E}$. For each $\\tilde{f}^*_i$, \nlet $\\tilde{c}^*_i \\in C'$ be a closest client, with $d(\\tilde{f}^*_i,\\tilde{c}^*_i) = \\tilde{\\lambda}^*_i$. \nNext, for each $\\tilde{c}^*_i$ and $\\tilde{\\lambda}^*_i$, let $\\tilde{\\Pi}^*_i \\subseteq\nE(\\vec{\\gamma}_{i})$ be the set of facilities $f\\in\nE(\\vec{\\gamma}_{i})$ such that  $d(f,\\tilde{c}^*_i) = \\tilde{\\lambda}^*_i$. Let us call $\\tilde{c}^*_i$ and $\\tilde{\\lambda}^*_i$ as the leader and radius of $\\tilde{\\Pi}^*_i$, respectively.\n Observe\nthat, for each $i \\in [k]$,  $\\tilde{\\Pi}^*_i$ contains  $\\tilde{f}^*_i$. Thus, if only we knew $\\tilde{c}^*_i$ and\n$\\tilde{\\lambda}^*_i$ for all $i \\in [k]$, we would be able to obtain a provably good solution.\n\n\nTo find the closest client $\\tilde{c}^*_i$ and its corresponding distance $\\tilde{\\lambda}^*_i$ in \\fpt time, we employ techniques of Cohen-Addad et al.~\\cite{cohen2019tight}, which they build on the work of Feldman and Langberg~\\cite{feldman2011unified}. \nThe idea is to reduce the search spaces small enough so as to allow brute-force search in \\fpt time. \nTo this end, first, note that, we already have a smaller client set, $|C'| = O(k\\nu^{-2}\\log |U|)$, since $C'$ is a client coreset.\n%to reduce number of clients we use client coreset $C'$,\nHence, to find $\\{\\tilde{c}^*_i\\}_{i \\in [k]}$, we enumerate all ordered $k$-multisets of $C'$ resulting in $\\bigO((k\\nu^{-2}\\log |U|)^k)$\ntime.\n%\nThen, to bound the search space of $\\lambda^*_i$ (which is at most $\\Delta = \\poly(|U|)$), we discretize the interval $[1,\\Delta]$ to\n$[[\\Delta]_\\eta]$, for some $\\eta >0$. Note that $[\\Delta]_\\eta \\le\n\\lceil \\log_{1+\\eta} \\Delta\\rceil = O(\\log |U|)$. Hence, enumerating all\nordered $k$-multisets of $[[\\Delta]_\\eta]$, we spend at most\n$\\bigO(\\log^k |U|)$ time. Thus, the total time for this step, guaranteeing $\\tilde{c}^*_i$ and\n$\\tilde{\\lambda}^*_i$ in some enumeration, is $\\bigO((k\\nu^{-2}\\log^2 |U|)^k)$, which is \\fpt.\n\nNext, using the facilities in $\\{\\tilde{\\Pi}^*_i\\}_{i \\in[k]}$, we find an approximate\nsolution for the instance $J_\\pazocal{E}$.\nAs a warm-up, we show in Lemma~\\ref{lemma:threeapx}\nthat picking exactly one facility from each $\\tilde{\\Pi}^*_i$ arbitrarily already\ngives a $(3+\\epsilon)$ approximate solution.\n%\nFinally, in Lemma~\\ref{lemma:partition}, we obtain a better\napproximation ratio by modeling the \\kmedianpm problem as a problem of\nmaximizing a monotone\nsubmodular function, relying on the ideas of Cohen-Addad et\nal~\\cite{cohen2019tight}.\n\n\\begin{lemma} \n\\label{lemma:threeapx}\nFor every $\\epsilon>0$, there exists a randomized $(3+\\epsilon)$-approximation\nalgorithm for the \\divkmedian problem which runs in time $f(k,t,\\epsilon) \\cdot\n\\poly(|U|)$, where\n$f(k,t,\\epsilon)=\\bigO((2^t \\epsilon^{-2} k^2 \\log k)^k)$.\n\\end{lemma}\n\\begin{proof} Let $I=\\divkins$ be an instance of \\divkmedian.\nLet  $J=((U,d),C, \\{E_1^*,\\dots,E_k^*\\},k)$ be an instance of \\kmedianpm corresponding\nto an optimal solution of $I$. That is, for some optimal solution $F^*=\\{f_1^*, \\dots,\nf_k^*\\}$ of $I$, we have $f_j^*\\in E_j^*$. Let $c^*_j \\in C'$ be the closest client to $f^*_j$, for $j \\in [k]$, with $d(f^*_j,c^*_j) = \\lambda_j$.\nNow, consider the enumeration iteration where leader set is $\\{c^*_j\\}_{j \\in [k]}$ and the radii is $\\{\\lambda_j^*\\}$. \nThe construction is illustrated in\nFigure~\\ref{fig:mainfptapx}.\n \n\nWe define $\\Pi^*_i$ to be the set of facilities in $E(\\vec{\\gamma}_i^*)$\nat a distance of at most $\\lambda_i^*$ from $c_i^*$. \nWe will now argue that picking one arbitrary facility from each $\\Pi^*_i$ gives a\n$3$-approximation with respect to an optimal pick.\n%In more detail, a solution for \\kmedianpm must pick exactly one facility from\n%each $E(\\vec{\\gamma}_j^*)$ for all $j \\in [k]$. Let $F^*=\\{f^*_1,\\dots,f^*_k\\}$\n%be an optimal solution such that $f^*_j \\in E(\\vec{\\gamma}_j^*)$ \n%for each $j \\in [k]$. \n%\nLet $C^*_j \\subseteq C'$ be a set of clients assigned to each facility $f^*_j$\nin optimal solution. Let $\\{f_1,\\dots,f_k\\}$ be the  arbitrarily chosen\nfacilities, such that $f_j \\in \\Pi^*_j$. Then for any $c \\in C_j$\n\\[\nd(c,f_j) \\leq d(c,f_j^*) + d(f_j^*,c_j^*) + d(c_j^*,f_j)).\n\\]\n\nBy the choice of $c_j^*$ we have \n$d(f_j^*,c_j^*) + d(c_j^*,f_j)) \\leq 2 \\lambda_j^* \\leq 2 d(c,f_j^*)$, \nwhich implies\n$\\sum_{c \\in C_j} d(c,f_j) \\leq 3 \\sum_{c \\in C_j} d(c,f_j^*).$\n%\nBy the properties of the coreset and bounded discretization\nerror~\\cite{cohen2019tight}, we obtain the approximation stated in the lemma.\n\\end{proof}\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nWe will now focus on our main result, stated in\nTheorem~\\ref{theorem:mainfptapx}. As mentioned before, we build upon the\nideas for \\kmedian of Cohen-Addad et al. of~\\cite{cohen2019tight}. \nTheir algorithm, however, does not apply\ndirectly to our setting, as we have to ensure that the chosen\nfacilities satisfy the constraints.\n\n\n\nA key observation is that by relying on the partition-matroid\nconstraint of the auxiliary submodular optimization problem, we can ensure that the\noutput solution will satisfy the \\pattern. Since at least one \\pattern\ncontains an optimal solution, we obtain the advertised approximation factor.\n\n\n\nIn the following lemma, we argue that this is indeed the case. Next, we\nwill provide an analysis of the running time of the algorithm. This\nwill complete the proof of Theorem~\\ref{theorem:mainfptapx}.\n%\n\\begin{lemma} \n\\label{lemma:partition}\nLet  $I=\\divkins$ be an instance of \\divkmedian to Algorithm~\\ref{algo:divkmed}\nand  $F^*=\\{f_1^*, \\dots, f_k^*\\}$ be an optimal solution of $I$.\nLet $J=((U,d), \\{E_1^*,\\cdots,E_k^*\\},C',k)$ be an instance of \\kmedianpm\ncorresponding to $F^*$, i.e,$f_i^* \\in E^*_i, i \\in [k]$.\nOn input $(J,\\epsilon')$,\nAlgorithm~\\ref{algo:kmedpm} outputs a set $\\hat{S}$ satisfying $\\cost(\\hat{S})\n\\leq (1+\\frac{2}{e}+\\epsilon)\\cost(F^*)$. Similarly, for \\divkmeans, $\\cost(\\hat{S})\n\\leq (1+\\frac{8}{e}+\\epsilon)\\cost(F^*)$.\n%, where $F^*=\\{f_1^*, \\dots, f_k^*\\}$ is an optimal solution to $J$.\n\\end{lemma}\n\\begin{proof}\n%Since $F^*$ is an optimal solution to \\divkmedian instance $I$, By definition, $f_i^*\\in E_i^*$.  \nConsider the iteration of Algorithm~\\ref{algo:kmedpm} where the chosen clients\nand radii are optimal, that is, $\\lambda_i^*=d(c_i^*,f_i^*)$ and this distance\nis minimal over all clients served by $f_i^*$ in the optimal solution.\nAssuming the input described in the statement of the lemma, it is clear that in\nthis iteration we have $f_i^*Â \\in \\Pi_i$ (see Algorithm~\\ref{algo:kmedpm},\nline~\\ref{algo:kmedpm:pi}).\n%\nFurthermore, given the partition-matroid constraint imposed on it, the proposed\nsubmodular optimization scheme is guaranteed to pick exactly one facility from\neach of $\\Pi_i$, for all $i$.\n\nOn the other hand, known results for submodular optimization show that this\nproblem can be efficiently approximated to a factor within $\\left(1-1/e\\right)$\nof the optimum~\\cite{calinescu2011maximizing}. It is not difficult\nto see this\ntranslates into a $(1+\\frac{2}{e}+\\epsilon)$-approximation ($1+\\frac{8}{e}+\\epsilon$ resp.) of the optimal choice\nof facilities, one from each of $\\Pi_i$~\\cite{cohen2019tight}. For complete calculations, please see Section \\ref{app:fptapx}.\n%This concludes the proof.\n\\end{proof}\n%\\todo{prefer not to avoid the statement \"It is not difficult to see\" -- Suhas} \n\n\\mpara{Running Time: }\nFirst we bound the running time of Algorithm~\\ref{algo:kmedpm}. Note that, the\nruntime of  Algorithm~\\ref{algo:kmedpm} is dominated by the two \\textit{for}\nloops (Line~2 and 3), since remaining steps, including finding approximate solution to the\nsubmodular function \\impr, runs in time $\\poly(|U|)$. The \\textit{for} loop of\nclients (Line~2) takes time $ \\bigO((k\\nu^{-2} \\log |U|)^k)$. Similarly, the \\textit{for}\nloop of discretized distances (Line~3) takes time $\\bigO(([\\Delta]_\\eta)^k) =\n\\bigO(\\log_{1+\\eta}^k |U|)$, since $\\Delta = \\poly(|U|)$. Hence, setting $\\eta = \\Theta(\\epsilon)$,\nthe overall\nrunning time of Algorithm~\\ref{algo:kmedpm} is bounded by\\footnote{We use the\nfact that, if $k \\le \\frac{\\log |U|}{\\log\\log |U|}$, then $(k \\log^2 |U|)^k =\n\\bigO(k^k \\poly(n))$, otherwise if $k \\ge \\frac{\\log |U|}{\\log \\log |U|}$, then\n$(k \\log^2 n)^k = \\bigO(k^k (k \\log k)^{2k})$.}\n\\[\n\\bigO\\left(\\left(\\frac{k \\log^{2} |U|}{\\epsilon^2 \\cdot \\log(1+\\epsilon)}\\right)^k  \\poly(|U|)\\right) = \\bigO\\left(\\left( \\frac{ k^3 \\log^2 k}{\\epsilon^2\n\t\\log(1+\\epsilon)}\\right)^k \\poly(|U|) \\right) \n\\]\nSince Algorithm~\\ref{algo:divkmed} invokes Algorithm~\\ref{algo:kmedpm} $\\bigO (2^{tk})$ times, its running time is bounded by\n$\n\\bigO\\left(\\left( \\frac{2^t k^3 \\log^2 k}{\\epsilon^2\n\t\\log(1+\\epsilon)}\\right)^k \\poly(|U|) \\right).\n$ \n\n%\\note{$\\poly(|U|)$ represents the running type of the submodular maximization\n%with the matroid constraint. According to \\cite{ene2019submodular}, $\\poly(|U|)\n%= \\frac{\\log^2|U|}{\\epsilon^3}$.}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% importing algorithms\n%\\subsection{A $(1+\\frac{2}{e}+\\epsilon)$ approximation algorithm}\n%In this subsection we present our main result.\n%The psuedocode of our \\fpt-approximation algorithm is presented in\n%Algorithm~\\ref{alg:divkmed}. In a nutshell the algorithm works in three phases:\n%($i$) we transform the \\divkmedian problem instance to a collection of\n%\\kmedianpm problem instances, ($ii$) we reduce the number of clients in each\n%\\kmedianpm instance using coresets, ($iii$) we solve each instance of the\n%\\kmedianpm problem with coresets using an approximation scheme, \n%($iv$) putting\n%everything together we obtain an $\\fpt(k,t,\\epsilon)$-approx algorithm for the\n%\\divkmedian problem, for $\\epsilon > 0$.\n\n\n\\begin{algorithm}\n\\caption{\\sc Div-$k$-Med$(I=\\divkins,\\epsilon)$}\n\\footnotesize\n\\label{algo:divkmed}\n\\KwIn{$I$, an instance of the \\divkmedian problem\\\\\n\\Indp \\Indp ~$\\epsilon$, a real number}\n\n\\KwOut{$T^*$, subset of facilities}\n\n\\ForEach{$\\vec{\\gamma} \\in \\{0,1\\}^t$} {\n    $E({\\vec{\\gamma}}) \\gets \\{f \\in {F} : \\vec{\\gamma} = \\vec{\\chi}_{f}\\}$\n%\\tcp*{facilities with characteristic vector~$\\vec{\\gamma}$}\n}\n$\\mathcal{E} \\gets \\{E({\\vec{\\gamma}}): \\vec{\\gamma} \\in \\{0,1\\}^t \\}$\n%\\tcp*{partition of facilities}\n\n$C' \\leftarrow \\textsc{coreset}((U,d),\\pazocal{F},C,k, \\nu \\leftarrow \\epsilon/16)$\\\\\n\n$T^* \\leftarrow \\emptyset$\\\\\n\\ForEach{multiset $\\{E(\\vec{\\gamma}_{1}),\\cdots,E(\\vec{\\gamma}_{k})\\} \\subseteq \\mathcal{E}$ of size $k$} {\n  %$\\vec{\\chi} \\gets \\sum_{j \\in [k]} \\vec{\\gamma}_{i_j}$\\\\\n  \\If{$\\sum_{i \\in [k]}\\vec{\\gamma}_{i} \\geq \\vec{r}$, element-wise} {\n    Duplicate facilities to make subsets in $\\{E(\\vec{\\gamma}_{1}), \\dots,\nE(\\vec{\\gamma}_{k})\\}$ disjoint\\\\\n    $T \\gets\n\\textsc{$k$-Median-PM}((U,d),\\{E(\\vec{\\gamma}_{1}),\\cdots,E(\\vec{\\gamma}_{k})\\},C',\\epsilon/4)$\\\\\n    \\If{$\\textsf{cost}(C',T) < \\textsf{cost}(C',T^*)$}{\n      $T^* \\gets T$\\\\\n    }\n  }\n}\n\\Return{$T^*$}\n\\end{algorithm}\n\n\n\n\n%\\input{fpt-apx-alg-2}\n\\iffalse\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 5.4": {
                    "name": "A bicriteria \\fpt approximation algorithm",
                    "content": "\n\\label{sec:bicri}\nFinally, we describe a straightforward approach to obtain a bicriteria\napproximation algorithm. This relies on the following simple\nobservation. We can first run our scheme for obtaining feasible\n{\\pattern}s to find a feasible solution. Then, we can\nignore the lower-bound constraints and run any\npolynomial-time approximation algorithm for \\kmedian. By taking the\nunion of the two solutions, we obtain at most $2k$ facilities which\nsatisfy the lower-bound constraints and achieve the quality guarantee\nof the chosen approximation algorithm (with respect to the optimal\nsolution of size $k$).\n\nWe can employ, for instance, the local-search heuristic of Arya et\nal., which yields a $(2k,3+\\epsilon)$-approximation~\\cite{arya2004local}, \nor the result of Byrka et al.~\\cite{byrka2014improved} to achieve a\n$(2k, 2.675)$-approximation.\n\\fi\n%\\begin{proof}[Proof of Theorem~\\ref{theorem:localsearch}]\n%We pick $k$ facilities arbitrarily and perform a $p$-swap\n%local-search until the algorithm converge. The obtained solution is a \n%$(3 + \\frac{2}{p})$ approximation, which trivially holds \n%from the local-search heuristic\n%of \\kmedian due to Arya et al.~\\cite{arya2004local}. However, the\n%solution does not ensure if the requirements in $\\vec{r}$ are satisfied.\n%\n%To satisfy the requirements, we find one feasible \\pattern \n%$\\{E(\\vec{\\gamma}_{1}),\\dots,E(\\vec{\\gamma}_{k})\\}$ using\n%Lemma~\\ref{lemma:feasiblecp}.\n%If the solution from local-search has at least one facility from each\n%facility group\n%$E({\\vec{\\gamma}_{i}})$, we are done, otherwise, we pick\n%one facility from each group which is not covered by the local-search solution,\n%so the final solution may contain at most $2k$ facilities.\n%%\n%We can find a feasible \\pattern in\n%$\\bigO(2^{tk})$ time using Lemma~\\ref{lemma:feasiblecp}, and local-search takes\n%$\\bigO(\\poly(|U|,p)$ time, making the overall runtime\n%$\\bigO(2^{tk} \\poly(|U|,p))$.\n%\\end{proof}\n\n%The bicriteria approximation holds for any approximation algorithm of \\kmedian.\n%Likewise, a $(2k, 2.611)$-approximation is possible by combining the results of Byrka\n%et al.~\\cite{byrka2014improved} and padding facilities to satisfy lower-bound constraints.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\iffalse\n"
                },
                "subsection 5.5": {
                    "name": "Solving set multicover problem without repetitions",
                    "content": "\n%\nBefore presenting our algorithmic result, it is convenient to present a formal\ndefinition of the set multicover problem without repetitions.\n%\n\\begin{definition}[Set multicover without repetitions (\\setmcwr)]\nGiven an universe $U=\\{e_1,\\dots,e_n\\}$, a collection of subsets\n$\\mathcal{S}\\subseteq 2^U$ and a vector\n$\\vec{r}=\\{r[1],\\dots,r[n]\\}\\subset \\integers$ of requirements (or demands).\nThe goal is to find $k$ {\\bf distinct} sets $S_{1},\\dots, S_{k} \\in \\mathcal{S}$\nsuch that each element $e_i \\in U$ intersects with at least $r[i]$ sets, that\nis, $|\\bigcup_{j \\in [k]} \\{S_{j} \\cap e_i\\}| \\geq r[i]$ for each $i\\in[n]$.\n\\end{definition}\n%\nRelated, but an easier variant of the problem allows a set to be picked multiple\ntimes~\\cite[\\S~13.2]{vazirani2013approximation}. Existence of a straight-forward reduction\nfrom set multicover with repetitions to the set cover problem enables us to make use of the\nexisting algorithmic solutions of the latter problem to solve the former problem\n(for details see Hua et al.~\\cite{hua2010dynamic}). \nIn this paper, we study a variant of the set multicover problem where only one copy of a\nset is allowed to be picked in a solution. Surprisingly, this problem variant is\nnot well studied with the exception of work by Chekuri et al.~\\cite{chekuri2012set}, where they\nstudy the problem in the geometric setting. To the best of our knowledge, we are\nnot aware of any existing results for a general variant of the set multicover problem\nwithout repetitions.\n%\n\\begin{proof}[proof of Theorem~\\ref{theorem:setmc}]\nGiven an instance\n$(U=\\{e_1,\\dots,e_n\\},\\mathcal{S}=\\{S_1,\\dots,S_m\\},\\vec{r},k)$ of \\setmcwr\nproblem, we reduce it to an instance $((U',d),C,F,\\mathcal{G},\\vec{r},k,t)$ of\nthe \\divkmediannif problem as follows: $U'=C=F=\\mathcal{S}$, $d:C \\times F\n\\rightarrow 1$, $t=n$, for each $i \\in [n]$, $G_i=\\{S_j: e_i \\in S_j, j \\in\n[m]\\}$ and finally, $\\mathcal{G}=\\{G_1,\\dots,G_n\\}$. We argue that there exists\na solution for the \\divkmediannif problem if and only if there exists a solution\nfor the \\setmcwr problem. Since we the requirement vector $\\vec{r}$ is preserved in\nthe reduction, a solution to \\setmcwr is also a solution to\n\\divkmediannif and vice versa.\n%\n%Let $\\{S_1,\\dots,S_k\\}$ be a solution for the \\setmcwr instance satisfying the\n%requirements in $\\vec{r}$, it is natural to see that $\\{S_1,\\dots,S_k\\}$ is a\n%solution for the \\divkmediannif instance.\n%\nRecall in Section~\\ref{}, we find all feasible {\\pattern}s for the\n\\divkmediannif problem, resulting in \\kmedianpm instances, which is done in time\n$\\bigO(2^{tk})$. Each feasible\n{\\pattern} is a $k$-multiset satisfying the requirements in $\\vec{r}$, however,\na valid solution for \\setmcwr must not have any repetitions. Likewise, we only pick\nsolution sets without repetitions. Since our reduction presented above is in\npolynomial time,\nthe algorithm runs in time $\\bigO(2^{|U| \\cdot k})$.\n\\end{proof}\n\\fi\n\n"
                }
            },
            "section 6": {
                "name": "Bicriteria approximation and heuristics",
                "content": "\n%\tA bicriteria \\fpt approximation algorithm}\n%\\input{fpt-apx-alg-2}\n\\label{sec:bicri}\nIn this section, we describe a bicriteria\napproximation algorithm that relies on a simple\nobservation: we can solve feasibility and clustering\nindependently, and merge the resulting solutions. \n\nFirst, we use a polynomial-time approximation algorithm $\\mathcal{A}$ for \\kmedian\nignoring the constraints in $\\reqvec$ to obtain a solution. If the obtained\nsolution does not satisfy all the requirements in $\\reqvec$. Then, we use a\nfeasibility algorithm $\\mathcal{B}$, to obtain a feasible\n{\\pattern} of at most size $k$. Picking one facility in each $E_i$\nof {\\pattern} will satisfy our requirements in $\\reqvec$.\n%we can just ignore the lower-bound constraints and run any polynomial-time approximation algorithm for \\kmedian. \nFinally, we return the union of the two solutions, which have at most $2k$ facilities that\nsatisfy the lower-bound constraints and achieve the quality guarantee\nof $\\mathcal{A}$ (w.r.t. the optimal solution of size $k$).\nWe can employ, for instance, the local-search heuristic of Arya et\nal., which yields a $(2k,3+\\epsilon)$-approximation~\\cite{arya2004local}, \nor the result of Byrka et al.~\\cite{byrka2014improved} to achieve a\n$(2k, 2.675)$-approximation. Recall from Proposition~\\ref{proposition:introduction:3} \nthat, even if we relax\nthe number of facilities to any function $f(k)$, it is unlikely to\napproximate the \\divkmedian problem in polynomial time. \nOur bicriteria approximation shows that this is not the case in \\fpt time.\n\nLeveraging the fact that $\\mathcal{B}$ only needs to find one feasible\n{\\pattern}, instead of using the time-consuming Lemma~\\ref{lemma:feasiblecp}, we\npropose the following, relatively efficient strategy to obtain a feasibile\nsolution leading to exponential speedup.\n\n%\\subsection{Faster feasibility algorithm}\n%We propose altenative strategies for obtaining any feasible solution needed for bicriteria algorithm.\n",
                "subsection 6.1": {
                    "name": "Dynamic programming approach (\\DP)",
                    "content": "\n\\begin{theorem}\\label{thm:dpfeasibility}\nThere exists a deterministic algorithm with time $\\bigO(kt2^t(r+1)^t \\poly(|U|))$ that\ncan decide and find a feasible solution for \\divkmedian. On the other hand,\nassuming \\seth, for every $\\epsilon > 0$ there exists no algorithm that\ndecides the feasibility of \\divkmedian in time \n$\\bigO((r+1 - \\epsilon)^t \\poly(|U|))$ for every $r \\ge 1$. \nHere $r = \\max_{i \\in [t]} r[i]$.\n\\end{theorem}\n\\begin{proof}\nFirst we given an algorithm for feasibility.\nAn instance of feasibility problem is \n$I= (\\mathcal{E}\\subseteq \\{0,1\\}^t, \\vec{f},\\vec{r})$, where \n%$|\\mathcal{E}| \\le 2^t$, \n$f: \\mathcal{E} \\rightarrow [n]$ is the frequency vector, and \n$\\vec{r} \\in \\{0,\\cdots,k\\}^t$ is the lower bound vector. \n%\nWe say a $k$-multiset $E$ of\n$\\mathcal{E}$ respects $f$, if for every $E_i \\in E$, $E$ contains $E_i$ at\nmost $f(E_i)$ times. The goal is to find a $k$-multiset $E^*$ of $\\mathcal{E}$\nrespecting $f$ such that $\\sum_{i \\in [k]} E^*_i \\ge \\vec{r}$.\n\n\nThe approach is similar to the dynamic program technique employed for {\\sc\nSetCover}. First, we obtain $\\mathcal{E}'$ from $\\mathcal{E}$ as\nfollows. For every element $E_i \\in \\mathcal{E}$, create $\\min\\{f(E_i),k\\}$\ncopies of $E_i$ in $\\mathcal{E}'$. Thus, $|\\mathcal{E}'| \\le k |\\mathcal{E}|$.\nNow let us arbitrarily order the elements in $\\mathcal{E}'$ as \n$\\mathcal{E}' = (E_1,E_2,\\cdots)$. \nFor every $i\\in [|\\mathcal{E}'|]$ and $\\vec{\\eta} \\in \\{0,\\cdots,k\\}^t$, we have an entry\n$A[i,\\eta]$ which is assigned the minimum number of elements in\n$E_1,\\cdots,E_i$ summing to at least $\\vec{\\eta}$. The dynamic program\nrecursion works as follows, as base case $A[0,\\vec{0}] =0$. For each $ \\vec{\\eta} \\ne \\vec{0}$,\n$A[0,\\vec{\\eta}] = \\infty $\n\\begin{align}\n  \\label{eq:dp}\n\tA[i,\\vec{\\eta}] = & \\min\\{1+A[i-1,\\vec{\\eta} - E_i], A[i-1,\\vec{\\eta}]\\}\\\\ \n\t& \\textit{ round negative entries in $\\vec{\\eta} - E_i$ to zero.} \\nonumber\n\\end{align}\n\nFinally, we check if $A[|\\mathcal{E}'|,\\vec{r}] \\le k$. Note that any solution\non $\\mathcal{E}'$ respects $f$ due to construction. Finally, the running time of\nthe above algorithm is $|\\mathcal{E}'|\\cdot (r+1)^t \\cdot t =\n\\bigO(kt2^t(r+1)^t)$. \n\nTo find a feasible solution, we update our dynamic program table as follows:\t\nFor $A[0,\\vec{0}] =''$. For each $ \\vec{\\eta} \\ne \\vec{0}$,\n\\begin{align*}\n\tA[0,\\vec{\\eta}] &= '00\\ldots0' \\quad \\textit{string of $k+1$ zeros}  \\\\\n\tA[i,\\vec{\\eta}] &= \n\t\\begin{cases}\n\t\tA[i-1,\\vec{\\eta}] \\quad \\text{if } |A[i-1,\\vec{\\eta}]| < |\\{E_i\\} \\cup A[i-1,\\vec{\\eta} - E_i]| \\\\\n\t\\{E_i\\} \\cup  A[i-1,\\vec{\\eta} - E_i] \\ \\text{otherwise}\n\t\\end{cases}\n\\end{align*}\n$\\textit{ round negative entries in $\\vec{\\eta} - E_i$ to zero, and union is for multiset.}$\n\nFinally, we check whether or not $|A[|\\mathcal{E}'|,\\vec{r}]| \\le k$.\n%\\todo{Ameet: Finding the solution?}\n%For the space complexity, note that each entry of the array takes only\n%$\\bigO(\\log k)$ bits, and hence $|A| = \\bigO(k2^t \\cdot (r+1)^t \\cdot \\log k)$\n%bits. However, note that each row entry $A[i,*]$ depends only on the row entry\n%$A[i-1,*]$, hence we need to store only $2$ rows of $A$ at any time. Hence, the\n%algorithm takes $\\bigO((r+1)^t \\log k)$ bits of space.\n%\nTo show the lower bound on runtime, note that if there exists an algorithm running\nin time $\\bigO((r+1 - \\epsilon)^t \\poly(|U|))$, for some $\\epsilon >0$, then we\ncan solve {\\sc SetCover} in time $\\bigO((2-\\epsilon)^{|\\mathcal{U}|}\n\\poly(|\\mathcal{U}|))$, where $\\mathcal{U}$ is the universe of the\n\\textsc{SetCover} instance. This is because $r=1$ for \\textsc{SetCover}, which\ncontradicts \\seth~\\cite[Conjecture~14.36]{cygan2015parameterized,cygan2016onproblems}.\n\\end{proof}\n%\\input{fpt-apx-alg-3}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 6.2": {
                    "name": "Linear programming approach (\\LP)",
                    "content": "\nIn this subsection, we propose a heuristic for finding a feasible solution based\non randomized rounding of the fractional solution of a linear program. The linear\nprogram formulation is as follows:\n%\n\\begin{align*}\n    \\text{Minimize} \\quad &  0 \\cdot x~\\text{such that}~\\mathcal{E} \\cdot x \\geq \\reqvec,\n    \\\\  &  \\sum_i x_i \\leq k, 0 \\leq x_i \\leq f(E_i). \n\\end{align*}\nWe solve the \\LP to obtain a fractional solution and round $x_i$ to an\ninteger value using randomized rounding strategy inspired by~\\cite{Raghavan1987rounding}.\n$$\nx'_i =\n\\begin{cases}\n\\lfloor{x_i}\\rfloor & \\text{with probability } 1 - x_i + \\lfloor{x_i}\\rfloor\\\\\n\\lceil{x_i}\\rceil & \\text{otherwise} \\\\\n\\end{cases}\n$$\nTherefore, it holds that $\\mathbf{E}(\\sum_i x'_i) \\leq k$.\n%\nHowever, the lower requirements might be unfilled,\nso the algorithm needs to verify the correctness and repeat the procedure a many times\nand produce another solution (E.g., by randomizing objective function).\n\nEven though the randomized rounding approach does not guarantee finding an\nexisting solution, the algorithm is very effective in real-world dataset, as\ndemonstrated in Section~\\ref{sec:experiments}.\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Local search heuristic (\\lsone)",
                    "content": "\nFirst, we present a local-search algorithm for \\kmedianpm problem and discuss how\nto apply this approach to solve \\divkmedian problem. Given an\ninstance \\kmedkpmins, we pick one facility from each $E_i$ at random as an\ninitial assignment, and continue to swap with facilities from the same group until\nthe solution is con verged i.e a facility $f \\in E_i$ is only allowed to swap with\nfacility $f' \\in E_i$ for all $i \\in [k]$.\n\nRecall that each feasible constraint pattern is an instance of \\kmedianpm,\nlikewise, we employ the heuristic discussed above for each instance to obtain a\nsolution with minimum cost. The runtime of the algorithm is $\\bigO(2^{tk}\n\\poly(|U|)$, since we have at most $\\bigO(2^{tk})$ feasible constraint patterns\nand each iteration of the local search can be executed in polynomial time. In our\nexperiments, we refer to this algorithm as \\lsone. Bounding the approximation ratio\nof \\lsone is left as an open problem.\n\n%We do not claim a formal theoretical bound on the quality of the solution\n%obtained. Bounding the approximation ratio of \\lsone implies which is a known\n%open problem posed by Hajighaya.\n\n\\iffalse\n"
                },
                "subsection 6.4": {
                    "name": "Benchmarks",
                    "content": "\nWe implemented and compared the performance of various bicriteria performance.\nAs a polynomial-time approximation algorithm for \\kmedian, we use the local search described in \\cite{arya2004local}.\n\nFor generating a data set, we prepared a generator scaling up the number of groups ($t$) or the centers' number.\n\nAll tests were executed on MacBook Pro with ARM 10-core CPU and 32-core GMU Apple M1 Max processing unit together with 64 GB RAM of the unified memory. \n\nFor the naive implemention, we used matrix multiplication from \\textit{numpy} library. Array representing $\\mathcal{E}$ is generated using \\textit{itertool}.\nThe linear programming solution was implemented with \\textit{Scipy} solver. For dynamic programming, \\textit{numpy} was used for creating an array.\n\n",
                    "subsubsection 6.4.1": {
                        "name": "Scaling up $k$",
                        "content": "\n\nWith a fixed number of groups, the number of clients, and randomized clients' membership, we increment $k$.\n\nWe analyze two cases. The first one describes the standard situation, when finding any solution does not require computing all permutations (in the first algorithm).\nThe second case expresses the worst case when with a limited number of solutions.\n\n\n\n\n\n\n\nFigures ~\\ref{fig:log_k_standard} and \\ref{fig:log_k_worst} represent runtime results for the starndard and the worst case accordingly.\nOnly the performance of the naive solution is significantly different in both cases.\nThe runtime or linear programming and dynamic programming solution remain similar in both cases. \nThe naive implementation typically performs better in a real case than dynamic programming although the theoretical (and worst-case) complexity remains worse.\nAdditionally, the complexity of obtaining a linear programming solution is not changing drastically while scaling $k$. T\n\n\n"
                    },
                    "subsubsection 6.4.2": {
                        "name": "Scaling up $t$",
                        "content": "\n\nWith a fixed $k$, number of clients and randomized clients' membership, we increment the number of groups.\n\nSimilarly, we analyze two cases as described in the previous section.\n\n\n\n\n\n\nFigures \\ref{fig:log_t_standard} and \\ref{fig:log_t_worst} represent runtime results for the starndard and the worst case accordingly.\nResults resemble the outcome of the previous computation.\nExponential growth is noticeable in linear programming solutions as well due to the growing number of variables.\nHowever, this heuristic still performs computations surprisingly fast.\n\\fi\n\n%\\input{improvements}\n"
                    }
                }
            },
            "section 7": {
                "name": "Experiments",
                "content": "\n\\label{sec:experiments}\nThis section discusses our experimental setup and results. Our objective is\nmainly to evaluate the scalability of the proposed methods.\n\n",
                "subsection 7.1": {
                    "name": "Experimental setup",
                    "content": "\n\n\\mpara{Hardware.}\nOur experiments make use of two hardware configurations: ($i$)\na {\\em desktop} with a $4$-core {\\em Haswell} CPU with $16$~GB of main memory,\nUbuntu 21.10; ($ii$) a {\\em compute node} with a $20$-core {\\em Cascade lake}\nCPU with $64$~GB of main memory, Ubuntu 20.04.\n\n\\mpara{Datasets.}\nWe use datasets from UCI machine learning repository~\\cite{dua2019uci} (for details check\nthe corresponding webpage of each dataset). Data are processed by assigning\ninteger values to categorical data and normalize each column to unit norm. We\nassume the set of clients and facilities to be the same i.e,, $U = C = F$.\n\n\\mpara{Data generator.} \nFor scalability, we generate synthetic data using {\\tt make\\_blob}\nfrom {\\tt scikit-learn}. The groups are generated by sampling data points\nuniformly at random and restricting the maximum groups a data point can belong\nto, i.e. each data point belongs to at least one and at most $t/2$ groups. \nWe ensure that the same instance is generated for each\nconfiguration by using an initialization seed value.\n\n\\mpara{Baseline.} For \\divkmedian, we use the local-search algorithm with no\nrequirement constraints as a baseline, denoted as \\lszero, which is a\n$5$-approximation for \\kmedian. Additionally, we implemented a\n$p$-swap local-search algorithm, denoted as \\lszeropswap, which is a\n$(3+\\frac{2}{p})$-approximation~\\cite{arya2001local}. \nWe observed no significant improvement in the cost of solution of\n\\lszeropswap compared to \\lszero with $p=2$.\nWe also experimented with trivial algorithms based\non {\\em brute force} and {\\em linear program} solvers, which fail to scale\nfor even modest size instance with $|U|=100, k= 6$.\n%\nFor \\divkmeans, we use {\\tt $k$-means++} with no requirement constraints as\nbaseline, denoted as \\KM, which is a $\\log(k)$-approximation\nfor \\kmeans~\\cite{vassilvitskii2006kmeans}. \n%\nFor each dataset we\nperform $5$ executions of \\lszero (or \\KM) using random initial assignments to obtain\na solution with minimum cost. \n\n\\mpara{Implementation.} \nOur implementation is written in {\\tt Python}\nprogramming language. We make use of {\\tt numpy} and {\\tt scipy} python packages for\nmatrix computations. We use {\\tt $k$-means++} implementation from \n{\\tt scikit-learn}. For coresets we use importance sampling, which results in\ncoresets of size $\\bigO(kD\\epsilon^{-2})$,\n$\\bigO(kD^3\\epsilon^{-2})$ for \\kmedian and \\kmeans, respectively, where $D$ is\nthe dimension of data~\\cite{feldman2011unified, bachem2017practical}.\n%\nFor discretizing distances we use the existing implementation of binning from \n{\\tt scikit-gstat} python package.\n\nOur exhaustive search algorithm is implemented as matrix multiplication\noperation, thereby we use optimized implementation of {\\tt numpy} to\nenumerate feasible constrained patterns. To achieve this, we generate two\nmatrices, a matrix $A_{|\\charpart| \\times t}$\nencoding bit vectors $[0,1]^t$ corresponding to subset lattice of\n$\\groups=\\{G_1,\\dots,G_t\\}$, and,\nmatrix $B_{{{|\\charpart|}\\choose{k}} \\times |\\charpart|}$ enumerating all combinations (with\nrepetitions) of choosing $k$ facilities from non-intersecting groups in\n$\\charpart$ and multiply $B \\times A = R_{{{|\\charpart|}\\choose{k}} \\times t}$.\nFinally, for each row of $R$\nwe verify if the requirements in $\\reqvec$ are satisfied elementwise to obtain\nof all feasible constraint patterns. More precisely, if $i$-th row of $R$\nsatisfy requirements in $\\reqvec$ elementwise, it implies that $i$-th row of $B$ is a feasible\nconstraint pattern. For finding one feasible\nconstrained pattern, we enumerate rows of the matrix $B$ in batches, $p$ rows\nat a time and multiply with matrix $A$ until we obtain one solution satisfying\n$\\reqvec$ elementwise. This is\nessential for scaling of bicriteria algorithms, where it is sufficient to find one feasible\nconstraint pattern.\n%\nOur dynamic program is implemented using {\\tt numpy} arrays. Observe\ncarefully in Equation~(\\ref{eq:dp}) that computing $A[i, \\vec{\\eta}]$ relies\nonly on the values of $A[i-1,\\vec{\\eta}]$, so we only use\n$2\\cdot(r+1)^t k$ memory.\n%\nTo reduce memory footprint and improve scalability, we avoid\nprecomputing of distances between datapoints, which requires $\\bigO(n^2)$ memory.\nInstead, we compute distances on-the-fly. Nevertheless, this has an additional overhead\nof $\\bigO(D)$ in time.\n\nOur implementation is available anonymously as open source~\\cite{sourcecode}.\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 7.2": {
                    "name": "Experimental results",
                    "content": "\nThis subsection discusses our experimental results.\n\n\n\n\\mpara{Scalability.}\nThe experimental setup of our scalability experiments is available in\nTable~\\ref{table:experiments:setup}. Our feasibility experiments execute on\nthe {\\em desktop} configuration. Bicriteria experiments are executed on the {\\em compute}\nnode. All reported runtimes are in seconds, and we terminated experiments that\ntook more than two hours. \n\nOur first set of experiments studies the scalability of finding one feasible\nconstraint pattern. In Figure~\\ref{fig:scaling:feasibility}, we report the runtime of exhaustive\nenumeration (\\ES), dynamic program (\\DP) and linear program (\\LP) algorithms for\nfinding a feasible constraint satisfaction pattern as a function of number of\nfacilities $|U|$ (left), number of cluster centers $k$ (center) and number of\ngroups $t$ (right). \n%\nFor each configuration of $|U|$, $t$ and $k$ in\nTable~\\ref{table:experiments:setup}, we report runtimes of 10 independent input\ninstances. We observed little variance in runtime among the independent inputs\nfor \\DP and no significant variance in runtime for \\LP and \\ES.\n%\nRecall that finding a feasible constraint pattern is \\np-hard and \\wtwo-hard\n(See Section~\\ref{sec:hardness}). Despite that, our algorithms solve instances with up to \n$100$ million facilities\nin less than one hour on a desktop computer, provided that number of\ncluster centers and groups are small i.e, $k=5, t=7$.\n\nSurprisingly, \\LP performs better with respect to runtime. However,\nrandomized rounding fails find a feasible solution for large\nvalues of $t > 8$, since the fractional solution obtained is sparse when\n$|\\charpart| \\approx 2^t > 2^8$. Additionally, when\nthe number of facilities is large i.e, $|U|=10^8$, \nthe runtime of \\LP, \\DP and \\ES converge.\n\n\n\n\n\n%LP fails to converge and present a feasible solution when there are many\n%solutions. Randomized rounding of LP fail to converge to a feasible solution\n%when the number of groups $t > 8$, $|\\mathcal{E}| > 128$.\n%It is difficult to find all feasible solutions using randomized rounding, seldom\n%the LP converges to the same solution when $t$ is large, say $t > 8$.\n\n\nOur second set of experiments studies the scalability of bicriteria algorithms.\nIn Figure~\\ref{fig:scaling:bicriteria:1}, we report the runtime of local search\n(\\lszero) combined with \\ES, \\DP and \\LP algorithms for solving \\divkmedian\nproblem, as a function of number of facilities $|U|$ (left), number of cluster\ncenters $k$ (center) and number of groups $t$ (right). \n%\nFor each configuration of $|U|$, $t$ and $k$ in\nTable~\\ref{table:experiments:setup}, we report runtimes of $5$ independent input\ninstances and observed little variance in runtime.\n%\nWe observed similar scalability for \\divkmeans, for which we\nmake use of {\\tt kmeans++} (\\KM) implementation from {\\tt scikit-learn}, along with\n\\ES, \\LP and \\DP (See Supplmentary~\\ref{fig:scaling:bicriteria:2}).\n\nOur third set of experiments studies the scalability of $\\ES + \\lsone$\nIn Figure~\\ref{fig:scaling:es-ls}, we report runtime as a function of number of\nfacilities $|U|$ (left), number of centers $k$ (center) and number of groups $t$\n(right). \n%\nFor each configuration of $|U|$, $t$ and $k$ in\nTable~\\ref{table:experiments:setup}, we report runtimes of $5$ independent input\ninstances and observed little variance in runtime.\n%\nWe observed high variance in runtime, as a result of variance in the number of\nfeasible constraint patterns among independent inputs. The algorithm manages to\nsolve two instances with up to 40 thousand facilities in approximately $2.5$ hours\non a desktop computer.\n\n\n%\\begin{figure*}\n%\\includegraphics[width=0.8\\linewidth]{figures/scaling-bicriteria-kmeans.pdf}\n%\\caption{\\label{fig:scaling:bicriteria:2}Scalability of bicriteria algorithm for \\divkmeans.}\n%\\end{figure*}\n\n%\\begin{figure*}\n%\\includegraphics[width=0.8\\linewidth]{figures/scaling-es-ls.pdf}\n%\\caption{\\label{fig:scaling:bicriteria:2}Scalability of \\ES+\\lsone for\n%\\divkmedian.}\n%\\end{figure*}\n\n\n\n\n\\mpara{Experiments on real datasets.}\nFor each dataset, we generate two disjoint groups $G_1, G_2$. For this we choose\ngender, except for {\\tt house-votes}, where we choose party affiliation. We use the\nprotected attributes race or age group to generate groups $G_3$ and $G_4$,\nrespectively, so groups $G_3, G_4$ intersect with either or both $G_1, G_2$.\nThe experiments are executed on {\\em desktop} with number of centers $k=6$, number of groups\n$t=4$ and requirement vector $\\reqvec=\\{3,3,2,1\\}$. That is, we have a\nrequirement that the chosen cluster centers must be an equal number of men and\nwomen, with additional requirements to pick at least\ntwo cluster centers that belong to a group representing race and one\ncenter that belongs to a group representing a certain age group. \n%\nFor each dataset, we execute $5$ iterations of each algorithm with different\ninitial assignment to report a solution with minimum cost and corresponding runtime.\n\n\nIn Table~\\ref{table:experiments:dataset}, we report dataset name, size $|U|$, \ndimension $D$ in Column~1-3, respectively. Column~4 is the\nruntime of \\lszero (baseline). We report results of bicriteria\napproximation algorithms $\\lszero+\\LP$ in Column~5-7, $\\lszero+\\ES$ in Column~8-10 and\n$\\lszero+\\DP$ in Column~11-13. For each bicriteria algorithm we report runtime,\n$\\zeta^*=\\frac{\\cost(ALG)}{\\cost(\\lszero)}$ which is the ratio of the cost bicriteria\nalgorithm to the cost of \\lszero and the size of reported solution $k^*$. Finally, in Column~14-15,\nColumn~16-17 and Column~18-19, we report results of $\\LP+\\lsone$, $\\ES+\\lsone$ and\n$\\fpt(3 + \\epsilon)$ approximation algorithm, respectively. For each of these algorithms we\nreport runtime and $\\zeta^*=\\frac{\\cost(ALG)}{\\cost(\\lszero)}$.\n\n%Recall that in bicriteria approximation, first we run \\lszero to obtain a\n%solution $S_1$ and check if the requirement in $\\reqvec$ are satisfied by $S_1$.\n%If not, then we remove the requirements which are satisfied from $\\reqvec$ to\n%solve find a constraint feasible pattern and pick facilities at random. However\n%LP performs better when the number of solutions are limited, which is the\n%opposite for exhaustive search algorithm. The runtime of dynamic program is very\n%stable with little variance for independent inputs, also DP solution reports the\n%minimum number of facilities required to satisfy $\\reqvec$.\n\nIn bicriteria algorithms, \\lszero consumes the majority ($> 90\\%$) of the runtime, and a minority ($<10\\%$) of the runtime is spent on finding a feasible\nconstraint pattern. This observation is trivial by comparing runtime of\nbicriteria algorithm(s) and \\lszero. \nAs expected, $\\lszero+\\DP$ returns solution with minimum size $k^*$ with no\nsignificant change in the cost of solution obtained from \\LP+\\lszero and \\ES+\\lszero.\nNote that the value of $\\zeta^* < 1.0$ since the size of solution\nobtained $k^* > k = 6$.\n\n%As expected \\fpt approximation algorithm does not scale well in practice. Even\n%though in theory the algorithm has a approximation guarantee of $1+ 2/e +\n%\\epsilon$, the $\\epsilon$ factor is quite high in reality due to the\n%discretization error of distances and to reduce the coreset size to a\n%modest size which is very essential to make the algorithm practically viable.\n\n%Linear program formulation fails to converge and produce many duplicate\n%feasible solutions. So in a very few datasets, ES+\\lsone gives better solutions,\n%however the solution obtained is not significantly different.\n\n%It still remains open to bound the approximation ratio of ES+\\lszero. \n%Bounding\n%this approximation ratio will also solve the problem of bounding the\n%approximation ratio of local-search algorithm.\n\nEven though the \\fpt approximation algorithms presented in Section~\\ref{sec:algorithm} have good\ntheoretical guarantees, they fail to perform in practice. We believe the reason\nis two-fold. First, the size of the coreset obtained using importance\nsampling $\\left(\\bigO(kD^2\\epsilon^{-2})\\right)$ is relatively large. Second, the\n$\\epsilon'$ factor used for discretizing distances is also large. In this regard,\nthere is still room for implementation engineering to make the algorithm\npractically viable.\n\n\n\n\n\n\n\n"
                }
            },
            "section 8": {
                "name": "Related work",
                "content": "\n\\label{sec:related}\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\kmedian is a classic problem in computer science.\nThe first constant-factor approximation for metric \\kmedian was presented by\nCharikar et al.~\\cite{charikar2002constant}, which was improved to\n$(3+\\epsilon)$ in a now seminal work by Arya et al.~\\cite{arya2004local}, using\na local-search heuristic. The best known approximation ratio for metric\ninstances stands at 2.675, which is due to\nByrka et al.~\\cite{byrka2014improved}. Kanungo et al.~\\cite{kanungo2004local}\ngave a $(9+ \\epsilon)$ approximation algorithm for \\kmeans, \nwhich was recently improved to $6.357$ by Ahmadian et al.\\cite{ahmadian2019better}.\n%\nOn the other side of the coin, the \\kmedian\nproblem is known to be \\np-hard to approximate to a factor less than\n$1+2/e$~\\cite{guha1998greedy}. Bridging this gap is a well known open problem.\n%\nIn the \\fpt landscape, finding an optimal\nsolution for \\kmedian/\\kmeans are known to be \\wtwo-hard with respect to parameter $k$\ndue to a reduction by Guha and Khuller~\\cite{guha1998greedy}. More recently,\nCohen-Addad et al.~\\cite{cohen2019tight} presented \\fpt approximation algorithms\nwith respect to parameter $k$, with approximation ratio\n$(1+\\frac{2}{e}+\\epsilon)$ and $(1+\\frac{8}{e}+ \\epsilon)$ for \\kmedian and\n\\kmeans, respectively. They showed that the ratio is essentially tight assuming \\gapeth. Their\nresult also implies a $(2+\\epsilon)$ approximation algorithm for \\matmedian in\n\\fpt time with respect to parameter $k$.\n\nIn recent years the attention has turned in part to variants of the problem with\nconstraints on the solution. One such variant is the red-blue median problem\n(\\rbmedian), in which the facilities are colored either red or blue, and a\nsolution may contain only up to a specified number of facilities of each\ncolor~\\cite{hajiaghayi2010budgeted}. This formulation was generalized by the\nmatroid-median problem (\\matmedian)~\\cite{krishnaswamy2011matroid}, where\nsolutions must be independent sets of a matroid. \n%\nConstant-factor approximation algorithms were given by \nHajiaghayi et al.~\\cite{hajiaghayi2010budgeted,hajiaghayi2012local} and\nKrishnaswamy et al.~\\cite{krishnaswamy2011matroid} for \\rbmedian and \\matmedian\nproblems, respectively. \n\n\n\n\\mpara{Algorithmic fairness.} In recent years, the notion of fairness\nin algorithm design has gained significant traction. The underlying\npremise concerns data sets in which different social groups, such as\nethnicities or people from different socioeconomic backgrounds,  can be identified.\nThe output of an algorithm, while suitable when measured\nby a given objective function, might negatively impact\none of said groups in a disproportionate\nmanner~\\cite{biddle2020predicting,berk2021fairness}.\n\nIn order to mitigate this shortcoming, constraints or penalties can be\nimposed on the objective to be optimized, so as to promote more\nequitable\noutcomes~\\cite{hardt2016equality,zafar2017fairness,dwork2018decoupled,fu2020fairness}.\n\nIn the context of clustering, which is the focus of the present work,\nexisting proposals have generally dealt with the notion of equal\nrepresentation within\nclusters~\\cite{chierichetti2017fair,rosner2018privacy,schmidt2019fair,huang2019coresets,backurs2019scalable,bercea2019cost}. That\nis, the \\textit{clients} in each cluster should not be comprised\ndisproportionately of any particular group, and all groups should\nenjoy sufficient representation in all clusters. In contrast, this paper deals with\nthe problem of representation constraints among the\n\\textit{facilities}, as formalized in the recent work of Thejaswi et\nal~\\cite{thejaswi2021diversity}. \n%\nWhile the\nproblem admits no polynomial-time approximation algorithms for the\ngeneral case, the authors of the original work presented constant-factor\napproximation algorithms for special cases~\\cite{thejaswi2021diversity}.\n\n\\iffalse\nIn this section we present a brief survey of related work.\n\nThe \\divkmedian problem was recently introduced by Thejaswi et\nal.~\\cite{thejaswi2021diversity}. They gave inapproximability results for the\ngeneral case where facility groups may intersect, and presented constant-factor\napproximation algorithms for tractable cases where facility groups are disjoint.\nFor the case where disjoint facility groups and the number of facility groups is\ntwo (red-blue), they presented a $5 + \\epsilon$ approximation by reducing the\nproblem to \\rbmedian and using a local-search heuristic by Hajiaghayi et\nal.~\\cite{hajiaghayi2012local}.  For the more general variant with any number of\ndisjoint facility groups, they presented an $8$-approximation algorithm based on\nlinear programing. The algorithm relies on a reduction to \\matmedian and builds\non the algorithm by Swamy et al.~\\cite{swamy2016improved}. For the most general\ncase, with possibly intersecting facility groups, they presented heuristics\nbased on local search.  To the best of our knowledge, there is no known\nalgorithm to solve the general variant of \\divkmedian  with theoretical\nguarantees.\n\n\n\n\n\nThe \\kmedian clustering formulation is a classic problem in computer science.\nThe first constant-factor approximation for metric \\kmedian was presented by\nCharikar et al.~\\cite{charikar2002constant}, which was improved to\n$(3+\\epsilon)$ in a now seminal work by Arya et al.~\\cite{arya2004local}, using\na local-search heuristic.  The best known approximation ratio for metric\ninstances stands at 2.675, which is due to \nByrka et al.~\\cite{byrka2014improved}. In the \\fpt landscape, finding an optimal\nsolution for \\kmedian is known to be \\wtwo-hard with respect to parameter $k$\ndue to a reduction by Guha and Khuller~\\cite{guha1998greedy}.  More recently,\nCohen-Addad et al.~\\cite{cohen2019tight} presented \\fpt approximation algorithms\nwith respect to parameter $k$, with a $(1+\\frac{2}{e}+\\epsilon)$ approximation\nratio. They showed that the ratio is essentially tight assuming \\gapeth. Their\nresult also implies a $(2+\\epsilon)$ approximation algorithm for \\matmedian in\n\\fpt time with respect to parameter $k$.\n\\fi\n\n"
            },
            "section 9": {
                "name": "Conclusions \\& future work",
                "content": "\n\\label{sec:conclusions}\nIn this paper we have provided a comprehensive analysis of the \\textit{diversity-aware $k$-median}\nproblem, a recently proposed variant of \\kmedian in the context of\nalgorithmic fairness. We have provided a thorough characterization of\nthe parameterized complexity of the problem, as well as the first\nfixed-parameter tractable constant-factor approximation algorithm. Our\nalgorithmic and hardness results naturally extend for {diversity-aware\n$k$-means} problem.\n\nDespite its theoretical guarantees, said approach is\nimpractical. Thus, we have proposed a faster dynamic program for\nsolving the feasibility problem, as well as an efficient, practical\nlinear-programming approach to serve as a building block for the\ndesign of effective heuristics. In a variety of experiments on\nreal-world and synthetic data, we have shown that our approaches are\neffective in a wide range of practical scenarios, and scale to\nreasonably large data sets.\n\nOur results open up several interesting directions for future\nwork. For instance, it remains unclear whether further speed-ups are\npossible in the solution of the feasibility problem. The exponent\nof $t\\log r$ in our algorithm is close to the known lower bound of\n$t$, so it is natural to ask whether this extra factor can be shaved off.\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{sources-short}\n\n\\appendix\n\\clearpage\n"
            },
            "section 10": {
                "name": "Proofs",
                "content": " \\label{app:proofs}\n\\iffalse\n",
                "subsection 10.1": {
                    "name": "proposition:hardness:approx2",
                    "content": "\n\\begin{proof}\nGiven an instance $(G, k)$ of the \\vertexcover problem such that $G=(V,E)$,\n$k \\leq |V|$, we construct an \\divkmedian problem instance\n$(C, \\pc{F}, \\mc{F}, R, k', d)$ as follows:\n%\n($i$) $C=V$, ($ii$) $\\pc{F}=V$,\n($iii$) for each edge $\\{u,v\\} \\in E$ we construct a set\n$F_i = \\{u, v\\}$, and $\\mc{F} = \\{F_1,\\dots,F_m\\}$,\n($iv$) $R = \\{1^m\\}$, ($v$) $k' = k$ and\n($vi$) $d(u,v) = 1$ for $(u,v) \\in C \\times F$.\n\nLet $S \\subseteq V$ be a solution for the \\vertexcover problem. From the\nconstruction it is clear that $|S \\cap F_i| \\geq 1$ for each $F_i \\in\n\\mathcal{F}$ since each $F_i$ is a set of vertices in an edge. $|S| \\leq k$ so\n$S$ is also a solution for the \\divkmedian problem. The argument for the other\ndirection is analogous to the previous arguments.\n\nThis establishes that if the \\divkmedian problem has an\nalgorithm with polynomial time and any approximation factor, then we can solve\nthe \\vertexcover problem in polynomial time, which is most likely not possible\nassuming $\\p \\neq \\np$.\n\\end{proof}\n\n"
                },
                "subsection 10.2": {
                    "name": "proposition:hardness:approx3",
                    "content": "\n\\begin{proof}\nBy the seminal work of Bartal~\\cite{bartal1996probabilistic},\na metric instance of the \\divkmedian problem can be embedded into a tree metric with\nat most $\\log(|C|+|\\pazocal{F}|)$-factor change in distance.\nIf there exists a polynomial-time algorithm to approximate the \\divkmedian problem on a tree metric\nwithin factor $c$, then there exists a $c\\,\\log(|C|+|\\pazocal{F}|)$-factor approximation algorithm\nfor any metric distance measure.\nHowever, the existence of such an algorithm contradicts our inapproximability result in\nTheorem~\\ref{theorem:hardness:approx}.\nSo the \\divkmedian problem is \\np-hard to approximate to any multiplicative factor\neven if the underlying metric space is a tree.\n\nMore strongly, if there exists a factor-$f$ approximation algorithm to solve the\n\\divkmedian problem on a tree, then we can obtain a factor-$f \\log(|C| + |\\pazocal{F}|)$ approximation algorithm for any general metric space using tree\nembedding. Again this contradicts our inapproximability results in\nTheorem~\\ref{theorem:hardness:approx}. So the \\divkmedian problem is\ninapproximable to polynomial factor even if the underlying metric space is a\ntree.\n\\end{proof}\n\\fi\n\n"
                },
                "subsection 10.3": {
                    "name": "Scalability experiments",
                    "content": "\nIn Figure~\\ref{fig:scaling:bicriteria:2} we report the scalability of bicriteria approximation\nalgorithms for \\divkmeans problem.\n\n\n\n\n"
                },
                "subsection 10.4": {
                    "name": "lemma:partition",
                    "content": " \\label{app:fptapx}\n%\\begin{proof}\n%First, we enumerate all feasible solutions of the \\divkmedian problem,\n%each feasible solution is an instance of the \\kmedianpm problem.\n%we transform the \\divkmedian instance $I$ into a collection\n%$\\{J_i\\}$ of \\kmedianpm instances such that one of them contains the optimal\n%solution to $I$. In the next step, which corresponds to\n%Algorithm~\\ref{alg:kmedpm}, we find an approximate solution to every instance\n%$\\{J_i\\}$. To this end, we first reduce the clients using \\coreset, then show\n%how to brute force on the \\coreset clients to obtain an approximate solution for\n%each $\\{J_i\\}$.\n\n%In more detail, the algorithm works as follows. Observe that, we can associate each facility $s\\in \\pazocal{F}$ with a characteristic vector $\\vec{\\chi}_s \\in \\{0,1\\}^t$ that indicates the groups which $s$ is a part of. Further, all the facilities with same characteristic vector $\\vec{\\gamma} \\in \\{0,1\\}^t$ belong to a unique intersection $\\bigcap_{\\ell \\in [t] : \\vec{\\gamma}[\\ell]=1} F_\\ell$.\n%for every unique intersection $Q$ of $\\{\\F_i\\}_{i\\in[t]}$, all the facilities in $Q$ have the same characteristic vector. \n%Hence, the idea, in step \\ref{alg:divkmed_charvec}, is to transform the clusters $\\{F_i\\}_{i \\in [t]}$ (which can be overlapping) into $2^t$ mutually disjoint sets $\\mathcal{E} := \\{E_{\\vec{\\gamma}}\\}_{\\vec{\\gamma}\\in\\{0,1\\}^t}$, such that the set $E_{\\vec{\\gamma}}$ corresponds to all the facilities with characteristic vector $\\vec{\\gamma} \\in \\{0,1\\}^t$.\n%Once we have transformed the instance into $2^t$ disjoint sets $\\mathcal{E}$, in step \\ref{alg:divkmed_iter}, we iterate over all possible $k$ multisets of $\\mathcal{E}$, and check if the sum of their characteristic vectors meets the demand vector. This corresponds to picking only those sets that are feasible for the original instance. Note that, every feasible $k$ multiset $\\{E_{i_1},\\cdots,E_{i_k}\\}$ instance is actually an instance $\\kmedianpm$, where we want to find an optimal $\\kmedian$ solution $W \\subseteq \\pazocal{F}$ with the (partition matroid) constraint $|W \\cap E_{i_j}|=1$, for all $j \\in [k]$. Algorithm~\\ref{alg:kmedpm} shows how to get an approximate solution for $\\kmedianpm$. Finally, in the end, we return the minimum solution over all the iterations. \n\n\\iffalse\nNow, we describe Algorithm~\\ref{alg:kmedpm} in detail. First, note that to make\nAlgorithm~\\ref{alg:kmedpm} efficient, we use a smaller client set $C'$, obtained\nusing a coreset of $C$ as in step \\ref{alg:divkmed_cset}. The idea of this\nalgorithm closely follows the idea of~\\cite{cohenaddad_et_al:LIPIcs:2019:10618}.\nLet the disjoint facility sets given be $\\{E_i\\}_{i \\in [k]}$. Let $F^* :=\n\\{f_i^* \\in E_i\\}_{i \\in [k]}$ be an optimal solution to the input instance. For\neach $f^*_i$, let $c^*_i \\in \\mathcal{C}$ be the closest client with\n$d(f^*_i,c^*_i) = \\lambda^*_i$. Next, for each $c^*_i$ and $\\lambda^*_i$, let\n$\\Pi_i \\subseteq E_i$ be the set of facilities $f$ such that  $d(f,c^*_i) =\n\\lambda^*_i$. Now, we know that, for each $i \\in [k]$,  $\\Pi_i$ contains the\noptimal solution $f^*_i$. As mentioned in the previous section, picking an\narbitrary facility from each $\\Pi_i$, already gives a $3$-approximation. Now the\ncrucial idea to obtain correct  $c^*_i$ and $\\lambda^*_i$, is that both the\nsearch spaces can be made so small that the brute force searching takes only\n\\fpt time. For the former, we use the coreset client $C'$. Since $|C'| =\nO(k\\nu^{-2}\\log n )$, we only have to consider $O((k\\nu^{-2}\\log n)^k)$\npossibilities\\footnote{As mentioned before, this is \\fpt time.}, which\ncorresponds enumerating to all $k$ multisets of $C'$, as in step\n\\ref{alg:kmedpm_iterc}. For bounding the search space of $\\lambda^*_i$ (which is\nat most $\\Delta^k = n^{\\Omega(k)}$), we discretize the range $[\\Delta]$. Fix\n$\\eta >0$. For every $a \\in [\\Delta]$, let $[a]_\\eta$ be the smallest power of\n$(1+\\eta)$ such that $(1+\\eta)^{[a]_\\eta} \\ge a$. Note that $[\\Delta]_\\eta \\le\n\\lceil \\log_{1+\\eta} \\Delta\\rceil = O(\\log n)$. Hence, we guess $\\lambda^*_i$\nfrom the set $[[\\Delta]_\\eta]$. So, the for loop in step~\\ref{alg:kmedpm_iterl},\niterates $O(\\log^k n)$ times.\n\\fi\n\nIn fact, we prove Theorem~\\ref{theorem:mainfptapx}. We primarily focus on\n\\divkmedian, indicating the parts of the proof for \\divkmeans. In essence, to\nachieve the results for \\divkmeans, we need to consider squared distances which\nresults in the claimed approximation ratio with same runtime bounds.\n\nAs mentioned before, to get a better approximation factor, the idea is to reduce\nthe problem of finding an optimal solution to \\kmedianpm to the problem of\nmaximizing a monotone submodular function. To this end, for each $S \\subseteq\n\\pazocal{F}$, we define the submodular function $\\impr(S)$ that, in a way,\ncaptures the cost of selecting $S$ as our solution. To define the function\n$\\impr$, we add a fictitious facility $F'_i$, for each $i \\in [k]$ such that\n$F'_i$ is at a distance $2\\lambda^*_i$ for each facility in $\\Pi_i$. We, then,\nuse the triangle inequality to compute the distance of $F'_i$ to all other\nnodes.\nThen, using an $(1-1/e)$-approximation algorithm (Line~12), we approximate\n$\\impr$. Finally, we return the set that has the minimum \\kmedian cost over all\niterations. \n\n\n\n%\\iffalse\n%\\begin{algorithm}\n%\\caption{FPT approximation for \\divkmedian}\\label{alg:divkmed}\n%\\begin{algorithmic}[1]\n%\\Procedure{FindDivkMed}{$(V,d),\\pazocal{F},C,\\{F_1,\\cdots,F_t\\},k,\\epsilon$}\n%%\\Require $n \\geq 0$\n%%\\Ensure $y = x^n$\n%\\For{ every $\\vec{\\gamma} \\in \\{0,1\\}^t$} \\label{alg:divkmed_charvec}\n%    %\\State $E_{\\vec{\\gamma}} \\leftarrow \\{s_i : s_i \\in F_\\ell \\iff \\vec{\\gamma}[\\ell] = 1\\}$\n%    \\State $E_{\\vec{\\gamma}} \\leftarrow \\{s \\in \\pazocal{F} : \\vec{\\chi}_{s} = \\vec{\\gamma}\\}$\n%\\EndFor\n%\\State $\\mathcal{E} \\leftarrow \\cup_{\\vec{\\gamma} \\in \\{0,1\\}^t} E_{\\vec{\\gamma}}$\n%%\\State $\\epsilon' \\leftarrow \\frac{e}{2} \\epsilon$\n%\\State $\\textsf{ALG} \\leftarrow \\emptyset$\n%\\State $C' \\leftarrow \\textsc{core-set}((V,d),\\pazocal{F},C,k, \\nu \\leftarrow \\epsilon/54)$ \\label{alg:divkmed_cset} \\Comment{\\coreset $C'$ of Theorem~\\ref{thm:coreset}}\n%\\For{every multiset $\\{E_{i_1},\\cdots,E_{i_k}\\} \\subseteq \\mathcal{E}$ of size $k$} \\label{alg:divkmed_iter}\n%%    \\State $T \\leftarrow \\emptyset$\n%    \\If{$\\sum_{j \\in [k]} \\vec{\\gamma}_{i_j}  \\ge \\vec{r}$} \\label{alg:divkmed_demchk} \\Comment{Coordinate-wise inequaltiy}\n%        \\State Duplicate facilities so as to make $E_{i_j}$s disjoint\n%        \\State $T \\leftarrow \\textsc{kMedPM}((V,d),\\{E_{i_1},\\cdots,E_{i_k}\\},C',\\epsilon/2) \\subseteq \\pazocal{F}$ \\label{alg:divkmed_kmedpm}\n%        \\If{$\\textsf{cost}(C',T) < \\textsf{cost}(C',\\textsf{ALG})$}\n%            \\State $\\textsf{ALG} \\leftarrow T$\n%        \\EndIf\n%    \\EndIf\n%\\EndFor\n%\\State \\Return \\textsf{ALG}\n%\\EndProcedure\n%\\end{algorithmic}\n%\\end{algorithm}\n%\\begin{algorithm}\n%\\caption{FPT approximation for \\kmedianpm}\\label{alg:kmedpm}\n%\\begin{algorithmic}[1]\n%\\fi\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\\begin{algorithm}\n\\footnotesize\n\\caption{\\sc ${\\kmedianpm(J=((U,d),\\{E_1,\\cdots,E_k\\},C'), \\epsilon')}$}\n\\label{algo:kmedpm}\n\\KwIn{$J$, an instance of the \\kmedianpm problem\\\\\n\\Indp \\Indp ~$\\epsilon'$, a real number}\n\n\\KwOut{$S^*$, a subset of facilities}\n\n$S^* \\leftarrow \\emptyset$,\n$\\eta \\leftarrow e\\epsilon'/2$\\\\\n\\ForEach{ordered multiset $\\{c'_{i_1},\\cdots, c'_{i_k}\\} \\subseteq C'$ of size $k$} {\n  \\ForEach{ordered multiset $\\Lambda = \\{\\lambda_{i_1},\\cdots, \\lambda_{i_k}\\}$ such that $ \\lambda_{i_1} \\subseteq [[\\Delta]_\\eta]$} {\n    \\For{$j=1$ to $k$} {\n      $\\Pi_j \\leftarrow \\{f \\in E'_j \\mid d_D(f,c'_{i_j}) = \\lambda_{i_j} \\}$ \\label{algo:kmedpm:pi}\\\\\n      Add a fictitious facility $F'_j$ \\\\\n      \\For{$f \\in \\Pi_j$} {\n        $d(F'_j,f) \\gets 2\\lambda_{i_j}$\n      }\n      \\For{$f \\notin \\Pi_j$} {\n        $d(F'_j,v) \\gets 2\\lambda_{i_j} + \\min_{f \\in \\Pi_j} d(f,v)$\n      }\n    }\n    \\For{$S \\subseteq F$, define $\\textsf{improve}(S) := \\textsf{cost}(C',F') - \\textsf{cost}(C',F' \\cup S)$} {\n      $S_{max} \\leftarrow S \\subseteq  \\pazocal{F}$ that maximizes\n      $\\textsf{improve}(S)$ s.t. $|S \\cap \\Pi_j|=1, \\forall j\\in[k]$\\\\\n      \\If{$\\textsf{cost}(C',S_{max}) < \\textsf{cost}(C',S^*)$} {\n        $S^* \\leftarrow S$\n      }\n    }\n  }\n}\n\\Return $S^*$\n\\end{algorithm}\n\n\\iffalse\n\\begin{algorithm}\n\\begin{algorithmic}\n\\Procedure{kMedPM}{$(V,d),\\{E_1,\\cdots,E_k\\},C',\\epsilon'$}\n\\State $S^* \\leftarrow \\emptyset$\n\\For{every multiset $\\{c'_{i_1},\\cdots, c'_{i_k}\\} \\subseteq C'$ of size $k$} \\label{alg:kmedpm_iterc}\n    \\For{every multiset $\\Lambda = \\{\\lambda_{i_1},\\cdots, \\lambda_{i_k}\\}$ such that $ \\lambda_{i_1} \\subseteq [[\\Delta]_\\eta]$} \\label{alg:kmedpm_iterl}\n        \\For{$j=1$ to $k$}\n            \\State $\\Pi_j \\leftarrow \\{f \\in E'_j \\mid d_D(f,c'_{i_j}) = \\lambda_{i_j} \\}$\n            \\State Add a fictitious facility $F'_j$ \n            \\State Define $d(F'_j,f) := 2\\lambda_{i_j}$ for all $f \\in \\Pi_j$ and\n            \\State $d(F'_j,v) := 2\\lambda_{i_j} + \\min_{f \\in \\Pi_j} d(f,v)$ for all other $v$ \\label{alg:kmedpm_dist}\n        \\EndFor\n        \\State For $S \\subseteq \\pazocal{F}$, define $\\textsf{improve}(S) := \\textsf{cost}(C',F') - \\textsf{cost}(C',F' \\cup S) $ \\label{alg:kmedpm_defimp}\n        \\State  $S_{max} \\leftarrow S \\subseteq  \\pazocal{F}$ that maximizes $\\textsf{improve}(S)$ such that $|S \\cap \\Pi_j|=1, \\forall j\\in[k]$  \\label{alg:kmedpm_submod}\n        \\State \\Comment{Use $(1-1/e)$ approximation algorithm} \\label{alg:kmedpm_subapx}\n        \\If{$\\textsf{cost}(C',S_{max}) < \\textsf{cost}(C',S^*)$}\n            \\State $S^* \\leftarrow S$\n        \\EndIf\n    \\EndFor\n\\EndFor\n\\Return $S^*$\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n\\fi\n\n\n\n\\textbf{Correctness: }\nGiven $I=\\divkins$ . Let $\\mathcal{J} :=\\{J_\\ell\\}$ be the instances of\n\\kmedianpm generated by Algorithm~\\ref{algo:divkmed} at Line~9 (For simplicity,\nwe do not consider client coreset  $C'$ here). For correctness, we show that\n$F^* = \\{f^*_i\\}_{i \\in [k]} \\subseteq \\pazocal{F}$ is feasible to $I$ if and\nonly if there exists $J_{\\ell^*} =  ((V,d),\\{E^*_{1},\\cdots, E^*_{k}\\},C) \\in\n\\mathcal{J}$ such that $F^*$ is feasible to $J_{\\ell^*}$. This is sufficient,\nsince the objective function of both the problems is same, and hence returning\nminimum over $\\mathcal{J}$ of an optimal (approximate) solution obtains an\noptimal (approximate resp.) solution to $I$.\nWe need the following proposition for the proof.\n\\begin{proposition} \\label{cl:setfacvec}\nFor all $E \\in \\mathcal{E}$, and for all $f \\in E$, we have $\\vec{\\chi}_f = \\vec{\\chi}_E$.\n\\end{proposition}\n\\begin{proof}\nFix $E \\in \\mathcal{E}$ and $f \\in E$. Since $E \\in \\mathcal{E}$, there exists\n$\\vec{\\gamma} \\in \\{0,1\\}^t$ such that $E_{\\vec{\\gamma}} = E$. But this means\n$\\vec{\\chi}_{E} = \\vec{\\gamma}$. On the other hand, since $f \\in\nE_{\\vec{\\gamma}}$, we have that $\\vec{\\chi}_f = \\vec{\\gamma}$.\n\\end{proof}\nSuppose $F^* = \\{f^*_i\\}_{i \\in [k]} \\subseteq \\pazocal{F}$ is a feasible solution to $I: $\n$\n\\sum_{i \\in [k]} \\vec{\\chi}_{f^*_i} \\ge \\vec{r}.\n$\nThen, consider the instance $J_{\\ell^*} =  ((V,d),\\{E^*_{1},\\cdots,\nE^*_{k}\\},C)$  with $E^*_i = E_{\\vec{\\chi}_{f^*_i}}$, for all $i \\in [k]$.\nSince, \n\\[\n\\sum_{i \\in [k]} \\vec{\\chi}_{E^*_i} = \\sum_{i \\in [k]} \\vec{\\chi}_{f^*_i} \\ge \\vec{r}\n\\]\nwe have that $J_{\\ell^*} \\in  \\mathcal{J}$.\nFurther, $F^*$ is feasible to $J_{\\ell^*}$ since $F^* \\cap E^*_{i} = f^*_i$ for all $i \\in [k]$.\nFor the other direction, fix an instance $J_{\\ell^*} =  ((V,d),\\{E^*_{1},\\cdots,\nE^*_{k}\\},C) \\in \\mathcal{J}$ and a feasible solution $F^* =\\{f^*_i\\}_{i \\in\n[k]}$ for $J_{\\ell^*}$. From Claim~\\ref{cl:setfacvec} and the feasiblity of\n$F^*$, we have $\\vec{\\chi}_{f^*_i} = \\vec{\\chi}_{E^*_i}$. Hence,\n\\[\n \\sum_{i \\in [k]} \\vec{\\chi}_{f^*_i} = \\sum_{i \\in [k]} \\vec{\\chi}_{E^*_i}  \\ge \\vec{r}\n\\]\nwhich implies $F^* =\\{f^*_i\\}_{i \\in [k]}$ is a feasible solution to $I$. To\ncomplete the proof, we need to show that the distance function defined in\nLine~10 is a metric, and \\impr function defined in Line~11 is a monotone\nsubmodular function. Both these proofs are the same as that in\n\\cite{cohen2019tight}. \n%For completeness, we include them in the appendix \\todo{Add..}.\n\n\n\\textbf{Approximation Factor: } For $I=\\divkins$, let\n$I'=((U,d),C',F,\\mathcal{G}, \\vec{r},k)$  be the instance with client coreset\n$C'$. \n%Following is the intuition that any $\\alpha$ approximation for \\kmedianpm on\n%$I'$ is also roughly an $\\alpha$ approximation for \\divkmedian on $I$.\nLet $F^* \\subseteq \\pazocal{F}$ be an optimal solution to $I$, and let\n$\\Tilde{F}^* \\subseteq \\pazocal{F}$ be an optimal solution to $I'$. Then, from\n\\coreset Lemma~\\ref{sec:algoritheorem:coresets}, we have that\n\\[\n(1-\\nu) \\cdot \\textsf{cost}(F^*,C) \\le \\textsf{cost}(F^*,C') \\le (1+\\nu) \\cdot \\textsf{cost}(F^*,C).\n\\]\n%Hence, from the correctness of Algorithm~\\ref{alg:divkmed}, we have that if\n%Algorithm~\\ref{alg:kmedpm} is an $\\alpha$ approximation for \\kmedianpm for\n%instance $I'$, then Algorithm~\\ref{alg:divkmed} is $(1+\\delta)\\alpha = (\\alpha\n%+ \\delta')$ approximation algorithm for \\divkmedian for $I$, for some $\\delta'\n%= \\delta \\alpha$. \n%The following lemma implies that the approximation factor of\n%Algorithm~\\ref{alg:divkmed} is at most $(1+2/e + \\epsilon')$ for some\n%$\\epsilon'$. \nThe following proposition, whose proof closely follows that\nin~\\cite{cohen2019tight}, bounds the approximation factor of\nAlgorithm~\\ref{algo:divkmed}.\n\n\n\n\n\\begin{proposition} \\label{lem:fptapxpm}\nFor $\\epsilon' >0$, let $J_\\ell=((V,d),\\{E_1,\\cdots,E_k\\},C',\\epsilon')$ be an input to Algorithm~\\ref{algo:kmedpm}, and let $S^*_\\ell$ be the set returned. Then,\n\\[\n\\textsf{cost}(C',S^*_\\ell) \\le (1+2/e + \\epsilon') \\cdot \\textsf{OPT}(J_\\ell),\n\\]\nwhere $\\textsf{OPT}(J_\\ell)$ is the optimal cost of \\kmedianpm on $J_\\ell$. Similarly, for \\divkmeans,\n\\[\n\\textsf{cost}(C',S^*_\\ell) \\le (1+8/e + \\epsilon') \\cdot \\textsf{OPT}(J_\\ell),\n\\]\n\\end{proposition}\n%Suppose $F^* \\subseteq \\pazocal{F}$ be an optimal solution to instance $I$.\nThis allows us to bound the approximation factor of Algorithm~\\ref{algo:divkmed}.\n\\begin{align*}\n\\textsf{cost}(T^*, C') \n%&\\le (1+\\epsilon/6) \\cdot \\textsf{cost}(\\textsf{ALG}, C')  \\qquad \\text{using } \\delta = \\epsilon/6\\\\\n=   \\min_{J_\\ell \\in \\mathcal{J}}  \\textsf{cost}(S^*_\\ell,C') \n\\le  (1+2/e+\\epsilon') \\cdot \\textsf{cost}(\\Tilde{F}^*,C') \\\\\n\\le (1+2/e +\\epsilon') \\cdot \\textsf{cost}(F^*,C')\n\\le (1+2/e +\\epsilon')(1+ \\nu) \\cdot \\textsf{cost}(F^*,C).\n\\end{align*}\nOn the other hand, we have $\\textsf{cost}(T^*,C) \\le (1+2\\nu) \\cdot \\textsf{cost}(T^*, C')$. Hence, using $\\epsilon'=\\epsilon/4$ and $\\nu = \\epsilon/16$, we have\n\\begin{align*}\n\\textsf{cost}(T^*,C) &\\le (1+2/e +\\epsilon')(1+ \\nu) (1+2\\nu) \\cdot \\textsf{cost}(F^*,C) \\\\\n&\\le (1+2/e+\\epsilon)\\cdot \\textsf{cost}(F^*,C)\n\\end{align*}\nfor $\\epsilon\\le 1/2$. Analogous calculations holds for \\divkmeans. This finishes the proof of Lemma~\\ref{lemma:partition}.\nNow, we prove Proposition~\\ref{lem:fptapxpm}.\n\n\n\\begin{proof}\nLet $F^*_\\ell$ be an optimal solution to $J_\\ell$. Then, since\n$\\textsf{cost}(C',F'\\cup F^*_\\ell) = \\textsf{cost}(C',F^*_\\ell)$, we have that\n$F^*_\\ell$ is a maximizer of the function $\\impr(\\cdot)$, defined at Line~11.\nHence due to submodular optimization, we have that\n\\[\n\\impr(S^*_\\ell) \\ge (1-1/e) \\cdot \\impr(F^*_\\ell).\n\\]\nThus,\n\\begin{align*}\n    \\cost(C',S^*_\\ell) &= \\cost(C',F' \\cup S^*_\\ell) \\\\\n    &= \\cost(C',F') - \\impr(S^*_\\ell)\\\\\n    &\\le  \\cost(C',F') - (1-1/e) \\cdot \\impr(F^*_\\ell)\\\\\n    &= \\cost(C',F') - (1-1/e) \\cdot \\left( \\cost(C',F') - \\cost(C',F^*_\\ell) \\right) \\\\\n    &= 1/e \\cdot \\cost(C',F') + (1-1/e) \\cdot \\cost(C',F^*_\\ell)\n\\end{align*}\n\\end{proof}\n\n\nThe following proposition bounds $\\cost(C',F')$ in terms of $\\cost(C',F^*_\\ell)$.\n\\begin{proposition} \n $\\cost(C',F') \\le (3+2\\eta) \\cdot \\cost(C',F^*_\\ell)$.\n\\end{proposition}\nSetting $\\eta = \\frac{e}{2} \\epsilon'$, finishes the proof  for \\divkmedian,\n\\begin{align*}\n \\cost(C',S^*_\\ell) &\\le (3+ e \\epsilon')/e \\cdot \\cost(C',F^*_\\ell) + (1-1/e)\n\\cdot \\cost(C',F^*_\\ell)\\\\ &\\le (1+2/e+\\epsilon') \\cdot \\cost(C',F^*_\\ell).\n\\end{align*}\nFor \\divkmeans, setting $\\eta = \\frac{e}{16} \\epsilon'$, finishes the proof of Proposition~\\ref{lem:fptapxpm},\n\\begin{align*}\n\\cost(C',S^*_\\ell) &\\le (3+ 2e\\epsilon'/16  )^2/e \\cdot \\cost(C',F^*_\\ell) + (1-1/e)\n \\cost(C',F^*_\\ell)\\\\ &\\le (1+8/e+\\epsilon') \\cdot \\cost(C',F^*_\\ell).\n\\end{align*}\nfor $\\eta \\le 1$.\n%\n%Finally, we prove the above proposition.\n\\begin{proof}\nTo this end, it is sufficient to prove that for any client $c' \\in C'$, it holds\nthat $d(c',F') \\le (3+2\\eta) d(c',F^*_\\ell)$. Fix $c' \\in C$, and let\n$f^*_{\\ell_j} \\in F^*_\\ell$ be the closest facility in $F^*_\\ell$ with\n$\\lambda^*_j$ such that $\\lambda^*_j = d(c'_{i_j},f^*_{\\ell_j})$. Now,\n\\[\nd(c',f^*_{\\ell_j}) \\ge d(c'_{i_j},f^*_{\\ell_j}) \\ge (1+\\eta)^{([\\lambda^*_j]_D -1)} \\ge \\frac{\\lambda^*_j}{1+\\eta}.\n\\]\nUsing, triangle inequality and the above equation, we have,\n\\begin{align*}\nd(c',F') &\\le d(c,F'_j) \\le  d(c',F^*_{\\ell_j}) + d(F^*_\\ell,F') \\le\nd(c',f^*_{\\ell_j}) + 2 \\lambda^*_j \\\\&\\le (3+2\\eta)  d(c',f^*_{\\ell_j}).\n\\end{align*}\n%\\paragraph{For \\divkmeans: }\n\\end{proof}\n%\\end{proof}\n\n"
                },
                "subsection 10.5": {
                    "name": "Other proofs",
                    "content": "\\label{app:otherproofs}\n\\paragraph{Proof of Corollary~\\ref{corollary:introduction:1}}\n\\seth implies that there is no $\\bigO(|V|^{k-\\epsilon})$ algorithm for\n\\dominatingset, for any $\\epsilon>0$. The reduction\nin~\\cite[Lemma~1]{thejaswi2021diversity} creates an instance of \\divkmedian (\\divkmeans resp.)\nwhere $F=V$. Hence, any \\fpt exact or approximate algorithm running in time\n$\\bigO(|F|^{k-\\epsilon})$ for \\divkmedian (\\divkmeans resp.) contradicts \\seth.\n\n$\\hfill\\square$\n\\paragraph{Proof of Proposition~\\ref{proposition:introduction:2}}\nFirst, note that any \\fpt algorithm $\\mathcal{A}$  achieving a multiplicative\nfactor approximation for \\divkmedian (\\divkmeans resp.) needs to solve the lower bound requirements\nfirst. Since these requirements capture\n\\dominatingset~\\cite[Lemma~1]{thejaswi2021diversity}, it means $\\mathcal{A}$\nsolves \\dominatingset in \\fpt time, which is a contradiction. Finally, noting\nthe fact that finding even $f(k)$ size dominating set, for any $f(k)$, is also\n\\wone-hard due to~\\cite{karthik2019on} finishes the proof.\n\n$\\hfill\\square$\n%\\paragraph{Proof of Proposition~\\ref{proposition:introduction:3}}\n%$\\hfill\\square$\n\n\n\n"
                }
            }
        },
        "figures": {
            "fig:mainfptapx": "\\begin{figure}\n\\centering\n\\scalebox{0.6}{\\input{tikz/construction}}\n\\caption{An illustration of facility selection for \\fpt algorithm for solving\n\\kmedianpm instance.\n%based on the ideas of Cohen-Addad et\n%al.~\\cite{cohen2019tight}.\\label{fig:mainfptapx} \\note{figure needs to be\n%modified, the point needs to be on the circumference of red circle.}}\n}\n\\label{fig:mainfptapx}\n\\end{figure}",
            "fig:log_k_standard": "\\begin{figure}\n    \\hspace*{-1cm}\n    \\includegraphics[scale=0.38]{figures/log2_k_standard.png}\n    \\caption{Runtimes of bicriteria algorithms with various approaches in medium case while scaling the number of medians}\n    \\label{fig:log_k_standard}\n\\end{figure}",
            "fig:log_k_worst": "\\begin{figure}\n    \\hspace*{-1cm}\n    \\includegraphics[scale=0.38]{figures/log2_k_worst.png}\n    \\caption{Runtimes of bicriteria algorithms with various approaches in th worst case while scaling the number of medians}\n    \\label{fig:log_k_worst}\n\\end{figure}",
            "fig:log_t_standard": "\\begin{figure}\n    \\hspace*{-1cm}\n    \\includegraphics[scale=0.38]{figures/log2_t_standard.png}\n    \\caption{Runtimes of bicriteria algorithms with various approaches in medium case while scaling the number of groups}\n    \\label{fig:log_t_standard}\n\\end{figure}",
            "fig:log_t_worst": "\\begin{figure}\n    \\hspace*{-1cm}\n    \\includegraphics[scale=0.38]{figures/log2_t_worst.png}\n    \\caption{Runtimes of bicriteria algorithms with various approaches in th worst case while scaling the number of groups}\n    \\label{fig:log_t_worst}\n\\end{figure}",
            "fig:scaling:feasibility": "\\begin{figure*}\n\\arraycolsep=0.0pt\\def\\arraystretch{0.0}\n\\captionlistentry{}\n\\label{fig:scaling:feasibility}\n\\captionlistentry{}\n\\label{fig:scaling:bicriteria:1}\n\\captionlistentry{}\n\\label{fig:scaling:es-ls}\n\n\\begin{tabular}{c}\n\\includegraphics[width=0.8\\linewidth]{figures/scaling-feasibility.pdf}\\\\\n{\\bf Figure~2: Scalability of algorithms for finding feasible constraint pattern.}\\\\\n\\includegraphics[width=0.8\\linewidth]{figures/scaling-bicriteria-kmedian.pdf}\\\\\n{\\bf Figure~3: Scalability of bicriteria algorithms for \\divkmedian.}\\\\\n\\includegraphics[width=0.78\\linewidth]{figures/scaling-es-ls.pdf}\\\\\n{\\bf Figure~4: Scalability of $\\ES+\\lsone$ algorithm for \\divkmedian.}\n\\end{tabular}\n%\\caption{\\label{fig:scaling:feasibility}Scalability of algorithms for finding a feasible constraint pattern.}\n\\end{figure*}",
            "fig:scaling:bicriteria:1": "\\begin{figure*}\n\\arraycolsep=0.0pt\\def\\arraystretch{0.0}\n\\captionlistentry{}\n\\label{fig:scaling:feasibility}\n\\captionlistentry{}\n\\label{fig:scaling:bicriteria:1}\n\\captionlistentry{}\n\\label{fig:scaling:es-ls}\n\n\\begin{tabular}{c}\n\\includegraphics[width=0.8\\linewidth]{figures/scaling-feasibility.pdf}\\\\\n{\\bf Figure~2: Scalability of algorithms for finding feasible constraint pattern.}\\\\\n\\includegraphics[width=0.8\\linewidth]{figures/scaling-bicriteria-kmedian.pdf}\\\\\n{\\bf Figure~3: Scalability of bicriteria algorithms for \\divkmedian.}\\\\\n\\includegraphics[width=0.78\\linewidth]{figures/scaling-es-ls.pdf}\\\\\n{\\bf Figure~4: Scalability of $\\ES+\\lsone$ algorithm for \\divkmedian.}\n\\end{tabular}\n%\\caption{\\label{fig:scaling:feasibility}Scalability of algorithms for finding a feasible constraint pattern.}\n\\end{figure*}",
            "fig:scaling:es-ls": "\\begin{figure*}\n\\arraycolsep=0.0pt\\def\\arraystretch{0.0}\n\\captionlistentry{}\n\\label{fig:scaling:feasibility}\n\\captionlistentry{}\n\\label{fig:scaling:bicriteria:1}\n\\captionlistentry{}\n\\label{fig:scaling:es-ls}\n\n\\begin{tabular}{c}\n\\includegraphics[width=0.8\\linewidth]{figures/scaling-feasibility.pdf}\\\\\n{\\bf Figure~2: Scalability of algorithms for finding feasible constraint pattern.}\\\\\n\\includegraphics[width=0.8\\linewidth]{figures/scaling-bicriteria-kmedian.pdf}\\\\\n{\\bf Figure~3: Scalability of bicriteria algorithms for \\divkmedian.}\\\\\n\\includegraphics[width=0.78\\linewidth]{figures/scaling-es-ls.pdf}\\\\\n{\\bf Figure~4: Scalability of $\\ES+\\lsone$ algorithm for \\divkmedian.}\n\\end{tabular}\n%\\caption{\\label{fig:scaling:feasibility}Scalability of algorithms for finding a feasible constraint pattern.}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{align*}\n    \\text{Minimize} \\quad &  0 \\cdot x~\\text{such that}~\\mathcal{E} \\cdot x \\geq \\reqvec,\n    \\\\  &  \\sum_i x_i \\leq k, 0 \\leq x_i \\leq f(E_i). \n\\end{align*}",
            "eq:2": "\\begin{align*}\n\\textsf{cost}(T^*, C') \n%&\\le (1+\\epsilon/6) \\cdot \\textsf{cost}(\\textsf{ALG}, C')  \\qquad \\text{using } \\delta = \\epsilon/6\\\\\n=   \\min_{J_\\ell \\in \\mathcal{J}}  \\textsf{cost}(S^*_\\ell,C') \n\\le  (1+2/e+\\epsilon') \\cdot \\textsf{cost}(\\Tilde{F}^*,C') \\\\\n\\le (1+2/e +\\epsilon') \\cdot \\textsf{cost}(F^*,C')\n\\le (1+2/e +\\epsilon')(1+ \\nu) \\cdot \\textsf{cost}(F^*,C).\n\\end{align*}",
            "eq:3": "\\begin{align*}\n\\textsf{cost}(T^*,C) &\\le (1+2/e +\\epsilon')(1+ \\nu) (1+2\\nu) \\cdot \\textsf{cost}(F^*,C) \\\\\n&\\le (1+2/e+\\epsilon)\\cdot \\textsf{cost}(F^*,C)\n\\end{align*}",
            "eq:4": "\\begin{align*}\n \\cost(C',S^*_\\ell) &\\le (3+ e \\epsilon')/e \\cdot \\cost(C',F^*_\\ell) + (1-1/e)\n\\cdot \\cost(C',F^*_\\ell)\\\\ &\\le (1+2/e+\\epsilon') \\cdot \\cost(C',F^*_\\ell).\n\\end{align*}",
            "eq:5": "\\begin{align*}\n\\cost(C',S^*_\\ell) &\\le (3+ 2e\\epsilon'/16  )^2/e \\cdot \\cost(C',F^*_\\ell) + (1-1/e)\n \\cost(C',F^*_\\ell)\\\\ &\\le (1+8/e+\\epsilon') \\cdot \\cost(C',F^*_\\ell).\n\\end{align*}"
        }
    }
}