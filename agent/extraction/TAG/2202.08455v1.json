{
    "meta_info": {
        "title": "Transformer for Graphs: An Overview from Architecture Perspective",
        "abstract": "Recently, Transformer model, which has achieved great success in many\nartificial intelligence fields, has demonstrated its great potential in\nmodeling graph-structured data. Till now, a great variety of Transformers has\nbeen proposed to adapt to the graph-structured data. However, a comprehensive\nliterature review and systematical evaluation of these Transformer variants for\ngraphs are still unavailable. It's imperative to sort out the existing\nTransformer models for graphs and systematically investigate their\neffectiveness on various graph tasks. In this survey, we provide a\ncomprehensive review of various Graph Transformer models from the architectural\ndesign perspective. We first disassemble the existing models and conclude three\ntypical ways to incorporate the graph information into the vanilla Transformer:\n1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and\n3) Improved Attention Matrix from Graphs. Furthermore, we implement the\nrepresentative components in three groups and conduct a comprehensive\ncomparison on various kinds of famous graph data benchmarks to investigate the\nreal performance gain of each component. Our experiments confirm the benefits\nof current graph-specific modules on Transformer and reveal their advantages on\ndifferent kinds of graph tasks.",
        "author": "Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, Yu Rong",
        "link": "http://arxiv.org/abs/2202.08455v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "8 pages, 1 figures"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n% Graphs are a kind of data structure which models a set of objects (nodes) and their relationships (edges). Recently, researches on analyzing graphs with machine learning have been receiving more and more attention because of the great expressive power of graphs. As a unique non-Euclidean data structure for machine learning, graph analysis focuses on tasks such as node classification, link prediction, and clustering. Graph neural networks (GNNs) are deep learning based methods that operate on graph domain. Due to its convincing performance, GNN has become a widely applied graph analysis method recently. However, most of the prevalent GNN perform xxx, i.e., xxx, which may limit their representation power,\n% efficiency and interpretability.\n\n% \u5f53\u524dGNN\u6a21\u578b\u7684\u53d1\u5c55\u548c\u57fa\u4e8eMessagePassing\u65b9\u6cd5\u7684\u95ee\u9898(over sm, pooling etc) --- \u9650\u5236\u4e86GNN\u7684\u8868\u8fbe\u80fd\u529b\u7684\u53d1\u5c55\u548c\u6027\u80fd\u63d0\u5347\u9047\u5230\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u4e00\u4e9b\u65b0\u5174\u7684\u9886\u57df\u7684\u6570\u636e\u4e0a\uff0c\u5316\u5b66\u5206\u5b50\uff0c\u86cb\u767d\u8d28\uff0c\u4ee5\u53ca\u5176\u4ed6\u3002\n%Graph is a kind of data structure that structurally depict a set of objects (nodes) with their relationships (edges). In recent years, due to the powerful expressive power of graph, the research of analyzing graph with machine learning has attracted more and more attention. As a unique non-Euclidean data structure for machine learning, graph analysis focuses on tasks such as node classification, link prediction, and clustering. Graph Neural Networks (GNNs) are deep learning-based methods that operate on the graph domain. Due to its convincing performance, GNNs have recently become a widely used graph analysis method.\n\nGraph is a kind of data structure that structurally depicts a set of objects (nodes) with their relationships (edges). As a unique non-Euclidean data structure, graph analysis focuses on tasks such as node classification \\cite{yang2016revisiting}, link prediction \\cite{zhang2018link}, and clustering \\cite{Aggarwal2010}. \n% In recent years, due to the rich expressive power of graph, the researches of analyzing graph via deep learning have attracted more and more attentions. \nRecent research on analyzing graphs using deep learning has attracted more and more attention due to the rich expressive power of deep learning models.\nGraph Neural Networks (GNNs) \\cite{kipf2016semi}, as a kind of deep learning-based method, are recently becoming a widely-used graph analysis tools due to their convincing performance. \nMost of the current GNNs are based on the Message Passing paradigm \\cite{gilmer2017neural}, the expressive power of which is bounded by the Weisfeiler-Lehamn isomorphism hierarchy \\cite{maron2019provably,xu2018powerful}. Worse still, as pointed out by \\cite{kreuzer2021rethinking},  GNNs suffer from the $\\textit{over-smoothing}$ problem due to repeated local aggregation, and the $\\textit{over-squashing}$ problem due to the exponential computation cost with the increase of model depth. Several studies \\cite{zhao2019pairnorm,rong2019dropedge,xu2018representation} try to address such problems. Nevertheless, none of them seems to be able to eliminate these problems from the Message Passing paradigm. \n\n\n%due to exponential blow-up in computation paths when the model depth increases \\cite{kreuzer2021rethinking}.\n%which constructs the model based on the neighborhood aggregation. However, the Message Passing suffers from many problems, such as over-somoothing, long-term relation modeling, \n%TODO\n%However, most of GNNs that based on Message Passing method all suffer from many problems, such as, over smoothing \\cite{zhao2019pairnorm,rong2019dropedge}, locality modeling, and so on; which limits the development of model expressive ability and model performance, and even encounter bottlenecks in some emerging fields of structured data analysis including chemical molecules, protein, and others.\n% However, most popular GNNs perform xxx, i.e. xxx, which may limit their representational power,\n% efficiency and interpretability.\n% Transformer \u4e00\u7c7b\u5f88\u5f3a\u7684\u6a21\u578b\uff0c\u5176\u53d8\u4f53\u5728CV\uff0cNLP\u7b49\u9886\u57df\u90fd\u53d6\u5f97\u4e86\u5f88\u4e0d\u9519\u7684\u6548\u679c\u3002Recently\uff0c\u4e5f\u6709\u4e00\u4e9b\u5c1d\u8bd5\u5728\u56fe\u7ed3\u6784\u4e0a\u5f15\u5165Transformer\u7ed3\u6784\u7684\u5de5\u4f5c\uff0c\u5e76\u4e14\u53d6\u5f97\u4e86\u4e00\u4e9b\u4ee4\u4eba\u60ca\u559c\u7684\u8fdb\u5c55\u3002\nOn the other hand, Transformers and its variants, as a powerful class of models, are playing an important role in various areas including Natural Language Processing (NLP)~\\cite{vaswani2017attention}, Computer Vision (CV)~\\cite{forsyth2011computer}, Time-series Analysis~\\cite{hamilton2020time}, Audio Processing~\\cite{purwins2019deep}, etc. Moreover, recent years have witnessed many successful Transformer variants in modeling graphs. These models have achieved competitive or even superior performance against GNNs in many applications, such as Quantum Property Prediction \\cite{hu2021ogblsc}, Catalysts Discovery \\cite{ocp_dataset} and Recommendation Systems \\cite{min2022masked}. \n% However, there is still no comprehensive literature reviews or systematic evaluations for these Transformer variants.\nThe literature reviews and systematic evaluations for these Transformer variants are, however, lacking.\n\n\n% \u5728\u8fd9\u4e2aSurevy\u4e2d\uff0c\u6211\u4eec\u9488\u5bf9Transformer\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e0a\u7684\u5e94\u7528\u505a\u4e86\u4e00\u4e2a\u4ecb\u7ecd\uff08\u5177\u4f53\u6d89\u53ca\u591a\u5c11\u4e2a\u6a21\u578bXXX\uff09\uff0c\u5e76\u4e14\u9488\u5bf9Transformer\u4e8e\u56fe\u4e0a\u4e00\u4e9b\u5177\u4f53\u6539\u9020\u65b9\u5f0f\u8fdb\u884c\u4e86\u5206\u7c7b\u6c47\u603b\u3002\u5177\u4f53\u7684\uff0c\u6211\u4eec\u5c06\u8fd9\u4e9b\u6539\u8fdb\u5206\u4e3a\u4e09\u7c7b\uff1aXXXXX\u3002 \u66f4\u8fdb\u4e00\u6b65\u7684\uff0c\u6211\u4eec\u8fd8\u5728\u51e0\u7c7b\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\u4e0a\u7edf\u4e00\u6d4b\u8bd5\u4e86\u8fd9\u4e9b\u6539\u8fdb\u7684\u6709\u6548\u6027\u3002 \u300c\u7ed3\u8bba\u300d\n\n\nThis survey gives an overview of the current research progress on incorporating Transformers in graph-structured data.\n%In this survey, we give an overview of existing work on incorporating Transformers in graph structured data.\n% This survey gives an overview of existing work on incorporating graph inductive bias in Transformer models. \n% Concretely, we provide a comparison of over 20 graph transformer models and and group them into categories. \nConcretely, we provide a comprehensive review of over 20 Graph Transformer models from the architectural design perspective. \nWe first dismantle the existing models and conclude three typical ways to incorporate the graph information into the vanilla Transformer: \\textbf{1) GNNs as Auxiliary Modules}, \\emph{i.e.}, directly inject GNNs into Transformer architecture. Specifically, according to the relative position between GNN layers and Transformer layers, existing GNN-Transformer architectures can be categorized into three types: (a) build Transformer blocks on top of GNN blocks, (b) stack GNN blocks and Transformer blocks in each layer, (c) parallel GNN blocks and Transformer blocks in each layer. \n\\textbf{2) Improved Positional Embedding from Graphs}, \\emph{i.e.}, compress the graph structure into positional embedding vectors and add them to the input before it is fed to the vanilla Transformer model. This graph positional embedding can be derived from the structural information of graphs, such as degree and centrality. \n%The graph positional embedding is normally generated from the adjacent matrix.\n\\textbf{3) Improved Attention Matrices from Graphs}, \\emph{i.e.}, inject graph priors into the attention computation via graph bias terms, or restrict a node only attending to local neighbours in the graph, which can be computationally formulated as an attention masking mechanism.\n%adapt self-attention mechanism to GNN-like architecture by restrict a node only attending to local node neighbours in the graph, this can be computationally formulated as an attention masking mechanism.\n\nAdditionally, in order to investigate the effectiveness of existing models in various kinds of graph tasks, we implement the representative components in three groups and conduct comprehensive ablation studies on six popular graph-based benchmarks to uniformly test the real performance gain of each component. \nOur experiments indicate that: 1). Current models incorporating the graph information can improve the performance of Transformer on both graph-level and node-level tasks. 2).  Utilizing GNN as auxiliary modules and improving attention matrix from graphs generally contributes more performance gains than encoding graphs into positional embeddings. 3). The performance gain on graph-level tasks is more significant than that on node-level tasks. \n%4) Different group of models enjoys different kinds of graph tasks. \n4) Different kinds of graph tasks enjoy different group of models.\n\n\n%we implement the representative components in three groups and conduct comprehensive ablation studies on several popular graph-based benchmarks to uniformly test the real effectiveness of each component.\n% Then, we group them into three categories: GNN as Embeddings in Transformer, incorporate graph prior via node positional embedding, and incorporate graph prior via attention matrix. \n%In general, we gain the following observations:\n%\\begin{enumerate}\n%    \\item Utilizing GNN as embeddings and improving attention matrix from graphs generally contributes more performance gains than encoding graphs into positional embeddings.\n%    \\item The effects of existing graph modules are generally more significant in small-graph tasks than large-graph tasks.\n%    \\item The effects of most components are sensitive to the model size, and adding graph components without extra hyper-parameter search could impair the performance.\n%\\end{enumerate}\n\n\n\n\n\n\n% To summarize, our contributions are: \n\n% \\begin{itemize}[noitemsep,topsep=0pt,parsep=5pt,partopsep=0pt,leftmargin=*]\n% \\item \n% To the best of our knowledge, we are the first to provide a detailed review over existing Transformer variants on graphs from the architectural design perspective.\n\n% % We present a general design pipeline and discuss the variants of each module. We also introduce researches on theoretical and empirical analyses of graph transformer models.\n% \\item \n% We dismantle the existing models and conclude three typical ways to incorporate the graph information into the vanilla Transformer: 1) GNN as Embedding, 2) Improved Positional Embedding from Graphs, and 3) Improved Attention Matrix from Graphs.\n% \\item \n% Extensive experimental evaluations on several popular graph-based benchmarks to investigate the real effectiveness of  each component.\n% \\item \n% We propose xxx open problems for future research. We provide a thorough analysis of each problem and propose future research directions.\n% \\end{itemize}\n\nThe rest of this survey is organized as follows. We first review the general vanilla Transformer Architecture in Section~\\ref{sec.prelimiary}.  Section~\\ref{sec.transformer_on_graphs} summarizes existing works about the Transformer variant on Graphs and systematically categorizes these methods into three groups. In Section~\\ref{sec.exp}, comprehensive ablation studies are conducted to verify the effectiveness and compatibility of these proposed models. In Section~\\ref{sec.conclusion}, we conclude this survey and discuss several future research directions.\n% about this area.  \n\n%The rest of this survey is organized as follows. In Section 2, we first introduce the general vanilla Transformer Architecture. Following that, in Section 3, we discuss in detail to review Transformer variants on Graphs. \n\n%Then we systematically categorize these methods and divide them into three categories. These are included in Section 4 to Section 6. In Section 7, comprehensive ablation studies are also carried out to verify the compatibility of these proposed techniques. In Section 8, we propose three open problems of graph transformer models as well as several future research directions. And finally, we conclude the survey in Section 9.\n\n\n\n"
            },
            "section 2": {
                "name": "Transformer Architecture",
                "content": "\\label{sec.prelimiary}\nTransformer~\\cite{vaswani2017attention} architecture was first applied to machine translation. \n%, and then was widely extended to many research areas such as computer vision \\cite{forsyth2011computer}, time series analysis \\cite{hamilton2020time}\uff0caudio processing \\cite{purwins2019deep}, and so on. \nIn the paper, Transformer is introduced as a novel encoder-decoder architecture built with multiple blocks of self-attention. Let $\\BX \\in \\mathbb{R}^{n\\times d}$ to be the input of each Transformer layer, where $n$ is number of tokens, $d$ is the dimension of each token, \nthen one block layer can be a function $f_\\theta: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$ with $f_\\theta(\\BX)=:\\BZ$ defined by:\n%\\begin{equation}\n\\begin{align}\n\\mathbf{A}&=\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}, \\label{eq:attention1}\\\\\n\\widetilde{\\BX}&=\\operatorname{SoftMax}(\\mathbf{A}) (\\mathbf{X} \\mathbf{V}), \\label{eq:attention2}\\\\\n\\mathbf{M}&=\\operatorname{LayerNorm}_{1}(\\widetilde{\\BX}\\BO+\\mathbf{X}), \\label{eq:attention3}\\\\\n\\mathbf{F}&=\\sigma(\\mathbf{M W}_1+\\mathbf{b}_1) \\mathbf{W}_2+\\mathbf{b}_2, \\label{eq:ffn}\\\\\n\\mathbf{Z}&=\\operatorname{LayerNorm}_{2}(\\mathbf{M}+\\mathbf{F}), \\label{eq:layernorm}\n%\\end{equation}\n\\end{align}\nwhere Equation \\ref{eq:attention1}, Equation \\ref{eq:attention2}, and Equation \\ref{eq:attention3} are the attention computation; while Equation \\ref{eq:ffn} and Equation \\ref{eq:layernorm} are the position-wise feed-forward network (FFN) layers.\nHere, $\\text{Softmax}(\\cdot)$ refers to the row-wise softmax function, $\\text{LayerNorm}(\\cdot)$ refers to layer normalization function \\cite{ba2016layer}, and $\\sigma$ refers to the activation function. $\\BQ,\\BK,\\BV,\\BO \\in \\mathbb{R}^{d \\times d}, \\BW_1 \\in \\mathbb{R}^{d\\times d_f},\\b_1 \\in \\mathbb{R}^{d_f},\\BW_2 \\in \\mathbb{R}^{d_f\\times d} ,\\b_2 \\in \\mathbb{R}^{d}$ are trainable parameters in the layer.\nFurthermore, it is common to consider multiple attention heads to extend the self-attention to Multi-head Self-attention (MHSA). Specifically, $\\BQ,\\BK,\\BV$ are decomposed into $H$ heads with $\\BQ^{(h)},\\BK^{(h)},\\BV^{(h)} \\in \\mathbb{R}^{d\\times d_h}$ with $d=\\sum_{h=1}^{H}d_h$, and the matrices $\\widetilde{\\BX}^{(h)} \\in \\mathbb{R}^{n \\times d_h}$ from attention heads are concatenated to obtain $\\widetilde{\\BX}$. In this case,\nEquation \\ref{eq:attention1} and Equation \\ref{eq:attention2} respectively become:\n% can be modified as:\n\\begin{align}\n\\BA^{(h)}=\\frac{1}{\\sqrt{d}} \\BX \\BQ^{(h)}(\\BX\\BK^{(h)})^{\\top}, \\\\\n\\widetilde{\\BX}=\\|_{h=1}^{H}(\\operatorname{SoftMax}(\\BA^{(h)}) \\BX \\BV^{(h)}).\n\\end{align}\nThe multi-head mechanism enables the model to implicitly learn representation from different aspects.\nApart from the attention mechanism, the paper uses $\\operatorname{sine}$ and $\\operatorname{cosine}$ functions with different frequencies as positional embedding to distinguish the position of each token in the sequence.\n\n\n\n% \\begin{table}[]\n% \\centering\n% \\caption{A summary of papers that applied Transformers on graph-structured data. \\textbf{GA}: GNNs as Embedding; \\textbf{PE}: Improved Positional Embedding from Graphs; \\textbf{AT}: Improved Attention Matrix from Graphs. }\n% \\label{tab_summary2}\n% \\small\n% \\setlength\\tabcolsep{3pt}{\n% \\resizebox{0.49\\textwidth}{!}{\n% \\begin{tabular}{@{}c|c@{}}\n% \\toprule\n%  &  \\textbf{Methods}   \\\\ \n% \\midrule\n% \\multirow{3}{*}{GA} & \\textbf{Grover}~\\cite{rong2020self}; \\textbf{Graph-Bert}~\\cite{zhang2020graph}; \\\\\n%  & \\textbf{Mesh Graphormer}~\\cite{lin2021mesh}; \\textbf{GraphiT}~\\cite{mialon2021graphit}; \\\\\n%  & \\textbf{Graphormer}~\\cite{ying2021transformers} \\\\\n%  \\midrule\n% \\multirow{5}{*}{PE} & \\cite{shiv2019novel}; \\textbf{GTOS}~\\cite{cai2020graph};  \\\\\n% & \\cite{wang2019self}; \\textbf{Graphormer}~\\cite{ying2021transformers};  \\\\\n% & \\textbf{Graph-Bert}~\\cite{zhang2020graph}; \\textbf{Mesh Graphormer}~\\cite{lin2021mesh}; \\\\\n% & \\textbf{Graph Trans}~\\cite{dwivedi2020generalization}; \\textbf{EGT}~\\cite{hussain2021edge}; \\\\\n% & \\textbf{SAN}~\\cite{kreuzer2021rethinking}; \\textbf{GraphiT}~\\cite{mialon2021graphit} \\\\\n% \\midrule\n% \\multirow{8}{*}{AT} & \\textbf{Mask-transformer} \\cite{min2022masked}; \\textbf{U2GNN}~\\cite{Nguyen2019UGT}; \\\\\n% & \\textbf{HeGT}~\\cite{yao2020heterogeneous}; \\textbf{Graformer}~\\cite{schmitt2020modeling}; \\\\\n% & \\cite{zhu2019modeling}; \\textbf{PLAN}~\\cite{khoo2020interpretable}; \\textbf{UniMP}~\\cite{shi2020masked}; \\\\\n% & \\textbf{GTOS}~\\cite{cai2020graph}; \\textbf{Graph Trans}~\\cite{dwivedi2020generalization}; \\\\\n% & \\textbf{SE(3)-Transformer} \\cite{fuchs2020se}; \\textbf{Gophormer}~\\cite{zhao2021gophormer}; \\\\\n% & \\textbf{EGT}~\\cite{hussain2021edge}; \\textbf{SAN}~\\cite{kreuzer2021rethinking}; \\\\\n% & \\textbf{GraphiT}~\\cite{mialon2021graphit}; \\textbf{Graphormer}~\\cite{ying2021transformers}; \\\\\n% & \\cite{wang2019self}; \\textbf{TorchMD-NET} \\cite{tholkeequivariant} \\\\\n% \\bottomrule\n% \\end{tabular}\n% }\n% }\n% \\end{table}\n\n\n%\\section{Transformer on Graphs: Overview}\n"
            },
            "section 3": {
                "name": "Transformer Architecture for Graphs",
                "content": "\\label{sec.transformer_on_graphs}\nThe self-attention mechanism in the standard Transformer actually considers the input tokens as a fully-connected graph, which is agnostic to the intrinsic graph structure among the data. Existing methods that enable Transformer to be aware of topological structures are generally categorized into three groups: 1) GNNs as auxiliary modules in Transformer~(\\textbf{GA}), 2) Improved positional embedding from graphs~(\\textbf{PE}), 3) Improved attention matrices from graphs~(\\textbf{AT}).  We summarize relevant literature in terms of these three dimensions in \\cref{tab_summary}.   \n\n\n\n\n\n\n%\\section{Incorporate graph prior via extra Graph Neural Networks}\n",
                "subsection 3.1": {
                    "name": "GNNs as Auxiliary Modules in Transformer",
                    "content": "\n\\label{sec.gnn_as_embedding}\nThe most direct solution of involving structural knowledge to benefit from global relation modeling of self-attention is to combine graph neural networks with Transformer architecture.\n%Instead of modifying the architecture of Transformer to involve structural knowledge, another direct solution is to combine graph neural networks with Transformer architecture. \nGenerally, according to the relative postion between GNN layers and Transformer layers, existing Transformer architectures with GNNs are categorized into three types as illustrated in Figure \\ref{gnn_emb}: (1) building Transformer blocks on top of GNN blocks, (2) alternately stacking GNN blocks and Transformer blocks, (3) parallelizing GNN blocks and Transformer blocks.\n\nThe first architecture is most-frequently adopted among the three options. For example,\nGraphTrans \\cite{jain2021representing} adds a Transformer subnetwork on top of a standard GNN layer. The GNN layer performs as a specialized architecture to learn local representations of the structure of a node's immediate neighbourhood, while the Transformer subnetwork computes all pairwise node interactions in a position-agnostic fashion, empowering the model global reasoning capability. GraphTrans is evaluated on graph classification task from biology, computer programming and chemistry, and achieves consistent improvement over benchmarks.\n\nGrover \\cite{rong2020self} consists of two GTransformer modules to represent node-level features and edge-level features respectively. In each GTransformer, the inputs are first fed into a tailored GNNs named dyMPN to extract vectors as queries, keys and values from nodes of the graph, followed by standard multi-head attention blocks. This bi-level information extraction framework enables the model to capture the structural information in molecular data and make it possible to extract global relations between nodes, enhancing the representational power of Grover.\n\nGraphiT \\cite{mialon2021graphit} also falls in the first architecture, which\nadopts one Graph Convolutional Kernel Network (GCKN) \\cite{chen2020convolutional} layer to produce a structure-aware representation from original features, and concatenate them as the input of Transformer architecture.\nHere, GCKNs is a multi-layer model that produces a sequence of graph feature maps similar to a GNN. Different from GNNs, each layer of GCKNs enumerates local sub-structures at each node, encodes them using a kernel embedding, and aggregates the sub-structure representations as outputs. These representations in a feature map carry more structural information than traditional GNNs based on neighborhood aggregation.\n\n\nMesh Graphormer \\cite{lin2021mesh} follows the second architecture by stacking a Graph Residual Block (GRB) on a multi-head self-attention layer as a Transformer block to model both local and global interactions among 3D mesh vertices and body joints. Specifically, given the contextualized features $\\BM$ generated by multi-head self-attentions (MHSA) in Equation \\ref{eq:attention3}, Mesh Graphormer improves the local interactions using a graph convolution in each Transformer block as:\n\\begin{equation}\n\\BM^{\\prime}=\\text{GraphConv}(\\BA^G, \\BM ; \\BW^{G})=\\sigma(\\BA^G \\BX \\BW^{G}).\n\\end{equation}\nwhere $\\BA^G \\in \\mathbb{R}^{n \\times n}$ denotes the adjacency matrix of a graph and $\\mathbf{W}_{G}$ denotes the trainable parameters. $\\sigma(\\cdot)$ implies the non-linear activation function.\nMesh Graphormer also implements another two variants: building GRB before MHSA and building those two blocks in parallel, but they perform worse than the proposed one.\n\nGraph-BERT \\cite{zhang2020graph} adopts the third architecture by utilizing a graph residual term in each attention layer as follows:\n\\begin{equation}\n    \\BM'=\\BM+\\text{G-Res}(\\BX,\\BX_r,\\BA^G),\n\\end{equation}\nwhere the notation $\\text { G-Res }(\\BX, \\BX_r,\\BA^G)$ represents the graph residual term introduced in \\cite{zhang2019gresnet} and $\\BX_r$ is the raw features of all nodes in the graph.\n%and they obtain best performance with graph-raw residual in their experiments, which can be denoted by $\\text { G-Res }(\\mathbf{H}^{(l-1)}, \\mathbf{X}_{i})=\\mathbf{A}\\mathbf{X}$\n\n\n\n\n\n\n\n%\\section{Incorporate graph prior via node positional embedding}\n"
                },
                "subsection 3.2": {
                    "name": "Improved Positional Embeddings from Graphs",
                    "content": "\n\\label{pe_tf}\nAlthough combining graph neural networks and Transformer has shown effectiveness in modeling graph-structured data, the best architecture to incorporate them remains an issue and requires heavy hype-parameter searching.\nTherefore, it is meaningful to explore a graph-encoding strategy without adjustment of the Transformer architecture.\nSimilar to the positional encoding in Transformer for sequential data such as sentences, it is also possible to compress the graph structure into positional embedding (PE) vectors and add them to the input before it is fed to the actual Transformer model:\n\\begin{equation}\n\\widetilde{\\BX} = \\BX + f_\\text{map}(\\BP),\n\\end{equation}\nwhere $\\BX \\in \\mathrm{R}^{n \\times d}$ is the matrix of input embeddings, $\\BP \\in \\mathrm{R}^{n \\times d_p}$ represents the graph embedding vectors, and $f_\\text{map}: \\mathrm{R}^{d_p} \\rightarrow \\mathrm{R}^{d}$ is a transformation network to align the dimension of both vectors.\nThe graph positional embedding $\\BP$ is normally generated from the adjacent matrix $\\BA^G \\in \\mathrm{R}^{n\\times n}$.\n\n\\cite{dwivedi2020generalization} adopt Laplacian eigenvectors as $\\BP$ in Graph Transformer. For each graph in the dataset, they pre-compute the Laplacian eigenvectors , which are defined by the factorization of the graph Laplacian matrix:\n\\begin{equation}\n    \\BU^{T} \\Lambda \\BU=\\BI-\\BD^{-1 / 2} \\BA^G \\BD^{-1 / 2},\n\\end{equation}\nwhere $\\BD$ is the degree matrix, and $\\Lambda, \\BU$ correspond to the eigenvalues and eigenvectors respectively. They use eigenvectors of the $k$ smallest non-trivial eigenvalues as the positional embedding, with $\\BP \\in \\mathbb{R}^{n \\times k}$ in this case. Since these eigenvectors have multiplicity occurring due to the arbitrary sign of eigenvectors, they randomly flip the sign of the eigenvectors during training.\n\n\n\\cite{hussain2021edge} employs pre-computed SVD vectors of the adjacent matrix as the positional embeddings $\\BP$. They use the largest $r$ singular values and corresponding left and right singular vectors to represent the positional encoding. \n\n\\begin{align}\n&\\BA^G \\stackrel{\\mathrm{SVD}}{\\approx} \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{T}=(\\mathbf{U} \\sqrt{\\boldsymbol{\\Sigma}}) \\cdot(\\mathbf{V} \\sqrt{\\boldsymbol{\\Sigma}})^{T}=\\hat{\\mathbf{U}} \\hat{\\mathbf{V}}^{T}, \\\\\n&\\BP=\\hat{\\mathbf{U}} \\| \\hat{\\mathbf{V}},\n\\end{align}\nwhere $\\BU,\\BV \\in \\mathrm{R}^{n\\times r}$ contain the $r$ left and right singular vectors corresponding to the top $r$ singular values in the diagonal matrix $\\Sigma \\in \\mathbb{R}^{r\\times r}$, $\\|$ denotes concatenation operator along columns. They also randomly flip the signs during training as a form of data augmentation to avoid over-fitting, since similar to Laplacian eigenvectors, the signs of corresponding pairs of left and right singular vectors can be arbitrarily flipped. \n\nDifferent from Eigen PE and SVD PE, which attempt to compress the adjacent matrix into dense positional embeddings, there exist some heuristic methods that encode specific structural information from the extraction of the adjacent matrix.\nFor example, \nGraphormer \\cite{ying2021transformers} uses the degree centrality as an additional signal to the neural network. To be specific, Graphformer assigns each node two real-valued embedding vectors according to its indegree and outdegree. For the $i$-th node, the degree-aware representation is denoted as:\n\\begin{equation}\n    \\tilde{\\x} = \\x + \\z_{\\mathrm{deg}^{-}(v_{i})}^{-}+\\z_{\\mathrm{deg}^{+}(v_{i})}^{+},\n\\end{equation}\nwhere $\\z^{-}, \\z^{+} \\in \\mathbb{R}^{d}$ are learnable embedding vectors specified by the indegree $\\operatorname{deg}^{-}(v_{i})$ and outdegree $\\operatorname{deg}^{+}(v_{i})$ respectively. For undirected graphs, $\\operatorname{deg}^{-}(v_{i})$ and $\\operatorname{deg}^{+}(v_{i})$ could be unified to be $\\operatorname{deg}(v_{i})$. By using the centrality encoding in the input, the softmax attention will catch the node importance signal in queries and keys of the Transformer.\n\nGraph-BERT \\cite{zhang2020graph} introduces three types of PE to embed the node position information, \\emph{i.e.}, an absolute WL-PE which represents different codes labeled by the Weisfeiler-Lehman algorithm, an intimacy based PE and a hop based PE which are both variant to the sampled subgraphs.  \n\\cite{cai2020graph} applies graph transformer to tree-structured abstract meaning representation (AMR) graph. It adopts a distance embedding for each node by encoding the minimum distance from the root node as a flag of the importance of the corresponding concept in the whole-sentence semantics. \\cite{kreuzer2021rethinking} proposes a learned positional encoding that can utilize the full Laplacian spectrum to learn the position of each node in a given graph.\n\n\n\n%\\section{Incorporate graph prior via attention matrix}\n"
                },
                "subsection 3.3": {
                    "name": "Improved Attention Matrices from Graphs",
                    "content": "\n\\label{at_tf}\nAlthough node positional embedding is a convenient practice to inject graph priors into Transformer architectures, the progress of compressing graph structure into fixed-sized vectors suffers from information loss, which might limit their effectiveness. For this sake, another group of works attempts to improve the attention matrix computation based on graph information:\n\\vspace{-1ex}\n\\begin{equation}\n\\vspace{-1ex}\n    \\begin{split}\n        \\BA =& f_\\text{G\\_att}(\\BX,\\BA^G,\\BE;\\BQ,\\BK,\\BW_1),\\\\\n        \\BM =& f_\\text{M}(\\BX,\\BA,\\BA^G,\\BE;\\BV,\\BW_2),\n    \\end{split}\n\\end{equation}\nwhere $\\BX$ is the input features, $\\BA^G$ is the adjacent matrix of the graph, $\\BE \\in \\mathbb{R}^{n\\times n\\times d_e}$ is the edge features if available, $\\BQ,\\BK,\\BV$ are the attention parameters, $\\BW_1,\\BW_2$ are extra graph encoding parameters. \n\nOne line of models adapts self-attention mechanism to GNN-like architectures by restricting a node only attending to local node neighbours in the graph, which can be computationally formulated as an attention masking mechanism:\n\\vspace{-1ex}\n\\begin{equation}\n\\vspace{-1ex}\n\\label{maskattention}\n    \\BA = (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}) \\odot \\BA^{G},\n\\end{equation}\nwhere $\\BA^G \\in \\mathbb{R}^{n \\times n}$ is the adjacent matrix of the graph. In \\cite{dwivedi2020generalization}, $\\BA^G_{ij}=1$ if there is an edge between $i$-th node and $j$-th node. Given the simple and generic nature of this architecture, they obtain competitive performance against standard GNNs on graph datasets. In order to encode edge features, they also extend Equation \\ref{maskattention} as:\n\\vspace{-1ex}\n\\begin{equation}\n      \\BA = (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}) \\odot \\BA^{G} \\odot \\BE'\\BW_E,  \n\\end{equation}\nwhere $\\BW_E \\in \\mathbb{R}^{d_e\\times d}$ is the parameter matrix, $\\BE'$ is the edge embedding matrix from the previous layer which is updated based on $\\BA$, we do not elaborate these details for simplicity.\n\nOne possible extension of this practice is masking the attention matrices of different heads with different graph priors. In the original multi-head self-attention blocks, different attention heads implicitly attend to information from different representation subspaces of different nodes. While in this case, using the graph-masking mechanism to enforce the heads explicitly attend to different subspaces with graph priors further improves the model representative capability for graph data.\nFor example,\n\\cite{yao2020heterogeneous} computes the attention based on the extended Levi graph. Since Levi graph is a heterogeneous graph that contains different types of edges. They \ufb01rst group all edge types into a single one to get a homogeneous subgraph referred to as connected subgraph. The connected subgraph is actually an undirected graph that contains the complete connected information in the original graph. Then they split the input graph into multiple subgraphs according to the edge types. Besides learning the directly connected relations, they introduce a fully-connected subgraph to learn the implicit relationships between indirectly connected nodes. Multiple adjacent matrices are assigned to different attention heads to learn a better representation for AMR task.\n\\cite{min2022masked} adopts a similar practice, which carefully designs four types of interaction graphs for modeling neighbourhood relations in CTR prediction task: induced subgraph, similarity graph, cross-neighbourhood graph, and complete graph. And they use the masking mechanism to encode these graph priors to improve neighbourhood representation.\n\nGraphiT \\cite{mialon2021graphit} extends the adjacent matrix to a kernel matrix, which is more flexible to encode various graph kernels. Besides, they use the same matrix for keys and queries following the recommendation of \\cite{tsai2019transformer} to reduce parameters without hurting the performance in practice, and adopt a degree matrix to reduce the overwhelming influence of highly connected graph components.\nThe update equation can be formulated as:\n\\begin{equation}\n\\aligned\n        \\BA &= (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{Q})^{\\top}) \\odot \\BK_r,\\\\\n        \\widetilde{\\BX} &= \\text{SoftMax}(\\BA)(\\BX\\BV),\\\\\n        \\BM &= \\text{LayerNorm}(\\BD^{-\\frac{1}{2}}\\widetilde{\\BX}+\\BX),\n\\endaligned\n\\end{equation}\nwhere $\\BD \\in \\mathbb{R}^{n\\times n}$ is the diagonal matrix of node degrees, $\\BK_r \\in \\mathbb{R}^{n\\times n}$ is the kernel matrix on the graph, which is used as diffusion kernel and p-step random walk kernel.\n% in their paper.\n\nAnother line of models attempts to add soft graph bias to attention scores.\nGraphormer \\cite{ying2021transformers} proposes a novel Spatial Encoding mechanism. Concretely, they consider a distance function $\\phi(v_i,v_j)$, which measures the spatial relation between nodes $v_i$ and $v_j$ in the graph. They select $\\phi(v_i,v_j)$ as the shortest path distance (SPD) between $v_i$ and $v_j$. If they are not connected, the output of $\\phi$ is set as a special value, \\emph{i.e.}, $-1$. They assign each feasible value of $\\phi$ a learnable scale parameter as a graph bias term. So the update rule is:\n\\begin{equation}\n    \\BA = (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}) + \\BB^{s}.\n\\end{equation}\n$\\BB^{s}$ is the bias matrix, where $\\BB^{s}_{ij}=b_{\\phi(v_i,v_k)}$ is the learnable scalar indexed by $\\phi(v_i,v_k)$, and shared across all layers. \nIn order to handle graph structure with edge features, they also design an edge feature bias term. Specifically, for each ordered node pair $(v_i,v_j)$, they search (one of) the shortest path $\\text{SP}_{ij}=(e_1,e_2,\\ldots,e_N)$ from $v_i$ to $v_j$, and then compute an average of dot-products of the edge features and a learnable embedding along the path. Combined with the above spatial bias, the unnormalized attention score can be modified as:\n\\begin{equation}\n    \\BA = (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}) + \\BB^{s}+\\BB^{c}\n\\end{equation}\nwhere $\\BB^{c}$ is the edge feature bias matrix. $\\BB^{c}_{ij}=\\frac{1}{N}\\sum^N_{n=1}x_{e_n}(w_n^E)^\\top$, where $x_{e_{n}}$ is the feature of the $n$-th edge $e_{n}$ in $\\mathrm{SP}_{i j}, w_{n}^{E} \\in \\mathbb{R}^{d_{e}}$ is the $n$-th weight embedding, and $d_{e}$ is the dimensionality of edge feature.\n\n\nGophormer \\cite{zhao2021gophormer} proposes proximity-enhanced multi-head attention (PE-MHA) to encode multi-hop graph information. Specifically, for a node pair $\\langle v_{i}, v_{j}\\rangle$, $M$ views of structural information is encoded as a proximity encoding vector, denoted as  $\\phi_{i j} \\in \\mathbb{R}^{M}$, to enhance the attention mechanism. The proximity-enhanced attention score $\\BA_{i j}$ is defined as:\n\\vspace{-1.5ex}\n\\begin{equation}\n\\vspace{-1.5ex}\n  \\BA_{ij} =    (\\frac{1}{\\sqrt{d}} \\x_i\\BQ(\\x_j\\BK)^\\top)+\\boldsymbol{\\phi}^{ij} \\b^\\top,\n\\end{equation}\nwhere $\\b \\in \\mathbb{R}^{M}$ is the learnable parameters that compute the bias of structural information.  The proximity encoding is calculated by $M$ structural encoding functions defined as:\n\\begin{equation}\n\\boldsymbol{\\phi}_{i j}=\\operatorname{Concat}(\\Phi_{m}(v_{i}, v_{j}) \\mid m \\in 0,1, \\ldots, M-1),\n\\end{equation}\nwhere each structural encoding function $\\Phi_{m}(\\cdot)$\n% :\\rightarrow \\mathbb{R}$ \nencodes a view of structural information. %In their paper, they consider two aspects of structural information for each node pair: (1) whether the node pairs are connected at specific order, (2) whether global node exists in this node pair. The proximity encoding functions are defined as:\n%\\begin{equation}\n%\\Phi_{m}(v_{i}, v_{j})= \n%\\begin{cases}\n%\\tilde{\\mathrm{A}}^{m}[i, j], & \\text { if } m<M-1 \\\\\n%\\mathbb{I}(i, j), & \\text { if } m=M-1,\n%\\end{cases}\n%\\end{equation}\n%where $\\mathbb{I}(i, j)=1$ if global nodes exists %in $\\langle v_{i}, v_{j}\\rangle$, otherwise 0.\n\nPLAN \\cite{khoo2020interpretable} also proposes a structure aware self-attention to model the tree structure of rumour propagation in social media. The modified attention calculation can be defined as:\n\\vspace{-1.5ex}\n\\begin{align}\n\\vspace{-2.5ex}\n  \\BA_{ij} &=    \\frac{1}{\\sqrt{d}}( \\x_i \\BQ(\\x_j\\BK)^\\top)+a_{ij}^K\\\\\n  \\BM_i &= \\sum^n_{j=1}\\text{SoftMax}(\\BA_{ij})(\\x_j\\BV+a_{ij}^V)\n\\end{align}\n%\\begin{align}\n%\\alpha_{i j}=\\operatorname{softmax}(\\frac{q_{i} k_{j}^{T}+a_{i j}^{K}}{\\sqrt{d_{k}}}) \\\\\n%z_{i}=\\sum_{j=1}^{n} \\alpha_{i j}(v_{j}+a_{i j}^{V})\n%\\end{align}\nBoth $a_{ij}^{V}$ and $a_{ij}^{K}$ are learnable parameter vectors that represent one of the \ufb01ve possible structural relationships between the pair of the tweets (i.e. parent, child, before, after and self).\n\n\n%\\cite{schmitt2020modeling} define self-attention as follows:\n%$$\n%\\boldsymbol{\\alpha}_{i}^{g}=\\sigma(\\frac{\\boldsymbol{H}_{i} \\boldsymbol{W}^{Q_{g}}(\\boldsymbol{H} \\boldsymbol{W}^{K_{g}})^{\\top}}{\\sqrt{d}}+\\gamma(\\boldsymbol{R})_{i})\n%$$\n%$\\boldsymbol{W}^{K_{g}}, \\boldsymbol{W}^{Q_{g}} \\in \\mathbb{R}^{d \\times d}$ are learned matrices and $\\gamma:$ $\\mathbb{Z} \\cup\\{\\infty\\} arrow \\mathbb{R}$ looks up learned scalar embeddings for the relative graph positions in $\\boldsymbol{R} \\in \\mathbb{R}^{N \\times N}$.\n%We define the relative graph position $R_{i j}$ between the nodes $n_{i}$ and $n_{j}$ with respect to two factors: (i) the text relative position $p$ in the original entity name if $n_{i}$ and $n_{j}$ stem from the same original entity, i.e., $(n_{i}, n_{j}) \\in$ SAME $_{p}$ for some $p$ and (ii) shortest path lengths otherwise.\n\n\n \n\n\n\n\\/*\n\n\n\n\n*/\n\n\n\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experimental Evaluations",
                "content": "\\label{sec.exp}\nWe conduct an extensive evaluation to study the effectiveness of different methods. On the basis of standard Transformer, methods in the three groups, auxiliary GNN modules (GA), positional embeddings (PE), and improved attention (AT) are compared.\nSince most methods are composed of more than one graph-specific module and are trained with various tricks, it is difficult to evaluate the effectiveness of each module in a fair and independent way. \nIn our experiments, we extract the representative modules of existing models and evaluate their performance individually. \n%In this section, we conduct an extensive evaluation on the effects of different methods. Since most models are composed of over one graph-specific modules and might adopt various training tricks, it is hard to equally evaluate the contribution of each module. Therefore, in our experiment, we extract some representative modules of existing models and evaluate their performance individually.\n%Specifically, we first implement the standTransformer architecture, based on which we compare the three groups of methods: Auxiliary GNN Modules (GA), positional embeddings (PE) and improved attention (AT).\nFor GA methods, we compare the three architectures described in Section \\ref{sec.gnn_as_embedding}. In both \\textit{alternately} and \\textit{parallel} settings, the GNNs and Transformer layers are combined before the FFN layers.\nPE methods include degree embedding \\cite{ying2021transformers}, Laplacian Eigenvectors \\cite{dwivedi2020generalization} and SVD vectors \\cite{hussain2021edge}. \nAT methods contain spatial bias (SPB) \\cite{ying2021transformers}, proximity-enhanced multi-head attention (PMA) \\cite{zhao2021gophormer}, attention masked with 1-hop adjacent matrix (Mask-1) \\cite{dwivedi2020generalization}.\nWe also mask attention with different hops of the adjacent matrix, denoted as (Mask-n), inspired by the multi-head masking mechanisms in \\cite{yao2020heterogeneous,min2022masked}.\n We evaluate the methods on both graph-level tasks on small graphs and node-level tasks on a single large graph. \n %For graph-level tasks, the datasets include ZINC(subset), ogbg-molhiv and ogbg-molpcba. For node-level tasks, the datasets include Flickr, ogbn-product, ogbn-arxiv. \n The details of the tasks and datasets are listed in Table \\ref{datasets}.\n \n \n\n \n \n \n ",
                "subsection 4.1": {
                    "name": "Settings and Implementation Details",
                    "content": "\n%To get rid of the heavy hype-parameter search of Transformer architecture for each method, we fix the model scale in three levels: small, middle and large, the corresponding hype-parameters are listed in Table \\ref{hype-parameters}. \nTo ensure a consistent and fair evaluation, we fix the scale of the Transformer architecture in three levels: small, middle, and large, \n%For the architecture of Transformer, we test three models of different sizes, small, middle and large, \nwhose configurations are listed in  Table \\ref{hype-parameters}.\nThe remaining hyper-parameters are fixed to their empirical value as shown in Table~\\ref{tab:exp:train:hyperpara}.\n%We fix other hyper-parameters to be the same: the attention dropout rate is set as 0.1, the FFN dropout rate is set as 0.1, the maximum steps for training is set as $400K$, the warm-up steps is set as $40K$ the peak learning rate is set as $2e\\text{-}4$, the batch size is set as 256, the learning rate decay is set as Linear decay, the Adam $\\epsilon$ is set as $1e\\text{-}8$, the Adam $\\beta_1,\\beta_2$ are set as 0.9 and 0.999, the gradient clip norm is set as 5.0, the weight decay value is set as $0.001$.\nFor node-level tasks in large-scale graphs, we adopt the shadow $k$-hop sampling \\cite{zeng2020deep} to generate a subgraph for a target node, to which the graph-aware modules are applied.  We calculate the required inputs of all modules based on the subgraph instead of the original graph because most of them are not computationally scalable to large graphs.\nWe set the maximum sampling hop to be $2$ and the maximum neighbourhood per node to be $10$. \n%The graph-aware modules are applied on the sampled subgraph.\nFor PE methods, we select the embedding size from $\\{3,4,5\\}$. For GA methods, we select the GNN type from GCN \\cite{kipf2016semi}, GAT \\cite{velivckovic2017graph} and GIN \\cite{xu2018powerful}.\nOur learning framework is built on PyTorch 1.8, and PyTorch Geometric 2.0, and the code is public at \\textcolor{blue}{\\href{https://github.com/qwerfdsaplking/Graph-Trans}{https://github.com/qwerfdsaplking/Graph-Trans}}.\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Experimental Results",
                    "content": "\n%Table \\ref{tab:all_res} reports the performance of 3 Transformers combined with the 4 methods, on 6 datasets. \nTable \\ref{tab:all_res} reports the performance of ten graph-specific modules from three categories on six datasets of graph-level and node-level tasks, respectively. \n%\n%3 Transformers combined with the 4 methods, on 6 datasets. \n%The performance of all methods are listed in Table \\ref{graph-level} and \\ref{node-level}. \n%We have the following observations:\nWe summarize the main observations as follows:\n%\\begin{itemize}[leftmargin=*]\n%    \\item In most cases, Transformers with graph-specific modules can improve their performance on graph-structured data. The improvement on small-graph tasks is more significant than that on large-graph tasks which require graph sampling. \n%    The reason could be that the sampled induced subgraphs fail to keep the original graph intact, incurring variance and information loss in the learning process.\n    %This could be ascribed to the variance and information loss of sampling strategy. Since the induced subgraph on which the models are based does not contain intact graph information, which incur a sub-optimal performance.\n%    \\item Generally, GA and AT methods bring more benefits than PE methods. \n%    We analyze that the weaknesses of PE are in two facets. First, as introduced in Section \\ref{pe_tf}, PE does not contain intact graph information. Second, since PE is only fed into the input layer of the network, the graph-structural information would decay layer by layer across the model, leading to a degeneration of the performance.\n    %A possible reason is that positional embeddings do not contain intact graph information, as described in Section \\ref{pe_tf}. \n    %Another possibility is that the positional embeddings are fed into the network only at the input layer, which means that the graph-structural information would decay after multiple layers of network, and thus impair the performance.\n%    \\item Model performance is sensitive to model sizes. As Table~\\ref{tab:all_res} shows, the best performed module of each dataset varies when the model size changes, apart from ogbg-molhiv. With some model sizes, adding a graph-specific module on the vanilla Transformer would degrade the performance. For example, on ogbg-molhiv dataset, the GA-parallel module \n%    boosts the performance with middle and large architectures but decline the performance remarkably with small architecture,\n%    which implies the importance of hyper-parameter searching.\n%\\end{itemize}\n\\begin{itemize}[leftmargin=*]\n\\item Not surprisingly, in most cases, the evaluated graph-specific modules of Transformer lead to to better performance. For example, on molpcba, we observe at most a 56\\% performance improvement compared with the vanilla Transformer.  This observation confirms the effectiveness of graph-specific modules on the various kinds of graph tasks. \n\\item The improvement on graph-level tasks is more significant than that on node-level tasks. This may be due to the graph sampling process when dealing with a single large graph. The sampled induced subgraphs fail to keep the original graph intact, incurring variance and information loss in the learning process.\n\\item GA and AT methods bring more benefits than PE methods. \n% We conclude that the weaknesses of PE are in two facets. \nIn conclusion, PE's weaknesses are twofold.\nFirst, as introduced in Section \\ref{pe_tf}, PE does not contain intact graph information. Second, since PE is only fed into the input layer of the network, the graph-structural information would decay layer by layer across the model, leading to a degeneration of the performance.\n\\item It seems that different kinds of graph tasks enjoy different group of models. GA methods achieve the best performance in more than half of the cases (5 out of 9 cases) of node-level tasks. More significantly,  AT methods achieve the best performance in almost all the cases (8 out of 9 cases) of graph-level tasks. We conjecture that GA methods are able to better encode the local information of the sampled induced subgraphs in node-level tasks, while AT methods are suitable for modeling the global information of the single graphs in graph-level tasks. \n %almost all best performance\n%GA methods obtain a better performance on node-level tasks, while AT methods is better on graph-level tasks. \n\\end{itemize}\nIn summary, our experiments confirm the value of the existing graph-specific module on Transformer and reveal their advantages for a variety of graph tasks. \n\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion and Future Directions",
                "content": "\\label{sec.conclusion}\nIn this survey, we present a comprehensive review of the current progress of applying Transformer model on graphs. \nBased on the injection part of graph data and graph model in Transformer, we classify the existing graph-specific modules into three typical groups: GNNs as auxiliary modules, improved positional embeddings from graphs, and improved attention matrices from graphs. \nTo investigate the real performance gains under a fair \nsetup, we implement the representative modules from three groups and compare them on six benchmarks with different tasks. \nWe hope our systematic review and comparison will foster understanding and stimulate new ideas in this area.\n\n% Despite the current successes, several directions that we believe still need to be fully investigated and would be promising starting points for future work include:\nIn spite of the current successes, we believe several directions would be promising to investigate further and to start from in the future, including:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{New paradigm of incorporating the graph and the Transformer.} \n    % Most of studies treat the graphs as a strong prior of modification of existing Transformer model. It is interesting to develop the new paradigm which is not just consider the graph as a prior and can better reflect the graph property. \n    Most studies treat the graphs as strong prior to modifying the Transformer model. There is a great interest to develop the new paradigm that not just takes graphs as a prior, but also better reflects the properties of graphs. \n    \\item \\textbf{Extending to other kinds of graphs.} \n    Existing Graph Transformer models mostly focus on homogeneous graphs, which consider the node type and edge type as the same. It is also important to explore their potential on other forms of graphs, such as heterogeneous graphs and hypergraphs.\n    \\item \\textbf{Extending to large-scale graphs.} Most existing methods are designed for small graphs, which might be computationally infeasible for large graphs. \n    As illustrated in our experiments, directly applying them to the sampled subgraphs would impair performance. Therefore, designing salable Graph-Transformer architecture is essential.\n\\end{itemize}\n\n\n\n%This paper presents a comprehensive review of existing Transformer models in graphs from the perspective of architectural design. We dismantle the existing models and summarize existing graph-specific modules in three typical groups: GNN as Embeddings, improved positional embedding from graphs and improved attention matrix from graphs. In order to conduct an empirical analysis on these methods, we implement the representative components and compare them on six benchmarks to investigate their real performance gain. We hope our systematic review and comparison will foster understanding and stimulate news ideas in this area.\n%Several directions that we believe still need to be fully investigated and would be promising starting points for future work include:\n\n%\\begin{itemize}[leftmargin=*]\n%    \\item Is Transformer architecture suitable for special graphs? Existing Graph Transformers mostly focus on homogeneous graph, which consider the node type and edge type as same. It is also important to explore their potential on other forms of graphs, such as heterogeneous graphs or hypergraphs.\n%    \\item How to effectively extend Transformer to large-scale graph? Most existing methods are designed for small graphs and require inputs induced from the entire graph, which might be computationally infeasible when dealing with large graphs. Directly applying them to the sampled subgraphs would impair the performance as illustrated in our experiments. Therefore, designing salable Graph-Transformer architecture is essential.\n%\\end{itemize}\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n\n%\\FloatBarrier\n\\clearpage\n%% The file named.bst is a bibliography style file for BibTeX 0.99c\n\\bibliographystyle{named}\n\\bibliography{ijcai22}\n\n"
            }
        },
        "tables": {
            "tab_summary": "\\begin{table}[]\n\\centering\n\\caption{A summary of papers that applied Transformers on graph-structured data. \\textbf{GA}: GNNs as Auxiliary Modules; \\textbf{PE}: Improved Positional Embedding from Graphs; \\textbf{AT}: Improved Attention Matrix from Graphs. }\n\\label{tab_summary}\n\\small\n\\setlength\\tabcolsep{3pt}{\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{c|ccc|c}\n\\toprule\n\\textbf{Method}   &  \\textbf{GA} & \\textbf{PE} & \\textbf{AT}  & \\textbf{Code}   \\\\ \n\\midrule\n\\cite{zhu2019modeling} &  & &\\checkmark & \\textcolor{blue}{\\href{https://github.com/Amazing-J/ structural-transformer}{\\checkmark}}\\\\\\hline\n\\cite{shiv2019novel} &  &\\checkmark & &  \\\\\\hline\n\\cite{wang2019self} &  &\\checkmark &\\checkmark &\\\\\\hline\nU2GNN~\\cite{Nguyen2019UGT} &  & &\\checkmark &\\textcolor{blue}{\\href{https://github.com/daiquocnguyen/Graph-Transformer}{\\checkmark}}\\\\\\hline\nHeGT~\\cite{yao2020heterogeneous} &  & &\\checkmark  &\\textcolor{blue}{\\href{https://github.com/QAQ-v/HetGT}{\\checkmark}}\\\\\\hline\nGraformer~\\cite{schmitt2020modeling} &  & &\\checkmark &\\textcolor{blue}{\\href{https://github.com/mnschmit/graformer}{\\checkmark}}\\\\\\hline\nPLAN~\\cite{khoo2020interpretable} & & &\\checkmark &\\textcolor{blue}{\\href{https://github.com/serenaklm/rumor_detection}{\\checkmark}}\\\\\\hline\nUniMP~\\cite{shi2020masked} &  & &\\checkmark &\\\\\\hline\nGTOS~\\cite{cai2020graph} &  & \\checkmark&\\checkmark &\\textcolor{blue}{\\href{https://github.com/jcyk/gtos}{\\checkmark}}\\\\\\hline\nGraph Trans~\\cite{dwivedi2020generalization} &  &\\checkmark &\\checkmark &\\textcolor{blue}{\\href{https://github.com/graphdeeplearning/graphtransformer}{\\checkmark}}\\\\\\hline\nGrover~\\cite{rong2020self} &  \\checkmark & & &\\textcolor{blue}{\\href{https://github.com/tencent-ailab/grover}{\\checkmark}}\\\\\\hline\nGraph-BERT~\\cite{zhang2020graph} &  \\checkmark & \\checkmark & &\\textcolor{blue}{\\href{https://github.com/jwzhanggy/Graph-Bert}{\\checkmark}}\\\\\\hline\nSE(3)-Transformer \\cite{fuchs2020se}& & &\\checkmark&\\textcolor{blue}{\\href{https://github.com/FabianFuchsML/se3-transformer-public}{\\checkmark}}\\\\\\hline\nMesh Graphormer~\\cite{lin2021mesh} & \\checkmark & \\checkmark&  &\\textcolor{blue}{\\href{https://github. com/microsoft/MeshGraphormer}{\\checkmark}}\\\\\\hline\nGophormer~\\cite{zhao2021gophormer} &  & &\\checkmark  &\\\\\\hline\nEGT~\\cite{hussain2021edge} & &\\checkmark  &\\checkmark  &\\textcolor{blue}{\\href{https://github.com/shamim-hussain/egt}{\\checkmark}}\\\\\\hline\nSAN~\\cite{kreuzer2021rethinking} &  & \\checkmark& \\checkmark&\\textcolor{blue}{\\href{https://github.com/DevinKreuzer/SAN}{\\checkmark}}\\\\\\hline\nGraphiT~\\cite{mialon2021graphit} & \\checkmark &\\checkmark &\\checkmark &\\textcolor{blue}{\\href{https://github.com/inria-thoth/GraphiT}{\\checkmark}}\\\\\\hline\nGraphormer~\\cite{ying2021transformers} &  \\checkmark &\\checkmark & \\checkmark &\\textcolor{blue}{\\href{https://github.com/microsoft/Graphormer}{\\checkmark}}\\\\\\hline\nMask-transformer \\cite{min2022masked} &  & &\\checkmark  &\\textcolor{blue}{\\href{https://github.com/qwerfdsaplking/F2R-HMT}{\\checkmark}}\\\\\\hline\n TorchMD-NET \\cite{tholkeequivariant} &&&\\checkmark&\\textcolor{blue}{\\href{https://github.com/torchmd/torchmd-net}{\\checkmark}}\\\\\n\n\\bottomrule\n\\end{tabular}\n}\n}\n\\end{table}",
            "datasets": "\\begin{table}[h]\n\\centering\n\\caption{Details of the datasets.}\n\\vspace{-2ex}\n\\resizebox{0.45\\textwidth}{!}{\n\\scriptsize\n\\begin{tabular}{@{\\hskip 0.03in}c@{\\hskip 0.03in}|@{\\hskip 0.03in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c}\n\\toprule\nDataset Type             & Name         & \\#Graph & \\#Nodes(Avg.) & \\#Edges(Avg.)  & Task type                  & Metric   \\\\ \\midrule\n\\multirow{3}{*}{Graph-level} & ZINC         & 12,000  & 23.2    & 49.8     & Regression                 & MAE      \\\\ %\\cline{2-7} \n                      & ogbg-molhiv  & 41,127  & 25.5    & 27.5     & Binary cla.      & ROC-AUC  \\\\ %\\cline{2-7} \n                       & ogbg-molpcba & 437,929 & 26.0    & 28.1     & Binary cla.      & AP       \\\\ \\midrule\n\\multirow{3}{*}{Node-level}  & Flickr       & 1       & 89,250   & 899,756   & Multi-class cla. & Accuracy \\\\ %\\cline{2-7} \n                       & ogbg-arxiv   & 1       & 169,343  & 1,166,243  & Multi-class cla. & Accuracy \\\\ %\\cline{2-7} \n                       & ogbg-product & 1       & 2,449,029 & 61,859,140 & Multi-class cla. & Accuracy \\\\ \\bottomrule\n\\end{tabular}\n}\n\\vspace{-2ex}\n \\label{datasets}\n\\end{table}",
            "hype-parameters": "\\begin{table}[h]\n\\caption{Hyper-parameters of various Transformer sizes.}\n\\vspace{-2ex}\n\\centering\n\\setlength\\tabcolsep{4pt}{\n\\resizebox{0.8\\linewidth}{!}{\n\\begin{tabular}{llll}\n\\toprule\n\\multicolumn{1}{l|}{}                              & \\multicolumn{3}{|c}{Transformer Size} \\\\\n\\multicolumn{1}{l|}{}                              & Small & Middle & Large \\\\ %\\hline\n \\midrule\n\\multicolumn{1}{l|}{\\#Layers}                      & 6     & 12     & 12    \\\\ %\\hline\n\\multicolumn{1}{l|}{Hidden Dimension}              & 80    & 80     & 512   \\\\ %\\hline\n\\multicolumn{1}{l|}{FFN Inner Hidden Dimension}    & 80    & 80     & 512   \\\\ %\\hline\n\\multicolumn{1}{l|}{\\#Attention Heads}             & 8     & 8      & 32    \\\\ %\\hline\n\\multicolumn{1}{l|}{Hidden Dimension of Each Head} & 10    & 10     & 16    \\\\ %\\hline\n                                                %   &       &        &       \\\\\n                                                %   &       &        &       \\\\\n                                                %   &       &        &       \\\\\n\\bottomrule  \n\\end{tabular}\n}\n}\n\\vspace{-2ex}\n\\label{hype-parameters}\n\\end{table}",
            "graph-level": "\\begin{table*}[h]\n\\footnotesize\n\\centering\n\\caption{Performance comparison on graph-level tasks. For each column, the numbers with background color are the best results in each group. The numbers in bold  are the best results across the groups.}\n\\begin{tabular}{cc|ccc|ccc|ccc}\n\\toprule\n   &        & \\multicolumn{3}{c|}{ZINC(MAE$\\downarrow$)} & \\multicolumn{3}{c|}{ogbg-molhiv(ROC-AUC$\\uparrow$)} & \\multicolumn{3}{c}{ogbg-molpcba(AP$\\uparrow$)} \\\\\n   &        &     Small & Middle &  Large &                Small & Middle &  Large &            Small & Middle &  Large \\\\\n\\midrule\nTF & vanilla &    0.6689 & 0.6700 & 0.6699 &               0.7466 & 0.7230 & 0.7269 &           0.1624 & 0.1673 & 0.1676 \\\\\\midrule\n\\multirow{3}{*}{GA} & \\cellcolor{LightCyan} before &    0.4700 & 0.4809 & 0.5169 &              0.6758 & 0.7339 & 0.7182 &           0.2105 & 0.1989 &\\cellcolor{LightCyan} 0.2269 \\\\\n   &\\cellcolor{LightCyan}  alter &    \\cellcolor{LightCyan} 0.3771 & 0.3412 & \\cellcolor{LightCyan} 0.2956 &              \\cellcolor{LightCyan}  0.7200 &  0.7086 & \\cellcolor{LightCyan} 0.7433 &           \\cellcolor{LightCyan} 0.2474 & 0.2417 & 0.2244 \\\\\n   &  \\cellcolor{LightCyan}  parallel &    0.3803 & \\cellcolor{LightCyan} \\textbf{0.2749} & 0.2984 &               0.7138 &\\cellcolor{LightCyan} 0.7750 & 0.7603 &           0.2345 & \\cellcolor{LightCyan} 0.2444 & 0.2205 \\\\\\midrule\n%\\cline{1-11}\n\\multirow{3}{*}{PE} & \\cellcolor{LightYellow}  degree &    \\cellcolor{LightYellow} 0.5364 & \\cellcolor{LightYellow} 0.5364 & 0.5435 &               \\cellcolor{LightYellow} 0.7506 & 0.6818 & \\cellcolor{LightYellow} 0.7357 &           0.1672 & 0.1646 & 0.1650 \\\\\n   &\\cellcolor{LightYellow} eig &    0.6031 & 0.6074 & 0.6166 &               0.7407 & \\cellcolor{LightYellow} 0.7279 & 0.7351 &           \\cellcolor{LightYellow} 0.2194 &\\cellcolor{LightYellow}  0.2087 & \\cellcolor{LightYellow} 0.2131 \\\\\n   & \\cellcolor{LightYellow} svd &    0.5811 & 0.5462 & \\cellcolor{LightYellow} 0.5400 &                0.7350 & 0.7155 & 0.7275 &           0.1648 & 0.1663 & 0.1767 \\\\\\midrule\n%\\cline{1-11}\n\\multirow{4}{*}{AT} & \\cellcolor{LightRed} SPB &    0.5122 & 0.4878 & 0.6100 &               0.7065 & 0.7589 & 0.7255 &           0.2409 & 0.2524 &  \\cellcolor{LightRed} \\textbf{0.2621} \\\\\n   &\\cellcolor{LightRed} PMA &    0.6194 & 0.6057 & 0.5963 &               0.6902 & 0.7054 & 0.7314 &           0.2115 & 0.1956 & 0.2518 \\\\\n   &\\cellcolor{LightRed} Mask-1 &   \\cellcolor{LightRed} \\textbf{0.2861} &  \\cellcolor{LightRed} 0.2772 & \\cellcolor{LightRed} \\textbf{0.2894} &               \\cellcolor{LightRed} \\textbf{0.7610} &  \\cellcolor{LightRed} \\textbf{0.7753} &  \\cellcolor{LightRed} \\textbf{0.7960} &           0.2573 &  \\cellcolor{LightRed} \\textbf{0.2662} & 0.1594 \\\\\n   & \\cellcolor{LightRed} Mask-n &    0.3906 & 0.3596 & 0.4765 &               0.7286 & 0.7423 & 0.7128 &            \\cellcolor{LightRed} \\textbf{0.2619} & 0.2577 & 0.2380 \\\\\n\\bottomrule\n\\end{tabular}\n\\label{graph-level}\n\\end{table*}",
            "node-level": "\\begin{table*}[h]\n\\footnotesize\n\\centering\n\\caption{Performance comparison on node-level tasks. For each column, the numbers with background color are the best results in each group. The numbers in bold  are the best results across the groups.}\n\\begin{tabular}{cc|ccc|ccc|ccc}\n\\toprule\n   &        & \\multicolumn{3}{c|}{Flickr(Acc$\\uparrow$)} & \\multicolumn{3}{c|}{ogbn-arxiv(Acc$\\uparrow$)} & \\multicolumn{3}{c}{ogbn-products(Acc$\\uparrow$)} \\\\\n   &        &       Small & Middle &  Large &           Small & Middle &  Large &              Small & Middle &  Large \\\\\n\\midrule\nTF & vanilla &      0.5270 & 0.5279 & 0.5214 &          0.5539 & 0.5571 & 0.5598 &             0.7887 & 0.7887 & 0.7956 \\\\\n\\midrule\n\\multirow{3}{*}{GA} & \\cellcolor{LightCyan} before &      0.5352 & 0.5369 &\\cellcolor{LightCyan}0.5272 &          0.5608 & 0.5590 & \\cellcolor{LightCyan} 0.5614 &              \\cellcolor{LightCyan} \\textbf{0.7953} & 0.7888 & 0.8012 \\\\\n   &  \\cellcolor{LightCyan} alter &       \\cellcolor{LightCyan} \\textbf{0.5374} & 0.5357 & 0.5162 &          0.5599 & 0.5555 & 0.5592 &             0.7905 & \\cellcolor{LightCyan} 0.7915 &  \\cellcolor{LightCyan} \\textbf{0.8057} \\\\\n   & \\cellcolor{LightCyan} parallel &      0.5370 &  \\cellcolor{LightCyan} \\textbf{0.5379} & 0.5209 &           \\cellcolor{LightCyan} \\textbf{0.5647} & \\cellcolor{LightCyan} 0.5600 & 0.5529 &             0.7878 & 0.7896 & 0.7999 \\\\\n\\midrule\n\\multirow{3}{*}{PE} & \\cellcolor{LightYellow} degree &     \\cellcolor{LightYellow} 0.5291 & 0.5250 & 0.5133 &          0.5551 & 0.5618 & 0.5502 &           \\cellcolor{LightYellow}  0.7920 & \\cellcolor{LightYellow} 0.7913 & 0.7947 \\\\\n   & \\cellcolor{LightYellow} eig &      0.5253 & 0.5278 & \\cellcolor{LightYellow} 0.5257 &         \\cellcolor{LightYellow} 0.5575 & 0.5637 & 0.5658 &             0.7893 & 0.7887 & \\cellcolor{LightYellow} 0.8017 \\\\\n   & \\cellcolor{LightYellow} svd &      0.5281 & \\cellcolor{LightYellow} 0.5317 & 0.5203 &          0.5614 &  \\cellcolor{LightYellow}\\textbf{0.5663} &  \\cellcolor{LightYellow}\\textbf{0.5706} &             0.7856 & 0.7893 & 0.8007 \\\\\n\\midrule\n\\multirow{4}{*}{AT} &\\cellcolor{LightRed}  SPB &      \\underline{0.5368} & \\underline{0.5364} & 0.5234 &          0.5503 & \\underline{0.5605} & 0.5576 &             0.7921 &  \\underline{\\textbf{0.7918}} & 0.7984 \\\\\n   & \\cellcolor{LightRed} PMA &      0.5240 & 0.5288 & 0.5204 &          0.5567 & 0.5571 & 0.5492 &             0.7877 & 0.7853 & 0.7945 \\\\\n   &\\cellcolor{LightRed}  Mask-1 &      0.5295 & 0.5300 & 0.5236 &          0.5598 & 0.5559 & 0.5583 &             0.7923 & 0.7917 & 0.7963 \\\\\n   &\\cellcolor{LightRed}  Mask-n &      0.5359 & 0.5349 &  \\underline{\\textbf{0.5348}} &          0.5593 & 0.5603 & 0.5576 &             \\underline{0.7935} & 0.7917 & \\underline{0.8016} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{node-level}\n\\end{table*}",
            "tab:all_res": "\\begin{table*}[h]\n\\scriptsize\n\\centering\n\\caption{Performance comparison on graph-level tasks. For each column, the values with underline are the best results in each group. The values in bold  are the best results across the groups.}\n\\vspace{-2ex}\n\\resizebox{1\\linewidth}{!}{\n\\begin{tabular}{@{\\hskip 0.03in}c@{\\hskip 0.04in}c@{\\hskip 0.03in}|@{\\hskip 0.03in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.03in}|@{\\hskip 0.03in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.03in}|@{\\hskip 0.03in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.03in}|@{\\hskip 0.03in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.03in}|@{\\hskip 0.03in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.03in}|@{\\hskip 0.03in}c@{\\hskip 0.05in}c@{\\hskip 0.05in}c@{\\hskip 0.03in}}\n\\toprule\n   &        & \\multicolumn{9}{c|@{\\hskip 0.03in}}{ graph-level tasks } & \\multicolumn{9}{c}{node-level tasks} \\\\\n   &        & \\multicolumn{3}{c|@{\\hskip 0.03in}}{ZINC(MAE$\\downarrow$)} & \\multicolumn{3}{c|@{\\hskip 0.03in}}{molhiv(ROC-AUC$\\uparrow$)} & \\multicolumn{3}{c|@{\\hskip 0.03in}}{molpcba(AP$\\uparrow$)}  & \\multicolumn{3}{c|@{\\hskip 0.03in}}{Flickr(Acc$\\uparrow$)} & \\multicolumn{3}{c|@{\\hskip 0.03in}}{arixv(Acc$\\uparrow$)} & \\multicolumn{3}{c}{product(Acc$\\uparrow$)} \\\\\n%\\cline{3-20}\n   &        &     Small & Middle &  Large &                Small & Middle &  Large &            Small & Middle &  Large  &       Small & Middle &  Large &           Small & Middle &  Large &              Small & Middle &  Large \\\\\n\\midrule\nTF & vanilla &    0.6689 & 0.6700 & 0.6699 &               0.7466 & 0.7230 & 0.7269 &           0.1624 & 0.1673 & 0.1676 &      0.5270 & 0.5279 & 0.5214 &          0.5539 & 0.5571 & 0.5598 &             0.7887 & 0.7887 & 0.7956 \\\\\\midrule\n\\multirow{3}{*}{GA} &  before &    0.4700 & 0.4809 & 0.5169 &              0.6758 & 0.7339 & 0.7182 &           0.2105 & 0.1989 & \\underline{0.2269} &      0.5352 & 0.5369 & \\underline{0.5272} &          0.5608 & 0.5590 & \\underline{0.5614} &             \\underline{\\textbf{0.7953}} & 0.7888 & 0.8012\\\\\n   & alter &    \\underline{0.3771} & 0.3412 & \\underline{0.2956} &            \\underline{0.7200} &  0.7086 & 0.7433 &            \\underline{0.2474} & 0.2417 & 0.2244 &      \\underline{\\textbf{0.5374}} & 0.5357 & 0.5162 &          0.5599 & 0.5555 & 0.5592 &             0.7905 &  \\underline{0.7915} &   \\underline{\\textbf{0.8057}} \\\\\n   &  parallel &    0.3803 & \\underline{\\textbf{0.2749}} & 0.2984 &               0.7138 & \\underline{0.7750} & \\underline{0.7603} &           0.2345 & \\underline{0.2444} & 0.2205  &      0.5370 &   \\underline{\\textbf{0.5379}} & 0.5209 &            \\underline{\\textbf{0.5647}} & \\underline{0.5600} & 0.5529 &             0.7878 & 0.7896 & 0.7999 \\\\\\midrule\n%\\cline{1-11}\n\\multirow{3}{*}{PE} &   degree &    \\underline{0.5364} & \\underline{0.5364} & 0.5435 &   \\underline{0.7506} & 0.6818 & \\underline{0.7357} &           0.1672 & 0.1646 & 0.1650 &     \\underline{0.5291} & 0.5250 & 0.5133 &          0.5551 & 0.5618 & 0.5502 &  \\underline{0.7920} & \\underline{0.7913} & 0.7947 \\\\\n   & eig &    0.6031 & 0.6074 & 0.6166 &               0.7407 & \\underline{0.7279} & 0.7351 & \\underline{0.2194} & \\underline{0.2087} & \\underline{0.2131} &      0.5253 & 0.5278 & \\underline{0.5257} & 0.5575 & 0.5637 & 0.5658 &             0.7893 & 0.7887 &  \\underline{0.8017} \\\\\n   &  svd &    0.5811 & 0.5462 & \\underline{0.5400} &                0.7350 & 0.7155 & 0.7275 &           0.1648 & 0.1663 & 0.1767 &      0.5281 & \\underline{0.5317} & 0.5203 &          \\underline{0.5614} &  \\underline{\\textbf{0.5663}} &  \\underline{\\textbf{0.5706}} &             0.7856 & 0.7893 & 0.8007 \\\\\\midrule\n%\\cline{1-11}\n\\multirow{4}{*}{AT} & SPB &    0.5122 & 0.4878 & 0.6100 &               0.7065 & 0.7589 & 0.7255 &           0.2409 & 0.2524 &  \\underline{\\textbf{0.2621}} &     \\underline{0.5368} & \\underline{0.5364} & 0.5234 &          0.5503 & \\underline{0.5605} & 0.5576 &             0.7921 &  \\underline{\\textbf{0.7918}} & 0.7984 \\\\\n   & PMA &    0.6194 & 0.6057 & 0.5963 &               0.6902 & 0.7054 & 0.7314 &           0.2115 & 0.1956 & 0.2518 &      0.5240 & 0.5288 & 0.5204 &          0.5567 & 0.5571 & 0.5492 &             0.7877 & 0.7853 & 0.7945 \\\\\n   & Mask-1 &   \\underline{\\textbf{0.2861}} &  \\underline{0.2772} &  \\underline{\\textbf{0.2894}} &                \\underline{\\textbf{0.7610}} &  \\underline{\\textbf{0.7753}} &  \\underline{\\textbf{0.7960}} &           0.2573 & \\underline{\\textbf{0.2662}} & 0.1594  &      0.5295 & 0.5300 & 0.5236 &          \\underline{0.5598} & 0.5559 & \\underline{0.5583} &             0.7923 & 0.7917 & 0.7963 \\\\\n   &  Mask-n &    0.3906 & 0.3596 & 0.4765 &               0.7286 & 0.7423 & 0.7128 &            \\underline{\\textbf{0.2619}} & 0.2577 & 0.2380 &      0.5359 & 0.5349 &  \\underline{\\textbf{0.5348}} &          0.5593 & 0.5603 & 0.5576 &             \\underline{0.7935} & 0.7917 & \\underline{0.8016} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\vspace{-3ex}\n\\label{tab:all_res}\n\\end{table*}",
            "tab:exp:train:hyperpara": "\\begin{table}[]\n\\footnotesize\n    \\centering\n    \\caption{The training hyper-parameters.}\n    \\vspace{-2ex}\n    \\resizebox{0.9\\linewidth}{!}{\n    \\begin{tabular}{l r|l r}\n    \\hline\n        attention dropout & 0.1 & FFN dropout & 0.1 \\\\\n        maximum steps & 1e+6 & warm-up steps & 4e+4 \\\\\n        peak learning rate & 2e-4 & batch size & 256 \\\\\n        weight decay value & 1e-3 & lr decay & Linear \\\\\n        gradient clip norm & 5.0 & Adam $\\epsilon, \\beta_1, \\beta_2$& 1e-8, 0.9, 0.99 \\\\\n    \\hline\n    \\end{tabular}\n    }\n    \\vspace{-2ex}\n    \\label{tab:exp:train:hyperpara}\n\\end{table}"
        },
        "figures": {
            "gnn_emb": "\\begin{figure}[htbp]\n\\centering\n\\subfigure[Before Trans]{\n\\includegraphics[width=0.26\\columnwidth]{figures/before.pdf}\n}\n\\subfigure[Alternatively]{\n\\includegraphics[width=0.26\\columnwidth]{figures/alter.pdf}\n}\n\\subfigure[Parallel]{\n\\includegraphics[width=0.39\\columnwidth]{figures/parallel.pdf}\n}\n\\caption{Three types of GNN-as-Auxiliary-Modules architecture.}\n\\vspace{-2ex}\n\\label{gnn_emb}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\\mathbf{A}&=\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}, \\label{eq:attention1}\\\\\n\\widetilde{\\BX}&=\\operatorname{SoftMax}(\\mathbf{A}) (\\mathbf{X} \\mathbf{V}), \\label{eq:attention2}\\\\\n\\mathbf{M}&=\\operatorname{LayerNorm}_{1}(\\widetilde{\\BX}\\BO+\\mathbf{X}), \\label{eq:attention3}\\\\\n\\mathbf{F}&=\\sigma(\\mathbf{M W}_1+\\mathbf{b}_1) \\mathbf{W}_2+\\mathbf{b}_2, \\label{eq:ffn}\\\\\n\\mathbf{Z}&=\\operatorname{LayerNorm}_{2}(\\mathbf{M}+\\mathbf{F}), \\label{eq:layernorm}\n%\\end{equation}\n\\end{align}",
            "eq:2": "\\begin{align}\n\\BA^{(h)}=\\frac{1}{\\sqrt{d}} \\BX \\BQ^{(h)}(\\BX\\BK^{(h)})^{\\top}, \\\\\n\\widetilde{\\BX}=\\|_{h=1}^{H}(\\operatorname{SoftMax}(\\BA^{(h)}) \\BX \\BV^{(h)}).\n\\end{align}",
            "eq:3": "\\begin{equation}\n\\BM^{\\prime}=\\text{GraphConv}(\\BA^G, \\BM ; \\BW^{G})=\\sigma(\\BA^G \\BX \\BW^{G}).\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\BM'=\\BM+\\text{G-Res}(\\BX,\\BX_r,\\BA^G),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\widetilde{\\BX} = \\BX + f_\\text{map}(\\BP),\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\BU^{T} \\Lambda \\BU=\\BI-\\BD^{-1 / 2} \\BA^G \\BD^{-1 / 2},\n\\end{equation}",
            "eq:7": "\\begin{align}\n&\\BA^G \\stackrel{\\mathrm{SVD}}{\\approx} \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{T}=(\\mathbf{U} \\sqrt{\\boldsymbol{\\Sigma}}) \\cdot(\\mathbf{V} \\sqrt{\\boldsymbol{\\Sigma}})^{T}=\\hat{\\mathbf{U}} \\hat{\\mathbf{V}}^{T}, \\\\\n&\\BP=\\hat{\\mathbf{U}} \\| \\hat{\\mathbf{V}},\n\\end{align}",
            "eq:8": "\\begin{equation}\n    \\tilde{\\x} = \\x + \\z_{\\mathrm{deg}^{-}(v_{i})}^{-}+\\z_{\\mathrm{deg}^{+}(v_{i})}^{+},\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\vspace{-1ex}\n    \\begin{split}\n        \\BA =& f_\\text{G\\_att}(\\BX,\\BA^G,\\BE;\\BQ,\\BK,\\BW_1),\\\\\n        \\BM =& f_\\text{M}(\\BX,\\BA,\\BA^G,\\BE;\\BV,\\BW_2),\n    \\end{split}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\vspace{-1ex}\n\\label{maskattention}\n    \\BA = (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}) \\odot \\BA^{G},\n\\end{equation}",
            "eq:11": "\\begin{equation}\n      \\BA = (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}) \\odot \\BA^{G} \\odot \\BE'\\BW_E,  \n\\end{equation}",
            "eq:12": "\\begin{equation}\n\\aligned\n        \\BA &= (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{Q})^{\\top}) \\odot \\BK_r,\\\\\n        \\widetilde{\\BX} &= \\text{SoftMax}(\\BA)(\\BX\\BV),\\\\\n        \\BM &= \\text{LayerNorm}(\\BD^{-\\frac{1}{2}}\\widetilde{\\BX}+\\BX),\n\\endaligned\n\\end{equation}",
            "eq:13": "\\begin{equation}\n    \\BA = (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}) + \\BB^{s}.\n\\end{equation}",
            "eq:14": "\\begin{equation}\n    \\BA = (\\frac{1}{\\sqrt{d}} \\mathbf{X Q}(\\mathbf{X}\\mathbf{K})^{\\top}) + \\BB^{s}+\\BB^{c}\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\vspace{-1.5ex}\n  \\BA_{ij} =    (\\frac{1}{\\sqrt{d}} \\x_i\\BQ(\\x_j\\BK)^\\top)+\\boldsymbol{\\phi}^{ij} \\b^\\top,\n\\end{equation}",
            "eq:16": "\\begin{equation}\n\\boldsymbol{\\phi}_{i j}=\\operatorname{Concat}(\\Phi_{m}(v_{i}, v_{j}) \\mid m \\in 0,1, \\ldots, M-1),\n\\end{equation}",
            "eq:17": "\\begin{align}\n\\vspace{-2.5ex}\n  \\BA_{ij} &=    \\frac{1}{\\sqrt{d}}( \\x_i \\BQ(\\x_j\\BK)^\\top)+a_{ij}^K\\\\\n  \\BM_i &= \\sum^n_{j=1}\\text{SoftMax}(\\BA_{ij})(\\x_j\\BV+a_{ij}^V)\n\\end{align}"
        },
        "git_link": "https://github.com/graphdeeplearning/graphtransformer"
    }
}