{
    "meta_info": {
        "title": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive  Attribute Leakage",
        "abstract": "Graph Neural Networks (GNNs) have shown great power in learning node\nrepresentations on graphs. However, they may inherit historical prejudices from\ntraining data, leading to discriminatory bias in predictions. Although some\nwork has developed fair GNNs, most of them directly borrow fair representation\nlearning techniques from non-graph domains without considering the potential\nproblem of sensitive attribute leakage caused by feature propagation in GNNs.\nHowever, we empirically observe that feature propagation could vary the\ncorrelation of previously innocuous non-sensitive features to the sensitive\nones. This can be viewed as a leakage of sensitive information which could\nfurther exacerbate discrimination in predictions. Thus, we design two feature\nmasking strategies according to feature correlations to highlight the\nimportance of considering feature propagation and correlation variation in\nalleviating discrimination. Motivated by our analysis, we propose Fair View\nGraph Neural Network (FairVGNN) to generate fair views of features by\nautomatically identifying and masking sensitive-correlated features considering\ncorrelation variation after feature propagation. Given the learned fair views,\nwe adaptively clamp weights of the encoder to avoid using sensitive-related\nfeatures. Experiments on real-world datasets demonstrate that FairVGNN enjoys a\nbetter trade-off between model utility and fairness. Our code is publicly\navailable at https://github.com/YuWVandy/FairVGNN.",
        "author": "Yu Wang, Yuying Zhao, Yushun Dong, Huiyuan Chen, Jundong Li, Tyler Derr",
        "link": "http://arxiv.org/abs/2206.03426v2",
        "category": [
            "cs.LG",
            "cs.CR",
            "cs.CY"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\\label{sec-introduction}\nAs the world becomes more connected, graph mining is playing a crucial role in many domains such as drug discovery  and recommendation system~\\cite{fan2019graph, chen2021structured, chemicalx}. As one of its major branches, learning %low-dimensional yet \ninformative node representation is a fundamental solution to many real-world problems %applications\nsuch as node classification and link prediction~\\cite{TDGNN, zhang2018link}. Numerous data-driven models have been developed for learning node representations, among which Graph Neural Networks (GNNs) have achieved unprecedented success owing to the combination of neural networks and feature propagation~\\cite{GCN, APPNP, DAGNN}. \n%\\td{...%see comment}\n%}%adding DPGNN reference here?} \nDespite the significant progress of GNNs in %learning expressive node representations that \ncapturing higher-order neighborhood information\n% dependencies\n~\\cite{GCNII}, leveraging multi-hop dependencies~\\cite{TDGNN}, \n%\\td{...%see comment}\n%}%adding DPGNN reference here noting imbalance?} \nand recognizing complex local topology contexts~\\cite{wijesinghe2022a}, predictions of GNNs have been demonstrated to be unfair and perpetuate undesirable discrimination~\\cite{dai2021say, shumovskaia2021linking, xu2021towards, nifty, bose2019compositional}. \n\n\n\nRecent studies have revealed that historical data may include %patterns of \nprevious discriminatory decisions dominated by sensitive features~\\cite{mehrabi2021survey,du2020fairness}. Thus, node representations learned from such data may explicitly inherit the existing societal biases and hence exhibit unfairness when applied in practice. Besides the sensitive features, network topology also serves as an implicit source of societal bias~\\cite{EDITS, dai2021say}. By the principle of network homophily~\\cite{mcpherson2001birds}, nodes with similar sensitive features tend to form closer connections than dissimilar ones. Since feature propagation smooths representations of neighboring nodes while separating distant ones, representations of nodes in different sensitive groups are further segregated and their corresponding predictions are unavoidably over-associated with sensitive features.\n\n\nBesides above topology-induced bias, feature propagation could introduce another potential issue, termed as the sensitive information leakage. Since feature propagation naturally allows feature interactions among neighborhoods, the correlation between two feature channels is likely to vary after feature propagation, which is termed as correlation variation. As such, some original innocuous feature channels that have lower correlation to sensitive channels and encode less sensitive information may become highly correlated to sensitive ones after feature propagation and hence encode more sensitive information, which is termed as {\\it sensitive attribute leakage}.\n% \\yd{...}\n% \\maybereword{Since feature propagation provides a natural way for feature interactions among neighborhoods, the correlation between two feature channels is very likely to vary after feature propagation, which is termed as correlation variation after propagation, and may cause the leakage of sensitive features to innocuous features that may have been originally uncorrelated or have low correlations to sensitive ones.}\n%comment below \n%%%%yd comment: I think the reason for the leakage here is not quite clear. Does it mean that for the nodes in one group, their accessible values (during the information propagation) on some specific feature dimensions follow a different distribution compared with that for the nodes in the other group? This might account for why the correlation between a feature dimension and the sensitive feature dimension changes.}\n%%%%yw comment: I think not only because of the original feature distribution, but also because of the network topology. If sensitive feature channel and feature channel A can be quickly smoothed during propagation along the network topology while feature channel B can only be slowly smoothed during propagation, then after limited number of propagations, feature channel A might encode more sensitive information than channel B.\n%%%%%\n% Then these seemingly innocuous but actually `sensitive' features could further aggravate discrimination when their information is leveraged in making predictions. \nSome research efforts have been invested in alleviating discrimination  made by GNNs. However, they either borrow approaches from traditional fair representation learning such as adversarial debiasing~\\cite{dai2021say, bose2019compositional} and contrastive learning~\\cite{kose2021fairness} or directly debiasing node features and graph topology~\\cite{EDITS, nifty} while overlooking the sensitive attribute leakage caused by correlation variation. %after feature propagation. %\\yd{...}%comment below\n%%%%%yd comment: If the embeddings are debiased after propagation, can we say that works bearing such property consider the sensitive leakage effect?}\n%%%%%\n%%%%%yw comment: I think your observation here is pretty insightful, any work that directly debiasing in the embedding space can alleviate such kind of issue. might need to rethink on this.\n%%%%%\n\n\\indent \nIn this work, we study a novel and detrimental phenomenon where feature propagation can vary feature correlations and cause the leakage of sensitive information to innocuous features. To address this issue, we propose a principled framework Fair View Graph Neural Network (FairVGNN) to effectively learn fair node representations and avoid sensitive attribute leakage. Our major contributions are as follows:\n\\vspace{-1ex}\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Problem}: We investigate the novel phenomenon that feature propagation could vary feature correlations and cause sensitive attribute leakage to innocuous feature channels, which could further exacerbate discrimination in predictions. \n    \n    \\item \\textbf{Algorithm}: To prevent sensitive attribute leakage, we propose a novel framework FairVGNN to automatically learn fair views by identifying and masking sensitive-correlated channels and adaptively clamping weights to avoid leveraging sensitive-related features in learning fair node representations.\n    \n    \\item \\textbf{Evaluation}: We perform experiments on real-world datasets to corroborate that FairVGNN can approximate the model utility while reducing discrimination.\n\\end{itemize}\n\n\\indent Section~\\ref{sec-preliminary} introduces preliminaries. In Section~\\ref{sec-leakage}, we formally introduce the phenomenon of correlation variation and sensitive attribute leakage in GNNs and design two feature masking strategies to highlight the importance of circumventing sensitive attribute leakage for alleviating discrimination. To automatically identify/mask sensitive-relevant \nfeatures, we propose FairVGNN in Section~\\ref{sec-method}, which consists of a generative adversarial debiasing module to prevent sensitive attribute leakage from the input perspective by learning fair feature views and an adaptive weight clamping module to prevent sensitive attribute leakage from the model perspective by clamping weights of sensitive-correlated channels of the encoder. In Section~\\ref{sec-experiment}, we evaluate FairVGNN by performing extensive experiments. Related work is presented in Section~\\ref{sec-relatedwork}. Finally, we conclude and discuss future work in Section~\\ref{sec-conclusion}. \n\n% \\indent The rest of this paper is organized as follows. In Section \\ref{sec-preliminary}, we introduce necessary notations, briefly review core concepts in fairness and GNNs. In Section~\\ref{sec-leakage}, we formally introduce the problem of correlation variation and sensitive leakage caused by feature propagation and design two naive feature masking strategies to highlight the importance of considering such type of sensitive leakage in alleviating discrimination. To automatically identify and mask sensitive-relevant feature channels after feature propagation, we propose the FairVGNN framework in Section~\\ref{sec-method}, which consists of the generative adversarial debiasing module to prevent sensitive leakage from the input perspective by learning fair feature views and the adaptive weight clipping module to prevent sensitive leakage from the model perspective by clamping weights of sensitive-correlated channels in the encoder. In Section~\\ref{sec-experiment}, we evaluate our framework by performing extensive experiments and provide clear interpretations on the learned fair feature views with less-biased node representations. Related work is then presented in Section~\\ref{sec-relatedwork}. Finally, we conclude and discuss future work in Section \\ref{sec-conclusion}.\\vspace{-1.25ex}\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\\label{sec-preliminary}\n%In this section, we introduce necessary notations and review two core concepts of fairness in machine learning.\n\n% \\vspace{-0.75ex}\n",
                "subsection 2.1": {
                    "name": "Notations",
                    "content": "\nWe denote an attributed graph by $G = (\\mathcal{V}, \\mathcal{E}, \\mathbf{X}, \\mathbf{A})$ where $\\mathcal{V} = \\{v_1, ..., v_n\\}$ is the set of $n$ nodes with $\\mathbf{Y}\\in\\mathbb{R}^{n}$ specifying their labels, $\\mathcal{E}$ is the set of $m$ edges with $e_{ij}$ being the edge between nodes $v_i$ and $v_j$, and $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$ is the node feature matrix with $\\mathbf{X}_{i} = \\mathbf{X}[i, :]^{\\top} \\in \\mathbb{R}^d$ indicating the features of node $v_i$, $\\mathbf{X}_{:j} = \\mathbf{X}[:, j]\\in\\mathbb{R}^n$ indicating the $j^{\\text{th}}$-channel feature. The network topology is described by its adjacency matrix $\\mathbf{A}\\in \\{0, 1\\}^{n\\times n}$, where $\\mathbf{A}_{ij} = 1$ \n%denotes an edge between nodes $v_i$ and $v_j$, \nwhen $e_{ij} \\in \\mathcal{E}$,\nand $\\mathbf{A}_{ij} = 0$ otherwise. Node sensitive features are specified by the $s^{\\text{th}}$-channel of $\\mathbf{X}$, i.e., $\\mathbf{S} = \\mathbf{X}_{:s} \\in \\mathbb{R}^{n}$. Details of all notations used in this work are summarized in Table~\\ref{tb:symbols} in Appendix~\\ref{sec-notation}.\n\n\\vspace{-1.5ex}\n"
                },
                "subsection 2.2": {
                    "name": "Fairness in Machine Learning",
                    "content": "\nGroup fairness and individual fairness are two commonly encountered fairness notions in real life~\\cite{du2020fairness}. Group fairness emphasizes that algorithms should not yield discriminatory outcomes for any specific demographic group~\\cite{EDITS} while individual fairness requires that similar individuals be treated similarly~\\cite{dong2021individual}. Here we focus on group fairness with a binary sensitive feature, i.e., $\\mathbf{S} \\in \\{0, 1\\}^n$, but our framework could be generalized to multi-sensitive groups and we leave this as one future direction. \n%However, the proposed framework could be generalized to multi-sensitive groups and we leave it as one potential future direction.\nFollowing~\\cite{dai2021say, nifty, EDITS}, we employ the difference of statistical parity and equal opportunity between two different sensitive groups, to evaluate the model fairness:\n\\begin{equation}\\label{eq-spdelta}\n    \\Delta_{\\text{sp}} = |P(\\hat{y} = 1|s = 0) - P(\\hat{y} = 1|s = 1)|,\n\\end{equation}\n\\vskip -3ex\n\\begin{equation}\n\\vspace{-.5ex}\n    \\Delta_{\\text{eo}} = |P(\\hat{y} = 1|y = 1, s = 0) - P(\\hat{y} = 1 |y = 1, s = 1)|,\n\\end{equation}\nwhere $\\Delta_{\\text{sp}} (\\Delta_{\\text{eo}})$ measures the difference of the independence level of the prediction $\\hat{y}$ (true positive rate) on the sensitive feature $s$ between two groups. Since group fairness expects algorithms to yield similar outcomes for different demographic groups, fairer machine learning models seek lower $\\Delta_{\\text{sp}}$ and $\\Delta_{\\text{eo}}$.\n\n\n%here is the older version of introducing fairness metrics, a little bit redundant and similar to previous works, so I simplified to the above version\n% Next, %In the following, \n% we introduce two widely-adopted metrics, statistical parity and equal opportunity, to evaluate the group fairness of any machine learning model.\n\n% \\begin{itemize}[leftmargin=*]\n%     \\item \\textit{\\textbf{Statistical Parity}}: it measures the level of independence of the prediction $\\hat{y}$ on the sensitive feature $s$ of any node $v\\in\\mathcal{V}$:\n%     \\begin{equation}\n%         P(\\hat{y} = 1|s = 0) = P(\\hat{y} = 1|s = 1), \\forall v \\in \\mathcal{V}.\n%     \\end{equation}\n    \n%     \\item \\textit{\\textbf{Equal Opportunity}}: it measures the level of independence of true positive rate on the sensitive feature $s$ of any node $v\\in\\mathcal{V}$:\n%     \\begin{equation}\n%         P(\\hat{y} = 1|y = 1, s = 0) = P(\\hat{y} = 1|y = 1, s = 1), \\forall v \\in \\mathcal{V}.\n%     \\end{equation}\n% \\end{itemize}\n\n% Following the literature~\\cite{dai2021say, nifty, EDITS}, we compute the difference of statistical parity and equal opportunity among different sensitive groups to quantitatively evaluate the model fairness: \n% \\begin{equation}\n%     \\Delta_{\\text{sp}} = |P(\\hat{y} = 1|s = 0) - P(\\hat{y} = 1|s = 1)|,\n% \\end{equation}\n% \\begin{equation}\n%     \\Delta_{\\text{eo}} = |P(\\hat{y} = 1|y = 1, s = 0) - P(\\hat{y} = 1 |y = 1, s = 1)|,\n% \\end{equation}\n% since group fairness expects algorithms to yield similar outcomes for different demographic groups, fairer machine learning models typically possess lower $\\Delta_{\\text{sp}}$ and $\\Delta_{\\text{eo}}$.\n\n% \\vspace{-1.25ex}\n% \\subsection{Graph Neural Networks}\\label{sec-preliminary-gnn}\n% A unified template of a graph convolutional layer is formalized as:\n% %The unified template of one graph convolutional layer can be summarized as:\n% \\begin{equation}\\label{eq-propagation}\n%     \\mathbf{h}_i^l = \\text{TRAN}^{l}(\\text{PROP}^{l}(\\mathbf{h}_{i}^{l - 1}, \\{\\mathbf{h}_j^{l - 1}|j\\in\\mathcal{N}_i\\})),\n% \\end{equation}\n%  where $\\mathcal{N}_i$ denotes the neighborhood set of node $v_i$ and $\\text{PROP}^l, \\text{TRAN}^l$ stand for neighborhood propagation and feature transformation at layer $l$. In neighborhood propagation, neighborhood representations are propagated and further fused with itself to get the intermediate representation $\\widehat{\\mathbf{h}}_i^l$. \n% %  representations of neighborhoods at the previous layer $\\{\\mathbf{h}_j^{l - 1}|j\\in\\mathcal{N}_i\\}$ are propagated to the center node $v_i$ and further fused with its own representation $\\mathbf{h}_i^{l - 1}$ from the previous layer via $\\text{AGG}^l$ function at layer $l$ to get the intermediate representation $\\widehat{\\mathbf{h}}_i^l$. \n% Then, the $\\text{TRAN}^l$ function is applied on $\\widehat{\\mathbf{h}}_i^l$ to get the final representation $\\mathbf{h}_i^l$ of node $v_i$ at layer $l$. Note that $\\mathbf{h}_i^0$ of node $v_i$ is typically initialized as the original node feature $\\mathbf{X}_{i}$. After stacking $L$ graph convolutional layers, every node  aggregates their neighborhood information up to $L$-hops away and we denote it as $\\mathbf{H}^L\\in\\mathbb{R}^{n\\times d^{L}}$. \n% %  Most graph convolutions, such as GCN~\\cite{GCN}, GraphSAGE~\\cite{Graphsage}, GAT~\\cite{GAT}, and GIN~\\cite{GIN}, can be obtained under this template by adopting and configuring different functions in $\\text{PROP}^{l}$ and $\\text{TRAN}^{l}$. Owing to the power of GNNs in node representation learning, this work employs it as the encoder-backbone.\n%  Many graph convolutions (e.g., GCN~\\cite{GCN}, GraphSAGE~\\cite{Graphsage}, and GIN~\\cite{GIN}) can be obtained under this template by configuring different $\\text{PROP}^{l}$ and $\\text{TRAN}^{l}$. In this work, the encoder of FairVGNN is designed following this template.\n% thus, this work is built upon a GNN encoder-backbone.\n\n%\\vspace{-1.25ex}\n\\vspace{-1.7ex}\n"
                }
            },
            "section 3": {
                "name": "Sensitive attribute leakage and correlation variation",
                "content": "\\label{sec-leakage}\nIn this section, we %systematically \nstudy the phenomenon where sensitive information leaks to innocuous feature channels after their correlations to the sensitive feature increase during feature propagation in GNNs, which we define as {\\it sensitive attribute leakage}\n% \\td{feature correlation variation induced sensitive leakage?}\n. We first empirically verify %that \nfeature channels with higher correlation to the sensitive channel would cause more discrimination in predictions~\\cite{zhao2021you}. We denote the Pearson correlation coefficient of the $i^{\\text{th}}$-feature channel to the sensitive channel as sensitive correlation and compute it as:\n\\begin{equation}\\label{eq-lc}\n    \\boldsymbol{\\rho}_{i} = \\frac{\\mathbb{E}_{v_j\\sim\\mathcal{V}}\\big((\\mathbf{X}_{ji} - \\mu_{i})(\\mathbf{S}_j - \\mu_{s})\\big)}{\\sigma_{i}\\sigma_{s}}, \\forall i\\in \\{1, 2, ..., d\\},\n\\end{equation}\nwhere $\\mu_i, \\sigma_i$ denote the mean and standard deviation of the channel $\\mathbf{X}_{:i}$. Intuitively, higher $\\boldsymbol{\\rho}_i$ indicates that the $i^{\\text{th}}$-feature channel encodes more sensitive-related information, which would impose more discrimination in the prediction. To further verify this assumption, we mask each channel and train a 1-layer MLP/GCN followed by a linear layer to make predictions. As suggested by~\\cite{nifty}, we do not add any activation function in the MLP/GCN to avoid capturing any nonlinearity.\n\n% \\begin{equation}\\label{eq-lc}\n%     \\boldsymbol{\\rho}_{i} = \\frac{\\mathbb{E}((\\mathbf{X}_{i} - \\mu_{i})(\\mathbf{S} - \\mu_{s}))}{\\sigma_{i}\\sigma_{s}}, \\forall i\\in \\{1, 2, ..., d\\},\n% \\end{equation}\n\nFigure~\\ref{fig-prelim}(a)-(b) visualize the relationships between the model utility/bias and the sensitive correlation of each masked feature channel. Clearly, we see that the discrimination does still exist even though we mask the sensitive channel (1$^\\text{st}$). \n%\\td{Compared with no masking, $\\Delta_{\\text{sp}}$ and $\\Delta_{\\text{eo}}$ tend to continuously decrease as we mask non-sensitive feature channels ranking higher according to $\\boldsymbol{\\rho}_{i}|$ due to their  }\nCompared with no masking situation, $\\Delta_{\\text{sp}}$ and $\\Delta_{\\text{eo}}$ almost always become lower when we mask %we mask any \nother non-sensitive feature channels (2$^\\text{nd}$-4$^\\text{th}$), which indicates the leakage of sensitive information to other non-sensitive feature channels. Moreover, we observe the decreasing trend of $\\Delta_{\\text{sp}}$ and $\\Delta_{\\text{eo}}$ when masking channels with higher sensitive correlation since these channels encode more sensitive information and masking them would alleviate more discrimination. % in %made\n%predictions. %\\yu{question below}\n\n% \\yy{I am wondering why masking the 4th channel leads to a worse performance in model fairness as shown in (b). It has a higher value than no masking. I am also thinking that will masking top-k features (masking k features rather than one channel) provide more intuitions? Can we expect such a result that when k increases the fairness performance gets better, but there is a difference in the curve slope, which indicates the importance of different channels.}\\yu{Good observation. Regarding your first question why masking 4th channel leads to worse performance, that's also what I am thinking about. One potential guess is that such feature channel has pretty low correlation to sensitive one such that removing it will not cause any fairness boost. But removing it will also kind of filtering out some information unrelated so that neural networks would focus more on the sensitive feature channel.}\\yy{Your explanation makes sense. And another possible reason could be that similar to (b), the coefficient distribution shifted and the 4th feature no longer remains top sensitive features. Maybe you can check that after the propagation, what is the coefficient of the 4-th channel. Since this unexpected observation only appears in GCN model rather than both models, I think this might be the core reason. If this is the case, it might be more natural to turn to the propagation from the first observation. (Something like, we observe a relationship between fairness and sensitive correlation while also noticing that for 4-th channel -> propagation impacts the correlation.)}\n\n\n\n\nFollowing the above observation, one natural way to prevent sensitive attribute leakage and alleviate discrimination is to mask the sensitive features as well as its highly-correlated non-sensitive features. However, feature propagation in GNNs could change feature distributions of different channels and consequentially vary feature correlations as shown by Figure~\\ref{fig-prelim}(c) where we visualize the sensitive correlations of the first 8 feature channels on German after a certain number of propagations. We see that correlations between the sensitive features %channel \nand other channels change \n% feature correlation changes \\yd{or the correlation between the channel of sensitive feature  and other channels} \nafter propagation. For example, some feature channels that are originally irrelevant to the sensitive one, such as the $7^{\\text{th}}$ feature channel, %would \nbecome highly-correlated and hence encode more sensitive information.\n\n% To demonstrate whether such increased correlation would aggravate discrimination in predictions, we train a linear classifier based on features after different layers of propagation and report the discrimination in Figure~\\ref{fig-prelim}(d). As before, we rule out any potential nonlinearity by avoiding using activation function in the linear classifier.\n% \\yd{Awesome solution for the problem we discussed before. But since we have mentioned we utilized linear classifier, do we have to emphasize that there is no activation function in a linear classifier? I think you may say we do not choose MLP despite its stronger fitting ability, as the performance of MLP might be entangled with potential nonlinearity.} \n%reply: good observation, I attached one sentence thereafter to say there is no activation function.\n% We see that the bias becomes higher as we increase the number of propagations. This is because when the linear classifier leverages innocuous features to make predictions, the increasing amount of sensitive information encoded in these innocuous features after propagation (verified by the increasing correlation in Figure~\\ref{fig-prelim}(c)) would simultaneously transfer to the predictions.\n% and cause additional discrimination.\n\n% Note that Credit has less correlation variation than German comparing Figure~\\ref{fig-prelim}(c)-(d). We hypothesize it is because the higher homophily of Credit than German triggers less change of feature correlation during feature propagation. In the extreme case where node features strictly obey the network homophily~\\cite{}, feature propagation would cause no change on feature distributions of every node and therefore the feature correlation would stay the same.\n\n%\\footnotetext{We respectively mask each feature channel and train a 1-layer MLP/GCN followed by a linear layer to make predictions. Dataset details and the experimental setting are detailed in Section~\\ref{sec-experimentsetting}.}\n\\footnotetext{We respectively mask each feature channel and train a 1-layer MLP/GCN followed by a linear prediction layer. Dataset and experimental details are given in Section~\\ref{sec-experimentsetting}.}\n\n\n% \\begin{table}[t]\n% \\footnotesize\n% \\setlength{\\extrarowheight}{.095pt}\n% \\setlength\\tabcolsep{3pt}\n% \\centering\n% \\caption{Results of models trained on masked features on German and Credit \\maybereword{unless specifically noting on the +- probably not needed here, not just due to being too wide (could be resolved), but too many values to easily see any trends from this table without taking a lot of time... if wanting to keep +- one value is missing for AUC, S2, GCN}}\n% \\label{tab-prelim}\n% \\begin{tabular}{|ll|ccc|ccc|}\n% \\hline\n% \\multicolumn{2}{|c|}{\\textbf{Encoder}} & \\multicolumn{3}{c|}{MLP} & \\multicolumn{3}{c|}{GCN} \\\\\n% \\hline\n% \\multicolumn{2}{|c|}{\\textbf{Strategy}} & S$_0$ & S$_1$ & S$_2$ & S$_0$ & S$_1$ & S$_2$ \\\\\n%  \\hline\n% \\multirow{5}{*}{\\makecell{German}} & AUC & 71.98$\\pm$1.85 & 69.89$\\pm$1.67 & 70.54$\\pm$2.36 & 74.11$\\pm$0.37 & 73.78$\\pm$0.73 & 72.75$\\pm$0.67\\\\\n%  & F1 & 82.32$\\pm$0.91 & 81.37$\\pm$1.23 & 81.44$\\pm$1.11 & 82.46$\\pm$0.89 & 81.65$\\pm$0.66 & 81.70$\\pm$0.84\\\\\n%  & ACC & 73.40$\\pm$1.18 & 70.88$\\pm$1.20 & 71.28$\\pm$1.63 & 73.44$\\pm$1.09 & 72.00$\\pm$0.91 & 71.76$\\pm$0.87\\\\\n%  & $\\Delta_{\\text{sp}}$ & 29.26$\\pm$12.80 & 8.25$\\pm$4.48 & 6.58$\\pm$4.69 & 35.17$\\pm$7.27 & 11.39$\\pm$1.65 & 8.29$\\pm$2.66\\\\\n%  & $\\Delta_{\\text{eq}}$ & 19.43$\\pm$10.38 & 4.75$\\pm$2.52 & 3.24$\\pm$2.12 & 25.17$\\pm$5.89 & 9.60$\\pm$1.24 & 6.91$\\pm$2.02\\\\\n%  \\hline\n% \\multirow{5}{*}{\\makecell{Credit}} & AUC & 74.46$\\pm$0.01 & 73.49$\\pm$0.03 & 73.49$\\pm$0.03 & 73.86$\\pm$0.02 & 72.92$\\pm$0.03 & 72.92\\\\\n%  & F1 & 81.64$\\pm$0.03 & 81.50$\\pm$0.01 & 81.50$\\pm$0.01 & 81.92$\\pm$0.02 & 81.84$\\pm$0.01 & 81.84$\\pm$0.03\\\\\n%  & ACC & 73.38$\\pm$0.03 & 73.20$\\pm$0.02 & 73.20$\\pm$0.02 & 73.67$\\pm$0.03 & 73.57$\\pm$0.01 & 73.57$\\pm$0.01\\\\\n%  & $\\Delta_{\\text{sp}}$ & 11.85$\\pm$0.03 & 11.50$\\pm$0.10 & 11.50$\\pm$0.10 & 12.86$\\pm$0.09 & 12.00$\\pm$0.02 & 12.00$\\pm$0.02\\\\\n%  & $\\Delta_{\\text{eq}}$ & 9.61$\\pm$0.04 & 9.20$\\pm$0.03 & 9.20$\\pm$0.03 & 10.63$\\pm$0.13 & 9.70$\\pm$0.02 & 9.70$\\pm$0.02\\\\\n%  \\hline\n% \\end{tabular}\n\n% \\begin{tablenotes}\n%   \\small\n%   \\item \\textbf{*} S$_0$: training using original feature matrix $\\mathbf{X}$ without any masking.\n  \n%   \\item \\textbf{*} S$_1$/S$_2$: training by masking the top-$4$ channels according to the rank of $\\boldsymbol{\\rho}^{\\text{origin}}$/$\\boldsymbol{\\rho}^{\\text{prop}}$.\n% \\end{tablenotes}\n% \\end{table}\n\n% \\begin{tabular}{|ll|ccc|ccc|}\n% \\hline\n% \\multicolumn{2}{|c|}{\\textbf{Encoder}} & \\multicolumn{3}{c|}{MLP} & \\multicolumn{3}{c|}{GCN} \\\\\n% \\hline\n% \\multicolumn{2}{|c|}{\\textbf{Strategy}} & S$_0$ & S$_1$ & S$_2$ & S$_0$ & S$_1$ & S$_2$ \\\\\n%  \\hline\n% \\multirow{5}{*}{\\makecell{German}} & AUC & 71.98 & 69.89 & 70.54 & 74.11 & 73.78 & 72.75\\\\\n%  & F1 & 82.32 & 81.37 & 81.44 & 82.46 & 81.65 & 81.70\\\\\n%  & ACC & 73.40 & 70.88 & 71.28 & 73.44 & 72.00 & 71.76\\\\\n%  & $\\Delta_{\\text{sp}}$ & 29.26 & 8.25 & 6.58 & 35.17 & 11.39 & 8.29\\\\\n%  & $\\Delta_{\\text{eq}}$ & 19.43 & 4.75 & 3.24 & 25.17 & 9.60 & 6.91\\\\\n%  \\hline\n% \\multirow{5}{*}{\\makecell{Credit}} & AUC & 74.46 & 73.49 & 73.49 & 73.86 & 72.92 & 72.92\\\\\n%  & F1 & 81.64 & 81.50 & 81.50 & 81.92 & 81.84 & 81.84\\\\\n%  & ACC & 73.38 & 73.20 & 73.20 & 73.67 & 73.57 & 73.57\\\\\n%  & $\\Delta_{\\text{sp}}$ & 11.85 & 11.50 & 11.50 & 12.86 & 12.00 & 12.00\\\\\n%  & $\\Delta_{\\text{eq}}$ & 9.61 & 9.20 & 9.20 & 10.63 & 9.70 & 9.70\\\\\n%  \\hline\n% \\end{tabular}\n\n% \\begin{tablenotes}\n%   \\small\n%   \\item \\textbf{*} S$_0$: training using original feature matrix $\\mathbf{X}$ without any masking.\n  \n%   \\item \\textbf{*} S$_1$/S$_2$: training by masking the top-$4$ channels according to the rank of $\\boldsymbol{\\rho}^{\\text{origin}}$/$\\boldsymbol{\\rho}^{\\text{prop}}$.\n% \\end{tablenotes}\n% \\end{table}\n\n\n\n\n\nAfter observing that feature propagation could vary feature correlation and cause sensitive attribute leakage,\n%and eventually introduce additional discrimination\nwe devise two simple but effective masking strategies to highlight the importance of considering correlation variation and sensitive attribute leakage in alleviating discrimination.\n% \\yd{Still the problem we have discussed: how to convince the audience that the bias is indeed introduced by higher linear correlation between any feature dimension and the sensitive feature dimension? It might be better (thought hard) to exclude the potential influence of non-linear dependencies.}\\yu{I think your suggestion here is awesome. One potential way I can think of is just by removing any activation function we use here to make the classifier purely linear, which is also aligned with the original GCN model coded in that NIFTY paper where they don't use any activation function in GCN.}\\yd{Indeed. This is the most intuitive solution for me, but it might be hard to theoretically exclude the influence of softmax function.}\nSpecifically, we first compute sensitive correlations of each feature channel according to 1) the original features $\\boldsymbol{\\rho}^{\\text{origin}}$ and 2) the propagated features $\\boldsymbol{\\rho}^{\\text{prop}}$. Then, we manually mask top-$k$ feature channels according to the absolute values of correlation given by $\\boldsymbol{\\rho}^{\\text{origin}}$ and $\\boldsymbol{\\rho}^{\\text{prop}}$, respectively, and train MLP/GCN/GIN on German/Credit dataset shown in Table~\\ref{tab-prelim}. \n% \\yy{And would it be better to mark the best performance in the table?}\nDetailed experimental settings are presented in Section~\\ref{sec-experiment}. From Table~\\ref{tab-prelim},\nwe have following insightful observations:\n% \\yd{do we really need the performance of GIN in Table 1 for some evaluation?}\n\\begin{inparaenum}\n    \\item Within the same encoder, masking sensitive and its related feature channels (S$_1$, S$_2$) would alleviate the discrimination while downgrading the model utility compared with no-masking (S$_0$).\n    %, which verifies the effectiveness of the masking strategy.\n    \\item GCN achieves better model utility but causes more bias compared with MLP on German and Credit. This implies graph structures also encode bias and leveraging them could aggravate discrimination in predictions, which is consistent with recent work~\\cite{dai2021say, EDITS}.\n    \\item Most importantly, S$_2$ achieves lower $\\Delta_{\\text{sp}}, \\Delta_{\\text{eo}}$ than S$_1$ for both MLP and GCN on German because the rank of sensitive correlation changes after feature propagation and masking according to S$_2$ leads to better fairness, which highlights the importance of considering feature propagation in determining which feature channels are more sensitive-correlated and required to be masked. Applying S$_1$ achieves the same utility/bias as S$_2$ on Credit due to less \n    correlation variations shown in Figure~\\ref{fig-prelim}(d).\n\\end{inparaenum} \n\n%The above analysis demonstrates the necessity to consider feature propagation in masking channels and alleviating discrimination. \nTo this end, we argue that it is necessary to consider feature propagation in masking feature channels in order to alleviate discrimination.  However, the correlation variation heavily depends on the propagation mechanism of GNNs.\n%and network properties of different datasets as shown in Table~\\ref{tab-prelim}.\nTo tackle this challenge, we formulate our problem as:\n\n\\textit{Given an attributed network $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, \\mathbf{X}, \\mathbf{A})$ with labels $\\mathbf{Y}$ for a subset of nodes $\\mathcal{V}_l \\subset \\mathcal{V}$, we aim to learn a fair view generator $g_{\\bm{\\Theta}_g}: g_{\\bm{\\Theta}_g}(\\mathbf{X}) \\rightarrow \\widetilde{\\mathbf{X}}$ with the expectation of simultaneously preserving task-related information and discarding sensitive information such that the downstream node classifier $f_{\\bm{\\Theta}_f}: f_{\\bm{\\Theta}_f}(\\mathbf{A}, \\widetilde{\\mathbf{X}}) \\rightarrow \\mathbf{Y}$ trained on $\\widetilde{\\mathbf{X}}$ could achieve better trade-off between model utility and fairness.}\n\n\n\n\n% The diagonal matrix of node degrees is denoted as $\\mathbf{D}\\in\\mathbb{R}^{n\\times n}$, where $\\mathbf{D}_{ii} = \\sum_{j}{\\mathbf{A}_{ij}}$ calculates the degree of the node $v_i$. We let $\\bar{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ represent the adjacency matrix with added self-loops and similarly let $\\bar{\\mathbf{D}}=\\mathbf{D} + \\mathbf{I}$. Then the normalized adjacency matrix can be defined as $\\widehat{\\mathbf{A}} = \\bar{\\mathbf{D}}^{-0.5}\\bar{\\mathbf{A}}\\bar{\\mathbf{D}}^{-0.5}$. The neighborhood node set of the center node $v_i$ is given by $\\mathcal{N}_{i} = \\{v_j|e_{ij}\\in\\mathcal{E}\\}$.\n\n\n\n\\vspace{-1ex}\n"
            },
            "section 4": {
                "name": "Framework",
                "content": "\\label{sec-method}\n%In this section, we give a detailed description of FairVGNN.  An illustration of the proposed framework is shown in Figure~\\ref{fig-fairvgnn}, which includes the generative adversarial debiasing module and the adaptive weight clamping module.\nIn this section, we give a detailed description of FairVGNN (shown in Figure~\\ref{fig-fairvgnn}), which includes the generative adversarial debiasing module and the adaptive weight clamping module. In the first module, we learn a generator that  generates different fair views of features to obfuscate the sensitive discriminator such that the encoder could obtain fair node representations for downstream tasks. In the second module, we propose to clamp weights of the encoder based on learned fair feature views, and provide a theoretical justification on its equivalence to minimizing the upper bound of the difference of representations between two different sensitive groups. Next, we introduce the details of each component.\n\n\\vspace{-1.5ex}\n",
                "subsection 4.1": {
                    "name": "Generative Adversarial Debiasing",
                    "content": "\nThis module includes a fair view generator $g_{\\bm{\\Theta}_g}$, a GNN-based encoder $f_{\\bm{\\Theta}_f}$, a sensitive discriminator $d_{\\bm{\\Theta}_d}$, and a classifier $c_{\\bm{\\Theta}_c}$ parametrized by $\\bm{\\Theta}_g, \\bm{\\Theta}_f, \\bm{\\Theta}_d, \\bm{\\Theta}_c$, respectively.\n% $\\boldsymbol{\\theta}_g, \\boldsymbol{\\theta}_f, \\boldsymbol{\\theta}_d, \\boldsymbol{\\theta}_c$, respectively. \nWe assume the view generator $g_{\\bm{\\Theta}_g}$ to be a learnable latent distribution from which we sample $K$-different masks and generate $K$-corresponding views $\\widetilde{\\mathbf{X}}^k, k\\in\\{1,2,..., K\\}$. The latent distribution would be updated towards generating less-biased views $\\widetilde{\\mathbf{X}}$ and the stochasticity of each view would enhance the model generalizability. Then each of these $K$-different views $\\widetilde{\\mathbf{X}}^k$ are fed to the encoder $f_{\\bm{\\Theta}_f}$ together with the network topology $\\mathbf{A}$ to learn node representations $\\widetilde{\\mathbf{H}}^k$ for downstream classifier $c_{\\bm{\\Theta}_c}$. Meanwhile, the learned node representations $\\widetilde{\\mathbf{H}}^k$ are used by the sensitive discriminator $d_{\\bm{\\Theta}_d}$ to predict nodes' sensitive features. This paves us a way to adopt adversarial learning to obtain the optimal fair view generator $g_{\\bm{\\Theta}_g^{*}}$ \n%from which\nwhere the generated views encode as much task-relevant information while discarding as much bias-relevant information as possible.\n%; thus, the \n% and hence \n%predictions made %on which \n%would encode less discrimination.\nWe begin with introducing the fairness-aware view generator $g_{\\bm{\\Theta}_g}$.\n\n\n\n\\vspace{-1ex}\n",
                    "subsubsection 4.1.1": {
                        "name": "Fairness-aware View Generator",
                        "content": "\\label{sec-g}\nAs observed in Table~\\ref{tab-prelim}, discrimination could be traced back to the sensitive features as well as their %its \nhighly-correlated non-sensitive features.\n% \\yd{Maybe sensitive feature does not generate discrimination by itself. They just bring sensitive information and discrimination can only arise when sensitive information is relied on by GNNs to make prediction.} \n% \\yu{You are exactly right. The sensitive feature would not necessasrily cause the discrimination but if there is some, it should either come from the sensitive feature or some other highly-correlated features. So here if we mask them, we could ensure there is no discrimination but definitely it is over-strong and maybe there is some more soft ideas like if the sensitive feature is not over-associated to the downstream prediction, we would not need to consider mask it.}\nTherefore, we propose to learn a view generator that automatically identifies and masks these features.\n% which \\maybereword{features are or more correlated} to the sensitive feature and discard sensitive-relevant information by masking these identified features.\nMore specifically, assuming the view generator as a conditional distribution $\\mathbb{P}_{\\widetilde{G}}$ parametrized by $\\bm{\\Theta}_g$, since bias originates from the node features $\\mathbf{X}$ and is further varied by the graph topology $\\mathbf{A}$, the conditional distribution of the view generator can be further expressed as a joint distribution of the attribute generator and the topological generator as $\\mathbb{P}_{\\widetilde{G}} = \\mathbb{P}_{\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{A}}}$. Since  our sensitive discriminator $d_{\\bm{\\Theta}_d}$ is directly trained on the learned node representations from GNN-based encoder $f_{\\bm{\\Theta}_f}$ as described in Section~\\ref{sec-cd}, we already consider the proximity-induced bias in alleviating discrimination and hence the network topology  is assumed to be fixed here, i.e., $\\mathbb{P}^{\\bm{\\Theta}_g}_{\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{A}}} =\\mathbb{P}^{\\bm{\\Theta}_g}_{\\widetilde{\\mathbf{X}}}$. We will leave the joint generation of fair feature and topological views as one future work.\n%old version modifed after considering yushun's comment\n% Moreover, as we will show in Section~\\ref{sec-experiment}, solely generating fair feature views with no topological modification has already achieved better fairness-utility trade-off, therefore the network topology in this work is assumed to be fixed here, i.e., $\\mathbb{P}^{\\boldsymbol{\\theta}_g}_{\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{A}}} =\\mathbb{P}^{\\boldsymbol{\\theta}_g}_{\\widetilde{\\mathbf{X}}}$.\n% \\yd{Will considering the topological information brings better performance?}\\yu{I think so just as your previous work EDITS and your current work, the network topology also serve as an implicit source to bring bias, also I think somehow the correlation variation caused by propagation is also related to the network topology} \\yd{It makes sense. Just not sure if it is a good motivation here to ignore topology: we already achieved better fairness-utility trade-off by only considering the features. When you initiate your methodology, you might have no idea about the performance.}\\yu{Your logic here is right. I reworded it a little bit} . \n\n% Next, we introduce how to model the fair view generator $\\mathbb{P}^{\\boldsymbol{\\theta}_g}_{\\widetilde{\\mathbf{X}}}$.\n\n% Therefore, we can safely decompose the joint generative distribution into the product of its marginal as $\\mathbb{P}_{\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{A}}}^{\\boldsymbol{\\theta}_g}(\\cdot|G) = \\mathbb{P}^{\\boldsymbol{\\theta}_{g_1}}_{ \\widetilde{\\mathbf{X}}}(\\cdot|\\mathbf{X})\\times \\mathbb{P}^{\\boldsymbol{\\theta}_{g_2}}_{\\widetilde{\\mathbf{A}}}(\\cdot|\\mathbf{A})$. \n\n% As motivated in Section~\\ref{sec-preliminary}, the discrimination in the predicting outcomes could be traced back to the input sensitive feature and its highly-correlated features due to sensitive leakage. Therefore to alleviate the discrimination and prevent the sensitive leakage from the source, we expect to generate the fair features $\\widetilde{\\mathbf{X}}\\in \\mathbb{R}^{n\\times d}$ that encodes less information about the sensitive feature. \n\nInstead of generating $\\widetilde{\\mathbf{X}}$ from scratch that completely loses critical information for GNN predictions, we generate $\\widetilde{\\mathbf{X}}$ conditioned on the original node features $\\mathbf{X}$, i.e., $\\mathbb{P}^{\\bm{\\Theta}_g}_{\\widetilde{\\mathbf{X}}} = \\mathbb{P}^{\\bm{\\Theta}_g}_{\\widetilde{\\mathbf{X}}}(\\cdot|\\mathbf{X})$. Following the preliminary experiments, we model the generation process of $\\widetilde{\\mathbf{X}}$ as identifying and masking sensitive features and their highly-correlated features in $\\mathbf{X}$. One natural way is to select features according to their correlations $\\boldsymbol{\\rho}_{i}$ to the sensitive features $\\mathbf{S}$ as defined in Eq.~\\eqref{eq-lc}. However, as shown by Figure~\\ref{fig-prelim}(c), feature propagation in GNNs triggers the correlation variation. %Therefore, \nThus, instead of masking according to initial correlations that might change after feature propagation, % we expect our model to learn by itself to identify which feature channels should be masked.\nwe train a learnable mask for feature selections in a data-driven fashion. Denote our mask as $\\mathbf{m} = [m_1, m_2, ..., m_d]\\in\\{0, 1\\}^d$ %such that:\nso that:\n\\begin{equation}\n    \\widetilde{\\mathbf{X}} = \\mathbf{X}\\odot\\mathbf{m} = [\\mathbf{X}_{1}^{\\top}\\odot \\mathbf{m}, \\mathbf{X}_{2}^{\\top}\\odot \\mathbf{m}, ..., \\mathbf{X}_{n}^{\\top}\\odot \\mathbf{m}],\n\\end{equation}\n% \\yy{I am wondering what is the difference of generating $K$ binary masks and generating one non-binary mask? Can we directly learn a mask where $m_d$ is a real value which means maintaining $m_d$ proportion of the original value? Will they have a similar performance or not?}\\yu{Great observations. My answer is first I think multiply each feature channel by the weight is feasible but it may also lead to over-fitting problem while if we use stochastic mask style, then it brings two benefits here: 1) we can avoid overfitting 2) we can increase the generalibility of the learned node representations since we create many mappings from stochastic represenations to the give label}\nthen learning the conditional distribution of the feature generator $\\mathbb{P}^{\\bm{\\Theta}_g}_{\\widetilde{\\mathbf{X}}}(\\cdot|\\mathbf{X})$ is transformed to learning a sampling distribution of the masker $\\mathbb{P}^{\\bm{\\Theta}_g}_{\\mathbf{m}}$. We assume the probability of masking each feature channel independently follows a Bernoulli distribution, i.e., $m_i \\sim \\text{Bernoulli}(1 - p_i), \\forall i\\in \\{1, 2, ..., d\\}$ with each feature channel $i$ being masked with the learnable probability $p_i\\in\\mathbb{R}$. In this way, we can learn which feature channels should be masked to achieve less discrimination through gradient-based techniques.\n%old version\n% In this way, we can automatically \\yd{the word automatically here might confuse the audience. Maybe you can say we can learn such mask through gradient-based techniques / end-to-end training / ...} learn the most important features to mask in \\yd{why in?} alleviating discrimination. \nSince the generator $g_{\\bm{\\Theta}_g}$ aims to obfuscate the discriminator $d_{\\bm{\\Theta}_d}$ that predicts the sensitive features based on the already-propagated node representations $\\widetilde{\\mathbf{H}}$ from the encoder $f_{\\bm{\\Theta}_f}$, the generated fair feature view $\\widetilde{\\mathbf{X}}$ would consider the effect of correlation variation by feature propagation rather than blindly follow the order of the sensitive correlations computed by the original features $\\mathbf{X}$. Generating fair feature view $\\widetilde{\\mathbf{X}}$ and forwarding it through the encoder $f_{\\bm{\\Theta}_f}$ and the classifier $c_{\\bm{\\Theta}_c}$ to make predictions involve sampling masks $\\mathbf{m}$ from the categorical Bernoulli distribution, the whole process of which is non-differentiable\ndue to the discreteness of masks. Therefore, we apply Gumbel-Softmax trick~\\cite{jang2016categorical} to approximate the categorical Bernoulli distribution. Assuming for each channel $i$, we have a learnable sampling score $\\boldsymbol{\\pi}_i = [\\pi_{i1}, \\pi_{i2}]$ with $\\pi_{i1}$ score keeping while $\\pi_{i2}$ score masking the channel $i$. Then the categorical distribution $\\text{Bernoulli}(1 - p_i)$ is softened by\\footnote{We use $p_{i1}$ instead of $p_i$ thereafter to represent the probability of keeping channel $i$.}:\n\\begin{align}\n    p_{ij} = \\frac{\\exp(\\frac{\\log(\\pi_{ij}) + g_{ij}}{\\tau})}{\\sum_{k= 1}^2\\exp(\\frac{\\log(\\pi_{ik}) + g_{ik}}{\\tau})}, \\forall j = 1, 2, i\\in\\{1, 2, ..., d\\},\n\\end{align}\n\\noindent where $g_{ij}\\sim \\text{Gumbel}(0, 1)$ and $\\tau$ is the temperature factor controlling the sharpness of the Gumbel-Softmax distribution. Then, to generate $\\widetilde{\\mathbf{X}}$ after we sample masks $\\mathbf{m}$ based on probability $p_{i1}$, we could either directly multiply feature channel $\\mathbf{X}_{:i}$ by the probability $p_{i1}$ or solely append the gradient of $p_{i1}$ to the sampled hard mask\\footnote{$\\mathbf{m} =\\mathbf{m} - p_{i1}.\\text{detach()} + p_{i1}$}, both of which are differentiable and can be trained end to end. After we approximate the generator $g_{\\bm{\\Theta}_g}$ via Gumbel-Softmax, we next model the GNN-based encoder $f_{\\bm{\\Theta}_f}$ to capture the information of both node features $\\mathbf{X}$ and network topology $\\mathbf{A}$. \n% \\vspace{-1ex}\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "GNN-based Encoder",
                        "content": "\\label{sec-e}\nIn order to learn from both the graph topology and node features, we employ $L-$layer GNNs as our encoder-backbone to obtain node representations $\\mathbf{H}^{L}$. Different graph convolutions adopt different propagation mechanisms,\n%which also capture different local interactions and hence impose  \nresulting in different variations on feature correlations.\n%old version\n% Different graph convolutions adopt and configure different functions for AGG and TRAN \\yd{this might be confusing. Do you mean different convolution mechanisms and feature transformation mechanisms are adopted for AGG and TRAN, respectively?} \\yu{yes, different graph convolutions using different propagation mechanism and decide how to make linear transformations like SAGE,they differentiate the contribution of center nodes and neighborhoods.}, which also capture different local interactions and hence impose different variations on feature correlations. \nHere we select GCN~\\cite{GCN}, GraphSAGE~\\cite{Graphsage}, and GIN~\\cite{GIN} as our encoder-backbones. In order to consider the variation induced by the propagation of GNN-based encoders, we apply the discriminator $d_{\\bm{\\Theta}_d}$ and classifier $c_{\\bm{\\Theta}_c}$ on top of the obtained node representations $\\mathbf{H}^L$ from the GNN-based encoders. Since both of the classifier and the discriminator are to make predictions, one towards sensitive groups and the other towards class labels, their model architectures are similar and therefore we introduce them together next. %in the next.\n\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Classifier and Discriminator",
                        "content": "\\label{sec-cd}\nGiven node representations $\\mathbf{H}^{L}$ obtained from any $L-$layer GNN-based encoder $f_{\\bm{\\Theta}_f}$, the classifier $c_{\\bm{\\Theta}_c}$ and the discriminator $d_{\\bm{\\Theta}_d}$ predict node labels $\\hat{\\mathbf{Y}}$ and sensitive attributes $\\hat{\\mathbf{S}}$ as:\n\\begin{equation}\n\\small\n    \\hat{\\mathbf{Y}} = c_{\\bm{\\Theta}_c}(\\mathbf{H}^L) = \\sigma\\big(\\text{MLP}_c(\\mathbf{H}^L)\\big),~~~\\hat{\\mathbf{S}} = d_{\\bm{\\Theta}_d}(\\mathbf{H}^L) = \\sigma\\big(\\text{MLP}_d(\\mathbf{H}^L)\\big),\n\\end{equation}\n% \\vspace{-3.5ex}\nwhere we use two different multilayer perceptrons (MLPs): $\\mathbb{R}^{d^L}\\rightarrow \\mathbb{R}$ for the classifier and the discriminator, and $\\sigma$ is the sigmoid operation.\n%$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$. \nAfter introducing the fairness-aware view generator, the GNN-based encoder, the MLP-based classifier and discriminator, we collect them together and adversarially train them with the following objective function. \n% \\td{can we add ~1 sentence to this column (or extending an existing) to add these ~2 additional lines?}\n\\vspace{-1ex}\n"
                    },
                    "subsubsection 4.1.4": {
                        "name": "Adversarial Training",
                        "content": "\\label{sec-advtraining}\nOur goal is to learn fair views from the original graph that encode as much task-relevant information while discarding as much sensitive-relevant information as possible. Therefore, we aim to optimize the whole framework from both the fairness and model utility perspectives. According to statistical parity, to optimize the fairness metric, a fair feature view should guarantee equivalent predictions between sensitive groups:\n\\begin{equation}\\label{eq-minsp}\n    \\bm{\\Theta}_g^{*} =\\argmin_{\\bm{\\Theta}_g}\\Delta_{\\text{sp}} = \\argmin_{\\bm{\\Theta}_g}|P(\\hat{y} = 1| s = 0) - P(\\hat{y} = 1 | s = 1)|,\n\\end{equation}\nwhere $P(\\hat{y}|s)$ is the predicted distribution given the sensitive feature. Assuming $\\hat{y}$ and $s$ are conditionally independent given $\\widetilde{\\mathbf{H}}$~\\cite{kamishima2011fairness}, to solve the global minimum of Eq.~\\eqref{eq-minsp}, we leverage adversarial training\n% ~\\cite{goodfellow2014generative}\n% \\yd{a small problem here is that adversarial training usually cannot arrive at the global minimum point. Mostly Nash equilibrium.} \nand compute the loss of the discriminator and generator $\\mathcal{L}_d, \\mathcal{L}_g$ as:\n\n%maybe not consider to mention this\n% $\\widetilde{\\mathbf{H}}$ and $s$ are conditionally independent given $\\widetilde{\\mathbf{X}}$, and further $\\widetilde{\\mathbf{X}}$ and $s$ are conditionally independent given $\\mathbf{X}$, then $P(\\hat{y}|s) = \\int_{\\mathbf{X}}P(\\hat{y}|\\widetilde{\\mathbf{H}})P(\\widetilde{\\mathbf{H}}|\\widetilde{\\mathbf{X}})P(\\widetilde{\\mathbf{X}}|\\mathbf{X})P(\\mathbf{X}|s) d\\mathbf{X}$ where $P(\\mathbf{X}|s)$ is the conditional prior totally dependent on the given dataset while $P(\\hat{y}|\\widetilde{\\mathbf{H}}), P(\\widetilde{\\mathbf{H}}|\\widetilde{\\mathbf{X}})$, and $P(\\widetilde{\\mathbf{X}}|\\mathbf{X})$ are modeled respectively via the classifier $c_{\\boldsymbol{\\theta}_c}$, the encoder $f_{\\boldsymbol{\\theta}_f}$, and the generator $g_{\\boldsymbol{\\theta}_g}$. \n\\vspace{-3ex}\n\\begin{equation}\\label{eq-advd}\n\\small\n\\max_{\\bm{\\Theta}_d}\\mathcal{L}_{\\text{d}} = \\mathbb{E}_{\\widetilde{\\mathbf{X}}\\sim\\mathbb{P}^{\\bm{\\Theta}_g}_{(\\widetilde{\\mathbf{X}}|\\mathbf{X})}}\n\\mathbb{E}_{v_i\\sim\\mathcal{V}}\n\\bigg(\\mathbf{S}_i\\log\\big(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^L_i)) + (1 - \\mathbf{S}_i)\\log(1 - d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^L_i)\\big)\\bigg),\n\\end{equation}\n\n\\vspace{-3ex}\n\\begin{equation}\\label{eq-advg}\n\\small\n\\min_{\\bm{\\Theta}_g}\\mathcal{L}_{\\text{g}} =\\mathbb{E}_{\\widetilde{\\mathbf{X}}\\sim\\mathbb{P}^{\\bm{\\Theta}_g}_{(\\widetilde{\\mathbf{X}}|\\mathbf{X})}}\n\\mathbb{E}_{v_i\\sim\\mathcal{V}}\n\\big(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^L_i) - 0.5\\big)^2 + \\alpha||\\mathbf{m} - \\mathbf{1}_d||_2^2,\n\\end{equation}\nwhere $\\widetilde{\\mathbf{H}}^L_i = f_{\\bm{\\Theta}_f}(\\widetilde{\\mathbf{X}}_i, \\mathbf{A})$ and $||\\mathbf{m} - \\mathbf{1}_d||_2^2$ regularizes the mask to be dense, which avoids masking out sensitive-uncorrelated but task-critical information. $\\alpha$ is the hyperparamter. Intuitively, Eq.~\\eqref{eq-advd} encourages our discriminator to correctly predict the sensitive features of each node under each generated view and Eq.~\\eqref{eq-advg} requires our generator to generate fair feature views that enforce the well-trained discriminator to randomly guess the sensitive features.\n%Theorem~\\ref{thm-same} proves the equivalence between the global minimum achieved by leveraging such generative adversarial training defined by Eq.~\\eqref{eq-advd}-\\eqref{eq-advg} and the desired global minimum of Eq.~\\eqref{eq-minsp}:\nIn Theorem~\\ref{thm-same}, we show that the  global minimum of Eq.~\\eqref{eq-advd}-\\eqref{eq-advg} is equivalent to the global minimum of Eq.~\\eqref{eq-minsp}:\n\\vspace{-1ex}\n\\begin{thm}\\label{thm-same}\n\nGiven $\\widetilde{\\mathbf{h}}^L$ as the representation of a specific node learned by L layer GNN-based encoder $f_{\\bm{\\Theta}_g}$ and $\\alpha = 0$ in Eq.~\\eqref{eq-advg}, the global optimum of Eq.~\\eqref{eq-advd}-\\eqref{eq-advg} is equivalent to the one of Eq.~\\eqref{eq-minsp}.\n\\end{thm}\n\\vspace{-2ex}\n\n\\begin{proof}\n% For each specific generated feature view $\\widetilde{\\mathbf{X}}$, we have:\n% \\begin{align}\n%     \\ell_d & = \\mathbb{E}_{v_i\\sim\\mathcal{V}}\n% [\\mathbf{S}_i\\log(d_{\\boldsymbol{\\theta}_d}(\\widetilde{\\mathbf{H}}^L_i)) + (1 - \\mathbf{S}_i)\\log(1 - d_{\\boldsymbol{\\theta}_d}(\\widetilde{\\mathbf{H}}^L_i))]\n% \\nonumber\\\\&= \\mathbb{E}_{v_i\\sim\\mathcal{V}_{s = 1}}\\log(d_{\\boldsymbol{\\theta}_d}(\\widetilde{\\mathbf{h}}^L)) + \\mathbb{E}_{v_i\\sim\\mathcal{V}_{s = 0}}(1 - \\log(d_{\\boldsymbol{\\theta}_d}(\\widetilde{\\mathbf{h}}^L)))\n% \\nonumber\\\\& =\\int_{\\widetilde{\\mathbf{h}}^L}{P(\\widetilde{\\mathbf{h}}^L|s = 1)}{\\log(d_{\\boldsymbol{\\theta}_d}(\\widetilde{\\mathbf{h}}^L)) + P(\\widetilde{\\mathbf{h}}^L|s = 0)}(1 - \\log(d_{\\boldsymbol{\\theta}_d}(\\widetilde{\\mathbf{h}}^L)))d\\widetilde{\\mathbf{h}}^L\n% \\end{align}\n% \\yy{note on mentioning the meaning of $\\widetilde{\\mathbf{h}}^L$} \\td{seems this is coming from removing the past 2.3 on GNNs? I think this might happen elsewhere with notations...}\n\nBased on Proposition 1. in \\cite{goodfellow2014generative} and Proposition 4.1. in \\cite{dai2021say}, the optimal discriminator is $d_{\\boldsymbol{\\theta}_d^*}(\\widetilde{\\mathbf{h}}^L) = \\frac{P(\\widetilde{\\mathbf{h}}^L|s = 1)}{P(\\widetilde{\\mathbf{h}}^L|s = 1) + P(\\widetilde{\\mathbf{h}}^L|s = 0)}$, which is exactly the probability when discriminator randomly guesses the sensitive features. Then we further substituted it into Eq.~\\eqref{eq-advg} and the optimal generator is achieved when $d_{\\boldsymbol{\\theta}_d^*}(\\widetilde{\\mathbf{h}}^L) = 0.5, \\text{i.e.}, P(\\widetilde{\\mathbf{h}}^L|s = 1) = P(\\widetilde{\\mathbf{h}}^L|s = 0)$. Then we have:\n\\begin{align}\n    P(\\hat{y} = 1|s &= 1) = \\int_{\\widetilde{\\mathbf{h}}^L}P(\\hat{y} = 1|\\widetilde{\\mathbf{h}}^L)P(\\widetilde{\\mathbf{h}}^L|s = 1)d\\widetilde{\\mathbf{h}}^L\n    \\nonumber\\\\& = \\int_{\\widetilde{\\mathbf{h}}^L}P(\\hat{y} = 1|\\widetilde{\\mathbf{h}}^L)P(\\widetilde{\\mathbf{h}}^L|s = 0)d\\widetilde{\\mathbf{h}}^L = P(\\hat{y} = 1|s = 0), \\nonumber\n\\end{align}\nwhich is obviously the global minimum of Eq.~\\eqref{eq-minsp}.\n\\end{proof}\n\\vspace{-2ex}\nNote that node representations $\\widetilde{\\mathbf{H}}^L$ have already been propagated in GNN-based encoder $f_{\\boldsymbol{\\theta}_f}$ and therefore, the optimal discriminator $d_{\\boldsymbol{\\theta}_d^*}$  could identify sensitive-related features after correlation variation. Besides the adversarial training loss to ensure the fairness of the generated view, the classification loss for training the classifier $c_{\\boldsymbol{\\theta}_c}$ is used to guarantee the model utility:\n\\vspace{-2ex}\n\\begin{equation}\n\\small\n\\min_{\\boldsymbol{\\theta}_c}\\mathcal{L}_{\\text{c}} = -\\mathbb{E}_{\\widetilde{\\mathbf{X}}\\sim\\mathbb{P}^{\\boldsymbol{\\theta}_g}_{(\\widetilde{\\mathbf{X}}|\\mathbf{X})}}\n\\mathbb{E}_{v_i\\sim\\mathcal{V}}\n\\bigg(\\mathbf{Y}_i\\log\\big(c_{\\boldsymbol{\\theta}_c}(\\widetilde{\\mathbf{H}}^L_i)\\big) + (1 - \\mathbf{Y}_i)\\log\\big(1 - c_{\\boldsymbol{\\theta}_c}(\\widetilde{\\mathbf{H}}^L_i)\\big)\\bigg)\n\\end{equation}\n\n% The detailed proof is given in Appendix~\\ref{proof}. Note that node representations $\\widetilde{\\mathbf{H}}^L$ have already been propagated in GNN-based encoder $f_{\\bm{\\Theta}_f}$ and therefore, the optimal discriminator $d_{\\bm{\\Theta}_d^*}$  could identify sensitive-related features after correlation variation. Besides the adversarial training loss to ensure the fairness of the generated view, the classification loss for training the classifier $c_{\\bm{\\Theta}_c}$ is used to guarantee the model utility:\n% \\begin{equation}\n% \\small\n% \\min_{\\bm{\\Theta}_c}\\mathcal{L}_{\\text{c}} = -\\mathbb{E}_{\\widetilde{\\mathbf{X}}\\sim\\mathbb{P}^{\\bm{\\Theta}_g}_{(\\widetilde{\\mathbf{X}}|\\mathbf{X})}}\n% \\mathbb{E}_{v_i\\sim\\mathcal{V}}\n% [\\mathbf{Y}_i\\log(c_{\\bm{\\Theta}_c}(\\widetilde{\\mathbf{H}}^L_i)) + (1 - \\mathbf{Y}_i)\\log(1 - c_{\\bm{\\Theta}_c}(\\widetilde{\\mathbf{H}}^L_i))]\n% \\end{equation}\n\n\n\n\\vspace{-2ex}\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Adaptive Weight Clamping",
                    "content": "\\label{sec-wc}\nAlthough the generator is theoretically guaranteed to achieve its global minimum by applying adversarial training,\n% \\yd{a small problem here too: mostly Nash equilibrium.}\nin practice the generated views may still encode sensitive information and the corresponding classifier may still make discriminatory decisions. This is because of the unstability of the training process of adversarial learning~\\cite{goodfellow2014generative} and the entanglement with training classifier.\n\nTo alleviate the above issue, we propose to adaptively clamp weights of the encoder $f_{\\bm{\\Theta}_f}$ based on the learned masking probability distribution from the generator $g_{\\bm{\\Theta}_g}$. After adversarial training, only the sensitive and its highly-correlated features would have higher probability to be masked and therefore, declining their contributions in $\\widetilde{\\mathbf{H}}^L$ by clamping their corresponding weights in the encoder would discourage the encoder from capturing these features and hence alleviate the discrimination. Concretely, within each training epoch after the adversarial training, we compute the probability of keeping features $\\mathbf{p}\\in\\mathbb{R}^{d}$ by sampling $K$ masks and calculate their mean $\\mathbf{p} = \\sum_{k = 1}^K\\mathbf{m}^k$. Then assuming the weights of the first layer in the encoder $f_{\\bm{\\Theta}_f}$ is $\\mathbf{W}^{f, 1}\\in\\mathbb{R}^{d_1\\times d}$, we clamp it by:\n\\begin{equation}\\label{eq-wc}\n    \\mathbf{W}^{f, 1}_{ij} = \\begin{cases}\n        \\mathbf{W}_{ij}^{f, 1}, & |\\mathbf{W}_{ij}^{f, 1}| \\le \\epsilon*\\mathbf{p}_j\\\\\n        \\text{sign}(\\mathbf{W}_{ij}^{f, 1}) * \\epsilon*\\mathbf{p}_j, & |\\mathbf{W}_{ij}^{f, 1}| > \\epsilon*\\mathbf{p}_j\n    \\end{cases},\n\\end{equation}\nwhere $\\epsilon\\in\\mathbb{R}$ is a prefix cutting threshold selected by hyperparameter tuning and $\\text{sign}:\\mathbb {R} \\to \\{-1,0,1\\}$ takes the sign of $\\mathbf{W}_{ij}^{f, 1}$. Intuitively, feature channels masked with higher probability (remained with lower probability $\\mathbf{p}_j$) would have lower threshold in weight clamping and hence their contributions to the representations $\\widetilde{\\mathbf{H}}^{L}$ are weakened.\n% \\yd{Will the reviewers be interested in how to choose the threshold?}\\yu{hyperparamter tuning and we have a sensitive analysis in figure 4.}\n% This is reasonable since by adversarial training, only the sensitive and its highly-correlated features would be masked more and therefore, declining their contributions in $\\widetilde{\\mathbf{H}}^L$ would discourage the encoder from capturing these features and hence alleviate the discrimination. \nNext, we theoretically rationalize this adaptive weight clamping by demonstrating its equivalence to minimizing the upper bound of the difference of representations between two sensitive groups:\n\\vspace{-2ex}\n\\begin{thm}\\label{thm-cffairness}\nGiven a 1-layer GNN encoder $f_{\\boldsymbol{\\theta}_f}$ with row-normalized adjacency matrix $\\mathbf{D}^{-1}\\mathbf{A}$ as the PROP and weight matrix $\\mathbf{W}^{f, 1}$ as TRAN and further assume that features of nodes from two sensitive groups in the network independently and identically follow two different Gaussian distributions, i.e., $\\mathbf{X}^{s_1}\\sim\\mathcal{N}(\\boldsymbol{\\mu}^{s_1}, \\boldsymbol{\\Sigma}^{s_1}), \\mathbf{X}^{s_2}\\sim\\mathcal{N}(\\boldsymbol{\\mu}^{s_2}, \\boldsymbol{\\Sigma}^{s_2})$, then the difference of representations $\\mathbf{H}^{s_1} - \\mathbf{H}^{s_2}$ also follows a Gaussian with the 2-norm of its mean $\\boldsymbol\\mu$ as:\n\\vspace{-1.5ex}\n\\begin{equation}\\label{eq-bound}\n\\footnotesize\n\\begin{split}\n    &||\\boldsymbol{\\mu}||_2 = ||(2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}||_2\n    \\le (2\\chi - 1)\\big(\\sum_{i = 1}^{d_1}(\\sum_{r\\in\\mathcal{S}}\\epsilon\\mathbf{p}_r\\Delta\\boldsymbol{\\mu}_r + \\sum_{k\\in\\mathcal{NS}}\\epsilon\\mathbf{p}_k\\Delta\\boldsymbol{\\mu}_k)^2\\big)^{0.5}\n\\end{split}\n\\end{equation}\n\\end{thm}\n\\vspace{-2ex}\n\\noindent where $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}^{s_1} - \\boldsymbol{\\mu}^{s_2}\\in\\mathbb{R}^{d}$ and $\\mathcal{S}, \\mathcal{NS}$ denote the sensitive and non-sensitive features, and $\\chi$ is the network homophily. \n\n\n\\begin{proof}\nSubstituting the row-normalized adjacency matrix $\\mathbf{D}^{-1}(\\mathbf{A}+\\mathbf{I})$, we have $f_{\\boldsymbol{\\theta}_f}(\\mathbf{X}) = \\mathbf{W}^{f, 1}\\mathbf{D}^{-1}(\\mathbf{A} + \\mathbf{I})\\mathbf{X}$, for any pair of nodes coming from two different sensitive groups $v_i\\in\\mathcal{V}_0, v_j\\in\\mathcal{V}_1$, we have:\n\\begin{equation}\\label{eq-dist0}\n% \\small\n\\begin{split}\n    &f_{\\boldsymbol{\\theta}_f}(\\mathbf{X}_i) - f_{\\boldsymbol{\\theta}_f}(\\mathbf{X}_j)\n    = \\mathbf{W}^{f, 1}\\big(\\mathbf{D}^{-1}(\\mathbf{A} + \\mathbf{I})\\mathbf{X}\\big)_i - \\mathbf{W}^{f, 1}\\big(\\mathbf{D}^{-1}(\\mathbf{A} + \\mathbf{I})\\mathbf{X}\\big)_j\n    \\\\=& \\mathbf{W}^{f, 1}(\\frac{1}{d_i + 1}\\sum_{v_p\\in\\mathcal{N}_i\\cup v_i}{\\mathbf{X}_p} - \\frac{1}{d_j + 1}\\sum_{v_q\\in\\mathcal{N}_j\\cup v_j}{\\mathbf{X}_q}),\n\\end{split}\n\\end{equation}\nif the network homophily is $\\chi$ and further assuming that neighboring nodes strictly obey the network homophily, i.e., among $|\\mathcal{N}_i\\cup v_i| = d_i + 1$ neighboring nodes of the center node $v_i$, $\\chi(d_i + 1)$ of them come from the same feature distribution as $v_i$ while $(1 - \\chi)(d_i + 1)$ of them come from the other feature distribution as $v_j$, then symmetrically we have:\n\\begin{equation}\n\\small\n    \\frac{1}{d_i + 1}\\sum_{v_p\\in\\mathcal{N}_i\\cup v_i}{\\mathbf{X}_p}\\sim\\mathcal{N}\\big(\\chi\\boldsymbol{\\mu}^{s_1} + (1 - \\chi)\\boldsymbol{\\mu}^{s_2}, (d_i + 1)^{-1}(\\chi \\boldsymbol{\\Sigma}^{s_1} + (1 - \\chi)\\boldsymbol{\\Sigma}^{s_2})\\big), \\nonumber\n\\end{equation}\n% \\vspace{-3ex}\n\\begin{equation}\\label{eq-dist2}\n\\small\n    \\frac{1}{d_j + 1}\\sum_{v_q\\in\\mathcal{N}_j\\cup v_j}{\\mathbf{X}_q}\\sim\\mathcal{N}\\big(\\chi\\boldsymbol{\\mu}^{s_2} + (1 - \\chi)\\boldsymbol{\\mu}^{s_1}, (d_j + 1)^{-1}(\\chi \\boldsymbol{\\Sigma}^{s_2} + (1 - \\chi)\\boldsymbol{\\Sigma}^{s_1})\\big).\n\\end{equation}\n\n% \\td{need to remove the appendix as it's causing eq numbering problems due to being redefined below}\nCombining Eq.~\\eqref{eq-dist2} and Eq.~\\eqref{eq-dist0}, the distribution of their difference would also be a Gaussian $f_{\\boldsymbol{\\theta}_f}(\\mathbf{X}_i) - f_{\\boldsymbol{\\theta}_f}(\\mathbf{X}_j)\\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, where:\n\\begin{equation}\n\\small\n    \\boldsymbol{\\mu} = \\mathbf{W}^{f, 1}\\big(\\chi\\boldsymbol{\\mu}^{s_1} + (1 - \\chi)\\boldsymbol{\\mu}^{s_2} - \\chi\\boldsymbol{\\mu}^{s_2} - (1-\\chi)\\boldsymbol{\\mu}^{s_1}\\big) = (2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}\n\\end{equation}\n\\begin{equation}\n\\small\n    \\boldsymbol{\\Sigma} = \\mathbf{W}^{f, 1}\\big((d_i + 1)^{-1}(\\chi \\boldsymbol{\\Sigma}^{s_1} + (1 - \\chi)\\boldsymbol{\\Sigma}^{s_2}) + (d_j + 1)^{-1}(\\chi \\boldsymbol{\\Sigma}^{s_2} + (1 - \\chi)\\boldsymbol{\\Sigma}^{s_1})\\big){\\mathbf{W}^{f, 1}}^{\\top}\n\\end{equation}\n\n\n% \\\\=& \\mathbb{E}_{v_i\\in\\mathcal{V}_1, v_j\\in\\mathcal{V}_0}(\\mathbf{W}^{f, 1}(\\frac{1}{d_i + 1}\\sum_{v_p\\in\\mathcal{N}_i\\cup v_i}{\\mathbf{X}_p} - \\frac{1}{d_j + 1}\\sum_{v_q\\in\\mathcal{N}_j\\cup v_j}{\\mathbf{X}_q}))\n    % \\\\=& \\mathbf{W}^{f, 1}\\mathbb{E}_{v_i\\in\\mathcal{V}_1, v_j\\in\\mathcal{V}_0}(\\frac{1}{d_i + 1}\\sum_{v_p\\in\\mathcal{N}_i\\cup v_i}{\\mathbf{X}_p} - \\frac{1}{d_j + 1}\\sum_{v_q\\in\\mathcal{N}_j\\cup v_j}{\\mathbf{X}_q}) \n    % \\\\=& \\mathbf{W}^{f, 1}(\\chi\\boldsymbol{\\mu}_1 + (1 - \\chi)\\boldsymbol{\\mu}_2 - (\\chi\\boldsymbol{\\mu}_2 + (1 - \\chi)\\boldsymbol{\\mu}_1)) = (2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}\nTaking the $2-$norm on the mean $\\boldsymbol{\\mu}$, splitting channels into sensitive ones $\\mathcal{S}$ and non-sensitive ones $\\mathcal{NS}$, i.e., $\\{1, 2, ..., d\\}=\\mathcal{S}\\cup\\mathcal{NS}$ and expanding $\\boldsymbol{\\mu}$ based on the input channel, we have:\n\\begin{equation}\\label{eq-dist3}\n\\small\n||(2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}||_2 = (2\\chi - 1)\\big(\\sum_{i = 1}^{d_1}(\\sum_{r\\in\\mathcal{S}}\\mathbf{W}^{f, 1}_{ir}\\Delta\\boldsymbol{\\mu}_r + \\sum_{k\\in\\mathcal{NS}}\\mathbf{W}^{f, 1}_{ik}\\Delta\\boldsymbol{\\mu}_k)^2\\big)^{0.5},\n\\end{equation}\nwhere $\\mathbf{W}^{f, 1}_{ir}, \\mathbf{W}^{f, 1}_{ik}$ represent the weights of the encoder from feature channel $r(k)$ to the hidden neuron $i$. Since we know that $|\\mathbf{W}^{f, 1}_{ir}| \\le \\epsilon\\mathbf{p}_r, |\\mathbf{W}^{f, 1}_{ik}| \\le \\epsilon\\mathbf{p}_k, \\forall r\\in\\mathcal{S}, k\\in\\mathcal{NS}$, we substitute the upper bound here into Eq.~\\eqref{eq-dist3} and finally end up with:\n% \\td{it's better to use $\\Bigg( \\bigg(, \\Big(, \\big(, (, etc.$ for nested parentheses as compared to $( ( ( etc.$}\n\\begin{equation}\n\\footnotesize\n\\begin{split}\n%\\resizebox{.95\\hsize}{!}{\n    &||\\boldsymbol{\\mu}||_2 = ||(2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}||_2\n    \\le (2\\chi - 1)\\big(\\sum_{i = 1}^{d_1}(\\sum_{r\\in\\mathcal{S}}\\epsilon\\mathbf{p}_r\\Delta\\boldsymbol{\\mu}_r + \\sum_{k\\in\\mathcal{NS}}\\epsilon\\mathbf{p}_k\\Delta\\boldsymbol{\\mu}_k)^2\\big)^{0.5}.\n    \\nonumber\n%}\n\\end{split}\n\\end{equation}\n\\vskip -2ex\n\\end{proof}\n\\vspace{-1ex}\n% \\td{if this eq is not later referenced I think it's better to just remove the eq number since it indeed looks strange to have it on separate line for the number then qed box, alternatively, expanding this where we put after the less than equal on a second under the equal}\n\nThe left side of Eq.~\\eqref{eq-bound} is the difference of representations between two sensitive groups and if it is large, i.e., $||\\boldsymbol{\\mu}||_2$ is very large, then the predictions between these two groups would also be very different, which reflects more discrimination in terms of the group fairness. Additionally, Theorem~\\ref{thm-cffairness} indicates that the upper bound of the group fairness between two sensitive groups depends on the network homophily $\\chi$, the initial feature difference $\\Delta\\boldsymbol{\\mu}$ and the masking probability $\\mathbf{p}$. As the network homophily $\\chi$ decreases, more neighboring nodes come from the other sensitive group and aggregating information of these neighborhoods would smooth node representations between different sensitive groups and reduce the bias. To the best of our knowledge, this is the first work relating the fairness with the network homophily. Furthermore, Eq.~\\eqref{eq-bound} proves that clamping weights of the encoder $\\mathbf{W}^{f, 1}$ upper bounds the group fairness.\n\n\n\n%\\vspace{-1ex}\n"
                },
                "subsection 4.3": {
                    "name": "Training Algorithm",
                    "content": "\nHere we present a holistic algorithm of the proposed FairVGNN. In comparison to vanilla adversarial training, additional computational requirements of FairVGNN come from generating $K$ different masks. However, since within each training epoch we can pre-compute the masks as Step 4 before adversarial training and the total number of views $K$ becomes constant compared with the whole time used for adversarial training as Step 6-14, the time complexity is still linear proportional to the size of the whole graph, i.e., $O(|\\mathcal{V}| + |\\mathcal{E}|)$. The total model complexity includes parameters of the feature masker $O(2d)$, the discriminator/classifier $O(2d^L)$ and the encoder $O(d\\prod_{l = 1}^L{d^l})$, which boils down to $O(\\max_{i\\in\\{0, 1, ..., L\\}}(d^i)^L)$ and hence the same as any other $L$-layer GNN backbones.\n\n\n\n% During each training epoch, we randomly generate $K$ different feature views $\\widetilde{\\mathbf{X}}^k$ from generator and forward them through the encoder to obtain node representations. Then we sequentially train the discriminator, classifier, and generator. After that, we adaptively clamp weights of the encoder.\n\\vspace{-1ex}\n\\setlength{\\textfloatsep}{4pt}\n\\begin{algorithm}[tbp!]\n \\DontPrintSemicolon\n \\footnotesize\n \\KwIn{an attributed graph $G = (\\mathcal{V}, \\mathcal{E}, \\mathbf{X}, \\mathbf{A}, \\mathbf{Y})$, Classifier $c_{\\bm{\\Theta}_c}$, Encoder $f_{\\bm{\\Theta}_f}$, Generator $g_{\\bm{\\Theta}_g}$, Discriminator $d_{\\bm{\\Theta}_d}$, $K$}\n \n \\KwOut{Learned fairness attribute $\\widetilde{\\mathbf{X}}$ and Predictions $\\hat{\\mathbf{Y}}$}\n\n%  Initialize model parameters $\\bm{\\Theta}_f, \\bm{\\Theta}_g, \\bm{\\Theta}_d, \\bm{\\Theta}_c$;\n \n \\While{not converged}{\n    $\\boldsymbol{\\pi} \\leftarrow \\mathbf{W}^{g_{\\bm{\\Theta}_g}}$\n    \n    \\For{$k\\leftarrow 1$ \\KwTo $K$}\n    {\n        $\\mathbf{m}^k \\sim \\text{Gumbel-softmax}(\\boldsymbol{\\pi})$ , $\\widetilde{\\mathbf{X}}^k \\leftarrow \\mathbf{X}\\odot\\mathbf{m}^k$, \\tcp*{Section~\\ref{sec-g}}\n        \n        \n        \n        $\\widetilde{\\mathbf{H}}_i^{L, k} \\leftarrow f_{\\bm{\\Theta}_f}(\\widetilde{\\mathbf{X}}^k, \\mathbf{A})$,  $\\widehat{\\mathbf{H}}_i^{L, k} \\leftarrow sg(\\widetilde{\\mathbf{H}}_i^{L, k})$    \\tcp*{Section~\\ref{sec-e}\\footnotemark}\n        \n    }\n    \n    \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_d$}\n    {\n        $\\mathcal{L}_{\\text{d}} \\leftarrow \\sum\\limits_{k = 1}^{K}\\sum\\limits_{v_i \\in \\mathcal{V}}[\\mathbf{S}_i\\log(d_{\\bm{\\Theta}_d}(\\widehat{\\mathbf{H}}^{L, k}_i)) + (1 - \\mathbf{S}_i)\\log(1 - d_{\\bm{\\Theta}_d}(\\widehat{\\mathbf{H}}^{L, k}_i))]$\n        \n        $\\bm{\\Theta_d} \\leftarrow \\bm{\\Theta_d}+\n        \\nabla_{\\bm{\\Theta}_d} \\mathcal{L}_d$, $\\bm{\\Theta_f} \\leftarrow \\bm{\\Theta_f}+\n        \\nabla_{\\bm{\\Theta}_f}\\mathcal{L}_f$ \\tcp*{Section~\\ref{sec-cd}}\n    }\n    \n    \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_c$}\n    {\n        $\\mathcal{L}_{\\text{c}} \\leftarrow \\sum\\limits_{k=1}^K\\sum\\limits_{v_i\\in\\mathcal{V}}[\\mathbf{Y}_i\\log(c_{\\bm{\\Theta}_c}(\\widetilde{\\mathbf{H}}^{L, k}_i)) + (1 - \\mathbf{Y}_i)\\log(1 - c_{\\bm{\\Theta}_c}(\\widetilde{\\mathbf{H}}^{L, k}_i))]$\n        \n        $\\bm{\\Theta_c} \\leftarrow \\bm{\\Theta_c}-\n        \\nabla_{\\bm{\\Theta}_c}\\mathcal{L}_c$, $\\bm{\\Theta_f} \\leftarrow \\bm{\\Theta_f}-\n        \\nabla_{\\bm{\\Theta}_f}\\mathcal{L}_f$ \\tcp*{Section~\\ref{sec-cd}}\n    }\n    \n    \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_g$}\n    {\n        $\\mathcal{L}_{\\text{g}}^k \\leftarrow \\sum\\limits_{k=1}^K\\sum\\limits_{v_i\\in\\mathcal{V}}||d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^{L, k}_i) - 0.5||_2^2,$\n    \n        $\\bm{\\Theta_g} \\leftarrow \\bm{\\Theta_g}-\n        \\nabla_{\\bm{\\Theta}_g}\\mathcal{L}_g$, $\\bm{\\Theta_f} \\leftarrow \\bm{\\Theta_f}-\n        \\nabla_{\\bm{\\Theta}_f}\\mathcal{L}_f$ \\tcp*{Section~\\ref{sec-advtraining}}\n    }\n    \n    $\\bm{\\Theta_f} \\leftarrow \\text{Clamp}(\\bm{\\Theta_f}, \\sum\\limits_{k = 1}^K\\mathbf{m}^k)$ \\tcp*{Section~\\ref{sec-wc}}\n    \n}\n\n$\\widetilde{\\mathbf{X}} = \\sum\\limits_{k=1}^K\\widetilde{\\mathbf{X}}^k, ~~\\hat{\\mathbf{Y}} = c_{\\bm{\\Theta}_c}(f_{\\bm{\\Theta}_f}(\\widetilde{\\mathbf{X}}, \\mathbf{A}))$~~~~\n\n\\KwRet{$\\widetilde{\\mathbf{X}}, \\widehat{\\mathbf{Y}}$}\n\n\n\n\\caption{\\small The algorithm of FairVGNN}\n\\label{alg-fairvgnn}\n\\end{algorithm}\n\\footnotetext{$sg$: stopgrad prevents gradients from being\nback-propagated.}\n\n\n\n\n\n\n% Full details about the algorithm and its complexity analysis is presented in Appendix~\\ref{app-algorithm}.\\yu{check}\n% %\\vspace{-2ex}\n% \\SetAlgoNoLine\n% \\SetAlgoNoEnd\n% \\begin{algorithm}[htbp!]\n%  \\DontPrintSemicolon\n%  \\footnotesize\n%  \\KwIn{an attributed graph $G = (\\mathcal{V}, \\mathcal{E}, \\mathbf{X}, \\mathbf{A})$ with node labels $\\mathbf{Y}$, Classifier $c_{\\bm{\\Theta}_c}$, Encoder $f_{\\bm{\\Theta}_f}$, Generator $g_{\\bm{\\Theta}_g}$, Discriminator $d_{\\bm{\\Theta}_d}$, $K$}\n \n%  \\While{not converged}{\n%     $\\boldsymbol{\\pi} \\leftarrow \\mathbf{W}^{g_{\\bm{\\Theta}_g}}$\n    \n%     Sample $K$ different feature views $\\widetilde{\\mathbf{X}}^k$ from generator\\tcp*{Section~\\ref{sec-g}}\n%     Obtain node representations $\\widetilde{\\mathbf{H}}_i^{L, k}$ under each view \\tcp*{Section~\\ref{sec-e}}\n    \n%     \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_d$}\n%     {\n%         Train the discriminator $g_{\\bm{\\Theta}_g}$ and encoder $f_{\\bm{\\Theta}_f}$\\tcp*{Section~\\ref{sec-cd}}\n%     }\n    \n%     \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_c$}\n%     {\n%         Train the classifier $c_{\\bm{\\Theta}_c}$ and encoder $f_{\\bm{\\Theta}_f}$\\tcp*{Section~\\ref{sec-cd}}\n%     }\n    \n%     \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_g$}\n%     {\n%         Train the generator $g_{\\bm{\\Theta}_g}$ and encoder $f_{\\bm{\\Theta}_f}$\\tcp*{Section~\\ref{sec-advtraining}} \n%     }\n    \n%     Adaptively clamp weights of $\\bm{\\Theta}_f$ \\tcp*{Section~\\ref{sec-wc}\\hspace{2.38ex}}\n    \n% }\n\n% $\\widetilde{\\mathbf{X}} = \\sum\\limits_{k=1}^K\\widetilde{\\mathbf{X}}^k, \\hat{\\mathbf{Y}} = c_{\\bm{\\Theta}_c}(f_{\\bm{\\Theta}_f}(\\widetilde{\\mathbf{X}}, \\mathbf{A}))$\n\n% \\KwRet{$\\widetilde{\\mathbf{X}}, \\widehat{\\mathbf{Y}}$}\n\n% \\caption{\\small The algorithm of FairVGNN}\n% \\label{alg-fairvgnn}\n% \\end{algorithm}\n\n% \\vspace{-2ex}\n% \\begin{table}[htbp!]\n% \\vskip -2ex\n% \\footnotesize\n% \\setlength{\\extrarowheight}{.095pt}\n% \\setlength\\tabcolsep{3pt}\n% \\caption{Basic dataset statistics.}\n% \\centering\n% \\vspace{-4ex}\n% \\begin{tabular}{lccc}\n%  \\Xhline{2\\arrayrulewidth}\n% \\textbf{Dataset} & \\textbf{German Credit} & \\textbf{Recidivism} & \\textbf{Credit defaulter}\\\\\n%  \\Xhline{1.5\\arrayrulewidth}\n% \\#Nodes & 1000 & 18,876 & 30,000 \\\\\n% \\#Edges & 22,242 & 321,308 & 1,436,858\\\\\n% \\#Features & 27 & 18 & 13\\\\\n% Sens. & Gender& Race & Age\\\\\n% Label & Good/bad Credit & Bail/no bail & Default/no default Payment\\\\\n\n% \\Xhline{2\\arrayrulewidth}\n% \\end{tabular}\n% \\label{tab-dataset}\n% \\vskip -1.5ex\n% \\end{table}%\\vspace{-2ex}\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\\label{sec-experiment}\nIn this section, we conduct extensive experiments to evaluate the effectiveness of FairVGNN.\n%\\vspace{-1ex}\n",
                "subsection 5.1": {
                    "name": "Experimental Settings",
                    "content": "\\label{sec-experimentsetting}\n\n%\\vspace{-0.5ex}\n",
                    "subsubsection 5.1.1": {
                        "name": "Datasets",
                        "content": "\\label{sec-dataset}\nWe validate the proposed approach on three benchmark datasets~\\cite{nifty, EDITS} with their statistics shown in Table~\\ref{tab-dataset}.\n% with detailed description given in Appendix~\\ref{app-dataset}.\n% \\begin{inparaenum}\n%     \\item \\textbf{German Credit}: node represents clients and edges are formed between clients with similar credit accounts. The task is to classify the credit risk of the clients as high or low with `gender' being the sensitive feature.\n%     \\item \\textbf{Recidivism}: nodes represents defendants released on bail during 1990-2009, and edges are formed between defendants of similar past criminal records and demographs.\n%     \\item \\textbf{Credit Defaulter}: nodes represents credit card users, and edges are formed between users if their share similar pattern in purchases and payments. The task is to predict whether a user will default on credit card payment with `age' being the sensitive feature.\n% \\end{inparaenum} \n\n\n\n\n\n\n% \\vspace{0ex}\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Baselines",
                        "content": "\n%We compare our proposed FairVGNN against serveral state-of-the-art models, which can be divided into two categories:\nSeveral state-of-the-art fair node representation learning models are compared with our proposed FairVGNN. We divide them into %the following \ntwo categories:\n\\begin{inparaenum}\n    \\item \\textbf{Augmentation-based}: this type of methods alleviates discrimination via graph augmentation, where sensitive-related information is removed by modifying the graph topology or node features. NIFTY~\\cite{nifty} simultaneously achieves the Counterfactual Fairness and the stability by contrastive learning. EDITS~\\cite{EDITS} approximates the inputs' discrimination via Wasserstein distance and directly minimizes it between sensitive and non-sensitive groups by pruning the graph topology and node features.\n    \\item \\textbf{Adversarial-based}:  The adversarial-based methods enforce the fairness of node representations by alternatively training the encoder to fool the discriminator and the discriminator to predict the sensitive attributes. FairGNN~\\cite{dai2021say} deploys an extra sensitive feature estimator to increase the amount of sensitive information\n    % , which would greatly benefit the adversarial debiasing.\n    % C-ENC~\\cite{bose2019compositional} learns a set of adversarial filters that remove information about different combinations of sensitive attributes. \n\\end{inparaenum}\nSince different GNN-backbones may cause different levels of sensitive attribute leakage, %due to different propagation mechanisms,\nwe consider to equip each of the above three bias-alleviating methods with three GNN-backbones: GCN~\\cite{GCN}, GIN~\\cite{GIN}, GraphSAGE~\\cite{Graphsage}, e.g., GCN-NIFTY represents the GCN encoder with NIFTY. \n% and for all three GNN-backbones, we only use 1-layer graph convolution to aggregate neighborhood information followed by an linear layer to make predictions. \n% The detailed model architectures of the GCN, GIN, and GraphSAGE encoders are introduced in Appendix~\\ref{app-architecture}.\n\\vspace{-1.5ex}\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Setup",
                        "content": "\nOur proposed FairVGNN is implemented using PyTorch-Geometric~\\cite{pytorch}. For EDITS\\footnote{\\url{https://github.com/yushundong/edits}},\n% C-ENC\\footnote{\\url{https://github.com/joeybose/Flexible-Fairness-Constraints}},\nNIFTY\\footnote{\\url{https://github.com/chirag126/nifty}} and FairGNN\\footnote{\\url{https://github.com/EnyanDai/FairGNN}}, we use the original code from the authors' GitHub repository. We aim to provide a rigorous and fair comparison between different models on each dataset by tuning hyperparameters for all models individually and detailed hyperparamter configuration of each baseline is in Appendix~\\ref{app-hyper}. Following~\\cite{nifty} and~\\cite{EDITS}, we use 1-layer GCN, GIN convolution and 2-layer GraphSAGE convolution respectively as our encoder $f_{\\bm{\\Theta}_f}$, and use 1 linear layer as our classifier $c_{\\bm{\\Theta}_c}$ and discriminator $d_{\\bm{\\Theta}_d}$. The detailed GNN architecture is described in Appendix~\\ref{app-architecture}. We fix the number of hidden unit of the encoder $f_{\\bm{\\Theta}_f}$ as 16, the dropout rate as 0.5, the number of generated fair feature views during each training epoch $K=10$. The learning rates and the training epochs of the generator $g_{\\bm{\\Theta}_g}$, the discriminator $d_{\\bm{\\Theta}_d}$, the classifier $c_{\\bm{\\Theta}_c}$ and the encoder $f_{\\bm{\\Theta}_f}$ are searched from $\\{0.001, 0.01\\}$ and $\\{5, 10\\}$, the prefix cutting threshold $\\epsilon$ in Eq.~\\eqref{eq-wc} is searched from $\\{0.01, 0.1, 1\\}$, the whole training epochs as $200, 300, 400$, and $\\alpha \\in \\{0, 0.5, 1\\}$. We use the default data splitting following~\\cite{nifty, EDITS} and experimental results are averaged over five repeated executions with five different seeds to remove any potential initialization bias.\n\n% For reproducibility, all codes of our models and hyperparameter configurations are publicly available \\footnote{\\url{https://github.com/codesubmission2022/fairvgnn}\\label{github}}.\n\n\n\n\\vspace{-2ex}\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Node Classification",
                    "content": "\n\\vspace{-0.5ex}\n",
                    "subsubsection 5.2.1": {
                        "name": "Performance comparison",
                        "content": "\nThe model utility and fairness of each baseline is shown in Table~\\ref{tab-main}. We observe that our FairVGNN consistently performs the best compared with other bias-alleviating methods in terms of the average rank for all datasets and across all evaluation metrics, which indicates the superiority of our model in achieving better trade-off between model utility and fairness. \n%Furthermore, FairVSAGE is even better than FairVGIN and FairVGCN because SAGE-encoder uses two different transformations $\\mathbf{W}^{l, 1}, \\mathbf{W}^{l, 2}$ in each layer $l$ to differentiate contributions of aggregating from nodes themselves and from their neighborhoods~\\cite{Graphsage} while GIN and GCN encoders treat them equally with only one unified transformation $\\mathbf{W}^l$. \nSince no fairness regularization is imposed on GNN encoders equipped with vanilla methods, they generally achieve better model utility. However for this reason, sensitive-related information is also completely free to be encoded in the learned node representations and hence causes higher bias. To alleviate such discrimination, all other methods propose different regularizations to constrain sensitive-related information in learned node representations, which also remove some task-related information and hence sacrifice model utility as expected in Table~\\ref{tab-main}. However, we do observe that our model can yield lower biased predictions\n% \\yd{maybe yield less biased predictions / outcomes} \nwith less utility sacrifice, which is mainly ascribed to two reasons:\n\\begin{inparaenum}\n    \\item We generate different fair feature views by randomly sampling masks from learned Gumbel-Softmax distribution and make predictions. This can be regarded as a data augmentation technique by adding noise to node features, which decreases the population risk and enhances the model generalibility~\\cite{shorten2019survey} by creating novel mapping from augmented training points to the label space.\n    \\item The weight clamping module clamps weights of encoder based on feature correlations to the sensitive feature channel, which adaptively remove/keep the sensitive/task-relevant information.\n\\end{inparaenum} \n\n\n\n\n\n\\vspace{-1.5ex}\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "Ablation study",
                        "content": "\nNext we conduct ablation study to fully understand the effect of each component of FairVGNN on alleviating discrimination. Concretely, we denote \\textbf{FairV w/o fm} as removing the module of generating fair feature views, \\textbf{FairV w/o ad wc} as removing the module of adaptive weight clamping, and \\textbf{FairV w/o fm\\&ad wc} as removing both of these two modules. Since computing thresholds in adaptive weight clamping needs the probability of feature masking from fair feature view generation in Eq.~\\eqref{eq-wc}, we instead directly take the prefix value $\\epsilon$ without $\\mathbf{p}_i$ as our cutting threshold in \\textsl{FairV w/o fm}. The utility and bias of these variants are presented in Table~\\ref{tab-ablation}. We observe that \\textsl{FairV w/o fm} and \\textsl{FairV w/o ad wc} perform worse than \\textsl{FairV}, which validates the effectiveness of different components in \\textsl{FairV} for learning fair node representations. Furthermore, the  worse performance of \\textsl{FairV w/o fm\\&ad wc} than \\textsl{FairV w/o fm} and \\textsl{FairV w/o wc} indicates the proposed two modules alleviate discrimination from two different aspects and their effects could be accumulated together. In most cases, \\textsl{FairV w/o fm} achieves more bias than \\textsl{FairV w/o ad wc}. This is because the original clamping threshold of sensitive feature channels $\\epsilon*\\mathbf{p}_i$ would be replaced by a higher threshold $\\epsilon$, which allows more sensitive information leakage to predictions.\n% \\yd{the audience might need more clarification here for the word non-adaptive.}\\yu{updated}\n\n%old version\n% This is because removing the module of fair feature view generation would make the weight clipping module non-adaptive \\yd{the audience might need more clarification here for the word non-adaptive.} so that more sensitive information could be used in predictions.\n\n\n\\vspace{-2.25ex}\n"
                    }
                },
                "subsection 5.3": {
                    "name": "Further Probe",
                    "content": "\n% \\td{consistency in capitalizing each word or not in section/subsections}\n%In this subsection, we take a deeper examination on the proposed FairVGNN to understand how it works and how each component affects its performance.\n\n\\vspace{-1ex}\n",
                    "subsubsection 5.3.1": {
                        "name": "Does adversarial training work?",
                        "content": "\nWe first remove the weight clamping to solely study the effect of adversarial training, and then remove the discriminator/generator respectively by setting their corresponding training epochs to be 0 and denote the corresponding models as \\textbf{FairVGNN w/o wc\\&d} and \\textbf{FairVGNN w/o wc\\&g}. We re-conduct the node classification with five different initializations following the previous setting and report the average bias in Figure~\\ref{fig-adv_ablation}. We can clearly see that after removing discriminator or generator, the model bias becomes even higher in both  situations, which indicates the importance of the competition between the discriminator and the generator in improving the discriminative power of discriminator to recognize sensitive features and the generating power of generator to generate fair feature views. Moreover, since the discriminator in \\textsl{FairVGNN w/o wc\\&g} can still recognize the sensitive features and then guide the encoder to extract less sensitive-related information, the bias of \\textsl{FairVGNN w/o wc\\&g} is lower than \\textsl{FairVGNN w/o wc\\&d} in most cases.\n%\\vspace{-1ex}\n\n\n\n\n\n\\vspace{-1.5ex}\n"
                    },
                    "subsubsection 5.3.2": {
                        "name": "Does adaptive weight clamping work?",
                        "content": "\nTo demonstrate the advantages of the proposed adaptive weight clamping, here we compare it with the non-adaptive weight clamping and spectral normalization, which is another technique of regularizing weight matrix to enhance the model robustness and counterfactual fairness~\\cite{nifty}. The prefix cutting thresholds in both the adaptive and non-adaptive weight clamping are set to be the same as the best ones tuned in Table~\\ref{tab-main} for SAGE/GCN/GIN to ensure the fair comparison. As shown in Table~\\ref{tab-fairnesswc}, we can see that except for GIN, the adaptive weight clamping always achieves lower bias while not hurting so much model utility. This is because for sensitive-related feature channels, multiplying masking probability by the prefix threshold would even lower the threshold and prevent more sensitive information from leaking to prediction through the encoder.\n% Full experimental results are provided in \\yy{note: broken refs} Table~\\ref{tab-fullwc} in Appendix~\\ref{app-wceffect}.\nWe also investigate the influence of prefix cutting threshold $\\epsilon$ in Eq.~\\eqref{eq-wc} on the model bias/utility. Higher $\\epsilon$ indicates less weight clamping on the encoder and more sensitive-related information is leveraged in predictions, which leads to higher bias.\n\n% % \\vspace{-4ex}\n% \\begin{table}[t]%[htbp!]\n% \\footnotesize\n% \\setlength{\\extrarowheight}{.11pt}\n% \\setlength\\tabcolsep{3pt}\n% \\centering\n% \\caption{Comparison with different weight regularization.}\n% \\vspace{-2ex}\n% \\label{tab-fairnesswc}\n% \\begin{tabular}{llccccc}\n% \\hline\n% \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Strategy}} & \\multicolumn{5}{c}{\\textbf{German}} \\\\\n%  &  & AUC (\\uparrow) & ACC (\\uparrow) & F1 (\\uparrow) & $\\Delta_{\\text{sp}}$ (\\downarrow) & $\\Delta_{\\text{eo}}$ (\\downarrow) \\\\\n%  \\hline\n% \\multirow{3}{*}{\\textbf{SAGE}} & Ad wc & \\textcolor{red}{73.84+0.52} & 70.00+0.25 & 81.91+0.63 & \\textcolor{red}{1.36+1.90} & \\textcolor{red}{1.22+1.49} \\\\\n%  & Wc & 72.43+1.60 & \\textcolor{red}{70.48+0.85} & \\textcolor{red}{82.03+0.82} & 4.85+4.10 & 2.50+2.12 \\\\\n%  & Sn & 73.00+1.53 & 70.00+1.07 & 81.82+0.59 & 3.74+3.22 & 1.89+1.08 \\\\\n%  \\hline\n% \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Strategy}} & \\multicolumn{5}{c}{\\textbf{Bail}} \\\\\n%  &  & AUC (\\uparrow) & ACC (\\uparrow) & F1 (\\uparrow) & $\\Delta_{\\text{sp}}$ (\\downarrow) & $\\Delta_{\\text{eo}}$ (\\downarrow) \\\\\n%  \\hline\n% \\multirow{3}{*}{\\textbf{GCN}} & Ad wc & 85.68+0.37 & 84.73+0.46 & 79.11+0.33 & \\textcolor{red}{6.53+0.67} & \\textcolor{red}{4.95+1.22} \\\\\n%  & Wc & 85.97+0.45 & 85.12+0.26 & 79.08+0.28 & 6.86+0.47 & 5.85+0.83 \\\\\n%  & Sn & \\textcolor{red}{86.10+0.61} & \\textcolor{red}{85.69+0.42} & \\textcolor{red}{79.66+0.63} & 7.53+0.17 & 6.43+0.81\\\\\n%  \\hline\n%  \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Strategy}} & \\multicolumn{5}{c}{\\textbf{Credit}} \\\\\n%  &  & AUC (\\uparrow) & ACC (\\uparrow) & F1 (\\uparrow) & $\\Delta_{\\text{sp}}$ (\\downarrow) & $\\Delta_{\\text{eo}}$ (\\downarrow) \\\\\n%  \\hline\n% \\multirow{3}{*}{\\textbf{GIN}} & Ad wc & \\textcolor{red}{74.05$\\pm$0.20} & \\textcolor{red}{79.94$\\pm$0.19} & \\textcolor{red}{87.84$\\pm$0.32} & 4.94$\\pm$1.10 & 2.39$\\pm$0.71 \\\\\n%  & Wc & 73.20$\\pm$1.20 & 79.03$\\pm$1.09 & 87.23$\\pm$0.94 & 7.03$\\pm$4.58 & 4.74$\\pm$3.47 \\\\\n%  & Sn & 71.12$\\pm$0.55 & 78.54$\\pm$2.00 & 86.53$\\pm$1.90 & \\textcolor{red}{2.60$\\pm$0.73} & \\textcolor{red}{0.87$\\pm$0.54}\\\\\n%  \\hline\n% \\end{tabular}\n% \\begin{tablenotes}\n%   \\scriptsize\n%   \\item \\textbf{*} \\textbf{Ad wc}: adaptively clamp weights of the encoder; \\textbf{Wc}: clamp weights of the encoder; and \\\\\\hspace{2.25ex}\\textbf{Sn}: spectral normalization of the encoder\n% \\end{tablenotes}\n% \\vskip -3ex\n% \\end{table}\n\n\n\n\n\n\n\n\n\n\n\n% \\vspace{-8ex}\n\n\n\n\n\n% \\subsubsection{Embedding Visualization}\n\n% \\begin{figure}[t]\n%      \\centering\n%      \\hspace{-1.5ex}\n%      \\begin{subfigure}[b]{0.235\\textwidth}\n%          \\centering\n%          \\includegraphics[width=1.02\\textwidth]{figure/creditsensitivevisualvanilla.pdf}\n%          \\vskip -0.25ex\n%          \\caption{GCN (0.102)}\n%          \\label{fig-tsnecreditgcn}\n%      \\end{subfigure}\n%      \\hspace{-1ex}\n%      \\begin{subfigure}[b]{0.235\\textwidth}\n%          \\centering\n%          \\includegraphics[width=1.02\\textwidth]{figure/creditsensitivevisual.pdf}\n%          \\vskip -0.25ex\n%          \\caption{FairVGCN (0.073)}\n%          \\label{fig-tsnecreditfairvgcn}\n%      \\end{subfigure}\n%      \\vskip -2ex\n%      \\caption{Results of models with different layers.}\n%      \\label{fig-layer}\n%      \\vskip -3.5ex\n% \\end{figure}\\vspace{-1ex}\n"
                    }
                }
            },
            "section 6": {
                "name": "Related Work",
                "content": "\\label{sec-relatedwork}\n%\\td{I think this whole subsection can be removed since this is a very general direction, and also we only focus on a single task of node classification here with fixing the underlying encoder to be a GNN}\n%\\textbf{Node Representation Learning}\n%Learning informative node representation is crucial in completing tasks such as node classification and link prediction. Based on how we regularize the similarity of representations between pairs of nodes, conventional node representation learning approaches can be summarized into two categorizes: factorization-based approaches~\\cite{qiu2018network} where the similarity is regularized by some deterministic metric observed from graph topology and random walk-based approaches~\\cite{perozzi2014deepwalk} where the similarity is regularized by stochastic measures given by random walks. GNNs have gained popularity in node representation learning, owing to the proximity-encoding power of propagation and learning power of linear transformation~\\cite{GCN,GCNII, Graphsage}. This work also employs GNNs as encoder to obtain fair node representations.\n\n%\\noindent \\textbf{Fairness on Graph Neural Networks}\nMost prior work on GNNs exclusively focus on optimizing the model utility while totally ignoring the bias encoded in the learned node representations, which would unavoidably cause social risks in high-stake discriminatory decisions~\\cite{EDITS}. \n%\\td{Assuming people are reluctant to share sensitive features,} \n%In view that people are reluctant to share their sensitive features, \nFairGNN~\\cite{dai2021say} leverages a sensitive feature estimator to enhance the amount of the sensitive attributes, \nwhich greatly benefits their adversarial debiasing procedure. NIFTY~\\cite{nifty} proposes a novel triplet-based objective function and a layer-wise weight normalization using the Lipschitz constant to promote counterfactual fairness and stability of the resulted node representations. EDITS~\\cite{EDITS} systematically summarizes the biased node representation learning into attribute bias and structure bias, and employs the Wasserstein distance approximator to alternately debias node features and network topology. More recently, REFEREE~\\cite{REFEREE} was proposed to provide structural explanations of bias in GNNs. Different from previous work, we study a novel problem that feature propagation could cause correlation variation and sensitive leakage to innocuous features, and our proposed framework FairVGNN expects to learn which feature channels should be masked to alleviate discrimination considering the effect of correlation variation. Recently, others have also explored this concept of varying correlation during feature propagation towards developing deeper GNNs~\\cite{DeCorr}. Besides the fairness issue by sensitive attributes, bias can also come from the node degree~\\cite{tang2020investigating}, graph condensation\\cite{jin2021graph}, or even class distribution~\\cite{dpgnn}, which we leave for future investigations.\n\n\n\n\n%\\vspace{-0.5ex}\n\\vspace{-1.5ex}\n"
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\\label{sec-conclusion}\nIn this paper, we focus on alleviating discrimination in learned node representations and made predictions on graphs from the perspective of sensitive leakage to innocuous features. Specifically, we empirically observe a novel problem that feature propagation could vary feature correlation and further cause sensitive leakage to innocuous feature channels, which may exacerbate discrimination in predictions. To tackle this problem, we propose FairVGNN to automatically mask sensitive-correlated feature channels considering the effect of correlation variation after feature propagation and adaptively clamp weights of encoder to absorb less sensitive information. Experimental results demonstrate the effectiveness of the proposed FairVGNN framework in achieving better trade-off between utility and fairness than other baselines. Some interesting phenomena are also observed such as the variation of correlation depends on different datasets, and the group fairness is related to the network homophily. \n%Inspired by this, we prepare to theoretically analyze the relationships among feature propagation, network homophily and correlation variation. In addition, we also prepare to leverage self-supervised learning to constrain the bias encoded in the learned node representation~\\cite{wang2022graph}, and consider fairness in multi-sensitive groups in the future work. \nThus, one future direction would be to theoretically analyze the relationships among feature propagation, network homophily and correlation variation. Furthermore, we plan to leverage self-supervised learning~\\cite{wang2022graph,jin2020self} to constrain the bias encoded in the learned node representation, and consider fairness in multi-sensitive groups in future work.\n% \\td{somehow the citation for DPGNN does not seem to fit well in this way. one way might be to keep ssl here. then, instead move the imbalance to the related work.}\n% \\td{add a sentence about wanting to leverage self supervised learning towards more generalizable representations, such as those in the imbalanced data situation (in this way you could combine the future direction with citing two prior works as compared to trying to cite DPGNN in the intro)  }\n\n\n\n\n\n\n\\vspace{-2ex}\n"
            },
            "section 8": {
                "name": "Acknowledgements",
                "content": "\n\\vspace{-0.5ex}\nYushun Dong and Jundong Li are supported by the National Science Foundation (NSF) under grant No. 2006844 and the Cisco Faculty Research Award.\n\n% \\clearpage %just here temporarily to check the reference length to ensure it fits on one page\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n% \\begin{acks}\n% To Robert, for the bagels and explaining CMYK and color spaces.\n% \\end{acks}\n%\\balance \n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\vspace{-2ex}\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{references}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n% \\appendix\n\n% \\section{Research Methods}\n\n% \\subsection{Part One}\n\n% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi\n% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id\n% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum\n% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et\n% lacinia dolor. Integer ultricies commodo sem nec semper.\n\n% \\subsection{Part Two}\n\n% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales\n% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non\n% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet\n% eros. Vivamus non purus placerat, scelerisque diam eu, cursus\n% ante. Etiam aliquam tortor auctor efficitur mattis.\n\n% \\section{Online Resources}\n\n% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in\n% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis\n% enim maximus. Vestibulum gravida massa ut felis suscipit\n% congue. Quisque mattis elit a risus ultrices commodo venenatis eget\n% dui. Etiam sagittis eleifend elementum.\n\n% Nam interdum magna at lectus dignissim, ac dignissim lorem\n% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar\n% massa et mattis lacinia.\n% \\clearpage\n% \\newpage\n\\appendix\n% \\section{Algorithm}\\label{app-algorithm}\n% In Appendix~\\ref{app-algorithm}, we present a holistic algorithm of the proposed FairVGNN framework. In comparison to vanilla adversarial training, additional computational requirements of FairVGNN comes from generating $K$ different masks. However, since within each training epoch we can pre-compute the masks as Step 4-7 before adversarial training and the total number of views $K$ becomes constant compared with the whole time used for adversarial training as Step 8-16, the time complexity is still linear proportional to the size of the whole graph, i.e., $O(|\\mathcal{V}| + |\\mathcal{E}|)$. The total model complexity includes parameters of the feature masker $O(2d)$, the discriminator/classifier $O(2d^L)$ and the encoder $O(d\\prod_{l = 1}^L{d^l})$, which boils down to $O(\\max_{i\\in\\{0, 1, ..., L\\}}(d^i)^L)$ and hence the same as any other $L$-layer GNN backbones.\n\n% \\setlength{\\textfloatsep}{4pt}\n% \\begin{algorithm}[htbp!]\n%  \\DontPrintSemicolon\n%  \\footnotesize\n%  \\KwIn{an attributed graph $G = (\\mathcal{V}, \\mathcal{E}, \\mathbf{X}, \\mathbf{A}, \\mathbf{Y})$, Classifier $c_{\\bm{\\Theta}_c}$, Encoder $f_{\\bm{\\Theta}_f}$, Generator $g_{\\bm{\\Theta}_g}$, Discriminator $d_{\\bm{\\Theta}_d}$, $K$}\n \n%  \\KwOut{Learned fairness attribute $\\widetilde{\\mathbf{X}}$ and Predictions $\\hat{\\mathbf{Y}}$}\n\n%  Initialize model parameters $\\bm{\\Theta}_f, \\bm{\\Theta}_g, \\bm{\\Theta}_d, \\bm{\\Theta}_c$;\n \n%  \\While{not converged}{\n%     $\\boldsymbol{\\pi} \\leftarrow \\mathbf{W}^{g_{\\bm{\\Theta}_g}}$\n    \n%     \\For{$k\\leftarrow 1$ \\KwTo $K$}\n%     {\n%         $\\mathbf{m}^k \\sim \\text{Gumbel-softmax}(\\boldsymbol{\\pi})$\n        \n%         $\\widetilde{\\mathbf{X}}^k \\leftarrow \\mathbf{X}\\odot\\mathbf{m}^k$, \\tcp*{Section~\\ref{sec-g}}\n        \n%         $\\widetilde{\\mathbf{H}}_i^{L, k} \\leftarrow f_{\\bm{\\Theta}_f}(\\widetilde{\\mathbf{X}}^k, \\mathbf{A})$,  $\\widehat{\\mathbf{H}}_i^{L, k} \\leftarrow sg(\\widetilde{\\mathbf{H}}_i^{L, k})$\\footnotemark    \\tcp*{Section~\\ref{sec-e}}\n        \n%     }\n    \n%     \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_d$}\n%     {\n%         $\\mathcal{L}_{\\text{d}} \\leftarrow \\sum\\limits_{k = 1}^{K}\\sum\\limits_{v_i \\in \\mathcal{V}}[\\mathbf{S}_i\\log(d_{\\bm{\\Theta}_d}(\\widehat{\\mathbf{H}}^{L, k}_i)) + (1 - \\mathbf{S}_i)\\log(1 - d_{\\bm{\\Theta}_d}(\\widehat{\\mathbf{H}}^{L, k}_i))]$\n        \n%         $\\bm{\\Theta_d} \\leftarrow \\bm{\\Theta_d}+\n%         \\nabla_{\\bm{\\Theta}_d} \\mathcal{L}_d$, $\\bm{\\Theta_f} \\leftarrow \\bm{\\Theta_f}+\n%         \\nabla_{\\bm{\\Theta}_f}\\mathcal{L}_f$ \\tcp*{Section~\\ref{sec-cd}}\n%     }\n    \n%     \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_c$}\n%     {\n%         $\\mathcal{L}_{\\text{c}} \\leftarrow \\sum\\limits_{k=1}^K\\sum\\limits_{v_i\\in\\mathcal{V}}[\\mathbf{Y}_i\\log(c_{\\bm{\\Theta}_c}(\\widetilde{\\mathbf{H}}^{L, k}_i)) + (1 - \\mathbf{Y}_i)\\log(1 - c_{\\bm{\\Theta}_c}(\\widetilde{\\mathbf{H}}^{L, k}_i))]$\n        \n%         $\\bm{\\Theta_c} \\leftarrow \\bm{\\Theta_c}-\n%         \\nabla_{\\bm{\\Theta}_c}\\mathcal{L}_c$, $\\bm{\\Theta_f} \\leftarrow \\bm{\\Theta_f}-\n%         \\nabla_{\\bm{\\Theta}_f}\\mathcal{L}_f$ \\tcp*{Section~\\ref{sec-cd}}\n%     }\n    \n%     \\For{epoch $\\leftarrow 1$ \\KwTo $epoch_g$}\n%     {\n%         $\\mathcal{L}_{\\text{g}}^k \\leftarrow \\sum\\limits_{k=1}^K\\sum\\limits_{v_i\\in\\mathcal{V}}||d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^{L, k}_i) - 0.5||_2^2,$\n    \n%         $\\bm{\\Theta_g} \\leftarrow \\bm{\\Theta_g}-\n%         \\nabla_{\\bm{\\Theta}_g}\\mathcal{L}_g$, $\\bm{\\Theta_f} \\leftarrow \\bm{\\Theta_f}-\n%         \\nabla_{\\bm{\\Theta}_f}\\mathcal{L}_f$ \\tcp*{Section~\\ref{sec-advtraining}}\n%     }\n    \n%     $\\bm{\\Theta_f} \\leftarrow \\text{Clamp}(\\bm{\\Theta_f}, \\sum\\limits_{k = 1}^K\\mathbf{m}^k)$ \\tcp*{Section~\\ref{sec-wc}}\n    \n% }\n\n% $\\widetilde{\\mathbf{X}} = \\sum\\limits_{k=1}^K\\widetilde{\\mathbf{X}}^k, \\hat{\\mathbf{Y}} = c_{\\bm{\\Theta}_c}(f_{\\bm{\\Theta}_f}(\\widetilde{\\mathbf{X}}, \\mathbf{A}))$\n\n% \\KwRet{$\\widetilde{\\mathbf{X}}, \\widehat{\\mathbf{Y}}$}\n\n% \\caption{\\small The algorithm of FairVGNN}\n% \\label{alg-fairvgnn}\n% \\end{algorithm}\n% \\footnotetext{$sg$: stopgrad prevents gradients from being\n% back-propagated.}\n\n\n\n\n% \\section{Proof}\\label{proof}\n\n% \\begin{proof}\n% For each specific generated feature view $\\widetilde{\\mathbf{X}}$, we have:\n% \\begin{align}\n%     \\ell_d & = \\mathbb{E}_{v_i\\sim\\mathcal{V}}\n% [\\mathbf{S}_i\\log(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^L_i)) + (1 - \\mathbf{S}_i)\\log(1 - d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^L_i))]\n% \\nonumber\\\\&= \\mathbb{E}_{v_i\\sim\\mathcal{V}_{s = 1}}\\log(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{h}}^L)) + \\mathbb{E}_{v_i\\sim\\mathcal{V}_{s = 0}}(1 - \\log(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{h}}^L)))\n% \\nonumber\\\\& =\\int_{\\widetilde{\\mathbf{h}}^L}{P(\\widetilde{\\mathbf{h}}^L|s = 1)}{\\log(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{h}}^L)) + P(\\widetilde{\\mathbf{h}}^L|s = 0)}(1 - \\log(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{h}}^L)))d\\widetilde{\\mathbf{h}}^L\n% \\end{align}\n% Based on Proposition 1. in \\cite{goodfellow2014generative} and Proposition 4.1. in \\cite{dai2021say}, the optimal discriminator is $d_{\\bm{\\Theta}_d^*}(\\widetilde{\\mathbf{h}}^L) = \\frac{P(\\widetilde{\\mathbf{h}}^L|s = 1)}{P(\\widetilde{\\mathbf{h}}^L|s = 1) + P(\\widetilde{\\mathbf{h}}^L|s = 0)}$, which is exactly the probability when discriminator random guess the sensitive features. Then we further substituted it into Eq.~\\eqref{eq-advg} and the optimal generator is achieved when $d_{\\bm{\\Theta}_d^*}(\\widetilde{\\mathbf{h}}^L) = 0.5, \\text{i.e.}, P(\\widetilde{\\mathbf{h}}^L|s = 1) = P(\\widetilde{\\mathbf{h}}^L|s = 0)$. Then we have:\n% \\begin{align}\n%     P(\\hat{y} = 1|s &= 1) = \\int_{\\widetilde{\\mathbf{h}}^L}P(\\hat{y} = 1|\\widetilde{\\mathbf{h}}^L)P(\\widetilde{\\mathbf{h}}^L|s = 1)d\\widetilde{\\mathbf{h}}^L\n%     \\nonumber\\\\& = \\int_{\\widetilde{\\mathbf{h}}^L}P(\\hat{y} = 1|\\widetilde{\\mathbf{h}}^L)P(\\widetilde{\\mathbf{h}}^L|s = 0)d\\widetilde{\\mathbf{h}}^L = P(\\hat{y} = 1|s = 0),\n% \\end{align}\n% which is obviously the global minimum of Eq.~\\eqref{eq-minsp}.\n% \\end{proof}\n\n\n\n% \\begin{proof}\n% Substituting the row-normalized adjacency matrix $\\mathbf{D}^{-1}(\\mathbf{A}+\\mathbf{I})$, we have $f_{\\bm{\\Theta}_f}(\\mathbf{X}) = \\mathbf{W}^{f, 1}\\mathbf{D}^{-1}(\\mathbf{A} + \\mathbf{I})\\mathbf{X}$, for any pair of nodes coming from two different sensitive groups $v_i\\in\\mathcal{V}_0, v_j\\in\\mathcal{V}_1$, we have:\n% \\begin{equation}\\label{eq-dist0}\n% % \\small\n% \\begin{split}\n%     &f_{\\bm{\\Theta}_f}(\\mathbf{X}_i) - f_{\\bm{\\Theta}_f}(\\mathbf{X}_j)\n%     = \\mathbf{W}^{f, 1}(\\mathbf{D}^{-1}(\\mathbf{A} + \\mathbf{I})\\mathbf{X})_i - \\mathbf{W}^{f, 1}(\\mathbf{D}^{-1}(\\mathbf{A} + \\mathbf{I})\\mathbf{X})_j\n%     \\\\=& \\mathbf{W}^{f, 1}(\\frac{1}{d_i + 1}\\sum_{v_p\\in\\mathcal{N}_i\\cup v_i}{\\mathbf{X}_p} - \\frac{1}{d_j + 1}\\sum_{v_q\\in\\mathcal{N}_j\\cup v_j}{\\mathbf{X}_q}),\n% \\end{split}\n% \\end{equation}\n% if the network homophily is $\\chi$ and further assuming that neighboring nodes strictly obey the network homophily, i.e., among $|\\mathcal{N}_i\\cup v_i| = d_i + 1$ neighboring nodes of the center node $v_i$, $\\chi(d_i + 1)$ of them come from the same feature distribution as $v_i$ while $(1 - \\chi)(d_i + 1)$ of them come from the other feature distribution as $v_j$, then symmetrically we have:\n% \\begin{equation}\n% \\small\n%     \\frac{1}{d_i + 1}\\sum_{v_p\\in\\mathcal{N}_i\\cup v_i}{\\mathbf{X}_p}\\sim\\mathcal{N}(\\chi\\boldsymbol{\\mu}_1 + (1 - \\chi)\\boldsymbol{\\mu}_2, (d_i + 1)^{-1}(\\chi \\boldsymbol{\\Sigma}_1 + (1 - \\chi)\\boldsymbol{\\Sigma}_2)), \\nonumber\n% \\end{equation}\n% % \\vspace{-3ex}\n% \\begin{equation}\\label{eq-dist2}\n% \\small\n%     \\frac{1}{d_j + 1}\\sum_{v_q\\in\\mathcal{N}_j\\cup v_j}{\\mathbf{X}_q}\\sim\\mathcal{N}(\\chi\\boldsymbol{\\mu}_2 + (1 - \\chi)\\boldsymbol{\\mu}_1, (d_j + 1)^{-1}(\\chi \\boldsymbol{\\Sigma}_2 + (1 - \\chi)\\boldsymbol{\\Sigma}_1)).\n% \\end{equation}\n\n% Combining Eq.~\\eqref{eq-dist2} and Eq.~\\eqref{eq-dist0}, the distribution of their difference would also be a Gaussian $f_{\\bm{\\Theta}_f}(\\mathbf{X}_i) - f_{\\bm{\\Theta}_f}(\\mathbf{X}_j)\\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, where:\n% \\begin{equation}\n% \\small\n%     \\boldsymbol{\\mu} = \\mathbf{W}^{f, 1}(\\chi\\boldsymbol{\\mu}_2 + (1 - \\chi)\\boldsymbol{\\mu}_1) = (2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}\n% \\end{equation}\n% \\begin{equation}\n% \\small\n%     \\boldsymbol{\\Sigma} = \\mathbf{W}^{f, 1}((d_i + 1)^{-1}(\\chi \\boldsymbol{\\Sigma}_1 + (1 - \\chi)\\boldsymbol{\\Sigma}_2) + (d_j + 1)^{-1}(\\chi \\boldsymbol{\\Sigma}_2 + (1 - \\chi)\\boldsymbol{\\Sigma}_1))\\mathbf{W}^{f, 1}^{\\top}\n% \\end{equation}\n\n\n% % \\\\=& \\mathbb{E}_{v_i\\in\\mathcal{V}_1, v_j\\in\\mathcal{V}_0}(\\mathbf{W}^{f, 1}(\\frac{1}{d_i + 1}\\sum_{v_p\\in\\mathcal{N}_i\\cup v_i}{\\mathbf{X}_p} - \\frac{1}{d_j + 1}\\sum_{v_q\\in\\mathcal{N}_j\\cup v_j}{\\mathbf{X}_q}))\n%     % \\\\=& \\mathbf{W}^{f, 1}\\mathbb{E}_{v_i\\in\\mathcal{V}_1, v_j\\in\\mathcal{V}_0}(\\frac{1}{d_i + 1}\\sum_{v_p\\in\\mathcal{N}_i\\cup v_i}{\\mathbf{X}_p} - \\frac{1}{d_j + 1}\\sum_{v_q\\in\\mathcal{N}_j\\cup v_j}{\\mathbf{X}_q}) \n%     % \\\\=& \\mathbf{W}^{f, 1}(\\chi\\boldsymbol{\\mu}_1 + (1 - \\chi)\\boldsymbol{\\mu}_2 - (\\chi\\boldsymbol{\\mu}_2 + (1 - \\chi)\\boldsymbol{\\mu}_1)) = (2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}\n% Taking the $2-$norm on the mean $\\boldsymbol{\\mu}$, splitting channels into sensitive ones $\\mathcal{S}$ and non-sensitive ones $\\mathcal{NS}$, i.e., $\\{1, 2, ..., d\\}=\\mathcal{S}\\cup\\mathcal{NS}$ and expand $\\boldsymbol{\\mu}$ based on the input channel, we have:\n% \\begin{equation}\\label{eq-dist3}\n% \\small\n% ||(2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}||_2 = (2\\chi - 1)(\\sum_{i = 1}^{d_1}(\\sum_{r\\in\\mathcal{S}}\\mathbf{W}^{f, 1}_{ir}\\Delta\\boldsymbol{\\mu}_r + \\sum_{k\\in\\mathcal{NS}}\\mathbf{W}^{f, 1}_{ik}\\Delta\\boldsymbol{\\mu}_k)^2)^{0.5},\n% \\end{equation}\n% where $\\mathbf{W}^{f, 1}_{ir}, \\mathbf{W}^{f, 1}_{ik}$ represent the weights of the encoder from feature channel $r(k)$ to the hidden neuron $i$. Since we know that $|\\mathbf{W}^{f, 1}_{ir}| \\le \\epsilon\\mathbf{p}_r, |\\mathbf{W}^{f, 1}_{ik}| \\le \\epsilon\\mathbf{p}_k, \\forall r\\in\\mathcal{S}, k\\in\\mathcal{NS}$, we substitute the upper bound here into Eq.~\\eqref{eq-dist3} and finally end up with:\n% \\begin{equation}\n% \\footnotesize\n% \\begin{split}\n%     &||\\boldsymbol{\\mu}||_2 = ||(2\\chi - 1)\\mathbf{W}^{f, 1}\\Delta\\boldsymbol{\\mu}||_2\n%     \\le (2\\chi - 1)(\\sum_{i = 1}^{d_1}(\\sum_{r\\in\\mathcal{S}}\\epsilon\\mathbf{p}_r\\Delta\\boldsymbol{\\mu}_r + \\sum_{k\\in\\mathcal{NS}}\\epsilon\\mathbf{p}_k\\Delta\\boldsymbol{\\mu}_k)^2)^{0.5}\n% \\end{split}\n% \\end{equation}\n% \\end{proof}\n\n\n"
            },
            "section 9": {
                "name": "Summary of Notations",
                "content": "\\label{sec-notation}\nTo facilitate understanding, we present a summary of commonly utilized notations and the corresponding descriptions in Table~\\ref{tb:symbols}.\n\n\n\n\n\\vspace{-1ex}\n"
            },
            "section 10": {
                "name": "Experimental Settings",
                "content": "\\label{app-experiment}\n",
                "subsection 10.1": {
                    "name": "Detailed Model Architecture",
                    "content": "\\label{app-architecture}\nA unified template of a graph convolutional layer is formalized as:\n%The unified template of one graph convolutional layer can be summarized as:\n\\begin{equation}\\label{eq-propagation}\n    \\mathbf{h}_i^l = \\text{TRAN}^{l}(\\text{PROP}^{l}(\\mathbf{h}_{i}^{l - 1}, \\{\\mathbf{h}_j^{l - 1}|j\\in\\mathcal{N}_i\\})),\n\\end{equation}\n where $\\mathcal{N}_i$ denotes the neighborhood set of node $v_i$ and $\\text{PROP}^l, \\text{TRAN}^l$ stand for neighborhood propagation and feature transformation at layer $l$. In neighborhood propagation, neighborhood representations are propagated and further fused with itself to get the intermediate representation $\\widehat{\\mathbf{h}}_i^l$. \n%  representations of neighborhoods at the previous layer $\\{\\mathbf{h}_j^{l - 1}|j\\in\\mathcal{N}_i\\}$ are propagated to the center node $v_i$ and further fused with its own representation $\\mathbf{h}_i^{l - 1}$ from the previous layer via $\\text{AGG}^l$ function at layer $l$ to get the intermediate representation $\\widehat{\\mathbf{h}}_i^l$. \nThen, the $\\text{TRAN}^l$ function is applied on $\\widehat{\\mathbf{h}}_i^l$ to get the final representation $\\mathbf{h}_i^l$ of node $v_i$ at layer $l$. Note that $\\mathbf{h}_i^0$ of node $v_i$ is typically initialized as the original node feature $\\mathbf{X}_{i}$. After stacking $L$ graph convolutional layers, every node  aggregates their neighborhood information up to $L$-hops away and we denote it as $\\mathbf{H}^L\\in\\mathbb{R}^{n\\times d^{L}}$. \n%  Most graph convolutions, such as GCN~\\cite{GCN}, GraphSAGE~\\cite{Graphsage}, GAT~\\cite{GAT}, and GIN~\\cite{GIN}, can be obtained under this template by adopting and configuring different functions in $\\text{PROP}^{l}$ and $\\text{TRAN}^{l}$. Owing to the power of GNNs in node representation learning, this work employs it as the encoder-backbone.\n Many graph convolutions %(e.g., GCN~\\cite{GCN}, GraphSAGE~\\cite{Graphsage}, and GIN~\\cite{GIN}) \n can be obtained under this template by configuring different $\\text{PROP}^{l}$ and $\\text{TRAN}^{l}$. In this work, the encoder of FairVGNN is designed following this template. \n%thus, this work is built upon a GNN encoder-backbone.\n\n\nWe use GCN, GIN and GraphSAGE as our GNN-backbones respectively for each bias-alleviating method. The basic graph convolution layer of these three backbones, respectively, are:\n\\begin{equation}\\label{eq:GCN}\n    \\mathbf{H}^{l} = \\widetilde{\\mathbf{D}}^{-0.5}(\\mathbf{A} + \\mathbf{I})\\widetilde{\\mathbf{D}}^{-0.5}\\mathbf{H}^{l - 1}\\mathbf{W}^{l},\n\\end{equation}\n\\begin{equation}\\label{eq:GIN}\n    \\mathbf{H}^{l} = \\text{MLP}^l((\\mathbf{A} + (1 + \\alpha)\\mathbf{I})\\mathbf{H}^{l - 1}),\n\\end{equation}\n\\begin{equation}\\label{eq:SAGE}\n    \\mathbf{H}^{l} = \\mathbf{W}^{l, 1}\\mathbf{H}^{l - 1} + \\mathbf{W}^{l, 2}\\mathbf{D}^{-1}\\mathbf{A}\\mathbf{H}^{l - 1},\n\\end{equation}\nwhere $\\widetilde{\\mathbf{D}}$ is the degree matrix with added self-loop, $\\mathbf{H}^{l - 1}$ is the node representation obtained from the previous layer and $\\mathbf{H}^{0} = \\mathbf{X}$. In this work, we only consider one graph convolution, therefore $l = 1$.\n\n\\vspace{-1ex}\n"
                },
                "subsection 10.2": {
                    "name": "Hyperparameter for Each Baseline",
                    "content": "\\label{app-hyper}\nAs different bias-alleviating methods have different model architectures, their hyperparameters are also different and are presented respectively in the following:\n\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{NIFTY}: dropout \\{0.0, 0.5, 0.8\\}, the number of hidden unit 16, learning rate $\\{1e^{-2}, 1e^{-3}, 1e^{-4}\\}$, project hidden unit 16, weight decay $\\{1e^{-4}, 1e^{-5}\\}$, drop edge rate $0.001$, drop feature rate $0.1$, regularization coefficient $\\{0.4, 0.5, 0.6, 0.7, 0.8\\}$.\n    \n    \\item \\textbf{EDITS}: initial learning rate 0.003, weight decay $1e^{-7}$, threshold proportions for Credit, German, and Recidivism dataset are 0.02, 0.25, 0.012 respectively.\n    \n    \\item \\textbf{FairGNN}: dropout $\\{0.0, 0.5, 0.8\\}$, the number of hidden unit 32, learning rate $\\{0.0001, 0.001, 0.01\\}$, weight decay $1e^{-5}$, regularization coefficients $\\alpha=4, \\beta=0.01$, sensitive number $200$, label number $500$.\n\\end{itemize}\n\n\\vspace{-1ex}\n"
                }
            },
            "section 11": {
                "name": "Dataset Details",
                "content": "\\label{app-dataset}\nHere we present the detailed description of three datasets we used to validate our proposed FairVGNN as follows:\n\\begin{itemize}[leftmargin=*]\n    % \\item \\textbf{German Credit} (German): nodes %represents \n    % are clients in a German bank, and edges are formed between clients if their credit accounts are similar. Node attributes include Gender, LoanAmount, and other account-related information. The task is to classify the credit risk of the clients as high or low with `gender' being the sensitive feature.\n    \\item \\textbf{German Credit} (German): nodes are clients in a German bank, node attributes include gender, loan amount, and other account-related details, with edges formed between clients if their credit accounts are similar. The task is to classify the credit risk of the clients as high or low with `gender' being the sensitive feature.\n    \\item \\textbf{Recidivism} (Bail): nodes %represent \n    are defendants released on bail during 1990-2009, and edges are formed between defendants if they share similar past criminal records and demographics. The task is to predict whether a defendant would be more likely to commit a violent or nonviolent crime once released on bail with `race' being the sensitive feature.\n    \\item \\textbf{Credit Defaulter} (Credit): nodes %represent \n    are credit card users, and edges are formed between users if their share similar pattern in purchases/payments. The task is to predict whether a user will default on credit card payment with `age' being the sensitive feature.\n\\end{itemize} \n\n"
            },
            "section 12": {
                "name": "Detailed Experimental Results",
                "content": "\n",
                "subsection 12.1": {
                    "name": "Effect of Weight Clamping",
                    "content": "\\label{app-wceffect}\nTable~\\ref{tab-fullwc} reports the full results of comparing our proposed adaptive weight clamping with two other weight regularization approaches: weight clamping and spectral normalization. We can clearly see that generally our proposed adaptive weight clamping achieves better trade-off between utility and fairness. This is because adaptive weight clamping clamps weights more on sensitive-related features and hence minimally remove critical information beneficial for classification. However, in some cases such as on Credit dataset, the GCN with adaptive weight clamping has higher bias than directly weight clamping. This is because blindly clamping weights with no selection would remove more information, some of which might overlap with sensitive information and hence cause less bias, while some of which might overlap with class-related information and hence cause lower model utility (the accuracy is 77.79 lower than 78.04 when using adaptive weight clamping).\n\n\n"
                },
                "subsection 12.2": {
                    "name": "Detailed Correlation Variation of German and Credit Datasets",
                    "content": "\\label{app-corrvalfull}\nHere we visualize the correlation variation of the first 13 feature channels on German and Credit after different layers of feature propagation. We clearly see that compared with German where some feature channels quickly become highly correlated to sensitive channel while some become less correlated. The sensitive correlation of feature channels on Credit changes more slowly. Therefore, masking according to the rank of original sensitive correlation $\\boldsymbol{\\rho}^{\\text{origin}}$ is roughly the same as the propagated sensitive correlation $\\boldsymbol{\\rho}^{\\text{prop}}$ and the performance of S$_1$ and S$_2$ are the same in Table~\\ref{tab-prelim} on Credit compared with German. We argue that the slow variance of feature correlation is because the higher homophily of Credit (0.9595) than German (0.8048) triggers less change of feature correlation during feature propagation. In the extreme case where node features strictly obey the network homophily, feature propagation would cause no change on feature distributions of every node and therefore the feature correlation would stay the same.\n\n%\\pagebreak\n\n\n\n% \\begin{figure}[t]\n%      \\centering\n%      \\hspace{-1.5ex}\n%      \\begin{subfigure}[b]{0.45\\textwidth}\n%          \\centering\n%          \\includegraphics[width=.9\\textwidth]{figure/germancorrelation_change_simple.pdf}\n%          \\vskip -0.5ex\n%          \\caption{German (0.8048)}\n%          \\label{fig-germancorrfull}\n%      \\end{subfigure}\n%      \\hspace{-1ex}\n%      \\begin{subfigure}[b]{0.45\\textwidth}\n%          \\centering\n%          \\includegraphics[width=0.9\\textwidth]{figure/creditcorrelation_change_simple.pdf}\n%          \\vskip -0.5ex\n%          \\caption{Credit (0.9595)}\n%          \\label{fig-creditcorrfull}\n%      \\end{subfigure}\n%      \\vskip -2ex\n%      \\caption{Correlation variation after feature propagation on German and Credit with bracket behind each dataset denoting the network homophily.}\n%      \\label{fig-corrvar}\n%     %  \\vskip -3.5ex\n% \\end{figure}\n\n"
                }
            }
        },
        "tables": {
            "tab-prelim": "\\begin{table}[t]%[htbp!]\n\\footnotesize\n\\setlength{\\extrarowheight}{.12pt}\n\\setlength\\tabcolsep{3.2pt}\n\\centering\n\\caption{Evaluating model utility and fairness when using various strategies of feature masking (or no masking).}\n%strategy: S0) original feature matrix without feature masking (i.e., S0) or with masking the top-$4$ channels based on the rank of $\\boldsymbol{\\rho}^{\\text{origin}}$ $\\boldsymbol{\\rho}^{\\text{prop}}$ (i.e., S1/S2). }}%\\maybereword{Results of models trained on masked features on German and Credit}}\n\\vskip -2ex\n\\begin{tabular}{ll|llll|llll}\n\\hline\n\\multirow{2}{*}{\\textbf{Encoder}} & \\multirow{2}{*}{\\textbf{Strategy}} & \\multicolumn{4}{c|}{\\textbf{German}} & \\multicolumn{4}{c}{\\textbf{Credit}} \\\\ %\\cline{3-6}\n% \\hline\n &  & AUC & F1 & $\\Delta_{\\text{sp}}$ & $\\Delta_{\\text{eo}}$ & AUC & F1 & $\\Delta_{\\text{sp}}$ & $\\Delta_{\\text{eo}}$ \\\\\n \\hline\n\\multirow{3}{*}{\\textbf{MLP}} & S$_0$ & 71.98 & 82.32 & 29.26 & 19.43 & 74.46 & 81.64 & 11.85 & 9.61 \\\\\n & S$_1$ & 69.89 & 81.37 & 8.25 & 4.75 & 73.49 & 81.50 & 11.50 & 9.20 \\\\\n & S$_2$ & 70.54 & 81.44 & 6.58 & 3.24 & 73.49 & 81.50 & 11.50 & 9.20 \\\\\n \\hline\n\\multirow{3}{*}{\\textbf{GCN}} & S$_0$ & 74.11 & 82.46 & 35.17 & 25.17 & 73.86 & 81.92 & 12.86 & 10.63 \\\\\n & S$_1$ & 73.78 & 81.65 & 11.39 & 9.60 & 72.92 & 81.84 & 12.00 & 9.70 \\\\\n & S$_2$ & 72.75 & 81.70 & 8.29 & 6.91 & 72.92 & 81.84 & 12.00 & 9.70\\\\\n \\hline\n \\multirow{3}{*}{\\textbf{GIN}} & S$_0$ & 72.71 & 82.78 & 13.56 & 9.47 & 74.36 & 82.28 & 14.48 & 12.35 \\\\\n & S$_1$ & 71.66 & 82.50 & 3.01 & 1.72 & 73.44 & 83.23 & 14.29 & 11.79\\\\\n & S$_2$ & 70.77 & 83.53 & 1.46 & 2.67 & 73.28 & 83.27 & 13.96 & 11.34\\\\\n \\hline\n \n\\end{tabular}\n\\begin{tablenotes}\n  \\footnotesize\n  \\item \\textbf{*} S$_0$: training using the original feature matrix $\\mathbf{X}$ without any masking.\n  \n  \\item \\textbf{*} S$_1$/S$_2$: training with masking the top-$4$ channels based on the rank of $\\boldsymbol{\\rho}^{\\text{origin}}$/$\\boldsymbol{\\rho}^{\\text{prop}}$.\n\\end{tablenotes}\n\\vskip -2ex\n\\vspace{-4ex}\n\\label{tab-prelim}\n\\end{table}",
            "tab-dataset": "\\begin{table}[htbp!]\n\\vskip -2ex\n\\small\n\\setlength{\\extrarowheight}{.095pt}\n\\setlength\\tabcolsep{3pt}\n\\caption{Basic dataset statistics.}\n\\centering\n\\vspace{-4ex}\n\\begin{tabular}{lccc}\n \\Xhline{2\\arrayrulewidth}\n\\textbf{Dataset} & \\textbf{German}& \\textbf{Credit} & \\textbf{Bail}\\\\\n \\Xhline{1.5\\arrayrulewidth}\n\\#Nodes & 1000& 30,000 & 18,876 \\\\\n\\#Edges & 22,242 & 1,436,858 & 321,308\\\\\n\\#Features & 27 & 13 & 18\\\\\nSens. & Gender & Age & Race\\\\\nLabel & Good/bad Credit & Default/no default Payment & Bail/no bail\\\\\n\n\\Xhline{2\\arrayrulewidth}\n\\end{tabular}\n\\label{tab-dataset}\n\\vskip -1ex\n\\end{table}",
            "tab-main": "\\begin{table*}[t!]\n\\tiny\n\\setlength{\\extrarowheight}{.095pt}\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\caption{Model utility and bias of node classification. We compare the proposed FairVGNN (i.e., FairV) against state-of-the-art baselines NIFTY, EDITS, and FairGNN (i.e., Fair) when equiped with various GNN backbones (i.e., GCN, GIN, and SAGE). The best and runner-up results are colored in \\textcolor{red}{red} and \\textcolor{blue}{blue}. $\\uparrow$ represents the larger, the better while $\\downarrow$ represents the opposite.}\n\\vskip -3.5ex\n\\begin{tabular}{l|l|ccccc|ccccc|ccccc|c}\n\\hline\n \\multirow{2}{*}{\\textbf{Encoder}}& \\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{5}{c|}{\\textbf{German}} & \\multicolumn{5}{c|}{\\textbf{Credit}} & \\multicolumn{5}{c|}{\\textbf{Bail}} & \\multirow{2}{*}{\n %\\shortstack{Avg.\\\\ Rank}\n \\begin{tabular}{@{}c@{}} \\textbf{Avg.} \\\\ \\textbf{(Rank)}\\end{tabular}\n %\\makecell{\\textbf{Avg. \\\\Rank}}\n  } \\\\\n &  & AUC ($\\uparrow$) & F1 ($\\uparrow$) & ACC ($\\uparrow$) & $\\Delta_{\\text{sp}}$ ($\\downarrow$) & $\\Delta_{\\text{eo}}$ ($\\downarrow$) & AUC ($\\uparrow$) & F1 ($\\uparrow$) & ACC ($\\uparrow$) & $\\Delta_{\\text{sp}}$ ($\\downarrow$) & $\\Delta_{\\text{eo}}$ ($\\downarrow$) & AUC ($\\uparrow$) & F1 ($\\uparrow$) & ACC ($\\uparrow$) & $\\Delta_{\\text{sp}}$ ($\\downarrow$) & $\\Delta_{\\text{eo}}$ ($\\downarrow$) \\\\\n \n \\hline\n \n\\multirow{5}{*}{\\textbf{GCN}} & Vanilla & \\textcolor{blue}{74.11$\\pm$0.37} & 82.46$\\pm$0.89 & \\textcolor{blue}{73.44$\\pm$1.09} & 35.17$\\pm$7.27 & 25.17$\\pm$5.89 & 73.87$\\pm$0.02 & 81.92$\\pm$0.02 & 73.67$\\pm$0.03 & 12.86$\\pm$0.09 & 10.63$\\pm$0.13 & 87.08$\\pm$0.35 & 79.02$\\pm$0.74 & 84.56$\\pm$0.68 & 7.35$\\pm$0.72 & 4.96$\\pm$0.62 & 9.17\\\\\n\n & NIFTY & 68.78$\\pm$2.69 & 81.40$\\pm$0.54 & 69.92$\\pm$1.14 & 5.73$\\pm$5.25 & 5.08$\\pm$4.29 & 71.96$\\pm$0.19 & 81.72$\\pm$0.05 & 73.45$\\pm$0.06 & 11.68$\\pm$0.07 & 9.39$\\pm$0.07 & 78.20$\\pm$2.78 & 64.76$\\pm$3.91 & 74.19$\\pm$2.57 & 2.44$\\pm$1.29 & 1.72$\\pm$1.08 & 9.69\\\\\n \n& EDITS & 69.41$\\pm$2.33 & 81.55$\\pm$0.59 & 71.60$\\pm$0.89 & 4.05$\\pm$4.48 & 3.89$\\pm$4.23 & 73.01$\\pm$0.11 & 81.81$\\pm$0.28 & 73.51$\\pm$0.30 & 10.90$\\pm$1.22 & 8.75$\\pm$1.21 & 86.44$\\pm$2.17 & 75.58$\\pm$3.77 & 84.49$\\pm$2.27 & 6.64$\\pm$0.39 & 7.51$\\pm$1.20 & 9.89\\\\\n  \n%  & EDITS & 69.99$\\pm$3.38 & 81.98$\\pm$0.55 & 70.40$\\pm$1.26 & 5.38$\\pm$2.95 & 9.88$\\pm$4.15 & 71.80$\\pm$0.92 & 82.20$\\pm$1.38 & 73.81$\\pm$1.43 & 7.91$\\pm$2.44 & 7.59$\\pm$2.13 & 85.64$\\pm$2.54 & 73.44$\\pm$6.44 & 82.85$\\pm$3.02 & 9.11$\\pm$1.70 & 7.54$\\pm$3.02 & 9.89\\\\\n  %& EDITS & 69.34$\\pm$4.69 & 79.93$\\pm$2.07 & 70.88$\\pm$1.86 & 5.85$\\pm$0.57 & 2.38$\\pm$2.25 & 71.96 & 81.97 & 73.54 & 14.49 & 11.69 & 85.00 & 73.30 & 83.24 & 10.54 & 12.75 \\\\\n  \n & FairGNN & 67.35$\\pm$2.13 & 82.01$\\pm$0.26 & 69.68$\\pm$0.30 & 3.49$\\pm$2.15 & 3.40$\\pm$2.15 & 71.95$\\pm$1.43 & 81.84$\\pm$1.19 & 73.41$\\pm$1.24 & 12.64$\\pm$2.11 & 10.41$\\pm$2.03 & 87.36$\\pm$0.90 & 77.50$\\pm$1.69 & 82.94$\\pm$1.67 & 6.90$\\pm$0.17 & 4.65$\\pm$0.14 & 9.17\\\\\n \n & FairVGNN  & 72.41$\\pm$2.10& 82.14$\\pm$0.42 & 70.16$\\pm$0.86 & 1.71$\\pm$1.68 & \\textcolor{blue}{0.88$\\pm$0.58} & 71.34$\\pm$0.41 & 87.08$\\pm$0.74 & 78.04$\\pm$0.33 & 5.02$\\pm$5.22 & 3.60$\\pm$4.31 & 85.68$\\pm$0.37 & 79.11$\\pm$0.33 & 84.73$\\pm$0.46 & 6.53$\\pm$0.67 & 4.95$\\pm$1.22 & 5.67\\\\\n \n \\hline\n \n\\multirow{5}{*}{\\textbf{GIN}} & Vanilla & 72.71$\\pm$1.44 & \\textcolor{blue}{82.78$\\pm$0.50} & \\textcolor{red}{73.84$\\pm$0.54} & 13.56$\\pm$5.23 & 9.47$\\pm$4.49 & 74.36$\\pm$0.21 & 82.28$\\pm$0.64 & 74.02$\\pm$0.73 & 14.48$\\pm$2.44 & 12.35$\\pm$2.86 & 86.14$\\pm$0.25 & 76.49$\\pm$0.57 & 81.70$\\pm$0.67 & 8.55$\\pm$1.61 & 6.99$\\pm$1.51 & 9.56\\\\\n\n & NIFTY & 67.61$\\pm$4.88 & 80.46$\\pm$3.06 & 69.92$\\pm$3.64 & 5.26$\\pm$3.24 & 5.34$\\pm$5.67 & 70.90$\\pm$0.24 & 84.05$\\pm$0.82 & 75.59$\\pm$0.66 & 7.09$\\pm$4.62 & 6.22$\\pm$3.26 & 82.33$\\pm$4.61 & 70.64$\\pm$6.73 & 74.46$\\pm$9.98 & 5.57$\\pm$1.11 & 3.41$\\pm$1.43 & 8.56\\\\\n \n \n & EDITS & 69.35$\\pm$1.64 & 82.80$\\pm$0.22 & 72.08$\\pm$0.66 & 0.86$\\pm$0.76 & 1.72$\\pm$1.14 & 72.35$\\pm$1.11 & 82.47$\\pm$0.85 & 74.07$\\pm$0.98 & 14.11$\\pm$14.45 & 15.40$\\pm$15.76 & 80.19$\\pm$4.62 & 68.07$\\pm$5.30 & 73.74$\\pm$5.12 & 6.71$\\pm$2.35 & 5.98$\\pm$3.66 & 11.36\\\\\n%  & EDITS & 69.46$\\pm$4.18 & 82.37$\\pm$0.31 & 70.32$\\pm$0.44 & 9.35$\\pm$2.63 & 15.69$\\pm$1.67 & 70.91$\\pm$3.56 & 82.19$\\pm$1.20 & 73.57$\\pm$1.77 & 9.56$\\pm$9.69 & 11.24$\\pm$11.92 & 77.73$\\pm$9.93 & 65.81$\\pm$11.65 & 74.88$\\pm$8.46 & 7.06$\\pm$5.93 & 5.48$\\pm$7.86 & 11.36\\\\\n \n & FairGNN & 72.95$\\pm$0.82 & \\textcolor{red}{83.16$\\pm$0.56} & 72.24$\\pm$1.44 & 6.88$\\pm$4.42 & 2.06$\\pm$1.46 & 68.66$\\pm$4.48 & 79.47$\\pm$5.29 & 70.33$\\pm$5.50 & \\textcolor{blue}{4.67$\\pm$3.06} & 3.94$\\pm$1.49 & 86.14$\\pm$0.89& 73.67$\\pm$1.17& 77.90$\\pm$2.21& 6.33$\\pm$1.49& 4.74$\\pm$1.64 & 7.64\\\\\n \n & FairVGNN & 71.65$\\pm$1.90 & 82.40$\\pm$0.14 & 70.16$\\pm$ 0.32 & \\textcolor{red}{0.43$\\pm$0.54} & \\textcolor{red}{0.34$\\pm$0.41} & 71.36$\\pm$0.72 & \\textcolor{blue}{87.44$\\pm$0.23} & \\textcolor{blue}{78.18$\\pm$0.20} & \\textcolor{red}{2.85$\\pm$2.01} & \\textcolor{red}{1.72$\\pm$1.80} & 83.22$\\pm$1.60 & 76.36$\\pm$2.20 & 83.86$\\pm$1.57 & 5.67$\\pm$0.76 & 5.77$\\pm$1.26 & \\textcolor{blue}{5.44}\\\\\n \n \\hline\n \n\\multirow{5}{*}{\\textbf{SAGE}} & Vanilla & \\textcolor{red}{75.74$\\pm$0.69} & 81.25$\\pm$1.72 & 72.24$\\pm$1.61 & 24.30$\\pm$6.93 & 15.55$\\pm$7.59 & \\textcolor{red}{74.58$\\pm$1.31} & 83.38$\\pm$0.77 & 75.28$\\pm$0.83 & 15.65$\\pm$1.30 & 13.34$\\pm$1.34 & 90.71$\\pm$0.69 & 80.99$\\pm$0.55 & 86.72$\\pm$0.48 & 2.16$\\pm$1.53 & \\textcolor{red}{0.84$\\pm$0.55} & 7.31\\\\\n\n & NIFTY & 72.05$\\pm$2.15 & 79.20$\\pm$1.19 & 69.60$\\pm$1.50 & 7.74$\\pm$7.80 & 5.17$\\pm$2.38 & 72.89$\\pm$0.44 & 82.60$\\pm$1.25 & 74.39$\\pm$1.35 & 10.65$\\pm$1.65 & 8.10$\\pm$1.91 & \\textcolor{red}{92.04$\\pm$0.89} & 77.81$\\pm$6.03 & 84.11$\\pm$5.49 & 5.74$\\pm$0.38 & 4.07$\\pm$1.28 & 8.06 \\\\\n \n & EDITS & 69.76$\\pm$5.46 & 81.04$\\pm$1.09 & 71.68$\\pm$1.25 & 8.42$\\pm$7.35 & 5.69$\\pm$2.16 & 75.04$\\pm$0.12 & 82.41$\\pm$0.52 & 74.13$\\pm$0.59 & 11.34$\\pm$6.36 & 9.38$\\pm$5.39 & 89.07$\\pm$2.26 & 77.83$\\pm$3.79 & 84.42$\\pm$2.87 & 3.74$\\pm$3.54 & 4.46$\\pm$3.50 & 11.36\\\\\n%  & EDITS & 71.02$\\pm$5.03 & 79.48$\\pm$1.33 & 70.48$\\pm$1.48 & 6.01$\\pm$4.24 & 10.32$\\pm$6.02 & 73.56$\\pm$1.25 & 82.31$\\pm$0.65 & 74.02$\\pm$0.72 & 10.51$\\pm$10.90 & 11.01$\\pm$11.64 & 88.45$\\pm$1.56 & 76.51$\\pm$3.16 & 83.49$\\pm$2.39 & 8.53$\\pm$7.05 & 7.60$\\pm$3.40 & 9.75\\\\\n \n & FairGNN & 65.85$\\pm$9.49 & 82.29$\\pm$0.32 & 70.64$\\pm$0.74 & 7.65$\\pm$8.07 & 4.18$\\pm$4.86 & 70.82$\\pm$0.74  & 83.97$\\pm$2.00 & 75.29$\\pm$1.62 & 6.17$\\pm$5.57 & 5.06$\\pm$4.46 & 91.53$\\pm$0.38 & \\textcolor{blue}{82.55$\\pm$0.98} & \\textcolor{blue}{87.68$\\pm$0.73} & \\textcolor{blue}{1.94$\\pm$0.82} & 1.72$\\pm$0.70 & 5.83\\\\\n \n & FairVGNN & 73.84$\\pm$0.52 & 81.91$\\pm$0.63 & 70.00$\\pm$0.25 & \\textcolor{blue}{1.36$\\pm$1.90} & 1.22$\\pm$1.49 & \\textcolor{blue}{74.05$\\pm$0.20} & \\textcolor{red}{87.84$\\pm$0.32} & \\textcolor{red}{79.94$\\pm$0.30} & 4.94$\\pm$1.10 & \\textcolor{blue}{2.39$\\pm$0.71} & \\textcolor{blue}{91.56$\\pm$1.71} & \\textcolor{red}{83.58$\\pm$1.88} & \\textcolor{red}{88.41$\\pm$1.29} & \\textcolor{red}{1.14$\\pm$0.67} & \\textcolor{blue}{1.69$\\pm$1.13} & \\textcolor{red}{2.92}\\\\\n \\hline\n\\end{tabular}\n\\label{tab-main}\n\\vskip -3ex\n\\end{table*}",
            "tab-ablation": "\\begin{table*}[t!]\n\\tiny\n\\setlength{\\extrarowheight}{.095pt}\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\caption{Model utility and bias of node classification of different variants of FairVGNN. The best and runner-up results are colored in \\textcolor{red}{red} and \\textcolor{blue}{blue}. $\\uparrow$ represents the larger, the better while $\\downarrow$ represents the opposite.}\n\\vskip -3.5ex\n\\begin{tabular}{l|l|ccccc|ccccc|cccccc}\n\\hline\n\\multirow{2}{*}{\\textbf{Encoder}} & \\multirow{2}{*}{\\textbf{Model Variants}} & \\multicolumn{5}{c|}{\\textbf{German}} & \\multicolumn{5}{c|}{\\textbf{Credit}} & \\multicolumn{5}{c}{\\textbf{Bail}} \\\\\n &  & AUC ($\\uparrow$) & F1 ($\\uparrow$) & ACC ($\\uparrow$) & $\\Delta_{\\text{sp}}$ ($\\downarrow$) & $\\Delta_{\\text{eo}}$ ($\\downarrow$) & AUC ($\\uparrow$) & F1 ($\\uparrow$) & ACC ($\\uparrow$) & $\\Delta_{\\text{sp}}$ ($\\downarrow$) & $\\Delta_{\\text{eo}}$ ($\\downarrow$) & AUC ($\\uparrow$) & F1 ($\\uparrow$) & ACC ($\\uparrow$) & $\\Delta_{\\text{sp}}$ ($\\downarrow$) & $\\Delta_{\\text{eo}}$ ($\\downarrow$) \\\\\n \\hline\n\\multirow{4}{*}{\\textbf{GCN}} & FairV & 72.69$\\pm$ 1.67 & 81.86$\\pm$ 0.49 & 69.84$\\pm$0.41 & 0.77$\\pm$ 0.39 & 0.46$\\pm$ 0.34 & 71.34$\\pm$0.41 & 87.08$\\pm$0.74 & 78.04$\\pm$0.33 & 5.02$\\pm$5.22 & 3.60$\\pm$4.31 & 85.68$\\pm$0.37 & 79.11$\\pm$0.33 & 84.73$\\pm$0.46 & 6.53$\\pm$0.67 & 4.95$\\pm$1.22 \\\\\n & FairV w/o fm & 73.63$\\pm$ 1.14 & 82.28$\\pm$0.28 & 70.88$\\pm$1.09 & 5.56$\\pm$3.89 & 4.41$\\pm$3.59 & 72.51$\\pm$0.32 & 86.15$\\pm$2.18 & 77.83$\\pm$2.15 & 6.94$\\pm$2.86 & 4.64$\\pm$2.73 & 86.98$\\pm$0.32 & 78.08$\\pm$0.53 & 84.59$\\pm$0.29 & 7.24$\\pm$0.26 & 5.75$\\pm$0.68 \\\\\n & FairV w/o wc & 72.08$\\pm$ 1.83 & 82.72$\\pm$ 0.50 & 71.04$\\pm$ 1.23 & 3.19$\\pm$ 3.51 & 0.59$\\pm$ 1.12 & 71.80$\\pm$0.47 & 87.27$\\pm$0.47 & 78.47$\\pm$0.34 & 9.05$\\pm$4.55 & 5.94$\\pm$3.61 & 85.93$\\pm$0.38 & 79.22$\\pm$0.29 & 85.38$\\pm$0.25 & 6.61$\\pm$0.48 & 5.82$\\pm$0.66 \\\\\n & FairV w/o fm\\&wc & 74.97$\\pm$0.94 & 82.30$\\pm$0.67 & 70.8$\\pm$0.88 & 7.74$\\pm$5.05 & 4.56$\\pm$4.15 & 73.09$\\pm$0.41 & 84.48$\\pm$2.14 & 76.40$\\pm$2.29 & 11.91$\\pm$2.34 & 9.27$\\pm$1.98 & 86.44$\\pm$0.16 & 78.75$\\pm$0.27 & 84.41$\\pm$0.28 & 8.32$\\pm$0.60 & 6.34$\\pm$0.32 \\\\\n \\hline\n\\multirow{4}{*}{\\textbf{GIN}} & FairV & 71.65$\\pm$1.90 & 82.40$\\pm$0.14 & 70.16$\\pm$0.32 & \\textcolor{red}{0.43$\\pm$0.54} & \\textcolor{red}{0.34$\\pm$0.41} & 71.36$\\pm$0.72 & 87.44$\\pm$0.23 & 78.18$\\pm$0.20 & \\textcolor{red}{2.85$\\pm$2.01} & \\textcolor{blue}{1.72$\\pm$1.80} & 83.22$\\pm$1.60 & 76.36$\\pm$2.20 & 83.86$\\pm$1.57 & 5.67$\\pm$0.76 & 5.77$\\pm$1.26 \\\\\n & FairV w/o fm & 73.76$\\pm$0.77 & \\textcolor{blue}{83.06$\\pm$0.67} & \\textcolor{blue}{71.68$\\pm$1.63} & 2.76$\\pm$2.64 & \\textcolor{blue}{0.57$\\pm$0.47} & 71.15$\\pm$0.63 & 87.09$\\pm$0.7 & 78.29$\\pm$0.53 & 3.36$\\pm$2.34 & 1.86$\\pm$1.19 & 85.12$\\pm$0.54 & 77.06$\\pm$0.83 & 83.13$\\pm$1.19 & 6.80$\\pm$0.28 & 5.97$\\pm$0.64 \\\\\n & FairV w/o wc & 72.65$\\pm$1.65 & 82.70$\\pm$0.30 & 71.20$\\pm$1.01 & 3.44$\\pm$3.19 & 0.97$\\pm$0.9 & 71.13$\\pm$0.59 & \\textcolor{blue}{87.96$\\pm$0.25} & \\textcolor{blue}{80.04$\\pm$0.22} & \\textcolor{blue}{3.16$\\pm$1.28} & \\textcolor{red}{1.47$\\pm$0.72} & 85.09$\\pm$2.36 & 79.07$\\pm$2.70 & \\textcolor{blue}{85.85$\\pm$2.13} & 5.24$\\pm$1.41 & 4.33$\\pm$2.05 \\\\\n & FairV w/o fm\\&wc & 73.41$\\pm$1.17 & \\textcolor{red}{83.20$\\pm$0.44} & \\textcolor{red}{72.40$\\pm$1.29} & 5.70$\\pm$4.57 & 1.01$\\pm$1 & 72.73$\\pm$0.32 & 86.10$\\pm$0.59 & 77.90$\\pm$0.63 & 6.66$\\pm$1.10 & 3.97$\\pm$0.41 & 86.32$\\pm$1.60 & 79.28$\\pm$1.39 & \\textcolor{red}{86.02$\\pm$0.40} & 7.48$\\pm$0.71 & 7.43$\\pm$2.38 \\\\\n \\hline\n\\multirow{4}{*}{\\textbf{SAGE}} & FairV & 73.84$\\pm$0.52 & 81.91$\\pm$0.63 & 70.00$\\pm$0.25 & \\textcolor{blue}{1.36$\\pm$1.90} & 1.22$\\pm$1.49 & 74.05$\\pm$0.20 & 87.84$\\pm$0.32 & 79.94$\\pm$0.30 & 4.94$\\pm$1.10 & 2.39$\\pm$0.71 & 91.56$\\pm$1.71 & 83.58$\\pm$1.88 & 88.41$\\pm$1.29 & \\textcolor{red}{1.14$\\pm$0.67} & 1.69$\\pm$1.13 \\\\\n & FairV w/o fm & \\textcolor{blue}{73.98$\\pm$1.40} & 81.36$\\pm$1.45 & 70.00$\\pm$1.50 & 3.67$\\pm$2.80 & 1.55$\\pm$2.01 & 73.58$\\pm$0.68 & 83.18$\\pm$2.32 & 74.97$\\pm$2.49 & 7.23$\\pm$3.91 & 5.05$\\pm$3.17 & 91.96$\\pm$0.57 & 84.04$\\pm$1.01 & \\textcolor{blue}{88.69$\\pm$0.79} & \\textcolor{blue}{1.51$\\pm$1.17} & \\textcolor{blue}{1.59$\\pm$0.35} \\\\\n & FairV w/o wc & 73.93$\\pm$2.16 & 82.02$\\pm$0.72 & 70.16$\\pm$1.25 & 2.80$\\pm$2.79 & 0.90$\\pm$1.06 & \\textcolor{blue}{74.05$\\pm$0.42} & \\textcolor{red}{88.10$\\pm$0.30} & \\textcolor{red}{80.16$\\pm$0.19} & 5.09$\\pm$1.30 & 2.67$\\pm$0.92 & \\textcolor{blue}{92.01$\\pm$0.74} & 84.64$\\pm$0.91 & \\textcolor{red}{89.24$\\pm$0.58} & 2.99$\\pm$0.94 & \\textcolor{red}{1.07$\\pm$1.19} \\\\\n & FairV w/o fm\\&wc & 73.87$\\pm$1.62 & 80.09$\\pm$1.73 & 70.08$\\pm$1.17 & 6.18$\\pm$1.31 & 4.68$\\pm$2.38 & \\textcolor{red}{74.57$\\pm$0.14} & 81.91$\\pm$0.92 & 73.61$\\pm$1.02 & 7.27$\\pm$3.22 & 5.03$\\pm$3.01 & \\textcolor{red}{92.05$\\pm$0.89} & 83.40$\\pm$1.79 & 88.44$\\pm$1.02 & 3.51$\\pm$0.87 & 2.05$\\pm$1.19\\\\\n \\hline\n\\end{tabular}\n\\label{tab-ablation}\n%\\vspace{-1ex}\n\\vskip -3.5ex\n\\end{table*}",
            "tab-fairnesswc": "\\begin{table}[t]%[htbp!]\n\\footnotesize\n\\setlength{\\extrarowheight}{.11pt}\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\caption{Comparison with different weight regularization.}\n\\vspace{-2.5ex}\n\\label{tab-fairnesswc}\n\\begin{tabular}{l|l|ccccc}\n\\hline\n\\begin{tabular}{@{}c@{}} \\textbf{Dataset} \\\\ \\textbf{(Model)}\\end{tabular} & \\multirow{1}{*}{\\textbf{Strategy}} & AUC ($\\uparrow$) & ACC ($\\uparrow$) & F1 ($\\uparrow$) & $\\Delta_{\\text{sp}}$ ($\\downarrow$) & $\\Delta_{\\text{eo}}$ ($\\downarrow$) \\\\\n \\hline\n\\multirow{3}{*}{\\begin{tabular}{@{}c@{}} \\textbf{German} \\\\ \\textbf{(SAGE)}\\end{tabular}} & Ad wc & \\textcolor{red}{73.84+0.52} & 70.00+0.25 & 81.91+0.63 & \\textcolor{red}{1.36+1.90} & \\textcolor{red}{1.22+1.49} \\\\\n & Wc & 72.43+1.60 & \\textcolor{red}{70.48+0.85} & \\textcolor{red}{82.03+0.82} & 4.85+4.10 & 2.50+2.12 \\\\\n & Sn & 73.00+1.53 & 70.00+1.07 & 81.82+0.59 & 3.74+3.22 & 1.89+1.08 \\\\\n \\hline\n \n\\multirow{3}{*}{\\begin{tabular}{@{}c@{}} \\textbf{Credit} \\\\ \\textbf{(GIN)}\\end{tabular}}  & Ad wc & \\textcolor{red}{74.05$\\pm$0.20} & \\textcolor{red}{79.94$\\pm$0.19} & \\textcolor{red}{87.84$\\pm$0.32} & 4.94$\\pm$1.10 & 2.39$\\pm$0.71 \\\\\n & Wc & 73.20$\\pm$1.20 & 79.03$\\pm$1.09 & 87.23$\\pm$0.94 & 7.03$\\pm$4.58 & 4.74$\\pm$3.47 \\\\\n & Sn & 71.12$\\pm$0.55 & 78.54$\\pm$2.00 & 86.53$\\pm$1.90 & \\textcolor{red}{2.60$\\pm$0.73} & \\textcolor{red}{0.87$\\pm$0.54}\\\\\n \\hline\n \n\\multirow{3}{*}{\\begin{tabular}{@{}c@{}} \\textbf{Bail} \\\\ \\textbf{(GCN)}\\end{tabular}} & Ad wc & 85.68+0.37 & 84.73+0.46 & 79.11+0.33 & \\textcolor{red}{6.53+0.67} & \\textcolor{red}{4.95+1.22} \\\\\n & Wc & 85.97+0.45 & 85.12+0.26 & 79.08+0.28 & 6.86+0.47 & 5.85+0.83 \\\\\n & Sn & \\textcolor{red}{86.10+0.61} & \\textcolor{red}{85.69+0.42} & \\textcolor{red}{79.66+0.63} & 7.53+0.17 & 6.43+0.81\\\\\n \\hline\n\n\\end{tabular}\n\\begin{tablenotes}\n  \\scriptsize\n  \\item \\textbf{*} \\textbf{Ad wc}: adaptively clamp weights of the encoder; \\textbf{Wc}: clamp weights of the encoder; and \\\\\\hspace{2.25ex}\\textbf{Sn}: spectral normalization of the encoder\n\\end{tablenotes}\n\\vskip -4ex\n\\end{table}",
            "tab-fullwc": "\\begin{table*}[h]\n\\scriptsize\n\\setlength{\\extrarowheight}{.095pt}\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\caption{Detailed comparison with different weight regularization strategies.}\n\\vskip -2.25ex\n\\label{tab-fullwc}\n\\begin{tabular}{l|l|ccccc|ccccc|ccccc}\n\\hline\n\\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Strategy}} & \\multicolumn{5}{c|}{\\textbf{German}} & \\multicolumn{5}{c|}{\\textbf{Credit}} & \\multicolumn{5}{c}{\\textbf{Bail}} \\\\\n &  & AUC $(\\uparrow)$ & ACC $(\\uparrow)$ & F1 $(\\uparrow)$ & $\\Delta_{\\text{sp}} (\\downarrow)$ & $\\Delta_{\\text{eo}} (\\downarrow)$ & AUC $(\\uparrow)$ & ACC $(\\uparrow)$ & F1 $(\\uparrow)$ & $\\Delta_{\\text{sp}} (\\downarrow)$ & $\\Delta_{\\text{eo}} (\\downarrow)$ & AUC $(\\uparrow)$ & ACC $(\\uparrow)$ & F1 $(\\uparrow)$ & $\\Delta_{\\text{sp}} (\\downarrow)$ & $\\Delta_{\\text{eo}} (\\downarrow)$ \\\\\n \\hline\n\\multirow{3}{*}{\\textbf{GCN}} & Ad wc & 72.41+2.10 & 70.16+0.86 & 82.15+0.42 & 1.71+1.68 & 0.88+0.58 & 71.34+0.41 & 78.04+0.33 & 87.08+0.74 & 5.02+5.22 & 3.60+4.31 & 85.68+0.37 & 84.73+0.46 & 79.11+0.33 & 6.53+0.67 & 4.95+1.22 \\\\\n & Wc & 73.34+0.91 & 70.08+0.59 & 81.91+0.38 & 4.54+2.98 & 4.22+3.35 & 69.61+4.11 & 77.79+0.98 & 86.70+0.72 & 4.67+4.01 & 3.48+3.02 & 85.97+0.45 & 85.12+0.26 & 79.08+0.28 & 6.86+0.47 & 5.85+0.83 \\\\\n & Sn & 71.15+0.95 & 70.24+0.48 & 82.01+0.35 & 3.94+5.37 & 2.82+3.76 & 70.16+2.65 & 75.96+1.40 & 84.87+2.03 & 5.67+4.53 & 4.45+3.62 & 86.10+0.61 & 85.69+0.42 & 79.66+0.63 & 7.53+0.17 & 6.43+0.81 \\\\\n \\hline\n\\multirow{3}{*}{\\textbf{GIN}} & Ad wc & 71.65+1.90 & 70.16+0.32 & 82.40+0.14 & 0.43+0.54 & 0.34+0.41 & 74.05+0.42 & 80.16+0.19 & 88.10+0.30 & 5.09+1.30 & 2.67+0.92 & 83.22+1.60 & 83.86+1.57 & 76.36+2.20 & 5.67+0.76 & 5.77+1.26 \\\\\n & Wc & 71.62+2.32 & 71.04+0.60 & 82.76+0.22 & 1.82+0.82 & 0.48+0.40 & 73.20+1.20 & 79.03+1.09 & 87.23+0.94 & 7.03+4.58 & 4.74+3.47 & 84.67+0.67 & 84.79+1.45 & 78.43+1.11 & 7.73+0.44 & 6.96+1.26 \\\\\n & Sn & 71.25+1.69 & 72.40+1.10 & 82.98+0.68 & 7.73+5.03 & 2.37+1.42 & 71.12+0.55 & 78.54+2.00 & 86.53+1.90 & 2.60+0.73 & 0.87+0.54 & 85.47+0.74 & 85.26+1.28 & 79.13+1.08 & 7.07+1.72 & 5.90+2.06 \\\\\n \\hline\n\\multirow{3}{*}{\\textbf{SAGE}} & Ad wc & 73.84+0.52 & 70.00+0.25 & 81.91+0.63 & 1.36+1.90 & 1.22+1.49 & 74.05+0.20 & 79.94+0.30 & 87.84+0.32 & 4.94+1.10 & 2.39+0.71 & 91.56+1.71 & 88.41+1.29 & 83.58+1.88 & 1.14+0.67 & 1.69+1.13 \\\\\n & Wc & 72.43+1.60 & 70.48+0.85 & 82.03+0.82 & 4.85+4.10 & 2.50+2.12 & 73.20+1.21 & 79.03+1.09 & 87.23+0.94 & 7.03+4.58 & 4.74+3.47 & 91.48+0.57 & 88.42+0.78 & 83.51+0.88 & 3.45+1.06 & 1.89+1.27 \\\\\n & Sn & 73.00+1.53 & 70.00+1.07 & 81.82+0.59 & 3.74+3.22 & 1.89+1.08 & 73.86+0.54 & 78.57+1.70 & 86.55+1.67 & 5.27+3.26 & 3.78+2.51 & 93.36+1.75 & 89.88+1.07 & 85.32+1.71 & 2.55+1.19 & 1.52+1.27\\\\\n \\hline\n\\end{tabular}\n\\begin{tablenotes}\n  \\footnotesize\n  \\item \\textbf{*} \\textbf{Ad wc}: adaptively clamp weights of the encoder;  \\textbf{Wc}: clamp weights of the encoder; and \\textbf{Sn}: spectral normalization of the encoder.\n%   \\item \\textbf{*} Ad wc: adaptively clamp weights of the encoder.\n  \n%   \\item \\textbf{*} Wc: clamp weights of the encoder.\n  \n%   \\item \\textbf{*} Sn: spectral normalization of the encoder\n\\end{tablenotes}\n\\vskip -2.25ex\n\\end{table*}",
            "tb:symbols": "\\begin{table}[htbp!]\n% \\small\n% \\vspace{0.3cm}\n\\setlength{\\extrarowheight}{.095pt}\n\\setlength\\tabcolsep{2pt}\n\\caption{Notations commonly used in this paper and the corresponding descriptions.} \n\\vspace{-1ex}\n\\label{tb:symbols}\n\\begin{tabular}{cc}\n\\hline\n\\hline\n\\textbf{Notations}       & \\textbf{Definitions or Descriptions} \\\\\n\\hline\n\\hline\n$G$   &  input graph\\\\\n$\\mathcal{V}$, $\\mathcal{E}$   &  node, edge set\\\\\n$\\mathbf{A}$   &  adjacency matrix\\\\\n% \\textcolor{purple}{$\\mathbf{X}$, $\\mathbf{A}$}  &  \\textcolor{purple}{node attribute and adjacency matrix}\\\\\n$\\mathbf{X}$  &  node attribute matrix\\\\\n$\\mathbf{m}$ & feature mask\\\\\n$\\tau$ & temperature factor\\\\\n$\\chi$ & network homophily\\\\\n$\\epsilon$ & prefix cutting threshold\\\\\n$\\mathbf{S}$   &  sensitive feature vector\\\\\n$\\widetilde{\\mathbf{X}}$  &  generated node attribute matrix\\\\\n$\\mathbf{Y}$   &  one-hot encoded label matrix for all nodes\\\\\n\n$\\Delta_{\\text{sp}}, \\Delta_{\\text{eo}}$   &  statistical parity and equality of opportunity\\\\\n$y, s$   &  class label and sensitive group label\\\\\n$\\rho_i$   &  the Pearson correlation coefficient of the $i^{\\text{th}}$ channel\\\\\n$\\mathbf{H}^L$ & representation learned after $L$-layers GNNs\\\\\n$\\mathbf{W}^{f,1}$ & weight of the first layer of the encoder\\\\\n\n$g, d, f, c$ & generator, discriminator, encoder and classifier\\\\\n% $\\bm{\\Theta}_g, \\bm{\\Theta}_d, \\bm{\\Theta}_f, \\bm{\\Theta}_c$ & paramters of generator, discriminator, encoder and classifier\\\\\n\n\\hline\n\\hline\n\\end{tabular}\n\\vspace{1ex}\n\\end{table}"
        },
        "figures": {
            "fig-prelim": "\\begin{figure*}[t]\n     \\centering\n     %\\vskip -0.25ex\n     \\includegraphics[width=1.0\\textwidth]{figure/prelimanalysis.png}\n     \\vskip -3.75ex\n     \\caption{Initial empirical investigation on sensitive leakage and correlation variation on German dataset. (a)-(b) visualize the relationships between model utility/fairness and the sensitive correlation $\\boldsymbol{\\rho}_i$ of each masked feature channel\\protect\\footnotemark. Masking channel with less sensitive correlation leads to more biased predictions and sometimes higher model utility. (c)-(d) shows the correlation variation caused by feature propagation on German and Credit datasets. In (c), we can see sensitive correlations of the 2$^{\\text{nd}}$ and 7$^{\\text{th}}$ feature channel significantly change after propagation while in (d), the correlations do not change so much.}\n     \\label{fig-prelim}\n     \\vskip -1.75ex\n\\end{figure*}",
            "fig-fairvgnn": "\\begin{figure}[t!]\n     \\centering\n     \\includegraphics[width=.5\\textwidth]{figure/framework.png}\n    %  \\vskip -1.25ex\n     \\caption{An overview of the Fair View Graph Neural Network (FairVGNN), with two main modules: (a) generative adversarial debiasing to learn fair view of features and (b) adaptive weight clamping to clamp weights of sensitive-related channels of the encoder.}\n     \\label{fig-fairvgnn}\n    %  \\vskip -4ex\n\\end{figure}",
            "fig-adv_ablation": "\\begin{figure}[t]%[htbp!]\n    \\vskip -1ex\n     \\centering\n     \\hspace{-1ex}\n     \\includegraphics[width=0.45\\textwidth]{figure/adv_ablation.pdf}\n     \\vskip -2.75ex\n      \\caption{\\hspace{-0.5ex} Model bias without the discriminator/generator.}\n     \\label{fig-adv_ablation}\n    %  \\vskip -2.5ex\n    \\vspace{-0.5ex}\n\\end{figure}",
            "fig-layer": "\\begin{figure}[t]%[htbp!]\n     \\centering\n     \\hspace{-3.5ex}\n     \\begin{subfigure}[b]{0.235\\textwidth}\n         \\centering\n         \\includegraphics[width=1.07\\textwidth]{figure/wcgermansage.pdf}\n         \\vskip -0.25ex\n         \\caption{German}\n         \\label{fig-germansagewc}\n     \\end{subfigure}\n     \\hspace{.25ex}\n     \\begin{subfigure}[b]{0.235\\textwidth}\n         \\centering\n         \\includegraphics[width=1.07\\textwidth]{figure/wccreditsage.pdf}\n         \\vskip -0.25ex\n         \\caption{Credit}\n         \\label{fig-creditsagewc}\n     \\end{subfigure}\n     \\vskip -2.25ex\n     \\caption{Results of different prefix cutting threshold.}\n     \\label{fig-layer}\n     \\vskip -1ex\n\\end{figure}",
            "fig-corrvar": "\\begin{figure}[t]\n     \\centering\n     %\\hspace{-1.5ex}\n     \\begin{subfigure}[b]{0.45\\textwidth}\n         \\centering\n         \\hspace{-8.5ex}\n         \\includegraphics[width=0.93\\textwidth]{figure/germancorrelation_change_simple.pdf}\n         \\vskip -1.5ex\n         \\caption{German (0.8048)}\n         \\label{fig-germancorrfull}\n     \\end{subfigure}\n     %\\hspace{-1ex}\n     \\vskip 1.5ex\n     \\begin{subfigure}[b]{0.45\\textwidth}\n         \\centering\n         \\hspace{-8.5ex}\n         \\includegraphics[width=0.93\\textwidth]{figure/creditcorrelation_change_simple.pdf}\n         \\vskip -1.5ex\n         \\caption{Credit (0.9595)}\n         \\label{fig-creditcorrfull}\n     \\end{subfigure}\n     \\vskip -1.5ex\n     \\caption{Correlation variation after feature propagation on the German and Credit datasets with the parentheses next to each dataset denoting the network homophily.}\n     \\label{fig-corrvar}\n    %  \\vskip -3.5ex\n\\end{figure}"
        },
        "equations": {
            "eq:eq-spdelta": "\\begin{equation}\\label{eq-spdelta}\n    \\Delta_{\\text{sp}} = |P(\\hat{y} = 1|s = 0) - P(\\hat{y} = 1|s = 1)|,\n\\end{equation}",
            "eq:1": "\\begin{equation}\n\\vspace{-.5ex}\n    \\Delta_{\\text{eo}} = |P(\\hat{y} = 1|y = 1, s = 0) - P(\\hat{y} = 1 |y = 1, s = 1)|,\n\\end{equation}",
            "eq:eq-lc": "\\begin{equation}\\label{eq-lc}\n    \\boldsymbol{\\rho}_{i} = \\frac{\\mathbb{E}_{v_j\\sim\\mathcal{V}}\\big((\\mathbf{X}_{ji} - \\mu_{i})(\\mathbf{S}_j - \\mu_{s})\\big)}{\\sigma_{i}\\sigma_{s}}, \\forall i\\in \\{1, 2, ..., d\\},\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\widetilde{\\mathbf{X}} = \\mathbf{X}\\odot\\mathbf{m} = [\\mathbf{X}_{1}^{\\top}\\odot \\mathbf{m}, \\mathbf{X}_{2}^{\\top}\\odot \\mathbf{m}, ..., \\mathbf{X}_{n}^{\\top}\\odot \\mathbf{m}],\n\\end{equation}",
            "eq:3": "\\begin{align}\n    p_{ij} = \\frac{\\exp(\\frac{\\log(\\pi_{ij}) + g_{ij}}{\\tau})}{\\sum_{k= 1}^2\\exp(\\frac{\\log(\\pi_{ik}) + g_{ik}}{\\tau})}, \\forall j = 1, 2, i\\in\\{1, 2, ..., d\\},\n\\end{align}",
            "eq:4": "\\begin{equation}\n\\small\n    \\hat{\\mathbf{Y}} = c_{\\bm{\\Theta}_c}(\\mathbf{H}^L) = \\sigma\\big(\\text{MLP}_c(\\mathbf{H}^L)\\big),~~~\\hat{\\mathbf{S}} = d_{\\bm{\\Theta}_d}(\\mathbf{H}^L) = \\sigma\\big(\\text{MLP}_d(\\mathbf{H}^L)\\big),\n\\end{equation}",
            "eq:eq-minsp": "\\begin{equation}\\label{eq-minsp}\n    \\bm{\\Theta}_g^{*} =\\argmin_{\\bm{\\Theta}_g}\\Delta_{\\text{sp}} = \\argmin_{\\bm{\\Theta}_g}|P(\\hat{y} = 1| s = 0) - P(\\hat{y} = 1 | s = 1)|,\n\\end{equation}",
            "eq:eq-advd": "\\begin{equation}\\label{eq-advd}\n\\small\n\\max_{\\bm{\\Theta}_d}\\mathcal{L}_{\\text{d}} = \\mathbb{E}_{\\widetilde{\\mathbf{X}}\\sim\\mathbb{P}^{\\bm{\\Theta}_g}_{(\\widetilde{\\mathbf{X}}|\\mathbf{X})}}\n\\mathbb{E}_{v_i\\sim\\mathcal{V}}\n\\bigg(\\mathbf{S}_i\\log\\big(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^L_i)) + (1 - \\mathbf{S}_i)\\log(1 - d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^L_i)\\big)\\bigg),\n\\end{equation}",
            "eq:eq-advg": "\\begin{equation}\\label{eq-advg}\n\\small\n\\min_{\\bm{\\Theta}_g}\\mathcal{L}_{\\text{g}} =\\mathbb{E}_{\\widetilde{\\mathbf{X}}\\sim\\mathbb{P}^{\\bm{\\Theta}_g}_{(\\widetilde{\\mathbf{X}}|\\mathbf{X})}}\n\\mathbb{E}_{v_i\\sim\\mathcal{V}}\n\\big(d_{\\bm{\\Theta}_d}(\\widetilde{\\mathbf{H}}^L_i) - 0.5\\big)^2 + \\alpha||\\mathbf{m} - \\mathbf{1}_d||_2^2,\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\small\n\\min_{\\boldsymbol{\\theta}_c}\\mathcal{L}_{\\text{c}} = -\\mathbb{E}_{\\widetilde{\\mathbf{X}}\\sim\\mathbb{P}^{\\boldsymbol{\\theta}_g}_{(\\widetilde{\\mathbf{X}}|\\mathbf{X})}}\n\\mathbb{E}_{v_i\\sim\\mathcal{V}}\n\\bigg(\\mathbf{Y}_i\\log\\big(c_{\\boldsymbol{\\theta}_c}(\\widetilde{\\mathbf{H}}^L_i)\\big) + (1 - \\mathbf{Y}_i)\\log\\big(1 - c_{\\boldsymbol{\\theta}_c}(\\widetilde{\\mathbf{H}}^L_i)\\big)\\bigg)\n\\end{equation}",
            "eq:eq-wc": "\\begin{equation}\\label{eq-wc}\n    \\mathbf{W}^{f, 1}_{ij} = \\begin{cases}\n        \\mathbf{W}_{ij}^{f, 1}, & |\\mathbf{W}_{ij}^{f, 1}| \\le \\epsilon*\\mathbf{p}_j\\\\\n        \\text{sign}(\\mathbf{W}_{ij}^{f, 1}) * \\epsilon*\\mathbf{p}_j, & |\\mathbf{W}_{ij}^{f, 1}| > \\epsilon*\\mathbf{p}_j\n    \\end{cases},\n\\end{equation}",
            "eq:eq-propagation": "\\begin{equation}\\label{eq-propagation}\n    \\mathbf{h}_i^l = \\text{TRAN}^{l}(\\text{PROP}^{l}(\\mathbf{h}_{i}^{l - 1}, \\{\\mathbf{h}_j^{l - 1}|j\\in\\mathcal{N}_i\\})),\n\\end{equation}",
            "eq:eq:GCN": "\\begin{equation}\\label{eq:GCN}\n    \\mathbf{H}^{l} = \\widetilde{\\mathbf{D}}^{-0.5}(\\mathbf{A} + \\mathbf{I})\\widetilde{\\mathbf{D}}^{-0.5}\\mathbf{H}^{l - 1}\\mathbf{W}^{l},\n\\end{equation}",
            "eq:eq:GIN": "\\begin{equation}\\label{eq:GIN}\n    \\mathbf{H}^{l} = \\text{MLP}^l((\\mathbf{A} + (1 + \\alpha)\\mathbf{I})\\mathbf{H}^{l - 1}),\n\\end{equation}",
            "eq:eq:SAGE": "\\begin{equation}\\label{eq:SAGE}\n    \\mathbf{H}^{l} = \\mathbf{W}^{l, 1}\\mathbf{H}^{l - 1} + \\mathbf{W}^{l, 2}\\mathbf{D}^{-1}\\mathbf{A}\\mathbf{H}^{l - 1},\n\\end{equation}"
        },
        "git_link": "https://github.com/YuWVandy/FairVGNN"
    }
}