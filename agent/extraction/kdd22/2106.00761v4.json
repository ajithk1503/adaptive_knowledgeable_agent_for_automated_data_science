{
    "meta_info": {
        "title": "Motif Prediction with Graph Neural Networks",
        "abstract": "Link prediction is one of the central problems in graph mining. However,\nrecent studies highlight the importance of higher-order network analysis, where\ncomplex structures called motifs are the first-class citizens. We first show\nthat existing link prediction schemes fail to effectively predict motifs. To\nalleviate this, we establish a general motif prediction problem and we propose\nseveral heuristics that assess the chances for a specified motif to appear. To\nmake the scores realistic, our heuristics consider - among others -\ncorrelations between links, i.e., the potential impact of some arriving links\non the appearance of other links in a given motif. Finally, for highest\naccuracy, we develop a graph neural network (GNN) architecture for motif\nprediction. Our architecture offers vertex features and sampling schemes that\ncapture the rich structural properties of motifs. While our heuristics are fast\nand do not need any training, GNNs ensure highest accuracy of predicting\nmotifs, both for dense (e.g., k-cliques) and for sparse ones (e.g., k-stars).\nWe consistently outperform the best available competitor by more than 10% on\naverage and up to 32% in area under the curve. Importantly, the advantages of\nour approach over schemes based on uncorrelated link prediction increase with\nthe increasing motif size and complexity. We also successfully apply our\narchitecture for predicting more arbitrary clusters and communities,\nillustrating its potential for graph mining beyond motif analysis.",
        "author": "Maciej Besta, Raphael Grob, Cesare Miglioli, Nicola Bernold, Grzegorz Kwasniewski, Gabriel Gjini, Raghavendra Kanakagiri, Saleh Ashkboos, Lukas Gianinazzi, Nikoli Dryden, Torsten Hoefler",
        "link": "http://arxiv.org/abs/2106.00761v4",
        "category": [
            "cs.SI",
            "cs.LG"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction and Motivation",
                "content": "\n\\label{sec:intro}\n\nOne of the central problems in graph mining and learning is link\nprediction~\\cite{lu2011link, al2006link, taskar2004link, al2011survey,\nzhang2018link, zhang2020revisiting}, in which one is interested in assessing\nthe likelihood that a given \\emph{pair of vertices} is, or may become, connected.\n%\n% In addition to predicting future interactions, this enables identifying missing\n% connections in the data.\n%\n%\\vspaceSQ{-0.3em}\n%\nHowever, recent works argue the importance of \\emph{higher-order graph\norganization}~\\cite{benson2016higher}, where one focuses on finding and\nanalyzing small recurring \\emph{subgraphs} called \\emph{motifs} (sometimes\nreferred to as \\emph{graphlets} or \\emph{graph patterns}) instead of individual\nlinks.\n%\nMotifs are central to many graph mining problems in computational biology,\nchemistry, and a plethora of other fields~\\cite{besta2021graphminesuite,\nbesta2021sisa, cook2006mining, jiang2013survey, horvath2004cyclic,\nchakrabarti2006graph, besta2019slim}.\n%\nSpecifically, motifs are building blocks of different networks, including\ntranscriptional regulation graphs, social networks, brain graphs,\nor air traffic patterns~\\cite{benson2016higher}.\n%\nThere exist many motifs, for example $k$-cliques, $k$-stars, $k$-clique-stars,\n$k$-cores, and others~\\cite{lee2010survey, jabbour2018pushing, besta2017push}. \n%\nFor example, cliques or quasi-cliques are crucial motifs in protein-protein\ninteraction networks~\\cite{bhattacharyya2009mining, li2005interaction}.\n%\nA huge number of works are dedicated to motif \\emph{counting}, \\emph{listing}\n(also called \\emph{enumeration}), or \\emph{checking for the existence} of a\ngiven motif~\\cite{besta2021graphminesuite, cook2006mining}.\n%\n% We illustrate some motifs in Figure~\\ref{fig:main}.\n%\nHowever, while a few recent schemes focus on predicting\n\\emph{triangles}~\\cite{benson2018simplicial, nassar2020neighborhood,\nnassar2019pairwise}, no works target the problem of \\emph{general motif\nprediction}, i.e., analyzing whether specified complex structures may\nappear in the data.\n%\nAs with link prediction, it would enable predicting the evolution of data, but\nalso finding missing structures in the available data. For example, one could\nuse motif prediction to find probable missing clusters of interactions in\nbiological (e.g., protein) networks, and use the outcomes to limit the\nnumber of expensive experiments conducted to find missing\nconnections~\\cite{lu2011link, martinez2016survey}.\n%\n% In addition to predicting future interactions, motif prediction would enable\n% identifying missing data (i.e., interactions between entities).\n\n\n\\vspaceSQT{-0.3em}\nIn this paper, we first (Section~\\ref{sec:motif-prediction}) establish and\nformally describe a general motif prediction problem, going beyond link\nprediction and showing how to predict\nhigher-order network patterns that will appear in the future (or which\nmay be missing from the data).\n%\nA key challenge is the appropriate \\emph{problem formulation}. Similarly to\nlink prediction, one wants a \\emph{score function} that -- for a given vertex\nset~$V_M$ -- assesses the chances for a given motif to appear. Still, the\nfunction must consider the combinatorially increased complexity of the problem (compared to\nlink prediction). \n%\n%\nIn general, contrary to a single link, a motif may be formed by an\n\\emph{arbitrary} set~$V_M$ of vertices, and the number of potential edges\nbetween these vertices can be large, i.e., $O(|V_M|^2)$. \n%\nFor example, one may be interested in analyzing whether a \\emph{group} of\nentities~$V_M$ may become a $k$-clique in the future, or whether a specific\nvertex~$v \\in V_M$ will become a \\emph{hub} of a $k$-star, connecting $v$ to\n$k-1$ other selected vertices from~$V_M \\setminus \\{v\\}$. \n%\nThis leads to novel issues, not present in link prediction.\n%\nFor example, what if \\emph{some} edges, belonging to the motif being\npredicted, already exist? How should they be treated by a score\nfunction? Or, how to enable users to apply their domain knowledge? For example,\nwhen predicting whether the given vertices will form some chemical particle, a\nuser may know that the presence of some link (e.g., some specific atomic bond)\nmay increase (or decrease) the chances for forming another bond. Now, how\ncould this knowledge be provided in the motif score function?\n%\nWe formally specify these and other aspects of the problem in a general\ntheoretical framework, and we provide example motif score functions.\n%\nWe explicitly consider correlations between edges forming a motif, i.e., the\nfact that the appearance of some edges may increase or decrease the overall chances of a given motif to appear.\n\n\n\n\n\n\n\n\\vspaceSQT{-0.3em}\nThen, we develop a learning\narchitecture based on graph neural networks (GNNs) to further enhance\nmotif prediction accuracy (Section~\\ref{sec:gnns}).\n%\nFor this, we extend the state-of-the-art SEAL link prediction\nframework~\\cite{zhang2018link} to support arbitrary motifs. \n%\nFor a given motif $M$, we train our architecture on what is the\n``right motif surroundings'' (i.e., nearby vertices and edges) that could\nresult in the appearance of~$M$. Then, for a given set of vertices~$V_M$, the\narchitecture infers the chances for~$M$ to appear.\n%\n%\nThe key challenge is to be able to capture the \\emph{richness} of different\nmotifs and their surroundings.\nWe tackle this with an appropriate selection of \\emph{negative}\nsamples, i.e., subgraphs that resemble the searched motifs but that are not\nidentical to them.\n%\nMoreover, when selecting the size of the ``motif surroundings''\nwe rely on an assumption also used in link\nprediction, which states that only the ``close surroundings'' (i.e., nearby\nvertices and edges, 1--2 hops away) of a link to be predicted have a significant\nimpact on whether or not this link would appear~\\cite{zhang2018link,\nzhang2020revisiting}. We use this assumption for motifs: as our evaluation\nshows, it ensures high accuracy while significantly reducing\nruntimes of training and inference (as only a small subgraph is used, instead\nof the whole input graph).\n%\nWe call our GNN architecture \\textbf{SEAM}: learning from Subgraphs, Embeddings\nand Attributes for \\textbf{Motif} prediction\\footnote{\\scriptsize In analogy to\nSEAL~\\cite{zhang2018link, zhang2020revisiting}, which stands for ``learning\nfrom Subgraphs, Embeddings, and Attributes for Link prediction''.}.\n%\nOur evaluation (Section~\\ref{sec:eval}) illustrates the high accuracy of \nSEAM (often more than 90\\%), for a variety of graph datasets and motif sizes.\n\n\nTo motivate our work, we now compare SEAM and\na proposed Jaccard-based heuristic that considers link correlations to two\nbaselines that straightforwardly \\emph{use link prediction independently for\neach motif link}: a Jaccard-based score and the state-of-the-art SEAL scheme\nbased on GNNs~\\cite{zhang2018link}. We show the results in\nFigure~\\ref{fig:intro}. The correlated Jaccard outperforms\na simple Jaccard, while the proposed SEAM is better than SEAL.\nThe benefits\ngeneralize to different graph datasets.\n%\nImportantly, we observe that the larger the motif\nto predict becomes (larger $k$), \\emph{the more advantages our architecture delivers}.\nThis is because larger motifs provide more room for \\emph{correlations between\ntheir associated edges}. Straightforward link prediction based schemes do not\nconsider this effect, while our methods do, which is why we offer more\nbenefits for more complex motifs. The advantages of SEAM over the correlated\nJaccard show that GNNs more robustly capture correlations and the structural\nrichness of motifs than simple manual heuristics. \nSimultaneously, heuristics do not need any training.\n%\n% The fundamental reason for the advantage of the proposed methods is the fact\n% that our methods consider the \\emph{correlations} between links that form a\n% motif, i.e., the appearance of two links is not independent.\n%\nFinally, SEAM also successfully predicts more\narbitrary \\emph{communities} or \\emph{clusters}~\\cite{lee2010survey,\ngibson2005discovering, besta2021graphminesuite, besta2021sisa}. They differ\nfrom motifs as they do not have a very specific fixed structure\n(such as a star) but simply have\nthe edge density above a certain threshold. \n%\nSEAM's high accuracy in predicting such structures illustrates its potential\nfor broader graph mining beyond motif analysis.\n\n\\emph{Overall, the key contributions of our paper are (1)\nidentifying and formulating the motif prediction problem and the associated\nscore functions, (2) showing how to solve this problem with heuristics and\ngraph neural networks, and (3) illustrating that graph neural networks can\nsolve this problem more effectively than heuristics.}\n"
            },
            "section 2": {
                "name": "Background and Notation",
                "content": " \\label{background}\n\nWe first describe the necessary background and notation.\n\n\\vspaceSQT{-0.3em}\n\\textbf{Graph Model}\n%\nWe model an undirected graph $G$ as a tuple $(V,E)$; $V$ and $E \\subseteq V\n\\times V$ are sets of nodes (vertices) and links (edges); $|V|=n$, $|E|=m$. Vertices are\nmodeled with integers $1, ..., n$; $V = \\{1, ..., n\\}$.\n%\n$N_v$ denotes the neighbors of $v \\in V$; $d(v)$ denotes\nthe degree of $v$.\n\n\n\\vspaceSQT{-0.3em}\n\\textbf{Link Prediction}\n%\nWe generalize the well-known link prediction problem.\n%\n% Similarity-based methods assume that\n% nodes tend to form links with other similar nodes. These methods stem from the\n% hypothesis that two nodes are similar if they are connected to similar nodes or\n% are near in the network according to a given distance function.\n%\n% Let us make the above ideas more formal. \n%\nConsider two unconnected vertices $u$ and $v$. We assign a \\emph{similarity\nscore} $s_{u,v}$ to them. All pairs of vertices that are not edges receive such\na score and are ranked according to it. The higher a similarity score is, the\n``more likely'' a given edge is to be missing in the data or to be created in\nthe future.  We stress that the link prediction scores are usually not based on\nany probabilistic notion (in the formal sense) and are only used to make\ncomparisons between pairs of vertices in the same input graph dataset.\n\n\\vspaceSQT{-0.3em}\nThere are numerous known similarity scores. First, a large number of scores are\ncalled \\emph{first order} because they only consider the neighbors of $u$ and\n$v$ when computing $s_{u,v}$. Examples are the \\textbf{Common Neighbors} scheme\n$s_{u,v}^{CN} = \\abs{N_u \\cap N_v}$ or the \\textbf{Jaccard} scheme\n$s_{u,v}^{J} = \\frac{\\abs{N_u \\cap N_v}}{\\abs{N_u \\cup\nN_v}}$~\\cite{besta2020communication}.\n%\nThese schemes assume that two vertices are more likely to be linked if they\nhave many common neighbors. \n%\nThere also exist similarity schemes that\nconsider vertices not directly attached to $u$ and $v$.\n%\nAll these schemes can be described using the same formalism of the\n\\emph{$\\gamma$-decaying heuristic} proposed by~\\cite{zhang2018link}.\nIntuitively, for a given pair of vertices~$(u,v)$, the $\\gamma$-decaying\nheuristic for $(u,v)$ provides a sum of contributions into the link prediction\nscore for $(u,v)$ from all other vertices, weighted in such a way that nearby\nvertices have more impact on the score. \n%\n\\if 0\n%\nAll these schemes can be described using the same formalism of the\n\\emph{$\\gamma$-decaying heuristic} $s_{u,v}^H \\equiv \\mathcal{H}(u,v) = \\eta\n\\sum_{l=1}^{\\infty} \\gamma^l f(u,v,l)$ proposed by~\\cite{zhang2018link}.\n%\n\\fi\n\n\n\\vspaceSQT{-0.3em}\n\\textbf{Graph Neural Networks}\n%\nGraph neural networks (GNNs) are a recent class of neural networks for learning\nover irregular data such as graphs~\\cite{scarselli2008graph,\nzhang2019heterogeneous, zhou2020graph, thekumparampil2018attention,\nwu2020comprehensive, sato2020survey, wu2020graph, zhang2020deep,\nchen2020bridging, cao2020comprehensive}.  There exists a plethora of models and\nmethods for GNNs; most of them consist of two fundamental parts: (1) an\naggregation layer that combines the features of the neighbors of each node, for\nall the nodes in the input graph, and (2) combining the scores into a new\nscore. \n%\nThe input to a GNN is a tuple $G = (A,X)$. The input graph $G$ having $n$\nvertices is modeled with an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$.\nThe features of vertices (with dimension~$d$) are modeled with a matrix\n$X\\in \\mathbb{R}^{n\\times d}$.\n%\n% Deep neural network on each vertex~\\cite{ben2019modular}, based on the values\n% aggregated from the vertex neighbors.\n% \n% The input for a GNN is generally of the form $(A,X)$ where $A\\in \\mathbb{R}^{n\n% \\times n}$ is the \\emph{adjacency matrix} of a graph $G=(V,E)$ containing $n$\n% vertices, and $X\\in \\mathbb{R}^{n\\times d}$ is the \\emph{feature matrix}\n% of~$G$, where $d$ is the dimension of the feature vector. More precisely, every\n% vertex $i \\in V$ has a feature vector $x_i \\in \\mathbb{R}^d$.\n\n\\if 0\n\\fi\n\\iftr\n"
            }
        },
        "figures": {
            "fig:intro": "\\begin{figure}[h]\n\\vspaceSQ{-2em}\n\\centerline{\\includegraphics[width=0.45\\textwidth]{motivation_plot___more___e.pdf}}\n\\vspaceSQ{-0.5em}\n%\n\\caption{\\textmd{\\textbf{Motivating our work (SEAM)}: the accuracy (\\%) of predicting\ndifferent motifs with SEAM compared to using a state-of-the-art SEAL link\nprediction scheme~\\cite{zhang2018link, zhang2020revisiting} and a naive one\nthat does not consider correlations between edges. \n%\nThe details of the experimental setup are in Section~\\ref{sec:eval} (the dataset is USAir).\n%\n\\textbf{Importantly: (1)} SEAM outperforms all other methods, \\textbf{(2)} the\naccuracy of SEAM \\emph{increases} with the size ($k$) of each motif, while in\nother methods it \\emph{decreases}.\n%\n}}\n\\label{fig:intro}\n\\vspaceSQ{-1em}\n\\end{figure}"
        }
    }
}