{
    "meta_info": {
        "title": "Heuristic Learning with Graph Neural Networks: A Unified Framework for  Link Prediction",
        "abstract": "Link prediction is a fundamental task in graph learning, inherently shaped by\nthe topology of the graph. While traditional heuristics are grounded in graph\ntopology, they encounter challenges in generalizing across diverse graphs.\nRecent research efforts have aimed to leverage the potential of heuristics, yet\na unified formulation accommodating both local and global heuristics remains\nundiscovered. Drawing insights from the fact that both local and global\nheuristics can be represented by adjacency matrix multiplications, we propose a\nunified matrix formulation to accommodate and generalize various heuristics. We\nfurther propose the Heuristic Learning Graph Neural Network (HL-GNN) to\nefficiently implement the formulation. HL-GNN adopts intra-layer propagation\nand inter-layer connections, allowing it to reach a depth of around 20 layers\nwith lower time complexity than GCN. Extensive experiments on the Planetoid,\nAmazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN.\nIt outperforms existing methods by a large margin in prediction performance.\nAdditionally, HL-GNN is several orders of magnitude faster than\nheuristic-inspired methods while requiring only a few trainable parameters. The\ncase study further demonstrates that the generalized heuristics and learned\nweights are highly interpretable.",
        "author": "Juzheng Zhang, Lanning Wei, Zhen Xu, Quanming Yao",
        "link": "http://arxiv.org/abs/2406.07979v2",
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ],
        "additionl_info": "Accepted by KDD 2024"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nLink prediction stands as a cornerstone in the domain of graph machine learning, facilitating diverse applications from knowledge graph reasoning~\\cite{redgnn} to drug interaction prediction~\\cite{wang2024accurate,zhang2023emerging} and recommender systems~\\cite{matrix_factorization}. While its significance is unquestionable, research in this area has not reached the same depth as that for node or graph classification~\\cite{dropedge,nodeformer,li2019semi,pooling}.\n\nIn graph machine learning, two fundamental sources of information play a pivotal role: node features and graph topology~\\cite{GPRGNN,GCN}. \nLink prediction task is inherently driven by graph topology~\\cite{survey1,SEAL,Neo-GNN}. Heuristics, which derive exclusively from graph topology, naturally align with link prediction. The appeal of heuristics lies in their simplicity and independence of learning. \nWhile heuristics are crafted from human intuition and insights, they can be broadly categorized into two types: local heuristics, which focus on neighboring nodes, and global heuristics, which focus on global paths~\\cite{survey1}.\n\nEffective link prediction benefits from both local and global topological information~\\cite{survey2}. For instance, in a triangular network, each pair of nodes shares two common neighbors, making local heuristics effective. Conversely, in a hexagonal network, where only length-5 paths connect each node pair, global heuristics may yield better results. Hence, the adaptive integration of multi-range topological information from both local and global heuristics is essential for accurate predictions.\n\nWhile heuristics prove effective in link prediction tasks, they inherently capture specific topology patterns, posing challenges in their generalization to diverse graphs~\\cite{NBFNet,Neo-GNN}. Moreover, heuristics are unable to leverage node features, limiting their efficacy on attributed graphs~\\cite{SEAL}.\nTo make heuristics more universal and general, recent research efforts have been directed toward establishing formulations for heuristics and learning heuristics from these formulations. Notable examples include SEAL~\\cite{SEAL}, NBFNet~\\cite{NBFNet}, and Neo-GNN~\\cite{Neo-GNN}. SEAL's $\\gamma$-decaying framework and NBFNet's path formulation are tailored for global heuristics, while Neo-GNN's MLP framework is tailored for local ones. \nTo obtain multi-range topological information, a unified formulation that accommodates both local and global heuristics is necessary, yet it remains undiscovered.\n\nOur motivation for constructing the unified formulation stems from the observation that both local and global heuristics can be expressed through adjacency matrix multiplications. Therefore, we unify local and global heuristics into a matrix formulation, enabling the accommodation and generalization of various local and global heuristics. \nIn contrast to previous works that construct formulations based on abstract functions such as SEAL~\\cite{SEAL}, NBFNet~\\cite{NBFNet}, and Neo-GNN~\\cite{Neo-GNN}, our unified formulation is developed through direct matrix operations. This unified formulation ensures rigorous equivalence to numerous local and global heuristics under specific configurations.\n\nTo learn generalized heuristics and acquire multi-range information, we propose the \\textbf{H}euristic \\textbf{L}earning \\textbf{G}raph \\textbf{N}eural \\textbf{N}etwork (HL-GNN) to efficiently implement the formulation.\nHL-GNN incorporates intra-layer propagation and inter-layer connections while excluding transformation and activation functions. This enables HL-GNN to effectively reach a depth of around 20 layers, while only requiring the training of a global GNN with a time complexity even lower than GCN.\nThe adaptive weights in HL-GNN facilitate the integration of multi-range topological information, and govern the trade-off between node features and topological information.\n% We derive the theoretical properties of the polynomial graph filter, which governs the trade-off between node features and topological information. \n% Moreover, we prove that the unified formulation is strictly more expressive than individual heuristics, and HL-GNN is strictly more expressive than both individual heuristics and conventional GNNs.\n\nOur comprehensive experiments, conducted on the Planetoid, Amazon, and OGB datasets, confirm the effectiveness and efficiency of our proposed HL-GNN. It consistently achieves state-of-the-art performance across numerous benchmarks, maintains excellent scalability, and stands out as the most parameter-efficient method among existing GNN methods. Furthermore, it demonstrates superior speed, surpassing existing heuristic-inspired methods by several orders of magnitude. \nHL-GNN is highly interpretable, as evidenced by the generalized heuristics and learned weights on real-world datasets as well as synthetic datasets.\n\nOur contributions can be summarized as follows:\n\n\\begin{itemize}\n\\item We unify local and global heuristics into a matrix formulation, facilitating the accommodation and generalization of heuristics. We demonstrate that numerous traditional heuristics align with our formulation under specific configurations.\n\n\\item We propose HL-GNN to efficiently implement the formulation, capable of reaching a depth of around 20 layers with lower time complexity than GCN. HL-GNN can adaptively balance the trade-off between node features and topological information.\n\n\\item Comprehensive experiments demonstrate that HL-GNN outperforms existing methods in terms of performance and efficiency. The interpretability of HL-GNN is highlighted through the analysis of generalized heuristics and learned weights.\n\n\\end{itemize}\n\n\n"
            },
            "section 2": {
                "name": "Related Works",
                "content": "\n",
                "subsection 2.1": {
                    "name": "Graph Neural Networks",
                    "content": "\nGraph Neural Networks (GNNs) have emerged as a powerful paradigm for learning node representations by exploiting neural networks to manipulate both node features and graph topology. These networks employ a message-passing mechanism, with notable examples including Graph Convolutional Network (GCN)~\\cite{GCN}, GraphSAGE~\\cite{graphsage}, and Graph Attention Network (GAT)~\\cite{GAT}. \nThrough iterative message propagation, GNNs enable each node representation to accumulate information from its neighboring nodes, thereby facilitating downstream tasks.\n\nWhile GNNs have emerged as potent solutions for node and graph classification~\\cite{dropedge,nodeformer,li2019semi,pooling}, they sometimes fall short in link prediction scenarios compared to traditional heuristics like Common Neighbors (CN)~\\cite{CN} or the Resource Allocation Index (RA)~\\cite{RA}. The primary issue lies in the inherent intertwining of node features and graph topology during the message-passing process in conventional GNNs. This entanglement causes node features to interfere with graph topology, impeding the effective extraction of topological information for link prediction tasks.\n\nAlthough in principle an arbitrary number of GNN layers can be stacked, practical GNNs are usually shallow, typically consisting of 2-3 layers, as conventional GNNs often experience a sharp performance drop after just 2 or 3 layers. A widely accepted explanation for this performance degradation with increasing depth is the over-smoothing issue~\\cite{over-smoothing,JKNet}, which refers to node representations becoming non-discriminative when going deep. While the adaptive integration of both local and global topological information is essential for link prediction, conventional GNNs usually cannot penetrate beyond 3 layers, restricting the extraction of global topological information.\n\n"
                },
                "subsection 2.2": {
                    "name": "Link Prediction",
                    "content": "\nLink prediction predicts the likelihood of a link forming between two nodes in a graph.\nThe problem of link prediction has traditionally been addressed by heuristic methods. These methods are primarily concerned with quantifying the similarity between two nodes based on the graph topology. Heuristic methods can be broadly categorized into two groups: local and global~\\cite{survey1,survey2}.\n\nLocal heuristics can be further divided into entirety-based heuristics and individual-based heuristics. Entirety-based heuristics, like Common Neighbors (CN)~\\cite{CN} and the Local Leicht-Holme-Newman Index (LLHN)~\\cite{LHN}, consider the cumulative count of common neighbors. In contrast, individual-based heuristics, exemplified by the Resource Allocation Index (RA)~\\cite{RA}, focus on nodes within the common neighborhood and incorporate detailed topological information such as the degree of each node.\n\nGlobal heuristics, on the other hand, leverage the entire graph topology. Methods such as the Katz Index (KI)~\\cite{KI} and the Global Leicht-Holme-Newman Index (GLHN)~\\cite{LHN} consider all possible paths between node pairs. The Random Walk with Restart (RWR)~\\cite{PPR} assesses the similarity between two nodes based on random walk probabilities. Some global heuristics are tailored to specific path lengths, like the Local Path Index (LPI)~\\cite{LP} and the Local Random Walks (LRW)~\\cite{LRW}.\n\n\nTraditional heuristic methods are manually designed and show limitations on complex real-world graphs, prompting a shift toward learning-based approaches. Embedding methods, including Matrix Factorization~\\cite{matrix_factorization}, DeepWalk~\\cite{deepwalk}, LINE~\\cite{line}, and Node2vec~\\cite{node2vec}, factorize network representations into low-dimensional node embeddings. However, embedding methods face limitations due to their inability to leverage node features on attributed graphs. \n\nRecent advancements have focused on enhancing GNNs with valuable topological information. Subgraph GNNs like SEAL~\\cite{SEAL}, GraIL~\\cite{grail}, and SUREL~\\cite{SUREL} explicitly encode subgraphs around node pairs. However, they require the running of a subgraph GNN with the labeling trick for each link during training and inference.\nTaking a different perspective, models like NBFNet~\\cite{NBFNet} and RED-GNN~\\cite{redgnn} adopt source-specific message passing, drawing inspiration from global heuristics. However, they require training a global GNN for each source node.\nSome methods opt for a single global GNN to improve scalability and efficiency. Neo-GNN~\\cite{Neo-GNN} uses two MLPs, while SEG~\\cite{SEG} uses a GCN layer and an MLP to approximate a heuristic function. BUDDY~\\cite{BUDDY} develops a novel GNN that passes subgraph sketches as messages. However, these methods primarily focus on local topological information and struggle to capture global topological information.\nIn contrast, the proposed HL-GNN can capture long-range information up to 20 hops while only requiring the training of a global GNN. Further details about the comparison of HL-GNN with existing methods are provided in Section~\\ref{para:comparison}.\n\n\n"
                }
            },
            "section 3": {
                "name": "Unified Heuristic Formulation",
                "content": "\n\n\nLet $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ denote a graph, with nodes $\\mathcal{V}$ and edges $\\mathcal{E}$. In this work, we consider undirected and unweighted graphs. We define $|\\mathcal{V}|=N$ as the number of nodes and $|\\mathcal{E}|=M$ as the number of edges. \nNode features are characterized by the node feature matrix $\\boldsymbol{X} \\in \\mathbb{R}^{N \\times F}$, where $F$ indicates the number of features. \nThe graph topology is encapsulated by the adjacency matrix $\\boldsymbol{A} \\in {\\{0,1\\}}^{N \\times N}$. The matrix $\\tilde{\\boldsymbol{A}}=\\boldsymbol{A}+\\boldsymbol{I}_N$ represents the adjacency matrix with self-loops, where $\\tilde{a}_{ij}=1$ signifies an edge between nodes $i$ and $j$. The node degree $i$ with self-loops is given by $\\tilde{d}_i=\\sum_j \\tilde{a}_{ij}$, with the diagonal degree matrix with self-loops denoted as $\\tilde{\\boldsymbol{D}}=\\operatorname{diag}(\\tilde{d}_1, \\cdots, \\tilde{d}_N)$. The set $\\Gamma_x$ represents the 1-hop neighbors of node $x$, encompassing node $x$ itself.\n\n\nWe introduce a set of normalized adjacency matrices, detailed in Table~\\ref{tab:adjacency_matrices}. This set comprises the symmetrically normalized matrix $\\Asym$, the row-stochastic normalized matrix $\\Ars$, and the column-stochastic normalized matrix $\\Acs$, which encompass diverse normalization techniques (left multiplication, right multiplication, or both) applied to the adjacency matrix.\nNext, we define the propagation operator $\\mathbb{A}$ to offer a choice among different types of adjacency matrices:\n\\begin{definition}\n    (\\textit{Propagation operator}). The \\textit{propagation operator} $\\mathbb{A} \\in \\mathbb{R}^{N \\times N}$ is defined as $\\mathbb{A} \\in \\{\\A, \\Asym, \\Ars, \\Acs\\}$. The expressions for the adjacency matrices $\\A, \\Asym, \\Ars, \\Acs$ are detailed in Table~\\ref{tab:adjacency_matrices}.\n\\end{definition}\n\nThe propagation operator encapsulates the prevalent propagation mechanisms commonly employed in GNNs.\nBy substituting the adjacency matrix $\\A$ with the propagation operator $\\mA$, we can opt for various propagation mechanisms that deliver diverse information. \n\n\n\n\n% Designing a unified framework for both local and global heuristics is non-trivial, primarily due to two significant challenges:\n% \\begin{itemize}[leftmargin=*]\n% \\item Local heuristics emphasize common neighbors, \n% while global heuristics assess paths between two nodes. \n% Merging these two distinct categories into a unified framework is challenging due to their different focuses.\n% \\footnote{+wln+ For more clear justification, try to use several words summarize the challenge. Besides, it seems that the second challenge could cover the first challenge? what are the differences? try to choose more distinguishable challenges. How about  ``Combination/match between heuristic and structure'' (ur second challenge) and ``Integration of local and global'' (ur first challenge)  }\n\n% \\item Heuristics are crafted from human intuition, experience, and domain-specific insights. \n% The diversity in their expressions make it challenging to generalize them into a unified framework.\n% \\end{itemize}\n% Therefore, any attempt to encapsulate heuristics into a unified framework requires a deep understanding of both the broader patterns and individual intricacies. \n% Given that heuristics are fundamentally influenced by graph topology, which is determined by the adjacency matrix, it becomes plausible to deduce heuristics directly from this matrix through appropriate manipulations. \n\n\n\nGiven that heuristics are fundamentally influenced by graph topology, it is possible to express various heuristics using adjacency matrices. \nThe $(i,j)$ entry of the 2-order adjacency matrix multiplication denotes the count of common neighbors for nodes $i$ and $j$. The $(i,j)$ entry of the $l$-order adjacency matrix multiplications denotes the number of length-$l$ paths between nodes $i$ and $j$. Hence, by employing distinct orders of adjacency matrix multiplications, we can extract varying insights from neighbors or paths.\nFollowing this intuition, we can express diverse heuristics in matrix form.\nWe provide a concise summary of heuristics, their mathematical expressions, and their corresponding matrix forms in Table~\\ref{tab:heursitics}.\nDetailed derivations of matrix forms of heuristics can be found in Appendix~\\ref{para:heuristic_proofs}.  \nNext, we introduce the definition of the heuristic formulation:\n\\begin{definition}\n  (\\textit{Heuristic formulation}). A \\textit{heuristic formulation} is denoted by a matrix $\\boldsymbol{H} \\in \\mathbb{R}^{N \\times N}$. Each entry $(i, j)$ in this matrix corresponds to the heuristic score for the link $(i, j)$, denoted as $\\boldsymbol{H}_{i, j}=h(i, j)$.\n\\end{definition}\n\nWe can unify both local and global heuristics in a formulation based on matrix forms of heuristics. Our proposed heuristic formulation parameterizes a combination of matrix multiplications:\n\\begin{equation}\n\\label{eq:formulation}\n\\boldsymbol{H} \n= \\sum_{l=0}^{L} \\left(\\beta^{(l)} \\prod_{m=0}^l \\mathbb{A}^{(m)} \\right),\n\\end{equation}\nwhere $\\mathbb{A}^{(m)} \\in \\{\\A, \\Asym, \\Ars, \\Acs\\}$ for $1 \\leq m \\leq L$ represent the propagation operators, and $\\mathbb{A}^{(0)} = \\boldsymbol{I}_N$. The coefficients $\\beta^{(l)}$ for $0 \\leq l \\leq L$ modulate the weights of different orders of matrix multiplications, and $L$ is the maximum order. \nNumerous traditional heuristics align with our formulation under specific configurations.\nTable~\\ref{tab:heursitics} showcases a selection of traditional heuristics and illustrates their alignment with our formulation through propagation operators $\\mathbb{A}^{(m)}$ and weights $\\beta^{(l)}$. \nWe assert the formulation's ability to accommodate heuristics in Proposition~\\ref{prop:formulation-heuristic}. The proof for Proposition~\\ref{prop:formulation-heuristic} can be found in Appendix~\\ref{para:heuristic_proofs}.\n\n\\begin{proposition}\n\\label{prop:formulation-heuristic}\n    Our formulation can accommodate a broad spectrum of local and global heuristics with propagation operators $\\mathbb{A}^{(m)}$ for $1 \\leq m \\leq L$, weight parameters $\\beta^{(l)}$ for $0 \\leq l \\leq L$, and maximum order $L$. \n\\end{proposition}\n\nUnlike previous methods that exclusively cater to either local or global heuristics, our formulation seamlessly integrates both aspects, presenting a unified solution. In contrast to prior works relying on abstract functions for heuristic approximation~\\cite{SEAL, NBFNet, Neo-GNN, SEG}, our formulation is developed through direct matrix operations. This formulation facilitates rigorous equivalence to numerous  local and global heuristics under specific configurations.\nIt is crucial to note that our heuristic formulation does not aim to accommodate all possible heuristics. \nInstead, it aims to distill the critical characteristics of heuristics, \nwith a specific focus on extracting common neighbors from local heuristics and global paths from global heuristics.\n\nExisting heuristics are primarily handcrafted and may not be optimal for real-world graphs.\nLeveraging the propagation operators, weight parameters and maximum order offers the potential to learn generalized, possibly more effective heuristics, which we will discuss in Section~\\ref{para:interpret} and Appendix~\\ref{para:heuristic_advance}.\n\n\n\n"
            },
            "section 4": {
                "name": "Heuristic Learning Graph Neural Network (HL-GNN)",
                "content": "\n\n",
                "subsection 4.1": {
                    "name": "Heuristic Learning Graph Neural Network",
                    "content": "\n",
                    "subsubsection 4.1.1": {
                        "name": "Motivation",
                        "content": "\nDirect matrix multiplication serves as a straightforward method to implement the heuristic formulation in Equation~\\eqref{eq:formulation}.\nHowever, it comes with high computational and memory costs. \nThe time complexity of direct matrix multiplication is $\\mathcal{O}(L^2 N^3)$ and the space complexity is $\\mathcal{O}(LN^2)$, where $N$ denotes the number of nodes. This is attributed to executing up to $L$-order matrix multiplications for $L$ times.\nThe significant time and space complexities present two major challenges of ensuring scalability and maintaining depth:\n\\begin{itemize}[leftmargin=*]\n\\item \\textit{Scalability.} To be scalable, the model must effectively handle large graphs.\nDatasets like OGB are substantially larger than those like Planetoid, making the value of $N$ a considerable strain on the time and space complexities.\n\\item  \\textit{Depth.} To effectively integrate global heuristics into the heuristic formulation, the value of $L$ must be sufficiently large to encapsulate messages from distant nodes. However, increasing $L$ further strains the time and space complexities.\n\\end{itemize}\nConsequently, there is a pressing need to mitigate the burdens of both time and space complexities.\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Architecture",
                        "content": "\n\nThe construction and computation of $N \\times N$ matrices impose significant computational and memory demands.\nOne potential technique is initializing $\\mathbb{A}^{(0)} = \\boldsymbol{X}$ instead of $\\mathbb{A}^{(0)} = \\boldsymbol{I}_N$.\nThis approach effectively reduces the feature dimensionality from $N$ to $F$, resulting in substantial time and space savings. Moreover, since heuristics cannot leverage node features on attributed graphs, initializing with $\\mathbb{A}^{(0)} = \\boldsymbol{X}$ allows the heuristic formulation to utilize node features.\nEven if node features are of low quality or completely absent, we can still train embeddings for each node. Therefore, $\\boldsymbol{X}$ can represent either raw node features or learnable node embeddings.\n\nFurther, we exploit the sparsity of the graph and employ a Graph Neural Network to compute the heuristic formulation. We propose the efficient and scalable \\textbf{H}euristic \\textbf{L}earning \\textbf{G}raph \\textbf{N}eural \\textbf{N}etwork (HL-GNN), expressed as:\n\\begin{equation}\n\\label{eq:HLGNN}\n\\boldsymbol{Z}^{(0)}=\\boldsymbol{X},\n\\quad \n\\boldsymbol{Z}^{(l)}=\\mA^{(l)} \\boldsymbol{Z}^{(l-1)},  \n\\quad \n\\boldsymbol{Z} = \\sum_{l=0}^L \\beta^{(l)} \\boldsymbol{Z}^{(l)},\n\\end{equation}\nwith $\\beta^{(l)}$ representing the learnable weight of the $l$-th layer, and $L$ representing the model's depth.\nAn illustration of our proposed HL-GNN is provided in Figure~\\ref{fig:HL-GNN}.\nWe do not impose constraints on $\\beta^{(l)}$, allowing them to take positive or negative values.\nAdaptive weights $\\beta^{(l)}$ facilitate the integration of multi-range topological information and govern the trade-off between node features and topological information.\nGiven the discrete nature of the propagation operators $\\mA^{(l)} \\in \\{\\A, \\Asym, \\Ars, \\Acs\\}$, \nthey obstruct the back-propagation process, necessitating their relaxation to a continuous form:\n\\begin{equation}\n\\label{eq:relaxation}\n\\mA^{(l)}= \\alpha_1^{(l)} \\Ars+\\alpha_2^{(l)} \\Acs+\\alpha_3^{(l)} \\Asym, \\quad \\text{for} \\quad 1 \\leq l \\leq L,\n\\end{equation}\nwhere $\\alpha_1^{(l)}, \\alpha_2^{(l)}, \\alpha_3^{(l)}$ are layer-specific learnable weights harmonizing three propagation mechanisms. \nThe continuous relaxation of the propagation operators enables gradient back-propagation, thereby allowing the model to be trained end-to-end.\nWe exclude the adjacency matrix $\\A$ in Equation~\\eqref{eq:relaxation} to ensure that the eigenvalues of $\\mA^{(l)}$ fall within the range $[0, 1]$. Moreover, we apply a softmax function to $\\boldsymbol{\\alpha}^{(l)}$, where $\\operatorname{softmax}(\\alpha_i^{(l)})=\\exp(\\alpha_i^{(l)}) /\\sum_{j=1}^3 \\exp(\\alpha_j^{(l)})$ for $i=1, 2, 3$.\nControlling the eigenvalues of $\\mA^{(l)}$ helps prevent numerical instabilities as well as issues related to exploding gradients or vanishing gradients.\n\nHL-GNN employs intra-layer propagation and inter-layer connections as described in Equation~\\eqref{eq:HLGNN}. \nThe salient trait is its elimination of representation transformation and non-linear activation at each layer, requiring only a few trainable parameters.\nWe assert the relationship between the learned representations $\\boldsymbol{Z}$ and the heuristic formulation $\\boldsymbol{H}$ in Proposition~\\ref{prop:relationship}. The proof for Proposition~\\ref{prop:relationship} can\nbe found in Appendix~\\ref{para:relationship_proof}.\n\n\\begin{proposition}\n\\label{prop:relationship}\n    The relationship between the learned representations $\\boldsymbol{Z}$ in Equation~\\eqref{eq:HLGNN} and the heuristic formulation $\\boldsymbol{H}$ in Equation~\\eqref{eq:formulation} is given by $\\boldsymbol{Z} = \\boldsymbol{H}\\boldsymbol{X}$, where $\\boldsymbol{X}$ is the node feature matrix.\n\\end{proposition}\n\nAccording to Proposition~\\ref{prop:relationship}, the learned representations $\\boldsymbol{Z}$ utilize heuristics as weights to combine features from all nodes. \nThe heuristic formulation $\\boldsymbol{H}$ can be effectively distilled through the message-passing process in HL-GNN. Consequently, HL-GNN has the ability to accommodate and generalize both local and global heuristics.\n% The contribution of one node's raw feature $\\boldsymbol{x}_j$ to another node's final representation $\\boldsymbol{z}_i$ is determined by their likelihood to form a link, indicated by the heuristic score $\\boldsymbol{H}_{i, j}$. \nOur method can be viewed as topological augmentation, employing the topological information embedded in $\\boldsymbol{H}$ to enhance raw node features $\\boldsymbol{X}$.\n\nFor sparse graphs, the time complexity of HL-GNN is $\\mathcal{O}(LMF)$, where $M$ is the number of edges. The space complexity of HL-GNN is $\\mathcal{O}(NF)$. On a large graph, typically containing millions of nodes, HL-GNN leads to remarkable time and space savings -- ten and five orders of magnitude, respectively -- compared to direct matrix multiplication.\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Training",
                        "content": "\nAfter acquiring the node representations, \nwe employ a predictor to compute the likelihood for each link by \n$s_{i j} = f_\\theta(\\boldsymbol{z}_i \\odot \\boldsymbol{z}_j)$,\nwhere $f_\\theta$ is a feed-forward neural network, $\\boldsymbol{z}_i$ and $\\boldsymbol{z}_j$ represent the representations of node $i$ and $j$ respectively, \nand the symbol \\(\\odot\\) denotes the element-wise product.\n\nMany methods categorize link prediction as a binary classification problem and conventionally employ the cross-entropy loss function. However, this might not always be the suitable strategy. Standard evaluation procedures in link prediction do not label positive pairs as 1 and negative pairs as 0. The primary objective is to rank positive pairs higher than negative pairs, aligning with the maximization of the Area Under the Curve (AUC). \nIn light of this, we adopt the AUC loss as described in~\\cite{AUC_loss}, ensuring it aligns conceptually with the evaluation procedure:\n\\begin{equation}\n\\mathcal{L} = \\min_{\\alpha, \\beta, \\theta} \n\\sum_{ (i, j) \\in \\mathcal{E} } \\sum_{ (i, k) \\in \\mathcal{E}^{-} } \n\\gamma_{i j} \\left(\\max(0, \\gamma_{i j} - s_{ij} + s_{ik})\\right)^2.\n\\end{equation}\nHere, \\(\\mathcal{E}^{-}\\) signifies the negative links uniformly sampling from the set $\\mathcal{V} \\times \\mathcal{V}-\\mathcal{E}$, \nand \\(\\gamma_{ij}\\) is an adaptive margin between positive link $(i, j)$ and negative link $(i, k)$. \nThe model is trained end-to-end, jointly optimizing the GNN parameters $\\alpha$ and $\\beta$, along with the predictor parameters $\\theta$.\n\n\n\n% \\subsection{Theoretical Analysis of HL-GNN}\n% HL-GNN employs intra-layer propagation and inter-layer connections as described in Equation~\\eqref{eq:HLGNN}. \n% The salient trait is its elimination of representation transformation and non-linear activation at each layer, requiring only a few trainable parameters.\n% This renders HL-GNN akin to a spectral-based GNN, and we can derive the spectral properties of the graph filter of HL-GNN, which will be discussed in Section~\\ref{para:graph_filter}.\n% HL-GNN leverages multi-range topological information to enhance node features.\n% Additionally, HL-GNN can effectively reach a depth of around 20 layers, in contrast to conventional GNNs typically consisting of 2-3 layers. \n% Based on these observations, we can derive the expressiveness of HL-GNN, which will be discussed in Section~\\ref{para:expressive}.\n\n\n% \\subsubsection{Spectral properties of HL-GNN}\n% \\label{para:graph_filter}\n% Spectral-based GNNs employ spectral polynomial graph filters to conduct graph convolutions. \n% The unified heuristic formulation $\\sum_{l=0}^{L}(\\beta^{(l)} \\prod_{m=0}^l \\mathbb{A}^{(m)})$ serves as a polynomial graph filter of order $L$ capable of approximating any graph filter~\\cite{graph_filter}.\n% Increasing $L$ allows us to capture longer-range dependencies and better approximate the optimal graph filter. \n% GPR-GNN~\\cite{GPRGNN} can be seen as a special case of our formulation, where the propagation operators $\\mathbb{A}^{(m)}$ are fixed to $\\Asym$.\n% For simplicity, we define $f(\\mathbb{A})=\\sum_{l=0}^{L} \\beta^{(l)} \\mathbb{A}^l$ as the graph filter. Let $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n$ be the eigenvalues of $\\mA$. \n% The eigenvalues are related to the frequency components of the graph signal, with larger eigenvalues corresponding to lower frequencies.\n% The following theorem holds, with the proof provided in Appendix~\\ref{para:filter_proof}:\n% \\begin{theorem}\n% \\label{prop:graph_filter}\n% Consider a polynomial graph filter $f(\\cdot)$ defined with coefficients $\\beta^{(l)}$ for $0 \\leq l \\leq L$.\n% Then the following statements hold:\n% \\begin{enumerate}\n%  \\item If $\\forall 0 \\leq l \\leq L$, $\\beta^{(l)} \\geq 0$, and $\\sum_{l=0}^{L} \\beta^{(l)} = 1$ with $\\beta^{(0)} < 1$, then:\n% $$\\left| \\frac{f(\\lambda_i)}{f(\\lambda_1)} \\right| < 1, \\quad \\text{for} \\quad i \\geq 2,$$\n% indicating that \\( f(\\cdot) \\) is a low-pass polynomial graph filter;\n%  \\item If $\\forall 0 \\leq l \\leq L$, $\\beta^{(l)} = (-\\xi)^l$ with $\\xi \\in (0, 1)$, then:\n% $$\\left| \\frac{\\lim_{L \\rightarrow \\infty} f(\\lambda_i)}{\\lim_{L \\rightarrow \\infty} f(\\lambda_1)} \\right| > 1, \\quad \\text{for} \\quad i \\geq 2,$$\n% indicating that \\( f(\\cdot) \\) is a high-pass polynomial graph filter.\n% \\end{enumerate}\n% \\end{theorem}\n\n% According to Theorem~\\ref{prop:graph_filter}, if the coefficients $\\beta^{(l)}$ are all non-negative and sum to one, the filter functions as a low-pass graph filter. This allows for the retention of more node features and local topological information while suppressing global topological information.\n% Conversely, if the coefficients $\\beta^{(l)}$ follow a geometric sequence with a negative ratio, the filter functions as a high-pass graph filter. This allows for the retention of more global topological information while suppressing node features and local topological information.\n% The learnable weights $\\beta^{(l)}$ adaptively determine whether the filter is low-pass or high-pass, adapting to specific characteristics of each dataset. These weights govern the trade-off between node features and topological information, and facilitate the integration of multi-range topological information.\n\n\n% \\subsubsection{Expressiveness of HL-GNN}\n% \\label{para:expressive}\n% Following~\\cite{labeling_trick, BUDDY}, we say that one model $M_1$ is strictly more expressive than another model $M_2$ when (i) all links discriminated by $M_2$ are also discriminated by $M_1$; and (ii) there exist two non-automorphic links, and $M_1$ can discriminate them while $M_2$ cannot. We denote this relationship as $M_1 \\succ M_2$. The formal definitions of graph isomorphism, graph automorphism, automorphic links, and strictly more expressive are provided in Appendix~\\ref{para:expressive_proof}.\n% We have the following result regarding the expressiveness of the unified heuristic formulation and HL-GNN, with the proof provided in Appendix~\\ref{para:expressive_proof}:\n\n% \\begin{theorem}\n% \\label{prop:expressive}\n%     Let $\\mathcal{G}_{\\text{attr}}$ denote the space of attributed graphs, and $\\mathcal{H}_{\\text{ind}}$ denote the set of individual heuristics that can be represented with the heuristic formulation.\n%     Let $\\mathrm{GNN}_k$ denote a GNN with $k$ layers. Then the following statements hold:\n%     \\begin{enumerate}\n%         \\item $\\forall h \\in \\mathcal{H}_{\\text{ind}}, \\boldsymbol{H} \\succ h;$\n%         \\item In $\\mathcal{G}_{\\text{attr}}, \\forall h \\in \\mathcal{H}_{\\text{ind}}, \\textnormal{HL-GNN} \\succ h;$\n%         \\item $\\forall k<L, \\textnormal{HL-GNN} \\succ \\mathrm{GNN}_k;$\n%     \\end{enumerate}\n%     where $\\boldsymbol{H}$ denotes the heuristic formulation defined in Equation~\\eqref{eq:formulation}, and $L$ is the depth of HL-GNN.\n% \\end{theorem}\n\n% The unified heuristic formulation is a polynomial of adjacency matrices, meaning any difference in the polynomial's terms will result in a different final outcome. Therefore, the heuristic formulation is strictly more expressive than individual heuristics that can be represented by the heuristic formulation.\n% HL-GNN can generalize various local and global heuristics and effectively leverage node features, leading to higher expressiveness than individual heuristics. While HL-GNN belongs to the class of message-passing GNNs, it can penetrate deeper than conventional GNNs, which typically consist of 2-3 layers like GCN and GAT, resulting in higher expressiveness than these conventional GNNs.\n% The higher expressiveness demonstrates that the heuristic formulation and HL-GNN are more effective at capturing topological information than individual heuristics. Additionally, HL-GNN's architecture proves to be more effective for link prediction tasks than conventional GNNs.\n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Comparison with Existing Methods",
                    "content": "\n\\label{para:comparison} \nWe evaluate the heuristic-learning ability, information range, and time complexity of HL-GNN by comparing it with conventional GNNs and heuristic-inspired GNN methods. A summary of these comparisons is provided in Table~\\ref{tab:comparison}.\nHL-GNN excels at accommodating and generalizing a wide range of both local and global heuristics.\nIn contrast, SEAL~\\cite{SEAL} focuses on subgraphs to learn local heuristics, while NBFNet~\\cite{NBFNet} concentrates on paths to learn global heuristics. \nNeo-GNN~\\cite{Neo-GNN} leverages two MLPs for local heuristic learning, and BUDDY~\\cite{BUDDY} uses subgraph sketches to represent local heuristics. Notably, most of these methods are limited to topological information within a 3-hop range. In contrast, HL-GNN can reach a depth of approximately 20 layers, providing a broader information range. Adaptive weights in HL-GNN enable the integration of both local and global topological information.\n\nHL-GNN has a time complexity of $\\mathcal{O}(LMF)$, which is the lowest among the compared methods. Unlike conventional GNNs, HL-GNN solely utilizes propagation mechanisms and omits transformation and activation functions.\nSEAL requires running a subgraph GNN with the labeling trick for each link, and NBFNet requires running a global GNN for each source node during training and inference. In contrast, HL-GNN only requires running a single global GNN during training and inference. Furthermore, HL-GNN avoids the need to extract topological information from common neighbors and subgraph sketches, as required by Neo-GNN and BUDDY, respectively. A detailed time complexity analysis of each method is included in Appendix~\\ref{para:time_comp_analysis}. \n\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\n\n",
                "subsection 5.1": {
                    "name": "Experiment Setup",
                    "content": "\n\\label{para:expsetup}\n\n",
                    "subsubsection 5.1.1": {
                        "name": "Datasets",
                        "content": "\nWe utilize nine datasets from three sources: Planetoid~\\cite{planetoid}, Amazon~\\cite{amazon}, and OGB~\\cite{OGB}. \nThe Planetoid datasets include \\texttt{Cora}, \\texttt{Citeseer}, and \\texttt{Pubmed}. The Amazon datasets include \\texttt{Photo} and \\texttt{Computers}. \nThe OGB datasets include \\texttt{ogbl-collab}, \\texttt{ogbl-ddi}, \\texttt{ogbl-ppa}, and \\texttt{ogbl-citation2}. \nDataset statistics can be found in Appendix~\\ref{para:dataset_statistics}.\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Baselines",
                        "content": "\nWe compare our model against a diverse set of baseline methods, including heuristics like CN~\\cite{CN}, RA~\\cite{RA}, KI~\\cite{KI}, and RWR~\\cite{PPR}, traditional embedding-based methods such as MF~\\cite{matrix_factorization}, Node2vec~\\cite{node2vec}, and DeepWalk~\\cite{deepwalk}, as well as conventional GNNs like GCN~\\cite{GCN} and GAT~\\cite{GAT}. Additionally, we benchmark HL-GNN against heuristic-inspired GNN methods like SEAL~\\cite{SEAL}, NBFNet~\\cite{NBFNet}, Neo-GNN~\\cite{Neo-GNN}, and BUDDY~\\cite{BUDDY}. This comprehensive comparison enable us to assess the performance and effectiveness of the proposed HL-GNN.\n\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Experimental settings",
                        "content": "\nIn accordance with previous works~\\cite{NBFNet,BUDDY}, we randomly sample 5\\% and 10\\% of the links for validation and test sets on non-OGB datasets. We sample the same number of non-edge node pairs as negative links. For the OGB datasets, we follow their official train/validation/test splits.\nFollowing the convention in previous works~\\cite{BUDDY,ILP-GNN}, we use Hits@100 as the evaluation metric for the Planetoid datasets, and we use AUC for the Amazon datasets. For the OGB datasets, we use their official evaluation metrics, such as Hits@50 for \\texttt{ogbl-collab}, Hits@20 for \\texttt{ogbl-ddi}, Hits@100 for \\texttt{ogbl-ppa}, and Mean Reciprocal Rank (MRR) for \\texttt{ogbl-citation2}~\\cite{OGB}. \n\nWe include a linear layer as preprocessing before HL-GNN to align the dimension of node features with the hidden channels of HL-GNN. We also leverage node embeddings on the OGB datasets to enhance the node representations. For the \\texttt{ogbl-collab} dataset, we follow OGB's guidelines and use the validation set for training. We evaluate HL-GNN over 10 runs without fixing the random seed. More details about the experiments are provided in Appendix~\\ref{para:exp_detail}.\n\n\n\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Main Results",
                    "content": "\n\nAs shown in Table~\\ref{tab:main_result}, HL-GNN consistently outperforms all the baselines on all of the datasets, highlighting its effectiveness and robustness for link prediction tasks. Table~\\ref{tab:main_result} reports the averaged results with standard deviations. Notably, HL-GNN achieves a remarkable gain of 7.0\\% and 16.7\\% in Hits@100 compared to the second-best method on the Planetoid datasets \\texttt{Cora} and \\texttt{Pubmed}, respectively.\nMoreover, our HL-GNN demonstrates its ability to handle large-scale graphs effectively, as evidenced by its superior performance on the OGB datasets. Specifically, HL-GNN achieves a gain of 13.9\\% in Hits@100 on \\texttt{ogbl-ppa}, and achieves 68.11\\% Hits@50 on \\texttt{ogbl-collab} and 89.43\\% MRR on \\texttt{ogbl-citation2}. Even when node features are absent or of low quality, HL-GNN maintains consistent performance by learning embeddings for each node, as demonstrated on datasets like \\texttt{ogbl-ddi}, which lack node features.\n\nHL-GNN outperforms all listed heuristics, indicating its capacity to generalize heuristics and integrate them with node features. According to Table~\\ref{tab:main_result}, local heuristics like CN and RA perform better than global heuristics on the OGB datasets, while global heuristics like KI and RWR perform better on the Planetoid and Amazon datasets. This underscores the importance of establishing a unified formulation that accommodates both local and global heuristics. Notably, we can use the configuration in Table~\\ref{tab:heursitics} to recover the heuristic RA from HL-GNN without training, achieving a performance of 49.33\\% Hits@100 on \\texttt{ogbl-ppa}. This result serves as a compelling lower bound for HL-GNN's performance on \\texttt{ogbl-ppa}.\nHL-GNN significantly outperforms conventional GNNs like GCN and GAT across all datasets. Additionally, HL-GNN also surpasses existing heuristic-inspired GNN methods, including SEAL, NBFNet, Neo-GNN, and BUDDY, suggesting that integrating information from multiple ranges is beneficial for link prediction tasks.\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Ablation Studies",
                    "content": "\n",
                    "subsubsection 5.3.1": {
                        "name": "Different information ranges",
                        "content": "\nThe adaptive weights $\\beta^{(l)}$ in HL-GNN facilitate the integration of multi-range information, encompassing both local and global topological information. To investigate the impact of information ranges, we conduct experiments isolating either local or global information.\nWe train a GNN variant using skip-connections of the first 3 layers as the output, exclusively considering local topological information. Similarly, we train another GNN variant using the final-layer output with GNN depth $L \\geq 5$ to exclusively consider global topological information.\nFigure~\\ref{fig:output} demonstrates that HL-GNN consistently outperforms GNN variants focusing solely on local or global topological information. This underscores HL-GNN's efficacy in adaptively combining both types of information. \n\n\n\n\n\n"
                    },
                    "subsubsection 5.3.2": {
                        "name": "Sufficient model depths",
                        "content": "\nIn HL-GNN, achieving sufficient model depth is crucial for learning global heuristics and capturing long-range dependencies. Our model can effectively reach a depth of around 20 layers without performance deterioration, as shown in Figure~\\ref{fig:deep}. In contrast, conventional GNNs often experience a sharp performance drop after just 2 or 3 layers.\nFor the Planetoid datasets \\texttt{Cora} and \\texttt{Pubmed}, shallow models yield poor performance, likely due to the absence of global topological information. Conversely, for the OGB datasets \\texttt{ogbl-collab} and \\texttt{ogbl-ddi}, deeper models (exceeding 15 layers) result in decreased performance, possibly due to the introduction of non-essential global information, which dilutes the crucial local information needed for accurate predictions.\n\n\n\n\n"
                    }
                },
                "subsection 5.4": {
                    "name": "Efficiency Analysis",
                    "content": "\n",
                    "subsubsection 5.4.1": {
                        "name": "Time efficiency",
                        "content": "\nOur HL-GNN demonstrates exceptional time efficiency with the lowest time complexity, as indicated in Table~\\ref{tab:comparison}. \nThe wall time for a single training epoch is provided in Table~\\ref{tab:wall_time}. Although HL-GNN generally has a larger depth $L$ compared to conventional GNNs, its experimental wall time per training epoch is comparable to models like GCN and GAT. In practice, HL-GNN requires slightly more time than GCN or GAT due to its increased depth. However, HL-GNN is several orders of magnitude faster than heuristic-inspired GNN methods such as SEAL, NBFNet, and Neo-GNN, thanks to its avoidance of running multiple GNNs and time-consuming manipulations like applying the labeling trick.\n\n\n\n\n\n\n"
                    },
                    "subsubsection 5.4.2": {
                        "name": "Parameter efficiency",
                        "content": "\nHL-GNN only demands a few parameters per layer, with the primary parameter cost incurred by the preprocessing step and the MLP predictor. Table~\\ref{tab:parameters} compares the number of parameters in HL-GNN with other GNN methods, clearly highlighting HL-GNN's superior parameter efficiency. Our model stands out as the most parameter-efficient among the listed conventional GNNs and heuristic-inspired GNN methods.\n\nWhile conventional GNNs excel in efficiency but may lack in performance, and heuristic-inspired GNN methods are effective but time and parameter-intensive, HL-GNN strikes a balance. It consistently achieves top-tier prediction performance on numerous link prediction benchmarks, maintains excellent scalability and time efficiency, and stands out as the most parameter-efficient method. \n\n\n\n\n"
                    }
                },
                "subsection 5.5": {
                    "name": "Interpretability Analysis",
                    "content": "\n",
                    "subsubsection 5.5.1": {
                        "name": "Generalized heuristics and learned weights",
                        "content": "\n\\label{para:interpret}\nLeveraging the capabilities of the unified formulation, we can derive generalized heuristics by analyzing the learned parameters of HL-GNN. \nThe generalized heuristics and learned weights $\\beta^{(l)}$ provide insights into the graph-structured data. \nThe learned weights $\\beta^{(l)}$ are visually depicted in Figure~\\ref{fig:weights}.\nDue to space constraints, the formulas for the generalized heuristics are provided in Appendix~\\ref{para:learned_heuristics}.\n \nFor the \\texttt{Cora} and \\texttt{Citeseer} datasets, the learned weights monotonically decrease, indicating that the graph filter serves as a low-pass filter. The weight $\\beta^{(0)}$ has the largest magnitude, suggesting that crucial information is primarily contained in node features. Local topological information from nearby neighbors plays a major role, while global topological information from distant nodes serves as a complementary factor.\nConversely, for the \\texttt{ogbl-collab} and \\texttt{ogbl-ddi} datasets, the weights do not monotonically increase or decrease. Instead, they experience a significant change, especially in the first 5 layers, indicating that the graph filter serves as a high-pass filter. \nThe weight $\\beta^{(2)}$ has the largest magnitude, suggesting that crucial information lies in local topology rather than node features. Moreover, for large values of $l$ on the \\texttt{ogbl-collab} and \\texttt{ogbl-ddi} datasets, the weights $\\beta^{(l)}$ become negative, suggesting that global topological information from distant nodes compensates for excessive information from nearby neighbors.\nThe learnable weights $\\beta^{(l)}$ govern the trade-off between node features and topological information, enabling the adaptive integration of multi-range topological information.\n\n\n\n\n\n"
                    },
                    "subsubsection 5.5.2": {
                        "name": "Leveraging generalized heuristics",
                        "content": "\n\nWith the generalized heuristic for each dataset, there is no need to train a GNN and an MLP predictor from scratch. Instead, we can simply follow the generalized heuristic and train an MLP predictor only, which is significantly more efficient than training from scratch. The performance of training the MLP alone is comparable to training from scratch, but it converges more quickly. \nThe training time for training from scratch versus training only the MLP is shown in Table~\\ref{tab:time_mlp}, while the performance details are provided in Appendix~\\ref{para:mlp_only}. \nThe slight decrease in performance can likely be attributed to the fact that, when training the GNN and MLP together, the gradients flow through both blocks, allowing them to adapt to each other. In contrast, training the MLP alone limits its ability to capture complex interactions between the two blocks.\n\n\n\n"
                    }
                },
                "subsection 5.6": {
                    "name": "Case Study",
                    "content": "\nWe construct two synthetic datasets, a triangular network, and a hexagonal network, to assess HL-GNN's ability to learn the most effective heuristic and obtain the desired range of information.\nThe triangular network consists of 1000 nodes, with every three nodes forming a triangle. As each pair of nodes shares two common neighbors, we anticipate that the learned heuristic would resemble a local heuristic focusing on 2-hop information. The learned weights are presented in Figure~\\ref{fig:case_study}, with $\\beta^{(2)}$ having the largest magnitude, corresponding to a local heuristic.\n\nThe hexagonal network also comprises 1000 nodes, with every six nodes forming a hexagon. Here, we expect the learned heuristic to resemble a global heuristic focusing on 5-hop information. As shown in Figure~\\ref{fig:case_study}, the weight $\\beta^{(5)}$ has the largest magnitude, corresponding to a global heuristic. In both cases, HL-GNN demonstrates its ability to adaptively learn the most effective heuristic based on the specific topology.\nThis also emphasizes the importance of developing a formulation that can effectively accommodate both local and global heuristics.\n\n\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nWe introduce a unified formulation that accommodates and generalizes both local and global heuristics using propagation operators and weight parameters. Additionally, we propose HL-GNN, which efficiently implements this formulation. HL-GNN combines intra-layer propagation and inter-layer connections, allowing the integration of multi-range topological information. Experiments demonstrate that HL-GNN achieves state-of-the-art performance and efficiency.\nThis study is confined to undirected graphs; for directed graphs, we preprocess them by converting them into undirected graphs. Extensions to multi-relational graphs, such as knowledge graphs, are left for future work.\n\n\n"
            },
            "section 7": {
                "name": "Acknowledgment",
                "content": "\n\n%This work is supported by National Key Research and Development Program of China under Grant 2023YFB2903904, \n%NSFC (No. 92270106),\nThis work is supported by National Key Research and Development Program of China (under Grant No.2023YFB2903904),\nNational Natural Science Foundation of China (under Grant No.92270106)\nand Beijing Natural Science Foundation (under Grant No.4242039).\n\n%%\n\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n%\\newpage\n\\clearpage\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{references}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\newpage\n\\appendix\n\\clearpage\n\n\n"
            },
            "section 8": {
                "name": "Proofs",
                "content": "\n",
                "subsection 8.1": {
                    "name": "prop:formulation-heuristic",
                    "content": "\n\\label{para:heuristic_proofs}\nWe provide detailed derivations of the matrix forms of heuristics and demonstrate their alignment with the unified heuristic formulation under specific configurations.\n\n\\begin{lemma}\n    Common Neighbors (CN), denoted as \n    \\[s_\\mathrm{CN}(i, j) = |\\Gamma_i \\cap \\Gamma_j| =(\\A^2)_{i, j},\\]\n    conforms to the formulation when \\(\\mA^{(1)}=\\mA^{(2)} = \\A\\), \\(\\beta^{(0)}= \\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\), \\(L = 2\\).\n\\end{lemma}\n\\begin{proof}\n    Using the adjacency matrices, we can derive the matrix form of the heuristic CN as follows:\n    \\begin{equation}\n    s_\\mathrm{C N}(i, j) = |\\Gamma_i \\cap \\Gamma_j| = \\sum_{k \\in \\mathcal{V}} \\tilde{a}_{i k} \\tilde{a}_{k j} = (\\A^2)_{i, j}.\n    \\end{equation}\n    Using the settings \\(\\mA^{(1)}=\\mA^{(2)} = \\A\\), \\(\\beta^{(0)}= \\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\), \\(L = 2\\) in Equation~\\eqref{eq:formulation} ensures that \\(\\boldsymbol{H}_{i, j}=h(i,j)=s_\\mathrm{C N}(i, j)\\).\n\\end{proof}\n\n\\begin{lemma}\n    The Local Leicht-Holme-Newman Index (LLHN), denoted as \n    \\[s_{\\mathrm{LLHN}}(i, j) = \\frac{|\\Gamma_i \\cap \\Gamma_j|}{\\tilde{d}_i \\tilde{d}_j}=(\\Ars \\Acs)_{i, j},\\] \n    conforms to the formulation when \\(\\mA^{(1)} = \\Ars\\), \\(\\mA^{(2)} = \\Acs\\), \\(\\beta^{(0)}=\\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\), \\(L = 2\\).\n\\end{lemma}\n\\begin{proof}\n    Using the adjacency matrices, we can derive the matrix form of the heuristic LLHN as follows:\n    \\begin{equation}\n    s_{\\mathrm{LLHN}}(i, j) = \\frac{|\\Gamma_i \\cap \\Gamma_j|}{\\tilde{d}_i \\tilde{d}_j} = \\sum_{k \\in \\mathcal{V}} \\frac{\\tilde{a}_{i k}}{\\tilde{d}_i} \\frac{\\tilde{a}_{k j}}{\\tilde{d}_j} = (\\Ars \\Acs)_{i, j}.\n    \\end{equation}\n     Using the settings \\(\\mA^{(1)} = \\Ars\\), \\(\\mA^{(2)} = \\Acs\\), \\(\\beta^{(0)}=\\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\), \\(L = 2\\) in Equation~\\eqref{eq:formulation} ensures that \\(\\boldsymbol{H}_{i, j}=h(i,j)=s_\\mathrm{LLHN}(i, j)\\).\n\\end{proof}\n\n\n\\begin{lemma}\n    The Resource Allocation Index (RA), denoted as \n    \\[s_{\\mathrm{RA}}(i,j)\\\\=\\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k}=(\\Acs \\A)_{i, j}=(\\A \\Ars)_{i, j},\\]\n    conforms to the formulation under two configurations: (i) \\(\\mA^{(1)} = \\Acs\\), \\(\\mA^{(2)} = \\A\\), \\(\\beta^{(0)}=\\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\), \\(L = 2\\); or (ii) \\(\\mA^{(1)} = \\A\\), \\(\\mA^{(2)} = \\Ars\\), \\(\\beta^{(0)}=\\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\), \\(L = 2\\).\n\\end{lemma}\n\\begin{proof}\n    Using the adjacency matrices, we can derive the matrix form of the heuristic RA as follows:\n    \\begin{equation}\n    s_{\\mathrm{RA}}(i, j)=\\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k}=\\sum_{k \\in \\mathcal{V}} \\frac{\\tilde{a}_{i k}}{\\tilde{d}_k} \\tilde{a}_{k j}=(\\Acs \\A)_{i, j}.\n    \\end{equation}    \n    Alternatively, the matrix form of the heuristic RA can also be expressed as:\n    \\begin{equation}\n    s_{\\mathrm{RA}}(i, j)=\\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k}=\\sum_{k \\in \\mathcal{V}} {\\tilde{a}_{i k}} \\frac{\\tilde{a}_{k j}}{\\tilde{d}_k}=(\\A \\Ars)_{i, j}.\n    \\end{equation}\n    Using the two settings: (i) \\(\\mA^{(1)} = \\Acs\\), \\(\\mA^{(2)} = \\A\\), \\(\\beta^{(0)}=\\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\), \\(L = 2\\); or (ii) \\(\\mA^{(1)} = \\A\\), \\(\\mA^{(2)} = \\Ars\\), \\(\\beta^{(0)}=\\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\), \\(L = 2\\) in Equation~\\eqref{eq:formulation} ensures that \\(\\boldsymbol{H}_{i, j}=h(i,j)=s_\\mathrm{RA}(i, j)\\).\n\\end{proof}\n\n\\begin{lemma}\n\\label{lemma:KI}\n    The Katz Index (KI), denoted as \n    $$s_{\\mathrm{KI}}(i, j)=\\sum_{l=1}^{\\infty} \\gamma^l |{\\textnormal{paths}}_{i,j}^l|=\\left(\\sum_{l=1}^{\\infty} \\gamma^l \\A^l\\right)_{i,j},$$\n    where ${\\textnormal{paths}}_{i,j}^l$ is the set of length-$l$ paths between nodes $i$ and $j$, $\\gamma$ is a damping factor, conforms to the formulation when $\\mA^{(m)} = \\A$ for $m \\geq 1$, $\\beta^{(0)} = 0$, $ \\beta^{(l)}=\\gamma^l$ for $l \\geq 1$, $L = \\infty$.\n\\end{lemma}\n\\begin{proof}\n    Using the adjacency matrices, we can derive the matrix form of the heuristic KI as follows:\n    \\begin{equation}\n    s_{\\mathrm{KI}}(i, j)=\\sum_{l=1}^{\\infty} \\gamma^l |{\\textnormal{paths}}_{i,j}^l|=\\sum_{l=1}^{\\infty} \\gamma^l(\\A^l)_{i,j}=\\left(\\sum_{l=1}^{\\infty} \\gamma^l \\A^l\\right)_{i,j}.\n    \\end{equation}\n    Using the settings $\\mA^{(m)} = \\A$ for $m \\geq 1$, $\\beta^{(0)} = 0$, $ \\beta^{(l)}=\\gamma^l$ for $l \\geq 1$, $L = \\infty$ in Equation~\\eqref{eq:formulation} ensures that \\(\\boldsymbol{H}_{i, j}=h(i,j)=s_\\mathrm{KI}(i, j)\\).\n\\end{proof}\n\n\\begin{lemma}\n    The Global Leicht-Holme-Newman Index (GLHN), denoted as \n    $$s_{\\mathrm{GLHN}}(i, j)=\\sum_{l=0}^{\\infty} \\phi^l |{\\textnormal{paths}}_{i,j}^l|= \\left(\\boldsymbol{I}_N+\\sum_{l=1}^{\\infty} \\phi^l \\A^l \\right)_{i,j},$$\n    where ${\\textnormal{paths}}_{i,j}^l$ is the set of length-$l$ paths between nodes $i$ and $j$, $\\phi$ is a damping factor, conforms to the formulation when $\\mA^{(m)} = \\A$ for $m \\geq 1$, $\\beta^{(0)} = 1$, $ \\beta^{(l)}=\\phi^l$ for $l \\geq 1$, $L = \\infty$.\n\\end{lemma}\n\\begin{proof}\n    Using the adjacency matrices, we can derive the matrix form of the heuristic GLHN as follows:\n    \\begin{equation}\n    \\begin{aligned}\n    s_{\\mathrm{GLHN}}(i, j)&=\\sum_{l=0}^{\\infty} \\phi^l |{\\textnormal{paths}}_{i,j}^l|=(\\boldsymbol{I}_N)_{i,j}+ \\sum_{l=1}^{\\infty} \\phi^l(\\A^l)_{i,j}\\\\\n    &=\\left(\\boldsymbol{I}_N+\\sum_{l=1}^{\\infty} \\phi^l \\A^l \\right)_{i,j}.\n    \\end{aligned}\n    \\end{equation}\n    Using the settings $\\mA^{(m)} = \\A$ for $m \\geq 1$, $\\beta^{(0)} = 1$, $ \\beta^{(l)}=\\phi^l$ for $l \\geq 1$, $L = \\infty$ in Equation~\\eqref{eq:formulation} ensures that \\(\\boldsymbol{H}_{i, j}=h(i,j)=s_\\mathrm{GLHN}(i, j)\\).\n\\end{proof}\n\n\n\\begin{lemma}\n\\label{lemma:RWR}\n    The Random Walk with Restart (RWR), denoted as $$s_{\\mathrm{RWR}}(i,j)=[\\boldsymbol{\\pi}_i(\\infty)]_j =\\left(\\sum_{l=0}^{\\infty}(1-\\alpha)\\alpha^l \\Ars^l\\right)_{i,j},$$ \n    conforms to the formulation when $\\mA^{(m)} = \\Ars$ for $m \\geq 1$,  $\\beta^{(l)}=(1-\\alpha)\\alpha^l$ for $l \\geq 0$, $L = \\infty$.\n\\end{lemma}\n\\begin{proof}\n    Random Walk with Restart (RWR) calculates the stationary distribution of a random walker starting at $i$, who iteratively moves to a random neighbor of its current position with probability $\\alpha$ or returns to $i$ with probability $1-\\alpha$. \n    Let $\\boldsymbol{\\pi}_i$ denote the probability vector of reaching any node starting a random walk from node $i$.\n    Let $\\Acs$ be the transition matrix and $(\\tilde{\\boldsymbol{A}}_{\\mathrm{cs}})_{ij} = (\\tilde{\\boldsymbol{A}} \\tilde{\\boldsymbol{D}}^{-1})_{ij}=\\frac{1}{\\tilde{d}_j}$ if $(i, j) \\in \\mathcal{E}$ and $(\\tilde{\\boldsymbol{A}}_{\\mathrm{cs}})_{ij}=0$ otherwise.\n    Let $\\boldsymbol{e}_i$ be an indicator vector with the $i^{\\text{th}}$ element being 1 and others being 0.\n    The probability of reaching each node can be iteratively approximated by\n    \\begin{equation}\n    \\label{iter}\n    \\boldsymbol{\\pi}_i(t)=\\alpha \\Acs \\boldsymbol{\\pi}_i(t-1)+(1-\\alpha) \\boldsymbol{e}_i.\n    \\end{equation}\n    The stationary distribution of this probability vector can be calculated as $t \\rightarrow \\infty$,\n    \\begin{equation}\n    \\label{infinity}\n    \\boldsymbol{\\pi}_i(\\infty)=(1-\\alpha) (\\boldsymbol{I}_N-\\alpha\\Acs)^{-1} \\boldsymbol{e}_i.\n    \\end{equation}\n    \n    We calculate the closed-form solution to the iterative function of Equation~\\eqref{iter} with $\\boldsymbol{\\pi}_i(0)=0$:\n    \\begin{equation}\n    \\boldsymbol{\\pi}_i(t)=\\sum_{l=0}^{t-1} (1-\\alpha)\\alpha^l \\Acs^l \\boldsymbol{e}_i,\n    \\end{equation}\n    where $t \\geq 1$.\n    We can set $t \\rightarrow \\infty$,\n    \\begin{equation}\n    \\boldsymbol{\\pi}_i(\\infty)=\\sum_{l=0}^{\\infty}(1-\\alpha) \\alpha^l \\Acs^l \\boldsymbol{e}_i=(1-\\alpha) (\\boldsymbol{I}_N-\\alpha\\Acs)^{-1} \\boldsymbol{e}_i,\n    \\end{equation}\n    which conforms to the stationary distribution of Equation~\\eqref{infinity}.\n    By substituting the indicator vector $\\boldsymbol{e}_i$ with the identity matrix $\\boldsymbol{I}_N$, we obtain the RWR matrix \n    \\begin{equation}\n    \\boldsymbol{\\Pi}(\\infty)=\\sum_{l=0}^{\\infty}(1-\\alpha)\\alpha^l \\Acs^l, \n    \\end{equation}\n    whose element $(j,i)$ specifies the probability of the random walker starts from node $i$ and locates at node $j$ in the stationary state.\n    The heuristic for link $(i,j)$ is given by this random walk probability from $i$ to $j$, denoted as $[\\boldsymbol{\\pi}_i(\\infty)]_j$ (or $[\\boldsymbol{\\pi}_i(\\infty)]_j+[\\boldsymbol{\\pi}_j(\\infty)]_i$ for symmetry):\n    \\begin{equation}\n    \\begin{aligned}\n        s_{\\mathrm{RWR}}(i,j)&=[\\boldsymbol{\\pi}_i(\\infty)]_j\\\\&=\\left[\\boldsymbol{\\Pi}(\\infty)\\right]_{j,i}\n        \\\\&=\\left(\\sum_{l=0}^{\\infty}(1-\\alpha)\\alpha^l \\Acs^l\\right)_{j,i}\\\\&=\\left(\\sum_{l=0}^{\\infty}(1-\\alpha)\\alpha^l \\Ars^l\\right)_{i,j}.\n    \\end{aligned}\n    \\end{equation}\n     Using the settings $\\mA^{(m)} = \\Ars$ for $m \\geq 1$,  $\\beta^{(l)}=(1-\\alpha)\\alpha^l$ for $l \\geq 0$, $L = \\infty$ in Equation~\\eqref{eq:formulation} ensures that \\(\\boldsymbol{H}_{i, j}=h(i,j)=s_\\mathrm{RWR}(i, j)\\).\n\\end{proof}\n\n\\begin{lemma}\n    The Local Path Index (LPI), denoted as \n    $$s_{\\mathrm{LPI}}(i, j)=\\sum_{l=2}^L \\gamma^{l-2} |{\\textnormal{paths}}_{i,j}^l|=\\left(\\sum_{l=2}^L \\gamma^{l-2} \\A^l\\right)_{i,j},$$\n    where ${\\textnormal{paths}}_{i,j}^l$ is the set of length-$l$ paths between nodes $i$ and $j$, $\\gamma$ is a damping factor, conforms to the formulation when $\\mA^{(m)} = \\A$ for $1 \\leq m \\leq L$, $\\beta^{(0)} = \\beta^{(1)} =0$, $ \\beta^{(l)}=\\gamma^{l-2}$ for $2 \\leq l \\leq L$.\n\\end{lemma}\n\\begin{proof}\n    Following Lemma~\\ref{lemma:KI}, using the adjacency matrices, we can derive the matrix form of the heuristic LPI as follows:\n    \\begin{equation}\n    s_{\\mathrm{LPI}}(i, j)=\\sum_{l=2}^L \\gamma^{l-2} |{\\textnormal{paths}}_{i,j}^l|=\\sum_{l=2}^L \\gamma^{l-2}(\\A^l)_{i,j}=\\left(\\sum_{l=2}^L \\gamma^{l-2} \\A^l\\right)_{i,j}.\n    \\end{equation}\n    Using the settings $\\mA^{(m)} = \\A$ for $1 \\leq m \\leq L$, $\\beta^{(0)} = \\beta^{(1)} =0$, $ \\beta^{(l)}=\\gamma^{l-2}$ for $2 \\leq l \\leq L$ in Equation~\\eqref{eq:formulation} ensures that \\(\\boldsymbol{H}_{i, j}=h(i,j)=s_\\mathrm{LPI}(i, j)\\).\n\\end{proof}\n\n\\begin{lemma}\n    The Local Random Walks (LRW), denoted as $$s_{\\mathrm{LRW}}(i,j)=\\frac{\\tilde{d}_i}{2M}[\\boldsymbol{\\pi}_i(L)]_j =\\left(\\sum_{l=0}^{L-1}\\frac{\\tilde{d}_i}{2M}(1-\\alpha)\\alpha^l \\Ars^l\\right)_{i,j},$$ \n    conforms to the formulation when $\\mA^{(m)} = \\Ars$ for $1 \\leq m \\leq L-1$,  $\\beta^{(l)}=\\frac{\\tilde{d}_i}{2M} (1-\\alpha)\\alpha^l$ for $0 \\leq l \\leq L-1$.\n\\end{lemma}\n\\begin{proof}\n    Following Lemma~\\ref{lemma:RWR},\n    \\begin{equation}\n    \\boldsymbol{\\pi}_i(L)=\\sum_{l=0}^{L-1} (1-\\alpha)\\alpha^l \\Acs^l \\boldsymbol{e}_i,\n    \\end{equation}\n    where $L \\geq 1$.\n    By substituting the indicator vector $\\boldsymbol{e}_i$ with the identity matrix $\\boldsymbol{I}_N$, we obtain the RWR matrix \n    \\begin{equation}\n    \\boldsymbol{\\Pi}(L)=\\sum_{l=0}^{L-1}(1-\\alpha)\\alpha^l \\Acs^l.\n    \\end{equation}\n    Using the adjacency matrices, we can derive the matrix form of the heuristic LRW as follows:\n    \\begin{equation}\n    \\begin{aligned}\n        s_{\\mathrm{LRW}}(i,j)&=\\frac{\\tilde{d}_i}{2M}[\\boldsymbol{\\pi}_i(L)]_j\\\\\n        &=\\frac{\\tilde{d}_i}{2M}\\left[\\boldsymbol{\\Pi}(L)\\right]_{j,i}\\\\\n        &=\\left(\\sum_{l=0}^{L-1}\\frac{\\tilde{d}_i}{2M}(1-\\alpha)\\alpha^l \\Acs^l\\right)_{j,i}\\\\\n        &=\\left(\\sum_{l=0}^{L-1}\\frac{\\tilde{d}_i}{2M}(1-\\alpha)\\alpha^l \\Ars^l\\right)_{i,j}.\n    \\end{aligned}\n    \\end{equation}\n     Using the settings $\\mA^{(m)} = \\Ars$ for $1 \\leq m \\leq L-1$,  $\\beta^{(l)}=\\frac{\\tilde{d}_i}{2M} (1-\\alpha)\\alpha^l$ for $0 \\leq l \\leq L-1$ in Equation~\\eqref{eq:formulation} ensures that \\(\\boldsymbol{H}_{i, j}=h(i,j)=s_\\mathrm{LRW}(i, j)\\).\n\\end{proof}\n\n\\begin{proposition*}\n    Our formulation can accommodate a broad spectrum of local and global heuristics with propagation operators $\\mathbb{A}^{(m)}$ for $1 \\leq m \\leq L$, weight parameters $\\beta^{(l)}$ for $0 \\leq l \\leq L$, and maximum order $L$. \n\\end{proposition*}\n\\begin{proof}\n    Building upon above lemmas, it becomes evident that a diverse range of local and global heuristics can be represented in matrix form. Our formulation is capable of accommodating a wide range of local and global heuristics under specific configurations. This serves to prove Proposition~\\ref{prop:formulation-heuristic}.\n\\end{proof}\n\n\n\n"
                },
                "subsection 8.2": {
                    "name": "prop:relationship",
                    "content": " \n\\label{para:relationship_proof}\n\\begin{proposition*}\n    The relationship between the learned representations $\\boldsymbol{Z}$ in Equation~\\eqref{eq:HLGNN} and the heuristic formulation $\\boldsymbol{H}$ in Equation~\\eqref{eq:formulation} is given by $\\boldsymbol{Z} = \\boldsymbol{H}\\boldsymbol{X}$, where $\\boldsymbol{X}$ is the node feature matrix.\n\\end{proposition*}\n\n\\begin{proof}\n    Using Equation~\\eqref{eq:HLGNN}, we have:\n    \\begin{equation}\n    \\boldsymbol{Z}^{(l)} = \\mathbb{A}^{(l)} \\boldsymbol{Z}^{(l-1)},\n    \\end{equation}\n    with the initial condition \n    \\begin{equation}\n    \\boldsymbol{Z}^{(0)} = \\boldsymbol{X}.\n    \\end{equation}\n    Iteratively using the equation, we get:\n    \\begin{equation}\n    \\label{eq:proof_iter}\n    \\boldsymbol{Z}^{(l)} = \\prod_{m=1}^l \\mathbb{A}^{(m)} \\boldsymbol{X}= \\prod_{m=0}^l \\mathbb{A}^{(m)} \\boldsymbol{X}.\n    \\end{equation}\n    The second equality is due to $\\mathbb{A}^{(0)}=\\boldsymbol{I}_N$.\n    Using Equation~\\eqref{eq:HLGNN}, we can express \\(\\boldsymbol{Z}\\) as:\n    \\begin{equation}\n    \\boldsymbol{Z} = \\sum_{l=0}^L \\beta^{(l)} \\boldsymbol{Z}^{(l)}.\n    \\end{equation}\n    Substituting Equation~\\eqref{eq:proof_iter}, we get:\n    \\begin{equation}\n    \\boldsymbol{Z} = \\sum_{l=0}^L \\left(\\beta^{(l)} \\prod_{m=0}^l \\mathbb{A}^{(m)} \\boldsymbol{X}\\right)=\\left(\\sum_{l=0}^L \\left(\\beta^{(l)} \\prod_{m=0}^l \\mathbb{A}^{(m)} \\right)\\right)\\boldsymbol{X}.\n    \\end{equation}\n    Now, examining Equation~\\eqref{eq:formulation}, it becomes evident that:\n    \\begin{equation}\n    \\boldsymbol{Z} = \\boldsymbol{H}\\boldsymbol{X}.\n    \\end{equation}\n\\end{proof}\n\n% \\subsection{Proof of Theorem~\\ref{prop:graph_filter}}\n% \\label{para:filter_proof}\n% We first introduce a lemma to prove Theorem~\\ref{prop:graph_filter}.\n% \\begin{lemma}\n% \\label{lemma:eigenvalue}\n%     For a connected graph $G$, the adjacency matrices $\\Asym$, $\\Ars$, and $\\Acs$ always share the same set of eigenvalues. Each matrix has an eigenvalue of 1, and all other eigenvalues $\\lambda$ satisfy $|\\lambda| < 1$.\n% \\end{lemma}\n% \\begin{proof}\n%     First we prove that the adjacency matrices $\\Asym$, $\\Ars$, and $\\Acs$ always share the same set of eigenvalues.\n    \n%     $\\Asym \\Rightarrow \\Ars$. If $\\lambda$ is an eigenvalue of $\\Asym$ with eigenvector $\\boldsymbol{x}$, we have  $\\tilde{\\boldsymbol{D}}^{-1/2} \\tilde{\\boldsymbol{A}} \\tilde{\\boldsymbol{D}}^{-1/2} \\boldsymbol{x}=\\lambda \\boldsymbol{x}$. \n%     Left multiplying with $\\tilde{\\boldsymbol{D}}^{-1/2}$ yields $\\tilde{\\boldsymbol{D}}^{-1} \\tilde{\\boldsymbol{A}} (\\tilde{\\boldsymbol{D}}^{-1/2} \\boldsymbol{x})= \\lambda (\\tilde{\\boldsymbol{D}}^{-1/2} \\boldsymbol{x})$. Hence, $\\lambda$ is also an eigenvalue of $\\Ars$.\n\n%     $\\Ars \\Rightarrow \\Acs$. If $\\lambda$ is an eigenvalue of $\\Ars$ with eigenvector $\\boldsymbol{x}$, we have  $\\tilde{\\boldsymbol{D}}^{-1} \\tilde{\\boldsymbol{A}} \\boldsymbol{x}=\\lambda \\boldsymbol{x}$. \n%     Left multiplying with $\\tilde{\\boldsymbol{D}}$ yields $\\tilde{\\boldsymbol{A}} \\tilde{\\boldsymbol{D}}^{-1}(\\tilde{\\boldsymbol{D}} \\boldsymbol{x})= \\lambda (\\tilde{\\boldsymbol{D}} \\boldsymbol{x})$. Hence, $\\lambda$ is also an eigenvalue of $\\Acs$.\n\n%     $\\Acs \\Rightarrow \\Asym$. If $\\lambda$ is an eigenvalue of $\\Acs$ with eigenvector $\\boldsymbol{x}$, we have  $\\tilde{\\boldsymbol{A}} \\tilde{\\boldsymbol{D}}^{-1} \\boldsymbol{x}=\\lambda \\boldsymbol{x}$. \n%     Left multiplying with $\\tilde{\\boldsymbol{D}}^{-1/2}$ yields $\\tilde{\\boldsymbol{D}}^{-1/2} \\tilde{\\boldsymbol{A}} \\tilde{\\boldsymbol{D}}^{-1/2}(\\tilde{\\boldsymbol{D}}^{-1/2} \\boldsymbol{x})= \\lambda (\\tilde{\\boldsymbol{D}}^{-1/2} \\boldsymbol{x})$. Hence, $\\lambda$ is also an eigenvalue of $\\Asym$.\n    \n\n%     Next we prove that $\\Asym$, $\\Ars$, and $\\Acs$ always have an eigenvalue 1. Let $\\boldsymbol{e}=[1,1, \\cdots, 1]^T \\in \\mathbb{R}^N$. We have $\\Ars \\boldsymbol{e}=\\boldsymbol{e}$ since each row of $\\Ars$ sums to 1. As $\\Asym$, $\\Ars$, and $\\Acs$ always share the same set of eigenvalues, 1 is an eigenvalue of $\\Asym$, $\\Ars$, and $\\Acs$. \n    \n\n%     Finally, we prove that all other eigenvalues $\\lambda$ of $\\Asym$, $\\Ars$, and $\\Acs$ satisfy $|\\lambda| < 1$. Suppose there exists an eigenvalue $\\lambda$ with $|\\lambda| > 1$ and an associated eigenvector $\\boldsymbol{x}$. Then $\\Ars \\boldsymbol{x} = \\lambda \\boldsymbol{x}$ holds, and consequently, $\\Ars^k \\boldsymbol{x} = \\lambda^k \\boldsymbol{x}$ holds. The right side in $\\Ars^k \\boldsymbol{x} = \\lambda^k \\boldsymbol{x}$ grows exponentially as $k$ approaches infinity, indicating that some entries of $\\Ars^k$ become larger than 1. However, since all entries of $\\Ars^k$ are positive and each row sums to 1, no entry of $\\Ars^k$ can be larger than 1, leading to a contradiction. Therefore, all other eigenvalues $\\lambda$ of $\\Asym$, $\\Ars$, and $\\Acs$ satisfy $|\\lambda| < 1$.\n    \n% \\end{proof}\n\n% Then we proceed to prove Theorem~\\ref{prop:graph_filter}. For simplicity, let us assume $\\mathbb{A}^{(1)}=\\mathbb{A}^{(2)}=\\cdots =\\mathbb{A}^{(L)}=\\mathbb{A}$. Let $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n$ be the eigenvalues of $\\mA$. \n\n% \\begin{theorem*}\n% Consider a polynomial graph filter $f(\\cdot)$ defined with coefficients $\\beta^{(l)}$ for $0 \\leq l \\leq L$.\n% Then the following statements hold:\n% \\begin{enumerate}\n%  \\item If $\\forall 0 \\leq l \\leq L$, $\\beta^{(l)} \\geq 0$, and $\\sum_{l=0}^{L} \\beta^{(l)} = 1$ with $\\beta^{(0)} < 1$, then:\n% $$\\left| \\frac{f(\\lambda_i)}{f(\\lambda_1)} \\right| < 1, \\quad \\text{for} \\quad i \\geq 2,$$\n% indicating that \\( f(\\cdot) \\) is a low-pass polynomial graph filter;\n%  \\item If $\\forall 0 \\leq l \\leq L$, $\\beta^{(l)} = (-\\xi)^l$ with $\\xi \\in (0, 1)$, then:\n% $$\\left| \\frac{\\lim_{L \\rightarrow \\infty} f(\\lambda_i)}{\\lim_{L \\rightarrow \\infty} f(\\lambda_1)} \\right| > 1, \\quad \\text{for} \\quad i \\geq 2,$$\n% indicating that \\( f(\\cdot) \\) is a high-pass polynomial graph filter.\n% \\end{enumerate}\n% \\end{theorem*}\n\n% \\begin{proof}\n% The eigenvalues are related to the frequency components of the graph signal. The largest eigenvalue $\\lambda_1$ corresponds to the lowest frequency, and the smallest eigenvalue $\\lambda_n$ corresponds to the highest frequency.\n% The low-pass filter case shows that if the weight parameters $\\beta^{(l)}$ are all non-negative and sum to one, then the graph filter $f(\\cdot)$ will amplify the lowest frequency component and attenuate the higher frequency components, as shown by $|f(\\lambda_i) / f(\\lambda_1)|<1$ for $i \\geq 2$. The high-pass filter case shows that if the weight parameters $\\beta^{(l)}$ follow a geometric sequence with a negative ratio, then the graph filter $f(\\cdot)$ will attenuate the lowest frequency component and amplify the higher frequency components, as shown by $|\\lim_{L\\rightarrow \\infty} f(\\lambda_i) /\\lim_{L\\rightarrow \\infty} f(\\lambda_1)|>1$ for $i \\geq 2$.\n\n% We start with the low-pass filter part. According to Lemma~\\ref{lemma:eigenvalue}, the matrices $\\Asym$, $\\Ars$, and $\\Acs$ all possess an eigenvalue of 1, with all other eigenvalues $\\lambda$ satisfying $|\\lambda| < 1$. The softmax combination of these matrices results in $\\mA$:\n% \\begin{equation}\n% \\mA=\\alpha_1 \\Ars+\\alpha_2 \\Acs+\\alpha_3 \\Asym,\n% \\end{equation}\n% where $\\operatorname{softmax}(\\alpha_i) = \\exp(\\alpha_i) / \\sum_{j=1}^3 \\exp(\\alpha_j)$ for $i=1,2,3$, and $\\alpha_1 + \\alpha_2 + \\alpha_3 = 1$. Consequently, $\\mA$ also has an eigenvalue of 1, and all other eigenvalues $\\lambda$ satisfy $|\\lambda| < 1$. Specifically, we denote $\\lambda_1 = 1$ and $|\\lambda_i| < 1$ for $i \\geq 2$. Then we know that\n% \\begin{equation}\n% f(\\lambda_1)=\\sum_{l=0}^{L} \\beta^{(l)}\\lambda_1^l=\\sum_{l=0}^{L} \\beta^{(l)} =1.\n% \\end{equation}\n% Since $|\\lambda_i|^l<1$ for $i \\geq 2$ and $l \\geq 1$, we have\n% \\begin{equation}\n% |f(\\lambda_i)| \\leq\\sum_{l=0}^{L} \\beta^{(l)}\\left|\\lambda_i^l\\right|=\\sum_{l=0}^{L} \\beta^{(l)}|\\lambda_i|^l < \\sum_{l=0}^{L} \\beta^{(l)}=1, \\quad \\forall i \\geq 2.\n% \\end{equation}\n% The only possibility that the equality holds in the second inequality is $\\beta^{(0)}=1$ and $\\beta^{(l)}=0$ for $l \\geq 1$. However, considering the assumptions $\\sum_{l=0}^{L} \\beta^{(l)}=1$ and $\\beta^{(0)} < 1$, we conclude that this is impossible. Hence, a strict inequality $<$ holds. Thus, we have $|f(\\lambda_i) / f(\\lambda_1)|<1$ for $i \\geq 2$.\n% The graph filter with the choice $\\beta^{(l)} \\geq 0$ for $0 \\leq l \\leq L$, and $\\sum_{l=0}^{L} \\beta^{(l)}=1$ with $\\beta^{(0)} < 1$ represents low-frequency components, thereby acting as a low-pass filter.\n\n% Moving to the high-pass filter part, we observe that\n% \\begin{equation}\n% \\lim_{L\\rightarrow \\infty} f(\\lambda)=\\lim_{L\\rightarrow \\infty} \\sum_{l=0}^{L} \\beta^{(l)} \\lambda^l=\\lim_{L\\rightarrow \\infty} \\sum_{l=0}^L(-\\xi \\lambda)^l=\\frac{1}{1+\\xi \\lambda}\n% \\end{equation}\n% where the last equality follows from the fact that $\\xi \\in (0,1)$. Thus, we have\n% \\begin{equation}\n% \\left|\\frac{\\lim_{L\\rightarrow \\infty} f(\\lambda_i)}{\\lim_{L\\rightarrow \\infty} f(\\lambda_1)}\\right|=\\left|\\frac{1+\\xi}{1+\\xi \\lambda_i}\\right|>1, \\quad \\forall i \\geq 2.\n% \\end{equation}\n% The strict inequality arises from the fact $|\\lambda_i| < 1$ for $i \\geq 2$. The graph filter with the choice $\\beta^{(l)} = (-\\xi)^l, \\xi \\in (0,1)$ represents high-frequency components, thereby acting as a high-pass filter.\n% \\end{proof}\n\n\n\n% \\subsection{Proof of Theorem~\\ref{prop:expressive}}\n% \\label{para:expressive_proof}\n% We introduce fundamental concepts before proving Theorem~\\ref{prop:expressive}. These concepts are introduced through third-order tensors $\\mathbf{A} \\in \\mathbb{R}^{N^2 \\times F}$, where $F$ denotes the number of features~\\cite{labeling_trick, BUDDY}. Elements $\\mathbf{A}_{i, i,:}$ correspond to node features, while elements $\\mathbf{A}_{i, j,:}$, where $i \\neq j$, correspond to edge features. In this context, a graph is defined as a tuple $G=(V, E, \\mathbf{A})$, and isomorphisms and automorphisms are interpreted as permutations acting on $\\mathbf{A}$:\n% \\begin{equation}\n%     (\\sigma \\cdot \\mathbf{A})_{i, j, k} = \\mathbf{A}_{\\sigma^{-1}(i), \\sigma^{-1}(j), k}, \\quad \\forall \\sigma \\in S_N,\n% \\end{equation}\n% where $S_N$ is the set of all bijections (permutations) over $N$ symbols.\n% We define the concepts of graph isomorphism and automorphism as follows:\n% \\begin{definition}\n%     (\\textit{Graph isomorphism and automorphism}). Let $G_1, G_2$ be two graphs represented by tensors $\\mathbf{A}_1, \\mathbf{A}_2 \\in \\mathbb{R}^{N^2 \\times F}$. An isomorphism between $G_1, G_2$ is a bijective map (permutation) $\\sigma \\in S_N$ such that $\\sigma \\cdot \\mathbf{A}_1 = \\mathbf{A}_2$. For a graph $G=\\mathbf{A}\\in \\mathbb{R}^{N^2 \\times F}$, $\\sigma \\in S_N$ is an automorphism if $\\sigma \\cdot \\mathbf{A} = \\mathbf{A}$.\n% \\end{definition} \n\n% Next, we define automorphisms between links:\n% \\begin{definition}\n%     (\\textit{Automorphic links}). Let $G=\\mathbf{A}$ be a graph, and $\\ell_1=(u_1, v_1), \\ell_2=(u_2, v_2) \\in \\mathcal{E}$ be two links. We say $\\ell_1, \\ell_2$ are automorphic if there exists an automorphism $\\sigma \\in S_N$ such that $\\sigma \\cdot \\ell_1 = (\\sigma(u_1), \\sigma(v_1)) = (u_2, v_2) = \\ell_2$.\n% \\end{definition}\n\n% Then we introduce the definition of strictly more expressive in link prediction models following~\\cite{labeling_trick, BUDDY}:\n\n% \\begin{definition}\n%     (\\textit{Strictly more expressive}). Let $M_1, M_2$ be two models, and $\\ell_1, \\ell_2$ be two non-automorphic links in graph $\\mathcal{G}$. We say $M_1$ is \\textit{strictly more expressive} than $M_2$ iff $\\forall\\ell_1, \\ell_2 \\in \\mathcal{E}, M_2(\\ell_1) \\neq M_2(\\ell_2) \\Rightarrow M_1(\\ell_1) \\neq M_1(\\ell_2) \\wedge \\exists \\ell_1, \\ell_2 \\in \\mathcal{E}$, s.t. $M_1(\\ell_1) \\neq M_1(\\ell_2) \\wedge M_2(\\ell_1)=M_2(\\ell_2)$.\n% \\end{definition}\n\n% Now, we can proceed to prove Theorem~\\ref{prop:expressive}.\n% \\begin{theorem*}\n%     Let $\\mathcal{G}_{\\text{attr}}$ denote the space of attributed graphs, and $\\mathcal{H}_{\\text{ind}}$ denote the set of individual heuristics that can be represented with the heuristic formulation.\n%     Let $\\mathrm{GNN}_k$ denote a GNN with $k$ layers. Then the following statements hold:\n%     \\begin{enumerate}\n%         \\item $\\forall h \\in \\mathcal{H}_{\\text{ind}}, \\boldsymbol{H} \\succ h;$\n%         \\item In $\\mathcal{G}_{\\text{attr}}, \\forall h \\in \\mathcal{H}_{\\text{ind}}, \\textnormal{HL-GNN} \\succ h;$\n%         \\item $\\forall k<L, \\textnormal{HL-GNN} \\succ \\mathrm{GNN}_k;$\n%     \\end{enumerate}\n%     where $\\boldsymbol{H}$ denotes the heuristic formulation defined in Equation~\\eqref{eq:formulation}, and $L$ is the depth of HL-GNN.\n% \\end{theorem*}\n\n% \\begin{figure}[tbp]\n%     \\centering\n%     \\includegraphics[width=0.46\\textwidth]{figs/Graph_instance.pdf}\n%     \\caption{A graph instance forming a loop with 24 nodes. Various colors are used to represent distinct node features.}\n%     \\label{fig:instance}\n% \\end{figure}\n\n% \\begin{proof}\n% For (1), The unified heuristic formulation is a polynomial of adjacency matrices, meaning any difference in the polynomial's terms will result in a different final outcome. Therefore, the heuristic formulation is strictly more expressive than any individual heuristic that can be represented by the heuristic formulation.\n\n% For (2), We prove the theorem using the graph instance in Figure~\\ref{fig:instance}, specifically focusing on the non-automorphic links $(v_1, v_7)$ and $(v_{13}, v_7)$.\n% If we disregard the distinct node features of nodes $v_6$ and $v_8$, as well as nodes $v_{20}$ and $v_{18}$, we observe that links $(v_1, v_7)$ and $(v_{13}, v_7)$ are automorphic. Heuristics rely solely on graph topology and do not consider node features. Therefore, any individual heuristic, whether local or global, cannot distinguish between links $(v_1, v_7)$ and $(v_{13}, v_7)$, as they are topologically equivalent.\n% However, HL-GNN can effectively reach a depth of around 20 layers, enabling it to learn distinct representations for nodes $v_1$ and $v_{13}$. Aggregating representations of nodes $v_1$, $v_7$, and nodes $v_{13}$, $v_7$ leads to different representations for links $(v_1, v_7)$ and $(v_{13}, v_7)$.\n\n% For (3), conventional GNNs, typically consisting of 2-3 layers, produce identical representations for nodes $v_1$ and $v_{13}$, since the difference in node features appears on the 5-hop neighbors of nodes $v_1$ and $v_{13}$. In other words, any GNN that cannot penetrate beyond 5 layers cannot learn distinct representations for nodes $v_1$ and $v_{13}$. Directly aggregating representations of nodes $v_1, v_7$, and nodes $v_{13}, v_7$ does not lead to different representations for links $(v_1, v_7)$ and $(v_{13}, v_7)$. Therefore, conventional GNNs cannot discriminate between links $(v_1, v_7)$ and $(v_{13}, v_7)$, while HL-GNN can.\n% \\end{proof}\n\n\n"
                }
            },
            "section 9": {
                "name": "Additional Experimental Details",
                "content": "\n",
                "subsection 9.1": {
                    "name": "Detailed Information about Datasets",
                    "content": "\n\\label{para:dataset_statistics}\n\n\n\nWe employ a diverse set of benchmark datasets to comprehensively evaluate the link prediction performance of HL-GNN. Each dataset captures distinct aspects of relationships and presents unique challenges for link prediction.\nWe utilize nine datasets from three sources: Planetoid~\\cite{planetoid}, Amazon~\\cite{amazon}, and OGB~\\cite{OGB}. \nThe Planetoid datasets include \\texttt{Cora}, \\texttt{Citeseer}, and \\texttt{Pubmed}. The Amazon datasets include \\texttt{Photo} and \\texttt{Computers}. \nThe OGB datasets include \\texttt{ogbl-collab}, \\texttt{ogbl-ddi}, \\texttt{ogbl-ppa}, and \\texttt{ogbl-citation2}. \nThe statistics of each dataset are shown in Table~\\ref{tab:dataset_statistics}.\n\nWe introduce each dataset briefly: \n\\begin{itemize}\n    \\item \\texttt{Cora}: This dataset represents a citation network. Nodes in the graph correspond to documents, and edges denote citations between documents. Each document is associated with a bag-of-words representation.\n    \\item \\texttt{Citeseer}: Similar to \\texttt{Cora}, \\texttt{Citeseer} is also a citation network. Nodes represent scientific articles, and edges signify citations. The node features include bag-of-words representations and information about the publication.\n    \\item \\texttt{Pubmed}: Another citation network, \\texttt{Pubmed} comprises nodes corresponding to scientific publications, and edges represent citations. Node features consist of binary indicators of word occurrences in the documents.\n    \\item \\texttt{Photo}: This dataset captures a social network, where nodes represent users and edges denote friendships. Node features encompass visual and textual information from user profiles.\n    \\item \\texttt{Computers}: Representing a co-purchase network, \\texttt{Computers} has nodes representing computers and edges indicating co-purchases. Node features involve TF-IDF representations of computer terms.\n    \\item \\texttt{ogbl-collab}: This dataset is an author collaboration network. Nodes are authors, and edges denote collaborations. The node features contain bag-of-words representations of authors' publications.\n    \\item \\texttt{ogbl-ddi}: \\texttt{ogbl-ddi} stands for drug-drug interaction, and this network models interactions between drugs. Edges indicate interactions, and the dataset contains information about drug structures.\n    \\item \\texttt{ogbl-ppa}: This dataset represents a protein-protein association network. Nodes correspond to proteins, and edges indicate associations based on various biological evidence. Node features include protein sequences and structural information.\n    \\item \\texttt{ogbl-citation2}: This dataset is a citation network that focuses on the dynamics of citations. Nodes represent papers, and edges indicate citation relationships. Node features consist of paper metadata.\n\\end{itemize}\n\n\nThese datasets are widely used for evaluating link prediction methods, each presenting unique challenges based on the nature of the relationships captured in the respective graphs.\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Detailed Information about Experiments",
                    "content": "\n\\label{para:exp_detail}\n\n\n\nIn our experiments, we employ a set of hyperparameters for training HL-GNN across various datasets. The hyperparameter details are outlined in Table~\\ref{tab:hyperparameter}. All experiments can be performed using an A100 (80GB) GPU. Hyperparameters are tuned via random search using Weights and Biases, involving 50 runs. The hyperparameters yielding the highest validation accuracy were selected, and results are reported on the test set.\n\nThe number of layers in HL-GNN is set to 20 for the Planetoid and Amazon datasets, and set to 15 for the OGB datasets. We keep the hidden dimensionality of HL-GNN the same as the raw node features on the Planetoid and Amazon datasets. Additionally, we leverage node embeddings on the OGB datasets to enhance the node representations. The dimensions of node embeddings are set to 256, 512, 256, and 64 for \\texttt{ogbl-collab}, \\texttt{ogbl-ddi}, \\texttt{ogbl-ppa}, and \\texttt{ogbl-citation2}, respectively.\n \nInspired by heuristics KI and RWR presented in Table~\\ref{tab:heursitics}, we explore different initialization strategies for $\\beta^{(l)}$. We adopt KI initialization as $\\beta^{(l)} = \\gamma^l$ and RWR initialization as $\\beta^{(l)} = (1-\\alpha)\\alpha^l$. We consistently adopt KI initialization for the OGB datasets and RWR initialization for the Planetoid and Amazon datasets (except for $\\texttt{Pubmed}$, which utilizes KI initialization). We do not impose constraints on $\\beta^{(l)}$, allowing them to take positive or negative values.\n\nThe MLP utilized in HL-GNN serves as the predictor and comprises 3 layers for most Planetoid and Amazon datasets, and comprises 2 layers for the OGB datasets. \nWe employ the Adam optimizer with a fixed learning rate of 0.001 for all datasets. Training is carried out for a specified number of epochs, ranging from 100 to 800, depending on the dataset. Dropout regularization is optimized among \\{0.3, 0.4, 0.5, 0.6, 0.7\\}.\n\n\n\n"
                }
            },
            "section 10": {
                "name": "Additional Experiments",
                "content": "\n",
                "subsection 10.1": {
                    "name": "More Ablation Studies",
                    "content": "\n",
                    "subsubsection 10.1.1": {
                        "name": "Different initialization strategies",
                        "content": "\nWe investigate the impact of various initial values for $\\beta^{(l)}$ on the final performance in this section. \nInspired by heuristics KI and RWR in Table~\\ref{tab:heursitics}, we explore different initialization strategies, including:\n\\begin{itemize}\n\\item KI initialization: $\\beta^{(l)} = \\gamma^l$.\n\\item RWR initialization: $\\beta^{(l)} = (1-\\alpha)\\alpha^l$.\n\\item Random initialization: $\\beta^{(l)} \\sim \\mathcal{N}(0, 1)$.\n\\item Uniform initialization: $\\beta^{(l)} = 1/L$.\n\\item Reverse-KI initialization: $\\beta^{(l)} = \\gamma^{L-l}$.\n\\item Final-layer initialization: $\\beta^{(L)}=1$, with others set to 0.\n\\end{itemize}\n\nThe results of these different initialization strategies are presented in Figure~\\ref{fig:initial}, with performance comparisons on four datasets. Notably, the KI and RWR initialization strategies outperform the others, underscoring the importance of emphasizing short-range dependencies, given their direct topological relevance.\n\nIt is noteworthy that even without extensive tuning, KI initialization at $\\gamma=0.5$ for the OGB datasets and RWR initialization at $\\alpha=0.2$ for the Planetoid and Amazon datasets consistently demonstrate strong performance. In practical applications, we consistently adopt KI initialization for the OGB datasets and RWR initialization for the Planetoid and Amazon datasets (except for $\\texttt{Pubmed}$, which utilizes KI initialization) in our experiments. The initialization strategies for each dataset are summarized in Table~\\ref{tab:hyperparameter}.\n\n\n\n\n\n\n"
                    },
                    "subsubsection 10.1.2": {
                        "name": "Introduction of transformation and activation functions",
                        "content": "\nTo investigate if the introduction of transformation and activation functions could potentially improve the performance of HL-GNN, we conduct additional experiments on these two functions. The results are summarized in Table~\\ref{tab:appen_trans}.\n\n\n\nThe results reveal that the introduction of transformation and activation functions do not lead to an improvement in performance. This outcome can be attributed to the disturbance of learned heuristics by non-linearity, preventing them from retaining the form of matrix multiplications, which is the foundation of our formulation.\nFurthermore, the inclusion of transformation matrices significantly increases the number of parameters, posing challenges to the learning process. Consequently, HL-GNN with transformation is unable to maintain the depth of around 20 layers. \nThe optimal performance is achieved with a limited depth of less than 5 layers. \nThis limitation contributes to the observed inferior performance of HL-GNN (w/ tran.) and HL-GNN (w/ ReLU act. \\& tran.).\n\n"
                    }
                },
                "subsection 10.2": {
                    "name": "Interpretability of Generalized Heuristics",
                    "content": "\n\\label{para:learned_heuristics}\nIn this section, we present the learned heuristic formulation $\\boldsymbol{H}$ to exhibit the interpretability of generalized heuristics. It's worth noting that each individual heuristic for a link $(i,j)$ can be directly extracted from the $(i,j)$ entry of $\\boldsymbol{H}$. For clarity, we simplify Equation~\\eqref{eq:relaxation} by omitting $\\alpha^{(l)}$ and utilizing $\\mathbb{A}^{(l)}=\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}$. For conciseness, we provide the first ten weights below.\n\n    \nFor $\\texttt{Cora}$,\n    \\begin{equation}\n    \\begin{aligned}\n\\boldsymbol{H}_{\\mathrm{Cora}}&=0.1795\\boldsymbol{I}+ 0.1894\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+0.1484\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n+0.1213\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n&+0.0963\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4 +0.0768\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5+0.0606\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6\n+0.0477\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n&+0.0371\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8+0.0285\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}\n\nFor $\\texttt{Citeseer}$,    \n    \\begin{equation}\n    \\begin{aligned}\n    \\boldsymbol{H}_{\\mathrm{Citeseer}}&=0.1993\\boldsymbol{I}+ 0.1759\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+0.1380\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n    +0.1111\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n    &+0.0878\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4+0.0694\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5+ 0.0544\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6+0.0422\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n    &+0.0324\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8+0.0244\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}\n\nFor $\\texttt{Pubmed}$, \n    \\begin{equation}\n    \\begin{aligned}\n\\boldsymbol{H}_{\\mathrm{Pubmed}}&=0.7386\\boldsymbol{I}+ 0.5335\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+0.3126\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n+0.2787\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n&+0.2429\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4+0.2252\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5+ 0.2039\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6+0.1884\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n&+0.1722\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8+0.1592\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}\n\nFor $\\texttt{ogbl-collab}$, \n    \\begin{equation}\n    \\begin{aligned}\n    \\boldsymbol{H}_{\\mathrm{collab}}&=0.1599\\boldsymbol{I}+ 0.1004\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+0.4826\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n    +0.2545\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n    &+0.2328\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4+0.1663\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5+ 0.1210\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6+0.0721\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n    &+0.0252\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8-0.0224\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}\n\nFor $\\texttt{ogbl-ddi}$, \n    \\begin{equation}\n    \\begin{aligned}\n    \\boldsymbol{H}_{\\mathrm{ddi}}&=0.1532\\boldsymbol{I}+ 0.2840\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+1.5298\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n    +0.1154\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n    &+0.1123\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4-0.0507\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5- 0.0739\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6-0.1022\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n    &-0.1102\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8-0.1164\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}\n\n\n\n"
                },
                "subsection 10.3": {
                    "name": "Leveraging Generalized Heuristics",
                    "content": "\n\\label{para:mlp_only}\n\n\n\nWith a generalized heuristic for each dataset, there is no need to train a GNN and predictor from scratch. Instead, we can simply follow the generalized heuristic and train an MLP predictor only, which is significantly more efficient than training from scratch. We utilize a pretrained preprocessing block, which is a linear layer that transforms the dimension of node features into hidden channels of the GNN. \nWe also train node embeddings on the OGB datasets to enhance the node representations.\n\nThe performance for training from scratch and training the MLP only is presented in Table~\\ref{tab:performance_mlp}. Training the MLP only yields comparable performance to training from scratch, but with quicker convergence. This highlights the efficacy of the learned generalized heuristics, allowing us to achieve comparable results efficiently by training a simple MLP. This significantly improves efficiency and conserves computational resources.\n\n\n\n\n\n"
                }
            },
            "section 11": {
                "name": "New Heuristics Derivation",
                "content": "\n\\label{para:heuristic_advance}\nExisting heuristics are primarily handcrafted and may not be optimal for real-world graphs.\nLeveraging the propagation operators, weight parameters and maximum order offers the potential to learn generalized, possibly more effective heuristics. We introduce two new heuristics below.\n\n\\begin{lemma}\nGiven $\\mA^{(1)}=\\Acs$, $\\mA^{(2)}=\\Ars$, $\\beta^{(0)}=\\beta^{(1)}=0$, $\\beta^{(2)}=1$, $L=2$ within the $h(i,j)$ formulation, we derive a new local heuristic represented as \n\\[ s(i, j)=\\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k^2}. \\]\n\\end{lemma}\n\\begin{proof}\n    From the given configurations and the $h(i,j)$ formulation, we obtain:\n    \\begin{equation}\n    h(i, j) = (\\Acs \\Ars)_{i, j} = \\sum_{k \\in \\mathcal{V}} \\frac{\\tilde{a}_{i k}}{\\tilde{d}_k} \\frac{\\tilde{a}_{k j}}{\\tilde{d}_k} = \\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k^2}.\n    \\end{equation}\n    Consequently, the new local heuristic is $s(i, j) = \\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k^2}$.\n\\end{proof}\n\nOur newly proposed heuristic draws parallels with both the Resource Allocation Index (RA)~\\cite{RA} and the Adamic-Adar Index (AA)~\\cite{AA}, yet introduces a larger penalty for high-degree nodes.\n\n\\begin{lemma}\nGiven $\\mA^{(1)}=\\mA^{(2)}=\\Asym$, $\\beta^{(0)}=\\beta^{(1)}=0$, $\\beta^{(2)}=1$, $L=2$ within the $h(i,j)$ formulation, we derive a new local heuristic represented as \n\\begin{equation}\n    s(i, j)=\\frac{1}{\\sqrt{\\tilde{d}_i \\tilde{d}_j}} \\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k}=\\frac{s_{\\mathrm{RA}}(i, j)}{\\sqrt{\\tilde{d}_i \\tilde{d}_j}}.\n\\end{equation}\n\\end{lemma}\n\\begin{proof}\n    From the given configurations and the $h(i,j)$ formulation, we obtain:\n    \\begin{equation}\n    \\begin{aligned}\n        h(i, j)&=(\\Asym^2)_{i, j}=\\sum_{k \\in \\mathcal{V}} \\frac{\\tilde{a}_{i k}}{\\sqrt{\\tilde{d}_i \\tilde{d}_k}} \\frac{\\tilde{a}_{k j}}{\\sqrt{\\tilde{d}_k \\tilde{d}_j}}\\\\\n        &=\\frac{1}{\\sqrt{\\tilde{d}_i \\tilde{d}_j}} \\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k}=\\frac{s_{\\mathrm{RA}}(i, j)}{\\sqrt{\\tilde{d}_i \\tilde{d}_j}}.\n    \\end{aligned}\n    \\end{equation}\n    Consequently, the new local heuristic is $s(i, j) = \\frac{s_{\\mathrm{RA}}(i, j)}{\\sqrt{\\tilde{d}_i \\tilde{d}_j}}$.\n\\end{proof}\n\n\nThe RA Index, represented by either $ s_{\\mathrm{RA}}(i, j) = (\\Acs \\A)_{i, j}$ or $s_{\\mathrm{RA}}(i, j) = (\\A \\Ars)_{i, j}$, is unnormalized. However, based on our formulation, we present a symmetrically normalized version of the RA Index that potentially offers improved efficacy compared to the original RA Index.\n\nAdjustments to the propagation operators and weight parameters also yield innovations in global heuristics. As one example, replacing the transition matrix $\\Acs$ in the Random Walk with Restart (RWR)~\\cite{PPR} with a symmetrically normalized matrix $\\Asym$ births the Flow Propagation (FP)~\\cite{FP} heuristic. A comprehensive list of novel global heuristics is beyond the scope of this section.\n\n"
            },
            "section 12": {
                "name": "Time complexity analysis",
                "content": "\n\\label{para:time_comp_analysis}\nGCN~\\cite{GCN}. The propagation cost is $\\mathcal{O}(LMF)$ and the transformation cost with weight matrices $\\boldsymbol{W}^{(l)}$ is $\\mathcal{O}(LNF^2)$ if we do not modify feature dimensionality with $\\boldsymbol{W}^{(l)}$. And the total cost is $\\mathcal{O}(LF(M+NF))$.\n\nGAT~\\cite{GAT}. Denote the number of attention heads as $K$ and the average degree of nodes as $D$. Attention computation cost is $\\mathcal{O}(NDF^2)$. The cost of computing one node's representation is $\\mathcal{O}(ND^2F^2)$. For $K$ attention heads and $L$ layers, the overall time complexity is $\\mathcal{O}(LKND^2F^2)$.\n\nSEAL~\\cite{SEAL}. Denote $E$ and $V$ as the average number of edges and vertices in the subgraphs. The complexity in constructing the enclosing subgraph is at most $\\mathcal{O}(D^3)$, and the cost of computing shortest path is dependent on the algorithm, and the most efficient Dijkstras algorithm is $\\mathcal{O}(V^2)$. The cost of the subgraph GNN is $\\mathcal{O}(LEF)$. The algorithm need to be done for each edge, and the overall cost is $\\mathcal{O}(M(V^2 + LEF))$.\n\nNBFNet~\\cite{NBFNet}. INDICATOR function takes $\\mathcal{O}(NF)$ cost, MESSAGE function takes $\\mathcal{O}(L(M+N)F)$ cost, and AGGREGATE function takes $\\mathcal{O}(LNF^2)$ cost. The algorithm need to be done for each source node, and the total cost is $\\mathcal{O}(LNF(M+NF))$.\n\nNeo-GNN~\\cite{Neo-GNN}. Denote the high-order matrix power as $L'$ and the average degree of nodes as $D$. Node structural feature computational cost is $\\mathcal{O}(NDF^2)$, computing high-order matrix power up to $L'$ is $\\mathcal{O}(L'N^2)$, computing output feature is $\\mathcal{O}(MF)$. And Neo-GNN also needs a conventional GNN which needs a $\\mathcal{O}(L(MF+NF^2))$ cost. The total cost is $\\mathcal{O}(LMF+NDF^2)$.\n\nBUDDY~\\cite{BUDDY}. Denote the cost of hash operations as $H$. The preprocessing cost of BUDDY is $\\mathcal{O}(LM(F+H))$. A link probability is computed by (i) extracting $L(L+2)$ structural features, which costs $\\mathcal{O}(L^2 H)$; (ii) An MLP on structural and node features, which costs $\\mathcal{O}(L^2 H+LF^2)$. The cost for computing probabilities for all links is $\\mathcal{O}(LM(LH+F^2))$. The total cost is $\\mathcal{O}(LM(LH+F^2))$.\n\n\n"
            },
            "section 13": {
                "name": "Limitation",
                "content": "\nOur heuristic formulation does not aim to encompass all possible heuristics. A successful formulation generalizes heuristic commonalities and applies them for effective link prediction. Our formulation distills the critical characteristics of heuristics, specifically focusing on extracting common neighbors from local heuristics and global paths from global heuristics.\n\nUpon careful consideration of the comprehensive survey~\\cite{survey1,survey2}, our formulation may not contain certain heuristics with normalization operators like union, log, minimum, and maximum in the denominator. For example, heuristics like Jaccard and Adamic-Adar, due to their use of union and log operators in the denominator, cannot be directly represented as matrix multiplications. \nHowever, our formulation introduces normalization through the three propagation mechanisms and layer-specific learnable weights to adaptively control the degree of normalization, potentially mitigating the need for additional operators.\n\n\n\n"
            }
        },
        "tables": {
            "tab:adjacency_matrices": "\\begin{table}[tbp]\n    \\small\n    \\caption{Notations and expressions of adjacency matrices.}\n    \\label{tab:adjacency_matrices}\n    \\centering\n    \\vspace{-5px}\n    \\setlength{\\tabcolsep}{8pt}\n    \\begin{tabular}{lcc} \n         \\toprule\n         \\textbf{Adjacency Matrix} & \\textbf{Notation} & \\textbf{Expression} \\\\\n         \\midrule\n         Matrix with Self-Loops & $\\tilde{\\boldsymbol{A}}$ & $\\boldsymbol{A} + \\boldsymbol{I}_N$ \\\\\n         Symmetrical Matrix & $\\Asym$ & $\\tilde{\\boldsymbol{D}}^{-1/2} \\tilde{\\boldsymbol{A}} \\tilde{\\boldsymbol{D}}^{-1/2}$ \\\\\n         Row-Stochastic Matrix & $\\Ars$ & $\\tilde{\\boldsymbol{D}}^{-1} \\tilde{\\boldsymbol{A}}$ \\\\\n         Column-Stochastic Matrix & $\\Acs$ & $\\tilde{\\boldsymbol{A}} \\tilde{\\boldsymbol{D}}^{-1}$ \\\\\n         Propagation Operator & $\\mathbb{A}$ & $\\{\\A, \\Asym, \\Ars, \\Acs\\}$ \\\\\n         \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:heursitics": "\\begin{table*}[tbp]\n\\caption{A selection of traditional local and global heuristics, their mathematical expressions, their matrix forms, and specific configurations within the unified heuristic formulation for alignment.}\n\\label{tab:heursitics}\n\\centering\n\\small\n\\begin{threeparttable}\n\\begin{tabular}{llcccc}\n    \\toprule\n    \\textbf{Type} & \\textbf{Method} & \\textbf{Expression}  &\\textbf{Matrix Form}& \\textbf{Propagation Operators $\\mA^{(m)}$} & \\textbf{Weight Parameters} $\\beta^{(l)}$ \\\\\n    \\midrule\n    Local & CN & $|\\Gamma_i \\cap \\Gamma_j|$  &$(\\A^2)_{i, j}$& $\\mA^{(1)}=\\mA^{(2)} = \\A$ & $\\beta^{(0)}= \\beta^{(1)} = 0$, $\\beta^{(2)} = 1$ \\\\\n    Local & LLHN & $\\frac{|\\Gamma_i \\cap \\Gamma_j|}{\\tilde{d}_i \\tilde{d}_j}$  &$(\\Ars \\Acs)_{i, j}$& \\(\\mA^{(1)} = \\Ars\\), \\(\\mA^{(2)} = \\Acs\\) & \\(\\beta^{(0)}=\\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\) \\\\\n    Local & RA & $\\sum_{k \\in \\Gamma_i \\cap \\Gamma_j} \\frac{1}{\\tilde{d}_k}$  &$(\\Acs \\A)_{i, j}$& \\(\\mA^{(1)} = \\Acs\\), \\(\\mA^{(2)} = \\A\\) \\tnote{*}  & \\(\\beta^{(0)}=\\beta^{(1)} = 0\\), \\(\\beta^{(2)} = 1\\) \\\\\n    Global& KI & $\\sum_{l=1}^{\\infty} \\gamma^l |{\\textnormal{paths}}_{i,j}^l|$  &$\\left(\\sum_{l=1}^{\\infty} \\gamma^l\\A^l\\right)_{i,j}$ & $\\mA^{(m)} = \\A$ for $m \\geq 1$ & $\\beta^{(0)} = 0$, $ \\beta^{(l)}=\\gamma^l$ for $l \\geq 1$ \\\\\n    Global& GLHN & $\\sum_{l=0}^{\\infty} \\phi^l |{\\textnormal{paths}}_{i,j}^l|$  &$\\left(\\boldsymbol{I}_N+\\sum_{l=1}^{\\infty} \\phi^l \\A^l \\right)_{i,j}$& $\\mA^{(m)} = \\A$ for $m \\geq 1$ & $\\beta^{(0)} = 1$, $ \\beta^{(l)}=\\phi^l$ for $l \\geq 1$ \\\\\n    Global& RWR & $[\\boldsymbol{\\pi}_i(\\infty)]_j$  &$\\left(\\sum_{l=0}^{\\infty}(1-\\alpha)\\alpha^l \\Ars^l\\right)_{i,j}$& $\\mA^{(m)} = \\Ars$ for $m \\geq 1$ & $\\beta^{(l)}=(1-\\alpha)\\alpha^l$ for $l \\geq 0$ \\\\\n    Global & LPI & $\\sum_{l=2}^L \\gamma^{l-2} |{\\textnormal{paths}}_{i,j}^l|$  &$\\left(\\sum_{l=2}^L \\gamma^{l-2} \\A^l\\right)_{i,j}$& $\\mA^{(m)} = \\A$ for $1 \\leq m \\leq L$ & $\\beta^{(0)} = \\beta^{(1)} =0$, $ \\beta^{(l)}=\\gamma^{l-2}$ for $2 \\leq l \\leq L$ \\\\\n    Global & LRW & $\\frac{\\tilde{d}_i}{2M}[\\boldsymbol{\\pi}_i(L)]_j$  &$\\left(\\sum_{l=0}^{L-1}\\frac{\\tilde{d}_i}{2M}(1-\\alpha)\\alpha^l \\Ars^l\\right)_{i,j}$& $\\mA^{(m)} = \\Ars$ for $1 \\leq m \\leq L-1$ & $\\beta^{(l)}=\\frac{\\tilde{d}_i}{2M} (1-\\alpha)\\alpha^l$ for $0 \\leq l \\leq L-1$ \\\\\n    \\bottomrule\n\\end{tabular}\n\\begin{tablenotes}\n    \\footnotesize\n    \\item[*] When setting \\(\\mA^{(1)} = \\A\\), \\(\\mA^{(2)} = \\Ars\\) in the formulation, it also aligns with the RA Index.\n  \\end{tablenotes}\n\\end{threeparttable}\n\\end{table*}",
            "tab:comparison": "\\begin{table}[tbp]\n    \\caption{Comparison of heuristic-learning ability, information range, and time complexity of HL-GNN with conventional GNNs and heuristic-inspired GNN methods.}\n    \\label{tab:comparison}\n    \\centering\n    \\small\n    \\begin{tabular}{lccc}\n    \\toprule\n         & \\textbf{Learned Heuristics}& \\textbf{Range}&\\textbf{Time Complexity}\\\\\n    \\midrule\n         \\textbf{GCN}&N.A.&3 hops&$\\mathcal{O}(LF(M+NF))$\\\\\n         \\textbf{GAT}&N.A.&3 hops&$\\mathcal{O}(LKND^2F^2)$\\\\\n         \\textbf{SEAL}&Local&3 hops&$\\mathcal{O}(M(V^2 + LEF))$\\\\\n         \\textbf{NBFNet}&Global&6 hops&$\\mathcal{O}(LNF(M+NF))$\\\\\n         \\textbf{Neo-GNN}&Local&2 hops&$\\mathcal{O}(LMF+NDF^2)$\\\\\n         \\textbf{BUDDY}& Local&3 hops&$\\mathcal{O}(LM(LH+F^2))$\\\\\n    \\midrule\n         \\textbf{HL-GNN}&Local / Global&20 hops&$\\mathcal{O}(LMF)$\\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:main_result": "\\begin{table*}[tbp]\n    \\caption{Results on link prediction benchmarks including the Planetoid, Amazon, and OGB datasets. Results are presented as average $\\pm$ standard deviation. The best and second-best performances are marked with \\textbf{bold} and \\underline{underline}, respectively. OOM denotes out of GPU memory.}\n    \\label{tab:main_result}\n    \\centering\n    \\begin{tabular}{lccccccccc}\n    \\toprule\n    & \\texttt{Cora} & \\texttt{Citeseer} & \\texttt{Pubmed} & \\texttt{Photo} & \\texttt{Computers} & \\texttt{collab} & \\texttt{ddi} & \\texttt{ppa} & \\texttt{citation2} \\\\\n    & Hits@100 & Hits@100 & Hits@100 & AUC & AUC & Hits@50 & Hits@20 & Hits@100 & MRR \\\\\n    \\midrule\n    \\textbf{CN} & 33.92\\footnotesize{$\\pm$0.46} & 29.79\\footnotesize{$\\pm$0.90} & 23.13\\footnotesize{$\\pm$0.15} & 96.73\\footnotesize{$\\pm$0.00} & 96.15\\footnotesize{$\\pm$0.00} & 56.44\\footnotesize{$\\pm$0.00} & 17.73\\footnotesize{$\\pm$0.00} & 27.65\\footnotesize{$\\pm$0.00} & 51.47\\footnotesize{$\\pm$0.00} \\\\\n    \\textbf{RA} & 41.07\\footnotesize{$\\pm$0.48} & 33.56\\footnotesize{$\\pm$0.17} & 27.03\\footnotesize{$\\pm$0.35} & 97.20\\footnotesize{$\\pm$0.00} & 96.82\\footnotesize{$\\pm$0.00} & 64.00\\footnotesize{$\\pm$0.00} & 27.60\\footnotesize{$\\pm$0.00} & 49.33\\footnotesize{$\\pm$0.00} & 51.98\\footnotesize{$\\pm$0.00} \\\\\n    \\textbf{KI} & 42.34\\footnotesize{$\\pm$0.39} & 35.62\\footnotesize{$\\pm$0.33} & 30.91\\footnotesize{$\\pm$0.69} & 97.45\\footnotesize{$\\pm$0.00} & 97.05\\footnotesize{$\\pm$0.00} & 59.79\\footnotesize{$\\pm$0.00} & 21.23\\footnotesize{$\\pm$0.00} & 24.31\\footnotesize{$\\pm$0.00} & 47.83\\footnotesize{$\\pm$0.00} \\\\\n    \\textbf{RWR} & 42.57\\footnotesize{$\\pm$0.56} & 36.78\\footnotesize{$\\pm$0.58} & 29.77\\footnotesize{$\\pm$0.45} & 97.51\\footnotesize{$\\pm$0.00} & 96.98\\footnotesize{$\\pm$0.00} & 60.06\\footnotesize{$\\pm$0.00} & 22.01\\footnotesize{$\\pm$0.00} & 22.16\\footnotesize{$\\pm$0.00} & 45.76\\footnotesize{$\\pm$0.00} \\\\\n    \\midrule\n    \\textbf{MF} & 64.67\\footnotesize{$\\pm$1.43} & 65.19\\footnotesize{$\\pm$1.47} & 46.94\\footnotesize{$\\pm$1.27} & 97.92\\footnotesize{$\\pm$0.37} & 97.56\\footnotesize{$\\pm$0.66} & 38.86\\footnotesize{$\\pm$0.29} & 13.68\\footnotesize{$\\pm$4.75} & 32.29\\footnotesize{$\\pm$0.94} & 51.86\\footnotesize{$\\pm$4.43} \\\\\n    \\textbf{Node2vec} & 68.43\\footnotesize{$\\pm$2.65} & 69.34\\footnotesize{$\\pm$3.04} & 51.88\\footnotesize{$\\pm$1.55} & 98.37\\footnotesize{$\\pm$0.33} & 98.21\\footnotesize{$\\pm$0.39} & 48.88\\footnotesize{$\\pm$0.54} & 23.26\\footnotesize{$\\pm$2.09} & 22.26\\footnotesize{$\\pm$0.88} & 61.41\\footnotesize{$\\pm$0.11} \\\\\n    \\textbf{DeepWalk} & 70.34\\footnotesize{$\\pm$2.96} & 72.05\\footnotesize{$\\pm$2.56} & 54.91\\footnotesize{$\\pm$1.25} & 98.83\\footnotesize{$\\pm$0.23} & 98.45\\footnotesize{$\\pm$0.45} & 50.37\\footnotesize{$\\pm$0.34} & 26.42\\footnotesize{$\\pm$6.10} & 35.12\\footnotesize{$\\pm$0.79} & 55.58\\footnotesize{$\\pm$1.75} \\\\\n    \\midrule\n    \\textbf{GCN} & 66.79\\footnotesize{$\\pm$1.65} & 67.08\\footnotesize{$\\pm$2.94} & 53.02\\footnotesize{$\\pm$1.39} & 98.61\\footnotesize{$\\pm$0.15} & 98.55\\footnotesize{$\\pm$0.27} & 47.14\\footnotesize{$\\pm$1.45} & 37.07\\footnotesize{$\\pm$5.07} & 18.67\\footnotesize{$\\pm$1.32} & 84.74\\footnotesize{$\\pm$0.21} \\\\\n    \\textbf{GAT} & 60.78\\footnotesize{$\\pm$3.17} & 62.94\\footnotesize{$\\pm$2.45} & 46.29\\footnotesize{$\\pm$1.73} & 98.42\\footnotesize{$\\pm$0.19} & 98.47\\footnotesize{$\\pm$0.32} & 55.78\\footnotesize{$\\pm$1.39} & 54.12\\footnotesize{$\\pm$5.43} & 19.94\\footnotesize{$\\pm$1.69} & 86.33\\footnotesize{$\\pm$0.54} \\\\\n    \\textbf{SEAL} & 81.71\\footnotesize{$\\pm$1.30}& 83.89\\footnotesize{$\\pm$2.15}& \\underline{75.54\\footnotesize{$\\pm$1.32}} & 98.85\\footnotesize{$\\pm$0.04}& \\underline{98.70\\footnotesize{$\\pm$0.18}} & 64.74\\footnotesize{$\\pm$0.43}& 30.56\\footnotesize{$\\pm$3.86} & 48.80\\footnotesize{$\\pm$3.16} & \\underline{87.67\\footnotesize{$\\pm$0.32}}\\\\\n    \\textbf{NBFNet} & 71.65\\footnotesize{$\\pm$2.27} & 74.07\\footnotesize{$\\pm$1.75} & 58.73\\footnotesize{$\\pm$1.99} & 98.29\\footnotesize{$\\pm$0.35} & 98.03\\footnotesize{$\\pm$0.54} & OOM & 4.00\\footnotesize{$\\pm$0.58} & OOM & OOM \\\\\n    \\textbf{Neo-GNN} & 80.42\\footnotesize{$\\pm$1.31} & 84.67\\footnotesize{$\\pm$2.16}& 73.93\\footnotesize{$\\pm$1.19} & 98.74\\footnotesize{$\\pm$0.55} & 98.27\\footnotesize{$\\pm$0.79} & 62.13\\footnotesize{$\\pm$0.58} & 63.57\\footnotesize{$\\pm$3.52}& 49.13\\footnotesize{$\\pm$0.60} & 87.26\\footnotesize{$\\pm$0.84} \\\\\n    \\textbf{BUDDY} & \\underline{88.00\\footnotesize{$\\pm$0.44}} & \\underline{92.93\\footnotesize{$\\pm$0.27}} & 74.10\\footnotesize{$\\pm$0.78} & \\underline{99.05\\footnotesize{$\\pm$0.21}} & 98.69\\footnotesize{$\\pm$0.34} & \\underline{65.94\\footnotesize{$\\pm$0.58}} & \\underline{78.51\\footnotesize{$\\pm$1.36}} & \\underline{49.85\\footnotesize{$\\pm$0.20}}& 87.56\\footnotesize{$\\pm$0.11} \\\\\n    \\textbf{HL-GNN} & \\textbf{94.22\\footnotesize{$\\pm$1.64}} & \\textbf{94.31\\footnotesize{$\\pm$1.51}} & \\textbf{88.15\\footnotesize{$\\pm$0.38}} & \\textbf{99.11\\footnotesize{$\\pm$0.07}} & \\textbf{98.82\\footnotesize{$\\pm$0.21}} & \\textbf{68.11\\footnotesize{$\\pm$0.54}} & \\textbf{80.27\\footnotesize{$\\pm$3.98}} & \\textbf{56.77\\footnotesize{$\\pm$0.84}}& \\textbf{89.43\\footnotesize{$\\pm$0.83}} \\\\\n    \\bottomrule\n    \\end{tabular}\n\n\\end{table*}",
            "tab:wall_time": "\\begin{table}[tbp]\n    \\caption{Wall time per epoch (in seconds) for training HL-GNN compared to other GNN methods. The shortest and second shortest times are marked with \\textbf{bold} and \\underline{underline}, respectively.}\n    \\label{tab:wall_time}\n    \\centering\n    \\small\n    \\setlength{\\tabcolsep}{7pt}\n    \\begin{tabular}{lccccc}\n    \\toprule\n       & \\texttt{Cora} & \\texttt{Citeseer} & \\texttt{Pubmed}& \\texttt{collab}  &\\texttt{ddi}\\\\\n    \\midrule\n         \\textbf{GCN}&  \\textbf{0.02}&  \\textbf{0.03}&  \\textbf{0.4}&  \\textbf{5.3}& \\textbf{9.2}\\\\\n         \\textbf{GAT}&  \\underline{0.05}&  0.06&  \\underline{0.5}&  \\underline{5.8}& \\underline{10.4}\\\\\n         \\textbf{SEAL}&  28.7&  27.3&  310&   5,130&  15,000\\\\\n         \\textbf{NBFNet}&  129&  115&  1,050&  /&  52,000\\\\\n         \\textbf{Neo-GNN}&  2.6&  1.4&  19.5&  101& 172\\\\\n         \\textbf{BUDDY}& 0.1& 0.1& 0.8& 10.5&17.6\\\\\n    \\midrule\n         \\textbf{HL-GNN}& 0.06&  \\underline{0.05}& \\underline{0.5}& 6.7& 16.2\\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:parameters": "\\begin{table}[tbp]\n    \\caption{Number of parameters for HL-GNN compared to other GNN methods. The least and second least number of parameters are marked with \\textbf{bold} and \\underline{underline}, respectively.}\n    \\label{tab:parameters}\n    \\centering\n    \\small\n    \\setlength{\\tabcolsep}{7pt}\n    \\begin{tabular}{lccccc}\n    \\toprule\n         & \\texttt{Cora} & \\texttt{Citeseer} & \\texttt{Pubmed}& \\texttt{collab}  &\\texttt{ddi}\\\\\n    \\midrule\n         \\textbf{GCN}&  \\underline{565k}&  \\underline{1.15M}&  \\underline{326k}&  \\underline{231k}& \\underline{1.36M}\\\\\n         \\textbf{GAT}&  566k&  \\underline{1.15M}&  327k&  394k& 1.55M\\\\\n         \\textbf{SEAL}&  2.30M&  3.46M&  1.82M&  1.63M& 6.19M\\\\\n         \\textbf{NBFNet}&  3.71M&  5.02M&  3.03M&  OOM& 11.04M\\\\\n         \\textbf{Neo-GNN}&  631k&  1.21M&  392k&  297k& \\underline{1.36M}\\\\\n         \\textbf{BUDDY}& 2.52M& 4.85M& 1.57M& 1.19M&2.71M\\\\\n    \\midrule\n         \\textbf{HL-GNN}& \\textbf{433k}&  \\textbf{1.01M}&  \\textbf{194k}& \\textbf{99k}& \\textbf{1.22M}\\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:time_mlp": "\\begin{table}[tbp]\n    \\caption{Total training time (in seconds) for training from scratch compared to training only the MLP predictor using the generalized heuristics.}\n    \\label{tab:time_mlp}\n    \\centering\n    \\small\n    \\setlength{\\tabcolsep}{6pt}\n    \\begin{tabular}{lccccc}\n    \\toprule\n          & \\texttt{Cora} & \\texttt{Citeseer} & \\texttt{Pubmed}& \\texttt{collab}  &\\texttt{ddi}\\\\\n    \\midrule\n         \\textbf{From Scratch}&  6.3&  5.6&  150&  5,360& 8,100\\\\\n         \\textbf{Predictor Only}&  2.8&  2.1&  0.8&  823& 572\\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:dataset_statistics": "\\begin{table*}[tbp]\n    \\caption{Statistics of the Planetoid, Amazon, and OGB datasets used in the experiments.}\n    \\label{tab:dataset_statistics}\n    \\centering\n    \\begin{tabular}{lccccccccc}\n    \\toprule &  \\texttt{Cora} & \\texttt{Citeseer} & \\texttt{Pubmed}& \\texttt{Photo}&\\texttt{Computers} & \\texttt{collab}  &\\texttt{ddi} & \\texttt{ppa}&\\texttt{citation2}\\\\\n    \\midrule \\textbf{\\# Nodes} & 2,708 & 3,327 & 18,717 &   7,650&13,752&235,868 & 4,267  & 576,289&2,927,963\\\\\n    \\textbf{\\# Edges} & 5,278 & 4,676 & 44,327 &   238,162&491,722&1,285,465& 1,334,889 & 30,326,273&30,561,187\\\\\n    \\textbf{\\# Features}& 1,433& 3,703& 500& 745& 767& 128&/ & 58&128\\\\\n    \\textbf{Dataset Split}& random & random & random &   random&random&time& time& throughput&protein\\\\\n    \\textbf{Average Degree}& 3.90& 2.81& 4.73&   62.26&71.51&5.45 & 312.84  & 52.62&10.44\\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table*}",
            "tab:hyperparameter": "\\begin{table*}[tbp]\n    \\caption{The detailed hyperparameter configurations used in the experiments.}\n    \\label{tab:hyperparameter}\n    \\centering\n    \\begin{tabular}{llccccccccc}\n    \\toprule  &Hyperparameter & \\texttt{Cora} & \\texttt{CiteSeer} & \\texttt{PubMed} & \\texttt{Photo} & \\texttt{Computers} & \\texttt{collab} & \\texttt{ddi}  & \\texttt{ppa}&\\texttt{citation2}\\\\\n    \\midrule\n     \\multirow{5}{*}{\\textbf{GNN}} &GNN \\# layers& 20 & 20 & 20 & 20 & 20 & 15 & 15  & 15  &15  \\\\\n      & GNN hidden dim.& 1,433 & 3,703 & 500 & 745 & 767 & 256 & 512  & 512  &256\\\\\n      & Node emb. dim. & - & - & - & - & - & 256 & 512  & 256&64\\\\\n      & $\\beta$ initialization & RWR & RWR & KI & RWR & RWR & KI & KI  & KI  &KI  \\\\\n      & Init. parameter & $\\alpha=0.2$ & $\\alpha=0.2$ & $\\gamma=0.2$ & $\\alpha=0.2$ & $\\alpha=0.2$ & $\\gamma=0.5$ & $\\gamma=0.5$  & $\\gamma=0.5$  &$\\gamma=0.6$\\\\\n      \\midrule\n      \\multirow{2}{*}{\\textbf{MLP}} &MLP \\# layers& 3 & 2 & 3 & 3 & 3 & 2 & 2  & 2  &2  \\\\\n      & MLP hidden dim.& 8,192 & 8,192 & 512 & 512 & 512 & 256 & 512  & 512  &256\\\\\n      \\midrule\n      \\multirow{5}{*}{\\textbf{Learning}}&Optimizer & Adam & Adam & Adam & Adam & Adam & Adam & Adam & Adam &Adam \\\\\n      & Loss function& BCE& BCE& BCE& BCE& BCE& AUC& AUC& AUC&AUC\\\\\n      & Learning rate& 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001  & 0.001  &0.001  \\\\\n      &\\# Epochs & 100 & 100 & 300 & 200 & 200 & 800 & 500  & 500  &100\\\\\n      &Dropout rate & 0.5 & 0.5 & 0.6 & 0.6 & 0.6 & 0.3 & 0.3  & 0.5&0.3  \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table*}",
            "tab:appen_trans": "\\begin{table*}[tbp]\n    \\caption{Ablation study on the introduction of transformation and activation functions.}\n    \\label{tab:appen_trans}\n    \\centering\n    \\begin{tabular}{lccccccc}\n    \\toprule\n        & \\texttt{Cora} & \\texttt{Citeseer} & \\texttt{Pubmed}& \\texttt{Photo}&\\texttt{Computers} & \\texttt{collab}  &\\texttt{ddi}\\\\\n       & Hits@100 & Hits@100 & Hits@100 & AUC &AUC & Hits@50 &Hits@20  \\\\\n    \\midrule\n     \\textbf{HL-GNN (w/ ReLU act.)}& 92.89\\footnotesize{$\\pm$1.89}& 93.15\\footnotesize{$\\pm$2.21}& 87.34\\footnotesize{$\\pm$1.27}& 98.23\\footnotesize{$\\pm$0.23}&97.79\\footnotesize{$\\pm$0.17}& 66.29\\footnotesize{$\\pm$0.71}&78.15\\footnotesize{$\\pm$2.69}\\\\\n      \\textbf{HL-GNN (w/ tran.)}& 87.35\\footnotesize{$\\pm$2.05}& 88.91\\footnotesize{$\\pm$2.45}& 83.39\\footnotesize{$\\pm$1.19}& 95.97\\footnotesize{$\\pm$0.18}&95.39\\footnotesize{$\\pm$0.41}& 60.47\\footnotesize{$\\pm$0.91}&73.04\\footnotesize{$\\pm$3.17}\\\\\n      \\textbf{HL-GNN (w/ ReLU act. \\& tran.)}& 86.23\\footnotesize{$\\pm$1.87}& 86.74\\footnotesize{$\\pm$2.36}& 82.15\\footnotesize{$\\pm$1.69}& 95.45\\footnotesize{$\\pm$0.31}&94.75\\footnotesize{$\\pm$0.35}& 59.84\\footnotesize{$\\pm$0.88}&71.53\\footnotesize{$\\pm$4.46}\\\\\n      \\midrule\n      \\textbf{HL-GNN} & \\textbf{94.22\\footnotesize{$\\pm$1.64}} & \\textbf{94.31\\footnotesize{$\\pm$1.51}} & \\textbf{88.15\\footnotesize{$\\pm$0.38}} & \\textbf{99.11\\footnotesize{$\\pm$0.07}} & \\textbf{98.82\\footnotesize{$\\pm$0.21}}  & \\textbf{68.11\\footnotesize{$\\pm$0.54}} &\\textbf{80.27\\footnotesize{$\\pm$3.98}} \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table*}",
            "tab:performance_mlp": "\\begin{table*}[tbp]\n    \\caption{Performance comparison between training from scratch and training only the MLP predictor using the generalized heuristics. The format is average score $\\pm$ standard deviation. }\n    \\label{tab:performance_mlp}\n    \\centering\n    \\begin{tabular}{lccccccc}\n    \\toprule\n          & \\texttt{Cora} & \\texttt{Citeseer} & \\texttt{Pubmed}& \\texttt{Photo}&\\texttt{Computers} & \\texttt{collab}  &\\texttt{ddi}\\\\\n          & Hits@100 & Hits@100 & Hits@100 & AUC &AUC & Hits@50 &Hits@20  \\\\\n    \\midrule\n         \\textbf{From Scratch}&  94.22\\footnotesize{$\\pm$1.64}&  94.31\\footnotesize{$\\pm$1.51}&  88.15\\footnotesize{$\\pm$0.38}&  99.11\\footnotesize{$\\pm$0.07} & 98.82\\footnotesize{$\\pm$0.21}  & 68.11\\footnotesize{$\\pm$0.54} &80.27\\footnotesize{$\\pm$3.98} \\\\\n         \\textbf{Predictor Only}&  92.34\\footnotesize{$\\pm$2.98}&  92.59\\footnotesize{$\\pm$2.60}&  85.26\\footnotesize{$\\pm$1.84}&  97.47\\footnotesize{$\\pm$0.79}& 96.81\\footnotesize{$\\pm$1.50}& 62.81\\footnotesize{$\\pm$2.62}&71.95\\footnotesize{$\\pm$5.68}\\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table*}"
        },
        "figures": {
            "fig:HL-GNN": "\\begin{figure}[tbp]\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]{figs/HLGNN.pdf}\n    \\caption{Illustration of the proposed Heuristic Learning Graph Neural Network (HL-GNN). Every rounded rectangle symbolizes a left multiplication operation.}\n    \\label{fig:HL-GNN}\n\\end{figure}",
            "fig:output": "\\begin{figure}[tbp]\n    \\centering\n    \\begin{subfigure}{0.49\\linewidth}\n        \\includegraphics[width=\\textwidth]{figs/L_cora.pdf}\n        \\caption{\\texttt{Cora}.}\n    \\end{subfigure}\n    % \\begin{subfigure}{0.49\\linewidth}\n    %     \\includegraphics[width=\\textwidth]{figs/L_citeseer.pdf}\n    %     \\caption{\\texttt{Citeseer}.}\n    % \\end{subfigure}\n    % \\begin{subfigure}{0.49\\linewidth}\n    %     \\includegraphics[width=\\textwidth]{figs/L_pubmed.pdf}\n    %     \\caption{\\texttt{Pubmed}.}\n    % \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\includegraphics[width=\\textwidth]{figs/L_collab.pdf}\n        \\caption{\\texttt{ogbl-collab}.}\n    \\end{subfigure}\n    \\caption{Ablation study on information ranges. We compare HL-GNN with two GNN variants, focusing on either local or global topological information, with different GNN depths.}\n    \\label{fig:output}\n\\end{figure}",
            "fig:deep": "\\begin{figure}[tbp]\n    \\centering\n    \\begin{subfigure}{0.49\\linewidth}\n        \\includegraphics[width=\\textwidth]{figs/L1.pdf}\n        \\caption{Planetoid datasets.}\n    \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\includegraphics[width=\\textwidth]{figs/L2.pdf}\n        \\caption{OGB datasets.}\n    \\end{subfigure}\n    \\caption{Test performance on the Planetoid and OGB datasets with different GNN depths.}\n    \\label{fig:deep}\n\\end{figure}",
            "fig:weights": "\\begin{figure}[tbp]\n    \\centering\n    \\vspace{-5px}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figs/optimal_cora.pdf}\n        \\caption{\\texttt{Cora}.}\n    \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figs/optimal_citeseer.pdf}\n        \\caption{\\texttt{Citeseer}.}\n    \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figs/optimal_collab.pdf}\n        \\caption{\\texttt{ogbl-collab}.}\n    \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figs/optimal_ddi.pdf}\n        \\caption{\\texttt{ogbl-ddi}.}\n    \\end{subfigure}\n    \\caption{Learned weights $\\beta^{(l)}$ with $L=20$ for the \\texttt{Cora} and \\texttt{Citeseer} datasets, and $L=15$ for the \\texttt{ogbl-collab} and \\texttt{ogbl-ddi} datasets.}\n    \\label{fig:weights}\n\\end{figure}",
            "fig:case_study": "\\begin{figure}[tbp]\n    \\centering\n    \\begin{subfigure}{0.49\\linewidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figs/triangle.pdf}\n        \\caption{Triangular network.}\n    \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figs/hexagon.pdf}\n        \\caption{Hexagonal network.}\n    \\end{subfigure}\n    \\caption{Learned weights $\\beta^{(l)}$ with $L=20$ for the synthetic triangular and hexagonal networks.}\n    \\label{fig:case_study}\n\\end{figure}",
            "fig:initial": "\\begin{figure}[tbp]\n    \\centering\n    \\begin{subfigure}{0.49\\linewidth}\n        \\includegraphics[width=\\textwidth]{figs/init_cora.pdf}\n        \\caption{\\texttt{Cora}.}\n    \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n    \\includegraphics[width=\\textwidth]{figs/init_citeseer.pdf}\n    \\caption{\\texttt{Citeseer}.}\n    \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\includegraphics[width=\\textwidth]{figs/init_pubmed.pdf}\n        \\caption{\\texttt{Pubmed}.}\n    \\end{subfigure}\n    \\begin{subfigure}{0.49\\linewidth}\n        \\includegraphics[width=\\textwidth]{figs/init_collab.pdf}\n        \\caption{\\texttt{ogbl-collab}.}\n    \\end{subfigure}\n    \\caption{Ablation study on different initialization strategies across various datasets.}\n    \\label{fig:initial}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{eq:formulation}\n\\boldsymbol{H} \n= \\sum_{l=0}^{L} \\left(\\beta^{(l)} \\prod_{m=0}^l \\mathbb{A}^{(m)} \\right),\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{eq:HLGNN}\n\\boldsymbol{Z}^{(0)}=\\boldsymbol{X},\n\\quad \n\\boldsymbol{Z}^{(l)}=\\mA^{(l)} \\boldsymbol{Z}^{(l-1)},  \n\\quad \n\\boldsymbol{Z} = \\sum_{l=0}^L \\beta^{(l)} \\boldsymbol{Z}^{(l)},\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:relaxation}\n\\mA^{(l)}= \\alpha_1^{(l)} \\Ars+\\alpha_2^{(l)} \\Acs+\\alpha_3^{(l)} \\Asym, \\quad \\text{for} \\quad 1 \\leq l \\leq L,\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\mathcal{L} = \\min_{\\alpha, \\beta, \\theta} \n\\sum_{ (i, j) \\in \\mathcal{E} } \\sum_{ (i, k) \\in \\mathcal{E}^{-} } \n\\gamma_{i j} \\left(\\max(0, \\gamma_{i j} - s_{ij} + s_{ik})\\right)^2.\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\begin{aligned}\n\\boldsymbol{H}_{\\mathrm{Cora}}&=0.1795\\boldsymbol{I}+ 0.1894\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+0.1484\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n+0.1213\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n&+0.0963\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4 +0.0768\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5+0.0606\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6\n+0.0477\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n&+0.0371\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8+0.0285\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}",
            "eq:6": "\\begin{equation}\n    \\begin{aligned}\n    \\boldsymbol{H}_{\\mathrm{Citeseer}}&=0.1993\\boldsymbol{I}+ 0.1759\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+0.1380\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n    +0.1111\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n    &+0.0878\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4+0.0694\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5+ 0.0544\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6+0.0422\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n    &+0.0324\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8+0.0244\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}",
            "eq:7": "\\begin{equation}\n    \\begin{aligned}\n\\boldsymbol{H}_{\\mathrm{Pubmed}}&=0.7386\\boldsymbol{I}+ 0.5335\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+0.3126\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n+0.2787\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n&+0.2429\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4+0.2252\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5+ 0.2039\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6+0.1884\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n&+0.1722\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8+0.1592\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}",
            "eq:8": "\\begin{equation}\n    \\begin{aligned}\n    \\boldsymbol{H}_{\\mathrm{collab}}&=0.1599\\boldsymbol{I}+ 0.1004\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+0.4826\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n    +0.2545\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n    &+0.2328\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4+0.1663\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5+ 0.1210\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6+0.0721\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n    &+0.0252\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8-0.0224\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}",
            "eq:9": "\\begin{equation}\n    \\begin{aligned}\n    \\boldsymbol{H}_{\\mathrm{ddi}}&=0.1532\\boldsymbol{I}+ 0.2840\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}+1.5298\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^2\n    +0.1154\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^3\\\\\n    &+0.1123\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^4-0.0507\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^5- 0.0739\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^6-0.1022\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^7\\\\\n    &-0.1102\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^8-0.1164\\tilde{\\boldsymbol{A}}_{\\mathrm{sym}}^9+\\cdots.\n    \\end{aligned}\n    \\end{equation}"
        },
        "git_link": "https://github.com/LARS-research/HL-GNN"
    }
}