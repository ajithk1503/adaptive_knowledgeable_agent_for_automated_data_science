{
    "meta_info": {
        "title": "ERNet: Unsupervised Collective Extraction and Registration in  Neuroimaging Data",
        "abstract": "Brain extraction and registration are important preprocessing steps in\nneuroimaging data analysis, where the goal is to extract the brain regions from\nMRI scans (i.e., extraction step) and align them with a target brain image\n(i.e., registration step). Conventional research mainly focuses on developing\nmethods for the extraction and registration tasks separately under supervised\nsettings. The performance of these methods highly depends on the amount of\ntraining samples and visual inspections performed by experts for error\ncorrection. However, in many medical studies, collecting voxel-level labels and\nconducting manual quality control in high-dimensional neuroimages (e.g., 3D\nMRI) are very expensive and time-consuming. Moreover, brain extraction and\nregistration are highly related tasks in neuroimaging data and should be solved\ncollectively. In this paper, we study the problem of unsupervised collective\nextraction and registration in neuroimaging data. We propose a unified\nend-to-end framework, called ERNet (Extraction-Registration Network), to\njointly optimize the extraction and registration tasks, allowing feedback\nbetween them. Specifically, we use a pair of multi-stage extraction and\nregistration modules to learn the extraction mask and transformation, where the\nextraction network improves the extraction accuracy incrementally and the\nregistration network successively warps the extracted image until it is\nwell-aligned with the target image. Experiment results on real-world datasets\nshow that our proposed method can effectively improve the performance on\nextraction and registration tasks in neuroimaging data. Our code and data can\nbe found at https://github.com/ERNetERNet/ERNet",
        "author": "Yao Su, Zhentian Qian, Lifang He, Xiangnan Kong",
        "link": "http://arxiv.org/abs/2212.03306v1",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "Published as a research track paper at KDD 2022. Code:  https://github.com/ERNetERNet/ERNet"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n% Brain extraction and registration are the two preliminary but essential steps in many neuroimaging applications, ranging from anatomical studies, functional analysis, radiotherapy, surgical assistance, and some other computer-aided diagnosis.\n% Brain extraction, a.k.a skull stripping, is the process of removing the non-cerebral tissues such as the skull, dura, and scalp from the brain Magnetic Resonance Imaging (MRI). The variability of these non-brain tissues is negligible in conventional neuroimaging studies, and removing them can reduce the image noise and computational cost of neuroimaging analysis algorithms.\n% Consequently, the accuracy of brain extraction has a major impact on the quality of subsequent tasks, such as image registration, tumors identification, and matter segmentation. \n% Conducting manual skull stripping is considered an approach to obtaining high-quality extracted brain images, which is usually performed by radiologists who outline brain regions manually.\n% However, this approach is not only labor-intensive and time-consuming, but also error-prone.\n% To address these limitations, many automated brain extraction algorithms~\\cite{smith2002fast,cox1996afni,shattuck2002brainsuite,segonne2004hybrid,iglesias2011robust,eskildsen2012beast,lutkenhoff2014optimized} are proposed\n% in recent years.\n% And yet, without additional manual corrections, their precision is limited and can still introduce errors in downstream studies.\n% Recently, deep learning techniques have been introduced for brain extraction due to their superior performance but face many challenges.\n% For instance, obtaining the annotations for model training is often expensive as expertise, effort and time are needed to produce precise labels, especially for the high-dimensional medical images, \\eg 3D MRI.\n\n\\textbf{Background.} Brain extraction (\\emph{a.k.a.} skull stripping) and registration are preliminary but crucial steps in many neuroimaging studies.\n%Examples of these studies include anatomical and functional analysis, radiotherapy, and surgical assistance.\nExamples of these studies include anatomical and functional analysis~\\cite{bai2017unsupervised,wang2017structural},\nmulti-modality fusion~\\cite{cai2018deep}, diagnostic assistance~\\cite{sun2009mining}.\nThe brain extraction step aims to remove the non-cerebral tissues such as the skull, dura, and scalp from the Magnetic Resonance Imaging (MRI) scan of a patient's head, while the registration step aims to align the extracted brain region with a template image of the standard brain. \nThe extraction and registration steps are essential preprocessing procedures in many neuroimaging studies. \nFor example, in anatomical and functional analysis, after removing and aligning the brain regions, the interference of non-neural tissues, imaging modalities, viewpoints can be eliminated, thus allowing precise quantification of changes in the shape, size, and position of anatomy and function. \nIn brain atrophy diagnosis, a patient’s brain region across different pathological stages needs to be first extracted from raw brain MRI scans and then aligned with a standard template to counteract the non-diagnostic changes. These essential processing steps help doctors accurately monitor the alteration of brain volume. \n%-------------------------------------------------------\n% \\begin{figure}\n%     \\centering\n%     \\begin{minipage}[l]{\\columnwidth}\n%         \\centering\n%   \t    \\includegraphics[width=\\textwidth]{fig/problem.pdf}\n%         \\vspace{-20pt}\n%     \\end{minipage}\n% \\caption{\n% \\textbf{Resource-efficient MTL. }\n% Three tasks are being learned on a multi-task dataset. The goal is to train a model for all three tasks that can be deployed on mobile devices with limited computational resources.\n% %\\tian{remove tag \"the author\"...therefore, you could remove the emoji?}\n% }\n% \\label{fig:problem}\n% \\vspace{-15pt}\n% \\end{figure}\n%-------------------------------------------------------\n\n\n\n\n\n% Image registration, as another essential step, aims to estimate the transformation between two or more images, and align them into one coordinate system.\n% Within the same coordinate system, the impact of imaging modalities, viewpoint, and time can be eliminated, thus allowing precise quantification of changes in the shape, size, and position of anatomy and function.\n% For example, a patient's brain MRI scans across different pathological stages can be fused together to counteract non-diagnostic movement, which helps doctors accurately monitor the growth of the tumor.\n% Conventional methods for image registration often have drawbacks such as intensive computation and local minimum problems due to unique characteristics of medical images, \\eg high dimensionality and heterogeneous pixel intensities.\n% Recent studies tackle these problems by leveraging deep learning techniques, which significantly improve the registration speed while ensuring competitive accuracy.\n% Nevertheless, these works typically rely on manual quality control to filter out inaccurate extraction results before performing subsequent registration.\n% Conducting such visual inspection is not only time-consuming and labor-intensive, but also suffers from intra- and inter-rater variability.\n\n\\textbf{State-of-the-Art.} In the literature, brain extraction and registration problems have been extensively studied~\\cite{kleesiek2016deep,lucena2019convolutional,sokooti2017nonrigid, dai2020dual}. Conventional approaches focus on developing methods for extraction~\\cite{kleesiek2016deep,lucena2019convolutional} and registration~\\cite{sokooti2017nonrigid, dai2020dual} separately under supervised settings, as shown in Figure~\\ref{fig:family 1}.\nHowever, in many medical studies, obtaining annotations of brain regions and transformations between images is often expensive as expertise, effort, and time are needed to produce precise labels, especially for high-dimensional neuroimages, \\eg 3D MRI.\nTo address this limitation, recent works~\\cite{smith2002fast,cox1996afni,shattuck2002brainsuite,segonne2004hybrid,balakrishnan2018unsupervised,zhao2019recursive} introduce a two-step approach for unsupervised extraction and registration by using automated brain extraction tools\\cite{smith2002fast,cox1996afni,shattuck2002brainsuite,segonne2004hybrid} and unsupervised registration models~\\cite{balakrishnan2018unsupervised,zhao2019recursive}, as shown in Figure~\\ref{fig:family 2}.\nNevertheless, these works typically rely on manual quality control to correct inaccurate extraction results before performing subsequent registration.\nConducting such visual inspection is not only time-consuming and labor-intensive, but also suffers from intra- and inter-rater variability, thus limiting the efficiency and performance of both tasks.\nMore importantly, most existing methods still conduct extraction and registration separetely and neglect the potential relationship between these two tasks. \n\n\\textbf{Problem Definition.} In this paper, we study the problem of unsupervised collective brain extraction and registration, as shown in Figure~\\ref{fig:intro}(a).\nOur goal is to capture the correlation of two tasks to boost their performance in an unsupervised setting. \nSpecifically, the brain region needs to be extracted from the source image accurately and well-aligned to the target image without any labeled data.\n\n\\textbf{Challenges.} Despite its value and significance, the problem of unsupervised collective  extraction and registration has not been studied before and is very challenging due to its unique characteristics listed below:\n% \\begin{itemize}\n%     \\item \\textit{Lack of labels in extraction:} Conventional learning-based extraction approaches are trained with a large number of training samples with ground truth labels. However, collecting voxel-level labels is very expensive and time-consuming in high-dimensional neuroimaging data.\n%     \\item \\textit{Lack of labels in registration:} The ground truth transformation between the source and target images is difficult to obtain. Though there are unsupervised registration methods~\\cite{balakrishnan2018unsupervised,zhao2019recursive} that optimize the transformation parameters by maximizing the similarity between images, these methods are only effective when the non-brain tissue of the source image is removed; otherwise, an erroneous transformation will be produced, rendering the registration invalid. Accordingly, obtaining the accurate transformation between the source and target images in an unsupervised setting remains largely unsolved.\n%     \\item \\textit{Dependencies between extraction and registration:}\n%     Conventional research mainly focuses on conducting extraction and registration tasks separately. However, these two tasks are highly correlated. Brain extraction has a decisive impact on the accuracy of the registration task, as shown in Figure~\\ref{fig:intro}(b). At the same time, the registration task can help the extraction task to capture cerebral/non-cerebral information from the source and target images. Thereby, a holistic solution is desired to manage the interdependence between the two tasks. \n% \\end{itemize}\n\n\\textbullet  \\  \n\\textit{Lack of labels for extraction:} Conventional learning-based extraction approaches are trained with a large number of training samples with ground truth labels. However, collecting voxel-level labels is very expensive and time-consuming in high-dimensional neuroimaging data.\n\n\\textbullet  \\  \n\\textit{Lack of labels for registration:} The ground truth transformation between source and target images is difficult to obtain. Though there are unsupervised registration methods~\\cite{balakrishnan2018unsupervised,zhao2019recursive} that optimize the transformation parameters by maximizing the similarity between images, these methods are only effective when the non-brain tissue of the source image is removed; otherwise, an erroneous transformation will be produced, rendering the registration invalid. Thus, obtaining the accurate transformation between source and target images in an unsupervised setting remains largely unsolved.\n\n\\textbullet  \\  \n\\textit{Dependencies between extraction and registration:} Conventional research mainly focuses on conducting extraction and registration tasks separately. However, these two tasks are highly correlated. Brain extraction has a decisive impact on the accuracy of the registration task, as shown in Figure~\\ref{fig:intro}(b). Meanwhile,  registration task can help extraction task to capture cerebral/non-cerebral information from the source and target images. Thereby, a holistic solution is desired to manage the interdependence between the two tasks.\n\n\n% %\\begin{itemize}\n%     %\\item \n% \\noindent \\textbullet \\  \n% \\textit{Lack of labels in extraction:} There are no brain region annotations available, i.e., there are no labels indicating the specific location of the brain tissue in the source image. Conventional learning-based extraction approaches are trained by supervised guidance of known ground truth labels. Without labels, a stand-alone solution can fail due to the lack of optimization objectives. Thus separating brain tissue from source image in an unsupervised setting is difficult.\n%     %item \n% \\\\\n% \\noindent \\textbullet \\ \n% \\textit{Lack of labels in registration:} The ground truth transformation between the source and target images is unknown, so it is impractical to directly learn the transformation parameters between images in a supervised manner. Though there are registration methods that optimize the transformation parameters by maximizing the similarity between images, these methods are only effective when the non-brain tissue of the source image is removed; otherwise, an erroneous transformation will be produced, rendering the registration invalid. Accordingly, obtaining the accurate transformation between the source and target images in an unsupervised setting remains largely unsolved.\n%     %\\item \\textit{Correlated tasks:} \n%     %\\item \n% \\\\\n% \\noindent \\textbullet \\ \n%     \\textit{Dependencies between extraction and registration:} \n%     Brain extraction and registration are two correlated tasks. Brain extraction has a decisive impact on the accuracy of the registration task. Either the removal of brain tissue (over stripping) or the inclusion of non-brain tissue (under stripping) cannot be rectified in the following steps, which causes the error to be propagated through succeeding registration. On the other hand, in such an unsupervised dilemma, the extraction task can only capture cerebral/non-cerebral information from the source image that has been well-aligned to the target image. An inaccurate registration result can affect the extraction task distinguishing the brain and non-brain tissue. Thereby, a holistic solution is desired to manage the interdependence between the two tasks.  \n% %\\end{itemize}\n%---------------------\n% Figure 2\n% \\begin{figure}\n%     \\centering\n%     \\begin{minipage}[l]{\\columnwidth}\n%         \\centering\n%   \t    \\includegraphics[width=\\textwidth]{fig/family.pdf}\n%         \\vspace{-20pt}\n%     \\end{minipage}\n% \\caption{\\textbf{Comparison of different parameter-sharing methods for multi-task learning.} A) soft sharing \\cite{misra2016cross, gao2019nddr}; B) naive hard sharing~\\cite{caruana1997multitask, baxter1997bayesian};  C) filter-wise hard sharing (this paper). }\n% \\label{fig:family}\n% \\vspace{-15pt}\n% \\end{figure}\n\n\n% \\begin{figure}[!h]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{fig/family_v5.pdf}\n%   \\caption{}\n%   \\label{fig:2}\n% \\end{figure}\n\n\n\n\n%---------------------\n\n\\textbf{Proposed Method.} To tackle the above issues, we propose a unified end-to-end framework, called ERNet (Extraction-Registration Network) for unsupervised collective brain extraction and registration.\nFigure~\\ref{fig: family} illustrates the comparison between our method and other state-of-the-art approaches.\nSpecifically, ERNet contains a pair of multi-stage extraction and registration modules, where the multi-stage extraction module progressively removes the non-brain tissue from the source image to produce an extracted brain image, and the multi-stage registration module incrementally aligns the extracted image to the target image.\nThese two modules help each other to boost extraction and registration performance simultaneously. \nThe unalignable portion, \\ie non-brain tissue,  revealed in the registration module guides the refinement process in the extraction module.\nMeanwhile, the registration module benefits from accurate brain extraction generated in the extraction module. \nBy bringing these two modules end-to-end, we achieve a joint optimization with no labels assistance. \n\n%\\textbf{Contributions.} Our main contributions are summarized below:\n%\n%\\textbullet  \\  \n%To the best of our knowledge, this is the first work to study the problem of unsupervised collective extraction and registration in neuroimaging data.\n%\n%\\textbullet  \\  \n%We propose an end-to-end extraction and registration method that does not require any labeled brain masks and known transformations for training.\n%\n%\\textbullet  \\  \nWe design a new regularization term to smooth the predicted brain mask during the training, which improves the extraction accuracy to a certain extent.\n%\\textbullet  \\  \nExtensive experiments are performed on multiple public brain MRI datasets. The results indicate that our proposed method significantly outperforms state-of-the-art approaches in both extraction and registration accuracy.\n\n% \\begin{itemize}\n% % \\setlength\\itemsep{-0.5em}\n%     \\item To the best of our knowledge, this is the first work to study unsupervised collective extraction and registration, where we integrated these tasks into a single system.\n%     \\item We propose an end-to-end extraction and registration method that does not require any labeled brain masks and known transformations for training.\n%     \\item We design a new regularization term to smooth the predicted brain mask, which improves the extraction accuracy to a certain extent.\n%     \\item Extensive experiments are performed on multiple public brain MRI datasets. The results indicate that our proposed method significantly outperforms state-of-the-art approaches in both extraction and registration accuracy.\n% \\end{itemize}\n\n% With the emergence of deep learning, convolutional neural networks (CNNs) have recently been introduced for deformable image registration due to their superior capability \\cite{balakrishnan2018unsupervised,balakrishnan2019voxelmorph,de2017end,li2017non,zhao2019recursive, de2019deep}. In contrast to traditional methods, which utilize iteration scheme to optimize the deformation, these CNN-based methods formulate the deformation as a transformation estimation problem and implicitly solve it in an end-to-end manner. Benefiting from the differentiable warping operation introduced by spatial transformer networks (STN) \\cite{jaderberg2015spatial}, the gradient of the whole process can be successfully back-propagated.\n% More recently, multi-stage registration methods have been developed \\cite{zhao2019recursive, de2019deep} based on cascaded neural networks, which tends to improve the registration accuracy by multiple stages of transformation. However, these methods often leads to blurry registered images and loss of details as they repeatedly interpolate the image by only considering the previous generation output. \n%However, they ignored the potential image blur caused by repeated sampling. These multi-stage approaches recursively transform the image generated by the previous stage, but this process will cause severe loss of image sharpness due to repeated interpolation attempts. In fact, image registration in real-life medical scenarios do not only require a high level of accuracy, but also the preservation of image information integrity. One example is brain network discovery, which can be critically impacted by the spatial alignment accuracy and voxel signal distortion. Figure \\ref{fig:1} demonstrates the desired result of deformable image registration.\n\n% \\textbf{Problem Definition.} The purpose of this work is to address the unsupervised deformable image registration problem, which performs a non-linear transformation to warp source moving image to align with the target fixed image. Deformable image registration problem is considerably challenging. Traditional image registration based on rigid transformation and affine transformation can meet the requirement if there is only a simple mapping relationship between the images, such as translation, rotation, scaling, etc. However, the difference in the anatomical structures between instances is enormous and to address this challenge requires a deformable transformation to complete the medical image registration. What makes the problem even more challenging is that high-quality ground truth is difficult to obtain and highly depends on the experience of the operator. Therefore, these reasons mark the crucial importance of developing effective unsupervised learning methods for image registration.\n% In this paper, we study the problem of anti-blur deformable image registration, as shown in Figure~\\ref{fig:1}. \n% The goal is to align the source image with a target image using multiple stages of registration steps, while maintaining the sharpness of the warped/registered images in the process.\n%The purpose of this work is to address the anti-blur deformable image registration problem. The existing methods fail to align the source image to the target image with a high level of accuracy and image sharpness. \n%Deformable image registration problem is considerably challenging. Rigid transformation and affine transformation can meet the requirement if there is only a simple mapping relationship between the images, such as translation, rotation, scaling, etc. However, the difference in the anatomical structures between subjects is enormous and to address this challenge requires a non-linear transformation to complete the medical image registration. What makes the problem even more challenging is that high-quality ground truth is difficult to obtain and highly depends on the experience of the operator. Therefore, these reasons mark the crucial importance of developing effective unsupervised learning methods for deformable image registration.\n% Despite its value and significance, the anti-blur deformable image registration problem is very challenging due to its unique characteristics listed below:\n% \\begin{itemize}\n%     \\item \\textit{Transform Estimation:} Most of the current unsupervised deformable image registrations complete the image registration by predicting only one transformation. This limitation has resulted in unsatisfactory registration effects, especially for complex and large deformations. In addition, a simple on-time transformation may cause the model to fall into a local optimum, thereby invalidating the final image registration outcome. \n% \\end{itemize}\n% \\begin{itemize}\n%     \\item \\textit{Transformation Estimation:} Most conventional deformable image registration methods accomplish this task by directly predicting a transformation. However, this design is difficult to achieve satisfactory registration results, especially when there are complex and large deformations between source and target images. Moreover, when processing high-dimensional images (e.g., 3D MRI and CT), one-step transformation approaches require manipulation of larger imagined quantities, producing lower accuracy and have limited capabilities.\n%     % one-step transformation might place a considerable burden on the network, producing in low-accuracy of the registration results.\n%     \\item \\textit{Information Retention:} Recent state-of-the-art methods use multi-stage design, which improves the registration accuracy to a certain extent. However, repeated transformations on the warped image across multiple stages may result in the loss of image information. Specifically, the high-frequency components of the image will be lost in multiple resampling, leading to a blurred interpolated image. In addition, an inaccurate deformation estimation from the previous stage will result in artifacts on the warped image being propagated into the following stages, rendering an irreversible error.\n% \\end{itemize}\n\n% In order to tackle the above issues, we propose a multi-stage Anti-Blur Network (ABN) for deformable registration methods. Figure \\ref{fig: family} illustrates the comparison between our approach and the state-of-the-art methods to the deformable image registration problem. Specifically, inspired by Long Short-Term Memory (LSTM) \\cite{hochreiter1997long}, we introduce a pair of short-term registration and long-term memory networks to learn the nonlinear deformations at each stage, where the short-term registration network is used to capture the deformation between current warped and target images, and the long-term memory network combines all the previous deformations to allow for resampling of the original source image during subsequent stages, thus preserving image sharpness to a greater extent. Empirical studies on natural and medical image registration tasks demonstrated that ABN outperform both single-stage and multi-stage existing methods in terms of image registration accuracy and sharpness.\n% The main contributions are summarized as follows:\n% \\begin{itemize}\n% % \\setlength\\itemsep{-0.5em}\n%     \\item To the best of our knowledge, this is the first work to study multi-stage method for anti-blur deformable image registration, which allows nonlinear transformations to be performed directly on the source image in a multi-stage setting.\n%     \\item We identify a new criterion to evaluate the performance of image registration by measuring the sharpness of registered image from information completeness viewpoint.\n%     \\item Extensive experiments are conducted on the 2D face image registration task and 3D brain MRI registration task, and the results indicate that our proposed method significantly outperforms state-of-the-art alternatives in terms of both registration accuracy and image sharpness.\n% \\end{itemize}\n% %-----------------------------------------------\n% % Problem Definition Section\n%\\input{tab_notation_v2}\n"
            },
            "section 2": {
                "name": "PRELIMINARIES",
                "content": " \n% In this section, we first introduce some related concepts and notations, then define the problem formally. Suppose we are given an unlabeled dataset $\\mathcal{D} = \\{\\{\\mathbf{I}_\\text{s}^{(1)}, \\cdots, \\mathbf{I}_\\text{s}^{(z)} \\},\\mathbf{I}_\\text{t}\\}$ that consists of $z$ source images (raw brain MRI scans) and one target image (template brain). Each source image is denoted as $\\mathbf{I}_\\text{s}^{(i)} \\in \\mathbb{R}^{W \\times H \\times D}$ and the target image is denoted as $\\mathbf{I}_\\text{t} \\in \\mathbb{R}^{W \\times H \\times D}$, where $W$, $H$ and $D$ are the width, height and depth of a 3D image. For simplicity, here we assume that $\\mathbf{I}_\\text{s}^{(i)}$ and $\\mathbf{I}_\\text{t}$ have the same dimensions. The difference in dimensions can be unified by resizing the image.\n% Let $\\mathcal{M} = \\{\\mathbf{M}^{(1)}, \\cdots, \\mathbf{M}^{(z)}\\}$ denotes the brain mask label set, where each $\\mathbf{M}^{(i)} \\in \\{0,1\\}^{W \\times H \\times D}$ is a binary tensor of $W \\times H \\times D$ labels. Every label point $\\mathbf{M}^{(i)}(x,y,z)=1$ indicates its corresponding point in the source image $\\mathbf{I}_\\text{s}^{(i)}(x,y,z)$ is cerebral tissue, where $x,y,z$ is index for the brain mask and the source image. \n% Now, each extracted (masked) image $\\mathbf{I}_\\text{e}^{(i)} \\in \\mathbb{R}^{W \\times H \\times D}$ can be denoted as $\\mathbf{I}_\\text{e}^{(i)} = \\mathbf{I}_\\text{s}^{(i)} \\circ \\mathbf{M}^{(i)}$, where $ \\circ $ is the element-wise product operator. Let $\\mathcal{A} = \\{\\mathbf{a}^{(1)}, \\cdots, \\mathbf{a}^{(z)}\\}$ denotes the affine transformation set, where each $\\mathbf{a}^{(i)} \\in \\mathbb{R}^{12}$ is a vector indicating the coordinate mapping between the extracted image $\\mathbf{I}_\\text{e}^{(i)}$ and the target image $\\mathbf{I}_\\text{t}$. Finally, the warped (registered) image $\\mathbf{I}_\\text{w}^{(i)} \\in \\mathbb{R}^{W \\times H \\times D}$ can be constructed as $\\mathbf{I}_\\text{w}^{(i)} = \\mathcal{T}\\left(\\mathbf{I}_\\text{e}^{(i)},\\mathbf{a}^{(i)}\\right)$, where $\\mathcal{T}(\\cdot, \\cdot)$ is the affine transformation operator, then we have $\\mathbf{I}_\\text{w}^{(i)}(x,y,z) = \\mathbf{I}_\\text{e}^{(i)}(x^{},y,z)$\n\n% As introduced in Section~\\ref{sec:intro}\n\n% The goal of unsupervised collective brain extraction and registration problem is learning a function $f_{\\theta}: \\mathbb{R}^{W \\times H \\times D}\\times \\mathbb{R}^{W \\times H \\times D} \\mapsto (\\mathcal{M}, \\mathcal{A})$, while both $\\mathcal{M}$ and $\\mathcal{A}$ are not observed during the training.\nIn this section, we first introduce related concept and notations, then define the unsupervised collective brain extraction and registration problem formally.\n\n",
                "subsection 2.1": {
                    "name": "Notations and Definitions",
                    "content": "\n\n\\noindent \\textbf{Definition 1 (Source and target images).}\nSuppose we are given a training dataset $\\mathcal{D} = \\left\\{\\left(\\mathbf{S}_i,\\mathbf{T}_i\\right)\\right\\}_{i=1}^{Z}$ that consists of $Z$ pairs of training samples. Each pair contains a source image $\\mathbf{S}_{i} \\in \\mathbb{R}^{W \\times H \\times D}$ (\\eg the raw MRI scan of a patient's head) and a target $\\mathbf{T}_{i} \\in \\mathbb{R}^{W \\times H \\times D}$ (\\eg a standard template of the brain region).\nHere $W$, $H$ and $D$ denote the width, height and depth dimensions of the 3D images. \nFor simplicity, we assume that the source and target images are resized to the same dimension, \\ie $W \\times H \\times D$. \nGenerally, in $\\mathcal{D}$, the target images in different pairs can be different.\nFor example, in the cross-modality studies~\\cite{cai2018deep}, we need to align the functional MRI (\\ie source image) with the structural MRI (\\ie target image) for each patient in the study (\\ie an image pair in $\\mathcal{D}$), where different patients will have different structural MRI images.\n%\\kong{can we add a short example of the application , ,when we need to use different target images for each pair? similar to the example we give below. such as cross modality analysis, aligning function to structure image.}\n%\\yao{Done.}\nIn many neuroimaging studies, however, all pairs in $\\mathcal{D}$ can share a same target image, which is a special case of the dataset $\\mathcal{D}$. \nFor example, in brain network analysis, the functional MRI images (\\ie source images) of all patients need to be aligned with a same template image (\\ie target image), \\eg MNI 152~\\cite{sun2009mining}. \n%Focusing on neuroimaging applications, we consider the target image to be the same in this paper. \nFor simplicity, in the following discussion, we omit the subscript $i$ of $\\mathbf{S}_{i}$ and $\\mathbf{T}_{i}$. \n\n\\noindent \\textbf{Definition 2 (Brain extraction mask).}\nBrain extraction mask $\\mathbf{M} \\in \\{0,1\\}^{W \\times H \\times D}$ is a binary tensor of the same dimensions as the source image $\\mathbf{S}$. \n$1$ in $\\mathbf{M}$ corresponds to the cerebral tissues on $\\mathbf{S}$ at the same location and $0$ otherwise.\nThe extracted image $\\mathbf{E} = \\mathbf{S} \\circ \\mathbf{M}$ is generated by applying the $\\mathbf{M}$ on $\\mathbf{S}$ via a element-wise product operator $\\circ$.\n\n\\noindent \\textbf{Definition 3 (Affine transformation and warped image).}\nWithout loss of generality, here we assume that the transformation in the registration task is affine-based. \nHowever, this work can be easily extended to other types of registration, \\eg  nonlinear/deformable registration.\nThe affine transformation parameters $\\mathbf{a} \\in \\mathbb{R}^{12} $ is a vector used to parameterized an affine transformation matrix $\\mathbf{A} \\in \\mathbb{R}^{4 \\times 4} $. The warped image $\\mathbf{W} = \\mathcal{T}\\left(\\mathbf{E},\\mathbf{a}\\right)$ is generated by applying the affine transformation on the extracted image $\\mathbf{E}$, where $\\mathcal{T}(\\cdot, \\cdot)$ is the affine transformation operator. The following relationship holds for $\\mathbf{W}$ and $\\mathbf{E}$ on the voxel level: \n\\begin{equation}\n\\label{equ:voxel_value}\n\\mathbf{W}_{xyz} = \\mathbf{E}_{x'y'z'},\n\\end{equation}\nwhere the correspondences between coordinates $x,y,z$ and $x',y',z'$ are calculated based on the affine transformation matrix $\\mathbf{A}$:\n\\begin{equation}\n\\begin{bmatrix}\nx'\\\\\ny'\\\\\nz' \\\\\n1\n\\end{bmatrix} = \\mathbf{A}\\begin{bmatrix}\nx\\\\\ny\\\\\nz \\\\\n1\n\\end{bmatrix} = \\begin{bmatrix}\na_{1} & a_{2} & a_{3} & a_{4}\\\\\na_{5} & a_{6} & a_{7} & a_{8}\\\\\na_{9} & a_{10} & a_{11} & a_{12}\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx\\\\\ny\\\\\nz \\\\\n1\n\\end{bmatrix}.\n\\end{equation}\n"
                },
                "subsection 2.2": {
                    "name": "Problem Formulation",
                    "content": "\n% The goal of collective brain extraction and registration is to learn a function: $f_{\\theta}: \\mathbb{R}^{W \\times H \\times D}\\times \\mathbb{R}^{W \\times H \\times D} \\rightarrow (\\{0,1\\}^{W \\times H \\times D}, \\mathbb{R}^{12})$. \n% \\kong{the function $f$ is a little bit hard to understand, because it outputs two tensors instead of one. How about we decompose the function into two components, similar to the generator (G) and discriminator (D) in GAN model? \n% We can define an extraction function $f(\\cdot)$ and a registration function $g(\\cdot,\\cdot)$. The goal is to optimize the parameters ($\\theta$ and $\\phi$) of the two functions $f_{\\theta}(\\cdot)$ and $g_{\\phi}(\\cdot,\\cdot)$.\n% For example,\n% $f_{\\theta}: \\mathbb{R}^{W \\times H \\times D}\\rightarrow \\mathbb{R}^{W \\times H \\times D}$, which generates a soft version of the brain mask, each element of the output tensor is a value between 0 and 1. \n% $g_{\\phi}: \\mathbb{R}^{W \\times H \\times D}\\times \\mathbb{R}^{W \\times H \\times D} \\rightarrow \\mathbb{R}^{12}$.\n% Maybe we could draw a figure similar to the GAN figure to explain how the E and R functions are connected together. By applying extraction function, we get the estimation of the mask $\\hat{M} = f_{\\theta}(S) $, then by element-wise product, we get the extracted image $\\hat{M} * S --> E$.  \n% Then $g_{\\phi}(E,T) --> a$. \n% In the figure, we then show how we use $a$ and $E$ to get the warped image $W$. I think a simple figure (without multi-stage, without loss function) to explain the relationship between these concepts ($S, T, f_{\\theta}, g_{\\phi}, E, a, W$) would be very helpful. This is similar to the GAN paper's figure, in which they explain the relationship between z (random noise), a real image, a fake image, a generator, a discriminator, the binary output of  discriminator. }\n\nThe goal of collective brain extraction and registration is to jointly learn the extraction function $f_{\\theta}: \\mathbb{R}^{W \\times H \\times D} \\rightarrow \\mathbb{R}^{W \\times H \\times D}$ and the registration function $g_{\\phi}: \\mathbb{R}^{W \\times H \\times D}\\times \\mathbb{R}^{W \\times H \\times D} \\rightarrow \\mathbb{R}^{12}$, as shown in Figure~\\ref{fig:formulation}.\nSpecifically, the extraction function $f_{\\theta}(\\cdot)$ takes the source image $\\mathbf{S}$ as input to predicts a brain extraction mask $\\hat{\\mathbf{M}} = f_{\\theta}(\\mathbf{S})$. Then, the registration function $g_{\\phi}(\\cdot, \\cdot)$ takes the extracted brain image $\\hat{\\mathbf{E}} = \\hat{\\mathbf{M}} \\circ \\mathbf{S}$ and the target image $\\mathbf{T}$ to predict the affine transformation parameter $\\hat{\\mathbf{a}} = g_{\\phi}(\\hat{\\mathbf{E}},\\mathbf{T})$. Finally, the warped image is $\\hat{\\mathbf{W}} = \\mathcal{T}(\\hat{\\mathbf{E}},\\hat{\\mathbf{a}})$. The optimal parameter $\\theta^*$ and $\\phi^*$ can be found by solving the following optimization problem:\n\\begin{equation}\n\\begin{split}\n\\label{equ:goal_training}\n\\theta^{*},\\phi^{*} &=\\underset{\\theta,  \\phi}{\\arg \\min } \\hspace{-3pt} \\sum_{\\left(\\mathbf{S},  \\mathbf{T}\\right)\\in \\mathcal{D}}\\left[ \\mathcal{L} \\left( \\hat{\\mathbf{W}}, \\mathbf{T}  \\right)\\right] \\\\\n& = \\underset{\\theta, \\phi}{\\arg \\min } \\hspace{-3pt}  \\sum_{\\left(\\mathbf{S}, \\mathbf{T}\\right)\\in \\mathcal{D}}\\left[ \\mathcal{L} \\left( \\mathcal{T} \\left( f_{\\theta}(\\mathbf{S}) \\circ \\mathbf{S},\\hspace{5pt} g_{\\phi} \\left(  f_{\\theta}(\\mathbf{S}) \\circ \\mathbf{S}, \\mathbf{T}  \\right)\\right),   \\mathbf{T}    \\right)\\right],\n\\end{split}\n\\end{equation}\nwhere the image pair $(\\mathbf{S},\\mathbf{T})$ is sampled from the training dataset $\\mathcal{D}$, and $\\mathcal{L}(\\cdot, \\cdot)$ is image dissimilarity criteria, \\eg mean square error.\n\n%\\yao{This formula seems too complicated. I am not sure whether we should just write it as $\\theta^{*},\\phi^{*} =\\underset{\\theta, \\hspace{1pt} \\phi}{\\arg \\min } \\hspace{1pt} \\sum_{\\left(\\mathbf{S}, \\hspace{1pt} \\mathbf{T}\\right)\\in \\mathcal{D}}\\left[ \\mathcal{L} \\left( \\hat{\\mathbf{W}}, \\mathbf{T}  \\right)\\right]  $}\n\n\n\n\n\n\n\n\n\n\n\n\n% Specifically, this function takes a test pair of source image $\\mathbf{S}$ and target image $\\mathbf{T}$ and predict an tuple of extraction mask and affine transformation parameter $(\\mathbf{M}, \\mathbf{a}) = f_{\\theta}(\\mathbf{S},\\mathbf{T})$. \n% The predicted brain extraction mask $\\mathbf{M}$ is applied on the $\\mathbf{S}$ to remove the non-cerebral tissues and generate the extracted image $\\mathbf{E}$. We further apply the affine transformation parameterized by $\\mathbf{a}$ on $\\mathbf{E}$ to align extracted cerebral tissues with $\\mathbf{T}$. The optimal parameter $\\theta^*$ of function $f_{\\theta}(\\cdot, \\cdot)$ is found by solving the following optimization problem:\n% \\begin{equation}\n% \\label{equ:goal_training}\n% \\theta^{*} =  \\underset{\\theta}{\\arg \\min } \\hspace{1pt} \\sum_{\\left(\\mathbf{S}, \\hspace{1pt} \\mathbf{T}\\right)\\in \\mathcal{D}}\\left[\\mathcal{L}\\left(\\mathcal{T}\\left(\\mathbf{S}\\circ \\mathbf{M}, \\mathbf{a}\\right),\\hspace{1pt} \\mathbf{T}\\right)\\right],\n% \\end{equation}\n% where $(\\mathbf{M}, \\mathbf{a}) = f_{\\theta}(\\mathbf{S},\\mathbf{T})$, the image pair $(\\mathbf{S},\\mathbf{T})$ is sampled from the training dataset $\\mathcal{D}$, and $\\mathcal{L}(\\cdot, \\cdot)$ is image dissimilarity criteria, \\eg mean square error. \n\nTo the best of our knowledge, this work is the first endeavour to find an optimal solution to the problem of unsupervised collective brain image extraction and registration in an end-to-end neural network. Our approach excludes the necessity of labeling the brain extraction masks and transformation between images of the training dataset, as opposed to other supervised methods~\\cite{kleesiek2016deep,lucena2019convolutional,sokooti2017nonrigid, dai2020dual}.\n\n\n% %-----------------------------------------------\n% % Method Section\n\n\n\n\n\\definecolor{myorange}{RGB}{249, 203, 156}\n\\definecolor{myviolet}{RGB}{218, 175, 244}\n\n\n\n\n%An overview of ABN for multi-stage deformable image registration.In each stage $k$, the \\emph{Short-Term Registration Network} $f_{S}$ predicts the current deformation field $\\phi_{S}^{(k)}$ between the previous warped image $\\mathbf{I}_{w}^{(k-1)}$ and target image $\\mathbf{I}_{t}$; \\emph{Long-Term Memory Network} $f_{L}$ fuses current deformation field $\\phi_{S}^{(k)}$ and previous combined deformation field $\\phi_{L}^{(k-1)}$ to generate the updated combined deformation field $\\phi_{L}^{(k)}$; \\emph{Spatial Transformation Layer} (ST Layer) performs the nonlinear transformation on the source image $\\mathbf{I}_{s}$ using $\\phi_{L}^{(k)}$ to produce the warped image $\\mathbf{A}_{i}^{(1)}$. The final warped registered image is $\\mathbf{A}_{c}^{(1)} \\cdot \\big(\\mathbf{I}_{e}^{(j-1)} \\circ \\mathbf{M}^{(j)}\\big)$.\\section{Our Approach}\n\\label{sec:method}\n\\noindent\\textbf{Overview.} Figure~\\ref{fig:network} presents an overview of the proposed ERNet framework for the unsupervised collective brain extraction problem. Our method is a multi-stage deep neural network consisting of two main modules: 1) \\emph{Multi-Stage Extraction Module} takes the raw source image $\\mathbf{S}$ as input, and gradually produces the extracted brain image $\\mathbf{E}^{M}$ after $M$ stages of extraction; 2) \\emph{Multi-Stage Registration Module} takes the extracted brain image $\\mathbf{E}^{M}$ and the target image $\\mathbf{T}$ as inputs, and incrementally aligns $\\mathbf{E}^{M}$ with $\\mathbf{T}$ through $N$ stages of registration. The final output is the warped image $\\mathbf{W}^{N}$. The whole framework is trained using backpropagation in an end-to-end unsupervised fashion, allowing feedback and collaboration between modules.\nNext we introduce the details of each module and the training process.\n% All of these components are trained using backpropagation in an end-to-end manner under unsupervised setting. \n% where each stage learns only a small transformation based on the results of the previous stages. This approach helps ABN to handle complex and large deformations. \n\n% \\noindent\\textbf{The key insight}: To overcome the problem of sharpness losing, at every stage, ABN always performs non-linear warping on the original source image, instead of on the warped image generated by the previous stage. Therefore, only one interpolation is needed to yield a warped image.\n\n% \\noindent\\textbullet~\\emph{Local registration network} takes the previous warped image $\\mathbf{I}_{w}^{(k-1)}$ and the target image $\\mathbf{I}_{t}$ as input, and generates a local deformation field $\\phi_{L}^{(k)}$. \n\n% \\noindent\\textbullet~\\emph{Global registration network} takes the previous global deformation field $\\phi_{G}^{(k-1)}$ and the current local deformation field $\\phi_{L}^{(k)}$ as input, and generates an updated global deformation field $\\phi_{G}^{(k)}$. \n\n% \\noindent\\textbullet~\\emph{Spatial transform layer} performs the non-linear transformation defined by $\\phi_{G}^{(k)}$ on the source image $\\mathbf{I}_{s}$.\n\n% In the following subsections, we will introduce the details of each of the above components and the approach for training.\n\n%ABN consists of several core neural networks in each stage: (1) a Local Registration Network that predicts a local transformation represented by a deformation field between current input images, (2) a Global Registration Network that learns a global deformation field from the current local deformation field and previous global deformation field, and (3) a Spatial Transformation Layer that directly warps the moving source image by a global deformation field. Once the Spatial Transformation Layer successfully generates a warped registered image, the Local Registration Network takes the warped registered image and the fixed target images to predict a local deformation field, recursively.\n%The Local Registration Network, Global Registration Network and Spatial Transformation Layer are trained together as an unsupervised end-to-end multi-stage registration model since there is no ground truth transformation indicating which deformation field should be predicted. After the Spatial Transformation Layer warped the moving source image at final stage, ABN is trained by optimizing the similarity between the final warped registered image and the fixed target image. Figure \\ref{fig:unsupervised} illustrates the overview of unsupervised image registration.\n"
                },
                "subsection 2.3": {
                    "name": "Multi-Stage Extraction Module",
                    "content": "\n\\label{sec: Extration Net}\nThis module proposes to solve the extraction task in a multi-stage fashion to obtain high extraction accuracy. It contains $M$ extraction stages with each stage $j$ consisting of two main components: 1) \\emph{Extraction Network} takes the previous extracted brain image $\\mathbf{E}^{j-1}$ as input, and generates a current brain mask $\\mathbf{M}^{j}$; 2) \\emph{Overlay Layer} takes the previous extracted brain image $\\mathbf{E}^{j-1}$ and the current extraction mask $\\mathbf{M}^{j}$ as inputs, and generates an updated extracted image $\\mathbf{\nE}^{j}$. The output of this module is the extracted brain image $\\mathbf{E}^{M}$ at the final stage $j = M$.\n\n",
                    "subsubsection 2.3.1": {
                        "name": "e",
                        "content": "\n\\label{sec: fe}\nThe extraction network $f_{e}(\\cdot)$ serves to gradually remove the non-cerebral tissues in the source image $\\mathbf{S}$ so that only cerebral tissues would remain on the extracted image at the final stage. At each stage $j$, based on the extracted brain image $\\mathbf{E}^{j-1}$ from the previous stage, it would produce a current extraction mask $\\mathbf{M}^{j}$ to remove the non-cerebral tissues believed to be still on $\\mathbf{E}^{j-1}$. Specifically, we adopt 3D U-Net \\cite{ronneberger2015u} as the base network to learn $f_{e}(\\cdot)$, which is the state-of-the-art architecture widely used in image registration and semantic segmentation. The output of the U-Net would go through a Heaviside step function to obtain the binary mask $\\mathbf{M}^{j}$ when performing inference:\n\\begin{equation}\n H(x) = \\begin{cases}\n        1, & \\text{if } x > 0, \\\\\n        0, & \\text{otherwise}.\n    \\end{cases}\n\\end{equation}\nNote that the derivative of the Heaviside step function does not exist for $x =0$ and is a constant $0$ for $x\\neq 0$. For the gradient to successfully backpropagate, we use a Sigmoid function with a large slope parameter $\\gamma$ to approximate the Heaviside step function when performing training:\n\\begin{equation}\nS(x) = \\frac{1}{1 +e^{-\\gamma x}}\n\\label{eq:sigmoid}\n\\end{equation}\n\n\n$f_{e}(\\cdot)$ follows a shared-weight design, which means that $f_{e}(\\cdot)$ is repetitively applied across stages with the same parameters. It can be formalized as:\n\\begin{equation}\n\\mathbf{M}^{j}=f_{e}\\left(\\mathbf{E}^{j-1}\\right),\n\\end{equation}\nwhere $\\mathbf{M}^{j}$ is the outputted brain mask of the $j$-th stage for $j = [1, \\cdots, M]$ and $\\mathbf{E}^{0} = \\mathbf{S}$.\n\n"
                    },
                    "subsubsection 2.3.2": {
                        "name": "Overlay Layer: $OL$",
                        "content": "\nThe overlay layer would remove the non-cerebral tissues remaining in the image by applying the current brain mask $\\mathbf{M}^{j}$ to the previous extracted image $\\mathbf{E}^{j-1}$. The updated extracted image is:\n\\begin{equation*}\n    \\mathbf{E}^{j} = \\mathbf{E}^{j-1}\\circ \\mathbf{M}^{j}\n\\end{equation*}\nwhere $\\circ$ is the element-wise product operator.\n\n"
                    }
                },
                "subsection 2.4": {
                    "name": "Multi-Stage Registration Module",
                    "content": "\nSimilar to the extraction module discussed in Section \\ref{sec: Extration Net}, we implement a multi-stage solution to address the registration task. This module consists of $N$ cascaded stages with each stage $k$ containing three main components: 1) \\emph{Registration Network} takes the previous warped image $\\mathbf{W}^{k-1}$ and the target image $\\mathbf{T}$ as inputs, and generates the current affine transformation $\\mathbf{A}_\\text{i}^{k}$; 2) \\emph{Composition Layer} takes the previous combined affine transformation $\\mathbf{A}_\\text{c}^{k-1}$ and the current affine transformation $\\mathbf{A}_\\text{i}^{k}$ as input, and generates an updated combined affine transformation $\\mathbf{A}_\\text{c}^{k}$; and 3) \\emph{Spatial Transformation Layer} transforms the extracted brain image $\\mathbf{E}^{M}$ using $\\mathbf{A}_\\text{c}^{k}$ to produce the warped image $\\mathbf{W}^{k}$. \n%Note that in the $k$-th cascaded stage, the extracted brain image $\\mathbf{E}^{M}$ and the warped image $\\mathbf{W}^{k-1}$ are jointly utilized to minimize the sharpness loss during the successive affine transformation operations.  \nThe output of this module is the warpped brain image $\\mathbf{W}^{N}$ at the final stage.\n\n",
                    "subsubsection 2.4.1": {
                        "name": "r",
                        "content": "\nThe registration network $f_{r}(\\cdot, \\cdot)$ is designed to gradually transform the extracted brain image to maximize its similarity with the target image. At each stage $k$, it predicts a current affine transformation $\\mathbf{A}_\\text{i}^{k}$ relying only on the previous warped image $\\mathbf{W}^{k-1}$ and the target image $\\mathbf{T}$.\n% An important reason to want a short-term transformation is to be able to control the transformation process more locally and more accurately, avoiding the under-transformation or over-transformation of the previous warped image. \nFollowing a similar approach to the extraction network $f_e$ in Section \\ref{sec: fe}, we adopt a 3D CNN based encoder to learn $f_{r}(\\cdot, \\cdot)$ and a shared weight design to utilize $f_{r}(\\cdot, \\cdot)$ repetitively across stages with the same parameters. \n%  It is possible to estimate the long-term overall performance athlete or estimates in the short term for each stage of preparation or participation in competitions \n% In order to avoid under/over deformation of the source image to the extent which the source image may become identical to the target image \n% This structure is comprised of an encoder-decoder with skip connections to integrate hierarchical information between low-level and high-level features. \nIt can be formalized as:\n\\begin{equation}\n\\mathbf{A}_\\text{i}^{k}=f_{r}\\left(\\mathbf{W}^{k-1}, \\mathbf{T}\\right),\n\\end{equation}\nwhere $\\mathbf{A}_\\text{i}^{k}$ is the output affine transformation of the $k$-th stage for $k = [1, \\cdots, N]$ and $\\mathbf{W}^{0} = \\mathbf{E}^{M}$.\n\n"
                    },
                    "subsubsection 2.4.2": {
                        "name": "COMP",
                        "content": "\nIn each stage $k$, after obtaining the current affine transformation $\\mathbf{A}_\\text{i}^{k}$ from $f_{r}(\\cdot, \\cdot)$, we would combine all previous transformation:\n\\begin{equation}\n\\mathbf{A}_\\text{c}^{k}=\\mathbf{A}_\\text{i}^{k} \\cdot \\mathbf{A}_\\text{c}^{k-1},\n\\end{equation}\nwhere $\\cdot$ is matrix product. When $k=1$, the initial affine transformation $\\mathbf{A}_\\text{c}^{0}$ is set to be an identity matrix representing no displacement. This layer serves as a bridge between the registration network and the spatial transformation layer. \n% As such, it enables the utilization of both the extracted brain image $\\mathbf{I}_\\text{e}^{(m)}$ and the warped image $\\mathbf{I}_\\text{w}^{(k-1)}$ to preserve the image sharpness for better details.\nAs such, it enables the transformation to be directly applied to the final extracted brain image $\\mathbf{E}^{M}$ to avoid image sharpness loss caused by multiple interpolations.\n\n% This network structure allows it to exhibit dynamic constitutive behavior as it contains the interconnect between different stages of deformations. Similar to $f_{R}(\\cdot)$, we adopt an U-Net based CNN to learn $\\text{COMP}(\\cdot)$ with a weight sharing across each stage. \n\n% Other different network architectures can be chosen, if they prove to have better performance. \n\n"
                    },
                    "subsubsection 2.4.3": {
                        "name": "Spatial Transformation Layer",
                        "content": "\nAn important step towards image registration is to reconstruct the warped image $\\mathbf{W}^{k}$ from the extracted brain image $\\mathbf{E}^{M}$ by affine transformation operator. Based on the combined transformation $\\mathbf{A}_\\text{c}^{k}$, we introduce a spatial transformation layer to resample the voxels into a uniform grid on the extracted image to acquire the warped image through $\\mathbf{W}^{k} = \\mathcal{T}(\\mathbf{E}^{M}, \\mathbf{A}_\\text{c}^{k})$. According to the definition of \naffine transformation operator in Eq.~(\\ref{equ:voxel_value}), we have\n\\begin{equation}\n     \\mathbf{W}_{xyz}^{k} = \\mathbf{E}_{x'y'z'}^{M} \\hspace{5pt},\n     \\label{equ:voxel_value_k}\n\\end{equation}\nwhere $[x', y', z', 1]^\\top = \\mathbf{A}_\\text{c}^{k}[x, y, z, 1]^\\top $.\n\n% resampling technique\n% enable interpolation of linear operators\n\nTo ensure the success of gradient propagation in this process, we use a differentiable transformation based on trilinear interpolation introduced by \\cite{jaderberg2015spatial}. That is, \n\\begin{equation}\n\\begin{split}\n \\mathbf{W}_{xyz}^{k}= \\sum_{o=1}^{W} \\sum_{p=1}^{H}  \\sum_{q=1}^{D} \\mathbf{E}_{opq}^{M} &\\cdot  \n\\max(0,1-|x' -o|) \\\\ \n \\cdot  \\max (0,1-|y' - p|) & \\cdot \\max (0,1-|z' - q|).\n\\end{split}\n\\end{equation}\nNotice that Eq.~(\\ref{equ:voxel_value_k}) always performs transformation on the extracted image $\\mathbf{E}^{M}$ instead of the previous warped images. Therefore, only one interpolation is required to produce the final warped brain image $\\mathbf{W}^{N}$, which better preserves the sharpness of $\\mathbf{W}^{N}$.\n\n% % and Figure \\ref{fig:mapping} demonstrates an example. \n% Since the process of warping is differentiable, our method, ABN, can backpropagate errors during optimization.\n\n\n% Similar to Eq.~\\eqref{equ:voxel_value} and \\eqref{equ:delta}, the voxel value at position $(x,y,z)$ of image $\\mathbf{I}_{w}^{(k)}$ can be calculate as:\n% \\begin{equation}\n% \\mathbf{I}_{w}^{(k)}(x,y,z) = \\mathbf{I}_{s}(x+ \\Delta x,y+\\Delta y,z+\\Delta z)\n% \\end{equation}\n% % \\begin{equation}\n% % (x',y',z') = \\phi(x,y,z)\n% % \\end{equation}\n% where\n% \\begin{equation}\n% \\Delta x = \\phi_{G}^{(k)}(x,y,z,1),\\quad \\Delta y = \\phi_{G}^{(k)}(x,y,z, 2),\\quad \\Delta z = \\phi_{G}^{(k)}(x,y,z, 3).\n% \\end{equation}\n% Since we focus on 3D image registration problem in this work, a 3D transformation with trilinear interpolation is adopted:\n% \\begin{equation}\n% \\begin{split}\n% &\\mathbf{I}_{s}(x+ \\Delta x,y+\\Delta y,z+\\Delta z) \\\\\n% =&\\sum_{n}^{W} \\sum_{m}^{H} \\sum_{l}^{D} \\mathbf{I}_{s}(n,m,l)\n% \\cdot\\max \\left(0,1-\\left|x+ \\Delta x-n\\right|\\right) \\\\ \n% \\cdot& \\max \\left(0,1-\\left|y+ \\Delta y-m\\right|\\right)\n% \\cdot\\max \\left(0,1-\\left|z+ \\Delta z-l\\right|\\right).\n% \\end{split}\n% \\end{equation}\n% % \\begin{equation}\n% % \\begin{split}\n% % \\mathbf{I}_{s}(x',y',z')=\\sum_{n}^{W} \\sum_{m}^{H} \\sum_{l}^{D} \\mathbf{I}_{s}(n,m,l)\n% % \\cdot \\max \\left(0,1-\\left|x'-n\\right|\\right) \\\\\n% % \\cdot \\max \\left(0,1-\\left|y'-m\\right|\\right)\n% % \\cdot \\max \\left(0,1-\\left|z'-l\\right|\\right)\n% % \\end{split}\n% % \\end{equation}\n% Similarly, bilinear interpolation is used in the Spatial Transformation Layer for 2D image registration task. \n% % and Figure \\ref{fig:mapping} demonstrates an example. \n% Since the process of warping is differentiable, our method, ABN, can backpropagate errors during optimization.\n\n\n\n\n\n\n\n% the Spatial Transformation Layer and interpolation function can be formulated as:\n% % \\begin{equation}\n% % I_{w}=\\mathcal{T}(\\mathbf{I}_{s}, \\phi_{G})\n% % \\end{equation}\n% % where\n% % \\begin{equation}\n% % \\begin{split}\n% % &\\mathcal{T}(\\mathbf{I}_{s}, \\phi_{G}) \\\\\n% % &=\\sum_{q \\in \\mathcal{Z}(p+\\phi_{G}(p))} \\mathbf{I}_{s}(q) \\prod_{d \\in\\{x, y, z\\}}\\left(1-\\left|p_{d}+\\phi_{G}(p_{d})-q_{d}\\right|\\right)\n% % \\end{split}\n% % \\end{equation}\n% \\begin{equation}\n% I_{w}^{(k)}=\\mathcal{T}(\\mathbf{I}_{s}, \\phi_{G}^{(k)})\n% \\end{equation}\n% and\n% \\begin{equation}\n% \\begin{array}{l}\n% \\mathcal{T}\\left(\\mathbf{I}_{s}, \\phi_{G}^{(k)}\\right) \\\\\n% =\\sum_{q \\in \\mathcal{Z}\\left(p+\\phi_{G}^{(k)}(p)\\right)} \\mathbf{I}_{s}(q) \\prod_{d \\in\\{x, y, z\\}}\\left(1-\\left|p_{d}+\\phi_{G}^{(k)}\\left(p_{d}\\right)-q_{d}\\right|\\right)\n% \\end{array}\n% \\end{equation}\n% Where $\\mathcal{T}(\\cdot)$ is the Spatial Transformation Layer which warps the moving source image $\\mathbf{I}_{s}$ by the predicted global deformation field $\\phi_{G}^{(k)}$, and generates the warped registered image $I_{w}^{(k)}$. The set $\\mathcal{Z}\\left(p+\\phi_{G}^{(k)}(p)\\right)$ defines $8$ neighbors around the voxel $\\left(p+\\phi_{G}^{(k)}(p)\\right)$, where $p$ is index of grid points. $d$ denotes three direction in 3D space. Similarly, bilinear interpolation is used in the Spatial transformation Layer for 2D image registration task. Since the process of warping is differentiable, our method, ABN, can backpropagate errors during optimization.\n\n% \\subsection{ABN-L Architecture}\n% \\label{sec:ABN-L}\n% Alternatively, we proposed a variant of ABN named as Anti-Blur Networks-Light (ABN-L). Both ABN and ABN-L are designed to preserve the sharpness of the image during the registration, but there is a trade-off between registration accuracy and computational costs. Different from ABN, ABN-L only uses one network $f_{G}(\\cdot)$ to handle the registration and combination of deformations simultaneously. The network progressively learns the deformation between $\\mathbf{I}_{s}$ and $\\mathbf{I}_{t}$ at each stage $k$ by fusing information from the previous warped image $\\mathbf{I}_{w}^{(k-1)}$, the target image $\\mathbf{I}_{t}$ and the previous deformation field $\\phi_{G}^{(k-1)}$, which can be written as: \n% \\begin{equation}\n% \\phi_{G}^{(k)}=f_{G}\\left(\\mathbf{I}_{w}^{(k-1)}, \\mathbf{I}_{t}, \\phi_{G}^{(k-1)}\\right).\n% \\end{equation}\n% Same to ABN, $\\phi_{G}^{(0)}$ and $\\mathbf{I}_{t}$ are used to initialize the network at the first stage $k=1$. Subsequently, a Spatial Transformation Layer is used to directly warp the source image $\\mathbf{I}_{s}$ by the predicted $\\phi_{G}^{(k)}$ to generate the warped image $\\mathbf{I}_{w}^{(k)}$.\n\n% \\input{fig_network1}\n"
                    }
                },
                "subsection 2.5": {
                    "name": "Unsupervised End-to-End Training",
                    "content": "\n\\label{section:end-to-end training}\n\nWe train our ERNet model in an unsupervised setting by minimizing the following objective function\n\\begin{equation}\n\\label{eq:loss}\n\\underset{\\mathbf{M}^{1}, \\cdots, ~\\mathbf{M}^{M}, ~\\mathbf{A}_\\text{c}^{N}}{\\min} \\mathcal{L}_{\\operatorname{sim}} \\left(\\mathbf{W}^{N}, \\mathbf{T}\\right)+ \\sum_{j=1}^{M}\\lambda \\mathcal{R}(\\mathbf{M}^{j}),\n\\end{equation}\nwhere $\\mathbf{W}^{N} = \\mathcal{T}(\\mathbf{E}^{M}, \\mathbf{A}_\\text{c}^{N})$ and $\\mathcal{L}_{\\operatorname{sim}}(\\cdot, \\cdot)$ is a loss function measuring the similarity between the final warped image $\\mathbf{W}^{N}$ and the target image $\\mathbf{T}$. Here we use the popular negative local cross-correlation loss, which is robust to voxel intensity variations often found across scans and datasets \\cite{balakrishnan2018unsupervised}. $\\mathcal{R}(\\cdot)$ is the regularization term for brain masks, and $\\lambda$ is a regularization parameter. Since the brain region is one connected entity, we would like our predicted brain masks to have the same properties across all stages. To put it more formally, if we view the brain mask as a 3D tensor with 6-connectivity, we would like it to have exactly one connected component. Though it is possible to count the number of connected components in a mask, such practice would be time-consuming ($O(W\\times H\\times D)$ by BFS algorithm) and not differentiable. For the purpose of both effective and efficient estimation, we use the $\\ell_2$-norm of the first-order derivative of $\\mathbf{M}^{j}$ as the regularization term:\n%\\begin{equation}\n%\\mathcal{R}(\\mathbf{M}^{j})=\\sum\\|\\nabla %\\mathbf{M}^{j}\\|^{2} = .\n%\\end{equation}\n\n\\begin{equation}\n\\mathcal{R}(\\mathbf{M}^{j})=\\sum_{x=1}^{W} \\sum_{y=1}^{H}  \\sum_{z=1}^{D}\\|\\nabla \\mathbf{M}_{xyz}^{j}\\|^{2}.\n\\end{equation}\n\nThis regularization term measures edge strength, \\ie the likelihood of a voxel to be an edge voxel. By minimizing the regularization term, we can suppress the occurrence of edges, which in term suppress additional connected components. Specifically, we approximate the first-order derivative by measuring differences between neighboring voxels.\nFor $\\nabla\\mathbf{M}_{xyz}^{j}=(\\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial x}, \\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial y}, \\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial z})$, we have $\\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial x} \\approx \\mathbf{M}_{(x+1)yz}^{j} - \\mathbf{M}_{xyz}^{j} $. The approximation of $\\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial y}$ and $\\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial z}$ follows.\n\n\n%This regularization term would suppress the occurrence of edges, which in term suppress additional connected components. Specifically, we approximate the spatial gradients by measuring differences between neighboring voxels.\n%For $\\nabla\\mathbf{M}_{xyz}^{j}=\\left(\\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial x}, \\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial y}, \\frac{\\partial \\mathbf{M}_{xyz}^{j}}{\\partial z}\\right)$, we approximate \n\nBenefiting from the differentiability of each component of this design, our model can be cooperatively and progressively optimized across each stage in an end-to-end manner. Such a training scheme allows us to find a  joint optimal solution to the collective brain extraction and registration task.\n\n% defined in Problem \\ref{prob 1}. In contrast, if we choose to train the extraction and registration modules in two separate sessions, in essence, we would be addressing Problems \\ref{prob 2} and \\ref{prob 3} in sequence, and the yielded solution would only be sub-optimal. \n\n\n% Figure \\ref{fig:unsupervised} illustrates the overview of unsupervised image registration and the below shows the formula of the loss function as: \n% \\begin{equation}\n% \\label{eq: loss}\n% \\mathcal{L}\\left(\\mathbf{I}_{s}, \\mathbf{I}_{t}, \\{\\phi_{G}^{(k)}\\}_{k=1}^n\\right)=\\mathcal{L}_{\\operatorname{sim}}\\left(\\mathbf{I}_{w}^{(n)}, \\mathbf{I}_{t}\\right)+ \\sum_{k=1}^{n}\\lambda \\mathcal{L}_{\\text {smooth }}(\\phi_{G}^{(k)}).\n% \\end{equation}\n\n\n\n\n% \\begin{equation}\n% \\mathbf{I}_{w}^{(n)}=\\mathcal{T}\\left(\\mathbf{I}_{s}, \\phi_{G}^{(n)}\\right)\n% \\end{equation}\n\n% Two separate similarity measures are being used in 2D face registration and 3D brain MRI registration. Same to \\cite{balakrishnan2018unsupervised}, the similarity loss $\\mathcal{L}_{\\text {sim }}(\\cdot)$ is measured by computing the negative local cross-correlation of $\\mathbf{I}_{w}^{(n)}$ and $\\mathbf{I}_{t}$ in 3D brain registration task. Differently, in the 2D face registration task, mean squared error (MSE) of corresponding pixel values is used as the loss function due to the same pixel intensity distribution between $\\mathbf{I}_{w}^{(n)}$ and $\\mathbf{I}_{t}$. The relevant datasets for these two registration tasks will be introduced in the following section.\n\n\n% $\\mathcal{R}(\\cdot)$ is a regularization function that ensures the smoothness of the the deformation field $\\phi_{L}^{(k)}$ and $\\lambda$ is a regularization parameter. \n\n% %-----------------------------------------------\n% % Experiment Section\n%\\section{Experiment and Analysis}\n"
                }
            },
            "section 3": {
                "name": "Experiments",
                "content": "\n\n",
                "subsection 3.1": {
                    "name": "Datasets",
                    "content": " \\label{section:Dataset}\n%In order to evaluate our proposed ABN method, we conduct experiments on both natural and medical image registration tasks, by using the following datasets.\n%In order to evaluate the performance of our method on image registration tasks, we conduct experiments on three different datasets, one with natural images (2D) and two with medical images (3D).\n\nWe evaluate the effectiveness of our proposed method on three different real-world 3D brain MRI datasets: 1) \\emph{LPBA40}~\\cite{shattuck2008construction} consists of 40 raw T1-weighted 3D brain MRI scans along with their brain masks. It also provides the corresponding segmentation ground truth of 56 anatomical structures; 2) \\emph{CC-359}~\\cite{souza2018open} consists of 359 raw T1-weighted 3D brain MRI scans and the corresponding brain masks. It also contains the labeled white matter as the ground truth; 3) \\emph{IBSR}~\\cite{rohlfing2011image} provides 18 raw T1-weighted 3D brain MRI scans along with the corresponding manually segmentation results. Due to the small sample size, We use this dataset only to test the model trained on CC359.\nThe brain mask and anatomical segmentations are used to evaluate the accuracy of extraction and registration, respectively. Datasets are splited into training, validation, and test sets, respectively. For more details, please refer to Appendix~\\ref{sec:appendix}. \n\n\\iffalse\n\\noindent\\textbullet\\ \\textit{LONI Probabilistic Brain Atlas (LPBA40)}~\\cite{shattuck2008construction}: \nThis dataset consists of 40 raw T1-weighted 3D brain MRI scans along with their brain masks. It also provides the corresponding segmentation ground truth of 56 anatomical structures. The brain mask and anatomical segmentations are used to evaluate the accuracy of extraction and registration, respectively. Same to~\\cite{balakrishnan2018unsupervised, zhao2019recursive}, we focus on atlas-based registration, where the first scan is the target image and the remaining scans need to align with it. Among the 39 scans, we use 30, 5, and 4 scans for training, validation, and test, respectively. All scans are resized to $96\\times96\\times96$ after cropping.\n\n\n\\noindent\\textbullet\\ \\textit{Calgary-Campinas-359 (CC-359)}~\\cite{souza2018open}: \nThis dataset consists of 359 raw T1-weighted 3D brain MRI scans and the corresponding brain masks. It also contains the labeled white matter as the ground truth. We use the brain masks and white-matter masks to evaluate the accuracy of extraction and registration, respectively. Same to LPBA40, we concentrate on atlas-based registration and split CC359 into 298, 30, and 30 scans for training, validation, and test sets. All scans are resized to $96\\times96\\times96$ after cropping.\n\n\n\\noindent\\textbullet\\ \\textit{Internet Brain Segmentation Repository (IBSR)} \\cite{rohlfing2011image}: \nThis dataset provides 18 raw T1-weighted 3D brain MRI scans along with the corresponding manually segmentation results. \n%We merge all segmentation results to construct the brain mask. \nDue to the small sample size, We use this dataset only to test the model trained on CC359. \n%Similarly, we follow atlas-based registration that all 18 scans need to align with the first scan of CC359. All scans are resized to $96\\times96\\times96$ after cropping.\nThus, all 18 scans need to align with the first scan of CC359. All scans are resized to $96\\times96\\times96$ after cropping.\n\\fi\n\n% \\noindent\\textbullet\\ \\textit{Flickr-Faces-HQ (FFHQ)} \\cite{karras2019style}: \n% %This dataset is extracted from FFHQ,\n% We used a subset of the FFHQ dataset, which contains 400 human face images, randomly sampled from the whole dataset. \n% %Following \\cite{dai2020dual}, we collect the pairs of source images and target images by applying random transformations to the raw image. \n% In order to construct pairs of source and target images, similar to \\cite{dai2020dual}, we random transform each face image using a random deformation field. % smoothed by a Gaussian filter. \n% %We then use the original and deformed image pairs as the target and source images, respectively, and form them in a pair. \n% We use the deformed images as the target image and the raw image as the source image. We resize all images to $64\\times64$, and randomly split the image pairs into training and test sets with the ratio of 80$\\%$ and 20$\\%$, respectively. \n\n% \\noindent\\textbullet\\ \\textit{LPBA40} \\cite{shattuck2008construction}: \n% This dataset consists of 40 T1-weighted 3D brain MRI scans and the corresponding segmentation ground truth of 56 anatomical structures. \n% The ground truth is used to evaluate the registration accuracy. \n% Same as \\cite{balakrishnan2018unsupervised, zhao2019recursive}, we focus on atlas-based registration in this experiment, in which the first scan in the dataset is the target image and the remaining scans need to be aligned with the target image. \n% Among the 39 scans,  30 scans are used for training and 9 scans are used for testing. \n% All scans are resized to $96\\times96\\times96$ after cropping.\n\n% \\noindent\\textbullet\\ \\textit{MindBoggle101} \\cite{klein2012101}: \n% This dataset contains 101 T1-weighted 3D brain MRI scans and the corresponding annotation of 25 cortical regions, but only 62 scans have their valid segmentation ground truth. \n% Following the recent work \\cite{liu2019probabilistic}, we focus on atlas-based registration using 41 scans for training and 20 scans for testing. All scans are resized to $96\\times96\\times96$ after cropping.\n\n\n% \\noindent\\textbullet\\ \\textbf{Face Image:}\n% For the 2D face image registration, we use FFHQ (Flickr-Faces-HQ) \\cite{karras2019style} dataset which contains 70,000 human face images. Similar to the approach use in \\cite{dai2020dual}, we collect the pairs of source images and target images by applying random transformations to the raw image. We first dealigned each image using a random deformation field smoothed by a Gaussian filter. We then take the original image and the dealigned image as the target image and source image respectively, and paced them in a pair. We randomly select 400 pairs of images and divided them into one group of 320 and the other group of 80 for training and test. Finally, all images are resized to $64\\times64$.\n\n% \\noindent\\textbullet\\ \\textbf{Brain MRI:} We use two public datasets for the 3D brain MRI registration, LPBA40 \\cite{shattuck2008construction} and MindBoggle101 \\cite{klein2012101}. LPBA40 consists of 40 T1-weighted scans and the corresponding segmentation ground truth of 56 anatomical structures. The ground truth is used to evaluate the registration accuracy. Same as \\cite{balakrishnan2018unsupervised, zhao2019recursive}, we focused on atlas-based registration in this experiment, in which the first scan in LPBA40 \n% is a target image and the rest of the scans need to align with it. Among the rest 39 scans, a group of 30 scans and the other group of 9 scans were used for training and testing, respectively. MindBoggle101 contains 101 T1-weighted scans and the corresponding annotation of 25 cortical regions, but only 62 scans have their valid segmentation ground truth. Following the recent work \\cite{liu2019probabilistic}, we used 42 images for training and 20 images for testing. All scans of two datasets were resized to $96\\times96\\times96$ after cropping.\n\n"
                },
                "subsection 3.2": {
                    "name": "Compared Methods",
                    "content": "\n\\label{section:Compared Methods}\n\n%We compare our approach to seven state-of-the-art methods, as shown in Table~\\ref{tab:methods}.\n\nWe compare our ERNet with several representative brain extraction and registration methods, as shown in Table~\\ref{tab:methods}. To the best of our knowledge, there is no existing solution that can perform the brain extraction and registration simultaneously under an unsupervised setting. Therefore, we designed two-stage pipelines for comparison using the following brain extraction and registration methods. \n\n% \\noindent\\textbullet\\ \\textit{Brain Extraction Tool (BET)} \\cite{smith2002fast}: This is a skull stripping method included in FMRIB's Software Library (FSL). It uses a deformable approach to fit the brain surface by applying locally adaptive set models.\n\n\\noindent\\textbullet\\ \\textit{Brain Extraction Tool (BET)} \\cite{smith2002fast}: This is a skull stripping method included in FSL package. It uses a deformable approach to fit the brain surface by applying locally adaptive set models.\n\n% \\noindent\\textbullet\\ \\textit{Brain Extraction Tool$^*$ (BET$^*$)} \\cite{smith2002fast}: This is an optimized version of BET, which separates the non-brain and brain regions by an intensity-based estimation at a threshold of 0.7.\n\n% \\noindent\\textbullet\\ \\textit{3dSkullStrip} \\cite{cox1996afni}: This is a modified version of BET that is included in the Analysis of Functional Neuro Images (AFNI) package. It performs skull stripping based on the expansion paradigm of the spherical surface.\n\\noindent\\textbullet\\ \\textit{3dSkullStrip} \\cite{cox1996afni}: This is a modified version of BET that is included in the AFNI package. It performs skull stripping based on the expansion paradigm of the spherical surface.\n\n\\noindent\\textbullet\\ \\textit{Brain Surface Extractor (BSE)} \\cite{shattuck2002brainsuite}: It extracts the brain region based on morphological operations and edge detection, which employs anisotropic diffusion filtering and a Marr Hildreth edge detector for brain boundary identification.\n\n% \\noindent\\textbullet\\ \\textit{FMRIB's Linear Image Registration Tool (FLIRT)} \\cite{jenkinson2001global}: This is a fully automated linear (affine) brain image registration tool that is part of the FMRIB's Software Library (FSL).\n\\noindent\\textbullet\\ \\textit{FMRIB's Linear Image Registration Tool (FLIRT)} \\cite{jenkinson2001global}: This is a fully automated affine brain image registration tool in FSL package.\n\n% \\noindent\\textbullet\\ \\textit{Advanced Normalization Tools (ANTs)} \\cite{avants2009advanced}: It is popularly considered a state-of-the-art medical image registration toolkit, which contains various transformation models and similarity metrics for image registration. Here we utilize affine transformation model and cross-correlation metric for registration.\n\n\\noindent\\textbullet\\ \\textit{Advanced Normalization Tools (ANTs)} \\cite{avants2009advanced}: It is considered a state-of-the-art medical image registration toolkit. Here we utilize affine transformation model and cross-correlation metric for registration.\n\n\\noindent\\textbullet\\ \\textit{VoxelMorph (VM)} \\cite{balakrishnan2018unsupervised}: This unsupervised, deformable image registration method employs a neural network to predict the nonlinear transformation between images.\n\n\\noindent\\textbullet\\ \\textit{Cascaded Registration Networks (CRN)} \\cite{zhao2019recursive}:\nIt is an unsupervised multi-stage image registration method. In different stages, the source image is repeatedly deformed to align with a target image.\n\n\\noindent\\textbullet\\ \\textit{ERNet}: This is our proposed model which consists of both extraction and registration modules in an end-to-end manner.\n\n\\noindent\\textbullet\\ \\textit{ERNet w/o Ext}: This is a variant of ERNet where we remove the extraction modules. Here it is a registration method only.\n\n%\\noindent\\textbullet\\ \\textit{Affine registration (Affine)} \\cite{avants2009advanced}: \n%This method breaks the image registration task into a composition of a linear transformation and a translation. \n% Affine mapping would suffice the task when only translation, rotation, scaling and shearing are needed. We used existing affine implementation in the publicly available software package - ANTs \\cite{avants2009advanced}.\n\n% \\noindent\\textbullet\\ \\textit{BSpline transform (BSpline)} \\cite{rueckert1999nonrigid}: This method uses control points and spline functions to describe the nonlinear geometric transformation domain.\n\n% \\noindent\\textbullet\\ \\textit{Demons algorithm (Demons)} \\cite{thirion1998image}: This method is inspired by the optical flow equations and considers non-rigid image registration as a diffusion process.\n\n% \\noindent\\textbullet\\ \\textit{Elastic registration (Elastic)} \\cite{bajcsy1989multiresolution,avants2009advanced}: This method estimates the elastic geometric deformation by updating transformation parameters iteratively. \n% % The Elastic is included in ANTs \\cite{avants2009advanced} software package. We ran the elastic registration with the default setting.\n\n\n% \\noindent\\textbullet\\ \\textit{Symmetric normalization (SyN)} \\cite{avants2008symmetric}: This is a top-performing traditional method for deformable image registration \\cite{klein2009evaluation, balakrishnan2018unsupervised}. This method optimizes the space of diffeomorphic maps by maximizing the cross-correlation between images. \n% % Same as Elastic, we ran SyN via ANTs \\cite{avants2009advanced} with a default setting.\n\n\n% \\noindent\\textbullet\\ \\textit{VoxelMorph (VM)} \\cite{balakrishnan2018unsupervised}: This is an unsupervised single-stage registration method, which use one network to predict the deformation between images.\n\n% % For network architectures, we used the latest version, VoxelMorph-2, and configured 10 convolutional layers with 16, 32, 32, 64, 64, 64, 32, 32, 32 and 16 filters.\n\n% \\noindent\\textbullet\\ \\textit{Cascaded Registration Networks (CRN)} \\cite{zhao2019recursive}:\n% This is a state-of-the-art learning-based method for unsupervised image registration with a multi-stage design. In different stages, the source image is repeatedly deformed to align with a target image.\n\n% % In terms of the setting of convolutional layers and filters of each stage, CRN is the same as VM.\n\n% \\noindent\\textbullet\\ \\textit{Anti-Blur Networks-Long (ABN-L)}: ABN-L is a variant of ABN, which only contains a long-term memory network $f_{L}(\\cdot)$ at each stage. The network handle the registration and combination of deformations simultaneously.\n\n% % ABN-L is our proposed method with a lightweight architecture. Each stage of the method contains only one sub-registration network to predict the deformation field. As introduced in Section \\ref{sec:ABN-L}, each sub-registration network has 3 inputs, thus we configured 10 convolutional layers of the sub-registration network with 32, 64, 64, 128, 128, 128, 64, 64, 64 and 32 filters. The number of stages of ABN-L is the same as that of CRN for fair comparison. \n\n% \\noindent\\textbullet\\ \\textit{Anti-Blur Networks (ABN)}:\n% This is our proposed model which consists of two sub-networks at each stage, the short-term registration and long-term memory networks. \n% % The number of convolutional layers in both sub-registration network is the same as that of VM and CRN. Again, to ensure a fair comparison, the number of stages of ABN is the same as that of CRN and ABN-L.\n% \\begin{table}[t]\n%     \\centering\n%     \\caption{Summary of compared methods.}\n%     \\label{tab:methods}\n%     \\vspace{-10pt}\n%     \\resizebox{\\linewidth}{!}{\n%     \\begin{tabular}{lcccc}\n%     \\toprule\n%     \\textbf{Methods}& \\textbf{Anti-blur}& \\textbf{Multi-stage}& \\textbf{Nonlinear}& \\textbf{Deep learning}\\\\\n%     \\midrule\n%     Affine { \\cite{avants2009advanced}} & \\cmark  & \\xmark & \\xmark & \\xmark \\\\\n%     BSpline { \\cite{rueckert1999nonrigid}} & \\cmark  & \\xmark & \\cmark & \\xmark\\\\\n%     Demons \\cite{thirion1998image}& \\cmark  & \\xmark & \\cmark & \\xmark\\\\\n%     Elastic \\cite{bajcsy1989multiresolution}& \\xmark  & \\cmark & \\cmark & \\xmark\\\\\n%     SyN \\cite{avants2008symmetric} & \\xmark  & \\cmark & \\cmark & \\xmark\\\\\n%     VM \\cite{balakrishnan2018unsupervised}& \\cmark  & \\xmark & \\cmark & \\cmark\\\\\n%     CRN \\cite{zhao2019recursive}& \\xmark  & \\cmark & \\cmark & \\cmark\\\\\n%     ABN-L (ours)& \\cmark  & \\cmark & \\cmark & \\cmark \\\\\n%     ABN (ours)& \\cmark  & \\cmark & \\cmark & \\cmark\\\\\n%     \\bottomrule\n%     \\end{tabular}\n%     }\n%     \\vspace{-10pt}\n% \\end{table}\n% \\begin{table}[t]\n%     \\centering\n%     \\caption{Summary of compared methods.}\n%     \\label{tab:methods}\n%     \\vspace{-10pt}\n%     \\resizebox{\\linewidth}{!}{\n%     \\begin{tabular}{lcccc}\n%     \\toprule\n%     \\textbf{Methods}& \\textbf{Extraction}& \\textbf{Registration}& \\textbf{Unsupervised}& \\textbf{Deep learning}\\\\\n%     \\midrule\n%     BET { \\cite{smith2002fast}} & \\cmark  & \\xmark & \\cmark & \\xmark \\\\\n%     BET$^*$ { \\cite{smith2002fast}} & \\cmark  & \\xmark & \\cmark & \\xmark\\\\\n%     3dSkullStrip \\cite{cox1996afni}& \\cmark  & \\xmark & \\cmark & \\xmark\\\\\n%     BSE \\cite{shattuck2002brainsuite}& \\cmark  & \\xmark & \\cmark & \\xmark\\\\\n%     \\midrule\n%     FLIRT \\cite{jenkinson2001global} & \\xmark  & \\cmark & \\cmark & \\xmark\\\\\n%     ANTs \\cite{avants2009advanced}& \\xmark  & \\cmark & \\cmark & \\xmark\\\\\n%     VM \\cite{balakrishnan2018unsupervised}& \\xmark  & \\cmark & \\cmark & \\cmark\\\\\n%     CRN \\cite{zhao2019recursive}& \\xmark  & \\cmark & \\cmark & \\cmark \\\\\n%     \\midrule\n%     ERNet w/o Ext& \\xmark  & \\cmark & \\cmark & \\cmark\\\\\n%     ERNet (ours)& \\cmark  & \\cmark & \\cmark & \\cmark\\\\\n%     \\bottomrule\n%     \\end{tabular}\n%     }\n%     \\vspace{-10pt}\n% \\end{table}\n%%%%% No time %%%%%%%%%%%\n\n\n\n\n\n% \\begin{table*}[t]\n%     \\centering\n%     \\caption{Results for 3D brain MRI registration. The results are reported as performance(mean $\\pm$ std {\\color{blue}(rank)}). The \"Avg. Rank\" column shows the average rank of each method across all metric. “$\\uparrow$” point out “the larger the better” and “$\\downarrow$” point out “the smaller the better”.}\n%     \\label{tab:3D res}\n%     \\vspace{-5pt}\n%     \\resizebox{0.9\\linewidth}{!}{\n%     \\begin{tabular}{llccccrc}\n%     \\toprule\n%     \\multirow{2}{*}{Dataset} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{Registration Accuracy} & \\multicolumn{2}{c}{Sharpness} & \\multicolumn{1}{c}{Time} & \\multirow{2}{*}{Avg. Rank $\\downarrow$}\\\\\n%     \\cmidrule(lr){3-4}\\cmidrule(lr){5-6}\\cmidrule(lr){7-7}\n%     & & Dice $\\uparrow$ & Jaccard $\\uparrow$ & SMD $\\uparrow$ & Tenengrad $\\uparrow$ & \\multicolumn{1}{c}{Sec $\\downarrow$}\\\\\n%     \\midrule\n%     \\multirow{9}{*}{Mindboggle101} & Affine \\cite{avants2009advanced} & 0.340 $\\pm$ 0.017 {\\color{blue}(9)} & 0.209 $\\pm$ 0.012 {\\color{blue}(9)} & 0.077 $\\pm$ 0.007 {\\color{blue}(3)} & 4.329 $\\pm$ 0.775 {\\color{blue}(1)} & 1.94 (CPU) & {\\color{blue}5.5}\\\\\n%     & BSpline \\cite{rueckert1999nonrigid} & 0.349 $\\pm$ 0.016 {\\color{blue}(8)} & 0.215 $\\pm$ 0.012 {\\color{blue}(8)} & 0.078 $\\pm$ 0.007 {\\color{blue}(2)} & 4.247 $\\pm$ 0.741 {\\color{blue}(2)} & 4.97 (CPU) & {\\color{blue}5.0}\\\\\n%     & Demons \\cite{thirion1998image} & 0.454 $\\pm$ 0.025 {\\color{blue}(2)} & 0.300 $\\pm$ 0.010 {\\color{blue}(2)} & 0.084 $\\pm$ 0.008 {\\color{blue}(1)} & 4.078 $\\pm$ 0.736 {\\color{blue}(5)} & 100.95 (CPU) & {\\color{blue}2.5}\\\\\n%     & Elastic \\cite{bajcsy1989multiresolution} & 0.426 $\\pm$ 0.013 {\\color{blue}(5)} & 0.275 $\\pm$ 0.010 {\\color{blue}(5)} & 0.075 $\\pm$ 0.007 {\\color{blue}(4)} & 3.864 $\\pm$ 0.670 {\\color{blue}(8)} & 14.14 (CPU) & {\\color{blue}5.5}\\\\\n%     & SyN \\cite{avants2008symmetric} & 0.399 $\\pm$ 0.013 {\\color{blue}(7)} & 0.253 $\\pm$ 0.010 {\\color{blue}(7)} & 0.078 $\\pm$ 0.007 {\\color{blue}(2)} & 4.025 $\\pm$ 0.698 {\\color{blue}(6)} & 5.05 (CPU) & {\\color{blue}5.5}\\\\\n%     & VM \\cite{balakrishnan2018unsupervised} & 0.433 $\\pm$ 0.021 {\\color{blue}(4)} & 0.281 $\\pm$ 0.017 {\\color{blue}(4)} & 0.078 $\\pm$ 0.007 {\\color{blue}(2)} & 4.000 $\\pm$ 0.663 {\\color{blue}(7)} & 0.02 (GPU) & {\\color{blue}4.3}\\\\\n%     & CRN \\cite{zhao2019recursive} & 0.405 $\\pm$ 0.014 {\\color{blue}(6)} & 0.257 $\\pm$ 0.011 {\\color{blue}(6)} & 0.058 $\\pm$ 0.007 {\\color{blue}(5)} & 3.335 $\\pm$ 0.531 {\\color{blue}(9)} & 0.18 (GPU) & {\\color{blue}6.5}\\\\\n%     & ABN-L (ours) & 0.444 $\\pm$ 0.018 {\\color{blue}(3)} & 0.290 $\\pm$ 0.015 {\\color{blue}(3)} & 0.078 $\\pm$ 0.007 {\\color{blue}(2)} & 4.084 $\\pm$ 0.677 {\\color{blue}(4)} & 0.31 (GPU) & {\\color{blue}3.0}\\\\\n%     & \\textbf{ABN (ours)} & 0.466 $\\pm$ 0.030 {\\color{blue}(1)} & 0.309 $\\pm$ 0.025 {\\color{blue}(1)} & 0.078 $\\pm$ 0.007 {\\color{blue}(2)} & 4.099 $\\pm$ 0.696 {\\color{blue}(3)} & 0.36 (GPU) & {\\color{blue}1.8}\\\\\n%     \\midrule\n%     \\multirow{9}{*}{LPBA40} & Affine \\cite{avants2009advanced} & 0.631 $\\pm$ 0.012 {\\color{blue}(9)} & 0.469 $\\pm$ 0.016 {\\color{blue}(9)} & 0.053 $\\pm$ 0.004 {\\color{blue}(3)} & 2.846 $\\pm$ 0.237 {\\color{blue}(3)} & 1.67 (CPU) & {\\color{blue}6.0}\\\\\n%     & BSpline \\cite{rueckert1999nonrigid} & 0.644 $\\pm$ 0.012 {\\color{blue}(8)} & 0.480 $\\pm$ 0.014 {\\color{blue}(8)} & 0.054 $\\pm$ 0.004 {\\color{blue}(2)} & 2.765 $\\pm$ 0.227 {\\color{blue}(5)} & 2.90 (CPU) & {\\color{blue}5.8}\\\\\n%     & Demons \\cite{thirion1998image} & 0.653 $\\pm$ 0.064 {\\color{blue}(6)} & 0.495 $\\pm$ 0.066 {\\color{blue}(5)} & 0.054 $\\pm$ 0.003 {\\color{blue}(2)} & 2.746 $\\pm$ 0.355 {\\color{blue}(6)} & 81.98 (CPU) & {\\color{blue}4.8}\\\\\n%     & Elastic \\cite{bajcsy1989multiresolution} & 0.674 $\\pm$ 0.012 {\\color{blue}(3)} & 0.514 $\\pm$ 0.013 {\\color{blue}(3)} & 0.051 $\\pm$ 0.003 {\\color{blue}(4)} & 2.494 $\\pm$ 0.190 {\\color{blue}(9)} & 13.82 (CPU) & {\\color{blue}4.8}\\\\\n%     & SyN \\cite{avants2008symmetric} & 0.670 $\\pm$ 0.011 {\\color{blue}(4)} & 0.510 $\\pm$ 0.014 {\\color{blue}(4)} & 0.053 $\\pm$ 0.004 {\\color{blue}(3)} & 2.679 $\\pm$ 0.209 {\\color{blue}(7)} & 4.55 (CPU) & {\\color{blue}4.5}\\\\\n%     & VM \\cite{balakrishnan2018unsupervised} & 0.652 $\\pm$ 0.017 {\\color{blue}(7)} & 0.490 $\\pm$ 0.018 {\\color{blue}(7)} & 0.055 $\\pm$ 0.003 {\\color{blue}(1)} & 2.889 $\\pm$ 0.194 {\\color{blue}(1)} & 0.03 (GPU) & {\\color{blue}4.0}\\\\\n%     & CRN \\cite{zhao2019recursive} & 0.656 $\\pm$ 0.016 {\\color{blue}(5)} & 0.494 $\\pm$ 0.017 {\\color{blue}(6)} & 0.045 $\\pm$ 0.002 {\\color{blue}(5)} & 2.595 $\\pm$ 0.144 {\\color{blue}(8)} & 0.23 (GPU) & {\\color{blue}6.0}\\\\\n%     & ABN-L (ours) & 0.676 $\\pm$ 0.017 {\\color{blue}(2)} & 0.518 $\\pm$ 0.018 {\\color{blue}(2)} & 0.054 $\\pm$ 0.003 {\\color{blue}(2)} & 2.830 $\\pm$ 0.178 {\\color{blue}(4)} & 0.44 (GPU) & {\\color{blue}2.5}\\\\\n%     & \\textbf{ABN (ours)} & 0.684 $\\pm$ 0.015 {\\color{blue}(1)} & 0.527 $\\pm$ 0.017 {\\color{blue}(1)} & 0.054 $\\pm$ 0.003 {\\color{blue}(2)} & 2.847 $\\pm$ 0.181 {\\color{blue}(2)} & 0.46 (GPU) & {\\color{blue}1.5}\\\\\n%     \\bottomrule\n%     \\end{tabular}}\n%     \\vspace{-15pt}\n% \\end{table*}\n\n"
                },
                "subsection 3.3": {
                    "name": "Evaluation Metrics",
                    "content": "\n\\label{section:Evaluation Metrics}\n%We use two metrics to assess the registered image quality: registration accuracy and image sharpness, which are detailed as follows. \n% Due to the differences between datasets used in the two tasks, separate evaluation metrics are used to assess the registration accuracy. \n% The 3D brain MRI datasets include the segmentation of anatomical structures that can be used to measure registration performance. However, the 2D face image dataset only contains different pairs of source images and target images. Thus their registration accuracy can only be evaluated by the similarity between the final warped images and the target images. Therefore, we use corresponding evaluation metrics for each task. \n% We introduce each corresponding evaluation metric in the following subsection.\nOur defined problem aims to identify the brain region within the source image and align the extracted cerebral tissues to the target image simultaneously. Thus, we evaluate the accuracy of extraction and registration to show the performance of our proposed method and compared methods as follows:\n\n",
                    "subsubsection 3.3.1": {
                        "name": "Extraction Performance.",
                        "content": "\nThe brain MRI datasets contain the brain mask ground truth, which is the label of brain tissue in the source image. To evaluate the extraction accuracy, we measure the volume overlap of brain masks by Dice score, which can be formulated as:\n%\\begin{equation}\n%\\text{Dice}_{\\mathrm{ext}}=2 \\cdot \\frac{|\\mathbf{M} \\cap %\\widetilde{\\mathbf{M}}|}{|\\mathbf{M}|+|\\widetilde{\\mathbf{M}}|},\n%\\end{equation}\n\\begin{equation}\n\\text{Dice}_{\\mathrm{ext}}=2 \\cdot \\frac{|\\mathbf{\\hat{M}} \\cap \\mathbf{M}|}{|\\mathbf{\\hat{M}}|+|\\mathbf{M}|},\n\\end{equation}\nwhere $\\mathbf{\\hat{M}}$ is the predicted brain mask and $\\mathbf{M}$ denotes the corresponding ground truth. If $\\mathbf{\\hat{M}}$ represents accurate extraction, we expect the non-zero regions in $\\mathbf{\\hat{M}}$ and $\\mathbf{M}$ to overlap well.\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Registration Performance.",
                        "content": "\nWe evaluate the registration accuracy by measuring the volume overlap of anatomical segmentations, which are the location labels of different tissues in the brain MRI image. If two images are well aligned, then their corresponding anatomical structures should overlap with each other. Likewise, the Dice score can evaluate the performance of registration, as follows:\n\\begin{equation}\n\\text{Dice}_{\\mathrm{reg}}=2 \\cdot \\frac{|\\mathbf{G}_\\text{w} \\cap \\mathbf{G}_\\text{t}|}{|\\mathbf{G}_\\text{w}|+|\\mathbf{G}_\\text{t}|}\n\\end{equation}\nwhere $\\mathbf{G}_\\text{w}=\\mathcal{T}\\left(\\mathbf{G}_\\text{s},\\mathbf{\\hat{a}}\\right)$.\n% \\begin{equation}\n% \\mathbf{S}_\\text{w}=\\mathcal{T}\\left(\\mathbf{S}_\\text{s}\\circ \\mathbf{M},\\mathbf{a}\\right).\n% \\end{equation}\n$\\mathbf{G}_\\text{s}$, $\\mathbf{G}_\\text{t}$ and $\\mathbf{G}_\\text{w}$ are anatomical structural segmentation of the source image, target image and warped image, respectively. $\\mathbf{\\hat{a}}$ is the predicted affine transformation parameters. A dice score of 1 means that the corresponding structures are well aligned after registration, a score of 0 indicates that there is no overlap. If the image contains multiple labeled anatomical structures, the final score is the average of the dice score of the each structure.\n\n% \\subsubsection{Registration Accuracy.}\n% \\label{section:Registration accuracy}\n% %\\hphantom{1}\n% %\\newline\n% %\\noindent\\textbullet\\ \\textbf{2D face registration:} \n% % As introduced in the previous section,\n% The 2D face dataset only contains the pairs of source images and target images. To evaluate the registration accuracy, we measure the image similarity by cross-correlation (CC) \\cite{rabiner1978digital} and structural similarity (SSIM) \\cite{wang2004image} between the final warped images and target images.\n\n% %\\noindent\\textbullet\\ \\textbf{3D brain MRI registration:} \n% The brain MRI datasets contain the segmentation ground truth of anatomical structures, which is the location label of different tissues in the brain. If the two images are aligned, then their anatomical structures should  overlap with each other. Thus, it is reasonable to use the structure overlap between two images to evaluate the registration performance. Similar to \\cite{balakrishnan2018unsupervised,zhao2019recursive,de2019deep}, we used Dice score and Jaccard coefficient to measure the overlap between two regions.\n% , which can be formulated as:\n% \\begin{equation}\n% {\\displaystyle Dice(X,Y)={\\frac {2|X\\cap Y|}{|X|+|Y|}}}  \n% \\end{equation}\n% \\begin{equation}\n% \\operatorname{Dice}(\\mathbf{S}_{w}, \\mathbf{S}_{t})=2 \\cdot \\frac{|\\mathbf{S}_{w} \\cap \\mathbf{S}_{t}|}{|\\mathbf{S}_{w}|+|\\mathbf{S}_{t}|}\n% \\end{equation}\n% where\n% \\begin{equation}\n% \\mathbf{S}_{w}=\\mathcal{T}\\left(\\mathbf{S}_{s}, \\phi\\right).\n% \\end{equation}\n% $\\mathbf{S}_{s}$, $\\mathbf{S}_{t}$ and $\\mathbf{S}_{w}$ are structural segmentation of the source image, target image and warped image, respectively. $\\phi$ is the predicted deformation field. \n% The final score is the average of the overlap for each anatomical structure.\n\n% \\subsubsection{Image Sharpness.}\n% %\\hphantom{1}\n% %\\newline\n% %\\indent \n% We quantify the sharpness of the final warped image by Sum Modulus Difference (SMD) \\cite{santos1997evaluation} and Tenengrad \\cite{yeo1993autofocusing,tenenbaum1970accommodation}. Those are popular no-reference sharpness evaluation metrics.\n% % SMD evaluated the sharpness of an image by calculating the difference between the values of each pixel and its neighboring pixels. \n% SMD evaluated the sharpness of an image by summing the differences between adjacent pixels. If the image is sharp, then we expect the difference is larger than that of a blurred image. \n% % %We used SMD to measure the difference between neighboring pixels, which can be computed as:\n% % \\begin{equation}\n% % \\begin{split}\n% % \\operatorname{SMD}(\\mathbf{I}_{w})=\\sum_{x}^{W} \\sum_{y}^{H} \\sum_{z}^{D}|\\mathbf{I}_{w}(x+1, y, z)-\\mathbf{I}_{w}(x, y, z)| \\\\\n% % +|\\mathbf{I}_{w}(x, y+1, z)-\\mathbf{I}_{w}(x, y, z)|\n% % +|\\mathbf{I}_{w}(x, y, z+1)-\\mathbf{I}_{w}(x, y, z)|\n% % \\end{split}\n% % \\end{equation}\n% % where $\\mathbf{I}_{w}(x, y, z)$ is the voxel value of the warped image $\\mathbf{I}_{w}$ at the 3D coordinate $(x, y, z)$. $W$, $H$ and $D$ represent the width, height and depth of the image, respectively.\n% Tenengrad evaluated the sharpness of an image by extracting its edge information. The image edge is the region where the intensity of the image signal changes rapidly, and the sharper the image, the faster the edge changes. \n\n% % We used Tenengrad, a edge-based metric, to measure the sharpness of image, which can be defined as:\n% % \\begin{multline}\n% % \\operatorname{Tenengrad}(\\mathbf{I}_{w})=\\sum_{n}^{W} \\sum_{m}^{H} \\sum_{l}^{D} \\Big(\\mathbf{G}_{x}^{2}(n,m,l) \\\\\n% % +\\mathbf{G}_{y}^{2}(n,m,l)+\\mathbf{G}_{z}^{2}(n,m,l)\\Big)\n% % \\end{multline}\n% % where $\\mathbf{G}_{x}$, $\\mathbf{G}_{y}$ and $\\mathbf{G}_{z}$ are respectively the gradients of the horizontal, vertical, and axial directions, computed using Sobel operators \\cite{sobel1972camera}. \n\n% \\subsection{Experimental Results}\n% We evaluate the proposed method by 2D face registration and 3D brain MRI registration. For each task, we quantify the performance of registration accuracy and the image sharpness by their corresponding metrics. Across all of these metrics, we find that ABN achieves higher registration accuracy and image sharpness than the state-of-the-art methods. \n\n"
                    }
                },
                "subsection 3.4": {
                    "name": "Experimental Results",
                    "content": "\nWe compare our ERNet with the baseline models regarding extraction and registration accuracy. For each task, we quantify the performance by its corresponding dice score. We also report the running time for each method to complete both tasks. Across all these metrics, we show that ERNet not only consistently achieves better extraction and registration performance than other alternatives, but is also more time-efficient.\n\n% \\subsubsection{Experiment Setting.}\n% We split the datasets into training and test sets as described in the Datasets section. \n% For 2D face registration, models are trained with batch size of 16 for 5$k$ epochs. \n% For 3D brain MRI registration, we reduce the batch size to 1 to address GPU memory limitation and models were trained for 1$k$ epochs. \n% For both tasks, models are optimized using Adam optimizer~\\cite{kingma2014adam} with a learning rate of $1 \\times 10^{-4}$. \n% The ratios of regularization in Eq.~\\eqref{eq:loss} is set to $\\lambda=10$.\n% For more details, please refer to supplementary material. The source code is available at \\url{https://github.com/anonymous3214/ABN}.\n% % Implementation details and code are available in the supplementary material.\n% % All models were implemented using PyTorch with the code available at \n\n",
                    "subsubsection 3.4.1": {
                        "name": "Experiment Setting.",
                        "content": "\n% We split the datasets into training sets and test sets as described in Section~\\ref{section:Dataset}. \n% Note that the IBSR dataset is used for testing only. We train the deep neural networks using the machine learning framework PyTorch.\n% Each training batch consists of one pair of images to address GPU memory limitation and models are optimized using Adam optimizer~\\cite{kingma2014adam} with a learning rate of $1 \\times 10^{-6}$. \n% We leverage image augmentation technique during training to expand the datasets, where a small random transformation with translation, rotation, and scaling is applied to images. For more details, please refer to supplementary material. \n% The source code is available at \nWe split the datasets into training, validation, and test sets as described in Appendix~\\ref{sec:appendix}. \nNote that the IBSR dataset is used for test only. \nThe training set is used to learn model parameters and the validation set is used to evaluate the performance of hyperparameter settings (e.g., the number of stages or smoothing regularization term). \nWe use the test set only once to report the final evaluation results for each model.\n%Each training batch consists of one pair of images to address GPU memory limitation and models are optimized using Adam optimizer~\\cite{kingma2014adam} with a learning rate of $1 \\times 10^{-6}$. \n% We leverage the image augmentation technique during training to expand the datasets, where a small random transformation with translation, rotation, and scaling is applied to images.\n%For more details, please refer to supplementary material. \n%The source code is available at \n%\\url{https://github.com/ERNetERNet/ERNet}.\n\n\n% Implementation details and code are available in the supplementary material.\n% All models were implemented using PyTorch with the code available at \n\n% described in Section \\ref{section:Compared Methods}.\n%\\input{tab_2d}\n%\\input{fig_res_2d}\n% \\vspace{-5}\n"
                    },
                    "subsubsection 3.4.2": {
                        "name": "Extraction and Registration Results.",
                        "content": "\nTable~\\ref{tab:main res} summarized the results of designed two-stage pipelines and proposed ERNet in both extraction and registration tasks.\nBased on the global competition in three datasets, ERNet outperforms the existing methods in all metrics.\nFor the extraction task, the performance of ERNet is superior to that of the other compared methods, especially on the CC359 dataset.\nSpecifically, we observed a gain in extraction dice score up to $10.5\\%$ compared to the best extraction method 3dSkullStrip.\nBesides, ERNet is more robust in the extraction task than other alternatives, as it performs consistently well and obtains the smallest standard deviation across all datasets.\n\nWhen observing registration performance, once again, ERNet outperforms all other methods across all datasets.\nMost notably, we find that the registration result of almost every method is bounded by the result of its corresponding extraction method.\n% , except for one datapoint outlier that occurred when testing the combination of BET$^{*}$ and CRN in the CC359 dataset. Even in the outlier case, the registration result is only slightly better than the extraction result.\nThis proves that the accuracy of extraction has a significant impact on the quality of the subsequent registration task.\nERNet captures this property to deliver an improved result via end-to-end collective learning.\nIn addition, the overall performance of ERNet w/o Ext, the variant of ERNet, is slightly worse than that of ERNet, indicating that the extraction network in ERNet is crucial and beneficial to improve both extraction and registration accuracy.  \n\n"
                    },
                    "subsubsection 3.4.3": {
                        "name": "Running Efficiency.",
                        "content": "\nTo evaluate the efficiency of ERNet, we compare its running time with other baselines. The run time is measured on the same machine with a Intel$^{\\circledR}$ Xeon$^{\\circledR}$ E5-2667 v4 CPU and an NVIDIA Tesla V100 GPU.\nAs shown in Table~\\ref{tab:main res}, ERNet is roughly 20 to 200 times faster than existing methods.\nThis is because ERNet can perform both extraction and registration tasks end-to-end on the same device efficiently.\nIn contrast, other alternatives handle these two tasks separately using two-step approaches, which is time-consuming.\nNo GPU implementations of BET, 3dSkullStrip, BSE, FLIRT and ANTs are made available~\\cite{smith2002fast,cox1996afni,shattuck2002brainsuite,jenkinson2001global,avants2009advanced}.\n\n"
                    },
                    "subsubsection 3.4.4": {
                        "name": "Qualitative Analysis.",
                        "content": "\nFigure~\\ref{fig:main result} shows visualized brain extraction and registration results of our ERNet compared with other two-step approaches on the LPBA40 test set.\nUpon inspection, we can see that extracted image of ERNet is more accurate than those of BET, 3dSkullStrip, and BSE.\nThe brain mask predicted by ERNet overlaps best with the ground truth mask of the source image, while the masks predicted by other extraction methods contain obvious non-brain tissues.\nIn terms of registration results, ERNet also clearly outperforms other compared methods. \nThe final registered image of ERNet is more similar to the target image than that of alternatives. \nMost notably, inaccurate extraction results with non-brain tissue also appear in the following registration results and hamper the final performance. \nThis supports our claim that a failed extraction can propagate to the following registration task, rendering an irreversible error.\nFurthermore, we demonstrate the intermediate extraction and registration results of ERNet in Figure~\\ref{fig:stage result} using a test sample of CC359 dataset.\nIt is clear that brain tissue is progressively extracted from the source image with the help of a multi-stage design. Likewise, the extracted image is transformed multiple times to align with the target image incrementally.\n\n%%%%% No time %%%%%%%%%%%\n\n    % \\begin{figure}[t]\n%     \\centering\n%     \\subfigure[Extraction Accuracy]{\n%         \\includegraphics[width=0.227\\textwidth]{fig/stage_extraction.pdf}\n%         \\label{fig:3D Dice}\n%     }\n%     \\subfigure[Registration Accuracy]{\n%         \\includegraphics[width=0.227\\textwidth]{fig/stage_registration.pdf}\n%         \\label{fig:3D Jaccard}\n%     }\n\n%     \\vspace{-10pt}\n%     \\caption{Performance of ERNet with different number of extraction and registration stages.}\n%     \\label{fig:stage dice}\n%     \\vspace{-10pt}\n% \\end{figure}\n\n% \\begin{figure}[t]\n%     \\centering\n%     \\subfigure[Extraction Accuracy]{\n% \\begin{tikzpicture}\n% \\begin{axis}[\n%     % inner sep=0,\n%     outer sep=0,\n%     width=0.57\\columnwidth,\n%     height=0.42\\columnwidth,\n%     xlabel={Regularization Parameter $\\lambda$},\n% x label style={at={(axis description cs:0.5,0.19)},anchor=north},\n% y label style={at={(axis description cs:0.27,.5)},anchor=south},\n%     % xlabel near ticks,\n%     % ylabel near ticks,\n%     ylabel={Dice (Extraction)},\n%     xmin=-0.5, xmax=10.5,\n%     ymin=0.912, ymax=0.947,\n%     xtick={0,2,4,6,8,10},\n%     ytick={0.915,0.920,0.925,0.930,0.935,0.940,0.945},\n%     ymajorgrids=true,\n%     grid style=dashed,\n%     tick label style={font=\\tiny},\n%     label style={font=\\tiny},\n% ]\n\n% \\addplot[\n%     dashed,\n%     color=red,\n%     mark=o,\n%     mark options=solid,\n%     ]\n%     coordinates {\n%     (0,0.9322)(0.1,0.9354)(0.3,0.9392)(0.5,0.9419)(1.0,0.9458)(3.0,0.9401)(8.0,0.9308)(10.0,0.9139)\n%     };\n\n% \\end{axis}\n% \\end{tikzpicture}\n%         % \\label{fig:3D Dice}\n%     }\n%     \\subfigure[Registration Accuracy]{\n% \\begin{tikzpicture}\n% \\begin{axis}[\n%     % inner sep=0,\n%     outer sep=0,\n%     width=0.57\\columnwidth,\n%     height=0.42\\columnwidth,\n%     xlabel={Regularization Parameter $\\lambda$},\n% x label style={at={(axis description cs:0.5,0.19)},anchor=north},\n% y label style={at={(axis description cs:0.27,.5)},anchor=south},\n%     % xlabel near ticks,\n%     % ylabel near ticks,\n%     ylabel={Dice (Extraction)},\n%     xmin=-0.5, xmax=10.5,\n%     ymin=0.912, ymax=0.947,\n%     xtick={0,2,4,6,8,10},\n%     ytick={0.915,0.920,0.925,0.930,0.935,0.940,0.945},\n%     ymajorgrids=true,\n%     grid style=dashed,\n%     tick label style={font=\\tiny},\n%     label style={font=\\tiny},\n% ]\n\n% \\addplot[\n%     dashed,\n%     color=red,\n%     mark=o,\n%     mark options=solid,\n%     ]\n%     coordinates {\n%     (0,0.9322)(0.1,0.9354)(0.3,0.9392)(0.5,0.9419)(1.0,0.9458)(3.0,0.9401)(8.0,0.9308)(10.0,0.9139)\n%     };\n\n% \\end{axis}\n% \\end{tikzpicture}\n%         % \\label{fig:3D Jaccard}\n%     }\n\n%     \\vspace{-10pt}\n%     \\caption{Performance of ERNet with different number of extraction and registration stages.}\n%     % \\label{fig:stage dice}\n%     \\vspace{-10pt}\n% \\end{figure}\n\n\n\n\n"
                    },
                    "subsubsection 3.4.5": {
                        "name": "Influence of Parameters.",
                        "content": "\nWe study three important hyperparameters of our ERNet, \\ie the number of stages for extraction and registration, the value of mask smoothing regularization parameter~$\\lambda$ and the value of Sigmoid function slope parameter~$\\gamma$.\n\n\\noindent\\textbf{Number of stages of extraction and registration.} In our multi-stage design, the number of stages corresponds to network depth and the number of repeated extraction and registration.\nIn other words, more stages mean more refinements of the stripping and alignment, which is usually beneficial to the improvement of the extraction and registration accuracy.\nAs shown in Table~\\ref{tab: stage res}, the ablation study demonstrates that both the extraction network and the registration network of ERNet are essential, and removing either one of them causes invalid results.\nAs illustrated in Figure~\\ref{fig:stage dice}, we vary the number of stages of extraction and registration to learn their effects.\nThe results indicate that the performance of extraction and alignment improves with additional stages.\nThis supports the idea that a multi-stage design yields improved overall performance.\n\n\\noindent\\textbf{Mask smoothing parameter $\\lambda$.} As mentioned in Section~\\ref{section:end-to-end training}, we introduce an regularization term to smooth the predicted masks.\nTo show the effectiveness of the smoothing regularization, we vary different values of the smoothing parameter $\\lambda$ as shown in Figure~\\ref{fig:lambda}(a).\nThe optimal Dice score occurs when $\\lambda$ =1, while the performance gets worse as the $\\lambda$ increases more or decreases.\nThis indicates that our ERNet benefits from mask smoothing regularization. \n\n\\noindent\\textbf{Sigmoid function slope parameter $\\gamma$.} To show the effectiveness of the shifted sigmoid function we introduced in Eq.~(\\ref{eq:sigmoid}), we evaluate the performance of our model under different $\\gamma$ settings in a range from $10^{-1}$ to $10^3$. As shown in Figure~\\ref{fig:lambda}(b), the model achieves the best performance when $\\gamma$ = $10^{1}$, which is better than using the standard sigmoid function ($\\gamma$ = $10^{0}$). As the $\\gamma$ increases to $10^{2}$ and $10^{3}$, the dice score drops significantly because the large flat region of the sigmoid function prevents the error from backpropagation.\n\n\n\n\n\n\n\n\n\n\n\n% \\subsubsection{Extraction Results.}\n% % \\hphantom{1}\n% % \\newline\n% %\\indent \n% Table~\\ref{tab:2D res} summarizes the results of four methods on 2D face registration task. It can be seen that ABN clearly outperforms all the baselines in terms of both registration accuracy (SSMI and CC) and image sharpness (SMD and Tenengrad). We observed a gain in SSMI of roughly $3.8\\%$ and in SDM up to $9.9\\%$ compared to the best baseline method CRN.\n% Notably, the registration accuracy of ABN-L is lower than that of ABN since ABN-L only contains a long-term memory network, which is insufficient for obtaining a high quality registration result.\n% % Compared to the best performing baseline CRN, ABN improves the registration accuracy by $3.8\\%$ in SSMI and increases the image sharpness by $9.9\\%$ in SMD. \n% % CRN achieves slightly better registration accuracy relative to our another proposed method ABN-L. However, ABN-L performs significantly better in preserving image sharpness than CRN.\n\n%\\input{fig_stage_res}\n%\\input{fig_face_stage_v2}\n\n% Figure \\ref{fig:face result} shows visual comparisons of 2D face registration results. Upon inspection, we can see that the final warped image of ABN is more similar to the target image than those of VM, CRN and ABN-L. Most notably, in terms of sharpness, ABN-L and ABN clearly outperform CRN. VM does not appear to have a loss in sharpness, but it is significantly worse than other methods in registration accuracy. Furthermore, we also compare the intermediate warped results of our method with other multi-stage method (i.e., CRN), as shown in Figure \\ref{fig:face stage}. It is clear that both methods allow the warped/registered images to progressively align to the target image with the help of a multi-stage fashion. However, the registration accuracy improvement of CRN is accompanied by a loss of image sharpness, while ABN gives much sharper images than CRN.\n\n% \\subsubsection{Results on 3D Brain MRI Registration.}\n% %\\hphantom{1}\n% %\\newline\n% %\\indent \n% Table \\ref{tab:3D res} shows the results of nine methods on two respective datasets: Mindboggle101 and LPBA40.\n% % The performances of Affine, BSpline, Demons, Elastic, SyN, VM and CRN are compared with our methods - ABN-L and ABN.\n% % The run time is measured on the same device with a Intel$^{\\circledR}$ Xeon$^{\\circledR}$ E5-2667 v4 CPU and a NVIDIA Tesla V100 GPU. No GPU implementations of BSpline, Demons, Elastic and SyN have been found in previous works \\cite{yaniv2018simpleitk,avants2009advanced,bajcsy1989multiresolution,avants2008symmetric,balakrishnan2018unsupervised,zhao2019recursive}.\n% In this task, Dice score and Jaccard are used to evaluate registration accuracy, while SMD and Tenengrad are used to assess sharpness performance.\n% The results of each method are shown with its average performance with standard deviation and its ranking among all other methods.\n% Based on the global competition in both datasets, the average ranking of ABN outperforms that of all baselines. \n% Specifically, ABN achieves superior performance against all compared methods on registration accuracy, while achieving competitive results in image sharpness persevering.\n% We observe that almost all multi-stage methods outperform the single-stage methods on registration accuracy, but only ABN-L and ABN are on par with single-stage methods in terms of image sharpness persevering.\n% Again, the overall performance of ABN-L was slightly inferior to that of ABN, indicating that the short-term registration network is crucial and beneficial to improve registration accuracy.\n% In order to evaluate our proposed methods, we compared them with several competitive approaches: Affine, BSpline, Demons, Elastic and SyN for traditional iteration-based methods, VM and CRN for deep learning based approaches. \n% In this task, we used Dice score and Jaccard coefficient to assess the performance of the registration accuracy. The run time is measured on the same device with a Intel$^{\\circledR}$ Xeon$^{\\circledR}$ E5-2667 v4 CPU and a NVIDIA Tesla V100 GPU. No GPU implementations of BSpline, Demons, Elastic and SyN have been found in previous works \\cite{yaniv2018simpleitk,avants2009advanced,bajcsy1989multiresolution,avants2008symmetric,balakrishnan2018unsupervised,zhao2019recursive}.\n%\\input{fig_res_3d}\n%\\input{tab_3d_v4}\n%\\input{fig_para_3d}\n\n% As shown in Table \\ref{tab:3D res}, again, ABN achieves image sharpness on par with the best method (VM) and the highest registration accuracy than any other alternative state-of-the-art method. Specifically, compared to the best performing learning-based method CRN, ABN improves the registration accuracy by $4.3\\%$ in Dice score and increases the image sharpness by $20\\%$ in SMD. In this task, the other method proposed by us, ABN-L also outperforms all of the baselines in terms of registration accuracy, and has a close performance to VM in sharpness preservation. VM has achieved the best result for image sharpness preservation, but is inferior to our proposed methods in registration accuracy. For traditional iteration-based methods, Elastic achieves a result closest to ABN-L, but proves to be much more time-consuming. Though Affine produces competitive result in image sharpness preservation, it is significantly worse in registration accuracy.\n\n% Figure \\ref{fig:brain result} shows the visual comparisons of one instance of the 3D brain MRI registration task on LPBA40 dataset. We can observe that our ABN method is more accurate in aligning the source image to the target image. Notably, although CRN has an impressive performance in registration accuracy, the warped registered image is obviously blurred, which can be verified with the focused example of the anatomical structure. The Affine only performs a global linear transformation, so its performance is significantly worse than other nonlinear-based registration methods in terms of registration accuracy. These results indicate that our proposed method effectively achieves a higher level of registration accuracy while preserving image sharpness.\n\n% \\subsubsection{Influence of Parameters.}\n% % In this subsection, we study the performance of all multi-stage methods with different stage numbers in 3D brain MRI registration. \n% We study the performance of all multi-stage methods at different stage number settings using the LPBA40 dataset.\n% For multi-stage methods, the number of stages corresponds to the depth of the network and the number of warping operations. In other words, more stages mean more refinements of the alignment, which is usually beneficial to the improvement of registration accuracy. However, a larger number of stages also consumes more GPU memory and increase the computation time. To investigate the influence of different stage settings, we compared 10 different versions of the CRN, ABN-L, and ABN, with the number of stages varies from 1 to 10.\n% \\indent For the 2D face registration task, as shown in Figure \\ref{fig:2D SSMI} and Figure \\ref{fig:2D CC}, ABN achieves the highest registration accuracy, especially when the number of stages is large, e.g., greater than 5, ABN outperforms all other methods. When assessing sharpness of registered images, it can be seen from Figure \\ref{fig:2D SMD} and Figure \\ref{fig:2D Tenengrad} that ABN and ABN-L consistently superior to CRN at any stage. We found that the image sharpness of CRN was consistent with the increase of the number of stages in this task. The reason is that CRN always perform the transformation from previous registered image. When the image is distorted in the previous stage, it can cause irreparable damage in the subsequent stages. In this case, further stages no longer affect the performance even the number of stages increased, resulting in registration accuracy and image sharpness remaining constant. \n% % A visualization of the example results can be found in the appendix.\n\n% As illustrated in Figure \\ref{fig:3D Dice}, both ABN-L and ABN significantly outperform CRN in the registration accuracy as the number of stages increases. In addition, ABN-L and ABN consistently preserve the image sharpness, as observed in Figures \\ref{fig:3D SMD} and \\ref{fig:3D Tenengrad}. Notably, in this task, the effectiveness of CRN in both registration accuracy and image sharpness decreases significantly when the number of stages increases. This supports our intuition that recursively deforming the image obtained\n% from the previous stage will result in image blurring and signal attenuation, which causes the registration accuracy to further deteriorate. Thus, our methods ABN-L and ABN outperform CRN in the multi-stage case in both registration accuracy and sharpness preserving.\n% %-----------------------------------------------\n% % Related Work Section\n% \\section{Related Work}\n% % We review the existing work in two aspects: traditional optimization-based and deep learning-based registration.\n% %This section briefly reviews the previous works related to our proposed method from two aspects.\n% This section briefly reviews the previous works for image registration. \n% %\\textbf{Traditional Optimization-based Registration} \n% Traditional image registration methods \\cite{avants2009advanced, bajcsy1989multiresolution, avants2008symmetric, jenkinson2001global, saad2009new, schnabel2001generic} optimal similarity between images by iteratively updating transformation parameters. The goal of this optimization is to ensure that the source image and the target image can be aligned to the greatest extent while ensuring the rationality of transformation. To this end, different similarity measures have been proposed for various registration scenarios. For example, Mean Square Error (MSE) on intensity differences is commonly used for image pairs with a similar intensity distribution. However, Normalized Cross-Correlation (NCC) and Mutual Information (MI) usually perform better when it comes to multi-modal registration, e.g., brain CT/MRI image registration for tumor localization. Furthermore, it is necessary to smooth the deformation field to avoid folds in the transformation \\cite{rueckert1999nonrigid, rohlfing2003volume, staring2007rigidity}. Unfortunately, the optimization of these traditional registration methods is usually performed in a high-dimensional parameter space, which is computationally expensive and time-consuming, limiting their uses in clinical workflows.\n\n% %\\textbf{Deep Learning-based Registration}\n% To address the potential limitations of traditional image registration, deep learning-based methods \\cite{balakrishnan2018unsupervised,balakrishnan2019voxelmorph,de2017end,zhao2019recursive,de2019deep,yang2017quicksilver,cao2017deformable,sokooti2017nonrigid,krebs2017robust} are being studied more and more extensively in medical image registration. Among them, supervised learning methods depend on the ground truth of transformation mappings. \n% % The registration performance is directly linked to the quality of ground truth. \n% In fact, the acquisition of high-quality ground truth in medical practices is often considerately expensive. As a result, the ground truth of supervised learning mainly coming from traditional methods \\cite{yang2017quicksilver, cao2017deformable} or synthesis \\cite{sokooti2017nonrigid, krebs2017robust, \n% dai2020dual}. To overcoming this limitation, unsupervised learning approaches have recently gained more attention, which learns image deformation by maximizing the similarity between the warped image and the target image. Benefiting from the success of STN \\cite{jaderberg2015spatial}, the gradient can be successfully back-propagated during the training. Several studies registered images by a single-stage design \\cite{balakrishnan2018unsupervised,balakrishnan2019voxelmorph,de2017end,li2017non}. \n% % For instance, VoxelMorph \\cite{balakrishnan2018unsupervised, balakrishnan2019voxelmorph} extracts image features based on the U-Net \\cite{isola2017image, ronneberger2015u} structure and directly predicts a deformation field. \n% Nonetheless, it is difficult to achieve desirable registration results with this schema, especially when there are complex and large distortions between images. \n% Recently, multi-stage methods \\cite{zhao2019recursive, de2019deep} have been proposed to improve the registration accuracy - they decompose a complex transformation step-by-step, recursively warp the source image until that image is well-aligned with the target image. \n% However, the above multi-stage registration methods are devoted to improving registration accuracy despite the exposure to information retention failure. \n% In this work, we pursue registration accuracy and information retention simultaneously.\n% \\begin{figure}[t]\n%   \\centering\n%   \\includegraphics[width=0.75\\linewidth]{fig/regularization_lambda_resized.pdf}\n%   \\vspace{-10pt}\n%   \\caption{Effect of varying the mask smoothing regularization parameter $\\lambda$ on Dice score of extraction task.}\n%   \\label{fig:lambda}\n%   \\vspace{-10pt}\n% \\end{figure}\n\n\n% \\begin{figure}[t]\n%   \\centering\n% \\begin{tikzpicture}\n% \\begin{axis}[\n%     width=0.84\\columnwidth,\n%     height=0.56\\columnwidth,\n%     xlabel={Regularization Parameter $\\lambda$},\n%     ylabel={Dice (Extraction)},\n%     xmin=-0.5, xmax=10.5,\n%     ymin=0.912, ymax=0.947,\n%     xtick={0,2,4,6,8,10},\n%     ytick={0.915,0.920,0.925,0.930,0.935,0.940,0.945},\n%     yticklabels={0.915,0.920,0.925,0.930,0.935,0.940,0.945},\n%     ymajorgrids=true,\n%     grid style=dashed,\n%     tick label style={font=\\small},\n%     label style={font=\\normalsize},\n% ]\n\n% \\addplot[\n%     dashed,\n%     color=red,\n%     mark=o,\n%     mark options=solid,\n%     ]\n%     coordinates {\n%     (0,0.9322)(0.1,0.9354)(0.3,0.9392)(0.5,0.9419)(1.0,0.9458)(3.0,0.9401)(8.0,0.9308)(10.0,0.9139)\n%     };\n\n% \\end{axis}\n% \\end{tikzpicture}\n%   \\vspace{-10pt}\n%   \\caption{Effect of varying the mask smoothing regularization parameter $\\lambda$ on Dice score of extraction task.}\n% %   \\label{fig:lambda}\n%   \\vspace{-10pt}\n% %\\label{fig:lambda}\n% \\end{figure}\n"
                    }
                }
            },
            "section 4": {
                "name": "Related Work",
                "content": "\n\\label{sec:related}\n\\noindent \\textbf{Brain extraction.} Over the past decade, myriad methods have been proposed, emphasizing the importance of the brain extraction problem.\n% In general, brain regions manually outlined by human experts in raw MRI images are considered as ground truth; however, this is time-consuming, label-intensive, and impractical to perform on a large scale.\nSmith et al.~\\cite{smith2002fast} proposed a deformable model to fit the brain surface using a locally adaptive set model.\n3dSkullStrip~\\cite{cox1996afni} is a modified version of~\\cite{smith2002fast}, which uses points lying outside the brain surface to guide the evolution of the mesh.\nShattuck et al.~\\cite{shattuck2002brainsuite} employs anisotropic diffusion filtering and a 2D Marr Hildreth edge detector to identify the brain boundary.\n% Apart from methods, several other traditional approaches~\\cite{segonne2004hybrid,iglesias2011robust,eskildsen2012beast,lutkenhoff2014optimized} are also commonly used for brain extraction.\nApart from methods, several other traditional approaches~\\cite{segonne2004hybrid,iglesias2011robust,eskildsen2012beast} are also commonly used for brain extraction.\nHowever, the aforementioned methods rely heavily on parameter setting and manual quality control, which are time-consuming and labor-intensive. \nRecently, deep learning-based methods have been introduced for brain extraction due to their superior capability and extreme speed.\nKleesiek et al.~\\cite{kleesiek2016deep} proposed a voxel-wise 3D CNN for skull stripping.\n%Lucena et al.~\\cite{lucena2019convolutional} introduced data augmentation and used silver standard masks to make the network robust.\nHwang et al.~\\cite{hwang20193d} suggested that 3D-UNet yields highly competitive results for skull stripping. \n%Most recently, Zhong et al.~\\cite{zhong2021dika} used Dual Self-Attention Module (DSAM) to fuse the age-specific intensity information and domain-invariant prior knowledge for guided skull stripping. \nThe above learning-based methods often demand a large amount of adequately labeled data for effective training.\nHowever, neuroimage datasets are usually small and expensive to annotate.\n\n\\noindent \\textbf{Image registration.}\n%Traditional image registration methods~\\cite{avants2009advanced, bajcsy1989multiresolution, avants2008symmetric, jenkinson2001global, saad2009new, schnabel2001generic} often try to maximize the similarity between images by iteratively optimizing the transformation parameters, where mean square error (MSE), normalized cross-correlation (NCC) and mutual information (MI), \\etc, are commonly used as intensity-based similarity measures.\nTraditional image registration methods~\\cite{avants2009advanced, avants2008symmetric, jenkinson2001global, saad2009new} often try to maximize the similarity between images by iteratively optimizing the transformation parameters, where normalized cross-correlation\n(NCC) and mutual information (MI), \\etc, are commonly used as intensity-based similarity measures.\nHowever, iteratively optimizing each pair of images tends to face the drawbacks of high computational cost and being trapped in local optima, resulting in failing to yield an efficient and robust registration result.\nRecently, many deep learning-based methods have been proposed due to their superior computational efficiency and registration performance.\nSokooti et al.~\\cite{sokooti2017nonrigid} proposed a multi-scale 3D CNN termed as RegNet to learn the artificially generated displacement vector field (DVF) for 3D chest CT registration.\n%Dai et al.~\\cite{dai2020dual} introduced dual-attention recurrent networks for neuroimaging data registration. \nWhile these methods present competitive results, they are all supervised.\nIn other words, the training procedure is guided by ground truth transformations. In practice, obtaining high-quality ground truth is often expensive in medical imaging.\nTo address this limitation, unsupervised registration methods~\\cite{balakrishnan2018unsupervised,zhao2019recursive} received much attention and delivered promising results.\nHowever, the above methods rely on accurate skull stripping results to perform the registration, where manual visual inspection is required to remove the inaccurate extracted image.\nSuch human involvement is not only time-consuming but also brings in biases.\nUnlike these works, we pursue unsupervised joint learning for extraction and registration.\n% %-----------------------------------------------\n% % Conclusion Section\n"
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\\label{sec:con} \n\n% In this paper, we studied the problem of \\emph{Resource-efficient Multi-task Learning} (MLT) with the key goal of designing a resource-friendly MLT model. \n% Our work is primarily motivated by two aspects. \n% First, in many real-world applications, there is an increasing demand of running MTL models on resource-constrained devices such as mobile or IoT devices. \n% Second, existing state-of-the-art MTL models often overlook the resource requirement during inference phase.  \n% To increase the \n% To satisfy the practical deployment requirements of low resource consumption and high accuracy (even with limited training dataset), we proposed a novel solution called \\sysname for fine-grained parameter sharing.\n% In a nutshell, \\sysname can \\emph{learn} how to share parameters directly on training data (akin to soft-sharing) while only consuming a constant computational cost per task (similar to hard-sharing). \n% In other words, \\sysname distills the benefits from both hard-sharing and soft-sharing approaches and enables fine-grained filter sharing among any number of task-specific CNNs of different architectures. \n% Our extensive evaluations on two datasets including the popular CLEVR showed that \\sysname achieves comparable or better accuracy than state-of-the-art soft-sharing approaches on several multi-task problems, while only consuming a fraction of resources. Furthermore, our ablation study showed that \\sysname can effectively learn the policy and that unlike existing MTL sharing (hard and soft) approaches, \\sysname is not subject to the accuracy degradation from the widely used batch normalization technique. \n\n\n\n\n\n% In this paper, we formulated the resource-constrained MTL as a filter sharing problem. To solve this problem, we proposed a novel architecture named \\sysname to learn to share filters across the task-specific CNNs of any number of tasks in any architecture. The results of extensive experiments show the \\sysname can achieve comparable high performance as soft sharing and be resources-efficient as hard sharing. What's more, we find the widely used BN trick may cause the current MTL sharing methods failed in a more general setting of tasks.  \n\n\n\n% In this paper, two novel models, ABN and ABN-L, were proposed for unsupervised deformable image registration in a multi-stage setting to achieve a high level of accuracy and information completeness. We identified a new criterion to evaluate the performance of image registration, which is the sharpness of the registered image.  Our methods learn a global transformation at each stage which allows transformations to be directly performed on moving source images and preserve the sharpness. Experimental results demonstrated that ABN and ABN-L consistently output accurate and clear registered images for both 2D face registration and 3D brain MRI registration tasks. \n\n% This paper presents a novel multi-stage neural network method called ABN for anti-blur deformable image registration.\n% %Different from previous works in deformable image registration, our proposed methods resolve the contradiction between registration accuracy and sharpness preservation by progressively learning a deformation at each stage. \n% Different from previous works, our proposed method can accurately register images nonlinearly while preserving the image sharpness. \n% Specifically, ABN improves the registration accuracy incrementally by a multi-stage design. \n% At the same time, a combined deformation of all previous stages is learned simultaneously. \n% This deformation is applied directly to the source image to preserve the image sharpness, avoiding information loss caused by multiple interpolations. \n% %Compared to all other state-of-the-art methods, experimental results demonstrated that ABN consistently generates a comparatively high level of accurate and sharp registered images in both 2D face registration and 3D brain MRI registration tasks.\n% Experimental results demonstrated that ABN consistently generates a comparatively high level of accurate and sharp registered images in both 2D face registration and 3D brain MRI registration tasks.\n\nIn this paper, we propose a novel unified end-to-end framework, called ERNet, for unsupervised collective extraction and registration.\nDifferent from previous work, our proposed method seamlessly integrated two tasks into one system to achieve joint optimization.\nSpecifically, ERNet contains a pair of multi-stage extraction and registration modules.\nThese two modules help each other boost extraction and registration performance simultaneously without any annotation information.\nMoreover, the multi-stage design allows each task to proceed incrementally, thus refining their respective performance to a better extent.\nThe experimental results demonstrate that ERNet not only outperforms state-of-the-art approaches in both extraction and registration accuracy but is also more robust and time-efficient.\n\n"
            },
            "section 6": {
                "name": "Acknowledgments",
                "content": "\n\\label{sec:ack}\nLifang He was supported by Lehigh's accelerator grant S00010293.\n%\\newpage\n% balance the two columns of the last page\n%\\balance\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{08_reference}\n\n%\\newpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Appendix for reproducibility",
                "content": "\n\\label{sec:appendix}\n\n%This section provides more details on dataset preprocessing, ERNet settings and baseline settings to support the reproducibility of the results in this paper. We have released our code and data publicly available at~\\url{https://github.com/ERNetERNet/ERNet}.\n\nThis section provides more details to support the reproducibility of the results in this paper. We have released our code and data publicly available at~\\url{https://github.com/ERNetERNet/ERNet}.\n\n",
                "subsection 7.1": {
                    "name": "Details of Data Preprocessing",
                    "content": "\nWe evaluate the effectiveness of our proposed method on three different public brain MRI datasets, LPBA40, CC-359 and IBSR. \n%Each dataset is split into training, validation and test sets, respectively. We apply intensity normalization to each scan and rescale voxel values to a range between zero and one.\nTable~\\ref{tab:dataset} summarizes the properties of the datasets.\n\n\n\n\\noindent\\textbullet\\ \\textit{LONI Probabilistic Brain Atlas (LPBA40)}~\\cite{shattuck2008construction}: \nThis dataset consists of 40 raw T1-weighted 3D brain MRI scans (40 different patients) along with their brain masks and the corresponding segmentation ground truth of 56 anatomical structures. The brain mask and anatomical segmentations are used to evaluate the accuracy of extraction and registration, respectively.\nSame to~\\cite{balakrishnan2018unsupervised, zhao2019recursive}, we focus on atlas-based registration, where the first scan is the target image and the remaining scans need to align with it. Among the 39 scans, we use 30, 5, and 4 scans for training, validation, and test, respectively. All scans are resized to $96\\times96\\times96$ after cropping.\n\n\\noindent\\textbullet\\ \\textit{Calgary-Campinas-359 (CC-359)}~\\cite{souza2018open}: \nThis dataset consists of 359 raw T1-weighted 3D brain MRI scans (359 different patients) and the corresponding brain masks. It also contains the labeled white matter as the ground truth. We use the brain masks and white-matter masks to evaluate the accuracy of extraction and registration, respectively. Same to LPBA40, we concentrate on atlas-based registration and split CC359 into 298, 30, and 30 scans for training, validation, and test sets. All scans are resized to $96\\times96\\times96$ after cropping.\n\n\\noindent\\textbullet\\ \\textit{Internet Brain Segmentation Repository (IBSR)} \\cite{rohlfing2011image}: \nThis dataset provides 18 raw T1-weighted 3D brain MRI scans (18 different patients) along with the corresponding segmentation results. \nWe merge all segmentation results to construct the brain mask. \nDue to the small sample size, We use this dataset only to test the model trained on CC359. \n%Similarly, we follow atlas-based registration that all 18 scans need to align with the first scan of CC359. All scans are resized to $96\\times96\\times96$ after cropping.\nThus, all 18 scans need to align with the first scan of CC359. All scans are resized to $96\\times96\\times96$ after cropping.\n\n"
                },
                "subsection 7.2": {
                    "name": "Details Settings of ERNet",
                    "content": "\n\\textbf{Training settings of ERNet.} Our experiments are running on Red Hat Enterprise Linux 7.3 with an Intel$^{\\circledR}$ Xeon$^{\\circledR}$ E5-2667 v4 CPU and an NVIDIA Tesla V100 GPU. \n%The code released is in Python 3.7.6 and the implementation of neural networks is based on PyTorch 1.7.1. We also employ Numpy 1.21.6, SimpleITK 2.0.2 and Nibabel 3.1.1 in the implementation. \nThe implementation of neural networks is based on PyTorch 1.7.1.\nDuring the training process, we apply batch gradient descent with each training batch consists of one pair of images to address GPU memory limitation. Models are optimized using Adam optimizer~\\cite{kingma2014adam} with a learning rate of $1 \\times 10^{-6}$. We also leverage the image augmentation technique to expand the datasets, where a transformation with random translation, rotation, and scaling is applied to source images. We detail it in Table~\\ref{tab:transformation}. \n\n\n\n%%%%% No time %%%%%%%%%%%\n\n    \n\\noindent\\textbf{Parameters settings of ERNet.} Since we design a multi-stage extraction and registration network, the extraction and registration stages are both set to 5 in this work. The best mask smoothing parameter $\\lambda$ in Eq.~(\\ref{eq:loss}) is 1, and the best sigmoid function slope parameter $\\gamma$ in Eq.~(\\ref{eq:sigmoid}) is $10^{1}$.\n%The extraction network takes a 3D U-Net as the backbone with an input size of $2\\times96\\times96\\times96$ and an output size of $1\\times96\\times96\\times96$.\nThe extraction network contains 10 convolutional layers with 16, 32, 32, 64, 64, 64, 32, 32, 32 and 16 filters. \nThe registration network adopt 3D CNNs and fully-connected layers to map the input to the dimension of $1\\times12$. It contains 6 convolutional layers with 16, 32, 64, 128, 256 and 512 filters. The output dimensions of the 2 fully-connected layers are 128 and 12.\n\n\\newcommand{\\code}[1]{\\texttt{#1}}\n\n"
                },
                "subsection 7.3": {
                    "name": "Settings of Baselines",
                    "content": "\n\n\\noindent\\textbf{Brain Extraction Tool (BET)} \\cite{smith2002fast}: This is a skull stripping method included in FSL package. It uses a deformable approach to fit the brain surface by applying locally adaptive set models. The command we use for BET is \\code{bet <input> <output> -f 0.5 -g 0 -m}, where \\texttt{f} and \\texttt{g} are fractional intensity threshold and gradient in fractional intensity threshold, respectively. We set them to default values.\n\n\\noindent\\textbf{3dSkullStrip}~\\cite{cox1996afni}: This is a modified version of BET that is included in the AFNI package. It performs skull stripping based on the expansion paradigm of the spherical surface. The command we use for 3dSkullStrip is \\code{3dSkullStrip -input <input>  -prefix <output> -mask\\_vol -fac 1000}. \\code{fac} is set to the default value.\n\n\\noindent\\textbf{Brain Surface Extractor (BSE)}~\\cite{shattuck2002brainsuite}: It extracts the brain region based on morphological operations and edge detection, which employs anisotropic diffusion filtering and a Marr Hildreth edge detector for brain boundary identification. The command we use for BSE is \\code{bse -i <input> -o <output> --mask <mask> -p --trim --auto --timer }. Hyperparameters are set to default values.\n\n\\noindent\\textbf{FMRIB's Linear Image Registration Tool (FLIRT)}~\\cite{jenkinson2001global}: This is a fully automated affine brain image registration tool in FSL package. The command we use for FLIRT is \\code{flirt -in <source> -ref <target> -out <output> -omat <output parameter> -bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 -searchrz -90 90 -dof 12  -interp trilinear}.\n\n\\noindent\\textbf{Advanced Normalization Tools (ANTs)} \\cite{avants2009advanced}: It is a state-of-the-art medical image registration toolkit. Here we utilize affine transformation model and cross-correlation metric for registration. \n\n\\noindent\\textbf{VoxelMorph (VM)} \\cite{balakrishnan2018unsupervised}: This unsupervised, deformable image registration method employs a neural network to predict the nonlinear transformation between images. \nFor network architectures, we use the latest version, VoxelMorph-2, and configure 10 convolutional layers with 16, 32, 32, 64, 64, 64, 32, 32, 32 and 16 filters. \n%The kernel size of each convolutional layer is 3 × 3.\nThe ratio of deformation regularization is set to 10.\n\n\\noindent\\textbf{Cascaded Registration Networks (CRN)} \\cite{zhao2019recursive}:\nIt is an unsupervised multi-stage registration method. In different stages, the source image is repeatedly deformed to align with a target image. Same to ERNet, the number of stages is set to 5. In each stage, we configure 10 convolutional layers with 16, 32, 32, 64, 64, 64, 32, 32, 32 and 16 filters. \n%The kernel size of each convolutional layer is 3 × 3. \nThe ratio of deformation regularization is set to 10.\n\n\\noindent\\textbf{ERNet w/o Ext}: This is a variant of ERNet where we remove the extraction modules. Here it is a registration method only.\n\n"
                }
            }
        },
        "tables": {
            "tab:methods": "\\begin{table}[t]\n    \\centering\n    \\caption{Summary of compared methods.}\n    \\label{tab:methods}\n    \\vspace{-10pt}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{lcccc}\n    \\toprule\n    \\textbf{Methods}& \\textbf{Extraction}& \\textbf{Registration}& \\textbf{Collaborative}& \\textbf{Deep learning}\\\\\n    \\midrule\n    BET { \\cite{smith2002fast}} & \\cmark  & \\xmark & \\xmark & \\xmark \\\\\n    % BET$^*$ { \\cite{smith2002fast}} & \\cmark  & \\xmark & \\cmark & \\xmark\\\\\n    3dSkullStrip \\cite{cox1996afni}& \\cmark  & \\xmark & \\xmark & \\xmark\\\\\n    BSE \\cite{shattuck2002brainsuite}& \\cmark  & \\xmark & \\xmark & \\xmark\\\\\n    \\midrule\n    FLIRT \\cite{jenkinson2001global} & \\xmark  & \\cmark & \\xmark & \\xmark\\\\\n    ANTs \\cite{avants2009advanced}& \\xmark  & \\cmark & \\xmark & \\xmark\\\\\n    VM \\cite{balakrishnan2018unsupervised}& \\xmark  & \\cmark & \\xmark & \\cmark\\\\\n    CRN \\cite{zhao2019recursive}& \\xmark  & \\cmark & \\xmark & \\cmark \\\\\n    \\midrule\n    ERNet w/o Ext& \\xmark  & \\cmark & \\xmark & \\cmark\\\\\n    ERNet (ours)& \\cmark  & \\cmark & \\cmark & \\cmark\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-15pt}\n\\end{table}",
            "tab:main res": "\\begin{table*}[t]\n    \\centering\n    \\caption{Results for brain extraction and registration in different datasets. The results are reported as performance(mean $\\pm$ std ) of extraction and registration of each compared method. The running time is reported as the average processing time for each image in its corresponding task.\n    “$\\uparrow$” point out “the larger the better” and “$\\downarrow$” point out “the smaller the better”.}\n    \\label{tab:main res}\n    \\vspace{-10pt}\n    \\resizebox{1.0\\linewidth}{!}{\n    \\begin{tabular}{lcccccccrc}\n    \\toprule\n    \\multicolumn{2}{c}{\\multirow{2}{*}{Methods}}                & \\multicolumn{6}{c}{Datasets}                                                        & \\multicolumn{2}{c}{\\multirow{2}{*}{Running Time}} \\\\\n    \\cmidrule(lr){3-8}\n    \n    \\multicolumn{2}{c}{}                                        & \\multicolumn{2}{c}{LPBA40} & \\multicolumn{2}{c}{CC359} & \\multicolumn{2}{c}{IBSR}  & \\multicolumn{2}{c}{}                              \\\\\n    \n    \\cmidrule(lr){1-2}\\cmidrule(lr){3-4}\\cmidrule(lr){5-6}\\cmidrule(lr){7-8}\\cmidrule(lr){9-10}\n    \n    \\multirow{2}{*}{Extraction} & \\multirow{2}{*}{Registration} & Extraction  & Registration & Extraction & Registration & Extraction & Registration & Extraction             & Registration             \\\\\n    \n    \n                                &                               & Dice$_{\\mathrm{ext}}$  $\\uparrow$        & Dice$_{\\mathrm{reg}}$ $\\uparrow$         & Dice$_{\\mathrm{ext}}$ $\\uparrow$       & Dice$_{\\mathrm{reg}}$ $\\uparrow$         & Dice$_{\\mathrm{ext}}$ $\\uparrow$       & Dice$_{\\mathrm{reg}}$ $\\uparrow$         & Sec $\\downarrow$                    & Sec $\\downarrow$ \\\\\n                                \n                                \n    \\midrule\n    % BET \\cite{smith2002fast} & FLIRT \\cite{jenkinson2001global} & 0.865 $\\pm$ 0.106 & 0.455 $\\pm$ 0.300 & 0.751 $\\pm$ 0.140 & 0.709 $\\pm$ 0.096 & 0.904 $\\pm$ 0.054 & 0.790 $\\pm$ 0.034 & 2.86 (CPU) & 4.06 (CPU)\\\\\n    \n    BET \\cite{smith2002fast} & FLIRT \\cite{jenkinson2001global} & 0.935 $\\pm$ 0.028 & 0.606 $\\pm$ 0.026 & 0.811 $\\pm$ 0.087 & 0.747 $\\pm$ 0.060 & 0.911 $\\pm$ 0.038 & 0.798 $\\pm$ 0.010 & 2.45 (CPU) & 4.57 (CPU)\\\\\n    \n    3dSkullStrip \\cite{cox1996afni} & FLIRT \\cite{jenkinson2001global}\n    & 0.902 $\\pm$ 0.032 & 0.594 $\\pm$ 0.018 & 0.849 $\\pm$ 0.037 & 0.790 $\\pm$ 0.034 & 0.869 $\\pm$ 0.039 & 0.787 $\\pm$ 0.020 & 178.56 (CPU) & 4.64 (CPU)\\\\\n    \n    BSE \\cite{shattuck2002brainsuite} & FLIRT \\cite{jenkinson2001global}\n    & 0.938 $\\pm$ 0.022 & 0.614 $\\pm$ 0.010 & 0.846 $\\pm$ 0.112 & 0.801 $\\pm$ 0.021 & 0.873 $\\pm$ 0.064 & 0.798 $\\pm$ 0.015 & 4.75 (CPU) & 4.35 (CPU) \\\\\n    \n    \\midrule\n    \n    % BET \\cite{smith2002fast} & ANTs \\cite{avants2009advanced} & 0.865 $\\pm$ 0.106 & 0.617 $\\pm$ 0.013 & 0.751 $\\pm$ 0.140 & 0.737 $\\pm$ 0.094 & 0.904 $\\pm$ 0.054 & 0.796 $\\pm$ 0.026 & 2.86 (CPU) & 2.96 (CPU)\\\\\n    \n    BET \\cite{smith2002fast} & ANTs \\cite{avants2009advanced} & 0.935 $\\pm$ 0.028 & 0.609 $\\pm$ 0.025 & 0.811 $\\pm$ 0.087 & 0.764 $\\pm$ 0.053 & 0.911 $\\pm$ 0.038 & 0.796 $\\pm$ 0.014 & 2.45 (CPU)& 2.76 (CPU)\\\\\n    \n    3dSkullStrip \\cite{cox1996afni} & ANTs \\cite{avants2009advanced} & 0.902 $\\pm$ 0.032 & 0.616 $\\pm$ 0.016 & 0.849 $\\pm$ 0.037 & 0.807 $\\pm$ 0.027 & 0.869 $\\pm$ 0.039 & 0.794 $\\pm$ 0.017 & 178.56 (CPU) & 2.89 (CPU)\\\\\n    \n    BSE \\cite{shattuck2002brainsuite} & ANTs \\cite{avants2009advanced} & 0.938 $\\pm$ 0.022 & 0.616 $\\pm$ 0.013 & 0.846 $\\pm$ 0.112 & 0.796 $\\pm$ 0.027 & 0.873 $\\pm$ 0.064 & 0.797 $\\pm$ 0.017 & 4.75 (CPU) & 2.52 (CPU)\\\\\n    \n    \\midrule\n    \n    BET \\cite{smith2002fast} & VM \\cite{balakrishnan2018unsupervised}\n    & 0.935 $\\pm$ 0.028 & 0.488 $\\pm$ 0.092 & 0.811 $\\pm$ 0.087 & 0.811 $\\pm$ 0.015 & 0.911 $\\pm$ 0.038 & 0.792 $\\pm$ 0.010 & 2.45 (CPU) & 0.02 (GPU)\\\\\n    \n    3dSkullStrip \\cite{cox1996afni} & VM \\cite{balakrishnan2018unsupervised}\n    & 0.902 $\\pm$ 0.032 & 0.479 $\\pm$ 0.094 & 0.849 $\\pm$ 0.037 & 0.809 $\\pm$ 0.018 & 0.869 $\\pm$ 0.039 & 0.785 $\\pm$ 0.014 & 178.56 (CPU) & 0.02 (GPU) \\\\\n    \n    BSE \\cite{shattuck2002brainsuite} & VM \\cite{balakrishnan2018unsupervised}\n    & 0.938 $\\pm$ 0.022 & 0.512 $\\pm$ 0.065 & 0.846 $\\pm$ 0.112 & 0.810 $\\pm$ 0.017 & 0.873 $\\pm$ 0.064 & 0.794 $\\pm$ 0.011 & 4.75 (CPU) & 0.02 (GPU)\\\\\n    \n    \\midrule\n    \n    BET \\cite{smith2002fast} & CRN \\cite{zhao2019recursive}\n    & 0.935 $\\pm$ 0.028 & 0.556 $\\pm$ 0.046 & 0.811 $\\pm$ 0.087 & 0.815 $\\pm$ 0.008 & 0.911 $\\pm$ 0.038 & 0.799 $\\pm$ 0.017 & 2.45 (CPU) & 0.10 (GPU) \\\\\n    \n    3dSkullStrip \\cite{cox1996afni} & CRN \\cite{zhao2019recursive}\n    & 0.902 $\\pm$ 0.032 & 0.528 $\\pm$ 0.056 & 0.849 $\\pm$ 0.037 & 0.813 $\\pm$ 0.009 & 0.869 $\\pm$ 0.039 & 0.796 $\\pm$ 0.014 & 178.56 (CPU) & 0.10 (GPU)\\\\\n    \n    BSE \\cite{shattuck2002brainsuite} & CRN \\cite{zhao2019recursive}\n    & 0.938 $\\pm$ 0.022 & 0.547 $\\pm$ 0.071 & 0.846 $\\pm$ 0.112 & 0.812 $\\pm$ 0.011 & 0.873 $\\pm$ 0.064 & 0.799 $\\pm$ 0.011 & 4.75 (CPU) & 0.10 (GPU)\\\\\n    \n    \\midrule\n    \n    BET \\cite{smith2002fast} & \\textbf{ERNet} (w/o Ext)\n    & 0.935 $\\pm$ 0.028 & 0.616 $\\pm$ 0.021 & 0.811 $\\pm$ 0.087 & 0.804 $\\pm$ 0.017 & 0.911 $\\pm$ 0.038 & 0.798 $\\pm$ 0.011 & 2.45 (CPU) & 0.04 (GPU)\\\\\n    \n    3dSkullStrip \\cite{cox1996afni} & \\textbf{ERNet} (w/o Ext)\n    & 0.902 $\\pm$ 0.032 & 0.606 $\\pm$ 0.006 & 0.849 $\\pm$ 0.037 & 0.815 $\\pm$ 0.007 & 0.869 $\\pm$ 0.039 & 0.791 $\\pm$ 0.014 & 178.56 (CPU) & 0.04 (GPU)\\\\\n    \n    BSE \\cite{shattuck2002brainsuite} & \\textbf{ERNet} (w/o Ext)\n    & 0.938 $\\pm$ 0.022 & 0.613 $\\pm$ 0.012 & 0.846 $\\pm$ 0.112 & 0.807 $\\pm$ 0.018 & 0.873 $\\pm$ 0.064 & 0.795 $\\pm$ 0.013 & 4.75 (CPU) & 0.04 (GPU)\\\\\n    \n    \\midrule\n    \n    \\multicolumn{2}{c}{\\textbf{ERNet (ours)}} & \\textbf {0.946 $\\pm$ 0.009} & \\textbf{0.626 $\\pm$ 0.008} & \\textbf{0.938 $\\pm$ 0.008} & \\textbf{0.818 $\\pm$ 0.006} & \\textbf{0.916 $\\pm$ 0.013} & \\textbf{0.800 $\\pm$ 0.011} & \\multicolumn{2}{c}{\\textbf{0.11 (GPU)}}\\\\\n    \n    \\bottomrule             \n    \\end{tabular}}\n    \\vspace{-10pt}\n\\end{table*}",
            "tab: stage res": "\\begin{table}[t]\n    \\centering\n    \\caption{Influence of number of stages on extraction and registration performance. “$\\uparrow$” point out “the larger the better”.}\n    \\label{tab: stage res}\n    \\vspace{-10pt}\n    \\resizebox{0.85\\linewidth}{!}{\n    \\begin{tabular}{cccc}\n    \\toprule\n    \n    \\multicolumn{2}{c}{Number of Stages} & Extraction & Registration \\\\\n    \n    \\cmidrule(lr){1-2}\n    \n    Extraction       & Registration      & Dice$_{\\mathrm{ext}}$ $\\uparrow$       & Dice$_{\\mathrm{reg}}$ $\\uparrow$         \\\\\n\n\n    \\midrule\n    0 & 0 & 0.216 $\\pm$ 0.018 & 0.252 $\\pm$ 0.158\\\\\n    \n    0 & 1 & 0.216 $\\pm$ 0.018 & 0.269 $\\pm$ 0.101\\\\ \n    \n    0 & 5 & 0.216 $\\pm$ 0.018 & 0.264 $\\pm$ 0.103\\\\ \n    \n    1 & 0 & 0.040 $\\pm$ 0.026 & 0.007 $\\pm$ 0.007\\\\ \n    \n    1 & 1 & 0.902 $\\pm$ 0.010 & 0.566 $\\pm$ 0.040\\\\\n    \n    1 & 5 & 0.927 $\\pm$ 0.007 & 0.604 $\\pm$ 0.017\\\\\n    \n    5 & 0 & 0.095 $\\pm$ 0.040 & 0.024 $\\pm$ 0.016\\\\ \n    \n    5 & 1 & 0.919 $\\pm$ 0.010 & 0.550 $\\pm$ 0.033\\\\\n    \n     \\textbf{5} &  \\textbf{5} &  \\textbf{0.946 $\\pm$ 0.009} &  \\textbf{0.626 $\\pm$ 0.008}\\\\\n    \n    \\bottomrule\n    \\end{tabular}}\n    \\vspace{-10pt}\n\\end{table}",
            "tab:dataset": "\\begin{table}[h]\n    \\centering\n    \\vspace{-8pt}\n    \\caption{Summary of datasets}\n    \\label{tab:dataset}\n    \\vspace{-9pt}\n    \\resizebox{0.88\\linewidth}{!}{\n    \\begin{tabular}{lccc}\n    \\toprule\n     & \\textbf{LPBA40}& \\textbf{CC359 }& \\textbf{IBSR}\\\\\n    \\midrule\n    Raw size  & $256\\times124\\times256$ & $171\\times256\\times256$ & $256\\times256\\times128$\\\\\n    Cropped size  & $96\\times96\\times96$ & $96\\times96\\times96$ & $96\\times96\\times96$\\\\\n    Training & 30  & 298 & -\\\\\n    Validation & 5  & 30 & -\\\\\n    Test & 4  & 30 & 18\\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\vspace{-8pt}\n\\end{table}",
            "tab:transformation": "\\begin{table}[h]\n    \\centering\n    \\vspace{-2pt}\n    \\caption{Range of random transformation.}\n    \\label{tab:transformation}\n    \\vspace{-8pt}\n    \\resizebox{0.68\\linewidth}{!}{\n    \\begin{tabular}{lccc}\n    \\toprule\n    \n    \\multicolumn{1}{c}{\\multirow{3}{*}{Datasets}} &   \\multicolumn{3}{c}{Transformation} \\\\\n    \n    \n    \n    \n    \\cmidrule(lr){2-4}\n    \n        &   Translation  & Rotation       & Scale         \\\\\n        &   (Voxels)  & (Degree)  & (Times)    \\\\\n\n\n    \\midrule\n    LPBA40 & $\\pm$ 5 & $\\pm$ 5  & 0.98 $\\sim$ 1.02\\\\\n    \n    CC359 & $\\pm$ 3 & $\\pm$ 3  & 0.99 $\\sim$ 1.01\\\\\n\n    \n    \\bottomrule\n    \\end{tabular}}\n    \\vspace{-13pt}\n\\end{table}"
        },
        "figures": {
            "fig:intro": "\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{fig/figure_1_v4.pdf}\n  \\vspace{-10pt}\n  \\caption{\n  %An example of the anti-blur deformable image registration problem. \nThe problem of unsupervised collective extraction and registration in neuroimaging data. (a) Given a raw scan of a patient's head (the source image) and a template image of standard brain region (the target image), the goal is to extract the brain region from the source image, and transform it to align with the target image. Neither the extraction label (\\textit{i.e.}, the brain region in the source image) nor the registration label (\\textit{i.e.}, the transformation required to align the source with the target) is available.\n Examples of different possible results are shown in (b).\n  The bottom-right box is the ideal result: the correct region of the brain in the source is extracted and is well-aligned with the target.\n  %The quadrant chart demonstrates the performance evaluation of the task.\n  %Ideally, the brain region should be well-extracted while aligning with the target image, as shown in the bottom right of the chart.\n  % achieve clearness and alignment, as shown in the bottom right of the registered image matrix.\n  }\n  \\label{fig:intro}\n  \\vspace{-10pt}\n\\end{figure*}",
            "fig: family": "\\begin{figure}[t]\n    \\centering\n    \\subfigure[\\textbf{Supervised  extraction\n    \\cite{kleesiek2016deep,lucena2019convolutional} + supervised registration \\cite{sokooti2017nonrigid, dai2020dual} }]{\n        \\includegraphics[width=0.98\\linewidth]{fig/family_1_v3.pdf}\n        \\label{fig:family 1}\n    }\n\n    \\subfigure[\\textbf{\n        %Pipeline-based Extraction \\cite{balakrishnan2018unsupervised} + Unsupervised Registration \\cite{balakrishnan2018unsupervised}\n        Unsupervised extraction \\cite{smith2002fast,cox1996afni,shattuck2002brainsuite,segonne2004hybrid} + unsupervised registration \\cite{balakrishnan2018unsupervised,zhao2019recursive}\n        }]{\n        \\includegraphics[width=0.98\\linewidth]{fig/family_2_v3.pdf}\n        \\label{fig:family 2}\n    }\n    \\subfigure[\\textbf{Unsupervised collective extraction and registration (ours)}]{\n        \\includegraphics[width=0.98\\linewidth]{fig/family_3_v3.pdf}\n        \\label{fig:family 3}\n        \n    }\n    \\vspace{-10pt}\n    \\caption{\n    %Comparing different solutions to the brain extraction and registration problem.\n    Related works in brain extraction and registration.\n    }\n    \\label{fig: family}\n    \\vspace{-20pt}\n\\end{figure}",
            "fig:formulation": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{fig/function.pdf}\n  \\vspace{-10pt}\n  \\caption{A demonstration of extraction and registration functions.}\n  \\label{fig:formulation}\n  \\vspace{-12pt}\n\\end{figure}",
            "fig:network": "\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.98\\linewidth]{fig/network_1_v6.pdf}\n    \\vspace{-10pt}\n  \\caption{An overview of ERNet for collective brain extration and registration. \\textcolor{myorange}{Multi-stage extraction module:} In each extraction stage $j$, the \\emph{ Extraction Network} $f_{e}$ predicts the current mask $\\mathbf{M}^{j}$ from previous extracted image $\\mathbf{E}^{j-1}$; the \\emph{Overlay Layer} (OL) performs an element-wise product between the previous extracted image $\\mathbf{E}^{j-1}$ and the current mask $\\mathbf{M}^{j}$ to generate the current extracted image $\\mathbf{E}^{j}$. The final extracted image is $\\mathbf{E}^{M}$. \\textcolor{myviolet}{Multi-stage registration module:} In each registration stage $k$, the \\emph{Registration Network} $f_{r}$ predicts the current affine transformation $\\mathbf{A}_\\text{i}^{k}$ between the previous warped image $\\mathbf{W}^{k-1}$ and target image $\\mathbf{T}$; the \\emph{Composition Layer} (COMP) fuses current affine transformation $\\mathbf{A}_\\text{i}^{k}$ and previous combined affine transformation $\\mathbf{A}_\\text{c}^{k-1}$ to generate the updated combined affine transformation $\\mathbf{A}_\\text{c}^{k}$; the \\emph{Spatial Transformation Layer} (STL) performs the transformation $\\mathbf{A}_\\text{c}^{k}$ on the final extracted image $\\mathbf{E}^{M}$ to produce the warped image $\\mathbf{W}^{k}$. The final output image is $\\mathbf{W}^{N}$.\n    }\n  \\label{fig:network}\n  \\vspace{-10pt}\n\\end{figure*}",
            "fig:main result": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{fig/main_result_v4.pdf}\n  \\vspace{-15pt}\n  \\caption{Visual comparisons for brain extraction and registration task. We render a 3D visualization of the image and display middle slice in three different planes: sagittal, axial and coronal. The right side contains the source and target images and their corresponding ground truth labels. We show extraction and registration results of each method and its corresponding predictive labels used for performance evaluations. To evaluate the brain extraction task, a predicted brain \\textbf{Mask} {\\color{red}(red)} should coincide as much as possible with the ground truth brain \\textbf{Mask} {\\color{red}(red)} of the source image. Likewise, in the brain registration task, a warped \\textbf{W}hite \\textbf{M}atter {\\color{\n green}(green)} should well-overlap with the \\textbf{W}hite \\textbf{M}atter {\\color{\n green}(green)} of the target image.}\n  \\label{fig:main result}\n  \\vspace{-20pt}\n\\end{figure}",
            "fig:stage result": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{fig/stage_result_v1.pdf}\n  \\vspace{-15pt}\n  \\caption{A demonstration of the multi-stage extraction and registration process. The example shows the extraction and registration result at each stage using an ERNet with 5-stage extraction and 5-stage registration.}\n  \\label{fig:stage result}\n  \\vspace{-20pt}\n\\end{figure}",
            "fig:stage dice": "\\begin{figure}[t]\n\\begin{tikzpicture}\n\\begin{axis}[\n    % inner sep=0,\n    title = {(a) Extraction Accuracy},\n    outer sep=0,\n    width=0.57\\columnwidth,\n    height=0.42\\columnwidth,\n    xlabel={Number of stages: (Extraction, Registration)},\nx label style={at={(axis description cs:0.5,0.17)},anchor=north},\ny label style={at={(axis description cs:0.254,.5)},anchor=south},\n    % xlabel near ticks,\n    % ylabel near ticks,\n    ylabel={Dice (Extraction)},\n    xmin=-0.3, xmax=6.3,\n    ymin=0.9, ymax=0.950,\n    xtick={0,1,2,3,4,5, 6},\n    ytick={0.905,0.915,0.925,0.935,0.945},\n    yticklabels={0.905,0.915,0.925,0.935,0.945},\n    xticklabels={(1,1), (2,2), (3,3), (4,4), (5,5), (6,6), (7,7)},\n    ymajorgrids=true,\n    grid style=dashed,\n    tick label style={font=\\tiny},\n    label style={font=\\tiny},\n    title style={font=\\footnotesize,align=center, yshift=-0.37\\columnwidth},\n]\n\n\\addplot[\n    dashed,\n    color=red,\n    mark=diamond,\n    mark options=solid,\n    ]\n    coordinates {\n    (0,0.9024)(1,0.9181)(2,0.9218)(3,0.9367)(4,0.9458)(5,0.9441)(6,0.9445)\n    };\n\n\\end{axis}\n\\end{tikzpicture}\n\\begin{tikzpicture}\n\\begin{axis}[\n    % inner sep=0,\n    title = {(b) Registration Accuracy},\n    outer sep=0,\n    width=0.57\\columnwidth,\n    height=0.42\\columnwidth,\n    xlabel={Number of stages: (Extraction, Registration)},\nx label style={at={(axis description cs:0.5,0.17)},anchor=north},\ny label style={at={(axis description cs:0.254,.5)},anchor=south},\n    % xlabel near ticks,\n    % ylabel near ticks,\n    ylabel={Dice (Registration)},\n    xmin=-0.3, xmax=6.30,\n    ymin=0.5626, ymax=0.630,\n    xtick={0,1,2,3,4,5,6},\n    ytick={0.565,0.575, 0.585, 0.595, 0.605, 0.615, 0.625},\n    yticklabels={0.565, 0.575, 0.585, 0.595, 0.605, 0.615, 0.625},\n    xticklabels={(1,1), (2,2), (3,3), (4,4), (5,5), (6,6), (7,7)},\n    ymajorgrids=true,\n    grid style=dashed,\n    tick label style={font=\\tiny},\n    label style={font=\\tiny},\n    title style={font=\\footnotesize,align=center, yshift=-0.37\\columnwidth},\n]\n\n\\addplot[\n    dashed,\n    color=blue,\n    mark=triangle,\n    mark options=solid,\n    ]\n    coordinates {\n    (0,0.5656)(1,0.5835)(2,0.5841)(3,0.6141)(4,0.6261)(5,0.6263)(6,0.6262)\n    };\n\n\\end{axis}\n\\end{tikzpicture}\n    \\vspace{-11pt}\n    \\caption{Performance of ERNet with different number of extraction and registration stages.}\n    \\label{fig:stage dice}\n    \\vspace{-11pt}\n\\end{figure}",
            "fig:lambda": "\\begin{figure}[t]\n\\begin{tikzpicture}\n\\begin{axis}[\n    % inner sep=0,\n    title = {(a) Varying of $\\lambda$},\n    outer sep=0,\n    width=0.57\\columnwidth,\n    height=0.42\\columnwidth,\n    xlabel={Regularization Parameter $\\lambda$},\nx label style={at={(axis description cs:0.5,0.16)},anchor=north},\ny label style={at={(axis description cs:0.254,.5)},anchor=south},\n    % xlabel near ticks,\n    % ylabel near ticks,\n    ylabel={Dice (Extraction)},\n    xmin=-0.5, xmax=10.5,\n    ymin=0.910, ymax=0.950,\n    xtick={0,2,4,6,8,10},\n    ytick={0.915, 0.920, 0.925, 0.930, 0.935,0.940, 0.945},\n    yticklabels={0.915, 0.920, 0.925, 0.930, 0.935,0.940, 0.945},\n    ymajorgrids=true,\n    grid style=dashed,\n    tick label style={font=\\tiny},\n    label style={font=\\tiny},\n    title style={font=\\footnotesize,align=center, yshift=-0.37\\columnwidth},\n]\n\n\\addplot[\n    dashed,\n    color=red,\n    mark=diamond,\n    mark options=solid,\n    ]\n    coordinates {\n    (0,0.9322)(0.1,0.9354)(0.3,0.9392)(0.5,0.9419)(1.0,0.9458)(3.0,0.9401)(8.0,0.9308)(10.0,0.9139)\n    };\n\n\\end{axis}\n\\end{tikzpicture}\n\\begin{tikzpicture}\n\\begin{semilogxaxis}[\n    %log ticks with fixed point,\n    % extra x tick style={\n    % log identify minor tick positions=true,\n    % },\n    % inner sep=0,\n    title = {(b) Varying of $\\gamma$},\n    outer sep=0,\n    width=0.57\\columnwidth,\n    height=0.42\\columnwidth,\n    xlabel={Sigmoid function slope parameter $\\gamma$},\nx label style={at={(axis description cs:0.5,0.16)},anchor=north},\ny label style={at={(axis description cs:0.254,.5)},anchor=south},\n    ylabel={Dice (Extraction)},\n    xmin=0.09, xmax=1100,\n    ymin=0.910, ymax=0.950,\n    xtick={0.1, 1, 10, 100, 1000},\n    ytick={0.915, 0.920, 0.925, 0.930, 0.935,0.940, 0.945},\n    yticklabels={0.915, 0.920, 0.925, 0.930, 0.935,0.940, 0.945},\n    ymajorgrids=true,\n    grid style=dashed,\n    tick label style={font=\\tiny},\n    label style={font=\\tiny},\n    title style={font=\\footnotesize,align=center, yshift=-0.37\\columnwidth},\n]\n\n\\addplot[\n    dashed,\n    color=red,\n    mark=diamond,\n    mark options=solid,\n    ]\n    coordinates {\n    (0.1,0.9265)(1,0.9370)(5,0.9449)(10,0.9458)(100,0.9416)(200,0.9384)(500,0.9349)(1000,0.9103)\n    };\n\n\\end{semilogxaxis}\n\\end{tikzpicture}\n    \\vspace{-12pt}\n     \\caption{Effect of varying the mask smoothing regularization parameter $\\lambda$ and sigmoid function slope parameter $\\gamma$.}\n    \\label{fig:lambda}\n    \\vspace{-12pt}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{equ:voxel_value}\n\\mathbf{W}_{xyz} = \\mathbf{E}_{x'y'z'},\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\begin{bmatrix}\nx'\\\\\ny'\\\\\nz' \\\\\n1\n\\end{bmatrix} = \\mathbf{A}\\begin{bmatrix}\nx\\\\\ny\\\\\nz \\\\\n1\n\\end{bmatrix} = \\begin{bmatrix}\na_{1} & a_{2} & a_{3} & a_{4}\\\\\na_{5} & a_{6} & a_{7} & a_{8}\\\\\na_{9} & a_{10} & a_{11} & a_{12}\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx\\\\\ny\\\\\nz \\\\\n1\n\\end{bmatrix}.\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\begin{split}\n\\label{equ:goal_training}\n\\theta^{*},\\phi^{*} &=\\underset{\\theta,  \\phi}{\\arg \\min } \\hspace{-3pt} \\sum_{\\left(\\mathbf{S},  \\mathbf{T}\\right)\\in \\mathcal{D}}\\left[ \\mathcal{L} \\left( \\hat{\\mathbf{W}}, \\mathbf{T}  \\right)\\right] \\\\\n& = \\underset{\\theta, \\phi}{\\arg \\min } \\hspace{-3pt}  \\sum_{\\left(\\mathbf{S}, \\mathbf{T}\\right)\\in \\mathcal{D}}\\left[ \\mathcal{L} \\left( \\mathcal{T} \\left( f_{\\theta}(\\mathbf{S}) \\circ \\mathbf{S},\\hspace{5pt} g_{\\phi} \\left(  f_{\\theta}(\\mathbf{S}) \\circ \\mathbf{S}, \\mathbf{T}  \\right)\\right),   \\mathbf{T}    \\right)\\right],\n\\end{split}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n H(x) = \\begin{cases}\n        1, & \\text{if } x > 0, \\\\\n        0, & \\text{otherwise}.\n    \\end{cases}\n\\end{equation}",
            "eq:5": "\\begin{equation}\nS(x) = \\frac{1}{1 +e^{-\\gamma x}}\n\\label{eq:sigmoid}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\mathbf{M}^{j}=f_{e}\\left(\\mathbf{E}^{j-1}\\right),\n\\end{equation}",
            "eq:7": "\\begin{equation*}\n    \\mathbf{E}^{j} = \\mathbf{E}^{j-1}\\circ \\mathbf{M}^{j}\n\\end{equation*}",
            "eq:8": "\\begin{equation}\n\\mathbf{A}_\\text{i}^{k}=f_{r}\\left(\\mathbf{W}^{k-1}, \\mathbf{T}\\right),\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\mathbf{A}_\\text{c}^{k}=\\mathbf{A}_\\text{i}^{k} \\cdot \\mathbf{A}_\\text{c}^{k-1},\n\\end{equation}",
            "eq:10": "\\begin{equation}\n     \\mathbf{W}_{xyz}^{k} = \\mathbf{E}_{x'y'z'}^{M} \\hspace{5pt},\n     \\label{equ:voxel_value_k}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n\\begin{split}\n \\mathbf{W}_{xyz}^{k}= \\sum_{o=1}^{W} \\sum_{p=1}^{H}  \\sum_{q=1}^{D} \\mathbf{E}_{opq}^{M} &\\cdot  \n\\max(0,1-|x' -o|) \\\\ \n \\cdot  \\max (0,1-|y' - p|) & \\cdot \\max (0,1-|z' - q|).\n\\end{split}\n\\end{equation}",
            "eq:12": "\\begin{equation}\n\\label{eq:loss}\n\\underset{\\mathbf{M}^{1}, \\cdots, ~\\mathbf{M}^{M}, ~\\mathbf{A}_\\text{c}^{N}}{\\min} \\mathcal{L}_{\\operatorname{sim}} \\left(\\mathbf{W}^{N}, \\mathbf{T}\\right)+ \\sum_{j=1}^{M}\\lambda \\mathcal{R}(\\mathbf{M}^{j}),\n\\end{equation}",
            "eq:13": "\\begin{equation}\n\\mathcal{R}(\\mathbf{M}^{j})=\\sum_{x=1}^{W} \\sum_{y=1}^{H}  \\sum_{z=1}^{D}\\|\\nabla \\mathbf{M}_{xyz}^{j}\\|^{2}.\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\text{Dice}_{\\mathrm{ext}}=2 \\cdot \\frac{|\\mathbf{\\hat{M}} \\cap \\mathbf{M}|}{|\\mathbf{\\hat{M}}|+|\\mathbf{M}|},\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\text{Dice}_{\\mathrm{reg}}=2 \\cdot \\frac{|\\mathbf{G}_\\text{w} \\cap \\mathbf{G}_\\text{t}|}{|\\mathbf{G}_\\text{w}|+|\\mathbf{G}_\\text{t}|}\n\\end{equation}"
        },
        "git_link": "https://github.com/ERNetERNet/ERNet"
    }
}