{
    "meta_info": {
        "title": "TransBO: Hyperparameter Optimization via Two-Phase Transfer Learning",
        "abstract": "With the extensive applications of machine learning models, automatic\nhyperparameter optimization (HPO) has become increasingly important. Motivated\nby the tuning behaviors of human experts, it is intuitive to leverage auxiliary\nknowledge from past HPO tasks to accelerate the current HPO task. In this\npaper, we propose TransBO, a novel two-phase transfer learning framework for\nHPO, which can deal with the complementary nature among source tasks and\ndynamics during knowledge aggregation issues simultaneously. This framework\nextracts and aggregates source and target knowledge jointly and adaptively,\nwhere the weights can be learned in a principled manner. The extensive\nexperiments, including static and dynamic transfer learning settings and neural\narchitecture search, demonstrate the superiority of TransBO over the\nstate-of-the-arts.",
        "author": "Yang Li, Yu Shen, Huaijun Jiang, Wentao Zhang, Zhi Yang, Ce Zhang, Bin Cui",
        "link": "http://arxiv.org/abs/2206.02663v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "9 pages and 2 extra pages of appendix"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nMachine learning (ML) models have been extensively applied in many fields such as recommendation, computer vision, financial market analysis, etc~\\cite{hinton2012deep,he2016deep,goodfellow2016deep,he2017neural,devlin2018bert,henrique2019literature}.\nHowever, the performance of ML models heavily depends on the choice of hyperparameter configurations (e.g., learning rate or the number of hidden layers in a deep neural network).\nAs a result, automatically tuning the hyperparameters has attracted lots of interest from both academia and industry~\\cite{quanming2018taking}.\nBayesian optimization (BO) is one of the most prevailing frameworks for automatic hyperparameter optimization (HPO)~\\cite{hutter2011sequential, bergstra2011algorithms, snoek2012practical}.\nThe main idea of BO is to use a surrogate model, typically a Gaussian Process (GP)~\\cite{rasmussen2004gaussian}, to describe the relationship between a hyperparameter configuration and its performance (e.g., validation error), and then utilize this surrogate to determine the next configuration to evaluate by optimizing an acquisition function that balances exploration and exploitation. \n\nHyperparameter optimization (HPO) is often a computationally-intensive process as one often needs to choose and evaluate hyperparameter configurations by training and validating the corresponding ML models.\nHowever, for ML models that are computationally expensive to train (e.g., deep learning models or models trained on large-scale datasets), vanilla Bayesian optimization (BO) suffers from the low-efficiency issue~\\cite{falkner2018bohb,li-mfeshb,li2021volcanoml} due to insufficient configuration evaluations within a limited budget.\n\n{\\bf (Opportunities) }\nProduction ML models usually need to be constantly re-tuned as new task / dataset comes or underlying code bases are updated, e.g., in the AutoML applications. \nThe optimal hyperparameters may also change as the data and code change, and so should be frequently re-optimized. \nAlthough they may change significantly, the region of good or bad configurations may still share some correlation with those of previous tasks~\\cite{yogatama2014efficient}, and this provides the opportunities towards a faster hyperparameter search.\nTherefore, \\textit{we can leverage the tuning results (i.e., observations) from previous HPO tasks (source tasks) to speed up the current HPO task (target task) via a transfer learning-based framework.}\n\n\n{\\bf (Challenges) }\nThe transfer learning for HPO consists of two key \noperations: \\emph{extracting} source knowledge from previous HPO \ntasks, and \\emph{aggregating} and \\emph{transfering}\nthese knowledge to a target domain.\nTo fully unleash the potential of TL, we need to address two main challenges when performing the above operations:\n1) {\\em The Complementary Nature among Source Tasks. }\nDifferent source tasks are often complementary and thus require us to treat them in a joint and cooperative manner.\nIgnoring the synergy of multiple source tasks might lead to the loss of auxiliary knowledge.\n2) {\\em Dynamics during Knowledge Aggregation. }\nAt the beginning of HPO, the knowledge from the source tasks could bring benefits due to the scarcity of observations on the target task. \nHowever, as the tuning process proceeds, \n%the learning\nwe should shift the focus to the target task.\nSince the target task gets more observations, transferring from source tasks might not be necessary anymore considering the bias and noises in the source tasks (i.e., negative transfer~\\cite{pan2010a}).\nExisting methods~\\cite{wistuba2016two,schilling2016scalable,feurer2018scalable} \nhave been focusing on these two challenges. However, none of them considers both simultaneously.\nThis motivates our work, which aims at developing a transfer learning framework that could 1) extract source knowledge in a {\\em cooperative} manner, and 2) transfer the auxiliary knowledge in an {\\em adaptive} way.\n\nIn this paper, we propose \\sys, a novel two-phase transfer learning framework for automatic HPO\nthat tries to address the above two challenges simultaneously.\n\\sys works under the umbrella of Bayesian optimization and designs a transfer learning (TL) surrogate to guide the HPO process.\nThis framework decouples the process of knowledge transfer into two phases and considers the knowledge extraction and knowledge aggregation separately in each phase (See Figure~\\ref{framework}).\nIn Phase one, \\sys builds a source surrogate that extracts and combines useful knowledge across multiple source tasks.  \nIn Phase two, \\sys integrates the source surrogate (in Phase one) and the target surrogate to construct the final surrogate, which we refer to as the transfer learning surrogate.\nTo maximize the generalization of the transfer learning surrogate, we adopt the cross-validation mechanism to learn the transfer learning surrogate in a principled manner.\nMoreover, instead of combining base surrogates with independent weights, \\sys can learn the optimal aggregation weights for base surrogates jointly.\nTo this end, we propose to learn the weights in each phase by solving a constrained optimization problem with a differentiable ranking loss function.\n\nThe empirical results of static TL scenarios showcase the stability and effectiveness of \\sys compared with state-of-the-art TL methods for HPO. In dynamic TL scenarios that are close to real-world applications, \\sys obtains strong performance -- the top-2 results on 22.25 out of 30 tuning tasks (Practicality). \nIn addition, when applying \\sys to neural architecture search (NAS), it achieves more than 5$\\times$ speedups than the state-of-the-art NAS approaches (Universality).\n\n{\\bf (Contributions )}\nIn this work, our main contributions are summarized as follows:\n\\begin{itemize}\n    \\item We present a novel two-phase transfer learning framework for HPO --- \\sys, which could address the aforementioned challenges simultaneously.\n    \\item We formulate the learning of this two-phase framework into constrained optimization problems. By solving these problems, \\sys could extract and aggregate the source and target knowledge in a joint and adaptive manner.\n    \\item To facilitate transfer learning research for HPO, we create and publish a large-scale benchmark, which takes more than 200K CPU hours and involves more than 1.8 million model evaluations.\n    \\item The extensive experiments, including static and dynamic TL settings and neural architecture search, demonstrate the superiority of \\sys over state-of-the-art methods.\n\\end{itemize}\n\n\\iffalse\n1) We present a novel two-phase transfer learning framework for HPO --- \\sys, which could address the aforementioned challenges simultaneously.\n2) We formulate the learning of this two-phase framework into constrained optimization problems. By solving these problems, \\sys could extract and aggregate the source and target knowledge in a joint and adaptive manner.\n3) To facilitate transfer learning research for HPO, we create and publish a large-scale benchmark, which takes more than 200K CPU hours and involves more than 1.8 million model evaluations.\n4) The extensive experiments, including static and dynamic TL settings and neural architecture search, demonstrate the superiority of \\sys over state-of-the-art methods.\n\\fi\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\nBayesian optimization (BO) has been successfully applied to hyperparameter optimization (HPO)~\\cite{bischl2021hyperparameter,li2020efficient,openbox,li2022hyper}.\nFor ML models that are computationally expensive to train (e.g., deep learning models or models trained on large datasets), BO methods~\\cite{hutter2011sequential,bergstra2011algorithms,snoek2012practical} suffer from the low-efficiency issue due to insufficient configuration evaluations within a limited budget.\nTo speed up HPO of ML algorithms with limited trials, recent BO methods extend the traditional black-box assumption by exploiting cheaper fidelities from the current task \\cite{klein2017fast, swersky2014freeze,kandasamy2017multi, klein2016learning,poloczek2017multi, falkner2018bohb,li-mfeshb,li2022hyper}. Orthogonal to these methods, we focus on borrowing strength from previously finished tasks to accelerate the HPO of the current task.\n\nTransfer learning (TL) methods for HPO aim to leverage auxiliary knowledge from previous tasks to achieve faster optimization on the target task. \nOne common way is to learn surrogate models from past tuning history and use them to guide the search of hyperparameters.\nFor instance, several methods learn all available information from both source and target tasks in a single surrogate, and make the data comparable through a transfer stacking ensemble~\\cite{pardoe2010boosting}, a ranking algorithm~\\cite{bardenet2013collaborative}, multi-task GPs~\\cite{swersky2013multi}, a mixed kernel GP~\\cite{yogatama2014efficient}, the GP noisy model~\\cite{joy2016flexible}, a multi-layer perceptron with Bayesian linear regression heads~\\cite{snoek2015scalable,perrone2018scalable} or replace GP with Bayesian neural networks~\\cite{springenberg2016bayesian}.\nSGPR~\\cite{golovin2017google} and SMFO~\\cite{wistuba2015sequential} utilize the knowledge from all source tasks equally and thus suffer from performance deterioration when the knowledge of source tasks is not applicable to the target task.\nFMLP~\\cite{schilling2015hyperparameter} uses multi-layer perceptrons as the surrogate model that learns the interaction between hyperparameters and datasets.\nSCoT~\\cite{bardenet2013collaborative} and MKL-GP~\\cite{yogatama2014efficient} fit a GP-based surrogate on merged observations from both source tasks and target task.\nTo distinguish the varied performance of the same configuration on different tasks, the two methods use the meta-features of datasets to represent the tasks;\nwhile the meta-features are often unavailable for broad classes of HPO problems~\\cite{feurer2018scalable}.\nDue to the high computational complexity of GP ($\\mathcal{O}(n^3)$), it is difficult for these methods to scale to a large number of source tasks and trials (scalability bottleneck). \n\nTo improve scalability, recent methods adopt the ensemble framework to conduct TL for HPO, where they train a base surrogate on each source task and the target task respectively and then combine all base surrogates into an ensemble surrogate with different weights. \nThis framework ignores the two aforementioned issues and uses the {\\em independent} weights.\nPOGPE \\cite{schilling2016scalable} sets the weights of base surrogates to constants.\nTST \\cite{wistuba2016two} linearly combines the base surrogates with a Nadaraya-Watson kernel weighting by defining a distance metric across tasks; the weights are calculated by using either meta-features (TST-M) or pairwise hyperparameter configuration rankings (TST-R).\nRGPE \\cite{feurer2018scalable} uses the probability that the base surrogate has the lowest ranking loss on the target task to estimate the weights.\nInstead of resorting to heuristics, \\sys propose to learn the joint weights in a principled way.\n\nWarm-starting methods~\\cite{lindauer2018warmstarting,kim2017learning} select several initial hyperparameter configurations as the start points of search procedures. \n\\citet{salinas2020quantile} deal with the heterogeneous scale between tasks with the Gaussian Copula Process.\nABRAC~\\cite{horvath2021hyperparameter} proposes a multi-task BO method with adaptive complexity to prevent over-fitting on scarce target observations.\nTNP~\\cite{wei2021meta} applies the neural process to jointly transfer surrogates, parameters, and initial configurations.\nRecently, transferring search space has become another way for applying transfer learning in HPO. ~\\citet{wistuba2015hyperparameter} prune the bad regions of search space according to the results from previous tasks. \nThis method suffers from the complexity of obtaining meta-features and relies on some other parameters to construct a GP model. \nOn that basis, ~\\citet{NIPS2019_9438} propose to utilize previous tasks to design a sub-region of the entire search space for the new task. \nWhile sharing some common spirits, these methods are orthogonal and complementary to our surrogate transfer method introduced in this paper.\n\nIn addition, our proposed two-phase framework inherits the advantages of the bi-level optimization~\\cite{bennett2008bilevel}.\nWhile previous methods in the literature focus on different tasks (e.g., evolutionary computation~\\cite{sinha2017review}), to the best of our knowledge, \\sys is the first method that adopts the concept of bi-level optimization into hyperparameter transfer learning.\n\n"
            },
            "section 3": {
                "name": "Bayesian Hyperparameter Optimization",
                "content": "\nThe HPO of ML algorithms can be modeled as a black-box optimization problem. \nThe goal is to find $argmin_{\\bm{x} \\in \\mathcal{X}}f(\\bm{x})$ in hyperparameter space $\\mathcal{X}$, where $f(\\bm{x})$ is the ML model's performance metric (e.g., validation error) corresponding to the configuration $\\bm{x}$. \nDue to the intrinsic randomness of most ML algorithms, we evaluate configuration $\\bm{x}$ and can only get its noisy result $y = f(\\bm{x}) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. \n\n{\\bf Bayesian optimization (BO)} is a model-based framework for HPO. \nBO first fits a probabilistic surrogate model $M:p(f|D)$ on the already observed instances $D=\\{(\\bm{x}_1, y_1),...,(\\bm{x}_{n-1}, y_{n-1})\\}$.\nIn the $n$-th iteration, BO iterates the following steps: 1) use surrogate $M$ to select a promising configuration $\\bm{x}_n$ that maximizes the acquisition function $\\bm{x}_{n}=\\arg\\max_{\\bm{x} \\in \\mathcal{X}}a(\\bm{x}; M)$, where the acquisition function is to balance the exploration and exploitation trade-off; 2) evaluate this point to get its performance $y_n$, and add the new observation $(\\bm{x}_{n}, y_{n})$ to $D$; 3) refit $M$ on the augmented $D$. \nExpected Improvement (EI)~\\cite{jones1998efficient} is a common acquisition function defined as follows:\n\\begin{equation}\n\\label{eq_ei}\na(\\bm{x}; M)=\\int_{-\\infty}^{\\infty} \\max(y^{\\ast}-y, 0)p_{M}(y|\\bm{x})dy,\n\\end{equation}\nwhere $M$ is the surrogate and $y^{\\ast}=\\min\\{y_1, ..., y_n\\}$. \nBy maximizing this EI function $a(\\bm{x}; M)$ over $\\mathcal{X}$, BO methods can find a configuration to evaluate for each iteration.\n\n\n\n\n"
            },
            "section 4": {
                "name": "The Proposed Method",
                "content": "\n\\label{sec4}\nIn this section, we present \\sys, a two-phase transfer learning (TL) framework for HPO. Before diving into the proposed framework, we first introduce the notations and settings for TL. Then we describe \\sys in details and end the section with discussions about its advantages.\n\n\\para{Basic Notations and Settings. }\nAs illustrated in Figure~\\ref{framework},\nwe denote observations from $K+1$ tasks as $D^1$, ..., $D^K$\nfor $K$ source tasks and $D^T$ for the target task. \nThe $i$-th source task has $n_i$ configuration observations: $D^i=\\{(\\bm{x}_j^i, y_j^i)\\}_{j=1}^{n_i}$ with $i=1,2,...,K$, which are obtained from previous tuning procedures.\nFor the target task, after completing $t$ iterations (trials), the observations in the target task are: $D^T=\\{(\\bm{x}_j^T, y_j^T)\\}_{j=1}^{t}$.\n\nBefore optimization, we train a base surrogate model for the $i$-th source task, denoted by $M^i$.\nEach base surrogate $M^i$ can be fitted on $D^i$ in advance (offline), and the target surrogate $M^T$ is trained on $D^T$ on the fly.\nSince the configuration performance $y$s in each $D^i$ and $D^T$ may have different numerical ranges, we standardize the $y$s in each task by removing the mean and scaling to unit variance. \nFor a hyperparameter configuration $\\bm{x}_j$, each base surrogate $M^i$ outputs a posterior predictive distribution at $\\bm{x}_j$, that's, $M^i(\\bm{x}_j) \\sim \\mathcal{N}(\\mu_{M^i}(\\bm{x}_j), \\sigma^2_{M^i}(\\bm{x}_j))$. For brevity, we denote the mean of this prediction at $\\bm{x}_j$ as $M^i(\\bm{x}_j)=\\mu_{M^i}(\\bm{x}_j)$.\n\n",
                "subsection 4.1": {
                    "name": "Overview",
                    "content": "\n\\sys aims to build a transfer learning surrogate model $M^{TL}$ on the target task, which outputs a more accurate prediction for each configuration by borrowing strength from the source tasks.\nThe cornerstone of \\sys is to decouple the combination of $K+1$ base surrogates with a novel two-phase framework:\n\n{\\em Phase 1.}\nTo leverage the complementary nature among source tasks, \\sys first linearly combines all source base surrogates into a single source surrogate with the weights ${\\bf w}$:\n\\begin{equation}\n    M^S = \\texttt{agg}(\\{M^1,...,M^K\\}; {\\bf w}).\n\\nonumber\n\\end{equation}\nIn this phase, the useful source knowledge from each source task is extracted and integrated into the source surrogate in a joint and cooperative manner.\n\n{\\em Phase 2.}\nTo support dynamics-aware knowledge aggregation, \\sys further combines the aggregated source surrogate with the target surrogate $M^T$ via weights ${\\bf p}$ in an adaptive manner, where $M^T$ is trained on the target observations $D^{T}$:\n\\begin{equation}\nM^{TL} = \\texttt{agg}(\\{M^S, M^T\\}; {\\bf p}).\n\\nonumber\n\\end{equation}\n% To reflect the generalization ability of $M^{TL}$ and address \\red{the source-target dynamics issue}, \\sys adopts the cross-validation mechanism to learn the parameters.\n\nSuch joint and adaptive knowledge transfer in two phases guarantees the efficiency and effectiveness of the final TL surrogate $M^{TL}$ in extracting and integrating the source and target knowledge. \nTo maximize the generalization ability of $M^{TL}$, the two-phase framework further learns the parameters ${\\bf w}$ and ${\\bf p}$ in a principled and automatic manner by solving the constrained optimization problems.\nIn the following, we describe the parameter learning and aggregation method.\n\n"
                },
                "subsection 4.2": {
                    "name": "Parameter Learning in Two-Phase Framework",
                    "content": "\n\\label{sec_tp_learning}\nNotice that\n${\\bf w}$ and ${\\bf p}$ play different \nroles --- ${\\bf w}$ combines $K$ source base surrogates to best fit the target observations, while ${\\bf p}$ balances between two surrogates $M^S$ and $M^T$. \nThe objective of \\sys is to maximize the generalization performance of $M^{TL}$. \nTo obtain ${\\bf w}$, we use the target observations $D^T$ to maximize the performance of source surrogate $M^{S}$.\nHowever, if we learn the parameter ${\\bf p}$ of $M^{TL}$ on $D^T$ by using the $M^{S}$ and $M^T$, where $M^{S}$ and $M^{T}$ are trained on $D^T$ directly, the learning process becomes an estimation of in-sample error and can not reflect the generalization of the final surrogate $M^{TL}$.\nTo address this issue, we adopt the cross-validation mechanism to maximize the generalization ability of $M^{TL}$ when learning ${\\bf p}$.\nIn the following, we first describe the general procedure to learn a surrogate $M^{S}$ on given observations $D$ (instead of $D^T$), and then introduce the method to learn the parameters $\\bf w$ and $\\bf p$, respectively.\n\n\\para{General Procedure: Fitting $M^S$ on Given Observations $D$.}\nOur strategy is to obtain the source surrogate $M^S$ as a weighted combination of the predictions of source base surrogates $\\{M^1,...,M^K\\}$:\n\\begin{equation}\n\\label{fS}\nM^S(\\bm{x}) = \\sum_{i=1}^K{w}_{i}M^i(\\bm{x}),\n\\end{equation}\nwhere $\\sum_{i}{w}_i = 1$ and ${w}_i \\in [0, 1]$.\nIntuitively, the weight $w_i$ reflects the quality of knowledge extracted from the corresponding source tasks. \nInstead of calculating weights independently, which may ignore the complementary nature among source tasks, we propose to combine source base surrogates $M^i$s in a joint and supervised manner, which reveals their cooperative contributions to $M^S$. \n\nTo derive $M^S$ in a principled way, we use a differentiable pairwise ranking loss function to measure the fitting error between the prediction of $M^S$ and the available observations $D$. \nIn HPO, ranking loss is more appropriate than mean square error --- the actual values of predictions are not the most important, and we care more about the partial orders over the hyperparameter space, e.g., the location of the optimal configuration.\nThis ranking loss function is defined as follows:\n\\begin{equation}\n\\begin{aligned}\n    & \\mathbb{L}({\\bf w}, M^{S}; D) = \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1,y_j<y_k}^{n}\\phi(M^S(\\bm{x}_k) - M^S(\\bm{x}_j)), \\\\\n    & \\phi(z) = log(1 + e^{-z}), \\\\\n\\end{aligned}\n\\label{ranking_loss}\n\\end{equation}\nwhere $n$ is the number of observations in $D$, $y$ is the observed performance of configuration $\\bm{x}$ in $D$, \nand the prediction of $M^S(\\bm{x}_j)$ at configuration $\\bm{x}_j$ is obtained by linearly combining the predictive mean of $M^i$ with a weight ${w}_i$, that's, $M^S(\\bm{x}_j)=\\sum_i { w}_iM^i(\\bm{x}_j)$.\n\nWe further turn the learning of source surrogate $M^{S}$, i.e., the learning of ${\\bf w}$, into the following constrained optimization problem:\n\\begin{equation}\n    \\label{eq:opt_source}\n    \\begin{aligned}\n        & \\underset{{\\bf w}}{\\text{minimize}}\n        & & \\mathbb{L}({\\bf w}, M^{S}; D) \\\\\n        & \\text{s.t.}\n        & & \\bm{1}^\\top{\\bf w}=1, {\\bf w}\\ge\\bm{0}, \\\\\n    \\end{aligned}\n\\end{equation}\nwhere the objective is the ranking loss of $M^S$ on $D$.\nThis optimization objective is continuously differentiable, and concretely, it is twice continuously differentiable. \nSo we can have the first derivative of the objective $\\mathbb{L}$ as follows:\n\\begin{equation}\n\\begin{aligned}\n    & \\frac{\\partial \\mathbb{L}}{\\partial {\\bf w}} = \\sum_{(j, k) \\in \\mathbb{P}}\\frac{neg\\_ez}{1 + neg\\_ez} \\ast (A_{[j]} - A_{[k]}), \\\\\n    & neg\\_ez = e^{(A_{[j]}{\\bf w} - A_{[k]}{\\bf w})}, \\\\\n\\end{aligned}\n\\label{der_loss}\n\\end{equation}\nwhere $\\mathbb{P}$ consists of pairs $(j, k)$ satisfying $y_j < y_k$, $A$ is the matrix formed by putting the predictions of $M^{1:K}$s together where the element at the $i$-th row and $j$-th column is $M^i(\\bm{x}_j)$, and $A_{[j]}$ is the row vector in the $j$-th row of matrix $A$.\nFurthermore, this optimization problem can be solved efficiently by applying many existing sequential quadratic programming (SQP) solvers ~\\cite{10.1145/192115.192124}. \n\n\\para{Learning Parameter ${\\bf w}$.}\nAs stated previously, to maximize the (generalization) performance of $M^{S}$, we propose to learn the parameter ${\\bf w}$ by fitting $M^{S}$ on the whole observations $D^T$.\nIn this way, the useful source knowledge from multiple source tasks can be fully extracted and integrated in a joint manner.\nTherefore, the parameters {\\bf w} can be obtained by calling the general procedure, i.e., solving the problem~\\ref{eq:opt_source}, where the available observations $D$ are set to $D^T$.\n\n\\iffalse\nOur objective is to maximize the (generalization) performance of $M^{TL}$. \nWe first learn the parameter {\\bf p} in $M^{TL}$, and {\\bf p} can be obtained in advance ({\\bf p} is fixed now).\nTo maximize the performance of $M^{TL}$, we need to maximize the performance of $M^{S}$ and $M^{T}$ respectively.\nSince $M_{-i}^S$ and $M_{-i}^T$ are fitted on the partial observations $D_{-i}^T$, both of them can not capture the entire knowledge of observations $D^T$. \nTo maximize the performance of $M^{S}$ and $M^{T}$, we propose to learn the parameter ${\\bf w}$ by fitting $M^{S}$ on the whole observations $D^T$; in addition, the final $M^{T}$ is fitted on $D^T$ too. Therefore, the parameters {\\bf w} can be obtained by solving the problem~\\ref{eq:opt_source}, where the available observations $D$ are set to $D^T$.\n\\fi\n\n\\para{Learning Parameter ${\\bf p}$.}\nTo reflect the generalization in $M^{TL}$, the parameter ${\\bf p}$ is learned with the cross-validation mechanism. \nWe first split $D^T$ into\n$N_{cv}$ partitions: $D^T_1$, ..., $D^T_{N_{cv}}$ with $N_{cv}=5$. \nFor each partition $i\\in [1:N_{cv}]$, we first fit a partial surrogate $M^S_{-i}$ on the observations $D^T_{-i}$ with observations in the $i$-th partition removed from $D^T$, and the surrogate $M^S_{-i}$ is learned on $D^T_{-i}$ using the general procedure; in addition, we also fit a partial surrogate model $M^T_{-i}$ on $D^T_{-i}$ directly.\nThen we combine the surrogates $M^S_{-i}$ and $M^T_{-i}$ linearly to obtain a $M_{-i}^{TL}$:\n\\begin{equation}\n\\begin{aligned}\n    M^{TL}_{-i} = {p}^S M^S_{-i} + {p}^T M^T_{-i},\\\\\n\\end{aligned}\n\\end{equation}\nwhere ${\\bf p} =[{p}^S, {p}^T]$. \nTherefore, we can obtain $N_{cv}$ partial surrogates $M_{-i}^S$ and $M_{-i}^T$ with $i\\in[1:N_{cv}]$.\nBased on the differentiable pairwise ranking loss function in Eq.~\\ref{ranking_loss}, the loss of $M_{-i}^{TL}$ on $D^{T}$ is defined as:\n\\begin{equation}\n\\begin{aligned}\n    & \\mathbb{L}_{cv}({\\bf p}, M^{TL}_{-i}; D^T) = \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1,y_j^T<y_k^T, k \\in D^T_{i}}^{n}\\phi(z), \\\\\n    & \\phi(z) = log(1 + e^{-z}), z=M^{TL}_{-i}(\\bm{x}_k) - M^{TL}_{-H(j)}(\\bm{x}_j)\\\\\n\\end{aligned}\n\\label{ranking_loss_cv}\n\\end{equation}\nwhere $n$ is the number of observations in $D^T$, $y^T$ is the observed performance of configuration $\\bm{x}^T$ in $D^T$, $H(j)$ indicates the partition id that configuration $\\bm{x}_j$ belongs to, and the prediction of $M^{TL}_{-i}$ at configuration $\\bm{x_k}$ is obtained by linearly combining the predictive mean of $M^S_{-i}$ and $M^{T}_{-i}$ with weight {\\bf p}, that's, $M_{-i}^{TL}(\\bm{x}_k)={p}^SM^{S}_{-i}(\\bm{x}_k)+{p}^TM^T_{-i}(\\bm{x}_k)$.\nSo the parameter {\\bf p} can be learned by solving a similar constrained optimization problem on $D^T$:\n\\begin{equation}\n    \\label{eq:opt_target}\n    \\begin{aligned}\n        & \\underset{{\\bf p}}{\\text{minimize}}\n        & & \\sum_{i=1}^{N_{cv}} \\mathbb{L}_{cv}({\\bf p}, M^{TL}_{-i}; D^T) \\\\\n        & \\text{s.t.}\n        & & \\bm{1}^\\top{\\bf p}=1, {\\bf p}\\ge\\bm{0}.\\\\\n    \\end{aligned}\n\\end{equation}\nFollowing the solution introduced in problem~\\ref{eq:opt_source}, the above optimization problem can be solved efficiently.\n\n\\para{Final TL Surrogate.} \nAfter ${\\bf w}$ and ${\\bf p}$ are obtained, as illustrated in Figure~\\ref{framework}, we first combine the source base surrogates into the source surrogate $M^S$ with ${\\bf w}$ (the Phase 1), and then integrate $M^{S}$ and $M^T$ with ${\\bf p}$ to obtain the final TL surrogate $M^{TL}$ (the Phase 2). To ensure the surrogate $M^{TL}$ still works in the BO framework, it is required to be a GP.\nHow to obtain the unified posterior predictive mean and variance from multiple GPs (base surrogates) is still an open problem. \nAs suggested by \\cite{feurer2018scalable}, the linear combination of multiple base surrogates works well in practice. \nTherefore, we aggregate the base surrogates with linear combination.\nThat's, suppose there are $N_B$ GP-based surrogates, and each base surrogate $M^b$ has a weight $w_b$ with $b = 1, ..., N_B$, the combined prediction under the linear combination technique is give by: $\\mu_{C}(\\bm{x})=\\sum_bw_b\\mu_{b}(\\bm{x})$ and $\\sigma^2_{C}(\\bm{x})=\\sum_bw_b^2\\sigma_b^2(\\bm{x})$.\n\n\n\\para{Algorithm Summary}\nAt initialization, we set the weight of each source surrogate in ${\\bf w}$ to $1/K$, and ${\\bf p}=[1, 0]$ when the number of trials is insufficient for cross-validation.\nAlgorithm~\\ref{algo:tptl_framework} illustrates the pseudo code of \\sys.\nIn the $i$-th iteration, we first learn the weights ${\\bf p}_i$ and ${\\bf w}_i$ by solving two optimization problems (Lines 2-3).\nSince we have the prior: as the HPO process of the target task proceeds, the target surrogate owns more and more knowledge about the objective function of the target task, therefore the weight of $M^{T}$ should increase gradually. \nTo this end, we employ a \\emph{max} operator, which enforces that the update of ${p}^{T}$ should be non-decreasing (Line 4).\nNext, by using linear combination, we build the source surrogate $M^{S}$ with weight ${\\bf w}_i$, and then construct the final TL surrogate $M^{TL}$ with ${\\bf p}_i$ (Line 5).\nFinally, \\sys utilizes $M^{TL}$ to choose a promising configuration to evaluate, and refit the target surrogate on the augmented observation (the BO framework, Lines 6-7).\n\n\\begin{algorithm}[tb]\n  \\small\n  \\caption{The \\sys Framework.}\n  \\label{algo:tptl_framework}\n  \\textbf{Input}: maximum number of trials $N^{T}$, observations from $K$ source tasks: $D^{1:K}$, and config. space $\\mathcal{X}$.\n  \\begin{algorithmic}[1]\n    \\FOR{$i \\in \\{1, 2, ..., N^{T}\\}$}\n      \\STATE Calculate the weight ${\\bf w}_i$ in $M^{S}$ by solving~(\\ref{eq:opt_source}).\n      \\STATE Calculate the weight ${\\bf p}_i$ in $M^{TL}$ by solving~(\\ref{eq:opt_target}).\n      \\STATE Employ non-decreasing prior on ${p}^{T}$: $p^{T}_i = \\operatorname{max}(p^{T}_i, p^{T}_{i-1})$.\n      \\STATE Build $M^{S}$, $M^{TL}$ with weights ${\\bf w}_i$ and ${\\bf p}_i$, respectively.\n      \\STATE Sample a large number of configurations randomly from $\\mathcal{X}$, compute their acquisition values according to the EI criterion in Eq.\\ref{eq_ei}, where $M = M^{TL}$, and choose the configuration $\\bm{x}_i = \\operatorname{argmax}_{x\\in\\mathcal{X}}ei(x, M^{TL})$.\n      \\STATE Evaluate $\\bm{x}_i$ and get its performance $y_i$, augment observations $D^{T}$ with $(\\bm{x_i}, y_i)$ and refit $M^{T}$ on the augmented $D^{T}$.\n    \\ENDFOR\n  \\STATE \\textbf{return} the best configuration in $D^T$.\n\\end{algorithmic}\n\\end{algorithm}\n\n"
                },
                "subsection 4.3": {
                    "name": "Discussion: Advantages of \\sys",
                    "content": "\nTo our knowledge, \\sys is the first method that conducts transfer learning for HPO in a supervised manner, instead of resorting to some heuristics.\nIn addition, this method owns the following desirable properties simultaneously.\n1) \\textbf{Practicality.} \nA practical HPO method should be insensitive to its hyperparameters, and do not depend on meta-features. \nThe goal of HPO is to optimize the ML hyperparameters automatically while having extra (or sensitive) hyperparameters itself actually violates its principle. \nIn addition, many datasets, including image and text data, lack appropriate meta-features to represent the dataset~\\cite{wistuba2016two,schilling2015hyperparameter,feurer2018scalable}.\nThe construction of TL surrogate in \\sys is insensitive to its hyperparameters and does not require meta-features.\n2)~\\textbf{Universality.} The 1st property enable \\sys to be a general transfer learning framework for Black-box optimizations, e.g., experimental design~\\cite{NEURIPS2019_d55cbf21}, neural architecture search~\\cite{dudziak2020brp}, etc; we include an experiment to evaluate \\sys on the NAS task in the section of experiment).\n3)~\\textbf{Scalability.} Compared with the methods that combine $k$ source tasks with $n$ trials into a single surrogate ($O(k^3n^3)$), \\sys has a much lower complexity $O(kn^3)$, which means that \\sys could scale to a large number of tasks and trials easily. \n4)~\\textbf{Theoretical Discussion.} \n\\sys also provides theoretical discussions about preventing the performance deterioration (negative transfer). \nBase on cross-validation and the non-decreasing constraint, {\\em the performance of \\sys, given sufficient trials, will be no worse than the method without transfer learning}, while the other methods cannot have this (See Appendix~\\ref{converge_analysis} for more details).\n\n\\iffalse\n\n\\fi\n\n"
                }
            },
            "section 5": {
                "name": "Experiments and Results",
                "content": "\n\\label{sec:exp_sec}\nIn this section, we evaluate \\sys from three perspectives: 1) stability and effectiveness on static TL tasks, 2) practicality on real-world dynamic TL tasks, and 3) universality when conducting neural architecture search.\n\n\n\n\n\n\n\n",
                "subsection 5.1": {
                    "name": "Experimental Setup",
                    "content": "\n\\para{\\textbf{Baselines.}}\nWe compare \\sys with eight baselines --\ntwo non-transfer methods: (1) Random search~\\cite{bergstra2012random}, (2) I-GP: independent Gaussian process-based surrogate fitted on the target task without using any source data, (3) SCoT~\\cite{bardenet2013collaborative}: it models the relationship between datasets and hyperparamter performance by training a single surrogate on the scaled and merged observations from both source tasks and the target task, (4) SGPR: the core TL algorithm used in the well-known service --- Google Vizier~\\cite{golovin2017google}, and four ensemble based TL methods: (5) POGPE~\\cite{schilling2016scalable}, (6) TST~\\cite{wistuba2016two}, (7) TST-M: a variant of TST using dataset meta-features~\\cite{wistuba2016two}, and (8) RGPE~\\cite{feurer2018scalable}.\n\n\\para{\\textbf{Benchmark on 30 OpenML Datasets.}}\nTo evaluate the performance of \\sys, we create and publish a large-scale benchmark. \nFour ML algorithms, including Random Forest, Extra Trees, Adaboost and LightGBM~\\cite{ke2017lightgbm}, are tuned on 30 real-world datasets (tasks) from OpenML repository~\\cite{10.1145/2641190.2641198}. \nThe design of hyperparameter space and meta-feature for each dataset is adopted from the implementation in Auto-Sklearn~\\cite{feurer2015efficient}. \nFor each ML algorithm on each dataset, we sample 20k configurations from the hyperparameter space randomly and store the corresponding evaluation results.\n% To support those TL methods that need meta-features, for each dataset a 46-dimensional meta-feature is generated based on~\\cite{feurer2015efficient}, including the number of instances, class entropy, skewness, kurtosis and etc.\nIt takes more than 200k CPU hours to collect these evaluation results. \nNote that, for reproducibility, we provide more details about this benchmark, including the datasets, the hyperparameter space of ML algorithms, etc., in Appendix~\\ref{a.1}.\n\n\\para{\\textbf{AutoML HPO Tasks.}}\nTo evaluate the performance of each method, the experiments are performed in a leave-one-out fashion. \nEach method optimizes the hyperparameters of a specific task over 20k configurations while treating the remaining tasks as the source tasks. \nIn each source task, only $N_S$ instances (here $N_S=50$) are used to extract knowledge from this task in order to test the efficiency of TL~\\cite{wistuba2016two, feurer2018scalable}.\n\nWe include the following three kinds of tasks:\n\n(a)~\\emph{\\bf Static TL Setting.} This experiment is performed in a leave-one-out fashion, i.e., we optimize the hyperparameters of the target task while treating the remaining tasks as the source tasks. \n\n(b)~\\emph{\\bf Dynamic TL Setting.} It simulates the real-world HPO scenarios, in which 30 tasks (datasets) arrive sequentially; when the $i$-th task appears, the former $i-1$ tasks are treated as the source tasks.\n\n(c)~\\emph{\\bf Neural Architecture Search (NAS).} It transfers tuning knowledge from conducting NAS on CIFAR-10 and CIFAR-100 to accelerate NAS on ImageNet16-120 based on NAS-Bench201~\\cite{dong2019bench}.\n\nIn addition, following~\\cite{feurer2018scalable}, all the compared methods are initialized with three randomly selected configurations, after which they proceed sequentially with a total of $N_{T}$ evaluations (trials). \nTo avoid the effect of randomness, each method is repeated 30 times, and the averaged performance metrics are reported.\n\n\\para{Evaluation Metric.}\n% Since the classification error is not commensurable across datasets, following the previous works~\\cite{bardenet2013collaborative,wistuba2016two,feurer2018scalable}, we adopt the \\emph{average rank} metric: For each target task, we rank all compared methods based on the performance of the best configuration they have found so far. \n% Furthermore, ties are being solved by giving the average rank. \n% For example, if one method observes the lowest validation error of 0.2, another two methods find 0.3, and the last method finds only 0.45, we would rank the methods with $1$, $\\frac{2+3}{2}$, $\\frac{2+3}{2}$, $4$.\nComparing each method in terms of classification error is questionable because the classification error is not commensurable across datasets. \nFollowing the previous works~\\cite{bardenet2013collaborative,wistuba2016two,feurer2018scalable}, we adopt the metrics as follows:\n\n\\emph{Average Rank.} For each target task, we rank all compared methods based on the performance of the best configuration they have found so far. \nFurthermore, ties are being solved by giving the average rank. \nFor example, if one method observes the lowest validation error of 0.2, another two methods find 0.3, and the last method finds only 0.45, we would rank the methods with $1$, $\\frac{2+3}{2}$, $\\frac{2+3}{2}$, $4$.\n\n\\emph{Average Distance to Minimum.} The average distance to the global minimum after $t$ trials is defined as:\n\\begin{equation}\n\\small\nADTM(\\mathcal{X}_t) = \\frac{1}{K}\\sum_{i \\in [1:K]}\\frac{min_{\\bm{x} \\in \\mathcal{X}_t} y^{i}_{\\bm{x}} - y^{i}_{min}}{y^i_{max} - y^{i}_{min}},\n\\end{equation}\nwhere $y^i_{min}$ and $y^i_{max}$ are the best and worst performance value on the $i$-th task, $K$ is the number of tasks, i.e., $K=30$, $y_{\\bm{x}}^i$ corresponds to the performance of configuration $\\bm{x}$ in the $i$-th task, and $\\mathcal{X}_t$ is the set of hyperparameter configurations that have been evaluated in the previous $t$ trials. \nThe relative distances over all considered tasks are averaged to obtain the final ADTM value.\n\n\n\\para{Implementations \\& Parameters.}\n\\sys implements the Gaussian process using SMAC3\\footnote{https://github.com/automl/SMAC3}~\\cite{hutter2011sequential,Lindauer2021SMAC3AV},\nwhich can support a complex hyperparameter space, including numerical, categorical, and conditional hyperparameters, and the kernel hyperparameters in GP are inferred by maximizing the marginal likelihood. \nThe two optimization problems in \\sys are solved by using SQP methods provided in SciPy~\\footnote{https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html}~\\cite{2020SciPy-NMeth}.\nIn the BO module, the popular EI acquisition function is used.\nAs for the parameters in each baseline, the bandwidth $\\rho$ in TST~\\cite{wistuba2016two} is set to 0.3 for all experiments; in RGPE, we sample 100 times ($S=100$) to calculate the weight for each base surrogate; \nin SGPR~\\cite{golovin2017google}, the parameter $\\alpha$, which determines the relative importance of standard deviations of past tasks and the current task, is set to 0.95 (Check Appendix~\\ref{reproduction} for reproduction details).\n\n\n\n\n\n\n\n\\iffalse\n\n\\fi\n\n"
                },
                "subsection 5.2": {
                    "name": "Comprehensive Experiments in Two TL Settings",
                    "content": "\n\\para{\\bf Static TL Setting.} To demonstrate the efficiency and effectiveness of transfer learning in the static scenario, we compare \\sys with the baselines on four benchmarks (i.e., Random Forest, LightGBM, Adaboost, and Extra Trees). \nConcretely, each task is selected as the target task in turn, and the remaining tasks are the source tasks;\nthen we can measure the performance of each baseline based on the results when tuning the hyperparameters of the target task.\nFurthermore, we use 29 and 5 source tasks respectively to evaluate the ability of each method when given a different amount of source knowledge in terms of the number of source tasks $N_{task}$. \nNote that, for each target task, the maximum number of trials is 75. \nFigure~\\ref{offline_exp1} and Figure~\\ref{offline_exp2} show the experiment results on four benchmarks with 29 and 5 source tasks respectively, using average rank; more results on ADTM can be found in Appendix~\\ref{a.3}.\n\nFirst, we can observe that the average rank of \\sys in Figure~\\ref{offline_exp1} and Figure~\\ref{offline_exp2} decreases sharply in the initial 20 trials.\nCompared with other TL methods, it shows that \\sys can extract and utilize the auxiliary source knowledge efficiently and effectively.\nRemarkably, \\sys exhibits a strong stability from two perspectives: 1) \\sys is stable on different benchmarks; and 2) it still performs well when given a different number of source tasks, e.g., in Figure~\\ref{offline_exp1} $N_{task}=29$, and $N_{task}=5$ in Figure~\\ref{offline_exp2}.\nRGPE is one of the most competitive baselines, and we take it as an example.\nRGPE achieves comparable or similar performance with \\sys in Figure~\\ref{offline_exp1_lgb} and Figure~\\ref{offline_exp1_adb} where $N_{task} = 29$. \nHowever, in Figure~\\ref{offline_exp2_lgb} and Figure~\\ref{offline_exp2_adb} RGPE exhibits a larger fluctuation over the trials compared with \\sys when $N_{task} = 5$.\nUnlike the baselines, \\sys extracts the source knowledge in a principled way, and the empirical results show it performs well in most circumstances, thus demonstrating its superior efficiency and effectiveness.\n\n\n\n\n\n\n\n\n\n\\para{Dynamic TL Setting.}\nTo simulate the real-world transfer learning scenario, we perform the dynamic experiment on different benchmarks. \nIn this experiment, 30 tasks arrive sequentially; when the $i$-th task arrives, the previous $i$-1 tasks are used as the source tasks. \nThe maximum number of trials for each task is 50, and we compare \\sys with TST, RGPE, and POGPE based on the best-observed performance on each task. Table~\\ref{online-table} reports the number of tasks on which each TL method gets the highest and second-highest performance. \nNote that the sum of each column may be more than 30 since some of the TL methods are tied for first or second place.\n\nAs shown in Table~\\ref{online-table}, \\sys achieves the largest number of top1 and top2 online performance among the compared methods. \nTake Adaboost as an example, \\sys gets 25 top2 results among 30 tasks, while this number is 13 for RGPE.\nRGPE gets a similar performance with TST on Lightgbm and Extra Trees, but its performance decreases on Adaboost. Thus, RGPE is not stable in this scenario.\nCompared with the baselines, \\sys could achieve more stable and satisfactory performance in the dynamic setting. \n\n\n"
                },
                "subsection 5.3": {
                    "name": "Applying \\sys to NAS",
                    "content": "\n\\label{sec:apply_nas}\nTo investigate the universality of \\sys in conducting Neural Architecture Search (NAS), here we use \\sys to extract and integrate the optimization knowledge from NAS tasks on CIFAR-10 and CIFAR-100 (with 50 trials each) to accelerate the NAS task on ImageNet with NAS-Bench201~\\cite{dong2019bench}. From Figure~\\ref{fig:nas}, we have that \\sys could achieve more than 5x speedups over the state-of-the-art NAS methods -- Bayesian Optimization (BO) and Regularized Evolution Algorithm (REA)~\\cite{real2019regularized}.\nTherefore, \\sys can also be applied to the NAS tasks.\n\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Ablation Studies",
                    "content": "\n\\label{sec:abla}\n\\para{Source Knowledge Learning.}\nThis experiment is designed to evaluate the performance of source surrogate $M^{S}$ learned in Phase 1. \n$M^{S}$ corresponds to the source knowledge extracted from the source tasks. \nIn this setting, the source surrogate is used to guide the optimization of hyperparameters instead of the final TL surrogate $M^{TL}$. \nThe quality of source knowledge learned by each TL method thus can be measured by the performance of $M^{S}$. \nFigure~\\ref{source_ext_exp3} shows the results of \\sys and three one-phase framework based methods: POGPE, TST, and RGPE on two benchmarks --- Adaboost and LightGBM. \nWe can observe that the proposed \\sys outperforms the other three baselines on both two metrics: average rank and ADTM.\nAccording to some heuristics, these baselines calculate the weights in $M^{S}$ independently. \nInstead, by solving the constrained optimization problem, \\sys can learn the optimal weights in $M^{s}$ in a joint and principled manner. \nMore results on the other two benchmarks can be found in Appendix~\\ref{a.3}.\n\n\n\\para{Target Weight Analysis.}\nHere we compare the target weight obtained in POGPE, RGPE, TST, and \\sys.\nFigure~\\ref{weight_rf} and ~\\ref{weight_ada} illustrate the trend of target weight on two benchmarks: Random Forest and Adaboost. \nThe target weight in POGPE is fixed to a constant - 0.5, regardless of the increasing number of trials; TST's remains low even when the target observations are sufficient; RGPE's shows a trend of fluctuation because the sampling-based ranking loss is not stable. \n\\sys's keeps increasing with the number of trials, which matches the intuition that the importance of the target surrogate should be low when target observations are insufficient and gradually increase as target observations grow.\n\n\\para{Scalability Analysis.}\nIn the static TL setting, we include different number of source tasks when conducting transfer learning (See Figures~\\ref{offline_exp1} and~\\ref{offline_exp2}, where $N_{task}$ = 5 and $N_{task}$ = 29); the stable and effective results show the scalability in terms of the number of source tasks.\nWe further investigate the optimization overhead of suggesting a new configuration, and measure the runtime of the baselines - POGPE, RGPE, TST, SCoT, and \\sys on Random Forest with 75 trials. \nTo investigate the scalability of \\sys, we measure the runtime of the competitive TL methods: POGPE, RGPE, TST, SCoT, and \\sys. \nEach method is tested on Random Forest with 75 trials, and we repeat each method 20 times.\nFigure \\ref{runtime} shows the experiment results, where the y-axis is the mean cumulative runtime in seconds on a log scale.\nWe do not take the evaluation time of each configuration into account, and only compare the optimization overhead of suggesting a new configuration. \nScoT's runtime increases rapidly among the compared methods as it has the $O(k^3n^3)$ complexity.\nSince both the two-phase and one-phase framework-based methods own the $O(n^3)$ complexity, it takes nearly the same optimization overhead for TST, POGPE, and \\sys to suggest a configuration in the first 75 trials.\nAlthough RGPE also has the $O(n^3)$ complexity, it depends on a sampling strategy to compute the surrogate weight, which introduces extra overhead to configuration suggestion.\nInstead, \\sys exhibits a similar scalability result like POGPE, which incorporates no optimization overhead due to the constant weights. \nThis shows that \\sys scales well in both the number of trials and tasks.\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nIn this paper, we introduced \\sys, a novel two-phase transfer learning (TL) method for hyperparameter optimization (HPO), which can leverage the auxiliary knowledge from previous tasks to accelerate the HPO process of the current task effectively.\nThis framework can extract and aggregate the source and target knowledge jointly and adaptively.\nIn addition, we published a large-scale TL benchmark for HPO with up to 1.8 million model evaluations; the extensive experiments, including static and dynamic transfer learning settings and neural architecture search, demonstrate the superiority of \\sys over the state-of-the-art methods.\n\n\\begin{acks}\nThis work was supported by the National Natural Science Foundation of China (No.61832001), Beijing Academy of Artificial Intelligence (BAAI), PKU-Tencent Joint Research Lab. Bin Cui is the corresponding author.\n\\end{acks}\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{reference}\n\n\\clearpage\n\n\\appendix\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n",
                "subsection 7.1": {
                    "name": "The Details of Benchmark",
                    "content": "\n\\label{a.1}\nAs described in Section~\\ref{sec:exp_sec}, we create a benchmark to evaluate the performance of TL methods. \nWe choose four ML algorithms that are widely used in data analysis, including Random Forest, Extra Trees, Adaboost and Lightgbm. \nThe implementation of each algorithm and the design of their hyperparameter space follows Auto-sklearn. \nFor each algorithm, the range and default value of each hyperparameter are illustrated in Tables ~\\ref{hp_adaboost}, \\ref{hp_trees} and \\ref{hp_lgb}. \nTo collect sufficient source HPO data for transfer learning, we select 30 real-world datasets from OpenML repository, and evaluate the validation performance (i.e., the balanced accuracy) of 20k configurations for each benchmark, which are randomly sampled from the hyperparameter space. \nThe datasets used in our benchmarks are of medium size, whose number of rows ranges from 2000 to 8192. For more details, see Table~\\ref{cls_datasets}. \nThe total number of model evaluations (observations) in our benchmarks reaches 1.8 million and it takes more than 100k CPU hours to evaluate all the configurations. \nFor reproduction purposes, we also upload the benchmark data (e.g., evaluation results and the corresponding scripts) along with this submission.\nThe benchmark data (with size  477.7Mb); due to the space limit (maximum 20Mb) on CMT3, we only upload a small subset of benchmark on one algorithm  LightGBM.\nAfter the review process, we will make the complete benchmark publicly available (e.g., on Google Drive).\n\n\n\n\n\n\n\n\n% \\begin{table}[h]\n% \\centering\n% \\begin{tabular}{lccc}\n%     \\toprule\n%     Hyperparameter & Range  & Default \\\\\n%     \\midrule\n%     loss & \\{hinge, squared\\_hinge\\} & squared\\_hinge \\\\\n%     penalty & \\{L1, L2\\} & L2 \\\\\n%     dual & \\{True, False\\} & True \\\\\n%     tol & [$1\\times 10^{-5}$, $1\\times 10^{-1}$] & $1\\times 10^{-4}$ \\\\\n%     C & [0.03125, 32768] & 1 \\\\\n%     \\bottomrule\n% \\end{tabular}\n% \\caption {Hyperparameters of Liblinear\\_svm}\n% \\label{hp_svm}\n% \\end{table}\n\n\n\n\n\n\n\n% \\begin{table}[h]\n% \\centering\n% \\resizebox{0.8\\columnwidth}{!}{\n% \\begin{tabular}{cccc}\n%     \\toprule\n%     Type of Classifier & \\#$\\lambda$ & cat (cond) & cont (cond) \\\\ \n%     \\midrule\n%     AdaBoost & 4 & 1 (-) & 3 (-) \\\\\n%     Random forest & 5 & 2 (-) & 3 (-) \\\\\n%     Extra trees & 5 & 2 (-) & 3 (-) \\\\\n%     Liblinear SVM & 5 & 3 (3) & 2 (-) \\\\\n%     Lightgbm & 6 & 0 (-) & 6 (-) \\\\\n%     \\bottomrule\n% \\end{tabular}\n% }\n% \\caption {The number of hyperparameters in each ML algorithm. We distinguish between categorical (cat) hyperparameters with discrete values and continuous (cont) numerical hyperparameters. Numbers in brackets are conditional hyperparameters.}\n% \\label{hp_algos}\n% \\end{table}\n\n\n\n"
                },
                "subsection 7.2": {
                    "name": "Feasibility of Transfer Learning",
                    "content": "\n\\label{a.2}\nTo verify the feasibility of transfer learning in the setting of HPO, we conduct an HPO experiment on two datasets --- quake and hypothyroid(2). \nWe tune the learning rate and n\\_estimators of Adaboost while fixing the other hyperparameters, and then evaluate the validation performance (the balanced accuracy) of each configuration. \nFigure~\\ref{task_heatmap} shows the performance on 2500 Adaboost configurations, where deeper color means better performance. \n\nIt is quite clear that the optimal configuration differs on the two datasets (tasks), which means re-optimization is essential for HPO. \nHowever, the performance distribution is somehow similar on the two datasets. \nFor example, they both perform badly in the lower-right region and perform well in the upper region. \nBased on this observation, it is natural to accelerate the re-optimization process with the auxiliary knowledge acquired from the previous tasks.\n\n\n"
                },
                "subsection 7.3": {
                    "name": "More Experiment Results",
                    "content": "\n\\label{a.3}\nIn this section, we provide more experiment results besides those in Section~\\ref{sec:exp_sec}.\n\n\\para{Static Experiments}\nFigure~\\ref{offline_exp1_adtm} shows the results of all considered methods on the four benchmarks, where the metric is ADTM.\nWe can observe that the proposed \\sys exhibits strong stability, and performs well across benchmarks.\n\n\\para{Source Knowledge Learning}\nThe additional results on Random Forest and Extra Trees are illustrated in Figure~\\ref{source_ext_exp3_2}. \nSimilar to the findings in Section~\\ref{sec:abla}, \nour method - \\sys shows excellent ability in extracting and integrating the source knowledge from previous tasks.\n\n\n"
                },
                "subsection 7.4": {
                    "name": "Convergence Discussion about \\sys",
                    "content": "\n\\label{converge_analysis}\nIn \\sys, when sufficient trials on the target task are obtained, the weight of target surrogate $p^{T}$ will approach 1 as the HPO proceeds. \nBased on the mechanism we adopted in \\sys --- cross-validation, we can observe that $p^{T}_{i}$ in the $i$-th trial will approach 1 as $i$ increases. \nTherefore, the final TL surrogate $M^{TL}$ will be set to the target surrogate $M^{T}$.\nSo we can have that, \\newline\n\\emph{With sufficient trials, the final TL surrogate will find the same optimum as the target surrogate does; that's, the final solution of surrogate $M^{TL}$ will be no worse than the one in $M^{T}$ given sufficient trials.} \\newline\nThe above finding demonstrates that \\sys can alleviate negative transfer~\\cite{pan2010survey}. In other words, it can avoid performance degradation compared with non-transfer methods -- the traditional BO methods.\n\n\n"
                }
            },
            "section 8": {
                "name": "Reproduction Details",
                "content": "\n\\label{reproduction}\nThe source code and the benchmark data are available in the compressed file {\\em ``benchmark\\_data\\_and\\_source\\_code.zip''} on CMT3. \nThe source code is also available in the anonymous repository~\\footnote{https://anonymous.4open.science/r/TransBO-EE01/} now.\nAll files in the benchmark should be placed under the folder {\\em `data/hpo\\_data'} of the project root directory.\nTo reproduce the experimental results in this paper, an environment of Python 3.6+ is required. We introduce the experiment scripts and installation of required tools in \\emph{README.md} and list the required Python packages in \\emph{requirements.txt} under the root directory. \nTake one experiment as an example, to evaluate the static TL performance of \\sys and other baselines on Random Forest using 29 source tasks with 75 trials, you need to execute the following script: \\newline\n{\\em\npython tools/static\\_benchmark.py --trial\\_num 75 --algo\\_id random\\_forest --methods rgpe,pogpe,tst,transbo --num\\_source\\_problem 29\n}\n\nPlease check the document \\emph{README.md} in this repository for more details, e.g., how to use the benchmark, and how to run the other experiments.\n\n"
            }
        },
        "tables": {
            "cmp-table": "\\begin{table}[tb]\n\\centering\n\\small\n\\caption{The summary of related methods. `Y' in column meta-feature indicates that the method needs meta features; `Y' in column hyperparameter corresponds to the approach with sensitive hyperparameters.}\n% \\vskip -0.1in\n  \\resizebox{0.8\\columnwidth}{!}{\n  \\begin{tabular}{l|ccccc}\n    \\toprule\nMethod  & \\tabincell{c}{Transfer \\\\model}& \\tabincell{c}{Comp\\\\lexity} & \\tabincell{c}{Meta\\\\feature} & \\tabincell{c}{Hyper\\\\parameter}  & \\tabincell{c}{Safe\\\\ness} \\\\\n\\hline\nFMLP   & \\multirow{3}{*}{\\tabincell{c}{single model}}  & -       & N & Y & N\\\\\nSCoT   &    & $\\mathcal{O}(k^3n^3)$ & Y       & N & N\\\\\nMKL-GP &    & $\\mathcal{O}(k^3n^3)$ & Y       & N  & N\\\\\n\\hline \nPOGPE  & \\multirow{4}{*}{\\tabincell{c}{heuristics-based}}   & $\\mathcal{O}(n^3)$ & N       & N & N\\\\\nSGPR &  & $\\mathcal{O}(n^3)$ & N & Y  & N\\\\\nTST    &  & $\\mathcal{O}(n^3)$ & N & Y & N\\\\\nRGPE   &  & $\\mathcal{O}(n^3)$ & N & Y  & N \\\\\n\\hline\n\\textbf{\\sys}  & \\tabincell{c}{learning-based} & $\\mathcal{O}(n^3)$  & N & N  & Y \\\\\n\\bottomrule\n  \\end{tabular}\n  }\n  \\label{cmp-table}\n%   \\vskip -0.15in\n\\end{table}",
            "online-table": "\\begin{table}[htb]\n\\centering\n\\caption{Dynamic TL results for Tuning four ML algorithms.}\n% \\vskip -0.8em\n\\small\n\\resizebox{1\\columnwidth}{!}{\n  \\begin{tabular}{lcccccccc}\n    \\toprule\n\\multirow{2}*{Method}  & \\multicolumn{2}{c}{Adaboost} & \\multicolumn{2}{c}{Random Forest} & \\multicolumn{2}{c}{Extra Trees} & \\multicolumn{2}{c}{LightGBM} \\\\\n& 1st & 2nd & 1st & 2nd & 1st & 2nd & 1st & 2nd \\\\\n\\hline\nPOGPE & 0 & 2 & 0 & 1 & 0 & 2 & 1 & 2 \\\\\n\\hline \nTST & 8 & 12 & 9 & 9 & 7 & 12 & 10 & 9 \\\\\n\\hline \nRGPE & 8 & 5 & 6 & 14 & 10 & 9 & 9 & 10 \\\\\n\\hline\n\\textbf{\\sys} & \\textbf{14} & 11 & \\textbf{15} & 6 & \\textbf{14} & 7 & \\textbf{12} & 10 \\\\\n\\bottomrule\n  \\end{tabular}\n}\n\\label{online-table}\n% \\vskip -0.8em\n\\end{table}",
            "hp_adaboost": "\\begin{table}[h]\n\\centering\n\\small\n\\begin{tabular}{lccc}\n    \\toprule\n    Hyperparameter & Range & Default \\\\\n    \\midrule\n    n\\_estimators & [50, 500] & 50 \\\\\n    learning\\_rate (log) & [0.01, 2] & 0.1 \\\\\n    algorithm & \\{SAMME.R, SAMME\\} & SAMME.R \\\\\n    max\\_depth & [2, 8] & 3 \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption {Hyperparameters of Adaboost.}\n\\label{hp_adaboost}\n\\end{table}",
            "hp_trees": "\\begin{table}[h]\n\\centering\n\\small\n\\begin{tabular}{lccc}\n    \\toprule\n    Hyperparameter & Range  & Default \\\\\n    \\midrule\n    criterion &  \\{gini, entropy\\} & gini \\\\\n    max\\_features & [0, 1] & 0.5 \\\\\n    min\\_sample\\_split & [2, 20] & 2 \\\\\n    min\\_sample\\_leaf & [1, 20] & 1 \\\\\n    bootstrap & \\{True, False\\} & True \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption {Hyperparameters of Random Forest and Extra Trees.}\n\\label{hp_trees}\n\\end{table}",
            "hp_lgb": "\\begin{table}[h]\n\\centering\n\\small\n\\begin{tabular}{lccc}\n    \\toprule\n    Hyperparameter & Range  & Default \\\\\n    \\midrule\n    n\\_estimators & [100, 1000] & 500 \\\\\n    num\\_leaves & [31, 2047] & 127 \\\\\n    learning\\_rate (log) & [0.001, 0.3] & 0.1 \\\\\n    min\\_child\\_samples & [5, 30] & 20 \\\\\n    subsample & [0.7, 1] & 1 \\\\\n    colsample\\_bytree & [0.7, 1] & 1 \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption {Hyperparameters of LightGBM.}\n\\label{hp_lgb}\n\\end{table}",
            "cls_datasets": "\\begin{table}[h]\n\\centering\n\\small\n\\resizebox{0.8\\columnwidth}{!}{\n\\begin{tabular}{cccc}\n    \\toprule\n    Name & \\#Rows & \\#Columns & \\#Categories \\\\ \n    \\midrule\n    balloon & 2001 & 1 & 2 \\\\\n    kc1 & 2109 & 21 & 2 \\\\\n    quake & 2178 & 3 & 2 \\\\\n    segment & 2310 & 19 & 7 \\\\\n    madelon & 2600 & 500 & 2 \\\\\n    space\\_ga & 3107 & 6 & 2 \\\\\n    splice & 3190 & 60 & 3 \\\\\n    kr-vs-kp & 3196 & 36 & 2 \\\\\n    sick & 3772 & 29 & 2 \\\\\n    hypothyroid(1) & 3772 & 29 & 4 \\\\\n    hypothyroid(2) & 3772 & 29 & 2 \\\\\n    pollen & 3848 & 5 & 2 \\\\\n    analcatdata\\_supreme & 4052 & 7 & 2 \\\\\n    abalone & 4177 & 8 & 26 \\\\\n    spambase & 4600 & 57 & 2 \\\\\n    winequality\\_white & 4898 & 11 & 7 \\\\\n    waveform-5000(1) & 5000 & 40 & 3 \\\\\n    waveform-5000(2) & 5000 & 40 & 2 \\\\\n    page-blocks(1) & 5473 & 10 & 5 \\\\\n    page-blocks(2) & 5473 & 10 & 2 \\\\\n    optdigits & 5610 & 64 & 10 \\\\\n    satimage & 6430 & 36 & 6 \\\\\n    wind & 6574 & 14 & 2 \\\\\n    musk & 6598 & 167 & 2 \\\\\n    delta\\_ailerons & 7129 & 5 & 2 \\\\\n    mushroom & 8124 & 22 & 2 \\\\\n    puma8NH & 8192 & 8 & 2 \\\\\n    cpu\\_small & 8192 & 12 & 2 \\\\\n    cpu\\_act & 8192 & 21 & 2 \\\\\n    bank32nh & 8192 & 32 & 2 \\\\\n    \\bottomrule\n\\end{tabular}\n}\n\\caption {Details of 30 datasets used in the benchmarks.}\n\\label{cls_datasets}\n\\end{table}"
        },
        "figures": {
            "offline_exp1": "\\begin{figure*}[htb]\n\t\\centering\n\t\\subfigure[Random Forest]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp1_random_forest_75_result.pdf}\n\t\t\t\\label{offline_exp1_rf}\n\t}}\n\t\\subfigure[LightGBM]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp1_lightgbm_75_result.pdf}\n\t\t\t\\label{offline_exp1_lgb}\n\t}}\n\t\\subfigure[Adaboost]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp1_adaboost_75_result.pdf}\n\t\t\t\\label{offline_exp1_adb}\n\t}}\n    \\subfigure[Extra Trees]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp1_extra_trees_75_result.pdf}\n\t\t\t\\label{offline_exp1_ext}\n\t}}\n\t\\caption{Static TL results for four algorithms with $N_{task} = 29$ source tasks.}\n  \\label{offline_exp1}\n\\end{figure*}",
            "offline_exp2": "\\begin{figure*}[htb]\n\t\\centering\n\t\\subfigure[Random Forest]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\\includegraphics[width=1\\linewidth]{images/exp2_random_forest_75_result.pdf}\n\t\t\t\\label{offline_exp2_rf}\n\t}}\n\t\\subfigure[LightGBM]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp2_lightgbm_75_result.pdf}\n\t\t\t\\label{offline_exp2_lgb}\n\t}}\n\t\\subfigure[Adaboost]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp2_adaboost_75_result.pdf}\n\t\t\t\\label{offline_exp2_adb}\n\t}}\n    \\subfigure[Extra Trees]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp2_extra_trees_75_result.pdf}\n\t\t\t\\label{offline_exp2_ext}\n\t}}\n\t\\caption{Static TL results for four algorithms with $N_{task} = 5$ source tasks}\n\n  \\label{offline_exp2}\n\\end{figure*}",
            "source_ext_exp3": "\\begin{figure*}[tb]\n\t\\centering\n\t\\subfigure[NAS on NASBench201]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/transfer.pdf}\n\t}\n\t\\label{fig:nas}\n\t}\n\t\\subfigure[Source Knowledge Learning]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/source_extraction/exp3_lightgbm_50_rank_result.pdf}\n\t}\n\t\\label{fig:source_trans}\n\t}\n\t\\subfigure[Target Weight]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/weight_ada.pdf}\n\t}\n\t\\label{fig:target_weight}\n\t}\n\t\\subfigure[Scalability Test]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/time.pdf}\n\t}\n\t\\label{fig:scalability}\n\t}\n% \t\\vskip -0.2em\n\t\\caption{Results on neural architecture search and ablation studies.}\n% \t\\vskip -0.8em\n  \\label{source_ext_exp3}\n\\end{figure*}",
            "algo_exp": "\\begin{figure*}[htb]\n\t\\centering\n\t\\subfigure[Random Forest (\\emph{cpu\\_small})]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/weight_rf.pdf}\n\t\t\t\\label{weight_rf}\n\t}}\n\t\\subfigure[Adaboost (\\emph{hypothyroid(2)})]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/weight_ada.pdf}\n\t\t\t\\label{weight_ada}\n\t}}\n\t\\subfigure[Scalability on Random Forest]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/time.pdf}\n\t\t\t\\label{runtime}\n\t}}\n\t\\caption{Target weight and scalability analysis.}\n  \\label{algo_exp}\n\\end{figure*}",
            "fig:nas": "\\begin{figure}[htb]\n\t\\centering\n\t\\scalebox{0.7}{\n\t  \\includegraphics[width=1\\linewidth]{images/transfer.pdf}\n\t}\n\t\\caption{Results on optimizing NAS on NASBench201.}\n\t\\label{fig:nas}\n\\end{figure}",
            "offline_exp1_adtm": "\\begin{figure*}[htb]\n\t\\centering\n\t\\subfigure[Random Forest (ADTM)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp1_random_forest_75_adtm_result.pdf}\n\t}}\n\t\\subfigure[LightGBM (ADTM)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp2_lightgbm_75_adtm_result.pdf}\n\t}}\n\t\\subfigure[Adaboost (ADTM)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp1_adaboost_75_adtm_result.pdf}\n\t}}\n\t\\subfigure[Extra Trees (ADTM)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp1_extra_trees_75_adtm_result.pdf}\n\t}}\n\t\\vskip -0.1in\n\t\\caption{Static results on four benchmarks with 75 trials.}\n\t\\vskip -0.15in\n  \\label{offline_exp1_adtm}\n\\end{figure*}",
            "source_ext_exp3_2": "\\begin{figure*}[htb]\n\t\\centering\n\t\\subfigure[Random Forest (Average Rank)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/source_extraction/exp3_random_forest_50_rank_result.pdf}\n\t}}\n\t\\subfigure[Random Forest (ADTM)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/source_extraction/exp3_adaboost_50_adtm_result.pdf}\n\t}}\n\t\\subfigure[Extra Trees (Average Rank)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/source_extraction/exp3_extra_trees_50_rank_result.pdf}\n\t}}\n    \\subfigure[Extra Trees (ADTM)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/source_extraction/exp3_extra_trees_50_adtm_result.pdf}\n\t}}\n\t\\vskip -0.1in\n\t\\caption{Results on source knowledge learning.}\n  \\label{source_ext_exp3_2}\n\\end{figure*}",
            "task_heatmap": "\\begin{figure} \n    \\subfigure[quake]{\n     \\begin{minipage}[h]{0.46\\linewidth}\n        \\centering   \n        \\includegraphics[width=1.\\linewidth]{images/quake.pdf}\n        \\vskip -0.15in\n     \\end{minipage}\n    }\n  \\subfigure[hypothyroid(2)]{\n     \\begin{minipage}[h]{0.46\\linewidth}   \n        \\centering\n        \\includegraphics[width=1.\\linewidth]{images/hypothyroid2.pdf}\n        \\vskip -0.15in\n     \\end{minipage}\n    }\n    \\vskip -0.15in\n    \\caption{Performance of 2500 Adaboost configurations on two tasks, in which each hyperparameter has 50 settings.}\n    \\label{task_heatmap}\n    \\vskip -0.1in\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{eq_ei}\na(\\bm{x}; M)=\\int_{-\\infty}^{\\infty} \\max(y^{\\ast}-y, 0)p_{M}(y|\\bm{x})dy,\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    M^S = \\texttt{agg}(\\{M^1,...,M^K\\}; {\\bf w}).\n\\nonumber\n\\end{equation}",
            "eq:3": "\\begin{equation}\nM^{TL} = \\texttt{agg}(\\{M^S, M^T\\}; {\\bf p}).\n\\nonumber\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{fS}\nM^S(\\bm{x}) = \\sum_{i=1}^K{w}_{i}M^i(\\bm{x}),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\begin{aligned}\n    & \\mathbb{L}({\\bf w}, M^{S}; D) = \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1,y_j<y_k}^{n}\\phi(M^S(\\bm{x}_k) - M^S(\\bm{x}_j)), \\\\\n    & \\phi(z) = log(1 + e^{-z}), \\\\\n\\end{aligned}\n\\label{ranking_loss}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\label{eq:opt_source}\n    \\begin{aligned}\n        & \\underset{{\\bf w}}{\\text{minimize}}\n        & & \\mathbb{L}({\\bf w}, M^{S}; D) \\\\\n        & \\text{s.t.}\n        & & \\bm{1}^\\top{\\bf w}=1, {\\bf w}\\ge\\bm{0}, \\\\\n    \\end{aligned}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\begin{aligned}\n    & \\frac{\\partial \\mathbb{L}}{\\partial {\\bf w}} = \\sum_{(j, k) \\in \\mathbb{P}}\\frac{neg\\_ez}{1 + neg\\_ez} \\ast (A_{[j]} - A_{[k]}), \\\\\n    & neg\\_ez = e^{(A_{[j]}{\\bf w} - A_{[k]}{\\bf w})}, \\\\\n\\end{aligned}\n\\label{der_loss}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\begin{aligned}\n    M^{TL}_{-i} = {p}^S M^S_{-i} + {p}^T M^T_{-i},\\\\\n\\end{aligned}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\begin{aligned}\n    & \\mathbb{L}_{cv}({\\bf p}, M^{TL}_{-i}; D^T) = \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1,y_j^T<y_k^T, k \\in D^T_{i}}^{n}\\phi(z), \\\\\n    & \\phi(z) = log(1 + e^{-z}), z=M^{TL}_{-i}(\\bm{x}_k) - M^{TL}_{-H(j)}(\\bm{x}_j)\\\\\n\\end{aligned}\n\\label{ranking_loss_cv}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\label{eq:opt_target}\n    \\begin{aligned}\n        & \\underset{{\\bf p}}{\\text{minimize}}\n        & & \\sum_{i=1}^{N_{cv}} \\mathbb{L}_{cv}({\\bf p}, M^{TL}_{-i}; D^T) \\\\\n        & \\text{s.t.}\n        & & \\bm{1}^\\top{\\bf p}=1, {\\bf p}\\ge\\bm{0}.\\\\\n    \\end{aligned}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n\\small\nADTM(\\mathcal{X}_t) = \\frac{1}{K}\\sum_{i \\in [1:K]}\\frac{min_{\\bm{x} \\in \\mathcal{X}_t} y^{i}_{\\bm{x}} - y^{i}_{min}}{y^i_{max} - y^{i}_{min}},\n\\end{equation}"
        }
    }
}