{
    "meta_info": {
        "title": "Improving Temporal Link Prediction via Temporal Walk Matrix Projection",
        "abstract": "Temporal link prediction, aiming at predicting future interactions among\nentities based on historical interactions, is crucial for a series of\nreal-world applications. Although previous methods have demonstrated the\nimportance of relative encodings for effective temporal link prediction,\ncomputational efficiency remains a major concern in constructing these\nencodings. Moreover, existing relative encodings are usually constructed based\non structural connectivity, where temporal information is seldom considered. To\naddress the aforementioned issues, we first analyze existing relative encodings\nand unify them as a function of temporal walk matrices. This unification\nestablishes a connection between relative encodings and temporal walk matrices,\nproviding a more principled way for analyzing and designing relative encodings.\nBased on this analysis, we propose a new temporal graph neural network called\nTPNet, which introduces a temporal walk matrix that incorporates the time decay\neffect to simultaneously consider both temporal and structural information.\nMoreover, TPNet designs a random feature propagation mechanism with theoretical\nguarantees to implicitly maintain the temporal walk matrices, which improves\nthe computation and storage efficiency. Experimental results on 13 benchmark\ndatasets verify the effectiveness and efficiency of TPNet, where TPNet\noutperforms other baselines on most datasets and achieves a maximum speedup of\n$33.3 \\times$ compared to the SOTA baseline. Our code can be found at\n\\url{https://github.com/lxd99/TPNet}.",
        "author": "Xiaodong Lu, Leilei Sun, Tongyu Zhu, Weifeng Lv",
        "link": "http://arxiv.org/abs/2410.04013v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "NeurIPS 2024 Paper"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Intorduction",
                "content": " \\label{sec:introduction}\nMany real-world dynamic systems can be abstracted as a temporal graph \\cite{DBLP:holme2012temporal}, where entities and interactions among them are denoted as nodes and edges with timestamps respectively. Temporal link prediction, aiming at predicting future interactions based on historical interactions, is a fundamental task for temporal graph learning, which can not only help us understand the evolution pattern of the temporal graph but also is crucial for a series of real-world tasks such as recommendations for online platforms \\cite{DBLP:conf/cikm/FanLZX0Y21,DBLP:conf/kdd/KumarZL19} and information diffusion prediction \\cite{DBLP:journals/csur/ZhouXTZ21,DBLP:conf/ijcai/LuJ0S0Z23}.\n\nRelative encodings have become an indispensable module for effective temporal link prediction \\cite{DBLP:conf/iclr/WangCLL021,DBLP:conf/log/LuoL22,DBLP:conf/nips/SouzaMKG22,DBLP:conf/nips/Yu23} where, without them, node representations computed independently by neighbor aggregation will fail to capture the pairwise information. As the toy example shown in \\figref{fig:node-wise_limit}, A and F will have the same node representation due to sharing the same local structure. Thus it can not be determined whether D will interact with A or F at $t_3$ according to their representations. However, by assigning nodes with relative encodings (i.e., additional node features) specific to the target link before computing the node representation, we can highlight the importance of each node and guide the representation learning process to extract pairwise information. For example, in Figure 1, we can infer from the relative encoding of E (in red circle) that D is more likely to interact with F than with A since D and F share a common neighbor, E (detailed discussed in \\secref{sec:relative_encoding}). Although achieving remarkable success, injecting pairwise information based on relative encodings is still far from \nsatisfactory. \n\n1) \\textbf{On Concept}, existing ways of constructing relative encodings are fragmented as they are derived from different heuristics. There still lacks a unified view on the connections between different relative encodings, which may allow a more principled way to design new relative encodings. 2) \\textbf{On Method Design}, most existing relative encodings are constructed based on structural connectivity between nodes (e.g., whether two nodes are neighbors), while the temporal information is ignored.  3) \\textbf{On Computation}, existing methods for relative encodings are inefficient, which usually involve time-consuming graph query operations and need to re-extract the relative encoding from scratch for each target link, making them even become the main computational bottleneck for some methods (shown in \\secref{sec:performance_efficiency}). \n\nTo tackle the above issues, this paper makes the following three technical contributions. 1) \\textbf{A Unified View  (Concept)}. We analyze the construction of existing relative encodings and find that they can be uniformly viewed as a function of temporal random walk matrices, where different relative encodings essentially correspond to the construction of a series of temporal walk matrices based on temporal walk weighting. The presented function provides a unified view to analyze existing methods and may allow a more principal way to design new relative encodings. 2) \\textbf{A Effective and Efficient Method (Method Design and Computation)}. Based on our analysis, we propose a Time decay matrix Projection-based graph neural Network for temporal link prediction, named \\model~for short. \\model\\ introduces a new temporal walk matrix that incorporates the time decay effect of the temporal graph, simultaneously considering both temporal and structural information. Moreover, \\model~ encodes the temporal walk matrix into a series of node representations by random feature propagation, which can be efficiently updated when the graph structure changes and is storage-efficient. Importantly, such node representations can be shared by different link likelihood computation processes to decode the pairwise information, reducing the redundancy computation of re-extracting the relative encodings and avoiding time-consuming graph query operations. 3) \\textbf{Theoretical and Empirical Analysis.} We conduct a theoretical analysis of the node representations obtained through random feature propagation. Our analysis demonstrates that these representations stem from the random projection of the proposed temporal walk matrix while preserving the inner product of different rows of the matrix. Moreover, we discuss the conditions under which random feature propagation can be applied to improve the computational and storage efficiency of other temporal walk matrices and provide the corresponding propagation mechanisms for temporal walk matrices of existing methods. Empirically, we conduct experiments on 13 benchmark datasets to verify the effectiveness and efficiency of the proposed method, where \\model~ outperforms other baselines on most datasets and achieves a maximum speedup of $33.3\\times$ compared to the SOTA baseline. Detailed ablation studies also verify the effectiveness of the proposed submodules.\n\n\n\n\n\n\n% Existing methods for temporal link prediction usually follow the node-level paradigm, which compute the temporal representations of nodes in an interaction independently by aggregating the representations of neighbor nodes. However, the representations obtained in this way might fail to capture the correlation between nodes, which is important for temporal link prediction \\cite{DBLP:conf/nips/Yu23,DBLP:conf/iclr/WangCLL021}. As the toy example shown in \\figref{fig:node-wise_limit}, A and F will have the same node representation due to sharing the same local structure. Thus it can not be determined whether D will interact with A or F at $t_3$ according to their representations. However, considering the correlation of D and F that they share a common neighbor E, it can be inferred that D might have a higher probability of interacting with F than with A. Recently, some methods are proposed to tackle this issue (referred as link-wise methods) \\cite{DBLP:conf/nips/Yu23,DBLP:conf/log/LuoL22}, which inject the pair-wise information by assigning nodes with relative encodings specific to the target link. As shown in \\figref{fig:node-wise_limit}, from the relative encoding of E, we can know that D and F have a common neighbor while D and A do not (detailed discussed in \\secref{sec:relative_encoding}). However, injecting pairwise information based on relative encodings is still far from satisfactory.\n\n% However injecting pairwise information based on relative encodings is still far from \n% satisfactory. 1) \\textbf{On concept}, existing ways of constructing relative encodings are fragmented as they are derived from different heuristics. There still lacks a unified view on the connections between different relative encodings, which may allow a more principled way to design new relative encodings. 2) \\textbf{On method design}, most existing relative encodings are constructed based on structural connectivity between nodes (e.g., whether two nodes are neighbors), while the temporal information is ignored.  3) \\textbf{On computation}, existing methods for relative encodings are inefficient, which usually involve time-consuming graph query operations and need to re-extract the relative encoding from scratch for each target link, making them even become the main computational bottleneck for some methods (shown in \\figref{fig:efficiency}). \n\n% To tackle the above issues, we first analyze the construction of existing relative encodings and find that they can be uniformly viewed as a function of the temporal random walk matrix. Based on this analysis, we propose a \\textbf{T}ime decay matrix \\textbf{P}rojection-based graph neural \\textbf{Net}work for temporal link prediction (\\model). \\model~introduces a new temporal walk matrix based on the time decay effect of the temporal graph, which simultaneously considers the temporal and structural information. Moreover, \\model~ encodes the temporal walk matrix into a series of node representations by random feature propagation, which provably preserves the pair-wise information between nodes and can be efficiently updated when the graph structure changes. Importantly, such node representations can be shared by different link likelihood computation processes to decode the pair-wise information, reducing the redundancy computation of re-extracting the relative encodings. We conduct experiments on 13 benchmark datasets, where \\model~ outperform other baselines on most datasets and achieve a maximum speedup of $33.3\\times$ compared to the SOTA baseline. Detailed ablation studies also verify the effectiveness of the proposed submodules. Our contributions can be summarised as follows.\n% \\begin{itemize}\n%     \\item We unify existing construction ways of relative encodings into a function of temporal walk matrices, providing a new dimension to analyse and design relative encodings.\n    \n%     % We analyze existing ways to construct relative encodings for temporal link prediction and summarize them into a unified function of the temporal walk matrix, opening new opportunities for principally designing new relative encodings. \n    \n%     \\item A temporal graph neural network \\model~is proposed, which designs a new temporal walk matrix to simultaneously consider the temporal and structural information and a random feature propagation mechanism to improve computation and storage efficiency. \n    \n%     \\item We emperically evaluate \\model~on 13 benchmark datasets to verify its effectiveness and efficiency over existing temporal link prediciton methods.\n% \\end{itemize}\n\n\n% To tackle the above issues, we first analyze the construction of existing relative encodings and find that they can be uniformly viewed as a function of the temporal random walk matrix. Based on this analysis, we propose a \\textbf{T}ime decay matrix \\textbf{P}rojection-based graph neural \\textbf{Net}work for temporal link prediction (\\model), which encodes the pair-wise information between nodes into a series of dynamically evolving node representations. Specifically, \\model~ assigns each node a random feature and designs an updating mechanism to maintain a series of hierarchical node representations, which incremental updating the node representations when a new interaction occurs. The total time complexity of maintaining the hierarchical node representations is linear to the number of edges in the dynamic graph and the node representations are shared by different link likelihood computation process, making it more effective than existing link-wise methods. Moreover, the xx. Theoretical analysis shows the obtained node representations can be considered as the projection of a series of temporal random walk matrices and preserve the path information between any two nodes. Experimental results on 13 benchmark datasets verify the effectiveness and efficiency of the proposed method. Our contributions can be summarized as follows. 1) AA. 2) BB. 3) CC.\n\n\n"
            },
            "section 2": {
                "name": "Preliminary",
                "content": " \\label{sec:pre}\nIn this section, we will first formally define some important concepts in this paper and then give a unified formulation of existing relative encodings.\n",
                "subsection 2.1": {
                    "name": "Definitons",
                    "content": "\n\\begin{definition}[Temporal Graph] \\label{def:dynamic_graph}\n     A temporal graph can be considered as a sequence of non-decreasing chronological interactions $\\mathcal{G} = \\left[\\left(\\{u_1,v_1\\},t_1\\right), \\left(\\{u_2,v_2\\},t_2\\right), \\cdots  \\right]$ with $0 \\leq t_1 \\leq t_2 \\leq \\cdots $, where $(\\{u_i,v_i\\},t_i)$ can be considered as a undirected link between $u_i$ and $v_i$ with timestamp $t_i$.  Each node $u$ can be associated with node feature $\\bm{x}_u \\in \\mathbb{R}^{d_N}$, and each interaction $\\left(\\{u,v\\},t\\right)$ has link feature $\\bm{e}_{u,v}^t \\in \\mathbb{R}^{d_E}$, where $d_N$ and $d_E$ denote the dimensions of the node feature and link feature. \n     % If the graph is non-attributed, we simply set the node feature and link feature to zero vectors, i.e., $\\bm{x}_u=\\bm{0}$ and $\\bm{e}_{u,v}^t=\\bm{0}$. \n\n\\end{definition}\n\n\\begin{definition}[Temporal Link Prediction]\\label{def:dynamic_link_prediction} The interaction sequence reflects the graph dynamics, and thus the ability of a model to capture the evolution pattern of a dynamic graph can be evaluated by how accurately it predicts the future interactions based on historical interactions. Formally, given the interactions before $t$ (i.e., $\\{(\\{u',v'\\},t')| t' < t\\}$) and two nodes $u$, $v$, the temporal link prediction task aims to predict whether there will be an interaction between $u$ and $v$ at $t$.\n\\end{definition}\n\n\\begin{definition}[K-hop Subgraph]\\label{def:dynamic_link_prediction} We use the notation $\\mathcal{G}(t) = (\\mathcal{V}(t), \\mathcal{E}(t))$ to denote the graph snapshot at $t$, where $\\mathcal{E}(t)$ includes all the interactions that happen before $t$ and $\\mathcal{V}(t)$ includes all the nodes appear in $\\mathcal{E}(t)$. Besides, we defined the k-hop subgraph of node $u$ as $\\mathcal{G}_u^k(t)=(\\mathcal{V}_u^k(t), \\mathcal{E}_u^k(t)$, where $\\mathcal{V}^k_u(t) \\subset \\mathcal{N}(t)$ is the set of nodes whose shortest path distance to $u$ is less than $k$ on $\\mathcal{G}(t)$ and $\\mathcal{E}^k_u(t) \\subset \\mathcal{E}(t)$ is the set of interactions between $\\mathcal{V}^k_u(t)$.\n% We use $\\mathcal{V}_{u,t}^k$ to denote set of nodes whose shortest path to $u$ on $\\mathcal{G}_t$ is less than $k$ and $\\mathcal{S}_{u,t}^k$ to denote the induced subgraph.\n\\end{definition}\n\n\\begin{definition}[Temporal Walk]\\label{def:dynamic_link_prediction} A k-step temporal walk $W$ on $\\mathcal{G}(t)$ is a sequence of node-time pair with decreased temporal order \\cite{DBLP:conf/iclr/WangCLL021}, which can be denoted as $W = \\left[(w_0,t_0),\\cdots, (w_k,t_k)\\right]$ with $t= t_0 > t_1 > \\cdots > t_k$ and $(\\{w_i,w_{i+1}\\},t_{i+1})$ is an edge on $\\mathcal{G}(t)$ for $0 \\leq i \\leq k-1$.  \\figref{fig:temporal_walk} shows a visual illustration of a temporal walk. Here, we stipulate the first timestamp $t_0$ as the current time $t$ to avoid undefinedness of $t_0$. We use the notation $\\mathcal{M}^k_{u,v}(t)$ to denote the set of all k-step temporal walks from $u$ to $v$ on $\\mathcal{G}(t)$. Specially, $\\mathcal{M}_{u,w}^{0}(t)=\\{[(u,t)]\\}$ if $u=w$ and $\\mathcal{M}_{u,w}^{0}(t)=\\emptyset$ otherwise. When there is no ambiguity, we replace $\\mathcal{M}^k_{u,v}(t)$ with $\\mathcal{M}^k_{u,v}$.\n\\end{definition}\n\n"
                },
                "subsection 2.2": {
                    "name": "Relative Encoding for Dynamic Link Prediction",
                    "content": " \\label{sec:relative_encoding}\n",
                    "subsubsection 2.2.1": {
                        "name": "Unified Framework",
                        "content": "\nGiven a future link $(u,v,t)$ to be predicted, existing temporal link prediction methods (referred as node-wise methods) usually first learn the node representations $\\bm{h}_{u}(t)$ and $\\bm{h}_{v}(t)$ independently, which are obtained by encoding their k-hop subgraphs,\n\\begin{gather}\n \\bm{h}_u(t) = f_{\\text{enc}}(\\mathcal{G}_u^k(t),\\bm{X}^N_{u,k},\\bm{X}^E_{u,k}), ~~~~~ \\bm{h}_v(t) = f_{\\text{enc}}(\\mathcal{G}_v^k(t),\\bm{X}^N_{v,k},\\bm{X}^E_{v,k}), \\label{eq:node-wise}\n\\end{gather} \nwhere $\\bm{X}^N_{u,k}$ and $\\bm{X}^E_{u,k}$ are the features of nodes and edges in $\\mathcal{G}_u^k(t)$ respectively \\footnote{For memory-based methods (i.e, TGN), $\\bm{X}_{u,k}^N$ represents the concatenation of node memories and features.}. The $f_{\\text{enc}}(\\cdot)$ here can be any encoding function that maps a graph into a representation such as a k-layer GNN with a pooling layer  Then the link likelihood $p_{u,v}^t$ is given $p_{u,v}^t =f_{\\text{dec}}(\\bm{h}_u(t),\\bm{h}_v(t))$. The $f_{\\text{dec}}(\\cdot)$ is a decoding function that maps the node representations into link likelihood such as an MLP with a Sigmoid output layer. Detailed discussion about existing methods can be found in \\appendixref{sec:baselines}.\n\nAs mentioned in the \\secref{sec:introduction}, learning representations independently might fail to capture the pairwise information of the given link. Thus recent methods (referred as link-wise methods) inject the pairwise information by constructing a relative encoding $\\bm{r}^{w|(u,v)}$ for each node $w \\in \\mathcal{V}_u^k(t) \\cup \\mathcal{V}_v^k(t)$ as an additional node feature (Detailed construction way for different methods will be introduced in \\secref{sec:existing_methods}).  Then \\equref{eq:node-wise} will be changed into \n\\begin{equation}\n    \\bm{h}_u(t) = f_{\\text{enc}}(\\mathcal{G}_u^k(t),\\bm{X}^N_{u,k} \\oplus \\bm{X}^R_{u,k},\\bm{X}^E_{u,k}), ~~~~~ \\bm{h}_v(t) = f_{\\text{enc}}(\\mathcal{G}_v^k(t),\\bm{X}^N_{v,k} \\oplus \\bm{X}^R_{v,k},\\bm{X}^E_{v,k}), \\label{eq:link-wise}\n\\end{equation}\nwhere $\\bm{X}_{u,k}^R$ is the relative encodings for nodes in $\\mathcal{G}_{u}^k(t)$ and $\\bm{X}^N_{u,k} \\oplus \\bm{X}^R_{u,k}$ indicate the new node features obtained by concatenating $\\bm{X}^N_{u,k}$ and $\\bm{X}^R_{u,k}$.  Intuitively, the relative encoding $\\bm{r}^{w|(u,v)}$ for each node $w$ reflects its importance to predicted link $(u,v,t)$, which can guide the representation learning process to extract the pairwise information specific to the predicted link from the subgraph.\nFor the detailed construction way, the relative encoding $\\bm{r}^{w|(u,v)}$ is the concatenation of two similarity features $\\bm{r}^{w|u}$ and $\\bm{r}^{w|v}$, where $\\bm{r}^{w|u}/\\bm{r}^{w|v}$ encodes some form of similarity between $u/v$ and $w$ (e.g., the number of k-step temporal walks from $u$ to $w$). Although the designs of similarity feature $\\bm{r}^{w|u}$ for different methods are induced from different heuristics, we find that they can be unified in a function about the temporal walk matrix, which is\n\\begin{equation}\n    \\bm{r}^{w|u} = g([A^{(0)}_{u,w}(t), A^{(1)}_{u,w}(t),\\cdots,A^{(k)}_{u,w}(t)]),~~~~ A_{u,w}^{(i)}(t) = \\sum_{W \\in \\mathcal{M}^i_{u,w}} s(W) ~~\\text{for} ~~0 \\leq i \\leq k.\n \\label{eq:unified_func}\n\\end{equation}\nThe $s(\\cdot)$ here is a score function that maps a temporal walk to a scalar and $\\bm{A}^{(i)}(t)$ denotes an i-hop temporal walk matrix whose each entry $A^{(i)}_{u,w}$ is the sum of the score of all i-step temporal walks from $u$ to $w$. $g(\\cdot)$ is a function applied on the vector of $[A^{(0)}_{u,w}(t), A^{(1)}_{u,w}(t),\\cdots,A^{(k)}_{u,w}(t)] \\in \\mathbb{R}^{k+1}$ to extract similarity feature. The above equation shows that each relative encoding implies a construction of the temporal walk matrix based on weighting each temporal walk (i.e., $s(\\cdot)$). Next, we will analyze existing relative encodings and show how they can be represented in the form of \\equref{eq:unified_func}.\n\n"
                    },
                    "subsubsection 2.2.2": {
                        "name": "Analysis of Existing Methods",
                        "content": " \\label{sec:existing_methods}\nOur analysis focuses on the four representative link-wise methods DyGFormer, PINT, NAT, and CAW, which cover all the link-wise methods in the benchmark of dynamic link prediction \\cite{DBLP:conf/nips/Yu23}.\n\n\\textbf{DyGFormer} \\cite{DBLP:conf/nips/Yu23}. The $\\bm{r}^{w|u}$ of DyGFormer is a one-dimensional vector representing the number of links between $w$ and $u$. For DyGFormer, we can first set the $s(\\cdot)$ to be a one-const function (i.e., $s(W)=1$ in for any $W$), which will make $A^{(k)}_{u,w}$ be the number of the k-step temporal walks from $u$ to $w$. Then, setting $g(\\cdot)$ to be a function that selects the second number of a vector (i.e., $g([x_0,x_1,..,x_{k}])=x_1$) will make \\equref{eq:unified_func} generate the similarity feature of DyGFormer.\n\n\\textbf{PINT} \\cite{DBLP:conf/nips/SouzaMKG22}. The $\\bm{r}^{w|u}$ of PINT  is a ($k+1$)-dimensional vector, where $r^{w|u}_{i}$ is the number of $(i-1)$-step temporal walks from $u$ to $w$ for $1\\leq i \\leq k+1$. Setting $s(\\cdot)$ and $g(\\cdot)$ can be set to a one-const function and an identity function respectively will make \\equref{eq:unified_func} outputs the similarity feature of PINT. \n\n\\textbf{NAT} \\cite{DBLP:conf/log/LuoL22}. NAT maintains a series of fix-sized hash maps $s_u^{(0)},...,s_u^{(k)}$\nand generates the $\\bm{r}^{w|u}$ based on the hash maps. As proved in \\appendixref{sec:nat}, if the size of the hash maps becomes infinite, the $\\bm{r}^{w|u}$ is equivalent to a $(k+1)$-dimensional vector, where, for $1 \\leq i \\leq k+1$, $r^{w|u}_{i}=1$ if the shortest temporal walk from $u$ to $w$ is less than $i$; otherwise, $r^{w|u}_{i}=0$. Let $h(\\cdot)$ be a binary function where $h(x)=1$ if $x>0$ and $h(x)=0$ otherwise. Setting the $s(\\cdot)$ to be a one-const function and $g(\\cdot)$ to a function of $g([x_0,x_1,...,x_k])=[h(\\sum_{i=0}^0 x_i),h(\\sum_{i=0}^1 x_i),...,h(\\sum_{i=0}^k x_i)]$ can make the \\equref{eq:unified_func} generate the similarity feature of NAT.  \n\n\\textbf{CAWN} \\cite{DBLP:conf/iclr/WangCLL021}. \nThe similarity feature $\\bm{r}^{w|u}$ of CAWN reflects the probability of a node $w$ being visited when performing a temporal walk from $u$. Specifically, CAWN first samples a set of temporal walks beginning from $u$ based on a causal sampling strategy. Then, for each node $w$, the similarity feature $\\bm{r}^{w|u}$ is extracted based on its occurrence in the sampled walks. As proved in \\appendixref{sec:cawn}, the similarity feature $\\bm{r}^{w|u}$ is the estimation of a ($k+1$)-dimensional vector, where, for $1 \\leq i \\leq k+1$,  $r^{w|u}_{i}$ is the probability of visiting $w$ through a $(i-1)$-step temporal walk matrix based on the sampling strategy of CAWN, which can be represented as $r^{w|u}_{i}=\\sum_{W \\in \\mathcal{M}^{i-1}_{u,w}} s'(W)$. The value $s'(W)$ for a given temporal walk $W=[(w_0,t_0),(w_1,t_1),...,(w_k,t_k)]$ is defined as $\\prod_{i=0}^{k-1} \\frac{\\text{exp}(-\\alpha(t_{i}-t_{i+1}))}{\\sum_{(\\{w',w\\},t')\\in \\mathcal{E}_{w_i,t_i}} \\textrm{exp}(-\\alpha(t_i-t'))}$, where $\\alpha$ is hyperparameter to control the sampling process, $\\mathcal{E}_{w_i,t_i} = \\{(\\{w',w\\},t')| t' < t_i\\}$ is the set of interactions attached to $w_i$ before $t_i$, and $\\frac{\\text{exp}(-\\alpha(t_{i}-t_{i+1}))}{\\sum_{(\\{w',w\\},t')\\in \\mathcal{E}_{w_i,t_i}}}$ can be considered as the probability of moving from $(w_{i},t_{i})$ to $(w_{i+1},t_{i+1})$ in the sampling process. Setting  $s(\\cdot)$ to $s'(\\cdot)$ and $g(\\cdot)$ to be an identity function can make \\equref{eq:unified_func} generate the similarity feature of CAWN.\n\n\\textbf{Conclusion}. According to the above analysis, we can conclude that (i) \\equref{eq:unified_func} provides a unified view to consider the injection of pairwise information from the construction of temporal walk matrix, where different relative encodings (implicitly or explicitly) correspond to a kind of temporal walk matrix. (ii) Examining existing relative encodings from the unified view reveals their limitations. First, the relative encodings of existing methods (except CAWN) ignore the temporal information carried by each temporal walk, where their score function $s(\\cdot)$ always yield $1$ and the entries of the temporal walk matrix is just the count of the temporal walks. Next, although CAWN considers temporal information, they estimate the temporal walk matrix from the sampled temporal walks, which needs time-consuming graph sampling and may introduce large estimation errors. In the next section, we will present a new temporal walk matrix to simultaneously consider the temporal and structural information and show how to efficiently maintain the proposed temporal walk matrix.\n\n\n\n\n"
                    }
                }
            },
            "section 3": {
                "name": "Methodology",
                "content": " \\label{sec:methodology}\n\n\n\\model~ mainly consists of two modules: Node Representation Maintaining (NRM) and Link Likelihood Computing (LLC). The NRM is responsible for encoding the pairwise information, which maintains a series of node representations. Such representations will be updated when new interaction occurs and can be used to decode the temporal walk information between two nodes. The LLC module is a prediction module, which utilizes the maintained node representations and auxiliary information (e.g., like features) to compute the likelihood of the link to be predicted. \n\n",
                "subsection 3.1": {
                    "name": "Node Representation Maintaining",
                    "content": " \\label{sec:tmm}\n\\textbf{Temporal Matrix Construction}. Based on the \\equref{eq:unified_func}, designing a temporal walk matrix relies on the definition of the score function $s(\\cdot)$, where the element of a temporal walk matrix is $A_{u,v}^{(k)}(t) = \\sum_{W \\in M_{u,v}^k} s(W)$. Unlike most previous methods that only count the number of temporal walks, we consider the temporal information carried by a temporal walk in $s(\\cdot)$ to simultaneously consider the temporal and structural information. Formally, let $t$ be the current time, given a temporal walk $W=[(w_0,t_0),(w_1,t_1),..,(w_k,t_k)]$, the value of the score function is  $s(W)=\\prod_{i=1}^k e^{-\\lambda (t-t_i)}$, where $\\lambda > 0$ is a hyperparameter to control the time decay weight. As the current time $t$ goes on, for each interaction $(\\{w_i,w_{i+1}\\},t_{i+1})$ in the temporal walk  $W$, its weight $e^{-\\lambda (t-t_i)}$ will decay exponentially. The design of the score function is motivated by the widely observed time decay effect \\cite{DBLP:holme2012temporal,DBLP:conf/www/NguyenLRAKK18} on the temporal graph, where the importance of interactions will decay as time goes on, benefiting better modeling the graph evolution patterns. In the following part of  \\secref{sec:methodology}, the notation of $s(\\cdot)$ and $\\bm{A}^{(k)}(t)$ will specifically refer to the score function and temporal walk matrix proposed in this part. Besides, for $\\bm{A}^{(0)}(t)$, we stipulate it as an identity matrix constantly.\n\n\n \n\n\\textbf{Node Representation Maintaining}. Directly computing the temporal walk matrices is impractical since we need to enumerate the temporal walks and each matrix needs expensive $O(n \\times n)$ space complexity to store. Thus, we implicitly maintain the temporal walk matrices by maintaining a series of node representations $\\bm{H}^{(0)}(t),\\bm{H}^{(1)}(t),...,\\bm{H}^{(k)}(t) \\in \\mathbb{R}^{n\\times d_{R}}$, where $d_R \\ll n$ is the dimension of node representations and $\\bm{H}_u^{(l)}(t) \\in \\mathbb{R}^{d_R}$ encodes the information about the $l$-step temporal walks beginning from $u$ for $0 \\leq l \\leq k$. The node representations will be updated when a new interaction occurs. Specifically, we first construct a random feature matrix $\\bm{P} \\in \\mathbb{R}^{n\\times d_R}$, where each entry of $\\bm{P}$ is independently drawn from Gaussian distribution with mean 0 and variance $\\frac{1}{d_R}$. Then we initialize $\\bm{H}^{(0)}(t)$ as $\\bm{P}$ and $\\bm{H}^{(1)}(t),\\bm{H}^{(2)}(t),...,\\bm{H}^{(k)}(t)$ as zero matrix. When a new interaction $(u,v,t)$ occurs, for $1 \\leq l \\leq k$, we update the representations of $u$ and $v$ by\n\\begin{equation}\n  \\bm{H}^{(l)}_u(t^+) = \\bm{H}^{(l)}_u(t) + e^{\\lambda t}*\\bm{H}^{(l-1)}_v(t),~~~~~ \\bm{H}^{(l)}_v(t^+) = \\bm{H}^{(l)}_v(t) + e^{\\lambda t}*\\bm{H}^{(l-1)}_u(t), \n  \\label{eq:update}\n\\end{equation}\nwhere the $t^+$ denotes the time right after $t$. A pseudocode for maintaining the node representations is shown in \\algoref{alg:maintaining_sum}. The maintaining mechanism here can be considered as a random feature propagation mechanism on the temporal graph, where we initialize the zero layer's representation of each node as a random feature and repeatedly propagate these features among nodes from the low layer to the high layer as interactions continuously appear. The following theorem shows the relationship between the node representations and the temporal walk matrices.\n\n\n\\begin{theorem}\n\\label{theorem:random_projection} \nIf any two interactions on temporal graph $\\mathcal{G}$ have different timestamps, the obtained representations $\\bm{H}^{(0)}(t),\\bm{H}^{(1)}(t),...,\\bm{H}^{(k)}(t)$ satisfy $e^{-\\lambda lt}*\\bm{H}^{(l)}(t)= \\bm{A}^{(l)}(t)\\bm{P}$ for $0 \\leq l \\leq k$.\n\\end{theorem} \nFor simplicity, we assume the timestamps of the interaction are different, and we show how to update the representations when multiple interactions have the same timestamps in \\appendixref{sec:batch_updating}. We leave the proof in the \\appendixref{sec:proof_of_theorem_1}. The above theorem shows that the obtained node representation is the projection (i.e. linear transformation) of the temporal walk matrices, where the transform matrix is the initial random feature matrix $\\bm{P}$. The next theorem shows that the node representations preserve the inner product of the temporal walk matrices. \n\\begin{theorem}\n\\label{theorem:preserve_inner_product} \nGiven any $\\epsilon \\in (0,1)$, let $\\|\\cdot\\|_2$ denote the L2 norm, $c_{u,v}^{l_1,l_2}$ denote $\\frac{1}{2}(\\| \\bm{A}_{u}^{(l_1)}(t)\\|_2^2 + \\| \\bm{A}_{v}^{(l_2)}(t)\\|_2^2)$, and $\\bm{\\bar{H}}^{(l)}(t)$ denote $e^{-\\lambda lt}*\\bm{H}^{(l)}(t)$, if dimension $d_R$ of node representations satisfy $d_R \\geq \\frac{24}{\\epsilon^2}\\log(4^{1/3}(k+1)n)$, then for any $1 \\leq u, v \\leq n$, and $0 \\leq l_1,l_2 \\leq k$, we have \n\\begin{equation}\n    \\mathbb{P}\\left ( \\left|\\langle \\bm{\\bar{H}}_u^{(l_1)}(t), \\bm{\\bar{H}}_v^{(l_2)}(t) \\rangle  - \\langle \\bm{A}_u^{(l_1)}(t), \\bm{A}_v^{(l_2)}(t) \\rangle \\right|\\leq  \\epsilon c_{u,v}^{l_1,l_2} \\right ) \\geq 1-\\frac{1}{(k+1)n},\n\\end{equation}\nwhere $\\langle \\cdot, \\cdot \\rangle$ denotes the inner product and $|\\cdot|$ denotes taking absolute value.\n\\end{theorem}\nWe leave the proof in \\appendixref{sec:proof_of_theorem_2}. The above theorem shows that the inner product of the node representations is approximately equal to the inner product of the temporal walk matrices (i.e., $\\langle \\bm{\\bar{H}}_u^{(l_1)}(t), \\bm{\\bar{H}}_v^{(l_2)}(t) \\rangle \\approx \\langle \\bm{A}_u^{(l_1)}(t)), \\bm{A}_v^{(l_2)}(t) \\rangle$). Specifically, given any error rate $\\epsilon$, if the dimension of the node representations satisfies a certain condition, the difference between $\\langle \\bm{\\bar{H}}_u^{(l_1)}(t), \\bm{\\bar{H}}_v^{(l_2)}(t) \\rangle$ and $\\langle \\bm{A}_u^{(l_1)}(t)), \\bm{A}_v^{(l_2)}(t) \\rangle$ for any $u,v, l_1,l_2$ will be less than $\\epsilon c_{u,v}^{l_1,l_2}$ with high probability ($\\geq 1 - \\frac{1}{(k+1)n}$). The error rate can approach 0 infinitely and thus the inner product of the node representations can approach that of temporal walk matrices infinitely, at the cost of increasing the dimension $d_R$. As we will see in \\secref{sec:ablation_study}, a small dimension ($\\ll n$) in practice is enabled to make the inner product of node representations a good estimation of that of temporal walk matrices. Additionally, due to the i-th row of $\\bm{A}^{(0)}(t)$ being the one-hot encoding of i (since it is an identity matrix), we have $\\langle \\bm{A}_u^{(l)}(t), \\bm{A}_w^{(0)}(t)\\rangle = A_{u,w}^{(l)}(t)$. Thus, we can obtain $[A^{(0)}_{u,w}(t), \\cdots, A^{(k)}_{u,w}(t)]$ in \\equref{eq:unified_func} by calculating the inner product between all layers of $u$'s representation and the zero layer of $w$'s representation, expressed as $[\\langle \\bm{\\bar{H}}_u^{(0)}(t), \\bm{\\bar{H}}_w^{(0)}(t) \\rangle, \\cdots, \\langle \\bm{\\bar{H}}_u^{(k)}(t), \\bm{\\bar{H}}_w^{(0)}(t) \\rangle]$.\n\n\n% The above theorem guarantees that the inner product of the node representations will be approximately equal to the inner product of the temporal walk matrices if the dimension satisfy certain condition. Specifically, the term $\\epsilon (\\| \\bm{\\bar{A}}_u^{(i_1)}\\|_2^2+\\|\\bm{\\bar{A}}_v^{(i_2)}\\|_2^2)$ can be consider as the tolerable error range between the inner product of the node representations and inner product of the temporal walk matrices. What the above theorem says is that the given any tolerable error range, if the dimension of the node representations is large that a specific bound (i.e., $\\frac{72}{\\epsilon}log(kn)$), the error between the inner product of the temporal walk matrices and the inner product of the node representations (i.e., $\\left | \\langle \\bm{H}_u^{(i_1)}, \\bm{H}_v^{(i_2)} \\rangle - \\langle \\bm{\\bar{A}}_u^{(i_1)}, \\bm{\\bar{A}}_v^{(i_2)} \\rangle \\right |$) will smaller that the error range with high probability (i.e., $1-\\frac{1}{kn}$). Thus, we can consider $\\langle \\bm{H}_u^{(i_1)}, \\bm{H}_v^{(i_2)} \\rangle$ as the estimation of $\\langle \\bm{\\bar{A}}_u^{(i_1)}, \\bm{\\bar{A}}_v^{(i_2)} \\rangle$ and in the next part we will shown how to use the obtained node representations to decode the pairwise information. Besides, we will see in the xx that the desired dimension for node representation in practice will indeed be much less that the number of nodes.\n\n\n\\textbf{Remark.} Compared to directly computing the temporal walk matrices $\\bm{A}^{(0)}(t),..,\\bm{A}^{(k)}(t)$, which needs to enumerate the temporal walks and $O((k+1)n^2)$ space complexity to store, maintaining the node representations largely improve the computation and storage efficiency, which only needs $O((k+1)nd_R)$ space complexity to store and  $O(kd_R)$ time complexity to update when a new interaction occurs. Actually, \\theoremref{theorem:preserve_inner_product} is the direct result of \\theoremref{theorem:random_projection} based on Johnson-Lindenstrauss Lemma \\cite{sivakumar2002algorithmic}, where the random projection can preserve the inner product and norm \\cite{vempala2005random}. Notably, the method for implicitly maintaining temporal walk matrices via random feature propagation can be extended to other types of temporal walk matrices, provided they meet a specific condition (i.e., the updating function of the temporal walk matrix can be written as the linear combination of its rows). This condition is not restrictive, and all the temporal walk matrices discussed in \\secref{sec:pre} fulfill it. We show their propagation mechanism and related discussion in \\appendixref{sec:other_matrices}. In conclusion, the unified function in \\equref{eq:unified_func}, combined with methods of implicitly maintaining the temporal walk matrices, provides a new way to design a more effective and efficient way to inject pairwise information.\n\n\n% implicitly maintaining the temporal walk matrix can also be applied to other construction of the temporal walk matrix as long as its updating function can be represented as the linear combination of rows of the temporal walk matrices (i.e., $k_1 \\bm{A}_{i_1}^{(j_1)}+k_2 \\bm{A}_{i_2}^{(j_2)}+...+k_m \\bm{A}_{i_m}^{(j_m)}$). Such a condition can cover a lots of temporal walk matrices, where the temporal walk matrices shown in \\secref{sec:pre} all satisfy this condition and we show their implicitly maintaining algorithmns in Appendix. \n\n\n"
                },
                "subsection 3.2": {
                    "name": "Link Likelihood Computing",
                    "content": " \\label{sec:method_link}\nGiven an interaction $(u,v,t)$ to be predicted, we compute its happening likelihood based on the obtained node representations and auxiliary features. Specifically, we first decode a pairwise feature $\\bm{f}_{u,v}(t)$ from the node representations obtained in \\secref{sec:tmm}. Then we compute the node embeddings $\\bm{h}_u(t)$ and $\\bm{h}_v(t)$ for node $u$ and $v$ respectively based on their historical interactions. Finally, we give the link likelihood based on $\\bm{h}_{u}(t),\\bm{h}_v(t),\\bm{f}_{u,v}(t)$. For notation simplicity, we omit the suffix of $\\bm{h}_{u}(t),\\bm{h}_v(t),\\bm{f}_{u,v}(t)$ and denote them as $\\bm{h}_{u},\\bm{h}_v,\\bm{f}_{u,v}$ in the following part.  \n\n\\textbf{Pairwise Feature Decoding}. Although we can obtain the $(k+1)$-dimensional feature in \\equref{eq:unified_func} by calculating the inner product between the zero-layer representation of one node and all layers of the other node's representation, this method does not consider the correlation between all layers of both nodes. Therefore, we use representations from all layers to decode the pairwise information. Specifically, we first take the node representation of u and v from different layers, which can be denoted as $\\bm{F}_{*}=[e^{-\\lambda 0 t}\\bm{H}_{*}^{(0)},...,e^{-\\lambda kt}\\bm{H}_{*}^{(k)}] \\in \\mathbb{R}^{(k+1)\\times d_R}$ with * could be u or v. Then we concatenate them together to get $\\bm{F}_{u,v}=[\\bm{F}_u,\\bm{F}_v] \\in \\mathbb{R}^{2(k+1)\\times d_R}$ and obtain the raw pairwise feature $\\bm{\\tilde{f}}_{u,v}$ by computing the inner product among different rows of $\\bm{F}_{u,v}$, which is $\\bm{\\tilde{f}}_{u,v}= \\textrm{flat} (\\bm{F}_{u,v}\\bm{F}_{u,v}^T)$ with $\\textrm{flat}(\\cdot)$ means flatten a matrix of $\\mathbb{R}^{2(k+1)\\times 2(k+1)}$ into a vector of $\\mathbb{R}^{4(k+1)^2}$. Finally, we feed the raw pairwise feature $\\bm{\\tilde{f}}_{u,v}$ into an MLP to get the pairwise feature $\\bm{f}_{u,v}$, which is $\\bm{f}_{u,v} = \\textrm{MLP}(\\textrm{log}(\\textrm{ReLU}(\\bm{\\tilde{f}}_{u,v})+1))$. The $\\textrm{ReLU}(\\cdot)$ here is used to reduce estimation error, where the inner product of temporal walk matrices should be larger than zero and we thus set it to be zero if the inner product of the node representations is negative. The $\\textrm{log}(\\cdot)$ is used to scale the raw pairwise feature, where the range of the inner product between different layers varies greatly and the $+1$ is the shift term to avoid the undefined value of $\\textrm{log}(0)$. We will see in \\secref{sec:ablation_study} that these two operations can improve the training stability.\n\n\\textbf{Auxiliary Feature Learning}. The auxiliary features such as link features also provide rich information for modeling the evolution patterns of the temporal graph. In this part, we follow the previous methods \\cite{DBLP:conf/iclr/CongZKYWZTM23,DBLP:conf/nips/Yu23} and consider the auxiliary feature learning as a sequential learning problem. Specifically, for node $u$, we take its recent $m$ interactions $S_u = [(\\{u,w_1\\},t_1),...,(\\{u,w_m\\},t_m)]$ before $t$ and learn node embedding $\\bm{h}_{u}$ from this sequence. We first fetch the node features $\\bm{X}_{u,N} = [\\bm{x}_{w_1},...,\\bm{x}_{w_m}] \\in \\mathbb{R}^{m\\times d_N}$ and edge features $\\bm{X}_{u,E} = [\\bm{e}_{u,w_1}^{t_1},..,\\bm{e}_{u,w_1}^{t_m}] \\in \\mathbb{R}^{m\\times d_E}$. For timestamps, we map the timestamps into temporal features $\\bm{X}_{u,T}=[\\phi(t-t_1),..,\\phi(t-t_n)] \\in \\mathbb{R}^{m\\times d_T}$ like \\cite{DBLP:conf/iclr/XuRKKA20}, where $\\phi(\\Delta t) = [\\cos(w_1 \\Delta t),..,\\cos(w_{d_T} \\Delta t)]$ is a time encoding function to learn the periodic temporal pattern. Besides, we construct a relative encoding sequence $\\bm{X}_{u,F}=[\\bm{f}_{u,w_1}\\oplus \\bm{f}_{v,w_1},...,\\bm{f}_{u,w_m}\\oplus \\bm{f}_{v,w_m}] \\in \\mathbb{R}^{m\\times 8(k+1)^2}$ to inject the pairwise features, where $\\bm{f}_{u,w_m}$ denote the pairwise feature of $u$ and $w_m$ and $\\oplus$ is the concatenation operation. After obtaining the above feature sequence, we concatenate them together and feed it into an MLP to get the final feature sequence $\\bm{Z}_u^{(0)} = \\textrm{MLP}([\\bm{X}_{u,N},\\bm{X}_{u,E},\\bm{X}_{u,T},\\bm{X}_{u,F}]) \\in \\mathbb{R}^{m \\times d}$. Subsequently, we stack $l$ layers of MLP-Mixer \\cite{DBLP:conf/nips/TolstikhinHKBZU21} to capture the temporal and structural dependencies within the feature sequence, which is \n\\begin{equation}\n\\begin{aligned}\n\\tilde{\\bm{Z}}_u^{(l)} &= \\bm{Z}_u^{(l-1)} + \\bm{W}_{1}^{(l)} \\text{GeLU}(\\bm{W}_{2}^{(l)}\\text{LayerNorm}(\\bm{Z}_u^{(l-1)})) \\\\\n\\bm{Z}_u^{(l)} &= \\tilde{\\bm{Z}}_u^{(l)} + \\bm{W}_{3}^{(l)} \\text{GeLU}(\\bm{W}_{4}^{(l)}\\text{LayerNorm}(\\bm{\\tilde{Z}}_u^{(l)})).\n\\end{aligned}\n\\end{equation}\nFinally, we get the node embedding by mean pooling $\\bm{h}_u = \\text{MEAN}(\\bm{Z}_u^{(l)})$. The procedure to get node embedding $\\bm{h_v}$ is similar and for the node that does not have $m$ interactions, we pad the feature sequence with zero. Then, the likelihood of the link $(u,v,t)$ is given by $p_{u,v}^t = \\text{MLP}([\\bm{h}_u,\\bm{h}_v,\\bm{f}_{u,v}])$, where $\\textrm{MLP}(\\cdot)$ is a 2-layer MLP model with Sigmoid activation function in its output layer. \n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n",
                "subsection 4.1": {
                    "name": "Experimental Settings",
                    "content": "\n\\textbf{Datasets and Baselines}. We conduct experiments on 13 benchmark datasets for temporal link prediction, which are Wikipedia, Reddit, MOOC, LastFM, Enron, Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote and Contact. Eleven popular temporal graph learning methods are selected as baselines including JODIE, DyRep, TGAT, TGN, CAWN, EdgeBank, TCL, GraphMixer, NAT, PINT, and DyGFormer. Details about datasets and baselines can be found in \\appendixref{sec:experimental_settings}.\n\n\\textbf{Task Settings}. The task settings strictly follow \\cite{DBLP:conf/nips/Yu23}. Specifically, we conduct experiments under two settings: 1) the transductive setting that predicts links between nodes that have been seen during training and 2) the inductive setting that predicts links between nodes that are not seen during training. Three different negative sampling strategies introduced by \\cite{DBLP:conf/nips/PoursafaeiHPR22} are used to sample the negative links and the Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) are adopted as the evaluation metrics. For dataset splitting, we chronologically split each dataset with $70\\%/15\\%/15\\%$ for training/validating/testing. The training and testing pipeline is the same as that in \\cite{DBLP:conf/nips/Yu23}. \n\n\\textbf{Model Configuration}. For \\model, the layer $l$ of node representations, the number of recent interactions $m$, and dimension $d_R$ of the node representations are set to 3, 20 and $10*\\text{log(2E)}$, where E is the number of the interactions. We find the best time decay weight $\\lambda$ via grid search within a range of $10^{-4}$ to $10^{-7}$. Specifically, the best $\\lambda$ for Contact is $10^{-4}$, the best $\\lambda$ for MOOC, Can. Parl. and UN Vote is $10^{-5}$, the best $\\lambda$ for Wikipedia, Reddit, Enron, Social Evo., Flights and US Legis. is $10^{-6}$, the best $\\lambda$ for LastFM, UCI and UN Trade is $10^{-7}$.  \n\n\\textbf{Implementatoin Details}. For baselines, we use the implementation of DyGLib \\cite{DBLP:conf/nips/Yu23}, which is a unified temporal graph learning library, and has tuned the best hyperparameters for each baseline. For baselines that are not included in DyGLib (i.e., NAT and PINT), we use their official implementation and find the best hyperparameters via grid search. Experiments are conducted on a Ubuntu server, whose CPU and GPU devices are one Intel(R) Xeon(R) Gold 6226R CPU @ 2.9GHz with 64 CPU cores and four GeForce RTX 3090 GPUs with 24 GB memory respectively. We run each experiment five times and report the average.\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Performance and Efficiency Comparison",
                    "content": " \\label{sec:performance_efficiency}\n\\textbf{Performance comparison among baselines}. The performance of \\model~and baselines is shown in \\tabref{tab:transductive_random}. Due to space limit, \\tabref{tab:transductive_random} only shows the results under the transductive setting with random negative sampling strategy and more similar results can be found in \\appendixref{apdx:performance_comparison}. As shown in \\tabref{tab:transductive_random}, \\model~ achieves the best performance among all the methods on most datasets, verifying its effectiveness. Besides, the link-wise methods (CAWN, NAT, PINT, and DyGFormer) perform better than the node-wise methods, indicating the importance of injecting the pairwise feature. Compared to the baselines, \\model\\ simultaneously considers the temporal and structural correlations between nodes and encodes the temporal walk matrix into node representations with theoretical guarantees, which contributes to its superior performance.\n\n\\textbf{Efficiency Analysis}. We compare the relative inference time of different methods to \\model\\ to evaluate their efficiency. The results on LastFM and MOOC are shown in \\figref{fig:efficiency} and more results can be found in \\appendixref{sec:efficiency}. As shown in \\figref{fig:efficiency}, \\model~not only achieves the best performance but is also more efficient than other link-wise methods, where \\model~achieves a 33$\\times$ and 71.5$\\times$ speedups compared to the SOTA baseline DyGFormer and CAWN respectively on LastFM. The CAWN and DyGFormer models utilize time-consuming graph queries (e.g., temporal walk sampling) to construct relative encodings, which constitute the main computational bottleneck and consume over $70\\%$ of the running time. In contrast, \\model~caches historical interactions into node representations and constructs pairwise features based on these representations, thereby enhancing efficiency.\n\n\\textbf{Scalability Analysis}. \nTo further verify the scalability of \\model, we generated a series of random graphs with an average degree fixed at 100 and the number of edges varying from $1\\text{e}5$ to $1\\text{e}8$. \\figref{fig:efficiency} shows the change of running time and GPU memory of \\model, where the growth of the running time and GPU memory is close to and less than the linear growth curve respectively, showing the good scalability of \\model. In contrast, PINT explicitly stores the temporal walk matrices and encounters out-of-memory error when the edge number reaches $1\\text{e}7$ (shown in\\appendixref{sec:scalability}), verifying the impracticability of explicitly storing temporal walk matrices.\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Study",
                    "content": " \\label{sec:ablation_study}\n\\textbf{Proposed Components}. To verify the effectiveness of the proposed components in \\model, we compare \\model~with the following variants: 1) w/o NR that remove the node representations and the corresponding features that decoded from them. 2) w/o Time that only considers the structural information by setting the time decay weight $\\lambda$ to be zero. 3) w/o Scale that remove the $\\log(\\cdot)$ and $\\text{ReLU}(\\cdot)$ in the pairwise feature decoding. As shown in \\tabref{tab:inject_way}, there is a dramatic performance drop of w/o NR, which shows that the pairwise information carried by the node representations plays a vital role in the performance of \\model. There is also an obvious performance drop of w/o Time, which confirms the necessity of incorporating temporal information in temporal walk matrix construction. Besides, the unreasonable poor performance of the w/o Scale is due to the various distribution of node representations across different layers, where, without scaling the raw pairwise features, the training will be unstable, and numerical overflow errors may even occur on some datasets. Further details on the distribution of node representations from different layers are provided in \\appendixref{sec:representation_distribution}.\n\n\\textbf{Dimension Change}. To verify the influence of the node representation dimension. We vary the dimensions from 1 to 128 and report the performance of \\model~(denoted as \\model-d). As shown in \\figref{fig:dimension_change}, the required dimension of node representations is small, where only 1-dimensional and 16-dimensional node representations can achieve satisfactory performance on MOOC and LastFM respectively. For different datasets, we observe that the average degree may be a main influence of the required dimension, where sparse graphs (like MOOC and Wikipedia) only need a small dimension, and dense graphs (like LastFM and Enron) may require a larger dimension. Empirically, setting the dimension to be $10*\\text{log(2E)}$ is enough to achieve satisfactory performance on all datasets, where $E$ is the number of edges. Results on more datasets can be found in \\appendixref{sec:dimension_change_appdx}. \n\n\n\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n\n% Among them, \\citet{DBLP:conf/iclr/WangCLL021} first pointed out that xx.\n\nTemporal link prediction \\cite{DBLP:journals/csur/QinY24} aims at predicting future interactions based on historical topology, which is crucial for a series of real-world applications \\cite{DBLP:conf/cikm/FanLZX0Y21,DBLP:conf/kdd/KumarZL19,DBLP:journals/tkde/YuLSDLL24}. Earlier methods considered the temporal graph as a series of graph snapshots that are sampled at regularly-spaced timestamps \\cite{DBLP:conf/wsdm/SankarWGZY20,DBLP:conf/aaai/ParejaDCMSKKSL20}, which will lose the fine-grained temporal information due to ignoring the temporal orders of interactions in a graph snapshot. Recently, some continuous-time temporal graph learning methods have been proposed \\cite{DBLP:journals/corr/tgn,DBLP:conf/kdd/KumarZL19,DBLP:conf/iclr/XuRKKA20,DBLP:conf/iclr/CongZKYWZTM23}, which consider the temporal graph as a sequence of interactions with irregular time intervals to fully capture the graph dynamics. For example, TGN \\cite{DBLP:journals/corr/tgn} maintained a dynamic memory vector for each node and generated node representations by aggregating memory vectors via temporal graph attention to capture the evolution pattern of the temporal graph. However, capturing pairwise information by merely aggregating representations of neighboring nodes \\cite{DBLP:conf/iclr/KipfW17} is challenging. To address this issue, the link-wise method was proposed, which constructs relative encodings as additional node features to inject the pairwise information into the representation learning process \\cite{DBLP:conf/iclr/WangCLL021,DBLP:conf/log/LuoL22,DBLP:conf/nips/Yu23,DBLP:conf/nips/SouzaMKG22}. For example, \\citet{DBLP:conf/nips/SouzaMKG22} proposed a relative encoding based on temporal walk counting and theoretically showed that constructing the relative encodings can improve the expressive power of models in distinguishing different links. Despite these advances, existing ways to construct the relative encodings are still far from satisfactory, where computation efficiency is a main concern and temporal information is seldom considered. In this paper, we unify existing relative encodings into a function of temporal walk matrices and explore encoding the pairwise information effectively and efficiently by temporal walk matrix projection.\n"
            },
            "section 6": {
                "name": "Limitation",
                "content": "\nOne limitation of our method is that the matrix construction approach requires manual predefined settings. Different networks may necessitate distinct construction methods, potentially leading to additional human effort in experimenting with various approaches. For instance, the proposed temporal walk matrix that incorporates the time decay effect may not be optimal for networks characterized by long-term dependencies. Developing an adaptive matrix construction technique will be an interesting direction for future research.\n\n"
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\nIn this paper, we study the problem of pairwise information injection for temporal link prediction. We unify existing construction ways of relative encodings into a unified function, which reveals a connection between the relative encoding and temporal walk matrix. Then we propose a new temporal link prediction model, \\model, to address the computational inefficiencies and the ignorance of temporal information in previous methods. \\model~ introduces a new temporal walk matrix to simultaneously consider the temporal and structural information and a random feature propagation mechanism to maintain the temporal walk matrices efficiently. Theoretically, \\model\\ preserves the inner product of the maintained temporal walk matrices and empirically outperforms other link-wise methods in both effectiveness and efficiency. An interesting future direction may be designing an adaptive feature propagation mechanism.\n\n\n% implicitly maintains a time decay temporal walk matrices through a random feature propagation mechanism. \\model~ theoretically encodes the pairwise information between nodes and empirically outperforms other link-wise methods in effectiveness and efficiency. An interesting future direction may be designing an adaptive feature propagation mechanism.\n\n% which introduces a walk matrix based on the time decay effect to simultaneously consider the temporal and structural information. Then we propose a random feature propagation mechanism to maintain a series of dynamic node representations in a computationally and storage efficient way. Theoretical analysis shows that the obtained node representations preserve the inner product of the proposed temporal walk matrices. Finally, we propose a temporal link prediction model to map the node representations and auxiliary features into link likelihood. Experimental results on 13 benchmark datasets verify the effectiveness and efficiency of the \\model. A future direction may be.\n"
            },
            "section 8": {
                "name": "Acknowledgments",
                "content": "\nThis work was supported by the National Natural Science Foundation of China (No. 62272023 and No. 62276015).\n\n\\bibliographystyle{unsrtnat}\n\\bibliography{reference.bib}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\clearpage\n\\appendix\n"
            },
            "section 9": {
                "name": "Analysis of Existing Relative Encodings",
                "content": " \\label{apdx:relative_encoding}\n",
                "subsection 9.1": {
                    "name": "NAT",
                    "content": " \\label{sec:nat}\nIn Algorithm 1 of NAT, it maintains a series of hash maps $s_u^{(0)},s_u^{(1)},..,s_u^{(k)}$ for each node $u$, which are sets of node ids. Next, we will first prove the statement holds if the size of the hash maps is infinite.\n\n\\textbf{Statement}. $s^{(i)}$ is a set of nodes that $u$ can approach through a temporal walk whose length is less than $i+1$. \n\nInitially, the $s_u^{(0)}$ is set as $\\{u \\}$ and $s_u^{(1)},..,s_u^{(k)}$ are set as empty set. The statement holds. When a new interaction $(\\{u,v\\},t)$ occurs and if the size of the hash maps is infinite, for $1 \\leq i \\leq k$, the updating function of hash maps can be represented as \n\\begin{equation}\n    \\bar{s}_u^{(i)} = s_u^{(i)} \\cup s_v^{(i-1)}, ~~~~~~ \\bar{s}_v^{(i)} = s_v^{(i)} \\cup s_u^{(i-1)},\n\\end{equation}\nwhere $\\bar{s}_u^{(i)}$ and $s_u^{(i)}$ denote the hash map after and before adding the interaction respectively. Then if the timestamp of $(\\{u,v\\},t)$ is larger than that of previous interactions, the newly generated temporal walks beginning from $u$ must first visit $v$ through $(\\{u,v\\},t)$ (proved in \\secref{sec:proof_of_theorem_1}), thus the newly added nodes that $u$ can visit through a temporal walk with length less than $i+1$ must belong to $s_v^{(i-1)}$. So $\\bar{s}_u^{(i)}$ will contain all the nodes that $u$ can approach through a temporal walk with length less than $i+1$ after adding $(\\{u,v\\},t)$. The statement holds.\n\nFor a node $w$, its similarity feature $\\bm{r}^{w|u}$ is a $(k+1)$-dimensional vector, where for $1 \\leq i\\leq k+1$, $r^{w|u}_{i}=1$ if $w \\in s_u^{(i-1)}$ and $r^{w|u}_{i}=0$ otherwise. Considering that the above statement holds, the similarity feature $\\bm{r}^{w|u}$ is equivalent to a $(k+1)$-dimensional vector, where $r^{w|u}_{i}=1$ if the shortest temporal walk from $u$ to $w$ is less than $i$; otherwise, $r^{w|u}_{i}=0$.\n\n\\begin{minipage}{0.67\\textwidth}\n\\begin{algorithm}[H]\n\\label{alg:causal_sampling}\n\\SetKwInOut{Input}{Input}\\SetKwInOut{Output}{Output}\n\\caption{Temporal Walk Extraction ($\\mathcal{G}(t)$, $\\alpha$, $k$, $u$)}\nInitialize $W$ to be $\\{(u,t)\\}$ \\;\n\\For{$i$ from $1$ to $k$}{\n$(w_{\\text{p}}, t_{\\text{p}}) \\leftarrow$ the last (node, time) pair in $W$\\;\nSample one $(\\{w_p,w'\\}, t')\\in \\mathcal{E}_{w_{\\text{p}},t_{\\text{p}}}$ with prob. $\\propto$ $\\exp(-\\alpha (t_{\\text{p}}-t'))$ \\;\n$W_i \\leftarrow W_i \\oplus (w',t')$\\;\n}\nReturn W\\;\n\\end{algorithm}\n\\end{minipage}\n\n\n"
                },
                "subsection 9.2": {
                    "name": "CAWN",
                    "content": " \\label{sec:cawn}\nFor constructing $\\bm{r}^{w|u}$, CAWN first repeatedly samples $m$ temporal walks of length $k$ begging at $u$ according to the sampling strategy in \\algoref{alg:causal_sampling}. Then $\\bm{r}^{w|u}$ is set to be a ($k+1$)-dimensional vector, where $r^{w|u}_{i}$ will be the number of walks whose i-th visited node is $w$ for $1 \\leq i \\leq k+1$, which can be represented as,\n\\begin{equation}\n    r^{w|u}_{i} = \\sum_{j=1}^{m} \\bm{1}_{W_j[i][0] = w},\n\\end{equation}\nwhere $W_j$ is the j-th sampled walks and $\\bm{1}_{W_j[i][0] = w}$ is $1$ if $W_j[i][0] = w$; otherwise, $\\bm{1}_{W_j[i][0] = w}$ is $0$. Due to each temporal walk being sampled independently, $\\bm{1}_{W_1[i][0] = w},\\cdots,\\bm{1}_{W_m[i][0] = w}$ are $m$ independent and identically distributed Bernoulli random variables $\\text{Ber}(\\mu_{w}^{i-1})$, where $\\mu_{w}^{i-1}$ is the probability of reaching $w$ from $u$ after performing a $(i-1)$-step temporal walk according to \\algoref{alg:causal_sampling}. And according to the strong law of large numbers (Theorem 1.3.1 of \\cite{vershynin2020high}), the mean of these random variables (i.e., $\\frac{1}{m}\\sum_{j=1}^m \\bm{1}_{W_j[i][0] = w} \\iff \\frac{1}{m}r^{w|u}_{i}$) will coverage to the mean as the number of sampled walks $m \\rightarrow \\infty$. The mean of the $\\text{Ber}(\\mu_{w}^{i-1})$ is $\\mu_{w}^{i-1}$. Expand all $(i-1)$-step temporal walks from $u$ to $w$, we have \n\\begin{equation}\n    \\mu_{w}^{i-1} = \\sum_{W \\in \\mathcal{M}_{u,w}^{i-1}} f(W),\n\\end{equation}\nwhere $f(\\cdot)$ is the sampled probability of a walk according to \\algoref{alg:causal_sampling}. For \\algoref{alg:causal_sampling}, if we are current at $(w_i,t_i)$, then the probability of moving to $(w_{i+1},t_{i+1})$ through an interaction $(\\{w_i,w_{i+1}\\},t_{i+1})$ is proportional to $\\exp(-\\alpha(t_{i}-t_{i+1}))$, which is $\\frac{\\textrm{exp}(-\\alpha(t_{i}-t_{i+1}))}{\\sum_{(w',t')\\in \\mathcal{E}_{w_i,t_i}} \\text{exp}(-\\alpha(t_i-t'))}$. Thus the probability of a temporal walk $W=[(w_0,t_0),(w_1,t_1),..,(w_k,t_k)]$ is \n\\begin{equation}\n    f(W) = \\prod_{i=0}^{k-1} \\frac{\\text{exp}(-\\alpha(t_{i}-t_{i+1}))}{\\sum_{(\\{w',w\\},t')\\in \\mathcal{E}_{w_i,t_i}} \\textrm{exp}(-\\alpha(t_i-t'))}\n\\end{equation}\nwhich is the same as the score function $s'(\\cdot)$ of CAWN we shown in \\secref{sec:existing_methods}. Thus, $r^{w|u}_{i}$ (multiplied with a const $\\frac{1}{m}$) is the same as $\\sum_{W \\in \\mathcal{M}_{u,w}^{i-1}} s'(W)$.\n\n\n\n"
                }
            },
            "section 10": {
                "name": "Proofs",
                "content": "\n",
                "subsection 10.1": {
                    "name": "Proof of Theorem 1",
                    "content": " \\label{sec:proof_of_theorem_1}\nThe equation in \\theoremref{theorem:random_projection} can be rewritten as $\\bm{H}^{(l)}(t) = e^{\\lambda lt}*\\bm{A}^{(l)}(t)\\bm{P}$. We can consider $e^{\\lambda lt}*\\bm{A}^{(l)}(t)$ as a new temporal walk matrix whose score function for a temporal walk $[(w_0,t_0),(w_1,t_1),...,(w_l,t_l)]$ at time $t$ is $e^{\\lambda lt}*s(W)$. Expanding it, we have\n\\begin{equation}\ne^{\\lambda lt}*s(W) = e^{\\lambda lt}*\\prod_{j=1}^l e^{-\\lambda(t-t_j)}  = \\prod_{j=1}^l e^{-\\lambda(t-t_j)}*e^{\\lambda t} = \\prod_{j=1}^l e^{\\lambda t_j}. \n\\end{equation}\nWe use the notation $\\bm{\\bar{A}}^{(l)}(t)$ to denote $e^{\\lambda lt}*\\bm{A}^{(l)}(t)$ and $\\bar{s}(W)$ to denote $e^{\\lambda lt}*s(W)$ for a $l$-step temporal walk in the following proof. The original problem is transformed into proving that $\\bm{H}^{(l)}(t) = \\bm{\\bar{A}}^{(l)}(t)\\bm{P}$. Note that for a given temporal walk $[(w_0,t_0),(w_1,t_1),...,(w_l,t_l)]$, each term of $\\bar{s}(W)$ (i.e., $\\prod_{j=1}^l e^{\\lambda t_j}$) will no change as time $t$ goes on. Thus the temporal walk matrices $\\bm{\\bar{A}}^{(l)}(t)$ will only change when a new interaction occurs. Next, we inspect how the $\\bm{\\bar{A}}^{(l)}(t)$ changes when a new interaction $(u,v,t)$ occurs. For each element $\\bar{A}_{i,j}^{(l)}(t)$, its change is caused by the newly generated $l$-step temporal walks from $i$ to $j$ after adding the interaction $(u,v,t)$, which can be written as \n\\begin{equation}\n    A_{i,j}^{(l)}(t^+) = A_{i,j}^{(l)}(t) + \\sum_{W \\in \\Delta M_{i,j}^l} \\bar{s}(W), \n    \\label{eq:set_form_updating}\n\\end{equation}\nwhere $t^+$ denotes the timestamps right after $t$ and $\\Delta M_{i,j}^l$ denote the newly generated $l$-step temporal walks from $i$ to $j$. According to the definition of the temporal walk, the new $l$-step temporal walks must begin from $u$ or $v$. Because if there is a temporal walk that does not begin from $u$ or $v$, it can be represented as $[(w_0,t_0),..,(w_i,t_i),(u,t_{i+1}),(v,t),..,(w_l,t_l))]$ or $[(w_0,t_0),..,(w_i,t_i),(v,t_{i+1}),(u,t),..,(w_l,t_l))]$, which means that there is an interaction $(\\{w_i,u\\},t_{i+1})$ or $(\\{w_i,v\\},t_{i+1})$ whose timestamp $t_i$ is larger than $t$. (Since the timestamps of a temporal walk are decreasing). This is impossible since $(u,v,t)$ is a newly happened interaction. Thus only the $u$-th row and $v$-th row of the $\\bar{\\bm{A}}^{(l)}(t)$ will change. Besides for each newly generated temporal walk from $u$, it must be $[(u,t),(v,t_1),..,(w_l,t_l))]$, where $[(v,t_1),..,(w_l,t_l))]$ corresponds to a $(l-1)$-step temporal walk beginning from $v$. And for any $(l-1)$-step temporal walk beginning from $v$, we can add a prefix $(u,t)$ to make it become a $l$-step temporal walk beginning from $u$. Thus for any $1 \\leq i \\leq n$, there is a one-to-one mapping between $\\Delta M_{u,i}^l$ and $M_{v,i}^{l-1}(t)$ with $M_{v,i}^{l-1}(t)$ denote the set of $(l-1)$-th temporal walks from $v$ to $i$ before $t$, and we can rewritten \\equref{eq:set_form_updating} as\n\\begin{equation}\nA_{u,i}^{(l)}(t^+) = A_{u,i}^{(l)}(t) + \\sum_{W \\in  M_{v,i}^{l-1}(t)} e^{-\\lambda t}*\\bar{s}(W) = A_{u,i}^{(l)}(t) + e^{-\\lambda t}*A_{v,i}^{(l-1)}(t),\n\\end{equation}\nThe updating function for $\\bar{A}_{v,i}^{(l)}(t)$ is also similar. We give a visual illustration of the new temporal walks in \\figref{fig:new_temporal_walk} for better understanding. Finally, writing the update formula in vector form, for any $1 \\leq l \\leq k$ we have\n\\begin{equation}\n  \\bm{\\bar{A}}^{(l)}_u(t^+) = \\bm{\\bar{A}}^{(l)}_u(t) + e^{\\lambda t}*\\bm{\\bar{A}}^{(l-1)}_v(t),~~~~~ \\bm{\\bar{A}}^{(l)}_v(t^+) = \\bm{\\bar{A}}^{(l)}_v(t) + e^{\\lambda t}*\\bm{\\bar{A}}^{(l-1)}_u(t), \n  \\label{eq:update}\n\\end{equation}\nwhich is the same as \\equref{eq:update}! Thus if $\\bm{H}^{(l)}(t) = \\bm{\\bar{A}}^{(l)}(t)\\bm{P}$ for $0 \\leq l \\leq k$, after adding a new interaction $(u,v,t)$, for $0 \\leq l \\leq k$, $\\bm{H}^{(l)}(t^+) = \\bm{\\bar{A}}^{(l)}(t^+)\\bm{P}$ still holds. Because we have\n\\begin{equation}\n    \\bm{H}_u^{(l)}(t^+) = \\bm{H}_u^{(l)}(t) + e^{\\lambda t}*\\bm{H}_v^{(l-1)}(t) = (\\bm{\\bar{A}}_u^{(l)}(t) + e^{\\lambda t}*\\bm{\\bar{A}}_v^{(l-1)}(t))\\bm{P} = \\bm{\\bar{A}}_u^{(l)}(t^+)\\bm{P}\n\\end{equation}\nNote that the equation in \\equref{theorem:random_projection} holds at initialization, thus the equation always holds and the theorem is proved.\n\n"
                },
                "subsection 10.2": {
                    "name": "Proof of Theorem 2",
                    "content": " \\label{sec:proof_of_theorem_2}\nThe \\theoremref{theorem:preserve_inner_product} can be considered as a special case of the Johnson-Lindenstrauss Lemma \\cite{sivakumar2002algorithmic}, where the random projection \\cite{vempala2005random,DBLP:conf/stoc/Charikar02,DBLP:conf/kdd/LiHC06} can preserve the norm and inner product. For the convenience of readers lacking relevant background, we follow \\cite{wright2022high} to provide the proof under the given conditions of this paper. We begin the proof by the following lemma.\n\\begin{lemma}[(Lemma 3.18 of \\cite{wright2022high})] \\label{lemma:probability_inequation}\nLet $\\bm{x} \\in \\mathbb{R}^d$ be a d-dimensional random vector whose entries are independent $\\mathcal{N}(0,\\frac{1}{d})$. Then for any $\\epsilon \\in[0,1]$,\n\n\\begin{equation}\n    \\mathbb{P}\\left(\\left|\\|\\bm{x}\\|_2^2 -1 \\right|> \\epsilon \\right) \\leq 2\\exp(\\frac{-\\epsilon^2d}{8})\n\\end{equation}\n\\end{lemma} \nThe following part can be divided into 1) We first give two corollaries and their proofs. 2) Then we give the proof of Theorem 2 based on the corollaries.\n",
                    "subsubsection 10.2.1": {
                        "name": "Two Corolarries",
                        "content": "\nBased on the above lemma, we can get the following corollaries. \n\\begin{corollary} \\label{coro:norm}\n    Given any $\\epsilon \\in (0,1), \\bm{x} \\in \\mathbb{R}^m$. Let $\\bm{P} \\in \\mathbb{R}^{d\\times m}$ be a random matrix whose entries are independent $\\mathcal{N}(0,\\frac{1}{d})$, if $d \\geq \\frac{8}{\\epsilon^2}\\log(\\frac{1}{\\delta})$, we have\n    \\begin{equation}\n        \\mathbb{P}\\left((1-\\epsilon)\\| \\bm{x} \\|_2^2 \\leq \\|\\bm{Px}\\|_2^2 \\leq (1+\\epsilon)\\| \\bm{x} \\|_2^2\\right) \\geq 1- 2\\delta \n    \\end{equation}\n\\end{corollary}\n\\textbf{Proof}. For the above corollary, we have \n\\begin{equation}\n\\begin{split}\n\\mathbb{P}\\left((1-\\epsilon)\\|\\bm{x}\\|_2^2 \\leq \\|\\bm{Px}\\|_2^2 \\leq (1+\\epsilon)\\|\\bm{x}\\|_2^2\\right) \\geq 1 - 2\\delta  \\\\ \\iff \\mathbb{P}\\left( \\left| \\frac{\\|\\bm{Px}\\|_2^2}{\\|\\bm{x}\\|_2^2} -1 \\right| \\leq \\epsilon \\right ) \\geq 1 - 2\\delta \\\\ \\iff \\mathbb{P}\\left( \\left| \\frac{\\|\\bm{Px}\\|_2^2}{\\|\\bm{x}\\|_2^2} -1 \\right| > \\epsilon \\right ) \\leq 2\\delta\n\\end{split}\n\\end{equation}\nNote that each entry of $\\bm{P}$ is an independent $\\mathcal{N}(0,\\frac{1}{d})$, thus $\\bm{Px}$ is a random vector whose each entry $(\\bm{Px})_k = \\sum_{i=1}^m P_{k,i}*x_i$ is an independent $\\mathcal{N}(0,\\frac{\\|\\bm{x}\\|_2^2}{d})$ and each entry of $\\frac{\\bm{Px}}{\\|\\bm{x}\\|_2}$ is an independent $\\mathcal{N}(0,\\frac{1}{d})$. Substituting it to \\lemmaref{lemma:probability_inequation} and taking $d \\geq \\frac{8}{\\epsilon^2}\\log(\\frac{1}{\\delta})$, we have\n\\begin{equation}\n\\begin{aligned}\n\\mathbb{P}\\left( \\left| \\frac{\\|\\bm{Px}\\|_2^2}{\\|\\bm{x}\\|_2^2} -1 \\right| > \\epsilon \\right ) &\\leq 2\\exp(\\frac{-\\epsilon^2d}{8}) \\\\ \\leq 2 &\\exp(\\frac{-\\epsilon^2}{8}*\\frac{8}{\\epsilon^2}*\\log(\\frac{1}{\\delta})) \\leq 2\\delta \n\\end{aligned}\n\\end{equation}\n\nBased on \\cororef{coro:norm}, we can get the following corollary.\n\\begin{corollary} \\label{coro:inner_product}\n    Given any $n$ $m$-dimensional vectors $\\bm{x}_1,...,\\bm{x}_n$, $\\epsilon \\in (0,1)$, let $\\bm{P} \\in \\mathbb{R}^{d \\times m}$ be a random matrix whose entries are independent $\\mathcal{N}(0,\\frac{1}{d})$, if $d \\geq \\frac{24}{\\epsilon ^2} \\log(4^{1/3}n)$, for any $1 \\leq i,j \\leq n$, we have\n    \\begin{equation}\n        \\mathbb{P}\\left(\\left| \\langle \\bm{P}\\bm{x_i}, \\bm{P}\\bm{x_j} \\rangle - \\langle \\bm{x}_i, \\bm{x}_j \\rangle \\right| \\leq \\frac{\\epsilon}{2} (\\|\\bm{x}_i\\|_2^2+\\|\\bm{x}_j\\|_2^2)\\right) \\geq 1 - \\frac{1}{n}\n    \\end{equation}\n\\end{corollary}\nlet $E_{i,j}^+$ and $E_{i,j}^-$ denote the event of $\\{\\left| \\|\\bm{P(x_i+x_j)}\\|_2^2 - \\|\\bm{x_i+x_j}\\|_2^2 \\right| \\leq \\epsilon \\|\\bm{x_i}+\\bm{x_j}\\|_2^2 \\}$ and $\\{ \\left| \\|\\bm{P(x_i-x_j)}\\|_2^2 - \\|\\bm{x_i-x_j}\\|_2^2 \\right| \\leq \\epsilon \\|\\bm{x_i}-\\bm{x_j}\\|_2^2 \\}$ respectively. Since $\\bm{x}_i + \\bm{x_j}$ and $\\bm{x}_i - \\bm{x_j}$ can also be consider two $m$-dimensional vectors. Thus take it and $d \\geq \\frac{24}{\\epsilon^2}\\log(4^{1/3}n)$ into \\cororef{coro:norm},  we have $\\mathbb{P}(E_{i,j}^+) \\geq 1-\\frac{1}{2n^3}$ and $\\mathbb{P}(E_{i,j}^-) \\geq 1-\\frac{1}{2n^3}$. Then let $C_{i,j}=E_{i,j}^+ \\cap E_{i,j}^-$ denote that $E_{i,j}^+$ and $E_{i,j}^-$ hold simultaneously, according to the union bound, we have\n\\begin{equation}\n    \\mathbb{P}(C_{i,j}) = 1 - \\mathbb{P}(\\overline{E_{i,j}^+} \\cup \\overline{E_{i,j}^-}) \\geq 1 -(\\mathbb{P}(\\overline{E_{i,j}^+}) + \\mathbb{P}(\\overline{E_{i,j}^-})) \\geq 1-\\frac{1}{n^3},\n\\end{equation}\nwhere $\\overline{E_{i,j}^+}$ denote that $E_{i,j}^+$ does not hold and $\\overline{E_{i,j}^+} \\cup \\overline{E_{i,j}^-}$ denotes that $\\overline{E_{i,j}^+}$ or $\\overline{E_{i,j}^-}$ holds. According to the union bound, we further have\n\\begin{equation}\n     \\mathbb{P}(\\cap _{i,j}C_{i,j}) = 1 - \\mathbb{P}(\\cup_{i,j}\\overline{C_{i,j}}) \\geq 1- \\sum_{i,j}\\mathbb{P}(\\overline{C_{i,j}}) \\geq 1 - n^2*\\frac{1}{n^3} \\geq 1 - \\frac{1}{n},\n\\end{equation}\nwhere $\\cap _{i,j}C_{i,j}$ denote that $C_{i,j}$ holds for any $i,j$ and $\\cup_{i,j}\\overline{C_{i,j}}$ denote that there exist $i,j$ that $\\overline{C_{i,j}}$ holds. If $C_{i,j}$ holds, we have\n\n\\begin{align} \n    (1-\\epsilon)\\|\\bm{x_i+x_j}\\|_2^2  \\leq \\|\\bm{P(x_i+x_j)}\\|_2^2 \\leq (1+\\epsilon)\\|\\bm{x_i+x_j}\\|_2^2  \\label{eq:pos} \\\\  \n    (1-\\epsilon)\\|\\bm{x_i-x_j}\\|_2^2  \\leq \\|\\bm{P(x_i-x_j)}\\|_2^2 \\leq (1+\\epsilon)\\|\\bm{x_i-x_j}\\|_2^2  \\label{eq:neg}\n\\end{align}\nMultiplying \\equref{eq:neg} with $-1$ and adding it to \\eqref{eq:pos}, we have\n\\begin{equation}\n    \\left| \\langle \\bm{Px_i}, \\bm{Px_j}\\rangle - \\langle \\bm{x_i},\\bm{x_j} \\rangle \\right | \\leq \\frac{\\epsilon}{2}(\\|\\bm{x_i}\\|_2^2 + \\| \\bm{x_j}\\|_2^2)\n\\end{equation}\nThus we have \n\\begin{equation}\n       \\mathbb{P}(\\cap _{i,j}C_{i,j}) \\geq 1 - \\frac{1}{n} \\implies \\\\ \n    \\mathbb{P}(\\left| \\langle \\bm{Px_i}, \\bm{Px_j}\\rangle - \\langle \\bm{x_i},\\bm{x_j} \\rangle \\right | \\leq \\frac{\\epsilon}{2}(\\|\\bm{x_i}\\|_2^2 + \\| \\bm{x_j}\\|_2^2)) \\geq 1 - \\frac{1}{n} \n\\end{equation}\nholds for any $1 \\leq i,j \\leq n$.\n"
                    },
                    "subsubsection 10.2.2": {
                        "name": "Proof based on Corollaries",
                        "content": "\nThe matrix of $\\bm{A}^{(l)}(t)$ in \\theoremref{theorem:preserve_inner_product} can be considered as $n$ vectors with $n$ dimensions, where each row of $\\bm{A}^{(l)}(t)$ is a $n$-dimensional vector. Similarly, we can consider $\\bm{A}^{(0)}(t), \\cdots, \\bm{A}^{(k)}(t)$ together as $n(k+1)$ vectors with $n$ dimensions. Considering that $\\bm{P} \\in \\mathbb{R}^{n\\times d_R}$ is a random matrix where each entry is an independent $\\mathcal{N}(0,\\frac{1}{d_R})$ and $e^{-\\lambda lt}*\\bm{H}^{(l)}(t)$ is the projection of $\\bm{A}^{(l)}(t)$, substitute it into \\cororef{coro:inner_product} and taking the number of vectors as $(k+1)n$, we can get \\theoremref{theorem:preserve_inner_product}. \n\n\n"
                    }
                }
            },
            "section 11": {
                "name": "Batch Updating Mechanism",
                "content": " \\label{sec:batch_updating}\nFor the situation where multiple interactions happen simultaneously, we can first compute each interaction's contribution independently and sum them together to update the node representations. The maintaining mechanism is shown in \\algoref{alg:batch_maintaining_sum}, where we packed the interactions that happen at the same time into a set $\\mathcal{B}$ and sum the independent contribution of each interaction in $\\mathcal{B}$ into $\\Delta \\bm{H}^{(1)},\\cdots, \\Delta \\bm{H}^{(k)}$. For simplicity, we initialize $\\Delta \\bm{H}^{(1)},\\cdots, \\Delta \\bm{H}^{(k)}$ each time and it can be replaced by some efficient implementation such as scatter\\_add operation in pytorch.\n\n\n\n"
            },
            "section 12": {
                "name": "Propagation Mechanism for Other Matrices",
                "content": " \\label{sec:other_matrices}\nFor simplicity, here we only consider the situation where the timestamp of each interaction is different and we can adopt a similar batch updating mechanism in \\secref{sec:batch_updating} to handle the situation where multiple interactions happen simultaneously.\nFor the updating of other types of temporal walk matrices, let $\\bm{A}(t)=[\\bm{A}^{(0)}(t),\\cdots,\\bm{A}^{(k)}(t)] \\in \\mathbb{R}^{n(k+1)\\times d}$ denotes the concatenation of the temporal walk matrices. If the following two situations can be satisfied simultaneously, we can apply the random feature propagation mechanism to implicitly maintain the temporal walk matrices. \n\n\\textbf{Condition 1}. After adding a new interaction $(u,v,t)$, the change of each row can be written as the linear combination of other rows, which is for each $1 \\leq i \\leq n(k+1)$, there exists $k_1,...,k_m$ and $l_1,...,l_m$ satisfying. \n\\begin{equation}\n    \\bm{A}_{i}(t^+) = k_1 \\bm{A}_{l_1}(t) + \\cdots +  k_m \\bm{A}_{l_m}(t), \\label{eq:linear_combination}\n\\end{equation}\nwhere $t^+$ is the time right after $t$.\n\n\\textbf{Condition 2}. After time moving $\\Delta t$ without adding new interactions, the change of each row can be written as the linear combination of other rows, which is for each $1 \\leq i \\leq n(k+1)$, there exists $k_1,...,k_m$ and $l_1,...,l_m$ satisfying. \n\\begin{equation}\n    \\bm{A}_{i}(t+\\Delta t) = k_1 \\bm{A}_{l_1}(t) + \\cdots +  k_m \\bm{A}_{l_m}(t)\n\\end{equation}\n\nThe motivation behind the conditions is that the projection is a linear operation. If the node representations are the projection of the temporal walk matrix at $t$ (i.e.,$\\bm{H}(t) = \\bm{A}(t)\\bm{P}$) and the updating function of the temporal walk matrix is the linear combination of other rows, then we can apply the same updating function on $\\bm{H}(t)$, which will make $\\bm{H}(t^+)$ still be the projection of the temporal walk matrix. For example, applying \\eqref{eq:linear_combination} to $\\bm{H}(t)$ will get\n\\begin{align}\n    \\bm{H}(t^+) &= k_1 \\bm{H}_{l_1}(t) + \\cdots + k_m \\bm{H}_{l_m}(t) \\\\ \\nonumber\n    &= (k_1\\bm{A}_{l_1}(t)+\\cdots + k_m \\bm{A}_{l_m}(t))\\bm{P} = \\bm{A}_i(t^+) \\bm{P} \\nonumber\n\\end{align}\nThen we can ensure that $\\bm{H}(t)$ is always the random projection of $\\bm{A}(t)$ and thus preserve the inner product of the $\\bm{A}(t)$.\n\n\n\n\n\n\n\n",
                "subsection 12.1": {
                    "name": "Detailed Updating Mechanism",
                    "content": "\nBased on the above analysis, we give the detailed propagation mechanism of methods in \\secref{sec:pre}. For NAT, PINT and DyGFormer, their temporal matrix element $A_{u,v}^{(l)}(t)$ is the number of the $l$-step temporal walks from $u$ to $v$ and their feature propagation mechanism is shown in \\algoref{alg:propagation1}, where the obtained node representations are the projection of the corresponding temporal walk matrix.\n\nFor CAWN, its score function for a temporal walk $W=[(w_0,t_0),(w_1,t_1),...,(w_l,t_l)]$ is defined as $s(W) = \\prod_{i=0}^{l-1} \\frac{\\text{exp}(-\\alpha(t_{i}-t_{i+1}))}{\\sum_{(\\{w',w\\},t')\\in \\mathcal{E}_{w_i,t_i}} \\textrm{exp}(-\\alpha(t_i-t'))}$. As time goes on, its element of temporal walk matrices will not change. When a new interaction $(u,v,t)$ happens, for a temporal walk matrix $\\bm{A}^{(l)}(t)$, its $u$-th row and $v$-th row will change. Formally, let $\\bm{d}(t) \\in \\mathbb{R}^n$ denotes a time decay degree vector, where for each node $u$, $d_u(t)$ is defined as $d_u(t) = \\sum_{(\\{(u,v'\\},t') \\in \\mathcal{E}_{u,t}} \\exp (-\\alpha (t'-t))$ with $\\mathcal{E}_{u,t}$ denoting the set of interactions attached to $u$ before $t$, then the updating function of the temporal walk matrix $\\bm{A}^{(k)}(t)$ can be represented as \n\\begin{equation}\n\\begin{split}\n    \\bm{A}_u^{(l)}(t^+) = \\frac{d_u(t)}{d_u(t)+1}* \\bm{A}_u^{(l)}(t) + \\frac{1}{d_u(t)+1}*\\bm{A}_v^{(l-1)}(t), \\\\ \n    \\bm{A}_v^{(l)}(t^+) = \\frac{d_v(t)}{d_v(t)+1}* \\bm{A}_v^{(l)}(t) + \\frac{1}{d_v(t)+1}*\\bm{A}_u^{(l-1)}(t),  \n\\end{split} \n\\end{equation}\nwhere $\\frac{d_u(t)}{d_u(t)+1}* \\bm{A}_u^{(l)}(t)$ corresponds to the change in score of the old $l$-step temporal walks begging from $u$ and $\\frac{1}{d_u(t)+1}*\\bm{A}_v^{(l-1)}(t)$ correspond to change caused by the newly generated $l$-step temporal walk from $u$ (the same for $v$ and similar analysis about the updating of temporal walk matrix can be found in \\appendixref{sec:proof_of_theorem_1}). Then we can give the propagation mechanism for CAWN in \\algoref{alg:propagation2} based on the above analysis, where $\\bm{d}$ corresponds to the time decay degree vector.\n\n"
                }
            },
            "section 13": {
                "name": "Broader Impact",
                "content": "\nWe proposed an effective and efficient temporal link prediction method, which may advance real-world scenarios that rely on link prediction as a cornerstone, such as recommendation systems. For potential negative impacts, overly accurate link prediction in some contexts may lead to imbalanced outcomes such as reduced diversity in recommendation systems.\n\n% \\begin{minipage}{0.8\\textwidth}\n% \\begin{algorithm}[H]\n%     \\caption{CAW-style Random Projections}\n%     \\label{alg:sample}\n%     \\begin{flushleft}\n%     \\textbf{Input}: Dynamic graph $\\mathcal{G}=\\{(u_i,v_i,t_i)\\}$, representation dimension $d$, node number $n$, time decay weight $\\lambda$, layer of representations $k$\\\\ \n%     \\textbf{Output}: Node representations $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k, \\bm{H}^{(i)} \\in R^{n\\times d}\\}$\n%     \\end{flushleft}\n%     \\begin{algorithmic}[1] %[1] enables line numbers\n%     \\STATE Initialize $\\bm{H}^{(0)}$ with entries drawn independently from $\\mathcal{N}(0,\\frac{1}{\\sqrt{d}})$. \n%      \\STATE Initialize $\\bm{H}^{(1)},\\bm{H}^{(2)},...,\\bm{H}^{(n)}$ as zero matrix, $t_{prev} \\gets 0$\n%     \\STATE Initialize $\\bm{d} \\in R^{n}$ as a zero vector\n%     \\FOR{$(u,v,t) \\in \\mathcal{G}$}\n%     \\FOR{$i=k$ \\TO $1$}\n%         \\STATE $\\bm{H}^{(i)}_u = \\bm{H}^{(i)}_u + \\frac{\\textrm{exp}(\\lambda t)}{d_u+\\textrm{exp}(\\lambda t)}(\\bm{H}^{(i-1)}_v - \\bm{H}^{(i)}_u)$\n%         \\STATE $\\bm{H}^{(i)}_v = \\bm{H}^{(i)}_v + \\frac{\\textrm{exp}(\\lambda t)}{d_v+\\textrm{exp}(\\lambda t)}(\\bm{H}^{(i-1)}_u - \\bm{H}^{(i)}_v)$\n%     \\ENDFOR\n%     \\STATE $d_u = d_u + \\textrm{exp}(\\lambda t)$\n%     \\STATE $d_v = d_v + \\textrm{exp}(\\lambda t)$\n%     \\STATE $t_{prev} \\gets t$\n%     \\ENDFOR\n%     \\STATE \\textbf{return} $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k\\}$\n%     \\end{algorithmic}\n% \\end{algorithm}\n% \\end{minipage}\n\n% \\begin{minipage}{0.8\\textwidth}\n% \\begin{algorithm}[H]\n%     \\caption{CAW-style Random Projections with Non-Intersection Batch}\n%     \\label{alg:sample}\n%     \\begin{flushleft}\n%     \\textbf{Input}: Dynamic graph $\\mathcal{G}=\\{(u_i,v_i,t_i)\\}$, representation dimension $d$, node number $n$, time decay weight $\\lambda$, layer of representations $k$\\\\ \n%     \\textbf{Output}: Node representations $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k, \\bm{H}^{(i)} \\in R^{n\\times d}\\}$\n%     \\end{flushleft}\n%     \\begin{algorithmic}[1] %[1] enables line numbers\n%     \\STATE Initialize $\\bm{H}^{(0)}$ with entries drawn independently from $\\mathcal{N}(0,\\frac{1}{\\sqrt{d}})$. \n%      \\STATE Initialize $\\bm{H}^{(1)},\\bm{H}^{(2)},...,\\bm{H}^{(n)}$ as zero matrix, $t_{prev} \\gets 0$\n%      \\STATE Initialize $\\bm{d} \\in R^{n}$ as a zero vector\n%     \\FOR{$\\{(u_1,v_1,t_1),...,(u_b,v_b,t_b)\\} \\in \\mathcal{G}$}\n%     \\STATE Initialize $\\bm{X}^{(1)},...,\\bm{X}^{(n)}$ as zero matrix\n%     \\STATE Initialize $\\bm{\\Delta d}$ as a zero vector\n%     % \\STATE $\\bm{d} = \\bm{d}*\\textrm{exp}(-\\lambda (t_b-t_{prev}))$\n%     \\FOR{$j=1$ \\TO $b$}\n%         \\STATE $\\Delta d_{u_j} = \\Delta d_{u_j} + \\textrm{exp}(\\lambda t_j)$\n%         \\STATE $\\Delta d_{v_j} = \\Delta d_{v_j} + \\textrm{exp}(\\lambda t_j)$\n%     \\ENDFOR\n%     \\FOR{$j=1$ \\TO $b$}\n%         \\FOR{$i=k$ \\TO $1$}\n%             \\STATE $\\bm{X}^{(i)}_{u_j} = \\bm{X}^{(i)}_{u_j} + \\frac{\\textrm{exp}(\\lambda t_j)}{d_{u_j}+ \\Delta d_{u_j}}(\\bm{H}^{(i-1)}_{v_j}-\\bm{H}^{(i)}_{u_j})$\n%             \\STATE $\\bm{X}^{(i)}_{v_j} =\\bm{X}^{(i)}_{v_j} +  \\frac{\\textrm{exp}(\\lambda t_j)}{d_{v_j} + \\Delta d_{v_j}}(\\bm{H}^{(i-1)}_{u_j} - \\bm{H}^{(i)}_{v_j})$\n%     \\ENDFOR\n%     \\ENDFOR\n%     \\FOR{$i=1$ \\TO $k$}\n%         \\STATE $\\bm{H}^{(i)} = \\bm{H}^{(i)} + \\bm{X}^{(i)}$\n%     \\ENDFOR\n%     \\STATE $t_{prev} \\gets t_b$\n%     \\STATE $\\bm{d} = \\bm{d} + \\bm{\\Delta d}$\n%     \\ENDFOR\n%     \\STATE \\textbf{return} $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k\\}$\n%     \\end{algorithmic}\n% \\end{algorithm}\n% \\end{minipage}\n\n\n\n% ==================================================\n% Online CAW-style Random Projections\n% =================================================\n% \\begin{minipage}{0.8\\textwidth}\n% \\begin{algorithm}[H]\n%     \\caption{CAW-style Random Projections}\n%     \\label{alg:sample}\n%     \\begin{flushleft}\n%     \\textbf{Input}: Dynamic graph $\\mathcal{G}=\\{(u_i,v_i,t_i)\\}$, representation dimension $d$, node number $n$, time decay weight $\\lambda$, layer of representations $k$\\\\ \n%     \\textbf{Output}: Node representations $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k, \\bm{H}^{(i)} \\in R^{n\\times d}\\}$\n%     \\end{flushleft}\n%     \\begin{algorithmic}[1] %[1] enables line numbers\n%     \\STATE Initialize $\\bm{H}^{(0)}$ with entries drawn independently from $\\mathcal{N}(0,\\frac{1}{\\sqrt{d}})$. \n%      \\STATE Initialize $\\bm{H}^{(1)},\\bm{H}^{(2)},...,\\bm{H}^{(n)}$ as zero matrix, $t_{prev} \\gets 0$\n%     \\STATE Initialize $\\bm{d} \\in R^{n}$ as a zero vector\n%     \\FOR{$(u,v,t) \\in \\mathcal{G}$}\n%     \\STATE $\\bm{d} = \\bm{d}*\\textrm{exp}(-\\lambda (t-t_{prev}))$\n%     \\FOR{$i=k$ \\TO $1$}\n%         \\STATE $\\bm{H}^{(i)}_u = \\bm{H}^{(i)}_u + \\frac{1}{d_u+1}(\\bm{H}^{(i-1)}_v - \\bm{H}^{(i)}_u)$\n%         \\STATE $\\bm{H}^{(i)}_v = \\bm{H}^{(i)}_v + \\frac{1}{d_u+1}(\\bm{H}^{(i-1)}_u - \\bm{H}^{(i)}_v)$\n%     \\ENDFOR\n%     \\STATE $d_u = d_u + 1$\n%     \\STATE $d_v = d_v + 1$\n%     \\STATE $t_{prev} \\gets t$\n%     \\ENDFOR\n%     \\STATE \\textbf{return} $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k\\}$\n%     \\end{algorithmic}\n% \\end{algorithm}\n% \\end{minipage}\n\n% \u8981\u6c42\uff1a\u540c\u4e00batch\u5185\u4e0d\u540c\u7684\u8fb9\u6ca1\u6709\u4ea4\u70b9\u6216\u8005\u540c\u4e00batch\u5185\u6240\u6709\u7684\u8fb9\u90fd\u662f\u540c\u4e00\u65f6\u523b\u53d1\u751f\u7684\n% \\begin{minipage}{0.8\\textwidth}\n% \\begin{algorithm}[H]\n%     \\caption{CAW-style Random Projections with Non-Intersection Batch}\n%     \\label{alg:sample}\n%     \\begin{flushleft}\n%     \\textbf{Input}: Dynamic graph $\\mathcal{G}=\\{(u_i,v_i,t_i)\\}$, representation dimension $d$, node number $n$, time decay weight $\\lambda$, layer of representations $k$\\\\ \n%     \\textbf{Output}: Node representations $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k, \\bm{H}^{(i)} \\in R^{n\\times d}\\}$\n%     \\end{flushleft}\n%     \\begin{algorithmic}[1] %[1] enables line numbers\n%     \\STATE Initialize $\\bm{H}^{(0)}$ with entries drawn independently from $\\mathcal{N}(0,\\frac{1}{\\sqrt{d}})$. \n%      \\STATE Initialize $\\bm{H}^{(1)},\\bm{H}^{(2)},...,\\bm{H}^{(n)}$ as zero matrix, $t_{prev} \\gets 0$\n%      \\STATE Initialize $\\bm{d} \\in R^{n}$ as a zero vector\n%     \\FOR{$\\{(u_1,v_1,t_1),...,(u_b,v_b,t_b)\\} \\in \\mathcal{G}$}\n%     \\STATE Initialize $\\bm{X}^{(1)},...,\\bm{X}^{(n)}$ as zero matrix\n%     \\STATE Initialize $\\bm{\\Delta d}$ as a zero vector\n%     \\STATE $\\bm{d} = \\bm{d}*\\textrm{exp}(-\\lambda (t_b-t_{prev}))$\n%     \\FOR{$j=1$ \\TO $b$}\n%         \\STATE $\\Delta d_{u_j} = \\Delta d_{u_j} + \\textrm{exp}(-\\lambda (t_b-t_j))$\n%         \\STATE $\\Delta d_{v_j} = \\Delta d_{v_j} + \\textrm{exp}(-\\lambda (t_b-t_j))$\n%     \\ENDFOR\n%     \\FOR{$j=1$ \\TO $b$}\n%         \\FOR{$i=k$ \\TO $1$}\n%             \\STATE $\\bm{X}^{(i)}_{u_j} = \\bm{X}^{(i)}_{u_j} + \\frac{\\textrm{exp}(-\\lambda (t_b-t_j))}{d_{u_j} + \\Delta d_{u_j}}(\\bm{H}^{(i-1)}_{v_j}-\\bm{H}^{(i)}_{u_j})$\n%             \\STATE $\\bm{X}^{(i)}_{v_j} =\\bm{X}^{(i)}_{v_j} +  \\frac{\\textrm{exp}(-\\lambda (t_b-t_j))}{d_{v_j} + \\Delta d_{v_j}}(\\bm{H}^{(i-1)}_{u_j} - \\bm{H}^{(i)}_{v_j})$\n%     \\ENDFOR\n%     \\ENDFOR\n%     \\FOR{$i=1$ \\TO $k$}\n%         \\STATE $\\bm{H}^{(i)} = \\bm{H}^{(i)} + \\bm{X}^{(i)}$\n%     \\ENDFOR\n%     \\STATE $t_{prev} \\gets t_b$\n%     \\STATE $\\bm{d} = \\bm{d} + \\bm{\\Delta d}$\n%     \\ENDFOR\n%     \\STATE \\textbf{return} $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k\\}$\n%     \\end{algorithmic}\n% \\end{algorithm}\n% \\end{minipage}\n\n\n% ==================================================\n% Online Decay Sum Random Projections\n% =================================================\n% \\begin{minipage}{0.67\\textwidth}\n% \\begin{algorithm}[H]\n%     \\caption{Decay Sum Random Projections}\n%     \\label{alg:sample}\n%     \\begin{flushleft}\n%     \\textbf{Input}: Dynamic graph $\\mathcal{G}=\\{(u_i,v_i,t_i)\\}$, representation dimension $d$, node number $n$, time decay weight $\\lambda$, layer of representations $k$\\\\ \n%     \\textbf{Output}: Node representations $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k, \\bm{H}^{(i)} \\in R^{n\\times d}\\}$\n%     \\end{flushleft}\n%     \\begin{algorithmic}[1] %[1] enables line numbers\n%     \\STATE Initialize $\\bm{H}^{(0)}$ with entries drawn independently from $\\mathcal{N}(0,\\frac{1}{\\sqrt{d}})$. \n%      \\STATE Initialize $\\bm{H}^{(1)},\\bm{H}^{(2)},...,\\bm{H}^{(n)}$ as zero matrix, $t_{prev} \\gets 0$\n%     \\FOR{$(u,v,t) \\in \\mathcal{G}$}\n%     \\FOR{$i=1$ \\TO $k$}\n%         \\STATE $\\bm{H}^{(i)} = \\bm{H}^{(i)}*e^{-i\\lambda(t-t_{prev})}$\n%     \\ENDFOR\n%     \\FOR{$i=k$ \\TO $1$}\n%         \\STATE $\\bm{H}^{(i)}_u = \\bm{H}^{(i)}_u + \\bm{H}^{(i-1)}_v$\n%         \\STATE $\\bm{H}^{(i)}_v = \\bm{H}^{(i)}_v + \\bm{H}^{(i-1)}_u$\n%     \\ENDFOR\n%     \\STATE $t_{prev} \\gets t$\n%     \\ENDFOR\n%     \\STATE \\textbf{return} $\\{\\bm{H}^{(i)}|0 \\leq i \\leq k\\}$\n%     \\end{algorithmic}\n% \\end{algorithm}\n% \\end{minipage}\n\n"
            },
            "section 14": {
                "name": "Experimental Settings",
                "content": " \\label{sec:experimental_settings}\n",
                "subsection 14.1": {
                    "name": "Datasets",
                    "content": " \\label{sec:datasets}\nExperiments are conducted on the following 13 benchmark datasets \\footnote{\\url{https://zenodo.org/record/7213796\\#.Y1cO6y8r30o}} collected by  \\cite{DBLP:conf/nips/PoursafaeiHPR22}.\n\\begin{itemize}\n    \\item  \\textbf{Wikipedia} is a bipartite interaction graph that records the edits on Wikipedia pages over a month. Nodes represent users and pages, and links denote the editing behaviors with timestamps. Each link is associated with a 172-dimensional Linguistic Inquiry and Word Count (LIWC) feature \\cite{pennebaker2001linguistic}.\n\n    \\item  \\textbf{Reddit} is a bipartite graph capturing user posts under subreddits over a month. Nodes represent users and subreddits, while links denote timestamped posts. Each link carries a 172-dimensional LIWC feature.\n    \n    \\item  \\textbf{MOOC} is a bipartite interaction network of online courses, where nodes represent students and course content units (e.g., videos, problem sets). Links indicate students' access to specific content units and have a 4-dimensional feature.\n    \n    \\item  \\textbf{LastFM} is a bipartite network detailing song-listening behaviors of users over one month. Nodes are users and songs, and links represent listening activities.\n    \n    \\item  \\textbf{Enron} records email communications between employees of the Enron Energy Corporation over three years.\n    \n    \\item  \\textbf{Social Evo.} is a mobile phone proximity network tracking daily activities within an undergraduate dormitory for eight months, with each link having a 2-dimensional feature.\n    \n    \\item  \\textbf{UCI} is an online communication network with nodes representing university students and links representing posted messages.\n    \n    \\item  \\textbf{Flights} is a dynamic flight network showing air traffic development during the COVID-19 pandemic. Nodes represent airports, and links denote tracked flights. Each link has a weight indicating the number of flights between two airports per day.\n    \n    \\item  \\textbf{Can. Parl.} is a dynamic political network recording interactions between Canadian Members of Parliament (MPs) from 2006 to 2019. Nodes represent MPs from electoral districts, and links are formed when two MPs vote \"yes\" on a bill. The weight of each link represents the number of times one MP voted \"yes\" in support of another MP in a year.\n    \n    \\item  \\textbf{US Legis.} is a senate co-sponsorship network tracking social interactions between US Senators. The weight of each link indicates the number of times two senators co-sponsored a bill in a given congress.\n    \n    \\item  \\textbf{UN Trade} captures food and agriculture trade between 181 nations over more than 30 years. The weight of each link represents the total normalized agriculture import or export values between two countries.\n    \n    \\item  \\textbf{UN Vote} records roll-call votes in the United Nations General Assembly. A link between two nations increases in weight each time both vote \"yes\" on an item.\n    \n    \\item  \\textbf{Contact} tracks the evolution of physical proximity among approximately 700 university students over a month. Each student has a unique identifier, and links indicate close proximity, with weights revealing the extent of physical closeness between students.\n\\end{itemize}\nThe statistics of the datasets are shown in \\tabref{tab:data_statistics}, where \\#N\\&L Feat stands for the dimensions of node and link features. \n\n\n\n\n\n\n\n\n\n\n% \\textbf{Implementation Details}. For \\model, the layer $k$ and dimension $d_R$ of the node representations are set to be 3 and $10*\\text{log(E)}$, where E is the number of the interactions, and we find the best time decay weight $\\lambda$ via grid search. For baselines, we use the implementation of DyGLib \\cite{DBLP:conf/nips/Yu23}, which is a unified temporal graph learning library, and tune thes the best hyperparameters for each baseline. For baselines that are not included in DyGLib (i.e., NAT and PINT), we use their official implementation and find the best hyperparameters via grid search. We run each experiment five times and report the average. More implementation details can be found in \\appendixref{sec:implementation}.\n\n"
                },
                "subsection 14.2": {
                    "name": "Baselines",
                    "content": " \\label{sec:baselines}\nWe select the following eleven popular baselines:\n\\begin{itemize}\n    \\item  \\textbf{JODIE} \\cite{DBLP:conf/kdd/KumarZL19} designs a recurrent architecture to maintain a memory vector for each node and a projection layer to map the node memories into future representation trajectories.\n    \n    \\item  \\textbf{DyRep} \\cite{DBLP:conf/iclr/TrivediFBZ19} considers each link as a temporal point process \\cite{diggle2006spatio} and designs a deep temporal point process model to capture the dynamics of the observed process.\n\n    \\item  \\textbf{TGAT} \\cite{DBLP:conf/iclr/XuRKKA20} proposes a time encoding function based on Bochner\u2019s Theorem \\cite{rudin2017fourier} and combines it with the graph attention mechanism \\cite{DBLP:conf/iclr/VelickovicCCRLB18} to learn dynamic node representations.\n    \n    \\item  \\textbf{TGN} \\cite{DBLP:journals/corr/tgn} proposes a general framework for temporal graph learning, which includes a memory module to maintain node memories and an embedding module to aggregate node memories.\n    \n    \\item  \\textbf{CAWN} \\cite{DBLP:conf/iclr/WangCLL021} captures the evolution pattern of the temporal graph by causal anonymous walks. For a given link, CAWN first sampled a set of temporal walks beginning from the two end nodes respectively and constructs relative encodings. The sampled walks together with relative encodings are then mapped into node representations via a sequential model.\n\n    \\item  \\textbf{EdgeBank} \\cite{DBLP:conf/nips/PoursafaeiHPR22} is a statistical method, that gives the likelihood of a link based on historical interactions between the two end nodes.\n    \n    \\item  \\textbf{TCL} \\cite{DBLP:journals/corr/tcl} propose a graph transformer architecture \\cite{DBLP:conf/nips/YingCLZKHSL21} to learning node representations, which samples neighbor nodes based on BFS and maps them into the node representation.\n    \n    \\item \\textbf{NAT} \\cite{DBLP:conf/log/LuoL22} propose a dictionary-type neighborhood representation to efficiently capture the correlation information between nodes, which maintains a series of N-caches to store the neighborhood information and use them to decode the pairwise information between nodes.\n\n    \\item \\textbf{PINT} \\cite{DBLP:conf/nips/SouzaMKG22} proposes an injective temporal message passing mechanism to learn node representations and relative positional features constructed based on temporal walk counting to inject pairwise information between nodes.\n\n    \\item  \\textbf{GraphMixer} \\cite{DBLP:conf/iclr/CongZKYWZTM23} proposes a simplified temporal graph learning architecture, which employs MLP-Mixer to learn the representation of a node from its historical interaction sequences.\n\n    \\item \\textbf{DyGFormer} \\cite{DBLP:conf/nips/Yu23} proposes a transformer architecture \\cite{DBLP:conf/nips/VaswaniSPUJGKP17} to learning node representations, which include a patching technique to capture the long-term histories of a node to and a neighbor co-occurrence encoding scheme to capture the correlation between nodes.\n\\end{itemize}\n\n"
                }
            },
            "section 15": {
                "name": "Additional Experimental Results",
                "content": "\n",
                "subsection 15.1": {
                    "name": "Performance Comparison",
                    "content": " \\label{apdx:performance_comparison}\nThe results under other settings are shown in \\tabref{tab:inductive_random}, \\tabref{tab:transductive_historical}, \\tabref{tab:inductive_historical}, \\tabref{tab:transductive_inductive} and \\tabref{tab:inductive_inductive}.\n\n"
                },
                "subsection 15.2": {
                    "name": "Efficicency Analysis",
                    "content": " \\label{sec:efficiency}\nEfficiency analysis results on Reddit, UCI, and Wikipedia are shown in \\figref{fig:efficiency_appdx}.\n\n\n"
                },
                "subsection 15.3": {
                    "name": "Scalability Analysis",
                    "content": " \\label{sec:scalability}\nThe scalability analysis result of PINT is shown in \\tabref{tab:scalability}, where OOM indicates the out-of-memory error.\n"
                },
                "subsection 15.4": {
                    "name": "Influence of Node Representation Dimension",
                    "content": " \\label{sec:dimension_change_appdx}\nResults on Wikipedia, Enron, and UCI are shown in \\figref{fig:dimension_change_appdx}.\n\n\n"
                },
                "subsection 15.5": {
                    "name": "Statistic Analysis of Node Representations",
                    "content": " \\label{sec:representation_distribution}\n\\tabref{tab:representation_distribution} shows the mean of node representation norm at different layers, where aEb indicates $a\\times 10 ^b$.\n\n% \\subsection{Ablation Study Discussion}\n% We conduct a detailed ablation study to investigate the influence of each module in \\model, which mainly includes the injection way of pairwise feature and the construction of the temporal walk matrix.\n\n% \\textbf{Injection way of pairwise feature}. In \\model, for a temporal link $(u,v,t)$, the pairwise feature is injected through the pairwise feature $\\bm{f}_{u,v}$ and the relative encoding sequence $\\bm{X}_{u,F}, \\bm{X}_{v,F}$ in the auxiliary feature learning. We remove the pairwise feature (denoted as w/o PF), relative encoding sequence (denoted as w/o RE), and both of them (denoted as w/o both) respectively, and report their performance.  As shown in \\tabref{tab:inject_way}, after removing all the pairwise information, there is a significant drop in the performance of \\model, showing the effectiveness of the constructed features. The performance drop of w/o RE shows the necessity of feeding the relative encodings into the auxiliary feature learning procedure, where the node representations are maintained unsupervised, and thus the pairwise feature may ignore some complex pairwise information. For w/o PF, the auxiliary feature learning procedure only considers the recent $m$ interactions, and thus removing the pairwise feature may lead to a performance drop since it is extracted based on all historical temporal walks of node $u$ and $v$.\n\n\n% Additionally, the auxiliary feature learning procedure only includes the most $M$ interactions and thus may not capture the full histories of the target node, which xx. \n\n% The performance drops of w/o RE and w/o PF indicate that the relative encoding and the pairwise feature are complementary. On one hand, the procedure to maintain the node representation is unsupervised, which may limit its representation ability and thus need to feed the relative encodings into the auxiliary feature learning procedure to capture some complicated pairwise information. On the other hand, the auxiliary feature learning procedure only takes the recent $m$ interactions, which can not consider all the walk information between two nodes and thus needs the pairwise feature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% \\clearpage\n% \\input{section-8-checkbox}\n\n\n"
                }
            }
        },
        "tables": {
            "tab:transductive_random": "\\begin{table}[]\n\\caption{Transductive results for different baselines under the random negative sampling strategy. \\textbf{blod} and \\underline{underline} highlight the best and second best result respectively.}\n\\resizebox{1.05\\textwidth}{!}{\n\\setlength{\\tabcolsep}{0.7mm}\n\\setlength{\\extrarowheight}{0.3mm}\n\\begin{tabular}{cccccccccccccc}\n\\toprule\nMetric              &  Dataset    & JODIE        & DyRep        & TGAT         & TGN          & CAWN         & EdgeBank     & TCL          & GraphMixer   & NAT          & PINT         & DyGFormer    & TPNet        \\\\  \\midrule\n\\multirow{14}{*}{AP}   &  Wikipedia    & $96.50{\\scriptscriptstyle \\pm 0.14}$ & $94.86{\\scriptscriptstyle \\pm 0.06}$ & $96.94{\\scriptscriptstyle \\pm 0.06}$ & $98.45{\\scriptscriptstyle \\pm 0.06}$ & $98.76{\\scriptscriptstyle \\pm 0.03}$ & $90.37{\\scriptscriptstyle \\pm 0.00}$ & $96.47{\\scriptscriptstyle \\pm 0.16}$ & $97.25{\\scriptscriptstyle \\pm 0.03}$ & $98.03{\\scriptscriptstyle \\pm 0.07}$ & $98.45{\\scriptscriptstyle \\pm 0.04}$ & $\\underline{99.03{\\scriptscriptstyle \\pm 0.02}}$ & $\\bm{99.32{\\scriptscriptstyle \\pm 0.03}}$\\\\\n                       &  Reddit       & $98.31{\\scriptscriptstyle \\pm 0.14}$ & $98.22{\\scriptscriptstyle \\pm 0.04}$ & $98.52{\\scriptscriptstyle \\pm 0.02}$ & $98.63{\\scriptscriptstyle \\pm 0.06}$ & $99.11{\\scriptscriptstyle \\pm 0.01}$ & $94.86{\\scriptscriptstyle \\pm 0.00}$ & $97.53{\\scriptscriptstyle \\pm 0.02}$ & $97.31{\\scriptscriptstyle \\pm 0.01}$ & $99.13{\\scriptscriptstyle \\pm 0.10}$ & $99.15{\\scriptscriptstyle \\pm 0.02}$ & $\\underline{99.22{\\scriptscriptstyle \\pm 0.01}}$ & $\\bm{99.27{\\scriptscriptstyle \\pm 0.00}}$\\\\\n                       &  MOOC         & $80.23{\\scriptscriptstyle \\pm 2.44}$ & $81.97{\\scriptscriptstyle \\pm 0.49}$ & $85.84{\\scriptscriptstyle \\pm 0.15}$ & $\\underline{89.15{\\scriptscriptstyle \\pm 1.60}}$ & $80.15{\\scriptscriptstyle \\pm 0.25}$ & $57.97{\\scriptscriptstyle \\pm 0.00}$ & $82.38{\\scriptscriptstyle \\pm 0.24}$ & $82.78{\\scriptscriptstyle \\pm 0.15}$ & $85.88{\\scriptscriptstyle \\pm 0.55}$ & $88.08{\\scriptscriptstyle \\pm 0.86}$ & $87.52{\\scriptscriptstyle \\pm 0.49}$ & $\\bm{96.39{\\scriptscriptstyle \\pm 0.09}}$\\\\\n                       &  LastFM       & $70.85{\\scriptscriptstyle \\pm 2.13}$ & $71.92{\\scriptscriptstyle \\pm 2.21}$ & $73.42{\\scriptscriptstyle \\pm 0.21}$ & $77.07{\\scriptscriptstyle \\pm 3.97}$ & $86.99{\\scriptscriptstyle \\pm 0.06}$ & $79.29{\\scriptscriptstyle \\pm 0.00}$ & $67.27{\\scriptscriptstyle \\pm 2.16}$ & $75.61{\\scriptscriptstyle \\pm 0.24}$ & $88.02{\\scriptscriptstyle \\pm 1.94}$ & $89.66{\\scriptscriptstyle \\pm 1.81}$ & $\\underline{93.00{\\scriptscriptstyle \\pm 0.12}}$ & $\\bm{94.50{\\scriptscriptstyle \\pm 0.08}}$\\\\\n                       &  Enron        & $84.77{\\scriptscriptstyle \\pm 0.30}$ & $82.38{\\scriptscriptstyle \\pm 3.36}$ & $71.12{\\scriptscriptstyle \\pm 0.97}$ & $86.53{\\scriptscriptstyle \\pm 1.11}$ & $89.56{\\scriptscriptstyle \\pm 0.09}$ & $83.53{\\scriptscriptstyle \\pm 0.00}$ & $79.70{\\scriptscriptstyle \\pm 0.71}$ & $82.25{\\scriptscriptstyle \\pm 0.16}$ & $90.60{\\scriptscriptstyle \\pm 0.66}$ & $92.20{\\scriptscriptstyle \\pm 0.15}$ & $\\underline{92.47{\\scriptscriptstyle \\pm 0.12}}$ & $\\bm{92.90{\\scriptscriptstyle \\pm 0.17}}$\\\\\n                       &  Social Evo.  & $89.89{\\scriptscriptstyle \\pm 0.55}$ & $88.87{\\scriptscriptstyle \\pm 0.30}$ & $93.16{\\scriptscriptstyle \\pm 0.17}$ & $93.57{\\scriptscriptstyle \\pm 0.17}$ & $84.96{\\scriptscriptstyle \\pm 0.09}$ & $74.95{\\scriptscriptstyle \\pm 0.00}$ & $93.13{\\scriptscriptstyle \\pm 0.16}$ & $93.37{\\scriptscriptstyle \\pm 0.07}$ & $88.92{\\scriptscriptstyle \\pm 3.45}$ & $94.42{\\scriptscriptstyle \\pm 0.03}$ & $\\bm{94.73{\\scriptscriptstyle \\pm 0.01}}$ & $\\bm{94.73{\\scriptscriptstyle \\pm 0.02}}$\\\\\n                       &  UCI          & $89.43{\\scriptscriptstyle \\pm 1.09}$ & $65.14{\\scriptscriptstyle \\pm 2.30}$ & $79.63{\\scriptscriptstyle \\pm 0.70}$ & $92.34{\\scriptscriptstyle \\pm 1.04}$ & $95.18{\\scriptscriptstyle \\pm 0.06}$ & $76.20{\\scriptscriptstyle \\pm 0.00}$ & $89.57{\\scriptscriptstyle \\pm 1.63}$ & $93.25{\\scriptscriptstyle \\pm 0.57}$ & $93.40{\\scriptscriptstyle \\pm 0.26}$ & $\\underline{96.45{\\scriptscriptstyle \\pm 0.11}}$ & $95.79{\\scriptscriptstyle \\pm 0.17}$ & $\\bm{97.35{\\scriptscriptstyle \\pm 0.04}}$\\\\\n                       &  Flights      & $95.60{\\scriptscriptstyle \\pm 1.73}$ & $95.29{\\scriptscriptstyle \\pm 0.72}$ & $94.03{\\scriptscriptstyle \\pm 0.18}$ & $97.95{\\scriptscriptstyle \\pm 0.14}$ & $98.51{\\scriptscriptstyle \\pm 0.01}$ & $89.35{\\scriptscriptstyle \\pm 0.00}$ & $91.23{\\scriptscriptstyle \\pm 0.02}$ & $90.99{\\scriptscriptstyle \\pm 0.05}$ & $98.57{\\scriptscriptstyle \\pm 0.12}$ & $98.80{\\scriptscriptstyle \\pm 0.02}$ & $\\underline{98.91{\\scriptscriptstyle \\pm 0.01}}$ & $\\bm{98.93{\\scriptscriptstyle \\pm 0.02}}$\\\\\n                       &  Can. Parl.   & $69.26{\\scriptscriptstyle \\pm 0.31}$ & $66.54{\\scriptscriptstyle \\pm 2.76}$ & $70.73{\\scriptscriptstyle \\pm 0.72}$ & $70.88{\\scriptscriptstyle \\pm 2.34}$ & $69.82{\\scriptscriptstyle \\pm 2.34}$ & $64.55{\\scriptscriptstyle \\pm 0.00}$ & $68.67{\\scriptscriptstyle \\pm 2.67}$ & $77.04{\\scriptscriptstyle \\pm 0.46}$ & $79.72{\\scriptscriptstyle \\pm 1.76}$ & $68.36{\\scriptscriptstyle \\pm 1.43}$ & $\\bm{97.36{\\scriptscriptstyle \\pm 0.45}}$ & $\\underline{90.28{\\scriptscriptstyle \\pm 0.37}}$\\\\\n                       &  US Legis.    & $75.05{\\scriptscriptstyle \\pm 1.52}$ & $75.34{\\scriptscriptstyle \\pm 0.39}$ & $68.52{\\scriptscriptstyle \\pm 3.16}$ & $75.99{\\scriptscriptstyle \\pm 0.58}$ & $70.58{\\scriptscriptstyle \\pm 0.48}$ & $58.39{\\scriptscriptstyle \\pm 0.00}$ & $69.59{\\scriptscriptstyle \\pm 0.48}$ & $70.74{\\scriptscriptstyle \\pm 1.02}$ & $\\underline{78.71{\\scriptscriptstyle \\pm 0.87}}$ & $74.85{\\scriptscriptstyle \\pm 0.97}$ & $71.11{\\scriptscriptstyle \\pm 0.59}$ & $\\bm{80.58{\\scriptscriptstyle \\pm 0.23}}$\\\\\n                       &  UN Trade     & $64.94{\\scriptscriptstyle \\pm 0.31}$ & $63.21{\\scriptscriptstyle \\pm 0.93}$ & $61.47{\\scriptscriptstyle \\pm 0.18}$ & $65.03{\\scriptscriptstyle \\pm 1.37}$ & $65.39{\\scriptscriptstyle \\pm 0.12}$ & $60.41{\\scriptscriptstyle \\pm 0.00}$ & $62.21{\\scriptscriptstyle \\pm 0.03}$ & $62.61{\\scriptscriptstyle \\pm 0.27}$ & $\\underline{73.95{\\scriptscriptstyle \\pm 1.16}}$ & $70.20{\\scriptscriptstyle \\pm 0.58}$ & $66.46{\\scriptscriptstyle \\pm 1.29}$ & $\\bm{87.24{\\scriptscriptstyle \\pm 0.65}}$\\\\\n                       &  UN Vote      & $63.91{\\scriptscriptstyle \\pm 0.81}$ & $62.81{\\scriptscriptstyle \\pm 0.80}$ & $52.21{\\scriptscriptstyle \\pm 0.98}$ & $65.72{\\scriptscriptstyle \\pm 2.17}$ & $52.84{\\scriptscriptstyle \\pm 0.10}$ & $58.49{\\scriptscriptstyle \\pm 0.00}$ & $51.90{\\scriptscriptstyle \\pm 0.30}$ & $52.11{\\scriptscriptstyle \\pm 0.16}$ & $\\underline{70.45{\\scriptscriptstyle \\pm 0.68}}$ & $66.25{\\scriptscriptstyle \\pm 0.78}$ & $55.55{\\scriptscriptstyle \\pm 0.42}$ & $\\bm{75.12{\\scriptscriptstyle \\pm 0.29}}$\\\\\n                       &  Contact      & $95.31{\\scriptscriptstyle \\pm 1.33}$ & $95.98{\\scriptscriptstyle \\pm 0.15}$ & $96.28{\\scriptscriptstyle \\pm 0.09}$ & $96.89{\\scriptscriptstyle \\pm 0.56}$ & $90.26{\\scriptscriptstyle \\pm 0.28}$ & $92.58{\\scriptscriptstyle \\pm 0.00}$ & $92.44{\\scriptscriptstyle \\pm 0.12}$ & $91.92{\\scriptscriptstyle \\pm 0.03}$ & $97.39{\\scriptscriptstyle \\pm 0.22}$ & $\\underline{98.64{\\scriptscriptstyle \\pm 0.02}}$ & $98.29{\\scriptscriptstyle \\pm 0.01}$ & $\\bm{98.66{\\scriptscriptstyle \\pm 0.01}}$\\\\ \\midrule\n                       &  Avg. Rank    &  7.85          &  8.77          &  8.54          &  5.00          &  7.00          &  10.54         &  9.77          &  8.31          &  4.15          &  3.69          &  \\underline{3.15}          &  \\textbf{1.08}         \\\\ \\midrule\n\\multirow{14}{*}{AUC}  &  Wikipedia    & $96.33{\\scriptscriptstyle \\pm 0.07}$ & $94.37{\\scriptscriptstyle \\pm 0.09}$ & $96.67{\\scriptscriptstyle \\pm 0.07}$ & $98.37{\\scriptscriptstyle \\pm 0.07}$ & $98.54{\\scriptscriptstyle \\pm 0.04}$ & $90.78{\\scriptscriptstyle \\pm 0.00}$ & $95.84{\\scriptscriptstyle \\pm 0.18}$ & $96.92{\\scriptscriptstyle \\pm 0.03}$ & $97.75{\\scriptscriptstyle \\pm 0.11}$ & $98.16{\\scriptscriptstyle \\pm 0.06}$ & $\\underline{98.91{\\scriptscriptstyle \\pm 0.02}}$ & $\\bm{99.30{\\scriptscriptstyle \\pm 0.02}}$\\\\\n                       &  Reddit       & $98.31{\\scriptscriptstyle \\pm 0.05}$ & $98.17{\\scriptscriptstyle \\pm 0.05}$ & $98.47{\\scriptscriptstyle \\pm 0.02}$ & $98.60{\\scriptscriptstyle \\pm 0.06}$ & $99.01{\\scriptscriptstyle \\pm 0.01}$ & $95.37{\\scriptscriptstyle \\pm 0.00}$ & $97.42{\\scriptscriptstyle \\pm 0.02}$ & $97.17{\\scriptscriptstyle \\pm 0.02}$ & $99.09{\\scriptscriptstyle \\pm 0.10}$ & $99.09{\\scriptscriptstyle \\pm 0.03}$ & $\\underline{99.15{\\scriptscriptstyle \\pm 0.01}}$ & $\\bm{99.22{\\scriptscriptstyle \\pm 0.00}}$\\\\\n                       &  MOOC         & $83.81{\\scriptscriptstyle \\pm 2.09}$ & $85.03{\\scriptscriptstyle \\pm 0.58}$ & $87.11{\\scriptscriptstyle \\pm 0.19}$ & $\\underline{91.21{\\scriptscriptstyle \\pm 1.15}}$ & $80.38{\\scriptscriptstyle \\pm 0.26}$ & $60.86{\\scriptscriptstyle \\pm 0.00}$ & $83.12{\\scriptscriptstyle \\pm 0.18}$ & $84.01{\\scriptscriptstyle \\pm 0.17}$ & $87.42{\\scriptscriptstyle \\pm 0.58}$ & $90.55{\\scriptscriptstyle \\pm 0.43}$ & $87.91{\\scriptscriptstyle \\pm 0.58}$ & $\\bm{97.17{\\scriptscriptstyle \\pm 0.08}}$\\\\\n                       &  LastFM       & $70.49{\\scriptscriptstyle \\pm 1.66}$ & $71.16{\\scriptscriptstyle \\pm 1.89}$ & $71.59{\\scriptscriptstyle \\pm 0.18}$ & $78.47{\\scriptscriptstyle \\pm 2.94}$ & $85.92{\\scriptscriptstyle \\pm 0.10}$ & $83.77{\\scriptscriptstyle \\pm 0.00}$ & $64.06{\\scriptscriptstyle \\pm 1.16}$ & $73.53{\\scriptscriptstyle \\pm 0.12}$ & $86.92{\\scriptscriptstyle \\pm 2.72}$ & $89.28{\\scriptscriptstyle \\pm 1.63}$ & $\\underline{93.05{\\scriptscriptstyle \\pm 0.10}}$ & $\\bm{94.39{\\scriptscriptstyle \\pm 0.04}}$\\\\\n                       &  Enron        & $87.96{\\scriptscriptstyle \\pm 0.52}$ & $84.89{\\scriptscriptstyle \\pm 3.00}$ & $68.89{\\scriptscriptstyle \\pm 1.10}$ & $88.32{\\scriptscriptstyle \\pm 0.99}$ & $90.45{\\scriptscriptstyle \\pm 0.14}$ & $87.05{\\scriptscriptstyle \\pm 0.00}$ & $75.74{\\scriptscriptstyle \\pm 0.72}$ & $84.38{\\scriptscriptstyle \\pm 0.21}$ & $91.68{\\scriptscriptstyle \\pm 0.83}$ & $92.87{\\scriptscriptstyle \\pm 0.34}$ & $\\underline{93.33{\\scriptscriptstyle \\pm 0.13}}$ & $\\bm{93.98{\\scriptscriptstyle \\pm 0.26}}$\\\\\n                       &  Social Evo.  & $92.05{\\scriptscriptstyle \\pm 0.46}$ & $90.76{\\scriptscriptstyle \\pm 0.21}$ & $94.76{\\scriptscriptstyle \\pm 0.16}$ & $95.39{\\scriptscriptstyle \\pm 0.17}$ & $87.34{\\scriptscriptstyle \\pm 0.08}$ & $81.60{\\scriptscriptstyle \\pm 0.00}$ & $94.84{\\scriptscriptstyle \\pm 0.17}$ & $95.23{\\scriptscriptstyle \\pm 0.07}$ & $90.84{\\scriptscriptstyle \\pm 3.72}$ & $96.16{\\scriptscriptstyle \\pm 0.02}$ & $\\underline{96.30{\\scriptscriptstyle \\pm 0.01}}$ & $\\bm{96.43{\\scriptscriptstyle \\pm 0.02}}$\\\\\n                       &  UCI          & $90.44{\\scriptscriptstyle \\pm 0.49}$ & $68.77{\\scriptscriptstyle \\pm 2.34}$ & $78.53{\\scriptscriptstyle \\pm 0.74}$ & $92.03{\\scriptscriptstyle \\pm 1.13}$ & $93.87{\\scriptscriptstyle \\pm 0.08}$ & $77.30{\\scriptscriptstyle \\pm 0.00}$ & $87.82{\\scriptscriptstyle \\pm 1.36}$ & $91.81{\\scriptscriptstyle \\pm 0.67}$ & $92.31{\\scriptscriptstyle \\pm 0.37}$ & $\\underline{95.57{\\scriptscriptstyle \\pm 0.16}}$ & $94.49{\\scriptscriptstyle \\pm 0.26}$ & $\\bm{96.79{\\scriptscriptstyle \\pm 0.05}}$\\\\\n                       &  Flights      & $96.21{\\scriptscriptstyle \\pm 1.42}$ & $95.95{\\scriptscriptstyle \\pm 0.62}$ & $94.13{\\scriptscriptstyle \\pm 0.17}$ & $98.22{\\scriptscriptstyle \\pm 0.13}$ & $98.45{\\scriptscriptstyle \\pm 0.01}$ & $90.23{\\scriptscriptstyle \\pm 0.00}$ & $91.21{\\scriptscriptstyle \\pm 0.02}$ & $91.13{\\scriptscriptstyle \\pm 0.01}$ & $98.69{\\scriptscriptstyle \\pm 0.10}$ & $98.89{\\scriptscriptstyle \\pm 0.02}$ & $\\underline{98.93{\\scriptscriptstyle \\pm 0.01}}$ & $\\bm{99.00{\\scriptscriptstyle \\pm 0.02}}$\\\\\n                       &  Can. Parl.   & $78.21{\\scriptscriptstyle \\pm 0.23}$ & $73.35{\\scriptscriptstyle \\pm 3.67}$ & $75.69{\\scriptscriptstyle \\pm 0.78}$ & $76.99{\\scriptscriptstyle \\pm 1.80}$ & $75.70{\\scriptscriptstyle \\pm 3.27}$ & $64.14{\\scriptscriptstyle \\pm 0.00}$ & $72.46{\\scriptscriptstyle \\pm 3.23}$ & $83.17{\\scriptscriptstyle \\pm 0.53}$ & $84.04{\\scriptscriptstyle \\pm 1.13}$ & $77.96{\\scriptscriptstyle \\pm 1.46}$ & $\\bm{97.76{\\scriptscriptstyle \\pm 0.41}}$ & $\\underline{92.05{\\scriptscriptstyle \\pm 0.34}}$\\\\\n                       &  US Legis.    & $82.85{\\scriptscriptstyle \\pm 1.07}$ & $82.28{\\scriptscriptstyle \\pm 0.32}$ & $75.84{\\scriptscriptstyle \\pm 1.99}$ & $83.34{\\scriptscriptstyle \\pm 0.43}$ & $77.16{\\scriptscriptstyle \\pm 0.39}$ & $62.57{\\scriptscriptstyle \\pm 0.00}$ & $76.27{\\scriptscriptstyle \\pm 0.63}$ & $76.96{\\scriptscriptstyle \\pm 0.79}$ & $\\underline{85.36{\\scriptscriptstyle \\pm 0.52}}$ & $82.10{\\scriptscriptstyle \\pm 0.85}$ & $77.90{\\scriptscriptstyle \\pm 0.58}$ & $\\bm{86.49{\\scriptscriptstyle \\pm 0.18}}$\\\\\n                       &  UN Trade     & $69.62{\\scriptscriptstyle \\pm 0.44}$ & $67.44{\\scriptscriptstyle \\pm 0.83}$ & $64.01{\\scriptscriptstyle \\pm 0.12}$ & $69.10{\\scriptscriptstyle \\pm 1.67}$ & $68.54{\\scriptscriptstyle \\pm 0.18}$ & $66.75{\\scriptscriptstyle \\pm 0.00}$ & $64.72{\\scriptscriptstyle \\pm 0.05}$ & $65.52{\\scriptscriptstyle \\pm 0.51}$ & $\\underline{77.61{\\scriptscriptstyle \\pm 1.36}}$ & $74.87{\\scriptscriptstyle \\pm 0.53}$ & $70.20{\\scriptscriptstyle \\pm 1.44}$ & $\\bm{89.17{\\scriptscriptstyle \\pm 0.46}}$\\\\\n                       &  UN Vote      & $68.53{\\scriptscriptstyle \\pm 0.95}$ & $67.18{\\scriptscriptstyle \\pm 1.04}$ & $52.83{\\scriptscriptstyle \\pm 1.12}$ & $69.71{\\scriptscriptstyle \\pm 2.65}$ & $53.09{\\scriptscriptstyle \\pm 0.22}$ & $62.97{\\scriptscriptstyle \\pm 0.00}$ & $51.88{\\scriptscriptstyle \\pm 0.36}$ & $52.46{\\scriptscriptstyle \\pm 0.27}$ & $\\underline{75.32{\\scriptscriptstyle \\pm 0.63}}$ & $70.69{\\scriptscriptstyle \\pm 1.02}$ & $57.12{\\scriptscriptstyle \\pm 0.62}$ & $\\bm{79.88{\\scriptscriptstyle \\pm 0.30}}$\\\\\n                       &  Contact      & $96.66{\\scriptscriptstyle \\pm 0.89}$ & $96.48{\\scriptscriptstyle \\pm 0.14}$ & $96.95{\\scriptscriptstyle \\pm 0.08}$ & $97.54{\\scriptscriptstyle \\pm 0.35}$ & $89.99{\\scriptscriptstyle \\pm 0.34}$ & $94.34{\\scriptscriptstyle \\pm 0.00}$ & $94.15{\\scriptscriptstyle \\pm 0.09}$ & $93.94{\\scriptscriptstyle \\pm 0.02}$ & $97.79{\\scriptscriptstyle \\pm 0.16}$ & $\\underline{98.90{\\scriptscriptstyle \\pm 0.02}}$ & $98.53{\\scriptscriptstyle \\pm 0.01}$ & $\\bm{98.91{\\scriptscriptstyle \\pm 0.01}}$\\\\ \\midrule\n                       &  Avg. Rank    &  7.15          &  8.69          &  8.92          &  5.08          &  7.15          &  10.31         &  10.15         &  8.62          &  4.08          &  3.46          &  \\underline{3.23}          &  \\textbf{1.08}        \\\\    \n                       \\bottomrule\n\\end{tabular}} \\label{tab:transductive_random}\n\\end{table}",
            "tab:data_statistics": "\\begin{table}[!htbp]\n\\centering\n\\caption{Statistics of the datasets.}\n\\label{tab:data_statistics}\n\\resizebox{1.01\\textwidth}{!}\n{\n\\setlength{\\tabcolsep}{0.45mm}\n{\n\\begin{tabular}{c|cccccccc}\n\\toprule\nDatasets    & Domains     & \\#Nodes & \\#Links   & \\#N\\&L Feat & Bipartite & Duration  & Unique Steps & Time Granularity    \\\\ \\midrule\nWikipedia   & Social      & 9,227  & 157,474   & -- \\& 172                & True      & 1 month    &  152,757      &  Unix timestamps   \\\\\nReddit      & Social      & 10,984 & 672,447   & -- \\& 172                & True      & 1 month    &  669,065      &  Unix timestamps          \\\\\nMOOC        & Interaction & 7,144  & 411,749   & -- \\& 4                  & True      & 17 months    &  345,600      & Unix timestamps         \\\\\nLastFM      & Interaction & 1,980  & 1,293,103 & -- \\& --                 & True      & 1 month    &   1,283,614     & Unix timestamps           \\\\\nEnron       & Social      & 184    & 125,235   & -- \\& --                 & False     & 3 years    &   22,632     &    Unix timestamps        \\\\\nSocial Evo. & Proximity   & 74     & 2,099,519 & -- \\& 2                  & False     & 8 months    &  565,932      &  Unix timestamps         \\\\\nUCI         & Social      & 1,899  & 59,835    & -- \\& --                 & False     & 196 days    &  58,911      &  Unix timestamps         \\\\\nFlights     & Transport   & 13,169 & 1,927,145 & -- \\& 1                  & False     & 4 months    &  122      &   days        \\\\\nCan. Parl.  & Politics    & 734    & 74,478    & -- \\& 1                  & False     & 14 years    &   14     &   years        \\\\\nUS Legis.   & Politics    & 225    & 60,396    & -- \\& 1                  & False     & 12 congresses    &   12     &  congresses    \\\\\nUN Trade    & Economics   & 255    & 507,497   & -- \\& 1                  & False     & 32 years    &    32    &   years        \\\\\nUN Vote     & Politics    & 201    & 1,035,742 & -- \\& 1                  & False     & 72 years    &    72    &     years      \\\\\nContact     & Proximity   & 692    & 2,426,279 & -- \\& 1                  & False     & 1 month    &    8,064    &   5 minutes         \\\\ \\bottomrule\n\\end{tabular}\n}\n}\n\\end{table}",
            "tab:inductive_historical": "\\begin{table}[t]\n\\caption{Inductive results for historical negative sampling.} \\label{tab:inductive_historical}\n\\resizebox{1.05\\textwidth}{!}{\n\\setlength{\\tabcolsep}{0.7mm}\n\\setlength{\\extrarowheight}{0.3mm}\n\\label{tab:inductive_historical}\n\\begin{tabular}{ccccccccccccc}\n\\toprule\nMetrics               & Datasets    & JODIE        & DyRep        & TGAT         & TGN          & CAWN         & TCL          & GraphMixer   & NAT        & PINT       & DyGFormer    & RPNet      \\\\ \\midrule\n\\multirow{14}{*}{AP}   &  Wikipedia    & $68.69{\\scriptscriptstyle \\pm 0.39}$ & $62.18{\\scriptscriptstyle \\pm 1.27}$ & $\\underline{84.17{\\scriptscriptstyle \\pm 0.22}}$ & $81.76{\\scriptscriptstyle \\pm 0.32}$ & $67.27{\\scriptscriptstyle \\pm 1.63}$ & $82.20{\\scriptscriptstyle \\pm 2.18}$ & $\\bm{87.60{\\scriptscriptstyle \\pm 0.30}}$ & $47.37{\\scriptscriptstyle \\pm 4.39}$ & $78.22{\\scriptscriptstyle \\pm 2.90}$ & $71.42{\\scriptscriptstyle \\pm 4.43}$ & $71.28{\\scriptscriptstyle \\pm 4.33}$\\\\\n                       &  Reddit       & $62.34{\\scriptscriptstyle \\pm 0.54}$ & $61.60{\\scriptscriptstyle \\pm 0.72}$ & $63.47{\\scriptscriptstyle \\pm 0.36}$ & $64.85{\\scriptscriptstyle \\pm 0.85}$ & $63.67{\\scriptscriptstyle \\pm 0.41}$ & $60.83{\\scriptscriptstyle \\pm 0.25}$ & $64.50{\\scriptscriptstyle \\pm 0.26}$ & $61.51{\\scriptscriptstyle \\pm 0.73}$ & $\\bm{67.56{\\scriptscriptstyle \\pm 0.83}}$ & $\\underline{65.37{\\scriptscriptstyle \\pm 0.60}}$ & $62.15{\\scriptscriptstyle \\pm 1.72}$\\\\\n                       &  MOOC         & $63.22{\\scriptscriptstyle \\pm 1.55}$ & $62.93{\\scriptscriptstyle \\pm 1.24}$ & $76.73{\\scriptscriptstyle \\pm 0.29}$ & $77.07{\\scriptscriptstyle \\pm 3.41}$ & $74.68{\\scriptscriptstyle \\pm 0.68}$ & $74.27{\\scriptscriptstyle \\pm 0.53}$ & $74.00{\\scriptscriptstyle \\pm 0.97}$ & $69.30{\\scriptscriptstyle \\pm 0.95}$ & $73.74{\\scriptscriptstyle \\pm 1.66}$ & $\\underline{80.82{\\scriptscriptstyle \\pm 0.30}}$ & $\\bm{81.85{\\scriptscriptstyle \\pm 1.60}}$\\\\\n                       &  LastFM       & $70.39{\\scriptscriptstyle \\pm 4.31}$ & $71.45{\\scriptscriptstyle \\pm 1.76}$ & $76.27{\\scriptscriptstyle \\pm 0.25}$ & $66.65{\\scriptscriptstyle \\pm 6.11}$ & $71.33{\\scriptscriptstyle \\pm 0.47}$ & $65.78{\\scriptscriptstyle \\pm 0.65}$ & $76.42{\\scriptscriptstyle \\pm 0.22}$ & $70.21{\\scriptscriptstyle \\pm 0.78}$ & $\\underline{77.96{\\scriptscriptstyle \\pm 1.55}}$ & $76.35{\\scriptscriptstyle \\pm 0.52}$ & $\\bm{82.27{\\scriptscriptstyle \\pm 1.22}}$\\\\\n                       &  Enron        & $65.86{\\scriptscriptstyle \\pm 3.71}$ & $62.08{\\scriptscriptstyle \\pm 2.27}$ & $61.40{\\scriptscriptstyle \\pm 1.31}$ & $62.91{\\scriptscriptstyle \\pm 1.16}$ & $60.70{\\scriptscriptstyle \\pm 0.36}$ & $67.11{\\scriptscriptstyle \\pm 0.62}$ & $72.37{\\scriptscriptstyle \\pm 1.37}$ & $62.69{\\scriptscriptstyle \\pm 1.02}$ & $\\bm{80.47{\\scriptscriptstyle \\pm 1.52}}$ & $67.07{\\scriptscriptstyle \\pm 0.62}$ & $\\underline{74.60{\\scriptscriptstyle \\pm 1.35}}$\\\\\n                       &  Social Evo.  & $88.51{\\scriptscriptstyle \\pm 0.87}$ & $88.72{\\scriptscriptstyle \\pm 1.10}$ & $93.97{\\scriptscriptstyle \\pm 0.54}$ & $90.66{\\scriptscriptstyle \\pm 1.62}$ & $79.83{\\scriptscriptstyle \\pm 0.38}$ & $94.10{\\scriptscriptstyle \\pm 0.31}$ & $94.01{\\scriptscriptstyle \\pm 0.47}$ & $84.89{\\scriptscriptstyle \\pm 4.62}$ & $95.75{\\scriptscriptstyle \\pm 1.06}$ & $\\bm{96.82{\\scriptscriptstyle \\pm 0.16}}$ & $\\underline{96.38{\\scriptscriptstyle \\pm 0.18}}$\\\\\n                       &  UCI          & $63.11{\\scriptscriptstyle \\pm 2.27}$ & $52.47{\\scriptscriptstyle \\pm 2.06}$ & $70.52{\\scriptscriptstyle \\pm 0.93}$ & $70.78{\\scriptscriptstyle \\pm 0.78}$ & $64.54{\\scriptscriptstyle \\pm 0.47}$ & $76.71{\\scriptscriptstyle \\pm 1.00}$ & $\\underline{81.66{\\scriptscriptstyle \\pm 0.49}}$ & $51.56{\\scriptscriptstyle \\pm 0.75}$ & $\\bm{85.49{\\scriptscriptstyle \\pm 0.22}}$ & $72.13{\\scriptscriptstyle \\pm 1.87}$ & $78.48{\\scriptscriptstyle \\pm 1.18}$\\\\\n                       &  Flights      & $61.01{\\scriptscriptstyle \\pm 1.65}$ & $62.83{\\scriptscriptstyle \\pm 1.31}$ & $\\underline{64.72{\\scriptscriptstyle \\pm 0.36}}$ & $59.31{\\scriptscriptstyle \\pm 1.43}$ & $56.82{\\scriptscriptstyle \\pm 0.57}$ & $64.50{\\scriptscriptstyle \\pm 0.25}$ & $\\bm{65.28{\\scriptscriptstyle \\pm 0.24}}$ & $51.63{\\scriptscriptstyle \\pm 1.10}$ & $53.46{\\scriptscriptstyle \\pm 0.61}$ & $57.11{\\scriptscriptstyle \\pm 0.21}$ & $54.67{\\scriptscriptstyle \\pm 0.76}$\\\\\n                       &  Can. Parl.   & $52.60{\\scriptscriptstyle \\pm 0.88}$ & $52.28{\\scriptscriptstyle \\pm 0.31}$ & $56.72{\\scriptscriptstyle \\pm 0.47}$ & $54.42{\\scriptscriptstyle \\pm 0.77}$ & $57.14{\\scriptscriptstyle \\pm 0.07}$ & $55.71{\\scriptscriptstyle \\pm 0.74}$ & $55.84{\\scriptscriptstyle \\pm 0.73}$ & $61.56{\\scriptscriptstyle \\pm 2.68}$ & $50.61{\\scriptscriptstyle \\pm 1.76}$ & $\\bm{87.40{\\scriptscriptstyle \\pm 0.85}}$ & $\\underline{68.97{\\scriptscriptstyle \\pm 1.60}}$\\\\\n                       &  US Legis.    & $52.94{\\scriptscriptstyle \\pm 2.11}$ & $62.10{\\scriptscriptstyle \\pm 1.41}$ & $51.83{\\scriptscriptstyle \\pm 3.95}$ & $61.18{\\scriptscriptstyle \\pm 1.10}$ & $55.56{\\scriptscriptstyle \\pm 1.71}$ & $53.87{\\scriptscriptstyle \\pm 1.41}$ & $52.03{\\scriptscriptstyle \\pm 1.02}$ & $\\underline{64.61{\\scriptscriptstyle \\pm 3.02}}$ & $59.37{\\scriptscriptstyle \\pm 1.84}$ & $56.31{\\scriptscriptstyle \\pm 3.46}$ & $\\bm{66.95{\\scriptscriptstyle \\pm 1.81}}$\\\\\n                       &  UN Trade     & $55.46{\\scriptscriptstyle \\pm 1.19}$ & $55.49{\\scriptscriptstyle \\pm 0.84}$ & $55.28{\\scriptscriptstyle \\pm 0.71}$ & $52.80{\\scriptscriptstyle \\pm 3.19}$ & $55.00{\\scriptscriptstyle \\pm 0.38}$ & $55.76{\\scriptscriptstyle \\pm 1.03}$ & $54.94{\\scriptscriptstyle \\pm 0.97}$ & $\\underline{70.04{\\scriptscriptstyle \\pm 2.07}}$ & $57.35{\\scriptscriptstyle \\pm 1.65}$ & $53.20{\\scriptscriptstyle \\pm 1.07}$ & $\\bm{78.83{\\scriptscriptstyle \\pm 0.53}}$\\\\\n                       &  UN Vote      & $61.04{\\scriptscriptstyle \\pm 1.30}$ & $60.22{\\scriptscriptstyle \\pm 1.78}$ & $53.05{\\scriptscriptstyle \\pm 3.10}$ & $63.74{\\scriptscriptstyle \\pm 3.00}$ & $47.98{\\scriptscriptstyle \\pm 0.84}$ & $54.19{\\scriptscriptstyle \\pm 2.17}$ & $48.09{\\scriptscriptstyle \\pm 0.43}$ & $64.91{\\scriptscriptstyle \\pm 1.58}$ & $\\bm{65.75{\\scriptscriptstyle \\pm 2.86}}$ & $52.63{\\scriptscriptstyle \\pm 1.26}$ & $\\underline{65.24{\\scriptscriptstyle \\pm 1.62}}$\\\\\n                       &  Contact      & $90.42{\\scriptscriptstyle \\pm 2.34}$ & $89.22{\\scriptscriptstyle \\pm 0.66}$ & $\\bm{94.15{\\scriptscriptstyle \\pm 0.45}}$ & $88.13{\\scriptscriptstyle \\pm 1.50}$ & $74.20{\\scriptscriptstyle \\pm 0.80}$ & $90.44{\\scriptscriptstyle \\pm 0.17}$ & $89.91{\\scriptscriptstyle \\pm 0.36}$ & $84.13{\\scriptscriptstyle \\pm 1.78}$ & $90.68{\\scriptscriptstyle \\pm 0.46}$ & $\\underline{93.56{\\scriptscriptstyle \\pm 0.52}}$ & $93.56{\\scriptscriptstyle \\pm 0.57}$\\\\ \\midrule\n                       &  Avg. Rank    &  7.46          &  7.62          &  5.69          &  6.31          &  8.08          &  5.92          &  5.23          &  7.62        &  \\underline{4.23}        &  4.62          &  \\textbf{3.15}       \\\\ \\midrule\n\\multirow{14}{*}{AUC}  &  Wikipedia    & $61.86{\\scriptscriptstyle \\pm 0.53}$ & $57.54{\\scriptscriptstyle \\pm 1.09}$ & $78.38{\\scriptscriptstyle \\pm 0.20}$ & $75.75{\\scriptscriptstyle \\pm 0.29}$ & $62.04{\\scriptscriptstyle \\pm 0.65}$ & $\\underline{79.79{\\scriptscriptstyle \\pm 0.96}}$ & $\\bm{82.87{\\scriptscriptstyle \\pm 0.21}}$ & $40.95{\\scriptscriptstyle \\pm 4.99}$ & $73.29{\\scriptscriptstyle \\pm 2.33}$ & $68.33{\\scriptscriptstyle \\pm 2.82}$ & $67.95{\\scriptscriptstyle \\pm 2.77}$\\\\\n                       &  Reddit       & $61.69{\\scriptscriptstyle \\pm 0.39}$ & $60.45{\\scriptscriptstyle \\pm 0.37}$ & $64.43{\\scriptscriptstyle \\pm 0.27}$ & $64.55{\\scriptscriptstyle \\pm 0.50}$ & $\\bm{64.94{\\scriptscriptstyle \\pm 0.21}}$ & $61.43{\\scriptscriptstyle \\pm 0.26}$ & $64.27{\\scriptscriptstyle \\pm 0.13}$ & $58.32{\\scriptscriptstyle \\pm 0.63}$ & $64.02{\\scriptscriptstyle \\pm 0.46}$ & $\\underline{64.81{\\scriptscriptstyle \\pm 0.25}}$ & $62.37{\\scriptscriptstyle \\pm 0.83}$\\\\\n                       &  MOOC         & $64.48{\\scriptscriptstyle \\pm 1.64}$ & $64.23{\\scriptscriptstyle \\pm 1.29}$ & $74.08{\\scriptscriptstyle \\pm 0.27}$ & $77.69{\\scriptscriptstyle \\pm 3.55}$ & $71.68{\\scriptscriptstyle \\pm 0.94}$ & $69.82{\\scriptscriptstyle \\pm 0.32}$ & $72.53{\\scriptscriptstyle \\pm 0.84}$ & $67.99{\\scriptscriptstyle \\pm 1.68}$ & $75.92{\\scriptscriptstyle \\pm 2.48}$ & $\\underline{80.77{\\scriptscriptstyle \\pm 0.63}}$ & $\\bm{84.46{\\scriptscriptstyle \\pm 0.88}}$\\\\\n                       &  LastFM       & $68.44{\\scriptscriptstyle \\pm 3.26}$ & $68.79{\\scriptscriptstyle \\pm 1.08}$ & $69.89{\\scriptscriptstyle \\pm 0.28}$ & $66.99{\\scriptscriptstyle \\pm 5.62}$ & $67.69{\\scriptscriptstyle \\pm 0.24}$ & $55.88{\\scriptscriptstyle \\pm 1.85}$ & $70.07{\\scriptscriptstyle \\pm 0.20}$ & $64.18{\\scriptscriptstyle \\pm 0.65}$ & $\\underline{75.02{\\scriptscriptstyle \\pm 0.66}}$ & $70.73{\\scriptscriptstyle \\pm 0.37}$ & $\\bm{77.10{\\scriptscriptstyle \\pm 0.78}}$\\\\\n                       &  Enron        & $65.32{\\scriptscriptstyle \\pm 3.57}$ & $61.50{\\scriptscriptstyle \\pm 2.50}$ & $57.84{\\scriptscriptstyle \\pm 2.18}$ & $62.68{\\scriptscriptstyle \\pm 1.09}$ & $62.25{\\scriptscriptstyle \\pm 0.40}$ & $64.06{\\scriptscriptstyle \\pm 1.02}$ & $68.20{\\scriptscriptstyle \\pm 1.62}$ & $61.69{\\scriptscriptstyle \\pm 0.68}$ & $\\bm{78.90{\\scriptscriptstyle \\pm 1.29}}$ & $65.78{\\scriptscriptstyle \\pm 0.42}$ & $\\underline{74.50{\\scriptscriptstyle \\pm 1.02}}$\\\\\n                       &  Social Evo.  & $88.53{\\scriptscriptstyle \\pm 0.55}$ & $87.93{\\scriptscriptstyle \\pm 1.05}$ & $91.87{\\scriptscriptstyle \\pm 0.72}$ & $92.10{\\scriptscriptstyle \\pm 1.22}$ & $83.54{\\scriptscriptstyle \\pm 0.24}$ & $93.28{\\scriptscriptstyle \\pm 0.60}$ & $93.62{\\scriptscriptstyle \\pm 0.35}$ & $84.89{\\scriptscriptstyle \\pm 4.72}$ & $96.29{\\scriptscriptstyle \\pm 0.56}$ & $\\bm{96.91{\\scriptscriptstyle \\pm 0.09}}$ & $\\underline{96.76{\\scriptscriptstyle \\pm 0.14}}$\\\\\n                       &  UCI          & $60.24{\\scriptscriptstyle \\pm 1.94}$ & $51.25{\\scriptscriptstyle \\pm 2.37}$ & $62.32{\\scriptscriptstyle \\pm 1.18}$ & $62.69{\\scriptscriptstyle \\pm 0.90}$ & $56.39{\\scriptscriptstyle \\pm 0.10}$ & $70.46{\\scriptscriptstyle \\pm 1.94}$ & $\\underline{75.98{\\scriptscriptstyle \\pm 0.84}}$ & $42.59{\\scriptscriptstyle \\pm 0.96}$ & $\\bm{81.34{\\scriptscriptstyle \\pm 0.29}}$ & $65.55{\\scriptscriptstyle \\pm 1.01}$ & $71.35{\\scriptscriptstyle \\pm 0.84}$\\\\\n                       &  Flights      & $60.72{\\scriptscriptstyle \\pm 1.29}$ & $61.99{\\scriptscriptstyle \\pm 1.39}$ & $\\underline{63.38{\\scriptscriptstyle \\pm 0.26}}$ & $59.66{\\scriptscriptstyle \\pm 1.04}$ & $56.58{\\scriptscriptstyle \\pm 0.44}$ & $\\bm{63.48{\\scriptscriptstyle \\pm 0.23}}$ & $63.30{\\scriptscriptstyle \\pm 0.19}$ & $48.39{\\scriptscriptstyle \\pm 1.37}$ & $49.76{\\scriptscriptstyle \\pm 0.89}$ & $56.05{\\scriptscriptstyle \\pm 0.21}$ & $53.08{\\scriptscriptstyle \\pm 0.87}$\\\\\n                       &  Can. Parl.   & $51.62{\\scriptscriptstyle \\pm 1.00}$ & $52.38{\\scriptscriptstyle \\pm 0.46}$ & $58.30{\\scriptscriptstyle \\pm 0.61}$ & $55.64{\\scriptscriptstyle \\pm 0.54}$ & $60.11{\\scriptscriptstyle \\pm 0.48}$ & $57.30{\\scriptscriptstyle \\pm 1.03}$ & $56.68{\\scriptscriptstyle \\pm 1.20}$ & $61.72{\\scriptscriptstyle \\pm 2.76}$ & $48.93{\\scriptscriptstyle \\pm 2.35}$ & $\\bm{88.68{\\scriptscriptstyle \\pm 0.74}}$ & $\\underline{69.11{\\scriptscriptstyle \\pm 1.18}}$\\\\\n                       &  US Legis.    & $58.12{\\scriptscriptstyle \\pm 2.94}$ & $\\underline{67.94{\\scriptscriptstyle \\pm 0.98}}$ & $49.99{\\scriptscriptstyle \\pm 4.88}$ & $64.87{\\scriptscriptstyle \\pm 1.65}$ & $54.41{\\scriptscriptstyle \\pm 1.31}$ & $52.12{\\scriptscriptstyle \\pm 2.13}$ & $49.28{\\scriptscriptstyle \\pm 0.86}$ & $66.95{\\scriptscriptstyle \\pm 4.17}$ & $62.82{\\scriptscriptstyle \\pm 2.18}$ & $56.57{\\scriptscriptstyle \\pm 3.22}$ & $\\bm{68.37{\\scriptscriptstyle \\pm 1.62}}$\\\\\n                       &  UN Trade     & $58.73{\\scriptscriptstyle \\pm 1.19}$ & $57.90{\\scriptscriptstyle \\pm 1.33}$ & $59.74{\\scriptscriptstyle \\pm 0.59}$ & $55.61{\\scriptscriptstyle \\pm 3.54}$ & $60.95{\\scriptscriptstyle \\pm 0.80}$ & $61.12{\\scriptscriptstyle \\pm 0.97}$ & $59.88{\\scriptscriptstyle \\pm 1.17}$ & $\\underline{70.43{\\scriptscriptstyle \\pm 2.07}}$ & $62.61{\\scriptscriptstyle \\pm 1.70}$ & $58.46{\\scriptscriptstyle \\pm 1.65}$ & $\\bm{80.70{\\scriptscriptstyle \\pm 1.03}}$\\\\\n                       &  UN Vote      & $65.16{\\scriptscriptstyle \\pm 1.28}$ & $63.98{\\scriptscriptstyle \\pm 2.12}$ & $51.73{\\scriptscriptstyle \\pm 4.12}$ & $\\underline{68.59{\\scriptscriptstyle \\pm 3.11}}$ & $48.01{\\scriptscriptstyle \\pm 1.77}$ & $54.66{\\scriptscriptstyle \\pm 2.11}$ & $45.49{\\scriptscriptstyle \\pm 0.42}$ & $67.69{\\scriptscriptstyle \\pm 1.92}$ & $\\bm{69.47{\\scriptscriptstyle \\pm 3.01}}$ & $53.85{\\scriptscriptstyle \\pm 2.02}$ & $65.75{\\scriptscriptstyle \\pm 1.85}$\\\\\n                       &  Contact      & $90.80{\\scriptscriptstyle \\pm 1.18}$ & $88.88{\\scriptscriptstyle \\pm 0.68}$ & $\\underline{93.76{\\scriptscriptstyle \\pm 0.41}}$ & $88.84{\\scriptscriptstyle \\pm 1.39}$ & $74.79{\\scriptscriptstyle \\pm 0.37}$ & $90.37{\\scriptscriptstyle \\pm 0.16}$ & $90.04{\\scriptscriptstyle \\pm 0.29}$ & $84.74{\\scriptscriptstyle \\pm 1.44}$ & $91.99{\\scriptscriptstyle \\pm 0.28}$ & $\\bm{94.14{\\scriptscriptstyle \\pm 0.26}}$ & $93.47{\\scriptscriptstyle \\pm 0.43}$\\\\ \\midrule\n                       &  Avg. Rank    &  7.23          &  8.08          &  5.92          &  6.00          &  7.46          &  6.00          &  5.38          &  7.92        &  \\underline{4.31}        &  4.38          &  \\textbf{3.31}    \\\\\n                       \\bottomrule\n\\end{tabular}}\n\\end{table}"
        },
        "figures": {
            "fig:node-wise_limit": "\\begin{wrapfigure}{r}{0.5\\textwidth}\n    \\centering\n    \\vspace{-0.4cm}\n    \\includegraphics[width=0.5\\textwidth]{figures/introduction.pdf}\n    \\vspace{-0.5cm}\n    \\caption{Without relative encodings, the learned node representations fail to capture the correlation between nodes. (For each link $(u,v,t)$, the relative encoding here for a node $w$ is [$g(u,w),g(v,w)$], where $g(u,w)=1$ if there is an interaction between u and w before t, otherwise $g(u,w)=0$.)}\n    \\vspace{-0.4cm}\n    \\label{fig:node-wise_limit}\n\\end{wrapfigure}",
            "fig:new_temporal_walk": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/new_temporal_walk.pdf}\n    \\caption{Illustration of the newly generated 3-step temporal walks beginning from $D$ after adding a new interaction $(\\{C,D\\},t_4)$  There is a one-to-one map between \n     $\\Delta M_{D,*}^{3}$ and $M_{C,*}^{2}(t_4)$.}\n    \\label{fig:new_temporal_walk}\n\\end{figure}",
            "fig:efficiency_appdx": "\\begin{figure}[t]\n     \\includegraphics[trim={0.5cm 0.0cm 2.5cm 1cm},clip,width=0.31\\textwidth]{figures/running_time_reddit.pdf}\n     \\hfill\n     \\includegraphics[trim={0.5cm 0.0cm 2.5cm 1cm},clip,width=0.31\\textwidth]{figures/running_time_uci.pdf}\n     \\hfill\n     \\includegraphics[trim={0.5cm 0.0cm 2.5cm 1cm},clip,width=0.31\\textwidth]{figures/running_time_wikipedia.pdf}\n\\caption{Efficiency analysis on more datasets.} \\label{fig:efficiency_appdx}\n\\end{figure}",
            "fig:dimension_change_appdx": "\\begin{figure}[t]\n     \\includegraphics[trim={0.1cm 0.5cm 2.5cm 1cm}, clip,width=0.31\\textwidth]{figures/dimension_change_wikipedia.pdf}\n     \\includegraphics[trim={0.5cm 0.5cm 2.5cm 1cm}, clip,width=0.31\\textwidth]{figures/dimension_change_enron.pdf}\n    \\includegraphics[trim={0.5cm 0.5cm 2.5cm 1cm}, clip,width=0.31\\textwidth]{figures/dimension_change_uci.pdf}\n\\caption{Influence of Node Representation Dimension} \\label{fig:dimension_change_appdx}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\bm{h}_u(t) = f_{\\text{enc}}(\\mathcal{G}_u^k(t),\\bm{X}^N_{u,k} \\oplus \\bm{X}^R_{u,k},\\bm{X}^E_{u,k}), ~~~~~ \\bm{h}_v(t) = f_{\\text{enc}}(\\mathcal{G}_v^k(t),\\bm{X}^N_{v,k} \\oplus \\bm{X}^R_{v,k},\\bm{X}^E_{v,k}), \\label{eq:link-wise}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\bm{r}^{w|u} = g([A^{(0)}_{u,w}(t), A^{(1)}_{u,w}(t),\\cdots,A^{(k)}_{u,w}(t)]),~~~~ A_{u,w}^{(i)}(t) = \\sum_{W \\in \\mathcal{M}^i_{u,w}} s(W) ~~\\text{for} ~~0 \\leq i \\leq k.\n \\label{eq:unified_func}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n  \\bm{H}^{(l)}_u(t^+) = \\bm{H}^{(l)}_u(t) + e^{\\lambda t}*\\bm{H}^{(l-1)}_v(t),~~~~~ \\bm{H}^{(l)}_v(t^+) = \\bm{H}^{(l)}_v(t) + e^{\\lambda t}*\\bm{H}^{(l-1)}_u(t), \n  \\label{eq:update}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\begin{aligned}\n\\tilde{\\bm{Z}}_u^{(l)} &= \\bm{Z}_u^{(l-1)} + \\bm{W}_{1}^{(l)} \\text{GeLU}(\\bm{W}_{2}^{(l)}\\text{LayerNorm}(\\bm{Z}_u^{(l-1)})) \\\\\n\\bm{Z}_u^{(l)} &= \\tilde{\\bm{Z}}_u^{(l)} + \\bm{W}_{3}^{(l)} \\text{GeLU}(\\bm{W}_{4}^{(l)}\\text{LayerNorm}(\\bm{\\tilde{Z}}_u^{(l)})).\n\\end{aligned}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\bar{s}_u^{(i)} = s_u^{(i)} \\cup s_v^{(i-1)}, ~~~~~~ \\bar{s}_v^{(i)} = s_v^{(i)} \\cup s_u^{(i-1)},\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    r^{w|u}_{i} = \\sum_{j=1}^{m} \\bm{1}_{W_j[i][0] = w},\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mu_{w}^{i-1} = \\sum_{W \\in \\mathcal{M}_{u,w}^{i-1}} f(W),\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    f(W) = \\prod_{i=0}^{k-1} \\frac{\\text{exp}(-\\alpha(t_{i}-t_{i+1}))}{\\sum_{(\\{w',w\\},t')\\in \\mathcal{E}_{w_i,t_i}} \\textrm{exp}(-\\alpha(t_i-t'))}\n\\end{equation}",
            "eq:9": "\\begin{equation}\ne^{\\lambda lt}*s(W) = e^{\\lambda lt}*\\prod_{j=1}^l e^{-\\lambda(t-t_j)}  = \\prod_{j=1}^l e^{-\\lambda(t-t_j)}*e^{\\lambda t} = \\prod_{j=1}^l e^{\\lambda t_j}. \n\\end{equation}",
            "eq:10": "\\begin{equation}\n    A_{i,j}^{(l)}(t^+) = A_{i,j}^{(l)}(t) + \\sum_{W \\in \\Delta M_{i,j}^l} \\bar{s}(W), \n    \\label{eq:set_form_updating}\n\\end{equation}",
            "eq:11": "\\begin{equation}\nA_{u,i}^{(l)}(t^+) = A_{u,i}^{(l)}(t) + \\sum_{W \\in  M_{v,i}^{l-1}(t)} e^{-\\lambda t}*\\bar{s}(W) = A_{u,i}^{(l)}(t) + e^{-\\lambda t}*A_{v,i}^{(l-1)}(t),\n\\end{equation}",
            "eq:12": "\\begin{equation}\n  \\bm{\\bar{A}}^{(l)}_u(t^+) = \\bm{\\bar{A}}^{(l)}_u(t) + e^{\\lambda t}*\\bm{\\bar{A}}^{(l-1)}_v(t),~~~~~ \\bm{\\bar{A}}^{(l)}_v(t^+) = \\bm{\\bar{A}}^{(l)}_v(t) + e^{\\lambda t}*\\bm{\\bar{A}}^{(l-1)}_u(t), \n  \\label{eq:update}\n\\end{equation}",
            "eq:13": "\\begin{equation}\n    \\bm{H}_u^{(l)}(t^+) = \\bm{H}_u^{(l)}(t) + e^{\\lambda t}*\\bm{H}_v^{(l-1)}(t) = (\\bm{\\bar{A}}_u^{(l)}(t) + e^{\\lambda t}*\\bm{\\bar{A}}_v^{(l-1)}(t))\\bm{P} = \\bm{\\bar{A}}_u^{(l)}(t^+)\\bm{P}\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\begin{split}\n\\mathbb{P}\\left((1-\\epsilon)\\|\\bm{x}\\|_2^2 \\leq \\|\\bm{Px}\\|_2^2 \\leq (1+\\epsilon)\\|\\bm{x}\\|_2^2\\right) \\geq 1 - 2\\delta  \\\\ \\iff \\mathbb{P}\\left( \\left| \\frac{\\|\\bm{Px}\\|_2^2}{\\|\\bm{x}\\|_2^2} -1 \\right| \\leq \\epsilon \\right ) \\geq 1 - 2\\delta \\\\ \\iff \\mathbb{P}\\left( \\left| \\frac{\\|\\bm{Px}\\|_2^2}{\\|\\bm{x}\\|_2^2} -1 \\right| > \\epsilon \\right ) \\leq 2\\delta\n\\end{split}\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\begin{aligned}\n\\mathbb{P}\\left( \\left| \\frac{\\|\\bm{Px}\\|_2^2}{\\|\\bm{x}\\|_2^2} -1 \\right| > \\epsilon \\right ) &\\leq 2\\exp(\\frac{-\\epsilon^2d}{8}) \\\\ \\leq 2 &\\exp(\\frac{-\\epsilon^2}{8}*\\frac{8}{\\epsilon^2}*\\log(\\frac{1}{\\delta})) \\leq 2\\delta \n\\end{aligned}\n\\end{equation}",
            "eq:16": "\\begin{equation}\n    \\mathbb{P}(C_{i,j}) = 1 - \\mathbb{P}(\\overline{E_{i,j}^+} \\cup \\overline{E_{i,j}^-}) \\geq 1 -(\\mathbb{P}(\\overline{E_{i,j}^+}) + \\mathbb{P}(\\overline{E_{i,j}^-})) \\geq 1-\\frac{1}{n^3},\n\\end{equation}",
            "eq:17": "\\begin{equation}\n     \\mathbb{P}(\\cap _{i,j}C_{i,j}) = 1 - \\mathbb{P}(\\cup_{i,j}\\overline{C_{i,j}}) \\geq 1- \\sum_{i,j}\\mathbb{P}(\\overline{C_{i,j}}) \\geq 1 - n^2*\\frac{1}{n^3} \\geq 1 - \\frac{1}{n},\n\\end{equation}",
            "eq:18": "\\begin{align} \n    (1-\\epsilon)\\|\\bm{x_i+x_j}\\|_2^2  \\leq \\|\\bm{P(x_i+x_j)}\\|_2^2 \\leq (1+\\epsilon)\\|\\bm{x_i+x_j}\\|_2^2  \\label{eq:pos} \\\\  \n    (1-\\epsilon)\\|\\bm{x_i-x_j}\\|_2^2  \\leq \\|\\bm{P(x_i-x_j)}\\|_2^2 \\leq (1+\\epsilon)\\|\\bm{x_i-x_j}\\|_2^2  \\label{eq:neg}\n\\end{align}",
            "eq:19": "\\begin{equation}\n    \\left| \\langle \\bm{Px_i}, \\bm{Px_j}\\rangle - \\langle \\bm{x_i},\\bm{x_j} \\rangle \\right | \\leq \\frac{\\epsilon}{2}(\\|\\bm{x_i}\\|_2^2 + \\| \\bm{x_j}\\|_2^2)\n\\end{equation}",
            "eq:20": "\\begin{equation}\n       \\mathbb{P}(\\cap _{i,j}C_{i,j}) \\geq 1 - \\frac{1}{n} \\implies \\\\ \n    \\mathbb{P}(\\left| \\langle \\bm{Px_i}, \\bm{Px_j}\\rangle - \\langle \\bm{x_i},\\bm{x_j} \\rangle \\right | \\leq \\frac{\\epsilon}{2}(\\|\\bm{x_i}\\|_2^2 + \\| \\bm{x_j}\\|_2^2)) \\geq 1 - \\frac{1}{n} \n\\end{equation}",
            "eq:21": "\\begin{equation}\n    \\bm{A}_{i}(t^+) = k_1 \\bm{A}_{l_1}(t) + \\cdots +  k_m \\bm{A}_{l_m}(t), \\label{eq:linear_combination}\n\\end{equation}",
            "eq:22": "\\begin{equation}\n    \\bm{A}_{i}(t+\\Delta t) = k_1 \\bm{A}_{l_1}(t) + \\cdots +  k_m \\bm{A}_{l_m}(t)\n\\end{equation}",
            "eq:23": "\\begin{align}\n    \\bm{H}(t^+) &= k_1 \\bm{H}_{l_1}(t) + \\cdots + k_m \\bm{H}_{l_m}(t) \\\\ \\nonumber\n    &= (k_1\\bm{A}_{l_1}(t)+\\cdots + k_m \\bm{A}_{l_m}(t))\\bm{P} = \\bm{A}_i(t^+) \\bm{P} \\nonumber\n\\end{align}",
            "eq:24": "\\begin{equation}\n\\begin{split}\n    \\bm{A}_u^{(l)}(t^+) = \\frac{d_u(t)}{d_u(t)+1}* \\bm{A}_u^{(l)}(t) + \\frac{1}{d_u(t)+1}*\\bm{A}_v^{(l-1)}(t), \\\\ \n    \\bm{A}_v^{(l)}(t^+) = \\frac{d_v(t)}{d_v(t)+1}* \\bm{A}_v^{(l)}(t) + \\frac{1}{d_v(t)+1}*\\bm{A}_u^{(l-1)}(t),  \n\\end{split} \n\\end{equation}"
        },
        "git_link": "https://github.com/lxd99/TPNet"
    }
}