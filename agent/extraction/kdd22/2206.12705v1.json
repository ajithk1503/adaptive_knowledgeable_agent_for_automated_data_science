{
    "meta_info": {
        "title": "p-Meta: Towards On-device Deep Model Adaptation",
        "abstract": "Data collected by IoT devices are often private and have a large diversity\nacross users. Therefore, learning requires pre-training a model with available\nrepresentative data samples, deploying the pre-trained model on IoT devices,\nand adapting the deployed model on the device with local data. Such an\non-device adaption for deep learning empowered applications demands data and\nmemory efficiency. However, existing gradient-based meta learning schemes fail\nto support memory-efficient adaptation. To this end, we propose p-Meta, a new\nmeta learning method that enforces structure-wise partial parameter updates\nwhile ensuring fast generalization to unseen tasks. Evaluations on few-shot\nimage classification and reinforcement learning tasks show that p-Meta not only\nimproves the accuracy but also substantially reduces the peak dynamic memory by\na factor of 2.5 on average compared to state-of-the-art few-shot adaptation\nmethods.",
        "author": "Zhongnan Qu, Zimu Zhou, Yongxin Tong, Lothar Thiele",
        "link": "http://arxiv.org/abs/2206.12705v1",
        "category": [
            "cs.LG",
            "cs.CV"
        ],
        "additionl_info": "Published in SIGKDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:introduction}\n\nAdaption to \\textit{unseen} environments, users, and tasks is crucial for deep learning empowered IoT applications to deliver consistent performance and customized services. \nData collected by IoT devices are often private and have a large diversity across users. \nFor instance, activity recognition with smartphone sensors should adapt to countless walking patterns and sensor orientation \\cite{bib:SenSys19:Gong}.\nHuman motion prediction with home robots needs fast learning of unseen poses for seamless human-robot interaction \\cite{bib:ECCV18:Gui}.\nIn these applications, the \\textit{new} data collected for model adaptation tend to relate to personal habits and lifestyle.\nHence, \\textit{on-device} model adaptation is preferred over uploading the data to cloud servers for retraining.\n\nYet on-device adaption of a deep neural network (DNN) demands \\textit{data efficiency} and \\textit{memory efficiency}.\nThe excellent accuracy of contemporary DNNs is attributed to training with high-performance computers on large-scale datasets \\cite{bib:book16:Goodfellow}.\nFor example, it takes $29$ hours to complete a $90$-epoch ResNet50 \\cite{bib:CVPR16:He} training on ImageNet ($1.2$ million training images) \\cite{bib:ILSVRC15} with $8$ NVIDIA Tesla P100 GPUs \\cite{bib:arXiv17:Goyal}.\nFor on-device adaptation, however, neither abundant \\textit{data} nor \\textit{resources} are available.\nA personal voice assistant, for example, may learn to adapt to users' accent and dialect within a few sentences, while a home robot should learn to recognize new object categories with few labelled images to navigate in new environments.\nFurthermore, such adaptation is expected to be conducted on low-resource platforms such as smart portable devices, home hubs, and other IoT devices, with only several $KB$ to $MB$ memory.\n\n\n\nFor \\textit{data-efficient} DNN adaptation, we resort to \\textit{meta learning}, a paradigm that learns to fast generalize to unseen tasks \\cite{bib:arXiv20:Hospedales}.\nOf our particular interest is \\textit{gradient-based} meta learning \\cite{bib:ICLR19:Antreas, bib:ICML17:Finn, bib:ICLR20:Raghu, bib:ICLR21:Oh} for its wide applicability in classification, regression and reinforcement learning, as well as the availability of gradient-based training frameworks for low-resource devices, \\eg TensorFlow Lite \\cite{bib:tfliteOndeviceTraining}.\n\\figref{fig:meta} explains major terminologies in the context of on-device adaptation.\nGiven a backbone, its weights are \\textit{meta-trained} on \\textit{many} tasks, to output a \\textit{model} that is expected to fast adapt to new \\textit{unseen} tasks.\nThe process of adaptation is also known as \\textit{few-shot learning}, where the meta-trained model is further retrained by standard stochastic gradient decent (SGD) on \\textit{few new} samples only.\n\nHowever, existing gradient-based meta learning schemes \\cite{bib:ICLR19:Antreas, bib:ICML17:Finn, bib:ICLR20:Raghu, bib:ICLR21:Oh} fail to support \\textit{memory-efficient} adaptation.\nAlthough \\textit{meta training} is conducted in the cloud, \\textit{few-shot learning} (adaptation) of the meta-trained model is performed on IoT devices.\nConsider to retrain a common backbone ResNet12 in a 5-way (5 new classes) 5-shot (5 samples per class) scenario.  \nOne round of SGD consumes 370.44MB peak dynamic memory, since the inputs of all layers must be stored to compute the gradients of these layers' weights in the backward path. \nIn comparison, inference only needs 3.61MB.\nThe necessary dynamic memory is a key bottleneck for on-device adaptation due to cost and power constraints, even though the meta-trained model only needs to be retrained with a few data.\n\nPrior efficient DNN training solutions mainly focus on parallel and distributed training on data centers \\cite{bib:arXiv16:Chen, bib:ICLR21:Chen, bib:NIPS17:Greff, bib:NIPS16:Gruslys, bib:NIPS20:Raihan}.\nOn-device training has been explored for \\textit{vanilla supervised training} \\cite{bib:ICASSP20:Gooneratne, bib:MLSys21:Mathur, bib:SenSys19:Lee}, where training and testing are performed on the \\textit{same} task. \nA pioneer study \\cite{bib:NIPS20:Cai} investigated on-device adaptation to new tasks via memory-efficient \\textit{transfer learning}. \nYet transfer learning is prone to overfitting when only a few samples are available \\cite{bib:ICML17:Finn}. \n\nIn this paper, we propose \\sysname, a new meta learning method for data- and memory-efficient DNN adaptation.\nThe key idea is to enforce \\textit{structured partial parameter updates} while ensuring \\textit{fast generalization to unseen tasks}. \nThe idea is inspired by recent advances in understanding gradient-based meta learning \\cite{bib:ICLR21:Oh, bib:ICLR20:Raghu}.\nEmpirical evidence shows that only the \\textit{head} (the last output layer) of a DNN needs to be updated to achieve reasonable few-shot classification accuracy \\cite{bib:ICLR20:Raghu} whereas the \\textit{body} (the layers closed to the input) needs to be updated for cross-domain few-shot classification \\cite{bib:ICLR21:Oh}.\nThese studies imply that certain weights are more important than others when generalizing to unseen tasks. Hence, we propose to automatically identify these \\textit{adaptation-critical weights} to minimize the memory demand in few-shot learning.\n\nParticularly, the critical weights are determined in two structured dimensionalities as, \n(\\textit{i}) layer-wise: we meta-train a layer-by-layer learning rate that enables a \\textit{static} selection of critical layers for updating; \n(\\textit{ii}) channel-wise: we introduce meta attention modules in each layer to select critical channels \\textit{dynamically}, \\ie depending on samples from new tasks. \nPartial updating of weights means that (structurally) sparse gradients are generated, reducing memory requirements to those for computing nonzero gradients.\nIn addition, the computation demand for calculating zero gradients can be also saved.\nTo further reduce the memory, we utilize \\textit{gradient accumulation} in few-shot learning and \\textit{group normalization} in the backbone.\nAlthough weight importance metrics and SGD with sparse gradients have been explored in vanilla training \\cite{bib:NIPS20:Raihan, bib:PIEEE20:Deng, bib:ICASSP20:Gooneratne, bib:ICLR16:Han}, it is unknown \\textit{(i)} how to identify adaptation-critical weights and \\textit{(ii)} whether meta learning is robust to sparse gradients, where the objective is fast adaptation to \\textit{unseen} tasks.\n\nOur main contributions are summarized as follows.\n\\begin{itemize}\n    \\item \n    We design \\sysname, a new meta learning method for data- and memory-efficient DNN adaptation to unseen tasks.\n    \\sysname automatically identifies adaptation-critical weights both layer-wise and channel-wise for low-memory adaptation. \n    The hierarchical approach combines static identification of layers and dynamic identification of channels whose weights are critical for few-shot adaptation.\n    To the best of our knowledge, \\sysname is the first meta learning method designed for on-device few-shot learning.\n    \\item\n    Evaluations on few-shot image classification and reinforcement learning show that, \\sysname not only improves the accuracy but also reduces the peak dynamic memory by a factor of 2.5 on average over the state-of-the-art few-shot adaptation methods. \n    \\sysname can also simultaneously reduce the computation by a factor of 1.7 on average.\n\\end{itemize}\n\nIn the rest of this paper, we introduce the preliminaries and challenges in \\secref{sec:background}, elaborate on the design of \\sysname in \\secref{sec:method}, present its evaluations in \\secref{sec:evaluation}, review related work in \\secref{sec:related}, and conclude in \\secref{sec:conclusion}.\n\n"
            },
            "section 2": {
                "name": "Preliminaries and Challenges",
                "content": "\n\\label{sec:background}\nIn this section, we provide the basics on meta learning for fast adaptation and highlight the challenges to enable on-device adaptation.\n\n\\fakeparagraph{Meta Learning for Fast Adaptation}\nMeta learning is a prevailing solution to adapt a DNN to unseen tasks with limited training samples, \\ie few-shot learning \\cite{bib:arXiv20:Hospedales}.\nWe ground our work on model-agnostic meta learning (MAML) \\cite{bib:ICML17:Finn}, a generic meta learning framework which supports classification, regression and reinforcement learning.\nGiven the dataset $\\mathcal{D}=\\{\\mathcal{S}, \\mathcal{Q}\\}$ of an unseen few-shot task, where $\\mathcal{S}$ (support set) and $\\mathcal{Q}$ (query set) are for training and testing, MAML trains a model $F(\\bm{w})$ with weights $\\bm{w}$ such that it yields high accuracy on $\\mathcal{Q}$ even when $\\mathcal{S}$ only contains a few samples.\nThis is enabled by simulating the few-shot learning experiences over abundant few-shot tasks sampled from a task distribution $p(\\mathsf{T})$.\nSpecifically, it meta-trains a backbone $F$ over few-shot tasks $\\mathsf{T}^{i}\\sim p(\\mathsf{T})$, where each $\\mathsf{T}^{i}$ has dataset $\\mathcal{D}^i=\\{\\mathcal{S}^i,\\mathcal{Q}^i\\}$, and then generates $F(\\bm{w}^\\mathrm{meta})$, an initialization for the unseen few-shot task $\\mathsf{T}^{\\mathrm{new}}$ with dataset $\\mathcal{D}^{\\mathrm{new}}=\\{\\mathcal{S}^{\\mathrm{new}},\\mathcal{Q}^{\\mathrm{new}}\\}$.\nTraining from $F(\\bm{w}^\\mathrm{meta})$ over $\\mathcal{S}^{\\mathrm{new}}$ is expected to achieve a high test accuracy on $\\mathcal{Q}^{\\mathrm{new}}$.\n\nMAML achieves fast adaptation via two-tier optimization. \nIn the \\textit{inner loop}, a task $\\mathsf{T}^i$ and its dataset $\\mathcal{D}^i$ are sampled. \nThe weights $\\bm{w}$ are updated to $\\bm{w}^{i}$ on support dataset $\\mathcal{S}^{i}$ via $K$ gradient descent steps, where $K$ is usually small, compared to vanilla training:\n\\begin{equation}\\label{eq:maml_inner}\n    \\medop{\\bm{w}^{i,k} = \\bm{w}^{i,k-1}-\\alpha\\nabla_{\\bm{w}}~\\ell\\left(\\bm{w}^{i,k-1};\\mathcal{S}^i\\right)\\quad\\mathrm{for}~k=1\\cdots K}\n\\end{equation}\nwhere $\\bm{w}^{i,k}$ are the weights at step $k$ in the inner loop, and $\\alpha$ is the inner step size.\nNote that $\\bm{w}^{i,0}=\\bm{w}$ and $\\bm{w}^{i}=\\bm{w}^{i,K}$.\n$\\ell(\\bm{w};\\mathcal{D})$ is the loss function on dataset $\\mathcal{D}$. \nIn the \\textit{outer loop}, the weights are optimized to minimize the sum of loss at $\\bm{w}^{i}$ on query dataset $\\mathcal{Q}^i$ across tasks. \nThe gradients to update weights in the outer loop are calculated w.r.t. the starting point $\\bm{w}$ of the inner loop.\n\\begin{equation}\\label{eq:maml_outer}\n    \\medop{\\bm{w} \\leftarrow \\bm{w}-\\beta \\nabla_{\\bm{w}} \\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)}\n\\end{equation}\nwhere $\\beta$ is the outer step size.\n\n\n\n \nThe meta-trained weights $\\bm{w}^\\mathrm{meta}$ are then used as initialization for few-shot learning into $\\bm{w}^\\mathrm{new}$ by $K$ gradient descent steps over $\\mathcal{S}^{\\mathrm{new}}$.\nFinally we assess the accuracy of $F(\\bm{w}^\\mathrm{new})$ on $\\mathcal{Q}^{\\mathrm{new}}$.\n\n\\fakeparagraph{Memory Bottleneck of On-device Adaptation}\nAs mentioned above, the meta-trained model $F(\\bm{w}^\\mathrm{meta})$ can adapt to unseen tasks via $K$ gradient descent steps.\nEach step is the same as the inner loop of meta-training \\equref{eq:maml_inner}, but on dataset $\\mathcal{S}^{\\mathrm{new}}$.\n\\begin{equation}\\label{eq:fsl}\n    \\medop{\\bm{w}^{\\mathrm{new},k} = \\bm{w}^{\\mathrm{new},k-1}-\\alpha\\nabla_{\\bm{w}^{\\mathrm{new}}}~\\ell\\left(\\bm{w}^{\\mathrm{new},k-1};\\mathcal{S}^{\\mathrm{new}}\\right)}\n\\end{equation}\nwhere $\\bm{w}^{\\mathrm{new},0}=\\bm{w}^\\mathrm{meta}$.\nFor brevity, we omit the superscripts of model adaption in \\equref{eq:fsl} and use $\\bm{g}(\\cdot)$ as the loss gradients w.r.t. the given tensor.\nHence, without ambiguity, we simplify the notations of \\equref{eq:fsl} as follows:\n\\begin{equation}\\label{eq:fsl_s}\n    \\medop{\\bm{w} \\leftarrow \\bm{w}-\\alpha \\bm{g}(\\bm{w})}\n\\end{equation}\n\nLet us now understand where the main memory cost for iterating \\equref{eq:fsl_s} comes from. \nFor the sake of clarity, we focus on a feed forward DNNs that consist of $L$ convolutional (\\texttt{conv}) layers or fully-connected (\\texttt{fc}) layers.\nA typical layer (see \\figref{fig:layer}) consists of two operations: (\\textit{i}) a linear operation with trainable parameters, \\eg convolution or affine; (\\textit{ii}) a parameter-free non-linear operation (may not exist in certain layers), where we consider max-pooling or ReLU-styled (ReLU, LeakyReLU) activation functions in this paper.\n\n\n\nTake a network consisting of \\texttt{conv} layers only as an example. \nThe memory requirements for storing the activations $\\bm{x}_l\\in\\mathbb{R}^{C_l\\times H_l \\times W_l}$ as well as the convolution weights $\\bm{w}_l \\in \\mathbb{R}^{C_l\\times C_{l-1}\\times S_l \\times S_l} $ of layer $l$ in words can be determined as\n\\[\n\\medop{m(\\bm{x}_l)= C_l H_l W_l \\; , \\; \\; m(\\bm{w}_l) = C_l C_{l-1} S_l^2}\n\\]\nwhere $C_{l-1}$, $C_l$, $H_l$, and $W_l$ stand for input channel number, output channel number, height and width of layer $l$, respectively; $S_l$ stands for the kernel size.\nThe detailed memory and computation demand analysis as provided in Appendix \\ref{app:analysis} reveals that the by far largest memory requirement is neither attributed to determining the activations $\\bm{x}_l$ in the forward path nor to determining the gradients of the activations $\\bm{g}(\\bm{x}_l)$ in the backward path. \nInstead, the memory bottleneck lies in the computation of the weight gradients $\\bm{g}(\\bm{w}_l)$, which requires the availability of the activations $\\bm{x}_{l-1}$ from the forward path. \nFollowing \\equref{eq:memorySimple} in Appendix \\ref{app:analysis}, the necessary memory in words is\n\\begin{equation}\n\\label{eq:MemoryContrib}\n\\medop{\\sum_{1 \\leq l \\leq L} m(\\bm{x}_{l-1})}\n\\end{equation}\n\n\\tabref{tab:examples} summarizes the memory consumption and the total computation of the commonly used few-shot learning backbone models \\cite{bib:ICML17:Finn, bib:NIPS18:Oreshkin}. \nThe requirements are based on the detailed analysis in Appendix~\\ref{app:analysis}.  \nWe can draw two intermediate conclusions. \n\\begin{itemize}\n    \\item \n    The total computation of adaptation (training) is approximately $2.7\\times$ to $3\\times$ larger compared to inference.\n    Yet the peak dynamic memory of training is far larger, $47 \\times$ to $103 \\times$ over inference. \n    The peak dynamic memory consumption of training is also also significantly higher than the static memory consumption from the model and the training samples in few-shot learning. \n    \\item\n    To enable adaptation for memory-constrained IoT devices, we need to find some way of getting rid of the major dynamic memory contribution in \\equref{eq:MemoryContrib}.\n\\end{itemize}\n\n"
            },
            "section 3": {
                "name": "Method",
                "content": "\\label{sec:method}\nThis section presents \\sysname, a new meta learning scheme that enables memory-efficient few-shot learning on unseen tasks.\n\n",
                "subsection 3.1": {
                    "name": "\\sysname Overview",
                    "content": "\n\\label{sec:overview}\nWe first provide an overview of \\sysname and introduce its main concepts, namely selecting critical gradients, using a hierarchical approach to determine adaption-critical layers and channels, and using a mixture of static and dynamic selection mechanisms. \n\n\\fakeparagraph{Principles}\nWe impose \\textit{structured sparsity} on the \\textit{gradients} $\\bm{g}(\\bm{w}_l)$ such that the corresponding tensor dimensions of $\\bm{x}_l$ do not need to be saved. \nThere are other options to reduce the dominant memory demand in \\equref{eq:MemoryContrib}.\nThey are inapplicable for the reasons below.\n\\begin{itemize}\n    \\item \n    One may trade-off computation and memory by recomputing activations $\\bm{x}_{l-1}$ when needed for determining $\\bm{w}_l$, see for example \\cite{bib:arXiv16:Chen, bib:NIPS16:Gruslys}. \n    Due to the limited processing abilities of IoT devices, we exclude this option. \n    \\item\n    It is also possible to prune activations $\\bm{x}_{l-1}$. \n    Yet based on our experiments in Appendix \\ref{app:sparse}, imposing sparsity on $\\bm{x}_{l-1}$ hugely degrades few-shot learning accuracy as this causes error accumulation along the propagation, see also \\cite{bib:NIPS20:Raihan}.\n    \\item\n    Note that unstructured sparsity, as proposed in \\cite{bib:CIKM21:Gao, bib:NIPS21:Oswald}, does not in general lead to memory savings, since there is a very small probability that all weight gradients for which an element of $\\bm{x}_{l-1}$ is necessary have been pruned. \n\\end{itemize}\n\n\\noindent\nWe impose sparsity on the gradients in a hierarchical manner.\n\n\\begin{itemize}\n    \\item \n    \\textbf{Selecting adaption-critical layers.} \n    We first impose layer-by-layer sparsity on $\\bm{g}(\\bm{w}_l)$.\n    It is motivated by previous results showing that manual freezing of certain layers does no harm to few-shot learning accuracy \\cite{bib:ICLR20:Raghu,bib:ICLR21:Oh}. \n    Layer-wise sparsity reduces the number of layers whose weights need to be updated. \n    We determine the adaptation-critical layers from the meta-trained \\textit{layer-wise sparse learning rates}. \n    \n    \\item\n    \\textbf{Selecting adaption-critical channels.}\n    We further reduce the memory demand by imposing sparsity on $\\bm{g}(\\bm{w}_l)$ within each layer. Noting that calculating $\\bm{g}(\\bm{w}_l)$ needs both the input channels $\\bm{x}_{l-1}$ and the output channels $\\bm{g}(\\bm{y}_{l})$, we enforce sparsity on both of them.\n    Input channel sparsity decreases memory and computation overhead, whereas output channel sparsity improves few-shot learning accuracy and reduces computation.\n    We design a novel \\textit{meta attention mechanism} to \\textit{dynamically} determine adaptation-critical channels. \n    They take as inputs $\\bm{x}_{l-1}$ and $\\bm{g}(\\bm{y}_{l})$ and determine adaptation-critical channels during few-shot learning, based on the given few data samples from new unseen tasks. \n    Dynamic channel-wise learning rates as determined by meta attention yield a significantly higher accuracy than a static channel-wise learning rate (see \\secref{sec:ablation}).\n\\end{itemize}\n\n\n\\fakeparagraph{Memory Reduction}  \nThe reduced memory demand due to our hierarchical approach can be seen in \\equref{eq:memorySimple} in Appendix \\ref{app:analysis}:\n\\begin{equation*}\n\\medop{\\sum_{1 \\leq l \\leq L} \\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l m(\\bm{x}_{l-1})}\n\\end{equation*}\nwhere $\\hat{\\alpha}_l \\in \\{ 0, 1\\}$ is the mask from the static selection of critical layers and $0 \\leq \\mu^{\\mathrm{fw}}_l \\leq 1$ denotes the relative amount of dynamically chosen input channels. \n\n\nNext, we explain how \\sysname selects adaptation-critical layers (\\secref{sec:LR}) and channels within layers (\\secref{sec:attention}) as well as the deployment optimizations (\\secref{sec:others}) for memory-efficient adaptation.\n\n"
                },
                "subsection 3.2": {
                    "name": "Selecting Adaption-Critical Layers by Learning Sparse Inner Step Sizes",
                    "content": "\n\\label{sec:LR}\nThis subsection introduces how \\sysname meta-learns adaptation-critical layers to reduce the number of updated layers during few-shot learning.\nParticularly, instead of manual configuration as in \\cite{bib:ICLR21:Oh, bib:ICLR20:Raghu}, we propose to automate the layer selection process.\nDuring meta training, we identify adaptation-critical layers by learning layer-wise sparse inner step sizes (\\secref{subsec:LR:ml}).\nOnly these critical layers with nonzero step sizes will be updated during on-device adaptation to new tasks (\\secref{subsec:LR:fsl}).\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Learning Sparse Inner Step Sizes in Meta Training",
                        "content": "\n\\label{subsec:LR:ml}\n\nPrior work \\cite{bib:ICLR19:Antreas} suggests that instead of a global fixed inner step size $\\alpha$, learning the inner step sizes $\\bm{\\alpha}$ for each layer and each gradient descent step improves the generalization of meta learning, where $\\bm{\\alpha} = \\alpha_{1:L}^{1:K} \\succeq \\bm{0}$.\nWe utilize such learned inner step sizes to infer layer importance for adaptation.\nWe learn the inner step sizes $\\bm{\\alpha}$ in the outer loop of meta-training while fixing them in the inner loop.\n\n\\fakeparagraph{Learning Layer-wise Inner Step Sizes}\nWe change the inner loop of \\equref{eq:maml_inner} to incorporate the per-layer inner step sizes:\n\\begin{equation}\\label{eq:inner_alpha}\n    \\medop{\\bm{w}_l^{i,k} = \\bm{w}_l^{i,k-1}-\\alpha^k_l\\nabla_{\\bm{w}_l}~\\ell\\left(\\bm{w}^{i,k-1}_{1:L};\\mathcal{S}^i\\right)}\n\\end{equation}\nwhere $\\bm{w}_l^{i,k}$ is the weights of layer $l$ at step $k$ optimized on task $i$ (dataset $\\mathcal{S}^{i}$).\nIn the outer loop, weights $\\bm{w}$ are still optimized as\n\\begin{equation}\\label{eq:outer_alpha}\n    \\medop{\\bm{w} \\leftarrow \\bm{w}-\\beta \\nabla_{\\bm{w}} \\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)}\n\\end{equation}\nwhere $\\bm{w}^{i}=\\bm{w}^{i,K}=\\bm{w}^{i,K}_{1:L}$, which is a function of $\\bm{\\alpha}$.\nThe inner step sizes $\\bm{\\alpha}$ are then optimized as\n\\begin{equation}\\label{eq:alpha}\n    \\medop{\\bm{\\alpha} \\leftarrow \\bm{\\alpha}-\\beta \\nabla_{\\bm{\\alpha}} \\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)}\n\\end{equation}\n\n\\fakeparagraph{Imposing Sparsity on Inner Step Sizes}\nTo facilitate layer selection, we enforce sparsity in $\\bm{\\alpha}$, \\ie encouraging a subset of layers to be selected for updating.\nSpecifically, we add a Lasso regularization term in the loss function of \\equref{eq:alpha} when optimizing $\\bm{\\alpha}$.\nHence, the final optimization of $\\bm{\\alpha}$ in the outer loop is formulated as\n\\begin{equation}\\label{eq:regularization}\n    \\medop{\\bm{\\alpha} \\leftarrow \\bm{\\alpha}-\\beta \\nabla_{\\bm{\\alpha}} (\\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)+\\lambda \\sum_{l,k} m(\\bm{x}_{l-1})\\cdot|\\alpha_l^k|)}\n\\end{equation}\nwhere $\\lambda$ is a positive scalar to control the ratio between two terms in the loss function.\nWe empirically set $\\lambda=0.001$.\n$|\\alpha_l^k|$ is re-weighted by $m(\\bm{x}_{l-1})$, which denotes the necessary memory in \\equref{eq:MemoryContrib} if only updating the weights in layer $l$. \n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Exploiting Sparse Inner Step Sizes for On-device Adaptation",
                        "content": "\n\\label{subsec:LR:fsl}\nWe now explain how to apply the learned $\\bm{\\alpha}$ to save memory during on-device adaptation.\nAfter deploying the meta-trained model to IoT devices for adaptation, at updating step $k$, for layers with $\\alpha_l^k=0$, the activations (\\ie their inputs) $\\bm{x}_{l-1}$ need not be stored, see \\equref{eq:memoryAll} and \\equref{eq:memorySimple} in Appendix~\\ref{app:analysis}. \nIn addition, we do not need to calculate the corresponding weight gradients $\\bm{g}(\\bm{w}_l)$, which saves computation, see \\equref{eq:MAC} in Appendix~\\ref{app:analysis}.\n\n\n"
                    }
                },
                "subsection 3.3": {
                    "name": "Selecting Adaption-Critical Channels within Layers via Sparse Meta Attention",
                    "content": "\n\\label{sec:attention}\n\n\n\nThis subsection explains how \\sysname learns a novel meta attention mechanism in each layer to dynamically select adaptation-critical channels for further memory saving in few-shot learning.\nDespite the widespread adoption of channel-wise attention for inference \\cite{bib:CVPR18:Hu, bib:CVPR20:Chen}, we make the first attempt to use attention for memory-efficient training (few-shot learning in our case).\nFor each layer, its meta attention outputs a dynamic channel-wise sparse attention score based on the samples from new tasks.\nThe sparse attention score is used to re-weight (also sparsify) the weight gradients.\nTherefore, by calculating only the nonzero gradients of critical weights within a layer, we can save both memory and computation.\nWe first present our meta attention mechanism during meta training (\\secref{sec:attention:ml}) and then show its usage for on-device model adaptation (\\secref{sec:attention:fsl}).\n\n",
                    "subsubsection 3.3.1": {
                        "name": "Learning Sparse Meta Attention in Meta Training",
                        "content": "\n\\label{sec:attention:ml}\nSince mainstream backbones in meta learning use small kernel sizes (1 or 3), we design the meta attention mechanism channel-wise.\n\\figref{fig:metaattention} illustrates the attention design during meta-training.\n\n\\fakeparagraph{Learning Meta Attention}\nThe attention mechanism is as follows.\n\\begin{itemize}\n    \\item \n    We assign an attention score to the weight gradients of layer $l$ in the inner loop of meta training.\n    The attention scores are expected to indicate which weights/channels are important and thus should be updated in layer $l$.\n    \\item\n    The attention score is obtained from two attention modules: one taking $\\bm{x}_{l-1}$ as input in the forward pass, and the other taking $\\bm{g}(\\bm{y}_l)$ as input during the backward pass.\n    We use $\\bm{x}_{l-1}$ and $\\bm{g}(\\bm{y}_l)$ to calculate the attention scores because they are used to compute the weight gradients $\\bm{g}(\\bm{w}_{l})$.\n\\end{itemize}\nConcretely, we define the forward and backward attention scores for a \\texttt{conv} layer as,\n\\begin{equation}\\label{eq:atten_fw}\n    \\medop{\\bm{\\gamma}^{\\mathrm{fw}}_{l} = h(\\bm{w}^{\\mathrm{fw}}_l;\\bm{x}_{l-1})\\in\\mathbb{R}^{C_{l-1}\\times 1 \\times 1}}\n\\end{equation}\n\\begin{equation}\\label{eq:atten_bw}\n    \\medop{\\bm{\\gamma}^{\\mathrm{bw}}_{l} = h(\\bm{w}^{\\mathrm{bw}}_l;\\bm{g}(\\bm{y}_l))\\in\\mathbb{R}^{C_{l}\\times 1 \\times 1}}\n\\end{equation}\nwhere $h(\\cdot;\\cdot)$ stands for the meta attention module, and $\\bm{w}^{\\mathrm{fw}}_l$ and $\\bm{w}^{\\mathrm{bw}}_l$ are the parameters of the meta attention modules.\nThe overall (sparse) attention scores $\\bm{\\gamma}_l\\in\\mathbb{R}^{C_{l}\\times C_{l-1} \\times 1 \\times 1}$ and is computed as, \n\\begin{equation}\\label{eq:atten_fwbw}\n    \\medop{\\gamma_{l,ba11} = \\gamma^{\\mathrm{fw}}_{l,a11} \\cdot \\gamma^{\\mathrm{bw}}_{l,b11}} \n\\end{equation}\nIn the inner loop, for layer $l$, step $k$ and task $i$, $\\bm{\\gamma}_l$ is (broadcasting) multiplied with the dense weight gradients to get the sparse ones,\n\\begin{equation}\\label{eq:inner_incr}\n    \\medop{\\bm{\\gamma}_l^{i,k}\\odot\\nabla_{\\bm{w}_l}~\\ell\\left(\\bm{w}^{i,k-1}_{1:L};\\mathcal{S}^i\\right)}\n\\end{equation}\nThe weights are then updated by,\n\\begin{equation}\\label{eq:inner_atten}\n    \\medop{\\bm{w}_l^{i,k} = \\bm{w}_l^{i,k-1}-\\alpha^k_l(\\bm{\\gamma}_l^{i,k}\\odot\\nabla_{\\bm{w}_l}~\\ell\\left(\\bm{w}^{i,k-1}_{1:L};\\mathcal{S}^i\\right))}\n\\end{equation}\nLet all attention parameters be $\\bm{w}^{\\mathrm{atten}} = \\{\\bm{w}^{\\mathrm{fw}}_l,\\bm{w}^{\\mathrm{bw}}_l\\}_{l=1}^{L}$. \nThe attention parameters $\\bm{w}^{\\mathrm{atten}}$ are optimized in the outer loop as,\n\\begin{equation}\\label{eq:outer_atten}\n    \\medop{\\bm{w}^{\\mathrm{atten}} \\leftarrow \\bm{w}^{\\mathrm{atten}}-\\beta \\nabla_{\\bm{w}^{\\mathrm{atten}}} \\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)}\n\\end{equation}\nNote that we use a dense forward path and a dense backward path in both meta-training and on-device adaptation, as shown in \\figref{fig:metaattention}.\nThat is, the attention scores $\\bm{\\gamma}^{\\mathrm{fw}}_{l}$ and $\\bm{\\gamma}^{\\mathrm{bw}}_{l}$ are only calculated locally and will not affect $\\bm{y}_l$ during forward and $\\bm{g}(\\bm{x}_{l-1})$ during backward.\n\n\\begin{algorithm}[t]\n\\caption{Clip and normalization}\\label{alg:clip}\n\\KwIn{softmax output (normalized) $\\bm{\\pi}\\in\\mathbb{R}^{C}$, clip ratio $\\rho$} \n\\KwOut{sparse $\\bm{\\gamma}$}\nSort $\\bm{\\pi}$ in ascending order and get sorted indices $d_{1:C}$\\; \nFind the smallest $c$ such that $\\sum_{i=1}^c\\pi_{d_i}\\ge\\rho$\\;\nSet $\\pi_{d_1:d_c}$ as 0 \\tcp*{if $\\rho=0$, do nothing}\nNormalize $\\bm{\\gamma} = \\bm{\\pi}/\\sum\\bm{\\pi}$\\;\nRe-scale $\\bm{\\gamma} = \\bm{\\gamma}\\cdot C$ \\tcp*{keeping step sizes' magnitude}\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\\caption{p-Meta}\\label{alg:micromaml}\n\\KwIn{meta-training task distribution $p(\\mathsf{T})$, backbone $F$ with initial weights $\\bm{w}$, meta attention parameters $\\bm{w}^{\\mathrm{atten}}$, inner step sizes $\\bm{\\alpha}$, outer step sizes $\\beta$} \n\\KwOut{meta-trained weights $\\bm{w}$, meta-trained meta attention parameters $\\bm{w}^{\\mathrm{atten}}$, meta-trained sparse inner step sizes $\\bm{\\alpha}$}\n\\While {not done} {\n    Sample a batch of $I$ tasks $\\mathsf{T}^i\\sim p(\\mathsf{T})$\\;\n    \\For {$i \\leftarrow 1$ \\KwTo $I$} {\n        Update $\\bm{w}^i$ in $K$ gradient descent steps with \\eqref{eq:inner_atten}\\;\n    }\n    Update $\\bm{w}$ with \\eqref{eq:outer_alpha}\\;\n    Update inner step sizes $\\bm{\\alpha}$ with \\eqref{eq:regularization}\\; \n    Update attention parameters $\\bm{w}^{\\mathrm{atten}}$ with \\equref{eq:outer_atten}\\;\n}\n\\end{algorithm}\n\n\\fakeparagraph{Meta Attention Module Design}\n\\figref{fig:metaattention} (upper part) shows an example meta attention module.\nWe adapt the inference attention modules used in \\cite{bib:CVPR18:Hu, bib:CVPR20:Chen}, yet with the following modifications.\n\\begin{itemize}\n    \\item \n    Unlike inference attention that applies to a single sample, training may calculate the averaged loss gradients based on a batch of samples.\n    Since $\\bm{g}(\\bm{w}_l)$ does not have a batch dimension, the input to softmax function is first averaged over the batch data, see in \\figref{fig:metaattention}.\n    \\item\n    We enforce sparsity on the meta attention scores such that they can be utilized to save memory and computation in few-shot learning.\n    The original attention in \\cite{bib:CVPR18:Hu, bib:CVPR20:Chen} outputs normalized scales in $[0,1]$ from softmax.\n    We clip the output with a clip ratio $\\rho\\in[0,1]$ to create zeros in $\\bm{\\gamma}$.\n    This way, our meta attention modules yield batch-averaged sparse attention scores $\\bm{\\gamma}^{\\mathrm{fw}}_{l}$ and $\\bm{\\gamma}^{\\mathrm{bw}}_{l}$. \n    \\algoref{alg:clip} shows this clipping and re-normalization process.\n    Note that \\algoref{alg:clip} is not differentiable.\n    Hence we use the straight-through-estimator for its backward propagation in meta training. \n\\end{itemize}\n\n\n\n\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Exploiting Meta Attention for On-device Adaptation",
                        "content": "\n\\label{sec:attention:fsl}\nWe now explain how to apply the meta attention to save memory during on-device few-shot learning.\nNote that the parameters in the meta attention modules are fixed during few-shot learning.\nAssume that at step $k$, layer $l$ has a nonzero step size $\\alpha_l^k$.\nIn the forward pass, we only store a sparse tensor $\\bm{\\gamma}_l^{\\mathrm{fw}}\\cdot\\bm{x}_{l-1}$, \\ie its channels are stored only if they correspond to nonzero entries in $\\bm{\\gamma}_l^{\\mathrm{fw}}$.\nThis reduces memory consumption as shown in \\equref{eq:memorySimple} in Appendix \\ref{app:analysis}.\nSimilarly, in the backward pass, we get a channel-wise sparse tensor $\\bm{\\gamma}_l^{\\mathrm{bw}}\\cdot\\bm{g}(\\bm{y}_{l})$.\nSince both sparse tensors are used to calculate the corresponding nonzero gradients in $\\bm{g}(\\bm{w}_l)$, the computation cost is also reduced, see \\equref{eq:MAC} in Appendix \\ref{app:analysis}.\nWe plot the meta attention during on-device adaptation in \\figref{fig:metaattention_fewshot}.\n\n\n"
                    }
                },
                "subsection 3.4": {
                    "name": "Summary of \\sysname",
                    "content": "\n\n\\algoref{alg:micromaml} shows the overall process of \\sysname during meta-training. \nThe final meta-trained weights $\\bm{w}$ from \\algoref{alg:micromaml} are assigned to $\\bm{w}^{\\mathrm{meta}}$, see \\secref{sec:background}.\nThe meta-trained backbone model $F(\\bm{w}^{\\mathrm{meta}})$, the sparse inner step sizes $\\bm{\\alpha}$, and the meta attention modules will be then deployed on edge devices and used to conduct a memory-efficient few-shot learning.\n\n"
                },
                "subsection 3.5": {
                    "name": "Deployment Optimization",
                    "content": "\n\\label{sec:others}\nTo further reduce the memory during few-shot learning, we propose gradient accumulation during backpropagation and replace batch normalization in the backbone with group normalization.\n\n",
                    "subsubsection 3.5.1": {
                        "name": "Gradient Accumulation",
                        "content": "\n\\label{sec:gradacc}\nIn standard few-shot learning, all the new samples (\\eg $25$ for $5$-way $5$-shot) are fed into the model as one batch.\nTo reduce the peak memory due to large batch sizes, we conduct few-shot learning with gradient accumulation (GA).\n\nGA is a technique that (\\textit{i}) breaks a large batch into smaller partial batches; (\\textit{ii}) sequentially forward/backward propagates each partial batches through the model; (\\textit{iii}) accumulates the loss gradients of each partial batch and get the final averaged gradients of the full batch. \nNote that GA does not increase computation, which is desired for low-resource platforms.\nWe evaluate the impact of different sample batch sizes in GA in Appendix~\\ref{app:batchsize}.\n\n\n\n"
                    },
                    "subsubsection 3.5.2": {
                        "name": "Group Normalization",
                        "content": "\n\\label{sec:norm}\nMainstream backbones in meta learning typically adopt batch normalization layers.\nBatch normalization layers compute the statistical information in each batch, which is dependent on the sample batch size.\nWhen using GA with different sample batch sizes, the inaccurate batch statistics can degrade the training performance (see Appendix \\ref{app:pool}).\nAs a remedy, we use group normalization \\cite{bib:ECCV18:Wu}, which does not rely on batch statistics (\\ie independent of the sample batch size).\nWe also apply meta attention on group normalization layers when updating their weights.\nThe only difference w.r.t. \\texttt{conv} and \\texttt{fc} layers is that the stored input tensor (also the one used for the meta attention) is not $\\bm{x}_{l-1}$, but its normalized version.\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Evaluation",
                "content": "\n\\label{sec:evaluation}\nThis section presents the evaluations of \\sysname on standard few-shot image classification and reinforcement learning benchmarks.\n\n\n\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "General Experimental Settings",
                    "content": "\n\\label{sec:settings}\n\n\\fakeparagraph{Compared Methods}\nWe test the meta learning algorithms below. \n\\begin{itemize}\n    \\item MAML \\cite{bib:ICML17:Finn}: the original model-agnostic meta learning.\n    \\item ANIL \\cite{bib:ICLR20:Raghu}: update the last layer only in few-shot learning.\n    \\item BOIL \\cite{bib:ICLR21:Oh}: update the body except the last layer.\n    \\item MAML++ \\cite{bib:ICLR19:Antreas}: learn a per-step per-layer step sizes $\\bm{\\alpha}$.\n    \\item \\sysname (\\ref{sec:LR}): can be regarded as a sparse version of MAML++, since it learns a sparse $\\bm{\\alpha}$ with our methods in \\secref{sec:LR}. \n    \\item \\sysname (\\ref{sec:LR}+\\ref{sec:attention}): the full version of our methods which include the meta attention modules in \\secref{sec:attention}. \n\\end{itemize}\nFor fair comparison, all the algorithms are re-implemented with the deployment optimization in \\secref{sec:others}. \n\n\\fakeparagraph{Implementation}\nThe experiments are conducted with tools provided by TorchMeta \\cite{bib:torchmeta, bib:torchmetarl}.\nParticularly, the backbone is meta-trained with full sample batch size (\\eg 25 for 5-way 5-shot) on meta training dataset.\nAfter each meta training epoch, the model is tested (\\ie few-shot learned) on meta validation dataset.\nThe model with the highest validation performance is used to report the final few-shot learning results on meta test dataset.\nWe follow the same process as TorchMeta \\cite{bib:torchmeta, bib:torchmetarl} to build the dataset. \nDuring few-shot learning, we adopt a sample batch size of 1 to verify the model performance under the most strict memory constraints.\n\nIn \\sysname, meta attention is applied to all \\texttt{conv}, \\texttt{fc}, and group normalization layers, except the last output layer, because (\\textit{i}) we find modifying the last layer's gradients may decrease accuracy; (\\textit{ii}) the final output is often rather small in size, resulting in little memory saving even if imposing sparsity on the last layer.  \nWithout further notations, we set $\\rho=0.3$ in forward attention, and $\\rho=0$ in backward attention across all layers, as the sparsity of $\\bm{\\gamma}_l^{\\mathrm{bw}}$ almost has no effect on the memory saving.\n\n\\fakeparagraph{Metrics}\nWe compare the peak memory and MACs of different algorithms.\nNote that the reported peak memory and MACs for \\sysname also include the consumption from meta attention, although they are rather small related to the backward propagation.\n\n"
                },
                "subsection 4.2": {
                    "name": "Performance on Image Classification",
                    "content": "\n\\label{sec:image}\n\n\\fakeparagraph{Settings}\nWe test on standard few-shot image classification tasks (both in-domain and cross-domain). \nWe adopt two common backbones, ``4Conv'' \\cite{bib:ICML17:Finn} which has 4 \\texttt{conv} blocks with 32 channels in each block, and ``ResNet12'' \\cite{bib:NIPS18:Oreshkin} which contains 4 residual blocks with $\\{64,128,256,512\\}$ channels in each block respectively.\nWe replace the batch normalization layers with group normalization layers, as discussed in \\secref{sec:norm}.\nWe experiment in both 5-way 1-shot and 5-way 5-shot settings.\nWe train the model on MiniImageNet \\cite{bib:NIPS16:Vinyals} (both meta training and meta validation dataset) with 100 meta epochs.\nIn each meta epoch, 1000 random tasks are drawn from the task distribution. \nThe task batch size is set to 4 in general, except for ResNet12 under 5-way 5-shot settings where we use 2.\nThe model is updated with 5 gradient steps (\\ie $K=5$) in both inner loop of meta-training and few-shot learning.  \nWe use Adam optimizer with cosine learning rate scheduling as \\cite{bib:ICLR19:Antreas} for all outer loop updating.\nThe (initial) inner step size $\\bm{\\alpha}$ is set to 0.01.\nThe meta-trained model is then tested on three datasets MiniImageNet \\cite{bib:NIPS16:Vinyals}, TieredImageNet \\cite{bib:ICLR18:Ren}, and CUB \\cite{bib:CUB} to verify both \\textit{in-domain} and \\textit{cross-domain} performance.\n\n\\fakeparagraph{Results}\n\\tabref{tab:image} shows the accuracy averaged over $5000$ new unseen tasks randomly drawn from the meta test dataset.\nWe also report the average number of GMACs and the average peak memory per task according to Appendix~\\ref{app:analysis}.\nClearly, \\sysname almost always yields the highest accuracy in all settings. \nNote that the comparison between ``\\sysname (\\ref{sec:LR})'' and ``MAML++'' can be considered as the ablation studies on learning sparse layer-wise inner step sizes proposed in \\secref{sec:LR}. \nThanks to the imposed sparsity on $\\bm{\\alpha}$, ``\\sysname (\\ref{sec:LR})'' significantly reduces the peak memory ($2.5\\times$ saving on average and up to $3.1\\times$) and the computation burden ($1.7\\times$ saving on average and up to $2.4\\times$) over ``MAML++''. \nNote that the imposed sparsity also cause a moderate accuracy drop.\nHowever, with the meta attention, ``\\sysname (\\ref{sec:LR}+\\ref{sec:attention})'' not only notably improves the accuracy but also further reduces the peak memory ($2.7\\times$ saving on average and up to $3.4\\times$) and computation ($1.9\\times$ saving on average and up to $2.6\\times$) over ``MAML++''.  \n``ANIL'' only updates the last layer, and therefore consumes less memory but also yields a substantially lower accuracy.\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Performance on Reinforcement Learning",
                    "content": "\n\\label{sec:reinforcement}\n\n\\fakeparagraph{Settings}\nTo show the versatility of \\sysname, we experiment with two few-shot reinforcement learning problems: 2D navigation and Half-Cheetah robot locomotion simulated with MuJoCo library \\cite{bib:IROS12:Todorov}.\nFor all experiments, we mainly adopt the experimental setup in \\cite{bib:ICML17:Finn}.\nWe use a neural network policy with two hidden \\texttt{fc} layers of size 100 and ReLU activation function.\nWe adopt vanilla policy gradient \\cite{bib:ML1992:Williams} for the inner loop and trust-region policy optimization \\cite{bib:ICML15:Schulman} for the outer loop.\nDuring the inner loop as well as few-shot learning, the agents rollout 20 episodes with a horizon size of 200 and are updated for one gradient step.\nThe policy model is trained for 500 meta epochs, and the model with the best average return during training is used for evaluation.\nThe task batch size is set to 20 for 2D navigation, and 40 for robot locomotion.\nThe (initial) inner step size $\\bm{\\alpha}$ is set to 0.1.\nEach episode is considered as a data sample, and thus the gradients are accumulated 20 times for a gradient step.\n\n\\fakeparagraph{Results}\n\\tabref{tab:mlp} lists the average return averaged over 400 new unseen tasks randomly drawn from simulated environments.\nWe also report the average number of GMACs and the average peak memory per task according to Appendix \\ref{app:analysis}.\nNote that the reported computation and peak memory do not include the estimations of the advantage \\cite{bib:ICML16:Duan}, as they are relatively small and could be done during the rollout. \n\\sysname consumes a rather small amount of memory and computation, while often obtains the highest return in comparison to others. \nTherefore, \\sysname can fast adapt its policy to reach the new goal in the environment with less on-device resource demand. \n\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Studies on Meta Attention",
                    "content": "\n\\label{sec:ablation}\nWe study the effectiveness of our meta attention via the following two ablation studies.\nThe experiments are conducted on ``4Conv'' in both 5-way 1-shot and 5-way 5-shot as \\secref{sec:image}.\n\n\\fakeparagraph{Sparsity in Meta Attention}\n\\tabref{tab:ablation} shows the few-shot classification accuracy with different sparsity settings in the meta attention. \n\nWe first do not impose sparsity on $\\bm{\\gamma}_l^{\\mathrm{fw}}$ and $\\bm{\\gamma}_l^{\\mathrm{bw}}$ (\\ie set both $\\rho$'s as 0), and adopt forward attention and backward attention separately. \nIn comparison to no meta attention at all, enabling either forward or backward attention improves accuracy. \nWith both attention enabled, the model achieves the best performance.\n\nWe then test the effects when imposing sparsity on $\\bm{\\gamma}_l^{\\mathrm{fw}}$ or $\\bm{\\gamma}_l^{\\mathrm{bw}}$ (\\ie set $\\rho>0$). \nWe use the same $\\rho$ for all layers.\nWe observe a sparse $\\bm{\\gamma}_l^{\\mathrm{bw}}$ often cause a larger accuracy drop than a sparse $\\bm{\\gamma}_l^{\\mathrm{fw}}$.\nSince a sparse $\\bm{\\gamma}_l^{\\mathrm{bw}}$ does not bring substantial memory or computation saving (see Appendix~\\ref{app:analysis}), we use $\\rho=0$ for backward attention and $\\rho=0.3$ for forward attention.\n\nAttention scores $\\bm{\\gamma}_l$ introduce a dynamic channel-wise learning rate according to the new data samples. \nWe further compare meta attention with a static channel-wise learning rate, where the channel-wise learning rate $\\bm{\\alpha}^{\\mathrm{Ch}}$ is meta-trained as the layer-wise inner step sizes in \\secref{sec:LR} while without imposing sparsity.\nBy comparing ``$\\bm{\\alpha}^{\\mathrm{Ch}}$'' with ``0, 0'' in \\tabref{tab:ablation}, we conclude that the dynamic channel-wise learning rate yields a significantly higher accuracy.\n\n\n\n\n\\fakeparagraph{Layer-wise Updating Ratios}\nTo study the resulted updating ratios across layers, \\ie the layer-wise sparsity of weight gradients, we randomly select 100 new tasks and plot the layer-wise updating ratios, see \\figref{fig:ratio} \\textit{Left (1:5)}. \nThe ``4Conv'' backbone has 9 layers ($L=9$), \\ie 8 alternates of \\texttt{conv} and group normalization layers, and an \\texttt{fc} output layer.\nAs mentioned in \\secref{sec:settings}, we do not apply meta attention to the output layer, \\ie $\\bm{\\gamma}_9=1$.\nThe used backbone is updated with 5 gradient steps ($K=5$). \nWe use $\\rho=0.3$ for forward attention, and $\\rho=0$ for backward. \nNote that \\algoref{alg:clip} adaptively determines the sparsity of $\\bm{\\gamma}_l$, which also means different samples may result in different updating ratios even with the same $\\rho$ (see \\figref{fig:ratio}). \nThe size of $\\bm{x}_{l-1}$ often decreases along the layers in current DNNs.\nAs expected, the latter layers are preferred to be updated more, since they need a smaller amount of memory for updating.\nInterestingly, even if with a small $\\rho$($=0.3$), the ratio of updated weights is rather small, \\eg smaller than $0.2$ in step 3 of 5-way 5-shot.\nIt implies that the outputs of softmax have a large discrepancy, \\ie only a few channels are adaptation-critical for each sample, which in turn verifies the effectiveness of our meta attention mechanism. \n\nWe also randomly pair data samples and compute the cosine similarity between their attention scores $\\bm{\\gamma}_l$.  \nWe plot the cosine similarity of step 1 in \\figref{fig:ratio} \\textit{Right}. \nThe results show that there may exist a considerable variation on the adaptation-critical weights selected by different samples, which is consistent with our observation in \\tabref{tab:ablation}, \\ie dynamic learning rate outperforms the static one.\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n\\label{sec:related}\n\n\\fakeparagraph{Meta Learning for Few-Shot Learning}\nMeta learning is a prevailing solution to few-shot learning \\cite{bib:arXiv20:Hospedales}, where the meta-trained model can learn an unseen task from a few training samples, \\ie data-efficient adaptation.\nThe majority of meta learning methods can be divided into two categories, (\\textit{i}) metric-based methods \\cite{bib:NIPS16:Vinyals, bib:NIPS17:Snell, bib:CVPR18:Sung} that learn an embedded metric for classification tasks to map the query samples onto the classes of labeled support samples, (\\textit{ii}) gradient-based methods \\cite{bib:ICLR19:Antreas, bib:ICML17:Finn, bib:ICLR20:Raghu, bib:ICLR20:Triantafillou, bib:ICLR21:Oh, bib:NIPS21:Oswald} that learn an initial model (and/or optimizer parameters) such that it can be adapted with gradient information calculated on the new few samples.   \nIn comparison to metric-based methods, we focus on gradient-based meta learning methods for their wide applicability in various learning tasks (\\eg regression, classification, reinforcement learning) and the availability of gradient-based training frameworks for low-resource devices \\cite{bib:tfliteOndeviceTraining}.\n\nParticularly, we aim at meta training a DNN that allows effective adaptation on memory-constrained devices.\nMost meta learning algorithms \\cite{bib:ICLR19:Antreas, bib:ICML17:Finn, bib:NIPS21:Oswald} optimize the backbone network for better generalization yet ignore the workload if the meta-trained backbone is deployed to low-resource platforms for model adaptation. \nManually fixing certain layers during on-device few-shot learning \\cite{bib:ICLR20:Raghu, bib:ICLR21:Oh, bib:AAAI21:Shen} may also reduce memory and computation, but to a much lesser extent as shown in our evaluations.\n\n\n\\fakeparagraph{Efficient DNN Training}\nExisting efficient training schemes are mainly designed for high-throughput GPU training on large-scale datasets.\nA general strategy is to trade memory with computation \\cite{bib:arXiv16:Chen, bib:NIPS16:Gruslys}, which is unfit for IoT device with a limited computation capability. \nAn alternative is to sparsify the computational graphs in backpropagation \\cite{bib:NIPS20:Raihan}.\nYet it relies on massive training iterations on large-scale datasets.\nOther techniques include layer-wise local training \\cite{bib:NIPS17:Greff} and reversible residual module \\cite{bib:NIPS17:Gomez}, but they often incur notable accuracy drops. \n\nThere are a few studies on DNN training on low-resource platforms, such as updating the last several layers only \\cite{bib:MLSys21:Mathur}, reducing batch sizes \\cite{bib:SenSys19:Lee}, and gradient approximation \\cite{bib:ICASSP20:Gooneratne}.\nHowever, they are designed for vanilla supervised training, \\ie train and test on the same task. \nOne recent study proposes to update the bias parameters only for memory-efficient transfer learning \\cite{bib:NIPS20:Cai}, yet transfer learning is prone to overfitting when trained with limited data \\cite{bib:ICML17:Finn}. \n\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\nIn this paper, we present \\sysname, a new meta learning scheme for data- and memory-efficient on-device DNN adaptation.\nIt enables structured partial parameter updates for memory-efficient few-shot learning by automatically identifying adaptation-critical weights both layer-wise and channel-wise. \nEvaluations show a reduction in peak dynamic memory by 2.5$\\times$ on average over the state-of-the-art few-shot adaptation methods.\nWe envision \\sysname as an early step towards adaptive and autonomous edge intelligence applications.\n\n"
            },
            "section 7": {
                "name": "Acknowledgement",
                "content": "\nPart of Zhongnan Qu and Lothar Thiele's work was supported by the Swiss National Science Foundation in the context of the NCCR Automation.\nThis research was supported by the Lee Kong Chian Fellowship awarded to Zimu Zhou by Singapore Management University.\nZimu Zhou is the corresponding author.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{cites_short}\n\n\\clearpage\n\\appendix\n\n"
            },
            "section 8": {
                "name": "Memory and Computation",
                "content": "\n\\label{app:analysis}\nIn the following, we derive the memory requirement and computation workload for inference and adaptation. We restrict ourselves to a feed-forward network of fully-connected (\\texttt{fc}) or convolutional (\\texttt{conv}) layers. \nNote that our analysis focuses on 2D \\texttt{conv} layers but can apply to other \\texttt{conv} layer types as well. \nWe assume the rectified linear activation function (ReLU) for all layers, denoted as $\\sigma$. For simplicity, we omit the bias, normalization layers, pooling or strides. \nWe use the notation $m(\\bm{x})$ to denote the memory demand in words to store tensor $\\bm{x}$. \nThe wordlength is denoted as $\\mathit{T}$. \n\nFor representing indexed summations we use the Einstein notation. \nIf index variables appear in a term on the right hand side of an equation and are not otherwise defined (free indices), it implies summation of that term over the range of the free indices. \nIf indices of involved tensor elements are out of range, the values of these elements are assumed to be 0. \n\n\n",
                "subsection 8.1": {
                    "name": "Single Layer",
                    "content": "\n\nWe start with a single layer and accumulate the memory and computation for networks with several layers afterwards. \nAssume the input tensor of a layer is $\\bm{x}$, the weight tensor is $\\bm{w}$, the result after the linear transformation is $\\bm{y}$, and the layer output after the non-linear operator is $\\bm{z}$ which is also the input to the next layer. \n\nFor convolutional layers, we have $\\bm{x} \\in \\mathbb{R}^{C_I \\times H_I \\times W_I}$ and elements $x_{cij}$, where $C_I$, $H_I$, and $W_I$ denote the number of input channels, height and width, respectively. \nIn a similar way, we have $\\bm{z} \\in \\mathbb{R}^{C_O \\times H_O \\times W_O}$ with elements $x_{fij}$ where $C_O$, $H_O$, and $W_O$ denote the number of output channels, height and width, respectively. Moreover, $\\bm{w} \\in \\mathbb{R}^{C_O \\times C_I \\times S \\times S}$ with elements $w_{fcmn}$. \nTherefore,\n\\[\n\\medop{m(\\bm{x})= C_I H_I W_I \\; , \\; \\; m(\\bm{y})= m(\\bm{z}) = C_O H_O W_O \\; , \\; \\; m(\\bm{w}) = C_O C_I S^2}\n\\]\nFor fully connected layers we have $\\bm{x} \\in \\mathbb{R}^{C_I}$, $\\bm{y}, \\bm{z} \\in \\mathbb{R}^{C_O}$, and $\\bm{w} \\in \\mathbb{R}^{C_O \\times C_I}$ with memory demand\n\\[\n\\medop{m(\\bm{x})= C_I  \\; , \\; \\; m(\\bm{y})= m(\\bm{z}) = C_O  \\; , \\; \\; m(\\bm{w}) = C_O C_I} \n\\]\n\n",
                    "subsubsection 8.1.1": {
                        "name": "Fully Connected Layer",
                        "content": "\nFor inference we derive the relations $y_{f} = w_{fc} x_c$ and $z_{f} = \\sigma(y_{f})$ for all admissible indices $f \\in [1, C_O]$. \nThe necessary dynamic memory has a size of about $m(\\bm{x}) + m(\\bm{y})$ words and we need about $m(\\bm{w})$ MAC operations. \n\nFor adaptation, we suppose that $\\frac{\\partial \\ell}{\\partial z_i}$ is already provided from the next layer. \nWe find $\\frac{\\partial \\ell}{\\partial y_i} = \\sigma'(y_i) \\cdot \\frac{\\partial \\ell}{\\partial z_i}$ with $\\sigma'(y_i) = \\begin{cases} 1 & \\mbox{if } y_i > 0 \\\\ 0 & \\mbox{if } y_i < 0  \\end{cases}$ which leads to $\\frac{\\partial \\ell}{\\partial x_i} = w_{ji} \\cdot \\frac{\\partial \\ell}{\\partial y_j}$. \nThe necessary dynamic memory is about $m(\\bm{x}) + m(\\bm{y}) \\cdot (1 + \\frac{1}{\\mathit{T}})$ words, where the last term comes from storing $\\sigma'(y_i)$ single bits from the forward path. \nWe need about $m(\\bm{w})$ MAC operations.\n\nAccording to the approach described in the paper we are only interested in the partial derivatives $\\frac{\\partial \\ell}{\\partial w_{fc}}$ if $\\alpha > 0$ for this layer, and if scales $\\gamma^{\\mathrm{bw}}_{f} > 0$ and $\\gamma^{\\mathrm{fw}}_{c} > 0$ for indices $f$, $c$. \nTo simplify the notation, let us define the critical ratios \n\\begin{gather*}\n\\medop{\\mu^{\\mathrm{fw}} = \\frac{\\text{number of nonzero elements of } \\gamma^{\\mathrm{fw}}_{c}}{C_I}} \\\\\n\\medop{\\mu^{\\mathrm{bw}} = \\frac{\\text{number of nonzero elements of } \\gamma^{\\mathrm{bw}}_{f}}{C_O}}\n\\end{gather*}\nwhich are 1 if all channels are determined to be critical for weight adaptation, and 0 if none of them. \n\nWe find $\\gamma^{\\mathrm{bw}}_{f} \\frac{\\partial \\ell}{\\partial w_{fc}}  \\gamma^{\\mathrm{fw}}_{c}  =  (\\gamma^{\\mathrm{bw}}_{f} \\frac{\\partial \\ell}{\\partial y_{f}}) \\cdot  (\\gamma^{\\mathrm{fw}}_{c} x_c)$.\nTherefore, we need $\\mu^{\\mathrm{fw}} \\mu^{\\mathrm{bw}} m(\\bm{w}) + \\mu^{\\mathrm{fw}} m(\\bm{x})$ words dynamic memory if $\\alpha > 0$ where the latter term considers the information needed from the forward path. \nWe require about $\\mu^{\\mathrm{fw}} \\mu^{\\mathrm{bw}} m(\\bm{w})$ MAC operations if $\\alpha > 0$. \n\n\n"
                    },
                    "subsubsection 8.1.2": {
                        "name": "Convolutional Layer",
                        "content": "\n\nThe memory analysis for a convolutional layer is very similar, just replacing matrix multiplication by convolution. \nFor inference we find $y_{fij} = w_{fcmn} x_{c, i+m-1, j+n-1}$ and $z_{fij} = \\sigma(y_{fij})$ for all admissible indices $f$, $i$, $j$. \nThe necessary dynamic memory has a size of about $\\max \\{ m(\\bm{x}), m(\\bm{y}) \\}$ words when using memory sharing between input and output tensors. \nWe need about $H_O W_O \\cdot m(\\bm{w})$ MAC operations. \n\nFor adaptation, we again suppose that $\\frac{\\partial \\ell}{\\partial z_{fij}}$ is provided from the next layer. \nWe find $\\frac{\\partial \\ell}{\\partial y_{fij}} = \\sigma'(y_{fij}) \\cdot \\frac{\\partial \\ell}{\\partial z_{fij}}$ and get $\\frac{\\partial \\ell}{\\partial x_{cij}} = w_{fcmn}  \\cdot \\frac{\\partial \\ell}{\\partial y_{f, i + m - 1, j + n - 1}}$. \nThe necessary memory is about $\\max \\{ m(\\bm{x}), m(\\bm{y}) \\} +  \\frac{ m(\\bm{y})}{\\mathit{T}}$ words, where the last term comes from storing $\\sigma'(y_{fij})$ single bits from the forward path. \nWe need about $H_I W_I \\cdot m(\\bm{w})$ multiply and accumulate operations.\n\nFor determining the weight gradients we find $\\frac{\\partial \\ell}{\\partial w_{fcmn}}  =  \\frac{\\partial \\ell}{\\partial y_{fij}} \\cdot x_{c, i + m -1, j + n - 1}$. \nWhen considering the scales for filtering, we yield $\\gamma^{\\mathrm{bw}}_{f} \\frac{\\partial \\ell}{\\partial w_{fcmn}}  \\gamma^{\\mathrm{fw}}_{c}  =  (\\gamma^{\\mathrm{bw}}_{f} \\frac{\\partial \\ell}{\\partial y_{fij}}) \\cdot  (\\gamma^{\\mathrm{fw}}_{c} x_{c, i + m -1, j + n - 1})$. \nAs a result, we need $\\mu^{\\mathrm{fw}} \\mu^{\\mathrm{bw}} m(\\bm{w}) +  \\mu^{\\mathrm{fw}} m(\\bm{x})$ words of dynamic memory if $\\alpha > 0$ where the latter term considers the information needed from the forward path. \nWe require about $\\mu^{\\mathrm{fw}} \\mu^{\\mathrm{bw}} H_O W_O m(\\bm{w})$ MAC operations if $\\alpha > 0$. \n\nFinally, let us determine the required memory and computation to determine the scales $\\gamma^{\\mathrm{fw}}_{c}$ and $\\gamma^{\\mathrm{bw}}_{f}$. \nAccording to \\figref{fig:metaattention}, we find as an upper bound for the memory $B \\cdot ( C_I + C_O)$ and $( C_I H_I W_I + 2 C_I^2 + C_O H_O W_O + 2 C_O^2 )$ MAC operations. \n\n\n"
                    }
                },
                "subsection 8.2": {
                    "name": "All Layers",
                    "content": "\n\nThe above relations are valid for a single layer. \nThe following relations hold for the overall network. \nIn order to simplify the notation, we consider a network that consists of convolution layers only.\nExtensions to mixed layers can simply be done.\n\nWe suppose $L$ layers with sizes $C_l$, $H_l$, $W_l$ and $S_l$ for the number of output channels, output width, output height and kernel size, respectively. \nWe assume that the step-sizes $\\alpha_l$ for some iteration of the adaption are given. \nThe memory requirement in words is\n\\[\n\\medop{m(\\bm{x}_l)= C_l H_l W_l \\; , \\; \\; m(\\bm{w}_l) = C_l C_{l-1} S_l^2}\n\\]\nand the word-length is again denoted as $\\mathit{T}$.\nWe define as $\\hat{\\alpha}_l = \\begin{cases} 1 & \\mbox{if } \\alpha_l > 0 \\\\ 0 & \\mbox{if } \\alpha_l = 0  \\end{cases}$ the mask that determines whether the weight adaptation for this layer is necessary or not.\n\nLet us first look at the forward path. \nThe necessary dynamic memory is about $\\max_{0 \\leq l \\leq L} \\{ m(\\bm{x}_l) \\}$ words. \nThe amount of MAC operations is $\\sum_{1 \\leq l \\leq L} H_l W_l m(\\bm{w}_l)$. \n\nThe backward path needs only to be evaluated until we reach the first layer where we require the computation of the gradients. \nWe define $l_\\mathit{min} = \\min \\{ l \\, | \\, \\hat{\\alpha}_{l} = 1 \\}$. \nFor the calculation of the partial derivatives of the activations we need dynamic memory of $\\max_{l_{\\mathit{min}} \\leq l \\leq L} \\{ m(\\bm{x}_l) \\} + \\frac{1}{\\mathit{T}} \\sum_{l_\\mathit{min} \\leq l \\leq L} m(\\bm{x}_l)$ words where the last term is due to storing the derivatives of the ReLU operations.\nWe need about $\\sum_{l_\\mathit{min} + 1 \\leq l \\leq L} H_{l-1} W_{l-1} m(\\bm{w}_l) $ MAC operations.\n\nThe second contribution of the backward path is for computing the weight gradients. \nThe memory and computation demand of the scales will be neglected as they are much smaller than other contributions. \nWe can determine the necessary dynamic memory as $\\max_{1 \\leq l \\leq L} \\{ \\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l \\mu^{\\mathrm{bw}}_l m(\\bm{w}_l)\\} + \\sum_{1 \\leq l \\leq L} \\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l m(\\bm{x}_{l-1})$, and we need $\\sum_{1 \\leq l \\leq L}  \\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l \\mu^{\\mathrm{bw}}_l H_l W_l m(\\bm{w}_l)$  MAC operations. \n\nConsidering all necessary dynamic memory with memory reuse for an adaptation step, we get an estimation of memory in words\n\\begin{multline}\n\\label{eq:memoryAll}\n\\medop{\\max_{0 \\leq l \\leq L} \\{ m(\\bm{x}_l) \\}  + \\sum_{1 \\leq l \\leq L} \\hat{\\alpha}_l m(\\bm{w}_l) +} \\\\ \\medop{+ \\sum_{1 \\leq l \\leq L} \\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l m(\\bm{x}_{l-1}) + \\frac{1}{\\mathit{T}} \\sum_{l_\\mathit{min} \\leq l \\leq L} m(\\bm{x}_l)}\n\\end{multline}\nif we accumulate the weight gradients before doing an SGD step and re-use some memory during back-propagation. More elaborate memory re-use can be used to slightly sharpen the bounds without a major improvement.\nFor conventional training, each parameter is in 32-bit floating point format, \\ie one word corresponds to 32-bit. As discussed in \\secref{sec:background}, we only consider max-pooling and ReLU-styled activation as the $\\sigma$ function. The wordlength $T$ in \\equref{eq:memoryAll} is set as 16 for max-pooling , and 32 for ReLU-styled activation.\nOne can see that under the typical assumptions for network parameters, the above memory requirement in words is dominated by \n\\begin{equation}\n\\label{eq:memorySimple}\n\\medop{\\sum_{1 \\leq l \\leq L} \\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l m(\\bm{x}_{l-1})}\n\\end{equation}\nThe necessary storage between the forward and backward path is reduced proportionally to $\\mu^{\\mathrm{fw}}_l$ with factor $m(\\bm{x}_{l-1})$. \n\nFinally, the amount of MAC computations can be estimated as \n\\begin{equation}\n\\label{eq:MAC}\n\\medop{\\sum_{1 \\leq l \\leq L} H_l W_l m(\\bm{w}_l) ( 1 + \n\\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l \\mu^{\\mathrm{bw}}_l) + \n\\sum_{l_\\mathit{min} \\leq l \\leq L} H_{l-1} W_{l-1} m(\\bm{w}_l)}\n\\end{equation}\nwhile neglecting lower order terms. Here it is important to note that all terms are of similar order. The approach used in the paper does not determine a trade-off between computation and memory, but reduces the amount of MAC operations. This reduction is less than the reduction in required dynamic memory. \n\n\n"
                }
            },
            "section 9": {
                "name": "Other Experiments",
                "content": "\n\\label{app:other}\n\n",
                "subsection 9.1": {
                    "name": "Pooling \\& Normalization",
                    "content": "\n\\label{app:pool}\n\nIn this section, we test the backbone network with different types of pooling and normalization. \nWithout further notations in the following experiments, we meta-train our ``4Conv'' backbone on MiniImageNet with full batch sizes, and conduct few-shot learning with gradient accumulation with a batch size of 1, as in \\secref{sec:evaluation}.\nHere, we report the results with the original ``MAML'' method \\cite{bib:ICML17:Finn} in \\tabref{tab:pool}. \nClearly, the discrepancy of batch statistics between meta-training phase and few-shot learning phase causes a large accuracy loss in batch normalization layers.\nBatch normalization works only if few-shot learning uses full batch sizes, \\ie without gradient accumulation, which however does not fit in our memory-constrained scenarios (see \\secref{sec:gradacc}).\nIn addition, max-pooling performs better than average-pooling.\nWe thus use group normalization and max-pooling in our backbone model, see \\secref{sec:evaluation}.\n\n\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Sample Batch Size",
                    "content": "\n\\label{app:batchsize}\n\nIn this section, we show the effects brought from different sample batch sizes.\nDuring few-shot learning phase, gradient accumulation is applied to fit in different on-device memory constraints. \nWe report the accuracy when adopting different sample batch sizes in gradient accumulation. \nAlthough group normalization eliminates the variance of batch statistics, adopting different batch sizes may still result in diverse performance due to the batch-averaged scores in meta attention.\nThe results in \\tabref{tab:batchsize} show that different batch sizes yield a similar accuracy level, which indicates that our meta attention module is relatively robust to batch sizes.\n\n\n\n\n"
                },
                "subsection 9.3": {
                    "name": "y",
                    "content": "\n\\label{app:sparse}\n\nOur meta attention modules take $\\bm{x}_{l-1}$ and $\\bm{g}(\\bm{y}_l)$ as inputs, and output attention scores which are used to create sparse $\\bm{g}(\\bm{w}_l)$.\nHowever, applying the resulted sparse attention scores on $\\bm{x}_{l-1}$ and $\\bm{g}(\\bm{y}_l)$ can also bring memory and computation benefits, as discussed in \\secref{sec:overview}.\nWe conduct the ablations when multiplying attention scores $\\bm{\\gamma}_l^{\\mathrm{fw}}$ and $\\bm{\\gamma}_l^{\\mathrm{bw}}$ on $\\bm{g}(\\bm{w}_l)$ (also the one used in the main text), or on $\\bm{x}_{l-1}$ and $\\bm{g}(\\bm{y}_{l})$ respectively.\nThe results in \\tabref{tab:sparse} show that a channel-wise sparse $\\bm{x}_{l-1}$ hugely degrades the performance, in comparison to only imposing sparsity on $\\bm{g}(\\bm{w}_l)$ while using a dense $\\bm{x}_{l-1}$ in the forward pass.  \nIn addition, directly adopting a sparse $\\bm{g}(\\bm{y}_{l})$ in backpropagation may even cause non-convergence in few-shot learning. \nWe think this is due to the fact that the error accumulates along the propagation when imposing sparsity on $\\bm{x}_{l-1}$ or $\\bm{g}(\\bm{y}_{l})$.\n\n\n\n"
                }
            }
        },
        "tables": {
            "tab:examples": "\\begin{table*}[t]\n    \\centering\n    \\caption{The static memory, the peak dynamic memory, and the total computation (GMACs $=10^9$MACs) of inference and adaptation for sample applications. For image classification we use batch size $=25$. For robot locomotion we use rollouts $=20$, horizon $=200$; each sample corresponds to a rollouted episode, and the case for an observation is reported in brackets. The calculation is based on Appendix \\ref{app:analysis}.}\n    \\label{tab:examples}\n    \\vspace{-0.2em}\n    \\footnotesize\n    \\begin{tabular}{llcccccc}\n    \t\\toprule\n    \t\\multirow{2}{*}{Application}    & \\multirow{2}{*}{Model / Benchmark} & \\multicolumn{2}{c}{Static Memory (MB)} & \\multicolumn{2}{c}{Peak Dynamic Memory (MB)} & \\multicolumn{2}{c}{GMACs}\\\\\n    \t\\cmidrule(lr){3-4} \\cmidrule(lr){5-6} \\cmidrule(lr){7-8} & & Model & Sample & Inference & Adaptation & Inference & Adaptation \\\\\n    \t\\midrule\n    \tImage Classification & 4Conv \\cite{bib:ICML17:Finn} / MiniImageNet \\cite{bib:NIPS16:Vinyals} & $0.13$ & $0.53$ & $0.90$ & $48.33$ & $0.72$ & $1.96$ \\\\\n    \tImage Classification & ResNet12 \\cite{bib:NIPS18:Oreshkin} / MiniImageNet \\cite{bib:NIPS16:Vinyals} & $32.0$ & $0.53$ & $3.61$ & $370.44$ & $62.08$ & $185.42$ \\\\\n    \tRobot Locomotion & MLP \\cite{bib:ICML17:Finn} / MuJoCo \\cite{bib:IROS12:Todorov} & $0.05$ & $0.016 (0.00008)$ & $0.08 (0.0004)$ & $3.72$  & $0.05$ & $0.15$ \\\\\n    \t\\bottomrule\n    \\end{tabular}\n\\end{table*}",
            "tab:image": "\\begin{table*}[t]\n    \\centering\n    \\caption{Few-shot image classification results on 4Conv and ResNet12. All methods are meta-trained on MiniImageNet, and are few-shot learned on the reported datasets: MiniImageNet, TieredImageNet, and CUB (denoted by Mini, Tiered, and CUB in the table). The total computation (\\# GMACs) and the peak memory (MB) during few-shot learning are reported based on the theoretical analysis in Appendix~\\ref{app:analysis}. }\n    \\label{tab:image}\n    \\footnotesize\n    \\begin{tabular}{llcccccccccc}\n        \\toprule\n        \\multicolumn{2}{l}{\\multirow{2}{*}{Benchmarks}}                                 & \\multicolumn{5}{c}{5-way 1-shot}                                                      & \\multicolumn{5}{c}{5-way 5-shot}                                                      \\\\\n                                                                                        \\cmidrule(lr){3-7}                                                                      \\cmidrule(lr){8-12}\n                                    &                                                   & Mini   & Tiered & CUB                                 & Mini      & Mini              & Mini   & Tiered & CUB                                 & Mini      & Mini              \\\\\n                                    &                                                   & \\multicolumn{3}{c}{Accuracy}                          & GMAC      & Memory            & \\multicolumn{3}{c}{Accuracy}                          & GMAC      & Memory            \\\\\n        \\midrule\n        \\multirow{6}{*}{4Conv}      & MAML \\cite{bib:ICML17:Finn}                       & 46.2\\% & 51.4\\% & 39.7\\%                              & 0.39      & 2.06              & 61.4\\% & 66.5\\% & 55.6\\%                              & 1.96      & 2.06              \\\\\n                                    & ANIL \\cite{bib:ICLR20:Raghu}                      & 46.4\\% & 51.5\\% & 39.2\\%                              & 0.14      & 0.92              & 60.6\\% & 64.5\\% & 54.2\\%                              & 0.72      & 0.92              \\\\\n                                    & BOIL \\cite{bib:ICLR21:Oh}                         & 44.7\\% & 51.3\\% & 42.3\\%                              & 0.39      & 2.05              & 60.5\\% & 65.3\\% & 58.3\\%                              & 1.96      & 2.05              \\\\\n                                    & MAML++ \\cite{bib:ICLR19:Antreas}                  & 48.2\\% & 53.2\\% & \\textbf{43.2\\%}                     & 0.39      & 2.06              & 63.7\\% & \\textbf{68.5\\%} & 59.1\\%                     & 1.96      & 2.06              \\\\\n                                    & \\sysname (\\ref{sec:LR})                           & 47.1\\% & 52.3\\% & 41.8\\%                              & 0.16      & 1.00              & 62.9\\% & 68.3\\% & 59.3\\%                              & 1.34      & 1.09              \\\\\n                                    & \\sysname (\\ref{sec:LR}+\\ref{sec:attention})       & \\textbf{48.8\\%} & \\textbf{53.9\\%} & 42.6\\%            & 0.15      & 0.99              & \\textbf{65.0\\%} & \\textbf{68.5\\%} & \\textbf{60.2\\%}   & 1.11      & 1.04              \\\\\n        \\midrule\n        \\multirow{6}{*}{ResNet12}   & MAML \\cite{bib:ICML17:Finn}                       & 51.7\\% & 57.4\\% & 41.3\\%                              & 37.08     & 54.69             & 64.7\\% & 69.6\\% & 53.8\\%                              & 185.42    & 54.69             \\\\\n                                    & ANIL \\cite{bib:ICLR20:Raghu}                      & 50.3\\% & 56.7\\% & 40.6\\%                              & 12.42     & 3.62              & 62.3\\% & 68.7\\% & 54.0\\%                              & 62.08     & 3.62              \\\\\n                                    & BOIL \\cite{bib:ICLR21:Oh}                         & 42.7\\% & 47.7\\% & 44.2\\%                              & 37.08     & 54.69             & 53.6\\% & 59.8\\% & 53.7\\%                              & 185.42    & 54.69             \\\\\n                                    & MAML++ \\cite{bib:ICLR19:Antreas}                  & 53.1\\% & 58.6\\% & 45.1\\%                              & 37.08     & 54.69             & 68.6\\% & \\textbf{73.4\\%} & 63.9\\%                     & 185.42    & 54.69             \\\\\n                                    & \\sysname (\\ref{sec:LR})                           & 51.8\\% & 58.3\\% & 40.6\\%                              & 25.84     & 17.66             & 68.8\\% & 72.6\\% & 65.9\\%                              & 124.15    & 18.95             \\\\\n                                    & \\sysname (\\ref{sec:LR}+\\ref{sec:attention})       & \\textbf{53.6\\%} & \\textbf{59.4\\%} & \\textbf{45.4\\%}   & 24.02     & 16.01             & \\textbf{69.7\\%} & 73.3\\% & \\textbf{66.6\\%}            & 116.79    & 17.17             \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table*}",
            "tab:mlp": "\\begin{table}[t]\n    \\centering\n    \\caption{Few-shot reinforcement learning results on 2D navigation and robot locomotion (larger return means better). A MLP with two hidden layers of size 100 is used as the policy model. The total computation (\\# GMACs) and the peak memory (MB) during few-shot learning are reported based on the theoretical analysis in Appendix~\\ref{app:analysis}.}\n    \\label{tab:mlp}\n    \\footnotesize\n    \\begin{tabular}{lcccccc}\n        \\toprule\n        \\multirow{2}{*}{Benchmarks}                     & \\multicolumn{3}{c}{20 Rollouts}               & \\multicolumn{3}{c}{20 Rollouts}           \\\\\n                                                        \\cmidrule(lr){2-4}                              \\cmidrule(lr){5-7}                             \n                                                        & \\multicolumn{3}{c}{Half-Cheetah Velocity}     & \\multicolumn{3}{c}{2D Navigation}         \\\\\n                                            \n                                                        & Return            & GMAC  & Memory            & Return            & GMAC  & Memory        \\\\\n        \\midrule\n        MAML \\cite{bib:ICML17:Finn}                     & -82.2             & 0.15  & 0.24              & -13.3             & 0.12  & 0.21          \\\\\n        ANIL \\cite{bib:ICLR20:Raghu}                    & -78.8             & 0.06  & 0.09              & -13.8             & 0.04  & 0.08          \\\\\n        BOIL \\cite{bib:ICLR21:Oh}                       & -76.4             & 0.15  & 0.23              & -12.4             & 0.12  & 0.21          \\\\\n        MAML++ \\cite{bib:ICLR19:Antreas}                & -69.6             & 0.15  & 0.24              & -17.6             & 0.12  & 0.21          \\\\\n        p-Meta (\\ref{sec:LR})                           & -65.5             & 0.11  & 0.12              & \\textbf{-11.2}    & 0.09  & 0.09          \\\\\n        p-Meta (\\ref{sec:LR}+\\ref{sec:attention})       & \\textbf{-64.0}    & 0.11  & 0.11              & -11.8             & 0.09  & 0.09          \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:ablation": "\\begin{table}[t]\n    \\centering\n    \\caption{Ablation results of meta attention on 4Conv.}\n    \\label{tab:ablation}\n    \\footnotesize\n    \\begin{tabular}{cccccccc}\n        \\toprule\n        \\multicolumn{2}{c}{$\\rho$}                          & \\multicolumn{3}{c}{5-way 1-shot}                      & \\multicolumn{3}{c}{5-way 5-shot}              \\\\\n        \\cmidrule(lr){1-2}                                  \\cmidrule(lr){3-5}                                      \\cmidrule(lr){6-8}\n        fw      & bw                                        & Mini   & Tiered & CUB                                 & Mini   & Tiered & CUB                         \\\\\n        \\midrule\n        x       & x                                         & 47.1\\% & 52.3\\% & 41.8\\%                              & 62.9\\% & 68.3\\% & 59.3\\%                      \\\\\n        0       & x                                         & 48.1\\% & 53.2\\% & 41.7\\%                              & 64.1\\% & 68.4\\% & 59.0\\%                      \\\\\n        x       & 0                                         & 47.8\\% & 53.1\\% & 40.9\\%                              & 63.9\\% & 68.5\\% & 60.0\\%                      \\\\\n        0       & 0                                         & \\textbf{49.0\\%} & \\textbf{54.2\\%} & \\textbf{43.1\\%}   & 64.5\\% & \\textbf{69.2\\%} & \\textbf{60.2\\%}    \\\\\n        0       & 0.3                                       & 48.5\\% & 53.4\\% & 42.2\\%                              & 64.7\\% & 68.2\\% & 59.3\\%                      \\\\\n        0.3     & 0                                         & 48.8\\% & 53.9\\% & 42.6\\%                              & \\textbf{65.0\\%} & 68.5\\% & \\textbf{60.2\\%}    \\\\\n        0.3     & 0.3                                       & 48.7\\% & 53.7\\% & 42.3\\%                              & 64.5\\% & 68.3\\% & 59.5\\%                      \\\\\n        0.5     & 0.5                                       & 48.2\\% & 53.4\\% & 42.7\\%                              & 64.8\\% & 68.1\\% & 59.1\\%                      \\\\\n        \\multicolumn{2}{c}{$\\bm{\\alpha}^{\\mathrm{Ch}}$}     & 47.8\\% & 52.8\\% & 41.0\\%                              & 63.6\\% & 68.1\\% & 58.1\\%                      \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n        \\footnotesize\n        \\item\n        x: no forward/backward meta attention, \\ie $\\bm{\\gamma}_l^{\\mathrm{fw}}=1$ or $\\bm{\\gamma}_l^{\\mathrm{bw}}=1$.\n        \\item\n        $\\bm{\\alpha}^{\\mathrm{Ch}}$: introducing an input- and output-channel-wise inner step sizes $\\bm{\\alpha}^{\\mathrm{Ch}}$ per layer. We use $\\bm{\\alpha}\\cdot\\bm{\\alpha}^{\\mathrm{Ch}}$ as the overall inner step sizes. $\\bm{\\alpha}^{\\mathrm{Ch}}$ is meta-trained as $\\bm{\\alpha}$ without imposing sparsity.\n    \\end{tablenotes}\n\\end{table}",
            "tab:pool": "\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\caption{Comparison between different pooling and normalization layers.}\n    \\label{tab:pool}\n    \\begin{tabular}{ccccc}\n        \\toprule\n        \\multicolumn{2}{c}{4Conv}       & \\multicolumn{3}{c}{5-way 1-shot}      \\\\\n        \\cmidrule(lr){1-2}              \\cmidrule(lr){3-5}                              \n        Pooling     & Norm.             & Mini   & Tiered & CUB                 \\\\\n        \\midrule\n        Average     & Batch             & 25.3\\% & 27.2\\% & 26.1\\%              \\\\\n        Average     & Group             & 45.8\\% & 50.3\\% & 40.2\\%              \\\\\n        Max         & Batch             & 27.6\\% & 28.9\\% & 26.5\\%              \\\\\n        Max         & Group             & 46.2\\% & 51.4\\% & 39.9\\%              \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:batchsize": "\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\caption{Ablation results of sample batch sizes.}\n    \\label{tab:batchsize}\n    \\begin{tabular}{lcccccc}\n        \\toprule\n                                    & \\multicolumn{3}{c}{5-way 1-shot}              & \\multicolumn{3}{c}{5-way 5-shot}              \\\\\n                                    \\cmidrule(lr){2-4}                              \\cmidrule(lr){5-7}\n        Batch Size                  & 1      & 2      & 5                           & 1      & 5      & 25                          \\\\\n        \\midrule\n        Mini                        & 48.8\\% & 48.7\\% & 48.3\\%                      & 65.0\\% & 65.1\\% & 64.7\\%                      \\\\\n        Tiered                      & 53.9\\% & 53.6\\% & 54.3\\%                      & 68.5\\% & 68.9\\% & 68.1\\%                      \\\\\n        CUB                         & 42.6\\% & 42.1\\% & 42.4\\%                      & 60.2\\% & 59.5\\% & 60.6\\%                      \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:sparse": "\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\caption{Ablation results of sparse $\\bm{x}_{l-1}$ and sparse $\\bm{g}(\\bm{y}_l)$.}\n    \\label{tab:sparse}\n    \\begin{tabular}{cccccccc}\n        \\toprule\n        \\multicolumn{2}{c}{$\\rho=0.3$}                  & \\multicolumn{3}{c}{5-way 1-shot}              & \\multicolumn{3}{c}{5-way 5-shot}              \\\\\n        \\cmidrule(lr){1-2}                              \\cmidrule(lr){3-5}                              \\cmidrule(lr){6-8}\n        fw                  & bw                        & Mini   & Tiered & CUB                         & Mini   & Tiered & CUB                         \\\\\n        \\midrule\n        x                   & x                         & 47.1\\% & 52.3\\% & 41.8\\%                      & 62.9\\% & 68.3\\% & 59.3\\%                      \\\\\n        $\\bm{g}(\\bm{w}_l)$  & x                         & 48.2\\% & 53.6\\% & 41.2\\%                      & 63.6\\% & 69.0\\% & 59.0\\%                      \\\\\n        $\\bm{x}_{l-1}$      & x                         & 37.4\\% & 37.9\\% & 35.4\\%                      & 47.9\\% & 49.3\\% & 42.5\\%                      \\\\\n        x                   & $\\bm{g}(\\bm{w}_l)$        & 48.0\\% & 53.0\\% & 42.6\\%                      & 64.0\\% & 67.8\\% & 59.9\\%                      \\\\\n        x                   & $\\bm{g}(\\bm{y}_{l})$      & 22.8\\% & 21.1\\% & 20.6\\%                      & 20.7\\% & 21.0\\% & 20.4\\%                      \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n        \\footnotesize\n        \\item\n        x: no forward/backward (sparse) meta attention, \\ie $\\bm{\\gamma}_l^{\\mathrm{fw}}=1$ or $\\bm{\\gamma}_l^{\\mathrm{bw}}=1$.\n    \\end{tablenotes}\n\\end{table}"
        },
        "figures": {
            "fig:meta": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.42\\textwidth]{figure/FSL.pdf} \n  \\caption{Meta learning and few-shot learning (FSL) in the context of on-device adaptation. The backbone $F(\\bm{w})$ is meta-trained into $F(\\bm{w}^\\mathrm{meta})$ on the cloud and is deployed to IoT devices to adapt to unseen tasks as $F(\\bm{w}^\\mathrm{new})$ via FSL.} \\label{fig:meta}\n  \\vspace{-2em}\n\\end{figure}",
            "fig:layer": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.4\\textwidth]{figure/typical_layer.png} \n  \\caption{A typical layer $l$ in DNNs. $\\bm{x}_{l-1}$ is the input tensor; $\\bm{x}_l$ is the output tensor, also the input tensor of layer $l+1$; $\\bm{y}_l$ is the intermediate tensor; $\\bm{w}_l$ is the weight tensor. }\n\\label{fig:layer}\n\\end{figure}",
            "fig:metaattention": "\\begin{figure}[t]\n\\vspace{-0.5em}\n  \\centering\n  \\includegraphics[width=0.5\\textwidth]{figure/metaatten.pdf} \n  \\vspace{-2.5em}\n  \\caption{Meta attention of layer $l$ during meta-training. The blue blocks correspond to tensors; the orange blocks correspond to computation units with parameters, and the green ones without. Each column of a tensor corresponds to one channel. The input tensor $\\bm{x}_{l-1}$ has 4 channels; the output tensor $\\bm{y}_{l}$ has 6 channels. The other dimensions (\\eg height, width and batch) are omitted here. The green block with $*$ stands for the operations involved to compute $\\bm{g}(\\bm{w}_l)$. In order to compute the gradients of the parameters in meta attention, \\ie $\\bm{w}_l^{\\mathrm{fw}}$ and $\\bm{w}_l^{\\mathrm{bw}}$, the full dense gradients $\\bm{g}(\\bm{w}_l)$ are computed during meta-training, and then are masked by $\\bm{\\gamma}_l$. An example meta attention module for a \\texttt{conv} layer is shown in the upper part. $B$ denotes the batch size. The newly added blocks related to the inference attention in \\cite{bib:CVPR20:Chen} are marked with solid lines.}\n\\label{fig:metaattention}\n\\end{figure}",
            "fig:metaattention_fewshot": "\\begin{figure}[t]\n\\vspace{-0.5em}\n  \\centering\n  \\includegraphics[width=0.5\\textwidth]{figure/metaatten_fewshot.pdf} \n  \\vspace{-2em}\n  \\caption{Meta attention of layer $l$ during on-device few-shot learning. Note that ``Forward'' part and ``Backward'' part are the same as \\figref{fig:metaattention}, which are omitted for simplicity. Meta attention modules are not optimized during few-shot learning, thus are expressed as parameter-free functions $h^{\\mathrm{fw}}$ and $h^{\\mathrm{bw}}$. The input $\\bm{x}_{l-1}$ stored during forward path is a sparse re-weighted tensor. }\n\\label{fig:metaattention_fewshot}\n\\end{figure}",
            "fig:ratio": "\\begin{figure*}[t]\n\\vspace{-0.5em}\n  \\centering\n  \\includegraphics[width=0.99\\textwidth]{figure/sparsity.pdf} \n  \\vspace{-0.5em}\n  \\caption{\\textit{Left (1:5):} Layer-wise updating ratios (mean $\\pm$ standard deviation) in each updating step. Note that the ratio of updated weights is determined by both static layer-wise inner step sizes $\\alpha_{1:L}^{1:K}$ and the dynamic meta attention scores $\\bm{\\gamma}_{1:L}$. The layer with an updating ratio of 0 means its $\\alpha=0$. \\textit{Right:} Cosine similarity (mean $\\pm$ standard deviation) of $\\bm{\\gamma}_{1:L}$ between random pair of data samples. The results are reported in step 1, because all samples are fed into the same initial model in step 1. }\n\\label{fig:ratio}\n\\end{figure*}"
        },
        "equations": {
            "eq:eq:maml_inner": "\\begin{equation}\\label{eq:maml_inner}\n    \\medop{\\bm{w}^{i,k} = \\bm{w}^{i,k-1}-\\alpha\\nabla_{\\bm{w}}~\\ell\\left(\\bm{w}^{i,k-1};\\mathcal{S}^i\\right)\\quad\\mathrm{for}~k=1\\cdots K}\n\\end{equation}",
            "eq:eq:maml_outer": "\\begin{equation}\\label{eq:maml_outer}\n    \\medop{\\bm{w} \\leftarrow \\bm{w}-\\beta \\nabla_{\\bm{w}} \\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)}\n\\end{equation}",
            "eq:eq:fsl": "\\begin{equation}\\label{eq:fsl}\n    \\medop{\\bm{w}^{\\mathrm{new},k} = \\bm{w}^{\\mathrm{new},k-1}-\\alpha\\nabla_{\\bm{w}^{\\mathrm{new}}}~\\ell\\left(\\bm{w}^{\\mathrm{new},k-1};\\mathcal{S}^{\\mathrm{new}}\\right)}\n\\end{equation}",
            "eq:eq:fsl_s": "\\begin{equation}\\label{eq:fsl_s}\n    \\medop{\\bm{w} \\leftarrow \\bm{w}-\\alpha \\bm{g}(\\bm{w})}\n\\end{equation}",
            "eq:1": "\\begin{equation}\n\\label{eq:MemoryContrib}\n\\medop{\\sum_{1 \\leq l \\leq L} m(\\bm{x}_{l-1})}\n\\end{equation}",
            "eq:2": "\\begin{equation*}\n\\medop{\\sum_{1 \\leq l \\leq L} \\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l m(\\bm{x}_{l-1})}\n\\end{equation*}",
            "eq:eq:inner_alpha": "\\begin{equation}\\label{eq:inner_alpha}\n    \\medop{\\bm{w}_l^{i,k} = \\bm{w}_l^{i,k-1}-\\alpha^k_l\\nabla_{\\bm{w}_l}~\\ell\\left(\\bm{w}^{i,k-1}_{1:L};\\mathcal{S}^i\\right)}\n\\end{equation}",
            "eq:eq:outer_alpha": "\\begin{equation}\\label{eq:outer_alpha}\n    \\medop{\\bm{w} \\leftarrow \\bm{w}-\\beta \\nabla_{\\bm{w}} \\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)}\n\\end{equation}",
            "eq:eq:alpha": "\\begin{equation}\\label{eq:alpha}\n    \\medop{\\bm{\\alpha} \\leftarrow \\bm{\\alpha}-\\beta \\nabla_{\\bm{\\alpha}} \\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)}\n\\end{equation}",
            "eq:eq:regularization": "\\begin{equation}\\label{eq:regularization}\n    \\medop{\\bm{\\alpha} \\leftarrow \\bm{\\alpha}-\\beta \\nabla_{\\bm{\\alpha}} (\\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)+\\lambda \\sum_{l,k} m(\\bm{x}_{l-1})\\cdot|\\alpha_l^k|)}\n\\end{equation}",
            "eq:eq:atten_fw": "\\begin{equation}\\label{eq:atten_fw}\n    \\medop{\\bm{\\gamma}^{\\mathrm{fw}}_{l} = h(\\bm{w}^{\\mathrm{fw}}_l;\\bm{x}_{l-1})\\in\\mathbb{R}^{C_{l-1}\\times 1 \\times 1}}\n\\end{equation}",
            "eq:eq:atten_bw": "\\begin{equation}\\label{eq:atten_bw}\n    \\medop{\\bm{\\gamma}^{\\mathrm{bw}}_{l} = h(\\bm{w}^{\\mathrm{bw}}_l;\\bm{g}(\\bm{y}_l))\\in\\mathbb{R}^{C_{l}\\times 1 \\times 1}}\n\\end{equation}",
            "eq:eq:atten_fwbw": "\\begin{equation}\\label{eq:atten_fwbw}\n    \\medop{\\gamma_{l,ba11} = \\gamma^{\\mathrm{fw}}_{l,a11} \\cdot \\gamma^{\\mathrm{bw}}_{l,b11}} \n\\end{equation}",
            "eq:eq:inner_incr": "\\begin{equation}\\label{eq:inner_incr}\n    \\medop{\\bm{\\gamma}_l^{i,k}\\odot\\nabla_{\\bm{w}_l}~\\ell\\left(\\bm{w}^{i,k-1}_{1:L};\\mathcal{S}^i\\right)}\n\\end{equation}",
            "eq:eq:inner_atten": "\\begin{equation}\\label{eq:inner_atten}\n    \\medop{\\bm{w}_l^{i,k} = \\bm{w}_l^{i,k-1}-\\alpha^k_l(\\bm{\\gamma}_l^{i,k}\\odot\\nabla_{\\bm{w}_l}~\\ell\\left(\\bm{w}^{i,k-1}_{1:L};\\mathcal{S}^i\\right))}\n\\end{equation}",
            "eq:eq:outer_atten": "\\begin{equation}\\label{eq:outer_atten}\n    \\medop{\\bm{w}^{\\mathrm{atten}} \\leftarrow \\bm{w}^{\\mathrm{atten}}-\\beta \\nabla_{\\bm{w}^{\\mathrm{atten}}} \\sum_i \\ell\\left(\\bm{w}^{i};\\mathcal{Q}^i\\right)}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:memorySimple}\n\\medop{\\sum_{1 \\leq l \\leq L} \\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l m(\\bm{x}_{l-1})}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:MAC}\n\\medop{\\sum_{1 \\leq l \\leq L} H_l W_l m(\\bm{w}_l) ( 1 + \n\\hat{\\alpha}_l \\mu^{\\mathrm{fw}}_l \\mu^{\\mathrm{bw}}_l) + \n\\sum_{l_\\mathit{min} \\leq l \\leq L} H_{l-1} W_{l-1} m(\\bm{w}_l)}\n\\end{equation}"
        }
    }
}