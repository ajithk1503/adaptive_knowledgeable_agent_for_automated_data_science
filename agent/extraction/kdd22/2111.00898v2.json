{
    "meta_info": {
        "title": "Availability Attacks Create Shortcuts",
        "abstract": "Availability attacks, which poison the training data with imperceptible\nperturbations, can make the data \\emph{not exploitable} by machine learning\nalgorithms so as to prevent unauthorized use of data. In this work, we\ninvestigate why these perturbations work in principle. We are the first to\nunveil an important population property of the perturbations of these attacks:\nthey are almost \\textbf{linearly separable} when assigned with the target\nlabels of the corresponding samples, which hence can work as \\emph{shortcuts}\nfor the learning objective. We further verify that linear separability is\nindeed the workhorse for availability attacks. We synthesize linearly-separable\nperturbations as attacks and show that they are as powerful as the deliberately\ncrafted attacks. Moreover, such synthetic perturbations are much easier to\ngenerate. For example, previous attacks need dozens of hours to generate\nperturbations for ImageNet while our algorithm only needs several seconds. Our\nfinding also suggests that the \\emph{shortcut learning} is more widely present\nthan previously believed as deep models would rely on shortcuts even if they\nare of an imperceptible scale and mixed together with the normal features. Our\nsource code is published at\n\\url{https://github.com/dayu11/Availability-Attacks-Create-Shortcuts}.",
        "author": "Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu",
        "link": "http://arxiv.org/abs/2111.00898v2",
        "category": [
            "cs.LG",
            "cs.CR"
        ],
        "additionl_info": "Published as a research track paper at KDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n\n% \\begin{wrapfigure}{R}{0.49\\textwidth}\n% \\centering\n% \\includegraphics[width=0.45\\textwidth]{imgs/setting_cat.png}\n% \\caption{An illustration of  availability attacks.   }\n% \\label{fig:unlearnable_setting}\n% \\end{wrapfigure}\n\n\n\n%The attacker perturbs training data to reduce test-time performance.\n\nSharing personal data online has become an important lifestyle for many people. Despite big datasets crawled from the Internet keep advancing the state-of-the-art deep models \\citep{devlin2018bert, he2020momentum, chen2020big}, there are increasing concerns about the unauthorized use of personal data \\citep{hill2019photos, prabhu2020large, carlini2020extracting}. For instance, a private company  has collected more than three billion face images to build commercial face recognition models without acquiring any user consent \\citep{hill2020secretive}. To address those concerns, many data poisoning attacks have been proposed to prevent  data from being learned by unauthorized deep models \\citep{feng2019learning,shen2019tensorclog,huang2021unlearnable,yuan2021neural,fowl2021preventing, fowl2021adversarial,tao2021provable}. They  add imperceptible perturbations to the training data so that the model cannot learn much information from the data and the model accuracy on unseen data is arbitrarily bad. These attacks make the data not \\emph{available/exploitable} by machine learning models and are known as \\emph{availability attack} \\citep{biggio2018wild}. We give an illustration of this type of attack in Figure~\\ref{fig:unlearnable_setting}. \n\n% In this work, the word `availability' denotes the adversarial targets at \\textbf{all} test examples. We note that the word is also used to denote the adversarial does not have a specific target class \\citep{munoz2017towards}.\n\n% \\begin{figure}[h]\n% \\centering\n%   \\includegraphics[width=0.6\\linewidth]{imgs/setting_cat.png}\n%   \\caption{An illustration of  availability attacks.}\n%   \\label{fig:unlearnable_setting}\n% \\end{figure}\n\n%builds a generative model that generates perturbations that maximize the loss of a target model on unseen data and then trains the target model with the perturbed training data for several steps. It conducts the above bi-level optimizations alternatively for multiple rounds and outputs the final perturbation as poisoning attack.\n\n\n\nIn literature, there are roughly three methods to construct the availability attack against deep neural networks. The first method generates the perturbations as the solution of a bi-level optimization problem \\citep{biggio2012poisoning,feng2019learning,fowl2021preventing,yuan2021neural}. The bi-level optimization problem updates the perturbations to minimize the loss  on perturbed data while maximizing the loss on clean data. \n\n%The bi-level optimization problem requires models trained on perturbed data to have  the maximum loss on unseen data. \nSecondly, \\citet{huang2021unlearnable} conceive a simpler poisoning attack called \\emph{error-minimizing noise}, where the perturbation on training data is crafted by minimizing the training loss. The intuition is that if the perturbation can reduce the loss to zero,  then there is nothing left for backpropagation in the regular training procedure. Recently,  \\citet{nakkiran2019discussion} and \\citet{fowl2021adversarial} point out that \\emph{error-maximizing noises}, which are commonly used as adversarial examples, can serve as an availability attack as well.  Despite these quite different approaches,  all of them are powerful availability attacks. Intrigued by this observation, we ask the following question: \n\n\n\\begin{center}\n\\emph{What is the underlying workhorse for  availability attacks against deep neural networks?}\n\\end{center}\n\n\n\nTo answer this question, we first take a close look at the perturbations of existing attacks. We visualize the perturbations of several availability attacks via two-dimensional T-SNEs \\citep{van2008visualizing} in Figure~\\ref{fig:tsne} and Figure~\\ref{fig:add_tsne} in Appendix~\\ref{apdx:more_tsne}. The experimental setup is depicted in Section~\\ref{sec:draw_tsne}.  Surprisingly, the perturbations with the same class label are well clustered, suggesting that the perturbations would be nearly \\textbf{linearly separable} in the original high-dimensional space. We confirm this by fitting the perturbations with linear models. The perturbations are assigned with the labels of their target examples. It turns out that simple logistic regression models can fit the perturbations of four representative attacks with $>90\\%$ training accuracy. \n\nConceptually, the reason why current availability attacks work could be that the imperceptible perturbations create a kind of \\emph{shortcut}. They are so simple, i.e., linearly-sperarable, that the deep models tend to rely on them to make prediction while ignoring the true features. This extends the concept of existing shortcut learning \\citep{geirhos2020shortcut}, which is often referred to that deep models tend to rely on some natural features, correlated with labels but not causal ones, e.g., ``grass'' is a shortcut for recognizing ``cow'' in natural images \\citep{beery2018recognition,geirhos2020shortcut}. In contrast, we expose a more explicit form of shortcuts and one may create such shortcut intentionally. \n\nTo further confirm that creating shortcut is also sufficient (not only necessary) for a successful availability attack, we reverse the above procedure: synthesizing some simple linearly-separable perturbations to see if they can serve as availability attacks. Specifically, we first generate some initial synthetic perturbations via a method in  \\citet{guyon2003design} and then add a new post-processing procedure  so that the synthetic perturbations remain effective when  data augmentations are applied. Extensive experiments on benchmark datasets and models demonstrate that synthetic perturbations can be as powerful as existing availability attacks. Notably, generating synthetic perturbations is significantly easier and cheaper than existing attacks as it does not require solving any optimization problems.  For example, recent attacks need dozens of hours to generate perturbations  for the ImageNet data, while generating synthetic perturbations only needs several seconds. This finding reveals that one can instantiate a successful availability attack by just creating shortcuts. \n\nThis paper unveils that deep learning models would overwhelmingly rely on spurious shortcuts even though the shortcuts are scaled down to an imperceptible magnitude, which exposes a fundamental vulnerability of deep models.  Our contributions  are summarized as follows:\n\\begin{itemize}\n    \\item We reveal that the perturbations of several existing availability attacks are (almost) linearly separable. \n    \\item We propose to use synthetic shortcuts to perform availability attack, which is much cheaper and easier to conduct.\n    %\\item Our findings and experiments explain how various of  availability attacks work. \n    \\item We link availability attacks with   shortcut learning and greatly widen the understanding of shortcuts in deep learning.  \n\\end{itemize}\n\n\n% To answer this question, we first take a close look at the perturbations of existing attacks. We visualize the perturbations of several availability attacks via two-dimensional T-SNEs \\citep{van2008visualizing} in Figure~\\ref{fig:tsne} and Figure~\\ref{fig:add_tsne} in Appendix~\\ref{apdx:more_tsne}. The experimental setup is depicted in Section~\\ref{sec:draw_tsne}.  Surprisingly, the perturbations with the same class label are well clustered, suggesting that the perturbations would be nearly \\textbf{linearly separable} in the original high-dimensional space. We confirm this by fitting the perturbations with linear models. The perturbations are assigned with the labels of their target examples. It turns out that simple logistic regression models can fit the perturbations of four representative attacks with $>90\\%$ training accuracy. This finding suggests that linearly-separable perturbations may be the key for existing availability attacks to succeed.\n\n\n% To further confirm that the linear separability is a sufficient (not only necessary) condition, we reverse the above procedure: synthesizing some simple linearly-separable perturbations to see if they can serve as availability attacks. Specifically, we first generate some initial synthetic perturbations via a method in  \\citet{guyon2003design} and then add a new post-processing procedure  so that the synthetic perturbations remain effective when  data augmentations are applied. Extensive experiments on benchmark datasets and models demonstrate that synthetic perturbations can be as powerful as existing availability attacks. Notably, generating synthetic perturbations is significantly easier and cheaper than existing attacks as it does not require solving any optimization problems.  For example, recent attacks need dozens of hours to generate perturbations  for the ImageNet data, while generating synthetic perturbations only needs several seconds. This finding reveals that using  linearly-separable perturbations is indeed the workhorse to the success of state-of-the-art availability attacks.\n\n% The above finding conceptually links the availability attacks with \\emph{shortcut learning} \\citep{geirhos2020shortcut}. Shortcut learning stands for the behavior that deep models tend to rely on features that  do not generalize on realistic test data. Such features are referred to as shortcuts. With this concept, the perturbations of availability attacks are also shortcuts. We note that the shortcuts in previous works are usually part of natural data, which are somehow heuristic, e.g., ``grass'' is a shortcut for recognizing ``cow'' in natural images \\citep{beery2018recognition,geirhos2020shortcut}. In this work, we expose a more explicit form of shortcuts and discuss extensively how to construct such shortcuts. We unveil that deep learning models would overwhelmingly rely on spurious shortcuts even though the shortcuts are scaled down to an imperceptible magnitude. This finding exposes a fundamental vulnerability of deep models and hence may be of  independent interest to the community.\n\n\n\n\n% Our contributions  are summarized as follows:\n% \\begin{itemize}\n%     \\item We reveal that the perturbations of several existing availability attacks are (almost) linearly separable. \n%     \\item We propose to use synthetic shortcuts to perform availability attack, which is much cheaper and easier to conduct.\n%     %\\item Our findings and experiments explain how various of  availability attacks work. \n%     \\item We link availability attacks with   shortcut learning and greatly widen the understanding of shortcuts in deep learning.  \n% \\end{itemize}\n\n\n\n\n",
                "subsection 1.1": {
                    "name": "Related Work",
                    "content": "\n\n% \\emph{Adversarial examples}, which are usually crafted through \\emph{error-maximizing noises}, are \n\n\n% \\citet{ilyas2019adversarial}  argue that the common adversarial noises (error-maximizing noises) are also features of the data. They show those features are are imperceptible to human but can be learned by models to improve test performance (so called \\emph{non-robust} features). Our work explains the difference between error-minimizing noise and error-maximizing noise. We show the error-minimizing noises are artificial simple features instead of inherent features of the clean data. \n\n\n%is highly effective against linear models as it is more involved with the training process.\n\n% in terms of the generation process\n\n% In this paper, we show that error-minimizing noise and error-maximizing noise have different working principles. Error-minimizing noise does not \n\n%Error-maximizing noise is also generated via an iterative optimization process but with an opposite objective: it is optimized to maximize the loss.\n\n%targets at trained models, i.e., \n\n% In this paper, we show that error-minimizing noise and error-maximizing noise have different working principles. Error-minimizing noise does not \n\n% Error-maximizing noise \n\n% Contrary to error-minimizing noise, error-maximizing noise is optimized to  maximize the loss.\n\n\n% %Adversarial examples \n\n% Not the same as adversarial examples. Exp. on linear models\n\n% %2. adversarial attacks, target at trained models.  \n\n% %investigate the  adversarial noise that is used to maximize the training loss. They\n\n% \\citet{ilyas2019adversarial}  argue that the common adversarial noises (error-maximizing noises) are also features of the data. They show those features are are imperceptible to human but can be learned by models to improve test performance (so called \\emph{non-robust} features). Our work explains the difference between error-minimizing noise and error-maximizing noise. We show the error-minimizing noises are artificial simple features instead of inherent features of the clean data. \n\n\\textbf{Data poisoning.} In general, data poisoning attacks perturb training data to intentionally cause some malfunctions of the target model \\citep{biggio2018wild,goldblum2020dataset,schwarzschild2021just}.  A common class of poisoning attacks aims to cause test-time error on some given samples \\citep{shan2020fawkes,geiping2020witches,cherepanova2021lowkey,zhang2021data} or on all unseen samples \\citep{feng2019learning,shen2019tensorclog,huang2021unlearnable,yuan2021neural,fowl2021preventing, fowl2021adversarial}. The latter attacks are also known as availability attacks \\citep{barreno2010security}.  In this work, we investigate  and reveal the workhorse of availability attacks. We show that the perturbations of these availability attacks are (almost) linearly separable. We further confirm that synthesised  linearly-separable perturbations can perform strong attacks.\n\n\nBackdoor attacks are another type of data poisoning attack that perturbs training data so that the attacker can  manipulate the target model's output with a designed trigger \\citep{shafahi2018poison,nguyen2020input,saha2020hidden,tang2020embarrassingly,nguyen2021wanet,doan2021backdoor}. The perturbations of backdoor attacks have two major differences compared to those of availability attacks. Firstly, the perturbations of availability attacks are  imperceptible. Secondly, advanced availability attacks   use a different perturbation for every sample while a backdoor trigger is applied to multiple samples. In the threat model of availability attacks, the data are probably crowdsourced. It is preferred to use different perturbations for different samples in such a setting. Otherwise, the adversarial learner can remove the perturbation from all related samples if any of the poisoned images are leaked.\n\n\n\n\n\n\n% There are defenses against availability attacks.  \\citet{huang2021unlearnable} show training with advanced data augmentation methods improves the test performance.  Another defense is to train the target model with \\emph{adversarial training} \\citep{madry2018towards}, which currently achieves the best performance \\citep{huang2021unlearnable,fowl2021adversarial,tao2021provable}. Recently, \\citet{radiya2021data} challenge the security of poisoning attacks from another view. They argue that unless existing attacks can fool all future defenses, they cannot protect the data well because the adversary can simply save the perturbed data and wait for better defenses\\footnote{\\revise{\\citet{radiya2021data} only verify this is achievable for targeted poisoning attacks \\citep{shan2020fawkes,cherepanova2021lowkey} though the general idea may also apply to availability attacks.}}. In this work, we propose  to use pre-trained models to defend availability attacks.   Our experiments demonstrate that using pre-trained models is a  powerful defense against availability attacks. We note that  \\citet{cina2021backdoor} have explored running poisoning attacks against pre-trained feature extractors. Nonetheless, they focus on backdoor attacks and do not advocate using pre-trained models as a defense.\n\n% \\revise{We note that although \\citet{cina2021backdoor} run backdoor attacks against pre-trained feature extractors, they do not show pre-trained models can be used as a denfense. Our understanding that pre-trained feature extractors can ignore shortcuts also explain their observations.\n\n\n%For example, when the training data are perturbed with error-minimizing noises, \\emph{Mixup} \\citep{zhang2017mixup} improves the test accuracy from $19.9\\%$ to $58.5\\%$.\n\n\n% once the perturbed data are released, they must be able to fool all future defenses. This makes  \n\n\n%\\textbf{Defenses against availability attacks.}\n\n\\textbf{Shortcut learning.} Recently, the community has realized that deep models may rely on shortcuts to make decisions \\citep{beery2018recognition,niven2019probing,geirhos2020shortcut}. Shortcuts are spurious features that are correlated with target labels but do not generalize on test data.  \\citet{beery2018recognition} show that a deep model would fail to recognize cows when the grass background is removed, suggesting that the model takes ``grass'' as a shortcut for ``cow''. \\citet{niven2019probing} show that large language models use the strong correlation between some simple words and labels to make decisions, instead of trying to understand the sentence. For instance, the word ``not'' is directly used to predict negative labels.  In this work, we show  shortcut learning exists more widely  than previously believed. Our experiments in Section~\\ref{sec:syntehtic_noise} demonstrate that deep models only pick shortcuts even if the shortcuts are scaled down to an imperceptible magnitude and mixed together with normal features.  These experiments reveal another form of shortcut learning, which has been unconsciously exploited by availability poisoning attacks. There also exist other synthesized datasets that offer a stratification of features \\citep{hermann2020shapes,shah2020pitfalls}. Those synthetic data contain shortcuts that can not be used as perturbations as they are visible and affect the normal data utility. For example, \\citet{shah2020pitfalls} generate synthetic data by vertically concatenating images from the MNIST and CIFAR-10 datasets.\n\n%Apart from heuristic observations, \\citet{shah2020pitfalls} give a specific construction of shortcuts. They vertically concatenate the images from MNIST and CIFAR-10 datasets and show that deep models only use MNIST features to make decisions.\n \n% \\textbf{Adversarial Training} \n\n\n"
                },
                "subsection 1.2": {
                    "name": "Notations",
                    "content": "\n\n\n\nWe use bold lowercase letters, e.g., $\\vv$, and bold capital letters, e.g., $\\mM$, to denote vectors and matrices, respectively. The $L_{p}$ norm of a vector $\\vv$ is denoted by $\\|\\vv\\|_{p}$. A sample consists of a feature vector $\\vx$ and label $y$. We use $\\sD$ to denote a dataset that is sampled from some data distribution $\\mathcal{D}$. In this paper, we focus on classification tasks. The classification loss of a  model $f$ on a given sample is denoted by $\\ell(f(\\vx), y)$.\n\n\n\n"
                }
            },
            "section 2": {
                "name": "Availability Attacks Use Linearly-Separable Perturbations",
                "content": "\n\\label{sec:why}\n\nIn this section, we investigate the common characteristic of existing availability attacks. First, We briefly introduce three different approaches to construct availability attacks. Then, we  visualize the perturbations of advanced attacks with two-dimensional T-SNEs. The plots suggest that the perturbations of all three kinds of attacks are some `easy' features. Finally, we verify that the perturbations of these attacks are almost linearly separable by fitting them with simple models.\n\n\n",
                "subsection 2.1": {
                    "name": "Three Types Of Availability Attacks",
                    "content": "\n\\label{sec:three_attacks}\n\n",
                    "subsubsection 2.1.1": {
                        "name": "The Alternative Optimization Approach",
                        "content": "\n\n\nWe first introduce the alternative optimization approach to generate perturbations for availability attacks. It solves the following bi-level objective,  \n%\\huishuai{We'd better explicitly write out how to generate the perturbation on the training data.}\n\n\\begin{equation}\n\\begin{aligned}\n\\label{eq:bi_level}\n&\\argmax_{\\{\\boldsymbol\\delta\\}\\in \\Delta} \\mathbb{E}_{(\\vx,y)\\sim \\mathcal{D}}[\\ell(f^{*}(\\vx), y)],\\\\\n&\\text{s.t. } f^{*}\\in \\argmin_{f} \\sum_{(\\vx,y)\\in\\sD} \\ell(f(\\vx+\\boldsymbol\\delta), y),\n\\end{aligned}\n\\end{equation}\nwhere $\\boldsymbol\\delta$ is a sample-wise perturbation and $\\Delta$ is a constraint set for  perturbations.  The two formulas in Equation~\\ref{eq:bi_level} directly reflect the goal of availability attacks. Specifically, the optimal solution on perturbed data (specified by the second formula) should have the largest loss on clean data (specified by the first formula). The constraint set $\\Delta$ is  set to make the perturbations imperceptible, e.g., a small $L_{p}$ norm ball. \n\n% the first objective tries to maximize the loss of a given model $f^{*}$ on clean data and the second objective requires $f^{*}$ to be the optimal solution on perturbed data.\n\n% to minimize the loss on perturbed data. \\huishuai{One thing is not clear about this formulation (1): the delta should be generated for the training data, but the first optimization is for samples of the population. Moreover, what is the output of the algorithm? Should it be  $\\argmax_{\\delta \\in \\Delta}$?}\n\nDirectly solving Equation~(\\ref{eq:bi_level}) is intractable for deep neural networks.  Recent works have designed multiple approximate solutions \\citep{feng2019learning,fowl2021preventing,yuan2021neural}. \\citet{feng2019learning} use multiple rounds of optimization to generate perturbations. At each round, they first approximately optimize the second objective by  updating a surrogate target model on perturbed data for a few steps. Then they approximately optimize the first objective by updating a generator for a few steps. The outputs of the generator are used as perturbations. Another example is the Neural Tangent Generalization Attacks (NTGAs) in \\citet{yuan2021neural}. They approximately optimize the bi-level objective based on the recent development of Neural Tangent Kernels \\citep{jacot2018neural}.\n\n\n\n"
                    },
                    "subsubsection 2.1.2": {
                        "name": "The Error-minimizing Noise",
                        "content": "\n\n\\citet{huang2021unlearnable} propose another bi-level objective to generate perturbations. Instead of solving Equation~(\\ref{eq:bi_level}), they use the following objective, \n\n%\\huishuai{We'd better explicitly write out how to generate the perturbation on the training data.}\n\n\\begin{equation}\n\\begin{aligned}\n\\label{eq:error_min}\n\\argmin_{\\{\\boldsymbol\\delta\\}\\in\\Delta} \\mathbb{E}_{(\\vx,y)\\sim\\mathbb{D}} [\\min_{f}\\ell(f(\\vx+\\boldsymbol\\delta), y)].\n\\end{aligned}\n\\end{equation}\n\nThe perturbations are intentionally optimized to reduce the training loss. The main motivation is that if the training loss is zero, then the target model will have nothing to learn from the data because there is nothing to backpropagate. A randomly initialized model is used as a surrogate of the target model. They also use multiple rounds of optimization to generate perturbations. At each round, they first train the surrogate model for a few steps to minimize the loss on perturbed data. Then they optimize the perturbations to also minimize the loss of the surrogate model. They repeat the above process until the loss on perturbed data is smaller than a pre-defined threshold. \n\n"
                    },
                    "subsubsection 2.1.3": {
                        "name": "Adversarial Examples",
                        "content": "\n\nInstead of using bi-level objectives, \\citet{fowl2021adversarial} show that the common objectives of adversarial examples are sufficient to generate powerful data poisoning perturbations. They use both  untargeted (the first objective) and targeted adversarial examples (the second objective),\n\n\\begin{equation}\n\\begin{aligned}\n\\label{eq:error_max}\n&\\argmax_{\\{\\boldsymbol\\delta\\}\\in \\Delta} \\mathbb{E}_{(\\vx,y)\\sim\\mathbb{D}} \\left[\\ell\\left(f\\left(\\vx+\\boldsymbol\\delta\\right), y\\right)\\right],\\\\\n&\\argmin_{\\{\\boldsymbol\\delta\\}\\in \\Delta} \\mathbb{E}_{(\\vx,y)\\sim\\mathbb{D}} \\left[\\ell\\left(f\\left(\\vx+\\boldsymbol\\delta\\right), y'\\right)\\right],\n\\end{aligned}\n\\end{equation}\nwhere $y'\\neq y$ is an incorrect label and $f$ is a trained model. Surprisingly, \\citet{fowl2021adversarial} demonstrate that these simple objectives can generate perturbations that achieve state-of-the-art attack performance.\n\n"
                    }
                },
                "subsection 2.2": {
                    "name": "Visualizing The Perturbations",
                    "content": "\n\\label{sec:draw_tsne}\n\nAlthough the three approaches in Section~\\ref{sec:three_attacks} have different objectives, they all manage to perform powerful attacks. Intrigued by this observation, we try to find out whether there is a common pattern among different types of availability attacks. If so, the common pattern may be the underlying workhorse for availability attacks.\n\n\nTo find such a common pattern, we first visualize different types of perturbations, including DeepConfuse \\citep{feng2019learning}, NTGA \\citep{yuan2021neural}, error-minimizing noises \\citep{huang2021unlearnable}, and adversarial examples \\citep{fowl2021adversarial}. We compute their two-dimensional t-SNEs \\citep{van2008visualizing}. These four attacks achieve advanced attack performance and cover all the three approaches in Section 2.1.  We use their official implementations to generate perturbations. Detailed configurations are in Appendix~\\ref{apdx:exp_details}.\n\n\n\n\n% two  We generate perturbations for the CIFAR-10 dataset. The target model is  ResNet18 \\citep{he2016deep}. The perturbations are are projected into a $L_{\\infty}$-norm ball with radius $\\epsilon=8/255$. For the classic approach, we choose two representative attacks DeepConfuse \\citep{feng2019learning} and NTGA \\citep{yuan2021neural}.    For DeepConfuse, NTGA\\footnote{\\url{https://github.com/lionelmessi6410/ntga}}, and error-minimizing noise\\footnote{\\url{https://github.com/HanxunH/Unlearnable-Examples}}, we choose the perturbations that achieve best attack performance on CIFAR-10 from official implementations.  For error-maximizing noise, we generate the perturbations following the implementation details described in \\citet{fowl2021adversarial}.  \n\nThe two-dimensional t-SNEs of DeepConfuse and error-minimizing noises   are shown in Figure~\\ref{fig:tsne}. The plots of NTGA and adversarial examples are presented in Figure~\\ref{fig:add_tsne} of Appendix~\\ref{apdx:more_tsne} due to the space limit.   Surprisingly, for all the attacks considered, the perturbations for the same class are well clustered, suggesting that even linear models can classify them well. For comparison purposes, we also compute the t-SNEs of the clean data. As shown in Figure~\\ref{fig:tsne}, in contrast with the t-SNEs, the projections of different classes of the clean data are mixed together, which indicates that they require a complex neural network to be correctly classified. This observation suggests that using linearly-separable perturbations may be the common pattern among availability attacks.\n\n"
                },
                "subsection 2.3": {
                    "name": "Availability Attacks Use Linearly-Separable Perturbations",
                    "content": "\n\nTo quantify the `linear separability' of the perturbations, we fit the perturbations with simple models and report the training accuracy. The perturbations are labeled with the labels of the corresponding target examples. The simple models include linear models and two-layer neural networks. Details can be found in Appendix~\\ref{apdx:exp_details}. We  choose  one-layer (linear) and two-layer neural networks because they are easy to implement in existing deep learning frameworks \\citep{abadi2016tensorflow,paszke2019pytorch}. We note there are other choices that also fit the task, e.g.,  support-vector machines.\n\n\n\n% \\begin{table}[t]\n% \\caption{Training accuracy (in \\%) of simple models on the perturbations of poisoning attacks.}\n\n% \\label{tbl:linear_acc}\n% \\centering\n% \\renewcommand{\\arraystretch}{1.25}\n% \\begin{tabular}{c|cccc}\n\n\n% \\hline\n% Model & DeepConfuse & NTGA &   Error-minimizing & Error-maximizing  \\\\ \\hline\n% Linear &    &   &    &  91.5  \\\\ \\hline\n% Two-layer &   &   &   &  99.9 \\\\ \\hline \n% \\end{tabular}\n% \\end{table} \n\n \n\nThe results are presented in Table~\\ref{tbl:linear_acc}.  Compared to the results on clean data,  simple models can easily fit the perturbations. On all attacks considered, linear models achieve more than $90\\%$ training accuracy and two-layer neural networks achieve nearly $100\\%$ training accuracy. These results confirm that the perturbations of advanced availability attacks are all (almost) linearly separable. \n\n\nExisting attacks against deep neural networks all use ReLU activation functions in their crafting models. Deep models with ReLU activation functions are known to learn piecewise linear functions in input space \\citep{arora2016understanding}. Therefore, it is natural to wonder whether the linear separability is stemming from the property of ReLU.  In Section~\\ref{apdx:tanh}, we replace the ReLU layers with Tanh layers in the crafting models of adversarial examples and error-minimizing noises. It turns out that simple models still can easily fit the new perturbations, which demonstrates that the linear separability is an intrinsic property of these availability attacks rather than something associated with a specific network structure.  \n\n\n% We fit the new perturbations with simple models and report the results in Table~\\ref{tbl:linear_acc_tanh}. It turns out that the perturbations are still linearly separable. This suggests that the linear separability is not stemming from the property of ReLU.  \n\n%\\dayu{Delete the later sentence} These findings  provide an explanation of the working principle of availability attacks: they succeed by providing shortcuts to target models.\n\n\n"
                },
                "subsection 2.4": {
                    "name": "Connecting To Shortcut Learning",
                    "content": "\n\n%Shortcut learning summarizes a general phenomenon when any learning system makes decisions based on spurious features that are useful for training but do not generalize on test data, referred to as \\emph{shortcut features}.\n\nThe fact that the perturbations can be easily fitted by linear models naturally connects to a recent concept named shortcut learning \\citep{geirhos2020shortcut}. Shortcut learning summarizes a general phenomenon when any learning system makes decisions based on spurious features that do not generalize on realistic test data\\footnote{\\citet{geirhos2020shortcut} use a more specific definition of shortcuts. They denote shortcuts as those features that do not generalize on out-of-distribution (OOD) data. We note that poisoning attacks would change the distribution of training data and hence make the clean test data `OOD' with respect to the trained model.}. Shortcut features have been found in different fields. For vision tasks, \\citet{beery2018recognition} show deep models fail to recognize cows when the grass background is removed from images, suggesting the grass background is a shortcut for predicting cows.  In the field of natural language processing, \\citet{niven2019probing} show language models use the strong correlation between some simple words and labels to make decisions, instead of really understanding the data. \n\n\n% \\revise{In \\citet{geirhos2020shortcut}, shortcut features are those that fail to generalize on realistic test data, which are often out-of-distribution (OOD) compared to the training data.} \n\n\n%With the presence of shortcut learning, it seems reasonable to postulate that availability attacks succeed by creating shortcuts to the target model. \\revise{The perturbations of existing attacks change the distribution of the training data}\n\n%\\revise{Poisoning attacks are intentionally designed to change the distribution of the training data so that   }\n\n% \\begin{wrapfigure}{R}{0.45\\textwidth}\n% \\centering\n% \\includegraphics[width=0.4\\textwidth]{imgs/shortcut_illustration.png}\n% \\caption{An illustration of  the shortcut learning phenomenon in this paper.   }\n% \\label{fig:shortcut}\n% \\end{wrapfigure}\n\n\n\nWith the presence of shortcut learning, it seems reasonable to postulate that the perturbations of existing attacks succeed by creating shortcuts to the target model. We give an illustration in Figure~\\ref{fig:shortcut}. A major difference between the perturbations of poisoning attacks and existing shortcut features is that the perturbations are of an imperceptible scale and mixed together with useful features. Since there is no direct evidence to show deep models will take this kind of shortcuts, %the observation that the perturbations are linearly separable may only be some superficial results of the underlying root cause. \nin the next section, we design experiments to confirm the postulated explanation.  We synthesize imperceptible and linearly-separable perturbations and show deep models are very vulnerable to such synthetic shortcuts.\n\n\n\n%There is no direct evidence to show models will take this type of shortcuts.\n\n% a reason explanation of the underlying workhorse of availability attacks is \n\n\n% Intuitively, ......., \n\n% valid,\n\n% but the shortcuts in previous works are very obvious\n\n\n\n% and may be used as shortcuts. to confirm we can performn strong attacks by providing shortcuts to target models.\n\n\n\n% suggest that the perturbations can be easily fitted by the simple models, e.g., small two-layer neural networks achieve $>99\\%$ training accuracy on all types of perturbations.\n\n\n\n\n\n% The projections of different classes of the original features are mixed together, which indicates that they require a complex neural network to be correctly classified. In contrast, error-minimizing noises from the same class are well clustered, supporting the observation that a linear model can classify them well. \n\n\n\n% We introduce some background knowledge in this section. First, we introduce  the problem setup of unlearnable examples. Then we introduce the error-minimizing noise in \\citet{huang2021unlearnable}. \n\n% \\subsection{Problem Statement}\n\n% We use \\emph{defender} to denote the data owner who want to protect the data from unauthorized learners. Let $\\mathbb{D}=\\{(\\vx_{i},y_{i})\\}_{i=1}^{n}$ be the clean training dataset containing $n$ feature and label pairs. The defender creates an unlearnable dataset $\\mathbb{D}_{u}$ that satisfies two requirements, 1) the examples in $\\mathbb{D}_{u}$ have small differences with those in $\\mathbb{D}$ so that $\\mathbb{D}_{u}$ is still usable by human; 2) models trained on $\\mathbb{D}_{u}$ have poor performance on clean data so that $\\mathbb{D}_{u}$ is not learnable by machine learning models.\n\n% % \\huishuai{1) the examples in $\\mathbb{D}_{u}$ have small differences with those in $\\mathbb{D}$ so that $\\mathbb{D}_{u}$ is still usable by human; 2) models trained on $\\mathbb{D}_{u}$ have poor performance on clean data so that $\\mathbb{D}_{u}$ is not learnable by machine learning models. }\n\n% With the first requirement, we can only perturb the data with imperceptible noises, i.e., \\[\\mathbb{D}_{u}=\\{(\\vx_{i}+\\boldsymbol\\delta_{i},y_{i})\\}_{i=1}^{n},\\;\\; \\norm{\\boldsymbol\\delta}_{p}\\leq \\epsilon,\\]\n% where $\\norm{\\cdot}_{p}$ is the $L_{p}$ norm and $\\epsilon$ is a small constant. Let $f_{\\theta}$ be a  model parametrized by $\\theta$, $\\ell(\\cdot,\\cdot)$ be the loss function, and \n% $\\theta_{*}=\\argmin_{\\theta} \\mathbb{E}_{(\\vx', y)\\sim \\mathbb{D}_{u}}\\ell(f_{\\theta}(\\vx'), y)$ be the parameters trained on $\\mathbb{D}_{u}$. The trained model $f_{\\theta_{*}}$  would have bad accuracy on clean test data because of the second requirement.\n\n\n%The unlearnable dataset $\\mathbb{D}_{u}$ is designed to have imperceptible difference with $\\mathbb{D}$. \n\n% introduce some background knowledge on the error-minimizing noise \n\n\n\n% \\subsection{Fitting }\n\n% In order to protect the data from unauthorized learners, unlearnable examples should satisfy two criteria. Firstly, the unlearnable example should have a small difference with  the corresponding clean example so that it is still recognizable by humans. Secondly, models trained on unlearnable examples should have low accuracy  on clean examples so that unknown third parties can not use unlearnable examples to train useful machine learning models. \n\n% %$\\sN=\\{\\boldsymbol\\delta_{i}\\}$  $\\sD_{s}=\\{\\boldsymbol\\vx_{i}\\}\\subset \\sD$\n\n\n% \\citet{huang2021unlearnable} use error-minimizing noise to craft unlearnable example. The noises for a subset of data  are generated through a bi-level optimization process: \n\n% \\begin{flalign}\n% \\label{eq:generate_em}\n% \\min_{\\theta} \\mathbb{E}_{(\\vx_{i},y_{i})\\sim\\mathbb{D}} [\\min_{\\{\\boldsymbol\\delta_{i}\\}}\\ell(f(\\vx_{i}+\\mathds{1}_{i}\\boldsymbol\\delta_{i}, y_{i}))], \\;\\;  \\text{s.t.} \\;\\; \\norm{\\boldsymbol\\delta_{i}}_{p}\\leq\\epsilon.\n% \\end{flalign}\n\n% In Eq~(\\ref{eq:generate_em}), $\\mathds{1}_{i}$ indicates whether the $i_{th}$ example needs to be unlearnable and $\\epsilon$ is a small $L_{p}$ norm bound to make the noise imperceptible. In \\citet{huang2021unlearnable}, every noise is optimized for multiple steps  after a certain number of updates on $\\theta$. This bi-level optimization process is repeated until the training error is smaller than a pre-defined threshold. Afterward, the noisy examples (unlearnable examples) are released to the public.  We note that the computational cost of generating error-minimizing noises is high because of this iterative  optimization process.\n\n\n% % Defenders need \n% % There are two prerequisites \n% %  There are two requirements on the defenders' ability when using error-minimizing noises. Firstly, defenders need access to  \n \n% %  extra data are needed for the outer optimization process. For example, we need access to data of different classes to run the outer optimization process even if we  only want to make the data of one specific class unlearnable. Secondly, the repeated bi-level optimization processes are computational expensive especially when the number of unlearnable examples is large. \n\n% % There are  \\emph{sample-wise} and \\emph{class-wise} error-minimizing noise in \\citet{huang2021unlearnable}. We  focus on the sample-wise error-minimizing noise. Sample-wise noise is different for every example while class-wise noise is the same for examples in the same class. Class-wise noise is more easily exposed than sample-wise noise. In Appendix~\\ref{apdx:efficient_defense}, we show that simply averaging the poisoned images can expose the class-wise noise.   Moreover, class-wise noise increases the risk of privacy breach as the exposure of any example leads to the exposure of the entire class. Class-wise noise also breaks the i.i.d. assumption of data.  The above disadvantages make  class-wise noise unsuitable for certain applications such as federated learning. Nonetheless, we show the proposed methods also can be used to generate class-wise noise in Appendix~\\ref{apdx:efficient_defense}. In the rest of the paper, the noises are sample-wise unless otherwise notified.\n\n% There are  \\emph{sample-wise} and \\emph{class-wise} error-minimizing noises in \\citet{huang2021unlearnable}.  Sample-wise noise is different for every example while class-wise noise is the same for examples in the same class. Class-wise noise is more easily exposed than sample-wise noise. In Appendix~\\ref{apdx:efficient_defense}, we show that simply averaging the poisoned images can expose the class-wise noise. Moreover, if the noise of any poisoned image is leaked, the unauthorized third party can easily make the entire class learnable again. Nonetheless, we show synthetic noise can also be used as class-wise unlearnable noise in Appendix~\\ref{apdx:efficient_defense}. In this paper, we  focus on the sample-wise error-minimizing noise and  in the following, the noises are sample-wise unless otherwise notified.\n\n"
                }
            },
            "section 3": {
                "name": "Linear Separability Is A Sufficient Condition For  Availability Attacks to Succeed",
                "content": "\n\\label{sec:syntehtic_noise}\n\nAlthough we have demonstrated that the perturbations of four advanced attacks are all almost linearly separable, it is  a bit early to claim that `linear separability' is the underlying working principle of availability poisoning attacks. Perturbations are linearly separable  may only be a necessary but not sufficient condition for poisoning attacks to succeed. In order to verify this postulated explanation, we use simple synthetic data  to serve as perturbations and compare their effectiveness with existing poisoning attacks. It turns out that the synthetic perturbations  are as powerful as advanced attacks.  \n\n%\\citet{hermann2020shapes,shah2020pitfalls} also\n% that using linearly separable perturbations is indeed the hidden working principle, we use simple synthetic data  to serve as perturbations. \n\n\nThe rest of this section is organized as follows. In Section~\\ref{subsec:generation}, we first give an algorithm for generating synthetic data as perturbations. In Section~\\ref{subsec:comparison}, we verify the effectiveness of synthetic perturbations on different models and datasets.\n\n\n",
                "subsection 3.1": {
                    "name": "Generating Synthetic Perturbations as Shortcuts",
                    "content": "\n\\label{subsec:generation}\n\n\nThe synthetic perturbations in this section are generated via two building blocks. In the first  block, we use a method in \\citet{guyon2003design} to generate samples from some normal distributions. In the second block, we transfer the  samples into the image format so that they can be applied  to benchmark vision tasks. We give the pseudocode in Algorithm~\\ref{alg:generation}.\n\n\nThe first building block  proceeds as follows. We first generate some points that are normally distributed  around the vertices of a hypercube. The points around the same vertex are assigned with the same label. Then for each class, we introduce  different covariance. Any two classes of the generated points can be easily classified by a hyperplane as long as the side length of the hypercube is reasonably large.\n\n\n\n\n\n\nIn the second building block, we pad each dimension of the sampled points and reshape them into two-directional images. The padding operation introduces local correlation into the synthetic images.  Local correlation is an inherent property of natural images. In Section~\\ref{apdx:no_pad}, we show the padding operation is necessary to make the synthetic  perturbations remain effective when data augmentation methods are applied.  \n\n\n\\begin{algorithm}\n\\caption{Generating Perturbations for Vision Datasets}\n   \\label{alg:generation}\n\\begin{algorithmic} [1]\n   \\STATE {\\bfseries Input:} number of classes $k$, number of examples in each class $\\{n_{i}\\}_{i=1}^{k}$, image size $(w,h)$,  patch size $p$, norm bound $\\epsilon$.\n   \n   \\medskip\n   \n   \\STATE Compute $w'=\\floor{w/p}+1$ and $h'=\\floor{h/p}+1$. \n   %\\huishuai{$m$ and $n$ are not guaranteed to be integers.}\n   \n   \\medskip\n   //\\textsl{ The first block: generate some initial data points.}\n   \n   \\STATE Create arrays \\{$\\mD^{(i)}\\in\\mathbb{R}^{n_{i}\\times w'h'}\\}_{i=1}^{k}$ that will contain points in $w'h'$-dimensional hypercube.\n   \n   \\FOR{$i=1$ {\\bfseries to} $k$} \n\t\t\t\\STATE Initialize $\\mD^{(i)}$ with samples from  $\\mathcal{N}(0,\\mI_{w'h'\\times w'h'})$. \n            \\STATE //\\textsl{ Introduce random covariance among columns.}\n\t\t\t\\STATE Uniformly sample the elements of $\\mA\\in\\mathbb{R}^{w'h'\\times w'h'}$ from $[-1,1]$.\n\t\t\t\\STATE Compute $\\mD^{(i)}=\\mD^{(i)}\\mA$.\n\t\t\t\\STATE Randomly choose an unused vertex and let $\\vc^{(i)}\\in\\mathbb{R}^{w'h'}$ be its coordinates.\n\t\t\t\\STATE //\\textsl{ Move the sampled points to the chosen vertex.}\n\t\t\t\\STATE Compute $\\mD^{(i)}=\\mD^{(i)}+\\vc^{(i)}$, i.e., $\\vc^{(i)}$ is added to each row of $\\mD^{(i)}$.\n\t\t\t\\STATE Assign the rows of $\\mD^{(i)}$ with label $i$. % \n   \\ENDFOR\n   \n   \\medskip\n   \n  //\\textsl{ The second block: duplicating each dimension to introduce local correlation.}\n  \\STATE Duplicate each dimension of the initial data points for $p^{2}$ times and reshape the results into two-dimensional $p\\times p$ patches.\n  \\STATE Put the patches together and take crops to generate synthetic noises with size $(w,h)$.\n  \n  \\medskip\n   //\\textsl{ Scale down the magnitude of synthetic data and harvesting perturbations.}\n  \\STATE Normalize each synthetic sample with $L_{2}$ norm bound $\\epsilon$.\n   \\STATE Add perturbations to clean images with the same labels.\n  \n\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n In Algorithm~\\ref{alg:generation}, the synthetic images are scaled down before being used as perturbations.  We visualize the synthetic perturbations and corresponding perturbed images in Figure~\\ref{fig:synthetic_images}. We also visualize the perturbations in \\citet{huang2021unlearnable} for a comparison. The details of perturbations can be found in Section~\\ref{subsec:comparison}. As shown in Figure~\\ref{fig:synthetic_images}, the synthetic perturbations do not affect data utility.\n \n\n\n \n%  We note that a small $L_{2}$ norm bound is used to normalize synthetic noises. This is different from the $L_{\\infty}$ norm bound used by normalizing error-minimizing noises.  We choose the $L_{2}$-norm bound to better keep the variation among the noises of different examples. The detailed configuration can be found in Section~\\ref{subsec:comparison}. \n\n\n\n\n\n% two types of noises and the corresponding unlearnable examples in  The synthetic noises are normalized with a $L_{2}$-norm bound $\\epsilon=\\sqrt{d}\\epsilon'$ where $d$ is the feature dimension and $\\epsilon'=8/255$. The error-minimizing noises are normalized with $L_{\\infty}$ bound $\\epsilon=8/255$. We use $L_{2}$ norm bound for synthetic noises to keep the variation among different images. The pixel value threshold and scaling down factor are both set as $0.5$ throughout the paper.  \n\n\n\n\n\n\n%we show the effectiveness of synthetic noises when only partial data are made unlearnable. We show synthetic noises are more robust against advanced data augmentation methods than error-minimizing noises. We also consider the cases when only partial data need to be unlearnable.\n\n% one class needed to be protected and \n\n% We demonstrate the effectiveness of the proposed algorithm with extensive experiments in Section~\\ref{subsec:effectiveness}. In Section~\\ref{subsec:face}, we consider an application scenario where the dataset contains face images and users want to protect their personal face images from being recognized by unauthorized models.\n\n% We first give an algorithm for generating unlearnable examples for\n\n% In Section~\\ref{}\n\n% We propose to use synthetic noises to generate unlearnable examples  \n\n% It is easy to \n\n\n\n%We note that the cost of Algorithm~\\ref{alg:generation} is very low as all operations have low computational cost.\n\n\n\n\n\n%\\citet{guyon2003design} generate $k$-class classification task \n\n\n\n\n% Our generation algorithm is based on the method proposed in \\citep{guyon2003design}. \n\n% 1. \n% 2. patches with small perturbation\n% 3. visualization\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Synthetic Perturbations Are Highly Effective as Availability Attacks",
                    "content": "\n\\label{subsec:comparison}\n\n%We adopt $L_{2}$-norm bound to keep the sample-wise variation among perturbations.\n\n\n% \\begin{wrapfigure}{R}{0.45\\textwidth}\n% \\centering\n% \\includegraphics[width=0.4\\textwidth]{imgs/resnet18_acc_cifar10.pdf}\n% \\caption{Training curves of ResNet-18 models on perturbed and clean data.}\n% \\label{fig:cifar10_trn_curve}\n% \\end{wrapfigure}\n\n\nNow we verify the effectiveness of synthetic perturbations and make comparisons with existing availability attacks. We perturb the entire training set following the main setup in previous works \\citep{feng2019learning,huang2021unlearnable,yuan2021neural,fowl2021adversarial}. That is, we synthesize a perturbation  for every training example. In Section~\\ref{subsec:singleclass} and~\\ref{apdx:different_percentage}, we show synthetic perturbations are still effective when only partial training data are poisoned. \n\n%We note that the perturbations are different for \n\n\n\n\n\n%The constraint bound is $L_{\\infty}$ for synthetic perturbations because we observe that the perturbations for the same class look very similar if we use $L_{\\infty}$ bound.\n\n%The perturbations in previous works are normalized by a $L_{\\infty}$ bound with $\\epsilon=8/255$.  We use  $\\epsilon'=6/255$. The patch size in Algorithm~\\ref{alg:pad} is set as $8$.\n\nWe use $L_{2}$-norm for synthetic perturbations to keep the sample-wise variation in the same class. We normalize the synthetic noises into a $L_{2}$-norm ball with radius $\\sqrt{d}\\epsilon'$, where $d$ is the dimension of the input. We evaluate synthetic perturbations on three benchmark datasets: SVHN \\citep{netzer2011reading}, CIFAR-10, CIFAR-100 \\citep{cifar}, and a subset of ImageNet \\cite{russakovsky2015imagenet}. Following the setup in \\citet{huang2021unlearnable}, we use the first 100 classes of the full dataset as the ImageNet subset.  The target model architectures include VGG \\citep{simonyan2014very}, ResNet \\citep{he2016deep}, and DenseNet \\citep{huang2017densely}.  We adopt standard  random cropping and flipping as data augmentation. The  hyperparameters for training are standard and can be found in Appendix~\\ref{apdx:exp_details}. We use $\\epsilon'=6/255$ for synthetic perturbations. The patch size in Algorithm~\\ref{alg:generation} is set as $8$. \n\n \n\nWe first compare synthetic perturbations with existing poisoning attacks. The comparisons are made on the CIFAR-10 dataset with ResNet-18 as the target model. We use the best-performing setup in their official implementations to generate perturbations. We present the comparison in Table~\\ref{tbl:compare}. Synthetic perturbations are as powerful as advanced poisoning attacks despite they are much easier to generate.\n\n\n\n\nThen we evaluate synthetic perturbations on different models and datasets. The test accuracy of target models is in Table~\\ref{tbl:different_datasets}. We also plot the training curves of target models on both clean and perturbed data in Figure~\\ref{fig:trn_curve}. The results in Table~\\ref{tbl:different_datasets} and Figure~\\ref{fig:trn_curve} further confirm the effectiveness of synthetic perturbations. \n\n\n\n\nWe note that generating synthetic perturbations is data irrelevant and only  takes several seconds using a single CPU core.  In contrast, existing attacks often need hours or even days to generate perturbations using GPUs. We compare the computational complexities of  synthetic perturbations and recent attacks in Section~\\ref{subsec:cost_comparison}.  \n\n% We perturb the CIFAR-10 dataset and choose the target model architecture as ResNet-18.  The training configurations of target models are standard and can be found in Appendix~\\ref{apdx:exp_details}. We plot training curves of the targeted model on both clean and perturbed data in Figure~\\ref{fig:cifar10_trn_curve}. We present the final accuracy and make comparisons with existing attacks in Table~\\ref{tbl:compare}.\n\n\n\n%We also evaluate synthetic perturbations on other benchmark  datasets (SVHN, CIFAR-100, and a subset of ImageNet) and models (VGG and DenseNet). Synthetic perturbations are highly effective across all datasets and models considered. We present the test accuracy and training curves in Appendix~\\ref{apdx:effect_synthetic}.\n\n% As shown in Table~\\ref{tbl:compare}, synthetic perturbations are as powerful as advanced poisoning attacks and reduce the test accuracy close to that of random guessing. The results in Table~\\ref{tbl:different_datasets} and Figure~\\ref{fig:trn_curve} further confirm the effectiveness of synthetic perturbations. Notably, generating synthetic perturbations is data irrelevant and only  takes several seconds using a single CPU core. We compare the computational complexities of  synthetic perturbations and recent attacks in Appendix~\\ref{apdx:synthetic_gaussian}.  \n\n\n\n%Notably, generating synthetic perturbations does not require any additional data or prior knowledge of target models. Moreover, synthetic perturbations are very cheap to generate.  For instance, generating synthetic perturbations for the entire training set only takes several seconds on a single CPU core while existing attacks could easily take hours on GPUs as they all use iterative optimization methods to solve their objectives. \n\n%\\huishuai{we may not need to emphasize the efficiency of the synthetic perturbation.}\n\nIn summary, our experiments demonstrate that using linearly-separable perturbations is indeed a sufficient condition for availability poisoning attacks to succeed. Moreover, these results also expose that deep models are very vulnerable to obscured shortcuts. This finding has two meanings to the community. First, it confirms that advanced availability poisoning attacks do succeed by providing shortcuts. Second, it  further exposes the shortcut learning problem, which is a fundamental vulnerability of deep models.\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Complexity Analysis",
                    "content": "\n\\label{subsec:cost_comparison}\n\n\nHere we give an analysis of the computational complexity of Algorithm~\\ref{alg:generation}. The complexity of generating synthetic perturbations is $\\mathcal{O}(nd/p^{2})$, where $n$ is the size of the dataset, $d$ is the dimension of clean data, and $p$ is the patch size. This complexity is mainly from introducing covariance into synthetic data (Line 6 in Algorithm~\\ref{alg:generation}). \n\nThe complexity of generating synthetic perturbations is significantly smaller than those of recent attacks. The complexity of running the algorithms in recent attacks is $\\mathcal{O}(nTL(dw+w^{2}))$, where $T$ is the number of iterations generating the poisons, $L$ is the network depth, and $w$ is the network width (for simplicity, we assume the network is of equal width). The term $nL(dw+w^{2})$ is the cost of one forward/backward pass. This complexity is strictly worse than that of generating synthetic perturbations.\n\n\nIn recent attacks, the number of iterations generating the poisons is usually large. For example, \\citet{huang2021unlearnable} use multiple gradient descent steps to solve the optimization problems in Eq~(\\ref{eq:error_min}). On the ImageNet dataset, they first run 100 SGD updates for the outer problem. Then they loop over every target example to optimize the inner problem. They run  20 SGD updates for each example. The above process is repeated until the training accuracy is larger than a pre-defined threshold. Another example is the attack in \\citet{fowl2021adversarial}. For each target example, they use 250 Projected Gradient Descent (PGD) steps to generate perturbations.\n\n\n% Although both synthetic noise and error-minimizing noise can be used to generate effective unlearnable examples, they have drastically different computational costs. Recall that generating error-minimizing noises requires solving  Eq~(\\ref{eq:generate_em}). At each round, \\citet{huang2021unlearnable} use multiple gradient descent steps to solve both the outer and inner optimization problems in Eq~(\\ref{eq:generate_em}). The optimization round is repeated until the training error is smaller than a pre-defined threshold. Take the ImageNet subset for an example, \\citet{huang2021unlearnable} first run 100 SGD updates for the outer problem. Then they loop over every example that needs to be unlearnable to optimize the inner problem. For each target example, they update the corresponding error-minimizing noise with 20 SGD updates in each round. They stop the generation process  when the training accuracy is larger than $99\\%$.  This generation process could be time-consuming especially when the image is high-resolution and the number of target examples is large.\n\n\n\n\n% .... For example...\n\n\n% Setup + result\n\nWe now give an empirical comparison. We measure the time costs of generating error-minimizing noises, adversarial examples, and synthetic perturbations. The device is a server with a single Tesla V100 GPU and an Intel Xeon E5-2667 CPU. We note that running Algorithm~\\ref{alg:generation} does not require a GPU. The time costs are tested on SVHN, CIFAR-10, and the ImageNet subset. The target model is ResNet18. We use the configurations described in \\citet{huang2021unlearnable} and \\citet{fowl2021adversarial} to generate error-minimizing noises and adversarial examples.  For synthetic perturbations, we use the same setting as that in Section~\\ref{subsec:comparison}. The time costs are reported in Table~\\ref{tbl:cost}. Generating synthetic perturbations is significantly cheaper than existing availability attacks.\n\n\n \n\n"
                }
            },
            "section 4": {
                "name": "Experiments under Different Settings",
                "content": "\n\n\n\n\n\n% \\subsection{Ablation Study}\n% \\label{subsec:ablation}\n\n\n\nHere we test synthetic  perturbations under different settings. We first consider two cases where not all the training data are poisoned. Although the main setting in previous works is to perturb the full training set, in practice we may only need to perturb part of the data. In the first case, we only apply Algorithm~\\ref{alg:generation} to some of the classes. In the second case, we perturb partial data that are randomly sampled from all the classes. Finally, we run experiments on a face dataset following the application scenario in \\citet{huang2021unlearnable}.\n\n% Now we consider the cases where not all the data are made unlearnable. In many practical datasets, some classes are more sensitive than  others, e.g., in medical datasets, the patients that are diagnosed with a certain disease may be more concerned about their data than healthy people. We first explore the effectiveness of Algorithm~\\ref{alg:generation}  when it is  applied  to some of the classes.    Then we test Algorithm~\\ref{alg:generation} on partial data that are randomly sampled from all the classes.\n\n",
                "subsection 4.1": {
                    "name": "Poisoning Some Classes of The Training Data",
                    "content": "\n\\label{subsec:singleclass}\n\n\n\n\n\n\n% the case where the partial data are from all classes.\n\n% that are needed to be unlearnable  (from all classes) are made unlearnable.\n\n\nIn many practical datasets, some classes are more sensitive than  others, e.g., in medical datasets, the patients that are diagnosed with a certain disease may be more concerned about their data than healthy people. We randomly sample some classes of the CIFAR-10 dataset and apply  Algorithm~\\ref{alg:generation} on all the examples of the sampled classes. After training, we report the test accuracy on clean classes and poisoned classes separately. \n\n% \\begin{table}[h]\n% \\small\n% \\centering\n% \\caption{Test accuracy (in \\%) on clean ($\\sC$) and unlearnable  ($\\sU$) classes. The numbers of unlearnable classes are 1, 3, and 5.}\n% \\label{tbl:some_class}\n% \\begin{tabular}{c|cc|cc|cc}\n% \\hline\n% \\hline\n% \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Method\\end{tabular}} &  \\multicolumn{2}{c|}{1}  &  \\multicolumn{2}{c|}{3} &  \\multicolumn{2}{c}{5}\\\\ \\cline{2-7}\n%   & $\\sC$ & $\\sU$ \n%   & $\\sC$ & $\\sU$\n%   & $\\sC$ & $\\sU$ \\\\ \\hline\n  \n%  EM & 94.6 & 2.4 & 93.9 & 1.1 & 93.8 & 3.1 \\\\\\hline  \n  \n% SYN & 94.7 & 2.7 & 94.0 & 0.6 & 93.2 & 2.9 \\\\\\hline \\hline\n  \n \n% \\end{tabular}\n% \\end{table}\n\n\n\nThe synthetic perturbations are generated in the same way as that in Section~\\ref{subsec:comparison}. For comparison, we also run experiments with error-minimizing noises using those generated in Section~\\ref{subsec:comparison}. The experiments are run on ResNet-18. The numbers of poisoned classes are 1, 3, and 5. The poisoned classes are randomly chosen and are the same for two types of noises. We report the results in  Table~\\ref{tbl:some_class}.  Algorithm~\\ref{alg:generation} is still highly effective when only some of the classes are poisoned.\n\n\n \n\n\n% We use the configuration in Section~\\ref{subsec:generation} to generate synthetic noises. \n\n\n% \\begin{table}[ht]\n% \\renewcommand{\\arraystretch}{1.25}\n% \\caption{Test accuracy (in \\%) of models trained on the clean dataset ($\\sD_{c}$), unlearnable dataset crafted via error-minimizing noises ($\\sD_{em}$), and unlearnable dataset crafted via synthetic noises ($\\sD_{syn}$). RN and DN denote ResNet and DenseNet, respectively. }\n\n% \\label{tbl:some_class}\n% \\centering\n% \\small\n% \\begin{threeparttable}\n% \\begin{tabular}{c|cc|cc}\n% \\hline\n% \\hline\n% \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Method\\end{tabular}} &  \\multicolumn{2}{c|}{\\textbf{One} perturbed class}  &  \\multicolumn{2}{c}{\\textbf{Three} perturbed classes}\\\\ \\cline{2-5}\n%   & Clean & Unlearnable \n%   & Clean & Unlearnabe \\\\ \\hline\n  \n%  EM & & & &  \\\\\\hline  \n  \n% SYN & & & &  \\\\\\hline \\hline\n  \n \n% \\end{tabular}\n% \\end{threeparttable}\n\n% \\end{table}\n\n% \\subsubsection{Different Unlearnable Percentages}\n% \\label{subsec:different_precentage}\n\n% Here we test the case where a given percentage of the training data is made unlearnable. Different from the setting in Section~\\ref{subsec:singleclass}, the unlearnable subset is randomly sampled from all the classes. The clean subset and the unlearnable subset are denoted by $\\sD_{c}$ and $\\sD_{u}$, respectively. For each $\\sD_{u}$, we train two models following the setup in \\citet{huang2021unlearnable}.  One model uses $\\sD_{u}\\cup\\sD_{c}$ as its training data and the other one only uses $\\sD_{c}$. The difference between the performances of those two models  represents how much information the former model gains from $\\sD_{u}$.\n\n% We test four different unlearnable percentages (from 20\\% to 80\\%) on the CIFAR-10 dataset. The experiments are run on  ResNet18 models. The configuration for generating unlearnable examples is the same as that in Section~\\ref{subsec:comparison}. The results are presented in Table~\\ref{tbl:different_percentage}. When the unlearnable subsets are in the training set, the performance gain of using the unlearnable subset crafted via synthetic noises is smaller than that of the subset crafted via error-minimizing noises. A smaller performance gain indicates less information the model learns from the unlearnable subset.\n\n\n% \\begin{table*}\n% \\centering\n% \\caption{Test accuracy (in \\%) with different unlearnable percentages. The clean subset is $\\sD_{c}$ and the unlearnable subset is $\\sD_{u}$. EM denotes error-minimizing noises and SYN denotes synthetic noises.}\n% \\renewcommand{\\arraystretch}{1.1}\n% \\label{tbl:different_percentage}\n% \\begin{adjustbox}{max width=\\textwidth}\n% \\begin{tabular}{c|clccccccccc}\n% \\hline \\hline\n% \\multirow{3}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Method\\end{tabular}}} & \\multicolumn{11}{c}{\\textbf{Unlearnable Percentages}} \\\\ \\cline{2-12} \n%  & \\multicolumn{2}{c|}{\\multirow{2}{*}{\\textbf{0\\%}}} & \\multicolumn{2}{c|}{\\textbf{20\\%}} & \\multicolumn{2}{c|}{\\textbf{40\\%}} & \\multicolumn{2}{c|}{\\textbf{60\\%}} & \\multicolumn{2}{c|}{\\textbf{80\\%}} & \\multirow{2}{*}{\\textbf{100\\%}} \\\\\n%  & \\multicolumn{2}{c|}{} & \\textbf{$\\sD_{c}\\cup\\sD_{u}$} & \\multicolumn{1}{c|}{$\\sD_{c}$} & \\textbf{$\\sD_{c}\\cup\\sD_{u}$} & \\multicolumn{1}{c|}{$\\sD_{c}$} & \\textbf{$\\sD_{c}\\cup\\sD_{u}$} & \\multicolumn{1}{c|}{$\\sD_{c}$} & \\textbf{$\\sD_{c}\\cup\\sD_{u}$} & \\multicolumn{1}{c|}{$\\sD_{c}$} &  \\\\ \\hline\n% \\multicolumn{1}{c|}{EM} & \\multicolumn{2}{c|}{94.7} & 93.9 & \\multicolumn{1}{c|}{93.4} & 92.8 & \\multicolumn{1}{c|}{92.3} & 90.9 & \\multicolumn{1}{c|}{90.6} & 86.8 & \\multicolumn{1}{c|}{84.5} & 19.9 \\\\ \\hline\n% \\multicolumn{1}{c|}{SYN} & \\multicolumn{2}{c|}{94.7} & 93.6 & \\multicolumn{1}{c|}{93.4} & 92.6 & \\multicolumn{1}{c|}{92.3} & 90.7 & \\multicolumn{1}{c|}{90.6} & 86.3 & \\multicolumn{1}{c|}{84.5} & 13.5 \\\\ \\hline \\hline\n% \\end{tabular}\n% \\end{adjustbox}\n% \\end{table*}\n\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Poisoning Different Percentages of The Training Data",
                    "content": "\n\\label{apdx:different_percentage}\n\nHere we show synthetic perturbations remain effective when only a given percentage of the training data is poisoned. We follow the experimental setup in \\citet{huang2021unlearnable,fowl2021adversarial}. Specifically, for each poisoning percentage, we train two models.  One model uses both the clean subset and the poisoned subset as its training data and the other one only uses the clean subset. The difference between the performances of those two models  represents how much information the former model gains from the poisoned data. A small performance gap indicates the former model gains little information from the poisoned data.\n\nWe test four different poisoning percentages (from 20\\% to 90\\%) on the CIFAR-10 dataset. The experiments are run on  ResNet-18 models. We compare the performance of synthetic perturbations with that of adversarial examples and error-minimizing noises \\citep{huang2021unlearnable,fowl2021adversarial}. The results are presented in Table~\\ref{tbl:different_percentage}. The performance gain of using the poisoned subset is small for all three attacks. This suggests that synthetic perturbations are still effective in this setting.\n\n\n \n\n\n%to train one model and $\\sD_{c}$ to train another. \n\n%\\subsubsection{Different Data Augmentation Methods}\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Experiments on Face Data",
                    "content": "\n\\label{subsec:face}\n\n\nHere we apply Algorithm~\\ref{alg:generation} on face datasets. We follow the application scenario in \\citet{huang2021unlearnable} (see Figure 4 in \\citet{huang2021unlearnable} for an illustration). The task is to use face images to predict biological identities.   We train an Inception-ResNet-v1 model \\citep{szegedy2017inception} on the WebFace Dataset \\citep{yi2014learning}. A random subset with 20\\%  samples is used for testing and the remaining samples are used for training. The WebFace dataset has 10575 identities and 50 of them are poisoned. We run Algorithm~\\ref{alg:generation} with the configuration in Section~\\ref{subsec:comparison} ($\\epsilon'=6/255$) to process the training images of the poisoned identities.   \n\n%We visualize the perturbed images of a randomly sampled unlearnable identity in Figure~\\ref{fig:synthetic_images_face}.\n\n\n\nWhen using Algorithm~\\ref{alg:generation}, the test accuracy of the poisoned identities is only 13.6\\% which is much lower than the test accuracy of the clean identities (>80\\%). The training curves are plotted in Figure~\\ref{fig:face_training_curve}.  When using error-minimizing noises, the test accuracy of the poisoned identities reported in \\citet{huang2021unlearnable} is $\\sim$16\\%.  These results confirm that Algorithm~\\ref{alg:generation} is also highly effective on face data.  We note that generating error-minimizing noises requires some auxiliary data. For example, \\citet{huang2021unlearnable} use 100 identities from the CelebA dataset \\citep{liu2015faceattributes} to generate error-minimizing noises for the 50 identities from the WebFace dataset. On the contrary, running Algorithm~\\ref{alg:generation} does not require any auxiliary data. \n\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Ablation Study",
                "content": "\n\\label{sec:abla}\n\nIn this section, we run some experiments to better understand our findings and the proposed algorithm. We first test whether the linear separability is stemming from the property of the ReLU activation function. Then we demonstrate the padding operation in Algorithm~\\ref{alg:generation} is necessary to make the perturbations robust against data augmentation methods.\n\n",
                "subsection 5.1": {
                    "name": "Linear Separability Is Not Stemming from ReLU",
                    "content": "\n\\label{apdx:tanh}\n\nIt is well known to the community that a ReLU-DNN learns a piecewise linear function in input space \\citep{arora2016understanding}. The crafting models of advanced availability attacks all use the ReLU activation by default. Therefore, the linear separability may be stemming from the property of ReLU. To verify this, we replace the ReLU layers with Tanh layers in the crafting models of error-minimizing noises \\citep{huang2021unlearnable} and adversarial examples \\citep{fowl2021adversarial}. We fit the new perturbations with the same simple models as those in Section~\\ref{sec:why}. The results are presented in Table~\\ref{tbl:linear_acc_tanh}. The new perturbations are still almost linearly separable: linear models achieve more than 90\\% training accuracy and two-layer neural networks achieve 100\\% training accuracy. This suggests that the linear separability is not associated with the ReLU activation function. \n\n\n \n\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "alg:generation",
                    "content": "\n\\label{apdx:no_pad}\n\n\n\n\n\nHere we explain why we duplicate each dimension of the initial data points into two-dimensional patches in Algorithm~\\ref{alg:generation}.  Intuitively, it is more convenient to  directly generate synthetic perturbations that have the same dimension as the original images. We will show  this straightforward approach can not be used as a powerful attack. \n\n\n% is to generate synthetic data with the same dimension as the original images. We will show  this trivial approach has unstable performance when common data augmentation methods are applied. \n\n\n\n\n\nWe directly use the output of the method in \\citet{guyon2003design} as the straightforward approach, i.e., the dimension of synthetic data is the same as the dimension of flattened images and we simply reshape the synthetic data into the image format. Other configurations are the same as those in Section~\\ref{sec:syntehtic_noise}. The models are trained with standard augmentation methods including random crop and flipping.  The training curves of the target models are plotted in Figure~\\ref{fig:nopad_trn_curve}. The test accuracy is still high when the data is poisoned, which does not meet the requirement of availability attacks. \n\nThe fact that the padding operation makes the perturbations remain effective may be because it introduces local correlation into the perturbations, which is an inherent property of natural images. In Appendix~\\ref{apdx:more_aug}, we show synthetic perturbations are still highly effective when more powerful data augmentation methods  are applied.\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\n\nThis work gives an explanation of the working principle of availability poisoning attacks. We show advanced attacks coincidentally  generate linearly-separable perturbations. We further synthesize linearly separable  perturbations to demonstrate that using linearly separable perturbations is sufficient for an availability attack to succeed. The proposed algorithm is an order of magnitude faster than existing attacks. Our findings also suggest deep models are more prone to shortcuts than previously believed. They will find and heavily rely on shortcuts even when the shortcuts are scaled down to an imperceptible magnitude. \n\n\n"
            },
            "section 7": {
                "name": "Acknowledgement",
                "content": "\n\nDa Yu and Jian Yin are with Guangdong Key Laboratory of Big Data Analysis and Processing, School of Computer Science and Engineering, Sun Yat-sen University. Jian Yin is supported by the National Natural Science Foundation of China (U1811264, U1811262, U1811261, U1911203, U2001211), Guangdong Basic and Applied Basic Research Foundation (2019B1515130001), Key-Area Research and Development Program of Guangdong Province \\\\(2018B010107005, 2020B0101100001).\n\n\n% explains how the unlearnable examples in \\citet{huang2021unlearnable} work. Based on our explanation, we propose a new algorithm for generating unlearnable examples that is an order of magnitude faster than the one in \\citet{huang2021unlearnable}. Our explanation also reveals that pre-trained models could be used to counteract unlearnable examples. An interesting future direction is to generate unlearnable examples for  data in different research fields such as natural language processing. Another intriguing prospect is making unlearnable examples more robust against countermeasures.\n\n% unlearnable examples for other learning tasks. Why deep models simply discard all other features when there \n\n% First, we explain why the unlearnable examples in \\citet{huang2021unlearnable} work so well. Then, based on our understanding, we propose to use synthetic data to generate unlearnable examples. The proposed method has significantly lower cost than the previous method. Finally, when the threat model is different, we show the vulnerability of unlearnable examples against pre-trained feature extractors.\n\\bibliography{acmart}\n\\bibliographystyle{ACM-Reference-Format}\n\n\n\\begin{appendices}\n\n\n\n\n\\begin{appendix}\n\n\n\n\n\\section{Additional t-SNE Plots}\n\\label{apdx:more_tsne}\n\n\n\nHere we plot the t-SNEs of two other attacks in Table~\\ref{tbl:linear_acc}, i.e., adversarial examples \\citep{fowl2021adversarial} and NTGA \\citep{yuan2021neural}. We use their official implementations to generate the perturbations (see Appendix~\\ref{apdx:exp_details} for details).   The t-SNEs are plotted in Figure~\\ref{fig:add_tsne}. The perturbations for the same class are well clustered. This observation is similar to  that from Figure~\\ref{fig:tsne}.\n\n\n\\begin{figure}[h]\n\\centering\n  \\includegraphics[width=0.85\\linewidth]{imgs/additional_tsne.pdf}\n  \\caption{T-SNEs of targeted adversarial examples \\citep{fowl2021adversarial} and NTGA \\citep{yuan2021neural}. Perturbations from the same class are well clustered. Notably, many embeddings of NTGA are overlapped. }\n  \\label{fig:add_tsne}\n\\end{figure}\n\n\\section{Implementation Details}\n\\label{apdx:exp_details}\n\n\\textbf{Implementation details of the experiments in Section~\\ref{sec:why}.} We generate perturbations of baseline algorithms using their official implementations:  DeepConfuse\\footnote{\\url{https://github.com/kingfengji/DeepConfuse}}, NTGA\\footnote{\\url{https://github.com/lionelmessi6410/ntga}},  error-minimizing noise\\footnote{\\url{https://github.com/HanxunH/Unlearnable-Examples}}, and adversarial examples\\footnote{\\url{https://github.com/lhfowl/adversarial_poisons}}. The configuration is set to be the one that achieves the best attack performance on CIFAR-10. Specifically, DeepConfuse uses an 8-layer U-Net \\citep{ronneberger2015u} as the crafting model. NTGA uses a 3-layer convolutional network. Error-minimizing noises and adversarial examples use standard ResNet-18 models. \n\nThe experimental setup for training the simple models is as follows. We train the simple models  with standard cross-entropy loss. Before training, all perturbations are flattened into 1-dimensional vectors and normalized to unit norm. The two-layer neural networks have a width of $30$. All models are trained with the L-BFGS optimizer \\citep{liu1989limited} for 50 steps.\n\n% , we implement their algorithm with the setup in Appendix A.1 of \\citet{fowl2021adversarial}. \n\n% In addition to the two attacks in Section~\\ref{sec:draw_tsne}, we also test two recent attacks NTGA \\citep{yuan2021neural} and error-maximizing noise \\citep{fowl2021adversarial}. For NTGA, we use the official implementation to generate perturbations\\footnote{\\url{https://github.com/lionelmessi6410/ntga}}.  These four chosen attacks achieve advanced attack performance and  cover all three approaches introduced in Section~\\ref{sec:three_attacks}.\n\n\\textbf{Implementation details of the experiments in Section~\\ref{sec:syntehtic_noise}.} We use the Stochastic Gradient Descent (SGD) optimizer with a momentum coefficient 0.9 for all experiments. For all datasets, we use a batchsize of 128. The learning rates of all models are set to follow the choices in the original papers \\citep{simonyan2014very,he2016deep,huang2017densely}. The learning rate for ResNet and DenseNet models is 0.1. The learning rate for VGG models is 0.01.  All models are trained for 100 epochs. The learning rate is divided by 10 at epoch 50 and 75.\n\n\n\n\n\\section{Training with More Data Augmentation Methods}\n\\label{apdx:more_aug}\n\nHere we demonstrate that synthetic noises can not be filtered out by state-of-the-art data augmentation methods. We test  four advanced data augmentation methods including Cutout \\citep{devries2017improved}, Mixup \\citep{zhang2017mixup}, CutMix \\citep{yun2019cutmix}, and Fast Autoaugment (FA) \\citep{lim2019fast}. Experimental results suggest that  synthetic perturbations are still highly effective when those augmentation methods are applied.\n\n\n% \\begin{figure}\n% \\centering\n%   \\includegraphics[width=0.97\\linewidth]{imgs/aug_trn_curve.pdf}\n%   \\caption{Training curves of ResNet18 models on the CIFAR-10 dataset. The title of each subplot shows the used augmentation method.  }\n%   \\label{fig:aug_trn_curve}\n% \\end{figure}\n\n\n\\begin{table}\n\\caption{Test accuracy (in \\%) of ResNet18 models on the CIFAR-10 dataset.}\n\\renewcommand{\\arraystretch}{1.25}  \n\\label{tbl:acc_aug}\n\\centering\n\\small\n\\begin{tabular}{c|c|c|c|c}\n\\hline\n\\hline\n  & Cutout &   Mixup   & CutMix &  FA      \\\\ \\hline  \n  \nError-min. Noises  & 18.9 &  57.4  & 32.3 &  41.6          \\\\ \\hline\nSynthetic Perturbations  & \\textbf{10.6}  & \\textbf{39.5}    & \\textbf{17.7} & \\textbf{24.4}   \\\\ \\hline \\hline\n\n\\end{tabular}\n\\end{table} \n\n\nWe train ResNet18 models on the CIFAR-10 dataset.  For all augmentation methods, we use the default configurations for CIFAR-10 from the original papers to set their parameters. Other experimental settings such as the noise strength and training recipe are the same as those in Section~\\ref{subsec:comparison}. The results are presented in Table~\\ref{tbl:acc_aug}. In Table~\\ref{tbl:acc_aug}, we also include the test accuracy of using error-minimizing noises for a comparison. The results suggest that   synthetic \nperturbations are more effective when advanced augmentation methods are applied. For example, when Fast Autoaugment (FA) is applied, the test accuracy of using synthetic perturbations is 24.4\\% while the test accuracy of using error-minimizing noises is 41.6\\%.\n\n\n\n\n\n% \\textbf{Implementation details of the experiments in Section~\\ref{sec:pretrained}.} To fine-tune the pre-trained model on the CIFAR-10 dataset, we first use bicubic upsampling to resize the image into $224\\times 224$. For the first approach, we use the same hyperparameters as Section~\\ref{sec:syntehtic_noise} to fine-tune the full model. For the second approach, we first extract the features of all samples and train linear classifiers on the extracted features. The linear classifiers are trained with the L-BFGS optimizer for 50steps.\n\n\n\n\\end{appendix}\\end{appendices}\n\n\n\n"
            }
        },
        "tables": {
            "tbl:linear_acc": "\\begin{table}[t]\n\\caption{Training accuracy (in \\%) of simple models on clean data and the perturbations of different attacks.}\n\n\\label{tbl:linear_acc}\n\\centering\n\n% \\renewcommand{\\arraystretch}{1.15}\n\\begin{tabular}{c|cc}\n\nAlgorithm & Linear Model & Two-layer NN  \\\\ \\hline\nClean Data &    $49.9$            &  $70.1$ \\\\ \n DeepConfuse \\citep{feng2019learning} & $100.0$  & $100.0$ \\\\ \n  NTGA \\citep{yuan2021neural} & $100.0$ & $100.0$ \\\\ \n Error-minimizing \\citep{huang2021unlearnable} & $100.0$ & $100.0$  \\\\ \n Adv. Examples (Untargeted) \\citep{fowl2021adversarial} & $91.5$ & $99.9$ \\\\ \n Adv. Examples (Targeted) \\citep{fowl2021adversarial} & $100.0$ & $100.0$ \\\\ \n\\end{tabular}\n\\end{table}",
            "tbl:compare": "\\begin{table}[h]\n\\caption{Accuracy on clean test data of CIFAR-10. The target model is ResNet-18. The training data are poisoned with different attacks. The closer the accuracy to random guessing, the better the attack efficiency.}\n\n\\label{tbl:compare}\n\\centering\n\n% \\renewcommand{\\arraystretch}{1.25}\n\\begin{tabular}{c|c}\n\n\n\nAlgorithm & Test Accuracy (in \\%)  \\\\ \\hline\nNo Perturbation &  $94.69$   \\\\\n TensorClog \\citep{shen2019tensorclog} &  $48.07$  \\\\ \n  Alignment \\citep{fowl2021preventing} &  $56.65$ \\\\ \n DeepConfuse \\citep{feng2019learning} &  $28.77$ \\\\ \n  NTGA \\citep{yuan2021neural} &  $33.29$ \\\\ \n Error-minimizing \\citep{huang2021unlearnable} & $19.93$  \\\\ \n Adversarial Examples \\citep{fowl2021adversarial} & $6.25$  \\\\ \n Synthetic Perturbations  &   $13.54$ \\\\\n\\end{tabular}\n\\end{table}",
            "tbl:different_datasets": "\\begin{table*}\n\\renewcommand{\\arraystretch}{1.25}\n\\caption{Accuracy (in \\%) on clean test data. The target models are trained on clean data ($\\sD_{c}$) and data perturbed by synthetic perturbations ($\\sD_{syn}$).}\n\n\\label{tbl:different_datasets}\n\\centering\n\n\\begin{threeparttable}\n\\begin{tabular}{c|cc|cc|cc|cc}\n\n\\hline\n\\multirow{2}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Target Model\\end{tabular}}} &  \\multicolumn{2}{c|}{\\textbf{SVHN}} & \\multicolumn{2}{c|}{\\textbf{CIFAR-10}} & \\multicolumn{2}{c|}{\\textbf{CIFAR-100}}  & \\multicolumn{2}{c}{\\textbf{ImageNet Subset}} \\\\ \\cline{2-9}\n  & \\textbf{$\\sD_{c}$} &  \\textbf{$\\sD_{syn}$} &\n  \\textbf{$\\sD_{c}$} &  \\textbf{$\\sD_{syn}$} &\n  \\textbf{$\\sD_{c}$} &  \\textbf{$\\sD_{syn}$} & \\textbf{$\\sD_{c}$} &  \\textbf{$\\sD_{syn}$}   \\\\ \\hline\n  \n  \nVGG-11 & 95.4  & 18.1       &   91.3  & 28.3 & 67.5 &   10.9  &  79.1 &  10.7                    \\\\ \\hline\nResNet-18 & 96.2 & 8.0      & 94.7  & 13.5 & 74.8 & 9.0  & 79.7 &   11.0     \\\\ \\hline\nResNet-50 & 96.4 & 7.8      &   94.8   & 14.9 & 75.2 & 8.4  & 82.4 &    10.8    \\\\ \\hline\nDenseNet-121 & 96.7 & 9.7   & 95.0   & 10.6 & 76.5 & 7.6   & 82.9 &     14.7  \\\\ \\hline \n\\end{tabular}\n\\end{threeparttable}\n\n\\end{table*}",
            "tbl:cost": "\\begin{table}\n\\caption{Time costs (in seconds) of generating error-minimizing noises, adversarial examples, and synthetic perturbations.}\n\n\\label{tbl:cost}\n\\centering\n\n\\begin{tabular}{c|ccc}\n\\hline\n\\hline\nMethod & SVHN  & CIFAR-10 &   ImageNet     \\\\ \\hline\nError-min. Noises & $\\sim$2.7k  & $\\sim$3.5k &   $>$28k   \\\\ \\hline\nAdv. Examples & $\\sim$3.3k  & $\\sim$4.1k &   $>$30k   \\\\ \\hline\nAlgorithm~\\ref{alg:generation} & $<$3 & $<$3  & $<$3   \\\\ \\hline \\hline\n\\end{tabular}\n\\end{table}",
            "tbl:some_class": "\\begin{table}\n\n\\centering\n\\caption{Test accuracy (in \\%) on clean ($\\sC$) and poisoned  ($\\sP$) classes. Numbers of poisoned classes are 1, 3, and 5.}\n\\label{tbl:some_class}\n\\begin{tabular}{c|cc|cc|cc}\n\\hline\n\\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Method\\end{tabular}} &  \\multicolumn{2}{c|}{1}  &  \\multicolumn{2}{c|}{3} &  \\multicolumn{2}{c}{5}\\\\ \\cline{2-7}\n  & $\\sC$ & $\\sP$ \n  & $\\sC$ & $\\sP$\n  & $\\sC$ & $\\sP$ \\\\ \\hline\n  \n Error-min. Noises \\cite{huang2021unlearnable} & 94.6 & 2.4 & 93.9 & 1.1 & 93.8 & 3.1 \\\\\\hline  \n  \nSynthetic Perturbations & 94.7 & 2.7 & 94.0 & 0.6 & 93.2 & 2.9 \\\\\\hline \\hline\n  \n \n\\end{tabular}\n\\end{table}",
            "tbl:different_percentage": "\\begin{table}[t]\n\\caption{Test accuracy (in \\%) with different poisoning percentages $p$.  Training with the poisoned subset does not improve the test accuracy much compared to training with clean data only.}\n\n\\label{tbl:different_percentage}\n\\centering\n\\renewcommand{\\arraystretch}{1.15}\n\\begin{tabular}{c|cccc}\n\nMethod & $p=$90\\%  & $p=$80\\%   & $p=$50\\%   & $p=$20\\%    \\\\ \\hline\nClean Data ($1-p$) &   82.6   &  86.5  & 92.4 & 93.9  \\\\ \n Error-min. Noises \\citep{huang2021unlearnable} & 85.2  & 86.8  & 92.8 &  94.1  \\\\ \nAdv. Examples \\citep{fowl2021adversarial} &  85.3  &  88.2 &  92.2  & 93.7 \\\\ \n Synthetic Perturbations & 85.7  & 86.3   & 92.9    & 94.0 \\\\ \n\\end{tabular}\n\\end{table}",
            "tbl:linear_acc_tanh": "\\begin{table}\n\\caption{Training accuracy (in \\%) of simple models on the perturbations of different attacks. The perturbations are generated with \\textbf{Tanh-DNNs.}}\n\n\\label{tbl:linear_acc_tanh}\n\\centering\n\n% \\renewcommand{\\arraystretch}{1.15}\n\\begin{tabular}{c|cc}\n\nAlgorithm & Linear Model & Two-layer NN  \\\\ \\hline\n Error-min. Noises \\citep{huang2021unlearnable} & $100.0$ & $100.0$  \\\\ \nAdv. Examples (Untargeted) \\citep{fowl2021adversarial} & $92.7$ & $100.0$ \\\\ \n Adv. Examples (Targeted) \\citep{fowl2021adversarial} & $100.0$ & $100.0$ \\\\ \n\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:unlearnable_setting": "\\begin{figure}[h]\n\\centering\n  \\includegraphics[width=1.0\\linewidth]{imgs/cat_setting-cropped.pdf}\n  \\caption{An illustration of clean-label availability attacks. }\n  \\label{fig:unlearnable_setting}\n\\end{figure}",
            "fig:tsne": "\\begin{figure*}[t]\n\\centering\n  \\includegraphics[width=0.8\\linewidth]{imgs/all_tsne.pdf}\n  \\caption{T-SNEs of the first three classes of clean CIFAR-10 data and the perturbations generated via DeepConfuse \\citep{feng2019learning} and error-minimizing noises \\citep{huang2021unlearnable}. The perturbations are flattened and normalized into unit norms. }\n  \\label{fig:tsne}\n\\end{figure*}",
            "fig:shortcut": "\\begin{figure}[h]\n\\centering\n  \\includegraphics[width=0.65\\linewidth]{imgs/shortcut_setting-cropped.pdf}\n  \\caption{An illustration of  how the perturbations of availability attacks work as shortcuts.  }\n  \\label{fig:shortcut}\n\\end{figure}",
            "fig:synthetic_images": "\\begin{figure*}\n\\centering\n  \\includegraphics[width=0.7\\linewidth]{imgs/visualization.pdf}\n  \\caption{Visualization of perturbed images and normalized perturbations. Columns a) and b)  use synthetic perturbations. Columns c) and d) use the attack in  \\citet{huang2021unlearnable}.  }\n  \\label{fig:synthetic_images}\n\\end{figure*}",
            "fig:trn_curve": "\\begin{figure*}\n\\centering\n  \\includegraphics[width=.9\\linewidth]{imgs/resnet18_acc_4datasets.pdf}\n  \\caption{Training curves of ResNet-18 models on perturbed and clean data. The word `poisoned' denotes the model is trained on perturbed data. The test performance is evaluated on clean data. The test accuracy is low throughout training when synthetic perturbations are added. }  \n  \\label{fig:trn_curve}\n\\end{figure*}",
            "fig:nopad_trn_curve": "\\begin{figure*}[h]\n\\centering\n  \\includegraphics[width=0.6\\linewidth]{imgs/resnet18_nopad_acc_3datasets.pdf}\n  \\caption{Training curves of ResNet18 models trained on SVHN, CIFAR-10, and CIFAR-100 datasets. The perturbations are \\textbf{NOT} processed by the padding opeartion. }\n  \\label{fig:nopad_trn_curve}\n\\end{figure*}",
            "fig:face_training_curve": "\\begin{figure}\n\\centering\n\\includegraphics[width=0.3\\textwidth]{imgs/face_trn_curve.pdf}\n\\caption{Training curves of an Inception-ResNet model on clean/poisoned WebFace.  }\n\\label{fig:face_training_curve}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\begin{aligned}\n\\label{eq:bi_level}\n&\\argmax_{\\{\\boldsymbol\\delta\\}\\in \\Delta} \\mathbb{E}_{(\\vx,y)\\sim \\mathcal{D}}[\\ell(f^{*}(\\vx), y)],\\\\\n&\\text{s.t. } f^{*}\\in \\argmin_{f} \\sum_{(\\vx,y)\\in\\sD} \\ell(f(\\vx+\\boldsymbol\\delta), y),\n\\end{aligned}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\begin{aligned}\n\\label{eq:error_min}\n\\argmin_{\\{\\boldsymbol\\delta\\}\\in\\Delta} \\mathbb{E}_{(\\vx,y)\\sim\\mathbb{D}} [\\min_{f}\\ell(f(\\vx+\\boldsymbol\\delta), y)].\n\\end{aligned}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\begin{aligned}\n\\label{eq:error_max}\n&\\argmax_{\\{\\boldsymbol\\delta\\}\\in \\Delta} \\mathbb{E}_{(\\vx,y)\\sim\\mathbb{D}} \\left[\\ell\\left(f\\left(\\vx+\\boldsymbol\\delta\\right), y\\right)\\right],\\\\\n&\\argmin_{\\{\\boldsymbol\\delta\\}\\in \\Delta} \\mathbb{E}_{(\\vx,y)\\sim\\mathbb{D}} \\left[\\ell\\left(f\\left(\\vx+\\boldsymbol\\delta\\right), y'\\right)\\right],\n\\end{aligned}\n\\end{equation}"
        },
        "git_link": "https://github.com/dayu11/Availability-Attacks-Create-Shortcuts"
    }
}