{
    "meta_info": {
        "title": "S2RL: Do We Really Need to Perceive All States in Deep Multi-Agent  Reinforcement Learning?",
        "abstract": "Collaborative multi-agent reinforcement learning (MARL) has been widely used\nin many practical applications, where each agent makes a decision based on its\nown observation. Most mainstream methods treat each local observation as an\nentirety when modeling the decentralized local utility functions. However, they\nignore the fact that local observation information can be further divided into\nseveral entities, and only part of the entities is helpful to model inference.\nMoreover, the importance of different entities may change over time. To improve\nthe performance of decentralized policies, the attention mechanism is used to\ncapture features of local information. Nevertheless, existing attention models\nrely on dense fully connected graphs and cannot better perceive important\nstates. To this end, we propose a sparse state based MARL (S2RL) framework,\nwhich utilizes a sparse attention mechanism to discard irrelevant information\nin local observations. The local utility functions are estimated through the\nself-attention and sparse attention mechanisms separately, then are combined\ninto a standard joint value function and auxiliary joint value function in the\ncentral critic. We design the S2RL framework as a plug-and-play module, making\nit general enough to be applied to various methods. Extensive experiments on\nStarCraft II show that S2RL can significantly improve the performance of many\nstate-of-the-art methods.",
        "author": "Shuang Luo, Yinchuan Li, Jiahui Li, Kun Kuang, Furui Liu, Yunfeng Shao, Chao Wu",
        "link": "http://arxiv.org/abs/2206.11054v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n% Multi-agent reinforcement learning (MARL) provides a framework for multiple agents to solve complex sequential decision-making problems, with broad applications including robotics control~\\cite{huttenrauch2017guided}, video gaming~\\cite{vinyals2019grandmaster} and autonomous driving~\\cite{kiran2021deep}. \n% To help address these problems, a number of deep MARL methods have been proposed recently to learn the cooperative ability among agents~\\cite{VDN,QMIX,QPLEX}. However, most existing methods ignore the dynamic interaction between agents, where each agent simply consider the information of all other agents equally in every decision-making process. In many real-world environments, the interaction relationships between different agents often change dynamically over time, which inevitably affect the exploration efficiency and damage the final performance.\n% % A number of deep MARL methods have been proposed recently to learn the cooperative multi-agent model~\\cite{VDN,QMIX,QPLEX}. \n% % However, current methods mostly ignore that the agent's observations from other entities are not of equal importance. Simply considering all agents equally at all times may negatively influence the effectiveness of the agent's decisions. \n\n% %However, current methods inadequately focus on treating the agent's observation from other entities differently in the environment. In fact, it is beneficial for agents to know what information of observation it should pay attention to. \n\n% To solve the above issue, attention mechanism~\\cite{attention} has attracted a lot of research interests. The attention mechanism enables us to learn the interaction relationship between agents and dynamically focus on the important information.\n% % As a solution to this issue, attention mechanism~\\cite{attention} has been widely used in a variety of computer vision and natural language processing tasks, such as image classification~\\cite{} and speech recognition~\\cite{}. \n\n% Recent developments of attention mechanism in MARL are mainly concerned with solving the dynamic input size of variable numbers of agents~\\cite{REFIL,UPDeT,EPC} and inter-agent communication~\\cite{MAAC,ATOC}. \n% % In MARL, some researchers utilize the attention mechanism process observations of variable numbers of agents~\\cite{REFIL,UPDeT,EPC}. Some exploit inter-agent communication by selecting relevant information through the attention mechanism~\\cite{MAAC,ATOC}. \n% While few of them consider some agent observation entities are useless, agents can neglect them and pay primary attention to critical states.\n\n% Therefore, to be practical, it is crucial for agent learning to select the actual state in a sparse way, which helps it to make decisions more effectively. Because sparsification can mask unimportant state information and form an intensive attention distribution.\n% However, suppose the agent focuses directly on the critical states at the beginning of the training process and completely ignores the other states. In that case, it will affect the exploration of the unknown states because the agent cannot determine which state is good. To solve this problem, we can use the sparse state as an auxiliary way to guide the agents to explore the vital states.\n\n\n\n\nMulti-agent reinforcement learning (MARL) provides a framework for multiple agents to solve complex sequential decision-making problems, with broad applications including robotics control~\\cite{li2021shapley,robot}, video gaming~\\cite{vinyals2019grandmaster,liu2019}, traffic light control~\\cite{trafic2017} and autonomous driving~\\cite{kiran2021deep,autonomous}. In the paradigm of centralized training with decentralized execution (CTDE)~\\cite{MADDPG,QMIX}, each local agent models a policy that treats the local observation as input. \n% In deep reinforcement learning, the local policies are modeled by a neural network usually implemented by recurrent neural networks.\n% However, most mainstream methods take local observations as input while underestimate the role of entities.\nHowever, the role of entities is underestimated by most mainstream methods.\nEntities are defined as fine-grained tokens of observations, \\eg, \\emph{obstacles, landmarks, enemies}, which determine the inference process of the model. Specifically, they treat all entities observed as a whole and contribute indiscriminately to the estimation of the value function. But in some cases, the importance of each entity changes dynamically over time steps.\n\n% However, most of the mainstream methods take the local observations as inputs while underestimate the role of entities. Entities are defined as the fine-grained tokens of the observation, \\eg  \\emph{enemy $1$ location, agent type, health, etc} in StarCraft II, which determine the inference procedures of models. Specifically, they treat all entities of an observation as a whole and indiscriminate their contributions to the estimation of value functions. While in some scenarios, the importance of the each entity changes dynamically along with the time step.\n\n\n\nTo better leverage the observation information, the attention mechanism has been adopted~\\cite{attention} for its ability to learn the interaction relationship among entities and dynamically focus on the crucial parts. Most existing attention mechanisms compute importance weights based on dense fully-connected graphs, where all participants are assigned scores according to their contribution to model decisions. In practice, however, not all entities are helpful for model inference, and discarding redundant entities can sometimes improve overall performance. Therefore, it is crucial for agents to learn to select valuable observations and exclude others. \nTo better illustrate this phenomenon, a visualization of the StarCraft II scene and the corresponding attention distribution is shown in Figure~\\ref{fig:ex}. The green agent H3 is very close to the red enemies Z0 and Z5. Hence agent H3 focusing only on enemies Z0 and Z5 is more effective. However, from the traditional dense attention distribution of H3, we can see that H3 assigns much attention to irrelevant entities. Note that the large state space brings great difficulties to policy learning for a more complex MARL environment.  \n\n\n\n% To better leverage the observation information, attention mechanism has been adopted~\\cite{attention} for its ability to learn the interaction relationship among entities and dynamically focus on the important parts. Nevertheless, most of the existing attention mechanism compute the importance weights based on the dense fully-connected map in which all the participants will be assigned scores according to their importance for the model decision. While in many real-world situations, not all entities are useful for the model inference, and sometimes the overall performance will get an improvement when these redundant entities are discarded. Therefore, it is crucial for agents to learn to select the useful observation information while exclude the others. \n% For example, in Figure~\\ref{fig:ex}, the visualization of the StarCraft II scenario and the corresponding attention distribution highlights an urgent demand for sparse attention mechanism. \n% Meanwhile, agent observation space gradually increases with the size of the population, causing great difficulty for policy learning. Sparse attention mechanism reduces the number of entities that agent needs to pay attention to, simplifying the exploration space of agent to some extent.\n% For example, in Figure~\\ref{fig:ex}, the visualization of the StarCraft II scenario and the corresponding attention distribution highlights the urgent demand for sparse attention mechanism. Meanwhile, agent observation space gradually increases with the size of the population, causing great difficulty for policy learning. The sparse attention mechanism reduces the number of entities that agent needs to pay attention to, simplifying the exploration space of agents to some extent.\n\n\n\n\n\n\n\n\n\n\n% In contrast, adopting sparse attention can well guide H3 to only perceive Z0 and Z5, which makes the reinforcement learning system more efficient.\n% At this time, the sparse attention mechanism reduces the number of states that the agent needs to perceive, which can greatly simplify the agent's exploration space.\n\nAn ideal way to solve this issue is to replace the traditional attention mechanism with sparse attention. \n% \\textcolor{red}{For example, SparseMAAC~\\cite{sparsemaac} extends MAAC~\\cite{MAAC} with sparsity by replacing the softmax activation function in attention mechanism with $\\gamma$-sparsemax directly. \n% It calculates the importance of different agents observations and actions, while we mainly focus on selecting key entities of individual observations.}\nFrom Figure~\\ref{fig:ex}(c), we can see that adopting sparse attention can well guide H3 only to perceive Z0 and Z5, reducing the observation space that the agent needs to perceive.  \nHowever, simply applying sparse attention to local agents will corrupt the training. The main reason is that the network cannot distinguish which entity is more important at the beginning of training. If the agents only focus on critical entities initially, it may lead to an inadequate exploration of the environment and thus converge to a suboptimal policy. More specifically, temporarily discarding some entities can be seen as a policy exploration behavior. Meanwhile, local policies need to execute their exploration strategies. When these two strategies are executed simultaneously, it is difficult for the model to converge.\n\n\n% , and discard some entities tentatively can be deemed as a kind of exploration strategy.\n% An ideal way to solve this issue is to replace the traditional attention mechanism with sparse attention. However, simply apply sparse attention on the local agents will corrupt the training. The main reason is that when the training starts, the network can not distinguish which entity is more important, and discard some entities tentatively can be deemed as a kind of exploration strategy.\n% At the same time, the local policy need to execute its exploration strategy. A model can hardly convergences when these two strategy are performed simultaneously.\n\n\n\nIn this paper, we propose a \\emph{Sparse State based MARL}~(S2RL) framework, where the sparse attention mechanism is utilized as the auxiliary for guiding the local agent training.\n% Specifically, we first model the local value function using a traditional self-attention mechanism. Then, we construct a corresponding utility function for each agent, which is implemented by a sparse attention mechanism.\n% Finally, in the central critic, the local value function and utility function respectively form the joint value function and auxiliary value function, which are further used to train the entire network.\nIn particular, we model the local utility function using a traditional self-attention mechanism. Then, we construct a corresponding auxiliary utility function for each agent, which is implemented by a sparse attention mechanism. The local utility and auxiliary utility functions respectively form the joint value and auxiliary value functions, which are further used to train the entire network.\nSince the sparse attention mechanism is considered auxiliary and thus does not corrupt the training process,  the auxiliary value function is also used to update the entire framework. To this end, local agents can learn patterns to focus on essential entities while ignoring redundancy.\n\n% In this paper, we propose a \\emph{Sparse State based MARL} (S2RL) framework where the spare attention mechanism is utilized as the auxiliary for guiding the training of local agents.\n% Concretely, we first model a local value function with traditional  self-attention mechanism. \n% Meanwhile, we construct a corresponding utility function for each agent which is implemented with spare attention mechanism.\n% Finally, in the central critic, and the local value functions and utility functions form the joint value function and auxiliary value function respectively which are further used to train the whole network.\n% The sparse attention mechanism is treated as an auxiliary hence will not corrupt the learning procedure. Since the auxiliary value function is also used to updated the whole framework, the local agents can learn the pattern to focus on important entities while neglecting the redundant.\n% Agent observation space gradually increases with the size of the population, causing great difficulty for policy learning. Sparse attention mechanism reduces the number of entities that agent needs to pay attention to, simplifying the exploration space of the intelligent body to some extent.\n\n\n% The implementation details will be introduced in \\emph{Section}~\\ref{sec4}.\n\n% \\subsection{Main Contributions}\n\nOur main contributions are summarized as follows:\n\\begin{itemize}\n\\item To the best of our knowledge, this paper is the first attempt that uses enhanced awareness of crucial states as the auxiliary in MARL to improve convergence rate and performance. \n% And reinforcement learning cannot only use crucial states, which prevents the algorithm from converging.\n\\item We propose the S2RL framework for local agents to perceive crucial states while preserving all states. The proposed framework thus addresses the inability to converge using only a small number of partial observations.\n\\item We design the S2RL framework as a plug-and-play module, making it general enough to be applied to various methods in the CTDE paradigm.\n\\item The extensive experiments on StarCraft II show that S2RL brings remarkable improvements to existing methods, especially in complicated scenarios.\n\\end{itemize}\n\n\n\nThe remainder of the paper is organized as follows.\nIn Section 2, we introduce the background of MARL and the CTDE framework. In Section 3, we propose our S2RL framework. Experimental results are presented in Section 4. Related works are presented in Section 5. Section 6 concludes the paper.\n\n\n\n% Third, we make S2RL a plug-and-play module making it general enough to be applied to various methods in the CTDE paradigm. At last, the extensive experiments on StarCraft II show that S2RL brings huge improvements to existing methods especially when in the complicated scenarios.\n% First, we found that it is not optimal for multi-agent reinforcement learning to perceive all states when dealing with complex tasks. Paying more attention to crucial states can help agents learn optimal policies more effectively. But reinforcement learning cannot only use crucial states, which prevents the algorithm from converging.\n\n\n% Second, we propose the S2RL framework to perceive crucial states while preserving all states. In particular, we model the local value function using a traditional self-attention mechanism. Then, we construct a corresponding utility function for each agent, which is implemented by a sparse attention mechanism. The local value function and utility function respectively form the joint value function and auxiliary value function, which are further used to train the entire network. The proposed framework thus addresses the inability to converge using only a small number of partial observations.\n% The proposed framework thereby addressing the inability to converge with a small number of partial observations at the beginning of training.\n\n% Third, we make S2RL a plug-and-play module making it general enough to be applied to various methods in the CTDE paradigm. At last, the extensive experiments on StarCraft II show that S2RL brings huge improvements to existing methods especially when in the complicated scenarios.\n\n\n\n\n\n% improve the performance and convergence speed of MARL, where the sparse attention mechanism is used to \n\n\n% , where the sparse attention mechanism is utilized as the auxiliary for guiding the local agent training.\n% Specifically, we first model the local value function using a traditional self-attention mechanism. Then, we construct a corresponding utility function for each agent, which is implemented by a sparse attention mechanism.\n% Finally, in the central critic, the local value function and utility function respectively form the joint value function and auxiliary value function, which are further used to train the entire network.\n% Since the sparse attention mechanism is considered auxiliary and thus does not corrupt the training process. And the auxiliary value function is also used to update the entire framework, hence local agents can learn patterns to focus on important entities while ignoring redundancy.\n\n% Our main contributions can be summarized as follows:\n% \\begin{itemize}\n% \\item We propose a novel method S2RL which utilizes sparse attention mechanism to compute auxiliary functions, and thus enable local agents to discard the useless information in observations dynamically.\n% \\item Our S2RL is general enough that can be applied into various methods in the paradigm of CTDE.\n% \\item The extensive experiments on StarCraft II show that S2RL brings huge improvements to existing methods especially when in the complicated scenarios.\n% \\end{itemize}\n\n\n\n\n\n% Visual example of agent performance on the StarCraft II super hard map $6h\\_vs\\_8z$. As shown in (a), the closest to the green agent $H4$ are the red enemies $Z0$ and $Z5$. So the corresponding strategy is that $H3$ only needs to focus on enemies $Z0$ and $Z5$, which are more likely to be annihilated. (b) shows the attention distribution of the $H3$ observations, finding that some weights are still assigned to irrelevant entities. In this case, a sparse probability distribution is required, as shown in (c).\n\n\n\n% where spare attention mechanism is utilized to discard irrelevant information in local observations. \n% First, the local value functions are estimated via a self-attention mechanism and composed into the joint value function in the central critic. \n% Meanwhile, utility functions are computed\n% via sparse attention, which are further used to calculate the auxiliary joint value function.\n% Finally, the whole neural network is trained with both joint value function and its auxiliary.\n% Hence, each agent can learn the pattern to neglect the useless entities dynamically.\n\n\n\n% Illustration of the proposed \\emph{Sparse State based MARL} (S2RL) method. We use the value-based MARL framework under the CTDE paradigm and apply the S2RL module in agent utility network. The core of S2RL is utilizing sparse attention module as auxiliary for guiding the training of dense attention. The weight distribution maps of softmax is dense while the distribution maps of sparsemax is more compact.\n\n\n\n\n\n\n\n\n"
            },
            "section 2": {
                "name": "PRELIMINARIES",
                "content": "\n",
                "subsection 2.1": {
                    "name": "Dec-POMDP",
                    "content": "\nA fully cooperative multi-agent sequential task can be described as a decentralized partially observable Markov decision process (Dec-POMDP)~\\cite{Dec-POMDP}, which is canonically formulated by the tuple:\n\\begin{align}\nM=<\\mathcal{I}, \\mathcal{S}, \\mathcal{U}, P, R, \\Omega, G, \\gamma >.\n\\end{align}\nIn the process, $\\mathcal{I}\\equiv\\{1,2,\\dots,N\\}$ is the finite set of agents and $s \\in \\mathcal{S}$ represents the global state of the environment.\nAt each time step, each agent $i \\in \\mathcal{I}$ receives an individual partial observation $o^{i} \\in {\\Omega}$ according to the observation function $G(s,i)$ and selects an action $u^i \\in \\mathcal{U}$, forming a joint action $\\boldsymbol{u}$. \nThis results in a transition to the next state $s'$ according to the state transition function $P(s'|s, \\boldsymbol{u}): \\mathcal{S} \\times \\mathcal{U} \\times \\mathcal{S} \\rightarrow \\left[0,1\\right]$. \nAll agents share the same global reward $r$ based on the reward function $ R(s, \\boldsymbol{u}): \\mathcal{S} \\times \\mathcal{U} \\rightarrow \\mathbb{R}$, and $\\gamma \\in \\left[0,1\\right)$ is the discount factor. Due to partially observable setting, each agent has an action-observation history $\\tau^{i} \\in \\mathcal{T} \\equiv(\\Omega \\times \\mathcal{U})^{*}$ and learns its individual policy $\\pi^{i}\\left(u^i | \\tau^{i}\\right)$ to jointly maximize the discounted return. The joint policy $\\boldsymbol{\\pi}$ induces a joint action-value function: $Q^{tot}_{\\boldsymbol{\\pi}}(s, \\boldsymbol{u})=\\mathbb{E}_{s_{0: \\infty}, \\boldsymbol{u}_{0: \\infty}}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t} \\mid s_{0}=s, \\boldsymbol{u}_{0}=\\boldsymbol{u}, \\boldsymbol{\\pi}\\right]$.\n\n\n% We model a fully cooperative multi-agent sequential decision-making task as a decentralised partially observable Markov decision process (Dec-POMDP), which is formulated as a tuple $\\mathcal{G}=\\left \\langle\\mathcal{N}, \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\mathcal{O}, \\Omega, \\gamma \\right \\rangle$. In the process, {$\\mathcal{N}$=\\{1,..,n\\}} is a finite set of agents and $s \\in \\mathcal{S}$ represents the true state of the environment. At each discrete time step, each agent $i \\in \\mathcal{N}$ makes action decision ${a_{i} \\in \\mathcal{A}}$ on its own  observation ${o_{i} \\in {\\Omega}}$ simultaneously to formulate a joint action vector $\\boldsymbol{a} = [a_i]$. It results in a state transition on the environment to the next global state described by the Markov transition probability function $\\mathcal{P}(s^{\\prime}\\left|s\\right., a): \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\left[0,1\\right]$. All agents share the same global reward function $r \\in \\mathcal{R}(s, a): \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, and $\\gamma \\in \\left[0,1\\right)$ is the discount factor. Due to partially observable setting, each agent has an action-observation history $\\tau_{i} \\in \\mathcal{T} \\equiv(\\Omega \\times \\mathcal{A})^{*}$ that conditions a stochastic policy $\\pi_{i}\\left(a \\mid \\tau_{i}\\right)$. The joint policy $\\boldsymbol{\\pi}$ induces a joint action-value function: $Q_{t o t}^{\\boldsymbol{\\pi}}(s, \\boldsymbol{a})=\\mathbb{E}_{s_{0: \\infty}, \\boldsymbol{a}_{0: \\infty}}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t} \\mid s_{0}=s, \\boldsymbol{a}_{0}=\\boldsymbol{a}, \\boldsymbol{\\pi}\\right]$\n\n"
                },
                "subsection 2.2": {
                    "name": "CTDE Framework",
                    "content": "\nThe centralized training and decentralized execution (CTDE) is a popular paradigm used in deep multi-agent reinforcement learning~\\cite{RIAL,QMIX,NCC,QPD,COMA}, which enables agents to learn their individual policies in a centralized way. During the centralized training process, the learning model can access the state and provide global information to assist the agents in exploring and training. However, each agent only makes decisions based on its local action-observation history during decentralized execution.\n% Centralized training with decentralized execution(CTDE) means that agents learn in a centralized training way via communicating among others or with access to additional global state information. But during the decentralized execution phase, each agent makes its decision conditioned only on local action-observation history.\nThe Individual-Global-Max principle~\\cite{QTRAN} guarantees the consistency between joint and local greedy action selections. Agents can obtain the optimal global reward by maximizing the individual utility function of each agent. Thus a more robust individual value function can benefit the whole team in cooperative multi-agent tasks. \n\nThe global Q-function $Q^{tot}_{\\boldsymbol{\\pi}}$ is calculated by all individual value functions: $Q^{to t}_{\\boldsymbol{\\pi}}(\\boldsymbol{\\tau}, \\mathbf{u})=F([Q_{\\pi}^{i}(\\tau^{i}, u^{i})]_{i=1}^N,s;\\theta)$\n% $F\\left(\\left[Q_{\\pi^i}^{i}\\left(\\tau^{i}, u^{i}\\right)\\right]_{i}^{N}\\right)$\n, where $\\boldsymbol{\\tau} \\equiv \\mathcal{T}^{n}$ is a joint action-observation history and $\\boldsymbol{u}$ is a joint action, $F$ is the credit assignment function parameterized by $\\theta$ to learn value function factorization. Each agent learns its own utility function by maximizing the global value function $Q^{tot}$, which is trained end-to-end to minimise the following TD loss:\n\\begin{align}\n    \\mathcal{L}(\\theta) =  \\mathbb{E}_{\\mathcal{D}} \\left[\\left(y^{tot} - Q^{tot}(\\boldsymbol{\\tau},\\boldsymbol{u},s;\\theta)\\right)^2\\right],\n\\end{align}\nwhere $\\mathcal{D}$ is the replay buffer, $y^{tot}=r+\\gamma\\max_{\\boldsymbol{u}'}Q^{tot}(\\boldsymbol{\\tau}',\\boldsymbol{u}',s';\\theta^-)$ and $\\theta^-$ is the parameter of the target network~\\cite{DQN}.\n\n\n\n\n\n\n\n\n\n\n\n% \\begin{algorithm}[tb]\n% \\caption{S2RL / Deep MARL Algorithms}\n% \\hspace*{0.02in} % \n% \\textbf{Initialize}: Replay buffer $\\mathcal{D}$, central mixing network $\\theta_{\\rho}$, target central mixing network $\\hat{\\theta}_{\\rho}$ = $\\theta_{\\rho}$, local network $\\theta_{\\tau}$ for agent ${1, \\dots, n}$\n% \\begin{algorithmic}[1] \n% \\FOR{$episode = 1$ to $M$}\n% \\STATE $s_0$ = initial state, ${o}_{0}^i=\\left[O\\left({s}^{0}, i\\right)\\right]_{i=1}^{N}$ and $h^0_i = 0$ for each agent $i$, $t = 0$\n% %\\STATE \n% \\WHILE{$s \\neq terminal$ and $t < T$} \n% % \\emph{For deep pMeta-RL only}\n% \\STATE $t = t + 1$\n% \\FOR{ each agent $i$}\n% \\STATE Calculate dense attention $O_{sta}$ by \\eqref{}\n% \\STATE Calculate sparse attention $O_{aux}$ by \\eqref{}\n% \\STATE $Q_{i}^{sta}\\left(\\tau_{i}, \\cdot\\right) = {agent}_i\\left(O_{sta}, GRU(o_i, h_i^{t-1})\\right)$\n% \\STATE $Q_{i}^{aux}\\left(\\tau_{i}, \\cdot\\right) = {agent}_i\\left(O_{aux}, GRU(o_i, h_i^{t-1})\\right)$\n% \\STATE Sample $a_i^t$ from $\\pi\\left(Q_{i}(o_{t}^{i},), \\epsilon(episode)\\right)$\n% \\ENDFOR\n% \\STATE Execute actions $a = (a_1, \\dots, a_n)$\n% \\STATE Receive reward $r_{t+1}$ and next state ${s}^{t+1}$\n% % \\ENDWHILE\n% \\STATE Store episodes in replay buffer $\\mathcal{D}$\n% \\STATE Sample a random minibatch of episodes from $\\mathcal{D}$\n% \\FOR{$r = 1, \\cdots, R$}\n% % \\STATE Sample a batch data from $\\mathcal{D}_i$ and context $\\mathbf{c}^i$\n% % \\STATE Sample $\\mathbf{z} \\sim q_{\\zeta}(\\mathbf{z}|\\mathbf{c}^i)$ and calculate $J_{\\pi_i}(\\phi_i), J_{Q_i}(\\theta_i), D_{\\text{KL}}(q_{\\zeta}(\\mathbf{z}|\\mathbf{c}^{i} \\big\\| p(\\mathbf{z}))$\n% \\STATE \\textbf{Personalized policy update:} \\\\ Update $Q_i^{\\pi_i}/(\\phi_{i,r}^c, \\theta_{i,r}^c, \\zeta_{i,r}^c)$ by \\eqref{per_update0} or \\eqref{per_update}\n% \\STATE \\textbf{Auxiliary meta-policy update:} \\\\ Update $Q_i^{\\pi}/(\\omega_{i,r}^c, \\vartheta_{i,r}^c, \\varpi_{i,r}^c)$ by\n% \\eqref{local_update0} or \\eqref{local_update}\n% \\ENDFOR\n% \\ENDFOR\n% \\STATE Update meta-policy $Q^{\\pi} / \\left(\\omega^c, \\vartheta^c, \\varpi^c\\right)$ by \\eqref{global_update0} or \\eqref{global_update}\n% \\ENDFOR\n% \\STATE \\textbf{Output}: meta-policy $Q^{\\pi} / \\left(\\omega, \\vartheta, \\varpi\\right)$, personalized policies $Q_i^{\\pi_i} / \\left(\\phi_i, \\theta_i, \\zeta_i\\right), i=1,\\cdots, N $\n% \\end{algorithmic}\n% \\label{alg2}\n% \\end{algorithm}\n\n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Sparse state based MARL",
                "content": "\\label{sec4}\n\nIn this section, we propose a novel sparse state based MARL framework that is general enough to be plugged into any existing value-based multi-agent algorithm.\n% In the follows, we detail the proposed S2RL  method as shown in Figure~\\ref{fig:method}. \nAs shown in Figure~\\ref{fig:method}, our framework adopts the CTDE paradigm, where each agent learns its individual utility network by optimizing the TD loss of the mixing network. During the execution, the mixing network is removed, and each agent acts according to its local policy derived from its value function. \nDistinguish from other value-based methods, our agents’ value functions or policies carry out the process of selection and discrimination according to the importance of different entities of state.\nTo enable efficient and effective learning among agents between different entities of state, our method be described by three steps: \n% \\textcolor{red}\n{1) selection; 2) discrimination; 3) learning.}\n\n",
                "subsection 3.1": {
                    "name": "Selection and Discrimination",
                    "content": "\nIt is a dynamic process to assign attentions based on the contribution of the observed entities to the value estimation.\nIn our framework, we adopt the self-attention module ~\\cite{attention} to capture the relational weights between the observed entities of the agents.\n% Assigning different attention to entities of observation according to their contributions for  estimation of value functions is a dynamic process. In our framework, we adopt the self-attention module~\\cite{attention} to capture the relationship weights between agent' observation entities.\nIn particular, an agent $i$ observes $M$ other entities at time step $t$, then the corresponding input of utility network $O_t^i$ is defined as $O_t^i = [{\\boldsymbol{o}^{i, 1}_t},\\ldots,{\\boldsymbol{o}^{i, M}_t}]^T \\in \\mathbb{R}^{{M} \\times d_E}$ with $d_E$ being the entity dimension and $o^{i, m}_t \\in \\mathbb{R}^{d_E}$ being the state information of the $m$-th ($m\\in \\{1,...,M \\}$) entity. \nAll observed entities are embedded to $d_X$ dimension via an embedding function $f(\\cdot): \\mathbb{R}^{d_E} \\rightarrow \\mathbb{R}^{d_X}$ as follows:\n\\begin{align}\nX_{t}^{i}=\\left\\{f(\\boldsymbol{o}_{t}^{i, 1}), \\ldots, f(\\boldsymbol{o}_{t}^{i, M})\\right\\},~i \\in \\mathcal{I}.\n\\end{align}\n% All observed entities are embedded to $d_X$ dimension via an embedding function $f(\\cdot): \\mathbb{R}^{d_E} \\rightarrow \\mathbb{R}^{d_X}$, denoted as\n% $X_{t}^{i}=\\left\\{f(\\boldsymbol{o}_{t}^{i, 1}), \\ldots, f(\\boldsymbol{o}_{t}^{i, M})\\right\\},~i \\in \\mathcal{I}.$\n% % follows:\n% % \\begin{align}\n% % X_{t}^{i}=\\left\\{f(\\boldsymbol{o}_{t}^{i, 1}), \\ldots, f(\\boldsymbol{o}_{t}^{i, M})\\right\\},~i \\in \\mathcal{I}.\n% % \\end{align}\nThen, the embedding feature of each agent ${X} \\in \\mathbb{R}^{{M} \\times d_X}$ is projected to query $Q = X {W}_{Q}$, key $K = X {W}_{K}$ and value $V = X {W}_{V}$ representation, where $\\{{W}_Q, {W}_K, {W}_V\\} \\in \\mathbb{R}^{d_X \\times d_X}$ are trainable weight matrices.\nThen, $Q$, $K$, $V$ are input into the self-attention layer to calculate the entities importance for the model decision, which is given by\n\\begin{align}\n \\operatorname{Attn}(Q, K, V)= \\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{X}}}\\right) V.\n\\end{align}\n\nOne of the limitations of the softmax activation function is that the resulting probability weights for any element never appear to be zero, which further leads to dense output probabilities. \n% However, in reinforcement learning, it is crucial for agents to learn to select valuable observations, which reduces the number of entities that agent needs to pay attention to, simplifying the exploration space.\nNevertheless, for the sake of simplifying the exploration space and selecting valuable observations, it is crucial for agents to reduce the number of entities to focus on.\nHence, a sparse probability distribution is desired to distinguish between critical and irrelevant entities, which can\taccelerate convergence and improve performance.\nTo start with, inspired by sparsemax~\\cite{sparsemax,entmax}, we consider introducing sparse states to enhance the perception of valuable entities of agent observation and neglect the others.\n\n% A limitation of the softmax activation function is that the resulting probability weights of any element never appear to be zero, resulting in dense output probabilities. In practice, it is crucial for agents to learn to select the valuable observation information, which implies a sparse probability distribution is desired to discriminate between actual states and secondary. Thus we introduce sparsemax~\\cite{sparsemax} as an alternative to replace the softmax in the attention mechanism. \n% Sparsemax commonly defines a threshold below which small probability values are truncated to zero. \nWe denote the products of the query with all keys ${Q K^{T}}$ as $Z \\in \\mathbb{R}^{M \\times M}$, which consists of $M$ rows $\\{{\\boldsymbol{z}_1}, \\ldots, {\\boldsymbol{z}_M}\\}$ with ${\\boldsymbol{z}_m} \\in \\mathbb{R}^{M}$ being the logits of the $m$-th row.\nAfterwards, we define a matrix sorting operator $\\operatorname{SortMat}(\\cdot)$ as follows:\n\\begin{align}\n\\widetilde{Z} = \\operatorname{SortMat}(Z) = [\\operatorname{SortVec}(\\boldsymbol{z}_1)^T,\\dots,\\operatorname{SortVec}(\\boldsymbol{z}_M)^T]^T,\n\\end{align}\nwhere $\\operatorname{SortVec}(\\cdot)$ sorts the elements of vector in descending order. \n% For example, $\\operatorname{SortVec}(\\boldsymbol{z}_m) = \\{z\\in [\\boldsymbol{z}_m] \\mid z_{j} > z_{k},~\\text{if}~j > k\\}$.\nThen we calculate\n\\begin{align}\n\\boldsymbol{n}(\\widetilde{Z}) = [n_1,\\dots,n_M]^T,\n\\end{align}\nwhere $n_m := \\max{\\{n \\in [M] \\mid 1+n \\widetilde{Z}_{m,n} > \\sum_{k \\leq n} \\widetilde{Z}_{m,k} \\}}$ is the maximal number of crucial elements in $\\boldsymbol{z}_m$ that we intend to preserve, while other elements is set to zero in the subsequent operations. \nWe define\n\\begin{align}\n\\boldsymbol{c} = [c_{1},\\cdots,c_{M}]^T\n\\end{align}\nwith $c_{m} = \\sum_{k \\leq n_m} Z_{m,k} -1$ and the scaling vector as\n\\begin{align}\n\\boldsymbol{\\mu} = [\\frac{1}{n_1},\\cdots,\\frac{1}{n_M}]^T.\n% \\begin{bmatrix} \n% \\frac{1}{n_1} & 0 & \\cdots & 0 \\\\ \n% 0 & \\frac{1}{n_2} & \\cdots  & 0 \\\\\n% \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n% 0 & 0 & \\cdots  & \\frac{1}{n_M}\n% \\end{bmatrix}.\n\\end{align}\nThen, the threshold matrix is calculated as\n\\begin{align}\n\\Delta = \\boldsymbol{1}_{M\\times 1} (\\boldsymbol{c} \\odot \\boldsymbol{\\mu})^T,\n\\end{align}\nwhere $\\boldsymbol{1}_{M\\times 1} \\in \\mathbb{R}^{M}$ is an all-one vector and $\\odot$ is the pointwise product. The sparse attention weights matrix $P$ is obtained by\n\\begin{align}\nP = [Z - \\Delta]_{+},\n\\end{align}\nwhere $[\\cdot]_{+}:= \\max \\{0, \\cdot \\}$.\n% The element in $Z$ above the corresponding coordinate in $\\Delta$ will be shifted by this amount, and the others less will be truncated to zero.\nThus, the sparse attention is given by\n\\begin{align}\n \\operatorname{sAttn}(Q, K, V)= P V,\n\\end{align}\nwhich can retain most of the essential properties of softmax while assigning zero probability to low-scoring choices. Therefore, the model will pay more attention to critical entities when making decisions, reducing the attention to other redundant entities.\n\n% Therefore, the model decision can based on the critical entities and other redundant entities are discarded.\n\n\n\n\n% Sparsemax is a new activation function similar to the traditional softmax, but the difference is that it can output sparse probabilities. Thus we use sparsemax as activation function to replace the softmax in the attention mechanism. \n\n% $P=\\operatorname{sparsemax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$\n\n% This sparse attention mechanism enables us to focus on essential state of token, which helps agents learn more efficiently and effectively. \n\n\n\n\n\n\n\n\n\n\n\n\\begin{algorithm}[!tb]\n\\caption{Sparse State based MARL Algorithm}\n\n\\textbf{Initialize}: Critic network $\\theta_{\\rho}$, target critic $\\hat{\\theta}_{\\rho}$ = $\\theta_{\\rho}$, agents' Q-value networks $\\theta_{\\pi} = (\\theta_1, \\dots, \\theta_N)$ and Replay buffer $\\mathcal{D}$\n\\begin{algorithmic}[1] \n\\For{each training episode $e$}\n\\State $t = 0$, $s_0$ = initial state, ${o}_{0}^i=G({s}_{0}, i)$ and $h^i_0 = 0$ for $i \\in \\mathcal{N}$\n%\\STATE \n\\While{$s \\neq terminal$ and $t < T$} \n% \\emph{For deep pMeta-RL only}\n\\State $t = t + 1$\n\\For{ each agent $i$}\n\\State Calculate dense attention $Y_{dense}$ by \\eqref{Osta}\n\\State Calculate sparse attention $Y_{sparse}$ by \\eqref{Oaux}\n\\State Calculate trajectory encode $h_t^i$ by \\eqref{h}\n\\State Obtain $Q_{dense}^{i}(\\tau^{i}, \\cdot)$ by \\eqref{Qsta}\n\\State Obtain $Q_{sparse}^{i}(\\tau^{i}, \\cdot)$ by \\eqref{Qaux}\n% \\State $Q_{dense}^{i}\\left(\\tau^{i}, \\cdot\\right) = {agent}^i\\left(Y_{dense}, h_t^i\\right)$\n% \\State $Q_{sparse}^{i}\\left(\\tau^{i}, \\cdot\\right) = {agent}^i\\left(Y_{sparse}, h_t^i\\right)$\n\\State Sample $u^i$ from $\\pi^i(Q_{dense}^i, \\epsilon)$\n\\EndFor\n\\State Execute actions $\\boldsymbol{u}_t = (u^1, \\dots, u^N)$\n\\State Receive reward $r_{t+1}$ and next state ${s}_{t+1}$\n\\EndWhile\n\\State Store episodes in replay buffer $\\mathcal{D}$\n\\State Sample a random minibatch of episodes from $\\mathcal{D}$\n\\State \\textbf{Dense Attention Loss:}\n\\State \\quad \\;\\; Compute $\\mathcal{L}_{td}(\\theta_{\\pi}, \\theta_{\\rho})$ by \\eqref{Lsta}\n\\State \\textbf{Auxiliary Sparse Attention Loss:}\n\\State \\quad \\;\\; Compute $\\mathcal{L}_{aux}(\\theta_{\\pi}, \\theta_{\\rho})$ by \\eqref{Laux}\n\\State Update $\\theta_{\\pi}$ and $\\theta_{\\rho}$ by \\eqref{totalloss}\n\\State Every $C$ episodes reset $\\hat{\\theta}_{\\rho}$ = $\\theta_{\\rho}$\n\\EndFor\n\\end{algorithmic}\n\\label{algo}\n\\end{algorithm}\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Learning with Sparse Loss",
                    "content": "\n% The direct may to achieve sparse attention mechanism is to replace the traditional self-attention activation function with sparsemax. However, the model can not distinguish which entity is more important at the beginning of training. To mitigate this issue, we utilize sparse attention mechanism shown in the right side of Figure~\\ref{fig:method} as auxiliary to guide  the training of local agents.  \nObviously, the sparse attention mechanism can be realized by directly replacing the traditional self-attention activation function with a sparse distribution function. However, the model cannot distinguish which entity is vital from the beginning. Thus directly adopting the sparse attention mechanism will have performance regression. To address this issue, we design the structure shown on the right side of Figure ~\\ref{fig:method} to guide the training of local agents, where we utilize two routes to exploit dense and sparse attention, respectively. Dense attention guarantees that the algorithm can converge, while sparse attention is a powerful auxiliary to enhance the agent perception of critical entities, thereby improving performance.\n\nTo do this, the sparse attention module and dense attention module share the weight matrices $\\{{W}_Q, {W}_K, {W}_V\\}$ and the GRU module. Denote the parameters of these two networks by $\\theta_{\\pi}$.\nThe projected matrix $Q$ and $K$ are fed into both dense and sparse attention. Then, we calculate the weighted sum of $V$ to obtain the output \n\\begin{equation}\\label{Osta}\nY_{dense} = \\operatorname{Attn}(Q, K, V) \\in \\mathbb{R}^{M \\times d_X},\n\\end{equation}\n\\begin{equation}\\label{Oaux}\nY_{sparse} = \\operatorname{sAttn}(Q,K,V)  \\in \\mathbb{R}^{M \\times d_X}.\n\\end{equation}\nIn our implementation, the GRU~\\cite{GRU} module is utilized to encode an agent's history of observations and actions via\n\\begin{equation}\\label{h}\nh_t^i = GRU(X_t^i, h_{t-1}^i).\n\\end{equation}\n%\\nonumber\nThen, $Y_{dense}$ and $Y_{sparse}$ are concatenated with the output of GRU separately to estimate the individual value function as follows:\n\\begin{equation}\\label{Qsta}\nQ_{dense}^{i}(\\tau^{i}, \\cdot) = \\operatorname{Agent}^i(Y_{dense}, h_{t}^i),\n\\end{equation}\n\\begin{equation}\\label{Qaux}\nQ_{sparse}^{i}(\\tau^{i}, \\cdot) = \\operatorname{Agent}^i(Y_{sparse}, h_{t}^i).\n\\end{equation}\n% Action selection is performed by each agent maximizing $Q_{dense}^{i}$ and $Q_{sparse}^{i}$ for subsequent computation of centralized training. Moreover, the action selected by $Q_{dense}^{i}$ is executed in the  environment.\n% For the exploration policy, $\\epsilon$-greedy is adopted and the exploration rate of episode $\\epsilon$ decreases over time.\n% Action selection is performed by each agent to maximize $Q_{dense}^{i}$ and $Q_{sparse}^{i}$ for subsequent computations in centralized training. \nEach agent selects the action that maximizes $Q_{dense}^{i}$ and $Q_{sparse}^{i}$ for subsequent computations in centralized training. \nIn addition, the action selected by $Q_{dense}^{i}$ is executed in the environment. For the exploration strategy, $\\epsilon$-greedy is adopted, and the exploration rate of $\\epsilon$ decreases over time.\n\n\n\n\nTo better learn the role of entities in credit assignment, we use a mixing network to estimate the global Q-values $Q_{dense}^{tot}$ and $Q_{sparse}^{tot}$, using per-agent utility $Q_{dense}^{i}$ and $Q_{sparse}^{i}$. Since the auxiliary estimation is calculated in the individual utility function, our proposed S2RL is seamlessly integrated with various valued-based algorithms. \nFor example, we can use the mixing network, a feed-forward neural network introduced by QMIX~\\cite{QMIX}. The mixing network mixes the agent network outputs monotonically.\nThe parameters of the mixing network parameterized by $\\theta_{\\rho}$ are conditioned on the global states and are generated by a hyper-network. Then, we minimize the following TD loss to update the dense attention module:\n\\begin{equation}\\label{Lsta}\n\\mathcal{L}_{td}\\left(\\theta_{\\pi}, \\theta_{\\rho}\\right)=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(r+\\gamma \\max _{\\boldsymbol{u}^{\\prime}} \\bar{Q}_{dense}^{tot}\\left(s^{\\prime}, \\boldsymbol{u}^{\\prime}\\right)-Q_{dense}^{tot}(s, \\boldsymbol{u})\\right)^{2}\\right],\n\\end{equation}\nwhere $\\bar{Q}_{dense}^{tot}$ is the target network, and the expectation is estimated with uniform samples from the same replay buffer ${\\mathcal{D}}$.\nIn the meanwhile, the AUX Loss is given by\n\\begin{equation}\\label{Laux}\n\\mathcal{L}_{aux}\\left(\\theta_{\\pi}, \\theta_{\\rho}\\right)=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(r+\\gamma \\max _{\\boldsymbol{u}^{\\prime}} \\bar{Q}_{sparse}^{tot}\\left(s^{\\prime}, \\boldsymbol{u}^{\\prime}\\right)-Q_{sparse}^{tot}(s, \\boldsymbol{u})\\right)^{2}\\right],\n\\end{equation}\nwhere $\\bar{Q}_{tot}^{aux}$ is the auxiliary target network.\n% and the expectation is estimated with uniform samples from the same replay buffer ${\\mathcal{D}}$ as the one for the TD Loss.\n\n\n\n\n\n\n% \\subsection{Overall Framework}\nIn our framework, S2RL services as a plug-in module in the agent utility networks. The outputs of S2RL modules are directly used for subsequent network computations. Then, each agent is trained by minimizing the total loss\n\\begin{equation}\\label{totalloss}\n    \\mathcal{L}\\left(\\theta_{\\pi}, \\theta_{\\rho}\\right) = \n     \\mathcal{L}_{td}(\\theta_{\\pi}, \\theta_{\\rho}) + \\lambda  \\mathcal{L}_{aux}(\\theta_{\\pi}, \\theta_{\\rho}),\n\\end{equation}\nwhere $\\lambda$ is a regularization parameter that controls the level of attention to critical states. Obviously, a larger $\\lambda$ allows our algorithm to pay more attention to some critical states, while a smaller $\\lambda$ allows for a more even distribution of attention. The overall framework is trained in an end-to-end centralized manner.\nThe complete algorithm is summarized in Algorithm ~\\ref{algo}.\n\n\n\n\n\n\n% During decentralized execution, the sparse attention module is removed, which means that agents infer by dense attention solely.\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "EXPERIMENTS",
                "content": "\nWe conduct experiments on the StarCraft Multi-Agent Challenge (SMAC)\\footnote{We use the SC2.4.10 version instead of the older SC2.4.6.2.69232. Performance is not comparable between different versions.}~\\cite{SMAC} to demonstrate the effectiveness of the proposed sparse state based MARL (S2RL) method. SMAC has become a standard benchmark for evaluating state-of-the-art MARL methods, which focuses on micromanagement challenges. The setup of SMAC is that each ally entity is controlled by an individual learning agent, while the enemy entities are controlled by a built-in AI. At each time step, agents can move in four cardinal directions, stop, take no-operation, or choose an enemy to attack. Thus, if there are $n_e$ enemies in the scenario, the action space for each ally unit consists of $n_e + 6$ discrete actions. Agents aim to inflict maximum damage on enemy entities to win the game. Therefore, proper tactics such as focusing fire and covering attack are required during battles. Learning these diverse interaction behaviors under partial observation is a crucial yet challenging task. In what follows, we detail the compared methods and parameter settings and then present the qualitative and quantitative performance of different methods.\n% We conduct experiments  on the StarCraft Multi-Agent Challenge (SMAC)\\footnote{We use SC2.4.10 version instead of the older SC2.4.6.2.69232. Performance is not comparable across versions.}~\\cite{SMAC} to demonstrate the effectiveness of the proposed sparse state based MARL (S2RL) method.\n% SMAC has become a common-used benchmark for evaluating state-of-the-art MARL approaches, which focuses on micromanagement challenges.\n% The setup of SMAC is that each of the ally entities is controlled by an individual learning agent, and enemy entities are controlled by a built-in AI. \n% At each timestep, agents can move in four cardinal directions, stop, take noop (do nothing), or select an enemy to attack. Therefore, if there are $n_e$ enemies in the map, the action space for each ally unit consists of $n_e + 6$ discrete actions.\n% Agents aims to inflict maximum damage on the enemy entities for winning the game.\n% Hence proper tactics such as focusing fire and avoiding overkill are required during battles. Learning these diverse interaction behaviors under partial observation is a challenging task. In the following, we will introduce the environments and the parameters, and then both qualitative and quantitative performances of different methods.\n\n",
                "subsection 4.1": {
                    "name": "Comparison Methods and Training Details",
                    "content": "\nOur method is compared with several baseline methods, including IQL, VDN~\\cite{VDN}, QMIX~\\cite{QMIX}, QTRAN~\\cite{QTRAN}, QPLEX~\\cite{QPLEX}, CWQMIX and OWQMIX ~\\cite{WQMIX}.\nOur S2RL implementation uses VDN, QMIX and QPLEX as an integrated architecture to verify its performance, called S2RL~(VDN), S2RL~(QMIX) and S2RL~(QPLEX).\nThese three SOTA methods are chosen for their robust performance in different multi-agent scenarios, while S2RL can also be easily applied to other frameworks.\n\n% Our methods are compared with several baseline methods, including IQL, VDN~\\cite{VDN}, QMIX~\\cite{QMIX}, QTRAN~\\cite{QTRAN}, QPLEX~\\cite{QPLEX}, CWQMIX and OWQMIX~\\cite{WQMIX}. \n% Our S2RL implementation uses VDN, QMIX and QPLEX as incorporated architecture to validate its performance, termed as S2RL(VDN), S2RL(QMIX), S2RL(QPLEX).\n% These three SOTA methods were selected for their robust performance across different multi-agent scenarios, while S2RL can be easily applicable to other frameworks.\n\n\n\n\n\n\nWe adopt the Python MARL framework (PyMARL)~\\cite{SMAC} to run all experiments.\nThe hyperparameters of the baseline methods are the same as those in PyMARL to ensure comparability.  The regularization parameter in \\eqref{totalloss} is set to $\\lambda = 1$.\nFor all experiments, the optimization is conducted using RMSprop with a learning rate of $5 \\times 10^{-4}$, a total timestep of $2$M, a smoothing constant of $0.99$, and no momentum or weight decay. For exploration, we use $\\epsilon-$greedy with $\\epsilon$ annealed linearly from $1.0$ to $0.05$ over $50K$ time steps and kept constant for the rest of the training. For four super hard exploration maps ({6h\\_vs\\_8z}, {3s5z\\_vs\\_3s6z}, {corridor}, {5s10z}), we extend the epsilon annealing time to $500K$ and the total timestep to $5M$, and three of them ({6h\\_vs\\_8z}, {corridor}, {5s10z}) optimized with Adam for both series of S2RL and all the baselines and ablations. Batches of $32$ episodes are sampled from the replay buffer, and all tested methods are trained end-to-end on fully unrolled episodes. All experiments on the SMAC benchmark use the default reward and observation settings of the SMAC benchmark~\\cite{SMAC}. All experiments in this section were carried out with $5$ different random seeds on NVIDIA GTX V100 GPU.\n\n% , and we randomly select $5$ experimental results for plotting the learning curves.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Overall Results",
                    "content": "\nTo demonstrate the efficiency of our proposed method, we conduct experiments on 6 challenging SMAC scenarios, which are classified into \\textbf{Easy} (\\emph{3s5z}), \\textbf{Hard} (\\emph{3s\\_vs\\_5z}) and \\textbf{Super-Hard} (\\emph{6h\\_vs\\_8z}, \\emph{3s5z\\_vs\\_3s6z}, \\emph{corridor}, \\emph{5s10z}). All of these scenarios are heterogeneous, where each army is composed of more than one entity type. It is worth mentioning that MARL algorithms are harder to converge on hard and super-hard maps and therefore need to focus more on important entities to speed up convergence. In this way, we are more interested in the performance of our method on these maps.\n% Since hard and super hard maps are more typically needed to learn which entities within the observation have a greater impact on the decision, which is the motivation of the S2RL, we are especially interested in the performance of our method on these maps.\n% We compare the proposed S2RL(VDN), S2RL(QMIX), S2RL(QPLEX) with various MARL baselines: IQL, VDN, QMIX, QTRAN, QPLEX, OWQMIX and CWQMIX. \n\n\n\n\n\nFigure~\\ref{fig:baseline} shows the overall performance of the tested algorithms in different scenarios. The results include the median performance and $25-75\\%$ percentiles are shaded to avoid the effect of any outliers as recommended in~\\cite{SMAC}. For the sake of demonstration, here we select the best plug-in method, referred to as S2RL in the following, to compare with other baseline algorithms.\nFirst of all, we can see that S2RL performs best on up to all six tasks, which means our proposed method can efficiently enhance the performance of agents in different scenarios. In the easy map, some algorithms have achieved good performance, and our S2RL is not significantly ahead. In contrast, our S2RL significantly improves the learning efficiency and final performance compared to the baselines in some hard and super-hard scenarios.\n% In the easy map, a serious of algorithms have achieved great performance, while our proposed S2RL significantly improves the learning efficiency and the final performance compared with baselines in both hard and super hard scenarios.\nSpecifically, in \\emph{6h\\_vs\\_8z} and \\emph{3s5z\\_vs\\_3s6z}, our S2RL consistently outperforms baselines by a large margin during training. \n% We hypothesize that agents do not require determining which entities are more critical on easy maps and can still find suitable strategies to defeat the enemy. The selection benefit brought by the sparse attention mechanism is not apparent. However, when the situation becomes more intricate, agents need to consider which entities are more critical to making decisions.\nThis is because the number of entities in easy maps is small, all entities are critical, and the selection gain brought by the sparse attention mechanism is not apparent. However, when the situation becomes more complex, and the agent needs to consider which entities are more critical to the decision, the benefits of the sparse attention mechanism are more pronounced.\n\n\n\n\n\n\n\n\n\n\n\n\n% \\paragraph{Robust Results}\nIn addition, to test the generalization of our method incorporated into various valued-based algorithms, we incorporate S2RL to VDN, QMIX and QPLEX respectively, and compare the final performance with vanilla agent utility networks in Figure~\\ref{fig:plugin}. In general, most of the learning curves of S2RL~(VDN), S2RL~(QMIX) and S2RL~(QPLEX) achieve gratifying results superior to VDN, QMIX and QPLEX.\nBesides, it is worth mentioning that our method pulls huge margins on tasks with more severe difficulties, demonstrating the effectiveness of S2RL. The experimental results show that in the super-hard map \\emph{6h\\_vs\\_8z}, our proposed S2RL~(QPLEX) improves the win rate by almost $55\\%$ compared to the naive QPLEX. Even more encouraging is that S2RL~(QMIX) can reach a win rate of $80\\%$ while QMIX basically does not learn any strategy.\n\n% In addition, the promotion of incorporating S2RL to QMIX and QPLEX is higher than VDN, which is relevant to the mixing network. We hypothesize that sparse attention mechanism select the critical entities and then discriminate their contributions to the estimation of value functions, which may promote the ability of credit assignment. Unlike QMIX and QPLEX, VDN represents joint action-value as a summation of agents’ individual Q-functions resulting in this poor representation of mixing network challenging to leverage the strengths of our approach.  \n\nFurthermore, the promotion of incorporating S2RL into QMIX and QPLEX is higher than VDN, which reveals the importance of the mixing network. We hypothesize that the sparse attention mechanism enables the model to select critical entities and further clarify their contributions, which may promote the power of credit assignment. Unlike QMIX and QPLEX, VDN represents the joint action-value as a summation of individual Q-functions, resulting in this poor representation of the mixing network challenging to leverage the strengths of our approach.  \n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Study",
                    "content": "\n% \\paragraph{Ablation Results}\n% To demonstrate the advantage of sparse auxiliary loss to the training process of agents, we carry out ablation studies to test its contribution.\n% three super hard maps (\\emph{6h\\_vs\\_8z}, \\emph{3s5z\\_vs\\_3s6z}, \\emph{corridor})\nTo evaluate the advantage of sparse auxiliary loss on the agent training process, we conduct ablation studies on three super hard maps (\\emph{5s10z}, \\emph{6h\\_vs\\_8z}, \\emph{3s5z\\_vs\\_3s6z}) to test its contribution.\nOur S2RL mainly consists of two parts: (A) dense attention, denoted Attn; (B) sparse attention as an auxiliary, noted as S2RL. We apply these two components to VDN, QMIX and QPLEX utility networks and compare their performance in Figure~\\ref{fig:ablation}.\n% Our proposed method S2RL consists of two mainly components: (A) Dense attention, noted as Attn; (B) Sparse attention as auxiliary, noted as S2RL. We apply these two components into VDN, QMIX, QPLEX utility network and compare their performance in Figure~\\ref{fig:ablation}. \nThe solid curves indicate that the agents use the dense attention module to calculate the importance of different entities. The dashed curves indicate that the agents learn to use the sparse attention module as an auxiliary to teach the dense attention module.\n% The solid curves represent that the agents use dense attention mudule to calculate the importance of different entities. The dotted curves represent that the agents learn to use sparse attention module as auxiliary to teach the dense attention mudule.\n\n% Generally speaking, the advantages of using S2RL become apparent gradually in the middle and late stages of training. We hypothesize that agents can not distinguish which entity is more important when the training starts. As the training progressed, agents explore more unknown states and are gradually able to distinguish which entities are more critical. \nGenerally speaking, the advantages of using S2RL gradually emerge in the middle and late stages of training. We assume that agents cannot distinguish which entity is more important at the beginning of training. As training progresses, agents explore more unknown states and are gradually able to distinguish which entities are more critical. Finally, the overall performance of agents is improved when they discard irrelevant entities.\nFurthermore, we find that using sparse attention achieves more significant improvements on \\emph{6h\\_vs\\_8z} and \\emph{corridor}.\nOn the \\emph{6h\\_vs\\_8z} scenario, $6$ Hydralisks face $24$ enemy Zealots, while on the \\emph{corridor} scenario, $6$ Zealots face $24$ enemy Zerglings.\nThe controllable agents in these scenarios are homogeneous, making it easier for them to explore cooperative strategies. Moreover, using the sparse attention module helps simplify the exploration space, making S2RL more advantageous in these scenarios.\n\n% Finally, the overall performance of agents get an improvement when they discard irrelevant entities.\n% Furthermore, we find that compared with \\emph{3s5z\\_vs\\_3s6z}, the use of sparse attention achieve more improvement on \\emph{6h\\_vs\\_8z} and \\emph{corridor}. \n% In \\emph{6h\\_vs\\_8z} map, $6$ Hydralisks face $24$ enemy Zealots, and in \\emph{corridor}, $6$ Zealots face $24$ enemy Zerglings.\n% The controlable agents in these scenarios are homogeneous, which makes them easier to explore collaborative strategies. Thus, the use of sparse attention module helps simplify the search strategy space, resulting in S2RL being more advantageous in these scenarios.\n\n\n% The results suggest that utilizing sparse attention mechanism as auxiliary loss can explore the diverse unknown entities of observations, which helps the agents to construct a more useful policy and achieves non-trivial performance.\n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Action Representations",
                    "content": "\n% Figure~\\ref{fig:visual} visualize the final trained model S2RL(QMIX) on SMAC corridor map to better explain why our approach performs well. In this super hard map, six friendly Zealots are facing 24 enemy Zerglings. The disparity in quantity means our agents need to learn cooperative strategies, such as moving, pulling, focusing fire, etc. Otherwise, agents are doomed to lose if they gather together. \n\nFigure ~\\ref{fig:visual} visualizes the final trained model S2RL~(QMIX) on the SMAC corridor scenario to better explain why our method performs well. In this super-hard scenario, 6 friendly Zealots face 24 enemy Zerglings. \nThe disparity in quantity means that our agents need to learn cooperative strategies such as moving, pulling and focusing fire. Otherwise, agents are doomed to lose if they gather together. \n\n% As shown in Figure~\\ref{fig:visual}(a), at the beginning of the game, Zealots $0$, whose route is highlighted in green, becomes a warrior leaving the team separately to attract the attention of most enemies in the green oval. Thus the others Zealots can eliminate a small part of the enemies in the red oval with higher probability of winning.\n\n\nAs shown in Figure~\\ref{fig:visual}(a), the game starts with the Zealots $0$ highlighted in green as a warrior, leaving the team separately to grab the attention of most of the enemies in the green oval. Thus other zealots can eliminate a small number of enemies in the red oval with a high probability of winning. In Figure~\\ref{fig:visual}(b), we can see that Zealots $1$, Zealots $2$ and Zealots $5$ are focusing fire on the enemy, thus speeding up the eradication of the enemy. In the meanwhile, Zealots $4$ stands out to attack enemies surrounding their teammates from a distance. These sophisticated strategies reflect that Zealots $4$ has a better sense of the situation and knows what it should do to protect its teammates. In the next time step, we recognize that Zealots $0$ is constantly moving to avoid being attacked, and the enemy marked by the red oval is successfully drawn and walking towards our team (see Figure~\\ref{fig:visual}(c)). Although doomed to sacrifice, Zealots $0$ gives teammates plenty of time to annihilate scattered enemies and rescue Zealots $0$. All in all, S2RL can effectively allow agents to immediately focus on critical entities and make decisions, especially in more intricate scenarios.\n\n% In Figure~\\ref{fig:visual-b} we can noted that Zealots $1$, Zealots $2$ and Zealots $5$ are focusing fire to attack the enemy, which speeds up the eradication of the enemy. In the meanwhile, Zealots $4$ stands out to attack the enemy who surrounded his teammates in the distance. These sophisticated strategies reflect that Zealots $4$ has a better judgment of the situation and knows what he should do to protect his teammates.\n\n\n% In the next time step, we recognize that Zealots $0$ moved constantly to avoid being attacked and the enemy labeled in the red oval is being attracted successfully and coming towards our team, as shown in Figure~\\ref{fig:visual-c}. Thus, we have enough time to Although doomed to sacrifice, he brings enough time for his teammates to annihilate the scattered enemies and rescue Zealots $0$.  To sum up, S2RL is effective for agents to focus immediate attention on critical entities and make decisions, especially in the more intricate scenarios.\n\n\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Related works",
                "content": "\n",
                "subsection 5.1": {
                    "name": "Value-based Methods in MARL",
                    "content": "\n\nRecently, value-based methods have been applied to multi-agent scenarios to solve complex Markov games and have achieved significant algorithmic progress. VDN~\\cite{VDN} represents the joint action-value as a summation of individual value functions. Due to its poor expression factorization, QMIX~\\cite{QMIX} improves VDN~\\cite{VDN} by using a mixing network for nonlinear aggregation while maintaining the monotonic relationship between centralized and individual value functions. Moreover, weighted QMIX~\\cite{WQMIX} adapts a twin network and encourages underestimated actions to alleviate the risk of suboptimal outcomes. The monotonic constraints of QMIX and similar methods lead to provably poor exploratory and suboptimal properties. To address the structural limitations, QTRAN~\\cite{QTRAN} constructs regularizations with linear constraints and relaxes them with a $\\ell_2$-norm penalty to improve tractability, but its constraints are computationally intractable. MAVEN~\\cite{MAVEN} relaxes QTRAN~\\cite{QTRAN} by two penalties and introduces a hierarchical model to coordinate diverse explorations among agents. In \\cite{QPLEX}, a duplex dueling network architecture is introduced for factoring joint value functions, which achieves state-of-the-art on a range of cooperative tasks. Additionally, some more advanced methods~\\cite{ROMA,RODE} introduce role-oriented frameworks to decompose complex MARL tasks. In general, these methods mainly focus on aggregating local agent utility networks into a central critic network, while our method improves the structure of individual agent networks for more robust performance.\n\n% Recently, value-based methods have been applied to multi-agent scenarios to solve complex Markov games and great algorithmic advances have been achieved.\n% % A major breakthrough has been achieved by decomposing Q-functions into monotonic combinations of per-agent utilities.\n% % Value-decomposition networks\n% VDN~\\cite{VDN} express the joint action-value as a summation of individual value functions. Due to its poor expressive factorization, QMIX~\\cite{QMIX} improves VDN~\\cite{VDN} by using a mixing network for non-linear aggregations while maintaining a monotonic relationship between centralized and individual value functions. Moreover, weighted QMIX~\\cite{WQMIX} adapts a twins network and encourages the underestimated actions to alleviate the risk of suboptimal results. The monotonic constraints of QMIX and similar methods lead to provably poor exploration and suboptimality. To address limitations of structure, QTRAN~\\cite{QTRAN} constructs regularizations with linear constraints and relaxes them with $L2$ penalties for tractability, but its constraints are computationally intractable. MAVEN~\\cite{MAVEN} relaxes QTRAN~\\cite{QTRAN} by two penalties and introduces a hierarchical model to coordinate diverse explorations among agents. In \\cite{QPLEX}, a duplex dueling network architecture called QPLEX for factoring joint value functions is introduced, which achieves state-of-the-art on a range of cooperative tasks. In addition, some more advanced methods~\\cite{ROMA,RODE} introduce the role-oriented framework to decompose complex MARL tasks. In general, the approaches proposed above mainly focus on aggregating the local agent utility network into the central critic network, while our method improves the structure of the individual agent network for achieving more robust performance.\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Attention Mechanism in MARL",
                    "content": "\n% Recently, MARL researchers have generated intense interest in attention models~\\cite{attention,SENet,GAT} due to their ability to extract communication channels, represent relations and incorporate information across large contexts.\n% ATOC~\\cite{ATOC} and MAAC~\\cite{MAAC} process the messages from other agents differently according to their state-dependent importance via an attention layer.\n% Moreover, TarMAC \\cite{Tarmac} also utilize a sender-receiver soft attention mechanism and multiple rounds of collaborative reasoning to allow targeted continuous communication between agents.\n% While CollaQ\\cite{zhang2020multi} consider using the attention mechanism to deal with a variable number of agents, solving dynamic reward assignment problems. \n% Qatten~\\cite{Qatten} employed a multi-head attention mechanism to compute the local action-value functions' weights and mix them to approximate the global Q-value. \n% EPC~\\cite{EPC} utilizes an attention mechanism to combine embeddings from the different observation-action encoders.\n% REFIL~\\cite{REFIL} uses multi-head attention into QMIX to generate a random mask group of agents. UPDET~\\cite{UPDeT} and LA-QTransformer~\\cite{zhou2021cooperative} decouple the policy distribution from the intertwined input observation with the aid of the transformer mechanism.\n% In the graph attention domain, G2ANet~\\cite{G2ANet} and HAMA~\\cite{HAMA} learn the relationship between agents by utilizing the graph attention mechanism. Nevertheless, these existing attention mechanisms compute the importance weights of all entities and ignore the dynamic importance between them. In this case, all participants will be assigned scores based on the dense fully-connected map, which forces the agents to pay attention to all entities.\nRecently, attention models are increasingly adopted in MARL algorithms~\\cite{attention,SENet,GAT}, since the attention mechanism is effective in extracting communication channels, representing relations, and incorporating information in large contexts. \nATOC~\\cite{ATOC} and MAAC~\\cite{MAAC} process messages from other agents differently through the attention layer according to their state-dependent importance. \nSparseMAAC~\\cite{sparsemaac} extends MAAC~\\cite{MAAC} with sparsity by directly replacing the softmax activation function in the attention mechanism with $\\gamma$-sparsemax.\n% It calculates the importance of different agents observations and actions, while we mainly focus on selecting key entities of individual observations.}\nIn addition, TarMAC \\cite{Tarmac} utilizes a sender-receiver soft attention mechanism and multiple rounds of cooperative reasoning to allow targeted continuous communication between agents. Then CollaQ\\cite{zhang2020multi} considers the use of attention mechanisms to handle a variable number of agents to solve the problem of dynamic reward distribution. \nQatten~\\cite{Qatten} employs an attention mechanism to compute the weights of local action-value functions and mix them to approximate the global Q-value. EPC~\\cite{EPC} utilizes an attention mechanism to combine embeddings from different observation-action encoders. REFIL~\\cite{REFIL} uses attention in QMIX to generate a random mask group of agents. UPDET~\\cite{UPDeT} decouple the policy distribution from intertwined input observations with the help of a transformer mechanism. \nMoreover, G2ANet~\\cite{G2ANet} and HAMA~\\cite{HAMA} construct the relationship between agents as a graph and utilize attention mechanisms to learn the relationship between agents. \n% However, these existing attention mechanisms compute the importance weights of all entities, ignoring the dynamic importance among them. In this case, all participants are assigned scores according to a dense fully connected graph, which forces agents to perceive all entities.\nHowever, most of these existing attention mechanisms compute the importance weights of all entities. In this case, all participants are assigned scores according to a dense fully connected graph, which forces agents to perceive all entities. \nSparseMAAC takes sparsity into account,  but it ignores that directly applying the sparse attention mechanism will disrupt sufficient exploration and push the algorithm towards suboptimal policies.\nIn this paper, agents learn to perceive more critical entities of observation in the decision-making process while all observation information is preserved.\n\n\n\n"
                }
            },
            "section 6": {
                "name": "CONCLUSION",
                "content": "\n% In this work, we investigate how cooperative MARL agents can benefit from extracting significant entities in the observation. We design a novel Sparse State based MARL (S2RL) algorithm which utilizes sparse attention mechanism as an auxiliary way to select the critical entities and neglect the extraneous information. Moreover, S2RL can be readily incorporated into various action-valued-based architectures such as VDN, QMIX, QPLEX, etc. Experimental results on StarCraft II micromanagement benchmarks and across different value-based backbones demonstrate that our method significantly outperforms existing cooperative MARL algorithms and achieves state-of-the-art. It is worth mentioning that our method pulls huge margins on tasks with more severe difficulties, demonstrating the effectiveness of S2RL.\nIn this work, we investigate how cooperating MARL agents benefit from extracting significant entities from observations. We design a novel sparse state based MARL algorithm that utilizes a sparse attention mechanism as an auxiliary way to select critical entities and ignore extraneous information. Moreover, S2RL can be easily integrated into various value-based architectures such as VDN, QMIX, QPLEX, etc. Experimental results on the StarCraft II micromanagement benchmark and different value-based backbones demonstrate that our method significantly outperforms existing collaborative MARL algorithms and achieves state-of-the-art. It is worth mentioning that our method pulls huge margins on complex tasks, demonstrating the effectiveness of S2RL.\nIt could be interesting to investigate the grouping between cooperating agents through sparseness for future work.\n\n% It could be interesting to investigate grouping among the cooperative agents by sparsification for future work. With this inferred knowledge, agents with the same sparse attention distribution can be regarded as a group that may conduct similar behavior patterns. In this way, agents can learn from both individual and population perspectives.\n% It could be interesting to investigate the grouping between cooperating agents through sparsification for future works. Armed with this inferred knowledge, agents with the same sparse attention distribution can be regarded as a group likely to engage in similar behavior patterns. In this way, agents can learn from the perspective of individuals and groups.\n\n\n\\begin{acks}\nThis work was supported by the National Key Research and Development Project of China (2021ZD0110400 $\\&$ 2018AAA0101900), National Natural Science Foundation of China (U19B2042), The University Synergy Innovation Program of Anhui Province (GXXT-2021-004), Zhejiang Lab (2021KE0AC02), Academy Of Social Governance Zhejiang University, Fundamental Research Funds for the Central Universities (226-2022-00064 $\\&$ 226-2022-00142), Artificial Intelligence Research Foundation of Baidu Inc., Program of ZJU and Tongdun Joint Research Lab, Shanghai AI Laboratory (P22KS00111).\n\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\balance\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{sample-base}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n% \\appendix\n\n% \\section{appendix}\n\n% \\subsection{appendix}\n\n\n"
            }
        },
        "figures": {
            "fig:ex": "\\begin{figure*}[!t]\n    \\centering\n    \\subfloat[Six friendly Hydralisks face 8 enemy Zealots.]{\\includegraphics[scale=0.50]{fig/attnhotmap/6h8z.pdf}\\label{fig:ex1}}\\;    \n    \\subfloat[Dense attention distribution]{\\includegraphics[scale=0.305]{fig/attnhotmap/6h8z_normal.pdf}\\label{fig:ex2}}\n    \\subfloat[Sparse attention distribution]{\\includegraphics[scale=0.305]{fig/attnhotmap/6h8z_sparse.pdf}\\label{fig:ex3}}\n    \\caption{A visualization example of agent performance on the StarCraft II super-hard scenario $6h\\_vs\\_8z$. As shown in (a), the closest to the green agent H3 are the red enemies Z0 and Z5. Thus the corresponding policy is that H3 only needs to focus on Z0 and Z5, which are more likely to be annihilated. (b) shows the softmax attention distribution of the H3 observations, finding that some weights are still assigned to irrelevant entities. In contrast, the sparse attention in (c) only focuses on Z0 and Z5.}\n    \\label{fig:ex}\n\\end{figure*}",
            "fig:method": "\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[scale=1]{fig/framework.pdf}\n    \\caption{Illustration of the proposed S2RL method. We use the value-based MARL framework under the CTDE paradigm and apply the S2RL method to an agent utility network. The core of S2RL is composed of the dense attention module and sparse attention module, where sparse attention serves as an auxiliary for guiding the dense attention training.}\n    \\label{fig:method}\n\\end{figure*}",
            "fig:baseline": "\\begin{figure*}[h]\n    \\centering\n    \\subfloat[3s5z]{\\includegraphics[scale=0.26]{fig/baseline/sc2_baseline_3s5z.pdf}}\n    \\subfloat[3s\\_vs\\_5z]{\\includegraphics[scale=0.26]{fig/baseline/sc2_baseline_3s_vs_5z.pdf}}\n    \\subfloat[5s10z]{\\includegraphics[scale=0.26]{fig/baseline/sc2_baseline_5s10z.pdf}}\n    \n    \\subfloat[corridor]{\\includegraphics[scale=0.26]{fig/baseline/sc2_baseline_corridor.pdf}}\n    \\subfloat[6h\\_vs\\_8z]{\\includegraphics[scale=0.26]{fig/baseline/sc2_baseline_6h_vs_8z.pdf}}\n    \\subfloat[3s5z\\_vs\\_3s6z]{\\includegraphics[scale=0.26]{fig/baseline/sc2_baseline_3s5z_vs_3s6z.pdf}}\n    \\caption{Learning curves of our S2RL and baselines on one easy map (3s5z), one hard map (3s\\_vs\\_5z), and 4 super-hard maps ({corridor}, {5s10z}, {6h\\_vs\\_8z}, {3s5z\\_vs\\_3s6z}).\n    All experimental results are illustrated with the median ($25-75\\%$ percentiles) performance and across 5 runs for a fair comparison.}\n    \\label{fig:baseline}\n\\end{figure*}",
            "fig:plugin": "\\begin{figure*}[h]\n    \\centering\n    \\subfloat[3s5z]{\\includegraphics[scale=0.26]{fig/plugin/sc2_plug_3s5z.pdf}}\n    \\subfloat[3s\\_vs\\_5z]{\\includegraphics[scale=0.26]{fig/plugin/sc2_plug_3s_vs_5z.pdf}}\n    \\subfloat[5s10z]{\\includegraphics[scale=0.26]{fig/plugin/sc2_plug_5s10z.pdf}}\n    \n    \\subfloat[corridor]{\\includegraphics[scale=0.26]{fig/plugin/sc2_plug_corridor.pdf}}\n    \\subfloat[6h\\_vs\\_8z]{\\includegraphics[scale=0.26]{fig/plugin/sc2_plug_6h_vs_8z.pdf}}\n    \\subfloat[3s5z\\_vs\\_3s6z]{\\includegraphics[scale=0.26]{fig/plugin/sc2_plug_3s5z_vs_3s6z.pdf}}\n    \\caption{The performance comparison between the vanilla methods and their S2RL variants. We integrate the proposed S2RL framework with VDN, QMIX and QPLEX.}\n    \\label{fig:plugin}\n\\end{figure*}",
            "fig:ablation": "\\begin{figure*}[h]\n    \\centering\n    \\subfloat[5s10z]{\\includegraphics[scale=0.26]{fig/ablation/sc2_ablation_5s10z.pdf}}\n    \\subfloat[6h\\_vs\\_8z]{\\includegraphics[scale=0.26]{fig/ablation/sc2_ablation_6h_vs_8z.pdf}}\n    \\subfloat[3s5z\\_vs\\_3s6z]{\\includegraphics[scale=0.26]{fig/ablation/sc2_ablation_3s5z_vs_3s6z.pdf}}\n    \\caption{Ablation studies regarding component of dense attention and auxiliary sparse attention.}\n    \\label{fig:ablation}\n\\end{figure*}",
            "fig:visual": "\\begin{figure*}[!t]\n    \\centering\n    \\subfloat[Strategy: Zealots $0$ leave the team separately to attract the attention of most enemies.]{\\includegraphics[scale=0.355]{fig/visual/corri_step5.pdf}\n    \\label{fig:visual-a}}\\;\n    \\subfloat[Strategy: Zealots $1$, $2$, $5$ focus fire cooperatively and Zealots $4$ attack the distant enemy to rescue his teammates. ]{\\includegraphics[scale=0.355]{fig/visual/corri_step11.pdf}\\label{fig:visual-b}}\\;\n    \\subfloat[Strategy: Zealots $0$ keep moving to avoid being attacked and others eliminate the scattered enemies. ]{\\includegraphics[scale=0.355]{fig/visual/corri_step13.pdf}\\label{fig:visual-c}}\n    \\caption{A visualization example of the sophisticated strategies adopted by S2RL~(QMIX) in the SMAC corridor scenario.\n    In this super-hard map, ally units are 6 Zealots labeled by green circle, while enemy units are 24 Zerglings.\n    Green and red shadows mark enemies attracted by ally units. Green arrows and red arrows indicate the direction in which ally units and enemy units will move, respectively. Yellow lines indicate enemies that ally units are attacking.}\n    \\label{fig:visual}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{align}\nM=<\\mathcal{I}, \\mathcal{S}, \\mathcal{U}, P, R, \\Omega, G, \\gamma >.\n\\end{align}",
            "eq:2": "\\begin{align}\n    \\mathcal{L}(\\theta) =  \\mathbb{E}_{\\mathcal{D}} \\left[\\left(y^{tot} - Q^{tot}(\\boldsymbol{\\tau},\\boldsymbol{u},s;\\theta)\\right)^2\\right],\n\\end{align}",
            "eq:3": "\\begin{align}\nX_{t}^{i}=\\left\\{f(\\boldsymbol{o}_{t}^{i, 1}), \\ldots, f(\\boldsymbol{o}_{t}^{i, M})\\right\\},~i \\in \\mathcal{I}.\n\\end{align}",
            "eq:4": "\\begin{align}\n \\operatorname{Attn}(Q, K, V)= \\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{X}}}\\right) V.\n\\end{align}",
            "eq:5": "\\begin{align}\n\\widetilde{Z} = \\operatorname{SortMat}(Z) = [\\operatorname{SortVec}(\\boldsymbol{z}_1)^T,\\dots,\\operatorname{SortVec}(\\boldsymbol{z}_M)^T]^T,\n\\end{align}",
            "eq:6": "\\begin{align}\n\\boldsymbol{n}(\\widetilde{Z}) = [n_1,\\dots,n_M]^T,\n\\end{align}",
            "eq:7": "\\begin{align}\n\\boldsymbol{c} = [c_{1},\\cdots,c_{M}]^T\n\\end{align}",
            "eq:8": "\\begin{align}\n\\boldsymbol{\\mu} = [\\frac{1}{n_1},\\cdots,\\frac{1}{n_M}]^T.\n% \\begin{bmatrix} \n% \\frac{1}{n_1} & 0 & \\cdots & 0 \\\\ \n% 0 & \\frac{1}{n_2} & \\cdots  & 0 \\\\\n% \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n% 0 & 0 & \\cdots  & \\frac{1}{n_M}\n% \\end{bmatrix}.\n\\end{align}",
            "eq:9": "\\begin{align}\n\\Delta = \\boldsymbol{1}_{M\\times 1} (\\boldsymbol{c} \\odot \\boldsymbol{\\mu})^T,\n\\end{align}",
            "eq:10": "\\begin{align}\nP = [Z - \\Delta]_{+},\n\\end{align}",
            "eq:11": "\\begin{align}\n \\operatorname{sAttn}(Q, K, V)= P V,\n\\end{align}",
            "eq:Osta": "\\begin{equation}\\label{Osta}\nY_{dense} = \\operatorname{Attn}(Q, K, V) \\in \\mathbb{R}^{M \\times d_X},\n\\end{equation}",
            "eq:Oaux": "\\begin{equation}\\label{Oaux}\nY_{sparse} = \\operatorname{sAttn}(Q,K,V)  \\in \\mathbb{R}^{M \\times d_X}.\n\\end{equation}",
            "eq:h": "\\begin{equation}\\label{h}\nh_t^i = GRU(X_t^i, h_{t-1}^i).\n\\end{equation}",
            "eq:Qsta": "\\begin{equation}\\label{Qsta}\nQ_{dense}^{i}(\\tau^{i}, \\cdot) = \\operatorname{Agent}^i(Y_{dense}, h_{t}^i),\n\\end{equation}",
            "eq:Qaux": "\\begin{equation}\\label{Qaux}\nQ_{sparse}^{i}(\\tau^{i}, \\cdot) = \\operatorname{Agent}^i(Y_{sparse}, h_{t}^i).\n\\end{equation}",
            "eq:Lsta": "\\begin{equation}\\label{Lsta}\n\\mathcal{L}_{td}\\left(\\theta_{\\pi}, \\theta_{\\rho}\\right)=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(r+\\gamma \\max _{\\boldsymbol{u}^{\\prime}} \\bar{Q}_{dense}^{tot}\\left(s^{\\prime}, \\boldsymbol{u}^{\\prime}\\right)-Q_{dense}^{tot}(s, \\boldsymbol{u})\\right)^{2}\\right],\n\\end{equation}",
            "eq:Laux": "\\begin{equation}\\label{Laux}\n\\mathcal{L}_{aux}\\left(\\theta_{\\pi}, \\theta_{\\rho}\\right)=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(r+\\gamma \\max _{\\boldsymbol{u}^{\\prime}} \\bar{Q}_{sparse}^{tot}\\left(s^{\\prime}, \\boldsymbol{u}^{\\prime}\\right)-Q_{sparse}^{tot}(s, \\boldsymbol{u})\\right)^{2}\\right],\n\\end{equation}",
            "eq:totalloss": "\\begin{equation}\\label{totalloss}\n    \\mathcal{L}\\left(\\theta_{\\pi}, \\theta_{\\rho}\\right) = \n     \\mathcal{L}_{td}(\\theta_{\\pi}, \\theta_{\\rho}) + \\lambda  \\mathcal{L}_{aux}(\\theta_{\\pi}, \\theta_{\\rho}),\n\\end{equation}"
        }
    }
}