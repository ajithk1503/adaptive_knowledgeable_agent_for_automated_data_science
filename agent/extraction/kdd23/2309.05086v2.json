{
    "meta_info": {
        "title": "Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler",
        "abstract": "We propose a neuralized undirected graphical model called Neural-Hidden-CRF\nto solve the weakly-supervised sequence labeling problem. Under the umbrella of\nprobabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded\nwith a hidden CRF layer models the variables of word sequence, latent ground\ntruth sequence, and weak label sequence with the global perspective that\nundirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can\ncapitalize on the powerful language model BERT or other deep models to provide\nrich contextual semantic knowledge to the latent ground truth sequence, and use\nthe hidden CRF layer to capture the internal label dependencies.\nNeural-Hidden-CRF is conceptually simple and empirically powerful. It obtains\nnew state-of-the-art results on one crowdsourcing benchmark and three\nweak-supervision benchmarks, including outperforming the recent advanced model\nCHMM by 2.80 F1 points and 2.23 F1 points in average generalization and\ninference performance, respectively.",
        "author": "Zhijun Chen, Hailong Sun, Wanhao Zhang, Chunyi Xu, Qianren Mao, Pengpeng Chen",
        "link": "http://arxiv.org/abs/2309.05086v2",
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "additionl_info": "13 pages, 4 figures, accepted by SIGKDD-2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{introduction}\n\n\nDeep learning has witnessed the insatiable appetite for humongous labeled training data.\nThis appetite for data motivated several lines of work, such as active learning~\\cite{matsushita2018deep}, semi-supervised learning~\\cite{ouali2020overview},  transfer learning~\\cite{weiss2016survey}, and more recently, \\textit{weak supervision} (WS) ~\\cite{zhang2021wrench, zhang2022survey}, which is of interest in this paper.\n\n\n\nAs a time/cost-efficient and easy-to-promote alternative to gold expert annotation, WS provides practitioners with \\textit{multiple heterogeneous weak supervision sources}, such as crowdsourcing annotators from the Internet, user-defined programs encoded external knowledge bases, patterns/rules, or pre-trained classifiers, etc~\\cite{ratner2017snorkel,zhang2021wrench, zhang2022survey}.\nAs the price of good accessibility and as the name ``weak supervision'' implies, these various weak sources often exhibit varying error rates, leading to the generation of conflicting and noisy labels in many instances.\n\n\n\nWS has been applied to various tasks, including the fundamental deep language understanding task---\\textit{sequence labeling}~\\cite{ma2016end}, whose importance has been well recognized in the natural language processing\ncommunity.\nIn this paper, we focus on the problem of sequence learning in the context of multiple heterogeneous weak supervision sources, which can be abbreviated as \\textit{weakly-supervised sequence labeling} (WSSL).\nIt has been extensively studied as another main research branch in the whole WS community in addition to normal independent classification tasks~\\cite{zhang2021wrench, zhang2022survey}, because of the importance of the sequence labeling problem itself and the challenges associated with the need to consider the internal dependencies among sequence labels when solving WSSL.\n\n\n\nTo address the WSSL problem, existing representative methods fall into three categories in intrinsic methodology:\n\\begin{itemize}\n\\item\nThe HMM-based graphical models~\\cite{nguyen2017aggregating,simpson2018bayesian, safranchik2020weakly, lison2020named, lison2021skweak} leverage the hidden Markov model (HMM)~\\cite{blunsom2004hidden} to model the generation process of latent truth label sequence and observed weak label sequence, and then apply the expectation maximization (EM) algorithm~\\cite{moon1996expectation} to infer truth labels.\n(Then, these inferred labels, in turn, can be used to train a final sequence labeler.)\nThough principled, these models fall short in leveraging token semantics and context information~\\cite{li2021bertifying}, as they either model input tokens as one-hot observations~\\cite{nguyen2017aggregating,simpson2018bayesian} or do not model them at all~\\cite{safranchik2020weakly, lison2020named, lison2021skweak}.\n\\item \nThe ``source-specific perturbation'' deep learning models~\\cite{nguyen2017aggregating, lan2019learning, zhang2021crowdsourcing}, train multiple weak source-specific deep models, obtained by inserting the \\textit{source-specific perturbation parameters} to the \\textit{unique shared deep model parameters}, and perform test using the \\textit{assumed optimal classifier} obtained by the shared deep model straightforwardly or a certain combination of the source-specific deep models.\nWith less principle, it is not clear how interpretable they are in terms of mechanism design.\n\\item \nThe recently proposed neuralized HMM-based graphical models~\\cite{li2021bertifying, li2022sparse} construct HMM-based directed graphical models in which the dependencies among variables of word sequence, latent ground truth  sequences and weak labels are sophisticatedly modeled, and rich contextual semantic information is introduced using deep learning techniques (e.g., the language model BERT~\\cite{devlin2018bert}).\n\\end{itemize}\n\n\n\nThe neural HMM-based graphical models have the methodological advantages of both the first two classes of approaches---i.e., the principled modeling of graphical models to model variable dependencies and the rich contextual knowledge that comes from using deep learning---and have achieved relatively most satisfactory performance empirically in the recent WS benchmark~\\cite{zhang2021wrench}.\nHowever, these methods internally split all the variables of interest into multiple \\textit{local} regions and model them separately, and separately model the conditional probabilities of the ground truth at each time step ($p(t_l \\mid t_{l-1}, \\mathbf{x})$) in the truth sequence. \nEssentially, this \\textit{per-state normalization} approach~\\cite{lafferty2001conditional} (coming from the per-step modeling) is the same as that of the MEMM model~\\cite{mccallum2000maximum}, directly making them suffer from the well-known thorny \\textit{label bias problem}~\\cite{lafferty2001conditional} that is often mentioned in sequence labeling problems~\\cite{sutton2012introduction, simoes2009information, wallach2004conditional}.\nIn short, this approach of using the local optimization perspective (coming from repeatedly considering patterns for the scale of a step instead of holistically considering the entire sequence) causes some useful information to be erased~\\cite{hannun2020label} and leads to some bias.\nIn fact, mainly because of this reason, the  canonical conditional random field (CRF)~\\cite{lafferty2001conditional} was deliberately proposed by scholars in order to solve the sequence labeling problem with a globalized perspective.\n\n\n\nIn this paper, we move one step further and explore: \\textit{when solving the WSSL problem, how can we capitalize on the graphical model with principled modeling of variable dependencies and the advanced deep learning model that can bring rich contextual knowledge, without introducing the label bias problem, in a unified model?}\nTo address this problem, we introduce Neural-Hidden-CRF, a neuralized graphical model embedded with a hidden CRF layer.\nNeural-Hidden-CRF is built on undirected graph theory and models three sets of variables---namely, word sequence, latent ground truth sequence, and weak label sequence---with a globalized perspective like CRFs instead of the HMM-based models always considering local knowledge.\nSpecifically, in Neural-Hidden-CRF, we use deep learning models (like the language model BERT) to flexibly transfer rich contextual semantic knowledge to the latent truth sequence, and use the embedded hidden CRF layer to capture the dependencies among the truth sequences, and use the \\textit{weak source transition matrices} to model the dependencies between the truth labels and the weak labels.\nBy doing so, our model benefits both from the expressiveness and reasonableness of graphical models for capturing sophisticated dependencies among variables and from the effectiveness of the deep learning models for obtaining  contextual semantic knowledge, while avoiding the label bias problem caused by the local  perspective.\nTo the best of our knowledge, this is the first work to apply a neuralized undirected graphical model to solve the WSSL problem.\nWe conduct extensive evaluations of the proposed Neural-Hidden-CRF on one crowdsourcing benchmark and three WS benchmarks, showing that Neural-Hidden-CRF is a robust weakly-supervised sequence labeler and outperforms the state-of-the-art.\n\\footnotemark\n\\footnotetext{The code is available at: \n\\url{https://github.com/junchenzhi/Neural-Hidden-CRF}.}\n\n\n\n\n",
                "subsection 1.1": {
                    "name": "Related Work",
                    "content": "\n\\textbf{WSSL Learning Paradigms.}\nTo address the WSSL problem, two learning paradigms exist~\\cite{zhang2021wrench}: \n(1) Two-stage paradigm: Researchers have developed \\textit{label models}~\\cite{nguyen2017aggregating,simpson2018bayesian, safranchik2020weakly, lison2020named, lison2021skweak}  (also known as \\textit{truth inference} models~\\cite{zheng2017truth}) to aggregate noisy weak labels for each instance, accomplished with an follow-up \\textit{end model} (i.e., classifier) learning process using the aggregated labels;\n(2) Joint paradigm: Later researchers also explored learning the classifier of interest directly from weak supervision labels through ad hoc \\textit{joint models} in an end-to-end manner. \nAs presented above, we categorize representative methods from the intrinsic methodological perspective, where each method mentioned in their original work is either emphasized for its truth inference capability~\\cite{simpson2018bayesian, safranchik2020weakly, lison2020named, lison2021skweak} or generalization performance~\\cite{lan2019learning} or, better yet, both~\\cite{nguyen2017aggregating, zhang2021crowdsourcing}.\n\n\n\n\\textbf{Other WSSL Works.}\nAll WSSL methods can be divided into probabilistic graphical model approach, deep learning model approach, and neuralized graphical model approach.\n(1) In probabilistic graphical model approach (and in addition to the HMM-based models~\\cite{nguyen2017aggregating,simpson2018bayesian, safranchik2020weakly, lison2020named, lison2021skweak}), \\citet{rodrigues2014sequence} in early 2014 used a partially directed graph containing a CRF for modeling to solve the truth inference from crowdsourcing labels;\n(2) In deep learning model approach (and in addition to the ``source-specific perturbation'' methods~\\cite{nguyen2017aggregating, lan2019learning, zhang2021crowdsourcing}), other methods~\\cite{rodrigues2018deep, sabetpour2020optsla, sabetpour2021truth, lan2019learning} are either based on the end-to-end deep neural architecture~\\cite{rodrigues2018deep}, or the customized optimization objective along with coordinate ascent optimization technology~\\cite{sabetpour2020optsla, sabetpour2021truth}, or the iterative solving framework similar to expectation–maximization algorithm~\\cite{chen2023learning}.\nHowever, all these methods do not have the advantages of the recently proposed neuralized HMM-based graphical models~\\cite{li2021bertifying, li2022sparse} and our Neural-Hidden-CRF in principled modeling for variants of interest and in harnessing the context information that provided by advanced deep learning models.\nAdditionally, it is worth mentioning the presence of numerous established WS methods that address the normal independent classification scenario~\\cite{zhang2021wrench,zhang2022survey,zhang2022knowledge,chen2020structured,chen2022adversarial}.\n\n\n\n\n\n"
                }
            },
            "section 2": {
                "name": "Neural-Hidden-CRF",
                "content": "\n\n\n\n\n",
                "subsection 2.1": {
                    "name": "Problem Formulation and Preliminaries",
                    "content": "\n\\label{Problem Formulation and Preliminaries}\n\n\n\n\\paragraph{Problem Formulation of WSSL}\nWe are given i.i.d. training data $\\mathcal{D}=\\{\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}\\}_{i=1}^{I}$, where $\\mathbf{x}^{(i)}=\\{x_{l}^{(i)}\\}_{l=1 }^{L} \\in \\mathcal{X}$ is an observed word sequence with $L$-length tokens,  $\\mathbf{y}^{(i)}=\\{y_{l}^{(i, j)}\\}_{j \\in \\mathcal{J}^{(i)}, l  \\in {(1,2, \\ldots, L)}}$ are the noisy weak labels attached to  $i^{\\text {th }}$ sentence, and $\\mathcal{J}^{(i)}$ represents the set of weak sources that labeled the $i^{t h}$ sentence among all $J$ sources.\nFor each sentence $\\mathbf{x}^{(i)}$, there is a latent ground truth sequence $\\mathbf{t}^{(i)}=\\{t_{l}^{(i)}\\}_{l=1 }^{L}\\in\\mathcal{T}$ \\textit{unobserved} to us; $t_{l}^{(i)}\\in\\{1,2, \\ldots, K\\}$, where $K$ denotes  number of categories.\n(In addition,  we use $y^{(i, j)}_l=0$ to denote that source $j^{t h}$ has not annotated sentence $\\mathbf{x}^{(i)}$.)\nOur goal is to learn from the weak supervision data $\\mathcal{D}=\\{\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}\\}_{i=1}^{I}$ to obtain a sequence labeler $f: \\mathcal{X} \\mapsto \\mathcal{T}$ with strong generalization.\n\n\n\n\\paragraph{Preliminaries on Undirected Graphical Models}\nHere we give the most fundamental overview of the underlying theory (as an optional reading part).\n\\textbf{(1)}\nThe set of nodes in an undirected graph, \nwhere edges connect any two nodes, is denoted as a \\textit{cluster}; if a cluster cannot be added to any node to make it a larger cluster, it is denoted as a \\textit{maximum cluster}~\\cite{klinger2007classical}.\n\\textbf{(2)}\nUnder the undirected graphical model theory, the probability distribution over all nodes is factored as the normalized product of \\textit{potential functions} of all maximum clusters~\\cite{klinger2007classical}:\n\\begin{equation}\n\\begin{array}{c}\np(X)=\\frac{1}{Z} \\prod_c \\phi_c\\left(X_c\\right) \\\\\nZ=\\sum_X \\prod_c \\phi_c\\left(X_c\\right),\n\\end{array}\n\\end{equation}\nwhere $X_c$ is the set of nodes in a maximum cluster $c$, $\\phi_c\\left(\\cdot\\right)$ is an arbitrary non-negative real-valued function, called the \\textit{potential function} (acting as a scoring role), and $Z$ is the normalization factor.\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Model",
                    "content": "\nWe first formally introduce our model Neural-Hidden-CRF in Sections~\\ref{Model}-~\\ref{Eventually Neuralized Model}, and briefly explain Neural-Hidden-CRF from another simpler vision in Section~\\ref{Understanding Our Model from a Simpler Perspective}.\nThe graphical representation of  Neural-Hidden-CRF is shown in Figure~\\ref{Figure:1}. \nNote that in Figure~\\ref{Figure:1} and the presentation (except for the derivation in the Appendix~\\ref{Calculation of the Likelihood}) that follows, we tacitly assume that each instance owns annotations from all weak supervision sources.\n\n\n\n",
                    "subsubsection 2.2.1": {
                        "name": "Model",
                        "content": "\n\\label{Model}\nIn order to present in a more understandable way, we first introduce the base version of the model Neural-Hidden-CRF and then introduce the process of its neuralization in Section~\\ref{Eventually Neuralized Model} to obtain the eventual neuralized version, i.e., the model we refer to by default.\nPreviously, the CRF~\\cite{lafferty2001conditional} was upgraded to its neuralized version (e.g., BiLSTM-CRF~\\cite{huang2015bidirectional} or BERT-CRF~\\cite{zhang2021wrench}) by the same neuralization process.\n\n\n\nSimilar to the original CRF theory \\cite{lafferty2001conditional} and corresponding to Figure~\\ref{Figure:1}, we define the probability of weak label sequence $\\mathbf{y}^{(i)}$ along with   ground truth sequence  $\\mathbf{t}^{(i)}$ given observation sequence  $\\mathbf{x}^{(i)}$ to be a normalized product of three kinds of \\textit{pseudo-potential function}\\footnote{Note that for the sake of clarity, here we do not start with the construction of our \\textit{potential functions} in the strict sense, as mentioned in Section~\\ref{Problem Formulation and Preliminaries}.\nInstead, we create and define the ``pseudo-potential function'', which is similar to the strict \\textit{potential function}, but its role is not equivalent to that of a strictly defined one.} (which can undergo some simple adaptions to form the strictly potential functions, as mentioned in Seciton~\\ref{Problem Formulation and Preliminaries}), i.e.,\n\\begin{equation}\n\\exp \\left(\\sum_{a} \\lambda_{a}\\text{state}_{a}(t^{(i)}_{l}, \\mathbf{x}^{(i)}, l)\\right),\n\\label{equation 2}\n\\end{equation}\n\n\n\n\\begin{equation}\n\\exp \\left(\\sum_{b} \\mu_{b}\\text{transition}_{b}(t^{(i)}_{l-1}, t^{(i)}_{l}, l)\\right),\n\\label{equation 3}\n\\end{equation}\n\n\n\\begin{equation}\n\\exp \\left(\\sum_{c,j} \\eta_{c,j}\\text{source}_{c,j}(y^{(i,j)}_l, t^{(i)}_l, l)\\right),\n\\label{equation 4}\n\\end{equation}\nwhere $l\\in {(1,2, \\ldots, L)}$ denotes the time step.\nFor now, it is sufficient to note that: \n\\textbf{(1)} Within these \\textit{pseudo-potential functions}, each of the \\textit{feature functions} $\\text{state}_{a}(\\cdot)$, $\\text{transition}_{b}(\\cdot)$, $\\text{source}_{c,j}(\\cdot)$---aiming at extracting features---can be the pre-defined indicator function that takes the value 1 when the internal declaration is satisfied, and 0 otherwise; the corresponding \\textit{weights} $\\lambda_{a}, \\mu_{b}, \\eta_{c,j}\\in(-\\infty,+\\infty)$ are model's parameters to be estimated;\n\\textbf{(2)} These exponential \\textit{pseudo-potential functions} play the role of scoring (for a specific instantiated value of $\\mathbf{x}^{(i)}$, $\\mathbf{t}^{(i)}$, $\\mathbf{y}^{(i)}$ on every time step) and will be non-negative.\nIn other words, the internal \\textit{feature functions} along with their  weights play the role of scoring;\n\\textbf{(3)} Intuitively, the three types of \\textit{feature functions},  $\\text{state}_{a}(\\cdot)$, $\\text{transition}_{b}(\\cdot)$, $\\text{source}_{c,j}(\\cdot)$---corresponding to  purple lines,  blue lines, and  green lines in  Figure~\\ref{Figure:1}---act between the token sequence  $\\mathbf{x}^{(i)}$ and the  truth $\\mathbf{t}^{(i)}$, between the interior of the truth sequence $\\mathbf{t}^{(i)}$, and between the truth  $\\mathbf{t}^{(i)}$ and the weak labels\n$\\mathbf{y}^{(i)}$, respectively.\nSpecifically, we will walk through more details about \\textit{features functions} in Section~\\ref{Feature Functions}.\n\n\n\nFor simplicity, we denote the above three \\textit{feature functions} and the corresponding  \\textit{weights} by the general notations:\n\\begin{equation}\n\\begin{aligned}\n&f_{w}(\\mathbf{y}^{(i)}_{l},t^{(i)}_{l-1},t^{(i)}_{l},\\mathbf{x}^{(i)}, l)=\\\\\n&\\left\\{\\begin{array}{l}\n\\text{state}_{a}(t^{(i)}_{l}, \\mathbf{x}^{(i)}, l) \\quad \\qquad  \\qquad\n\\text{if } w=1,.., A\\\\\n\\text{transition}_{b}(t^{(i)}_{l-1}, t^{(i)}_{l}, l) \\qquad \n\\enspace\n\\;   \\, \\text{if } w=A+1,.., A+B \\\\\n\\text{source}_{c,j}(y^{(i,j)}_l, t^{(i)}_l, l)   \\quad \\qquad \\text{ if } w=A+B+1,.., A+B+\\sum_j C_j,\n\\end{array}\\right.\n\\end{aligned}\n\\label{equation 5}\n\\end{equation}\nalong with\n\\begin{equation}\\theta_{w}=\\left\\{\\begin{array}{ll}\\lambda_{a} & \\text{if } w=1,.., A \\\\\\mu_{b} & \\text{if } w=A+1,..., A+B \\\\\n\\eta_{c,j} & \\text{if } w=A+B+1,..., A+B+\\sum_j C_j,\n\\end{array}\\right.\n\\label{equation 6}\n\\end{equation}\nwhere $A$, $B$, and $\\sum_j C_j$ denote the specific number of a certain type of feature function, respectively.\n\n\n\nThus, our conditional model can be expressed as:\n\\begin{equation}\n\\begin{aligned}\n&  p(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta) \\\\\n=&\\frac{1}{\\boldsymbol{Z}(\n\\mathbf{x}^{(i)}; \\Theta)} \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l},\nt^{(i)}_{l-1},\nt^{(i)}_{l},\n\\mathbf{x}^{(i)}, l)\\right),\n\\end{aligned}\n\\label{equation 7}\n\\end{equation}\nwhere $\\boldsymbol{Z}(\\mathbf{x}^{(i)}; \\Theta)$ is the instance-specific normalization factor (also called \\textit{partition function} in the CRF~\\cite{lafferty2001conditional}) defined as:\n\\begin{equation}\n\\boldsymbol{Z}(\\mathbf{x}^{(i)}; \\Theta)=\\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l}, t^{(i)}_{l-1}, t^{(i)}_{l}, \\mathbf{x}^{(i)}, l)\\right).\n\\label{equation 8}\n\\end{equation}\nMore intuitively, if  $\\exp \\left(\\sum_l \\sum_w \\theta_w \\cdot f_w(\\mathbf{y}_l^{(i)}, t_{l-1}^{(i)}, t_l^{(i)}, \\mathbf{x}^{(i)}, l)\\right)$, representing getting the score for a specific instantiation of ($\\mathbf{y}^{(i)}$, $\\mathbf{t}^{(i)}$, $\\mathbf{x}^{(i)}$), abbreviated with \n$\\Phi(\\mathbf{y}^{(i)},\\mathbf{t}^{(i)},\\mathbf{x}^{(i)})$, then Equation~\\ref{equation 7} can be rewritten as:\n\\begin{equation}\np(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta) = \n\\frac{\\Phi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)})}{\\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\Phi\\left(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}\\right)}.\n\\label{equation 9}\n\\end{equation}\nEssentially, our model given by Equation~\\ref{equation 7} or Equation~\\ref{equation 9} is inherently aligns with the underlying theory mentioned in Section~\\ref{Problem Formulation and Preliminaries}.\\footnote{Referring to more material on CRFs and our tutorial\n(\\url{https://github.com/junchenzhi/Neural-Hidden-CRF}) would help to enhance the comprehension of our model.}\n\n\n\n\\paragraph{The Embodiment of the Global Optimization Perspective}\nWe can notice that the our pseudo-potential functions and feature functions in Equations~\\ref{equation 2}-\\ref{equation 3} do not have a direct probabilistic interpretation, but instead represent constraints or scores on the configurations of the random variable.\nAs a result, the model expressed by Equation~\\ref{equation 7} yields a \\textit{global normalized score} for $p(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)} ; \\Theta)$.\nThis \\textit{global normalization} approach is unlike all HMMs, where $(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)})$ is split into multiple uni-directional dependent random variables (i.e., a set consisting of many $(y^{(i,j)}_l, t^{(i)}_l)$) based on some strict independence assumptions and each conditional probability distribution between random variables is normalized (e.g., \\textit{local normalization} for per-step in~\\citet{li2021bertifying, li2022sparse}) to further obtain the probability of the joint distribution $(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)})$.\nSimply put, our approach models/trains holistically (the learned knowledge is global), while the HMMs~\\cite{li2021bertifying, li2022sparse} decompose the modeling into multiple uni-directional dependent local regions and model the patterns for the scale of a step (the learned knowledge is local).\nAs a result of the holistic undirected graphical modeling, our method can result in model parameters that are not constrained by probabilistic forms, thus enjoying more flexible scoring.\nAs for the \\textit{label bias problem}, our model circumvents this  by adopting the \\textit{global normalization} rather than the \\textit{local normalization} in~\\citet{li2021bertifying, li2022sparse}, just as the CRF model does with respect to the MEMM model~\\cite{lafferty2001conditional,hannun2020label}.\nPlease refer to~\\citet{hannun2020label} for more information on the label bias problem.\n\n\n"
                    },
                    "subsubsection 2.2.2": {
                        "name": "Feature Functions",
                        "content": "\n\\label{Feature Functions}\n(1) For feature function $\\text{state}_{a}(\\cdot)$, like the original CRF theory, we can define the following example:\n\\begin{small}\n\\begin{equation}\n\\text{state}_{a}(t^{(i)}_{l}, \\mathbf{x}^{(i)}, l)=\\left\\{\\begin{array}{ll}1 & \\text { if } t^{(i)}_{l}= \\texttt{PERSON}\n\\text { and } x^{(i)}_{l}=\\texttt{John} \\\\0 & \\text { otherwise}.\n\\end{array}\\right.\n\\label{equation 10}\n\\end{equation} \n\\end{small}\nIf the corresponding weight $\\lambda_a$ is a relatively large value, whenever the internal declaration in $\\text{state}_a(\\cdot)$ is true, it increases the probability of the sequence $\\mathbf{t}^{(i)}$.\nIntuitively, the model would prefer the tag \\texttt{PERSON} for the word \\texttt{John}.\nFormally, whenever the internal declaration in $\\text{state}_{a}(\\cdot)$ is satisfied, this feature function along with its weights $\\lambda_{a}$ will contribute factor $\\exp (\\lambda_{a} \\cdot 1)$ to the numerator in Equation~\\ref{equation 7}.\n\n\n\n(2) For feature function $\\text{transition}_{b}(\\cdot)$, we can define the following example:\n\\begin{small}\n\\begin{equation}\n\\text{transition}\n_{b}(t^{(i)}_{l-1}, t^{(i)}_{l}, l)\n=\\left\\{\\begin{array}{ll}1 & \\text {if } \nt^{(i)}_{l-1}=\\texttt{OTHER} \\text{ and }\nt^{(i)}_{l}=\\texttt{PERSON} \\\\\n0 & \\text {otherwise}.\n\\end{array}\\right.\n\\end{equation}\n\\label{equation 11}\n\\end{small}\nAlso, whenever the internal declaration in $\\text{transition}_{b}(\\cdot)$ is satisfied, this feature function along with its weights $\\mu_{b}$ will contribute factor $\\exp (\\mu_{b} \\cdot 1)$ to the numerator in Equation~\\ref{equation 7}.\nSince the number of categories is $K$, naturally we can define all $K^{2}$ feature functions of $\\text{transition}_{b}(\\cdot)$.\nThe set of parameters concerning $\\text{transition}_{b}(\\cdot)$ can referred to as the \\textit{CRF transition matrix} (with size of $K \\times K$), which essentially captures the dependence within the label sequence.\n\n\n\n(3) For feature function $\\text{source}_{c,j}(\\cdot)$, we can define the following example:\n\\begin{small}\n\\begin{equation}\n\\text{source}\n_{c,j}(y^{(i,j)}_{l}, t^{(i)}_{l}, l)\n=\\left\\{\\begin{array}{ll}\n1 & \\text { if } \ny^{(i,j)}_{l}=\\text {PERSON and }\nt^{(i)}_{l}=\\text {PERSON} \\\\\n0 & \\text { otherwise}.\n\\end{array}\\right.\n\\end{equation}\n\\label{equation 12}\n\\end{small}\nSimilar to feature function $\\text{transition}_{b}(\\cdot)$, we can define all $K^{2}$ feature functions of $\\text{source}_{c,j}(\\cdot)$ for weak source $j^{th}$.\nFor the particular $j^{\\text {th}}$ source, weights $\\{\\eta_{c,j}\\}_{c=1 }^{K^{2}}$ naturally form a $K \\times K$ matrix that can represents the behavior pattern of this source.\nThus, the higher the ability of a source, the larger the value of the diagonal elements of its matrix relative to the value of the non-diagonal elements.\nSimilar to the \\textit{CRF transition matrix}, we can refer to  this matrix as \\textit{weak source transition matrix}.\n\n\n\n"
                    },
                    "subsubsection 2.2.3": {
                        "name": "Eventually Neuralized Model",
                        "content": "\n\\label{Eventually Neuralized Model}\nHere we introduce a deep sequence  network, such as the language model BERT without the last softmax layer, between sequence $\\mathbf{x}$ and sequence $\\mathbf{t}$ to complete the model's neuralization.\nThus: (\\textit{i}) In the basic version of the model described above, for the time step $l$ in the sentence $\\mathbf{x}^{(i)}$, the feature function $\\text{state}_{a}(\\cdot)$ along with it's weight $\\lambda_{a}$ provide the factor $\\exp (\\lambda_{a} \\cdot \\text { state}_{a}(t_{l}^{(i)}, \\mathbf{x}^{(i)}, l)) $---which represents the degree of support of the model for ($\\mathbf{x}^{(i)}_{l}, t^{(i)}_{l} $)---for the numerator in Equation~\\ref{equation 7};\n(\\textit{ii}) In the current neuralized model, we use\n\\begin{equation}\n\\text {extract}_{l, t^{(i)}_{l}}(\\sigma_{BERT}(\\mathbf{x}^{(i)})),\n\\label{equation 13}\n\\end{equation}\nwhich also represents the degree of support of the model for ($\\mathbf{x}^{(i)}_{l}, t^{(i)}_{l} $), as the factor provided to the numerator in Equation~\\ref{equation 7}.\nIn Equation~\\ref{equation 13}, $\\sigma_{BERT}(\\cdot)$ is the output logits of BERT, \nand $\\operatorname{extract}_{l, t_{l}^{(i)}}(\\cdot)$ extracts the probability mass of the category $t^{(i)}_{l}$ in the $l$-step element of the input. \n\n\n\n"
                    },
                    "subsubsection 2.2.4": {
                        "name": "Understanding Our Model from a Simpler Perspective.",
                        "content": "\n\\label{Understanding Our Model from a Simpler Perspective}\n\nOur Neural-Hidden-CRF, for weakly-supervised sequence labeling learning, shares similarities with CRFs (e.g., BERT-CRF), for supervised sequence labeling learning.\nWe show the graphical representation of CRF vs. Neural-Hidden-CRF in Appendix~\\ref{Probabilistic Graphical Representation}.\n\\textbf{(1)}\nFirst, BERT-CRF is a discriminative model concerning the  label sequence $\\mathbf{t}^{(i)}$  given the sentence $\\mathbf{x}^{(i)}$:\n\\begin{equation}\np(\\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta)=\\frac{\\exp (\\operatorname{score}_{\\Theta}(\\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))}{\\sum_{\\mathbf{t}^{(i)}} \\exp (\\operatorname{score}_{\\Theta}(\\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))},\n\\end{equation}\n\\label{equation 15}\n\\begin{equation}\n\\operatorname{score}_{\\Theta}(\\mathbf{t}^{(i)}, \\mathbf{x}^{(i)})=\\sum_{l=1}^L(\\text{Emission}_{l, t_l^{(i)}}+\\text {CrfTransition}_{t_{l-1}^{(i)}, t_l^{(i)}}),\n\\end{equation}\nwhere $\\text{Emission} \\in \\mathbb{R}^{L \\times K}$ is the \\textit{emission score matrix} coming from the logit outputs of BERT ($\\text{Emission}=f_{\\Theta_{\\textbf{BERT}}}(\\mathbf{x}^{(i)})$), and\nCrfTransition $\\in \\mathbb{R}^{K \\times K}$ is the \\textit{CRF transition matrix}.\nModel parameters are $\\Theta=\\{\\Theta_{\\text{BERT}},   \\text{CrfTransition}  \\}$.\n\\textbf{(2)}\nSimilarly, our proposed Neural-Hidden-CRF is also a exponential discriminative model, concerning the weak label sequence $\\mathbf{y}^{(i)}$ and label sequence $\\mathbf{t}^{(i)}$  given the sentence $\\mathbf{x}^{(i)}$:\n\\begin{equation}\np(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta)=\\frac{\\exp (\\operatorname{score}_{\\Theta}(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))}{\\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp (\\operatorname{score}_{\\Theta}(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))},\n\\label{equation 16}\n\\end{equation}\n\\begin{small}\n\\begin{equation}\n\\begin{aligned}\n&\n\\operatorname{score}_{\\Theta}(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)})=\\sum_{l=1}^L(\\text {Emission}_{l, t_l^{(i)}}+\\text {CrfTransition}_{t_{l-1}^{(i)}, t_l^{(i)}} + \\\\\n&\\text{WeskSourceTransition\\#1}_{t_{l}^{(i)}, y_l^{(i,1)}} + ... + \\text{WeskSourceTransition\\#J}_{t_{l}^{(i)}, y_l^{(i,J)}}),\n\\end{aligned}\n\\label{equation 17}\n\\end{equation}\n\\end{small}\nwhere $\\text{Emission}$, $\\text{CrfTransition}$ have the same meaning as those in model BERT-CRF above, and each $\\text{WeakSourceTransition} \\in \\mathbb{R}^{K \\times K}$ refers to the \\textit{weak source transition matrix} introduced in Section~\\ref{Feature Functions}.\nModel parameters are $\\Theta=\\{\\Theta_{\\text{BERT}}, \\text{CrfTransition},$ $\\text{WeakSourceTransition\\#1}, ...,  \\text{WeakSourceTransition\\#J} \\}$.\n\n\n\n\n\n"
                    }
                },
                "subsection 2.3": {
                    "name": "Learning",
                    "content": "\n\\label{Learning}\nGiven the weak supervision data $\\mathcal{D}=\\{\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}\\}_{i=1}^{I}$ and the model constructed above, we estimate the parameters of the model by maximizing the conditional log-likelihood involving the latent ground truth variable:\n\\begin{equation}\n\\begin{aligned}\n& \\mathcal{L}(\\Theta)  =\\sum_{i} \\log p(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta).\n\\end{aligned}\n\\label{equation 18}\n\\end{equation}\nFurther, \n\\begin{small}\n\\begin{equation}\n\\begin{aligned}\n& \\log p(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta) \\\\\n=& \\log \\sum_{\\mathbf{t}^{(i)}}   p(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta) \\\\\n=&\\log  \\frac{1}{\\boldsymbol{Z}(\\mathbf{x}^{(i)}; \\Theta)} \\sum_{\\mathbf{t}^{(i)}}  \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l}, t^{(i)}_{l-1}, t^{(i)}_{l},\n\\mathbf{x}^{(i)}, l)\\right),\n\\end{aligned}\n\\label{equation 19}\n\\end{equation}\n\\end{small}\nwhere instance-specific normalization factor $\\boldsymbol{Z}(\\mathbf{x}^{(i)}; \\Theta)$ defined before is \n\\begin{equation}\n\\boldsymbol{Z}(\\mathbf{x}^{(i)}; \\Theta)=\\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l},\nt^{(i)}_{l-1},\nt^{(i)}_{l},\n\\mathbf{x}^{(i)}, l)\\right).\n\\label{equation 20}\n\\end{equation}\nCalculation of $\\log  \\sum_{\\mathbf{t}^{(i)}}  \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l}, t^{(i)}_{l-1}, t^{(i)}_{l},\\mathbf{x}^{(i)}, l)\\right)$ and $\\log  \\boldsymbol{Z}(\n\\mathbf{x}^{(i)}; \\Theta)$---similar to the corresponding calculation of the CRF---can be efficiently solved by dynamic programming algorithm.\nThe detail derivations are shown in Appendix~\\ref{Calculation of the Likelihood}. \n\n\n\nThe above $\\mathcal{L}(\\Theta)$ provides a unified objective function for optimization in Neural-Hidden-CRF, which can be done with standard stochastic optimization techniques, such as SGD~\\cite{goodfellow2016deep} or Adam~\\cite{kingma2014adam}.\n\n\n\n"
                },
                "subsection 2.4": {
                    "name": "Inference",
                    "content": "\n\\label{Inference}\nAt the test phase, given a new test sequence $\\mathbf{x}$, we want to infer the most probable ground truth sequence $\\mathbf{t}^*=\\arg \\max _{\\mathbf{t}^*} p(\\mathbf{t}^* \\mid \\mathbf{x} ;\\Theta)$.\nHere we can ignore the parameters of the weak source transition matrix part and use the classifier (e.g., BERT-CRF or BiLSTM-CRF) within Neural-Hidden-CRF to make the inference.\nLike the CRFs, this inference problem can be solved efficiently with the canonical Viterbi algorithm \\cite{forney1973viterbi}, which applies the dynamic programming.\n\n\n\n"
                },
                "subsection 2.5": {
                    "name": "Implementation Details",
                    "content": "\n\\label{Implementation Details}\n\\paragraph{Parameter initialization}\nIn our model, similar to the initialization in weak supervision model MAX-MIG~\\cite{cao2019max}, we can initialize the parameters of the weak  sources (i.e., the \\textit{weak source transition matrix}, denoted as $\\mathbf{\\Pi}^{(j)}$) as:\n\\begin{equation}\n\\pi_{m n}^{(j)}=\\rho\\cdot\\frac{\\sum_{i=1}^{I} \\sum_{l=1}^{L}\\mathbb{I}(t^{(i)}_{l}=m) \\mathbb{I}(y^{(i, j)}_{l}=n)}\n{\\sum_{i=1}^{I} \\sum_{l=1}^{L}\\mathbb{I}(t^{(i)}_{l}=m) \\mathbb{I}(y_{l}^{(i, j)} \\neq 0)}\\mathrm{,}\n\\label{equation 21}\n\\end{equation}\nwhere $\\rho$ is a hyper-parameter and $t^{(i)}_{l}$ can be easily obtained by majority voting method.\nIn addition, for the parameters of the classifier part (i.e., parameters in Neural-Hidden-CRF other than the weak source  transition matrices), we can easily pre-train the classifier using the labels inferred by majority voting to obtain a better  parameter initialization for the model.\n\n\n\n"
                },
                "subsection 2.6": {
                    "name": "Others: Computational Complexity",
                    "content": "\n\\label{Others: Computational Complexity}\nThe computational complexities of our method and some representative methods are shown in Table~\\ref{Table:1}, which contains the complexities of (1) performing the probability calculation on likelihood/objective during learning and (2) performing inference. In summary, our method has the same complexities as many existing methods.\n\n\n\n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Experiments",
                "content": "\n\\label{experiments}\n\n\n\n",
                "subsection 3.1": {
                    "name": "Setup",
                    "content": "\n\n\n\n",
                    "subsubsection 3.1.1": {
                        "name": "Datasets",
                        "content": "\n\\label{Datasets}\nWe evaluate the proposed Neural-Hidden-CRF on four widely-used, publicly available WS datasets, including the CoNLL-03 (MTurk) dataset~\\cite{rodrigues2014sequence,rodrigues2018deep} contributed by crowdsourcing workers from Amazon Mechanical Turk (MTurk)\\footnote{\\url{https://www.mturk.com/}}, and three datasets~\\cite{zhang2021wrench} (CoNLL-03 (WS),  WikiGold (WS), MIT-Restaurant (WS)) labeled from  artificially pre-defined label functions.\nTable~\\ref{Table:2} shows the main statistics.\nSpecifically:\n\\textbf{(1) CoNLL-03 (MTurk)}~\\cite{rodrigues2014sequence,rodrigues2018deep} is constructed on the well-established CoNLL-03 dataset~\\cite{sang2003introduction} through introducing additional crowdsourcing annotations.\nThe goal is to recognize named entities (\\textit{person, location, organization, miscellaneous}) together with their different parts (\\textit{begin, inside}) in the sentence. \nWe shuffled and divided the original $3250$ test samples in ~\\citet{rodrigues2014sequence} into a validation set and a test set containing 2000/1250 samples, respectively;\n\\textbf{(2) CoNLL-03 (WS), WikiGold (WS) and MIT-Restaurant (WS)}\nare utilized and open-sourced in the recently proposed WS benchmark called Wrench~\\cite{zhang2021wrench,rodrigues2018deep}.\nThese three datasets cover three different domains, and detailed information about them is provided in~\\citet{zhang2021wrench}.\n\n\n\n\n"
                    },
                    "subsubsection 3.1.2": {
                        "name": "Compared Methods.",
                        "content": "\n\\textbf{(1) On CoNLL-03 (MTurk).}\nWe consider the following  methods:\n(\\textit{i}) \nMV-BiLSTM/MV-BiLSTM-CRF: They are the two-stage learning baselines, which first estimate the ground truth from weak labels by MV (Majority Voting), and then train the LSTM/LSTM-CRF;\n(\\textit{ii}) \nCL (VW), CL (VW+B) and CL (MW): They are three variants of the representative WSSL method Crowd-Layer~\\cite{rodrigues2018deep}, where ``VW'', ``VW+B'' and ``MW'' refer to  three different ways of parameterizing weak source reliability; \n(\\textit{iii}) \nLSTM-Crowd~\\cite{nguyen2017aggregating}, LSTM-Crowd-cat~\\cite{nguyen2017aggregating}, \\citet{zhang2021crowdsourcing}, and CONNET~\\cite{lan2019learning}: These four methods, which apply the ``source-specific perturbation'' mentioned in Section~\\ref{introduction}, dominate the deep learning-based WSSL methods and show the competitive results~\\cite{lan2019learning};\n(\\textit{iv}) \nOptSLA~\\cite{sabetpour2020optsla} and AggSLC~\\cite{sabetpour2021truth}: They both follow the approach of constructing an optimization objective containing weak source weights, classifier parameters, latent ground truth, and iteratively updating them using a coordinate ascent algorithm;  \n(\\textit{v}) \nCRF-MA~\\cite{rodrigues2013learning}: This is a partial directed graphical model where the ground truth sequence is also modeled as a latent variable and each weak source's behavior pattern is modeled by a specific scalar;\n(\\textit{vi}) \nHMM-Crowd~\\cite{nguyen2017aggregating} and BSC-seq~\\cite{simpson2018bayesian}: They belong to the HMM-based graphical models mentioned in Section~\\ref{introduction}, where the latter is a Bayesian version of the former;\n(\\textit{vii}) \nFinally, we consider Gold, denoting the classifier (BiLSTM-CRF) trained in the ideal case when true labels are known.\n\\textbf{(2) On CoNLL-03 (WS), WikiGold (WS) and MIT-Restaurant (WS).}\nWe compared many methods by using the results reported from benchmark Wrench~\\cite{zhang2021wrench}.\nSpecifically, they involves the advanced CONNET~\\cite{lan2019learning}, CHMM~\\cite{li2021bertifying}, the HMM-based graphical model called HMM~\\cite{lison2020named}, and the label models (WMV~\\cite{zhang2021wrench}, DS~\\cite{dawid1979maximum}, DP~\\cite{ratner2016data}, MeTal~\\cite{ratner2019training}, FS~\\cite{fu2020fast}) for classification task with certain adaptations.\n\n\n\n\n"
                    },
                    "subsubsection 3.1.3": {
                        "name": "Configurations.",
                        "content": "\nThe hyper-parameter settings are shown in Appendix~\\ref{Experimental Configurations}.\n(Also, note that some suggestions for setting hyper-parameters are provided in Appendix~\\ref{Suggestions for Setting Hyperparameters}.)\nFurther:\n\\textbf{(1) On CoNLL-03 (MTurk).}\nWe applied the canonical BiLSTM-CRF~\\cite{ma2016end}\\footnote{We used the publicly available implementation:\n\\url{https://github.com/ZubinGou/NER-BiLSTM-CRF-PyTorch}.} as the classifier backbone of our model and comparison methods.\n\\textbf{(2) On CoNLL-03 (WS), WikiGold (WS) and MIT-Restaurant (WS).}\nOur experiments on these datasets build on the recent great benchmark Wrench~\\cite{zhang2021wrench}, where we adhered rigorously to their various settings and used their open-source code as the foundation for implementing our method.\nWe used the more advanced language model BERT of the two available choices (BiLSTM and BERT) provided by Wrench as the backbone. \n\n\n\n\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Results and Analysis",
                    "content": "\n",
                    "subsubsection 3.2.1": {
                        "name": "Main Results",
                        "content": "\nTables~\\ref{Table:5} and \\ref{Table:6}---concerning the CoNLL-03 (MTurk) dataset and the other three WS datasets, respectively---show the prediction performance of all methods on the test data and the inference performance on the training/test data, i.e., the performance of inferring the latent ground truth.\\footnote{It is worth noting that, unlike the metrics of ``inference on train data'' in Table~\\ref{Table:5} and consistent with the approach in the WS benchmark~\\cite{zhang2021wrench}, we report in Table~\\ref{Table:6} the inference performance of all methods on the test data, where weak labels are also available.}\nFirst, \\textit{we find that our model Neural-Hidden-CRF substantially outperforms all the comparison methods by a large margin on the most important average F1 metric on dataset CoNLL-03 (MTurk) and the other three datasets.}\nThe more robust performance demonstrated by our Neural-Hidden-CRF relative to the SOTA neuralized HMM-based CHMM~\\cite{li2021bertifying} largely showcases the effectiveness of our model in leveraging the global optimization perspective offered by the undirected graphical model.\nFurther and more specifically, on the average F1 metric, Neural-Hidden-CRF outperforms the recently proposed AggSLC~\\cite{sabetpour2021truth} by $4.81$  points on  CoNLL-03 (MTurk), and exceeds the SOTA method CHMM~\\cite{li2021bertifying} by $2.80/2.23$ points on the three WS datasets.\nIt is also worth noting that the comparison methods~\\cite{nguyen2017aggregating, zhang2021crowdsourcing, lan2019learning, sabetpour2021truth} on CoNLL-03 (MTurk) dataset, apply either the same backbone (i.e., the GloVe 100-dimensional word embeddings along with BiLSMT-CRF in~\\citet{nguyen2017aggregating}) as ours, or more advanced backbones (i.e., BERT-BiLSTM-CRF  in~\\citet{zhang2021crowdsourcing}, Efficient ELMO along with BiLSTM-CRF in~\\citet{lan2019learning}, BERT in AggSLC~\\cite{sabetpour2021truth}) than ours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompared with the inference metrics, we are more interested in prediction metrics, because in general, our ultimate objective revolves around developing a robust sequence labeler endowed with strong generalization.\nFor the prediction metric, we find that our Neural-Hidden-CRF outperforms all comparison methods across all datasets, often with considerable margins.\nIn terms of inference performance, in addition to achieving the second-best result on the MIT-Restaurant (WS) dataset, Neural-Hidden-CRF still often maintains a significant lead on the remaining three datasets and outperforms the second-best by $1.49$/$4.77$/$0.80$ points.\n\n\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Weak Source Parameter Estimation and Interpretability",
                        "content": "\nBenefiting from our use of the interpretable weak source transition matrices in the neuralized undirected graphical model rather than hard-to-interpret neural network parameters to model weak source behavior patterns,  we can now conduct a post-hoc study for the estimated matrices.\nMethodologically, for each specific weak supervision source, its parameters can form a matrix of size $K \\times K$. \nThese parameters possess interpretability on the behavioral pattern of the source. \nThat is, the matrix's element at position $(i, j)$ denotes scoring information for the case when the truth is $i$ and the weak label is $j$, where a larger value reflects a greater likelihood.\nEmpirically, Figure~\\ref{Figure:3} shows the results for the respective first weak source on the four datasets.\nThese results substantiate the accuracy of estimating the weak source transition matrices, and validate that the weak source transition matrices we model do have the \\textit{interpretability} in expressing the label transition patterns of weak sources (similar to the CRF transition matrix in the CRF model~\\cite{lafferty2001conditional}).\nAlso, such a result further demonstrates the effectiveness of  Neural-Hidden-CRF from another side.\nIn additin, the parameters of the weak source transition matrices estimated in Figure~\\ref{Figure:3} are unrestricted (i.e., each parameter takes the value space of $(-\\infty,+\\infty)$), without satisfying probabilistic statutes (as in the HMMs).\nAlso, our CRF transition matrix is similar.\nFor example, the elements in the first row of our estimated CRF transition matrix on CoNLL-03 (MTurk) are $[0.24, 2.94, 2.57, 1.69, -3.18, -4.29, 1.11, -3.57, -3.09]$.\nThese illustrate the resulting flexible scoring comes from the mechanism of holistic undirected graphical modeling and holistic parameter configuration.\n\n\n\n\n\n\n"
                    },
                    "subsubsection 3.2.3": {
                        "name": "Equipped with Other Backbones",
                        "content": "\nThe deep model in our model assumes a backbone role as a feature extractor for sentence sequences.\nTheoretically, a more powerful deep model would be more conducive to extracting more useful contextual semantic information and delivering more accurate prediction information about the truth sequences, thus having more potential to improve the final performance.\nHere we conducted a small-scale study on  partial datasets, where the obtained  results align with the above analysis.\nThat is, for the prediction task on datasets CoNLL-03 (WS) and WikiGold (WS), our Neural-Hidden-CRF yields suboptimal F1 performance relative to the original BERT-based one when we apply the relatively weaker deep model BiLSTM (provided by the benchmark Wrench~\\cite{zhang2021wrench})---BiLSTM-based/BERT-based: $67.63(\\pm1.08)$/$69.16(\\pm0.92)$, $65.21(\\pm1.45)$/$66.87(\\pm1.79)$.\n(Settings of \\texttt{Batch}/\\texttt{Lr}/\\texttt{Lr\\_weak}/\\texttt{\\(\\rho\\)}: $64$/$0.005$/$0.0001$/$3.0$, $32$/$0.001$/$0.0001$/$3.0$.)\n\n\n\n"
                    },
                    "subsubsection 3.2.4": {
                        "name": "Ablation Study",
                        "content": "\nHere we consider an extensive array of possible variants, involving \n\\textit{the ablation of different components} (variants i-iv),\n\\textit{the use of different parameter initialization} (variants v-vii), and \n\\textit{the freezing of model parameters} (variant viii).\nSpecifically:\n\\textbf{(\\textit{i})} W/o-weak-transition:\nWe ablate the weak source transition matrix, where we use the results inferred by the MV (Majority Voting) to represent the latent truth sequence and perform supervised learning, so that the dependencies between the truth sequence $\\mathbf{t}$ and the weak label sequence $\\mathbf{y}$ are not taken into account;\n\\textbf{(\\textit{ii})} W/o-crf-transition and \\textbf{(\\textit{iii})} Small-crf-transition\\footnote{Note that w.r.t. variants iii and iv, we investigate the performance of the variants under more ratios in the Appendix~\\ref{Performance of More Variants}.\\label{web}}: \nWe ablate/deduce the CRF transition matrix. \nW/o-crf-transition denotes we do not consider the CRF transition matrix at all during training and prediction/inference; \nSmall-crf-transition denotes we proceed normally during training as usual, but use $0.5$ times the value of the CRF transition matrix during prediction/inference;\n\\textbf{(\\textit{iv})} Small-emission\\footref{web}: \nWe deduce the emission values, where we also proceed normally during training, but use 0.5 times the value of the emission values (e.g., the BERT’s outputs) during prediction/inference; \\footnote{Note that it is not feasible to completely ablate emission values in the prediction, because we need to take sentence sequence $\\mathbf{x}$ to predict truth sequence $\\mathbf{t}$.} \n\\textbf{(\\textit{v})} Other-clasifier-init: \nWe perform the possibly inadequate learning of the parameters of the classifier part during initialization in an attempt to obtain a weaker initialization ($50$ back-propagations on the CoNLL-03 (MTurk) dataset and one epoch learning on the other three datasets);\n\\textbf{(\\textit{vi})} Other-worker-init: We initialize the diagonal/non-diagonal elements of the weak source transition matrix to $1/classes$/$0$, respectively;\n\\textbf{(\\textit{vii})} Other-both-init: \nWe use both of the parameter initialization ways above;\n\\textbf{(\\textit{viii})} Freeze-source:\nWe freeze the learning of the weak source parameters in the training phase.\n \n\n\nIn Table~\\ref{Table:7}, we see that:\n\\textbf{(\\textit{i})}\nThe method shows substantial performance degradation when either the weak source transition matrix or CRF transition matrix are ablated, or emission values are attenuated; \n\\textit{these results directly indicate the indispensable role of all three modules}\n(i.e., the weak source transition matrix, the CRF transition matrix, and the emission value) and the most significant of the weak source transition matrix;\n\\textbf{(\\textit{ii})}\nFurther, smaller emission values relative to a smaller CRF transition matrix produce a more pronounced performance degradation, illustrating the more dramatic sensitivity for emission values of our method;\n\\textbf{(\\textit{iii})}\nOn most of the datasets, our initialization of the classifier part and the weak source part is effective; a suitable parameter initialization allows our model to achieve better performance;\n\\textbf{(\\textit{iv})}\nFurther learning of the weak source parameters is necessary for the learning process of Neural-Hidden-CRF;\n\\textbf{(\\textit{v})}\nIn addition, we find that the vairant Other-classifier-init outperforms Neural-Hidden-CRF in prediction on CoNLL-03 (MTurk).\nThis is not surprising because we do not perform detailed tuning of our method, and the seemingly weaker parameter initialization happens to have stronger performance when combined with other hyperparameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Conclusion",
                "content": "\n\\label{conclusion}\nThis paper presents Neural-Hidden-CRF, the first neuralized undirected graphical model, for learning from weak-supervised sequence labels.\nNeural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence, where truth sequence is provided with rich contextual semantic information by the deep learning model.\nOur method, therefore, benefits both from the principled modeling of graphical models and from contextual knowledge of deep learning models, while avoiding the label bias problem caused by the local optimization perspective.\nOur empirical evaluations on multiple benchmarks demonstrate that Neural-Hidden-CRF significantly improves  state-of-the-art and  provides a new solution to weakly-supervised sequence labeling.\n\n\n\n\\begin{acks}\nThis work was supported by National Natural Science Foundation of China Under Grant Nos (61972013, 61932007, 62141209).\n\\end{acks}\n\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n% \\bibliography{sample-base}\n\\balance\n% \\bibliography{references}\n\\bibliography{ref}\n%%\n%% If your work has an appendix, this is the place to put it.\n\n\n\n\\appendix\n\\onecolumn\n\\setcounter{figure}{0}\n\\setcounter{table}{0}\n\\renewcommand{\\thefigure}{A.\\arabic{figure}}\n\\renewcommand{\\thetable}{A.\\arabic{table}}\n"
            },
            "section 5": {
                "name": "Appendix",
                "content": "\n\\label{Appendix}\n\n\n",
                "subsection 5.1": {
                    "name": "Calculation of the Likelihood",
                    "content": "\n\\label{Calculation of the Likelihood}\nFirst, we have the likelihood:\n\\begin{small}\n\\begin{equation}\n\\begin{aligned}\n& \\log p(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta) \\\\\n=& \\log \\sum_{\\mathbf{t}^{(i)}}   p(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta) \\\\\n=&\\log  \\frac{1}{\\boldsymbol{Z}(\n\\mathbf{x}^{(i)}; \\Theta)} \\sum_{\\mathbf{t}^{(i)}}  \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l},\nt^{(i)}_{l-1},\nt^{(i)}_{l},\n\\mathbf{x}^{(i)}, l)\\right)\\\\\n=&\\log  \\sum_{\\mathbf{t}^{(i)}}  \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l},\nt^{(i)}_{l-1},\nt^{(i)}_{l},\\mathbf{x}^{(i)}, l)\\right)\n- \\log \\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l},\nt^{(i)}_{l-1},\nt^{(i)}_{l},\n\\mathbf{x}^{(i)}, l)\\right)\\\\\n=&\\log  \\sum_{\\mathbf{t}^{(i)}}  \\exp (\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))\n- \\log  \\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}}  \\exp (\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)})),\n\\end{aligned}\n\\label{A:1}\n\\tag{A.1}\n\\end{equation}\n\\end{small}\nwhere we use $\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)})$ to implement the abbreviation.\nWe present the detail calculations of $\\log \\sum_{\\mathbf{t}^{(i)}} \\exp (\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))$ and $\\log \\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp (\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))$ in the following.\n\n",
                    "subsubsection 5.1.1": {
                        "name": "(i)",
                        "content": "\n\n\n\nFirst, we define\n$\\boldsymbol{\\alpha}^{(i)}_{l, k} \\triangleq \\log \\left[\n\\sum_{\\mathbf{t}^{(i)}_{1 \\sim l-1}}\n\\exp (\\Psi(\\mathbf{y}^{(i)}_{1 \\sim l},\n\\mathbf{t}^{(i)}_{1 \\sim l-1},\nt^{(i)}_l=k,\n\\mathbf{x}^{(i)}_{1 \\sim l}\n))\\right]$,\nwhich is used to express the logarithm of the cumulative sum of the scores after the exponential operation for each path that satisfies ``the state of  $\\mathbf{t}^{(i)}$ at time step $l$ is $k$''; here the path is considered only from the beginning to the time step $l$.\n\n\nThen, we have:\n\\begin{small}\n\\begin{equation}\n\\begin{aligned}\n& \\log \\left[\\sum_{\\mathbf{t}^{(i)}} \\exp (\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))\\right] \\\\\n=& \\log \\left[\n\\sum_{\\mathbf{t}^{(i)}_{1 \\sim L}} \\exp (\\Psi(\\mathbf{y}^{(i)}_{1 \\sim L}, \\mathbf{t}^{(i)}_{1 \\sim L},\n\\mathbf{x}^{(i)}_{1 \\sim L}))\\right] \\\\\n=& \\log \\left[\\sum_{k=1}^K\n\\sum_{\\mathbf{t}^{(i)}_{1 \\sim L-1}} \\exp (\\Psi(\\mathbf{y}^{(i)}_{1 \\sim L}, \\mathbf{t}^{(i)}_{1 \\sim L},\nt_{L}=k,\n\\mathbf{x}^{(i)}_{1 \\sim L}))\\right] \n\\\\\n=& \\log \\left[\\sum_{k=1}^K \\exp (\\boldsymbol{\\alpha}^{(i)}_{L-1, k})\\right]. \n\\end{aligned}\n\\label{A:2}\n\\tag{A.2}\n\\end{equation}\n\\end{small}\nThus, we transform the original objective of calculating $\\log \\sum_{\\mathbf{t}^{(i)}} \\exp (\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))$ into the calculating the\n``log\\_sum\\_exp'' (i.e., the successive operations of $\\exp (\\cdot)$, cumulative calculation and $\\log (\\cdot)$) of vector $\\boldsymbol{\\alpha}^{(i)}_{L-1, :}$.\n\n\nNow we use dynamic programming to calculate $\\boldsymbol{\\alpha}^{(i)}$.\nThe recursive calculation of $\\boldsymbol{\\alpha}^{(i)}$ is as follows:\n\\begin{small}\n\\begin{equation}\n\\begin{aligned}\n\\boldsymbol{\\alpha}^{(i)}_{l, k} \\triangleq\n& \n \\log \\left[\\sum_{\\mathbf{t}^{(i)}_{1 \\sim l-1}} \\exp (\\Psi(\\mathbf{y}^{(i)}_{1 \\sim l}, \\mathbf{t}^{(i)}_{1 \\sim l-1}, t_l=k, \\mathbf{x}^{(i)}_{1 \\sim l}))\\right]\n\\\\\n=& \n\\log \\left[\\sum_{k^{\\prime}=1}^K\n\\sum_{\\mathbf{t}_{1 \\sim l-2}} \\exp \\left(\\Psi(\\mathbf{y}^{(i)}_{1 \\sim l-1}, \\mathbf{t}^{(i)}_{1 \\sim l-2},\nt^{(i)}_{l-1}=k^{\\prime},\n\\mathbf{x}^{(i)}_{1 \\sim l-1})\n+ E^{(i)}_{k, l}\n+ T_{k^{\\prime}, k}\n+ W_{k, \\mathbf{y}^{(i)}_l}\n\\right)\\right] \n\\\\\n=& \n\\log \\left[\\sum_{k^{\\prime}=1}^K\n \\exp (\\boldsymbol{\\alpha}^{(i)}_{l-1, k^{\\prime}}\n+ E^{(i)}_{k, l}\n+ T_{k^{\\prime}, k}\n+ W_{k, \\mathbf{y}^{(i)}_l}\n)\\right], \n\\end{aligned}\n\\label{A:3}\n\\tag{A.3}\n\\end{equation}\n\\end{small}\nwhere $W_{k, \\mathbf{y}_l^{(i)}}=\\sum_{j \\in \\mathcal{J}^{(i)}} \\pi_{k, y_l^{(i, j)}}^{(j)}$, and $E^{(i)}_{k, l}$, $T_{k^{\\prime}, k}$, $W_{k, \\mathbf{y}^{(i)}_l}$ denote the\n\\textit{emission score}, the \\textit{CRF transition score}, and the \\textit{weak source transition score}, which originate from the three kind of feature functions.\n\n\nThe boundary case of $\\boldsymbol{\\alpha}^{(i)}$ is:\n\\begin{small}\n\\begin{equation}\n\\boldsymbol{\\alpha}^{(i)}_{0, k}\n=\\left\\{\\begin{array}{ll}\n\\underbrace{0}_{\\text{obtained from $\\log(1)$}}\n& \\text { if } \nk=\n\\text {BEGIN} \\\\\n\\underbrace{-10000}_{\\text{replace $\\log(0)=-\\infty$}} & \\text { otherwise}.\n\\end{array}\\right.\n\\label{A:4}\n\\tag{A.4}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "(i)",
                        "content": "\n\nFirst, we define\n$\\boldsymbol{\\beta}^{(i)}_{l, k} \\triangleq \\log \\left[\n\\sum_{\\mathbf{y}^{(i)}_{1 \\sim l}}\n\\sum_{\\mathbf{t}^{(i)}_{1 \\sim l-1}}\n\\exp (\\Psi(\\mathbf{y}^{(i)}_{1 \\sim l},\n\\mathbf{t}^{(i)}_{1 \\sim l-1},\nt^{(i)}_l=k,\n\\mathbf{x}^{(i)}_{1 \\sim l}\n))\\right]$.\nSimilar to the derivation in Equation~\\ref{A:2}, we can do the following derivation for $\\log \\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp (\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))$.\n\nThus, we have:\n\\begin{small}\n\\begin{equation}\n\\begin{aligned}\n& \\log \\left[ \\sum_{\\mathbf{y}^{(i)}}\\sum_{\\mathbf{t}^{(i)}} \\exp (\\Psi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))\\right] \\\\\n=& \\log \\left[\n\\sum_{\\mathbf{y}^{(i)}_{1 \\sim L}}\n\\sum_{\\mathbf{t}^{(i)}_{1 \\sim L}} \\exp (\\Psi(\\mathbf{y}^{(i)}_{1 \\sim L}, \\mathbf{t}^{(i)}_{1 \\sim L},\n\\mathbf{x}^{(i)}_{1 \\sim L}))\\right] \\\\\n=& \\log \\left[\\sum_{k=1}^K\n\\sum_{\\mathbf{y}^{(i)}_{1 \\sim L}}\n\\sum_{\\mathbf{t}^{(i)}_{1 \\sim L-1}} \\exp (\\Psi(\\mathbf{y}^{(i)}_{1 \\sim L}, \\mathbf{t}^{(i)}_{1 \\sim L},\nt^{(i)}_{L}=k,\n\\mathbf{x}^{(i)}_{1 \\sim L}))\\right] \n\\\\\n=& \\log \\left[\\sum_{k=1}^K \\exp (\\boldsymbol{\\beta}^{(i)}_{L-1, k})\\right]. \n\\end{aligned}\n\\label{A:5}\n\\tag{A.5}\n\\end{equation}\n\\end{small}\nThe recursive calculation of $\\boldsymbol{\\beta}^{(i)}$ is:\n\\begin{small}\n\\begin{equation}\n\\begin{aligned}\n\\boldsymbol{\\beta}^{(i)}_{l, k} \\triangleq\n& \n \\log \\left[\n \\sum_{\\mathbf{y}^{(i)}_{1 \\sim l}}\n \\sum_{\\mathbf{t}^{(i)}_{1 \\sim l-1}} \\exp (\\Psi(\\mathbf{y}^{(i)}_{1 \\sim l}, \\mathbf{t}^{(i)}_{1 \\sim l-1}, t^{(i)}_l=k, \\mathbf{x}^{(i)}_{1 \\sim l}))\\right]\n\\\\\n=& \n\\log \\left[\\sum_{k^{\\prime}=1}^K\n \\sum_{\\mathbf{y}^{(i)}_{1 \\sim l-1}}\n\\sum_{\\mathbf{t}^{(i)}_{1 \\sim l-2}} \n\\sum_{\\mathbf{y}^{(i)}_{l}}\n\\exp \\left(\\Psi(\\mathbf{y}^{(i)}_{1 \\sim l-1}, \\mathbf{t}^{(i)}_{1 \\sim l-2},\nt^{(i)}_{l-1}=k^{\\prime},\n\\mathbf{x}^{(i)}_{1 \\sim l-1})\n+ E^{(i)}_{k, l}\n+ T_{k^{\\prime}, k}\n+ W_{k, \\mathbf{y}^{(i)}_l}\n\\right)\\right] \n\\\\\n=& \n\\log \\left[\\sum_{k^{\\prime}=1}^K\n \\sum_{\\mathbf{y}^{(i)}_{1 \\sim l-1}}\n\\sum_{\\mathbf{t}^{(i)}_{1 \\sim l-2}} \n\\exp \\left(\\Psi(\\mathbf{y}^{(i)}_{1 \\sim l-1}, \\mathbf{t}^{(i)}_{1 \\sim l-2},\nt^{(i)}_{l-1}=k^{\\prime},\n\\mathbf{x}^{(i)}_{1 \\sim l-1})\n+ E^{(i)}_{k, l}\n+ T_{k^{\\prime}, k}\n+ \n\\log \\sum_{\\mathbf{y}^{(i)}_l} \\exp (W_{k, \\mathbf{y}^{(i)}_l})\n\\right)\\right] \n\\\\\n=& \n\\log \\left[\\sum_{k^{\\prime}=1}^K\n \\exp \\left(\\boldsymbol{\\beta}^{(i)}_{l-1, k^{\\prime}}\n+ E^{(i)}_{k, l}\n+ T_{k^{\\prime}, k}\n+ \n\\log \\sum_{\\mathbf{y}^{(i)}_l} \\exp (W_{k, \\mathbf{y}^{(i)}_l})\n\\right)\\right],\n\\end{aligned}\n\\label{A:6}\n\\tag{A.6}\n\\end{equation}\n\\end{small}\nwhere the meanings of  $E^{(i)}_{k, l}$, $T_{k^{\\prime}, k}$, $W_{k, \\mathbf{y}^{(i)}_l}$ are the same as those of the corresponding symbols in Equation~\\ref{A:3}.\nAlso, the boundary case is:\n\\begin{small}\n\\begin{equation}\n\\boldsymbol{\\beta}^{(i)}_{0, k}\n=\\left\\{\\begin{array}{ll}\n\\underbrace{0}_{\\text{obtained from $\\log(1)$}}\n& \\text { if } \nk=\n\\text {BEGIN} \\\\\n\\underbrace{-10000}_{\\text{replace $\\log(0)=-\\infty$}} & \\text { otherwise}.\n\\end{array}\\right.\n\\label{A:7}\n\\tag{A.7}\n\\end{equation}\n\\end{small}\nSpecifically, in Equation~\\ref{A:6}, we can also use dynamic programming to  calculate $\\log \\sum_{\\mathbf{y}^{(i)}_l} \\exp (W_{k, \\mathbf{y}^{(i)}_l})$.\n\n\n\n\n\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Probabilistic Graphical Representation",
                    "content": "\n\\label{Probabilistic Graphical Representation}\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Experimental Configurations",
                    "content": "\n\\label{Experimental Configurations}\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Suggestions for Setting Hyperparameters",
                    "content": "\n\\label{Suggestions for Setting Hyperparameters}\n\nWhen applying our Neural-Hidden-CRF to other datasets, in most cases, we recommend considering the following suggestions for setting hyperparameters.\n\n\n\\begin{itemize}\n\\item\nFor \\texttt{Batch} (batch size):\nOur suggested finding space is $\\{8, 16, 32, 64, ...\\}$, and batch size should not be set to $1$ (which would not be conducive to the challenging multi-source weak supervision learning context);\n\\item \nFor \\texttt{Lr\\_weak} (learning rate of weak source transition matrix):\nWe suggest that \\texttt{Lr\\_weak} be set equal to or less than the learning rate of the CRF layer (i.e., \\texttt{Lr\\_crf});\n\\item \nFor \\texttt{\\(\\rho\\)} (in Equation~\\ref{equation 21}):\nOur suggested finding space is $\\{2.0, 3.0, 4.0, 5.0, 6.0\\}$ for most cases;\n\\item \nFor the pre-train of the classifier part of the model (mentioned in Section~\\ref{Implementation Details}):\nWe suggest using better super-parameters (e.g., batch size, learning rates, etc.) for pre-training to get a better parameter initialization.\n\\end{itemize}\n\n\n\n\n"
                },
                "subsection 5.5": {
                    "name": "Performance of More Variants",
                    "content": "\n\\label{Performance of More Variants}\n\n\n\n\n\n\n\n\n"
                }
            }
        },
        "tables": {
            "Table:1": "\\begin{table}[h]\n\\caption{Computational Complexity. \n$L$/$K$/$J$: sequence length/\\# categories/\\# weak sources.}\n\\centering\n\\scalebox{0.73}{\n\\begin{threeparttable}  \n{\n\\begin{tabular}{  l c c}\n\\toprule\nMethod\n&  Probability calculation  & Inference\n\\tabularnewline\n\\midrule\n\\rule{0pt}{7pt}MV + BERT-CRF$^{\\S}$\n&  $O(LK^2)$  &  $O(LK^2)$ \n\\\\\n\\cline{2-3}\n\\rule{0pt}{9pt}LSTM-Crowd~\\cite{nguyen2017aggregating}$^{*}$, LSTM-Crowd-cat~\\cite{nguyen2017aggregating}$^{*}$\n& \n\\multirow{2}{*}{$O(JLK^2)$} & \n\\multirow{2}{*}{$O(LK^2)$ } \\\\\nCONNET~\\cite{lan2019learning}$^{*}$, Zhang et al.~\\cite{zhang2021crowdsourcing}$^{*}$\n& &     \\\\\n\\cline{2-3}\n\\rule{0pt}{10pt}Ours$^{*}$\n  &   $O(JLK^2)$  &  $O(LK^2)$ \n\\tabularnewline\n\\bottomrule\n\\end{tabular}\n}\n\\begin{tablenotes}\n \\footnotesize   \n\\item[1] $\\S$: \nWhen we use the labels inferred from a truth inference method and perform supervised training, e.g., MV+BERT-CRF, the complexities of  MV+BERT-CRF are the same as CRF~\\cite{collins2015log}.\n\\item[2] $*$: \nOur method has the same complexities as the ``source-specific perturbation'' methods LSTM-Crowd~\\cite{nguyen2017aggregating}, LSTM-Crowd-cat~\\cite{nguyen2017aggregating}, CONNET~\\cite{lan2019learning} and Zhang et al.~\\cite{zhang2021crowdsourcing}. \nThis is because: \n(\\textit{i}) For the probability calculation complexity, since the ``source-specific perturbation'' methods require to learn $J$ source-specific models, their complexities ($O(JLK^2)$) are $J$ times the corresponding complexity of CRF ($O(LK^2)$). Also, when our method utilizes the dynamic programming algorithm to compute our Equation~\\ref{A:3} (the most significant consumers of computing) in the Appendix~\\ref{Calculation of the Likelihood}, its complexity is also $O(JLK^2)$;\n(\\textit{ii}) For the inference complexity, each Viterbi decoding process required by these methods is the same as for the CRF, and therefore the complexities are all $O(LK^2)$.\n\\item[3]\nNote that here we consider the complexities on one instance, and by the general convention, we do not consider the complexity arising from the deep neural backbone, which has an equivalent effect for all methods.\n\\end{tablenotes}\n\\end{threeparttable} \n}\n\\smallskip\n\\label{Table:1}\n\\hfil\n\\captionsetup{type=table,skip=5pt}\n\\end{table}",
            "Table:2": "\\begin{table}[h]\n\\caption{Statistics of all datasets.}\n\\centering\n\\scalebox{0.758}{\n\\fontsize{1pt}{7pt}\n\\begin{threeparttable}  \n\\setlength{\\tabcolsep}{0.5mm}{\n\\begin{tabular}{  l l r r r}\n\\toprule\n Dataset\n&  Domain\n  & \\#Data(train/val/test)\n   & \\#Entities\n   & \\#Source  \n \\tabularnewline\n\\midrule\nCoNLL (MTurk)\n& News &  5,985/2,000/1,250  & 4 & 47\n\\\\\nCoNLL (WS)& News  & 14,041/3,250/3,453   &4 &16   \\\\\nWikiGold (WS)\n& Web Text &  1,355/169/170  & 4 & 16\\\\\nMIT-Rest. (WS)\n  &  Review & 7,159/500/1,521 & 8  &16\n\\tabularnewline\n\\bottomrule\n\\end{tabular}\n}\n\\end{threeparttable} \n\\smallskip\n}\n\\label{Table:2}\n\\hfil\n\\captionsetup{type=table,skip=5pt}\n\\end{table}",
            "Table:5": "\\begin{table*}[t]\n\\centering\n\\caption{Performance  ($\\%$)  on  \n CoNLL-03  (MTurk) dataset. Results  are averaged over 20 runs.\nThe best results under the F1 metric of most interest are marked in \\textbf{bold}.}\n\\scalebox{0.85}{\n\\begin{threeparttable}  \n\\begin{tabular}\n{l lcccc cc\n>{\\columncolor{lightgray!20}}c}\n\\toprule  \\multicolumn{2}{c}{} & \\multicolumn{3}{c}{\\textbf{Prediction on test data}$^{\\S}$} & \\multicolumn{3}{c}{\\textbf{Inference on train data}$^{*}$ } & \\multicolumn{1}{c}{}  \\\\\n  \\cmidrule(lr){3-5} \n  \\cmidrule(lr){6-8}\n\\textbf{Paradigm} &\n\\textbf{Method}  & \\textbf{Precision}       & \\textbf{Recall}        & \\textbf{F1}           & \\textbf{Precision}      \n              & \\textbf{Recall}   &              \\textbf{ F1}\n                          & \\textbf{\\underline{Avg. F1}}\n\\tabularnewline\n\\midrule\n\\multirow{2}{*}{Two-stage WSSL} & \n MV + BiLSTM-CRF & 87.19($\\pm$1.19)  &  65.00($\\pm$3.28)    & 74.41($\\pm$2.11)  &  86.27($\\pm$1.08)  &  66.06($\\pm$2.3) \n & 74.79($\\pm$1.38) &  74.60\n\\tabularnewline\n& \n MV + BiLSTM & 82.21($\\pm$1.46)   &  61.30($\\pm$2.57)        & 70.20($\\pm$1.69)   &  80.62($\\pm$1.01)  &  61.82($\\pm$2.36) \n & 69.96($\\pm$1.64)   &  70.08\n\\tabularnewline\n\\midrule\n\\multirow{11}{*}{One-stage WSSL} \n& \n CL (VW)~\\cite{rodrigues2018deep}  & 83.93($\\pm$0.83)  &  61.50($\\pm$2.07)        & 70.96($\\pm$1.46)  &  82.90($\\pm$0.71)  &  64.02($\\pm$1.76)\n & 72.24($\\pm$1.29)   &  71.60\n\\tabularnewline\n& \n CL (VW+B)~\\cite{rodrigues2018deep}    & 81.93($\\pm$1.57)   &  61.00($\\pm$2.89)       & 69.87($\\pm$1.62)  &  80.31($\\pm$1.38) &  61.70($\\pm$2.65)\n & 69.75($\\pm$1.73) &  69.81\n\\tabularnewline\n& \n CL (MW)~\\cite{rodrigues2018deep}\n   & 83.93($\\pm$0.89)  &  61.33($\\pm$1.65)       & 70.86($\\pm$1.65)\n  &  82.24($\\pm$0.55) &  62.91($\\pm$1.26)\n & 71.27($\\pm$0.88)\n &   71.07\n  \\tabularnewline\n  &\n LSTM-Crowd~\\cite{nguyen2017aggregating}$^{\\dag}$   & 82.38  &  62.10  &  70.82 &   -   &  -   &   -   & -\n\\tabularnewline\n &\nLSTM-Crowd-cat~\\cite{nguyen2017aggregating}$^{\\dag}$   &    79.61    & 62.87  &  70.26 &  -  &  -  & -  &  -\n\\tabularnewline\n &\n\\citet{zhang2021crowdsourcing}$^{\\dag}$ \n&  78.84  &  75.67  &  77.95\n&  -   &  -   &   - &  -\n\\tabularnewline\n &\nCONNET~\\cite{lan2019learning}$^{\\dag}$  & 87.77($\\pm$0.25) & 72.79($\\pm$0.04) & 79.99($\\pm$0.08) &  - &  -  &    -   &  -\n\\tabularnewline\n\n &\nAggSLC~\\cite{sabetpour2021truth}$^{\\dag}$     & 70.95 & 77.16 &  73.93 &  83.02   &  78.69   &   80.79 &  77.36\n\\tabularnewline\n &\nCRF-MA~\\cite{rodrigues2014sequence}$^{\\dag}$     & 49.4 & 85.6 & 62.6  &  86.0   &  65.6   &   74.4  &  68.5\n\\tabularnewline\n \\cmidrule(lr){2-9} \n & \n\\textbf{Neural-Hidden-CRF}\n     &  82.25($\\pm$1.05)   &  80.93($\\pm$1.05)    &  \\textbf{82.06}($\\pm$0.63)\n &  84.41($\\pm$1.04) &  80.28($\\pm$0.74) & \n\\textbf{82.28}($\\pm$0.49) &  \\textbf{82.17}\n\\tabularnewline\n\\midrule\n\\multirow{4}{*}{Truth Inference} & \nMV   &  -   &  -     &  -  & 79.12($\\pm$0.00) & 58.50($\\pm$0.00)   & 67.27($\\pm$0.00)  &  -\n\\tabularnewline\n &\nOptSLA~\\cite{sabetpour2020optsla}$^{\\dag}$  &  -   &  -     &  -  &  79.42 &   77.59  & 78.49 &  -\n\\tabularnewline\n &\nHMM-Crowd~\\cite{nguyen2017aggregating}$^{\\dag}$   &  -   &  -     &  -  & 77.40 &  72.29   & 74.76 &  -\n\\tabularnewline\n &\nBSC-seq~\\cite{simpson2018bayesian}$^{\\dag}$  &  -   &  -     &  -  & 80.3 &  74.8   & 77.4 &  -\n\\tabularnewline\n\\midrule\n-\n& \n\\gc{Gold (Upper Bound)}     &  \\gc{91.94($\\pm$0.66)}   &   \\gc{91.49($\\pm$0.87)}      &  \\gc{91.71($\\pm$0.75)}  &  \\gc{100} &  \\gc{100}   & \\gc{100} &  \\gc{95.86} \n\\tabularnewline\n\\bottomrule\n\\end{tabular}\n\\begin{tablenotes}\n\\footnotesize   \n\\item[1] $\\S$/$*$: Learn from weak supervision labels on the train data and predict on the test data/learn from weak supervision labels on the train data and infer the latent ground truth labels.\n\\item[2] $\\dag$:\nResults are reported from the original works.\nNote that there are some blanks in these results, as most of these methods reported one of two metrics in their original works.\n\\end{tablenotes}\n\\end{threeparttable} \n}\n\\smallskip\n\\label{Table:5}\n\\hfil\n\\captionsetup{type=table,skip=5pt}\n\\end{table*}",
            "Table:6": "\\begin{table*}[t]\n\\centering\n\\caption{Performance  ($\\%$)  on WS benchmark datasets from~\\citet{zhang2021wrench}.\nOur results  are averaged over 20 runs.\nThe best results  are marked in \\textbf{bold}.\nEach table cell contains F1 score with standard deviation and (Precision, Recall) in the bracket.}\n\\scalebox{0.85}{\n\\begin{threeparttable}  \n\\begin{tabular}\n{l lcccc cc\n>{\\columncolor{lightgray!20}}c}\n \\toprule \n         \\multicolumn{2}{c}{} & \\multicolumn{3}{c}{\\textbf{Prediction on test data$^{\\S}$}} &\n    \\multicolumn{3}{c}{\\textbf{Inference on test data$^{*}$}} & \\multicolumn{1}{c}{}  \\\\\n  \\cmidrule(lr){3-5} \n  \\cmidrule(lr){6-8}\n\\textbf{Paradigm} &\n\\textbf{Method}\n      & \\textbf{CoNLL-03}\n              & \\textbf{WikiGold}\n                        & \\textbf{MIT-Rest.}\n                            & \\textbf{CoNLL-03}      \n              & \\textbf{WikiGold}\n                         & \\textbf{MIT-Rest.}\n                          & \\textbf{\\underline{Avg.F1(P/I)}}\n\\tabularnewline\n\\midrule\n\\multirow{16}{*}{Two-stage WSSL} & \n\\multirow{2}{*}{MV + BERT-CRF~\\cite{zhang2021wrench}$^{\\dag}$} & 66.63($\\pm$0.85)  &  62.09($\\pm$1.06)       & 42.95($\\pm$0.43)  \n& 60.36($\\pm$0.00) & 52.24($\\pm$0.00)  & \\textbf{48.71}($\\pm$0.00)\n  &  57.22/53.77\n\\tabularnewline\n&  & \\gc{(67.68/65.62)}  &  \\gc{(61.89/62.29) }      & \\gc{(63.18/32.54)} \n& \\gc{(59.06/61.72)}  & \\gc{(48.95/56.00)} \n & \\gc{(74.25/36.24)}   &  -\n\\tabularnewline\n  \\cmidrule(lr){2-9} \n&  \\multirow{2}{*}{WMV + BERT-CRF~\\cite{zhang2021wrench}$^{\\dag}$} & 64.38($\\pm$1.09) &  59.96($\\pm$1.08)       & 42.62($\\pm$0.23)\n&  60.26($\\pm$0.00) &   52.87($\\pm$0.00)  & 48.19($\\pm$0.00)\n &   55.65/53.77\n\\tabularnewline\n&  & \\gc{(66.55/62.35)}  & \\gc{(60.33/59.73)}        & \\gc{(63.56/32.06)}  \n& \\gc{(59.03/61.54)}   &  \\gc{(50.74/55.20)} \n & \\gc{(73.73/35.80)}   &  -\n\\tabularnewline\n  \\cmidrule(lr){2-9} \n&  \\multirow{2}{*}{DS + BERT-CRF~\\cite{dawid1979maximum}$^{\\dag}$} & 53.89($\\pm$1.42)  &  48.89($\\pm$1.59)       & 42.26($\\pm$0.78) \n& 46.76($\\pm$0.00) &  42.17($\\pm$0.00)  & 46.81($\\pm$0.00) \n &   48.35/42.25\n\\tabularnewline\n&  & \\gc{(54.10/53.68)}  &  \\gc{(46.80/51.20)}       & \\gc{(62.65/31.89)} \n& \\gc{(45.29/48.32)}   &  \\gc{(40.05/44.53)} \n & \\gc{(71.71/34.75)} &  -\n\\tabularnewline\n  \\cmidrule(lr){2-9} \n&  \\multirow{2}{*}{DP + BERT-CRF~\\cite{ratner2016data}$^{\\dag}$} & 65.48($\\pm$0.37)  &  61.09($\\pm$1.53)      & 42.27($\\pm$0.53) \n& 62.43($\\pm$0.22) &  54.81($\\pm$0.13)   & 47.92($\\pm$0.00)\n &   56.28/55.05\n\\tabularnewline\n&  & \\gc{(66.76/64.28)}  &  \\gc{(61.07/61.12)}       & \\gc{(62.81/31.86)} \n &  \\gc{(61.62/63.26)} &  \\gc{(53.10/56.64)}\n & \\gc{(73.24/35.61)}  &  -\n\\tabularnewline\n  \\cmidrule(lr){2-9} \n&  \\multirow{2}{*}{MeTal + BERT-CRF~\\cite{ratner2019training}$^{\\dag}$} & 65.11($\\pm$0.69)  &  58.94($\\pm$3.22)      & 42.26($\\pm$0.49) \n& 60.32($\\pm$0.08) &  52.09($\\pm$0.23)   & 47.66($\\pm$0.00)\n  &   55.44/53.37\n\\tabularnewline\n&  & \\gc{(66.87/63.45)}  &  \\gc{(61.53/56.75)}       & \\gc{(62.82/31.84)} \n&  \\gc{(59.07/61.63)} &  \\gc{(50.31/54.03)}\n & \\gc{(73.40/35.29)}\n  & -\n\\tabularnewline\n  \\cmidrule(lr){2-9} \n&  \\multirow{2}{*}{FS + BERT-CRF~\\cite{fu2020fast}$^{\\dag}$} & 67.34($\\pm$0.75)  &  66.44($\\pm$1.40)       & 13.80($\\pm$0.23)  \n& 62.49($\\pm$0.00) &  58.29($\\pm$0.00)   & 13.86($\\pm$0.00)\n  &   49.19/44.88\n\\tabularnewline\n&  & \\gc{(70.05/64.83)}  &  \\gc{(72.86/61.17)}       & \\gc{(72.63/7.62)} \n& \\gc{(63.25/61.76)}  &  \\gc{(62.77/54.40)}\n & \\gc{(84.20/7.55)} &  -\n\\tabularnewline\n  \\cmidrule(lr){2-9} \n&  \\multirow{2}{*}{HMM + BERT-CRF~\\cite{lison2020named}$^{\\dag}$} & 67.49($\\pm$0.89)  &  63.31($\\pm$1.02)       & 39.51($\\pm$0.72)  \n& 62.18($\\pm$0.00) &  56.36($\\pm$0.00)  & 42.65($\\pm$0.00)\n  &   56.77/53.73\n\\tabularnewline\n&  & \\gc{(71.26/64.14)}  &  \\gc{(70.95/57.33)}       & \\gc{(62.49/28.90)}  \n&  \\gc{(66.42/58.45)} &  \\gc{(61.51/52.00)}\n & \\gc{(71.44/30.40)} \n &  -\n\\tabularnewline\n  \\cmidrule(lr){2-9} \n&  \\multirow{2}{*}{CHMM + BERT-CRF~\\cite{li2021bertifying}$^{\\dag}$} & 66.72($\\pm$0.41)  &  63.06($\\pm$1.91)       & 42.79($\\pm$0.22) \n& 63.22($\\pm$0.26) &  58.89($\\pm$0.97)   & 47.34($\\pm$0.57)\n  &   57.52/56.48\n\\tabularnewline\n&  & \\gc{(67.17/66.27)}  &  \\gc{(62.12/64.11)}       & \\gc{(63.19/32.35)}  \n&  \\gc{(61.93/64.56)} &  \\gc{(55.71/62.45)}\n & \\gc{(73.05/35.02)}  &  -\n\\tabularnewline\n\\midrule\n\n\\multirow{4}{*}{One-stage WSSL} \n &\n\\multirow{2}{*}{CONNET~\\cite{lan2019learning}$^{\\dag}$}  &    67.83($\\pm$0.62)    &   64.18($\\pm$1.71)  &  42.37($\\pm$0.72) &  - &  -  &    -   &  58.13/-\n\\tabularnewline\n&  & \\gc{(69.37/66.40)}  &  \\gc{(72.17/57.92)}       & \\gc{(62.88/31.95)}  &  - &  -\n & -  &  -\n\\tabularnewline\n  \\cmidrule(lr){2-9} \n& \n\\multirow{2}{*}{\\textbf{Neural-Hidden-CRF}}\n     &  \\textbf{69.16}($\\pm$0.92)   & \\textbf{66.87}($\\pm$1.79)\n         &  \\textbf{44.94}($\\pm$0.99)\n &  \\textbf{67.99}($\\pm$0.58)   & \\textbf{59.69}($\\pm$0.68)  & \n48.44($\\pm$0.86) &   \\textbf{60.32}/\\textbf{58.71}\n\\tabularnewline\n&  & \\gc{(73.13/65.64)}  &  \\gc{(73.00/61.87)}       & \\gc{(58.27/36.66)}  &  \\gc{(73.12/63.55)} &  \\gc{(71.23/51.44)}\n & \\gc{(68.17/37.85)} &  -\n\\tabularnewline\n\\midrule\n\\multirow{2}{*}{-}\n& \n\\multirow{2}{*}{\\gc{Gold + BERT-CRF$^{\\dag}$}}     &  \\gc{87.38($\\pm$0.34)}   &   \\gc{86.78($\\pm$0.84)}      &  \\gc{78.83($\\pm$0.44)}  &  \\gc{100.00($\\pm$0.00)} &  \\gc{100.00($\\pm$0.00)}   & \\gc{100.00($\\pm$0.00)} &  \\gc{84.33/100.00} \n\\tabularnewline\n&  & \\gc{(87.70/87.06)} &  \\gc{(87.27/86.29)} & \n\\gc{(79.14/78.53)}\n&  \\gc{(100.00/100.00)} & \\gc{(100.00/100.00)}\n & \\gc{(100.00/100.00)}  &  \\gc{-} \n\\tabularnewline\n\\bottomrule\n\\end{tabular}\n\\begin{tablenotes}\n \\footnotesize   \n \\item[1] $\\S$/$*$: Learn from weak supervision labels on the train data and predict on the test data/directly learn from weak supervision labels available on the test data and infer the ground truth labels.\n\\item[2] $\\dag$: Results are reported from~\\citet{zhang2021wrench}.\n\\end{tablenotes}\n\\end{threeparttable} \n}\n\\smallskip\n\\label{Table:6}\n\\hfil\n\\captionsetup{type=table,skip=5pt}\n\\end{table*}",
            "Table:7": "\\begin{table*}[t]\n\\centering\n\\caption{Performance (F1, $\\%$) on  ablation study.\nResults are averaged over 10 runs.}\n\\scalebox{0.68}{\n\\begin{threeparttable}  \n\\begin{tabular}\n{l cccc\n>{\\columncolor{lightgray!20}}c}\n\\toprule \n\\textbf{Method}\n & \\textbf{CoNLL-03(MTurk) (P/I)$^{\\S}$}\n      & \\textbf{CoNLL-03(WS) (P/I/I)$^{*}$}\n              & \\textbf{WikiGold(WS) (P/I/I)$^{*}$}\n                        & \\textbf{MIT-Restaurant(WS) (P/I/I)$^{*}$}\n                          & \\textbf{\\underline{Avg.(P/I/I)}$^{*}$}\n\\tabularnewline\n\\midrule\nW/o-weak-transition & 74.41($\\pm$2.11)/74.79($\\pm$1.38)   &   66.63($\\pm$0.85)/68.61($\\pm$0.72)/65.43($\\pm$0.51)      &  62.09($\\pm$1.06)/60.82($\\pm$1.76)/52.32($\\pm$0.26) \n &  42.95($\\pm$0.43)/45.00($\\pm$0.71)/48.01($\\pm$0.73)   &    61.52/62.31/55.25\n  \\tabularnewline\nW/o-crf-transition & 80.79($\\pm$0.73)/80.96($\\pm$0.23)   &   68.73($\\pm$0.71)/70.35($\\pm$0.40)/66.78($\\pm$0.67)      &  63.89($\\pm$1.59)/62.26($\\pm$2.14)/58.67($\\pm$1.15) \n &  40.94($\\pm$0.86)/42.72($\\pm$1.01)/40.24($\\pm$4.13)   &    63.59/64.08/55.23\n  \\tabularnewline\nSmall-crf-transition & 81.95($\\pm$0.70)/82.25($\\pm$0.39)   &   69.05($\\pm$0.63)/71.25($\\pm$0.76)/67.79($\\pm$1.13)      &  65.71($\\pm$1.68)/64.54($\\pm$1.12)/59.38($\\pm$1.20) \n &  42.20($\\pm$1.77)/44.19($\\pm$1.22)/47.79($\\pm$0.62)   &    64.73/65.56/58.32\n  \\tabularnewline\nSmall-emission & 68.27($\\pm$4.93)/71.20($\\pm$4.40)   &   65.99($\\pm$1.11)/69.52($\\pm$1.53)/64.62($\\pm$2.05)      &  61.47($\\pm$4.16)/60.57($\\pm$2.90)/58.45($\\pm$2.78) \n &  43.48($\\pm$1.84)/45.95($\\pm$0.64)/47.09($\\pm$1.71)   &    59.80/61.81/56.72\n  \\tabularnewline\nOther-classifier-init\n & \\textbf{82.43}($\\pm$0.64)/82.18($\\pm$0.45)   &  69.01($\\pm$0.67)/71.66($\\pm$0.57)/67.07($\\pm$0.84)        &  63.70($\\pm$2.99)/63.15($\\pm$3.30)/53.61($\\pm$0.87)  \n & 42.81($\\pm$1.13)/43.95($\\pm$1.09)/27.61($\\pm$5.63)    &  64.49/65.24/49.43\n\\tabularnewline\nOther-worker-init & 55.15($\\pm$10.82)/54.51($\\pm$11.35)  & 66.53($\\pm$0.74)/68.96($\\pm$0.48)/65.42($\\pm$0.96)        & 62.40($\\pm$1.59)/60.68($\\pm$1.47)/53.12($\\pm$1.00) \n&  41.57($\\pm$0.64)/45.04($\\pm$1.00)/39.96($\\pm$8.15)    &   56.41/57.30/52.83\n\\tabularnewline\nOther-both-init & 43.00($\\pm$13.07)/40.51($\\pm$11.60) &  66.40($\\pm$1.18)/68.85($\\pm$0.97)/65.86($\\pm$1.04)      & 63.43($\\pm$1.26)/61.88($\\pm$1.35)/52.95($\\pm$0.81) \n & 40.55($\\pm$0.88)/43.81($\\pm$0.89)/36.91($\\pm$8.85)   &   53.35/53.76/51.91\n\\tabularnewline\nFreeze-source & 79.75($\\pm$1.09)/80.63($\\pm$0.26)  &  67.58($\\pm$0.80)/70.29($\\pm$0.74)/67.46($\\pm$0.47)       & 65.70($\\pm$1.87)/65.34($\\pm$2.08)/58.03($\\pm$1.81) \n & 44.54($\\pm$0.35)/46.19($\\pm$0.36)/47.04($\\pm$0.84)   &   64.39/65.61/57.51\n\\tabularnewline\n\\midrule\n\\textbf{Neural-Hidden-CRF}\n    &  82.06($\\pm$0.63)/\\textbf{82.28}($\\pm$0.49)   & \\textbf{69.16}($\\pm$0.92)/\\textbf{71.89}($\\pm$0.55)/\\textbf{67.99}($\\pm$0.58)   \n         &  \\textbf{66.87}($\\pm$1.79)/\\textbf{65.55}($\\pm$1.33)/\\textbf{59.69}($\\pm$0.68)  \n &  \\textbf{44.94}($\\pm$0.99)/\\textbf{46.61}($\\pm$0.91)/\\textbf{48.44}($\\pm$0.86)     &   \\textbf{65.76}/\\textbf{66.58}/\\textbf{58.71}\n\\tabularnewline\n\\bottomrule\n\\end{tabular}\n\\begin{tablenotes}\n \\footnotesize   \n \\normalsize\n \\item[1] $\\S$: ``I'' denotes we learn from weak supervision labels on the train data and infer the latent ground truth labels.\n \\item[2] $*$:  ``I/I'' denote we learn from weak supervision labels on train/test data and infer the latent ground truth labels on the train/test data, respectively.\nNote that the latter three datasets are different from dataset ConLL-03 (MTurk), because they also contain weak supervision labels on the test data.\n\\end{tablenotes}\n\\end{threeparttable} \n}\n\\smallskip\n\\label{Table:7}\n\\hfil\n\\captionsetup{type=table,skip=5pt}\n\\end{table*}"
        },
        "figures": {
            "Figure:1": "\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.49\\textwidth]{fig/model.pdf} % Reduce the figure size so that it is slightly narrower than the column.\n\\caption{Probabilistic graphical representation of Neural-Hidden-CRF.}\n\\label{Figure:1}\n\\end{figure*}",
            "Figure:3": "\\begin{figure*}[t] \n\t\\centering \n\t\\vspace{-0.0cm} \n\t\\subfigtopskip=-0pt \n\t\\subfigbottomskip=0pt \n\t\\subfigcapskip=-5pt \n  \t\\subfigure[Dataset ConNLL-03 (MTurk)]{\n\t\t\\label{level.sub.3}\n\t\t\\includegraphics[width=0.5\\linewidth]{fig/transition_crowdsourcing.pdf}}\n        \\hspace{6mm}\n\t\\subfigure[Dataset ConNLL-03 (MTurk)]{\n\t\t\\label{level.sub.4}\n\t\t\\includegraphics[width=0.2\\linewidth]{fig/correlation_crowdsourcing.pdf}}\n\n\t\\subfigure[Dataset ConNLL-03 (WS)]{\n\t\t\\label{level.sub.3}\n\t\t\\includegraphics[width=0.5\\linewidth]{fig/transition_conll.pdf}}\n  \\hspace{6mm}\n\t\\subfigure[Dataset ConNLL-03 (WS)]{\n\t\t\\label{level.sub.4}\n\t\t\\includegraphics[width=0.2\\linewidth]{fig/correlation_conll.pdf}}\n\t\\subfigure[Dataset WikiGold (WS)]{\n\t\t\\label{level.sub.3}\n\t\t\\includegraphics[width=0.5\\linewidth]{fig/transition_wikigold.pdf}}\n  \\hspace{6mm}\n\t\\subfigure[Dataset WikiGold (WS)]{\n\t\t\\label{level.sub.4}\n\t\t\\includegraphics[width=0.2\\linewidth]{fig/correlation_wikigold.pdf}}\n  \t\\subfigure[Dataset MIT-Restaurant (WS)]{\n\t\t\\label{level.sub.3}\n\t\t\\includegraphics[width=0.5\\linewidth]{fig/transition_mit.pdf}}\n  \\hspace{6mm}\n\t\\subfigure[Dataset MIT-Restaurant (WS)]{\n\t\t\\label{level.sub.4}\n\t\t\\includegraphics[width=0.2\\linewidth]{fig/correlation_mit.pdf}}\n \\caption{Comparison between the real weak source  transition matrix and the two weak source transition matrices estimated by Neural-Hidden-CRF on the four datasets.\n(\\textit{i})\nReal: it denotes the probabilistic confusion matrix that we compute by using the truth labels and weak labels in the dataset (in fact, we cannot obtain the real matrix under the theory of our model);\n(\\textit{ii}) \nOur (1):  it is obtained by setting all non-positive elements of the weak source transition matrix to $0$ and normalizing the elements on each row;\n(\\textit{iii}) \nOur (2):  it is obtained by exponentiating all elements of the weak source transition matrix and normalizing the elements on each row;\n(\\textit{iv}) \nIn each sub-figure on correlation coefficient, we calculate the element-level values.} \n\\label{Figure:3}\n\\end{figure*}",
            "Figure:A1": "\\begin{figure*}[ht]\n\\centering\\includegraphics[width=0.95\\textwidth]{fig/appendix_figure.pdf} % Reduce the figure size so that it is slightly narrower than the column.\n\\caption{Performance of more variants for supplementary ablation study. Results are averaged over 20 runs.}\n\\label{Figure:A1}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\begin{array}{c}\np(X)=\\frac{1}{Z} \\prod_c \\phi_c\\left(X_c\\right) \\\\\nZ=\\sum_X \\prod_c \\phi_c\\left(X_c\\right),\n\\end{array}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\exp \\left(\\sum_{a} \\lambda_{a}\\text{state}_{a}(t^{(i)}_{l}, \\mathbf{x}^{(i)}, l)\\right),\n\\label{equation 2}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\exp \\left(\\sum_{b} \\mu_{b}\\text{transition}_{b}(t^{(i)}_{l-1}, t^{(i)}_{l}, l)\\right),\n\\label{equation 3}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\exp \\left(\\sum_{c,j} \\eta_{c,j}\\text{source}_{c,j}(y^{(i,j)}_l, t^{(i)}_l, l)\\right),\n\\label{equation 4}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\begin{aligned}\n&f_{w}(\\mathbf{y}^{(i)}_{l},t^{(i)}_{l-1},t^{(i)}_{l},\\mathbf{x}^{(i)}, l)=\\\\\n&\\left\\{\\begin{array}{l}\n\\text{state}_{a}(t^{(i)}_{l}, \\mathbf{x}^{(i)}, l) \\quad \\qquad  \\qquad\n\\text{if } w=1,.., A\\\\\n\\text{transition}_{b}(t^{(i)}_{l-1}, t^{(i)}_{l}, l) \\qquad \n\\enspace\n\\;   \\, \\text{if } w=A+1,.., A+B \\\\\n\\text{source}_{c,j}(y^{(i,j)}_l, t^{(i)}_l, l)   \\quad \\qquad \\text{ if } w=A+B+1,.., A+B+\\sum_j C_j,\n\\end{array}\\right.\n\\end{aligned}\n\\label{equation 5}\n\\end{equation}",
            "eq:6": "\\begin{equation}\\theta_{w}=\\left\\{\\begin{array}{ll}\\lambda_{a} & \\text{if } w=1,.., A \\\\\\mu_{b} & \\text{if } w=A+1,..., A+B \\\\\n\\eta_{c,j} & \\text{if } w=A+B+1,..., A+B+\\sum_j C_j,\n\\end{array}\\right.\n\\label{equation 6}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\begin{aligned}\n&  p(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta) \\\\\n=&\\frac{1}{\\boldsymbol{Z}(\n\\mathbf{x}^{(i)}; \\Theta)} \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l},\nt^{(i)}_{l-1},\nt^{(i)}_{l},\n\\mathbf{x}^{(i)}, l)\\right),\n\\end{aligned}\n\\label{equation 7}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\boldsymbol{Z}(\\mathbf{x}^{(i)}; \\Theta)=\\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l}, t^{(i)}_{l-1}, t^{(i)}_{l}, \\mathbf{x}^{(i)}, l)\\right).\n\\label{equation 8}\n\\end{equation}",
            "eq:9": "\\begin{equation}\np(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta) = \n\\frac{\\Phi(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)})}{\\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\Phi\\left(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}\\right)}.\n\\label{equation 9}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\text {extract}_{l, t^{(i)}_{l}}(\\sigma_{BERT}(\\mathbf{x}^{(i)})),\n\\label{equation 13}\n\\end{equation}",
            "eq:11": "\\begin{equation}\np(\\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta)=\\frac{\\exp (\\operatorname{score}_{\\Theta}(\\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))}{\\sum_{\\mathbf{t}^{(i)}} \\exp (\\operatorname{score}_{\\Theta}(\\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))},\n\\end{equation}",
            "eq:12": "\\begin{equation}\n\\operatorname{score}_{\\Theta}(\\mathbf{t}^{(i)}, \\mathbf{x}^{(i)})=\\sum_{l=1}^L(\\text{Emission}_{l, t_l^{(i)}}+\\text {CrfTransition}_{t_{l-1}^{(i)}, t_l^{(i)}}),\n\\end{equation}",
            "eq:13": "\\begin{equation}\np(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta)=\\frac{\\exp (\\operatorname{score}_{\\Theta}(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))}{\\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp (\\operatorname{score}_{\\Theta}(\\mathbf{y}^{(i)}, \\mathbf{t}^{(i)}, \\mathbf{x}^{(i)}))},\n\\label{equation 16}\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\begin{aligned}\n& \\mathcal{L}(\\Theta)  =\\sum_{i} \\log p(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}; \\Theta).\n\\end{aligned}\n\\label{equation 18}\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\boldsymbol{Z}(\\mathbf{x}^{(i)}; \\Theta)=\\sum_{\\mathbf{y}^{(i)}} \\sum_{\\mathbf{t}^{(i)}} \\exp \\left(\\sum_{l}\\sum_{w} \\theta_{w} \\cdot f_{w}(\\mathbf{y}^{(i)}_{l},\nt^{(i)}_{l-1},\nt^{(i)}_{l},\n\\mathbf{x}^{(i)}, l)\\right).\n\\label{equation 20}\n\\end{equation}",
            "eq:16": "\\begin{equation}\n\\pi_{m n}^{(j)}=\\rho\\cdot\\frac{\\sum_{i=1}^{I} \\sum_{l=1}^{L}\\mathbb{I}(t^{(i)}_{l}=m) \\mathbb{I}(y^{(i, j)}_{l}=n)}\n{\\sum_{i=1}^{I} \\sum_{l=1}^{L}\\mathbb{I}(t^{(i)}_{l}=m) \\mathbb{I}(y_{l}^{(i, j)} \\neq 0)}\\mathrm{,}\n\\label{equation 21}\n\\end{equation}"
        },
        "git_link": "https://github.com/junchenzhi/Neural-Hidden-CRF"
    }
}