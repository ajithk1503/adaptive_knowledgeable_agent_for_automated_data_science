{
    "meta_info": {
        "title": "FedWalk: Communication Efficient Federated Unsupervised Node Embedding  with Differential Privacy",
        "abstract": "Node embedding aims to map nodes in the complex graph into low-dimensional\nrepresentations. The real-world large-scale graphs and difficulties of labeling\nmotivate wide studies of unsupervised node embedding problems. Nevertheless,\nprevious effort mostly operates in a centralized setting where a complete graph\nis given. With the growing awareness of data privacy, data holders who are only\naware of one vertex and its neighbours demand greater privacy protection. In\nthis paper, we introduce FedWalk, a random-walk-based unsupervised node\nembedding algorithm that operates in such a node-level visibility graph with\nraw graph information remaining locally. FedWalk is designed to offer\ncentralized competitive graph representation capability with data privacy\nprotection and great communication efficiency. FedWalk instantiates the\nprevalent federated paradigm and contains three modules. We first design a\nhierarchical clustering tree (HCT) constructor to extract the structural\nfeature of each node. A dynamic time warping algorithm seamlessly handles the\nstructural heterogeneity across different nodes. Based on the constructed HCT,\nwe then design a random walk generator, wherein a sequence encoder is designed\nto preserve privacy and a two-hop neighbor predictor is designed to save\ncommunication cost. The generated random walks are then used to update node\nembedding based on a SkipGram model. Extensive experiments on two large graphs\ndemonstrate that Fed-Walk achieves competitive representativeness as a\ncentralized node embedding algorithm does with only up to 1.8% Micro-F1 score\nand 4.4% Marco-F1 score loss while reducing about 6.7 times of inter-device\ncommunication per walk.",
        "author": "Qiying Pan, Yifei Zhu",
        "link": "http://arxiv.org/abs/2205.15896v2",
        "category": [
            "cs.DC",
            "cs.CR",
            "cs.LG"
        ],
        "additionl_info": "10 pages, 8 figures, to be published in the Proceedings of the 28th  ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n%XXWhat is node embedding, why it is important/applications of node embedding, current practiceXX\n\nWith the rise of graph convolutional networks and the abundant graph-structured data, graph representation learning has drawn great attention as it effectively extracts the structural information from complex graphs. \nNode embedding is a representation learning problem that captures the features of nodes and topological information in a graph with vectors.\nThe essence of node embedding is to represent each node such that the distance in the embedding space measures the dissimilarity among nodes in the original graph. \n%Let us suppose there is a global graph $G=(V,E)$ and we need to embed all nodes $v\\in V$ with vectors $\\vec{x_v}$.\nIf two nodes have similar structural features, the distance of their corresponding embedding vectors is small. Node embedding can be deployed in various downstream prediction tasks, such as link prediction \\cite{wang2018shine}, anomaly detection \\cite{yu2018netwalk}, and so on. \n\nTypical node embedding approaches follow a similar encoder-decoder framework. First, an encoder is defined to map a node to a vector. Then a node proximity function is defined to examine whether the current encoding result approximates the topological information of the original graph. Last but not least, the encoder is optimized based on the previous examination result. The key differences among various node embedding approaches are the definitions of encoders and node proximity functions. For unsupervised node embedding, node proximity functions typically rely on random walk methods due to their expressiveness and efficiency. Deepwalk \\cite{perozzi2014deepwalk} pioneers the random-walk-based embedding by leveraging unbiased random walk to measure the similarity between each node. Node2vec \\cite{grover2016node2vec} later replaces unbiased random walk with biased one to capture local microscopic and global macroscopic views of a graph. On the other hand, for supervised node embedding, node proximity functions are usually defined based on the labels in the downstream tasks. As neural networks have shown their great power in various fields, some frameworks also deploy neural networks to embed nodes in supervised and unsupervised settings \\cite{kipf2016semi, hamilton2017inductive, velivckovic2017graph}. \n\n%XXGrowing awareness of privacy generate federated paradigm. What is federated paradigm. \n\nAll the representation learning methods discussed above operate in a centralized setting, where a server owns the complete graph information. However, the growing awareness of privacy and the establishment of regulations and laws, e.g., GDPR \\cite{EUdataregulations2018} and CCPA \\cite{CAdata}, demand stricter data privacy protection. \n%restrict the sharing of private data.  Regulations and laws such as GDPR \\cite{EUdataregulations2018} in European Union and CCPA \\cite{CAdata} in California are established world wide to protect users' raw data. \nWith the increasing computing power of mobile devices, a federated paradigm is proposed by Google and later being widely studied and deployed in real life\\cite{mcmahan2017communication}. In the federated paradigm, devices with private data work collaboratively with a central server to conduct machine learning and data analytics tasks without uploading the raw data. The core of the federated paradigm is to train local models on individual devices first. Then the trained local models are aggregated at the server side to generate a global model after multiple iterations between clients and the server. In this whole process, only the extracted insights are uploaded to the server without revealing clients' raw data. \nThe applications of federated learning on regular data structures, e.g., images \\cite{liu2020fedvision}, text\\cite{yang2018applied}, have demonstrated tremendous success in real life.\n\n%XXExisting federated graph embedding falls into xx categories: node-level embedding, graph-level embedding. Intro to node-level embedding, intro to other categories.XX\n\nWith the wide prevalence of graph-structured data, the federation of graph embedding, especially graph neural network, has also attracted significant attention. Existing federated graph embedding studies can be categorized into three types depending on how much graph information each local client owns: node-level\\cite{mei2019sgnn,meng2021cross}, subgraph-level\\cite{wu2021fedgnn} and graph-level\\cite{zhou2020vertically,zheng2021asfgnn,peng2021differentially} federated graph embedding. \nWhereas graph-level and subgraph-level federated graph embedding still assume each client holds a correlated complete graph or a part of a global graph, node-level federated graph embedding emphasizes the privacy of individual nodes in one global graph. Each node in the graph is owned by one client and it is only aware of its neighboring situation.\n%Graph-level federated graph embedding assumes that each client holds one or more distinctive and private graphs. All the local graphs share common vertices. Clients learn node embedding based on their local graph(s) and upload the results to the server for aggregation. Subgraph-level federated graph embedding assumes that each client owns a part of the global graph and neither clients nor the server holds the complete graph information. \n%A particular case of the subgraph-level setting is the user-item graph generated in recommendation system. \n%The setting with the finest granularity is the node-level federated graph embedding. In the node-level federated embedding, \nThe server only maintains the minimal node membership information. \n %Node-level settings emphasize the privacy of individual nodes in one global graph. \n %Similar to the previous two settings, the central server is not allowed to know the complete graph.\n Many real-world networks such as social networks \\cite{zhou2008brief} and sensor networks \\cite{yick2008wireless} naturally born with this setting. \n It is straightforward to notice that node-level graph embedding is the most challenging task since it contains the least structure information on each local client.\n\nPrevious works on node embedding in federated node-level scenarios mainly concentrate on supervised settings with labels from downstream tasks.\n%Very few federated node embeddings are proposed. \nSGNN\\cite{mei2019sgnn} takes advantage of a degree list to compute node distance and manages to perform supervised node embedding under privacy constraints.  GraphFL\\cite{wang2020graphfl}, motivated by meta learning, addresses semi-supervised node embedding in federated networks. Nevertheless, in reality, most data are unlabeled; manual labeling data is also expensive due to the large scale of real-world graphs. These all motivate the study of unsupervised node embedding in the federated environment, which has barely been studied yet. \\textit{\nConsidering the huge demand in processing unlabeled graph data and the growing restrictions on node-level data access, we aim at filling this gap in this paper. }\n\n% In centralized settings, all the graph information including edges, nodes and node features is accessible to the server. Graph Neural Network (GNN) is proposed to address this problem.  \n\n%Direct deployment of GNN cannot embed nodes in privacy-preserving federated networks since it needs both node features to compute initial node embeddings and link information to aggregate embeddings of neighbouring nodes to a local node. But these information cannot be directly sent to the server due to privacy concerns. \n\nThe challenges for providing unsupervised node embedding in the node-level federated setting come from three perspectives. First, \\textit{learning}. Inaccessibility of graph information impedes deployment of conventional node proximity functions in federated graphs. With each client possessing just one-node information, achieving centralized competitive representation performance calls for new designs.\nSecond, \\textit{privacy}. How to protect raw graph connection information from other clients and the central server, preferably theoretically guaranteed, is non-trivial. Naive deployment of the differential privacy measures may greatly affect the expressiveness of the learned results. Last but not least, \\textit{communication efficiency.} The node size of a practical network can scale to tens of thousands or even larger. Straightforward collaborations among all clients incur prohibitive communication costs in practice and hinder its deployment. How to embed nodes communicationally efficiently should be seriously addressed. \n\n%First, we need to design a communication strategy involving local devices and a server to generate random walk sequences given that each device stores its corresponding one-hop neighbour information and this information cannot be leaked. Second, after generating the random walk sequence, we need to encode the sequence to preserve the privacy of edge information and to control the utility loss caused by the encoding. \n\n\n\nIn this paper, we propose a federated random-walk-based node embedding approach, named FedWalk, that successfully operates in decentralized node-level visibility graphs. \n%We propose a new scheme called FedWalk that can perform random walks in federated networks, thus obtaining node embeddings. \nTo do so, FedWalk first adopts a novel hierarchical clustering tree (HCT) constructor which hierarchically clusters vertices based on their linkage similarity in a federated graph. \n%Next, it uses a Device-to-Device (D2D) communication framework to walk from one device to another. \nSecond, it has a random walk sequence encoder to output a sequence with $\\epsilon-$differential privacy. Last but not least, FedWalk includes a two-hop neighbor predictor to predict the possible two-hop nodes without actually visiting the direct ones. \nTo the best of our knowledge, FedWalk is the first work for node-level federated unsupervised graph embedding, where each node is isolated and maintained by a data holder with only one-hop connection information available.\n%Both the random walk sequence encoder and the two-hop neighbour predictor rely on the HCT constructed at the preparation stage.  \nIn summary, our contributions are summarized as follows:\n\\begin{itemize}\n    \\item We propose a novel federated unsupervised node embedding framework that robustly captures graph structures, guarantees differential privacy and operates with great communication efficiency.\n    \\item We propose a differentially private HCT constructor that extracts structural information from decentralized data holders. A dynamic time warping algorithm seamlessly handles the structural heterogeneity across different nodes.\n    %A novel degree matrix is used to extract 2-hop neighbouring information of each vertex.\n    \\item We propose a novel random walk generator with a differential private sequence encoder and a neighbor predictor for communication efficiency. \n    \\item We prove that the dissimilarity value loss of node pairs in our framework is bounded. We also theoretically quantify the reduced communication cost benefited from our neighbor predictor.\n    \\item Extensive experiments on two datasets of different scales demonstrate that FedWalk can achieve centralized competitive results by losing up to 1.8\\% Micro-F1 score and 4.4\\% Marco-F1 score while decreasing about 6.7 times of inter-device communication per random walk. \n\\end{itemize}\n\n%The paper proceeds as follows.  We first review the related works in section \\ref{sec:rw}. Then we present the preliminaries in section \\ref{sec:pre}. In section \\ref{sec:design}, we introduce our methods FedWalk in details. Next, we display our experimental results in section \\ref{sec:exp}. Last we conclude the paper in section \\ref{sec:con}. \n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec:rw}\nIn this section, we review the related works of this paper from three perspectives: unsupervised node embedding in the centralized setting; the emerging federated learning and analytics paradigm; the growing works on federated graph representation.\n",
                "subsection 2.1": {
                    "name": "Unsupervised Node Embedding",
                    "content": "\nNode embedding uses low-dimensional vectors to represent nodes in a graph. In practice, the scale of a graph is usually too big to have all nodes labeled. The inevitable absence of supervision information triggers the formation and development of unsupervised node embedding algorithms.  We can classify these algorithms into two sorts: (1) shallow networks with one hidden layer; (2) deep networks with multiple graph-based layers.  \n\nRandom-walk-based shallow embedding supports effective solutions for many graph-related tasks\\cite{goyal2018graph}. DeepWalk\\cite{perozzi2014deepwalk} pioneers this branch by \ntreating the path of random walk similar to the word sequences processing in the NLP field and takes advantage of the Skipgram model to accomplish embedding. Since DeepWalk, we have witnessed significant efforts in random-walk-based shallow embedding, including Node2Vec\\cite{grover2016node2vec}, HARP\\cite{chen2018harp}, etc.\n%Motivated by this idea, Node2Vec\\cite{grover2016node2vec} further customizes the different representation requirements of structural and neighbour information through biased random walks. Walklets\\cite{perozzi2017don} modifies the random walk strategy by skipping some nodes to form a corpus representing special distance dependency. HARP\\cite{chen2018harp} introduces a new way to initiate node embedding to avoid from being stuck in local optimal. \n%Verse\\cite{tsitsulin2018verse} incorporates random walk and similarity function to embed nodes more precisely. \n\nDeep embedding leverages multi-layer convolutional networks to learn graph representations. \n%Since GCN \\cite{kipf2016semi}, extensive efforts have been made to learn representations via multi-layer graph-based convolutional networks.\nApplication of convolutional networks in node embedding is started with GCN\\cite{kipf2016semi} propagating neighboring embedding through layers. Since then, extensive efforts \\cite{hamilton2017inductive} \\cite{velivckovic2017graph} %\\cite{pan2018adversarially}  \nhave been made to improve representation performance in practical large graphs.\n%GCN suffers from slow propagation speed. \n%To address this issue, GraphSAGE\\cite{hamilton2017inductive} speeds up the propagation using negative sampling. Another method called GAT \\cite{velivckovic2017graph} assigns different weights to the neighbours so that propagation performs in a fine-grained way. Geom-GCN\\cite{pei2020geom} incorporates geometric aggregation scheme with GNN to capture long-range dependencies in the graph. Graph autoencoder\\cite{pan2018adversarially} takes GCN as encoder and predicts links from reconstructed adjacent matrix. \nAlbeit the above well-developed node embedding methods, all of them require upload of raw data to the central server, which gradually becomes hard due to laws and regulations. \n\n\n"
                },
                "subsection 2.2": {
                    "name": "Federated Learning and Analytics",
                    "content": "\nFederated learning and analytics have attracted great interest from both academia and industry due to their capability in conducting learning and analytic tasks without centralizing the raw data\\cite{mcmahan2017communication}. Specifically, federated learning has been widely studied for collaborative neural network training. %FedAvg\\cite{mcmahan2017communication} presents a collaborative learning framework in which the server aggregates all the local weights to obtain a global model. \nTremendous efforts have been devoted to improving federated learning in terms of convergence speed, training accuracy, communication efficiency, fairness, etc. Several good surveys are presented here for interested readers\\cite{li2020federated}.   %\\cite{mcmahan2017communication,li2020federated}.\nIn practice, federated learning has also been applied in natural language processing\\cite{yang2018applied}, computer vision\\cite{liu2020fedvision},  and many other areas. \n\nFederated analytics, a recently proposed sibling problem, extends the applications of federated paradigm from neural network training to diverse data analytic tasks or unsupervised learning scenarios\\cite{ramage_2020}.\n%Though some machine learning models can be easily deployed in the federated learning framework, some need modification of the general architecture due to the non-trivial correlation between local datasets and the difficulty of model aggregation. \n%Such modifications usually occur in semi-supervised and unsupervised data analysis scenarios. \n%An algorithm is proposed to represent the pattern of radio frequency identification data streams\\cite{wu2013modeling}. \nTriHH\\cite{zhu2020federated} discovers the heavy hitters in a population of data without uploading the data to the server. Federated location heatmap from Google applies distributed differential privacy to generate location heatmaps\\cite{bagdasaryan2021sparse}. FedFPM \\cite{zibo} presents a unified framework for frequent pattern analysis.\n%What's more, a federated framework called FedFPM is proposed to mine frequent mining \\cite{zibo}.  \n%Still various kinds of federated machine learning algorithms need to be further studied. \nWhile most federated designs follow an iterative collaboration pattern, the exact extracted insights and aggregation methods vary greatly across different problems. This work pushes the boundary of the federated applications by studying a novel federated unsupervised node embedding problem. FedWalk also encompasses its design to provide communicating efficient, differentially private, centralized competitive node embedding.  \n\n"
                },
                "subsection 2.3": {
                    "name": "Federated Learning on Graphs",
                    "content": "\n%Scientists have been working on addressing graph-related problems under federated settings. \nUnlike classical federated learning problems, the explicit linkage between nodes acts as a barrier to adapting algorithms in centralized graphs to federated ones. Due to privacy concerns, graphs cannot be uploaded to the server and can only be stored locally on each data holders. Despite this, some studies still succeed in designing algorithms to compute node embedding in federated graphs, especially under the subgraph-level visibility\\cite{wu2021fedgnn} and graph-level visibility\\cite{zhou2020vertically}\\cite{zheng2021asfgnn}\\cite{peng2021differentially}, where each client owns a part of the graph or one correlated graph.  In the most strict node-level federated graph works, devices only store the feature and edge information of one node. Leakage of node feature and edge information is forbidden. Under some circumstances, even the sharing of node labels is limited. This setting is applicable in social networks, Internet of Things (IoT) networks, and so on. Some studies have addressed the problem of inaccessibility of node data. SGNN\\cite{mei2019sgnn} uses one-hot encoding to process local information and introduces a new node distance calculation method based on an ordered degree list to process structural information. A paradigm called CNFGNN\\cite{meng2021cross} alternatively trains models on local devices and the server in a graph representing spatio-temporal data structure. \n\nAlthough there are some novel methods to train federated GNN, their models can only be applied in supervised settings with nodes labeled. How to provide node embedding in unsupervised federated scenarios remains to be solved. Furthermore, the extensive communication cost in federated node-level graph embedding should be carefully addressed to be practical in real-world systems.\n%Besides the frequent exchanges of encoded information among devices facilitate their algorithms, which incurs great cost of device-to-device communication. \n\n%Graph Neural Network (GNN) is the state-of-art method to find node embedding in one or multiple graphs. \n%Due to privacy concerns, graphs cannot be uploaded to server and stored only in local devices. Neural networks to obtain node embedding in federated graphs is called federated GNN. \n%Some scientists have proposed different neural network models in different settings of federated graphs. \n\n%Federated graph problems can be sorted into three categories: node-level, subgraph-level and graph-level. In a node-level federated graph the device only stores feature and edge information of one node. Leakage of node feature and edge information is forbidden. Under some circumstances, even the sharing of node labels is limited. This setting is applicable in social network, Internet of Things (IoT) networks and so on. Some studies have addressed the problem of inaccessibility of node data. SGNN\\cite{mei2019sgnn} uses one-hot encoding to process local information and introduces a new node distance calculation method based on ordered degree list to process structural information. A paradigm called CNFGNN\\cite{meng2021cross} alternatively trains models on local devices and on server in a graph representing spatio-temporal data structure. \n%In a subgraph-level federated settings, a global graph is split into small subgraphs which stored on different devices. A typical example is the recommendation system. For each user, it has its own recommendation user-item network. A framework to tackle the recommendation problem called FedGNN\\cite{wu2021fedgnn} trains the subgraph locally and noises the weights through differential privacy techniques to protect user privacy. \n%Graph-level settings require each device to have its own graph with common nodes but different node features and edges. This models the situation where organizations or enterprises own different user information and are unwilling to share with each other. Two algorithms called  VFNN\\cite{zhou2020vertically} and ASFGNN\\cite{zheng2021asfgnn} both deploy an encryption method called secret sharing to enable computation of encrypted values to train GNN in graph-level settings.  FKGE\\cite{peng2021differentially} dealing with knowledge graphs adds a Laplace noise to protect information and takes advantage of Generative Adversarial Network (GAN) to compensate the utility loss of noise. \n\n\n\n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Preliminaries",
                "content": "\n\\label{sec:pre}\nIn this section, we first present a basic introduction to the classical node embedding process in the central situation. %As edges are inaccessible in federated settings but important to solve the node embedding problem, we provide an overview of a special data structure, a hierarchical clustering tree, which can represent the structural information of vertices in graphs.%\nThen considering the special emphasis of privacy in a federated system, we introduce the preliminaries on a widely-used privacy protection measure, differential privacy. \n\n",
                "subsection 3.1": {
                    "name": "Classical Node Embedding",
                    "content": "\nNode embedding is one of the most fundamental problems in graph theory.\nFor a graph $G=(V,E)$, node embedding represents the structural information of vertices. Vertices that are linked with shorter distances or share similar topological structures tend to be embedded with shorter distances. %Fig \\ref{nea} plots the raw graph of Zachary karate club dataset\\cite{zachary1977information} in which each vertex represents a person and its color distinguishes the university karate club the corresponding person belongs to. Each edge represents a tie between two people. We utilize DeepWalk\\cite{perozzi2014deepwalk} to generate a 128-dimension node embeddings through 5 80-vertex-long walks starting from every vertex. Then the embeddings are compressed into 2-dimension vectors by Principle Components Analysis (PCA) to visualize the outputs. One can conclude that vertices linked with each other in the original graph are closer in the embedding spaces. \n\nThe node embedding algorithm is generally composed of three parts: (1) an encoder; (2) a dissimilarity function; (3) an optimizer. The encoder (ENC) is a function that maps a vertex to a fixed-dimension vector defined as Eq.\\ref{eq:enc}.  \n\\begin{equation}\n    ENC:V\\rightarrow \\mathbb{R}^d\n    \\label{eq:enc}\n\\end{equation}\nOne of the most basic encoders is shallow encoding which serves as a look-up table for all the vertices. A matrix stores all the embedding vectors and an indicator vector assigns different embedding vectors to different vertices. \n\nThe dissimilarity function (DISSIM) in the form of Eq. \\ref{eq:sim} defines to what extent two vertices are dissimilar to each other. \n\\begin{equation}\n\\label{eq:sim}\n    DISSIM:V\\times V\\rightarrow \\mathbb{R}\n\\end{equation}\nA typical dissimilar function is the probability of two vertices that don't co-occur in a random walk.  \n\nThe optimizer is a machine learning algorithm deployed to optimize the parameters in the encoder aiming to realize that the distance between two embedding vectors approximates the corresponding similarity value. The goal can be formulated as \n\\begin{equation}\n    \\label{eq:goal}\n    \\min_{ENC} \\sum _{v_1,v_2\\in V}\\text{dist}(ENC(v_1),ENC(v_2))- DISSIM(v_1,v_2).\n\\end{equation}\n\nIn unsupervised settings, random-walk-based graph embedding is widely used to learn vector representations for nodes. \n%To achieve this goal, a strategy called random walk which efficiently defines a node similarity function is deployed to explore the structure of the graph\\cite{perozzi2014deepwalk}.\nA random walk generator randomly samples the neighbors of the last vertex to form a sequence of vertices of a certain length. Using the embeded node proximity information in these sequences, models like the Skipgram model map the results of the random walks into real values representing certain node proximity. The probability of co-occurrence of two vertices in a random walk sequence represents the similarity between two vertices. \n%Then introduces a model called Skipgram in the natural language processing field as the optimizer to improve the node embeddings iteratively based on the random walk sequences. In NLP, Skipgram model is deployed to predict the next word in a word sequence by maximizing the probability of the target word within a word window which is a word sequence of small length. This can be easily adapted to node embedding by considering the random walk sequences as word sequences in graph scenario. \n\n\n"
                },
                "subsection 3.2": {
                    "name": "Differential Privacy",
                    "content": "\nDifferential privacy (DP) \\cite{dwork2006differential} is a prevalent measure defining whether an information-sharing strategy preserves privacy or not. It is motivated by the idea that if a single element in the dataset is replaced, the result of published information differs very small from the ideal one. The formal definition is shown in Def.\\ref{def:dp}. \n\\begin{definition}\n\\label{def:dp}\nLet $\\epsilon\\in \\mathbb{R}^+$ and $\\mathcal{A}$ be a randomized algorithm. The algorithm $\\mathcal{A}$ is defined as $\\epsilon-$ differential privacy if for all datasets $D_1 $ and $D_2$ whose elements are the same except for one element and all sets $S\\subseteq \\text{Range}(\\mathcal{A})$, \n\\begin{equation}\n    \\text{Pr}[\\mathcal{A}(D_1)\\in S]\\leq \\exp{(\\epsilon)}\\text{Pr}[\\mathcal{A}(D_2)\\in S]\n\\end{equation}\n\\end{definition}\nMany DP mechanisms have been  proposed, such as Laplace mechanism \\cite{dwork2006calibrating}, exponential mechanism\\cite{mcsherry2007mechanism} and randomized response\\cite{warner1965randomized}. \n%In this paper, we take advantage of exponential mechanism to protect edge information. \n\n"
                }
            },
            "section 4": {
                "name": "Design",
                "content": "\nIn this section, we first briefly introduce the overall design of our framework, FedWalk. We then present two core components of FedWalk, hierarchical clustering tree constructor and random walk generator in detail. \n\\label{sec:design}\n",
                "subsection 4.1": {
                    "name": "Design Overview",
                    "content": "\n\nFedWalk is a random-walk-based graph embedding algorithm for unsupervised federated node embedding when each client \\footnote{Clients and data holders are used interchangeable in this paper.} only has its neighboring information. The server is not allowed to access the raw local graph information and is only aware of which node is in the graph.\nFedWalk can not only embeds structure information without labeled data but also provides differential privacy to the clients' local connection information. \nFedWalk can be divided into three parts as shown in Fig. \\ref{fig:fedwalk}.\nTo tackle the problem of inaccessibility of global graph, we introduce a differentially private \\textit{hierarchical clustering tree (HCT) constructor} based on a newly-proposed distance computing metric that clusters vertices according to their structural information. The tree is constructed gradually among clients under the coordination of a central server. \nA revised \\textit{random walk generator} is then developed to construct random walk sequences in federated settings based on the constructed hierarchical clustering tree. \nIn the random walk generator, FedWalk contains a sequence encoder to encrypt the edge information to provide privacy and a two-hop neighbor predictor to save the communication cost among devices. \n%As mentioned before, the sequence encoder and the two-hop neighbour predictor complete their tasks based on the results of the tree obtained in the first part. \nThe typical \\textit{Skip-Gram model} is then used to improve node embedding based on the random walk sequence generated in the second part. \n\n%XXA decentralized communication algorithm is devised for users to generate random walk sequences based on the constructed hierachical clustering tree.XX We further propose a two-hop neighbor predictor to minimize the Device-to-Device communications during the generation of random walk sequence. \n\n%The solution can be divided into three parts as shown in Fig. \\ref{fig:fedwalk}. The first part is construction of a binary hierarchical clustering tree based on a new distance computing method. The tree is going to be used to encode random walk sequences and predict 2-hop neighbour in the next stage. The second part is a revised random walk generator to construct random walk sequences in federated settings. In the random walk generator, we introduce a sequence encoder to encrypt the edge information and a two-hop neighbour predictor to save the communication cost among devices. As mentioned before, the sequence encoder and the two-hop neighbour predictor complete their tasks based on the results of the tree obtained in the first part. The third part is the typical Skipgram Model to improve node embedding based on the random walk sequence generated in the second part. \n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Hierarchical Clustering Tree Constructor",
                    "content": "\nThe HCT constructor aims to build a hierarchical clustering tree in federated settings which will later be used for encoding and transmission of random walk sequences. It is designed to preserve structural information with privacy guaranteed. \n\n%The whole process works iteratively following the federated paradigm. \n\n\nA hierarchical clustering tree (HCT) clusters vertices in the graph using edge information. It is widely used in NLP tasks\\cite{ushioda1996hierarchical}, biological information processing\\cite{yin2014measure} and so on. A hierarchical clustering structure separates vertices into two groups and then these groups further split themselves into two small groups. This splitting process continues until every group only contains one vertex.  The leaf nodes in the tree correspond to the vertices in the graph. A none-leaf node, sometimes called an  internal node, clusters a group of vertices by putting them to its left subtree or right subtree. As Figure \\ref{fig:hct} illustrates, the root node clusters the vertices into two groups $\\{ 1,2,3\\}$ and $\\{ 4,5,6\\}$. The internal nodes further split the groups into different clusters. It is straightforward to observe that every HCT has $|V|-1$ internal nodes for a certain graph $G=(V,E)$.\n\n\n\n% There are various of binary trees to cluster vertices. To evaluate whether a tree clusters vertices well or not, a likelihood function $\\mathcal{L}(T)$\\cite{clauset2006structural} defined in Eq.\\ref{cm} estimates the probability of the tree representing the structural information of original graph $T$. \n% \\begin{equation}\n%     \\label{cm}\n%     \\mathcal{L}(T)=\\prod_{i=1}^{|V|-1}(\\theta _i)^{E_i}(1-\\theta _i)^{L_iR_i-E_i}\n% \\end{equation}\n% where $L_i$($R_i$) is the number of leaves in the left(right) subtree  of $i$th internal node, $E_i$ is the number of edges crossing two clusters of vertices split by the $i$th internal node and $\\theta _i=\\frac{E_i}{L_iR_i}$ is an estimator of probability that an edge crosses the two clusters.   XXwhy explain this?XX\n\n\nIn a centralized graph, a typical framework to construct a hierarchical clustering tree is first to compute the structural dissimilarity matrix (e.g., topological overlapping matrix \\cite{ravasz2002hierarchical}) between all vertices pairs of the graph. \n%A widely-used matrix is the topological overlapping matrix in which two vertices directly connecting and sharing more common neighbors are considered more similar\\cite{ravasz2002hierarchical}. \nThen it utilizes a  hierarchical clustering algorithm to build a tree based on the dissimilarity matrix. However, since the federated settings forbid direct update of edge information to the server, we need to design a new approach to compute the dissimilarity matrix.\n\nIn our design, the constructor first computes an ordered degree matrix for each vertex to represent its structural feature. Dynamic Time Warping (DTW) is introduced to compute the dissimilarity between each vertex pair due to the heterogeneous sizes of the matrices  emerged in our setting. Then these dissimilarity values help to form a hierarchical clustering tree. \nThe process is illustrated in Fig.\\ref{fig:hctcon}. The complete algorithm for HCT construction is displayed in Alg. \\ref{hct-constr} in Appendix B due to the space limit.\nWe then present the design details as follows. \n\n\n\n%\\subsubsection{PrivaCT}\n%PrivaCT\\cite{phcfn} is an algorithm to construct a HCT in a federated networks. It takes advantage of degree vector of each vertex to obtain neighbour information under privacy-preserving constraints. Based on the degree vectors, we can calculate the dissimilarity matrix using the distance between different degree vectors. The matrix enables an iterative update of HCT aiming to maximize modified Dasgputa's cost function deploying Monte-Carlo method. \n\n\n\\paragraph{Ordered Degree matrix} We first define an ordered degree matrix for each vertex to compute the dissimilarity between two different nodes. \nThe server randomly numbers the vertices in its system and virtually groups all the vertices into $k$ bins. Each bin contains at least one vertex. Larger $k$ preserves finer structural information while smaller $k$ preserves more privacy. In our scenario, $k$ is upper-bounded by $\\ln{|V|}$ due to privacy concerns. The server then sends the group plan to each device. Since every device stores a subgraph representing the link relationship between itself and neighboring devices, each device can count the number $c_i,i\\in [1,2,\\cdots , k]$ of its neighbors in the $i$-th bin. $c_i$ values are then noised with Laplacian noise $Lap(0,\\frac{1}{\\epsilon})$ to guarantee $\\epsilon-$ differential privacy, forming a vector $\\bm{c_v}=(c'_1,c'_2,\\cdots ,c'_k)\\in \\mathbb{R}^k$ for a vertex $v\\in V$. It is worth mentioning that elements in $\\bm{c_v}$ can be negative real numbers due to the Laplacian noise. Moreover, the sum of vector elements $\\sum_{i=1}^kc'_i$ is the\nnoised degree value of vertex $v$. \n3\nThese vectors $\\bm{c_{v_1}},\\bm{c_{v_2}},\\cdots, \\bm{c_{v_n}}$ are sent to the server and the server advertises these vectors to all the devices. \nEach device corresponds to a vertex $v$ with degree \\textit{deg(v)} in the graph. Denote the neighbors of a node $v$ as $u_1, u_2, \\cdots, u_{deg(v)}$.  Each device  organize its neighboring vertices into an ordered list $ N(v)$, where $N(v)=\\left[ u_{(1)},u_{(2)},\\cdots ,  u_{(deg(v))}\\right]$, in an ascending order based on the estimated degrees of its neighboring vertices. \nIn this way, each device can form an ordered degree matrix $\\bm{M_v}$ (Eq.\\ref{degree_matrix}) of size $|N(v)|\\times k$ where each row is the degree vector of the corresponding neighboring vertex. \n\\begin{equation}\n\\label{degree_matrix}\n    \\bm{M_v}=\\left[ \n    \\begin{aligned}\n    &\\quad\\bm{c_{u_1}}\\\\\n    &\\quad\\bm{c_{u_2}}\\\\\n    &\\quad\\quad\\vdots \\\\\n    &\\bm{c_{u_{|N(v)|}}}\\\\\n    \\end{aligned}\\right]\n\\end{equation}\n\n\n\nThis ordered degree matrix represents 2-hop structural information for each vertex. Metrics from different devices have different numbers of rows because they have different numbers of neighbors. Each device uploads this ordered matrix to the server to calculate the dissimilarity matrix.  \n\n\n\\paragraph{Dissimilarity matrix} \nGiven the structure-embedded ordered degree matrix from the previous step, the server constructs a dissimilarity matrix for the whole graph. \nA dissimilarity matrix in a graph of size $|V|\\times |V|$ quantifies dissimilarity for each node pair. Since the ordered degree matrix is expressive of the structural information of a vertex, we use the distance between two ordered degree matrices to determine the dissimilarity matrix.  However, the metrics of each device can have unequal numbers of rows because they have different numbers of neighbors. Classical l1-norm of the subtraction of two metrics no longer applies. Therefore, we adopt Dynamic Time Warping (DTW)\\cite{berndt1994using} which computes the distance between two sequences of unequal length by deploying the dynamic programming algorithm. The algorithm can be further extended to compute distances of arrays of different dimensions. \n\nLet us suppose that two matrices $\\bm{M_u}$ and $\\bm{M_v}$ have $x$ and $y$ rows where $x\\ge y$ correspondingly. As mentioned before, the first matrix represents neighboring vertices $N(u)$ while the second matrix represents neighboring vertices $N(v)$. Each row vector can be seen as a feature vector of a vertex. The server uses l1-norm to compute the dissimilarity between two row vectors, which aims to pair each vertex in $N(u)=[u_1,u_2,\\cdots ,u_x]$ with exactly one vertex in $N(v)=[v_1,v_2,\\cdots ,v_y]$ with smallest dissimilarity. This can be achieved using dynamic programming formulated in Eq. \\ref{dp} \n\\begin{equation}\n    \\label{dp}\n    \\text{cost}(i,j)=\n    \\begin{aligned}\n    \\\\\n    &\\min \\{\\text{cost}(i-1 ,j),\n\\text{cost}(i,j-1), \\text{cost}(i-1,j-1)\\}\\\\\n&+||\\bm{c_{u_i}}-\\bm{c_{v_j}}||_1, \\forall i\\in [1,x], j \\in [1,y]\n\\end{aligned}\n\\end{equation}\nwhere cost is a function measuring how well the former $i$ vertices in $N(u)$ are paired with the former $j$ vertices in $N(v)$. \n\\begin{equation}\n    \\text{dissim}(u,v)=\\text{cost}(x,y)\n    \\label{dissim}\n\\end{equation} is the dissimilarity value for two vertices $u$ and $v$. The dissimilarity value is noised due to privacy restrictions but it is proved to have upper-bounded information loss according to the following Theorem \\ref{th:ub}. Proofs of our theorems are presented in the appendix.\n\n\\begin{theorem}\n\\label{th:ub}\nThe expectation $\\mathbb{E}[\\text{dissim}(u,v)-\\text{dissim}'(u,v)]$ where $\\text{dissim}'(u,v)$ is the original dissimilarity value of vertices $u$ and $v$ is bounded by \n\\begin{equation}\n    \\mathbb{E}[\\text{dissim}(u,v)-\\text{dissim}'(u,v)]\\leq \\frac{3k(\\max_{v\\in V}|N(v)|)^2}{2\\epsilon}.\n    \\label{eq:ub}\n\\end{equation}\n\\end{theorem}\n% \\begin{proof}\n% See Appendix A. \n% \\end{proof}\nIn practice, the left-hand side in Eq.\\ref{eq:ub} is dependent upon the degrees of two vertices.\n\nThe server computes the dissimilarity for every vertex pair in this way and gets a dissimilarity matrix. Then it generates an HCT using a hierarchical algorithm based on this dissimilarity matrix. Considering the large scale of practical networks, we use hybrid hierarchical clustering \\cite{sun2009efficient} to construct the HCT. Other compatible clustering algorithms can also be used in our framework. \n\n%The HCT will later be used to encode the random walk sequence and to predict the 2-hop neighbors. All the steps taken by the device can run in parallel to save construction time. \n\n\n\n%The dissimilarity function $\\theta:V^2\\rightarrow \\mathbb{R}$ is defined by \n%\\begin{equation}\n%    \\label{dissim_func}\n %   \\theta (u,v)=||\\bm{c_u}-\\bm{c_v}||_1\n%\\end{equation}\n%where $c_u$ and $c_v$ are the degree vector formed before. The server computes the dissimilarity score $\\theta (u,v)$ for each vertex pair in $V$ and obtains a dissimilarity matrix $S=\\{ s_{ij} \\}\\in \\mathbb{R}^{|V|\\times |V|}$ where $s_{ij}=\\theta (v_i,v_j)$ for all $i,j\\in \\left[ 1,2,\\cdots , |V|\\right]$.\n%\\paragraph{Dasgputa's cost function} Dasgputa's cost function \\cite{dasgupta2016cost} measures how good the tree clusters the vertices using the least common ancestor nodes,  denoted as $i\\lor j$, for two leaves $i,j$. The function is designed based on the fact that if $i$ and $j$ are closer in the graph, the number of leaves in the subtree $T[i\\lor j]$ is smaller. Motivated by the idea, a modified Dasaputa's cost function is proposed utilizing dissimilarity function $S$ in Eq. \\ref{dcf}. \n%\\begin{equation}\n %   \\label{dcf}\n    %\\text{cost}(T)=\\sum %_{i,j\\in \\text{leaves}(T)}s_{ij}|\\text{leaves}(T[i\\lor j])|\n%\\end{equation}\n%where leaves$(T)$ returns a set of all the leaves in $T$. For a graph, a well-clustered HCT should separate two distanced vertices quite apart and generate large Dasgputa's cost value. \n%\\paragraph{Iterative update}We can formulate the HCT problem as an optimization function with the decision variable $T$\n%\\begin{equation}\n%\\label{hct_opt}\n%\\begin{aligned}\n%&\\max_T\\quad\\text{cost}(T)=\\sum _{i,j\\in \\text{leaves}(T)}s_{ij}|\\text{leaves}(T[i\\lor j])|\\\\\n %   &\\begin{array}{clr}\n  % s.t.    &  s_{ij}= ||\\bm{c_{v_i}}-\\bm{c_{v_j}}||_1&\\forall v_i,v_j\\in V\\\\\n   %      & \\bm{c_v}=(c'_1,c'_2,\\cdots ,c'_k)&\\forall v\\in V\\\\\n         %& %c'_i=c_i+Lap(0,\\frac{1}{\\epsilon})&\\forall i\\in [1,2,\\cdots ,k]\n    %\\end{array}\n%\\end{aligned}\n%\\end{equation}\n%Finding optimal solution to the maximization problem formulated as Eq.\\ref{hct_opt} is NP-hard\\cite{dasgupta2016cost}. But we can deploy Markov chain Monte Carlo (MCMC) method to approximate the optimal solution. In this problem, every HCT is considered as a state. For each internal node in HCT, \n%\\begin{figure}[htbp]\n%\\centering\n%\\subfigure[pic1.]{\n%\\begin{minipage}[t]{0.27\\linewidth}\n%\\centering\n%\\includegraphics[width=1in]{tree_t2.pdf}\n%\\end{minipage}\n%}\n%\\subfigure[pic2.]{\n%\\begin{minipage}[t]{0.27\\lin%ewidth}\n%\\centering\n%\\includegraphics[width=1in]{%tree_t2.pdf}\n%\\end{minipage}\n%}\n%\\subfigure[pic3.]{\n%\\begin{minipage}[t]{0.27\\linewidth}\n%\\centering\n%\\includegraphics[width=1in]{tree_t2.pdf}\n%\\end{minipage}\n%}\n%\\centering\n%\\caption{ pics}\n%\\end{figure}\n"
                },
                "subsection 4.3": {
                    "name": "Random Walk Generator",
                    "content": "\nBased on the constructed HCT, we design a random walk generator to generate random walk of limited length $l$ starting with a specific vertex. The server sends each vertex a tuple $(l,\\epsilon, p)$ to define and start a random walk. Value $\\epsilon$ is the noise parameter to encode vertex with quantifiable privacy level. Value $p$ is the probability of whether to use a 2-hop neighbor predictor, acting like a knob on controlling communication cost. As long as receiving the tuple, the corresponding device randomly selects a vertex from its neighbors. It utilizes the random walk sequence encoder to encode its vertex index and appends the encoder result to the tail of the walk sequence. Then it decides whether to trigger the 2-hop neighbor predictor based on the probability $p$. The length $l$ is assumed to be greater than 2 to capture enough graph structures.\n%If $l<=2$, it is meaningless to predict 2-hop neighbour. \nIf the predictor is triggered, it encodes the selected 2-hop neighbor vertex index and appends the encoded index to the output sequence as well. \nIt sends the tuple $(l-2, \\epsilon ,p)$ and the current walk sequence to the predicted 2-hop neighbor vertex to continue the walk. If the predictor is not triggered, it directly sends the tuple $(l-1, \\epsilon ,p)$ and the current walk sequence ended to the selected neighbor vertex. The random walk stops when the first number received in the tuple is 1. The corresponding device, also representing the ending vertex of the whole random walk, sends the encoded random walk sequence to the server. The complete algorithms are presented in Alg.\\ref{random-walk-gen}. The detailed procedures for random walk sequence encoder and two-hop neighbor predictor are presented as follows.\n\n\\paragraph{Random walk sequence encoder}\n\nThe random walk sequence encoder takes advantage of exponential mechanism to noise the elements in the sequence to protect the privacy of edge information. \n\nExponential mechanism deals with contradiction between information loss and privacy protection through a score function $u:D\\times \\text{Range}(\\mathcal{A})\\rightarrow \\mathbb{R}$, which measures how informative the output of $\\mathcal{A}$ is given $d\\in D$.  The mechanism assigns a sampling probability defined in Eq. \\ref{em} to the element in the range of $\\mathcal{A}$ given different input $d\\in D$ and samples an output based on the probability.  \n\\begin{equation}\n    \\label{em}\n    \\text{Pr}[r|d] = \\frac{\\exp (\\epsilon u(d,r))\\mu (r)}{\\int \\exp (\\epsilon u(d,r))\\mu (r)dr}\n\\end{equation}\nwhere $\\mu (r)$ is the distribution of $r$ in Range of $\\mathcal{A}$. \n\\begin{algorithm}[h]\n\\caption{Random Walk Generator}\n\\label{random-walk-gen}\n\\begin{algorithmic}[1]\n\\REQUIRE a HCT $T$; dissimilarity matrix; the length of the walk $l$; vertices $V$; noise parameter $\\epsilon$; triggering probability $p$; number of walks per vertex $\\gamma$;\\\\\n\\textbf{Server operation}\\\\\n\\STATE Send $T$ and dissimilarity matrix to all devices; \\\\\n\\FOR{$i=1\\rightarrow \\gamma $}\n\\FOR{each device}\n\\STATE Server sends a tuple $(l,\\epsilon ,p)$ to the local device;\\\\\n\\STATE Execute \\textbf{Device operation};\\\\\n\\STATE Server receives the random walk sequence;\n\\STATE Server inputs the random walk sequence to the SkipGram Model;\\\\\n\\ENDFOR\n\\ENDFOR\n\n\\quad \\\\\n\\textbf{Device operation}\\\\\n\\IF{the random walk sequence is empty}\n\\STATE{Initiate a new random walk sequence with an empty array}\n\\ENDIF\n\\STATE Randomly select a vertex $u$ from its neighbors;\\\\\n\\STATE Encode the vertex index according to Eq.\\ref{sp};\\\\\n\\STATE Append the encoded index to the end of the random walk sequence;\\\\\n\\IF{$l=1$}\n\\STATE Send the random walk sequence to the server; \\\\\n\\ELSIF{$l>2$}\n\\STATE Randomize a real number from $[0,1)$ ;\\\\\n\\IF{the randomized number$<p$}\n\\STATE Choose the row $\\bm{c_u}$ which corresponds to the degree vector of $u$ from the degree vector dictionary;\\\\\n\\STATE Initiate a vertex pool with an empty set;\\\\\n\\FOR{bin $i$ in bins}\n\\STATE Add $\\bm{c_u}[i]$ vertices which are nearest to $u$ in tree $T$ to the vertex pool;\\\\\n\\ENDFOR\n\\STATE Randomly pick a vertex $u'$ from the pool;\\\\\n\\STATE Encode the vertex index $u$ according to Eq.\\ref{sp};\\\\\n\\STATE Append the encoded index of $u$ to the end of the random walk sequence;\\\\\n\\STATE Device sends a tuple $(l-2,\\epsilon,p)$ and the current random walk sequence to device $u'$;\\\\\n\\STATE Device $u'$ execute \\textbf{Device operation};\\\\\n\\RETURN\n\\ENDIF\n\\ENDIF\n\\STATE Device sends a tuple $(l-1,\\epsilon,p)$ and the current random walk sequence to device $u$;\\\\\n\\STATE Device $u$ execute \\textbf{Device operation};\\\\\n\\end{algorithmic}\n\\end{algorithm}\n\n\nThe advantage of exponential mechanism is the ensuring of the utility of encoding result due to its dependence upon the score function. Another advantage is that it provides a way to encode discrete datasets. In our scenario, the random walk sequence should still imply the structural information of the graph despite alternation of the original sequence. In addition, the dataset $V$ to encrypt is a discrete vertex set. Hence, we choose exponential mechanism to preserve $\\epsilon-$ differential privacy. \n\nWe define the score function to be used in Eq. \\ref{em}  as following: \n\\begin{equation}\n    \\label{sf}\n    u(v_1,v_2)= -\\text{dissim}(v_1,v_2)|\\text{leaves}(T[v_1\\lor v_2])|\n\\end{equation}\nwhere dissim function is defined in Eq.\\ref{dissim}, $T$ is the subtree rooted by the least common ancestor of $v_1$ and $v_2$ nodes in HCT constructed before and leaves function returns all the leaves of a subtree. For two vertices with similar structural information, they tend to have a smaller dissimilarity value and are put more closely in the HCT. If two leaves are close to each other in a tree, $T[v_1\\lor v_2]$ have very few leaves. Hence, Eq.\\ref{sf} measures how similar two vertices are in terms of structure, which implies how informative the random walk sequence is if the original vertex is substituted with the other. \n\nIt is obvious that any vertex $v\\in V$ follows a uniform distribution \n\\begin{equation}\n\\label{pr}\n    Pr[v]=\\frac{1}{|V|}.\n\\end{equation}\nHence, after we plug Eq.\\ref{sf} and Eq.\\ref{pr} into Eq.\\ref{em}, the sampling probability becomes\n\\begin{equation}\n    \\label{sp}\n    \\text{Pr}[v_2|v_1] = \\frac{\\exp (-\\epsilon \\text{dissim}(v_1,v_2)|\\text{leaves}(T[v_1\\lor v_2]))}{\\sum_{v_2\\in V} \\exp (-\\epsilon \\text{dissim}(v_1,v_2)|\\text{leaves}(T[v_1\\lor v_2]))}\n\\end{equation}\nUsing Eq.\\ref{sp}, we encode the random walk sequence by sampling one vertex given the original index. \n\\paragraph{Two-hop neighbor predictor}\nTo decrease inter-device communication cost, we design a two-hop neighbor predictor to directly build a bridge from the $i$th element in the sequence to the $(i+2)$th element, saving the communication with the $(i+1)$th element. Our predictor takes advantage of HCT and the ordered degree matrix $\\bm{M_v}$ obtained in the HCT constructor. The predictor is triggered given probability $p$ customized by the server. \n\n\nEach device is aware of the vectors $\\bm{c_{u_1}}, \\bm{c_{u_2}},\\cdots ,\\bm{c_{|N(v)|}}$ with respect to its 1-hop neighbors. The device has randomly selected one neighbor denoted as $u_i$ before. In HCT constructor, the degree vector $\\bm{c_{u_i}}=(c_1',c_2',\\cdots ,c_k')$ of $u_i$ provides the number of its 2-hop neighbor connected to $u_i$ in $k$ different bins. Since HCT hierarchically clusters vertices according to their linkage information, for bin $j$, the predictor selects $c_j'$ vertices that are closest to the $u_i$ in HCT. These selected vertices form a pool. Then a vertex is uniformly drawn from this pool as the predicted 2-hop neighbor of $v$. In this way, the device can directly communicate with the device with the predicted 2-hop neighbor index. This reduces the times of communication by saving the contact involving the 1-hop neighbor $u_i$. We formally analyze the saved communication cost caused by our predictor and present the theorem as follows. \n\\begin{theorem}\nFor a $l-$long random walk sequence, it is expected that 2-hop neighbor predictor will reduce $|V|\\gamma \\mathcal{A}$ device to device communication given the triggering probability $p$ and the number of walks per vertex $\\gamma $ where \n\\begin{equation}\n    \\mathcal{A}=l-1-\\left[ \\frac{l-2}{1+p}+(2-2p-\\frac{1}{1+p})\\frac{1-(-p)^{l-2}}{1+p}\\right]\n\\end{equation}\n\\label{probcomm}\n\\end{theorem}\n% \\begin{proof}\n% See Appendix A. \n% \\end{proof}\nAs long as a random walk sequence is generated, the server computes the probability of co-occurrence for any vertex pair in this sequence, which provides a measurement of the similarity of two vertices in a graph. A Skipgram model is used to update node embedding based on the computed probability such that the distance between node embedding vectors approximates the corresponding vertex similarity. \n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\nIn this section, we present the empirical results and analysis of our proposed algorithm. We first examine how FedWalk performs compared with the existing baseline in traditional node embedding tasks. Then we discuss the influence of key hyper-parameters upon the model performance.  \n\\label{sec:exp}\n% \\subsection{Dataset}\n% We evaluate the performance of FedWalk based on three commonly used real world social network datasets:  Blogcatalog\\cite{tang2009relational}, Flickr\\cite{tang2009relational} and Youtube\\cite{tang2009scalable}. Table \\ref{dataset} presents the basic information of three graphs. Specifically, \n% \\begin{table}[htbp]\n% \t\\centering  \n% \t\\begin{tabular}{|c|c|c|c|}  \n% \t\t\\hline  \n% \t\t&Blogcatalog & Flickr & Youtube \\\\\n% \t\t\\hline\n% \t\t$|V|$&10312&80513&1138499 \\\\ \n% \t\t\\hline\n% \t$|E|$\t&333983&5899882&2990443 \\\\\n% \t\\hline\n% \t$|L|$\t&39&195&47 \\\\\n% \t\t\\hline\n% \t\\end{tabular}\n% \t\\caption{Overview of three graphs\t\t\\label{dataset} }\n% \\end{table}\n% \\begin{itemize}\n%     \\item Blogcatalog dataset is crawled from a blog posting website Blogcatalog\\footnote{ http://www.blogcatalog.com} representing the online social relationship between bloggers . The labels of the vertices are the interest of the bloggers. \n%     \\item Flickr dataset is crawled from an image sharing website Flickr\\footnote{http://www.Flickr.com }. The vertices are the users and the edges represent that users add others to their contact lists. The labels are the interest groups of the users.\n%     \\item Youtube dataset is crawled from a video sharing website Youtube\\footnote{http://www.youtube.com/} representing the online social relationship between video viewers. The labels are groups of viewers that enjoy common video genres. \n% \\end{itemize}\n",
                "subsection 5.1": {
                    "name": "Experiment Setup",
                    "content": "\nThe performance of FedWalk is measured through multi-label classification tasks. First, we deploy FedWalk to embed the vertices. Then, we randomly split the two datasets into a training dataset and a testing dataset based on a proportion $T_R$. $T_R$ is the proportion of training data in full data. We set the vertex embedding vectors of the training data as features $\\bm{x}$ and the labels of the training data as targets $\\bm{y}$ and feed the $(\\bm{x}, \\bm{y})$ pairs into a one-vs-rest logistic regression. $\\bm{y}$ is the indicator vector of the multiple labels for one vertex. Last we test the regression model using testing data. \n\nWe evaluate the performance of FedWalk based on two social network datasets:  Blogcatalog\\cite{tang2009relational} and Flickr\\cite{tang2009relational}. Specifically, \n% \\begin{table}[htbp]\n% \t\\centering  \n% \t\\begin{tabular}{|c|c|c|}  \n% \t\t\\hline  \n% \t\t&Blogcatalog & Flickr  \\\\\n% \t\t\\hline\n% \t\t$|V|$&10312&80513\\\\ \n% \t\t\\hline\n% \t$|E|$\t&333983&5899882 \\\\\n% \t\\hline\n% \t$|L|$\t&39&195 \\\\\n% \t\t\\hline\n% \t\\end{tabular}\n% \t\\caption{Overview of two graphs\t\t\\label{dataset} }\n% \\end{table}\n\\begin{itemize}\n    \\item Blogcatalog dataset is crawled from a blog posting website Blogcatalog\\footnote{ http://www.blogcatalog.com} representing the online social relationship between bloggers. The vertex labels are the interest of the bloggers. \n    \\item Flickr dataset is crawled from an image sharing website Flickr\\footnote{http://www.Flickr.com }. The vertices are the users and the edges represent that users add others to their contact lists. The labels are the interest groups of the users.\n  %  \\item Youtube dataset is crawled from a video sharing website Youtube\\footnote{http://www.youtube.com/} representing the online social relationship between video viewers. The labels are groups of viewers that enjoy common video genres. \n\\end{itemize}\nBlogcatalog dataset has 10312 vertices, 333983 edges, and 39 labels; Flickr dataset has  80513 vertices, 5899882 edges, and 195 labels. \nTwo graphs are of different sizes so that the scalability of our algorithms can be examined.\n%Table \\ref{dataset} presents the basic information of two graphs.\n\nSince we are the first to study the node-level federated unsupervised embedding problems, and the existing federated works cannot be applied in our scenario, we compare our FedWalk with DeepWalk\\cite{perozzi2014deepwalk}, a classical node embedding method in centralized settings. It regards the random walk sequences as the word sequences and deploys Skipgram model to embed the nodes with vectors. In our experiments, both methods embed vertices with walk length $l=40$, window size $w=10$, embedding dimension $d=128$ and $\\gamma =80$ walks per vertex. For FedWalk, we set the number of bins $k=\\lfloor \\ln |V| \\rfloor$, the noise parameter $\\epsilon=2$ and the probability of triggering 2-hop neighbor predictor $p=0.2$. The experiments are done with an Intel Core i7-10700 CPU clocked at 2.90GHz, an 8GB-memory NVIDIA RTX 3070, and 16GB memory. \n\nTo quantify the performance of multi-label classification, we use macro-F1 and micro-F1 to measure its accuracy. \n\\begin{itemize}\n\\item Macro-F1 is the average F1 score of all labels. \n\\begin{equation}\n    Macro-F1 = \\frac{\\sum _{l\\in L}F1(l)}{|L|}\n\\end{equation}\n\\item Micro-F1 is calculated by counting the overall true positives, false negatives and false negatives for all labels. \n\\begin{equation}\n    Micro-F1 = \\frac{2PR}{P+R}\n\\end{equation}\nwhere $P=\\frac{\\sum _{l\\in L}TP(l)}{\\sum _{l\\in L}TP(l)+FP(l)}$ and $R=\\frac{\\sum _{l\\in L}TP(l)}{\\sum _{l\\in L}TP(l)+FN(l)}$. \n\\end{itemize}\n"
                },
                "subsection 5.2": {
                    "name": "Results",
                    "content": "\n\n\\noindent\n\\textbf{Blogcatalog.}\nFor this dataset, we evaluate the performance of FedWalk and DeepWalk by setting the training dataset ratio $T_R$ from 0.1 to 0.6. The results are illustrated in Fig. \\ref{fig:bc}. FedWalk performs close to DeepWalk does both in Micro-F1 score and in Macro-F1 score. As $T_R$ increases, the gap between these two methods closes. When $T_R=0.6$, FedWalk only loses 1.8\\% Micro-F1 score and 1.0\\%  Macro-F1 score compared to the scores of DeepWalk. \n\n\n\\noindent\n\\textbf{Flickr.}\n%\\paragraph{Flickr}\nFor Flickr dataset, we set the training dataset ratio $T_R$ from 0.01 to 0.1 due to its large scale. The results are shown in Fig. \\ref{fig:fr}. The results are consistent with those obtained from Blogcatalog dataset. Generally, DeepWalk has comparable performance to the one of FedWalk. When $T_R$ gets larger, the difference between the results of our method and the baseline method becomes smaller. When $T_R=0.1$, DeepWalk only loses 1.5\\% Micro-F1 score and 4.4\\% Macro-F1 score. \n\n\n%\\paragraph{Youtube} For Youtube dataset, we set the training dataset ratio $T_R$ from 0.01 to 0.1 due to its large scale. The results are displayed in Fig. \\ref{fig:yt}. The plot implies that the outcome of DeepWalk method approaches the baseline method. With increasing training dataset ratio $T_R$, Micro F1 score and Macro F1 score of FedWalk method differ less from the two scores of DeepWalk method. When $T_R=0.1$, FedWalk loses 1.8\\% Micro F1 score and 0.9\\% Macro F1 score. \n\n\n\n\nThe classification results of these two methods show that our method FedWalk can achieve performance very close to the centralized method DeepWalk. This implies that our method has limited utility loss in the federated regime.  \n"
                },
                "subsection 5.3": {
                    "name": "Sensitivity Analysis",
                    "content": "\n\\textbf{Number of bins $k$.}\n%\\paragraph{Number of bins $k$}\nFigure \\ref{fig:k} shows the effect of the number of bins $k$. Since Flickr Dataset has a lot more vertices than Blogcatalog Dataset, we test the performance upon different $k$ values.  For Blogcatalog dataset, increasing the number of bins from 3 to 9 results in 18.06\\%, 15.09\\%, 13.73\\% and 10.84\\% raise of Micro-F1 score respectively when $T_R=0.1,0.2,0.5$ and 0.6. For Flickr dataset, increasing the number of bins from 5 to 11 results in 20.4\\%, 16.90\\%, 11.11\\% and 10.43\\% raise of Micro-F1 score respectively when $T_R=0.01,0.02,0.09$ and 0.01. Both two bar plots show that larger $k$ leads to better performance of FedWalk model given different training ratios $T_R$. A larger number of bins generates a more representative degree vector, thus leading to a better HCT. A good HCT provides an outcome with the high utility of good random walk sequence encoder and more accurate prediction of the two-hop neighbor predictor. This produces more representative vertex embedding. \n\n\n\\noindent\n\\textbf{Laplace noise parameter $\\epsilon$.}\nFigure \\ref{fig:epsilon} shows the effect of Laplace noise parameter $\\epsilon$ upon the performance of FedWalk model. For Blogcatalog dataset, increasing $\\epsilon$ from 0.5 to 4 results in 3.90\\%, 3.33\\%, 2.00\\% and 1.73\\% raise of Micro-F1 score respectively when $T_R=0.1,0.2,0.5$ and 0.6. For Flickr dataset, the same increase of $\\epsilon$ results in 11.23\\%, 7.14\\%, 1.86\\% and 1.85\\% raise of Micro-F1 score respectively when $T_R=0.01,0.02,0.09$ and 0.01. Both two plots illustrate that the model with larger $\\epsilon$ performs better for different training ratios $T_R$. $\\epsilon$ represents the extent to which privacy is preserved. Small $\\epsilon$ protects more privacy while losing more utility. In this way, the overall performance degrades as $\\epsilon$ decreases. \n\\par \n\n\n\n\n%\\paragraph{Probability of triggering predictor $p$}\n\n\\noindent\n\\textbf{Probability of triggering predictor $p$.}\nWe test the effect of the probability of triggering two-hop neighbor predictor $p$ given different numbers of walks per vertex. Figure \\ref{fig:pcom} and Figure \\ref{fig:pcom2} show the influence upon the number of inter-device communication. Increasing $p$ from 0 to 0.4 results in approximately 23.80\\% decrease of the number of inter-device communication. For every 40-vertex-long random walk, it takes 39, 35.35, 32.30, 29.72 and 27.49 times of inter-device communication respectively when $p=0,0.1,0.2,0.3$ and 0.4.  The figures show that larger $p$ saves more inter-device communication as the predictor saves the communication cost by skipping the 1 hop neighbor and directly communicating to the predicted 2 hop neighbor. \n\nFigure \\ref{fig:p} displays the trade-off relationship between communication cost and accuracy of FedWalk model. \nFigure \\ref{fig:pacc} and Figure \\ref{fig:pacc2} demonstrate that smaller $p$ value leads to more representative vertex embedding. Increasing $p$ from 0 to 0.4 results in 10.47\\%, 8.35\\%, 7.13\\% and 6.86\\% decrease of Micro-F1 score respectively when the numbers of walks per vertex are 20, 40, 60, and 80. For Flickr dataset, the same increase of $p$ results in 13.08\\%, 10.64\\%, 8.08\\% and 3.88\\% decrease of Micro-F1 score respectively when the numbers of walks per vertex are 10, 20, 40 and 60. Especially when $p>0.2$, the decrease of micro-F1 score is obvious. \n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\label{sec:con}\nIn this paper, we propose the first unsupervised node embedding algorithm in the node-level federated graphs. Our framework, FedWalk, provides centralized competitive representation capability with reduced communication cost and enhanced data privacy. In FedWalk, we first propose a differentially private HCT constructor to capture structural information. We then propose a random walk generator which includes a sequence encoder to preserve privacy and a two-hop neighbor predictor to save communication cost. \nFedWalk is theoretically proved to have bounded dissimilarity value loss and quantifiable communication reduction while preserving $\\epsilon-$ differential privacy. Extensive experiments on real-world datasets validate the effectiveness and efficiency of our design.\n%The experiment results have validated that FedWalk generates node embedding close to the results of centralized node embedding algorithms upon two datasets and meanwhile keeps data privacy and reduces communication costs. Both theoretical and empirical analysis show the great potential of FedWalk in federated graph presentation problems. \n\n\\begin{acks}\nThis research is supported by SJTU Explore-X grant.\n\\end{acks}\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main.bbl}\n\n\\newpage\n\\appendix \n"
            },
            "section 7": {
                "name": "Proofs",
                "content": "\nIn this section, we provide proofs for the theorems mentioned in this paper. \n\n",
                "subsection 7.1": {
                    "name": "th:ub",
                    "content": "\n\\begin{proof}\nLet the noised ordered degree matrix of vertex $u$ be $\\bm{M_u}$ and the original ordered degree matrix of vertex $u$ be $\\bm{M_u}'$ such that $M_{u,ij}=M_{u,ij}'+R_{u,ij}$. Recall that the dissimilarity value is calculated using Eq.\\ref{dp} and Eq.\\ref{dissim}. In this way, \n\\begin{equation}\n\\begin{aligned}\n     &\\mathbb{E}[\\text{dissim}(u,v)-\\text{dissim}'(u,v)]\\\\\n     =&\\mathbb{E}\\left[ \\sum ||\\bm{c_{u_i}}-\\bm{c_{v_j}}||_2-\\sum ||\\bm{c_{u_{i'}}'}-\\bm{c_{v_{j'}}'}||_2\\right]\n\\end{aligned}\n\\end{equation}\nwhere $(i,j)$ and $(i',j')$ are some pairs selected from the $[1,|N(u)|]\\times [1,|N(v)|]$ integer space according to Eq.\\ref{dp}. Thus, \n\\begin{equation}\n\\begin{aligned}\n     &\\mathbb{E}[\\text{dissim}(u,v)-\\text{dissim}'(u,v)]\\\\\n     =&\\mathbb{E}\\left[\\sum ||\\bm{c_{u_i}}-\\bm{c_{v_j}}||_1-\\sum ||\\bm{c_{u_{i'}}'}-\\bm{c_{v_{j'}}'}||_1\\right]\\\\\n     \\leq &\\mathbb{E}\\left[\\sum_{i=1}^{|N(u)|}\\sum_{j=1}^{|N(v)|} ||\\bm{c_{u_i}}-\\bm{c_{v_j}}||_1- \\sum_{i'=1}^{|N(u)|}\\sum_{j'=1}^{|N(v)|} ||\\bm{c_{u_{i'}}'}-\\bm{c_{v_{j'}}'}||_1\\right]\\\\\n     =&\\mathbb{E}\\left[\\sum_{i=1}^{|N(u)|}\\sum_{j=1}^{|N(v)|} ||\\bm{c_{u_i}}-\\bm{c_{v_j}}||_1-||\\bm{c_{u_{i}}'}-\\bm{c_{v_{j}}'}||_1\\right]\\\\\n     = &\\mathbb{E}\\left[\\sum_{i=1}^{|N(u)|}\\sum_{j=1}^{|N(v)|} \\sum_{l=1}^k |M_{u,il}-M_{v,jl}|-|M_{u,il}'-M_{v,jl}'|\\right]\\\\\n     \\leq &\\mathbb{E}\\left[\\sum_{i=1}^{|N(u)|}\\sum_{j=1}^{|N(v)|} \\sum_{l=1}^k |R_{u,il}-R_{v,jl}|\\right]\\\\\n\\end{aligned}\n\\end{equation}\nSince the noised item $R$ follows an i.i.d. Laplace distribution, we have \n$f_{R_{u,il}-R_{v,jl}}(x)=\\frac{\\epsilon}{4}\\left( e^{-\\epsilon|x|}+|x|\\epsilon e^{-\\epsilon|x|}\\right)$\nwhich implies that $\\mathbb{E}_{|x|}\\left[ f_{R_{u,il}-R_{v,jl}}(x)\\right ]=\\frac{3}{2\\epsilon}$. Hence, we bound the expectation\n\\begin{equation}\n\\begin{aligned}\n     &\\mathbb{E}[\\text{dissim}(u,v)-\\text{dissim}'(u,v)]\\\\\n     \\leq &k|N(u)||N(v)|\\frac{3}{2\\epsilon}\\\\\n     \\leq &\\frac{3k(\\max_{v\\in V}|N(v)|)^2}{2\\epsilon}\n\\end{aligned}\n\\end{equation}\n\\end{proof}\n\n"
                },
                "subsection 7.2": {
                    "name": "probcomm",
                    "content": "\n\\begin{proof}\nLet the expected times of inter-device communication be $\\mathbb{E}_l$ for one $l-$vertex-long random walk. \n\nWhen $l=1$, $\\mathbb{E}_1=0$. When $l=2,$ $\\mathbb{E}_2=1$. \nWhen $l\\ge 3$,  the following relationship holds\n\\begin{equation}\n\\mathbb{E}_{l}=p(\\mathbb{E}_{l-2}+1)+(1-p)(\\mathbb{E}_{l-1}+1)\n\\end{equation}\nwhich can be rewritten in the form of \n\\begin{equation}\n\\mathbb{E}_{l}-\\mathbb{E}_{l-1}=1-p(\\mathbb{E}_{l-1}-\\mathbb{E}_{l-2}).\n\\end{equation}\nThis gives us a recurrence relation of $\\mathbb{E}_l-\\mathbb{E}_{l-1}$, which implies that \n\\begin{equation}\n    \\mathbb{E}_l = \\frac{l-2}{1+p}+(2-2p-\\frac{1}{1+p})\\frac{1-(-p)^{l-2}}{1+p}.\n\\end{equation}\nWhen $p=0$, it is expected to have $l-1$ times of communication per random walk sequence. Thus, it is expected to save \n\\begin{equation}\n    l-1-\\left[ \\frac{l-2}{1+p}+(2-2p-\\frac{1}{1+p})\\frac{1-(-p)^{l-2}}{1+p}\\right]\n\\end{equation}\ntimes of communication per random walk sequence. \nIn total, it is expected to save \n\\begin{equation}\n    |V|\\gamma \\mathcal{A}\n\\end{equation}\nwhere \n\\begin{equation}\n    \\mathcal{A}=l-1-\\left[ \\frac{l-2}{1+p}+(2-2p-\\frac{1}{1+p})\\frac{1-(-p)^{l-2}}{1+p}\\right]\n\\end{equation}\ntimes of inter-device communication.\n\\end{proof}\n"
                }
            },
            "section 8": {
                "name": "Pseduo Code",
                "content": "\n",
                "subsection 8.1": {
                    "name": "Pseduo code of HCT constructor",
                    "content": "\n\\begin{algorithm}[h]\n\\caption{HCT constructor}\n\\label{hct-constr}\n\\begin{algorithmic}[1]\n\\REQUIRE noise parameter $\\epsilon$, number of bins $k$, vertices $V$;\n\\ENSURE a hierarchical clustering tree;\\\\\n\\textbf{Server operation}\\\\\n\\STATE Group $V$ into $k$ bins randomly;\\\\\n\\FOR{each device}\n\\STATE Server sends the group plan to the local device;\\\\\n\\STATE Execute \\textbf{Device operation 1};\\\\\n\\STATE Server receives the noised vector $\\bm{c_v}$;\n\\ENDFOR\n\\STATE Collect all the vectors to a dictionary; \\\\\n\\FOR{each device}\n\\STATE Server sends the dictionary to the local device;\\\\\n\\STATE Execute \\textbf{Device operation 2};\\\\\n\\STATE Server receives the degree matrix;\n\\ENDFOR\n\\STATE Compute a dissimilarity matrix using DTW; \\\\\n\\STATE Construct an HCT based on the dissimilarity matrix using a hierarchical clustering algorithm; \\\\\n\\quad \\\\\n\\textbf{Device operation 1}\\\\\n\\STATE Count the number of its neighbors belonging to each bins and get a $k$-dim vector;\\\\\n\\STATE Add noise $Lap(0,\\frac{1}{\\epsilon})$ to each element of $\\bm{c_v}$;\\\\\n\\STATE Send the noised vector $\\bm{c_v}$ to the server; \\\\\n\\quad \\\\\n\\textbf{Device operation 2}\\\\\n\\STATE Form its ordered degree matrix $\\bm{M_v}$ according to Eq. \\ref{degree_matrix};\\\\\n\\STATE Send the degree matrix to the server; \\\\\n\\end{algorithmic}\n\n\\end{algorithm}\n\n\n"
                }
            }
        },
        "figures": {
            "fig:fedwalk": "\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{fedwalk-crop.pdf}\n    \\caption{FedWalk framework}\n    \\label{fig:fedwalk}\n\\end{figure}",
            "fig:hct": "\\begin{figure}[h]\n    \\centering\n    \\subfigure[Original Graph]\n    {\n        \\includegraphics[width=0.22\\textwidth]{small_graph_crop.pdf}\n      \n    }\n    \\subfigure[Corresponding HCT]\n    {\n        \\includegraphics[width=0.22\\textwidth]{hct_crop.pdf}\n  \n    }\n    \\caption{An example of HCT}\n    \\label{fig:hct}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:hctcon": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{hct_4_crop.pdf}\n    \\caption{HCT constructor framework}\n    \\label{fig:hctcon}\n\\end{figure}",
            "fig:bc": "\\begin{figure}[h]\n    \\centering\n    \\subfigure[Micro-F1 score]\n    {\n        \\includegraphics[width=0.22\\textwidth]{micro-f1-bc.pdf}\n      \n    }\n    \\subfigure[Macro-F1 score]\n    {\n        \\includegraphics[width=0.22\\textwidth]{macro-f1-bc.pdf}\n  \n    }\n    \\caption{Blogcatalog dataset}\n    \\label{fig:bc}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:fr": "\\begin{figure}[h]\n    \\centering\n    \\subfigure[Micro-F1 score]\n    {\n        \\includegraphics[width=0.22\\textwidth]{micro-f1-fr.pdf}\n      \n    }\n    \\subfigure[Macro-F1 score]\n    {\n        \\includegraphics[width=0.22\\textwidth]{macro-f1-fr.pdf}\n  \n    }\n    \\caption{Flickr dataset}\n    \\label{fig:fr}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:k": "\\begin{figure}[h]\n    \\centering\n    \\subfigure[Blogcatalog dataset]\n    {\n        \\includegraphics[width=0.22\\textwidth]{k-bc.pdf}\n      \n    }\n    \\subfigure[Flickr dataset]\n    {\n        \\includegraphics[width=0.22\\textwidth]{k-fr.pdf}\n  \n    }\n    \\caption{Sensitivity analysis of parameter $k$}\n    \\label{fig:k}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:epsilon": "\\begin{figure}[h]\n    \\centering\n    \\subfigure[Blogcatalog dataset]\n    {\n        \\includegraphics[width=0.22\\textwidth]{epsilon-bc.pdf}\n      \n    }\n    \\subfigure[Flickr dataset]\n    {\n        \\includegraphics[width=0.22\\textwidth]{epsilon-fr.pdf}\n  \n    }\n    \\caption{Sensitivity analysis of parameter $\\epsilon$}\n    \\label{fig:epsilon}\n    % \\vspace{-0.3cm}\n\\end{figure}",
            "fig:p": "\\begin{figure}[h]\n    \\centering\n    \\subfigure[Communication cost(Blogcatalog)]\n    {\n        \\includegraphics[width=0.22\\textwidth]{p-comm.pdf}\n        \\label{fig:pcom}\n      \n    }\n    \\subfigure[Micro-F1 score(Blogcatalog)]\n    {\n        \\includegraphics[width=0.22\\textwidth]{p-microf1.pdf}\n        \\label{fig:pacc}\n    }\n    \\subfigure[Communication cost(Flickr)]\n    {\n        \\includegraphics[width=0.22\\textwidth]{p-comm2.pdf}\n        \\label{fig:pcom2}\n      \n    }\n    \\subfigure[Micro-F1 score(Flickr)]\n    {\n        \\includegraphics[width=0.22\\textwidth]{p-microf12.pdf}\n        \\label{fig:pacc2}\n    }\n    \\caption{Sensitivity analysis of parameter $p$}\n    \\label{fig:p}\n    % \\vspace{-0.3cm}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    ENC:V\\rightarrow \\mathbb{R}^d\n    \\label{eq:enc}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{eq:sim}\n    DISSIM:V\\times V\\rightarrow \\mathbb{R}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\label{eq:goal}\n    \\min_{ENC} \\sum _{v_1,v_2\\in V}\\text{dist}(ENC(v_1),ENC(v_2))- DISSIM(v_1,v_2).\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{degree_matrix}\n    \\bm{M_v}=\\left[ \n    \\begin{aligned}\n    &\\quad\\bm{c_{u_1}}\\\\\n    &\\quad\\bm{c_{u_2}}\\\\\n    &\\quad\\quad\\vdots \\\\\n    &\\bm{c_{u_{|N(v)|}}}\\\\\n    \\end{aligned}\\right]\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\label{dp}\n    \\text{cost}(i,j)=\n    \\begin{aligned}\n    \\\\\n    &\\min \\{\\text{cost}(i-1 ,j),\n\\text{cost}(i,j-1), \\text{cost}(i-1,j-1)\\}\\\\\n&+||\\bm{c_{u_i}}-\\bm{c_{v_j}}||_1, \\forall i\\in [1,x], j \\in [1,y]\n\\end{aligned}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\text{dissim}(u,v)=\\text{cost}(x,y)\n    \\label{dissim}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\label{em}\n    \\text{Pr}[r|d] = \\frac{\\exp (\\epsilon u(d,r))\\mu (r)}{\\int \\exp (\\epsilon u(d,r))\\mu (r)dr}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\label{sf}\n    u(v_1,v_2)= -\\text{dissim}(v_1,v_2)|\\text{leaves}(T[v_1\\lor v_2])|\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\label{pr}\n    Pr[v]=\\frac{1}{|V|}.\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\label{sp}\n    \\text{Pr}[v_2|v_1] = \\frac{\\exp (-\\epsilon \\text{dissim}(v_1,v_2)|\\text{leaves}(T[v_1\\lor v_2]))}{\\sum_{v_2\\in V} \\exp (-\\epsilon \\text{dissim}(v_1,v_2)|\\text{leaves}(T[v_1\\lor v_2]))}\n\\end{equation}"
        }
    }
}