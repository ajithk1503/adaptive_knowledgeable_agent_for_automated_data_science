{
    "meta_info": {
        "title": "Efficient and Joint Hyperparameter and Architecture Search for  Collaborative Filtering",
        "abstract": "Automated Machine Learning (AutoML) techniques have recently been introduced\nto design Collaborative Filtering (CF) models in a data-specific manner.\nHowever, existing works either search architectures or hyperparameters while\nignoring the fact they are intrinsically related and should be considered\ntogether. This motivates us to consider a joint hyperparameter and architecture\nsearch method to design CF models. However, this is not easy because of the\nlarge search space and high evaluation cost. To solve these challenges, we\nreduce the space by screening out usefulness yperparameter choices through a\ncomprehensive understanding of individual hyperparameters. Next, we propose a\ntwo-stage search algorithm to find proper configurations from the reduced\nspace. In the first stage, we leverage knowledge from subsampled datasets to\nreduce evaluation costs; in the second stage, we efficiently fine-tune top\ncandidate models on the whole dataset. Extensive experiments on real-world\ndatasets show better performance can be achieved compared with both\nhand-designed and previous searched models. Besides, ablation and case studies\ndemonstrate the effectiveness of our search framework.",
        "author": "Yan Wen, Chen Gao, Lingling Yi, Liwei Qiu, Yaqing Wang, Yong Li",
        "link": "http://arxiv.org/abs/2307.11004v1",
        "category": [
            "cs.IR",
            "cs.LG"
        ],
        "additionl_info": "Accepted by KDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec::intro}\n\nCollaborative Filtering (CF) is the most widely used approach for Recommender Systems~\\cite{sarwar2001item,koren2008factorization,he2017neural,kabbur2013fism}, aiming at calculating the similarity of users and items to recommend new items to potential users. \nThey mainly use Neural Networks to build models for users and items, simulating the interaction procedure and predict the preferences of users for items.\nRecent works also built CF models based on Graph Neural Networks (GNNs)~\\cite{gao2023survey,ying2018graph, wang2019neural, he2020lightgcn}.\nWhile CF models may have different performance on different scenes~\\cite{dacrema2019we}, recent works~\\cite{zheng2022automlrecsys,chen2022automated} have begun to apply Automated Machine Learning (AutoML) to search data-specific CF models.\nPrevious works, including SIF~\\cite{yao2020searching}, AutoCF~\\cite{gao2021efficient} and~\\cite{wang2022profiling}, applied Neural Architecture Search (NAS) on CF tasks.\nThey split the architectures of CF models into several parts and searched each part on architecture space. \n\nHowever, most of these methods focus on NAS in architecture space, \nonly considering hyperparameters as fixed settings and therefore omitting the dependencies among them. \nA CF model can be decided by a given architecture and a configuration of hyperparameters. \nEspecially in the task of searching best CF models, hyperparameter choice can affect search efficiency and the evaluation performance an architecture can receive on a given dataset. \nRecent methods mainly focus on model search, ignoring the important role of hyperparameters.\nFor instance, SIF focuses on interaction function, while it uses grid search on hyperparameters space.\nAutoCF does not use hyperparameter tuning on each architecture, which may make the searched model sub-optimal since proper hyperparameters vary for different architectures.\n\\cite{wang2022profiling} includes GNN models in the architecture space, but the search and evaluation cost for architectures may be high.\n\n\nWe find that these works only focus on either hyperparameters or fixed parts in CF architecture, neglecting the relation between architecture and hyperparameters.\nIf architecture cannot be evaluated with proper hyperparameters, \na sub-optimal model may be searched, possibly causing a reduction in performance. \nTo summarize, there exists a strong dependency between hyperparameters and architectures.\nThat is, the hyperparameters of a model are based on the design of architectures, \nand the choices of hyperparameters also affect the best performance an architecture may approach. \nWe suppose that hyperparameters can be adaptively changed when the architecture change in CF tasks, \nso we consider that the CF search problem can be defined on a joint space of hyperparameters and architectures.\nTherefore, the CF problem can be modeled as a joint search problem on architecture and hyperparameter space.\nWhile hyperparameters and architectures both influences the cost and efficiency of CF search task, \nthere exist challenges for joint search problems:\n(1) Since the joint search space is designed to include both hyperparameters and architectures, the joint search problem has a large search space, \nwhich may make it more difficult to find the proper configuration of hyperparameters and architectures;\n(2) In a joint search problem, since getting better performance on a given architecture requires determining its hyperparameters, the evaluation cost may be more expensive.\n\n\nWe propose a general framework, which can optimize CF architectures and their hyperparameters at the same time in search procedure. \nThe framework of our method is shown in Figure~\\ref{fig:search_two_stage}, consisting of two stages.\nPrior to searching hyperparameters and architectures,  we have a full understanding of the search space and reduce hyperparameter space to improve efficiency.\nSpecifically, we reduced the hyperparameter space by their performance ranking on CF tasks from different datasets.\nWe also propose a frequency-based sampling strategy on the user-item matrix for fast evaluation.\n%Considering the CF dataset as a user-item bipartite graph, we sample users and items with higher frequency of interaction.\nIn first stage, we search and evaluate models on reduced space and subsampled datasets, and we jointly search architecture and hyperparameters with a surrogate model.\nIn second stage, we propose a knowledge transfer-based evaluation strategy to leverage surrogate model to larger datasets.\nThen we evaluate the model with transferred knowledge and jointly search the hyperparameters and architectures to find the best choice in original dataset. \n\nOverall, we make the following important contributions:\n%\\begin{itemize}[leftmargin=*]\n\\begin{itemize}[noitemsep, topsep=2pt, leftmargin=8pt]\n\t\\item We propose an approach that can jointly search CF architectures and hyperparameters to \n\tget performance from different datasets.\n\t\n\t\\item We propose a two-stage search algorithm to efficiently optimize the problem. \n\tThe algorithm is based on a full understanding of search space and transfer ability between datasets.\n\tIt can jointly update CF architectures and hyperparameters and transfer knowledge from small datasets to large datasets.\n\t\n\t\\item Extensive experiments on real-world datasets demonstrate that our proposed approach can efficiently search configurations in designed space. \n\tFurthermore,  results of ablation and case study show the superiority of our method. \n\\end{itemize}\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec::related}\n\n",
                "subsection 2.1": {
                    "name": "Automated Machine Learning (AutoML)",
                    "content": "\n\\label{sec::relatedwork1}\nAutomated Machine Learning  (AutoML)~\\cite{hutter2019automatedck,quanming2018auto} refers to a type of method that can learn models adaptively to various tasks. \nRecently, AutoML has achieved great success in designing the state-of-the-art model for various applications such as image classification and segmentation~\\cite{zoph2017neural,tan2019efficientnet,liu2019auto}, natural language modeling~\\cite{so2019evolved},  and knowledge graph embedding~\\cite{zhang2019autokge}.\n\nAutoML can be used in mainly two fields: \\textit{Neural Architecture Search} (NAS) and \\textit{Hyperparameter Optimization} (HPO). \n\n\\begin{itemize}[leftmargin=*]\n\\item \nNAS~\\cite{zoph2017neural,pham2018efficient,li2019random} splits architectures into several components and searches for each part of the architecture to achieve the whole part.\nDARTS~\\cite{liu2018darts} uses gradient descent on continuous relaxation of the architecture representation, while NASP~\\cite{yao2019differentiable} improves DARTS by including proximal gradient descent on architectures. \n\n\\item \nHPO~\\cite{bergstra2012random,li2017hyperband,feurer2019auto}, usually tuning the hyperparameters of a given architecture, always plays an important role in finding the best hyperparameters for the task.\nRandom Search is a frequently used method in HPO for finding proper hyperparameters. \nAlgorithms for HP search based on model have been developed for acceleration~\\cite{claesen2015hyperparameter}, including Bayesian Optimization (BO) methods like Hyperopt~\\cite{bergstra2013hyperopt},BOHB~\\cite{hutter2011sequential} and BORE~\\cite{tiao2021bore}, etc.\n\\end{itemize}\n\n\nRecent studies on AutoML have shown that incorporating HPO into the NAS process can lead to better performance and a more effective exploration of the search space. \nFor example, a study on ResNet~\\cite{zela2018towards} showed that considering the NAS process as HPO can lead to improved results. \nOther works, such as AutoHAS~\\cite{dong2020autohas}, have explored the idea of considering hyperparameters as a choice in the architecture space. \nFEATHERS~\\cite{seng2023feathers} has focused on the joint search problem in Federated Learning. \nST-NAS~\\cite{cai2021stnas} uses weight-sharing NAS on architecture space and consider HP as part of architecture encoding.\nThese studies demonstrate the potential of joint search on hyperparameters and architectures in improving the performance and efficiency of machine learning models.\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Collaborative Filtering (CF)",
                    "content": "\n\\label{sec::relatedwork2}\n",
                    "subsubsection 2.2.1": {
                        "name": "Classical CF Models",
                        "content": "\n\nCollaborative Filtering (CF)~\\cite{sarwar2001item} is the most fundamental solution for Recommender Systems (RecSys). \nCF models are usually designed to learn user preferences based on the history of user-item interaction. \nMatrix Factorization (MF)~\\cite{koren2008factorization} generates IDs of users and items, using a high-dimensional vector to represent the location of users and items' features. \nThe inner product is used as interaction function, calculating the similarity of user/item vectors. \n%Besides, the interaction history can also be encoded to represent users and items by projecting them into high-dimensional space. \nThe MF-based method has been demonstrated effective in SVD++~\\cite{koren2008factorization} and FISM~\\cite{kabbur2013fism}.\nNCF~\\cite{he2017neural} applied neural networks to building CF models, using a fused model with MF and multi-layer perceptron (MLP) as an interaction function, taking user/item embeddings as input, and inferring preference scores. \nJNCF~\\cite{chen2019joint} extended NCF by using user/item history to replace user/item ID as the input encoding. \n\n \nRecently, the user-item interaction matrix can also be considered as a bipartite graph, thus Graph Neural Networks (GNNs)~\\cite{gao2023survey} are also applied to solve CF tasks for their ability to capture the high-order relationship between users and items~\\cite{wang2019neural}.\nThey consider both users and items as nodes and the interaction of users and items as edges in bipartite graph. \nFor example, PinSage\\cite{ying2018graph} uses sampling on a graph according to node (user/item) degrees, and learn the parameters of GNN on these sampled graphs.\nNGCF~\\cite{wang2019neural} uses a message-passing function on both users themselves and their neighbor items and collects both the information to build proper user and item embeddings. \nLightGCN~\\cite{he2020lightgcn} generates embeddings for users and items with simple SGC layers.\n\n\n"
                    },
                    "subsubsection 2.2.2": {
                        "name": "AutoML for CF",
                        "content": "\nRecently, AutoML has been frequently used in CF tasks, \naiming at finding proper hyperparameters and architectures for different tasks~\\cite{hutter2019automatedck,quanming2018auto}.\nHyperparameter Optimization (HPO) has been applied on the embedding dimension of RecSys models.\n%These works focus on features of users and items, and the components in their search space include embedding dimension and interaction function.\nFor example, \nAutoDim~\\cite{zhao2021autodim} searches embedding dimension in different fields, aiming at assigning embedding dimension for duplicated content.\nPEP~\\cite{liu2021learnable} used learnable thresholds to identify the importance of parameters in the embedding matrix,\nand trains the embedding matrix and thresholds by sub-gradient decent. \nAutoFIS~\\cite{liu2020autofis} is designed to learn feature interactions by adding an attention gate to  every potential feature interaction.\nMost of these works tune the embedding dimension adaptively on Recommender Systems tasks, mostly in Click-Through-Rate (CTR) tasks.\n\nNeural Architecture Search (NAS)  has also been applied on CF tasks, including SIF~\\cite{yao2020searching} AutoCF~\\cite{gao2021efficient} and~\\cite{wang2022profiling}.\nIn detail, \nSIF adopts the one-shot architecture search for adaptive interaction function in the CF model.\nAutoCF designs an architecture space of neural network CF models, and the search space is divided into four parts: encoding function, embedding function, interaction function and prediction function. \nAutoCF selects an architecture and its hyperparameters in the space with a performance predictor. \nHyperparameters are considered as a discrete search component in search space, neglecting its continuous characteristic. \n\\cite{wang2022profiling} designs the search space on graph-based models,  which also uses random search on the reduced search space.\n\n\n\n"
                    }
                }
            },
            "section 3": {
                "name": "Search Problem",
                "content": "\n\\label{sec::problem}\nAs mentioned in the introduction, \nfinding a proper CF model should be considered as a joint search problem on hyperparameters and architectures.\nWe propose to use AutoML to find architectures and their proper hyperparameters efficiently. \nThe joint search problem on CF hyperparameters and architectures can be modeled as a bilevel optimization problem as follows:\n\n\\begin{mdefinition}[Joint Automated Hyperparameters and Architecture Search for CF] \nLet $f^{*}$ denote the proper CF model, and then the joint search problem for CF can be formulated as:\n\\label{prob::def}\n\\begin{align}\n    \\alpha^*, h^* \n    & = \\max_{\\alpha \\in \\mathcal{A}, h \\in \\mathcal{H}}\n    \\mathcal{M}(f(\\mathbf{P}^{*}; \\alpha, h), \\mathcal{S}_{\\textit{val}}),\n    \\label{eq:autocf1}\n    \\\\\n    \\text{\\;s.t.\\;} \n    & \\mathbf{P}^* \n    = \\argmax_\\mathbf{P} \\mathcal{M}(f(\\mathbf{P}; \\alpha, h), \\mathcal{S}_{\\textit{tra}}) \\label{eq:autocf2} .\n\\end{align}\nwhere $\\mathcal{H}$ contains all possible choices of hyperparameters $h$, \nwhere $\\mathcal{A}$ contains all possible choices of architectures $\\alpha$, \n$\\mathcal{S}_{\\textit{val}}$ and $\\mathcal{S}_{\\textit{tra}}$ denote the training and validation datasets, \n$\\mathbf{P}$ denotes the learnable parameters of the CF architecture $\\alpha$, \nand $\\mathcal{M}$ denotes the performance measurement, such as \\text{Recall} and \\text{NDCG}.\n\\end{mdefinition}\n\n\nWe encounter the following key challenges in effectively and efficiently solving the search problem:\nFirstly, \nthe joint search space must contains a wide range of architectural operations within $\\mathcal{A}$, as well as frequently utilized hyperparameters in the learning stage within $\\mathcal{H}$. \nSince this space contains various types of components, including continuous hyperparameters and categorical architectures, it is essential to appropriately encode the joint search space for effective exploration.\nSecondly, considering the dependency between hyperparameters and architectures, the search strategy should be robust and efficient, meeting the accuracy and efficiency requirements of real-world applications. \nCompared to previous AutoML works on CF tasks, our method is the first to consider a joint search on hyperparameters and architectures on CF tasks. \n\n\n",
                "subsection 3.1": {
                    "name": "A",
                    "content": "\nIn this paper, the general architecture of CF models can be separated into four parts~\\cite{gao2021efficient}: \nInput Features, Feature Embedding, Interaction Function, and Prediction Function.\nBased on the frequently used operations, we build the architecture space $\\mathcal{A}$, illustrated in Table~\\ref{tab::architecture_space}.\n\n\n\n\n\\para{Input Features} \nThe input features of CF architectures come from original data: user-item rating matrix. \nWe can apply the interactions between users and items and map them to high-dimension vectors. \nThere are two manners for encoding users and items as input features: one-hot encoding (\\texttt{ID}) and multi-hot encoding (\\texttt{H}). \nAs for one-hot encoding (\\texttt{ID}), we consider the reorganized id of both users and items as input. Since the number of users and items is different, we should maintain two matrices when we generate these features.\nAs for multi-hot encoding (\\texttt{H}), we can consider the interaction of users and items. \nA user vector can be encoded in several places by its historical interactions with different items. \nThe items can be encoded in the same way.\n\n\n\n\n\n\n\\para{Feature Embedding}\nThe function of feature embedding maps the input encoding with high dimension into vectors with lower dimension.\nAccording to designs in Section~\\ref{sec::relatedwork2}, we can elaborate the embedding manners in two categories: Neural Networks based (NN-based) embeddings and Graph Neural Networks based (Graph-based) embeddings. The embedding function is related to the size of input features. \nAs for NN-based methods, a frequently used method of calculation is \\texttt{Mat}, mainly consists of a \\textit{lookup-table} in \\texttt{ID} level, and mean pooling on both users/items sides. \nWe can also use a multi-layer perceptron (\\texttt{MLP}) for each user and item side, helping convert multi-hot interactions into low-dimensional vectors.\nAs for Graph-based methods, \nthe recent advances in GNNs use more complex graph neural networks to aggregate the neighborhoods, such as \\texttt{GraphSAGE}~\\cite{hamilton2017inductive}, \\texttt{HadamardGNN}~\\cite{wang2019neural}, and \\texttt{SGC}~\\cite{he2020lightgcn} etc. \n%These methods also take interaction history as input (history can be regarded as the neighboring relation). \n\n\\para{Interaction Function}\nThe interaction function calculates the relevance between a given user and an item. \nIn this operation, the output is a vector affected by the embeddings of the user and item. The inner product is frequently used in many research works on CF tasks. We split the inner product and consider an element-wise product as an essential operation, which is noted as \\texttt{multiply}.\nIn coding level, we can also use \\texttt{minus}, \\texttt{max}, \\texttt{min} and \\texttt{concat}. \nThey help us join users and items with different element-wise calculations. \n\n\n\\para{Prediction Function}\nThis operation stage helps turn the output of the interaction function into an inference of similarity. \nAs for the output vector of a specific interaction, a simple way is to use summation on the output vector. \nThus, \\texttt{multiply+SUM} can be considered the inner product in this way. \nBesides, we use a weight vector with learnable parameters, noted as \\texttt{VEC}. \nMulti-layer perceptron (\\texttt{MLP}) can also be used for more complex prediction on similarity. \n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "H",
                    "content": "\nBesides the model architecture, \nthe hyperparameter (HP) setting also plays an essential role in determining the performance of the CF model.\nThe used components for hyperparameter space are illustrated in the first column of Table~\\ref{tab:hp_space_origin}.\n\nThe CF model, like any machine learning model, consists of standard hyperparameters such as learning rate and batch size. \nSpecifically, excessively high learning rates can hinder convergence, while overly low values result in slow optimization. \n%Similarly, the batch size represents a trade-off between efficiency and effectiveness, where selecting a larger batch size may improve computational efficiency but could potentially sacrifice model performance.\nThe choice of batch size is also a trade-off between efficiency and effectiveness.\n\nIn addition, CF model has specific and essential hyperparameters, which may not be so sensitive in other machine learning models. \nEmbedding dimension for users and items influence the representative ability of CF models.\nBesides, the embedding size determines the model's capacity to store all information of users and items. \nIn general, too-large embedding dimension leads to over-fitting, and too-small embedding dimension cannot fit the complex user-item interaction data.\nThe regularization term is always adopted to address the over-fitting problem. \n\n\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Search Strategy",
                "content": "\nAs mentioned in Section~\\ref{sec::intro}, \njoint search on hyperparameters and architectures have two challenges in designing a search strategy effectively and efficiently: the large search space and the high evaluation costs on large network datasets. \nPrevious research works on model design with joint search space of hyperparameters and architectures such as~\\cite{dong2020autohas, zela2018towards} consider the search problem in the manner of  Figure~\\ref{fig:search_one_stage}. \nThey considered the search component in space jointly and used different search algorithms to make the proper choice. \nThe search procedure can be costly on a large search space and dataset. \nTherefore, addressing the challenges of a vast search space and the costly evaluation process is crucial.\n\nThe first challenge means the joint search space of $\\mathcal{H}$ and $\\mathcal{A}$ described in Section~\\ref{sec::problem} is large for accurate search. \nThus, we need to reduce the search space. In practice, we choose to screen $\\mathcal{H}$ choices by comparing relative ranking in controlled variable experiments, which is explained in Section~\\ref{sec::strategy::screen_hp}.\nThe second challenge means the evaluation time cost in Equation~(\\ref{eq:autocf2}) is high. \nA significant validation time will lower the search efficiency. \nThus, we apply a sampling method on datasets by interaction frequency, elaborated in Section~\\ref{sec::strategy::arch_transfer}.\n\n\nIn comparison to conventional joint search problem in Figure~\\ref{fig:search_one_stage}, \nwe design a two-stage search algorithm in Figure~\\ref{fig:search_two_stage}. \nWe use Random Forest (RF) Regressor, a surrogate model, to improve the efficiency of the search algorithm. \nTo transfer the knowledge, including the relation modeling between hyperparameters, architectures, and evaluated performance, we learn the surrogate model's parameters $\\theta$ in the first stage, and we use $\\theta$ as initialization of RF model in the second stage. \nOur search algorithm is shown in Algorithm~\\ref{alg::search_alg} and described in detail in Section~\\ref{sec::strategy::arch_search}.\nBesies, we have a discussion on our choices in Section~\\ref{sec::strategy::discussion}, elaborating how we solve the challenge in the joint search problem.\n\n\n",
                "subsection 4.1": {
                    "name": "Screening Hyperparameter Choices",
                    "content": "\n\\label{sec::strategy::screen_hp}\n\nWe screen the hyperparameter (HP) choices from $\\mathcal{H}$ to $\\hat{\\mathcal{H}}$ with two techniques.\nFirst, we shrink the HP space by comparing relative performance ranking of a special HP while fixing the others. \nAfter we get the ranking distribution of different HPs, we find the performance distribution among different choices of a HP, thus we may find the proper range or shrunk set of a given HP.\nSecond, we decouple the HP space by calculating the consistency of different HP. \nIf the consistency of a HP is high, that means the performance can change positively or negatively by only alternating this HP. \nThus, this HP can be tuning separately neglecting its relation with other HPs in HP set.\n\n\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Shrink the hyperparameter space",
                        "content": "\nThe screening method on hyperparameter space is based on analysis of ranking distribution on the performance with fixed value for a certain HP and random choices for other HPs and architectures.\nIn this part, we denote a selection of hyperparameter $h \\in \\mathcal{H}$ as a vector,  noted as $h = \\left(h^{(1)}, h^{(2)},\\dots, h^{(n)}\\right)$.\n%The range for $i$-th HP $h^{(n)}$ is $\\mathcal{H}_i$. \nFor instance, $h^{(1)}$ means optimizer, and $h^{(2)}$ means learning rate. \n\n\nTo obtain the ranking distribution of a certain HP $h^{(i)}$, we start with a controlled variable experiment.\nWe vary $h^{(i)}$ in discrete values as the third column in Table~\\ref{tab:hp_space_origin}, and we vary other HPs in original range as the second column.\nSpecifically, given $H_i$ as a discrete set of HP $h^{(i)}$, we choose a value $\\lambda \\in H_i$ and we can obtain the ranking $\\texttt{rank}(\\mathsf{h}, \\lambda)$ of the anchor HP $ \\mathsf{h} \\in \\mathcal{H}_i $ by fixing other HPs except the $i$-th HP. \n\nTo ensure a fair evaluation of different architecture, \nwe traverse the architecture space and calculate rank of performance with different configurations, then we can get the distribution of a type of HP. \nThe relative performance ranking with different HPs is shown as violin plots in Figure~\\ref{fig:perf_rank_hp_space}.\nIn this figure, we can get $\\mathcal{\\hat H}$ through the distribution of different HP values. \nWe learn that the proper choice for optimizer can be shrunk to Adam and Adagrad; Proper range for learning rate is (1e-5, 1e-2); Proper range for embedding dimension can be reduced to [2, 64]; And we can fix weight decay in our experiments. \nWe demonstrate the conclusion in the fourth column in Table~\\ref{tab:hp_space_origin}.\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Decouple the hyperparameter space",
                        "content": "\n\n\n\nTo decouple the search space, \nwe consider the consistency of the ranking of hyperparameters when only alternating a given hyperparameter.\nFor the $i$-th element $h^{(i)}$ of $h\\in \\mathcal{H}$, \nwe can change different values for $h^{(i)}$, \nand then we can decouple the search procedure of the $i$-th hyperparameter with others.\nWe use Spearman Rank-order Correlation Coefficient (\\texttt{SRCC}) to show the consistency of various types of HPs, which is definited in Equation~(\\ref{eq:srcc_def}).\n\n\\begin{equation}\\label{eq:srcc_def}\n\t\\texttt{SRCC}(\\lambda_1, \\lambda_2) = 1-\\frac{\\sum_{h\\in\\mathcal{H}_i}|\\texttt{rank}(h,\\lambda_1)-\\texttt{rank}(h,\\lambda_2)|^2}{|\\mathcal{H}_i|\\cdot(|\\mathcal{H}_i|^2-1)}.\n\\end{equation}\nwhere $|\\mathcal{H}_i|$ means the number of anchor hyperparameters in $\\mathcal{H}_i$. \n\\texttt{SRCC} demonstrates the matching rate of rankings for anchor hyperparameters in $\\mathcal{H}_i$ with respect to $h^{(i)} = \\lambda_2$ and $h^{(i)} = \\lambda_2$.\n\nThe \\texttt{SRCC} of the $i$-th HP is evaluated by the average of $\\texttt{SRCC}(\\lambda_1, \\lambda_2)$ among different pairs of $\\lambda \\in H_i$, as is shown in Equation~(\\ref{eq:srcc_avg}).\n\n\\begin{equation}\\label{eq:srcc_avg}\n\t\\texttt{SRCC}_{i} = \\frac{1}{|H_i|^2} \\sum_{(\\lambda_1, \\lambda_2)\\in H_i\\times H_i}\\texttt{SRCC}(\\lambda_1, \\lambda_2).\n\\end{equation}\n\n%For the definition of \\texttt{SRCC}, it will be in the range of $[-1,1]$, larger consistency indicates that alternating the value of the $i$-th HP (i.e. from $\\theta_1$ to $\\theta_2$) does not influence a lot on the ranking of other hyperparameters.\n\nThe \\texttt{SRCC} results is demonstrated in Figure~\\ref{fig::srcc_hp_decouple}, \nwe can directly find that the important hyperparameter with higher \\texttt{SRCC} has a more linear relationship with its values. \nSince high embedding dimension model is time-costly during training and evaluation, we decide to use lower dimension in the first stage to reduce validation time, and then raise them on the original dataset to get better performance. \n\nTo summarize, we shrink the range of HP search space and find the consistency of different HPs. \nThe shrunk space shown in Table~\\ref{tab:hp_space_origin} help us search more accurately, and the consistency analysis on performance ranking help us find the dependency between different HPs, thus we can tune HP with high consistency separately. \n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Evaluating Architecture with Sampling",
                    "content": " \n\\label{sec::strategy::arch_transfer}\n\nTo evaluate architecture more efficiently,  we collect performance information evaluated from subgraphs,  since the subgraph can approximate the properties of whole graph~\\cite{zhang2022kgtuner}.\nIn this part,  we introduce our frequency-based sampling method, and then we show the transfer ability of subgraphs by testing the consistency of architectures' performance from subsampled dataset to origin dataset.\n\n\n",
                    "subsubsection 4.2.1": {
                        "name": "Matrix sampling method",
                        "content": "\nSince dataset for CF based on interaction of records, our sampling method is based on items' appearance frequency.\nThat is, we can subsample the original dataset when we preserve part of the bipartite graph, \nand the relative performance on smaller datasets should have a similar consistency (i.e. ranking distribution of performance on $\\mathcal{S}_{\\textit{val}}$ and $\\hat{\\mathcal{S}}_{\\textit{val}}$).\n\n\nThe matrix subsample algorithm is demonstrated in Algorithm~\\ref{alg::subsample}.\nFirst, we set the number of user-item interactions to be preserved first,  which can be controlled by a sample ratio $\\gamma$, $\\gamma\\in (0,1)$.\nWe calculate the interactions for each item, and then we preserve the item with a higher frequency.\nThe items can be chosen in a fixed list (i.e. \\texttt{topk}, Line 6-7 in Algorithm~\\ref{alg::subsample}), \nor the interaction frequency count of items can be normalized to a probability, \nthen different items have corresponding possibility to be preserved (i.e. \\texttt{distribute}, Line 8-10 in Algorithm~\\ref{alg::subsample}).\n\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "Transfer ability of architecture on subsampling matrix",
                        "content": "\n\nTo ensure that the relative performance ranking on subsampled datasets is similar to that on original datasets, we need to test the consistency of architecture ranking on different datasets.\nWe evaluate the transfer ability among from subgraph to whole graph by \\texttt{SRCC}.\n\nFor a given value of $\\gamma$, we choose to select a sample set of architecture from $A_{\\gamma}\\in\\mathcal{A}$.\nThen we evaluate them on a subsampled dataset $\\hat{\\mathcal{S}}$ and origin dataset $\\mathcal{S}$. The relative rank of $\\alpha\\in A_{\\gamma}$ on $\\hat{\\mathcal{S}}$ and $\\mathcal{S}$ is noted as $\\texttt{rank}(\\alpha, \\hat{\\mathcal{S}})$ and $\\texttt{rank}(\\alpha, \\mathcal{S})$.\n%We first get the rank of a fixed architecture on both smaller datasets as a group of pair data, and then we calculate \\texttt{SRCC}. \n\n\\begin{equation}\n\t\\texttt{SRCC}_{\\gamma} = 1-\\frac{\\sum_{\\alpha\\in A_{\\gamma}}|\\texttt{rank}(\\alpha, \\hat{\\mathcal{S}})-\\texttt{rank}(\\alpha, \\mathcal{S})|^2}{|A_{\\gamma}|\\cdot(|A_{\\gamma}|^2-1)}.\n\\end{equation}\n\nWe can choose different subsampled dataset $\\hat{\\mathcal{S}}$ to get average consistency.\nAs is demonstrated in Figure~\\ref{fig:srcc_sp_ml100k}, \nsample ratio with higher \\texttt{SRCC} has better transfer ability among graphs with sample mode \\texttt{topk}, and the proper sample ratio should be in $\\gamma \\in [0.2,1)$.\n\nTo summarize, through the sampling method, the evaluation cost will be reduced, and thus the search efficiency is improved. \nSince the ranking distribution among subgraphs is similar to that of the original dataset, we can transfer the evaluation modeling from small to large dataset.\n\n\n\\begin{algorithm}[t]\n\t\\caption{\\texttt{MatrixSample} Sampling Algorithm}\n\t\\label{alg::subsample}\n\t\\begin{flushleft}\n\t\t\\textbf{Input}: Matrix Dataset $\\mathcal{S}\\in\\mathbb{R}^{M \\times N}$ ($M$: number of users; $N$: number of items), sample ratio $\\gamma\\in(0,1)$, subsample mode (\\texttt{topk} or \\texttt{distribute}) \\\\\n\t\t\\textbf{Output}: Subsampled Matrix Dataset $\\hat{\\mathcal{S}}$ \\\\\n\t\\end{flushleft}\n\t\\begin{algorithmic}[1] %\n\t\t\\STATE Initialize item frequency set $f\\leftarrow \\{\\}$ ;\n\t\t\\FOR{ $j = 1,2,\\cdots, N$}\n\t\t\\STATE Calculate the users interacted with item $j$, record it as $f_j$ ;\n\t\t\\STATE $f \\leftarrow f \\bigcup f_j $;\n\t\t\\ENDFOR\n\t\t\\IF{subsample mode == \\texttt{topk}}\n\t\t\\STATE Rank $f$, choose top $\\gamma N$ items in $\\hat{f}$, neglect  unrelated users;\n\t\t\\ELSIF{subsample mode == \\texttt{distribute}}\n\t\t\\STATE Calculate probability $\\beta_j=f_j/\\sum_{k=1}^{N}f_k$;\n\t\t\\STATE Select $\\hat{f}$ with respect to $\\{\\beta_j\\}$;\n\t\t\\ENDIF\n\t\t\\STATE Construct $\\hat{\\mathcal{S}}$ with $f$;\n\t\t\\STATE \\textbf{return} $\\hat{\\mathcal{S}}$.\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n"
                    }
                },
                "subsection 4.3": {
                    "name": "Two-stage Joint Hyperparameter and Architecture Search",
                    "content": " \n\\label{sec::strategy::arch_search}\n\n%\\subsubsection{Efficient search algorithm}\nAs discussed above, \nthe evaluation cost can be highly reduced with sampling methods. \nSince the sampling method ensure the transfer ability from subgraphs to whole graphs, \nwe propose a two-stage joint search algorithm, shown in  Algorithm~\\ref{alg::search_alg}, and the framework is also shown in Figure~\\ref{fig:search_two_stage}. \nThe main notations in algorithm can be found in Table~\\ref{tab::notations} in Appendix. \nWe also compare our method with conventional method in Figure~\\ref{fig:search_procedure}.\n\nTo briefly summarize our algorithm:\nIn the first stage (Lines 3-14), \nwe sample several subgraphs with frequency-based methods, \nand preserve $\\gamma=0.2$ of interactions from the original rating matrix.\nBased our understanding of hyperparameters in Section~\\ref{sec::strategy::screen_hp}, \nwe select architecture and its hyperparameters in our reduced hyperparameter space.\n\nWe use a surrogate model to find proper architecture and hyperparameters to improve search efficiency, noted as $c(\\cdot, \\theta)$, $\\theta$ is the parameter of the model.\nAfter we get evaluations of architecture and hyperparameters, we update parameters $\\theta$ of $c$.\nIn our framework, we choose BORE~\\cite{tiao2021bore} as a surrogate and Random Forest (RF) as the regressor to modulate the relation between search components and performance. \nDetails of the BORE+RF search algorithm (\\texttt{RFSurrogate}) is shown in Algorithm~\\ref{alg::surrogate} in Appendix.\n\nWe first transfer the parameters of $c$ trained in the first stage by collected configurations and performance.\nThen, in the second stage (Lines 15-22), \nwe select architectures and hyperparameters and evaluate them in the original dataset.\nBesides, we increase the embedding dimension to reach better performance.\nSimilarly, the configurations and performance are recorded for the surrogate model (RF+BORE), \nwhich can give us the next proper configuration.\nFinally, after two-stage learning on the subsampled and original datasets, we get the final output of architecture and hyperparameters as the best choice of architecture and its hyperparameters.\n\n\n\\begin{algorithm}[t]\n\t\\caption{Joint Search Algorithm.}\n\t\\begin{flushleft}\n\t\\label{alg::search_alg}\n\t\\textbf{Input}: Train/valid set $S_{\\textit{tra}}, S_{\\textit{val}}$, \n\thyperparameter space $\\mathcal{H} $, architecture space $\\mathcal{A}$, \n\tsample number $L$, surrogate model $c(\\cdot, \\theta)$ \\\\\n\t\\textbf{Output}: Best hyperparameters $h^*$ , architecture $\\alpha^*$\\\\\n\t\\end{flushleft}\n\n\t\\begin{algorithmic}[1] %\n\t\t\\STATE Screen $\\mathcal{H} $ to $\\hat{\\mathcal{H}}$   according to Section~\\ref{sec::strategy::screen_hp};\n\t\t\\STATE Initialize configuration set $D \\leftarrow \\{ \\},k,j\\leftarrow 0 $ ;\\\\\n\t\t\\%\\textbf{stage one} start\n\t\t\\FOR{ $i = 1,2,\\cdots, L$ }\n\t\t\\STATE Sample matrix  $\\hat{\\mathcal{S}}_i=\\texttt{MatrixSample}(\\mathcal{S},\\gamma)$;\n\t\t\\STATE Split train/valid/test dataset $\\hat{\\mathcal{S}}_i=\\hat{\\mathcal{S}}_{i,\\textit{tra}}\\bigcup\\hat{\\mathcal{S}}_{i,\\textit{val}}\\bigcup\\hat{\\mathcal{S}}_{i, \\textit{tst}}$;\n\t\t\\STATE Initialize surrogate model $c(\\cdot;\\theta)$;\n\t\t\\WHILE{not converge}\n\t\t\\STATE Select $\\alpha_k$ and $h_k$  by \\texttt{RFSurrogate};\n\t\t\\STATE Evaluate $p_k \\leftarrow \\mathcal{M}(f(\\textbf{P*};\\alpha_k, h_k), \\hat{\\mathcal{S}}_{i,\\textit{val}})$ ;\n\t\t\\STATE Save to set $D \\leftarrow D \\bigcup \\{ \\alpha_k, h_k, p_k \\} $;\n\t\t\\STATE Update $c(\\cdot;\\theta)$ with $D$ by \\texttt{RFSurrogate};\n\t\t\\STATE $k\\leftarrow k+1$;\n\t\t\\ENDWHILE\n\t\t\\ENDFOR\n\t\t\n\t\t\\STATE Transfer parameters $\\theta$ in $c$;\n\n\t\t\\%\\textbf{stage two} start \n\t\t\\WHILE{not converge}\n\t\t\\STATE Select $\\alpha_j$ and $h_j$ by \\texttt{RFSurrogate}; \n\t\t\\STATE Evaluate $p_j \\leftarrow \\mathcal{M}(f(\\textbf{P*};\\alpha_j, h_j), \\mathcal{S}_{\\textit{val}})$ ;\n\t\t\\STATE Save to set $D \\leftarrow D \\bigcup \\{ \\alpha_j, h_j, p_j \\} $;\n\t\t\\STATE Update $c(\\cdot;\\theta)$ with $D$ by \\texttt{RFSurrogate};\n\t\t\\STATE $j\\leftarrow j+1$;\n\t\t\\ENDWHILE\n\t\t\\STATE \\textbf{return} $\\alpha^*, h^*$.\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\\label{sec::exp}\n\nExtensive experiments are performed to evaluate the performance of our search strategy by answering the following several research questions:\n\n\\begin{itemize}[leftmargin=*]\n\t\\item \\textbf{RQ1}: How does our algorithm work in comparison to other CF models and automated model design works?\n\t\\item \\textbf{RQ2}: How efficiently does our search algorithm work in comparison to other typical search algorithms? \n\t\\item \\textbf{RQ3}: What is the impact of every part of our design? \n\t\\item \\textbf{RQ4}: How specific is our model for different CF tasks? \t\n\\end{itemize}\n\n\n\n\n\n\n\n\n",
                "subsection 5.1": {
                    "name": "Experimental Settings",
                    "content": "\n\n",
                    "subsubsection 5.1.1": {
                        "name": "Datasets",
                        "content": "\nWe use MovieLens-100K, MovieLens-1M, Yelp, and Amazon-Book for CF tasks. \nThe detailed statistics and preprocess stage of datasets are shown in Table~\\ref{tab::dataset_details} in Appendix~\\ref{sec::appen::dataset}.\n\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Evaluation metrics",
                        "content": "\nAs for evaluation metrics, \nwe choose two widely used metrics for CF tasks, Recall$@K$ and NDCG$@K$.\nAccording to recent works~\\cite{he2017neural, wang2019neural}, \nwe set the length for recommended candidates $K$ as 20. \nWe use Recall@20 and NDCG@20 as validation.\nAs for loss function, \nwe use BPR loss~\\cite{rendle2009bpr}, \nthe state-of-the-art loss function for optimizing recommendation models.\n\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Baselines for Comparison",
                        "content": "\n\\label{sec::experiments::baselines}\n\nSince we encode both hyperparameters and architectures, we can use previous search algorithms on hyperparameters~\\cite{bergstra2012random, snoek2012practical, falkner2018bohb, tiao2021bore} and extend them on a joint search space. The details of search algorithms can be found in Appendix~\\ref{sec::append::baseline_algo}.\n\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Performance Comparison (RQ1)",
                    "content": "\nFor CF tasks, we compare our results with NN-based CF models and Graph-based CF models.\nBesides, we also compare our search algorithm with other search algorithms designed for CF models.\nThe search space is based on analysis of hyperparameter understanding in Section~\\ref{sec::strategy::screen_hp}.\nWe report the performance on four datasets in Table~\\ref{tab::model_perform}. \n\nWe summarize the following observation:\n\n\\begin{itemize}[leftmargin=*]\n\t\\item \n\tWe find that in our experiment, our CF model trained by searched hyperparameters and architectures can achieve better performance than the classical CF models. \n\tSome single models also perform well due to their special design. \n\tFor example, LightGCN performs well on ML-100K and Yelp, while NGCF also performs well on ML-1M.\n\tSince our search algorithm has included various operations used in CF architectures, the overall architecture space can cover the architectures of classical models. \n\t\\item \n\tCompared to NAS method on CF models, our method works better than SIF for considering multiple operations in different stages of architecture. \n\tOur methods also outperform AutoCF by 3.10\\% to 12.1\\%.\n\tThe reason for that is we have an extended joint search space for both hyperparameters and architectures.\n\tBesides, our choice for hyperparameters is shrunk to a proper range to improve search efficiency and performance.\n\t\\item \n\tWe can observe in Table~\\ref{tab::model_perform} that our searched models can achieve the best performance compared with all other baselines. Note that our proposed method can outperform the best baseline by 2.33\\% to 13.02\\%.\n\tThe performance improvement on small datasets is better than that on large ones.\n\\end{itemize}\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Algorithm Efficiency (RQ2)",
                    "content": "\n\nWe compare the different search algorithms mentioned in Section~\\ref{sec::experiments::baselines}. \nThe search results are shown in Figure~\\ref{fig:search_algo_compare} on dataset ML-100K and Figure~\\ref{fig:search_algo_compare_ml_1m} on dataset ML-1M. \nWe plot our results by the output of the search algorithm.\nAs is demonstrated in Figure~\\ref{fig:search_algo_compare_ml_1m} and~\\ref{fig:search_algo_compare}, \nsearch algorithms of BO perform better than random search, since the BO method considers a Gaussian Process surrogate model for simulating the relationship between performance output and hyperparameters and architectures. \nWe find that BORE+RF outperforms other search strategies in efficiency. \nThe reason is that the surrogate model RF can better classify one-hot encoding of architectures. \nBesides we also compare different time curves for BORE+RF in single-stage and two-stage evaluations. \n%Our two-stage algorithm used a subsampling method on the rating matrix, which can improve search efficiency. \n%Since the surrogate model can be transferred to the origin dataset with high consistency in performance, it can achieve better performance.\nSince our two-stage algorithm uses subsampling method on rating matrix, and ensure the consistency between subgraph and whole graph, we can achieve better performance and higher search efficiency.\n\n\n\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Ablation Study (RQ3)",
                    "content": "\n\\label{sec::search_ablation}\nIn this subsection, we analyze how important and sensitive the various components of our framework are.\nWe focus on the performance's improvement and efficiency by reducing and decoupling the hyperparameter space. \nWe also elaborate on how we choose sample ratio and effectiveness of tuning hyperparameters.\n\n",
                    "subsubsection 5.4.1": {
                        "name": "Plausibility of screening hyperparameters",
                        "content": "\nTo further validate the effectiveness of our design of screening hyperparameter choices, we choose hyperparameters on the origin space, shrunk space, and decoupled space.\nThe search time curve is shown in Figure~\\ref{fig:hp_ablation}.\n\nWe demonstrate that screening hyperparameter choices can help improve performance on CF tasks and search efficiency. According to our results, performance on $\\hat{\\mathcal{H}}$ is better than the one on the origin space $\\mathcal{H}$.\nThe reason is that the shrunk hyperparameter space has a more significant possibility of including better HP choices for CF architectures to achieve better performance. \nThe search efficiency is improved since the choice of hyperparameters is reduced, and proper hyperparameters can be found more quickly in a smaller search space. \nWe also find that the search efficiency reduces when we increase the batch size and embedding dimension when we search on the decoupled search space. \nWhile increasing batch size and embedding dimension help improve final performance, the cost evaluation is higher, which may reduce search efficiency. \n\n\n"
                    },
                    "subsubsection 5.4.2": {
                        "name": "Choice of Sampling Ratio",
                        "content": "\nTo find the impact of changing sampling ratio, we search with different sampling ratios on different datasets in the same controlled search time.  \nThe evaluation results (Recall@20) for experiments on different sampling ratio settings are listed in Table~\\ref{tab::subsample_ratio}.\n\nAccording to the table, smaller sampling ratios have lower performance. \nThe reason is that the user-item matrix sampled by low sample ratio may not capture the consistency in the origin matrix,  as the consistency results shown in Section~\\ref{sec::strategy::arch_transfer}.\nWhile sampled dataset generated by higher sample ratio has more considerable consistency with the original dataset, the results in a limited time may not be better. \nThe reason is that too much time on evaluation in the first stage may have fewer results for the surrogate model to learn the connection between performance and configurations of hyperparameters and architectures.\nThus, the first stage has a trade-off between sample ratio and time cost,  and we choose 20\\% as our sample ratio in our experiments.\n\n\n\n\n\n\n"
                    },
                    "subsubsection 5.4.3": {
                        "name": "Tuning on Hyperparameters",
                        "content": "\n\nTo show the effectiveness of our design on joint search, we propose to apply our joint search method to previous research work AutoCF~\\cite{gao2021efficient}.\nThe results are shown in Table~\\ref{tab::hp_tune_compare}. \nWe apply our search strategy to AutoCF to include hyperparameters in our search space,  noted as AutoCF(HP).\nWe find that hyperparameter searched on shrunk space can perform better than the one that uses architecture space and random search on different datasets. \n\n\n\n\n\n"
                    }
                },
                "subsection 5.5": {
                    "name": "Case Study (RQ4)",
                    "content": "\nIn this part, we mainly focus on our search strategy's search results of different architectures. \nThe search results with top performance share some similarities, while different architecture operations have different performances.\n\nAccording to search results, we present the top models for each task on all datasets in Table~\\ref{tab::case_study_top}.\nIt is easy to find that interaction history-based encoding features may have better performance and more powerful representative ability. Besides, the embedding function of \\texttt{SGC} has stronger representative ability since it can capture the high-order relationship. \nBoth \\texttt{SGC} and \\texttt{HadanardGCN} collect infomation from different layers, simply designed \\texttt{SGC} have stronger ability.\nAs for the interaction function, we find the classical element-wise \\texttt{multiply} can receive strong performance; The prediction function of learnable vector and \\texttt{MLP} can capture more powerful and complex interaction than \\texttt{SUM}.\nWe can find that these top models of each task have similar implementations, but there also have some differences among different datasets. One top architecture on a given dataset may not get the best performance on another one.\n\nIn summary, the proper architectures for different datasets may not be the same, \nbut these top results may share some same operations in architecture structures. \nThe result and analysis can help human experts to design more powerful CF architectures.\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion and future work",
                "content": "\n\\label{sec::conclusion}\nIn this work, we consider a joint search problem on hyperparameters and architectures for Collaborative Filtering models. \nWe propose a search framework based on a search space consisting of frequently used hyperparameters and operations for architectures. \nWe make a complete understanding of hyperparameter space to screen choices of hyperparameters, \nWe propose a two-stage search algorithm to find proper hyperparameters and architectures configurations efficiently. \nWe design a surrogate model that can jointly update CF architectures and hyperparameters and can be transferred from small to large datasets.\nWe do experiments on several datasets, including comparison on different models, search algorithms and ablation study.\n\nFor future work, we find it important to model CF models based on Knowledge Graphs as a search problem. \nWith additional entities for items and users, deep relationships can be mined for better performance. \nAn extended search framework can be built on larger network settings.\nModels on extensive recommendation tasks and other data mining tasks can also be considered as a search problem.\n\n\n\\begin{acks}\nThis work is partially supported by the National Key Research and Development Program of China under 2021ZD0110303, the National Natural Science Foundation of China under 62272262, 61972223, U1936217, and U20B2060, and the Fellowship of China Postdoctoral Science Foundation under 2021TQ0027 and 2022M710006.\n\\end{acks}\n\n\\clearpage\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{bibliography}\n\\clearpage\n\\nobalance\n\n\\appendix\n\\setcounter{table}{0}   \n\\setcounter{figure}{0}\n\\setcounter{algorithm}{0}\n\\renewcommand{\\thetable}{A\\arabic{table}}\n\\renewcommand{\\thefigure}{A\\arabic{figure}}\n\\renewcommand{\\thealgorithm}{A\\arabic{algorithm}}\n\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n",
                "subsection 7.1": {
                    "name": "Search Space",
                    "content": "\nThe main notations in this paper are listed in Table~\\ref{tab::notations}, and we discuss some details for search space in this section. \n\n",
                    "subsubsection 7.1.1": {
                        "name": "Hyperparameter Choice",
                        "content": "\nWe explain how to reduce the hyperparameters by Figure~\\ref{fig:perf_rank_hp_space} in this section.\nWe shrink hyperparameter search space based on the performance ranking distribution. \nWe can split the hyperparameters into four categories:\n\n\\begin{itemize}[leftmargin=*]\n\t\\item Reduction: \n\tThis kind of hyperparameter is usually categorical, like optimizer. \n\tThe choices of hyperparameters can be reduced.\n\t\\item Shrunk range: \n\tThis kind of hyperparameter is usually selected in a continuous range, such as learning rate.\n\tThe choices of these values can be constrained to a smaller range.\n\t\\item Monotonous related:\n\tThe performance with  this kind of hyperparameter usually rises when the hyperparameter increase, such as embedding dimension. However, we can not choose these HPs with too large values for limited memory. Thus, we choose a smaller value in first stage and a larger one in second stage.\n\t\\item No obvious pattern:\n\tWe do not have to change this kind of HP in our experiment, just as weigh decay. \n\\end{itemize}\n\n\n%\\subsubsection{Architecture Choice}\n%We explain about the choices for embedding functions in this section.\n%\n%As for GNN-based search operation, \n%\\texttt{GraphSAGE}~\\cite{hamilton2017inductive} first samples the user-item graph by importance of neighborhoods,\n%then concatenates the vectors from sampled graphs as embedding for users/items;\n%\\texttt{HadamardGCN}~\\cite{wang2019neural} constructs messages not only from one-step neighborhoods,\n%but also from the user/item node itself;\n%\\texttt{SGC}~\\cite{he2020lightgcn} is an easily designed GCN, combining output from each layer and adding learnable weights between different layers.\n\n"
                    }
                },
                "subsection 7.2": {
                    "name": "Search Algorithms",
                    "content": "\n\n",
                    "subsubsection 7.2.1": {
                        "name": "Surrogate Model Design",
                        "content": "\nWe demonstrate our search algorithm with surrogate model in Algorithm~\\ref{alg::surrogate}.\n\nWe design our search algorithm with BORE and Random Forest (RF) regressor.  \n%It consists of a surrogate model with an regressor. \nIn the first stage, we train the surrogate model with $D$, update parameters of surrogate model.  \nThe output of BORE+RF can help give an inference between $(0,1)$, and we choose the least one as output. \nAfter the first stage, we save the parameters of this surrogate model, and transfer it to second stage, which we do experiment on larger datasets.\nThe configuration of hyperparameters and architectures and the performance on larger dataset can also update the parameters of surrogate model. \nWith the knowledge we learn on the first stage, the surrogate model can better choose the proper architecture and hyperparameter for CF tasks.\n\n\\begin{algorithm}[t]\n\t\\caption{\\texttt{RFSurrogate} RF+BORE surrogate model}\n\t\\begin{flushleft}\n\t\t\\label{alg::surrogate}\n\t\t\\textbf{Input}: Training data $S_{\\textit{tra}}$ and evaluation data $S_{\\textit{val}}$, \n\t\tReduced hyperparameter space $\\hat{\\mathcal{H}}$,  \n\t\tArchitecture space $\\mathcal{A}$ , percentage threshold $\\tau=0.2$, RF regressor $y=c(\\alpha||h;\\theta)$ \\\\\n\t\t\\textbf{Output}: A proper configuration of hyperparameters $h^*$ and architecture $\\alpha^*$\\\\\n\t\\end{flushleft}\n\t\n\t\\begin{algorithmic}[1] %\n\t\t\\STATE Initialize configuration set $D \\leftarrow \\{\\} , i\\leftarrow 0$  \\\\\n\t\t\\% \\textbf{Initialize surrogate model}; \\\\\n\t\t\\FOR{i = 1, 2, \\dots, N}\n\t\t\\STATE Select architecture $\\alpha_i \\in \\mathcal{A}$, hyperparameter $h_i\\in \\mathcal{H}$ ;\n\t\t\\STATE Get CF evaluation performance $p_i = \\mathcal{M}(f(\\mathbf{P}^{*}; \\alpha_i, h_i), \\mathcal{S}_{\\textit{val}})$;\n\t\t\\STATE Save to set $D \\leftarrow D \\bigcup \\{ \\alpha_i, h_i, p_i \\} $; \\\\\n\t\t\\% \\textbf{BORE} \\\\\n\t\t\\STATE Find $p_\\tau$ as the $\\tau$-quantile of the performance set $\\{p_i\\}$ ;\n\t\t\\IF{$p_i < \\tau$}\n\t\t\\STATE Set $z_i$ label 0;\n\t\t\\ELSIF{$p_i \\geq \\tau$}\n\t\t\\STATE Set $z_i$ label 1;\n\t\t\\ENDIF\n\t\t\\STATE Fit the surrogate model with $D$;\n\t\t\\ENDFOR\n\t\t\n\t\t\\% \\textbf{Update surrogate model} (with a new data);\\\\\n\t\t\\STATE Get performance $p_k = \\mathcal{M}(f(\\mathbf{P}^{*}; \\alpha_k, h_k), \\mathcal{S}_{\\textit{val}})$; \n\t\t\\STATE Save to set $D \\leftarrow D \\bigcup \\{ \\alpha_k, h_k, p_k \\} $ ;\n\t\t\\STATE Set $z_i$;\n\t\t\\STATE Fit to update RF model $c(;\\theta)$ ;\n\t\t\n\t\t\\% \\textbf{Find next choice} through $c(\\cdot;\\theta)$; \\\\\n\t\t\\STATE $z_{\\text{max}}\\leftarrow 0$\n\t\t\\STATE Randomly sample configuration set  $H=\\{h_l, \\alpha_l \\}$ ;\n\t\t\\FOR{configuration in $H$}\n\t\t\\STATE $z_l=c(\\alpha_l||h_l;\\theta)$;\n\t\t\\IF{$z_l > z_{\\text{max}}$}\n\t\t\\STATE $h^*$, $\\alpha^* = h_l, \\alpha_l$;\n\t\t\\ENDIF\n\t\t\\ENDFOR\n\n\t\t\\STATE return $h^*$, $\\alpha^*$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\n\n"
                    },
                    "subsubsection 7.2.2": {
                        "name": "Fair Comparison",
                        "content": "\nCompared with hyperparameters, the choice of architectures can affect the performance of CF models more.\nThus, to compare different experiment settings fairly, we use the average of top5 configurations instead for the search time curve, noted as \\texttt{top5avg}.     \n\n\n\n"
                    },
                    "subsubsection 7.2.3": {
                        "name": "Search Procedure",
                        "content": "\nWe show the comparison of conventional joint search method and our method in Figure~\\ref{fig:search_procedure}. \nIn Figure~\\ref{fig:search_procedure}, conventional one-stage  method search components separately, while our method search hyperparameters on a shrunk space. \nBesides, the evaluation time is lower in the first stage in our method. \n\n\n"
                    }
                },
                "subsection 7.3": {
                    "name": "Discussion",
                    "content": "\n\\label{sec::strategy::discussion}\n\nIn this section, we discuss about the method we have chosen, and the difference with previous works on joint search problems.\n\nThe first is the reason for screening in the hyperparameters space rather than the architecture space.\nThe search space of hyperparameters mainly consists of components of continuous values with infinite choices. \nAnd screening range of hyperparameters~\\cite{zhang2022knowledge,wang2022profiling} is proven effective on deep neural networks and graph-based models.\nArchitecture space components are typically categorized, and each one is necessary in some manner and should not be ignored.\nFurthermore, we believe that it is unnecessary to reduce the architecture space after conducting a fair ranking of different architectures.\n\n\n%The second is how our methodology differs from previous research studies on joint search, such as~\\cite{dong2020autohas, zela2018towards, seng2023feathers, cai2021stnas}.\nOur work is the first on CF tasks, which is different from previous research studies on joint search.\n\\cite{zela2018towards} mainly focuses on a joint search problem on typical neural networks.\nAnother study on the joint search problem, AutoHAS~\\cite{dong2020autohas}, focuses on model search with weight sharing.\nFEATHERS~\\cite{seng2023feathers} focuses on a joint search problem on Federate Learning, an aspect of Reinforcement Learning.\nIn ST-NAS~\\cite{cai2021stnas}, the sub-models are candidates of a designed super-net, and they sample sub-ST-models from the super-net and weights are update using training loss while updating HPs with the validation loss.\n\n\n%As for sampling method, we sample on items but not users because we are motivated to recommend different items for different users through users' preference. Thus, the mostly connected items can be the most representative ones.  \n%As for the frequency-base sample algorithm, we can also consider it in a graph perspective, the frequency of an item equals its degree in a bipartite graph. \n\n"
                },
                "subsection 7.4": {
                    "name": "Data Preprocess",
                    "content": "\n\\label{sec::appen::dataset}\n\n",
                    "subsubsection 7.4.1": {
                        "name": "Details of Datasets",
                        "content": "\nIn our experiments, we choose four real-world raw datasets and build them for evaluation on CF tasks. \n\n\\begin{itemize}[leftmargin=*]\n\t\\item \\textbf{MovieLens-100K}\n\t\\footnote{https://grouplens.org/datasets/movielens/100k} \n\tThis widely used movie-rating dataset\n\tcontains 100,000 ratings on movies from 1 to 5. We also convert\n\tthe rating form to binary form, where each entry is c vmarked as 0\n\tor 1, indicating whether the user has rated the item.\n\t\\item \\textbf{MovieLens-1M}\n\t\\footnote{https://grouplens.org/datasets/movielens/1m}\n\tThis widely used movie-rating dataset\n\tcontains more than 1 million ratings on movies from 1 to 5. Similarly, for\tMovieLens-1M, we  build an implicit dataset.\n\t\\item \\textbf{Yelp} \n\t\\footnote{https://www.yelp.com/dataset/download}\n\tThis is published officially by Yelp, a crowd-sourced review\n\tforum website where users can write comments and reviews\n\tfor various POIs, such as hotels, restaurants, etc.\n\t\\item \\textbf{Amazon-Book}\n\t\\footnote{https://nijianmo.github.io/amazon/index.html}\n\tThis book-rating dataset is collected from\n\tusers’ uploaded review and rating records on Amazon.\n\\end{itemize}\n\n\n\n\n"
                    },
                    "subsubsection 7.4.2": {
                        "name": "Preprocess",
                        "content": "\nSince the origin dataset in Table~\\ref{tab::dataset_details} is too large for training, \nwe reduce them in frequency order. \nWe use 10-core select on Yelp and 50-core select on Amazon-Book.\n$N$-core means we choose users and items that appear more than $N$ times in the whole record history.\nAfter we select the dataset, we split the data into training, validation and test sets. \nWe shuffle each set when we start a new evaluation task.\n\n\n\n"
                    },
                    "subsubsection 7.4.3": {
                        "name": "Dataset sampling",
                        "content": "\nSome papers studying long-tail recommendation~\\cite{zheng2021disentangling} or self-supervised recommendation~\\cite{wu2021self} may discuss the impact of data sparsity.\nWe can sample the subgraph according to the popularity~\\cite{he2016fast}, mainly user-side and item-side, and the long-tail effect on the item-side is more severe.\nThus we sample the rating matrix based on the item frequency. \nThe more the item appears in rating records, the more likely it is reserved in the subsampled matrix.\n\n\n"
                    }
                },
                "subsection 7.5": {
                    "name": "Detailed Settings of Experiments",
                    "content": "\n",
                    "subsubsection 7.5.1": {
                        "name": "Baseline algorithms",
                        "content": "\n\\label{sec::append::baseline_algo}\n\\begin{itemize}[leftmargin=*]\n\t\\item \\textbf{Random Search}~\\cite{bergstra2012random}.\n\tIt is a simple method for finding hyperparameters and architectures with random choices on either continuous or categorical space.\n\t\\item \\textbf{Bayesian Optimization}~\\cite{snoek2012practical}.\n\tBayesian Optimization (BO) is a search algorithm based on the analysis of posterior information, and it can use Gaussian Process as a surrogate model. \n\t\\item \\textbf{BOHB}~\\cite{falkner2018bohb}.\n\tBOHB is a method consisting of both Bayesian Optimization (BO) and HyperBand (HB), helping modulate functions with a lower time budget.\n\t\\item \\textbf{BORE}~\\cite{tiao2021bore}.\n\tBORE is a BO method considering expected improvement as a binary classification problem. \n\tIt can be combined with regressors, including Random Forest (RF), Multi-Layer Perceptron (MLP), and Gaussian Process (GP). We choose RF as a regressor in our experiments.\n\\end{itemize}\n\n\n"
                    },
                    "subsubsection 7.5.2": {
                        "name": "Hardware Environment",
                        "content": "\nWe implement models with PyTorch 1.12 and run experiments on a 64-core Ubuntu 20.04 server with NVIDIA GeForce RTX 3090 GPU with 24 GB memories each. \nIt takes 3.5-4 hours to search on a dataset with one million records.\n"
                    },
                    "subsubsection 7.5.3": {
                        "name": "Codes",
                        "content": "\n%Our coding files can be found in~\\href{https://anonymous.4open.science/r/CodeForKDD23-17BE}{HERE}.\nWe have released our implementation code for experiments in~\\href{https://github.com/overwenyan/Joint-Search}{https://github.com/overwenyan/Joint-Search}\n\n%\\href{https://github.com/overwenyan/CodeForKDD23}{HERE}.\n\n\n\n"
                    }
                }
            }
        },
        "tables": {
            "tab::architecture_space": "\\begin{table}[t]\n    \\centering\n    \\caption{The operations we use for architecture space.}\n    \\label{tab::architecture_space}\n%    \\vspace{-10pt}\n    \\setlength\\tabcolsep{2pt}\n    \\begin{tabular}{c|c|C{90px}}\n        \\toprule\n        \\bf  Architecture  & \\multicolumn{2}{c}{\\bf Operations} \\\\ \\midrule\n        \\multirow{2}{*}{\\bf Input Features}  &  User &\\texttt{ID}, \\texttt{H}  \\\\ \\cmidrule{2-3}\n        & Item &\\texttt{ID},\\, \\texttt{H}  \\\\ \\midrule\n        {\\bf Features Embedding}   &  NN-based  &\\texttt{Mat},\\, \\texttt{MLP} \\\\ \\cmidrule{2-3}\n        & Graph-based  & \\texttt{GraphSAGE}, \\,\\texttt{HadamardGCN} ,\\, \\texttt{SGC}  \\\\ \n        \\midrule\n        \\bf  Interaction Function   & \\multicolumn{2}{c}{ \\texttt{multiply},\\, \\texttt{minus},\\, \\texttt{min},\\, \\texttt{max},\\, \\texttt{concat}}   \\\\ \n        \\midrule\n        \\bf Prediction Function &\\multicolumn{2}{c}{ \\texttt{SUM},\\, \\texttt{VEC}, \\, \\texttt{MLP} }\\\\ \n        \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:hp_space_origin": "\\begin{table*}[t]\n\t\\centering\n\t\\caption{Origin hyperparameter space, discretized values, and shrunk range after using screening method in Section~\\ref{sec::strategy::screen_hp}. }\n\t\\label{tab:hp_space_origin}\n%\t\\vspace{-10pt}\n\t\\begin{tabular}{c|c|c|c}\n\t\t\\toprule\n\t\tHyperparameter & Original range & Discrete values & Shrunk range \\\\\n\t\t\\midrule\n\t\toptimizer \t\t& Adagrad, Adam, SGD & Adagrad, Adam, SGD & Adagrad, Adam \\\\ \n\t\tlearning rate   &  [1e-6, 1e-1]   &  $\\{ 10^{-6}, 10^{-5},\\dots,10^{0}\\}$ & [1e-5, 1e-2]       \\\\ \n\t\tembedding dimension   & [1,512] &    $\\{2^{0},2^{1},\\dots, 2^{9}\\}$   &    [2,64]    \\\\ \n\t\tweight decay \t& [1e-5, 1e-1] \t & $\\{ 10^{-5}, 10^{-4},\\dots,10^{-1}\\}$ &  1e-1      \\\\ \n\t\tbatch size \t& [500, 5000] \t & $\\{ 500, 1000,\\dots, 5000 \\}$    & $\\{ 2000 \\}$    \\\\ \n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table*}",
            "tab::model_perform": "\\begin{table*}[t]\n%\t\\small\n\t\\caption{Comparison of different methods on CF tasks.}\n\t\\label{tab::model_perform}\n%\t\\vspace{-10pt}\n\t\\setlength\\tabcolsep{2pt}\n\t\\centering\n\t\\begin{tabular}{cccccccccc}\n\t\t\\toprule\n\t\t\\multicolumn{2}{c}{\\bf Dataset}   & \\multicolumn{2}{c}{\\bf MovieLens-100K} & \\multicolumn{2}{c}{\\bf MovieLens-1M} & \\multicolumn{2}{c}{\\bf Yelp} & \\multicolumn{2}{c}{\\bf Amazon-Book} \\\\ \\midrule\n\t\t\\multicolumn{2}{c}{\\bf Metric} &\\bf Recall@20 &  \\bf NDCG@20 &\\bf  Recall@20 &   \\bf NDCG@20 &\\bf Recall@20 &  \\bf NDCG@20  &\\bf  Recall@20 &   \\bf NDCG@20  \\\\ \\midrule\n\t\t&  MF~\\cite{koren2009matrix}       \t& 0.1145    &  0.1179\t&  0.0896   &  0.0584\t&  0.0307   &  0.0286 \t&  0.0291   &   0.0213   \\\\\n\t\t&  FISM~\\cite{kabbur2013fism}      \t&  0.1434 \t&  0.1422 \t&  0.0995   &  0.0621 \t&  0.0528   &  0.0316  \t&  0.0302   &   0.0211   \\\\\n\t\t&  NCF~\\cite{he2017neural}       \t&  0.1980 \t&  0.1521   &  0.1204   &  0.0684   &  0.0534   &  0.0335  \t&  0.0315   &   0.0223   \\\\\n\t\t&  J-NCF~\\cite{zheng2017joint}   \t&  0.2016  \t&  0.1448 \t&  0.1265   &  0.0629 \t&  0.0636   &  0.0393 \t&  0.0351   &   0.0252   \\\\ \\midrule\n\t\t&  Pinsage~\\cite{ying2018graph}    \t&  0.1561  \t&  0.1421 \t&  0.1354  \t&  0.0631 \t&  0.0561   &  0.0417 \t&  0.0321   &   0.0239   \\\\\n\t\t&  NGCF~\\cite{wang2019neural}       &  0.1654  \t&  0.1479\t&  0.1678   &  0.0852  \t&  0.0573   &  0.0454  \t&  0.0349   &   0.0247   \\\\\n\t\t&  LightGCN~\\cite{he2020lightgcn}  \t&  0.2342\t&  0.1755\t&  0.1637   &  0.0842 \t&  0.0689   &  0.0471  \t&  0.0355   &   0.0273   \\\\ \\midrule\n\t\t&  SIF~\\cite{yao2020searching} \t\t&  0.1935   &  0.1532\t&  0.1294   &  0.0695  \t&   0.0581  &  0.0459  \t&  0.0350   &   0.0249   \\\\\n\t\t%\t\t&  Profiling~\\cite{wang2022profiling}  &  0.2743   &  0.2019  \t&  -   &  -  \t&  -   &  - \t&  -   &     -       \\\\\n\t\t&  AutoCF~\\cite{gao2021efficient}  \t&  0.2259   &  0.1703 \t&  0.1309   &  0.0739 \t&  0.0643   &  0.0467  \t&  0.0354   &   0.0265   \\\\ \n%\t\t&   Ours (w/o tune HP)  \t\t&  0.2542   &  0.1821  \t&  -   &  -  \t&  -   &  - \t&  -   &     -       \\\\\n\t\t&  \\bf Ours  \t\t\t\t\t\t&  0.2647   &  0.1913\t&  0.1787   &  0.0945  \t&  0.0721   &  0.0482  \t&  0.0365   &   0.0281   \\\\ \n\t\t\\midrule\n%\t\t& \\bf Improvement   \t\t\t\t&  13.02\\% \t&  12.33 \\% & 9.16 \\% \t& 12.23\\% \t&   4.64\\%\t&\t 2.33\\%\t&\t3.11\\%\t&   2.93\\%   \\\\ \n\t\t& \\bf Improvement   \t\t\t\t&  13.02\\% \t&  9.00 \\% & 6.50 \\% \t& 10.92\\% \t&   4.64\\%\t&\t 2.33\\%\t&\t2.82\\%\t&   2.93\\%   \\\\ \n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table*}",
            "tab::case_study_top": "\\begin{table*}[ht]\n%\t\\small\n\t\\centering\n\t\\caption{Top 3 architectures for each datasets.}\n\t\\label{tab::case_study_top}\n%\t\\vspace{-10pt}\n\t\\begin{tabular}{c|ccc}\n\t\t\\toprule\n\t\t\\bf  Dataset & Top-1 & Top-2 &  Top-3  \\\\  \\midrule\n\t\t\\bf   ML-100K    &  \\texttt{$\\langle$H,H,SGC,SGC,min,VEC$\\rangle$}   &      \\texttt{$\\langle$ID,ID,SGC,SGC,multiply,SUM$\\rangle$}         &   \\texttt{$\\langle$H,H,Mat,Mat,multiply,VEC$\\rangle$}    \\\\\n\t\t\\bf     ML-1M     &  \\texttt{$\\langle$H,H,SGC,SGC,min,VEC$\\rangle$}   &  \\texttt{$\\langle$H,H,HadamardGCN,HadamardGCN,min,VEC$\\rangle$} & \\texttt{$\\langle$H,H,Mat,Mat,multiply,VEC$\\rangle$}      \\\\\n\t\t\\bf     Yelp     &   \\texttt{$\\langle$H,H,SGC,SGC,multiply,MLP$\\rangle$}   &  \\texttt{$\\langle$H,H,SGC,SGC,multiply,VEC$\\rangle$}  &      \\texttt{$\\langle$H,H,MLP,MLP,multiply,VEC$\\rangle$}        \\\\\n\t\t\\bf Amazon-Book & \\texttt{$\\langle$H,H,SGC,SGC,min,MLP$\\rangle$} & \\texttt{$\\langle$ID,ID,SGC,SGC,multiply,VEC$\\rangle$}& \\texttt{$\\langle$ID,H,Mat,MLP,max,VEC$\\rangle$}\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table*}",
            "tab::subsample_ratio": "\\begin{table}[t]\n%\t\\small\n\t\\centering\n\t\\caption{Performance with different sample ratio $\\gamma$.}\n\t\\label{tab::subsample_ratio}\n%\t\\vspace{-10pt}\n\t\\begin{tabular}{c|cccc}\n\t\t\\toprule\n\t\t\\bf  sample ratio & 5\\%  & 10\\% & \\bf 20\\% & 50\\%  \\\\\n\t\t\\midrule\n\t\\bf\tML-100K    \t&  0.2113 \t& 0.2224 \t&  \\bf 0.2646 \t&  0.2623  \\\\\n\t\\bf\tML-1M     \t&  0.1471  \t& 0.1632\t&  \\bf 0.1787\t&  0.1715  \\\\\n\t\\bf\tYelp     \t&  0.0523  \t& 0.0654 \t&  \\bf 0.0721 \t&  0.0706  \\\\\n\t\\bf\tAmazon-Book\t&  0.0311  \t& 0.0342 \t& \\bf 0.0365 \t&  0.0356   \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}",
            "tab::hp_tune_compare": "\\begin{table}[t]\n%\t\\small\n\t\\centering\n\t\\caption{Comparison on search algorithm with/without HP tuning.}\n\t\\label{tab::hp_tune_compare}\n%\t\\vspace{-10pt}\n\t\\begin{tabular}{c|cccc}\n\t\t\\toprule\n\t\t\\bf  Performance & AutoCF & AutoCF (HP) & Improvement  \\\\\n\t\t\\midrule\n\t\t\\bf\tML-100K    \t&  0.2259 \t& 0.2335 \t&   3.36\\%\t \\\\\n\t\t\\bf\tML-1M     \t&  0.1309  \t& 0.1358 \t&   3.74\\% \t\\\\\n\t\t\\bf\tYelp     \t&  0.0643  \t& 0.0672 \t&   4.51\\%    \\\\\n\t\t\\bf\tAmazon-Book\t&  0.0354  \t& 0.0359 \t&  \t1.41\\%   \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}",
            "tab::notations": "\\begin{table}[t]\n\t%\t\\small\n\t\\centering\n\t\\caption{Notations}\n\t\\label{tab::notations}\n\t%\t\\vspace{-10pt}\n\t\\begin{tabular}{c|c}\n\t\t\\toprule\n\t\t\\bf  Variable & Definition \\\\ \n\t\t\\midrule\n\t\t$h, h^*$\t& Hyperparameters (HP), best HP from $\\mathcal{H}$ \\\\\n\t\t$\\hat{h}$ & HP from $\\mathcal{\\hat{H}}$\\\\\n\t\t$h^{(i)}$ & The $i$-th HP of $h$, $h=\\left(h^{(1)}, h^{(2)}, h^{(i)}, \\dots, h^{(n)}\\right)$ \\\\\n\t\t$H_i$ & Discrite values set of $h^{(i)}$ \\\\\n\t\t$\\lambda, \\lambda_1, \\lambda_2$ & Some values of hyperparameter from $H_i$ \\\\\n\t\t$\\texttt{rank}(\\mathsf{h}, \\lambda)$ & Relative performance ranking when $h^{(i)}=\\lambda$ \\\\\n\t\t$\\mathcal{H}$     \t&   Origin space of hyperparameters    \\\\\n\t\t$\\mathcal{H}_i$     \t&   Subset of $\\mathcal{H}$, the $i$-th HP is selected from $H_i$ \\\\ \n\t\t$\\mathcal{\\hat{H}}$     \t&  Subset of $\\mathcal{H}$, screened space of hyperparameters    \\\\\n\t\t\\midrule\n\t\t$\\alpha, \\alpha^*$ & Architecture/Best architecture from $\\mathcal{A}$ \\\\\n\t\t$\\mathcal{A}$    \t&    Space of architecture     \\\\\n\t\t\\midrule\n\t\t$\\mathcal{S}$    \t&    Origin dataset (whole graph)     \\\\\n\t\t$\\mathcal{\\hat{S}}$ & The $i$-th subsampled dataset (subgraph) \\\\\n\t\t$\\gamma$ &  Subsample ratio \\\\\n\t\t$p$, $p_i$ & Test performance \\\\\n\t\t$D$ & Records of $(\\alpha, h, p)$  \\\\\n\t\t$c(\\cdot; \\theta)$ & Surrogate model with parameter $\\theta$ \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}",
            "tab::dataset_details": "\\begin{table}[t]\n%\t\\small\n\t\\centering\n\t\\caption{Statistics of datasets}\n\t\\label{tab::dataset_details}\n%\t\\vspace{-10pt}\n\t\\begin{tabular}{c|cccc}\n\t\t\\toprule\n\t\t\\bf  Name & $\\#$users & $\\#$items &$\\#$records & density \\\\ \\midrule\n\t\t\\bf\tML-100K    \t&    943    & 1,682 &  100,000  &  6.304\\%  \\\\\n\t\t\\bf\tML-1M     \t&   6,040  & 3,952 &  1,000,209 &   4.190\\%   \\\\\n\t\t\\bf\tYelp     \t&    6,102  & 18,599 &  445,576   &  0.393\\%  \\\\\n\t\t\\bf\tAmazon-Book &  25,774  & 80,211 & 3,040,864  &  0.147\\%   \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:search_strategy_whole": "\\begin{figure*}[t]\n\t\\centering\n\t\\begin{subfigure}[t]{0.29\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figs/search_one_stagev2.pdf}\n%\t\t\\vspace{-13pt}\n\t\t\\caption{Conventional methods}\n\t\t\\label{fig:search_one_stage}\n\t\\end{subfigure}\n\t\\begin{subfigure}[t]{0.69\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figs/search_two_stagev2.pdf}\n%\t\t\\vspace{-10pt}\n\t\t\\caption{Our method (Algorithm~\\ref{alg::search_alg})}\n\t\t\\label{fig:search_two_stage}\n\t\\end{subfigure}\n%\t\\vspace{-6pt}\n\t\\caption{Joint search strategy of conventional methods and our method.}\n\t\\label{fig:search_strategy_whole}\n\\end{figure*}",
            "fig:perf_rank_hp_space": "\\begin{figure*}[ht]\n\t\\centering\n\t\\begin{subfigure}[t]{0.29\\textwidth} \n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figs/opt_violin_plot_ml-100k.pdf}\n%\t\t\\vspace{-15pt}\n\t\\end{subfigure}\n\t\\begin{subfigure}[t]{0.29\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figs/learning_rate_violin_plot_ml-100k.pdf}\n%\t\t\\vspace{-15pt}\n\t\\end{subfigure}\n\t\\begin{subfigure}[t]{0.29\\textwidth} \n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figs/embedding_size_violin_plot_ml-100k.pdf}\n%\t\t\\vspace{-15pt}\n\t\\end{subfigure}\n\t\\newline\n\t\\begin{subfigure}[t]{0.29\\textwidth} \n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figs/weight_decay_violin_plot_ml-100k.pdf}\n%\t\t\\vspace{-15pt}\n\t\\end{subfigure}\n\t\\begin{subfigure}[t]{0.29\\textwidth} \n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figs/batch_size_violin_plot_ml-100k.pdf}\n%\t\t\\vspace{-15pt}\n\t\\end{subfigure}\n%\t\\vspace{-10pt}\n\t\\caption{Ranking distribution of hyperparameters.}\n\t\\label{fig:perf_rank_hp_space}\n\\end{figure*}",
            "fig::srcc_hp_decouple": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{figs/srcc_hp_ml-100k.pdf}\n%\t\\vspace{-10pt}\n\t\\caption{Consistency of each hyperparameters.}\n\t\\label{fig::srcc_hp_decouple}\n\\end{figure}",
            "fig:srcc_sp_ml100k": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.42\\textwidth]{figs/srcc_sp_ml-100kv2.pdf}\n%\t\\vspace{-10pt}\n\t\\caption{Consistency of different sample ratio. The light grey aera is the standard deviation of \\texttt{SRCC} on different $\\hat{\\mathcal{S}}$. }\n\t\\label{fig:srcc_sp_ml100k}\n\\end{figure}",
            "fig:perf_ablation": "\\begin{figure*}[h]\n\t\\centering\n\t\\begin{subfigure}[t]{0.24\\textwidth} \n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figs/search_algo_ml-100k.pdf} % \n%\t\t\\vspace{-15px}\n\t\t\\caption{}\n\t\t\\label{fig:search_algo_compare}\n\t\\end{subfigure}\n\t\\begin{subfigure}[t]{0.24\\textwidth} \n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figs/search_algo_ml-1m.pdf} % \n%\t\t\\vspace{-15px}\n\t\t\\caption{}\n\t\t\\label{fig:search_algo_compare_ml_1m}\n\t\\end{subfigure}\n\t\\begin{subfigure}[t]{0.24\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figs/hpspace_ablation_ml-100k.pdf} % \n%\t\t\\vspace{-15px}\n\t\t\\caption{}\n\t\t\\label{fig:hp_ablation}\n\t\\end{subfigure}\n\t\\begin{subfigure}[t]{0.24\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figs/hpspace_ablation_ml-1m.pdf} % \n%\t\t\\vspace{-15px}\n\t\t\\caption{}\n\t\t\\label{fig:hp_ablation_ml1m}\n\t\\end{subfigure}\n%\t\\vspace{-10pt}\n\t\\caption{Comparison for different search algorithms and HP space. \n\t\t(a)-(b) Time curve of different search algorithms on ML-100K and ML-1M; \n\t\t(c)-(d) ablation studies on our search strategy in different search space on ML-100K and ML-1M.}\n\t\\label{fig:perf_ablation}\n\\end{figure*}",
            "fig:search_procedure": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{figs/search_procedurev1.pdf}\n%\t\\vspace{-13pt}\n\t\\caption{Comparison of search procedure using one-stage method and our method.}\n\t\\label{fig:search_procedure}\n\\end{figure}"
        },
        "equations": {
            "eq:eq:srcc_def": "\\begin{equation}\\label{eq:srcc_def}\n\t\\texttt{SRCC}(\\lambda_1, \\lambda_2) = 1-\\frac{\\sum_{h\\in\\mathcal{H}_i}|\\texttt{rank}(h,\\lambda_1)-\\texttt{rank}(h,\\lambda_2)|^2}{|\\mathcal{H}_i|\\cdot(|\\mathcal{H}_i|^2-1)}.\n\\end{equation}",
            "eq:eq:srcc_avg": "\\begin{equation}\\label{eq:srcc_avg}\n\t\\texttt{SRCC}_{i} = \\frac{1}{|H_i|^2} \\sum_{(\\lambda_1, \\lambda_2)\\in H_i\\times H_i}\\texttt{SRCC}(\\lambda_1, \\lambda_2).\n\\end{equation}",
            "eq:1": "\\begin{equation}\n\t\\texttt{SRCC}_{\\gamma} = 1-\\frac{\\sum_{\\alpha\\in A_{\\gamma}}|\\texttt{rank}(\\alpha, \\hat{\\mathcal{S}})-\\texttt{rank}(\\alpha, \\mathcal{S})|^2}{|A_{\\gamma}|\\cdot(|A_{\\gamma}|^2-1)}.\n\\end{equation}"
        },
        "git_link": "https://github.com/overwenyan/Joint-Search"
    }
}