{
    "meta_info": {
        "title": "Learning for Counterfactual Fairness from Observational Data",
        "abstract": "Fairness-aware machine learning has attracted a surge of attention in many\ndomains, such as online advertising, personalized recommendation, and social\nmedia analysis in web applications. Fairness-aware machine learning aims to\neliminate biases of learning models against certain subgroups described by\ncertain protected (sensitive) attributes such as race, gender, and age. Among\nmany existing fairness notions, counterfactual fairness is a popular notion\ndefined from a causal perspective. It measures the fairness of a predictor by\ncomparing the prediction of each individual in the original world and that in\nthe counterfactual worlds in which the value of the sensitive attribute is\nmodified. A prerequisite for existing methods to achieve counterfactual\nfairness is the prior human knowledge of the causal model for the data.\nHowever, in real-world scenarios, the underlying causal model is often unknown,\nand acquiring such human knowledge could be very difficult. In these scenarios,\nit is risky to directly trust the causal models obtained from information\nsources with unknown reliability and even causal discovery methods, as\nincorrect causal models can consequently bring biases to the predictor and lead\nto unfair predictions. In this work, we address the problem of counterfactually\nfair prediction from observational data without given causal models by\nproposing a novel framework CLAIRE. Specifically, under certain general\nassumptions, CLAIRE effectively mitigates the biases from the sensitive\nattribute with a representation learning framework based on counterfactual data\naugmentation and an invariant penalty. Experiments conducted on both synthetic\nand real-world datasets validate the superiority of CLAIRE in both\ncounterfactual fairness and prediction performance.",
        "author": "Jing Ma, Ruocheng Guo, Aidong Zhang, Jundong Li",
        "link": "http://arxiv.org/abs/2307.08232v1",
        "category": [
            "cs.LG",
            "cs.CY",
            "stat.ML"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n% importance of fairness and counterfactual fairness\nRecent years have witnessed a rapid development of machine learning based prediction \\cite{schwartz2004fair,corbett2018measure,brennan2009evaluating} in various high-impact applications such as personalized recommendation \\cite{wu2021learning,mehrotra2018towards}, ranking in searches \\cite{geyik2019fairness,pitoura2020fairness}, and social media analysis \\cite{leonelli2021fair,aguirre2021gender}. \n%Recent years have witnessed a rapid development of machine learning based prediction in various high-stake decision making scenarios such as recommendation \\cite{wu2021learning}, ranking in searches \\cite{geyik2019fairness}, college admission \\cite{schwartz2004fair}, employment \\cite{corbett2018measure}, and bail judgments \\cite{brennan2009evaluating}. \nRecent literatures \\cite{berk2018fairness} have shown that the predictions based on traditional machine learning often exhibit biases against certain demographic subgroups that are described by certain protected attributes (a.k.a. sensitive attributes) such as race, gender, age, and sexual orientation. %For instance, \nThus, how to develop a \\textit{fair} predictor has attracted a surge of attentions \\cite{grgic2016case,hardt2016equality,zafar2017fairness,zemel2013learning,bellamy2019ai,bird2019fairness,wadsworth2018achieving}. Among them, the seminal work of \\textit{counterfactual fairness} \\cite{kusner2017counterfactual} makes use of the causal mechanism to model how discrimination is exhibited, and eliminates it at the individual level based on the Pearl's causal structural models \\cite{pearl2009causal}. The intuition of counterfactual fairness is to encourage the predictions made from different versions of the same individual to be equal. {\nFor example, the predictions for \n``in an online talent search, how would a certain candidate be ranked if this candidate had been a male/female?\"\n%``what would the probability of recidivism be if a certain criminal had been white/black?\" \nshould be identical to achieve the notion of counterfactual fairness.}\n\n%in various domains which require anti-discrimination decision-making mechanisms, such as college admission, employment, insurance, and crime prediction. The principle is to make fair prediction for different individuals or sub-population, i.e., people with certain sensitive attributes such as race, gender, age, and sexual orientation. A typical example is the prediction of criminal risk \\cite{larson2016we}, extensive literature \\cite{angwin2016machine} has shown discrimination against black criminals. One of the main reasons is that the observed data might be biased across different sensitive subgroups. For example, the race and age of the criminal may influence other observed features such as type of crime, which leads to bias for the predictors directly trained with such features. Traditional fairness notions, such as fairness through unawareness \\cite{grgic2016case}, equality of opportunity \\cite{hardt2016equality,zafar2017fairness}, demographic parity \\cite{zemel2013learning} suffer from such bias because they are statistical metrics. To addresses this problem, \\textit{counterfactual fairness} \\cite{kusner2017counterfactual} introduces the causal perspective to model the generation of the discrimination, and eliminates it at an individual level based on Pearl's causal structural model \\cite{pearl2009causal}. The intuition of counterfactual fairness is to encourage the prediction made from different versions of the same individual to be equal, e.g., the prediction for ``what would the probability of re-offending be if a certain criminal had been white/black?\" should be the same to achieve counterfactual fairness.\n\n% Causal graph: what is causal model; limitation of current work; motivation example\nA prerequisite of existing methods to achieve counterfactual fairness is the prior human knowledge of causal models.\n%\nA causal model \\cite{pearl2009causal,pearl2009causality} typically consists of a causal graph and the corresponding structural equations that describe the causal relationships among different variables.\n%\nExisting works on counterfactual fairness \\cite{kusner2017counterfactual,russell2017worlds,wu2019counterfactual,xu2019achieving} overwhelmingly rely on the assumption that the underlying causal model is (at least partially) known and correct, in order to mitigate the biases across different sensitive subgroups.\n%\nHowever, existing work often suffers from the following major limitation: In real world, the underlying causal model is often unknown, especially when the data is high-dimensional \\cite{belloni2017program,wang2020high}. The construction of a trustworthy causal model often requires knowledge from domain experts, which is expensive in both time and labor. In addition, it is extremely challenging to validate the correctness of the obtained causal model. Without external guidance of human knowledge, other existing works mostly rely on causal discovery techniques \\cite{spirtes2000causation,pearl2009causality,kalisch2007estimating,le2016fast,heckerman1999bayesian,spirtes2016causal} to learn the causal model from observational data, but these methods can suffer from various mistakes in discovering the causal relations, and thus lead the predictor to pick up biased information of the sensitive attribute \\cite{nauta2019causal}. %rely on strong assumptions (e.g., faithfulness \\cite{uhler2013geometry}) and are not robust to selection bias, as discussed in \\cite{nauta2019causal}. \n% are still limited due to a variety of factors such as selection biases, as discussed in \\cite{nauta2019causal}.\n%However, these works often have the following two major limitations: \\emph{\\textbf{(1) Insufficient human knowledge of causal model:}} In real-world applications, the underlying causal model is often unknown, especially when the data is high-dimensional []. The construction of a trustworthy causal model often requires knowledge from domain experts, but this process is both time and labor expensive, and the correctness of the obtained causal model is hard to be validated. \n%\\emph{\\textbf{(2) Incorrect causal model by causal discovery:}} Without external guidance of human knowledge, a vast majority of existing works rely on different causal discovery techniques \\cite{spirtes2000causation,pearl2009causality,kalisch2007estimating,le2016fast} to learn the causal model from observational data, but these methods are still limited due to a variety of factors such as confounding variables \\cite{nauta2019causal}. % such as: \\emph{2.a) the existence of spurious statistical correlations}; and \\emph{2.b) imbalanced distribution of sensitive subgroups}. Firstly, the existence of spurious statistical correlations may make causal discovery models to make mistakes, e.g., wrong directions of edges in the causal graph~\\cite{arjovsky2019invariant}. Secondly, the imbalanced distribution of different sensitive subgroups will make the learned causal model leaning toward majority sensitive subgroups rather than the minority sensitive subgroups, which will make the predictor based on such causal model biased. \n\n\n\n\n\nHere, the toy example in Fig.~\\ref{fig:example1} intuitively explains two scenarios with incorrect causal models.\n%\nFig.~\\ref{fig:example1}(a) shows an example of a true causal model (often determined by domain experts) in which we aim to predict the salary (prediction target $Y$) of people in different races (described by the sensitive attribute $S$). We assume that the level of education (observed feature $X_1$) of each person is a cause, and the salary also influences the type of car each person would like to purchase (observed feature $X_2$). Unobserved variables $U$ (e.g., geographic location) could also have a causal effect on the observed variables. \n%\nTo learn a counterfactually fair predictor, most existing works \\cite{kusner2017counterfactual,russell2017worlds} utilize a given causal model, and only use those variables which are not causally influenced by the sensitive attribute (i.e., non-descendants of $S$) for prediction. \n%\nWe now consider two cases when the given causal model is incorrect: \n1) Consider an incorrect causal model $\\mathcal{M}_1$ in Fig.~\\ref{fig:example1}(b), where the direction of the causal relation $Y\\rightarrow X_2$ is reversed (highlighted in red). Note that $X_2$ is causally influenced by $S$ in the true causal model $\\mathcal{M}$. If a predictor is based on $\\mathcal{M}_1$, $X_2$ would be directly used in prediction, and thus it violates counterfactual fairness with biases from the sensitive attribute. % due to many factors such a can be in causal discovery is quite \n%where $X_2$ (e.g., purchased car) does not causally affect $Y$, but it has statistical correlation with $Y$ (in this example, it is because that $X_2$ is $Y$'s descendant). Such correlation is often referred as to a \\textit{spurious correlation}, \n%which could often lead to an incorrect causal model $\\mathcal{M}_1$ by causal discovery~\\cite{spirtes2000causation,pearl2009causality,kalisch2007estimating,le2016fast}. \n%The intervention on $X_2$ would not causally affect $Y$, but if the causal graph learned by a causal discovery algorithm incorrectly takes $X_2$ as the causal variable for $Y$, \n%Then the predictor led by the incorrect causal model $\\mathcal{M}_1$ will be biased, and may further result in unfair predictions.\n%\n2) Consider another incorrect causal model $M_2$, where an existing causal relation $S\\rightarrow X_1$ in the true causal model $\\mathcal{M}$ is ignored. Predictors based on $M_2$ would directly use $X_1$ in prediction, which results in biases. Unfortunately, causal models are quite common to be incorrectly assumed or discovered  \\cite{spirtes2000causation,pearl2009causality,kalisch2007estimating,le2016fast}.\n%Assume the data contains $90\\%$ people in race A and $10\\%$ people in race B. As the data is very imbalanced regarding the sensitive attribute, the causal model fitted on the observed data by causal discovery could be leaning toward the majority sensitive subgroup $A$ rather than the minority sensitive subgroup $B$, resulting in an incorrect causal model, for example, as $M_2$ shown in Fig.~\\ref{fig:example1}(c). Then when it comes to the counterfactual question ``what would the salary be if a person had been in race B?\", the incorrect causal model $M_2$ will ignore the influence from $S$ to $X_1$ and directly make use of $X_1$ for prediction. As $X_1$ is causally influenced by $S$ in the true causal model $\\mathcal{M}$, this incorrect causal model $M_2$ will lead to biased prediction for this counterfactual question. We defer the detailed analysis of these two scenarios to Section 2.\n\n% and the ground-truth causal model in these two races are shown in Fig.~\\ref{fig:example1}(c) and (d)\n\n\n% well aligned with the ground-truth causal model of people in race A while being quite different from the ground-truth causal model of people in race B. Then when it comes to the counterfactual question ``what would the salary be if a person had been in race B?\", the learned causal model may ignore the influence from $S$ to $X_1$ and directly make use of $X_1$ for prediction. As $X_1$ is causally influenced by $S$ in race B, this learned causal model will lead to biased prediction for this counterfactual question.\n%Consider the data contains $80\\%$ white people and $20\\%$ black people, if the causal model fitted on the  data is assumed to be complete and consistent in all the races, then when we use the predictor based on such assumption to make predictions, the fitted causal model may be very close to the ground-truth causal model of white people, which leads to unfair prediction for the black people. Another case is if the prior knowledge of the causal model is  given by a domain expert who is only familiar with the case of the black people, but the prior knowledge might not be applicable for the white people, it will also bring bias to the prediction for the white people.\n%Consider the training data contains people from two different races: white and black. If the causal model fitted on the training data is assumed to be complete and consistent in all the races, then when we use the predictor based on such causal model to make predictions for an unseen race such as Hispanic, it may not be generalized well due to the distribution shift among different races (e.g., \n%for Hispanic people, the causal relation between people's income and the car they purchased might be very different from the white and black people). Another case is if the training data contains $80\\%$ white people and $20\\%$ black people, and the test data contains $80\\%$ black people and $20\\%$ white people, the discrepancy of data distribution may also make it hard to learn a completely correct causal model and build a fair predictor based on it.\n\n%Existing counterfactual fairness methods \\cite{kusner2017counterfactual} would fit a consistent causal model for these different races, the fair predictors based on such causal model may not be generalized to unseen races, i.e.,\n%it limits the generalization of predictor's fairness. \n%lead to different levels of fairness. This implies that there does not exist a predictor that can generalize to all the sensitive groups.\n\n%Figure \\ref{fig:example1} illustrates an example causal model of our problem, where $S$ denotes the sensitive attribute (e.g., race). In most settings of fairness [], we assume that $S$ may have direct causal effect on all the variables except the target label $Y$ (e.g., salary). We let $X_1$ denote the observed causal features of $Y$ (e.g., level of education). Unobserved variables $U$ may exist in the causal graph, which could also have causal effect on $Y$ (e.g., ability).\n%\n%$X_2$ represents the observed features which do not have causal effect on $Y$, but may have statistical correlations with $Y$ because it is a descendant of $Y$, or due to the existence of confounders, which are variables influence both the variable and $Y$.\n%\n\n% example: incorrect graph: (missing nodes/edges, additional edges: statistical->causal due to confounder/outcome of Y/collider conditioned on x) lead to unfairness\n%On the other hand, the causal model in different sensitive subgroups might be different,  which limits the generalization ability of the fair predictor across different sensitive subgroups. In particular, as the training data of the predictor is often incomplete or imbalanced, it makes it difficult to catch the information for the minority or unseen subgroups to make fair prediction. % example: when there is some domain shift among sensitive groups, what will it cause to fair prediction.\n\n% Challenge: 1) spurious, generalization 2) unobserved confounder 3) exclude the influence of sensitive attributes\n% Invariant model\nTo address the aforementioned issues of insufficient human knowledge of causal model, we study a novel problem of \\textit{learning counterfactually fair predictor with unknown causal models}. %aiming to make predictions based on the counterfactually fair information in different sensitive subgroups without constructing any causal model. \n% \nAlthough it is in principle impossible to achieve counterfactual fairness without any causal model \\cite{kusner2017counterfactual}, we take initial explorations to mitigate the unfairness based on certain general assumptions, and circumvent the prerequisite of explicit prior knowledge. \n%through capturing representation of the counterfactually fair information from data in different sensitive subgroups. \n%\n%Most of the existing counterfactual fairness methods are based on a given causal model, which provides important prior knowledge to model the generation of discrimination, it is difficult to make prediction or counterfactual fairness related analysis without the causal model. \n% no causal model -> hard to exclude the unfair information\n% spurious -> IRM, sensitive -> MMD, unobserved -> proxy, generalization -> IRM&MMD\nHowever, this studied problem remains a daunting task mainly due to the following challenges: \n% 1. remove descendant without causal model\n1) In order to achieve counterfactual fairness, the causal effect from the sensitive attribute $S$ to the prediction must be removed \\cite{kusner2017counterfactual,russell2017worlds}, but an unknown causal model brings challenges to track the influence of the sensitive attribute and eliminate the biases; \n% 2. unobserved variables\n2) There might exist unobserved variables which can be used to predict the target (e.g., ``geographic location\" in the salary prediction example), \n%There might exist unobserved variables which can serve as causal explanation for the target (e.g., ``geographic location\" in the salary prediction example), \nbut without a correct causal model, it is harder to capture these unobserved variables for prediction due to the lack of prior knowledge regarding these variables. \n% 3. potential biases (e.g., spurious correlations)\n3) %As the two examples shown in Fig.~\\ref{fig:example1}, \nMany factors (e.g.,  failure in obtaining correct causal relations) %(such as spurious correlations and the imbalanced data) \nmay lead to unfair predictions, but it is difficult to exclude their influence without a correct causal model. \nIn a nutshell, all of these challenges are essential due to the lack of counterfactual data. \n%4) The imbalanced data distribution regarding sensitive subgroups increases the difficulty of training a fair predictor.\n%4) The training and test data may contain different sensitive subgroups or bear different distributions regarding sensitive subgroups, which increases the difficulty of training a robustly fair predictor.\n\n%1) there may exist some non-causal variables with \\textit{spurious correlations} with the target, that is, these variables are not the causal explanation of the target, but they are statistically correlated, e.g., they are causally affected by a confounder. These spurious correlations often result in incorrect causal model; \n%2) According to real-world observation, the distribution of variables with these spurious correlations are often unstable and shift from different environments \\cite{arjovsky2019invariant}, e.g., different sensitive subgroups, which increases the difficulty of training a robustly fair predictor;  4) \n\n% exclude spurious -> invariant; unobserved causal variable -> utilize causal/spurious features to capture the latent factors; sensitive -> MMD\nTo tackle these challenges, we propose a novel framework --- \\textit{ \\textbf{C}ounterfactua\\textbf{L}ly f\\textbf{AI}r and invariant p\\textbf{RE}dictor} \\textit{(\\mymodel)}, which learns counterfactually fair representations for target prediction. \n%\n%This framework is based on some general assumptions regarding the causal model, which can be widely used in many real-world scenarios. \n%Generally speaking, we reduce the prior knowledge of causal model based on some general assumptions and simplification\nTo remove the biases from sensitive attributes without any given causal model (challenge 1), we develop a counterfactual data augmentation module to implicitly capture the causal relations in data, and generate counterfactuals for each individual with different sensitive attribute values. \n% fair\n%Generally speaking, we utilize a generative neural network model for counterfactual data augmentation to avoid using explicit prior knowledge of the causal model. \n%Specifically, in the counterfactual data augmentation module, we use a variational auto-encoder (VAE) based network to encode the original data into latent embeddings and reconstruct the input data with a decoder. \nIn this way, \\mymodel~ can learn fair representations by using a counterfactual fairness constraint to minimize the difference between the predictions made on the original data and on its counterfactuals.\n% unobserved\nTo capture the unobserved variables which can help counterfactually fair prediction (challenge 2), \n\\mymodel~ maps the observed variables to a latent representation space to encode the unobserved variables that can facilitate the prediction. The aforementioned counterfactual fairness constraint can preserve those unobserved variables which are not biased. \n%\\mymodel~ utilizes the observed variables to make the learned representations encode the unobserved factors that causally affect the prediction target. \n%To remove the effect of the sensitive attributes in the latent embeddings,\n%To remove the effect of the sensitive attributes on the learned representation, \n%we leverage two different techniques---Maximum Mean Discrepancy (MMD) based distribution matching and adversarial learning, and further render two different implementations of the counterfactual data augmentation module.\n% instantiate \\mymodel~ with two different implementations, including a Maximum Mean Discrepancy (MMD) \\cite{long2015learning,shalit2017estimating} matching method and an adversarial learning method. \n%\n%\nTo further reduce the factors which potentially impede counterfactual fairness (challenge 3), we exclude the variables with \\textit{spurious correlations} to the target (i.e., variables that appear to be causal to the target but are not, e.g., $X_2$ in Fig.~\\ref{fig:example1}(a)) from the learned representations. Spurious correlations can easily lead to incorrect causal models. Besides, removing these variables can often benefit model prediction performance, as shown in \\cite{arjovsky2019invariant}. %Considering this, \\mymodel~ encourages the representations to capture an \\textit{invariant} relationship with the target. Invariant relationship means the relationship has a stable property across all different sensitive subgroups. \n%In this way, we can exclude the influence of the spurious correlations because they often vary across different sensitive subgroups, as discussed in \\cite{arjovsky2019invariant,chang2020invariant}. \n%As for the imbalanced data issue, on the one hand, the invariant representation can exclude some inconsistent information among different sensitive subgroups, on the other hand, when we eliminate the influence from the sensitive attribute, the biases caused by their imbalanced distribution can also be mitigated.\n%Furthermore, the invariant representation and the techniques to remove the effect of sensitive attributes in \\mymodel~ can also tackle the issues caused by the domain shift problem between training data and test data.\n%The proposed model \\mymodel~ explores the causal variables from the original dataset which exhibits an \\textit{invariant} relationship with the target, which means it maintains a stable property across all different sensitive subgroups. \n%In this way, the spurious correlation can be excluded by utilizing its instability. \\mymodel~ also utilizes all the observed features to learn a representation which include the latent factors which are target-causative. In addition, we use two different methods, including a Maximum Mean Discrepancy (MMD) [] matching technique and an adversarial learning method to exclude the influence of the sensitive attributes. \nWe summarize our main contributions as follows: \n\\begin{itemize}\n    \\item  \\textbf{Problem:} We study an important problem of learning counterfactually fair predictor from observational data. We analyze its importance, challenges, and impacts. %, and discuss the impact of incorrect causal models.\n\\item  \\textbf{Algorithm:} We propose a novel framework \\mymodel~ for this problem. Specifically, we learn  fair representations based on counterfactual data augmentation. Besides, we exclude spurious correlations to further reduce potential biases. \n\\item  \\textbf{Experiments:} We conduct extensive experiments to evaluate our framework on synthetic and real-world datasets. The results show that \\mymodel~ outperforms the existing baselines.\n\\end{itemize}\n\n\n\n\n\n%\\vspace{-2mm}\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n%In this section, we will formally describe the used notations, provide preliminaries of counterfactual fairness, and present the limitations of existing methods of counterfactually fair predictions. %At last, we introduce some case studies to illustrate what information is needed for counterfactually fair prediction. \n\n%\\vspace{-2mm}\n",
                "subsection 2.1": {
                    "name": "Notations",
                    "content": "\nIn this paper, we use upper-cased letters, e.g., $X$, to denote random variables, lower-cased letters, e.g., $x$, to denote specific values.\n%, and calligraphic uppercase, e.g., $\\mathcal{X}$, to denote the spaces. \n$P(X)$ refers to the probabilistic function of $X$. We use $X$, $S$, $U$, $Y$ to represent the observed non-sensitive features/attributes, sensitive attribute, unobserved variables, prediction label/target for any instance, respectively. \n%The observed dataset is denoted by $D=\\{X,S,Y\\}$. \n%Specifically, we use the superscript $D^s=\\{X^s,S^s,Y^s\\}$ to denote the corresponding data of a specific sensitive subgroup $s$ regarding the sensitive attribute $S$. \nSpecifically, we use $X^s,Y^s$ to denote the corresponding features and target of any instance with the observation of a specific sensitive attribute value $S=s$, where $s\\in \\mathcal{S}$, and $\\mathcal{S}$ is the space of the sensitive attribute value. \n$\\hat{Y}$ denotes the predicted label (for classification tasks) or target (for regression tasks).\n%Specifically, each sensitive subgroup $s\\in \\mathcal{S}$ of $n_s$ instances is denoted with a superscript $D^s=\\{X^s,S^s,Y^s\\}$. \n\n%\\vspace{-2mm}\n"
                },
                "subsection 2.2": {
                    "name": "Counterfactual Fairness",
                    "content": "\nCounterfactual fairness \\cite{kusner2017counterfactual} is an individual-level fairness notion based on the causal mechanism. \n% introduction of causal framework, do-calculus\nIt is built upon the Pearl's causal framework \\cite{pearl2009causal}, which is defined as a triple $(U, V, F)$ such that: \n\\begin{itemize}\n    \\item $U$ is the set of latent variables, which are often assumed to be exogenous and consequently independent of each other;\n    \\item $V$ is a set of observed variables, which are endogenous and determined by variables in $U\\union V$;\n    \\item $F=\\{f_1(\\cdot),f_2(\\cdot),...,f_{|V|}(\\cdot)\\}$ is a set of functions (referred to as \\textit{structural equations}) which describe the causal relationships among the above variables. For each variable $V_i\\in V$, $ V_i = f_i(pa_i,U_{pa_i})$, where ``$pa_i\\subseteq V \\setminus{V_i}$” and ``$U_{pa_i}\\subseteq U$\" are variables that directly determine $V_i$. \n\\end{itemize}\nA causal model is associated with a \\textit{causal graph}, which is a directed acyclic graph (DAG). Each node in the causal graph corresponds to a variable in the causal model, and each directed edge represents a causal relationship. For example, for observed variables $A, B$, the value of the \\textit{counterfactual} \"what would $A$ have been if $B$ had been set to $b$?\" is denoted by $A_{B\\leftarrow b}$. %The counterfactual is obtained by \\textit{intervention}. For any observed variable $V_i$, an \\textit{intervention} on it is conducted by a \\textit{do-calculus} operation $do(V_i=v)$ \\cite{pearl2009causality}, which replaces the structural equation $V_i = f_i(pa_i, U_{pa_i} )$ with the equation $V_i = v$ for a specific value $v$, then reruns the modified data-generation process where the value of variable $V_i$ is set to $v$. \n\n% counterfactual fairness\nBased on a given causal model, a predictor uses a function $\\hat{Y}=f(X,S)$ to make the prediction for each instance. The predictor is \\textit{counterfactually fair} \\cite{kusner2017counterfactual} if under any context $X=x$ and $S=s$, \n\\begin{equation}\nP(\\hat{Y}_{S\\leftarrow s}=y|X=x,S=s) = P(\\hat{Y}_{S\\leftarrow s'}=y|X=x,S=s),\n\\end{equation}\nfor all $y$ and $s'\\ne s$. Here $\\hat{Y}_{S\\leftarrow s}=f(X_{S\\leftarrow s},s)$ denotes the prediction made on the counterfactuals when the value of $S$ had been set to $s$.%intervention $do(S=s)$. \n\n\n% example: incorrect graph: (missing nodes/edges, additional edges: statistical->causal due to confounder/outcome of Y/collider conditioned on x) lead to unfairness\n\n% example: when there is some domain shift among sensitive groups, what will it cause to fair prediction.\n\n%\\subsection{Limitations of Existing Works} \n%\\vspace{-2mm}\n"
                },
                "subsection 2.3": {
                    "name": "Biases under Incorrect Causal Models",
                    "content": "\nTo achieve the notion of counterfactual fairness, existing works often~\\cite{kusner2017counterfactual,russell2017worlds} follow a two-step process: 1) First, they use the observed data to fit the causal model and infer the posterior distribution $P(U|X,S)$ of unobserved variables $U$; 2) Second, they train a counterfactually fair predictor based on the fitted causal model. In particular, this step can be achieved in different ways: an initial work \\cite{kusner2017counterfactual} trains the predictor with only unobserved variables $U$ and the non-descendants of $S$ as input. We refer to this method as \\cfpa. Another work \\cite{russell2017worlds} considers a counterfactual fairness objective $|f(X_{S\\leftarrow s},s)-f(X_{S\\leftarrow s'},s')|$ for each instance, aiming to minimize the difference between the predictions made on different counterfactuals of the sensitive attribute. We refer to this method as \\cfpb. In this subsection, we use some simple examples to show the biases in the prediction of these existing counterfactual fairness methods when the given causal model is incorrect.\n\n% example 1. get correct causal model is hard -> the bad consequence (acc and fairness) when the causal model is wrong (graph or equation) -> we can catch enough information by using different environments.\n\\noindent\\textbf{Example 1. } First, we consider the case when the counterfactual fairness methods have been given an incorrect causal model as shown in Fig.~\\ref{fig:example1}(b).  %regarding variables having spurious correlations to the target. % due to spurious correlations. \nIn the aforementioned salary prediction example,\n%shown in Fig. \\ref{fig:example1}, we aim to predict the salary (label $Y$) of different people in multiple races (sensitive attribute $S$). Other observed features include the level of education ($X_1$), which has a causal effect on $Y$, and the cars that the people own ($X_2$). \nthe ground truth causal model $\\mathcal{M}$ is shown in Fig. \\ref{fig:example1}(a). It indicates that people's salary can causally influence their choices of cars to purchase. % which may also be affected by people's races. \nIn this example, we let the causal model $\\mathcal{M}$ be as follows:\n$$\nP(S=1)=0.5, P(S=0)=0.5, \\epsilon_1,\\epsilon_y,\\epsilon_2 \\sim \\mathcal{N}(0,1),\n$$\n$$\nX_1 \\leftarrow S + U + \\epsilon_1, Y \\leftarrow X_1 + \\epsilon_y, X_2 \\leftarrow Y + \\epsilon_2.\n$$\n$X_2$ is correlated with $Y$ because it is $Y$'s child node, but this correlation may lead the model to incorrectly take $X_2$ as one of $Y$'s parent nodes, as the incorrect causal model $\\mathcal{M}_1$ shown in Fig. \\ref{fig:example1}(b). Then the goal of counterfactual fairness:\n$\nP(\\hat{Y}_{S\\leftarrow s}^{\\mathcal{M}_1}|X=x,S=s) = P(\\hat{Y}_{S\\leftarrow s'}^{\\mathcal{M}_1}|X=x,S=s)\n$ defined on $\\mathcal{M}_1$ is different from what is defined on the true causal model $\\mathcal{M}$.\n%\nBased on the incorrect causal model $\\mathcal{M}_1$, \\cfpa~ will take $X_2$ as an input to the predictor, but $X_2$ contains biased information because it is actually a descendant of the sensitive attribute, thus it will bring bias into prediction. For \\cfpb, if we assume a linear predictor $\\hat{Y}=W_1 X_1 + W_2 X_2 + W_S S$, then the fairness penalty on the incorrect causal model would be:\n$$\n\\mathbb{E}(|f(X^{\\mathcal{M}_1}_{S\\leftarrow 1},1)-f(X^{\\mathcal{M}_1}_{S\\leftarrow 0},0)|) = |W_1+W_S|,\n$$\nwhile the fairness penalty based on the true causal model would be:\n$$\n\\mathbb{E}(|f(X^{\\mathcal{M}}_{S\\leftarrow 1},1)-f(X^{\\mathcal{M}}_{S\\leftarrow 0},0)|) = |W_1+W_2+W_S|.\n$$\nSuch difference can lead to inappropriate learning results for the parameters in the predictor. As the fairness penalty based on the incorrect causal model has no constraint on $W_2$, the predictor can not exclude the biases contained in $X_2$.\n%Beyond this simple example, a general conclusion is that many factors may lead to incorrect prior knowledge of the causal model, and as a result, it can bring biases to both the model prediction and counterfactual fairness.\n\n% example 2. inconsistent causal model leads to bad consequeces (acc and fairness) -> we can solve it by using different environments\n%\\noindent\\textbf{Domain shift across different sensitive subgroups.}\n\\noindent\\textbf{Example 2. } % Imbalanced Data\n%We now consider another case when the causal model is fitted in an imbalanced dataset. \nWe now consider another case of incorrect causal model shown in Fig.~\\ref{fig:example1}(c). \nIn the salary prediction example, consider that the dataset contains a majority\nsensitive subgroup $S=0$ (e.g., race A) and a minority sensitive subgroup $S=1$ (e.g., race B). The ground-truth causal model is assumed to be as below:%on these two subgroups are as shown in Fig.~\\ref{fig:example1}(c) and (d), which can be summarized in a complete model:\n$$\nP(S=1)=0.1, P(S=0)=0.9,\\epsilon_1,\\epsilon_y,\\epsilon_2 \\sim \\mathcal{N}(0,1),\n$$\n$$\nX_1 \\leftarrow S + U + \\epsilon_1, Y \\leftarrow X_1 + \\epsilon_y, X_2 \\leftarrow Y + \\epsilon_2.\n$$\nAs the subgroup $S=1$ is underrepresented, the fitted causal model may miss the causal relation $S\\rightarrow X_1$ for $S=1$, i.e., the fitted causal model is biased (as the causal model $\\mathcal{M}_2$ shown in Fig.~\\ref{fig:example1}(c)). Then for \\cfpa, $X_1$ and $X_2$ will be taken as input for prediction because they are considered to be non-descendants of $S$, but as $X_1$ and $X_2$ are actually biased because they are descendants of $S$, the predictor will also be biased consequently. Let us take the predictor $\\hat{Y}=X_1$ for example. The predictor makes prediction $\\hat{Y}_{S\\leftarrow 0}=X_{1,S\\leftarrow 0} = U+\\epsilon_1$ and $\\hat{Y}_{S\\leftarrow 1}=X_{1,S\\leftarrow 1} = U+\\epsilon_1+1$ in when  $S\\leftarrow 0$ and $S\\leftarrow 1$, respectively, and this is obviously not counterfactually fair. For \\cfpb, the fairness penalty on this biased causal model $\\mathcal{M}_2$ is:\n$$\n\\mathbb{E}(|f(X^{\\mathcal{M}_2}_{S\\leftarrow 1},1)-f(X^{\\mathcal{M}_2}_{S\\leftarrow 0},0)|) = |W_S|,\n$$\nwhile the fairness penalty based on the true causal model $\\mathcal{M}$ is:\n$$\n\\mathbb{E}(|f(X^{\\mathcal{M}}_{S\\leftarrow 1},1)-f(X^{\\mathcal{M}}_{S\\leftarrow 0},0)|) = |W_1+W_2+W_S|.\n$$\nSuch difference may lead to inappropriate use of $X_1$ and $X_2$, and thus  bring biases to the predictor.\n% Furthermore, we observe that the data distribution often shift among different sensitive subgroups, e.g., people's preferences of cars may also be influenced by their races, which cause $P(Y|X_2)$ and $P(X_2^{M_s})$ to vary across different sensitive subgroups, which leads to unstable performance in prediction and fairness. \n% For example, if we assume that when $S=s$ and $S=s'$: \n% $$\n% X_2 = \\beta Y + \\epsilon_2,\n% $$\n% where $\\beta$ is a parameter which may vary across different sensitive subgroups. Then the fairness penalty would be:\n% \\begin{equation}\n% \\begin{split}\n%     &\\mathbb{E}[|f(X^{M_s}_{S\\leftarrow s},s)-f(X^{M_{s'}}_{S\\leftarrow s'},s')|\\,|x,s]\\\\\n%     \\,=\\,\\,& \\mathbb{E}[|W_s(s\\!-\\!s') \\!+\\! W_1(X_{1,S\\leftarrow s}^{M_s}\\!\\!-\\!X_{1,S\\leftarrow s'}^{M_{s'}})\\!+\\!W_2(X_{2,S\\leftarrow s}^{M_s}\\!\\!-\\!X_{2,S\\leftarrow s'}^{M_{s'}})|\\,|x,s]\\\\\n%     \\,=\\,\\,& \\mathbb{E}[|W_s(s-s') + W_1(s-s') + \\beta W_2(s-s')|\\,|x,s]\\\\\n%     \\,=\\,\\,& \\mathbb{E}[|(W_s+W_1+\\beta W_2)(s-s')| \\,|x,s].\n%     % \\,=\\,\\,& \\mathbb{E}_q[\\log p(\\bm{a}|\\bm{z},\\bm{t},\\bm{c})] - \\mathbb{E}_{q(\\bm{t}|\\bm{a})}KL(q(\\bm{c}|\\bm{t})||p(\\bm{c})) \\\\\n%     % -\\,\\,& \\mathbb{E}_{q{(\\bm{c}|\\bm{t})}}KL(q(\\bm{t}|\\bm{a})||p(\\bm{t}|\\bm{c}))\\\\ \n%     % -\\,\\,& \\sum\\nolimits_{k=1}^K \\mathbb{E}_{q(\\bm{t}|\\bm{a})q(\\bm{c}|\\bm{t})} KL(q(\\bm{z}^{(k)}|\\bm{a})||p(\\bm{z}^{(k)})).\n% \\end{split}\n% \\end{equation}\n% If the model is trained on sensitive subgroups $s$ and $s'$, to minimize the fairness penalty, the value of $W_s+W_1+\\beta W_2$ is encouraged to be close to $0$. However, if we have $\\beta=\\beta'$ in a new sensitive subgroup, then the fairness penalty can not be stable in the new sensitive subgroup. Previous work \\cite{russell2017worlds} has mentioned that different causal models induce different constraints to achieve counterfactual fairness, and only constant predictors (the predictors which always output a constant for any instance) can satisfy all the constraints. \n%\n\nAs a summary, existing counterfactual fairness machine learning methods heavily rely on given causal models, and would result in biases when the given causal models are incorrect.\n%above limitations of existing counterfactual fairness methods motivate us to develop a novel way to avoid the issues caused by the incorrect causal models.\n\n%to learn \\textit{invariant} latent factors across different sensitive subgroups to 1) remove the potential bias caused by incorrect causal models; 2) increase the generalization of the predictor in different sensitive subgroups.\n\n\n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "The Proposed Framework",
                "content": "\nIn this section, we introduce the proposed framework \\mymodel, which targets at achieving counterfactual fairness without relying on explicit prior knowledge about the causal model. \n%\nTo achieve this goal, \\mymodel~ learns counterfactually fair representations with counterfactual data augmentation, and then makes predictions based on the learned representations. %The framework is based on some simple and general assumptions. \n%\n% Concretely, \\mymodel~ pretrains a counterfactual data augmentation module which generates counterfactuals with different values of sensitive attribute for each instance. %In the counterfactual data augmentation module, we employ a variational auto-encoder to encode the original data into a latent embedding space, and reconstruct the data based on the embeddings. \n% %\n% %To generate counterfactuals based on these embeddings, we remove the influence of the sensitive attribute on the embeddings to capture the ``fair\" generative factors. %Two different implementations are used, including distribution matching and adversarial learning.\n% %\n% With the generated counterfactuals, \\mymodel~ learns  representations to achieve counterfactual fairness by mitigating the difference between representations learned from original data and corresponding counterfactuals. %More specifically, we minimize the difference between predictions made with the representations learned from original data and from their corresponding counterfactuals. \n% %\n% To further reduce the potential biases, \\mymodel~ captures the invariant information across different sensitive subgroups to exclude those potential biases related to variables with unstable spurious correlations to the target. \n%To further reduce the potential biases, \\mymodel~ captures the invariant information across different sensitive subgroups which exhibits a stable relationship to the target. In this way, we preserve those information which is indeed causative to the target, and exclude those potential biases related to variables with unstable spurious correlations to the target. \n%\n%Furthermore, \\mymodel~ learns the representations by: 1) capturing the invariant information across different sensitive subgroups which exhibits a stable relationship to the target, in this way, we preserve those information which is indeed causative to the target, and exclude those potential bias caused by the unstable spurious correlations; 2) removing the influence of the sensitive attribute on the learned representations by two different implementations: distribution matching and adversarial learning. During the process, the learned representations can capture the unobserved factors which are causative to the target and not influenced by the sensitive attribute. \n%\n%Furthermore, as the invariant representations exclude the unstable relationships that vary across different sensitive subgroups, and we also remove the influence of the sensitive attribute, \n%we can capture the information which are essentially causative and fair across different sensitive subgroups, \n%the underrepresented sensitive subgroups can thus suffer less from the issues of biased causal model resulted from the imbalanced data.\n \n%Furthermore, the invariance of the representation can enhance the generalization of the predictor in different data distributions. \n%\\mymodel~ learns an invariant predictor across different sensitive subgroups, aiming to capture the causal variables with respect to the prediction target. \n% In existing counterfactual fairness methods, it requires much prior knowledge about the causal graph (at least, it should be known beforehand that which observed variables are not the descendants of the sensitive attributes). As we can use an invariant model to identify the causal variables which causally influence the label, the assumption of the causal graph can be much more general. \n\n\n\n%\\vspace{-2mm}\n",
                "subsection 3.1": {
                    "name": "Assumptions and Examples",
                    "content": "\n%\\vspace{-1mm}\nBefore technical details, we first present the key concepts and assumptions of \\mymodel, and then use general examples of causal models (Fig.~\\ref{fig:casestudy}) to describe the information needed in \\mymodel. \n\nPrevious works of counterfactual fairness \\cite{kusner2017counterfactual} have discussed three levels of required prior knowledge about the causal model: 1) Level 1 only requires to know which observed features are non-descendants of the sensitive attribute, and only uses them for prediction; \n2) Level 2 postulates and infers the unobserved variables with partial prior knowledge of the causal model, and also uses them for prediction; \n3) Level 3 makes assumptions on the causal model (e.g., additive noise model \\cite{hoyer2008nonlinear}), postulates the complete causal model, and \nthen uses the inferred unobserved/observed non-descendants of the sensitive attribute for prediction. \n%then builds a fair predictor based on the inferred unobserved/observed non-descendants of the sensitive attribute. \nThese three levels make increasingly stronger assumptions on the underlying causal model. But even the first level still requires to figure out which variables are non-descendants of the sensitive attribute. In this work, we aim to propose a principled way for counterfactually fair prediction without relying on the prior knowledge of the causal model. The main assumptions in our framework are listed as follows:\n%\n\n%\\begin{enumerate}\n    %\\item There exist causal variables which are not the descendants of the sensitive attributes.\n% no parent\n%\\vspace{-2mm}\n\\begin{assumption} The sensitive attribute is not causally influenced by any other variables. This is a common assumption in most of existing fairness works \\cite{kusner2017counterfactual,russell2017worlds,berk2018fairness}, as the commonly-used sensitive attributes such as race and gender usually do not have any causes.\n\\end{assumption}\n    % \\item With the values of the variables not in $\\{S,des(S)\\}$ fixed, for the descendants of sensitive attribute $des(S)$, we have \n    % \\begin{equation}\n    %     P(des(S)|S=s) \\ne  P(des(S)|S=s'),\n    % \\end{equation}\n    % for the values of the sensitive attribute $s\\ne s'$, and for any value $s$:\n    % \\begin{equation}\n    %     P(des(S)|S=s) = P(des(S)_{S\\leftarrow s}|S=s)\n    % \\end{equation}\n    % %(consistent, no selection bias(collider/confounder))\n    % \\item For the non-descendants of sensitive attributes $ndes(S)$, we assume their distributions are the same across different sensitive subgroups: \n    % \\begin{equation}\n    %     P(ndes(S)|S=s) =  P(ndes(S)|S=s').\n    % \\end{equation}\n    % for any sensitive attribute $s\\ne s'$, and for any $s$:\n    % \\begin{equation}\n    %     P(ndes(S)|S=s) = P(ndes(S)_{S\\leftarrow s}|S=s)\n    % \\end{equation}\n% invariant\n%\\vspace{-2mm}\n\\begin{assumption} If a variable $X_c$ directly affects $Y$ (i.e., an edge $X_c\\rightarrow Y$ exists in the causal model), we assume $P(Y|X_c)$ is stable across different sensitive subgroups, but for the variables $X_s$ which do not causally affect $Y$, $P(Y|X_s)$ may be unstable in different sensitive subgroups. This assumption and its variants are widely used in invariant  learning \\cite{arjovsky2019invariant,ahuja2020invariant}. \n\\end{assumption}\n%\\end{enumerate}\n%\n%\\subsection{Case Studies}\n% three method of CF: all require prior knowledge of causal model -> consider different cases\n\n\n\nAs the ground truth causal model can be complicated, to investigate more general settings, we consider several different types of variables in the causal model, including descendant and non-descendant variables of $S$, causal and non-causal variables of $Y$, and observed and unobserved variables. Here we conduct several case studies on the causal model, and each corresponds to a causal graph shown in Fig.~\\ref{fig:casestudy}. %where each arrow refers to a causal relationship and each dashed arrow denotes a possible relationship between two variables. \nSuppose there is a ground truth causal model $\\mathcal{M}$, we call the variables in $\\mathcal{M}$ which causally affect the prediction target $Y$ (i.e., $Y$ is the descendant of such variables) as \\textit{causal variables} of $Y$. \n%\nIn all the causal models in Fig.~\\ref{fig:casestudy}, $X_1$ is a causal variable of $Y$, but it is also a descendant of $S$, thus it can not be directly used for counterfactually fair prediction. As shown in Fig.~\\ref{fig:casestudy}(b) and (c), $X_0$ is also a causal variable of $Y$, and is non-descendant of $S$, thus $X_0$ is supposed to be used for fair prediction. \n%\n$X_2$ is not a causal variable of $Y$, but it has statistically spurious correlations to $Y$. The reason may be that $X_2$ is $Y$'s descendant, as shown in Fig.~\\ref{fig:casestudy}(b), or $X_2$ and $Y$ are affected by some common variables, as shown in Fig.~\\ref{fig:casestudy}(c). As discussed in \\cite{arjovsky2019invariant,chang2020invariant}, the spurious correlations between $X_2$ and $Y$ often vary across different sensitive subgroups and thus degrade the model prediction performance. Besides, if these non-causal variables are also descendants of sensitive attribute, incorporating them into prediction would also impede counterfactual fairness. Therefore, in our framework, we exclude these non-causal variables to further avoid potential biases. \nAbove cases are all about observed variables, for those unobserved variables which are causative to $Y$, such as $U$ in Fig.~\\ref{fig:casestudy}(d), we try to better capture these unobserved variables by utilizing the observed variables which have correlations with them.\n%In Fig.~\\ref{fig:casestudy}(a), $S$ and $X_1$ are causal variables to $Y$, but all the variables are descendant of $S$, thus none of them can provide information to construct a fair predictor. In Fig.~\\ref{fig:casestudy}(b), $X_0$ is an observed causal variable of $Y$, and $X_0$ is not the descendant of $S$, thus $X_0$ is supposed to be used for fair prediction. \n%\n%In Fig.~\\ref{fig:casestudy}(c), $X_2$ is not a causal variable to $X_1$, so its statistical relation with $Y$ may vary in different sensitive subgroups, thus we need exclude it to avoid potential incorrect causal model and to achieve better generalization. In Fig. ~\\ref{fig:casestudy}(d), there is an unobserved variable $U$ which needs to be captured for fair prediction. In this case, our model is expected to learn $U$ from the observational data, and we develop a fair predictor based on it. \n\nOverall, in our framework, we \\textit{learn representations to capture the causal variables which are not influenced by the sensitive attribute}.\n\n\n%\\vspace{-3mm}\n"
                },
                "subsection 3.2": {
                    "name": "Overview of \\mymodel~ Framework",
                    "content": "\n% Invariant predictor\nExisting counterfactual fairness works \\cite{kusner2017counterfactual,russell2017worlds} involve counterfactual inference for predictor training, but it is often infeasible in real-world applications due to the lack of a correct causal model, especially when the data is noisy and high-dimensional \\cite{belloni2017program}. \nWithout enough knowledge about the causal model, inferring the unobserved variables and learning a fair predictor can be quite challenging. \nHere, we define the goal of our framework with respect to counterfactual fairness, and show an overview of the methodology. %then describe the detailed methods of fair representation learning in the following subsections.\n\n%Fortunately, the data in different sensitive subgroups can be utilized for further information. Here we have two main goals to achieve: 1) inferring the causal variables; 2) removing the dependency of sensitive attributes. \n\n% Given the data collected from multiple sensitive subgroups, we formulate the task of learning a counterfactually fair predictor as an optimization problem, whose objective is to capture the counterfactually fair variables as below with a fairness constraint:\n% \\begin{equation}\n%     \\min \\mathds{E}[\\mathcal{L}(Y, \\hat{Y})],\\quad \\text{s.t.} \\quad \\hat{Y}=f(X,S), \\quad  \\hat{Y} \\bigCI do(S) | X,S,\n% \\end{equation}\n% where $\\mathcal{L}(\\cdot)$ is a loss function which measures the prediction error, $f(\\cdot)$ is a predictor based on the sensitive feature $S$ and non-sensitive features $X$. The conditional independence constraint ensures that for each individual, an intervention on $S$ will not influence the result of the predictor. %which can be further reinterpreted as below:\n\nBased on the aforementioned preliminaries, we know that the key point of this problem is to capture the information which elicits a fair predictor, such as the causal variables that are non-descendants of $S$. In our framework, we use the observed features to learn a representation $Z=\\Phi(X)$ which captures the fair information, and then build a predictor $\\hat{Y}=g(Z)$ on top of it. \n% Our framework is then based on the following optimization problem:\n% \\begin{equation}\n%     \\min \\mathds{E}[\\mathcal{L}(Y, \\hat{Y})], \\,\\, \\text{s.t.} \\,\\, \\hat{Y}=g(Z),  \\,\\, Z=\\Phi(X,S), \\,\\, Z \\bigCI do(S) | X,S,\n%     \\label{eq:optimization}\n% \\end{equation}\n% where $Z$ is the learned representation, $g(\\cdot)$ is a predictor for $Y$ that is built upon the representation $Z$, $\\Phi(\\cdot)$ is a function which learns representations $Z$ from $X$ and $S$. \n%In the causal model, a variable $W$ which is independent of $S$ must satisfy the following three conditions: 1) $W$ is not a descendant of $S$; 2) $W$ is not an ancestor of $S$; 3) $W$ and $S$ are not simultaneously influenced by a variable. According to the Assumption (1) in Section 3.1, the sensitive attribute $S$ does not have any parent variables, so if the learned representation $Z$ can satisfy $Z\\bigCI S$, it means $Z$ is not influenced by $S$. Then if the predictor is totally based on $Z$, then once given $Z$, the intervention on $S$ will not influence the prediction of $Y$, that satisfies the constraint $\\hat{Y} \\bigCI do(S) | Z$.\nIn the implementation, we learn the representations $Z$ in the following ways: (1) To capture the causal variables of $Y$, we leverage the invariant risk minimization loss \\cite{arjovsky2019invariant} to exclude those non-causal variables with unstable spurious correlations to $Y$. (2) To avoid taking the biases from the sensitive attribute into prediction, we develop a counterfactual data augmentation module, and encourage the learned representation to achieve the following goal: \n% \\begin{equation}\n%     P(Z|S=s, X=x, do(S=s))=P(Z|S=s, X=x, do(S=s')),\n% \\end{equation}\nfor any $s\\ne s'$, and any $x$, \n$\n    P(\\Phi(x_{S\\leftarrow s}))=P(\\Phi(x_{S\\leftarrow s'})).\n$\nIntuitively, it means that for each individual with observed features $x$ and sensitive attribute value $s$, the distributions of the representations learned from its original version and its counterfactuals should be the same.\n\n\\SetKwComment{Comment}{/* }{ */}\n\\begin{algorithm}[t]\n\\caption{The proposed \\mymodel~ framework}\\label{alg:framework}\n\\KwData{Instances of observable variables $\\{X,S,Y\\}$}\n\\KwResult{Counterfactually fair predictor $\\hat{Y}=f(X,S)$}\n/*~1. Counterfactual Data Augmentation~*/ \\\\\nTrain a VAE with encoder $\\Psi(\\cdot)$ and decoder $D(\\cdot)$ with loss function in Eq.~(\\ref{eq: loss_mmd}) (\\mymodela) or Eq.~(\\ref{eq:loss_adversarial}) (\\mymodelb) \\\\\n\\For {each instance of random variables $\\{X,S,Y\\}$}{\nGenerate $K$ samples ${H}^1,...,{H}^K$ with $H = \\Psi(X,Y)$ \\\\\n\\For {$s \\in \\mathcal{S}$}{\n    $X^{CF}_{s}, Y^{CF}_{s} = \\text{\\textsc{Aggregate}}(D({H}^1,s),...,D({H}^K,s))$\n}\n} \n/*~2. Fair representation learning~*/ \\\\\nTrain a model $f=g\\circ\\Phi$ consisting of a representation learner $\\Phi(\\cdot)$ and a predictor $g(\\cdot)$ \\\\\n\\For {each instance of random variables $\\{X,S,Y\\}$}{\n    $Z=\\Phi(X)$, $\\hat{Y}=g(Z)$ \\\\\n    \\For {$s\\in\\mathcal{S}$}{\n    $Z^{CF}_{s}=\\Phi(X^{CF}_{s})$, $\\hat{Y}^{CF}_{s}=g(Z^{CF}_{s})$\n    }\n    Back-propagation with loss function in Eq.~(\\ref{eq:loss_f})\n}\n\\end{algorithm}\n\nAlgorithm 1 shows an overview of our framework, including counterfactual data augmentation and fair representation learning. Detailed techniques will be introduced in the following subsections.\n\n%\\vspace{-2mm}\n"
                },
                "subsection 3.3": {
                    "name": "Counterfactual Data Augmentation",
                    "content": "\n%\\vspace{1mm}\nThe lack of counterfactual data is the essential challenge to achieve counterfactual fairness. Thus, we pretrain a counterfactual data augmentation module to generate counterfactuals for each instance by manipulating its sensitive attribute. Then, the augmented counterfactuals together with original data are utilized to learn fair representations. The counterfactual data augmentation module is based on a variational auto-encoder (VAE) \\cite{kingma2014auto} with an encoder-decoder structure. Specifically, the encoder in the VAE takes $\\{X,Y\\}$ as input, encodes them into a latent embedding space, and then the decoder reconstructs the original data $\\{X,Y\\}$ with the embeddings $H$ (notice that the embedding $H$ is different from the representation $Z$ introduced in the previous subsection. $H$ is the output of the bottleneck layer of the VAE in counterfactual data augmentation to generate counterfactuals) and sensitive attribute $S$. Note that $S$ is only used as an input of the decoder to enable counterfactual generation in later steps. \nThe reconstruction loss $\\mathcal{L}_r$ is:\n\\begin{equation}\n    \\mathcal{L}_r = \\mathbb{E}_{q(H|X,Y)}[-\\log(p(X,Y|H,S))] + \\text{KL}[q(H|X,Y)\\|p(H)],\n\\end{equation}\nwhere $p(H)$ is a prior distribution, e.g., standard normal distribution $\\mathcal{N}(0,I)$. $\\text{KL}[\\cdot\\|\\cdot]$ is the Kullback-Leibler (KL) divergence. \n\nTo generate counterfactuals with the embeddings $H$ and a manipulated  sensitive attribute value later, we need to capture more ``fair\" generative factors (i.e., those generative factors which are not causal influenced by $S$) in the embeddings, i.e., in encoder, we remove the causal influence of the sensitive attribute on the embedding $H$. Based on Assumption 1, if there is no dependency between the embeddings and sensitive attribute, then the embeddings encode no descendants of sensitive attributes. \n%\\subsubsection{Removing the effect of the sensitive attribute}\nNow, we introduce two different implementations to remove the causal effect of $S$ on $H$ by minimizing the dependency between them. These implementations include the distribution matching based \\mymodel~(\\mymodela) and the adversarial learning based \\mymodel~(\\mymodelb). %The details of these two implementations are in the following subsection.\n\n%\\vspace{0.05in}\n\\noindent\\textbf{Distribution matching based \\mymodel.}\n%This implementation approximately achieves the target in Eq.~(\\ref{eq:optimization}). \nTo remove the influence of the sensitive attribute, we  use the distribution matching technique \\cite{shalit2017estimating,long2015learning} on the embeddings for different sensitive subgroups. We refer this implementation as \\mymodela.\n%To remove the effect of the sensitive attribute, the learned representation needs to capture the information of the latent variables which are not influenced by the sensitive attribute. In \\mymodela, we use representation distribution matching among different sensitive subgroups to exclude these dependency. \nIn particular, we minimize the Maximum Mean Discrepancy (MMD) \\cite{long2015learning,shalit2017estimating} among the embedding distributions of different sensitive subgroups. %This is a sensitive subgroup level method to heuristically remove the effect of the sensitive attribute.\n% $P(\\Phi(X)|S=s, X=X_{S\\leftarrow s})=P(\\Phi(X)|S=s, X=X_{S\\leftarrow s'})$?\n% Distribution matching\n\n% \\begin{equation}\n%     P(\\Phi(X,S)|S=s)=P(\\Phi(X,S)|S=s')\n% \\end{equation}\n%     for all $s, s'\\in \\mathcal{S}_{tr}$.\nThe loss function of training the counterfactual data augmentation model  with distribution matching is as below:\n% \\begin{equation}\n%     \\min_{\\Phi,g(.)} \\sum_s\\mathcal{L}_{IRM}^s + \\alpha \\sum_{s\\ne s'}MMD(P(\\Phi(X,S)|s), P(\\Phi(X,S)|s')),\n% \\end{equation}\n\\begin{equation}\n    \\min \\mathcal{L}_r + \\alpha \\frac{1}{N_p}\\sum\\nolimits_{s\\ne s'}MMD(P(H|s), P(H|s')),\n\\label{eq: loss_mmd}\n\\end{equation}\nwhere $N_p=\\frac{|\\mathcal{S}|\\times(|\\mathcal{S}|-1)}{2}$ is the number of pairs of different sensitive attribute values, and $|\\mathcal{S}|$ is the number of different sensitive attribute values.  \nThe second term is the distribution matching penalty, which aims to achieve $P(H|S=s)=P(H|S=s')$ for all pairs of different sensitive subgroups $(s, s')$. Here $\\alpha\\ge0$ is a hyperparameter which controls the importance of the distribution balancing term. \n%It is formulated as:\n% \\begin{equation}\n%   \\mathcal{L}_{dist} = \\sum_{s\\in\\mathcal{S}_{tr}}\\sum_{s'\\in\\mathcal{S}_{tr}\\setminus s} MMD(P(\\Phi(X,S)|s), P(\\Phi(X,S)|s')). \n% \\end{equation}\n%and $\\mathcal{S}_{tr}$ is the set of the sensitive attributes in the training data.\n\n%\\vspace{0.05in}\n\\noindent\\textbf{Adversarial Learning based \\mymodel.}\n%The above distribution matching based framework \\mymodela~tries to match the representations in different sensitive subgroups. To further refine the granularity of dependency removal, \nWe also propose an adversarial learning based implementation, referred as~\\mymodelb. In this implementation, we train a discriminator $h(\\cdot)$ which uses the embeddings to distinguish instances that bear different values of the sensitive attribute. The objective function is  as below:\n\\begin{equation}\n    \\min_{\\Psi(\\cdot)}\\max_{h(\\cdot)}  \\mathcal{L}_{r} + \\alpha'\\frac{1}{|\\mathcal{S}|} \\sum\\nolimits_{s\\in\\mathcal{S}}\\mathbb{E}_{X^s,S^s}[\\log P(h(H)=s)],\n\\label{eq:loss_adversarial}\n\\end{equation}\nwhere $\\Psi(\\cdot)$ is the encoder. The first term is the aforementioned reconstruction loss. The second term calculates the probability that the discriminator makes correct predictions for each instance's sensitive attribute. Therefore, the sensitive attribute predictor $h(\\cdot)$ is playing an adversarial game with the encoder $\\Psi(\\cdot)$. In this way, the embeddings are encouraged to exclude the information related to the sensitive attribute. Here $\\alpha'\\ge0$ is a hyperparameter to control the weight of the sensitive attribute discriminator. The minimax problem is optimized with an alternating gradient descent process.\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Fair Representation Learning",
                    "content": "\n",
                    "subsubsection 3.4.1": {
                        "name": "Counterfactually Fair Representations",
                        "content": " \nWith the counterfactual data augmentation module, we generate counterfactuals by feeding the embeddings $H$ and a sensitive attribute value $s'$ different from the original one $s$ into the decoder ${D}(\\cdot)$, and taking the output $(X^{CF}_{s'},Y^{CF}_{s'})=D(H,s')$ as the counterfactuals corresponding to  $S\\leftarrow s'$. For each instance and each sensitive attribute value, we generate $K$ samples of embeddings $({H}^{1},...,{H}^{K})$, and aggregate the corresponding counterfactuals by an operation $\\textsc{Aggregate}(\\cdot)$ (e.g., mean). For notation simplicity, we still denote the aggregated counterfactual data as $(X^{CF}_{s'},Y^{CF}_{s'})=\\textsc{Aggregate}(D({H}^1,s'),...,D({H}^K,s'))$. Based on these counterfactuals, we train a representation learner $\\Phi(\\cdot)$ which maps instance features $X$ into representations: $Z=\\Phi(X)$, and we use a predictor $g(\\cdot)$ to make predictions based on $Z$. \n\nTo learn counterfactually fair representations $Z$, we add a counterfactual fairness constraint to mitigate the discrepancy between the representations learned from original data and its corresponding counterfactuals. The constraint is formulated as:\n\\begin{equation}\n    \\mathcal{L}_c =\\! \\frac{1}{|\\mathcal{S}|-1}\\sum_{s'\\ne s} d(Z, Z^{CF}_{s'})=\\!\\frac{1}{|\\mathcal{S}|-1}\\sum_{s'\\ne s} d(\\Phi(X), \\Phi(X^{CF}_{s'})),\n\\end{equation}\nwhere $X^{CF}_{s'}$ is the counterfactual generated in counterfactual data augmentation corresponding to $S\\leftarrow s'$, and $d(\\cdot,\\cdot)$ is a distance metric such as cosine distance to measure the discrepancy between two representations.\n\n"
                    },
                    "subsubsection 3.4.2": {
                        "name": "Invariant Representations",
                        "content": " \nAs aforementioned, the non-causal variables which have spurious correlations to the target $Y$ are likely to degrade the model prediction performance, and may also incorporate potential biases from sensitive attributes to prediction.\n%As aforementioned, the variables which have spurious correlations with the target $Y$ often \nIt has been shown in~\\cite{arjovsky2019invariant} that the relationships from these variables to $Y$ often vary across different domains, e.g., different sensitive subgroups.\n%The variables which have spurious correlations with the label are likely to be descents of $S$ once $S$ causally influence any of causal variables, the label or the spurious variables.\n%\nTherefore, to exclude the influence of such non-causal variables on the learned representations and capture the causal variables of $Y$, we leverage the invariant risk minimization (IRM) loss~\\cite{arjovsky2019invariant} for the sensitive subgroup $s$ as below:\n\\begin{equation}\n\\mathcal{L}_{IRM}^s =R^s(g\\circ\\Phi) + \\lambda\\left\\|\\bigtriangledown_{w|w=1.0}R^s(w\\cdot (g\\circ\\Phi))\\right\\|_{2}^2,\n\\label{eq:irm1}\n\\end{equation}\nwhere $\\mathcal{L}^s_{IRM}$ is the IRM loss in the sensitive subgroup $s$, the first term $R^s(g\\circ\\Phi)=\\mathbb{E}[\\mathcal{L}(g(\\Phi(X^s,S^s)),Y^s)]$ is the prediction loss under sensitive subgroup $s$, %$\\Phi(\\cdot)$ is the representation learning function, $g(\\cdot)$ is a predictor based on the representation, \nand $w$ is a scalar and is fixed as $w=1.0$. \nAccording to \\cite{arjovsky2019invariant}, the gradient of $R^s(w\\cdot (g\\circ\\Phi))$ w.r.t. $w$ can reflect the ``invariance\" of the learned representations. \nTherefore, in the above formulation, the second term measures the invariance of the relationship between the representations and the target across different sensitive groups. \nHere, $\\lambda$ is a hyperparameter for the trade-off between the prediction performance and the level of invariance. The IRM loss aims to ensure that the predictor can be optimal in all the different sensitive subgroups, thus the unstable spurious correlations varying across sensitive subgroups can be excluded.\n\n\n%where , , and a predictor with parameter $w$ is based on the representations. \n%$R^s(w\\circ\\Phi)$ is the prediction error in the sensitive subgroup $s$. According to \\cite{arjovsky2019invariant}, here $w$ can be fixed as $w=1.0$ to make a dummy predictor. The first term in the above formulation is the prediction error in sensitive subgroup $s$ with the dummy predictor, and the second term contains the gradient of $w$ when $w=1.0$, which can reflect the optimality of the dummy predictor, and can be considered as the ``invariance\" of the learned representations. \n%And $\\lambda$ is a hyperparameter for the trade-off between the prediction performance and the level of invariance. The IRM loss aims to ensure that the dummy predictor based on the representation can be optimal in all the different sensitive subgroups, thus the unstable spurious correlations can be excluded.\n\nTo put it all together, the overall loss function for fair representation learning is as follows:\n\\begin{equation}\n    \\mathcal{L} = \\frac{1}{|\\mathcal{S}|}\\sum\\nolimits_{s\\in\\mathcal{S}}\\mathcal{L}^s_{IRM} + \\beta\\mathcal{L}_c,\n    \\label{eq:loss_f}\n\\end{equation}\nwhere $\\beta$ is the weight of the counterfactual fairness constraint.\nMore implementation details can be found in Appendix A. \n\n%\\vspace{2mm}\n"
                    }
                }
            },
            "section 4": {
                "name": "Experimental Evaluations",
                "content": "\nIn this section, we conduct extensive experiments to evaluate the proposed framework \\mymodel~ on two real-world datasets and one synthetic dataset. Before showing the detailed results, we first present the details of used datasets and the experimental settings. %All the code and datasets will be available after publication.\n\n\n\n% fair prediction tasks: 1) predicting the salary of different adults \\cite{xu2019achieving}; and 2) predicting the performance of students in law school \\cite{kusner2017counterfactual}. We evaluate the proposed method in different aspects to show its superiority. \n\n\n\n\n\n\n\n\n% \\begin{table*}[t]\n% \\centering\n%  \\caption{Performance of predictors on different sensitive subgroups.}\n%  \\vspace{-2mm}\n%  \\label{tab:fairness}\n%   \\begin{tabular}{lcccccccc}\n%     \\toprule\n%     %\\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Law school} & \\multicolumn{2}{c}{COMPAS} & \\multicolumn{2}{c}{Adult} \\\\\n%     \\multirow{2}{*}{Method} & \\multicolumn{4}{c}{Law school} &  \\multicolumn{3}{c}{Adult} \\\\\n%      & RMSE  & MAE  & MMD & Wass &  Accuracy  & MMD & Wass  \\\\\n%     \\midrule\n%     Constant & $0.00\\pm 0.00$ & $0.00\\pm 0.00$ & $0.00\\pm 0.00$ & $0.00\\pm 0.00$ & $0.00\\pm 0.00$ & $0.00\\pm 0.00$ \\\\\n%     Full                & &&&&&\\\\\n%     Unaware                & &&&&&\\\\\n%     \\hline\n%     CFPred-1                & &&&&&\\\\\n%     CFPred-2                & &&&&&\\\\\n%     \\hline\n%     ERM & &&&&&\\\\\n%     IRM                & &&&&&\\\\\n%     ERM-MMD                & &&&&&\\\\\n%     \\hline\n%     \\mymodela & &&&&&\\\\\n%     \\mymodelb & &&&&&\\\\\n%     \\bottomrule\n% \\end{tabular}\n% \\vspace{-3mm}\n% \\end{table*}\n\n%\\vspace{-0.1in}\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\n\n\n% \\JL{I think we need to briefly mention why we simulate data in this way. For example, you can say you how the data can be used for the evaluations in Section 4.3 for the spurious correlations and imbalanaced data}\n\n%, and $W_S$ is a parameter which has different values $\\{0.1,0.2,1.0,2.0\\}$ when the sensitive attribute is $\\{0,1,2,3\\}$\n\n\\noindent \\textbf{Law School.} This dataset %Admission Council has released a survey record conducted across $163$ law schools in the United States \\cite{wightman1998lsac}, which \ncontains academic information of students in $163$ law schools. %including their entrance exam scores (LSAT), grade-point average (GPA) collected prior to law school, and their first year average grade (FYA). \nOur goal is to predict each student's first year average grade (FYA), and this is a regression task.\nWe take \\textit{race} as their sensitive attribute, and take grade-point average (GPA) and entrance exam scores (LSAT) as two observed features.  Here, we select persons in races of white, black, and asian. The dataset contains $20,412$ instances. \nWe use the level-2 causal model in \\cite{kusner2017counterfactual} as the true causal model with causal graph shown in Fig.~\\ref{fig:causalmodel_real}(a).\n\n\\noindent \\textbf{Adult.} UCI Adult income dataset\\footnote{https://archive.ics.uci.edu/ml/datasets/adult} contains census data for different adults and the target here is to predict whether their income exceeds 50K/yr. We take \\textit{race} as the sensitive attribute $S$, and their \\textit{income} as the prediction label $Y$. This is a binary classification task. We select persons in the races of white, black, and Asian-Pac-Islander. In addition to the sensitive attribute of \\textit{race}, we use other $5$ attributes for prediction. The dataset contains $31,979$ instances. Here, we follow \\cite{wu2019counterfactual} and consider the causal model used by them as the ground truth. The causal graph is shown in Fig.~\\ref{fig:causalmodel_real}(b).\n\n%We normalize the numerical attributes and use the one-hot encoding of the categorical attributes. \n\n\\noindent \\textbf{Synthetic Dataset.} Here, we use a ground truth causal model to generate the synthetic data. The true causal graph is shown in Fig.~\\ref{fig:causalmodel_sythetic}(a), containing a sensitive attribute $S$ with four different categorical values $\\{0,1,2,3\\}$, an unobserved variable $U$, a causal variable $X_0$ which is non-descendant of $S$, a causal variable $X_1$ which is descendant of $S$, and a variable $X_2$ which is the descendant of $Y$. The structural equations are as follows:\n$$\nS \\sim \\text{Catgorical}(\\pi), U\\sim \\mathcal{N}(0,\\sigma_U^2),X_0 = \\mathcal{N}(0,\\sigma_0^2),\n$$\n$$\nX_1 = W_S S + U + \\mathcal{N}(0,\\sigma_{S,1}^2), Y = X_1 + X_0 + \\mathcal{N}(0,\\sigma_{S,Y}^2),\n$$\n\\begin{equation}\n    X_2 = Y + \\mathcal{N}(0,\\sigma_{S,2}^2), \n    \\label{eq:synthetic}\n\\end{equation}\nwhere $\\pi=\\{0.5,0.4,0.05,0.05\\}$, $\\sigma_U=\\sigma_0=1$, $\\sigma_{S,*}$ and $W_S$ are set as $\\{0.5, 1.0, 1.5, 2.0\\}$ and $\\{0.1,0.2,1.0,2.0\\}$ respectively for four values of sensitive attribute. In this dataset, the spurious correlation $X_2 \\rightarrow Y$ and the imbalanced distribution of sensitive subgroups may lead to incorrect causal models, as shown in \\cite{nauta2019causal}. We will further investigate the impact of these two situations in Section 4.4. \n\n\n\n%\\textbf{COMPAS.} ProPublica [] released information of prisoners in Broward County, Florida who are in awaiting of sentencing hearing. The information includes each prisoner's race, age, number of juvenile felonies, juvenile misdemeanors, the type of crime they committed, the number of prior offenses they have, and whether they recidivated. We take \\textit{race} as their sensitive attribute and \\textit{whether they recidivated} as label.\n\n\\vspace{2mm}\n"
                },
                "subsection 4.2": {
                    "name": "Experimental Settings",
                    "content": "\n\n\\noindent \\textbf{Baselines.} To investigate the effectiveness of our framework in learning counterfactually fair predictors from observational data, we compare the proposed framework with multiple state-of-the-art methods. First, we briefly introduce all the compared baseline methods and their settings: \n\\begin{itemize}\n    \\item \\textbf{Constant Predictor:} A predictor which has constant output for any input. We obtain this constant predictor by finding a constant which can minimize the mean squared error (MSE) loss on the training data. \n\\item  \\textbf{Full Predictor:} Full predictor takes \\textit{all} the observed attributes (except the attribute used as label) as input for prediction. %We use linear regression for the regression task and logistic regression for the classification task. \n\\item \\textbf{Unaware Predictor:} Unaware predictor is based on the notion of fairness through unawareness~\\cite{grgic2016case}. It takes all features except the sensitive attribute as input to predict the label. \n\\item  \\textbf{Counterfactual Fairness Predictor:} We use two different counterfactual fairness predictors here, including \\textbf{\\cfpa}~  \\cite{kusner2017counterfactual} and  \\textbf{\\cfpb}~ \\cite{russell2017worlds}. These methods require a given causal model. \n\\end{itemize}\n\nFor baselines full/unaware/counterfactual fair predictors, we use linear regression for regression and logistic regression for classification. \nMore details of baselines can be found in Appendix B.\n% 5) \\textbf{Empirical Risk Minimization (ERM):} It is considered as a variant of our proposed framework \\mymodel. Here, we only use the empirical risk minimization loss (the first term of Eq.~(\\ref{eq:irm1})) in prediction without the counterfactual fairness constraint and invariant penalty by setting $\\beta=0$ and $\\lambda=0$. \n% 6) \\textbf{Invariant Risk Minimization (IRM):} It is another variant of \\mymodel. Here, we remove the counterfactual fairness constraint in our framework by setting $\\beta=0$. \n% 7) \\textbf{\\mymodel-NI:} As the third variant of our proposed framework, we remove the invariant penalty by setting $\\lambda=0$ in \\mymodela. For baselines full/unaware/counterfactual fair predictors, we use linear regression for  regression tasks and logistic regression for classification tasks. More details of baselines are in Appendix B.\n%We mainly divide these baselines into three categories: 1) \n\n\n\n\n\n\n\n% \\begin{figure}[t]\n% \\centering\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.9in]{figures/law_true.pdf}\n%         \\caption{True causal model $\\mathcal{M}_1$}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.9in]{figures/law_false.pdf}\n%         \\caption{False causal model $\\mathcal{M}_2$}\n%     \\end{subfigure}\n%      \\vspace{-4mm}\n%   \\caption{The true/false causal model of the law school dataset.}\n%   \\label{fig:causalmodel_law}\n% \\vspace{-4mm}\n% \\end{figure}\n\n% \\begin{figure}[t]\n% \\centering\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.9in]{figures/adult_true.pdf}\n%         \\caption{True causal model $\\mathcal{M}_1$}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.9in]{figures/adult_false.pdf}\n%         \\caption{False causal model $\\mathcal{M}_2$}\n%     \\end{subfigure}\n%      \\vspace{-4mm}\n%   \\caption{The true/false causal model of the Adult dataset.}\n%   \\label{fig:causalmodel_adult}\n% \\vspace{-4mm}\n% \\end{figure}\n\n% \\begin{figure*}[t]\n% \\centering\n% \\includegraphics[height=0.9in]{figures/law_true.pdf}\n%   \\caption{The true/false causal model of all the datasets.}\n%   \\label{fig:causalmodel_dataset}\n% \\vspace{-4mm}\n% \\end{figure*}\n\n\n% \\begin{table*}[t]\n% \\centering\n%  \\caption{Results comparison of different predictors on two real-world datasets.}\n%  %\\vspace{-2mm}\n%  \\label{tab:prediction}\n%   \\begin{tabular}{l||cccc||cccc}\\hline\n%     %\\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Law school} & \\multicolumn{2}{c}{COMPAS} & \\multicolumn{2}{c}{Adult} \\\\\n%     \\multirow{2}{*}{Method} & \\multicolumn{4}{|c||}{Law school} &  \\multicolumn{3}{c}{Adult} \\\\ \\cline{2-8} \n%      & RMSE ($\\downarrow$)  & MAE ($\\downarrow$)  & MMD ($\\downarrow$) & Wass ($\\downarrow$) &  Accuracy ($\\uparrow$)  & MMD ($\\downarrow$) & Wass ($\\downarrow$)  \\\\\n%     \\hline\n%     Constant & $0.938\\pm 0.003$ & $0.759\\pm 0.002$ & $\\bm{0.000\\pm 0.000}$ & $\\bm{0.000\\pm 0.000}$ & $0.759 \\pm 0.001$  & $\\bm{0.000\\pm 0.000}$ &  $\\bm{0.000\\pm 0.000}$ \\\\\n%     Full  & \\bm{$0.886 \\pm 0.004$}   & \\bm{$0.715 \\pm 0.003$} & $291.643 \\pm 5.213$ & $15.663 \\pm 1.326$ & \\bm{$0.815 \\pm 0.002$} & $ 50.513\\pm 3.283$ & $5.217 \\pm 0.582$\\\\\n%     Unaware & $0.902 \\pm 0.002$ & $0.728 \\pm 0.004$ & $39.144 \\pm 3.248$ & $8.321 \\pm 1.283$ & $0.794 \\pm 0.001$ & $16.832\\pm 2.377$ & $1.983 \\pm 0.462$\\\\\n%     \\hline\n%     \\cfpa & $0.932 \\pm 0.003$ & $0.738 \\pm 0.002$ & $4.307 \\pm 0.003$ & \\underline{$0.019 \\pm 0.001$} & $0.767 \\pm 0.002$ & $3.582 \\pm 0.007$ & $0.025\\pm 0.002$\\\\\n%     \\cfpb & $0.929 \\pm 0.004$ & $0.735 \\pm 0.003$ & $4.325 \\pm 0.002$ &$0.020 \\pm 0.012$ & $0.772 \\pm 0.003$&  $3.623 \\pm 0.004$& $0.029\\pm 0.004$ \\\\\n%     \\hline\n%     ERM & $0.898 \\pm 0.003$ & $0.722 \\pm 0.002$ & $42.862 \\pm 5.213$ & $8.385 \\pm 3.921$ & $0.798 \\pm 0.002$ & $13.032\\pm 3.271$ & $2.127 \\pm 0.528$\\\\\n%     IRM &  $0.903 \\pm 0.002$ & $0.726 \\pm 0.003$ & $40.910 \\pm 5.211$ & $6.326 \\pm 2.904$ & $0.791 \\pm 0.003$ & $8.418 \\pm 0.061$& $0.054\\pm 0.003$ \\\\\n%     \\mymodel-NI & $0.926 \\pm 0.003$ & $0.732 \\pm 0.002$ & $5.342 \\pm 0.015$ & $0.026 \\pm 0.005$ & $0.782 \\pm 0.002$ & $3.755 \\pm 0.056$& $0.027 \\pm 0.002$ \\\\\n%     \\hline\n%     \\mymodela~(ours) & $0.923 \\pm 0.002$ & $0.731 \\pm 0.003$ & $\\underline{4.297 \\pm 0.002}$ & \\underline{$0.019 \\pm 0.001$} & $0.778 \\pm 0.002$& \\underline{$3.552 \\pm 0.021$} & \\underline{$0.023\\pm 0.002$}\\\\\n%     \\mymodelb~(ours) & $0.920 \\pm 0.002$& $0.730 \\pm 0.002$& \\underline{$4.289 \\pm 0.002$}& \\underline{$0.018 \\pm 0.001$} & $0.780 \\pm 0.003$& \\underline{$3.547\\pm 0.007$}& \\underline{$0.023 \\pm 0.002$}\\\\\n%     \\hline\n% \\end{tabular}\n% %\\vspace{-3mm}\n% \\end{table*}\n\n\n\n\n\n\\noindent \\textbf{Evaluation Metrics.}\nGenerally, the evaluation metrics consider two different aspects: prediction performance and counterfactual fairness. To measure the model prediction performance, we employ the commonly used metrics -- Root Mean Square Error (RMSE) and mean absolute error (MAE) for regression tasks and accuracy for classification tasks. \nTo evaluate different methods with respect to counterfactual fairness, we compare the distribution divergence of the predictions made on different counterfactuals generated by the ground truth causal model. %Specifically, we first generate counterfactuals corresponding to different sensitive attribute values, then %we evaluate all the methods on the generated counterfactuals, and \n%compare the distribution of the predictions among different counterfactuals. \nIf a predictor is counterfactually fair, the distributions of the predictions under different ground-truth counterfactuals are expected to be the same. Here, we use two distribution distance metrics (including Wasserstein-1 distance (Wass) \\cite{ruschendorf1985wasserstein} and Maximum Mean Discrepancy (MMD) \\cite{long2015learning,shalit2017estimating}) to measure the distribution divergence. \n%The counterfactual corresponding to $S\\leftarrow s$ is denoted by $D_{S\\leftarrow s}=\\{X_{S\\leftarrow s},S\\leftarrow s, Y_{S\\leftarrow s}\\}$. \nWe compute the divergence of prediction distributions in every pair of counterfactuals ($S\\leftarrow s$ and $S\\leftarrow s'$ for any $s\\ne s'$), then take the average value as the final result. The smaller the average values of MMD and Wass are, the better a predictor performs in counterfactual fairness. For the synthetic data, the ground truth causal model is known, while for the real-world datasets, we adopt the widely accepted causal models as mentioned in Section 4.1. \n\n%Specifically, we assume the data is generated from a true causal model, then we fit the causal model\\JL{What do you mean fit the causal model?} and use the parameters\\JL{parameters of what?} to generate and sample the counterfactual data. We compare the prediction of the methods\\JL{which methods?} among different interventions on the sensitive attribute, and use several distribution distance metrics to measure the difference of predictions under different versions of counterfactual data. Here, we use Wasserstein-1 distance \\cite{ruschendorf1985wasserstein} and maximum mean discrepancy \\JL{give a reference for this metric} to measure the distribution differences.\n\n\\noindent \\textbf{Hyperparameter Settings.} For all these three datasets, we split the training/validation/test set as $60\\%/20\\%/20\\%$. All the presented results are on the test data. We set the number of training epochs as $500$, the representation dimension as $10$, $\\alpha=2.0$, $\\alpha'=1.0$, $K=20$, $\\beta=5.0$, and $\\lambda=1.0$. %More details can be found in Appendix B.\n%For the methods which need causal models (CFP-U and CFP-O), \n%We use Pyro \\cite{bingham2019pyro} to implement the causal models. %The number of sampling in the counterfactual generation is set as $500$. For the baselines CFP-U and CFP-O, the epochs for the causal model training is set as $1,000$ and the learning rate is set as $0.001$.\n\n%\\vspace{-0.05in}\n\n\n\n\n%\\subsection{Domain Shift across Sensitive Subgroups}\n% domain shift\n% comparison\n% Generalization for imbalanced / test subgroups\n% unobserved variables\n%To evaluate the generalization of the proposed method, we split the training/test data with respect to the sensitive subgroups. For each dataset, we select the white and black people as the training data, and the Asian people as test data. The distribution of each dataset is shown in Fig. \\ref{fig:domainshift}, we can observe obvious domain shift among different sensitive subgroups. Table \\ref{tab:fairness} shows the experimental result of all the predictors in prediction and fairness, we can observe that the overall performance drops a little due to the domain shift between the training and test data. Compared to the CFPred method, our model achieves both better prediction performance and fairness. IRM performs better than ERM in this case because of its invariant penalty term increases its generalization across different domains. ERM-MMD also achieves good generalization with its distribution balancing term. \\mymodelb~ performs slightly better than \\mymodela~ due to its individual-level adversarial learning to remove the sensitive information. \n\n%\\vspace{-0.05in}\n"
                },
                "subsection 4.3": {
                    "name": "Experimental Results on Real-world Data",
                    "content": "\nTo assess the superiority of the proposed framework \\mymodel, we compare its two implementations \\mymodela~ and \\mymodelb~ against other predictors on two real-world datasets Law School and Adult. We show the ground truth causal models of these two datasets in Fig.~\\ref{fig:causalmodel_real} although our proposed framework and its variants do not rely on the causal model. Table \\ref{tab:prediction} presents the performance of different methods regarding prediction and counterfactual fairness. The best results are shown in \\textbf{bold}, and the runner-up results are \\underline{underlined}.\n%\nGenerally speaking, existing methods which are not designed for counterfactual fairness have higher MMD and Wass, although they can use the biased features to achieve better prediction performance. %Our method achieve both fairness and good data utility for prediction.\nWe make the following observations from Table~\\ref{tab:prediction}:\n\\begin{itemize}\n%\n\\item Among all the compared methods, the constant predictor has the worst performance in prediction as it lacks capability to distinguish different instances. However, it always satisfies counterfactual fairness because it has constant output. \n\\item The full predictor performs well in prediction, as it utilizes all the features (both sensitive and non-sensitive). But the use of sensitive attribute also brings biases to the prediction, as demonstrated by its high values on fairness metrics. \n\\item The unaware predictor removes certain biases by ignoring the sensitive attribute, but it cannot exclude the implicit biases caused by inappropriate usage of the descendants of the sensitive attribute. \n\\item Both \\cfpa~and \\cfpb~ infer the latent variables based on the given causal model, so they perform well if the given causal model is correct. \n\\item Our proposed \\mymodel~consistently outperform other baselines (except the constant predictor) under different fairness metrics, and also have better prediction performance than many other fairness-aware baselines (including \\cfpa~ and \\cfpb). It implies that \\mymodel~ can achieve a good balance between prediction performance and counterfactual fairness.\n\\item The variants \\mymodela~and~\\mymodelb~ generally have similar performance, but \\mymodelb~ is slightly better in fairness, it may benefit from the effectiveness of its adversarial learning mechanism in removing the sensitive information.\n%\\item ERM, IRM, and \\mymodel-NI are variants of \\mymodel. The comparison between them and \\mymodel~ shows that both invariant representations  and counterfactual data augmentation contribute to \\mymodel~ for counterfactual fairness.\n\\end{itemize}\n%\n\n\n\\vspace{0.05in}\n"
                },
                "subsection 4.4": {
                    "name": "Experimental Results on Synthetic Data",
                    "content": "\nThe above experiments on real-world datasets have demonstrated the superiority of \\mymodel. Here, we perform further studies on the synthetic dataset to show the impact of incorrect causal models. % caused by spurious correlations and imbalanced data.\n\n\\noindent \\textbf{Incorrect causal model $\\mathbf{M_1}$.} %\n%To evaluate the performance and fairness of different methods, we compare them with respect to two settings: 1) The ground-truth causal model is known; 2) The causal model is incorrect. \nIn this experiment, we use the synthetic data to showcase the impact of an incorrect causal model as the example shown in Fig. \\ref{fig:causalmodel_sythetic}(b). % resulted from spurious correlations. \nThe true causal model of the synthetic data is shown in Fig. \\ref{fig:causalmodel_sythetic}(a). %, while the spurious correlation between $X_2$ and $Y$ results in an incorrect causal model as shown in Fig. \\ref{fig:causalmodel_sythetic}(b). \nHere, causal relations regarding $X_2$ in $\\mathcal{M}_1$ are reversed. \nAs all the baselines (except \\cfpa~and \\cfpb) do not rely on the causal model for prediction, so their results are not influenced by the correctness of the causal model. Here, we investigate the influence of the incorrect causal model on \\cfpa~and \\cfpb~ and compare their performance with our proposed framework. \n% and Fig. \\ref{fig:causalmodel_sythetic}(b) shows a common mistake in causal discovery, where the direction of some edges ($X_2\\rightarrow Y$ and $S\\rightarrow X_2$) are opposite from the ground truth, these mistakes can be led by the spurious correlation between $X_2$ and $Y$. \n%\n%As shown in Table \\ref{tab:spurious}, all the predictors, except the \\cfpa~and \\cfpb~ methods, do not rely on the causal model, so their performances are not influenced by the correctness of the causal model. \n%We make the following observations from the results shown in Table~\\ref{tab:spurious}:\nFrom the results shown in Table~\\ref{tab:spurious}, %(best results are shown in bold and the runner-up is presented with an underline), \nwe find the fairness of \\cfpa~ and \\cfpb~ are obviously affected by the incorrect causal model.\n%\nAlthough \\cfpa~ and \\cfpb~ with incorrect causal model have slightly better performance in prediction, that is because based on the incorrect causal model, they may take $X_2$ into prediction, which however, brings biases for prediction. \n%\nOur proposed framework does not assume the existence of any given causal model for prediction. The counterfactual data augmentation enables us to eliminate the influence of sensitive attributes to the prediction. Furthermore, the learned invariant representations in \\mymodel~ exclude the adverse impacts of non-causal variables with spurious correlations and leverage the causal variables to learn representations, thus $X_2$ is encouraged to be excluded from prediction.\n\n\n% but both \\mymodela~and~\\mymodelb maintain good results as they do not need causal model for prediction.\n% In this experiment, the invariant representations exclude the spurious correlation, using the causal variables to make accurate prediction. %and we use the distribution matching or adversarial learning based implementations to remove the influence of the sensitive attribute, which lead us to achieve competitive performance in counterfactual fairness.\n\n\\noindent \\textbf{Incorrect causal model $\\mathbf{M_2}$.} Now, we use the  synthetic data to showcase the impact of another incorrect causal model as shown in Fig.~\\ref{fig:causalmodel_sythetic}(c). %resulted from imbalanced data. \n%Here, the causal relation $S\\rightarrow X_1$ is ignored. \n%\n%As mentioned in Section 4.1, the synthetic data is imbalanced across different sensitive subgroups. Here, we consider the first two subgroups as the majority (90\\%) and the last two subgroups as the minority (10\\%). \nAs described in Section 4.1, we set the parameter $W_S$ in Eq.~(\\ref{eq:synthetic}), which determines the relation $S\\rightarrow X_1$, to be small on the majority sensitive subgroups ($S=0,1$) but relatively large on the minority sensitive subgroups ($S=2,3$). \n%Here, the imbalanced data may lead to the ignorance of the sensitive attribute's influence on $X_1$ in the minority sensitive subgroups. Specifically, in this example, the fitted causal model may miss the causal relation $S\\rightarrow X_1$ and thus will be biased (as shown in Fig.~\\ref{fig:causalmodel_sythetic}(c)). \nHere, the incorrect causal model misses the causal relation $S\\rightarrow X_1$ (as shown in Fig.~\\ref{fig:causalmodel_sythetic}(c)). \n%\n%As described in Section 4.2, when we evaluate the fairness of different predictors, we first use the ground truth causal model as shown in Fig.~\\ref{fig:causalmodel_sythetic}(a) to generate the counterfactuals for the sensitive attribute assignment $S\\leftarrow 0,1,2,3$, respectively, and then we compare the prediction differences between different counterfactuals. %If a predictor is counterfactually fair, the prediction made on any counterfactuals should be expected to be same. \n%Again, as only \\cfpa~and \\cfpb~are affected by the incorrect causal models, we compare them with our proposed frameworks \\mymodela~ and \\mymodelb~ with respect to counterfactual fairness. \n%Specifically, we use the true causal model shown in Fig.~\\ref{fig:causalmodel_sythetic}(a) to generate the counterfactuals for the sensitive attribute  $S\\leftarrow 0,1,2,3$, respectively, and then \nWe compare the prediction differences between pairs of different counterfactuals generated by the true causal model shown in Fig.~\\ref{fig:causalmodel_sythetic}(a). \nThe results are shown in Table \\ref{tab:imbalance}, %(the best result is shown in bold and the runner-up is presented with an underline), \nwhere we select two pairs of counterfactuals: $(S\\leftarrow 0$ and $S\\leftarrow 1)$ and $(S\\leftarrow 0$ and $S\\leftarrow 2)$. \n%where we select three pairs of counterfactuals: $(S\\leftarrow 0$ and $S\\leftarrow 1)$ and $(S\\leftarrow 0$ and $S\\leftarrow 2)$. %, and $(S\\leftarrow 2$ and $S\\leftarrow 3)$.\n%The first comparison is between the predictions made on the pair of counterfactuals  $(S\\leftarrow 0$ and $S\\leftarrow1)$. \nAs $W_S$ is small when $S=0$ and $S=1$, the biased causal model would not bring too much bias from the sensitive attribute to the prediction in the two counterfactuals $(S\\leftarrow 0$ and $S\\leftarrow 1)$, so the discrepancy between this pair is relatively lower than the other pair. But for the counterfactuals of $S\\leftarrow 2$ (and also $S\\leftarrow3$), \\cfpa~ and \\cfpb~ suffer more from the biased causal model. As observed in Table \\ref{tab:imbalance},\nwhen \\cfpa~ and \\cfpb~ are under the biased causal model, the prediction discrepancy between the pair of counterfactuals $(S\\leftarrow 0$ and $S\\leftarrow 2)$ becomes larger than the case when \\cfpa~ and \\cfpb~ are under the true causal model. \nSimilar observations can also be found in the pair $(S\\leftarrow 2$ and $S\\leftarrow 3)$, as shown in Appendix C. \nOur framework outperforms the baselines due to the following key factors: the fair generative factors captured in  counterfactual data augmentation remove the influence of the observed sensitive attribute to the generated counterfactuals. Therefore, the counterfactual fairness constraint \nmitigates the influence of sensitive attribute on the learned representations, and  makes our framework suffer less from  imbalanced sensitive subgroups.\n%2) the invariant representations exclude the unstable relations that vary across different sensitive subgroups.  %\\JL{We need to tailor the results analysis to the CFA method and ICFP}\n\n%This ignorance leads to biased causal models and will further impact \\cfpa~and\\cfpb since they rely on the causal model for prediction.\n\n\n\n% To evaluate the performance and fairness of different methods, we compare them with respect to two settings: 1) The ground-truth causal model is known; 2) The causal model is incorrect. In the law school dataset, we assume the true causal model is as shown in Fig. \\ref{fig:causalmodel_law}(a), which is as described in \\cite{kusner2017counterfactual}. \n% Fig. \\ref{fig:causalmodel_law}(b) shows a common mistake in causal discovery, where the direction of some edges ($S\\rightarrow L$ and $L\\rightarrow \\epsilon_L$) are opposite from the ground truth. The structural equations of the true/false causal model are as below:\n% $$\n%     G = b_G + w_G^S S + \\epsilon_G, \\quad\\quad  G = b_G + w_G^S S + \\epsilon_G,\n% $$\n% $$\n%     L = b_L + w_L^S S + \\epsilon_L, \\quad\\quad S = b_S + w_S^L L, \\, \\epsilon_L = b_\\epsilon + w_\\epsilon^L L\n% $$\n% \\begin{equation}\n%     Y = b_Y + w_Y^S S + \\epsilon_F, \\quad\\quad Y = b_Y + w_Y^S S + \\epsilon_F\n% \\end{equation}\n% where $b,w$ are parameters of the causal model. The incorrect causal models are similarly assumed in other datasets, and the details can be found in the Appendix A. As shown in Table \\ref{tab:prediction}, all the predictors, except the CFPred methods, do not rely on the causal model, so their performances are not influenced by the correctness of the causal model. \n% Among all the methods, the constant predictor has the worst performance in prediction as it lacks of ability to distinguish the different instances, but it always satisfies counterfactual fairness as it has constant output. \n% The full predictor performs well in prediction, as it can utilize all the features, no matter sensitive or non-sensitive, but the sensitive attributes also brings bias to the prediction. \n% The unaware predictor removes some bias by ignoring the sensitive attributes, but it can not exclude the implicit bias caused by the descendants of the sensitive attributes. \n% The CFPred methods infers the latent variables based on the causal model, so it performs well if the causal model is correct, otherwise, both its performance and fairness are affected by the incorrectness of the causal model. Our proposed method \\mymodel~ consistently achieve competitive performance with the correct causal model based CFPred methods, and \\mymodel~ is not affected by the incorrect causal model.\n\n\n\n%First, we compare the performance of different methods under the true/false causal models. Second, \n%The true and false causal models are shown in Fig.~\\ref{fig:causalmodel_dataset}(a) and (b). In the true causal model, $X_2$ is a descendant of $Y$, but the false model takes it as a parent node of $Y$, and also ignore the relation from $S$ to $X_2$. The results of different predictors are shown in Table X. \n\n\n\n%\n%\n%To further validate the , we control the level of distribution shift by synthetically generating a variable and use it as an observed feature for prediction. We \n\n% \\begin{figure}[t]\n% \\centering\n%   \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.1in]{figures/domain1.pdf}\n%         \\caption{Law school}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.1in]{figures/domain2.pdf}\n%         \\caption{Adult}\n%     \\end{subfigure}\n%      \\vspace{-4mm}\n%   \\caption{Domain shift across sensitive subgroups.}\n%   \\label{fig:domainshift}\n% \\vspace{-4mm}\n% \\end{figure}\n\n\n\n\n\n\n\n%\\vspace{-3mm}\n"
                },
                "subsection 4.5": {
                    "name": "Ablation Study",
                    "content": "\nTo evaluate the effectiveness of each component in our method, we provide ablation study with the following variants:\n1) \\textbf{Empirical Risk Minimization (ERM):} ERM can be considered as a variant of our proposed framework \\mymodel. Here, we only use the empirical risk minimization loss (the first term of Eq.~(\\ref{eq:irm1})) in prediction without the counterfactual fairness constraint and invariant penalty by setting $\\beta=0$ and $\\lambda=0$. \n2) \\textbf{Invariant Risk Minimization (IRM) \\cite{arjovsky2019invariant}:} Here, we remove the counterfactual fairness constraint in our framework by setting $\\beta=0$. \n3) \\textbf{\\mymodel-NI:} As the third variant of our proposed framework, we remove the invariant penalty by setting $\\lambda=0$ in \\mymodel. From the results shown in Fig.~\\ref{fig:ablation}, the counterfactual data augmentation and invariant penalty both contribute to the overall fairness performance.\n\n"
                },
                "subsection 4.6": {
                    "name": "Parameter Study",
                    "content": "\n%Due to the space limit, we only show the performance on \\mymodela, but similar observations can also be found in \\mymodelb.\nWe set the hyperparameter $\\alpha\\in\\{0.01, 0.1, 1.0, 10, 100\\}$, the sampling number $K\\in\\{1, 5, 10, 20, 100\\}$, $\\beta\\in\\{0.01, 0.1, 1.0, 10, 100\\}$, $\\lambda\\in\\{0.01, 0.1, 1.0, 10, 100\\}$, and compare the performance of our proposed framework in Fig.~\\ref{fig:param}. Here we only show the results of \\mymodela~ on the law school dataset, as similar patterns can be observed in \\mymodelb~ and other datasets. As observed in  Fig.~\\ref{fig:param}(a), $\\alpha$ controls the ``fairness\" of the embedding in counterfactual data augmentation. Larger values of $\\alpha$ can improve the counterfactual fairness of the framework, and have no obvious impact on the prediction performance. With larger $K$ in Fig.~\\ref{fig:param}(b), the performance of counterfactual fairness also improves because more samples are generated in counterfactual data augmentation.\n$\\beta$ controls the importance of counterfactual fairness constraint, $\\lambda$ controls the invariance penalty of the representations. \n%Fig. \\ref{fig:param} shows the prediction RMSE and the averaged MMD when we vary $\\beta\\in\\{0.01, 0.1, 1.0, 10, 100\\}$, $\\lambda\\in\\{0.01, 0.1, 1.0, 10, 100\\}$. %If it is not being varied, $\\lambda$ is fixed at $1.0$ and $\\alpha=10$, $\\beta=10$, $K=5$.\n%\nAs shown in Fig. \\ref{fig:param}(c), with the increase of $\\beta$, the framework focuses more on removing the biases from the sensitive attribute, which may sacrifice some information to predict the target, and thus results in higher RMSE, but can achieve better fairness. As shown in Fig. \\ref{fig:param}(d), with the increase of $\\lambda$, the framework may exclude more variables with unstable relationships to the target across different sensitive subgroups, it may thus lose some information specific to each sensitive subgroup, but can also contribute to better fairness.\n%$\\alpha\\in\\{0.01, 0.1, 1.0, 10, 100\\}$, and $K=\\{1, 5, 10, 20\\}$ $\\alpha$ controls the weight of distribution matching penalty in counterfactual data augmentation, and $K$ is the number of samples in counterfactual data augmentation.  \n%$\\lambda$ and $\\alpha$, the proposed framework has higher test RMSE, but\\JL{???} can achieve better fairness. \\JL{We need to beef up this subsection by explicitly giving guide to the readers which values we should set for $\\alpha$ and $\\gamma$} \nFrom the observations, the framework achieves a good trade-off on the prediction performance and counterfactual fairness with proper parameter settings. %, e.g., $\\lambda\\in [0.1,1]$ and $\\beta\\in [1,10]$. \n%Due to the space limit, we only show the parameter studies for $\\beta$ and $\\lambda$ in \\mymodela~ on the Law School dataset, but similar conclusions can also be made in \\mymodelb, other datasets,  and other parameters such as $\\alpha$ and $K$, as shown in Appendix C. \n% dataset: level of domain shift\n% model: parameter study%\\vspace{2mm}\n"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n%\\vspace{-1mm}\n% In this section, we briefly review related work in two aspects: counterfactual fairness and invariant risk minimization.\n\n\\noindent \\textbf{Counterfactual Fairness.} \nRecently, aside from traditional statistical fairness notions \\cite{zemel2013learning,hardt2016equality,zafar2017fairness,dieterich2016compas,chouldechova2017fair,chouldechova2018frontiers,barocas2017fairness}, \n%Recently, aside from traditional fairness notions such as statistical parity \\cite{zemel2013learning}, equalized odds \\cite{hardt2016equality,zafar2017fairness}, predictive parity \\cite{dieterich2016compas}, and calibration \\cite{chouldechova2017fair}, \ncausal-based fairness notions \\cite{makhlouf2020survey,kusner2017counterfactual,russell2017worlds} have attracted a surge of attentions because of its strong capability of modeling how the discrimination is exhibited.\n%The causal-based fairness notions can be grouped into intervention [], counterfactual [], direct and indirect effect [], and path-specific effect identifiability results []. \nAmong them, the notion of counterfactual fairness \\cite{kusner2017counterfactual} assesses fairness at the individual level. \n%The counterfactual fairness is satisfied for each individual when the probability distribution of the outcome prediction stay the same in the real and the counterfactual worlds. \nMost of the existing counterfactual fairness studies \\cite{xu2019achieving,kusner2017counterfactual,grari2020adversarial} are based on a given ground-truth causal model or rely on causal discovery methods \\cite{spirtes2000causation,pearl2009causality,kalisch2007estimating}. Multi-world fairness \\cite{russell2017worlds} considers the situation when the ground-truth causal model cannot be decided, but it still requires a candidate set containing causal models which may be true, and proposes an optimization based method to achieve counterfactual fairness with the average of the causal models in the candidate set. \nMany methods based on traditional causal discovery are limited in certain scenarios, such as low-dimensional and linear settings. \nRecent studies \\cite{kim2021counterfactual,grari2022adversarial,zuo2022counterfactual} provide more discussion about counterfactual fairness under different assumptions and scenarios. \nBut in conclusion, most of the above methods require much explicit prior knowledge of the causal model to remove the influence of the sensitive attribute on the prediction, and lack discussion of the impact of incorrect causal models. \n\n\\noindent \\textbf{Invariant Risk Minimization.} Invariant risk minimization (IRM) \\cite{arjovsky2019invariant} and its variants \\cite{guo2021out,ahuja2020invariant,krueger2020out,jin2020domain,mahajan2020domain,chang2020invariant} are originally proposed for out-of-distribution (OOD) generalization \\cite{krueger2020out,sagawa2019distributionally}. %by learning representations of causal features.\n%\nIt is based on the theorem that the representations of causal features elicit the existence of an optimal predictor across different domains.  %(i.e., \\textit{environment}).\n%\nFrom a causal perspective, IRM identifies these causal features and excludes those features with spurious correlations as these correlations are not robust across different domains.\n%\n%Hence, IRM and its variants \\cite{guo2021out,ahuja2020invariant,krueger2020out,jin2020domain,mahajan2020domain,chang2020invariant} have achieved empirically good generalization capability in underrepresented or unseen environments.\n%\n%The OOD generalization can also be considered from a robust optimization aspect, which aims to optimize the worst domain specific risk of a model in a set of domains~\\cite{krueger2020out,sagawa2019distributionally}.\n%\n%Kuang et al.~\\cite{kuang2018stable} investigated a relevant problem---stable learning.\n%\n%The goal is to learn OOD generalizable models from a single training domain.\n%\n%They proposed a method that takes each feature as a cause of the label and matches the distributions of all other features.\n%\n%Yu et al.~\\cite{yu2020causality} presented a comprehensive survey of methods identifying causal features in the set of raw features, which are potentially useful for learning interpretable and OOD generalizable models.\n% Chang et al.~\\cite{chang2020invariant} showed that imposing the conditional independence between the predicted label and the domain given feature representations is helpful in selecting causal text features for the task of rationalization.\n%\n%The key constraint of IRM is that $E\\bigCI Y|Z$, where $Z$ is the causal variables (observed/unobserved). \nThe connections between fairness and IRM are discussed in~\\cite{arjovsky2019invariant,creager2020environment,veitch2021counterfactual}. IRM can learn representations to capture causal features which have invariant relationships to the prediction target. However, the representations may still contain the information of domains (e.g., different sensitive attributes), which may cause biases to prediction. Our work investigate to bridge this gap between IRM and counterfactual fairness.\n\n%\\vspace{-1mm}\n\n%\\input{deriv}\n\n\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n%\\vspace{1mm}\nIn this work, we study a novel problem of learning counterfactually fair predictors from observational data with unknown causal models. We propose a principled framework \\mymodel. More specifically, we specify this framework by learning counterfactually fair representations for each instance, and make predictions based on the representations. To learn fair representations, a variational auto-encoder based counterfactual data augmentation module is developed to generate counterfactual data with different values of sensitive attribute for each instance. We further reduce potential biases by applying the invariant penalty in each sensitive subgroup to exclude the \nvariables with spurious correlations to the target. %, and remove the influence of the sensitive attribute on the representations with two different implementations---a distribution matching based method and an adversarial learning based method. \nWe evaluate the proposed framework under both real-world benchmark datasets and synthetic data. Extensive experimental results validate the superiority of the proposed framework over existing fairness predictors in different aspects. Overall, this paper provides insights for promoting counterfactual fairness in a more realistic scenario without given correct causal models, and also shows the impact of incorrect causal models. In the future, more research work on counterfactual fairness in real-world cases, such as missing and noisy data, is worth further exploration.\n\n"
            },
            "section 7": {
                "name": "Acknowledgements",
                "content": "\nJing Ma, Aidong Zhang, and Jundong Li are supported by the National Science Foundation under grants (IIS-1955151, IIS-2006844, IIS-2008208, IIS-2106913, IIS-2144209, IIS-2223769, CNS-2154962, CNS-2213700, BCS-2228534, and CCF-2217071), the Commonwealth Cyber Initiative awards (VV-1Q23-007 and HV-2Q23-003), the JP Morgan Chase Faculty Research Award, the Cisco Faculty Research Award, the Jefferson Lab subcontract 23-D0163, the UVA 3 Cavaliers seed grant, and the 4-VA collaborative research grant.\n\n%aiming to learn a predictor through capturing representation of the counterfactually fair information from different sensitive subgroups.\n%We address the challenge that the variables with spurious correlations to the prediction label often leads to incorrect and unstable causal model. With less prior knowledge on the causal model, we propose a novel framework \\mymodel, and specify this framework with different implementation instantiations. The proposed framework is evaluated on multiple real-world benchmark datasets. Extensive experimental results validate the superiority of the proposed framework over existing fairness predictors in different aspects.\n\n%\\JL{Conclusion needs to be fixed}\n\n\\clearpage\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{ref}\n\\clearpage\n\\appendix\n"
            },
            "section 8": {
                "name": "Implementation Details",
                "content": "\nWe use two fully connected layers in neural networks to implement $\\Phi(\\cdot)$, $g(\\cdot)$ and $h(\\cdot)$, respectively. The softmax function is used on top of $h(\\cdot)$ when the sensitive attribute is categorical. LeakyRelu is used as activation functions in our framework. We aggregate the counterfactuals with mean operation, and we use mean square error (MSE) to compute the target prediction loss. For \\mymodela, we adopt the implementation of MMD from~\\cite{long2015learning}, and the optimization problem can be solved by traditional stochastic gradient descent algorithms. For \\mymodelb, following \\cite{chang2020invariant}, the minimax optimization problem is conducted with an alternating gradient descent process. We use cosine distance to implement $d(\\cdot,\\cdot)$. \n\n"
            },
            "section 9": {
                "name": "Details of Experiment Settings",
                "content": "\n",
                "subsection 9.1": {
                    "name": "Full Introduction of Baselines",
                    "content": "\n\\begin{itemize}\n    \\item \\textbf{Constant Predictor:} A predictor which has constant output can obviously satisfy counterfactual fairness. We obtain this constant predictor by finding a constant which can minimize the mean squared error (MSE) loss on the training data.\n    \\item \\textbf{Full Predictor:} Full predictor takes \\textit{all} the observed attributes (except the attribute used as label) as input for prediction. We use linear regression for the regression task and logistic regression for the classification task. \n    \\item \\textbf{Unaware Predictor:} Unaware predictor is based on the notion of fairness through unawareness~\\cite{grgic2016case}. It takes all features except the sensitive attribute as input to predict the label through linear regression for the regression task and logistic regression for the classification task.\n    \\item \\textbf{Counterfactual Fairness Predictor:} We use two different counterfactual fairness predictors here: 1) As introduced in \\cite{kusner2017counterfactual}, the predictor infers the latent variables and uses them along with the observed variables which are non-descendants of the sensitive attributes; 2) As described in \\cite{russell2017worlds}, the predictor takes the input of both sensitive and non-sensitive attributes, with a fairness term added in the loss function which minimize the difference of the predictions made on two counterfactuals. We refer to these two methods as \\cfpa~ and \\cfpb, respectively. We follow the original implementations in \\cite{kusner2017counterfactual,russell2017worlds}, where \\cfpa~ uses linear regression for the regression task and logistic regression for the classification task, and \\cfpb~ is implemented with neural networks.\n    %A counterfactual fairness predictor follows a 3-step process as described in []: 1) fitting a causal model and inferring the latent variables; 2) generating counterfactual data with the causal model; 3) training a predictor. \n    %\\item{\\textbf{Empirical Risk Minimization (ERM):}} It is considered as a variant of our proposed framework \\mymodel. \n    %Here, we only use the empirical risk minimization loss (the first term of Eq.~(\\ref{eq:irm1})) in prediction without the counterfactual fairness constraint and invariant penalty by setting $\\beta=0$ and $\\lambda=0$. % by removing the invariant penalty and distribution matching components in our framework (set $\\lambda=0$ and $\\alpha=0$).\n    %\\item \\textbf{Invariant Risk Minimization (IRM):} It is another variant of \\mymodel. Here, we remove the counterfactual fairness constraint in our framework by setting $\\beta=0$.\n    %An predictor based on invariant risk minimization \\cite{arjovsky2019invariant} targets at learning an invariant predictor, which is supposed to be \\textit{optimal} across different \\textit{environments}. Here we treat each sensitive subgroup as an environment. \n    %\\item \\textbf{\\mymodel-NI:} As the third variant of our proposed framework, we remove the invariant penalty by setting $\\lambda=0$.\n    %We add a distribution matching term into the loss function of empirical risk minimization method. This distribution matching term is calculated by the Maximum Mean Discrepancy (MMD) \\cite{long2015learning,shalit2017estimating} distance of the representation distribution among different sensitive groups. This method is denoted by ERM-MMD. \n\\end{itemize}\n\n"
                },
                "subsection 9.2": {
                    "name": "Detailed Experimental Setup",
                    "content": "\nWe use Pyro \\cite{bingham2019pyro} to implement the causal models. The number of sampling in the counterfactual generation is set as $500$. For the baselines CFP-U and CFP-O, the epochs for the causal model training is set as $2,000$ and the learning rate is set as $0.001$. All the presented results are averaged over ten executions of experiments. \n\n"
                }
            },
            "section 10": {
                "name": "More Experimental Results",
                "content": "\nTable \\ref{tab:imbalance_complete} shows the discrepancy of predictions made on different counterfactuals. In addition to the two pairs of counterfactuals ($S\\leftarrow 0$ and $S\\leftarrow 1$) and ($S\\leftarrow 0$ and $S\\leftarrow 2$) shown in Table \\ref{tab:imbalance}, Table \\ref{tab:imbalance_complete} also shows the results in pair ($S\\leftarrow 2$ and $S\\leftarrow 3$). Generally, the observation on the pair  ($S\\leftarrow 2$ and $S\\leftarrow 3$) is similar to the aforementioned observation on the pair ($S\\leftarrow 0$ and $S\\leftarrow 2$). \n\n\n%\\subsection{More Parameter Studies}\n\n% \\begin{figure}[t]\n% \\centering\n%   \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.1in]{figures/alpha.pdf}\n%         \\caption{Vary $\\alpha$}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.1in]{figures/K.pdf}\n%         \\caption{Vary $K$}\n%     \\end{subfigure}\n%   \\caption{Performance of \\mymodela~with different settings of hyperparameters $\\alpha$ and $K$.}\n%   \\label{fig:param_alpha}\n% \\end{figure}\n\n% We set the hyperparameter $\\alpha=\\{0.01, 0.1, 1.0, 10, 100\\}$ and the sampling number $K=\\{1, 5, 10, 100\\}$, and compare the performance of our proposed framework in Fig.~\\ref{fig:param_alpha}. Here we only show the results of \\mymodela~ on the law school dataset, as similar patterns can be observed in \\mymodelb~ and other datasets. As observed in  Fig.~\\ref{fig:param_alpha}, $\\alpha$ controls the ``fairness\" of the embedding in counterfactual data augmentation. Larger values of $\\alpha$ can improve the counterfactual fairness of the framework, and have no obvious impact on the prediction performance. With larger $K$, the performance of counterfactual fairness also improves because more samples are generated in counterfactual data augmentation.\n\n\n"
            }
        },
        "tables": {
            "tab:prediction": "\\begin{table*}[t]\n\\centering\n \\caption{Results comparison of different predictors on two real-world datasets. Our method \\mymodel~ can achieve the best performance in counterfactual fairness with competitive prediction performance.}\n %\\vspace{-2mm}\n  \\def\\arraystretch{1.2}% \n \\label{tab:prediction}\n  \\begin{tabular}{l||cccc||cccc}\\hline\n    %\\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Law school} & \\multicolumn{2}{c}{COMPAS} & \\multicolumn{2}{c}{Adult} \\\\\n    \\multirow{2}{*}{Method} & \\multicolumn{4}{|c||}{Law school} &  \\multicolumn{3}{c}{Adult} \\\\ \\cline{2-8} \n     & RMSE ($\\downarrow$)  & MAE ($\\downarrow$)  & MMD ($\\downarrow$) & Wass($\\downarrow$) &  Accuracy ($\\uparrow$)  & MMD ($\\downarrow$) & Wass ($\\downarrow$)  \\\\\n    \\hline\n    Constant & $0.952\\pm 0.003$ & $0.772\\pm 0.002$ & $\\bm{0.000\\pm 0.000}$ & $\\bm{0.000\\pm 0.000}$ & $0.745 \\pm 0.001$  & $\\bm{0.000\\pm 0.000}$ &  $\\bm{0.000\\pm 0.000}$ \\\\\n    Full  & \\bm{$0.896 \\pm 0.004$}   & \\bm{$0.723 \\pm 0.003$} & $259.744 \\pm 5.213$ & $65.656 \\pm 1.326$ & \\bm{$0.815 \\pm 0.002$} & $50.513\\pm 3.283$ & $5.217 \\pm 0.582$\\\\\n    Unaware & \\underline{$0.909 \\pm 0.002$} & $0.734 \\pm 0.004$ & $39.144 \\pm 3.248$ & $10.093 \\pm 1.254$ & $0.809 \\pm 0.003$ & $16.832\\pm 2.377$ & $1.983 \\pm 0.462$\\\\\n    \\hline\n    \\cfpa~(true) & $0.932 \\pm 0.003$ & $0.738 \\pm 0.002$ & $4.307 \\pm 0.003$ & \\underline{$0.019 \\pm 0.001$} & $0.745 \\pm 0.002$ & $3.582 \\pm 0.007$ & $0.025\\pm 0.002$\\\\\n    \\cfpb~(true)& $0.929 \\pm 0.004$ & $0.735 \\pm 0.003$ & $4.325 \\pm 0.002$ &$0.020 \\pm 0.012$ & $0.748 \\pm 0.003$&  $3.623 \\pm 0.004$& $0.029\\pm 0.004$ \\\\\n    \\hline\n    % ERM & $0.898 \\pm 0.003$ & $0.722 \\pm 0.002$ & $42.862 \\pm 5.213$ & $8.385 \\pm 3.921$ & $0.798 \\pm 0.002$ & $13.032\\pm 3.271$ & $2.127 \\pm 0.528$\\\\\n    % IRM &  $0.903 \\pm 0.002$ & $0.726 \\pm 0.003$ & $40.910 \\pm 5.211$ & $6.326 \\pm 2.904$ & $0.791 \\pm 0.003$ & $8.418 \\pm 0.061$& $0.054\\pm 0.003$ \\\\\n    % \\mymodel-NI & $0.926 \\pm 0.003$ & $0.732 \\pm 0.002$ & $5.342 \\pm 0.015$ & $0.026 \\pm 0.005$ & $0.782 \\pm 0.002$ & $3.755 \\pm 0.056$& $0.027 \\pm 0.002$ \\\\\n    % \\hline\n    \\mymodela~(ours) & \\underline{$0.909 \\pm 0.002$} & \\underline{$0.733 \\pm 0.003$} & $\\underline{4.297 \\pm 0.002}$ & \\underline{$0.019 \\pm 0.001$} & $0.778 \\pm 0.002$& \\underline{$3.552 \\pm 0.021$} & \\underline{$0.023\\pm 0.002$}\\\\\n    \\mymodelb~(ours) & $0.910 \\pm 0.002$& $0.734 \\pm 0.002$& \\underline{$4.289 \\pm 0.002$}& \\underline{$0.018 \\pm 0.001$} & $0.780 \\pm 0.003$& \\underline{$3.547\\pm 0.007$}& \\underline{$0.023 \\pm 0.002$}\\\\\n    \\hline\n\\end{tabular}\n%\\vspace{2mm}\n\\end{table*}",
            "tab:spurious": "\\begin{table}[t]\n\\small\n\\centering\n \\caption{Study on synthetic data about the adverse effects of incorrect causal model $\\mathcal{M}_1$.} % resulted from spurious correlations\n% \\vspace{-2mm}\n \\label{tab:spurious}\n \\def\\arraystretch{1.2}% \n  \\begin{tabular}{l||cc||cc}\n\\hline\nMethod     & RMSE  & MAE  & MMD & Wass \\\\\n    \\hline\n%   Constant & $1.50 \\pm 0.01$ & $1.04 \\pm 0.01$ & $0.00\\pm 0.00$   & $0.00\\pm 0.00$  \\\\\n%Full     & $0.91 \\pm 0.03$ & $0.72 \\pm 0.03$ & $24.52 \\pm 2.10$ & $5.08 \\pm 0.02$ \\\\\n%Unaware  & $1.10 \\pm 0.01$ & $0.78 \\pm 0.02$ & $16.63 \\pm 1.56$ & $4.39 \\pm 0.02$ \\\\\n%\\hline\n\\cfpa~(true)    & $1.34 \\pm 0.01$ & $0.88 \\pm 0.01$ & $8.42 \\pm 0.70$  & $3.07 \\pm 0.01$ \\\\\n\\cfpa~ (false) & $\\underline{1.30 \\pm 0.01}$ & $\\underline{0.83\\pm 0.02}$  & $10.11\\pm0.52$   & $3.79\\pm 0.03$  \\\\\n\\cfpb~(true)    & $1.32 \\pm 0.01$ & $0.87 \\pm 0.01$ & $8.48 \\pm 0.83$  & $3.32 \\pm 0.02$ \\\\\n\\cfpb~(false) & $\\bm{1.29\\pm0.01}$   & $\\bm{0.81\\pm0.01}$   & $10.94\\pm 0.61$  & $3.84\\pm0.02$   \\\\\n\\hline\n%ERM      & $1.09 \\pm 0.02$ & $0.76 \\pm 0.02$ & $15.59\\pm 1.03$  & $4.56 \\pm 0.04$ \\\\\n%IRM      & $1.20 \\pm 0.02$ & $0.79 \\pm 0.02$ & $14.83\\pm 0.82$  & $4.04\\pm0.02$   \\\\\n%ERM-MMD  & $1.30 \\pm 0.01$ & $0.83 \\pm 0.01$ & $7.83 \\pm 0.05$  & $2.76 \\pm 0.02$ \\\\\n%\\hline\n\\mymodel-M   & $1.32 \\pm 0.01$ & $0.87 \\pm 0.02$ & \\underline{$7.52\\pm 0.08$}   & \\underline{$2.63\\pm 0.02$}  \\\\\n\\mymodel-A   & $1.31 \\pm 0.01$ & $0.85 \\pm 0.03$ & \\bm{$7.49\\pm0.05$}    & \\bm{$2.58\\pm0.01$}  \\\\\n    \\hline\n\\end{tabular}\n%\\vspace{3mm}\n\\end{table}",
            "tab:imbalance": "\\begin{table}[t]\n\\centering\n\\small\n \\caption{Study on synthetic data regarding the adverse effects of incorrect causal model $\\mathcal{M}_2$.} %resulted from imbalanced data\n %\\vspace{-2mm}\n \\label{tab:imbalance}\n \\def\\arraystretch{1.2}% \n  \\begin{tabular}{l||cc||cc}\n    \\hline\n    \\multirow{2}{*}{Method} & \\multicolumn{2}{c||}{$S\\leftarrow 0$ and $S\\leftarrow 1$} &  \\multicolumn{2}{c}{$S\\leftarrow 0$ and $S\\leftarrow 2$} \\\\ \\cline{2-5}\n     & MMD  & WASS  & MMD & Wass  \\\\\n    \\hline\n    \\cfpa~(true) & $\\underline{6.05 \\pm 0.02}$ &\\underline{$1.10\\pm 0.02$}  & $7.97 \\pm 0.03$ & $2.55\\pm 0.02$   \\\\\n    \\cfpa~(false) &$6.63 \\pm 0.09$  & $1.24 \\pm 0.04$ & $9.33 \\pm 1.00$  & $3.62 \\pm 0.01$ \\\\\n    \\cfpb~(true) &$6.34 \\pm 0.07$ & $1.13\\pm0.03$   & $8.31\\pm 0.98$  & $2.84\\pm 0.03$ \\\\\n    \\cfpb~(false) &$6.83 \\pm 0.08$  & $1.35 \\pm 0.05$ & $9.92 \\pm 1.01$  & $3.98 \\pm 0.02$ \\\\\n    \\hline\n    \\mymodela & \\underline{$6.12 \\pm 0.04$}  & \\underline{$1.13 \\pm 0.02$} & \\underline{$7.94 \\pm 0.06$}  & \\underline{$2.52 \\pm 0.01$}  \\\\\n    \\mymodelb & \\bm{$6.05 \\pm 0.03$}  & \\bm{$1.11 \\pm 0.03$} & \\bm{$7.42 \\pm 0.04$}  & \\bm{$2.49\\pm 0.01$}\\\\\n    \\hline\n\\end{tabular}\n%\\vspace{-3mm}\n\\end{table}",
            "tab:imbalance_complete": "\\begin{table}[H]\n\\centering\n \\caption{Study on synthetic data regarding the adverse effects of incorrect causal model $\\mathcal{M}_2$.} %resulted from imbalanced data\n \\vspace{-2mm}\n \\label{tab:imbalance_complete}\n  \\begin{tabular}{l||cc}\n    \\hline\n    \\multirow{2}{*}{Method}  & \\multicolumn{2}{c}{$S\\leftarrow 2$ and $S\\leftarrow 3$}\\\\ \\cline{2-3}\n     & MMD  & WASS  \\\\\n    \\hline\n%    Constant & $0.000\\pm 0.000$ & $0.000\\pm 0.000$ & $0.000\\pm 0.000$ & $0.000\\pm 0.000$ & $0.000 \\pm 0.000$  & $0.000\\pm 0.000$  \\\\\n%    Full & $15.649\\pm 2.203$  & $3.498 \\pm 0.035$ & $25.932 \\pm 2.057$ & $6.345 \\pm 0.045$ & $27.421 \\pm 1.493$ & $6.683 \\pm 0.053$ \\\\\n%    Unaware & $13.531 \\pm 1.984$ & $2.873 \\pm 0.052$ & $18.336 \\pm 1.495$ & $5.477 \\pm 0.031$ & $19.334 \\pm 1.050$ & $5.751 \\pm 0.034$ \\\\\n%    \\hline\n    \\cfpa~(true)  & $8.407 \\pm 0.810$  & $2.900\\pm0.092$   \\\\\n    \\cfpa~(false) & $10.317 \\pm 1.011$ & $3.780 \\pm 0.052$ \\\\\n    \\cfpb~(true) & $8.793 \\pm 0.927$  & $3.136 \\pm 0.040$ \\\\\n    \\cfpb~(false) & $10.337 \\pm 1.002$ & $3.864 \\pm 0.030$ \\\\\n    \\hline\n%     ERM & $12.562 \\pm 1.073$ & $2.942 \\pm 0.044$ & $18.527 \\pm 1.392$ & $5.086 \\pm 0.022$ & $17.492 \\pm 1.283$ & $5.380 \\pm 0.023$ \\\\\n% IRM & $11.367 \\pm 0.803$ & $2.243 \\pm 0.038$ & $17.323 \\pm 0.927$ & $4.831 \\pm 0.008$ & $15.806 \\pm 0.905$ & $4.753 \\pm 0.035$ \\\\\n% ERM-MMD & $6.621 \\pm 0.082$  & $1.139 \\pm 0.019$ & $8.110 \\pm 0.085$  & $2.946 \\pm 0.010$ & $8.315 \\pm 0.052$  & $2.859 \\pm 0.011$ \\\\\n%     \\hline\n    \\mymodela &  \\underline{$8.108 \\pm 0.024$}  & \\underline{$2.860 \\pm 0.004$} \\\\\n    \\mymodelb &  \\bm{$7.902 \\pm 0.055$}  & \\bm{$2.761 \\pm 0.005$}\\\\\n    \\hline\n\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:example1": "\\begin{figure}[t]\n\\centering\n        \\includegraphics[width=.47\\textwidth]{figures/example1.pdf}\n        %\\vspace{-0.05in}\n       % \\vspace{-1mm}\n        \\caption{An illustrative example of incorrect causal models.} \n        %\\vspace{-0.2in}\n         \\label{fig:example1}\n         %\\vspace{-2mm}\n \\end{figure}",
            "fig:casestudy": "\\begin{figure*}[t]\n\\centering\n  \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.in]{figures/case_1.pdf}\n       % \\vspace{-2mm}\n        \\caption{}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.in]{figures/case_2.pdf}\n        %\\vspace{-2mm}\n        \\caption{}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.in]{figures/case_3.pdf}\n       % \\vspace{-2mm}\n        \\caption{}\n    \\end{subfigure}\n       %  \\vspace{-2mm}\n    \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.in]{figures/case_4.pdf}\n       % \\vspace{-2mm}\n        \\caption{}\n    \\end{subfigure}\n    % \\vspace{2mm}\n  \\caption{Case studies of different kinds of variables in causal models. Each white (gray) node denotes an observed (unobserved) variable, each arrow denotes a causal relationship, and each dashed arrow denotes a possible causal relationship. $S,Y,U$ denotes the sensitive attribute, the prediction target, and the unobserved variable, respectively. $X_1$ is a causal variable of $Y$ and is a descendent of $S$, $X_0$ is a causal variable of $Y$ and is non-descendent of $S$, and $X_2$ is a variable with spurious correlations to $Y$.}\n  \\label{fig:casestudy}\n\\end{figure*}",
            "fig:causalmodel_real": "\\begin{figure}[t]\n\\centering\n  \\begin{subfigure}[b]{0.232\\textwidth}\n        \\centering\n        \\includegraphics[height=1.in]{figures/law_true.pdf}\n        \\caption{Law school}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.24\\textwidth}\n        \\centering\n        \\includegraphics[height=1.in]{figures/adult_true.pdf}\n        \\caption{Adult}\n    \\end{subfigure}\n     %\\vspace{-7mm}\n  \\caption{The ground truth causal models of two real-world datasets Law School and Adult.}\n  \\label{fig:causalmodel_real}\n%\\vspace{3mm}\n\\end{figure}",
            "fig:causalmodel_sythetic": "\\begin{figure}[t]\n\\centering\n\\begin{subfigure}[b]{0.15\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/synthetic_true.pdf}\n        \\caption{$\\mathcal{M}$}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/synthetic_false_1.pdf}\n        \\caption{$\\mathcal{M}_1$}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/synthetic_false_2.pdf}\n        \\caption{$\\mathcal{M}_2$}\n    \\end{subfigure}\n%   \\begin{subfigure}[b]{0.19\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.9in]{figures/law_true.pdf}\n%         \\caption{True causal model $\\mathcal{M}_1$}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.19\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.9in]{figures/adult_true.pdf}\n%         \\caption{False causal model $\\mathcal{M}_2$}\n%     \\end{subfigure}\n     %\\vspace{-3mm}\n  \\caption{The true causal model ($\\mathcal{M}$) and two incorrect causal models ($\\mathcal{M}_1$ and $\\mathcal{M}_2$) of the synthetic dataset.}\n  \\label{fig:causalmodel_sythetic}\n%\\vspace{-6mm}\n\\end{figure}",
            "fig:ablation": "\\begin{figure}[t]\n\\centering\n  \\begin{subfigure}[b]{0.24\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/ablation_prediction.pdf}\n        %\\vspace{-0.2in}\n        \\caption{Prediction}\n        %\\vspace{-1mm}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/ablation_fairness.pdf}\n        %\\vspace{-0.2in}\n        \\caption{Fairness}\n        %\\vspace{-1mm}\n    \\end{subfigure}\n  \\caption{Ablation Study on Synthetic Dataset.}\n%  \\vspace{-0.15in}\n  \\label{fig:ablation}\n%\\vspace{-2mm}\n\\end{figure}",
            "fig:param": "\\begin{figure}[t]\n\\centering\n  \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/alpha.pdf}\n        \\caption{Vary $\\alpha$}\n        %\\vspace{2mm}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/K.pdf}\n        \\caption{Vary $K$}\n        %\\vspace{2mm}\n    \\end{subfigure}\n     \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/beta.pdf}\n        %\\vspace{-0.2in}\n        \\caption{Vary $\\beta$}\n        %\\vspace{-1mm}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.1in]{figures/lamda.pdf}\n        %\\vspace{-0.2in}\n        \\caption{Vary $\\lambda$}\n        %\\vspace{-1mm}\n    \\end{subfigure}\n   \\vspace{-3mm}\n  \\caption{Performance of \\mymodel~with different settings of hyperparameters.}\n%  \\vspace{-0.15in}\n  \\label{fig:param}\n%\\vspace{-2mm}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\nP(\\hat{Y}_{S\\leftarrow s}=y|X=x,S=s) = P(\\hat{Y}_{S\\leftarrow s'}=y|X=x,S=s),\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\mathcal{L}_r = \\mathbb{E}_{q(H|X,Y)}[-\\log(p(X,Y|H,S))] + \\text{KL}[q(H|X,Y)\\|p(H)],\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\min \\mathcal{L}_r + \\alpha \\frac{1}{N_p}\\sum\\nolimits_{s\\ne s'}MMD(P(H|s), P(H|s')),\n\\label{eq: loss_mmd}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\min_{\\Psi(\\cdot)}\\max_{h(\\cdot)}  \\mathcal{L}_{r} + \\alpha'\\frac{1}{|\\mathcal{S}|} \\sum\\nolimits_{s\\in\\mathcal{S}}\\mathbb{E}_{X^s,S^s}[\\log P(h(H)=s)],\n\\label{eq:loss_adversarial}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\mathcal{L}_c =\\! \\frac{1}{|\\mathcal{S}|-1}\\sum_{s'\\ne s} d(Z, Z^{CF}_{s'})=\\!\\frac{1}{|\\mathcal{S}|-1}\\sum_{s'\\ne s} d(\\Phi(X), \\Phi(X^{CF}_{s'})),\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\mathcal{L}_{IRM}^s =R^s(g\\circ\\Phi) + \\lambda\\left\\|\\bigtriangledown_{w|w=1.0}R^s(w\\cdot (g\\circ\\Phi))\\right\\|_{2}^2,\n\\label{eq:irm1}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mathcal{L} = \\frac{1}{|\\mathcal{S}|}\\sum\\nolimits_{s\\in\\mathcal{S}}\\mathcal{L}^s_{IRM} + \\beta\\mathcal{L}_c,\n    \\label{eq:loss_f}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    X_2 = Y + \\mathcal{N}(0,\\sigma_{S,2}^2), \n    \\label{eq:synthetic}\n\\end{equation}"
        },
        "git_link": "https://github.com/anonymous202008/ICFP"
    }
}