{
    "meta_info": {
        "title": "Molecular Property Prediction Based on Graph Structure Learning",
        "abstract": "Molecular property prediction (MPP) is a fundamental but challenging task in\nthe computer-aided drug discovery process. More and more recent works employ\ndifferent graph-based models for MPP, which have made considerable progress in\nimproving prediction performance. However, current models often ignore\nrelationships between molecules, which could be also helpful for MPP. For this\nsake, in this paper we propose a graph structure learning (GSL) based MPP\napproach, called GSL-MPP. Specifically, we first apply graph neural network\n(GNN) over molecular graphs to extract molecular representations. Then, with\nmolecular fingerprints, we construct a molecular similarity graph (MSG).\nFollowing that, we conduct graph structure learning on the MSG (i.e.,\nmolecule-level graph structure learning) to get the final molecular embeddings,\nwhich are the results of fusing both GNN encoded molecular representations and\nthe relationships among molecules, i.e., combining both intra-molecule and\ninter-molecule information. Finally, we use these molecular embeddings to\nperform MPP. Extensive experiments on seven various benchmark datasets show\nthat our method could achieve state-of-the-art performance in most cases,\nespecially on classification tasks. Further visualization studies also\ndemonstrate the good molecular representations of our method.",
        "author": "Bangyi Zhao, Weixia Xu, Jihong Guan, Shuigeng Zhou",
        "link": "http://arxiv.org/abs/2312.16855v1",
        "category": [
            "cs.LG",
            "q-bio.BM"
        ],
        "additionl_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nThe accurate prediction of molecular properties is a critical task in the field of drug discovery. By utilizing computational methods, this task can be accomplished with great efficiency, reducing both time and expense associated with identifying drug candidates. This is particularly important considering that the average cost of developing a new drug is currently estimated to be approximately \\$2.8 billion~\\cite{fleming2018artificial,wieder2020compact} and the development period lasts a dozen of years, let alone the high risk of clinical failure~\\cite{sarkar2023artificial}. Naturally, a molecule can be abstracted as a topological graph, where atoms are treated as nodes and bonds are viewed as edges. In the past few years, deep graph learning methods, especially various graph neural networks (GNNs) have been applied in this field, offering effective molecular graph representations for accurate molecular property prediction~\\cite{duvenaud2015convolutional,song2020communicative,sun2020graph}. In GNNs, nodes iteratively update their representations after aggregating information from their neighbours and a final graph-pooling layer will generate a graph representation for the molecule. Up to now, various message passing layers have been proposed and applied, including GAT~\\cite{velivckovic2017graph}, MPNN~\\cite{gilmer2017neural} and GIN~\\cite{xu2018powerful}. And later studies further considered to integrate edge features into the passing messages in order to improve the expressive power of their models, like DMPNN~\\cite{yang2019analyzing} and CMPNN~\\cite{song2020communicative}.\n\nDespite the considerable progress, most of recent studies focus only on the message passing within individual molecules. The relationships among molecules are often ignored, which could also play an important role in property prediction~\\cite{wang2021hierarchical}. One relatively easy and effective way is to construct a relationship graph among molecules using the structural similarity, because a critical assumption of medicinal chemistry is that structurally similar molecules tend to have similar biological activities~\\cite{johnson1990concepts}. For example, fingerprint (carrying the structural information of the molecules) similarity search is often used in virtual screening~\\cite{muegge2016overview}. However, this assumption is not always true since a phenomenon called activity cliff (AC) exits. An AC is defined as a pair of structurally similar compounds with a large potency difference against a given target~\\cite{maggiora2006outliers,stumpfe2012exploring,stumpfe2014recent,stumpfe2020advances}. Thus, the relationship graph constructed by structural similarity may be not ``perfect'' for the downstream tasks. We need to take certain measures to enhance this relationship graph if we want to make full and proper use of it.\n\nTo address these problems above, we propose a novel two-level graph representation learning method for molecular property prediction, called GSL-MPP. Our method operates in a two-level molecular graph representation framework: (i) Atom-level molecular graph representation where molecular graphs composed of atoms and bonds represent the intra-structures of molecules; and (ii) molecule-level graph representation where inter-molecule similarity graph (MSG in short) is constructed by fingerprint similarity to encode similarities between molecules that allows effective label propagation among connected similar molecules. Intra-molecular representation is done by GNNs, and inter-molecular representation is finished by graph structure learning (GSL). This two-level graph representation enables us to comprehensively exploit both intra-molecule and inter-molecule information to get better molecular representations and overcome (to some degree) the AC problem, consequently boosting MPP performance.\n\nSpecifically, we applies metric-based iterative graph structure learning in our method. The MSG structure and molecular embeddings are updated for $T$ times. During each iteration, GSL-MPP learns a better MSG structure based on better molecular embeddings, and in turn, learns better molecular embeddings with a better MSG structure. Besides, during the training process, we also add a GSL-specific loss to the common supervised loss for better MSG structure learning on both classification tasks and regression tasks. Our method is evaluated on 7 benchmark datasets including 4 classification tasks and 3 regression tasks.  Experimental results show that our model can achieve state-of-the-art performance in most cases. Ablation studies show that the combination of fingerprint similarity and GSL is of particular effectiveness.\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n",
                "subsection 2.1": {
                    "name": "Molecular Property Prediction",
                    "content": "\nMost methods for predicting molecular properties can be summarized using a general framework. In this framework, we first transform the input molecule $m$ into a specific-length vector $h$ using a representation function, $h = g(m)$. Then another prediction function is used to predict a specific property $y$ based on $h$, $y = f(h)$. During this period, a good molecular representation is of vital importance to address molecular property prediction problems. \n\n\nAt early stages, traditional chemical fingerprints such as Extended Connectivity Fingerprints (ECFP)~\\cite{Morgan1965MorganAlgorithm,glen2006circular} are used to encode a molecule to a vector. These fingerprints could carry the structural information of the molecules~\\cite{nguyen2023perceiver}. \n\n\nIn order to improve the expressive power, recent works started to use the graph neural networks (GNNs) to acquire graph-level representation as molecular embedding. Examples include graph convolutional network (GCN)~\\cite{duvenaud2015convolutional}, graph attention network (GAT)~\\cite{velivckovic2017graph}, message passing neural network (MPNN)~\\cite{gilmer2017neural} and graph isomorphism network (GIN)~\\cite{xu2018powerful}.Later works extend the MPNN framework to consider bond information during message passing procedure, like DMPNN~\\cite{yang2019analyzing} and CMPNN~\\cite{song2020communicative}. Besides, CD-MVGNN~\\cite{ma2022cross} also considers both atom-level and bond-level message passing, and a cross-dependency mechanism is designed to ensure these two views rely on information from each other during feature updates, thereby enhancing expressive capabilities. \n\n\nRecently, many efforts have also been made to integrate transformer to graph neural network. Molecule Attention Transformer(MAT)~\\cite{maziarka2020molecule} attempts to  incorporate node distance and graph structural information when calculating attention scores. Another work Grover~\\cite{rong2020self} combines message-passing networks with the Transformer architecture to create a more expressive molecular encoder that captures information at two hierarchical levels. CoMPT~\\cite{chen2021learning} is also built upon the Transformer architecture. Unlike previous graph Transformer models that treated molecules as fully connected graphs, this approach employs a message diffusion mechanism inspired by heat diffusion phenomena to integrate information from the adjacency matrix, alleviating the over-smoothing issue. \n\n\nHowever, these methods only focus on the structure of a single molecule, while ignoring the important role of inter-molecular relationships for property prediction.\n"
                },
                "subsection 2.2": {
                    "name": "Graph Structure Learning",
                    "content": "\nThe expressive power of GNNs often depends on the input graph structure. However, the initial graph structure is not always optimal for downstream tasks. On the one hand, the original graph is constructed from the original feature space, which may not reflect the \"true\" graph topology after feature extraction and transformation. On the other hand, errors can also occur when data is measured or collected, making the graph noisy or even incomplete. Graph structure learning (GSL) is one of the methods that can effectively solve this problem, through learning and optimizing the graph structure~\\cite{zhu2021survey}. Recently, ~\\cite{chen2020iterative} proposed the method of iterative deep graph learning (IDGL) for jointly and iteratively learning graph structure and node embeddings in the field of natural language processing (NLP). \nIt was later used by ~\\cite{wang2021property} for few-shot molecular property prediction. Compared to ~\\cite{wang2021property}, our method is not based on few-shot situation and the datasets and baselines we choose are not for few-shot either. Besides, The specific implementation of GSL is different. More importantly, we try to construct an initial graph between molecules before we apply GSL, which is confirmed to be necessary in ablation study.\n\n"
                }
            },
            "section 3": {
                "name": "Method",
                "content": "\\label{sec:method}\n",
                "subsection 3.1": {
                    "name": "Overview",
                    "content": "\nThe structure of our method GSL-MPP is illustrated in Fig.~\\ref{fig:model}, which is operated on a two-level graph learning framework. Specifically, the two-level graph learning framework consists of (i) the lower level: atom-level molecular graphs encoded by GNN to extract the initial molecular representations, and (ii) the upper level: a molecule-level similarity graph, on which graph structure learning (GSL) is performed to iteratively learn the final molecular embeddings, where inter-molecular relationships are exploited. \n\nThe workflow of GSL-MPP is as follows: (1) molecule graphs are first encoded by a GNN to obtain initial molecular embeddings. Meanwhile, molecules are represented as feature vectors using fingerprints. (2) With the molecular feature vectors, the initial molecular similarity graph (MSG) is constructed, where each node is a molecule  initially represented by the above GNN embeddings, and each edge attached with a weight --- the similarity between the two corresponding molecules. (3) GSL is performed on the MSG, which \n iteratively updates the molecular embeddings and the graph structure. (4) The final molecular embeddings are used for property prediction. %The complete algorithm of our method is shown in Algorithm 1.\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Molecular Graph Embedding",
                    "content": "\nHere, we describe how to represent a molecular graph as an initial vector by GNN. \nA molecule $m$ can be abstracted as an attributed graph where $G_m = (\\mathcal{V}, \\mathcal{E})$, in which $\\left| \\mathcal{V}\\right| = n_v$ refers to a set of $n_v$ atoms (nodes) and $\\left| \\mathcal{E}\\right| = n_e$ refers to a set of $n_e$ bonds (edges) in the molecule. $x_v$ are used to represent the initial feature of node $v$ and $\\mathcal{N}_v$ denotes the set of neighbors of node $v$.\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Node embedding",
                        "content": "\nWe use Graph Isomorphism Network (GIN)~\\cite{xu2018powerful} as intra-molecule GNN to extract each node's embedding:\n\\begin{equation}\n    h_v^{(k)} = MLP^{(k)} ((1 + \\epsilon^{(k)}) \\cdot h_v^{(k-1)} + \\sum_{u\\in \\mathcal{N}(v)} h_u^{(k-1)}),\n\\end{equation}\nwhere MLP means multi-layer perceptron, $h_v^{(k)}$ is the representation vector of node $v$ at the $k$-th layer. We initialize $h_v^{(0)} = x_v$, and $\\epsilon$ is a learnable parameter.\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Graph pooling",
                        "content": "\nAfter gaining each node's embedding, a READOUT operation is applied to get the initial molecular embedding $h_g$:\n\\begin{equation}\n    h_g = READOUT(\\left\\{ \\left. h_v^{k}\\right| v \\in G \\right\\} \\left| k = 0, 1, ..., K \\right.).\n\\end{equation}\n\n"
                    }
                },
                "subsection 3.3": {
                    "name": "Constructing Molecular Similarity Graph (MSG)",
                    "content": "\nOur inter-molecule graph reflects the relationships between molecules, where each node indicates a molecule, and each edge means the relationship between two molecules. As shown in Fig.~\\ref{fig:model}, the initial feature vector of each node is the molecule's embedding obtained by GNN (their embedding matrix is denoted as $X_r$), and the initial adjacency matrix $A^{(0)}$ is calculated by the structural similarity between molecules. Here, we calculate the structural similarity between molecules based on their Extended Connectivity Fingerprints (ECFP)~\\cite{Morgan1965MorganAlgorithm}. \n\nECFPs are circular fingerprints, possessing several beneficial characteristics: 1) They can be calculated fast; 2) They are not predefined and can capture an almost limitless range of molecular characteristics including stereochemical information; 3) They indicate the presence of specific substructures, facilitating interpretation of computation results~\\cite{rogers2010extended}.\nSpecifically, we get each molecule's ECFP and calculate the Tanimoto Coefficient as the similarity score. A hyperparameter $\\epsilon_{tc}$ acts as a threshold to obtain a sparse matrix. That is, we mask off those elements in the adjacency matrix that are smaller than $\\epsilon_{tc}$. We apply molecular fingerprints to construct $A^{(0)}$ because it contains useful structural information~\\cite{nguyen2023perceiver} and could offer an informative initial inter-molecule graph.\n\n"
                },
                "subsection 3.4": {
                    "name": "Structure Learning on Molecular Similarity Graph",
                    "content": "\nAs we have discussed, this similarity graph constructed above may not be good enough for downstream tasks, therefore here graph structure learning is employed to enhance the graph by exploiting inter-molecule relationships. Specifically, initial matrix built with fingerprint similarity only measure structural similarity between molecules and may not ``perfectly'' reflect true molecular property similarity, so we use GSL to refine it. The core of GSL is the structure learner that could be grouped into three types: (1) Metric-based approaches use a metric function like cosine similarity on pairwise node embeddings to calculate edge weights; (2) Neural approaches employ neural networks to infer edge weights; and (3) Direct approaches treat all elements of the adjacency matrix as learnable parameters~\\cite{zhu2021survey}. \n\n\nIn this paper, following IDGL~\\cite{chen2020iterative}, we adopt the metric-based approach and employ $m$-perspective weighted cosine similarity as the metric function:\n\\begin{equation}\n    s_{ij}^p = \\cos (w_p \\odot v_i, w_p \\odot v_j), \\quad s_{ij} = \\frac{1}{m} \\sum_{p=1}^{m} s_{ij}^p,\n\\label{eq:similarity-metric} \n\\end{equation}\nwhere $s_{ij}^p$ estimates the cosine similarity between nodes $v_i$ and $v_j$, each perspective $p$ considers one part of the semantics contained in the vectors and corresponds to a learnable weight vector $w_p$. The obtained $s_{ij}$ is the entry in row $i$ and column $j$ of the newly learned adjacency matrix $A$. Also the $\\epsilon$-neighborhood sparsification technique is applied to obtaining a sparse and non-negative adjacency matrix.    \n\nThe node embeddings $H_r$ and the adjacency matrix $A$ will be alternately refined for $T$ times. At the $t$-th iteration, $A^{(t)}$ is calculated from the previously updated node embeddings $H_r^{(t-1)}$ by Eq.~(\\ref{eq:similarity-metric}). Then we use the learned graph structure $A^{(t)}$ as supplementary to optimize the initial graph $A^{(0)}$:\n\\begin{equation}\n    \\widetilde A^{(t)} = \\lambda A^{(0)} + (1 - \\lambda)\\left\\{ \\eta A^{(t)} + (1 - \\eta) A^{(1)}   \\right\\},\n\\label{eq:update-adj} \n\\end{equation}\nwhere $A^{(1)}$ is the adjacency matrix learned from $X_r$ at the 1-st iteration in order to maintain the initial node information. $\\lambda$ and $\\eta$ are hyperparameters.\n\nAfter learning the adjacency matrix, we employ an $L$-layer inter-molecule GNN to learn node embeddings, and in the $l$-th layer, $H_r^{(t, l)}$ is updated by\n\\begin{equation}\n    H_r^{(t, l)} = ReLU (\\widetilde A^{(t)} H_r^{(t, l-1)} W_r^{(l)}),\n\\label{eq:update-embeddings} \n\\end{equation}\n$H_r^{(t)} = H_r^{(t, L)}$ is the final node embeddings in this iteration and $H_r^{(t, 0)} = X_r$. \n\n"
                },
                "subsection 3.5": {
                    "name": "Loss Function",
                    "content": "\nAfter $T$ rounds of iteration, the node (molecule) embeddings $H_r^{(T)}$ represent the final molecular representations. Based on this, predictions can be made for specific property $\\hat y$ with a fully connected layer (FC) as follows:\n\\begin{equation}\n    \\hat y = FC(H_r^{(T)}).\n\\label{eq:prediction} \n\\end{equation}\n\nThe whole loss function used in our method consists of two parts: the label prediction loss and the GSL-specific loss. The label prediction loss function $\\mathcal{L}_{pred}$ is obtained in a manner similar to existing methods:\n\\begin{equation}\n    \\quad \\mathcal{L}_{pred} = \\ell (\\hat y, y),\n    \\label{eq:prediction-loss} \n\\end{equation}\nwhere $\\hat y$ represents the predicted value, $y$ is the ground truth, and $\\ell$ represents the loss function used. In classification tasks, it is the Cross Entropy Loss, and in regression tasks, it is the Mean Squared Error Loss.\n\nSince the quality of the learned inter-molecule graph structure is of great importance for our method, we further design a GSL-specific loss, hoping that the learned adjacency matrix does not contain wrong edges.\nWe use $S_{train}$ to represent molecules in training set and $\\widetilde A^{(T)}$ to represent the final adjacency matrix after being refined $T$ times. In classification tasks, there exists a ground truth for the matrix, $A^*$($A^*_{ij} = 1$ if $y_i = y_j$ else 0), i.e., molecules with the same label should be connected by edges. Thus, we define the GSL-specific loss as\n\\begin{equation}\n    \\mathcal{L}_{GSL} = \\sum_{x_i, x_j \\in S_{train}} (\\widetilde A_{ij}^{(T)} - A_{ij}^*)^2\n    \\label{eq:gsl-loss-clf} \n\\end{equation}\nHowever, in regression tasks, the prediction of a molecule is a real value and no native ground truth exists. We have to define it by ourselves. For the convenience of calculation, we only consider those molecular pairs with large difference (beyond a certain threshold $\\epsilon_y$) in predicted values when calculating the GSL-specific loss:\n\\begin{equation}\n    \\mathcal{L}_{GSL} = \\sum_{x_i, x_j \\in S_{train}} (\\widetilde A^{(T)}_{ij})^2 , \\quad  x_i, x_j \\quad satisfy \\quad \\left| y_i - y_j \\right| > \\epsilon_y.\n    \\label{eq:gsl-loss-reg}\n\\end{equation}\nThe whole loss function combines both the task prediction loss and the GSL-specific loss, that is, $\\mathcal{L} = \\mathcal{L}_{pred} + \\mathcal{L}_{GSL}$.\n\n"
                },
                "subsection 3.6": {
                    "name": "Algorithm",
                    "content": "\nThe algorithm of our method is presented in Algorithm 1. After obtaining the initial molecular embeddings and constructing the initial inter-molecule similarity graph MSG (corresponding to the adjacency matrix), $T$ iterations of GSL are applied. During each iteration, the adjacency matrix is refined based on the node embeddings gained in the last iteration,  while the node embeddings are updated based on adjacency matrix obtained in the last iteartion.\n\n\\begin{algorithm}\n\\caption{The GSL-MPP algorithm}\n\\begin{algorithmic}[1]\n\\STATE Obtain initial molecular embedding $h_{g, i}$ for each molecule $m_i$ by a graph-based molecular encoder (an intra-molecule GNN);\n\\STATE $X_r \\leftarrow$ embedding matrix of all $h_{g, i}$;\n\\STATE $H_r^{(0)} \\leftarrow X_r$;\n\\STATE Construct an initial molecule similarity matrix $A^{(0)}$ using molecular fingerprint similarity;\n\\FOR {$t = 1$ to $T$}          \n    \\STATE Use GSL to learn a refined adjacency matrix $A^{(t)}$ by $H_r^{(t - 1)}$ using Eq.~(\\ref{eq:similarity-metric});\n    \\STATE Combine initial and refined adjacency matrices $A^{(0)}$ and $A^{(t)}, A^{(1)}$ to obtain $\\widetilde A^{(t)}$ by Eq.~(\\ref{eq:update-adj});\n    \\STATE Initialize node embeddings by $H_r^{(t, 0)} = X_r$;\n    \\FOR {$l = 1$ to $L$}\n        \\STATE Update node embedding $H_r^{(t, l)}$ by inter-molecule GNN using Eq.~(\\ref{eq:update-embeddings});\n    \\ENDFOR\n    \\STATE $H_r^{(t)} \\leftarrow H_r^{(t, L)}$;\n\\ENDFOR\n\\STATE Obtain prediction $\\hat y$ using $H_r^{(T)}$ by Eq.~(\\ref{eq:prediction});\n\\IF{in training phase}\n\\STATE Calculate $\\mathcal{L}_{pred}$ by Eq.~(\\ref{eq:prediction-loss}) and $\\mathcal{L}_{GSL}$ by Eq.~(\\ref{eq:gsl-loss-clf}) or Eq.~(\\ref{eq:gsl-loss-reg}) for $S_{train}$;\n\\STATE $\\mathcal{L} \\leftarrow \\mathcal{L}_{pred} + \\mathcal{L}_{GSL}$;\n\\STATE Back-propagate $\\mathcal{L}$ to update model weights;\n\\ENDIF\n\\end{algorithmic}\n\\end{algorithm}\n\n\n"
                }
            },
            "section 4": {
                "name": "Performance Evaluation",
                "content": "\\label{sec:performance}\n% In this section, we first evaluate the proposed model and compare it with different SOTAs of molecular property classification\n% and regression. We then conduct ablation studies on each component of our model. Finally, we also present the visualization of some molecular representations. \n\n",
                "subsection 4.1": {
                    "name": "Experimental Setting",
                    "content": "\n\\paragraph{Datasets.}We use 7 benchmark datasets from MoleculeNet~\\cite{wu2018moleculenet} for experiments, among which 4 are classification tasks and 3 are regression\ntasks. Specifically, BACE is about the binding results of several inhibitors; BBBP is the blood\u2013brain barrier penetration dataset;\nSIDER and Clintox are two multi-task datasets corresponding to side effects and toxicity respectively; ESOL, Lipophilicity and Freesolv are regression datasets about physical chemistry properties.\n\nScaffold splitting of ~\\cite{yang2019analyzing} is adopted to split the\ndatasets into training, validation, and test, with a 0.8/0.1/0.1\nratio, which is more empirical and challenging than random splitting. Following previous works~\\cite{ma2022cross,rong2020self}, we use three independent runs on three random-seeded scaffold splitting for each dataset.\n\n\\paragraph{Baselines.} We compare our method against 12 baselines. %Most of these models were introduced in MoleculeNet ~\\cite{wu2018moleculenet} and GROVER \n%as follows: \nTF\\_Robust~\\cite{ramsundar2015massively} is a DNN-based multitask framework that takes molecular fingerprints\nas input. GCN~ (GraphConv)~\\cite{duvenaud2015convolutional}, Weave~\\cite{kearnes2016molecular} and SchNet~\\cite{schutt2017schnet} are three graph convolutional models. MPNN~\\cite{gilmer2017neural} and its variants MGCN~\\cite{lu2019molecular}, DMPNN~\\cite{yang2019analyzing} and CMPNN~\\cite{song2020communicative} are models considering the edge features during message passing. AttentiveFP~\\cite{xiong2019pushing} is an extension of the graph attention network. GROVER~\\cite{rong2020self} and CoMPT~\\cite{chen2021learning} are two transformer-based models. Here, GROVER is compared  without the pretrain process for a fair comparison. CoMPT is a transformer-based model utilizing both nodes and edges information in message passing process while CD-MVGNN~\\cite{ma2022cross} also constructs two views for atoms and bonds respectively.\n\n\\paragraph{Evaluation metrics.} Following the evaluation criteria adopted by these baseline models, we use AUC-ROC to evaluate the performance of classification tasks, and RMSE to evaluate regression tasks.\n\n\\paragraph{Implementation details.} Our model apply a polynomial decay scheduler to the learning rate with two linear increase warm-up epochs and polynomial decay afterward. The power of polynomial decay is set to 1, indicating a linear decay. The final learning rate is 1e-9 and the max\\_epoch is 300. For the proposed model, on each dataset we try different hyper-parameter combinations, and take the hyper-parameter set with the best result. While building the initial inter-molecule graph, the radius of used ECFP is 2. The threshold of GSL-specific loss for regression tasks ($\\epsilon_y$) is 0.01. More details of the hyper-parameter setting in the implementation of our model are presented in Table \\ref{table:hyper-parameter}. \n\n  \n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Performance Comparison",
                    "content": "\nTable \\ref{table:experiment} presents the results of our model and the baselines on all datasets. boldfaced values are the best results, and underlined values are the 2nd best results. From  \\ref{table:experiment} we have the following observations: (1) Compared to the SOTA model CD-MVGNN, Our model performs better on 3/4 classification datasets with a 2.4\\% AUC lift on BBBP. Since our model is based on a simple GIN without complicated message passing procedures used in CD-MVGNN and CoMPT, this result indicates the effectiveness of the inter-molecule graph for prediction tasks. (2) Our model performs relatively poor on regression tasks compared to SOTAs, which may be explained by the lack of real ground truth of relationship graphs in regression tasks. However, our model still achieve 2nd best results on 2/3 datasets. (3) Though our model uses GIN for intra-molecule graphs, it outperforms GIN on 7 of the 8 datasets. Especially, on the ClinTox dataset, our model gets up to 7.8\\% performance improvement. This also reflects the effectiveness of our molecule similarity graph construction and graph structure learning.\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Study",
                    "content": "\nTo investigate the contribution of each component of our model, an ablation study is conducted. We consider four variant models for comparison as follows:\n\\begin{itemize}\n    \\item \\textbf{Not any}: directly use $H_r^{(0)}$ to predict. It is almost a GIN network.\n    \\item \\textbf{Only $A^{(0)}$}: apply GNN on the initial molecular similarity graph $A^{(0)}$ constructed by ECFP similarity without GSL.\n    \\item \\textbf{Only GSL}: use de novo GSL without an initial graph reference $A^{(0)}$.\n    \\item \\textbf{No GSL-Loss}: use $A^{(0)}$ and GSL, but apply only the prediction loss.\n\\end{itemize}\n\nAblation results are given in Table \\ref{table:ablation}. Here we mainly consider the contribution of the initial adjacency matrix $A^{(0)}$ constructed by ECFP fingerprints and GSL process in our method. The results of ``Not any'', ``Only $A^{(0)}$'' and ``No GSL-Loss'' confirm that the use of $A^{(0)}$ could improve the performance of our model and the improvement will be much more significant when combined with GSL. Besides, it is interesting to notice that ``Only GSL'' often performs worse than ``Not any\", which probably means learning an inter-molecule graph from scratch might be difficult and it is necessary for us to utilize the chemical information of molecular fingerprints to build an initial graph. Finally, while comparing ``No GSL-Loss\" and the complete ``Our Model\", we can see that GSL-specific loss does make a difference for our method.\n\n\n\nWe also conduct experiments to show the results of using different values of some important hyper-parameters on all the datasets. \nTable \\ref{table:lambda} reports the results of applying different values of $\\lambda$, which is used to balance the learned graph structure and the initial graph structure. It can be seen that applying a large $\\lambda$ value (0.8 or 0.9) will generate a relatively good results on most datasets, which indicates the importance of the initial inter-molecule graph.   \nBesides, Table \\ref{table:T} shows the impact of the number of iteration $T$ on performance. We can see that as $T$ increases from 1 to 5,  performance on most datasets does not show  continuous improvement, which means that the best $T$ is data dependent. \n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Visualization of Molecular Representations",
                    "content": "\nTo check the molecular representation learning ability of our model, we apply t-distributed Stochastic Neighbor Embedding (t-SNE) with default hyper-parameters to visualize the final molecular representations on four datasets, including two classification datasets (BACE and BBBP) and two regression datasets (FreeSolv and ESOL). The results are shown in Fig.~\\ref{fig:tsne}.\n\nWe can see that molecules of different labels have a clear boundary for both two classification datasets, especially for BBBP. Molecules of the same label tend to be clustered together, while molecules of different labels are located apart. Also, there seems a certain distribution pattern existing among the molecules of different property values for the two regression datasets. For the FreeSolv dataset, molecules tend to move from the outer region to the inner region as the property value decreases. As for the ESOL dataset, molecules tend to move from upper left to lower right as the property value decreases. These results indicate that our model generates reasonable representations of molecules for downstream tasks.\n\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Scaling Our Model to Larger Datasets",
                    "content": "\nDuring GSL, the similarity metric function calculate similarity scores for all pairs of graph nodes, which requires $\\mathcal{O}(n^2)$ complexity. So we need to address the scalability issue if the size of datasets becomes larger. Following IDGL~\\cite{chen2020iterative}, we apply an anchor-based method. During each iteration, We learn a node-anchor similarity matrix $R \\in \\mathbb{R}^{n \\times s}$ instead of the original complete adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$. $s$ represents the number of anchor nodes, which is a hyperparameter that can be set according to different datasets. By using $R$ instead of $A$, the time and space complexity can be reduced from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(ns)$.  Therefore, Eq. (3) in the paper can be rewritten as the following:\n\\begin{equation}\n    s_{ik}^p = \\cos (w^p \\odot v_i, w^p \\odot u_k), \\quad s_{ik} = \\frac{1}{m} \\sum_{p=1}^{m} s_{ik}^p\n\\end{equation}\nwhere $s_{ik}$ is the similarity score between node $v_i$ and anchor $u_k$. The procedure of message passing should also be changed accordingly. The node-anchor similarity matrix $R$ allows only direct connections between nodes and anchors. We call a direct travel between a node and an anchor as one-step transition described by $R$. Based on theories of stationary Markov random walks, we can actually recover $A$ from $R$ by calculating the two-step transition probabilities.\n\nUsing the above anchor-based GSL, We firstly evaluate whether introducing anchor nodes will have a great impact on the original prediction performance of our model. Results are given in Table \\ref{table:anchor-baseline}. We can find that anchor-based GSL performs a little worse than the original GSL in these molecule datasets but the performance degradation is not significant. So we think it is appropriate for us to apply anchor-based GSL in larger-scale molecule datasets.\n\n\n\n\n\nAfter completing the above evaluation, we test the anchor-based GSL method on the HIV dataset which includes over 40000 molecules and compare it with some existing models. Except for CD-MVGNN, the results of other models on the HIV data set are from PharmHGT~\\cite{jiang2023pharmacophoric}. PharmHGT is a recently proposed model based on the Transformer structure, which treats molecules as heterogeneous graphs. The ROC-AUC of CD-MVGNN on the HIV data set is obtained experimentally by ourselves. Results are given in Table \\ref{table:anchor-hiv}. Our method is able to achieve the optimal ROC-AUC on the HIV dataset, showing that after introducing anchor nodes, our method can be well extended to larger-scale datasets and achieve satisfactory results.\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\\label{sec:conclusion}\nIn this paper, we propose a new model based on two-level molecular representation for molecular property prediction. Unlike previous attempts focusing exclusively on message passing between atoms or bonds within individual molecule graphs, we further take use of the inter-molecule graph. Concretely, we utilize the chemical information of molecular fingerprints to construct an initial molecular similarity graph, and employ graph structure learning to refine the graph. Molecular embeddings based on GSL on the inter-molecular similarity graph are used for MPP. \nExtensive experiments show that our model can achieve state-of-the-art performance in most cases, especially on the classification tasks. Ablation studies also validate the major designed components of the model. \n\nHowever, there is still room to improve our model in the following directions: (1) Using more sophisticated graph-based models to encode molecular graphs rather than GIN. (2) Designing new metrics other than weighted cosine similarity for graph structure learning. (3) Exploring new and more effective GSL methods. \n\n\n\n\\bibliographystyle{plain}\n\\bibliography{reference}\n% \\begin{thebibliography}{10}\n\n% \\bibitem{chen2021learning}\n% Jianwen Chen, Shuangjia Zheng, Ying Song, Jiahua Rao, and Yuedong Yang.\n% \\newblock Learning attributed graph representations with communicative message passing transformer.\n% \\newblock {\\em arXiv preprint arXiv:2107.08773}, 2021.\n\n% \\bibitem{chen2020iterative}\n% Yu~Chen, Lingfei Wu, and Mohammed Zaki.\n% \\newblock Iterative deep graph learning for graph neural networks: Better and robust node embeddings.\n% \\newblock {\\em Advances in neural information processing systems}, 33:19314--19326, 2020.\n\n% \\bibitem{duvenaud2015convolutional}\n% David~K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al{\\'a}n Aspuru-Guzik, and Ryan~P Adams.\n% \\newblock Convolutional networks on graphs for learning molecular fingerprints.\n% \\newblock {\\em Advances in neural information processing systems}, 28, 2015.\n\n% \\bibitem{fleming2018artificial}\n% Nic Fleming.\n% \\newblock How artificial intelligence is changing drug discovery.\n% \\newblock {\\em Nature}, 557(7706):S55--S55, 2018.\n\n% \\bibitem{gilmer2017neural}\n% Justin Gilmer, Samuel~S Schoenholz, Patrick~F Riley, Oriol Vinyals, and George~E Dahl.\n% \\newblock Neural message passing for quantum chemistry.\n% \\newblock In {\\em International conference on machine learning}, pages 1263--1272. PMLR, 2017.\n\n% \\bibitem{glen2006circular}\n% Robert~C Glen, Andreas Bender, Catrin~H Arnby, Lars Carlsson, Scott Boyer, and James Smith.\n% \\newblock Circular fingerprints: flexible molecular descriptors with applications from physical chemistry to adme.\n% \\newblock {\\em IDrugs}, 9(3):199, 2006.\n\n% \\bibitem{jiang2023pharmacophoric}\n% Yinghui Jiang, Shuting Jin, Xurui Jin, Xianglu Xiao, Wenfan Wu, Xiangrong Liu, Qiang Zhang, Xiangxiang Zeng, Guang Yang, and Zhangming Niu.\n% \\newblock Pharmacophoric-constrained heterogeneous graph transformer model for molecular property prediction.\n% \\newblock {\\em Communications Chemistry}, 6(1):60, 2023.\n\n% \\bibitem{johnson1990concepts}\n% Mark~A Johnson and Gerald~M Maggiora.\n% \\newblock {\\em Concepts and applications of molecular similarity}.\n% \\newblock Wiley, 1990.\n\n% \\bibitem{kearnes2016molecular}\n% Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley.\n% \\newblock Molecular graph convolutions: moving beyond fingerprints.\n% \\newblock {\\em Journal of computer-aided molecular design}, 30:595--608, 2016.\n\n% \\bibitem{lu2019molecular}\n% Chengqiang Lu, Qi~Liu, Chao Wang, Zhenya Huang, Peize Lin, and Lixin He.\n% \\newblock Molecular property prediction: A multilevel quantum interactions modeling perspective.\n% \\newblock In {\\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 1052--1060, 2019.\n\n% \\bibitem{ma2022cross}\n% Hehuan Ma, Yatao Bian, Yu~Rong, Wenbing Huang, Tingyang Xu, Weiyang Xie, Geyan Ye, and Junzhou Huang.\n% \\newblock Cross-dependent graph neural networks for molecular property prediction.\n% \\newblock {\\em Bioinformatics}, 38(7):2003--2009, 2022.\n\n% \\bibitem{maggiora2006outliers}\n% Gerald~M Maggiora.\n% \\newblock On outliers and activity cliffs why qsar often disappoints, 2006.\n\n% \\bibitem{maziarka2020molecule}\n% {\\L}ukasz Maziarka, Tomasz Danel, S{\\l}awomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanis{\\l}aw Jastrz{\\k{e}}bski.\n% \\newblock Molecule attention transformer.\n% \\newblock {\\em arXiv preprint arXiv:2002.08264}, 2020.\n\n% \\bibitem{Morgan1965MorganAlgorithm}\n% H.~L. Morgan.\n% \\newblock The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service.\n% \\newblock {\\em Journal of Chemical Documentation}, 5(2):107--113, 1965.\n\n% \\bibitem{muegge2016overview}\n% Ingo Muegge and Prasenjit Mukherjee.\n% \\newblock An overview of molecular fingerprint similarity search in virtual screening.\n% \\newblock {\\em Expert opinion on drug discovery}, 11(2):137--148, 2016.\n\n% \\bibitem{nguyen2023perceiver}\n% Ngoc-Quang Nguyen, Gwanghoon Jang, Hajung Kim, and Jaewoo Kang.\n% \\newblock Perceiver cpi: a nested cross-attention network for compound--protein interaction prediction.\n% \\newblock {\\em Bioinformatics}, 39(1):btac731, 2023.\n\n% \\bibitem{ramsundar2015massively}\n% Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay Pande.\n% \\newblock Massively multitask networks for drug discovery.\n% \\newblock {\\em arXiv preprint arXiv:1502.02072}, 2015.\n\n% \\bibitem{rogers2010extended}\n% David Rogers and Mathew Hahn.\n% \\newblock Extended-connectivity fingerprints.\n% \\newblock {\\em Journal of chemical information and modeling}, 50(5):742--754, 2010.\n\n% \\bibitem{rong2020self}\n% Yu~Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang.\n% \\newblock Self-supervised graph transformer on large-scale molecular data.\n% \\newblock {\\em Advances in Neural Information Processing Systems}, 33:12559--12571, 2020.\n\n% \\bibitem{sarkar2023artificial}\n% Chayna Sarkar, Biswadeep Das, Vikram~Singh Rawat, Julie~Birdie Wahlang, Arvind Nongpiur, Iadarilang Tiewsoh, Nari~M Lyngdoh, Debasmita Das, Manjunath Bidarolli, and Hannah~Theresa Sony.\n% \\newblock Artificial intelligence and machine learning technology driven modern drug discovery and development.\n% \\newblock {\\em International Journal of Molecular Sciences}, 24(3):2026, 2023.\n\n% \\bibitem{schutt2017schnet}\n% Kristof Sch{\\\"u}tt, Pieter-Jan Kindermans, Huziel~Enoc Sauceda~Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M{\\\"u}ller.\n% \\newblock Schnet: A continuous-filter convolutional neural network for modeling quantum interactions.\n% \\newblock {\\em Advances in neural information processing systems}, 30, 2017.\n\n% \\bibitem{song2020communicative}\n% Ying Song, Shuangjia Zheng, Zhangming Niu, Zhang-Hua Fu, Yutong Lu, and Yuedong Yang.\n% \\newblock Communicative representation learning on attributed molecular graphs.\n% \\newblock In {\\em IJCAI}, volume 2020, pages 2831--2838, 2020.\n\n% \\bibitem{stumpfe2012exploring}\n% Dagmar Stumpfe and J{\\\"u}rgen Bajorath.\n% \\newblock Exploring activity cliffs in medicinal chemistry: miniperspective.\n% \\newblock {\\em Journal of medicinal chemistry}, 55(7):2932--2942, 2012.\n\n% \\bibitem{stumpfe2020advances}\n% Dagmar Stumpfe, Huabin Hu, and J{\\\"u}rgen Bajorath.\n% \\newblock Advances in exploring activity cliffs.\n% \\newblock {\\em Journal of Computer-Aided Molecular Design}, 34(9):929--942, 2020.\n\n% \\bibitem{stumpfe2014recent}\n% Dagmar Stumpfe, Ye~Hu, Dilyana Dimova, and J{\\\"u}rgen Bajorath.\n% \\newblock Recent progress in understanding activity cliffs and their utility in medicinal chemistry: miniperspective.\n% \\newblock {\\em Journal of medicinal chemistry}, 57(1):18--28, 2014.\n\n% \\bibitem{sun2020graph}\n% Mengying Sun, Sendong Zhao, Coryandar Gilvary, Olivier Elemento, Jiayu Zhou, and Fei Wang.\n% \\newblock Graph convolutional networks for computational drug development and discovery.\n% \\newblock {\\em Briefings in bioinformatics}, 21(3):919--935, 2020.\n\n% \\bibitem{velivckovic2017graph}\n% Petar Veli{\\v{c}}kovi{\\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.\n% \\newblock Graph attention networks.\n% \\newblock {\\em arXiv preprint arXiv:1710.10903}, 2017.\n\n% \\bibitem{wang2021property}\n% Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou.\n% \\newblock Property-aware relation networks for few-shot molecular property prediction.\n% \\newblock {\\em Advances in Neural Information Processing Systems}, 34:17441--17454, 2021.\n\n% \\bibitem{wang2021hierarchical}\n% Yaqing Wang, Song Wang, Quanming Yao, and Dejing Dou.\n% \\newblock Hierarchical heterogeneous graph representation learning for short text classification.\n% \\newblock {\\em arXiv preprint arXiv:2111.00180}, 2021.\n\n% \\bibitem{wieder2020compact}\n% Oliver Wieder, Stefan Kohlbacher, M{\\'e}laine Kuenemann, Arthur Garon, Pierre Ducrot, Thomas Seidel, and Thierry Langer.\n% \\newblock A compact review of molecular property prediction with graph neural networks.\n% \\newblock {\\em Drug Discovery Today: Technologies}, 37:1--12, 2020.\n\n% \\bibitem{wu2018moleculenet}\n% Zhenqin Wu, Bharath Ramsundar, Evan~N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh~S Pappu, Karl Leswing, and Vijay Pande.\n% \\newblock Moleculenet: a benchmark for molecular machine learning.\n% \\newblock {\\em Chemical science}, 9(2):513--530, 2018.\n\n% \\bibitem{xiong2019pushing}\n% Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et~al.\n% \\newblock Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism.\n% \\newblock {\\em Journal of medicinal chemistry}, 63(16):8749--8760, 2019.\n\n% \\bibitem{xu2018powerful}\n% Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\n% \\newblock How powerful are graph neural networks?\n% \\newblock {\\em arXiv preprint arXiv:1810.00826}, 2018.\n\n% \\bibitem{yang2019analyzing}\n% Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et~al.\n% \\newblock Analyzing learned molecular representations for property prediction.\n% \\newblock {\\em Journal of chemical information and modeling}, 59(8):3370--3388, 2019.\n\n% \\bibitem{zhu2021survey}\n% Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Yuanqi Du, Jieyu Zhang, Qiang Liu, Carl Yang, and Shu Wu.\n% \\newblock A survey on graph structure learning: Progress and opportunities.\n% \\newblock {\\em arXiv e-prints}, pages arXiv--2103, 2021.\n\n% \\end{thebibliography}\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\n"
            }
        },
        "tables": {
            "table:hyper-parameter": "\\begin{table}\n\\caption{Hyper-parameter settings.}\n\\label{table:hyper-parameter}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{ccc}\n\\hline\nHyper-parameter    & Description                                                            & Value range                        \\\\ \\hline\nmax\\_lr            & maximum learning rate of  polynomial decay scheduler                   & 0.0001$\\sim$0.01             \\\\\nweight\\_decay      & weight\\_decay weight decay percentage for Adam optimizer               & 0.00001$\\sim$0.001           \\\\\ngin\\_layers        & number of the intra-molecule GIN layers                                & 2$\\sim$5                     \\\\\ngin\\_hidden\\_size  & number of the hidden dimensionality in the intra-molecule GIN          & 32, 64, 128, 256             \\\\\ntc\\_epsilon        & threshold of Tanimoto Coefficient for $A^{(0)}$ ($\\epsilon_{tc}$)                               & 0.0, 0.1, 0.2, 0.3, 0.5, 0.7 \\\\\ngsl\\_iter          & number of the iterations for graph structure learning ($T$)              & 1$\\sim$5                     \\\\\ngsl\\_gnn\\_layers   & number of the inter-molecule GCN layers                                & 2, 3                         \\\\\ngsl\\_hidden\\_size  & number of the hidden dimensionality in the inter-molecule GCN          & 32, 64, 128, 256             \\\\\ngsl\\_epsilon       & threshold of similarity score in GSL ($\\epsilon_{gsl}$)                                    & 0, 0,1, 0,2, 0.5             \\\\\ngsl\\_perspective   & number of perspective used in GSL ($m$)                                      & 1, 2, 4, 8, 16               \\\\\ngsl\\_skip\\_conn    & the ratio of initial matrix while updating the graph structure ($\\lambda$)      & 0.1, 0.3, 0.5, 0.7, 0.9      \\\\\ngsl\\_update\\_ratio & the ratio of t-th learned matrix while updating the graph structure ($\\eta$) & 0.1, 0.3, 0.6, 0.8. 1.0      \\\\\ndropout            & dropout rate                                                           & 0, 0.1, 0.2, 0.4, 0.6        \\\\ \ngsl\\_coff          & the coefficient of the GSL related loss ($\\mu$)                                & 0.1, 0.3, 0.5, 0.7, 0.9 \\\\\n\n\\hline\n\\end{tabular}\n}\n\\end{table}",
            "table:experiment": "\\begin{table}\n\\caption{Performance comparison between our model and baselines. Mean and standard deviation of AUC or RMSE values are reported.}\n\\label{table:experiment}\n\\renewcommand{\\arraystretch}{1.5}\n\\resizebox{\\columnwidth}{!}{\\begin{tabular}{c|cccc|ccc}\n\\hline\n            & \\multicolumn{4}{c|}{Classifcation (ROC-AUC)}                                                                                                              & \\multicolumn{3}{c}{Regression (RMSE)}                                                                         \\\\ \\hline\nDataset     & \\multicolumn{1}{c|}{BACE}                 & \\multicolumn{1}{c|}{BBBP}                 & \\multicolumn{1}{c|}{ClinTox}              & SIDER                & \\multicolumn{1}{c|}{FreeSolv}             & \\multicolumn{1}{c|}{ESOL}                 & Lipop                \\\\ \\hline\nTFRobust    & \\multicolumn{1}{c|}{0.824\u00b10.022}          & \\multicolumn{1}{c|}{0.860\u00b10.087}          & \\multicolumn{1}{c|}{0.765\u00b10.085}          & 0.607\u00b10.033          & \\multicolumn{1}{c|}{4.122\u00b10.085}          & \\multicolumn{1}{c|}{1.722\u00b10.038}          & 0.909\u00b10.060          \\\\\nGraphConv   & \\multicolumn{1}{c|}{0.854\u00b10.011}          & \\multicolumn{1}{c|}{0.877\u00b10.036}          & \\multicolumn{1}{c|}{0.845\u00b10.051}          & 0.593\u00b10.035          & \\multicolumn{1}{c|}{2.900\u00b10.135}          & \\multicolumn{1}{c|}{1.068\u00b10.050}          & 0.712\u00b10.049          \\\\\nWeave       & \\multicolumn{1}{c|}{0.791\u00b10.008}          & \\multicolumn{1}{c|}{0.837\u00b10.065}          & \\multicolumn{1}{c|}{0.823\u00b10.023}          & 0.543\u00b10.034          & \\multicolumn{1}{c|}{2.398\u00b10.250}          & \\multicolumn{1}{c|}{1.158\u00b10.055}          & 0.813\u00b10.042          \\\\\nSchNet      & \\multicolumn{1}{c|}{0.750\u00b10.033}          & \\multicolumn{1}{c|}{0.847\u00b10.024}          & \\multicolumn{1}{c|}{0.717\u00b10.042}          & 0.545\u00b10.038          & \\multicolumn{1}{c|}{3.215\u00b10.755}          & \\multicolumn{1}{c|}{1.045\u00b10.064}          & 0.909\u00b10.098          \\\\\nMPNN        & \\multicolumn{1}{c|}{0.815\u00b10.044}          & \\multicolumn{1}{c|}{0.913\u00b10.041}          & \\multicolumn{1}{c|}{0.879\u00b10.054}          & 0.595\u00b10.030          & \\multicolumn{1}{c|}{2.185\u00b10.952}          & \\multicolumn{1}{c|}{1.167\u00b10.430}          & 0.672\u00b10.051          \\\\\nDMPNN       & \\multicolumn{1}{c|}{0.852\u00b10.053}          & \\multicolumn{1}{c|}{0.919\u00b10.030}          & \\multicolumn{1}{c|}{0.897\u00b10.040}          & 0.632\u00b10.023          & \\multicolumn{1}{c|}{2.177\u00b10.914}          & \\multicolumn{1}{c|}{0.980\u00b10.258}          & 0.653\u00b10.046          \\\\\nMGCN        & \\multicolumn{1}{c|}{0.734\u00b10.030}          & \\multicolumn{1}{c|}{0.850\u00b10.064}          & \\multicolumn{1}{c|}{0.634\u00b10.042}          & 0.552\u00b10.018          & \\multicolumn{1}{c|}{3.349\u00b10.097}          & \\multicolumn{1}{c|}{1.266\u00b10.147}          & 1.113\u00b10.041          \\\\\nCMPNN       & \\multicolumn{1}{c|}{0.869\u00b10.023}          & \\multicolumn{1}{c|}{0.929\u00b10.025}          & \\multicolumn{1}{c|}{0.922\u00b10.017}          & 0.617\u00b10.016          & \\multicolumn{1}{c|}{2.060\u00b10.505}          & \\multicolumn{1}{c|}{0.838\u00b10.140}          & \\underline{0.625\u00b10.027}    \\\\\nAttentiveFP & \\multicolumn{1}{c|}{0.863\u00b10.015}          & \\multicolumn{1}{c|}{0.908\u00b10.050}          & \\multicolumn{1}{c|}{0.933\u00b10.020}          & 0.605\u00b10.060          & \\multicolumn{1}{c|}{2.030\u00b10.420}          & \\multicolumn{1}{c|}{0.853\u00b10.060}          & 0.650\u00b10.030          \\\\\nCD-MVGNN    & \\multicolumn{1}{c|}{\\textbf{0.892\u00b10.011}} & \\multicolumn{1}{c|}{\\underline{0.933\u00b10.006}}    & \\multicolumn{1}{c|}{\\underline{0.945\u00b10.037}}    & \\underline{0.639\u00b10.012}    & \\multicolumn{1}{c|}{\\textbf{1.552\u00b10.123}} & \\multicolumn{1}{c|}{\\textbf{0.779\u00b10.026}} & \\textbf{0.553\u00b10.013} \\\\ \\hline\nGROVER\\textsuperscript{*}     & \\multicolumn{1}{c|}{0.858}                & \\multicolumn{1}{c|}{0.911}                & \\multicolumn{1}{c|}{0.884}                & 0.624                & \\multicolumn{1}{c|}{1.987}                & \\multicolumn{1}{c|}{0.911}                & 0.643                \\\\\nCoMPT       & \\multicolumn{1}{c|}{0.838\u00b10.035}          & \\multicolumn{1}{c|}{0.926\u00b10.028}          & \\multicolumn{1}{c|}{0.876\u00b10.031}          & 0.612\u00b10.026          & \\multicolumn{1}{c|}{2.006\u00b10.628}          & \\multicolumn{1}{c|}{0.822\u00b10.090}          & 0.663\u00b10.035          \\\\ \\hline\nOurModel    & \\multicolumn{1}{c|}{\\underline{0.871\u00b10.038}}    & \\multicolumn{1}{c|}{\\textbf{0.957\u00b10.008}} & \\multicolumn{1}{c|}{\\textbf{0.947\u00b10.020}} & \\textbf{0.652\u00b10.014} & \\multicolumn{1}{c|}{\\underline{1.974\u00b10.315}}    & \\multicolumn{1}{c|}{\\underline{0.799\u00b10.118}}    & 0.693\u00b10.063          \\\\ \\hline\n\\end{tabular}}\n\\begin{tablenotes}\n\t\\item *Here, GROVER does not use pretrained model for a fair comparison, and standard deviation is not provided in its the original paper.\n     \\end{tablenotes}\n\\end{table}",
            "table:ablation": "\\begin{table}\n\\caption{Ablation study on four variants of our model.}\n\\label{table:ablation}\n\\renewcommand{\\arraystretch}{1.5}\n\\resizebox{\\columnwidth}{!}{\\begin{tabular}{c|cccc|ccc}\n\\hline\n            & \\multicolumn{4}{c|}{Classification (ROC-AUC)}                                                                                                             & \\multicolumn{3}{c}{Regression (RMSE)}                                                                         \\\\ \\hline\n     & \\multicolumn{1}{c|}{BACE}                 & \\multicolumn{1}{c|}{BBBP}                 & \\multicolumn{1}{c|}{ClinTox}              & SIDER                & \\multicolumn{1}{c|}{FreeSolv}             & \\multicolumn{1}{c|}{ESOL}                 & Lipop                \\\\ \\hline\nNot any      & \\multicolumn{1}{c|}{0.809\u00b10.032}          & \\multicolumn{1}{c|}{0.943\u00b10.005}          & \\multicolumn{1}{c|}{0.932\u00b10.012}          & 0.593\u00b10.017          & \\multicolumn{1}{c|}{2.055\u00b10.405}          & \\multicolumn{1}{c|}{0.962\u00b10.093}          & 0.888\u00b10.046          \\\\\nonly A0     & \\multicolumn{1}{c|}{0.856\u00b10.049}          & \\multicolumn{1}{c|}{0.951\u00b10.002}          & \\multicolumn{1}{c|}{0.939\u00b10.033}          & 0.639\u00b10.030          & \\multicolumn{1}{c|}{3.433\u00b10.552}          & \\multicolumn{1}{c|}{0.945\u00b10.076}          & 0.724\u00b10.040          \\\\\nonly GSL    & \\multicolumn{1}{c|}{0.850\u00b10.025}          & \\multicolumn{1}{c|}{0.943\u00b10.017}          & \\multicolumn{1}{c|}{0.875\u00b10.049}          & 0.580\u00b10.021          & \\multicolumn{1}{c|}{2.638\u00b10.098}          & \\multicolumn{1}{c|}{1.147\u00b10.256}          & 0.899\u00b10.196          \\\\\nNo GSL-Loss & \\multicolumn{1}{c|}{0.865\u00b10.034}          & \\multicolumn{1}{c|}{0.953\u00b10.015}          & \\multicolumn{1}{c|}{0.935\u00b10.016}          & 0.651\u00b10.026          & \\multicolumn{1}{c|}{2.134\u00b10.155}          & \\multicolumn{1}{c|}{0.821\u00b10.100}          & 0.711\u00b10.047          \\\\\nOur Model   & \\multicolumn{1}{c|}{\\textbf{0.871\u00b10.038}} & \\multicolumn{1}{c|}{\\textbf{0.957\u00b10.008}} & \\multicolumn{1}{c|}{\\textbf{0.947\u00b10.020}} & \\textbf{0.652\u00b10.014} & \\multicolumn{1}{c|}{\\textbf{1.974\u00b10.315}} & \\multicolumn{1}{c|}{\\textbf{0.799\u00b10.118}} & \\textbf{0.693\u00b10.063} \\\\ \\hline\n\\end{tabular}}\n\\end{table}",
            "table:lambda": "\\begin{table}\n\\caption{Results for different $\\lambda$ values on different datasets.}\n\\label{table:lambda}\n\\renewcommand{\\arraystretch}{1.5}\n\\resizebox{\\columnwidth}{!}{\\begin{tabular}{cccccccc}\n\\hline\nlambda & BACE                 & BBBP                 & ClinTox              & SIDER                & FreeSolv             & ESOL                 & Lipop                \\\\ \\hline\n0.1    & 0.702\u00b10.1            & 0.924\u00b10.006          & 0.914+0.026          & 0.564+0.035          & 2.812+0.239          & 0.983+0.094          & 0.714+0.056          \\\\\n0.3    & 0.846\u00b10.062          & 0.944\u00b10.008          & \\textbf{0.947\u00b10.020} & 0.606+0.018          & 2.371+0.171          & 0.986+0.112          & 0.749+0.05           \\\\\n0.5    & 0.839\u00b10.043          & 0.921\u00b10.017          & 0.939+0.018          & 0.616+0.007          & 2.224+0.231          & 0.87+0.147           & 0.706+0.059          \\\\\n0.7    & 0.849\u00b10.049          & 0.942\u00b10.004          & 0.916+0.037          & 0.634+0.006          & 2.126+0.153          & 0.931+0.056          & 0.725+0.066          \\\\\n0.8    & 0.850\u00b10.028          & \\textbf{0.957\u00b10.008} & 0.927+0.023          & \\textbf{0.652\u00b10.014} & \\textbf{1.974\u00b10.315} & 0.827+0.09           & \\textbf{0.693\u00b10.063} \\\\\n0.9    & \\textbf{0.871\u00b10.038} & 0.939\u00b10.013          & 0.925+0.023          & 0.643+0.004          & 2.279+0.089          & \\textbf{0.799\u00b10.118} & 0.707+0.066          \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}",
            "table:T": "\\begin{table}\n\\caption{Results for  different $T$ values  on different datasets.}\n\\label{table:T}\n\\renewcommand{\\arraystretch}{1.5}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{cccccccc}\n\\hline\n$T$ & BACE                 & BBBP                 & ClinTox              & SIDER                & FreeSolv             & ESOL                 & Lipop                \\\\ \\hline\n1 & 0.848\u00b10.062          & 0.945\u00b10.007          & 0.93\u00b10.028           & 0.64\u00b10.017           & 2.724\u00b10.484          & 0.863\u00b10.046          & 0.718\u00b10.047          \\\\\n2 & 0.858\u00b10.045          & 0.923\u00b10.023          & \\textbf{0.947\u00b10.020} & 0.636\u00b10.015          & \\textbf{1.974\u00b10.315} & 0.86\u00b10.127           & 0.751\u00b10.053          \\\\\n3 & 0.857\u00b10.043          & 0.944\u00b10.008          & 0.93\u00b10.005           & \\textbf{0.652\u00b10.014} & 2.182\u00b10.364          & 0.872\u00b10.087          & \\textbf{0.693\u00b10.063} \\\\\n4 & \\textbf{0.871\u00b10.038} & \\textbf{0.957\u00b10.008} & 0.907\u00b10.018          & 0.64\u00b10.007           & 2.209\u00b10.41           & 0.861\u00b10.093          & 0.752\u00b10.047          \\\\\n5 & 0.851\u00b10.057          & 0.937\u00b10.011          & 0.918\u00b10.034          & 0.63\u00b10.015           & 2.309\u00b10.291          & \\textbf{0.799\u00b10.118} & 0.751\u00b10.058          \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}",
            "table:anchor-baseline": "\\begin{table}\n\\caption{Performance comparison between original and anchor-based GSL.}\n\\label{table:anchor-baseline}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{c|c|c|c|c|c|c|c}\n\\hline\n             & BACE  & BBBP  & ClinTox & SIDER & FreeSolv & ESOL  & Lipop \\\\ \\hline\nOrigin       & 0.865 & 0.953 & 0.935   & 0.651 & 2.134    & 0.821 & 0.711 \\\\ \\hline\nAnchor-based & 0.818 & 0.949 & 0.95    & 0.612 & 2.208    & 0.794 & 0.747 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}",
            "table:anchor-hiv": "\\begin{table}\n\\centering\n\\caption{Performance comparison between our model (using anchor-based GSL) and baselines.}\n\\label{table:anchor-hiv}\n\\begin{tabular}{c|c}\n\\hline\n            & ROC-AUC\\%       \\\\ \\hline\nOur model   & \\textbf{81.8} \\\\\nPharmHGT    & 80.6          \\\\\nDMPNN       & 78.6          \\\\\nCD-MVGNN    & 78.4          \\\\\nCoMPT       & 78.1          \\\\\nCMPNN       & 77.4          \\\\\nAttentiveFP & 75.7          \\\\\nMPNN        & 74.1          \\\\\nGROVER      & 62.5          \\\\ \\hline\n\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:model": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{model6.png}\n    \\caption{The workflow of GSL-MPP. In the initial molecular similarity graph (MSG), each node is a molecule initially represented by the GNN and each edge is attached with the FP similarity between the two corresponding molecules. GSL is then performed on the MSG.}\n    \\label{fig:model}\n\\end{figure}",
            "fig:tsne": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{t-sne-4dataset.png}\n    \\caption{Visualization of molecular representations for 4 datsets: (a) BBBP,  (b) BACE,  (c) FreeSolv and (d) ESOL. For classification datasets BACE and BBBP, molecules of label 1 are colored in red and molecules of label 0 are colored in blue. For regression datasets FreeSolv and ESOL, the colors of the points change from red to blue as the property value increases.}\n    \\label{fig:tsne}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    h_v^{(k)} = MLP^{(k)} ((1 + \\epsilon^{(k)}) \\cdot h_v^{(k-1)} + \\sum_{u\\in \\mathcal{N}(v)} h_u^{(k-1)}),\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    h_g = READOUT(\\left\\{ \\left. h_v^{k}\\right| v \\in G \\right\\} \\left| k = 0, 1, ..., K \\right.).\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    s_{ij}^p = \\cos (w_p \\odot v_i, w_p \\odot v_j), \\quad s_{ij} = \\frac{1}{m} \\sum_{p=1}^{m} s_{ij}^p,\n\\label{eq:similarity-metric} \n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\widetilde A^{(t)} = \\lambda A^{(0)} + (1 - \\lambda)\\left\\{ \\eta A^{(t)} + (1 - \\eta) A^{(1)}   \\right\\},\n\\label{eq:update-adj} \n\\end{equation}",
            "eq:5": "\\begin{equation}\n    H_r^{(t, l)} = ReLU (\\widetilde A^{(t)} H_r^{(t, l-1)} W_r^{(l)}),\n\\label{eq:update-embeddings} \n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\hat y = FC(H_r^{(T)}).\n\\label{eq:prediction} \n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\quad \\mathcal{L}_{pred} = \\ell (\\hat y, y),\n    \\label{eq:prediction-loss} \n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\mathcal{L}_{GSL} = \\sum_{x_i, x_j \\in S_{train}} (\\widetilde A_{ij}^{(T)} - A_{ij}^*)^2\n    \\label{eq:gsl-loss-clf} \n\\end{equation}",
            "eq:9": "\\begin{equation}\n    \\mathcal{L}_{GSL} = \\sum_{x_i, x_j \\in S_{train}} (\\widetilde A^{(T)}_{ij})^2 , \\quad  x_i, x_j \\quad satisfy \\quad \\left| y_i - y_j \\right| > \\epsilon_y.\n    \\label{eq:gsl-loss-reg}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    s_{ik}^p = \\cos (w^p \\odot v_i, w^p \\odot u_k), \\quad s_{ik} = \\frac{1}{m} \\sum_{p=1}^{m} s_{ik}^p\n\\end{equation}"
        }
    }
}