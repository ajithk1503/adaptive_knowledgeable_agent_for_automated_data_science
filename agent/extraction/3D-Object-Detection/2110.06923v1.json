{
    "meta_info": {
        "title": "Object DGCNN: 3D Object Detection using Dynamic Graphs",
        "abstract": "3D object detection often involves complicated training and testing\npipelines, which require substantial domain knowledge about individual\ndatasets. Inspired by recent non-maximum suppression-free 2D object detection\nmodels, we propose a 3D object detection architecture on point clouds. Our\nmethod models 3D object detection as message passing on a dynamic graph,\ngeneralizing the DGCNN framework to predict a set of objects. In our\nconstruction, we remove the necessity of post-processing via object confidence\naggregation or non-maximum suppression. To facilitate object detection from\nsparse point clouds, we also propose a set-to-set distillation approach\ncustomized to 3D detection. This approach aligns the outputs of the teacher\nmodel and the student model in a permutation-invariant fashion, significantly\nsimplifying knowledge distillation for the 3D detection task. Our method\nachieves state-of-the-art performance on autonomous driving benchmarks. We also\nprovide abundant analysis of the detection model and distillation framework.",
        "author": "Yue Wang, Justin Solomon",
        "link": "http://arxiv.org/abs/2110.06923v1",
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "additionl_info": "Accepted to NeurIPS 2021"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Related Work",
                "content": "\n\n\\textbf{2D object detection.} Object recognition research has been transitioning from models with hand-crafted components to models with limited post-processing. One-stage detectors~\\cite{liu2016ssd,redmonF17,lin2017focal} remove the complicated region proposal networks in two-stage objectors~\\cite{ren2015faster,he2017maskrcnn}, yielding more efficient training and testing. Anchor-free methods~\\cite{zhou2019objects,Law2018} further simplify the one-stage pipeline by shifting from per-anchor prediction to per-pixel prediction. However, these methods still make dense predictions and rely on NMS to reduce redundancy. To alleviate this issue, DETR~\\cite{detr} formulates object detection as a set-to-set prediction problem. It introduces a set-to-set loss that implicitly penalizes redundant boxes, removing the necessity of post-processing. To accelerate convergence, Deformable DETR~\\cite{zhu2021deformable} proposes deformable self-attention and streamlines the optimization process. %Motivated by DETR and Deformable DETR, o\nOur method also formulates 3D object detection as set prediction, but with a customized design for 3D. \n \n\\textbf{3D object detection.} VoxelNet~\\cite{Zhou_2018_CVPR} generalizes one-stage object detection to 3D. It uses 3D dense convolutions to learn representations on voxelized point clouds, which is too inefficient to capture fine-grained features. To address that, PIXOR~\\cite{Yang2018PIXORR3} and PointPillars~\\cite{Lang_2019_CVPR} project points to a birds-eye view (BEV) and operate on 2D feature maps; PointNet~\\cite{qi2017pointnet} aggregates features within each BEV pixel. We use a variant of PointPillars~\\cite{Lang_2019_CVPR} for 3D detection (\\S\\ref{method:preliminary}). These methods are efficient but drop information along the vertical axis. To accompany the BEV projection, MVF~\\cite{Zhou2019EndtoEndMF} adds a spherical projection. PillarOd~\\cite{wang2020pillar} and CenterPoint~\\cite{yin2021center} use pillar-centric object detection, making predictions per BEV pixel (pillar) rather than per anchor. These anchor-free methods simplify 3D object detection while maintaining efficiency.  Beyond SSD-style~\\cite{liu2016ssd} one-stage models, Complex-YOLO~\\cite{Simon2018ComplexYOLOR3} extends YOLO to 3D for real-time perception. PointRCNN~\\cite{shi2019pointrcnn} employs a two-stage architecture for high-quality detection. To improve representations of two-stage models, PVRCNN~\\cite{shi2020pv} proposes a point-voxel feature set abstraction layer to leverage the flexible receptive fields of PointNet-based networks. Unlike works on point clouds, LaserNet~\\cite{Meyer2019LaserNetAE} operates on raw range scans with comparable performance.  \\cite{ku2018joint,Chen2016Multiview3O,Xu2017PointFusionDS} combine point clouds with camera images. % to glean complementary representations from both modalities. \nFrustum-PointNet~\\cite{qi2017frustum} leverages 2D object detectors to form a frustum crop of points and then uses PointNet to aggregate features. \\cite{Liang_2018_ECCV} describes an end-to-end learnable architecture that exploits continuous convolutions to fuse feature maps. \nVoteNet~\\cite{qi2019deep, qi2020imvotenet} generalizes Hough voting~\\cite{Hough} for 3D object detection in point clouds. DOPS~\\cite{Najibi2020DOPSLT} extends VoteNet and predicts 3D object shapes. \nIn addition to visual input, \\cite{yang18b} shows that high-definition (HD) maps can boost performance of 3D object detectors. \\cite{Liang_2019_CVPR} argues that multi-tasking can learn better representations than single-tasking. Beyond supervised learning, \\cite{Wong2019IdentifyingUI} learns a perception model for unknown classes. \n\n\\textbf{DGCNN.} DGCNN~\\cite{dgcnn} pioneered learning point cloud representations via dynamic graphs. It models point clouds as connected graphs, which are dynamically built using $k$-nearest neighbors in the latent space. DGCNN learns per-point features through message passing. However, it operates on point clouds for single object recognition and semantic segmentation. One of our key contributions is to generalize DGCNN to model scene-level object relations for 3D detection. \n\n% \\justin{just a few sentences.  say we're building on DGCNN and tell us the highlights:  k nearest neighbors per layer.  Conclude with the fact that DGCNN only operates on point clouds for single object recognition/segmentation tasks, while a key contribution of our work is to link it to a grid of features for scene-based object detection.}\n\n\\textbf{Knowledge distillation (KD).} KD compresses knowledge from an ensemble of models into a single smaller model~\\cite{Bucilua2006}. \\cite{knowledgedistillation} generalizes this idea and combines it with deep learning. KD transfers knowledge from a teacher model to a student model by minimizing a loss, in which the target is the distribution of class probabilities induced by the teacher. \\cite{Romero15-iclr,Zagoruyko2017AT,Tung2019SimilarityPreservingKD,Park2019RelationalKD,Yim_2017_CVPR,tian2019crd,Oord2018RepresentationLW,he2019moco,chen2020simple,tian2019contrastive,FurlanelloLTIA18,clark2019bam} improve knowledge distillation for classification. \n% In the original approach, a small network imitates the \n% output of a larger network; many follow-up works improve various aspects of this idea. FitNets~\\cite{Romero15-iclr} matches intermediate representations of the student network to those of the teacher network. Attention Transfer~\\cite{Zagoruyko2017AT} focuses on feature maps as opposed to outputs, eliciting similar response patterns in the teacher and student. SPKD~\\cite{Tung2019SimilarityPreservingKD} improves Attention Transfer by preserving pairwise feature map similarities. Relational KD~\\cite{Park2019RelationalKD} transfers relations between data examples. FSP~\\cite{Yim_2017_CVPR} views the distilled knowledge to be dense flow transferred between layers. CRD~\\cite{tian2019crd} connects KD to representation learning via a contrastive objective~\\cite{Oord2018RepresentationLW,he2019moco,chen2020simple,tian2019contrastive}. Self-distillation is a special case of KD, where the teacher model and student model have the same capacity. BAN~\\cite{FurlanelloLTIA18} introduces sequential self-distillation, which distills knowledge from the model's latest snapshot. In natural language processing (NLP), BAM~\\cite{clark2019bam} uses BAN to distill from single-task models to a multi-task model, enabling the multi-task model to surpass its single-task teachers. \nBeyond image classification, KD has been extended to improve object detection. \\cite{NIPS2017_6676} leverages FitNets for object detection, addressing obstacles such as class imbalance, loss instability, and feature distribution mismatch. \\cite{LiJY17} distills between region proposals, accelerating training with added instability. To address this issue, \\cite{Wang_2019_CVPR} uses fine-grained representation imitation using object masks. \n\\cite{Liu2020ContinualUO} uses KD to tackle a continual learning problem. %in object detection. %Our model makes use of privileged information during training to transfer knowledge between 3D point cloud--based object detection models. \n\n\\textbf{Privileged information.} \\cite{VapnikV09} introduces the framework of learning with privileged information in the context of support vector machines (SVMs), wherein additional information is accessible during training but not testing. \\cite{LopSchBotVap16} unifies KD and learning using privileged information theoretically. \\cite{su2017adapting} identifies practical applications, e.g., transferring knowledge from localized data to non-localized data, from high resolution to low resolution, from color images to edge images, and from regular images to distorted images. To mediate uncertainty and improve training efficiency, \\cite{Lambert_2018_CVPR} makes the variance of Dropout~\\cite{srivastava14a} a function of privileged information. We extend these methods to 3D data, in which privileged information consists of dense point clouds aggregated from LiDAR sequences. \n\n% \\section{Method}\n\n% We establish preliminaries about 3D object detection in \\S\\ref{method:preliminary}. Then, we summarize Object DGCNN and detail its components in \\S\\ref{method:objdgcnn}. Finally, we present our set-to-set distillation pipeline in \\S\\ref{method:distill}. \n\n"
            },
            "section 2": {
                "name": "Overview",
                "content": "\n\nOur target application of object detection differs from the recognition and segmentation tasks considered for DGCNN.  Our point clouds typically contain too many points to apply DGCNN and its peers directly to the entire scene.  Moreover, the size of our output set, a small set of bounding boxes, differs from the size of our input set, a huge set of points in $\\R^3$.\n\nFollowing state-of-the-art in large-scale object detection, our pipeline learns a grid-based intermediate representation to capture local features (\\S\\ref{method:preliminary}).  We test two standard learning-based methods for collecting local point cloud features on a birds-eye view (BEV) grid.   While in principle it might be possible to avoid grids entirely in our pipeline, this BEV representation is far more efficient and---as observed in previous work---is sufficient to find objects reliably in autonomous driving, where there is likely only one object above any given grid cell on the ground plane.\n\nOur main architecture contribution is the Object DGCNN pipeline (\\S\\ref{method:objdgcnn}), which transitions from this BEV grid of features to a \\emph{set} of object bounding boxes.  Object DGCNN draws inspiration from the DGCNN architecture; its layers alternate between local feature transformations and $k$-nearest neighbor aggregation to capture relationships between objects.  Unlike conventional DGCNN, however, Object DGCNN incorporates features from the BEV grid in each of its layers; each layer incorporates several queries into the BEV to refine object position estimates.  The output of Object DGCNN is a \\emph{set} of objects in the scene.  We use a permutation-invariant loss \\eqref{eq:sup_loss} to measure divergence from the ground truth set of objects.\n\nThe pipeline above does not require hand-designed post-processing like NMS; our output boxes are usable directly for object detection.  Beyond simplifying the object detection pipeline, this allows us to propose object detection-specific distillation procedures (\\S\\ref{sec:distill}) that further improve performance.  These use one network to train another, e.g., to train a network operating on sparse point clouds to output features that imitate those learned by a network trained on denser, more detailed point clouds.\n\n"
            },
            "section 3": {
                "name": "Local Features",
                "content": "\n\\label{method:preliminary}\n\nWe begin with a point cloud $\\X=\\{\\bx_1, \\ldots, \\bx_i, \\ldots, \\bx_N\\}\\subset\\R^3$ with per-point features $\\F = \\{\\bff_1, \\ldots, \\bff_i, \\ldots, \\bff_N\\}\\subset \\R^K$, ground-truth bounding boxes $\\B=\\{\\bb_1, \\ldots, \\bb_j, \\ldots, \\bb_M\\}\\subset\\R^9$, and categorical labels $\\C = \\{c_j, \\ldots, c_j, \\ldots, c_M\\}\\subset\\mathbb{Z}$. Each $\\bb_j$ contains position, size, heading angle, and velocity in the birds-eye view (BEV); our architecture aims to predict these boxes and their labels from the point cloud and its features. \n\nAs an initial step, modern 3D object detection models scatter points into either BEV pillars or 3D voxels and then use convolutional neural networks to extract features on a grid.  This strategy accelerates object detection for large point clouds. We test two neural network architectures for BEV feature extraction, detailed below.\n\n%\\textbf{PointPillars.} \n\nPointPillars~\\cite{Lang_2019_CVPR} maps sparse point clouds onto a dense BEV pillar map on which 2D convolutions can be applied.  Suppose $F_P(i)$ returns the points in pillar $i$, that is, the set of points in a vertical column above point $i$ on the ground. When collecting features from points to pillars, multiple points can fall into the same pillar. In this case, PointNet~\\cite{qi2017pointnet} ($\\mathrm{PN}$) is used to obtain pillar-wise features:\n\\begin{equation}\n     \\begin{split}\n        \\bff^{\\mathrm{pillar}}_i = \\mathrm{PN}(\\{\\bff_j |  \\bx_j \\in F_P(\\bp_i)\\}), \n     \\end{split}\n\\end{equation}\nwhere $\\bff^{\\mathrm{pillar}}_i$ is the feature of pillar $\\bp_i$. %Since the point clouds are sparsely distributed in the scene, w\nWe set the features for empty pillars to $\\mathbf{0}$. This results in a dense 2D grid $\\F^{\\mathrm{pillar}} \\subset \\R^{H^{\\mathrm{p}}\\times W^{\\mathrm{p}} \\times C^{\\mathrm{p}}}$, where $H^{\\mathrm{p}},  W^{\\mathrm{p}} $ and $C^{\\mathrm{p}}$ are the height, width, number of channels of this 2D pillar map, respectively. Multiple stacked convolutional layers further embed the pillar features to the final feature map $\\F^{\\mathrm{d}}\\subset\\R^{H^{\\mathrm{d}}\\times W^{\\mathrm{d}}\\times C^{\\mathrm{d}}}$. %, the feature map to the detection head. \n\n%\\textbf{SparseConv.} \nAn alternative BEV embedding is SparseConv~\\cite{sparseconv}. If $F_V(i)$ returns the set of points in voxel $i$, SparseConv collects point-wise features into voxel-wise features by \n\\begin{equation}\\label{eq:voxelfeature}\n     \\begin{split}\n        \\bff^{\\mathrm{voxel}}_i = \\mathrm{PN}(\\{\\bff_j |  \\bx_j \\in F_V(i)\\}), \n     \\end{split}\n\\end{equation}\nwhere $\\bff^{\\mathrm{voxel}}_i$ contains the features of voxel $i$. In contrast to PointPillars, SparseConv conducts 3D sparse convolutions to refine the voxel-wise features. Finally, we compress these sparse voxels to a BEV 2D grid by filling empty voxels with zeros and averaging along the $z$-axis. For ease of notation, we also denote the resulting 2D grid $\\F^{\\mathrm{d}}\\subset\\R^{H^{\\mathrm{d}}\\times W^{\\mathrm{d}}\\times C^{\\mathrm{d}}}$. \n\n% Conceptually, PointPillars and SparseConv perform voxelization, densification, and feature learning in similar ways. The differences are: PointPillars scatters the point clouds into a 2D grid while SparseConv uses a 3D grid, and PointPillars performs densification before 2D dense convolutions while SparseConv does 3D sparse convolutions before densification.  % <--- all this is background\n\n\n"
            },
            "section 4": {
                "name": "Object DGCNN",
                "content": "\n\\label{method:objdgcnn}\n\n\n\nAfter obtaining the BEV features $\\F^{\\mathrm{d}}$ using one of the architectures above, we predict a set of bounding boxes as well as a label for each box.\nThe key difference between our architecture and most recent 3D object detection methods is that ours produces a \\emph{set} of bounding boxes rather than a box per grid cell followed by NMS, as in~\\cite{wang2020pillar,yin2021center}.  Hence, we need to transition from a grid of per-pillar features to an unordered set of objects; we detail our approach below.\nWe address two key issues:  prediction of the bounding boxes and evaluation of the loss.\n\n\\textbf{Desiderata.} \nObject DGCNN uses a DGCNN-inspired architecture but incorporates grid-based BEV features, built on the philosophy that local features (\\S\\ref{method:preliminary}) are reasonable to store on a dense grid, but object predictions are better modeled using sets. Hence, we require a new architecture and set-to-set loss that encourage bounding box diversity. \n\n%After encoding point clouds using BEV feature maps, \nObject DGCNN uses $L$ layers that follow a series of set-based computations to produce bounding box predictions from the BEV feature maps.  Each layer employs the following steps (Figure \\ref{fig:objdgcnn}):\n%The steps of Object DGCNN, illustrated in Figure \\ref{fig:objdgcnn}, are as follows:\n\\begin{enumerate}\n    %\\item encode point clouds into BEV feature maps using PointPillars or SparseConv;\n    \\item predict a set of query points and attention weights;\n    \\item collect BEV features from keypoints determined by the queries; and\n    \\item model object-object interactions via DGCNN.\n    % \\item make per-query predictions;\n    % \\item match the prediction set with the ground-truth set in a one-to-one fashion; and\n    % \\item compute the set-to-set object detection loss.\n\\end{enumerate}\nEach layer results in a more refined set of bounding box predictions, one per query. At the end of these layers, we match the prediction set with the ground-truth set in a one-to-one fashion and evaluate a set-to-set object detection loss.\n\n% We consider a point cloud $\\X=\\{\\bx_1, \\ldots, \\bx_i, \\ldots, \\bx_N\\}\\subset\\R^3$ with features $\\F = \\{\\bff_1, \\ldots, \\bff_i, \\ldots, \\bff_N\\}\\subset \\R^K$ and ground-truth bounding boxes $\\B=\\{\\bb_1, \\ldots, \\bb_j, \\ldots, \\bb_N\\}\\subset\\R^9$ \\justin{should this be $M$ instead of $N$?} where $\\bb_j$ contains the position, size, heading angle, and velocity in the BEV \\justin{what does this stand for?} respectively. Our goal is to predict a set of bounding boxes $\\hat{\\B}=\\{\\hat{\\bb_1}, \\ldots, \\hat{\\bb_j}, \\ldots, \\hat{\\bb_M}\\}\\subset\\R^9$ with minimal distance \\justin{wouldn't this be zero?  like the ground truth?  something is weird about this paragraph---seems to be talking about training and testing/inference in parallel} to the ground-truth in the metric space. In contrast to existing methods, our model does not assign the ground-truth boxes to a dense grid in a one-to-many fashion, but rather compute set-to-set loss directly.\n\n% \\textbf{Point cloud encoder.} To extract features using grid-based convolutions, we voxelize the point clouds. \\justin{doesn't the previous section criticize methods that do this? why is it ok? why not use a point cloud feature?} We define a function $F_P(\\bv_i)$ to return the set of points in voxel $\\bv_i$. When projecting features from points to voxels, multiple points can fall into the same voxel. To aggregate features from points in a voxel, a PointNet~\\cite{qi2017pointnet} (denoted as $\\mathrm{PN}$) is used to aggregate features from points to get pillar-wise features\\justin{wait, is it pillars or is it voxels? also this sentence is a little hard to follow}, where\n% \\begin{equation}\\label{eq:voxelfeature}\n%      \\begin{split}\n%         \\bff^{\\mathrm{v}}_i = \\mathrm{PN}(\\{\\bff_j | \\forall \\bx_j \\in F_P(\\bv_i)\\}).\n%      \\end{split}\n% \\end{equation}\n% Then, we use PointPillars~\\cite{Lang_2019_CVPR} and/or SparseConv~\\cite{3DSparseConvNet} (depending on the actual use case) to extract features and turn them into\\justin{what does this mean?} BEV. To take multiple scales into account, we use features from multiple stages in the encoder\\justin{not sure what this means}. We denote the multi\\-scale BEV features of various stage $l$ as $\\F^l\\subset R^{H\\times W\\times K^l}$ where $H, W$, and $K^l$ are height, width, and number of channels of the BEV features. Optionally, we use deformable self-attention~\\cite{zhu2021deformable} to further improve the feature representations. \n\n% \\textbf{Object queries.} Similar to DETR~\\cite{detr}, Object DGCNN learns a set of \\emph{object queries} $\\Q=\\{\\bq_1, \\ldots, \\bq_i, \\ldots, \\bq_{M^*}\\}\\subset\\R^{Q}$, which are embeddings of potential objects and encode box parameters. During training, these priors are optimized jointly with the neural network weights so they can reflect the real dataset priors. \n\n%\\textbf{Object queries.}%<---got rid of this and merged with next paragraph\n\n\\textbf{Single layer.} \nInspired by DETR~\\cite{detr}, each layer $\\ell\\in\\{0,\\ldots,L-1\\}$ of Object DGCNN operates on a set of \\emph{object queries} $\\Q_\\ell=\\{\\bq_{\\ell 1}, \\ldots, \\bq_{\\ell M^*}\\}\\subset\\R^{Q}$, producing a new set $\\Q_{\\ell+1}$.  Although queries are fully learnable, our intuition is that they represent progressively refined object positions. \n\nThe initial set of object queries $\\Q_0$ is learned jointly with the neural network weights, yielding a dataset-specific prior.  Beyond this fixed initial set, below we detail how to incorporate scene information to obtain $\\Q_{\\ell+1}$ from $\\Q_{\\ell}$ using an approach inspired by DGCNN~\\cite{dgcnn} and deformable self-attention~\\cite{zhu2021deformable}.  For notational convenience, we drop the $\\ell$ subscript.\n\n%\\textbf{Linking BEV features to object queries.} \n\n% Object queries encode dataset priors that facilitate %, it remains challenging for them to make \n% per-scene predictions. We link the grid-based BEV features to the object queries through a deformable self-attention layer~\\cite{zhu2021deformable}.\n\nStarting from  each query $\\bq_i$ (or, without the index dropped, $\\bq_{\\ell i}$), we decode a reference point $\\bp_i\\in\\R^2$, a set of offsets $\\{\\boldsymbol{\\delta}_{i0},  \\ldots, \\boldsymbol{\\delta}_{iK}\\}\\subset\\R^2$, and a set of attention weights $\\{w_{i0}, \\ldots, w_{iK}\\}\\subset\\R$: \n\\begin{equation}\\label{eq:query-point}\n    \\begin{split}\n        &\\bp_i = \\Phi_\\mathrm{ref} (\\bq_i), \\qquad\\qquad\n        \\{\\boldsymbol{\\delta}_i^0, \\ldots, \\boldsymbol{\\delta}_i^k, \\ldots \\boldsymbol{\\delta}_i^K\\} = \\Phi_\\mathrm{neighbor}(\\bq_i), \\\\\n        &\\{w_i^0, \\ldots, w_i^k, \\ldots w_i^K\\} = \\Phi_\\mathrm{atten}(\\bq_i),\n    \\end{split} \n\\end{equation}\nwhere $\\Phi_\\mathrm{ref}$, $\\Phi_\\mathrm{neighbor}$, and $\\Phi_\\mathrm{atten}$ are shared neural networks among the queries. We think of $\\bp_i$ as a hypothesis for the center of the $i$-th object; the $\\boldsymbol\\delta$'s represent the positions of $K$ informative points relative to the position of the object that determine its geometry.\n\nNext, we collect a BEV feature $\\bff_{ik}$ associated to each neighbor point $\\bp_{ik}=\\bp_i+\\boldsymbol{\\delta}_{ik}$ :\n\\begin{equation}\\label{eq:bilinear}\n    \\begin{split}\n        \\bff_{ik}  = f_{\\mathrm{bilinear}}(\\F^d, \\bp_i+\\boldsymbol{\\delta}_{ik}),\n    \\end{split} \n\\end{equation}\nwhere $f_{\\mathrm{bilinear}}$ bilinearly interpolates the BEV feature map $\\F^d$. Note this step is the interaction between our set-based architecture manipulating query points $\\bq_i$ and the grid-based feature map $\\F^d$.  We then aggregate a single object query feature $\\bff_i^o$ from the $\\bff_{ik}$s:\n\\begin{equation}\\label{eq:query-feat}\n    \\begin{split}\n        \\bff_i^o = \\sum_k \\frac{e^{w_{ik}}}{\\sum_k e^{w_{ik}}} \\bff_{ik}.\n    \\end{split} \n\\end{equation}\nThis generates scene-aware features; each object query ``attends'' to a certain area in the scene.\n\n% is predicted by $\\bp_i=\\Phi_r^{\\mathrm{ref}}(\\bff^{e1}_{ij})$ where $h_\\phi^{\\mathrm{ref}}$ is a neural network. \n% Its corresponding features $\\bff^p_i$ are sampled from the BEV fearure map via bilinear interpolation. \n% Along with each neighbor point, we also predict a set of attention weights $\\{w_i^0, \\ldots, w_i^k, \\ldots w_i^K\\}\\subset\\R$ with another \n\n% Along each reference point, a set of neighbor points $\\{\\bp_j\\}\\subset\\R^2$ and a set of attention weights $\\{w_j\\}$ are predicted in the same way. In the meanwhile, their corresponding features $\\{\\bff^p_j\\}$ are sampled from the BEV feature map via bilinear interpolation. The final BEV features $\\bo_i^q$ for each query is the weighted average of the sampled features given by $\\bo_i^q=\\sum_j w_j \\bff^p_j$. Through this query procedure, each object query is equipped with point cloud BEV features and able to predict the bounding boxes.\n\nIn the current layer $\\ell$, the queries have not yet interacted with each other. To incorporate neighborhood information in object detection estimates, we use DGCNN-style operations to model a sparse set of relations. We construct a graph between the queries using a nearest neighbor search in feature space. In particular, we connect each query feature $\\bff_i^o$ to its 16 nearest neighbors as ablated in Table~\\ref{table:dgcnn-k}. %\\justin{do your ablation tests evaluate this 16 value? if so add a pointer to that section}.\n%\nIdentically to DGCNN, we learn a feature per edge $e_{ij}$ and then aggregate back to the vertices $i$ to produce the new set of object queries.  In detail, we write:\n\\begin{equation}\n    \\bq_{(\\ell+1)i} = \\max_{\\textrm{edges }e_{ij}} \\Phi_{\\mathrm{edge}}(\\bff_i^o, \\bff_j^o),\n\\end{equation}\nwhere $\\max$ denotes a channel-wise maximum and $\\Phi_{\\mathrm{edge}}$ is a neural network for computing edge features.  This completes our layer for computing $\\Q_{\\ell+1}$ from $\\Q_\\ell$. Optionally, we repeat this last step multiple times, in effect applying DGCNN to the features $\\bff_i^o$ to get the point set $\\Q_{\\ell+1}$.\n\n% We compute a feature $\\bff^{e0}_{ij}=\\bff_i^o\\vert\\vert\\bff_j^o$ describing each edge $e_{ij}$, where $\\vert\\vert$ denotes concatenation. Then, we use a neural network $\\Phi_{\\mathrm{edge}}$ to refine the edge features: \n% \\begin{equation}\\label{eq:edge-conv}\n%     \\begin{split}\n%         \\bff^{e*}_{ij}=\\Phi_{\\mathrm{edge}}(\\bff^{e0}_{ij}).\n%     \\end{split} \n% \\end{equation}\n\n% Finally, the per-object feature $\\bff_i^{o*}$ is retrieved by \n% \\begin{equation}\\label{eq:edge-pool}\n%     \\begin{split}\n%         \\bff_i^{o*} = \\max_j \\bff^{e*}_{ij}, \\forall e_{ij},\n%     \\end{split} \n% \\end{equation}\n% where $\\max$ is a channel-wise maximum operation.\n\n\n\n\n% \\begin{equation}\\label{eq:edge-conv}\n%     \\begin{split}\n%         \\bff^{e*}_{ij}=\\Phi^{\\mathrm{Edge}}(\\bff^{e0}_{ij}).\n%     \\end{split} \n% \\end{equation}\n\n% $\\bff^{e1}_{ij}=\\Phi^{\\mathrm{Edge}}(\\bff^{e0}_{ij})$. \n\n% Finally, the per object features $\\bo_i^l$ are gathered through a local max pooling over neighbors where $l$ denotes the level of features when multiple DGCNN are stacked. \n\n\\textbf{Set-to-set loss.} \nAfter $L$ Object DGCNN layers as described above, we are left with a set of $M^\\ast$ queries $\\Q_L$ used to predict our bounding boxes.  For each query $\\bq_{Li}$, we use a classification network to predict a categorical label $\\hat{c_i}$ and a regression network to predict bounding box parameters $\\hat{\\bb_i}$. Our final task is to assign the predictions to the ground-truth boxes and compute a set-to-set loss.\n\nMost object detection models minimize a loss $\\mathcal{L}_\\mathrm{od}$ given by\n\\begin{equation}\\label{eq:od_loss}\n     \\begin{split}\n       \\mathcal{L_{\\mathrm{od}}} = \\sum_{j=1}^{\\hat{M}} -\\log \\hat{p}_{\\hat{\\sigma}(j)}(\\hat{c}_j) + 1_{\\{c_{\\hat{\\sigma}(j)}\\neq\\varnothing\\}} \\mathcal{L}_{\\mathrm{box}}(\\hat{\\bb}_j, \\bb_{\\hat{\\sigma}(j)}),\n     \\end{split}\n\\end{equation}\nwhere $\\hat{M}=H^{\\mathrm{d}}*W^{\\mathrm{d}}$, $\\hat{\\sigma}(*)$ returns the corresponding index of the ground-truth bounding box, $\\hat{p}_{\\hat{\\sigma}(j)}(c_j)$ is the probability of class $c_{\\hat{\\sigma}(j)}$ for the prediction with index $\\sigma(j)$, $\\varnothing$ denotes an invalid box, and $\\mathcal{L}_\\mathrm{box}$ is typically the $\\mathcal{L}_1$ distance. Different matchings $\\hat{\\sigma}$ yield different optimization landscapes and hence different prediction models. Pillar-OD~\\cite{wang2020pillar} and CenterPoint~\\cite{yin2021center} employ a simple $\\hat{\\sigma}$ to determine the ground-truth box used to evaluate the box predicted at BEV pixel $j$:\n\\begin{equation}\\label{eq:od-sigma}\n     \\hat\\sigma_{\\mathrm{overlap}}(j) = \\left\\{\n             \\begin{array}{lr}\n             j^{'}, \\qquad \\mathrm{if\\;} \\bb_{j^{'}} \\mathrm{\\;overlaps\\; with \\; BEV \\; pixel\\;} j; \\\\\n             \\varnothing, \\qquad\\mathrm{otherwise} .\n             \\end{array}\n\\right.\n\\end{equation}\nThis strategy can assign a box to multiple nearby BEV pixels. This one-to-many assignment provides dense supervision for the object detector and eases optimization. Since the training objective encourages each BEV pixel to predict the same surrounding box, however, redundant boxes are inevitable. So, NMS is usually required to remove redundant boxes at inference time.\n\nRather than performing dense predictions in the BEV, we make per-query predictions. \nTypically, $M^{\\ast}$ is much larger than the number of ground-truth boxes $M$. To account for this difference, we pad the set of ground-truth boxes with $\\varnothing$s (no object) up to $M^{\\ast}$. %\\justin{should this be $M^\\ast$?}\\yue{yes}\nFollowing~\\cite{detr}, we use an objective built on an optimal matching between these two sets. We define the optimal bipartite matching as \n\\begin{equation}\\label{eq:matching}\n     \\begin{split}\n       \\sigma^{\\ast} = \\argmin_{\\sigma \\in \\mathcal{P}}\\sum_{j=1}^M -1_{\\{c_j=\\varnothing\\}}\\hat{p}_{\\sigma(j)}(c_j) + 1_{\\{c_j=\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(\\bb_j, \\hat{\\bb}_{\\sigma(j)}),\n     \\end{split}\n\\end{equation}\nwhere $\\mathcal{P}$ denotes the set of permutations, $\\hat{p}_{\\sigma(j)}(c_j)$ is the probability of class $c_j$ for the prediction with index $\\sigma(j)$, and $\\mathcal{L}_{\\mathrm{box}}$ is the $\\mathcal{L}_1$ loss for bounding box parameters. We use the Hungarian algorithm~\\cite{Kuhn55thehungarian} to solve this assignment problem, as in~\\cite{StewartAN16,detr}. Our final set-to-set loss adapts \\eqref{eq:od_loss}:\n\\begin{equation}\\label{eq:sup_loss}\n     \\begin{split}\n       \\mathcal{L_{\\mathrm{sup}}} = \\sum_{j=1}^N -\\log\\hat{p}_{\\sigma^{\\ast}(j)}(c_j) + 1_{\\{c_j=\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(\\bb_j, \\hat{\\bb}_{\\sigma^{\\ast}(j)}).\n     \\end{split}\n\\end{equation}\n%where $\\sigma^{\\ast}(*)$ is the optimal assignment computed in \\eqref{eq:matching} and $\\mathcal{L_{\\mathrm{sup}}}$ denotes the supervised loss. %<--- they're right above\n\n"
            },
            "section 5": {
                "name": "Distillation",
                "content": "\n\\label{method:distill}\n\n\n\nObject DGCNN enables a new set-to-set knowledge distillation (KD) pipeline. KD usually involves a teacher model $\\mathcal{T}$ and a student model $\\mathcal{S}$. The common practice is to align the outputs of the student with those of the teacher using $\\mathcal{L}_2$ distance or KL-divergence. In past 3D object detection methods, since final performance heavily relies on NMS and the predictions are post-processed to be a smaller set, distilling the teacher to the student is neither efficient nor effective. Since our set-based detection model is NMS-free, we can easily distill the information between models with homogeneous detection heads %\\justin{what does this mean?} \n(per-query object detection head in our case). First, we train a teacher $\\mathcal{T}$ using the method above with the loss in \\eqref{eq:sup_loss}. Then, we train a student $\\mathcal{S}$ with supervision given by $\\mathcal{T}$ and the ground-truth. The class label and box parameters predicted by the teacher for each object query are $c_j^{\\mathcal{T}}$ and $\\bb_j^{\\mathcal{T}}$, respectively. The corresponding student outputs are $c_j^{\\mathcal{S}}$ and $\\bb_j^{\\mathcal{S}}$. We find an optimal matching between the output set of the teacher and that of the student:\n\\begin{equation}\\label{eq:distill:matching}\n     \\begin{split}\n       \\sigma^{\\ast}_d = \\argmin_{\\sigma_d \\in \\mathcal{P}}\\sum_j^N -\\log p_{\\sigma_d(j)}(c_j^{\\mathcal{T}}) + \\mathcal{L}_{\\mathrm{box}}(\\bb_j^{\\mathcal{T}}, \\bb_{\\sigma_d(j)}^{\\mathcal{S}}).\n     \\end{split}\n\\end{equation}\nThen, the optimal matching's KD loss is given by\n\\begin{equation}\\label{eq:distill_loss}\n     \\begin{split}\n       \\mathcal{L_{\\mathrm{distill}}} = \\sum_j^N -\\log\\hat{p}_{\\sigma^{\\ast}_d(j)}(c_j^{\\mathcal{T}}) + \\mathcal{L}_{\\mathrm{box}}(\\bb_j^{\\mathcal{T}}, \\bb_{\\sigma^{\\ast}_d(j)}^{\\mathcal{S}}).\n     \\end{split}\n\\end{equation}\n%where $\\sigma^{\\ast}_d(*)$ is the optimal assignment for this distillation set-to-set matching. % just defined this\nSo the overall loss during KD is $\\mathcal{L} = \\alpha\\mathcal{L_{\\mathrm{sup}}} + \\beta\\mathcal{L_{\\mathrm{distill}}}$,\n% \\begin{equation}\\label{eq:overall_loss}\n%      \\begin{split}\n%     \\mathcal{L} = \\alpha\\mathcal{L_{\\mathrm{sup}}} + \\beta\\mathcal{L_{\\mathrm{distill}}},\n%      \\end{split}\n% \\end{equation}\nwhere $\\alpha$ and $\\beta$ balance the supervised loss and distillation loss. In practice, we use $\\alpha$ = $\\beta$ = 1."
            },
            "section 6": {
                "name": "Experiments",
                "content": "\n\n\n\n\n\n\n\n\n\n\nWe present our experiments in four parts. We introduce the dataset, metrics, implementation, and optimization details in~\\S\\ref{sec:detail}. Then, we demonstrate performance on the nuScenes dataset~\\cite{nuScenes2019} in \\S\\ref{sec:objdgcnn}. We present knowledge distillation results in \\S\\ref{sec:distill}. Finally, we provide ablation studies in~\\S\\ref{sec:ablation}. %on DGCNN versus self-attention and different distillation setups in~\\S\\ref{sec:ablation}.\n\n",
                "subsection 6.1": {
                    "name": "Training \\& testing procedures",
                    "content": "\n\\label{sec:detail}\n\\textbf{Dataset.} We experiment on the nuScenes dataset~\\cite{nuScenes2019}. nuScenes provides rich annotations and diverse scenes. It has 1K short sequences captured in Boston and Singapore with 700, 150, 150 sequences for training, validation, and testing, respectively. Each sequence is $\\sim\\!20$s and contains 400 frames. This dataset provides annotation every 0.5s, leading to 28K, 6K, 6K annotated frames for training, validation, and testing. nuScenes uses 32-beam LiDAR, producing 30K points per frame. Following common practice, we use calibrated vehicle pose information to aggregate every 9 non-key frames to key frames, so each annotated frame has $\\sim\\!300$K points. The annotations include 23 classes with a long-tail distribution, of which 10 classes are included in the benchmark. \n\n\\textbf{Metrics.} The major metrics are mean average precision (mAP) and the nuScenes detection score (NDS). In addition, we use a set of true positive metrics (TP metrics), which include average translation error (ATE), average scale error (ASE), average orientation error (AOE), average velocity error (AVE), and average attribute error (AAE). These metrics are computed in the physical unit. \n\n% In contrast to computing mAP based on intersection-over-union (IoU) in 2D, nuScenes defines a match by the 2D center distance on the ground plane to account for objects with small footprints. nuScenes calculates AP as the normalized area under the precision recall curve for recall and precision over 10\\%. The mAP is the average AP over matching thresholds of $\\mathrm{D}$ = \\{0.5, 1, 2, 4\\} meters and the set of classes $\\mathrm{C}$, given by $\\mathrm{mAP}=\\frac{1}{\\lVert\\mathrm{C}\\rVert\\lVert\\mathrm{D}\\rVert}\\sum_{c\\in\\mathrm{C}}\\sum_{d\\in\\mathrm{D}}\\mathrm{AP}_{c, d}$. In addition, nuScenes defines a set of true positive metrics (TP metrics) for each prediction that is matched to a ground-truth box with a 2m center distance. The TP metrics include average translation error (ATE), average scale error (ASE), average orientation error (AOE), average velocity error (AVE), and average attribute error (AAE). These metrics are computed in the physical unit. For each TP metric, the mean TP metric (mTP) is computed over all classes as  $\\mathrm{mTP}=\\frac{1}{\\mathrm{C}}\\sum_{c\\in\\mathrm{C}}\\mathrm{TP}_c$.%\\justin{$\\mathbb C$ should be the set of complex numbers...} \n% To capture all aspects of the detection task, a consolidated scalar metric--the NDS--is defined by \n% \\begin{equation}\n%     \\begin{split}\n%     \\mathrm{NDS} = \\frac{1}{10}[5\\mathrm{mAP}+\\sum_{\\mathrm{mTP}\\in \\mathbb{TP}}(1-\\min(1, \\mathrm{mTP}))].\n%     \\end{split}\n% \\end{equation}\n\n\\textbf{Model architecture.} Our model consists of three parts: a point-based feature extractor, a DGCNN to encode object queries and to connect the point cloud features to object queries, and a detection head to output the categorical label and bounding box parameters. We experiment with PointPillars~\\cite{Lang_2019_CVPR} and SparseConv~\\cite{sparseconv} as feature extractors. The three blocks of the PointPillars backbone have $[3, 5, 5]$ convolutional layers, with dimensions $[64, 128, 256]$ and strides $[2, 2, 2]$; the input features are downsampled to 1/2, 1/4, 1/8 of the original feature map. For SparseConv, we use four blocks of $[3, 3, 3, 2]$ 3D sparse convolutional layers, with dimensions $[16, 32, 64, 128]$ and strides $[2, 2, 2, 1]$; the input features are downsampled to 1/2, 1/4, 1/8, 1/8 of the original feature map. For SparseConv, %after the features are learned through this process, \nwe transform the features into BEV by collapsing the $z$-axis. Both backbones use two deformable self-attention~\\cite{zhu2021deformable} layers with dimensions $[256, 256]$ to transform the BEV features. Then, we use two DGCNNs to encode the object queries. Each DGCNN~\\cite{dgcnn} contains two EdgeConv layers with dimensions $[256, 256]$, both with 16 nearest neighbors. %\\justin{following sentence is hard to read:} \nFor each object query, we predict four points in the BEV to obtain and aggregate the BEV features. \n% a reference point along with four offsets, which are used to find its four neighbor points, in the BEV is predicted from the DGCNN features. \nThe final feature for this object query is the weighted sum of features of these four BEV points. The final detection head takes the features of each object query and predicts  class label and bounding box parameters w.r.t.\\ the reference point. \n\n\\textbf{Training \\& inference.} We use AdamW~\\cite{loshchilov2018decoupled} to train the model.  The weight decay for AdamW is $10^{-2}$. Following a cyclic schedule~\\cite{cyclic}, the learning rate is initially $10^{-4}$ and gradually increased to $10^{-3}$, which is finally decreased to $10^{-8}$. The model is initialized with a pre-trained PointPillars network on the same dataset. We train for 20 epochs on 8 RTX 3090 GPUs. During inference, we take the top 100 objects with highest classification scores as the final predictions.% \\justin{take them for what? not sure what this means. does not the number of objects vary in each scene}\nWe \\emph{do not} use any post-processing such as NMS. For evaluation, we use the toolkit provided with the nuScenes dataset.\n\n% \\textbf{Inference.} During inference, we take the top 100 objects with highest classification scores as the final predictions.% \\justin{take them for what? not sure what this means. does not the number of objects vary in each scene}\n% We \\emph{do not} use any post-processing such as NMS. For evaluation, we use the toolkit provided with the nuScenes dataset.\n\n\n\n"
                },
                "subsection 6.2": {
                    "name": "Object DGCNN",
                    "content": "\n\\label{sec:objdgcnn}\nWe compare to top-performing methods on the nuScenes dataset in Table~\\ref{table:det}. PointPillars~\\cite{Lang_2019_CVPR} is an anchor-based method with reasonable trade-off between performance and efficiency. FreeAnchor~\\cite{zhang2019freeanchor} extends PointPillars by learning how to assign anchors to the ground-truth. RegNetX-400MF-SECFPN~\\cite{radosavovic2020designing} uses neural architecture search (NAS) to learn a flexible neural network for 3D detection; it is essentially a variant of PointPillars with an enhanced backbone network. Different from anchor-based methods, Pillar-OD~\\cite{wang2020pillar} makes predictions per pillar, alleviating the class imbalance issue caused by anchors. CenterPoint~\\cite{yin2021center} exploits similar detection heads, with better performance using better training scheduling and data augmentation. For these methods, we use re-implementations in MMDetection3D~\\cite{mmdet3d2020}, which match the performances in the original papers.\n\nWe mainly compare to CenterPoint with both PointPillars and SparseConv backbones, denoted as ``voxel'' and ``pillar'' respectively. Our method outperforms other methods significantly including CenterPoint with NMS. Without NMS, the performance of CenterPoint drops considerably while our method is unaffected by NMS. This finding verifies the DGCNN implicitly models object relations and removes redundant boxes. \n\n"
                },
                "subsection 6.3": {
                    "name": "Set-to-set distillation",
                    "content": "\n\\label{sec:distill}\n\n\nIn this section, we present experiments involving our set-to-set distillation pipeline. We conduct three types of distillation. First, we distill a teacher model with a SparseConv backbone to a student model with a PointPillars backbone (denoted as ``voxel$\\rightarrow$pillar''). This aligns with the common knowledge distillation setup for classification.  We %compare our proposed distillation objective to its \ncompare to feature-based distillation and pseudo label based methods. % counterpart, \n The objective of feature-based distillation is to align the middle-level features of the teacher model and the student model while the pseudo label based methods generate pseudo training examples with the pre-trained teacher networks. As Table~\\ref{table:distill} shows, our set-to-set distillation achieves better performance, confirming that distilling the last stage of the object detection model is more effective than distilling feature maps. \n\nSecond, we perform self-distillation~\\cite{FurlanelloLTIA18} (denoted as ``voxel$\\rightarrow$voxel'' and ``pillar$\\rightarrow$pillar''), where the teacher and the student are identical and take the same point clouds as input. As Table~\\ref{table:self-distill} shows, even when the teacher network and the student network have the same capacity, self-distillation still introduces a performance boost. This finding is consistent with the results in~\\cite{FurlanelloLTIA18}.\n\nFinally, we try distillation with privileged information~\\cite{LopSchBotVap16}, where the teacher gets access to privileged information but the student does not. Following~\\cite{wang2020pad}, the teacher takes dense point clouds, and the student takes sparse point clouds (denoted as \"dense$\\rightarrow$sparse\").  To limit computation time, we train each model over a shorter period of time. The goal is for the student model to learn the same representations as the teacher model without knowing the dense inputs. In Table~\\ref{table:sparse-distill}, we compare this setup with self-distillation, where the difference is the teacher model and the student model take the same sparse point clouds in self-distillation. The student achieves better performance when the teacher takes dense point clouds. The result suggests that set-to-set knowledge distillation is an effective approach to transfer insight from privileged information. \n\n\n\n"
                },
                "subsection 6.4": {
                    "name": "Ablation",
                    "content": "\n\\label{sec:ablation}\n\n\nWe provide ablation studies on different components of our model to verify assorted design choices. First, we study the improvements of DGCNN over its counterpart,  multi-head self-attention~\\cite{Vaswani2017}. The multi-head self-attention has 8 heads with embedding dimension 256 and LayerNorm~\\cite{BaKH16}, following common usage. The DGCNN has two EdgeConv layers with dimensions $[256, 256].$ The number of neighbors $K$ in EdgeConv is 16. In principle, DGCNN is a sparse version of multi-head self-attention; the sparse structure reduces overhead in back-propagation and leads to sharper ``attention maps'' as well as faster convergence.\n\n\n\n\nTable~\\ref{table:dgcnn-attention} shows the comparisons: DGCNN consistently outperforms multi-head self-attention. This aligns with our hypothesis: objects are distributed sparsely in the scene, so dense interactions among objects are neither efficient nor effective. Furthermore, we study the effect of number of neighbors in DGCNN. When it is 1, The model reduces to an architecture without object interaction. As we increase the number, it approaches multi-head self-attention. As shown in Table~\\ref{table:dgcnn-k}, the sweet spot is 16, which appears to balance object interactions and sparsity. \n\nWe also investigate improvements introduced when more DGCNNs are stacked in Table~\\ref{table:dgcnn-layer}. This result suggests it is beneficial to incorporate multiple DGCNNs to model the dynamic object relations. % \\justin{and?}\\section{Conclusion}\n\nObject DGCNN is a highly-efficient 3D object detector for point clouds. It is able to learn object interactions via dynamic graphs and is optimized through a set-to-set loss, leading to NMS-free detection. The success of Object DGCNN indicates that many post-processing operations in 3D object detection are likely unnecessary and can be replaced with suitable neural network modules. Moreover, we introduce a set-to-set knowledge distillation pipeline enabled by the Object DGCNN. This new pipeline significantly simplifies knowledge distillation for 3D object detection and may be applicable to other tasks like 3D model compression. \n% Our model and distillation pipeline can be incorporated as-is into any object detection model for point clouds. \nBeyond the direct usage of our model, our experiments suggest several future directions to address current limitations. For example, our method is initialized with a pre-trained backbone network. Training the model from scratch remains elusive due to the sparse set-to-set supervision; solving this issue may yield improved generalization as in~\\cite{he2018rip}. Furthermore, studying 3D-specific feature extractors %that generalize over broader sets of point cloud data %\\justin{not sure what this means} \nwill improve the speed and generalizability of 3D object detection. Finally, the large amount of unlabeled data available at training time can serve as another type of privileged information to apply self-supervised learning to 3D domains through set-to-set distillation.\n\n\\textbf{Potential impact.} Our method aims to improve the object detection pipeline, which is crucial for the safety of autonomous driving systems. One potential negative impact of our work is that it still lacks theoretical guarantees, similar to many deep learning methods. Future work to improve applicability in this domain might consider challenges of \\emph{explainability} and \\emph{transparency}. "
                }
            },
            "section 7": {
                "name": "Acknowledgement",
                "content": "\nThe MIT Geometric Data Processing group acknowledges the generous support of Army Research Office grants W911NF2010168 and W911NF2110293, of Air Force Office of Scientific Research award FA9550-19-1-031, of National Science Foundation grants IIS-1838071 and CHS-1955697, from the CSAIL Systems that Learn program, from the MIT--IBM Watson AI Laboratory, from the Toyota--CSAIL Joint Research Center, from a gift from Adobe Systems, from an MIT.nano Immersion Lab/NCSOFT Gaming Program seed grant, and from the Skoltech--MIT Next Generation Program. % \\input{section/broader_impact}\n\n{\\small\n    \\bibliographystyle{unsrt}\n    \\bibliography{neurips_2021}\n}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
            },
            "section 8": {
                "name": "Checklist",
                "content": "\n\n\\begin{comment}\n%%% BEGIN INSTRUCTIONS %%%\nThe checklist follows the references.  Please\nread the checklist guidelines carefully for information on how to answer these\nquestions.  For each question, change the default \\answerTODO{} to \\answerYes{},\n\\answerNo{}, or \\answerNA{}.  You are strongly encouraged to include a {\\bf\njustification to your answer}, either by referencing the appropriate section of\nyour paper or providing a brief inline description.  For example:\n\\begin{itemize}\n  \\item Did you include the license to the code and datasets? \\answerYes{See Section~\\ref{gen_inst}.}\n  \\item Did you include the license to the code and datasets? \\answerNo{The code and the data are proprietary.}\n  \\item Did you include the license to the code and datasets? \\answerNA{}\n\\end{itemize}\nPlease do not modify the questions and only use the provided macros for your\nanswers.  Note that the Checklist section does not count towards the page\nlimit.  In your paper, please delete this instructions block and only keep the\nChecklist section heading above along with the questions/answers below.\n%%% END INSTRUCTIONS %%%\n\\end{comment}\n\n\\begin{enumerate}\n\n\\item For all authors...\n\\begin{enumerate}\n  \\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n    \\answerYes{}\n  \\item Did you describe the limitations of your work?\n    \\answerYes{}\n  \\item Did you discuss any potential negative societal impacts of your work?\n    \\answerYes{}\n  \\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\n    \\answerYes{}\n\\end{enumerate}\n\n\\item If you are including theoretical results...\n\\begin{enumerate}\n  \\item Did you state the full set of assumptions of all theoretical results?\n    \\answerNA{}\n\t\\item Did you include complete proofs of all theoretical results?\n    \\answerNA{}\n\\end{enumerate}\n\n\\item If you ran experiments...\n\\begin{enumerate}\n  \\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\n    \\answerYes{}\n  \\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\n    \\answerYes{}\n\t\\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\n    \\answerNo{}\n\t\\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\n    \\answerYes{}\n\\end{enumerate}\n\n\\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n\\begin{enumerate}\n  \\item If your work uses existing assets, did you cite the creators?\n    \\answerYes{}\n  \\item Did you mention the license of the assets?\n    \\answerNA{}\n  \\item Did you include any new assets either in the supplemental material or as a URL?\n    \\answerNo{}\n  \\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\n    \\answerNA{}\n  \\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\n    \\answerNo{}\n\\end{enumerate}\n\n\\item If you used crowdsourcing or conducted research with human subjects...\n\\begin{enumerate}\n  \\item Did you include the full text of instructions given to participants and screenshots, if applicable?\n    \\answerNA{}\n  \\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\n    \\answerNA{}\n  \\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\n    \\answerNA{}\n\\end{enumerate}\n\n\\end{enumerate}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\clearpage\n"
            },
            "section 9": {
                "name": "Supplementary Material",
                "content": "\n\n",
                "subsection 9.1": {
                    "name": "Results of distillation with longer training.",
                    "content": "\nIn this section, we provide results when the model is distilled with 1, 6, and 20 epochs. The teacher model and the student model are both constructed with a PointPillars backbone. Table~\\ref{table:self-distill-longer} shows the results. In all regimes, the models with distillation improve over their baselines, which verifies the efficacy of the set-to-set distillation. \n\n\n\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Time complexity.",
                    "content": "\nWe compare the time complexity of PointPillars, Pillar-OD, CenterPoint, and our proposed model. Table~\\ref{table:time-complexity} shows that our model is more efficient than others at inference time thanks to its NMS-free characteristic. The performance is measured on a single Nvidia RTX 3090.\n\n\n\n"
                },
                "subsection 9.3": {
                    "name": "Visualization.",
                    "content": "\n\nWe provide visualization of predictions by our model in Figure~\\ref{viz}. Without NMS, our model makes a sparse set of predictions. \n\n\n"
                }
            }
        },
        "equations": {
            "eq:1": "\\begin{equation}\n     \\begin{split}\n        \\bff^{\\mathrm{pillar}}_i = \\mathrm{PN}(\\{\\bff_j |  \\bx_j \\in F_P(\\bp_i)\\}), \n     \\end{split}\n\\end{equation}",
            "eq:eq:voxelfeature": "\\begin{equation}\\label{eq:voxelfeature}\n     \\begin{split}\n        \\bff^{\\mathrm{voxel}}_i = \\mathrm{PN}(\\{\\bff_j |  \\bx_j \\in F_V(i)\\}), \n     \\end{split}\n\\end{equation}",
            "eq:eq:query-point": "\\begin{equation}\\label{eq:query-point}\n    \\begin{split}\n        &\\bp_i = \\Phi_\\mathrm{ref} (\\bq_i), \\qquad\\qquad\n        \\{\\boldsymbol{\\delta}_i^0, \\ldots, \\boldsymbol{\\delta}_i^k, \\ldots \\boldsymbol{\\delta}_i^K\\} = \\Phi_\\mathrm{neighbor}(\\bq_i), \\\\\n        &\\{w_i^0, \\ldots, w_i^k, \\ldots w_i^K\\} = \\Phi_\\mathrm{atten}(\\bq_i),\n    \\end{split} \n\\end{equation}",
            "eq:eq:bilinear": "\\begin{equation}\\label{eq:bilinear}\n    \\begin{split}\n        \\bff_{ik}  = f_{\\mathrm{bilinear}}(\\F^d, \\bp_i+\\boldsymbol{\\delta}_{ik}),\n    \\end{split} \n\\end{equation}",
            "eq:eq:query-feat": "\\begin{equation}\\label{eq:query-feat}\n    \\begin{split}\n        \\bff_i^o = \\sum_k \\frac{e^{w_{ik}}}{\\sum_k e^{w_{ik}}} \\bff_{ik}.\n    \\end{split} \n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\bq_{(\\ell+1)i} = \\max_{\\textrm{edges }e_{ij}} \\Phi_{\\mathrm{edge}}(\\bff_i^o, \\bff_j^o),\n\\end{equation}",
            "eq:eq:od_loss": "\\begin{equation}\\label{eq:od_loss}\n     \\begin{split}\n       \\mathcal{L_{\\mathrm{od}}} = \\sum_{j=1}^{\\hat{M}} -\\log \\hat{p}_{\\hat{\\sigma}(j)}(\\hat{c}_j) + 1_{\\{c_{\\hat{\\sigma}(j)}\\neq\\varnothing\\}} \\mathcal{L}_{\\mathrm{box}}(\\hat{\\bb}_j, \\bb_{\\hat{\\sigma}(j)}),\n     \\end{split}\n\\end{equation}",
            "eq:eq:od-sigma": "\\begin{equation}\\label{eq:od-sigma}\n     \\hat\\sigma_{\\mathrm{overlap}}(j) = \\left\\{\n             \\begin{array}{lr}\n             j^{'}, \\qquad \\mathrm{if\\;} \\bb_{j^{'}} \\mathrm{\\;overlaps\\; with \\; BEV \\; pixel\\;} j; \\\\\n             \\varnothing, \\qquad\\mathrm{otherwise} .\n             \\end{array}\n\\right.\n\\end{equation}",
            "eq:eq:matching": "\\begin{equation}\\label{eq:matching}\n     \\begin{split}\n       \\sigma^{\\ast} = \\argmin_{\\sigma \\in \\mathcal{P}}\\sum_{j=1}^M -1_{\\{c_j=\\varnothing\\}}\\hat{p}_{\\sigma(j)}(c_j) + 1_{\\{c_j=\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(\\bb_j, \\hat{\\bb}_{\\sigma(j)}),\n     \\end{split}\n\\end{equation}",
            "eq:eq:sup_loss": "\\begin{equation}\\label{eq:sup_loss}\n     \\begin{split}\n       \\mathcal{L_{\\mathrm{sup}}} = \\sum_{j=1}^N -\\log\\hat{p}_{\\sigma^{\\ast}(j)}(c_j) + 1_{\\{c_j=\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(\\bb_j, \\hat{\\bb}_{\\sigma^{\\ast}(j)}).\n     \\end{split}\n\\end{equation}",
            "eq:eq:distill:matching": "\\begin{equation}\\label{eq:distill:matching}\n     \\begin{split}\n       \\sigma^{\\ast}_d = \\argmin_{\\sigma_d \\in \\mathcal{P}}\\sum_j^N -\\log p_{\\sigma_d(j)}(c_j^{\\mathcal{T}}) + \\mathcal{L}_{\\mathrm{box}}(\\bb_j^{\\mathcal{T}}, \\bb_{\\sigma_d(j)}^{\\mathcal{S}}).\n     \\end{split}\n\\end{equation}",
            "eq:eq:distill_loss": "\\begin{equation}\\label{eq:distill_loss}\n     \\begin{split}\n       \\mathcal{L_{\\mathrm{distill}}} = \\sum_j^N -\\log\\hat{p}_{\\sigma^{\\ast}_d(j)}(c_j^{\\mathcal{T}}) + \\mathcal{L}_{\\mathrm{box}}(\\bb_j^{\\mathcal{T}}, \\bb_{\\sigma^{\\ast}_d(j)}^{\\mathcal{S}}).\n     \\end{split}\n\\end{equation}"
        },
        "git_link": "https://github.com/WangYueFt/detr3d"
    }
}