{
    "meta_info": {
        "title": "Pre-training Molecular Graph Representation with 3D Geometry",
        "abstract": "Molecular graph representation learning is a fundamental problem in modern\ndrug and material discovery. Molecular graphs are typically modeled by their 2D\ntopological structures, but it has been recently discovered that 3D geometric\ninformation plays a more vital role in predicting molecular functionalities.\nHowever, the lack of 3D information in real-world scenarios has significantly\nimpeded the learning of geometric graph representation. To cope with this\nchallenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework\nwhere self-supervised learning (SSL) is performed by leveraging the\ncorrespondence and consistency between 2D topological structures and 3D\ngeometric views. GraphMVP effectively learns a 2D molecular graph encoder that\nis enhanced by richer and more discriminative 3D geometry. We further provide\ntheoretical insights to justify the effectiveness of GraphMVP. Finally,\ncomprehensive experiments show that GraphMVP can consistently outperform\nexisting graph SSL methods.",
        "author": "Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, Jian Tang",
        "link": "http://arxiv.org/abs/2110.07728v2",
        "category": [
            "cs.LG",
            "cs.CV",
            "eess.IV",
            "q-bio.QM"
        ],
        "additionl_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": " \\label{sec:intro}\nIn recent years, drug discovery has drawn increasing interest in the machine learning community. Among many challenges therein, how to discriminatively represent a molecule with a vectorized embedding remains a fundamental yet open challenge. The underlying problem can be decomposed into two components: how to design a common latent space for molecule graphs ({\\ie}, designing a suitable encoder) and how to construct an objective function to supervise the training ({\\ie}, defining a learning target). Falling broadly into the second category, our paper studies self-supervised molecular representation learning by leveraging the consistency between 3D geometry and 2D topology.\n\nMotivated by the prominent success of the pretraining-finetuning pipeline~\\citep{devlin2018bert}, unsupervisedly pre-trained graph neural networks for molecules yields promising performance on downstream tasks and becomes increasingly popular~\\citep{velivckovic2018deep,sun2019infograph,liu2018n,hu2019strategies,You2020GraphCL,you2021graph}. The key to pre-training lies in finding an effective proxy task ({\\ie}, training objective) to leverage the power of large unlabeled datasets. Inspired by \\citep{schutt2017schnet,liu2018n,liu2021spherical} that molecular properties~\\citep{gilmer2017neural,liu2018n} can be better predicted by 3D geometry due to its encoded energy knowledge, we aim to make use of the 3D geometry of molecules in pre-training. However, the stereochemical structures are often very expensive to obtain, making such 3D geometric information scarce in downstream tasks. To address this problem, we propose the Graph Multi-View Pre-training (\\model) framework, where a 2D molecule encoder is pre-trained with the knowledge of 3D geometry and then fine-tuned on downstream tasks without 3D information. Our learning paradigm, during pre-training, injects the knowledge of 3D molecular geometry to a 2D molecular graph encoder such that the downstream tasks can benefit from the implicit 3D geometric prior even if there is no 3D information available.\n\nWe attain the aforementioned goal by leveraging two pretext tasks on the 2D and 3D molecular graphs: one contrastive and one generative SSL. Contrastive SSL creates the supervised signal at an \\textbf{inter-molecule} level: the 2D and 3D graph pairs are positive if they are from the same molecule, and negative otherwise; Then contrastive SSL~\\citep{wang2020understanding} will align the positive pairs and contrast the negative pairs simultaneously. Generative SSL~\\citep{vincent2008extracting,kingma2013auto,higgins2016beta}, on the other hand, obtains the supervised signal in an \\textbf{intra-molecule} way: it learns a 2D/3D representation that can reconstruct its 3D/2D counterpart view for each molecule itself. \nTo cope with the challenge of measuring the quality of reconstruction on molecule 2D and 3D space, we further propose a novel surrogate objective function called variation representation reconstruction (VRR) for the generative SSL task, which can effectively measure such quality in the continuous representation space. The knowledge acquired by these two SSL tasks is complementary, so our \\model\\ framework integrates them to form a more discriminative 2D molecular graph representation. \nConsistent and significant performance improvements empirically validate the effectiveness of GraphMVP.\n\nWe give additional insights to justify the effectiveness of GraphMVP. First, \\model\\, is a self-supervised learning approach based on maximizing mutual information (MI) between 2D and 3D views, enabling the learnt representation to capture high-level factors~\\citep{belghazi2018mutual,tschannen2019mutual,bachman2019learning} in molecule data. Second, we find that 3D molecular geometry is a form of privileged information~\\cite{vapnik2009new,vapnik2015learning}. It has been proven that using privileged information in training can accelerate the speed of learning. We note that privileged information is only used in training, while it is not available in testing. This perfectly matches our intuition of pre-training molecular representation with 3D geometry.\n\nOur contributions include\n(1) To our best knowledge, we are the first to incorporate the 3D geometric information into graph SSL;\n(2) We propose one contrastive and one generative SSL tasks for pre-training. Then we elaborate their difference and empirically validate that combining both can lead to a better representation;\n(3) We provide theoretical insights and case studies to justify why adding 3D geometry is beneficial;\n(4) We achieve the SOTA performance among all the SSL methods.\n\n\\textbf{Related work.}\nWe briefly review the most related works here and include a more detailed summarization in~\\Cref{appendix:related_work}. Self-supervised learning (SSL) methods have attracted massive attention to graph applications~\\citep{liu2021graph,xie2021self,wu2021self,liu2021self}. In general, there are roughly two categories of graph SSL: contrastive and generative, where they differ on the design of the supervised signals. Contrastive graph SSL~\\citep{velivckovic2018deep,sun2019infograph,hu2019strategies,You2020GraphCL,you2021graph} constructs the supervised signals at the \\textbf{inter-graph} level and learns the representation by contrasting with other graphs, while generative graph SSL~\\citep{hamilton2017inductive,liu2018n,hu2019strategies,hu2020gpt} focuses on reconstructing the original graph at the \\textbf{intra-graph} level. One of the most significant differences that separate our work from existing methods is that all previous methods \\textbf{merely} focus on 2D molecular topology. However, for scientific tasks such as molecular property prediction, 3D geometry should be incorporated as it provides complementary and comprehensive information~\\citep{schutt2017schnet,liu2021spherical}. To fill this gap, we propose GraphMVP to leverage the 3D geometry in graph self-supervised pre-training.\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": " \\label{sec:preliminary}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nWe first outline the key concepts and notations used in this work. Self-supervised learning (SSL) is based on the \\textit{view} design, where each view provides a specific aspect and modality of the data. Each molecule has two natural views: the 2D graph incorporates the topological structure defined by the adjacency, while the 3D graph can better reflect the geometry and spatial relation. From a chemical perspective, 3D geometric graphs focus on the \\textit{energy} while 2D graphs emphasize the \\textit{topological} information; thus they can be composed for learning more informative representation in \\model. \\textit{Transformation} is an atomic operation in SSL that can extract specific information from each view. Next, we will briefly introduce how to represent these two views.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\textbf{2D Molecular Graph} represents molecules as 2D graphs, with atoms as nodes and bonds as edges respectively. We denote each 2D graph as $g_{\\text{2D}} = (X, E)$, where $X$ is the atom attribute matrix and $E$ is the bond attribute matrix. Notice that here $E$ also includes the bond connectivity. Then we will apply a transformation function $T_{\\text{2D}}$ on the topological graph. Given a 2D molecular graph $g_{\\text{2D}}$, its representation $h_{2D}$ can be obtained from a \\textit{2D graph neural network (GNN)} model:\n\\begin{equation} \\label{eq:2d_gnn}\nh_{\\text{2D}} = \\text{GNN-2D}(T_{\\text{2D}}(g_{\\text{2D}})) = \\text{GNN-2D}(T_{\\text{2D}}(X, E)).\n\\end{equation}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\textbf{3D Molecular Graph} additionally includes spatial positions of the atoms, and they are needless to be static since atoms are in continual motion on \\textit{a potential energy surface}~\\citep{axelrod2020geom}. \\footnote{A more rigorous way of defining conformer is in~\\citep{moss1996basic}: a conformer is an isomer of a molecule that differs from another isomer by the rotation of a single bond in the molecule.} The 3D structures at the local minima on this surface are named \\textit{conformer}. As the molecular properties are conformers ensembled~\\citep{hawkins2017conformation}, \\model\\, provides a novel perspective on adopting 3D conformers for learning better representation. Given a conformer $g_{\\text{3D}} = (X, R)$, its representation via a \\textit{3D GNN} model is:\n\\begin{equation} \\label{eq:3d_gnn}\nh_{\\text{3D}} = \\text{GNN-3D}(T_{\\text{3D}}(g_{\\text{3D}})) = \\text{GNN-3D}(T_{\\text{3D}}(X, R)),\n\\end{equation}\nwhere $R$ is the 3D-coordinate matrix and $T_{\\text{3D}}$ is the 3D transformation.\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nIn what follows, for notation simplicity, we use $\\vx$ and $\\vy$ for the 2D and 3D graphs, {\\ie}, $\\vx \\triangleq g_{\\text{2D}}$ and $\\vy \\triangleq g_{\\text{3D}}$. Then the latent representations are denoted as $h_\\vx$ and $h_\\vy$.\n"
            },
            "section 3": {
                "name": "\\model: Graph Multi-View Pre-training",
                "content": " \\label{sec:method}\nOur model, termed as Graph Multi-View Pre-training (\\model), conducts self-supervised learning (SSL) pre-training with 3D information. The 3D conformers encode rich information about the molecule energy and spatial structure, which are complementary to the 2D topology. Thus, applying SSL between the 2D and 3D views will provide a better 2D representation, which implicitly embeds the ensembles of energies and geometric information for molecules.\n\nIn the following, we first present an overview of \\model, and then introduce two pretext tasks specialized concerning 3D conformation structures. Finally, we summarize a broader graph SSL family that prevails the 2D molecular graph representation learning with 3D geometry.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                "subsection 3.1": {
                    "name": "Overview of \\model",
                    "content": " \\label{sec:overview}\n\n\nIn general, \\model\\, exerts 2D topology and 3D geometry as two complementary views for each molecule. By performing SSL between these views, it is expected to learn a 2D representation enhanced with 3D conformation, which can better reflect certain molecular properties.\n\nAs generic SSL pre-training pipelines, \\model\\, has two stages: pre-training then fine-tuning. In the pre-training stage, we conduct SSL via auxiliary tasks on data collections that provide both 2D and 3D molecular structures. During fine-tuning, the pre-trained 2D GNN models are subsequently fine-tuned on specific downstream tasks, where only 2D molecular graphs are available.\n\nAt the SSL pre-training stage, we design two pretext tasks: one contrastive and one generative. We conjecture and then empirically prove that these two tasks are focusing on different learning aspects, which are summarized into the following two points. (1) From the perspective of representation learning, contrastive SSL utilizes \\textbf{inter-data} knowledge and generative SSL utilizes \\textbf{intra-data} knowledge. For contrastive SSL, one key step is to obtain the negative view pairs for inter-data contrasting; while generative SSL focuses on each data point itself, by reconstructing the key features at an intra-data level. (2) From the perspective of distribution learning, contrastive SSL and generative SSL are learning the data distribution from a \\textbf{local} and \\textbf{global} manner, respectively. Contrastive SSL learns the distribution locally by contrasting the pairwise distance at an inter-data level. Thus, with sufficient number of data points, the local contrastive operation can iteratively recover the data distribution. Generative SSL, on the other hand, learns the global data density function directly.\n\nTherefore, contrastive and generative SSL are essentially conducting representation and distribution learning with different intuitions and disciplines, and we expect that combining both can lead to a better representation. We later carry out an ablation study (\\Cref{sec:experiment_each_loss_component}) to verify this empirically.\nIn addition, to make the pretext tasks more challenging, we take views for each molecule by randomly masking $M$ nodes (and corresponding edges) as the transformation function, {\\ie}, $T_{\\text{2D}} = T_{\\text{3D}} =\\text{mask}$. This trick has been widely used in graph SSL~\\citep{hu2019strategies,You2020GraphCL,you2021graph} and has shown robust improvements.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 3.2": {
                    "name": "Contrastive Self-Supervised Learning between 2D and 3D Views",
                    "content": " \\label{sec:contrastive_SSL}\nThe main idea of contrastive self-supervised learning (SSL)~\\citep{oord2018representation,chen2020simple} is first to define positive and negative pairs of views from an inter-data level, and then to align the positive pairs and contrast the negative pairs simultaneously~\\citep{wang2020understanding}. For each molecule, we first extract representations from 2D and 3D views, {\\ie}, $h_\\vx$ and $h_\\vy$. Then we create positive and negative pairs for contrastive learning: the 2D-3D pairs $(\\vx,\\vy)$ for the same molecule are treated as positive, and negative otherwise. Finally, we align the positive pairs and contrast the negative ones. The pipeline is shown in~\\Cref{fig:both_ssl}. In the following, we discuss two common objective functions on contrastive graph SSL.\n\n\\textbf{InfoNCE} is first proposed in~\\citep{oord2018representation}, and its effectiveness has been validated both empirically~\\citep{chen2020simple,he2020momentum} and theoretically~\\citep{arora2019theoretical}. Its formulation is given as follows:\n\\begin{equation}\\label{eq:objective_infonce}\n\\small{\n\\fontsize{7.8}{1}\\selectfont\n\\begin{aligned}\n\\mathcal{L}_{\\text{InfoNCE}}=-\\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)}\\Big[\\log \\frac{\\exp(f_{\\vx}(\\vx, \\vy))}{\\exp(f_{\\vx}(\\vx, \\vy)) + \\sum\\limits_{j}  \\exp(f_{\\vx}(\\vx^{j},\\vy)})+\\log\\frac{\\exp(f_{\\vy}(\\vy,\\vx))}{\\exp(f_{\\vy}(\\vy,\\vx)) + \\sum\\limits_{j} \\exp(f_{\\vy}(\\vy^{j},\\vx))} \\Big],\n\\end{aligned}\n}\n\\end{equation}\nwhere $\\vx^{j}, \\vy^{j}$ are randomly sampled 2D and 3D views regarding to the anchored pair $(\\vx,\\vy)$. $f_{\\vx}(\\vx,\\vy)$ and $f_{\\vy}(\\vy,\\vx)$ are scoring functions for the two corresponding views, with flexible formulations. Here we adopt $f_\\vx(\\vx,\\vy) = f_\\vy(\\vy,\\vx) = \\langle h_\\vx, h_\\vy \\rangle$. More details are in~\\Cref{sec:app:contrastive_ssl}.\n\n\\textbf{Energy-Based Model with Noise Contrastive Estimation (EBM-NCE)} is an alternative that has been widely used in the line of graph contrastive SSL~\\citep{sun2019infograph,hu2019strategies,You2020GraphCL,you2021graph}. Its intention is essentially the same as InfoNCE, to align positive pairs and contrast negative pairs, while the main difference is the usage of binary cross-entropy and extra noise distribution for negative sampling:\n\\begin{equation} \\label{eq:objective_ebm_nce}\n\\small{\n\\begin{aligned}\n\\mathcal{L}_{\\text{EBM-NCE}}\n& = -\\frac{1}{2} \\mathbb{E}_{p(\\vy)} \\Big[\\mathbb{E}_{p_{n}(\\vx|\\vy)} \\log \\big(1-\\sigma(  f_x(\\vx, \\vy))\\big) + \\mathbb{E}_{p(\\vx|\\vy )} \\log \\sigma(  f_x(\\vx, \\vy)) \\Big]\\\\\n& ~~~~~ -\\frac{1}{2} \\mathbb{E}_{p(\\vx)} \\Big[\\mathbb{E}_{p_{n}(\\vy|\\vx)} \\log \\big(1-\\sigma(  f_y(\\vy,\\vx))\\big) + \\mathbb{E}_{p(\\vy,\\vx)} \\log \\sigma(  f_y(\\vy,\\vx)) \\Big],\n\\end{aligned}\n}\n\\end{equation}\nwhere $p_n$ is the noise distribution and $\\sigma$ is the sigmoid function. We also notice that the final formulation of EBM-NCE shares certain similarities with Jensen-Shannon estimation (JSE)~\\citep{nowozin2016f}. However, the derivation process and underlying intuition are different: EBM-NCE models the conditional distributions in MI lower bound (\\Cref{eq:MI_objective}) with EBM, while JSE is a special case of variational estimation of f-divergence. Since this is not the main focus of \\model, we expand the a more comprehensive comparison in~\\Cref{sec:app:contrastive_ssl}, plus the potential benefits with EBM-NCE.\n\nFew works~\\citep{hassani2020contrastive} have witnessed the effect on the choice of objectives in graph contrastive SSL. In \\model, we treat it as a hyper-parameter and further run ablation studies on them, {\\ie}, to solely use either InfoNCE ($\\mathcal{L}_{\\text{C}} = \\mathcal{L}_{\\text{InfoNCE}}$) or EMB-NCE ($\\mathcal{L}_{\\text{C}} = \\mathcal{L}_{\\text{EBM-NCE}}$).\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 3.3": {
                    "name": "Generative Self-Supervised Learning between 2D and 3D Views",
                    "content": " \\label{sec:generative_SSL}\nGenerative SSL is another classic track for unsupervised pre-training~\\citep{kingma2013auto,chen2016infogan,larsson2016learning,kingma2018glow}. It aims at learning an effective representation by self-reconstructing each data point. Specifically to drug discovery, we have one 2D graph and a certain number of 3D conformers for each molecule, and our goal is to learn a robust 2D/3D representation that can, to the most extent, recover its 3D/2D counterparts. By doing so, generative SSL can enforce 2D/3D GNN to encode the most crucial geometry/topology information, which can improve the downstream performance.\n\nThere are many options for generative models, including variational auto-encoder (VAE)~\\citep{kingma2013auto}, generative adversarial networks (GAN)~\\citep{goodfellow2014generative}, flow-based model~\\citep{dinh2016density}, etc. In \\model, we prefer VAE-like method for the following reasons: (1) The mapping between two molecular views is stochastic: multiple 3D conformers correspond to the same 2D topology; (2) An explicit 2D graph representation ({\\ie}, feature encoder) is required for downstream tasks; (3) Decoders for structured data such as graph are often highly nontrivial to design, which make them a suboptimal choice. \n\n\\textbf{Variational Molecule Reconstruction.}\nTherefore we propose a \\textit{light} VAE-like generative SSL, equipped with a \\textit{crafty} surrogate loss, which we describe in the following. We start with an example for illustration. When generating 3D conformers from their corresponding 2D topology, we want to model the conditional likelihood $p(\\vy|\\vx)$. By introducing a reparameterized variable $\\vz_\\vx = \\mu_{\\vx} + \\sigma_{\\vx} \\odot \\epsilon$, where $\\mu_\\vx$ and $\\sigma_\\vx$ are two flexible functions on $h_\\vx$, $\\epsilon \\sim \\mathcal{N}(0,I)$ and $\\odot$ is the element-wise production,\nwe have the following lower bound:\n\\begin{equation} \\label{eq:variational_lower_bound}\n\\small\n\\begin{aligned}\n\\log p(\\vy|\\vx)\n\\ge \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\log p(\\vy|\\vz_\\vx) \\big] - KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)). \n\\end{aligned}\n\\end{equation}\nThe expression for $\\log p(\\vx|\\vy)$ can be similarly derived. \\Cref{eq:variational_lower_bound} includes a conditional log-likelihood and a KL-divergence term, where the bottleneck is to calculate the first term for structured data. This term has also been recognized as the \\textit{reconstruction term}: it is essentially to reconstruct the 3D conformers ($\\vy$) from the sampled 2D molecular graph representation ($\\vz_{\\vx}$). However, performing the graph reconstruction on the data space is not trivial: since molecules ({\\eg}, atoms and bonds) are discrete, modeling and measuring on the molecule space will bring extra obstacles.\n\n\\textbf{Variational Representation Reconstruction (VRR).}\nTo cope with this challenge, we propose a novel surrogate loss by switching the reconstruction from data space to representation space. Instead of decoding the latent code $z_{\\vx}$ to data space, we can directly project it to the 3D representation space, denoted as $q_\\vx(z_\\vx)$. Since the representation space is continuous, we may as well model the conditional log-likelihood with Gaussian distribution, resulting in L2 distance for reconstruction, {\\ie}, $\\|q_{\\vx}(z_{\\vx}) - \\text{SG}(h_\\vy(\\vy))\\|^2$. Here SG is the stop-gradient operation, assuming that $h_{\\vy}$ is a fixed learnt representation function. SG has been widely adopted in the SSL literature to avoid model collapse~\\citep{grill2020bootstrap,chen2021exploring}. We call this surrogate loss as variational representation reconstruction (VRR):\n\\begin{equation} \\label{eq:variational_lower_bound_approximation}\n\\small\n\\begin{aligned}\n\\mathcal{L}_{\\text{G}} = \\mathcal{L}_{\\text{VRR}}\n = & \\frac{1}{2} \\Big[ \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\| q_{\\vx}(\\vz_\\vx) - \\text{SG}(h_{\\vy}) \\|^2 \\big] + \\mathbb{E}_{q(\\vz_\\vy|\\vy)} \\big[ \\| q_{\\vy}(\\vz_\\vy) - \\text{SG}(h_{\\vx}) \\|_2^2 \\big] \\Big]\\\\\n& + \\frac{\\beta}{2} \\cdot \\Big[ KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)) + KL(q(\\vz_\\vy|\\vy) || p(\\vz_\\vy)) \\Big].\n\\end{aligned}\n\\end{equation}\nWe give a simplified illustration for the generative SSL pipeline in~\\Cref{fig:both_ssl} and the complete derivations in~\\Cref{sec:app:generative_ssl}. As will be discussed in~\\Cref{sec:mutual_information}, VRR is actually maximizing MI, and MI is invariant to continuous bijective function~\\citep{belghazi2018mutual}. Thus, this surrogate loss would be exact if the encoding function $h$ satisfies this condition. However, we find that GNN, though does not meet the condition, can provide quite robust performance, which empirically justify the effectiveness of VRR.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 3.4": {
                    "name": "Multi-task Objective Function",
                    "content": "\nAs discussed before, contrastive SSL and generative SSL essentially learn the representation from distinct viewpoints. A reasonable conjecture is that combining both SSL methods can lead to overall better performance, thus we arrive at minimizing the following complete objective for \\model:\n\\begin{equation} \\label{eq:graphmvp}\n\\small\n\\begin{aligned}\n\\mathcal{L}_{\\text{\\model}}\n& = \\alpha_1 \\cdot \\mathcal{L}_{\\text{C}} + \\alpha_2 \\cdot \\mathcal{L}_{\\text{G}},\\\\\n\\end{aligned}\n\\end{equation}\nwhere $\\alpha_1, \\alpha_2$ are weighting coefficients. A later performed ablation study (\\Cref{sec:experiment_each_loss_component}) delivers two important messages: (1) Both individual contrastive and generative SSL on 3D conformers can consistently help improve the 2D representation learning; (2) Combining the two SSL strategies can yield further improvements. Thus, we draw the conclusion that \\model\\, (\\Cref{eq:graphmvp}) is able to obtain an augmented 2D representation by fully utilizing the 3D information.\n\nAs discussed in~\\Cref{sec:intro}, existing graph SSL methods only focus on the 2D topology, which is in parallel to \\model: 2D graph SSL focuses on exploiting the 2D structure topology, and \\model\\, takes advantage of the 3D geometry information. Thus, we propose to merge the 2D SSL into GraphMVP. Since there are two main categories in 2D graph SSL: generative and contrastive, we propose two variants \\modelAM\\, and \\modelCP\\, accordingly. Their objectives are as follows:\n\\begin{equation} \\label{eq:graphmvp_variants}\n\\small\n\\begin{aligned}\n\\mathcal{L}_{\\text{\\modelAM}} = \\mathcal{L}_{\\text{\\model}} + \\alpha_3 \\cdot \\mathcal{L}_{\\text{Generative 2D-SSL}},~~~~~\n\\mathcal{L}_{\\text{\\modelCP}} = \\mathcal{L}_{\\text{\\model}} + \\alpha_3 \\cdot \\mathcal{L}_{\\text{Contrastive 2D-SSL}}.\n\\end{aligned}\n\\end{equation}\nLater, the empirical results also help support the effectiveness of \\modelAM\\, and \\modelCP, and thus, we can conclude that existing 2D SSL is complementary to \\model.\n"
                }
            },
            "section 4": {
                "name": "Experiments and Results",
                "content": " \\label{sec:experiments}\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                "subsection 4.1": {
                    "name": "Experimental Settings",
                    "content": " \\label{sec:experiments_setting}\n\\textbf{Datasets.} We pre-train models on the same dataset then fine-tune on the wide range of downstream tasks. We randomly select 50k qualified molecules from GEOM~\\citep{axelrod2020geom} with both 2D and 3D structures for the pre-training. As clarified in~\\Cref{sec:overview}, conformer ensembles can better reflect the molecular property, thus we take $C$ conformers of each molecule. For downstream tasks, we first stick to the same setting of the main graph SSL work~\\citep{hu2019strategies,You2020GraphCL,you2021graph}, exploring 8 binary molecular property prediction tasks, which are all in the low-data regime. Then we explore 6 regression tasks from various low-data domains to be more comprehensive. We describe all the datasets in~\\Cref{app:sec:dataset}. \n\n\\textbf{2D GNN.} We follow the research line of SSL on molecule graph~\\citep{hu2019strategies,you2021graph,You2020GraphCL}, using the same Graph Isomorphism Network (GIN)~\\citep{xu2018powerful} as the backbone model, with the same feature sets.\n\n\\textbf{3D GNN.} We choose SchNet~\\citep{schutt2017schnet} for geometric modeling, since SchNet: (1) is found to be a strong geometric representation learning method under the fair benchmarking; (2) can be trained more efficiently, comparing to the other recent 3D models. More detailed explanations are in~\\Cref{appendix:3d_gnn}.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 4.2": {
                    "name": "Main Results on Molecular Property Prediction.",
                    "content": " \\label{sec:main_results}\nWe carry out comprehensive comparisons with 10 SSL baselines and random initialization. For pre-training, we apply all SSL methods on the same dataset based on GEOM~\\citep{axelrod2020geom}. For fine-tuning, we follow the same setting~\\citep{hu2019strategies,You2020GraphCL,you2021graph} with 8 low-data molecular property prediction tasks.\n\n\\textbf{Baselines.} Due to the rapid growth of graph SSL~\\citep{liu2021graph,xie2021self,wu2021self}, we are only able to benchmark the most well-acknowledged baselines: EdgePred~\\citep{hamilton2017inductive}, InfoGraph~\\citep{sun2019infograph}, GPT-GNN\\citep{hu2020gpt}, AttrMask \\& ContextPred\\citep{hu2019strategies}, GraphLoG\\citep{xu2021self}, G-\\{Contextual, Motif\\}\\citep{rong2020self}, GraphCL\\citep{You2020GraphCL}, JOAO\\citep{you2021graph}.\n\n\\textbf{Our method.} \\model\\, has two key factors: i) masking ratio ($M$) and ii) number of conformers for each molecule ($C$). We set $M=0.15$ and $C=5$ by default, and will explore their effects in the following ablation studies in~\\Cref{sec:effect_of_M_and_C}. For EBM-NCE loss, we adopt the empirical distribution for noise distribution. For~\\Cref{eq:graphmvp_variants}, we pick the empirically optimal generative and contrastive 2D SSL method: that is AttrMask for \\modelAM\\, and ContextPred for \\modelCP.\n\n\n\nThe main results on 8 molecular property prediction tasks are listed in~\\Cref{tab:main_results}. We observe that the performance of \\model\\, is significantly better than the random initialized one, and the average performance outperforms the existing SSL methods by a large margin. In addition, \\modelAM\\, and \\modelCP\\, consistently improve the performance, supporting the claim: \\textbf{3D geometry is complementary to the 2D topology}. \\model\\, leverages the information between 3D geometry and 2D topology, and 2D SSL plays the role as regularizer to extract more 2D topological information; they are extracting different perspectives of information and are indeed complementary to each other.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Study: The Effect of Masking Ratio and Number of Conformers",
                    "content": " \\label{sec:effect_of_M_and_C}\nWe analyze the effects of masking ratio $M$ and the number of conformers $C$ in \\model. In~\\Cref{tab:main_results}, we set the $M$ as 0.15 since it has been widely used in existing SSL methods~\\citep{hu2019strategies,You2020GraphCL,you2021graph}, and $C$ is set to 5, which we will explain below. We explore on the range of $M \\in \\{0, 0.15, 0.3\\}$ and $C \\in \\{1, 5, 10, 20\\}$, and report the average performance. The complete results are in~\\Cref{app:sec:effect_of_M_and_C}.\n\n\n\nAs seen in~\\Cref{tab:ablation_masking}, the improvement is more obvious from $M=0$ (raw graph) to $M=0.15$ than from $M=0.15$ to $M=0.3$. This can be explained that subgraph masking with larger ratio will make the SSL tasks more challenging, especially comparing to the raw graph ($M=0$).\n\n\\Cref{tab:ablation_n_conformers} shows the effect for $C$. We observe that the performance is generally better when adding more conformers, but will reach a plateau above certain thresholds. This observation matches with previous findings~\\citep{axelrod2020molecular}: adding more conformers to augment the representation learning is not as helpful as expected; while we conclude that adding more conformers can be beneficial with little improvement. One possible reason is, when generating the dataset, we are sampling top-$C$ conformers with highest possibility and lowest energy. In other words, top-5 conformers are sufficient to cover the most conformers with equilibrium state (over 80\\%), and the effect of larger $C$ is thus modest.\n% https://arxiv.org/pdf/2107.12375.pdf: no marked difference was observed between using a single or multiple molecular conformers for network training [87] (or \\citep{axelrod2020molecular})\n\nTo sum up, adding more conformers might be helpful, but the computation cost can grow linearly with the increase in dataset size. On the other hand, enlarging the masking ratio will not induce extra cost, yet the performance is slightly better. Therefore, we would encourage tuning masking ratios prior to trying a larger number of conformers from the perspective of efficiency and effectiveness.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study: The Effect of Objective Function",
                    "content": " \\label{sec:experiment_each_loss_component}\n\nIn~\\Cref{sec:method}, we introduce a new contrastive learning objective family called EBM-NCE, and we take either InfoNCE and EBM-NCE as the contrastive SSL. For the generative SSL task, we propose a novel objective function called variational representation reconstruction (VRR) in~\\Cref{eq:variational_lower_bound_approximation}. As discussed in~\\Cref{sec:generative_SSL}, stochasticity is important for \\model\\, since it can capture the conformer distribution for each 2D molecular graph. To verify this, we add an ablation study on \\textit{representation reconstruction (RR)} by removing stochasticity in VRR. Thus, here we deploy a comprehensive ablation study to explore the effect for each individual objective function (InfoNCE, EBM-NCE, VRR and RR), followed by the pairwise combinations between them.\n\nThe results in \\Cref{tab:ablation_objective_function} give certain constructive insights as follows:\n(1) Each individual SSL objective function (middle block) can lead to better performance. This strengthens the claim that adding 3D information is helpful for 2D representation learning.\n(2) According to the combination of those SSL objective functions (bottom block), adding both contrastive and generative SSL can consistently improve the performance. This verifies our claim that conducting SSL at both the inter-data and intra-data level is beneficial.\n(3) We can see VRR is consistently better than RR on all settings, which verifies that stochasticity is an important factor in modeling 3D conformers for molecules.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 4.5": {
                    "name": "Broader Range of Downstream Tasks",
                    "content": "\nThe 8 binary downstream tasks discussed so far have been widely applied in the graph SSL research line on molecules~\\cite{hu2019strategies,You2020GraphCL,you2021graph}, but there are more tasks where the 3D conformers can be helpful. Here we test 4 extra regression property prediction tasks and 2 drug-target affinity tasks.\n\nAbout the dataset statistics, more detailed information can be found in~\\Cref{app:sec:dataset}, and we may as well briefly describe the affinity task here. Drug-target affinity (DTA) is a crucial task~\\citep{pahikkala2015toward,wen2017deep,ozturk2018deepdta} in drug discovery, where it models both the molecular drugs and target proteins, with the goal to predict their affinity scores. One recent work~\\citep{nguyen2019graphdta} is modeling the molecular drugs with 2D GNN and target protein (as an amino-acid sequence) with convolution neural network (CNN). We adopt this setting by pre-training the 2D GNN using \\model.\nAs illustrated in~\\Cref{tab:main_regression}, the consistent performance gain verifies the effectiveness of our proposed \\model.\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 4.6": {
                    "name": "Case Study",
                    "content": "\nWe investigate how \\model\\, helps when the task objectives are challenging with respect to the 2D topology but straightforward using 3D geometry (as shown in~\\Cref{fig:case}). We therefore design two case studies to testify how \\model\\, transfers knowledge from 3D geometry into the 2D representation.\n\nThe first case study is \\textit{3D Diameter Prediction}. For molecules, usually, the longer the 2D diameter is, the larger the 3D diameter (largest atomic pairwise l2 distance). However, this does not always hold, and we are interested in using the 2D graph to predict the 3D diameter. The second case study is \\textit{Long-Range Donor-Acceptor Detection}. Molecules possess a special geometric structure called donor-acceptor bond, and we want to use 2D molecular graph to detect this special structure. We validate that \\model\\, consistently brings improvements on these 2 case studies, and provide more detailed discussions and interpretations in~\\Cref{appendix:case}.\n\n\n"
                }
            },
            "section 5": {
                "name": "Theoretical Insights",
                "content": " \\label{sec:theoretical_insight}\nIn this section, we provide the mathematical insights behind GraphMVP. We will first discuss both contrastive and generative SSL methods (\\Cref{sec:contrastive_SSL,sec:generative_SSL}) are maximizing the mutual information (MI) and then how the 3D geometry, as privileged information, can help 2D representation learning.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\vspace{-1ex}\n",
                "subsection 5.1": {
                    "name": "Maximizing Mutual Information",
                    "content": " \\label{sec:mutual_information}\n\\vspace{-1ex}\nMutual information (MI) measures the non-linear dependence~\\citep{belghazi2018mutual} between  two random variables: the larger MI, the stronger dependence between the variables. Therefore for \\model, we can interpret it as maximizing MI between 2D and 3D views: to obtain a more robust 2D/3D representation by sharing more information with its 3D/2D counterparts. This is also consistent with the sample complexity theory~\\citep{erhan2010does,arora2019theoretical,garg2020functional} where SSL as functional regularizer can reduce the uncertainty in representation learning. We first derive a lower bound for MI (see derivations in~\\Cref{sec:app:MI}), and the corresponding objective function $\\mathcal{L}_{\\text{MI}}$ is\n\\begin{equation} \\label{eq:MI_objective}\n\\small\nI(X;Y) \\ge \\mathcal{L}_{\\text{MI}} = \\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\big[ \\log p(\\vy|\\vx) + \\log p(\\vx|\\vy) \\big].\n\\end{equation}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\textbf{Contrastive Self-Supervised Learning.}\nInfoNCE was initialized proposed to maximize the MI directly~\\citep{oord2018representation}.\nHere in \\model, EBM-NCE estimates the conditional likelihood in~\\Cref{eq:MI_objective} using EBM, and solves it with NCE~\\cite{gutmann2010noise}. As a result, EBM-NCE can also be seen as maximizing MI between 2D and 3D views. The detailed derivations can be found in~\\Cref{app:ebm_nce}.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\textbf{Generative Self-Supervised Learning.}\nOne alternative solution is to use a variational lower bound to approximate the conditional log-likelihood terms in~\\Cref{eq:MI_objective}. Then we can follow the same pipeline in~\\Cref{sec:generative_SSL}, ending up with the surrogate objective, {\\ie}, VRR in~\\Cref{eq:variational_lower_bound_approximation}.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 5.2": {
                    "name": "3D Geometry as Privileged Information",
                    "content": " \\label{sec:privileged_info}\nWe show the theoretical insights from privileged information that motivate GraphMVP. We start by considering a supervised learning setting where $(\\bm{u}_i,\\bm{l}_i)$ is a feature-label pair and $\\bm{u}_i^*$ is the privileged information~\\cite{vapnik2009new,vapnik2015learning}. The privileged information is defined to be additional information about the input $(\\bm{u}_i,\\bm{l}_i)$ in order to support the prediction. For example, $\\bm{u}_i$ could be some CT images of a particular disease, $\\bm{l}_i$ could be the label of the disease and $\\bm{u}_i^*$ is the medical report from a doctor. VC theory~\\cite{vapnik2013nature,vapnik2015learning} characterizes the learning speed of an algorithm from the capacity of the algorithm and the amount of training data. Considering a binary classifier $f$ from a function class $\\mathcal{F}$ with finite VC-dimension $\\text{VCD}(\\mathcal{F})$. With probability $1-\\delta$, the expected error is upper bounded by \n\\begin{equation}\n\\small\n    R(f)\\leq R_n(f) +\\mathcal{O}\\bigg( \\big( \\frac{\\text{VCD}(\\mathcal{F})-\\log\\delta}{n} \\big)^\\beta \\bigg)\n\\end{equation}\nwhere $R_n(f)$ denotes the training error and $n$ is the number of training samples. When the training data is separable, then $R_n(f)$ will diminish to zero and $\\beta$ is equal to $1$. When the training data is non-separable, $\\beta$ is $\\frac{1}{2}$. Therefore, the rate of convergence for the separable case is of order $1/n$. In contrast, the rate for the non-separable case is of order $1/\\sqrt{n}$. We note that such a difference is huge, since the same order of bounds require up to 100 training samples versus 10,000 samples. Privileged information makes the training data separable such that the learning can be more efficient. Connecting the results to GraphMVP, we notice that the 3D geometric information of molecules can be viewed as a form of privileged information, since 3D information can effectively make molecules more separable for some properties~\\cite{schutt2017schnet,liu2018n,liu2021spherical}. Besides, privileged information is only used in training, and it well matches our usage of 3D geometry for pre-training. In fact, using 3D structures as privileged information has been already shown quite useful in protein classification~\\cite{vapnik2009new}, which serves as a strong evidence to justify the effectiveness of 3D information in graph SSL pre-training.\n\\vspace{-1ex}\n"
                }
            },
            "section 6": {
                "name": "Conclusion and Future Work",
                "content": "\n\\vspace{-1ex}\nIn this work, we provide a very general framework, coined \\model. From the domain perspective, \\model\\, (1) is the first to incorporate 3D information for augmenting 2D graph representation learning and (2) is able to take advantages of 3D conformers by considering stochasticity in modeling. From the aspect of technical novelties, \\model\\, brings following insights when introducing 2 SSL tasks:\n(1) Following~\\Cref{eq:MI_objective}, \\model\\, proposes EBM-NCE and VRR, where they are modeling the conditional distributions using EBM and variational distribution respectively.\n(2) EBM-NCE is similar to JSE, while we start with a different direction for theoretical intuition, yet EBM opens another promising venue in this area.\n(3) VRR, as a generative SSL method, is able to alleviate the potential issues in molecule generation~\\citep{zhavoronkov2019deep,gao2020synthesizability}.\n(4) Ultimately, \\model\\, combines both contrastive SSL (InfoNCE or EBM-NCE) and generative SSL (VRR) for objective function.\nBoth empirical results (solid performance improvements on 14 downstream datasets) and theoretical analysis can strongly support the above domain and technical contributions.\n\nWe want to emphasize that \\model\\, is model-agnostic and has the potential to be expanded to many other low-data applications. This motivates broad directions for future exploration, including but not limited to: (1) More powerful 2D and 3D molecule representation methods. (2) Different application domain other than small molecules, {\\eg}, large molecules like proteins.\n\n\\clearpage\n"
            },
            "section 7": {
                "name": "Reproducibility Statement",
                "content": "\nTo ensure the reproducibility of the empirical results, we include our code base in the supplementary material, which contains: instructions for installing conda virtual environment, data preprocessing scripts, and training scripts. Our code is available on \\href{https://github.com/chao1224/GraphMVP}{GitHub} for reproducibility. Complete derivations of equations and clear explanations are given in~\\Cref{sec:app:MI,sec:app:contrastive_ssl,sec:app:generative_ssl}.\n\n"
            },
            "section 8": {
                "name": "Acknowledgement",
                "content": "\nThis project is supported by the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R\\&D Project (AI4D-CORE-06). This project was also partially funded by IVADO Fundamental Research Project grant PRF-2019-3583139727.\n\n\\bibliographystyle{plain}\n{\\small\n\\bibliography{iclr2022_conference}\n}\n\n\\newpage\n\\appendix\n\n% \\hypersetup{linkcolor=black}\n% \\setlength{\\cftbeforesecskip}{6pt}\n% \\setcounter{tocdepth}{2} % Show sections\n% \\tableofcontents\n% \\hypersetup{linkcolor=red}\n\n%{\n%  \\hypersetup{hidelinks}\n%  \\tableofcontents\n%   \\noindent\\hrulefill\n%}\n%\\addtocontents{toc}{\\protect\\setcounter{tocdepth}{2}} \n\n\\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC\n\\renewcommand \\thepart{} % make \"Part\" text invisible\n\\renewcommand \\partname{}\n\\part{\\Large\\centering{Appendix}}\n % Start the appendix part\n\\parttoc % Insert the appendix TOC\n\n\\clearpage\n%%%%%%%%%% Related Work %%%%%%%%%%\n"
            },
            "section 9": {
                "name": "Self-Supervised Learning on Molecular Graph",
                "content": " \\label{appendix:related_work}\nSelf-supervised learning (SSL) methods have attracted massive attention recently, trending from vision~\\citep{chen2020simple,he2020momentum,caron2020unsupervised,chen2021exploring,OcCo}, language~\\citep{oord2018representation,devlin2018bert,brown2020language} to graph~\\citep{velivckovic2018deep,sun2019infograph,liu2018n,hu2019strategies,You2020GraphCL,you2021graph}. In general, there are two categories of SSL: contrastive and generative, where they differ on the design of the supervised signals. Contrastive SSL realizes the supervised signals at the \\textbf{inter-data} level, learning the representation by contrasting with other data points; while generative SSL focuses on reconstructing the original data at the \\textbf{intra-data} level. Both venues have been widely explored~\\citep{liu2021graph,xie2021self,wu2021self,liu2021self}.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                "subsection 9.1": {
                    "name": "Contrastive graph SSL",
                    "content": "\nContrastive graph SSL first applies transformations to construct different \\textit{views} for each graph. Each view incorporates different granularities of information, like node-, subgraph-, and graph-level. It then solves two sub-tasks simultaneously: (1) aligning the representations of views from the same data; (2) contrasting the representations of views from different data, leading to a uniformly distributed latent space~\\citep{wang2020understanding}. The key difference among existing methods is thus the design of view constructions. InfoGraph~\\citep{velivckovic2018deep,sun2019infograph} contrasted the node (local) and graph (global) views. ContextPred~\\citep{hu2019strategies} and G-Contextual~\\cite{rong2020self} contrasted between node and context views. GraphCL and JOAO~\\citep{You2020GraphCL,you2021graph} made comprehensive comparisons among four graph-level transformations and further learned to select the most effective combinations.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 9.2": {
                    "name": "Generative graph SSL",
                    "content": "\nGenerative graph SSL aims at reconstructing important structures for each graph. By so doing, it consequently learns a representation capable of encoding key ingredients of the data. EdgePred~\\citep{hamilton2017inductive} and AttrMask~\\citep{hu2019strategies} predicted the adjacency matrix and masked tokens (nodes and edges) respectively. GPT-GNN~\\citep{hu2020gpt} reconstructed the whole graph in an auto-regressive approach.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 9.3": {
                    "name": "Predictive graph SSL",
                    "content": "\nThere are certain SSL methods specific to the molecular graph. For example, one central task in drug discovery is to find the important substructure or motif in molecules that can activate the target interactions. G-Motif~\\citep{rong2020self} adopts domain knowledge to heuristically extract motifs for each molecule, and the SSL task is to make prediction on the existence of each motif. Different from contrastive and generative SSL, recent literature~\\citep{wu2021self} takes this as predictive graph SSL, where the supervised signals are self-generated labels.\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\textbf{SSL for Molecular Graphs.} Recall that all previous methods in~\\Cref{app:tab:compare} \\textbf{merely} focus on the 2D topology. However, for science-centric tasks such as molecular property prediction, 3D geometry should be incorporated as it provides complementary and comprehensive information~\\citep{schutt2017schnet,liu2021spherical}. To mitigate this gap, we propose GraphMVP to leverage the 3D geometry with unsupervised graph pre-training.\n\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                }
            },
            "section 10": {
                "name": "Molecular Graph Representation",
                "content": " \\label{app:sec:molecule_representation}\nThere are two main methods for molecular graph representation learning. The first one is the molecular fingerprints. It is a hashed bit vector to describe the molecular graph. There has been re-discoveries on fingerprints-based methods~\\citep{ramsundar2015massively,liu2018practical,liu2019loss,meyer2019learning,alnammi2021evaluating,jiang2021could}, while its has one main drawback: Random forest and XGBoost are very strong learning models on fingerprints, but they fail to take benefits of the pre-training strategy.\n\nGraph neural network (GNN) has become another mainstream modeling methods for molecular graph representation. Existing methods can be generally split into two venues: 2D GNN and 3D GNN, depending on what levels of information is considered. 2D GNN focuses on the topological structures of the graph, like the adjacency among nodes, while 3D GNN is able to model the ``energy'' of molecules by taking account the spatial positions of atoms.\n\nFirst, we want to highlight that \\model\\, is model-agnostic, {\\ie}, it can be applied to any 2D and 3D GNN representation function, yet the specific 2D and 3D representations are not the main focus of this work.\nSecond, we acknowledge there are a lot of advanced 3D~\\citep{fuchs2020se,satorras2021n,liu2021spherical,jumper2021highly} and 2D~\\citep{gilmer2017neural,yang2019analyzing,liu2018n,xu2018powerful,corso2020principal,demirel2021analysis} representation methods.\nHowever, considering the \\textit{graph SSL literature} and \\textit{graph representation liteature} (illustrated below), we adopt GIN~\\citep{xu2018powerful} and SchNet~\\citep{schutt2017schnet} in current \\model.\n\n",
                "subsection 10.1": {
                    "name": "2D Molecular Graph Neural Network",
                    "content": "\nThe 2D representation is taking each molecule as a 2D graph, with atoms as nodes and bonds as edges, {\\ie}, $g_{\\text{2D}} = (X, E)$. $X\\in\\mathbb{R}^{n\\times d_n}$ is the atom attribute matrix, where $n$ is the number of atoms (nodes) and $d_n$ is the atom attribute dimension. $E\\in\\mathbb{R}^{m\\times d_e}$ is the bond attribute matrix, where $m$ is the number of bonds (edges) and $d_m$ is the bond attribute dimension. Notice that here $E$ also includes the connectivity. Then we will apply a transformation function $T_{\\text{2D}}$ on the topological graph. Given a 2D graph $g_{\\text{2D}}$, its 2D molecular representation is:\n\\begin{equation}\nh_{\\text{2D}} = \\text{GNN-2D}(T_{\\text{2D}}(g_{\\text{2D}})) = \\text{GNN-2D}(T_{\\text{2D}}(X, E)).\n\\end{equation}\nThe core operation of 2D GNN is the message passing function~\\citep{gilmer2017neural}, which updates the node representation based on adjacency information. We have variants depending on the design of message and aggregation functions, and we pick GIN~\\citep{xu2018powerful} in this work.\n\n\\paragraph{GIN} There has been a long research line on 2D graph representation learning~\\citep{gilmer2017neural,yang2019analyzing,liu2018n,xu2018powerful,corso2020principal,demirel2021analysis}. Among these, graph isomorphism network (GIN) model~\\citep{xu2018powerful} has been widely used as the backbone model in recent graph self-supervised learning work~\\citep{hu2019strategies,You2020GraphCL,you2021graph}. Thus, we as well adopt GIN as the base model for 2D representation.\n\nRecall each molecule is represented as a molecular graph, {\\ie}, $g_{\\text{2D}} = (X, E)$, where $X$ and $E$ are feature matrices for atoms and bonds respectively. Then the message passing function is defined as:\n\\begin{equation}\nz_i^{(k+1)} = \\text{MLP}_{\\text{atom}}^{(k+1)} \\Big(z_i^{(k)} + \\sum_{j \\in \\mathcal{N}(i)} \\big( z_j^{(k)} + \\text{MLP}_{\\text{bond}}^{(k+1)}(E_{ij}) \\big) \\Big),\n\\end{equation}\nwhere $z_0=X$ and $\\text{MLP}_{\\text{atom}}^{(k+1)}$ and $\\text{MLP}_{\\text{bond}}^{(k+1)}$ are the $(l+1)$-th MLP layers on the atom- and bond-level respectively. Repeating this for $K$ times, and we can encode $K$-hop neighborhood information for each center atom in the molecular data, and we take the last layer for each node/atom representation. The graph-level molecular representation is the mean of the node representation:\n\\begin{equation}\nz(\\vx) = \\frac{1}{N} \\sum_{i} z_i^{(K)}\n\\end{equation}\n\n\n%%%%%%%%%% 3D GNN %%%%%%%%%%\n"
                },
                "subsection 10.2": {
                    "name": "3D Molecular Graph Neural Network",
                    "content": " \\label{appendix:3d_gnn}\nRecently, the 3D geometric representation learning has brought breakthrough progress in molecule modeling~\\citep{schutt2017schnet,fuchs2020se,satorras2021n,liu2021spherical,jumper2021highly}. 3D molecular graph additionally includes spatial locations of the atoms, which needless to be static since, in real scenarios, atoms are in continual motion on \\textit{a potential energy surface}~\\citep{axelrod2020geom}. The 3D structures at the local minima on this surface are named \\textit{molecular conformation} or \\textit{conformer}. As the molecular properties are a function of the conformer ensembles~\\citep{hawkins2017conformation}, this reveals another limitation of existing mainstream methods: to predict properties from a single 2D or 3D graph cannot account for this fact~\\citep{axelrod2020geom}, while our proposed method can alleviate this issue to a certain extent.\n\nFor specific 3D molecular graph, it additionally includes spatial positions of the atoms. We represent each conformer as $g_{\\text{3D}} = (X, R)$, where $R \\in \\mathbb{R}^{n \\times 3}$ is the 3D-coordinate matrix, and the corresponding representation is:\n\\begin{equation}\nh_{\\text{3D}} = \\text{GNN-3D}(T_{\\text{3D}}(g_{\\text{3D}})) = \\text{GNN-3D}(T_{\\text{3D}}(X, R)),\n\\end{equation}\nwhere $R$ is the 3D-coordinate matrix and $T_{\\text{3D}}$ is the 3D transformation. Note that further information such as plane and torsion angles can be solved from the positions.\n\n\\paragraph{SchNet}\nSchNet~\\citep{schutt2017schnet} is composed of the following key steps:\n\\begin{equation}\n\\begin{aligned}\n& z_i^{(0)} = \\text{embedding} (x_i)\\\\\n& z_i^{(t+1)} = \\text{MLP} \\Big( \\sum_{j=1}^{n} f(x_j^{(t-1)}, r_i, r_j) \\Big)\\\\\n& h_i = \\text{MLP} (z_i^{(K)}),\n\\end{aligned}\n\\end{equation}\nwhere $K$ is the number of hidden layers, and\n\\begin{equation}\nf(x_j, r_i, r_j) = x_j \\cdot e_k(r_i - r_j) = x_j \\cdot \\exp(- \\gamma \\| \\|r_i - r_j\\|_2  - \\mu \\|_2^2)\n\\end{equation}\nis the continuous-filter convolution layer, enabling the modeling of continuous positions of atoms.\n\nWe adopt SchNet for the following reasons. (1) SchNet is a very strong geometric representation method after \\textit{fair} benchmarking. (2) SchNet can be trained more efficiently, comparing to the other recent 3D models. To support these two points, we make a comparison among the most recent 3D geometric models~\\cite{fuchs2020se,satorras2021n,liu2021spherical} on QM9 dataset. QM9~\\citep{wu2018moleculenet} is a molecule dataset approximating 12 thermodynamic properties calculated by density functional theory (DFT) algorithm. Notice: UNiTE~\\citep{qiao2021unite} is the state-of-the-art 3D GNN, but it requires a commercial software for feature extraction, thus we exclude it for now.\n\n\n\n\\Cref{tab:app:qm9} shows that, under a fair comparison (w.r.t. data splitting, seed, cuda version, etc), SchNet can reach pretty comparable performance, yet the efficiency of SchNet is much better. Combining these two points, we adopt SchNet in current version of \\model.\n\n"
                },
                "subsection 10.3": {
                    "name": "Summary",
                    "content": "\nTo sum up, in \\model, the most important message we want to deliver is how to design a well-motivated SSL algorithm to extract useful 3D geometry information to augment the 2D representation for downstream fine-tuning. \\model\\, is model-agnostic, and we may as well leave the more advanced 3D~\\citep{fuchs2020se,satorras2021n,liu2021spherical,jumper2021highly} and 2D~\\citep{yang2019analyzing,liu2018n,corso2020principal} GNN for future exploration.\n\nIn addition, molecular property prediction tasks have rich alternative representation methods, including SMILES~\\cite{weininger1988smiles,hirohara2018convolutional}, and biological knowledge graph~\\cite{wang2021multi,liu2022structured}. There have been another SSL research line on them~\\cite{liustructured,zhu2021dual,fang2021molecular}, yet they are beyond the scope of discussion in this paper.\n\n\\clearpage\n%%%%%%%%%% MI and SSL %%%%%%%%%%\n"
                }
            },
            "section 11": {
                "name": "Maximize Mutual Information",
                "content": " \\label{sec:app:MI}\nIn what follows, we will use $X$ and $Y$ to denote the data space for $2D$ graph and $3D$ graph respectively. Then the latent representations are denoted as $h_\\vx$ and $h_\\vy$. \n\n",
                "subsection 11.1": {
                    "name": "Formulation",
                    "content": "\nThe standard formulation for mutual information (MI) is\n\\begin{equation} \\label{eq:app:MI}\n\\begin{aligned}\nI(X;Y)\n& = \\mathbb{E}_{p(\\vx,\\vy)} \\big[ \\log \\frac{p(\\vx,\\vy)}{p(\\vx) p(\\vy)} \\big].\n\\end{aligned}\n\\end{equation}\n\nAnother well-explained MI inspired from wikipedia is given in~\\Cref{fig:app:MI}.\n\n\nMutual information (MI) between random variables measures the corresponding non-linear dependence. As can be seen in the first equation in~\\Cref{eq:app:MI}, the larger the divergence between the joint ($p(\\vx, \\vy$) and the product of the marginals $p(\\vx) p(\\vy)$, the stronger the dependence between $X$ and $Y$.\n\nThus, following this logic, maximizing MI between 2D and 3D views can force the 3D/2D representation to capture higher-level factors, {\\eg}, the occurrence of important substructure that is semantically vital for downstream tasks. Or equivalently, maximizing MI can decrease the uncertainty in 2D representation given 3D geometric information.\n\n"
                },
                "subsection 11.2": {
                    "name": "A Lower Bound to MI",
                    "content": "\nTo solve MI, we first extract a lower bound:\n\\begin{equation}\n\\begin{aligned}\nI(X;Y)\n& = \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log \\frac{p(\\vx,\\vy)}{p(\\vx) p(\\vy)} \\Big]\\\\\n& \\ge \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log \\frac{p(\\vx,\\vy)}{\\sqrt{p(\\vx) p(\\vy)}} \\Big]\\\\\n& = \\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log \\frac{(p(\\vx,\\vy))^2}{p(\\vx) p(\\vy)} \\Big]\\\\\n& = \\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log p(\\vx|\\vy) \\Big] + \\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log p(\\vy|\\vx) \\Big]\\\\\n& = -\\frac{1}{2} [H(Y|X) + H(X|Y)].\n\\end{aligned}\n\\end{equation}\n\nThus, we transform the MI maximization problem into minimizing the following objective:\n\\begin{equation} \\label{eq:app:MI_objective}\n\\begin{aligned}\n\\mathcal{L}_{\\text{MI}} & = \\frac{1}{2} [H(Y|X) + H(X|Y)].\n\\end{aligned}\n\\end{equation}\n\nIn the following sections, we will describe two self-supervised learning methods for solving MI. Notice that the methods are very general, and can be applied to various applications. Here we apply it mainly for making 3D geometry useful for 2D representation learning on molecules.\n\n\\clearpage\n"
                }
            },
            "section 12": {
                "name": "Contrastive Self-Supervised Learning",
                "content": " \\label{sec:app:contrastive_ssl}\nThe essence of contrastive self-supervised learning is to align positive view pairs and contrast negative view pairs, such that the obtained representation space is well distributed~\\citep{wang2020understanding}. We display the pipeline in~\\Cref{fig:app:contrastive_ssl}. Along the research line in graph SSL~\\citep{liu2021graph,xie2021self,wu2021self,liu2021self}, InfoNCE and EBM-NCE are the two most-widely used, as discussed below.\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                "subsection 12.1": {
                    "name": "InfoNCE",
                    "content": " \\label{sec:app:infonce}\nInfoNCE~\\citep{oord2018representation} is first proposed to approximate MI~\\Cref{eq:app:MI}:\n\\begin{equation}\\label{eq:app:InfoNCE}\n\\footnotesize{\n\\begin{aligned}\n\\mathcal{L}_{\\text{InfoNCE}} =\n    -\\frac{1}{2} \\mathbb{E} &\\left[\n      \\log \\frac{\\exp(f_{\\vx}(\\vx, \\vy))}{\\exp(f_{\\vx}(\\vx, \\vy)) + \\sum_j \\exp(f_{\\vx}(\\vx^{j},\\vy)})  + \\log \\frac{\\exp(f_{\\vy}(\\vy,\\vx))}{\\exp(f_{\\vy}(\\vy,\\vx)) + \\sum_j \\exp{f_{\\vy}(\\vy^{j},\\vx)}} \\right],\n\\end{aligned}\n}\n\\end{equation} \nwhere $\\vx^{j}, \\vy^{j}$ are randomly sampled 2D and 3D views regarding to the anchored pair $(\\vx,\\vy)$. $f_{\\vx}(\\vx,\\vy), f_{\\vy}(\\vy,\\vx)$ are scoring functions for the two corresponding views, whose formulation can be quite flexible. Here we use $f_\\vx(\\vx,\\vy) = f_\\vy(\\vy,\\vx) = \\exp(\\langle h_\\vx, h_\\vy \\rangle)$. \n\n\\paragraph{Derivation of InfoNCE}\n\\begin{equation}\n\\begin{aligned}\nI(X;Y) - \\log (K)\n& = \\mathbb{E}_{p(\\vx, \\vy)} \\big[\\log \\frac{1}{K} \\frac{p(\\vx, \\vy)}{p(\\vx) p(\\vy)} \\big]\\\\\n& = \\sum_{\\vx^{i},\\vy^{i}} \\big[\\log \\frac{1}{K} \\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})} \\big]\\\\\n& \\ge -\\sum_{\\vx^{i},\\vy^{i}} \\big[\\log \\big( 1 + (K-1) \\frac{p(\\vx^{i}) p(\\vy^{i})}{p(\\vx^{i},\\vy^{i})} \\big)\\big]\\\\\n& = -\\sum_{\\vx^{i},\\vy^{i}} \\big[\\log \\frac{\\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})} + (K-1)}{\\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})}} \\big]\\\\\n& \\approx -\\sum_{\\vx^{i},\\vy^{i}} \\big[ \\log \\frac{\\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})} + (K-1)\\mathbb{E}_{\\vx^{j} \\ne \\vx^{i}}\\frac{p(x^{j},\\vy^{i})}{p(\\vx^{j}) p(\\vy^{i})} }{\\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})}} \\big] \\quad \\text{// \\textcircled{1}}\\\\\n& = \\sum_{\\vx^{i},\\vy^{i}} \\big[ \\log \\frac{\\exp(f_\\vx(\\vx^{i},\\vy^{i}))}{\\exp(f_\\vx(\\vx^{i},\\vy^{i})) + \\sum_{j=1}^K f_\\vx(\\vx^{j},\\vy^{i})} \\big],\n\\end{aligned}\n\\end{equation}\nwhere we set $ f_\\vx(\\vx^{i},\\vy^{i}) = \\log \\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i})p(\\vy^{i})}$.\n\nNotice that in \\textcircled{1}, we are using data $x \\in X$ as the anchor points. If we use the $y \\in Y$ as the anchor points and follow the similar steps, we can obtain\n\\begin{equation}\n\\begin{aligned}\nI(X;Y) - \\log(K) \\ge \\sum_{\\vy^{i},\\vx^{i}} \\big[ \\log \\frac{\\exp(f_\\vy(\\vy^{i},\\vx^{i}))}{\\exp f_\\vy(\\vy^{i},\\vx^{i}) + \\sum_{j=1}^K \\exp (f_\\vy(\\vy^{j},\\vx^{i}))} \\big].\n\\end{aligned}\n\\end{equation}\n\nThus, by add both together, we can have the objective function as~\\Cref{eq:app:InfoNCE}.\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 12.2": {
                    "name": "EBM-NCE",
                    "content": " \\label{app:ebm_nce}\nWe here provide an alternative approach to maximizing MI using energy-based model (EBM). To our best knowledge, we are the \\textbf{first} to give the rigorous proof of using EBM to maximize the MI.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                    "subsubsection 12.2.1": {
                        "name": "Energy-Based Model (EBM)",
                        "content": "\nEnergy-based model (EBM) is a powerful tool for modeling the data distribution. The classic formulation is:\n\\begin{equation} \\label{eq:app:EBM_original}\np(\\vx) = \\frac{\\exp(-E(\\vx))}{A},\n\\end{equation}\nwhere the bottleneck is the intractable partition function $A = \\int_\\vx \\exp(-E(\\vx)) d\\vx$. Recently, there have been quite a lot progress along this direction~\\citep{gutmann2010noise,du2020improved,song2020score,song2021train}. Noise Contrastive Estimation (NCE)~\\citep{gutmann2010noise} is one of the powerful tools here, as we will introduce later.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                    },
                    "subsubsection 12.2.2": {
                        "name": "EBM for MI",
                        "content": "\nRecall that our objective function is~\\Cref{eq:app:MI_objective}: $\\mathcal{L}_{\\text{MI}} = \\frac{1}{2} [H(Y|X) + H(X|Y)]$. Then we model the conditional likelihood with energy-based model (EBM). This gives us\n\\begin{equation} \\label{eq:app:EBM}\n\\begin{aligned}\n\\mathcal{L}_{\\text{EBM}} = -\\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log \\frac{\\exp(f_\\vx(\\vx, \\vy))}{A_{\\vx|\\vy}} + \\log \\frac{\\exp(f_\\vy(\\vy, \\vx))}{A_{\\vy|\\vx}} \\Big],\n\\end{aligned}\n\\end{equation}\nwhere $f_\\vx(\\vx, \\vy) = -E(\\vx|\\vy)$ and $f_\\vy(\\vy, \\vx) = -E(\\vy|\\vx)$ are the negative energy functions, and $A_{\\vx|\\vy}$ and $A_{\\vy|\\vx}$ are the corresponding partition functions.\n\nUnder the EBM framework, if we solve~\\Cref{eq:app:EBM} with Noise Contrastive Estimation (NCE)~\\citep{gutmann2010noise}, the final EBM-NCE objective is\n\\begin{equation} \\label{eq:app:EBM_NCE}\n\\begin{aligned}\n\\mathcal{L}_{\\text{EBM-NCE}}\n= & -\\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(y)} \\Big[ \\mathbb{E}_{p_n(\\vx|\\vy)} [\\log \\big(1-\\sigma(f_\\vx(\\vx, \\vy))\\big)] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy)} [\\log \\sigma(f_\\vx(\\vx, \\vy))] \\Big] \\\\\n& ~~ - \\frac{1}{2}  \\mathbb{E}_{p_{\\text{data}}(x)} \\Big[ \\mathbb{E}_{p_n(\\vy|\\vx)} [\\log \\big(1-\\sigma(f_\\vy(\\vy, \\vx))\\big)] + \\mathbb{E}_{p_{\\text{data}}(\\vy|\\vx)} [\\log \\sigma(f_\\vy(\\vy, \\vx))] \\Big].\n\\end{aligned}\n\\end{equation}\nNext we will give the detailed derivations.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                    },
                    "subsubsection 12.2.3": {
                        "name": "Derivation of conditional EBM with NCE",
                        "content": "\nWLOG, let's consider the $p_\\theta(\\vx|\\vy)$ first, and by EBM it is as follows:\n\\begin{equation} \\label{eq:app:EBM_SSL}\np_\\theta(\\vx|\\vy) = \\frac{\\exp(-E(\\vx|\\vy))}{ \\int \\exp(-E({\\tilde \\vx}|\\vy)) d{\\tilde \\vx}} = \\frac{\\exp(f_\\vx(\\vx, \\vy))}{\\int \\exp(f_\\vx({\\tilde \\vx}|\\vy)) d{\\tilde \\vx}} = \\frac{\\exp(f_\\vx(\\vx, \\vy))}{A_{\\vx|\\vy}}.\n\\end{equation}\n\nThen we solve this using NCE. NCE handles the intractability issue by transforming it as a binary classification task. We take the partition function $A_{\\vx|\\vy}$ as a parameter, and introduce a noise distribution $p_n$. Based on this, we introduce a mixture model: $\\vz=0$ if the conditional $\\vx|\\vy$ is from $p_n(\\vx|\\vy)$, and $\\vz=1$ if $\\vx|\\vy$ is from $p_{\\text{data}}(\\vx|\\vy)$. So the joint distribution is:\n\\begin{equation*}\n    p_{n,\\text{\\text{data}}}(\\vx|\\vy) = p(\\vz=1) p_{\\text{data}}(\\vx|\\vy) + p(\\vz=0) p_n(\\vx|\\vy)\n\\end{equation*}\n\nThe posterior of $p(\\vz=0|\\vx,\\vy)$ is\n\\begin{equation*}\np_{n,\\text{\\text{data}}}(\\vz=0|\\vx,\\vy) = \\frac{p(\\vz=0) p_n(\\vx|\\vy)}{p(z=0) p_n(\\vx|\\vy) + p(\\vz=1) p_{\\text{data}}(\\vx|\\vy)} = \\frac{\\nu \\cdot p_n(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\text{data}}(\\vx|\\vy)},\n\\end{equation*}\nwhere $\\nu = \\frac{p(\\vz=0)}{p(\\vz=1)}$.\n\nSimilarly, we can have the joint distribution under EBM framework as:\n\\begin{equation*}\np_{n, \\theta}(\\vx) = p(z=0) p_n(\\vx|\\vy) + p(z=1) p_{\\theta}(\\vx|\\vy)\n\\end{equation*}\nAnd the corresponding posterior is:\n\\begin{equation*}\np_{n,\\theta}(\\vz=0|\\vx,\\vy) = \\frac{p(\\vz=0) p_n(\\vx|\\vy)}{p(\\vz=0) p_n(\\vx|\\vy) + p(\\vz=1) p_{\\theta}(\\vx|\\vy)} = \\frac{\\nu \\cdot p_n(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)}\n\\end{equation*}\n\nWe indirectly match $p_\\theta(\\vx|\\vy)$ to $p_{\\text{data}}(\\vx|\\vy)$ by fitting $p_{n,\\theta}(\\vz|\\vx,\\vy)$ to $p_{n,\\text{\\text{data}}}(\\vz|\\vx,\\vy)$ by minimizing their KL-divergence:\n\\begin{equation} \\label{eq:app:EBM_01}\n\\begin{aligned}\n& \\min_\\theta  D_{\\text{KL}}(p_{n,\\text{\\text{data}}}(\\vz|\\vx,\\vy) || p_{n,\\theta}(\\vz|\\vx,\\vy)) \\\\\n& =  \\mathbb{E}_{p_{n,\\text{\\text{data}}}(\\vx,\\vz|\\vy)} [\\log p_{n,\\theta}(\\vz|\\vx,\\vy)] \\\\\n& =   \\int \\sum_\\vz p_{n,\\text{\\text{data}}}(\\vx,\\vz|\\vy) \\cdot \\log p_{n,\\theta}(\\vz|\\vx,\\vy) d \\vx\\\\\n& =  \\int \\Big\\{ p(\\vz=0) p_{n,\\text{\\text{data}}}(\\vx|\\vy,\\vz=0) \\log p_{n,\\theta}(\\vz=0|\\vx,\\vy) \\\\\n & \\quad\\quad\\quad\\quad + p(\\vz=1) p_{n,\\text{\\text{data}}}(\\vx|\\vz=1,\\vy) \\log p_{n,\\theta}(\\vz=1|\\vx,\\vy) \\Big\\} d\\vx \\\\\n& =  \\nu \\cdot \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[\\log p_{n,\\theta}(\\vz=0|\\vx,\\vy) \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[\\log p_{n,\\theta}(\\vz=1|\\vx,\\vy) \\Big] \\\\\n& =  \\nu \\cdot \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{\\nu \\cdot p_n(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{p_\\theta(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)} \\Big].\\\\\n\\end{aligned}\n\\end{equation}\nThis optimal distribution is an estimation to the actual distribution (or data distribution), {\\ie}, $p_\\theta(\\vx|\\vy) \\approx p_{\\text{data}}(\\vx|\\vy)$. We can follow the similar steps for $p_\\theta(\\vy|\\vx) \\approx p_{\\text{data}}(\\vy|\\vx)$. Thus following~\\Cref{eq:app:EBM_01}, the objective function is to maximize\n\\begin{equation} \\label{eq:app:EBM_02}\n\\begin{aligned}\n\\nu \\cdot \\mathbb{E}_{p_{\\text{data}}(\\vy)} \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{\\nu \\cdot p_n(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vy)} \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{p_\\theta(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)} \\Big].\n\\end{aligned}\n\\end{equation}\n\nThe we will adopt three strategies to approximate~\\Cref{eq:app:EBM_02}:\n\\begin{enumerate}\n    \\item \\textbf{Self-normalization.} When the EBM is very expressive, {\\ie}, using deep neural network for modeling, we can assume it is able to approximate the normalized density directly~\\cite{mnih2012fast,song2021train}. In other words, we can set the partition function $A=1$. This is a self-normalized EBM-NCE, with normalizing constant close to 1, {\\ie}, $p(\\vx) = \\exp(-E(\\vx)) = \\exp(f(\\vx))$ in~\\Cref{eq:app:EBM_original}.\n    \\item \\textbf{Exponential tilting term.} Exponential tilting term~\\cite{arbel2020generalized} is another useful trick. It models the distribution as $\\tilde p_\\theta(\\vx) = q(\\vx) \\exp(-E_\\theta(\\vx)) $, where $q(\\vx)$ is the reference distribution. If we use the same reference distribution as the noise distribution, the tilted probability is $\\tilde p_\\theta(\\vx) = p_n(\\vx) \\exp(-E_\\theta(\\vx))$ in~\\Cref{eq:app:EBM_original}.\n    \\item \\textbf{Sampling.} For many cases, we only need to sample 1 negative points for each data, {\\ie}, $\\nu=1$.\n\\end{enumerate}\n\nFollowing these three disciplines, the objective function to optimize $p_{\\theta}(\\vx|\\vy)$ becomes\n\\begin{equation}\n\\begin{aligned}\n% \\widehat{\\mathcal{L}}_{\\text{EBM,NCE}}' \n& \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{ p_n(\\vx|\\vy)}{ p_n(\\vx|\\vy) + \\tilde p_{\\theta}(\\vx|\\vy)} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{\\tilde p_\\theta(\\vx|\\vy)}{ p_n(\\vx|\\vy) + \\tilde p_{\\theta}(\\vx|\\vy)} \\Big]\\\\\n= &   \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{1}{1 + p_{\\theta}(\\vx|\\vy)} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{p_\\theta(\\vx|\\vy)}{1 + p_{\\theta}(\\vx|\\vy)} \\Big]\\\\\n= &   \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{\\exp (-  f_\\vx(\\vx, \\vy))}{\\exp (-  f_\\vx(\\vx, \\vy)) + 1} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{1}{\\exp (-  f_\\vx(\\vx, \\vy)) + 1} \\Big]\\\\\n= &   \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\big(1-\\sigma(  f_\\vx(\\vx, \\vy))\\big) \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\sigma(  f_\\vx(\\vx, \\vy)) \\Big].\n\\end{aligned}\n\\end{equation}\n\nThus, the final EBM-NCE contrastive SSL objective is\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}_{\\text{EBM-NCE}}\n& = -\\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(\\vy)} \\Big[\\mathbb{E}_{p_{n}(\\vx|\\vy)} \\log \\big(1-\\sigma(  f_\\vx(\\vx, \\vy))\\big) + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\log \\sigma(  f_\\vx(\\vx, \\vy)) \\Big]\\\\\n& ~~~~ - \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(\\vx)} \\Big[\\mathbb{E}_{p_{n}(\\vy|\\vx)} \\log \\big(1-\\sigma(  f_\\vy(\\vy,\\vx))\\big) + \\mathbb{E}_{p_{\\text{data}}(\\vy,\\vx)} \\log \\sigma(  f_\\vy(\\vy,\\vx)) \\Big].\n\\end{aligned}\n\\end{equation}\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                    }
                },
                "subsection 12.3": {
                    "name": "EBM-NCE v.s. JSE and InfoNCE",
                    "content": "\nWe acknowledge that there are many other contrastive objectives~\\citep{poole2019variational} that can be used to maximize MI. However, in the research line of graph SSL, as summarized in several recent survey papers~\\citep{xie2021self,liu2021graph,wu2021self}, the two most used ones are InfoNCE and Jensen-Shannon Estimator (JSE)~\\citep{nowozin2016f,hjelm2018learning}.\n\nWe conclude that JSE is very similar to EBM-NCE, while the underlying perspectives are totally different, as explained below.\n\n\\begin{enumerate}\n\\item \\textbf{Derivation and Intuition.} Derivation process and underlying intuition are different. JSE~\\citep{nowozin2016f} starts from f-divergence, then with variational estimation and Fenchel duality on function $f$. Our proposed EBM-NCE is more straightforward: it models the conditional distribution in the MI lower bound~\\Cref{eq:app:MI_objective} with EBM, and solves it using NCE.\n\n\\item \\textbf{Flexibility.} Modeling the conditional distribution with EBM provides a broader family of algorithms. NCE is just one solution to it, and recent progress on score matching~\\citep{song2020score,song2021train} and contrastive divergence~\\citep{du2020improved}, though no longer contrastive SSL, adds on more promising directions. Thus, EBM can provide a potential unified framework for structuring our understanding of self-supervised learning.\n\n\\item \\textbf{Noise distribution.} Starting from~\\citep{hjelm2018learning}, all the following works on graph SSL~\\cite{sun2019infograph,xie2021self,liu2021graph,wu2021self} have been adopting the empirical distribution for noise distribution. However, this is not the case in EBM-NCE. Classic EBM-NCE uses fixed distribution, while more recent work~\\citep{arbel2020generalized} extends it with adaptively learnable noise distribution. With this discipline, more advanced sampling strategies (w.r.t. the noise distribution) can be proposed, {\\eg}, adversarial negative sampling in \\citep{hu2021adco}.\n\\end{enumerate}\n\nIn the above, we conclude three key differences between EBM-NCE and JSE, plus the solid and straightforward derivations on EBM-NCE. We believe this can provide a insightful perspective of SSL to the community.\n\nAccording to the empirical results~\\Cref{sec:experiment_each_loss_component}, we observe that EBM-NCE is better than InfoNCE. This can be explained using the claim from~\\citep{khosla2020supervised}, where the main technical contribution is to construct many positives and many negatives per anchor point. The binary cross-entropy in EBM-NCE is able to realize this to some extent: make all the positive pairs positive and all the negative pairs negative, where the softmax-based cross-entropy fails to capture this, as in InfoNCE.\n\nTo conclude, we are introduce using EBM in modeling MI, which opens many potential venues. As for contrastive SSL, EBM-NCE provides a better perspective than JSE, and is better than InfoNCE on graph-level self-supervised learning.\n\n\\clearpage\n"
                }
            },
            "section 13": {
                "name": "Generative Self-Supervised Learning",
                "content": " \\label{sec:app:generative_ssl}\nGenerative SSL is another classic track for unsupervised pre-training~\\citep{kingma2013auto,larsson2016learning,kingma2018glow}, though the main focus is on distribution learning. In \\model, we start with VAE for the following reasons:\n\\begin{enumerate}\n    \\item One of the biggest attributes of our problem is that the mapping between two views are stochastic: multiple 3D conformers can correspond to the same 2D topology. Thus, we expect a stochastic model~\\citep{nielsen2020survae} like VAE, instead of the deterministic ones.\n    \\item For pre-training and fine-tuning, we need to learn an explicit and powerful representation function that can be used for downstream tasks.\n    \\item The decoder for structured data like graph are often complicated, {\\eg.}, the auto-regressive generation. This makes them suboptimal.\n\\end{enumerate}\n\nTo cope with these challenges, in \\model, we start with VAE-like generation model, and later propose a \\textit{light-weighted} and \\textit{smart} surrogate loss as objective function.\nNotice that for notation simplicity, for this section, we use $h_{\\vy}$ and $h_{\\vx}$ to delegate the 2D and 3D GNN respectively.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                "subsection 13.1": {
                    "name": "Variational Molecule Reconstruction",
                    "content": "\nAs shown in~\\Cref{eq:app:MI_objective}, our main motivation is to model the conditional likelihood:\n\\begin{equation*}\n\\begin{aligned}\n\\mathcal{L}_{\\text{MI}} & = -\\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} [\\log p(\\vx|\\vy) + \\log p(\\vy|\\vx)]\n\\end{aligned}\n\\end{equation*}\n\nBy introducing a reparameterized variable $\\vz_\\vx = \\mu_{\\vx} + \\sigma_{\\vx} \\odot \\epsilon$, where $\\mu_\\vx$ and $\\sigma_\\vx$ are two flexible functions on $h_\\vx$, $\\epsilon \\sim \\mathcal{N}(0,I)$ and $\\odot$ is the element-wise production,\nwe have a lower bound on the conditional likelihood:\n\\begin{equation} \\label{eq:app:log_likelihood_01}\n\\begin{aligned}\n\\log p(\\vy|\\vx)\n \\ge \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\log p(\\vy|\\vz_\\vx) \\big] - KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)). \n\\end{aligned}\n\\end{equation}\nSimilarly, we have\n\\begin{equation} \\label{eq:app:log_likelihood_02}\n\\log p(\\vx|\\vy) \\ge \\mathbb{E}_{q(\\vz_\\vy|\\vy)} \\big[ \\log p(\\vx|\\vz_\\vy) \\big] - KL(q(\\vz_\\vy|\\vy) || p(\\vz_\\vy)),\n\\end{equation}\nwhere $\\vz_\\vy = \\mu_\\vy + \\sigma_\\vy \\odot \\epsilon$. Here $\\mu_\\vy$ and $\\sigma_\\vy$ are flexible functions on $h_\\vy$, and $\\epsilon \\sim \\mathcal{N}(0, I)$.\nFor implementation, we take multi-layer perceptrons (MLPs) for $\\mu_\\vx, \\mu_\\vy, \\sigma_\\vx, \\sigma_\\vy$.\n\nBoth the above objectives are composed of a conditional log-likelihood and a KL-divergence. The conditional log-likelihood has also been recognized as the \\textit{reconstruction term}: it is essentially to reconstruct the 3D conformers ($\\vy$) from the sampled 2D molecular graph representation ($\\vz_{\\vx}$). However, performing the graph reconstruction on the data space is not easy: since molecules are discrete, modeling and measuring are not trivial.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 13.2": {
                    "name": "Variational Representation Reconstruction",
                    "content": "\nTo cope with data reconstruction issue, we propose a novel generative loss termed variation representation reconstruction (VRR). The pipeline is in~\\Cref{fig:app:generative_ssl}.\n\n\nOur proposed solution is very straightforward. Recall that MI is invariant to continuous bijective function~\\citep{belghazi2018mutual}. So suppose we have a representation function $h_{\\vy}$ satisfying this condition, and this can guide us a surrogate loss by transferring the reconstruction from data space to the continuous representation space:\n\\begin{equation*}\n\\begin{aligned}\n\\mathbb{E}_{q(\\vz_\\vx|\\vx)}[\\log p(\\vy|\\vz_\\vx)]\n& = - \\mathbb{E}_{q(\\vz_\\vx|\\vx)}[ \\| h_{\\vy}(g_x(\\vz_\\vx)) - h_{\\vy}(\\vy) \\|_2^2 ] + C,\n\\end{aligned}\n\\end{equation*}\nwhere $g_x$ is the decoder and $C$ is a constant, and this introduces to using the mean-squared error (MSE) for \\textbf{reconstruction on the representation space}.\n\nThen for the reconstruction, current formula has two steps: i) the latent code $z_{\\vx}$ is first mapped to molecule space, and ii) it is mapped to the representation space. We can approximate these two mappings with one projection step, by directly projecting the latent code $z_{\\vx}$ to the 3D representation space, {\\ie}, $q_\\vx(z_\\vx) \\approx h_{\\vy}( g_\\vx ( z_\\vx ))$. This gives us a variation representation reconstruction (VRR) SSL objective as below:\n\\begin{equation*}\n\\begin{aligned}\n\\mathbb{E}_{q(\\vz_\\vx|\\vx)}[\\log p(\\vy|\\vz_\\vx)]\n& = - \\mathbb{E}_{q(\\vz_\\vx|\\vx)}[ \\| q_x(\\vz_\\vx) - h_{\\vy}(\\vy) \\|_2^2 ] + C.\n\\end{aligned}\n\\end{equation*}\n\n\\paragraph{$\\beta$-VAE}\nWe consider introducing a $\\beta$ variable~\\citep{higgins2016beta} to control the disentanglement of the latent representation. To be more specific, we would have\n\\begin{equation}\n\\begin{aligned}\n\\log p(\\vy|\\vx)\n& \\ge \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\log p(\\vy|\\vz_\\vx) \\big] - \\beta \\cdot KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)).\n\\end{aligned}\n\\end{equation}\n\n\\paragraph{Stop-gradient}\nFor the optimization on variational representation reconstruction, related work have found that adding the stop-gradient operator (SG) as a regularizer can make the training more stable without collapse both empirically~\\citep{grill2020bootstrap,chen2021exploring} and theoretically~\\citep{tian2021understanding}. Here, we may as well utilize this SG operation in the objective function:\n\\begin{equation}\n\\mathbb{E}_{q(\\vz_\\vx|\\vx)}[\\log p(\\vy|\\vz_\\vx)] = - \\mathbb{E}_{q(\\vz_\\vx|\\vx)}[ \\| q_x(\\vz_\\vx) - \\text{SG}(h_{\\vy}(\\vy)) \\|_2^2 ] + C.\n\\end{equation}\n\n\\paragraph{Objective function for VRR}\nThus, combining both two regularizers mentioned above, the final objective function for VRR is:\n\\begin{equation} \\label{eq:app:final_vrr}\n\\begin{aligned}\n\\mathcal{L}_{\\text{VRR}}\n& = \\frac{1}{2} \\Big[ \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\| q_x(\\vz_\\vx) - \\text{SG}(h_\\vy) \\|^2 \\big] + \\mathbb{E}_{q(\\vz_\\vy|\\vy)} \\big[ \\| q_y(\\vz_\\vy) - \\text{SG}(h_\\vx) \\|_2^2 \\big] \\Big]\\\\\n& \\quad\\quad + \\frac{\\beta}{2} \\cdot \\Big[ KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)) + KL(q(\\vz_\\vy|\\vy) || p(\\vz_\\vy)) \\Big].\n\\end{aligned}\n\\end{equation}\nNote that MI is invariant to continuous bijective function~\\citep{belghazi2018mutual}, thus this surrogate loss would be exact if the encoding function $h_{\\vy}$ and $h_{\\vx}$ satisfy this condition. However, we find GNN (both GIN and SchNet) can, though do not meet the condition, provide quite robust performance empirically, which justify the effectiveness of VRR.\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 13.3": {
                    "name": "Variational Representation Reconstruction and Non-Contrastive SSL",
                    "content": "\nBy introducing VRR, we provide another perspective to understand the generative SSL, including the recently-proposed non-contrastive SSL~\\citep{grill2020bootstrap,chen2021exploring}.\n\nWe provide a unified structure on the intra-data generative SSL:\n\\begin{itemize}\n    \\item Reconstruction to the data space, like~\\Cref{eq:variational_lower_bound,eq:app:log_likelihood_01,eq:app:log_likelihood_02}.\n    \\item Reconstruction to the representation space, {\\ie}, VRR in~\\Cref{eq:app:final_vrr}.\n    \\begin{itemize}\n        \\item If we \\textbf{remove the stochasticity}, then it is simply the representation reconstruction (RR), as we tested in the ablation study~\\Cref{sec:experiment_each_loss_component}.\n        \\item If we \\textbf{remove the stochasticity} and assume two views are \\textbf{sharing the same representation function}, like CNN for multi-view learning on images, then it is reduced to the BYOL~\\citep{grill2020bootstrap} and SimSiam~\\citep{chen2021exploring}. In other words, these recently-proposed non-contrastive SSL methods are indeed special cases of VRR.\n    \\end{itemize}\n\\end{itemize}\n\n\n\\clearpage\n"
                }
            },
            "section 14": {
                "name": "Dataset Overview",
                "content": " \\label{app:sec:dataset}\n\n",
                "subsection 14.1": {
                    "name": "Pre-Training Dataset Overview",
                    "content": "\nIn this section, we provide the basic statistics of the pre-training dataset (GEOM).\n\nIn~\\Cref{fig:hist}, we plot the histogram (logarithm scale on the y-axis) and cumulative distribution on the number of conformers of each molecule. As shown by the histogram and curves, there are certain number of molecules having over 1000 possible 3d conformer structures, while over 80\\% of molecules have less than 100 conformers.\n\n\n\nIn~\\Cref{fig:hist}, we plot the histogram of the summation of top (descending sorted by weights) \\{1,5,10,20\\} conformer weights. The physical meaning of the weight is the portion of each conformer occurred in nature. We observe that the top 5 or 10 conformers are sufficient as they have dominated nearly all the natural observations. Such long-tailed distribution is also in alignment with our findings in the ablation studies. We find that utilizing top five conformers in the \\model\\, has reached an idealised spot between effectiveness and efficiency.\n\n\n"
                },
                "subsection 14.2": {
                    "name": "Downstream Dataset Overview",
                    "content": " \\label{app:sec:downstream_datasets}\nIn this section, we review the four main categories of datasets used for downstream tasks.\n\n\\paragraph{Molecular Property: Pharmacology}\nThe Blood-Brain Barrier Penetration (BBBP)~\\cite{martins2012bayesian} dataset measures whether a molecule will penetrate the central nervous system.\nAll three datasets, Tox21~\\cite{tox21}, ToxCast~\\cite{wu2018moleculenet}, and ClinTox~\\cite{gayvert2016data} are related to the toxicity of molecular compounds.\nThe Side Effect Resource (SIDER)~\\cite{kuhn2015sider} dataset stores the adverse drug reactions on a marketed drug database.\n\n\\paragraph{Molecular Property: Physical Chemistry}\nDataset proposed in~\\cite{delaney2004esol} measures aqueous solubility of the molecular compounds. Lipophilicity (Lipo) dataset is a subset of ChEMBL~\\cite{gaulton2012chembl} measuring the molecule octanol/water distribution coefficient. CEP dataset is a subset of the Havard Clean Energy Project (CEP)~\\cite{hachmann2011harvard}, which estimates the organic photovoltaic efficiency.\n\n\\paragraph{Molecular Property: Biophysics}\nMaximum Unbiased Validation (MUV)~\\cite{rohrer2009maximum} is another sub-database from PCBA, and is obtained by applying a refined nearest neighbor analysis. HIV is from the Drug Therapeutics Program (DTP) AIDS Antiviral Screen~\\cite{zaharevitz2015aids}, and it aims at predicting inhibit HIV replication. BACE measures the binding results for a set of inhibitors of $\\beta$-secretase 1 (BACE-1), and is gathered in MoleculeNet~\\cite{wu2018moleculenet}. Malaria~\\cite{gamo2010thousands} measures the drug efficacy against the parasite that causes malaria.\n\n\\paragraph{Drug-Target Affinity}\nDavis~\\citep{davis2011comprehensive} measures the binding affinities between kinase inhibitors and kinases, scored by the $K_d$ value (kinase dissociation constant).\nKIBA~\\citep{tang2014making} contains binding affinities for kinase inhibitors from different sources, including $K_i$, $K_d$ and $\\text{IC}_{50}$. KIBA scores~\\citep{ozturk2018deepdta} are constructured to optimize the consistency among these values.\n\n\n\n\n"
                }
            },
            "section 15": {
                "name": "Experiments Details",
                "content": "\n\n",
                "subsection 15.1": {
                    "name": "Self-supervised Learning Baselines",
                    "content": " \\label{appendix:hyper}\nFor the SSL baselines in main results (\\Cref{tab:main_results}), generally we can match with the original paper, even though most of them are using larger pre-training datasets, like ZINC-2m. Yet, we would like to add some specifications.\n\\begin{itemize}\n    \\item G-\\{Contextual, Motif\\}\\citep{rong2020self} proposes a new GNN model for backbone model, and does pre-training on a larger dataset. Both settings are different from us.\n    \\item JOAO~\\citep{you2021graph} has two versions in the original paper. In this paper, we run both versions and report the optimal one.\n    \\item Almost all the graph SSL baselines are reporting the test performance with optimal validation error, while GraphLoG~\\citep{xu2021self} reports 73.2 in the paper with the last-epoch performance. This can be over-optimized in terms of overfitting, and here we rerun it with the same downstream evaluation strategy as a fair comparison.\n\\end{itemize}\n\\clearpage\n\n\n"
                },
                "subsection 15.2": {
                    "name": "Ablation Study: The Effect of Masking Ratio and Number of Conformers",
                    "content": " \\label{app:sec:effect_of_M_and_C}\n\n\n\n\n\n\n"
                },
                "subsection 15.3": {
                    "name": "Ablation Study: Effect of Each Loss Component",
                    "content": "\n\n\n\\clearpage\n\n"
                },
                "subsection 15.4": {
                    "name": "Broader Range of Downstream Tasks: Molecular Property Prediction Prediction",
                    "content": " \\label{sec:app:regression_results}\n\n\n"
                },
                "subsection 15.5": {
                    "name": "Broader Range of Downstream Tasks: Drug-Target Affinity Prediction",
                    "content": "\n\n\n\n\n\n"
                },
                "subsection 15.6": {
                    "name": "appendix:case",
                    "content": "\n\\textbf{Shape Analysis (3D Diameter Prediction).} Diameter is an important measure in molecule~\\cite{liu2010using,melnik2003distance}, and genome~\\cite{finn2017comparative} modelling. Usually, the longer the 2D diameter (longest adjacency path) is, the larger the 3D diameter (largest atomic pairwise l2 distance). However, this is not always true. Therefore, we are particularly interested in using the 2D graph to predict the 3D diameter when the 2D and 3D molecular landscapes are with large differences (as in~\\Cref{fig:case} and ~\\Cref{fig:case_appendix}). We formulate it as a $n$-class recognition problem, where $n$ is the number of class after removing the consecutive intervals. We provide numerical results in~\\Cref{table:diam} and more visualisation examples in~\\Cref{fig:case_vis}.\n\n\n\n\n\n\\textbf{Long-Range Donor-Acceptor Detection.} Donor-Acceptor structures such as hydrogen bonds have key impacts on the molecular geometrical structures (collinear and coplanarity), and physical properties (melting point, water affinity, viscosity etc.). Usually, atom pairs such as ``O...H'' that are closed in the Euclidean space are considered as the donor-acceptor structures~\\cite{kaur2020understanding}.\nOn this basis, we are particularly interested in using the 2D graph to recognize (i.e., binary classification) donor-acceptor structures which have larger ranges in the 2D adjacency (as shown in~\\Cref{fig:case}). Similarly, we select the molecules whose donor-acceptor are close in 3D Euclidean distance but far in the 2D adjacency. We provide numerical results in~\\Cref{table:donor}. Both tables show that MVP is the MVP :)\n\n\n\n\n\\paragraph{Chirality.} We have also explored other tasks such as predicting the molecular chirality, it is a challenging setting if only 2D molecular graphs are provided~\\cite{pattanaik2020message}. We found that \\model\\,brings negligible improvements due to the model capacity of SchNet. We save this in the ongoing work.\n\n\n\n"
                }
            }
        },
        "tables": {
            "tab:main_results": "\\begin{table}[t!]\n\\caption{\nResults for molecular property prediction tasks.\nFor each downstream task, we report the mean (and standard deviation) ROC-AUC of 3 seeds with scaffold splitting.\nFor \\model\\,, we set $M=0.15$ and $C=5$.\nThe best and second best results are marked \\textbf{\\underline{bold}} and \\textbf{bold}, respectively.\n\\vspace{-0.24cm}\n}\n\\label{tab:main_results}\n% \\scriptsize\n\\small\n\\setlength{\\tabcolsep}{3pt}\n\\centering\n\\begin{tabular}{l c c c c c c c c c}\n\\toprule\nPre-training & BBBP & Tox21 & ToxCast & Sider & ClinTox & MUV & HIV & Bace & Avg\\\\\n\\midrule\n-- & 65.4(2.4) & 74.9(0.8) & 61.6(1.2) & 58.0(2.4) & 58.8(5.5) & 71.0(2.5) & 75.3(0.5) & 72.6(4.9) & 67.21 \\\\\n\\midrule\nEdgePred & 64.5(3.1) & 74.5(0.4) & 60.8(0.5) & 56.7(0.1) & 55.8(6.2) & 73.3(1.6) & 75.1(0.8) & 64.6(4.7) & 65.64 \\\\\nAttrMask & 70.2(0.5) & 74.2(0.8) & 62.5(0.4) & 60.4(0.6) & 68.6(9.6) & 73.9(1.3) & 74.3(1.3) & 77.2(1.4) & 70.16 \\\\\nGPT-GNN & 64.5(1.1) & \\textbf{75.3(0.5)} & 62.2(0.1) & 57.5(4.2) & 57.8(3.1) & 76.1(2.3) & 75.1(0.2) & 77.6(0.5) & 68.27 \\\\\nInfoGraph & 69.2(0.8) & 73.0(0.7) & 62.0(0.3) & 59.2(0.2) & 75.1(5.0) & 74.0(1.5) & 74.5(1.8) & 73.9(2.5) & 70.10 \\\\\nContextPred & 71.2(0.9) & 73.3(0.5) & 62.8(0.3) & 59.3(1.4) & 73.7(4.0) & 72.5(2.2) & 75.8(1.1) & 78.6(1.4) & 70.89 \\\\\nGraphLoG & 67.8(1.7) & 73.0(0.3) & 62.2(0.4) & 57.4(2.3) & 62.0(1.8) & 73.1(1.7) & 73.4(0.6) & 78.8(0.7) & 68.47 \\\\\nG-Contextual & 70.3(1.6) & 75.2(0.3) & 62.6(0.3) & 58.4(0.6) & 59.9(8.2) & 72.3(0.9) & 75.9(0.9) & 79.2(0.3) & 69.21 \\\\\nG-Motif & 66.4(3.4) & 73.2(0.8) & 62.6(0.5) & 60.6(1.1) & 77.8(2.0) & 73.3(2.0) & 73.8(1.4) & 73.4(4.0) & 70.14 \\\\\nGraphCL & 67.5(3.3) & 75.0(0.3) & 62.8(0.2) & 60.1(1.3) & 78.9(4.2) & \\textbf{77.1(1.0)} & 75.0(0.4) & 68.7(7.8) & 70.64 \\\\\nJOAO & 66.0(0.6) & 74.4(0.7) & 62.7(0.6) & 60.7(1.0) & 66.3(3.9) & 77.0(2.2) & \\textbf{76.6(0.5)} & 72.9(2.0) & 69.57 \\\\\n\\midrule\n% 3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0\n\\model\\, & 68.5(0.2) & 74.5(0.4) & 62.7(0.1) & \\textbf{62.3(1.6)} & \\textbf{79.0(2.5)} & 75.0(1.4) & 74.8(1.4) & 76.8(1.1) & 71.69 \\\\\n% 3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0\n\\modelAM & \\textbf{70.8(0.5)} & \\textbf{\\underline{75.9(0.5)}} & \\textbf{\\underline{63.1(0.2)}} & 60.2(1.1) & \\textbf{\\underline{79.1(2.8)}} & \\textbf{\\underline{77.7(0.6)}} & 76.0(0.1) & \\textbf{79.3(1.5)} & \\textbf{72.76} \\\\\n% 3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0\n\\modelCP & \\textbf{\\underline{72.4(1.6)}} & 74.4(0.2) & \\textbf{\\underline{63.1(0.4)}} & \\textbf{\\underline{63.9(1.2)}} & 77.5(4.2) & 75.0(1.0) & \\textbf{\\underline{77.0(1.2)}} & \\textbf{\\underline{81.2(0.9)}} & \\textbf{\\underline{73.07}} \\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-2ex}\n\\end{table}",
            "tab:ablation_objective_function": "\\begin{wraptable}[13]{r}{0.5\\textwidth}\n\\setlength{\\tabcolsep}{3.53pt}\n\\vspace{-0.4cm}\n\\small\n\\caption{\nAblation on the objective function.\n\\vspace{-0.35cm}\n}\n\\label{tab:ablation_objective_function}\n\\small\n\\centering\n\\begin{tabular}{l c c c}\n\\toprule\n\\model\\, Loss & Contrastive & Generative & Avg\\\\\n\\midrule\nRandom & & & 67.21\\\\\n\\midrule\nInfoNCE only & \\checkmark & & 68.85\\\\\nEBM-NCE only & \\checkmark & & 70.15\\\\\nVRR only & & \\checkmark & 69.29\\\\\nRR only & & \\checkmark & 68.89\\\\\n\\midrule\nInfoNCE + VRR & \\checkmark & \\checkmark & 70.67\\\\\nEBM-NCE + VRR & \\checkmark & \\checkmark & 71.69\\\\\nInfoNCE + RR  & \\checkmark & \\checkmark & 70.60\\\\\nEBM-NCE + RR  & \\checkmark & \\checkmark & 70.94\\\\\n\\bottomrule\n\\end{tabular}\n\\end{wraptable}",
            "tab:main_regression": "\\begin{table}[t!]\n\\setlength{\\tabcolsep}{7pt}\n\\centering\n\\caption{\nResults for four molecular property prediction tasks (regression) and two DTA tasks (regression).\nWe report the mean RMSE of 3 seeds with scaffold splitting for molecular property downstream tasks, and mean MSE for 3 seeds with random splitting on DTA tasks.\nFor \\model\\,, we set $M=0.15$ and $C=5$.\nThe best performance for each task is marked in \\textbf{\\underline{bold}}.\nWe omit the std here since they are very small and indistinguishable. For complete results, please check~\\Cref{sec:app:regression_results}.\n\\vspace{-4ex}\n}\n\\label{tab:main_regression}\n\\begin{tabular}{l c c c c c c c c}\n\\toprule\n & \\multicolumn{5}{c}{Molecular Property Prediction} & \\multicolumn{3}{c}{Drug-Target Affinity}\\\\\n\\cmidrule(lr){2-6} \\cmidrule(lr){7-9} Pre-training & ESOL & Lipo & Malaria & CEP & Avg & Davis & KIBA & Avg\\\\\n\\midrule\n-- \n& 1.178 & 0.744 & 1.127 & 1.254 & 1.0756\n& 0.286 & 0.206 & 0.2459\\\\\n\\midrule\nAM\n& 1.112 & 0.730 & 1.119 & 1.256 & 1.0542\n& 0.291 & 0.203 & 0.2476\\\\\nCP\n& 1.196 & 0.702 & 1.101 & 1.243 & 1.0606\n& 0.279 & 0.198 & 0.2382 \\\\\nJOAO\n& 1.120 & 0.708 & 1.145 & 1.293 & 1.0663\n& 0.281 & 0.196 & 0.2387\\\\\n\\midrule\n\\model\n% 3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000_morefeat/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_1_100_0\n& 1.091 & 0.718 & 1.114 & 1.236 & 1.0397\n% 3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000_morefeat/CL_1_VAE_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0\n& 0.280 & 0.178 & 0.2286\\\\\n\\modelAM\\,\\,\\,\n% 3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000_morefeat/CL_1_VAE_1_AM_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0\n& 1.064 & 0.691 & 1.106 & \\textbf{\\underline{1.228}} & 1.0221\n% 3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000_morefeat/CL_1_VAE_0.1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0\n& \\textbf{\\underline{0.274}} & 0.175 & 0.2248\\\\\n\\modelCP\\,\\,\\,\n% 3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000_morefeat/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0\n& \\textbf{\\underline{1.029}} & \\textbf{\\underline{0.681}} & \\textbf{\\underline{1.097}} & 1.244 & \\textbf{\\underline{1.0128}}\n% 3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000_morefeat/CL_1_VAE_0.1_CP_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0\n& 0.276 & \\textbf{\\underline{0.168}} & \\textbf{\\underline{0.2223}}\\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-1ex}\n\\end{table}",
            "app:tab:compare": "\\begin{table}[b]\n\\small\n\\caption{\nComparison between \\model\\, and existing graph SSL methods.\n\\vspace{-0.6ex}\n}\n\\label{app:tab:compare}\n\\center\n\\setlength{\\tabcolsep}{10pt}\n\\begin{tabular}{l c c c c c}\n\\toprule\n\\multirow{2}{*}{SSL Pre-training} & \\multicolumn{2}{c}{Graph View} & \\multicolumn{3}{c}{SSL Category}\\\\\n\\cmidrule(lr){2-3}\n\\cmidrule(lr){4-6}\n& 2D Topology & 3D Geometry & Generative & Contrastive & Predictive\\\\\n\\midrule\nEdgePred~\\citep{hamilton2017inductive} & \\checkmark & - & \\checkmark & - & -\\\\\nAttrMask~\\citep{hu2019strategies} & \\checkmark & - & \\checkmark & - & -\\\\\nGPT-GNN~\\citep{hu2020gpt} & \\checkmark & - & \\checkmark & - & -\\\\\nInfoGraph~\\citep{velivckovic2018deep,sun2019infograph} & \\checkmark & - & - & \\checkmark& -\\\\\nContexPred~\\citep{hu2019strategies} & \\checkmark & - & - & \\checkmark& -\\\\\nGraphLoG~\\citep{xu2021self} & \\checkmark & - & - & \\checkmark& -\\\\\nG-Contextual~\\citep{rong2020self} & \\checkmark & - & - & \\checkmark& -\\\\\nGraphCL~\\citep{You2020GraphCL} & \\checkmark & - & - & \\checkmark& -\\\\\nJOAO~\\citep{you2021graph} & \\checkmark & - & - & \\checkmark& -\\\\\nG-Motif~\\citep{rong2020self} & \\checkmark & - & - & - & \\checkmark\\\\\n\\midrule\n\\model\\,(Ours) & \\checkmark & \\checkmark & \\checkmark & \\checkmark& -\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:app:qm9": "\\begin{table}[H]\n\\centering\n\\scriptsize\n\\caption{\nReproduced MAE on QM9. 100k for training, 17,748 for val, 13,083 for test. The last column is the approximated running time.\n}\n\\label{tab:app:qm9}\n\\begin{tabular}{l c c c c c c c c c c c c r}\n\\toprule\n & alpha & gap & homo & lumo & mu & cv & g298 & h298 & r2 & u298 & u0 & zpve & time\\\\\n\\midrule\nSchNet~\\citep{schutt2017schnet} & 0.077 & 50 & 32 & 26 & 0.030 & 0.032 & 15 & 14 & 0.122 & 14 & 14 & 1.751 & 3h\\\\\nSE(3)-Trans~\\citep{fuchs2020se} & 0.143 & 59 & 36 & 36 & 0.052 & 0.068 & 68 & 72 & 1.969 & 68 & 74 & 5.517 & 50h\\\\\nEGNN~\\citep{satorras2021n} & 0.075 & 49 & 29 & 26 & 0.030 & 0.032 & 11 & 10 & 0.076 & 10 & 10 & 1.562 & 24h\\\\\nSphereNet~\\citep{liu2021spherical} & 0.054 & 41 & 22 & 19 & 0.028 & 0.027 & 10 & 8 & 0.295 & 8 & 8 & 1.401 & 50h\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:mol_dataset_summary": "\\begin{table}[H]\n\\centering\n\\small\n\\caption{Summary for the molecule chemical datasets.}\n\\begin{tabular}{l l r r r r}\n    \\toprule\n    Dataset & Task & \\# Tasks & \\# Molecules & \\# Proteins & \\# Molecule-Protein pairs\\\\\n    \\midrule\n    BBBP & Classification & 1 & 2,039 & -& -\\\\\n    Tox21 & Classification & 12 & 7,831 & -& -\\\\\n    ToxCast & Classification & 617 & 8,576 & -& -\\\\\n    Sider & Classification & 27 & 1,427 & -& -\\\\\n    ClinTox & Classification & 2 & 1,478 & -& -\\\\\n    MUV & Classification & 17 & 93,087 & -& -\\\\\n    HIV & Classification & 1 & 41,127 & -& -\\\\\n    Bace & Classification & 1 & 1,513 & -& -\\\\\n    \\midrule\n    Delaney & Regression & 1 & 1,128 & -& -\\\\\n    Lipo & Regression & 1 & 4,200 & -& -\\\\\n    Malaria & Regression & 1 & 9,999 & -& -\\\\\n    CEP & Regression & 1 & 29,978 & -& -\\\\\n    \\midrule\n    Davis & Regression & 1 & 68 & 379 & 30,056 \\\\\n    KIBA & Regression & 1 & 2,068 & 229 & 118,254 \\\\\n    \\bottomrule\n\\end{tabular}\n\\label{tab:mol_dataset_summary}\n\\end{table}"
        },
        "figures": {
            "fig:both_ssl": "\\begin{figure}[ht]\n\\centering\n\\vspace{-2ex}\n\\includegraphics[width=\\textwidth]{figures/Diagram_Bundle_4.pdf}\n\\vspace{-2ex}\n\\caption{\nOverview of the pre-training stage in \\model. The black dashed circles denote subgraph masking, and we mask the same region in the 2D and 3D graphs. Multiple views of the molecules (herein: Halicin) are mapped to the representation space via 2D and 3D GNN models, where we conduct \\model\\ for SSL pre-training, using both contrastive and generative pretext tasks.\n}\n\\label{fig:both_ssl}\n\\end{figure}",
            "fig:case": "\\begin{figure}[htbp!]\n\\centering\n\\vspace{-2ex}\n\\includegraphics[width=\\textwidth]{figures/Case_Study_2.pdf}\n\\vspace{-5ex}\n\\caption{We select the molecules whose properties can be easily resolved via 3D but not 2D. The randomly initialised 2D GNN achieves accuracy of $38.9\\pm0.8$ and $77.9\\pm1.1$, respectively. The \\model\\, pre-trained ones obtain scores of $42.3\\pm1.3$ and $81.5\\pm0.4$, outperforming all the precedents in~\\Cref{sec:main_results}. We plot cases where random initialization fails but \\model\\, is correct.}\n\\label{fig:case}\n\\vspace{-4ex}\n\\end{figure}",
            "fig:app:MI": "\\begin{figure}[htb!]\n\\centering\n\\includegraphics[width=0.5\\textwidth]{figures/MutualInfo_Final.pdf}\n\\caption{Venn diagram of mutual information. Inspired by wikipedia.}\n\\label{fig:app:MI}\n\\end{figure}",
            "fig:app:contrastive_ssl": "\\begin{figure}[htb!]\n\\centering\n\\includegraphics[width=\\textwidth]{figures/Diagram_Contrastive_final.pdf}\n\\vspace{-4ex}\n\\caption{\nContrastive SSL in \\model. The black dashed circles represent subgraph masking.\n}\n\\label{fig:app:contrastive_ssl}\n\\vspace{-2ex}\n\\end{figure}",
            "fig:app:generative_ssl": "\\begin{figure}[H]\n\\centering\n\\includegraphics[width=\\textwidth]{figures/Diagram_Generative_final.pdf}\n\\vspace{-4ex}\n\\caption{VRR SSL in \\model. The black dashed circles represent subgraph masking.\n}\n\\label{fig:app:generative_ssl}\n\\hfill\n\\end{figure}",
            "fig:case_appendix": "\\begin{figure}[ht]\n\\centering\n\\vspace{-2ex}\n\\includegraphics[width=\\textwidth]{figures/Case_Study_Appendix.pdf}\n\\caption{Molecules selection, we select the molecules that lies in the black dash box.}\n\\label{fig:case_appendix}\n\\end{figure}",
            "fig:case_vis": "\\begin{figure}[ht]\n\\centering\n\\vspace{-2ex}\n\\includegraphics[width=\\textwidth]{figures/case_study_SSL_better.png}\n\\vspace{-2ex}\n\\caption{Molecule examples where GraphMVP successfully recognizes the 3D diameters while random initialisation fails, legends are in a format of ``molecule id''-``2d diameter''-``3d diameter''.}\n\\label{fig:case_vis}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation} \\label{eq:2d_gnn}\nh_{\\text{2D}} = \\text{GNN-2D}(T_{\\text{2D}}(g_{\\text{2D}})) = \\text{GNN-2D}(T_{\\text{2D}}(X, E)).\n\\end{equation}",
            "eq:2": "\\begin{equation} \\label{eq:3d_gnn}\nh_{\\text{3D}} = \\text{GNN-3D}(T_{\\text{3D}}(g_{\\text{3D}})) = \\text{GNN-3D}(T_{\\text{3D}}(X, R)),\n\\end{equation}",
            "eq:eq:objective_infonce": "\\begin{equation}\\label{eq:objective_infonce}\n\\small{\n\\fontsize{7.8}{1}\\selectfont\n\\begin{aligned}\n\\mathcal{L}_{\\text{InfoNCE}}=-\\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)}\\Big[\\log \\frac{\\exp(f_{\\vx}(\\vx, \\vy))}{\\exp(f_{\\vx}(\\vx, \\vy)) + \\sum\\limits_{j}  \\exp(f_{\\vx}(\\vx^{j},\\vy)})+\\log\\frac{\\exp(f_{\\vy}(\\vy,\\vx))}{\\exp(f_{\\vy}(\\vy,\\vx)) + \\sum\\limits_{j} \\exp(f_{\\vy}(\\vy^{j},\\vx))} \\Big],\n\\end{aligned}\n}\n\\end{equation}",
            "eq:3": "\\begin{equation} \\label{eq:objective_ebm_nce}\n\\small{\n\\begin{aligned}\n\\mathcal{L}_{\\text{EBM-NCE}}\n& = -\\frac{1}{2} \\mathbb{E}_{p(\\vy)} \\Big[\\mathbb{E}_{p_{n}(\\vx|\\vy)} \\log \\big(1-\\sigma(  f_x(\\vx, \\vy))\\big) + \\mathbb{E}_{p(\\vx|\\vy )} \\log \\sigma(  f_x(\\vx, \\vy)) \\Big]\\\\\n& ~~~~~ -\\frac{1}{2} \\mathbb{E}_{p(\\vx)} \\Big[\\mathbb{E}_{p_{n}(\\vy|\\vx)} \\log \\big(1-\\sigma(  f_y(\\vy,\\vx))\\big) + \\mathbb{E}_{p(\\vy,\\vx)} \\log \\sigma(  f_y(\\vy,\\vx)) \\Big],\n\\end{aligned}\n}\n\\end{equation}",
            "eq:4": "\\begin{equation} \\label{eq:variational_lower_bound}\n\\small\n\\begin{aligned}\n\\log p(\\vy|\\vx)\n\\ge \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\log p(\\vy|\\vz_\\vx) \\big] - KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)). \n\\end{aligned}\n\\end{equation}",
            "eq:5": "\\begin{equation} \\label{eq:variational_lower_bound_approximation}\n\\small\n\\begin{aligned}\n\\mathcal{L}_{\\text{G}} = \\mathcal{L}_{\\text{VRR}}\n = & \\frac{1}{2} \\Big[ \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\| q_{\\vx}(\\vz_\\vx) - \\text{SG}(h_{\\vy}) \\|^2 \\big] + \\mathbb{E}_{q(\\vz_\\vy|\\vy)} \\big[ \\| q_{\\vy}(\\vz_\\vy) - \\text{SG}(h_{\\vx}) \\|_2^2 \\big] \\Big]\\\\\n& + \\frac{\\beta}{2} \\cdot \\Big[ KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)) + KL(q(\\vz_\\vy|\\vy) || p(\\vz_\\vy)) \\Big].\n\\end{aligned}\n\\end{equation}",
            "eq:6": "\\begin{equation} \\label{eq:graphmvp}\n\\small\n\\begin{aligned}\n\\mathcal{L}_{\\text{\\model}}\n& = \\alpha_1 \\cdot \\mathcal{L}_{\\text{C}} + \\alpha_2 \\cdot \\mathcal{L}_{\\text{G}},\\\\\n\\end{aligned}\n\\end{equation}",
            "eq:7": "\\begin{equation} \\label{eq:graphmvp_variants}\n\\small\n\\begin{aligned}\n\\mathcal{L}_{\\text{\\modelAM}} = \\mathcal{L}_{\\text{\\model}} + \\alpha_3 \\cdot \\mathcal{L}_{\\text{Generative 2D-SSL}},~~~~~\n\\mathcal{L}_{\\text{\\modelCP}} = \\mathcal{L}_{\\text{\\model}} + \\alpha_3 \\cdot \\mathcal{L}_{\\text{Contrastive 2D-SSL}}.\n\\end{aligned}\n\\end{equation}",
            "eq:8": "\\begin{equation} \\label{eq:MI_objective}\n\\small\nI(X;Y) \\ge \\mathcal{L}_{\\text{MI}} = \\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\big[ \\log p(\\vy|\\vx) + \\log p(\\vx|\\vy) \\big].\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\small\n    R(f)\\leq R_n(f) +\\mathcal{O}\\bigg( \\big( \\frac{\\text{VCD}(\\mathcal{F})-\\log\\delta}{n} \\big)^\\beta \\bigg)\n\\end{equation}",
            "eq:10": "\\begin{equation}\nh_{\\text{2D}} = \\text{GNN-2D}(T_{\\text{2D}}(g_{\\text{2D}})) = \\text{GNN-2D}(T_{\\text{2D}}(X, E)).\n\\end{equation}",
            "eq:11": "\\begin{equation}\nz_i^{(k+1)} = \\text{MLP}_{\\text{atom}}^{(k+1)} \\Big(z_i^{(k)} + \\sum_{j \\in \\mathcal{N}(i)} \\big( z_j^{(k)} + \\text{MLP}_{\\text{bond}}^{(k+1)}(E_{ij}) \\big) \\Big),\n\\end{equation}",
            "eq:12": "\\begin{equation}\nz(\\vx) = \\frac{1}{N} \\sum_{i} z_i^{(K)}\n\\end{equation}",
            "eq:13": "\\begin{equation}\nh_{\\text{3D}} = \\text{GNN-3D}(T_{\\text{3D}}(g_{\\text{3D}})) = \\text{GNN-3D}(T_{\\text{3D}}(X, R)),\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\begin{aligned}\n& z_i^{(0)} = \\text{embedding} (x_i)\\\\\n& z_i^{(t+1)} = \\text{MLP} \\Big( \\sum_{j=1}^{n} f(x_j^{(t-1)}, r_i, r_j) \\Big)\\\\\n& h_i = \\text{MLP} (z_i^{(K)}),\n\\end{aligned}\n\\end{equation}",
            "eq:15": "\\begin{equation}\nf(x_j, r_i, r_j) = x_j \\cdot e_k(r_i - r_j) = x_j \\cdot \\exp(- \\gamma \\| \\|r_i - r_j\\|_2  - \\mu \\|_2^2)\n\\end{equation}",
            "eq:16": "\\begin{equation} \\label{eq:app:MI}\n\\begin{aligned}\nI(X;Y)\n& = \\mathbb{E}_{p(\\vx,\\vy)} \\big[ \\log \\frac{p(\\vx,\\vy)}{p(\\vx) p(\\vy)} \\big].\n\\end{aligned}\n\\end{equation}",
            "eq:17": "\\begin{equation}\n\\begin{aligned}\nI(X;Y)\n& = \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log \\frac{p(\\vx,\\vy)}{p(\\vx) p(\\vy)} \\Big]\\\\\n& \\ge \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log \\frac{p(\\vx,\\vy)}{\\sqrt{p(\\vx) p(\\vy)}} \\Big]\\\\\n& = \\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log \\frac{(p(\\vx,\\vy))^2}{p(\\vx) p(\\vy)} \\Big]\\\\\n& = \\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log p(\\vx|\\vy) \\Big] + \\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log p(\\vy|\\vx) \\Big]\\\\\n& = -\\frac{1}{2} [H(Y|X) + H(X|Y)].\n\\end{aligned}\n\\end{equation}",
            "eq:18": "\\begin{equation} \\label{eq:app:MI_objective}\n\\begin{aligned}\n\\mathcal{L}_{\\text{MI}} & = \\frac{1}{2} [H(Y|X) + H(X|Y)].\n\\end{aligned}\n\\end{equation}",
            "eq:eq:app:InfoNCE": "\\begin{equation}\\label{eq:app:InfoNCE}\n\\footnotesize{\n\\begin{aligned}\n\\mathcal{L}_{\\text{InfoNCE}} =\n    -\\frac{1}{2} \\mathbb{E} &\\left[\n      \\log \\frac{\\exp(f_{\\vx}(\\vx, \\vy))}{\\exp(f_{\\vx}(\\vx, \\vy)) + \\sum_j \\exp(f_{\\vx}(\\vx^{j},\\vy)})  + \\log \\frac{\\exp(f_{\\vy}(\\vy,\\vx))}{\\exp(f_{\\vy}(\\vy,\\vx)) + \\sum_j \\exp{f_{\\vy}(\\vy^{j},\\vx)}} \\right],\n\\end{aligned}\n}\n\\end{equation}",
            "eq:19": "\\begin{equation}\n\\begin{aligned}\nI(X;Y) - \\log (K)\n& = \\mathbb{E}_{p(\\vx, \\vy)} \\big[\\log \\frac{1}{K} \\frac{p(\\vx, \\vy)}{p(\\vx) p(\\vy)} \\big]\\\\\n& = \\sum_{\\vx^{i},\\vy^{i}} \\big[\\log \\frac{1}{K} \\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})} \\big]\\\\\n& \\ge -\\sum_{\\vx^{i},\\vy^{i}} \\big[\\log \\big( 1 + (K-1) \\frac{p(\\vx^{i}) p(\\vy^{i})}{p(\\vx^{i},\\vy^{i})} \\big)\\big]\\\\\n& = -\\sum_{\\vx^{i},\\vy^{i}} \\big[\\log \\frac{\\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})} + (K-1)}{\\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})}} \\big]\\\\\n& \\approx -\\sum_{\\vx^{i},\\vy^{i}} \\big[ \\log \\frac{\\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})} + (K-1)\\mathbb{E}_{\\vx^{j} \\ne \\vx^{i}}\\frac{p(x^{j},\\vy^{i})}{p(\\vx^{j}) p(\\vy^{i})} }{\\frac{p(\\vx^{i},\\vy^{i})}{p(\\vx^{i}) p(\\vy^{i})}} \\big] \\quad \\text{// \\textcircled{1}}\\\\\n& = \\sum_{\\vx^{i},\\vy^{i}} \\big[ \\log \\frac{\\exp(f_\\vx(\\vx^{i},\\vy^{i}))}{\\exp(f_\\vx(\\vx^{i},\\vy^{i})) + \\sum_{j=1}^K f_\\vx(\\vx^{j},\\vy^{i})} \\big],\n\\end{aligned}\n\\end{equation}",
            "eq:20": "\\begin{equation}\n\\begin{aligned}\nI(X;Y) - \\log(K) \\ge \\sum_{\\vy^{i},\\vx^{i}} \\big[ \\log \\frac{\\exp(f_\\vy(\\vy^{i},\\vx^{i}))}{\\exp f_\\vy(\\vy^{i},\\vx^{i}) + \\sum_{j=1}^K \\exp (f_\\vy(\\vy^{j},\\vx^{i}))} \\big].\n\\end{aligned}\n\\end{equation}",
            "eq:21": "\\begin{equation} \\label{eq:app:EBM_original}\np(\\vx) = \\frac{\\exp(-E(\\vx))}{A},\n\\end{equation}",
            "eq:22": "\\begin{equation} \\label{eq:app:EBM}\n\\begin{aligned}\n\\mathcal{L}_{\\text{EBM}} = -\\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} \\Big[ \\log \\frac{\\exp(f_\\vx(\\vx, \\vy))}{A_{\\vx|\\vy}} + \\log \\frac{\\exp(f_\\vy(\\vy, \\vx))}{A_{\\vy|\\vx}} \\Big],\n\\end{aligned}\n\\end{equation}",
            "eq:23": "\\begin{equation} \\label{eq:app:EBM_NCE}\n\\begin{aligned}\n\\mathcal{L}_{\\text{EBM-NCE}}\n= & -\\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(y)} \\Big[ \\mathbb{E}_{p_n(\\vx|\\vy)} [\\log \\big(1-\\sigma(f_\\vx(\\vx, \\vy))\\big)] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy)} [\\log \\sigma(f_\\vx(\\vx, \\vy))] \\Big] \\\\\n& ~~ - \\frac{1}{2}  \\mathbb{E}_{p_{\\text{data}}(x)} \\Big[ \\mathbb{E}_{p_n(\\vy|\\vx)} [\\log \\big(1-\\sigma(f_\\vy(\\vy, \\vx))\\big)] + \\mathbb{E}_{p_{\\text{data}}(\\vy|\\vx)} [\\log \\sigma(f_\\vy(\\vy, \\vx))] \\Big].\n\\end{aligned}\n\\end{equation}",
            "eq:24": "\\begin{equation} \\label{eq:app:EBM_SSL}\np_\\theta(\\vx|\\vy) = \\frac{\\exp(-E(\\vx|\\vy))}{ \\int \\exp(-E({\\tilde \\vx}|\\vy)) d{\\tilde \\vx}} = \\frac{\\exp(f_\\vx(\\vx, \\vy))}{\\int \\exp(f_\\vx({\\tilde \\vx}|\\vy)) d{\\tilde \\vx}} = \\frac{\\exp(f_\\vx(\\vx, \\vy))}{A_{\\vx|\\vy}}.\n\\end{equation}",
            "eq:25": "\\begin{equation*}\n    p_{n,\\text{\\text{data}}}(\\vx|\\vy) = p(\\vz=1) p_{\\text{data}}(\\vx|\\vy) + p(\\vz=0) p_n(\\vx|\\vy)\n\\end{equation*}",
            "eq:26": "\\begin{equation*}\np_{n,\\text{\\text{data}}}(\\vz=0|\\vx,\\vy) = \\frac{p(\\vz=0) p_n(\\vx|\\vy)}{p(z=0) p_n(\\vx|\\vy) + p(\\vz=1) p_{\\text{data}}(\\vx|\\vy)} = \\frac{\\nu \\cdot p_n(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\text{data}}(\\vx|\\vy)},\n\\end{equation*}",
            "eq:27": "\\begin{equation*}\np_{n, \\theta}(\\vx) = p(z=0) p_n(\\vx|\\vy) + p(z=1) p_{\\theta}(\\vx|\\vy)\n\\end{equation*}",
            "eq:28": "\\begin{equation*}\np_{n,\\theta}(\\vz=0|\\vx,\\vy) = \\frac{p(\\vz=0) p_n(\\vx|\\vy)}{p(\\vz=0) p_n(\\vx|\\vy) + p(\\vz=1) p_{\\theta}(\\vx|\\vy)} = \\frac{\\nu \\cdot p_n(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)}\n\\end{equation*}",
            "eq:29": "\\begin{equation} \\label{eq:app:EBM_01}\n\\begin{aligned}\n& \\min_\\theta  D_{\\text{KL}}(p_{n,\\text{\\text{data}}}(\\vz|\\vx,\\vy) || p_{n,\\theta}(\\vz|\\vx,\\vy)) \\\\\n& =  \\mathbb{E}_{p_{n,\\text{\\text{data}}}(\\vx,\\vz|\\vy)} [\\log p_{n,\\theta}(\\vz|\\vx,\\vy)] \\\\\n& =   \\int \\sum_\\vz p_{n,\\text{\\text{data}}}(\\vx,\\vz|\\vy) \\cdot \\log p_{n,\\theta}(\\vz|\\vx,\\vy) d \\vx\\\\\n& =  \\int \\Big\\{ p(\\vz=0) p_{n,\\text{\\text{data}}}(\\vx|\\vy,\\vz=0) \\log p_{n,\\theta}(\\vz=0|\\vx,\\vy) \\\\\n & \\quad\\quad\\quad\\quad + p(\\vz=1) p_{n,\\text{\\text{data}}}(\\vx|\\vz=1,\\vy) \\log p_{n,\\theta}(\\vz=1|\\vx,\\vy) \\Big\\} d\\vx \\\\\n& =  \\nu \\cdot \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[\\log p_{n,\\theta}(\\vz=0|\\vx,\\vy) \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[\\log p_{n,\\theta}(\\vz=1|\\vx,\\vy) \\Big] \\\\\n& =  \\nu \\cdot \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{\\nu \\cdot p_n(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{p_\\theta(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)} \\Big].\\\\\n\\end{aligned}\n\\end{equation}",
            "eq:30": "\\begin{equation} \\label{eq:app:EBM_02}\n\\begin{aligned}\n\\nu \\cdot \\mathbb{E}_{p_{\\text{data}}(\\vy)} \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{\\nu \\cdot p_n(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vy)} \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{p_\\theta(\\vx|\\vy)}{\\nu \\cdot p_n(\\vx|\\vy) + p_{\\theta}(\\vx|\\vy)} \\Big].\n\\end{aligned}\n\\end{equation}",
            "eq:31": "\\begin{equation}\n\\begin{aligned}\n% \\widehat{\\mathcal{L}}_{\\text{EBM,NCE}}' \n& \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{ p_n(\\vx|\\vy)}{ p_n(\\vx|\\vy) + \\tilde p_{\\theta}(\\vx|\\vy)} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{\\tilde p_\\theta(\\vx|\\vy)}{ p_n(\\vx|\\vy) + \\tilde p_{\\theta}(\\vx|\\vy)} \\Big]\\\\\n= &   \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{1}{1 + p_{\\theta}(\\vx|\\vy)} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{p_\\theta(\\vx|\\vy)}{1 + p_{\\theta}(\\vx|\\vy)} \\Big]\\\\\n= &   \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\frac{\\exp (-  f_\\vx(\\vx, \\vy))}{\\exp (-  f_\\vx(\\vx, \\vy)) + 1} \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\frac{1}{\\exp (-  f_\\vx(\\vx, \\vy)) + 1} \\Big]\\\\\n= &   \\mathbb{E}_{p_{n}(\\vx|\\vy)} \\Big[ \\log \\big(1-\\sigma(  f_\\vx(\\vx, \\vy))\\big) \\Big] + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\Big[ \\log \\sigma(  f_\\vx(\\vx, \\vy)) \\Big].\n\\end{aligned}\n\\end{equation}",
            "eq:32": "\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}_{\\text{EBM-NCE}}\n& = -\\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(\\vy)} \\Big[\\mathbb{E}_{p_{n}(\\vx|\\vy)} \\log \\big(1-\\sigma(  f_\\vx(\\vx, \\vy))\\big) + \\mathbb{E}_{p_{\\text{data}}(\\vx|\\vy )} \\log \\sigma(  f_\\vx(\\vx, \\vy)) \\Big]\\\\\n& ~~~~ - \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(\\vx)} \\Big[\\mathbb{E}_{p_{n}(\\vy|\\vx)} \\log \\big(1-\\sigma(  f_\\vy(\\vy,\\vx))\\big) + \\mathbb{E}_{p_{\\text{data}}(\\vy,\\vx)} \\log \\sigma(  f_\\vy(\\vy,\\vx)) \\Big].\n\\end{aligned}\n\\end{equation}",
            "eq:33": "\\begin{equation*}\n\\begin{aligned}\n\\mathcal{L}_{\\text{MI}} & = -\\frac{1}{2} \\mathbb{E}_{p(\\vx,\\vy)} [\\log p(\\vx|\\vy) + \\log p(\\vy|\\vx)]\n\\end{aligned}\n\\end{equation*}",
            "eq:34": "\\begin{equation} \\label{eq:app:log_likelihood_01}\n\\begin{aligned}\n\\log p(\\vy|\\vx)\n \\ge \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\log p(\\vy|\\vz_\\vx) \\big] - KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)). \n\\end{aligned}\n\\end{equation}",
            "eq:35": "\\begin{equation} \\label{eq:app:log_likelihood_02}\n\\log p(\\vx|\\vy) \\ge \\mathbb{E}_{q(\\vz_\\vy|\\vy)} \\big[ \\log p(\\vx|\\vz_\\vy) \\big] - KL(q(\\vz_\\vy|\\vy) || p(\\vz_\\vy)),\n\\end{equation}",
            "eq:36": "\\begin{equation*}\n\\begin{aligned}\n\\mathbb{E}_{q(\\vz_\\vx|\\vx)}[\\log p(\\vy|\\vz_\\vx)]\n& = - \\mathbb{E}_{q(\\vz_\\vx|\\vx)}[ \\| h_{\\vy}(g_x(\\vz_\\vx)) - h_{\\vy}(\\vy) \\|_2^2 ] + C,\n\\end{aligned}\n\\end{equation*}",
            "eq:37": "\\begin{equation*}\n\\begin{aligned}\n\\mathbb{E}_{q(\\vz_\\vx|\\vx)}[\\log p(\\vy|\\vz_\\vx)]\n& = - \\mathbb{E}_{q(\\vz_\\vx|\\vx)}[ \\| q_x(\\vz_\\vx) - h_{\\vy}(\\vy) \\|_2^2 ] + C.\n\\end{aligned}\n\\end{equation*}",
            "eq:38": "\\begin{equation}\n\\begin{aligned}\n\\log p(\\vy|\\vx)\n& \\ge \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\log p(\\vy|\\vz_\\vx) \\big] - \\beta \\cdot KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)).\n\\end{aligned}\n\\end{equation}",
            "eq:39": "\\begin{equation}\n\\mathbb{E}_{q(\\vz_\\vx|\\vx)}[\\log p(\\vy|\\vz_\\vx)] = - \\mathbb{E}_{q(\\vz_\\vx|\\vx)}[ \\| q_x(\\vz_\\vx) - \\text{SG}(h_{\\vy}(\\vy)) \\|_2^2 ] + C.\n\\end{equation}",
            "eq:40": "\\begin{equation} \\label{eq:app:final_vrr}\n\\begin{aligned}\n\\mathcal{L}_{\\text{VRR}}\n& = \\frac{1}{2} \\Big[ \\mathbb{E}_{q(\\vz_\\vx|\\vx)} \\big[ \\| q_x(\\vz_\\vx) - \\text{SG}(h_\\vy) \\|^2 \\big] + \\mathbb{E}_{q(\\vz_\\vy|\\vy)} \\big[ \\| q_y(\\vz_\\vy) - \\text{SG}(h_\\vx) \\|_2^2 \\big] \\Big]\\\\\n& \\quad\\quad + \\frac{\\beta}{2} \\cdot \\Big[ KL(q(\\vz_\\vx|\\vx) || p(\\vz_\\vx)) + KL(q(\\vz_\\vy|\\vy) || p(\\vz_\\vy)) \\Big].\n\\end{aligned}\n\\end{equation}"
        },
        "git_link": "https://github.com/chao1224/GraphMVP"
    }
}