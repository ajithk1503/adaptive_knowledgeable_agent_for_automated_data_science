{
    "meta_info": {
        "title": "Causal Attention for Interpretable and Generalizable Graph  Classification",
        "abstract": "In graph classification, attention and pooling-based graph neural networks\n(GNNs) prevail to extract the critical features from the input graph and\nsupport the prediction. They mostly follow the paradigm of learning to attend,\nwhich maximizes the mutual information between the attended graph and the\nground-truth label. However, this paradigm makes GNN classifiers recklessly\nabsorb all the statistical correlations between input features and labels in\nthe training data, without distinguishing the causal and noncausal effects of\nfeatures. Instead of underscoring the causal features, the attended graphs are\nprone to visit the noncausal features as the shortcut to predictions. Such\nshortcut features might easily change outside the training distribution,\nthereby making the GNN classifiers suffer from poor generalization. In this\nwork, we take a causal look at the GNN modeling for graph classification. With\nour causal assumption, the shortcut feature serves as a confounder between the\ncausal feature and prediction. It tricks the classifier to learn spurious\ncorrelations that facilitate the prediction in in-distribution (ID) test\nevaluation, while causing the performance drop in out-of-distribution (OOD)\ntest data. To endow the classifier with better interpretation and\ngeneralization, we propose the Causal Attention Learning (CAL) strategy, which\ndiscovers the causal patterns and mitigates the confounding effect of\nshortcuts. Specifically, we employ attention modules to estimate the causal and\nshortcut features of the input graph. We then parameterize the backdoor\nadjustment of causal theory -- combine each causal feature with various\nshortcut features. It encourages the stable relationships between the causal\nestimation and prediction, regardless of the changes in shortcut parts and\ndistributions. Extensive experiments on synthetic and real-world datasets\ndemonstrate the effectiveness of CAL.",
        "author": "Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, Tat-Seng Chua",
        "link": "http://arxiv.org/abs/2112.15089v2",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "Accepted to KDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n% TODO: 1. background of gnns; it is crucial to attend the critical part of input graph, while filtering the trivial part; for explainability, generalization, robustness.\nGraph neural networks (GNNs) \\cite{dwivedi2020benchmarking, kipf2016semi} have exhibited impressive performance of graph classification across various domains, such as chemical molecules, social networks, and transaction graphs.\nSuch a success mainly comes from the powerful representation learning of GNNs, which incorporates the graph structure and encodes them into the representations in an end-to-end way.\nHence, it is crucial to emphasize the critical part of the input graph, while filtering the trivial part out \\cite{lin2021generative, DIR, RC-Explainer,ReFine}.\nFor example, when classifying the mutagenic property of a molecular graph \\cite{morris2020tudataset}, GNNs are expected to latch on the functional groups (\\ie nitrogen dioxide (NO$_{2}$)), instead of the irrelevant patterns (\\ie carbon rings) \\cite{debnath1991structure,XGNN};\nwhen detecting fraud in a transaction network, malicious behaviors or coalitions of users are more informative than benign features.\n\n\n% TODO: 2. however, most of gnns adopt the conventional paradigm of supervised learning: minimizing the empirical risks between the prediction, and the ground-truth labels; attention- & pooling gnns make a further step: maximize the mutual information between the attentive or selective subgraphs and the ground-truth labels.\n\n% The current training paradigm of GNNs struggles to \\wxx{identify} the critical features from the input graphs.\n% Specifically, most of GNNs \\cite{zhou2018graph,kipf2016semi,hamilton2017inductive,xu2018how} follow a conventional paradigm of supervised learning  --- minimizing the loss between the prediction on the full input graph and the ground-truth label.\n% Despite great success, it makes the GNNs obscure as a black box, failing to exhibit what knowledge is exploited to make predictions.\n\\syd{Towards specifying the critical parts in graphs, some follow-on studies \\cite{velivckovic2018graph,kim2020find,gao2019graph,ying2018hierarchical} adopt the paradigm of ``learning to attend'' \\cite{DBLP:conf/icml/XuBKCCSZB15,DBLP:conf/nips/VaswaniSPUJGKP17} ---\n% minimizing the loss between the prediction on the attended subgraph and the ground-truth label.\nmaximizing the mutual information between the attended graph and the ground-truth label --- to find the attended graph that maximizes the predictive performance.\nSpecifically, there are two research lines in this paradigm:\n(1) Attention-based methods \\cite{velivckovic2018graph,kim2020find,brody2021attentive,thekumparampil2018attention,li2015gated}.\nThey often utilize the attention modules for nodes or edges to locate the attended graphs.\nThese attention modules act like soft masks to identify the importance of each edge and node to the final representations and predictions.\n% to locate the attended graphs, they often generate soft attention-masks for the edges or nodes.\n% which identify their contributions to the representations and predictions.\n(2) Pooling-based methods \\cite{lee2019self,gao2019graph,ying2018hierarchical,zhang2018end}.\nThey directly adopt hard masks to select a subset of nodes or edges as the attended graphs, to perform the information propagations.\nThese attended graphs aim to approach the features that are beneficial for minimizing the training loss, instead of distinguishing the causal and noncausal effects.}\n% Clearly, these attended graphs are viewed as the knowledge memorized in GNNs to support the predictions.\n% (1) For the input graph, attention-based methods \\cite{velivckovic2018graph,kim2020find,brody2021attentive,thekumparampil2018attention,li2015gated} learn the attention scores for the edges or nodes, which identify their contributions to the representations and predictions.\n% (2) Pooling-based methods \\cite{lee2019self,gao2019graph,ying2018hierarchical,zhang2018end} directly select some local parts of edges or nodes to perform the information propagation, while completely ignoring the rest.\n% Clearly, the attentive graphs or selective subgraphs are viewed as the knowledge memorized in GNNs to make predictions.\n\n% \\syd{Clearly, these methods always adopt the most distinguishing subgraphs to generate GNN representations and make predictions.}\n% Clearly, the attentive or selective subgraphs explicitly present what knowledge is used to generate GNN representations and make predictions.\n\n\n\n% TODO: 3. despite of success, limitations: they are founded on an infeasible assumption --- the training & testing data are IID; when OOD test, some trivial subgraphs easily function as the shortcuts, offer unstable & spurious correlations between trivial subgraph and ground-truth label.\n\n\\syd{Unfortunately, recent efforts \\cite{geirhos2018imagenet,geirhos2020shortcut,arjovsky2019invariant,knyazev2019understanding} have shown that the current attention or pooling learning methods are prone to exploit the shortcut features to make decisions.}\nThese shortcuts usually come from the data selection biases, noisy features, or some trivial patterns from graphs, which are noncausal but discriminative in training data.\nDue to the existence of these shortcuts, models can capture shortcut features to finish the classification tasks without struggling to learn causal features.\n% However, the current attention and pooling methods are prone to emphasize spurious correlations between the trivial patterns and the label, but ignore the \\syd{causal information}.\nFor example, instead of probing into the causal effect of the functional groups, the attended graphs prefer ``carbon rings'' as the cues of the ``mutagenic'' class, because most training ``mutagenic'' molecules are in the ``carbon rings'' context.\nWhile such correlations represent statistical relations inherent in the training data and are beneficial to the in-distribution (ID) test evaluations, they inevitably cause a huge performance drop in the out-of-distribution (OOD) test data \\wxx{that are at odds with the training distribution}.\nTaking the molecule classification as an example again, when most test ``non-mutagenic'' molecules appear in the ``carbon rings'' context, the attended graphs mislead the GNNs to still predict ``mutagenic''.\nAs the assumption that the test data conforms to the training distribution is often infeasible in real-world scenarios, the poor generalization of these methods hinders their deployment on critical applications.\n\n% TODO: 4. \n% change the prediction to label ?\nTo resolve this issue, we first take a causal look at the decision-making process of GNNs for graph classification, which delineates the relationships among the causal feature, shortcut feature, and prediction.\n\\wxx{With our causal assumption in Figure \\ref{fig:causal},} the shortcut feature serves as a confounder \\cite{pearl2000models}. \nIt opens a backdoor path \\cite{pearl2014interpretation} and makes the causal feature and prediction spuriously correlated, \\eg misclassifying ``non-mutagenic'' molecules with ``carbon rings'' to the ``mutagenic'' molecules.\nHence, mitigating the confounding effect is promising to exploit the causal features while filtering out the shortcut patterns, thereby enhancing the generalization.\n% for GNNs models.\n% We argue that shortcut feature serves as a confounder \\cite{pearl2000models}, which opens a backdoor path from causal feature to the prediction, making them spuriously correlated, \\eg misclassifying a “non-mutagenic” molecules with “carbon rings” to the \"mutagenic\" molecules.\n% Hence, mitigating the confounding effect is promising to purify the causal relationship, thereby enhancing the generalization for GNNs.\n\n% Towards this end, we propose a new \\underline{p}aradigm of \\underline{d}econfounded \\underline{t}raining (DTP) --- \n% maximizing the causal effect of the attended subgraph on predicting the label, while reducing the confounding effect of the complement subgraph.\nTowards this end, we propose the \\underline{C}ausal \\underline{A}ttention \\underline{L}earning (CAL) strategy  --- \nmaximizing the causal effect of the attended graph on predicting the label, while reducing the confounding effect of the shortcut features.\nOur attended graph aims to approach the causal features in the graph (\\eg nitrogen dioxide), while its complement targets the shortcut features (\\eg carbon rings).\n% maximizing the mutual information between the attended subgraph and the ground-truth label, meanwhile reducing the confounding effect of the non-attended subgraph on the prediction.\n% Specifically, we first hire the attention modules to split the input graph into two complementary parts: causal attended-graph and trivial attended-graph.\nSpecifically, we first apply attention modules to generate the estimations of the causal and shortcut features from the input graphs.\nWe then parameterize the backdoor adjustment in the causal theory \\cite{pearl2000models,pearl2014interpretation}, which combines each causal estimation with various shortcut estimations and encourages these combinations to maintain a stable prediction.\nIt encourages the invariant relationships between the causal patterns and the predictions, regardless of the changes in the shortcut parts and distribution shifts.\nWe apply CAL to various GNN architectures for graph classification.\nExperimental results on numerous synthetic and real-world datasets demonstrate the better generalization and insightful interpretations of CAL. \n% It pushes the model to find stable invariant feature\n% and obtain their representations by a GNN module.\n% To keep the prediction invariant to the variant distribution caused by the shortcut feature,\n% Then we make each causal attended-graph combined with diverse trivial attended-graphs, and we hope the prediction of the combined attended-graph always keeps consistent across different types of trivial patterns. \n% The proposed CAL actually applies the backdoor adjustment \\cite{pearl2000models} to mitigate the confounder.\n% It enables GNN models to identify causal features that are stable and invariant across different distributions, thereby establishing better OOD generalization.\n\nOur technical contributions are summarized as:\n\\begin{itemize}[leftmargin=*]\n    \\item We emphasize the generalization issue of current attention- and pooling-based GNNs in graph classification. \n    From the causal perspective, we ascribe such an issue to the confounding effect of the shortcut features.\n    % which are usually caused by the data biases\n    \\item \\syd{We present a novel Causal Attention Learning (CAL) strategy for graph classification.\n    It makes GNNs exploit the causal features while filtering out the shortcut patterns.}\n    % \\syd{It can better distinguishes the causal feature even from the biased training data.}\n    \n    % \\item To mitigate the confounding effect, we present a deconfounded training paradigm, DTP, which better distinguishes the critical \\syd{information} from the trivial \\syd{information}.\n    % \\syd{and establishes stable performance on out-of-distribution issue}\n\n    \\item Extensive experiments on synthetic and real-world datasets justify the effectiveness of CAL.\n    More visualizations with in-depth analyses demonstrate the interpretability and rationality of CAL.\n    % Extensive qualitative and quantitative experiments justify the effectiveness of DTP.\n    \n\\end{itemize}\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n",
                "subsection 2.1": {
                    "name": "Notations",
                    "content": "\nWe denote a graph by $\\mathcal{G} = \\{\\Mat{A}, \\Mat{X}\\}$ with the node set $\\mathcal{V}$ and edge set $\\mathcal{E}$.\nLet $\\Mat{X} \\in \\mathbb{R}^{|\\mathcal{V}| \\times \\mathrm{F}}$ be the node feature matrix, where $\\Mat{x}_i = \\Mat{X}[i,:]$ is the $\\mathrm{F}$-dimensional attribute vector of node $v_i\\in\\mathcal{V}$. \n% $v_i\\in\\mathcal{V}= \\{v_1, ..., v_{|\\mathcal{V}|}\\}$. \n% As for $\\mathcal{E} = \\{e_1, ..., e_{\\mathcal{|E|}}\\}$, $e_n=(v_i, v_j)\\in\\mathcal{E}$ \\wx{indicates the existence of the edge from node $v_i$ to node $v_j$.}\n% that there exists an edge between node $v_i$ and $v_j$. \nWe \\wx{use} the adjacency matrix $\\Mat{A} \\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{V}| }$ to \\wx{delineate} the whole graph structure, where $\\Mat{A}[i,j]=1$ if edge $(v_i, v_j) \\in \\mathcal{E}$\\wx{, otherwise} $\\Mat{A}[i,j]=0$.\nWe define $\\mathrm{GConv}(\\cdot)$ as a GNN layer module and \\wxx{denote} the node representation matrix \\wxx{by} $\\Mat{H}\\in\\Space{R}^{|\\Set{V}|\\times d}$, whose $i$-th row $\\Mat{h}_{i}=\\Mat{H}[i,:]$ denotes the representation of node $v_i$.\n% \\wx{We denote a graph by $\\mathcal{G} = \\{\\mathcal{V}, \\mathcal{E}\\}$ with the node set $\\mathcal{V}$ and edge set $\\mathcal{E}$.}\n% Let $\\Mat{X} \\in \\mathbb{R}^{|\\mathcal{V}| \\times \\mathrm{F}}$ be the node feature matrix, where $\\Mat{x}_i = \\Mat{X}[i,:]$ is the $\\mathrm{F}$-dimensional attribute vector of node $v_i\\in\\mathcal{V}$. \n% % $v_i\\in\\mathcal{V}= \\{v_1, ..., v_{|\\mathcal{V}|}\\}$. \n% % As for $\\mathcal{E} = \\{e_1, ..., e_{\\mathcal{|E|}}\\}$, $e_n=(v_i, v_j)\\in\\mathcal{E}$ \\wx{indicates the existence of the edge from node $v_i$ to node $v_j$.}\n% % that there exists an edge between node $v_i$ and $v_j$. \n% We \\wx{use} the adjacency matrix $\\Mat{A} \\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{V}| }$ to \\wx{delineate} the whole graph structure, where $\\Mat{A}[i,j]=1$ if $(v_i, v_j) \\in \\mathcal{E}$\\wx{, otherwise} $\\Mat{A}[i,j]=0$.\n% We define the node representation matrix $\\Mat{H}\\in\\Space{R}^{|\\Set{V}|\\times d}$, whose $i$-th row $\\Mat{h}_{i}=\\Mat{H}[i,:]$ denotes the representation of node $v_i$.\n% they compute a weighted average of the transformed features of the neighbor nodes (followed by a nonlinearity $\\sigma$) as the new representation $\\Mat{h}_i'$ of node $v_i$:\n\n% they compute a weighted average of the transformed features of the neighbor nodes (followed by a nonlinearity $\\sigma$) as the new representation $\\Mat{h}_i'$ of node $v_i$:\n% \\begin{equation}\n% \\Mat{h}_i' = \\sigma\\left(\\sum_{j\\in\\mathcal{N}(i) }\\alpha_{ij}\\cdot\\Mat{W}\\Mat{h}_j\\right)\n% \\label{equ:gnn}\n% \\end{equation}\n\n"
                },
                "subsection 2.2": {
                    "name": "Attention Mechanism in GNNs",
                    "content": "\nIn GNNs, attention can be defined over edges or nodes.\nFor edge-level attentions \\cite{velivckovic2018graph,brody2021attentive,kim2020find,thekumparampil2018attention,lee2019graph}, they utilize weighted message passing and aggregation to update node representations $\\Mat{H}'$:\n\\begin{equation}\n\\Mat{H}' = \\mathrm{GConv}\\left(\\Mat{A} \\odot \\Mat{M}_a, \\Mat{H}\\right)\n\\label{equ:gnn1}\n\\end{equation}\nwhere $\\Mat{M}_a\\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$ denotes the attention matrix that \\wxx{is often derived} from trainable parameters and node representations.\nFor node-level attention, several studies \\cite{li2015gated,knyazev2019understanding,lee2019self} define the self-attention mask to select the most attentive node representations:\n\\begin{equation}\n\\Mat{H}' = \\mathrm{GConv}\\left(\\Mat{A}, \\Mat{H} \\odot \\Mat{M}_x\\right)\n\\label{equ:gnn2}\n\\end{equation}\nwhere $\\Mat{M}_x\\in\\Space{R}^{|\\Set{V}|\\times 1}$ represents the node-level attentions, which can be generated by a network (\\eg GNNs or MLPs); $\\odot$ is the broadcasted element-wise product. \nHereafter, we can make further pooling operation \\cite{lee2019self} for the output node representations $\\Mat{H}^{out}$ and summarize the graph representation $\\Mat{h}_\\mathcal{G}$ for graph $\\mathcal{G}$ via the readout function $f_\\mathrm{readout}(\\cdot)$.\nThen we use a classifier $\\Phi$ to project the graph representation into a probability distribution $\\Mat{z}_\\mathcal{G}$:\n\\begin{equation}\n\\Mat{h}_\\mathcal{G} = f_\\mathrm{readout}\\left(\\{\\Mat{h}_{i}^{out}| i \\in \\mathcal{V} \\}\\right), \\quad z_\\mathcal{G} = \\Phi(\\Mat{h}_\\mathcal{G}).\n\\end{equation}\nThese methods follow the paradigm of \"learning to attend\" by minimizing the following empirical risk:\n% Considering the graph classification task for the dataset $\\mathcal{D}$ , the supervised objective function is:\n\\begin{equation}\n    \\mathcal{L}_{\\mathrm{CE}} = -\\frac{1}{|\\mathcal{D}|} \\sum_{\\mathcal{G}\\in\\mathcal{D}}\\Trans{\\Mat{y}}_\\mathcal{G}\\mathrm{log}(\\Mat{z}_\\mathcal{G}).\n\\end{equation}\nwhere $\\mathcal{L}_{\\mathrm{CE}}$ is the cross-entropy loss over the training data $\\mathcal{D}$, and $\\Mat{y}_{\\mathcal{G}}$ is the ground-truth label vector of $\\mathcal{G}$.\nHowever, this learning strategy heavily relies on the statistical correlations between the input graphs and labels. Hence, they will inevitably capture the noncausal shortcut features to make predictions."
                }
            },
            "section 3": {
                "name": "Methodology",
                "content": "\n\nIn this section, we first analyze the GNN learning from the perspective of causality.\nFrom our causal assumption, we identify the shortcut feature as a confounder.\nThen we propose the causal attention learning strategy to alleviate the confounding effect.\n\n",
                "subsection 3.1": {
                    "name": "A Causal View on GNNs",
                    "content": "\n\n\n\nWe take a causal look at the GNN modeling and construct a \\wxx{Structural Causal Model (SCM)} \\cite{pearl2000models} in Figure \\ref{fig:causal}.\nIt presents the causalities among five variables: graph data $G$, causal feature $C$, shortcut feature $S$, graph representation $R$, and prediction $Y$, where the link from one variable to another indicates the cause-effect relationship: cause $\\rightarrow$ effect. We list the following explanations for SCM:\n%=====wx=======\n% \\wx{Before learning critical graphs, we take a causal look at the GNN modeling and construct a Structural Causal Model (SCM) \\cite{pearl2000models} as Figure \\ref{fig:causal} shows.\n% The SCM presents the causalities among four variables: critical information $C$, trivial information $T$, graph representation $R$, and graph label $Y$, where the link from one variable to another variable indicates the cause-effect relationship: cause $\\rightarrow$ effect:}\n%=====wx=======\n% We first construct a Structural Causal Model (SCM) \\cite{pearl2000models} based on the direct causality between graph data $G$, trivial subgraph $T$, graph representation $R$, and graph label $Y$. As illustrated in Figure \\ref{fig:causal} (Left), the one-way arrow between two variables indicates causality: cause $\\rightarrow$ effect.\n% We will introduce in detail the causal relationship between variables in this SCM.\n% meaningless\n\\begin{itemize}[leftmargin=*]\n    \\item $\\boldsymbol{C\\leftarrow G \\rightarrow S}$.\n    The variable $C$ denotes the causal feature that truly reflects the intrinsic property of the graph data $G$. \n    While $S$ represents the shortcut feature which is usually caused by the data biases or trivial patterns.\n    Since $C$ and $S$ naturally coexist in graph data $G$, these causal effects are established.\n    \n    \\item $\\boldsymbol{C \\rightarrow R \\leftarrow S}$.\n    The variable $R$ is the representation of the given graph data $G$.\n    To generate $R$, the conventional learning strategy \\wxx{takes the shortcut feature $S$ and the causal feature $C$ as input to distill discriminative information.}\n    % Due to the message-passing scheme of GNNs, causal feature $C$ and shortcut feature $S$ will inevitably merge together. \n    % They both make contributions to generate the graph representation $R$.\n    \n    \\item $\\boldsymbol{R \\rightarrow Y}$.\n    The ultimate goal of graph representation learning is to predict the properties of the input graphs.\n    The classifier will make prediction $Y$ \\wxx{based on} the graph representation $R$.\n    % Therefore, the causal relationship $R \\rightarrow Y$ is obvious.\n\\end{itemize}    \n    \nScrutinizing this SCM, we recognize a backdoor path between $C$ and $Y$, \\ie $C \\leftarrow G \\rightarrow S \\rightarrow R \\rightarrow Y$, wherein the shortcut feature $S$ \\wxx{plays a confounder} role between $C$ and $Y$. Even if $C$ has no direct link to $Y$, the backdoor path will cause $C$ to establish a spurious correlation with $Y$, \\eg making wrong predictions based on shortcut feature $S$ instead of causal feature $C$.\nHence, it is crucial to cut off the backdoor path and make the GNN exploit causal features.\n\n"
                },
                "subsection 3.2": {
                    "name": "Backdoor Adjustment",
                    "content": "\nWe have realized that shielding the GNNs from the confounder $S$ is the key to exploiting causal features.\nInstead of modeling the confounded $P(Y|C)$ in Figure \\ref{fig:causal}, we should achieve the graph representation learning by eliminating the backdoor path.\nBut how to achieve this?\nFortunately, causal theory \\cite{pearl2000models, pearl2014interpretation} provides us with a feasible solution:\nwe can exploit the $\\textbf{do}$-\\textbf{calculus} on the variable $C$ to remove the backdoor path by estimating $P_m(Y|C)=P(Y|do(C))$. \nIt needs to stratify the confounder $S$ between $C$ and $Y$. \nTherefore, we can obtain the following three essential conclusions:\n\\begin{itemize}[leftmargin=*]\n    \\item The marginal probability $P(S = s)$ is invariant under the intervention, because the shortcut feature will not be affected by cutting off the backdoor path. Thus, $P(s) = P_m(s)$.\n    \\item The conditional probability $P(Y|C,s)$ is invariant, because $Y$’s response to $C$ and $S$ has nothing to do with the causal effect between $C$ and $S$. Then we can get: $P_m(Y|C, s) = P(Y|C, s)$.\n    \\item Obviously, the variables $C$ and $S$ are independent under the causal intervention, which we have: $P_m(s|C)=P_m(s)$.\n\\end{itemize}\nBased on the above conclusions, we have:\n\\begin{equation}\n\\begin{aligned}\n\\label{equ:backdoor}\n    P(Y|do(C)) & = P_m(Y|C) \\\\\n    & = \\sum\\nolimits_{s\\in\\mathcal{T}}P_m(Y|C, s)P_m(s|C) \\quad (Bayes\\,Rule) \\\\\n    & = \\sum\\nolimits_{s\\in\\mathcal{T}}P_m(Y|C, s)P_m(s) \\quad (Independency)\\\\\n    & = \\sum\\nolimits_{s\\in\\mathcal{T}}P(Y|C, s)P(s),\n\\end{aligned}\n\\end{equation}\nwhere $\\mathcal{T}$ denotes the confounder set; \n$P(Y|C, s)$ represents the conditional probability given the causal feature $C$ and confounder $s$; \n$P(s)$ is the prior probability of the confounder. Equation~\\eqref{equ:backdoor} is usually called \\textbf{backdoor adjustment} \\cite{pearl2014interpretation}, which is a powerful tool to eliminate the confounding effect. \nHowever, there exist two challenges for implementing Equation~\\eqref{equ:backdoor}:\ni) The confounder set $\\mathcal{T}$ is commonly unobservable and hard to obtain.\nii) Due to the discrete nature of graph data, it seems difficult to \\wxx{directly manipulate the graph data, conditioning on domain-specific constraints (\\eg valency rules in molecule graphs).}\n% change a data’s trivial part to generate a new counterfactual sample.\nIn section \\ref{subs:causal}, we will introduce a simple yet effective solution to overcome these issues.\n\n"
                },
                "subsection 3.3": {
                    "name": "Causal and Trivial Attended-graph",
                    "content": "\\label{sec31}\n\nGiven a graph $\\mathcal{G} = \\{\\Mat{A}, \\Mat{X}\\}$, we formulate the soft masks on the graph structure and node feature as $\\Mat{M}_a\\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$ and $\\Mat{M}_x\\in \\mathbb{R}^{|\\mathcal{V}| \\times 1}$, respectively.\n\\wx{Wherein, each element of the masks indicates the attention score relevant to the task of interest, which often falls into the range of $(0,1)$.}\n% And the value range of the soft mask is between $(0,1)$.\nGiven an arbitrary mask $\\Mat{M}$, we define its complementary mask as $\\overline{\\Mat{M}}=\\Mat{1}-\\Mat{M}$, where $\\Mat{1}$ is \\wx{the all-one matrix.}\nTherefore, we can divide the full graph $\\mathcal{G}$ into two attended-graphs: $\\mathcal{G}_1=\\{\\Mat{A}\\odot \\Mat{M}_a, \\Mat{X}\\odot \\Mat{M}_x\\}$ and $\\mathcal{G}_2=\\{\\Mat{A}\\odot \\wx{\\overline{\\Mat{M}}_a, \\Mat{X}\\odot \\overline{\\Mat{M}}_x\\}}$.\n% On the contrary, these two graphs constitute the original graph: $\\mathcal{G}_1+\\mathcal{G}_2=\\mathcal{G}$.\n\nWith the inspection on the data-generating process, recent studies \\cite{DIR, lin2021generative, ying2019gnnexplainer,knyazev2019understanding} argue that the label of a graph is usually determined by its causal part.\nConsidering a molecular graph, its mutagenic property relies on the existence of relevant functional groups \\cite{RC-Explainer};\nTaking the digit image in the form of superpixel graph as another example, the coalition of digit-relevant nodes determines its label.\nFormally, given a graph $\\Set{G}$, we define the attended graph collecting all causal features as the \\textbf{causal attended-graph} $\\mathcal{G}_c$, while the counterpart forms the \\textbf{trivial attended-graph} $\\mathcal{G}_t$.\nHowever, the ground-truth attended-graph is usually unavailable in real-world applications.\nHence, we aim to capture the causal and trivial attended-graph from the full graph by learning the masks: $\\mathcal{G}_c=\\{\\Mat{A}\\odot \\Mat{M}_a, \\Mat{X}\\odot \\Mat{M}_x\\}$ and $\\mathcal{G}_t=\\{\\Mat{A}\\odot \\overline{\\Mat{M}}_a, \\Mat{X}\\odot \\overline{\\Mat{M}}_x\\}$.\nLearning to identify causal attended-graphs not only guides the representation learning of GNNs, but also answers \"What knowledge does the GNN use to make predictions?\", which is crucial to the applications on explainability, privacy, and fairness.\n\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Causal Attention Learning",
                    "content": " \\label{sec34}\n\nTo implement the aforementioned backdoor adjustment, we propose the \\textbf{C}ausal \\textbf{A}ttention \\textbf{L}earning (CAL) framework:\n\n",
                    "subsubsection 3.4.1": {
                        "name": "Estimating soft masks.",
                        "content": "\n% \\vspace{5pt}\n% \\noindent\\textbf{Estimate masks for $\\mathcal{G}_c$ and $\\mathcal{G}_t$.}\nTowards effective causal intervention, it is necessary to separate the causal and shortcut features from the full graphs.\nTo this end, we hire attention modules, which yield two branches for the causal and trivial proposals.\n% Based on the above descriptions, we found that it is necessary to disentangle the trivial subgraphs from the original graphs to effectively apply causal intervention.\nGiven a GNN-based encoder $f(\\cdot)$ and a graph $\\mathcal{G}=\\{\\Mat{A}, \\Mat{X}\\}$, we can obtain the node representations:\n\\begin{equation}\n\\Mat{H}=f(\\Mat{A}, \\Mat{X}).\n\\end{equation}\n% where $\\Mat{H}\\in\\Space{R}^{|\\Set{V}|\\times d}$ stores the $d$-dimensional representations of all nodes, whose $i$-th row $\\Mat{h}_{i}$ denotes the representation of node $v_i$.\nThen we adopt two MLPs: $\\mathrm{MLP_{node}}(\\cdot)$ and $\\mathrm{MLP_{edge}}(\\cdot)$ to estimate the attention scores from two orthogonal perspectives: node-level and edge-level. For node $v_i$ and edge $(v_i, v_j)$ we can obtain:\n\\begin{equation}\n\\alpha_{c_i}, \\alpha_{t_i}=\\sigma(\\mathrm{MLP_{node}}(\\Mat{h}_i)),\n\\label{equ:node_att}\n\\end{equation}\n\\begin{equation}\n\\beta_{c_{ij}}, \\beta_{t_{ij}}=\\sigma(\\mathrm{MLP_{edge}}(\\Mat{h}_i||\\Mat{h}_j)),\n\\label{equ:edge_att}\n\\end{equation}\nwhere $\\sigma(\\cdot)$ is softmax function, $||$ denotes concatenation operation;\n$\\alpha_{c_i}, \\beta_{c_{ij}}$ represent the node-level attention score for node $v_i$ and edge-level attention score for edge  $(v_i, v_j)$ in causal attended-graph;\nanalogously, $\\alpha_{t_i}, \\beta_{t_{ij}}$ are for trivial attended-graph.\nNote that $\\alpha_{c_i} + \\alpha_{t_i}=1$, and $\\beta_{c_{ij}} + \\beta_{t_{ij}}=1$.\nThese attention scores indicate how much the model pays attention to each node or edge in the corresponding attended-graph.\nNow we can construct the soft masks $\\Mat{M}_x$, $\\overline{\\Mat{M}}_x$, $\\Mat{M}_a$, and $\\overline{\\Mat{M}}_a$ based on the attention scores $\\alpha_{c_i}$, $\\alpha_{t_i}$, $\\beta_{c_{ij}}$, and $\\beta_{t_{ij}}$, respectively.\nFinally, we can decompose the original graph $\\mathcal{G}$ into the initial causal and trivial attended-graphs: $\\mathcal{G}_c=\\{\\Mat{A}\\odot \\Mat{M}_a, \\Mat{X}\\odot \\Mat{M}_x\\}$ and $\\mathcal{G}_t=\\{\\Mat{A}\\odot \\overline{\\Mat{M}}_a, \\Mat{X}\\odot \\overline{\\Mat{M}}_x\\}$.\n% Now we can construct the masks $\\Mat{M}_x$, \\wx{$\\overline{\\Mat{M}}_x$} based on the attention scores $\\alpha_{c_i}, \\alpha_{t_i}$ and $\\Mat{M}_a$, $\\overline{\\Mat{M}_a}$ from the attention scores $\\beta_{c_{ij}}, \\beta_{t_{ij}}$, respectively.\n% Please note that the node-level attention score $\\alpha_{c_i}$ or $\\alpha_{t_i}$ is scalar designed for the whole feature vector, so soft-mask $\\Mat{M}_x$ or \\wx{$\\overline{\\Mat{M}}_x$} keeps the same value in each row.\n\n% \\textbf{Disentangle the representation of $\\mathcal{G}_c$ and $\\mathcal{G}_t$.}\n"
                    },
                    "subsubsection 3.4.2": {
                        "name": "Disentanglement.",
                        "content": "\nUntil now, we have distributed the attention scores at the granularity of nodes and edges to create the initial attended-graphs. \nNow we need to make the causal and trivial attended-graphs to capture the causal and shortcut features from the input graphs, respectively.\n% distinguish the representations of the causal/trivial attended-graph and push them to the opposite directions.\n% Now we need to distinguish the representations of the causal/trivial attended-graph and push them to the opposite directions.\nSpecifically, we adopt two GNN layers to obtain the representations of attended-graphs and make predictions via readout function and classifiers:\n% \\begin{equation}\\label{equ:zc}\n% \\Mat{h}_{\\mathcal{G}_c} = f_{\\mathrm{readout}}(\\mathrm{GConv}_c(\\mathcal{G}_c)), \\quad \\Mat{z}_{\\mathcal{G}_c} = \\Phi_{c}(\\Mat{h}_{\\mathcal{G}_{c}}),\n% \\end{equation}\n% \\begin{equation}\\label{equ:zt}\n% \\Mat{h}_{\\mathcal{G}_t} = f_{\\mathrm{readout}}(\\mathrm{GConv}_t(\\mathcal{G}_t)), \\quad \\Mat{z}_{\\mathcal{G}_t} = \\Phi_{t}(\\Mat{h}_{\\mathcal{G}_{t}}).\n% \\end{equation}\n\\begin{equation}\\label{equ:zc}\n\\Mat{h}_{\\mathcal{G}_c} = f_{\\mathrm{readout}}(\\mathrm{GConv}_c(\\Mat{A}\\odot \\Mat{M}_a, \\Mat{X}\\odot \\Mat{M}_x)), \\quad \\Mat{z}_{\\mathcal{G}_c} = \\Phi_{c}(\\Mat{h}_{\\mathcal{G}_{c}}),\n\\end{equation}\n\\begin{equation}\\label{equ:zt}\n\\Mat{h}_{\\mathcal{G}_t} = f_{\\mathrm{readout}}(\\mathrm{GConv}_t(\\Mat{A}\\odot \\overline{\\Mat{M}}_a, \\Mat{X}\\odot \\overline{\\Mat{M}}_x)), \\quad \\Mat{z}_{\\mathcal{G}_t} = \\Phi_{t}(\\Mat{h}_{\\mathcal{G}_{t}}).\n\\end{equation}\n% According to the definition, the causal attended-graph is decisive for classification, so we classify its representation to the ground-truth label. \nThe causal attended-graph aims to estimate the causal features, so we classify its representation to the ground-truth label. \nThus, we define the supervised classification loss as:\n\\begin{equation}\n    \\mathcal{L}_{\\mathrm{sup}} = -\\frac{1}{|\\mathcal{D}|} \\sum_{\\mathcal{G}\\in\\mathcal{D}}\\wx{\\Trans{\\Mat{y}}_\\mathcal{G}}\\mathrm{log}(\\Mat{z}_{\\mathcal{G}_c}).\n    \\label{equ:loss1}\n\\end{equation}\nwhere $\\mathcal{L}_{\\mathrm{sup}}$ is the cross-entropy loss over the training data $\\mathcal{D}$. \nThe trivial attended-graph aims to approach the trivial patterns that are unnecessary for classification. \nHence, we push its prediction evenly to all categories and define the uniform classification loss as:\n\n\\begin{equation}\n    \\mathcal{L}_{\\mathrm{unif}} = \\frac{1}{|\\mathcal{D}|} \\sum_{\\mathcal{G}\\in\\mathcal{D}}\\mathrm{KL}(\\Mat{y}_\\mathrm{unif}, \\Mat{z}_{\\mathcal{G}_t}).\n    \\label{equ:loss2}\n\\end{equation}\nwhere $\\mathrm{KL}$ denotes the KL-Divergence, $\\Mat{y}_\\mathrm{unif}$ represents the uniform distribution.\n% such that the trivial patterns should be predicted to all categories with equal probability.\nBy optimizing the above two objectives, we can effectively disentangle causal and trivial features.\nPlease note that prior efforts \\cite{ReFine,RC-Explainer,lin2021generative, ying2019gnnexplainer,knyazev2019understanding} have shown that the mutual information between the causal part and label is greater than that between the full graph and label, due to the widespread trivial patterns or noise.\nHence, the proposed disentanglement will not make the captured causal attended-graph converge to the full graph (noiseless full graph is a special case), which is not an optimal solution.\nSee Section \\ref{sec45} for more supporting evidence and analyses.\n\n"
                    },
                    "subsubsection 3.4.3": {
                        "name": "Causal intervention",
                        "content": "\\label{subs:causal}\nAs shown in Equation \\eqref{equ:backdoor}, \\wxx{one promising solution to alleviating the confounding effect is the backdoor adjustment} --- that is, stratifying the confounder and pairing the target causal attended-graph with every stratification of trivial attended-graph to compose the ``intervened graphs''.\nHowever, due to the irregular graph data, it is impossible to make the intervention on data-level, \\eg changing a graph's trivial part to generate a counterfactual graph data.\nTowards this end, we make the implicit intervention on representation-level and propose the following loss guided by the backdoor adjustment:\n\\begin{gather}\n    \\Mat{z}_{\\Set{G}'} = \\Phi(\\Mat{h}_{\\Set{G}_{c}} + \\Mat{h}_{\\Set{G}_{t'}}),\n    \\label{equ:inter}\n\\end{gather}\n\\begin{gather}\n    \\Lapl_{\\text{caus}} = -\\frac{1}{|\\Set{D}|\\cdot|\\hat{\\Set{T}}|} \\sum_{\\Set{G}\\in\\Set{D}}\\sum_{t'\\in\\hat{\\Set{T}}}\\Trans{\\Mat{y}}_{\\Set{G}}\\log{(\\Mat{z}_{\\Set{G}'})},\n    \\label{equ:loss3}\n\\end{gather}\nwhere $\\Mat{z}_{\\Set{G}'}$ is the prediction from a classifier $\\Phi$ on ``implicit intervened graph'' $\\Set{G}'$;\n$\\Mat{h}_{\\Set{G}_{c}}$ is the representation of causal attended-graph $\\Set{G}_{c}$ derived from Equation \\eqref{equ:zc};\nwhile $\\Mat{h}_{\\Set{G}_{t'}}$ is the representation of stratification $\\Set{G}_{t'}$ obtained via Equation \\eqref{equ:zt};\n$\\hat{\\Set{T}}$ is the estimated stratification set of the trivial attended-graph, which collects the appearing trivial features from training data.\nIn practice, we apply random addition to make the intervention in Equation \\eqref{equ:inter}.\nWe define the Equation \\eqref{equ:loss3} as the causal intervention loss. It pushes the predictions of such intervened graphs to be invariant and stable across different stratifications, due to the shared causal features.\nFinally, the objective of CAL can be defined as the sum of the losses:\n\\begin{equation}\n    \\mathcal{L}= \\mathcal{L}_{\\mathrm{sup}} + \\lambda_1\\mathcal{L}_{\\mathrm{unif}} + \\lambda_2\\mathcal{L}_{\\mathrm{caus}}\n    \\label{equ:loss_all}\n\\end{equation}\nwhere $\\lambda_1$ and $\\lambda_2$ are hyper-parameters that determine the strength of disentanglement and causal intervention, respectively.\nThe detailed algorithm of CAL is provided in Appendix \\ref{app1}, Alg.\\ref{alg1}, and the overview of CAL is depicted in Figure \\ref{fig:model}.\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\nTo verify the superiority and effectiveness of the proposed CAL, we conduct experiments to answer the following research questions:\n\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{RQ1:} How effective is the proposed CAL in alleviating the out-of-distribution (OOD) issue?\n\\item \\textbf{RQ2:} Can the proposed CAL achieve performance improvements on real-world datasets?\n\\item \\textbf{RQ3:} For the different components in CAL, what are their roles and impacts on performance?\n\\item \\textbf{RQ4:} Does CAL capture the causal attended-graphs with significant patterns and insightful interpretations?\n\\end{itemize}\n\n",
                "subsection 4.1": {
                    "name": "Experimental Settings",
                    "content": "\n% To demonstrate the superiority of CAL over biased datasets\n",
                    "subsubsection 4.1.1": {
                        "name": "Datasets",
                        "content": "\nWe conduct experiments on both synthetic datasets and real-world datasets.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Synthetic graphs:} \n    Following \\cite{ying2019gnnexplainer}, we create the synthetic dataset for graph classification, which contains a total of 8,000 samples with 4 classes, and keeps balance (2,000 samples) for each class.\n    As shown in Figure \\ref{fig:syn_dataset}, each sample consists of two parts: causal subgraph and trivial subgraph. More details about the causal and trivial subgraph are provided in Appendix \\ref{app2}.\n    The task is to predict the type of the causal part in the whole graph.\n    For simplicity, we choose the ``House'' class to define the bias-level:\n    \\begin{equation}\n        b = \\frac{\\# \\mathrm{\\text{Tree-House}}}{\\# \\mathrm{House}}\n        \\label{equ:bias}\n    \\end{equation}\n    where $\\#\\mathrm{\\text{Tree-House}}$ denotes the number of ``House'' causal subgraphs with the ``Tree'' trivial subgraphs, and $\\# \\mathrm{House}$ presents the number of graphs in the ``House'' class, which is 2,000.\n    We set the proportion of ``Tree'' in the other three classes to $1 - b$.\n    Obviously, for the unbiased dataset, $b=0.5$.\n    We abbreviate the synthetic dataset with bias-level $b$ as SYN-$b$.\n    We keep the same bias-level on the training/validation set and keep the testing set unbiased. \n    Please refer to Appendix \\ref{app2} for more details.\n\n    \\item \\textbf{Real-world graphs:} \n    We conduct experiments on three biological datasets (MUTAG, NCI1, PROTEINS), three social datasets (COLLAB, IMDB-B, IMDB-M) \\cite{morris2020tudataset}, and two superpixel datasets (MNIST, CIFAR-10) \\cite{knyazev2019understanding}. More details, such as statistics and splitting of datasets, are provided in Appendix \\ref{app2}.\n\\end{itemize}\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Baselines.",
                        "content": "\nTo verify the superiority of CAL, we adopt the following prevalent graph classification solutions as baselines:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Attention-based methods:} GAT \\cite{velivckovic2018graph}, GATv2 \\cite{brody2021attentive}, SuperGAT \\cite{kim2020find}, GlobalAttention \\cite{li2015gated}, AGNN \\cite{thekumparampil2018attention}. \n    \\item \\textbf{Pooling-based methods:} SortPool \\cite{zhang2018end}, DiffPool \\cite{ying2018hierarchical}, Top-$k$ Pool \\cite{gao2019graph}, SAGPool \\cite{lee2019self}.\n    \\item \\textbf{Kernel-based methods:} Graphlet kernel (GK) \\cite{shervashidze2009efficient}, Weisfeiler Lehman Kernel (WL) \\cite{shervashidze2011weisfeiler}, Deep Graph kernels (DGK) \\cite{yanardag2015deep}.\n    \\item \\textbf{GNN-based methods:} GCN \\cite{kipf2016semi}, GIN \\cite{xu2018how}\n\\end{itemize}\n\\syd{Besides these methods, we also consider the state-of-the-art algorithms: IRM \\cite{arjovsky2019invariant} and DRO \\cite{sagawa2019distributionally}, which are particularly designed for OOD issues. Please note that these methods require specific environments or group annotations for each training example, therefore we consider them as the methods with upper bound performance.}\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Hyper-parameters",
                        "content": " All training hyper-parameters and model configurations are summarized in Appendix \\ref{app3}. Codes are released at \\url{https://github.com/yongduosui/CAL}.\n\n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Performance on Synthetic Graphs (RQ1)",
                    "content": "\n\nTo explore whether CAL can alleviate the OOD issue, we first conduct experiments on SYN-$b$ with different biases: $b \\in \\{0.1, 0.2, ..., 0.9\\}$.\nThe experimental results are summarized in Table \\ref{table:main_syn} and Figure \\ref{fig:syn_norm_results}.\nWe have the following \\textbf{Obs}ervations:\n\n\\textbf{Obs 1: Refining discriminative features without considering the causality leads to poor OOD generalization.}\nFor the unbiased dataset, most attention- and pooling-based baselines, such as GlobalAtt, SuperGAT, SortPool, Top-$k$ Pool, outperform GCN.\nIt indicates the effectiveness of extracting discriminative features in the ID setting.\nHowever, as the bias-level goes to extremes, the performance dramatically deteriorates.\nFor instance, the performance drop of attention-based methods ranges from $7.37\\%\\sim12.75\\%$ on SYN-$0.1$, and $3.79\\%\\sim13.79\\%$ on SYN-$0.9$;\nPooling-based methods drop from  $7.82\\%\\sim14.24\\%$ and $3.99\\%\\sim12.10\\%$ for SYN-$0.1$ and SYN-$0.9$.\nThese indicate that simply extracting discriminative features by attention or pooling module is prone to capture the data biases.\nThese are also beneficial for reducing the training loss but lead to poor OOD generalization. \n\\syd{Taking SYN-$0.9$ as an example, most ``House'' co-occur with ``Tree'' in the training data, so the model will mistakenly learn shortcut features from the ``Tree''-type trivial subgraphs to make predictions, instead of probing the ``House''-type causal subgraphs. \nThis will mislead the model to adopt the ``Tree'' pattern to make decisions in the inference stage.}\n% regard some part of ``Tree''-type trivial subgraphs as an important part of ``House''-type causal subgraphs. \n% It forms a spurious correlation between ``Tree'' trivial subgraphs and ``House'' label, which misleads the model to adopt the ``Tree'' pattern to make decisions in inference stage.\n\n\\textbf{Obs 2: GNNs with better ID performance tend to have worse OOD generalization.}\nFor the unbiased dataset, GIN achieves the best performance (96.74\\%), while GAT (92.69\\%) outperforms the GCN (90.94\\%).\nThis indicates that the in-distribution (ID) performance of these models exhibits such an order: GIN $>$ GAT $>$ GCN.\n% This indicates that the generalization ability of the models is ranked as GIN, GAT, GCN.\nHowever, when the bias is changed to 0.1 and 0.9, the performance of GIN drops by $9.55\\%$ and $7.36\\%$, GAT drops by $8.71\\%$ and $5.47\\%$ and GCN drops by $6.60\\%$ and $5.43\\%$, respectively.\nIt shows that the rankings of models' robustness against OOD issues are in the opposite order: GCN $>$ GAT $>$ GIN.\nThis indicates that GNNs with better ID performance are prone to learn more shortcut features.\nSimilar trends also occur in other baselines.\nAfter adopting the proposed CAL, this phenomenon is significantly alleviated, which verifies the effectiveness of CAL in overcoming the OOD issue.\n\n\n\n\\textbf{Obs 3: Mitigating the confounder achieves more stable performance on OOD datasets.}\nWe first define the performance discount on SYN-$b$ as the accuracy on SYN-$b$ normalized by the accuracy on unbiased SYN-0.5. \nIt indicates the degree of the performance degradation on biased synthetic datasets, without considering the model's ID generalization.\nWe plot the performance discount curves on SYN-$b$ with $b \\in \\{0.1, 0.2, ..., 0.9\\}$.\nAs depicted in Figure \\ref{fig:syn_norm_results}, we observe that pooling-based methods outperform GIN in a small range of bias-levels ($0.2\\sim0.8$), while the performance drops sharply when $b=0.1$ or $0.9$. For example, the performance discount of Top-$k$ Pool drops from 0.95 to 0.88 as $b$ reduces from 0.2 to 0.1.\nAttention-based methods perform worse than GIN when $b<0.5$.\nFor $b>0.5$, AGNN achieves better performance than GIN, while GlobalAttention often performs worse.\nThese results reflect that attention- or pooling-based methods all have their own weaknesses, such that they cannot consistently overcome the diverse distribution shifts.\nEquipped with CAL, GIN (red curve) consistently outperforms all the baselines on all ranges of bias-levels and obviously keeps a large gap, which further demonstrates the significance of mitigating the confounding effect, and the effectiveness of CAL.\nFor comprehensive comparisons, we also plot two upper bound methods: IRM and DRO (dash lines), which require additional annotation information of trivial subgraphs for training. \nWe observe that, even without additional information, CAL achieves comparable performance with these upper bound methods.\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Performance on Real-world Graphs (RQ2)",
                    "content": "\nUnlike synthetic graphs, there may not exist visible or specific patterns of the causal/trivial subgraphs in real-world graphs.\nHowever, there still exist irregular core-subgraphs \\cite{ReFine, lin2021generative, ying2019gnnexplainer,knyazev2019understanding} that determine the predictions, which will inevitably involve different degrees of biases caused by the complementary parts.\nSimilar to SYN-$b$, they mislead the GNNs to learn the spurious correlations.\nHence, we verify the practicability of CAL on eight real-world datasets.\nWe report the results of the baselines from the original papers by default and reproduce the missing results.\nThe results are summarized in Table \\ref{table:main_all} and we make the following \\textbf{Obs}ervations:\n\n\\textbf{Obs 4: The OOD issue is widespread in real-world datasets.}\nAttention-based and pooling-based methods are on a par with GNNs, and they both outperform graph kernel-based methods in most cases.\nIt can be seen from the last six rows in Table \\ref{table:main_all}, when CAL is applied to different GNN models, it consistently produces further performance improvements.\n% For GCN, GIN, and GAT models, CAL achieves different degrees of performance improvements on seven, eight, and eight out of ten datasets, respectively. \nIt demonstrates that the distribution shifts also widely exist in real-world datasets.\nSpecifically, we can find that GCN often performs worse than other GNNs, attention-based or pooling-based methods, while the performance significantly improves after adopting CAL. \nFor instance, on IMDB-B and MNIST datasets, GCN+CAL achieves $1.92\\%$ and $4.52\\%$ relative improvements, respectively. \nThis indicates that GCN is vulnerable to the distribution shift in certain datasets. \nThanks to the causality, CAL will push GCN to pay more attention to causal features, which can establish robustness against the widespread OOD issues and achieve better generalization.\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study (RQ3)",
                    "content": "\nIn this section, we investigate the impact of the node/edge attention, random combination and the loss coefficients $\\lambda_1$ and $\\lambda_2$.\n\n\\textbf{Node Attention v.s. Edge Attention.}\nNode Attention (NA) and Edge Attention (EA) refine the features from two orthogonal views: node-level and edge-level.\nHere we want to examine the effect of adopting NA or EA alone.\nWe adopt GCN as the encoder to conduct experiments on four biased synthetic datasets and two real-world datasets.\nGCN+CAL w/o NA or EA represents the node/edge attention scores in Equation~\\eqref{equ:node_att}/\\eqref{equ:edge_att} are evenly set as 0.5.\nThe experimental results are shown in Figure \\ref{fig:ab1}. We can find that:\n(1) Comparing NA with EA, the performance of CAL without NA is significantly worse than that without EA, which indicates that the node feature contains more significant information compared with graph structure.\n(2) Just adopting NA or EA alone still achieves better performance than baselines, which demonstrates that only applying NA or EA can also disentangle the causal/trivial attended-graph and achieve causal intervention to some extent.\n\n\\textbf{Random Combination.}\nWe need to stratify the confounder distribution for causal intervention.\nWith the random combination, each causal feature will combine with different types of trivial patterns.\nTo verify its importance, we change the \"Random Addition\" module in Figure \\ref{fig:model} to \"Addition\", which just adopts the addition operation orderly, and we rename it as ``GCN+CAL w/o RD''.\nThe experimental results are shown in Figure \\ref{fig:ab1}. We can find that:\n(1) The performance drops severely compared with GCN+CAL, which demonstrates the importance of the causal intervention.\n(2) GCN+CAL w/o RD can also outperform the GCN baselines. \nWe conjecture that just implementing disentanglement makes GNN pay more attention to the causal features, which will slightly ignore the data biases or trivial patterns.\nThese results also reflect that disentanglement and causal intervention will help each other to improve their own effectiveness.\n\n\n\n\n\n\\textbf{Loss coefficients $\\lambda_1$ and $\\lambda_2$.}\nAccording to Equation~\\eqref{equ:loss_all}, $\\lambda_1$ denotes the strength of the disentanglement for the causal/trivial features, while $\\lambda_2$ controls the strength of the causal intervention.\nTo explore their impacts, we use GCN as the encoder and conduct experiments on two biased synthetic datasets and two real-world datasets.\nWe fix one coefficient as 0.5 and change the other one in $(0,1)$ with a step size of 0.1.\nThe experimental results are shown in Figure \\ref{fig:ab2}. We can find that:\n(1) $\\lambda_1$ achieves better performance in a range of $0.3\\sim0.7$. Too small or too large values will cause performance degradation.\n(2) $\\lambda_2$ is not as stable as $\\lambda_1$. The optimal range is around $0.3\\sim0.5$. \nIt leads to a strong decline at $0.5\\sim0.8$, which indicates that coefficient $\\lambda_2$ should be set prudently.\n\n"
                },
                "subsection 4.5": {
                    "name": "Visualization and Analysis (RQ4)",
                    "content": "\\label{sec45}\n% In this section, we first give visualizations to depict the captured causal attended-graphs.\n% Then we show the distribution of the misclassification to explain the performance improvements.\n\n\n\n\\textbf{Causal attended-graphs.} \nWe plot node/edge attention areas of the causal attended-graphs based on the attention scores in CAL.\nWe adopt a GCN-based encoder and apply CAL on SYN-$b$ and MNIST superpixel graphs.\nThe visualizations are shown in Figure \\ref{fig:syn_vis}.\nNodes with darker colors and edges with wider lines indicate higher attention scores.\nWe surprisingly find that almost all the darker colors and wider lines precisely distribute on the deterministic areas, such as the causal subgraphs we defined in the synthetic dataset and the nodes located on digit pixels in MNIST superpixel graphs. \nIt further demonstrates that the proposed CAL can effectively capture the causal features with insightful interpretations. \n\n\\noindent\\textbf{The explanation for performance improvements.} \nFigure \\ref{fig:bias_error} displays the distribution of misclassification on SYN-$b$.\nThe abscissa represents the predictions, and the ordinate denotes the ground-truth types.\nThe numbers in each row denote the proportion for each class.\nFigure \\ref{fig:bias_error} (Left) shows that the wrong predictions of graphs with ``BA'' are mainly distributed in ``Cycle'', ``Grid'' and ``Diamond'' classes, while the wrong predictions of graphs with ``Tree'' mainly concentrate on the ``House'' class (highlighted by the red circle).\nOn one hand, most of the ``House'' co-occur with ``Tree'' in the training data, GCN tends to capture the shortcut features, \\eg ``Tree'' patterns, to make decisions.\nTherefore, the other three causal subgraphs with ``Tree'' will mainly be misclassified as ``House'' in the testing set.\nOn the other hand, only a few ``House'' causal subgraphs co-occur with ``BA'', so the other three causal subgraphs with ``BA'' will almost not be misclassified as ``House''.\nIn contrast, Figure \\ref{fig:bias_error} (Right) shows that, by applying CAL, the concentration of misclassification is obviously alleviated.\nThis demonstrates that CAL improves performance by mitigating the confounding effect.\n"
                }
            },
            "section 5": {
                "name": "Related work",
                "content": "\n\n% We briefly review some related studies, including attention mechanism, out-of-distribution generalization and causal inference.\n\n\\textbf{Attention Mechanism} selects the informative features from data, which has obtained great success in computer vision \\cite{hu2018squeeze,dosovitskiy2020image,wang2021causal,yang2021causal} and natural language processing tasks \\cite{DBLP:conf/nips/VaswaniSPUJGKP17,devlin2019bert}.\nIn recent years, attention mechanism has gradually become prevalent in the GNN field.\nThe attention modules for GNNs can be defined over edges \\cite{velivckovic2018graph,brody2021attentive,kim2020find,lee2019graph,thekumparampil2018attention} or over nodes \\cite{li2015gated,lee2019self,lee2018graph}.\nDespite effectiveness, attention learning still stays at how to better fit the statistical correlations between data and labels.\nHence, the learned attentions are inherently biased in OOD settings.\nRecent studies \\cite{wang2021causal,yang2021causal} propose the causal attention modules to alleviate the bias.\nCaaM \\cite{wang2021causal} adopts the adversarial training to generate the data partition in each iteration to achieve the causal intervention.\nCATT \\cite{yang2021causal} proposes in-sample and cross-sample attentions based on front-door adjustment.\nHowever, they are both tailored for computer vision tasks, while cannot transfer to graph learning tasks, due to the irregular and challenging graph-structure data.\nDistinct from them, we utilize the disentanglement and causal intervention strategies to strengthen the attention modules for GNNs. \n% The proposed attention modules are designed for the graph-structure data from node and edge perspectives, thereby capturing more precise causal features.\n\n\n\n% \\subsection{Out-of-distribution Generalization}\n\\noindent\\textbf{OOD Generalization} \\cite{hendrycks2016baseline,arjovsky2019invariant,rosenfeld2020risks,sagawa2019distributionally} has been extensively explored in recent years.\nIRM \\cite{arjovsky2019invariant} minimizes the empirical risk under different environments.\nGroup-DRO \\cite{sagawa2019distributionally} adversarially explores the group with the worst risk and achieves generalization by minimizing the empirical risk of the worst group.\nExisting efforts \\cite{arjovsky2019invariant,rosenfeld2020risks,sagawa2019distributionally} mainly focus on computer vision or natural language processing tasks, while the GNN field is of great need but largely unexplored.\nFurthermore, these methods require the environment or group prior information for each training sample, which is expensive in practice.\n% Although these efforts effectively alleviate the generalization vulnerability caused by the distribution shift, they all need the environment or group prior information for each sample, which is expensive to manually label in practice.\nTo alleviate this dilemma, we adopt causal intervention to strengthen the causal relationship between the causal feature and prediction, thereby achieving better generalization.\n\n% \\subsection{Causal Inference}\n\\noindent\\textbf{Causal Inference}s \\cite{pearl2000models,pearl2014interpretation} endows the model with the ability to pursue real causality.\nA growing number of studies \\cite{zhang2020causal,hu2021distilling,tang2020long,niu2021counterfactual} have shown that causal inference is beneficial to diverse computer vision tasks.\nCONTA \\cite{zhang2020causal} uses backdoor adjustment to eliminate the confounder in weakly supervised semantic segmentation tasks.\nDDE \\cite{hu2021distilling} proposes to distill the colliding effect between the old and the new data to improve class-incremental learning.\nUnlike computer vision, the application of causal intervention in the GNN community is still in its infancy.\nCGI \\cite{feng2021should} explores how to select trustworthy neighbors for GNN in the inference stage, and demonstrates its effectiveness in node classification.\nRecent work \\cite{zevcevic2021relating} studies the connection between GNNs and SCM from a theoretical perspective.\n\\syd{Different from them, we introduce a causal attention learning strategy to mitigate the confounding effect for GNNs.\nIt encourages GNNs to pay more attention to causal features, which will enhance the robustness against the distribution shift.}\n\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nIn this work, we revisit the GNN modeling for graph classification from a causal perspective.\nWe find that current GNN learning strategies are prone to exploit the shortcut features to support their predictions.\nHowever, the shortcut feature actually plays a confounder role. \nIt establishes a backdoor path between the causal feature and the prediction, which misleads the GNNs to learn spurious correlations.\nTo mitigate the confounding effect, we propose the causal attention learning (CAL) strategy for GNNs.\n% Specifically, we adopt attention modules to estimate the causal features from graphs. \n% Then we make the given causal feature combined with diverse shortcut features.\nCAL is guided by the backdoor adjustment from the causal theory.\nIt encourages the GNNs to exploit causal features while ignoring the shortcut parts.\nExtensive experimental results and analyses verify its effectiveness.\nFuture studies include adopting powerful disentanglement methods and more advanced causal intervention strategies to improve the CAL.\nWe will also make efforts to apply CAL to other graph learning tasks, such as node classification or link prediction.\n\n"
            },
            "section 7": {
                "name": "Acknowledgments",
                "content": "\nThis work is supported by the National Key Research and Development Program of China (2020AAA0106000), and the National Natural Science Foundation of China (U19A2079, U21B2026). This research is also supported by CCCD Key Lab of Ministry of Culture and Tourism and Sea-NExT Joint Lab.\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{main}\n\\appendix\n\n\\clearpage\n\n% \\section{More Implementation Details}\n"
            },
            "section 8": {
                "name": "Appendix",
                "content": "\n\n",
                "subsection 8.1": {
                    "name": "Algorithm",
                    "content": "\\label{app1}\nWe provide the detailed implementation of the proposed casual attention learning (CAL) in Algorithm \\ref{alg1}.\nWe adopt the causal attended-graph for prediction in the inference stage.\n\n\\begin{algorithm}[htb]\n\\caption{Casual Attention Learning} \n\\label{alg1}\n\\begin{algorithmic}[1]\n\\SetKwInOut{Input}{Input}\n\\SetKwInOut{Output}{Output} \n% \\REQUIRE Dataset $\\mathcal{D}$, GNN encoder $f(\\cdot)$.\n\\REQUIRE{Dataset $\\mathcal{D}$, $f(\\cdot)$, attention modules, classifiers, $\\lambda_1$, $\\lambda_2$} \n\\ENSURE{The trained parameters.}\n\\FOR {sampled $M$ graphs $\\{\\mathcal{G}^k=\\{\\Mat{A}^k, \\Mat{X}^k\\}\\}_{k=1}^M$}\n\\FOR {$k\\leftarrow 1$ \\textbf{to} $M$}\n\\STATE $\\Mat{H}^k \\leftarrow f(\\Mat{A}^k, \\Mat{X}^k\\})$\n\\STATE Compute $\\alpha_{c_i}, \\alpha_{t_i}\\leftarrow$ Equation~\\eqref{equ:node_att} for all nodes of $\\mathcal{G}^k$\n\\STATE Compute $\\beta_{c_{ij}}, \\beta_{t_{ij}}\\leftarrow $ Equation~\\eqref{equ:edge_att} for all edges of $\\mathcal{G}^k$\n\\STATE Get masks $\\Mat{M}_a^k$ and $\\Mat{M}_x^k$ based on $\\beta_{c_{ij}}$ and $\\alpha_{c_i}$\n\\STATE Get masks $\\overline{\\Mat{M}_a^k}$ and $\\overline{\\Mat{M}_x^k}$ based on $\\beta_{t_{ij}}$ and $\\alpha_{t_i}$\n\\STATE $\\mathcal{G}_c^k\\leftarrow \\{\\Mat{A}^k\\odot \\Mat{M}_a^k, \\Mat{X}^k\\odot \\Mat{M}_x^k\\}$ \\tcp{causal}\n\\STATE $\\mathcal{G}_t^k\\leftarrow \\{\\Mat{A}^k\\odot \\overline{\\Mat{M}_a^k}, \\Mat{X}^k\\odot \\overline{\\Mat{M}_x^k}\\}$ \\tcp{trivial}\n\\STATE $\\Mat{h}_{\\mathcal{G}_{c}^k}, \\Mat{z}_{\\mathcal{G}_c^k} \\leftarrow$ Equation~\\eqref{equ:zc}\n\\STATE $\\Mat{h}_{\\mathcal{G}_{t}^k}, \\Mat{z}_{\\mathcal{G}_t^k} \\leftarrow$ Equation~\\eqref{equ:zt}\n\\ENDFOR\n\\STATE Supervised loss: $\\mathcal{L}_{\\mathrm{sup}} \\leftarrow$ Equation~\\eqref{equ:loss1}\n\\STATE Uniform loss: $\\mathcal{L}_{\\mathrm{unif}} \\leftarrow$ Equation~\\eqref{equ:loss2}\n\\STATE $I\\leftarrow\\ \\mathrm{Shuffle}([1,...,M])$ \n\\FOR {$k\\leftarrow 1$ \\textbf{to} $M$}\n\\STATE $i\\leftarrow I[k]$\n\\STATE  $\\Mat{h}_{\\mathcal{G}^k} \\leftarrow \\Mat{h}_{\\mathcal{G}_{c}^k} + \\Mat{h}_{\\mathcal{G}_{t}^i}$ \\tcp{random combination}\n\\STATE  $\\Mat{z}_{\\mathcal{G}^k} \\leftarrow \\mathrm{\\Phi}(\\Mat{h}_{\\mathcal{G}^{k}})$\n\\ENDFOR\n\\STATE Causal loss: $\\mathcal{L}_{\\mathrm{caus}} \\leftarrow$ Equation~\\eqref{equ:loss3}\n\\STATE Total loss: $\\mathcal{L} \\leftarrow \\mathcal{L}_{\\mathrm{sup}} + \\lambda_1\\mathcal{L}_{\\mathrm{unif}} + \\lambda_2\\mathcal{L}_{\\mathrm{caus}}$\n\\STATE Update all the trainable parameters to minimize $\\mathcal{L}$\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\n"
                },
                "subsection 8.2": {
                    "name": "Datasets Details",
                    "content": "\\label{app2}\nIn this section, we give more details about the synthetic datasets and real-world datasets.\n\n\\textbf{1) Synthetic graphs.} \nFor each synthetic graph instance, it consists of two subgraphs: trivial and critical subgraphs. \nWe introduce the proposed trivial subgraph and critical subgraph as follows:\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{Trivial subgraph}. There exist two types of trivial subgraphs: BA-SHAPES and Tree.\nThe BA-SHAPES is a Barabási-Albert (BA) graph \\cite{barabasi1999emergence}, and we abbreviate it as ``BA'' in this paper.\nThe ``Tree'' graph is a base 12-level balanced binary tree \\cite{ying2019gnnexplainer}.\nTo reduce the influence, we control the number of nodes in the two kinds of trivial subgraphs to be similar.\n\\item \\textbf{Causal subgraph}. There are four types of causal subgraphs: ``House'', ``Cycle'', ``Grid'', ``Diamond''. The visualizations of these trivial subgraphs and causal subgraphs are depicted in Figure \\ref{fig:syn_dataset}.\n\\end{itemize}\nFor each synthetic graph instance, a causal subgraph is randomly attached on one node of a trivial subgraph. \nThen the resulting graph is further perturbed by adding 10\\% random edges.\nWe take the one-hot form of the node degree as the node feature and set the dimension of node feature to 20.\nThe synthetic graph examples are displayed in Figure \\ref{fig:syn_dataset}. The statistics of the synthetic datasets are summarized in Table \\ref{table:dataset_s}. \nWe split the dataset into training, validation and testing set with the ratio of 7: 1: 2.\n\n\n\n\\textbf{2) Real-world graphs.} \nTo demonstrate the practicality of the proposed CAL, we conduct experiments on TUDataset \\cite{morris2020tudataset} and Superpixel graphs \\cite{knyazev2019understanding}.\nFor TUDataset, we gather three biological datasets (MUTAG, NCI1, PROTEINS) and three social networks datasets (COLLAB, IMDB-B, IMDB-M), which are commonly used in graph classification benchmarks \\cite{xu2018how, dwivedi2020benchmarking}.\nFollowing \\cite{dwivedi2020benchmarking,xu2018how,ying2018hierarchical}, we use 10-fold cross-validation and report average accuracy and standard deviation.\nThe superpixel graphs \\cite{knyazev2019understanding, dwivedi2020benchmarking} includes MNIST and CIFAR-10, which are classical image classification datasets converted into graphs using superpixels technology \\cite{achanta2012slic} and assigning each node's features as the superpixel coordinates and intensity.\nFollowing \\cite{dwivedi2020benchmarking, knyazev2019understanding}, we split the MNIST and CIFAR-10 to 55K training/5K validation/10K testing, and 45K training/5K validation/10K testing, respectively.\nAll the detailed statistics about the real-world datasets are summarized in Table \\ref{table:dataset_s}.\n\n"
                },
                "subsection 8.3": {
                    "name": "Hyper-parameters",
                    "content": "\\label{app3}\nAs for training parameters, we train the models for 100 epochs with batch size of 128. \nWe optimize all models with the Adam optimizer.\n% and default choice of learning rate (\\textit{e.g.}, $0.001$, $0.002$) for all experiments.\nFor SYN-$b$ and TUDataset, we use GCN, GIN and GAT as GNN encoders with 3 layers and 128 hidden units.\nFor Superpixel graphs MNIST and CIFAR-10, we use the GNN encoders with 4 layers and 146 hidden units as \\cite{dwivedi2020benchmarking}.\nFor all the baselines, we follow the default settings from original papers and reproduce the missing results.\nFor the proposed CAL, we search $\\lambda_1$ and $\\lambda_2$ in $(0.1, 1.0)$ with a step size of $0.1$ and report the results with the best settings.\nWe adopt NVIDIA 2080 Ti (11GB GPU) to conduct all our experiments, the training time comparison is shown as Table \\ref{table:time}.\n\n\n\n"
                }
            }
        },
        "tables": {
            "table:main_syn": "\\begin{table*}[htb]\n\\centering\n\\caption{Test Accuracy (\\%) of graph classification on synthetic datasets with diverse biases. The number in brackets represents the performance degradation compared with the unbiased dataset. Our methods are highlighted with a gray background.}\n% \\vspace{20mm}\n\\label{table:main_syn}\n\\setlength{\\tabcolsep}{5.4mm}{\\begin{tabular}{l|ccccc}\n\\toprule\nMethod & SYN-$0.1$  & SYN-$0.3$  & Unbiased & SYN-$0.7$ & SYN-$0.9$ \\\\ \n\\toprule\n% \\hline\nGATv2 \\cite{brody2021attentive}  & 87.25 ($\\downarrow 7.37\\%$) & 92.19 ($\\downarrow 2.12\\%$) & 94.19 & 93.31 ($\\downarrow 0.93\\%$) & 90.62 ($\\downarrow 3.79\\%$) \\\\\nSuperGAT \\cite{kim2020find}  & 83.81 ($\\downarrow 12.75\\%$) & 91.94 ($\\downarrow 4.29\\%$) & 96.06 & 88.50 ($\\downarrow 7.89\\%$) & 82.81 ($\\downarrow 13.79\\%$) \\\\\nGlobalAtt \\cite{li2015gated} & 87.19 ($\\downarrow 10.40\\%$) & 93.75 ($\\downarrow 3.66\\%$) & 97.31 & 94.62 ($\\downarrow 2.76\\%$) & 91.50 ($\\downarrow 5.97\\%$) \\\\\nAGNN \\cite{thekumparampil2018attention}  & 84.56 ($\\downarrow 11.69\\%$) & 93.06 ($\\downarrow 2.81\\%$) & 95.75 & 94.81 ($\\downarrow 0.98\\%$) & 88.12 ($\\downarrow 7.97\\%$) \\\\\n\\hline\nDiffPool \\cite{ying2018hierarchical} & 82.28 ($\\downarrow 8.69\\%$)& 88.02 ($\\downarrow 2.32\\%$) & 90.11 & 88.83 ($\\downarrow 1.42\\%$) & 84.50 ($\\downarrow 6.23\\%$)\\\\\nSortPool \\cite{zhang2018end} & 80.70 ($\\downarrow 14.24\\%$)& 92.33 ($\\downarrow 1.88\\%$) & 94.10 & 92.14 ($\\downarrow 2.08\\%$) & 90.35 ($\\downarrow 3.99\\%$) \\\\\nTop-$k$ Pool \\cite{gao2019graph} & 84.31 ($\\downarrow 11.81\\%$)& 93.53 ($\\downarrow 2.17\\%$) & 95.60 & 94.44 ($\\downarrow 1.21\\%$) & 88.02 ($\\downarrow 7.93\\%$)\\\\\nSAGPool \\cite{lee2019self} & 88.08 ($\\downarrow 7.82\\%$)& 90.86 ($\\downarrow 4.91\\%$)& 95.55 & 92.22 ($\\downarrow 3.49\\%$) & 83.99 ($\\downarrow 12.10\\%$)\\\\\n\\hline\nGCN  \\cite{kipf2016semi} & 84.94 ($\\downarrow 6.60\\%$)& 89.38 ($\\downarrow 1.72\\%$) & 90.94 & 90.25 ($\\downarrow 0.76\\%$) & 86.00 ($\\downarrow 5.43\\%$) \\\\ \n\\rowcolor{gray!20} GCN + CAL & 89.38 ($\\downarrow 6.03\\%$) & 93.50 ($\\downarrow 1.70\\%$) & 95.12  & 95.06 ($\\downarrow 0.06\\%$)  & 93.31 ($\\downarrow 1.90\\%$) \\\\ \n\\hline\nGIN \\cite{xu2018how} & 87.50 ($\\downarrow 9.55\\%$) & 93.94 ($\\downarrow 2.89\\%$) & 96.74 & 94.88 ($\\downarrow 1.92\\%$) & 89.62 ($\\downarrow 7.36\\%$) \\\\\n\\rowcolor{gray!20} GIN + CAL & 93.19 ($\\downarrow 3.87\\%$) & 96.31 ($\\downarrow 0.65\\%$) & 96.94 & 96.56 ($\\downarrow 0.39\\%$) & 95.25 ($\\downarrow 1.74\\%$)  \\\\\n\\hline\nGAT \\cite{velivckovic2018graph} & 84.62 ($\\downarrow 8.71\\%$)& 89.50 ($\\downarrow 3.44\\%$) & 92.69 & 92.31 ($\\downarrow 0.41\\%$) & 87.62 ($\\downarrow 5.47\\%$) \\\\\n\\rowcolor{gray!20} GAT + CAL & 92.44 ($\\downarrow 4.37\\%$)& 96.25 ($\\downarrow 0.42\\%$) & 96.66 & 96.12 ($\\downarrow 0.56\\%$) & 92.56 ($\\downarrow 4.24\\%$) \\\\ \n\\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "table:main_all": "\\begin{table*}[htb]\n\\centering\n\\caption{Test Accuracy (\\%) of classification. For TUDataset, we perform 10-fold cross-validation and report the mean and standard derivations. Our methods are highlighted with gray background. If the performance improves, the number is bolded.}\n% \\vspace{20mm}\n\\label{table:main_all}\n\\setlength{\\tabcolsep}{3.1mm}{\\begin{tabular}{l|ccc|ccc|cc}\n\\toprule\nDataset &  MUTAG & NCI1  & PROTEINS & COLLAB & IMDB-B & IMDB-M & MNIST & CIFAR-10  \\\\ \n\\toprule\nGK \\cite{shervashidze2009efficient}  & 81.58\\footnotesize{$\\pm$2.11} & 62.49\\footnotesize{$\\pm$0.27} &  71.67\\footnotesize{$\\pm$0.55} & 72.84\\footnotesize{$\\pm$0.28} & 65.87\\footnotesize{$\\pm$0.98} & 43.89\\footnotesize{$\\pm$0.38} & - & - \\\\ \nWL \\cite{shervashidze2011weisfeiler} & 82.05\\footnotesize{$\\pm$0.36} & 82.19\\footnotesize{$\\pm$0.18} &  74.68\\footnotesize{$\\pm$0.50} & 79.02\\footnotesize{$\\pm$1.77} & 73.40\\footnotesize{$\\pm$4.63} & 49.33\\footnotesize{$\\pm$4.75} & - & - \\\\\nDGK \\cite{yanardag2015deep} \t\t & 87.44\\footnotesize{$\\pm$2.72} & 80.31\\footnotesize{$\\pm$0.46} & 75.68\\footnotesize{$\\pm$0.54} & 73.09\\footnotesize{$\\pm$0.25} & 66.96\\footnotesize{$\\pm$0.56} & 44.55\\footnotesize{$\\pm$0.52} & - & - \\\\\n\\hline \nGlobalAtt \\cite{li2015gated}     & 88.27\\footnotesize{$\\pm$8.65} & 81.17\\footnotesize{$\\pm$1.04} &  72.60\\footnotesize{$\\pm$4.37} & 81.48\\footnotesize{$\\pm$1.46} & 69.10\\footnotesize{$\\pm$3.80} & 51.40\\footnotesize{$\\pm$2.91} & - & - \\\\\nAGNN \\cite{thekumparampil2018attention}\t\t & 79.77\\footnotesize{$\\pm$8.54} & 79.96\\footnotesize{$\\pm$2.37} &  75.66\\footnotesize{$\\pm$3.94} & 81.10\\footnotesize{$\\pm$2.39} & 73.10\\footnotesize{$\\pm$4.07} & 49.73\\footnotesize{$\\pm$3.72} & - & - \\\\\nDiffPool \\cite{ying2018hierarchical} & 85.61\\footnotesize{$\\pm$6.22} & 75.06\\footnotesize{$\\pm$3.66} &  76.25\\footnotesize{$\\pm$4.21} & 79.24\\footnotesize{$\\pm$1.66} & 74.47\\footnotesize{$\\pm$3.84} & 49.20\\footnotesize{$\\pm$3.10} & - & -  \\\\\nSortPool \\cite{zhang2018end}         & 86.17\\footnotesize{$\\pm$7.53} & 79.00\\footnotesize{$\\pm$1.68} & 75.48\\footnotesize{$\\pm$1.62} & 77.84\\footnotesize{$\\pm$1.22} & 73.00\\footnotesize{$\\pm$3.50} & 49.53\\footnotesize{$\\pm$2.29} & - & - \\\\\n\\hline\nGCN  \\cite{kipf2016semi}             & 88.20\\footnotesize{$\\pm$7.33} & 82.97\\footnotesize{$\\pm$2.34} &  75.65\\footnotesize{$\\pm$3.24} & 81.72\\footnotesize{$\\pm$1.64} & 73.89\\footnotesize{$\\pm$5.74} & 51.53\\footnotesize{$\\pm$3.28} & 90.49 & 54.68 \\\\ \n\\rowcolor{gray!20} GCN + CAL & \\textbf{89.24\\footnotesize{$\\pm$8.72}} & \\textbf{83.48\\footnotesize{$\\pm$1.94}} & \\textbf{76.28\\footnotesize{$\\pm$3.65}} & \\textbf{82.08\\footnotesize{$\\pm$2.40}} & \\textbf{74.40\\footnotesize{$\\pm$4.55}} & \\textbf{52.13\\footnotesize{$\\pm$2.96}} & \\textbf{94.58} & \\textbf{56.21} \\\\ \n\\hline\nGIN \\cite{xu2018how} & 89.42\\footnotesize{$\\pm$7.40} & 82.71\\footnotesize{$\\pm$1.52} &  76.21\\footnotesize{$\\pm$3.83} & 82.08\\footnotesize{$\\pm$1.51} & 73.40\\footnotesize{$\\pm$3.78} & 51.53\\footnotesize{$\\pm$2.97} &  96.51 & 56.36 \\\\\n\\rowcolor{gray!20} GIN + CAL & \\textbf{89.91\\footnotesize{$\\pm$8.34}} & \\textbf{83.89\\footnotesize{$\\pm$1.93}} &  \\textbf{76.92\\footnotesize{$\\pm$3.31}} & \\textbf{82.68\\footnotesize{$\\pm$1.25}} & \\textbf{74.13\\footnotesize{$\\pm$5.21}} & \\textbf{52.60\\footnotesize{$\\pm$2.36}} &  \\textbf{96.93} & \\textbf{56.63} \\\\ \n\\hline\nGAT \\cite{velivckovic2018graph}   & 88.58\\footnotesize{$\\pm$7.54} & 82.11\\footnotesize{$\\pm$1.43} &  75.96\\footnotesize{$\\pm$3.26} & 81.42\\footnotesize{$\\pm$1.41} & 72.70\\footnotesize{$\\pm$4.37} & 50.60\\footnotesize{$\\pm$3.75} &  95.53 & 64.22 \\\\\n\\rowcolor{gray!20} GAT + CAL & \\textbf{89.94\\footnotesize{$\\pm$8.78}} & \\textbf{83.55\\footnotesize{$\\pm$1.42}} &  \\textbf{76.39\\footnotesize{$\\pm$3.65}} & \\textbf{82.12\\footnotesize{$\\pm$1.95}} & \\textbf{73.30\\footnotesize{$\\pm$4.16}} & \\textbf{50.93\\footnotesize{$\\pm$3.84}} &  \\textbf{95.91} & \\textbf{66.16} \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "table:dataset_s": "\\begin{table}[htb]\n\\centering\n\\vspace{-2mm}\n\\caption{Statistics of datasets used in experiments.}\n\\vspace{-2mm}\n\\label{table:dataset_s}\n\\setlength{\\tabcolsep}{2.5mm}{\\begin{tabular}{l|cccc}\n\\toprule\n\\textbf{Dataset} & \\#\\textbf{Graphs} & \\#\\textbf{Nodes} & \\#\\textbf{Edges} & \\#\\textbf{Classes} \\\\ \n\\toprule\nSYN-$b$ & 8000 & 230$\\sim$247 & 542$\\sim$1000 & 4 \\\\\n\\hline\nMUTAG & 188 & 17.93 & 19.79 & 2 \\\\\nNCI1 & 4110 & 29.87 & 32.30 & 2 \\\\\nPROTEINS & 1113 & 39.06 & 72.82 & 2 \\\\\n\\hline\nCOLLAB & 5000 & 74.49 & 2457.78 & 3\\\\\nIMDB-B & 1000 & 19.77 & 96.53 & 2 \\\\\nIMDB-M & 1500 & 13.00 & 65.94 & 3\\\\\n\\hline\nMNIST & 70000 & 70.57 & 564.66 & 10 \\\\\nCIFAR-10 & 60000 & 117.63 & 941.04 & 10 \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}",
            "table:time": "\\begin{table}[htb]\n\\centering\n\\vspace{-2mm}\n\\caption{Training time (minutes) comparison.}\n\\vspace{-2mm}\n\\label{table:time}\n\\setlength{\\tabcolsep}{1.8mm}{\\begin{tabular}{l|ccccc}\n\\toprule\nMethod & SYN-$b$ & MUTAG & NCI1 & IMDB-M & MNIST \\\\ \n\\toprule\nGCN         & 4.16 & 1.03 & 12.71 & 4.61 & 57.20  \\\\\nGCN + CAL   & 6.67 & 1.35 & 17.37 & 6.16 & 75.80 \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}"
        },
        "figures": {
            "fig:causal": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{Figs/causal_revised.pdf}\n    \\vspace{-2mm}\n    \\caption{\\wxx{Structural causal model for graph classification.}}\n    \\label{fig:causal}\n    \\vspace{-4mm}\n\\end{figure}",
            "fig:model": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{Figs/model.pdf}\n    \\vspace{-4mm}\n    \\caption{The overview of the proposed Causal Attention Learning (CAL) framework.}\n    \\label{fig:model}\n    \\vspace{-4mm}\n\\end{figure}",
            "fig:syn_dataset": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{Figs/datasets2.pdf}\n    \\vspace{-4mm}\n    \\caption{Illustration of the synthetic datasets.}\n    \\vspace{-4mm}\n    \\label{fig:syn_dataset}\n\\end{figure}",
            "fig:syn_norm_results": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{Figs/syn01.pdf}\n    \\vspace{-4mm}\n    \\caption{The performance discount on synthetic datasets with different bias-levels.}\n    \\label{fig:syn_norm_results}\n    \\vspace{-4mm}\n\\end{figure}",
            "fig:ab1": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{Figs/ablation1.pdf}\n    \\vspace{-4mm}\n    \\caption{The comparison of different components in CAL.}\n    \\label{fig:ab1}\n    \\vspace{-4mm}\n\\end{figure}",
            "fig:ab2": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{Figs/ablation2.pdf}\n    \\vspace{-8mm}\n    \\caption{Parameter sensitivity of loss coefficients $\\lambda_1$ and $\\lambda_2$.}\n    \\label{fig:ab2}\n    \\vspace{-8mm}\n\\end{figure}",
            "fig:syn_vis": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.90\\linewidth]{Figs/vis2.pdf}\n    \\vspace{-4mm}\n    \\caption{Visualizations of causal attended-graphs. (Left): Synthetic graphs, (Right): MNIST superpixel graphs.}\n    \\vspace{-4mm}\n    \\label{fig:syn_vis}\n\\end{figure}",
            "fig:bias_error": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{Figs/bias.pdf}\n    \\vspace{-8mm}\n    \\caption{The misclassification distribution.\n    Red circle highlights the concentration degree of misclassification.}\n    \\label{fig:bias_error}\n    \\vspace{-6mm}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\Mat{H}' = \\mathrm{GConv}\\left(\\Mat{A} \\odot \\Mat{M}_a, \\Mat{H}\\right)\n\\label{equ:gnn1}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\Mat{H}' = \\mathrm{GConv}\\left(\\Mat{A}, \\Mat{H} \\odot \\Mat{M}_x\\right)\n\\label{equ:gnn2}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\Mat{h}_\\mathcal{G} = f_\\mathrm{readout}\\left(\\{\\Mat{h}_{i}^{out}| i \\in \\mathcal{V} \\}\\right), \\quad z_\\mathcal{G} = \\Phi(\\Mat{h}_\\mathcal{G}).\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\mathcal{L}_{\\mathrm{CE}} = -\\frac{1}{|\\mathcal{D}|} \\sum_{\\mathcal{G}\\in\\mathcal{D}}\\Trans{\\Mat{y}}_\\mathcal{G}\\mathrm{log}(\\Mat{z}_\\mathcal{G}).\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\begin{aligned}\n\\label{equ:backdoor}\n    P(Y|do(C)) & = P_m(Y|C) \\\\\n    & = \\sum\\nolimits_{s\\in\\mathcal{T}}P_m(Y|C, s)P_m(s|C) \\quad (Bayes\\,Rule) \\\\\n    & = \\sum\\nolimits_{s\\in\\mathcal{T}}P_m(Y|C, s)P_m(s) \\quad (Independency)\\\\\n    & = \\sum\\nolimits_{s\\in\\mathcal{T}}P(Y|C, s)P(s),\n\\end{aligned}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\Mat{H}=f(\\Mat{A}, \\Mat{X}).\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\alpha_{c_i}, \\alpha_{t_i}=\\sigma(\\mathrm{MLP_{node}}(\\Mat{h}_i)),\n\\label{equ:node_att}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\beta_{c_{ij}}, \\beta_{t_{ij}}=\\sigma(\\mathrm{MLP_{edge}}(\\Mat{h}_i||\\Mat{h}_j)),\n\\label{equ:edge_att}\n\\end{equation}",
            "eq:equ:zc": "\\begin{equation}\\label{equ:zc}\n\\Mat{h}_{\\mathcal{G}_c} = f_{\\mathrm{readout}}(\\mathrm{GConv}_c(\\Mat{A}\\odot \\Mat{M}_a, \\Mat{X}\\odot \\Mat{M}_x)), \\quad \\Mat{z}_{\\mathcal{G}_c} = \\Phi_{c}(\\Mat{h}_{\\mathcal{G}_{c}}),\n\\end{equation}",
            "eq:equ:zt": "\\begin{equation}\\label{equ:zt}\n\\Mat{h}_{\\mathcal{G}_t} = f_{\\mathrm{readout}}(\\mathrm{GConv}_t(\\Mat{A}\\odot \\overline{\\Mat{M}}_a, \\Mat{X}\\odot \\overline{\\Mat{M}}_x)), \\quad \\Mat{z}_{\\mathcal{G}_t} = \\Phi_{t}(\\Mat{h}_{\\mathcal{G}_{t}}).\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    \\mathcal{L}_{\\mathrm{sup}} = -\\frac{1}{|\\mathcal{D}|} \\sum_{\\mathcal{G}\\in\\mathcal{D}}\\wx{\\Trans{\\Mat{y}}_\\mathcal{G}}\\mathrm{log}(\\Mat{z}_{\\mathcal{G}_c}).\n    \\label{equ:loss1}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\mathcal{L}_{\\mathrm{unif}} = \\frac{1}{|\\mathcal{D}|} \\sum_{\\mathcal{G}\\in\\mathcal{D}}\\mathrm{KL}(\\Mat{y}_\\mathrm{unif}, \\Mat{z}_{\\mathcal{G}_t}).\n    \\label{equ:loss2}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    \\mathcal{L}= \\mathcal{L}_{\\mathrm{sup}} + \\lambda_1\\mathcal{L}_{\\mathrm{unif}} + \\lambda_2\\mathcal{L}_{\\mathrm{caus}}\n    \\label{equ:loss_all}\n\\end{equation}"
        },
        "git_link": "https://github.com/yongduosui/CAL"
    }
}