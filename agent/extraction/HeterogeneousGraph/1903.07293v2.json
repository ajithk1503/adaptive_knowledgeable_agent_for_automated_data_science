{
    "meta_info": {
        "title": "Heterogeneous Graph Attention Network",
        "abstract": "Graph neural network, as a powerful graph representation technique based on\ndeep learning, has shown superior performance and attracted considerable\nresearch interest. However, it has not been fully considered in graph neural\nnetwork for heterogeneous graph which contains different types of nodes and\nlinks. The heterogeneity and rich semantic information bring great challenges\nfor designing a graph neural network for heterogeneous graph. Recently, one of\nthe most exciting advancements in deep learning is the attention mechanism,\nwhose great potential has been well demonstrated in various areas. In this\npaper, we first propose a novel heterogeneous graph neural network based on the\nhierarchical attention, including node-level and semantic-level attentions.\nSpecifically, the node-level attention aims to learn the importance between a\nnode and its metapath based neighbors, while the semantic-level attention is\nable to learn the importance of different meta-paths. With the learned\nimportance from both node-level and semantic-level attention, the importance of\nnode and meta-path can be fully considered. Then the proposed model can\ngenerate node embedding by aggregating features from meta-path based neighbors\nin a hierarchical manner. Extensive experimental results on three real-world\nheterogeneous graphs not only show the superior performance of our proposed\nmodel over the state-of-the-arts, but also demonstrate its potentially good\ninterpretability for graph analysis.",
        "author": "Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Peng Cui, P. Yu, Yanfang Ye",
        "link": "http://arxiv.org/abs/1903.07293v2",
        "category": [
            "cs.SI"
        ],
        "additionl_info": "10 pages"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nThe real-world data usually come together with the graph structure, \nsuch as social networks, citation networks, and the world wide web. Graph neural network (GNN), \nas a powerful deep representation learning method for such graph data, \nhas shown superior performance on network analysis and aroused considerable research interest. \nFor example, \\cite{gnn05,gnn09,ggnn}\nleverage deep neural network to learn node representations based on node features %initial features \nand the graph structure. \n% and use the learned representations to classify each node independently.\nSome works \\cite{16chebyshev,gcn,graphsage} \npropose the graph convolutional networks by generalizing the convolutional operation to graph.\nA recent research trend in deep learning is the attention mechanism,\nwhich deals with variable sized data and encourages the model to focus on the most salient parts of data. \nIt has demonstrated the effectiveness in deep neural network framework and is widely applied to various applications, such as text analysis \\cite{bahdanau2014neural}, knowledge graph \\cite{schlichtkrull2018modeling} and image processing \\cite{show_xu2015show}.\nGraph Attention Network (GAT) \\cite{gat}, a novel convolution-style graph neural network, leverages attention mechanism for the homogeneous graph which includes only one type of nodes or links.\n\nDespite the success of attention mechanism in deep learning, it has not been considered in the graph neural network framework for heterogeneous graph. As a matter of fact, the real-world graph usually comes with multi-types of nodes and edges, \nalso widely known as heterogeneous information network (HIN) \\cite{Shi2017ASO}. For convenience, we uniformly call it heterogeneous graph in this paper.\nBecause the heterogeneous graph contains more comprehensive information and rich semantics, it has been widely used in many data mining tasks. Meta-path \\cite{sun2011pathsim}, a composite relation connecting two objects, is a widely used structure to capture the semantics. \nTaking the movie data IMDB\\footnote{https://www.imdb.com} shown in Figure \\ref{fig_hin}(a) as an example, it contains three types of nodes include movie, actor and director.\nA relation between two movies can be revealed by meta-path Movie-Actor-Movie (\\emph{MAM}) which describes the co-actor relation,\nwhile Movie-Director-Movie (\\emph{MDM}) means that they are directed by the same director.\nAs can be seen, depending on the meta-paths,\nthe relation between nodes in the heterogeneous graph can have different semantics.\n%Although some previous works introduce the attention mechanism for heterogeneous graph based applications, e.g., the recommendation \\cite{mcrec,han2018aspect}. However, in essence, they just leverage heterogeneous information to improve the performance of applications, and they are not specifically designed for the framework of heterogeneous graph based neural network. \nDue to the complexity of heterogeneous graph, traditional graph neural network cannot be directly applied to heterogeneous graph.\n% because of the following two reasons:\n%\\begin{enumerate}[(1)]\n%\t\\item Heterogeneous graph has various types of nodes and relations. How to deal with such complex structural information and  preserve the heterogeneous neighbors of each node is an urgent problem that need to be solved. \n%\t\\item There are may exist several meta-paths in heterogeneous graph that can extract diversity semantic information. How to select some meaningful meta-path and fuse the semantic information is an open problem.\n%\t\\item Given a meta-path, each node has lots of meta-path based neighbors. How to distinguish the subtle difference of there neighbors and  some informative neighors should be considered.\n%%\t\\item Application\n%\\end{enumerate}\n\n\n%The notations we will use \nBased on the above analysis, when designing graph neural network architecture with attention mechanism for heterogeneous graph, \nwe need to address the following new requirements.\n\n\\textbf{Heterogeneity of graph.}\nThe heterogeneity is an intrinsic property of heterogeneous graph, i.e., various types of nodes and edges.\nFor example, different types of nodes have different traits and their features may fall in different feature space. Still taking IMDB as an example, the feature of an actor may involve in sex, age and nationality. On the other hand, the feature of movie may involve in plot and actors. \nHow to handle such complex structural information and  preserve the diverse feature information simultaneously is an urgent problem that needs to be solved. \n\n\\textbf{Semantic-level attention.} Different meaningful and complex semantic information are involved in heterogeneous graph, which are usually reflected by meta-paths \\cite{sun2011pathsim}. \nDifferent meta-paths in heterogeneous graph may extract diverse semantic information. How to select the most meaningful meta-paths and fuse the semantic information for the specific task is an open problem \\cite{schain,chen2017task,Shang2016MetaPathGE}.\n%\tSemantic-level attention implies that different meta-paths make different contributions to the downstream applications and should have different attention values . \nSemantic-level attention aims to learn the importance of each meta-path and assign proper weights to them. Still taking IMDB as an example, \\emph{The Terminator} can either connect to  \\emph{The Terminator 2} via  Movie-Actor-Movie (both starred by \\emph{Schwarzenegger}) or connect to \\emph{Birdy} via Movie-Year-Movie (both shot in \\emph{1984}). \nHowever, when identifying the genre of the movie \\emph{The Terminator}, \\emph{MAM} usually plays more important role, rather than \\emph{MYM}.\nTherefore, treating different meta-paths equally is unpractical and will weaken the semantic information provided by some useful meta-paths.\n\n\\textbf{Node-level attention.}\nIn a heterogeneous graph, nodes can be connected via various types of relation, e.g., meta-path.\nGiven a meta-path, each node has lots of meta-path based neighbors. How to distinguish the subtle difference of there neighbors and select some informative neighors is required.\nFor each node, node-level attention aims to learn the importance of meta-path based neighbors and assign different attention values to them.\nStill taking IMDB as an example, \nwhen using the meta-path Movie-Director-Moive (the movies are with the same director), \\emph{The Terminator}  will connect to \\emph{Titanic} and \\emph{The Terminator 2}  via director \\emph{James Cameron}.\n%\t\t\\emph{The Terminator} can connect to \\emph{Titanic} or \\emph{The Terminator 2} via meta-path \\emph{MDM} (all the movies are directed by \\emph{James Cameron}).  \\emph{The Terminator}  and \\emph{The Terminator 2} are both sci-fi movies wherese \\emph{Titanic} is  a romantic movie.\nTo better identify the genre of \\emph{The Terminator} as sci-fi movie, the model should pay more attention to \\emph{The Terminator 2}, rather than \\emph{Titanic}. Therefore, how to design a model which can discover the subtle differences of neighbors and learn their weights properly will be desired.\n%\twill be desired.\n\n% $Schwarzenegger$ may be the most representative actor.%, $Schwarzenegger$\n% s2. Node-level attention mechanism\n\n%\t It requires that the nodes with different types should be in different spaces. \n%\tHow to preserve meta-path based\n%\tsemantic relations among different  nodes \n%\tthe heterogeneous relations between nodes \n%\tand incorporate node feature into a unified analytics is an urgent problem that needs to be solved. \n%\\end{itemize}\n\nIn this paper, \n% to tackle the above fundamental problems, \nwe propose a novel \n\\textbf{H}eterogeneous graph \\textbf{A}ttention \\textbf{N}etwork, named HAN, which considers both of node-level and semantic-level attentions.\nIn particular, given the node features as input, we use the type-specific transformation matrix to project different types of node features into the same space.\nThen the node-level attention is able to learn the attention values between the nodes and their meta-path based neighbors, while the semantic-level attention aims to learn the attention values of different meta-paths for the specific task in the heterogeneous graph. Based on the learned attention values in terms of the two levels, our model can get the optimal combination of neighbors and multiple meta-paths in a hierarchical manner, which enables the learned node embeddings to better capture the complex structure and rich semantic information  in a heterogeneous graph.\nAfter that, \n%\tsemantic-level attention can learn the importance of each meta-path and get the optimal weighted combination of  the semantic-specific node embedding for the specific task. \n%\tFinally, with the guide of a few labeled data, % for the specific task, \nthe overall model can be optimized via backpropagation in an end-to-end manner.\n% Extensive experiments on various heterogeneous graph datasets, in comparison with the state-of-the-arts, are conducted on two network tasks to assess the performance of HAT. Moreover, the parameter analysis and visualization well demonstrate the interpretability of HAT.\n\nThe contributions of our work are summarized as follows:\n\n\\textbullet\\  To our best knowledge, this is the first attempt to study the heterogeneous graph neural network based on attention mechanism. Our work enables the graph neural network to be directly applied to the heterogeneous graph, and further facilitates the heterogeneous graph based applications.\n\t%\tinvestigate to formalize the heterogeneous graph neural network, which can handle arbitrary heterogeneous graph structure (various types of nodes and relations) and learn the node representation based on structural information and feature information.\n\t%\t attempt to generalize attention mechanism to heterogeneous graph neural network framework.\n\t\n\\textbullet\\   We propose a novel heterogeneous graph attention network (HAN) which includes both of the node-level and semantic-level attentions.\n\tBenefitting from such hierarchical attentions, the proposed HAN can take the importance of nodes and meta-paths into consideration simultaneously. Moreover, our model is high efficiency, with the linear complexity with respect to the number of meta-path based node pairs, which can be applied to large-scale heterogeneous graph.\n\t\n\\textbullet\\  \tWe conduct extensive experiments to evaluate the performance of the proposed model. \n\tThe results show the superiority of the proposed model by comparing with the state-of-the-art models.\n\t%\tThe results not only show the superiority of the proposed model and but also\n\tMore importantly, by analysing \n\tthe hierarchical attention mechanism, the proposed HAN\n\tdemonstrates its potentially good interpretability for heterogeneous graph analysis.\n\n\n%\t\tWe investigate the problem of the attention mechanism in heterogeneous graph data. \n%\tTo our best knowledge, \n%\tthis is the first attempt to generalize attention mechanism to heterogeneous graph neural network framework.\n%\t\t\\textbullet ~To our best knowledge, this is the first attempt to generalize attention mechanism to heterogeneous graph neural\n%\t\tnetwork framework.\n%\t\tTo our best knowledge, this is the first attempt to design a semi-supervised graph neural network for heterogeneous graph  based solely on attention mechanism. The proposed HAN not only handle arbitrary heterogeneous graph structure(various types of nodes and relations) but also can learn the subtle difference of nodes and meta-paths. \n\n\n%\t\t\\textbullet ~\n%\tWe propose a novel semi-supervised graph neural network for heterogeneous graph which include node-level and semantic-level attentions.\n%\tBenefit from node-level and semantic-level attentions, the proposed HAN can take the importance of nodes and meta-paths into consideration in a hierarchical manner. \n%\tBenefit from node-level and semantic-level attentions, the proposed HAN can preserve the complex structure and capture the rich semantic information from heterogeneous graph.\n%\textract the complex structure and rich semantic information from heterogeneous graph.\n%\t into account simultaneously, which can\n%\textract the complex structure and rich semantic information from heterogeneous graph.\n%\tMeanwhile, the proposed HAN utilizes the structure and feature information in an uniform way.\n%  in a uniform and elegant way.\n%\tMoreover, our model is high efficiency, with the linear complexity with respect to the number of meta-path based node pairs, which can be applied to large-scale heterogeneous graph.\n\n%\t\t\\textbullet ~\n%\tWe conduct extensive experiments to evaluate the performance of the proposed model. \n%\tThe results show the superiority of the proposed model by comparing with the state-of-the-art models.\n%%\tThe results not only show the superiority of the proposed model and but also\n%More important, via analysing \n%the hierarchical attention mechanism, the proposed HAN\n%\t demonstrates its  potentially good interpretability for heterogeneous graph analysis.\n% by comparing with the state-of-the-art algorithms.\n% WHAN's more, via analysing \n%\tthe hierarchical attention mechanism.\n% , the proposed model has better interpretability.\n% analyzing\n%\\end{itemize}\n\n%The remainder of this paper is organized as follows. Section 2\n%introduces the related works and Section 3 describes the preliminaries.\n% notations used\n%in the paper and presents some preliminary knowledge. Then, We\n%propose the heterogeneous graph attention network in\n% Section 4 details our proposed HAN model. \n%  Section 5 reports the experimental results, and Section 6 concludes the paper.\n% Experiments and detailed analysis are\n%reported in Section 5. Finally, we conclude the paper in Section 6.\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n% We first introduce the related methods of graph neural network, \n% and briefly teased out some network embedding algorithms.\n%We first provide an overview of graph neural networks. Moreover, since graph neural networks, \n%as a network representation learning method, are closely related with network embedding,\n%we will also introduce the related network embedding methods here.\n",
                "subsection 2.1": {
                    "name": "Graph Neural Network",
                    "content": "\nGraph neural networks (GNNs) which aim to extend the deep neural network to deal with arbitrary graph-structured data are introduced in \\cite{gnn05,gnn09}. Yujia Li et al. \\cite{ggnn} proposes a propagation model which can incorporate gated recurrent units to propagate information across all nodes. Recently, there is a surge of generalizing convolutional operation on the graph-structured data. The graph convolutional neural work generally falls into two categories, namely spectral domain and non-spectral domain. On one hand, spectral approaches work with a spectral representation of the graphs. Joan Bruna et al. \\cite{gft14iclr} extends convolution to general graphs by finding the corresponding Fourier basis. \n%\tRather than Fourier transform,\nMicha{\\\"e}l et al. \\cite{16chebyshev} utilizes\nK-order Chebyshev polynomials to approximate smooth filters in the spectral domain. Kipf et al. \\cite{gcn} proposes a spectral approach, named Graph Convolutional Network,\nwhich designs a graph\nconvolutional network via a localized first-order approximation of spectral graph convolutions. \n% some works \\cite{gnn05} try to generalize convolutional network to graph-structured data. \nOn the other hand, we also have non-spectral approaches, which define convolutions directly on the graph, operating on groups of spatially close neighbors. Hamilton et al. \\cite{graphsage} introduces GraphSAGE which \n%\tsamples a fixed size node neighbors,\n%\tMeanwhile, \\cite{graphsage} introduces GraphSAGE which samples a \ufb01xed size node neighbors, \n%\tand \nperforms a neural network based aggregator over a fixed size node neighbor. It can learn a function that generates embeddings by aggregating features from a node\u2019s local neighborhood.\n%The node embedding can\n%\tSo it inherently limits the set of neighbors visible and drops some information.\n%\tArguably the most popular approach in recently has been\n%\t% For example, \n%\tthe Graph Convolutional Network (GCN) of \\cite{gcn}, \n\n\n%Although some previous works introduce the attention mechanism for heterogeneous graph based applications, e.g., the recommendation \\cite{mcrec,han2018aspect}. However, in essence, they just leverage heterogeneous information to improve the performance of applications, and they are not specifically designed for the framework of heterogeneous graph based neural network. \n\n%TODO\n%\n%intro han hu\n\n\nAttention mechanisms, e.g.,\nself-attention  \\cite{attisallyouneed} and soft-attention \\cite{bahdanau2014neural}, \nhave become one of the most influential mechanisms in deep learning. Some previous works introduce the attention mechanism for graph based applications, e.g., the recommendation \\cite{mcrec,han2018aspect}. \nInspired by attention mechanism, \n%a very recent work \nGraph Attention Network \\cite{gat} is proposed to learn the importance between nodes and its neighbors and fuse the neighbors to \n%and fuse the information supported by neighbors adequately to \n%and \nperform node classification. \nHowever, the above graph neural network cannot deal with various types of nodes and edges and can only be applied to the homogeneous graphs. \n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Network Embedding",
                    "content": "\n\nNetwork embedding, i.e., network representation learning (NRL), is proposed to embed network into a low dimensional space while preserving the network structure and property so that the learned embeddings can be applied to the downstream network tasks.\nFor example, the random walk based methods \\cite{perozzi2014deepwalk,grover2016node2vec}, %[deepwalk, node2vec], \nthe deep neural network based methods \\cite{wang2016structural}, the matrix factorization based methods \\cite{hope,mnmf}, and others, e.g., LINE \\cite{Tang2015LINELI}. However, all these algorithms are proposed for the homogeneous graphs. Some elaborate reviews\ncan be found in \\cite{cui2018survey,goyal2017graph}.\n\nHeterogeneous graph embedding mainly focuses on preserving the meta-path based structural information.\nESim \\cite{Shang2016MetaPathGE} accepts user-defined meta-paths as guidance to learn vertex vectors in a user-preferred embedding space for similarity search. \nEven through ESim can utilize multiple meta-paths, it cannot learn the importance of meta-paths. To achieve the best performance, ESim needs to conduct grid search to find the optimal weights of hmeta-paths. It is pretty hard to find the optimal combination for specific task.\n% propose a meta-path guided embedding method to perform similarity search for HINs. \nMetapath2vec \\cite{Dong2017metapath2vecSR} designs a meta-path based random walk \n%\tto extract semantic information \nand utilizes skip-gram to perform heterogeneous graph embedding. However, metapath2vec can only utilize one meta-path and may ignore some useful information.\n%\tembed various kinds of node. \n% and apply skip-gram to learn network representations. \nSimilar to metapath2vec, HERec \\cite{HERec} proposes a type constraint strategy to filter the node sequence and capture the complex semantics reflected in heterogeneous graph.\n% ASPEM \\cite{shi2018aspem} preserves the multiple aspects  semantics information in HINs.\nHIN2Vec \\cite{Fu2017HIN2VecEM} carries out multiple prediction training tasks which learn the latent vectors of nodes and meta-paths simultaneously. Chen et al. \\cite{chen2018pme} proposes a projected metric embedding model, named PME, which can preserve node proximities via Euclidian Distance. %\tas a metric to preserve both the first- and the second-order proximity simultaneously.\nPME projects different types of node into the same relation space and conducts\n%capture both first-order and second-order proximities in a unified way \n%for \nheterogeneous link prediction.\nTo study the problem of comprehensive describe heterogeneous graph, Chen et al. \\cite{shi2018easing} proposes HEER which can embed heterogeneous graph via edge representations. Fan et al. \\cite{fan2018gotcha} proposes a embedding model metagraph2vec, where both the structures and semantics are maximally preserved for malware detection.\nSun et al. \\cite{sun2018joint} proposes meta-graph-based network embedding models, which simultaneously considers\nthe hidden relations of all meta information of a meta-graph.\nIn summary, all these aforementioned algorithms do not consider the attention mechanism in heterogeneous graph representation learning.\n\n\n%\\section{Preliminaries}\n%Meta-path can be asymmetric or symmetric.\n%Taking IMDB shown in Figure \\ref{liuchengtu}(a) as an example, symmetric meta-path $Movie \\xrightarrow{play}Actor  \\xrightarrow{play}Movie$ means the co-actor relation.\n%Meanwhile, asymmetric meta-path $Movie \\xrightarrow{shoot} Director$ means the movie is directed by the director.\n\n\n\n%\tThe complexity of heterogeneous information network drives us to provide \n%We usually exploit\n%the meta level (i.e., schema-level) description to  understand the object types and link types better in heterogeneous graph. Specifically, given the typed essence, a heterogeneous graph can be abstracted as a network schema, \n%denoted as $\\mathcal{S}=(\\mathcal{A},\\mathcal{R})$, which illustrates the object types and their interaction relations.\n%\twhich is a meta template define over object types. \n%\tFigure \\ref{fig_schema} gives network schemas for the experimental datasets.\n\n%A meta-path $\\Phi$ \\cite{sun2011pathsim} is defined as a path in the form of \n%$\\mathcal{A}_1 \\stackrel{\\mathcal{R}_1}{\\longrightarrow }\\mathcal{A}_2\\stackrel{\\mathcal{R}_2}{\\longrightarrow} ...\\stackrel{\\mathcal{R}_{l}}{\\longrightarrow}\\mathcal{A}_{l+1}$. The varying relation between  meta-path based node pair $(\\mathcal{A}_1, \\mathcal{A}_{l+1})$ can revealed by multiple meta-paths. \n\n%\t \\xrightarrow{shoot} Movie$ \n%\tmeans the two movies are directed by the same director. %attend the same conference. \n\n%\tThese meta-paths can reveal varying interaction semantics between meta-path based node pair $(\\mathcal{A}_1, \\mathcal{A}_{l+1})$.\n\n\n"
                }
            },
            "section 3": {
                "name": "Preliminary",
                "content": "\nA heterogeneous  graph is a special kind of information network, which contains either multiple types of objects or multiple types of links. \n\n\n\\begin{definition}{\\textbf{ Heterogeneous  Graph }\\cite{sun2013mining}.}\n\tA  heterogeneous graph, denoted as $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$, consists of an object set $\\mathcal{V}$ and a link set $\\mathcal{E}$.\n\t%\tand an attribute information matrix \\mathbf{X}. \n\tA heterogeneous graph is also associated with a node type mapping function $\\phi:\\mathcal{V}\\rightarrow \\mathcal{A}$ and \n\ta link type mapping function $\\psi: \\mathcal{E}\\rightarrow \\mathcal{R}$. $\\mathcal{A}$ and $\\mathcal{R}$ denote \n\tthe sets of predefined object types and link types, where $|\\mathcal{A}|+|\\mathcal{R}|>2$.\n\\end{definition}\n\n\\textbf{Example}. As shown in Figure \\ref{fig_hin}(a), we construct a heterogeneous graph to model the IMDB. It consists of multiple types of objects ( Actor (A), Movie (M), Director (D))  and relations (shoot relation between movies and directors, role-play relation between actors and movies). \n%\n%The complexity of heterogeneous graph drives us to provide the meta level (e.g., schema-level) description for understanding the object types and link types better in the network. Hence, the concept of network schema is proposed to describe the meta structure of a network.\n\n%\\begin{definition}\\textbf{Network schema} \\cite{sun2013mining,sun2009ranking}.\n%\tThe network schema is denoted as $\\mathcal{S} = (\\mathcal{A}, \\mathcal{R})$. It is a meta template\n%\tfor an information network $\\mathcal{G} = \\{\\mathcal{V}, \\mathcal{E}\\}$ with the object type mapping $\\phi: \\mathcal{V} \\rightarrow \\mathcal{A}$ and the link type mapping $\\psi: \\mathcal{E} \\rightarrow \\mathcal{R}$, which is a directed graph defined over object types $\\mathcal{A}$, with edges as relations from $\\mathcal{R}$.\n%\\end{definition}\n%\\textbf{Example}.\n%We present the network schema of IMDB in Figure \\ref{fig_schema}(b), consisting of multiple types of objects. \n%We also show  the network schema of DBLP and ACM  in Figure \\ref{fig_schema}(a) and Figure \\ref{fig_schema}(c), respectively.\n\n\n\nIn heterogeneous graph, two objects can be connected via different semantic paths, which are called meta-paths.\n%\\begin{definition}{\\textbf{Meta-path}\\citep{sun2011pathsim}\n%%\\textbf{Meta-path}} \\citep{sun2011pathsim}. \n%A meta-path $\\Phi$ is defined as a path in the form of $A_1 \\xrightarrow{R_1} A_2 \\xrightarrow{R_2} \\cdots \\xrightarrow{R_l} A_{l+1}$ (abbreviated as $A_1A_2 \\cdots A_{l+1}$), which describes a composite relation $R = R_1 \\circ R_2 \\circ \\cdots \\circ R_l$ between object $A_1$ and $A_{l+1}$, where $\\circ$ denotes the composition operator on relations.\n%\n%\\end{definition}\n\\begin{definition}{\\textbf{Meta-path }\\cite{sun2011pathsim}.}\n\t%\\textbf{Meta-path}} \\citep{sun2011pathsim}. \n\tA meta-path $\\Phi$ is defined as a path in the form of $A_1 \\xrightarrow{R_1} A_2 \\xrightarrow{R_2} \\cdots \\xrightarrow{R_l} A_{l+1}$ (abbreviated as $A_1A_2 \\cdots A_{l+1}$), which describes a composite relation $R = R_1 \\circ R_2 \\circ \\cdots \\circ R_l$ between objects $A_1$ and $A_{l+1}$, where $\\circ$ denotes the composition operator on relations.\n\\end{definition}\n\n\n\n\n\n\\textbf{Example}. As shown in Figure \\ref{fig_hin}(a), two movies can be connected via multiple meta-paths, e.g., Movie-Actor-Movie (\\emph{MAM}) and Movie-Director-Movie (\\emph{MDM}). Different meta-paths always reveal different semantics. For example, the \\emph{MAM} means the co-actor relation, while Movie-Director-Movie (\\emph{MDM}) means they are directed by the same director.\n\n\n%Meta-path, as a powerful graph mining method, has been widely used in heterogeneous graph mining.\nGiven a meta-path $\\Phi$, there exists a set of meta-path based neighbors of each node which can reveal diverse structure information and rich semantics in a heterogeneous graph. \n\\begin{definition}\\textbf{Meta-path based Neighbors.}\n\tGivien a node $i$ and a meta-path $\\Phi$ in a heterogeneous graph, the meta-path based neighbors $\\mathcal{N}_i^{\\Phi}$ of node $i$ are defined as the set of nodes which connect with node $i$ via meta-path $\\Phi$. Note that the node's neighbors includes itself.\n\\end{definition}\n\\textbf{Example}. Taking Figure \\ref{fig_hin}(d) as an example, given the meta-path Movie-Actor-Movie, the meta-path based neighbors of $m_1$ includes $m1$ (itself), $ m_2$ and $m_3$. Similarly, the neighbors of $m_1$ based on meta-path Movie-Director-Movie includes $m_1$ and  $m_2$. Obviously, meta-path based neighbors can exploit different aspects of structure information in heterogeneous graph. We can get meta-path based neighbors by\nthe multiplication of a sequences of adjacency matrices.\n%Furthermore, we model the heterogeneous graph neural network which can learning node representation via integrating rich features and diverse relations. Here we define graph neural network under the scenario of heterogeneous graph as follows. \n%\\begin{definition}\\textbf{Heterogeneous graph neural network.} Heterogeneous graph neural network is a semi-supervised graph neural network which can deal with various kinds of objects and their interactions for graph mining. With the guide of a few label, such a neural network learn the node embedding $\\mathbf{Z}$ based on network structure (e.g., $\\mathcal{A}, \\mathcal{R}$) and node feature (e.g., $\\mathbf{h}$).\n%\\end{definition}\n\nGraph neural network has been proposed to deal with arbitrary graph-structured data. However, all of them are designed for homogeneous network \\cite{gcn,gat}. Since meta-path and meta-path based neighbors are two fundamental structures in a heterogeneous graph, next, we will present a novel graph neural network for heterogeneous graph data, which is able to exploit the subtle difference of nodes and meta-paths. The notations we will use throughout the article are summarized in Table \\ref{tab_notation}.\n\n\n\n\n\n"
            },
            "section 4": {
                "name": "The Proposed Model",
                "content": "\nIn this section, we propose a novel semi-supervised graph neural network for heterogeneous graph.\n%To address the two challenges introduced in section 1, \nOur model follows a hierarchical attention structure: node-level attention $\\rightarrow$ semantic-level attention. Figure \\ref{liuchengtu} presents the whole framework of HAN. \nFirst, we propose a node-level attention to learn the weight of meta-path based neighbors and aggregate them to get the semantic-specific node embedding.\nAfter that, HAN can tell the difference of meta-paths via semantic-level attention and \nget the optimal weighted combination of  the semantic-specific node embedding for the specific task. \n\n%\t fuse the semantic-specific node embedding for the specific task.\n\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "Node-level Attention",
                    "content": "\nBefore aggregating the information from meta-path neighbors for each node, we should notice that the meta-path based neighbors of each node play a different role and show different importance in learning node embedding for the specific task.\nHere we introduce node-level attention can learn the importance of meta-path based neighbors for each node in a heterogeneous graph and aggregate the representation of these meaningful neighbors to form a node embedding.\n\n%Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. \n\nDue to the heterogeneity of nodes, different types of nodes have different feature spaces.\nTherefore, for each type of nodes (e.g.,node with type $\\phi_i$), we design the type-specific transformation matrix  $\\mathbf{M}_{\\phi_i}$ to project \nthe features of different types of nodes into the same feature space. Unlike \\cite{hamilton2018embedding}, the type-specific transformation matrix  is based on node-type rather than edge-type. The projection process can be shown as follows:\n\\begin{equation}\n\\mathbf{h}_i'= \\mathbf{M}_{\\phi_i} \\cdot \\mathbf{h}_i,\\\\\n%\\mathbf{h}_i'= \\mathbf{M}_{\\phi_i} \\cdot \\mathbf{x}_i,\n\\end{equation}\nwhere $\\mathbf{h}_i$  and $\\mathbf{h}_i'$ are the original and projected feature of node $i$, respectively. By type-specific projection operation, the node-level attention can handle arbitrary types of nodes.\n%\tvarious kinds of node\n%\tproposed model can caculate the similarity between transformed\n\nAfter that, we leverage self-attention  \\cite{attisallyouneed} to learn the weight among various \nkinds of nodes. \n%\tIt should be noticed that node-level attention is a shared mechanism which reused for all nodes.\n%\tthe weight of between Node i, j based on mp k\n%\tmp $\\Phi:\\phi_i\\rightarrow ... \\rightarrow \\phi_j  $\n% Assuming node $i$ and node $j$ are connected via meta-path $\\Phi$, \nGiven a node pair $(i,j)$ which are connected via meta-path $\\Phi$,\nthe node-level attention $e_{ij}^{\\Phi}$ can learn the importance $e_{ij}^{\\Phi}$ which means how important node $j$ will be for node $i$. The importance of meta-path based node pair $(i,j)$ can be formulated as follows:\n\\begin{equation}\ne_{ij}^{\\Phi}=att_{node}( \\mathbf{h}_i', \\mathbf{h}_j';\\Phi).\n\\label{eq_2}\n\\end{equation}\nHere $att_{node}$ denotes the deep neural network which performs the node-level attention. Given meta-path $\\Phi$, $att_{node}$ is shared for all meta-path based node pairs. It is because there are some similar connection patterns under one meta-path. The above Eq. (\\ref{eq_2}) shows that given meta-path $\\Phi$, the weight of meta-path based node pair $(i,j)$ depends on their features.\nPlease note that, $e_{ij}^{\\Phi}$ is asymmetric, i.e., the importance of node $i$ to node $j$ and the importance of node $j$ to node $i$ can be quite difference. It shows node-level attention can preserve the asymmetry which is a critical property of heterogenous graph.\n\n%\tIn order to tackle the challenge of various degree of nodes, we leverage \nThen we inject the structural information into the model via\n%tackle the challenge of various degree of nodes by performing \nmasked attention\n% masked attention \nwhich means we only calculate the $e_{ij}^{\\Phi}$ for nodes $j\\in \\mathcal{N}^{\\Phi}_i$, where $\\mathcal{N}^{\\Phi}_i$ denotes the meta-path based neighbors of node $i$ (include itself). After obtaining the importance between meta-path based node pairs, we normalize them to get the weight coefficient $\\alpha_{ij}^{\\Phi}$ \nvia softmax function:\n%\\begin{equation}\n%\t\\alpha_{ij}^{\\Phi}=softmax_j^{\\Phi}(a^T_{\\Phi} \\cdot [M_{\\phi_i}h_i||M_{\\phi_j}h_j]),\n%\\end{equation}\n\\begin{equation}\n\\alpha_{ij}^{\\Phi}\n=softmax_j(e_{ij}^{\\Phi})\n%=\\frac{\\exp(e_{ij}^{\\Phi})}{\\sum_{k\\in \\mathcal{N}_i^{\\Phi}} \\exp(e_{ik}^{\\Phi})} \\\\\n=\\frac{\\exp \\bigl(\\sigma(\\mathbf{a}^\\mathrm{T}_{\\Phi} \\cdot [\\mathbf{h}_i'\\Vert \\mathbf{h}_j'])\\bigl)}{\\sum_{k\\in \\mathcal{N}_i^{\\Phi}} \\exp \\bigl(\\sigma(\\mathbf{a}^\\mathrm{T}_{\\Phi} \\cdot [\\mathbf{h}_i'\\Vert \\mathbf{h}_k'])\\bigr)},    \\\\\\\n\\label{eq3}\n\\end{equation}\nwhere $\\sigma$ denotes the activation function, $\\Vert$ denotes the concatenate operation and \n$\\mathbf{a}_{\\Phi}$ is the node-level attention vector for meta-path $\\Phi$. As we can see from Eq. (\\ref{eq3}), the weight coefficient of $(i,j)$ depends on their features. Also please note that the weight coefficient $\\alpha_{ij}^{\\Phi}$ is asymmetric which means they make different contribution to each other. Not only because the concatenate order in the numerator, but also because they have different neighbors so the normalize term (denominator) will be quite difference.\n\nThen, the meta-path based embedding of node $i$ can be aggregated by the neighbor's projected features with the corresponding coefficients as follows:\n\n%\twe can weight combine the information supported by meta-path based neighbors and \n%\tget the meta-path based embedding of each node. \n%\tThe above process can be formulated as follows:\n\\begin{equation}\n\\mathbf{z}^{\\Phi}_i=\\sigma \\biggl( \\sum_{j \\in \\mathcal{N}_i^{\\Phi}} \\alpha_{ij}^{\\Phi} \\cdot \\mathbf{h}_j'  \\biggr).\n\\label{node_agg}\n\\end{equation}\nwhere $\\mathbf{z}^{\\Phi}_i$ is the learned embedding of node $i$ for the meta-path $\\Phi$. \nTo better understand the aggregating process of node-level, we also give a brief explanation in Figure \\ref{fig_explain} (a). Every node embedding is aggregated by its neighors.\n%Figure \\ref{fig_explain} (a) gives a vivid description of Eq. (\\ref{node_agg}).\nSince the attention weight $\\alpha_{ij}^{\\Phi}$ is generated for single meta-path, it is semantic-specific and able to caputre one kind of semantic information.\n%\twhere $\\mathbf{h}^{\\Phi}_i$ is the meta-path based embedding of node $i$.\n\nSince heterogeneous graph present the property of scale free,\nthe variance of graph data is quite high.\nTo tackle the above challenge,\nwe extend node-level attention to multihead attention so that the training process is more stable. Specifically, we repeat the node-level attention for $K$ times \nand concatenate the learned embeddings as the semantic-specific embedding:\n\\begin{equation}\n\\mathbf{z}^{\\Phi}_i=\n%\t \\Vert_{k=1}^K  \n\\overset{K}{\\underset{k=1}{\\Vert}}\n\\sigma \\biggl( \\sum_{j \\in \\mathcal{N}_i^{\\Phi}} \\alpha_{ij}^{\\Phi} \\cdot \\mathbf{h}_j'  \\biggr).\n% \\sigma( \\sum_{j \\in N_i^{\\Phi}} \\alpha_{ij}^{\\Phi} \\cdot M_{\\phi_j} \\cdot h_j)  )  ,\n\\end{equation}\n%\twhere $||$ denotes the concatenate operation.\nGiven the meta-path set  $\\left\\lbrace \\Phi_1,\\ldots,\\Phi_{P}\\right\\rbrace $, after feeding node features into node-level attention, \nwe can obtain $P$ groups of semantic-specific node embeddings, \ndenoted as $\\left\\lbrace \\mathbf{Z}_{\\Phi_1},\\ldots,\\mathbf{Z}_{\\Phi_{P}}\\right\\rbrace $.\n%\t, from $|P|$ meta-path based homogeneous networks.\n\n%Some explanation.\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Semantic-level Attention",
                    "content": "\nGenerally, every node in a heterogeneous graph contains multiple types of semantic information and semantic-specific node embedding can only reflect node\nfrom one aspect.\nTo learn a more comprehensive node embedding, we need to\n% describe node from multiple a\n%To give a comprehensive description of each node when learn node embedding, we need to \nfuse multiple semantics which can be revealed by meta-paths. To address the challenge of meta-path selection and semantic fusion in a heterogeneous graph, we propose a novel semantic-level attention to \nautomatically learn the importance of different meta-paths and fuse them for the specific task. \nTaking  $P$ groups of semantic-specific node embeddings learned from node-level attention as input, the learned weights of each meta-path $(\\mathbf{\\beta}_{\\Phi_1},\\ldots,\\mathbf{\\beta}_{\\Phi_{P}})$ can be shown as follows:\n\\begin{equation}\n(\\mathbf{\\beta}_{\\Phi_1},\\ldots,\\mathbf{\\beta}_{\\Phi_{P}})=att_{sem}(\\mathbf{Z}_{\\Phi_1},\\ldots,\\mathbf{Z}_{\\Phi_{P}}).\n\\end{equation}\n%\tHere we define semantic-level attention \n%Here $att_{sem}$ is performed by deep neural network.\nHere $att_{sem}$ denotes the deep neural network which performs the semantic-level attention. \nIt shows that the semantic-level attention can capture various types of semantic information behind a heterogeneous graph.\n\n%Then we give a detal description of semantic-level attention. \nTo learn the importance of each meta-path, we first transform semantic-specific embedding through a nonlinear transformation (e.g., one-layer MLP). Then we measure the importance of the semantic-specific embedding as the similarity of transformed embedding with a semantic-level attention vector $\\mathbf{q}$. Furthermore, we average the importance of all the semantic-specific node embedding which can be explained as  the importance of each meta-path.\n%Then we utilize a semantic-level attention vector $\\mathbf{q}$ to meansure the importance of semantic-specific embedding which can be explained as  the importance of each meta-path.\nThe importance of each meta-path, denoted as $w_{\\Phi_i}$, is shown as follows:\n\n\\begin{equation}\n%w_{\\Phi_i} = \\mathbf{q}^\\mathrm{T} \\cdot \\tanh(\\mathbf{W}\\cdot \\mathbf{Z}_{\\Phi_i}+\\mathbf{b}),\nw_{\\Phi_p} =\\frac{1}{|\\mathcal{V}|}\\sum_{i \\in \\mathcal{V}} \\mathbf{q}^\\mathrm{T} \\cdot \\tanh(\\mathbf{W}\\cdot \\mathbf{z}_{i}^{\\Phi_p}+\\mathbf{b}),\n\\end{equation}\nwhere $\\mathbf{W}$ is the weight matrix,  $\\mathbf{b}$ is the bias vector, $\\mathbf{q}$ is the semantic-level attention vector. Note that for the meaningful comparation, all above parameters are shared for all meta-paths and semantic-specific embedding.\nAfter obtaining the importance of each meta-path, we normalize them via softmax function. \nThe weight of meta-path $\\Phi_i$, denoted as  $\\beta_{\\Phi_i}$,  can be obtained by normalizing the above importance of all meta-paths using softmax function,\n\\begin{equation}\n\\beta_{\\Phi_p}=\\frac{\\exp(w_{\\Phi_p})}{\\sum_{p=1}^{P} \\exp(w_{\\Phi_p})} ,\n\\end{equation}\nwhich can be interpreted as the contribution of the meta-path $\\Phi_p$ for specific task. Obviously, the higher $\\beta_{\\Phi_p}$, the more important meta-path $\\Phi_p$ is. Note that for different tasks, meta-path $\\Phi_p$ may has different weights.\nWith the learned weights as coefficients, \nwe can fuse these semantic-specific embeddings to obtain the final embedding $\\mathbf{Z}$ as follows:\t\n\\begin{equation}\n\\mathbf{Z}=\\sum_{p=1}^{P} \\beta_{\\Phi_p}\\cdot \\mathbf{Z}_{\\Phi_p}.\n\\label{sem_agg}\n\\end{equation}\nTo better understand the aggregating process of semantic-level, we also give a brief explanation in Figure \\ref{fig_explain} (b). The final embedding is aggregated by all semantic-specific embedding.\nThen we can apply the final embedding to specific tasks and design different loss fuction. \n%For example, in node classification, we can minimize the Cross-Entropy between the ground-truth and the prediction:\nFor semi-supervised node classification, we can minimize the Cross-Entropy over all labeled node between the ground-truth and the prediction:\n\\begin{equation}\nL=-\\sum_{l \\in \\mathcal{Y}_{L}} \\mathbf{Y}^{l} \\ln (\\mathbf{C}\\cdot \\mathbf{Z}^{l}),\n%L=-(\\mathbf{Y} \\log (\\mathbf{C}\\cdot \\mathbf{H}) + (1-\\mathbf{Y})\\log (1-\\mathbf{C} \\cdot \\mathbf{H})),\n%%L = -\\sum y_ilog(C)\n\\end{equation}\nwhere $\\mathbf{C}$ is the parameter of the classifier,\n$\\mathcal{Y}_L$ is the set of node indices that have labels,\n$\\mathbf{Y}^{l}$ and $\\mathbf{Z}^{l}$ are the labels and embeddings of labeled nodes.\n% $\\mathbf{Y}$ is the label of $\\mathbf{H}$. \nWith the guide of labeled data, we can optimize the proposed model via back propagation and learn the embeddings of nodes. The overall process of HAN in shown in Algorithm \\ref{alg:aggre}.\n% \\usepackage{multirow}\n\n\n%hh\n%\\begin{table*}[]\n%\t\\centering\n%\t\\caption{Statistics of the datasets.}\n%\t\\label{table_datasets}\n%\t\\begin{tabular}{|c|c|c|c|c|c|c|}\n%\t\t\\hline\n%\t\tDatasets & \\multicolumn{2}{c|}{Relations} & Fea.                  & Train                & Val.                 & Test                  \\\\ \\hline\n%\t\t\\multirow{2}{*}{ACM}  & PA           & 9744            & \\multirow{2}{*}{1830} & \\multirow{2}{*}{600} & \\multirow{2}{*}{300} & \\multirow{2}{*}{2125} \\\\ \\cline{2-3}\n%\t\t& PS           & 3025            &                       &                      &                      &                       \\\\ \\hline\n%\t\t\\multirow{3}{*}{DBLP} & PA           & 19645           & \\multirow{3}{*}{334}  & \\multirow{3}{*}{800} & \\multirow{3}{*}{400} & \\multirow{3}{*}{2857} \\\\ \\cline{2-3}\n%\t\t& PC           & 14328           &                       &                      &                      &                       \\\\ \\cline{2-3}\n%\t\t& PT           & 88420           &                       &                      &                      &                       \\\\ \\hline\n%\t\t\\multirow{2}{*}{IMDB} & MA           & 14340           & \\multirow{2}{*}{1232} & \\multirow{2}{*}{300} & \\multirow{2}{*}{300} & \\multirow{2}{*}{2687} \\\\ \\cline{2-3}\n%\t\t& MD           & 4780            &                       &                      &                      &                       \\\\ \\hline\n%\t\\end{tabular}\n%\t\n%\\end{table*}\n%\n%nn\n%%%% 0122 \u7f29\u51cf\u7248\uffe5%%%%%%%%\n%\\begin{algorithm}[ht]\n%\t%     \\SetAlgoNoLine % \u4e0d\u8981\u7b97\u6cd5\u4e2d\u7684\u7ad6\u7ebf\n%\t\\SetKwInOut{Input}{\\textbf{Input}}\\SetKwInOut{Output}{\\textbf{Output}} % \u66ff\u6362\u5173\u952e\u8bcd\n%\t\n%\t\\Input{\n%\t\tThe heterogeneous  graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, \\\\\n%\t\tThe node feature \\{$\\mathbf{h}_i, \\forall i\\in \\mathcal{V}$\\},\\\\\n%\t\tThe meta-path set $ \\left\\lbrace \\Phi_0,\\Phi_1,\\ldots,\\Phi_{P}\\right\\rbrace $. \\\\\n%\t\tThe number of attention head $K$, \\\\\n%\t\t% meta-path length $K$, \\\\\n%\t\t% differentiable aggregator functions ${AGGREGATE_k}, \\forall k\\in\\{1,\\ldots,K-1\\}$\n%\t\t% Aggregator Functions ${AGGREGATE_k}, \\forall k\\in\\{1,\\ldots,K-1\\}$\n%\t}\n%\t\\Output{\n%\t\tThe final embedding $\\mathbf{Z}$ ,\\\\\n%\t\tThe node-level attention weight $\\alpha$ ,\\\\\n%\t\tThe semantic-level attention weight $\\beta$ .\\\\\n%\t}\n%\t\\BlankLine\n%\t\n%\t\n%\tType-specific transformation $\\mathbf{h}_i' \\leftarrow \\mathbf{M}_{\\phi_i} \\cdot \\mathbf{h}_i$ \\;\n%\t\\For {$ \\Phi_i  \\in \\left\\lbrace \\Phi_0,\\Phi_1,\\ldots,\\Phi_{P}\\right\\rbrace $}{\n%\t\t\n%\t\t%\t$\\mathbf{h}^{K}_v \\leftarrow \\mathbf{e}_v,\\forall v\\in\\mathcal{V}$  \\; % \u5206\u53f7 \\; \u533a\u5206\u4e00\u884c\u7ed3\u675f\n%\t\t\n%\t\t\n%\t\t\\For {$k=1...K$}{\n%\t\t\t\n%\t\t\t\\For {$i \\in \\mathcal{V}$ }{\n%\t\t\t\tFind meta-path based neighbors $\\mathcal{N}^{\\Phi}_i $ \\;\n%\t\t\t\t\n%\t\t\t\t\\For {$j \\in \\mathcal{N}^{\\Phi}_i $}{\n%\t\t\t\t\tLearn the weight coefficient $\\alpha^{\\Phi}_{ij}$ using Eq. (3)\\;\n%\t\t\t\t\t\n%\t\t\t\t\t\n%\t\t\t\t\t%\t\t\t$\\mathbf{h}^k_{v^{\\prime\\prime}}\\leftarrow CONCAT(\\mathbf{h}^{K}_{v^{\\prime\\prime}}, \\mathbf{h}^k_{\\mathcal{N}^t(v^{\\prime\\prime})})$\\;\n%\t\t\t\t}\n%\t\t\t\tLearn the semantic-specific node embedding $\\mathbf{z}^{\\Phi}_i  $ using Eq. (4)\\;\n%\t\t\t}\n%\t\t\t\n%\t\t\tConcatenate the learned embeddings from all attention head $\\mathbf{z}^{\\Phi}_i  $ using Eq. (5) \\;\n%\t\t}\n%\t\t%sss \\;\n%\t\t\n%\t\tLearn the weight of meta-path $\\beta_{\\Phi_i}$ using Eq. (8)\\;\n%\t\t\n%\t\tFuse the semantic-specific embedding $\\textbf{Z} $ using Eq. (9) \\;\n%\t\t\n%\t}\n%\tCalculate Cross-Entropy $L$ using Eq. (10) \\;\n%\t\n%\tBack propagation and update parameters in HAN\\;\n%\t%\t\tEnd-to-end optimization \\;\n%\t\\Return{$Z, \\alpha, \\beta$. }\n%\t\\caption{The overall process of HAN.}\n%\t\\label{alg:aggre}\n%\\end{algorithm}\n%\n%%%%%%%%%%%%0122 \u6ce8\u91ca\u7248\u672c%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{algorithm}[ht]\n\t%     \\SetAlgoNoLine % \u4e0d\u8981\u7b97\u6cd5\u4e2d\u7684\u7ad6\u7ebf\n\t\\SetKwInOut{Input}{\\textbf{Input}}\\SetKwInOut{Output}{\\textbf{Output}} % \u66ff\u6362\u5173\u952e\u8bcd\n\t\n\t\\Input{\n\t\tThe heterogeneous  graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, \\\\\n\t\tThe node feature \\{$\\mathbf{h}_i, \\forall i\\in \\mathcal{V}$\\},\\\\\n\t\tThe meta-path set $ \\left\\lbrace \\Phi_0,\\Phi_1,\\ldots,\\Phi_{P}\\right\\rbrace $. \\\\\n\t\tThe number of attention head $K$, \\\\\n\t\t% meta-path length $K$, \\\\\n\t\t% differentiable aggregator functions ${AGGREGATE_k}, \\forall k\\in\\{1,\\ldots,K-1\\}$\n\t\t% Aggregator Functions ${AGGREGATE_k}, \\forall k\\in\\{1,\\ldots,K-1\\}$\n\t}\n\t\\Output{\n\t\tThe final embedding $\\mathbf{Z}$ ,\\\\\n\t\tThe node-level attention weight $\\alpha$ ,\\\\\n\t\tThe semantic-level attention weight $\\beta$ .\\\\\n\t}\n\t\\BlankLine\n\t\\For {$ \\Phi_i  \\in \\left\\lbrace \\Phi_0,\\Phi_1,\\ldots,\\Phi_{P}\\right\\rbrace $}{\n\t\t\n\t\t%\t$\\mathbf{h}^{K}_v \\leftarrow \\mathbf{e}_v,\\forall v\\in\\mathcal{V}$  \\; % \u5206\u53f7 \\; \u533a\u5206\u4e00\u884c\u7ed3\u675f\n\t\t\n\t\t\n\t\t\\For {$k=1...K$}{\n\t\t\tType-specific transformation $\\mathbf{h}_i' \\leftarrow \\mathbf{M}_{\\phi_i} \\cdot \\mathbf{h}_i$ \\;\n\t\t\t\\For {$i \\in \\mathcal{V}$ }{\n\t\t\t\tFind the  meta-path based neighbors $\\mathcal{N}^{\\Phi}_i $ \\;\n\t\t\t\t\n\t\t\t\t\\For {$j \\in \\mathcal{N}^{\\Phi}_i $}{\n\t\t\t\t\tCalculate the weight coefficient $\\alpha^{\\Phi}_{ij}$ \\;\n\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t%\t\t\t$\\mathbf{h}^k_{v^{\\prime\\prime}}\\leftarrow CONCAT(\\mathbf{h}^{K}_{v^{\\prime\\prime}}, \\mathbf{h}^k_{\\mathcal{N}^t(v^{\\prime\\prime})})$\\;\n\t\t\t\t}\n\t\t\t\tCalculate the semantic-specific node embedding $\\mathbf{z}^{\\Phi}_i \\leftarrow \\sigma \\biggl( \\sum_{j \\in \\mathcal{N}_i^{\\Phi}} \\alpha_{ij}^{\\Phi} \\cdot \\mathbf{h}_j'  \\biggr)$\\;\n\t\t\t}\n\t\t\tConcatenate the learned embeddings from all attention head $\\mathbf{z}^{\\Phi}_i \\leftarrow\n\t\t\t%\t \\Vert_{k=1}^K  \n\t\t\t\\overset{K}{\\underset{k=1}{\\Vert}}\n\t\t\t\\sigma \\biggl( \\sum_{j \\in \\mathcal{N}_i^{\\Phi}} \\alpha_{ij}^{\\Phi} \\cdot \\mathbf{h}_j'  \\biggr)$ \\;\n\t\t}\n\t\t%sss \\;\n\t\tCalculate the weight of meta-path $\\beta_{\\Phi_i}$ \\;\n\t\tFuse the semantic-specific embedding $\\textbf{Z} \\leftarrow \\sum_{i=1}^{P} \\beta_{\\Phi_i}\\cdot \\textbf{Z}_{\\Phi_i}$ \\;\n\t}\n\tCalculate Cross-Entropy $L=-\\sum_{l \\in \\mathcal{Y}_{L}} \\mathbf{Y}_{l} \\ln (\\mathbf{C}\\cdot \\mathbf{Z}_{l})$ \\;\n\tBack propagation and update parameters in HAN\\;\n\t%\t\tEnd-to-end optimization \\;\n\t\\Return{$Z, \\alpha, \\beta$. }\n\t\\caption{The overall process of HAN.}\n\t\\label{alg:aggre}\n\\end{algorithm}\n%\n%%Tss\n\n"
                },
                "subsection 4.3": {
                    "name": "Analysis of the Proposed Model",
                    "content": "\n% \\subsubsection{Complexity}\nHere we give the analysis of the proposed HAN as follows:\n\n%\\begin{enumerate}[(1)]\n%\t\\item \n\t\\textbullet\\ The proposed model can deal with various types of nodes and relations and fuse rich semantics in a heterogeneous graph. The information can transfer from one kind of nodes to another kind of nodes via diverse relation.  Benefitted from such a heterogeneous graph attention network, different types of node embedding can enhance the mutual integration, mutual promotion and mutual upgrade. \n\t%\tthe proposed model can give a comprehensive description of each node.\n\t%\t\\item\n\t%\tOur model is a general framework, and many existing models can be considered as our special case. For example, if we only utilize node-level attention for single symmetric meta-path, our model is semantically equivalent to the GAT.\n\t\n\\textbullet\\\n\tThe proposed HAN is highly efficient and can be easily parallelized.\n\t%  which means attention computation \n\t% \n\tThe computation of attention can compute individually across all nodes and meta-paths.\n\tGiven a meta-path $\\Phi$, the time complexity of \n\tnode-level attention is $O(V_{\\Phi} F_1F_2 K+ E_{\\Phi}F_1K)$, \n\t% where $F_{\\Phi}^1F_{\\Phi}^2$ is the shape of the transform matrix,\n\twhere $K$ is the number of attention head,\n\t% where  is the shape of transform matrix features\uff0c\n\t$V_{\\Phi}$ is the number of nodes,  \n\t$E_{\\Phi}$ is the number of meta-path based node pairs, \n\t$F_1$ and $F_2$ are the numbers of rows and columns of the transformation matrix, respectively.\n\tThe overall complexity is linear to the number of nodes and \n\tmeta-path based node pairs. The proposed model can be easily parallelized, because the node-level and semantic-level attention can be parallelized across node paris and meta-paths, respectively.\n\tThe overall complexity is linear to the number of nodes and meta-path based node pairs.\n\t\n\\textbullet\\\n\tThe hierarchical attention is shared for the whole heterogeneous graph \n\twhich means the number of parameters is not \n\tdependent on the scale of a heterogeneous graph and can be used for the inductive problems \\cite{graphsage}. \n\tHere inductive means the model  can generate node embeddings for previous unseen nodes or even unseen graph. \n\t\n\\textbullet\\\n\tThe proposed model has potentionally good interpretability for the learned node embedding which is a big advantage for heterogeneous graph analysis.\n\t%\thas potentially good interpretability for\n\tWith the learned importance of nodes and meta-paths, \n\tthe proposed model can pay more attention to some meaningful nodes or meta-paths for the specific task and give a more comprensive description of a heterogeneous graph. Based on the attention values, we can check which nodes or meta-paths make the higher (or lower) contributions for our task, which is beneficial to analyze and explain our results.\n\t%\tIt is obvious that we check which nodes or meta-paths make the higher (or lower) contributions for better explaination.\n\t%\t\tFurthermore, analyzing the learned\n\t%\t\tattention weights may enable a leap in model interpretability.\n\t% \\item\n\t\n\t\n\t%\\end{itemize}\n\t% Desirable properties for a graph convolutional layer:\n\t\n\t% Computational and storage ef\ufb01ciency (\u223c O(V + E)); \n\t% Fixed number of parameters (independent of input size); \n\t% Localisation (acts on a local neighbourhood of a node); \n\t% Specifying different importance to different neighbours; \n\t% Applicability to inductive problems.\n\t\n\t\n\t% Computationally ef\ufb01cient: attention computation can be parallelized across all edges of the graph, \n\t% and aggregation across all nodes!\n\t% Storage ef\ufb01cient\u2014a sparse version does not require storing more than O(V + E) entries anywhere; \n\t% Fixed number of parameters (dependent only on the desirable feature count, not on the node count); \n\t% Trivially localised (as we aggregate only over neighbourhoods); \n\t% Allows for (implicitly) specifying different importance to different neighbours.\n\t\n\t% Readily applicable to inductive problems (as it is a shared edge-wise mechanism)!\n\t% The proposed HAT is highly efficient in both computation and storage.\n\t% % It can be  easily parallelized.\n\t% node-level attention can be parallelized across all meta-path based node pairs and \n\t% semantic-level attention can be parallelized for each meta-path.\n\t% % among all meta-paths.\n\t% % No eigendecompositions or similar costly matrix operations are required. \n\t% Given the meta-path $\\Phi$, the time complexity of each node-level attention \n\t% % computing F 0 features may \n\t% can be expressed as $O(V FF_0 K+ EF_0K)$, \n\t% where $F_{\\Phi}$ is the number of input features, \n\t% and $V_{\\Phi}$ and $E_{\\Phi}$ are the numbers of nodes and edges for each meta-path, \n\t% respectively. \n\t% % This complexity is on par with the baseline methods, such as .\n\t% % such as Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017). \n\t% \\subsubsection{semantic-level Attention}%#Optimization for each meta-path}\n\t% Applying multi-head attention multiplies the storage and parameter requirements by a factor of K, \n\t% while the individual heads\u2019 computations are entirely independent and can be parallelized.\n\t% \\subsubsection{Overall.}\n\t% ss\n\t% \\subsubsection{Parallelization.}\n\t% Our model can be easily parallelized\n\t% node-level attention can be parallelized across all meta-path based node pairs and \n\t% semantic-level attention can be parallelized for each meta-path.\n\t\n\t%\\begin{figure}\n\t%\t%\t\\vspace{2.5cm}\n\t%\t\\centering\n\t%\t\\includegraphics[width=5cm]{fig/schema.eps}\n\t%%\t\tpricai_0323_v2.eps}\n\t%\t\\caption{Network schema of HINs for the experimental datasets.}\n\t%\t\\label{fig_schema}\n\t%\\end{figure}\n%\\end{enumerate}\n\n\n\n\n\n\n\n\n\n%\\subsection{Explanation}\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n",
                "subsection 5.1": {
                    "name": "Datasets",
                    "content": "\nThe detailed descriptions of the heterogeneous graph used here are shown in Table \\ref{table_datasets}.\n%, and their schemas are shown in Figure \\ref{fig_schema}.\n\n\n\\textbullet\\\t\\textbf{DBLP\\footnote{https://dblp.uni-trier.de}.} We\n\textract a subset of DBLP which contains 14328 papers (P), 4057 authors (A), 20 conferences (C), 8789 terms (T). \n\tThe authors are divided into four areas: \\emph{database, data mining, machine learning, information retrieval}.\n\t% We obtain the ground truth from the dataset \\(dblp-4area\\) \\cite{sun2011pathsim}, which\n\tAlso, we \n\tlabel each author's research area according to the conferences they submitted. \n\t% We select some keywords in papers as the feature of authors.\n\tAuthor features are the elements of a bag-of-words represented of keywords.\n\tHere we employ the\n\tmeta-path set \\{\\emph{APA}, \\emph{APCPA}, \\emph{APTPA}\\} to perform experiments.\n\n\\textbullet\\\n\t\\textbf{ACM\\footnote{http://dl.acm.org/}.} We extract papers published in KDD, \n\tSIGMOD, SIGCOMM, MobiCOMM, and VLDB and divide the papers into three classes (\\emph{Database, Wireless Communication, Data Mining}). Then we construct a heterogeneous graph that comprises 3025 papers (P), 5835 authors (A) and \n\t56 subjects (S). \n\tPaper features correspond to elements of a bag-of-words represented of keywords.\n\tWe employ the meta-path set \\{\\emph{PAP}, \\emph{PSP}\\} to perform experiments.\n\tHere we label the papers according to the conference they published.\n\n\\textbullet\\\n\t\\textbf{IMDB.} \n\tHere we extract a subset of IMDB which contains 4780 movies (M), 5841 actors (A) and 2269 directors (D). \n\tThe movies are divided into three classes (\\emph{Action, Comedy, Drama}) according to their genre. Movie features correspond to elements of a bag-of-words represented of plots.\n\tWe employ the meta-path set \\{\\emph{MAM}, \\emph{MDM}\\} to perform experiments.\n\t% \\textbf{Yelp\\footnote{https://www.yelp.com/dataset/download}.} \n\t% We extracted businesses located in North Carolina (NC), Wisconsin (WI), Pennsylvania (PA) and Edinburgh (EDH). \n\t% Then we constructed a HIN that comprises businesses (B), users (U), cities (C), reviews (R) and tips (T). \n\t% We employ the meta-path set \\{BCB, BRURB, BTUTB\\} to extract homogeneous networks.\n\t%  Here we use the state information provided in the dataset as the ground truth.\n\t%\t\t\t\\item\n\t%\t\t\t\\textbf{Yelp\\footnote{https://www.yelp.com/dataset/download}.} We extracted extracted business located in three states\n\t%\t\t\tof the US: North Carolina (NC), Wisconsin (WI), Pennsylvania (PA); and\n\t%\t\t\tin Edinburgh (EDH) of the UK. From the extracted information, we constructed a HIN that comprises businesses (B), users (U), cities (C), reviews (R) and tips (T). We consider the meta-path set \\{BCB, BRURB, BPUPB\\}. Here we use the state information provided in the dataset as the ground truth.\n\n%%%%%%%%%%%%%%%%%%%%0122 \u6ce8\u91ca\u6389\u7684\u7248\u672c %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%\\begin{figure}\n%\t\\centering\n%\t%\t\\subfigure[NMI values on DBLP.]{\\includegraphics[width=0.48\\columnwidth]{fig2.eps}}\n%\t\\subfigure[DBLP]{\\includegraphics[width=0.32\\columnwidth]{fig/schema0819_1.eps}}\n%\t\\subfigure[IMDB]{\\includegraphics[width=0.32\\columnwidth]{fig/schema0819_2.eps}}\n%\t\\subfigure[ACM]{\\includegraphics[width=0.32\\columnwidth]{fig/schema0819_3.eps}}\n%\t%\t\\subfigure[s3]{\\includegraphics[width=0.48\\columnwidth]{fig2.eps}}\n%\t\\caption{Network schema of heterogeneous graph for the experimental datasets.}\n%\t\\label{fig_schema}\n%\\end{figure}\n\n\n%\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n%\\hline\n%Data              & Metrics                   & Train & DW & ESim   & mp2vec & HERec  & GCN    & GAT    & HAT$_{nd}$ & HAT$_{mp}$ & HAT   \\\\ \\hline\n%\\multirow{8}{*}{ACM}  & \\multirow{4}{*}{Macro-F1} & 20\\%     & 0.7725   & 0.7732 & 0.6509 & 0.6617 & 0.8681 & 0.8623 & 0.8815 & 0.8904 & \\textbf{0.8940} \\\\ \\cline{3-12} \n%                      &                           & 40\\%     & 0.8047   & 0.8012 & 0.6993 & 0.7089 & 0.8768 & 0.8704 & 0.8841 & 0.8941 & \\textbf{0.8979} \\\\ \\cline{3-12} \n%                      &                           & 60\\%     & 0.8255   & 0.8244 & 0.7147 & 0.7238 & 0.8810 & 0.8756 & 0.8791 & \\textbf{0.9000} & 0.8951 \\\\ \\cline{3-12} \n%                      &                           & 80\\%     & 0.8417   & 0.8300 & 0.7381 & 0.7392 & 0.8829 & 0.8733 & 0.8848 & 0.9017 & \\textbf{0.9063} \\\\ \\cline{2-12} \n%                      & \\multirow{4}{*}{Micro-F1} & 20\\%     & 0.7692   & 0.7689 & 0.6500 & 0.6603 & 0.8677 & 0.8601 & 0.8799 & 0.8885 & \\textbf{0.8922} \\\\ \\cline{3-12} \n%                      &                           & 40\\%     & 0.7999   & 0.7970 & 0.6975 & 0.7073 & 0.8764 & 0.8679 & 0.8831 & 0.8927 & \\textbf{0.8964} \\\\ \\cline{3-12} \n%                      &                           & 60\\%     & 0.8211   & 0.8202 & 0.7129 & 0.7224 & 0.8812 & 0.8740 & 0.8768 & \\textbf{0.8985} & 0.8933 \\\\ \\cline{3-12} \n%                      &                           & 80\\%     & 0.8388   & 0.8289 & 0.7369 & 0.7384 & 0.8835 & 0.8711 & 0.8826 & 0.8995 & \\textbf{0.9054} \\\\ \\hline\n%\\multirow{8}{*}{DBLP} & \\multirow{4}{*}{Macro-F1} & 20\\%     & 0.7743   & 0.9164 & 0.9016 & 0.9168 & 0.9079 & 0.9097 & 0.9117 & 0.9203 & \\textbf{0.9224} \\\\ \\cline{3-12} \n%                      &                           & 40\\%     & 0.8102   & 0.9204 & 0.9082 & 0.9216 & 0.9148 & 0.9120 & 0.9146 & 0.9208 & \\textbf{0.9240} \\\\ \\cline{3-12} \n%                      &                           & 60\\%     & 0.8367   & 0.9244 & 0.9132 & 0.9280 & 0.9189 & 0.9080 & 0.9178 & 0.9238 & \\textbf{0.9280} \\\\ \\cline{3-12} \n%                      &                           & 80\\%     & 0.8481   & 0.9253 & 0.9189 & 0.9234 & 0.9238 & 0.9173 & 0.918  & 0.9253 & \\textbf{0.9308} \\\\ \\cline{2-12} \n%                      & \\multirow{4}{*}{Micro-F1} & 20\\%     & 0.7937   & 0.9273 & 0.9153 & 0.9269 & 0.9171 & 0.9196 & 0.9205 & 0.9299 & \\textbf{0.9311} \\\\ \\cline{3-12} \n%                      &                           & 40\\%     & 0.8273   & 0.9307 & 0.9203 & 0.9318 & 0.9231 & 0.9216 & 0.9238 & 0.9300 & \\textbf{0.9330} \\\\ \\cline{3-12} \n%                      &                           & 60\\%     & 0.8527   & 0.9339 & 0.9248 & 0.9370 & 0.9262 & 0.9184 & 0.9269 & 0.9331 & \\textbf{0.9370} \\\\ \\cline{3-12} \n%                      &                           & 80\\%     & 0.8626   & 0.9344 & 0.9280 & 0.9327 & 0.9309 & 0.9255 & 0.9269 & 0.9329 & \\textbf{0.9399} \\\\ \\hline\n%\\multirow{8}{*}{IMDB} & \\multirow{4}{*}{Macro-F1} & 20\\%     & 0.4072   & 0.3210 & 0.4116 & 0.4165 & 0.4573 & 0.4944 & 0.4978 & \\textbf{0.5087} & 0.5000 \\\\ \\cline{3-12} \n%                      &                           & 40\\%     & 0.4519   & 0.3194 & 0.4422 & 0.4386 & 0.4801 & 0.5064 & 0.5211 & 0.5085 & \\textbf{0.5271} \\\\ \\cline{3-12} \n%                      &                           & 60\\%     & 0.4813   & 0.3168 & 0.4511 & 0.4627 & 0.4915 & 0.5190 & 0.5173 & 0.5209 & \\textbf{0.5424} \\\\ \\cline{3-12} \n%                      &                           & 80\\%     & 0.5035   & 0.3206 & 0.4515 & 0.4764 & 0.5181 & 0.5299 & 0.5266 & 0.5160 & \\textbf{0.5438} \\\\ \\cline{2-12} \n%                      & \\multirow{4}{*}{Micro-F1} & 20\\%     & 0.4638   & 0.3528 & 0.4565 & 0.4581 & 0.4978 & 0.5528 & 0.5417 & 0.5501 & \\textbf{0.5573} \\\\ \\cline{3-12} \n%                      &                           & 40\\%     & 0.4999   & 0.3547 & 0.4824 & 0.4759 & 0.5171 & 0.5591 & 0.5639 & 0.5515 & \\textbf{0.5797} \\\\ \\cline{3-12} \n%                      &                           & 60\\%     & 0.5221   & 0.3564 & 0.4909 & 0.4988 & 0.5229 & 0.5644 & 0.5609 & 0.5666 & \\textbf{0.5832} \\\\ \\cline{3-12} \n%                      &                           & 80\\%     & 0.5433   & 0.3559 & 0.4881 & 0.5099 & 0.5461 & 0.5697 & 0.5638 & 0.5649 & \\textbf{0.5851} \\\\ \\hline\n%\\end{tabular}\n%\t\n\n%\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n%\t\\hline\n%\tData                  & Metrics                   & Train & DW    & ESim  & mp2vec & HERec & GCN   & GAT   & HAT$_{nd}$ & HAT$_{mp}$    & HAT           \\\\ \\hline\n%\t\\multirow{8}{*}{ACM}  & \\multirow{4}{*}{Macro-F1} & 20\\%  & 77.25 & 77.32 & 65.09  & 66.17 & 86.81 & 86.23 & 88.15       & 89.04          & \\textbf{89.40} \\\\ \\cline{3-12} \n%\t&                           & 40\\%  & 80.47 & 80.12 & 69.93  & 70.89 & 87.68 & 87.04 & 88.41       & 89.41          & \\textbf{89.79} \\\\ \\cline{3-12} \n%\t&                           & 60\\%  & 82.55 & 82.44 & 71.47  & 72.38 & 88.10 & 87.56 & 87.91       & \\textbf{90.00} & 89.51          \\\\ \\cline{3-12} \n%\t&                           & 80\\%  & 84.17 & 83.00 & 73.81  & 73.92 & 88.29 & 87.33 & 88.48       & 90.17          & \\textbf{90.63} \\\\ \\cline{2-12} \n%\t& \\multirow{4}{*}{Micro-F1} & 20\\%  & 76.92 & 76.89 & 65.00  & 66.03 & 86.77 & 86.01 & 87.99       & 88.85          & \\textbf{89.22} \\\\ \\cline{3-12} \n%\t&                           & 40\\%  & 79.99 & 79.70 & 69.75  & 70.73 & 87.64 & 86.79 & 88.31       & 89.27          & \\textbf{89.64} \\\\ \\cline{3-12} \n%\t&                           & 60\\%  & 82.11 & 82.02 & 71.29  & 72.24 & 88.12 & 87.40 & 87.68       & \\textbf{89.85} & 89.33          \\\\ \\cline{3-12} \n%\t&                           & 80\\%  & 83.88 & 82.89 & 73.69  & 73.84 & 88.35 & 87.11 & 88.26       & 89.95          & \\textbf{90.54} \\\\ \\hline\n%\t\\multirow{8}{*}{DBLP} & \\multirow{4}{*}{Macro-F1} & 20\\%  & 77.43 & 91.64 & 90.16  & 91.68 & 90.79 & 90.97 & 91.17       & 92.03          & \\textbf{92.24} \\\\ \\cline{3-12} \n%\t&                           & 40\\%  & 81.02 & 92.04 & 90.82  & 92.16 & 91.48 & 91.20 & 91.46       & 92.08          & \\textbf{92.40} \\\\ \\cline{3-12} \n%\t&                           & 60\\%  & 83.67 & 92.44 & 91.32  & 92.80 & 91.89 & 90.80 & 91.78       & 92.38          & \\textbf{92.80} \\\\ \\cline{3-12} \n%\t&                           & 80\\%  & 84.81 & 92.53 & 91.89  & 92.34 & 92.38 & 91.73 & 91.80       & 92.53          & \\textbf{93.08} \\\\ \\cline{2-12} \n%\t& \\multirow{4}{*}{Micro-F1} & 20\\%  & 79.37 & 92.73 & 91.53  & 92.69 & 91.71 & 91.96 & 92.05       & 92.99          & \\textbf{93.11} \\\\ \\cline{3-12} \n%\t&                           & 40\\%  & 82.73 & 93.07 & 92.03  & 93.18 & 92.31 & 92.16 & 92.38       & 93.00          & \\textbf{93.30} \\\\ \\cline{3-12} \n%\t&                           & 60\\%  & 85.27 & 93.39 & 92.48  & 93.70 & 92.62 & 91.84 & 92.69       & 93.31          & \\textbf{93.70} \\\\ \\cline{3-12} \n%\t&                           & 80\\%  & 86.26 & 93.44 & 92.80  & 93.27 & 93.09 & 92.55 & 92.69       & 93.29          & \\textbf{93.99} \\\\ \\hline\n%\t\\multirow{8}{*}{IMDB} & \\multirow{4}{*}{Macro-F1} & 20\\%  & 40.72 & 32.10 & 41.16  & 41.65 & 45.73 & 49.44 & 49.78       & \\textbf{50.87} & 50.00          \\\\ \\cline{3-12} \n%\t&                           & 40\\%  & 45.19 & 31.94 & 44.22  & 43.86 & 48.01 & 50.64 & 52.11       & 50.85          & \\textbf{52.71} \\\\ \\cline{3-12} \n%\t&                           & 60\\%  & 48.13 & 31.68 & 45.11  & 46.27 & 49.15 & 51.90 & 51.73       & 52.09          & \\textbf{54.24} \\\\ \\cline{3-12} \n%\t&                           & 80\\%  & 50.35 & 32.06 & 45.15  & 47.64 & 51.81 & 52.99 & 52.66       & 51.60          & \\textbf{54.38} \\\\ \\cline{2-12} \n%\t& \\multirow{4}{*}{Micro-F1} & 20\\%  & 46.38 & 35.28 & 45.65  & 45.81 & 49.78 & 55.28 & 54.17       & 55.01          & \\textbf{55.73} \\\\ \\cline{3-12} \n%\t&                           & 40\\%  & 49.99 & 35.47 & 48.24  & 47.59 & 51.71 & 55.91 & 56.39       & 55.15          & \\textbf{57.97} \\\\ \\cline{3-12} \n%\t&                           & 60\\%  & 52.21 & 35.64 & 49.09  & 49.88 & 52.29 & 56.44 & 56.09       & 56.66          & \\textbf{58.32} \\\\ \\cline{3-12} \n%\t&                           & 80\\%  & 54.33 & 35.59 & 48.81  & 50.99 & 54.61 & 56.97 & 56.38       & 56.49          & \\textbf{58.51} \\\\ \\hline\n%\\end{tabular}\n\n"
                },
                "subsection 5.2": {
                    "name": "Baselines",
                    "content": "\n\n\nWe compare with some state-of-art baselines, include the (heterogeneous) network embedding methods and graph neural network based methods, to verfify the effectiveness of the proposed HAN. \nTo verify the effectiveness of our node-level attention and semantic-level attention, respectively,\nwe also test two variants of HAN. \n%We also highlight the kind of data available to each method during training (X: features, A: adjacency matrix, Y: labels).\n\n%\\textbullet\\ \\textbf{MCRec$_{avg}$}:\n\\textbullet\\  DeepWalk \\cite{perozzi2014deepwalk}:\n\tA random walk based network embedding method which designs for the homogeneous graphs. \n\tHere we ignore the heterogeneity of nodes and perform DeepWalk on the whole heterogeneous graph. \n\t%\tWe implement the model with the code from the authors\\footnote{ https://github.com/phanein/deepwalk}.\n\t\n\\textbullet\\   ESim \\cite{Shang2016MetaPathGE}: \n\tA \n\t%\t\theterogeneous network\n\theterogeneous graph embedding method which can capture semantic information from multiple meta-paths. \n\tBecause it is difficult to search the weights of a set of meta-paths, we assign the weights learned from HAN to ESim. \n\t%\tWe implement the model with the code from the authors\\footnote{https://github.com/shangjingbo1226/ESim}.\n\t\n\\textbullet\\  metapath2vec  \\cite{Dong2017metapath2vecSR}: \n\tA \n\theterogeneous graph\n\t%\t\theterogeneous network \n\tembedding method which performs meta-path based random walk and utilizes skip-gram to embed the heterogeneous graphs. \n\t%\tWe implement the model with the code from the authors\\footnote{https://ericdongyx.github.io}. \n\tHere we test all the meta-paths for metapath2vec and report the best performance.\n\t%\t\tcan embed the semantic information extract from a single meta-path.\n\t\n\t%\t\tHere we only report the best result of a single meta-path.\n\t\n\\textbullet\\  HERec \\cite{HERec}:\n\tA heterogeneous graph embedding method which designs a type constraint strategy to filter the node sequence\n\t%\t\tperform the type constraint and filter random walk based on meta-path \n\tand utilizes skip-gram to embed the heterogeneous graphs. \n\t%\tWe implement the model with the code from the authors\\footnote{https://github.com/librahu/HERec}. \n\tHere we test all the meta-paths for HERec and report the best performance.\n\t%\t\tcan embed the semantic information extract from a single meta-path.\n\t\n\t%\t\tHere we only report the best result of a single meta-path.\n\t\n\\textbullet\\   GCN \\cite{gcn}:\n\tIt is a semi-supervised graph convolutional network that designed for the homogeneous graphs. \n\t%\tWe implement the model with the code from the authors\\footnote{https://github.com/tkipf/gcn}. \n\tHere we test all the meta-paths for GCN and report the best performance.\n\t\n\t%\t\tHere we only report the best result of a single meta-path.\n\t\n\\textbullet\\   GAT \\cite{gat}:\n\tIt is a semi-supervised neural network which considers the attention mechanism on the homogeneous graphs. \n\t%\tLike GCN, GAT is a semi-supervised model.\n\t%\t  We implement the model with the code from the authors\\footnote{https://github.com/PetarV-/GAT}.\n\tHere we test all the meta-paths for GAT and report the best performance.\n\t\n\t%\t\t\\item GraphInception \\cite{}\n\t%\t\t\n\t%\t\tTODO\n\t%\t\tHere we only report the best result of a single meta-path.\n\t% \\item GAT$_{avg}$  : \n\t% A variant of GAT which treats all meta-paths equally and averages the embeddings learned from each meta-path.\n\t\n\t\n\\textbullet\\   HAN$_{nd}$  : \n\tIt is a variant of HAN, which removes node-level attention and assigns the same importance to each neighbor.\n\t%\tassign them the same weights.\n\t\n\\textbullet\\  HAN$_{sem}$  : \n\tIt is a variant of HAN, which removes the semantic-level attention and assigns  the same importance to each meta-path.\n\t%\t\tA variant of the proposed HAN, which employs a semantic-level constant attention(assigning the same importance to each meta-path).\n\t\n\\textbullet\\  HAN: \n\t%\tOur proposed approach for heterogeneous network embedding, \n\tThe proposed semi-supervised graph neural network\n\t%\t based solely on attention mechanism\n\t%\t\twhich can fuses the semantic information of different meta-path according to their importance.\n\twhich employs node-level attention and semantic-level attention simultaneously.\n\t\n\t%\t\t We assign equal weights to different meta-path, without learning the weights of meta-path via attention mechanism.\n\n\n%\n%\\subsection{Evaluation Metrics}\n\n%\tIn our experiments, we perform the task of classification,\n%\tclustering, attention analysis and visualization. For node classification task, we\n%\tchoose \\emph{Macro-F1} and \\emph{Micro-F1} to evaluate the results.\n%\t\\begin{itemize}\n%\t\t\\item\n%\t\t\\emph{Macro-F1}. It calculates metrics for each label, and find their\n%\t\tunweighted mean. This does not take label imbalance into account.\n%\t\t\\begin{equation}\n%\t\tMacro-F1=\\frac{\\sum_{Y\\in C}F1(Y)}{|C|},\n%\t\t\\end{equation}\n%\t\twhere \\emph{F1(Y)} is the F1-meansure for the label Y, C is the overall label\n%\t\tset.\n%\t\t\\item\n%\t\t\\emph{Micro-F1}. It calculate metrics globally by counting the total true\n%\t\tpositives, false negatives and false positives.\n%\t\t\\begin{equation}\n%\t\tPr=\\frac{\\sum_{Y\\in C}TP(Y)}{\\sum_{Y\\in C}(TP(Y)+FP(Y))},\n%\t\t\\end{equation}\n%\t\t\\begin{equation}\n%\t\tR=\\frac{\\sum_{Y\\in C}TP(Y)}{\\sum_{Y\\in C}(TP(Y)+FN(Y))},\\\\\\\n%\t\t\t\t\\end{equation}\n%\t\t\\begin{equation}\n%\t\tMicro-F1=\\frac{2*Pr*R}{Pr+R},\n%\t\t\\end{equation}\n%\t\twhere \\emph{TP} means true postivies, \\emph{FP} means false positives and \\emph{FN} means\n%\t\tfalse negatives.\n%\t\\end{itemize}\t\t\n%For clustering task, we select \\emph{Normalized  Mutual  Informatio (NMI)} and \\emph{Adjusted Rand Index (ARI)}\n%\t\tto evaluate the performance. \n%\\begin{itemize}\n%\t\\item \n%\t\\emph{NMI}.\n%\tIt is a measure for determining the\n%\tquality of clustering. It is an external measure because we need the\n%\tclass labels of the instances to determine the \\emph{NMI}. It is defined as\n%\tfollows:\n%\n%\t\\begin{equation}\n%\tNMI(Y,C)=\\frac{2*I(Y;C)}{[H(Y)+H(C)]}.\n%\t\\end{equation}\n%\t\n%\twhere $Y$ is class label, $C$ is the predicted cluster label, $H$ is entropy and $I$ is\n%\tmutual information.\n%\t\\item\n%\t\\emph{ARI}.\n%\tIt defines on the Rand Index (RI) and  adjustes for the chance grouping of elements. The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n%\t\\begin{equation}\n%\tRI = \\frac{a+b}{C_n^2},\n%\t\\end{equation}\n%\twhere  $a+b$ can be considered as the number of agreements between two possible clustera, $n$ means the number of elements and $C_n^2$ means the total number of pairs. And ARI can be defined as:\n%\t\\begin{equation}\n%\t\tARI=\\frac{RI-E[RI]}{max(RI)-E[RI]},\n%\t\\end{equation}\n%\twhere $E[]$ means the expection.\n%\\end{itemize}\n\n\n\n%sssss\n"
                },
                "subsection 5.3": {
                    "name": "Implementation Details",
                    "content": "\n%We implement the HAN model using Tensorflow\\footnote{https://www.tensorflow.org}. \nFor the proposed HAN, we \nrandomly initialize parameters\n% with the Gaussian distribution \n% and \nand optimize the \nmodel with Adam\n%Adaptive Moment Estimation (Adam) \n\\cite{adam}.\nFor the proposed HAN, we set the learning rate to 0.005, \nthe regularization parameter to 0.001, the dimension of the semantic-level attention vector $\\mathbf{q}$ to 128, the number of attention head $K$ to 8, the dropout of attention to 0.6.\nAnd we use early stopping with a patience of 100, i.e. we stop training if the validation loss does not decrease for 100 consecutive epochs. To make our experiments repeatable, we make our dataset and codes publicly available at website\\footnote{https://github.com/Jhy1993/HAN}.For GCN and GAT, we optimize their parameters using the validation set. \nFor semi-supervised graph neural network, including GCN, GAT and HAN, we split exactly the same training set, validation set and test set to ensure fairness.\n%For single meta-path based methods, we test all the meta-paths for them and report the best performance. \nFor random walk based methods include DeepWalk, ESim, metapath2vec, and HERec, \nwe set window size to 5, walk length to 100, \nwalks per node to 40, the number of negative samples to 5. For a fair comparison, we set the embedding dimension to 64 for all the above algorithms. \n%All the experiments are conducted on a machine with two GPUs (NVIDIA GTX-1080 *2) and two CPUs (Intel Xeon E5-2690 * 2). We will release the code of HAN after being accepted.\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Classification",
                    "content": "\nHere we employ \n%$\\ {k}$ nearest neighbor ($\\ {k}$\u2212NN) \nKNN classifier with ${k=5}$ to perform node classification. \nSince the variance of graph-structured data can be quite high, we repeat the process for 10 times and report the averaged \\emph{Macro-F1} and \\emph{Micro-F1} in Table \\ref{table_fenlei}. \n\nBased on Table \\ref{table_fenlei}, we can see that HAN achieves the best performance. \n%Compare to HIN embedding methods, homogeneous network embedding methods including \n% DeepWalk and AutoEncoder fail to performa well. \nFor traditional heterogeneous graph embedding method, ESim which can leverage multiple meta-paths performs better than metapath2vec.\nGenerally, graph neural network based methods which combine the structure and feature information, e.g., GCN and GAT, usually perform better. \nTo go deep into these methods, compared to simply average over node neighbors, e.g., GCN and HAN$_{nd}$, \nGAT and HAN can weigh the information properly and improve the performance of the learned embedding. \n% learn a more sophisticated embedding.\n% perform better than other methods.\n% by telling the difference of neighbors and \nCompared to GAT, the proposed HAN, which designs for heterogeneous graph, captures the rich semantics successfully and shows its superiority.\nAlso, without node-level attention (HAN$_{nd}$) or semantic-level attention (HAN$_{sem}$), \nthe performance becomes worse than HAN, which indicates the importance of modeling the attention mechanism on both of the nodes and semantics. Note that in ACM and IMDB, HAN improves classification results more significantly than in DBLP. Mainly because \\emph{APCPA} is the much more important than the rest meta-paths.\n%HAN improves classification results more significantly than\n%It\u2019s interesting that AMPE avg has the worst performance in DBLP. \nWe will explain this phenomenon in Section 5.7 by analyzing the semantic-level attention.\n\n\nThrough the above analysis, we can find that the proposed HAN achieves the best performance on all datasets. The results demonstrate that it is quite important to capture the importance of nodes and meta-paths in heterogeneous graph analysis.\n%Based on the analysis of semantic-level attention, we can find that APCPA is the most useful meta-path and other meta-path are not really useful. So even if our model can fuse multiple meta-paths, maybe it\u2019s still hard to achieve much higher performance on DBLP.\n\n%   \n% \\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n% \\hline\n% Datasets              & Training              & Metrics  & DW     & ESim   & mp2vec & HERec  & GCN    & GAT    & HATnd & HATmp & HAT   \\\\ \\hline\n% \\multirow{8}{*}{ACM}  & \\multirow{2}{*}{20\\%} & Macro-F1 & 0.7725 & 0.7732 & 0.6509 & 0.6617 & 0.8681 & 0.8623 & 0.8815 & 0.8904 & 0.8940 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.7692 & 0.7689 & 0.6500 & 0.6603 & 0.8677 & 0.8601 & 0.8799 & 0.8885 & 0.8922 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{40\\%} & Macro-F1 & 0.8047 & 0.8012 & 0.6993 & 0.7089 & 0.8768 & 0.8704 & 0.8841 & 0.8941 & 0.8979 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.7999 & 0.7970 & 0.6975 & 0.7073 & 0.8764 & 0.8679 & 0.8831 & 0.8927 & 0.8964 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{60\\%} & Macro-F1 & 0.8255 & 0.8244 & 0.7147 & 0.7238 & 0.8810 & 0.8756 & 0.8791 & 0.9000 & 0.8951 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.8211 & 0.8202 & 0.7129 & 0.7224 & 0.8812 & 0.8740 & 0.8768 & 0.8985 & 0.8933 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{80\\%} & Macro-F1 & 0.8417 & 0.8300 & 0.7381 & 0.7392 & 0.8829 & 0.8733 & 0.8848 & 0.9017 & 0.9063 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.8388 & 0.8289 & 0.7369 & 0.7384 & 0.8835 & 0.8711 & 0.8826 & 0.8995 & 0.9054 \\\\ \\hline\n% \\multirow{8}{*}{DBLP} & \\multirow{2}{*}{20\\%} & Macro-F1 & 0.7743 & 0.9164 & 0.9016 & 0.9168 & 0.9079 & 0.9097 & 0.9117 & 0.9203 & 0.9224 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.7937 & 0.9273 & 0.9153 & 0.9269 & 0.9171 & 0.9196 & 0.9205 & 0.9299 & 0.9311 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{40\\%} & Macro-F1 & 0.8102 & 0.9204 & 0.9082 & 0.9216 & 0.9148 & 0.9120 & 0.9146 & 0.9208 & 0.9240 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.8273 & 0.9307 & 0.9203 & 0.9318 & 0.9231 & 0.9216 & 0.9238 & 0.9300 & 0.9330 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{60\\%} & Macro-F1 & 0.8367 & 0.9244 & 0.9132 & 0.9280 & 0.9189 & 0.9080 & 0.9178 & 0.9238 & 0.9280 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.8527 & 0.9339 & 0.9248 & 0.9370 & 0.9262 & 0.9184 & 0.9269 & 0.9331 & 0.9370 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{80\\%} & Macro-F1 & 0.8481 & 0.9253 & 0.9189 & 0.9234 & 0.9238 & 0.9173 & 0.918  & 0.9253 & 0.9308 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.8626 & 0.9344 & 0.9280 & 0.9327 & 0.9309 & 0.9255 & 0.9269 & 0.9329 & 0.9399 \\\\ \\hline\n% \\multirow{8}{*}{IMDB} & \\multirow{2}{*}{20\\%} & Macro-F1 & 0.4072 & 0.3210 & 0.4116 & 0.4165 & 0.4573 & 0.4944 & 0.4978 & 0.5087 & 0.5000 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.4638 & 0.3528 & 0.4565 & 0.4581 & 0.4978 & 0.5528 & 0.5417 & 0.5501 & 0.5573 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{40\\%} & Macro-F1 & 0.4519 & 0.3194 & 0.4422 & 0.4386 & 0.4801 & 0.5064 & 0.5211 & 0.5085 & 0.5271 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.4999 & 0.3547 & 0.4824 & 0.4759 & 0.5171 & 0.5591 & 0.5639 & 0.5515 & 0.5797 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{60\\%} & Macro-F1 & 0.4813 & 0.3168 & 0.4511 & 0.4627 & 0.4915 & 0.5190 & 0.5173 & 0.5209 & 0.5424 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.5221 & 0.3564 & 0.4909 & 0.4988 & 0.5229 & 0.5644 & 0.5609 & 0.5666 & 0.5832 \\\\ \\cline{2-12} \n%                       & \\multirow{2}{*}{80\\%} & Macro-F1 & 0.5035 & 0.3206 & 0.4515 & 0.4764 & 0.5181 & 0.5299 & 0.5266 & 0.5160 & 0.5438 \\\\ \\cline{3-12} \n%                       &                       & Micro-F1 & 0.5433 & 0.3559 & 0.4881 & 0.5099 & 0.5461 & 0.5697 & 0.5638 & 0.5649 & 0.5851 \\\\ \\hline\n% \\end{tabular}\n\n\n\n\n\n\n\n%\t\\subse\n"
                },
                "subsection 5.5": {
                    "name": "Clustering",
                    "content": "\nWe also conduct the clustering task to evaluate the embeddings learned from the above algorithms. \n%\tAfter obtaining the learned embeddings of nodes, \nOnce the proposed HAN trained, we can get all the node embedding via feed forward.\nHere we utilize the KMeans to perform node clustering and the number of clusters $K$ is set to the number of classes. We use the same ground-truth as in node classification.\nAnd we adopt \\emph{NMI} and \\emph{ARI} to assess the quality of the clustering results. \nSince the performance of KMeans is affected by initial centroids, we repeat the process for 10 times and report the average results in Table \\ref{table_julei}.\n\nAs can be seen in Table \\ref{table_julei}, we can find that HAN performs consistently much better than all baselines. Also, \ngraph neural network based algorithms usually achieve better performance. \n% Moreover, by assigning various weights to \nBesides, without distinguishing the importance of nodes or meta-paths, \nmetapath2vec and GCN cannot perform well.\nWith the guide of multiple meta-paths, HAN performs significantly better than GCN and GAT. \nOn the other hand, without node-level attention (HAN$_{nd}$) or semantic-level attention (HAN$_{sem}$), the performance of HAN has shown various degrees of degeneration. It demonstrates that via assigning the different importance to nodes and meta-paths, the proposed HAN can learn a more meaningful node embedding. \n\nBased on the above analysis, we can find that the propsed HAN can give a comprehensive description of heterogeneous graph and achieve a significant improvements.\n\n\n%ss\n\n\n%nn\n%ss\n\n\n\n\n\n% \\usepackage{multirow}\n\n"
                },
                "subsection 5.6": {
                    "name": "Analysis of Hierarchical Attention Mechanism",
                    "content": "\n% An interesting characteristic of HAN is tHAN it can learn the importance of node neighbor and meta-paths via attention mechanism.  \nA salient property of HAN is the incorporation of the hierarchical mechanism, \nwhich takes the importance of node neighbors and meta-paths into consideration in learning representative embedding. \nRecall that we have learned the node-level attention weight $\\alpha^{\\Phi}_{ij}$ and the semantic-level attention weight $\\beta_{\\Phi_i}$. To better understand the importance of the neighbors and meta-paths, we provide a detailed analysis on the hierarchical attention mechanism.\n\n%Recall that we have learned the attention weights {\u03b1 u,i,\u03c1 } \u03c1 \u2208M for an interaction between user u and item i. Since meta-paths serve as important interaction context, the attention weights provide explicit evidence to understand why an interaction happens.\n%\\subsubsection{sdsds}\n%ssss\n%Here we \n\\textbf{Analysis of node-level attention.} \nAs mentioned before, given a specific task, our model can learn the attention values between nodes and its neighbors in a meta-path. Some important neighbors which are useful for the specific task tend to have larger attention values.\nHere we take the paper P831 \\footnote{Xintao Wu, Daniel Barbara, Yong Ye. Screening and Interpreting Multi-item Associations Based on Log-linear Modeling, KDD'03} in ACM dataset as an illustrative example. \nGiven a meta-path Paper-Author-Paper which describes the co-author of different papers, we enumerate the meta-path based neighbors of paper P831 and their attention values are shown in Figure \\ref{fig_node_att}. From Figure \\ref{fig_node_att}(a), we can see that P831 connects to  P699 \\footnote{Xintao Wu, Jianpin Fan, Kalpathi Subramanian. B-EM: a classifier incorporating bootstrap with EM approach for data mining, KDD'02} and P133 \\footnote{Daniel Barbara, Carlotta Domeniconi, James P. Rogers. Detecting outliers using transduction and statistical testing, KDD'06}, which all belong to \\emph{Data Mining}; conects to P2384 \\footnote{Walid G. Aref, Daniel Barbara, Padmavathi Vallabhaneni. The Handwritten Trie: Indexing Electronic Ink, SIGMOD'95} and P2328 \\footnote{Daniel Barbara, Tomasz Imielinski. Sleepers and Workaholics: Caching Strategies in Mobile Environments, VLDB'95} while P2384 and P2328 both belong to \\emph{Database}; connects to P1973 \\footnote{Hector Garcia-Holina, Daniel Barbara. The cost of data replication, SIGCOMM'81} while P1973 belongs to \\emph{Wireless Communication}. From Figure \\ref{fig_node_att}(b), we can see that paper P831 gets the highest attention value from node-level attention which means the node itself plays the most important role in learning its representation. It is reasonable because all information supported by neighbors are usually viewed as a kind of supplementary information. Beyond itself, P699 and P133 get the second and third largest attention values.\nThis is because P699 and P133 also belong to \\emph{Data Mining} and they can make significant contribution to identify the class of  P831.\nThe rest neighbors get minor attention values that because they do not belong to \\emph{Data Mining} and cannot make important contribution to identify the P831's class.\nBased on the above analysis, we can see that the node-level attention can tell the difference among neighbors and assign higher weights to some meaningful neighbors. \n%Please also note that the node neighbors include itself and \n%\t\\begin{figure}\n%\t\t\\centering\n%\t\t\\subfigure[metapath2vec.]{\\includegraphics[width=0.49\\columnwidth]{fig/metapath2vec_0825.eps}}\n%\t\t%\t\t{fig/metapath2vec0805.eps}}\n%\t\t\\subfigure[GCN.]{\\includegraphics[width=0.49\\columnwidth]{fig/GCN_0825.eps}}\n%\t\t\\subfigure[GAT.]{\\includegraphics[width=0.49\\columnwidth]{fig/GAT_0825.eps}}\n%\t\t\\subfigure[HAN.]{\\includegraphics[width=0.49\\columnwidth]{fig/HANE_0825.eps}}\n%\t\t\\caption{Visualization embedding on DBLP.}\n%\t\t\\label{fig_vis}\n%\t\\end{figure}\n\n\n\n%\n%\\begin{figure*}\n%\t\\centering\n%\t\\subfigure[Dimension of the final embedding $\\mathbf{Z}$]{\\includegraphics[width=0.49\\columnwidth]{fig/para_dim_0903.eps}}\n%\t\n%\t\\subfigure[Dimension of semantic-level attention vector $\\mathbf{q}$]{\\includegraphics[width=0.49\\columnwidth]{fig/para_att_2_0903.eps}}\n%\t\n%\t\\subfigure[Depth of node-level attention.]{\\includegraphics[width=0.49\\columnwidth]{fig/para_depth.eps}}\n%\n%\t\\subfigure[Number of attention head $K$.]{\\includegraphics[width=0.49\\columnwidth]{fig/para_head.eps}}\n%\t\n%\t\\caption{}\n%\t\\label{fig_para2}\n%\\end{figure*}\n%%\t\\subsubsection{Depth of node-level attention.}\n\n\n\n\n%\t\\subsubsection{Depth of node-level attention.}\n\n\\textbf{Analysis of semantic-level attention.} \n%\tBy distinguishing the difference among these meta-paths, HAT achieves the best performance in all tasks. In this section, we will examine the learned attention value and illustrate why it can improve the performances. \n%\tTo verify the importance of different meta-path for given task, we visualize the experiment results based on single meta-path and corresponding attention value.\n%\tTo verify the importance of different meta-path for given task,   \nAs mentioned before, the proposed HAN can learn the importance of meta-paths for the specific task.\n%\ts different meta-paths can capture varying semantics in HIN.\nTo verify the ability of semantic-level attention, taking DBLP and ACM as examples, we report the clustering results (\\emph{NMI}) of single meta-path and corresponding attention values in Figure \\ref{fig_att_metapath}. Obviously, there is a positive correlation between the performance of a single meta-path and its attention value. \nFor DBLP, HAN gives \\emph{APCPA} the largest weight, which means that HAN considers the \\emph{APCPA} as the most critical meta-path in identifying the author's research area. It makes sense because the author's research area and the conferences they submitted are highly correlated. For example, some natural language processing researchers mainly submit their papers to ACL or EMNLP, whereas some data mining researchers may submit their papers to KDD or WWW.\n%whereas some computer vision researchers mainly submit their papers to CVPR or ECCV.\n%Computer Vision a\n%the labels of the authors are based on the conferences they submitted. \nMeanwhile, it is difficult for \\emph{APA} to identify the author's research area well. If we treat these meta-paths equally, e.g., HAN$_{sem}$, the performance will drop significantly. \nBased on the attention values of each meta-path, we can find that the meta-path \\emph{APCPA} is much more useful than \\emph{APA} and \\emph{APTPA}. So even the proposed HAN can fuse them, \\emph{APCPA} still plays a leading role in identifying the author's research area  while \\emph{APA} and \\emph{APTPA} do not.\nIt also explains why the performance of HAN in DBLP may not be as significant as in ACM and IMDB.\n%that although our model can fuse multiple meta-paths, maybe it\u2019s still hard to achieve much higher performance on DBLP.\nWe get the similar conclusions on ACM.\nFor ACM, the results show that HAN gives the most considerable weight to \\emph{PAP}. Since the performance of \\emph{PAP} is slightly better than \\emph{PSP}, so HAN$_{sem}$ can achieve good performance by simple average operation. We can see that semantic-level attention can reveal the difference between these meta-paths and weights them adequately.\n\n\n"
                },
                "subsection 5.7": {
                    "name": "Visualization",
                    "content": "\n\n\nFor a more intuitively comparation, we conduct the task of visualization,  which aims to layout a heterogeneous graph on a low dimensional space. Specifically, we learn the node embedding based on the proposed model and project the learned embedding into a 2-dimensional space.\n%we visualize the learned embeddings on a 2-dimensional space. \nHere we utilize t-SNE \\cite{maaten2008visualizing} to visualize the author embedding in DBLP and coloured the nodes based on their research areas. \n%We employ t-SNE \\cite{maaten2008visualizing} for embedding visualization and plot the author embedding in DBLP in Figure \\ref{fig_vis}.\n% further vislualize \n\n\nFrom Figure \\ref{fig_vis}, we can find that GCN and GAT which design for the homogeneous graphs do not perform well. The authors belong to different research areas are mixed with each other.\nMetapath2vec performs much better than the above homogeneous graph neural networks.\nIt demonstrates that the proper meta-path(e.g., \\emph{APCPA}) can make a significant contribution to heterogeneous graph analysis.\nHowever, since metapath2vec can only take only one meta-path into consideration, the boundary is still blurry.\nFrom Figure \\ref{fig_vis}, we can see that the visualization of  HAN peform best. \nWith the guide of multiple meta-paths,  the embedding learned by HAN has high intra-class similarity and separates the authors in different research area with distinct boundaries. \n\n"
                },
                "subsection 5.8": {
                    "name": "Parameters Experiments",
                    "content": "\nIn this section, we investigate the sensitivity of parameters and report the results of clustering (\\emph{NMI}) on ACM dataset with various parameters in Figure \\ref{fig_para2}.\n\n\\textbullet\\  \\textbf{Dimension of the final embedding $\\mathbf{Z}$.} We first test the effect of the dimension of the final embedding $\\mathbf{Z}$. \n\tThe result is shown in Figure \\ref{fig_para2}(a).\n\tWe can see that with the growth of the embedding dimension, \n\tthe performance raises first and then starts to drop slowly. \n\tThe reason is that HAN needs a suitable dimension to encode the semantics information and \n\tlarger dimension may introduce additional redundancies. \n\n\\textbullet\\   \\textbf{Dimension of semantic-level attention vector $\\mathbf{q}$.} Since the ability of semantic-level attention is affected by the dimension of the semantic-level attention vector $\\mathbf{q}$, we explore the experimental results \n\twith various dimension. The result is shown in Figure \\ref{fig_para2}(b).\n\tWe can find that the performance of HAN grows with the dimension of semantic-level attention vector and achieves the best performance when the dimension of $\\mathbf{q}$ is set to 128.\n\tAfter that, the performance of HAN starts to degenerate which may because of overfitting.\n\t\n\t%\t\\item \\textbf{Depth of node-level attention.} TODO\n\n\\textbullet\\  \\textbf{Number of attention head $K$.} In order to check the impact of multihead attention, we explore the performance of HAN with various number of attention head. The result is shown in Figure \\ref{fig_para2}(c). Note that the multihead attention is removed when the number of attention head is set to 1. Based on the results, we can find that the more number of attention head will generally improve the performance of HAN. However, with the change of attention head, the performance of HAN improve only slightly.\n\tMeanwhile, we also find that multihead attention can make the training process more stable. \n\t\n\n\n\n%\t\\begin{figure}\n%\t\t\\centering\n%\t\t\\subfigure[Dimension of embedding.]{\\includegraphics[width=0.49\\columnwidth]{fig/para_dim.eps}}\n%\t\t\\subfigure[Dimension of semantic-level attention.]{\\includegraphics[width=0.49\\columnwidth]{fig/para_att_2.eps}}\n%%\t\t\\subfigure[Depth of node-level attention.]{\\includegraphics[width=0.5\\columnwidth]{fig/para_depth.eps}}\n%%\t\t\\subfigure[Number of attention head.]{\\includegraphics[width=0.5\\columnwidth]{fig/para_head.eps}}\n%\t\t\\caption{Parameter Sensitivity of HAN.}\n%\t\t\\label{fig_para}\n%\t\\end{figure}\n\n\n%\tTo discover the influence of the number of layers in HAN, \n%\twe conduct experiments with the depth of node-level attention range from 1 to 3(set depth to 4 will cause OOM). \n%\tWe observe tHAN it is difficult to train a deeper model on a HIN without large amount of nodes.\n%\tFurthermore, overfitting may be an issue as the number of parameters increases with the depth of node-level attention.\n%\t\\subsubsection{Number of attention head.}\n%\tHere we compare the experimental results with the various number of attention head.\n%\tWe can easily find tHAN the more attention head, the higher performance HAN trends to achieve.\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nIn this paper, we tackle several fundamental problems in heterogeneous graph analysis and propose a semi-supervised heterogeneous graph neural network based solely on attention mechanism.\nThe proposed HAN can capture complex structures and rich semantics behind heterogeneous graph.\nThe proposed model leverages node-level attention and semantic-level attention to learn the importance of nodes and meta-paths, respectively. \nMeanwhile, the proposed model utilizes the structural information and the feature information in a uniform way.\n%More important, by visualizing the learned attention weights, the proposed HAN dem\nExperimental results include classification and clustering \ndemonstrate the effectiveness of HAN. By analyzing the learned attention weights include both node-level and semantic-level, \n% has potentially good interpretability\nthe proposed HAN has proven its potentially good interpretability.\n\n%In the future, we plan to leverage a more complex structure (e.g., meta-graph) or a higher order structure(e.g., motif) to design heterogeneous graph neural network.\n\n"
            },
            "section 7": {
                "name": "Acknowledgments",
                "content": "\nThis work is supported in part by the National Natural Science Foundation of China (No. 61702296, 61772082,  61532006), the Beijing Municipal Natural Science Foundation (4182043), and the CCF-Tencent Open Fund.\n%TODO \n%function \n%capture higher-order interactions\n%\tthat can integrate the  attribute information \n%\tand the structural information in the most beautiful way possible.\n% We have presented graph attention networks (GATs), novel convolution-style neural networks that operate on graph-structured data, \n% leveraging masked self-attentional layers. The graph attentional layer utilized throughout these networks is \n% computationally ef\ufb01cient (does not require costly matrix operations, and is parallelizable across all nodes in the graph), \n% allows for (implicitly) assigning different importance to different nodes within a neighborhood while dealing with different \n% sized neighborhoods, and does not depend on knowing the entire graph structure upfront\u2014thus addressing many of the theoretical\n%  issues with previous spectral-based approaches. Our models leveraging attention have successfully achieved or matched\n%   state-of-the-art performance across four well-established node classi\ufb01cation benchmarks, \n% both transductive and inductive (especially, with completely unseen graphs used for testing).\n\n\n\n%\n%%\\section{The Body of The Paper}\n%Typically, the body of a paper is organized into a hierarchical\n%structure, with numbered or unnumbered headings for sections,\n%subsections, sub-subsections, and even smaller sections.  The command\n%\\texttt{{\\char'134}section} that precedes this paragraph is part of\n%such a hierarchy.\\footnote{This is a footnote.} \\LaTeX\\ handles the\n%numbering and placement of these headings for you, when you use the\n%appropriate heading commands around the titles of the headings.  If\n%you want a sub-subsection or smaller part to be unnumbered in your\n%output, simply append an asterisk to the command name.  Examples of\n%both numbered and unnumbered headings will appear throughout the\n%balance of this sample document.\n%\n%Because the entire article is contained in the \\textbf{document}\n%environment, you can indicate the start of a new paragraph with a\n%blank line in your input file; that is why this sentence forms a\n%separate paragraph.\n%\n%\n%Other common constructs that may occur in your article are the forms\n%for logical constructs like theorems, axioms, corollaries and proofs.\n%ACM uses two types of these constructs:  theorem-like and\n%definition-like.\n%\n%Here is a theorem:\n%\\begin{theorem}\n%  Let $f$ be continuous on $[a,b]$.  If $G$ is\n%  an antiderivative for $f$ on $[a,b]$, then\n%  \\begin{displaymath}\n%    \\int^b_af(t)\\,dt = G(b) - G(a).\n%  \\end{displaymath}\n%\\end{theorem}\n%\n%Here is a definition:\n%\\begin{definition}\n%  If $z$ is irrational, then by $e^z$ we mean the\n%  unique number that has\n%  logarithm $z$:\n%  \\begin{displaymath}\n%    \\log e^z = z.\n%  \\end{displaymath}\n%\\end{definition}\n%\n%The pre-defined theorem-like constructs are \\textbf{theorem},\n%\\textbf{conjecture}, \\textbf{proposition}, \\textbf{lemma} and\n%\\textbf{corollary}.  The pre-defined de\\-fi\\-ni\\-ti\\-on-like constructs are\n%\\textbf{example} and \\textbf{definition}.  You can add your own\n%constructs using the \\textsl{amsthm} interface~\\cite{Amsthm15}.  The\n%styles used in the \\verb|\\theoremstyle| command are \\textbf{acmplain}\n%and \\textbf{acmdefinition}.\n%\n%Another construct is \\textbf{proof}, for example,\n%\n%\\begin{proof}\n%  Suppose on the contrary there exists a real number $L$ such that\n%  \\begin{displaymath}\n%    \\lim_{x\\rightarrow\\infty} \\frac{f(x)}{g(x)} = L.\n%  \\end{displaymath}\n%  Then\n%  \\begin{displaymath}\n%    l=\\lim_{x\\rightarrow c} f(x)\n%    = \\lim_{x\\rightarrow c}\n%    \\left[ g{x} \\cdot \\frac{f(x)}{g(x)} \\right ]\n%    = \\lim_{x\\rightarrow c} g(x) \\cdot \\lim_{x\\rightarrow c}\n%    \\frac{f(x)}{g(x)} = 0\\cdot L = 0,\n%  \\end{displaymath}\n%  which contradicts our assumption that $l\\neq 0$.\n%\\end{proof}\n%%\n%%\\section{Conclusions}\n%%This paragraph will end the body of this sample document.\n%%Remember that you might still have Acknowledgments or\n%%Appendices; brief samples of these\n%%follow.  There is still the Bibliography to deal with; and\n%%we will make a disclaimer about that here: with the exception\n%%of the reference to the \\LaTeX\\ book, the citations in\n%%this paper are to articles which have nothing to\n%%do with the present subject and are used as\n%%examples only.\n%%%\\end{document}  % This is where a 'short' article might terminate\n%%\n%%\n%%\n%%\\appendix\n%%%Appendix A\n%%\\section{Headings in Appendices}\n%%The rules about hierarchical headings discussed above for\n%%the body of the article are different in the appendices.\n%%In the \\textbf{appendix} environment, the command\n%%\\textbf{section} is used to\n%%indicate the start of each Appendix, with alphabetic order\n%%designation (i.e., the first is A, the second B, etc.) and\n%%a title (if you include one).  So, if you need\n%%hierarchical structure\n%%\\textit{within} an Appendix, start with \\textbf{subsection} as the\n%%highest level. Here is an outline of the body of this\n%%document in Appendix-appropriate form:\n%%\\subsection{Introduction}\n%%\\subsection{The Body of the Paper}\n%%\\subsubsection{Type Changes and  Special Characters}\n%%\\subsubsection{Math Equations}\n%%\\paragraph{Inline (In-text) Equations}\n%%\\paragraph{Display Equations}\n%%\\subsubsection{Citations}\n%%\\subsubsection{Tables}\n%%\\subsubsection{Figures}\n%%\\subsubsection{Theorem-like Constructs}\n%%\\subsubsection*{A Caveat for the \\TeX\\ Expert}\n%%\\subsection{Conclusions}\n%%\\subsection{References}\n%%Generated by bibtex from your \\texttt{.bib} file.  Run latex,\n%%then bibtex, then latex twice (to resolve references)\n%%to create the \\texttt{.bbl} file.  Insert that \\texttt{.bbl}\n%%file into the \\texttt{.tex} source file and comment out\n%%the command \\texttt{{\\char'134}thebibliography}.\n%%% This next section command marks the start of\n%%% Appendix B, and does not continue the present hierarchy\n%%\\section{More Help for the Hardy}\n%%\n%%Of course, reading the source code is always useful.  The file\n%%\\path{acmart.pdf} contains both the user guide and the commented\n%%code.\n%%\n%%\\begin{acks}\n%%  The authors would like to thank Dr. Yuhua Li for providing the\n%%  MATLAB code of the \\textit{BEPS} method.\n%%\n%%  The authors would also like to thank the anonymous referees for\n%%  their valuable comments and helpful suggestions. The work is\n%%  supported by the \\grantsponsor{GS501100001809}{National Natural\n%%    Science Foundation of\n%%    China}{http://dx.doi.org/10.13039/501100001809} under Grant\n%%  No.:~\\grantnum{GS501100001809}{61273304}\n%  and~\\grantnum[http://www.nnsf.cn/youngscientists]{GS501100001809}{Young\n%    Scientists' Support Program}.\n%\n%\\end{acks}\n\n% Please add the following required packages to your document preamble:\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{han}%sample-bibliography}\n\n"
            }
        },
        "tables": {
            "tab_notation": "\\begin{table}\n\t\\caption{Notations and Explanations.}\n\t\\label{tab_notation}\n\t\\begin{tabular}{ccl}\n\t\t\\toprule\n\t\tNotation&Explanation\\\\\n\t\t\\midrule\n\t\t\n%\t\t$\\mathcal{G}$ & Heterogeneous graph \\\\\n%\t\t$\\mathcal{V}$ & Object set \\\\\n%\t\t$\\mathcal{E}$ & Link set \\\\\n%\t\t$\\mathcal{S}$ & Network schema \\\\\n%\t\t$\\mathcal{A}$ & Node type set \\\\\n%\t\t$\\mathcal{R}$ & Link type set \\\\\n\t\t\n\t\t${\\Phi}$ & Meta-path \\\\\n\t\t${\\mathbf{h}}$& Initial node feature \\\\\n\t\t$\\mathbf{M}_{\\phi}$ & Type-specific transformation matrix \\\\\n\t\t$\\mathbf{h}' $& Projected node feature \\\\\n\t\t$e_{ij}^{\\Phi}$ & Importance of meta-path based node pair ($i$,$j$) \\\\\n\t\t$\\mathbf{a}_{\\Phi}$ & Node-level attention vector for meta-path $\\Phi$ \\\\\n\t\t$\\alpha_{ij}^{\\Phi}$ & Weight of \n\t\tmeta-path based node pair ($i$,$j$) \\\\\n\t\t\n\t\t$\\mathcal{N}^{\\Phi}$ & Meta-path based neighbors \\\\\n\t\t$\\mathbf{Z}_{\\Phi}$ & Semantic-specific node embedding \\\\\n\t\t$\\mathbf{q}$ & Semantic-level attention vector \\\\\n\t\t\n\t\t$w_{\\Phi}$ & Importance of meta-path $\\Phi$ \\\\\n\t\t$\\beta_{\\Phi}$ & Weight of meta-path $\\Phi$ \\\\\n\t\t$\\mathbf{Z}$ & The final embedding \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}",
            "table_datasets": "\\begin{table*}[]\n\t\\centering\n\t\\caption{Statistics of the datasets.}\n\t\\label{table_datasets}\n\t\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n\t\t\\hline\n\t\tDataset               & Relations(A-B) & Number of A & Number of B & Number of A-B & Feature               & Training             & Validation           & Test                  & Meta-paths \\\\ \\hline\n\t\t\\multirow{3}{*}{DBLP} & Paper-Author   & 14328       & 4057        & 19645         & \\multirow{3}{*}{334}  & \\multirow{3}{*}{800} & \\multirow{3}{*}{400} & \\multirow{3}{*}{2857} & \\emph{APA}        \\\\ \\cline{2-5} \\cline{10-10} \n\t\t& Paper-Conf     & 14328       & 20          & 14328         &                       &                      &                      &                       & \\emph{APCPA}      \\\\ \\cline{2-5} \\cline{10-10} \n\t\t& Paper-Term     & 14327       & 8789        & 88420         &                       &                      &                      &                       & \\emph{APTPA}      \\\\ \\hline\n\t\t\\multirow{2}{*}{IMDB} & Movie-Actor    & 4780        & 5841        & 14340         & \\multirow{2}{*}{1232} & \\multirow{2}{*}{300} & \\multirow{2}{*}{300} & \\multirow{2}{*}{2687} & \\emph{MAM}        \\\\ \\cline{2-5} \\cline{10-10} \n\t\t& Movie-Director & 4780        & 2269        & 4780          &                       &                      &                      &                       & \\emph{MDM}        \\\\ \\hline\n\t\t\\multirow{2}{*}{ACM}  & Paper-Author   & 3025        & 5835        & 9744          & \\multirow{2}{*}{1830} & \\multirow{2}{*}{600} & \\multirow{2}{*}{300} & \\multirow{2}{*}{2125} & \\emph{PAP}        \\\\ \\cline{2-5} \\cline{10-10} \n\t\t& Paper-Subject  & 3025        & 56          & 3025          &                       &                      &                      &                       & \\emph{PSP}        \\\\ \\hline\n\t\\end{tabular}\n\t\n\\end{table*}",
            "table_fenlei": "\\begin{table*}[]\n\t\\centering\n\t\\caption{Qantitative results (\\%) on the node classification task.}\\label{table_fenlei}\n\t%\n\t\\begin{tabular}\n\t\t%\t{|p{1cm}<{\\centering\\arraybackslash}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|p{1cm}<{\\centering}|}\n\t\t{|c|c|c||c|c|c|c|c|c||c|c|c|}\n\t\t%\t{p{3.5cm}p{3cm}p{2.5cm}}p{5cm}}\n\t\t\n\t\t\\hline\n\t\tDatasets                  & Metrics                   & Training & {DeepWalk}    & ESim  & {metapath2vec} & HERec & GCN   & {GAT}   & HAN$_{nd}$ & {HAN$_{sem}$}    & HAN           \\\\ \\hline\n\t\t\\multirow{8}{*}{ACM}  & \\multirow{4}{*}{Macro-F1} & 20\\%  & 77.25 & 77.32 & 65.09  & 66.17 & 86.81 & 86.23 & 88.15       & 89.04          & \\textbf{89.40} \\\\\n\t\t&                           & 40\\%  & 80.47 & 80.12 & 69.93  & 70.89 & 87.68 & 87.04 & 88.41       & 89.41          & \\textbf{89.79} \\\\\n\t\t&                           & 60\\%  & 82.55 & 82.44 & 71.47  & 72.38 & 88.10 & 87.56 & 87.91       & \\textbf{90.00} & 89.51          \\\\\n\t\t&                           & 80\\%  & 84.17 & 83.00 & 73.81  & 73.92 & 88.29 & 87.33 & 88.48       & 90.17          & \\textbf{90.63} \\\\ \\cline{2-12} \n\t\t& \\multirow{4}{*}{Micro-F1} & 20\\%  & 76.92 & 76.89 & 65.00  & 66.03 & 86.77 & 86.01 & 87.99       & 88.85          & \\textbf{89.22} \\\\\n\t\t&                           & 40\\%  & 79.99 & 79.70 & 69.75  & 70.73 & 87.64 & 86.79 & 88.31       & 89.27          & \\textbf{89.64} \\\\\n\t\t&                           & 60\\%  & 82.11 & 82.02 & 71.29  & 72.24 & 88.12 & 87.40 & 87.68       & \\textbf{89.85} & 89.33          \\\\\n\t\t&                           & 80\\%  & 83.88 & 82.89 & 73.69  & 73.84 & 88.35 & 87.11 & 88.26       & 89.95          & \\textbf{90.54} \\\\ \\hline\n\t\t\\multirow{8}{*}{DBLP} & \\multirow{4}{*}{Macro-F1} & 20\\%  & 77.43 & 91.64 & 90.16  & 91.68 & 90.79 & 90.97 & 91.17       & 92.03          & \\textbf{92.24} \\\\\n\t\t&                           & 40\\%  & 81.02 & 92.04 & 90.82  & 92.16 & 91.48 & 91.20 & 91.46       & 92.08          & \\textbf{92.40} \\\\\n\t\t&                           & 60\\%  & 83.67 & 92.44 & 91.32  & 92.80 & 91.89 & 90.80 & 91.78       & 92.38          & \\textbf{92.80} \\\\\n\t\t&                           & 80\\%  & 84.81 & 92.53 & 91.89  & 92.34 & 92.38 & 91.73 & 91.80       & 92.53          & \\textbf{93.08} \\\\ \\cline{2-12} \n\t\t& \\multirow{4}{*}{Micro-F1} & 20\\%  & 79.37 & 92.73 & 91.53  & 92.69 & 91.71 & 91.96 & 92.05       & 92.99          & \\textbf{93.11} \\\\\n\t\t&                           & 40\\%  & 82.73 & 93.07 & 92.03  & 93.18 & 92.31 & 92.16 & 92.38       & 93.00          & \\textbf{93.30} \\\\\n\t\t&                           & 60\\%  & 85.27 & 93.39 & 92.48  & 93.70 & 92.62 & 91.84 & 92.69       & 93.31          & \\textbf{93.70} \\\\\n\t\t&                           & 80\\%  & 86.26 & 93.44 & 92.80  & 93.27 & 93.09 & 92.55 & 92.69       & 93.29          & \\textbf{93.99} \\\\ \\hline\n\t\t\\multirow{8}{*}{IMDB} & \\multirow{4}{*}{Macro-F1} & 20\\%  & 40.72 & 32.10 & 41.16  & 41.65 & 45.73 & 49.44 & 49.78       & \\textbf{50.87} & 50.00          \\\\\n\t\t&                           & 40\\%  & 45.19 & 31.94 & 44.22  & 43.86 & 48.01 & 50.64 & 52.11       & 50.85          & \\textbf{52.71} \\\\\n\t\t&                           & 60\\%  & 48.13 & 31.68 & 45.11  & 46.27 & 49.15 & 51.90 & 51.73       & 52.09          & \\textbf{54.24} \\\\\n\t\t&                           & 80\\%  & 50.35 & 32.06 & 45.15  & 47.64 & 51.81 & 52.99 & 52.66       & 51.60          & \\textbf{54.38} \\\\ \\cline{2-12} \n\t\t& \\multirow{4}{*}{Micro-F1} & 20\\%  & 46.38 & 35.28 & 45.65  & 45.81 & 49.78 & 55.28 & 54.17       & 55.01          & \\textbf{55.73} \\\\\n\t\t&                           & 40\\%  & 49.99 & 35.47 & 48.24  & 47.59 & 51.71 & 55.91 & 56.39       & 55.15          & \\textbf{57.97} \\\\\n\t\t&                           & 60\\%  & 52.21 & 35.64 & 49.09  & 49.88 & 52.29 & 56.44 & 56.09       & 56.66          & \\textbf{58.32} \\\\\n\t\t&                           & 80\\%  & 54.33 & 35.59 & 48.81  & 50.99 & 54.61 & 56.97 & 56.38       & 56.49          & \\textbf{58.51} \\\\ \\hline\n\t\\end{tabular}\n\\end{table*}",
            "table_julei": "\\begin{table*}[]\n\t\\centering\n\t\\caption{Qantitative results (\\%) on the node clustering task.}\n\t\\label{table_julei}\n\t%\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}\n\t%\t\\hline\n\t%\tDatasets              & Metrics & DeepWalk & ESim   & mp2vec & HERec  & GCN    & GAT    & HAT$_{nd}$ & HAT$_{mp}$ & HAT   \\\\ \\hline\n\t%\t\\multirow{2}{*}{ACM}  & NMI     & 0.4161   & 0.3914 & 0.1223 & 0.1229 & 0.5140 & 0.5729 & 0.6099      & 0.6105      & \\textbf{0.6156} \\\\ \\cline{2-11} \n\t%\t& ARI     & 0.3510   & 0.3432 & 0.1310 & 0.0651 & 0.5301 & 0.6043 & 0.6148      & 0.5945      & \\textbf{0.6439} \\\\ \\hline\n\t%\t\\multirow{2}{*}{DBLP} & NMI     & 0.7653        & 0.6632 & 0.7430 & 0.7673 & 0.7501 & 0.7150 & 0.7530      & 0.7731      & \\textbf{0.7912} \\\\ \\cline{2-11} \n\t%\t& ARI     & 0.8135        & 0.6831 & 0.7850 & 0.8098 & 0.8049 & 0.7726 & 0.8146      & 0.8346      & \\textbf{0.8476} \\\\ \\hline\n\t%\t\\multirow{2}{*}{IMDB} & NMI     & 0.0145   & 0.0055      & 0.0120 & 0.0120 & 0.0545 & 0.0845 & 0.0916      & 0.1031      & \\textbf{0.1087} \\\\ \\cline{2-11} \n\t%\t& ARI     & 0.0215   & 0.0010      & 0.0170 & 0.0165 & 0.0440 & 0.0746 & 0.0798      & 0.0951      & \\textbf{0.1001} \\\\ \\hline\n\t%\\end{tabular}\n\t\\begin{tabular}{|c|c||c|c|c|c|c|c||c|c|c|}\n\t\t\\hline\n\t\tDatasets              & Metrics & DeepWalk    & ESim  & metapath2vec & HERec & GCN   & GAT   & HAN$_{nd}$ & HAN$_{sem}$ & HAN           \\\\ \\hline\n\t\t\\multirow{2}{*}{ACM}  & NMI     & 41.61 & 39.14 & 21.22  & 40.70 & 51.40 & 57.29 & 60.99       & 61.05       & \\textbf{61.56} \\\\\n\t\t& ARI     & 35.10 & 34.32 & 21.00  & 37.13 & 53.01 & 60.43 & 61.48       & 59.45       & \\textbf{64.39} \\\\ \\hline\n\t\t\\multirow{2}{*}{DBLP} & NMI     & 76.53 & 66.32 & 74.30  & 76.73 & 75.01 & 71.50 & 75.30       & 77.31       & \\textbf{79.12} \\\\\n\t\t& ARI     & 81.35 & 68.31 & 78.50  & 80.98 & 80.49 & 77.26 & 81.46       & 83.46       & \\textbf{84.76} \\\\ \\hline\n\t\t\\multirow{2}{*}{IMDB} & NMI     & 1.45  & 0.55  & 1.20   & 1.20  & 5.45  & 8.45  & 9.16        & 10.31       & \\textbf{10.87} \\\\\n\t\t& ARI     & 2.15  & 0.10  & 1.70   & 1.65  & 4.40  & 7.46  & 7.98        & 9.51       & \\textbf{10.01} \\\\ \\hline\n\t\\end{tabular}\n\\end{table*}"
        },
        "figures": {
            "fig_hin": "\\begin{figure}\n\t%\t\\vspace{2.5cm}\n\t\\centering\n\t\\includegraphics[width=1\\columnwidth, height=4cm]{fig/liuchengtu0214.png}\n\t%\t{fig/liuchengtu1105_4.eps}\n\t%\t\t0904.eps}%liuchengtu_6.eps}\n\t%\t\tpricai_0323_v2.eps}\n\t\\caption{\n\t\tAn illustrative example of a heterogenous graph (IMDB). \n\t\t(a) Three types of nodes (i.e., actor, movie, director).\n\t\t(b) A heterogenous graph IMDB  consists three types of nodes  and two types of connections.\n\t\t(c) Two meta-paths involved in IMDB (i.e., Moive-Actor-Moive and Movie-Director-Movie). (d) Moive $m_1$ and its meta-path based neighbors (i.e., $m_1$, $m_2$ and $m_3$). \n\t}\n\t\\label{fig_hin}\n\\end{figure}",
            "liuchengtu": "\\begin{figure}\n\t%\t\\vspace{2.5cm}\n\t\\centering\n\t\\includegraphics[width=1\\columnwidth,height=5cm]{fig/moxingtu0214.png}\n\t%\t\t0904.eps}%liuchengtu_6.eps}\n\t%\t\tpricai_0323_v2.eps}\n\t\\caption{The overall framework of the proposed HAN. (a) All types of nodes are projected into a unified feature space and the weight of meta-path based node pair can be learned via node-level attention.  (b) Joint learning the weight of each meta-path and fuse the semantic-specific node embedding via\n\t\tsemantic-level attention. \n\t\t(c) Calculate the loss and end-to-end optimization for the proposed HAN.\n\t}\n\t\\label{liuchengtu}\n\\end{figure}",
            "fig_explain": "\\begin{figure}\n\t\\centering\n\t% seaborn style 0821\n\t%\t\\subfigure[NMI values on DBLP.]{\\includegraphics[width=0.48\\columnwidth]{fig2.eps}}\n\t\\includegraphics[width=1\\columnwidth]{fig/jhy.png}\n\t\n\t\\caption{Explanation of aggregating process in both node-level and semantic-level.}\n\t\\label{fig_explain}\n\\end{figure}",
            "fig_node_att": "\\begin{figure}\n\t\\centering\n\t%\t\\subfigure[NMI values on DBLP.]{\\includegraphics[width=0.48\\columnwidth]{fig2.eps}}\n\t\\subfigure[Meta-path based neighbors of P831]{\\includegraphics[width=0.49\\columnwidth]{fig/node_att_1_2.png}}\n\t\\subfigure[Attention values of P831's neighbors]{\\includegraphics[width=0.49\\columnwidth]{fig/node_att_2_0825.png}}%%{fig/node_att_2.eps}}\n\t%\t\\subfigure[s3]{\\includegraphics[width=0.48\\columnwidth]{fig2.eps}}\n\t\\caption{Meta-path based neighbors of node P831 and corresponding attention values (Different colors mean different classes, e.g., \\emph{green} means Data Mining, \\emph{blue} means Database, \\emph{orange} means Wireless Communication).}\n\t\\label{fig_node_att}\n\\end{figure}",
            "fig_att_metapath": "\\begin{figure}\n\t\\centering\n\t% seaborn style 0821\n\t%\t\\subfigure[NMI values on DBLP.]{\\includegraphics[width=0.48\\columnwidth]{fig2.eps}}\n\t\\subfigure[NMI values on DBLP]{\\includegraphics[width=0.49\\columnwidth]{fig/att_dblp_nmi_0825.png}}\n\t\\subfigure[NMI values on ACM]{\\includegraphics[width=0.49\\columnwidth]{fig/att_acm_nmi_0825.png}}\n\t%\t\\subfigure[s3]{\\includegraphics[width=0.48\\columnwidth]{fig2.eps}}\n\t\\caption{Performance of single meta-path and corresponding attention value.}\n\t\\label{fig_att_metapath}\n\\end{figure}",
            "fig_vis": "\\begin{figure*}\n\t\\centering\n\t\n\t%\t\t{fig/metapath2vec0805.eps}}\n\t\\subfigure[GCN]{\\includegraphics[width=0.49\\columnwidth]{fig/GCN_0825.png}}\n\t\\subfigure[GAT]{\\includegraphics[width=0.49\\columnwidth]{fig/GAT_0825.png}}\n\t\\subfigure[metapath2vec]{\\includegraphics[width=0.49\\columnwidth]{fig/metapath2vec_0825.png}}\n\t\\subfigure[HAN]{\\includegraphics[width=0.49\\columnwidth]{fig/HANE_0825.png}}\n\t\\caption{Visualization embedding on DBLP. Each point indicates one author and its color indicates the research area. }\n\t\\label{fig_vis}\n\\end{figure*}",
            "fig_para2": "\\begin{figure*}\n\t\\centering\n\t\\subfigure[Dimension of the final embedding $\\mathbf{Z}$]{\\includegraphics[width=0.55\\columnwidth,height=4cm]{fig/para_dim_0903.png}}\n\t\\subfigure[Dimension of the semantic-level attention vector $\\mathbf{q}$]{\\includegraphics[width=0.55\\columnwidth,height=4cm]{fig/para_att_2_0903.png}}\n\t%\t\\subfigure[Depth of node-level attention.]{\\includegraphics[width=0.49\\columnwidth]{fig/para_depth.eps}}\n\t\\subfigure[Number of attention head $K$.]{\\includegraphics[width=0.55\\columnwidth,height=4cm]{fig/para_head_1029_2.png}}\n\t\\caption{Parameter sensitivity of HAN w.r.t. Dimension of the final embedding $Z$,  Dimension of the semantic-level attention vector $q$ and Number of attention head $K$.}\n\t\\label{fig_para2}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\mathbf{h}_i'= \\mathbf{M}_{\\phi_i} \\cdot \\mathbf{h}_i,\\\\\n%\\mathbf{h}_i'= \\mathbf{M}_{\\phi_i} \\cdot \\mathbf{x}_i,\n\\end{equation}",
            "eq:2": "\\begin{equation}\ne_{ij}^{\\Phi}=att_{node}( \\mathbf{h}_i', \\mathbf{h}_j';\\Phi).\n\\label{eq_2}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\alpha_{ij}^{\\Phi}\n=softmax_j(e_{ij}^{\\Phi})\n%=\\frac{\\exp(e_{ij}^{\\Phi})}{\\sum_{k\\in \\mathcal{N}_i^{\\Phi}} \\exp(e_{ik}^{\\Phi})} \\\\\n=\\frac{\\exp \\bigl(\\sigma(\\mathbf{a}^\\mathrm{T}_{\\Phi} \\cdot [\\mathbf{h}_i'\\Vert \\mathbf{h}_j'])\\bigl)}{\\sum_{k\\in \\mathcal{N}_i^{\\Phi}} \\exp \\bigl(\\sigma(\\mathbf{a}^\\mathrm{T}_{\\Phi} \\cdot [\\mathbf{h}_i'\\Vert \\mathbf{h}_k'])\\bigr)},    \\\\\\\n\\label{eq3}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\mathbf{z}^{\\Phi}_i=\\sigma \\biggl( \\sum_{j \\in \\mathcal{N}_i^{\\Phi}} \\alpha_{ij}^{\\Phi} \\cdot \\mathbf{h}_j'  \\biggr).\n\\label{node_agg}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\mathbf{z}^{\\Phi}_i=\n%\t \\Vert_{k=1}^K  \n\\overset{K}{\\underset{k=1}{\\Vert}}\n\\sigma \\biggl( \\sum_{j \\in \\mathcal{N}_i^{\\Phi}} \\alpha_{ij}^{\\Phi} \\cdot \\mathbf{h}_j'  \\biggr).\n% \\sigma( \\sum_{j \\in N_i^{\\Phi}} \\alpha_{ij}^{\\Phi} \\cdot M_{\\phi_j} \\cdot h_j)  )  ,\n\\end{equation}",
            "eq:6": "\\begin{equation}\n(\\mathbf{\\beta}_{\\Phi_1},\\ldots,\\mathbf{\\beta}_{\\Phi_{P}})=att_{sem}(\\mathbf{Z}_{\\Phi_1},\\ldots,\\mathbf{Z}_{\\Phi_{P}}).\n\\end{equation}",
            "eq:7": "\\begin{equation}\n%w_{\\Phi_i} = \\mathbf{q}^\\mathrm{T} \\cdot \\tanh(\\mathbf{W}\\cdot \\mathbf{Z}_{\\Phi_i}+\\mathbf{b}),\nw_{\\Phi_p} =\\frac{1}{|\\mathcal{V}|}\\sum_{i \\in \\mathcal{V}} \\mathbf{q}^\\mathrm{T} \\cdot \\tanh(\\mathbf{W}\\cdot \\mathbf{z}_{i}^{\\Phi_p}+\\mathbf{b}),\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\beta_{\\Phi_p}=\\frac{\\exp(w_{\\Phi_p})}{\\sum_{p=1}^{P} \\exp(w_{\\Phi_p})} ,\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\mathbf{Z}=\\sum_{p=1}^{P} \\beta_{\\Phi_p}\\cdot \\mathbf{Z}_{\\Phi_p}.\n\\label{sem_agg}\n\\end{equation}",
            "eq:10": "\\begin{equation}\nL=-\\sum_{l \\in \\mathcal{Y}_{L}} \\mathbf{Y}^{l} \\ln (\\mathbf{C}\\cdot \\mathbf{Z}^{l}),\n%L=-(\\mathbf{Y} \\log (\\mathbf{C}\\cdot \\mathbf{H}) + (1-\\mathbf{Y})\\log (1-\\mathbf{C} \\cdot \\mathbf{H})),\n%%L = -\\sum y_ilog(C)\n\\end{equation}"
        },
        "git_link": "https://github.com/Jhy1993/HAN"
    }
}