{
    "meta_info": {
        "title": "Subgraph Neighboring Relations Infomax for Inductive Link Prediction on  Knowledge Graphs",
        "abstract": "Inductive link prediction for knowledge graph aims at predicting missing\nlinks between unseen entities, those not shown in training stage. Most previous\nworks learn entity-specific embeddings of entities, which cannot handle unseen\nentities. Recent several methods utilize enclosing subgraph to obtain inductive\nability. However, all these works only consider the enclosing part of subgraph\nwithout complete neighboring relations, which leads to the issue that partial\nneighboring relations are neglected, and sparse subgraphs are hard to be\nhandled. To address that, we propose Subgraph Neighboring Relations Infomax,\nSNRI, which sufficiently exploits complete neighboring relations from two\naspects: neighboring relational feature for node feature and neighboring\nrelational path for sparse subgraph. To further model neighboring relations in\na global way, we innovatively apply mutual information (MI) maximization for\nknowledge graph. Experiments show that SNRI outperforms existing state-of-art\nmethods by a large margin on inductive link prediction task, and verify the\neffectiveness of exploring complete neighboring relations in a global way to\ncharacterize node features and reason on sparse subgraphs.",
        "author": "Xiaohan Xu, Peng Zhang, Yongquan He, Chengpeng Chao, Chaoyang Yan",
        "link": "http://arxiv.org/abs/2208.00850v3",
        "category": [
            "cs.AI",
            "cs.LG"
        ],
        "additionl_info": "received by IJCAI2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nKnowledge graphs (KGs) are collections of structured knowledge represented by factual triples (\\textit{entity, relation, entity}), which are essential for many applications, such as question answering \\cite{huang2019kgqa}, recommendation systems \\cite{wang2018ripple}. However, even state-of-the-art KGs suffer from incompleteness issue, e.g. FreeBase \\cite{boll2008freebase}, and WikiData \\cite{vran2012wiki}. To complete KGs, link prediction task aims at inferring missing links between entities on original KGs. But in fact, there are many newly emerging entities added into real-world KGs constantly over time \\cite{Trivedi2017ke}, e.g., new user added into e-commerce database or new molecules in biomedical KGs. In order to predict links between brand-new entities, inductive link prediction task has been an active area of research, which requires model with the inductive ability for reasoning on graphs consisting of unseen nodes. \n\n \n\n\nWhereas inductive link prediction is a difficult task as it requires generalization from training entities to unseen entities. Most previous link prediction methods \\cite{bordes2013transe,yang2015distmult} learn specific embedding for each entity, which are hard to generalize to unseen entities.\nRecently, motivated by graph neural network (GNN) with the ability of aggregating local information, several inductive models based on GNN have been proposed. GraIL \\cite{teru2020grail} models enclosing subgraph of target triple to capture topological structure (see Test graph A in Figure \\ref{fig:eg}), which owns inductive ability. On the basis of GraIL, several works \\cite{Chen2021tact,mai2021compile} further utilize enclosing subgraph structure to predict links inductively. However, all above methods \nonly consider the enclosing part of subgraph without complete neighboring relations, which leads to two challenging issues. \n% First, they fail to capture complete neighboring information due to the nature of enclosing subgraph.these methods initialize node feature only by positional information, which is not expressive and robust. \nFirst, they lose partial neighboring relations due to the nature of enclosing subgraph. But all neighboring relations contain valuable information to characterize entities (called \\textit{neighboring relational feature}). For example, in Figure \\ref{fig:eg}, part of relations \\textit{mother\\_of} and \\textit{born\\_in} around node \\textit{Messi} are excluded from enclosing subgraph (red paths in Train graph), but they characterize the ``human'' attribution of \\textit{Messi}.\n%  So, it is necessary to retain neighboring relations in a more complete way.\n% Second, enclosing subgraph may be empty or sparse as shown in Table 1,\nSecond, enclosing subgraph may be empty or sparse,\nand all above methods cannot work well in this case, e.g. no usable connecting path existing between \\textit{LeBron} and \\textit{L.A.} in Test graph B. In this case, all above methods cannot work without enclosing subgraph. In fact, we can still reason inductively by some relational paths across target nodes (called \\textit{neighboring relational path}), e.g. the relational path (\\textit{gender}, \\textit{lives\\_in}, \\textit{located\\_in}) in Train graph (green paths). \n\nBased on the above observations, we propose a novel inductive reasoning model, called Neighboring Relational Path Infomax, SNRI, which can effectively exploit complete neighboring information in subgraphs and model neighboring relational paths in a global way by MI maximization. Specifically, SNRI models complete neighboring relations in two aspects: \\textit{neighboring relational feature} for node initializing and \\textit{neighboring relational path} for sparse subgraph modeling. In contrast to previous works \\cite{teru2020grail}, we first extract enclosing subgraph for each triple but reserve complete neighboring relations for each entity. These neighboring relations are then aggregated in an attentive manner to represent entities feature. After that, we utilize neighboring relations of target triples again to build neighboring relational paths in a global way by MI maximization mechanism and apply a joint strategy for training. In this way, SNRI can effectively incorporate complete relational information into enclosing subgraph and model neighboring relational paths, thus improving the performance of inductive link prediction.\n\nOur key contributions are summarized as follows: \n1) We propose a novel inductive reasoning model, SNRI, which effectively integrates complete neighboring relations into the enclosing subgraph from two aspects: neighboring relational feature and neighboring relational path.\n2) We innovatively apply MI maximization to inductive link prediction by maximizing local and global representation to model subgraph and neighboring relational paths in a global way.\n3) Experiments conducted on benchmark datasets show that our work outperforms existing inductive reasoning by a large margin and demonstrate the effectiveness of characterizing entities and modeling sparse subgraphs.\n\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Link Prediction Methods",
                    "content": "\n\\paragraph{Transductive methods.} \nTransductive methods learn an entity-specific embedding for each node, such as 1) translation-based TransE \\cite{bordes2013transe} and TransH \\cite{Wang2014transh}; 2) factorization-based RESCAL \\cite{nickle2012rescal}, and 3) GNN-based R-GCN \\cite{sch2018rgcn} and CompGCN \\cite{vash2020compgcn}. The major differences among them are the scoring function and whether utilize structure information. However, all above models have one thing in common: reasoning over original KGs, and thus difficult to predict missing links between unseen nodes.\n% \\paragraph{Transductive methods.} Transductive methods learn an entity-specific embedding for each node, 1) such as distance models apply distance-based scoring functions such as TransE, TransH, RotatE. Semantic matching models exploit similarity-based scoring functions, such as RESCAL, DistMult, ComplEx. GNN-based models capture structural information inherently stored in KG instead of processing each triplet independently, such as R-GCN, Comp-GCN. However, the above models demand node embeddings when predicting missing links, thus difficult to predict missing links between unseen nodes. \n\n% \\subsection{Inductive Relation Prediction}\n\\paragraph{Inductive methods.} \nInductive models have generalizing ability for reasoning on unseen nodes. They are categorized into rule-based and graph-based methods. Rule-based methods explicitly learn logical rules for reasoning, which is independent to entities and thus inductive. \n% AMIE3 induce logical rules by enumerating statistical relational patterns, but it suffers the scalability issue. To solve that, \nSome differentiable methods, NeuralLP \\cite{yang2017nlp} and DRUM \\cite{sade2019drum}, learn logical rules and rule-confidence simultaneously in an end-to-end differentiable manner. However, they ignore the structure around the target triple, leading to a low expressive ability.\n% Inductive models have generalizing ability for handling unseen nodes. They are categorized into two families: rule-based and graph structure-based models. Due to that logical rules are independent to entities, rule-based models are inductive approaches. AMIE3 induce logical rules by enumerating statistical relational patterns, but it suffers the scalability issue. To solve that, differentiable methods, NeuralLP and DRUM, learn logical rules and rule-confidence simultaneously in an end-to-end differentiable manner by using TensorLog operators. However, they do not take into account subgraph structure around the missing link, leading to insufficient expressive ability.\nIn recent years, graph neural network (GNN) has been a powerful tool in link prediction. Some graph-based methods, such as LAN \\cite{wang2019lan}, aggregate neighboring node embeddings to obtain embeddings of unseen nodes, but they have limitation that unseen nodes have to be surrounded by known neighboring nodes. For reasoning inductively by structure information, GraIL \\cite{teru2020grail} is the first method proposed to model enclosing subgraph structure around the target triple. \nInspired by GraIL, CoMPILE \\cite{mai2021compile} proposes a communicative message passing network to strengthen the message interactions between edges and entitles, thus enables a sufficient flow of relation information. \n% In our work, we also consider the message interactions between edges and entitles in a more concise way.\n% TACT puts forward relational correlation network to model semantic correlations between relations. \nHowever, all these models based on enclosing subgraph suffer two problems: 1) partial neighboring relations are neglected when extracting the enclosing subgraph, and 2) when enclosing subgraph is sparse or even empty, they are hard to reason inductively. In contrast, our work keeps integrated neighboring relations and builds neighboring relational paths to handle sparse subgraphs, which has a better ability for reasoning.\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Contrastive Learning",
                    "content": "\nContrastive learning is an important approach of self-supervised learning, which trains an encoder to be contrastive between representations that captures statistical dependencies of interest and those that do not \\cite{vel2019dgi}. Contrastive Learning has shown great superiority in many downstream applications \\cite{devlin2019bert,he2020mc}. Recently, many works \\cite{qiu2020gcc,vel2019dgi} apply contrastive Learning for GNN. DGI \\cite{vel2019dgi} maximizes mutual information (MI) between local representation and global representation of graph to capture more common local features from both local and global perspectives. However, DGI only applies MI to unweighted graph with simple relations. Motivated by DGI, DRGI \\cite{liang2021drgi} introduces MI to knowledge graphs to handle multi-relational graphs, but it is still a transductive model. For catching neighboring relations in a global way, we innovatively apply MI into the inductive link prediction task by maximizing MI of subgraph and graph representations. \n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Methods",
                "content": "\nIn this section, we introduce our proposed method SNRI in detail. % Inductive link prediction in KGs aims at predicting relations between unseen entities. \nThe overall task is to score a triple $ (u, r_t, v) $ in a KG $ G=|V,R| $ inductively, i.e. to predict the likelihood of the \\textit{target relation}  $ r_t $ between the unseen \\textit{target nodes} $ u$ and $ v $, where $ V $ and $ R $ are sets of nodes and relations.     \nAn overview of our proposed SNRI is shown in Figure \\ref{fg:framework}.\nSNRI mainly consists of four parts: 1) subgraph extraction and neighboring relational feature module to initialize node features, 2) subgraph neural network to learn representations of subgraphs, 3) self-supervised mutual information mechanism to model neighboring relations in a global way, and 4) a joint training strategy to optimize model. \n\n",
                "subsection 3.1": {
                    "name": "Neighboring Relational Feature Module",
                    "content": "\n\n\\paragraph{Subgraph extraction.} We first extract enclosing subgraph $ \\mathcal{G}(u, r_t, v) $  around target triple $ (u, r_t, v) $ following  GraIL \\cite{teru2020grail}. There are three steps for subgraph extraction. First, we obtain node sets of $ k $-hop neighborhood, $ \\mathcal{N}_k(u) $ and $ \\mathcal{N}_k(v) $, of two target nodes $ u $ and $ v $ respectively. Then, we obtain the enclosing subgraph by taking intersection of $\\mathcal{N}_k(u) \\cap  \\mathcal{N}_k(v) $. In the end, we filter out nodes that are isolated or at a distance greater than $ k $  from either of the target nodes. But different from GraIL, we reserve complete neighboring relations $ \\mathcal{N}^r(u) $ of each node, which contains relations partially omitted by enclosing subgraph.\n\n\\paragraph{Node initialization.}\nSince inductive reasoning demands node attributes cannot be used, and GNN requires a node feature matrix $ \\mathbf{X} $ as input \\cite{gilmer2017mp}, our work initialize the node feature $ \\mathbf{h}^0_i$ for node $ i $ by combining positional feature $ \\mathbf{h}^{pos}_i$ and neighboring relational feature $ \\mathbf{h}^{rel}_i$ (see lower left of Figure \\ref{fig:eg}). First, we obtain the positional feature $ \\mathbf{h}^{pos}_i \\in \\mathbb{R}^{d_p}$ by double radius vertex labeling \\cite{zhang2018gnn} scheme :\n%  Specifically, positional feature $ \\mathbf{h}^{pos}_i \\in \\mathbb{R}^{d_p}$is defined as:\n\\begin{equation}\n  \\mathbf{h}^{pos}_i = [\\text{one-hot}(d(i, u)) \\oplus \\text{one-hot}( d(i, v) ])),\n\\end{equation}\nwhere $ d(i, u) $ and $ d(i, v) $ denote the shortest distance from node $ i $ to target head node $ u $ and target tail node $ v $; $ \\oplus $ represents the concatenation operation; Second, we propose the following message passing for node $ i $ in an attentive manner to capture neighboring relational feature $ \\mathbf{h}^{rel}_i \\in \\mathbb{R}^{d}$ :\n\\begin{align}\n  \\mathbf{h}^{rel}_i  &= \\sum_{r \\in \\mathcal{N}^r(i)} \\alpha_r \\mathbf{e}_r, \\\\\n  \\alpha_r = \\text{softmax}(\\mathbf{e}_r, \\mathbf{e}_{r_t}) &= \\frac{\\text{exp}(\\mathbf{e}_r^\\top \\mathbf{e}_{r_t})}{\\sum_{r^\\prime \\in \\mathcal{N}^r(i)}\\text{exp}(\\mathbf{e}_{r^\\prime}^\\top \\mathbf{e}_{r_t})},\n\\end{align}\n% in which  $ \\mathcal{N}^r(i) $ denotes the neighboring relational of node $ i $, \nwhere $ \\mathbf{e}_r $ and $ \\mathbf{e}_{r_t} $ are relation embeddings of neighboring relation $ r $ and target relation $ r_t $, and $ \\alpha_r $ reflects the importance of relation $ r $ to node $ i $ under target relation $ r_t $.\n% is the attention weight between the neighboring relation $ r $ and target relation $ r_t $, which indicates the importance of relation $ r $  under $ r_t $. \n% $ \\alpha_r $ is calculated by the semantic similarity:\n% \\begin{equation}\n%   \\alpha_r = \\text{softmax}(\\mathbf{e}_r, \\mathbf{e}_{r_t}) = \\frac{\\text{exp}(\\mathbf{e}_r^\\top \\mathbf{e}_{r_t})}{\\sum_{r^\\prime \\in \\mathcal{N}^r(i)}\\text{exp}(\\mathbf{e}_{r^\\prime}^\\top \\mathbf{e}_{r_t})},\n% \\end{equation}\n% where $ r^\\prime $ refers to each neighboring relation in $ \\mathcal{N}^r(i) $, and $ \\mathbf{e}_{r_t} $ the relation embedding of target relation $ r_t $.\nIn the end, we represent feature $ \\mathbf{h}_i \\in \\mathbb{R}^d $ of node $ i $ by concatenation of $\\mathbf{h}^{rel}_i $ and $\\mathbf{h}^{pos}_i$, and project node embeddings to the same embeddings space as relations by $ \\mathbf{W}_0 \\in \\mathbb{R}^{(d+d_p) \\times d } $ :\n\\begin{equation}\n  \\mathbf{h}^0  _i = \\mathbf{W}_0 [\\mathbf{h}^{rel}_i \\oplus \\mathbf{h}^{pos}_i].\n\\end{equation}\nWe argue that the feature of nodes with complete neighboring relational semantics are more expressive and robust.\n\n% scheme by aggregating complete neighboring relations in an attentive manner,  \n\n"
                },
                "subsection 3.2": {
                    "name": "Subgraph Neural Network",
                    "content": "\n\nWith the initial feature of nodes, we input sampled subgraphs to subgraph neural network in SNRI (see lower right of Figure \\ref{fg:framework}). As the main component of SNRI, the subgraph neural network models subgraph by two steps: 1) obtain representation of enclosing subgraph by GNN; 2) extract and model neighboring relational paths across target triple.\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Enclosing Subgraph Module",
                        "content": "\n\nWe first input the subgraph $ \\mathcal{G}(u, r_t, v) $ of target triple $ (u, r_t, v) $ to GNN to learn representation of enclosing subgraph. For sufficiently modeling correlations between relations, our GNN model considers the interaction between nodes and relations. We define our nodes' updating function in $ k $-th layer as: \n\\begin{equation}\n    \\mathbf{h}_{i}^{k} =\\sum_{r \\in R} \\sum_{j \\in \\mathcal{N}_{r}(i)} \\alpha_{i, r} \\mathbf{W}_{r}^{k} \\phi(\\mathbf{e}^{k-1}_r, \\mathbf{h}_{j}^{k-1}),\n\\end{equation}\n\\begin{equation}\n  \\alpha_{i, r} =\\sigma_{2}\\left(\\mathbf{W}_{2} \\boldsymbol{c}_{i, r}+\\boldsymbol{b}_{2}\\right),\n\\end{equation}\n\\begin{equation}\n  \\mathbf{c}_{i, r} =\\sigma_{1}\\left(\\mathbf{W}_{1}\\left[\\mathbf{h}_{i}^{k-1} \\oplus \\mathbf{h}_{j}^{k-1} \\oplus \\mathbf{e}^{k-1}_r \\oplus \\mathbf{e}^{k-1}_{r_t}\\right]+\\mathbf{b}_{1}\\right),\n\\end{equation}\n%     \\alpha_{i, r} =\\sigma_{2}\\left(\\mathbf{W}_{2} \\boldsymbol{c}_{i, r}+\\boldsymbol{b}_{2}\\right)\n%     \\mathbf{c}_{i, r} =\\sigma_{1}\\left(\\mathbf{W}_{1}\\left[\\mathbf{h}_{i}^{k-1} \\oplus \\mathbf{h}_{j}^{k-1} \\oplus \\mathbf{e}^{k-1}_r \\oplus \\mathbf{e}^{k-1}_{r_t}\\right]+\\mathbf{b}_{1}\\right)\n% \\end{equation}\nwhere $ \\mathcal{N}_r(i)$ denotes the immediate outgoing neighbors of node $ i $ under relation $ r $; $ \\mathbf{W}_r^k $ is the transformation matrix for relation $ r $ for propagating messages; $ \\sigma_1, \\sigma_2 $ are Sigmoid function; $ \\alpha_{i, r} $ is the attention weight of edge ($ i, r, j $); $ \\phi(\\mathbf{e}^{k-1}_r, \\mathbf{h}_{j}^{k-1}) $ is a fusion operation to share hidden feature of nodes and relations. Inspired by \\cite{vash2020compgcn}, we set the default fusion operation as subtraction $ \\phi(\\mathbf{e}, \\mathbf{h}) = \\mathbf{e} - \\mathbf{h}$ to discriminate direction of relation.\nIn addition, to keep nodes and relations the same embedding space, relation embeddings are also transformed as follows:\n\\begin{equation}\n  \\mathbf{e}_r^{k} = \\mathbf{W}_{rel}^{k} \\mathbf{e}_r^{k-1}.\n\\end{equation} \n\nInspired by CoMPILE \\cite{mai2021compile}, we feed all node embeddings $\\mathbf{H}^L$ of the last layer to a Gated Recurrent Unit (GRU) \\cite{cho2014gru} to increase the expressive ability of network:\n\\begin{equation}\n  \\mathbf{H}^L = \\operatorname{GRU}(\\mathbf{H}^{L}).\n\\end{equation}\n\nFinally, to obtain the representation $ \\mathbf{h}_{\\mathcal{G}} $ of subgraph $ \\mathcal{G} $, we use an average readout function:\n\\begin{equation}\n  \\mathbf{h}_{\\mathcal{G}} = \\frac{1}{|{V}_{\\mathcal{G}}|} \\sum_{i \\in {V}_{\\mathcal{G}}} \\mathbf{h}^L_i,\n\\end{equation}\nwhere $ V_{\\mathcal{G}} $ denotes the set of nodes in subgraph $ \\mathcal{G} $.\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Neighboring Relational Path Module",
                        "content": "\n\n% Note that enclosing subgraph only considers relational paths connecting two target nodes, which leads to potential issue that when enclosing subgraph is sparse or even empty, message passing network cannot work. \nTo solve the issue of sparse subgraph, we propose to explore neighboring relations to model \\textit{neighboring relational paths}. This procedure can be seen in Figure \\ref{fg:framework}. Specifically, a neighboring relational path is a relational sequence across the target nodes, i.e. $ p = (r_u, r_t, r_v) $, where $ r_u \\in \\mathcal{N}^{rel}(u)$ and $ r_v \\in \\mathcal{N}^{rel}(v) $ are relations around target nodes $ u $ and $ v $. We denote $ \\mathcal{P}_{(u,v)} $ as the set of all neighboring relational paths across $ u $ and $ v $ in subgraph.\n\nFor each neighboring relational path $ p$, we first model it with Gated Recurrent Network (GRU) \\cite{cho2014gru} as follows:\n\\begin{equation}\n  \\mathbf{p} = \\operatorname{GRU}(p) = \\operatorname{GRU}(\\mathbf{e}_{r_u},\\mathbf{e}_{r_t},\\mathbf{e}_{r_v}).\n\\end{equation}\nThen, we aggregate all path representations with attention to obtain the subgraph path representation $ \\mathbf{p}_{\\mathcal{G}} $: \n\\begin{equation}\n  \\mathbf{p}_\\mathcal{G} = \\sum_{p \\in \\mathcal{P}} \\beta_p \\mathbf{p}\n\\end{equation}\n\\begin{equation}\n  \\beta_{p} = \\frac{\\text{exp}(\\mathbf{p}^\\top \\mathbf{e}_{r_t})}{\\sum_{p^\\prime \\in \\mathcal{P}_{(u,v)}}\\text{exp}(\\mathbf{p}^{\\prime \\top} \\mathbf{e}_{r_t})}.\n\\end{equation}\n\n"
                    },
                    "subsubsection 3.2.3": {
                        "name": "Supervised Learning",
                        "content": "\nTo organize above two modules in a unified framework, we combine the enclosing subgraph information $ \\mathbf{h}_{\\mathcal{G}} $ and neighboring relational path information $ \\mathbf{p}_{\\mathcal{G}} $ as the final representation of subgraph $ \\mathbf{s}_{\\mathcal{G}} $ :\n\\begin{equation}\n  \\mathbf{s}_{\\mathcal{G}} = [\\mathbf{h}_{\\mathcal{G}} \\oplus \\mathbf{p}_{\\mathcal{G}}],\n\\end{equation}\nand assign score with embeddings of target triple $ (u, r_t, v) $:\n\\begin{equation}\n  f(u, v_t, r) = \\mathbf{W}_s[\\mathbf{h}^L_u \\oplus \\mathbf{h}^L_v\\oplus \\mathbf{e}^L_{r_t} \\oplus \\mathbf{s}_{\\mathcal{G}}],\n\\end{equation}\n% \\begin{equation}\n%   f(u, v_t, r) = \\mathbf{W}_s[\\mathbf{h}^L_u \\oplus \\mathbf{h}^L_v\\oplus \\mathbf{e}^L_{r_t} \\oplus \\mathbf{h}_{\\mathcal{G}} \\oplus \\mathbf{p}_{\\mathcal{G}}],\n% \\end{equation}\nwhere $ \\mathbf{h}^L_u, \\mathbf{h}^L_u, \\text{and } \\mathbf{e}^L_{r_t}$ denote the embedding of target nodes $ u, v $ and target relation $ r $ in $ L $-th layer of GNN respectively. Finally, for supervised learning, we construct a margin-based loss function with equal negative triples by replacing heads or tails:\n\\begin{equation}\\label{eq:sup}\n  \\mathcal{L}_{sup} = \\sum_{(u, r_t, v)\\in \\mathcal{G}} \\text{max}(0, f(u^\\prime, r_t^\\prime, v^\\prime) - f(u, r_t, v) + \\gamma),\n\\end{equation}\nwhere$ (u, r_t, v) $ and $ (u^\\prime, r_t^\\prime, v^\\prime) $ refer to positive and negative samples, and $ \\gamma $ is the margin hyperparameter. \n\n\n"
                    }
                },
                "subsection 3.3": {
                    "name": "MI Maximization in SNRI",
                    "content": "\n% Since our model relies on modeling subgraph of target triple, to avoid that our model over-emphasizes local structural information, \n\nTo avoid the subgraph neural network in SNRI over-emphasizing local structure, we further model neighboring relations in a global way by maximizing local-global (i.e. subgraph-graph) mutual information (MI), that is, we seek to enable neighboring relational features and paths to capture global information of entire KG.\n% , $G=\\{\\mathcal{G}_1, \\cdots, \\mathcal{G}_N\\}$ , represented by a summary vector $ \\mathbf{s}_{G}$. \n% =\\{\\mathcal{G}_1 \\cdots \\mathcal{G}_N\\}\n\nTo obtain global representation $ \\mathbf{s}_{G}$ for $ G $, we use a readout function to summarize the obtained subgraph representations:\n\\begin{equation}\n  \\mathbf{s}_{G} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{s}_{ \\mathcal{G}_i},\n\\end{equation}\nwhere $ N $ is the number of triples in knowledge graph $ G $; $ \\mathcal{G}_i \\in G$ is the subgraph of $ i $-th triple. \nThen, we utilize the Jensen-Shannon (JS) MI estimator \\cite{Sun2021sugar} to maximize the estimated MI over subgraph and graph representations. Specifically, a discriminator $ \\mathcal{D}(\\mathbf{s}_{\\mathcal{G}}, \\mathbf{s}_{G}) $ is employed, which assign the probability score to subgraph-graph pair. Note that $ \\mathcal{D} $ should be higher for subgraphs contained within the graph. Following DGI \\cite{vel2019dgi}, we heuristically apply a bilinear function as the discriminator:\n\\begin{equation}\n  \\mathcal{D}(\\mathbf{s}_{\\mathcal{G}}, \\mathbf{s}_{G}) = \\sigma(\\mathbf{s}_{\\mathcal{G}}^\\top \\mathbf{W}_{MI} \\mathbf{s}_{G}),\n\\end{equation}\nwhere $ \\sigma $ is the sigmoid function and $ \\mathbf{W}_{MI} $ is a learnable scoring matrix. Since self-supervised MI mechanism is contrastive, negative graph $ G(\\tilde{\\mathbf{X}}, \\tilde{\\mathbf{A}}) $ is constructed by a corruption function $ \\mathcal{C} $ : \n\\begin{equation}\n  \\tilde{G}(\\tilde{\\mathbf{X}}, \\mathbf{A}) \\sim \\mathcal{C}(G(\\mathbf{X}, \\mathbf{A})),\n\\end{equation}\nwhere $ \\mathbf{X} $ is the initial feature of nodes described in section 3.2, and $ \\mathbf{A} $ is the adjacency matrix of $ G $. The corruption function $ \\mathcal{C}(\\cdot) $ preserves original structure but corrupts nodes feature by row-wise shuffling of $ \\mathbf{X} $.\n\n\n\n\nThe MI objective for knowledge graph is realized by contrasting positive and negative subgraph-graph pairs: \n\\begin{align}\\label{eq:MI_loss}\n  \\mathcal{L}_{MI} &=\\frac{1}{N+M}(\\sum_{i=1}^{N} \\mathbb{E}_{(\\mathbf{X}, \\mathbf{A})}\\left[\\log \\mathcal{D}\\left( \\mathbf{s}_{\\mathcal{G}_i}, \\mathbf{s}_{G} \\right)\\right]  \\nonumber \\\\\n  &+ \\sum_{j=1}^{M} \\mathbb{E}_{(\\tilde{\\mathbf{X}}, {\\mathbf{A}})}\\left[\\log \\left(1-\\mathcal{D}\\left(\\tilde{\\mathbf{s}}_{\\mathcal{G}_j}, \\mathbf{s}_{G}\\right)\\right)\\right]),\n\\end{align}\nwhere $ N, M $ denote the number of positive and negative samples; $ \\tilde{\\mathbf{s}}_{\\mathcal{G}} $ refers to the representation of negative subgraph sampled from $\\tilde{G}$.\n\n\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Joint Training Strategy",
                    "content": "\n\nThe final learning objective of our work is defined as the combination of the supervised loss in Eq. \\ref{eq:sup} and MI loss in Eq. \\ref{eq:MI_loss}:\n\\begin{equation}\n  \\mathcal{L} = \\mathcal{L}_{sup} + \\lambda \\mathcal{L}_{MI}, \n\\end{equation}\nwhere $ \\lambda $ controls the contribution of the self-supervised MI mechanism. By this joint training strategy, our model is capable of modeling subgraph with complete relations while capturing neighboring relations aware of both local and global structural properties.\n\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\n\n\n% In this section, we first introduce experimental configurations: datasets, evaluation protocol, hyperparameter settings and baselines. Then, we conduct our proposed model SNRI on several datasets and show the superiority of SNRI. Finally, we prove the effectiveness of each part in SNRI by ablation studies and further experiments. The code of SNRI will be publicly available once accepted.\n\n",
                "subsection 4.1": {
                    "name": "Experimental Configurations",
                    "content": "\n\n\\paragraph{Datasets.} WN18RR \\cite{dett2018wn18} and FB15k-237 \\cite{tout2015fb} are common datasets used in transductive link prediction. For inductive link prediction task, we use the variants of WN18RR and FB15k-237 proposed by GraIL \\cite{teru2020grail}, where entities in test set are not contained in train set and each dataset generate four versions datasets with increasing size. The statistics of the datasets is shown in Table \\ref{tb:data}.\n\n\n\\paragraph{Evaluation protocol.} To compare fairly with the prior methods, we use the same evaluation protocol as \\cite{teru2020grail}: AUC-PR for classification metrics and Hits@10 for ranking metrics. \nAUC-PR is an indicator for classification task by computing the area under the precision-recall curve. To compute AUC-PR, along with all positive triples in test set, we score an equal number of negative triples sampled by corrupting head or tail with a random entity. \nHits@10 is the proportion of correct entities ranked in top 10 of candidate entities. For calculating Hits@10, we compare positive triples with sampled negative triples by assigning scores, to see whether the true triple can rank top 10.\nEach result is obtained by averaging over 5 runs for accurate evaluation.\n\n\\paragraph{Hyperparameter settings.}  For subgraph extraction, we extract enclosing subgraph with 3 hops. In training process, we manually specify the hyperparameters as follows: learning rate to 0.001, dropout rate to 0.5, embedding dimension to 32. The margin $ \\gamma $ in supervised loss function is set to 10, and coefficient $ \\lambda $ in joint loss function is set to 5. The maximum number of training epochs is set to 30. We use Adam \\cite{kinma2015adam} as optimizer to train our model. All experiments are implemented by PyTorch and run on NVIDIA RTX TITAN.\n\n\\paragraph{Baselines.} We compare our model to several state-of-the-art methods, including Neural-LP \\cite{yang2015distmult}, DRUM \\cite{sade2019drum}, GraIL \\cite{teru2020grail}, and CoMPILE \\cite{mai2021compile}. Neural-LP and DRUM are rule-based models learning logical rules and rule-confidence simultaneously in an end-to-end differentiable manner. GraIL and CoMPILE are graph-based models reasoning inductively by enclosing subgraph.\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Main Results",
                    "content": "\n\n\\paragraph{Comparison with baselines.} The results comparing with baseline models are shown in Table \\ref{tb:main result}. The results show that our proposed model SNRI significantly outperforms baselines on the majority of datasets in terms of both AUC-PR and Hits@10 evaluation protocol, which demonstrates the effectiveness of our proposed model. Specifically, the average boosts of SNRI on WN18RR and FB15k-237 in Hits@10 reach up to 5.34\\% and 3.64\\% respectively compared with SOTA model CoMPILE, and it can be seen that the performance on WN18RR is more significant. This is because all previous works based on enclosing subgraph are hard to do reasoning when subgraph is sparse. As presented in Table \\ref{tb:data} WN18RR has a lower ratio of \\#T to \\#E than FB15k-237, which means that subgraphs in WN18RR are more likely to be sparse, and thus scant of structure information for reasoning. In contrast, our model can deal with sparse subgraph powerfully by modeling neighboring relational features and neighboring relational paths to exploit complete neighboring relations sufficiently. But for FB15k-237, the improvement is less significant, which may be because subgraphs in FB15k-237 have very high density so that it is much easier for baselines to handle.\n\n\\paragraph{Effective modeling of sparse subgraph.}\\label{sec:sparse} In this section, we tend to further verify that our proposed SNRI is capable of modeling complete neighboring relations to handle sparse subgraphs. We evaluate the ranking performance of CoMPILE and our proposed SNRI on subgraphs with different densities in WN18RR v1 and v4. Concretely, we divide subgraphs into three ranges according to the number of nodes in subgraph and then calculate Hits@10 of each range. As presented in Figure \\ref{fg:sparse result}, SNRI performs better on WN18RR v1 and v4 across all ranges, especially for the range with low subgraph density. This result shows that SNRI possesses a better inductive ability for sparse subgraphs, and proves the necessity of capturing complete neighboring relations.\n\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Study",
                    "content": "\n\n\n\nIn this section, we perform ablation study on WN18RR v1 and v4 to investigate the impact of each component in SNRI, namely, 1) neighboring relational feature (called \\textbf{SNRI w/o NRF}), 2) neighboring relational paths (called \\textbf{SNRI w/o NRP}), and 3) MI maximization (called \\textbf{SNRI w/o MI}), by removing them respectively. Table \\ref{tb:ablation result} shows the results of ablation studies. We can find that all variants of SNRI perform worse than the original SNRI, which demonstrates the effectiveness of each component.\n\n% 1) neighboring relational feature, 2) neighboring relational paths, and 3) MI maximization. For investigating the neighboring relational feature of node, we omit it when initializing node feature, and we call this \\textbf{SNRI w/o NRF}. For the neighboring relational path, we score the target triple without it, and we call this \\textbf{SNRI w/o NRP}. Additionally, we train model without self-supervised loss to check whether the global information captured by MI maximization gives rise to the overall performance, and we call this \\textbf{SNRI w/o MI}.\n\n\\paragraph{\\textbf{SNRI w/o NRF}.} After removing the neighboring relational feature, the Hits@10 value averagely reduces by 0.7\\%. The reason may be that nodes feature with only positional information are less expressive, which cannot characterize the node effectively, and when nodes in subgraph are plentiful the positional feature is unstable and less robust. In contrast, complete neighboring relations are more effective and robust to characterize node features. \n\\paragraph{\\textbf{SNRI w/o NRP}.} From the result of SNRI w/o NRP, we can notice that performance drops a lot when neighboring relational paths are omitted. Associated with the result of section \\ref{sec:sparse}, this result demonstrates neighboring relational paths are effective in handling sparse subgraphs. SNRI w/o NRF together with SNRI w/o NRP demonstrates the effectiveness of utilizing complete neighboring relations which are omitted by enclosing subgraph. \n\\paragraph{\\textbf{SNRI w/o MI}.} Additionally, removing MI maximization results in an average reduction of 1.7\\%. This result implies global information is helpful to model neighboring relations better. We can observe that complete neighboring relations play a greater role in SNRI than MI maximization, but better performance can be obtained by considering complete neighboring relations and MI maximization simultaneously.\n\n"
                },
                "subsection 4.4": {
                    "name": "Case Study",
                    "content": "\n\n\nFrom WN18RR and FB15k-237, We select some target relations and then display the top 2 important neighboring relational paths in Table \\ref{tb:case result}. The result shows that SNRI can learn correct neighboring relational paths and tend to assign a high score for the path with multiple relational types, indicating that SNRI prefers more informative neighboring relational paths to reason inductively. For example, considering target relation \\textit{related\\_form}, the path \\textit{(related\\_form, related\\_form, \\_also\\_see)} gets a larger importance weight than \\textit{(related\\_form, related\\_form, related\\_form)} with single relational type.\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\nIn this paper, we propose a novel model called SNRI for inductive link prediction on knowledge graph, which can effectively exploit complete neighboring relations and learn global structure information. SNRI utilizes complete neighboring relations to characterize neighboring relational features of nodes in a more expressive manner, and then models neighboring relational path in a global way by MI maximization. The experiments on two benchmark datasets demonstrate our proposed SNRI significantly outperforms several existing state-of-the-art methods for the inductive link prediction task, and verify the effectiveness of modeling complete neighboring relations in a global way to characterize node features and reason on sparse subgraphs.\n\n\n"
            },
            "section 6": {
                "name": "Acknowledgements",
                "content": "\nThe authors gratefully acknowledge the support of the National Natural Science Foundation of China (Grant No. 61876223, No. 61832004), Youth Innovation Promotion Association,Chinese Academy of Sciences (No.2020163), and International Cooperation and Exchanges NSFC (Grant No. 62061136006). \n\n\n\n\n% \\appendix\n\n% \\section{\\LaTeX{} and Word Style Files}\\label{stylefiles}\n\n% \\cite{gottlob:nonmon}\n% The \\LaTeX{} and Word style files are available on the IJCAI--22\n% website, \\url{https://ijcai-22.org/}.\n% These style files implement the formatting instructions in this\n% document.\n\n% The \\LaTeX{} files are {\\tt ijcai22.sty} and {\\tt ijcai22.tex}, and\n% the Bib\\TeX{} files are {\\tt named.bst} and {\\tt ijcai22.bib}. The\n% \\LaTeX{} style file is for version 2e of \\LaTeX{}, and the Bib\\TeX{}\n% style file is for version 0.99c of Bib\\TeX{} ({\\em not} version\n% 0.98i). The {\\tt ijcai22.sty} style differs from the {\\tt\n% ijcai21.sty} file used for IJCAI--21.\n\n% The Microsoft Word style file consists of a single file, {\\tt\n% ijcai22.docx}. This template differs from the one used for\n% IJCAI--21.\n\n% These Microsoft Word and \\LaTeX{} files contain the source of the\n% present document and may serve as a formatting sample.\n\n% Further information on using these styles for the preparation of\n% papers for IJCAI--22 can be obtained by contacting {\\tt\n% proceedings@ijcai.org}.\n\n%% The file named.bst is a bibliography style file for BibTeX 0.99c\n\n\\bibliographystyle{named}\n\\bibliography{ijcai22}\n\n"
            }
        },
        "tables": {
            "tb:main result": "\\begin{table*}[t]\n  \\centering\n  \\resizebox{\\textwidth}{!}{\n  \\begin{tabular}{lrrrrrrrrrrrrrrrr}\n    \\toprule\n    & \\multicolumn{8}{c}{WN18RR} & \\multicolumn{8}{c}{FB15k-237}\\\\\n    \\cmidrule(lr){2-9} \\cmidrule(lr){10-17} \n    &\\multicolumn{2}{c}{v1} & \\multicolumn{2}{c}{v2} & \\multicolumn{2}{c}{v3} & \\multicolumn{2}{c}{v4} & \\multicolumn{2}{c}{v1} & \\multicolumn{2}{c}{v2} & \\multicolumn{2}{c}{v3} & \\multicolumn{2}{c}{v4} \\\\\n     \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(lr){12-13} \\cmidrule(lr){14-15} \\cmidrule(lr){16-17}\n    Method &AP &  H@10 & AP &  H@10 & AP &  H@10 & AP &  H@10 & AP &  H@10 & AP &  H@10 & AP &  H@10 & AP &  H@10 \\\\ \n    \\midrule \n        Neural-LP & 86.02 &74.37  & 83.78 & 68.93 & 62.90 & 46.18 & 82.06 & 67.13 & 69.64 & 52.92  & 76.55 & 58.94 & 73.95 & 52.90 & 75.74 & 55.88  \\\\\n    \\text {DRUM}  & 86.02 & 74.37 & 84.05  & 68.93& 63.20 & 46.18 & 82.06 & 67.13 & 69.71 & 52.92  & 76.44 & 58.73 & 74.03 & 52.90 & 76.20 & 55.88  \\\\\n    \\text {RuleN} & 90.26 & 80.85 & 89.01  & 78.23& 76.46 & 53.39 & 85.75 & 71.59 & 75.24 & 49.76  & 88.70 & 77.82 & 91.24 & 87.69 & 91.79 & 85.60  \\\\\n    \\text {GraIL} & 94.32 & 82.45 & 94.18  & 78.68& 85.80 & 58.43 & 92.72 & 73.41 & 84.69 & 64.15  & 90.57 & 81.80 & 91.68 & 82.83 & 94.46 & 89.29   \\\\\n    \\text {CoMPILE} & 98.23 &83.60  & 99.56 &  79.82& 93.60 &  60.69& \\textbf{99.80}  &75.49 & 85.50 &  67.64 & 91.68 & 82.98 & \\textbf{93.12} & 84.67 & \\textbf{94.90} & 87.44  \\\\\n    \\midrule \n    SNRI & \\textbf{99.10} & \\textbf{87.23} & \\textbf{99.92} & \\textbf{83.10} & \\textbf{94.90} & \\textbf{67.31} & 99.61 & \\textbf{83.32} & \\textbf{86.69} & \\textbf{71.79} & \\textbf{91.77} & \\textbf{86.50} & 91.22 & \\textbf{89.59} & 93.37 & \\textbf{89.39} \\\\\n    \\bottomrule\n  \\end{tabular}}\n  \\caption{AUC-PR and Hits@10 results on the inductive benchmark datasets extracted from WN18RR and FB15k-237. We use AP and H@10 to denote AUC-PR and Hits@10, respectively. The best performance is highlighted.}\n  \\label{tb:main result}\n\\end{table*}",
            "tb:data": "\\begin{table}[t]\n  \\raggedright\n  \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{ccrrrrrr}\n      \\toprule\n      & & \\multicolumn{3}{c}{\\text { WN18RR }} & \\multicolumn{3}{c}{\\text { FB15k-237 }} \\\\\n      \\cmidrule(lr){3-5} \\cmidrule(lr){6-8} \n      & & \\#R& \\#N & \\# T & \\#R& \\#N & \\# T  \\\\\n      \\midrule\n      \\multirow{2}{*}{v1} \n      & train& 9 & 2746 & 6678 & 183 & 2000 & 5226 \\\\\n      & test & 9 & 922 & 1991 & 146 & 1500 & 2404 \\\\\n      \\midrule\n      \\multirow{2}{*}{v2} \n      & train&  10 & 6954 & 18968 & 203 & 3000 & 12085 \\\\\n      & test &  10 & 2923 & 4863 & 176 & 2000 & 5092 \\\\\n      \\midrule\n      \\multirow{2}{*}{v3} \n      & train& 11 & 12078 & 32150 & 218 & 4000 & 22394 \\\\\n      & test & 11 & 5084 & 7470 & 187 & 3000 & 9137 \\\\\n      \\midrule\n      \\multirow{2}{*}{v4} \n      & train& 9 & 3861 & 9842 & 222 & 5000 & 33916 \\\\\n      & test & 9 & 7208 & 15157 & 204 & 3500 & 14554 \\\\\n      \\bottomrule\n    \\end{tabular}}\n    \\caption{Statistics of inductive datasets. We use \\#R, \\#N, and \\#T to denote the number of relations, nodes, and triples, respectively.} \\label{tb:data}\n\\end{table}",
            "tb:ablation result": "\\begin{table}\n  \\centering  \n  \\begin{tabular}{lrr}\n    \\toprule\n    & \\multicolumn{2}{c}{WN18RR} \\\\\n    \\cmidrule(lr){2-3}\n    Method & v1 &v4\\\\\n    \\midrule\n    SNRI w/o NRF & 86.96 & 82.26\\\\\n    SNRI w/o NRP & 85.91 & 82.01\\\\\n    SNRI w/o MI  & 84.84 & 82.43\\\\\n    \\midrule\n    SNRI         & 87.23 & 83.32 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Ablation results of Hits@10 on inductive WN18RR v1 and v4.} \\label{tb:ablation result}\n\\end{table}",
            "tb:case result": "\\begin{table}\n  \\centering\n  \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{clr}\n      \\toprule\n      Target Relations & Neighboring relational path & weight\\\\\n      \\midrule\n      \n      \\multirow{2}{*}{\\textit{related\\_form}} &\\textit{(related\\_form, related\\_form, \\_also\\_see)} & 0.51 \\\\\n      &\\textit{(related\\_form,  related\\_form, related\\_form)} & 0.31 \\\\\n      \\midrule\n      \\multirow{2}{*}{\\textit{adjoins}}&\\textit{(country, adjoins, jurisdiction\\_of\\_office)} & 0.40 \\\\\n      &\\textit{(adjoin, adjoins, jurisdiction\\_of\\_office)} & 0.11 \\\\\n      \\midrule\n      \\multirow{2}{*}{\\textit{dated\\_participant}} & \\textit{(people, dated\\_participant, breakup\\_participant)} & 0.99 \\\\\n      &\\textit{(award\\_nominee, dated\\_participant, dated\\_participant)} & 0.01 \\\\\n      \\bottomrule\n    \\end{tabular}\n  }\n  \\caption{Some neighboring relational paths with importance weight in inductive FB15k-237 dataset.} \\label{tb:case result}\n\\end{table}"
        },
        "figures": {
            "fig:eg": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[scale=0.36]{figures/example2.pdf}\n  \\caption{Two explanatory cases in inductive link prediction. Methods based on enclosing subgraph (red paths) can reason on Test graph A, but hard to handle sparse Test graph B. In contrast, our work utilizes neighboring relations not included in enclosing subgraph simultaneously to build neighboring relational paths (green paths) for reasoning on sparse Test graph B. \n    % A toy example for inductive link prediction. Partial relations \\textit{mother\\_of} and \\textit{born\\_in} are excluded by enclosing subgraph (red paths) and sparse Test graph B is hard to utilize enclosing graph.\n    }\\label{fig:eg}\n\\end{figure}",
            "fg:framework": "\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[scale=0.5]{figures/framework2.pdf}\n  \\caption{An overview of our proposed SNRI, which consists of the following steps: 1) extract subgraphs with complete neighboring relations, and initialize the node features by neighboring relational features; 2) feed subgraphs into subgraph neural network to learn representations; 3) maximize MI between subgraph-graph to model neighboring relations in a global way, and 4) train model by a joint strategy.\n  }\\label{fg:framework}\n\\end{figure*}",
            "fg:sparse result": "\\begin{figure}[tp]\n  \\centering\n  \\includegraphics[scale=0.42]{figures/sparse_exp3.png}\n  \\caption{The performance comparison of SNRI and CoMPILE under different ranges of subgraph density on inductive WN18RR v1 and v4.} \\label{fg:sparse result}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n  \\mathbf{h}^{pos}_i = [\\text{one-hot}(d(i, u)) \\oplus \\text{one-hot}( d(i, v) ])),\n\\end{equation}",
            "eq:2": "\\begin{align}\n  \\mathbf{h}^{rel}_i  &= \\sum_{r \\in \\mathcal{N}^r(i)} \\alpha_r \\mathbf{e}_r, \\\\\n  \\alpha_r = \\text{softmax}(\\mathbf{e}_r, \\mathbf{e}_{r_t}) &= \\frac{\\text{exp}(\\mathbf{e}_r^\\top \\mathbf{e}_{r_t})}{\\sum_{r^\\prime \\in \\mathcal{N}^r(i)}\\text{exp}(\\mathbf{e}_{r^\\prime}^\\top \\mathbf{e}_{r_t})},\n\\end{align}",
            "eq:3": "\\begin{equation}\n  \\mathbf{h}^0  _i = \\mathbf{W}_0 [\\mathbf{h}^{rel}_i \\oplus \\mathbf{h}^{pos}_i].\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\mathbf{h}_{i}^{k} =\\sum_{r \\in R} \\sum_{j \\in \\mathcal{N}_{r}(i)} \\alpha_{i, r} \\mathbf{W}_{r}^{k} \\phi(\\mathbf{e}^{k-1}_r, \\mathbf{h}_{j}^{k-1}),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n  \\alpha_{i, r} =\\sigma_{2}\\left(\\mathbf{W}_{2} \\boldsymbol{c}_{i, r}+\\boldsymbol{b}_{2}\\right),\n\\end{equation}",
            "eq:6": "\\begin{equation}\n  \\mathbf{c}_{i, r} =\\sigma_{1}\\left(\\mathbf{W}_{1}\\left[\\mathbf{h}_{i}^{k-1} \\oplus \\mathbf{h}_{j}^{k-1} \\oplus \\mathbf{e}^{k-1}_r \\oplus \\mathbf{e}^{k-1}_{r_t}\\right]+\\mathbf{b}_{1}\\right),\n\\end{equation}",
            "eq:7": "\\begin{equation}\n  \\mathbf{e}_r^{k} = \\mathbf{W}_{rel}^{k} \\mathbf{e}_r^{k-1}.\n\\end{equation}",
            "eq:8": "\\begin{equation}\n  \\mathbf{H}^L = \\operatorname{GRU}(\\mathbf{H}^{L}).\n\\end{equation}",
            "eq:9": "\\begin{equation}\n  \\mathbf{h}_{\\mathcal{G}} = \\frac{1}{|{V}_{\\mathcal{G}}|} \\sum_{i \\in {V}_{\\mathcal{G}}} \\mathbf{h}^L_i,\n\\end{equation}",
            "eq:10": "\\begin{equation}\n  \\mathbf{p} = \\operatorname{GRU}(p) = \\operatorname{GRU}(\\mathbf{e}_{r_u},\\mathbf{e}_{r_t},\\mathbf{e}_{r_v}).\n\\end{equation}",
            "eq:11": "\\begin{equation}\n  \\mathbf{p}_\\mathcal{G} = \\sum_{p \\in \\mathcal{P}} \\beta_p \\mathbf{p}\n\\end{equation}",
            "eq:12": "\\begin{equation}\n  \\beta_{p} = \\frac{\\text{exp}(\\mathbf{p}^\\top \\mathbf{e}_{r_t})}{\\sum_{p^\\prime \\in \\mathcal{P}_{(u,v)}}\\text{exp}(\\mathbf{p}^{\\prime \\top} \\mathbf{e}_{r_t})}.\n\\end{equation}",
            "eq:13": "\\begin{equation}\n  \\mathbf{s}_{\\mathcal{G}} = [\\mathbf{h}_{\\mathcal{G}} \\oplus \\mathbf{p}_{\\mathcal{G}}],\n\\end{equation}",
            "eq:14": "\\begin{equation}\n  f(u, v_t, r) = \\mathbf{W}_s[\\mathbf{h}^L_u \\oplus \\mathbf{h}^L_v\\oplus \\mathbf{e}^L_{r_t} \\oplus \\mathbf{s}_{\\mathcal{G}}],\n\\end{equation}",
            "eq:eq:sup": "\\begin{equation}\\label{eq:sup}\n  \\mathcal{L}_{sup} = \\sum_{(u, r_t, v)\\in \\mathcal{G}} \\text{max}(0, f(u^\\prime, r_t^\\prime, v^\\prime) - f(u, r_t, v) + \\gamma),\n\\end{equation}",
            "eq:15": "\\begin{equation}\n  \\mathbf{s}_{G} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{s}_{ \\mathcal{G}_i},\n\\end{equation}",
            "eq:16": "\\begin{equation}\n  \\mathcal{D}(\\mathbf{s}_{\\mathcal{G}}, \\mathbf{s}_{G}) = \\sigma(\\mathbf{s}_{\\mathcal{G}}^\\top \\mathbf{W}_{MI} \\mathbf{s}_{G}),\n\\end{equation}",
            "eq:17": "\\begin{equation}\n  \\tilde{G}(\\tilde{\\mathbf{X}}, \\mathbf{A}) \\sim \\mathcal{C}(G(\\mathbf{X}, \\mathbf{A})),\n\\end{equation}",
            "eq:eq:MI_loss": "\\begin{align}\\label{eq:MI_loss}\n  \\mathcal{L}_{MI} &=\\frac{1}{N+M}(\\sum_{i=1}^{N} \\mathbb{E}_{(\\mathbf{X}, \\mathbf{A})}\\left[\\log \\mathcal{D}\\left( \\mathbf{s}_{\\mathcal{G}_i}, \\mathbf{s}_{G} \\right)\\right]  \\nonumber \\\\\n  &+ \\sum_{j=1}^{M} \\mathbb{E}_{(\\tilde{\\mathbf{X}}, {\\mathbf{A}})}\\left[\\log \\left(1-\\mathcal{D}\\left(\\tilde{\\mathbf{s}}_{\\mathcal{G}_j}, \\mathbf{s}_{G}\\right)\\right)\\right]),\n\\end{align}",
            "eq:18": "\\begin{equation}\n  \\mathcal{L} = \\mathcal{L}_{sup} + \\lambda \\mathcal{L}_{MI}, \n\\end{equation}"
        },
        "git_link": "https://github.com/Tebmer/SNRI"
    }
}