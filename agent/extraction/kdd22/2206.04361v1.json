{
    "meta_info": {
        "title": "Model Degradation Hinders Deep Graph Neural Networks",
        "abstract": "Graph Neural Networks (GNNs) have achieved great success in various graph\nmining tasks.However, drastic performance degradation is always observed when a\nGNN is stacked with many layers. As a result, most GNNs only have shallow\narchitectures, which limits their expressive power and exploitation of deep\nneighborhoods.Most recent studies attribute the performance degradation of deep\nGNNs to the \\textit{over-smoothing} issue. In this paper, we disentangle the\nconventional graph convolution operation into two independent operations:\n\\textit{Propagation} (\\textbf{P}) and \\textit{Transformation}\n(\\textbf{T}).Following this, the depth of a GNN can be split into the\npropagation depth ($D_p$) and the transformation depth ($D_t$). Through\nextensive experiments, we find that the major cause for the performance\ndegradation of deep GNNs is the \\textit{model degradation} issue caused by\nlarge $D_t$ rather than the \\textit{over-smoothing} issue mainly caused by\nlarge $D_p$. Further, we present \\textit{Adaptive Initial Residual} (AIR), a\nplug-and-play module compatible with all kinds of GNN architectures, to\nalleviate the \\textit{model degradation} issue and the \\textit{over-smoothing}\nissue simultaneously. Experimental results on six real-world datasets\ndemonstrate that GNNs equipped with AIR outperform most GNNs with shallow\narchitectures owing to the benefits of both large $D_p$ and $D_t$, while the\ntime costs associated with AIR can be ignored.",
        "author": "Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, Bin Cui",
        "link": "http://arxiv.org/abs/2206.04361v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "11 pages, 10 figures"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nThe recent success of Graph Neural Networks (GNNs)~\\cite{zhang2020deep} has boosted research on various data mining and knowledge discovery tasks on graph-structured data. \nGNNs provide a universal framework to tackle node-level, edge-level, and graph-level tasks, including social network analysis~\\cite{qiu2018deepinf}, chemistry and biology~\\cite{DBLP:conf/nips/DaiLCDS19}, recommendation~\\cite{wu2020graph, jiang2022zoomer}, natural language processing~\\cite{bastings2017graph}, and computer vision~\\cite{qi2018learning}. \n\n% Following the classic studies in the graph signal processing field, GNN is first proposed in~\\cite{bruna2013spectral}.\n% However, the excessive computation cost of eigendecomposition hinders~\\cite{bruna2013spectral} from real-world practice.\n% GCN proposes a much simplified version of previous convolution operations on graph-structured data, making it the first feasible attempt on GNNs.\n% In recent years, the graph convolution operation proposed by GCN gradually becomes the canonical form of most GNN architectures~\\cite{xx}.\n\nIn recent years, the graph convolution operation proposed by Graph Convolutional Network (GCN)~\\cite{kipf2016semi} gradually becomes the canonical form of layer designs in most GNN models~\\cite{wu2019simplifying,klicpera2018predict,zhang2021rod}.\nSpecifically, the graph convolution operation in GCN can be disentangled into two independent operations: \\textit{Propagation} (\\textbf{P}) and \\textit{Transformation} (\\textbf{T}).\nThe \\textbf{P} operation can be viewed as a particular form of the Laplacian smoothing~\\cite{DBLP:journals/corr/abs-1905-09550},\n% during which each node reinforces its representation by its neighbors.\nafter which the representations of nearby nodes would become similar.\nThe \\textbf{P} operation greatly reduces the difficulties of the downstream tasks since most real-world graphs follow the homophily assumption~\\cite{mcpherson2001birds} that connected nodes tend to belong to similar classes.\nThe \\textbf{T} operation applies non-linear transformations to the node representations, thus enabling the model to capture the data distribution of the training samples.\nAfter the disentanglement, the depth of a GNN is split into the propagation depth ($D_p$) and the transformation depth ($D_t$).\nA GNN with larger $D_p$ enables each node to exploit information from deeper neighborhoods, and a larger $D_t$ gives the model higher expressive power.\n\nDespite the remarkable success of GNNs, deep GNNs are rarely applied in various tasks as simply stacking many graph convolution operations leads to drastic performance degradation~\\cite{kipf2016semi}.\nAs a result, most GNNs today only have shallow architectures~\\cite{kipf2016semi, wu2019simplifying, DBLP:conf/iclr/VelickovicCCRLB18}, which limits their performance.\nMany novel architectures and strategies have been proposed to alleviate this problem, yet they disagree on the major cause for the performance degradation of deep GNNs. \nAmong the suggested reasons, most existing studies~\\cite{feng2020graph,chen2020measuring,DBLP:conf/iclr/ZhaoA20, godwin2021very, rong2019dropedge, miao2021lasagne, DBLP:conf/iclr/ChienP0M21, yan2021two, cai2020note} consider the \\textit{over-smoothing} issue as the major cause for the performance degradation of deep GNNs.\nThe \\textit{over-smoothing} issue~\\cite{li2018deeper} refers to the phenomenon that node representations become indistinguishable after many graph convolution operations.\nIt is proved in~\\cite{zhang2021node} that the differences between the node representations are only determined by the node degrees after applying infinity \\textbf{P} operations.\n\nIn this paper, we conduct a comprehensive analysis to review the \\textit{over-smoothing} issue in deep GNNs and try to identify the major cause for the performance degradation of deep GNNs.\nWe find that the \\textit{over-smoothing} issue does happen after dozens of \\textbf{P} operations, but the performance degradation of deep GNNs is observed far earlier than the appearance of the \\textit{over-smoothing} issue.\nThus, the \\textit{over-smoothing} issue is not the major cause for the performance degradation of deep GNNs.\n\nOn the contrary, the experiment results illustrate that the major cause for the performance decline is the \\textit{model degradation} issue caused by large $D_t$ (i.e., stacking many \\textbf{T} operations).\nThe \\textit{model degradation} issue has been known to the community since the discussion in~\\cite{he2016deep}.\nIt refers to the phenomenon that both the training accuracy and the test accuracy drop when the layer number of the network increases.\nAlthough both are caused by increasing the layer number, the \\textit{model degradation} issue is different from the \\textit{overfitting} issue since the training accuracy remains high in the latter.\nThere have been many studies explaining the causes for the appearance of the \\textit{model degradation} issue in deep neural networks~\\cite{he2016deep, balduzzi2017shattered}.\n\nTo help GNNs to enjoy the benefits of both large $D_p$ and $D_t$, we propose \\textit{Adaptive Initial Residual} (AIR), a plug-and-play module that can be easily combined with all kinds of GNN architectures.\nAdaptive skip connections are introduced among the \\textbf{P} and the \\textbf{T} operations by AIR which alleviates the \\textit{model degradation} issue and the \\textit{over-smoothing} issue at the same time.\nExperiment results on six real-world datasets show that simple GNN methods equipped with AIR outperform most GNNs that only have shallow architectures.\nFurther, simple GNN methods equipped with AIR show better or at least steady predictive accuracy as $D_p$ and $D_t$ increases, which validates the positive effects of AIR on fighting against the \\textit{model degradation} issue and the \\textit{over-smoothing} issue.\n% The source code of simple GNN methods equipped with AIR can be found in Github (\\textit{\\url{https://github.com/PKU-DAIR/SGL/AIR}}).\n\n% \\sza{可用来凑字数，没有实际内容，有些句子和前面重复，要用的话得稍微改一下}\n% The contributions of this paper can be summarized as follows: \n% \\begin{itemize}\n%     \\item \\textit{Novel taxonomy of GNN architectures.}\n%     We disentangle the conventional graph convolution operation into two independent operations: \\textbf{P} and \\textbf{T}.\n%     Following this, the depth of a GNN model is decomposed into the propagation depth ($D_p$) and the transformation depth ($D_t$).\n%     Previous GNN architectures are then classified into three categories according to the ordering of the \\textbf{P} and \\textbf{T} operations.\n%     \\item \\textit{Clarification of previous misunderstanding.}\n%     We conduct a comprehensive analysis of the performance degradation of deep GNNs.\n%     The analysis shows that not the \\textit{over-smoothing} issue but the \\textit{model degradation} issue is the major cause of the performance degradation.\n%     \\item \\textit{Plug-and-play module, AIR.}\n%     We propose a plug-and-play module, \\textit{Adaptive Initial Residual} (AIR), to alleviate the \\textit{model degradation} issue and the \\textit{over-smoothing} issue simultaneously, which is compatible with all kinds of GNN architectures.\n%     Experiment results on seven real-world datasets validates its effects on fighting against the \\textit{model degradation} issue and the \\textit{over-smoothing} issue.\n% \\end{itemize}\n\n% \\sza{可用来凑字数}\n% The rest of the paper is organized as follows, \n\n\n% \\section{Related Work}\n% \\sza{看最后字数要不要加related work}\n\n\n\n\n\n"
            },
            "section 2": {
                "name": "Preliminary",
                "content": "\nIn this section, we first explain the problem formulation.\nThen we disentangle the graph convolution operation in GCN into two independent operations: \\textit{Propagation} (\\textbf{P}) and \\textit{Transformation} (\\textbf{T}).\nThis disentanglement split the depth of a GNN model into the propagation depth ($D_p$) and the transformation depth ($D_t$).\nAfter that, the theoretical benefits of enlarging $D_p$ and $D_t$ will be discussed briefly.\nFinally, we classify the existing GNN architectures into three categories according to the ordering of the \\textbf{P} and \\textbf{T} operations.\n\n\n",
                "subsection 2.1": {
                    "name": "Problem Formalization",
                    "content": "\nIn this paper, we consider an undirected graph $\\mathcal{G}$ = ($\\mathcal{V}$, $\\mathcal{E}$) with $|\\mathcal{V}| = N$ nodes and $|\\mathcal{E}| = M$ edges. \n$\\mathbf{A}$ is the adjacency matrix of $\\mathcal{G}$, weighted or not. \nEach node possibly has a feature vector $\\in \\mathbb{R}^d$, stacking up to an $N \\times d$ matrix $\\mathbf{X}$.\nAnd $\\mathbf{X}_i$ refers to the feature vector of node $i$.\n$\\mathbf{D}=\\operatorname{diag}\\left(d_{1}, d_{2}, \\cdots, d_{n}\\right) \\in \\mathbb{R}^{N \\times N}$ denotes the degree matrix of $\\mathbf{A}$, where $d_{i}=\\sum_{j \\in \\mathcal{V}} \\mathbf{A}_{i j}$ is the degree of node $i$. \nIn this paper, we focus on the semi-supervised node classification task, where only part of the nodes in $\\mathcal{V}$ are labeled.\n$\\mathcal{V}_l$ denotes the labeled node set, and $\\mathcal{V}_u$ denotes the unlabeled node set.\nThe goal of this task is to predict the labels for nodes in $\\mathcal{V}_u$ under the limited supervision of labels for nodes in $\\mathcal{V}_l$.\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Graph Convolution Operation",
                    "content": "\nFollowing the classic studies in the graph signal processing field~\\cite{sandryhaila2013discrete}, the graph convolution operation is first proposed in~\\cite{bruna2013spectral}.\nHowever, the excessive computation cost of eigendecomposition hinders~\\cite{bruna2013spectral} from real-world practice.\nGraph Convolutional Network (GCN)~\\cite{kipf2016semi} proposes a much-simplified version of previous convolution operations on graph-structured data, making it the first feasible attempt on GNNs.\nIn recent years, the graph convolution operation proposed by GCN has gradually become the canonical form of most GNN architectures~\\cite{zhang2020reliable,miao2021degnn,zhang2021rod,yang2020factorizable,yang2020distilling}. \n\nThe graph convolution operation in GCN is formulated as:\n\\begin{equation}\n\\small\n    \\text{Graph Convolution}(\\mathbf{X})=\\sigma\\big(\\mathbf{\\hat{A}}\\mathbf{X}\\mathbf{W}), \\quad \\mathbf{\\hat{A}} = \\widetilde{\\mathbf{D}}^{-\\frac{1}{2}}\\widetilde{\\mathbf{A}}\\widetilde{\\mathbf{D}}^{-\\frac{1}{2}},\n    \\label{eq_GC}\n\\end{equation}\n\n\\noindent where $\\mathbf{\\hat{A}}$ is the normalized adjacency matrix, $\\widetilde{\\mathbf{A}}=\\mathbf{A}+\\mathbf{I}_{N}$ is the adjacency matrix $\\mathbf{A}$ with self loops added, and $\\mathbf{I}_{N}$ is the identity matrix.\n$\\mathbf{\\hat{D}}$ is the corresponding degree matrix of $\\mathbf{\\hat{A}}$.\n$\\mathbf{W}$ is the learnable transformation matrix and $\\sigma$ is the non-linear activation function.\n\nFrom an intuitive view, the graph convolution operation in GCN firstly propagates the representation of each node to their neighborhoods and then transforms the propagated representations to specific dimensions by non-linear transformation.\n\"Graph convolution operation in GCN\" is referred to as \"graph convolution operation\" in the rest of this paper if not specified.\n\n\n"
                },
                "subsection 2.3": {
                    "name": "T",
                    "content": "\nFrom the intuitive view in the above subsection, the graph convolution operation can be disentangled into two consecutive operations realizing different functionalities: \\textit{Propagation} (\\textbf{P}) and \\textit{Transformation} (\\textbf{T}).\nTheir corresponding formula form is as follows:\n\\begin{align}\n    \\label{eq_EP}\n\t\\text{\\textit{Propagation}}(\\mathbf{X}) &= \\textbf{P}(\\mathbf{X}) = \\mathbf{\\hat{A}}\\mathbf{X}, \\\\\n\t\\label{eq_ET}\n\t\\text{\\textit{Transformation}}(\\mathbf{X}) &= \\textbf{T}(\\mathbf{X})= \\sigma(\\mathbf{X}\\mathbf{W}),\n\\end{align}\n\\noindent where $\\mathbf{\\hat{A}}$, $\\mathbf{W}$ and $\\sigma$ all have the same meanings as in Equation~\\ref{eq_GC}.\nIt is evident that conducting the graph convolution operation is equivalent to first conducting the \\textbf{P} operation then conducting the \\textbf{T} operation, which can be expressed as follows:\n\\begin{equation}\n\\small\n    \\text{Graph Convolution}(\\mathbf{X}) = \\textbf{T}(\\textbf{P}(\\mathbf{X})). \\nonumber\n\\end{equation}\n\nGCN defines its model depth as the number of graph convolution operations in the model since it considers one graph convolution operation as one layer.\nHowever, after the disentanglement, we can describe the depths of GNNs more precisely by two new metrics: the propagation depth ($D_p$) and the transformation depth ($D_t$).\n\nFigure~\\ref{fig:GCN} shows an illustrative example of a two-layer GCN.\nTo note that, the GCN will degrade to an MLP if the normalized adjacency matrix $\\mathbf{\\hat{A}}$ is set to the identity matrix $\\mathbf{I}_N$, i.e., removing all the \\textbf{P} operations in the model.\n% More detailed analysis and classification of current GNN approaches can be found in Appendix~\\ref{app:moredetails}.\n\n\n"
                },
                "subsection 2.4": {
                    "name": "Theoretical Benefits of Deep GNNs",
                    "content": "\nGCN achieves the best performance when composed of only two or three layers.\nThere have been many studies aiming at designing deep GNNs recently, and some of them~\\cite{liu2020towards, zhu2021simple} achieve state-of-the-art performance on various tasks.\nIn this subsection, we will briefly discuss the theoretical benefits of deep GNNs and explain that both enlarging $D_p$ and $D_t$ will increase the model expressivity.\n\n",
                    "subsubsection 2.4.1": {
                        "name": "Benefits of enlarging $D_p$",
                        "content": "\nEnlarging $D_p$ is equivalent to enlarging the receptive field of each node.\nIt is proved in~\\cite{morris2019weisfeiler} that GCN has the same expressive power as the first-order Weisfeiler-Lehman graph isomorphism test.\nThus, enlarging the receptive field of each node makes it easier for the model to discriminate between two different nodes since it is more probable that they have highly different receptive fields.\n~\\cite{cong2021provable} proves that once the model is properly trained, the expressive power of GCN grows strictly as the layer number increases due to the enlargement of the receptive field.\nTo sum up, enlarging $D_p$ increases the model expressivity, which can be proved in the view of the Weisfeiler-Lehman test.\n\n"
                    },
                    "subsubsection 2.4.2": {
                        "name": "Benefits of enlarging $D_t$",
                        "content": "\nAnalyzing the change of the model expressivity when enlarging $D_t$ is easier than when enlarging $D_p$.\nAll know that the expressive power of a Multi-Layer Perceptron (MLP) grows strictly along with the increase of the layer number.\nAs introduced in the previous subsection, $D_t$ stands for the number of the non-linear transformations contained in the model.\nThus, enlarging $D_t$, i.e., increasing the number of non-linear transformations, also increases the model expressivity.\n\n\n"
                    }
                },
                "subsection 2.5": {
                    "name": "Three Categories of GNN Architectures",
                    "content": "\nAccording to the ordering the model arranges the \\textbf{P} and \\textbf{T} operations, we roughly classify the existing GNN architectures into three categories: \\textbf{PTPT}, \\textbf{PPTT}, and \\textbf{TTPP}.\n\n",
                    "subsubsection 2.5.1": {
                        "name": "PTPT",
                        "content": "\n\\textbf{PTPT} architecture is the original GNN design that is proposed by GCN~\\cite{kipf2016semi}, and is widely adopted by mainstream GNNs, like GraphSAGE~\\cite{hamilton2017inductive}, GAT~\\cite{DBLP:conf/iclr/VelickovicCCRLB18}, and GraphSAINT~\\cite{DBLP:conf/iclr/ZengZSKP20}.\n\\textbf{PTPT} architecture still uses the graph convolution operation in GCN, where the \\textbf{P} and \\textbf{T} operations are entangled.\nIn a more general view, the \\textbf{P} and \\textbf{T} operations are organized in a order like \\textbf{PTPT}...\\textbf{PT} in the \\textbf{PTPT} architecture.\nAs a result, \\textbf{PTPT} architecture has the strict restriction that $D_p = D_t$.\nFor example, if a model wants to enlarge the receptive field of each node, the natural idea is to enlarge $D_p$.\nHowever, enlarging $D_p$ in the \\textbf{PTPT} architecture requires enlarging $D_t$ at the same time. \nIt would add a significant number of training parameters to the model, which exacerbates training difficulty.\n\n\n"
                    },
                    "subsubsection 2.5.2": {
                        "name": "PPTT",
                        "content": "\n\\textbf{PPTT} architecture is first proposed by SGC~\\cite{wu2019simplifying}, which claims that the strength of GNN lies mainly in not the \\textbf{T} operation but the \\textbf{P} operation.\nIt disentangles the graph convolution operation and presents the \\textbf{PPTT} architecture, where the \\textbf{P} and \\textbf{T} operations are arranged as \\textbf{PP}...\\textbf{PTT}...\\textbf{T}.\nThis architecture is then adopted by many recent GNN studies~\\cite{zhang2022pasca}, e.g., GAMLP~\\cite{zhang2021gamlp}, SIGN~\\cite{frasca2020sign}, S$^2$GC~\\cite{zhu2021simple}, and GBP~\\cite{DBLP:conf/nips/ChenWDL00W20}.\nCompared with \\textbf{PTPT} architecture, \\textbf{PPTT} architecture breaks the chain of $D_p = D_t$, thus has more flexible design options.\nFor the same scenario that a model wants to enlarge the receptive field of each node, \\textbf{PPTT} architecture can add the number of the stacked \\textbf{P} operations (i.e., enlarging $D_p$) without changing $D_t$, which avoids increasing the training difficulty.\nBesides, the \\textbf{P} operations are only needed to be executed once during preprocessing since they can be fully disentangled from the training process.\nThis valuable property of \\textbf{PPTT} architecture enables it with high scalability and efficiency.\n\n\n"
                    },
                    "subsubsection 2.5.3": {
                        "name": "TTPP",
                        "content": "\n\\textbf{TTPP} is another disentangled GNN architecture, which was first proposed by APPNP~\\cite{klicpera2018predict}.\nBeing the dual equivalent to the \\textbf{PPTT} architecture, \\textbf{TTPP} architecture orders the \\textbf{P} and \\textbf{T} operations as \\textbf{TT}...\\textbf{TPP}...\\textbf{P}, where the behavior of the stacked \\textbf{P} operations can be considered as label propagation.\nDAGNN~\\cite{liu2020towards}, AP-GCN~\\cite{spinelli2020adaptive}, GPR-GNN~\\cite{DBLP:conf/iclr/ChienP0M21} and many other GNN models all follow the \\textbf{TTPP} architecture.\nAlthough \\textbf{TTPP} architecture also enjoy the flexibility brought by the disentanglement of the graph convolution operation, it is much less scalable than \\textbf{PPTT} architecture as the stacked \\textbf{P} operations are entangled with the training process, which hinders its application on large graphs.\nOn the positive side, the stacked \\textbf{T} operations can considerably reduce the dimensionality of the input features, which boosts the efficiency of \\textbf{TTPP} architecture in the later stacked \\textbf{P} operations. \n\n\n\n\n\n"
                    }
                }
            },
            "section 3": {
                "name": "Over-smoothing",
                "content": "\nIn this section, we first define \\textit{smoothness level} and introduce metrics to measure it at node and graph levels.\nThen, we review the \\textit{over-smoothing} issue and the reasons why it happens.\nThe rest of this section is an empirical analysis trying to figure out whether the \\textit{over-smoothing} issue is the major cause behind the performance degradation of deep GNNs.\n\n\n",
                "subsection 3.1": {
                    "name": "Smoothness Measurement",
                    "content": "\n\\label{sec.smooth_metric}\nSmoothness level measures the similarities among node pairs in the graph.\nConcretely, a higher smoothness level indicates that it happens with a higher probability that two randomly picked nodes from the given node set are similar.\n\nHere we borrow the metrics from DAGNN~\\cite{liu2020towards} to evaluate the smoothness level both at the node level and graph level.\nHowever, we replace the Euclidean distance in~\\cite{liu2020towards} with the cosine similarity to better measure the similarity between two papers in the citation network since the features of nodes are always constructed by word frequencies.\nWe formally define ``Node Smoothness Level (NSL)'' and ``Graph Smoothness Level (GSL)'' as follows:\n\\begin{definition}[\\textbf{Node Smoothing Level}]\n\\label{df.nsl}\nThe Node Smoothing Level of node $i$, $NSL_i$, is defined as: \n\\begin{equation}\n\\small\nNSL_i = \\frac{1}{N-1}\\sum_{j\\in \\mathcal{V}, j\\neq i}\\frac{\\mathbf{X}_i \\cdot \\mathbf{X}_j}{|\\mathbf{X}_i||\\mathbf{X}_j|}\n\\end{equation}\n% where $\\mathbf{X}_i$ and $\\mathbf{X}_j$ refer to the representation of node $i$ and node $j$, respectively.\n% And $|\\mathbf{X}_i|$ refers to the length of the vector $\\mathbf{X}_i$.\n\\end{definition}\n\n\\begin{definition}[\\textbf{Graph Smoothing Level}]\n\\label{df.gsl}\nThe Graph Smoothing Level of the whole graph, $GSL$, is defined as: \n\\begin{equation}\n\\small\nGSL = \\frac{1}{N}\\sum_{i\\in \\mathcal{V}}NSL_i\n\\end{equation}\n\\end{definition}\n\n$NSL_i$ measures the average similarities between node $i$ and every other node in the graph.\nCorresponding to $NSL_i$, $GSL$ measures the average similarities between the node pairs in the graph.\nNote that both metrics are positively correlated to the smoothness level.\n\n\n\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Over-smoothing",
                    "content": "\n\\label{sec.review_over_smoothing}\nThe \\textit{over-smoothing} issue~\\cite{li2018deeper} describes a phenomenon that the output representations of nodes become indistinguishable after applying the GNN model.\nAnd the \\textit{over-smoothing} issue always happens when a GNN model is stacked with many layers.\n\nIn the conventional \\textbf{PTPT} architecture adopted by GCN, $D_p$ and $D_t$ are restrained to have the same value.\n% , which eliminates the possibility of analysing the respective effect caused by only enlarging $D_p$ or $D_t$.\nHowever, after the disentanglement of the graph convolution operation in this paper, it is feasible to analyze the respective effect caused by only enlarging $D_p$ or $D_t$.\nIn the analysis below, we will show that a huge $D_p$ is the actual reason behind the appearance of the \\textit{over-smoothing} issue.\n\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Enlarging $D_p$",
                        "content": "\nIn the \\textbf{P} operation (Equation~\\ref{eq_EP}), each time the normalized adjacency matrix $\\mathbf{\\hat{A}}$ multiplies with the input matrix $\\mathbf{X}$, information one more hop away can be acquired for each node.\nHowever, if we apply the \\textbf{P} operations for infinite times, the node representations within the same connected component would reach a stationary state, leading to indistinguishable outputs.\nConcretely, when adopting $\\mathbf{\\hat{A}}=\\widetilde{\\mathbf{D}}^{r-1}\\tilde{\\mathbf{A}}\\widetilde{\\mathbf{D}}^{-r}$, $\\mathbf{\\hat{A}}^{\\infty}$ follows\n\\begin{equation}\n\\small\n\\label{eq.stationary}\n\\hat{\\mathbf{A}}^{\\infty}_{i,j} = \\frac{(d_i+1)^r(d_j+1)^{1-r}}{2m+n},\n\\end{equation}\nwhich shows that when $D_p$ approaches $\\infty$, the influence from node $j$ to node $i$ is only determined by their node degrees. \nCorrespondingly, the unique information of each node is fully smoothed, leading to indistinguishable representations, i.e., the \\textit{over-smoothing} issue.\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Enlarging $D_t$",
                        "content": "\nEnlarging $D_t$, the number of the non-linear transformations, has no direct effect on the appearance of the \\textit{over-smoothing} issue.\nTo support this claim, we evaluate the classification accuracies of a fully-disentangled \\textbf{PPTT} GNN model, SGC, and the corresponding $GSL$ on the popular Cora~\\cite{yang2016revisiting} dataset.\nWe enlarge SGC's $D_t$ while fix its $D_p=3$ to rule out the effects $D_p$ poses on the outputs.\nThe experimental results in Figure~\\ref{fig.add_dt} show that the $GSL$ only fluctuates within a small interval.\nThere is no sign that the \\textit{over-smoothing} issue would happen when only $D_t$ increases.\n\nCarried out under the \\textbf{PTPT} architecture,~\\cite{Oono2020Graph} proves that the singular values of the learnable transformation matrix $\\mathbf{W}$ and the non-linear activation function $\\delta$ also correlate with the appearance of the \\textit{over-smoothing} issue.\nHowever, the assumptions~\\cite{Oono2020Graph} adopts are rather rare in real-world scenarios (e.g., assuming a dense graph).\nOn the real-world dataset Cora, the above experiment shows that enlarging $D_t$ has no correlation with the \\textit{over-smoothing} issue under the fully-disentangled \\textbf{PPTT} architecture.\n\n\n"
                    }
                },
                "subsection 3.3": {
                    "name": "Over-smoothing",
                    "content": "\n\\label{sec.misconception}\nMost previous studies~\\cite{li2018deeper, zhang2019attributed} claim that the \\textit{over-smoothing} issue is major cause for the failure of deep GNNs.\nThere have been lines of works aiming at designing deep GNNs.\nFor example, DropEdge~\\cite{rong2019dropedge} randomly removes edges during training, and Grand~\\cite{feng2020graph} randomly drops raw features of nodes before propagation.\nDespite their ability to go deeper while maintaining or even achieving better predictive accuracy, the explanations for their effectiveness are misleading to some extent.\nIn this subsection, we empirically analyze whether the \\textit{over-smoothing} issue is the major cause for the performance degradation of deep GNNs.\n% The experimental analysis about misconceptions other than the \\textit{over-smoothing} issue can be found in Appendix~\\ref{app:misconceptions}.\n\n\n",
                    "subsubsection 3.3.1": {
                        "name": "Relations between $D_p$ and $GSL$",
                        "content": " \nIn Section~\\ref{sec.review_over_smoothing}, we have illustrated that the \\textit{over-smoothing} issue would always happen when $D_p$ approaches $\\infty$.\nHowever, the variation trend of the risk for the appearance of the \\textit{over-smoothing} issue (i.e., $GSL$ of the graph) when $D_p$ grows from a relatively small value is not revealed.\nTo evaluate the single effect enlarging $D_p$ poses on the $GSL$ of the graph, we enlarge $D_p$ in a \\textbf{PPTT} GNN model, SGC~\\cite{wu2019simplifying}, and measure the $GSL$ of the intermediate node representations after all the \\textbf{P} operations.\nThe experiment results on the Cora dataset are shown in Figure~\\ref{fig.smooth_nsl}, and $GSL$ shows a monotonously increasing trend as $D_p$ grows.\n\n\\textit{Remark 1: The risk for the appearance of the \\textit{over-smoothing} issue increases as $D_p$ grows.}\n\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Large $D_p$ Might Not Be The Major Cause",
                        "content": " \nTo investigate the relations between the smoothness level and the node classification accuracy, we increase the number of graph convolution operation in vanilla GCN ($D_p = D_t$) and a modified GCN with $\\hat{\\mathbf{A}}^{2}$ being the normalized adjacency matrix (i.e., $D_p = 2D_t$) on the PubMed dataset~\\cite{yang2016revisiting}.\nSupposing that the \\textit{over-smoothing} issue is the major cause for the performance degradation of deep GNNs, the classification accuracy of the GCN with $D_p=2D_t$ should be much lower than the one of vanilla GCN.\nThe experimental results are shown in Figure~\\ref{fig.smooth_acc}.\nWe can see that even with larger $D_p$ (i.e., higher smoothness level), GCN with $D_p = 2D_t$ always has similar classification accuracy with vanilla GCN ($D_p = D_t$) when $D_t$ ranges from $1$ to $8$, and the excessive number of \\textbf{P} operations seems to begin dominating the performance decline only when $D_p$ exceeds $16$ ($2 \\times 8$).\nHowever, the performance of vanilla GCN starts to drop sharply when $D_p$ exceeds $2$, which is much smaller than $16$ (appearance of the performance gap in Figure~\\ref{fig.smooth_acc}).\n\n\\textit{Remark 2: Considering the performance degradation of deep GNNs often happens even when the layer number is less than $10$, the over-smoothing issue might not be the major cause for it.}\n\n\n"
                    },
                    "subsubsection 3.3.3": {
                        "name": "Large $D_t$ Dominates Performance Degradation",
                        "content": " \nTo dig out the actual limitation of deep GCNs, we adopt a two-layer GCN.\nThe normalized adjacency matrix $\\hat{\\mathbf{A}} $ is set to $(\\widetilde{\\mathbf{D}}^{-\\frac{1}{2}}\\widetilde{\\mathbf{A}}\\widetilde{\\mathbf{D}}^{-\\frac{1}{2}})^{\\lfloor D_p/2 \\rfloor}$ in the first layer of this GCN model; and $\\hat{\\mathbf{A}}$ is set to $(\\widetilde{\\mathbf{D}}^{-\\frac{1}{2}}\\widetilde{\\mathbf{A}}\\widetilde{\\mathbf{D}}^{-\\frac{1}{2}})^{\\lceil D_p/2 \\rceil}$ in the second layer.\nThis modified version of GCN will be referred to as ``GCN with $D_t=2$'' in the rest of the analysis.\nWe report the classification accuracies of vanilla GCN and ``GCN with $D_t=2$'' as $D_p$ increases in Figure~\\ref{fig.smooth_ak}.\nThe experimental results show that the accuracy of ``GCN with $D_t = 2$'' does drop as $D_p$ grows, yet the decline is relatively small, while the accuracy of vanilla GCN (fix $D_p=D_t$) faces a sharp decline.\nThus, it can be inferred that although individually enlarging $D_p$ will increase the risk for the appearance of the \\textit{over-smoothing} issue as the previous analysis shows, the performance is only slightly influenced. \nHowever, the performance will drop drastically if we simultaneously increase $D_t$.\n\n\\textit{Remark 3: Large $D_p$ will harm the classification accuracy of deep GNNs, yet the decline is relatively small.\nOn the contrary, large $D_t$ is the major cause for the performance degradation of deep GNNs.}\n\n\n\n\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "What Is Behind Large $D_t$?",
                "content": "\n\\label{finding2}\nTo learn the fundamental limitation caused by large $D_t$, we first evaluate the classification accuracy of deep MLPs on the PubMed dataset and then extend the conclusions to deep GNNs.\n\n",
                "subsection 4.1": {
                    "name": "Deep MLPs Also Perform Bad",
                    "content": " \nWe evaluate the predictive accuracy of MLP as $D_t$, i.e., the number of MLP layers, grows on the PubMed dataset, and the black line in Figure~\\ref{fig.mlp} shows the evaluation results.\nIt can be drawn from the results that the classification accuracy of MLP also decreases sharply when $D_t$ increases.\nThus, the performance degradation caused by large $D_t$ also exists in MLP.\nIt reminds us that the approaches to easing the training of deep MLPs might also help alleviate the performance degradation caused by large $D_t$ in deep GNNs.\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Skip Connections Can Help",
                    "content": " \nThe widely-used approach that eases the training of deep MLPs is to add skip connections between layers~\\cite{he2016deep, huang2017densely}.\nHere, we add residual and dense connections to MLP and generate two MLP variants: ``MLP+Res'' and ``MLP+Dense'', respectively.\nThe classification accuracies of these two models as $D_t$ grows is shown in Figure~\\ref{fig.mlp}.\nCompared with plain deep MLP, the classification accuracies of both ``MLP+Res'' and ``MLP+Dense'' do not encounter massive decline when $D_t$ increases.\nThe evaluation results illustrate that adding residual or dense connections can effectively alleviate the performance degradation issue caused by large $D_t$ in deep MLPs.\n\n"
                },
                "subsection 4.3": {
                    "name": "Extension to deep GNNs",
                    "content": "\nThe empirical analysis in Section~\\ref{sec.misconception} shows that large $D_t$ is the major cause for the performance degradation of deep GNNs.\nHowever, it remains a pending question whether the widely-used approach to ease the training of deep MLPs can also alleviate the issue of deep GNNs. \nThus, on the PubMed dataset, we evaluate the classification accuracies of ``ResGCN'' and ``DenseGCN'' in~\\cite{li2019deepgcns}, which adds residual and dense connections between GCN layers, respectively.\nThe experimental results in Figure~\\ref{fig.res_acc} illustrate that the performance decline of both ``ResGCN'' and ``DenseGCN'' can be nearly ignored compared to the massive performance decline of GCN.\n\n\n"
                },
                "subsection 4.4": {
                    "name": "What do Skip Connections Help With Here?",
                    "content": "\n",
                    "subsubsection 4.4.1": {
                        "name": "Overfitting?",
                        "content": "\nIt is pretty natural to guess that the \\textit{overfitting} issue is the major cause for the performance degradation resulting from large $D_t$.\nConcretely, the \\textit{overfitting} issue comes from the case when an over-parametric model tries to fit a distribution with limited and biased training data, which results in a large generalization error.\nTo validate whether the \\textit{overfitting} issue is behind large $D_t$, we evaluate the training accuracy and the test accuracy of GCN as the number of layers increases on the PubMed dataset.\nThe evaluation results are shown in Figure~\\ref{fig.overfit}.\nFigure~\\ref{fig.overfit} shows that not only the test accuracy but also the training accuracy drops rapidly as the model layer exceeds $4$.\nThe decline of the training accuracy illustrates that the actual reason behind the performance degradation caused by large $D_t$ is not the \\textit{overfitting} issue.\n\n\n"
                    },
                    "subsubsection 4.4.2": {
                        "name": "Model Degradation!",
                        "content": " \nThe \\textit{mode degradation} issue is first formally introduced in~\\cite{he2016deep}.\nIt refers to the phenomenon that the model performance gets saturated and then degrades rapidly as the model grows deep.\nWhat differentiates it from the \\textit{overfitting} issue is that both the training accuracy and the test accuracy rather than just the test accuracy drops massively.\n~\\cite{he2016deep} does not explain the reasons for the appearance of the \\textit{model degradation} issue but only presents a neat solution -- skip connections.\nMany recent studies are trying to explore what is the leading cause for the \\textit{model degradation} issue, and most of them probe into this problem from the gradient view~\\cite{balduzzi2017shattered}.\nThe trend of the training accuracy and the test accuracy of deep GCN is precisely consistent with the phenomenon the \\textit{model degradation} issue refers to.\n\n\\textit{Remark 4: The \\textit{model degradation} issue behind large $D_t$ is the major cause for the performance degradation of deep GNNs.\nMoreover, adding skip connections between layers can effectively alleviate the performance degradation of deep GNNs.}\n\n\n\n% \\section{Discussions about previous studies}\n% xxx\n\n\n"
                    }
                }
            },
            "section 5": {
                "name": "Adaptive Initial Residual (AIR)",
                "content": "\nUnder the above findings, we propose a plug-and-play module termed \\textit{Adaptive Initial Residual} (AIR), which adds adaptive initial residual connections between the \\textbf{P} and \\textbf{T} operations.\nThe adaptive initial residual connections between the \\textbf{P} operations aim to alleviate the \\textit{over-smoothing} issue and take advantage of deeper information in a node-adaptive manner, while the main aim of the adaptive skip connections between the \\textbf{T} operations is to alleviate the \\textit{model degradation} issue.\nWe will introduce AIR in detail in the remainder of this section.\nAfter that, the applications of AIR to three kinds of GNN architectures will also be discussed.\n\n",
                "subsection 5.1": {
                    "name": "T",
                    "content": "\nAfter the disentanglement of the graph convolution operation, nearly all the GNNs can be split into consecutive parts where each part is a sequence of continuous \\textbf{P} operations or \\textbf{T} operations.\nUnlike skip connections in~\\cite{he2016deep}, we directly construt a connection from the original inputs following~\\cite{chen2020simple}.\n\n",
                    "subsubsection 5.1.1": {
                        "name": "P",
                        "content": "\nFor a sequence of continuous \\textbf{P} operations, we pull an adaptive fraction of input feature $\\mathbf{H}^{(0)}_i=\\mathbf{X}_i$ of node $i$ at each \\textbf{P} operation.\nDenote the representation of node $i$ at the $(l-1)$th operation as $\\mathbf{H}^{(l-1)}_i$.\nThen, the adaptive fraction $\\alpha^{(l)}_i \\in \\mathbb{R}$ of node $i$ at $l$-th operation is computed as follows:\n\\begin{equation}\n\\small\n    \\alpha^{(l)}_i = \\text{sigmoid}\\left[\\left(\\mathbf{H}^{(l-1)}_i\\big|\\mathbf{H}^{(0)}_i\\right)\\mathbf{U}\\right], \n\\end{equation}\n\\noindent where $\\mathbf{U}$ is a learnable vector that transforms the concatenated vector into a scalar.\n\nAll the positions of the $i$th row vector, $\\bm{\\alpha}^{(l)}_i$, of the adaptive weighting matrix $\\bm{\\alpha}^{(l)}$ are occupied by the same element $\\alpha^{(l)}_i$:\n\\begin{equation}\n\\small\n    \\bm{\\alpha}^{(l)}_i = [\\alpha^{(l)}_i, \\alpha^{(l)}_i, ..., \\alpha^{(l)}_i]. \\nonumber\n\\end{equation}\n\nThen, for each $l \\geq 2$, the $l$-th \\textbf{P} operation equipped with AIR within a part can be formulated as follows:\n\\begin{equation}\n\\small\n\\label{eq.p_air}\n    \\mathbf{H}^{(l)} = \\mathbf{\\hat{A}}\\left[(\\mathbf{1}-\\bm{\\alpha}^{(l-1)}) \\odot \\mathbf{H}^{(l-1)}+\\bm{\\alpha}^{(l-1)} \\odot \\mathbf{H}^{(0)}\\right],\n\\end{equation}\n\\noindent where $\\mathbf{H}^{(l)}$ and $\\mathbf{H}^{(l-1)}$ are the representation matrices after the $l$-th and the $(l-1)$th operation, respectively.\n$\\mathbf{1}$ is an all one matrix, and $\\odot$ denotes the Hadamard product.\n$\\mathbf{\\hat{A}}$ is the normalized adjacency matrix in Equation~\\ref{eq_EP} and $\\mathbf{H}^{(0)}$ is the input matrix of this part.\n\n% \\red{equivalence to preprocessing, functionalities}\nEquipped with AIR, the continuous \\textbf{P} operations are equivalent to first computing the propagated inputs, $\\left[\\mathbf{H}^{0}, \\mathbf{\\hat{A}}\\mathbf{H}^{(0)}, \\mathbf{\\hat{A}}^{2}\\mathbf{H}^{(0)}, ...\\right]$, then assigning each of them with learnable coefficient in a node-adaptive manner.\nUnder this view, it is evident that AIR can help alleviate the \\textit{over-smoothing} issue.\nThe model can assign larger coefficients to deeper propagated inputs for nodes that require deep information.\nFor nodes that require only local information, the coefficients for deep propagated features can be assigned with values around zero.\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "T",
                        "content": "\nDifferent from the adaptive initial residual connections between \\textbf{P} operations, the connections between \\textbf{T} operations exclude the learnable coefficients for the input feature, and a fixed one is adopted instead since these two ways are almost equivalent in this scenario. \n\nSimilar to the \\textbf{P} operations introduced above, for each $l \\geq 2$, the $l$-th \\textbf{T} operation equipped with AIR within a part can be formulated as follows:\n\\begin{equation}\n\\small\n    \\mathbf{H}^{(l)} = \\sigma\\left[\\left(\\mathbf{H}^{(l-1)}+\\mathbf{H}^{(0)}\\right)\\mathbf{W}\\right],\n\\end{equation}\n\\noindent where $\\mathbf{W}$ is the learnable transformation matrix and $\\sigma$ is the non-linear activation function.\nNote that the dimensions of the inputs and the latent representations might be different.\nA linear projection layer is adopted to transform the inputs to the given dimension under such scenarios.\n\nThe initial residual connections~\\cite{chen2020simple} that we adopt here share the same intuition with the residual connections in~\\cite{he2016deep} that a deep model should at least achieve the same performance as a shallow one.\nThus, adopting the \\textbf{T} operation with AIR is expected to alleviate the \\textit{model degradation} issue caused by large $D_t$.\n\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Applications to Existing GNNs",
                    "content": "\n",
                    "subsubsection 5.2.1": {
                        "name": "TTPP",
                        "content": "\nFor the disentangled \\textbf{PPTT} and \\textbf{TTPP} GNN architectures, the model can be split into two parts, the one of which only consists of the \\textbf{P} operations, the other one of which only consists of the \\textbf{T} operations.\nThus, the \\textbf{P} and \\textbf{T} operations can be easily replaced by the \\textbf{P} and \\textbf{T} operations equipped with AIR introduced in the above subsection.\n\nThe \\textbf{P} operation equipped with AIR pulls an adaptive amount of the original inputs directly over and fuses it with the representation matrix generated by the previous \\textbf{P} operation. \nThe fused matrix is considered as the new input to the $l$-th \\textbf{P} operation.\nThe \\textbf{T} operation equipped with AIR adds the outputs of the previous \\textbf{T} operation and a fixed amount of the original inputs.\n\nThe examples of adopting AIR under the \\textbf{PPTT} and \\textbf{TTPP} architectures are given in Figure~\\ref{fig.pptt_air} and~\\ref{fig.ttpp_air}, respectively.\n\n\n\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "PTPT",
                        "content": "\nThe AIR for the \\textbf{PTPT} GNN architecture is slightly different from the one for the \\textbf{PPTT} and \\textbf{TTPP} GNN architectures since the \\textbf{P} and \\textbf{T} operations are entangled in the \\textbf{PTPT} architecture.\nThe GNN models under the \\textbf{PTPT} architecture can be deemed as a sequence of the graph convolution operations.\nThus, we construct adaptive initial residual connections directly between the graph convolution operations.\n\nIf the dimensions of the inputs and latent representations are different, a linear projection layer is used to transform the inputs to the given dimension.\nFor each $l \\geq 2$, the $l$-th graph convolution operation equipped with AIR can be formulated as:\n\\begin{equation}\n\\small\n    \\mathbf{H}^{(l)} = \\sigma\\left[\\mathbf{\\hat{A}}\\left((\\mathbf{1}-\\bm{\\alpha}^{(l-1)}) \\odot \\mathbf{H}^{(l-1)}+\\bm{\\alpha}^{(l-1)} \\odot \\mathbf{H}^{(0)}\\right)\\mathbf{W}\\right],\n\\end{equation}\n\\noindent which is equivalent to applying the \\textbf{P} operation equipped with AIR (Equation~\\ref{eq.p_air}) and the \\textbf{T} operation (Equation~\\ref{eq_ET}) consecutively.\n\nFigure~\\ref{fig.ptpt_air} provides an overview of how to adopt AIR under the \\textbf{PTPT} GNN architecture.\n\n\n\n"
                    }
                }
            },
            "section 6": {
                "name": "AIR Evaluation",
                "content": "\nIn this section, we evaluate the proposed AIR under three different GNN architectures.\nWe adapt AIR to three GNN models: SGC~\\cite{wu2019simplifying} (\\textbf{PPTT}), APPNP~\\cite{klicpera2018predict} (\\textbf{TTPP}), and GCN~\\cite{kipf2016semi} (\\textbf{PTPT}), which are representative GNN models for the three GNN architectures, respectively.\nFirstly, we introduce the utilized datasets and the experimental setup.\nThen, we compare SGC+AIR, APPNP+AIR, and GCN+AIR with baseline methods regarding predictive accuracy, ability to go deep, robustness to graph sparsity, and efficiency.\n\n\n\n% \\zwt{图c的结果折现太明显了：做平滑}\n\n",
                "subsection 6.1": {
                    "name": "Experimental Settings",
                    "content": "\n\\label{sec:settings}\n\\noindent\\textbf{Datasets.}\nWe adopt the three popular citation network datasets (Cora, Citeseer, PubMed)~\\cite{DBLP:journals/aim/SenNBGGE08} and three large OGB datasets (ogbn-arxiv, ogbn-products, ogbn-papers100M)~\\cite{hu2020ogb} to evaluate the predictive accuracy of each method on the node classification task.\nTable~\\ref{datasets} in Appendix~\\ref{app:datasets} presents an overview of these six datasets.\n\n\n\\noindent\\textbf{Baselines.}\nWe choose the following baselines: GCN~\\cite{kipf2016semi}, GraphSAGE~\\cite{hamilton2017inductive}, JK-Net~\\cite{xu2018representation}, ResGCN~\\cite{li2019deepgcns}, APPNP~\\cite{klicpera2018predict}, AP-GCN~\\cite{spinelli2020adaptive}, DAGNN~\\cite{liu2020towards}, SGC~\\cite{wu2019simplifying},      SIGN~\\cite{frasca2020sign}, S$^2$GC~\\cite{zhu2021simple}, and GBP~\\cite{DBLP:conf/nips/ChenWDL00W20}.\nThe hyperparameter details for SGC+AIR, APPNP+AIR, GCN+AIR, and all the baseline methods can be found in Appendix~\\ref{app_parameter}.\n\n\n\n"
                },
                "subsection 6.2": {
                    "name": "End-to-End Comparison",
                    "content": " \nThe evaluation results of GCN+AIR, APPNP+AIR, SGC+AIR, and all the compared baselines on the six datasets are summarized in Table~\\ref{table.performance}.\nEquipped with AIR, GCN, APPNP, and SGC all achieve far better performance than their respective original version.\nFor example, the predictive accuracies of GCN+AIR, APPNP+AIR, and SGC+AIR exceed the one of their original version by $0.9\\%$, $0.9\\%$, and $2.2\\%$ on the PubMed dataset, respectively.\nFurther, GCN+AIR, APPNP+AIR, and SGC+AIR also outperform or achieve comparable performance with state-of-the-art baseline methods within their own GNN architectures.\n\nIt is worth noting that the performance advantage of SGC+AIR over compared baseline methods on the two largest datasets, ogbn-products, and ogbn-papers100M, is more significant than the one on the smaller datasets.\nThis contrast is because AIR enables both larger $D_p$ and $D_t$ in GNNs, which helps the model exploit more valuable deep information on large datasets.\n\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Analysis of model depth",
                    "content": "\nIn this subsection, we conduct experiments on the ogbn-arxiv dataset to validate that simple GNN methods can support large $D_p$ and large $D_t$ when equipped with AIR.\n\nWe first increase $D_p$ or $D_t$ individually under the \\textbf{PPTT} and \\textbf{TTPP} architectures.\nIn Figure~\\ref{fig.dp_change}, we fix $D_t=3$ and increase $D_p$ from $1$ to $20$.\nFigure~\\ref{fig.dp_change} shows that SGC+AIR outperforms SGC throughout the experiment, and the performance of APPNP+AIR begins to exceed the one of APPNP when $D_p$ surpasses $10$.\nThe experimental results clearly illustrate that AIR can significantly reduce the risk for the appearance of the \\textit{over-smoothing} issue.\n\nIn Figure~\\ref{fig.dt_change}, we fix $D_p$ to $10$ and increase $D_t$ from $1$ to $10$.\nWhile SGC and APPNP both encounter significant performance drop as $D_t$ exceeds $4$, the predictive accuracies of SGC+AIR and APPNP+AIR maintains or even becomes higher when $D_t$ grows.\nThis sharp contrast illustrates that AIR can greatly alleviate the \\textit{model degradation} issue.\nThus, equipped with AIR, SGC and APPNP can better exploit the deep information and achieve higher predictive accuracy.\n\nUnder the \\textbf{PTPT} architecture, we increase both $D_p$ and $D_t$ since the \\textbf{P} and \\textbf{T} operations are entangled in this architecture.\nThe experimental results are shown in Figure~\\ref{fig.dpdt_change}.\nCompared with baseline methods GCN, JK-Net, and ResGCN, GCN+AIR shows a steady increasing trend in predictive accuracy as the number of layers grows, which again validates the effectiveness of AIR.\n\n\n\n"
                },
                "subsection 6.4": {
                    "name": "Performance-Efficiency Analysis",
                    "content": "\n\\label{effi}\n\n\n\nIn this subsection, we evaluate the efficiency of AIR on the ogbn-arxiv dataset.\nHere we only report the training time since the training stage always consumes the most resources in real-world scenarios.\nThe training time results of SGC, APPNP, and GCN with or without AIR are shown in Figure~\\ref{fig:efficiency}.\n$D_p$ and $D_t$ are fixed to $3$ for all the compared methods, and each method is trained for $500$ epochs.\n\nThe experimental results in Figure~\\ref{fig:efficiency} show that the additional time costs introduced by AIR vary from only $11\\%$ to $26\\%$, based on the training time of their respective original versions.\nThe time costs associated with AIR are perfectly accepted compared with the considerable performance improvement shown in Table~\\ref{table.performance}.\nFurther, Figure~\\ref{fig:efficiency} illustrates that SGC, which belongs to the \\textbf{PPTT} architecture, consumes much less training time than APPNP and GCN, which belong to the \\textbf{TTPP} and \\textbf{PTPT} architecture, respectively.\n\n\n"
                }
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\nIn this paper, we perform an empirical analysis of current GNNs and find the root cause for the performance degradation of deep GNNs: the \\textit{model degradation} issue introduced by large transformation depth ($D_t$). \nThe \\textit{over-smoothing} issue introduced by large propagation depth ($D_p$) does harm the predictive accuracy.\nHowever, we find that the \\textit{model degradation} issue always happens much earlier than the \\textit{over-smoothing} issue when $D_p$ and $D_t$ increase at similar speeds.\nBased on the above analysis, we present Adaptive Initial Residual (AIR), a plug-and-play module that helps GNNs simultaneously support large propagation and transformation depth.\nExtensive experiments on six real-world graph datasets demonstrate that simple GNN methods equipped with AIR outperform state-of-the-art GNN methods, and the additional time costs associated with AIR can be ignored.\n\n\\begin{acks}\nThis work is supported by NSFC (No. 61832001, 61972004), Beijing Academy of Artificial Intelligence (BAAI), and PKU-Tencent Joint Research Lab. Wentao Zhang and Zeang Sheng contributed equally to this work, and Bin Cui is the corresponding author.\n\\end{acks}\n\n% \\section*{Acknowledgments}\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{sample-base}\n\n\\clearpage\n\n\\appendix\n\n\n\n\n\n\n"
            },
            "section 8": {
                "name": "More Misleading Explanations",
                "content": "\n\\label{app:misconceptions}\n",
                "subsection 8.1": {
                    "name": "Entanglement",
                    "content": " \nSome recent works~\\cite{DBLP:conf/sigir/0001DWLZ020, liu2020towards} argue that the major factor compromising the performance of deep GNNs is the entanglement of the \\textbf{P} and \\textbf{T} operations in the current graph convolution operation.\n% For example, DAGNN~\\cite{liu2020towards} claims that the entanglement of the \\textbf{P} and \\textbf{T} operations is the major reason for the performance decline. \nHowever, the evaluation results of ResGCN, DenseGCN, and vanilla GCN on the PubMed dataset when $D_t$ grows in Figure~\\ref{fig.res_acc} show that ResGCN and DenseGCN do not encounter significant performance degradation although they are under the entangled design.\nThus, GNNs can go deep even under the entangled design, and the entanglement of the \\textbf{P} and \\textbf{T} operations may not be the actual limitation of the GNN depth.\n\n\nWhat is worth noting is that previous works~\\cite{zhu2021simple, liu2020towards}, which disentangle the \\textbf{P} and \\textbf{T} operations and state that they support deep architectures, only provide experimental results that illustrate they can go deep on $D_p$.\n% are only able to go deep on the propagation depth, $D_p$.\nTo validate whether they can also go deep on $D_t$, we run DAGNN in two different settings: the first controls $D_t=2$ and increases $D_p$, the second controls $D_t=D_p$ and increases $D_p$.\nThe test accuracy of DAGNN under these two settings on the PubMed dataset is shown in Figure~\\ref{fig:coupled}.\n\nFigure~\\ref{fig:coupled} illustrates that DAGNN experiences massive performance decline when the model owns large $D_t$ rather than only large $D_p$.\nIf the entanglement dominates the performance degradation of deep GNNs, DAGNN should also be able to go deep on $D_t$.\nHowever, the sharp performance decline still exists if we increase $D_t$ along with $D_p$ in DAGNN.\nThus, many recent works that claim they support large GNN depth can only go deep on $D_p$, yet are unable to go deep on $D_t$. \n\n\n"
                },
                "subsection 8.2": {
                    "name": "Gradient Vanishing",
                    "content": "\n% \\red{add gradient explosion}\nGradient vanishing means that the low gradient in the shallow layers makes it hard to train the model weights when the network goes deeper, and it has a domino effect on all of the further weights throughout the network. \n\nTo evaluate whether the gradient vanishing exists in deep GNNs, we perform node classification experiments on the Cora dataset and plot the gradient -- the mean absolute value of the gradient matrix of the first layer in the 2-layer and 7-layer GCN. \nThe experimental results are reported in Figure~\\ref{fig:gradient}. \n\nThe evaluation results show that the gradients of the 7-layer GCN are as large as the ones of the 2-layer GCN, or even larger in the initial training phases, although the test accuracy of the 7-layer GCN is lower than the one of the 2-layer GCN.\nThe explanation for the initial gradient rise of the 7-layer GCN might be that the larger model needs more momentum to adjust and then jump out of the suboptimal local minima in the initial model training stage.\nThus, this experiment illustrates that gradient vanishing is not the leading cause for the performance degradation of deep GNNs.\n\n\n\n\n\n\n"
                }
            },
            "section 9": {
                "name": "Influence of Graph Sparsity on AIR",
                "content": "\n\\label{sparse}\nTo simulate extreme sparse scenarios in the real world, we design three sparsity settings on the PubMed dataset to test the performance of our proposed AIR when faced with the edge sparsity, label sparsity, and feature sparsity issues, respectively.\n\n\n\\para{Edge Sparsity.} We randomly remove some edges in the original graph to simulate the edge sparsity situation.\nThe edges removed from the original graph are fixed across all the compared methods under the same edge remaining rate.\nThe experimental results in Figure~\\ref{fig.sparse_edge} show that all the compared methods perform similarly since the edges play the most crucial role for GNN methods.\nHowever, it can be easily drawn from the results that GCN, APPNP, and SGC all receive considerable performance improvement after being equipped with AIR.\n\n\\para{Label Sparsity.} In the label sparsity setting, we vary the training nodes per class from $1$ to $20$ and report the test accuracy of each compared method.\nThe experimental results in Figure~\\ref{fig.sparse_label} show that the test accuracies of all the compared methods increase as the number of training nodes per class ascends.\nIn the meantime, three GNN methods equipped with AIR all outperform their original version throughout the experiment.\n\n\\para{Feature Sparsity.} In a real-world situation, the features of some nodes in the graph might be missing.\nWe follow the same experimental design in the edge sparsity setting yet remove node features instead of edges.\nThe results in Figure~\\ref{fig.sparse_feat} illustrate that our proposed AIR enables the three GNN baselines great anti-interference abilities when faced with feature sparsity.\nFor example, the test accuracies of SGC+AIR and APPNP+AIR only drop slightly even there is only $20\\%$ node features available.\n\nThe above evaluation under three different sparsity settings illustrates that AIR offers excellent help to simple GNN methods.\nWhen adopted on sparse graphs, more \\textbf{P} operations in GNNs are always needed since there is more hidden information in the graph, which is probably reachable by long-range connections.\nMoreover, more \\textbf{T} operations is also preferred since it offers higher expressive power.\nThus, the ability to go deep on $D_p$ and $D_t$ brought by AIR is the main contributor to the great performance improvement of GNN baselines on sparse graphs.\n\n\n",
                "subsection 9.1": {
                    "name": "Dataset Details",
                    "content": "\n\\label{app:datasets}\nCora, Citeseer, and PubMed are three popular citation network datasets, where nodes stand for research papers, and an edge exists between a node pair if one cites the other.\nThe raw features of nodes in these three datasets are constructed by counting word frequencies.\nThe ogbn-arxiv dataset and ogbn-papers100M are also citation networks, yet much bigger than Cora, Citeseer and PubMed.\nThere are more than 169k and 111M nodes in the ogbn-arxiv dataset and the ogbn-papers100M dataset, respectively.\nThe ogbn-products dataset is an undirected and unweighted graph representing an Amazon product co-purchasing network.\nThe details of the adopted six datasets can be found in Table~\\ref{datasets}.\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Experimental Environment",
                    "content": "\nThe experiments are conducted on a server with two Intel(R) Xeon(R) Platinum 8255C CPUs and a Tesla V100 GPU 32GB version.\nThe operating system of the server is Ubuntu 16.04. \nWe use Python 3.6, PyTorch 1.7.1, and CUDA 10.1 for programming and acceleration.\n\n\n"
                },
                "subsection 9.3": {
                    "name": "Hyperparameter Settings",
                    "content": "\n\\label{app_parameter}\nWe first provide hyperparameter details on the three citation networks.\nFor SGC+AIR, $D_t$ is fixed to $2$, and $D_p$ is set to $10$, $15$, and $30$ on the Cora, Citeseer, and PubMed dataset, respectively.\nAnd hidden size is set to $200$ on all three citation networks, while the learning rate is set to $0.1$ on the Cora and Citeseer datasets, and $0.05$ on the PubMed dataset.\nFor APPNP+AIR, $D_t$, $D_p$ and hidden size is set to $2$, $10$ and $200$ on the three datasets, respectively.\nAnd the learning rate is set to the same values as SGC+AIR.\nFor GCN+AIR, the number of layers is set to $6$, $4$, $16$ on the Cora, Citeseer, and PubMed datasets, respectively.\nThe hidden size is set to $32$ on the Cora and PubMed datasets and $16$ on the Citeseer dataset.\nAnd the learning rate is set to $0.01$ on the Cora and Citeseer datasets and $0.1$ on the PubMed dataset. \n\nThe hyperparameter settings on the three ogbn datasets are as follows:\nFor SGC+AIR, $D_t$ is set to $6$ on the ogbn-arxiv and ogbn-papers100M datasets, $2$ on the ogbn-products dataset.\n$D_p$ is set to $5$ on the ogbn-arxiv and ogbn-products datasets, $15$ on the ogbn-papers100M dataset.\nThe hidden size and learning rate are set to $0.001$ and $1024$ on all three datasets.\nOn the ogbn-arxiv dataset, $D_p$ and hidden size are set to $16$ and $256$ for APPNP+AIR and GCN+AIR.\n$D_t$ is set to $2$ and $16$ for APPNP+AIR and GCN+AIR, respectively.\nThe learning rate is set to $0.005$ and $0.001$ for APPNP+AIR and GCN+AIR, respectively.\n\nOther hyperparameters are tuned with the toolkit OpenBox~\\cite{li2021openbox} or follow the settings in their original paper.\nThe source code can be found in Anonymous Github (\\textit{\\blue{\\url{https://github.com/PKU-DAIR/AIR}}}).\nPlease refer to ``README.md'' in the Github repository for more reproduction details.\n\n\n"
                }
            }
        },
        "tables": {
            "table.performance": "\\begin{table*}[tpb!]\n\\vspace{-1mm}\n\\caption{Test accuracy on the node classification task. ``OOM'' means ``out of memory''.}\n\\vspace{-1mm}\n\\centering\n{\n\\noindent\n\\renewcommand{\\multirowsetup}{\\centering}\n\\resizebox{0.75\\linewidth}{!}{\n\\begin{tabular}{ccccccc}\n\\toprule\n\\textbf{Methods} & \\textbf{Cora} & \\textbf{Citeseer} & \\textbf{PubMed} & \\textbf{ogbn-arxiv} & \\textbf{ogbn-products} & \\textbf{ogbn-papers100M}\\\\\n\\midrule\nGCN& 81.8$\\pm$0.5 & 70.8$\\pm$0.5 &79.3$\\pm$0.7 & 71.74$\\pm$0.29 & OOM & OOM \\\\\nGraphSAGE& 79.2$\\pm$0.6 & 71.6$\\pm$0.5 & 77.4$\\pm$0.5 &71.49$\\pm$0.27 & \\underline{78.29$\\pm$0.16} & 64.83$\\pm$0.15 \\\\\n% \\midrule\n\nJK-Net& 81.8$\\pm$0.5  & 70.7$\\pm$0.7 & 78.8$\\pm$0.7 & 72.19$\\pm$0.21 & OOM & OOM   \\\\\nResGCN& 81.2$\\pm$0.5  & 70.8$\\pm$0.4 & 78.6$\\pm$0.6 & 72.62$\\pm$0.37 & OOM & OOM    \\\\\n\\midrule\n\\textbf{GCN+AIR} & 83.2$\\pm$0.7 & 71.6$\\pm$0.6 & 80.2$\\pm$0.7 & \\textbf{72.69$\\pm$0.28} & OOM & OOM \\\\\n\\midrule\n\nAPPNP& 83.3$\\pm$0.5 & 71.8$\\pm$0.5 & 80.1$\\pm$0.2 & 71.83$\\pm$0.31 & OOM & OOM \\\\\nAP-GCN& 83.4$\\pm$0.3& 71.3$\\pm$0.5& 79.7$\\pm$0.3 &  71.92$\\pm$0.23 & OOM & OOM \\\\\nDAGNN & \\textbf{84.4$\\pm$0.5} & \\underline{73.3$\\pm$0.6} & 80.5$\\pm$0.5  & 72.09$\\pm$0.25 & OOM & OOM \\\\\n\\midrule\n\\textbf{APPNP+AIR} & 83.8$\\pm$0.6 & \\textbf{73.4$\\pm$0.5} & \\underline{81.0$\\pm$0.6}  & 72.16$\\pm$0.22 & OOM & OOM \\\\\n\\midrule\n\nSGC & 81.0$\\pm$0.2 & 71.3$\\pm$0.5 & 78.9$\\pm$0.5  &71.42$\\pm$0.26 & 75.94$\\pm$0.22 & 63.29$\\pm$0.19\\\\\nSIGN & 82.1$\\pm$0.3 & 72.4$\\pm$0.8 & 79.5$\\pm$0.5 & 71.95$\\pm$0.11 & 76.83$\\pm$0.39 & 64.28$\\pm$0.14\\\\\nS$^2$GC& 82.7$\\pm$0.3 & 73.0$\\pm$0.2 &79.9$\\pm$0.3 & 71.83$\\pm$0.31 & 77.13$\\pm$0.24 & 64.73$\\pm$0.21\\\\\nGBP& 83.9$\\pm$0.7 & 72.9$\\pm$0.5 & 80.6$\\pm$0.4 &72.24$\\pm$0.23 & 77.68$\\pm$0.25 & \\underline{65.24$\\pm$0.13}\\\\\n\\midrule \n\\textbf{SGC+AIR} & \\underline{84.0$\\pm$0.6} & 72.0$\\pm$0.5 & \\textbf{81.1$\\pm$0.6}  & \\underline{72.67$\\pm$0.28} & \\textbf{81.44$\\pm$0.16} & \\textbf{67.23$\\pm$0.2} \\\\\n% APPNP+AIR & \\textbf{84.5$\\pm$0.6} & \\textbf{72.4$\\pm$0.5} & \\textbf{81.0$\\pm$0.6}  & \\textbf{72.16$\\pm$0.22} & OOM & OOM \\\\\n% GCN+AIR & \\textbf{83.2$\\pm$0.7} & \\textbf{72.1$\\pm$0.6} & \\textbf{80.2$\\pm$0.7} & \\textbf{72.69$\\pm$0.28} & OOM & OOM \\\\\n\\bottomrule\n\\end{tabular}}}\n\\label{table.performance}\n% \\vspace{-3mm}\n\\end{table*}",
            "datasets": "\\begin{table*}[tpb!]\n\\small\n\\centering\n% \\vspace{-2mm}\n\\caption{Overview of datasets.}\n\\vspace{-2mm}\n\\label{datasets}\n\\resizebox{.80\\linewidth}{!}{\n\\begin{tabular}{cccccccc}\n\\toprule\n\\textbf{Dataset}&\\textbf{\\#Nodes}& \\textbf{\\#Features}&\\textbf{\\#Edges}&\\textbf{\\#Classes}&\\textbf{\\#Train/Val/Test}\\\\\n\\midrule\nCora& 2,708 & 1,433 &5,429&7& 140/500/1,000\\\\\nCiteseer& 3,327 & 3,703&4,732&6& 120/500/1,000\\\\\nPubmed& 19,717 & 500 &44,338&3& 60/500/1,000\\\\\nogbn-arxiv& 169,343 & 128 & 1,166,243 & 40 &  91K/30K/47K \n%44,625/22,312/22,312 \n\\\\\nogbn-products& 2,449,029 & 100 & 61,859,140 & 47 &  %155,310/23,297/54,358 \n196K/49K/2,204K\\\\\nogbn-papers100M & 111,059,956 & 128 & 1,615,685,872 & 172 & \n1,207K/125K/214K\\\\\n% Industry & 1,000,000 & 64 & 1,434,382 & 253 & 5K/10K/30K&short-form video network\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table*}"
        },
        "figures": {
            "fig:GCN": "\\begin{figure}[tpb!]\n% \\vspace{-6mm}\n\t\\centering\n\t\\includegraphics[width=.95\\linewidth]{figure/GCN-fra_1.pdf}\n \t\\vspace{-2mm}\n\t\\caption{The relationship between GCN and MLP.}\n\t\\label{fig:GCN}\n\t\\vspace{-2mm}\n\\end{figure}",
            "fig.oversmooth": "\\begin{figure*}[htp]\n%\\vspace{-6mm}\n\\centering  \n\\subfigure[The influence of $D_p$ to $GSL$.]{\n\\label{fig.smooth_nsl}\n\\includegraphics[width=0.3\\textwidth]{figure/dp_change_gsl.pdf}}\n\\subfigure[The influence of $D_p$ to model performance.]{\n\\label{fig.smooth_acc}\n\\includegraphics[width=0.3\\textwidth]{figure/smooth_acc.pdf}}\n\\subfigure[The influence of $D_t$ to model performance.]{\n\\label{fig.smooth_ak}\n\\includegraphics[width=0.3\\textwidth]{figure/smooth_ak.pdf}}\n\\vspace{-3mm}\n\\caption{\\textit{Over-smoothing} is not the major contributor to the performance degradation of deep GNNs.}\n\\label{fig.oversmooth}\n\\vspace{-2mm}\n\\end{figure*}",
            "fig.add_dt": "\\begin{figure}[htp]\n\t\\centering\n\t\\includegraphics[width=.65\\linewidth]{figure/add_dt.pdf}\n\t\\vspace{-2mm}\n\t\\caption{The influence of $D_t$ to $GSL$.}\n\t\\vspace{-2mm}\n\t\\label{fig.add_dt}\n\\end{figure}",
            "fig.res_dense": "\\begin{figure}[pbt!]\n% \\vspace{-6mm}\n\\centering  \n\\subfigure[Adding skip connections to MLP.]{\n\\label{fig.mlp}\n\\includegraphics[width=0.23\\textwidth]{figure/mlp_res_dense.pdf}}\n% \\hspace{1mm}\n\\subfigure[Adding skip connections to GCN.]{\n\\label{fig.res_acc}\n\\includegraphics[width=0.23\\textwidth]{figure/res_acc.pdf}}\n% \\hspace{1mm}\n\\vspace{-2mm}\n\\caption{The influence of skip connections in MLP and GCN.}\n\\label{fig.res_dense}\n\\vspace{-2mm}\n\\end{figure}",
            "fig.overfit": "\\begin{figure}[tp!]\n\t\\centering\n\t\\includegraphics[width=.65\\linewidth]{figure/overfit.pdf}\n\t\\vspace{-2mm}\n\t\\caption{The influence of transformation depth in GCN.}\n\t\\label{fig.overfit}\n\t\\vspace{-2mm}\n\\end{figure}",
            "fig.depth_ana": "\\begin{figure*}[tbp!]\n% \\vspace{-3mm}\n\\centering  \n\\subfigure[fix $D_t$ change $D_p$ under \\textbf{PPTT} and \\textbf{TTPP}]{\n\\label{fig.dp_change}\n\\includegraphics[width=0.3\\textwidth]{figure/dp_change.pdf}}\n\\subfigure[fix $D_p$ change $D_t$ under \\textbf{PPTT} and \\textbf{TTPP}]{\n\\label{fig.dt_change}\n\\includegraphics[width=0.3\\textwidth]{figure/dt_change.pdf}}\n\\subfigure[change $D_p$ ($=D_t$) under \\textbf{PTPT}]{\n\\label{fig.dpdt_change}\n\\includegraphics[width=0.3\\textwidth]{figure/dpdt_change.pdf}}\n\\vspace{-1mm}\n\\caption{Test accuracy with varied $D_p$s and $D_t$s under different GNN architectures.}\n\\label{fig.depth_ana}\n\\vspace{-1mm}\n\\end{figure*}",
            "fig:efficiency": "\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=.65\\linewidth]{figure/efficiency.pdf}\n  \t\\vspace{-1mm}\n\t\\caption{Efficiency comparison on the ogbn-arxiv dataset.}\n\t\\label{fig:efficiency}\n \t\\vspace{-2mm}\n\\end{figure}",
            "fig.three_figs": "\\begin{figure*}[tpb!]\n% \\vspace{-2.5mm}\n\\centering  \n\\subfigure[Fixing $D_t=D_p$ degrades the performance badly when $D_p$ becomes large on the Cora dataset.]{\n\\label{fig:coupled}\n\\includegraphics[width=0.3\\textwidth]{figure/coupled.pdf}}\n\\hspace{4mm}\n\\subfigure[First layer gradient comparison of GCN with different layers on the Cora dataset.]{\n\\label{fig:gradient}\n\\includegraphics[width=0.3\\textwidth]{figure/grad.pdf}}\n\\vspace{-3mm}\n\\caption{Entanglement and gradient vanishing are not the major cause for the performance degradation of deep GNNs.}\n\\label{fig.three_figs}\n\\vspace{-2mm}\n\\end{figure*}",
            "fig.sparsity": "\\begin{figure*}[tpb!]\n\\centering  \n\\subfigure[Edge Sparsity]{\n\\includegraphics[width=0.30\\textwidth]{figure/edge_exp.pdf}\n\\label{fig.sparse_edge}\n}\\hspace{-1mm}\n\\subfigure[Label Sparsity]{\n\\includegraphics[width=0.315\\textwidth]{figure/label_exp.pdf}\n\\label{fig.sparse_label}\n}\\hspace{-1mm}\n\\subfigure[Feature Sparsity]{\n\\includegraphics[width=0.31\\textwidth]{figure/feature_exp.pdf}\n\\label{fig.sparse_feat}\n}\\hspace{-1mm}\n\\vspace{-2mm}\n\\caption{Test accuracy on the PubMed dataset under different levels of feature, edge and label sparsity.}\n\\label{fig.sparsity}\n\\vspace{-2mm}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\small\n    \\text{Graph Convolution}(\\mathbf{X})=\\sigma\\big(\\mathbf{\\hat{A}}\\mathbf{X}\\mathbf{W}), \\quad \\mathbf{\\hat{A}} = \\widetilde{\\mathbf{D}}^{-\\frac{1}{2}}\\widetilde{\\mathbf{A}}\\widetilde{\\mathbf{D}}^{-\\frac{1}{2}},\n    \\label{eq_GC}\n\\end{equation}",
            "eq:2": "\\begin{align}\n    \\label{eq_EP}\n\t\\text{\\textit{Propagation}}(\\mathbf{X}) &= \\textbf{P}(\\mathbf{X}) = \\mathbf{\\hat{A}}\\mathbf{X}, \\\\\n\t\\label{eq_ET}\n\t\\text{\\textit{Transformation}}(\\mathbf{X}) &= \\textbf{T}(\\mathbf{X})= \\sigma(\\mathbf{X}\\mathbf{W}),\n\\end{align}",
            "eq:3": "\\begin{equation}\n\\small\n    \\text{Graph Convolution}(\\mathbf{X}) = \\textbf{T}(\\textbf{P}(\\mathbf{X})). \\nonumber\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\small\n\\label{eq.stationary}\n\\hat{\\mathbf{A}}^{\\infty}_{i,j} = \\frac{(d_i+1)^r(d_j+1)^{1-r}}{2m+n},\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\small\n    \\alpha^{(l)}_i = \\text{sigmoid}\\left[\\left(\\mathbf{H}^{(l-1)}_i\\big|\\mathbf{H}^{(0)}_i\\right)\\mathbf{U}\\right], \n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\small\n    \\bm{\\alpha}^{(l)}_i = [\\alpha^{(l)}_i, \\alpha^{(l)}_i, ..., \\alpha^{(l)}_i]. \\nonumber\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\small\n\\label{eq.p_air}\n    \\mathbf{H}^{(l)} = \\mathbf{\\hat{A}}\\left[(\\mathbf{1}-\\bm{\\alpha}^{(l-1)}) \\odot \\mathbf{H}^{(l-1)}+\\bm{\\alpha}^{(l-1)} \\odot \\mathbf{H}^{(0)}\\right],\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\small\n    \\mathbf{H}^{(l)} = \\sigma\\left[\\left(\\mathbf{H}^{(l-1)}+\\mathbf{H}^{(0)}\\right)\\mathbf{W}\\right],\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\small\n    \\mathbf{H}^{(l)} = \\sigma\\left[\\mathbf{\\hat{A}}\\left((\\mathbf{1}-\\bm{\\alpha}^{(l-1)}) \\odot \\mathbf{H}^{(l-1)}+\\bm{\\alpha}^{(l-1)} \\odot \\mathbf{H}^{(0)}\\right)\\mathbf{W}\\right],\n\\end{equation}"
        },
        "git_link": "https://github.com/PKU-DAIR/AIR"
    }
}