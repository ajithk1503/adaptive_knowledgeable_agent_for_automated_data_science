{
    "meta_info": {
        "title": "PolarFormer: Multi-camera 3D Object Detection with Polar Transformer",
        "abstract": "3D object detection in autonomous driving aims to reason \"what\" and \"where\"\nthe objects of interest present in a 3D world. Following the conventional\nwisdom of previous 2D object detection, existing methods often adopt the\ncanonical Cartesian coordinate system with perpendicular axis. However, we\nconjugate that this does not fit the nature of the ego car's perspective, as\neach onboard camera perceives the world in shape of wedge intrinsic to the\nimaging geometry with radical (non-perpendicular) axis. Hence, in this paper we\nadvocate the exploitation of the Polar coordinate system and propose a new\nPolar Transformer (PolarFormer) for more accurate 3D object detection in the\nbird's-eye-view (BEV) taking as input only multi-camera 2D images.\nSpecifically, we design a cross attention based Polar detection head without\nrestriction to the shape of input structure to deal with irregular Polar grids.\nFor tackling the unconstrained object scale variations along Polar's distance\ndimension, we further introduce a multi-scalePolar representation learning\nstrategy. As a result, our model can make best use of the Polar representation\nrasterized via attending to the corresponding image observation in a\nsequence-to-sequence fashion subject to the geometric constraints. Thorough\nexperiments on the nuScenes dataset demonstrate that our PolarFormer\noutperforms significantly state-of-the-art 3D object detection alternatives.",
        "author": "Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, Yu-Gang Jiang",
        "link": "http://arxiv.org/abs/2206.15398v6",
        "category": [
            "cs.CV",
            "cs.AI"
        ],
        "additionl_info": "Accepted to AAAI2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n\n3D object detection is an enabling capability of autonomous driving in unconstrained real-world scenes \\cite{wang2022detr3d,wang2021fcos3d}.\nIt aims to predict the location, dimension and orientation of individual objects of interest in a 3D world.\nDespite favourable cost advantages, multi-camera based 3D object detection~\\cite{wang2021fcos3d,wang2022probabilistic,wang2022detr3d,zhou2019objects} remains particularly challenging.\nTo obtain 3D representation, \ndense depth estimation is often leveraged \\cite{philion2020lift},\nwhich however is not only expensive in computation\nbut error prone.\nTo bypass depth inference, more recent methods \\cite{wang2022detr3d,li2022bevformer} exploit \nquery-based 2D detection~\\cite{carion2020end}\nto learn a set of sparse and virtual embedding for multi-camera 3D object detection, yet incapable of effectively modeling the geometry structure among objects.\nTypically, the canonical Cartesian coordinate system with {\\em perpendicular} axis is adopted in\neither 2D \\cite{zhou2019objects,wang2021fcos3d} or 3D \\cite{wang2022detr3d,li2022bevformer} space.\nThis is largely restricted by convolution based models used.\nIn contrast, the physical world perceived under each camera {\\em in the ego car's perspective} is in shape of wedge intrinsic to the camera imaging geometry with radical \n{\\em non-perpendicular} axis (Figure \\ref{fig:polarcardi}).\nBearing this imaging property in mind, we conjugate that the Polar coordinate system should be more appropriate and natural than the often adopted Cartesian counterpart\nfor 3D object detection.\nIndeed, the Polar coordinate has been exploited in a few LiDAR-based 3D perception methods \\cite{zhang2020polarnet,bewley2020range,rapoport2021s,zhu2021cylindrical}.\nHowever, they are limited algorithmically due to the adoption of convolutional networks restricted to rectangular grid structure and local receptive fields.\n\nMotivated by the aforementioned insights, in this work a novel {\\bf\\em Polar Transformer} (PolarFormer) model for multi-camera 3D object detection in a Polar coordinate system is introduced (Figure~\\ref{fig:learning_targets}).\nSpecifically, we first learn the representation of polar rays corresponding to image regions in a sequence-to-sequence cross-attention formulation.\n%\nThen we rasterize a BEV Polar representation consisting of a set of Polar rays evenly distributed around 360 degrees.\n%\nTo deal with irregular Polar grids as suffered by conventional LiDAR based solutions \\cite{zhang2020polarnet,bewley2020range,rapoport2021s,zhu2021cylindrical}, \nwe propose a cross-attention based decoder head design without\nrestriction to the shape of input structure.\n%\nFor tackling the challenge of unconstrained object scale variation along Polar's distance dimension, we resort to a multi-scale Polar BEV representation learning strategy.\n\n%%%%%%%%%%%% contribution\nThe {\\bf contributions} of this work are summarized as follows: \n%\n\\textbf{(I)}\nWe propose a new {\\em Polar Transformer} (PolarFormer) model for multi-camera 3D object detection in the Polar coordinate system.\n%\n\\textbf{(II)}\nThis is achieved based on two Polar-tailored designs:\nA cross-attention based decoder design\nfor dealing with the irregular Polar girds,\nand a multi-scale Polar representation learning strategy\nfor handling the unconstrained object scale variations over Polar's distance dimension.\n%\n\\textbf{(III)}\nExtensive experiments on the nuScenes dataset show that our PolarFormer achieves leading performance for camera-based 3D object detection (Figure \\ref{fig:teaser})."
            },
            "section 2": {
                "name": "Related work",
                "content": "\n\n\\paragraph{Monocular/multi-camera 3D object detection}\nImage-based 3D object detection aims to estimate the object location, dimension and orientation in the 3D space alongside its category given only image input.\n%\nTo solve this ill-posed problem, \n\\cite{zhou2019objects,wang2021fcos3d} naively build their detection pipelines by augmenting 2D detectors~\\cite{zhou2019objects,tian2019fcos} in a data driven fashion.\n%\nPGD~\\cite{wang2022probabilistic} further captures the uncertainty and models the relationship of different objects by utilizing geometric prior. \nContrast to the above image-only methods, depth-based methods~\\cite{xu2018multi,ding2020learning,wang2019pseudo,you2019pseudo,ma2019accurate,reading2021categorical} use depth cues as 3D information to mitigate the naturally ill-posed problem.\n%\nRecently, multi-camera-based 3D object detection emerges. DETR3D~\\cite{wang2022detr3d} considers detecting objects across all cameras collectively. \n%\nIt learns a set of sparse and virtual query embedding, without explicitly building the geometry structure among objects/queries. \n%\n~\\cite{li2022bevformer} considers detecting objects in BEV,\n%\nperforming end-to-end object detection via object queries.\nNote that multi-camera setting uses the same amount of training data as the monocular pipelines.\nBoth multi-camera and monocular paradigms share the same evaluation metrics.\n%\n\\paragraph{Bird's-eye-view (BEV) representation}\nRecently there is a surge of interest in transforming the monocular or multi-view images from ego car cameras into the bird's-eye-view coordinate~\\cite{roddick2018orthographic,philion2020lift,li2021hdmapnet,roddick2020predicting,reading2021categorical,saha2021translating} followed by specific optimization tasks (\\eg,~3D object detection, semantic segmentation).\n%\nA natural solution~\\cite{philion2020lift,hu2021fiery} is to \nlearn the BEV representation by leveraging the pixel-level dense depth estimation. This however is error-prone due to lacking ground-truth supervision.\n%\nAnother line of research aims to bypass the depth prediction and directly leverage a Transformer~\\cite{chitta2021neat,can2021structured,saha2021translating} or a FC layer~\\cite{li2021hdmapnet,roddick2020predicting,yang2021projecting} to learn the transformation from camera inputs to the BEV coordinate.\n%\nA similar attempt as ours is conducted in \\cite{saha2021translating} but limited in a couple of aspects:\n{\\bf (i)}\nIt is restricted to monocular input for a straightforward 2D segmentation task while we consider multiple cameras collectively for more challenging 3D object detection;\n{\\bf (ii)}\nWe uniquely provide a solid multi-scale Polar BEV transformation to tackle the unconstrained object scale variations and followed by a jointly optimized cross-attention based Polar head.\n%\n\\paragraph{3D object detection in Polar coordinate}\n3D object detection in the Polar or Polar-like coordinate system\nhas been attempted in LiDAR-based perception methods.\nFor example, CyliNet~\\cite{zhu2021cylindrical} introduces range-based guidance for extracting Polar-consistent features.\n%\nIn particular, it adapts a Cartesian heatmap to a Polar version for object classification, whilst learning relative heading angles and velocities. However, CyliNet still lags clearly behind the Cartesian counterpart. \nRecently, PolarSteam~\\cite{chen2021polarstream} designs a learnable sampling module for relieving object distortion in Polar coordinate and uses range-stratified convolution and normalization for flexibly extracting the features over different ranges. \n%\nLimited by the convolution based network, it remains inferior \ndespite of these special designs.\n%\nIn contrast to all these works, we resort to the cross-attention mechanism, tackling the challenges of object scale variance and appearance distortion in the Polar coordinate principally."
            },
            "section 3": {
                "name": "Method",
                "content": "\nIn 3D object detection task, we are given a set of $N$ monocular views $\\{\\mathbf{I_n}, \\mathbf{\\Pi_n}, \\mathbf{E_n}\\}_{n=1}^N$ consisting of input images $\\mathbf{I_n} \\in \\mathbb{R}^{H\\times W\\times 3}$, camera intrinsics $\\mathbf{\\Pi_n} \\in \\mathbb{R}^{3\\times3}$ and camera extrinsics $\\mathbf{E_n} \\in \\mathbb{R}^{4\\times4}$. \nThe objective of our {\\em Polar Transformer} (PolarFormer) is to learn an effective \nBEV Polar representation \nfrom multiple camera views for facilitating the prediction of object locations, dimensions, orientations and velocities in the Polar coordinate system.\n%\nPolarFormer consists of the following components.\nA {\\em cross-plane encoder} first\nproduces a multi-scale feature representation of\neach input image, characterized by a cross-plane attention mechanism in which Polar queries attend to input images to generate 3D features in BEV. \nA {\\em Polar alignment module} then aggregates Polar rays from multiple camera views to generate a structured Polar map. Further, a {\\em BEV Polar encoder} enhances the Polar features with multi-scale feature interaction. Finally, a {\\em Polar detection head} decodes the Polar map and predicts the objects in the Polar coordinate system. \n%\nFor tackling the unconstrained object scale variation with multi-granularity of details,\nwe consider a multi-scale BEV Polar representation structure. \nAs shown in Figure \\ref{fig:multi-scale}, image features with different scales have unique cross-plane encoders\nand interact with each other in a shared Polar BEV encoder. \nMulti-scale Polar BEV maps are then queried by Polar decoder head.\nAn overall architecture of PolarFormer is depicted in Figure \\ref{fig:pipeline}.\n\n",
                "subsection 3.1": {
                    "name": "Cross-plane encoder",
                    "content": "\nThe goal of cross-plane encoder is to associate an image with BEV Polar rays. According to the geometric model of camera, for any camera coordinate $ (x^{(C)}, y^{(C)}, z^{(C)})\\in \\mathbb{R}^3$, the transformation to image coordinate $ (x^{(I)}, y^{(I)})\\in \\mathbb{R}^2$could be described as:\n\\begin{equation}\\label{eq:cam2img}\n    s\\begin{bmatrix}\n    x^{(I)}\\\\\n    y^{(I)}\\\\\n    1\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n    f_x & 0 & u_0 \\\\\n    0 & f_y & v_0 \\\\\n    0 & 0 & 1\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    x^{(C)} \\\\\n    y^{(C)} \\\\\n    z^{(C)} \n    \\end{bmatrix},\n\\end{equation}\nwhere $f_x$, $f_y$, $u_0$ and $v_0$ are camera intrinsic parameters in $\\mathbf{\\Pi}$, $x^{(C)}$, $y^{(C)}$ and $z^{(C)}$ are the horizontal, vertical, depth coordinate respectively. $s$ is the scale factor. For any BEV Polar coordinate $(\\rho^{(P)}, \\phi^{(P)})$, we have:\n\\begin{equation}\\label{eq:amizuth}\n    \\phi^{(P)} = \\arctan \\frac{x^{(C)}}{z^{(C)}} = \\arctan \\frac{x^{(I)}-u_0}{f_x},\n\\end{equation}\n\\begin{equation}\\label{eq:radius}\n\\begin{split}\n    \\rho^{(P)} = \\sqrt{(x^{(C)})^2 + (z^{(C)})^2} \n         =z^{(C)}\\sqrt{(\\frac{x^{(I)}-u_0}{f_x})^2+1}.\n\\end{split}\n\\end{equation}\nEq. \\eqref{eq:amizuth} suggests that the azimuth $\\phi^{(P)}$ is irrelevant to the vertical value of image coordinate. It is hence natural to build a one-to-one relationship between Polar rays and image columns \\cite{saha2021translating}. However, we need object depth $z^{(C)}$ to compute radius $\\rho^{(P)}$, the estimation of which is ill-posed. Instead of explicit depth estimation, we leverage \nthe attention mechanism~\\cite{vaswani2017attention} to model the relationship between pixels along the image column and positions along the Polar ray.  \n\nLet $\\mathbf{f}_{n,u,w} \\in \\mathbb{R}^{H_u\\times C}$ represent the image column from $n$th camera, $u$th scale and $w$th column, and\n$\\mathbf{\\dot{p}}_{n,u,w} \\in \\mathbb{R}^{R_u \\times C}$ denote the corresponding {\\em Polar ray} query we introduce, where $H$ and $R$ are the image feature map's height and Polar map's range. We formulate cross-plane attention as:\n% is formulated as:\n\\begin{equation}\n  \\begin{split}\n  \\mathbf{p}_{n,u,w} &= \\mathrm{MultiHead}(\\mathbf{\\dot{p}}_{n,u,w},\\mathbf{f}_{n,u,w},\\mathbf{f}_{n,u,w}) \\\\\n  &= \\mathrm{Concat}(\\mathrm{head_1}, \\dots, \\mathrm{head_h})\\mathbf{W}^O_u,\n  \\end{split}\n\\end{equation}\nwhere\n\\begin{equation}\n  \\begin{split}\n  \\mathrm{head_i} \\!=\\! \\mathrm{Attention}(\\mathbf{\\dot{p}}_{n,u,w}\\mathbf{W}_{i,u}^Q, \\mathbf{f}_{n,u,w}\\mathbf{W}_{i,u}^K, \\mathbf{f}_{n,u,w}\\mathbf{W}_{i,u}^V),\n  \\end{split}\n\\end{equation}\nwhere $\\mathbf{W}_{i,u}^Q \\in \\mathbb{R}^{d_{model} \\times d_q}$, $\\mathbf{W}_{i,u}^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $\\mathbf{W}_{i,u}^V \\in \\mathbb{R}^{d_{model} \\times d_v}$, $\\mathbf{W}^O_u \\in \\mathbb{R}^{hd_{model} \\times d_k}$\nare the projection parameters, $d_q=d_k=d_v=d_{model}/h$, $d$ is the feature dimension and $h$ is the number of heads. \n\nStacking the Polar ray features $\\mathbf{p}_{n,u,w} \\in \\mathbb{R}^{R_u \\times C}$ along azimuth axis, we obtain the Polar feature map (\\ie, {\\em BEV Polar representation}) $\\mathbf{P}_{n,u}$ for $n$th camera and $u$th scale as:\n\\begin{equation}\n    \\mathbf{P}_{n,u}\\!=\\!\\mathrm{Stack}([\\mathbf{p}_{n,u,1}, \\dots,\\mathbf{p}_{n,u,W_u}], \\mathrm{dim}\\!=\\!1) \\!\\in\\! \\mathbb{R}^{R_u \\!\\times\\! W_u \\!\\times\\! C },\n\\end{equation}\nwhere $W_u$ denotes the azimuth dimension.\nThis sequence-to-sequence cross-attention-based encoder can encode geometric imaging prior and implicitly learn a proxy for depth efficiently. Next, we show how to integrate independent Polar rays from multiple cameras into a coherent and structured Polar BEV map. \n\n"
                },
                "subsection 3.2": {
                    "name": "Polar alignment across multiple cameras",
                    "content": "\nOur Polar alignment module transforms Polar rays from different camera coordinates to a shared world coordinate. Taking multi-view Polar feature maps $\\{\\mathbf{P}_{n,u}\\}_{n=1}^N$ and camera matrix $\\{\\mathbf{\\Pi}_n,\\mathbf{E}_n\\}_{n=1}^N$as inputs, it produces a coherent BEV Polar map $\\mathbf{G}_u \\in \\mathbb{R}^{\\mathcal{R}_u\\times\\mathcal{N}_u\\times \\mathcal{C}}$, covering all camera views, where $\\mathcal{R}_u$, $\\mathcal{N}_u$ and $\\mathcal{C}$ are the dimensions of radius, azimuth and feature. \nConcretely, it first generates a set of 3D points in the cylindrical coordinate uniformly, denoted by $\\mathcal{G}^{(P)}=\\{(\\rho^{(P)}_{i},\\phi^{(P)}_{j},z^{(P)}_{k})|i=1,\\dots,\\mathcal{R}_u;j=1,\\dots,\\mathcal{N}_u; k=1,\\dots,\\mathcal{Z}_u\\}$, where $\\mathcal{Z}_u$ is the number of points along $z$ axis. Since cylindrical coordinate and Polar coordinate share radius and azimuth axis, their superscripts are both denoted with $P$. The points are then projected to the image plane of $n$th camera to retrieve the index of Polar ray, estimated by:\n\\begin{equation}\\label{eq:alignment_proj}\n    \\begin{bmatrix}\n    sx^{(I)}_{i,j,k,n} \\\\[5pt]\n    sy^{(I)}_{i,j,k,n}\\\\[5pt]\n    s \\\\[5pt]\n    1\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n    \\mathbf{\\Pi}_n & 0 \\\\\n    0 & 1\n    \\end{bmatrix}\n    \\mathbf{E}_n\n    \\begin{bmatrix}\n    \\rho^{(P)}_i\\sin{\\phi^{(P)}_j} \\\\[5pt]\n    \\rho^{(P)}_i\\cos{\\phi^{(P)}_j} \\\\[5pt]\n    z^{(P)}_k \\\\[5pt]\n    1\n    \\end{bmatrix},\n\\end{equation}\nwhere $s$ is the scale factor. Coherent BEV Polar map of $u$th scale can be then generated by:\n\\begin{equation}\n\\label{eq:corherent_sampling}\n\\begin{split}\n    &\\mathbf{G}_u(\\rho^{(P)}_i,\\phi^{(P)}_j)\n    =\\frac{1}{\\sum_{n=1}^N\\sum_{k=1}^\\mathcal{Z} \\lambda_n(\\rho^{(P)}_i,\\phi^{(P)}_j,z^{(P)}_k)} \\\\ \n    & \\cdot \\sum_{n=1}^N\\sum_{k=1}^\\mathcal{Z}\\lambda_n(\\rho^{(P)}_i,\\phi^{(P)}_j,z^{(P)}_k){\\mathcal{B}(\\mathbf{P}_{n,u},(\\overline{x}^{(I)}_{i,j,k,n}, \\overline{r}_{i,j,n}))},\n\\end{split}\n\\end{equation}\nwhere $\\lambda_n(\\rho^{(P)}_i,\\phi^{(P)}_j,z^{(P)}_k)$ is binary weighted factor indicating visibility in $n$th camera, $\\mathcal{B}(\\mathbf{P}, (x,y))$ denotes the bilinear sampling $\\mathbf{P}$ at location $(x,y)$, $\\overline{x}^{(I)}_{i,j,k,n}$ and  $\\overline{r}_{i,j,n}$ denote the normalized Polar ray index and radius index. Note the radius $r$ is the distance between the point and the camera origin in BEV. Our Polar alignment module incorporates the features at different heights by generating the points along $z$ axis. \nAs validated in Table \\ref{table:coordinate}, learning Polar representation is superior over Cartesian coordinate due to  \nminimal information loss and higher consistency with raw visual data.\n\n"
                },
                "subsection 3.3": {
                    "name": "Polar BEV encoder at multiple scales",
                    "content": "\nWe leverage multi-scale feature maps for handling object scale variance in the Polar coordinate. \nTo that end,\nthe BEV Polar encoder performs information exchange among neighbouring pixels and across multi-scale feature maps. \nFormally, let $\\{\\mathbf{G}_u\\}_{u=1}^U$\nbe the input multi-scale Polar feature maps and $\\hat{x}_q \\in [0, 1]^2$ be the normalized coordinates of the reference points for each qeury element $q$, we introduce a multi-scale deformable attention module \\cite{zhu2020deformable} as:\n\\begin{equation}\\label{eq:ms_deform_attn}\n\\begin{split}\n    & \\mathrm{MSDeformAttn}(\\mathbf{z}_q,x_q, \\{\\mathbf{G}_u\\}_{u=1}^U) = \\\\\n    & \\sum_{m=1}^M \\mathbf{W}_m [\\sum_{u=1}^{U}\\sum_{k=1}^K A_{muqk}\\mathbf{W'}_m\\mathbf{G}_u(\\zeta_u(\\hat{x}_q)+\\Delta x_{muqk})],\n\\end{split}\n\\end{equation}\nwhere $m$ and $k$ are the index of the attention head and the sampling point. $\\mathbf{z}_q$ is the query feature. $\\Delta x_{muqk}$ and $A_{muqk}$ denote the sampling offset and the attention weight of the $k$th sampling point in $u$th feature level and $m$th attention head. The attention weight $A_{muqk}$ is normalized by $\\sum_{u=1}^U\\sum_{k=1}^K A_{muqk} = 1$. Sampling offsets $\\Delta x_{muqk}$ are generated by applying MLP layers on query $q$. Function $\\zeta_u$ generates the sampling offsets and rescales the normalized coordinate $\\hat{x}_q$ to the $u$th feature scale. $\\mathbf{W}_m$ and $\\mathbf{W}'_m$ are learnable parameters. \nServing as query, each pixel in the multi-scale feature maps exploits the information from both neighbouring pixels and pixels across scales.\nThis enables learning richer semantics across all feature scales.\n\n"
                },
                "subsection 3.4": {
                    "name": "Polar BEV decoder at multiple scales",
                    "content": "\nThe Polar decoder decodes the above multi-scale Polar features to make predictions in the Polar coordinate. \nWe construct the Polar BEV decoder\nwith deformable attention \\cite{zhu2020deformable}.\nSpecifically, we query $q$ in Eq. \\eqref{eq:ms_deform_attn} as learnable parameters. \n\nUnlike 2D reference points in the encoder,\nhere the reference points are in 3D cylindrical coordinate, equal to Polar coordinate when projected to BEV.\n%\nThe classification branch in each decoder layer outputs the confidence score vector $\\mathbf{c}\\in \\mathbb{R}^{\\mathcal{O}}$, where $\\mathcal{O}$ is the number of categories. The key learning targets of regression branch are in polar coordinate instead of Cartesian coordiante, as illustrated in Figure \\ref{fig:learning_targets}. \nFor simplicity, superscript $^{(P)}$ is omitted. \nReference points $(\\rho,\\phi, z)$ are iteratively refined in the decoder. \nWith reference points, the regression branch regresses the offsets $d_\\rho$, $d_\\phi$ and $d_z$. \nThe learning targets for orientation $\\theta$ and velocity $v$ are relative to azimuths of objects and separated to orthogonal\ncomponents $\\theta_{\\phi}$, $\\theta_{\\rho}$, $v_{\\phi}$ and $v_{\\rho}$, defined by:\n\\begin{equation}\n        \\bar{\\theta}_{ori} = \\theta_{ori} - \\phi,\\quad \\ \n        \\theta_{\\phi} = \\sin{\\bar{\\theta}_{ori}},\\quad \\ \n        \\theta_{\\rho} = \\cos{\\bar{\\theta}_{ori}},\n\\end{equation}\nand\n\\begin{equation}\n        \\bar{\\theta}_{v} = \\theta_{v} - \\phi, \\quad \\ \n        v_{\\phi} = v_{abs}\\sin{\\bar{\\theta}_{v}}, \\quad \\ \n        v_{\\rho} = v_{abs}\\cos{\\bar{\\theta}_{v}}.\n\\end{equation}\nHere, $\\theta_{ori}$ is the yaw angle of the bounding box. $v_{abs}$ and $\\theta_v$ are the absolute value and angle of velocity. We regress the object size $l$, $w$ and $h$ as $\\log l$, $\\log w$ and $\\log h$. \nWe adopt Focal loss \\cite{lin2017focal} and L1 loss for classification and regression respectively.\n\n\\iffalse\n"
                },
                "subsection 3.5": {
                    "name": "Temporal module",
                    "content": "\nTo fully leverage the sequence data, we further conduct temporal fusion between the current sweep and one past sweep in the BEV representation space. Specifically, we concatenate two temporally adjacent BEV Polar feature maps along the feature dimension and then deploy\nseveral convolution layers to reduce the dimension back to the origin. Extra encoder layers consisting of two residual blocks are applied before the fusion and BEV feature map of the past sweep is aligned according to ego-motion. Thanks to the convenience provided by BEV representation, this simple design of temporal fusion boosts the performance by a large extent. \n\\fi\n\n\\iffalse\n"
                },
                "subsection 3.6": {
                    "name": "Model architecture",
                    "content": "\nAs shown in Figure \\ref{fig:pipeline}, the detection model takes a set of surrounding view images as input and feature maps of the backbone last two stages are used to construct the feature pyramid network. \nFeature maps of different levels are then transformed to BEV Polar maps via our proposed QueryPolar, in which the corresponding multi-scale image patches are tokenized as independent sequences and queried by the corresponding Polar rays in a cross-attention formulation. \nTo make the use of advanced 3D object detection algorithms, the Polar map is transformed to the rectilinear grid map using the proposed camera-position-aware sampling strategy. \nBEV maps from different cameras are merged by weighted sum according to their visibility (for overlapped regions, we calculate the average). \nAn optional temporal module based on axial attention is build to fuse features of past frames with that of the current frame spatially and temporally. \nSpecifically, if the video data is provided, axial attention will be conducted along height, width and time axes in sequence. \nTo enable large receptive fields, bottlenecks with dcn are used as topdown layers for BEV reasoning. \nAt last, we follow CenterPoint~\\cite{yin2021center} to construct the object detection head.\nFor map learning models, since it is aimed at online map construction, we remove the encoder in the view-transformer and set the depth of decoder as 2 for efficiency. \n\\fi\\newcommand{\\lack}[1]{\\textcolor{cyan}{#1}}\n\\newcommand{\\bug}[1]{\\textcolor{brown}{#1}}\n\\newcommand{\\es}[1]{\\textcolor{magenta}{#1}}\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:exp}\n\n\\paragraph{Dataset}\nWe evaluate the PolarFormer on the nuScenes dataset~\\cite{nuscenes2019}.\nIt provides images with a resolution of $1600 \\times 900$ from 6 surrounding cameras (Figure~\\ref{fig:teaser}). \nThe total of 1000 scenes, where each sequence is roughly 20 seconds long and annotated every 0.5 second, is split officially into \\texttt{train/val/test} set with 700/150/150 scenes. \n%\n\\paragraph{Implementation details}\nWe implement our approach based on the codebase \\texttt{mmdetection3d}~\\cite{mmdet3d2020}.\n%\nFollowing DETR3D~\\cite{wang2022detr3d} and FCOS3D~\\cite{wang2021fcos3d}, a ResNet-101~\\cite{he2016deep}, with 3rd and 4th stages equipped with deformable convolutions is adopted as the backbone architecture. \nThe number of cross-plane encoder layer is set to 3 for each feature scale.\nThe resolution of radius and azimuth for our multi-scale Polar BEV maps are (64,~256), (32,~128), (16,~64) respectively.\n%\nWe use 6 Polar BEV encoder and 6 decoder layers.\n%\nFollowing DETR3D~\\cite{wang2022detr3d}, our backbone is initialized from a checkpoint of FCOS3D~\\cite{wang2021fcos3d} trained on nuScenes 3D detection task, while the rest is initialized randomly. \nWe use above setting for \\texttt{prototype} verification.\nTo fully leverage the sequence data, we further conduct temporal fusion between the current frame and one history sweep in the BEV space. \n%\nFollowing BEVDet4D~\\cite{huang2022bevdet4d}, we simply concatenate two temporally adjacent multi-scale Polar BEV maps along the feature dimension and feed to the BEV Polar encoder.\n%\nWe randomly sample a history sweep from [3T; 27T] during training,\nand sample the frame at 15T for inference. \nT ($ \\approx 0.083s$) refers to the time interval between two sweep frames.\n%\nWe term our temporal version as \\texttt{PolarFormer-T}.\n%\n\n\\paragraph{Training }\nFollowing DETR3D~\\cite{wang2022detr3d} we train our models for 24 epochs with the AdamW optimizer and cosine annealing learning rate scheduler on 8 NVIDIA V100 GPUs.\nThe initial learning rate is $2 \\times 10^{-4}$, and the weight decay is set to $0.075$. \nTotal batch size is set to 48 across six cameras.\nSynchronized batch normalization is adopted.\nAll experiments use the original input resolution.\nNote our image variant uses the same amount of training data as the monocular pipelines~\\cite{wang2021fcos3d} and the multi-camera counterparts~\\cite{wang2022detr3d,li2022bevformer}.\nMulti-camera and monocular paradigms share the same evaluation metrics.\n\n%\n\n\\paragraph{Inference } \nWe evaluate our model on nuScenes validation set and test server.\nWe do not adopt model-agnostic trick such as model ensemble and test time augmentation.\n\n",
                "subsection 4.1": {
                    "name": "Comparison with the state of the art",
                    "content": "\n\n% {\\bf nuScenes}\nWe compare our method with the state of the art on both \\texttt{test} and \\texttt{val} sets of nuScenes. \nIn addition to the \n{\\bf (i)} \n\\texttt{prototype} setting mentioned in implementation details,\nwe also evaluate our model in the\n{\\bf (ii)}\n\\texttt{improved} setting, with VoVNet (V2-99)~\\cite{lee2019energy} as backbone architecture with a pretrained checkpoint from DD3D~\\cite{park2021dd3d} (fine-tuned on extra DDAD15M~\\cite{packnet} dataset) to boost performance.\n\nTable~\\ref{table:state-of-the-art} compares the results on nuScenes \\texttt{test} set.\nWe observe that our \\texttt{PolarFormer} achieves the best performance under both the \n{\\bf (i)}\n\\texttt{prototype} and\n{\\bf (ii)}\n\\texttt{improved} setting\n{\nin terms of \\texttt{mAP} and \\texttt{NDS} metrics, indicating the superiority of learning representation in the Polar coordinate. \n}\nWith temporal information \\texttt{PolarFormer-T} can further boot performance substantially. \nAdditional experiments results on \\texttt{val} set t and qualitative\nresults are shown in supplementary materials.\n\n\n%%%%%%%%% Ablations\n"
                },
                "subsection 4.2": {
                    "name": "Ablation studies",
                    "content": "\nWe conduct a series of ablation studies on nuScenes \\texttt{val} set to \nvalidate the design of PolarFormer.\nEach proposed component and important hyperparameters are examined thoroughly.\n\n\\paragraph{Polar \\textit{v.s.} Cartesian }\nTable~\\ref{table:coordinate} ablates the coordinate system.\nWe make several observations:\n\\textbf{(I)}\nLearning the representation and making the prediction both on Cartesian, Centerpoint~\\cite{yin2021center} gives a strong baseline with 0.378 mAP and 0.454 NDS;\nAfter applying circle NMS, Centerpoint* can further improve;\n\\textbf{(II)}\nOur PolarFormer-CC (with Cartesian feature and prediction) outperforms the Centerpoint equipped with specially designed CBGS head~\\cite{yin2021center};\n\\textbf{(III)}\nWhen Polar BEV map is used to feed into a Cartesian decoder head, \\textit{on-par} performance is achieved with the post-processing counterpart;\n\\textbf{(IV)}\nPolarFormer, our full model that predicts all 10 categories with one head and without any post-processing procedure, exceeds highly optimized Centerpoint by 1.1\\% in mAP and 0.2\\% in NDS.\nThis suggests the significance of \\textit{Polar} \nin both representation learning and exploitation (\\ie, decoding).\n\n\\paragraph{Visualizations }\nWith the quantitative evaluation in Figure~\\ref{fig:polarcardi}, \\textbf{(I)}\nit is evident that our model in Polar coordinate yields better results than Cartesian consistently.\nSpecifically, Polar outperforms Cartesian by mAP 3.1\\% and NDS 0.3\\% in nearby area, mAP 1.3\\% and NDS 0.7\\% in medium area.\n\\textbf{(II)}\nAs shown in Figure~\\ref{cartesian_det} and Figure~\\ref{polar_det}, compared to the Polar map, Cartesian usually downsamples the nearby area (red) with information loss, while upsamples the distant area (green) without actual information added.\nThis would explain the inferiority of Cartesian.\n\\textbf{(III)}\nFigure~\\ref{fig:query_attn} shows the attention map of the decoder query in multi-scale Polar BEV features.\nFor better viewing, we resize the multi-scale features into the same resolution.\nThe bottom/top corresponds to the largest/smallest scale features.\nThe \\textit{y} axis represents the radius of the Polar map.\nIt is shown that larger objects represented in the small map (top) are close to the ego car (small radius) whilst small objects in the large map (bottom) distribute through the distant area.\nThis is highly consistent with the geometry structure of raw images\n(Figure~\\ref{fig:teaser}), which has shown to be a more effective coordinate \nfor 3D object detection as above.\n\n\\paragraph{Architecture }\n\\textbf{(I)}\nWe first evaluate three designs of positional embedding (PE):\n2D learnable PE, fixed Sine PE, and 3D PE (generated based on a set of 3D points for each Polar ray position).\n\\textbf{(II)}\nAs our cross-plane encoder transforms different levels of feature from FPN into Polar rays independently, we can fuse the multi-level features into a single BEV or naturally shape multiple BEVs with different \\textit{or} same resolutions;\nTable~\\ref{table:multi-scale} clearly shows that a model with multi-scale Polar BEVs outperforms the single-scale counterpart under either coordinate.\nIn contrast, little performance gain is achieved from multi-scale features in Cartesian.\nThis suggests that object scale variation is a {\\em unique} challenge with Polar, but absent with Cartesian.\nOur design consideration is thus verified.\n\\textbf{(III)} \nWe study the resolution of polar map by adjusting the \\textit{azimuth} $\\bm{\\mathcal{N}_1}$ and \\textit{radius} $\\bm{\\mathcal{R}_1}$  (the number of Polar query in the cross-plane encoder);\nTable \\ref{table:resolution} shows that the \\textit{angle} with 256 and \\textit{radius} with 64 gives the best performance."
                }
            },
            "section 5": {
                "name": "Conclusions",
                "content": "\n\nWe have proposed the Polar Transformer (PolarFormer) for 3D object detection in multi-camera 2D images from the ego car's perspective.\nWith a rasterized BEV Polar representation geometrically aligned to visual observation, \nPolarFormer overcomes irregular Polar grids by a cross-attention based decoder.\n%\nFurther, a multi-scale representation learning strategy is designed \nfor tackling the intrinsic object scale variation challenge.\n%\nExtensive experiments on the nuScenes dataset validate \nthe superiority of our PolarFormer over previous alternatives on 3D object detection.\n\\vspace{2mm}\n\\noindent \\textbf{Acknowledgment} This work was supported by the National Key R$\\&$D Program of China (Grant No. 2018AAA0102803, 2018AAA0102802, 2018AAA0102800), \nthe Natural Science Foundation of China (Grant No. 6210020439, U22B2056, 61972394, 62036011, 62192782, 61721004, 62102417), \nLingang Laboratory (Grant No. LG-QS-202202-07),\nNatural Science Foundation of Shanghai (Grant No. 22ZR1407500)\nBeijing Natural Science Foundation (Grant No. L223003, JQ22014), \nthe Major Projects of Guangdong Education Department for Foundation Research and Applied Research (Grant No. 2017KZDXM081, 2018KZDXM066), \nGuangdong Provincial University Innovation Team Project (Grant No. 2020KCXTD045). \nJin Gao was also supported in part by the Youth Innovation Promotion Association, CAS.\n% \\newpage\n% Use \\bibliography{yourbibfile} instead or the References section will not appear in your paper\n\\bibliography{main}\n\n%%supp\n% \\newpage\n\\appendix\n\n\n"
            },
            "section 6": {
                "name": "Appendix",
                "content": "\n\n% \\iffalse\n",
                "subsection 6.1": {
                    "name": "Experiments on the val set of nuScenes",
                    "content": "\nTable~\\ref{table:nusc_val} shows that our method achieves leading performance on the \\texttt{val} set for both \\texttt{mAP} and \\texttt{NDS} metrics.\nUnder the \\texttt{improved} setting, PolarFormer shines on all metrics except \\texttt{mASE} and \\texttt{mAAE}.\nAgain, \\texttt{PolarFormer-T} outperforms the alternative BEVFormer by a clear margin,\nindicating the superiority of learning representation in the Polar coordinate. \n\n% \\fi\n\n"
                },
                "subsection 6.2": {
                    "name": "The coordinate choice for object prediction and loss optimization",
                    "content": "\n\nUntil now, we have shown that making the predictions in the Polar coordinate is crucial and superior over in the Cartesian counterpart.\nWe conjugate that \nthis is because with object locations projected under the Polar coordinate, a better distribution of reference points can be learned due to taking a more consistent perspective \\wrt{} the multi-camera image observation.\nInterestingly, we note a discrepancy in the coordinate choice \nbetween {\\em object location} prediction and loss optimization.\n% Note rotation, velocity and object dimension are still predicted and calculated loss in Polar coordinate.\nIn particular, we find that when optimizing the object localization loss in the Polar coordinate, the loss converges very slowly and a significant performance degradation is also observed (Table \\ref{tab:coord_target_loss}). \nA plausible obstacle is the numerical discontinuity between 0 and $2\\pi$ in azimuth, which however is continuous in the physical world. \n\n"
                },
                "subsection 6.3": {
                    "name": "Visualization",
                    "content": "\nWe visualize our 3D object detection and BEV semantic segmentation results in Figure~\\ref{fig:supp_vis_1}, Figure~\\ref{fig:supp_vis_2} and Figure~\\ref{fig:supp_vis_3}.\n\n"
                },
                "subsection 6.4": {
                    "name": "Limitations and potential societal impact",
                    "content": "\n\\paragraph{Limitations} Since existing BEV based 3D object detection methods \nconsider usually the Cartesian coordinate,\nthere is a lacking of\nelaborately designed off-the-shelf LiDAR detection heads and corresponding tricks suitable for the Polar coordinate based methods as we propose here.\nThat means there is some room for performance gain \nin head design, which will be one of the future works.\n\n\\paragraph{Societal impact} \nOur method can be used as the perception module for autonomous driving. However, our system is not perfect yet and hence not fully trustworthy in real-world deployment. Also, the current system is not exhaustively evaluated and tested due to limited resources as existing alternative works.\nAutonomous driving is still a largely immature field with many ongoing and unsolved matters including those complex legitimate issues, and many of those may be concerned with this work too.\n\n"
                },
                "subsection 6.5": {
                    "name": "Scale variance of objects on Polar BEV feature map",
                    "content": "\nAlthough Polar BEV feature map could preserve rich features in non-far regions when compared with Cartesian BEV feature map, it has to overcome scale variance problem, a challenge which is not presented in Cartesian BEV feature map. The object scale variance lies in that the size of an object in the Polar BEV feature map could reduce increasingly when it moves away from the ego-vehicle. Here we give an example of objects moving far away from the ego-vehicle in radial direction and provide simple mathematical proof. \n\n\nWe consider the bounding box of an object as a square with length $2a$ ($a>0)$ and denote the object by $ABCD$ and $A’B’C’D’$  in the Cartesian and Polar BEV feature map, respectively (Figure \\ref{fig:supp_cartesian_coord} and \\ref{fig:supp_polar_coord}) .The occupied area of the object in the Polar BEV feature map is denoted by $S$. Note that the coordinate of $D$ is $(d, h)$, where $d\\in[1,50], h\\in[1, 50]$, denoting the detection range.\n\nRegarding $d$ as the only variable, we could  prove $S(d)$ is monotonically decreasing functioned with $d$ by calculating $S'(d)$. \n\nFirst, the functions in  Polar BEV feature map for the curve $D'A'$, $A'B'$, $B'C'$, and $C'D'$ are\n\\begin{equation}\n\\begin{split}\n    &\\varphi_{D^{\\prime}A^{\\prime}}(\\rho,d)=\\arctan\\frac{\\sqrt{\\rho^2-d^2}}{d},\\\\\n    &\\rho\\in[\\sqrt{d^2+h^2},\\sqrt{d^2+(h+2a)^2} ],\n\\end{split}\n\\end{equation}\n\\begin{equation}\n\\begin{split}\n    &\\varphi_{A^{\\prime}B^{\\prime}}(\\rho,d)=\\arctan\\frac{h+2a}{\\sqrt{\\rho^2-(h+2a)^2}},\\\\\n    &\\rho\\in[\\sqrt{d^2+(h+2a)^2},\\sqrt{(d+2a)^2+(h+2a)^2}\n\\end{split}\n\\end{equation}\n\\begin{equation}\n\\begin{split}\n    &\\varphi_{B^{\\prime}C^{\\prime}}(\\rho,d)=\\arctan\\frac{\\sqrt{\\rho^2-(d+2a)^2}}{d+2a},\\\\\n    &\\rho\\in[\\sqrt{(d+2a)^2+h^2},\\sqrt{(d+2a)^2+(h+2a)^2}]\n\\end{split}\n\\end{equation}\n\n\\begin{equation}\n\\begin{split}\n    &\\varphi_{C^{\\prime}D^{\\prime}}(\\rho,d)=\\arctan\\frac{h}{\\sqrt{\\rho^2-h^2}},\\\\\n    &\\rho\\in[\\sqrt{d^2+h^2},\\sqrt{(d+2a)^2+h^2}]\n\\end{split}\n\\end{equation}\n\nThus, the area of curve $D^{\\prime}A^{\\prime}B^{\\prime}C^{\\prime}$ is\n\\begin{align*}\\tag{5}\nS(d)&=\\left(\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{d^2+(h+2a)^2}} \\varphi_{D^{\\prime}A^{\\prime}}(\\rho,d) d\\rho +  \\right.\\\\ &\\left. \\int_{\\sqrt{d^2+(h+2a)^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}} \\varphi_{A^{\\prime}B^{\\prime}}(\\rho,d) d\\rho\\right)\\\\\n&-\\left(\\int_{\\sqrt{(d+2a)^2+h^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}} \\varphi_{B^{\\prime}C^{\\prime}}(\\rho,d) d\\rho+ \\right.\\\\ &\\left.\n\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{(d+2a)^2+h^2}} \\varphi_{C^{\\prime}D^{\\prime}}(\\rho,d) d\\rho\\right) \\\\\n&=\\left(\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{d^2+(h+2a)^2}} \\arctan\\frac{\\sqrt{\\rho^2-d^2}}{d} d\\rho + \n\\right.\\\\ &\\left.\\int_{\\sqrt{d^2+(h+2a)^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}} \\arctan\\frac{h+2a}{\\sqrt{\\rho^2-(h+2a)^2}} d\\rho\\right)\\\\\n& -\\left(\\int_{\\sqrt{(d+2a)^2+h^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}} \\arctan\\frac{\\sqrt{\\rho^2-(d+2a)^2}}{d+2a} d\\rho+\\right.\\\\ &\\left.\n\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{(d+2a)^2+h^2}} \\arctan\\frac{h}{\\sqrt{\\rho^2-h^2}} d\\rho\\right)\n\\end{align*}\n\nTo proof $S(d)$ is monotonically decreasing functioned with $d$, we are going to show $S^{\\prime}(d)<0. (d\\in [1,50], h\\in [1, 50], a > 0)$\n\nFirst, we get $S^{\\prime}(d)$ as follows:\n\\begin{align*}\\tag{6}\nS^{\\prime}(d)\n&=\\left(\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{d^2+(h+2a)^2}}-\\frac{1}{\\sqrt{\\rho^2-d^2}}d\\rho+\\right.\\\\ &\\left.\n\\frac{d}{\\sqrt{d^2+(h+2a)^2}}\\arctan\\frac{h+2a}{d}-\\right.\\\\ &\\left.\n\\frac{d}{\\sqrt{d^2+h^2}}\\arctan\\frac{h}{d}\\right)\\\\\n& +\\left(\\frac{d+2a}{\\sqrt{(d+2a)^2+(h+2a)^2}}\\arctan\\frac{h+2a}{d+2a}-\\right.\\\\ &\\left.\n\\frac{d}{\\sqrt{d^2+(h+2a)^2}}\\arctan\\frac{h+2a}{d}\\right)\\\\\n& -\\left(\\int_{\\sqrt{(d+2a)^2+h^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}}-\\frac{1}{\\sqrt{\\rho^2-(d+2a)^2}}d\\rho+\\right.\\\\ &\\left.\n\\frac{d+2a}{\\sqrt{(d+2a)^2+(h+2a)^2}}\\arctan\\frac{h+2a}{d+2a}-\\right.\\\\ &\\left. \n\\frac{d+2a}{\\sqrt{(d+2a)^2+h^2}}\\arctan\\frac{h}{d+2a}\\right)\\\\\n&-\\left(\\frac{d+2a}{\\sqrt{(d+2a)^2+h^2}}\\arctan\\frac{h}{d+2a}-\\right.\\\\ &\\left.\n\\frac{d}{\\sqrt{d^2+h^2}}\\arctan\\frac{h}{d}\\right)\\\\\n&=\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{d^2+(h+2a)^2}} -\\frac{1}{\\sqrt{\\rho^2-d^2}}d\\rho-\\\\\n&\\int_{\\sqrt{(d+2a)^2+h^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}}-\n\\frac{1}{\\sqrt{\\rho^2-(d+2a)^2}}d\\rho\\\\\n& =\\ln\\frac{\\sqrt{(d+2a)^2+(h+2a)^2}+h+2a}{\\sqrt{(d+2a)^2+h^2}+h}-\\\\\n&\\ln\\frac{\\sqrt{d^2+(h+2a)^2}+h+2a}{\\sqrt{d^2+h^2}+h}\\end{align*}\nTake $f(x)=\\frac{\\sqrt{x^2+(h+2a)^2}+h+2a}{\\sqrt{x^2+h^2}+h}, x\\in [d,d+2a]$, we have\n\\begin{align*}\\tag{7}\nf^{\\prime}(x)&=\\left(\\frac{x}{\\sqrt{x^2+(h+2a)^2}}(\\sqrt{x^2+h^2}+h)-\\right.\\\\ &\\left.\n\\frac{x}{\\sqrt{x^2+h^2}}(\\sqrt{x^2+(h+2a)^2}+h+2a)\\right)\\\\\n&\\cdot(\\sqrt{x^2+h^2}+h)^2 \\\\\n&=\\frac{x}{(\\sqrt{x^2+h^2}+h)^2\\sqrt{x^2+(h+2a)^2}\\sqrt{x^2+h^2}}\\\\\n&\\cdot\\left[\\sqrt{x^2+h^2}(\\sqrt{x^2+h^2}+h)-\\right.\\\\ &\\left.\\sqrt{x^2+(h+2a)^2}(\\sqrt{x^2+(h+2a)^2}+h+2a)\\right] \\\\\n&=\\frac{x}{(\\sqrt{x^2+h^2}+h)^2\\sqrt{x^2+(h+2a)^2}\\sqrt{x^2+h^2}}\\\\\n&\\cdot\\left[q(h)-q(h+2a)\\right] < 0\\end{align*}\nwhere, $q(t)=\\sqrt{x^2+t^2}(\\sqrt{x^2+t^2}+t)$ is a monotonically increasing function in the domain $[h,h+2a]$\nObviously, $f(x)$ is a monotonically decreasing function with positive values in the domain $[d,d+2a]$. And $g(x)=\\ln(x)$ is a monotonically increasing function in the positive field. Thus, $g(f(x)$ is monotonically decreasing functioned with $x$ in the domain $[d,d+2a]$, which means $g(f(d+2a))< g(f(d))$. Therefore, we have\n\\begin{align*}\\tag{8}\nS^{\\prime}(d)\n&=\\ln\\frac{\\sqrt{(d+2a)^2+(h+2a)^2}+h+2a}{\\sqrt{(d+2a)^2+h^2}+h} \\\\\n&-\\ln\\frac{\\sqrt{d^2+(h+2a)^2}+h+2a}{\\sqrt{d^2+h^2}+h} \\\\\n&=g(f(d+2a))-g(f(d))< 0\n\\end{align*}\n\nSo the occupied area of the object decreases when $d$ increases, and symmetrically, we could obtain the same conclusion for $h$. When objects move far away from the ego-vehicle in the radial direction, both $d$ and $h$ increase, thus $S$ will decrease too. For a generic object with arbitrary orientation, we could consider that it is formed by multiple small squares and the same conclusion can be drawn.\n% Use \\bibliography{yourbibfile} instead or the References section will not appear in your paper\n% \\bibliography{aaai23}\n\n\n\n"
                }
            }
        },
        "figures": {
            "fig:supp_vis_1": "\\begin{figure*}[h]\n     \\centering\n     \\begin{subfigure}[b]{\\textwidth}\n         \\centering\n         \\makebox[\\textwidth][c]{\\includegraphics[width=1.\\textwidth]{images/1.pdf}}\n        %  \\caption{$y=x$}\n         \\label{fig:1}\n     \\end{subfigure}\n     \\vfill\n     \\begin{subfigure}[b]{\\textwidth}\n         \\centering\n         \\makebox[\\textwidth][c]{\\includegraphics[width=1.\\textwidth]{images/2.pdf}}\n        %  \\caption{$y=3sinx$}\n         \\label{fig:2}\n     \\end{subfigure}\n     \\caption{Visualizations (Part I): Given multi-camera images, our method dedicates to perform 3D object detection and BEV semantic segmentation.}\n     \\label{fig:supp_vis_1}\n    %  \\vfill\n    %  \\begin{subfigure}[b]{0.9\\textwidth}\n    %      \\centering\n    %      \\includegraphics[width=\\textwidth]{images/3.pdf}\n    %     %  \\caption{$y=5/x$}\n    %      \\label{fig:3}\n    %  \\end{subfigure}\n    %  \\vfill\n    %  \\begin{subfigure}[b]{0.9\\textwidth}\n    %      \\centering\n    %      \\includegraphics[width=\\textwidth]{images/4.pdf}\n    %     %  \\caption{$y=5/x$}\n    %      \\label{fig:4}\n    %  \\end{subfigure}\n\\end{figure*}",
            "fig:supp_vis_2": "\\begin{figure*}[htbp]\n    %  \\caption{Visualizations}\n    %   \\label{fig:supp_vis}\n     \\centering\n    %  \\begin{subfigure}[b]{0.9\\textwidth}\n    %      \\centering\n    %      \\includegraphics[width=\\textwidth]{images/1.pdf}\n    %     %  \\caption{$y=x$}\n    %      \\label{fig:1}\n    %  \\end{subfigure}\n    %  \\vfill\n    %  \\begin{subfigure}[b]{0.9\\textwidth}\n    %      \\centering\n    %      \\includegraphics[width=\\textwidth]{images/2.pdf}\n    %     %  \\caption{$y=3sinx$}\n    %      \\label{fig:2}\n    %  \\end{subfigure}\n    %  \\vfill\n     \\begin{subfigure}[b]{\\textwidth}\n         \\centering\n         \\makebox[\\textwidth][c]{\\includegraphics[width=1.\\textwidth]{images/3.pdf}}\n        %  \\caption{$y=5/x$}\n         \\label{fig:3}\n     \\end{subfigure}\n     \\vfill\n     \\begin{subfigure}[b]{\\textwidth}\n         \\centering\n         \\makebox[\\textwidth][c]{\\includegraphics[width=1.\\textwidth]{images/4.pdf}}\n        %  \\caption{$y=5/x$}\n         \\label{fig:4}\n     \\end{subfigure}\n      \\caption{Visualizations (Part II): Given multi-camera images, our method dedicates to perform 3D object detection and BEV semantic segmentation.}\n     \\label{fig:supp_vis_2}\n\\end{figure*}",
            "fig:supp_vis_3": "\\begin{figure*}[htbp]\n    %  \\caption{Visualizations}\n    %   \\label{fig:supp_vis}\n     \\centering\n    %  \\begin{subfigure}[b]{0.9\\textwidth}\n    %      \\centering\n    %      \\includegraphics[width=\\textwidth]{images/1.pdf}\n    %     %  \\caption{$y=x$}\n    %      \\label{fig:1}\n    %  \\end{subfigure}\n    %  \\vfill\n    %  \\begin{subfigure}[b]{0.9\\textwidth}\n    %      \\centering\n    %      \\includegraphics[width=\\textwidth]{images/2.pdf}\n    %     %  \\caption{$y=3sinx$}\n    %      \\label{fig:2}\n    %  \\end{subfigure}\n    %  \\vfill\n     \\begin{subfigure}[b]{\\textwidth}\n         \\centering\n         \\makebox[\\textwidth][c]{\\includegraphics[width=1.\\textwidth]{images/5.pdf}}\n        %  \\caption{$y=5/x$}\n         \\label{fig:4}\n     \\end{subfigure}\n      \\caption{Visualizations (Part III): Given multi-camera images, our method dedicates to perform 3D object detection and BEV semantic segmentation.}\n     \\label{fig:supp_vis_3}\n\\end{figure*}"
        },
        "equations": {
            "eq:eq:cam2img": "\\begin{equation}\\label{eq:cam2img}\n    s\\begin{bmatrix}\n    x^{(I)}\\\\\n    y^{(I)}\\\\\n    1\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n    f_x & 0 & u_0 \\\\\n    0 & f_y & v_0 \\\\\n    0 & 0 & 1\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    x^{(C)} \\\\\n    y^{(C)} \\\\\n    z^{(C)} \n    \\end{bmatrix},\n\\end{equation}",
            "eq:eq:amizuth": "\\begin{equation}\\label{eq:amizuth}\n    \\phi^{(P)} = \\arctan \\frac{x^{(C)}}{z^{(C)}} = \\arctan \\frac{x^{(I)}-u_0}{f_x},\n\\end{equation}",
            "eq:eq:radius": "\\begin{equation}\\label{eq:radius}\n\\begin{split}\n    \\rho^{(P)} = \\sqrt{(x^{(C)})^2 + (z^{(C)})^2} \n         =z^{(C)}\\sqrt{(\\frac{x^{(I)}-u_0}{f_x})^2+1}.\n\\end{split}\n\\end{equation}",
            "eq:1": "\\begin{equation}\n  \\begin{split}\n  \\mathbf{p}_{n,u,w} &= \\mathrm{MultiHead}(\\mathbf{\\dot{p}}_{n,u,w},\\mathbf{f}_{n,u,w},\\mathbf{f}_{n,u,w}) \\\\\n  &= \\mathrm{Concat}(\\mathrm{head_1}, \\dots, \\mathrm{head_h})\\mathbf{W}^O_u,\n  \\end{split}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n  \\begin{split}\n  \\mathrm{head_i} \\!=\\! \\mathrm{Attention}(\\mathbf{\\dot{p}}_{n,u,w}\\mathbf{W}_{i,u}^Q, \\mathbf{f}_{n,u,w}\\mathbf{W}_{i,u}^K, \\mathbf{f}_{n,u,w}\\mathbf{W}_{i,u}^V),\n  \\end{split}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\mathbf{P}_{n,u}\\!=\\!\\mathrm{Stack}([\\mathbf{p}_{n,u,1}, \\dots,\\mathbf{p}_{n,u,W_u}], \\mathrm{dim}\\!=\\!1) \\!\\in\\! \\mathbb{R}^{R_u \\!\\times\\! W_u \\!\\times\\! C },\n\\end{equation}",
            "eq:eq:alignment_proj": "\\begin{equation}\\label{eq:alignment_proj}\n    \\begin{bmatrix}\n    sx^{(I)}_{i,j,k,n} \\\\[5pt]\n    sy^{(I)}_{i,j,k,n}\\\\[5pt]\n    s \\\\[5pt]\n    1\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n    \\mathbf{\\Pi}_n & 0 \\\\\n    0 & 1\n    \\end{bmatrix}\n    \\mathbf{E}_n\n    \\begin{bmatrix}\n    \\rho^{(P)}_i\\sin{\\phi^{(P)}_j} \\\\[5pt]\n    \\rho^{(P)}_i\\cos{\\phi^{(P)}_j} \\\\[5pt]\n    z^{(P)}_k \\\\[5pt]\n    1\n    \\end{bmatrix},\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:corherent_sampling}\n\\begin{split}\n    &\\mathbf{G}_u(\\rho^{(P)}_i,\\phi^{(P)}_j)\n    =\\frac{1}{\\sum_{n=1}^N\\sum_{k=1}^\\mathcal{Z} \\lambda_n(\\rho^{(P)}_i,\\phi^{(P)}_j,z^{(P)}_k)} \\\\ \n    & \\cdot \\sum_{n=1}^N\\sum_{k=1}^\\mathcal{Z}\\lambda_n(\\rho^{(P)}_i,\\phi^{(P)}_j,z^{(P)}_k){\\mathcal{B}(\\mathbf{P}_{n,u},(\\overline{x}^{(I)}_{i,j,k,n}, \\overline{r}_{i,j,n}))},\n\\end{split}\n\\end{equation}",
            "eq:eq:ms_deform_attn": "\\begin{equation}\\label{eq:ms_deform_attn}\n\\begin{split}\n    & \\mathrm{MSDeformAttn}(\\mathbf{z}_q,x_q, \\{\\mathbf{G}_u\\}_{u=1}^U) = \\\\\n    & \\sum_{m=1}^M \\mathbf{W}_m [\\sum_{u=1}^{U}\\sum_{k=1}^K A_{muqk}\\mathbf{W'}_m\\mathbf{G}_u(\\zeta_u(\\hat{x}_q)+\\Delta x_{muqk})],\n\\end{split}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n        \\bar{\\theta}_{ori} = \\theta_{ori} - \\phi,\\quad \\ \n        \\theta_{\\phi} = \\sin{\\bar{\\theta}_{ori}},\\quad \\ \n        \\theta_{\\rho} = \\cos{\\bar{\\theta}_{ori}},\n\\end{equation}",
            "eq:6": "\\begin{equation}\n        \\bar{\\theta}_{v} = \\theta_{v} - \\phi, \\quad \\ \n        v_{\\phi} = v_{abs}\\sin{\\bar{\\theta}_{v}}, \\quad \\ \n        v_{\\rho} = v_{abs}\\cos{\\bar{\\theta}_{v}}.\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\begin{split}\n    &\\varphi_{D^{\\prime}A^{\\prime}}(\\rho,d)=\\arctan\\frac{\\sqrt{\\rho^2-d^2}}{d},\\\\\n    &\\rho\\in[\\sqrt{d^2+h^2},\\sqrt{d^2+(h+2a)^2} ],\n\\end{split}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\begin{split}\n    &\\varphi_{A^{\\prime}B^{\\prime}}(\\rho,d)=\\arctan\\frac{h+2a}{\\sqrt{\\rho^2-(h+2a)^2}},\\\\\n    &\\rho\\in[\\sqrt{d^2+(h+2a)^2},\\sqrt{(d+2a)^2+(h+2a)^2}\n\\end{split}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\begin{split}\n    &\\varphi_{B^{\\prime}C^{\\prime}}(\\rho,d)=\\arctan\\frac{\\sqrt{\\rho^2-(d+2a)^2}}{d+2a},\\\\\n    &\\rho\\in[\\sqrt{(d+2a)^2+h^2},\\sqrt{(d+2a)^2+(h+2a)^2}]\n\\end{split}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\begin{split}\n    &\\varphi_{C^{\\prime}D^{\\prime}}(\\rho,d)=\\arctan\\frac{h}{\\sqrt{\\rho^2-h^2}},\\\\\n    &\\rho\\in[\\sqrt{d^2+h^2},\\sqrt{(d+2a)^2+h^2}]\n\\end{split}\n\\end{equation}",
            "eq:11": "\\begin{align*}\\tag{5}\nS(d)&=\\left(\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{d^2+(h+2a)^2}} \\varphi_{D^{\\prime}A^{\\prime}}(\\rho,d) d\\rho +  \\right.\\\\ &\\left. \\int_{\\sqrt{d^2+(h+2a)^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}} \\varphi_{A^{\\prime}B^{\\prime}}(\\rho,d) d\\rho\\right)\\\\\n&-\\left(\\int_{\\sqrt{(d+2a)^2+h^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}} \\varphi_{B^{\\prime}C^{\\prime}}(\\rho,d) d\\rho+ \\right.\\\\ &\\left.\n\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{(d+2a)^2+h^2}} \\varphi_{C^{\\prime}D^{\\prime}}(\\rho,d) d\\rho\\right) \\\\\n&=\\left(\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{d^2+(h+2a)^2}} \\arctan\\frac{\\sqrt{\\rho^2-d^2}}{d} d\\rho + \n\\right.\\\\ &\\left.\\int_{\\sqrt{d^2+(h+2a)^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}} \\arctan\\frac{h+2a}{\\sqrt{\\rho^2-(h+2a)^2}} d\\rho\\right)\\\\\n& -\\left(\\int_{\\sqrt{(d+2a)^2+h^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}} \\arctan\\frac{\\sqrt{\\rho^2-(d+2a)^2}}{d+2a} d\\rho+\\right.\\\\ &\\left.\n\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{(d+2a)^2+h^2}} \\arctan\\frac{h}{\\sqrt{\\rho^2-h^2}} d\\rho\\right)\n\\end{align*}",
            "eq:12": "\\begin{align*}\\tag{6}\nS^{\\prime}(d)\n&=\\left(\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{d^2+(h+2a)^2}}-\\frac{1}{\\sqrt{\\rho^2-d^2}}d\\rho+\\right.\\\\ &\\left.\n\\frac{d}{\\sqrt{d^2+(h+2a)^2}}\\arctan\\frac{h+2a}{d}-\\right.\\\\ &\\left.\n\\frac{d}{\\sqrt{d^2+h^2}}\\arctan\\frac{h}{d}\\right)\\\\\n& +\\left(\\frac{d+2a}{\\sqrt{(d+2a)^2+(h+2a)^2}}\\arctan\\frac{h+2a}{d+2a}-\\right.\\\\ &\\left.\n\\frac{d}{\\sqrt{d^2+(h+2a)^2}}\\arctan\\frac{h+2a}{d}\\right)\\\\\n& -\\left(\\int_{\\sqrt{(d+2a)^2+h^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}}-\\frac{1}{\\sqrt{\\rho^2-(d+2a)^2}}d\\rho+\\right.\\\\ &\\left.\n\\frac{d+2a}{\\sqrt{(d+2a)^2+(h+2a)^2}}\\arctan\\frac{h+2a}{d+2a}-\\right.\\\\ &\\left. \n\\frac{d+2a}{\\sqrt{(d+2a)^2+h^2}}\\arctan\\frac{h}{d+2a}\\right)\\\\\n&-\\left(\\frac{d+2a}{\\sqrt{(d+2a)^2+h^2}}\\arctan\\frac{h}{d+2a}-\\right.\\\\ &\\left.\n\\frac{d}{\\sqrt{d^2+h^2}}\\arctan\\frac{h}{d}\\right)\\\\\n&=\\int_{\\sqrt{d^2+h^2}}^{\\sqrt{d^2+(h+2a)^2}} -\\frac{1}{\\sqrt{\\rho^2-d^2}}d\\rho-\\\\\n&\\int_{\\sqrt{(d+2a)^2+h^2}}^{\\sqrt{(d+2a)^2+(h+2a)^2}}-\n\\frac{1}{\\sqrt{\\rho^2-(d+2a)^2}}d\\rho\\\\\n& =\\ln\\frac{\\sqrt{(d+2a)^2+(h+2a)^2}+h+2a}{\\sqrt{(d+2a)^2+h^2}+h}-\\\\\n&\\ln\\frac{\\sqrt{d^2+(h+2a)^2}+h+2a}{\\sqrt{d^2+h^2}+h}\\end{align*}",
            "eq:13": "\\begin{align*}\\tag{7}\nf^{\\prime}(x)&=\\left(\\frac{x}{\\sqrt{x^2+(h+2a)^2}}(\\sqrt{x^2+h^2}+h)-\\right.\\\\ &\\left.\n\\frac{x}{\\sqrt{x^2+h^2}}(\\sqrt{x^2+(h+2a)^2}+h+2a)\\right)\\\\\n&\\cdot(\\sqrt{x^2+h^2}+h)^2 \\\\\n&=\\frac{x}{(\\sqrt{x^2+h^2}+h)^2\\sqrt{x^2+(h+2a)^2}\\sqrt{x^2+h^2}}\\\\\n&\\cdot\\left[\\sqrt{x^2+h^2}(\\sqrt{x^2+h^2}+h)-\\right.\\\\ &\\left.\\sqrt{x^2+(h+2a)^2}(\\sqrt{x^2+(h+2a)^2}+h+2a)\\right] \\\\\n&=\\frac{x}{(\\sqrt{x^2+h^2}+h)^2\\sqrt{x^2+(h+2a)^2}\\sqrt{x^2+h^2}}\\\\\n&\\cdot\\left[q(h)-q(h+2a)\\right] < 0\\end{align*}",
            "eq:14": "\\begin{align*}\\tag{8}\nS^{\\prime}(d)\n&=\\ln\\frac{\\sqrt{(d+2a)^2+(h+2a)^2}+h+2a}{\\sqrt{(d+2a)^2+h^2}+h} \\\\\n&-\\ln\\frac{\\sqrt{d^2+(h+2a)^2}+h+2a}{\\sqrt{d^2+h^2}+h} \\\\\n&=g(f(d+2a))-g(f(d))< 0\n\\end{align*}"
        }
    }
}