{
    "meta_info": {
        "title": "LightPath: Lightweight and Scalable Path Representation Learning",
        "abstract": "Movement paths are used widely in intelligent transportation and smart city\napplications. To serve such applications, path representation learning aims to\nprovide compact representations of paths that enable efficient and accurate\noperations when used for different downstream tasks such as path ranking and\ntravel cost estimation. In many cases, it is attractive that the path\nrepresentation learning is lightweight and scalable; in resource-limited\nenvironments and under green computing limitations, it is essential. Yet,\nexisting path representation learning studies focus on accuracy and pay at most\nsecondary attention to resource consumption and scalability.\n  We propose a lightweight and scalable path representation learning framework,\ntermed LightPath, that aims to reduce resource consumption and achieve\nscalability without affecting accuracy, thus enabling broader applicability.\nMore specifically, we first propose a sparse auto-encoder that ensures that the\nframework achieves good scalability with respect to path length. Next, we\npropose a relational reasoning framework to enable faster training of more\nrobust sparse path encoders. We also propose global-local knowledge\ndistillation to further reduce the size and improve the performance of sparse\npath encoders. Finally, we report extensive experiments on two real-world\ndatasets to offer insight into the efficiency, scalability, and effectiveness\nof the proposed framework.",
        "author": "Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen",
        "link": "http://arxiv.org/abs/2307.10171v1",
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.DB"
        ],
        "additionl_info": "This paper has been accepted by ACM SIGKDD-23"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n\\label{sec:intro}\nMotivated in part by an increasing number of intelligent transportation and smart city services that operate on movement paths, path representation learning (PRL) has received remarkable attention~\\cite{DBLP:conf/icde/Guo0HJ18,DBLP:journals/pvldb/FangPCDG21,DBLP:journals/tkde/ZhangZSQ20}.\n%%%%%\nPath representation learning aims to learn a generic path representation (PR) vector (ref. $\\mathcal{R}^d$ in Figure \\ref{fig:re}) that can be utilized in a range of different downstream tasks. This is in contrast to task-specific path representation learning performed by supervised methods that yield representations that work well on task-labeled data but work poorly in other tasks. \nFor example, in Figure \\ref{fig:re} \\emph{Lightpath} takes as input a path $p$ and returns a generic PR that can support a variety of tasks, e.g., travel time estimation and path ranking.\n%%%%%\n\n\n% \\textcolor{red}{Consider the travel time estimation example from Google Map in Figure \\ref{fig:re}(a), there are three candidate paths from source A to destination B, i.e., path $p_1$, path $p_2$ and path $p_3$. We observer: (1) each path consists of a sequence of edges, e.g., $p_1=\\langle e_1,e_2,\\cdots,e_N$, where $N=10$ in this case; (2) If the system recommends path solely based on the\n% estimated travel time, then $p_1$ and $p_2$ are the best choice. However, this travel time generally is estimated by supervised methods, which return a task-specific path representation for task-labeled data. In addition, these PRs have poor generation for other tasks. In contrast, as shown in Figure \\ref{fig:re}(b), we aim to learn a generic path representation (PR) that works well for different downstream applications, e.g., travel time estimation and path ranking. }\n\n\n% \\textcolor{red}{\\paragraph{Example} Figure~\\ref{fig:re}(a) gives an example of three paths, $p_1$, $p_2$ and $p_3$, from source A to destination B in a map service. Each path is represented by a sequence of edges, e.g., $p_1=\\langle e_1,e_2,\\cdots,e_{10}\\rangle$, where $e_i^{end} = e_{i+1}^{start}, 1 \\leq i \\leq 9$, where $e_i^{start}$ and $e_i^{end}$ denote the starting and ending vertices of $e_i$, respectively. We can observe that the estimated travel time of these three paths are different, which are 7, 7 and 8 minutes, respectively. And path $p_1$ is the recommended path from this map service. }\n\n% \\textcolor{red}{When we want to train a model to estimate the travel time/path ranking of a given path, we normally formulate it as a supervised problem in an end-to-end manner, which is shown in Figure~\\ref{fig:re}~(b). An input path $p_1$ is fed into a path encoder, producing a path representation vector $PR$, which is in blue box in Figure~\\ref{fig:re}~(b). Then, the regression model takes input as PR and is trained with labels of different downstream tasks, e.g., travel time estimation and path ranking, etc.}\n\n% \\textcolor{red}{This example shows that it is more sensible to design meaningful generic path representation learning methods instead of different supervised methods for different downstream applications.} \n%\n\nIn fact, a variety of intelligent transportation services involve paths, e.g., travel cost estimation~\\cite{DBLP:conf/ijcai/YangGHT021,DBLP:conf/icde/HuG0J19,DBLP:conf/sigmod/Yuan0BF20,DBLP:journals/pvldb/WuZGHYJ21,DBLP:conf/icde/00020BF21,DBLP:journals/pvldb/TranMLYHS20}, \n% routing~\\cite{DBLP:conf/sigmod/WeiWL20,DBLP:conf/sigmod/WangWKL21,DBLP:journals/pvldb/PedersenYJ20,DBLP:journals/vldb/GuoYHJC20,DBLP:conf/icde/HuangWZ021}, \n%\ntrajectory analysis~\\cite{DBLP:conf/sigmod/Shang0B18,DBLP:conf/sigmod/Shang0B18a,DBLP:journals/pvldb/HanCMG22,DBLP:journals/pvldb/WangLCL20,DBLP:journals/pvldb/WangBCSQ19,DBLP:journals/vldb/PedersenYJ20,DBLP:journals/sigmod/GuoJ014,DBLP:journals/pvldb/PedersenYJ20}, and path ranking~\\cite{yang2020context,DBLP:journals/pvldb/0026HFZL020,DBLP:conf/ijcai/YangGHT021,DBLP:journals/vldb/GuoYHJC20,DBLP:conf/icde/LiuJYZ18}. Path representations that are both accurate and compact, thus facilitating efficient operations, are in high demand as they hold the potential to significantly improve the services that use them.\n% accurate and informative path representations can significantly improve intelligent transportation services and ultimately improve the user experience. \nIndeed, \nrecent path representation learning methods, in particular deep learning based methods, demonstrate impressive and state-of-the-art performance on a wide variety of downstream tasks. \n%\n\n\n\nHowever, existing path representation learning methods focus on accuracy improvement and pay at best secondary attention to scalability and resource usage. The resulting models often include large numbers of layers and parameters, driving up computational costs, power consumption, and memory consumption, especially for long paths.\n% \n% For instance, a model with many parameters may achieve good accuracy, but (1) Training large size model in cloud server consumes more power;\nAlthough path encoders with many parameters may achieve good accuracy, they have two limitations. First, using large path encoders in the cloud consumes substantial energy, which is not eco-friendly (cf. Fig 1(a)). Second, increasingly many users enjoy personalized services, e.g., personalized travel time estimation based on their own trajectories. Due to privacy concerns, such personalized services often require the path encoder to be deployed in resource-limited edge environments, such as on individual users' mobile phones (cf. Fig 1(b)), without having to upload their trajectories to the cloud.\n%\n%\n% (2) it cannot be used in resource-restricted environments, e.g., on edge devices. \n% \\textcolor{red}{Especially as shown in Figure\\ref{fig:re}(a), although we can deploy the large-size deep learning models on the cloud server directly. However, The larger-size model conducts training on the cloud server, the more power consumes, which is not energy-efficient and eco-friendly. Thus, lightweight path representation learning saves lots of power energy. However, could service has poor privacy issues. As shown in Figure\n% \\ref{fig:re}(b), if we want to construct different regression models and estimate personalized travel time (TT) for Alice and Bob respectively, the cloud service is easy to disclose sensitive personal information that users do not want others to know. In contrast, edge device has good properties to protect users' privacy, but larger model size limits the usage of edge device for personalized service. }\n% conduct personalized travel time (TT) estimation, e.g., \n%\n% \\textcolor{red}{For example, as shown in Figure \\ref{fig:re}(b), although there are many cloud services that can support the training of large-size deep learning models, the cloud services can easily disclose individual drivers’ identities (e.g., driver name or ID number) and\n% associated personal sensitive locations, which prevents the usage of\n% deep learning models running on cloud servers.}\n% \\textcolor{red}{Although we can deploy the large-sized deep learning models on cloud server directly, as is shown in Figure~\\ref{fig:re}~(b), it is essential to upload a large amount of paths onto the cloud. However, those paths with labels of downstream tasks are usually traveled by some drivers, which may contain the individual information of the drivers, e.g., the home and working address. So, there is a potential of personal information leakage if the raw information of paths needs to upload to the server. However, if we can generate a path representation on our edge device, e.g., mobile phone, and then upload the resulted path representation to the cloud server, we can circumvent the personal information leakage, which is shown in Figure~\\ref{fig:re}~(b). }\n% Which calls forIn contrast, we prefer to train a lightweight path encoder and learn PRs using local edge device to protect the privacy of the driver. Then we upload the learned PRs and build regression models on cloud server to conduct the downstream application estimation.}\n%\nMore generally, it is sensible to enable lightweight path representation learning that works in resource-limited environments.\n% On the other hand, increasingly more users access intelligent transportation services through edges devices, such as mobile phones. \n\nNext, existing path representation methods suffer from two limitations. \n\n%Thus, it is essential to construct a lightweight and scalable path representation learning model that is not only accurate but is friendly to resource-restrictive platforms. \n%\n%In this paper, we strive to provide a lightweight and scalable unsupervised path representation learning method, which aims to solve the following two significant limitations.\n\n\n\n\n\n\n\\paragraph{\\textbf{Poor scalability w.r.t. path length}}\nSince a path is a sequence of road-network edges, path representation learning benefits from models that are good at capturing sequential relationships, such as\n%  \nthe Transformer~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17}.\n%\nHowever, a Transformer-based method~\\cite{DBLP:conf/cikm/ChenLCBLLCE21} employs a self-attention mechanism, where one edge attends to all other edges in a path in each attention, resulting in quadratic complexity, $\\mathcal{O}\\left( N^2\\right)$, in the path length $N$.\n%\nThis results in poor scalability to long paths with many edges.\n% with respect to path length, where the path length is the number of edges in a path. \n%\nFigure~\\ref{fig:scale} gives an example of the scalability w.r.t. path length $N$, covering both memory consumption and computational cost, in terms of GPU memory~(gMem.) and Giga floating point operations per second~(GFLOPs). \n%\n% In Figures~\\ref{fig:subfig:mac} and~\\ref{fig:subfig:flops}, the \\emph{Lightpath} is a method propose in this paper. In contrast, \n% Transformer-based method~\\cite{DBLP:conf/cikm/ChenLCBLLCE21} (cf. Figure~\\ref{fig:subfig:tte}) employs self-attention mechanism, where one edge attends to all other edges in a path in each attention, resulting in quadratic complexity, $\\mathcal{O}\\left( N^2\\right)$.\n%\nWe observe that when the path length $N$ increases from 50 to 200 edges, the Transformer-based method performs poorly. % e.g., the GFLOPs of the Transformer-based at $N=200$ is almost $4\\times$ the GFLOPs at $N=50$. \\BY{4 times is not quaratic. inconsistent with complexity analysis. perhaps remove the 4 times sentence.}\n%\nA method that scales better w.r.t. $N$ is desirable. \n% Intuitively, such a model looks like a slim rectangle,\n% $\\mathcal{O}\\left( N^{{\\prime}2}\\right)$ is called for, where $N^{\\prime}$ is much less than $N$, \n% as shown in Figure~\\ref{fig:subfig:lp}.\n\n\\paragraph{\\textbf{Very large model size. }}\nMany existing PRL models have large numbers of parameters, which restricts their use in resource-limited environments.\n%\n% \nFor example, in a Transformer-based method~\\cite{DBLP:conf/cikm/ChenLCBLLCE21}, where the Transformer stacks $L$ transformer layers, each layer employs multi-head (i.e., $M$ heads) attentions. Thus, the Transformer functions like a large cuboid, with a complexity of $\\mathcal{O}\\left(L \\cdot M \\cdot N^2\\right)$, as shown in Figure~\\ref{fig:subfig:tte}. \n%\nFor example, Table~\\ref{tab:mp} shows the numbers of parameters of Transformer-based path encoders when varying the number of layers among $12\\text{, }24\\text{, }48\\text{, and }96$ while fixing the number heads at 8 per layer and the feature dimension of the encoder at $512$. We observe that the model parameters grow dramatically when the number of encoder layers increase, preventing the models from being deployed in resource-limited environments.\n%\n%In addition, when setting different number of path encoder layers, i.e., $12,24,48,96$, with $8$ heads per layer, and then set the dimension of the encoder to $512$, Table~\\ref{tab:mp} shows the parameter statistic for the different settings of encoder layers. We observe that the model parameters grow dramatically when encoder layers increase, preventing them from being deployed to resource-limited environments.\n\nMoreover, models with large amounts of parameters also suffer from high storage and computational costs, which is not eco-friendly.\n% . Such costs are unattractive, particularly in resource-limited environments.\n%\n%As shown in Figure~\\ref{fig:subfig:tte}, Transformer-based PRL functions like a large cuboid and has computational complexity $\\mathcal{O}\\left( L\\cdot M \\cdot N^{2}\\right)$, which implies high storage and computational requirements and inefficient inference.\n% \n% The Transformer-based model with multiple layers and heads suffers from high storage and computation requirements and leads to inefficient inference. \n%\nMore specifically, as shown in Figure~\\ref{fig:subfig:mac}, for path length $N=200$, the Transformer-based model consumes almost 3.4GiB GPU memory. \n% \n%\n% In addition, although there are many cloud services supporting large-size deep learning models, the cloud services are easily disclosing individual drivers' identities (e.g., driver name or ID number) and associated personal sensitive locations, which prevents the usage of deep learning models running on cloud servers.\n% (1) Many uncertainties exist for online cloud services, e.g., latencies, and network traffic, which lead to bad user experiences. (2) As for the user,  cloud services will expose personal privacy, such as driver ID, house address, etc..\n%\n% \\textcolor{red}{Thus, a lightweight path representation learning method that works on edge devices and is accurate and efficient is desirable. We train a lightweight path encoder and learn PRs using a local edge device to protect the driver's privacy. Then we upload the learned PRs and build regression models on the cloud server to conduct the downstream application estimation to ensure the response time (c.f., Figure \\ref{fig:re}(b)).}\n%\n\n% To tackle all the above limitations, we propose LightPath, a lightweight path representation learning approach that is accurate and efficient.\n% thus addressing the three limitations.\n%\n% To enable \\emph{LightPath}, we first propose sparsity operation that is able to reduce path sequence length from $N$ to $N^{\\prime}$, where $N^{\\prime}$ is much less than $N$. \n% % with respect to different reduction ratios (e.g, 0.1) (Ref Figure~\\ref{fig:subfig:sp}). \n% Then, we propose a path reconstruction framework via auto-encoder architecture. This framework aims to learn the path representation with the reduced path with the encoder, but it considers the topology information of the full path in the decoder. Therefore, the path representation can be learned with reduced path length, lifting the challenge of path scalability, w.r.t. path length.\n% we introduce the encoder-decoder schema that is able to reconstruct the path. This enables the learned path representations to capture the road network topology information of the full path set, thus addressing the first limitation. \n%\n\n% Next, we propose a novel multi-scale relational reasoning (MSRR), which trains a relation head to discriminate how entities relate to themselves (intra-class reasoning) and other entities (inter-class reasoning) to enable rich and efficient path representation learning. Specifically, our MSRR contains two objectives, i.e., cross-network and cross-view relational reasoning based on a dual path encoder. In particular, we focus on relations between views of the same object and relations between different objects in different scenes, which allows the path encoder to acquire both intra-class and inter-class knowledge.\n% As shown in Figure~\\ref{fig:stsamples}(b), we adopt commutative operation to ensure the encoder achieves the inter-class knowledge, which addresses the second limitation and results in the linear complexity, i.e., $\\mathcal{O}\\left(K\\right)$ when there are $K$ training samples in minibatch. Specifically, commutative operation means we roll the path representations in the minibatch for a given view (i.e., view 2) to construct the inter-class relation pair. For example, we roll the path representations in view 2 to construct different inter-class pairs, i.e., $\\langle p_1,p_2 \\rangle$, $\\langle p_2, p_K \\rangle$, and $\\langle p_K, p_1 \\rangle$. In this case, we have $K-1$ different inter-class pairs for each path when there are $K$ paths in minibatch.\n   \n\n% the path representations of two views for each path conduct commutative operation, which addresses the second limitation and results in the linear complexity, i.e., $\\mathcal{O}\\left(K\\right)$ when there are $K$ training samples in minibatch. Specifically, commutative operation means that we roll the path representations in the minibatch for a given view (i.e., view 2) to construct the negative relation pair. For example, we roll the path representations in view 2 to construct different negative pairs, i.e., $\\langle p_1,p_2 \\rangle$, $\\langle p_2, p_K \\rangle$, and $\\langle \\p_K, p_1 \\rangle$. In this case, we have $K-1$ different negative pairs for each path when there are $K$ paths in minibatch.  \n%\n\n% Finally, to enable a lightweight path representation framework, we propose global-local knowledge distillation (GLKD) to further reduce model size and capacity. \n% %\n% We first pre-train a larger Teacher model and then train a smaller student model based on GLKD. The global knowledge distillation captures the path representation similarity between a larger teacher model and a smaller student model. The local knowledge distillation captures the edge representation similarity between the two models. The global-local knowledge distillation ensures the quality of the learned path representations because it mimics not only the path representations from the teacher model but also the spatial dependencies of edge representations from the teacher model, thus ensuring the accurate path representation and enabling the student model to be deployed on resource-limited edge devices. This global-local knowledge distillation address the third limitation.  \n%\n\\textbf{Proposed Solution. }To tackle the above limitations, we propose \\emph{LightPath}, a lightweight and scalable path representation learning approach. To address the first limitation, we first propose a sparse auto-encoder targeting good scalability, w.r.t., path length. In particular, the introduction of sparseness reduces the path length from $N$ to $N^{\\prime}$ by removing edges and returning a sparse path of length $N^{\\prime}$. The sparse path is fed into a Transformer-based encoder, which reduces the complexity from $\\mathcal{O}\\left( L\\cdot M \\cdot N^{2}\\right)$ to $\\mathcal{O}\\left( L\\cdot M \\cdot N^{\\prime 2}\\right)$. As shown in Figure~\\ref{fig:subfig:ste}, this reduces a huge cuboid to a slimmer cuboid. \n%\nTo avoid information loss due to the removed edges, we connect the encoder with a decoder, with the aim of reconstructing the full path. This enables scalable yet effective unsupervised training. \n% Then, our path reconstruction decoder takes as input a sparse path and the learnable edges that represent removed edges in original path to reconstruct the original path which ensures there is no edge information missing, thus enabling a faster and more robust unsupervised training.\n%\n\n% To address the problem of inefficient unsupervised training led by prevalent contrastive learning, we then propose a novel multi-scale relational reasoning (MSRR) strategy that results in the linear complexity, i.e., $\\mathcal{O}\\left(K\\right)$ when there are $K$ training samples in minibatch.\n% In particular, we focus on relations between views of the same\n% object and relations between different objects in different scenes,\n% which allows the path encoder to acquire both intra-class and interclass\n% knowledge. Then, we trains a relation head to discriminate how entities (i.e., identical path representation) relate to themselves (intra-class reasoning) and other entities (inter-class reasoning), which enable the rich and efficient path representation learning. In addition, we construct the intra-class and inter-class reasoning based on commutative operation that rolls the path representations to build different intra-class and inter-class pairs, which has linear complexity, as shown in Figure~\\ref{fig:stsamples}(b). \n% To address the second limitation, \n\n%To further improve the sparse auto-encoder training, we then propose efficient relational reasoning (RR) learning, a novel framework for path representation learning. RR is derived from relational reasoning under a multi-view self-supervised learning setting. More specifically, the objective of RR contains 1) Cross-network relational reasoning and 2) Cross-view relational reasoning. The first one can be achieved by minimizing a simple cross-entropy loss between the representations of positive (same path) and negative (different path) from different path encoders (i.e., main and auxiliary encoder). The intuition is that the slowly-updated auxiliary network acts as a stable expert to encode historical observations, which guides the main encoder to learn to explore richer and better representations without relying on extra negatives to avoid collapse. In contrast, the second one minimizes the cross-entropy loss between the representations of positive and negative from different views through the main encoder to regularize the training of cross-network relational reasoning. More specifically, such method can reduce the quadratic complexity of prevalent contrastive learning to linear complexity, thus enabling the efficient sparse auto-encoder training. \nTo further improve the training of the sparse encoder, we add an additional training scheme based on relational reasoning. In particular, for each path $p_i$, we construct two distinct sparse path views, denoted as $p_i ^{1}$ and $p_i ^{2}$, using different reduction ratios, e.g., removing 40\\% and 80\\%, respectively. % We consider them as different views of the path (i.e., $p_i ^{1}$ and $p_i ^{2}$). \n%\nThen, we propose a dual sparse path encoder, including the original main encoder, and an additional, auxiliary encoder. The dual sparse path encoder encodes the two path views. Thus, we achieve four path presentations $PR_i ^{1}$, $PR_i ^{2}$, $\\hat{PR}_i ^{1}$, and $\\hat{PR}_i ^{2}$ for path $p_i$ according to the two path views and the two sparse path encoders, where $PR$ and $\\hat{PR}$ denote the representations from the main and the auxiliary encoders, respectively. Finally, given two path representations, we train a relational reasoning network to determine whether the two path representations are from the same “relation.” If they are from the same path, we consider them as positive relations; otherwise, they are negative relations.\n\n% To improve the training of sparse encoder, we introduce an efficient relational reasoning~(RR) module into the above-mentioned framework based on self-supervised learning mechanism. Firstly, for a path $p$, we apply two sparse operations to generata different sparse paths, e.g., $p'_{1}$ and $p'_{2}$, which we consider them as two different views of path $p$. Secondly, we introduce dual sparse path encoders, namely main-and auxiliary-encoder, to encode these two sparse paths, respectively. We can generate four different path representations~(PRs) for a single input path, i.e., $PR_1^{main}$~($PR_1^{aux}$) and $PR_2^{main}$~($PR_2^{aux}$) are views of $p'_{1}$ and $p'_{2}$ from the main~(auxiliary) encoder, respectively. When it comes another path $\\tilde{p}$, it also has the following four views, i.e., $\\widetilde{PR_1^{main}}$, $\\widetilde{PR_1^{aux}}$, $\\widetilde{PR_2^{main}}$, and $\\widetilde{PR_2^{aux}}$. Next, the relational reasoning is a module that aims to report a score whether two PRs, i.e., one pair of PRs, are from the same path or not. More specifically, we propose two different kinds of relational reasoning: 1) Cross-network relational reasoning and 2) Cross-view relational reasoning. Cross-network relational reasoning aims to report the relation scores from different sparse-encoders, i.e., $PR_1^{main}$ and $PR_1^{aux}$ are PRs of the same sparse path $p'_1$ from different sparse path encoders, while $\\widetilde{PR_1^{aux}}$ is PR from different sparse path $\\tilde{p}_1^{\\prime}$. Thus, relation score between $PR_1^{main}$ and $PR_1^{aux}$ should be 1, and the score between $PR_1^{main}$ and $\\widetilde{PR_1^{aux}}$ should be 0. Similarly, cross-view relational reasoning tries to report the relation scores between PRs from the same sparse-encoder, i.e., relation score between $PR_1^{main}$ and $PR_2^{main}$ should be 1, and score between $PR_1^{main}$ and $\\widetilde{PR_2^{main}}$ should be 0. By incorporate these two different relational reasoning modules, we can learn a more robust path representation. \n\n% Such a formulation guarantees that the ideal solution induces efficient path representation learning that explores richer and better representations without relying on extra negatives to avoid collapse. On top of this, to reduce the quadratic complexity of contrastive learning, relational reasoning mechanism just needs to construct the meaningful intra-class and inter-class pairs to enable the sparse path encoder to capture the knowledge from the same path pair and different path pair, thus resulting in the linear complexity, i.e., i.e., $\\mathcal{O}\\left(K\\right)$ when there are $K$ training samples in minibatch, as shown in Figure~\\ref{fig:stsamples}(b). Such a design avoids the requirement of large amount of negative samples using by contrastive loss, thus enabling efficient training for path representation learning.\n\n% Specifically, commutative operation means we roll the path representations in the minibatch for a given view (i.e., view 2) to construct the inter-class relation pair. For example, we roll the path representations in view 2 to construct different inter-class pairs, i.e., $\\langle p_1,p_2 \\rangle$, $\\langle p_2, p_K \\rangle$, and $\\langle p_K, p_1 \\rangle$. In this case, we have $K-1$ different inter-class pairs for each path when there are $K$ paths in minibatch.\n\n%\nTo address the second limitation, we propose a global-local knowledge distillation framework that aims to reduce the model size of the main path encoder, which not only enables use in resource-limited environments but also improves accuracy. \n%\n%\nTo this end, we consider the main path encoder as a teacher, and we create a lightweigth sparse encoder with fewer layers and one head as a student, further reducing a slimmer cuboid to a slim rectangle (cf. Figure (\\ref{fig:subfig:lp})).\n%\nThe global knowledge distillation tries to push the lightweight student to mimic the  teacher from a global semantic level (i.e., path representation level), while the local knowledge distillation can push the lightweight student to mimic the edge correlations from the teacher,\n% the captures the path representation similarity between a larger teacher model and a smaller student model. The local knowledge distillation captures the edge representation similarity between the two models. \n% The global-local knowledge distillation ensures the quality of the learned path representations because it mimics not only consider the global level path representations \n% but also the local level edges that captures different spatial dependencies, \nthus building a lightweight encoder while maintaining or even improving the accuracy of downstream tasks. \n\n\n\nTo the best of our knowledge, this is the first study that systematically targets lightweight and scalable path representation learning.\n% by introducing sparsity operation, multi-scale relational reasoning, and global-local knowledge distillation. \nThe study makes four main contributions.\n\n\\begin{itemize}\n    \\item \\textit{Sparse Auto-encoder.} We propose a unified sparse auto-encoder framework that provides \\emph{LightPath} with good scalability. w.r.t.\\ path length.\n    % takes as input path sequence sparse paths with reduced path lengths, %, reducing path length from $N$ to $N^{\\prime}$ to \n    % achieve good scalability. w.r.t the path length. % $N$. %The sparse encoder takes as input a sparse path and learnable path representation, and a lightweight decoder reconstructs the original input path from the latent edge representation and removed edges, ensuring that no edge information is missed during learning.\n    % based on different reduction ratio $\\gamma$. Then we employ an encoder-decoder schema to ensure the learned path representation captures complete information of the entire path set.\n    % no information missing during learning.\n    \\item \\textit{Relational Reasoning.} We introduce relational reasoning to enable efficient sparse auto-encoder training. Specifically, we propose two types of relational reasoning objectives for accurate and efficient path representation learning. These two objectives regularize each other and yield a more effective path encoder.\n    \\item \\textit{Global-local Knowledge Distillation.} We propose a novel global-local knowledge distillation framework that enables a lightweight student sparse encoder to mimic a larger teacher sparse encoder from global and local perspectives. \n    % The resulting lightweight model that can be deployed on edge devices while achieving accurate performance at different downstream tasks. \n    \\item \\textit{Extensive Experiments.} We report on extensive experiments on two large-scale, real-world datasets with two downstream tasks. The results offer evidence of the efficiency and scalability of the proposed framework as compared with nine baselines.\n\\end{itemize}\n\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\label{sec:pre}\n%\nWe first cover important concepts that underlie the paper's proposal and then state the problem addressed.\n%\n",
                "subsection 2.1": {
                    "name": "Definitions",
                    "content": "\n\\begin{myDef}\n    \\textbf{Road Network.}~A road network is defined as a graph $\\mathbf{G}=(V,E)$, where $V$ is a set of vertices $v_i$ that represents road intersections and $E \\subseteq V \\times V$ represents a set of edges $e_i=(v_j,v_k)$ that denotes road segments.\n\\end{myDef}\n%\\textbf{Road Network.}~A road network is defined as a graph $\\mathbf{G}=(V,E)$, where $V$ is a set of vertices $v_i$ that represents road intersections, $E \\subseteq V \\times V$ represents a set of edges $e_i=(v_j,v_k)$ that denotes road segments.\n%A road network is defined as a graph $\\mathbf{G}=(V,E)$, where $V$ is a set of vertices $v_i$ that represents road segment, $E \\subseteq V \\times V$ represents a set of edges $e_i=(v_j,v_k)$ that denotes the connection between two adjacent road segments.\n% \\begin{myDef}\n%     \\textbf{GPS Trajectory.}~A GPS trajectory of a moving object is defined as a sequence of spatio-temporal sample points, each of which contains a location (i.e., longitude and latitude) and a timestamp.\n% \\end{myDef}\n\n\n%\n% \\begin{myDef}\n%     \\textbf{Path.}~A path $p=\\langle e_1, e_2,e_3,\\cdots,e_N \\rangle$ is a sequence of connected edges, where $e_i \\in E$ denotes the edge in path $p$ and $p.\\Phi=[1,2,3,\\cdots,N]$ denotes the order of edges in $p$.  \n%     % $i \\in \\Omega$ and $\\Omega$ is a set of edge order in a path, e.g., $\\Omega=[1,2,3,\\cdots,N]$.\n% \\end{myDef}\n\n% \\begin{myDef}\n%     \\textbf{Path.}~A path $p=\\langle e_1, e_2,e_3,\\cdots,e_N \\rangle$ is a sequence of connected edges, where $e_i \\in E$ denotes the edge in path $p$. We denote $\\Phi(e_i)$ returns order of edge $e_i$ in $p$. \n%     % $i \\in \\Omega$ and $\\Omega$ is a set of edge order in a path, e.g., $\\Omega=[1,2,3,\\cdots,N]$.\n%\\end{myDef}\n%\n% \\begin{myDef}\n%     \\textbf{Sparse Path.}~A sparse path $p^{\\prime}=\\bigl \\{e_i \\bigr \\}_{i \\in \\Omega}$ contains a subset of edges in path $p$, where $p^{\\prime}.\\Omega$ is a subset of $p.\\Phi$, where $|p^{\\prime}.\\Omega| \\leq |p.\\Phi|$ and $p^{\\prime}.\\Omega \\cap p.\\Phi = p^{\\prime}.\\Omega$. \n% \\end{myDef}\n\n% \\noindent\n% \\textbf{Example.} Given a path $p=\\langle e_1,e_3,e_4,e_6,e_7 \\rangle$ and $p.\\Phi=[1,2,3,4,5]$, then sparse path $p^{\\prime}=\\bigl \\{ e_1,e_4,e_7 \\bigr \\}$, where $p^{\\prime}.\\Omega =[1,3,5]$, is one of the sparse paths for $p$.\n\n%new defs \n\\begin{myDef}\n    \\textbf{Path.}~A path $p=\\langle e_1, e_2,e_3,\\cdots,e_N \\rangle$ is a sequence of connected edges, where $e_i \\in E$ denotes an edge in path and two adjacent edges share a vertex. Next, $p$. $N$ denotes the length of path, i.e., the number of edges in $p$. We let $p.\\Phi=[ 1,2,3,\\cdots,N ]$ denote a sequence of orders of the edges in $p$.  \n\\end{myDef}\n\\begin{myDef}\n    \\textbf{Sparse Path.}~A sparse path $p^{\\prime}=\\langle e_i \\rangle_{i \\in p^{\\prime}.\\Omega}$ contains a subset of the edges in path $p$, where $p^{\\prime}.\\Omega$ is a sub-sequence of $p.\\Phi$.\n    % which is denoted as the original edge orders of $p^{\\prime}$ in $p$, where $p^{\\prime}.\\Omega \\subseteq p.\\Phi$. \n\\end{myDef}\n\\noindent\n\\textbf{Example.} Given a path $p=\\langle e_1,e_3,e_4,e_6,e_7 \\rangle$ and $p.\\Phi=[1,2,3,4,5]$ then path $p^{\\prime}=\\langle e_1,e_4,e_7 \\rangle$, where $p^{\\prime}.\\Omega =[1,3,5]$, is one of the sparse paths for $p$.\n%\n% \\begin{myDef}\n%     \\textbf{Path Views and representations}. Two different path views represent the sparse paths obtained using two different reduction ratios $\\gamma$. For example, given a path $p$ and corresponding reduction ratios $gamma_1=0.6$ and $gamma_2=0.8$. We denote path view 1 as $p^{1}$, which is sparse path with respect to $\\gamma_1$. Meanwhile, we denote path view 2 as $p^{2}$, which is sparse path in terms of $\\gamma_2$. Then, we denote $P^{1}$ and $P^{2}$ are path representations achieved by main encoder. $\\hat{P}^{1}$ and $\\hat{P}^{2}$ are path representations obtained through auxiliary encoder.\n% \\end{myDef}\n%\n% \\begin{myDef}\n%     \\textbf{Downstream Tasks.} A downstream task is a task that makes predictions based on path representations. In particular, we consider travel time estimation, path ranking score estimation, and path recommendation.\n% \\end{myDef}\n\n\\begin{myDef}\n    \\textbf{Edge Representation.} The edge representation of an edge in a road network graph is a vector in $\\mathbb{R}^{d_k}$, where $d_k$ is the dimensionality of the vector. For simplicity, we reuse $e_i$ to denote an edge representation.\n\\end{myDef}\n\n\n\\begin{myDef}\n    \\textbf{Path Representation.} The path representation $\\mathit{PR}$ of a path $p$ is a vector in $\\mathbb{R}^{d}$.\n\\end{myDef}\n% \\begin{myDef}\n% \\label{def:transformer}\n%     \\textbf{Transformer Layer.} Given a sequence of edge representations $\\mathbf{X}=\\langle x_1, x_2, x_3, \\cdots, x_N \\rangle$ for a path $p$. Transformer layer takes as input a $\\mathbf{X}$ and returns the encoded edge representations $\\mathbf{Z}=\\langle z_1, z_2, z_3, \\cdots, z_N \\rangle$ that capture the correlation of different edges. Especially, each Transformer layer consists of multi-head attention and position-wise feed-forward networks.\n    \n% %     \\noindent\n% %     \\textbf{Scaled Dot-Product Attention.} The Scaled Dot-Product Attention aims at mapping a query with keys and values. In practice, we define it as:\n% %     \\begin{equation}\n% %         \\text { Attention }(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^{T}}{\\sqrt{d_{k}}}\\right) \\mathbf{V},\n% %     \\end{equation}\n% % \\noindent\n% % where $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ denote the packed query, key, and value matrix of dimension $d_k$, $d_k$, $d_v$, respectively. softmax is a function to obtain the weights on the values. \n\n% \\noindent\n% \\textbf{Multi-Head Attention.} Instead of employing a single attention function, we define multi-head attention that linearly projects the queries, keys and values into $M$ subspaces with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Then, we formulate it as:\n\n% \\begin{equation}\n% \\begin{aligned}\n% &\\mathbf{Z}=\\mathbf{MultiHead}(\\mathbf{X})=\\mathit{\\text{Concat}}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{M}\\right) \\cdot \\mathbf{W}^{O}\\text{ , } \\\\\n% & \\mathit{\\text { head }_i}(\\cdot)=\n% % \\text { Attention }\\left(\\mathbf{X} \\mathbf{W}_{i}^{Q}, \\mathbf{\\mathbf{X} W}_{i}^{K}, \\mathbf{\\mathbf{X}} \\mathbf{W}_{i}^{V}\\right)=\\\\\n% \\operatorname{softmax}\\left(\\left(\\mathbf{X} \\mathbf{W}_{i}^{Q}\\right) \\left(\\mathbf{\\mathbf{X} W}_{i}^{K}\\right)^{T}/{\\sqrt{d_{k}}}\\right) \\left(\\mathbf{\\mathbf{X}} \\mathbf{W}_{i}^{V}\\right)\\text{ , }\n% \\end{aligned}\n% \\end{equation}\n% where $\\mathit{\\text{Concat}(\\cdot,\\cdot)}$ represents concatenation. $\\mathbf{W}_{i}^{Q} \\in \\mathbb{R}^{d_{model}\\times d_k}$,  $\\mathbf{W}_{i}^{K} \\in \\mathbb{R}^{d_{model}\\times d_k}$, $\\mathbf{W}_{i}^{V} \\in \\mathbb{R}^{d_{model}\\times d_v}$, $\\mathbf{W}^{O} \\in \\mathbb{R}^{Md_{v}\\times d_{model} }$ are projections parameter matrices for scaled dot-product attention. $M$ denotes number of heads. $d_{model}$ represents the feature dimension of final output.\n\n% % \\noindent\n% % \\textbf{Position-wise Feed-Forward Networks. }Except the attention sub-layers, each of the layers in Transformer (encoder/decoder) contains a fully connected feed-forward network (FFN), which is used to each position separately and identically. This FFN consists of two linear transformations with ReLU activation in between. Specifically, we have\n% % \\begin{equation}\n% % \\operatorname{FFN}(\\mathbf{Z})=\\max \\left(0, \\mathbf{Z} \\mathbf{W_{1}^{FFN}}+\\mathbf{b_{1}^{FFN}}\\right) \\mathbf{W_{2}^{FFN}}+\\mathbf{b_{2}^{FFN}}\\text{ , }\n% % \\end{equation} \n% % \\noindent\n% % where $\\mathbf{W_{1}^{FFN}}$, $\\mathbf{W_{2}^{FFN}}$, $\\mathbf{b_{1}^{FFN}}$, and $\\mathbf{b_{2}^{FFN}}$ are learnable parameters of feed-forward network.\n% \\end{myDef}\n%\n% \\begin{myDef}\n%     \\textbf{Teacher and Student model.} Knowledge distillation generally includes a small student model (cf. Figure~\\ref{fig:subfig:lp}) learning to mimic a large teacher model (cf. Figure~\\ref{fig:subfig:sp}) and using the teacher’s knowledge to achieve similar or superior accuracy.\n% \\end{myDef}\n\n"
                },
                "subsection 2.2": {
                    "name": "Problem Definition",
                    "content": "\nGiven a set of paths $\\mathbb{P}=\\left\\{p_{i}\\right\\}_{i=1}^{|\\mathbb{P}|}$ in a road network $G$, scalable and efficient path representation learning aims to learn a function $SPE_{\\theta}\\left(\\cdot \\right)$ that can generate a generic path representation for each path $p_i \\in \\mathbb{P}$ without relying on labels. It can be formulated as follows. \n% \\begin{equation}\n% SEPRL_{\\theta}\\left(p_{i}, d_t\\right): \\mathbb{R}^{d_{d_t}} \\times \\mathbb{R}^{M \\times d} \\rightarrow \\mathbb{R}^{d_{h}},\n% \\end{equation}\n\n\\begin{equation}\n\\mathit{PR} =SPE_{\\theta}\\left(p_{i}\\right):\\mathbb{R}^{N \\times d_k} \\rightarrow \\mathbb{R}^{d}\\text{ , }\n\\end{equation}\n\\noindent\nwhere $\\mathit{PR}$ is learned path representation, $\\theta$ represents the learnable parameters for the sparse path encoder, $N$ is the path length and $d_k$ and $d$ are the feature dimensions for an edge and a final path representation, respectively. %The resulted path representations $d_h$ can then be directly used in downstream tasks, such as travel cost estimation.\n% \\subsection{Transformer based Encoder}\n\n"
                },
                "subsection 2.3": {
                    "name": "Downstream Tasks",
                    "content": "\nA downstream task is a task that consumes a path representation. We consider travel time estimation and path ranking score estimation. In particular, we formulate task estimation as a regression problem and define the corresponding regression model as:\n\\begin{equation}\nReg_{task_k(\\psi)}\\left(\\mathit{PR}_{i}\\right):\\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\text{ , }\n\\end{equation}\n%\n% aims at learning a regression model $Reg_{task_k(\\psi)}(\\cdot)$ using limited labeled data that takes as input the learned PR and returns the corresponding value for different downstream tasks. We consider travel time estimation and path ranking score estimation. Specifically, we formulate the downstream task estimation as:\n% \\begin{equation}\n% DT =Reg_{\\psi}\\left(PR_{i}\\right):\\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\text{ , }\n% \\end{equation}\n\\noindent\nwhere $task_k(\\psi)$ is a learnable parameter for task $k$ and $\\mathit{PR}_i$ is the learned path representation of $p_i$.\n% is a task that make estimations based on the learned PR \n% \\subsection{Solution Overview}\n% \\begin{figure}[t]\n%     \\centering\n%     \\includegraphics[scale=0.39]{./figures/so.pdf}\n%     \\caption{Solution Overview.}\n%     \\label{fig:so}\n% \\vspace{-10pt}\n% \\end{figure}\n% Figure~\\ref{fig:so} shows an overview of the proposed \\emph{LightPath}, which consists of the following three modules: 1) Sparse Auto-encoder, 2) Relational reasoning, and 3) Global-local knowledge distillation. The details of those modules are provided in Section 3, 4 and 5, respectively. To create \\emph{LightPath}, we first train a large teacher encoder that has multiple layers and heads based on sparse auto-encoder and relational reasoning. Then, we employ global-local knowledge distillation to compress the large teacher encoder to achieve a small student encoder that has much less layers and heads. \n\n"
                }
            },
            "section 3": {
                "name": "Sparse Path Encoder",
                "content": "\n\\label{sec:spe}\n\n",
                "subsection 3.1": {
                    "name": "Transformer based Encoder",
                    "content": "\n\\label{sec:sub:transformer}\nWe first introduce the Transformer based encoder due to its parallelism pipeline and effectiveness for long sequence modeling. Thus, given a sequence of edge representations $\\mathbf{X}_p=\\langle e_1, e_2, e_3, \\cdots, e_N \\rangle$ for a path $p$. Transformer based encoder takes as input $\\mathbf{X}_p$ and returns the encoded edge representations $\\mathbf{Z}_p=\\langle z_1, z_2, z_3, \\cdots, z_N \\rangle$ that capture the correlation of different edges. Especially, instead of employing a single attention function, we define multi-head attention that linearly projects the queries, keys and values into $M$ subspaces with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Then, we formulate it as:\n\n\\begin{equation}\n\\mathbf{Z}_p=\\mathbf{MultiHead}(\\mathbf{X}_p)=\\mathit{\\text{Concat}}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{M}\\right) \\cdot \\mathbf{W}^{O}\\text{ , } \n\\end{equation}\n\\begin{equation}\n\\mathit{\\text { head }_i}(\\cdot)=\n% \\text { Attention }\\left(\\mathbf{X} \\mathbf{W}_{i}^{Q}, \\mathbf{\\mathbf{X} W}_{i}^{K}, \\mathbf{\\mathbf{X}} \\mathbf{W}_{i}^{V}\\right)=\\\\\n\\operatorname{softmax}\\left(\\left(\\mathbf{X}_p \\mathbf{W}_{i}^{Q}\\right) \\left(\\mathbf{\\mathbf{X}_p W}_{i}^{K}\\right)^{T}/{\\sqrt{d_{k}}}\\right) \\left(\\mathbf{\\mathbf{X}_p} \\mathbf{W}_{i}^{V}\\right)\\text{ , }\n\\end{equation}\n\nwhere $\\mathit{\\text{Concat}(\\cdot,\\cdot)}$ represents concatenation. $\\mathbf{W}_{i}^{Q} \\in \\mathbb{R}^{d_{model}\\times d_k}$,  $\\mathbf{W}_{i}^{K} \\in \\mathbb{R}^{d_{model}\\times d_k}$, $\\mathbf{W}_{i}^{V} \\in \\mathbb{R}^{d_{model}\\times d_v}$, $\\mathbf{W}^{O} \\in \\mathbb{R}^{Md_{v}\\times d_{model} }$ are projections parameter matrices for scaled dot-product attention with respect to the learnable parameter $\\theta$ in \\emph{LightPaht}. $M$ denotes number of heads. $d_{model}$ represents the feature dimension of final output. $\\mathbf{Z}_p \n\\in \\mathbb{R}^{N \\times d_k}$.\n\nExcept the attention sub-layers, each of the layers in Transformer based encoder also contains a fully connected feed-forward network (FFN), which is used to each position separately and identically. This FFN consists of two linear transformations with ReLU activation in between. Specifically, we have\n\\begin{equation}\n\\operatorname{FFN}(\\mathbf{Z}_p)=\\max \\left(0, \\mathbf{Z}_p \\mathbf{W_{1}^{FFN}}+\\mathbf{b_{1}^{FFN}}\\right) \\mathbf{W_{2}^{FFN}}+\\mathbf{b_{2}^{FFN}}\\text{ , }\n\\end{equation} \n\\noindent\nwhere $\\mathbf{W_{1}^{FFN}}$, $\\mathbf{W_{2}^{FFN}}$, $\\mathbf{b_{1}^{FFN}}$, and $\\mathbf{b_{2}^{FFN}}$ are learnable parameters of feed-forward network, and $\\operatorname{FFN}(\\mathbf{Z}_p) \n\\in \\mathbb{R}^{N \\times d}$.\n\nHowever, Transformer based encoder suffers from poor scalability w.r.t. path length and large mode size (ref. as to Section \\ref{sec:intro}). To this end, we aim to study a sparse path encoder.\n\n"
                },
                "subsection 3.2": {
                    "name": "Overview",
                    "content": "\nFigure~\\ref{fig:spe} illustrates the sparse path encoder framework, which includes a sparsity operation, a sparse path encoder, and a path reconstruction decoder. The sparsity operation takes as input a full path and returns a sparse path with respect to a reduction ratio $\\gamma$. Sparse path encoder takes as input a sparse path and learnable path representation and outputs learned path representations.\nNext, we introduce a path reconstruction decoder to reconstruct the path, thus ensuring the learned path representation captures the entire path information.\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Sparsity Operation",
                    "content": "\n\\label{sec:pr}\nA path consists of a sequence of edges $p={\\langle e_1, e_2,e_3,\\cdots,e_N} \\rangle$, which are the basic processing units of different sequential models.\n% Transformer in both Encoder and Decoder. \nThe processing times of sequential models become longer when the path gets longer.\n% With the increasing of the path sequence length, the Transformer Encoder and Decoder should take longer time to process long sequence.\nThus,\n% Toward this end,\nwe propose a sparsity operation, which is an approach to reduce the path length from $N$ to $N^{\\prime}$, where $N^{\\prime}$ is much less than $N$.\n% an approach, called \\textit{sparsity operation}, that reduces the $N$-length path sequence to $N^{\\prime}$-length sequence. \nFor simplicity, we conduct path reduction by randomly removing a subset of edges in a path based on a high reduction ratio $\\gamma$ (e.g., $\\gamma=0.6$).\nA high reduction ratio $\\gamma$ (the ratio for edge removal) can significantly reduce the length of each input path, thus enabling the scalability of the path.  \nSpecifically, we construct the sparsity operation as:\n\\begin{equation}\n    p^{\\prime}= f\\left(p,\\gamma \\right)=\\langle e_j \\rangle_{j \\in \\Omega}\\text{ , }\n\\end{equation}\n\\noindent\n%  $\\bigl\\{\\cdot \\bigr\\}$ indicates a set of edges with corresponding edge index, e.g., $i$ or $j$. $e_i$ is $i$-th edge in input path.\nwhere $p$ is input path. $p^{\\prime}$ denotes the sparse path.\n% $\\Phi$ is edge order in a path $p$, i.e., $\\Phi=[1,2,3,\\cdots,N]$. $\\Omega$ represents the original edge order for the edges in the sparse path and $|\\Omega|=N^{\\prime}$. \nFor example, as shown in Figure~\\ref{fig:spe}, if we have a path $p = \\langle e_1,e_3,e_4,e_6,e_7 \\rangle$, then we conduct sparsity operation, which randomly removes a subset of edges in $p$, i.e., $\\langle e_1,e_4,e_7 \\rangle$ based on reduction ratio $\\gamma=0.6$ and achieve the sparse path $p^{\\prime}=\\langle e_3, e_6 \\rangle$ and $p^{\\prime}.\\Omega=[2,4]$. Thus, we can reduce path from $N$ to $N^{\\prime}$, i.e., from $5$ to $2$ in this example.\n%\n\n"
                },
                "subsection 3.4": {
                    "name": "Learnable Path Representation",
                    "content": "\nWe use Transformer as our path encoder since it processes the input edges parallelly with respect to the self-attention mechanism. In contrast, the recurrent neural network (RNN) family is inefficient due to its recurrent nature.\n%\nTo avoid achieving path representation through extra aggregation function~\\cite{DBLP:conf/ijcai/YangGHT021}, we add a super extra learnable path representation \nrepresentative $\\mathit{PR}$ in front of each sparse path. Moreover, $\\mathit{PR}$ is attached to position $0$ for every path, thus enabling it to capture global information of the path during the training procedure.  Thus, we update the $p^{\\prime}$ as:\n\\begin{equation}\n    p^{\\prime} = \\langle \\mathit{PR}\\rangle + \\langle e_j \\rangle_{j \\in \\Omega} =\\langle e_k \\rangle_{k \\in \\Omega^{\\prime}}\\text{ , }\n\\end{equation}\n\\noindent\nwhere $e_0 = \\mathit{PR}$ denotes a virtual edge and $\\Omega^{\\prime} = [0, \\Omega]$.\n% To this end, we construct the new path sequence as $\\mathbf{p}^{\\prime}={\\langle e_i \\rangle}_{i=1}^{N^{\\prime}+1}$. \n\nTo preserve the sequential information of the path, we add learnable position embedding into the sparse path representations based on order information in $\\Omega^{\\prime}$. Specifically, we have:\n\\begin{equation}\n\\mathbf{X}_p^{\\prime}=\\mathit{\\text{Concat}}\\langle x_k \\rangle_{k \\in \\Omega^{\\prime}}, \\text{ where } x_k = e_k + pos_k  \\text{ , } \n\\end{equation}\n\\noindent\nwhere $pos_k$ represents the learnable position embedding for edges in the sparse path and $\\mathbf{X}_p^{\\prime}$ represents the sparse path edge representation after concatenation. \n\nTake Figure~\\ref{fig:spe} as an example, we first construct $p^{\\prime}=\\langle \\mathit{PR}, e_3, e_6 \\rangle$. Then, we add corresponding position embedding to the edge vectors of $p^{\\prime}$, i.e., positions $0$, $2$, and $4$,\n% In particular, we construct position embedding $0$ for learnable path embedding $PR$. In contrast, we further construct the position embedding for $\\langle e_3,e_6 \\rangle$ based on its order in original path $\\mathbf{p}$, i.e., position $2$ and $4$.  \nwhere the added position embeddings can help the Transformer encoder to be aware of the input order instead of treating them as a set of unordered path edges. Meanwhile, they enable the learned path representation $\\mathit{PR}$ to capture global-level semantics in the sense that edges might play a different role in a road network. The intuition is that the super learnable path representation representative\n$\\mathit{PR}$ can attend attention with other edges, which captures global-level semantic. In contrast, the learnable edge representation aims to construct a full path set and reconstruct the specific edge in input path.\n%\n\n% \\paragraph{Multi-Head Self Attention }We have witnessed the considerable success of self-attention as it has been widely used in different domains, such as computer vision, natural language process, and graph representation learning. We follow the scaled dot-product attention principle, which can be formulated as mapping a query with keys and values. In practice, we define it as:\n% \\begin{equation}\n% \\text { Attention }(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^{T}}{\\sqrt{d_{k}}}\\right) \\mathbf{V},\n% \\end{equation}\n% \\noindent\n% where $\\mathbf{Q}$,$\\mathbf{K}$,$\\mathbf{V}$ denote the packed query, key, and value vectors of dimension $d_k$, $d_k$, $d_v$, respectively. Notably, we know that the self-attention mechanism conducts position-insensitive transformation, thus enabling the path encoder to capture the input order information with respect to the added position embedding.\n%\n\n% Then, we adopt multi-head self-attention to model scaled path sequence vectors $\\mathbf{\\mathcal{P}}=\\langle \\hat{e}_0,\\hat{e}_1,\\hat{e}_2, \\cdots, \\hat{e}_{N^{\\prime}+1} \\rangle \\in \\mathbb{R}^{\\left(N^{\\prime}+1 \\right) \\times d}$, which are mapped to output encoded embeddings $\\mathbf{Z}=\\langle \\mathbf{z_0}, \\mathbf{z_1}, \\mathbf{z_2}, \\cdots, \\mathbf{z_{\\left(N^{\\prime}+1 \\right)}} \\rangle \\in \\mathbb{R}^{\\left(N^{\\prime}+1 \\right) \\times d_h}$. In particular, the initial scaled path sequence vectors are projected into $M$ subspaces with different learnable parameters with respect to queries, keys, and values. We denote each subspace as a \"head\", which enables the Transformer encoder to jointly capture information from several independent \"head\" at different position. Next, we achieve the encoded embedding by conducting self-attention for each subspace based on concatenation and corresponding projection. Then, we formulate it as:\n\n% \\begin{equation}\n% \\begin{aligned}\n% &\\mathbf{Z}=\\mathbf{M H-Attn}(\\mathbf{\\mathcal{P}})=\\mathit{\\text{Concat}}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{M}\\right) \\cdot \\mathbf{W}^{O}, \\\\\n% &\\mathit{\\text { head }_i}=\\text { Attention }\\left(\\mathbf{\\mathcal{P}} \\mathbf{W}_{i}^{Q}, \\mathbf{\\mathcal{P} W}_{i}^{K}, \\mathbf{\\mathcal{P}} \\mathbf{W}_{i}^{V}\\right),\n% \\end{aligned}\n% \\end{equation}\n% \\noindent\n% where $\\mathit{\\text{Concat}}$ represents concatenation. $\\mathbf{W}_{i}^{Q},\\mathbf{W}_{i}^{K},\\mathbf{W}_{i}^{V} \\in \\mathbb{R}^{d\\times d / M}$, $\\mathbf{W}^{O} \\in \\mathbb{R}^{d\\times d_h }$ are projections parameter matrices for self-attention.\n% \\paragraph{Position-wise Feed-Forward Networks }Excepting the multi-head attention layers, we also process the encoded embeddings $\\mathbf{Z}$ through a fully connected feed-forward network, which consists of two linear transformations with a ReLU activation in between. Specifically, we define it as follows:\n% \\begin{equation}\n% \\operatorname{FFN}(x)=\\max \\left(0, x \\mathbf{W_{1}}+\\mathbf{b_{1}}\\right) \\mathbf{W_{2}}+\\mathbf{b_{2}},\n% \\end{equation} \n% \\noindent\n% where $\\mathbf{W_{1}}$, $\\mathbf{W_{2}}$, $\\mathbf{b_{1}}$, and $\\mathbf{b_{2}}$ are learnable parameters of feed-forward network.\n\n"
                },
                "subsection 3.5": {
                    "name": "Transformer Path Encoder",
                    "content": " %in Section~\\ref{sec:pre}\nTo achieve better performance, we usually stack multiple Transformer layers, each consisting of two sub-layers: multi-head attention and position-wise feed-forward network mentioned above (ref. as to Section~\\ref{sec:sub:transformer} ). Motivated by~\\cite{DBLP:conf/cvpr/HeZRS16}, we employ a residual connection around each sub-layers, followed by layer normalization~\\cite{DBLP:journals/corr/BaKH16}. The stacked transformer model can be formulated as:\n\\begin{equation}\nZ_p^{\\prime}=\\operatorname{LayerNorm}(\\mathbf{X}_p^{\\prime}+\\text { MultiHead }(\\mathbf{X}_p^{\\prime}))\\text{ , } \n\\end{equation}\n\\begin{equation}\n\\mathit{PR}=\\operatorname{LayerNorm}\\left(Z_p^{\\prime}+\\operatorname{FFN}\\left(Z_p^{\\prime}\\right)\\right)\\text{ ,}\n\\end{equation}\n\\noindent\nwhere LayerNorm represents layer normalization and $\\mathit{PR}$ is learned path representation.\n%\n% Specifically, we adopt dual path encoder to generate path representation for different views to enable the corss-network relational reasoning (ref. to~\\ref{subsec:cn}). \n\n% In particular, the path encoder takes as input sparse path $p^{\\prime}$, which includes learnable path representation, and returns path representation $PR$. \n% \\paragraph{Dual Path Encoder }As shown in Figure~\\ref{fig:spe}, we adopt dual path encoder to generate path representation for different views to enable the corss-network relational reasoning (ref. to~\\ref{subsec:cn}). In particular, the path encoder takes as input scaled path sequence $\\mathbf{p}^{\\prime}$, which includes learnable path embedding, and returns path representation (i.e., $\\mathbf{Z}^{\\prime}[0]$) and encoded edge embeddings (i.e. $\\mathbf{Z}^{\\prime}[1:]$).\n%\n% In particular, our sparse path encoder embeds the scaled path sequence with learnable path embedding vector, which is projected by a linear process with added position embeddings. \n% %\n% Then, a sparse path encoder returns the learned path representation and a set of intermediate edge embeddings, respectively, where the learned path representation captures the global information of the scaled path sequence through attention mechanism. \n% an embedded vectors for the scaled path sequence and path representation that captures the global information of scaled path sequence through attention mechanism.\n% is used to process embedding vectors, which results in the learnable path embedding is able to capture the global information of a subset of unremoved path edges through self-attention.\n%\nRemarkably, our path encoder only takes as input a small subset of edges (e.g., 60\\%) of the full path edges, which means we ignore the removed edges and just consider unremoved edges during the encoder stage to enable the path scalability. Path scalability enables us to train our path encoders concerning different lengths of path effectively and reduce the corresponding computational cost and memory usage. \n% In contrast, we also use a lightweight Transformer Decoder to reconstruct the input path, which enables the learned path representations contains the complete information of entire path. \n%\n\n"
                },
                "subsection 3.6": {
                    "name": "Path Reconstruction Decoder",
                    "content": "\nTo capture the global information of the full path, we further introduce a lightweight path decoder to reconstruct the removed edges in a path.\n% \n%\nAs shown in Figure~\\ref{fig:spe}, we first complement the encoded path edges and path representation with a shared, learnable vector that represents the presence of a removed edge based on the original index of each edge in a path. Then, we add the position embedding vectors to all edge representation, which enables the learnable path representation vector to capture the global information of the entire path.\nNext, the path decoder takes as input the full set of representations, including (1) path representation, (2) encoded unremoved edges, and (3) removed edges.\n%\nWe select a more lightweight decoder structure, which has less number of Transformer layers.\n%\n% In a specific, we only apply our path decoder during the path representation learning stage, which is only used to perform the path reconstruction task.\n%\n% As a result, \nSince the path decoder is only used to perform path reconstruction, the architecture of our path decoder can be more flexible and independent of the path encoder. Thus, the decoder is much shallower than the encoder, e.g., one layer for the decoder and 12 layers for the encoder, which significantly reduces training time. \n% We employ very small decoders, which are shallower than the path encoder. For example, our default path decoder just has 1 layers vs. 12 layers of the path encoder. To this end, the entire edges in a path is only handled by the lightweight path decoder, which significantly reduces training time. \nWe reconstruct the input path by predicting the removed edges to ensure the learned path representation contains complete information about the entire path. We employ mean squared error (MSE) as our reconstruction loss function and compute MSE between the reconstructed and initial edge representations in the edge level. We only employ MSE loss on removed edges, which can be formulated as follows:\n\\begin{equation}\n    \\mathcal{L}_{rec}= \\frac{1}{N}\\sum_{i=1}^{N}\\left(e_{i}-\\hat{e_{i}} \\right)^{2}\\text{ , }\n\\end{equation}\n%\n% \\vspace{-5pt}\n\\noindent\nwhere $e_i$ and $\\hat {e_{i}}$ are the initial and predicted masked edge representation, respectively. $N$ represents the number of edges for each input path.\n\n\n% \\vspace{-10pt}\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[scale=0.45]{./figures/fig5.pdf}\n%     \\caption{Illustration of Efficient Unsupervised Training Framework. We first construct two-path views (i.e., $p_{1}^{1}$ and $p_{1}^{2}$) with respect to different reduction ratios (i.e., $\\gamma=0.6$ and $\\gamma=0.8$). Then we employ dual path encoder, including the main encoder and auxiliary encoder, to generate path representations for each view. \n%     % A path reconstruction scheme is used to capture the full information of the full path set and \n%     Multi-scale relational reasoning is deployed to learn effective path representation. \n%     % $g_\\varphi$ and $\\hat{g_\\varphi}$ represent Transformer-based path encoder. $f_\\theta$ and $\\hat{f_\\theta}$ are multi-layer perception with batch normalization.\n%     }\n%     \\label{fig:bf}\n%     \\vspace{-10pt}\n% \\end{figure}\n%\n"
                }
            },
            "section 4": {
                "name": "Relational Reasoning Path Representation Learning",
                "content": "\n\\label{sec:msrr}\n\n",
                "subsection 4.1": {
                    "name": "Overview",
                    "content": "\nTo further enhance sparse auto-encoder (cf. Section~\\ref{sec:spe}) training, we propose a novel self-supervised relational reasoning (RR) framework, as shown in Figure~\\ref{fig:msrr}. \n%\nThe intuition behind this is that we train a relation head $RRH_{\\varphi}(\\cdot)$ to discriminate how path representations relate to\nthemselves (same class) and other paths (different class).  \n%\nIn particular, this framework consists of path representation construction (cf. Figure~\\ref{fig:subfig:prc}) and relational reasoning (cf. Figure~\\ref{fig:subfig:msrr}), which includes cross-network relational reasoning and cross-view relational reasoning. To train our dual sparse auto-encoder, we first generate two path views, denotes as $p_1^1$ and $p_1^2$, based on two different reduction ratios $\\gamma_1$ and $\\gamma_2$. After this, by processing these two path views via the main encoder and the auxiliary encoder of the sparse path encoder, we construct different paths on multiple views in the representation space.\n% Especially, we first employ sparse path encoder (i.e., path reduction) to generate two different path views in terms of different $\\gamma_1$ and $\\gamma_2$.\n% Sparsity operation takes as input full path sequence and outputs two views of scaled path sequence with respect to different reduction ratios. \n% Then, to efficiently learn path representations, our dual path encoder, including the main and auxiliary encoders (cf. sparse path encoder in Section~\\ref{sec:spe}), takes as input two different path views and returns corresponding path representations. Given a path $p$ and two reduction ratios $\\gamma_1=0.6$ and $\\gamma_2=0.8$. We denote path view 1 as $p^{1}^1$, which is sparse path with respect to the $\\gamma_1$. We denote path view 2 as $p^{2}$, which is sparse path in terms of the $\\gamma_2$. Then, we denote $P^{1}$ and $P^{2}$ are path representations achieved by main encoder. $\\hat{P}^{1}$ and $\\hat{P}^{2}$ are path representations obtained through auxiliary encoder.\n% To effectively learn path representation based on two path views, we use the Siamese encoder architecture to encode the scaled path sequences, which consists of a main encoder and an auxiliary encoder. \n% Auxiliary encoder cannot conduct gradient during a training phase. We slowly update its parameters using momentum updating technology based on the main encoder.\n%\n% %\n% Siamese path encoder takes as input different path views and outputs corresponding path representations from two path encoders. \nFinally, we employ relational reasoning to enable efficient path representation learning.\n%\n% The intuition behind momentum updating is \n% the slowly-moving auxiliary network in our path encoder acts as a stable “mean teacher” to encode historical observations, which guides the main encoder to learn to explore richer and better path representations without depending on additional negatives to avoid collapse. To this end,\n% Siamese path encoder is able to discover and capture both the current and historical observations from different views, such that we can learn useful path representations for the downstream task like path ranking. In addition, we introduce relational reasoning that only consider the relation between different samples, thus avoiding the requirement of larger amount of negative samples and enable the efficiency of unsupervised training. \n% where MSRR contains cross-network relational reasoning and cross-view relational reasoning. \n% Next, we detail these two terms of RR. \n\n% \\begin{figure}[t]\n%     \\centering\n%     \\includegraphics[scale=0.7]{./figures/MSRR0713.pdf}\n%     \\caption{Multi-Scale Relational Reasoning}\n%     \\label{fig:cnrr}\n\n% \\end{figure}\n\n% \\subsection{Siamese Path Encoder}\n% \\label{subsec:spe}\n% We apply Siamese path encoder structure to learn path representations from different views from main encoder and auxiliary encoder, respectively. In particular, the auxiliary encoder updates its parameters based on main encoder by using momentum updating technology.\n% %\n% The intuition behind momentum updating is \n% the slowly-moving auxiliary network in our path encoder acts as a stable “mean teacher” to encode historical observations, which guides the main encoder to learn to explore richer and better path representations without depending on additional negatives to avoid collapse. To this end,\n% Siamese path encoder is able to discover and capture both the current\n% and historical observations from different views.\n% %\n% Our Siamese path encoder contains path encoder and projection head. \n\n% \\noindent\n% \\textbf{Path Encoder.} We use Transformer~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17} as our path encoder (i.e., main Transformer $g_{\\varphi}$ and auxiliary Transformer $\\hat{g_{\\varphi}}$ in Figure~\\ref{fig:bf} \n% since it adopts self-attention mechanism and uses position embedding, which promote the usage of path reduction. To be noticed, the auxiliary Transformer cannot conduct gradient during training, but it slowly updates its parameters using momentum updating technology according to the parameter of main Transformer.\n\n% % The intuition behind momentum updating is the slowly-moving\n% % auxiliary network in our path encoder acts as a stable “mean teacher”\n% % to encode historical observations, which guides the main encoder\n% % to learn to explore richer and better path representations without\n% % depending on additional negatives to avoid collapse. To this end,\n% % Siamese path encoder is able to discover and capture both the current\n% % and historical observations from different views.\n\n% \\noindent\n% \\textbf{Projection Head.} Motivated by SimCLR~\\cite{DBLP:conf/icml/ChenK0H20}, we further use a small multi-layer perceptron(MLP) (i.e., projection heads,  $f_{\\theta}$ and $f_{\\theta}^{\\prime}$) to transform path representations to the space that contrastive relation reasoning loss is applied. Similarly, projection head $f_{\\theta}^{\\prime}$ also do not directly conduct gradient during training, where its parameters are updated based on projection head $f_{\\theta}$ by using momentum updating technology.\n% This MLP head contains a linear layer with output size 1024 followed by batch normalization~\\cite{DBLP:conf/nips/SanturkarTIM18}, Parametric rectified linear units (PReLU)~\\cite{DBLP:conf/iccv/HeZRS15}, and a final linear layer with output dimension 128. \n\n% In particular, we can formulate the momentum updating principle of Siamese path encoder as follows:\n% \\begin{equation}\n% \\label{eq:momentum}\n% \\begin{split}\n%     &\\hat{\\varphi}^{t}= m \\times \\hat{\\varphi}^{(t-1)} +(1-m)\\times \\varphi^{t},\\\\\n%     &\\hat{\\theta}^{t}= m \\times \\hat{\\theta}^{(t-1)} +(1-m)\\times \\theta^{t},\n% \\end{split}\n% \\end{equation}\n\n% \\begin{equation}\n%     \\hat{\\theta}^{t}= m \\times \\hat{\\theta}^{(t-1)} +(1-m)\\times \\theta^{t}\n% \\end{equation}\n\n%\n% \\noindent\n% where $m$, $\\varphi$ and $\\hat{\\varphi}$ are momentum and parameters of path encoders, respectively. $\\theta$ and $\\hat{\\theta}$ are the parameters of projection head.\n\n"
                },
                "subsection 4.2": {
                    "name": "Dual Sparse Path Encoder",
                    "content": "\nIn this section, we introduce our dual sparse path encoder (SPE) that is employed to generate different path representations based on different path views. As shown in Figure \\ref{fig:subfig:prc}, given a path $p_1$, we first generate sparse paths in terms of two different reduction ratios $\\gamma_1$ and $\\gamma_2$. We consider them as different path views, i.e., path view 1 and path view 2. Then, our dual sparse path encoder, including a main encoder and an auxiliary encoder, takes as input two different path views (i.e., $p_1^{1}$ and $p_1^{2}$) and returns different path representations. Specifically, each encoder takes as input two different path views and returns two different path representations, where solid and dotted $\\square$ denote the path representations returned from main encoder based on path view 1 and path view 2, respectively, i.e., $\\mathit{PR}_1^{1}$ and $\\mathit{PR}_1^{2}$. In contrast, solid and dotted $\\triangle$ represent the path representations achieved from auxiliary encoder based on both path views, respectively, i.e,. $\\hat{\\mathit{PR}}_1^{1}$ and $\\hat{\\mathit{PR}}_1^{2}$. To this end, we construct four different path representations for a given path, which promote our design of cross-network relational reasoning and cross-view relational reasoning in turn. Finally, we formulate it as:\n\\begin{equation}\n    \\mathit{PR}_i^{j} =SPE_{\\theta}(p_i^{j},\\gamma)\\text{ , } \\hat{\\mathit{PR}}_i^{j} =SPE_{\\hat{\\theta}}(p_i^{j},\\gamma)\\text{ , }\n\\end{equation}\n\\noindent\nwhere $\\mathit{PR}_i^{j}$ and $\\hat{\\mathit{PR}}_i^{j}$ are path representations obtained from the main encoder and the auxiliary encoder, respectively. $p_i$ denotes the $i$-th path in the path set. $j \\in [1,2]$ denotes the path views. $\\theta$ and $\\hat{\\theta}$ are the parameters for the main encoder and auxiliary encoder.\n\n"
                },
                "subsection 4.3": {
                    "name": "Relational Reasoning",
                    "content": "\n\n",
                    "subsubsection 4.3.1": {
                        "name": "Cross-Network Relational Reasoning",
                        "content": "\n\\label{subsec:cn}\nIn \\emph{LightPath}, we employ a dual sparse path encoder, which includes main and auxilary encoder, as shown in Figure~\\ref{fig:subfig:prc}. We first construct path representations through sparsity operation based on different reduction ratios $\\gamma_1$ and $\\gamma_2$. Given a set of path $\\bigl \\{p_1,p_2,\\cdots, p_K\\bigr \\}$, we can have a set of path representations $\\bigl \\{ \\mathit{PR}_{1}^{1}, \\mathit{PR}_{2}^{1}, \\cdots, \\mathit{PR}_{K}^{1} \\bigr \\}$ from main encoder and $\\bigl \\{ \\hat{\\mathit{PR}}_{1}^{1},\\hat{\\mathit{PR}}_{2}^{1},\\cdots, \\hat{\\mathit{PR}}_{K}^{1} \\bigr \\}$ or $\\bigl \\{ \\hat{\\mathit{PR}}_{1}^{2},\\hat{\\mathit{PR}}_{2}^{2},\\cdots,\\hat{\\mathit{PR}}_{K}^{2} \\bigr \\}$ from auxiliary encoder by using path representation construction. Then we employ relation aggregation $a(\\cdot)$ that joins the positive path representation relations $\\langle \\mathit{PR}_{i}^{1}$, $\\hat{\\mathit{PR}}_{i}^{1} \\rangle$ or $\\langle \\mathit{PR}_{i}^{1}$, $\\hat{\\mathit{PR}}_{i}^{2} \\rangle$ and the negative path representation relations $\\langle \\mathit{PR}_{i}^{1}$, $\\hat{\\mathit{PR}}_{\\backslash i}^{1} \\rangle$, where $i$ denotes the $i$-th path sample and $\\backslash i \\neq i$ represents randomly selected path representations in a minibatch. Take Figure~\\ref{fig:subfig:msrr} as an example, where $K=3$. we join $\\langle \\mathit{PR}_{1}^{1}$, $\\hat{\\mathit{PR}}_{1}^{1} \\rangle$ as a positive relation pair (representation from same path), and $\\langle \\mathit{PR}_{1}^{1}$, $\\hat{\\mathit{PR}}_{2}^{1} \\rangle$ as a negative relation pair (representation from different paths) through aggregation function $a$.\n%\nNext, the relational head $RRH_{\\varphi}(\\cdot)$, which is non-linear function approximator parameterized by $\\varphi$, takes as input representation relation pairs of cross-network and returns a relation score $y$. Finally, we formulate the cross-network relational reasoning task as a binary classification task, where we use binary cross-entropy loss to train our sparse path encoder, which is given as follows.\n\n\n% \\begin{equation}\n% \\label{eq:loss_cn}\n% \\begin{split}\n%     & \\mathcal{L}_{cn}=\\underset{\\boldsymbol{\\theta}, \\boldsymbol{\\theta^{\\prime}},\\varphi}{\\operatorname{argmin}} \\sum_{i=1}^{K}\\sum_{j=1}^{2}  \\underbrace{\\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(P_{i}^{j}, \\hat{P_{i}^{j}}\\right)\\right), t=1\\right)}_{\\text {intra-reasoning }} \\\\ \n%     &+ \\underbrace{\\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(P_{i}^{j}, \\hat{P_{\\backslash i}^{j}}\\right)\\right), t=0\\right)}_{\\text {inter-reasoning }},\n% \\end{split}\n% \\end{equation}\n\n\\begin{equation}\n\\label{eq:loss_cn}\n\\mathcal{L}_{cn}=\\underset{\\boldsymbol{\\theta}, \\varphi}{\\operatorname{argmin}} \\sum_{i=1}^{K}\\sum_{j=1}^{2}  \\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(\\mathit{PR}_{i}^{j}, \\hat{\\mathit{PR}}_{i}^{j}\\right)\\right), t=1\\right) +  \\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(\\mathit{PR}_{i}^{j}, \\hat{\\mathit{PR}}_{\\backslash i}^{j}\\right)\\right), t=0\\right)\\text{ , }\n\\end{equation}\n\n\\noindent\nwhere $K$ is the the number of path samples in the minibatch. $a(\\cdot,\\cdot)$ is an aggregation function. $\\mathcal{L}$ is a loss function between relation score and a target relation value. $t$ is a target relation values.\n%\n\nThe intuition behind this is to discriminate path presentations of same path and different paths, which are from different views across dual sparse path encoder and are able to distill the knowledge from historical observations, as well as stabilizing the main encoder training. \nTo realize this, we adopt Siamese architecture for our dual sparse path encoder, where the auxiliary encoder does not directly receive the gradient during the training procedure. In contrast, we update its parameters by leveraging the momentum updating principle:\n% As mentioned in~\\ref{subsec:spe}, we realize this by leveraging the momentum updating principle.\n% The intuition behind this is to discriminate how path samples relate to themselves (intra-reasoning) and other path samples (inter-reasoning) both from the same views and different views across two path encoders to distill the knowledge from historical observations, and thus achieving the stable main path encoder and projection head (i.e., $g_{\\varphi}$ and $f_{\\theta}$) training. As mentioned in~\\ref{subsec:spe}, we realize this by leveraging the momentum updating principle.\n\n% \\begin{equation}\n% \\label{eq:momentum}\n% \\begin{split}\n%     &\\hat{\\varphi}^{t}= m \\times \\hat{\\varphi}^{(t-1)} +(1-m)\\times \\varphi^{t}\\\\\n%     &\\hat{\\theta}^{t}= m \\times \\hat{\\theta}^{(t-1)} +(1-m)\\times \\theta^{t}\n% \\end{split}\n% \\end{equation}\n\n\\begin{equation}\n\\label{eq:momentum}\n    \\hat{\\theta}^{t}= m \\times \\hat{\\theta}^{(t-1)} +(1-m)\\times \\theta^{t},\n\\end{equation}\n% %\n\\noindent\nwhere $m$ is momentum parameters. $\\theta$ and $\\hat{\\theta}$ are the parameters of the main encoder and the auxiliary encoder.\n%\n\n"
                    },
                    "subsubsection 4.3.2": {
                        "name": "Cross-View Relational Reasoning",
                        "content": "\n\n% With the exception of the contrastive relational reasoning across two path encoders, \nTo enhance the learning ability of our \\emph{LightPath}, we further consider the ties between two views within main encoder, which acts as a strong requarization to enhance the learning ability of our methods. We do not have to include such relational reasoning within the auxiliary encoder because it will not directly compute gradient during training, and our goal is to train main encoder.\n% relational reasoning between two views within the main path encoder to enhance the learning ability of cross-network relational reasoning. We do not conduct relational reasoning within the auxiliary path encoder since this encoder does not directly receiving the gradient.\nFigure~\\ref{fig:subfig:msrr} shows the design of our cross-view relational reasoning, which contains two similar representations from two views based on $\\gamma_1$ and $\\gamma_2$. \n%\nThe intuition of cross-view relational reasoning is to preserve the relation between two views of the same path and discriminate them from the view of other paths. \n%\n\nSimilar with cross-network, given a set of paths $\\bigl \\{ p_1, p_2,\\cdots, p_K \\bigr \\}$.\n%\nWe first achieve two set of path representations in terms of two path views based on main encoder, i.e., $\\bigl \\{ \\mathit{PR}_1^{1}, \\mathit{PR}_2^{1},\\cdots,\\mathit{PR}_k^{1} \\bigr \\}$ and $\\bigl \\{ \\mathit{PR}_1^{2}, \\mathit{PR}_2^{2},\\cdots,PR_K^{2} \\bigr \\}$. Then, we join the positive relation pairs (e.g., $\\langle \\mathit{PR}_i^{1}, \\mathit{PR}_i^{2} \\rangle$) and negative relation pairs (e.g., $\\langle \\mathit{PR}_i^{1}, \\mathit{PR}_{\\backslash{i}}^{2} \\rangle$) through aggregation function. For example, as shown in Figure~\\ref{fig:subfig:msrr}, there are 3 paths in the set. Thus, we can denote $\\langle \\mathit{PR}_1^{1}, \\mathit{PR}_1^{2} \\rangle$ as a positive pair and $\\langle \\mathit{PR}_1^{1}, \\mathit{PR}_3^{2} \\rangle$ as a negative pair.\n%\nThen, we further employ relational head $RRH_{\\varphi}(\\cdot)$, which takes as input a positive pair and a negative pairs from different views, to achieve the corresponding relation score $y$ for the cross-view relational reasoning.\n%\nLast, we formulate the cross-view relational reasoning loss to discriminate how different views of a  path is related to themselves and other paths. In this phase, the complete learning objective can be specified as:\n\n% \\begin{equation}\n% \\label{eq:loss_cv}\n% \\begin{split}\n%     & \\mathcal{L}_{cv}=\\underset{\\boldsymbol{\\theta}, \\boldsymbol{\\theta},\\varphi}{\\operatorname{argmin}} \\sum_{i=1}^{K}  \\underbrace{\\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(P_{i}^{1}, P_{i}^{2}\\right)\\right), t=1\\right)}_{\\text {intra-reasoning }} \\\\ \n%     &+ \\underbrace{\\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(P_{i}^{1}, P_{\\backslash i}^{2}\\right)\\right), t=0\\right)}_{\\text {inter-reasoning }},\n% \\end{split}\n% \\end{equation}\n\\begin{equation}\n\\label{eq:loss_cv}\n\\mathcal{L}_{cv}=\\underset{\\mathbf{\\theta},\\mathbf{\\varphi}}{\\operatorname{argmin}} \\sum_{i=1}^{K}  \\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(\\mathit{PR}_{i}^{1}, \\mathit{PR}_{i}^{2}\\right)\\right), t=1\\right) +\\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(\\mathit{PR}_{i}^{1}, \\mathit{PR}_{\\backslash i}^{2}\\right)\\right), t=0\\right)\\text{ , }\n\\end{equation}\n\\noindent\nwhere $K$ is the the number of path samples in the minibatch.\n\n"
                    },
                    "subsubsection 4.3.3": {
                        "name": "RR",
                        "content": "\n\nTo train our dual path encoder end-to-end and efficient learn path representations for downstream tasks, we jointly leverage both the cross-network and cross-view relation reasoning loss. Specifically, the overall objective function is formulated as Eq.~\\ref{eq:msrr}.\n\n\\begin{equation}\n\\label{eq:msrr}\n    \\min_{\\theta, \\varphi}\\mathcal{L}_{RR}=\\mathcal{L}_{cn} +\\mathcal{L}_{cv}\n\\end{equation}\n\n"
                    }
                },
                "subsection 4.4": {
                    "name": "LightPath Training",
                    "content": "\nTo train our sparse path encoder and learn path representations for downstream tasks, we jointly minimize the reconstruction and RR loss. Specifically, the overall objective function is defined as:\n\n\\begin{equation}\n\\label{eq:loss_}\n    \\mathcal{L}=\\mathcal{L}_{rec}+\\mathcal{L}_{RR}\n\\end{equation}\n\n\n\n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[scale=0.7]{./figures/cv0707.pdf}\n%     \\caption{Cross-View Relational Reasoning.}\n%     \\label{fig:cvrr}\n% \n% \\end{figure}\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Global Local Knowledge Distillation (GLKD)",
                "content": "\n\n% Large Transformer models have recently exhibited significant performance improvement in different domains, e.g., computer vision, natural language process (NLP)~\\cite{DBLP:conf/sigmod/Luo00CLQ21,DBLP:journals/pvldb/TangFLTDLMO21}, graph analysis~\\cite{DBLP:conf/sigmod/YangSHX22,DBLP:conf/sigmod/KimCP0HH21,DBLP:conf/sigmod/ChaiC00LM20,DBLP:journals/pvldb/ZhouSLF20,DBLP:conf/sigmod/TaoL019}, and trajectory analysis~\\cite{DBLP:journals/pvldb/WangLCL20,DBLP:conf/sigmod/Shang0B18a,DBLP:journals/pvldb/FangPCDG21} (cf. Figure~\\ref{fig:subfig:tte}). However, such state-of-the-art performance generally involves very wide and deep Transformer architectures, which results in (1) a time-consuming model, including training and inference; (2) high memory demand. Therefore, it is challenging to deploy these models on resource-limited mobile devices. To alleviate this challenge, \n\nSo far, we realize our \\emph{LightPath} through sparse auto-encoder and relational reasoning and transform it from large cuboid (cf. Figure~\\ref{fig:subfig:tte} to a slim cuboid (cf. Figure~\\ref{fig:subfig:ste}). To enable the \\emph{LightPath} that can be deployed on resource-limited mobile devices, we introduce our global-local knowledge distillation (GLKD) to further reduce the size of the sparse auto-encoder, as shown in Figure~\\ref{fig:kd}. We first train a large cuboid teacher encoder with multiple transformer layers and heads (cf. Figure~\\ref{fig:subfig:ste}) based on path reconstruction (cf. Section~\\ref{sec:spe}) and relational reasoning (cf. Section~\\ref{sec:msrr} ). Then, we employ a small rectangle student encoder (cf. Figure~\\ref{fig:subfig:lp}), which has less layers and heads, to mimic a large teacher model and use the teacher's knowledge to obtain similar or superior performance based on GLKD. \n% The main purpose of LGKD is to compress a large teacher model to obtain a lightweight student model by considering both local correlated edge information and global path information without any performance loss.\nSpecifically, GLKD constructs a local knowledge distillation by matching the representations of correlated edges. On such a basis, the global term distills the knowledge from teacher to student that enabling the informative and powerful path representation for the student model. \n% As shown in Figure~\\ref{fig:kd}, our student model aims to mimic the behavior of the path representation from the teacher model with global path representation knowledge distillation (i.e., Global KD loss). In contrast, local edge representation knowledge distillation aims at matching edge correlation information from the teacher model to the student model (i.e., Local KD loss).\n\n\n\n",
                "subsection 5.1": {
                    "name": "Global-path Representation Distillation",
                    "content": "\n% It is worth noticing that path representations play a significant role in path-related tasks (e.g., routing, path recommendations) in intelligent transportation systems. After projecting to latent space, the representations of the same path from teacher encoder $\\mathcal{T}_{\\theta}$ and student encoder $\\mathcal{S}_{\\theta}$ are expected to form the same global structure corresponding to the spatial-temporal dependencies of the path sequence.\n%\n\n% As shown in Figure~\\ref{fig:kd}, we propose a global path representation knowledge distillation to mimic the behavior of path representations of teacher encoder for the shallower student encoder. And thus, the student encoder can not only deploy on resource constrains mobile devices, but also the learned path representation from student encoder $\\mathcal{S}_{\\theta}$ can achieve the same even higher performance on different downstream tasks.\n%\n\nGiven a path $p_i=\\langle e_1,e_2,e_3,\\cdots,e_N\\rangle$, where $N$ is the number of edges in a path. We define $\\mathit{PR}^\\mathcal{T}(p_{i})$ and $\\mathit{PR}^{\\mathcal{S}}(p_{i})$ represent the path representations achieved from teacher encoder $\\mathcal{T}_{\\theta}$ and student encoder $\\mathcal{S}_{\\theta}$. The intuition of global path representation knowledge distillation is to let the student encoder mimic the global properties captured by a large cuboid teacher encoder. And thus, the goal of global path representation knowledge distillation is to put closer the path representation from teacher encoder and student encoder in the latent space. We formalize this problem as minimizing a latent space distance  representation pairs in terms of the large cuboid teacher encoder and the rectangle student encoder. The formulation of the objective function is given as follows: \n\\begin{equation}\n\\small\n    \\min_{\\theta}\\mathcal{L}_{global}\\left(\\mathbf{\\mathit{PR}}^{\\mathcal{T}}({p_{i}}), \\mathbf{\\mathit{PR}}^{\\mathcal{S}}(p_{i})\\right)=\\left\\|sp(\\mathbf{\\mathit{PR}}^{\\mathcal{T}}(p_{i})/t)-sp(\\mathbf{\\mathit{PR}}^{\\mathcal{S}}(p_{i})/t)\\right\\|^{2}\\textbf{,}\n\\end{equation}\n\\noindent\nwhere $sp(\\cdot)$ is exponential function. $t$ denotes the temperature. Using a higher value for $t$ produces a softer probability distributions over path representations.\n\n"
                },
                "subsection 5.2": {
                    "name": "Local-edge Correlation Distillation",
                    "content": "\n% Following the existing methods for knowledge distillation~\\cite{}, \nThe goal of local-edge structure distillation is to preserve the local similarity of the edge correlations in a path. In particular, it is expected that the representation of the same edge in a path represented by the teacher encoder and the student encoder should be close to each other. The intuition is that a rectangle student encoder mimics the edge correlations in a path captured by a large cuboid teacher encoder. Using a similarity measurement, we formulate the local-edge structure distillation problem as minimizing the latent space distance of edge representations from the teacher encoder and then student encoder. \n%\n\nIn specific, given a path $p = \\langle e_1,e_2,e_3,\\cdots e_N \\rangle$ in a road network, where $N$ is the number of edges in a path. Through applying an $L$-layers Transformer encoder (i.e., teacher encoder $\\mathcal{T}_{\\theta}$) and $L^{\\prime}$-layers Transformer encoder (i.e., student encoder $\\mathcal{S}_{\\theta}$) upon sparse path $p^{\\prime}$, where $L \\ll L^{\\prime}$, the edge representation that captures spatial dependencies are derived as follows. \n\\begin{equation}\n    F^{\\mathcal{T}}(e_{i})^{N^{\\prime}}_{i=1} = \\mathcal{T}_{\\theta}(p)\\text{ , } F^{\\mathcal{S}}(e_{i})^{N^{\\prime}}_{i=1} = \\mathcal{S}_{\\theta}(p)\\text{ , }\n\\end{equation}\n\\noindent\nwhere $F^{\\mathcal{T}}(e_{i})^{N^{\\prime}}_{i=1}$ and $F^{\\mathcal{S}}(e_{i})^{N^{\\prime}}_{i=1}$ represent the edge representation with respect to the teacher and student encoder, respectively.\n%\n\nIn this phase, the goal of learning is to reduce the latent space distance between same edge pair from the teacher and student encoder, respectively. To this end, we aim to minimize the following objective functions between edge representation pairs in terms of the parameters of the student encoder.\n\n\\begin{equation}\n\\small\n    \\min_{\\theta}\\mathcal{L}_{local}\\left(\\mathbf{F}^{\\mathcal{T}}(e_{i}), \\mathbf{F}^{\\mathcal{S}}(e_{i})\\right)=\\frac{1}{n}\\sum^{n}_{i=1}\\left\\|sp(\\mathbf{F}^{\\mathcal{T}}(e_{i})/t)-sp(\\mathbf{F}^{\\mathcal{S}}(e_{i})/t)\\right\\|^{2}\\textbf{,}\n\\end{equation}\n\\noindent\nwhere $sp(\\cdot)$ represents exponential function. $t$ denotes the temperature. Using a higher value for $t$ produces a softer probability distributions over edges.\n%\n\n"
                },
                "subsection 5.3": {
                    "name": "GLKD",
                    "content": "\n\nTo train our global and local knowledge distillation in an end-to-end fashion, we jointly leverage both the global and local knowledge distillation loss. Specifically, the overall objective function to minimize is defined in Eq.~\\ref{eq:ad}.\n\n\\begin{equation}\n\\label{eq:ad}\n\\min_{\\theta}\\mathcal{L}_{GLKD}=\\alpha*\\mathcal{L}_{global} +\\left(1-\\alpha\\right)*\\mathcal{L}_{local}\\text{ , }\n\\end{equation}\n\\noindent\nwhere $\\alpha$ is balancing factor.\n\n\n% \\begin{equation}\n% \\label{eq:loss_}\n%     \\mathcal{L}=\\underbrace{\\mathcal{L}_{rec}}_{\\text{Reconstruction Loss}}+\\underbrace{\\mathcal{L}_{cn}+\\mathcal{L}_{cv}}_{\\text{MSCRR Loss}}+\\underbrace{\\mathcal{L}_{GLKD}}_{\\text{GLKD Loss}}\n% \\end{equation}\n% \\noindent\n% where $\\lambda$ is a balancing factor.\n"
                }
            },
            "section 6": {
                "name": "Experiments",
                "content": "\n",
                "subsection 6.1": {
                    "name": "Experimental Setup",
                    "content": "\n",
                    "subsubsection 6.1.1": {
                        "name": "Datasets",
                        "content": " We conduct experiments on two real-world datasets and one synthetic dataset to enable fair comparisons with existing studies. Based on two real-world datasets, we report results for two downstream tasks: travel time estimation~\\cite{DBLP:journals/corr/abs-2203-16110,DBLP:conf/sigmod/Yuan0BF20}, and path ranking~\\cite{DBLP:journals/pvldb/0026HFZL020, DBLP:conf/icde/Yang020,yang2020context}. Due to the lack of large amounts of long paths in the real-world datasets, we construct one synthetic dataset that contains paths with lengths of 100, 150, and 200 to verify the efficiency and scalability of \\emph{LightPath}.\n% Then, we report performance comparison with path lengths 50 on both real-world datasets. In contrast, we only report the accuracy results for the Chengdu dataset for path length 80 since there are not enough paths with this length that enable \\emph{LightPath} training for the Aalborg dataset.\n%\n\n\\noindent\n%~\\cite{DBLP:conf/icde/Guo0HJ18}\n\\textbf{Aalborg, Denmark:} We collect the road network of Aalborg from OpenStreetMap\\footnote{\\href{https://www.openstreetmap.org}{https://www.openstreetmap.org}} that contains 10,017 nodes and 11,597 edges. Specifically, this dataset contains 180 million GPS records from 183 vehicles sampled at 1 Hz over a two-year period from 2017 to 2018. After map matching~\\cite{DBLP:conf/gis/NewsonK09}, we obtain 39,160 paths with length 50. \n%\n% Chengdu, China\\protect\\footnote{\\href{https://outreach.didichuxing.com/research/opendata/en/}{https://outreach.didichuxing.com/research/opendata/en/}}:} October and November 2016\n\n\\noindent\n\\textbf{Chengdu, China:} This dataset was collected from Chengdu, China, on October and November 2016. We obtain the corresponding road network from OpenStreetMap. The network contains 6,632 nodes and 17,038 edges. The GPS data was sampled at about 1/4-1/2 Hz. We obtain 50,000 paths through map matching with lengths 50.\n% and 40,000 paths with lengths 80.\n\n%\n\\noindent\n\\textbf{Synthetic: }Due to the lack of large amounts of long paths in the real-world datasets, we construct one synthetic dataset to verify the efficiency and scalability of \\emph{LightPath}. In particular, we first randomly pick 500 nodes in road network of Aalborg dataset, and then expand each node to a path by random walking until the path length reach the threshold (i.e., 100, 150, 200), which we refer as to generation process. Subsequently, we iterate the generation process 10 times to construct 5,000 paths for each path length.    \n\n"
                    },
                    "subsubsection 6.1.2": {
                        "name": "Downstream Tasks",
                        "content": " We report the results on two downstream tasks: \n\n\\noindent\n\\textbf{Path Travel Time Estimation: }We obtain travel time (in seconds) for each path from the trajectory. We aim to utilize a regression model to predict the travel time based on the learned path representations. We employ Mean Absolute Error(\\textbf{MAE}), Mean Absolute Relative Error(\\textbf{MARE}), and Mean Absolute Percentage Error(\\textbf{MAPE}) to evaluate the performance of travel time estimations. The smaller values of these metrics, the better performance we achieve.\n\n\\noindent\n\\textbf{Path Ranking: }Each path is assigned a ranking score in the range $[0,1]$, which is obtained from historical trajectories by following the existing studies~\\cite{yang2020context,DBLP:conf/icde/Yang020}. \nMore specifically, we take the path that is used by a driver in the historical trajectories as the trajectory path, which is denoted as the top ranked path.\n% With a historical trajectory of a driver, we treat the path used by the trajectory, called the trajectory path, as the top ranked path. \nThen, we \n% use a path generation algorithm to \ngenerate multiple paths connecting the same source and destination via path finding algorithms~\\cite{DBLP:journals/tkde/LiuJYZ18}. Finally, we calculate the similarity between a generated path and the trajectory path as a ranking score. The higher ranking score indicates a generated path is more similar to the trajectory path, and the trajectory path itself has a score of 1 and ranks the highest. \n% %\nTo measure the path ranking, we apply \\textbf{MAE}, the Kendall rank correlation coefficient ($\\tau$), and the Spearman's rank correlation coefficient ($\\rho$), which are widely used metrics in path ranking, to evaluate the effectiveness of path ranking.\n\n% \\begin{table*}[t]\n% \\caption{{Overall Accuracy on Travel Time Estimation and Ranking Score Estimation}}\n% % \\small\n% \\centering\n% \\begin{tabular}{l|lll|lll|lll|lll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{Method}} & \\multicolumn{6}{l|}{\\textbf{City A}}                                                     & \\multicolumn{6}{l}{\\textbf{City B}}                                                     \\\\ \\cline{2-13} \n%                         & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}  & \\multicolumn{3}{l|}{\\textbf{Path Ranking}} & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}} & \\multicolumn{3}{l}{\\textbf{Path Ranking}} \\\\ \\cline{2-13} \n%                         &\\textbf{MAE} &\\textbf{MARE} &\\textbf{MAPE} & \\textbf{MAE}    &\\textbf{$\\tau$} &\\textbf{$\\rho$}    &\\textbf{MAE}     & \\textbf{MARE}    & \\textbf{MAPE}   & \\textbf{MAE} &\\textbf{$\\tau$} &\\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% \\emph{Node2vec}  &154.07     &0.20    &25.22       &0.24        &0.59           &0.64              &267.28       &0.23   &26.30          &0.15       &0.74                 & 0.77                \\\\ \\hline\n% \\emph{MoCo}  &146.29     &0.19    &21.60       &0.25        &0.53           &0.57              &237.14       &0.20   &23.13          &0.15       &0.77                 &0.81                 \\\\ \\hline\n% \\emph{Toast}  &137.27     &0.17   &20.43      &0.24        &0.59           &0.63              &240.57       &0.21   &23.50          &0.11       &0.65                 &0.68                 \\\\ \\hline\n% \\emph{t2vec}  &147.24     &0.19    &22.13       &0.25        &0.52           &0.56              &242.96       &0.21   &23.65          &0.14       &0.77                 & 0.82                \\\\ \\hline\n% \\emph{NeuTraj}  &117.06     &0.15    &18.09       &0.25        &0.60           &0.64              &232.96       &0.20   &22.73          &0.12       &0.79                 &0.83                 \\\\ \\hline\n% \\emph{PIM}  &102.09     &0.14    &14.92       &0.20        &0.63           &0.67              &223.34       &0.19   &21.69          &0.12       &0.80                 &0.84                 \\\\ \\hline\n% \\emph{HMTRL}  &101.81      &0.13     &14.51        &0.17        &0.68          &0.72              &218.94      &0.19   &21.22          &0.09       &0.83                 &0.84                \\\\ \\hline\n% \\emph{PathRank}  &115.37     &0.15    &16.41       &0.21        &0.64           &0.68              &229.85      &0.20   &22.53          & 0.11      &0.81                 &0.82                \\\\ \\hline\n% \\emph{LightPath-Sup}  &105.51     &0.15    &16.35       &0.14        &0.68           &0.72              &218.67      &0.19   &21.36          & 0.13      &0.76                 &0.79                \\\\ \\hline\n% \\rowcolor{Gray}\n% \\emph{LightPath}   \n% & \\multicolumn{1}{l}{\\textbf{85.76}}        & \\multicolumn{1}{l}{\\textbf{0.11}}          & \\multicolumn{1}{l|}{\\textbf{12.12}}         & \\multicolumn{1}{l}{\\textbf{0.13}}         & \\multicolumn{1}{l}{\\textbf{0.73}}         &\\textbf{0.77}         \n% % & \\multicolumn{1}{l}{\\textbf{86.78}}        & \\multicolumn{1}{l}{\\textbf{0.11}}          & \\multicolumn{1}{l}{\\textbf{12.39}}         & \\multicolumn{1}{l}{\\textbf{0.14}}         & \\multicolumn{1}{l}{\\textbf{0.71}}         & \\textbf{0.75} \n% &\\textbf{212.61}     &\\textbf{0.18}   &\\textbf{20.75}          &\\textbf{0.07}      &\\textbf{0.87}                &\\textbf{0.88}                 \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:ttpr}\n%\n% \\end{table*}\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n\n\n\n"
                    },
                    "subsubsection 6.1.3": {
                        "name": "Models for Downstream Tasks",
                        "content": "\nFor all unsupervised learning methods, we first achieve the corresponding $d$ dimensionality path representation and then we build a regression model that takes as input a path representation and output estimated the travel time and path ranking, respectively.\n% For both travel time and path ranking estimation tasks, we aim to build a regression model to estimate them based on the learned path representations from all unsupervised learning baselines respectively.\nIn particular, we select ensemble model \\textit{Gradient Boosting Regressor}(GBR)~\\cite{DBLP:conf/nips/PeterDHN17} as our prediction model since they are regression problems.  \n\n\n"
                    },
                    "subsubsection 6.1.4": {
                        "name": "Baselines",
                        "content": "\nWe compare~\\emph{LightPath} with 9 baselines, including 6 unsupervised learning-based methods and 3 supervised learning-based methods. \n%\n% The unsupervised methods are: (1) \\textbf{Node2vec}~\\cite{DBLP:conf/kdd/GroverL16} is an unsupervised node representation model that learn node representation in a graph. We achieve the path representation by aggregating the node representations of the nodes in a path. \n% %\n% (2) \\textbf{MoCo}~\\cite{DBLP:conf/cvpr/He0WXG20} is a momentum contrast for unsupervised visual representation learning. Here we use momentum contrast to learn path representations. \n% %\n% (3) \\textbf{Toast}~\\cite{DBLP:conf/cikm/ChenLCBLLCE21} first uses auxiliary traffic context information to learn road segment representation based on skip-gram model, and then utilizes stacked transformer encoder layer to train trajectory representation through route recovery and trajectory discrimination tasks. We use same schema to learn path representations. \n% %\n% (4) \\textbf{t2vec}~\\cite{DBLP:conf/icde/LiZCJW18} is a trajectory representation learning method for similarity computation based on encoder-decoder framework, which is trained to reconstruct the original trajectory. We use a sequence of edges in a path to represent a trajectory.\n% %\n% (5)\\textbf{NeuTraj}~\\cite{DBLP:conf/icde/YaoCZB19} is a method that revised the structure of LSTM to\n% learn representations of grid in the process of training their\n% framework. To support our task with it, we replace the grid with edges in their framework.\n% %\n% (6) \\textbf{PIM}~\\cite{DBLP:conf/ijcai/YangGHT021} is an unsupervised path representation learning approach based on mutual information maximization to learn path representation.\n% %\n% The supervised methods that learn task-specific representations for specific downstream tasks, which are (1) \\textbf{HMTRL}~\\cite{DBLP:journals/pvldb/0026HFZL020} is a unified route representation learning framework for multi-modal transportation recommendation. \n% %\n% (2) \\textbf{PathRank}~\\cite{yang2020context} is a supervised path representation learning model based on GRUs, which treats departure time and driver ID as additional information.\n% %\n% (3) \\textbf{LightPath-Sup} is a supervised version of our \\emph{LightPath}, where we train it in a supervised manner.\n%\nThe details of these baseline methods are summarized as follows:\n\\begin{itemize}\n    \\item \\textbf{Node2vec}~\\cite{DBLP:conf/kdd/GroverL16} is an unsupervised node representation model that learn node representation in a graph. We achieve the path representation by aggregating the node representations of the nodes in a path. \n    \n    \\item \\textbf{MoCo}~\\cite{DBLP:conf/cvpr/He0WXG20} is a momentum contrast for unsupervised visual representation learning. Here we use momentum contrast to learn path representations.\n    % \n    \\item \\textbf{Toast}~\\cite{DBLP:conf/cikm/ChenLCBLLCE21} first uses auxiliary traffic context information to learn road segment representation based on the skip-gram model and then utilizes a stacked transformer encoder layer to train trajectory representation through route recovery and trajectory discrimination tasks. We use the same schema to learn path representations.\n    \n    \\item \\textbf{t2vec}~\\cite{DBLP:conf/icde/LiZCJW18} is a trajectory representation learning method for similarity computation based on the encoder-decoder framework, which is trained to reconstruct the original trajectory. We use a sequence of edges in a path to represent a trajectory. \n    \\item \\textbf{NeuTraj}~\\cite{DBLP:conf/icde/YaoCZB19} is a method that revised the structure of LSTM to\n    learn representations of the grid in the process of training their\n    framework. To support our task with it, we replace the grid with edges in their framework.\n    \\item \\textbf{PIM}~\\cite{DBLP:conf/ijcai/YangGHT021} is an unsupervised path representation learning approach that first generates negative samples using curriculum learning and then employs global and local mutual information maximization to learn path representations.\n    \\item \\textbf{HMTRL}~\\cite{DBLP:journals/pvldb/0026HFZL020} is a supervised path representation learning framework for multi-modal transportation recommendation. \n    \\item \\textbf{PathRank}~\\cite{yang2020context} is a supervised path representation learning model based on GRUs, which treats departure time and driver ID as additional information.\n    \n    \\item \\textbf{CompactETA}~\\cite{DBLP:conf/kdd/FuMYW20} aims to estimate travel time based on a real time predictor and an asynchronous updater.  \\item \\textbf{HierETA}~\\cite{DBLP:conf/kdd/ChenXGFMCC22} is a supervised multi-view trajectory representation method to estimate the travel time. \n    \\item \\textbf{LightPath-Sup} is a supervised version of our \\emph{LightPath}, where we train it in a supervised manner.\n\\end{itemize}\n\n\tFor all unsupervised learning methods, we first use unlabeled training data (e.g., 30K unlabeled Aalborg dataset and 50K unlabeled Chengdu dataset) to train path encoders to obtain path representations. Then a regression model takes as input path representations and returns the estimated travel time and path ranking score using a limited labeled training dataset, e.g., the 12K labeled Aalborg dataset. In comparison, for all supervised learning methods, we directly train path encoders using a limited labeled training dataset, e.g., the 12K labeled Aalborg dataset and Chengdu dataset.\n\t\n"
                    },
                    "subsubsection 6.1.5": {
                        "name": "Implementation Details",
                        "content": "\nWe employ an asymmetrical sparse auto-encoder architecture and randomly initialize all learnable parameters with uniform distributions. \n%\nIn particular, we adopt Siamese architecture, where we update the parameters of the auxiliary encoder based on the momentum updating principle based on the main encoder and we set the momentum parameter $m=0.99$.\n%\nWe employ node2vec~\\cite{DBLP:conf/kdd/GroverL16} to embed each edge to 128-dimensional vectors and set the dimension for path representation to 128. For a fair comparison, we set the path representation dimensionality of all baseline methods as 128.\n%\n% Our projection head contains a linear layer with output size 1024 followed by batch normalization~\\cite{DBLP:conf/nips/SanturkarTIM18}, Parametric rectified linear units (PReLU)~\\cite{DBLP:conf/iccv/HeZRS15}, and a final linear layer with output dimension 128. \n%\nWe select concatenate as the relation aggregation function $a(\\cdot,\\cdot)$.\n%\nWe use the AdamW optimizer with a cosine decay learning rate schedule over 400 epochs, with a warm-up period of 40 epochs. We set the base learning rate to 1e-3 and betas as $(0.9,0.95)$.\n%\nWe vary $\\gamma$ from 0.1,0.3,0.5,0.7,0.9 to study the effect of path scalability and efficiency for the \\emph{LightPath}.\n%\nIn addition, we consider four different path lengths, i.e., 50, 100, 150, and 200, to study the effectiveness, efficiency, and scalability of the \\emph{LightPath}. \n%\nWe then evaluate our \\emph{LightPath} as well as all baselines on a powerful Linux server with 40 Intel(R) Xeon(R) W-2155 CPU @ 3.30GHz and two TITAN RTX GPU cards. Finally, all algorithms are implemented in PyTorch 1.11.0.\n%\n% \n\n\n\n"
                    }
                },
                "subsection 6.2": {
                    "name": "Experimental Results",
                    "content": "\n%%%%City A Aalborg City B Chengdu\n",
                    "subsubsection 6.2.1": {
                        "name": "Overall Performance",
                        "content": "\nTable~\\ref{tab:ttpr} shows the overall performance of our \\emph{LightPath} and all the compared baselines on both datasets in terms of different evaluation metrics. Especially, we select 30K unlabeled paths on Aalborg and Chengdu, respectively, but we only have 12K labeled paths for both datasets. \n\n% to train all unsupervised methods, while using 12K labeled paths for both datasets to train supervised methods. \nThus, we use 30K unlabeled paths to train path encoder for unsupervised-based methods. However, supervised approaches can only use the 12K labeled paths.  \n%\nOverall, \\emph{LightPath} outperforms all the baselines on these two tasks for both datasets, which demonstrates the advance of our model. \n%\nSpecifically, we can make the following observations. Graph representation learning based approach \\emph{Node2vec} is much worse than \\emph{LightPath}. This is because \\emph{Node2vec} fails to capture spatial dependencies in a path. In contrast, \\emph{LightPath} considers the spatial dependencies through the self-attention mechanism, thus achieving better performance.\n%\n\nAlthough \\emph{MoCo} considers the dependencies among edges in a path, this method still performs worse. The main reason is that \\emph{MoCo} can leverage the spatial dependencies, but it converges very slow since it needs large amounts of negative samples to enable training.\n%\n\\emph{LightPath} also outperform \\emph{t2vec} and \\emph{NeuTraj}, which both are first design to learn trajectory representation for trajectory similarity computation. This suggests that random drops on some edges and not reconstruct these edges in a path resulting in spatial information missing, thus achieving the worse performance on downstream tasks. \n%\n\\emph{PIM} consistently outperforms all other unsupervised baselines, which demonstrates the effectiveness of representation learning. The main reason is that \\emph{PIM} is designed for path representation learning. However, \\emph{PIM} is InfoNCE based method and has high computation complexity,\n% when the large negative samples are available; thus, it is\nmaking it hard to deploy on resource-limited edge devices.\n%\n% For \\emph{HMTRL}, \\emph{PathRank}, and \\emph{LightPath} these supervised methods, \n%\n\\emph{HMTRL}, \\emph{PathRank} and \\emph{LightPath-Sup} are three supervised learning methods that achieve relatively worse performance due to the lack of labeled training data. Since labeling data\n% achieving labeled data \nis very time-consuming and expensive. We consider a scenario where labeled data is limited in this paper.\n\n\n% a user can use his/her trajectory to fine tune to get personalzied service. in this case, we do not want to use many trajectories from the user. \n\n"
                    },
                    "subsubsection 6.2.2": {
                        "name": "LightPath",
                        "content": "\nModel pre-training aims to create generic representations that can then be fine-tuned for multiple downstream tasks using a limited labeled dataset. Especially for many personalized services (e.g., personalized travel time estimation or path ranking estimation) in transportation applications, a user can use his/her trajectory to fine-tune the pre-trained model and then achieve the personalized service, where we do not have many trajectories from the user.\n% Thus, pre-training plays a pivotal role in \n%\nIn this experiment, we evaluate the effect of Pre-training. We employ \\emph{LightPath} as a pre-training method for the supervised method \\emph{LightPath-Sup}. Specifically, we first train \\emph{LightPath} in an unsupervised fashion, and then we use the learned transformer path encoder to initialize the transformer in \\emph{LightPath-Sup}. Here, it takes as input a sequence of edge representations and predicts the travel time and path ranking score. \n%\nFigure~\\ref{fig:pretrain} illustrates the performance of \\emph{LightPath-Sup} w. and w/o pre-training over two downstream tasks on both datasets. When employing non-pre-trained \\emph{LightPath-Sup}, we train it using 12K labeled training paths. We notice that (1) when employing pre-training, we can obtain the same performance with no-pre-trained \\emph{LightPath-Sup} using less labeled data. For example, \n% when treating \\emph{LightPath} as pre-training method,\n\\emph{LightPath-Sup} w. pre-training only needs 8K, and 10K labeled training paths for the Aalborg and Chengdu, respectively, to achieve the same performance of \\emph{LightPath-Sup} w/o pre-training with 12k labeled samples on the task of travel time estimation. (2) \n% when using pre-training,\n\\emph{LightPath-Sup} w. pre-training achieves higher performance than\n% the non-pre-trained\n\\emph{LightPath-Sup} w/o pre-training. We observe similar results on the task of path ranking, demonstrating that \\emph{LightPath} can be used as a pre-training method to enhance supervised methods.\n\n%\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n\n\n% \\begin{table}[t]\n% \\centering\n% \\caption{Effect of Variants of \\emph{LightPath}}\n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{Method}}                                                  & \\multicolumn{6}{l}{\\textbf{Aalborg}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% \\textit{\\textbf{w/o RR}}                                                          & \\multicolumn{1}{l|}{94.90}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.85}         & \\multicolumn{1}{l|}{0.17}         & \\multicolumn{1}{l|}{0.66}         & 0.70         \\\\ \\hline\n% \\textit{\\textbf{w/o Rec.}}                                                          & \\multicolumn{1}{l|}{103.45}        & \\multicolumn{1}{l|}{0.14}          & \\multicolumn{1}{l|}{15.76}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.65}         & 0.69         \\\\ \\hline\n% \\textit{\\textbf{w/o ME}} & \\multicolumn{1}{l|}{91.57}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.09}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.68}         & 0.72         \\\\ \\hline\n% % \\textit{\\textbf{w/o PH}}  & \\multicolumn{1}{l|}{90.21}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{13.04}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.69}         & 0.74         \\\\ \\hline\n% \\textit{\\textbf{w/o CN}}                                               & \\multicolumn{1}{l|}{93.17}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.35}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.68}          & 0.73         \\\\ \\hline\n% \\textit{\\textbf{w/o CV}}                                                  & \\multicolumn{1}{l|}{89.84}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.51}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.68}         & 0.72         \\\\ \\hline\n% % \\rowcolor{Gray}\n% \\textit{\\textbf{LightPath}}                 \n% % & \\multicolumn{1}{l|}{86.78}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.39}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.71}         & 0.75 \n% &                      \\multicolumn{1}{l|}{\\textbf{85.76}}        & \\multicolumn{1}{l|}{\\textbf{0.11}}          & \\multicolumn{1}{l|}{\\textbf{12.12}}         & \\multicolumn{1}{l|}{\\textbf{0.13}}         & \\multicolumn{1}{l|}{\\textbf{0.73}}         &\\textbf{0.77}\n% \\\\ \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{Method}}                                                  & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% \\textit{\\textbf{w/o RR}}                                                          & \\multicolumn{1}{l|}{224.31}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.90}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.76}         & 0.79         \\\\ \\hline\n% \\textit{\\textbf{w/o Rec.}}                                                          & \\multicolumn{1}{l|}{229.24}        & \\multicolumn{1}{l|}{0.20}          & \\multicolumn{1}{l|}{22.36}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.69}         & 0.73         \\\\ \\hline\n% \\textit{\\textbf{w/o ME}} & \\multicolumn{1}{l|}{223.81}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.86}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.78}         & 0.81         \\\\ \\hline\n% % \\textit{\\textbf{w/o PH}}  & \\multicolumn{1}{l|}{220.58}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.60}         & \\multicolumn{1}{l|}{0.11}         & \\multicolumn{1}{l|}{0.83}         & 0.85         \\\\ \\hline\n% \\textit{\\textbf{w/o CN}}                                               & \\multicolumn{1}{l|}{217.14}       & \\multicolumn{1}{l|}{0.18}          & \\multicolumn{1}{l|}{21.29}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.80}         & 0.83         \\\\ \\hline\n% \\textit{\\textbf{w/o CV}}                                                  & \\multicolumn{1}{l|}{215.59}       & \\multicolumn{1}{l|}{0.18}          & \\multicolumn{1}{l|}{21.20}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.81}         & 0.83         \\\\ \\hline\n% % \\rowcolor{Gray}\n% \\textit{\\textbf{LightPath}}                                                       & \\multicolumn{1}{l|}{\\textbf{212.61}}        & \\multicolumn{1}{l|}{\\textbf{0.18}}          & \\multicolumn{1}{l|}{\\textbf{20.75}}         & \\multicolumn{1}{l|}{\\textbf{0.07}}         & \\multicolumn{1}{l|}{\\textbf{0.87}}         &\\textbf{0.88}         \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:basicl}\n\n% \\end{table}\n\n% \\subsubsection{Performance for Different Path Length 20 50 80} \n% In Table~\\ref{tab:80}, we further compare \\emph{LightPath} with all baselines under path sequence length 80 on Chengdu dataset.\n% % since we do not have enough paths with a length of 80 for the Aalborg dataset. \n% We can observe that the proposed model \\emph{LightPath} consistently outperforms all other baselines. Specifically, \\emph{LightPath} achieves the better performance than \\emph{Node2vec} since it cannot consider the spatial-temporal dependencies. \n% %\n% We can observe that \\emph{MoCo}, \\emph{Toast}, \\emph{t2vec}, \\emph{ NeuTraj}, and \\emph{PIM} are also much worse than \\emph{LightPath} in the form of path length 80, which shows 1) the poor scalability of path length; 2) ineffectiveness of path representation learning.\n% %\n% In addition, \\emph{LightPath} consistently outperforms the \\emph{HMTRL} and \\emph{PathRank} due to the limited labeled data. \n% Since we do not have enough paths with a length of 80 on the Aalborg dataset, we do not include experiments on Aalborg. \n% \\begin{table}[t]\n% \\caption{Performance for Different Path Length 20 50 80}\n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{Method}} & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                     \\\\ \\cline{2-7} \n%                         & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                          & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                         \\\\ \\cline{2-7} \n%                         & \\multicolumn{1}{l|}{\\textbf{MAE}}    & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% \\emph{Node2vec}                & \\multicolumn{1}{l|}{400.43}       & \\multicolumn{1}{l|}{0.23}     & \\multicolumn{1}{l|}{23.87}      & \\multicolumn{1}{l|}{0.24}    & \\multicolumn{1}{l|}{0.64}    &0.68     \\\\ \\hline\n% \\emph{MoCo}                    & \\multicolumn{1}{l|}{432.81} & \\multicolumn{1}{l|}{0.24} & \\multicolumn{1}{l|}{24.96} & \\multicolumn{1}{l|}{0.19}    & \\multicolumn{1}{l|}{0.65}    &0.69     \\\\ \\hline\n% \\emph{Toast}                     & \\multicolumn{1}{l|}{377.72} & \\multicolumn{1}{l|}{0.21} & \\multicolumn{1}{l|}{19.74} & \\multicolumn{1}{l|}{0.17}    & \\multicolumn{1}{l|}{0.67}    &0.73     \\\\ \\hline\n% \\emph{t2vec}                   & \\multicolumn{1}{l|}{396.09} & \\multicolumn{1}{l|}{0.21} & \\multicolumn{1}{l|}{21.47} & \\multicolumn{1}{l|}{0.20}    & \\multicolumn{1}{l|}{0.61}    &0.63     \\\\ \\hline\n\n% \\emph{NeuTraj}                 & \\multicolumn{1}{l|}{378.68} & \\multicolumn{1}{l|}{0.20} & \\multicolumn{1}{l|}{21.84} & \\multicolumn{1}{l|}{0.18}    & \\multicolumn{1}{l|}{0.67}    &0.71     \\\\ \\hline\n% \\emph{PIM}                     & \\multicolumn{1}{l|}{367.66} & \\multicolumn{1}{l|}{0.20} & \\multicolumn{1}{l|}{21.88} & \\multicolumn{1}{l|}{0.17}    & \\multicolumn{1}{l|}{0.68}    &0.71     \\\\ \\hline\n% \\emph{HMTRL}                   & \\multicolumn{1}{l|}{346.06}       & \\multicolumn{1}{l|}{0.19}     & \\multicolumn{1}{l|}{20.04}      & \\multicolumn{1}{l|}{0.16}    & \\multicolumn{1}{l|}{0.72}    &0.74     \\\\ \\hline\n% \\emph{PathRank}                & \\multicolumn{1}{l|}{352.91}       & \\multicolumn{1}{l|}{0.19}     & \\multicolumn{1}{l|}{20.78}      & \\multicolumn{1}{l|}{0.17}    & \\multicolumn{1}{l|}{0.69}    &0.72     \\\\ \\hline\n% \\rowcolor{Gray}\n% \\emph{LightPath}               & \\multicolumn{1}{l|}{339.86} & \\multicolumn{1}{l|}{0.18} & \\multicolumn{1}{l|}{19.96} & \\multicolumn{1}{l|}{0.15}    & \\multicolumn{1}{l|}{0.75}    &0.79     \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:80}\n% \n% \\end{table}\n"
                    },
                    "subsubsection 6.2.3": {
                        "name": "Ablation Studies",
                        "content": "\nTo verify the effectiveness of different components in \\emph{LightPath}, we conduct ablation studies on \\emph{LightPath}: a) effect of variants of \\emph{LightPath}, specifically reconstruction (Rec) loss, relational reasoning (RR) loss, cross-network loss and cross-view loss; b) effect of global-local knowledge distillation.\n%\n\n\\textbf{a)} \\textit{Effect of variants of \\emph{LightPath}}, we consider five variants of \\emph{LightPath}: 1) \\textit{w/o RR}; 2) \\textit{w/o Rec.}; 3) \\textit{w/o ME}; \n% 3) \\textit{w/o PH}; \n4) \\textit{w/o CN}; 5) \\textit{w/o CV}. \n%\nIn \\textit{w/o RR}, we only consider path reconstruction loss and use main encoder; In \\textit{w/o Rec.}, we only consider relational reasoning loss;  In \\textit{w/o ME}, we consider both path reconstruction and relational reasoning losses, but we do not consider Siamese architectures in dual path encoder; \n% In \\textit{w/o PH}, we do not consider projection head for both main encoder and auxiliary encoder; \nIn \\textit{w/o CN}, we remove the cross-network loss in RR; And in \\textit{w/o CV}, we remove cross-view loss in RR.\n% conduct no momentum encoder and only consider path reconstruction loss, in \\textit{w/o Momentum Encoder}, we consider both reconstruction loss and contrastive relational reasoning loss, but not consider the momentum encoder, in \\textit{w/o PH}, we do not consider prediction head for both path encoder and momentum encoder, in \\textit{w/o CN}, we remove the cross-network loss in MSCRR, and in \\textit{w/o CV}, we remove cross-view loss in MSCRR. \n%\nTable~\\ref{tab:basicl} reports the results on both dataset. We can observe that (1) \\emph{LightPath} \\textit{w/o Rec.} achieves the worst performance because the learned $\\mathit{PR}$ only capture information from sparse path while ignoring the removed edges, which verifies the importance of path reconstruction decoder; (2) \\emph{LightPath} \\textit{w/o RR} also achieves the poor performance, which implies the effectiveness of self-supervised relational reasoning.\n%\n(3) We observe that the performance of \\emph{LightPath} degrades without cross-network and cross-view loss on both datasets, which further demonstrates the effectiveness of our relational reasoning loss.\n%\n(4) We notice that \\emph{LightPath} achieves the best performance. This result implies\nthat all the proposed modules contribute positively to the final\nperformance, which validates that \\emph{LightPath} takes advantage of all designed components.\n%\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n\n\n% \\begin{table}[t]\n% \\centering\n% \\caption{Effect of KD, Global Loss and Local Loss}\n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{Method}}                                                  & \\multicolumn{6}{l}{\\textbf{Aalborg}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% \\textit{\\textbf{w/o KD}}                                                  & \\multicolumn{1}{l|}{87.77}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.94}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.70}         &0.74          \\\\ \\hline\n% \\textit{\\textbf{w/o Global}}                                                  & \\multicolumn{1}{l|}{90.24}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.31}         & \\multicolumn{1}{l|}{0.18}         & \\multicolumn{1}{l|}{0.67}         &0.71          \\\\ \\hline\n% \\textit{\\textbf{w/o Local}}                                                  & \\multicolumn{1}{l|}{89.23}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.78}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.69}         &0.73          \\\\ \\hline\n% % \\rowcolor{Gray}\n% \\textit{\\textbf{LightPath}}          \n% % & \\multicolumn{1}{l|}{86.78}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.39}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.71}         & 0.75 \n% &                      \\multicolumn{1}{l|}{\\textbf{85.76}}        & \\multicolumn{1}{l|}{\\textbf{0.11}}          & \\multicolumn{1}{l|}{\\textbf{12.12}}         & \\multicolumn{1}{l|}{\\textbf{0.13}}         & \\multicolumn{1}{l|}{\\textbf{0.73}}         &\\textbf{0.77}     \n% \\\\ \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{Method}}                                                  & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% \\textit{\\textbf{w/o KD}}                                                          & \\multicolumn{1}{l|}{213.26}       & \\multicolumn{1}{l|}{0.18}          & \\multicolumn{1}{l|}{20.97}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.84}         &0.86          \\\\ \\hline\n% \\textit{\\textbf{w/o Global}}  & \\multicolumn{1}{l|}{220.32}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.52}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.79}         &0.81          \\\\ \\hline\n% \\textit{\\textbf{w/o Local}}                                               & \\multicolumn{1}{l|}{215.03}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.02}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.82}         &0.84          \\\\ \\hline\n\n% % \\rowcolor{Gray}\n% \\textit{\\textbf{LightPath}}                                                       & \\multicolumn{1}{l|}{\\textbf{212.61}}        & \\multicolumn{1}{l|}{\\textbf{0.18}}          & \\multicolumn{1}{l|}{\\textbf{20.75}}         & \\multicolumn{1}{l|}{\\textbf{0.07}}         & \\multicolumn{1}{l|}{\\textbf{0.87}}         &\\textbf{0.88}         \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:glkd}\n\n% \\end{table}\n\n\n\n% \\begin{table*}[htp]\n% \\centering\n\n% \\caption{Model Scallability Comparison}\n% \\begin{tabular}{l|l|l|l|l}\n% \\toprule[2pt]\n% \\multirow{2}{*}{N} & 50                & 100                & 150                & 200                \\\\ \\cline{2-5} \n%                   & GFLOPs/gMem./Para. & GFLOPs/gMem./Para. & GFLOPs/gMem./Para. & GFLOPs/gMem./Para. \\\\ \\toprule[1pt]\n% \\emph{MoCo}              & 30.69/4.27/8.71    & 62.08/5.19/8.71    & 92.47/6.37/8.71    & 122.66/7.75/8.71   \\\\ \\hline\n% \\emph{Toast}             & 28.43/2.23/1.81    & 56.87/2.63/1.81    & 93.30/2.95/1.81    & 113.74/3.38/1.81   \\\\ \\hline\n% \\emph{t2vec}             & 5.79/3.01/1.69     & 11.58/3.15/1.69    & 17.48/4.15/1.69    & 23.16/4.75/1.69    \\\\ \\hline\n% \\emph{NeuTraj}           & 8.69/3.29/1.97     & 17.37/3.98/1.97    & 23.06/4.46/1.97    & 34.74/5.54/1.97    \\\\ \\hline\n% \\emph{PIM}               & 10.62/2.02/0.66    & 21.23/2.28/0.66    & 31.85/2.55/0.66    & 42.47/2.80/0.66    \\\\ \\hline\n% \\emph{HMTRL}             & 1.66/2.58/0.53     & 3.40/2.63/0.53        &4.51/2.72/0.53   & 6.80/2.85/0.53                    \\\\ \\hline\n% \\emph{PathRank}          & 1.56/2.03/0.49     &3.20/3.12/0.49   &4.78/3.15/0.49    &6.37/3.47/0.49                    \\\\ \\hline\n% \\emph{LightPath-Sup}     & 6.16/3.67/2.59     & 12.00/4.99/2.59                   &17.84/5.27/2.59                    &23.68/5.57/2.59                    \\\\ \\hline\n% \\emph{LightPath}         & 3.18/1.33/1.57     & 6.23/1.43/1.57     & 9.27/1.52/1.57     & 12.31/1.65/1.57    \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:mscnn}\n%\n% \\end{table*}\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n% \\begin{table*}[htp]\n% % \\label{tab:mscnn}\n\n% \\caption{\\textcolor{red}{Model Scallability Comparison}}\n% \\begin{tabular}{l|lll|lll|lll|lll}\n% \\toprule[2pt]\n% \\multirow{2}{*}{N} & \\multicolumn{3}{l|}{50}                                          & \\multicolumn{3}{l|}{100}                                         & \\multicolumn{3}{l|}{150}                                         & \\multicolumn{3}{l}{200}                                         \\\\ \\cline{2-13} \n%                    & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{gMem.} & Para. & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{gMem.} & Para. & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{gMem.} & Para. & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{gMem.} & Para. \\\\ \\toprule[1pt]\n% \\emph{MoCo}               & \\multicolumn{1}{l|}{30.69}  & \\multicolumn{1}{l|}{4.27}  & 8.71  & \\multicolumn{1}{l|}{62.08}  & \\multicolumn{1}{l|}{5.19}  & 8.71  & \\multicolumn{1}{l|}{92.47}  & \\multicolumn{1}{l|}{6.37}  & 8.71  & \\multicolumn{1}{l|}{122.66} & \\multicolumn{1}{l|}{7.75}  & 8.71  \\\\ \\hline\n% \\emph{Toast}              & \\multicolumn{1}{l|}{28.43}  & \\multicolumn{1}{l|}{2.23}  & 1.81  & \\multicolumn{1}{l|}{56.87}  & \\multicolumn{1}{l|}{2.63}  & 1.81  & \\multicolumn{1}{l|}{93.30}  & \\multicolumn{1}{l|}{2.95}  & 1.81  & \\multicolumn{1}{l|}{113.74} & \\multicolumn{1}{l|}{3.38}  & 1.81  \\\\ \\hline\n% \\emph{t2vec}              & \\multicolumn{1}{l|}{5.79}   & \\multicolumn{1}{l|}{3.01}  & 1.69  & \\multicolumn{1}{l|}{11.58}  & \\multicolumn{1}{l|}{3.15}  & 1.69  & \\multicolumn{1}{l|}{17.48}  & \\multicolumn{1}{l|}{4.15}  & 1.69  & \\multicolumn{1}{l|}{23.16}  & \\multicolumn{1}{l|}{4.75}  & 1.69  \\\\ \\hline\n% \\emph{NeuTraj}            & \\multicolumn{1}{l|}{8.69}   & \\multicolumn{1}{l|}{3.29}  & 1.97  & \\multicolumn{1}{l|}{17.37}  & \\multicolumn{1}{l|}{3.98}  & 1.97  & \\multicolumn{1}{l|}{23.06}  & \\multicolumn{1}{l|}{4.46}  & 1.97  & \\multicolumn{1}{l|}{34.74}  & \\multicolumn{1}{l|}{5.54}  & 1.97  \\\\ \\hline\n% \\emph{PIM}                & \\multicolumn{1}{l|}{10.62}  & \\multicolumn{1}{l|}{2.02}  & 0.66  & \\multicolumn{1}{l|}{21.23}  & \\multicolumn{1}{l|}{2.28}  & 0.66  & \\multicolumn{1}{l|}{31.85}  & \\multicolumn{1}{l|}{2.55}  & 0.66  & \\multicolumn{1}{l|}{42.47}  & \\multicolumn{1}{l|}{2.80}  & 0.66  \\\\ \\hline\n% \\emph{HMTRL}              & \\multicolumn{1}{l|}{1.66}   & \\multicolumn{1}{l|}{2.58}  & 0.53  & \\multicolumn{1}{l|}{3.40}   & \\multicolumn{1}{l|}{2.63}  & 0.53  & \\multicolumn{1}{l|}{4.51}   & \\multicolumn{1}{l|}{2.72}  & 0.53  & \\multicolumn{1}{l|}{6.80}   & \\multicolumn{1}{l|}{2.85}  & 0.53  \\\\ \\hline\n% \\emph{PathRank}           & \\multicolumn{1}{l|}{1.56}   & \\multicolumn{1}{l|}{2.03}  & 0.49  & \\multicolumn{1}{l|}{3.20}   & \\multicolumn{1}{l|}{3.12}  & 0.49  & \\multicolumn{1}{l|}{4.78}   & \\multicolumn{1}{l|}{3.15}  & 0.49  & \\multicolumn{1}{l|}{6.37}   & \\multicolumn{1}{l|}{3.47}  & 0.49  \\\\ \\hline\n% \\emph{LightPath-Sup}      & \\multicolumn{1}{l|}{6.16}   & \\multicolumn{1}{l|}{3.67}  & 2.59  & \\multicolumn{1}{l|}{12.00}  & \\multicolumn{1}{l|}{4.99}  & 2.59  & \\multicolumn{1}{l|}{17.84}  & \\multicolumn{1}{l|}{5.27}  & 2.59  & \\multicolumn{1}{l|}{23.68}  & \\multicolumn{1}{l|}{5.57}  & 2.59  \\\\ \\hline\n% \\emph{LightPath}          & \\multicolumn{1}{l|}{3.18}   & \\multicolumn{1}{l|}{1.33}  & 1.57  & \\multicolumn{1}{l|}{6.23}   & \\multicolumn{1}{l|}{1.43}  & 1.57  & \\multicolumn{1}{l|}{9.27}   & \\multicolumn{1}{l|}{1.52}  & 1.57  & \\multicolumn{1}{l|}{12.31}  & \\multicolumn{1}{l|}{1.61}  & 1.57  \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:mscnn}\n% \n% \\end{table*}\n\n\n\\textbf{b)} \\textit{Effect of KD, global KD loss, local KD loss}: We further study the effect of global-local knowledge distillation. We compared our framework with three variants: 1) \\textit{w/o KD}, which denotes the performance of the teacher model; 2) \\textit{w/o global KD loss}, which removes global loss from global-local knowledge distillation; and 3) \\textit{w/o local KD loss}, which removes local loss from global-local knowledge distillation. As shown in Table~\\ref{tab:glkd}, compared with \\textit{KD}, \\emph{LightPath} achieves a better performance, which verifies that the teacher model can improve the performance of the student model. Both global and local loss can improve the performance of the learned path representation of the student model. In specific, global loss makes more contributions to the learned path representations. As a result, removing global loss degrades performance significantly.\n\n\n% \\begin{figure}[t]\n%      \\centering\n%      \\begin{subfigure}[b]{0.23\\textwidth}\n%          \\centering\n%          \\includegraphics[width=\\textwidth]{./figures/fig10a.pdf}\n%          \\caption{Travel Time (Aalborg)}\n%          \\label{fig:subfig:tteaal}\n%      \\end{subfigure}\n%      \\hfill\n%      \\begin{subfigure}[b]{0.23\\textwidth}\n%          \\centering\n%          \\includegraphics[width=\\textwidth]{./figures/fig10b.pdf}\n%          \\caption{Path Ranking (Aalborg)}\n%          \\label{fig:subfig:pr}\n%      \\end{subfigure}\n     \n%      \\begin{subfigure}[b]{0.23\\textwidth}\n%          \\centering\n%          \\includegraphics[width=\\textwidth]{./figures/fig10c.pdf}\n%          \\caption{Travel Time (Chengdu)}\n%          \\label{fig:subfig:ttecd}\n%      \\end{subfigure}\n%      \\hfill\n%      \\begin{subfigure}[b]{0.23\\textwidth}\n%          \\centering\n%          \\includegraphics[width=\\textwidth]{./figures/fig10d.pdf}\n%          \\caption{Path Ranking (Chengdu)}\n%          \\label{fig:subfig:prcd}\n%      \\end{subfigure}\n     \n%         \\caption{Momentum v.s. Reduction Ratio $\\gamma$.}\n%         \\label{fig:mrr}\n% \n% \\end{figure}\n\n% \\begin{table}[t]\n% \\caption{Effect of Reduction Ratio $\\gamma$}\n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{}                                                  & \\multicolumn{6}{l}{\\textbf{City A}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% 0.1                                                 & \\multicolumn{1}{l|}{94.15}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.54}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.67}         &0.70          \\\\ \\hline\n% 0.3                                                 & \\multicolumn{1}{l|}{93.96}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.11}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.67}         &0.71          \\\\ \\hline\n% 0.5                                                 & \\multicolumn{1}{l|}{90.75}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.10}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.70}         &0.74          \\\\ \\hline\n% 0.7                                                 & \\multicolumn{1}{l|}{88.08}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.46}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.69}         &0.73          \\\\ \\hline\n% % 0.85                                                 & \\multicolumn{1}{l|}{87.19}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.41}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.69}         &0.71  \\\\ \\hline\n% \\rowcolor{Gray}\n% 0.9                                &                      \\multicolumn{1}{l|}{86.78}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.39}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.71}         &0.75 \\\\ \\toprule[2pt]        \n% % 0.95                                                 & \\multicolumn{1}{l|}{88.80}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.53}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.67}         &0.71 \\\\ \\hline\n% % AllRec.                                                 & \\multicolumn{1}{l|}{85.76}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.12}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.73}         &0.77 \\\\  \\hline\n\n% % NoRec.                                                 & \\multicolumn{1}{l|}{103.05}        & \\multicolumn{1}{l|}{0.14}          & \\multicolumn{1}{l|}{15.76}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.65}         &0.69 \\\\ \\toprule[2pt]\n% % \\multirow{3}{*}{}                                                  & \\multicolumn{6}{l}{\\textbf{City B}}                                                                                                                                                              \\\\ \\cline{2-7} \n% %                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n% %                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% % 0.1                                                 & \\multicolumn{1}{l|}{220.84}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.78}         & \\multicolumn{1}{l|}{0.10}         & \\multicolumn{1}{l|}{0.69}         &0.73          \\\\ \\hline\n% % 0.3                                                 & \\multicolumn{1}{l|}{218.69}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.42}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.76}         &0.78          \\\\ \\hline\n% % 0.5                                                 & \\multicolumn{1}{l|}{213.87}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.08}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.81}         &0.84          \\\\ \\hline\n% % 0.7                                                 & \\multicolumn{1}{l|}{213.23}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.93}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.84}         &0.85          \\\\ \\hline\n% % 0.85                                                 & \\multicolumn{1}{l|}{}        & \\multicolumn{1}{l|}{}          & \\multicolumn{1}{l|}{}         & \\multicolumn{1}{l|}{}         & \\multicolumn{1}{l|}{}         &          \\\\ \\hline\n% % \\rowcolor{Gray}\n% % 0.9                                                 & \\multicolumn{1}{l|}{212.61}        & \\multicolumn{1}{l|}{0.18}          & \\multicolumn{1}{l|}{20.75}         & \\multicolumn{1}{l|}{0.07}         & \\multicolumn{1}{l|}{0.87}         &0.88  \\\\ \\hline\n% % 0.95                                                 & \\multicolumn{1}{l|}{}        & \\multicolumn{1}{l|}{}          & \\multicolumn{1}{l|}{}         & \\multicolumn{1}{l|}{}         & \\multicolumn{1}{l|}{}         &    \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:gamma}\n% \n% \\end{table}\n% \\begin{table}[t]\n% \\caption{Effect of Reduction Ratio $\\gamma$}\n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{$\\gamma$}}                                                  & \\multicolumn{6}{l}{\\textbf{Aalborg}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% % \\rowcolor{Gray}\n% 0.1                                                 & \\multicolumn{1}{l|}{82.79}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{11.95}         & \\multicolumn{1}{l|}{0.12}         & \\multicolumn{1}{l|}{0.74}         &0.77          \\\\ \\hline\n% 0.3                                                 & \\multicolumn{1}{l|}{84.75}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.14}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.73}         &0.77          \\\\ \\hline\n% 0.5                                                 & \\multicolumn{1}{l|}{84.81}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{11.86}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.72}         &0.76          \\\\ \\hline\n% 0.7                                                 & \\multicolumn{1}{l|}{85.91}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.49}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.71}         &0.75          \\\\ \\hline\n% % 0.85                                                 & \\multicolumn{1}{l|}{87.19}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.41}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.69}         &0.71  \\\\ \\hline\n\n% 0.9                                &                      \\multicolumn{1}{l|}{85.76}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.12}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.73}         &0.77 \\\\ \\toprule[2pt]        \n% \\end{tabular}\n% \\label{tab:gamma}\n%\n% \\end{table}\n\n\n"
                    },
                    "subsubsection 6.2.4": {
                        "name": "Parameter Sensitivity Analysis",
                        "content": " We proceed to study three important hyper-parameters, including 1) model scalability w.r.t. reduction ratio and path length, 2) Effect of Reduction Ratio $\\gamma$, 3) the parameter of temperature for global-local knowledge distillation, and 4) effect of balancing factor $\\alpha$.\n\n\n\n% \\begin{table}[t]\n% \\caption{Effect of Temperature $t$ in KD}\n% % \n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{$t$}}                                                  & \\multicolumn{6}{l}{\\textbf{Aalborg}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% 1                                                 & \\multicolumn{1}{l|}{92.01}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.30}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.65}         &0.69          \\\\ \\hline\n% 3                                                 & \\multicolumn{1}{l|}{94.11}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.54}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.68}         &0.72          \\\\ \\hline\n% 5                                                 & \\multicolumn{1}{l|}{90.39}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.85}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.66}         &0.70          \\\\ \\hline\n% 7                                                 & \\multicolumn{1}{l|}{89.64}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.76}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.70}         &0.74          \\\\ \\hline\n% % \\rowcolor{Gray}\n% % 9                                                 & \\multicolumn{1}{l|}{86.78}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.39}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.71}         &0.75          \\\\ \\hline\n% 9\n\n% &                      \\multicolumn{1}{l|}{\\textbf{85.76}}        & \\multicolumn{1}{l|}{\\textbf{0.11}}          & \\multicolumn{1}{l|}{\\textbf{12.12}}         & \\multicolumn{1}{l|}{\\textbf{0.13}}         & \\multicolumn{1}{l|}{\\textbf{0.73}}         &\\textbf{0.77}          \\\\ \\hline\n% 11                                                       & \\multicolumn{1}{l|}{87.15}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.43}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.70}         & 0.74         \\\\ \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{$t$}}                                                  & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% 1                                                 & \\multicolumn{1}{l|}{225.70}        & \\multicolumn{1}{l|}{0.20}          & \\multicolumn{1}{l|}{22.02}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.79}         &0.82          \\\\ \\hline\n% 3                                                 & \\multicolumn{1}{l|}{217.07}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.77}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.81}         &0.84          \\\\ \\hline\n% 5                                                 & \\multicolumn{1}{l|}{216.93}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.24}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.83}         &0.86          \\\\ \\hline\n% 7                                                 & \\multicolumn{1}{l|}{214.88}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.98}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.86}         &0.87          \\\\ \\hline\n% % \\rowcolor{Gray}\n% 9                                                 & \\multicolumn{1}{l|}{\\textbf{212.61}}        & \\multicolumn{1}{l|}{\\textbf{0.18}}          & \\multicolumn{1}{l|}{\\textbf{20.75}}         & \\multicolumn{1}{l|}{\\textbf{0.07}}         & \\multicolumn{1}{l|}{\\textbf{0.87}}         &\\textbf{0.88}          \\\\ \\hline\n% 11                                                      & \\multicolumn{1}{l|}{214.17}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.00}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.83}         & 0.85         \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:temperature}\n% \n% \\end{table}\n\n\\paragraph{Model Scalability} In the sequel, we explore the model scalability in terms of reduction ratio and path length based on the synthetic dataset. Table~\\ref{tab:msca} depicts the results for both \\emph{LightPath} and its teacher model, with varying $\\gamma = 0,0.1,0.3,0.5,0.7,0.9$. $\\gamma=0$ denotes we do not conduct sparsity operation for the input path, i.e., using a classic Transformer based encoder. We can observe that the GFLOPs and gMem. (GiB) decrease with the increase in the reduction ratio. It is because the higher value of $\\gamma$ is, the more edges we can remove. \n% In contrast, the GFLOPs and MAC increase with the rise of path length. The longer the path length, the more edges contained in a path. \nSecond, \\emph{LightPath} has significantly reduced model complexity, w.r.t., GFLOPs and gMem.. For example, we can reduce the training GFLOPs by $2.54 \\times$ for the \\emph{LightPath} by increasing the reduction ratio $\\gamma$ from $0$ to $0.9$ in terms of path length 200. \n% under reduction ratio $=0$ and $=0.9$ with the path length 200. \nMoreover, \\emph{LightPath} also shows better performance (i.e., GFLOPs and gMem.) over teacher model, e.g., $1.79 \\times$ GFLOPs speedup with reduction ratio $\\gamma=0.9$. \n%\nThird, the parameters (Para. (Millions)) of teacher model is at least $3.5 \\times$ of \\emph{LightPath}, which implies the effectiveness of our proposed framework.\n%\nOverall, \\emph{LightPath} shows potential of scalability to support path representation learning for long paths. \n\n% \\noindent\n% \\paragraph{Model Scalability Comparison} In this section, we further explore the scalability performance (i.e., GFLOPs, gMem.(GiB), and Para. (Millions)) of \\emph{LightPath} compared with different baselines with respect to different path lengths. We do not consider \\emph{Node2vec} since it is graph representation learning based approach.\n% % We select \\emph{PIM} since (1) it achieves the better performance compared with other unsupervised baselines (cf. Table~\\ref{tab:ttpr}); (2) it is a InforNCE based contrastive path representation learning method. In contrast, we choose \\emph{Toast} since it is Transformer-based method. \n% Table~\\ref{tab:mscnn} reports the experiment results. We observe that, (1) \\emph{LightPath} has significant computation performance improvement over unsupervised baselines, where the GFLOPs becomes much larger than \\emph{LigthPath} when path length increases, e.g., at least $9 \\times$ compared with \\emph{Toast}. In addition, unsupervised baselines also need larger memory compared with \\emph{LightPath} when path length gets long, such as 3.38 (\\emph{Toast}) v.s. 1.65 (\\emph{LightPath}) when path length is $200$;\n% %\n% (2) although the \\emph{LightPath}'s model parameters is at least $2 \\times$ compared with \\emph{PIM}, the GFLOPs and gMem. of \\emph{LightPath} is much less than \\emph{PIM}. In particular, \\emph{LightPath} has much less GFLOPs and gMem. compared with \\emph{PIM} when path length is $200$. i.e., 42.47 v.s. 12.31 and 2.80 v.s. 1.65; \n% %\n% (3) although supervised \\emph{HMTRL} and \\emph{PathRank} achieves the better GFLOPs, gMem., and Para. than \\emph{LightPath}. However, these two methods learn a task-specific path representation learning, which has a poor generalization on different downstream tasks. In contrast, \\emph{LightPath} achieves the better GFLOPs, gMem., and Para. than \\emph{LightPath-Sup} since \\emph{LightPath-Sup} has multiple transformer layers and attention heads. \n% % \\emph{LightPath} has significant computation performance improvement over \\emph{Toast}, where the GFLOPs becomes much larger than \\emph{LigthPath} when path length increases, i.e., at least $9 \\times$. In addition, \\emph{Toast} also needs larger memory compared with \\emph{LightPath} when path length gets long, i.e. 3.38 v.s. 1.65 when path length is $200$. \n% %\n% Overall, our \\emph{LightPath} shows good potential scalability to support deploying it on resource-limited environments. \n\n\n\n%\\noindent\n%\\paragraph{Effect of Reduction Ratio $\\gamma$} We further investigate the effect of the $\\gamma$ in our sparse auto-encoder on City A, where $\\gamma \\in [0.1, 0.3, 0.5, 0.7, 0.9]$. Table~\\ref{tab:gamma} report the results, we observe that (1) the optimal reduction ratios are surprisingly high. The $\\gamma=0.1$ is good for City A on both downstream tasks. (2) Higher reduction ratio promote the good scalability, w.r.t. path length, for our dual sparse auto-encoder.\n\n\\noindent\n\\paragraph{Effect of Reduction Ratio $\\gamma$} \nTo study the impact of reduction ratio $\\gamma$ in the final performance, we conduct an experiment by varying the $\\gamma$ from 0.1 to 0.9 on Aalborg, which is shown in Table~\\ref{tab:gamma}. We can observe that the overall performance in both downstream tasks degrades a little when $\\gamma$ increases, which is reasonable as the the model has more input information. However, we can also observe the performance differences are not so significant, which suggests the effectiveness of our proposed framework. Even when a high reduction ratio is applied, the performance does not does not go down too much. Therefore, our proposed method can achieve good scalability while ensuring accuracy. \n%Wthat te ve can also observe that there is no clear differences among different selections of $\\gamma$, i.e., MARE and MAPE in Travel Time Estimation, and Path Ranking in $\\tau$ and $\\rho$. This phenomenon can be accounted to the fact that the training objectives we selected in both tasks are MAE, so the performance on MAE have a clear trend while other two do not have. In Table~\\ref{tab:msca}, we can observe that $\\gamma$ has the best performance in terms of GLOPs and gMem., which is the lightest. Therefore, we select $\\gamma=0.9$ as the optimal reduction ratio in our paper. \n\n%We further investigate the effect of the $\\gamma$ in our sparse auto-encoder on City A, where $\\gamma \\in [0.1, 0.3, 0.5, 0.7, 0.9]$. Table~\\ref{tab:gamma} report the results, we observe that (1) the optimal reduction ratios are surprisingly high. The $\\gamma=0.1$ is good for City A on both downstream tasks. (2) Higher reduction ratio promote the good scalability, w.r.t. path length, for our dual sparse auto-encoder.\n\n% \\emph{LightPath} achieves the better performance on both tasks with the increase of the reduction ratio $\\gamma$, especially for reduction ratio $\\gamma=0.9$, w \n% \\textit{Effect of $\\gamma$ and $m$}. We further investigate the effect of the  $\\gamma$ in our sparse auto-encoder and $m$ in Eq.~\\ref{eq:momentum} on both datsets, where $\\gamma \\in [0.1, 0.3, 0.5, 0.7, 0.9]$ and $m \\in [0, 0.8, 0.9, 0.99, 0.999, 1]$. In Figure~\\ref{fig:mrr}, we can observe that \\emph{LightPath} achieves better performance on both tasks with the increase of the reduction ratio $\\gamma$ with respect to the fixed $m$, especially for reduction ratio $=0.9$. On the other hand, for a certain reduction ratio $\\gamma$, we find that $m=0$ and $m=1$ usually gives a poor performance on both tasks, in contrast, $m=0.99$ achieves the best performance, which is consistent with hypothesis proposed in BYOL~\\cite{DBLP:conf/nips/GrillSATRBDPGAP20} and SimSiam~\\cite{DBLP:conf/cvpr/ChenH21}.\n% \\begin{table}[t]\n% \\footnotesize\n% \\caption{Model Scallability Comparison}\n% % \n% \\begin{tabular}{l|l|l|l}\n% \\toprule[2pt]\n% \\multirow{2}{*}{$N$} & \\textbf{LightPath} & \\textbf{PIM}     & \\textbf{Toast}   \\\\ \\cline{2-4} \n%                   & GFLOPs/gMem./Para.   & GFLOPs/gMem./Para. & GFLOPs/gMem./Para. \\\\ \\toprule[1pt]\n%  50              & 3.18/1.33/1.57     & 10.62/2.02/0.66  & 28.43/2.23/1.81  \\\\ \\hline\n%  100             & 6.23/1.43/1.57     & 21.23/2.28/0.66  & 56.87/2.63/1.81  \\\\ \\hline\n%  150             & 9.27/1.52/1.57     & 31.85/2.55/0.66  & 93.30/2.95/1.81  \\\\ \\hline\n%  200             & 12.31/1.65/1.57    & 42.47/2.80/0.66  & 113.74/3.38/1.81 \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:msc}\n% \n% \\end{table}\n\n%% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n% \\begin{table*}[]\n% \\centering\n\n% \\caption{Model Scallability Comparison}\n% \\begin{tabular}{l|l|l|l|l}\n% \\toprule[2pt]\n% \\multirow{2}{*}{N} & 50                & 100                & 150                & 200                \\\\ \\cline{2-5} \n%                   & GFLOPs/gMem./Para. & GFLOPs/gMem./Para. & GFLOPs/gMem./Para. & GFLOPs/gMem./Para. \\\\ \\toprule[1pt]\n% \\emph{MoCo}              & 30.69/4.27/8.71    & 62.08/5.19/8.71    & 92.47/6.37/8.71    & 122.66/7.75/8.71   \\\\ \\hline\n% \\emph{Toast}             & 28.43/2.23/1.81    & 56.87/2.63/1.81    & 93.30/2.95/1.81    & 113.74/3.38/1.81   \\\\ \\hline\n% \\emph{t2vec}             & 5.79/3.01/1.69     & 11.58/3.15/1.69    & 17.48/4.15/1.69    & 23.16/4.75/1.69    \\\\ \\hline\n% \\emph{NeuTraj}           & 8.69/3.29/1.97     & 17.37/3.98/1.97    & 23.06/4.46/1.97    & 34.74/5.54/1.97    \\\\ \\hline\n% \\emph{PIM}               & 10.62/2.02/0.66    & 21.23/2.28/0.66    & 31.85/2.55/0.66    & 42.47/2.80/0.66    \\\\ \\hline\n% \\emph{HMTRL}             & 1.66/2.58/0.53     & 3.40/2.63/0.53        &4.51/2.72/0.53   & 6.80/2.85/0.53                    \\\\ \\hline\n% \\emph{PathRank}          & 1.56/2.03/0.49     &3.20/3.12/0.49   &4.78/3.15/0.49    &6.37/3.47/0.49                    \\\\ \\hline\n% \\emph{LightPath-Sup}     & 6.16/3.67/2.59     & 12.00/4.99/2.59                   &17.84/5.27/2.59                    &23.68/5.57/2.59                    \\\\ \\hline\n% \\emph{LightPath}         & 3.18/1.33/1.57     & 6.23/1.43/1.57     & 9.27/1.52/1.57     & 12.31/1.65/1.57    \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:mscnn}\n%\n% \\end{table*}\n\n\n% \\begin{table}[t]\n% \\caption{Effect of Reduction Ratio $\\gamma$}\n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{$\\gamma$}}                                                  & \\multicolumn{6}{l}{\\textbf{Aalborg}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% % \\rowcolor{Gray}\n% 0.1                                                 & \\multicolumn{1}{l|}{82.79}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{11.95}         & \\multicolumn{1}{l|}{0.12}         & \\multicolumn{1}{l|}{0.74}         &0.77          \\\\ \\hline\n% 0.3                                                 & \\multicolumn{1}{l|}{84.75}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.14}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.73}         &0.77          \\\\ \\hline\n% 0.5                                                 & \\multicolumn{1}{l|}{84.81}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{11.86}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.72}         &0.76          \\\\ \\hline\n% 0.7                                                 & \\multicolumn{1}{l|}{85.91}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.49}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.71}         &0.75          \\\\ \\hline\n% % 0.85                                                 & \\multicolumn{1}{l|}{87.19}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.41}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.69}         &0.71  \\\\ \\hline\n\n% 0.9                                &                      \\multicolumn{1}{l|}{85.76}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.12}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.73}         &0.77 \\\\ \\toprule[2pt]        \n% \\end{tabular}\n% \\label{tab:gamma}\n%\n% \\end{table}\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n\n\n\n% \\begin{table}[t]\n% \\centering\n% \\caption{Effect of Temperature $t$ in KD}\n% % \\vspace{-5pt}\n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{$t$}}                                                  & \\multicolumn{6}{l}{\\textbf{Aalborg}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% 1                                                 & \\multicolumn{1}{l|}{92.01}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.30}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.65}         &0.69          \\\\ \\hline\n% 3                                                 & \\multicolumn{1}{l|}{94.11}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.54}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.68}         &0.72          \\\\ \\hline\n% 5                                                 & \\multicolumn{1}{l|}{90.39}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.85}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.66}         &0.70          \\\\ \\hline\n% 7                                                 & \\multicolumn{1}{l|}{89.64}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.76}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.70}         &0.74          \\\\ \\hline\n% % \\rowcolor{Gray}\n% % 9                                                 & \\multicolumn{1}{l|}{86.78}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.39}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.71}         &0.75          \\\\ \\hline\n% 9\n\n% &                      \\multicolumn{1}{l|}{\\textbf{85.76}}        & \\multicolumn{1}{l|}{\\textbf{0.11}}          & \\multicolumn{1}{l|}{\\textbf{12.12}}         & \\multicolumn{1}{l|}{\\textbf{0.13}}         & \\multicolumn{1}{l|}{\\textbf{0.73}}         &\\textbf{0.77}          \\\\ \\hline\n% 11                                                       & \\multicolumn{1}{l|}{87.15}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.43}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.70}         & 0.74         \\\\ \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{$t$}}                                                  & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% 1                                                 & \\multicolumn{1}{l|}{225.70}        & \\multicolumn{1}{l|}{0.20}          & \\multicolumn{1}{l|}{22.02}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.79}         &0.82          \\\\ \\hline\n% 3                                                 & \\multicolumn{1}{l|}{217.07}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.77}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.81}         &0.84          \\\\ \\hline\n% 5                                                 & \\multicolumn{1}{l|}{216.93}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.24}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.83}         &0.86          \\\\ \\hline\n% 7                                                 & \\multicolumn{1}{l|}{214.88}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.98}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.86}         &0.87          \\\\ \\hline\n% % \\rowcolor{Gray}\n% 9                                                 & \\multicolumn{1}{l|}{\\textbf{212.61}}        & \\multicolumn{1}{l|}{\\textbf{0.18}}          & \\multicolumn{1}{l|}{\\textbf{20.75}}         & \\multicolumn{1}{l|}{\\textbf{0.07}}         & \\multicolumn{1}{l|}{\\textbf{0.87}}         &\\textbf{0.88}          \\\\ \\hline\n% 11                                                      & \\multicolumn{1}{l|}{214.17}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.00}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.83}         & 0.85         \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:temperature}\n\n% \\end{table}\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n\n\n% \\begin{table}[t]\n% \\centering\n% \\caption{Effect of Balancing Factor $\\alpha$}\n% % \\vspace{-5pt}\n% \\begin{tabular}{l|llllll}\n% \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{$\\alpha$}}                                                  & \\multicolumn{6}{l}{\\textbf{Aalborg}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% 0                                                  & \\multicolumn{1}{l|}{90.24}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.78}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.69}         &0.73          \\\\ \\hline\n% 0.2                                                  & \\multicolumn{1}{l|}{89.35}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.85}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.69}         &0.73          \\\\ \\hline\n% 0.4                                                  & \\multicolumn{1}{l|}{91.57}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.17}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.69}         &0.73          \\\\ \\hline\n\n% % \\rowcolor{Gray}\n% 0.6 \n% &                      \\multicolumn{1}{l|}{\\textbf{85.76}}        & \\multicolumn{1}{l|}{\\textbf{0.11}}          & \\multicolumn{1}{l|}{\\textbf{12.12}}         & \\multicolumn{1}{l|}{\\textbf{0.13}}         & \\multicolumn{1}{l|}{\\textbf{0.73}}         &\\textbf{0.77}         \\\\ \\hline\n% % & \\multicolumn{1}{l|}{\\textbf{86.78}}        & \\multicolumn{1}{l|}{\\textbf{0.11}}          & \\multicolumn{1}{l|}{\\textbf{12.39}}         & \\multicolumn{1}{l|}{\\textbf{0.14}}         & \\multicolumn{1}{l|}{\\textbf{0.71}}         &\\textbf{0.75}          \\\\ \\hline\n% 0.8                                                  & \\multicolumn{1}{l|}{87.44}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.76}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.70}         &0.75          \\\\ \\hline\n% 1                                                  & \\multicolumn{1}{l|}{89.23}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.78}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.69}         &0.73          \\\\ \\toprule[2pt]\n% \\multirow{3}{*}{\\textbf{Method}}                                                  & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                              \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n%                                                                                   & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% 0                                                  & \\multicolumn{1}{l|}{220.32}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.52}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.79}         &0.81          \\\\ \\hline\n% 0.2                                                  & \\multicolumn{1}{l|}{217.10}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.34}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.78}         &0.80          \\\\ \\hline\n% 0.4                                                  & \\multicolumn{1}{l|}{217.33}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.21}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.85}         &0.87          \\\\ \\hline\n\n% % \\rowcolor{Gray}\n% 0.6                                                  & \\multicolumn{1}{l|}{\\textbf{212.61}}        & \\multicolumn{1}{l|}{\\textbf{0.18}}          & \\multicolumn{1}{l|}{\\textbf{20.75}}         & \\multicolumn{1}{l|}{\\textbf{0.07}}         & \\multicolumn{1}{l|}{\\textbf{0.87}}         &\\textbf{0.88}          \\\\ \\hline\n% 0.8                                                  & \\multicolumn{1}{l|}{214.34}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.93}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.84}         &0.86          \\\\ \\hline\n% 1                                                  & \\multicolumn{1}{l|}{215.03}        & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.02}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.82}         &0.84          \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\label{tab:glkdbf}\n\n% \\end{table}\n\n\\noindent\n\\paragraph{Effect of Temperature $t$ of Knowledge Distillation} To study the effect of the temperature $t$, we conduct a parameter study on both datasets, which is reported in Table~\\ref{tab:temperature}. We can observe that the performance of \\emph{LightPath} varies with different temperatures. It can be figured out that the best temperature $t$ is $9$, which indicates warm temperature can mitigate the peakiness of the teacher model and results in better performance. \n\n\n\n\n\n\n\\noindent\n\\paragraph{Effect of Balancing Factor $\\alpha$} To study the effect of the balancing factor of global-local knowledge distillation, we conduct a parameter study on both datasets. Based on the results reported in Table~\\ref{tab:glkdbf}, we observe that the performance of our model changes when varying $\\alpha$. We can observe that the optimal $\\alpha$ is 0.6, which means that global and local knowledge distillation loss can contribute to the \\emph{LightPath}'s performance. When $\\alpha=0$, the global knowledge distillation loss is ignored, which yields poor performance. When $\\alpha=1.0$, the local knowledge distillation loss is ignored, and the performance also performs poorly. This confirms our conjecture that the\ntwo proposed global-local knowledge distillation losses can regularize each other and\nachieve better results than only optimizing one of them (i.e., $\\alpha=0.0$ or $\\alpha=1.0$).\n\n\n\t\n"
                    }
                },
                "subsection 6.3": {
                    "name": "Comparison with Whole Path Input ",
                    "content": "\n\t\nCompared with whole path input, we consider a variant “LightPath w/o RR” where only reconstruction loss is used and the relational reasoning (RR) loss is disabled. In this setting, we can see in the table \\ref{tab:fullpath} that LightPath w/o RR ($\\gamma=0$), i.e., using whole paths, outperforms LightPath w/o RR ($\\gamma=0.1$), i.e., using partial paths. This means that, when only using the reconstruction loss, using whole paths are indeed better than using partial paths. However, when using both losses, LightPath ($\\gamma=0.1$) outperforms LightPath w/o RR ($\\gamma=0$). This demonstrates that the relational reasoning loss, which employs partial paths to create different views, is indeed effective.\\emph{LigthPath}, \n\n"
                },
                "subsection 6.4": {
                    "name": "Comparison with Fixed Interval Strategy",
                    "content": "\n\t\nWe then conducted additional experiments where we removed edges at fixed intervals strategy. Specifically, we set the fixed interval to 10 and removed $n$ edges out of every 10 (for instance, we deleted 1 edge out of every 10 edges, i.e., corresponding to a removal ratio of $\\gamma = 0.1$). The results on travel time estimation in Aalborg shown in Table \\ref{tab:fixstrategy}, which indicate that the fixed interval edge removal strategy (ref. as to first three rows) achieves the worse performance compared with the random edge removal strategy (ref. as to last row).\n\t\n\t\n\t\n\t\n\n\t\n\n",
                    "subsubsection 6.4.1": {
                        "name": "Model Efficiency",
                        "content": " We finally evaluate the model efficiency, including training and inference phases. Figure~\\ref{fig:mee} illustrates the corresponding results. The first observation is that \\emph{LightPath} outperforms \\emph{PIM} and \\emph{Toast} in both training and inference phases. In the training phase, \\emph{LightPath} is more than $3\\times$ faster than \\emph{PIM} and almost $5\\times$ faster than \\emph{Toast} when path length is 200. In the testing phase, we measure the running time for each path sample. As observed, \\emph{LightPath} achieves up to at least 100\\% and almost 200\\% performance improvement compared with \\emph{PIM} and \\emph{Toast} when path length is 200.  \n\n\n"
                    }
                }
            },
            "section 7": {
                "name": "Related Work",
                "content": "\n",
                "subsection 7.1": {
                    "name": "Path Representation Learning",
                    "content": "\nPath Representation Learning (PRL) aims to learn effective and informative path representations in road network that can be applied to various downstream applications, i.e., travel cost estimation, and path ranking. Existing PRL studies can be categorised as supervised learning (SL) based~\\cite{DBLP:conf/icde/Yang020,yang2020context,DBLP:journals/pvldb/0026HFZL020}, unsupervised learning (UL) based~\\cite{DBLP:conf/ijcai/YangGHT021}, and weakly supervised learning (WSL) based~\\cite{DBLP:journals/corr/abs-2203-16110} approaches. \n%\nSL based methods aim at learning a task-specific path representation with the availability of large amounts of labeled training\ndata~\\cite{DBLP:journals/pvldb/0026HFZL020,DBLP:conf/icde/Yang020,yang2020context}, which has a poor generality for other tasks. \n% \n%\nUL methods are to learn general path representation learning that does not need labeled training data and generalizes well to multiple downstream tasks~\\cite{DBLP:conf/ijcai/YangGHT021,DBLP:journals/corr/abs-2203-16110}. In contrast, WSL methods try to learn a generic temporal path representation by introducing meaningful weak labels, e.g., traffic congestion indices, that are easy and\ninexpensive to obtain, and are relevant to different tasks~\\cite{DBLP:journals/corr/abs-2203-16110}. \n%\nHowever, we aim to learn generic path representations instead of temporal path representations in this paper. Thus, we do not select WSL method as baseline method.\n%\nIn particular, these methods are computationally expensive and hard to deploy in resource-limited environments.\n% \n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n\n% \\begin{table*}[]\n\n% \\begin{tabular}{l|lll|lll|lll}\n% \\toprule[2pt]\n% \\multirow{2}{*}{} & \\multicolumn{3}{l|}{\\textbf{LightPath}}                                                  & \\multicolumn{3}{l|}{\\textbf{PIM}}                                                        & \\multicolumn{3}{l}{\\textbf{Toast}}                                                      \\\\ \\cline{2-10} \n%                   & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{MAC}  & Para.                 & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{MAC}  & Para.                 & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{MAC}  & Para.                 \\\\ \\toprule[1pt]\n% N=50              & \\multicolumn{1}{l|}{3.18}   & \\multicolumn{1}{l|}{1.33} & \\multirow{4}{*}{1.57} & \\multicolumn{1}{l|}{10.62}  & \\multicolumn{1}{l|}{2.02} & \\multirow{4}{*}{0.66} & \\multicolumn{1}{l|}{28.43} & \\multicolumn{1}{l|}{2.23} & \\multirow{4}{*}{1.81} \\\\ \\cline{1-3} \\cline{5-6} \\cline{8-9}\n% N=100             & \\multicolumn{1}{l|}{6.23}   & \\multicolumn{1}{l|}{1.43} &                       & \\multicolumn{1}{l|}{21.23}  & \\multicolumn{1}{l|}{2.28} &                       & \\multicolumn{1}{l|}{56.87}  & \\multicolumn{1}{l|}{2.63} &                       \\\\ \\cline{1-3} \\cline{5-6} \\cline{8-9}\n% N=150             & \\multicolumn{1}{l|}{9.27}   & \\multicolumn{1}{l|}{1.52} &                       & \\multicolumn{1}{l|}{31.85}  & \\multicolumn{1}{l|}{2.55} &                       & \\multicolumn{1}{l|}{93.30}  & \\multicolumn{1}{l|}{2.95} &                       \\\\ \\cline{1-3} \\cline{5-6} \\cline{8-9}\n% N=200             & \\multicolumn{1}{l|}{12.31}  & \\multicolumn{1}{l|}{1.65} &                       & \\multicolumn{1}{l|}{42.47}  & \\multicolumn{1}{l|}{2.80} &                       & \\multicolumn{1}{l|}{113.74} & \\multicolumn{1}{l|}{3.38} &                       \\\\ \\toprule[2pt]\n% \\end{tabular}\n% \\end{table*} \n\n% Please add the following required packages to your document preamble:\n% % \\usepackage{multirow}\n% \\begin{table}[]\n% \\begin{tabular}{|l|lll|}\n% \\hline\n% \\multirow{2}{*}{} & \\multicolumn{3}{l|}{\\textbf{LightPath}}                                                                                   \\\\ \\cline{2-4} \n%                   & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{MAC}  & \\begin{tabular}[c]{@{}l@{}}Parameters\\\\ (Millions)\\end{tabular} \\\\ \\hline\n% N=50              & \\multicolumn{1}{l|}{3.18}   & \\multicolumn{1}{l|}{1.33} & \\multirow{4}{*}{1.57}                                           \\\\ \\cline{1-3}\n% N=100             & \\multicolumn{1}{l|}{6.23}   & \\multicolumn{1}{l|}{1.43} &                                                                 \\\\ \\cline{1-3}\n% N=150             & \\multicolumn{1}{l|}{9.27}   & \\multicolumn{1}{l|}{1.52} &                                                                 \\\\ \\cline{1-3}\n% N=200             & \\multicolumn{1}{l|}{12.31}  & \\multicolumn{1}{l|}{1.65} &                                                                 \\\\ \\hline\n% \\multirow{2}{*}{} & \\multicolumn{3}{l|}{\\textbf{PIM}}                                                                                         \\\\ \\cline{2-4} \n%                   & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{MAC}  & \\begin{tabular}[c]{@{}l@{}}Parameters\\\\ (Millions)\\end{tabular} \\\\ \\hline\n% N=50              & \\multicolumn{1}{l|}{10.62}  & \\multicolumn{1}{l|}{2.02} & \\multirow{4}{*}{0.66}                                           \\\\ \\cline{1-3}\n% N=100             & \\multicolumn{1}{l|}{21.23}  & \\multicolumn{1}{l|}{2.28} &                                                                 \\\\ \\cline{1-3}\n% N=150             & \\multicolumn{1}{l|}{31.85}  & \\multicolumn{1}{l|}{2.55} &                                                                 \\\\ \\cline{1-3}\n% N=200             & \\multicolumn{1}{l|}{42.47}  & \\multicolumn{1}{l|}{2.80} &                                                                 \\\\ \\hline\n% \\multirow{2}{*}{} & \\multicolumn{3}{l|}{\\textbf{Toast}}                                                                                       \\\\ \\cline{2-4} \n%                   & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{MAC}  & \\begin{tabular}[c]{@{}l@{}}Parameters\\\\ (Millions)\\end{tabular} \\\\ \\hline\n% N=50              & \\multicolumn{1}{l|}{28.43}  & \\multicolumn{1}{l|}{2.23} & \\multirow{4}{*}{1.81}                                           \\\\ \\cline{1-3}\n% N=100             & \\multicolumn{1}{l|}{56.87}  & \\multicolumn{1}{l|}{2.63} &                                                                 \\\\ \\cline{1-3}\n% N=150             & \\multicolumn{1}{l|}{93.30}  & \\multicolumn{1}{l|}{2.95} &                                                                 \\\\ \\cline{1-3}\n% N=200             & \\multicolumn{1}{l|}{113.74} & \\multicolumn{1}{l|}{3.38} &                                                                 \\\\ \\hline\n% \\end{tabular}\n% \\end{table}\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\n"
                },
                "subsection 7.2": {
                    "name": "Self-supervised Learning",
                    "content": "\n% Self-supervised learning (SSL) tries to learn informative representations without the availability of labels through well-defined pretext tasks. \nState-of-the-art self-supervised learning can be classified into contrastive learning-based and relation reasoning-based methods. Contrastive learning-based methods~\\cite{DBLP:journals/corr/abs-1807-03748,DBLP:journals/corr/abs-1808-06670,DBLP:conf/iclr/VelickovicFHLBH19,DBLP:conf/ijcai/YangGHT021,DBLP:conf/sigmod/BabaevOKIGNT22}, especially for InfoNCE loss-based, commonly generate different views of same input data through different augmentation strategies, and then discriminate positive and negative samples. However, these methods suffer from their quadratic complexity, w.r.t. the number of data samples, given that it needs a large number of negative samples to guarantee that the mutual information lower bound is tight enough~\\cite{DBLP:journals/corr/abs-1808-06670}. \n%\n%\nIn contrast, relation reasoning-based methods~\\cite{DBLP:conf/nips/PatacchiolaS20,DBLP:journals/corr/abs-2011-13548} aim to learn relation reasoning\nhead that discriminates how entities relate to themselves and other entities, which results in linear complexity. However, existing studies construct relation reasoning between different views from the same encoder, ignoring the effect of different views between different encoders, i.e., main encoder and auxiliary encoder in Siamese encoder architecture. \n\n\n"
                }
            },
            "section 8": {
                "name": "Conclusion",
                "content": "\n\nWe design a lightweight and scalable framework called \\emph{LightPath} for unsupervised path representation learning. In this framework, we first propose sparse auto-encoder that is able to reduce path length $N$ to $N^{\\prime}$, where $N$ is much larger than $N^{\\prime}$, which in turn reduces the computation complexity of the model. Then, we use path reconstruction decoder to reconstruct the input path to ensure no edges information missing. Next, we propose a novel self-supervised relational reasoning approach, which contains cross-network relational reasoning and cross-view relational reasoning loss, to enable efficient unsupervised training. After that, we introduce global-local knowledge distillation to further reduce the size of sparse path encoder and improve the performance. Finally, extensive experiments on two real-world datasets verify the efficiency, scalability, and effectiveness of \\emph{LightPath}.\n%\n% \n\n\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
            },
            "section 9": {
                "name": "Acknowledgments",
                "content": "\nThis work was supported in part by Independent Research Fund Denmark under agreements 8022-00246B and 8048-00038B, the VILLUM FONDEN under agreements 34328 and 40567.\n%Bibliography\n\n\\bibliographystyle{unsrt}  \n\\bibliography{references}  \n\n\n"
            }
        },
        "tables": {
            "tab:mp": "\\begin{table*}[t]\n\\centering\n\\caption{Model Parameter Size with Varying Encoder Layers}\n\n\\begin{tabular}{l|l|l|l|l}\n\\toprule[2pt]\nEncoder Layers L                                                  & 12     & 24      & 48     & 96      \\\\ \\toprule[1pt]\n\\begin{tabular}[c]{@{}l@{}}Parameters\\\\ (Millions)\\end{tabular} & 29.85 & 55.07 & 105.51 & 206.40 \\\\ \\toprule[2pt]\n\\end{tabular}\n\\label{tab:mp}\n\n\\end{table*}",
            "tab:ttpr": "\\begin{table*}[!htp]\n\t\\caption{{Overall Accuracy on Travel Time Estimation and Ranking Score Estimation}}\n\t\\centering\n\t\\small\n\t\n\t\\begin{tabular}{l|llllll|llllll}\n\t\t\\toprule[2pt]\n\t\t\\multirow{3}{*}{\\textbf{Method}} & \\multicolumn{6}{l|}{\\textbf{Aalborg}}                                                                                                                & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                \\\\ \\cline{2-13} \n\t\t& \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                 & \\multicolumn{3}{l|}{\\textbf{Path Ranking}}                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                 & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                   \\\\ \\cline{2-13} \n\t\t& \\multicolumn{1}{l|}{\\textbf{MAE}}    & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}}  & \\multicolumn{1}{l|}{\\textbf{$\\tau$}}  & \\textbf{$\\rho$}  & \\multicolumn{1}{l|}{\\textbf{MAE}}    & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}}  & \\multicolumn{1}{l|}{\\textbf{$\\tau$}}  & \\textbf{$\\rho$}  \\\\ \\toprule[1pt]\n\t\t\\emph{Node2vec}                & \\multicolumn{1}{l|}{154.07} & \\multicolumn{1}{l|}{0.20} & \\multicolumn{1}{l|}{25.22} & \\multicolumn{1}{l|}{0.24} & \\multicolumn{1}{l|}{0.59} & 0.64 & \\multicolumn{1}{l|}{267.28} & \\multicolumn{1}{l|}{0.23} & \\multicolumn{1}{l|}{26.30} & \\multicolumn{1}{l|}{0.15} & \\multicolumn{1}{l|}{0.74} & 0.77 \\\\ \\hline\n\t\t\\emph{MoCo}                    & \\multicolumn{1}{l|}{146.29} & \\multicolumn{1}{l|}{0.19} & \\multicolumn{1}{l|}{21.60} & \\multicolumn{1}{l|}{0.25} & \\multicolumn{1}{l|}{0.53} & 0.57 & \\multicolumn{1}{l|}{237.14} & \\multicolumn{1}{l|}{0.20} & \\multicolumn{1}{l|}{23.13} & \\multicolumn{1}{l|}{0.15} & \\multicolumn{1}{l|}{0.77} & 0.81 \\\\ \\hline\n\t\t\\emph{Toast}                   & \\multicolumn{1}{l|}{137.27} & \\multicolumn{1}{l|}{0.17} & \\multicolumn{1}{l|}{20.43} & \\multicolumn{1}{l|}{0.24} & \\multicolumn{1}{l|}{0.59} & 0.63 & \\multicolumn{1}{l|}{240.57} & \\multicolumn{1}{l|}{0.21} & \\multicolumn{1}{l|}{23.50} & \\multicolumn{1}{l|}{0.11} & \\multicolumn{1}{l|}{0.65} & 0.68 \\\\ \\hline\n\t\t\\emph{t2vec}                            & \\multicolumn{1}{l|}{147.24} & \\multicolumn{1}{l|}{0.19} & \\multicolumn{1}{l|}{22.13} & \\multicolumn{1}{l|}{0.25} & \\multicolumn{1}{l|}{0.52} & 0.56 & \\multicolumn{1}{l|}{242.96} & \\multicolumn{1}{l|}{0.21} & \\multicolumn{1}{l|}{23.65} & \\multicolumn{1}{l|}{0.14} & \\multicolumn{1}{l|}{0.77} & 0.82 \\\\ \\hline\n\t\t\\emph{NeuTraj}                          & \\multicolumn{1}{l|}{117.06} & \\multicolumn{1}{l|}{0.15} & \\multicolumn{1}{l|}{18.09} & \\multicolumn{1}{l|}{0.25} & \\multicolumn{1}{l|}{0.60} & 0.64 & \\multicolumn{1}{l|}{232.96} & \\multicolumn{1}{l|}{0.20} & \\multicolumn{1}{l|}{22.73} & \\multicolumn{1}{l|}{0.12} & \\multicolumn{1}{l|}{0.79} & 0.83 \\\\ \\hline\n\t\t\\emph{PIM}                     & \\multicolumn{1}{l|}{102.09} & \\multicolumn{1}{l|}{0.14} & \\multicolumn{1}{l|}{14.92} & \\multicolumn{1}{l|}{0.20} & \\multicolumn{1}{l|}{0.63} & 0.67 & \\multicolumn{1}{l|}{223.34} & \\multicolumn{1}{l|}{0.19} & \\multicolumn{1}{l|}{21.69} & \\multicolumn{1}{l|}{0.12} & \\multicolumn{1}{l|}{0.80} & 0.84 \\\\ \\hline\n\t\t\\emph{HMTRL}                   & \\multicolumn{1}{l|}{101.81} & \\multicolumn{1}{l|}{0.13} & \\multicolumn{1}{l|}{14.51} & \\multicolumn{1}{l|}{0.17} & \\multicolumn{1}{l|}{0.68} & 0.72 & \\multicolumn{1}{l|}{218.94} & \\multicolumn{1}{l|}{0.19} & \\multicolumn{1}{l|}{21.22} & \\multicolumn{1}{l|}{0.09} & \\multicolumn{1}{l|}{0.83} & 0.84 \\\\ \\hline\n\t\t\\emph{PathRank}                         & \\multicolumn{1}{l|}{115.37} & \\multicolumn{1}{l|}{0.15} & \\multicolumn{1}{l|}{16.41} & \\multicolumn{1}{l|}{0.21} & \\multicolumn{1}{l|}{0.64} & 0.68 & \\multicolumn{1}{l|}{229.85} & \\multicolumn{1}{l|}{0.20} & \\multicolumn{1}{l|}{22.53} & \\multicolumn{1}{l|}{0.11} & \\multicolumn{1}{l|}{0.81} & 0.82 \\\\ \\hline\n\t\t\\emph{CompactETA}                         & \\multicolumn{1}{l|}{106.47} & \\multicolumn{1}{l|}{0.15} & \\multicolumn{1}{l|}{16.22} & \\multicolumn{1}{l|}{0.17} & \\multicolumn{1}{l|}{0.67} & 0.70 & \\multicolumn{1}{l|}{236.28} & \\multicolumn{1}{l|}{0.20} & \\multicolumn{1}{l|}{23.13} & \\multicolumn{1}{l|}{0.11} & \\multicolumn{1}{l|}{0.79} & 0.80 \\\\ \\hline\n\t\t\\emph{HierETA}                         & \\multicolumn{1}{l|}{\\textit{88.95}} & \\multicolumn{1}{l|}{\\textit{0.12}} & \\multicolumn{1}{l|}{\\textit{14.23}} & \\multicolumn{1}{l|}{\\textit{0.15}} & \\multicolumn{1}{l|}{\\textit{0.71}} & \\textit{0.74} & \\multicolumn{1}{l|}{\\textit{215.39}} & \\multicolumn{1}{l|}{\\textit{0.19}} & \\multicolumn{1}{l|}{\\textit{21.66}} & \\multicolumn{1}{l|}{\\textit{0.09}} & \\multicolumn{1}{l|}{\\textit{0.84}} & \\textit{0.85} \\\\ \\hline\n\t\t\\emph{LightPath-Sup}           & \\multicolumn{1}{l|}{105.51} & \\multicolumn{1}{l|}{0.15} & \\multicolumn{1}{l|}{16.35} & \\multicolumn{1}{l|}{0.14} & \\multicolumn{1}{l|}{0.68} & 0.72 & \\multicolumn{1}{l|}{218.67} & \\multicolumn{1}{l|}{0.19} & \\multicolumn{1}{l|}{21.36} & \\multicolumn{1}{l|}{0.13} & \\multicolumn{1}{l|}{0.76} & 0.79 \\\\ \\hline\n\t\t% \\rowcolor{Gray}\n\t\t\\emph{LightPath}                        & \\multicolumn{1}{l|}{\\textbf{85.76}}  & \\multicolumn{1}{l|}{\\textbf{0.11}} & \\multicolumn{1}{l|}{\\textbf{12.12}} & \\multicolumn{1}{l|}{\\textbf{0.13}} & \\multicolumn{1}{l|}{\\textbf{0.73}} & \\textbf{0.77} & \\multicolumn{1}{l|}{\\textbf{212.61}} & \\multicolumn{1}{l|}{\\textbf{0.18}} & \\multicolumn{1}{l|}{\\textbf{20.75}} & \\multicolumn{1}{l|}{\\textbf{0.07}} & \\multicolumn{1}{l|}{\\textbf{0.87}} & \\textbf{0.88} \\\\ \\toprule[2pt]\n\t\\end{tabular}\n\t\\label{tab:ttpr}\n\t\n\\end{table*}",
            "tab:basicl": "\\begin{table}[t]\n\\centering\n\\small\n\\caption{Effect of Variants of \\emph{LightPath}}\n\\begin{tabular}{l|llllll|llllll}\n\\toprule[2pt]\n\\multirow{3}{*}{\\textbf{Method}} & \\multicolumn{6}{l|}{\\textbf{Aalborg}}                                                                                                                                                              & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                              \\\\ \\cline{2-13} \n                                 & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l|}{\\textbf{Path Ranking}}                                           & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-13} \n                                 & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n\\textit{\\textbf{w/o RR}}         & \\multicolumn{1}{l|}{90.90}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.85}         & \\multicolumn{1}{l|}{0.17}         & \\multicolumn{1}{l|}{0.66}         & 0.70         & \\multicolumn{1}{l|}{224.31}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.90}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.76}         & 0.79         \\\\ \\hline\n\\textit{\\textbf{w/o Rec.}}       & \\multicolumn{1}{l|}{103.45}       & \\multicolumn{1}{l|}{0.14}          & \\multicolumn{1}{l|}{15.76}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.65}         & 0.69         & \\multicolumn{1}{l|}{229.24}       & \\multicolumn{1}{l|}{0.20}          & \\multicolumn{1}{l|}{22.36}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.69}         & 0.73         \\\\ \\hline\n\\textit{\\textbf{w/o ME}}         & \\multicolumn{1}{l|}{91.57}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.09}         & \\multicolumn{1}{l|}{0.16}         & \\multicolumn{1}{l|}{0.68}         & 0.72         & \\multicolumn{1}{l|}{223.81}       & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.86}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.78}         & 0.81         \\\\ \\hline\n\\textit{\\textbf{w/o CN}}         & \\multicolumn{1}{l|}{93.17}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.35}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.68}         & 0.73         & \\multicolumn{1}{l|}{217.14}       & \\multicolumn{1}{l|}{0.18}          & \\multicolumn{1}{l|}{21.29}         & \\multicolumn{1}{l|}{0.08}         & \\multicolumn{1}{l|}{0.80}         & 0.83         \\\\ \\hline\n\\textit{\\textbf{w/o CV}}         & \\multicolumn{1}{l|}{89.84}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.51}         & \\multicolumn{1}{l|}{0.15}         & \\multicolumn{1}{l|}{0.68}         & 0.72         & \\multicolumn{1}{l|}{215.59}       & \\multicolumn{1}{l|}{0.18}          & \\multicolumn{1}{l|}{21.20}         & \\multicolumn{1}{l|}{0.09}         & \\multicolumn{1}{l|}{0.81}         & 0.83         \\\\ \\hline\n\\textit{\\textbf{LightPath}}      & \\multicolumn{1}{l|}{\\textbf{85.76}} & \\multicolumn{1}{l|}{\\textbf{0.11}} & \\multicolumn{1}{l|}{\\textbf{12.12}} & \\multicolumn{1}{l|}{\\textbf{0.13}} & \\multicolumn{1}{l|}{\\textbf{0.73}} & \\textbf{0.77} & \\multicolumn{1}{l|}{\\textbf{212.61}} & \\multicolumn{1}{l|}{\\textbf{0.18}} & \\multicolumn{1}{l|}{\\textbf{20.75}} & \\multicolumn{1}{l|}{\\textbf{0.07}} & \\multicolumn{1}{l|}{\\textbf{0.87}} & \\textbf{0.88} \\\\ \\toprule[2pt]\n\\end{tabular}\n\\label{tab:basicl}\n\\end{table}",
            "tab:glkd": "\\begin{table}[t]\n\\centering\n\\small\n\\caption{Effect of KD, Global Loss and Local Loss}\n\\begin{tabular}{l|llllll|llllll}\n\\toprule[2pt]\n\\multirow{3}{*}{\\textbf{Method}} & \\multicolumn{6}{l|}{\\textbf{Aalborg}}                                                                                                                                                                    & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                                     \\\\ \\cline{2-13} \n                                 & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                           & \\multicolumn{3}{l|}{\\textbf{Path Ranking}}                                              & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                            & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                              \\\\ \\cline{2-13} \n                                 & \\multicolumn{1}{l|}{\\textbf{MAE}}   & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}}  & \\multicolumn{1}{l|}{\\textbf{$\\tau$}}  & \\textbf{$\\rho$}  & \\multicolumn{1}{l|}{\\textbf{MAE}}    & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}}  & \\multicolumn{1}{l|}{\\textbf{$\\tau$}}  & \\textbf{$\\rho$}  \\\\ \\toprule[1pt]\n\\textit{\\textbf{w/o KD}}         & \\multicolumn{1}{l|}{87.77}          & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.94}          & \\multicolumn{1}{l|}{0.14}          & \\multicolumn{1}{l|}{0.70}          & 0.74          & \\multicolumn{1}{l|}{213.26}          & \\multicolumn{1}{l|}{0.18}          & \\multicolumn{1}{l|}{20.97}          & \\multicolumn{1}{l|}{0.08}          & \\multicolumn{1}{l|}{0.84}          & 0.86          \\\\ \\hline\n\\textit{\\textbf{w/o Global}}     & \\multicolumn{1}{l|}{90.24}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.31}          & \\multicolumn{1}{l|}{0.18}          & \\multicolumn{1}{l|}{0.67}          & 0.71          & \\multicolumn{1}{l|}{220.32}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.52}          & \\multicolumn{1}{l|}{0.09}          & \\multicolumn{1}{l|}{0.79}          & 0.81          \\\\ \\hline\n\\textit{\\textbf{w/o Local}}      & \\multicolumn{1}{l|}{89.23}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.78}          & \\multicolumn{1}{l|}{0.16}          & \\multicolumn{1}{l|}{0.69}          & 0.73          & \\multicolumn{1}{l|}{215.03}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.02}          & \\multicolumn{1}{l|}{0.08}          & \\multicolumn{1}{l|}{0.82}          & 0.84          \\\\ \\hline\n\\textit{\\textbf{LightPath}}      & \\multicolumn{1}{l|}{\\textbf{85.76}} & \\multicolumn{1}{l|}{\\textbf{0.11}} & \\multicolumn{1}{l|}{\\textbf{12.12}} & \\multicolumn{1}{l|}{\\textbf{0.13}} & \\multicolumn{1}{l|}{\\textbf{0.73}} & \\textbf{0.77} & \\multicolumn{1}{l|}{\\textbf{212.61}} & \\multicolumn{1}{l|}{\\textbf{0.18}} & \\multicolumn{1}{l|}{\\textbf{20.75}} & \\multicolumn{1}{l|}{\\textbf{0.07}} & \\multicolumn{1}{l|}{\\textbf{0.87}} & \\textbf{0.88} \\\\ \\toprule[2pt]\n\\end{tabular}\n\\label{tab:glkd}\n\\end{table}",
            "tab:msca": "\\begin{sidewaystable*}[htp]\n\\centering\n\\caption{Model Scalability vs. Reduction Ratio~($\\gamma$) and Path Length~(N)(Aalborg) }\n\\small\n\\begin{tabular}{l|lllllllllllll}\n\\toprule[2pt]\n\\multirow{3}{*}{$N$} & \\multicolumn{13}{l}{\\textbf{\\emph{LightPath}}}                                                                                                                           \\\\ \\cline{2-14} \n                             & \\multicolumn{2}{l|}{$\\gamma=0$}                                      & \\multicolumn{2}{l|}{$\\gamma=0.1$}                                                                              & \\multicolumn{2}{l|}{$\\gamma=0.3$}                                                                              & \\multicolumn{2}{l|}{$\\gamma=0.5$}                                                                              & \\multicolumn{2}{l|}{$\\gamma=0.7$}                                                                              & \\multicolumn{2}{l|}{$\\gamma=0.9$}                                                                              & \\multirow{2}{*}{\\begin{tabular}[c]{@{}l@{}}Para.\\\\ \\end{tabular}} \\\\ \\cline{2-13}\n                             & \\multicolumn{1}{l|}{GFLOPs}  & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\end{tabular}} &                                                                           \\\\ \\toprule[1pt]\n$ 50$                           & \\multicolumn{1}{l|}{8.01}   & \\multicolumn{1}{l|}{1.47}    & \\multicolumn{1}{l|}{7.48}  & \\multicolumn{1}{l|}{1.39}                                               & \\multicolumn{1}{l|}{6.44}  & \\multicolumn{1}{l|}{1.37}                                               & \\multicolumn{1}{l|}{5.39}  & \\multicolumn{1}{l|}{1.36}                                               & \\multicolumn{1}{l|}{4.34}  & \\multicolumn{1}{l|}{1.34}                                               & \\multicolumn{1}{l|}{3.18}  & \\multicolumn{1}{l|}{1.33}                                               & 1.570                                                                     \\\\ \\hline\n$ 100$                          & \\multicolumn{1}{l|}{15.77}  & \\multicolumn{1}{l|}{1.60}    & \\multicolumn{1}{l|}{14.72} & \\multicolumn{1}{l|}{1.50}                                               & \\multicolumn{1}{l|}{12.62} & \\multicolumn{1}{l|}{1.48}                                               & \\multicolumn{1}{l|}{10.52} & \\multicolumn{1}{l|}{1.46}                                               & \\multicolumn{1}{l|}{8.43}  & \\multicolumn{1}{l|}{1.44}                                               & \\multicolumn{1}{l|}{6.23}  & \\multicolumn{1}{l|}{1.43}                                               & 1.570                                                                     \\\\ \\hline\n$ 150$                          & \\multicolumn{1}{l|}{23.53}  & \\multicolumn{1}{l|}{1.72}    & \\multicolumn{1}{l|}{21.95} & \\multicolumn{1}{l|}{1.64}                                               & \\multicolumn{1}{l|}{18.81} & \\multicolumn{1}{l|}{1.60}                                               & \\multicolumn{1}{l|}{15.66} & \\multicolumn{1}{l|}{1.58}                                               & \\multicolumn{1}{l|}{12.52} & \\multicolumn{1}{l|}{1.55}                                               & \\multicolumn{1}{l|}{9.27}  & \\multicolumn{1}{l|}{1.52}                                               & 1.570                                                                     \\\\ \\hline\n$ 200$                          & \\multicolumn{1}{l|}{31.29}  & \\multicolumn{1}{l|}{1.90}    & \\multicolumn{1}{l|}{29.19}  & \\multicolumn{1}{l|}{1.81}                                               & \\multicolumn{1}{l|}{24.99} & \\multicolumn{1}{l|}{1.77}                                               & \\multicolumn{1}{l|}{20.80}  & \\multicolumn{1}{l|}{1.73}                                               & \\multicolumn{1}{l|}{16.61} & \\multicolumn{1}{l|}{1.68}                                               & \\multicolumn{1}{l|}{12.31} & \\multicolumn{1}{l|}{1.65}                                               & 1.570                                                                     \\\\ \\toprule[2pt]\n\\multirow{3}{*}{} & \\multicolumn{13}{l}{\\textbf{\\textit{LightPath w/o KD}}}     \\\\ \\cline{2-14} \n                             & \\multicolumn{2}{l|}{$\\gamma=0$}                                      & \\multicolumn{2}{l|}{$\\gamma=0.1$}                                                                              & \\multicolumn{2}{l|}{$\\gamma=0.3$}                                                                              & \\multicolumn{2}{l|}{$\\gamma=0.5$}                                                                              & \\multicolumn{2}{l|}{$\\gamma=0.7$}                                                                              & \\multicolumn{2}{l|}{$\\gamma=0.9$}                                                                              & \\multirow{2}{*}{\\begin{tabular}[c]{@{}l@{}}Para.\\\\ \\end{tabular}}                                           \\\\ \\cline{2-13}\n                             & \\multicolumn{1}{l|}{GFLOPs}  & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} & \\multicolumn{1}{l|}{GFLOPs} & \\multicolumn{1}{l|}{\\begin{tabular}[c]{@{}l@{}}gMem.\\\\ \\end{tabular}} &                                                                           \\\\ \\hline\n$ 50$                           & \\multicolumn{1}{l|}{33.68}  & \\multicolumn{1}{l|}{1.78}    & \\multicolumn{1}{l|}{30.64} & \\multicolumn{1}{l|}{1.70}                                               & \\multicolumn{1}{l|}{22.55} & \\multicolumn{1}{l|}{1.61}                                               & \\multicolumn{1}{l|}{18.47} & \\multicolumn{1}{l|}{1.53}                                               & \\multicolumn{1}{l|}{12.39} & \\multicolumn{1}{l|}{1.47}                                               & \\multicolumn{1}{l|}{5.70}  & \\multicolumn{1}{l|}{1.41}                                               & 5.525                                                                     \\\\ \\hline\n$ 100$                          & \\multicolumn{1}{l|}{66.60}  & \\multicolumn{1}{l|}{2.53}    & \\multicolumn{1}{l|}{60.52}  & \\multicolumn{1}{l|}{2.37}                                               & \\multicolumn{1}{l|}{48.36} & \\multicolumn{1}{l|}{2.11}                                               & \\multicolumn{1}{l|}{36.19} & \\multicolumn{1}{l|}{1.86}                                               & \\multicolumn{1}{l|}{24.03}  & \\multicolumn{1}{l|}{1.72}                                               & \\multicolumn{1}{l|}{11.26} & \\multicolumn{1}{l|}{1.58}                                               & 5.525                                                                     \\\\ \\hline\n$ 150$                          & \\multicolumn{1}{l|}{99.53}  & \\multicolumn{1}{l|}{3.44}    & \\multicolumn{1}{l|}{90.41} & \\multicolumn{1}{l|}{3.23}                                               & \\multicolumn{1}{l|}{72.16}  & \\multicolumn{1}{l|}{2.76}                                               & \\multicolumn{1}{l|}{53.91} & \\multicolumn{1}{l|}{2.35}                                               & \\multicolumn{1}{l|}{35.65}  & \\multicolumn{1}{l|}{2.03}                                               & \\multicolumn{1}{l|}{16.82} & \\multicolumn{1}{l|}{1.82}                                               & 5.525                                                                     \\\\ \\hline\n$ 200$                          & \\multicolumn{1}{l|}{132.54} & \\multicolumn{1}{l|}{4.74}    & \\multicolumn{1}{l|}{120.29} & \\multicolumn{1}{l|}{4.30}                                               & \\multicolumn{1}{l|}{95.96} & \\multicolumn{1}{l|}{3.53}                                               & \\multicolumn{1}{l|}{71.64} & \\multicolumn{1}{l|}{2.94}                                               & \\multicolumn{1}{l|}{47.31} & \\multicolumn{1}{l|}{2.43}                                               & \\multicolumn{1}{l|}{22.37} & \\multicolumn{1}{l|}{2.10}                                               & 5.525                                                                     \\\\ \\toprule[2pt]\n\\end{tabular}\n\\label{tab:msca}\n\\end{sidewaystable*}",
            "tab:gamma": "\\begin{table}[t]\n\\centering\n\\caption{Effect of Reduction Ratio $\\gamma$}\n\\begin{tabular}{l|llllll}\n\\toprule[2pt]\n\\multirow{3}{*}{\\textbf{$\\gamma$}}                                                  & \\multicolumn{6}{l}{\\textbf{Aalborg}}                                                                                                                                                              \\\\ \\cline{2-7} \n                                                                                  & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                        & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                           \\\\ \\cline{2-7} \n                                                                                  & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}} & \\multicolumn{1}{l|}{\\textbf{MAE}} & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n% \\rowcolor{Gray}\n0.1                                                 & \\multicolumn{1}{l|}{\\textbf{82.79}}        & \\multicolumn{1}{l|}{\\textbf{0.11}}          & \\multicolumn{1}{l|}{11.95}         & \\multicolumn{1}{l|}{\\textbf{0.12}}         & \\multicolumn{1}{l|}{\\textbf{0.74}}         &\\textbf{0.77}          \\\\ \\hline\n0.3                                                 & \\multicolumn{1}{l|}{84.75}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.14}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.73}         &0.77          \\\\ \\hline\n0.5                                                 & \\multicolumn{1}{l|}{84.81}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{\\textbf{11.86}}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.72}         &0.76          \\\\ \\hline\n0.7                                                 & \\multicolumn{1}{l|}{85.91}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.49}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.71}         &0.75          \\\\ \\hline\n% 0.85                                                 & \\multicolumn{1}{l|}{87.19}        & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.41}         & \\multicolumn{1}{l|}{0.14}         & \\multicolumn{1}{l|}{0.69}         &0.71  \\\\ \\hline\n\n0.9                                &                      \\multicolumn{1}{l|}{85.76}        & \\multicolumn{1}{l|}{0.11}          & \\multicolumn{1}{l|}{12.12}         & \\multicolumn{1}{l|}{0.13}         & \\multicolumn{1}{l|}{0.73}         &0.77 \\\\ \\toprule[2pt]        \n\\end{tabular}\n\\label{tab:gamma}\n\n\\end{table}",
            "tab:temperature": "\\begin{table}[t]\n\\centering\n\\caption{Effect of Temperature $t$ in KD}\n\\begin{tabular}{l|llllll|llllll}\n\\toprule[2pt]\n\\multirow{3}{*}{t} & \\multicolumn{6}{l|}{\\textbf{Aalborg}}                                                                                                                                                                    & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                                     \\\\ \\cline{2-13} \n                   & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                           & \\multicolumn{3}{l|}{\\textbf{Path Ranking}}                                              & \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                            & \\multicolumn{3}{l}{\\textbf{Path Ranking}}                                              \\\\ \\cline{2-13} \n                   & \\multicolumn{1}{l|}{\\textbf{MAE}}   & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}}  & \\multicolumn{1}{l|}{\\textbf{$\\tau$}}  & \\textbf{$\\rho$}  & \\multicolumn{1}{l|}{\\textbf{MAE}}    & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}}  & \\multicolumn{1}{l|}{\\textbf{$\\tau$}}  & \\textbf{$\\rho$}  \\\\ \\toprule[1pt]\n1                  & \\multicolumn{1}{l|}{90.01}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.30}          & \\multicolumn{1}{l|}{0.16}          & \\multicolumn{1}{l|}{0.65}          & 0.69          & \\multicolumn{1}{l|}{225.70}          & \\multicolumn{1}{l|}{0.20}          & \\multicolumn{1}{l|}{22.02}          & \\multicolumn{1}{l|}{0.09}          & \\multicolumn{1}{l|}{0.79}          & 0.82          \\\\ \\hline\n3                  & \\multicolumn{1}{l|}{94.11}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.54}          & \\multicolumn{1}{l|}{0.15}          & \\multicolumn{1}{l|}{0.68}          & 0.72          & \\multicolumn{1}{l|}{217.07}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.77}          & \\multicolumn{1}{l|}{0.09}          & \\multicolumn{1}{l|}{0.81}          & 0.84          \\\\ \\hline\n5                  & \\multicolumn{1}{l|}{90.39}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.76}          & \\multicolumn{1}{l|}{0.15}          & \\multicolumn{1}{l|}{0.66}          & 0.70          & \\multicolumn{1}{l|}{216.93}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.24}          & \\multicolumn{1}{l|}{0.08}          & \\multicolumn{1}{l|}{0.83}          & 0.86          \\\\ \\hline\n7                  & \\multicolumn{1}{l|}{89.64}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.76}          & \\multicolumn{1}{l|}{0.15}          & \\multicolumn{1}{l|}{0.70}          & 0.74          & \\multicolumn{1}{l|}{214.88}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.98}          & \\multicolumn{1}{l|}{0.08}          & \\multicolumn{1}{l|}{0.86}          & 0.87          \\\\ \\hline\n9                  & \\multicolumn{1}{l|}{\\textbf{85.76}} & \\multicolumn{1}{l|}{\\textbf{0.11}} & \\multicolumn{1}{l|}{\\textbf{12.12}} & \\multicolumn{1}{l|}{\\textbf{0.13}} & \\multicolumn{1}{l|}{\\textbf{0.73}} & \\textbf{0.77} & \\multicolumn{1}{l|}{\\textbf{212.61}} & \\multicolumn{1}{l|}{\\textbf{0.18}} & \\multicolumn{1}{l|}{\\textbf{20.75}} & \\multicolumn{1}{l|}{\\textbf{0.07}} & \\multicolumn{1}{l|}{\\textbf{0.87}} & \\textbf{0.88} \\\\ \\hline\n11                 & \\multicolumn{1}{l|}{87.15}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.43}          & \\multicolumn{1}{l|}{0.14}          & \\multicolumn{1}{l|}{0.70}          & 0.74          & \\multicolumn{1}{l|}{214.17}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.00}          & \\multicolumn{1}{l|}{0.08}          & \\multicolumn{1}{l|}{0.83}          & 0.85          \\\\ \\toprule[2pt]\n\\end{tabular}\n\\label{tab:temperature}\n\\end{table}",
            "tab:glkdbf": "\\begin{table}[!htp]\n\t\n\t\\centering\n\t\\caption{Effect of Balancing Factor $\\alpha$}\n\t\\begin{tabular}{l|llllll|llllll}\n\t\t\\toprule[2pt]\n\t\t\\multirow{3}{*}{\\textbf{$\\alpha$}} & \\multicolumn{6}{l|}{\\textbf{Aalborg}}                                                                                                                                                                                                  & \\multicolumn{6}{l}{\\textbf{Chengdu}}                                                                                                                                                                                                   \\\\ \\cline{2-13} \n\t\t& \\multicolumn{3}{l|}{\\textbf{Travel Time Estimation}}                                                           & \\multicolumn{3}{l|}{\\textbf{Path Ranking}}                                                                            & \\multicolumn{3}{l|}{Travel Time Estimation}                                                                     & \\multicolumn{3}{l}{Path Ranking}                                                                                     \\\\ \\cline{2-13} \n\t\t& \\multicolumn{1}{l|}{\\textbf{MAE}}   & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}}  & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} & \\multicolumn{1}{l|}{\\textbf{MAE}}    & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\multicolumn{1}{l|}{\\textbf{MAPE}}  & \\multicolumn{1}{l|}{\\textbf{MAE}}  & \\multicolumn{1}{l|}{\\textbf{$\\tau$}} & \\textbf{$\\rho$} \\\\ \\toprule[1pt]\n\t\t0                                               & \\multicolumn{1}{l|}{90.24}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.78}          & \\multicolumn{1}{l|}{0.16}          & \\multicolumn{1}{l|}{0.69}                         & 0.73                         & \\multicolumn{1}{l|}{220.32}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.52}          & \\multicolumn{1}{l|}{0.09}          & \\multicolumn{1}{l|}{0.79}                         & 0.81                         \\\\ \\hline\n\t\t0.2                                             & \\multicolumn{1}{l|}{89.35}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.85}          & \\multicolumn{1}{l|}{0.14}          & \\multicolumn{1}{l|}{0.69}                         & 0.73                         & \\multicolumn{1}{l|}{217.10}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.34}          & \\multicolumn{1}{l|}{0.09}          & \\multicolumn{1}{l|}{0.78}                         & 0.80                         \\\\ \\hline\n\t\t0.4                                             & \\multicolumn{1}{l|}{91.57}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{13.17}          & \\multicolumn{1}{l|}{0.15}          & \\multicolumn{1}{l|}{0.69}                         & 0.73                         & \\multicolumn{1}{l|}{217.33}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.21}          & \\multicolumn{1}{l|}{0.08}          & \\multicolumn{1}{l|}{0.85}                         & 0.87                         \\\\ \\hline\n\t\t0.6                                             & \\multicolumn{1}{l|}{\\textbf{85.76}} & \\multicolumn{1}{l|}{\\textbf{0.11}} & \\multicolumn{1}{l|}{\\textbf{12.12}} & \\multicolumn{1}{l|}{\\textbf{0.13}} & \\multicolumn{1}{l|}{\\textbf{0.73}}                & \\textbf{0.77}                & \\multicolumn{1}{l|}{\\textbf{212.61}} & \\multicolumn{1}{l|}{\\textbf{0.18}} & \\multicolumn{1}{l|}{\\textbf{20.75}} & \\multicolumn{1}{l|}{\\textbf{0.07}} & \\multicolumn{1}{l|}{\\textbf{0.87}}                & \\textbf{0.88}                \\\\ \\hline\n\t\t0.8                                             & \\multicolumn{1}{l|}{87.44}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.76}          & \\multicolumn{1}{l|}{0.14}          & \\multicolumn{1}{l|}{0.70}                         & 0.75                         & \\multicolumn{1}{l|}{214.34}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{20.93}          & \\multicolumn{1}{l|}{0.08}          & \\multicolumn{1}{l|}{0.84}                         & 0.86                         \\\\ \\hline\n\t\t1                                               & \\multicolumn{1}{l|}{89.23}          & \\multicolumn{1}{l|}{0.12}          & \\multicolumn{1}{l|}{12.78}          & \\multicolumn{1}{l|}{0.16}          & \\multicolumn{1}{l|}{0.69}                         & 0.73                         & \\multicolumn{1}{l|}{215.03}          & \\multicolumn{1}{l|}{0.19}          & \\multicolumn{1}{l|}{21.02}          & \\multicolumn{1}{l|}{0.08}          & \\multicolumn{1}{l|}{0.82}                         & 0.84                         \\\\ \\toprule[2pt]\n\t\\end{tabular}\n\t\\label{tab:glkdbf}\n\\end{table}",
            "tab:fullpath": "\\begin{table}[htp]\n\t\\centering\n\t\\caption{Comparison with Whole Path Input}\n\t\\begin{tabular}{l|lll}\n\t\t\\toprule[2pt]\n\t\t\\multirow{3}{*}{}            & \\multicolumn{3}{l}{\\textbf{Aalborg}}                                   \\\\ \\cline{2-4} \n\t\t& \\multicolumn{3}{l}{\\textbf{Travel Time Estimation}}                    \\\\ \\cline{2-4} \n\t\t& \\multicolumn{1}{l|}{\\textbf{MAE}}   & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\textbf{MAPE}  \\\\ \\toprule[1pt]\n\t\t\\emph{LightPath w/o RR} ($\\gamma=0.1$) & \\multicolumn{1}{l|}{91.97} & \\multicolumn{1}{l|}{0.12} & 13.53 \\\\ \\hline\n\t\t\\emph{LightPath w/o RR} ($\\gamma=0.1$) & \\multicolumn{1}{l|}{90.85} & \\multicolumn{1}{l|}{0.12} & 13.39 \\\\ \\hline\n\t\t\\emph{LightPath} ($\\gamma=0.1$)        & \\multicolumn{1}{l|}{82.79} & \\multicolumn{1}{l|}{0.11} & 11.95 \\\\ \\toprule[2pt]\n\t\\end{tabular}\n\t\\label{tab:fullpath}\n\t\\vspace{-5pt}\n\\end{table}",
            "tab:fixstrategy": "\\begin{table}[htp]\n\t\\centering\n\t\\caption{Comparison with Fixed Interval Strategy}\n\t\\begin{tabular}{l|l|lll}\n\t\t\\toprule[2pt]\n\t\t\\multirow{3}{*}{\\textbf{Strategy}}       & \\multirow{3}{*}{$\\gamma$} & \\multicolumn{3}{l}{\\textbf{Aalborg}}                                           \\\\ \\cline{3-5} \n\t\t&                           & \\multicolumn{3}{l}{\\textbf{Travel Time Estimation}}                            \\\\ \\cline{3-5} \n\t\t&                           & \\multicolumn{1}{l|}{MAE}   & \\multicolumn{1}{l|}{\\textbf{MARE}} & \\textbf{MAPE} \\\\ \\toprule[1pt]\n\t\t\\emph{LightPath-Fixed}  & 1/10                      & \\multicolumn{1}{l|}{87.77} & \\multicolumn{1}{l|}{0.12}          & 12.95         \\\\ \\hline\n\t\t\\emph{LightPath-Fixed}  & 5/10                      & \\multicolumn{1}{l|}{89.63} & \\multicolumn{1}{l|}{0.12}          & 12.86         \\\\ \\hline\n\t\t\\emph{LightPath-Fixed}  & 9/10                      & \\multicolumn{1}{l|}{92.87} & \\multicolumn{1}{l|}{0.12}          & 12.23         \\\\ \\hline\n\t\t\\emph{LightPath-Random} & 0.9                       & \\multicolumn{1}{l|}{85.76} & \\multicolumn{1}{l|}{0.11}          & 12.12         \\\\ \\toprule[2pt]\n\t\\end{tabular}\n\t\\label{tab:fixstrategy}\n\\end{table}"
        },
        "figures": {
            "fig:re": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.6]{fig1.pdf}\n    \\caption{Intuition of the Lightweight Path Representation Learning Problem}\n    \\label{fig:re}\n\n\\end{figure}",
            "fig:scale": "\\begin{figure}[b]\n     \\centering\n     \\begin{subfigure}[b]{0.45\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{fig2a.pdf}\n         \\caption{GPU Memory}\n         \\label{fig:subfig:mac}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.45\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{fig2b.pdf}\n         \\caption{GFLOPs}\n         \\label{fig:subfig:flops}\n     \\end{subfigure}\n        \\caption{Scalability w.r.t. Path Length.}\n        \\label{fig:scale}\n\n\\end{figure}",
            "fig:tt": "\\begin{figure*}[t]\n     \\centering\n     \\begin{subfigure}[b]{0.3\\textwidth}\n         \\centering\n         \\includegraphics[scale=0.31]{fig7a.pdf}\n         \\caption{Traditional Transformer Encoder}\n         \\label{fig:subfig:tte}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.3\\textwidth}\n         \\centering\n         \\includegraphics[scale=0.31]{fig7b.pdf}\n         \\caption{Sparse Transformer Encoder}\n         \\label{fig:subfig:ste}\n     \\end{subfigure}\n     \\begin{subfigure}[b]{0.3\\textwidth}\n         \\centering\n    \\includegraphics[scale=0.31]{fig7c.pdf}\n    \\caption{LightPath}\n    \\label{fig:subfig:lp}\n     \\end{subfigure}\n        \\caption{Encoder Architectures: (a) A traditional transformer encoder with $L$ layers and $M$ heads, takes as input a path ($N$-Length) and has complexity $\\mathcal{O}\\left(L\\cdot M \\cdot N^{2}\\right)$; (b) A sparse transformer encoder takes as input a sparse path (i.e., reducing path length from $N$ to $N^{\\prime}$), resulting in $\\mathcal{O}\\left(L\\cdot M \\cdot N^{\\prime 2}\\right)$ complexity; (c) \\emph{LightPath} further compresses the traditional transformer in terms of layers and heads, yielding complexity $\\mathcal{O}\\left(L^{\\prime} \\cdot N^{\\prime 2}\\right)$, making it more scalable and lightweight than a traditional transformer encoder.}\n        \\label{fig:tt}\n      \n\\end{figure*}",
            "fig:spe": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.6]{fig3.pdf}\n    \\caption{Sparse Auto-encoder. We remove a subset of edges from a path based on a reduction ratio $\\gamma$ to obtain a sparse path. We introduce a learnable path representation in front of the sparse path. And then, we fed the resulting sparse path vectors with position embeddings to a Transformer based encoder. We then introduce a learnable edge representation, denoted as a triangle, to represent the removed edges. The encoded edges in the sparse path and the removed edge representations with position embeddings are processed by a decoder that reconstructs the edges in the original path.}\n    \\label{fig:spe}\n\n\\end{figure}",
            "fig:msrr": "\\begin{figure}[t]\n     \\centering\n     \\begin{subfigure}[b]{0.5\\textwidth}\n         \\centering\n         \\includegraphics[scale=0.45]{fig4a.pdf}\n         \\caption{Path Representation Construction Using Dual Sparse Path Encoders}\n         \\label{fig:subfig:prc}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.5\\textwidth}\n         \\centering\n         \\includegraphics[scale=0.45]{fig4b.pdf}\n         \\caption{Relational Reasoning}\n         \\label{fig:subfig:msrr}\n        %  \\vspace{-10pt}\n\\end{subfigure}\n\\caption{Illustration of RR Training: (a) Given an input path $p_1$, we construct two path views (i.e., $p_1^1$ and $p_1^2$) through two reduction ratios $\\gamma_1$ and $\\gamma_2$, based on which a main encoder and an auxiliary encoder are employed to generate path representations for each view (i.e., $\\mathit{PR}_1^1$, $\\mathit{PR}_1^2$, $\\hat{\\mathit{PR}}_1^1$, and $\\hat{\\mathit{PR}}_1^2$). (b) After getting corresponding path representations for paths in a minibatch, a relational reasoning path representation learning scheme, which utilizes both cross-network and cross-view relational reasoning modules, is deployed. In particular, for both modules, an aggregation function $a$ joins positives (representations of the same paths, e.g., $a(\\mathit{PR}_1^1,\\hat{\\mathit{PR}}_1^1)$, $a(\\mathit{PR}_1^1, \\mathit{PR}_1^2)$) and negatives (randomly paired representations, e.g., $a(\\mathit{PR}_1^1,\\hat{\\mathit{PR}}_3^1)$, $a(\\mathit{PR}_1^1, \\mathit{PR}_3^2)$) through a commutative operator. Then relation head module $RRH_{\\varphi}(\\cdot)$ estimates the relation score $y$, which must be 1 for positive and 0 for negatives. Both cross-network and cross-view objectives are optimized minimizing the binary cross-entropy (BCE) between prediction and target relation value $t$. In this example, $i \\in [1,2,3]$ denotes the number of paths in the minibatch and $j  \\in [1,2]$ represents the number of views.}\n\\label{fig:msrr}\n\n\\end{figure}",
            "fig:kd": "\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.6]{fig5.pdf}\n    \\caption{Illustration of GLKD. Given an input path, we formulate our GLKD as a weighted sum of global path representation knowledge distillation (GPRKD) loss and local edge representation knowledge distillation (LERKD) loss.}\n    \\label{fig:kd}\n\n\\end{figure}",
            "fig:pretrain": "\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[b]{0.45\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{fig6a.pdf} %%fig7an bn cn dn\n\t\t\\caption{Travel Time Estimation}\n\t\t\\label{fig:subfig:tteaal}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}[b]{0.45\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{fig6b.pdf}\n\t\t\\caption{Path Ranking Estimation}\n\t\t\\label{fig:subfig:pr}\n\t\\end{subfigure}\n\t\n\t\\begin{subfigure}[b]{0.45\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{fig6c.pdf}\n\t\t\\caption{Travel Time Estimation}\n\t\t\\label{fig:subfig:ttecd}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}[b]{0.45\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{fig6d.pdf}\n\t\t\\caption{Path Ranking Estimation}\n\t\t\\label{fig:subfig:prcd}\n\t\\end{subfigure}\n\t\n\t\\caption{Effects of Pre-training.}\n\t\\label{fig:pretrain}\n\t\n\\end{figure}",
            "fig:mee": "\\begin{figure}[t]\n\n     \\centering\n     \\begin{subfigure}[b]{0.45\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{fig8b.pdf}\n         \\caption{Training}\n         \\label{fig:subfig:training}\n        %  \\vspace{-5pt}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.45\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{fig8b.pdf}\n         \\caption{Testing}\n         \\label{fig:subfig:inference}\n        %  \\vspace{-5pt}\n     \\end{subfigure}\n        \\caption{Model Efficiency Evaluation}\n        \\label{fig:mee}\n\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\mathit{PR} =SPE_{\\theta}\\left(p_{i}\\right):\\mathbb{R}^{N \\times d_k} \\rightarrow \\mathbb{R}^{d}\\text{ , }\n\\end{equation}",
            "eq:2": "\\begin{equation}\nReg_{task_k(\\psi)}\\left(\\mathit{PR}_{i}\\right):\\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\text{ , }\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\mathbf{Z}_p=\\mathbf{MultiHead}(\\mathbf{X}_p)=\\mathit{\\text{Concat}}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{M}\\right) \\cdot \\mathbf{W}^{O}\\text{ , } \n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\mathit{\\text { head }_i}(\\cdot)=\n% \\text { Attention }\\left(\\mathbf{X} \\mathbf{W}_{i}^{Q}, \\mathbf{\\mathbf{X} W}_{i}^{K}, \\mathbf{\\mathbf{X}} \\mathbf{W}_{i}^{V}\\right)=\\\\\n\\operatorname{softmax}\\left(\\left(\\mathbf{X}_p \\mathbf{W}_{i}^{Q}\\right) \\left(\\mathbf{\\mathbf{X}_p W}_{i}^{K}\\right)^{T}/{\\sqrt{d_{k}}}\\right) \\left(\\mathbf{\\mathbf{X}_p} \\mathbf{W}_{i}^{V}\\right)\\text{ , }\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\operatorname{FFN}(\\mathbf{Z}_p)=\\max \\left(0, \\mathbf{Z}_p \\mathbf{W_{1}^{FFN}}+\\mathbf{b_{1}^{FFN}}\\right) \\mathbf{W_{2}^{FFN}}+\\mathbf{b_{2}^{FFN}}\\text{ , }\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    p^{\\prime}= f\\left(p,\\gamma \\right)=\\langle e_j \\rangle_{j \\in \\Omega}\\text{ , }\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    p^{\\prime} = \\langle \\mathit{PR}\\rangle + \\langle e_j \\rangle_{j \\in \\Omega} =\\langle e_k \\rangle_{k \\in \\Omega^{\\prime}}\\text{ , }\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\mathbf{X}_p^{\\prime}=\\mathit{\\text{Concat}}\\langle x_k \\rangle_{k \\in \\Omega^{\\prime}}, \\text{ where } x_k = e_k + pos_k  \\text{ , } \n\\end{equation}",
            "eq:9": "\\begin{equation}\nZ_p^{\\prime}=\\operatorname{LayerNorm}(\\mathbf{X}_p^{\\prime}+\\text { MultiHead }(\\mathbf{X}_p^{\\prime}))\\text{ , } \n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\mathit{PR}=\\operatorname{LayerNorm}\\left(Z_p^{\\prime}+\\operatorname{FFN}\\left(Z_p^{\\prime}\\right)\\right)\\text{ ,}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    \\mathcal{L}_{rec}= \\frac{1}{N}\\sum_{i=1}^{N}\\left(e_{i}-\\hat{e_{i}} \\right)^{2}\\text{ , }\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    \\mathit{PR}_i^{j} =SPE_{\\theta}(p_i^{j},\\gamma)\\text{ , } \\hat{\\mathit{PR}}_i^{j} =SPE_{\\hat{\\theta}}(p_i^{j},\\gamma)\\text{ , }\n\\end{equation}",
            "eq:13": "\\begin{equation}\n\\label{eq:loss_cn}\n\\mathcal{L}_{cn}=\\underset{\\boldsymbol{\\theta}, \\varphi}{\\operatorname{argmin}} \\sum_{i=1}^{K}\\sum_{j=1}^{2}  \\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(\\mathit{PR}_{i}^{j}, \\hat{\\mathit{PR}}_{i}^{j}\\right)\\right), t=1\\right) +  \\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(\\mathit{PR}_{i}^{j}, \\hat{\\mathit{PR}}_{\\backslash i}^{j}\\right)\\right), t=0\\right)\\text{ , }\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\label{eq:momentum}\n    \\hat{\\theta}^{t}= m \\times \\hat{\\theta}^{(t-1)} +(1-m)\\times \\theta^{t},\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\label{eq:loss_cv}\n\\mathcal{L}_{cv}=\\underset{\\mathbf{\\theta},\\mathbf{\\varphi}}{\\operatorname{argmin}} \\sum_{i=1}^{K}  \\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(\\mathit{PR}_{i}^{1}, \\mathit{PR}_{i}^{2}\\right)\\right), t=1\\right) +\\mathcal{L}\\left(RRH_{\\varphi}\\left(a\\left(\\mathit{PR}_{i}^{1}, \\mathit{PR}_{\\backslash i}^{2}\\right)\\right), t=0\\right)\\text{ , }\n\\end{equation}",
            "eq:16": "\\begin{equation}\n\\label{eq:msrr}\n    \\min_{\\theta, \\varphi}\\mathcal{L}_{RR}=\\mathcal{L}_{cn} +\\mathcal{L}_{cv}\n\\end{equation}",
            "eq:17": "\\begin{equation}\n\\label{eq:loss_}\n    \\mathcal{L}=\\mathcal{L}_{rec}+\\mathcal{L}_{RR}\n\\end{equation}",
            "eq:18": "\\begin{equation}\n\\small\n    \\min_{\\theta}\\mathcal{L}_{global}\\left(\\mathbf{\\mathit{PR}}^{\\mathcal{T}}({p_{i}}), \\mathbf{\\mathit{PR}}^{\\mathcal{S}}(p_{i})\\right)=\\left\\|sp(\\mathbf{\\mathit{PR}}^{\\mathcal{T}}(p_{i})/t)-sp(\\mathbf{\\mathit{PR}}^{\\mathcal{S}}(p_{i})/t)\\right\\|^{2}\\textbf{,}\n\\end{equation}",
            "eq:19": "\\begin{equation}\n    F^{\\mathcal{T}}(e_{i})^{N^{\\prime}}_{i=1} = \\mathcal{T}_{\\theta}(p)\\text{ , } F^{\\mathcal{S}}(e_{i})^{N^{\\prime}}_{i=1} = \\mathcal{S}_{\\theta}(p)\\text{ , }\n\\end{equation}",
            "eq:20": "\\begin{equation}\n\\small\n    \\min_{\\theta}\\mathcal{L}_{local}\\left(\\mathbf{F}^{\\mathcal{T}}(e_{i}), \\mathbf{F}^{\\mathcal{S}}(e_{i})\\right)=\\frac{1}{n}\\sum^{n}_{i=1}\\left\\|sp(\\mathbf{F}^{\\mathcal{T}}(e_{i})/t)-sp(\\mathbf{F}^{\\mathcal{S}}(e_{i})/t)\\right\\|^{2}\\textbf{,}\n\\end{equation}",
            "eq:21": "\\begin{equation}\n\\label{eq:ad}\n\\min_{\\theta}\\mathcal{L}_{GLKD}=\\alpha*\\mathcal{L}_{global} +\\left(1-\\alpha\\right)*\\mathcal{L}_{local}\\text{ , }\n\\end{equation}"
        }
    }
}