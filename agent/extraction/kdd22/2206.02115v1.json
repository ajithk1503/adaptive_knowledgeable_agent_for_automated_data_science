{
    "meta_info": {
        "title": "Learning Binarized Graph Representations with Multi-faceted Quantization  Reinforcement for Top-K Recommendation",
        "abstract": "Learning vectorized embeddings is at the core of various recommender systems\nfor user-item matching. To perform efficient online inference, representation\nquantization, aiming to embed the latent features by a compact sequence of\ndiscrete numbers, recently shows the promising potentiality in optimizing both\nmemory and computation overheads. However, existing work merely focuses on\nnumerical quantization whilst ignoring the concomitant information loss issue,\nwhich, consequently, leads to conspicuous performance degradation. In this\npaper, we propose a novel quantization framework to learn Binarized Graph\nRepresentations for Top-K Recommendation (BiGeaR). BiGeaR introduces\nmulti-faceted quantization reinforcement at the pre-, mid-, and post-stage of\nbinarized representation learning, which substantially retains the\nrepresentation informativeness against embedding binarization. In addition to\nsaving the memory footprint, BiGeaR further develops solid online inference\nacceleration with bitwise operations, providing alternative flexibility for the\nrealistic deployment. The empirical results over five large real-world\nbenchmarks show that BiGeaR achieves about 22%~40% performance improvement over\nthe state-of-the-art quantization-based recommender system, and recovers about\n95%~102% of the performance capability of the best full-precision counterpart\nwith over 8x time and space reduction.",
        "author": "Yankai Chen, Huifeng Guo, Yingxue Zhang, Chen Ma, Ruiming Tang, Jingjie Li, Irwin King",
        "link": "http://arxiv.org/abs/2206.02115v1",
        "category": [
            "cs.IR"
        ],
        "additionl_info": "Accepted by SIGKDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nRecommender systems, aiming to perform personalized information filtering~\\cite{pinsage}, are versatile to many Internet applications. \nLearning vectorized user-item representations (i.e., embeddings), as the core of various recommender models, is the prerequisite for online inference of user-item interactions~\\cite{lightgcn,ngcf}.\nWith the explosive data expansion (e.g., Amazon owns over 150M users and 350M products~\\cite{amazon,amazon_2}), one major existing challenge however is to perform inference efficiently.\nThis usually requires large memory and computation consumption (e.g., for Amazon 500M-scaled \\textit{full-precision}\\footnote{{\\cyk It could be single-precision or double-precision;  we use float32 as the default for explanation and conducting experiments throughout this paper.}} embedding table) on certain data centers~\\cite{tailor2020degree}, and therefore hinders the deployment to devices with limited resources~\\cite{tailor2020degree}.\n\n  \n\n\nTo tackle this issue, \\textit{representation quantization} recently provides the promising feasibility.\nGenerally, it learns to quantize latent features of users and items via converting the continuous full-precision representations into discrete low-bit ones.\nThe quantized representations thus are conducive to model size reduction and inference speed-up with low-bit arithmetic on devices where CPUs are typically more affordable than expensive GPUs~\\cite{banner2018scalable,bahri2021binary}.\nTechnically, quantization can be categorized into multi-bit, 2-bit (i.e., ternarized), and 1-bit (i.e., binarized) quantization. \nWith only one bit-width, representation binarization for recommendation takes the most advantage of representation quantization and therefore draws the growing attention recently~\\cite{hashgnn,kang2019candidate}.\n\nDespite the promising potentiality, it is still challenging to develop realistic deployment mainly because of the large performance degradation in Top-K recommendation~\\cite{hashgnn,kang2019candidate}.\nThe crux of the matter is the threefold \\textit{information loss}:\n\\begin{itemize}[leftmargin=*,topsep=2pt,parsep=0.5pt]\n\\item \\textbf{Limited expressivity of latent features.}\nBecause of the discrete constraints, mapping full-precision embeddings into compact binary codes with equal expressivity is NP-hard~\\cite{haastad2001some}.\n{\\cyk Thus, instead of proposing complex and deep neural structures for quantization~\\cite{erin2015deep,zhu2016deep}, $\\sign(\\cdot)$ function is widely adopted to achieve {\\small $O(1)$} embedding binarization~\\cite{hashgnn,wang2017survey,lin2017towards}.} \nHowever, this only guarantees the sign (+/-) correlation for each embedding entry.\n{\\cyk Compared to the original full-precision embeddings, binarized targets produced from $\\sign(\\cdot)$ are naturally less informative}.\n\n\\item \\textbf{Degraded ranking capability.}\nRanking capability, as the essential measurement of Top-K recommendation, is the main objective to work on. \n{\\cyk Apart from the inevitable feature loss in numerical quantization, previous work further ignores the discrepancy of hidden knowledge that is inferred by full-precision and binarized embeddings~\\cite{kang2019candidate,hashgnn}.\nHowever, such hidden knowledge is vital to reveal users' preference towards different items, losing of which may thus draw degraded ranking capability and sub-optimal model learning accordingly.\n}\n\n\\item \\textbf{Inaccurate gradient estimation.}\nDue to the non-differentiability of quantization function $\\sign(\\cdot)$, \\textit{Straight-Through} \\textit{Estimator} (STE) \\cite{bengio2013estimating} is widely adopted to assume all propagated gradients as 1 in backpropagation~\\cite{hashgnn,lin2017towards,qin2020forward}.\nIntuitively, the integral of 1 is a certain linear function other than $\\sign(\\cdot)$, whereas this may lead to inaccurate gradient estimation and produce inconsistent optimization directions in the model training.\n\\end{itemize}\n\n\n\n\n% 既然表达能力天然有限，那么一个直观的解决办法就是从源头增加embedding的语义\n\nTo address aforementioned problems, we propose a novel quantization framework, namely \\textbf{BiGeaR}, to learn the \\textit{\\underline{Bi}narized} \\textit{\\underline{G}raph} \\textit{R\\underline{e}present\\underline{a}tions} for \\textit{Top-K} \\textit{\\underline{R}ecommendation}.\nBased on the natural bipartite graph topology of user-item interactions, we implement BiGeaR with the inspiration from graph-based models, i.e., Graph Convolutional Networks (GCNs)~\\cite{kipf2016semi,graphsage}.\nWith the emphasis on deepening the exploration of multi-hop subgraph structures,  GCN-based recommender models capture the high-order interactive relations and well simulate \\textit{Collaborative} \\textit{Filtering} for recommendation~\\cite{lightgcn,ngcf,pinsage}.\nSpecifically, BiGeaR sketches graph nodes (i.e., users and items) with binarized representations, which facilitates nearly one bit-width representation compression.\nFurthermore, our model BiGeaR decomposes the prediction formula (i.e., \\textit{inner} \\textit{product}) into bitwise operations (i.e., {\\tt Popcount} and {\\tt XNOR}).\nThis dramatically reduces the number of floating-point operations (\\#FLOP) and thus introduces theoretically solid acceleration {\\cyk for online inference}.\n{\\cyk To avoid large information loss}, as shown in Figure~\\ref{fig:intro}(a), BiGeaR technically consists of multi-faceted quantization reinforcement at the \\textit{pre}-, \\textit{mid}-, and \\textit{post}-stage of binarized representation learning:\n\\begin{enumerate}[leftmargin=*,topsep=2pt,parsep=0.5pt]\n\\item At the pre-stage of model learning, we propose the \\textbf{graph} \\textbf{layer-wise} \\textbf{quantization} {\\cyk (from low- to high-order interactions)} to consecutively binarize the user-item features with different semantics. \nOur analysis indicates that such layer-wise quantization can actually achieve the \\textit{magnification} \\textit{effect} of \\textit{feature} \\textit{uniqueness},\n{\\cyk which significantly compensates for the limited expressivity of embeddings binarization.}\nThe empirical study also justifies that, this is more effective to enrich the quantization informativeness, rather than simply increasing the embedding sizes in the conventional manner~\\cite{hashgnn,lin2017towards,qin2020forward,rastegari2016xnor}.\n\n\\item During the mid-stage of embedding quantization, BiGeaR introduces the \\textbf{self-supervised inference distillation} to develop the \\textit{low-loss} ranking capability inheritance.\nTechnically, it firstly extracts several \\textit{pseudo-positive} \\textit{training} \\textit{samples} that are inferred by full-precision embeddings of BiGeaR.\nThen these samples {\\cyk serve} as the direct regularization target to the quantized embeddings, such that they {\\cyk can distill the ranking knowledge from full-precision ones to have similar inference results}.\nDifferent from the conventional knowledge distillation, our approach is tailored specifically for Top-K recommendation and therefore boosts the performance {\\cyk with acceptable training costs}.\n\n\\item As for the post-stage backpropagation of quantization optimization, we propose to utilize the approximation of \\textit{Dirac} \\textit{delta} \\textit{function} (i.e., $\\delta$ function)~\\cite{bracewell1986fourier} for more \\textbf{accurate gradient estimation}. \nIn contrast to STE, our gradient estimator provides the consistent optimization direction with $\\sign(\\cdot)$ in both forward and backward propagation. \nThe empirical study demonstrates its superiority over other gradient estimators.\n\\end{enumerate}\n\n\\noindent\\textbf{Empirical Results.}\nThe extensive experiments over five real-world benchmarks show that, BiGeaR significantly outperforms the state-of-the-art quantization-based recommender model by 25.77\\%$\\sim$40.12\\% and 22.08\\%$\\sim$39.52\\% \\textit{w.r.t.} Recall and NDCG metrics.\nFurthermore, it attains 95.29\\%$\\sim$100.32\\% and 95.32\\%$\\sim$101.94\\% recommendation capability compared to the best-performing full-precision model, {\\cyk with over 8$\\times$ inference acceleration and space compression}. \n\n% \\vspace*{0.3\\baselineskip} \n\n\\noindent\\textbf{Discussion.} \nIt is worthwhile mentioning that BiGeaR is related to \\textit{hashing-based} models (i.e., learning to hash)~\\cite{kang2021learning,kang2019candidate}, {\\cyk as, conceptually, binary hashing can be viewed as 1-bit quantization}. However, as shown in Figure~\\ref{fig:intro}(b), they have different {\\cyk motivations}.\nHashing-based models are usually designed for fast candidate generation, followed by full-precision \\textit{re-ranking} algorithms for accurate prediction.\nMeanwhile, BiGeaR is \\textit{end-to-end} that aims to make predictions within the proposed architecture.\nHence, we believe BiGeaR is \\textit{technically} \\textit{related} but \\textit{motivationally} \\textit{orthogonal} to them.\n \n\n% \\vspace*{0.3\\baselineskip} \n\\noindent\\textbf{Organization.}\nWe present BiGeaR methodology and model analysis in~\\cref{sec:end} and~\\cref{sec:analysis}. \nThen we report the experiments and review the related work in~\\cref{sec:exp} and~\\cref{sec:work} with the conclusion in ~\\cref{sec:con}.\n\n\n\n% \\vspace{-0.05in}\n"
            },
            "section 2": {
                "name": "BiGeaR Methodology",
                "content": "\n\\label{sec:end}\nIn this section, we formally introduce:\n\\textit{(1) graph layer-wise quantization for feature magnification;\n(2) inference distillation for ranking capability inheritance;\n(3) gradient estimation for better model optimization.}\nBiGeaR framework is illustrated in Figure~\\ref{fig:framework}(a).\nThe notation table and pseudo-codes are attached in Appendix~\\ref{app:notation} and~\\ref{app:algo}.\n\n\n\n\n% \\subsection{Preliminaries}\n\n\\textbf{Preliminaries: graph convolution.} Its general idea is to learn node representations by iteratively propagating and aggregating latent features of neighbors via the graph topology~\\cite{wu2019simplifying,lightgcn,kipf2016semi}. \nWe adopt the graph convolution paradigm working on the continuous space from LightGCN~\\cite{lightgcn} that recently shows good recommendation performance.\nLet {\\small$\\boldsymbol{v}^{(l)}_u, \\boldsymbol{v}^{(l)}_i\\in \\mathbb{R}^{d}$} denote the continuous feature embeddings of user $u$ and item $i$ computed after $l$ layers of information propagation. \n{\\small $\\mathcal{N}(x)$} represents $x$'s neighbor set.\nThey can be iteratively updated by utilizing information from the ($l$-$1$)-th layer: \n\\begin{small}\n\\begin{equation}\n\\label{eq:gcn}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\resizebox{0.9\\linewidth}{!}{$\n\\displaystyle\n\\boldsymbol{v}^{(l)}_u = \\sum_{i\\in \\mathcal{N}(u)} \\frac{1}{\\sqrt{|\\mathcal{N}(u)|\\cdot|\\mathcal{N}(i)|}}\\boldsymbol{v}^{(l-1)}_i, \\ \\ \\boldsymbol{v}^{(l)}_i = \\sum_{u\\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{|\\mathcal{N}(i)|\\cdot|\\mathcal{N}(u)|}}\\boldsymbol{v}^{(l-1)}_u.\n$}\n\\end{equation}\n\\end{small}%\n\n\n\\vspace{-0.1in}\n",
                "subsection 2.1": {
                    "name": "Graph Layer-wise Quantization",
                    "content": "\nWe propose the \\textit{graph layer-wise quantization} mainly by computing \\textbf{quantized embeddings} and \\textbf{embedding scalers}:\n(1) these quantized embeddings sketch the full-precision embeddings with $d$-dimensional binarized codes (i.e., $\\{-1, 1\\}^d$);\nand (2) each embedding scaler reveals the value range of original embedding entries. \nSpecifically, during the graph convolution at each layer, we track the intermediate information (e.g., {\\small $\\boldsymbol{v}^{(l)}_u$}) and perform the layer-wise 1-bit quantization in parallel as:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\boldsymbol{q}_u^{(l)} = \\sign\\big(\\boldsymbol{v}^{(l)}_u\\big), \\ \\ \\boldsymbol{q}_i^{(l)} = \\sign\\big(\\boldsymbol{v}^{(l)}_i\\big),\n\\end{equation}\n\\end{small}%\nwhere embedding segments {\\small$\\boldsymbol{q}_u^{(l)}$, $\\boldsymbol{q}_i^{(l)}$ $\\in$ $\\{-1, 1\\}^d$} retain the node latent features directly from {\\small$\\boldsymbol{v}^{(l)}_u$} and {\\small$\\boldsymbol{v}^{(l)}_i$}.\nTo equip with the layer-wise quantized embeddings, we further include a layer-wise positive embedding scaler for each node (e.g., {\\small$\\alpha_u^{(l)}$ $\\in$ $\\mathbb{R}^+$}), such that {\\small$\\boldsymbol{v}^{(l)}_u$ $\\doteq$ $\\alpha_u^{(l)}$$\\boldsymbol{q}^{(l)}_u$}. Then for each entry in {\\small$\\alpha_u^{(l)}$$\\boldsymbol{q}^{(l)}_u$}, it is still binarized by {\\small$\\{-\\alpha_u^{(l)}$}, {\\small$\\alpha_u^{(l)}\\}$}.\nIn this work, we compute the mean of L1-norm as:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\alpha_u^{(l)} = \\frac{1}{d}\\cdot||\\boldsymbol{v}_u^{(l)}||_1, \\ \\ \\alpha_i^{(l)} = \\frac{1}{d} \\cdot ||\\boldsymbol{v}_i^{(l)}||_1.\n\\end{equation}\n\\end{small}%\nInstead of setting {\\small $\\alpha_u^{(l)}$} and {\\small $\\alpha_i^{(l)}$} as learnable, such \\textit{deterministic} computation is simple yet effective to provide the scaling functionality whilst substantially pruning the parameter search space. The experimental demonstration is in Appendix~\\ref{app:scaler}. \n\nAfter $L$ layers of quantization and scaling, we have built the following \\textbf{binarized embedding table} for each graph node $x$ as:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\mathcal{A}_x = \\{\\alpha_x^{(0)}, \\alpha_x^{(1)}, \\cdots, \\alpha_x^{(L)}\\}, \\ \\ \\mathcal{Q}_x = \\{\\boldsymbol{q}^{(0)}_x, \\boldsymbol{q}^{(1)}_x, \\cdots, \\boldsymbol{q}^{(L)}_x\\}.\n\\end{equation}\n\\end{small}%\nFrom the technical perspective, BiGeaR binarizes the intermediate semantics at different layers of {\\cyk the receptive field~\\cite{velivckovic2017graph,wu2020comprehensive} for each node}.\nThis, essentially, achieves the \\textbf{magnification effect of feature uniqueness} to enrich user-item representations via the interaction graph exploration. We leave the analysis in~\\cref{sec:necessity}.\n\n\n\\vspace{-0.1in}\n"
                },
                "subsection 2.2": {
                    "name": "Prediction Acceleration",
                    "content": "\n\\textbf{Model Prediction.}\nBased on the learned embedding table, we predict the matching scores by adopting the inner product:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\label{eq:score}\n\\widehat{y}_{u,i} =  \\big<f(\\mathcal{A}_u, \\mathcal{Q}_u), f(\\mathcal{A}_i, \\mathcal{Q}_i)\\big>,\n\\end{equation}\n\\end{small}%\nwhere function $f(\\cdot, \\cdot)$ in this work is implemented as:  \n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\label{eq:useQ}\nf(\\mathcal{A}_u, \\mathcal{Q}_u) = \\Big|\\Big|_{l=0}^L w_l  \\alpha_u^{(l)} \\boldsymbol{q}^{(l)}_u, \\ \\ f(\\mathcal{A}_i, \\mathcal{Q}_i) = \\Big|\\Big|_{l=0}^L w_l \\alpha_i^{(l)} \\boldsymbol{q}^{(l)}_i. \n\\end{equation}\n\\end{small}%\nHere $\\big|\\big|$ represents concatenation of binarized embedding segments, in which weight $w_l$ measures the contribution of each segment in prediction.\n$w_l$ can be defined as a hyper-parameter or a learnable variable (e.g., with attention mechanism~\\cite{velivckovic2017graph}).\nIn this work, we set $w_l$ $\\propto$ $l$ to linearly increase $w_l$ for segments from lower-layers to higher-layers, mainly for its computational simplicity and stability.\n\n\n\n\\textbf{Computation Acceleration.}\nNotice that for each segment of {\\small $f(\\mathcal{A}_u, \\mathcal{Q}_u)$}, e.g., {\\small $w_l\\alpha_u^{(l)} \\boldsymbol{q}^{(l)}_u$}, entries are binarized by two values (i.e., {\\small$-w_l  \\alpha_u^{(l)}$} or {\\small$w_l  \\alpha_u^{(l)}$}). \nThus, we can achieve the prediction acceleration by decomposing Equation~(\\ref{eq:score}) with \\textit{bitwise operations}. \nConcretely, in practice, {\\small$\\boldsymbol{q}^{(l)}_u$} and {\\small$\\boldsymbol{q}^{(l)}_i$} will be firstly encoded into basic $d$-bits binary codes, denoted by {\\small$\\boldsymbol{{\\ddot q}}^{(l)}_u, \\boldsymbol{{\\ddot q}}^{(l)}_i \\in \\{0,1\\}^d$}.\nThen we replace Equation~(\\ref{eq:score}) by introducing the following formula:\n\\begin{small}\n\\begin{equation}\n\\label{eq:bit}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\widehat{y}_{u,i} = \\sum_{l=0}^{L} w_l^2\\alpha_u^{(l)}\\alpha_i^{(l)}\\cdot \\big(2{\\tt Popcount} \\big({\\tt XNOR}(\\boldsymbol{{\\ddot q}}^{(l)}_u, \\boldsymbol{{\\ddot q}}^{(l)}_i  )\\big) - d\\big).\n\\end{equation}\n\\end{small}%\nCompared to the original computation approach in Equation~(\\ref{eq:score}), our bitwise-operation-supported prediction in Equation~(\\ref{eq:bit}) reduces the number of floating-point operations (\\#FLOP) with {\\tt Popcount} and {\\tt XNOR}. We illustrate an example in Figure~\\ref{fig:framework}(b).\n\n\\vspace{-0.1in}\n"
                },
                "subsection 2.3": {
                    "name": "Self-supervised Inference Distillation",
                    "content": "\nTo alleviate the \\textit{asymmetric inference capability} issue between full-precision representations and binarized ones, we introduce the \\textit{self-supervised inference distillation} such that binarized embeddings can well inherit the inference knowledge from full-precision ones.\nGenerally, we treat our full-precision intermediate embeddings (e.g., {\\small $\\boldsymbol{v}_u^{(l)}$}) as the \\textbf{teacher} embeddings and the quantized segments as the \\textbf{student} embeddings.\nGiven both teacher and student embeddings, we can obtain their prediction scores as {\\small $\\widehat{{y}}_{u,i}^{tch}$} and {\\small $\\widehat{{y}}_{u,i}^{std}$}.\nFor Top-K recommendation, then our target is to reduce their discrepancy as:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\argmin \\mathcal{D}(\\widehat{{y}}_{u,i}^{tch}, \\widehat{{y}}_{u,i}^{std}).\n\\end{equation}\n\\end{small}%\nA straightforward implementation of function $\\mathcal{D}$ from the conventional \\textit{knowledge distillation}~\\cite{hinton2015distilling,anil2018large} is to minimize their Kullback-Leibler divergence (KLD) or mean squared error (MSE). \nDespite their effectiveness in classification tasks (e.g., visual recognition~\\cite{anil2018large,xie2020self}), they may not be appropriate for Top-K recommendation as:\n\\begin{itemize}[leftmargin=*,topsep=2pt,parsep=0.5pt]\n\\item Firstly, both KLD and MSE in $\\mathcal{D}$ encourage the student logits (e.g., {\\small $\\widehat{{y}}_{u,i}^{std}$}) to be similarly distributed with the teacher logits in a macro view. But for ranking tasks, they may not well learn the relative order of user preferences towards items, which, however, is the key to improving Top-K recommendation capability.\n\n\\item Secondly, they both develop the distillation over the whole item corpus, which may be computational excessive for realistic model training.\nAs the item popularity usually follows the Long-tail distribution~\\cite{park2008long,tang2018ranking}, learning the relative order of those frequently interacted items located at the tops of ranking lists actually contributes more to the Top-K recommendation performance.\n\\end{itemize}\n\nTo develop effective inference distillation, we propose to extract additional \\textit{pseudo-positive training samples} from teacher embeddings to regularize the targeted embeddings on each convolutional layer. \nConcretely, let $\\sigma$ represent the activation function (e.g., Sigmoid). We first pre-train the \\textbf{teacher} embeddings to minimize the \\textit{Bayesian Personalized Ranking} (BPR) loss~\\cite{rendle2012bpr}:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{-2pt}\n\\label{eq:hd-bpr}\n\\mathcal{L}^{tch}_{BPR} = -\\sum_{u \\in \\mathcal{U}} \\sum_{i\\in \\mathcal{N}(u) \\atop j\\notin \\mathcal{N}(u)} \\ln \\sigma(\\widehat{y}^{\\,tch}_{u,i} - \\widehat{y}^{\\,tch}_{u,j}),\n\\end{equation}\n\\end{small}%\nwhere {\\small $\\mathcal{L}^{tch}_{BPR}$} forces the prediction of an observed interaction to be higher than its unobserved counterparts, and the teacher score {\\small $\\widehat{y}^{\\,tch}_{u,i}$} is computed as {\\small $\\widehat{y}^{\\,tch}_{u,i} = \\big<\\big|\\big|_{l=0}^L w_l\\boldsymbol{v}_u^{(l)}, \\big|\\big|_{l=0}^L w_l\\boldsymbol{v}_i^{(l)}\\big>$}.\nPlease notice that we only disable binarization and its associated gradient estimation in pre-training.\nAfter it is well-trained, for each user $u$, we retrieve the layer-wise teacher inference towards all items $\\mathcal{I}$:\n\\begin{small}\n\\begin{equation}\n\\label{eq:tch_emb}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{-2pt}\n\\widehat{\\emb{y}}^{\\,tch, (l)}_{u} = \\big< w_l\\widehat{\\emb{v}}_u^{(l)},  w_l\\widehat{\\emb{v}}_i^{(l)}\\big>_{i \\in \\mathcal{I}}.\n\\end{equation}\n\\end{small}%\nBased on the segment scores {\\small $\\boldsymbol{\\widehat{y}}_u^{\\,tch, (l)}$} at the $l$-th layer, we first sort out Top-R items with the highest matching scores, denoted by {\\small $S^{(l)}_{tch}(u)$}. And hyper-parameter R $\\ll$ $\\mathcal{I}$.\nInspired by~\\cite{tang2018ranking}, then we propose our layer-wise inference distillation as follows:\n\\begin{small}\n\\begin{equation}\n\\label{eq:tch_layer_wise}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\resizebox{0.9\\linewidth}{!}{$\n\\displaystyle\n\\mathcal{L}_{ID}(u) = \\sum_{l=0}^L \\mathcal{L}_{ID}^{(l)}(\\widehat{\\boldsymbol{y}}_u^{\\,std, (l)}, S^{(l)}_{tch}(u)) = -\\frac{1}{R} \\sum_{l=0}^L \\sum^{R}_{k=1} w_k \\cdot \\ln\\sigma(\\widehat{y}_{u,S^{(l)}_{tch}(u, k)}^{\\,std, (l)}),\n$}\n\\end{equation}\n\\end{small}%\nwhere student scores {\\small $\\widehat{\\emb{y}}^{\\,std, (l)}_{u}$} is computed similarly to Equation~(\\ref{eq:tch_emb}) and {\\small $S^{(l)}_{tch}(u, k)$} returns the $k$-th high-scored item from the pseudo-positive samples.\n{\\small $w_k$} is the ranking-aware weight presenting two major effects:\n(1) since samples in {\\small $S^{(l)}_{tch}(u)$} are not necessarily all ground-truth positive, $w_k$ thus balances their contribution to the overall loss;\n(2) it dynamically adjusts the weight importance for different ranking positions in {\\small $S^{(l)}_{tch}(u)$}.\nTo achieve these, $w_k$ can be developed by following the parameterized geometric distribution for approximating the tailed item popularity~\\cite{rendle2014improving}:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\label{eq:wk}\nw_k = \\lambda_1 \\exp(-\\lambda_2\\cdot k),\n\\end{equation}\n\\end{small}%\nwhere {\\small $\\lambda_1$} and {\\small $\\lambda_2$} control the loss contribution level and sharpness of the distribution. \nIntuitively, {\\small $\\mathcal{L}_{ID}$} encourages highly-recommended items from full-precision embeddings to more frequently appear in the student's inference list.\nMoreover, our distillation approach regularizes the embedding quantization in a layer-wise manner as well;\nthis will effectively narrow their inference discrepancy for a more correlated recommendation capability.\n\n\\textbf{Objective Function.}\nCombining {\\small $\\mathcal{L}^{std}_{BPR}$} that calculates BPR loss (similar to Equation~(\\ref{eq:hd-bpr})) with the student predictions from Equation~(\\ref{eq:score}) and {\\small $\\mathcal{L}_{ID}$} for all training samples, our final objective function for learning embedding binarization is defined as:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\mathcal{L} = \\mathcal{L}^{std}_{BPR} + \\mathcal{L}_{ID} + \\lambda ||\\Theta||_2^2, \n\\end{equation}\n\\end{small}%\nwhere {\\small $||\\Theta||_2^2$} is the $L$2-regularizer of node embeddings parameterized by hyper-parameter {\\small $\\lambda$} to avoid over-fitting.\n\n\n\n \n\\vspace{-0.1in}\n"
                },
                "subsection 2.4": {
                    "name": "Gradient Estimation",
                    "content": "\n\\label{sec:gradient}\nAlthough \\textit{Straight-Through Estimator (STE)}~\\cite{bengio2013estimating} enables an executable gradient flow for backpropagation, it however may cause the issue of inconsistent optimization direction in forward and backward propagation: as the integral of the constant 1 in STE is a linear function, other than $\\sign(\\cdot)$ function. \nTo furnish more accurate gradient estimation, in this paper, we utilize the approximation of \\textit{Dirac delta function}~\\cite{bracewell1986fourier} for gradient estimation.\n\n\n\n\nConcretely, let {\\small $u(\\phi)$} denote the \\textit{unit-step function}, a.k.a., Heaviside step function~\\cite{step_function}, where {\\small $u(\\phi)$ $=$ $1$} for {\\small $\\phi$ $>$ $0$} and {\\small $u(\\phi)$ $=$ $0$} otherwise. \nObviously, we can take a translation from step function to $\\sign(\\cdot)$ by {\\small $\\sign(\\phi)$ $=$ 2$u(\\phi)$ - 1}, and thus theoretically {\\small $\\frac{\\partial \\sign(\\phi)}{\\partial \\phi}$ $=$ $2\\frac{\\partial u(\\phi)}{\\partial \\phi}$}. As for {\\small $\\frac{\\partial u(\\phi)}{\\partial \\phi}$}, it has been proved~\\cite{bracewell1986fourier} that, {\\small $\\frac{\\partial u(\\phi)}{\\partial \\phi}$ $=$ $0$ if $\\phi$ $\\neq$ $0$}, and {\\small $\\frac{\\partial u(\\phi)}{\\partial \\phi}$ $=$ $\\infty$} otherwise, which exactly is the \\textit{Dirac delta function}, a.k.a., the unit impulse function {\\small $\\delta(\\cdot)$}~\\cite{bracewell1986fourier} shown in Figure~\\ref{fig:gradient}(a).\nHowever, it is still intractable to directly use {\\small $\\delta(\\cdot)$} for gradient estimation. \nA feasible solution is to approximate {\\small $\\delta(\\cdot)$} by introducing zero-centered Gaussian probability density as follows:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n \\delta(\\phi) = \\lim_{\\beta \\rightarrow \\infty} \\frac{|\\beta|}{\\sqrt{\\pi}} \\exp(-(\\beta\\phi)^2),\n\\end{equation}\n\\end{small}%\nwhich imlies that: \n\\begin{small}\n\\begin{equation}\n\\label{eq:norm_gradient}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\frac{\\partial \\sign(\\phi)}{\\partial \\phi} \\doteq \\frac{2\\gamma}{\\sqrt{\\pi}} \\exp(-(\\gamma\\phi)^2).\n\\end{equation}\n\\end{small}%\nAs shown in Figure~\\ref{fig:gradient}(b)-(c), hyper-parameter {\\small $\\gamma$} determines the sharpness of the derivative curve for approximation to $\\sign(\\cdot)$.\n\n\nIntuitively, our proposed gradient estimator follows the main direction of factual gradients with $\\sign(\\cdot)$ in model optimization. \nThis will produce a coordinated value quantization from continuous embeddings to quantized ones, and thus a series of evolving gradients can be estimated for the inputs with diverse value ranges. \nAs we will show in~\\cref{exp:gradient} of experiments, our gradient estimator can work better than other recent estimators~\\cite{gong2019differentiable,qin2020forward,darabi2018bnn,sigmoid,RBCN}.\n\n\n\n\n\n\n\n% \\vspace{-0.05in}\n"
                }
            },
            "section 3": {
                "name": "Model Analysis",
                "content": "\n\\label{sec:analysis}\n\n",
                "subsection 3.1": {
                    "name": "Magnification of Feature Uniqueness",
                    "content": "\n\\label{sec:necessity}\nWe take user $u$ as an example for illustration and the following analysis can be popularized to other nodes without loss of generality.\nSimilar to sensitivity analysis in statistics~\\cite{koh2017understanding} and influence diffusion in social networks~\\cite{xu2018representation}, we measure how the latent feature of a distant node $x$ finally affects $u$'s representation segments before binarization (e.g., {\\small $\\boldsymbol{v}^{(l)}_u$}), supposing $x$ is a multi-hop neighbor of $u$.\nWe denote the \\textbf{feature enrichment ratio} {\\small $\\mathbb{E}_{x \\rightarrow u}^{(l)}$} as the L1-norm of Jacobian matrix {\\small $\\begin{matrix}\\left[{\\partial \\boldsymbol{v}^{(l)}_u}/\\ {\\partial \\boldsymbol{v}^{(0)}_{x_u}} \\right]\\end{matrix}$}, by detecting the absolute influence of all fluctuation in entries of {\\small $\\boldsymbol{v}^{(0)}_{x}$} to {\\small $\\boldsymbol{v}^{(l)}_u$}, i.e., {\\small $\\mathbb{E}_{x\\rightarrow u}^{(l)}$ = $\\begin{matrix}\\left|\\left| \\left[{\\partial \\boldsymbol{v}^{(l)}_u} /\\ {\\partial \\boldsymbol{v}^{(0)}_{x}} \\right] \\right|\\right|_1\\end{matrix}$}.\nFocusing on a $l$-length path $h$ connected by the node sequence: {\\small $x_h^l$, $x_h^{l-1}$, $\\cdots$, $x_h^1$, $x_h^0$}, where {\\small $x_h^l$ = $u$} and {\\small $x_h^0$ = $x$}, we follow the chain rule to develop the derivative decomposition as: \n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\begin{aligned}\n\\frac{\\partial \\boldsymbol{v}^{(l)}_u}{\\partial \\boldsymbol{v}^{(0)}_{x}}  = \\sum_{h=1}^H\n\\begin{matrix}\n\\left[\\frac{\\partial \\boldsymbol{v}^{(l)}_{x_h^l}}{\\partial \\boldsymbol{v}^{(0)}_{x_h^0}} \\right]_h\n\\end{matrix}  \n& = \\sum_{h=1}^H \\prod_{k=l}^1\n\\begin{matrix} \\frac{1}{\\sqrt{|\\mathcal{N}(x^k_h)}|} \\cdot \\frac{1}{\\sqrt{|\\mathcal{N}(x^{k-1}_h)}|} \\cdot \\boldsymbol{I}\\end{matrix} \\\\\n& = \\begin{matrix} \\sqrt{\\frac{|\\mathcal{N}(u)|}{|\\mathcal{N}(x)|}} \\end{matrix} \\sum_{h=1}^H \\prod_{k=1}^l \\begin{matrix} \\frac{1}{|\\mathcal{N}(x^k_h)|}  \\cdot \\boldsymbol{I} \\end{matrix},\\\\\n\\end{aligned}\n% & = \\sqrt{\\frac{|\\mathcal{N}(u)|}{|\\mathcal{N}(x_u)|}}  \\cdot \\mathbb{E}\\text{ [random walk]?}\n\\end{equation}\n\\end{small}%\nwhere $H$ is the number of paths between $u$ and $x$ in total.\nSince all factors in the computation chain are positive, then:\n\\begin{small}\n\\begin{equation}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\begin{matrix}\n\\mathbb{E}_{x\\rightarrow u}^{(l)} = \\left|\\left| \\left[\\frac{\\partial \\boldsymbol{v}^{(l)}_u}{\\partial \\boldsymbol{v}^{(0)}_{x}} \\right]\\right|\\right|_1 = d \\cdot \\sqrt{\\frac{|\\mathcal{N}(u)|}{|\\mathcal{N}(x)|}}\\end{matrix}  \\cdot \\sum_{h=1}^H \\prod_{k=1}^l \\begin{matrix} \\frac{1}{|\\mathcal{N}(x^k_h)|}\\end{matrix}.\n\\end{equation}\n\\end{small}%\nNote that here the term {\\small $\\sum_{h=1}^H \\prod_{k=1}^l 1/|\\mathcal{N}(x^k_h)|$} is exactly the probability of the $l$-length random walk starting at $u$ that finally arrives at $x$, which can be interpreted as:\n\\begin{small}\n\\begin{equation}\n\\label{eq:prob}\n\\setlength\\abovedisplayskip{2pt}\n\\setlength\\belowdisplayskip{2pt}\n\\mathbb{E}_{x\\rightarrow u}^{(l)} \\propto \\frac{1}{\\sqrt{|\\mathcal{N}(x)|}} \\cdot Prob(\\text{$l$-step random walk from $u$ arrives at $x$}).\n\\end{equation}\n\\end{small}%\n\n\\textbf{Magnification Effect of Feature Uniqueness.}\nEquation~(\\ref{eq:prob}) implies that, with the equal probability to visit adjacent neighbors, {distant nodes with fewer degrees (i.e., {\\small $|\\mathcal{N}(x)|$}) will contribute more feature influence to user $u$.\nBut most importantly, in practice, these $l$-hop neighbors of user $u$ usually represent certain \\textit{esoteric} and \\textit{unique} objects with less popularity.\nBy collecting the intermediate information in different depth of the graph convolution, we can achieve the \\textbf{feature magnification effect} for all unique nodes that live within $L$ hops of graph exploration, which finally enrich $u$'s semantics in all embedding segments for quantization.\n\n% \\vspace*{0.3\\baselineskip} \n% \\textbf{Comparison with Conventional Methodology.}\n% State-of-the-art quantized recommender model~\\cite{hashgnn} detaches the embedding quantization into two steps:\n% (1) it firstly learns node embeddings via a 2-layer GCN model~\\cite{graphsage};\n% then (2) it feeds the outputs of all convolution iterations into $\\sign(\\cdot)$ for embedding binarization.\n% This mainly takes inspiration from the quantization methodology in Computer Vision literature~\\cite{rastegari2016xnor,qin2020forward,erin2015deep,lin2017towards} to plug quantization as a follow-up encoder that is posterior to the neural architecture.\n% However, since graph data is non-Euclidean structured, a straightforward work adaptation based on Euclidean-structured data (e.g., images) may not effectively capture the rich semantics within the user-item interaction graph for quantization preparation.\n% As we will show later in~\\cref{sec:lw_exp}, even by continually increasing the embedding dimension $d$, conventional quantization methodology will draw a very limited and slow model improvement.\n% On the contrary, our proposed graph layer-wise quantization can learn informative quantized embeddings more effectively for better performance.\n\n\n\n\n\n\n\\subsection{Complexity Analysis}\n\\label{sec:complexity}\nTo discuss the feasibility for realistic deployment, we compare BiGeaR with the best full-precision model LightGCN~\\cite{lightgcn}, as they are \\textit{end-to-end} with offline model training and online prediction. \n\n\\textbf{Training Time Complexity.}\n{\\small $M$}, {\\small $N$}, and {\\small $E$} represent the number of users, items, and edges; {\\small $S$} and {\\small $B$} are the epoch number and batch size.\nWe use BiGeaR$_{tch}$ and BiGeaR$_{std}$ to denote the pre-training version and binarized one, respectively.\nAs we can observe from Table~\\ref{tab:time}, \n(1) both BiGeaR$_{tch}$ and BiGeaR$_{std}$ takes asymptotically similar complexity of graph convolution with LightGCN (where BiGeaR$_{std}$ takes additional {\\small $O(2Sd(L+1)E)$} complexity for binarization).\n(2) For {\\small $\\mathcal{L}_{BPR}$} computation, to prevent \\textit{over-smoothing} issue~\\cite{li2019deepgcns,li2018deeper}, usually {\\small $L\\leq 4$}; compare to the convolution operation, the complexity of {\\small $\\mathcal{L}_{BPR}$} is acceptable. \n(3) For {\\small $\\mathcal{L}_{ID}$} preparation, after the training of BiGeaR$_{tch}$, we firstly obtain the layer-wise prediction scores with {\\small $O(MNd(L+1))$} time complexity and then sort out the Top-R pseudo-positive samples for each user with {\\small $O(N + R\\ln R)$}.\nFor BiGeaR$_{std}$, it takes a layer-wise inference distillation from BiGeaR$_{tch}$ with {\\small $O\\big(SRd(L+1)E \\big)$}.\n(4) To estimate the gradients for BiGeaR$_{std}$, it takes {\\small $O(2Sd(L+1)E)$} for all training samples. \n\n \\begin{table}[tbh]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\footnotesize\n\\caption{Traing time complexity.}\n\\vspace{-0.05in}\n\\label{tab:time}\n\\setlength{\\tabcolsep}{0.1mm}{\n\\begin{tabular}{c | c | c | c}\n\\toprule\n  ~      & {\\footnotesize LightGCN}     &  {\\footnotesize BiGeaR$_{tch}$} \t\t&  {\\footnotesize BiGeaR$_{std}$}\\\\\n\\midrule\n\\midrule\n{\\scriptsize Graph Normalization}           &   {\\scriptsize $O\\big(2E\\big)$}           & {\\scriptsize $O\\big(2E\\big)$}  \t& {\\scriptsize -}\\\\\n\\midrule[0.1pt]\n{\\scriptsize Conv. and Quant.}   &   {\\scriptsize $O\\big(\\frac{2SdE^2L}{B}\\big)$}     & {\\scriptsize $O\\big(\\frac{2SdE^2L}{B}\\big)$}    & {\\scriptsize $O\\big(2Sd(\\frac{E^2L}{B}+(L+1)E)\\big)$} \\\\\n\\midrule[0.1pt]\n{\\scriptsize $\\mathcal{L}_{BPR}$ Loss}    &    {\\scriptsize $O\\big(2SdE\\big)$}      & {\\scriptsize $O\\big(2Sd(L+1)E\\big)$}    & {\\scriptsize $O\\big(2Sd(L+1)E\\big)$} \\\\\n\\midrule[0.1pt]\n{\\scriptsize $\\mathcal{L}_{ID}$ Loss}    &    {\\scriptsize -}      & {\\scriptsize $O\\big(MNd(L+1)(N+R\\ln R)\\big)$}    & {\\scriptsize $O\\big(SRd(L+1)E \\big)$} \\\\\n\\midrule[0.1pt]\n{\\scriptsize Gradient Estimation} &   {\\scriptsize -}       & {\\scriptsize -}  & {\\scriptsize $O\\big(2Sd(L+1)E\\big)$}\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\\vspace*{0.3\\baselineskip} \n\\textbf{Memory overhead and Prediction Acceleration.}\nWe measure the memory footprint of embedding tables for online prediction.\nAs we can observe from the results in Table~\\ref{tab:prediction}:\n(1) Theoretically, the embedding size ratio of our model over LightGCN is {\\small $\\frac{32d}{(L+1)(32+d)}$}. Normally, {\\small $L\\leq 4$} and {\\small $d\\geq 64$}, thus our model achieves at least 4$\\times$ space cost compression.\n(2) As for the prediction time cost, we compare the number of binary operations (\\#BOP) and floating-point operations (\\#FLOP) between our model and LightGCN.\nWe find that BiGeaR replaces most of floating-point arithmetics (e.g., multiplication) with bitwise operations. \n% Furthermore, our model has fewer FLOPs than LightGCN as {\\small $6 \\ll 2(L+1)d$}.\n\\begin{table}[tbh]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\footnotesize\n\\caption{Complexity of space cost and online prediction.}\n\\vspace{-0.05in}\n\\label{tab:prediction}\n\\setlength{\\tabcolsep}{2.2mm}{\n\\begin{tabular}{c | c | c c}\n\\toprule\n  ~          & {\\footnotesize Embedding size}   &  {\\footnotesize \\#FLOP}\t& {\\footnotesize \\#BOP}     \t\\\\\n\\midrule\n\\midrule\n {\\scriptsize LightGCN}      & {\\scriptsize $O\\big(32(M+N)d\\big)$}       &   {\\scriptsize $O\\big(2MNd\\big)$}      &   {-}       \t\\\\\n\\midrule[0.1pt]\n{\\scriptsize BiGeaR}       & {\\scriptsize $O\\big((M+N)(L+1)(32+d)\\big)$}     & {\\scriptsize $O\\big(4MN(L+1)\\big)$}            & {\\scriptsize $O\\big(2MN(L+1)d\\big)$}   \t\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n% \\vspace{-0.05in}\n\\section{Experimental Results}\n\\label{sec:exp}\nWe evaluate our model on Top-K recommendation task with the aim of answering the following research questions:\n\\begin{itemize}[leftmargin=*,topsep=0.5pt,parsep=0.5pt]\n\\item \\textbf{RQ1.} How does BiGeaR perform compared to state-of-the-art full-precision and quantization-based models?\n\n\\item \\textbf{RQ2.} How is the practical resource consumption of BiGeaR?\n\n\\item \\textbf{RQ3.} How do proposed components affect the performance?\n\n\\end{itemize} \n\n\\subsection{Experiment Setup}\n\n\\noindent\\textbf{Datasets.}\nTo guarantee the fair comparison, we directly use five widely evaluated datasets (including the training/test splits) from: MovieLens\\footnote{\\url{https://grouplens.org/datasets/movielens/1m/}}~\\cite{hashgnn,he2016fast,chen2021modeling,chen2021attentive}, Gowalla\\footnote{\\url{https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/gowalla}}~\\cite{ngcf,hashgnn,lightgcn,dgcf}, Pinterest\\footnote{\\url{https://sites.google.com/site/xueatalphabeta/dataset-1/pinterest_iccv}}~\\cite{geng2015learning,hashgnn}, Yelp2018\\footnote{\\url{https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/yelp2018}}~\\cite{ngcf,dgcf,lightgcn}, Amazon-Book\\footnote{\\url{https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/amazon-book}}~\\cite{ngcf,dgcf,lightgcn}.\nDataset statistics and descriptions are reported in Table~\\ref{tab:datasets} and Appendix~\\ref{app:dataset}.\n\n\n\\begin{table}[h]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\small\n\\caption{The statistics of datasets.}\n\\vspace{-0.05in}\n\\label{tab:datasets}\n\\setlength{\\tabcolsep}{0.7mm}{\n\\begin{tabular}{c | c  c  c  c  c}\n\\toprule \n             & { MovieLens}  & { Gowalla}   & { Pinterest}  &  { Yelp2018} & { Amazon-Book}\\\\\n\\midrule\n\\midrule[0.1pt]\n    {\\#Users}  & {6,040}   & {29,858}   & {55,186}   & {31,668}  &{52,643}  \\\\ \n    {\\#Items}  & {3,952}   & {40,981}   & {9,916}    & {38,048}  &{91,599}  \\\\\n\\midrule[0.1pt]\n    {\\#Interactions} & {1,000,209} & {1,027,370} & {1,463,556} & {1,561,406} & {2,984,108} \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\n\\noindent\\textbf{Evaluation Metric.}\nIn Top-K recommendation evaluation, we select two widely-used evaluation protocols Recall@K and NDCG@K to evaluate Top-K recommendation capability. \n\n\n\n\\vspace*{0.3\\baselineskip} \n\\noindent\\textbf{Competing Methods.}\nWe include the following recommender models: \n(1) 1-bit quantization-based methods including graph-based (GumbelRec~\\cite{gumbel1,gumbel2,zhang2019doc2hash}, HashGNN~\\cite{hashgnn}) and general model-based (LSH~\\cite{lsh}, HashNet~\\cite{hashnet}, CIGAR~\\cite{kang2019candidate}), \nand (2) full-precision  models including neural-network-based (NeurCF~\\cite{neurcf}) and graph-based (NGCF~\\cite{ngcf}, DGCF~\\cite{dgcf}, LightGCN~\\cite{lightgcn}).\nDetailed introduction of these methods are attached in Appendix~\\ref{app:method}.\n\n\n\nWe exclude early quantization-based recommendation baselines, e.g., CH~\\cite{liu2014collaborative}, DiscreteCF~\\cite{zhang2016discrete},  DPR~\\cite{zhang2017discrete}, and full-precision solutions, e.g., GC-MC~\\cite{berg2017graph}, PinSage~\\cite{pinsage}, mainly because the above competing models~\\cite{kang2019candidate,lightgcn,ngcf,neurcf} have validated the superiority.\n\n\\vspace*{0.3\\baselineskip} \n\n\\noindent\\textbf{Experiment Settings.}\nOur model is implemented by Python 3.7 and PyTorch 1.14.0 with non-distributed training. \nThe experiments are run on a Linux machine with 1 NVIDIA V100 GPU, 4 Intel Core i7-8700 CPUs, 32 GB of RAM with 3.20GHz.\nFor all the baselines, we follow the official reported hyper-parameter settings, and for methods lacking recommended settings, we apply a grid search for hyper-parameters.\nThe embedding dimension is searched in \\{$32, 64, 128, 256, 512, 1024$\\}. \nThe learning rate $\\eta$ is tuned within \\{$10^{-4}, 10^{-3}, 10^{-2}$\\} and the coefficient of $L2$ normalization $\\lambda$ is tuned among \\{$10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}$\\}. \nWe initialize and optimize all models with default normal initializer and Adam optimizer~\\cite{adam}. \nWe report all hyper-parameters in Appendix~\\ref{app:parameter} for reproducibility.\n\n\n\n\\begin{table*}[tbh]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\small\n  \\caption{\\small Performance comparison (waveline and underline represent the best performing full-precision and quantization-based models). }\n  \\vspace{-0.05in}\n  \\label{tab:top20}\n  \\setlength{\\tabcolsep}{1.4mm}{\n  % \\addvbuffer[1pt 1pt]{\n  \\begin{tabular}{c|c c|c c|c c|c c|c c} \n    \\toprule\n    \\multirow{2}*{Model} & \\multicolumn{2}{c|}{MovieLens (\\%)} & \\multicolumn{2}{c|}{Gowalla (\\%)} & \\multicolumn{2}{c|}{Pinterest (\\%)} & \\multicolumn{2}{c|}{Yelp2018 (\\%)} &  \\multicolumn{2}{c}{Amazon-Book (\\%)} \\\\\n        ~ & Recall@20 & NDCG@20  & Recall@20 & NDCG@20  & Recall@20  & NDCG@20  & Recall@20  & NDCG@20  & Recall@20  & NDCG@20 \\\\\n    \\midrule\n    \\midrule\n    NeurCF           & {21.40} $\\pm$ {\\footnotesize 1.51}   & {37.91} $\\pm$ {\\footnotesize 1.14}   & {14.64} $\\pm$ {\\footnotesize 1.75}  & {23.17} $\\pm$ {\\footnotesize 1.52} & {12.28} $\\pm$ {\\footnotesize 1.88} & {13.41} $\\pm$ {\\footnotesize 1.13}   & {4.28} $\\pm$ {\\footnotesize 0.71}   & {7.24} $\\pm$ {\\footnotesize 0.53}  &{3.49} $\\pm$ {\\footnotesize 0.75}  &{6.71} $\\pm$ {\\footnotesize 0.72} \\\\\n    NGCF             & {24.69} $\\pm$ {\\footnotesize 1.67}  & {39.56} $\\pm$ {\\footnotesize 1.26}   & {16.22} $\\pm$ {\\footnotesize 0.90}  & {24.18} $\\pm$ {\\footnotesize 1.23}  & {14.67} $\\pm$ {\\footnotesize 0.56}  & {13.92} $\\pm$ {\\footnotesize 0.44} & {5.89} $\\pm$ {\\footnotesize 0.35} & {9.38} $\\pm$ {\\footnotesize 0.52}   &{3.65} $\\pm$ {\\footnotesize 0.73} &{6.90} $\\pm$ {\\footnotesize 0.65} \\\\\n    DGCF             & {25.28} $\\pm$ {\\footnotesize 0.39}  & {45.98} $\\pm$ {\\footnotesize 0.58}   & {18.64} $\\pm$ {\\footnotesize 0.30}  & {25.20} $\\pm$ {\\footnotesize 0.41} & {\\uwave{15.52}} $\\pm$ {\\footnotesize 0.42} & {\\uwave{16.51}} $\\pm$ {\\footnotesize 0.56} & {6.37} $\\pm$ {\\footnotesize 0.55}  & {11.08} $\\pm$ {\\footnotesize 0.48}   &{4.32} $\\pm$ {\\footnotesize 0.34} &{7.73} $\\pm$ {\\footnotesize 0.27}\\\\\n    LightGCN          & {\\uwave{26.28}}  $\\pm$ {\\footnotesize 0.20}  & {\\uwave{46.04}} $\\pm$ {\\footnotesize 0.18}   & {\\uwave{19.02}}  $\\pm$ {\\footnotesize 0.19}  & {\\uwave{25.71}} $\\pm$ {\\footnotesize 0.25}  & {15.33} $\\pm$ {\\footnotesize 0.28}  & {16.29} $\\pm$ {\\footnotesize 0.24}  & {\\uwave{6.79}} $\\pm$ {\\footnotesize 0.31}  & {\\uwave{12.17}} $\\pm$ {\\footnotesize 0.27} &{\\uwave{4.84}} $\\pm$ {\\footnotesize 0.09}  &{\\uwave{8.11}} $\\pm$ {\\footnotesize 0.11} \\\\\n    \\midrule[0.1pt]\n    \\cellcolor{best}\\textbf{BiGeaR}  &{\\textbf{25.57}} $\\pm$ {\\footnotesize 0.33} &{\\textbf{45.56}} $\\pm$ {\\footnotesize 0.31} &{\\textbf{18.36}} $\\pm$ {\\footnotesize 0.14} &{\\textbf{24.96}} $\\pm$ {\\footnotesize 0.17} &{\\textbf{15.57}} $\\pm$ {\\footnotesize 0.22} &{\\textbf{16.83}} $\\pm$ {\\footnotesize 0.46} &{\\textbf{6.47}} $\\pm$ {\\footnotesize 0.14} &{\\textbf{11.60}} $\\pm$ {\\footnotesize 0.18} &{\\textbf{4.68}} $\\pm$ {\\footnotesize 0.11} &{\\textbf{8.12}} $\\pm$ {\\footnotesize 0.12} \\\\   \n    \\textbf{Capability} &{97.30\\%} &{98.96\\%} &{96.53\\%} &{97.08\\%} &{100.32\\%} &{101.94\\%} &{95.29\\%} &{95.32\\%} &{96.69\\%} &{100.12\\%} \\\\\n    \\midrule \n    LSH             & {11.38} $\\pm$ {\\footnotesize 1.23}  & {14.87} $\\pm$ {\\footnotesize 0.76}   & {8.14} $\\pm$ {\\footnotesize 0.98}   & {12.19} $\\pm$ {\\footnotesize 0.86} & {7.88} $\\pm$ {\\footnotesize 1.21}  & {9.84} $\\pm$ {\\footnotesize 0.90}   & {2.91} $\\pm$ {\\footnotesize 0.51}   & {5.06} $\\pm$ {\\footnotesize 0.67} &{2.41}\t$\\pm$ {\\footnotesize 0.95} &{4.39}\t$\\pm$ {\\footnotesize 1.16} \\\\\n    HashNet         & {15.43} $\\pm$ {\\footnotesize 1.73} & {24.78} $\\pm$ {\\footnotesize 1.50}   & {11.38} $\\pm$ {\\footnotesize 1.25}   & {16.50} $\\pm$ {\\footnotesize 1.42} & {10.27}  $\\pm$ {\\footnotesize 1.48} & {11.64} $\\pm$ {\\footnotesize 0.91}  & {3.37}  $\\pm$ {\\footnotesize 0.78}  & {7.31} $\\pm$ {\\footnotesize 1.16}  &{2.86}\t$\\pm$ {\\footnotesize 1.51}\t&{4.75}\t$\\pm$ {\\footnotesize 1.33} \\\\\n    CIGAR & {14.84} $\\pm$ {\\footnotesize 1.44} & {24.63} $\\pm$ {\\footnotesize 1.77}   & {11.57} $\\pm$ {\\footnotesize 1.01}   & {16.77} $\\pm$ {\\footnotesize 1.29} & {10.34}  $\\pm$ {\\footnotesize 0.97} & {11.87} $\\pm$ {\\footnotesize 1.20}  & {3.65}  $\\pm$ {\\footnotesize 0.90}  & {7.87} $\\pm$ {\\footnotesize 1.03}  &{3.05}  $\\pm$ {\\footnotesize 1.32}  &{4.98} $\\pm$ {\\footnotesize 1.24} \\\\\n    GumbelRec       & {16.62} $\\pm$ {\\footnotesize 2.17} & {29.36} $\\pm$ {\\footnotesize 2.53} & {12.26}  $\\pm$ {\\footnotesize 1.58} & {17.49} $\\pm$ {\\footnotesize 1.08} & {10.53} $\\pm$ {\\footnotesize 0.79} & {11.86} $\\pm$ {\\footnotesize 0.86} & {3.85} $\\pm$ {\\footnotesize 1.39} & {7.97} $\\pm$ {\\footnotesize 1.59} &{2.69} $\\pm$ {\\footnotesize 0.55} &{4.32}  $\\pm$ {\\footnotesize 0.47} \\\\\n    HashGNN$_h$     & {14.21} $\\pm$ {\\footnotesize 1.67} & {24.39} $\\pm$ {\\footnotesize 1.87}  & {11.63} $\\pm$ {\\footnotesize 1.47}  & {16.82} $\\pm$ {\\footnotesize 1.35}  & {10.15} $\\pm$ {\\footnotesize 1.43} & {11.96}  $\\pm$ {\\footnotesize 1.58} & {3.77}  $\\pm$ {\\footnotesize 1.02} & {7.75} $\\pm$ {\\footnotesize 1.39} &{3.09}\t$\\pm$ {\\footnotesize 1.29} &{5.19} $\\pm$ {\\footnotesize 1.03}\t\\\\\n    HashGNN$_s$     & {\\underline{19.87}} $\\pm$ {\\footnotesize 0.93} & {\\underline{37.32}} $\\pm$ {\\footnotesize 0.81}  & {\\underline{13.45}} $\\pm$ {\\footnotesize 0.65}  & {\\underline{19.12}} $\\pm$ {\\footnotesize 0.68} & {\\underline{12.38}}  $\\pm$ {\\footnotesize 0.86} & {\\underline{13.63}} $\\pm$ {\\footnotesize 0.75} & {\\underline{4.86}} $\\pm$ {\\footnotesize 0.36} & {\\underline{8.83}} $\\pm$ {\\footnotesize 0.27} &{\\underline{3.34}} $\\pm$ {\\footnotesize 0.25} \t&{\\underline{5.82}} $\\pm$ {\\footnotesize 0.24}\t\\\\\n    \\midrule[0.1pt]\n    \\cellcolor{best}\\textbf{BiGeaR}  &{\\textbf{25.57}} $\\pm$ {\\footnotesize 0.33} &{\\textbf{45.56}} $\\pm$ {\\footnotesize 0.31} &{\\textbf{18.36}} $\\pm$ {\\footnotesize 0.14} &{\\textbf{24.96}} $\\pm$ {\\footnotesize 0.17} &{\\textbf{15.57}} $\\pm$ {\\footnotesize 0.22} &{\\textbf{16.83}} $\\pm$ {\\footnotesize 0.46} &{\\textbf{6.47}} $\\pm$ {\\footnotesize 0.14} &{\\textbf{11.60}} $\\pm$ {\\footnotesize 0.18} &{\\textbf{4.68}} $\\pm$ {\\footnotesize 0.11} &{\\textbf{8.12}} $\\pm$ {\\footnotesize 0.12} \\\\   \n    \\textbf{Gain}   &{28.69\\%} &{22.08\\%} &{36.51\\%} &{30.54\\%} &{25.77\\%} &{23.48\\%} &{33.13\\%} &{31.37\\%}  &{40.12\\%} &{39.52\\%} \\\\\n    \\textbf{$p$-value} &{5.57e-7} &{2.64e-8} &{2.21e-7} &{7.69e-8} &{2.5e-5} &{3.51e-5} &{3.27e-6} &{5.30e-8} &{3.49e-6} &{7.14e-8} \\\\\n    \\bottomrule\n  \\end{tabular}}\n  % }\n\\end{table*}\n\n\n\\subsection{Performance Analysis (RQ1)}\nWe evaluate Top-K recommendation by varying K in \\{20, 40, 60, 80, 100\\}.\nWe summarize the Top@20 results in Table~\\ref{tab:top20} for detailed comparison and plot the Top-K recommendation curves in Appendix~\\ref{app:topk}. From Table~\\ref{tab:top20}, we have the following observations:\n\\begin{itemize}[leftmargin=*,topsep=0.5pt,parsep=0.5pt]\n\\item \\textbf{Our model offers a competitive recommendation capability to state-of-the-art full-precision recommender models.}\n(1) BiGeaR generally outperforms most of full-precision recommender models excluding LightGCN over five benchmarks.\nThe main reason is that our model and LightGCN take similar graph convolution methodology with network simplification~\\cite{lightgcn}, e.g., removing self-connection and feature transformation, which is proved to be effective for Top-K ranking and recommendation.\nMoreover, BiGeaR collects the different levels of interactive information in multi depths of graph exploration, which significantly enriches semantics to user-item representations for binarization.\n(2) Compared to the state-of-the-art method LightGCN, our model develops about 95\\%$\\sim$102\\% of performance capability \\textit{w.r.t.} Recall@20 and NDCG@20 throughout all datasets.\nThis shows that our proposed model designs are effective to narrow the performance gap with full-precision model LightGCN. \nAlthough the binarized embeddings learned by BiGeaR may not achieve the \\textit{exact} expressivity parity with the full-precision ones learned by LightGCN, considering the advantages of space compression and inference acceleration that we will present later, we argue that such performance capability is acceptable, especially for those resource-limited deployment scenarios. \n\n\n\\item \\textbf{Compared to all binarization-based recommender models, BiGeaR presents the empirically remarkable and statistically significant performance improvement.}\n(1) Two conventional methods (LSH, HashNet) for general item retrieval tasks underperform CIGAR, HashGNN and BiGeaR, showing that a direct model adaptation may be too trivial for Top-K recommendation. \n(2) Compared to CIGAR, graph-based models generally work better.\nThis is mainly because, CIGAR combines general neural networks with \\textit{learning to hash} techniques for fast candidate generation;\non the contrary, graph-based models are more capable of exploring multi-hop interaction subgraphs to directly simulate the high-order \\textit{collaborative filtering} process for model learning.\n(3) Our model further outperforms HashGNN by about 26\\%$\\sim$40\\% and 22\\%$\\sim$40\\% \\textit{w.r.t.} Recall@20 and NDCG@20, proving the effectiveness of our proposed multi-faceted optimization components in embedding binarization. \n(4) Moreover, the significance test in which $p$-value $<$ 0.05 indicates that the improvements over all five benchmarks are statistically significant.\n\n\\end{itemize}\n\n\n\n\\subsection{Resource Consumption Analysis (RQ2)}\nWe analyze the resource consumption in \\textit{training}, \\textit{online inference}, and \\textit{memory footprint} by comparing to the best two competing models, i.e., LightGCN and HashGNN.\nDue to the page limits, we report the empirical results of MovieLens dataset in Table~\\ref{tab:consumption}.\n\\begin{enumerate}[leftmargin=*,topsep=0.5pt,parsep=0.5pt]\n\\item $T_{train}$: we set batch size $B=2048$ and dimension size $d=256$ for all models. \nWe find that HashGNN is fairly time-consuming than LightGCN and BiGeaR. \nThis is because HashGNN adopts the early GCN framework~\\cite{graphsage} as the backbone; LightGCN and BiGeaR utilize more simplified graph convolution architecture in which operations such as self-connection, feature transformation, and nonlinear activation are all removed~\\cite{lightgcn}.\nFurthermore, BiGeaR needs 5.1s and 6.2s per epoch for pretraining and quantization, both of which take slightly more yet asymptotically similar time cost with LightGCN, basically following the complexity analysis in~\\cref{sec:complexity}.\n\n\\item $T_{infer}$: we randomly generate 1,000 queries for online prediction and conduct experiments with the vanilla NumPy\\footnote{{\\url{https://www.lfd.uci.edu/~gohlke/pythonlibs/}}} on CPUs.\nWe observe that HashGNN$_s$ takes a similar time cost with LightGCN.\nThis is because, while HashGNN$_h$ purely binarizes the continuous embeddings, its relaxed version HashGNN$_s$ adopts a Bernoulli random variable to provide the probability of replacing the quantized digits with original real values~\\cite{hashgnn}.\nThus, although HashGNN$_h$ can use Hamming distance for prediction acceleration, HashGNN$_s$ with the improved recommendation accuracy can only be computed by floating-point arithmetics.\nAs for BiGeaR, thanks to its bitwise-operation-supported capability, it runs about 8$\\times$ faster than LightGCN whilst maintaining the similar performance on MovieLens dataset.\n\n\\item $S_{ET}$: we only store the embedding tables that are necessary for online inference. \nAs we just explain, HashGNN$_s$ interprets embeddings by randomly selected real values, which, however, leads to the expansion of space consumption. \nIn contrast to HashGNN$_s$, BiGeaR can separately store the binarized embeddings and corresponding scalers, making a balanced trade-off between recommendation accuracy and space overhead.\n\\end{enumerate}\n\n\n\\begin{table}[th]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\footnotesize\n\\caption{Resource consumption on MovieLens dataset.}\n\\vspace{-0.05in}\n\\label{tab:consumption}\n\\setlength{\\tabcolsep}{2.5mm}{\n\\begin{tabular}{c | c  c  c  c}\n\\toprule\n              & LightGCN   &  HashGNN$_h$ &  HashGNN$_s$  &  BiGeaR\\\\\n\\midrule[0.1pt]\n\\midrule[0.1pt]\n  $T_{train}\\mathbin{/}${\\scriptsize \\#epcoch}     &   {4.91s}     &   {186.23s}  & {204.53s}    &   {(5.16+6.22)s}\\\\\n\\midrule[0.1pt]\n   $T_{infer}\\mathbin{/}${\\scriptsize \\#query}    & {32.54ms}    & {2.45ms}  & {31.76ms}   & {3.94ms}\\\\\n\\midrule[0.1pt]\n  $S_{ET}$    & {9.79MB}    & {0.34MB}  & {9.78MB}   & {1.08MB}\\\\\n\\midrule\n\\midrule\n  Recall@20    & {26.28\\%}    & {14.21\\%}  & {19.87\\%}   & {25.57\\%}\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{figure}[tp]\n\\begin{minipage}{0.5\\textwidth}\n\\hspace{-0.1in}\n\\includegraphics[width=3.5in]{figs/dim_layer}\n\\end{minipage} \n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\vspace{-0.05in}\n\\caption{Study of graph layer-wise quantization. }\n\\label{fig:dim_layer}\n\\end{figure}\n \n\\subsection{Study of Layer-wise Quantization (RQ3.A)}\n\\label{sec:lw_exp}\nTo verify the magnification of feature uniqueness in layer-wise quantization, we modify BiGeaR and propose two variants, denoted as BiGeaR$_{w/o\\,LW}$ and BiGeaR$_{w/o\\,FU}$. \nWe report the results in Figure~\\ref{fig:dim_layer} by denoting Recall@20 and NDCG@20 in red and blue, respectively, and vary color brightness for different variants. From these results, we have the following explanations.\n\\begin{itemize}[leftmargin=*,topsep=0.5pt,parsep=0.5pt]\n\\item Firstly, BiGeaR$_{w/o\\,LW}$ discards the layer-wise quantization and adopts the conventional manner by quantizing the last outputs from $L$ convolution iterations. \nWe fix dimension $d=256$ and vary layer number $L$ for BiGeaR, and only vary dimension $d$ for variant BiGeaR$_{w/o\\,LW}$ with fixed $L=2$.\n(1) Even by continuously increasing the dimension size from 64 to 1024, BiGeaR$_{w/o\\,LW}$ slowly improves both Recall@20 and NDCG@20 performance. \n(2) By contrast, our layer-wise quantization presents a more efficient capability in improving performance by increasing $L$ from 0 to 3.\nWhen $L=4$, BiGeaR usually exhibits a conspicuous performance decay, mainly because of the common \\textit{over-smoothing issue} in graph-based models~\\cite{li2019deepgcns,li2018deeper}.\nThus, with a moderate size $d=256$ and convolution number $L \\leq 3$, BiGeaR can attain better performance with acceptable computational complexity.\n\n\\item Secondly, BiGeaR$_{w/o\\,FU}$ omits the feature magnification effect by adopting the way used in HashGNN~\\cite{graphsage,hashgnn} as:\n\\begin{equation}\n\\setlength\\abovedisplayskip{0pt}\n\\setlength\\belowdisplayskip{0pt}\n\\begin{matrix}\n\\boldsymbol{v}^{(l)}_x = \\sum_{z\\in \\mathcal{N}(x)} \\frac{1}{|\\mathcal{N}(z)|}\\boldsymbol{v}^{(l-1)}_z.\n\\end{matrix}\n\\end{equation}\nSimilar to the analysis in~\\cref{sec:necessity}, such modification will finally disable the ``magnification term'' in Equation~(\\ref{eq:prob}) and simplify it to the vanilla random walk for graph exploration. \nAlthough BiGeaR$_{w/o\\,FU}$ presents similar curve trends with BiGeaR when $L$ increases, the general performance throughout all five datasets is unsatisfactory compared to BiGeaR.\nThis validates the effectiveness of BiGeaR's effort in magnifying unique latent features, which enriches user-item representations and boosts Top-K recommendation performance accordingly.\n\\end{itemize}\n\n\n\n\n\n\\subsection{Study of Inference Distillation (RQ3.B)} \n\n\\subsubsection{\\textbf{Effect of Layer-wise Distillation.}}\nWe study the effectiveness of our inference distillation by proposing two ablation variants, namely \\textsl{noID} and \\textsl{endID}.\nWhile \\textsl{noID} totally removes our information distillation in model training, \\textsl{endID} downgrades the original layer-wise distillation to only distill information from the last layer of graph convolution. \nAs shown in Table~\\ref{tab:distillation}, both \\textsl{noID} and \\textsl{endID} draw notable performance degradation.\nFurthermore, the performance gap between \\textsl{endID} and BiGeaR shows that it is efficacious to conduct our inference distillation in a layer-wise manner for further performance enhancement.\n\n\n\\begin{table}[h]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\footnotesize\n\\caption{Learning inference distillation.}\n\\vspace{-0.05in}\n\\label{tab:distillation}\n\\setlength{\\tabcolsep}{0.6mm}{\n\\begin{tabular}{c |c c|c c|c c|c c|c c}\n\\toprule\n \\multirow{2}*{Variant} & \\multicolumn{2}{c|}{MovieLens} & \\multicolumn{2}{c|}{Gowalla} & \\multicolumn{2}{c|}{Pinterest} & \\multicolumn{2}{c|}{Yelp2018} & \\multicolumn{2}{c}{Amazon-book} \\\\\n               ~  & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20\\\\\n\\midrule\n\\midrule\n    \\multirow{2}*{\\footnotesize \\textsl{noID}}    &{24.40}& {44.06}   & {17.85}& {24.28}  & {15.23}& {16.38}  & {6.18} & {11.22} & {4.07} & {7.31}\\\\\n~        &\\textit{\\color{blue} \\scriptsize{-4.58\\%}}  &\\textit{\\color{blue} \\scriptsize{-3.29\\%}}  &\\textit{\\color{blue} \\scriptsize{-2.78\\%}}  &\\textit{\\color{blue} \\scriptsize{-2.72\\%}}  &\\textit{\\color{blue} \\scriptsize{-2.18\\%}}  &\\textit{\\color{blue} \\scriptsize{-2.85\\%}}  &\\textit{\\color{blue} \\scriptsize{-4.48\\%}}  &\\textit{\\color{blue} \\scriptsize{-3.28\\%}} &\\textit{\\color{blue} \\scriptsize{-13.03\\%}}  &\\textit{\\color{blue}  \\scriptsize{-9.98\\%}}\\\\\n  \\midrule[0.1pt]\n    \\multirow{2}*{\\footnotesize \\textsl{endID}}    &{25.02}& {44.75}   & {18.05}& {24.73}  & {15.28}& {16.58}  & {6.29} & {11.37} & {4.44} & {7.78}\\\\\n~        &\\textit{\\color{blue} \\scriptsize{-2.15\\%}}  &\\textit{\\color{blue} \\scriptsize{-1.78\\%}}  &\\textit{\\color{blue} \\scriptsize{-1.69\\%}}  &\\textit{\\color{blue} \\scriptsize{-0.92\\%}}  &\\textit{\\color{blue} \\scriptsize{-1.86\\%}}  &\\textit{\\color{blue} \\scriptsize{-1.49\\%}}  &\\textit{\\color{blue} \\scriptsize{-2.78\\%}}  &\\textit{\\color{blue} \\scriptsize{-1.98\\%}} &\\textit{\\color{blue} \\scriptsize{-5.13\\%}}  &\\textit{\\color{blue}  \\scriptsize{-4.19\\%}}\\\\\n\\midrule[0.1pt]\n  \\multirow{2}*{\\footnotesize \\textsl{KLD}}    &{24.32}& {44.38}   & {17.63}& {24.07}  & {14.78}& {15.92}  & {5.83} & {10.36} & {4.13} & {7.21}\\\\\n  ~        &\\textit{\\color{blue} \\scriptsize{-4.89\\%}}  &\\textit{\\color{blue} \\scriptsize{-2.59\\%}}  &\\textit{\\color{blue} \\scriptsize{-3.98\\%}}  &\\textit{\\color{blue} \\scriptsize{-3.57\\%}}  &\\textit{\\color{blue} \\scriptsize{-5.07\\%}}  &\\textit{\\color{blue} \\scriptsize{-5.41\\%}}  &\\textit{\\color{blue} \\scriptsize{-9.89\\%}}  &\\textit{\\color{blue} \\scriptsize{-10.69\\%}} &\\textit{\\color{blue} \\scriptsize{-11.75\\%}}  &\\textit{\\color{blue}  \\scriptsize{-11.21\\%}}\\\\\n  \\midrule[0.1pt]\n\\cellcolor{best}{\\footnotesize \\textbf{BiGeaR} }  &\\textbf{25.57}& \\textbf{45.56}   & \\textbf{18.36}& \\textbf{24.96}  & \\textbf{15.57}& \\textbf{16.83}  & \\textbf{6.47}& \\textbf{11.60} & \\textbf{4.68}& \\textbf{8.12}\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\n\\subsubsection{\\textbf{Conventional Knowledge Distillation.}}\nTo compare with the conventional approach, we modify BiGeaR by applying KL divergence for layer-wise teacher and student logits, i.e., {\\small $\\boldsymbol{\\widehat{y}}_u^{\\,tch, (l)}$} v.s. {\\small $\\boldsymbol{\\widehat{y}}_u^{\\,std, (l)}$}.\nWe denote this variant as \\textsl{KLD}.\nAs we can observe from Table~\\ref{tab:distillation}, using conventional knowledge distillation with KL divergence leads to suboptimal performance. \nThis is because KL divergence encourages the teacher and student training objects to have a similar logit distribution, but users' relative order of item preference can not be well learned from this process.\nCompared to the conventional approach, our proposed layer-wise Inference distillation is thus more effective for ranking information distillation.\n\n\n\n\\subsection{Study of Gradient Estimation (RQ3.C)}\n\\label{exp:gradient}\nWe compare our gradient estimation with several recently studied estimators, such as \\textit{Tanh-like}~\\cite{qin2020forward,gong2019differentiable}, \\textit{SSwish}~\\cite{darabi2018bnn}, \\textit{Sigmoid}~\\cite{sigmoid}, and \\textit{projected-based estimator}~\\cite{RBCN} (denoted as PBE), by implementing them in BiGeaR.\nWe report their Recall@20 in Figure~\\ref{fig:quant_f} and compute the performance gain of our approach over these estimators accordingly. \nWe have two main observations:\n\\begin{enumerate}[leftmargin=*,topsep=0.5pt,parsep=0.5pt]\n\\item Our proposed approach shows the consistent superiority over all other gradient estimators.\nThese estimators usually use \\textit{visually similar} functions, e.g., tanh($\\cdot$), to approximate $\\sign(\\cdot)$.\nHowever, these functions are not necessarily \\textit{theoretically relevant} to $\\sign(\\cdot)$. This may lead to inaccurate gradient estimation.\nOn the contrary, as we explain in~\\cref{sec:gradient}, we transfer the unit-step function $u(\\cdot)$ to $\\sign(\\cdot)$ by $\\sign(\\cdot)$ = 2$u(\\cdot)$ - 1.\nThen we can further estimate the gradients of $\\sign(\\cdot)$ with the approximated derivatives of $u(\\cdot)$.\nIn other words, our approach follows the main optimization direction of factual gradients with $\\sign(\\cdot)$;\nand different from previous solutions, this guarantees the coordination in both forward and backward propagation. \n\n\\item Furthermore, compared to the last four datasets, MovieLens dataset confronts a larger performance disparity between our approach and others.\nThis is because MovieLens dataset is much denser than the other datasets, i.e., $\\frac{\\#Interactions}{\\#Users \\cdot \\#Items}$ = 0.0419 $\\gg$ \\{0.00084, 0.00267, 0.0013, 0.00062\\}, which means that users tend to have more item interactions and complicated preferences towards different items. \nConsequently, this posts a higher requirement for the gradient estimation capability in learning ranking information.\nFortunately, the empirical results in Figure~\\ref{fig:quant_f} demonstrate that our solution well fulfills these requirements, especially for dense interaction graphs.\n\\end{enumerate}\n\n\\begin{figure}[t]\n\\begin{minipage}{0.5\\textwidth}\n% \\hspace{-0.2in}\n\\includegraphics[width=3.3in]{figs/estimator_recall}\n\\end{minipage} \n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\vspace{-0.15in}\n\\caption{Gradient estimator comparison \\textit{w.r.t.} Recall@20.}\n\\label{fig:quant_f}\n\\end{figure}\n\n\n\n\n\n% \\vspace{-0.05in}\n\\section{Related Work}\n\\label{sec:work}\n\n\\textbf{Full-precision recommender models.}\n(1) \\textit{Collaborative Filtering (CF)} is a prevalent methodology in modern recommender systems~\\cite{covington2016deep,pinsage,yang2022hrcf}. \nEarlier CF methods, e.g., \\textit{Matrix Factorization}~\\cite{koren2009matrix,rendle2012bpr}, reconstruct historical interactions to learn user-item embeddings.\nRecent neural-network-based models, e.g., NeurCF~\\cite{neurcf} and attention-based models~\\cite{chen2017attentive,he2018nais}, further boost performance with neural networks.\n(2) \\textit{Graph-based} methods focus on studying the interaction graph topology for recommendation. \nGraph convolutional networks (GCNs)~\\cite{graphsage,kipf2016semi} inspire early work, e.g., GC-MC~\\cite{berg2017graph}, PinSage~\\cite{pinsage}, and recent models, e.g., NGCF~\\cite{ngcf}, DGCF~\\cite{dgcf}, and LightGCN~\\cite{lightgcn}, mainly because they can well simulate the high-order CF signals among high-hop neighbors for recommendation.\n\n% \\vspace*{0.3\\baselineskip} \n\n\\textbf{Learning to hash.}\nHashing-based methods map dense floating-point embeddings into binary spaces for \\textit{Approximate Nearest Neighbor} (ANN) search acceleration.\nA representative model LSH~\\cite{lsh} has inspired a series of work for various tasks, e.g., fast retrieval of images~\\cite{hashnet}, documents~\\cite{li2014two,zhang2020discrete}, and categorical information~\\cite{kang2021learning}.\nFor Top-K recommendation, models like DCF~\\cite{zhang2016discrete}, DPR~\\cite{zhang2017discrete} include neural network architectures.\nA recent work CIGAR~\\cite{kang2019candidate} proposes adaptive model designs for fast candidate generation.\nTo investigate the graph structure of user-item interactions, model HashGNN~\\cite{hashgnn} applies hashing techniques into graph neural networks for recommendation.\nHowever, one major issue is that solely using learned binary codes for prediction usually draws a large performance decay.\nThus, to alleviate the issue, CIGAR further equips with additional full-precision recommender models (e.g., BPR-MF~\\cite{rendle2012bpr}) for fine-grained \\textit{re-ranking};\nHashGNN proposes relaxed version by mixing full-precision and binary embedding codes.\n\n% \\vspace*{0.3\\baselineskip} \n\n\\textbf{Quantization-based models.}\n% Quantization is a popular topic in CV~\\cite{rastegari2016xnor,qin2020forward,erin2015deep,lin2017towards} and NLP~\\cite{bai2020binarybert,shen2020q}.\nQuantization-based models share similar techniques with hashing-based methods, e.g., $\\sign(\\cdot)$ is usually adopted mainly because of its simplicity.\nHowever, quantization-based models do not pursue extreme encoding compression, and thus they develop multi-bit, 2-bit, and 1-bit quantization for performance adaptation.\nRecently, there is growing attention to quantize graph-based models, such as Bi-GCN~\\cite{bigcn} and BGCN~\\cite{bahri2021binary}\nHowever, these two models are mainly designed for geometric classification tasks, but their capability in product recommendation is unclear.\nThus, in this paper, we propose BiGeaR to learn 1-bit user-item representation quantization for Top-K recommendation. \nDifferent from binary hashing-based methods, BiGeaR aims to make predictions within its own framework, making a balanced trade-off between efficiency and performance.\n\n\n\n% \\vspace{-0.05in}\n\\section{Conclusion and Future Work}\n\\label{sec:con}\nIn this paper, we present BiGeaR to learn binarized graph representations for recommendation with multi-faceted binarization techniques.\nThe extensive experiments not only validate the performance superiority over competing binarization-based recommender systems, but also justify the effectiveness of all proposed model components.\nIn the future, we plan to investigate two major possible problems. \n(1) It is worth developing binarization techniques for model-agnostic recommender systems with diverse learning settings~\\cite{song2021semi,yang2021discrete,song2022graph}. \n(2) Instead of using $\\sign(\\cdot)$ for quantization, developing compact multi-bit quantization methods with similarity-preserving is promising to improve ranking accuracy.\n\n\n% \\clearpage\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\n\n\\begin{acks}\nThe work described in this paper was partially supported by the National Key Research and Development Program of China (No. 2018AAA0100204), the Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 2410021, Research Impact Fund, No. R5034-18), and the CUHK Direct Grant (4055147).\n\\end{acks}\n\n% \\newpage\n\n% \\newpage\n\\bibliographystyle{ACM-Reference-Format}\n{\n\\bibliography{ref}\n}\n%%\n%% If your work has an appendix, this is the place to put it.\n\\newpage\n% \\balance\n\\clearpage\n% \\balance\n\\appendix\n\\setcounter{table}{0}\n\\setcounter{figure}{0}\n\n\\begin{table}[th]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\caption {Notations and meanings. }\n\\vspace{-0.05in}\n\\label{tab:notation}\n  \\footnotesize\n  \\begin{tabular}{c|l} \n     \\hline\n          {\\bf Notation} & {\\bf Explanation}\\\\\n    \\hline\\hline\n      $d$, $L$    & {Embedding dimensions and graph convolution layers.} \\\\\n    \\hline\n      $\\mathcal{U}$, $\\mathcal{I}$ & Collection of users and items. \\\\\n    \\hline\n       $\\mathcal{N}(x)$ & {Neighbors of node $x$.} \\\\\n    \\hline\n          $\\boldsymbol{v}_{x}^{(l)}$ & {Full-precision embedding of node $x$ at $l$-th convolution.}\\\\\n    \\hline\n          $\\boldsymbol{q}_{x}^{(l)}$ & {Binarized embedding of of node $x$ at $l$-th quantization.}\\\\\n    \\hline\n        $\\alpha_{x}^{(l)}$ & {$l$-th embedding scaler of node $x$.}\\\\\n    \\hline\n        $\\mathcal{A}_x$ and $\\mathcal{Q}_x$ & {Binarized embedding table of $x$ learned by BiGeaR. } \\\\\n    \\hline\n        $w_l$   & {$l$-th weight in predicting matching score.} \\\\\n    \\hline\n         $y_{u,i}$     & {A scalar indicates the existence of user-item interaction.} \\\\\n    \\hline \n      $\\widehat{y}^{tch}_{u,i}$  &   {Predicted score based on full-precision embeddings.}\\\\\n    \\hline\n    $\\widehat{y}^{std}_{u,i}$  &   {Predicted score based on binarized embeddings.}\\\\\n    \\hline\n      $\\emb{\\widehat{y}}^{tch,\\,(l)}_{u}$  & {Predicted scores of $u$ based on $l$-th embeddings segments.}\\\\\n    \\hline\n      $\\emb{\\widehat{y}}^{std,\\,(l)}_{u}$  & {Predicted scores of $u$ based on $l$-th quantized segments.}\\\\\n    \\hline \n      $S_{tch}^{(l)}(u)$ & {pseudo-positive training samples of $u$.}\\\\\n    \\hline\n      $w_k$   &   {$k$-th weight in inference distillation loss.}\\\\\n    \\hline     \n      $\\mathcal{L}_{BPR}^{tch}$, $\\mathcal{L}_{BPR}^{std}$  & {BPR loss based on full-precision and binarized scores.} \\\\\n    \\hline     \n      $\\mathcal{L}_{ID}$  & {Inference distillation loss.} \\\\\n    \\hline     \n      $\\mathcal{L}$ & {Objective function of BiGeaR.} \\\\\n    \\hline\n      $u(\\cdot)$, $\\delta(\\cdot)$ & Unit-step function and Dirac delta function.\\\\\n    \\hline\n      $\\lambda$, $\\lambda_1$, $\\lambda_2$, $\\gamma$, $\\eta$ & {Hyper-parameters and the learning rate.} \\\\\n    \\hline\n  \\end{tabular}\n\\end{table}\n\n\n\\section{Notation Table}\n\\label{app:notation}\nWe list key notations in Table~\\ref{tab:notation}.\n\n\n\\begin{figure*}[tbh]\n\\begin{minipage}{1\\textwidth}\n\\hspace{-0.05in}\n\\includegraphics[width=7in]{figs/topk.eps}\n\\end{minipage} \n\\vspace{-0.05in}\n\\caption{Top-K recommendation curve.}\n\\label{fig:topk}\n\\end{figure*}\n\n\\section{Pseudo-codes of BiGeaR}\n\\label{app:algo}\nThe pseudo-codes of BiGeaR are attached in Algorithm~\\ref{alg:bigear}.\n\n\n\\begin{algorithm}[t]\n\\footnotesize\n\\caption{BiGeaR algorithm.}\n\\label{alg:bigear}\n\\LinesNumbered  \n\\KwIn{Interaction graph; trainable embeddings {\\footnotesize $\\boldsymbol{v}_{\\{\\cdots\\}}$}; hyper-parameters: {\\footnotesize $L$, $\\eta$, $\\lambda$, $\\lambda_1$, $\\lambda_2$, $\\gamma$. } }\n\\KwOut{Prediction function $\\mathcal{F}(u,i)$} \n$\\mathcal{A}_u \\gets \\emptyset$, $\\mathcal{A}_i \\gets \\emptyset$, $\\mathcal{Q}_u \\gets \\emptyset$, $\\mathcal{Q}_i \\gets \\emptyset$;\\\\\n\\While{\\rm{BiGeaR not converge}}{\n    \\For{$l = 1, \\cdots, L$}{\n         $\\boldsymbol{v}^{(l)}_u \\gets\\sum_{i\\in \\mathcal{N}(u)} \\frac{1}{\\sqrt{|\\mathcal{N}(u)|\\cdot|\\mathcal{N}(i)|}}\\boldsymbol{v}^{(l-1)}_i$, \\\\ \n         $\\boldsymbol{v}^{(l)}_i\\gets \\sum_{u\\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{|\\mathcal{N}(i)|\\cdot|\\mathcal{N}(u)|}}\\boldsymbol{v}^{(l-1)}_u$. \\\\\n          \\If{{\\color{blue} with inference distillation}}{\n              $\\boldsymbol{q}_u^{(l)}\\gets \\sign\\big(\\boldsymbol{v}^{(l)}_u\\big), \\ \\ \\boldsymbol{q}_i^{(l)}\\gets \\sign\\big(\\boldsymbol{v}^{(l)}_i\\big)$, \\\\\n              $\\alpha_u^{(l)} \\gets \\frac{||\\boldsymbol{V}_u^{(l)}||_1}{d}$, $\\alpha_i^{(l)} \\gets \\frac{||\\boldsymbol{V}_i^{(l)}||_1}{d}$; \\\\\n              Update ($\\mathcal{A}_u$, $\\mathcal{Q}_u$), ($\\mathcal{A}_i$, $\\mathcal{Q}_i$) with $\\alpha_u^{(l)}\\boldsymbol{q}_u^{(l)}$, $\\alpha_i^{(l)}\\boldsymbol{q}_i^{(l)}$; \\\\              \n          }\n        }\n\n     $\\widehat{y}^{\\,tch}_{u,i}\\gets \\Big<\\big|\\big|_{l=0}^L w_l\\boldsymbol{v}_u^{(l)}, \\big|\\big|_{l=0}^L w_l\\boldsymbol{v}_i^{(l)}\\Big>$. \\\\\n\n      \\If{{\\color{blue} with inference distillation}}{\n            $\\boldsymbol{q}_u^{(0)} \\gets \\sign\\big(\\boldsymbol{v}^{(0)}_u\\big), \\ \\ \\boldsymbol{q}_i^{(0)} \\gets \\sign\\big(\\boldsymbol{v}^{(0)}_i\\big)$, \\\\\n              $\\alpha_u^{(0)} \\gets \\frac{||\\boldsymbol{V}_u^{(0)}||_1}{d}$, $\\alpha_i^{(0)} \\gets \\frac{||\\boldsymbol{V}_i^{(0)}||_1}{d}$; \\\\\n              Update ($\\mathcal{A}_u$, $\\mathcal{Q}_u$), ($\\mathcal{A}_i$, $\\mathcal{Q}_i$) with $\\alpha_u^{(0)}\\boldsymbol{q}_u^{(0)}$, $\\alpha_i^{(0)}\\boldsymbol{q}_i^{(0)}$;\n            $\\widehat{y}^{\\,std}_{u,i} =  \\big<f(\\mathcal{A}_u, \\mathcal{Q}_u), f(\\mathcal{A}_i, \\mathcal{Q}_i)\\big>$; \\\\ \n             $\\{\\widehat{y}^{tch,\\,(l)}_{u,i}\\}_{l=0, 1, \\cdots, L} \\gets \\text{get score segments from} \\widehat{y}^{\\,tch}_{u,i}$; \\\\ \n             $\\{\\widehat{y}^{std,\\,(l)}_{u,i}\\}_{l=0, 1, \\cdots, L} \\gets \\text{get score segments from}  \\widehat{y}^{\\,std}_{u,i}$; \\\\ \n              $\\mathcal{L}_{ID} \\gets$ compute loss with {\\scriptsize $\\{\\widehat{y}^{tch,\\,(l)}_{u,i}\\}_{l=0, 1, \\cdots, L}$, $\\{\\widehat{y}^{std,\\,(l)}_{u,i}\\}_{l=0, 1, \\cdots, L}$}. \\\\\t\n             $\\mathcal{L} \\gets$ compute $\\mathcal{L}^{std}_{BPR} and \\mathcal{L}_{ID}$. \\\\\t\n\n      }\n      \\Else{\n            $\\mathcal{L} \\gets$ compute $\\mathcal{L}^{tch}_{BPR}$. \\\\ \t\n      }\n\n       Optimize BiGeaR with regularization;\\\\ \n\n}\n\\KwRet $\\mathcal{F}$.\\\\\n\\end{algorithm}\n\n\\section{Datasets}\n\\label{app:dataset}\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{MovieLens}~\\cite{hashgnn,he2016fast,chen2021modeling,chen2021attentive} is a widely adopted benchmark for movie recommendation. Similar to the setting in~\\cite{hashgnn,he2016fast,chen2021modeling}, $y_{u,i} = 1$ if user $u$ has an explicit rating score towards item $i$, otherwise $y_{u,i} = 0$. In this paper, we use the MovieLens-1M data split.\n\n\\item \\textbf{Gowalla}~\\cite{ngcf,hashgnn,lightgcn,dgcf} is the check-in dataset~\\cite{liang2016modeling} collected from Gowalla, where users share their locations by check-in. To guarantee the quality of the dataset, we extract users and items with no less than 10 interactions similar to~\\cite{ngcf,hashgnn,lightgcn,dgcf}. \n\n\\item \\textbf{Pinterest}~\\cite{geng2015learning,hashgnn} is an implicit feedback dataset for image recommendation~\\cite{geng2015learning}. Users and images are modeled in a graph. Edges represent the pins over images initiated by users. In this dataset, each user has at least 20 edges. \n% We delete repeated edges between users and images to avoid data leakage in model evaluation.  \n\n\\item \\textbf{Yelp2018}~\\cite{ngcf,dgcf,lightgcn} is collected from Yelp Challenge 2018 Edition, where local businesses such as restaurants are treated as items. We retain users and items with over 10 interactions similar to~\\cite{ngcf,dgcf,lightgcn}.\n\n\\item \\textbf{Amazon-Book}~\\cite{ngcf,dgcf,lightgcn} is organized from the book collection of Amazon-review for product recommendation~\\cite{he2016ups}. Similarly to~\\cite{ngcf,lightgcn,dgcf}, we use the 10-core setting to graph nodes.  \n\\end{itemize} \n\n\\section{Competing Methods}\n\\label{app:method}\n\\begin{itemize}[leftmargin=*,topsep=0.5pt,parsep=0.5pt]\n\\item \\textbf{LSH}~\\cite{lsh} is a representative hashing method to approximate the similarity search for massive high-dimensional data. We follow the adaptation in~\\cite{hashgnn} to it for Top-K recommendation. \n\n\\item \\textbf{HashNet}~\\cite{hashnet} is a state-of-the-art deep hashing method that is originally proposed for multimedia retrieval tasks.\nWe use the same adaptation strategy in~\\cite{hashgnn} to it for recommendation.\n\n\\item \\textbf{CIGAR}~\\cite{kang2019candidate} is a hashing-based method for fast item candidate generation, followed by complex full-precision re-ranking algorithms. We use its quantization part for fair comparison. \n\n\\item \\textbf{GumbelRec} is a variant of our model with the implementation of Gumbel-softmax for categorical variable quantization~\\cite{gumbel1,gumbel2,zhang2019doc2hash}.\nGumbelRec utilizes the Gumbel-softmax trick to replace $\\sign(\\cdot)$ function for embedding binarization.  \n\n\\item \\textbf{HashGNN}~\\cite{hashgnn} is the state-of-the-art end-to-end 1-bit quantization recommender system. \n\\textbf{HashGNN$_h$} denotes its vanilla \\textit{hard encoding} version; and \\textbf{HashGNN$_s$} is the relaxed version of replacing several quantized digits with the full-precision ones.\n\n\\item \\textbf{NeurCF}~\\cite{neurcf} is a classical neural network model to capture user-item nonlinear feature interactions for collaborative filtering.\n\n\\item \\textbf{NGCF}~\\cite{ngcf} is a state-of-the-art graph-based collaborative filtering model that largely follows the standard GCN~\\cite{kipf2016semi}. \n\n\\item \\textbf{DGCF}~\\cite{dgcf} is one of the latest graph-based model that learns disentangled user intents for better Top-K recommendation.\n\n\\item \\textbf{LightGCN}~\\cite{lightgcn} is another latest GCN-based recommender system that presents a more concise and powerful model structure \nwith state-of-the-art performance.\n\\end{itemize}\n\n\\section{Hyper-parameter Settings}\n\\label{app:parameter}\nWe report all hyper-parameter settings in Table~\\ref{tab:hyperparameter}.\n\n\n\\begin{table}[thb]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\small\n\\caption{Hyper-parameter settings for the five datasets.}\n\\vspace{-0.05in}\n\\label{tab:hyperparameter}\n\\setlength{\\tabcolsep}{1.5mm}{\n\\begin{tabular}{c | c  c  c  c  c}\n\\toprule\n                  & MovieLens   & Gowalla    & Pinterest     & Yelp2018  & Amazon-Book\\\\\n\\midrule \n\\midrule\n  $B$               &  2048       &   2048     & 2048        &  2048   &  2048  \\\\\n  $d$         &  256        &   256      &  256    & 256  & 256   \\\\\n  $\\eta$            & $1\\times10^{-3}$    & $1\\times10^{-3}$  & $5\\times10^{-4}$      & $5\\times10^{-4}$        & $5\\times10^{-4}$      \\\\\n  $\\lambda$         & $1\\times10^{-4}$    & $5\\times10^{-5}$  & $1\\times10^{-4}$      & $1\\times10^{-4}$        & $1\\times10^{-6}$      \\\\\n  $\\lambda_1$   & 1 & 1 & 1 & 1 & 1\\\\\n  $\\lambda_2$   &  0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\\\\n  $\\gamma$      & 1  & 1 & 1 & 1 & 1\\\\\n  $L$       & 2 & 2 & 2 & 2 & 2 \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\\section{Additional Experimental Results}\n\n\\subsection{Top-K Recommendation Curve}\n\\label{app:topk} \nWe curve the Top-K recommendation by varying K from 20 to 100 and compare BiGeaR with several selected models.\nAs shown in Figure~\\ref{fig:topk}, BiGeaR consistently presents the performance superiority over HashGNN, and shows the competitive recommendation accuracy with DGCF and LightGCN.\n\n\\subsection{Implementation of Embedding Scaler ${\\alpha}^{(l)}$}\n\\label{app:scaler}\n\nWe set the embedding scaler to learnable (denoted by \\textsl{LB}) and show the results in Table~\\ref{tab:scaler}.\nWe observe that, the design of learnable embedding scaler does not achieve the expected performance. \nThis is probably because there is no direct mathematical constraint to it and thus the parameter search space is too large to find the optimum by stochastic optimization.\n\n\n\\begin{table}[h]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\footnotesize\n\\caption{Implementation of Embedding Scaler.}\n\\vspace{-0.05in}\n\\label{tab:scaler}\n\\setlength{\\tabcolsep}{0.65mm}{\n\\begin{tabular}{c |c c|c c|c c|c c|c c}\n\\toprule\n ~ & \\multicolumn{2}{c|}{MovieLens} & \\multicolumn{2}{c|}{Gowalla} & \\multicolumn{2}{c|}{Pinterest} & \\multicolumn{2}{c|}{Yelp2018} & \\multicolumn{2}{c}{Amazon-book} \\\\\n               ~  & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20\\\\\n\\midrule\n\\midrule[0.1pt]\n  \\multirow{2}*{\\footnotesize \\textsl{LB}}    &{23.07} & {41.42}   & {17.01}& {23.11}  & {14.19}& {15.29}  & {6.05} & {10.80} & {4.52} & {7.85}\\\\\n  ~        &\\textit{\\color{blue} \\scriptsize{-9.78\\%}}  &\\textit{\\color{blue} \\scriptsize{-9.09\\%}}  &\\textit{\\color{blue} \\scriptsize{-7.35\\%}}  &\\textit{\\color{blue} \\scriptsize{-7.41\\%}}  &\\textit{\\color{blue} \\scriptsize{-8.86\\%}}  &\\textit{\\color{blue} \\scriptsize{-9.15\\%}}  &\\textit{\\color{blue} \\scriptsize{-6.49\\%}}  &\\textit{\\color{blue} \\scriptsize{-6.90\\%}} &\\textit{\\color{blue} \\scriptsize{-3.42\\%}}  &\\textit{\\color{blue}  \\scriptsize{-3.33\\%}}\\\\\n  \\midrule[0.1pt]\n\\cellcolor{best}{\\footnotesize \\textbf{BiGeaR} }  &\\textbf{25.57}& \\textbf{45.56}   & \\textbf{18.36}& \\textbf{24.96}  & \\textbf{15.57}& \\textbf{16.83}  & \\textbf{6.47}& \\textbf{11.60} & \\textbf{4.68}& \\textbf{8.12}\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\n\n\\subsection{\\textbf{Implementation of $w_l$.}}\n\\label{app:wl}\nWe try the following three additional implementation of $w_l$ and report the results in Tables~\\ref{tab:wl}.\n\\begin{enumerate}[leftmargin=*]\n\\item {\\small $w_l$ = $\\frac{1}{L+1}$} equally contributes for all embedding segments.\n\n\\item {\\small $w_l$ = $\\frac{1}{L+1-l}$} is positively correlated to the $l$ value, so as to highlight higher-order structures of the interaction graph. \n\n\\item {\\small $w_l$ = $2^{-(L+1-l)}$} is positively correlated to $l$ with exponentiation.\n\\end{enumerate}\nThe experimental results show that implementation (2) performs fairly well compared to the others, demonstrating the importance of highlighting higher-order graph information.\nThis corroborates the design of our implementation in BiGeaR, i.e., {\\small $w_l$ $\\propto$ $l$}, which however is simpler and effective with better recommendation accuracy.\n\n\n\n\\begin{table}[h]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\footnotesize\n\\caption{Implementation of $w_l$.}\n\\vspace{-0.05in}\n\\label{tab:wl}\n\\setlength{\\tabcolsep}{0.85mm}{\n\\begin{tabular}{c | c c | c c | c c | c c | c c }\n\\toprule\n~  & \\multicolumn{2}{c|}{MovieLens} & \\multicolumn{2}{c|}{Gowalla} & \\multicolumn{2}{c|}{Pinterest} & \\multicolumn{2}{c|}{Yelp2018} & \\multicolumn{2}{c}{Amazon-Book}\\\\\n               ~  & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 &R@20 & N@20\\\\\n\\midrule[0.1pt]\n\\midrule[0.1pt]\n  (1)           &{22.75}  &{41.13} &{16.15} &{21.82} &{14.16} &{15.48} &{5.88} &{10.32} &{4.46} &{7.63} \\\\\n  (2)           &{25.07}  &{44.64} &{17.81} &{24.46} &{15.26} &{16.57} &{6.40} &{11.38} &{4.58} &{7.96} \\\\        \n  (3)     &{21.23}  &{37.81} &{15.24} &{20.71} &{12.93} &{14.28} &{5.24} &{9.51} &{3.74} &{64.98} \\\\  \n\\midrule[0.1pt]\n\\cellcolor{best}\\textbf{Best}     &{\\textbf{25.57}}  &{\\textbf{45.56}} &{\\textbf{18.36}} &{\\textbf{24.96}} &{\\textbf{15.57}} &{\\textbf{16.83}} &{\\textbf{6.47}} &{\\textbf{11.60}}  &{\\textbf{4.68}}  &{\\textbf{8.12}}  \\\\ \n      \n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\\subsection{\\textbf{Implementation of $w_k$.}}\n\\label{app:wk}\nWe further evaluate different $w_k$:\n\\begin{enumerate}[leftmargin=*]\n\\item  {\\small $w_k$ = $\\frac{R-k}{R}$} is negatively correlated to the ranking position $k$.\n\\item  {\\small $w_k$ = $\\frac{1}{k}$} is inversely proportional to position $k$.\n\\item  {\\small $w_k$ = $2^{-k}$} is exponential to the value of $-k$.\n\\end{enumerate}\nWe observe from Table~\\ref{tab:wk} that the implementation (3) works slightly worse than Equation~(\\ref{eq:wk}) but generally better than the other two methods. \nThis show that the exponential modeling is more effective to depict the importance contribution of items for approximating the tailed item popularity~\\cite{rendle2014improving}.\nMoreover, Equation~(\\ref{eq:wk}) introduces hyper-parameters to provide the flexibility of adjusting the function properties for different datasets.\n\n\n\\begin{table}[h]\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\centering\n\\footnotesize\n\\caption{Implementation of $w_k$.}\n\\vspace{-0.05in}\n\\label{tab:wk}\n\\setlength{\\tabcolsep}{0.86mm}{\n\\begin{tabular}{c | c c | c c | c c | c c | c c }\n\\toprule\n~ & \\multicolumn{2}{c|}{MovieLens} & \\multicolumn{2}{c|}{Gowalla} & \\multicolumn{2}{c|}{Pinterest} & \\multicolumn{2}{c|}{Yelp2018} & \\multicolumn{2}{c}{Amazon-Book}\\\\\n               ~  & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 &R@20 & N@20\\\\\n\\midrule[0.1pt]\n\\midrule[0.1pt]\n  (1)           &{24.97}  &{44.33} &{17.96} &{24.87} &{15.11} &{16.20} &{6.28} &{11.21} &{4.43} &{7.78} \\\\\n  (2)           &{25.08}  &{45.19} &{17.95} &{24.95} &{15.18} &{16.34} &{6.27} &{11.25} &{4.48} &{7.92} \\\\        \n  (3)     &{25.16}  &{44.92} &{18.32} &{24.81} &{15.26} &{16.65} &{6.33} &{11.36} &{4.53} &{8.06} \\\\  \n\\midrule[0.1pt]\n\\cellcolor{best}\\textbf{Best}     &{\\textbf{25.57}}  &{\\textbf{45.56}} &{\\textbf{18.36}} &{\\textbf{24.96}} &{\\textbf{15.57}} &{\\textbf{16.83}} &{\\textbf{6.47}} &{\\textbf{11.60}}  &{\\textbf{4.68}}  &{\\textbf{8.12}}  \\\\ \n      \n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\n% \\subsection{Study of Gradient Estimation}\n% We attached the comparison with other gradient estimators \\textit{w.r.t.} NDCG@20 in Figure~\\ref{fig:quant_f_ndcg}.\n\n\n\n\n% \\begin{figure}[h]\n% \\begin{minipage}{0.5\\textwidth}\n% \\includegraphics[width=3.3in]{figs/quant_f_ndcg_with_gain}\n% \\end{minipage} \n% \\setlength{\\abovecaptionskip}{0.2cm}\n% \\setlength{\\belowcaptionskip}{0.2cm}\n% \\vspace{-0.05in}\n% \\caption{Gradient estimator comparison \\textit{w.r.t.} NDCG@20.}\n% \\label{fig:quant_f_ndcg}\n% \\end{figure}\n\n\n"
                }
            }
        },
        "figures": {
            "fig:intro": "\\begin{figure}[tp]\n\\begin{minipage}{0.5\\textwidth}\n\\hspace{-0.2in}\n\\includegraphics[width=3.5in]{figs/intro}\n\\end{minipage} \n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\vspace{-0.05in}\n\\caption{Illustration of BiGeaR.}\n\\label{fig:intro}\n\\end{figure}",
            "fig:framework": "\\begin{figure*}[tp]\n\\begin{minipage}{1\\textwidth}\n\\includegraphics[width=7in]{figs/framework_1}\n\\end{minipage} \n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\vspace{-0.05in}\n\\caption{BiGeaR first pre-trains the full-precision embeddings and then triggers the (1) graph layer-wise quantization, (2) inference distillation, and (3) accurate gradient estimation to learn the binarized representations (Best view in color).}\n\\label{fig:framework}\n\\end{figure*}",
            "fig:gradient": "\\begin{figure}[tp]\n\\begin{minipage}{0.5\\textwidth}\n\\hspace{-0.2in}\n\\includegraphics[width=3.6in]{figs/gradient}\n\\end{minipage} \n\\setlength{\\abovecaptionskip}{0.2cm}\n\\setlength{\\belowcaptionskip}{0.2cm}\n\\vspace{-0.05in}\n\\caption{Gradient estimation.}\n\\label{fig:gradient}\n\\end{figure}"
        }
    }
}