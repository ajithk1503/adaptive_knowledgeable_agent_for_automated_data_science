{
    "meta_info": {
        "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering",
        "abstract": "Given a graph with textual attributes, we enable users to `chat with their\ngraph': that is, to ask questions about the graph using a conversational\ninterface. In response to a user's questions, our method provides textual\nreplies and highlights the relevant parts of the graph. While existing works\nintegrate large language models (LLMs) and graph neural networks (GNNs) in\nvarious ways, they mostly focus on either conventional graph tasks (such as\nnode, edge, and graph classification), or on answering simple graph queries on\nsmall or synthetic graphs. In contrast, we develop a flexible\nquestion-answering framework targeting real-world textual graphs, applicable to\nmultiple applications including scene graph understanding, common sense\nreasoning, and knowledge graph reasoning. Toward this goal, we first develop a\nGraph Question Answering (GraphQA) benchmark with data collected from different\ntasks. Then, we propose our G-Retriever method, introducing the first\nretrieval-augmented generation (RAG) approach for general textual graphs, which\ncan be fine-tuned to enhance graph understanding via soft prompting. To resist\nhallucination and to allow for textual graphs that greatly exceed the LLM's\ncontext window size, G-Retriever performs RAG over a graph by formulating this\ntask as a Prize-Collecting Steiner Tree optimization problem. Empirical\nevaluations show that our method outperforms baselines on textual graph tasks\nfrom multiple domains, scales well with larger graph sizes, and mitigates\nhallucination.~\\footnote{Our codes and datasets are available at:\n\\url{https://github.com/XiaoxinHe/G-Retriever}}",
        "author": "Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi",
        "link": "http://arxiv.org/abs/2402.07630v3",
        "category": [
            "cs.LG"
        ],
        "additionl_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec: intro}\n\n\n\n\n\n\\textbf{Graphs and Large Language Models (LLMs).} The advent of LLMs has significantly shaped the artificial intelligence landscape. As these models are applied to increasingly diverse tasks, their ability to process complex structured data will be increasingly vital. In particular, in our interconnected world, a significant portion of real-world data inherently possesses a graph structure, such as the Web, e-commerce, recommendation systems, knowledge graphs, and many others. Moreover, many of these involve graphs with textual attributes (\\ie \\emph{textual graphs}), making them well-suited for LLM-centric methods. This has spurred interest in combining graph-based technologies, particularly graph neural networks (GNNs), with LLMs to enhance their reasoning on graphs~\\citep{wang2023can_graphllm_survey, jin2023large_graphllm_survey, li2023survey_graphllm_survey}. \n\n% \\textbf{The Present Work: Enabling `Chat With Your Graph'.} While existing works integrate LLMs and GNNs in various ways, they mostly focus on conventional graph tasks such as node, edge and graph classification~\\citep{he2023harnessing_nc}, or answering simple questions on small or synthetic graphs~\\citep{wang2023can_graphllm_survey, perozzi2024let_graphtoken}. In contrast, we develop a flexible question-answering framework targeting\n% real-world textual graph applications. This framework enables users to `chat with their graph' through a conversational interface, suitable even for complex and creative questions about graphs. As shown in Figure~\\ref{fig: chat}, our \\textit{G-Retriever} shows effective graph understanding abilities, while also incorporating the human-friendliness of conversational interfaces.\n\n\\textbf{The Present Work: Enabling `Chat With Your Graph'.} While existing works integrate LLMs and GNNs in various ways, they mostly focus on conventional graph tasks such as node, edge and graph classification~\\citep{he2023harnessing_nc}, or answering simple questions on small or synthetic graphs~\\citep{wang2023can_graphllm_survey, perozzi2024let_graphtoken}.  In contrast, we develop a flexible question-answering framework targeting complex and real-world graphs.  This framework enables users to `chat with their graph' via a unified conversational interface,\nrepresenting a leap towards intuitive interaction with graph data, as demonstrated in Figure~\\ref{fig: chat}.\n\n\n\\textbf{The Need for a Comprehensive GraphQA Benchmark.} \nQuestion answering (QA) is a fundamentally important task in natural language processing, serving as a key benchmark for assessing LLMs and providing a unified interface for various capabilities. Despite extensive research in QA,  a comprehensive benchmark specifically tailored for the graph modality is lacking. In contrast to existing benchmarks that focus on basic graph-based reasoning tasks such as node degree, edge existence, and shortest path~\\citep{fatemi2023talk_graph_qa, wang2023can_graphllm_survey}, our benchmark addresses complex and real-world graph applications including common sense reasoning, scene understanding, and knowledge graph reasoning (refer to Figure~\\ref{fig: dataset}). This is vital for measuring progress toward a model capable of answering a wide range of questions about graphs from diverse applications.\n\n\n\\textbf{New Architecture for GraphQA.}\nTo enable effective and efficient graph QA, even on large graphs, we propose \\textit{G-Retriever}, a new framework combining the strengths of GNNs, LLMs, and RAG (Figure~\\ref{fig: overview}).  Next, we will discuss the motivation, strengths, and details of our model.\n\n\n\\textbf{Tackling Hallucination in Graph LLMs.} LLMs are prone to hallucination, a phenomenon where the generated content is factually inaccurate or nonsensical~\\citep{huang2023survey_hallucination}. We validate the presence of this issue in graph settings. In particular, we employ a baseline method that adapts MiniGPT-4~\\citep{zhu2023minigpt} to graphs, where a frozen LLM interacts with a trainable GNN that encodes graph data as a soft prompt, as in GraphToken~\\citep{perozzi2024let_graphtoken}. Our findings, shown in Table~\\ref{tab: hallucination}, indicate that hallucination, an important problem in text-based LLMs, is also prevalent in Graph LLMs.  This may be attributed to the baseline's inability to recall the entire graph structure from a single graph embedding, leading to the generation of incorrect nodes or edges during the QA task. In contrast, by employing RAG for direct information retrieval from the actual graph, our \\textit{G-Retriever} mitigates this issue, as substantiated by Table~\\ref{tab: hallucination}. \n\n\n\n\n\\textbf{Enhancing Scalability and Efficiency in Graph LLMs.} Recent research endeavors have explored translating graphs into natural language, such as by flattening nodes and edges into a text sequence, enabling their processing by LLMs for graph-based tasks~\\citep{zhao2023graphtext_basic_graph_reasoning, fatemi2023talk_graph_qa}. However, this method faces critical scalability issues. Converting a graph with thousands of nodes and edges into a text sequence results in an excessive number of tokens, surpassing the input capacity of many LLMs. An alternative of truncating the graph text sequence to fit the LLM\u2019s input token limit leads to loss of information and response quality. \\textit{G-Retriever} overcomes these issues with its RAG component, which allows for effective scaling to larger graphs by selectively retrieving only relevant parts of the graph.\n\n\n\n\\textbf{Tailoring the RAG Approach to Graphs.} Existing RAG methodologies are primarily designed for simpler data types or knowledge graphs, where information is retrieved in a manner isolated from the graph structure~\\citep{gao2023retrieval_rag_survey, baek-etal-2023-knowledge_kaping, sen-etal-2023-knowledge_rigel, kang2023knowledge_SURGE}. Hence, we introduce a new retrieval approach for general textual graphs. Notably, we formulate subgraph retrieval as a Prize-Collecting Steiner Tree (PCST) optimization problem, which takes the neighborhood information into account during retrieval. This also allows the return of a subgraph most relevant to a query, thereby improving explainability.\n\n\n\nThe contributions of this paper are outlined as follows:\n\\begin{itemize}\n    \\item \\textbf{Pioneering the integration of Graph RAG.} We present the first retrieval approach for general textual graph tasks, which greatly enhances scalability and efficiency.\n    \\item \\textbf{Enabling `Chat with Your Graph'.} We develop a flexible question answering framework to handle complex and real-world textual graphs through a unified conversational interface.\n    \\item \\textbf{Introducing A Novel GraphQA Benchmark.} We introduce a diverse benchmark targeted at real-world graph question answering, filling a crucial research gap.\n    \\item \\textbf{Empirical Findings.} We demonstrate the efficiency and effectiveness of \\textit{G-Retriever} in multiple domains and present the significant finding of hallucination in graph LLMs.\n\\end{itemize}\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec: related work}\n\\textbf{Graphs and Large Language Models.} A significant body of research has emerged at the intersection of graph-based techniques and LLMs~\\citep{pan2023integrating_graphllm_survey, li2023survey_graphllm_survey, jin2023large_graphllm_survey, wang2023can_graphllm_survey, zhang2023graph_graphllm_survey}. This exploration spans diverse aspects, ranging from the design of general graph models~\\citep{ye2023natural_general_graph_model, liu2023one_general_graph_model, yu2023thought_general_graph_model, lei2023boosting_general_graph_model, tang2023graphgpt_general_graph_model,\nperozzi2024let_graphtoken},  and multi-modal architectures~\\citep{li2023graphadapter_multimodal_model, yoon2023multimodal_multimodal_model} to practical applications. Noteworthy applications include fundamental graph reasoning~\\citep{zhang2023graph_basic_graph_reasoning, chai2023graphllm_basic_graph_reasoning, zhao2023graphtext_basic_graph_reasoning}, node classification~\\citep{he2023harnessing_nc, huang2023can_nc, sun2023large_nc, chen2023label_nc, yu2023empower_nc, chen2023exploring_nc, qin2023disentangled_nc}, graph classification/regression~\\citep{qian2023can_gc, zhao2023gimlet_gc}, and leveraging LLMs for knowledge graph-related tasks~\\citep{tian2023graph_kg, jiang2023structgpt_kg, luo2023reasoning_rog_kg}. \n\n\\textbf{Retrieval-Augmented Generation (RAG).} \nThe concept of Retrieval-Augmented Generation, initially proposed by \\citet{lewis2020retrieval_rag}, has gained increased attention for its ability to mitigate the issue of hallucination within LLMs and enhance trustworthiness and explainability~\\citep{gao2023retrieval_rag_survey}. \nDespite its success in language-related tasks, the application of retrieval-based approaches to general graph tasks remains largely unexplored. Most existing work focuses primarily on the knowledge graph~\\citep{sun2024thinkongraph, baek-etal-2023-knowledge_kaping, sen-etal-2023-knowledge_rigel, kang2023knowledge_SURGE}.\nOur research is the first to apply a retrieval-based approach to general graph tasks, marking a novel advancement in the field and demonstrating the versatility of RAG beyond language processing. \n\n\\textbf{Parameter-Efficient Fine-Tuning (PEFT).} The field of LLMs has witnessed significant advancements through various parameter-efficient fine-tuning techniques. These methodologies have played a crucial role in refining LLMs, boosting their performance while minimizing the need for extensive parameter training. Notable among these techniques are prompt tuning, as introduced by~\\citet{lester2021power_prompt_tuning}, and prefix tuning, proposed by~\\citet{li2021prefix_tuning}. Furthermore, methods like LoRA~\\citep{hu2021lora}, and the LLaMA-adapter~\\citep{zhang2023llama_adapter}, have been influential. These advancements in PEFT have laid the foundation for the development of sophisticated multimodal models. Prominent examples in this domain include MiniGPT-4~\\citep{zhu2023minigpt}, LLaVA~\\citep{liu2023visual_llava}, and NExT-Chat~\\citep{wu2023next_chat}.\nThere are also emerging efforts in applying PEFT to graph LLMs, such as GraphLLM~\\citep{chai2023graphllm_basic_graph_reasoning} and GraphToken~\\citep{perozzi2024let_graphtoken} for basic graph reasoing tasks and GNP~\\citep{tian2023graph_kg} for multi-option QA on knowledge graphs.\n\n\n\n"
            },
            "section 3": {
                "name": "Formalization",
                "content": "\nThis section establishes the notation and formalizes key concepts related to textual graphs, language models for text encoding, and large language models and prompt tuning.\n\n\\textbf{Textual Graphs.} A textual graph is a graph where nodes and edges possess textual attributes. \nFormally, it can be defined as $G=(V,E, \\{x_n\\}_{n\\in V}, \\{x_e\\}_{e\\in E})$, where $V$ and $E$ represent the sets of nodes and edges, respectively. Additionally, $x_n\\in D^{L_n}$ and $x_e\\in D^{L_e}$ denote sequential text associate with a node $n\\in V$ or an edge $e\\in E$, where $D$ represents the vocabulary, and $L_n$ and $L_e$ signify the length of the text associated with the respective node or edge.\n\n\\textbf{Language Models for Text Encoding.} \nIn the context of textual graphs, language models (LMs) are essential for  encoding the text attributes associated with nodes and edges, thereby learning representations that capture their semantic meaning. \nFor a node $n$ with text attributes $x_n \\in D^{L_n}$, an LM encodes these attributes as:\n\\begin{equation}\n    z_n = \\text{LM}(x_n)\\in\\mathbb{R}^d,\n\\end{equation}\nwhere $z_n$ is the output of the LM, and $d$ is the dimension of the output vector.\n\n\\textbf{Large Language Models and Prompt Tuning.}\nLLMs have introduced a new paradigm for task-adaptation known as ``pre-train, prompt, and predict'', replacing the traditional ``pre-train, fine-tune'' paradigm. In this paradigm, the LLM is first pre-trained on a large corpus of text data to learn general language representations. Then, rather than fine-tuning the model on task-specific labeled data, the model is prompted with a textual prompt that specifies the task and context. Subsequently, the model generates the output directly based on the prompt and the input.%~\\citep{liu2023pre_survey}.\n\n\nThe LLM, parameterized by weights $\\theta$, takes a sequence of tokens $X$,  and a prompt $P$ as input, and generates a sequence of tokens $Y = \\{y_1, y_2, \\ldots, y_r\\}$ as output. Formally, the probability distribution of the output sequence given the concatenated input sequence and prompt, \\ie $[P;X]$, is defined as\n\\begin{equation}\np_\\theta(Y|[P;X])=\\prod_{i=1}^r p_\\theta(y_i|y_{<i}, [P;X]).\n\\end{equation}\nHere, $y_{<i}$ represents the prefix of sequence $y$ up to position $i-1$, and $p(y_i | y_{<i}, [P;X])$ represents the probability of generating token $y_i$ given $y_{<i}$ and $[P;X]$.\n\nSoft prompt tuning eliminates the need for manual prompt design. Given a series of $p$ tokens $X=\\{x_1, x_2, \\ldots, x_p\\}$, after being processed by the text embedder, it forms a matrix $X_e\\in\\mathbb{R}^{p\\times d_l}$, where $d_l$ is the dimension of the embedding space. Soft prompts can be represented as parameters $P_e \\in \\mathbb{R}^{q\\times d_l}$, where $q$ is the length of the prompt. The prompt is then concatenated with the embedded input, forming a single matrix $[P_e; X_e] \\in \\mathbb{R}^{(q+p)\\times d_l}$. This combined matrix is processed by the self-attention layers in LLM as usual. Training involves maximizing the likelihood of $Y$ through backpropagation, with gradient updates applied solely to $P_e$, while $\\theta$  remains fixed.\n\n\n"
            },
            "section 4": {
                "name": "Proposed GraphQA Benchmark",
                "content": "\n\n\n\nOur GraphQA represents a comprehensive and diverse benchmark for graph question-answering. It is tailored to assess the capabilities of models in answering a wide range of questions about graphs across diverse domains.\n% exitsing benchmark/model is only for one kind of specific task\n% more complex, conversational questions (for decoder-only LLM)\n% real-world scenerios (vs synthetic)\n\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "Data Format",
                    "content": "\nEach entry in the GraphQA benchmark consists of a textual graph, a question related to the graph, and one or more corresponding answers, as illustrated in Figure~\\ref{fig: dataset}.\n\n\\textbf{Textual Graphs.} The textual graph is converted into a natural language format, resulting in a list of nodes and edges, akin to a CSV file format. It is important to note that while multiple methods exist for textualizing a graph, our focus is not on identifying the optimal solution. Instead, we prioritize a straightforward yet empirically effective approach for representing graphs in natural language, facilitating the benchmark's use in diverse GraphQA scenarios.\n\n\\textbf{Questions and Answers.} Questions are designed to explore specific elements or relationships within the graph. Answers, residing within the attributes of nodes or edges, often require multi-hop reasoning for accurate identification.\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Description of Datasets",
                    "content": "\nThe GraphQA benchmark integrates three existing datasets: \\texttt{ExplaGraphs}, \\texttt{SceneGraphs}, and \\texttt{WebQSP}. Table~\\ref{tab: dataset} presents the summary statistics of these datasets. It is important to note that these datasets were not originally developed for this work. However, a significant contribution of our research is the standardization and processing of these diverse datasets into a uniform data format suitable for the GraphQA benchmark. These datasets, previously utilized in different contexts, are reintroduced with a new focus tailored for GraphQA. For a detailed comparison with the original datasets, see the Appendix~\\ref{appendix: GraphQA benchmark}.\n\n\n{\\texttt{ExplaGraphs}} is a dataset for generative commonsense reasoning, focusing on creating explanation graphs for stance prediction in debates. It offers detailed, unambiguous commonsense-augmented graphs to evaluate arguments supporting or refuting a belief. The primary task is to assess whether arguments are supportive or contradictory, using accuracy as the metric. We have converted the triplet-form provided in~\\citet{saha2021explagraphs} into a standard graph format.\n\n{\\texttt{SceneGraphs}}, a visual question answering dataset, includes 100,000 scene graphs. Each graph details objects, attributes, and relations within an image. This dataset challenges users with tasks requiring spatial understanding and multi-step inference. The task is to answer open-ended questions based on a textual description of a scene graph, evaluated on accuracy. We have sampled from the GQA dataset~\\citep{hudson2019gqa} and constructed standard graphs from the provided JSON files.\n\n\n\n{\\texttt{WebQSP}} is a large-scale multi-hop knowledge graph QA dataset consisting of 4,737 questions. It was proposed by~\\citet{yih2016value_webqsp} and, following~\\citet{luo2023reasoning_rog}, utilizes a subset of Freebase, encompassing facts within 2-hops of entities mentioned in the questions. The task involves answering questions that require multi-hop reasoning. Given the possibility of multiple answers for the same question, the hit@1 metric is used to assess the precision of the top returned answer. \n\n"
                }
            },
            "section 5": {
                "name": "G-Retriever",
                "content": "\n\\label{sec: method}\n\n\n\n\nIn this section, we introduce \\textit{G-Retriever}, a new architecture tailored for GraphQA, which integrates the strengths of GNNs, LLMs, and RAG. To allow efficient fine-tuning while preserving the LLM's pretrained language capabilities, we freeze the LLM and use a soft prompting approach on the output of the GNN.\n% Our approach can be fine-tuned to improve its graph understanding through soft prompt tuning. \nOur RAG-based design mitigates hallucinations through direct retrieval of the graph, while allowing our approach to scale to graphs exceeding the LLM's context window size. To adapt RAG to graphs, we formulate subgraph retrieval as a PCST optimization problem. This approach also allows us to enhance explainability by returning the retrieved subgraph.\n\n\n\\textit{G-Retriever} comprises four main steps: indexing, retrieval, subgraph construction and generation, as depicted in Figure~\\ref{fig: overview}.  The implementation details of each step are elaborated in the following sections.\n\n",
                "subsection 5.1": {
                    "name": "Indexing",
                    "content": "\n\nWe initiate the RAG approach by generating node and graph embeddings using a pre-trained LM. These embeddings are then stored in a nearest neighbor data structure. \n\nTo elaborate, consider $x_n\\in D^{L_n}$ as the text attributes of node $n$. Utilizing a pre-trained LM, such as SentenceBert~\\citep{reimers2019sentence_bert}, we apply the LM to $x_n$, yielding the representation $z_n$:\n\\begin{equation}\n        z_n=\\text{LM}(x_n)\\in\\mathbb{R}^d,\n\\end{equation}\nwhere $d$ denotes the dimension of the output vector. Similar preprocessing steps are applied to edges. Refer to Figure~\\ref{fig: overview}, Step 1 for an illustrative representation.\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Retrieval",
                    "content": "\nFor retrieval, we employ the same encoding strategy to the query $x_q$, to ensure  consistent treatment of textual information:\n\\begin{equation}\n    z_q=\\text{LM}(x_q)\\in\\mathbb{R}^d.\n\\end{equation}\n\nNext, to identify the most relevant nodes and edges for the current query, we use a k-nearest neighbors retrieval approach. This method yields a set of `relevant nodes/edges' based on the similarity between the query and each node or edge. The retrieval operation is defined as:\n% \\begin{equation}\n%         V_k = \\text{argtopk}_{n\\in V}\\text{ cos}(z_q, z_n),\\quad\n%         E_k = \\text{argtopk}_{e\\in E}\\text{ cos}(z_q, z_e),\n% \\end{equation}\n\\begin{equation}\n\\begin{split}\n    V_k &= \\text{argtopk}_{n\\in V}\\text{ cos}(z_q, z_n)\\\\\n    E_k &= \\text{argtopk}_{e\\in E}\\text{ cos}(z_q, z_e),\n\\end{split}\n\\end{equation}\nwhere $z_n$ and $z_e$ are the embeddings of node $n$ and edge $e$, respectively. \nWe use the cosine similarity function, $\\text{cos}(\\cdot, \\cdot)$, to measure the similarity between the query representation and the node/edge embeddings. The argtopk operation retrieves the top-k elements based on this similarity, providing a set of nodes $V_k$ and edges $E_k$ considered most relevant to the query. See Step 2 of Figure~\\ref{fig: overview}.\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Subgraph Construction",
                    "content": "\nThis step aims to construct a subgraph that encompasses as many relevant nodes and edges as possible, while keeping the graph size manageable. This approach offers two key benefits: Firstly, it helps to filter out nodes and edges that are not pertinent to the query. This is crucial because irrelevant information can overshadow the useful data, potentially diverting the focus of the subsequent LLM from the information of interest. Secondly, it enhances efficiency; by keeping the graph size manageable, it becomes feasible to translate the graph into natural language and then input it into the LLM for processing. The Prize-Collecting Steiner Tree algorithm~\\citep{bienstock1993note_pcst} serves as our primary method for identifying such optimally sized and relevant subgraphs. See Step 3 in Figure~\\ref{fig: overview}.\n\n\n\\textbf{Prize-Collecting Steiner Tree (PCST).}\nThe PCST problem aims to find a connected subgraph that maximizes the total prize values of its nodes while minimizing the total costs of its edges. Our approach assigns higher prize values to nodes and edges more relevant to the query, as measured by cosine similarity. Specifically, the top $k$ nodes/edges are assigned descending prize values from $k$ down to 1, with the rest assigned zero. The node prize assignment is as follows:\n\\begin{equation}\n    \\text{prize}(n) =\n\\begin{cases}\nk - i, & \\text{if } n \\in V_k \\text{ and } n \\text{ is the top } i\\text{ node}, \n\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\label{eq: topk}\n\\end{equation}\nEdge prizes are assigned similarly. The objective is to identify a subgraph, $S^* =(V^*,E^*)$, that optimizes the total prize of nodes and edges, minus the costs associated with the size of the subgraph:\n% \\begin{equation}\n%     S^*=\\underset{\\substack{S\\subseteq G,\\\\ S \\text{ is connected}}}{\\text{argmax}}\\sum_{n\\in V_S} \\text{prize}(n)+\\sum_{e\\in E_S}\\text{prize}(e)-\\text{cost}(S), \\,\\text{where cost}(S)=|E_S| \\times C_e,\n% \\end{equation}\n\\begin{equation}\n    S^*=\\underset{\\substack{S\\subseteq G,\\\\ S \\text{ is connected}}}{\\text{argmax}}\\sum_{n\\in V_S} \\text{prize}(n)+\\sum_{e\\in E_S}\\text{prize}(e)-\\text{cost}(S), \\,\n\\end{equation}\nwhere\n\\begin{equation}\n    \\text{cost}(S)=|E_S| \\times C_e,\n\\end{equation}\nand $C_e$ denotes a predefined cost per edge, which is adjustable to control the subgraph size.\n\n\nThe original PCST algorithm is designed for node prizes only. However, given the significance of edge semantics in certain scenarios, we adapt the algorithm to accommodate edge prizes as follows:\nConsider an edge e with a cost $C_e$ and a prize $P_e$. If $C_e > P_e$, it can be treated as a reduced edge cost of $C_e - P_e$. However, if $P_e > C_e$, negative edge costs are not allowed in the original algorithm. Our solution involves replacing edge $e$ with a `virtual node' $v_e$, connected to both endpoints of $e$. This virtual node is assigned a prize of $P_e - C_e$, and the cost of the two new edges leading to the virtual node is set to zero. This modification effectively mirrors the original problem, as including edge $e$ in the original graph is analogous to including the virtual node in the modified graph. Finally, we optimize the PCST problem using a near-linear time approach \\cite{hegde2015nearly}.\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Answer Generation",
                    "content": "\n\n\\textbf{Graph Encoder.} Let $S^* = (V^*, E^*)$ represent the retrieved subgraph. We use a graph encoder to model the structure of this graph, specifically using a standard Graph Attention Network (GAT)~\\citep{velivckovic2017graph_gat}. Our approach for encoding the retrieved subgraph is defined as follows:\n\\begin{equation}\nh_g = \\text{POOL}( \\text{GNN}_{\\phi_1}(S^*)) \\in\\mathbb{R}^{d_g},\n\\end{equation}\nHere, POOL denotes the mean pooling operation, and $d_g$ is the dimension of the graph encoder.\n\n\\textbf{Projection Layer.} We incorporate a multilayer perceptron (MLP) to align the graph token with the vector space of the LLM:\n\\begin{equation}\n    \\hat h_g = \\text{MLP}_{\\phi_2}(h_g)\\in\\mathbb{R}^{d_l},\n\\end{equation}\nwhere $d_l$ is the dimension of the LLM's hidden embedding.\n\n\\textbf{Text Embedder.} To leverage the text-reasoning capabilities of LLMs, we transform the retrieved subgraph $S^*$ into a textual format. This transformation involves flattening the textual attributes of the nodes and edges, as illustrated in the green box in Figure~\\ref{fig: dataset}. We refer to this operation as $\\text{textualize}(\\cdot)$. Subsequently, we combine the textualized graph with the query to generate a response. Let $x_q$ denote the query; we concatenate it with the textualized graph $\\text{textualize}(S^*)$. We then map the result to an embedding $h_t$ using a text embedder, which is the first layer of a pretrained and frozen LLM:\n\\begin{equation}\nh_t = \\text{TextEmbedder}([\\text{textualize}(S^*);x_q])\\in \\mathbb{R}^{L\\times{d_l}},\n\\end{equation}\nwhere $[;]$ represents the concatenation operation, and $L$ is the number of tokens. \n\n\\textbf{LLM Generation with Graph Prompt Tuning.}\nThe final stage involves generating the answer $Y$ given the graph token $\\hat h_g$, acting as a soft prompt, and the text embedder output $h_t$. \n% \\bh{check this part}\nThese inputs are fed through the self-attention layers of a pretrained frozen LLM, with parameter $\\theta$. The generation process is represented as follows:\n\\begin{equation}\n    p_{\\theta,\\phi_1, \\phi_2}(Y|S^*, x_q) = \\prod_{i=1}^r p_{\\theta,\\phi_1, \\phi_2}(y_i|y_{<i}, [\\hat h_g; h_t]),\n\\end{equation}\nwhere $[\\hat h_g; h_t]$ concatenates the graph token $\\hat h_g$ and the text embedder output $h_t$. While $\\theta$ is frozen, the graph token $\\hat{h}_g$ receives gradients, enabling the optimization of the parameters of the graph encoder $\\phi_1$ and the projection layer $\\phi_2$ through standard backpropagation.\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Experiments",
                "content": "\n\\label{sec: experiments}\n",
                "subsection 6.1": {
                    "name": "Experiment Setup",
                    "content": "\nIn the indexing step, we use SentenceBert~\\citep{reimers2019sentence_bert} as the LM to encode all node and edge attributes. In the generation step, we use the open-source Llama2-7b~\\citep{touvron2023llama_llama2} as the LLM and Graph Transformer~\\citep{shi2020masked_graph_trans} as the graph encoder. Additional details are provided in Appendix~\\ref{appendix: implementation}. \n\n\n% \\subsection{Model Configurations}\n\n\n\n"
                },
                "subsection 6.2": {
                    "name": "Main Results",
                    "content": "\nIn our experiments, we consider three model configurations:\n\\emph{1) Inference-only}: Using a frozen LLM for direct question answering; \\emph{2) Frozen LLM w/ prompt tuning (PT)}: Keeping the parameters of the LLM frozen and adapting only the prompt; \\emph{3) Tuned LLM}: Fine-tuning the LLM with LoRA~\\citep{hu2021lora}. We provide more details in Appendix~\\ref{app: main table}.\n\n\n\n\nTable~\\ref{tab: main} demonstrates the effectiveness of our method across three datasets in various configurations. In the inference-only setting, \\textit{G-Retriever} surpasses all baselines. Notably, LLM can perform even better when no graph knowledge is provided (\\ie question only), which might be attributed to the complexity and potential noise in the knowledge. For frozen LLM with prompt tuning, \\textit{G-Retriever} outperforms traditional prompt tuning and GraphToken~\\citep{perozzi2024let_graphtoken}, a graph prompt tuning-based method, with average performance increases of 40.6\\% and 30.8\\% respectively. Furthermore, when tuned with LoRA, \\textit{G-Retriever} achieves the best performance. \n\n"
                },
                "subsection 6.3": {
                    "name": "Efficiency Evaluation",
                    "content": "\n\n\n\n% \\begin{table}[t]\n% \\centering\n% \\footnotesize\n% \\caption{Average number of tokens and nodes before and after implementing retrieval.}\n% \\vskip 0.1in\n% \\label{tab: efficiency}\n% \\begin{tabular}{m{3cm}m{2cm}m{2cm}m{2cm}m{2cm}}\n% \\toprule\n% \\multirow{2}{*}{Dataset}\n% & \\multicolumn{2}{c}{Before Retrieval (Avg.)} & \\multicolumn{2}{c}{After Retrieval (Avg.)} \\\\\n% \\cmidrule(lr){2-3}\\cmidrule(lr){4-5}\n%  & \\# Tokens & \\# Nodes & \\# Tokens & \\# Nodes \\\\\n% \\midrule\n% \\texttt{SceneGraphs} & 1,396 & 19 & 235 ($\\downarrow$83\\%) & 5 ($\\downarrow$74\\%)\\\\\n% \\texttt{WebQSP} & 100,627 & 1,371 & 610 ($\\downarrow$99\\%) & 18 ($\\downarrow$99\\%)\\\\\n% \\bottomrule\n% \\end{tabular}\n% \\end{table}\n\nThe efficiency of our approach is highlighted by the data in Table~\\ref{tab: efficiency}. Implementing our graph-based retrieval significantly decreases the number of tokens required to describe the graphs in text, reduces the number of nodes in graphs, and speeds up the training process. Specifically, for the \\texttt{SceneGraphs} dataset, tokens decreased by 83\\%, nodes by 74\\%, and training time by 29\\%. For the \\texttt{WebQSP} dataset, tokens decreased by 99\\%, nodes by 99\\%, and training time by 67\\%. These substantial reductions demonstrate the method's efficiency and potential in managing large-scale graph data.\n\n\n\n"
                },
                "subsection 6.4": {
                    "name": "Mitigation of Hallucination",
                    "content": "\n\n\nTo evaluate hallucination, we instructed the models to answer graph-related questions, specifically identifying supporting nodes or edges from the graph. We manually reviewed 100 responses from both our method and the baseline (\\ie LLM with graph prompt tuning), verifying the existence of the nodes and edges referenced in the model's output within the actual graph.  Table~\\ref{tab: hallucination_gqa} shows that \\textit{G-Retriever} significantly reduces hallucinations by 54\\% compared to the baseline, as our graph retrieval ensures that the data is sourced directly from the actual graph, leading to less hallucination.\nSee Appendix~\\ref{appendix: hallucination} for details.\n\n\n\n"
                },
                "subsection 6.5": {
                    "name": "Ablation Study",
                    "content": "\n\nIn this ablation study, we assess the individual impact of key components within our pipeline. As shown in Table~\\ref{tab: ablation}, there are performance drops when any of these components are removed, with the graph encoder and textualized graph showing declines of 22.51\\% and 19.19\\%, respectively. This demonstrates their complementary effects in representing the graph in both textual and embedded formats. Additionally, the retrieval on graphs is also important to the overall performance. Further details are available in Appendix~\\ref{appdix: Details of Ablation Study}. We also present additional studies on our framework: it is robust to the choice of graph encoders (see Appendix~\\ref{appendix: gnn}) and benefits from the increased scale of LLMs (see Appendix~\\ref{appendix: llm}).\n\n\n\n\nAdditionally, we include a detailed comparison with existing retrieval methods (see Appendix~\\ref{appendix: graph_rag}), a discussion on the complexity (see Appendix~\\ref{appendix: complexity}), and demonstrations on how to use \\textit{G-Retriever} to `chat with your graph' (see Appendix~\\ref{appendix: demo}).\n\n\n\n\n"
                }
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\n\\label{sec: conclusion}\nIn this work, we introduce a new GraphQA benchmark for real-world graph  question answering and present \\textit{G-Retriever}, an architecture adept at complex and creative queries. Experimental results show that \\textit{G-Retriever} surpasses baselines in textual graph tasks across multiple domains, scales effectively with larger graph sizes, and demonstrates resistance to hallucination.\n\n% Limitation/Future work: the retrieval part: frozen->trainable\n\n\\textbf{Limitations and Future Work:} Currently, \\textit{G-Retriever} employs a static retrieval component. Future developments could investigate more sophisticated RAG where the retrieval is trainable.\n\n"
            },
            "section 8": {
                "name": "Acknowledgment",
                "content": "\nXB is supported by NUS Grant ID R-252-000-B97-133.\n\n\\bibliography{neurips_2024}\n\\bibliographystyle{plainnat}\n\n\n% \\section{Submission of papers to NeurIPS 2024}\n\n\n% Please read the instructions below carefully and follow them faithfully.\n\n\n% \\subsection{Style}\n\n\n% Papers to be submitted to NeurIPS 2024 must be prepared according to the\n% instructions presented here. Papers may only be up to {\\bf nine} pages long,\n% including figures. Additional pages \\emph{containing only acknowledgments and\n% references} are allowed. Papers that exceed the page limit will not be\n% reviewed, or in any other way considered for presentation at the conference.\n\n\n% The margins in 2024 are the same as those in previous years.\n\n\n% Authors are required to use the NeurIPS \\LaTeX{} style files obtainable at the\n% NeurIPS website as indicated below. Please make sure you use the current files\n% and not previous versions. Tweaking the style files may be grounds for\n% rejection.\n\n\n% \\subsection{Retrieval of style files}\n\n\n% The style files for NeurIPS and other conference information are available on\n% the website at\n% \\begin{center}\n%   \\url{http://www.neurips.cc/}\n% \\end{center}\n% The file \\verb+neurips_2024.pdf+ contains these instructions and illustrates the\n% various formatting requirements your NeurIPS paper must satisfy.\n\n\n% The only supported style file for NeurIPS 2024 is \\verb+neurips_2024.sty+,\n% rewritten for \\LaTeXe{}.  \\textbf{Previous style files for \\LaTeX{} 2.09,\n%   Microsoft Word, and RTF are no longer supported!}\n\n\n% The \\LaTeX{} style file contains three optional arguments: \\verb+final+, which\n% creates a camera-ready copy, \\verb+preprint+, which creates a preprint for\n% submission to, e.g., arXiv, and \\verb+nonatbib+, which will not load the\n% \\verb+natbib+ package for you in case of package clash.\n\n\n% \\paragraph{Preprint option}\n% If you wish to post a preprint of your work online, e.g., on arXiv, using the\n% NeurIPS style, please use the \\verb+preprint+ option. This will create a\n% nonanonymized version of your work with the text ``Preprint. Work in progress.''\n% in the footer. This version may be distributed as you see fit, as long as you do not say which conference it was submitted to. Please \\textbf{do\n%   not} use the \\verb+final+ option, which should \\textbf{only} be used for\n% papers accepted to NeurIPS.\n\n\n% At submission time, please omit the \\verb+final+ and \\verb+preprint+\n% options. This will anonymize your submission and add line numbers to aid\n% review. Please do \\emph{not} refer to these line numbers in your paper as they\n% will be removed during generation of camera-ready copies.\n\n\n% The file \\verb+neurips_2024.tex+ may be used as a ``shell'' for writing your\n% paper. All you have to do is replace the author, title, abstract, and text of\n% the paper with your own.\n\n\n% The formatting instructions contained in these style files are summarized in\n% Sections \\ref{gen_inst}, \\ref{headings}, and \\ref{others} below.\n\n\n% \\section{General formatting instructions}\n% \\label{gen_inst}\n\n\n% The text must be confined within a rectangle 5.5~inches (33~picas) wide and\n% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point\n% type with a vertical spacing (leading) of 11~points.  Times New Roman is the\n% preferred typeface throughout, and will be selected for you by default.\n% Paragraphs are separated by \\nicefrac{1}{2}~line space (5.5 points), with no\n% indentation.\n\n\n% The paper title should be 17~point, initial caps/lower case, bold, centered\n% between two horizontal rules. The top rule should be 4~points thick and the\n% bottom rule should be 1~point thick. Allow \\nicefrac{1}{4}~inch space above and\n% below the title to rules. All pages should start at 1~inch (6~picas) from the\n% top of the page.\n\n\n% For the final version, authors' names are set in boldface, and each name is\n% centered above the corresponding address. The lead author's name is to be listed\n% first (left-most), and the co-authors' names (if different address) are set to\n% follow. If there is only one co-author, list both author and co-author side by\n% side.\n\n\n% Please pay special attention to the instructions in Section \\ref{others}\n% regarding figures, tables, acknowledgments, and references.\n\n\n% \\section{Headings: first level}\n% \\label{headings}\n\n\n% All headings should be lower case (except for first word and proper nouns),\n% flush left, and bold.\n\n\n% First-level headings should be in 12-point type.\n\n\n% \\subsection{Headings: second level}\n\n\n% Second-level headings should be in 10-point type.\n\n\n% \\subsubsection{Headings: third level}\n\n\n% Third-level headings should be in 10-point type.\n\n\n% \\paragraph{Paragraphs}\n\n\n% There is also a \\verb+\\paragraph+ command available, which sets the heading in\n% bold, flush left, and inline with the text, with the heading followed by 1\\,em\n% of space.\n\n\n% \\section{Citations, figures, tables, references}\n% \\label{others}\n\n\n% These instructions apply to everyone.\n\n\n% \\subsection{Citations within the text}\n\n\n% The \\verb+natbib+ package will be loaded for you by default.  Citations may be\n% author/year or numeric, as long as you maintain internal consistency.  As to the\n% format of the references themselves, any style is acceptable as long as it is\n% used consistently.\n\n\n% The documentation for \\verb+natbib+ may be found at\n% \\begin{center}\n%   \\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}\n% \\end{center}\n% Of note is the command \\verb+\\citet+, which produces citations appropriate for\n% use in inline text.  For example,\n% \\begin{verbatim}\n%    \\citet{hasselmo} investigated\\dots\n% \\end{verbatim}\n% produces\n% \\begin{quote}\n%   Hasselmo, et al.\\ (1995) investigated\\dots\n% \\end{quote}\n\n\n% If you wish to load the \\verb+natbib+ package with options, you may add the\n% following before loading the \\verb+neurips_2024+ package:\n% \\begin{verbatim}\n%    \\PassOptionsToPackage{options}{natbib}\n% \\end{verbatim}\n\n\n% If \\verb+natbib+ clashes with another package you load, you can add the optional\n% argument \\verb+nonatbib+ when loading the style file:\n% \\begin{verbatim}\n%    \\usepackage[nonatbib]{neurips_2024}\n% \\end{verbatim}\n\n\n% As submission is double blind, refer to your own published work in the third\n% person. That is, use ``In the previous work of Jones et al.\\ [4],'' not ``In our\n% previous work [4].'' If you cite your other papers that are not widely available\n% (e.g., a journal paper under review), use anonymous author names in the\n% citation, e.g., an author of the form ``A.\\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.\n\n\n% \\subsection{Footnotes}\n\n\n% Footnotes should be used sparingly.  If you do require a footnote, indicate\n% footnotes with a number\\footnote{Sample of the first footnote.} in the\n% text. Place the footnotes at the bottom of the page on which they appear.\n% Precede the footnote with a horizontal rule of 2~inches (12~picas).\n\n\n% Note that footnotes are properly typeset \\emph{after} punctuation\n% marks.\\footnote{As in this example.}\n\n\n% \\subsection{Figures}\n\n\n% \\begin{figure}\n%   \\centering\n%   \\fbox{\\rule[-.5cm]{0cm}{4cm} \\rule[-.5cm]{4cm}{0cm}}\n%   \\caption{Sample figure caption.}\n% \\end{figure}\n\n\n% All artwork must be neat, clean, and legible. Lines should be dark enough for\n% purposes of reproduction. The figure number and caption always appear after the\n% figure. Place one line space before the figure caption and one line space after\n% the figure. The figure caption should be lower case (except for first word and\n% proper nouns); figures are numbered consecutively.\n\n\n% You may use color figures.  However, it is best for the figure captions and the\n% paper body to be legible if the paper is printed in either black/white or in\n% color.\n\n\n% \\subsection{Tables}\n\n\n% All tables must be centered, neat, clean and legible.  The table number and\n% title always appear before the table.  See Table~\\ref{sample-table}.\n\n\n% Place one line space before the table title, one line space after the\n% table title, and one line space after the table. The table title must\n% be lower case (except for first word and proper nouns); tables are\n% numbered consecutively.\n\n\n% Note that publication-quality tables \\emph{do not contain vertical rules.} We\n% strongly suggest the use of the \\verb+booktabs+ package, which allows for\n% typesetting high-quality, professional tables:\n% \\begin{center}\n%   \\url{https://www.ctan.org/pkg/booktabs}\n% \\end{center}\n% This package was used to typeset Table~\\ref{sample-table}.\n\n\n% \\begin{table}\n%   \\caption{Sample table title}\n%   \\label{sample-table}\n%   \\centering\n%   \\begin{tabular}{lll}\n%     \\toprule\n%     \\multicolumn{2}{c}{Part}                   \\\\\n%     \\cmidrule(r){1-2}\n%     Name     & Description     & Size ($\\mu$m) \\\\\n%     \\midrule\n%     Dendrite & Input terminal  & $\\sim$100     \\\\\n%     Axon     & Output terminal & $\\sim$10      \\\\\n%     Soma     & Cell body       & up to $10^6$  \\\\\n%     \\bottomrule\n%   \\end{tabular}\n% \\end{table}\n\n% \\subsection{Math}\n% Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \\$\\$ anyway; see \\url{https://tex.stackexchange.com/questions/503/why-is-preferable-to} and \\url{https://tex.stackexchange.com/questions/40492/what-are-the-differences-between-align-equation-and-displaymath} for more information.)\n\n% \\subsection{Final instructions}\n\n% Do not change any aspects of the formatting parameters in the style files.  In\n% particular, do not modify the width or length of the rectangle the text should\n% fit into, and do not change font sizes (except perhaps in the\n% \\textbf{References} section; see below). Please note that pages should be\n% numbered.\n\n\n% \\section{Preparing PDF files}\n\n\n% Please prepare submission files with paper size ``US Letter,'' and not, for\n% example, ``A4.''\n\n\n% Fonts were the main cause of problems in the past years. Your PDF file must only\n% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to\n% achieve this.\n\n\n% \\begin{itemize}\n\n\n% \\item You should directly generate PDF files using \\verb+pdflatex+.\n\n\n% \\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the\n%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can\n%   also use the program \\verb+pdffonts+ which comes with \\verb+xpdf+ and is\n%   available out-of-the-box on most Linux machines.\n\n\n% \\item \\verb+xfig+ \"patterned\" shapes are implemented with bitmap fonts.  Use\n%   \"solid\" shapes instead.\n\n\n% \\item The \\verb+\\bbold+ package almost always uses bitmap fonts.  You should use\n%   the equivalent AMS Fonts:\n% \\begin{verbatim}\n%    \\usepackage{amsfonts}\n% \\end{verbatim}\n% followed by, e.g., \\verb+\\mathbb{R}+, \\verb+\\mathbb{N}+, or \\verb+\\mathbb{C}+\n% for $\\mathbb{R}$, $\\mathbb{N}$ or $\\mathbb{C}$.  You can also use the following\n% workaround for reals, natural and complex:\n% \\begin{verbatim}\n%    \\newcommand{\\RR}{I\\!\\!R} %real numbers\n%    \\newcommand{\\Nat}{I\\!\\!N} %natural numbers\n%    \\newcommand{\\CC}{I\\!\\!\\!\\!C} %complex numbers\n% \\end{verbatim}\n% Note that \\verb+amsfonts+ is automatically loaded by the \\verb+amssymb+ package.\n\n\n% \\end{itemize}\n\n\n% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask\n% you to fix it.\n\n\n% \\subsection{Margins in \\LaTeX{}}\n\n\n% Most of the margin problems come from figures positioned by hand using\n% \\verb+\\special+ or other commands. We suggest using the command\n% \\verb+\\includegraphics+ from the \\verb+graphicx+ package. Always specify the\n% figure width as a multiple of the line width as in the example below:\n% \\begin{verbatim}\n%    \\usepackage[pdftex]{graphicx} ...\n%    \\includegraphics[width=0.8\\linewidth]{myfile.pdf}\n% \\end{verbatim}\n% See Section 4.4 in the graphics bundle documentation\n% (\\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})\n\n\n% A number of width problems arise when \\LaTeX{} cannot properly hyphenate a\n% line. Please give LaTeX hyphenation hints using the \\verb+\\-+ command when\n% necessary.\n\n% \\begin{ack}\n% Use unnumbered first level headings for the acknowledgments. All acknowledgments\n% go at the end of the paper before the list of references. Moreover, you are required to declare\n% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).\n% More information about this disclosure can be found at: \\url{https://neurips.cc/Conferences/2024/PaperInformation/FundingDisclosure}.\n\n\n% Do {\\bf not} include this section in the anonymized submission, only in the final paper. You can use the \\texttt{ack} environment provided in the style file to automatically hide this section in the anonymized submission.\n% \\end{ack}\n\n\n% \\section*{References}\n\n\n% References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for\n% the references. Any choice of citation style is acceptable as long as you are\n% consistent. It is permissible to reduce the font size to \\verb+small+ (9 point)\n% when listing the references.\n% Note that the Reference section does not count towards the page limit.\n% \\medskip\n\n\n% {\n% \\small\n\n\n% [1] Alexander, J.A.\\ \\& Mozer, M.C.\\ (1995) Template-based algorithms for\n% connectionist rule extraction. In G.\\ Tesauro, D.S.\\ Touretzky and T.K.\\ Leen\n% (eds.), {\\it Advances in Neural Information Processing Systems 7},\n% pp.\\ 609--616. Cambridge, MA: MIT Press.\n\n\n% [2] Bower, J.M.\\ \\& Beeman, D.\\ (1995) {\\it The Book of GENESIS: Exploring\n%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:\n% TELOS/Springer--Verlag.\n\n\n% [3] Hasselmo, M.E., Schnell, E.\\ \\& Barkai, E.\\ (1995) Dynamics of learning and\n% recall at excitatory recurrent synapses and cholinergic modulation in rat\n% hippocampal region CA3. {\\it Journal of Neuroscience} {\\bf 15}(7):5249-5262.\n% }\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\\newpage\n\\appendix\n\n% \\section{Appendix / supplemental material}\n\n\n% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.\n% All such materials \\textbf{SHOULD be included in the main submission.}\n"
            },
            "section 9": {
                "name": "Impact Statements",
                "content": "\n\\label{appendix: impact statements}\nAs LLMs are applied to increasingly diverse tasks, their ability to process complex structured data will be increasingly vital. Our work aims to enhance LLMs' ability to interact with graph-structured data, while resisting hallucination, thus improving model reliability. We also enhance explainability, both by returning the retrieved subgraph, and through the use of conversational interfaces for `chatting with a graph', which allows for better human-AI interaction and for models to behave in a way that is more well-aligned with human expectations. \n\n"
            },
            "section 10": {
                "name": "Experiment",
                "content": "\n\\label{app: experiment}\n\n",
                "subsection 10.1": {
                    "name": "Implementation Settings",
                    "content": "\n\\label{appendix: implementation}\n\n\nExperiments are conducted using 2 NVIDIA A100-80G GPUs. Each experiment is replicated four times, utilizing different seeds for each run to ensure robustness and reproducibility.\n\n\n\\textbf{Graph Encoder.} We use Graph Transformer~\\citep{shi2020masked_graph_trans} as the GNN backbone. Our configuration employs 4 layers, each with 4 attention heads, and a hidden dimension size of 1024.\n\n\n\\textbf{LLM.} We use the open-sourced Llama2-7b~\\citep{touvron2023llama_llama2} as the LLM backbone. In fine-tuning the LLM with LoRA~\\citep{hu2021lora}, the lora\\_r parameter (dimension for LoRA update matrices) is set to 8, and lora\\_alpha (scaling factor) is set to 16. The dropout rate is set to 0.05.  In prompt tuning, the LLM is configured with 10 virtual tokens. The number of max text length is 512, the number of max new tokens, \\ie the maximum numbers of tokens to generate, is 32. \n% \\bh{explain 'new tokens'?}\n\n\\textbf{PCST}. For retrieval over graphs via PCST, for the \\texttt{SceneGraphs} dataset, we select the top $k$ nodes and edges, setting $k$ to 3. Here, the cost of edges, denoted as $C_e$, is set to 1. Regarding the \\texttt{WebQSP} dataset, we set $k=3$ for nodes and $k=5$ for edges, with the edge cost, $C_e$, adjusted to 0.5.\nFor the \\texttt{ExplaGraphs} dataset, which is characterized by a small graph size averaging 5.17 nodes and 4.25 edges (as detailed in Table~\\ref{tab: dataset}), the entire graph can fit in the LLM's context window size. Consequently, we aim to retrieve the whole graph by setting $k$ to 0, effectively returning the original graph unaltered.\n\n\n% Additionally, the cost of edges, denoted as $C_e$, is set to 0.5.\n% For retrieval over graphs via PCST, we select the top $k$ nodes and edges, setting $k$ to 3. Additionally, the cost of edges, denoted as $C_e$, is set to 0.5.\n\n\\textbf{Optimization.} We use the AdamW~\\citep{loshchilov2017decoupled_adamw} optimizer. We set the initial learning rate at 1e-5, with a weight decay of 0.05. The learning rate decays with a half-cycle cosine decay after the warm-up period. \nThe batch size is 4, and the number of epochs is 10. To prevent overfitting and ensure training efficiency, an early stopping mechanism is implemented with a patience setting of 2 epochs.\n\n\n"
                },
                "subsection 10.2": {
                    "name": "Details of Model Configurations",
                    "content": "\n\\label{app: main table}\n\nIn our experiments, we consider three model configurations:\n\n\n% \\emph{1) Inference-only}: Using a frozen LLM for direct question answering, with question (see Figure~\\ref{fig: inference_q}), or with textual graph and question (see Figure~\\ref{fig: inference_q_g}).\n\n\n% \\begin{figure}[!ht]\n% \\centering\n%      \\begin{subfigure}[b]{0.48\\textwidth}\n%          \\centering\n%          \\includegraphics[width=\\textwidth]{figs/baseline_inference_query.pdf}\n%          \\caption{Inference-only w/ question}\n%          \\label{fig: inference_q}\n%      \\end{subfigure}\n%      \\begin{subfigure}[b]{0.48\\textwidth}\n%          \\centering\n%          \\includegraphics[width=\\textwidth]{figs/baseline_inference_query_textualgraph.pdf}\n%          \\caption{Inference-only w/ textual graph and question}\n%          \\label{fig: inference_q_g}\n%      \\end{subfigure}\n% \\caption{Model configuration \\emph{1) Inference-Only}.}\n% \\label{fig: inference}\n% \\end{figure}\n\n\n\\emph{1) Inference-only}: Using a frozen LLM for direct question answering with textual graph and question, see Figure~\\ref{fig: inference}.\n\n\n\\begin{itemize}\n    \\item Zero-shot. In this approach, the model is given a textual graph description and a task description, and is immediately asked to produce the desired output. No additional examples or demonstrations are provided.\n\n    \\item Zero-CoT. Zero-shot Chain-of-thought (Zero-CoT) prompting~\\citep{kojima2022large_zero_cot} is a follow-up to CoT prompting~\\citep{wei2022chain_cot}, which introduces an incredibly simple zero shot prompt by appending the words \"Let's think step by step.\" to the end of a question.\n    \\item CoT-BAG. Build-a-Graph Prompting (BAG)~\\citep{wang2023can_graphllm_survey} is a prompting technique  that adds \"Let\u2019s construct a graph with the nodes and edges first.\" after the textual description of the graph is explicitly given.\n    \\item  KAPING. KAPING~\\citep{baek-etal-2023-knowledge_kaping} is a zero-shot knowledge-augmented prompting method for knowledge graph question answering. It first retrieves triples related to the question from the graph, then prepends them to the input question in the form of a prompt, which is then forwarded to LLMs to generate the answer.\n\\end{itemize}\n\n\n\\emph{2) Frozen LLM w/ prompt tuning (PT)}: Keeping the parameters of the LLM frozen and adapting only the prompt. This includes soft prompt tuning (see Figure~\\ref{fig: pt}),\nGraphToken~\\citep{perozzi2024let_graphtoken}, which is a graph prompt tuning method, and our \\textit{G-Retriever} method (see Figure\\ref{fig: ours}).\n\n\n\\emph{3) Tuned LLM}: Fine-tuning the LLM with LoRA. This includes standard fine-tuning of an LLM for downstream tasks using LoRA (see Figure~\\ref{fig: lora}) and G-Retriever with LoRA (see Figure~\\ref{fig: lora_ours}).\n\n\n\n\n"
                },
                "subsection 10.3": {
                    "name": "Details of Ablation Study",
                    "content": "\n\\label{appdix: Details of Ablation Study}\n\n\nThis section illustrates the modifications made to the original architecture in the ablation study, as presented in Figure~\\ref{fig: ablaiton}.\n\n\\textbf{Without Graph Encoder (w/o GraphEncoder):} In this setting, we replaced the graph encoder with trainable soft tokens, setting the number of these virtual tokens to 10.\n\n\\textbf{Without Projection Layer (w/o Projection Layer):} Here, we removed the projection layer following the graph encoder. We configured the output dimension of the graph encoder to be 4,096, matching the hidden dimension of Llama2-7b. This allows the output graph token (the yellow token in Figure~\\ref{fig: wo_proj}) to be concatenated directly with the LLM tokens (blue tokens).\n\n\\textbf{Without Textualized Graph (w/o Textualized Graph):} In this configuration, we modified the textual input to the LLM. Rather than using a combination of the question and the textualized graph, we solely used the question.\n\n\n\n\n\n\n"
                },
                "subsection 10.4": {
                    "name": "The Choice of Graph Encoder",
                    "content": "\n\\label{appendix: gnn}\n\n% \\begin{table}[!ht]\n%     \\centering\n%     \\caption{Performance of different graph encoders on the \\texttt{WebQSP} dataset.}\n%     \\label{tab: gnn}\n%     \\begin{tabular}{l|ccc}\n%     \\toprule\n%          Graph Encoder &  GCN & GAT & GraphTransformer\\\\\n%          Hit@1& 0.6853 & 0.6732 & 0.6910\\\\\n%          \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\nIn addition to the  Graph Transformer~\\citep{shi2020masked_graph_trans},\nwe  explore other GNNs as the graph encoder, such as  GCN~\\citep{kipf2016semi_gcn} and the GAT~\\citep{velivckovic2017graph_gat}. The comparative results of these models on the \\texttt{WebQSP} and \\texttt{ExplaGraphs} datasets are presented in Table~\\ref{tab: gnn}.\n\n\n\nThe results demonstrate that our proposed method exhibits consistent robustness across different graph encoders. Notably, all three encoders \u2013 GCN, GAT, and GraphTransformer \u2013 demonstrate competitive and closely aligned performance on the \\texttt{WebQSP} dataset, with Hit@1 scores of 70.70, 70.27, and 70.49, respectively. However, the performance differentiation becomes more pronounced on the \\texttt{ExplaGraphs} dataset, where GraphTransformer exhibits a superior Hit@1 score of 0.8516, followed by GAT and GCN with scores of 0.8430 and 0.8394, respectively. This variation in performance across the datasets highlights the importance of encoder selection based on the specific characteristics and requirements of the dataset.\n\n% We choose GAT as the defult graph encoder, because it can use edge features while GCN only utilize the node features (in our case, both node and edge has features).  GraphTransformer performs better than GAT in these two datasets but GAT is more time and parameter-efficient. \n\n% The performance metrics, as shown in Table~\\ref{tab: gnn}, reveal that our method maintains robustness regardless of the graph encoder used. Notably, all three encoders \u2013 GCN, GAT, and GraphTransformer \u2013 exhibit closely competitive performance, with Hit@1 scores of 0.6853, 0.6732, and 0.6910, respectively. This minimal variance in performance underscores the adaptability and stability of our approach across different graph encoding techniques.\n\n\n\n"
                },
                "subsection 10.5": {
                    "name": "The Choice of LLM",
                    "content": "\n\\label{appendix: llm}\n\nAs for the choice of LLM, we considered both Llama2-7b and Llama2-13b. Our experiments demonstrate that stronger LLMs enhance the effectiveness of our method, as shown in Table~\\ref{tab: llm}, indicating that it benefits from the increased scale of the LLMs.\n\n\n\n\n"
                }
            },
            "section 11": {
                "name": "GraphQA Benchmark",
                "content": "\n\\label{appendix: GraphQA benchmark}\n\n\n\n\nIn this section, we detail how our GraphQA benchmark differs from the original datasets, including the specific processing steps we employed. For concrete examples that illustrate the differences between the raw text in the original dataset and in our GraphQA benchmark, please refer to Table~\\ref{tab: dataset difference}.\n\n\n\\texttt{ExplaGraphs}. The original dataset\\footnote{https://explagraphs.github.io/}~\\citep{saha2021explagraphs} represents relationships using triplets. We have standardized this format by converting the triplets into a graph representation. Specifically, each head and tail in a triplet is transformed into a node, and the relation is transformed into an edge. Since the test dataset labels are not available, we have utilized only the training and validation (val) datasets from the original collection. We further divided these into training, val, and test subsets, using a 6:2:2 ratio.\n\n\\texttt{SceneGraphs}. The original GQA dataset is designed for real-world visual reasoning and compositional question answering, aiming to address key shortcomings of previous VQA datasets~\\citep{hudson2019gqa}. It comprises 108k images, each associated with a Scene Graph. In our study, we focus differently on graph question answering; hence, we did not utilize the image counterparts, leveraging only the scene graphs from the original dataset. Additionally, the original dataset describes images using JSON files. We simplified the object IDs to suit our research needs. We randomly sampled 100k samples from the original dataset and divided them into training, validation, and test subsets, following a 6:2:2 ratio.\n\n\n\\texttt{WebQSP}. We follow the preprocessing steps from RoG\\footnote{https://huggingface.co/datasets/rmanluo/RoG-webqsp}~\\citep{luo2023reasoning_rog}. The original dataset uses a list of triplets format, which we have transformed into our unified graph format. Furthermore, to avoid discrimination between capital and lowercase words, we have converted all words to lowercase. We used the same dataset split as in the original dataset.\n\n\n\n"
            },
            "section 12": {
                "name": "Graph Retrieval-Augmented Generation (GraphRAG)",
                "content": "\n\\label{appendix: graph_rag}\n\n",
                "subsection 12.1": {
                    "name": "Comparison with Existing GraphRAG Methods",
                    "content": "\n\nMost existing GraphRAG methods are designed specifically for knowledge graphs and focus on node, edge, or triples-level retrieval~\\citep{baek-etal-2023-knowledge_kaping, sen-etal-2023-knowledge_rigel, kang2023knowledge_SURGE, sun2024thinkongraph}. Our method is different in two main ways: 1) It focuses on more general textual graphs, not just knowledge graphs. 2) It enables the return of a subgraph most closely related to a query, rather than a list of top-k triples. The triples in other methods are chosen in isolation from the graph, failing to capture neighborhood information effectively. In contrast, our method takes the context into account during retrieval.\n\n\n\n% \\xh{Our PCST-based retrieval method, while similar to these GraphRAG methods in that all aim to conditionally retrieve information based on the query to augment the input for the LLM, significantly differs in the nature of retrieved information, the adaptability of the retriever, and the types of graphs it can handle. These distinctions highlight our method's broader applicability beyond the existing strategies focused on KGs.}\n\n\n\n% \\begin{table}[!ht]\n% \\caption{Comparison with existing GraphRAG methods.}\n% \\vskip 0.1in\n% \\label{tab: graph rag}\n%     \\centering\n%     \\begin{tabular}{lccc}\n%     \\toprule\n%     Method & Retrieved Information & Retriever & Type of \n%     Graphs\\\\\n%     \\midrule\n%     KAPING~\\citep{baek-etal-2023-knowledge_kaping} & top-k triples & static & textual graphs\\\\\n%     RIGEL~\\citep{sen-etal-2023-knowledge_rigel}&top-k triples & trainable & knowledge graphs\\\\\n%     SURGE~\\citep{kang2023knowledge_SURGE}& a subgraph & trainable & knowledge graphs\\\\\n%     Think-on-Graph~\\citep{sun2024thinkongraph} & a subgraph & static & knowledge graphs\\\\\n%     G-Retriever & a subgraph & static & textual graphs \\\\\n%     \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n\n"
                },
                "subsection 12.2": {
                    "name": "The Impact of K for Retrieval",
                    "content": "\n\nWe identify the most relevant nodes and edges and use a k-nearest neighbors retrieval approach (see Equation~\\ref{eq: topk}). Small k values may omit crucial knowledge or information relevant to the query, while large k values could introduce excessive information, distracting the model from the essential details. To evaluate the impact of the number of k, we have conducted additional experiments by varying the choice of k to 3, 5, 10, and 20.\n\n\n\nAs shown in Table~\\ref{tab: choice of k}, the Hit@1 metric initially rises for small k values, peaks at a certain point, and then declines for large k values. Determining the optimal k value can be achieved through techniques like cross-validation using a validation set.\n\n"
                },
                "subsection 12.3": {
                    "name": "The Choice of Similarity Function",
                    "content": "\n\nThe choice of similarity function is also important. In this work, we use cosine similarity, a widely adopted metric for measuring vector similarity in models that process vision and language. For instance, CLIP also employs cosine similarity to assess the similarity between text and image features. Although it might not be the optimal choice, we believe that cosine similarity is a general, representative, and valid choice for facilitating fast retrieval tasks.\n\n"
                },
                "subsection 12.4": {
                    "name": "The Quality of Retrieval",
                    "content": "\nWe quantify the quality of our retrieval method as follows: We examine the retrieval subgraph, and if the label is contained within it, we consider it a successful retrieval. We calculate the retrieval success rate of our method and the retrieval method proposed in KAPING~\\citep{baek-etal-2023-knowledge_kaping} on the WebQSP dataset.\n\n\nAs shown in Table~\\ref{tab: retrieval quality}, these results validate the effectiveness of our retrieval method. In contrast to the triple-based retrieval in KAPING, which relies on the similarity between triples and the query, our PCST-based subgraph retrieval is more accurate as it takes the graph structure into account. By design, it retrieves connected subgraphs, capturing not just nodes or edges with high relevance scores, but also those that act as \"bridges\" connecting other highly relevant nodes and edges.\n\n\n"
                }
            },
            "section 13": {
                "name": "Discussion on the Complexity",
                "content": "\n\\label{appendix: complexity}\n\n",
                "subsection 13.1": {
                    "name": "The integration of GNNs, LLMs and GraphRAG",
                    "content": "\n\\textit{G-Retriever} is framework integrate the strengths of GNNs, LLMs and GraphRAG. The LLM+X framework, which involves enriching LLMs with multi-modal capabilities by integrating an LLM with an encoder from another modality, is a widely adopted approach. Notable examples include Llava, MiniGPT-4, and Flamingo, among others. They are not complex in terms of understanding or implementation. Regarding the integration of GraphRAG, it does not require training and can be implemented during the preprocessing stage or on the fly. This approach does not significantly increase time complexity or computational complexity. On the contrary, it can substantially reduce the size of the graph (\\eg eliminating 99\\% of nodes in the \\texttt{WebQSP} dataset), which in turn speeds up the overall running time (\\eg reducing it from 18.7 min/epoch to 6.2 min/epoch on the \\texttt{WebQSP} dataset).\n\n"
                },
                "subsection 13.2": {
                    "name": "Computational Resources",
                    "content": "\nUtilizing two A100 GPUs, each with 80GB of memory, we conducted tests on Llama2-7b and \\texttt{WebQSP} datasets. Our experiments had a training batch size of 16 and an evaluation batch size of 32, yielding the following results.\n\n\n\nThese results highlight efficiency improvements via graph RAG, which significantly reduces graph size (e.g., eliminating 99\\% of nodes in the \\texttt{WebQSP} dataset) and speeds up running time.\n\n"
                }
            },
            "section 14": {
                "name": "Hallucination in Graph LLMs",
                "content": "\n\\label{appendix: hallucination}\n\n\nIn this section, we present quantitative results regarding hallucinations in the \\texttt{SceneGraphs} dataset.\n\n\\textbf{Baseline.} For our baseline, we adapted MiniGPT-4~\\citep{zhu2023minigpt} to graph contexts. This approach involves a frozen LLM interacting with a trainable GNN that encodes graph data as a soft prompt, denoted as LLM+Graph Prompt Tuning. We focus on graph prompt tuning as the baseline, instead of converting the graph into text, since the textual representation of the graph is large and consistently exceeds the input token limits of LLMs. \n% For example, as shown in Table~\\ref{tab: efficiency}, the average number of tokens is 1,396 and 100,627  for \\texttt{SceneGraphs} and \\texttt{WebQSP} respectively, out\n\n\n\n\\textbf{Experiment Design.}\nWe instructed the LLM to answer graph-related questions and to list nodes or edges in the explanation graph that support its answers. Since standard answers for these questions do not exist, allowing the LLM to respond flexibly, it becomes challenging to evaluate its responses. To address this, we manually examined 100 responses generated by our method and the LLM with graph prompt tuning, verifying whether the nodes and edges referenced in the LLM's output actually exist in the graph. \n% It is important to note that if the LLM cites multiple supporting nodes or edges in its response, and any of these are nonexistent (or `hallucinated'), we classify the entire answer as a hallucination. This strict criterion simplifies the evaluation process.\n\n\\textbf{Evaluation Metrics.}\nWe assessed the model's faithfulness using three metrics: the fraction of valid nodes (denoted as Valid Nodes), the fraction of valid edges (denoted as Valid Edges), and the fraction of times the entire set of nodes and edges cited was valid (denoted as Fully Valid Graphs).\n% To measure faithfulness, we use the fraction of valid nodes, fraction of valid edges, fraction of times the entire set of nodes and edges is valid. \n\n\n\\textbf{Results.} \nThe results, as depicted in Table~\\ref{tab: hallucination_gqa}, illustrate the comparative effectiveness of the \\textit{G-Retriever} over the baseline LLM+Graph Prompt Tuning method in reducing hallucinations. The LLM+Graph Prompt Tuning approach demonstrated a significantly lower accuracy in referencing graph elements, with only 31\\% of nodes and 12\\% of edges being valid, and the entire set of nodes and edges being valid only 8\\% of the time. In contrast, \\textit{G-Retriever} showed substantial improvements: 77\\% validity in nodes, 76\\% in edges, and 62\\% in the overall validity of referenced node-edge sets. These results underscore the significant reduction in hallucinations with \\textit{G-Retriever}, particularly in the challenging task of accurately citing both nodes and edges in graph-based contexts.\n\n\n\n\n\n% \\textbf{Results.} \n% The results, as presented in Table~\\ref{tab: hallucination_gqa}, reveal that the LLM+Graph Prompt Tuning method entirely lacks the capability to recall knowledge from the original graph data. It consistently hallucinates a graph with node 1 as Argument 1, node 2 as Argument 2, and an edge between them with the attribute `support' or `counter', as exemplified in Table~\\ref{tab: hallucination_gqa}. Conversely, our \\textit{G-Retriever} method significantly increases the number of answers with faithful references to the original nodes and edges in the graph. \n\n\n\n\n"
            },
            "section 15": {
                "name": "Demonstrations",
                "content": "\n\\label{appendix: demo}\n\nWe demonstrate the interaction capabilities of G-Retriever with creative questions on different datasets: \\texttt{ExplaGraphs}, \\texttt{SceneGraphs}, and \\texttt{WebQSP}. These examples are showcased in Tables~\\ref{tab: chat_expla_graph}, \\ref{tab: chat_scene_graph}, and \\ref{tab: chat_webqsp}, respectively. Additionally, the examples are visualized in Figure~\\ref{fig: chat}.\n\n\n\n\n\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n"
            }
        },
        "tables": {
            "tab: hallucination": "\\begin{table}[t]\n\\caption{Observation and mitigation of hallucination in graph LLMs.}\n    \\label{tab: hallucination}\n    \\vskip 0.1in\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{m{4cm}m{9cm}}\n    \\toprule\n    \\multirow{2}{*}{User}     \n&  \\begin{minipage}{.3\\textwidth}\n    \\includegraphics[width=0.8\\linewidth]{figs/scene.pdf}\n\\end{minipage}\\\\\n&  What kind of animal is in the bushes? \\newline Answer the question and list the nodes or edges from the graph that support your answer.\\\\\n \n\\midrule\nLLM w/ Graph Prompt Tuning\n& \\textcolor{red}{[Incorrect response with hallucinated nodes and edges]} \\newline \nThe animal in the bushes is a deer.\\newline\nNodes:\\newline \n* \\textcolor{red}{Deer (node 1)} \\newline * Bushes (node 2)\\newline\nEdges:\\newline \n* \\textcolor{red}{Deer $\\rightarrow$ Bushes (edge 1)}\\newline\n* \\textcolor{red}{Deer $\\rightarrow$ Grass (edge 2)}\\newline\n* \\textcolor{red}{Bushes $\\rightarrow$ Grass (edge 3)}\n\\\\\n\\midrule\n\\textit{G-Retriever} (Ours)\n& \\textcolor{blue}{[Correct response with accurate node and edge references]} \\newline\nBased on the information provided in the graph, the animal in the bushes is an elephant.\\newline\nThe nodes that support this answer are:\\newline\n* Node 1: The \\textcolor{blue}{elephant} is mentioned in the first line of the graph.\\newline\nThe edges that support this answer are:\\newline\n* The edge \\textcolor{blue}{connects the elephant to the bushes}, indicating that the {elephant} is {in the bushes.}\\newline\nTherefore, the answer to the question is: The animal in the bushes is an elephant.\n\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab: dataset": "\\begin{table}[t]\n    \\centering\n    \\scriptsize\n    \\caption{Summary of datasets used in GraphQA benchmark.}\n    \\vskip 0.1in\n    \\label{tab: dataset}\n    \\begin{tabular}{lccc}\n    \\toprule\n         Dataset \n         & \\texttt{ExplaGraphs} \n         &  \\texttt{SceneGraphs} \n         & \\texttt{WebQSP}\\\\\n    \\midrule\n    \\#Graphs & 2,766 & 100,000 & 4,737\\\\\n    Avg. \\#Nodes & 5.17 & 19.13 & 1370.89\\\\\n    Avg. \\#Edges & 4.25 & 68.44 & 4252.37\\\\\n    Node Attribute\n    & Commonsense concepts\n    & Object attributes (\\eg color, shape)  \n    & Entities in Freebase\n    \\\\\n    Edge Attribute\n    & Commonsense relations\n    & Relations (\\eg actions, spatial relations)  \n    & Relations in Freebase\n    \\\\\n    Task\n    & Common sense reasoning\n    & Scene graph question answering \n    & Knowledge based question answering \n    \\\\\n    Evaluation Matrix & Accuracy & Accuracy & Hit@1\n    \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: main": "\\begin{table*}[!ht]\n\\caption{Performance comparison across \\texttt{ExplaGraphs}, \\texttt{SceneGraphs}, and \\texttt{WebQSP} datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings. Mean scores and standard deviations (mean \u00b1 std) are presented. The first best result for each task is highlighted in \\textbf{bold} and the second best result is highlighted with an \\underline{underline}.\n% A dashed entry indicates the non-applicability of the `Question-Only' method for \\texttt{SceneGraphs}, where questions cannot be answered without the textual graph.\n}\n\\footnotesize\n    \\label{tab: main}\n    % \\vskip 0.1in\n    \\centering\n    \\hspace*{-0.3in}\n    \\begin{tabular}{l|lcccc}\n    \\toprule\n    Setting & Method & \\texttt{ExplaGraphs} & \\texttt{SceneGraphs} & \\texttt{WebQSP} \\\\\n    \\midrule\n          \\multirow{4}{*}{Inference-only}\n          % & Question only & 0.6232 & -- & 61.16\\\\\n          & Zero-shot & 0.5650 & 0.3974 & 41.06 \\\\\n          & Zero-CoT~\\citep{kojima2022large_zero_cot} & 0.5704 & 0.5260 & 51.30\\\\\n          & CoT-BAG~\\citep{wang2023can_graphllm_survey} & 0.5794 & 0.5680 & 39.60  \\\\\n          & KAPING~\\citep{baek-etal-2023-knowledge_kaping} & {0.6227} & {0.4375} & {52.64}\\\\\n          % & CoT\\\\\n    \\midrule\n    \\multirow{4}{*}{Frozen LLM w/ PT}\n        & Prompt tuning \n        & 0.5763 \u00b1 0.0243\n        & 0.6341 \u00b1 0.0024\n        & 48.34 \u00b1 0.64\n        \\\\\n        & {GraphToken~\\citep{perozzi2024let_graphtoken}}\n        & {0.8508 \u00b1 0.0551}\n        & {0.4903 \u00b1 0.0105}\n        & {57.05\t\u00b1 0.74}\n        \\\\\n        & \\textit{G-Retriever}\n        & 0.8516 \u00b1 0.0092\n        & \\underline{0.8131 \u00b1 0.0162}\n        & \\underline{70.49 \u00b1 1.21}\n        \\\\\n        & $\\Delta_\\text{Prompt tuning}$ \n        & $\\uparrow$ 47.77\\%\n        & $\\uparrow$ 28.23\\%\n        & $\\uparrow$ 45.81\\%\\\\\n    \\midrule\n    \\multirow{3}{*}{Tuned LLM}\n    & LoRA \n    & \\underline{0.8538 \u00b1 0.0353}\n    & 0.7862 \u00b1 0.0031\n    & 66.03 \u00b1 0.47\\\\\n    & \\textit{G-Retriever} w/ LoRA \n    & \\textbf{0.8705 \u00b1 0.0329}\n    & \\textbf{0.8683 \u00b1 0.0072}\n    & \\textbf{73.79 \u00b1 0.70}\\\\\n    & $\\Delta_\\text{ LoRA}$ \n    & $\\uparrow$ 1.95\\%\n    & $\\uparrow$ 11.74\\%\n    & $\\uparrow$ 10.44\\%\\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table*}",
            "tab: efficiency": "\\begin{table}[t]\n\\centering\n\\footnotesize\n\\caption{Retrieval on graphs significantly improves efficiency.}\n\\vskip 0.1in\n\\label{tab: efficiency}\n\\begin{tabular}{lcccccc}\n\\toprule\n\\multirow{2}{*}{Dataset}\n& \\multicolumn{3}{c}{Before Retrieval (Avg.)} & \\multicolumn{3}{c}{After Retrieval (Avg.)} \\\\\n\\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n & \\# Tokens & \\# Nodes &Min/Epoch \n & \\# Tokens & \\# Nodes &Min/Epoch \\\\\n\\midrule\n\\texttt{SceneGraphs} \n& 1,396 & 19 & 123.1\n& 235 ($\\downarrow$83\\%) & 5 ($\\downarrow$74\\%) & 86.8 ($\\downarrow$29\\%)\n\\\\\n\\texttt{WebQSP} \n& 100,627 & 1,371 & 18.7\n& 610 ($\\downarrow$99\\%) & 18 ($\\downarrow$99\\%) & 6.2($\\downarrow$67\\%)\n\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab: gnn": "\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\caption{Performance of different graph encoders on the \\texttt{WebQSP} and \\texttt{ExplaGraphs} datasets.}\n    \\vskip 0.1in\n    \\label{tab: gnn}\n    \\begin{tabular}{lcc}\n    \\toprule\n    Graph Encoder&  \\texttt{WebQSP}  & \\texttt{ExplaGraphs}\\\\\n    \\midrule\n    GCN~\\citep{kipf2016semi_gcn}   \n    &  70.70\n    & 0.8394  \\\\\n    GAT~\\citep{velivckovic2017graph_gat} \n    & 70.27\n    & 0.8430 \\\\\n    Graph Transformer~\\citep{shi2020masked_graph_trans} \n    & 70.49\n    & 0.8516\\\\\n    \\bottomrule\n    \\end{tabular}\n    \n\\end{table}",
            "tab: llm": "\\begin{table}[!ht]\n    \\centering\n    \\caption{Performance of different LLMs on the \\texttt{WebQSP} dataset.}\n    \\vskip 0.1in\n    \\label{tab: llm}\n    \\begin{tabular}{lcc}\n    \\toprule\n         LLM & Llama2-7b & Llama2-13b \\\\\n         \\midrule\n         Hit@1 & {70.49} & {75.58} \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: dataset difference": "\\begin{table}[!ht]\n\\caption{Comparison of text formats in original datasets and our GraphQA benchmark.}\n    \\label{tab: dataset difference}\n    \\vskip 0.1in\n\\tiny\n    \\centering\n    \\hspace*{-0.5in}\n    \\begin{tabular}{m{1.5cm}m{7cm}m{7cm}}\n    \\toprule\n\nDataset &Original dataset& GraphQA Benmark \\\\\n\\midrule\n\\texttt{ExplaGraphs}\n&\n(entrapment; capable of; being abused) \n(being abused; created by; police)\n(police; capable of; harm)\n(harm; used for; people)\n(people; part of; citizens)\n&\nnode\\_id,node\\_attr\\newl\n0,entrapment\\newl\n1,being abused\\newl\n2,police\\newl\n3,harm\\newl\n4,people\\newl\n5,citizens\\newl\n\nsrc,edge\\_attr,dst\\newl\n0,capable of,1\\newl\n1,created by,2\\newl\n2,capable of,3\\newl\n3,used for,4\\newl\n4,part of,5\\newl\n\\\\\n\\midrule\n\\texttt{SceneGraphs}\n&\n{{\"width\": 500, \"objects\": {\"681267\": {\"name\": \"banana\", \"h\": 34, \"relations\": [{\"object\": \"681262\", \"name\": \"to the left of\"}], \"w\": 64, \"attributes\": [\"small\", \"yellow\"], \"y\": 55, \"x\": 248}, \"681265\": {\"name\": \"spots\", \"h\": 16, \"relations\": [], \"w\": 26, \"attributes\": [], \"y\": 92, \"x\": 245}, \"681264\": {\"name\": \"bananas\", \"h\": 50, \"relations\": [{\"object\": \"681259\", \"name\": \"to the left of\"}], \"w\": 49, \"attributes\": [\"small\", \"yellow\"], \"y\": 32, \"x\": 268}, \"681263\": {\"name\": \"picnic\", \"h\": 374, \"relations\": [], \"w\": 499, \"attributes\": [\"delicious\"], \"y\": 0, \"x\": 0}, \"681262\": {\"name\": \"straw\", \"h\": 95, \"relations\": [{\"object\": \"681268\", \"name\": \"to the right of\"}, {\"object\": \"681267\", \"name\": \"to the right of\"}, {\"object\": \"681253\", \"name\": \"to the right of\"}], \"w\": 15, \"attributes\": [\"white\", \"plastic\"], \"y\": 55, \"x\": 402}, \"681261\": {\"name\": \"meat\", \"h\": 27, \"relations\": [{\"object\": \"681255\", \"name\": \"on\"}, {\"object\": \"681255\", \"name\": \"inside\"}], \"w\": 24, \"attributes\": [\"small\", \"brown\", \"delicious\"], \"y\": 123, \"x\": 68}, \"681260\": {\"name\": \"rice\", \"h\": 57, \"relations\": [{\"object\": \"681255\", \"name\": \"on\"}, {\"object\": \"681258\", \"name\": \"to the left of\"}], \"w\": 93, \"attributes\": [\"piled\", \"white\"], \"y\": 162, \"x\": 57}, \"681269\": {\"name\": \"onions\", \"h\": 16, \"relations\": [], \"w\": 24, \"attributes\": [\"green\"], \"y\": 147, \"x\": 90}, \"681268\": {\"name\": \"tablecloth\", \"h\": 374, \"relations\": [{\"object\": \"681262\", \"name\": \"to the left of\"}], \"w\": 396, \"attributes\": [\"white\"], \"y\": 0, \"x\": 0}, \"681258\": {\"name\": \"bowl\", \"h\": 99, \"relations\": [{\"object\": \"681255\", \"name\": \"next to\"}, {\"object\": \"681257\", \"name\": \"of\"}, {\"object\": \"681255\", \"name\": \"near\"}, {\"object\": \"681256\", \"name\": \"to the right of\"}, {\"object\": \"681260\", \"name\": \"to the right of\"}, {\"object\": \"681255\", \"name\": \"to the right of\"}], \"w\": 115, \"attributes\": [\"full\"], \"y\": 184, \"x\": 178}, \"681259\": {\"name\": \"plantains\", \"h\": 70, \"relations\": [{\"object\": \"681264\", \"name\": \"to the right of\"}], \"w\": 45, \"attributes\": [\"red\"], \"y\": 0, \"x\": 346}, \"681256\": {\"name\": \"spoon\", \"h\": 65, \"relations\": [{\"object\": \"681255\", \"name\": \"on\"}, {\"object\": \"681257\", \"name\": \"to the left of\"}, {\"object\": \"681255\", \"name\": \"in\"}, {\"object\": \"681258\", \"name\": \"to the left of\"}], \"w\": 140, \"attributes\": [\"large\", \"metal\", \"silver\"], \"y\": 196, \"x\": 0}, \"681257\": {\"name\": \"dish\", \"h\": 81, \"relations\": [{\"object\": \"681258\", \"name\": \"inside\"}, {\"object\": \"681256\", \"name\": \"to the right of\"}, {\"object\": \"681258\", \"name\": \"in\"}, {\"object\": \"681255\", \"name\": \"to the right of\"}], \"w\": 108, \"attributes\": [\"cream colored\"], \"y\": 199, \"x\": 187}, \"681254\": {\"name\": \"meal\", \"h\": 111, \"relations\": [], \"w\": 130, \"attributes\": [], \"y\": 121, \"x\": 58}, \"681255\": {\"name\": \"plate\", \"h\": 138, \"relations\": [{\"object\": \"681257\", \"name\": \"to the left of\"}, {\"object\": \"681254\", \"name\": \"of\"}, {\"object\": \"681254\", \"name\": \"with\"}, {\"object\": \"681258\", \"name\": \"near\"}, {\"object\": \"681258\", \"name\": \"to the left of\"}], \"w\": 176, \"attributes\": [\"white\", \"full\"], \"y\": 111, \"x\": 30}, \"681253\": {\"name\": \"banana\", \"h\": 30, \"relations\": [{\"object\": \"681262\", \"name\": \"to the left of\"}], \"w\": 73, \"attributes\": [\"small\", \"yellow\"], \"y\": 87, \"x\": 237}}, \"height\": 375}}\n& \nnode\\_id,node\\_attr\\newline\n0,\"name: banana; attribute: small, yellow; (x,y,w,h): (248, 55, 64, 34)\"\\newline\n1,\"name: spots; (x,y,w,h): (245, 92, 26, 16)\"\\newline\n2,\"name: bananas; attribute: small, yellow; (x,y,w,h): (268, 32, 49, 50)\"\\newline\n3,\"name: picnic; attribute: delicious; (x,y,w,h): (0, 0, 499, 374)\"\\newline\n4,\"name: straw; attribute: white, plastic; (x,y,w,h): (402, 55, 15, 95)\"\\newline\n5,\"name: meat; attribute: small, brown, delicious; (x,y,w,h): (68, 123, 24, 27)\"\\newline\n6,\"name: rice; attribute: piled, white; (x,y,w,h): (57, 162, 93, 57)\"\\newline\n7,\"name: onions; attribute: green; (x,y,w,h): (90, 147, 24, 16)\"\\newline\n8,\"name: tablecloth; attribute: white; (x,y,w,h): (0, 0, 396, 374)\"\\newline\n9,\"name: bowl; attribute: full; (x,y,w,h): (178, 184, 115, 99)\"\\newline\n10,\"name: plantains; attribute: red; (x,y,w,h): (346, 0, 45, 70)\"\\newline\n11,\"name: spoon; attribute: large, metal, silver; (x,y,w,h): (0, 196, 140, 65)\"\\newline\n12,\"name: dish; attribute: cream colored; (x,y,w,h): (187, 199, 108, 81)\"\\newline\n13,\"name: meal; (x,y,w,h): (58, 121, 130, 111)\"\\newline\n14,\"name: plate; attribute: white, full; (x,y,w,h): (30, 111, 176, 138)\"\\newline\n15,\"name: banana; attribute: small, yellow; (x,y,w,h): (237, 87, 73, 30)\"\\newline\nsrc,edge\\_attr,dst\\newline\n0,to the left of,4\\newl\n2,to the left of,10\\newl\n4,to the right of,8\\newl\n4,to the right of,0\\newl\n4,to the right of,15\\newl\n5,on,14\\newl\n5,inside,14\\newl\n6,on,14\\newl\n6,to the left of,9\\newl\n8,to the left of,4\\newl\n9,next to,14\\newl\n9,of,12\\newl\n9,near,14\\newl\n9,to the right of,11\\newl\n9,to the right of,6\\newl\n9,to the right of,14\\newl\n10,to the right of,2\\newl\n11,on,14\\newl\n11,to the left of,12\\newl\n11,in,14\\newl\n11,to the left of,9\\newl\n12,inside,9\\newl\n12,to the right of,11\\newl\n12,in,9\\newl\n12,to the right of,14\\newl\n14,to the left of,12\\newl\n14,of,13\\newl\n14,with,13\\newl\n14,near,9\\newl\n14,to the left of,9\\newl\n15,to the left of,4\\newl\n\\\\\n\\midrule\n\\texttt{WebQSP}\n& \n[['FedEx Cup', 'sports.sports\\_award\\_type.winners', 'm.0n1v8cy'],\\newline\n  ['Brandt Snedeker', 'sports.sports\\_award\\_winner.awards', 'm.0n1v8cy'],\\newline\n  ['FedEx Cup', 'common.topic.article', 'm.08q5wy'],\\newline\n  ['FedEx Cup', 'common.topic.notable\\_for', 'g.12559n8g\\_'],\\newline\n  ['Sports League Award Type', 'freebase.type\\_profile.published', 'Published'],\\newline\n  ['FedEx Cup', 'common.topic.notable\\_types', 'Sports League Award Type'],\\newline\n  ['m.0n1v8cy', 'sports.sports\\_award.award\\_winner', 'Brandt Snedeker'],\\newline\n  ['Sports League Award Type', 'type.type.expected\\_by', 'Award'],\\newline\n  ['Sports League Award Type', 'common.topic.article', 'm.06zxtxj'],\\newline\n  ['2012 PGA Tour', 'sports.sports\\_league\\_season.awards', 'm.0n1v8cy'],\\newline\n  ['Sports League Award Type', 'freebase.type\\_hints.included\\_types', 'Topic'],\\newline\n  ['Sports League Award Type', 'type.type.domain', 'Sports'],\\newline\n  ['m.0n1v8cy', 'sports.sports\\_award.award', 'FedEx Cup'],\\newline\n  ['Sports League Award Type',\n   'freebase.type\\_profile.strict\\_included\\_types',\n   'Topic'],\\newline\n  ['Sports League Award Type', 'freebase.type\\_profile.kind', 'Classification'],\\newline\n  ['m.0n1v8cy', 'sports.sports\\_award.season', '2012 PGA Tour'],\\newline\n  ['Sports League Award Type', 'type.type.properties', 'Winners']]\n&\nnode\\_id,node\\_attr\\newl\n0,fedex cup\\newl\n1,m.0n1v8cy\\newl\n2,brandt snedeker\\newl\n3,m.08q5wy\\newl\n4,g.12559n8g\\_\\newl\n5,sports league award type\\newl\n6,published\\newl\n7,award\\newl\n8,m.06zxtxj\\newl\n9,2012 pga tour\\newl\n10,topic\\newl\n11,sports\\newl\n12,classification\\newl\n13,winners\\newl\n\nsrc,edge\\_attr,dst\n\n0,sports.sports\\_award\\_type.winners,1\\newline\n2,sports.sports\\_award\\_winner.awards,1\\newline\n0,common.topic.article,3\\newline\n0,common.topic.notable\\_for,4\\newline\n5,freebase.type\\_profile.published,6\\newline\n0,common.topic.notable\\_types,5\\newline\n1,sports.sports\\_award.award\\_winner,2\\newline\n5,type.type.expected\\_by,7\\newline\n5,common.topic.article,8\\newline\n9,sports.sports\\_league\\_season.awards,1\\newline\n5,freebase.type\\_hints.included\\_types,10\\newline\n5,type.type.domain,11\\newline\n1,sports.sports\\_award.award,0\\newline\n5,freebase.type\\_profile.strict\\_included\\_types,10\\newline\n5,freebase.type\\_profile.kind,12\\newline\n1,sports.sports\\_award.season,9\\newline\n5,type.type.properties,13\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab: choice of k": "\\begin{table}[!ht]\n    \\centering\n    \\caption{The impact of k on the \\texttt{webqsp} dataset.}\n    \\vskip 0.1in\n    \\label{tab: choice of k}\n    \\begin{tabular}{lcccc}\n    \\toprule\n    k     &  3  & 5 & 10 & 20\\\\\n    \\midrule\n    Hit@1   & 0.6977 & 0.7063 & 0.7248 & 0.7039\\\\\n         \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: retrieval quality": "\\begin{table}[!ht]\n    \\centering\n    \\caption{The quality of retrieval methods on the \\texttt{WebQSP} dataset.}\n    \\vskip 0.1in\n    \\label{tab: retrieval quality}\n    \\begin{tabular}{lc}\n    \\toprule\n         Method& Retrieval Accuracy \\\\\n         \\midrule\n         KAPING~\\citep{baek-etal-2023-knowledge_kaping} (top-k triple retrieval)& 60.81\\\\\n         \\textit{G-Retriever} & 70.49\\\\\n         \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: my_label": "\\begin{table}[!ht]\n    \\centering\n    \\caption{Performance and Efficiency of Various Methods on the \\texttt{WebQSP} dataset.}\n    \\vskip 0.1in\n    \\label{tab: my_label}\n    \\begin{tabular}{llll}\n    \\toprule\n    Settting & Method & Hit@1 & Time\\\\\n    \\midrule\n    \\multirow{2}{*}{Inference-only}\n    &  Question only & 61.16 & 31 min\\\\\n    & Textual graph and question & 41.06 & 40 min\\\\ \n    \\midrule\n    \\multirow{2}{*}{Frozen LLM w/ PT}\n    & Prompt Tuning & 48.34 &18.7 min/epoch\\\\\n    & G-Retriever & 70.49 & 6.2 min/epoch\\\\\n    \\midrule\n    \\multirow{2}{*}{Tuned LLM}\n    & LoRA & 66.03 & 19 min/epoch\\\\\n    & \\textit{G-Retriever} w/ LoRA & 73.79 & 6.9 min/epoch\\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: chat_expla_graph": "\\begin{table}[!ht]\n\\caption{Example of creative chat on \\texttt{ExplaGraphs}.}\n    \\label{tab: chat_expla_graph}\n    \\vskip 0.1in\n    \\centering\n    \\small\n    \\begin{tabularx}{\\textwidth}{l|X}\n    \\toprule\n    Graph &\n    node\\_id,node\\_attr\\newline\n0,human\\newline\n1,cosmetic surgery\\newline\n2,women\\newline\n3,change appearance\\newline\n4,addiction\\newline\n5,searching for perfection\\newline\n6,stop\\newline\n7,positive\\newline\n\nsrc,edge\\_attr,dst\\newline\n0,receives action,1\\newline\n2,part of,0\\newline\n1,used for,3\\newline\n3,capable of,4\\newline\n4,created by,5\\newline\n4,not capable of,6\\newline\n4,is not a,7\n\\\\\n\\midrule\nUser & Write a 500-word argument essay based on the information provided in the explanation graph.\\\\\n\\midrule\nModel &{The explanation graph provides a detailed analysis of the reasons why women undergo cosmetic surgery. According to the graph, the main reasons include the desire to change one's appearance, the influence of societal beauty standards, and the addiction to searching for perfection. These reasons are further supported by the various nodes and edges in the graph, which provide additional insights into the motivations and consequences of cosmetic surgery.\\newline\nFirstly, the node labeled {\"human\"} highlights the fundamental human desire to feel attractive and confident in one's appearance. This desire is deeply ingrained in our culture, where beauty is often associated with happiness, success, and social acceptance. As a result, many women feel pressure to conform to societal beauty standards, which can lead to feelings of insecurity and low self-esteem. In response to these feelings, some women may turn to cosmetic surgery as a means of improving their appearance and boosting their self-confidence.\\newline\nSecondly, the node labeled \"women\" highlights the gendered nature of cosmetic surgery. While men also undergo cosmetic surgery, the majority of cosmetic surgery patients are women. This is likely due to the fact that women are more likely to be exposed to societal beauty standards and are more likely to internalize these standards. As a result, women may be more likely to feel insecure about their appearance and more likely to seek cosmetic surgery as a means of improving their looks.\\newline\nThirdly, the node labeled \"addiction\" highlights the potential negative consequences of cosmetic surgery. While some women may initially feel improved confidence and self-esteem after undergoing cosmetic surgery, the graph suggests that this initial boost may not last. In fact, some women may become addicted to the constant pursuit of perfection, leading to a cycle of surgery and dissatisfaction. This can have serious psychological and emotional consequences, including depression, anxiety, and a loss of self-esteem.\\newline\nFourthly, the node labeled \"searching for perfection\" highlights the unrealistic expectations that many women have about the results of cosmetic surgery. The graph suggests that women may have unrealistic expectations about the ability of cosmetic surgery to improve their appearance and boost their self-esteem. This can lead to disappointment and dissatisfaction with the results of surgery, as well as a continued pursuit of perfection through additional surgery.\\newline\nFinally, the node labeled \"stop\" highlights the importance of critical thinking and self-reflection when it comes to cosmetic surgery. The graph suggests that women should be aware of the potential negative consequences of cosmetic surgery and should carefully consider the reasons why they are undergoing surgery. They should also be aware of the unrealistic expectations that they may have about the results of surgery and should be prepared to address any negative emotions or feelings of dissatisfaction that may arise.\\newline\nIn conclusion, the explanation graph provides a detailed analysis of the reasons why women undergo cosmetic surgery. The graph highlights the fundamental human desire to feel attractive and confident in one's appearance, the gendered nature of cosmetic surgery, the potential negative consequences of cosmetic surgery, and the importance of critical thinking and self-reflection when it comes to surgery. By understanding these reasons and the potential consequences of cosmetic surgery, women can make informed decisions about whether or not to undergo surgery and can work to address any negative emotions or feelings of dissatisfaction that may arise.}\n\\\\\n\\bottomrule\n    \\end{tabularx}\n\\end{table}",
            "tab: chat_scene_graph": "\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\caption{Example of creative chat on \\texttt{SceneGraphs}. We utilize GPT-4 to generate an image from the graph, assisting readers in visualizing it}\n    \\vskip 0.1in\n    \\label{tab: chat_scene_graph}\n    \\begin{tabularx}{\\textwidth}{l|X}\n    \\toprule\n         Graph \n         &  node\\_id,node\\_attr\\newline\n0,name: head; attribute: black; (x,y,w,h): (386, 185, 8, 4)\\newline\n1,name: swimming pool; attribute: small, water; (x,y,w,h): (465, 92, 35, 3)\\newline\n2,name: shore; (x,y,w,h): (49, 85, 436, 108)\\newline\n3,name: fence; attribute: wired; (x,y,w,h): (11, 158, 167, 46)\\newline\n4,name: mountains; attribute: covered; (x,y,w,h): (393, 53, 107, 12)\\newline\n5,name: water; attribute: blue; (x,y,w,h): (0, 50, 499, 154)\\newline\n6,name: sheep; attribute: walking, grazing; (x,y,w,h): (243, 191, 20, 13)\\newline\n7,name: sky; attribute: blue, clear, cloudy; (x,y,w,h): (0, 0, 501, 60)\\newline\n8,name: sheep; attribute: white, fluffy; (x,y,w,h): (409, 176, 13, 10)\\newline\n9,name: field; attribute: large, green, grassy; (x,y,w,h): (0, 138, 503, 112)\\newline\n10,name: grass; (x,y,w,h): (0, 84, 500, 166)\\newline\n11,name: sheep; attribute: white, fluffy; (x,y,w,h): (398, 178, 13, 11)\\newline\n12,name: sheep; attribute: white, fluffy; (x,y,w,h): (385, 179, 17, 11)\\newline\n13,name: sheep; attribute: white, fluffy; (x,y,w,h): (364, 181, 14, 10)\\newline\n14,name: shore; attribute: sandy; (x,y,w,h): (190, 123, 171, 12)\\newline\nsrc, edge\\_attr, dst\\newline\n0,to the left of,11\\newl\n0,to the right of,13\\newl\n3,to the left of,6\\newl\n6,to the right of,3\\newl\n6,grazing on,10\\newl\n6,in,9\\newl\n6,in,10\\newl\n7,above,5\\newl\n8,eating,10\\newl\n8,to the right of,11\\newl\n8,to the right of,12\\newl\n11,to the left of,8\\newl\n11,to the right of,0\\newl\n11,to the right of,13\\newl\n11,to the right of,12\\newl\n11,eating,10\\newl\n12,eating,10\\newl\n12,to the left of,8\\newl\n12,to the right of,13\\newl\n12,to the left of,11\\newl\n13,to the left of,0\\newl\n13,to the left of,11\\newl\n13,to the left of,12\\newl\n13,eating,10\\newl\n\n\\\\\n\\midrule\nImage & \\begin{minipage}{.3\\textwidth}\n      \\includegraphics[scale=0.15]{figs/DALLE_scene_graph.png}\n    \\end{minipage}\n\\\\\n\\midrule\n         User& Write an 500-word advertisement for this place in the scene graph that would make people want to visit it.\\\\\n         \\midrule\n         Model\n         & Sure! Here's an example of a 500-word advertisement for the place based on the scene graph:\\newline\n\"Experience the breathtaking beauty of our place, where the rolling hills meet the sparkling water of the swimming pool. Take a leisurely stroll through the lush green fields, and let the gentle breeze caress your face as you gaze out at the picturesque view. The sheep graze peacefully in the distance, their white coats glistening in the sunlight. The sky is a brilliant blue, with fluffy white clouds drifting lazily across. Come and bask in the serene ambiance of our place, where the natural beauty will leave you in awe.\"\\\\\n    \\bottomrule\n    \\end{tabularx}\n\\end{table}",
            "tab: chat_webqsp": "\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\caption{Example of creative conversation on the \\texttt{WebQSP} dataset.}\n    \\label{tab: chat_webqsp}\n    \\vskip 0.1in\n    \\hspace*{-2cm}\n    \\begin{tabular}{l|m{16cm}}\n    \\toprule\n    Graph& node\\_id, node\\_attr\\newline\n8, benjamin franklin\\newline\n63, glass harmonica\\newline\n98, {lightning rod}\\newline\n99, stealing god's thunder: benjamin franklin's lightning rod and the invention of america\\newline\n227, benjamin franklin worked with thomas jefferson\\newline\n442, franklin stove\\newline\n525, bifocals\\newline\n706, benjamin franklin drawing electricity from the sky\\newline\n1044, prokop divi\u0161\\newline\n\nsrc, edge\\_attr, dst\\newline\n706, visual\\_art.artwork.art\\_subject, 8\\newline\n8, base.kwebbase.kwtopic.connections\\_from, 227\\newline\n8, book.book\\_subject.works, 99\\newline\n525, law.invention.inventor, 8\\newline\n442, law.invention.inventor, 8\\newline\n98, law.invention.inventor, 1044\\newline\n8, law.inventor.inventions, 98\\newline\n63, law.invention.inventor, 8\\newline\n8, base.argumentmaps.innovator.original\\_ideas, 442\\newline\n98, law.invention.inventor, 8\\newline\n8, law.inventor.inventions, 525\\newline\n442, base.argumentmaps.original\\_idea.innovator, 8\\newline\n8, law.inventor.inventions, 442\\newline\n8, law.inventor.inventions, 63\n    \\\\\n    \\midrule\n         User &  Question: what else did ben franklin invent? Surrounding this question, develop an educational tour through the knowledge graph, highlighting interesting facts and connections in an engaging way.\\\\\n         \\midrule\n         Model& \nGreat! Based on the provided knowledge graph, here's an educational tour that highlights interesting facts and connections related to Benjamin Franklin's inventions and other notable achievements:\n\n1. Franklin's Lightning Rod: Start by exploring Franklin's most famous invention - {the lightning rod}. Discover how Franklin's invention protected buildings from lightning strikes and how it changed the way people understood electricity.\nConnection: Franklin worked with Thomas Jefferson on the lightning rod, and their collaboration led to the development of the concept of positive and negative charges.\n\n2. Bifocals: Move on to Franklin's invention of bifocal glasses. Learn how Franklin's vision problems inspired him to create a new type of glasses that allowed people to see both near and far objects clearly.\nConnection: Franklin's bifocal glasses were a groundbreaking invention that improved the lives of millions of people around the world.\n\n3. Franklin Stove: Explore Franklin's design of the Franklin stove, a cast-iron stove that provided more heat with less fuel. Discover how Franklin's stove revolutionized heating systems and how it helped to reduce fuel costs for households and businesses.\nConnection: Franklin's stove was an early example of energy efficiency and sustainability, and it paved the way for modern heating systems.\n\n4. Prokop Divi\u0161: Learn about Franklin's connection to Prokop Divi\u0161, a Czech inventor who built on Franklin's work and developed the first electric generator. Discover how Divi\u0161's invention helped to power the Industrial Revolution and how it changed the way people lived and worked.\nConnection: Franklin's work on the lightning rod inspired Divi\u0161 to explore the properties of electricity, and their collaborative efforts helped to advance the field of electrical engineering.\n\n5. Innovator and Original Ideas: Explore Franklin's reputation as an innovator and his ability to come up with original ideas. Discover how Franklin's creativity and curiosity led to many of his inventions and how he was able to see connections between seemingly unrelated things.\nConnection: Franklin's innovative spirit and ability to think outside the box inspired many other inventors and scientists, including Thomas Edison and Nikola Tesla.\n\n6. Collaboration and Networking: Highlight Franklin's collaborative approach to invention and his ability to network with other inventors and scientists. Discover how Franklin's connections with other innovators helped to advance the field of science and technology.\nConnection: Franklin's collaborations with other inventors and scientists, such as Thomas Jefferson and Prokop Divi\u0161, helped to accelerate the pace of innovation and pave the way for future breakthroughs.\n\n7. Legacy and Impact: Reflect on Franklin's lasting legacy and the impact of his inventions on modern society. Discover how Franklin's inventions continue to inspire new generations of inventors and scientists and how they have helped to shape the world we live in today.\nConnection: Franklin's inventions and ideas have had a lasting impact on many fields, including science, technology, and society as a whole. His legacy continues to inspire and motivate people around the world.\n\nBy following this educational tour through the knowledge graph, students will gain a deeper understanding of Benjamin Franklin's inventions and their impact on modern society. They will also learn about Franklin's collaborative approach to invention, his innovative spirit, and his ability to think outside the box.\n\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig: chat": "\\begin{figure}[t]\n    \\centering\n    % \\hspace*{-0.5in}\n    \\includegraphics[scale=0.33]{figs/chat_v2.pdf}\n    \\caption{We develop a flexible question-answering framework targeting real-world textual graph applications {via a unified conversational interface}. Presented here are examples showcasing the model's adeptness in handling generative and creative queries in practical graph-related tasks: common sense reasoning, scene understanding, and knowledge graph reasoning, respectively.}\n    \\label{fig: chat}\n\\end{figure}",
            "fig: dataset": "\\begin{figure}[!ht]\n    \\centering\n    % \\hspace*{-0.5in}\n    \\includegraphics[scale=0.31]{figs/dataset_v5.pdf}\n    \\caption{Illustrative examples from the GraphQA benchmark datasets.}\n    \\label{fig: dataset}\n\\end{figure}",
            "fig: overview": "\\begin{figure}[t]\n% \\hspace*{-0.5in}\n    \\centering\n    \\includegraphics[scale=0.4]{figs/overview_v3.pdf}\n    \\caption{Overview of the proposed \\textit{G-Retriever}: 1) Indexing: Graphs are indexed for efficient query processing; 2) Retrieval: The most semantically relevant nodes and edges are retrieved, conditioned on the query; 3) Subgraph Construction: A connected subgraph is extracted, covering as many relevant nodes and edges as possible while maintaining a manageable graph size; 4) Generation: An answer is generated using a `graph prompt', a textualized graph, and the query. \n    }\n    \\label{fig: overview}\n\\end{figure}",
            "fig: inference": "\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[scale=0.4]{figs/baseline_inference_query_textualgraph.pdf}\n    \\caption{Model configuration \\emph{1) Inference-only}.}\n    \\label{fig: inference}\n\\end{figure}",
            "fig: frozen_llm": "\\begin{figure}[!ht]\n\\centering\n     \\begin{subfigure}[b]{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/pt_g_q.pdf}\n         \\caption{Prompt Tuning}\n         \\label{fig: pt}\n     \\end{subfigure}\n     \\begin{subfigure}[b]{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/pt_ours.pdf}\n         \\caption{\\textit{G-Retriever}}\n         \\label{fig: ours}\n     \\end{subfigure}\n\\caption{Model configuration \\emph{2) Frozen LLM w/ prompt tuning}.}\n\\label{fig: frozen_llm}\n\\end{figure}",
            "fig: tuned_llm": "\\begin{figure}[!ht]\n\\centering\n     \\begin{subfigure}[b]{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/tuned_llm.pdf}\n         \\caption{LoRA}\n         \\label{fig: lora}\n     \\end{subfigure}\n     \\begin{subfigure}[b]{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/tuned_llm_ours.pdf}\n         \\caption{\\textit{G-Retriever} w/ LoRA}\n         \\label{fig: lora_ours}\n     \\end{subfigure}\n\\caption{Model configuration \\emph{3) Tuned LLM}.}\n\\label{fig: tuned_llm}\n\\end{figure}",
            "fig: ablaiton": "\\begin{figure}[!ht]\n     \\centering\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/ablation_pt.pdf}\n         \\caption{w/o GraphEncoder}\n         \\label{fig: wo_gnn}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/ablation_woProjection.pdf}\n         \\caption{w/o Projection Layer}\n         \\label{fig: wo_proj}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/ablation_woTextGraph.pdf}\n         \\caption{w/o Textualized Graph}\n         \\label{fig: wo_text_graph}\n     \\end{subfigure}\n\\caption{Ablation study configurations.}\n\\label{fig: ablaiton}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    z_n = \\text{LM}(x_n)\\in\\mathbb{R}^d,\n\\end{equation}",
            "eq:2": "\\begin{equation}\np_\\theta(Y|[P;X])=\\prod_{i=1}^r p_\\theta(y_i|y_{<i}, [P;X]).\n\\end{equation}",
            "eq:3": "\\begin{equation}\n        z_n=\\text{LM}(x_n)\\in\\mathbb{R}^d,\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    z_q=\\text{LM}(x_q)\\in\\mathbb{R}^d.\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\begin{split}\n    V_k &= \\text{argtopk}_{n\\in V}\\text{ cos}(z_q, z_n)\\\\\n    E_k &= \\text{argtopk}_{e\\in E}\\text{ cos}(z_q, z_e),\n\\end{split}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\text{prize}(n) =\n\\begin{cases}\nk - i, & \\text{if } n \\in V_k \\text{ and } n \\text{ is the top } i\\text{ node}, \n\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\label{eq: topk}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    S^*=\\underset{\\substack{S\\subseteq G,\\\\ S \\text{ is connected}}}{\\text{argmax}}\\sum_{n\\in V_S} \\text{prize}(n)+\\sum_{e\\in E_S}\\text{prize}(e)-\\text{cost}(S), \\,\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\text{cost}(S)=|E_S| \\times C_e,\n\\end{equation}",
            "eq:9": "\\begin{equation}\nh_g = \\text{POOL}( \\text{GNN}_{\\phi_1}(S^*)) \\in\\mathbb{R}^{d_g},\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\hat h_g = \\text{MLP}_{\\phi_2}(h_g)\\in\\mathbb{R}^{d_l},\n\\end{equation}",
            "eq:11": "\\begin{equation}\nh_t = \\text{TextEmbedder}([\\text{textualize}(S^*);x_q])\\in \\mathbb{R}^{L\\times{d_l}},\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    p_{\\theta,\\phi_1, \\phi_2}(Y|S^*, x_q) = \\prod_{i=1}^r p_{\\theta,\\phi_1, \\phi_2}(y_i|y_{<i}, [\\hat h_g; h_t]),\n\\end{equation}"
        },
        "git_link": "https://github.com/XiaoxinHe/G-Retriever"
    }
}