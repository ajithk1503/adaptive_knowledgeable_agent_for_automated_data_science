{
    "meta_info": {
        "title": "Disentangled Ontology Embedding for Zero-shot Learning",
        "abstract": "Knowledge Graph (KG) and its variant of ontology have been widely used for\nknowledge representation, and have shown to be quite effective in augmenting\nZero-shot Learning (ZSL). However, existing ZSL methods that utilize KGs all\nneglect the intrinsic complexity of inter-class relationships represented in\nKGs. One typical feature is that a class is often related to other classes in\ndifferent semantic aspects. In this paper, we focus on ontologies for\naugmenting ZSL, and propose to learn disentangled ontology embeddings guided by\nontology properties to capture and utilize more fine-grained class\nrelationships in different aspects. We also contribute a new ZSL framework\nnamed DOZSL, which contains two new ZSL solutions based on generative models\nand graph propagation models, respectively, for effectively utilizing the\ndisentangled ontology embeddings. Extensive evaluations have been conducted on\nfive benchmarks across zero-shot image classification (ZS-IMGC) and zero-shot\nKG completion (ZS-KGC). DOZSL often achieves better performance than the\nstate-of-the-art, and its components have been verified by ablation studies and\ncase studies. Our codes and datasets are available at\nhttps://github.com/zjukg/DOZSL.",
        "author": "Yuxia Geng, Jiaoyan Chen, Wen Zhang, Yajing Xu, Zhuo Chen, Jeff Z. Pan, Yufeng Huang, Feiyu Xiong, Huajun Chen",
        "link": "http://arxiv.org/abs/2206.03739v1",
        "category": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "additionl_info": "Accepted by KDD'22"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n% \\todo{Writing Logic:}\n\n% 1. ZSL and KG-aware ZSL are popular and worth studying;\n\n% 2. Existing KG-aware ZSL methods (paradigms):\n\n% \\begin{itemize}\n%     \\item pre-training KG + mapping or generation\n%     \\item GCN-based propagation\n% \\end{itemize}\n\n% 3. Common issues in encoding graph-structured data and in the context of ZSL: neglect the entanglement of the latent factors behind the class embeddings or class features.\n% \\textcolor{blue}{An example in the ZSL context: for an unseen classes Zebra, Horse is an important seen class in terms of semantic of ``subClassof'', while Tiger is another important seen class in terms of semantic of ``hasStripe''.}\n\n% 4. propose to learn the disentangled representation of classes \\todo{[Disentangled Semantic Encoder]} and propose a framework which can apply the disentangled class representations into different KG-based ZSL paradigms \\todo{[Entangled ZSL Learner]}:\n% \\begin{itemize}\n%     \\item generation: semantic combination\n%     \\item propagation: similarity graph with different semantics \n% \\end{itemize}\n\n% 5. From the view of ZSL nature, the locality and compositionality of semantic features is underexplored.\n\n\n\n\n% With the renaissance of deep learning, various machine learning tasks in, such as computer vision and natural language processing, have made huge breakthroughs.\n% However, deep learning technology often relies on a large number of labeled training samples, and operates on a closed world assumption; i.e., making predictions with targets (e.g., classification labels) that have been seen in the training stage (i.e., \\textit{seen classes}).\n% These limitations constitute a serious bottleneck for building a general model for the real-world applications.\n% For example, to address new classes %labels that rapidly grow, \n% with growing requests, it is impractical to always annotate sufficient training data and re-train existing models, which are often very large, \n% %especially a complex one\n% from scratch.\n% %to address these new classes.\n% To this end, \n\nZero-shot Learning (ZSL), which \n%empowers models the ability \nenables models to predict new classes that have no training samples (i.e., \\textit{unseen classes}), has attracted a lot of research interests in many machine learning tasks, such as image classification \\cite{xian2019survey,frome2013devise}, relation extraction \\cite{li2020logic} and Knowledge Graph (KG) completion \\cite{qin2020zsgan,wang2021structure}.\nTo handle these unseen classes, most existing ZSL  methods adopt a knowledge transfer strategy: transferring samples, sample features or model parameters from the classes that have training samples (i.e., seen classes) to these unseen classes, with the guidance of some auxiliary information which usually depicts the relationships between classes.\n%Such auxiliary information describes the relationships between classes and can be regarded as a prior for transfer.\n%which seen knowledge to transfer and how the transferred knowledge are integrated to support a prediction involving unseen classes.\nFor example, in zero-shot image classification (ZS-IMGC), some studies utilize visual attributes of objects to transfer image features learned from seen classes to unseen classes and build classifiers for the later \\cite{lampert2013attribute,xian2018feature}.\nOther popular auxiliary information includes class's literal name \\cite{frome2013devise}, textual descriptions \\cite{qin2020zsgan,zhu2018noisy} and so on.\n\n\n\n% Recently, more and more studies try to leverage structural auxiliary information \\todo{such as knowledge graphs (KGs) which contain richer class correlations to augment ZSL}.\nRecently, more and more studies leverage KG~\\cite{Pan2016,hogan2021knowledge}, an increasingly popular solution for managing graph structured data, to represent complex auxiliary information for augmenting ZSL  \\cite{chen2021knowledge}.\n%structural auxiliary information such as knowledge graphs (KGs) to augment ZSL .\nKGs that are composed of relational facts can model diverse relationships between classes.\nFor example, Wang et al. \\cite{wang2018zero} incorporate class hierarchies from a lexical KG named WordNet \\cite{miller1995wordnet}; works such as \\cite{roy2020improving,geng2021benchmarking} explore common sense class knowledge from ConceptNet \\cite{speer2017conceptnet}.\nAs a kind of KGs, ontologies, also known as ontological schemas when they act as parts of KGs for meta information, can represent more complex and logical inter-class relationships.\n% with e.g., class expressions and relation constraints.\n%type constraints and compositionality.\nFor example, Chen et al. \\cite{chen2020ontology} use an ontology in OWL\\footnote{Web Ontology Language (\\url{https://www.w3.org/TR/owl-features/})} to express the compositionality of classes; Geng et al. \\cite{geng2021ontozsl} define the domain and range constraints of KG relations using ontological schemas, as shown in Figure~\\ref{fig:intro} (b).\n%In addition to the rich and valuable class correlations expressed by KGs and ontologies, both of them are flexible \nIn addition, ontologies are also able to represent and integrate traditional auxiliary information such as attributes and textual descriptions.\nFor example, as Figure~\\ref{fig:intro} (a) shows, animal visual attributes with binary values can be represented in graph with the attributes transformed into entities.\n%An example in the left of Figure~\\ref{fig:intro} shows the visual attributes are integrated in the ontology of image classes in the form of graph nodes.\n\nTo exploit these KGs, two ZSL paradigms have been widely\n%are \ninvestigated.\n%in the ZSL systems.\nOne is a pipeline including two main steps.\n%architecture developed with semantic embedding techniques.\n%Specifically, \nFirstly, the KG is embedded, based on which the ZSL classes that are already aligned with KG entities are represented using vectors with their relationships kept in the vector space. \nSecondly, \n%or ontologies are first embeded using KG embedding or ontology embedding algorithms to generate semantically meaningful vector representations of classes, and then \na compatibility function between the class vector and the sample input (or features) is learned.\nIt can either be a mapping function, which projects the sample input and the class vector into the same space such that a testing sample can be matched with an arbitrary class via e.g., Euclidean distance \\cite{li2020logic,chen2020ontology,frome2013devise},\nor a generative model, which generates labeled samples or features for unseen classes \\cite{geng2021ontozsl,qin2020zsgan}.\n%Usually, methods of the latter achieve better performance (see more discussions in Section~\\ref{imgc_formulation}).\n% \\todo{One is pre-training the KGs using Graph Embedding techniques to generate semantically meaningful class representations and then learning a compatibility function between the class representations and sample features by mapping \\cite{li2020logic,chen2020ontology,roy2020improving} or generative \\cite{geng2021ontozsl,geng2020generative,qin2020zsgan} networks.}\nThe other paradigm is based on graph information propagation.\nIt often uses Graph Neural Networks (GNNs) to propagate classifier parameters or sample features from nodes of seen classes to nodes of unseen classes \\cite{wang2018zero,kampffmeyer2019rethinking,chen2020zero}.\nMethods of both paradigms, together with KGs, always lead to state-of-the-art performance on many ZSL tasks.\n%State-of-the-art performance is often achieved when these KGs (or ontologies) and these two paradigms are well utilized.\n\nNevertheless, existing methods of both paradigms still have big space for improvement.\n%Although significant progress has been made, existing methods from these two paradigms \n%They all neglect the entanglement of the latent factors behind the learned class representations or class features, in particular when learning from complex KGs or ontologies which have several types of semantic relations.\nIn a real-world KG, an entity is often linked to other entities for knowledge of different aspects.\nFor example, Kobe Bryant is connected to NBA teams for his career knowledge, and connected to his daughters for family knowledge.\n%It is obvious that \n%a class may have multiple distinct aspects and different aspects indicate the relatedness to different classes.\nThis also happens in those KGs (especially ontologies) used for augmenting ZSL.\nAs shown in Figure~\\ref{fig:intro} (a), \n%in the scenario of zero-shot animal recognition, \n\\textit{Zebra} is connected to \\textit{Horse} via \\textit{rdfs:subClassOf} for knowledge on taxonomy,\n%(they have the same parent class \\textit{Equine} via the relation \\textit{rdfs:subClassOf})), \nand connected to  \\textit{Tiger} and \\textit{Panda} via \\textit{imgc:hasAttribute} for knowledge on visual characteristics.\n%(they share the attributes \\textit{Stripe} and \\textit{Black and White} respectively via the relation \\textit{imgc:has Attribute}).\n%In learning the vector representations of classes or transferring features among classes, \n% the embedding of \\textit{Zebra} should have two distinct components, one corresponds to the ``\\textit{rdfs:subClassOf}'' aspect and should be closer to the corresponding component of \\textit{Horse} rather than that of \\textit{Tiger} and \\textit{Panda}, while the ``\\textit{imgc:hasAttribute}'' component should be closer to \\textit{Tiger} and \\textit{Panda} rather than \\textit{Horse}.\nThus the vector representation of \\textit{Zebra} should be closer to \\textit{Horse} than \\textit{Tiger} and \\textit{Panda} considering the aspect of taxonomy, and be closer to  \\textit{Tiger} and \\textit{Panda} than \\textit{Horse} considering the visual characteristics.\nThe existing KG-based ZSL methods all neglect this important KG characteristic on entanglement, which prevents them from \n%However, existing methods can not provide such multi-aspect (disentangled) representations or features, the entangled embeddings they generated are infeasible to \ncapturing more fine-grained inter-class relationships in different aspects, and limits their performance.\n%and interpretability.\n%This limits their performance and interpretability in utilizing KGs, especially ontologies, for ZSL.\n%which limits the performance and interpretability when applying KGs or ontologies to ZSL systems.\n\n\n\n\n\nIn this work, we focused on augmenting ZSL by ontologies, proposed to investigate \\textbf{D}isentangled \\textbf{O}ntology embeddings and developed \n%representation learning for ZSL from the auxiliary information view.\n%Specifically, we focus on the ontology-based auxiliary information and propose \na general \\textbf{ZSL} framework named \\textbf{DOZSL}.\n% in which a \\textbf{D}isentangled \\textbf{O}ntology embedding technique is investigated to generate the disentangled representations of classes in terms of the characteristics of the ontology used for \\textbf{ZSL}, \n DOZSL first learns multiple disentangled vector representations (embeddings) for each class according to its semantics of different aspects defined in an ontology, where a new disentangled embedding method with ontology property-aware neighborhood aggregation and triple scoring is proposed, \n and then adopts an entangled ZSL learner, which builds upon a Generative Adversarial Network (GAN)-based generative model and a Graph Convolutional Network (GCN)-based graph propagation model, respectively, to incorporate these disentangled class representations.\n%Inspired by the observations that the semantic aspects of classes are extremely determined by the relations (a.k.a properties) in ontologies which reflect the correlation relationships among classes at a high level, we propose a property guided disentangled mechanism to figure out the entanglement of latent factors.\n%More specifically, we decouple the class embedding into multiple independent components and leverage typical Knowledge Graph Embedding (KGE) algorithms to enable each component to describe a semantic aspect of class by extracting relevant components of class embeddings according to the given property.\nTo apply the generative model, we concatenate the disentangled representations; while to apply the propagation model, we generate one graph for semantics of one aspect with the disentangled representations.\n%\n%We demonstrate the effectiveness of our framework \nWe evaluate DOZSL with five datasets of zero-shot image classification (ZS-IMGC) and zero-shot KG completion (ZS-KGC).\n%In IMGC, we learn disentangled representations of image classes from an ontology which contains rich domain knowledge of the task.\n%In KGC, we embed the ontological schema of the KG to complete, and get disentangled representations of the seen and unseen relations.\n%we shift the unseen targets to KG relations and learn disentangled representations for them from their ontologies, which are also known as meta KGs to represent high-level knowledge of KG relations.\nSee Figure~\\ref{fig:intro} for segments of the ontology for one IMGC dataset and the ontological schema for a KG to complete.\n%The snapshots of ontologies used in these two tasks are shown in Figure~\\ref{fig:intro}, we will provide a detailed formulation for readers in the next section. \n%\\todo{[Some summary for evaluation results.]}\n%\nIn summary, our contributions are the following:\n\\begin{itemize}\n\\item To the best of our knowledge, this is among the first to investigate disentangled semantic embeddings for ZSL.\n%from the auxiliary information view in ZSL systems.\n\\item A property guided disentangled embedding method is developed for ontologies used in ZSL, and a general ZSL framework named DOZSL, which is able to support both generative models and propagation models, is proposed.\n%a general ontology-based ZSL framework that aims to learn disentangled ontology embeddings for ZSL.\n%To achieve this goal, we not only develop a property guided disentanglement mechanism to learn disentangled class representations from ontologies, but also employ a generative model and a propagation model to perform ZSL predictions. \n\\item The work includes extensive evaluation,\nwhere DOZSL often outperforms the baselines including the state-of-the-art methods on five datasets of two tasks, and the effectiveness of DOZSL's components is verified by ablation studies. \n%extensively validate the effectiveness of our disentangled ontology embeddings and ontology-based framework.\n\\end{itemize}\n\n\n"
            },
            "section 2": {
                "name": "Preliminaries and Related Work",
                "content": "\n%We first begin by formally introducing the zero-shot learning in two tasks we investigated, and their ontologies we used.\n%We first introduce ZSL and two ZSL tasks that are used for evaluation, and then introduce the ontologies.\n\n\n",
                "subsection 2.1": {
                    "name": "Zero-shot Learning",
                    "content": "\\label{zsl_formulation}\n\n",
                    "subsubsection 2.1.1": {
                        "name": "Zero-shot Image Classification (ZS-IMGC)",
                        "content": "\\label{imgc_formulation}\nZSL has been thoroughly studied in Computer Vision for image classification with new classes whose images are not seen during training.\nFormally, let $\\mathcal{D}_{tr} = \\{(x, y) | x \\in \\mathcal{X}_s, y \\in \\mathcal{Y}_s\\}$ be the training set, where $x$ is the CNN features of a training image and $y$ is its class in $\\mathcal{Y}_s$ which is a set  of seen classes, and $\\mathcal{D}_{te} = \\{(x, y) | x \\in \\mathcal{X}_u, y \\in \\mathcal{Y}_u\\}$ be the testing set,\nwhere $\\mathcal{Y}_u$, the set of unseen classes, has no overlap with $\\mathcal{Y}_{s}$.\nGiven $\\mathcal{D}_{tr}$ and some auxiliary information $\\mathcal{A}$ for describing the relationships between seen and unseen classes, ZS-IMGC aims to learn a classifier for each unseen class.\n%Specially, two settings are often evaluated at the testing stage\nThere are often two evaluation settings: standard ZSL which recognizes the testing samples in $\\mathcal{X}_u$ by only searching in $\\mathcal{Y}_u$ and generalized ZSL which recognizes the testing samples in $\\mathcal{X}_s \\cup \\mathcal{X}_u$ by searching in $\\mathcal{Y}_s \\cup \\mathcal{Y}_u$.\n\nWidely used auxiliary information includes class attributes \\cite{lampert2013attribute,xian2018feature,xu2020attribute}, textual information \\cite{frome2013devise,zhu2018noisy} and KGs \\cite{wang2018zero,roy2020improving,geng2021ontozsl,chen2021zero}.\nTo support ZSL, they are often embedded to generate one semantic vector for each class, such as binary/numerical attribute vectors, pre-trained word embeddings, learnable sentence embeddings, and KG embeddings.\nNext, a compatibility function between the class vectors and the vector representations of samples is often learned to conduct knowledge transfer.\nMapping function is a typical practice, which maps the image features to the space of class vector \\cite{frome2013devise,lampert2013attribute,chen2020ontology} or vice versa \\cite{zhang2017learning} or to a shared common space \\cite{fu2015zero}.\nHowever, all of these mappings are trained by seen data, and thus have a strong bias towards seen classes during prediction, especially in generalized ZSL.\n%where the label space includes both seen and unseen classes.\nRecently, thanks to generative models such as GANs \\cite{goodfellow2014generative}, several methods  \\cite{xian2018feature,zhu2018noisy} have been proposed to synthesize samples (or features) for unseen classes conditioned on their class vectors.\nThis converts the ZSL problem to a standard supervised learning problem with the aforementioned bias issue alleviated.\n\nBesides, to explicitly exploit the structural inter-class relationships that exist in a KG, some ZSL works explore a graph information propagation strategy.\nIn these works, classes are often aligned with KG entities, and a powerful GNN such as GCN \\cite{kipf2016semi} is then trained to output a classifier\n(i.e., a class-specific parameter vector) \nfor each class, \n%by regressing the weight vectors learned from e.g., CNN, \nthrough which the classifiers of unseen classes are approximated by aggregating the classifiers of seen classes.\nOne typical work is by Wang et al. \\cite{wang2018zero}, the subsequent works adopt similar ideas but vary in optimizing the graph propagation \\cite{kampffmeyer2019rethinking,geng2020explainable}.\nEspecially, some of them consider the multiple types of relations in the KGs by developing multi-relational GCN \\cite{chen2020zero}, or spliting the multi-relation KGs into multiple single-relation graphs and applying several parameter-shared GCNs to propagate features \\cite{wang2021zero}.\n\n\n% In this paper, our proposed framework is implemented with not only GAN-based generative models but also GCN-based propagation models.\n% In particular, based on the disentangled ontology embeddings, the ontology graph can be disentangled into several semantic graphs, each of which represents a latent and disentangled relationship among classes.\n% The propagation models are then separately applied to propagate features among classes in each disentangled latent space.  \n\n\n\n"
                    },
                    "subsubsection 2.1.2": {
                        "name": "Zero-shot KG Completion (ZS-KGC)",
                        "content": "\\label{kgc_formulation}\n\n%Different from the clustered instances in IMGC, \nIn this task, a KG composed of relational facts is to be completed. \nIt is denoted as $\\mathcal{G} =\\{ \\mathcal{E}, \\mathcal{R}, \\mathcal{T}\\}$, where $\\mathcal{E}$ is a set of entities, $\\mathcal{R}$ is a set of relations, and $\\mathcal{T} = \\{(h, r, t)| h, t \\in \\mathcal{E}; r \\in \\mathcal{R}\\}$ is a set of relational facts in form of RDF triple.\nThe completion is to predict a missing but plausible triple with two of $h,r,t$ given. \n%In this study, we evaluate our method by predicting $t$ with $h$ and $r$ given. \n%The task of KG completion (KGC) is to complete the triple facts in KGs when one of $h,r,t$ is missing.\nTypical KGC methods first embed entities and relations into vector spaces (i.e., $x_h, x_r$ and $x_t$) and conduct vector computations to discover missing triples.\nThe embeddings are trained by existing triples and assume all testing entities and relations are available at training time.\nZS-KGC is thus proposed to predict for unseen entities or relations that are newly added during testing and have no associated training triples.\n\nSome ZS-KGC approaches devote to dealing with unseen entities by utilizing the auxiliary connections with seen entities \\cite{wang2019logic}, introducing their textual descriptions \\cite{wang2021structure},\n% and attributes \\cite{hao2020inductive}, \nor learning entity-independent graph representations so that naturally generalizing to unseen entities \\cite{teru2019inductive,MorsE}.\nIn contrast, the works for unseen relations are relatively underexplored. Both Qin et al. \\cite{qin2020zsgan} and Geng et al. \\cite{geng2021ontozsl}  leverage GANs to synthesize valid embeddings for unseen relations conditioned on their auxiliary information which are textual descriptions and ontological schemas, respectively.\n%The widely adopted auxiliary information for relations in $R_s \\cup R_u$ includes their textual descriptions \\cite{qin2020zsgan} and the relation semantics from the ontological schemas of KGs \\cite{geng2021ontozsl}.\n\nIn this study, we target at unseen relations.\nTwo disjoint relation sets: the seen relation set $\\mathcal{R}_s$ and the unseen relation set $\\mathcal{R}_u$ are set.\nThe triple set $\\mathcal{T}_s = \\{(h, r_s, t)| h, t \\in \\mathcal{E}; r_s \\in \\mathcal{R}_s\\}$ is collected for training, and $\\mathcal{T}_u = \\{(h, r_u, t)| h, t \\in \\mathcal{E}; r_u \\in \\mathcal{R}_u\\}$ is collected to evaluate the completion of the triples of unseen relations.\nA closed set of entities is considered following previous works, i.e., each entity that appears in the testing set has appeared during training.\n%the training set.\n\n% In our work, we leverage disentangled ontology embedding techniques to learn disentangled representations for seen and unseen relations from their ontological schemas.\n% In addition to experimenting with generative models in our framework, we also develop the GNN-based propagation model for more comprehensive evaluation.\n\n"
                    }
                },
                "subsection 2.2": {
                    "name": "Ontology",
                    "content": "\\label{ontology_formulation}\nOntology is famous for representing and exchanging general or domain knowledge, often with hierarchical concepts as the backbone and properties for\n%the general concepts (i.e., types of things) that exist in a domain and the properties that \ndescribing semantic relationships \\cite{horrocks2008ontologies}.\n%They can be quite complicated with different kinds of semantics especially when defined by some complex languages such as OWL.\nIn this study, we use a simple form of ontology, namely in RDF Schema (RDFS)\\footnote{\\url{https://www.w3.org/TR/rdf-schema/}}, while those more complicated OWL ontologies can be transformed into RDFS ones following some criteria.\n% by extracting relevant knowledge.\nAn ontology can be used as a schema of a KG, defining entity types, relations and so on.\n%\nAccordingly, we represent an ontology as $\\mathcal{O}=\\{\\mathcal{C}, \\mathcal{P}, \\mathcal{T}_o\\}$, where $\\mathcal{C}$ is the set of concepts (a.k.a. types), $\\mathcal{P}$ is the set of properties, and $\\mathcal{T}_o=\\mathcal{C} \\times \\mathcal{P} \\times \\mathcal{C}$ is the set of triples.\nTo serve as auxiliary information for ZSL, an ontology models the relevant domain knowledge of a given ZSL task.\nFor example, in IMGC, concepts are used to represent image classes and image attributes; in KGC, ontology triples can be used to define    \ndomains (i.e., head entity types) and ranges (i.e., tail entity types) of KG relations.\nNote we sometimes also call concept as concept node in introducing ontology embedding.\n%\n\nOntology properties can be either built-in properties of RDFS, such as \\textit{rdfs:subClassOf} and \\textit{rdfs:subPropertyOf}, or user defined for a specific task, \n%While the properties introduced include not only domain-specific properties \nsuch as \\textit{imgc:hasAtrtibute}. % defined by\n%but also some properties derived from well-known ontology languages in the Semantic Web community such as \n%RDFS.\n%(e.g., \\textit{rdfs:subClassOf}, \\textit{rdfs:subPropertyOf}).\n% and OWL\\footnote{Web Ontology Language, \\url{https://www.w3.org/TR/owl2- overview/}}.\n%An ontology can express valuable domain knowledge about a task.\nFigure~\\ref{fig:intro} shows two ontology segments for ZS-IMGC and ZS-KGC.\nThe triple (\\textit{Zebra, imgc:hasAttribute, Stripe}) means that an animal class \\textit{Zebra} has an attribute \\textit{Stripe} in decoration,\nwhile the triple (\\textit{radiostation\\_in\\_city, rdfs:subPropertyOf, has\\_office\\_in\\_city}) means that the KG relation \\textit{radiostation\\_in\\_city} is a subrelation of  \\textit{has\\_office\\_in\\_city}.  \nIt is worth mentioning that properties are also often defined with hierarchies, as the concepts. One general property is often defined for semantics of one aspect, and then more sub-properties are defined for more fine-grained semantics. Thus we can often easily find out relevant properties for different semantic aspects of an ontology by simple visualization of the property hierarchies.\n\n%It is noted that the triples of an ontology and a KG both compose a multi-relational graph.\n%whose nodes represent entities or concepts and edges are labeled by relations or properties.\n%The difference is that an ontology often represent highly abstract domain knowledge.\n%In some cases, an ontology is used to describe the meta information (schema) of a KG that is composed of relational facts (data).\n%For example, a concept can summarize a set of similar entities and serves as their type, and a property can model the meta relations between these entity types.\n%The ontologies thus are also known as meta KGs when they act as the schemas of KGs composed of facts (data).\nIn our ZS-KGC case study, we adopt ontologies developed in \\cite{geng2021ontozsl} as the auxiliary information for completing relational facts of their corresponding KGs in the zero-shot setting, where KG relations are modeled as ontology concepts and their meta-relationships are modeled by ontology properties.\nOur DOZSL framework contains a disentangled ontology encoder to learn disentangled representations for all concept nodes in an ontology, through which the fined-grained inter-concept relationships can be figured out and well utilized in downstream zero-shot learning and prediction steps.\n\n\n\n\n\n"
                },
                "subsection 2.3": {
                    "name": "Disentangled Representation Learning",
                    "content": "\nThe goal of disentangled representation learning is to learn embedding including various separate components behind the data.\n% , such as texts \\cite{john2018disentangled}, images \\cite{chen2016infogan}, and user behaviors \\cite{ma2019learning}.\nIn the field of the graph, DisenGCN \\cite{ma2019disentangled} is the first work tending to learn disentangled node representations, which uses a neighborhood routing mechanism to identify the latent factor that may have caused the link from a given node to one of its neighbors.\nHowever, it mainly focuses on homogeneous graphs with a single relation type.\n% further improve it by making the disentangled components more independent \\cite{liu2020independence} or disentangling at the graph level \\cite{yang2020factor}.\n% Despite the success of disentangled representation learning for graphs, the above models mainly focus on homogeneous graphs with a single relation type.\nTo process graphs with more diverse relation types,\n% Recently, there are also several efforts that apply disentangled representation learning to heterogeneous KGs.\n% Zhang et al. \\cite{zhang2021knowledge} apply DisenGCN to KGs, but still ignore the types of relations.\n%the diverse relation edges in the KGs.\nDisenE \\cite{kou2020disene} and DisenKGAT \\cite{wu2021disenkgat}, which leverage an attention mechanism and a dynamic assignment mechanism, respectively, disentangle the entity embeddings according to the relations in a KG.\nDifferent from these works, we propose to learn disentangled ontology embeddings in terms of the characteristics of the ontology used for ZSL and develop a novel disentanglement mechanism which is guided by the properties in an ontology.\n% It decouples the embedding of an ontology concept into multiple distinct components, \n%and leverages KGE techniques to \n% enabling each component to explicitly describe a semantic aspect specified by a property.\n\n\nThere are also some works that explore the disentangled representation learning in ZSL \\cite{xu2020attribute,li2021generalized}.\nHowever, they all focus on disentangling the representations of samples such as the image features learned by CNNs, none of them have  taken into account the impact of learning disentangled auxiliary information representations, especially when richer but complex auxiliary information are introduced.\nIn contrast, our work made the first attempt.\n% In this paper, we propose to learn disentangled ontology embeddings for ZSL to investigate fine-grained class relationships in different aspects.\n\n\n\n% \\begin{figure*}\n% \\centering\n% \\includegraphics[height=6cm]{figures/framework.pdf}\n% \\caption{An example illustration of DOZSL framework at training stage with $K=3$. Different color means xxx.}\n% \\label{fig:framework}\n% \\end{figure*}\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Methodology",
                "content": "\n\n%To learn disentangled ontology embeddings and apply them to perform downstream ZSL tasks, \nAs shown in Figure~\\ref{fig:framework}, \\textbf{DOZSL} includes two core modules: \\textbf{Disentangled Ontology Encoder} learning disentangled ontology embeddings, and\n\\textbf{Entangled ZSL Learner} utilizing the embeddings for generation-based and propagation-based ZSL methods.\n% presents the overall architecture. Next, we will introduce them in detail.\n\n\n\n\n",
                "subsection 3.1": {
                    "name": "Disentangled Ontology Encoder",
                    "content": "\\label{sec:doe}\n\n% This module learns disentangled ontology embeddings, where the embedding of each concept node is disentangled into multiple distinct components and each component encodes the semantics of one specific aspect.\n% For a concept $c$, its embedding is denoted as $\\bm{c} = [\\bm{c}^1, \\bm{c}^2, ..., \\bm{c}^K]$, where $K$ is the number of components, $\\bm{c}^k \\in \\mathbb{R}^{\\frac{d}{K}}$ represents $k$-th component and $d$ is the embedding dimension.\n%is used to represent the $k$-th semantic aspect of concept $c$.\n\n% This module learns disentagled ontology embeddings. \nIn DOZSL, the embedding of each concept node $c$ is disentangled into multiple distinct components as $\\bm{c} = [\\bm{c}^1, \\bm{c}^2, ..., \\bm{c}^K]$, where $K$ is the component numbers,  $\\bm{c}^k \\in \\mathbb{R}^{\\frac{d}{K}}$ represents the $k$-th component encoding semantics of one aspect of $c$ and $d$ is the embedding size.\n\n% As a concept is characterized by its neighboring concepts, to learn its disentangled embedding, we first aggregate the information from the neighborhood.\n% As each component represents a specific semantic aspect, not all neighbors but only a subset of neighbors actually carry the valuable information of the aspect.\nTo learn disentangled embedding for each concept, we first aggregate information from its graph neighborhoods that characterize it. \nIn the aggregation of each component for a concept, only a subset of neighbors actually carries valuable information since each component represents a specific semantic aspect. \n% To identify the subset of aspect-specific neighbors, a neighborhood routing strategy  utilizing the attention mechanism to learn different importance of the neighbors under different aspects is often used \\cite{ma2019disentangled,wu2021disenkgat,zhang2021knowledge}.\n%is widely studied.\nTo identify the aspect-specific subset, we follow the attention-based neighborhood routing strategy in previous works \\cite{ma2019disentangled,wu2021disenkgat}.\nAlso, considering the various relation types in the ontologies, we propose a \\textbf{property-aware attention mechanism}.\n% Specifically, for each neighboring concept node of $c_i$, its importance degree (i.e., attention value) at the $k$-th aspect is computed by the similarity between its $k$-th component embedding and the $k$-th component embedding of $c_i$ in the subspace of their connection property, following the assumption that one neighboring node has more similar embedding if it contributes more in the aggregation.\nSpecifically, for the $k$-th aspect, the attention value of one neighbor $c_j$ of concept $c_i$ is computed by the similarity of the $k$-th component embeddings of $c_j$ and $c_i$ in the subspace of their connection property $p$ following the assumption that when a neighbor contributes more to $c_i$ in the aggregation, their property-aware representations are more similar, formally:\n%a property specific subspace, \n%based on the observation that when a neighbor contributes more to $c_i$ at the $k$-th aspect in the aggregation, their $k$-th component embeddings in terms of the connection property are more similar.\n% Let $c_j$ be a neighboring concept node of $c_i$, connected by the property $p$, its attention value at the $k$-th aspect is computed as follows:\n% \\begin{equation}\n\\begin{align}\\label{eq:attention}\n\\alpha_{(c_i,p,c_j)}^{k,l} &= softmax((h_{i,k,p}^l)^T \\cdot h_{j,k,p}^l)\n    \\nonumber \\\\\n&= \\frac{exp((h_{i,k,p}^l)^T \\cdot h_{j,k,p}^l)}{\\sum_{(c_{j'}, p')\\in \\mathcal{N}(i)} exp((h_{i,k,p'}^l)^T \\cdot h_{j',k,p'}^l)} \\\\\nh_{i,k,p}^l &= h_{i,k}^l \\circ W_p, \\;\\;\\; h_{j,k,p}^l = h_{j,k}^l \\circ W_p \n\\end{align}  \n% \\end{equation}\nwhere $l\\in \\{0, 1, ..., L-1\\}$ with $L$ as the number of aggregation layers. \n$h_{i,k,p}^l$ is\n% is  calculated as $h_{i,k}^l \\circ W_p$ representing\nthe $k$-th component embedding of $c_i$ w.r.t. property $p$ in the $l$-th aggregation layer, $\\circ$ denotes the Hadamard product, and $W_p$ is a learnable projection matrix of $p$ for projecting $c_i$'s $k$-th component embedding $h_{i,k}^l$ into the property specific subspace.\n$\\mathcal{N}(i) = \\{ (c_{j'}, p') | (c_i, p', c_{j'}) \\in \\mathcal{T}_o \\} \\cap \\{ (c_i, p_s)\\}$ is the set of pairs of neighboring concept nodes and properties of $c_i$, which also includes $c_i$ itself with a special self-connection property $p_s$. $\\mathcal{T}_o$ is the ontology triple set. \n% Namely, $(c_{j'}, p') \\in \\mathcal{N}(i)$ if ($c_i$, $p'$, $c_{j'}$) holds.\n%is connected by $c_i$ by $p'$.\nA dot-product similarity is adopted here.\n\n%With the attention score, \nWith attention values, we separately aggregate the neighborhood information for representing each component and also update the property embedding after each aggregation as:\n%Similarly, we also adopt a property-aware aggregation strategy which aggregate the message from relevant neighbors with their connected properties.\n% Specifically, the $k$-th component embedding of concept $c_i$ outputted at the $l$-th aggregation layer is as follows:\n\\begin{equation}\\label{RGAT}\nh_{i,k}^{l+1} = \\sigma(\\sum_{(c_j,p)\\in \\mathcal{N}(i)} \\alpha_{(c_i,p,c_j)}^{k,l} \\phi (h_{j,k}^l, h_p^l, W_p)), \\;\\;\nh_p^{l+1} =  h_p^{l}\\cdot \\Theta_p^l\n\\end{equation}\nwhere $h_p^l$ is the embedding of property $p$ in the $l$-th layer.\n% We also update $h_p^l$ after each aggregation as $h_p^{l+1} = \\Theta_p^l \\cdot h_p^{l}$, where \n$\\Theta_p^l$ is the layer-specific linear transformation matrix for $p$.\n$\\phi$ is a combination operator for fusing the information of neighboring concept nodes and property edges.\nHere, we refer to CompGCN \\cite{vashishth2020composition} to implement it via e.g. vector multiplication.\n$h_{i,k}^0$ is randomly initialized, and\n$h_{i,k}^L$ is outputted at last layer which has encoded the neighborhood information specific to aspect $k$. We make $\\bm{c}_i^k  = h_{i,k}^L$ for simplicity.\n% The output of the final layer $h_{i,k}^L$ is the $k$-th component embedding which has encoded the neighborhood information and is also denoted as $\\bm{c}_i^k$ for simplicity. \n%In this way, we make full use of the graph context of concepts under each semantic aspect.\n\n\nTo further improve the disentanglement, \nwe propose to refine the semantics of each disentangled component embedding of concepts according to their associated properties.\nIt is inspired by the characteristic of knowledge in ontologies, i.e., ontology properties are often represented with hierarchies, thus one general property can always be selected for representing one distinct semantic aspect of a concept; for example, the properties \\textit{imgc:hasAttribute} and \\textit{rdfs:subClassOf} in Figure~\\ref{fig:intro} represent the semantics on animal visual characteristics and taxonomy, respectively.\n% which is\n% different from previous methods performing an attentive scoring mechanism \\cite{wu2021disenkgat,kou2020disene} and\n% is featured by ontologies.\n%of the graph node embeddings, \n% existing disentangled methods for multi-relational graphs often conduct a component-level triple prediction to refine the disentangled representations learned above.\n% Specifically, for each triple, they first calculate the triple score in each component space, and then leverage an attention mechanism to fuse the scores by different components, where the attention values are computed based on the relatedness between the components and the relation in the triple.\n% In this way, the importance of different components is adaptively adjusted according to the relation, and the semantics of different components are further figured out.\n% While \n% In learning disentangled embeddings for ontologies for ZSL, since \n% In ontologies,\n% knowledge, including the properties, are often represented with hierarchies, thus one general property can always be selected for representing one distinct semantic aspect of a concept; for example, the properties \\textit{imgc:hasAttribute} and \\textit{rdfs:subClassOf} in Figure~\\ref{fig:intro} represent the semantics on animal visual characteristics and taxonomy, respectively.\n% Therefore, different from previous methods which perform an attentive scoring mechanism, we prefer to refine each disentangled component embedding of concepts according to the associated property.\n%and enable each disentangled component embedding corresponds to the semantics of a specific property. \n\nTo achieve this goal, we \\textit{(i)} select a set of properties for aspects of the semantics of the concepts to encode (e.g., \\textit{imgc:hasAttributes} for visual characteristics in the ontology for IMGC) and set the number of disentangled components to be the number of selected properties, \nand \\textit{(ii)} design a \\textbf{property guided triple scoring mechanism} extracting property-specific components to constitute a valid ontology triple.\nSpecifically, \n% given $K$ properties in the ontologies, \nfor an ontology triple ($c_i, p_k, c_j$), we extract the $k$-th components of $\\bm{c}_i$ and $\\bm{c}_j$ with respective to property $p_k$, and \n% leverage KG embedding methods which carry various scoring functions to calculate the triple score with the extracted components.\nleverage the score function on KG embedding methods to calculate the triple score with the extracted components.\n% A higher score indicates a stronger relatedness between these extracted components and the relation $p_k$.\nIn this way, we accurately endow each component embedding with a specific semantic meaning w.r.t properties.\nHere, the score function of TransE \\cite{bordes2013translating} is adopted to compute the triple score as: \n% which assumes the relation in each triple as a translation between two entities to calculate the triple score as follows:\n\\begin{equation}\\label{eq:score}\nq_{(c_i, p_k, c_j)} = f(-||\\bm{c}_i^k + \\bm{p}_k -  \\bm{c}_j^k||)\n\\end{equation}\nwhere $\\bm{c}_i^k$ and $\\bm{c}_j^k$ denote the extracted component embeddings of concepts $c_i$ and $c_j$ respectively, and $\\bm{p}_k$ represents the embedding of property $p_k$.\n$f$ is the logistic sigmoid function.\n%To train the embeddings of concept components and relations, \nA higher score indicates a stronger relatedness between $\\bm{c}_i^k$, $\\bm{p}_k$ and $\\bm{c}_j^k$.\nFinally, we use the standard cross entropy with label smoothing to train the whole disentangled ontology encoder as:\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{L} = -\\frac{1}{B}\\frac{1}{|\\mathcal{C}|} \\sum_{(c_i, p_k) \\in batch} \\sum_{n} (t_n \\cdot log(q_{(c_i, p_k, c_j^n)}) + \\\\ (1-t_n) \\cdot log(1- q_{(c_i,p_k,c_j^n)}))\n\\end{aligned}\n\\end{equation}\nwhere $B$ is the batch size, $\\mathcal{C}$ is the concept node set of the ontology, $t_n$ is the label of the given query $(c_i, p_k)$, whose value is $1$ when the triple $(c_i, p_k, c_j^n)$ holds and $0$ otherwise.\n% \\todo{{$f$ is a function that represents ...}}\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Entangled ZSL Learner",
                    "content": "\nWith the disentangled ontology embeddings, we next show how to utilize them for ZSL.\nSpecifically, we develop two kinds of methods.\n% To utilize the disentangled ontology embeddings for ZSL, we develop two kinds of methods based on the existing two representative ZSL paradigms.\nIn consideration of the effectiveness of GANs in learning the compatibility between class vectors and their samples, the first method is generation-based leveraging GANs to generate discriminative samples for classes (each of which corresponds to an ontology concept).\nThe other is propagation-based propagating features among  classes based on the disentangled graphs generated from the original ontology.\n\n%  and the other is propagation-based propagating features among classes based on the disentangled graphs generated from the original ontology.\n%in each disentangled semantic space with a disentangled semantic graph.\n%Next, we will introduce them in details.\n\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Generation-based",
                        "content": "\\label{sec:gb}\n%To incorporate with GANs, \nWe first get the embedding of each class by concatenating all $K$ component embeddings of its corresponding ontology concept (i.e., $\\bm{c}_i = [\\bm{c}_i^1, \\bm{c}_i^2, ..., \\bm{c}_i^K]$) ,\n%which represent the concepts with multiple disentangled semantic aspects, \nand then adopt a typical scheme of GAN for feature generation.\nSpecifically, the GAN consists of three networks: a generator $G$ synthesizing sample features for a class from random noises conditioned on its embedding; a feature extractor $E$ providing the real sample features; and a discriminator $D$ distinguishing the generated features from the real ones.\nWe generate sample features instead of raw samples for both higher accuracy and efficiency, as in many works \\cite{xian2018feature,geng2021ontozsl,qin2020zsgan}.\n%In addition, some  regularizers are used to encourage inter-class discrimination among the generated features.\n\nFormally,  for a class $c_i$, the generator $G$ takes as input \nits embedding and \n%concatenated concept embedding $\\bm{c}_i = [\\bm{c}_i^1, \\bm{c}_i^2, ..., \\bm{c}_i^K]$ and \na random noise vector $z$ sampled from Normal distribution, %$\\mathcal{N}(0,1)$, \nand generates its features: $\\hat{x} = G(z, \\bm{c}_i)$.\nThe loss of $G$ is defined as:\n\\begin{equation}\n\t\\mathcal{L}_G = - \\mathbb{E}[D(\\hat{x})] + \\lambda_1 \\mathcal{L}_{cls} (\\hat{x}) + \\lambda_2 \\mathcal{L}_{R}\n\\end{equation}\nwhere the first term is the Wasserstein loss, the second term % \\cite{arjovsky2017wasserstein}\n% eliminating the mode collapse problem during generation,\n% $\\mathcal{L}_{cls} (\\hat{x})$ \nis a supervised classification loss for classifying the synthesized features,\nand \n% $\\mathcal{L}_{R}$ \nthe third is\n% %proposed in \\cite{zhu2018noisy}, which \nfor regularizing the mean of generated features of each class to be the mean of its real features.\nThe latter two  both encourage the generated features to have more inter-class discrimination.\n$\\lambda_1$ and $\\lambda_2$ are the corresponding weight coefficients.\n\nThe discriminator $D$ takes as input the synthesized features $\\hat{x}$ from $G$ and the real features $x$ from $E$. Its loss is defined as:\n\\begin{equation}\n% \\begin{aligned}\n\t\\mathcal{L}_D = \\mathbb{E}[D(x, \\bm{c}_i)] - \\mathbb{E}[D(\\hat{x})] - \\beta \\mathbb{E}[(|| \\bigtriangledown_{\\tilde{x}} D(\\tilde{x}) ||_p -1)^2]\n% \t\\end{aligned}\n\\end{equation}\nwhere the first two terms approximate the Wasserstein distance of the distributions of $x$ and $\\hat{x}$.\n%real features and synthesized features, \nThe last term is the gradient penalty to enforce the gradient of $D$ to have unit norm \n%(i.e., Lipschitz constraint proposed by \\cite{gulrajani2017improved}), \nin which $\\tilde{x} = \\varepsilon x + (1-\\varepsilon) \\hat{x}$ with $\\varepsilon \\sim U(0,1)$. \n$\\beta$ is the weight coefficient.\n\nIn view of the different data form in different ZSL tasks, we adopt different feature extractor $E$.\n% In evaluation, for different data in different ZSL tasks, we adopt different feature extraction networks.\n%to extract the real sample features.\nFor ZS-IMGC, we employ ResNet101 \\cite{he2016resnet} to extract the features of images following previous works \\cite{xian2019survey}; and for ZS-KGC, we follow \\cite{qin2020zsgan,geng2021ontozsl} to learn cluster-structured features for KG relations.\n%where a relation whose associated entity pairs are grouped as its instances, \n% where a relation's embedding is represented by the embeddings of its associated entities, and all the relation embeddings are trained in a matching-based way via a margin ranking loss.\n%More details please refer to \\cite{qin2020zsgan,geng2021ontozsl}.\nIn general, $E$ is trained in advance with only samples of seen classes, \n% and the samples of unseen classes are unseen,\nand is fixed during adversarial training.\nAlso, our framework is compatible to different feature extractors.\n%using the samples of seen concepts.\n%During the training of GAN, they are fixed and only the generator and the discriminator are optimized.\n\n% Once the GAN is trained with seen data, \nWith well trained GAN,\nwe use generator $G$ to synthesize features and train task-specific prediction models for unseen classes.\n%with random noises and their corresponding concept embeddings.\n%We can then train classifiers \n%Similarly, we also utilize these generated unseen data in a task-specific way.\nIn ZS-IMGC, we train a softmax classifier for each unseen class to classify its testing images;\nin ZS-KGC, a testing triple is completed by calculating the similarity between the generated embedding of the relation $r$ and the joint embedding of the entity pair $(h,t)$. \n% A higher score means the triple is more likely to hold.\n%where the candidate resulting in the highest similarity score will be output as the tail entity.\n\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Propagation-based",
                        "content": "\n%Each disentangled component of a concept embedding summarizes the semantic features of the concept under a specific aspect, based on which \nWith disentangled concept embeddings, \n% we can analyze, extract and utilize more fine-grained relatedness between concepts. \nmore fine-grained relatedness between concepts could be utilized.\nTherefore, as shown in Figure~\\ref{fig:framework}, we generate one semantic graph for each component, where nodes correspond to the classes (relations in KGC) in the dataset and  edges are generated by calculating the cosine similarity between the component embeddings of two class nodes, and conduct graph propagation on it to transfer features between classes under each semantic aspect.\n%under each component subspace.\n% In this way, the knowledge graph is disentangled into several semantic graphs, each of which represent a latent and disentangled correlation among classes.\n%As shown in Figure~\\ref{fig:framework}, for $K$ disentangled components, we obtain $K$ semantic graphs.\nThe initialized node features are  the class's component embedding.\nFormally, we represent the $k$-th semantic graph as $\\mathbf{G}_k(A_k, S_k)$, where $S_k \\in \\mathbb{R}^{m \\times \\frac{d}{K}}$ is the input feature matrix of graph nodes, and $A_k \\in \\mathbb{R}^{m \\times m}$ is the graph adjacency matrix indicating the connections among $m$ classes defined as below, $\\tau$ denotes the similarity threshold.\n% More specifically, $A_k$ is defined as:\n\\begin{equation}\nA_k (i,j) = \n\\left\\{\n\\begin{array}{cc}\n    1 & \\text{if} \\quad sim(\\bm{c}_i^k, \\bm{c}_j^k) > \\tau\n    \\\\\n    0 & \\text{otherwise}\n\\end{array}\n\\right.\n\\end{equation}\n\nSince $\\mathbf{G}_k$ is a graph with one single relation, we use GCN for feature propagation. Each graph convolutional layer performs as: \n% in each semantic graph. \n% For the $k$-th semantic graph $\\mathbf{G}_k(A_k, S_k)$, the graph convolution propagation in the $l$-th layer is performed as follows:\n\\begin{equation}\n    H_k^{l+1} = \\sigma (\\widehat{A}_k H_k^{l} \\Phi_k^{l})\n\\end{equation}\nwhere \n% $H_k^l$ and $H_k^{l+1}$ represent the input and output, respectively. \n$\\widehat{A}_k$ is the normalized adjacent matrix, and $\\Phi_k^{l}$ is a layer-specific weight matrix shared among all semantic graphs.\n$H_k^{0}=S_k$.\n% is the input at the first layer.\n% Notably, we make the weight parameters shared among all the semantic graphs.\n\n%In previous propagation-based ZSL methods, the GCN is trained to output a ZSL classifier for each graph node.\n%Next, we will show how to obtain a classifier for each concept node in our method and employ the learned classifiers to perform zero-shot predictions.\n%Specifically, \n%after $T$ graph convolutions, \nFor each semantic graph, the GCN outputs a set of  node embeddings $Z_k \\in \\mathbb{R}^{m \\times F}$, through which we can obtain a set of classifiers $\\widetilde{\\mathcal{W}}$ for all $m$ classes as: $\\widetilde{\\mathcal{W}} = \\varphi (Z_1, Z_2, ..., Z_K)$, where $\\varphi$ is a fusion function.\nIn our experiments, we implement $\\varphi$ by averaging: $\\widetilde{\\mathcal{W}}=\\frac{1}{K}\\sum_{k} Z_k$, or linear transformation:\n$\\widetilde{\\mathcal{W}} = W_1([Z_1; Z_2; ...; Z_K])$ where \n%$[\\cdot;\\cdot]$ denotes the concatenate operation, and \n$W_1 \\in \\mathbb{R}^{KF\\times F}$ is a trainable transformation matrix.\nThen, following \\cite{wang2018zero,kampffmeyer2019rethinking,wang2021zero}, we compute the Mean Square Error between the fused classifiers and the ground-truth classifiers as loss function:\n\\begin{equation}\n    \\mathcal{L}_{GCN} = \\frac{1}{|\\widetilde{\\mathcal{W}_s}|} \\sum_{\\widetilde{w} \\in \\widetilde{\\mathcal{W}_s}} (\\widetilde{w} - gt(\\widetilde{w}))^2\n\\end{equation}\nwhere $\\widetilde{\\mathcal{W}_s} \\subset \\widetilde{\\mathcal{W}}$ is the set of classifiers of the seen classes,\n%$m$ is the number of seen classes, \n%and $\\left\\{w_1, w_2, ..., w_m \\right\\}$ are \n$gt(\\widetilde{w})$ denotes the corresponding ground-truth.\nDifferent from the traditional classifier which is a network trained using labeled samples, the classifier here is actually a real-valued vector that represents the class-specific features, and is obtained by averaging the features of all the training samples of one class in our paper.\n% In previous works \\cite{wang2018zero,kampffmeyer2019rethinking,wang2021zero}, the classifier is a real-valued parameter vector usually obtained by extracting the last-layer weight matrix of the pre-trained feature extraction networks (e.g., ResNet101), which represents the category specific feature parameters.\n% Alternatively, we opt to obtain\n% To obtain the classifier of each seen class, we average all the features of its training samples. The averaged representation can be viewed as a prototype of this class in the sample space.\nThe sample features are also extracted via the  feature extractor $E$ mentioned in Section \\ref{sec:gb}. \n%learned from the training samples of seen classes.\nBy using these ground-truth seen classifiers to supervise the training of GCNs, classifiers of the unseen classes can be learned by aggregation.\n%aggregating the classifier features of seen classes.\n% In prediction, for an input sample, \nDuring prediction, for an input testing sample,\nwe first extract its features using the same feature extractors, and then perform classification or completion by calculating the similarity between the learned classifiers and the extracted features.\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Evaluation",
                "content": "\n%In this section, we evaluate our DOZSL by the two different tasks of zero-shot image classification and zero-shot KG completion.\n\n",
                "subsection 4.1": {
                    "name": "Experiment Settings",
                    "content": "\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Datasets and Ontologies",
                        "content": "\nFor ZS-IMGC, we use a popular benchmark named Animals with Attributes (AwA) \\cite{xian2019survey} and two benchmarks ImNet-A and ImNet-O extracted from ImageNet by Geng et al. \\cite{geng2021ontozsl}.\nAwA  is for coarse-grained animal image classification wth $50$ classes and $37,322$ images.\nImNet-A is for more fine-grained animal image classification and ImNet-O is for fine-grained general object classification. \n%are two fine-grained datasets.\nThe classes are split into a seen set and an unseen set, following \\cite{xian2019survey}.\n%\nFor ZS-KGC, we use two KGs provided in \\cite{qin2020zsgan} for completion, i.e., NELL-ZS and Wiki-ZS extracted from NELL and Wikidata\\footnote{NELL (\\url{http://rtw.ml.cmu.edu/rtw/}) and Wikidata (\\url{https://www.wikidata.org/})}, respectively.\nIn each KG, the relations are split into a training set with seen relations, a validation set and a testing set with unseen relations, following \\cite{qin2020zsgan}.\n%Each KG has three disjoint relation sets following the splits proposed in \\cite{qin2020zsgan}: a training set with seen relations, a validation set and a testing set with unseen relations.\nAccordingly, their associated triples compose a training set, a validation set and a testing set.\nIt is ensured that all entities are seen.\n% that appear in the testing and validation sets have appeared in the training set.\n\n\nEach dataset has an ontology as its auxiliary information.\n%, developed in \\cite{geng2021ontozsl,geng2021benchmarking}.\n%which contains rich  domain knowledge of image classes and KG relations.\nWe use the ontologies developed in \\cite{geng2021ontozsl} and take the latest version released in \\cite{geng2021benchmarking}.\n%and extract five subsets to serve as the auxiliary information for the five benchmarks.\nFor ZS-IMGC, the ontologies contain class hierarchies (taxonomies), class visual attributes and attribute hierarchies.\nIn our property guided disentangled embedding, we select two general properties: \\textit{rdfs:subClassOf} for semantic aspect on taxonomy, and \\textit{imgc:hasAttribute} for semantic aspect on visual characteristics.\n%Then, we reuse mete-property \\textit{rdfs:subClassOf} to represent the hierarchical relationships among different image classes and among different image attributes, and reformulate the annotated attributes of classes using a high-level property \\textit{imgc:hasAttribute}.\nFor ZS-KGC, the ontologies contain type constraints of the head and tail entities of relations, represented by properties \\textit{rdfs:domain} and \\textit{rdfs:range}, relation hierarchies represented by property \\textit{rdfs:subProperty}, and type hierarchies represented by property \\textit{rdfs:subClassOf}.\nThese four properties are selected as general properties used in ontology encoder.\nSee Table~\\ref{tab:datasets} for detailed statistics.\n\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Variants of DOZSL and Baselines",
                        "content": "\n\nIn disentangled ontology encoder, we compare two settings for component embeddings that are fed to score triple (Eq. \\eqref{eq:score}): aggregating neighborhood information (Eq. \\eqref{eq:attention} and \\eqref{RGAT}), and randomly initializing component embeddings without neighborhood aggregation.\nThis leads to two DOZSL variants.\nMeanwhile, they can be combined with two downstream ZSL methods: generation-based with GAN and propagation-based with GCN.\nThus we have four DOZSL variants and denote them as ``DOZSL(X+Y)'', where X can be AGG (neighborhood aggregation) and RD (random initialization), Y can be GAN and GCN.\n\n%After learning disentangled ontology embeddings, we have two options to make downstream ZSL predictions, i.e., generation based and propagation based.\n%Meanwhile, for a more detailed analysis of our disentanglement strategy, we also develop a version of method which ignores the neighborhood information but directly performs the property-curated triple scoring using randomly initialized component embeddings for disentanglement.\n%Therefore, we have four variants of our DOZSL framework.\n%For distinction, we use the format ``DOZSL(X)+Y'' to denote the specific variants, where ``Y'' denotes the ZSL Learner in DOZSL, it can be ``DOZSL(X)+GEN'' for generation based one and ``DOZSL(X)+PRO'' for propagation based one, and ``X'' denotes the disentanglement methods, i.e., ``DOZSL(AGG)+Y'' for aggregating the neighborhood information before triple scoring, while ``DOZSL(RD)+Y'' for not.\n\nThe baselines include those generation-based and propagation-based ZSL methods that often achieve state-of-the-art performance on many ZSL datasets.\n%One side is for different ZSL methods, and we mainly compare with the generation-based and propagation-based ZSL methods which incorporate with ontology-based auxiliary information.\n\\textbf{OntoZSL} \\cite{geng2021ontozsl} is a generation-based method that uses GANs to synthesize samples, where  we take TransE as its ontology encoder for a fair comparison.\n\\textbf{DGP} \\cite{kampffmeyer2019rethinking} is a propagation-based method using a two-layers GCN which only supports single-relation graphs.\n%As for the propagation-based methods, we choose three kinds of baselines which can capture the structural inter-class relationships in the ontologies.\n%The first is building upon GCN but ignoring the multiple types of relationships as done in \\cite{gao2019know,nayak2020zero}, in this paper, we adopt an advanced GCN-based propagation model \\textbf{DGP} proposed in \\cite{kampffmeyer2019rethinking} as the baseline.\nTo deal with the multi-relation ontology graph, we take the method proposed in \\cite{wang2021zero} as a baseline.\nMeanwhile, two relation-aware GNNs, RGCN \\cite{schlichtkrull2018rgcn} and CompGCN \\cite{vashishth2020composition}, are also used to implement another two propagation-based ZSL baselines.\n%The second is to convert the multi-relational graphs into multiple single-relational graphs as in \\cite{wang2021zero,chen2020zero,lu2020multi},\n%in this paper, we choose the method proposed by Wang et al. \\cite{wang2021zero} for a comparison.\n%For the third, we adapt existing relation-aware GCN models such as RGCN \\cite{schlichtkrull2018rgcn} and CompGCN \\cite{vashishth2020composition} in the ZSL systems, denoted as \\textbf{RGCN-ZSL} and \\textbf{CompGCN-ZSL} respectively,  which have been rarely investigated.\n%\nWe also consider different disentangled and non-disentangled semantic embedding methods for more baselines.\n%The baselines from the other side is to compare with different ontology encoding methods, where the disentangled ones and the non-disentangled ones are both considered.\nFor non-disentangled embedding, we choose classical \\textbf{TransE}, and \\textbf{RGAT} which also performs attentive relation-aware graph  aggregation.\n%which lead to \\textbf{DOZSL(RD)} and \\textbf{DOZSL(AGG)}, respectively.\n%Specially, \\textbf{RGAT} is a method which also performs attentive relation-aware graph  aggregation.\nFor disentangled embedding, we choose two state-of-the-art methods \\textbf{DisenE} \\cite{kou2020disene} and \\textbf{DisenKGAT} \\cite{wu2021disenkgat}.\nThese embedding methods can also be combined with GAN-based and GCN-based ZSL learners as in DOZSL, leading to baselines such as ``DisenKGAT+GAN''.\n%to encode the ontologies.\n%For all the compared ontology encoders, we respectively incorporate them with the two ZSL learners to constitute the baselines.\n%Similarly, they are represented in the format of ``Z+Y'' where ``Z'' means the different ontology encoders and ``Y'' is the GAN based or GCN based ZSL methods.\nNote ``TransE+GAN'' is equivalent to OntoZSL.\n%Notably, the combination of the TransE-based ontology encoder and the GAN-based ZSL methods are OntoZSL implemented in our paper, we mark this point in the result tables.\n\n%In summary, the baselines we compared can be categorized into two groups.\n%One is generation based which includes OntoZSL, two generation-based variants of DOZSL and the baselines combining different ontology encoders with GANs; the other is propagation based which includes DGP, the method proposed in \\cite{wang2021zero}, RGCN-ZSL, CompGCN-ZSL, two propagation-based variants of DOZSL and the baselines combining different ontology encoders with our GCN-based propagation models.\n\n%Obviously, our proposed framework is pluggable which can integrate different ontology encoders and different ZSL methods. It is more flexible compared with other ZSL methods, especially those propagation-based ones.\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Evaluation Metrics",
                        "content": "\nFor ZS-IMGC, we report macro accuracy following \\cite{xian2019survey}, where accuracy of each class is first calculated with its testing images, and the accuraccies of all testing classes are then averaged.\n% This is mainly due to the imbalanced testing images across different classes.\n%For zero-shot IMGC task, we evaluate DOZSL and baselines by classification accuracy.\n%As the imbalanced samples across classes, we follow previous works \\cite{xian2019survey} to report the class-averaged (macro) accuracy.\n%Specifically, the per-class accuracy -- the ratio of correct predictions over all the testing samples of this class is first calculated, and then the accuracies of all targeted classes are averaged as the final metric.\nFor standard ZSL testing, we compute accuracy on all unseen classes, denoted as $acc$; while for generalized ZSL testing, we first calculate accuracy for all the seen classes and all the unseen classes separately, denoted as $acc_s$ and $acc_u$, respectively, and then report a harmonic mean $H = (2 \\times acc_s \\times acc_u)/(acc_s + acc_u)$.% as a more comprehensive metric.\n\n%Following previous works, we formulate the KGC problem in our work as completing the tail entity $t$ given the head entity $h$ and the relation $r$ in a testing triple.\n\n\n\n% \\vspace{-0.25cm}\n\nOur ZS-KGC task is to predict the tail entity $t$ given a head entity $h$ and an unseen relation $r_u$.\n%\nThus for the input of a testing triple ($h, r_u$), we rank a set of candidate entities\n% \\footnote{Specifically, as there may be more than one ground truth tail entity for a query tuple, we use a filtering setting: the candidate entities are all the KG entities, with other correct tail entities filtered out and only the current testing one kept.}\naccording to their predicted scores of being the tail entity, and see the rank of the ground truth tail entity --- the smaller rank, the better performance. \nAs in most KGC works, we report Mean Reciprocal Ranking ($MRR$) and $hit@k$ (i.e., the ratio of testing samples whose ground truths are ranked in the top-$k$ position).\n$k$ is set to $1, 5, 10$.\n%\n%Specifically, for each query tuple ($h, r$), we assume there are a ground-truth tail entity $t$ and a candidate entity set $C_{(h,r)}$, the model needs to assign the highest ranking score to $t$ against other candidate entities.\n%Accordingly, in the zero-shot testing, the model needs to complete the triples of $r_u$ by ranking $t$ with the candidate tail entities $t' \\in C_{(h,r_u)}$.\n%Therefore, we adopt two metrics commonly used in the KGC literature \\cite{wang2017knowledge}: mean reciprocal ranking ($MRR$) and $hit@k$ to measure the results of all testing triples.\n%$MRR$ represents the average of the reciprocal predicted ranks of all correct tail entities; while $hit@k$ denotes the percentage of testing samples whose correct tail entities are ranked in the top-$k$ positions.\nDifferent from ZS-IMGC where predicting the class label of an image tends to be confused by other classes, the prediction for a seen relation in ZS-KGC is relatively independent of the prediction for an unseen relation. Thus the generalized ZSL testing setting in ZS-KGC, which is a simple addition of normal KGC, is not considered in our paper.\n%Notably, we do not set generalized ZSL testing setting here, since the candidate space only involves entities during testing and the prediction with unseen relations is independent of the prediction with seen relations, while the latter is a traditional KGC task which is out of the scope of this paper.\n\n\\vspace{-0.2cm}\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Main Results",
                    "content": "\n\n",
                    "subsubsection 4.2.1": {
                        "name": "ZS-IMGC",
                        "content": "\\label{imgc_results_report}\n\nThe results are reported based on these settings. \nFor ontology encoder, we set the component embedding size and the property embedding size to $100$.\n%in the ZS-IMGC task for a fair comparison.\n%The number of components $K$ is set according to the property number in the ontologies.\n$K$ is set to $2$ (corresponding to \\textit{rdfs:subClassOf} and \\textit{imgc:hasAttribute}) for all DOZSL(RD) variants, but to $5$ for all DOZSL(AGG) variants since two reverse properties and a self-connection property are added during aggregation.\n%Specially, we set $K$ to be the number of the properties for all DOZSL(RD) variants, i.e., $K=2$, while set $K$ to be twice the number of the relations plus one for all DOZSL(AGG) variants (i.e., $K=5$) since the reverse property edges and self-connection edges are added when conducting the graph aggregation.\nThe initial learning rate is set to $0.001$. The number of the aggregation layer for DOZSL(AGG) variants is set to $1$.\n\n\nFor ZSL learner, we employ ResNet101 to extract $2,048$-dimensional image features.\nIt is ensured that unseen classes of all the three datasets have never appeared in training ResNet101.\nRegarding GAN, the generator and discriminator both consist of two fully connected layers with $4,096$ hidden units;\ntheir learning rates are both set to $0.0001$;\n%the generator \n%outputs $2,048$-dimensional image features with \n%has $4,096$ hidden units; \n%the discriminator also has $4,096$ hidden units and outputs a $2$-dimensional vector that indicates whether the input features are real or not; \nthe dimension of noise vector $z$ is set to $100$; $\\lambda_1$, $\\lambda_2$ and $\\beta$ are set to $0.01$, $5$ and $10$, respectively.\nRegarding GCN, the size of the classifier vector is $2,048$; $2$ convolutional layers with a hidden dimension of $2,048$ are used; the learning rate is set to $0.001$.\nAs for the optimum similarity threshold for creating semantic graphs, we provide a detailed evaluation in Section~\\ref{ablation_studies}.\n\nFor baselines DisenE and DisenKGAT, we test different $K$ values and report the better ones in the main body, and attach the complete results in Appendix~\\ref{disenkge_sensitivity_study}.\nMore details please see our released codes. \n%Note we have released our implementations of DOZSL and some baselines, as well as their settings.  \n\n%\\todo{For more reliable comparisons, we also re-implemented some of the baselines and released the codes.\n%More parameter settings of our framework and the baselines please refer to our released codes.\n%Specially, \n%The whole results with different values are attached in Appendix~\\ref{disenkge_sensitivity_study}.}\n\n\n\\noindent\\textbf{Overall Results.}\nThe results are shown in the left side of Table~\\ref{tab:overall_results}.\nWe can see DOZSL always achieves the best performance on AwA and ImNet-O, no matter what downstream ZSL learners  are applied (+GAN or +GCN).\nOn ImNet-A, DOZSL is still the best in most cases. \nAlthough DOZSL does not outperform RGCN-ZSL on the metric of $H$, the result is still comparable.\n% It can also be validated by the superior performance of RGAT and DisenKGAT, two methods which also encode the neighborhood information, on most metrics against TransE and DisenE, respectively.\n\n%\\noindent \\textbf{DOZSL(RD) VS TransE, DOZSL(AGG) VS RGAT.}\n\\noindent \\textbf{Results on Ontology Encoders.}\nFirst, we find the methods with our disentangled embeddings often outperform those methods with non-disentangled embeddings.\nIn particular, DOZSL(AGG) outperforms RGAT and TransE on all the datasets no matter what ZSL learners are used.\n%\nSecond, we find DOZSL(AGG) often performs better than DOZSL(RD) on most metrics.\nThis indicates the superiority of capturing neighborhood information in learning disentangled ontology embeddings.\n%\nThird, our property guided component-wise triple score is quite effective in learning disentangled embeddings.\nThis can be verified by the fact that DOZSL(AGG) outperforms DisenE and DisenKGAT on all the three datasets. \nEven without aggregation, DOZSL(RD) is still quite good in most cases.\n\n\\noindent \\textbf{Results on ZSL Learners.}\nUsing either GAN or GCN can make our framework perform better than the baselines.\n%No matter GAN-based or GCN-based ZSL learners both facilitate our framework achieving better performance over baselines.\nEspecially, when the input ontology embedding is fixed, we can often select one of them for better performance. \n%applying the same semantic embedddings.\nFor example, on AwA, \\textit{i)} DOZSL(RD+GAN) has worse performance than DisenE+GAN and DisenE+GCN, but DOZSL(RD+GCN) outperforms DisenE+GCN and DisenE+GAN; \\textit{ii)} using GCN with DOZSL(AGG) can achieve good performance, while using GAN with DOZSL(AGG) achieves even higher performance on both metrics $H$ and $acc$.\nMoreover, our DOZSL variants with GCN perform better than previous propagation-based ZSL methods in most situations, illustrating that our method can more effectively capture the structural class relationships in ontologies.  \n\n% Although DOZSL(RD+GAN) performs worse with TransE+GAN on AwA and ImNet-A, it performs much better than TransE+GCN on the metric of $H$, illustrating that its better generalization capability in the generalized ZSL setting, i.e., it can distinguish samples well when the label space has both seen and unseen classes.\n%Also, the inconsistent performance of DOZSL(RD) across different datasets and different ZSL methods also motivates us to develop more robust disentangled methods when directly disentangling the embeddings of concepts according to the triple scoring module.\n%This further verifies the effectiveness of our property guided triple score in learning disentangled ontology embeddings.\n\n\n\n\n%\\noindent \\textbf{DOZSL(RD) VS DisenE\n%, DOZSL(AGG) VS DisenKGAT.\n%}\n%In comparison with DisenKGAT which also aggregates the neighborhood information at component level, our ``DOZSL(AGG+GAN)'' and ``DOZSL(AGG+GCN)'' both achieve superior performance on three datasets.\n\n\n\n\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "ZS-KGC",
                        "content": "\nFor ontology encoder, we re-use the settings in ZS-IMGC. The dimension of component embedding and property embedding is set to $200$.\n%We adopt the same parameter settings of ontology encoder as ZS-IMGC to learn $200$-dimensional component embeddings and property embeddings for ZS-KGC.\n$K$ is $4$ for DOZSL(RD) and is $9$ for DOZSL(AGG) considering the reverse properties and the self-connection property.\n%The component size $K$ is also set according to the number of meta-properties in the ontologies, i.e., $K=4$ for the DOZSL(RD) variants and $K=9$ for the DOZSL(AGG) variants.\nThe feature extractors are pre-trained to extract $200$-dimensional and $100$-dimensional relation features for NELL-ZS and Wiki-ZS, respectively, following the settings in \\cite{qin2020zsgan,geng2021ontozsl}, with TransE-based embeddings as the input.\n%Following \\cite{qin2020zsgan,geng2021ontozsl}, we pre-train the feature extractor to extract $200$-dimensional and  $100$-dimensional real relations embeddings for NELL-ZS and Wiki-ZS, respectively.\n%The parameters for pre-training are consistent with those used in \\cite{qin2020zsgan,geng2021ontozsl}.\n%Specially, we choose TransE as the input pre-trained KG embeddings of the feature extractor in this paper.\n%\nFor ZSL learner, we also employ the same GAN and GCN architectures as in ZS-IMGC, but use some different settings.\n%Also, we adopt the same Entangled ZSL Learner network architecture as ZS-IMGC but employ different parameter settings.\nRegarding the GAN for NELL-ZS, the generator has $250$ hidden units,\n%and outputs $200$-dimensional relation embeddings, \nwhile the discriminator has $200$ hidden units.\n%and outputs a $2$-dimensional vector to indicate the input embedding is real or not.\nRegarding the GAN for Wiki-ZS, the corresponding unit numbers are $200$ and $100$.\n%While for Wiki-ZS, the hidden units of the generator is $200$ and that of the discriminator is $100$.\nFor both datasets, the noise vector size is set to $15$; $\\lambda_1$, $\\lambda_2$ are set to $1$ and $3$, respectively.\nRegarding GCN, the classifier vector size is $200$ for NELL-ZS and $100$ for Wiki-ZS.\n%As for the GCN-based method, we also let the model to output $200$-dimensional classifiers with $200$ hidden units for NELL-ZS, and output $100$-dimensional classifiers with $100$ hidden units for Wiki-ZS.\nAs in ZS-IMGC, the selection of similarity thresholds for creating semantic graphs is evaluated in Section~\\ref{ablation_studies}; different $K$ values are tested for DisenE and DisenKGAT with the optimum performance reported in Table \\ref{tab:overall_results} and the complete results attached in Appendix \\ref{disenkge_sensitivity_study}.\n\n\\noindent \\textbf{Overall Results.}\nThe results are presented in the right of Table~\\ref{tab:overall_results}.\nOn NELL-ZS, our method achieves the best on $hit@10$ and $hit@5$, DOZSL(RD+GAN) and DOZSL(RD+GCN) are both very competitive to the baseline RGCN-ZSL and better than other baselines on $hit@1$ and $MRR$.\nOn Wiki-ZS, two baselines RGAT+GAN and RGCN-ZSL perform the best, but our method DOZSL(RD+GAN) is very close to them, especially on $MRR$ ($19.1$ vs $19.0$) and $hit@1$ ($14.2$ vs $14.0$).\n\n%In the GAN-based method group, DOZSL achieves the best performance on NELL-ZS on all evaluation metrics, and on Wiki-ZS, it achieves the best on $hit@5$ and achieves the second best on other metrics.\n%In the GCN-based method group, DOZSL achieves the best performance on NELL-ZS on $hit@10$ and $hit@5$ and achieves the second best on $hit@1$ and $MRR$. \n%While on Wiki-ZS, DOZSL performs the best on the metric of $hit@1$ and achieves comparable results on other metrics compared with other baselines except for RGCN-ZSL.\n\n\\noindent \\textbf{Results on Ontology Encoders.}\nIt can be observed that the performance gap between DOZSL(RD) and DOZSL(AGG) is narrowed, and DOZSL(RD) even performs better on some metrics, which can be attributed to the following reasons.\n(1) The neighborhood information of concepts in ZS-IMGC task is richer than that in ZS-KGC task, especially for the concepts in NELL-ZS's ontology.\nStatistically, the average number of surrounding neighbors for NELL-ZS is around $3.4$, while the number for ImNet-A is around $5.9$.\n(2) The properties in the ontologies of ZS-IMGC task such as \\textit{imgc:hasAttribute} are 1-N; while most properties in the ontologies of ZS-KGC task are 1-1.\nThe embedding methods that ignore aggregating the neighborhood are often not good at handling these 1-N properties.\nBesides, in comparison with the disentangled and non-disentangled baselines, our methods always have superior performances, i.e., on most metrics, DOZSL(AGG) performs better than RGAT and DisenKGAT, and DOZSL(RD) outperforms DisenE and TransE by a large margin.\n\n\n\n\\noindent \\textbf{Results on ZSL Learners.}\nGiven the same ontology embeddings, we find the performance varies from one ZSL learner to another.\n%have different performance.\n%on both NELL-ZS and Wiki-ZS.\nThe GCN-based learner usually performs better than the GAN-based one on NELL-ZS, while the GAN-based learner reversely performs better on Wiki-ZS.\nThis motivates us to conduct an in-depth analysis about the interaction between the datasets and the ZSL methods, so that making a more suitable selection for better performance.\nMoreover, RGCN-ZSL also shows the promising ability of relation-aware GNNs on ZS-KGC task.\n\n\n\n\n\n\n\n%\\subsubsection{Summary}\n%To sum up, our proposed ZSL framework achieves promising performance on most metrics across different ZSL tasks.\n%In particular, the proposed disentangled ontology encoder performs better than existing disentangled and non-disentangled semantic encoding methods in most cases.\n%Although some variants such as DOZSL(RD) whose performance on some datasets is slightly away from our expectations, this motivates us to develop more robust methods to make full use of the semantics in the ontologies.\n%Moreover, it can be observed that some GCN-based baselines such as RGCN-ZSL also has a good performance on some datasets, this also motivates us to take into consideration injecting the idea of disentanglement into the previous GCN-based propagation models in the future.\n\n\n\n\\vspace{-0.2cm}\n"
                    }
                },
                "subsection 4.3": {
                    "name": "Ablation Studies",
                    "content": "\\label{ablation_studies}\n\n%In this section, we investigate the impact of some critical modules of the framework to analyze it deeply and comprehensively.\nWe conduct extensive ablation studies to analyze the impact of different factors in DOZSL, including the property guided triple scoring, the neighborhood aggregation, the similarity threshold for constructing semantic graphs and the classifier fusion.\n%Specifically, we conduct ablation studies on the property-curated triple scoring mechanism and the neighborhood aggregation in the disentangled ontology encoder, and the choice of similarity threshold and fusion function in the propagation-based ZSL learners.\n\n\n\n\\noindent\\textbf{Property Guided Triple Scoring.}\nWe replace the property guided triple scoring in DOZSL(RD) and DOZSL(AGG) by the widely-adopted attentive triple scoring and keep the same setting of $K$.\n%to investigate its impact on the disentanglement,\nThis leads to two new variants, denoted as $\\text{DOZSL(RD}_{\\text{atten}})$ and $\\text{DOZSL(AGG}_{\\text{atten}})$, respectively.\n%By doing this, our methods degrade into the methods that are similar to DisenE and DisenKGAT, respectively, which attentively extract the relevant components of concepts to compute the triple score according to the property given in a triple.\nThese variants' results with GAN are reported in Table~\\ref{tab:ablation_ontology_encoder_gen}, the results with GCN are attached in Appendix~\\ref{ablation_ontology_encoder}.\nWe can find that $\\text{DOZSL(RD}_{\\text{atten}})$ and $\\text{DOZSL(AGG}_{\\text{atten}})$ always obtain dramatically worse results than $\\text{DOZSL(RD})$ and $\\text{DOZSL(AGG})$, respectively, on all the datasets of the two tasks, with the only exception of $\\text{DOZSL(RD}_{\\text{atten}}$+GAN) on AwA.\nThese results illustrate the effectiveness of our proposed property guided triple scoring.\nThe except may be due to the imbalanced associated triples of different properties in AwA's ontology: \\textit{imgc:hasAttribute} has $1,562$ associated triples, which can well train its corresponding component, while \\textit{rdfs:subClassOf} has only $197$ associated triples, making its corresponding component under fitted.\nThe two components are concatenated and fed to GANs together, thus they may influence each other.\n%The imbalanced triple number may cause the corresponding \\textit{imgc:hasAttribute} components are well trained while the \\textit{rdfs:subClassOf} components are poorly trained, and the concatenated concept embeddings which will be inputted into GANs may further the quality of generated samples.\nIn contrast, the GCN-based method, which performs independent feature propagation in isolated semantic graphs, suffers less from the imbalance issue.\n%imbalanced training of the two kinds of semantics to some extent.\n%Still, the superior performance of our methods on the whole illustrates the effectiveness of our proposed triple scoring mechanism.\n%By accurately extracting the property specific components, the semantic relatedness between the concept components and the properties are further enhanced.\n\n\n\\noindent\\textbf{Neighborhood Aggregation.}\n%To make full use of the neighborhood information of concepts in the graph, we conduct an attentive property-aware aggregation mechanism under each component subspace.\n%However, considering that our disentanglement is to endow each disentangled component with one kind of semantic corresponding to a specific property in the ontologies, \nIn DOZSL, we aggregate information from all the neighboring concepts in the ontology, with an attention mechanism for combination.\nHere, we want to test a more straightforward solution, i.e., aggregating information from a neighborhood subset which only includes concepts that are connected by the property corresponding to the embedding component.\n%property-specific neighbors for each distinct component?\nThis leads to new variants denoted by $\\text{DOZSL(AGG}_{\\text{sub}})$.\n%Towards this end, we develop a variant of DOZSL(AGG) by alternatively aggregating the information of the neighbors which are connected via a specific property for the component which corresponds to the semantic of this property, i.e., collecting the message of property-specific subgraph for each component, denoted as $\\text{DOZSL(AGG}_{\\text{sub}})$.\nThe results with GAN are shown in Table~\\ref{tab:ablation_ontology_encoder_gen}, the results with GCN are in Appendix~\\ref{ablation_ontology_encoder}.\nIn comparison with DOZSL (AGG), $\\text{DOZSL(AGG}_{\\text{sub}})$ performs worse on most metrics across two tasks, except for  $\\text{DOZSL}$ $\\text{(AGG}_{\\text{sub}}$+GAN) on ImNet-O w.r.t. $H$ and $\\text{DOZSL(AGG}_{\\text{sub}}$+GCN) on NELL-ZS.\nThe overall worse results of $\\text{DOZSL(AGG}_{\\text{sub}})$ indicate that learning a component embedding should (attentively) aggregate all the neighboring concepts rather than select a part of them according to the specific properties.\n%embeddings not merely rely on a specific subgrap, and richer neighbors provide more comprehensive information for the centre concept.\nThe exceptions may be due to the simple neighborhoods in %the ontologies of \nNELL-ZS and ImNet-O and/or the independent propagation in each semantic graph.\n\n\n\\noindent\\textbf{Similarity Threshold and Classifier Fusion.}\n%To robustly build the semantic graphs with more suitable similarity threshold and robustly fuse the generated classifiers with more suitable fusion function in the GCN-based method, in this section, \nWe compare different similarity thresholds ranging from $0.85$ to $0.999$ for constructing semantic graphs, and compare different classifier fusion functions, under different ontology encoding methods.\n%and the choice of the fusion function under different disentangled ontology embeddings.\nThe results are reported in Figure~\\ref{ablation_study_sim_fusion} in Appendix~\\ref{ablation_sim_fusion}, from which we can find that the optimum similarity threshold varies when different ontology encoding methods are used, and the two fusion functions --- Average and Linear Transformation both positively contribute to the learning of the classifier.\nPlease see Appendix~\\ref{ablation_sim_fusion} for more details.\n\n\n\n\n\n\n\\vspace{-0.2cm}\n"
                },
                "subsection 4.4": {
                    "name": "Case Study",
                    "content": "\nWe use examples from NELL-ZS to analyze disentanglement of concept embeddings we learned.\n%into multiple distinct components according to their semantic aspects.\n%We present the examples from NELL-ZS to show an intuitive impression.\nIn the left of Figure~\\ref{case_study}, we visualize the component embeddings of KG relations learned from NELL-ZS's ontology by DOZSL(RD), where different colors indicate different components.\nWe can find that \\textit{i)} the embeddings are clustered into different groups under each component's subspace, and \\textit{ii)} the component embeddings of each relation are divided into different clusters across different components.\nThese observations illustrate that \\textit{i)} our method indeed captures the semantically similarity among relation concepts under each semantic aspect and \\textit{ii)} different relatedness is presented across different aspects.\n%As such, the fine-grained relationships among different KG relations are presented and utilized.\n\nAlso, to further verify that different components represent different semantic aspects, for each relation, we randomly select two neighbors from the cluster of each component.\nThe right of Figure~\\ref{case_study} presents two examples.\nFor relation \\textit{league\\_players}, its two neighbors from the first component are \\textit{league\\_teams} and \\textit{league\\_coaches}, the head entity types of these three relations are identical, i.e., \\textit{sports\\_league}; while its two neighbors from the second component are \\textit{athlete\\_beat\\_athlete} and \\textit{sports\\_team\\_position\\_athlete}, their tail entity types are \\textit{athlete}.\nAccording to these two examples, we can find that these four components respectively reflect four semantic aspects of the relations, i.e., \\textit{rdfs:domain}, \\textit{rdfs:range}, \\textit{rdfs:subPropertyOf} and \\textit{rdfs:subClassOf}, and we can also conclude that the semantic of one component is a fixed across different relations. \n\n% \\todo{The consistent conclusion can be summarized from from the neighbors of relation \\textit{person\\_graduated\\_ from\\_university}, indicating that the semantic aspect of each component of different concepts are shared.} \n\n\n\n\n\n% \\vspace{-0.2cm}\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion and Discussion",
                "content": "\nIn this study, we focused on ontology augmented ZSL and proposed a novel property guided disentangled ontology embedding method.\nWith the new disentangled embeddings, different semantic aspects of ZSL classes are figured out and more fine-grained inter-class relationships are extracted, through which the ontology can be better utilized.\nTo integrate these disentangled embeddings, we also developed a general ZSL framework DOZSL, including a GAN-based generative model and a GCN-based propagation model.\nExtensive evaluations with ablation studies and case studies on five datasets of ZS-IMGC and ZS-KGC show that DOZSL often outperforms the state-of-the-art baselines and its components are quite effective.\n\nDOZSL is compatible to both ZSL learners developed by us, and they together lead to higher robustness and better performance.\n%our DOZSL framework lead to more robust results, and DOZSL is compatible to both. \n%highly flexible to integrate different networks.\nMeanwhile, the performance of DOZSL is less competitive to the state-of-the-art on one of the five datasets.\nThis motivates us to take an in-depth analysis of this dataset and its ontology, and to develop more robust disentangled embedding methods and ZSL learners in the future.\n%on the characteristics of the ontologies in different cases and develop more robust methods to deal with various ontologies and make full use of the contained semantics in the future.  \nWe also realize some relation-aware GNNs such as RGCN achieve quite promising results on some datasets. This motivates us to study the propagation-based ZSL learner with these GNNs.\nLastly, we will apply and evaluate DOZSL in other tasks such as open information extraction and visual question answering.\n%in our consideration.\n\n% \\section{Rights Information}\n\n\n\n% \\section{Acknowledgments}\n\\begin{acks}\nThis work is partially funded by NSFCU19B2027/91846204, %Jiaoyan Chen is founded by\nthe EPSRC project ConCur (EP/V050869/1) and %. Jeff Z. Pan is partially supported by\nthe Chang Jiang Scholars Program (J2019032).\n%\\todo{Jiaoyan Chen is mainly supported by the SIRIUS Centre for Scalable Data Access (Research Council of Norway, project 237889) and Samsung Research UK.}\n\\end{acks}\n\n% \\section{Appendices}\n\n\n% \\begin{acks}\n% %To Robert, for the bagels and explaining CMYK and color spaces.\n% TODO\n% \\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\balance\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{sample-base}\n\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\n\n\n\\appendix\n\n\n\n\n\n\n"
            },
            "section 6": {
                "name": "Sensitivity Study of DisenE and DisenKGAT",
                "content": "\\label{disenkge_sensitivity_study}\nIn this section, we study the sensitivity of the number of components $K$ used in the baselines DisenE \\cite{kou2020disene} and DisenKGAT \\cite{wu2021disenkgat}.\nSpecifically, we $K$ to $2$ and $4$, two values with which the baselines perform  well, and experiment with the GAN-based learner.\nThe results on the six datasets of the two ZSL tasks are presented in Table~\\ref{tab:disenkge_K_results}.\nWe can find that DisenE gets higher performance on all the three ZS-IMGC datasets and on Wiki-ZS when $K=2$.\nIt also gets better results on most metrics on NELL-ZS when $K=4$.\nAs for DisenKGAT, the optimum $K$ values on AwA, ImNet-A, ImNet-O, NELL-ZS and Wiki-ZS are $4, 2, 2, 2, 4$, respectively.\n\n"
            },
            "section 7": {
                "name": "Ablation Study of The Ontology Encoder with GCN-based methods",
                "content": "\\label{ablation_ontology_encoder}\nIn this section, we report the results of ablation studies on the property guided triple scoring and the neighborhood aggregation in the disentangled ontology encoder when incorporating with GCN-based methods.\nThe results are shown in Table~\\ref{tab:ablation_ontology_encoder_pro}.\n\n\n\n\n\n"
            },
            "section 8": {
                "name": "Ablation Study of The GCN-based Learner",
                "content": "\\label{ablation_sim_fusion}\nIn this section, we study the impact of the similarity threshold and the classifier fusion function under different disentangled ontology embeddings, using all our evaluation datasets.\nThe results are presented in Figure~\\ref{ablation_study_sim_fusion}.\nSpecifically, we report the results of the metric of $acc$ (i.e., the standard ZSL testing setting) for ZS-IMGC task and the results of the metrics of $hit@10$ and $MRR$ for ZS-KGC task.\nMoreover, the curve of the Average fusion function is decorated with circular, while the curve of the Linear Transformation fusion function is decorated with triangle.\nDifferent ontology encoding methods are presented in different colors.\n\n\n\n\n"
            }
        },
        "tables": {
            "tab:datasets": "\\begin{table}\n\\small\n\\centering\n\\caption{\\small Statistics of benchmarks in two ZSL tasks and their ontologies. Trip./Conp./Prop. in the column of \\# Ontologies denotes the number of triples/concepts/properties. S/U denotes seen/unseen classes.\nTr/V/Te is short for  training/validation/testing.}\\label{tab:datasets}\n\\vspace{-2.5mm}\n\\begin{tabular}{c|c|p{0.5cm}<{\\centering}p{0.8cm}<{\\centering}p{1.4cm}<{\\centering}|p{1.8cm}<{\\centering}}\n\\hline\n\\multirow{3}{*}{\\textbf{Datasets}} \n&\n\\multirow{2}{*}{\\textbf{\\#Classes}} & \\multicolumn{3}{c|}{ \\textbf{\\#Images}} \n& \\multirow{2}{*}{\\textbf{\\# Ontologies}}\n\\\\\n& & \n& Training & Testing & \\\\ \n& \\multicolumn{1}{c|}{Total(S/U)}\n&Total & S/U \n& \\multicolumn{1}{c|}{S/U}\n& Trip./Conp./Prop.\n\\\\\\hline\n AwA \n& 50(40/10) \n& 37,322  & 23,527/0 & 5,882/7,913\n& 1,759 / 202 / 2 \n\\\\\nImNet-A \n& 80(28/52)\n& 77,323 & 36,400/0 & 1,400/39,523\n& 545 / 214 / 2\n\\\\\nImNet-O \n& 35(10/25)\n& 39,361  & 12,907/0\n& \\ \\ \\ 500/25,954\n&220 / 111 / 2\n\\\\\n\\hline\n\\end{tabular}\n\\begin{flushleft}\n% \\quad $\\diamond$ S/U denotes seen/unseen classes.\n\\end{flushleft}\n% \\vspace{1cm}\n\\begin{tabular}{l|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}cc}\n\\hline\n% \\toprule[0.6pt]\n\\multirow{2}{*}{\\bf Datasets} \n& \\multicolumn{1}{c}{\\multirow{2}{*}{\\bf \\#Entity }} \n& \\multicolumn{1}{c}{\\multirow{2}{*}{\\bf \\#Triples }} \n& \\multicolumn{1}{c}{\\bf \\#Relations}\n& \\multicolumn{1}{c}{\\bf \\# Ontologies} \\\\\n& &  & \\multicolumn{1}{c}{ Tr/V/Te} \n & \\multicolumn{1}{c}{Trip./Conp./Prop.} \n\\\\\\hline\nNELL-ZS  \n& 65,567 & 188,392  & 139/10/32 \n& 3,055 / 1,186 / 4  \\\\\nWiki-ZS     \n& 605,812 &  724,967 & 469/20/48\n& 4,821 / 1,904 / 4\n\\\\\n\\hline\n\\end{tabular}\n% \\vspace{1mm}\n% \\begin{flushleft}\n% \\quad $\\diamond$ Tr/V/Te is short for  training/validation/testing.\n% \\end{flushleft}\n\n\\vspace{-0.3cm}\n\\end{table}",
            "tab:overall_results": "\\begin{table*}[]\n\\small\n\\caption{$Acc$uracy and $H$ ($\\%$) of ZS-IMGC on AwA, ImNet-A and ImNet-O. $MRR$ and $hit@k$ ($\\%$) of ZS-KGC on NELL-ZS and Wiki-ZS. The best results in a method category (resp. in the whole column) are in bold (resp. underlined).\nTransE+GAN equals OntoZSL.\n}\n\\label{tab:overall_results}\n\\vspace{-0.25cm}\n\\setlength\\tabcolsep{3pt}\n\\begin{tabular}{c|l|p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}|p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}|p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}|c|p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}|p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}}\n\\hline\n% \\toprule[0.5pt]\n\\multicolumn{1}{c|}{\\multirow{2}{*}{\\bf Category}} & \\multicolumn{1}{c|}{\\multirow{2}{*}{\\bf Methods}}  & \\multicolumn{2}{c}{\\bf AwA}   & \\multicolumn{2}{c}{\\bf ImNet-A} & \\multicolumn{2}{c|}{\\bf ImNet-O} &\n&\n\\multicolumn{4}{c}{\\bf NELL-ZS} & \\multicolumn{4}{c}{\\bf Wiki-ZS}\n\\\\\n& & $acc$ & $H$ & $acc$ & $H$ & $acc$ & $H$ & & $hit@10$ & $hit@5$ & $hit@1$ & $MRR$ & $hit@10$ & $hit@5$ & $hit@1$ & $MRR$ \\\\\n% \\hline\n\\cline{1-8}\\cline{10-17}\n% \\midrule[0.3pt]\n\\multirow{6}{*}{Generation}\n& \\ TransE+GAN\n& 58.28 & 54.77\n& 39.44 & 31.61  \n& 31.93 & 27.82 \n&\n& 34.9 & 29.0 & 15.6 & 22.5 \n& 27.6 & 22.3 & 13.6 & 18.7\n\\\\\n& \\ RGAT+GAN\n& 63.95 &  57.52\n& 39.20 &  31.17  \n& 35.13 & 27.68 \n&\n& 34.8 & 28.7 & 16.2 & 22.7\n& 27.9 & 22.5 & \\underline{\\textbf{14.2}} & \\underline{\\textbf{19.1}}\n\\\\\n% \\cmidrule[0.2pt]{2-16}\n\\cline{2-8}\\cline{10-17}\n& \\ DisenE+GAN\n& 59.40 & 49.47 & 33.60 & 29.96\n& 31.62 & 26.69 \n&\n& 34.8 & 29.1 & 15.3  & 22.2 \n& \\textbf{28.0} & 22.7 & 13.8 & 18.9 \n\\\\\n& \\ DisenKGAT+GAN\n& 61.81& 54.41\n& 35.90 & 31.09 \n& 34.94 & 27.33  \n&\n& 35.9 & 29.5 & 15.7 & 22.9\n& 27.6 & 22.4 & 13.8 & 18.8\n\\\\\n% \\cmidrule[0.2pt]{2-16}\n\\cline{2-8}\\cline{10-17}\n& \\ DOZSL(RD+GAN)\n& 52.35 & 46.91\n& 37.12 & 30.18 \n& 34.48 &  28.57\n&\n& \\textbf{36.4} & \\textbf{29.9} & \\textbf{16.5}  & \\textbf{23.4}\n& 27.9 & \\textbf{22.7} & 14.0 & 19.0\n\\\\\n& \\ DOZSL(AGG+GAN)\n& \\underline{\\textbf{66.36}} & \\underline{\\textbf{57.62}}\n& \\underline{\\textbf{40.26}} & \\textbf{32.82}  \n& \\textbf{36.00} & \\textbf{28.74}  \n&\n& 36.2 & 29.5 & 16.1 & 23.0\n& 27.7 & 22.7 & 13.3 & 18.6\n\\\\\n% \\midrule[0.3pt]\n\\cline{1-8}\\cline{10-17}\n\\multirow{10}{*}{Propagation}\n& \\ DGP\n& 59.03 & 28.97\n& 35.72 & 29.98 \n& 34.89 &  29.76  \n&\n& 36.2 & 29.5 & 16.1 & 23.0\n& 27.7 & 22.7 & 13.3 & 18.6\n\\\\\n% Dual Graph \n& \\ Wang et al. \\cite{wang2021zero} \n& 43.81 & 42.13 \n& 34.33 & 21.95   \n& 32.73 & 26.86\n&\n& 35.8 & 29.6 & 15.7 & 22.8 \n& 26.8 & 21.9 & 13.5 & 18.3\n\\\\\n& \\ RGCN-ZSL \n& 44.90 & 24.95 \n& 37.36 & \\underline{\\textbf{33.01}}\n& 31.19 & 23.39 \n&\n& 37.4 & 30.7 & \\underline{\\textbf{17.0}} & \\underline{\\textbf{24.1}}\n& \\underline{\\textbf{28.5}} & \\underline{\\textbf{23.2}} & 13.7 & \\underline{\\textbf{19.1}}\n\\\\\n& \\ CompGCN-ZSL     \n& 53.46 & 29.33 \n& 38.34 & 29.01 \n& 28.95  & 27.35 \n&\n& 36.0 & 29.7 & 16.4 & 23.2  \n& 28.0 & 22.7 & 13.5 & 18.8  \n\\\\\n% \\cmidrule[0.2pt]{2-16}\n\\cline{2-8}\\cline{10-17}\n& \\ TransE+GCN\n& 63.56 &  36.15\n& 36.69 & 22.12 \n& 33.16 & 24.72\n&\n& 35.8 & 29.8 & 16.0 & 22.9\n& 26.6 & 21.5 & 13.6 & 18.3\n\\\\\n& \\ RGAT+GCN\n& 58.83 & 37.35\n& 37.53 & 31.27  \n& 35.47 & 28.49\n&\n& 36.1 & 29.8 & 16.0 & 22.9\n& 26.6 & 21.6 & 13.7 & 18.3 \n\\\\\n\\cline{2-8}\\cline{10-17}\n& \\ DisenE+GCN\n& 58.34 & 50.86 \n& 32.56 & 27.76 \n& 32.02 & 26.33\n&\n& 35.5 & 29.7 & 15.6 & 22.7\n& 26.7 & 21.7 & 13.7 & 18.3 \n\\\\\n& \\ DisenKGAT+GCN\n& 61.24 & 37.43\n& 37.55 & 32.27\n& 35.92  & 29.50\n&\n& 35.7 & 29.5 & 16.1 & 23.0\n& 27.5 & 22.1 & 13.8 & 18.6\n\\\\\n% \\cmidrule[0.2pt]{2-16}\n\\cline{2-8}\\cline{10-17}\n& \\ DOZSL(RD+GCN)\n& 62.79 & \\textbf{52.74}\n& 36.01 & 30.29  \n& 33.66 & 31.19 \n&\n& \\underline{\\textbf{38.0}} & \\underline{\\textbf{31.2}} & 16.5 & 23.9\n& 26.7 & 21.9 & \\textbf{13.8} & 18.5 \n\\\\\n& \\ DOZSL(AGG+GCN)\n& \\textbf{63.88} & 44.52\n& \\textbf{38.69} & 32.12  \n& \\underline{\\textbf{37.42}} & \\underline{\\textbf{31.77}}\n&\n& 36.2 & 29.3 & 16.2 & 23.0\n& 27.5 & 22.4  & 13.6 & 18.7\n\\\\\n\\hline\n% \\bottomrule[0.5pt]\n\\end{tabular}\n\\end{table*}",
            "tab:ablation_ontology_encoder_gen": "\\begin{table*}\n\\small\n\\caption{Results ($\\%$) of ablation studies with GAN.\nThe better results in each group are in bold.\n}\n\\label{tab:ablation_ontology_encoder_gen}\n\\vspace{-0.25cm}\n\\begin{tabular}{l|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}}\n\\hline\n\\multicolumn{1}{c|}{\\multirow{2}{*}{\\bf Methods}}  & \\multicolumn{2}{c}{\\bf AwA}   & \\multicolumn{2}{c}{\\bf ImNet-A} & \\multicolumn{2}{c|}{\\bf ImNet-O} &\n\\multicolumn{4}{c}{\\bf NELL-ZS} & \\multicolumn{4}{c}{\\bf Wiki-ZS}\n\\\\\n& $acc$ & $H$ & $acc$ & $H$ & $acc$ & $H$ & $hit@10$ & $hit@5$ & $hit@1$ & $MRR$ & $hit@10$ & $hit@5$ & $hit@1$ & $MRR$ \\\\\n\\hline\nDOZSL(RD+GAN)\n& 52.35 & 46.91 & \\textbf{37.12} & \\textbf{30.18}\n& \\textbf{34.48} & \\textbf{28.57} \n& \\textbf{36.4} & \\textbf{29.9} & \\textbf{16.5}  & \\textbf{23.4}\n& \\textbf{27.9} & \\textbf{22.7} & 14.0 & \\textbf{19.0}\n\\\\\n$\\text{DOZSL(RD}_{\\text{atten}}$+GAN)\n& \\textbf{59.40} & \\textbf{49.47} & 33.60 & 29.96\n& 31.62 & 26.69 \n& 34.8 & 29.1 & 15.3 & 22.2 \n& 27.2 & 22.0 & \\textbf{14.2} & 18.8 \n\\\\\n\\hline\nDOZSL(AGG+GAN)\n& \\textbf{66.36} & \\textbf{57.62}\n& \\textbf{40.26} & \\textbf{32.82}\n& \\textbf{36.00} & 28.74 \n& \\textbf{36.2} & 29.5 & \\textbf{16.1} & \\textbf{23.0}\n& \\textbf{27.7} & \\textbf{22.7} & \\textbf{13.3} & \\textbf{18.6}\n\\\\\n$\\text{DOZSL(AGG}_{\\text{atten}}$+GAN)\n& 61.51 & 51.06\n& 34.34 & 30.67\n& 30.71 & 26.72\n& 35.8 & \\textbf{29.6} & 15.9 & 22.9\n& 26.7 & 21.8 & 13.0 & 18.1\n\\\\\n$\\text{DOZSL(AGG}_{\\text{sub}}$+GAN)\n& 61.29 & 50.65\n& 34.93 & 28.45 \n& 35.46 & \\textbf{29.40}\n& 35.7 & 29.0 & 15.2 & 22.3\n& 27.1 & 21.8 & 12.6 & 17.9 \n\\\\\n% \\bottomrule[0.5pt]\n\\hline\n\\end{tabular}\n\\end{table*}",
            "tab:disenkge_K_results": "\\begin{table*}[]\n\\small\n\\caption{Results ($\\%$) of DisenE and DisenKGAT with GAN w.r.t different $K$ values.\nThe better results are in bold.\n}\n\\label{tab:disenkge_K_results}\n\\vspace{-0.3cm}\n\\begin{tabular}{l|c|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}}\n\\hline\n% \\toprule[0.5pt]\n\\multicolumn{1}{c|}{\\multirow{2}{*}{\\bf Methods}}  & \\multicolumn{1}{c|}{\\multirow{2}{*}{\\bf K}} & \\multicolumn{2}{c}{\\bf AwA}   & \\multicolumn{2}{c}{\\bf ImNet-A} & \\multicolumn{2}{c|}{\\bf ImNet-O} &\n\\multicolumn{4}{c}{\\bf NELL-ZS} & \\multicolumn{4}{c}{\\bf Wiki-ZS}\n\\\\\n& & $acc$ & $H$ & $acc$ & $H$ & $acc$ & $H$ & $hit@10$ & $hit@5$ & $hit@1$ & $MRR$ & $hit@10$ & $hit@5$ & $hit@1$ & $MRR$ \\\\\n\\hline\nDisenE+GAN & 2\n& \\textbf{59.40} & \\textbf{49.47} & \\textbf{33.60} & \\textbf{29.96}\n& \\textbf{31.62} & \\textbf{26.69} \n& 34.5 & 28.2 & \\textbf{15.4}  & 22.0\n& \\textbf{28.0} & \\textbf{22.7} & 13.8 & \\textbf{18.9} \n\\\\\nDisenE+GAN & 4\n& 44.59 & 41.69 & 24.27 & 23.79 \n& 21.62 & 21.80 \n& \\textbf{34.8} & \\textbf{29.1} & 15.3 & \\textbf{22.2} \n& 27.2 & 22.0 & \\textbf{14.2} & 18.8 \n\\\\\n\\hline\nDisenKGAT+GAN & 2\n& 60.20 & 54.08\n& \\textbf{35.90} &  \\textbf{31.09} \n& \\textbf{34.94} & \\textbf{27.33}  \n& \\textbf{35.9} & \\textbf{29.5} & 15.7 & \\textbf{22.9}\n& 27.5 & 22.0 & 13.7 & 18.6 \n\\\\\nDisenKGAT+GAN & 4\n& \\textbf{61.81}& \\textbf{54.41}\n& 31.35 & 28.57 \n& 31.58 & 27.13 \n& 35.0 & 28.7 & \\textbf{16.1} & 22.5\n& \\textbf{27.6} & \\textbf{22.4} & \\textbf{13.8} & \\textbf{18.8}\n\\\\\n\\hline\n\\end{tabular}\n\\end{table*}",
            "tab:ablation_ontology_encoder_pro": "\\begin{table*}\n\\small\n\\caption{Results ($\\%$) of ablation studies with GCN. The better results in each group are in bold.\n}\n\\label{tab:ablation_ontology_encoder_pro}\n\\vspace{-0.3cm}\n\\begin{tabular}{l|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}|p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}p{0.6cm}<{\\centering}}\n\\hline\n\\multicolumn{1}{c|}{\\multirow{2}{*}{\\bf Methods}}  & \\multicolumn{2}{c}{\\bf AwA}   & \\multicolumn{2}{c}{\\bf ImNet-A} & \\multicolumn{2}{c|}{\\bf ImNet-O} &\n\\multicolumn{4}{c}{\\bf NELL-ZS} & \\multicolumn{4}{c}{\\bf Wiki-ZS}\n\\\\\n& $acc$ & $H$ & $acc$ & $H$ & $acc$ & $H$ & $hit@10$ & $hit@5$ & $hit@1$ & $MRR$ & $hit@10$ & $hit@5$ & $hit@1$ & $MRR$ \\\\\n% \\midrule[0.3pt]\n\\hline\nDOZSL(RD+GCN)\n& \\textbf{62.79} & \\textbf{52.74}\n& \\textbf{36.01} & \\textbf{30.29} \n& \\textbf{33.66} & \\textbf{31.19} \n& \\textbf{38.0} & \\textbf{31.2} & \\textbf{16.5} & \\textbf{23.9} \n& \\textbf{26.7} & \\textbf{21.9} & \\textbf{13.8} & \\textbf{18.5} \n\\\\\n$\\text{DOZSL(RD}_{\\text{atten}}$+GCN)\n& 58.34 & 50.86 \n& 32.56 & 27.76 \n& 32.02 & 26.33\n& 35.5 & 29.7 & 15.6 & 22.7  \n& 26.3 & 21.4 & 13.4 & 18.1 \n\\\\\n\\hline\nDOZSL(AGG)+PRO\n& \\textbf{63.88} & \\textbf{44.52} \n& \\textbf{38.69} & \\textbf{32.12} \n& \\textbf{37.42} & \\textbf{31.77} \n& 36.2 & 29.3 & \\textbf{16.2} & 23.0 \n& \\textbf{27.5} & \\textbf{22.4} & 13.6 & \\textbf{18.7} \n\\\\\n$\\text{DOZSL(AGG}_{\\text{atten}}$+GCN)\n& 54.40 & 32.00 \n& 36.18 & 27.55 \n& 31.47 & 26.54 \n& 35.3 & 28.8 & 15.6 & 22.3 \n& 27.5 & 22.1 & \\textbf{13.7} & 18.7   \n\\\\\n$\\text{DOZSL(AGG}_{\\text{sub}}$+GCN)\n& 63.66 & 33.19 \n& 35.03 & 26.63 \n& 35.37 & 31.11 \n& \\textbf{36.9} & \\textbf{30.0} & 15.8 & \\textbf{23.1} \n& 27.1 & 21.8 & 13.4 & 18.3 \\\\\n\\hline\n\\end{tabular}\n\\end{table*}"
        },
        "figures": {
            "fig:intro": "\\begin{figure}\n\\centering\n\\includegraphics[width=0.49\\textwidth]{figures/intro3.pdf}\n\\vspace{-5mm}\n\\caption{\n(a) an ontology segment for zero-shot image classification where \\textit{Zebra} is an unseen class while the other animals are seen classes; and (b) an ontological schema segment for zero-shot KG completion where \\textit{has\\_office\\_in\\_city} is an unseen relation while the other relations are seen.\n%, respectively.\nThe unseen class (or relation) connects itself to different seen classes (or relations) in different semantic aspects.\n%under different semantics.\n% An example of the structural correlations between unseen class \\textit{Zebra} and two seen classes \\textit{Horse} and \\textit{Tiger} in animal recognition. It is clear that an unseen class is always related to different seen classes under different semantics.\n}\n\\label{fig:intro}\n\\vspace{-5mm}\n\\end{figure}",
            "fig:framework": "\\begin{figure*}\n\\centering\n\\includegraphics[height=5cm]{figures/framework3.pdf}\n\\vspace{-2.5mm}\n\\caption{Illustration of DOZSL with $K=3$. Different color means different semantic aspects.}\n\\label{fig:framework}\n\\vspace{-3mm}\n\\end{figure*}",
            "case_study": "\\begin{figure}\n\\centering  \n\\includegraphics[width=0.85\\linewidth]{figures/NELL_cases2.pdf}\n\\vspace{-0.2cm}\n\\caption{Cases of relations in NELL-ZS. Best viewed in color.}\n\\label{case_study}\n\n\\end{figure}",
            "ablation_study_sim_fusion": "\\begin{figure*}\n\\centering  \n\\vspace{-0.35cm} %设置与上面正文的距离\n% \\subfigtopskip=2pt %设置子图与上面正文或别的内容的距离\n\\subfigbottomskip=-2pt %设置第二行子图与第一行子图的距离，即下面的头与上面的脚的距离\n% \\subfigcapskip=-2pt %设置子图与子标题之间的距离\n\\subfigure[Accuracy on AwA]{\n\\label{level.sub.1}\n\\includegraphics[width=0.25\\linewidth]{figures/plot_awa_acc.pdf}}\n\\quad %默认情况下两个子图之间空的较少，使用这个命令加大宽度\n\\subfigure[Accuracy on ImNet-A]{\n\\label{level.sub.2}\n\\includegraphics[width=0.25\\linewidth]{figures/plot_imnet_a_acc.pdf}}\n\\quad\n\\subfigure[Accuracy on ImNet-O]{\n\\label{level.sub.3}\n\\includegraphics[width=0.25\\linewidth]{figures/plot_imnet_o_acc.pdf}}\n\\\\\n\\subfigure[Hit@10 and MRR on NELL-ZS]{\n\\label{level.sub.4}\n\\includegraphics[width=0.26\\linewidth]{figures/plot_nell_hit10.pdf}\n\\includegraphics[width=0.26\\linewidth]{figures/plot_nell_mrr.pdf}\n}\n\\quad\n\\subfigure[Hit@10 and MRR on Wiki-ZS]{\n\\label{level.sub.6}\n\\includegraphics[width=0.26\\linewidth]{figures/plot_wiki_hit10.pdf}\n\\includegraphics[width=0.26\\linewidth]{figures/plot_wiki_mrr.pdf}\n}\n\\vspace{-0.2cm}\n\\caption{Results of GCN-based DOZSL variants using different ontology encoders with different similarity thresholds and different classifier fusion functions. Best viewed in color.}\n\\label{ablation_study_sim_fusion}\n\\end{figure*}"
        },
        "equations": {
            "eq:eq:attention": "\\begin{align}\\label{eq:attention}\n\\alpha_{(c_i,p,c_j)}^{k,l} &= softmax((h_{i,k,p}^l)^T \\cdot h_{j,k,p}^l)\n    \\nonumber \\\\\n&= \\frac{exp((h_{i,k,p}^l)^T \\cdot h_{j,k,p}^l)}{\\sum_{(c_{j'}, p')\\in \\mathcal{N}(i)} exp((h_{i,k,p'}^l)^T \\cdot h_{j',k,p'}^l)} \\\\\nh_{i,k,p}^l &= h_{i,k}^l \\circ W_p, \\;\\;\\; h_{j,k,p}^l = h_{j,k}^l \\circ W_p \n\\end{align}",
            "eq:RGAT": "\\begin{equation}\\label{RGAT}\nh_{i,k}^{l+1} = \\sigma(\\sum_{(c_j,p)\\in \\mathcal{N}(i)} \\alpha_{(c_i,p,c_j)}^{k,l} \\phi (h_{j,k}^l, h_p^l, W_p)), \\;\\;\nh_p^{l+1} =  h_p^{l}\\cdot \\Theta_p^l\n\\end{equation}",
            "eq:eq:score": "\\begin{equation}\\label{eq:score}\nq_{(c_i, p_k, c_j)} = f(-||\\bm{c}_i^k + \\bm{p}_k -  \\bm{c}_j^k||)\n\\end{equation}",
            "eq:1": "\\begin{equation}\n\\begin{aligned}\n\\mathcal{L} = -\\frac{1}{B}\\frac{1}{|\\mathcal{C}|} \\sum_{(c_i, p_k) \\in batch} \\sum_{n} (t_n \\cdot log(q_{(c_i, p_k, c_j^n)}) + \\\\ (1-t_n) \\cdot log(1- q_{(c_i,p_k,c_j^n)}))\n\\end{aligned}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\t\\mathcal{L}_G = - \\mathbb{E}[D(\\hat{x})] + \\lambda_1 \\mathcal{L}_{cls} (\\hat{x}) + \\lambda_2 \\mathcal{L}_{R}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n% \\begin{aligned}\n\t\\mathcal{L}_D = \\mathbb{E}[D(x, \\bm{c}_i)] - \\mathbb{E}[D(\\hat{x})] - \\beta \\mathbb{E}[(|| \\bigtriangledown_{\\tilde{x}} D(\\tilde{x}) ||_p -1)^2]\n% \t\\end{aligned}\n\\end{equation}",
            "eq:4": "\\begin{equation}\nA_k (i,j) = \n\\left\\{\n\\begin{array}{cc}\n    1 & \\text{if} \\quad sim(\\bm{c}_i^k, \\bm{c}_j^k) > \\tau\n    \\\\\n    0 & \\text{otherwise}\n\\end{array}\n\\right.\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    H_k^{l+1} = \\sigma (\\widehat{A}_k H_k^{l} \\Phi_k^{l})\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\mathcal{L}_{GCN} = \\frac{1}{|\\widetilde{\\mathcal{W}_s}|} \\sum_{\\widetilde{w} \\in \\widetilde{\\mathcal{W}_s}} (\\widetilde{w} - gt(\\widetilde{w}))^2\n\\end{equation}"
        },
        "git_link": "https://github.com/zjukg/DOZSL"
    }
}