{
    "meta_info": {
        "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced  Text-Attributed Graph Representation Learning",
        "abstract": "Representation learning on text-attributed graphs (TAGs) has become a\ncritical research problem in recent years. A typical example of a TAG is a\npaper citation graph, where the text of each paper serves as node attributes.\nInitial graph neural network (GNN) pipelines handled these text attributes by\ntransforming them into shallow or hand-crafted features, such as skip-gram or\nbag-of-words features. Recent efforts have focused on enhancing these pipelines\nwith language models (LMs), which typically demand intricate designs and\nsubstantial computational resources. With the advent of powerful large language\nmodels (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and\nto utilize general knowledge, there is a growing need for techniques which\ncombine the textual modelling abilities of LLMs with the structural learning\ncapabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to\ncapture textual information as features, which can be used to boost GNN\nperformance on downstream tasks. A key innovation is our use of explanations as\nfeatures: we prompt an LLM to perform zero-shot classification, request textual\nexplanations for its decision-making process, and design an LLM-to-LM\ninterpreter to translate these explanations into informative features for\ndownstream GNNs. Our experiments demonstrate that our method achieves\nstate-of-the-art results on well-established TAG datasets, including Cora,\nPubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23.\nFurthermore, our method significantly speeds up training, achieving a 2.88\ntimes improvement over the closest baseline on ogbn-arxiv. Lastly, we believe\nthe versatility of the proposed method extends beyond TAGs and holds the\npotential to enhance other tasks involving graph-text data. Our codes and\ndatasets are available at: https://github.com/XiaoxinHe/TAPE.",
        "author": "Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, Bryan Hooi",
        "link": "http://arxiv.org/abs/2305.19523v5",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "In Proceedings of ICLR 2024"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n%XH Many real-world graphs possess text attributes, and can be termed as text-attributed graphs~\\citep{yang2021graphformers}. In TAGs, nodes represent textual entities such as documents or sentences, and edges represent relationships between them. For example, the \\texttt{ogbn-arxiv} dataset~\\citep{hu2020open} is a TAG representing a citation network, where each node represents a paper, with the title and abstract as node attributes. More generally, the combination of textual attributes and graph topology provides a rich source of information to enhance representation learning on important applications such as text classification~\\citep{yang2015network_TADW, wang2016linked, yasunaga2017graph, chien2021node_giant, zhao2022learning_em}, recommendation systems~\\citep{zhu2021textgnn}, social networks, and fake news detection~\\citep{liu2019fine_fact_verify}.\n% \\xb{Many real-world graphs possess textual information, and are often referred to text-attributed graphs~\\citep{yang2021graphformers}. In TAGs, nodes typically represent text entities such as documents or sentences, while edges signify relationships between these entities. For example, consider the \\texttt{ogbn-arxiv} dataset~\\citep{hu2020open}, which represents a citation network in TAG form, where each node corresponds to a paper, with its title and abstract serving as node attributes. More generally, the fusion of textual attributes with graph topology provides a rich source of information to significantly enhance representation learning on important applications such as text classification~\\citep{yang2015network_TADW, wang2016linked, yasunaga2017graph, chien2021node_giant, zhao2022learning_em}, recommendation systems~\\citep{zhu2021textgnn}, social networks, and fake news detection~\\citep{liu2019fine_fact_verify}.}\nMany real-world graphs possess textual information, and are often referred to text-attributed graphs~\\citep{yang2021graphformers}. In TAGs, nodes typically represent text entities, such as documents or sentences, while edges signify relationships between these entities. For example, the \\texttt{ogbn-arxiv} dataset~\\citep{hu2020open} represents a citation network in TAG form, where each node corresponds to a paper, with its title and abstract serving as node attributes. More generally, the combination of textual attributes with graph topology provides a rich source of information,  significantly enhancing representation learning for important applications, such as text classification~\\citep{yang2015network_TADW, wang2016linked, yasunaga2017graph, chien2021node_giant, zhao2022learning_em}, recommendation systems~\\citep{zhu2021textgnn}, social networks, and fake news detection~\\citep{liu2019fine_fact_verify}.\n\n\n\n\n\\textbf{Representation learning on TAGs.}\nPrior research has explored various approaches for representation learning on TAGs. \nThe standard GNN pipeline (illustrated in Figure \\ref{fig: overview} in light yellow),  first encodes the textual attributes of each node using shallow or hand-crafted features such as skip-gram~\\citep{mikolov2013distributed_skipgram} or bag-of-words (BoW)~\\citep{harris1985distributional_bow} (refer to Table~\\ref{tab: text preprocess}). The resulting node features are then used as input for a GNN. For instance, the Open Graph Benchmark (OGB)~\\citep{hu2020open} generated BoW and skip-gram~\\citep{mikolov2013distributed_skipgram} features for the \\texttt{ogbn-products} and \\texttt{ogbn-arxiv} datasets respectively.\n% The GNN model can then learn to propagate and aggregate information across the graph structure to produce a final embedding for the downstream tasks.\n% that captures both its semantic meaning and its relational dependencies with other nodes in the graph.\nThese processed features are readily available within popular graph libraries, such as PyTorch Geometric (PyG)~\\citep{fey2019fast_pyg} and Deep Graph Library (DGL)~\\citep{wang2019deep_dgl}, and have been widely used by the graph community. \nHowever, these shallow text embeddings are limited in the complexity of the semantic features they can capture, especially when compared to approaches based on multi-layer LMs.\n% , with a common method being the use of a combination of language models (LMs) and graph neural networks (GNNs)~\\citep{chien2021node_giant, zhao2022learning_em, dinh2022e2eg}\n\n%XH \\textbf{LM-based pipelines for TAGs.} Recent work has therefore focused on designing LM-based pipelines to better capture the context and nuances of the text in TAGs~\\citep{chien2021node_giant, zhao2022learning_em, dinh2022e2eg}. \n\n\\textbf{LM-based pipeline for TAGs.} \nRecent works have therefore focused on designing LM-based techniques to better capture the context and nuances of text within TAGs~\\citep{chien2021node_giant, zhao2022learning_em, dinh2022e2eg}.\nIn this approach, pre-trained LMs are fine-tuned and used to generate node embeddings that are tailored to the specific TAG tasks (depicted in Figure \\ref{fig: overview} in light gray).\nFor example, \\citet{chien2021node_giant} fine-tuned an LM using a neighborhood prediction task, while \\citet{zhao2022learning_em} fine-tuned an LM to predict the label distribution from a GNN's outputs. \nLM-based models have achieved state-of-the-art (SOTA) results in node classification on \\texttt{ogbn-arxiv} and \\texttt{ogbn-products}~\\citep{zhao2022learning_em}. However, these works typically entail intricate designs and demand substantial computational resources.\n% s.a. GIANT \\citet{chien2021node_giant} which was trained with 8 Nvidia V100 GPUs.}\nFurthermore, for scalability reasons, existing works mostly rely on relatively small LMs, such as BERT~\\citep{devlin2018bert} and DeBERTa~\\citep{he2021deberta}, and thus lack the complex reasoning abilities associated with larger language models.\n\n\n\n\\textbf{Large Language Models.}\n% The development of large pre-trained models such as GPT~\\citep{brown2020language_gpt} has revolutionized language modeling. LLMs have resulted in clear improvements on many natural language processing (NLP) tasks, and enabled sophisticated language processing capabilities such as complex and zero-shot reasoning. Scaling laws~\\citep{kaplan2020scaling_law} have revealed predictable rules for performance improvements with model and training data size. Furthermore, LLMs have exhibited `emergent abilities' that were not explicitly trained for, such as arithmetic, multi-step reasoning and instruction following~\\citep{wei2022emergent}. \n% Despite LLMs' success in other domains including computer vision~\\citep{tsimpoukelli2021multimodal}, their potential benefits for TAG tasks have yet to be explored, making it an exciting and promising avenue for future research. Therefore, we would like to explore it in this work.\nThe advent of large pre-trained models, exemplified by GPT~\\citep{brown2020language_gpt}, has revolutionized the field of language modeling. LLMs have notably enhanced performance across various natural language processing (NLP) tasks, and enabled sophisticated language processing capabilities such as complex and zero-shot reasoning. \nFurthermore, scaling laws~\\citep{kaplan2020scaling_law} have revealed predictable rules for performance improvements with model and training data size. Additionally, LLMs have exhibited ``emergent abilities'' that were not explicitly trained for, such as arithmetic, multi-step reasoning and instruction following~\\citep{wei2022emergent}. \nWhile LLMs have found new success in domains like computer vision~\\citep{tsimpoukelli2021multimodal}, their potential benefits when applied to TAG tasks remain largely uncharted. This presents an exciting and promising avenue for future research, and it is precisely this untapped potential that we aim to explore in this work.\n\n% This presents an exciting and promising avenue for future research, which is the precise focus of this work.\n\n\n% XH : In this paper, we explicitly distinguish between ``LMs'' and ``LLMs''. We use ``LMs'' to refer to relatively small language models that can be trained and tuned with an academic lab budget. We use ``LLM'' to refer to very large language models that are capable of learning complex patterns, such as GPT-3/4. These models typically have tens or hundreds of billions of parameters and require significant computational resources to train and use. The size and complexity of recent LLMs have raised concerns about their scalability, as they can be too large even to run inference on with an academic research lab's machines. To address this, LLMs are typically made available through language modeling as a service (LMaaS)~\\citep{sun2022black_lmaas}, which enables developers to utilize LLMs without the need for extensive computational resources or expertise. In this paper, our objective is to extract information from an LLM in a way that is compatible with LMaaS. Hence, we do not require fine-tuning the LLM or extracting its logits, but only its output in text form. In contrast, existing LM-based pipelines~\\citep{chien2021node_giant, zhao2022learning_em, dinh2022e2eg} are not directly compatible with LLMs, as they require fine-tuning the LMs, as well as accessing their latent embeddings or logits, which GPT-3.5 and 4 do not provide. Hence, to the best of our knowledge, the use of LLMs in TAG tasks remains unexplored.\n\n\\textbf{LMs vs. LLMs.} In this paper, we make a clear distinction between ``LMs'' and ``LLMs''. We use LMs to refer to relatively small language models that can be trained and fine-tuned within the constraints of  {\\it an academic lab budget}. We refer to LLMs as very large language models that are capable of learning significantly more complex linguistic patterns than LMs, such as GPT-3/4. These models typically have tens or hundreds of billions of parameters and require {\\it substantial computational resources} to train and use, \\eg GPT-3 was trained on a supercomputer with 10,000 GPUs. The size and complexity of recent LLMs have raised concerns about their scalability, as they can be too large even to run inference on the machines typically available within academic research labs. To address this issue, LLMs are often made accessible through {\\it language modeling as a service} (LMaaS)~\\citep{sun2022black_lmaas}. This approach enables developers to harness the power of LLMs without necessitating extensive computational resources or specialized expertise. In the context of this paper, one of our primary objectives is to extract information from an LLM in a LMaaS-compatible manner. As a result, we do not require fine-tuning the LLM or extracting its logits; rather, we focus solely on obtaining its output in textual form. In contrast, existing LM-based techniques~\\citep{chien2021node_giant, zhao2022learning_em, dinh2022e2eg} are not directly compatible with LLMs, as they require fine-tuning of LMs, as well as accessing their latent embeddings or logits, which GPT-3/4 do not provide. Consequently, to the best of our knowledge, the use of LLMs in TAG tasks remains unexplored.\n% Another crucial advantage of utilizing LMaaS to extract information is its ability to offer {\\it clear interpretability} of the LLM's reasoning. This interpretability is of paramount importance in the development of trusted learning algorithms.\n\n\n\n\n\n\n% XH : \\textbf{The present work: LLM augmentation using explanations.} \n%To assess the utility of LLMs in improving representation learning on TAGs, we conduct a preliminary study to evaluate the zero-shot classification performance of GPT-3.5 on \\texttt{ogbn-arxiv}. We use a task-specific prompt that included the title and abstract of an arXiv paper, and a question. This results in a high zero-shot accuracy of 73.5\\% on \\texttt{ogbn-arxiv}, outperforming many fully trained GNN baselines, including RevGAT~\\citep{li2021training_revgat} with OGB features which achieved 70.8\\% accuracy, along with high quality and coherent text explanations, but falls short of achieving the SOTA accuracy of 76.6\\%~\\citep{zhao2022learning_em}.\n\n\n% [neurips] To study the potential of LLMs in improving the quality of TAG embeddings, we conduct a preliminary study to evaluate the zero-shot classification performance of GPT-3.5 on \\texttt{ogbn-arxiv}. We use a task-specific prompt that included the title and abstract of an arXiv paper, and a question. This results in a high zero-shot accuracy of 73.5\\% on \\texttt{ogbn-arxiv}, outperforming many fully trained GNN baselines, including RevGAT~\\citep{li2021training_revgat} with OGB features which achieved 70.8\\% accuracy, along with high quality and coherent text explanations, but falls short of achieving the SOTA accuracy of 76.6\\%~\\citep{zhao2022learning_em}.\n% but still not achieving SOTA.\n% XB: \\textbf{Preliminary study.} To assess the effectiveness of LLMs in improving representation learning for TAGs, we conducted an initial investigation into the zero-shot classification capability of GPT-3.5 applied to the \\texttt{ogbn-arxiv} dataset. In this evaluation, we used a task-specific prompt that includes both the title and abstract of an arXiv paper, along with a question to predict the correct answer. The result with GPT-3.5 achieved a {\\it promising} zero-shot accuracy of 73.5\\% on the \\texttt{ogbn-arxiv} dataset, outperforming several fully trained GNN baselines, including RevGAT~\\citep{li2021training_revgat} with OGB features which achieved 70.8\\% accuracy. Additionally, GPT-3.5 offered high-quality and coherent text explanations, {\\it but falls short} of achieving the SOTA accuracy of 76.6\\%~\\citep{zhao2022learning_em}.\n\n\\textbf{Preliminary study.} To assess the potential of LLMs in enhancing representation learning for TAGs, we conducted an initial investigation into leveraging GPT-3.5 for zero-shot classification on the \\texttt{ogbn-arxiv} dataset. Using task-specific prompts consisting of paper titles, abstracts, and questions, GPT-3.5 achieved a promising accuracy of 73.5\\%, along with high-quality text explanations, surpassing several fully trained GNN baselines like RevGAT~\\citep{li2021training_revgat} with OGB features (70.8\\% accuracy), but falling short of the SOTA accuracy of 76.6\\%~\\citep{zhao2022learning_em}.\n\n\n\n\n% XH: Hence, we propose a novel framework aimed at harnessing the explanatory insights of LLMs to enhance representation learning on TAGs.\n% A key innovation is our use of \\emph{explanations as features}: \n% by instructing a powerful model to explain its predictions, we prompt it to reveal its most pertinent prior knowledge about a sample, and reasoning steps. Hence, this concise summary encodes the LLM\u2019s knowledge in a way that can be easily understood by a smaller model (analogous to human experts using explanations to convey their insights and reasoning). \n% % we prompt an LLM to perform zero-shot class prediction for the target task and request textual explanations for its decision-making process. \n% % These explanations are then translated into informative features for TAGs using a dedicated \\emph{LLM-to-LM interpreter}. \n% To motivate this, observe in Figure \\ref{fig: overview} that the explanations (yellow box) highlight and expand upon key useful information from the text such as ``deep learning techniques such as DeconvNet,'' and the relationship between text recognition and information retrieval, benefiting from the LLM's general knowledge. This suggests that these explanations serve as rich features to enhance downstream stages of a TAG pipeline. Concretely, we develop a custom prompt to query an LLM such as GPT or Llama to generate both a \\emph{ranked prediction list} and a \\emph{textual explanation} of its predictions. \n% These predictions and explanations are transformed into useful node features by fine-tuning a smaller LM such as DeBERTa~\\citep{he2021deberta} on the target task, providing tailored features for the training of any downstream GNN, here we consider the node classification task. This smaller LM functions as an \"interpreter,\"  enabling seamless communication between the LLM (handling text input and output) and the GNN (handling vectorial representation). \n\n% XB: \\textbf{Enhancing TAGs' representation through LLMs' explanations.} In this work, we introduce an innovative framework designed to harness the explanatory capabilities of LLMs to enhance representation learning of TAGs. A key innovation of our framework is the concept of \\emph{explanations as features}. By instructing a powerful model to explain its predictions, we prompt it to reveal its most pertinent prior knowledge about a given sample, along with the underlying reasoning steps. Hence, this concise summary encapsulates the LLM's knowledge in a format easily comprehensible by a smaller model (analogous to human experts using explanations to convey their insights and reasoning). To motivate this concept further, observe in Figure \\ref{fig: overview} that the explanations (in the yellow box) highlight and expand upon key crucial information from the text, such as ``deep learning techniques such as DeconvNet,'' and the relationship between text recognition and information retrieval, benefiting from the LLM's general knowledge. As a result, these explanations emerge as valuable features that can enhance subsequent phases of a TAG pipeline. In practice, we develop a tailored prompt to query an LLM such as GPT or Llama2 to generate both a \\emph{ranked prediction list} and a \\emph{textual explanation} for its predictions. These predictions and explanations are then transformed into informative node features through fine-tuning a smaller LM such as DeBERTa~\\citep{he2021deberta} for the target task, providing tailored features for the training of any downstream GNN, here the node classification task. This smaller LM functions as an {\\it \"interpreter\"}, facilitating seamless communication between the LLM (handling text input and output) and the GNN (managing vectorial representation). \n\n\\textbf{The present work: LLM augmentation using explanations.} We introduce a novel framework that leverages LLMs to improve representation learning on TAGs. A key innovation is the concept of \\emph{explanations as features}. By prompting a powerful LLM to explain its predictions, we extract its relevant prior knowledge and reasoning steps, making this information digestible for smaller models, akin to how human experts use explanations to convey insights. To illustrate this concept further, observe in Figure \\ref{fig: overview} that the explanations (in the yellow box) highlight and expand upon key crucial information from the text, such as ``deep learning techniques such as DeconvNet,'' and the relationship between text recognition and information retrieval. These explanations draw from the LLM's general knowledge and serve as valuable features for enhancing subsequent TAG pipeline phases. In practice, we design a tailored prompt to query an LLM such as GPT or Llama2 to generate both a \\emph{ranked prediction list} and a \\emph{textual explanation} for its predictions. These predictions and explanations are then transformed into informative node features through fine-tuning a smaller LM such as DeBERTa~\\citep{he2021deberta} for the target task, providing tailored features for any downstream GNNs. This smaller model acts as an {\\it interpreter}, facilitating seamless communication between the LLM (handling text) and the GNN (managing vectorial representation). \n\n\n% [neurips] Hence, we propose a framework to leverage LLMs to generate high-quality node features for TAGs. A key innovation is our use of \\emph{explanations as features}: we use LLM inference APIs to generate explanations, then transform these explanations into informative features for TAGs. To motivate this, observe in Figure \\ref{fig: overview} that the explanations (yellow box) highlight and expand upon key useful information from the text such as ``deep learning techniques such as DeconvNet,'' and the relationship between text recognition and information retrieval, benefiting from the LLM's general knowledge. This suggests that these explanations serve as rich features to enhance downstream stages of a TAG pipeline. Concretely, we develop a custom prompt to query LLMs to generate both a \\emph{ranked prediction list} and a \\emph{textual explanation} of its predictions, which are transformed by a subsequent smaller LM such as DeBERTa~\\citep{he2021deberta} into enriched node features that can be used for training any downstream GNN, here we consider the node classification task.\n\n% XH : Figure~\\ref{fig: comparison} shows that our method outperforms both the shallow embedding pipelines and LM-based pipelines on \\texttt{ogbn-arxiv}, and with $2.88\\times$ lower computation time than the GLEM baseline.\n\n\n\n\nOur contributions are summarized as follows:\n\\begin{itemize}\n    \\item \\textbf{Novel LMaaS-compatible approach.} We propose the first LMaaS-compatible approach, to the best of our knowledge, for leveraging LLMs to enhance representation learning on TAGs. Our innovations involve extracting explanations from an LLM, here GPT-3.5 and Llama2, and subsequently employing an LLM-to-LM interpreter to translate textual explanations into enriched node vector representations for downstream GNNs. \n    % \\xb{Our approach  improves modularity, robustness and interpretability compared to prior LM+GNN models.} \n    Our approach improves modularity and efficiency compared to prior LM+GNN models.\n    \\item \\textbf{SOTA performance.} Extensive experiments demonstrate that our {method} significantly boost the performance of various GNN models across diverse datasets. \n    Notably, we achieve top-1 performance on \\texttt{ogbn-arxiv} with significantly lower computation time, {\\ie $2.88\\times$ faster than GLEM}, and also excel in the TAG versions of \\texttt{PubMed} and \\texttt{Cora} datasets. \n    \\item \\textbf{Data contribution.} We provide open-source access to our codes, pre-trained networks and enriched features. Additionally, recognizing the absence of raw text data for \\texttt{Cora} and \\texttt{PubMed} in common repositories (\\eg PyG, DGL), we have collected and released these datasets in TAG format. Furthermore, we introduce the new \\texttt{tape-arxiv23} citation graph dataset, extending beyond GPT-3's knowledge cutoff, \\ie Sept. 2021. These datasets can serve as valuable resources for the NLP and GNN research community.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n\\textbf{Shallow embedding pipeline for TAGs.}\nIn the context of learning representations on TAGs, a common approach involves combining graph-based learning with language modeling techniques.\nOne prevalent strategy is to transform text attributes into shallow or hand-crafted features, such as skip-gram~\\citep{mikolov2013distributed_skipgram} or BoW~\\citep{harris1985distributional_bow} features. Detailed information is available in Table~\\ref{tab: text preprocess}. These engineered features can then be fed as inputs to a graph-based learning algorithm, such as a graph convolutional network (GCN)~\\citep{kipf2016semi_gcn}, which learns embeddings capturing the graph structure while incorporating the extracted text features. Shallow embedding methods are widely used in the graph community due to their simplicity and computational efficiency, such as for designing GNN architectures~\\citep{velivckovic2017graph_gat, chiang2019cluster_gcn, velickovic2019deep_graph_informax, zhang2021graphless} or benchmarking graph learning~\\citep{yang2016revisiting, hu2020open}. However, they may have limitations in capturing complex semantic relationships and fully leveraging the richness of text attributes, particularly in scenarios involving intricate semantic relationships and contextual information.\n\n% Then, the resulting embeddings can be fed as input to a graph-based learning algorithm, such as a graph convolutional network (GCN)~\\citep{kipf2016semi_gcn}. The GCN will learn embeddings that capture the graph structure while incorporating the text features extracted earlier.\n% Therefore, deeper embedding approaches have been explored to address these limitations and achieve more expressive representations on TAGs.\n\n\n\n\\textbf{LM-based pipeline for TAGs.}\nTo overcome the limitations of shallow embedding approaches, researchers have explored deep embedding techniques by fine-tuning pre-trained LMs, such as BERT~\\citep{devlin2018bert}, to generate node embeddings that are specifically adapted to the domain and context of the TAGs. These deep embeddings effectively capture the semantic richness of text attributes, leading to improved performance on various TAG-related tasks. Integrating LM-based embeddings and graph-based learning can be done through different approaches. One approach is to use a cascaded architecture, where the node features are first encoded independently by the LMs, and then fed into GNN models. This representation paradigm has been widely adopted in subsequent works, such as TextGNN~\\citep{zhu2021textgnn}, GIANT~\\citep{chien2021node_giant}, GPT-GNN~\\citep{hu2020gpt_gnn}, SimTeg~\\citep{duan2023simteg}, as well as in studies related to knowledge graphs~\\citep{yasunaga2021qa, zhang2022greaselm} and  fact verification~\\citep{liu2019fine_fact_verify, zhou2019gear} that are beyond the scope of this work. An alternative approach involves fusing text encoding and graph aggregation into an iterative workflow, enabling the model to refine both the text representations and the node embeddings simultaneously, such as Graphormer~\\citep{yang2021graphformers}, DRAGON~\\citep{yasunaga2022deep_dragon}, and GLEM~\\citep{zhao2022learning_em}, to name a few. \n\n\n\\textbf{LLM-based pipeline for TAGs.}\nIncorporating LLMs into TAG tasks presents a promising frontier. LLMs such as ChatGPT~\\citep{brown2020language_gpt} by OpenAI, PaLM~\\citep{chowdhery2022palm} by Google, and LLaMA~\\citep{touvron2023llama} by Meta, have demonstrated their effectiveness across a spectrum of NLP tasks. However, their potential benefits for TAG tasks have yet to be fully explored. While some recent research efforts have sought to evaluate the capacity of LLMs in understanding graph-structured data and enhance their graph processing capabilities~\\citep{wang2023can, zhang2023graph_toolformer, guo2023gpt4graph}, these endeavors, while valuable, may not be directly aligned with our specific focus on TAGs. By exploring LLM-based methods designed specifically for TAGs, we can unlock new possibilities for improving TAG prediction performance and advancing our understanding of text attributes within graph-based data. Notably, our initial attempt has already inspired further research endeavors in this direction.\n%\\xb{Why not keeping this important sentence without citing the papers?}%~\\citep{ye2023natural,chen2023exploring,duan2023simteg}. \\xb{DO WE BREAK ANONYMITY BY CITING THESE PAPERS THAT CITE OURS?}\n\n% While recent work, such as Graph-ToolFormer~\\citep{zhang2023graph_toolformer}, focuses on empowering LLMs with graph reasoning abilities, it is not directly aligned with our scenario of TAGs. \n\n% \\xh{\n% \\textbf{Explanations as features.} Explaining model predictions is a rich area of study within the domain of explainable AI research. Nevertheless, to the best of our knowledge, no existing approaches have harnessed explanations from one model to craft informative features for another model.\n% }\n\n% [neurips] Large language models, such as ChatGPT~\\citep{brown2020language_gpt} from OpenAI, PaLM~\\citep{chowdhery2022palm} from Google and LLaMA~\\citep{touvron2023llama} from Meta, have demonstrated their effectiveness in various NLP tasks. However, the potential benefits of LLMs for TAG tasks have yet to be fully explored. While recent work, such as Graph-ToolFormer~\\citep{zhang2023graph_toolformer}, focuses on empowering LLMs with graph reasoning abilities, it is not directly aligned with our scenario of TAGs. Exploring LLM-based methods specifically designed for TAGs can unlock new possibilities for improving TAG prediction performance and advancing our understanding of text attributes in graph-based data.\n\n\n\n"
            },
            "section 3": {
                "name": "Formalization",
                "content": "\n\n% In this section, we introduce some preliminary concepts related to language models, large language models, and graph neural networks for node classification on TAGs.\nIn this section, we introduce notation and formalize some concepts related to language models, large language models, and graph neural networks for node classification on TAGs.\n\n% \\paragraph{Text-attributed graphs.}\n\\textbf{Text-attributed graphs.}\n% XH: Formally, a TAG can be represented as $\\mathcal{G} = (\\mathcal{V}, A, \\{s_n\\}_{n \\in \\mathcal{V}})$, where $\\mathcal{V}$ is a set of $N$ nodes, $A\\in \\mathbb{R}^{N\\times N}$ is the adjacency matrix, and $s_n$ is a sequential text feature associated with node $n \\in \\mathcal{V}$.% and $N$ is the number of nodes.  \n Formally, a TAG can be represented as $\\mathcal{G} = (\\mathcal{V}, A, \\{s_n\\}_{n \\in \\mathcal{V}})$, where $\\mathcal{V}$ is a set of $N$ nodes, $A\\in \\mathbb{R}^{N\\times N}$ is the adjacency matrix, and $s_n\\in\\mathcal{D}^{L_n}$ is a sequential text associated with node $n \\in \\mathcal{V}$, with $\\mathcal{D}$ as the words or tokens dictionary, and $L_n$ as the sequence length. \n % and $N$ is the number of nodes.  \n% What is $s_n$? a vector or a set of words?)}\\xh{(a set of words such as sentences and paragraphs)\nIn this paper, we investigate node classification on TAGs.  Specifically, given some labeled nodes $\\mathcal{L} \\subset \\mathcal{V}$ , the goal is to predict the labels of the remaining unlabeled nodes $\\mathcal{U} = \\mathcal{V} \\setminus \\mathcal{L}$.\n\n% \\paragraph{Language models for text classification.}\n\\textbf{Language models for text classification.}\nIn the context of TAGs,  LMs can be employed to encode the text attributes associated with each node and learn a representation that captures the semantic meaning of the text. Let $s_n\\in\\mathcal{D}^{L_n}$ denote the text attributes of node $n$, and $\\textrm{LM}$ be a pre-trained network, such as BERT~\\citep{devlin2018bert} or DeBERTa~\\citep{he2021deberta}. Then, the text attributes of node $n$ can be encoded by applying the LM to $s_n$ as follows:\n\\begin{equation}\n    h_n=\\textrm{LM}(s_n) \\in \\mathbb{R}^d,\n\\end{equation}\nwhere $h_n$ is the output of the LM, and $d$ is the dimension of the output vector.\n\nTo perform node classification, the output is employed as input to a classifier, such as a logistic regression or a neural network. The goal is to learn a function that maps the encoded text attributes to the corresponding node labels. % $f: \\mathbb{R}^d \\rightarrow Y$ \n\n% One significant advantage of using LMs for node classification is their capacity to capture intricate relationships between words and phrases in the text. Furthermore, pre-trained LMs are readily available and can be fine-tuned on a small amount of labeled data, making them a popular choice for node classification on TAGs.\n\n% \\paragraph{Large language models and prompting.}\n\\textbf{Large language models and prompting.}\nLLMs have introduced a new paradigm for task-adaptation known as ``pre-train, prompt, and predict'', replacing the traditional ``pre-train, fine-tune'' procedure. In this paradigm, the LLM is first pre-trained on a large corpus of text data to learn general language representations. Then, rather than fine-tuning the model on task-specific labeled data, the model is prompted with a natural language prompt that specifies the task and context, and the model generates the output directly based on the prompt and the input~\\citep{liu2023pre_survey}.\n\n\nThe prompt can take various forms, such as a single sentence or a longer passage, and can include additional information or constraints to guide the model's behavior. Let $\\mathcal{M}$ be an LLM that takes as input a sequence of tokens $x=(x_1,x_2,\\ldots,x_q)$ and produces as output a sequence of tokens $y=(y_1,y_2,\\ldots,y_m)$. The model $\\mathcal{M}$ is typically trained to optimize a conditional probability distribution $p(y|x)$, which assigns a probability to each possible output sequence $y$ given $x$. To include a prompt $p$ with the input sequence $x$, we can concatenate them into a new sequence $\\hat{x}=(p,x_1,x_2,\\ldots,x_q)$. We then use $\\hat{x}$ to compute the conditional probability distribution $p(y|\\hat{x})$. Formally, the probability of the output sequence $y$ given $\\hat{x}$ is:\n\\begin{equation}\np(y|\\hat{x})=\\prod_{i=1}^m p(y_i|y_{<i}, \\hat{x}),\n\\end{equation}\nwhere $y_{<i}$ represents the prefix of sequence $y$ up to position $i-1$, and $p(y_i | y_{<i}, \\hat{x})$ represents the probability of generating token $y_i$ given $y_{<i}$ and $\\hat{x}$. \n% The probability $p(y_i | y_{<i}, \\hat{x}, p)$ can be computed using the attention mechanism of the LLM, which can be used to identify the most relevant parts of the prompt and the input for generating each output token.\n\n\n% \\paragraph{Graph neural networks for node classification.}\n\\textbf{Graph neural networks for node classification.}\nIn node classification, the task is to label each node in a graph based on its attributes and connections with other nodes. GNNs operate by aggregating information from a node's neighbors, then updating the node's representation based on the aggregated information. Formally, the $k$-th layer of a GNN is designed as:\n\\begin{equation}\n    h_i^{k} = f^{k}(h_i^{k-1}, \\, \\textrm{AGG}(\\{h_j^{k-1}: j\\in \\mathcal{N}_i\\})) \\in \\mathbb{R}^d,\n\\end{equation}\nwhere $h_i^{k}\\in\\mathbb{R}^{d}$ is the representation of node $i$ at layer $k$ and $\\mathcal{N}_i \\subseteq \\mathcal{V}$ is the set of neighbors of node $i$. Function $f^{k}$ is a differentiable function that updates the representation of a node based on its previous-layer representation and the aggregated information from its neighbors. This function is typically implemented as a neural network layer (\\eg a multi-layer perceptron, or an attention mechanism). $\\textrm{AGG}$ is also a differentiable function (\\eg sum, mean, etc.) that aggregates the representations of a node's neighbors to produce a summary vector. The final representation is fed into a fully connected layer and a softmax function for class prediction.\n%The final hidden representation of a node is passed through a fully connected layer followed by a softmax function to obtain the node class prediction.\n\n\n\n"
            },
            "section 4": {
                "name": "Proposed Method",
                "content": "\n% \\vspace{-0.25cm}\nIn this section, we describe our LLM-based pipeline designed for node classification on TAGs. As illustrated in Figure~\\ref{fig: overview}, the key idea\nis to leverage the LLM's explanations as informative features for a downstream GNN. To achieve this goal, our method involves three main steps: 1) LLM-based prediction and explanation generation, 2) fine-tuning an LM interpreter, and 3) training a GNN.\n% 1) use a custom prompt to query LLMs to generate both a ranked prediction list and a textual explanation of its predictions; 2) fine-tune LMs on the original and auxiliary text attributes and transform them into node features; and 3) train a GNN on the enriched features. \n\n\n% We use the ChatGPT via OpenAI official API with backend model \u201cgpt-3.5-turbo\u201d. \n",
                "subsection 4.1": {
                    "name": "Generating Predictions and Explanations with LLMs",
                    "content": "\n\n%As discussed in our introduction, due to the size of LLMs, a key design goal of our approach is to be \\emph{LMaaS-compatible}, \\ie only requiring API access to an LLM, where the input and output of the LLM are in text form, but not requiring to fine-tune the LLM or to access its embeddings or logits.\nAs outlined in the introduction, our approach is designed to be \\emph{LMaaS-compatible} given the scale of LLMs. This means that we aim to operate solely through API access to an LLM, using text-based input and output, without requiring fine-tuning the LLM or accessing its embeddings or logits.\n\n% XH: In lieu of these, our idea is to query the LLM in an `open-ended' way, \\ie instructing it to make multiple predictions and to explain its decisions, to more effectively elicit its learned features and general knowledge in text form, which can be further processed using a \\emph{LLM-to-LM interpreter} to create informative node features for downstream GNNs. With this motivation, for each paper node $i \\in \\mathcal{V}$, we generate a prompt that includes the title and abstract of the paper and an open-ended question about the paper's topic. The specific phrasing of the question part of the prompt is tailored to the task and dataset, see Table~\\ref{tab: prompt}. The general prompt structure is as follows:\nIn lieu of these requirements, our approach focuses on querying the LLM in an ``open-ended'' manner, \\ie instructing the LLM to make multiple predictions and provide explanations for its decisions. By doing so, we aim to effectively extract its reasoning abilities and general knowledge in text format. These text-based outputs are then processed using an \\emph{LLM-to-LM interpreter} to create informative node features for downstream GNNs. With this objective, for each paper node $i \\in \\mathcal{V}$, we generate a prompt that includes the title and abstract of the paper, along with an open-ended question about the paper's topic. The specific phrasing of the question part of the prompt is tailored to the task and dataset, as shown in Table~\\ref{tab: prompt}. The general structure of the prompt is as follows:\n\n% [neurips] In lieu of these, our idea is to query the LLM in an `open-ended' way, \\ie instructing it to make multiple predictions and to explain its decisions, to more effectively elicit its learned features and general knowledge in text form, which can then be transformed into informative node features for downstream GNNs. With this motivation, for each paper node $i \\in \\mathcal{V}$, we generate a prompt that includes the title and abstract of the paper and an open-ended question about the paper's topic. The specific phrasing of the question part of the prompt is tailored to the task and dataset, see Table~\\ref{tab: prompt}. The general prompt format is as follows:\n\\begin{tcolorbox}% [width=10cm]\n\\textbf{Abstract:} [paper abstract]\\\\\n\\textbf{Title:} [paper title]\\\\\n\\textbf{Question:} [ask the model to predict one or more class labels of the paper, ordered from most to least likely, and provide explanations for its predictions]\\\\\n\\textbf{Answer:}\n\\end{tcolorbox}\n\nQuerying the LLM results in a ranked prediction list and a textual explanation for each paper:\n\\begin{tcolorbox}% [width=10cm]\n(\\textbf{Ranked Predictions}) [a ranked prediction list]\\\\\n(\\textbf{Explanations}) [model-generated explanation for the predictions]\n\\end{tcolorbox}\nThese predictions and explanations serve as supplementary text attributes for the downstream LMs and GNN models, as detailed in the subsequent section.\n% [neurips] We use these predictions and explanations as additional text attributes for the downstream LMs and GNN models, which we will describe in the next section.\n\n"
                },
                "subsection 4.2": {
                    "name": "Fine-Tuning LM Interpreter and Node Feature Extraction",
                    "content": "\n\n\\textbf{Original text and explanation features.} \n%XH: We first transform both the original text (\\ie title and abstract), and the LLM's explanations, into fixed-length node features for use in downstream GNNs. Our approach is to fine-tune a smaller LM which serves to `interpret' the text explanation of the LLM. The rationale for this is that the LLM and LM each have their strengths: the LLM is more powerful and has more knowledge, while LMs are small enough to be fine-tuned. Thus, the LM serves to `interpret' the LLM's output, with the text explanation  acting as an effective intermediate medium for communication. Then, fine-tuning the LM allows it to learn to extract the most useful and task-salient features from the explanation. We provide a theoretical analysis in the appendix~\\ref{theory}.\nOur initial step involves converting both the original text, \\ie title and abstract, and the LLM's explanations into fixed-length node features suitable for downstream GNN applications. Our approach is to fine-tune a smaller LM, which acts as an ``interpreter'' for the LLM's text explanations. The rationale behind this step is that both the LLM and LM possess distinct advantages: the LLM has greater power and more knowledge but is less flexible, while the LM has less skills but is compact enough to be fine-tuned to a specific task. Thus, the LM serves to interpret the LLM's output for the GNN, with the text explanation acting as an effective intermediate medium for communication. Then, fine-tuning the LM enables it to extract the most valuable and task-relevant features from the explanations. \n% \\xb{I MOVED THIS SENTENCE TO SECTION 5.3:\\sout{We explain that using explanations as features complements raw text features from a theoretical perspective in Appendix~\\ref{theory}.}} \n% Further theoretical analysis is available in Appendix~\\ref{theory}. \n\nConcretely, we first fine-tune pre-trained LMs as follows: let $\\textrm{LM}_{\\textrm{orig}}$ and $\\textrm{LM}_{\\textrm{expl}}$ be pre-trained LMs that take as input the original $s^{\\textrm{orig}}$ and the explanation $s^{\\textrm{expl}}$ text sequences, respectively. We obtain text embeddings for each source as follows:\n\\begin{equation}\n    \\begin{split}\n        h_{\\textrm{orig}} = \\textrm{LM}_{\\textrm{orig}}(s^{\\textrm{orig}})\\in \\mathbb{R}^{N\\times d}, \\quad \n        h_{\\textrm{expl}} = \\textrm{LM}_\\textrm{expl}(s^{\\textrm{expl}})\\in \\mathbb{R}^{N\\times d}.\n    \\end{split}\n\\end{equation}\nWe further apply a Multi-Layer Perceptron (MLP) to the output of the LMs to obtain a $N \\times C$-dimensional prediction matrix representing the LM's predictions for each node (in logits):\n\\begin{equation}\n    \\begin{split}\n        y_{\\textrm{orig}}= \\textrm{MLP}_{\\textrm{orig}}(h_{\\textrm{orig}}) \\in \\mathbb{R}^{N\\times C}, \\quad \n        y_{\\textrm{expl}}= \\textrm{MLP}_\\textrm{expl}(h_{\\textrm{expl}}) \\in \\mathbb{R}^{N\\times C}.\n    \\end{split}\n\\end{equation}\nWe fine-tune these LMs and MLPs using cross-entropy loss. Finally, the text embeddings from both sources, $h_{\\textrm{orig}}$ and $h_{\\textrm{expl}}$, are used as enriched features for training downstream GNNs.\n\n\\textbf{Ranked prediction features.}\n% XH: In addition to explanations, the LLM also provides a top-$k$ ranked prediction list for each node, which is also informative. To capture this, the top-$k$ predictions for node $i$ are first one-hot encoded as vectors $p_{i,1}, \\dots, p_{i,k} \\in \\mathbb{R}^{C}$, which are then concatenated into a $kC$-dimensional vector and linearly transformed into a fixed-sized vector of length $d_P$.  This process produces a prediction feature matrix $h_{\\textrm{pred}} \\in \\mathbb{R}^{N\\times d_P}$ across all nodes.\nIn addition to the explanations, the LLM also provides a top-$k$ ranked prediction list for each node, which adds valuable information. To incorporate this knowledge, the top-$k$ predictions for node $i$ are first one-hot encoded as vectors $p_{i,1}, \\dots, p_{i,k} \\in \\mathbb{R}^{C}$. These vectors are subsequently concatenated into a $kC$-dimensional vector, followed by a linear transformation to produce a fixed-sized vector of length $d_P$. This process produces a prediction feature matrix as $h_{\\textrm{pred}} \\in \\mathbb{R}^{N\\times d_P}$ across all nodes.\n\n% [neurips] In addition to the explanations, recall that the LLM also outputs its top-$k$ ranked prediction list (from most to least likely), which is also informative. To capture this, the top-$k$ predictions for node $i$ are first one-hot encoded as vectors $p_{i,1}, \\dots, p_{i,k} \\in \\mathbb{R}^{C}$, then concatenated into a $kC$-dimensional vector, then linearly encoded into a fixed-sized vector of length $d_P$. Across all nodes, this produces a prediction feature matrix $h_{\\textrm{pred}} \\in \\mathbb{R}^{N\\times d_P}$.\n\n% XH: In summary, our features are denoted as $h_\\textrm{TAPE} = \\{h_\\textrm{orig}, h_\\textrm{expl}, h_\\textrm{pred}\\}$, where `TAPE' stands for \\underline{T}itle, \\underline{A}bstract, \\underline{P}rediction and \\underline{E}xplanation of each node. Importantly, our framework only requires using these as \\emph{frozen} features for training downstream GNNs, ensuring that the LM (and LLM) do not participate in the GNN training process. This significantly improves the ease-of-use, modularity, and efficiency of our framework, compared to approaches like GLEM, which involve an expensive iterative training process between the LM and GNN, which explains our significant speedup over GLEM (\\eg $2.88\\times$ speedup on \\texttt{ogbn-arxiv}) even when using the same backbone LM and GNN. \n\n% XB: In summary, our features are denoted as $h_\\textrm{TAPE} = \\{h_\\textrm{orig}, h_\\textrm{expl}, h_\\textrm{pred}\\}$, where ``TAPE'' stands for \\underline{T}itle, \\underline{A}bstract, \\underline{P}rediction and \\underline{E}xplanation for each node. Importantly, our framework \n% only requires these features to be used in a \\emph{frozen} state during the training of downstream GNNs, ensuring that the LM and LLM do not participate in the GNN training process. This characteristic significantly improves the ease-of-use, modularity, and efficiency of our framework, compared to approaches like GLEM. GLEM involves an expensive iterative training process between the LM and GNN, which explains our substantial speedup over GLEM (\\eg a $2.88\\times$ speedup on \\texttt{ogbn-arxiv}) even when using the same backbone LM and GNN.\n\nIn summary, we denote our features as $h_\\textrm{TAPE} = \\{h_\\textrm{orig}, h_\\textrm{expl}, h_\\textrm{pred}\\}$, where ``TAPE'' stands for \\underline{T}itle, \\underline{A}bstract, \\underline{P}rediction and \\underline{E}xplanation for each node. Importantly, our framework requires these features to remain frozen during downstream GNN training, ensuring that the LM and LLM do not participate in the GNN training process. This characteristic significantly enhances ease-of-use, modularity, and efficiency compared to approaches like GLEM, which involve an expensive iterative LM-GNN training process. As a result, we achieve a substantial speedup over GLEM, \\eg a $2.88\\times$ speedup on \\texttt{ogbn-arxiv} even when utilizing the same backbone LM and GNN.\n\n\n\n\n% [neurips] In summary, we denote our features as $h_\\textrm{TAPE} = \\{h_\\textrm{orig}, h_\\textrm{expl}, h_\\textrm{pred}\\}$, representing the \\underline{T}itle, \\underline{A}bstract, \\underline{P}rediction and \\underline{E}xplanation of each node. Importantly, our framework only requires using these as \\emph{frozen} features for training downstream GNNs, so that the LM (and LLM) do not participate in the GNN training process. This significantly improves the ease-of-use and efficiency of our framework, compared to approaches like GLEM, which involve an expensive iterative training process between the LM and GNN, which explains our significant speedup over GLEM (\\eg $2.88\\times$ speedup on \\texttt{ogbn-arxiv}) even when using the same backbone LM and GNN. \n\n% Let $f_{\\textrm{pred}}$ be the GNN model trained on $h_{\\textrm{pred}}$:\n% \\begin{equation}\n%     \\hat y_{pred} = f_{pred}(h_{pred}, A) \\in \\mathbb{R}^{N\\times C}.\n% \\end{equation}\n\n"
                },
                "subsection 4.3": {
                    "name": "GNN Training on Enriched Features",
                    "content": "\n% XH: Finally, we aim to train a GNN on the $h_\\textrm{TAPE}$ features. We aim to avoid increasing the memory requirements of the GNN due to the feature combination, as well as avoiding changes to the GNN architecture. Thus, we use ensembling as a simple and effective way of combining these features: we separately train GNN models $f_{\\textrm{orig}}$, $f_{\\textrm{expl}}$, and $f_{\\textrm{pred}}$ on the features $h_{\\textrm{orig}}$, $h_{\\textrm{expl}}$, and $h_{\\textrm{pred}}$ respectively to predict the ground truth node labels:\n\nOur final step is to train a GNN using the $h_\\textrm{TAPE}$ features. We aim to achieve this without increasing the memory requirements of the GNN or making any changes to its architecture. To accomplish this, we use an ensemble approach, as a simple and effective way of combining the features. Specifically, we independently train GNN models $f_{\\textrm{orig}}$, $f_{\\textrm{expl}}$, and $f_{\\textrm{pred}}$ on the features $h_{\\textrm{orig}}$, $h_{\\textrm{expl}}$, and $h_{\\textrm{pred}}$, respectively, to predict the ground truth node labels:\n\\begin{equation}\n\\begin{split}\n\\hat y_{\\textrm{orig}/\\textrm{expl}/\\textrm{pred}} = f_{\\textrm{orig}/\\textrm{expl}/\\textrm{pred}}(h_{\\textrm{orig}/\\textrm{expl}/\\textrm{pred}}, A) \\in \\mathbb{R}^{N\\times C}.\n\\end{split}\n\\end{equation}\nWe then fuse these predictions by taking their average:\n% [neurips]Finally, we ensemble the output representations from each GNN model by taking their average:\n\\begin{equation}\n\\hat y = \\textrm{mean}(\\hat y_{\\textrm{orig}}, \\hat y_{\\textrm{expl}}, \\hat y_{\\textrm{pred}}) \\in \\mathbb{R}^{N\\times C}.\n\\end{equation}\n% [neurips] Each of the 3 models tends to perform reasonably well independently, as shown in Table~\\ref{tab: ablation}, so simple averaging works well. Meanwhile, this approach allows us to capture complementary information from different sources of input features and improve the overall performance of the model. \n% XH: Each of the three models performs well individually (as shown in Table~\\ref{tab: ablation}), making simple averaging effective. This approach enables us to capture complementary information from diverse input sources, enhancing overall model performance.\nEach of the three models performs well individually as shown in Table~\\ref{tab: ablation}, which validates the effectiveness of simple averaging. This strategy enables us to capture complementary information from diverse input sources, ultimately enhancing the overall model's performance.\n\n"
                },
                "subsection 4.4": {
                    "name": "Theoretical Analysis",
                    "content": "\\label{subsec: theorm}\nIn this section, we aim to demonstrate that explanations generated by an LLM can be valuable features for a smaller LM. Specifically, the explanations $E$ are helpful if they possess \\emph{fidelity} in describing the LLM's reasoning; and the LLM is \\emph{non-redundant}, utilizing information not used by the smaller LM. Let $E$ be the textual explanations generated by an LLM; $Z_L$ and $Z$ are embeddings from the LLM and smaller LM respectively, $y$ is the target and $H(\\cdot |\\cdot)$ is the conditional entropy. The detailed proof is in Appendix~\\ref{theory}. \n\n\\textbf{Theorem. }Given the following conditions 1) \\emph{Fidelity}: $E$ is a good proxy for $Z_L$ such that $H(Z_l | E)=\\epsilon$, with $\\epsilon>0$,  2) \\emph{Non-redundancy}: $Z_L$ contains information not present in $Z$, expressed as $H(y| Z, Z_L) = H(y|Z) - \\epsilon'$, with $\\epsilon'>\\epsilon$. Then it follows that $H(y | Z, E) < H(y | Z)$.\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\nWe evaluate TAPE on five TAG datasets: \\texttt{Cora}~\\citep{mccallum2000automating_cora}, \\texttt{PubMed}~\\citep{sen2008collective_pubmed}, \\texttt{ogbn-arxiv}, \\texttt{ogbn-products}~\\citep{hu2020open}, and \\texttt{tape-arxiv23}. \nFor \\texttt{Cora} and \\texttt{PubMed}, raw text data of the articles is unavailable in common graph libraries such as PyG and DGL. Hence, we  collected and formatted the missing text data for these datasets in TAG format. Additionally, given the popularity of these datasets, their TAG version will be released publicly for reproducibility and new research projects.  For \\texttt{ogbn-products}, given its substantial scale of 2 million nodes and 61 million edges and considering our academic resource budget, we conducted experiments on a subgraph sample. Details can be found in Appendix~\\ref{app sec: dataset}.\n\n\n%\\subsection{Main Results}\n\n\n\n",
                "subsection 5.1": {
                    "name": "Main Results",
                    "content": "\n% XH: We compare against GNN- and LM-based methods, and report the results in Table~\\ref{tab: performance}. Regarding GNNs, we select three widely used architectures: GCN~\\citep{kipf2016semi_gcn}, GraphSAGE~\\citep{sun2021scalable_sagn}, and RevGAT~\\citep{li2021training_revgat} \\xh{along with a basic MLP baseline that doesn't incorporate graph-related information.} For each GNN, we try different kinds of node features, including 1) OGB features~\\citep{hu2020open}, denoted as $h_\\textrm{OGB}$, 2) GIANT features~\\citep{chien2021node_giant} $h_\\textrm{GIANT}$, and 3) our proposed features $h_\\textrm{TAPE} = \\{h_\\textrm{orig}, h_\\textrm{expl}, h_\\textrm{pred}\\}$. For LMs, we experiment with two approaches: 1) Fine-tuning DeBERTa on labeled nodes, denoted as $\\textrm{LM}_\\textrm{finetune}$, and 2) Zero-shot ChatGPT (gpt-3.5-turbo) using the same prompts as our approach, denoted as $\\textrm{LLM}$. \n\n\n\nWe conduct a comprehensive evaluation of our proposed TAPE method by comparing with existing GNN- and LM-based methods, with the results summarized in Table~\\ref{tab: performance}. For GNN comparisons, we consider three widely utilized architectures:  GCN~\\citep{kipf2016semi_gcn}, GraphSAGE~\\citep{sun2021scalable_sagn}, and RevGAT~\\citep{li2021training_revgat} along with a basic MLP baseline that operates independently off graph-related information. We explore three types of node features: 1) shallow features (detailed in Table~\\ref{tab: text preprocess}), denoted as $h_\\textrm{shallow}$, 2) GIANT features~\\citep{chien2021node_giant} $h_\\textrm{GIANT}$, and 3) our proposed features $h_\\textrm{TAPE}$, comprising $h_\\textrm{orig}$, $h_\\textrm{expl}$, and $h_\\textrm{pred}$. For LM-based methods, we investigate two approaches: 1) fine-tuning DeBERTa on labeled nodes, denoted as $\\textrm{LM}_\\textrm{finetune}$, and 2) using zero-shot ChatGPT (gpt-3.5-turbo) with the same prompts as our approach, denoted as $\\textrm{LLM}$. \n\n\n% XH: Our approach yields the best results for both datasets and all downstream models. Regarding GNN-based methods, we observe that shallow features (\\ie $h_\\textrm{OGB}$) yield subpar performance. However, by incorporating LM-based features (\\ie $h_\\textrm{GIANT}$), we observe an improvement in performance. Our proposed features, which leverage the LLM, further enhance the results. In the case of LMs, fine-tuned LMs (\\ie $\\textrm{LM}_\\textrm{finetune}$) achieve competitive results, underscoring the importance of text attributes in a TAG setting. \n\n% XB: Our approach consistently outperforms other methods on all datasets and across all  models, demonstrating the effectiveness of our method in enhancing TAG representation learning. Among GNN-based methods, shallow features ($h_\\textrm{OGB}$) yields subpar performance, while LM-based features ($h_\\textrm{GIANT}$) leads to a noticeable improvement in performance. In the case of LMs, fine-tuned LMs (\\ie $\\textrm{LM}_\\textrm{finetune}$) achieve competitive results, emphasizing the importance of text attributes in enhancing TAG representation learning.\n% Our proposed novel features, leveraging the power of the LLM, further enhance the results.  \nOur approach consistently outperforms other methods on all datasets and across all  models, demonstrating its effectiveness in enhancing TAG representation learning. Among GNN-based methods, shallow features (\\ie $h_\\textrm{shallow}$) yields subpar performance, while LM-based features (\\ie $h_\\textrm{GIANT}$) improves results. In the case of LMs, fine-tuned LMs (\\ie $\\textrm{LM}_\\textrm{finetune}$) also perform well. Our proposed novel features, leveraging the power of the LLM, further enhance the results.  \n\nAdditionally, we expanded our experimentation to include the open-source Llama2~\\citep{touvron2023llama}, demonstrating the feasibility of a cost-effective (free) alternative, as shown in Table~\\ref{rebuttal tab: llama2}. Furthermore, to address the potential label leakage concern in LLM, we took the initiative to construct a novel dataset, namely \\texttt{tape-arxiv23}, comprising papers published in 2023 or later -- well beyond the knowledge cutoff for GPT-3.5. The results clearly illustrate strong generalization capabilities: while the LLM achieves 73.56\\% accuracy, our approach outperforms it with 84.23\\%. \n\n\n"
                },
                "subsection 5.2": {
                    "name": "Scalability",
                    "content": "\n\\vspace{-0.25cm}\n\nOur proposed method surpasses not only pure LMs and shallow embedding pipelines but also the LM-based pipelines on the \\texttt{ogbn-arxiv} dataset, achieving a superior balance between accuracy and training time, as illustrated in Figure~\\ref{fig: comparison}. Specifically, our method achieved significantly higher accuracy than the SOTA GLEM~\\citep{zhao2022learning_em} method while utilizing the same LM and GNN models. Furthermore, our approach requires only $2.88\\times$ less computation time. These efficiency improvements are attributed to our decoupled training approach for LMs and GNNs, avoiding the iterative (\\ie multi-stage) approach used in GLEM. Moreover, unlike the iterative approach, our model allows for parallelizing the training of LM$_\\textrm{orig}$ and LM$_\\textrm{expl}$, further reducing overall training time when performed simultaneously. \n\n\n\n\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Ablation Study",
                    "content": "\n% \\vspace{-0.25cm}\n% \\subsection{Ablation Study \\xb{and Theoretical Analysis}}\n\\label{subsec: ablation study}\nWe perform an ablation study on the \\texttt{ogbn-arxiv} dataset~\\citep{hu2020open} to evaluate the relevance of each module within our framework. The results are summarized in Table~\\ref{tab: ablation} and Figure~\\ref{fig: ablation2}. Across all methods and for both the validation and test sets, our proposed method consistently outperforms the other settings. This underscores the value of incorporating explanations and predictions into node embeddings. \n% Our case study (Figure~\\ref{fig: casestudy}) suggests this improvement can be attributed to the concise and focused nature of LLM-generated explanations, as well as their reasoning ability and utilization of external knowledge.\n\nWe provide time analysis and cost estimation in Appendix~\\ref{sec: time and money}, detail \\texttt{tape-arxiv23} dataset collection in Appendix~\\ref{app: arxiv2023}, use open-sourced llama as the LLM in Appendix~\\ref{sec: llama}, include a case study in Appendix~\\ref{sec: case study}, discuss prompt design in Appendix~\\ref{sec: prompt design}, examine LM finetuning effects in Appendix~\\ref{sec: effect of lm finetuning}, explore the impact of various LMs in Appendix~\\ref{sec: effect of different LMs}, and analyze memory usage in Appendix~\\ref{sec: memory}.\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\n\nGiven the increasing importance of integrating text and relationships, coupled with the emergence of LLMs, we foresee that TAG tasks will attract even more attention in the coming years. The convergence of LLMs and GNNs presents new opportunities for both research and industrial applications. As a pioneering work in this field, we believe that our contribution will serve as a strong baseline for future studies in this domain.\n\n\n\n\n\\textbf{Limitation and future work.}\nAn inherent limitation of our approach lies in the requirement for customized prompts for each dataset. Currently, we rely on manually crafted prompts, which may not be optimal for the node classification task for every dataset. The efficacy of these prompts may fluctuate depending on the specific characteristics of the dataset and the specific task at hand.\nFuture work can focus on automating the prompt generation process, exploring alternative prompt designs, and addressing the challenges of dynamic and evolving TAGs.\n\n"
            },
            "section 7": {
                "name": "Acknowledgment",
                "content": "\nBryan Hooi is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2023) (Grant A-8001996-00-00) and Xavier Bresson is supported by NUS Grant ID R-252-000-B97-133. The authors would like to express their gratitude to the reviewers for their feedback, which has improved the clarity and contribution of the paper.\n\n\n% It is important that the work published in ICLR is reproducible. Authors are strongly encouraged to include a paragraph-long Reproducibility Statement at the end of the main text (before references) to discuss the efforts that have been made to ensure reproducibility. This paragraph should not itself describe details needed for reproducing the results, but rather reference the parts of the main paper, appendix, and supplemental materials that will help with reproducibility. For example, for novel models or algorithms, a link to a anonymous downloadable source code can be submitted as supplementary materials; for theoretical results, clear explanations of any assumptions and a complete proof of the claims can be included in the appendix; for any datasets used in the experiments, a complete description of the data processing steps can be provided in the supplementary materials. Each of the above are examples of things that can be referenced in the reproducibility statement. This optional reproducibility statement will not count toward the page limit, but should not be more than 1 page.\n\n\n\n"
            },
            "section 8": {
                "name": "Reproducibility Statement",
                "content": " \n\n\nIn this statement, we provide references to the relevant sections and materials that will assist readers and researchers in replicating our results.\n\n%  XH : \\textbf{Theorem proof.} For a comprehensive understanding of the theorem presented in our work, please refer to Section~\\ref{subsec: theorm} of the main paper. A detailed proof of the theorem can be found in Appendix~\\ref{theory}.\n\n\\textbf{Theorem.} For a comprehensive understanding of the theorem presented in Section \\ref{subsec: theorm}, please refer to Appendix~\\ref{theory} for a detailed proof.\n\n\n\n\n\\textbf{Dataset description.} We summarize all datasets used in our study in Appendix~\\ref{app sec: dataset}, providing information on their sources and any necessary preprocessing steps. Additionally, for the newly introduced \\texttt{tape-arxiv23} dataset, we offer a detailed description of the data collection and processing steps in Appendix~\\ref{app: arxiv2023}.\n\n\\textbf{Open access to codes, datasets, trained models, and enriched features.} Our source code can be accessed at the following url: \\url{https://github.com/XiaoxinHe/TAPE}. Within this repository, we provide a script with step-by-step instructions on how to replicate the main results presented in our paper. Additionally, we offer download links for the \\texttt{Cora} and \\texttt{PubMed} datasets in TAG form, along with the new dataset \\texttt{tape-arxiv23}. These datasets can serve as valuable resources for the NLP and GNN research community. Furthermore, this repository includes the checkpoints for all trained models (.ckpt) and the TAPE features (.emb) used in our project, making it easy for researchers focusing on downstream GNN tasks to access enriched features.\n\n\n% Therefore, finding an automated or more adaptive way to generate prompts could be a valuable avenue for future research. \n\n% \\section{Submission of conference papers to ICLR 2024}\n\n% ICLR requires electronic submissions, processed by\n% \\url{https://openreview.net/}. See ICLR's website for more instructions.\n\n% If your paper is ultimately accepted, the statement {\\tt\n%   {\\textbackslash}iclrfinalcopy} should be inserted to adjust the\n% format to the camera ready requirements.\n\n% The format for the submissions is a variant of the NeurIPS format.\n% Please read carefully the instructions below, and follow them\n% faithfully.\n\n% \\subsection{Style}\n\n% Papers to be submitted to ICLR 2024 must be prepared according to the\n% instructions presented here.\n\n% %% Please note that we have introduced automatic line number generation\n% %% into the style file for \\LaTeXe. This is to help reviewers\n% %% refer to specific lines of the paper when they make their comments. Please do\n% %% NOT refer to these line numbers in your paper as they will be removed from the\n% %% style file for the final version of accepted papers.\n\n% Authors are required to use the ICLR \\LaTeX{} style files obtainable at the\n% ICLR website. Please make sure you use the current files and\n% not previous versions. Tweaking the style files may be grounds for rejection.\n\n% \\subsection{Retrieval of style files}\n\n% The style files for ICLR and other conference information are available online at:\n% \\begin{center}\n%    \\url{http://www.iclr.cc/}\n% \\end{center}\n% The file \\verb+iclr2024_conference.pdf+ contains these\n% instructions and illustrates the\n% various formatting requirements your ICLR paper must satisfy.\n% Submissions must be made using \\LaTeX{} and the style files\n% \\verb+iclr2024_conference.sty+ and \\verb+iclr2024_conference.bst+ (to be used with \\LaTeX{}2e). The file\n% \\verb+iclr2024_conference.tex+ may be used as a ``shell'' for writing your paper. All you\n% have to do is replace the author, title, abstract, and text of the paper with\n% your own.\n\n% The formatting instructions contained in these style files are summarized in\n% sections \\ref{gen_inst}, \\ref{headings}, and \\ref{others} below.\n\n% \\section{General formatting instructions}\n% \\label{gen_inst}\n\n% The text must be confined within a rectangle 5.5~inches (33~picas) wide and\n% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).\n% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the\n% preferred typeface throughout. Paragraphs are separated by 1/2~line space,\n% with no indentation.\n\n% Paper title is 17~point, in small caps and left-aligned.\n% All pages should start at 1~inch (6~picas) from the top of the page.\n\n% Authors' names are\n% set in boldface, and each name is placed above its corresponding\n% address. The lead author's name is to be listed first, and\n% the co-authors' names are set to follow. Authors sharing the\n% same address can be on the same line.\n\n% Please pay special attention to the instructions in section \\ref{others}\n% regarding figures, tables, acknowledgments, and references.\n\n\n% There will be a strict upper limit of 9 pages for the main text of the initial submission, with unlimited additional pages for citations. \n\n% \\section{Headings: first level}\n% \\label{headings}\n\n% First level headings are in small caps,\n% flush left and in point size 12. One line space before the first level\n% heading and 1/2~line space after the first level heading.\n\n% \\subsection{Headings: second level}\n\n% Second level headings are in small caps,\n% flush left and in point size 10. One line space before the second level\n% heading and 1/2~line space after the second level heading.\n\n% \\subsubsection{Headings: third level}\n\n% Third level headings are in small caps,\n% flush left and in point size 10. One line space before the third level\n% heading and 1/2~line space after the third level heading.\n\n% \\section{Citations, figures, tables, references}\n% \\label{others}\n\n% These instructions apply to everyone, regardless of the formatter being used.\n\n% \\subsection{Citations within the text}\n\n% Citations within the text should be based on the \\texttt{natbib} package\n% and include the authors' last names and year (with the ``et~al.'' construct\n% for more than two authors). When the authors or the publication are\n% included in the sentence, the citation should not be in parenthesis using \\verb|\\citet{}| (as\n% in ``See \\citet{Hinton06} for more information.''). Otherwise, the citation\n% should be in parenthesis using \\verb|\\citep{}| (as in ``Deep learning shows promise to make progress\n% towards AI~\\citep{Bengio+chapter2007}.'').\n\n% The corresponding references are to be listed in alphabetical order of\n% authors, in the \\textsc{References} section. As to the format of the\n% references themselves, any style is acceptable as long as it is used\n% consistently.\n\n% \\subsection{Footnotes}\n\n% Indicate footnotes with a number\\footnote{Sample of the first footnote} in the\n% text. Place the footnotes at the bottom of the page on which they appear.\n% Precede the footnote with a horizontal rule of 2~inches\n% (12~picas).\\footnote{Sample of the second footnote}\n\n% \\subsection{Figures}\n\n% All artwork must be neat, clean, and legible. Lines should be dark\n% enough for purposes of reproduction; art work should not be\n% hand-drawn. The figure number and caption always appear after the\n% figure. Place one line space before the figure caption, and one line\n% space after the figure. The figure caption is lower case (except for\n% first word and proper nouns); figures are numbered consecutively.\n\n% Make sure the figure caption does not get separated from the figure.\n% Leave sufficient space to avoid splitting the figure and figure caption.\n\n% You may use color figures.\n% However, it is best for the\n% figure captions and the paper body to make sense if the paper is printed\n% either in black/white or in color.\n% \\begin{figure}[h]\n% \\begin{center}\n% %\\framebox[4.0in]{$\\;$}\n% \\fbox{\\rule[-.5cm]{0cm}{4cm} \\rule[-.5cm]{4cm}{0cm}}\n% \\end{center}\n% \\caption{Sample figure caption.}\n% \\end{figure}\n\n% \\subsection{Tables}\n\n% All tables must be centered, neat, clean and legible. Do not use hand-drawn\n% tables. The table number and title always appear before the table. See\n% Table~\\ref{sample-table}.\n\n% Place one line space before the table title, one line space after the table\n% title, and one line space after the table. The table title must be lower case\n% (except for first word and proper nouns); tables are numbered consecutively.\n\n% \\begin{table}[t]\n% \\caption{Sample table title}\n% \\label{sample-table}\n% \\begin{center}\n% \\begin{tabular}{ll}\n% \\multicolumn{1}{c}{\\bf PART}  &\\multicolumn{1}{c}{\\bf DESCRIPTION}\n% \\\\ \\hline \\\\\n% Dendrite         &Input terminal \\\\\n% Axon             &Output terminal \\\\\n% Soma             &Cell body (contains cell nucleus) \\\\\n% \\end{tabular}\n% \\end{center}\n% \\end{table}\n\n% \\section{Default Notation}\n\n% In an attempt to encourage standardized notation, we have included the\n% notation file from the textbook, \\textit{Deep Learning}\n% \\cite{goodfellow2016deep} available at\n% \\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style\n% is not required and can be disabled by commenting out\n% \\texttt{math\\_commands.tex}.\n\n\n% \\centerline{\\bf Numbers and Arrays}\n% \\bgroup\n% \\def\\arraystretch{1.5}\n% \\begin{tabular}{p{1in}p{3.25in}}\n% $\\displaystyle a$ & A scalar (integer or real)\\\\\n% $\\displaystyle \\va$ & A vector\\\\\n% $\\displaystyle \\mA$ & A matrix\\\\\n% $\\displaystyle \\tA$ & A tensor\\\\\n% $\\displaystyle \\mI_n$ & Identity matrix with $n$ rows and $n$ columns\\\\\n% $\\displaystyle \\mI$ & Identity matrix with dimensionality implied by context\\\\\n% $\\displaystyle \\ve^{(i)}$ & Standard basis vector $[0,\\dots,0,1,0,\\dots,0]$ with a 1 at position $i$\\\\\n% $\\displaystyle \\text{diag}(\\va)$ & A square, diagonal matrix with diagonal entries given by $\\va$\\\\\n% $\\displaystyle \\ra$ & A scalar random variable\\\\\n% $\\displaystyle \\rva$ & A vector-valued random variable\\\\\n% $\\displaystyle \\rmA$ & A matrix-valued random variable\\\\\n% \\end{tabular}\n% \\egroup\n% \\vspace{0.25cm}\n\n% \\centerline{\\bf Sets and Graphs}\n% \\bgroup\n% \\def\\arraystretch{1.5}\n\n% \\begin{tabular}{p{1.25in}p{3.25in}}\n% $\\displaystyle \\sA$ & A set\\\\\n% $\\displaystyle \\R$ & The set of real numbers \\\\\n% $\\displaystyle \\{0, 1\\}$ & The set containing 0 and 1 \\\\\n% $\\displaystyle \\{0, 1, \\dots, n \\}$ & The set of all integers between $0$ and $n$\\\\\n% $\\displaystyle [a, b]$ & The real interval including $a$ and $b$\\\\\n% $\\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\\\\n% $\\displaystyle \\sA \\backslash \\sB$ & Set subtraction, i.e., the set containing the elements of $\\sA$ that are not in $\\sB$\\\\\n% $\\displaystyle \\gG$ & A graph\\\\\n% $\\displaystyle \\parents_\\gG(\\ervx_i)$ & The parents of $\\ervx_i$ in $\\gG$\n% \\end{tabular}\n% \\vspace{0.25cm}\n\n\n% \\centerline{\\bf Indexing}\n% \\bgroup\n% \\def\\arraystretch{1.5}\n\n% \\begin{tabular}{p{1.25in}p{3.25in}}\n% $\\displaystyle \\eva_i$ & Element $i$ of vector $\\va$, with indexing starting at 1 \\\\\n% $\\displaystyle \\eva_{-i}$ & All elements of vector $\\va$ except for element $i$ \\\\\n% $\\displaystyle \\emA_{i,j}$ & Element $i, j$ of matrix $\\mA$ \\\\\n% $\\displaystyle \\mA_{i, :}$ & Row $i$ of matrix $\\mA$ \\\\\n% $\\displaystyle \\mA_{:, i}$ & Column $i$ of matrix $\\mA$ \\\\\n% $\\displaystyle \\etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\\tA$\\\\\n% $\\displaystyle \\tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\\\\n% $\\displaystyle \\erva_i$ & Element $i$ of the random vector $\\rva$ \\\\\n% \\end{tabular}\n% \\egroup\n% \\vspace{0.25cm}\n\n\n% \\centerline{\\bf Calculus}\n% \\bgroup\n% \\def\\arraystretch{1.5}\n% \\begin{tabular}{p{1.25in}p{3.25in}}\n% % NOTE: the [2ex] on the next line adds extra height to that row of the table.\n% % Without that command, the fraction on the first line is too tall and collides\n% % with the fraction on the second line.\n% $\\displaystyle\\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\\\ [2ex]\n% $\\displaystyle \\frac{\\partial y} {\\partial x} $ & Partial derivative of $y$ with respect to $x$ \\\\\n% $\\displaystyle \\nabla_\\vx y $ & Gradient of $y$ with respect to $\\vx$ \\\\\n% $\\displaystyle \\nabla_\\mX y $ & Matrix derivatives of $y$ with respect to $\\mX$ \\\\\n% $\\displaystyle \\nabla_\\tX y $ & Tensor containing derivatives of $y$ with respect to $\\tX$ \\\\\n% $\\displaystyle \\frac{\\partial f}{\\partial \\vx} $ & Jacobian matrix $\\mJ \\in \\R^{m\\times n}$ of $f: \\R^n \\rightarrow \\R^m$\\\\\n% $\\displaystyle \\nabla_\\vx^2 f(\\vx)\\text{ or }\\mH( f)(\\vx)$ & The Hessian matrix of $f$ at input point $\\vx$\\\\\n% $\\displaystyle \\int f(\\vx) d\\vx $ & Definite integral over the entire domain of $\\vx$ \\\\\n% $\\displaystyle \\int_\\sS f(\\vx) d\\vx$ & Definite integral with respect to $\\vx$ over the set $\\sS$ \\\\\n% \\end{tabular}\n% \\egroup\n% \\vspace{0.25cm}\n\n% \\centerline{\\bf Probability and Information Theory}\n% \\bgroup\n% \\def\\arraystretch{1.5}\n% \\begin{tabular}{p{1.25in}p{3.25in}}\n% $\\displaystyle P(\\ra)$ & A probability distribution over a discrete variable\\\\\n% $\\displaystyle p(\\ra)$ & A probability distribution over a continuous variable, or over\n% a variable whose type has not been specified\\\\\n% $\\displaystyle \\ra \\sim P$ & Random variable $\\ra$ has distribution $P$\\\\% so thing on left of \\sim should always be a random variable, with name beginning with \\r\n% $\\displaystyle  \\E_{\\rx\\sim P} [ f(x) ]\\text{ or } \\E f(x)$ & Expectation of $f(x)$ with respect to $P(\\rx)$ \\\\\n% $\\displaystyle \\Var(f(x)) $ &  Variance of $f(x)$ under $P(\\rx)$ \\\\\n% $\\displaystyle \\Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\\rx)$\\\\\n% $\\displaystyle H(\\rx) $ & Shannon entropy of the random variable $\\rx$\\\\\n% $\\displaystyle \\KL ( P \\Vert Q ) $ & Kullback-Leibler divergence of P and Q \\\\\n% $\\displaystyle \\mathcal{N} ( \\vx ; \\vmu , \\mSigma)$ & Gaussian distribution %\n% over $\\vx$ with mean $\\vmu$ and covariance $\\mSigma$ \\\\\n% \\end{tabular}\n% \\egroup\n% \\vspace{0.25cm}\n\n% \\centerline{\\bf Functions}\n% \\bgroup\n% \\def\\arraystretch{1.5}\n% \\begin{tabular}{p{1.25in}p{3.25in}}\n% $\\displaystyle f: \\sA \\rightarrow \\sB$ & The function $f$ with domain $\\sA$ and range $\\sB$\\\\\n% $\\displaystyle f \\circ g $ & Composition of the functions $f$ and $g$ \\\\\n%   $\\displaystyle f(\\vx ; \\vtheta) $ & A function of $\\vx$ parametrized by $\\vtheta$.\n%   (Sometimes we write $f(\\vx)$ and omit the argument $\\vtheta$ to lighten notation) \\\\\n% $\\displaystyle \\log x$ & Natural logarithm of $x$ \\\\\n% $\\displaystyle \\sigma(x)$ & Logistic sigmoid, $\\displaystyle \\frac{1} {1 + \\exp(-x)}$ \\\\\n% $\\displaystyle \\zeta(x)$ & Softplus, $\\log(1 + \\exp(x))$ \\\\\n% $\\displaystyle || \\vx ||_p $ & $\\normlp$ norm of $\\vx$ \\\\\n% $\\displaystyle || \\vx || $ & $\\normltwo$ norm of $\\vx$ \\\\\n% $\\displaystyle x^+$ & Positive part of $x$, i.e., $\\max(0,x)$\\\\\n% $\\displaystyle \\1_\\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\\\\n% \\end{tabular}\n% \\egroup\n% \\vspace{0.25cm}\n\n\n\n% \\section{Final instructions}\n% Do not change any aspects of the formatting parameters in the style files.\n% In particular, do not modify the width or length of the rectangle the text\n% should fit into, and do not change font sizes (except perhaps in the\n% \\textsc{References} section; see below). Please note that pages should be\n% numbered.\n\n% \\section{Preparing PostScript or PDF files}\n\n% Please prepare PostScript or PDF files with paper size ``US Letter'', and\n% not, for example, ``A4''. The -t\n% letter option on dvips will produce US Letter files.\n\n% Consider directly generating PDF files using \\verb+pdflatex+\n% (especially if you are a MiKTeX user).\n% PDF figures must be substituted for EPS figures, however.\n\n% Otherwise, please generate your PostScript and PDF files with the following commands:\n% \\begin{verbatim}\n% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps\n% ps2pdf mypaper.ps mypaper.pdf\n% \\end{verbatim}\n\n% \\subsection{Margins in LaTeX}\n\n% Most of the margin problems come from figures positioned by hand using\n% \\verb+\\special+ or other commands. We suggest using the command\n% \\verb+\\includegraphics+\n% from the graphicx package. Always specify the figure width as a multiple of\n% the line width as in the example below using .eps graphics\n% \\begin{verbatim}\n%    \\usepackage[dvips]{graphicx} ...\n%    \\includegraphics[width=0.8\\linewidth]{myfile.eps}\n% \\end{verbatim}\n% or % Apr 2009 addition\n% \\begin{verbatim}\n%    \\usepackage[pdftex]{graphicx} ...\n%    \\includegraphics[width=0.8\\linewidth]{myfile.pdf}\n% \\end{verbatim}\n% for .pdf graphics.\n% See section~4.4 in the graphics bundle documentation (\\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})\n\n% A number of width problems arise when LaTeX cannot properly hyphenate a\n% line. Please give LaTeX hyphenation hints using the \\verb+\\-+ command.\n\n% \\subsubsection*{Author Contributions}\n% If you'd like to, you may include  a section for author contributions as is done\n% in many journals. This is optional and at the discretion of the authors.\n\n% \\subsubsection*{Acknowledgments}\n% Use unnumbered third level headings for the acknowledgments. All\n% acknowledgments, including those to funding agencies, go at the end of the paper.\n\n\n% \\newpage\n\\bibliography{iclr2024_conference}\n\\bibliographystyle{iclr2024_conference}\n\n\\newpage\n\\appendix\n% \\section{Appendix}\n% You may include other additional sections here.\n"
            },
            "section 9": {
                "name": "Theoretical Analysis",
                "content": "\\label{theory}\n\nIn this section, we aim to demonstrate that explanations generated by an LLM can provide valuable features for another model (such as a smaller LM). This is true under two key conditions:\n\\begin{enumerate}\n\\item \\emph{Fidelity:} The explanations effectively represent LLM's reasoning over the raw text, containing most of the information from the LLM's hidden state.\n\\item \\emph{Non-redundancy:} The LLM possesses unique knowledge not captured by another model.\n\\end{enumerate}\n\n\nWe formulate our theorem as follows:\n\\begin{theorem}\nGiven the following conditions: \n\n1) Fidelity: $E$ is a good proxy for $Z_L$ such that\n\\begin{equation}\n    H(Z_l | E)=\\epsilon, {\\quad \\epsilon>0}\n\\end{equation}\n\n2) Non-redundancy: $Z_L$ contains information not present in $Z$, expressed as\n\\begin{equation}\n    H(y| Z, Z_L) = H(y|Z) - \\epsilon', \\quad \\epsilon'>\\epsilon\n\\end{equation}\n% Let $E$ be a proxy for $Z_L$, where the conditional entropy $H(Z_L | E) = \\epsilon$. Furthermore, let $Z_L$ contain information not present in $Z$, expressed as $H(y | Z, Z_L) = H(y | Z) - \\epsilon'$, with $\\epsilon' > \\epsilon$. \n\nThen, it follows that:\n\\begin{equation}\nH(y | Z, E) < H(y | Z)\n\\end{equation}\n\nwhere $E$ is textual explanations generated by an LLM, $Z_L$ is the vectorial representation of the raw text modeled by the LLM, $Z$ is the vectorial representation of the raw text modeled by the other model, $y$ is the target and $H(\\cdot |\\cdot)$ is the conditional entropy.\n\\end{theorem}\n\n\\begin{proof}\n\nWe aim to demonstrate that the conditional entropy of $y$ given both $Z$ and $E$, denoted as $H(y | Z, E)$, is less than the conditional entropy of $y$ given only $Z$, denoted as $H(y | Z)$. \n\nStarting with:\n\\begin{equation}\nH(y | Z, E)\n\\end{equation}\n\nWe apply the properties of entropy to decompose this expression into two components:\n\n\\begin{equation}\nH(y | Z, E) = H(y | Z, Z_L, E) + I(y; Z_L | Z, E) \\label{eq:decomp}\n\\end{equation}\n\n% The first term on the right-hand side is the conditional entropy of $y$ given $Z$, $Z_L$, and $E$, while the second term is the mutual information between $y$ and $Z_L$ given $Z$ and $E$.\n\nNow, we utilize the following upper bound of conditional mutual information:\n\\begin{align}\nI(y; Z_L | Z, E) &= H(Z_L | Z, E) - H(Z_L | y, Z, E) \\\\\n& \\leq H(Z_L | Z, E) \\label{eq:bound}\n\\end{align}\nwhere the first line follows from the definition of mutual information, and the second line follows from the nonnegativity of conditional entropy.\n\nSubstituting \\eqref{eq:bound} into \\eqref{eq:decomp}, we rewrite the conditional entropy as:\n\\begin{equation}\nH(y | Z, E) \\leq H(y | Z, Z_L, E) + H(Z_L | Z, E)\n\\end{equation}\n\nSince conditional entropy increases when conditioning on fewer variables, we further have:\n\n\\begin{equation}\nH(y | Z, Z_L, E) + H(Z_L | Z, E) \\leq H(y | Z, Z_L) + H(Z_L | E)\n\\end{equation}\n\nApplying the \"Fidelity\" and \"Non-redundancy\" conditions:\n\n\\begin{equation}\nH(y | Z, Z_L) + H(Z_L | E) \\leq H(y | Z) - \\epsilon' + \\epsilon\n\\end{equation}\n\nFinally, as $\\epsilon' > \\epsilon$, we have:\n\n\\begin{equation}\nH(y | Z) - \\epsilon' + \\epsilon < H(y | Z)\n\\end{equation}\n\nConsequently, we have proven that:\n\n\\begin{equation}\nH(y | Z, E) < H(y | Z)\n\\end{equation}\n\nThis completes the proof.\n\\end{proof}\n\n\n"
            },
            "section 10": {
                "name": "time analysis and money estimation",
                "content": "\n\\label{sec: time and money}\nOur primary dataset, \\texttt{ogbn-arxiv}, with 169,343 nodes and 1,166,243 edges, serves as a representative case for our approach. On average, our input sequences consist of approximately 285 tokens, while the output sequences comprise around 164 tokens. For the ChatGPT-3.5 Turbo API, priced at \\$0.0015 per 1,000 input tokens and \\$0.002 per 1,000 output tokens, with a token per minute rate limit of 90,000, the monetary estimation for \\texttt{ogbn-arxiv} is as follows:\n\\begin{equation}\n\\textit{Cost} = ((285 \\times 0.0015) / 1000 + (164 \\times 0.002) / 1000) \\times 169,343 \\approx 128 \\,\\textit{USD}\n\\end{equation}\n\nConsidering the token rate limit, we estimate the deployment time as follows:\n\\begin{equation}\n\\textit{Time} = 169,343/ (90,000/285) \\approx 536 \\textit{min} \\approx 9\\textit{h}\n\\end{equation}\n\n\\textbf{Cost-Effective Alternatives.} Additionally, we have explored cost-effective alternatives, such as leveraging open-source LLMs like llama2. The use of llama2 is entirely free, and the querying process to llama2-13b-chat takes approximately 16 hours when utilizing 4 A5000 GPUs. \n\n\\textbf{Efficiency through Single Query and Reuse.} Our method requires only one query to the LLM, with predictions and explanations stored for subsequent use. This not only enhances efficiency but also minimizes the number of API calls, contributing to cost-effectiveness. We also release the gpt responses for public use.\n\n"
            },
            "section 11": {
                "name": "Addressing Label Leakage Concerns with a New Dataset",
                "content": "\\label{app: arxiv2023}\n\nGPT-3.5's training data might include certain arXiv papers, given its comprehensive ingestion of textual content from the internet. However, the precise composition of these arXiv papers within GPT-3.5's training remains undisclosed, rendering it infeasible to definitively identify their inclusion. It is essential to emphasize that the challenge of label leakage is widespread and affects various language model benchmarks, such as the prominent BIG-bench~\\citep{srivastava2022beyond_bigbench} and TruthfulQA~\\citep{lin2021truthfulqa}.\n\nTo address this concern, we created a novel dataset \\texttt{tape-arxiv23} for our experiments. We made sure that this dataset only included papers published in 2023 or later, which is well beyond the knowledge cutoff for GPT-3.5, as it was launched in November 2022. The creation of this new dataset was meticulously executed. We collected all cs.ArXiv papers published from January 2023 to September 2023 from the arXiv daily repository~\\footnote{\\url{https://arxiv.org/}}. We then utilized the Semantic Scholar API~\\footnote{\\url{https://www.semanticscholar.org/product/api}} to retrieve citation relationships. This process yielded a comprehensive graph containing 46,198 papers and 78,548 connections. Our codes to collect and build the dataset is available at: \\url{https://github.com/XiaoxinHe/tape_arxiv_2023}.\n\n\n"
            },
            "section 12": {
                "name": "Llama as a cost-efficient alternative",
                "content": "\n\\label{sec: llama}\nWe extend out experiment to the open-source LLM \"llama-2-13b-chat\"  (llama for short), which demonstrates the feasibility of a cost-effective (free) alternative, see Table~\\ref{rebuttal tab: llama2}.\n\nIt is worth noting that although llama exhibits a lower performance compared to GPT-3.5 in terms of both zero-shot accuracy and explanation quality, our pipeline still maintains its robust performance. As an illustration, we achieved an accuracy of 76.19\\% on the \\texttt{ogbn-arxiv} dataset using llama, slightly below the 77.50\\% achieved with GPT-3.5. We attribute this impressive level of generalization to the complementary nature of the explanations themselves, which serve as a rich source of semantic information supplementing the original text such as title and abstract.\n\n\n\n\n"
            },
            "section 13": {
                "name": "Case Study",
                "content": "\n\\label{sec: case study}\n\nTo investigate the impact of using explanations as features in improving node classification on TAGs, we conduct an analysis on predicted samples from the \\texttt{PubMed} dataset. Figure~\\ref{fig: casestudy} presents a case where the GNN model trained with original text attributes as features incorrectly predicts the label for node 12390 (as experimentally induced diabetes), while the model trained with explanations generated by LLMs as features correctly predicts the label (as type 2 diabetes).\n\n\n\nThis improvement can be attributed to two main factors. Firstly, compared to the original text attributes, which consist of the title and abstract text, the explanations generated by the LLM are more concise and focused. This aids the subsequent LM in generating node embeddings that capture the essential semantics without the need to compress an excessive amount of information into a fixed-length representation. Secondly, LLMs possess reasoning capabilities and the ability to leverage general knowledge, which prove crucial in achieving accurate predictions. For instance, the explanations generated by LLMs explicitly link type 2 diabetes to MKR mice and db/db mice (which are common animal models of type 2 diabetes), as well as the insulinopenic mice / streptozotocin to experimentally induced diabetes. This knowledge is either absent or only implicitly specified in the original text attributes.\n\n"
            },
            "section 14": {
                "name": "Prompt Design",
                "content": "\n\\label{sec: prompt design}\n\nTable~\\ref{tab: prompt} outlines the prompts used for various datasets. Each prompt includes the abstract and title of the paper, followed by a task-specific question. The question is formulated to query the model about a particular aspect of the paper and request an explanation for the prediction. The answer section is left blank for the model to fill in. Generally, our analysis finds that the current instructions allow the LLM to produce output that conforms well to the expected format without significant deviations, allowing the answers to be straightforwardly extracted from the text output of the LLM. \n\n\n\n\n\\paragraph{Exploring Prompt Variations.}\n\nWe have extensively explored the influence of various prompts on the \\texttt{ogbn-arxiv} dataset, as outlined in Table~\\ref{tab: prompt experiment} and Table~\\ref{tab: prompt design}.\n\nTable~\\ref{tab: prompt experiment} indicates that, generally, most prompts yield similar performance. However, a minor performance improvement is observed when the title is positioned after the abstract. This finding aligns with the principle suggested by \\cite{zhao2021calibrate} that placing more critical information later in the prompt can be beneficial.\n\nFurther analysis presented in Table~\\ref{tab: prompt design} demonstrates a positive correlation between the LLM's zero-shot accuracy and the overall accuracy of our method, implying that higher zero-shot prediction scores lead to enhanced TAPE accuracy. Despite the variation in prompt designs, our methodology consistently achieves similar accuracy levels, ranging from 0.7660 to 0.7750 with the RevGAT as the GNN backbone. This consistency underscores the robustness of our proposed TAPE to different prompt configurations.\n\n\n\n\n\n\n\n\n\n"
            },
            "section 15": {
                "name": "Dataset",
                "content": "\\label{app sec: dataset}\n\nWe conduct experiments on five TAGs -- \\texttt{Cora}~\\citep{mccallum2000automating_cora}, \\texttt{PubMed}~\\citep{sen2008collective_pubmed}, \\texttt{ogbn-arxiv}, \\texttt{ogbn-products}~\\citep{hu2020open}, and \\texttt{tape-arxiv23}.  \nFor \\texttt{Cora} and \\texttt{PubMed}, we collected the raw text data since they are not available in common repositories like PyG and DGL. For \\texttt{ogbn-products}, given its substantial scale of 2 million nodes and 61 million edges, we have employed a node sampling strategy to obtain a subgraph containing 54k nodes and 74k edges. Additionally, we introduced the \\texttt{tape-arxiv23} citation graph dataset, extending beyond the knowledge cutoff of GPT-3. This dataset serves as a valuable resource for the research community. Table~\\ref{tab: dataset} provides a summary of the dataset statistics.\n\n\n\n\n",
                "subsection 15.1": {
                    "name": "Dataset Description",
                    "content": "\n\\textbf{Cora~\\citep{mccallum2000automating_cora}.}\nThe \\texttt{Cora} dataset comprises 2,708 scientific publications classified into one of seven classes -- \ncase based, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory, with a citation network consisting of 5,429 links. The papers were selected in a way such that in the final corpus every paper cites or is cited by at least one other paper. \n\n\n\\textbf{PubMed~\\citep{sen2008collective_pubmed}.}\nThe Pubmed dataset consists of 19,717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes -- Experimental induced diabetes, Type 1 diabetes, \n and Type 2 diabetes. The citation network consists of 44,338 links. \n\n\\textbf{ogbn-arxiv~\\citep{hu2020open}.}\nThe \\texttt{ogbn-arxiv} dataset is a directed graph that represents the citation network between all computer science arXiv papers indexed by MAG~\\citep{wang2020microsoft_mag}. Each node is an arXiv paper, and each directed edge indicates that one paper cites another one. The task is to predict the 40 subject areas of arXiv CS papers, \\eg, cs.AI, cs.LG, and cs.OS, which are manually determined (\\ie labeled) by the paper\u2019s authors and arXiv moderators. \n\n\\textbf{ogbn-products~\\citep{hu2020open}.}\nThe \\texttt{ogbn-products} dataset represents an Amazon product co-purchasing network, with product descriptions as raw text. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels.\n\n\\textbf{tape-arxiv23}. \nThe \\texttt{tape-arxiv23} dataset is a directed graph that represents the citation network between all computer science arXiv papers published in 2023 or later. Similar to \\texttt{ogbn-arxiv}, each node is an arXiv paper, and each directed edge indicates that one paper cites another one. The task is to predict the 40 subject areas of arXiv CS papers, \\eg, cs.AI, cs.LG, and cs.OS, which are manually determined (\\ie labeled) by the paper\u2019s authors and arXiv moderators. \n\n\n\n\n"
                },
                "subsection 15.2": {
                    "name": "Dataset splits and random seeds",
                    "content": "\nIn our experiments, we adhered to specific dataset splits and employed random seeds for reproducibility.\nFor the \\texttt{ogbn-arxiv} and \\texttt{ogbn-products} dataset, we adopted the standard train/validation/test split provided by OGB~\\citep{hu2020open}.\nAs for the \\texttt{Cora}, \\texttt{PubMed} datasets, and \\texttt{tape-arxiv23}, we performed the train/validation/test splits ourselves, where 60\\% of the data was allocated for training, 20\\% for validation, and 20\\% for testing. Additionally, we utilized random seeds to ensure the reproducibility of our experiments, enabling the consistent evaluation of our proposed method on the respective datasets, which can be found in our linked code repository.\n\n\n"
                },
                "subsection 15.3": {
                    "name": "Shallow Embedding Methods for Node Feature Extraction",
                    "content": "\n\nTable~\\ref{tab: text preprocess} provides an overview of the text preprocessing and feature extraction methods commonly used in graph libraries such as PyG and DGL, which are widely adopted in GNN research.\n\n\n\nThese text preprocessing and feature extraction methods facilitate the extraction of node features from the text attributes of TAG datasets, enabling the utilization of GNN models for node classification tasks. While these methods are easy to apply and computationally efficient, it is important to note that they rely on traditional language modeling techniques that may not capture the full semantic meaning in the text. This limitation can impact the expressiveness of the extracted node features and potentially affect the development of techniques for downstream tasks.\n\n\n"
                }
            },
            "section 16": {
                "name": "Experiment Details",
                "content": "\\label{app sec: experiment}\n\n",
                "subsection 16.1": {
                    "name": "Computing Environment and Resources",
                    "content": "\nThe implementation of the proposed method utilized the PyG and DGL modules, which are licensed under the MIT License. The experiments were conducted in a computing environment with the following specifications: LM-based experiments were performed on four NVIDIA RTX A5000 GPUs, each with 24GB VRAM. On the other hand, the GNN-based experiments were conducted on a single GPU.\n\n"
                },
                "subsection 16.2": {
                    "name": "Hyperparameters",
                    "content": "\n\nTable~\\ref{tab: hyperparam} provides an overview of the hyperparameters used for the GCN~\\citep{kipf2016semi_gcn}, SAGE~\\citep{hamilton2017inductive_sage}, and RevGAT~\\citep{li2021training_revgat} models. These hyperparameters were selected based on the official OGB repository~\\footnote{\\url{https://github.com/snap-stanford/ogb}}, and the RevGAT and language model hyperparameters follow those used in the GLEM repository~\\footnote{\\url{https://github.com/AndyJZhao/GLEM}}. It is important to note that these hyperparameters were not tuned on a per-dataset basis, but instead were used consistently across all three TAG datasets based on those from prior work, and also set consistently across both our proposed method and the baselines. This demonstrates the generality and ease of use of our method, as well as its compatibility with existing GNN baselines.\n\n\n\n\n\n"
                },
                "subsection 16.3": {
                    "name": "Detailed Ablation Study",
                    "content": "\\label{subsec: ablation2}\n\nWe conducted a detailed ablation study on the \\texttt{ogbn-arxiv} dataset to assess the impact of different sources of node features. The study focused on three types of node features: original text features ($h_\\textrm{orig}$), explanation as features ($h_\\textrm{expl}$), and predictions as features ($h_\\textrm{pred}$). We systematically removed one of these features at a time while keeping the other components unchanged in our model.\n\n\n\n\nThe results of the ablation study are illustrated in Figure~\\ref{fig: ablation2}. The figure presents the performance of the model when each type of node feature is removed. It is observed that using the full set of features yields the best performance, while leaving out any of the features leads to a drop in performance. However, the extent of the performance drop may vary depending on the specific GNN model used.\n\nThis ablation study provides additional insights to complement the findings presented in section~\\ref{subsec: ablation study}. While Table~\\ref{tab: ablation} compared the performance of using the full set of features versus using just one of them, this ablation study specifically focuses on comparing the performance of using the full set of features versus leaving one of them out. Although the experimental design differs, the overall message conveyed remains consistent, emphasizing the significance of considering all the various sources of node features for achieving optimal performance in node classification tasks.\n\n\n\n\n% \\subsection{A new dataset to address the issue of label leakage}\n% \\xh{\n% It is possible that training data might include certain arXiv papers, given its comprehensive ingestion of textual content from the internet. However, the precise composition of these arXiv papers within GPT-3.5's training remains undisclosed, rendering it infeasible to definitively identify their inclusion. It is essential to emphasize that the challenge of label leakage is widespread and affects various language model benchmarks, such as the prominent BIG-bench~\\citep{srivastava2022beyond_bigbench} and TruthfulQA~\\citep{lin2021truthfulqa}.\n\n% To address this concern, we created a completely new dataset for our experiments. We made sure that this dataset only included papers published in 2023 or later, which is well beyond the knowledge cutoff for GPT-3.5, as it was launched in November 2022, and its official knowledge cut-off date is September 2021. The creation of this new dataset was meticulously executed, starting with a set of random cs.ArXiv papers published after 2023. Subsequently, we expanded this set by adding papers connected within three hops, all of which were also published after 2023. This process resulted in a graph containing 2,329 papers and 6,964 connections.\n\n% As shown in Table~\\ref{rebutal tab: new arxiv dataset}, the outcomes obtained from this new dataset convincingly demonstrate the ability of both the LLM (71.12\\% accuracy) and our approach (83.14\\%) to generalize effectively beyond their training data. This highlights the robustness of our approach and showcases its practical applicability in real-world scenarios.\n\n\n% }\n\n\n% \\begin{table}[!ht]\n\n% \\caption{{\n% Node classification accuracy on a novel dataset comprising papers intentionally published post the GPT-3.5 launch in November 2022, specifically within the year 2023.}\n% }\n% \\small\n%     \\label{rebutal tab: new arxiv dataset}\n%     \\centering\n%     \\begin{tabular}{ccccc}\n%     \\toprule\n%         LLM \n%         & LM$_{\\textrm{finetune}}$ \n%         & $h_\\textrm{TAPE}$ + GCN\n%         & $h_\\textrm{TAPE}$ + SAGE\n%         & $h_\\textrm{TAPE}$ + RevGAT\\\\\n%     \\midrule\n%     0.7112 \n%     & 0.6542 \u00b1 0.0294\n%     & 0.7495 \u00b1 0.0476 \n%     & 0.8001 \u00b1 0.0565\n%     & \\textbf{0.8314 \u00b1 0.0669} \\\\\n%     \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n% \\subsection{ogbn-products}\n% \\xh{We have expanded our experimentation to include the \\texttt{ogbn-products} dataset, which represents an Amazon product co-purchasing network, with product descriptions as raw text.\n\n% Given the substantial scale of the ogbn-products dataset for academic GPUs of 2 million nodes and 61 million edges, we have employed a node sampling strategy to obtain a subgraph containing 5.4k nodes and 7.4k edges. Please refer to Table~\\ref{rebuttal tab: products} in the attached PDF for further details. The results obtained for this new dataset demonstrate the effectiveness of our method in a different TAG scenario, showing its potential in more complex situations.\n% }\n\n% \\begin{table}[!ht]\n\n% \\caption{Node classification accuracy of \\texttt{ogbn-products}  (subset) using three different GNN models and two LM baselines (LLM and LM$_\\textrm{finetune}$). $G\\uparrow$ denotes the improvements of our approach over the same GNN trained on $h_\\textrm{OGB}$; $L\\uparrow$ denotes the improvements of our approach over $LM_\\textrm{finetune}$. The results are averaged over four runs with different seeds, and the best results are \\textbf{in bold}.}\n% % \\caption{{Node classification accuracy for a subset of the \\texttt{ogbn-products} dataset.}}\n% \\scriptsize\n%     \\label{rebuttal tab: products}\n%     \\centering\n%     \\begin{tabular}{p{0.75cm}ccccccc}\n%     \\toprule\n%      \\multirow{2}{*}{Method}\n%     & \\multicolumn{3}{c}{GNN} \n%     & \\multicolumn{3}{c}{LM}\n%     & \\multicolumn{1}{c}{Ours}\\\\\n%     \\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\\cmidrule(lr){8-8}\n%     &$h_{\\textrm{OGB}}$ & $h_{\\textrm{GIANT}}$ & $G\\uparrow$& LLM & LM$_{\\textrm{finetune}}$ & $L\\uparrow$ & $h_{\\textrm{TAPE}}$ \n%     \\\\\n%     \\midrule\n    \n%      {GCN}\n%     & {0.7052 \u00b1 0.0051}\n%     & {--}\n%     & {13.39\\%}\n%     & {0.7440}\n%     & {0.7297 \u00b1 0.0023}\n%     & {9.58\\%}\n%     & {0.7996 \u00b1 0.0041}\n%     \\\\\n%      {SAGE}\n%     & {0.6913 \u00b1 0.0026}\n%     & {--}\n%     & {17.71\\%}\n%     & {0.7440}\n%     & {0.7297 \u00b1 0.0023}\n%     & {11.51\\%}\n%     & {0.8137 \u00b1 0.0043}\n%     \\\\\n%      RevGAT\n%     & 0.6964 \u00b1 0.0017\n%     & --\n%     & 18.24\\%\n%     & 0.7440\n%     & 0.7297 \u00b1 0.0023\n%     & 12.84\\%\n%     & \\textbf{0.8234 \u00b1 0.0036}\n%     \\\\\n    \n%     \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n\n\n\n\n"
                }
            },
            "section 17": {
                "name": "Effect of LM Finetuning",
                "content": "\n\\label{sec: effect of lm finetuning}\nWe conduct an ablation study on \\texttt{ogbn-arxiv} to explore the impact of language model (LM) fine-tuning. Specifically, we aim to address the following research questions (RQs):\n\\begin{itemize}\n    \\item RQ1: Is fine-tuning the LM necessary?\n    \\item RQ2: Is it necessary to use different LMs for encoding  the original text and explanations?\n\\end{itemize}\n\nTo address these questions, we examine three settings: 1) Without Fine-Tuning: Utilizing a pre-trained LM to encode the original text and the explanations without any fine-tuning.\n2) {Fine-Tuning (Same LM):} Fine-tuning a single LM for both the original text and the explanations. 3) {Fine-Tuning (Different LMs):} Fine-tuning two separate LMs, one for the original text and another for the explanations.\n\n\n\n\nOur observations include:\n\nFor RQ1: Table~\\ref{tab: lm_finetuning} underscores the importance of fine-tuning the LM. It reveals a marked decline in performance without fine-tuning, compared with the settings where the LM is fine-tuned.\n\nFor RQ2: Fine-tuning, whether with the same LM or with different LMs, yields similar outcomes, with a slight advantage for using two distinct LMs. However, the marginal difference suggests that our approach could be simplified and expedited by utilizing a single LM.\n\n\n"
            },
            "section 18": {
                "name": "Effect of different LMs",
                "content": "\n\\label{sec: effect of different LMs}\nTo access the influence of different LMs, we expand our investigation beyond deberta-base. Specifically, following the approach taken in SimTAG~\\citep{duan2023simteg}, we include two additional widely-used LMs from the MTEB~\\citep{muennighoff2022mteb} leaderboard. The selection is based on their model size and performance in classification and retrieval tasks: all-roberta-large-v1~\\citep{reimers2019sentence} and e5-large~\\citep{wang2022text}.\n\nThe outcomes of our study are detailed in Table~\\ref{tab: LM ablation}. Notably, our model exhibits insensitivity to the choice of a specific LM, underscoring its robustness to variations in LM selection.\n\n\n\n\n\n\n\n% \\begin{table}[!ht]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{lcccc}\n%     \\toprule\n%          \\multirow{2}{*}{Model} \n%          &  \\multicolumn{2}{c}{Memory} \n%          &  \\multirow{2}{*}{Time}\n%          & \\multirow{2}{*}{Accuracy}\\\\\n%          \\cmidrule(lr){2-3}\n%          & LM & GNN \\\\\n%          \\midrule\n%          Pure LM\n%          & 8,834 MB\n%          & --\n%          & 108 min\n%          & 0.7361 \u00b1 0.0004\\\\\n%          GNN w/ shallow feature\n%          & --\n%          & 4,430 MB\n%          & 2 min\n%          & 0.7083 \u00b1 0.0017\n%          \\\\\n%          LM-based GLEM\n%          & 11,064 MB\n%          & 8,112 MB\n%          & 551 min\n%          & 0.7657 \u00b1 0.0029\n%          \\\\\n%          LLM-based TAPE (Ours)\n%          & 8,834\tMB\n%          & 4,430 MB\n%          & 192 min\n%          & 0.7750 \u00b1 0.0012\n%          \\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{Caption}\n%     \\label{tab:my_label}\n% \\end{table}\n\n\n"
            },
            "section 19": {
                "name": "Memory Utilization",
                "content": "\n\\label{sec: memory}\nTable~\\ref{tab: memory usage} presents the memory utilization  for experiments conducted on the \\texttt{ogbn-arxiv} dataset.\n\n\n\nThere is a trade-off between memory consumption and accuracy. Our model appears to be the most efficient in terms of memory-to-accuracy ratio. It does not require more memory than the pure LM or GNN with shallow feature models, yet it delivers the best accuracy.\n\n\n% The models are evaluated with DeBERTa-base as the LM backbone and RevGAT as the GNN backbone. This comparison includes various training paradigms for fusing LMs and GNNs, including our proposed method and the state-of-the-art GLEM method. All experiments were executed on 4 NVIDIA RTX A5000 24GB GPUs, with a batch size of 36.\n\n% high-level motivation\n% summary of theorm\n% explain fidelity & non-redundacy\n\n"
            },
            "section 20": {
                "name": "GLEM",
                "content": "\n\\cite{zhao2022learning_em} evaluated GLEM on the \\texttt{ogbn-arxiv} dataset. We extended our evaluation of GLEM with the \\texttt{Cora} and \\texttt{PubMed} datasets for a more comprehensive comparison with our method. Results are reported in Table~\\ref{rebuttal tab: glem}\n\n\n\n\n% \\begin{table}[!ht]\n% % \\vspace{-0.2cm}\n% \\caption{Node classification accuracy for the \\texttt{Cora}, \\texttt{PubMed}, \\texttt{ogbn-arxiv}, \\texttt{ogbn-products} and \\texttt{tape-arxiv23} datasets. $G\\uparrow$ denotes the improvements of our approach over the same GNN trained on shallow features $h_\\textrm{shallow}$; $L\\uparrow$ denotes the improvements of our approach over LM$_\\textrm{finetune}$. The results are averaged over four runs with different seeds, and the best results are \\textbf{in bold}.}\n% \\footnotesize\n%     \\centering\n%     \\begin{tabular}{llccccccc}\n%     \\toprule\n%     \\multirow{2}{*}{Dataset}     \n%     & \\multirow{2}{*}{Method}\n%     & \\multicolumn{3}{c}{GNN} \n%     & \\multicolumn{3}{c}{LM}\n%     & \\multicolumn{1}{c}{Ours}\\\\\n%     \\cmidrule(lr){3-5}\\cmidrule(lr){6-8}\\cmidrule(lr){9-9}\n%     &  \n%     &$h_{\\textrm{shallow}}$\n%     & $h_{\\textrm{GIANT}}$\n%     & $G\\uparrow$\n%     & LLM \n%     & LM$_{\\textrm{finetune}}$ \n%     & $L\\uparrow$ \n%     & $h_{\\textrm{TAPE}}$ \n%     \\\\\n%     \\midrule\n%          \\multirow{3}{*}{\\texttt{Cora}} \n%          & {MLP}\n%          & {0.6388}\n%          & {0.7196}\n%          & {37.41\\%}\n%          & {0.6769}\n%          & {0.7606}\n%          & {13.35\\%}\n%          & {0.8778}\n%          \\\\\n%          & GCN \n%          & 0.8911\n%          & {0.8423}\n%          & 2.33\\%\n%          & 0.6769\n%          & 0.7606\n%          & 16.59\\%\n%          & 0.9119\n%          \\\\\n%          & SAGE\n%          & 0.8824\n%          & {0.8455}\n%          & 5.28\\%\n%          & 0.6769\n%          & 0.7606\n%          & 18.13\\%\n%          & \\textbf{0.9290}\\\\\n%          & RevGAT\n%          & 0.8911\n%          & {0.8353}\n%          & 4.14\\%\n%          & 0.6769\n%          & 0.7606\n%          & 18.04\\%\n%          & 0.9280\n%          \\\\\n%          \\midrule\n%     \\multirow{3}{*}{\\texttt{PubMed}} \n%     & {MLP}\n%          & {0.8635}\n%          & {0.8175}\n%          & {10.77\\%}\n%          & {0.9342}\n%          & {0.9494}\n%          & {0.75\\%}\n%          & {0.9565}\n%          \\\\\n%          & GCN \n%          & 0.8031\n%          & {0.8419}\n%          & 17.43\\%\n%          & 0.9342\n%          & 0.9494\n%          & -0.66\\%\n%          & 0.9431\n%          \\\\\n%          & SAGE\n%          & 0.8881\n%          & {0.8372}\n%          & 8.30\\%\n%          & 0.9342\n%          & 0.9494\n%          & 1.31\\%\n%          & \\textbf{0.9618}\n%          \\\\\n%          & RevGAT\n%          & 0.8850\n%          & {0.8502}\n%          & 8.52\\%\n%          & 0.9342\n%          & 0.9494\n%          & 1.15\\% \n%          & {0.9604}\n%          \\\\\n%          \\midrule\n%     \\multirow{3}{*}{\\texttt{ogbn-arxiv}}\n%     & {MLP}\n%          & {0.5336}\n%          & {0.7308}\n%          & {42.19\\%}\n%          & {0.7350}\n%          & {0.7361}\n%          & {3.07\\%}\n%          & {0.7587}\n%          \\\\\n%          & GCN \n%          & 0.7182\n%          & 0.7329\n%          & 4.71\\%\n%          & 0.7350\n%          & 0.7361\n%          & 2.16\\%\n%          & 0.7520\n%          \\\\\n%          & SAGE\n%          & 0.7171\n%          & 0.7435\n%          & 6.98\\%\n%          & 0.7350\n%          & 0.7361\n%          & 4.22\\%\n%          & 0.7672\n%          \\\\\n%          & RevGAT\n%          & 0.7083\n%          & 0.7590\n%          & 9.42\\%\n%          & 0.7350\n%          & 0.7361\n%          & 5.28\\%\n%          & \\textbf{0.7750}\n%          \\\\\n%          \\midrule\n%          \\multirow{4}{*}{\\texttt{ogbn-products}}\n%          &MLP \n%          & 0.5385\n%          & 0.6125\n%          & 46.3\\%\n%          & {0.7440}\n%          & 0.7297\n%          & 7.96\\%\n%          & 0.7878\n%          \\\\\n%     & {GCN}\n%     & {0.7052}\n%     & {0.6977}\n%     & {13.39\\%}\n%     & {0.7440}\n%     & {0.7297}\n%     & {9.58\\%}\n%     & {0.7996}\n%     \\\\\n%      & {SAGE}\n%     & {0.6913}\n%     & {0.6869}\n%     & {17.71\\%}\n%     & {0.7440}\n%     & {0.7297}\n%     & {11.51\\%}\n%     & {0.8137}\n%     \\\\\n%      & RevGAT\n%     & 0.6964\n%     & {0.7189}\n%     & 18.24\\%\n%     & 0.7440\n%     & 0.7297\n%     & 12.84\\%\n%     & \\textbf{0.8234}\n%     \\\\\n%     \\midrule\n%     \\multirow{4}{*}{\\texttt{tape-arxiv23}} \n%     & MLP \n%     & 0.6202\n%     & {0.5574}\n%     & 35.20\\% \n%     & 0.7356\n%     & 0.7358\n%     & 12.25\\%\n%     & 0.8385\n%     \\\\\n%     & GCN \n%     & 0.6341\n%     & {0.5672}\n%     & 27.42\\%\n%     & 0.7356 \n%     & 0.7358\n%     & 8.94\\%\n%     & 0.8080\n%     \\\\\n%     & SAGE \n%     & 0.6430\n%     & {0.5665}\n%     & 30.45\\% \n%     & 0.7356 \n%     & 0.7358\n%     & 12.28\\%\n%     & 0.8388\n%     \\\\\n%     & RevGAT \n%     & 0.6563\n%     & {0.5834}\n%     & 28.34\\%\n%     & 0.7356\n%     & 0.7358\n%     & 12.64\\%\n%     & \\textbf{0.8423}\n%     \\\\\n%     \\bottomrule    \n%     \\end{tabular}\n    \n% \\end{table}\n"
            }
        },
        "tables": {
            "tab: performance": "\\begin{table}[!ht]\n% \\vspace{-0.2cm}\n\\caption{Node classification accuracy for the \\texttt{Cora}, \\texttt{PubMed}, \\texttt{ogbn-arxiv}, \\texttt{ogbn-products} and \\texttt{tape-arxiv23} datasets. $G\\uparrow$ denotes the improvements of our approach over the same GNN trained on shallow features $h_\\textrm{shallow}$; $L\\uparrow$ denotes the improvements of our approach over LM$_\\textrm{finetune}$. The results are averaged over four runs with different seeds, and the best results are \\textbf{in bold}.}\n\\tiny\n    \\label{tab: performance}\n    \\centering\n    % \\hspace*{-0.5cm}\n    \\begin{tabular}{llccccccc}\n    % \\begin{tabular}{p{1.2cm}p{0.75cm}ccccccc}\n    % \\begin{tabular}{p{1cm}p{0.7cm}>{\\centering}p{1.6cm}>{\\centering}p{1.6cm}>{\\centering}p{0.7cm}>{\\centering}p{0.7cm}>{\\centering}p{1.6cm}>{\\centering}p{0.8cm}p{1.6cm}}\n    \\toprule\n    \\multirow{2}{*}{Dataset}     \n    & \\multirow{2}{*}{Method}\n    & \\multicolumn{3}{c}{GNN} \n    & \\multicolumn{3}{c}{LM}\n    & \\multicolumn{1}{c}{Ours}\\\\\n    \\cmidrule(lr){3-5}\\cmidrule(lr){6-8}\\cmidrule(lr){9-9}\n    &  \n    &$h_{\\textrm{shallow}}$\n    & $h_{\\textrm{GIANT}}$\n    & $G\\uparrow$\n    & LLM \n    & LM$_{\\textrm{finetune}}$ \n    & $L\\uparrow$ \n    & $h_{\\textrm{TAPE}}$ \n    \\\\\n    \\midrule\n         \\multirow{3}{*}{\\texttt{Cora}} \n         & {MLP}\n         & {0.6388 \u00b1 0.0213}\n         & {0.7196 \u00b1 0.0000}\n         & {37.41\\%}\n         & {0.6769}\n         & {0.7606 \u00b1 0.0378}\n         & {13.35\\%}\n         & {0.8778 \u00b1 0.0485}\n         \\\\\n         & GCN \n         & 0.8911 \u00b1 0.0015\n         & {0.8423 \u00b1 0.0053}\n         & 2.33\\%\n         & 0.6769\n         & 0.7606 \u00b1 0.0378\n         & 16.59\\%\n         & 0.9119 \u00b1 0.0158\n         \\\\\n         & SAGE\n         & 0.8824 \u00b1 0.0009\n         & {0.8455 \u00b1 0.0028}\n         & 5.28\\%\n         & 0.6769\n         & 0.7606 \u00b1 0.0378\n         & 18.13\\%\n         & \\textbf{0.9290 \u00b1 0.0307}\\\\\n         & RevGAT\n         & 0.8911 \u00b1 0.0000\n         & {0.8353 \u00b1 0.0038}\n         & 4.14\\%\n         & 0.6769\n         & 0.7606 \u00b1 0.0378\n         & 18.04\\%\n         & 0.9280 \u00b1 0.0275\n         \n         \\\\\n         \\midrule\n    \\multirow{3}{*}{\\texttt{PubMed}} \n    & {MLP}\n         & {0.8635 \u00b1 0.0032}\n         & {0.8175 \u00b1 0.0059}\n         & {10.77\\%}\n         & {0.9342}\n         & {0.9494 \u00b1 0.0046}\n         & {0.75\\%}\n         & {0.9565 \u00b1 0.0060}\n         \\\\\n         & GCN \n         & 0.8031 \u00b1 0.0425\n         & {0.8419 \u00b1 0.0050}\n         & 17.43\\%\n         & 0.9342\n         & 0.9494 \u00b1 0.0046\n         & -0.66\\%\n         & 0.9431 \u00b1 0.0043\n         \\\\\n         & SAGE\n         & 0.8881 \u00b1 0.0002\n         & {0.8372 \u00b1 0.0082}\n         & 8.30\\%\n         & 0.9342\n         & 0.9494 \u00b1 0.0046\n         & 1.31\\%\n         & \\textbf{0.9618 \u00b1 0.0053}\n         \\\\\n         & RevGAT\n         & 0.8850 \u00b1 0.0005\n         & {0.8502 \u00b1 0.0048}\n         & 8.52\\%\n         & 0.9342\n         & 0.9494 \u00b1 0.0046\n         & 1.15\\% \n         & {0.9604 \u00b1 0.0047}\n         \\\\\n         \\midrule\n    \\multirow{3}{*}{\\texttt{ogbn-arxiv}}\n    & {MLP}\n         & {0.5336 \u00b1 0.0038}\n         & {0.7308 \u00b1 0.0006}\n         & {42.19\\%}\n         & {0.7350}\n         & {0.7361 \u00b1 0.0004}\n         & {3.07\\%}\n         & {0.7587 \u00b1 0.0015}\n         \\\\\n         & GCN \n         & 0.7182 \u00b1 0.0027\n         & 0.7329 \u00b1 0.0010\n         & 4.71\\%\n         & 0.7350\n         & 0.7361 \u00b1 0.0004\n         & 2.16\\%\n         & 0.7520 \u00b1 0.0003\n         \\\\\n         & SAGE\n         & 0.7171 \u00b1 0.0017\n         & 0.7435 \u00b1 0.0014\n         & 6.98\\%\n         & 0.7350\n         & 0.7361 \u00b1 0.0004\n         & 4.22\\%\n         & 0.7672 \u00b1 0.0007\n         \\\\\n         & RevGAT\n         & 0.7083 \u00b1 0.0017\n         & 0.7590 \u00b1 0.0019\n         & 9.42\\%\n         & 0.7350\n         & 0.7361 \u00b1 0.0004\n         & 5.28\\%\n         & \\textbf{0.7750 \u00b1 0.0012}\n         \\\\\n         \\midrule\n         \\multirow{4}{*}{\\texttt{ogbn-products}}\n         &MLP \n         & 0.5385 \u00b1 0.0017\n         & {0.6125 \u00b1 0.0078}\n         & 46.3\\%\n         & {0.7440}\n         & {0.7297 \u00b1 0.0023}\n         & 7.96\\%\n         & 0.7878 \u00b1 0.0082\n         \\\\\n    & {GCN}\n    & {0.7052 \u00b1 0.0051}\n    & {0.6977 \u00b1 0.0042}\n    & {13.39\\%}\n    & {0.7440}\n    & {0.7297 \u00b1 0.0023}\n    & {9.58\\%}\n    & {0.7996 \u00b1 0.0041}\n    \\\\\n     & {SAGE}\n    & {0.6913 \u00b1 0.0026}\n    & {0.6869 \u00b1 0.0119}\n    & {17.71\\%}\n    & {0.7440}\n    & {0.7297 \u00b1 0.0023}\n    & {11.51\\%}\n    & {0.8137 \u00b1 0.0043}\n    \\\\\n     & RevGAT\n    & 0.6964 \u00b1 0.0017\n    & {0.7189 \u00b1 0.0030}\n    & 18.24\\%\n    & 0.7440\n    & 0.7297 \u00b1 0.0023\n    & 12.84\\%\n    & \\textbf{0.8234 \u00b1 0.0036}\n    \\\\\n    \\midrule\n    \\multirow{4}{*}{\\texttt{tape-arxiv23}} \n    & MLP \n    & 0.6202 \u00b1 0.0064 \n    & {0.5574 \u00b1 0.0032}\n    & 35.20\\% \n    & 0.7356\n    & 0.7358 \u00b1 0.0006\n    & 12.25\\%\n    & 0.8385 \u00b1 0.0246\n    \\\\\n    & GCN \n    & 0.6341 \u00b1 0.0062 \n    & {0.5672 \u00b1 0.0061}\n    & 27.42\\%\n    & 0.7356 \n    & 0.7358 \u00b1 0.0006\n    & 8.94\\%\n    & 0.8080 \u00b1 0.0215\n    \\\\\n    & SAGE \n    & 0.6430 \u00b1 0.0037 \n    & {0.5665 \u00b1 0.0032}\n    & 30.45\\% \n    & 0.7356 \n    & 0.7358 \u00b1 0.0006\n    & 12.28\\%\n    & 0.8388 \u00b1 0.0264   \n    \\\\\n    & RevGAT \n    & 0.6563 \u00b1 0.0062\n    & {0.5834 \u00b1 0.0038}\n    & 28.34\\%\n    & 0.7356\n    & 0.7358 \u00b1 0.0006\n    & 12.64\\%\n    & \\textbf{0.8423 \u00b1 0.0256}\n    \\\\\n    \\bottomrule    \n    \\end{tabular}    \n\\end{table}",
            "tab: comparison": "\\begin{table}[t]\n% \\vspace{-0.5cm}\n\\caption{Experiments on \\texttt{ogbn-arxiv} dataset with DeBERTa-base~\\citep{he2021deberta} as LM backbone and RevGAT~\\citep{li2021training_revgat} as GNN backbone for comparison of different training paradigms of fusing LMs and GNNs, including our proposed method and the state-of-the-art GLEM method~\\citep{zhao2022learning_em}.  The validation and test accuracy, number of parameters, maximum batch size (Max bsz.), and total training time on 4 NVIDIA RTX A5000 24GB GPUs are reported. \n% A comparison of different training paradigms for fusing language models (LMs) and graph neural networks (GNNs), using DeBERTa-base~\\citep{he2021deberta} as the LM backbone and RevGAT~\\citep{li2021training_revgat} as the GNN backbone. The max batch size (bsz.) and total time are tested on 4 NVIDIA RTX A5000 24GB GPUs.\n}\n\\label{tab: comparison}\n\\small\n    \\centering\n    \\begin{tabular}{lccccc}\n    \\toprule\n    Method\n    &  Val acc. &  Test acc. & Params. & Max bsz. & Total time \\\\\n    \\midrule\n    LM$_{orig}$\n    & 0.7503 \u00b1 0.0008\n    & 0.7361 \u00b1 0.0004\n    & 139,223,080\n    & 36\n    & 1.73h \\\\\n    \\midrule\n    GNN-$h_{\\textrm{shallow}}$\n    & 0.7144 \u00b1 0.0021\n    & 0.7083 \u00b1 0.0017\n    & 427,728\n    & all nodes\n    & 1.80min \\\\\n    \\midrule\n    GLEM-G-Step\n    & 0.7761 \u00b1 0.0005\n    & 0.7657 \u00b1 0.0029\n    & 1,837,136\n    & all nodes\n    & \\multirow{2}{*}{9.18h} \\\\\n    GLEM-L-Step\n    & 0.7548 \u00b1 0.0039\n    & 0.7495 \u00b1 0.0037\n    & 138,632,488\n    & 36\n    &\\\\\n    \\midrule\n    TAPE-LM$_{\\textrm{orig}}$-Step\n    & 0.7503 \u00b1 0.0008\n    & 0.7361 \u00b1 0.0004\n    & 139,223,080\n    & 36\n    & 1.73h\\\\\n    TAPE-LM$_{\\textrm{expl}}$-Step\n    & 0.7506 \u00b1 0.0008\n    & 0.7432 \u00b1 0.0012\n    & 139,223,080\n    & 36\n    & 1.40h\\\\\n    TAPE-GNN-${h_\\textrm{TAPE}}$-Step\n    & {0.7785 \u00b1 0.0016}\n    & 0.7750 \u00b1 0.0012\n    & 1,837,136\n    & all nodes\n    & 3.76min\n    \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: ablation": "\\begin{table}[t]\n\\small\n% \\caption{Ablation Study on the \\texttt{ogbn-arxiv} dataset to study the effects of different node features. Results are averaged over 4 runs with 4 different seeds. The best results are \\textbf{in bold}.}\n\\caption{Ablation study on the \\texttt{ogbn-arxiv} dataset, showing the effects of different node features on the performance. Node features include the original text attributes ($h_{\\textrm{orig}}$), the explanations ($h_{\\textrm{expl}}$  and predicted $h_{\\textrm{pred}}$) generated by LLM, and the proposed method ($h_{\\textrm{TAPE}}$). Results are averaged over 4 runs with 4 different seeds. The best results are \\textbf{in bold}.}\n    \\label{tab: ablation}\n    \\centering\n    \\begin{tabular}{lccccc}\n    \\toprule\n    \\multicolumn{2}{l}{Method}   &  $h_{\\textrm{orig}}$ & $h_{\\textrm{expl}}$ & $h_{\\textrm{pred}}$ & $h_{\\textrm{TAPE}}$\\\\\n    \\midrule\n    \\multirow{2}{*}{GCN}\n    & val & 0.7624 \u00b1 0.0007 & 0.7577 \u00b1 0.0008 & 0.7531 \u00b1 0.0006 & 0.7642 \u00b1 0.0003\\\\\n    & test & 0.7498 \u00b1 0.0018 & 0.7460 \u00b1 0.0013 & 0.7400 \u00b1 0.0007 & \\textbf{0.7520 \u00b1 0.0003}\\\\\n    \\midrule\n    \\multirow{2}{*}{SAGE}\n    & val & 0.7594 \u00b1 0.0012 & 0.7631 \u00b1 0.0016 & 0.7612 \u00b1 0.0010 & 0.7768 \u00b1 0.0016\\\\\n    & test & 0.7420 \u00b1 0.0018 & 0.7535 \u00b1 0.0023 & 0.7524 \u00b1 0.0015 & \\textbf{0.7672 \u00b1 0.0007}\\\\\n    \\midrule\n    \\multirow{2}{*}{RevGAT}\n    & val & 0.7588 \u00b1 0.0021 &  0.7568 \u00b1 0.0027 &  0.7550 \u00b1 0.0015 & 0.7785 \u00b1 0.0016\\\\\n    & test & 0.7504 \u00b1 0.0020 & 0.7529 \u00b1 0.0052 & 0.7519 \u00b1 0.0031 & \\textbf{0.7750 \u00b1 0.0012}\n    \\\\ \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "rebuttal tab: llama2": "\\begin{table}[!ht]\n    \\caption{Node classification accuracy for the \\texttt{Cora}, \\texttt{PubMed} and \\texttt{ogbn-arxiv} datasets. }\n\\scriptsize\n    \\label{rebuttal tab: llama2}\n    \\centering\n    % \\hspace*{-0.5cm}\n    % \\begin{tabular}{p{1.1cm}p{0.75cm}cccccc}\n    \\begin{tabular}{llcccccc}\n    % \\begin{tabular}{p{1cm}p{0.7cm}>{\\centering}p{1.6cm}>{\\centering}p{1.6cm}>{\\centering}p{0.7cm}>{\\centering}p{0.7cm}>{\\centering}p{1.6cm}>{\\centering}p{0.8cm}p{1.6cm}}\n    \\toprule\n    \\multirow{2}{*}{Dataset}     \n    & \\multirow{2}{*}{Method}\n    & \\multicolumn{3}{c}{{llama2-13b-chat}} \n    & \\multicolumn{3}{c}{{GPT3.5}}\\\\\n    \\cmidrule(lr){3-5}\\cmidrule(lr){6-8}\n    &  \n    & {LLM} & {LM$_{\\textrm{finetune}}$} & {$h_\\textrm{TAPE}$}\n    & LLM & LM$_{\\textrm{finetune}}$ & {$h_\\textrm{TAPE}$}\n    \n    \\\\\n    \\midrule\n         \\multirow{3}{*}{\\texttt{Cora}} \n         & GCN \n         & {0.5746}\n         & {0.6845 \u00b1 0.0194}\n         & {0.9045 \u00b1 0.0231}\n         & 0.6769\n         & 0.7606 \u00b1 0.0378\n         & 0.9119 \u00b1 0.0158\n         \\\\\n         & SAGE\n         & {0.5746}\n         & {0.6845 \u00b1 0.0194}\n         & {0.9170 \u00b1 0.0337}\n         & 0.6769\n         & 0.7606 \u00b1 0.0378\n         & 0.9290 \u00b1 0.0307\\\\\n         & RevGAT\n         & {0.5746}\n         & {0.6845 \u00b1 0.0194}\n         & {0.9313 \u00b1 0.0237}\n         & 0.6769\n         & 0.7606 \u00b1 0.0378\n         & 0.9280 \u00b1 0.0275\\\\\n         \\midrule\n    \\multirow{3}{*}{\\texttt{PubMed}} \n         & GCN \n         & {0.3958}\n         & {0.9121 \u00b1 0.0026}\n         & {0.9362 \u00b1 0.0050}\n         & 0.9342\n         & 0.9494 \u00b1 0.0046\n         & 0.9431 \u00b1 0.0043\n         \\\\\n         & SAGE\n         & {0.3958}\n         & {0.9121 \u00b1 0.0026}\n         & {0.9581 \u00b1 0.0073}\n         & 0.9342\n         & 0.9494 \u00b1 0.0046\n         & {0.9618 \u00b1 0.0053}\n         \\\\\n         & RevGAT\n         & {0.3958}\n         & {0.9121 \u00b1 0.0026}\n         & {0.9561 \u00b1 0.0068}\n         & 0.9342\n         & 0.9494 \u00b1 0.0046\n         & {0.9604 \u00b1 0.0047}\n         \\\\\n         \\midrule\n    \\multirow{3}{*}{\\texttt{ogbn-arxiv}}\n         & GCN \n         & {0.4423}\n         & {0.6941 \u00b1 0.0020}\n         & {0.7418 \u00b1 0.0031}\n         & 0.7350\n         & 0.7361 \u00b1 0.0004\n         & 0.7520 \u00b1 0.0003\n         \\\\\n         & SAGE\n         & {0.4423}\n         & {0.6941 \u00b1 0.0020}\n         & {0.7536 \u00b1 0.0028}\n         & 0.7350\n         & 0.7361 \u00b1 0.0004\n         & 0.7672 \u00b1 0.0007\n         \\\\\n         & RevGAT\n         & {0.4423}\n         & {0.6941 \u00b1 0.0020}\n         & {0.7619 \u00b1 0.0027}\n         & 0.7350\n         & 0.7361 \u00b1 0.0004\n         & {0.7750 \u00b1 0.0012}\n         \\\\\n         \\midrule\n         % \\multirow{3}{*}{\\texttt{ogbn-products}}\n         % & GCN\n         % \\\\\n         % & SAGE\n         % \\\\\n         % & RevGAT\n         % \\\\\n         % \\midrule\n         \\multirow{3}{*}{\\texttt{tape-arxiv23}}\n         & GCN\n         & 0.4452\n         & 0.7677 \u00b1 0.0042\n         & 0.8045 \u00b1 0.0264\n         & 0.7356\n         & 0.7832 \u00b1 0.0052\n         & 0.8080 \u00b1 0.0215\n         \\\\\n         & SAGE\n         & 0.4452\n         & 0.7677 \u00b1 0.0042\n         & 0.8378 \u00b1 0.0302\n         & 0.7356\n         & 0.7832 \u00b1 0.0052\n         & 0.8388 \u00b1 0.0264\n         \\\\\n         & RevGAT\n         & 0.4452\n         & 0.7677 \u00b1 0.0042\n         & 0.8407 \u00b1 0.0308\n         & 0.7356\n         & 0.7832 \u00b1 0.0052\n         & {0.8423 \u00b1 0.0256}\n         \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: prompt": "\\begin{table}[!ht]\n\\small\n    \\centering\n    % \\begin{tabular}{lcc}\n    \\caption{Prompts used in this work to query the LLM.}\n    \\label{tab: prompt}\n    \\begin{tabularx}{\\textwidth}{lX}\n    \\toprule\n         Dataset & Prompt\\\\\n         \\midrule\n         \\texttt{Cora}\n         &Abstract: \\promptfield{abstract text} \\newl Title: \\promptfield{title text} \\newl Question: Which of the following sub-categories of AI does this paper belong to: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory? If multiple options apply, provide a comma-separated list ordered from most to least related, then for each choice you gave, explain how it is present in the text. \\newl \\newl Answer: \\\\\n         \\midrule\n         \\texttt{Pubmed}\n         &Abstract: \\promptfield{abstract text} \\newl Title: \\promptfield{title text} \\newl Question: Does the paper involve any cases of Type 1 diabetes, Type 2 diabetes, or Experimentally induced diabetes? Please give one or more answers of either Type 1 diabetes, Type 2 diabetes, or Experimentally induced diabetes; if multiple options apply, provide a comma-separated list ordered from most to least related, then for each choice you gave, give a detailed explanation with quotes from the text explaining why it is related to the chosen option. \\newl \\newl Answer:\\\\\n         \\midrule\n         \\texttt{ogbn-arxiv}\n         &{Abstract: \\promptfield{abstract text} \\newl Title: \\promptfield{title text} \\newl Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least likely, in the form ``cs.XX'', and provide your reasoning. \\newl \\newl Answer:}\\\\\n         \\midrule\n         \\texttt{ogbn-products}\n         & Product description: {\\color{blue}<product description>} \\newl Question: Which of the following category does this product belong to: 1) Home \\& Kitchen, 2) Health \\& Personal Care, 3) Beauty, 4) Sports \\& Outdoors, 5) Books, 6) Patio, Lawn \\& Garden, 7) Toys \\& Games, 8) CDs \\& Vinyl, 9) Cell Phones \\& Accessories, 10) Grocery \\& Gourmet Food, 11) Arts, Crafts \\& Sewing, 12) Clothing, Shoes \\& Jewelry, 13) Electronics, 14) Movies \\& TV, 15) Software, 16) Video Games, 17) Automotive, 18) Pet Supplies, 19) Office Products, 20) Industrial \\& Scientific, 21) Musical Instruments, 22) Tools \\& Home Improvement, 23) Magazine Subscriptions, 24) Baby Products, 25) NAN, 26) Appliances, 27) Kitchen \\& Dining, 28) Collectibles \\& Fine Art, 29) All Beauty, 30) Luxury Beauty, 31) Amazon Fashion, 32) Computers, 33) All Electronics, 34) Purchase Circles, 35) MP3 Players \\& Accessories, 36) Gift Cards, 37) Office \\& School Supplies, 38) Home Improvement, 39) Camera \\& Photo, 40) GPS \\& Navigation, 41) Digital Music, 42) Car Electronics, 43) Baby, 44) Kindle Store, 45) Kindle Apps, 46) Furniture \\& Decor? Give 5 likely categories as a comma-separated list ordered from most to least likely, and provide your reasoning. \\newl\\newl Answer: \n         \\\\\n         \\midrule\n         \\texttt{tape-arxiv23}\n         &{Abstract: \\promptfield{abstract text} \\newl Title: \\promptfield{title text} \\newl Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least likely, in the form ``cs.XX'', and provide your reasoning. \\newl \\newl Answer:}\\\\\n         \\bottomrule\n    \\end{tabularx}\n\\end{table}",
            "tab: prompt experiment": "\\begin{table}[!ht]\n\\small\n    \\centering\n    % \\begin{tabular}{lcc}\n    \\caption{Prompts used for our experiments studying the effect of different prompts. Most prompts have similar performance. }\n    \\label{tab: prompt experiment}\n    \\begin{tabularx}{\\textwidth}{lXc}\n    \\toprule\n         Description & Prompt & Accuracy\\\\\n         \\midrule\n         Default prompt & Abstract: \\promptfield{abstract text} \\newl Title: \\promptfield{title text} \\newl Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least likely, in the form ``cs.XX'', and provide your reasoning. \\newl \\newl Answer: & 0.720\\\\  \n         \\midrule\n         Title first & Title: \\promptfield{title text} \\newl Abstract: \\promptfield{abstract text} \\newl Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least likely, in the form ``cs.XX'', and provide your reasoning. \\newl \\newl Answer: & 0.695\\\\  \n         \\midrule\n         Focus on text content & Title: \\promptfield{title text} \\newl Abstract: \\promptfield{abstract text} \\newl Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least likely, in the form ``cs.XX''. Focus only on content in the actual text and avoid making false associations. Then provide your reasoning.  & 0.695\\\\  \n         \\midrule\n         Chain of thought prompt & Title: \\promptfield{title text} \\newl Abstract: \\promptfield{abstract text} \\newl Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least likely, in the form ``cs.XX''. Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.   & 0.705\\\\  \n         \\bottomrule\n    \\end{tabularx}\n\\end{table}",
            "tab: prompt design": "\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\caption{{Study of the robustness of prompt on \\texttt{ogbn-arxiv} dataset.}}\n    \\label{tab: prompt design}\n    \\begin{tabular}{lcccc}\n    \\toprule\n         &  {LLM (zero-shot)} \n         & {TAPE (GCN)} \n         & {TAPE (SAGE)} \n         & {TAPE (RevGAT)}\n         \\\\\n         \\midrule\n         {Default prompt }\n         & {0.720}\n         & {0.7520 \u00b1 0.0003\t}\n         & {0.7672 \u00b1 0.0007\t}\n         & {0.7750 \u00b1 0.0012}\n         \\\\\n         {Focus on text content         }\n         & {0.695}\n         & {0.7425 \u00b1 0.0021\t}\n         & {0.7598 \u00b1 0.0006\t}\n         & {0.7660 \u00b1 0.0017}\n         \\\\\n         {Chain of thought prompt}\n         & {0.705}\n         & {0.7424 \u00b1 0.0019\t}\n         & {0.7597 \u00b1 0.0034\t}\n         & {0.7667 \u00b1 0.0028}\n         \\\\\n         \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: dataset": "\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\caption{Statistics of the TAG datasets}\n    \\label{tab: dataset}\n    \\begin{tabular}{lccccc}\n    \\toprule\n         Dataset &  \\#Nodes &\\#Edges & Task  & Metric & Augmentation \\\\\n         \\midrule\n         \\texttt{Cora}& 2,708 &5,429 & 7-class classif. & Accuracy & \\cmark \\\\\n         % \\texttt{CiteSeer}~\\citep{giles1998citeseer}& 3,312 &4,732 & 6-class classif.& Accuracy\n         % \\\\\n         \\texttt{Pubmed}& 19,717 & 44,338 & 3-class classif.& Accuracy & \\cmark\n         \\\\\n         \n         \\texttt{ogbn-arxiv} & 169,343 & 1,166,243 & 40-class classif.& Accuracy & \n         \\\\\n         \\texttt{ogbn-products} (subset) & 54,025 &74,420 & 47-class classif. & Accuracy & \\\\\n         \\texttt{tape-arxiv23}\n         & {46,198}\n         & {78,548}\n         &{40-class-classif.}\n         & {Accuracy}\n         & \\cmark\\\\\n         \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: text preprocess": "\\begin{table}[!ht]\n\\caption{Details of text preprocessing and feature extraction methods used for TAG datasets.}\n\\small\n    \\label{tab: text preprocess}\n    \\centering\n    \\begin{tabularx}{\\textwidth}{lllX}\n    \\toprule\n         Dataset  &Methods & Features & Description \\\\\n         \\midrule\n         \\texttt{Cora}\n         & BoW\n         & 1,433\n         & After stemming and removing stopwords there is a vocabulary of size 1,433 unique words. All words with document frequency less than 10 were removed.\\\\\n         % \\midrule\n         % \\texttt{CiteSeer} \n         % & BoW\n         % & 3,703\n         % & After stemming and removing stopwords there is with a vocabulary of size 3,703 unique words. All words with document frequency less than 10 were removed.\\\\\n         \\midrule\n         \\texttt{PubMed} \n         & TF-IDF\n         & 500\n         & Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words. \\\\\n         \\midrule\n         \\texttt{ogbn-arxiv}\n         & skip-gram\n         & 128\n         & The embeddings of individual words are computed by running the skip-gram model~\\citep{mikolov2013distributed_skipgram} over the MAG~\\citep{wang2020microsoft_mag} corpus.\\\\\n         \\midrule\n         \\texttt{ogbn-products} \n         & BoW\n         & 100\n         & Node features are generated by extracting BoW features from the product descriptions followed by a Principal Component Analysis to reduce the dimension to 100.\n         \\\\\n         \\midrule\n         \\texttt{tape-arxiv23}\n         & word2vec\n         & 300\n         & The embeddings of individual words are computed by running the word2vec model.\n         \\\\\n         \\bottomrule\n    \\end{tabularx}\n\\end{table}",
            "tab: hyperparam": "\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\caption{Hyperparameters for the GCN, SAGE, and RevGAT models.}\n    \\label{tab: hyperparam}\n    % \\begin{tabular}{lccc}\n    \\begin{tabular}{m{4cm}m{2cm}m{2cm}m{2cm}}\n    \\toprule\nHyperparameters & GCN& SAGE&RevGAT\\\\\n\\midrule\n\\# layers & 3 & 3 & 3\\\\\nhidden dim & 256 & 256 & 256 \\\\\nlearning rate & 0.01 & 0.01 & 0.002\\\\\ndropout & 0.5 & 0.5 & 0.75\\\\\nepoch & 1000 & 1000 & 1000\\\\\nwarmup epochs & 0 & 0 & 50\\\\\nearly stop & 50 & 50 & 50\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab: lm_finetuning": "\\begin{table}[!ht]\n    \\centering\n    \\caption{{Effect of LM finetuning on \\texttt{ogbn-arxiv}}}\n    \\label{tab: lm_finetuning}\n    \\small\n    \\begin{tabular}{lcccc}\n    \\toprule\n          {LM}\n          & {MLP} \n          & {GCN} \n          & {SAGE} \n          & {RevGAT}\n          \\\\\n          \\midrule\n         {Without Fine-Tuning }\n         & {0.5797 \u00b1 0.0217}\n         & {0.4178 \u00b1 0.1148}\n         & {0.4507 \u00b1 0.0529}\n         & {0.7507 \u00b1 0.0189}\\\\\n         {Fine-Tuning (Same LM)}\n         & {0.7566 \u00b1 0.0015}\n         & {0.7442 \u00b1 0.0012}\n         & {0.7676 \u00b1 0.0032}\n         & {0.7728 \u00b1 0.0014}\n         \\\\\n         {Fine-Tuning (Different LMs) }\n         & {0.7587 \u00b1 0.0015}\n         & {0.7520 \u00b1 0.0003}\n         & {0.7672 \u00b1 0.0007}\n         & {0.7750 \u00b1 0.0012}\n         \\\\\n         \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab: LM ablation": "\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\caption{{Effect of different LMs on \\texttt{ogbn-arxiv}}}\n    \\label{tab: LM ablation}\n    \\begin{tabular}{lcccc}\n    \\toprule\n    {LM} \n    & {MLP} \n    & {GCN} \n    & {SAGE} \n    & {RevGAT}\n    \\\\\n    \\midrule\n    {deberta-base}\n    & {0.7587 \u00b1 0.0015}\n    & {0.7520 \u00b1 0.0003}\n    & {0.7672 \u00b1 0.0007}\n    & {0.7750 \u00b1 0.0012}\n    \\\\\n         \n    {all-roberta-large-v1} \n    & {0.7587 \u00b1 0.0003}\n    & {0.7412 \u00b1 0.0015}\n    & {0.7695 \u00b1 0.0008 }\n    & {0.7737 \u00b1 0.0004}\n     \\\\\n    {e5-large}\n     & {0.7595 \u00b1 0.0015}\n     & {0.7443 \u00b1 0.0021}\n     & {0.7688 \u00b1 0.0010}\n     & {0.7730 \u00b1 0.0006}\n     \\\\\n     \\bottomrule\n    \\end{tabular}\n    \n\\end{table}",
            "tab: memory usage": "\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\caption{{Memory Usage on \\texttt{ogbn-arxiv} dataset with DeBERTa-base ad LM backbone and RevGAT as GNN backbone for comparision of different training paradigms of fusing LMs and GNNs, including our proposed method and the state-of-the-art GLEM method. All experiments are performed on 4 NVIDIA RTX A5000 24GB GPUs with a batch size of 36.}}\n    \\label{tab: memory usage}\n    \\begin{tabular}{lccc}\n    \\toprule\n         \\multirow{2}{*}{{Model}} \n         & \\multicolumn{2}{c}{{Memory}} \n         & \\multirow{2}{*}{{Accuracy}}\\\\\n         \\cmidrule(lr){2-3}\n         & {LM} & {GNN} \\\\\n         \\midrule\n         {Pure LM}\n         & {8,834 MB}\n         & {--}\n         & {0.7361 \u00b1 0.0004}\n         \\\\\n         {GNN w/ shallow feature}\n         & {--}\n         & {4,430 MB}\n         & {0.7083 \u00b1 0.0017}\n         \\\\\n         {LM-based GLEM}\n         & {11,064 MB}\n         & {8,112 MB}\n         & {0.7657 \u00b1 0.0029}\n         \\\\\n         {LLM-based TAPE (Ours)}\n         & {8,834\tMB}\n         & {4,430 MB}\n         & {0.7750 \u00b1 0.0012}\n         \\\\\n         \\bottomrule\n    \\end{tabular}\n    \n\\end{table}",
            "rebuttal tab: glem": "\\begin{table}[!ht]\n      \\centering\n        \\caption{GLEM~\\citep{zhao2022learning_em}}\n    \\label{rebuttal tab: glem}\n    \\centering\n    \\small\n    \\begin{tabular}{lccc}\n    \\toprule\n         Dataset &  GCN & SAGE & RevGAT\\\\\n         \\midrule\n        {\\texttt{Cora}} \n        & {0.8732 \u00b1 0.0066} \n        & {0.8801 \u00b1 0.0054} \n        & {0.8856 \u00b1 0.006}\\\\\n        {\\texttt{PubMed}} & \n        {0.9469 \u00b1 0.0010}\n        & {0.9459 \u00b1 0.0018}\n        & {0.9471 \u00b1 0.002}\\\\\n        \\texttt{ogbn-arxiv} & 0.7593 \u00b1 0.0019 & 0.7550 \u00b1 0.0024 & 0.7697 \u00b1 0.0019\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig: overview": "\\begin{figure}[t]\n\\vspace{-0.6cm}\n    \\centering\n    \\includegraphics[scale=0.41]{figs/overview-v14.pdf}\n    \\caption{Our framework leverages large language models (LLMs) to enhance representation learning on TAGs. First, textual attributes of each node, \\ie title and abstract, are wrapped in a custom prompt (green box) to query the LLM, here GPT-3.5~\\citep{brown2020language_gpt}, which generates a ranked prediction list and explanation (yellow box). Next, the original text, predictions, and explanation are used to fine-tune a language model (LM), here DeBERTa~\\citep{he2021deberta}, and then transformed into vectorial node features. Finally, these enriched node features, \\ie $h_\\textrm{orig}$, $h_\\textrm{expl}$ and $h_\\textrm{pred}$, are used in any downstream GNN, \\eg RevGAT~\\citep{li2021training_revgat} to predict unknown node classes.}\n    \\label{fig: overview}\n    \\vspace{-0.4cm}\n\\end{figure}",
            "fig: comparison": "\\begin{figure}[t]\n\\vspace{-0.5cm}\n    \\centering\n    \\includegraphics[scale=0.45]{figs/comparison-v8.pdf}\n    \\caption{The performance trade-off between node classification accuracy and total training time on \\texttt{ogbn-arxiv}~\\citep{hu2020open} for various training approaches that combine language models (LMs) and graph neural networks (GNNs). The experiment employs DeBERTa-base~\\citep{he2021deberta} as the LM backbone and RevGAT~\\citep{li2021training_revgat} as the GNN backbone,  with the size of the marker indicating the number of parameters.}\n    \\label{fig: comparison}\n    \\vspace{-0.25cm}\n\\end{figure}",
            "fig: casestudy": "\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[scale=0.15]{figs/comparison-v7.pdf}\n    \\caption{Case study comparing features for node classification on the \\texttt{PubMed} dataset: (a) Original text attributes and (b) Explanations generated by LLMs. The GNN model trained with (b) accurately predicts the label for node 12390 (type 2 diabetes), while the model trained with (a) predicts the incorrect label (experimentally induced diabetes). This improvement can be attributed to the concise and focused nature of LLM-generated explanations, as well as their reasoning ability and utilization of external knowledge.}\n\\label{fig: casestudy}\n\\end{figure}",
            "fig: ablation2": "\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[clip, trim=1cm 24cm 2cm 1.5cm, width=0.8\\textwidth]{figs/ablation2-v2.pdf} \n    \\caption{Effect of node features. We study the effects of different sources of node features on the \\texttt{ogbn-arxiv} dataset, \\ie original text features ($h_\\textrm{orig}$), explanation as features ($h_\\textrm{expl}$) and predictions as features ($h_\\textrm{pred}$), by removing one of them in turn from our model while keeping the other components unchanged.}\n    \\label{fig: ablation2}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    h_n=\\textrm{LM}(s_n) \\in \\mathbb{R}^d,\n\\end{equation}",
            "eq:2": "\\begin{equation}\np(y|\\hat{x})=\\prod_{i=1}^m p(y_i|y_{<i}, \\hat{x}),\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    h_i^{k} = f^{k}(h_i^{k-1}, \\, \\textrm{AGG}(\\{h_j^{k-1}: j\\in \\mathcal{N}_i\\})) \\in \\mathbb{R}^d,\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\begin{split}\n        h_{\\textrm{orig}} = \\textrm{LM}_{\\textrm{orig}}(s^{\\textrm{orig}})\\in \\mathbb{R}^{N\\times d}, \\quad \n        h_{\\textrm{expl}} = \\textrm{LM}_\\textrm{expl}(s^{\\textrm{expl}})\\in \\mathbb{R}^{N\\times d}.\n    \\end{split}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\begin{split}\n        y_{\\textrm{orig}}= \\textrm{MLP}_{\\textrm{orig}}(h_{\\textrm{orig}}) \\in \\mathbb{R}^{N\\times C}, \\quad \n        y_{\\textrm{expl}}= \\textrm{MLP}_\\textrm{expl}(h_{\\textrm{expl}}) \\in \\mathbb{R}^{N\\times C}.\n    \\end{split}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\begin{split}\n\\hat y_{\\textrm{orig}/\\textrm{expl}/\\textrm{pred}} = f_{\\textrm{orig}/\\textrm{expl}/\\textrm{pred}}(h_{\\textrm{orig}/\\textrm{expl}/\\textrm{pred}}, A) \\in \\mathbb{R}^{N\\times C}.\n\\end{split}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\hat y = \\textrm{mean}(\\hat y_{\\textrm{orig}}, \\hat y_{\\textrm{expl}}, \\hat y_{\\textrm{pred}}) \\in \\mathbb{R}^{N\\times C}.\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\textit{Cost} = ((285 \\times 0.0015) / 1000 + (164 \\times 0.002) / 1000) \\times 169,343 \\approx 128 \\,\\textit{USD}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\textit{Time} = 169,343/ (90,000/285) \\approx 536 \\textit{min} \\approx 9\\textit{h}\n\\end{equation}"
        },
        "git_link": "https://github.com/XiaoxinHe/TAPE"
    }
}