{
    "meta_info": {
        "title": "Classification With an Edge: Improving Semantic Image Segmentation with  Boundary Detection",
        "abstract": "We present an end-to-end trainable deep convolutional neural network (DCNN)\nfor semantic segmentation with built-in awareness of semantically meaningful\nboundaries. Semantic segmentation is a fundamental remote sensing task, and\nmost state-of-the-art methods rely on DCNNs as their workhorse. A major reason\nfor their success is that deep networks learn to accumulate contextual\ninformation over very large windows (receptive fields). However, this success\ncomes at a cost, since the associated loss of effecive spatial resolution\nwashes out high-frequency details and leads to blurry object boundaries. Here,\nwe propose to counter this effect by combining semantic segmentation with\nsemantically informed edge detection, thus making class-boundaries explicit in\nthe model, First, we construct a comparatively simple, memory-efficient model\nby adding boundary detection to the Segnet encoder-decoder architecture.\nSecond, we also include boundary detection in FCN-type models and set up a\nhigh-end classifier ensemble. We show that boundary detection significantly\nimproves semantic segmentation with CNNs. Our high-end ensemble achieves > 90%\noverall accuracy on the ISPRS Vaihingen benchmark.",
        "author": "Dimitrios Marmanis, Konrad Schindler, Jan Dirk Wegner, Silvano Galliani, Mihai Datcu, Uwe Stilla",
        "link": "http://arxiv.org/abs/1612.01337v2",
        "category": [
            "cs.CV"
        ],
        "additionl_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\\label{introduction}\n\nSemantic image segmentation (a.k.a.\\ landcover classification) is the\nprocess of turning an input image into a raster map, by assigning every\npixel to an object class from a predefined class nomenclature.\n%\nAutomatic semantic segmentation has been a fundamental problem of\nremote sensing data analysis for many\nyears~\\citep{Fu1969,Richards2013}.\n%\nIn recent years there has been a growing interest to perform semantic\nsegmentation also in urban areas, using conventional aerial images or\neven image data recorded from low-flying drones.\n%\nImages at such high resolution (GSD 5-30$\\,$cm) have quite different\nproperties.\n%\nIntricate spatial details emerge like for instance road markings, roof\ntiles or individual branches of trees, which increase the spectral\nvariability within an object class. On the other hand, the spectral\nresolution of sensors is limited to three or four broad bands\n%\nso spectral material signatures are less distinctive.\n%\nHence, a large portion of the semantic information is encoded in the\nimage texture rather than the individual pixel intensities, and much\neffort has gone into extracting features from the raw images that make\nthe class information\nexplicit~\\citep[e.g.][]{Franklin1993,Barnsley1996,DallaMura2010,Tokarczyk2015}.\n\nAt present the state-of-the-art tool for semantic image segmentation,\nin remote sensing as well as other fields of image analysis, are deep\nconvolutional neural networks (DCNNs).%\n%\n\\footnote{For example, the 15 best-performing participants in the\n  \\emph{ISPRS Vaihingen} benchmark, not counting multiple entries from\n  the same group, all use DCNNs.} %\n%\nFor semantic segmentation one uses so-called fully convolutional\nnetworks (\\textsc{fcn}s), which output the class likelihoods for an entire\nimage at once. \\textsc{fcn}s have become a standard tool that is readily\navailable in neural network software.\n\n\n\n\n\n\nWhy are DCNNs so successful (if given sufficient training data and\ncomputing resources)? Much has been said about their ability to learn\nthe complete mapping from raw images to class labels (``end-to-end\nlearning''), thus making heuristic feature design obsolete.\n%\nAnother strength is maybe even more important for their excellent\nperformance: deep networks capture a lot of \\emph{context} in a\ntractable manner. Each convolution layer combines information\nfrom nearby pixels, and each pooling layer enlarges the footprint\nof subsequent convolutions in the input image. Together, this means\nthat the output at a given pixel is influenced by a large spatial\nneighborhood.\n%\nWhen the task is pixel-wise semantic segmentation%\n%\n\\footnote{As opposed to, e.g., object recognition or speech\n  recognition.}, %\n%\ntheir unparalleled ability to represent context however comes at a\nprice. There is a trade-off between strong downsampling, which allows\nthe network to see a large context, but loses high-frequency detail;\nand accurate localization of the object boundaries, which requires\njust that local detail.\n%\nWe note that in generic computer vision with close-range images that\neffect is much less critical. In a typical photo, say a portrait or a\nstreet scene, there are few, big individual objects and only few\nobject boundaries, whose precise location moreover is only defined up\nto a few pixels.\n%\nIn some cases segmentation is even defined as finding the\nboundaries between very few (say, $<5$) dominant\nregions~\\citep{russell2009segmenting} that make up the image.\n%\nOn the contrary, for our remote sensing task we expect at least tens to\nhundreds of small segments, such as individual cars, trees, etc.\n\nThere has been some research that tries to mitigate the blurring of\nboundaries due to down-sampling and subsequent up-sampling, either by\nusing the a-trous convolution (dilated convolution)\n~\\citep{Yu2016multiscale,ChenPK0Y16,sherrah2016fully}\nor by adding skip connections from early to deep layers of the\nnetwork, so as to reintroduce the high-frequency detail after\nupsampling~\\citep{Dosovitskij2015, badrinarayanan2017segnet,\nmarmanis2016semantic}.\n%\nStill, we find that when applied to remote sensing data with many small\nobjects, \\fcns tend to blur object\nboundaries and visually degrade the result (see Fig.1).\n\nIn this paper, we propose to explicitly represent \\emph{class-boundaries}\nin the form of pixel-wise contour likelihoods, and to\ninclude them in the segmentation process. By class-boundaries we mean\nthe boundaries between regions that have different semantic class,\ni.e., we aim for a ``semantic edge-detection''.\n%\nOur hypothesis is that if those boundaries, which by definition\ncorrespond to the location of the label transitions, are made\navailable to the network, then it should learn to align the\nsegmentation to them.\n\nImportantly, recent work, in particular the \\emph{holistically nested\n  edge detection} \\citep[\\hed][]{xie2015holistically,\n  Kokkinos2016pushing} has shown that edge detection can also be\nformulated as a \\textsc{fcn}, reaching excellent results.\n%\nWe can therefore merge the two tasks into a single network, train them\ntogether and exploit synergies between them.\n%\nThe result is an end-to-end trainable model for semantic segmentation\nwith a built-in awareness of semantically meaningful boundaries.\n%\nWe show experimentally that explicitly taking into account class\nboundaries significantly improves labeling accuracy, for our datasets\nup to $6$\\%.\n\nOverall, our boundary-aware ensemble segmentation network reaches\nstate-of-the-art accuracy on the ISPRS semantic labeling\nbenchmark.\n%\nIn particular, we find that adding boundary detection consistently\nimproves the segmentation of man-made object classes with well-defined\nboundaries. On the contrary, we do not observe an improvement for\nvegetation classes, which have intrinsically fuzzy boundaries at our\ntarget resolution.\n%\nMoreover, our experiments suggest that integrated boundary detection\nis beneficial both for light encoder/decoder architectures with\ncomparatively few parameters like \\segnet, and for high-performance\nnetworks with \\emph{fully connected layers}, such as the \\textsc{vgg}\nfamily, which are much heavier to train in terms of memory usage and\ncomputation time.\n\nOur tests also confirm, perhaps unsurprisingly, that DCNNs perform optimally when\nmerged into \\textit{ensemble models}. Combining multiple semantic\nsegmentation networks seems beneficial to reduce the bias of\nindividual models, both when using the same architecture with\ndifferent initializations, and when using different model\narchitectures with identical initializations.\n\nIn terms of practical relevance, a main message of this paper is that,\nwith DCNNs, semantic segmentation is practically usable also for very\nhigh resolution urban remote sensing.\n%\nIt is typically thought that object extraction algorithms are good\nenough for (possibly semi-automatic) applications when their\npredictions are at least 80\\% correct~\\citep{mayer2006}.\n%\nIn our experiments, the $F_1$-score (harmonic mean between precision\nand recall) surpasses this threshold for all object classes, including\nsmall ones like cars. Frequent, well-defined man-made classes reach\nwell above 90\\%.\n\n\n%============\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\\label{sec:related}\n\nSemantic segmentation has been a core topic of computer vision as well\nas remote sensing for many years. A full review is beyond the scope of\nthis paper, we refer the reader to textbooks such\nas~\\citep{Szeliski2010,Richards2013}. Here, we concentrate on methods\nthat employ neural networks.\n\nEven in the early phase of the CNN revival, semantic segmentation\nwas tackled by \\citet{grangier2009deep}. The study investigates\nprogressively deeper CNNs for predicting pixel labels from a local\nneighborhood, and already shows very promising results, albeit at\ncoarse spatial resolution.\n%\n\\citet{SocherEtAl2012:CRNN} start their semantic segmentation pipeline\nwith a single convolution and pooling layer. On top of that ``feature\nextractor'' they stack recursive neural networks (RNN), which are\nemployed on local blocks of the previous layer in a rather convolutional\nmanner. The RNNs have random weights, there is no end-to-end\ntraining. Notably, that work uses RGB-D images as input and processes\ndepth in a separate stream, as we do in the present work.\n%\nRelated to this is the approach of \\citet{pinheiro2014recurrent}, who\nuse ``recurrent CNNs'', meaning that they stack multiple shallow\nnetworks with tied convolution weights on top of each other, at each\nlevel using the predicted label maps from the previous level and an\nappropriately down-scaled version of the raw image as input.\n%\n\\citet{farabet2013learning} train CNNs with $3$ convolutional layers and\na fully connected layer for semantic segmentation, then post-process\nthe results with a CRF or by averaging over super-pixels to obtain a\nsmooth segmentation. Like in our work, that paper generates an image\npyramid and processes each scale separately with the CNN, but in\ncontrast to our work the filter weights are tied across scales.\n%\nAn important milestone was the \\emph{fully convolutional network}\n(\\textsc{fcn}) of \\citet{long2015fully}. In that work it was shown\nthat the final, fully connected layers of the network can be seen as a\nlarge stack of convolutions, which makes it possible to compute\nspatially explicit label maps efficiently.\n%\nAn further important work in this context is the \\emph{Holistically-\nNested Edge Detection} (\\textsc{hed}) of~\\citet{xie2015holistically}, who\nshowed that an \\fcn\\ trained to output edge maps instead of class\nlabels is also an excellent edge detector. Their network was\ninitialized with the \\vgg object detection network, so arguably the\nedge detection is supported by the semantic information captured in\nthat network.\n%\nVariants of \\hed have been explored by other authors,\n\\citep{Kokkinos2016pushing} confirming that CNNs are at present also\nthe state of the art for edge detection.\n\nTo undo the loss of spatial resolution due to the pooling layers of\n\\textsc{fcn}, \\citet{noh2015learning} propose to add an unpooling and\nupsampling (``deconvolution'') network~\\citep{zeiler2014visualizing}\non top of it. The result is a sort of encoder-decoder structure that\nupsamples the segmentation map back to the resolution of the input\nimage.\n%\n\\citet{yang2016object} employ a similar strategy for the opposite\npurpose: their primary goal is not semantic labeling but rather a\n``semantically informed'' edge detection which accentuates edges that\nlie on object contours.\n%\nAlso related is the work of \\citet{bertasius2015high}. They find\ncandidate edge pixels with conventional edge detection, read out the\nactivations at those pixels from the convolution layers of the\n(frozen) \\vgg object detection network, and separately train a\nclassifier on the vector of activations to separate object boundaries\nfrom other edges.\n%\nFor semantic segmentation, it has also been proposed to additionally\nadd skip connections from lower layers in the encoder part (before\npooling) to corresponding decoder layers (after unpooling) so as to\nre-inject high-frequency image contours into the upsampling process\n~\\citep{Dosovitskij2015, marmanis2016semantic}.\n%\nThis architecture has been simplified by the \\segnet model of\n\\citet{badrinarayanan2017segnet}: the fully connected layers are\ndiscarded, which drastically reduces the number of free\nparameters. Moreover, that architecture makes it possible to keep\ntrack of pixel indices during max-pooling and restore the values to\nthe correct position during unpooling.\n\nIn the context of individual object detection it has been\nproposed to train the encoder/detector part first, freeze it, and\ntrain the decoder part separately~\\citep{pinheiro2016learning}.\n%\nThe \\deeplab network of \\citet{chen2015semantic-a} explores a different\nupsampling strategy: low-resolution output from the \\fcn\\ is first\nupsampled bilinearly, then refined with a fully connected\nCRF~\\citep{Kraehenbuehl2011} whose pairwise potentials are modulated\nby colour differences in the original image.\n%\nLater \\deeplab was extended to simultaneously learn edge\ndetection and semantic segmentation~\\citep{chen2015semantic}. This is\nperhaps the work most closely related to ours, motivated by the same\nintuition that there are synergies between the two tasks, because\nobject boundaries often coincide with edges.\n%\nGoing even further, \\citet{dai2016instance} construct a joint network\nfor detecting object instances, assigning them to a semantic class,\nand extracting a mask for each object -- i.e., per-object class\nboundaries. The method is very efficient for well-localized compact\nobjects (``things''), since the object instances can be detected first\nso as to restrict subsequent processing to those regions of\ninterest. On the other hand, it appears less applicable to remote\nsensing, where the scene is to a large extent composed of objects\nwithout a well-defined bounding box (``stuff'').\n\n\nRegarding applications in remote sensing, shallow neural networks were\nalready used for semantic segmentation before the advent of deep\nlearning, e.g.~\\citet{Bischof1992} use a classical multi-layer\nperceptron to predict the semantic class of a pixel from a small\nneighborhood window.\n%\nShallow architectures are still in use:\n\\citet{malmgren2015convolutional} train a relatively shallow CNN with\n3 convolution layer and 2 pooling layers to classify pixels in SAR\nimages.\n%\n\\citet{langkvist2016classification} make per-pixel predictions with\nshallow CNNs (where the convolution weights are found by clustering\nrather than end-to-end training) and smooth the results by averaging\nover independently generated super-pixels.\n%\nThe mentioned works predict individually for each pixel, on the\ncontrary~\\citet{mnih2010learning} have designed a shallow, fully\nconnected network for patch-wise prediction of road pixels.\n%\nAn also relatively shallow \\fcn\\ with 3 convolution layers and 1\npooling layer is used in~\\citep{saito2016multiple}.\n%\n\nIn the last few years, different deep CNN variants have been proposed\nfor semantic segmentation of remote sensing images.\n\\citet{paisitkriangkrai2015effective} learn three separate 6-layer\nCNNs that predict semantic labels for a single pixel from three\ndifferent neighborhoods. The scores are averaged with those of a\nconventional random forest classifier trained on per-pixel features,\nand smoothed with a conditional random field.\n%\n\\citet{marcu2016dual} design a network for patchwise 2-class\nprediction. It takes as input patches of two different sizes (to\nrepresent local and global context), passes them through separate deep\nconvolutional architectures, and combines the results in three deep,\nfully connected layers to directly output 16$\\times$16 patches of\npairwise labels.\n\nMore often, recent works adopt the \\fcn architecture. Overall, the\nresults indicate that the empirical findings from computer vision\nlargely translate to remote sensing images. Both our own work\n\\citep{marmanis2016semantic} and \\citet{sherrah2016fully} advocate a\ntwo-stream architecture that learns separate convolution layers for\nthe spectral information and the DSM channel, and recommend to start\nfrom pretrained networks for the spectral channels. Our work further\nsupports the practice of training multiple copies of the same CNN\narchitecture and averaging their results~\\citep{marmanis2016semantic},\nand~\\citet{sherrah2016fully} reports that the a-trous convolution\ntrick slightly mitigates the information loss due to pooling, at the\ncost of much larger (40$\\times$) computation times.\n%\n\\citet{mou2016spatiotemporal} prefer to avoid upsampling altogether,\nand instead combine the coarser semantic segmentation with a\nsuper-pixel segmentation of the input image to restore accurate\nsegmentation boundaries (but not small objects below the scale of the\n\\fcn output). Of course, such a strategy cannot be trained end-to-end\nand heavily depends on the success of the low-level super-pixel\nsegmentation.\n\nA formal comparison between per-pixel CNNs and \\fcns has been carried\nout by \\citet{volpi2017dense}. It shows advantages for \\textsc{fcn}, but\nunfortunately both networks do not attain the state of the art,\npresumably because their encoder-decoder network lacks skip\nconnections to support the upsampling steps, and has been trained from\nscratch, losing the benefit of large-scale pretraining.\n%\nA similar comparison is reported in \\citep{kampffmeyer2016semantic},\nwith a single upsampling layer, and also trained from scratch. Again\nthe results stay below the state-of-the-art but favor\n\\textsc{fcn}. Median-balancing of class frequencies is also tested, but seems to\nintroduce a bias towards small classes. An interesting aspect of that\npaper is the quantification of the network's prediction uncertainty,\nbased on the interpretation of drop-out as approximate Bayesian\ninference~\\citep{gal2016dropout}. As expected, the uncertainty is highest\nnear class-contours.\n\n%============\n"
            },
            "section 3": {
                "name": "The Model",
                "content": "\\label{sec:method}\n\nIn the following, we describe our network architecture for\nboundary-aware semantic segmentation in detail.\n%\nFollowing our initial hypothesis, we include edge detection early in\nthe process to support the subsequent semantic labeling.\n%\nAs further guiding principles, we stick to the deep learning paradigm\nand aim for models that can be learned end-to-end; we build on\nnetwork designs, whose performance has been independently confirmed;\nand, where possible, we favor efficient, lean networks with\ncomparatively few tunable weights as primary building blocks.\n\nWhen designing image analysis pipelines there is invariably a\ntrade-off between performance and usability, and DCNNs are no\nexception. One can get a respectable and useful result with a rather\nelegant and clean design, or push for maximum performance on the\nspecific task, at the cost of a (often considerably) more complex and\nunwieldy model.\n%\nIn this work we explore both directions: on the one hand, our basic\nmodel is comparatively simple with $8.8\\cdot10^7$ free parameters\n(\\hedh{}+\\segneth, single-scale; see description below), and can be\ntrained with modest amounts of training data.\n%\nOn the other hand, we also explore the maximum performance our\napproach can achieve. Indeed, our high-end model, with multi-scale\nprocessing and ensemble learning, achieves $>$90\\% overall accuracy on\nthe ISPRS Vaihingen benchmark. But diminishing returns mean that this\nrequires a more convoluted architecture with $9\\times$ higher memory\nfootprint and training time.\n\nSince remote sensing images are too large to pass through a CNN, all\ndescribed networks operate on tiles of $256\\times 256$\npixels. We classify overlapping tiles with\nthree different strides (150, 200, and 220 pixels) and sum the\nresults.\n\nWe start by introducing the building blocks of our model, and then\ndescribe their integration and the associated technical details.\n%\nThroughout, we refrain from repeating formal mathematical definitions\nfor their own sake. Equation-level details can be found in the\noriginal publications. Moreover, we make our networks publicly\navailable to ensure repeatability.%\n\\footnote{\\scriptsize{\\url{https://github.com/deep-unlearn/ISPRS-Classification-With-an-Edge}}}\n\n\n% =========== Sub Parts ============= %\n\n",
                "subsection 3.1": {
                    "name": "Building blocks",
                    "content": "\n\n",
                    "subsubsection 3.1.1": {
                        "name": "\\segneth encoder-decoder network",
                        "content": "\n\n\\segnet \\citep{badrinarayanan2017segnet} is a crossbreed between a\nfully convolutional network~\\citep{long2015fully} and an\nencoder-decoder architecture. The input image is first passed through\na sequence of convolutions, \\emph{ReLU} and $max$-pooling layers.\nDuring $max$-pooling, the network tracks the spatial location of the\nwinning maximum value at every output pixel.  The output of this\n\\emph{encoding} stage is a representation with reduced spatial\nresolution. That ``bottleneck'' forms the input to the \\emph{decoding}\nstage, which has the same layers as the encoder, but in reverse\norder. Max-pooling layers are replaced by \\emph{unpooling}, where the\nvalues are restored back to their original location, then convolution\nlayers interpolate the higher-resolution image.\n\nSince the network does not have any fully connected layers (which\nconsume $>$90\\% of the parameters in a typical image processing CNN)\nit is much lighter. \\segnet is thus very memory-efficient and\ncomparatively easy to train.\n%\nWe found, in agreement with its creators, that \\segnet on its\nown does not always reach the performance of much heavier architectures\nwith fully connected layers.\n%\nHowever, it turns out that \\emph{in combination with learned class\n  boundaries}, \\segnet matches the more expensive competitors,\nsee Section~\\ref{sec:experiments}.\n\n\nOur variant, denotes as \\segneth, consists of two parallel \\segnet\nbranches, one for the colour channels and one for the digital\nelevation model (DEM).\n%\nThe colour branch is initialized with the existing \\segnet weights, as\ntrained on the Pascal dataset%\n%\n\\footnote{\\scriptsize{\\url{http://mi.eng.cam.ac.uk/~agk34/resources/SegNet/segnet_pascal.caffemodel}}} %\nby \\citet{badrinarayanan2017segnet}.\nThe second branch processes a two-channel image consisting of nDSM and\nDSM, and is initialized randomly using \"Xavier\" weight initialization,\na technique designed to keep the gradient magnitude roughly the same\nacross layers~\\citep{glorot2010}.\n%\nThe outputs from the two streams are then concatenated, and fed\nthrough a 1$\\times$1 convolution that linearly combines the vector of\nfeature responses at each location into a score per class. Those class\nscores are further converted to probabilities with a \\textit{softmax}\nlayer.\n\n\\citet{Kokkinos2016pushing} reported significant quantitative\nimprovements by an explicit multi-scale architecture, which passes\ndown-scaled versions of the input image through identical copies of\nthe network and fuses the results.\n%\nGiven the small size of \\segneth we have also experimented with that\nstrategy, using three scales. We thus set up three copies of the\ndescribed two-stream \\segneth with individual per-scale weights. Their\nfinal predictions, after fusing the image and height streams, are\nupsampled as needed with fractional stride convolution layers%\n%\n\\footnote{Sometimes inaccurately called ``deconvolution''\n  layers. Technically, these layers perform convolution, but sample\n  the input feature maps from the previous layer with a\n  stride $<$1. E.g., a stride of $\\frac{1}{2}$ will double the\n  resolution.} %\n%\nand fused before the final prediction of the class scores.\n%\nThe multi-scale strategy only slightly improves the results by\n$<$0.5\\%, presumably because remote sensing images, taken in nadir\ndirection from distant viewpoints, exhibit only little perspective\neffect and thus less scale variation (only due to actual scale\nvariability in metric object coordinates).\n%\nStill, the small improvement is consistent over all tiles of our\nvalidation set, hence we include the multi-scale option in the\nhigh-end variant of our system.\n\n"
                    },
                    "subsubsection 3.1.2": {
                        "name": "\\hedh boundary-detection network",
                        "content": "\n\nWe aim to include explicit boundary information in our processing,\nthus we require an edge detector that can be integrated into the\nlabeling framework.  In a nutshell, \\hed (Holistically-Nested Edge\nDetection) is an multi-scale encoder-decoder CNN trained to output an\nimage of edge likelihoods.\n%\nAn important feature of \\hed is that it uses multi-scale prediction in\nconjunction with\ndeep supervision~\\citep{lee2015deeply}. That is, the\n(rectified) output of the last convolution before each pooling layer\nis read out of the network as prediction for that particular scale,\nand supervised during training by an additional (Euclidean distance)\nloss function. The multi-scale predictions are then combined into a\nfinal boundary probability map.\n%\nImportantly, even though \\hed has no explicit mechanism to enforce\nclosed contours, we find that by learning from global image context it\ndoes tend to generate closed contours, which is important to support\nour segmentation task.\n\nFor our purposes, we again add a second branch for the DSM, and modify\n\\hed to better take into account the uncertainty of boundary\nannotations, by using a regression loss w.r.t.\\ a continuous\n``boundary score'' $\\mathbf{y}$ rather than a hard classification loss.\n%\nThe intuition is that the location of a class boundary has an inherent\nuncertainty. We therefore prefer to embrace and model the uncertainty,\nand predict the ``likelihood'' of the boundary being present at a\ncertain pixel. The advantage is best explained by looking at pixels\nvery close to the annotated boundary: using those pixels as negative\n(i.e., background) training samples will generate label noise; using\nthem as positive samples contradicts the aim to predict precise\nboundary locations; and excluding them from the training altogether\nmeans depriving the network of training samples close to the boundary,\nwhere they matter most.\n%\nFor our task, the desired edges are the segment boundaries, i.e., the\nlabel transitions in the training data. To turn them into soft scores\nwe dilate the binary boundary image with a diamond structuring\nelement. The structuring element defines the width of the\n``uncertainty band'' around an annotated boundary and depends on the\nground sampling distance.\n\nThen, we weight each class-boundary pixel according\n  to its truncated Euclidean distance to the nearest background pixel,\n  using the distance transform: $Y=\\beta\\cdot D_{t}^{\\ell\n    2}(B_d)$.\n%\nThe operator $D_{t}^{\\ell 2}(B_d)$ denotes the truncated Euclidean\ndistance from a particular pixel to the nearest background pixel.\n%\nThe factor $\\beta=\\frac{|B_d=0|}{|B_d|}$ compensates the relative\nfrequencies of boundary and background pixels, to avoid overfitting to\nthe dominant background.\n%\nFinally, the weights are normalized to the interval $[0\\hdots 1]$\nto be consistent with the original \\hed model.\n%\nAs above, we set up two separate streams for color images and for the\nheight also during boundary detection, and refer to our version as\n\\hedh. The image stream is initialized with the original \\hed\nweights, whereas the DEM stream is trained from scratch. For output\n(including side outputs) the two streams are fused by concatenation,\nconvolution with a $1\\times1$ kernel and fractional convolution to the\noutput resolution.\n%\nA graphical overview of the boundary network is given in\nFigure~\\ref{fig:edge-network}, visual results\nof class-boundaries are shown in Figure~\\ref{fig:class-contours-examples}.\n%\n\n\n\n\n\\FloatBarrier\n\n"
                    },
                    "subsubsection 3.1.3": {
                        "name": "\\dlreth semantic segmentation network",
                        "content": "\n\nFrom the literature~\\citep{simonyan2014very} it is known that, also\nfor strong classifiers like CNNs, ensemble averaging tends to improve\nclassification results.\n%\nWe thus also experiment with an ensemble of multiple CNNs.\n%\nTo that end we use our previous semantic segmentation network, here\ntermed \\dlreth, which has already shown competitive performance for\nremote sensing problems~\\citep{marmanis2016semantic}.\n%\nThat model in fact is an ensemble of two identical \\fcn architectures\ninitialized with different weights, namely those of \\emph{VGG} and\n\\emph{Pascal}, see \\cite{marmanis2016semantic}.\n%\nFor clarity we refer to the ensemble as \\dlreth, and to its two\nmembers as \\dlrethVGG, respectively \\dlrethPAS.\n%\nContrary to \\segneth this model is derived from the standard \\fcn and\nhas fully connected layers. It is thus heavier in terms of memory and\ntraining, but empirically improves the predictions, presumably because\nthe network can learn extensive high-level, global object information.\n%\nCompared to the original \\fcn~\\citep{long2015fully}, our variant has\nadditional skip connections to minimize information loss from\ndownsampling. As above, it has separate streams for image and DEM\ndata.\n\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Integrated class boundary detection and segmentation",
                    "content": "\n\nOur strategy to combine class boundary detection with semantic\nsegmentation is straight-forward: we append the class-boundary network\n(\\hedh) before the segmentation network (\\dlreth or \\segneth) and\ntrain the complete architecture.\n%\nIn the image stream, the input is the raw colour image; in the DSM\nstream it is a 2-channel image consisting of the raw DSM and the nDSM\n(generated by automatically filtering above-ground objects and\nsubtracting the result from the DSM).\n%\nIn both cases, the input is first passed through the \\hedh network\n(Figure~\\ref{fig:full-architectures}(top)), producing a scalar image\nof boundary likelihoods.\n%\nThat image is concatenated to the raw input as an additional channel\nto form the input for the corresponding stream of the semantic\nsegmentation network (note, in CNN language this can be seen as a skip\nconnection that bypasses the boundary detection layers).\n%\nFor \\segneth, a further skip connection bypasses most\nof the segmentation network and reinjects the colour image boundaries\nas an extra channel after merging the image and DSM streams, see\nFigure~\\ref{fig:full-architectures}(middle). This additional skip\nconnection re-introduces the class-boundaries deep into the\nclassifier, immediately before the final label prediction.\n%\nFor \\dlreth this did not seem necessary, since the\narchitecture already includes a number of long skip connections from\nrather early layers, see Figure~\\ref{fig:full-architectures}(bottom).\n%\nThe entire processing chain is trained end-to-end (see below for\ndetails), using boundaries as supervision signal for the \\hedh layers\nand segmentations for the remaining network.\n\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Ensemble learning",
                    "content": "\n\nAs discussed above, ensemble averaging typically improves DCNN\npredictions further.\n%\nWe thus also test that strategy and combine the predictions of three\ndifferent boundary-aware networks (given the effort to train deep\nnetworks, ``ensembles'' are typically small).\n%\nAs described above, we have trained two versions of the \\dlreth\nnetwork with integrated \\hedh boundary detector, one initialized with\nthe original \\fcn{} (Pascal VOC) weights%\n%\n\\footnote{\\scriptsize{\\url{http://dl.caffe.berkeleyvision.org/fcn32s-heavy-pascal.caffemodel}}} %\n%\nand the other initialized with weights from the \\vgg-16 variant.%\n%\n\\footnote{\\scriptsize{\\url{http://www.robots.ox.ac.uk/~\n    vgg/software/very\\_deep/caffe/VGG\\_ILSVRC\\_16\\_layers.caffemodel}}} %\n%\nTheir individual performance is comparable, with \\dlrethVGG\nslightly better overall. In both cases, the DEM channel is again\ntrained from random initializations.\n\nEnsemble predictions are made by simply averaging the\nindividual class probabilities predicted by \\dlrethVGG, \\dlrethPAS and \\segneth.%\n%\n\\footnote{We did not experiment with trained fusion layers, since the\n  complete ensemble is too large to fit into GPU memory.} %\n%\n%\nEmpirically the ensemble predictions give a significant performance\nboost, seemingly \\dlreth and\n\\segneth are to some degree complementary,\nsee experimental results in Section~\\ref{sec:experiments}.\n%\nNote though, the two \\dlreth models, each with $>$100 million\nparameters (see section \\ref{sec:experiments}), are memory-hungry and\nexpensive to train, thus we do not generally recommend such a\nprocedure, except when aiming for highest accuracy.\n%\nFigure~\\ref{fig:ensemble-workflow} depicts the complete ensemble.\n%\n\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Implementation details and training",
                    "content": "\n\nThe overall network with boundary detection is fairly deep, and the\nboundary and labeling parts use different target outputs and loss\nfunctions.\n%\nWe found that under these circumstances training must proceed in\nstages to achieve satisfactory performance, even if using pre-trained\ncomponents.\n%\nA cautious strategy, in which each component is first trained\nseparately, gave the best results. First, we train the boundary\ndetector (\\hedh) separately, using \\hed weights to\ninitialize the image stream and small random weights for the DSM\nstream. That step yields a DCNN boundary detector tailored to our\naerial data.\n%\nThe actual segmentation network and loss is added only after this\nstrong ``initialization'' of the boundary detector. Thus, the\nboundary detector from the start delivers sensible results for\ntraining the semantic labeling component, and only needs to be\nfine-tuned to optimally interact with the subsequent layers.\n%\nMoreover, for \\segneth that two-stage training is carried out\nseparately for each of the three scales. The separate single-scale\nsegmentation networks are then combined into one multi-scale\narchitecture and refined together.\n%\nEmpirically, separating the scales stabilizes the learning of the\nlower resolutions. When trained together immediately, they tend to\nconverge to weak solutions and the overall result is dominated by the\n(still very competitive) highest resolution.\n\n\\paragraph{Normalization of gradients}\n%\nRegarding the common problem of exploding or vanishing gradients\nduring training, we stuck to the architectures recommended by the\noriginal authors, meaning that \\segneth does\nuse \\emph{batch normalization} \\citep{badrinarayanan2017segnet},\nwhile \\hedh does not \\citep{xie2015holistically}.\n%\nA pragmatic solution is to use a large base learning rate appropriate\nfor \\segneth and add layer-specific scale factors to decrease the\nlearning rate in the \\hedh layers.\n%\nWe also found that batch normalization in the final layers, after the\n\\segneth decoder phase, strongly sparsifies the feature\nmaps. For our goal of dense per-pixel prediction this effect is\ndetrimental, causing a $\\approx\\,$1\\% drop in labeling accuracy.\nWe thus switch-off batch normalization for those layers.\n\n\\paragraph{Drop-out}\n%\nThe original \\segnet relies heavily on drop-out. The authors recommend\nto randomly drop 50\\% of the trainable decoder weights in each\niteration.\n%\nWe found this drastic regularization to negatively affect our results,\nthus we decrease it to 20\\% for the highest resolution, respectively\n10\\% for the two lower ones.\n%\nFurther research is needed to understand this big difference. We\nsuspect that it might have to do with the different image statistics.\n%\nIn close-range images, each object normally covers a large image area,\nthus both image intensities and semantic labels are strongly\ncorrelated over fairly large regions.\n%\nIn remote sensing data, with its many small objects, nearby\nactivations might not be able to ``stand in'' as easily for a dropped\nconnection, especially in the early layers of the decoder.\n\n\\paragraph{Data Augmentation}\n%\nDCNNs need large amounts of training data, which are not always\navailable.\n%\nIt is standard practice to artificially increase the amount and\nvariation of training data by randomly applying plausible\ntransformations to the inputs.\n%\nSynthetic data augmentation is particularly relevant for remote\nsensing data to avoid over-fitting: in a typical mapping project the\ntraining data comes in the form of a few spatially contiguous regions\nthat have been annotated, not as individual pixels randomly scattered\nacross the region of interest. This means that the network is prone to\nlearn local biases like the size of houses or the orientation of the\nroad network particular to the training region.\n%\nRandom transformations -- in the example, scaling and rotation -- will\nmitigate such over-fitting.\n\nIn our experiments we used the following transformations for data\naugmentation, randomly sampled per mini-batch of the stochastic\ngradient descent (SGD) optimisation: scaling in the range $[1\\hdots\n  1.2]$, rotation by $[0^\\circ\\hdots 15^\\circ]$ degrees, linear shear\nwith $[0^\\circ\\hdots 8^\\circ]$, translation by $[-5\\hdots 5]$ pixels,\nand reflections w.r.t.\\ the vertical and horizontal axis\n(independently, with equal probability).\n\n\n%============\n"
                }
            },
            "section 4": {
                "name": "Experiment Results",
                "content": "\\label{sec:experiments}\n\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\n\\bigskip\n"
                },
                "subsection 4.2": {
                    "name": "ISPRS Vaihingen Dataset",
                    "content": "\n\nWe conduct experimental evaluations on the ISPRS Vaihingen 2D semantic\nlabeling challenge.\n%\nThis is an open benchmark dataset provided online.%\n%\n\\footnote{\\scriptsize{\\url{http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-vaihingen.html}}} %\n%\nThe dataset consists of a color infrared orthophoto, a DSM\ngenerated by dense image matching, and manually annotated ground truth\nlabels. Additionally, a nDSM has been released by one of the\norganizers \\citep{markususe}, generated by automatically\nfiltering the DSM to a DTM and subtracting the two.\n%\nOverall, there are $33$ tiles of $\\approx2500\\times2000$ pixels at a\nGSD of $\\approx9$cm.\n%\n$16$ tiles are available for training and validation, the remaining\n$17$ are withheld by the challenge organizers for testing.\n%\nWe thus remove $4$ tiles (image numbers 5, 7, 23, 30) from the\ntraining data and use them as validation set for our experiments. All\nresults refer to that validation set, unless noted otherwise.\n\n% =================================================== %\n\n\n\n% ================================================== %\n\n"
                },
                "subsection 4.3": {
                    "name": "ISPRS Potsdam Dataset",
                    "content": "\n\nWe additionally conduct experiments on the ISPRS\n  Potsdam semantic labeling dataset.%\n  %\n  \\footnote{\\scriptsize{\\url{http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html}}} %\n  %\n  The data is rather similar to Vaihingen, with 4-band RGB-IR images,\n  as well as DSM and nDSM from dense matching and filtering.\n  %\n  There are $38$ image tiles of size $6000\\times6000$ pixels,\nwith GSD $\\approx5$cm.\n%\n$24$ tiles are densely annotated for training, whereas the remaining\n14 are withheld as test set. For our experiments we have removed four\ntiles (image numbers 7\\_8, 4\\_10, 2\\_11, 5\\_11) from the training data\nand use them as validation set. All statistics on Potsdam refer to\nthat validation set.\n\nWe point out an issue with the reference data of Potsdam, which has\nimplications for our method: the trees are mostly deciduous and the\ndata was recorded in leaf-off conditions. However, the human\nannotators appear to have guessed the area of the tree crown from the\nbranches and have drawn an imaginary, solid crown area. This\nannotation contradicts the image evidence, see\nFigure~\\ref{fig:unreal-trees}. In particular, semantic boundaries\nextracted from the reference data are not aligned with actual image\ndiscontinuities.  We also note that, other than in Vaihingen, there\nare comparatively large areas marked as \\emph{background} (rejection)\nclass, with large intra-class variability.\n\n",
                    "subsubsection 4.3.1": {
                        "name": "Training details and parameters",
                        "content": "\n\nAs described above, each labeling and boundary detection network was\nfirst trained individually to convergence, then the pretrained pieces\nwere assembled and fine-tuned by further training\niterations. For a tally of the unknown weights,\nsee Table~\\ref{tab:params}.\n\nFor the individual network parts, we always start from a quite high\nlearning rate (\\textit{lr=$10^{-8}$}) and decrease it by a factor\n$\\times10$ every $12000$ iterations. The total of number of iterations\nwas $100000$. The \\segneth part was trained with batch size $2$, the\n\\hedh boundary detector with batch size $5$.\n\nThe complete, assembled boundary+segmentation model was trained for\n$30000$ iterations, starting with a smaller learning rate\n(\\textit{lr=$10^{-12}$}).  Batch size had to be reduced to $1$ to\nstay within the memory capacity of an Nvidia Titan-X GPU.\n\nThe remaining hyper-parameters were set to\n$momentum=0.9$ and $weight-decay=0.00015$, for all models.\n\n\n\n\n"
                    }
                },
                "subsection 4.4": {
                    "name": "Results",
                    "content": "\n\nWe evaluate the different components of our model by gradually adding\ncomponents. We start from the basic \\segneth, augment\nit with class boundaries, then with multi-scale processing. Finally,\nwe include it in a DCNN ensemble.\n%\nWe will not separately discuss post-processing of the outputs with\nexplicit smoothness or context models like CRFs.\n%\nIn our tests, post-processing with a fully connected CRF, as for\nexample in \\citep{ChenPK0Y16,marmanis2016semantic}, did not\nimprove the results -- if anything, it degraded the performance on\nsmall objects, especially the \\emph{car} class. For completeness,\nresults of our networks with and without smoothing are available on\nthe Vaihingen benchmark site.\n%\nWe conclude that our DCNNs already capture the context in large\ncontext windows. Indeed, this is in our view a main reason for their\nexcellent performance.\n\n",
                    "subsubsection 4.4.1": {
                        "name": "Basic CNN Results",
                        "content": "\n\n\\textbf{Vaihingen:}\nThe basic single-scale \\segneth reaches $84.8$\\% overall\naccuracy over the validation set.\n%\nThis is in line with other researchers' findings on the Vaihingen\nbenchmark: straight-forward adaptations of state-of-the-art DCNN\nmodels from the computer vision literature typically reach\naround $85$\\%.\n%\nOur network performs particularly well on impervious ground and\nbuildings, whereas it is challenged by low vegetation, which is\nfrequently confused with the tree class. Detailed per-class results\nare given in Table~\\ref{tab:perclass}.\n\nFor comparison, we also run our earlier \\dlreth\nmodel~\\cite{marmanis2016semantic}, i.e., an ensemble of only\n\\dlrethVGG and \\dlrethPAS, without explicit class-boundary detection.\n%\nThat model performs comparably, with $85.5$\\% overall\naccuracy. Interestingly, it is significantly better at classifying low\nvegetation, and also beats \\segneth on impervious surfaces and trees.\n%\nOn the contrary, it delivers clearly worse segmentations of buildings.\n\n\\textbf{Potsdam:} On the Potsdam dataset the\n\\segneth-\\textit{sc1} network performs\nsignificantly better than the standard \\dlrethPAS\nand \\dlrethVGG, with $84.9$\\%, $80.9$\\% and $81.4$\\%, overall\naccuracy respectively.\n%\nThe reason for the relatively weak results of the two larger networks is\nunclear, but we did not manage to improve them further. We will see below\nthat including boundary information closes the performance gap.\n%\nFor detailed results refer to Table~\\ref{tab:perclass}.\n\n\n"
                    },
                    "subsubsection 4.4.2": {
                        "name": "Effect of Class Boundaries",
                        "content": "\n\n\\textbf{Vaihingen:}\nWe now go on to evaluate the main claim of our paper, that explicit\nclass boundary detection within the network improves segmentation\nperformance.\n%\nAdding the \\hedh boundary detector to \\segneth reaches $89.8$\\%\noverall accuracy (\\hedh{}+\\segneth-\\textit{sc1}), a gain of more than\n$5$ percent points, see Table~\\ref{tab:perclass}.\n\nThe per-class results in Table~\\ref{tab:perclass} reveal that class\nboundaries significantly boost the performance of\n\\segneth for \\emph{all} target classes, including the\nvegetation classes that do not have sharp, unambiguous boundaries.\n%\nWe suspect that in vegetation areas, where matching-based DSMs are\nparticularly inaccurate, even imprecise boundaries can play a\nnoticeable role in delimiting high from low vegetation.\n%\nMoreover, one might speculate that boundaries derived from semantic\nsegmentation maps carry information about the extent and\ndiscontinuities at object level, and can to some degree mitigate a\nmain limitation of \\segneth, namely the lack of fully\nconnected layers that could extract long-range context. It is however\nan open, and rather far-reaching, question to what extent locally\nderived object boundary information can substitute large-scale\nsemantic context.\n\nFor the \\dlreth ensemble, we observe a similar, albeit\nweaker effect. Overall accuracy increases by $3$ percent points to\n$88.8$\\% (\\hedh{}+\\dlreth). There are however losses\nfor the \\emph{car} and \\emph{low vegetation} classes, whereas the\ngains are due to better segmentation of buildings and trees.\n\nAs a side note, we found that the accuracy of the ground truth is\nnoticeably lower for the vegetation classes as well as the cars.\n%\nThis can in part be explained by inherent definition uncertainty,\nespecially in the case of vegetation.\n%\nStill, we claim that the upper performance bound for automatic\nsegmentation methods is probably well below $100$\\% for Vaihingen, due\nto uncertainty and biases of the ground truth of both the training and\ntest set.\n%\nSee also Section~\\ref{sec:experiments-manual}.\n\n\\textbf{Potsdam:} Similar effects can\nbe observed on the Potsdam dataset, where the class boundaries also\nsignificantly improve the overall accuracy by up to $4.5$ percent\npoints. Detailed results are given in Table~\\ref{tab:perclass}.\n%\nAn exception from this trend is the tree class in\n\\segneth-\\textit{sc1}: adding class boundaries reduces its\ncorrectness significantly, from $74.4$\\% to $68.6$\\%.\n%\nWe attribute this to the guessed tree-crown annotations mentioned\nabove. Since their imaginary boundaries are not reflected in the image\ndata, adding them apparently hurts, rather than helps the localization\nof the tree outlines.\n%\nCuriously, the trees do get a boost from boundary information when\nusing the \\dlrethPAS or \\dlrethVGG networks. We were not yet able to\ndetermine the reason for this behavior. We speculate that possibly\nthese networks, with their higher capacity and pre-training for object\ndetection, can extract additional evidence for the presence/absence of\na tree from the edge maps and are better able to infer the ``guessed''\ntree outline from context.\n\n\n\n"
                    },
                    "subsubsection 4.4.3": {
                        "name": "Effect of Multi-scale CNN",
                        "content": "\n\n\\textbf{Vaihingen:}\nNext, we test what can be gained by explicit multi-scale\nprocessing.\n%\nThis is inspired by \\citet{Kokkinos2016pushing}, who show significant\nimprovements with multi-scale processing.\n%\nOur implementation uses exactly the same architecture, with three\nstreams that independently process inputs of different\nscale and fuse the results within the network.\n\nWe run this option only for \\segneth. The \\dlreth network has\nmultiple fully connected layers and should therefore be able to\ncapture context globally over the entire input region, thus we do not\nexpect an explicit multi-scale version to improve the\nresults. Moreover, it is too large to fit multiple copies of the\nnetwork into GPU memory.\n\nEmpirically, multi-scale processing did not improve the results to the\nsame extent as in \\citep{Kokkinos2016pushing}. We only gain $0.2$\npercent points, see Table~\\ref{tab:multiscale}. Apparently, the\nsingle-scale network already captures the relevant information. We\nsuspect that the gains from an explicit multi-scale architecture are\nlargely achieved by better covering strong scale variations due to\ndifferent perspective effects and camera viewpoints.\n%\nIn nadir-looking remote sensing images such effects are absent and\nscale variations occur only due to actual size variations in object\nspace, which seem to be adequately captured by the simpler network.\n%\nNevertheless, since the gains are quite consistent across different\nvalidation images, we keep the multi-scale option included for the\nremaining tests on Vaihingen. We note though, this triples the memory\nconsumption and is therefore not generally recommended.\n%\nGiven the small differences, we did not employ\nmulti-scale processing in the Potsdam experiments.\n\n\n\n% ============================================================ %\n\n\n\n\n"
                    },
                    "subsubsection 4.4.4": {
                        "name": "Effect of the Ensemble",
                        "content": "\n\n\\textbf{Vaihingen:}\nSeveral works have confirmed that also for DCNN models ensemble\nlearning is beneficial to reduce individual model biases. We have also\nobserved this effect in earlier work and thus test what can be gained\nby combining several \\emph{boundary-aware} segmentation networks.\n\nWe run the three introduced networks \\segneth, \\dlrethPAS and\n\\dlrethVGG, all with an integrated \\hedh boundary detector, and\naverage their predictions.\n%\nThe ensemble beats both the stand-alone \\segneth model\nand the two-model ensemble of (boundary-enhanced) \\dlreth,\nsee Table~\\ref{tab:multiscale}.\n%\nThe advantage over \\segneth\nis marginal, whereas a clear improvement can be observed over \\dlreth.\nIn other words, \\segneth alone stays behind its\n\\dlrethVGG and \\dlrethPAS\ncounterparts, but when augmented with class-boundaries it outperforms\nthem, and reaches almost the performance of the full ensemble. It\nseems that for the lighter and less global \\segneth model the boundary\ninformation is particularly helpful.\n\nWe point out that, by and large, the quantitative behavior is\nconsistent across the four individual tiles of the validation set. In\nall four cases, \\hedh{}+\\dlreth clearly beats \\dlreth, and similarly\n\\hedh{}+\\segneth-\\textit{sc1} comprehensively beats\n\\segneth-\\textit{sc1}.\n%\nRegarding multi-scale processing, \\hedh{}+\\segneth-\\textit{Msc} wins\nover \\hedh{}+\\segneth-\\textit{sc1} except for one case (image 7), where the\ndifference are a barely noticeable $0.03$ percent points (ca.\\ 1500\npixels / 12 m$^\\text{2}$).\n%\nEnsemble averaging again helps for the other three test tiles, with an\naverage gain of $0.21$ percent points, while for image 7 the ensemble\nprediction is better than \\hedh{}+\\dlreth but does not quite reach\nthe \\segneth performance.\n%\nA further analysis of the results is left for future work, but will\nlikely require a larger test set.\n%\nFor a visual impression, see\nFigure~\\ref{fig:Vaihingen-visualization}.\n\n%====================================================== %\n%\n\n\\textbf{Potsdam}:\n%\nFor the ensemble model we proceed in the same way and average the\nindividual predictions of \\segneth,\n\\dlrethPAS and \\dlrethVGG.\n%\nAlso for Potsdam, the ensemble with class boundary support for each of\nthe three members performs best, see\nTable~\\ref{tab:Postdam-validation}.\n%\nNote that, although all three networks exhibit almost identical\noverall performance of $85\\%$, averaging them boosts the accuracy by\nanother percent point.\n%\nVisual examples are shown in Figure~\\ref{fig:Potsdam-visualization}.\n\n\n"
                    },
                    "subsubsection 4.4.5": {
                        "name": "Effects of nDSM Errors",
                        "content": "\n\n\\textbf{Vaihingen:}\n%\nOn the official Vaihingen test set the performance of our ensemble\ndrops to 89.4\\%, see below. We have visually checked the results and\nfound a number of regions with large, uncharacteristic prediction\nerrors.\n%\nIt turns out that there are gross errors in the test set that pose an\nadditional, presumably unintended, difficulty. In the nDSM\nof~\\citet{markususe}, a number of large industrial buildings are\nmissing, since the ``ground'' surface follows their roofs, most likely\ndue to incorrect filtering parameters.\n%\nThe affected buildings cover a significant area: 3.1\\% (154'752\npixels) of image 12, 9.3\\% (471'686 pixels) of image 31, and 10.0\\%\n(403'818 pixels) of image 33.\n\nBy itself this systematic error could be regarded as a recurring\ndeficiency of automatically found nDSMs, which should be handled by the\nsemantic segmentation. But unfortunately, they only occur\nin the test set, while in the training set no comparable\nsituations exist. It is thus impossible for a machine learning model\nto handle them correctly.\n\nTo get an unbiased result we thus manually corrected the nDTMs of the\nfour affected buildings.\n%\nWe then reran the testing, without altering the trained model in any\nway, and obtained an overall accuracy of $90.3$\\%, almost perfectly in\nline with the one on our validation set, and $0.9$ percent points up\nfrom the biased result.\n\nIn the following evaluation we thus quote both numbers. We regard the\n$90.3$\\% as our ``true'' performance on a test\nset whose properties are adequately represented in the training data.\n%\nSince competing methods did however, to our knowledge, not use the\ncorrected test set, they should be compared to the $89.4$\\% achieved\nwith the biased nDSM.\n%\nWe note however that the discovered errors are significant in the\ncontext of the benchmark: the bias of almost $1$ percent point is\nlarger than the typical differences between recent competing methods.\n%\nSince the experiment mainly serves to illustrate the quality and\ninfluence of the available reference data, we did not repeat it for\nPotsdam (where it would have been very hard to repair especially\nthe tree annotations).\n\n\n\n"
                    }
                },
                "subsection 4.5": {
                    "name": "Comparison to state of the art",
                    "content": "\n\n\\textbf{Vaihingen:}\n%\nOur proposed \\textit{class-contour ensemble model} is among the top\nperformers on the official benchmark test set, reaching $89.4$\\%\noverall accuracy, respectively $90.3$\\% with the correct nDSM. Note,\nthe model names on the benchmark website differ from those used here,\nplease refer to Table~\\ref{tab:online-submitted-models}. The strongest\ncompetitors at the time of writing%\n  %\n  \\footnote{The field moves forward at an astonishing pace, during the\n    review of the present paper several groups have reached\n    similar or even slightly higher accuracy. We cannot comment on\n    these works, since no descriptions of their methodologies have\n    appeared yet.} %\n  %\n  were INRIA\n\\citep[$89.5$\\%,][]{maggiori2016high}, using a variant of\n\\textsc{fcn}, and ONERA \\citep[$89.8$\\%,][]{audebert2016semantic},\nwith a variant of \\segnet.\n%\nImportantly, we achieve above $90$\\% accuracy over man-made classes,\nwhich are the most well-defined ones, where accurate segmentation\nboundaries matter most, see Table~\\ref{tab:error-matrix}.\n\nDetailed results for the top-ranking published models in the benchmark\nare given in\nTable~\\ref{tab:compare-participants}. Table~\\ref{tab:online-submitted-models}\n  lists the benchmark identifiers of the different model variants we\n  have described.\n%\nOverall, the performance of different models is very similar, which\nmay not be all that surprising, since the top performers are in fact\nall variants of \\fcn or \\segnet.\n%\nWe note that our model and INRIA are the most ``puristic'' ones, in that\nthey do not use any hand-engineered image features. ONERA uses the\nNDVI as additional input channel; DST seemingly includes a random\nforest in its ensemble, whose input features include the NDVI\n(Normalized Vegetation Index) as well\nas statistics over the DSM normals. It appears that the additional\nfeatures enable a similar performance boost as our class boundaries,\nalthough it is at this point unclear whether they encode the same\ninformation.\n%\nInterestingly, our model scores rather well on the \\emph{car} class,\nalthough we do not use any stratified sampling to boost rare\nclasses. We believe that this is in part a consequence of not\nsmoothing the label probabilities.\n\nOne can also see that after correcting the nDSM for large errors, our\nperformance is better than most competitors on impervious surfaces as\nwell as on buildings.\n%\nThe bias in the test data thus seems to affect all models.\n%\nSomewhat surprisingly, our scores on the vegetation classes are also\non par with the competitors, although intuitively contours cannot\ncontribute as much for those classes, because their boundaries are not\nwell-defined. Still, they significantly improve the segmentation of\nvegetation, c.f.~Table~\\ref{tab:perclass}.\n%\nEmpirically, the class-boundary information boosts segmentation of the\n\\emph{tree} and \\emph{low vegetation} classes to a level reached by\nmodels that use a dedicated NDVI channel. A closer look at the\nunderlying mechanisms is left for future work.\n\n\n\n\n\n\n\n%%%%%%%%%%%%%%%% VISUALIZATIONs Vaihingen %%%%%%%%%%%%%%\n\n\n\n\n%%%%%%%%%%%%%%%% VISUALIZATIONs Potsdam %%%%%%%%%%%%%%\n\n\n\n"
                },
                "subsection 4.6": {
                    "name": "A word on data quality",
                    "content": "\n\\label{sec:experiments-manual}\n\nIn our experiments, we repeatedly noticed inaccuracies of the ground\ntruth data, such as those shown in Figure~\\ref{fig:errors-annotation}\n(similar observations were made by\n\\citet{paisitkriangkrai2015effective}).\n%\nObviously, a certain degree of uncertainty is unavoidable when\nannotating data, in particular in remote sensing images with their\nsmall objects and many boundaries.\n%\nWe thus decided to re-annotate one image (\\emph{image-23}) from our\nVaihingen validation set with great care,\nto assess the ``inherent'' labeling\naccuracy.\n%\nWe did this only for the two easiest classes \\emph{buildings} and\n\\emph{cars}, since the remaining classes have significant definition\nuncertainty and we could not ensure to use exactly the same\ndefinitions as the original annotators.\n\nWe then evaluate the new annotation, the ground truth from the\nbenchmark, and the output of our best model, against each other.\n%\nResults are shown in Table~\\ref{tab:gt-comparison}.\n%\nOne can see significant differences, especially for the \\emph{cars}\nwhich are small and have a large fraction of pixels near the\nboundary. Considering the saturating progress on the benchmark\n(differences between recent submissions are generally $<2$\\%) there is\na very real danger that annotation errors influence the results and\nconclusions.\n%\nIt may be surprising, but the Vaihingen dataset (and seemingly also\nthe Potsdam dataset) is reaching its limits after barely 3 years of\nactivity. This is a very positive and tangible sign of progress, and a\nstrong argument in favor of public benchmarks. But it is also a\nmessage to the community: if we want to continue using benchmarks --\nwhich we should -- then we have to make the effort and extend/renew\nthem every few years.\n%\nIdeally, it may be better to move to a new dataset altogether, to\navoid overfitting and ensure state-of-the-art sensor quality.\n\n\n\n\n\n\n%============\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\\label{sec:conclusion}\n\nWe have developed DCNN models for semantic segmentation of\nhigh-resolution aerial images, which explicitly represent and extract\nthe boundaries between regions of different semantic classes.\n%\nEmpirically, including class boundaries significantly improves\ndifferent DCNN architectures, and was the single most important\nperformance boost in our final model, which achieves excellent\nperformance on the ISPRS Vaihingen and Potsdam benchmarks.\n\nMoreover, we have presented an extensive study of semantic\nsegmentation architectures, including presence or absence of fully\nconnected layers, use of class boundaries, multi-scale processing, and\nmulti-network ensembles.\n\nOne aspect that we have not yet investigated, but that might be\nneeded to fully exploit the information in the segmentation\nboundaries, are class-specific boundaries. Our current boundaries are\nclass-agnostic, they do not know which classes they actually separate.\n%\nIt appears that this information could be preserved and used. Pushing\nthis idea to its extremes, it would in fact be enough to detect\n\\emph{only} the class boundaries, if one can ensure that they form\nclosed regions.\n\nAlthough DCNNs are the state-of-the-art tool for semantic segmentation,\nthey have reached a certain degree of saturation, and further\nimprovements of segmentation quality will probably be small, tedious,\nand increasingly problem-specific.\n%\nNevertheless, there are several promising directions for future\nresearch. We feel that model size is becoming an issue. Given the\nexcessive size and complexity of all the best-performing DCNN models,\nan interesting option would be to develop methods for compressing\nlarge, deep models into smaller, more compact ones for further\nprocessing. First ideas in this direction have been brought up by\n\\citet{hinton2015distilling}.\n\n"
            },
            "section 6": {
                "name": "References",
                "content": "\\label{sec:references}\n\n\n\n% =========== REFERENCES\n\n\\bibliographystyle{elsarticle-harv}\n\\bibliography{ref}\n\n\n"
            }
        },
        "tables": {
            "tab:params": "\\begin{table}[ht]\n\\centering\n\\caption{Sizes of model components in terms of trainable parameters.\nAll models are dimensioned to fit on a single Nvidia Titan-X GPU\n(except for the ensemble, for which averaging is done on the CPU).\nSuffix \\textit{sc1} denotes a single-scale model using only the full\nresolution, \\textit{Msc} denotes the multi-scale model.}\n\\label{tab:params}\n%\n\\begin{tabular}{|l|r|}\n\\hline\n\\hedh{}+\\segneth-\\textit{sc1} &   $88\\cdot10^6$ \\\\ \\hline\\hline\n\\hedh{}+\\segneth-\\textit{Msc} &   $206\\cdot10^6$ \\\\\n\\hedh{}+\\dlrethPAS \t      &   $300\\cdot10^6$ \\\\\n\\hedh{}+\\dlrethVGG            &   $300\\cdot10^6$ \\\\ \\hline\n\\hedh{}+\\dlreth{}+\\segneth    &   $806\\cdot10^6$ \\\\ \\hline\n\\end{tabular}\n\\end{table}",
            "tab:perclass": "\\begin{table}[ht]\n\\centering\n\\caption{Adding an explicit class-boundary model improves semantic\n  segmentation, for both tested CNN architectures and both\n  investigated datasets.  \\hedh and \\emph{sc1} denote use of\n  class-boundaries and restriction to a single scale,\n  respectively. Scores are (true positive) detection rates. See text\n  for details.}\n\\label{tab:perclass}\n%\n\\begin{adjustbox}{center}\n  \\small\n\\begin{tabular}{|c|l|ccccc||c|}\n\\hline\n%\n& & \\textit{Impervious} & \\textit{Building} & \\textit{Low Veg.} & \\textit{Tree} & \\textit{Car} & \\textit{OA} \\\\ \\hline\\hline\n%\n\\multirow{4}{*}{Vaihingen}\n& \\segneth-\\textit{sc1} &\n87.5 \\%  & 93.8 \\%  & 59.0 \\% & 79.0 \\% & 63.0 \\% &  84.8 \\% \\\\\n& \\hedh{}+\\segneth-\\textit{sc1} &\n91.2 \\% & 95.6 \\% & 70.8 \\% & 92.3 \\% & 69.0 \\% &  \\textbf{89.8} \\% \\\\ \\cline{2-8}\n& \\dlreth &\n89.3 \\% & 87.5 \\% & 77.3 \\% & 88.8 \\% & 68.3 \\% & 85.8 \\% \\\\\n& \\hedh{}+\\dlreth &\n89.3 \\% & 93.5 \\% & 73.0 \\% & 90.8 \\% & 62.0 \\% & 88.8 \\% \\\\ \\hline\\hline\n%\n\\multirow{6}{*}{Potsdam}\n& \\segneth-\\textit{sc1} &\n84.7 \\% & 95.2 \\% & 78.9 \\% & 74.4 \\% & 80.8 \\% &  84.9 \\% \\\\\n& \\hedh{}+\\segneth-\\textit{sc1} &\n85.0 \\% & 96.7 \\% & 84.2 \\% & 68.6 \\% & 85.8 \\% & \\textbf{85.1} \\% \\\\ \\cline{2-8}\n& \\dlrethPAS &\n84.0 \\% & 88.5 \\% & 72.0 \\% & 74.7 \\% & 75.0 \\% & 80.9 \\% \\\\\n& \\hedh{}+\\dlrethPAS &\n84.9 \\% & 96.2 \\% & 76.7 \\% & 79.1 \\% & 86.7 \\% & 84.5 \\% \\\\ \\cline{2-8}\n& \\dlrethVGG &\n84.3 \\% & 90.6 \\% & 71.8 \\% & 74.9 \\% & 74.4 \\% & 81.4 \\% \\\\ \n& \\hedh{}+\\dlrethVGG &\n85.2 \\% & 96.8 \\% & 74.3 \\% & 79.1 \\% & 85.4 \\% & 85.0 \\% \\\\ \\hline\n%\n\\end{tabular}\n\\end{adjustbox}\n\\end{table}",
            "tab:multiscale": "\\begin{table}[ht]\n  \\caption{Multi-scale processing and ensemble learning over the ISPRS\n    Vaihingen dataset. The results (overall accuracies) on the\n    validation set confirm that gains are mostly due to the class\n    boundary detection, whereas multi-scale processing and ensemble\n    prediction only slightly improve the results further. See text for\n    details.}\n\\label{tab:multiscale}\n%\n{\\footnotesize\n\\begin{adjustbox}{center}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n\\hline\n& \\textit{Scene}\n& \\hedh{}+\\dlreth{}+\\segneth\n& \\hedh{}+\\segneth-\\textit{Msc}\n& \\hedh{}+\\segneth-\\textit{sc1}\n& \\segneth-\\textit{sc1}\n& \\hedh{}+\\dlreth\n& \\dlreth \\\\\n\\hline\n\\multirow{5}{*}{\\rotatebox{90}{Vaihingen}}\n&\\textit{Image 5} &\n91.5 \\% & 91.2 \\% & 91.1 \\% & 86.2 \\% & 90.4 \\% & 86.3 \\% \\\\\n&\\textit{Image 7} &\n89.2 \\% & 89.6 \\% & 89.6 \\% & 84.2 \\% & 89.1 \\% & 87.1 \\% \\\\\n&\\textit{Image 23} &\n92.0 \\% & 90.8 \\% & 90.2 \\% & 85.6 \\% & 89.3 \\% & 83.7 \\% \\\\\n&\\textit{Image 30} &\n88.7 \\% & 88.5 \\% & 88.4 \\% & 83.2 \\% & 86.7 \\% & 86.1 \\% \\\\ \\cline{2-8}\n&\\textit{OA} &\n\\textbf{90.3 \\%} & 90.0 \\% & 89.8 \\% & 84.8 \\% & 88.8 \\% & 85.8 \\% \\\\\n\\hline\n\\end{tabular}\n\\end{adjustbox}\n}\n\\end{table}",
            "tab:Postdam-validation": "\\begin{table}[ht]\n  \\caption{Results on Potsdam validation set for individual networks\n    and network ensemble. The class boundaries improve the\n    classification accuracy of all three individual networks, and thus\n    also of the ensemble.}\n    \\label{tab:Postdam-validation}\n    %\n      {\\footnotesize\n    \\begin{adjustbox}{center}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n\\hline & \\textit{Scene} & \\hedh{}+\\dlreth{}+\\segneth & \\hedh{}+\\segneth-\\textit{sc1} & \\segneth-\\textit{sc1} & \\hedh{}+\\dlrethPAS & \\dlrethPAS & \\hedh{}+\\dlrethVGG & \\dlrethVGG \\\\ \\hline\n%\n\\multirow{5}{*}{\\rotatebox{90}{Potsdam}}\n&\\textit{Image 2\\_11} &\n84.5 \\% & 81.1 \\% & 81.9 \\% & 82.2 \\% & 76.2 \\% & 82.9 \\% & 78.8 \\% \\\\\n&\\textit{Image 5\\_11} &\n90.9 \\% & 89.8 \\% & 89.3 \\% & 90.2 \\% & 86.2 \\% & 90.6 \\% & 86.7 \\% \\\\\n&\\textit{Image 4\\_10} &\n81.8 \\% & 83.3 \\% & 82.7 \\% & 79.2 \\% & 79.2 \\% & 79.6 \\% & 79.2 \\% \\\\\n&\\textit{Image 7\\_8 } &\n87.7 \\% & 86.3 \\% & 85.6 \\% & 86.6 \\% & 81.6 \\% & 86.7 \\% & 80.9 \\% \\\\\n\\cline{2-9}\n&\\textit{OA} &\n\\textbf{86.2 \\%} & 85.1 \\% & 84.9 \\% & 84.5 \\% & 80.9 \\% & 85.0 \\% & 81.4 \\% \\\\\n\\hline\n\\end{tabular}\n\\end{adjustbox}\n}\n\\end{table}",
            "tab:error-matrix": "\\begin{table}[ht]\n\\centering\n\\caption{Confusion matrix of our best result (\\textit{DLR\\_9}) on the\n  private Vaihingen test set. Values are percent of predicted pixels\n  (rows sum to 100\\%, off-diagonal elements are false alarm rates).}\n\\label{tab:error-matrix}\n%\n{\\small\n\\begin{tabular}{|cl|ccccc|}\n\\cline{3-7}\n\\multicolumn{2}{c|}{} & \\multicolumn{5}{c|}{reference}\\\\\n\\multicolumn{2}{c|}{} & \\textit{Impervious} & \\textit{Building} & \\textit{Low-Veg} & \\textit{Tree} & \\textit{Car} \\\\\n\\hline\n& \\textit{Impervious} &\n\\textbf{93.2 \\%} & 2.2 \\% & 3.6 \\% & 0.9 \\% & 0.1 \\% \\\\\n& \\textit{Building} &\n2.8 \\% & \\textbf{95.3 \\%} & 1.6 \\% & 0.3 \\% & 0.0 \\% \\\\\n\\parbox[t]{3mm}{\\multirow{-2}{*}{\\rotatebox[origin=c]{90}{predicted}}}\n& \\textit{Low-Veg} &\n3.7 \\% & 1.4 \\% & \\textbf{82.5 \\%} & 12.5 \\% & 0.0 \\% \\\\\n& \\textit{Tree} &\n0.7 \\% & 0.2 \\% & 6.9 \\% & \\textbf{92.3\\%} & 0.0 \\% \\\\\n& \\textit{Car} &\n19.5 \\% & 7.0 \\% & 0.5 \\% & 0.4 \\% & \\textbf{72.60 \\%} \\\\\n\\hline\n& \\textit{Precision} &\n91.6 \\% & 95.0 \\% & 85.5 \\% & 87.5 \\% & 92.1 \\% \\\\\n& \\textit{Recall} &\n93.2 \\% & 95.3 \\% & 82.5 \\% & 92.3\\% & 72.6 \\% \\\\\n& \\textit{F1-score} &\n92.4 \\% & 95.2 \\% & 83.9 \\% & 89.9 \\% & 81.2 \\% \\\\\n\\hline\n%\\specialrule{.2em}{.1em}{.1em}\n\\end{tabular}\n}\n\\end{table}",
            "tab:online-submitted-models": "\\begin{table}[th]\n\\centering\n\\caption{Short names of our model variants on the ISPRS Vaihingen 2D\n  benchmark website.}\n\\label{tab:online-submitted-models}\n%\n\\small\n\\begin{tabular}{|c|l|}\n\\hline\n\\textit{Abbreviation} & \\multicolumn{1}{c|}{\\textit{Model Details}} \t\t\\\\ \\hline\nDLR\\_1  & \\dlreth                                      \\\\\nDLR\\_2  & \\dlreth{}+CRF                                \\\\\nDLR\\_3  & \\hedh{}+\\dlreth                              \\\\\nDLR\\_4  & \\hedh{}+\\dlreth{}+CRF                        \\\\\nDLR\\_5  & \\hedh{}+\\segnet                              \\\\\nDLR\\_6  & \\hedh{}+\\segneth{}+CRF                       \\\\\nDLR\\_7  & \\hedh{}+\\dlreth{}+\\segneth                   \\\\\nDLR\\_8  & \\hedh{}+\\dlreth{}+\\segneth{}+CRF             \\\\\nDLR\\_9  & \\hedh{}+\\dlreth{}+\\segneth, nDSM corrections \\\\\nDLR\\_10 & \\hedh{}+\\dlreth{}+\\segneth{}+CRF, nDSM corrections \\\\\n\\hline\n%\\specialrule{.2em}{.1em}{.1em}\n\\end{tabular}\n\\end{table}",
            "tab:compare-participants": "\\begin{table}[ht]\n\\centering\n\\caption{Per-class $F_1$-scores and overall accuracies of top\n  performers on the Vaihingen benchmark (numbers copied from benchmark\n  website). DLR\\_7 is our ensemble model, DLR\\_9 is our ensemble with\n  corrected nDSM. Acronyms are taken from the official ISPRS-Vaihingen\n  website.}\n\\label{tab:compare-participants}\n%\n\\small\n\\begin{tabular}{|l|ccccc|c|}\n\\hline\n& \\textit{Impervious} & \\textit{Building} & \\textit{Low-Veg} & \\textit{Tree}    & \\textit{Car}     & \\textit{OA} \\\\\n\\hline\nDST\\_2 &\n90.5 \\% & 93.7 \\% & 83.4 \\% & 89.2 \\% & 72.6 \\% & 89.1 \\% \\\\\nINR &\n91.1 \\% & 94.7 \\% & 83.4 \\% & 89.3 \\% & 71.2 \\% & 89.5 \\% \\\\\nONE\\_6 &\n91.5 \\% & 94.3 \\% & 82.7 \\% & 89.3 \\% & \\textbf{85.7 \\%}  & 89.4 \\% \\\\\nONE\\_7 &\n91.0 \\% & 94.5 \\% & \\textbf{ 84.4 \\%} & 89.9 \\% & 77.8 \\% & 89.8 \\% \\\\\nDLR\\_7 &\n91.4 \\% & 93.8 \\% & 83.0 \\% & 89.3 \\% & 82.1 \\% & 89.4 \\% \\\\\nDLR\\_9 &\n\\textbf{92.4 \\%} & \\textbf{95.2 \\%} & 83.9 \\% & \\textbf{89.9 \\%} & 81.2 \\% & \\textbf{90.3 \\%} \\\\\n\\hline\n%\\specialrule{.2em}{.1em}{.1em}\n\\end{tabular}\n\\end{table}",
            "tab:gt-comparison": "\\begin{table}[htb]\n\\caption{Inter-comparison between ISPRS Vaihingen ground truth, our\n  own annotation, and our best models. Significant differences occur,\n  suggesting that the benchmark may have to be re-annotated or\n  replaced. See text for details.}\n\\label{tab:gt-comparison}\n%\n\\centering\n\\begin{adjustbox}{center}\n\\footnotesize\n\\begin{tabular}{l|c|c||c|c||c|c|}\n\\cline{2-7}\n& \\multicolumn{2}{c||}{\\hedh+\\segneth-\\textit{Msc}}\n& \\multicolumn{2}{c||}{\\hedh+\\dlreth+\\segneth}\n& benchmark label & our label\\\\\n\\cline{2-7}\n& benchmark label & our label & benchmark label & our label & our label  & benchmark label \\\\\n\\hline\n\\multicolumn{1}{|c|}{\\textit{Building}} &\n97.3 \\% & 97.8 \\% & 97.7 \\% & 98.0 \\% & 97.9 \\% & 94.7 \\% \\\\ \\hline\n\\multicolumn{1}{|c|}{\\textit{Car}} &\n84.6 \\% & 88.1 \\% & 79.8 \\% & 83.3 \\% & 93.2 \\% &  88.8 \\% \\\\ \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\end{table}"
        },
        "figures": {
            "fig:teaser": "\\begin{figure}[t!]\n\\centering\n\n\\label{fig:teaser}\n\\begin{tabular}{lll}\n\t  \\includegraphics[scale=0.3]{img-ref.jpg}\n \t& \\includegraphics[scale=0.3]{fcn-vgg-ref.jpg}\n\t& \\includegraphics[scale=0.3]{cc-ensemble-ref.jpg}\n\\end{tabular}\n\\caption{Semantic segmentation. Left: input image. Middle: DCNN\n    segmentation, object boundaries tend to be blurred. Right: We\n    propose to mitigate this effect by including an explicit object\n    boundary detector in the network.}\n\\end{figure}",
            "fig:edge-network": "\\begin{figure}[]\n  \\includegraphics[width=\\textwidth]{class-contour-net-refine-5.jpg}\n  \\caption{The Class-Boundary network \\hedh. Color\n    and height are processed in separate streams. Before each pooling\n    level (red squares) the network outputs a set of scale-dependent\n    class-boundaries, which are fused into the final multi-scale\n    boundary prediction.  Yellow circles denote concatenation of\n    feature maps.  }\n        \\label{fig:edge-network}\n\\end{figure}",
            "fig:class-contours-examples": "\\begin{figure}[]\n  \\includegraphics[width=\\textwidth]{class-boundaries-vis-2.jpg}\n\n    \\caption{Examples of class boundary predictions. Left: input\n      image. Middle: per-scale outputs. Right: estimated multi-scale\n      boundary map.}\n      \t \\label{fig:class-contours-examples}\n\\end{figure}",
            "fig:full-architectures": "\\begin{figure}[]\n  \\centering\n\\includegraphics[width=0.8\\textwidth]{class-boundary-vis-2.jpg}\\\\ [-0.5ex]\n%\n\\includegraphics[trim={0 1cm 0 1.0cm},clip,width=\\textwidth]{multi-scale-fusion-abstraction-2-2.jpg}\\\\ [-2.5ex]\n%\n\\includegraphics[trim={0 0 0 0.2cm},clip,width=\\textwidth]{network-abstraction-vgg-fcn-refined-2-2.jpg}\n%\n\\caption{CNN architectures tested in this work. Top:\n  \\hedh architecture for class-boundary delineation.\n  Middle: multi-scale \\segneth architecture.\n  Bottom: \\dlrethPAS architecture (identical to\n  \\dlrethVGG except for initial weights).  The\n  boundary detection network is collapsed (violet box) for better\n  readability. Encoder parts that reduce spatial resolution are marked\n  in red, decoder parts that increase resolution are green. Orange\n  denotes fusion by concatenation, $(1\\times1)$ convolution, and\n  upsampling (as required).  The red circle with the plus sign denotes\n  element-wise summation of feature maps, the black box symbolises the\n  (logistic, respectively Euclidean) loss.}\n  \\label{fig:full-architectures}\n\\end{figure}",
            "fig:ensemble-workflow": "\\begin{figure}[H]\n%  \\includegraphics[width=\\textwidth]{/ensemble-workflow-diagram/ensemble-workflow-3.jpg}\n\\centering\n  \\includegraphics[trim={0 10cm 0 0},clip,width=0.85\\textwidth]{ensemble-workflow-3}\n\\caption{Ensemble prediction with \\segneth, \\dlrethPAS and\n    \\dlrethVGG. The \\hedh component extracts the class boundaries.}\n\\label{fig:ensemble-workflow}\n\\end{figure}",
            "fig:unreal-trees": "\\begin{figure}[t!]\n\\centering\n\\begin{tabular}{lll}\n  \\includegraphics[width=0.3\\textwidth]{image.jpg} &\n  \\includegraphics[width=0.3\\textwidth]{labels.jpg} &\n  \\includegraphics[width=0.3\\textwidth]{cc-labels.jpg}\n\\end{tabular}\n\\caption{Annotation ambiguity of trees in leaf-off condition in the\n  Potsdam dataset.  Even though there is no image evidence (left),\n  annotators tend to hallucinate a tree crown around the empty\n  branches, denoted by yellow color in the labels (middle). The\n  corresponding label class-boundaries (right) do not reflect\n  discontinuities in the input images, thus contradicting our model\n  assumptions.}\n\\label{fig:unreal-trees}\n\\end{figure}",
            "fig:Vaihingen-visualization": "\\begin{figure}[htbp]\n\\centering\n\\begin{adjustbox}{center}\n\\begin{tabular}{cccc}\nimage & \\dlreth & \\hedh{}+\\segneth & \\hedh{}+\\dlreth{}+\\segnet \\\\\n\\includegraphics[width=0.24\\textwidth]{/2/img.jpg} &\n\\includegraphics[width=0.24\\textwidth]{/2/DLR_1.jpg} &\n\\includegraphics[width=0.24\\textwidth]{/2/DLR_5.jpg} &\n\\includegraphics[width=0.24\\textwidth]{/2/DLR_9.jpg} \\\\\n& & & \\\\\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/3/img.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/3/DLR_1.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/3/DLR_5.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/3/DLR_9.jpg} \\\\\n& & & \\\\\n\\includegraphics[width=0.24\\textwidth]{/4/area27.jpg} &\n\\includegraphics[width=0.24\\textwidth]{/4/DLR_1.jpg} &\n\\includegraphics[width=0.24\\textwidth]{/4/DLR_5.jpg} &\n\\includegraphics[width=0.24\\textwidth]{/4/DLR_9.jpg} \\\\\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Example predictions on the official Vaihingen test set. The\nsecond to fourth column show the outputs of \\dlreth (DLR\\_1),\n  \\hedh+\\segnet (DLR\\_5) and \\hedh+\\dlreth+\\segnet (DLR\\_7).\n  \\textbf{White}: Impervious Surfaces, \\textbf{Blue}: Buildings,\n  \\textbf{Cyan}: Low-Vegetation, \\textbf{Green}: Trees,\n  \\textbf{Yellow}: Cars.}\n\\label{fig:Vaihingen-visualization}\n\\end{figure}",
            "fig:Potsdam-visualization": "\\begin{figure}[ht]\n\\centering\n\\begin{adjustbox}{center}\n\\begin{tabular}{cccc}\nimage & \\segneth & \\hedh{}+\\segneth & \\hedh{}+\\dlreth{}+\\segneth \\\\\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/p1/image.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/p1/segnet-no-cb.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/p1/segnet-cb.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/p1/ensemble_all.jpg} \\\\\n& & & \\\\\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/p2/image.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/p2/segnet_no_cb.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/p2/segnet_cb.jpg} &\n\\includegraphics[angle=90,origin=c,width=0.24\\textwidth]{/p2/cb_ensemble.jpg} \\\\\n% & & & \\\\\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Example predictions on our Potsdam validation set.  The\n  second to fourth columns show results for \\segneth, \\hedh{}+\\segneth\n  and \\hedh{}+\\dlreth{}+\\segneth.  \\textbf{Dark Blue}: Impervious\n  Surfaces, \\textbf{Light Blue}: Buildings, \\textbf{Green}:\n  Low-Vegetation, \\textbf{Yellow}: Trees, \\textbf{Orange}: Cars.}\n\\label{fig:Potsdam-visualization}\n\\end{figure}",
            "fig:errors-annotation": "\\begin{figure}[H]\n\\includegraphics[width=\\textwidth]{Errors_in_annotation.jpg}\n\\caption{Examples of ground truth labeling errors. Yellow/red circles\n  denote missing or incorrectly delineated objects.}\n\\label{fig:errors-annotation}\n\\end{figure}"
        },
        "git_link": "https://github.com/deep-unlearn/ISPRS-Classification-With-an-Edge"
    }
}