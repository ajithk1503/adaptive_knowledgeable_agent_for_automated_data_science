{
    "meta_info": {
        "title": "COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive  Learning",
        "abstract": "Graph contrastive learning (GCL) improves graph representation learning,\nleading to SOTA on various downstream tasks. The graph augmentation step is a\nvital but scarcely studied step of GCL. In this paper, we show that the node\nembedding obtained via the graph augmentations is highly biased, somewhat\nlimiting contrastive models from learning discriminative features for\ndownstream tasks. Thus, instead of investigating graph augmentation in the\ninput space, we alternatively propose to perform augmentations on the hidden\nfeatures (feature augmentation). Inspired by so-called matrix sketching, we\npropose COSTA, a novel COvariance-preServing feaTure space Augmentation\nframework for GCL, which generates augmented features by maintaining a \"good\nsketch\" of original features. To highlight the superiority of feature\naugmentation with COSTA, we investigate a single-view setting (in addition to\nmulti-view one) which conserves memory and computations. We show that the\nfeature augmentation with COSTA achieves comparable/better results than graph\naugmentation based models.",
        "author": "Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, Irwin King",
        "link": "http://arxiv.org/abs/2206.04726v2",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "This paper is accepted by the ACM KDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\nMany Graph Neural Networks (GNNs)~\\cite{kipf2016semi,deeper_look2,DBLP:conf/www/ZhangZMKK22,DBLP:conf/cikm/SongMZK21,9737635,yang2020featurenorm} focus on (semi-)supervised learning, which requires access to abundant labels. Recent trends in Self-Supervised Learning (SSL) have resulted in several methods that do not require labels~\\cite{kipf2016variational,hamilton2017inductive}. Among SSL methods, Contrastive Learning (CL) already achieved comparable performance with its supervised counterparts on many tasks~\\cite{chen2020simple,gao2021simcse}. Recently, CL has been applied to the graph domain. A typical Graph Contrastive Learning (GCL) method constructs multiple graph views by stochastic augmentation of the input to learn representations by contrasting positive samples with negative samples~\\cite{zhu2020deep,peng2020graph,zhu2021graph}. However, the irregular structure of graphs complicates the adaptation of augmentation techniques used on images and prevents  extending of theoretical analysis for vision-based contrastive learning to graphs. Thus, many works focus on the empirical design of hand-crafted graph augmentations (GA) for graph contrastive learning (\\ie, random edge/node/attribute dropping)~\\cite{you2020graph,zhu2020deep,zhu2021graph}. Notably, some latest works point out that random data augmentations are problematic as their noise may not be relevant to downstream tasks~\\cite{DBLP:journals/corr/AdversaGA,DBLP:conf/nips/whatGoodVeiw}. In certain scenarios (\\ie, recommendation systems\\cite{yang2022hrcf,chen2021attentive,chen2021modeling}), GCL  achieves the desired performance gain under extremely sparse GAs (with an edge dropout rate 0.9)~\\cite{wu2021self} but   method~\\cite{yu2022graph} achieves similar results without GAs. Such observations naturally raise the question: are there better augmentation strategies for GCL other than GA?\n\n\n\nTo this end, we show that the embeddings obtained with GA are highly biased compared to the embeddings obtained with feature augmentation (FA), that is, the embeddings obtained with FA (\\eg, an injection of random noise into the embedding) exhibit the so-called weak law of large numbers (WLLN). Specifically, for any error $\\varepsilon \\geq 0$, $\\lim_{k \\rightarrow \\infty} \\mathcal{P}\\left(\\|\\mathbb{E}(\\tilde{\\bm{x}}^{(k)}) - \\bm{x}\\|_1 > \\varepsilon\\right)=0$, where $\\tilde{\\bm{x}}^{(k)}$ denotes the embedding obtained by augmenting $\\bm{x}$, and $\\bm{x}$ is the original embedding without augmentation. For the i.i.d.  %of independent and identically distributed \nrandom variables, as the sample size $k$ increases, the expectation of embedding after augmentation, $\\mathbb{E}(\\tilde{\\bm{x}}^{(k)})$ , tends toward the real mean $\\bm{x}$ (embedding without augmentation). \n%\n%\nIn contrast to FA, GA violates the weak law of large numbers. \n%\n%\n% We find that, unlike other vanilla data augmentations, graph augmentation tend to produce biased augmentations of which expectation doesn't converge to the original sample.\n%\n%\nAs shown in Figure~\\ref{fig:issuseb}, the population mean for the embedding obtained by the FA is in the densest region and in the proximity of the embedding of the original sample (without augmentation). In contrast, we cannot see such a trend in the case of the GA in Figure~\\ref{fig:issusea}. In other words, GA introduces some bias, whereas FA produces unbiased embeddings. \n\nWe assert that a successful contrastive objective should promote similarity/dissimilarity between features of encoded attributes by implicitly grouping/separating related/unrelated nodes according to their attribute space, respectively. However, as the GA strategy results in the bias (Figure~\\ref{fig:issusea}), attraction/separation of embeddings in the feature space does not necessarily result in an optimal attraction/separation of desired nodes in the attribute space, which may result in suboptimal pre-training for downstream tasks. Figure~\\ref{fig:conterexample} and Section~\\ref{sec:motivation} further illustrate and motivate the above two scenarios.\nFurthermore, the adoption of GA in GCL often increases the complexity as \nGCL compares the node features obtained from multiple views (\\eg, multiple network streams) to obtain correlated views of the same graph. However, this strategy is prohibitive on large graphs as, in the worst-case scenario, multi-view GCL requires a time and space complexity quadratic \\wrt the number of views and nodes. Thus, apart from the multi-view setting, we also investigate a single-view GCL setting.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Our Contributions.} Instead of the GA, we propose to perform augmentation on the hidden feature vectors (feature augmentation). Inspired by matrix sketching, we propose \\textbf{COSTA}, a novel \\textbf{CO}variance-pre\\textbf{S}erving fea\\textbf{T}ure space \\textbf{A}ugmentation framework for GCL, which produces augmented features by generating a ``good sketch'' of original features. To highlight the superiority of feature augmentation, apart from the multi-view setting, we show many results in the single-view setting, which conserves the memory usage and computations. We empirically show that COSTA (even the single-view variant, \\ie, COSTA$_{SV}$) achieves comparable or better results than other GA strategies. \n\n\\vspace{0.1cm}\nOur contributions are threefold:\n%\n\\renewcommand{\\labelenumi}{\\roman{enumi}.}\n\\hspace{-1.0cm}\n\\begin{enumerate}[leftmargin=0.6cm]\n\\item We point out the issue of bias introduced by the topology graph augmentation in the GCL framework, and we advocate feature augmentation strategies to prevent the aforementioned bias.\n\n\\item Inspired by matrix sketching, we propose COSTA, a simple and effective covariance-preserving feature augmentation framework for GCL, which generates augmented features by generating a ``good sketch'' (variance is bounded) of original features.\n\n\\item As an alternative to the multi-view GCL setting, we propose the single-view GCL setting, which produces equivalent or better results than the multi-view GCL while requiring less memory and incurring shorter computations.\n%\\vspace{-0.2cm}\n\\end{enumerate}\n\n%\\vspace{0.1cm}\nTo our best knowledge, this is the first work which considers feature augmentation (in the single-view setting) in GCL with the matrix sketching step performing feature augmentation.\n\n"
            },
            "section 2": {
                "name": "Related Works",
                "content": "\n\\label{sec:relatedwork}\n",
                "subsection 2.1": {
                    "name": "Data Augmentation",
                    "content": "\n\\vspace{0.1cm} \n\\noindent\\textbf{Input Space Augmentation},\n%Input space data augmentation, \n studied across many domains, usually refers to the augmentation performed in the input space. In computer vision, image transformations such as rotation, flipping, color jitters, translation, and noise injection~\\cite{shorten2019survey} as well as more recent cut-off and random erasure~\\cite{devries2017dataset} are very popular. In neural language processing, input space augmentations include token-level random augmentations such as synonym replacement, word swapping, word insertion, and deletion~\\cite{wei2019eda}. In the graph domain, input space augmentation is referred to as graph augmentation. Attribute masking, edge permutation, and node dropout are common graph augmentation strategies~\\cite{you2020graph}. Adaptive graph augmentations based on %heuristics such as \n node centrality and  PageRank centrality were studies by Zhu \\etal~\\cite{zhu2021graph} and Page \\etal~\\cite{page1999pagerank} respectively with the goal of masking different edges with varying probability. We discuss the negative effect of graph augmentation (\\eg, edge and node removal) later in the text. %in the following pages\n% It should be emphasized that, unlike other input space Date Augmentation, any topology-level graph augmentation will introduce bias into the hidden feature space.\n\n\\vspace{0.1cm} \\noindent\\textbf{Feature Augmentation }\n%Feature space augmentation \nstrategies generate augmented samples in the feature space instead of the input space~\\cite{feng2021survey}. Wang \\etal~\\cite{wang2019implicit} augment features in the hidden feature space, resulting in a feature representation that corresponds to another sample with the same class label but different semantics. So-called instance augmentations add perturbations to original instances~\\cite{wang2019implicit}. Many few-shot learning approaches \\cite{hariharan2017low} estimate the ``analogy transformations'' between examples of known classes to apply them to examples of novel classes.  \n%\nFinally, feature augmentations are popular in many research domains, \\ie, semi-supervised learning, one-shot learning, and few-shot learning. However, no prior work has combined FA with contrastive learning in the graph domain the way COSTA performs FA.\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Graph Contrastive  Learning",
                    "content": "\nInspired by contrastive methods in vision and NLP~\\cite{he2020momentum, chen2020simple, gao2021simcse}, CL has also been adapted to the graph domain. By adapting DeepInfoMax~\\cite{bachman2019learning} to graph representation learning, DGI~\\cite{velickovic2019deep} learns  embedding %of nodes and graphs \nby maximizing the mutual information to discriminate between nodes of original and corrupted graphs. REFINE \\cite{zhu2021refine} uses a simple negative sampling term inspired by skip-gram models. Fisher-Bures Adversarial GCN \\cite{uai_ke} uses adversarial perturbations of graph Laplacian. Inspired by SimCLR~\\cite{chen2020simple}, GRACE~\\cite{zhu2020deep}  correlates graph views by pushing closer representations of the same node in different views and pushing apart representations of different nodes. Another example of a SimCLR strategy is the recent GraphCL method~\\cite{hafidi2020graphcl}. In contrast to GRACE, which learns node embedding, GraphCL learns embeddings for graph-level tasks. The above multi-view  methods suffer from the large memory and computational footprint, respectively. Although COLES~\\cite{zhu2021contrastive} proposes a robust single-view GCL approach, it works the best with linear GNNs such as S\\textsuperscript{2}GC~\\cite{zhu2021simple}.\nThus, apart from multi-view COSTA, we also study the single-view GCL setting with FA. \n\n"
                }
            },
            "section 3": {
                "name": "Preliminaries",
                "content": "\n\\label{sec:SVGCL}\n",
                "subsection 3.1": {
                    "name": "Notations",
                    "content": "\nIn this paper, a graph with node features is denoted as $G=(\\mathcal{V}, \\mathcal{E}, \\bm{X})$, where $\\mathcal{V}$ is the vertex set, $\\mathcal{E}$ is the edge set, and $\\bm{X} \\in \\mathbb{R}^{n \\times d}$ is the feature matrix (\\ie, the $i$-th row of $\\bm{X}$ is the feature vector $\\bm{x}_i$ of node $v_{i}$). \n%\nLet $n=|\\mathcal{V}|$ and $m=|\\mathcal{E}|$ be the numbers of vertices and edges respectively. We use $\\bm{A} \\in\\{0,1\\}^{n \\times n}$ to denote the adjacency matrix of $G$, \\ie, the $(i, j)$-th entry in $\\bm{A}$ is 1 if and only if there is an edge between $v_{i}$ and $v_{j}$.\n%\nThe degree of a node $v_{i}$, denoted as $d_{i}$, is the number of edges incident with $v_{i}$.\n%\nThe degree matrix $\\bm{D}$ is a diagonal matrix, and its $i$-th diagonal entry is $d_{i}$. \n%\nFor a $d$-dimensional vector, $\\bm{x} \\in \\mathbb{R}^d, \\|\\bm{x}\\|_{2}$ is the Euclidean norm of $\\bm{x}$.\n%\nWe use ${x}_{i}$ to denote the $i$-th entry of $\\bm{x}$, and $\\operatorname{diag}(\\bm{x}) \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix such that the $i$-th diagonal entry is $x_{i}$. \n%\nWe use $\\bm{A}_{i:}$ and $\\bm{A}_{:i}$ to denote the $i$-th row and column of $\\bm{A}$ respectively, and ${A}_{ij}$ for the $(i, j)$-th entry of $\\bm{A}$. \n%\nThe trace of a square matrix $\\bm{A}$ is denoted by $\\operatorname{Tr}(\\bm{A})$, which is the sum along the diagonal of $\\bm{A}$. \n%\nThe singular value decomposition of $\\bm{A}$ is denoted as $\\bm{A} = \\bm{U}\\bm{\\Sigma}\\bm{V}^\\top$ where $\\bm{U}=\\left[\\bm{u}_{1}, \\ldots, \\bm{u}_{n}\\right], \\bm{\\Sigma}=\\operatorname{diag}\\left (\\sigma_{1}, \\ldots, \\sigma_{d}\\right)$, and $\\bm{V}=\\left[\\bm{v}_{1}, \\ldots, \\bm{v}_{d}\\right]$. \n%\nWe use $\\| \\bm{A} \\|_2$ to denote the spectral norm of $\\bm{A}$, which is the largest singular value $\\sigma_{\\max}$. \n%\nWe use $\\|\\bm{A}\\|_F$ for the Frobenius norm, which is $\\|\\bm{A}\\|_{\\mathrm{F}}=\\sqrt{\\sum_{i,j}\\left|a_{i j}\\right|^{2}}=\\sqrt{\\operatorname{\nTr}\\left(\\bm{A^\\top A}\\right)}=\\sqrt{\\sum_{i=1}^{d} \\sigma_{i}^{2}(\\bm{A})}$.\n\n"
                },
                "subsection 3.2": {
                    "name": "Multi-view Graph Contrastive Learning (MV-GCL)",
                    "content": "\nFollowing the conventions presented in \\cite{zhu2020deep, zhu2021graph},\n%\nMV-GCL learns node representations by maximizing the mutual information (MI) between views of the same graph.\n%\nBelow, we introduce components of MV-GCL: (i) graph augmentation, (ii) GNN-based encoders, (iii) projection head, and (iv) a contrastive loss. \n\n\\vspace{0.1cm}\n\\noindent\\textbf{Graph Augmentation.} %Graph Augmentation \n$\\mathcal{T}_{GA}$ generates augmented  $(\\tilde{\\bm{A}}, \\tilde{\\bm{X}})$ by directly adding random perturbations to the original graph $({\\bm{A}}, {\\bm{X}})$. \n%\nDifferent augmented graphs are constructed given one input $(\\bm {A}, \\bm {X})$, yielding correlated views $(\\tilde{\\bm{A}}_i, \\tilde{\\bm{X}}_i)$ that represent the augmented adjacent matrix and node features in the $i$-th view.\n%\nIn the common GCL setting~\\cite{zhu2020deep, zhu2021graph}, the graph structure is augmented via edge permutation. Node features are augmented via attribute masking.\n% \\item\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Graph Neural Network Encoders.} The GNN encoder $f_i:\n% \\mathbb{R}^{n\\timesn} \\times \\mathbb{R}^{n\\times|d|} \\rightarrow \\mathbb{R}^{n\\times|q|}$ \n\\mathbb{R}^{n\\times n} \\times \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n\\times d}$ \nextracts hidden node features $\\bm{H}_i\\in \\mathbb{R}^{n\\times d}$ from the $i$-th augmented graph $(\\tilde{\\bm{A}}_i, \\tilde{\\bm{X}}_i)$. \n%\nUsually, multiple encoders are applied to obtain the hidden node features $\\bm{H}_i$ of different views as:\n\\begin{equation}\n    \\bm{H}_1 = f_1 (\\tilde{\\bm{A}}_1, \\tilde{\\bm{X}}_1), \\cdots, \\bm{H}_k = f_k (\\tilde{\\bm{A}}_k, \\tilde{\\bm{X}}_k).\n\\end{equation}\nThe GNN encoders are implemented as a two-layer Graph Convolution Network (GCN):\n%\\vspace{-0.3cm}\n    \\begin{equation}\n    \\begin{aligned}\n    \\mathrm{GCN}_{l} (\\bm{X}, \\bm{A}) &=\\sigma\\left (\\hat{\\bm{D}}^{-\\frac{1}{2}} \\hat{\\bm{A}} \\hat{\\bm{D}}^{-\\frac{1}{2}} \\bm{X} \\bm{W}_l\\right), \\\\\n    f (\\bm{X}, \\bm{A}) &=\\mathrm{GCN}_{2}\\left (\\mathrm{GCN}_{1} (\\bm{X}, \\bm{A}), \\bm{A}\\right),\n    \\end{aligned}\n    \\end{equation}\nwhere $\\hat{\\bm{A}}=\\bm{A}+\\bm{I}$ is the adjacency matrix with self-loops, $\\bm{D}$ is the degree matrix, $\\sigma (\\cdot)$ is an activation function, \\eg, $\\operatorname{ReLU} (\\cdot)=\\max (0, \\cdot)$, and $\\bm{W}_l$ is a trainable weight matrix for the $l$-th layer.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Feature Augmentation $\\mathcal{T}_{FA}$.}\nWe apply feature augmentation on $\\bm{H}$. We elaborate on the proposed FA and detail its properties in Section~\\ref{sec:cpfa}. FA results in the augmented feature maps  fed into the projection head describe below.  \n\n\\vspace{0.1cm}\n\\noindent\\textbf{Projection Head.} The projection head $\\theta(\\cdot)$ is a small network that maps representations to the space where contrastive loss is applied. It is implemented as a multi-layer perceptron (MLP) with one hidden layer to obtain $\\bm{Z}_{i}=\\theta\\left (\\bm{H}_{i}\\right)=\\bm{W}^{ (2)} \\sigma\\left (\\bm{W}^{ (1)} \\bm{H}_{i}\\right)$, where $\\sigma$ is the ReLU non-linearity. As described in~\\cite{chen2020simple}, it is beneficial to define the contrastive loss on $\\bm{Z}_{i}$ rather than $\\bm{H}_{i}$.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Contrastive Loss}. Let two feature matrices $\\bm{U} \\in \\mathbb{R}^{n \\times d}$ and $\\bm{V} \\in \\mathbb{R}^{n\\times d}$, where $\\bm{U}=\\bm{Z}_1$ and $\\bm{V} = \\bm{Z}_2$ are node features obtained from two different views. Then for any node $i$, its embedding generated in one view, $\\bm{U}_{i:}$, is treated as the anchor, its embedding generated in another view, $\\bm{V}_{i:}$, forms the positive sample. Remaining node embeddings $\\bm{U}_{j:}$ and $\\bm{V}_{j:}$ such that $j\\neq i$ (from two views) are naturally regarded as negative samples. The contrastive loss function $\\mathcal{L}$ for all positive pairs is defined as:\n\\begin{equation}\n\\label{eq:mvcontrastiveloss}\n\\begin{aligned}\n&\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^{n} \\Big[{(\\bm{U}^\\top_{i:}, \\bm{V}_{i:})}/\\tau -\\log \\Big(\n\\sum_{j=1}^{n} e^{\\left (\\bm{U}_{i:}^\\top, \\bm{V}_{j:}\\right) / \\tau}+\\sum_{j=1}^{n} e^{\\left (\\bm{U}^\\top_{i:}, \\bm{U}_{j:}\\right) / \\tau}\\Big) \\Big].\n\\end{aligned}\n\\end{equation}\n\nNote that computing Eq.~(\\ref{eq:mvcontrastiveloss}) is both memory costly and time consuming as it requires the computation of three large similarity matrices, $\\bm{UV}^\\top$, $\\bm{UU}^\\top$, $\\bm{VU}^\\top \\in \\mathbb{R}^{n\\times n}$ for two views. Therefore, the memory consumption and runtime  depend on the number of views multiplied by the number of nodes, making such a multi-view setting challenging to run on large-scale graphs.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Single-view Graph Contrastive Learning (SV-GCL).} To validate the effectiveness of graph augmentation and feature augmentation, apart from MV-GCL, we use a special case of multi-view GCL that shares the same augmented graph for two views and is thus equivalent to single-view Graph Contrastive Learning (SV-GCL). We note that SV-GCL has a computational advantage, \\ie, only the features of a single view are calculated, and distances between features within the view. Distances between views are not needed. SV-GCL also provides a fairer way to compare the effectiveness of graph augmentations and feature augmentations as otherwise the multi-view setting would be the reason for the performance gain rather than the graph augmentation strategy. %And we will show that in our experiments.\\definecolor{beaublue}{rgb}{0.88,15,1}\n\\definecolor{blackish}{rgb}{0.2, 0.2, 0.2}\n\\definecolor{Gray}{gray}{0.9}\n\\definecolor{LightCyan}{rgb}{0.88,1,1}\n"
                }
            },
            "section 4": {
                "name": "Methodology",
                "content": "\nSection~\\ref{sec:motivation}  presents our motivation. \nSection \\ref{sec:cpfa} presents \\textbf{COSTA}, \\textbf{CO}variance pre\\textbf{S}erving fea\\textbf{T}ure space \\textbf{A}ugmentation framework. Section~\\ref{sec:matrixskeching} relates COSTA to the problem of matrix sketching, which generates desired augmented samples with theoretical guarantee. \n\n",
                "subsection 4.1": {
                    "name": "Motivation",
                    "content": "\n\\label{sec:motivation} \nWe motivate COSTA with a simple experimental example, which shows that the node embedding obtained under graph augmentation is highly biased compared to feature augmentation. Inspired by WLLN (the weak law of large numbers explained in the introduction), below we quantify the bias introduced by the data augmentation $\\mathcal{T}(\\cdot)$ as follows. Let $\\bm{x}_i$ be the original embedding of the $i$-th node and  $\\tilde{\\mathcal{X}}_i$ be its augmentation set where each embedding $\\tilde{\\bm{x}}^{(k)}_i\\in\\tilde{\\mathcal{X}}_i$ is obtained by stochastic transformation, \\ie, $\\mathcal{T}(f(\\cdot))$ or $f(\\mathcal{T}(\\cdot))$. Let the transformation distribution of $\\bm{x}_i$ be  $\\tilde{\\mathcal{T}}(\\bm{x}_i)$, then: % defined as:\n%\\vspace{-0.2cm}\n\\begin{equation}\n% \\begin{aligned}\n      \\text{Bias} (\\mathcal{T} (\\bm{x}_i))=\\Big\\| \\mathbb{E}_{\\tilde{\\bm{x}}_i \\sim \\tilde{\\mathcal{T}}(\\bm{x}_i)} (\\tilde{\\bm{x}}_i) - \\bm{x}_i \\Big\\|_2  \\approx \\Big\\|  \\frac{1}{|\\tilde{\\mathcal{X}}_i|}\\sum_{k=1}^{|\\tilde{\\mathcal{X}}_i|} \\tilde{x}^{(k)}_i\\!  - \\bm{x}_i \\Big\\|_2.\n% \\end{aligned}\n\\label{eq:bias}\n\\end{equation}\n% \\label{sec:motivation}\nFollowing Eq.~\\eqref{eq:bias}, given a randomly chosen node (Cora dataset), we randomly generate $|\\tilde{\\mathcal{X}}| = 500$ augmented samples by the graph augmentation (GA) by edge permutation, and the feature augmentation (FA) by directly adding random noise to $\\bm{x}$. In both cases, we use the same encoder to obtain the node embeddings. The output layer of the encoder has two dimensions to facilitate visualization. Figure~\\ref{fig:issuse} depicts the distribution of node embeddings for both types of augmentations. Clearly, the expectation of node embeddings $\\mathbb{E} (\\mathcal{T}_{\\text{FA}}({\\bm{x}}))$ (the blue star in Figure~\\ref{fig:issuseb}) obtained by the FA converges to its original embedding $\\bm{x}$ (the red triangle in Figure~\\ref{fig:issuseb}), whereas expectation of node embeddings $\\mathbb{E} (\\mathcal{T}_{\\text{GA}}({\\bm{x}}))$ obtained via the GA deviates far from $\\bm{x}$, indicating the following:\n\n\n\\definecolor{beaublue}{rgb}{0.9, 0.95, 0.9}\n\\definecolor{blackish}{rgb}{0.2, 0.2, 0.2}\n%\\vspace{-0.2cm}\n\\begin{tcolorbox}[width=1.0\\linewidth, colframe=blackish, colback=LightCyan, boxsep=0mm, arc=2mm, left=2mm, right=2mm, top=1mm, bottom=2mm]\n%\\begin{tcolorbox}[colframe=black]\n%\\vspace{-0.15cm}\n\\begin{equation}\n    \\operatorname{Bias}(\\mathcal{T}_\\text{GA}(\\bm{x}))\\gg\\operatorname{Bias} (\\mathcal{T}_\\text{FA}(\\bm{x})).\n\\end{equation}\n%\\vspace{-0.1cm}\n\\end{tcolorbox}\n\nAs shown, GA-based contrastive learning suffers from optimizing the biased setting. %augmented node embeddings. \n%As the metric learning type of loss function, \nA standard contrastive loss aims to %learn an instance discrimination features $f(\\bm{x})$ for downstream tasks, by \nmaximize the similarity of embeddings within the same augmentation set (\\eg, $\\tilde{\\bm{x}}^{(1)}_i$, $\\tilde{\\bm{x}}^{(2)}_i \\!\\in \\tilde{\\mathcal{X}}_i$ ) and minimize the similarity of embeddings between different sets (\\eg, $\\tilde{\\bm{x}}^{(1)}_i\\!\\in \\tilde{\\mathcal{X}}_i$ and $\\tilde{\\bm{x}}^{(1)}_j\\!\\in \\tilde{\\mathcal{X}}_j$). To perform well, such an objective requires the augmented embeddings to adhere to the unbiased case described above because as the bias tends to zero, the expectation of augmented embeddings converges to ${\\bm{x}}_i$, \\ie, $\\mathbb{E}({\\tilde{\\bm{x}}}_i)\\rightarrow{\\bm{x}}_i$. Thus, pushing away $\\tilde{\\bm{x}}^{(k)}_i$ from $\\tilde{\\bm{x}}^{(k)}_j$ if $i\\neq j$ separates embeddings of different instances, whereas the augmented embeddings $\\tilde{\\bm{x}}^{(k)}_i$  concentrate around  $\\bm{x}_i$. %, augmented samples away will lead to dissimilar the learned embedding $f(\\bm{x})$, \nIn contrast, when the bias is large, \\ie, $\\|\\mathbb{E}(\\tilde{\\bm{x}}_i) - \\bm{x}_i\\|_2 \\gg 0 $, separating augmented embeddings of different instance (\\ie, $\\tilde{\\bm{x}}_i^{(k)}$, $\\tilde{\\bm{x}}_j^{(k)}$, $i\\neq j$) may not increase the discrimination of learned embeddings (\\ie, $\\bm{x}_i, \\bm{x}_j)$ for downstream tasks. Figure~\\ref{fig:conterexample} (bottom) illustrates such a case.\n\nThis motivates us to explore new ways of performing augmentations for GCL. Instead of explicitly eliminating the bias in the current GCL, we apply  feature augmentations as an alternative.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Covariance-Preserving Feature Augmentation",
                    "content": "\n\\label{sec:cpfa} \nPrevious section indicates the graph augmentation produces the bias. We adopt the feature augmentation  for GCL loss because FA lets us  control the variance and reduce the bias. \n% Below we explain how to form it.\n%\n%\n% ideal augmented samples $\\tilde{x}$ of sample $\\bm{x}$, and samples $\\bm{x}$ themselves should all come from the same true distribution $p_{\\mathcal{X}}(\\bm{x})$. To that end, $p_{\\mathcal{X}} (\\bm{x})$ could be learned from real samples $\\bm{X}$ via the variational inference~\\cite{kipf2016variational} or adversarial training~\\cite{pan2018adversarially}. However, these methods are inefficient and require additional networks to train. \n% %\n% %\n% Instead of obtaining $\\bm{\\tilde{x}}$ via drawing from explicitly modeled $p_{\\mathcal{X}} (\\bm{x})$, we alternatively achieve the same effect by making second-order statistics $\\bm{\\tilde{X}}^\\top \\bm{\\tilde{X}}$ to be similar to $\\bm{{X}}^\\top \\bm{{X}}$ so that each $\\bm{\\tilde{x}} \\in \\bm{\\tilde{X}}$ can be regarded as being i.i.d sampled from a distribution $p_{\\tilde{\\mathcal{X}}} (\\bm{x}) \\sim p_{\\mathcal{X}} (\\bm{x})$. \n\n\\begin{tcolorbox}[width=1.0\\linewidth, colframe=blackish, colback=LightCyan, boxsep=0mm, arc=2mm, left=2mm, right=2mm, top=2mm, bottom=2mm]\n%\\begin{tcolorbox}[colframe=black]\nThus, we propose \\textbf{the feature augmentation framework} in which the augmented feature matrix $\\tilde{\\bm{X}} \\in \\mathbb{R}^{k \\times d}$, given the original feature matrix $\\bm{X}\\in \\mathbb{R}^{n\\times d}$, is obtained via:\n\\begin{equation}\n\\begin{aligned}\n    \\label{eq:coverr}\n    &\\tilde{\\bm{X}} = \\bm{PX} + \\bm{E},\\\\\n     \\text{such that }\\;&\\|\\bm{X}^{\\top}\\bm{X}-\\tilde{\\bm{X}}^{\\top}\\tilde{\\bm{X}}\\|_2 \\leq \\varepsilon \\text{Tr} (\\bm{X}^\\top\\bm{X}).\n\\end{aligned}\n\\end{equation}\n\\end{tcolorbox}\n\\noindent $\\bm{P}\\in \\mathbb{R}^{k \\times n}$ in Eq. \\eqref{eq:coverr} denotes an affine transformation, $\\bm{E}$ is the random noise matrix and $\\varepsilon$ is the error which controls the quality of approximation  $\\|\\bm{X}^{\\top}\\bm{X}-\\tilde{\\bm{X}}^{\\top}\\tilde{\\bm{X}}\\|_2$. Note that the affine transformation can be either deterministic or stochastic.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Connection to the Gaussian Noise Injection.} One special case of COSTA is the Gaussian noise injection \\cite{wang2019implicit,yu2022graph} which produces the augmented feature matrix $\\tilde{\\bm{X}}$ by adding random noise sampled %from $\\mathcal{N} (0, \\varepsilon\\bm{I})$ \nas $\\tilde{\\bm{X}}_{i:} \\sim \\!~\\! \\mathcal{N} (\\bm{X}_{i:}, \\varepsilon\\bm{I})$, where $\\varepsilon\\geq 0$ controls the strength of noise.\n%\nThis is equivalent to setting $\\bm{P} = \\bm{I}$ and $\\bm{E}_{i:} \\sim \\mathcal{N} (0, \\varepsilon\\bm{I})$ in Eq.~(\\ref{eq:coverr}). \n"
                },
                "subsection 4.3": {
                    "name": "Feature Augmentation via Matrix Sketching",
                    "content": "\n\\label{sec:matrixskeching}\n\\begin{definition}[Matrix Sketching~\\cite{liberty2013simple}]\n\\label{def:matrixsketch}\nLet $\\bm{X} \\in \\mathbb{R}^{n \\times d}$ be the given feature matrix, $\\bm{P} \\in \\mathbb{R}^{k \\times n}$ be a sketching matrix, \\eg, random projection or row selection matrix. The sketch of $\\bm{X}$ is defined as $\\tilde{\\bm{X}}=\\bm{P} \\bm{X} \\in \\mathbb{R}^{k \\times d}$. Usually, $\\tilde{\\bm{X}}$ contains fewer rows than $\\bm{P}$, where $k\\ll n$ but $\\tilde{\\bm{X}}$ still preserves many properties of $\\bm{P}$.\n\\end{definition}\n%By Definition~\\ref{def:matrixsketch}, \nEq.~(\\ref{eq:coverr}) %can be regarded as\nperforms matrix sketching.\n%\nObtaining the augmented feature matrix $\\tilde{\\bm{X}}$ requires  a good  sketch of $\\bm{X}$ such that second-order statistics of the original and sketched matrices are similar. \n%\nIn what follows, we use SVD, random row selection, or random projection to form a sketch of $\\bm{X}$. We %theoretically \nprove that $\\tilde{\\bm{X}}$ obtained by sketching satisfies $\\| \\bm{X}^\\top\\bm{X}-\\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}} \\|_2 \\leq \\varepsilon \\operatorname{Tr} (\\bm{X}^\\top\\bm{X})$ for small $\\varepsilon\\geq 0$.\n\n\\vspace{0.1cm} \n\\noindent\\textbf{Matrix Sketching via SVD.} One solution for Eq.~(\\ref{eq:coverr}) can be obtained through the singular value decomposition ($SVD$) where:\n\\begin{equation}\n    \\bm{P} = \\bm{U}^\\top, \\bm{X} = \\bm{U} \\bm{\\Sigma} \\bm{V}^\\top.\n\\end{equation}\n\\begin{lemma}\nLet $\\tilde{\\bm{X}} = \\bm{P}\\bm{X}$ and  $\\bm{X} = \\bm{U} \\bm{\\Sigma} \\bm{V}^\\top\\!$ where $\\,\\bm{U}=\\left [\\bm{u}_{1}, \\ldots, \\bm{u}_{n}\\right]$, $\\bm{\\Sigma }=\\operatorname{diag}\\left (\\sigma_{1}, \\ldots, \\sigma_{d}\\right)$, $\\bm{V}=\\left[\\bm{v}_{1}, \\ldots, \\bm{v}_{d}\\right]$. Then $\\|\\bm{X}^\\top\\bm{X}-\\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2$ is bounded as:\n%\\vspace{-0.3cm}\n\\begin{equation}\n    \\|\\bm{X}^\\top\\bm{X}-\\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 \\leq \\frac{\\sigma_{k+1}}{\\sigma_{\\max}} \\operatorname{Tr} (\\bm{X}^\\top\\bm{X}).\n\\end{equation}\n\\label{lemma:svd}\n\\end{lemma}\n%\\vspace{-0.5cm}\n\\begin{proof}\nSee Appendix~\\ref{proof:svd}.\n\\end{proof}\n%\\vspace{-0.2cm}\n\\begin{remark}\nThe upper bound of  $\\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}})\\|_2$ is controlled by the $(k+1)$-th largest eigenvalue $\\sigma_{k+1}$. Usually, $\\frac{\\sigma_{k+1}}{\\sigma_{\\max}}$ is small as $\\sigma_{k+1} \\ll \\sigma_{\\max}$ even when $k$ is small. However, SVD is computationally intensive and unsuitable for decomposition of large feature matrices.\n\\end{remark}\n\n\\vspace{0.1cm} \\noindent\\textbf{Random Row Selection (RS).}\nRandomized algorithms  trade accuracy for efficiency and strive for high accuracy and low runtime. \n%\nA sketch of a matrix $\\tilde{\\bm{X}}$ can be constructed via randomly stacking the rows of the original matrix $\\bm{X}$.\n%\nRandom row selection employs a small subset of rows based on a pre-defined probability distribution $\\mathcal{P}(i)$ to form a sketch. The random assignment matrix $\\bm{P}\\in \\mathbb{R}^{k \\times n}$  stacks one-hot vectors, \\ie, $\\bm{P}$ = $\\{\\bm{e}_{i} \\in \\mathbb{R}^n| \\mathcal{P} (i)=\\frac{\\|\\bm{X}_{i:}\\|_2}{\\|\\bm{X}\\|_F}\\} \\in \\mathbb{R}^{k \\times n}$, where $\\bm{e}_i$ indicates that the $i$-th row is selected.\n\\begin{lemma}\n Let $\\bm{X} \\in \\mathbb{R}^{n \\times d}$. Let $\\tilde{\\bm{X}} \\in \\mathbb{R}^{m \\times d}$ be a matrix whose rows are randomly selected from rows of  $\\bm{X} \\in \\mathbb{R}^{n \\times d}$. It holds that:\n\\begin{equation}\n \\mathcal{P}\\left(\\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 \\leq \\varepsilon \\operatorname{Tr} (\\bm{X}^\\top\\bm{X})\\right) \\geq 1 - e^ {\\left(-\\frac{(\\varepsilon \\sqrt{k}-1)^2}{8}\\right)}.\n\\end{equation}\n\\label{lemma:randomselect}\n%\\vspace{-0.4cm}\n\\end{lemma}\n\\begin{proof}\nSee Appendix~\\ref{proof:randomselect}.\n\\end{proof}\n%\\vspace{-0.5cm}\n%\\vspace{-0.6cm}\n\\begin{remark}\nThe failure probability $\\delta_{RS} = e ^{(-\\frac{(\\varepsilon \\sqrt{k}-1)^2}{8})}$ is exponentially decreasing with the error $\\varepsilon$ meaning that we can bound $\\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2$ given small $\\varepsilon\\geq 0$ with a high probability $1-\\delta_{RS}$.\n\\end{remark}\n\n\\vspace{0.1cm} \\noindent\\textbf{Random Projection (RP).}\nA sketch of matrix can be RP. The projection matrix is defined as $\\bm{P} \\in \\mathbb{R}^{n\\times k}$ whose entry $p_{ij}$ is sampled from $\\mathcal{N} (0, 1)$. %\nIdeally, we expect $\\bm{P}$ to provide a stable sketch that approximately preserves the distance between all pairs of columns in the original matrix. As the computation of dense matrix $\\bm{P}$ is time-consuming, a sparse version from Appendix~\\ref{sec:vsrp}) can be used.  \n\n\\begin{lemma}\n\\label{lemma:rp}\n let $\\tilde{\\bm{X}} = \\frac{1}{\\sqrt{k}}\\bm{P}\\bm{X}$ where $p_{ij}$ %\\in \\bm{P}$ and\n is the $(i,j)$-th element of $\\bm{P}$, and \n $p_{ij}\\sim \\mathcal{N}(0,1)$. For $\\varepsilon \\in (0,1)$, it holds that:\n\\begin{equation}\n\\label{eq:rpbound}\n \\mathcal{P}\\left(\\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 \\leq \\varepsilon \\operatorname{Tr} (\\bm{X}^\\top\\bm{X})\\right) \\geq 1 - e^{-\\frac{\\varepsilon^2k}{8}}.\n\\end{equation}\n\\end{lemma}\n%\\vspace{-0.5cm}\n\\begin{proof}\nSee Appendix~\\ref{proof:rp}.\n\\end{proof}\n%\\vspace{-0.5cm}\n\\begin{remark}\nNote that the failure probability of the random projection $\\delta_{PR}$ is less than $\\delta_{RS}$ when $k > \\varepsilon^{-2}$.\n\\end{remark}\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\nBelow, we  provide \n%an overview of the dataset statistics, \ndetails of experimental settings, and we discuss our results. We  answer the following research questions (\\textbf{RQ}):\n%\n\\begin{itemize}\n    \\item \\textbf{RQ1:} What is the bias problem in graph augmentation? \n    \\item \\textbf{RQ2:} Does the proposed %SV-GCL method with the \n    feature augmentation work for problems of practical interest? How does its accuracy/speed compare with   MV-GCL models that adopt the graph augmentation strategy, and with other models? Does SV-GCL perform well in comparison to MV-GCL models? \n    \\item \\textbf{RQ3:} What is the performance of the feature augmentation given different matrix sketching schemes? What are the major factors that contribute to the success of the proposed feature augmentation method?\n    \\item \\textbf{RQ4:} How is the effectiveness affected by the number of augmented samples?\n\\end{itemize}\n\n\n\n\n% \\subsection{The Role of Data Augmentation in Graph Contrastive Learning}\n% \\subsection{Comparison with the State-of-the-art Methods}\n",
                "subsection 5.1": {
                    "name": "The Bias Problem of Graph Augmentation with GNNs (RQ1)",
                    "content": "\nIn Section~\\ref{sec:motivation}, we intuitively point out the pitfall of the topology GA by providing an illustrative example. Based on that observation, we hypothesize that the  topology GA  introduces a substantial bias into the node embeddings used by the contrastive loss, which deteriorates the quality of features from the pre-training step, thus affecting downstream tasks. In this section, we conduct a quantitative analysis of this problem.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Experimental Protocol.}\nWe adopt the edge perturbation and attribute masking, the most commonly used strategies for the graph augmentation~\\cite{zhu2020deep, zhu2021graph, velickovic2019deep}. To show the difference between the attribute GA and the topology GA, we use MLP and GNN to encode the original features. For a fair comparison, MLP and GNN share the fixed weights. We only change the type of augmentations used to obtain the node embedding. \nSpecifically, We denote GNN with the edge perturbation (EP) as $gnn\\_e$, GNN with attribute masking (AM) as $gnn\\_a$, GNN with AM and EP as $gnn\\_ea$, MLP with AM as $nn\\_a$.\n% All the other experimental factors are the same. As a result, we define four variants, $nn\\_f, gnn\\_f, gnn\\_e, gnn\\_ef$ corresponding to MLP with attribute masking, GNN with attribute masking, GNN with edge perturbation, and GNN with attribute masking plus edge perturbation, respectively.\nFor each node from Cora, we generate $500$ augmented samples in four variants to compute their bias by Eq.~(\\ref{eq:bias}). \n\n\\vspace{0.1cm}\n\\noindent\\textbf{Bias Patterns of Graph Augmentations.}\n%We draw the distribution of bias. Apparently, as shown in \\\nFigure~\\ref{fig:biasdistri} shows  distributions containing bias. It is obvious that $gnn\\_ea$ contains more node embeddings with a larger bias compared to $nn\\_ea$, judging by the %from the fact that the \ndistribution shift towards right. % toward higher bias.\nThis confirms our hypothesis that the graph augmentation introduces the bias. Moreover, %we note that \nthe long-tailed trend is mainly caused by the edge perturbation. Figure~\\ref{fig:bpdegree} shows that nodes with a small degree  exhibit more bias, which confirms our insight \n%The observation meets our expectation since, for a node, its embedding obtained via GNN is closely related to the node that it is connected to and, therefore, for a node with a small degree, \nthat for node embeddings of low-degree nodes, obtained via GNN, removing any edges perturbs these embedding significantly. Real-world graphs (citation network, social network, \\etc) follow the power-law distribution (shown in Figure~\\ref{fig:powlaw}) meaning a few of nodes connect with the majority of the edges, whereas the majority of nodes have only a few of edges (low degree). For example, 54\\%, 68\\%, 71\\%, 53\\% nodes of Cora, CitSeer, PubMed, and DBLP datasets have less than 3 edges. Thus, applying the graph augmentation in %, the node embeddings used for \ncontrastive learning results in a large bias.%loss are biased in general.\n\n% \\begin{table}[t]\n%  \\caption{Graph Aug. + self-contrast}\n%     \\label{tab:my_label}\n%     \\centering\n%     \\begin{tabular}{c|ccc}\n%     \\toprule\n%     \\toprule\n%          Method & Cora & Citeseer & WikiCS\\\\\n%     \\midrule\n%          Graph Aug. + self-contrast&  82.1 & 70.93 &  77.67\\\\\n%     \\bottomrule\n%     \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Comparison with the State-of-the-Art Methods (RQ2)",
                    "content": "\nIn this section, we compare COSTA to other baseline models to answer \\textbf{RQ2}. We use the same experimental setup as the representative MV-GCL method (\\ie, GCA~\\cite{zhu2021graph}, and GRACE~\\cite{zhu2020deep}) to perform a fair comparison to these methods. %Without special statements, \nUnless stated otherwise, the random projection is employed as the default setting of COSTA as it balances well between accuracy and efficiency. A detailed comparison between different feature augmentations given different matrix sketching schemes is shown in Section~\\ref{sec:otherfa}.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Datasets.}\nTo evaluate our method, we adopt nine commonly used benchmark datasets in the previous works~\\cite{zhu2020deep, zhu2021graph,velickovic2019deep}, including citation networks (Cora, CiteSeer, Pubmed, DBLP, Coauthor-CS, and Coauthor-Physics) and social networks (Wiki-CS, Amazon-Computers, Amazon-Photo)~\\cite{kipf2016semi,sinha2015overview,mcauley2015image, mernyei2020wiki}. Detailed descriptions and statistics are given in Appendix~\\ref{sec:dataset}. Apart from Wiki-CS adopting the public split, other datasets are randomly divided into 10\\%, 10\\%, 80\\% for training, validation, and testing.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Evaluation Protocol.} For each experiment, we adopt the same %linear \nevaluation scheme as in~\\cite{velickovic2019deep, zhu2020deep, zhu2021graph}, where each model is firstly trained in an unsupervised manner on the whole graph with node features. Then, we transform the raw features into the resulting embeddings with the use of the trained encoder. Next, we train an $\\ell_2$-regularized logistic regression classifier from the Scikit-Learn library~\\cite{pedregosa2011scikit} with the use of embeddings obtained in the previous step. We also perform a grid search over the regularization parameter with the following values $\\{2^{-10}, 2^{-9}, \\dots , 2^{-1}\\}$. \nWe compute the classification accuracy and report the mean and standard deviations for 20 model initializations and splits. \n\n\\vspace{0.1cm}\n\\noindent\\textbf{Baselines.} \nTo compare COSTA with previous works, we choose the representative baselines from traditional graph self-supervised learning, autoencoder-based model, and contrastive-based graph self-supervised learning.  Methods include (i) Random walk based models: Deepwalk~\\cite{perozzi2014deepwalk} and node2vec~\\cite{grover2016node2vec}, (ii) Autoencoder Based models: GAE and VGAE~\\cite{kipf2016variational}, (iii) the contrastive-based models including Deep Graph Infomax (DGI)~\\cite{velickovic2019deep}, Graphical Mutual Information Maximization (GMI)~\\cite{peng2020graph}, Graph Barlow Twins (G-BT)~\\cite{bielak2021graph} and Multi-View Graph Representation Learning (MVGRL)~\\cite{hassani2020contrastive}, GRACE~\\cite{zhu2020deep} and GCA~\\cite{zhu2021graph}. Note that all the contrastive models use the topology graph augmentation by default. \n% Furthermore, To directly compare our proposed method with supervised counterparts, we also report the performance of two representative models Graph Convolutional Networks (GCN)\\cite{kipf2016semi} and Graph Attention Networks (GAT) \\cite{velivckovic2017graph}, where they are trained in an end-to-end manner. \nFor all baselines, we report their performance based on the official implementations and we use  use default hyper-parameters from original papers. \n\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Main Results.}\nTables~\\ref{tab:mainresult1} and \\ref{tab:mainresult2} %~3 \nshow that COSTA achieves competitive performance compared to the baseline methods, and even surpasses them on most datasets. These results demonstrate that COSTA is an effective framework leveraging the advantage of feature augmentations. Specifically, the superiority of COSTA is confirmed by the fact that both single- and multi-view COSTA variants, COSTA$_{SV}$ and COSTA$_{MV}$, outperform several MV-GCL models that use the topology graph augmentation (\\ie, GCA, GRACE, MVGRL) on several datasets (Cora, CiteSeer, DBLP, Wiki-CS, Amazon-Computers, AM-Photo and Coauthor-Physics) and achieve comparable results on PubMed, and Coauthor-CS datasets. We note that  datasets on which COSTA$_{SV}$ does not achieve SOTA have a small number of nodes with low node degrees (\\ie, only around 10\\% of nodes in Amazon-photo and Coauthor-Physics have the degree less than 3, meaning less bias is introduced by the topology GA. However, COSTA$_{SV}$ requires less runtime and memory  to achieve the comparable performance. This is attributed to the single-view design (SV-GCL) of COSTA$_{SV}$ (Section~\\ref{sec:SVGCL}). We also note that COSTA$_{MV}$ typically outperforms COSTA$_{SV}$ by a small margin which suggests that single-view augmentation strategies are a good choice for GA.   In addition, we note that most of the contrastive learning models outperform models based on the reconstruction error (\\ie, GAE, VGAE, DeepWalk, Node2Vec), which reflects the superiority of contrastive learning.\n\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Running time.} \nWe measure the runtime to further validate the practicality of the single-view COSTA (COSTA$_{SV}$) in terms of time complexity. We mainly compare it to GCA and GRACE (Amazon-Computers dataset) because both GCA and GRACE are the representative models for the multi-view contrastive learning framework utilizing graph augmentations. Note that  GCA uses adaptive graph augmentations. To form  Figure~\\ref{fig:runningtime}, we sampled a subgraph with a fixed number of nodes from $1,000$ to $8,000$. Figure~\\ref{fig:runningtime} shows the training time for 1,000 epochs given  different numbers of nodes. The figure shows that our method is faster than the other two models. What stands out is that the gap between them becomes more apparent as the number of nodes increases. COSTA$_{SV}$ becomes 2 times faster than the other models at $\\geq 5,000$ nodes. We attribute this to the single-view setting with the feature augmentation. Note that COSTA$_{SV}$  computes the node feature matrix once before feeding it into the projection head, and the feature augmentation %lets us  downsample the augmented features whose number is less than the number of nodes. %These two facts lead to the computation of \neffectively can be understood as reducing the number of nodes, as our feature augmentation acts on columns of hidden feature matrix. \nThus, we form one relatively small similarity matrix for the contrastive loss. \n% in Eq.~(\\ref{eq:svgclloss})\nIn contrast, the MV-GCL framework incurs a higher complexity. Our experiments confirm that COSTA$_{SV}$ is  efficient in practice. Moreover, COSTA$_{SV}$ can be accelerated by employing sketching by a sparse matrix without sacrificing its performance, as shown in Figure~\\ref{fig:density} and Appendix~\\ref{sec:vsrp}.\n\n"
                },
                "subsection 5.3": {
                    "name": "Ablations/Performance Analysis (RQ3 \\& 4)",
                    "content": "\n% In this section, we performed set of qualitative analyses on in order to better understand the properties of feature augmentation. \n\nBelow we use the single-view COSTA (COSTA$_{SV}$), as multi-view COSTA has a similar performance. \nWe firstly show the superiority of %COSTA with \nrandom projection by comparing it with other matrix sketching variants. Subsequently, we ablate and discuss the factors that lead to the success of the random projection. Finally, we show the effect of the number of augmented samples that influence the error bound.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Comparison with Other Feature Augmentations.}\n\\label{sec:otherfa}\nWe compare the random projection with other matrix sketching strategies. Table~\\ref{tab:diffaugmentation} uses the common tested based on COSTA$_{SV}$: only the definition of $\\bm{P}$ and $\\bm{E}$ are varied. We note that the random projection  works consistently better than all the other strategies as the random projection introduces a lesser error compared to the random selection and random noise strategies. It is somewhat surprising that although the random projection is not the optimal solution to Eq.~(\\ref{eq:coverr}), % with minimal error, \nit still outperforms the SVD-based  sketching. Such a good performance comes from the following facts: (i) the error bound of random projection is sufficiently small to maintain a good sketch; and (ii) compared to SVD, which is deterministic, random projection adds stochastic perturbations to the model (variance is a source of feature augmentations), which serves as a regularization. % that prevents the risk of overfitting.\n\n\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Ablations \\wrt the Augmentation Type in GCL.}\nBelow, we investigate different augmentation types, \\ie, the topology graph augmentation and the feature augmentation. To minimize other factors other than the augmentation strategy that might affect the results (\\ie, multi-view setting), we opt for the single-view COSTA. %setting where only one graph view is used. % for GCL. \nWe only replace the augmentation type. Table~\\ref{tab:AblationStudyOnAug} shows that  without any augmentations, the model performs badly, showing the necessity of data augmentations. Furthermore, we observe that applying either the topology graph augmentation or the feature augmentation can improve results. However, the improvement of feature augmentation is larger compared to the topology graph augmentation, highlighting  the effectiveness of COSTA. %This is probably because the topology graph augmentation affects the eigenspace alignment of graph views~\\cite{DBLP:journals/corr/abs-2106-12484} thus, highly rely on the multi-view setting. \n%We also notice the improvements are minor using both types of augmentations, again strengthening  the argument for the effectiveness of the feature augmentation.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Ablations on the Random Projection.}\nTo see where the performance improvement comes from, we conduct an ablation study on the COSTA$_{SV}$ with the random projection. Firstly, we  fix the random projection so that it remains the same in each epoch, denoting this variant as $ran\\_proj\\_fix$, and then we eliminate the random projection completely ($none\\_ran\\_proj$). We experiment on Cora, CiteSeer, PubMed, and DBLP. Figure~\\ref{fig:ablation} shows that fixing the random projection throughout the experiment causes the performance drop in all four datasets. In contrast, forming a new random projection matrix per epoch generates a variety of  feature augmentations obeying the variance bound of random projection. As projecting is performed along columns of the hidden feature matrix, this is an equivalent of drawing different new nodes (obeying the mean and variance) with each change of random projection. %, which has a regularization which improves the generalization. \nNotably, removing the random projection completely from training  decreases the accuracy on three datasets, as expected. % indicating the necessity of the feature augmentation in SV-GCL.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Downsampling \\vs Upsampling the Node Dimension.}\n\\label{sec:downsampling}\nAccording to the error bound derived in Eq.~(\\ref{eq:rpbound}), the bound is related to the number of augmented samples $k$. The use of COSTA lets us control the number of augmented features by adjusting the number of rows of $\\bm{P} \\in \\mathbb{R}^{k\\times n}$ in Eq.~(\\ref{eq:coverr}). Downsampling and upsampling are applied by setting $k<|\\mathcal{V}|$ and $k>|\\mathcal{V}|$ respectively. We denote the $\\frac{k}{|\\mathcal{V}|}$ as the reduction ratio. Figure~\\ref{fig:reduction} shows the relation between the performance and the reduction ratio. Specifically, we obtain the best performance for downsampling (the sketched rows are fewer than the number of nodes), which also accelerates computations of the contrastive loss (smaller similarity/dissimilarity matrices).\n"
                }
            },
            "section 6": {
                "name": "Conclusions",
                "content": "\nWe have quantitatively and qualitatively analyzed the problems stemming from the topology graph augmentation of current GCL methods, and we have shown that such a strategy suffers from the bias problem. To overcome this bias, we have proposed the feature augmentation framework COSTA. We theoretically proved that the quality of augmented features obtained via COSTA are guaranteed and COSTA accelerates the speed of GCL by working well in the single-view mode. Our results are equivalent or better  than results of the standard   contrastive multi-view graph augmentation models that rely on topology-based augmentations.% \\end{itemize}\n%***************************************************\n\n"
            },
            "section 7": {
                "name": "Acknowledgments",
                "content": "\n%We thank anonymous reviewers for their valuable comments. \nThis work was supported by the National Key Research and Development Program of China (No. 2018AAA0100204) and the Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 2410021, Research Impact Fund, No. R5034-18).\n\n\n\\bibliographystyle{plain}\n\\bibliography{kdd}\n%\\newpage\n\\appendix\n\n\\vspace{0.3cm}\n"
            },
            "section 8": {
                "name": "------ Appendices ------",
                "content": "\n\n\\vspace{0.3cm}\n\n%\\section{Appendix} \n% \\subsection{Single-veiw graph contrastive learning \\noindent(\\textbf{SV-GCL})}\n% \\label{sec:SVGCL}\n% The proposed SV-GCL framework follows the common GCL paradigm, except that SV-GCL does not perform the graph augmentation but it performs the feature augmentation and it adopts the single-view setting. %In contrast to the MV-GCL, \n% Below, we highlight several key points by introducing each module of our SV-GC.  Figure~\\ref{fig:model} shows our  framework. Similar to MV-GCL, the SV-GCL also has four major components:\n\n% \\vspace{0.1cm}\n% \\noindent\\textbf{Graph Neural Network Encoder.} \n%  Encoder $f (\\cdot)$ transforms the original graph $ (\\bm{A}, \\bm{X})$ into a node feature vectors $\\bm{X} \\in \\mathbb{R}^{n \\times d}$ (row vectors of $\\bm{X}$). The %goal of the \n%  GNN encoder %, in contrast to MV-GCL, is to acquire\n%  provides us the hidden node features on which we apply augmentation instead of generating correlated views \\etc. %for contrasting. \n%  The architecture of this encoder follows MV-GCL. However, SV-GCL does not require multiple encoders because the node embeddings are acquired only once %using the original graph. This results in\n%  as our idea works with a single-view architecture, requiring less memory/shorter runtime. \n \n% \\vspace{0.1cm}\n% \\noindent\\textbf{Feature Augmentation.}  In place of $\\mathcal{T}_{\\text{feature}}$, \n% we apply an affine transformation $\\tilde{\\bm{X}}= \\bm{P}\\bm{X} + \\bm{E}$ as our feature augmentation to generate augmented samples $\\tilde{\\bm{X}}\\in\\mathbb{R}^{k\\times d}$ (row vectors of $\\tilde{\\bm{X}}$) from the node embeddings $\\bm{X}\\in \\mathbb{R}^{n\\times d}$, where $\\bm{P}\\in\\mathbb{R}^{k \\times n}$ is the projection matrix and $\\bm{E}\\in \\mathbb{R}^{k\\times d}$ is the random noise matrix. Section~\\ref{sec:matrixskeching} discusses ways to form  $\\bm{P}$ and corresp. guarantees. % discusses how to obtain $\\bm{P}$ under certain conditions. %in Section~\\ref{sec:matrixskeching}.\n% % Ideally, each augmented sample $\\tilde{\\bm{X}}_{i:} \\in \\tilde{\\bm{X}}$ is independently drawn from the same dution as $\\bm{X}\\in \\mathbb{R}^{n\\times d}$ as: $\\tilde{\\bm{X}}_{i:} \\sim p_{\\bm{X}}(\\bm{X}_{:i})$. \n% %Note that, we directly apply augmentation on the node embedding rather than the input graph.\n\n% \\vspace{0.1cm}\n% \\noindent\\textbf{Projection Head.} SV-GCL adopts the same projection head $\\theta (\\cdot)$ as MV-GCL for mapping the augmented samples $\\tilde{\\bm{X}}$ into the space on which we apply the contrastive loss.\n\n% \\vspace{0.1cm}\n% \\noindent\\textbf{Self-Contrastive Loss.} To compute the loss of SV-GCL with the feature augmentation, we begin by obtaining $k$  feature vectors of sample augmentations %$\\mathcal{T}_{\\text{feature}}$, \n% forming the feature matrix $\\tilde{\\bm{X}} \\in \\mathbb{R}^{k \\times d}$. As each feature vector $\\tilde{\\bm{X}}_{i:}$ of augmented sample is generated independently based on the real sample and following some distribution (\\eg, with the unbiased mean matching that sample and bounded variance), we compute only the self-similarity of each feature vector $\\tilde{\\bm{X}}_{i:}$ with itself %($\\tilde{\\bm{X}}_{i:}$)\n% \\eg,  $\\theta(\\tilde{\\bm{X}}_{i:},{\\tilde{\\bm{X}}_{i:}}) = C$. The negative samples are the remaining augmented samples $\\tilde{\\bm{X}}_{j:}$ such that $j \\neq i$. Thus, optimizing Eq.~(\\ref{eq:mvcontrastiveloss}) is equivalent to minimizing the following objective:\n%     \\begin{equation}\n%           \\mathcal{L} = \\frac{1}{k}\\sum_{i=1}^k\\left[C -\\log \\left (\\sum_{j\\neq i} e^{\\theta\\left (\\tilde{\\bm{X}}_{i:}, \\tilde{\\bm{X}}_{j:}\\right) / \\tau}\\right)\\right].\n%           \\label{eq:svgclloss}\n%     \\end{equation}\n% %which is akin to maximizing pairwise distances with the LogSumExp transformation. \n% Intuitively, pushing all feature vectors away from each other should indeed cause them to be roughly uniformly distributed~\\cite{wang2020understanding}. Obviously, Eq.~(\\ref{eq:svgclloss}) is computationally more efficient than the Eq.~(\\ref{eq:mvcontrastiveloss}) as only one similarity matrix $\\bm{MM}^\\top$ is computed and stored. Notice that the size of the matrix $\\tilde{\\bm{X}}$ is controlled by the number of augmented samples $k$. We further demonstrate that we can substantially reduce runtime and memory consumption by a downsampling strategy ($k \\ll |\\mathcal{V}|$) while still obtaining an equivalent or better results than the MV-GCL model (\\eg, see Section~\\ref{sec:downsampling}).\n"
            },
            "section 9": {
                "name": "Error Bounds of Different Feature Augmentations Preserving Second-order Statistics",
                "content": "\n% \\subsection{Proof of Lemma~\\ref{lemma:svd}}\n% \\subsection{Lemma Proof}\n\\noindent\\textbf{Proof of Lemma~\\ref{lemma:svd} (Matrix Sketching via SVD).}\n\\label{proof:svd}\n\\begin{proof}\nAccording to the Eckart-Young-Mirsky theorem~\\cite{golub1987generalization},  $\\bm{X}_k = \\bm{P}^\\top \\bm{P} \\bm{X}$ is  the best $k$-rank approximation of $\\bm{X}$ and  $\\|\\bm{X}-\\bm{X}_k\\|_2 = \\sigma_{k+1}$. Thus, we have:\n\\begin{equation*}\n\\begin{aligned}\n    \\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}})\\|_2 &= \\|\\bm{X}^\\top \\bm{X} - \\bm{X}^\\top \\bm{P}^\\top \\bm{P}\\bm{X}\\|_2, \\\\\n    & = \\|\\bm{X}^\\top(\\bm{X} - \\bm{P}^\\top\\bm{P}\\bm{X})\\|_2 = \\|\\bm{X}^\\top(\\bm{X} - \\bm{X}_k)\\|_2,\\\\\n    & \\leq \\|\\bm{X}^\\top\\|_2\\| \\bm{X} - \\bm{X}_k\\|_2 = \\|\\bm{X}\\|_2^2 \\frac{\\|\\bm{X} - \\bm{X}_k\\|_2}{\\|\\bm{X}\\|_2}, \\\\\n    &\\leq \\frac{\\sigma_{k+1}}{\\sigma_1} \\|\\bm{X}\\|^2_F = \\frac{\\sigma_{k+1}}{\\sigma_1} \\operatorname{Tr} (\\bm{X}^\\top\\bm{X}).\n\\end{aligned}\n\\end{equation*}\n\\end{proof}\n\\noindent\\textbf{Proof of Lemma~\\ref{lemma:randomselect} (Random Row Selection).}\n\\label{proof:randomselect}\nTo prove this Lemma, we use the following theorem from~\\cite{drineas2006fast}.\n\\begin{theorem}\nLet $\\bm{A} \\in \\mathbb{R}^{d \\times n}, \\bm{B} \\in \\mathbb{R}^{r \\times d}$ and $k \\in \\mathbb{Z}^{+}$such that $1 \\leq k \\leq n$ and $\\left\\{p_{i}\\right\\}_{i=1}^{n}$ be probability distribution over rows of $\\bm{A}$ and columns of $\\bm{B}$ such that $p_{i} \\geq \\frac{\\beta \\|\\bm{A}_{i:}\\|_2 \\| \\bm{B}_{i:} \\|_2}{\\sum_{j=1}^{n}\\| \\bm{A}_{j:}\\|_2 \\|\\bm{A}_{j:} \\|_2}$ for some positive constant $\\beta \\leq 1$. If matrix $C \\in \\mathbb{R}^{d \\times k}$ is constructed by sampling columns of $A$ according to $\\left\\{p_{i}\\right\\}_{i=1}^{n}$ and matrix $\\bm{D} \\in \\mathbb{R}^{k \\times r}$ is constructed by picking same rows of $\\bm{B}$, then with probability at least $1-\\delta$:\n$$\n\\|\\bm{AB}-\\bm{CD}\\|_{F}^{2} \\leq \\frac{\\mu^{2}}{\\beta c}\\|\\bm{A}\\|_{F}^{2}\\|\\bm{B}\\|_{F}^{2},\n$$\nwhere $\\delta \\in(0,1), \\mu=1+\\sqrt{(8 / \\beta) \\log (1 / \\delta)}$.\n\\label{theorem:1}\n\\end{theorem}\n\nWe set $\\bm{A} = \\bm{X}^\\top \\in \\mathbb{R}^{d\\times n}$, $\\bm{B} = \\bm{X} \\in \\mathbb{R}^{n \\times d}$, $\\bm{C} = \\tilde{\\bm{X}}^\\top \\in \\mathbb{R}^{d \\times k}$ and $\\bm{D} = \\tilde{\\bm{X}} \\in \\mathbb{R}^{k \\times d}$. Note that the distribution $p_{i} = \\frac{\\beta \\|\\bm{A}_{i:}\\|_2 \\| \\bm{B}_{i:} \\|_2}{\\sum_{j=1}^{n}\\|\\bm{A}_{j:}\\|_2 \\|\\bm{A}_{j:} \\|_2} = \\frac{ \\|\\bm{X}_{i:}\\|}{\\|\\bm{X}\\|_F}$ holds with $\\beta = 1$. Using theorem~\\ref{theorem:1}, we obtain bound:\n\\begin{equation}\n    \\begin{aligned}\n    &\\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top \\tilde{\\bm{X}}\\|_F^2 \\leq \\frac{\\mu^{2}}{k}\\|\\bm{A}\\|_{F}^{4}, \\\\\n    \\Rightarrow &\\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top \\tilde{\\bm{X}}\\|_F \\leq \\frac{\\mu}{\\sqrt{k}}\\|\\bm{A}\\|_{F}^{2}  \\text{   (As $\\|\\bm{X}\\|_2 \\leq \\|\\bm{X}\\|_F$)},\\\\\n    \\Rightarrow & \\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top \\tilde{\\bm{X}}\\|_2 \\leq \\frac{\\mu}{\\sqrt{k}}\\operatorname{Tr}(\\bm{X}^\\top\\bm{X}),\n    \\end{aligned}\n\\end{equation}\nwhere $\\delta \\in(0,1), \\mu=1+\\sqrt{8\\log (1 / \\delta)}$. By setting $\\frac{\\mu}{\\sqrt{k}} = \\varepsilon$, we have:\n\\begin{equation}\n    \\mathcal{P}(\\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top \\tilde{\\bm{X}}\\|_F \\leq \\varepsilon \\operatorname{Tr}(\\bm{X}^\\top\\bm{X})) \\geq 1 - e^{\\left( -\\frac{(\\varepsilon \\sqrt{k}-1)^2}{8}\\right)},\n\\end{equation}\nwhich completes the proof.\n\n\\noindent\\textbf{Proof of Lemma~\\ref{lemma:rp} (Random Projection).}\n\\label{proof:rp}\nWe prove the lemma by showing the following inequality  holds for any $\\bm{x} \\in \\mathbb{R}^{n}$:\n\\begin{equation}\n\\label{eq:normpre}\n    \\mathcal{P}\\left ( (1-\\varepsilon)\\|\\bm{x}\\|_2^{2} \\leq\\left\\|\\frac{1}{\\sqrt{k}} \\bm{P} \\bm{x}\\right\\|^{2} \\leq (1+\\varepsilon)\\|\\bm{x}\\|_2^{2}\\right) \\geq 1-e^{ \\left(-\\frac{k\\varepsilon^2}{8}\\right)}.\n\\end{equation}\nWe firstly show that  $\\mathbb{E}\\left[\\left\\|\\frac{1}{\\sqrt{k}} \\bm{A} \\bm{x}\\right\\|^{2}\\right]=\\|\\bm{x}\\|_2^2$:\n\\begin{equation}\n\\begin{aligned}\n          & \\mathbb{E}\\left[\\frac{1}{\\sqrt{k}} \\bm{P}\\bm{x}\\right] = \\mathbb{E}\\left[\\sum_i^k \\frac{1}{k}\\sum_{j}(P_{ij} x_j)^2\\right] = \\mathbb{E}\\left[\\sum_i^k\\frac{1}{k}\\sum_{j,j'}(P_{ij} P_{ij'} x_j x_{j'})\\right]\\\\\n          & = \\sum_i^k\\frac{1}{k}\\mathbb{E}\\left[\\sum_j \\bm{P}^2_{jj}\\bm{x}^2_j\\right] =\\sum_i^k\\frac{1}{k} \\sum_j x^2_j\\\\\n          & = \\|\\bm{x}\\|^2_2.\n\\end{aligned}\n\\end{equation}\nThis implies $\\bm{X}_j \\sim \\mathcal{N}(0, 1))$ where $X_j = \\frac{\\bm{A}_{j:}\\bm{x}}{\\|\\bm{x}\\|_2}$. Then we obtain:\n\\begin{equation}\n\\begin{aligned}\n& \\mathcal{P}(\\| \\frac{1}{\\sqrt{k}}\\|\\bm{P}\\bm{x}\\|_2^2 > (1+\\varepsilon)\\|\\bm{x}\\|_2)  = \\mathcal{P}(\\sum_{j=1}^k X^2_j  > (1+\\varepsilon)k),\\\\\n&=\\mathcal{P}\\left(e^{t \\sum_{i=1}^{k} X_{j}^{2}}>e^{t(1+\\varepsilon) k}\\right)\\\\\n& \\leq \\frac{\\mathbb{E}\\left[e^{t \\sum_{j=1}^k X_{j}^{2}}\\right]}{e^{t(1+\\varepsilon) k}} \\quad\\text{  (apply Markovs inequality)}\\\\ \n&= \\frac{\\prod_{j=1}^{k} \\mathbb{E}\\left[e^{t X_{j}^{2}}\\right]}{e^{t(1+\\varepsilon) k}} \\quad\\text{      (as $X_j$ is i.i.d)}\\\\ \n&= \\frac{(\\mathbb{E}\\left[e^{t X_{j}^{2}}\\right])^k}{e^{t(1+\\varepsilon) k}}  \\text{      (for $1<t<1/2$, it holds that $\\mathbb{E}\\left[e^{t X_{i}^{2}}\\right] \\leq\\left(\\frac{1}{\\sqrt{1-2 t}}\\right)$)}\\\\\n& \\leq \\left(\\frac{1}{\\sqrt{1-2 t}}\\right)^{k}\\left(\\frac{1}{e^{t(1+\\varepsilon)}}\\right)^{k}\\\\\n& =\\left(e^{-\\varepsilon+\\ln (1+\\varepsilon)}\\right)^{k / 2} \\quad\\text{ (using $\\ln (1+\\varepsilon) \\leq \\varepsilon-\\frac{\\varepsilon^2}{4}$)}\\\\\n& \\leq e^{-\\varepsilon^2k/8}.\n\\end{aligned}\n\\end{equation}\nThus, we have:\n\\begin{equation}\n    \\mathcal{P}(\\|\\ \\frac{1}{\\sqrt{k}}\\|\\bm{P}\\bm{x}\\|_2^2 \\leq (1+\\varepsilon)\\|\\bm{x}\\|^2_2)) \\leq 1 - e^{-\\varepsilon^2k/8}.\n\\end{equation}\nIn the similar way, it is easy to prove that:\n\\begin{equation}\n    \\mathcal{P}(\\|\\ \\frac{1}{\\sqrt{k}}\\|\\bm{P}\\bm{x}\\|_2^2 \\geq (1-\\varepsilon)\\|\\bm{x}\\|^2_2)) \\leq 1 - e^{-\\varepsilon^2k/8}.\n\\end{equation}\nThus, it holds that:\n\\begin{equation}\n\\mathcal{P}((1-\\varepsilon)\\|\\bm{x}\\|^2_2 \\leq \\frac{1}{\\sqrt{k}}\\|\\bm{P}\\bm{x}\\|_2^2 \\leq (1+\\varepsilon)\\|\\bm{x}\\|^2_2) \\geq 1 - e^{-\\varepsilon^2k/8}.\n\\end{equation}\n\n\nSuppose $\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}} \\succeq 0$ and $\\bm{x}$ is the eigenvector of $\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}$ corresponding to its largest eigenvalue $\\sigma_{\\max}$, then we have:\n\\begin{align}\n    \\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 &= \\bm{x}^\\top (\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}) \\bm{x} = \\sigma_{\\max} \\\\\n    & = \\bm{x}^T \\bm{X}^T\\bm{X} \\bm{x} - \\bm{x}^T \\tilde{\\bm{X}}^T\\tilde{\\bm{X}} \\bm{x} \\\\\n    & = \\|\\bm{Xx}\\|_2^2 - \\|\\tilde{\\bm{X}}\\bm{x}\\|_2^2 \\\\\n    & = \\|\\bm{Xx}\\|_2^2 - \\|\\bm{PXx}\\|_2^2 .\n\\end{align}\nApplying Eq.~(\\ref{eq:normpre}), there is at least probability $1 - e^{-\\varepsilon^2k/8}$ such that:\n\\begin{equation}\n   \\begin{aligned}\n    \\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 & \\leq \\|\\bm{Xx}\\|_2^2 + (\\varepsilon-1) \\|\\bm{Xx}\\|_2^2\\\\\n   & = \\varepsilon \\|\\bm{Xx}\\|^2_2 \\\\\n   &\\leq \\varepsilon \\|\\bm{X}\\|^2_2\\|\\bm{x}\\|^2_2 \\text{  $(\\|\\bm{x}\\|^2_2 = 1)$}\\\\ \n%   &\\leq \\varepsilon \\|\\bm{X}\\|^2_F\\\\\n   & \\leq \\varepsilon \\operatorname{Tr}(\\bm{X}^\\top\\bm{X})\n\\end{aligned} \n\\end{equation}\nIn similar way, it is easy to show that above equation also holds for $\\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}} - \\bm{X}^\\top\\bm{X} \\succeq 0$, Thus, we have:\n\\begin{equation}\n    \\mathcal{P}(\\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 \\leq \\varepsilon \\operatorname{Tr}(\\bm{X}^\\top\\bm{X})) \\geq  1 - e^{-\\varepsilon^2k/8},\n\\end{equation}\nwhich completes the proof.\n\n"
            },
            "section 10": {
                "name": "Accelerating the Feature Augmentation with a Very Sparse Random Projection",
                "content": "\nTo accelerate the random projection, approach \\cite{li2006very} presented a sparse random projection as an improvement over the Gaussian random projection, in which entries of $\\bm{P}$ are  i.i.d. sampled from:\n\\begin{equation}\n    p_{ij}=\\left\\{\\begin{array}{rl}\n\\sqrt{s} &\\text { with probability } \\frac{1}{2s}, \\\\\n0 &\\text { with probability } 1-\\frac{1}{s}, \\\\\n-\\sqrt{s} &\\text { with probability } \\frac{1}{2s},\n\\end{array}\\right.\n\\label{eq:sij}\n\\end{equation}\nwhere $\\frac{1}{s}$ denotes the density of matirx $\\bm{P}$.\n% \\begin{figure}[h] \n% %   \\begin{minipage}[t]{0.32\\textwidth} \n% %     \\includegraphics[width=\\textwidth]{AAAI2021/picture/ratio.png}\n% %     \\caption{Classification accuracy in different reduction ratios.}\n% %     \\label{fig:density}\n% %   \\end{minipage}% \n% %   \\hspace{0.1cm}\n%  \\begin{minipage}[t]{0.4\\textwidth} \n%     \\includegraphics[width=\\textwidth]{Picture/MatrixDensity.png}\n%     \\caption{Classification accuracy \\wrt different densities.}\n%     \\label{fig:density}\n%   \\end{minipage}% \n% \\end{figure}\n\\label{sec:vsrp}\n% \\noindent\\textbf{Dense vs. Sparse Random Projection}\n\n\\vspace{0.2cm}\n%\\noindent\\textbf{Experiments.}\nThe computation cost of the random projection is related to the sparsity of the sparse random matrix (SRP). The low density of the SRP  reduces the computational cost while it may affect the performance at the same time. To explore the relationship between the density and performance, we vary the density of the random matrix in the range of $[0.001, 0.002, 0.004, 0.008, 0.016, 0.32, 0.064] $. We plot the relationship between the performance and density on three datasets in Figure~\\ref{fig:density}. It is apparent that the performance improves  as the density increases. However, the performance reaches a relatively high peak at a low density ($\\frac{1}{s} < 0.01$). This suggests that one could enjoy the efficiency provided by the SPMM without sacrificing the performance.\n\n"
            },
            "section 11": {
                "name": "Statistics of Datasets",
                "content": "\n\\label{sec:dataset}\n\n\n\n\n\n\n\n%\\newpage\nBelow we describe  datasets from Table \\ref{tab:my_label}:\n\\begin{itemize}\n    \\item\n    \\textbf{Cora, CiteSeer, Pubmeb, DBLP.} These are well-known citation network datasets, in which nodes represent publications and edges indicate their citations. All nodes are labeled according to paper subjects~\\cite{kipf2016semi}.\n    \\item \n    \n    \\textbf{WikiCS.} It is a network of  Wikipedia pages related to the computer science, with edges showing cross-references. Each article is assigned to one of 10 subfields (classes), with characteristics computed using the content's averaged GloVe embeddings. We make no changes to the 20 train/val/test data splits provided by~\\cite{mernyei2020wiki}.\n    \\item \n    \n    \\textbf{Am-Computer, AM-Photo.} Both of these networks are based on Amazon's co-purchase data. Nodes represent products, while edges show how frequently they were purchased together. Each product is described using a Bag-of-Words representation based on the reviews (node features). There are ten node classes (product categories) and eight node classes (product categories), respectively~\\cite{mcauley2015image}.\n    \\item \n    \n    \\textbf{Coauthor-CS, Coauthor-Physics.} These are two networks that were extracted from the Microsoft Academic Graph dataset. The edges reflect a collaboration between two authors, while the nodes represent writers. The keywords that each author uses in their articles are utilized to categorize them (Bag-of-Words representation; node features). According to~\\cite{sinha2015overview}, there are 15 author research fields (node classes) and 5 author research fields (node classes).\n\\end{itemize}\n"
            }
        },
        "tables": {
            "tab:diffaugmentation": "\\begin{table*}[t]\n    \\centering\n    \\caption{Results (Cora, CiteSeer, and WikiCS) given a common testbed for different feature augmentation strategies realized by Eq.~(\\ref{eq:coverr}). The symbol $\\bm{e}_i$ denotes the one-hot vector and $\\mathcal{O}$ is the zero matrix.}\n    \\label{tab:diffaugmentation}\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l|l|ll|ccc}\n\\toprule\n\\toprule\n        \\textbf{Feature Augmentation}  & \\textbf{Type} & $\\bm{P}\\in\\mathbb{R}^{k \\times n}$ & $\\bm{E}\\in \\mathbb{R}^{k \\times d}$ & \\textbf{Cora} & \\textbf{CiteSeer} & \\textbf{WikiCS} \\\\\n        \\midrule\n        Gaussian Noise Injection & Stochastic &$\\bm{P}=\\bm{I}$ &  $\\bm{E}\\sim \\mathcal{N} (0, 1)$ &$0.8271$ &$0.7134$ & $0.7823$\\\\\n% \\midrule\n         SVD & Deterministic &$\\bm{P} = \\bm{U}^\\top, \\bm{X} = \\bm{U} \\bm{\\Sigma} \\bm{V}^\\top$ &$\\bm{E} = \\mathcal{O}$ & $0.8269$ & $0.7142$ & $0.7814$\\\\\n% \\midrule\n         Random Selection & Stochastic&$\\bm{P}$ = $\\{\\bm{e}_{i} | \\mathcal{P} (i)=\\frac{\\|\\bm{X}_{i:}\\|_2}{\\|\\bm{X}\\|_F}\\}$ & $\\bm{E} = \\mathcal{O}$ &$0.8245$ & $0.7121$ & $0.7811$\\\\\n% \\midrule\n\\rowcolor{LightCyan} Random Projection & Stochastic &\n%          $\\bm{P} = \\bm{S}$, $S_{ij}=\\left\\{\\begin{array}{r}\n% \\sqrt{s} \\text { w/ } \\frac{1}{2 s} \\\\\n% 0 \\text { w/ } 1-\\frac{1}{s} \\\\\n% -\\sqrt{s} \\text { w/ } \\frac{1}{2 s}\n% \\end{array}$ \n$\\bm{P}\\sim \\mathcal{N} (0, 1)$\n     & $\\bm{E} = \\mathcal{O}$ &$\\bm{0.8425}$& $\\bm{0.7247}$ &  $\\bm{0.7911}$\\\\\n    \\bottomrule\n    \\bottomrule\n    \\end{tabular}\n    }\n    % \\vspace{-0.4cm}\n\\end{table*}",
            "tab:mainresult1": "\\begin{table*}[t]\n\\centering\n\\caption{Node classification in terms of accuracy (\\%) with standard deviation. The highest performance is highlighted in boldface. COSTA$_{MV}$ and COSTA$_{SV}$ denote the variants of multi-view and single-view setting respectively, OOM indicates Out-Of-Memory.}\n\\label{tab:mainresult1}\n\\resizebox{0.9\\textwidth}{!}{\n\\begin{tabular}{lcccccc}\n% \\toprule\n\\toprule \n\\textbf{Method} & \\textbf{Training Data} & \\textbf{Wiki-CS} & \\textbf{Amazon-Computers} & \\textbf{Amazon-Photo} & \\textbf{Coauthor-CS} & \\textbf{Coauthor-Physics} \\\\\n\\midrule Raw features & $\\bm{X}$ & $71.98 \\pm 0.00$ & $73.81 \\pm 0.00$ & $78.53 \\pm 0.00$ & $90.37 \\pm 0.00$ & $93.58 \\pm 0.00$ \\\\\nNode2vec & $\\bm{A}$ & $71.79 \\pm 0.05$ & $84.39 \\pm 0.08$ & $89.67 \\pm 0.12$ & $85.08 \\pm 0.03$ & $91.19 \\pm 0.04$ \\\\\nDeepWalk & $\\bm{A}$ & $74.35 \\pm 0.06$ & $85.68 \\pm 0.06$ & $89.44 \\pm 0.11$ & $84.61 \\pm 0.22$ & $91.77 \\pm 0.15$ \\\\\nDeepWalk  & $\\bm{X}, \\bm{A}$ & $77.21 \\pm 0.03$ & $86.28 \\pm 0.07$ & $90.05 \\pm 0.08$ & $87.70 \\pm 0.04$ & $94.90 \\pm 0.09$ \\\\\n% \\midrule \nGAE     & $\\bm{X}, \\bm{A}$ & $70.15 \\pm 0.01$ & $85.27 \\pm 0.19$ & $91.62 \\pm 0.13$ & $90.01 \\pm 0.71$ & $94.92 \\pm 0.07$ \\\\\nVGAE    & $\\bm{X}, \\bm{A}$ & $75.63 \\pm 0.19$ & $86.37 \\pm 0.21$ & $92.20 \\pm 0.11$ & $92.11 \\pm 0.09$ & $94.52 \\pm 0.00$ \\\\\nDGI     & $\\bm{X}, \\bm{A}$ & $75.35 \\pm 0.14$ & $83.95 \\pm 0.47$ & $91.61 \\pm 0.22$ & $92.15 \\pm 0.63$ & $94.51 \\pm 0.52$ \\\\\nGMI     & $\\bm{X}, \\bm{A}$ & $74.85 \\pm 0.08$ & $82.21 \\pm 0.31$ & $90.68 \\pm 0.17$ & OOM              & OOM              \\\\\nMVGRL   & $\\bm{X}, \\bm{A}$ & $77.52 \\pm 0.08$ & $87.52 \\pm 0.11$ & $91.74 \\pm 0.07$ & $92.11 \\pm 0.12$ & $95.33 \\pm 0.03$ \\\\\nGRACE   & $\\bm{X}, \\bm{A}$ & $78.31 \\pm 0.05$ & $87.80 \\pm 0.23$ & $92.53 \\pm 0.16$ & $\\mathbf{92.95 \\pm 0.03}$ & $95.72 \\pm 0.03$ \\\\\nGCA     & $\\bm{X}, \\bm{A}$ & $78.23 \\pm 0.04$ & $87.54 \\pm 0.49$ & $92.24 \\pm 0.21$ & $92.95 \\pm 0.13$ & $95.73 \\pm 0.03$ \\\\\nG-BT  & $\\bm{X}, \\bm{A}$ & $76.83 \\pm 0.73$ & $87.93 \\pm 0.36$ & $92.46 \\pm 0.35$ & $92.91 \\pm 0.25$ & $95.25 \\pm 0.13$ \\\\\n\\midrule\n\\rowcolor{LightCyan} COSTA$_{SV}$ & $\\bm{X}, \\bm{A}$ & $79.03 \\pm 0.05$ & $88.26 \\pm 0.03$ & $92.30 \\pm 0.25$ & $\\mathbf{92.95} \\pm 0.12$ & $\\bm{95.74 \\pm 0.02}$ \\\\\n\\rowcolor{LightCyan} COSTA$_{MV}$ & $\\bm{X}, \\bm{A}$ & $\\mathbf{79.12 \\pm 0.02}$ & $\\mathbf{88.32 \\pm 0.03}$ & $\\mathbf{92.56 \\pm 0.45}$ & $92.94 \\pm 0.10$ & $95.60 \\pm 0.02$ \\\\\n\\bottomrule\n% \\bottomrule\n% GCN & $\\bm{X}, \\bm{A}, \\bm{Y}$ & $77.19 \\pm 0.12$ & $86.51 \\pm 0.54$ & $92.42 \\pm 0.22$ & ${93.03 \\pm 0.31}$ & ${95.65 \\pm 0.16}$ \\\\\n% GAT & $\\bm{X}, \\bm{A}, \\bm{Y}$ & ${77.65 \\pm 0.11}$ & ${86.93 \\pm 0.29}$ & ${92.56 \\pm 0.35}$ & $92.31 \\pm 0.24$ & $95.47 \\pm 0.15$ \\\\\n% \\midrule\n\\end{tabular}\n}\n\\end{table*}",
            "tab:my_label": "\\begin{table}[!h]\n    \\centering\n    \\begin{tabular}{lcccc}\n\\toprule\n\\toprule\nDataset & \\#Nodes & \\#Edges & \\#Features & \\#Classes \\\\\n\\midrule Wiki-CS~\\cite{mernyei2020wiki}  & 11,701 & 216,123 & 300 & 10 \\\\\nAmazon-Computers\\cite{mcauley2015image} & 13,752 & 245,861 & 767 & 10 \\\\\nAmazon-Photo\\cite{mcauley2015image}  & 7,650 & 119,081 & 745 & 8 \\\\\nCoauthor-CS~\\cite{sinha2015overview} & 18,333 & 81,894 & 6,805 & 15 \\\\\nCoauthor-Physics~\\cite{sinha2015overview} & 34,493 & 247,962 & 8,415 & 5 \\\\\nCora~\\cite{kipf2016semi} & 2,708 & 5,429 & 1,433 & 7\\\\\nCiteseer~\\cite{kipf2016semi} & 3,327 & 4,732 & 3,703 & 6 \\\\\nPubmed~\\cite{kipf2016semi} & 19,717 & 44,338 & 500 & 3 \\\\\nDBLP~\\cite{kipf2016semi} & 17,716 & 105,734 & 1,639 & 4 \\\\\n\\bottomrule\n\\bottomrule\n\\end{tabular}\n\\caption{Statistics of datasets used in our experiments.}\n\\label{tab:my_label}\n\\end{table}"
        },
        "figures": {
            "fig:issuse": "\\begin{figure}[t]\n    \\centering\n     \\begin{subfigure}[t]{0.4\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Picture/nn_aug_1.pdf}\n         \\caption{FA is unbiased.}\n         \\label{fig:issuseb}\n     \\end{subfigure}\n    %  \\hfill\n     \\begin{subfigure}[t]{0.4\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Picture/gnn_aug_1.pdf}\n         \\caption{GA is biased.}\n         \\label{fig:issusea}\n     \\end{subfigure}\n     \\caption{The distribution of node embeddings on Cora is generated by 500$\\times$ graph augmentations.~\\ref{fig:issuseb} corresponds to the feature augmentation (Gaussian noise injection).~\\ref{fig:issusea} corresponds to the graph augmentation (edge permutation \\& attribute masking). We use 2D embeddings for visualization.\n     }\n     \\label{fig:issuse}\n    %  \\vspace{-0.5cm}\n\\end{figure}",
            "fig:conterexample": "\\begin{figure}[tp]\n    \\centering\n    \\begin{subfigure}[c]{0.6\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Picture/unbias.pdf}\n         \\caption{CL under unbiased augmentation.}\n         \\label{fig:auga}\n     \\end{subfigure}\n    %  \\hfill\n     \\begin{subfigure}[c]{0.6\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Picture/bias.pdf}\n         \\caption{CL under biased augmentation.}\n         \\label{fig:augb}\n     \\end{subfigure}\n    \\caption{({Fig.~\\ref{fig:auga}}) Our strategy results in an unbiased augmentation strategy. (Fig.~\\ref{fig:augb}) A counter-example illustrates the problem of using a biased augmentation strategy in CL.}\n    \\label{fig:conterexample}\n    % \\vspace{-0.5cm}\n\\end{figure}",
            "fig:model": "\\begin{figure}[t]\n    \\centering\n    \\begin{subfigure}[c]{0.7\\textwidth}\n    \\includegraphics[width=\\textwidth]{Picture/MVGCL.pdf}\n    \\caption{\\label{fig:mvgcl} Multi-View GCL.}\n    \\end{subfigure}\n    % \\hspace{0.2cm}\n    \\begin{subfigure}[c]{0.7\\textwidth}\n    \\includegraphics[width=\\textwidth]{Picture/SVGCL.pdf}\n    \\caption{\\label{fig:svgcl} Single-View GCL.}\n    \\end{subfigure}\n    \\label{fig:model}\n    \\caption{The illustrations of MV-GCL (standard) in Fig.~\\ref{fig:mvgcl} and SV-GCL (simplified) in Fig~\\ref{fig:svgcl}. For simplicity, the architecture of MV-GCL is shown using two views only and SV-GCL is the trivial case where two views are the same. MV-GCL contrast two views while SV-GCL perform self-contrast.}\n    %\\vspace{-0.2cm}\n\\end{figure}",
            "fig:biasdistri": "\\begin{figure*}[t]\n    \\centering\n    \\begin{subfigure}[t]{0.27\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Picture/dist_1.pdf}\n         \\caption{NN with AM ($nn\\_a$)}\n         \\label{fig:disa}\n     \\end{subfigure}\n     \\begin{subfigure}[t]{0.23\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Picture/dist_2.pdf}\n         \\caption{GNN with AM ($gnn\\_a$)}\n         \\label{fig:disb}\n     \\end{subfigure}\n     \\begin{subfigure}[t]{0.23\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Picture/dist_3.pdf}\n         \\caption{GNN with EP ($gnn\\_ea$)}\n         \\label{fig:disc}\n     \\end{subfigure}\n     \\begin{subfigure}[t]{0.23\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Picture/dist_4.pdf}\n         \\caption{GNN with EP \\& AM ($gnn\\_ea)$}\n         \\label{fig:disd}\n     \\end{subfigure}\n     \n    % \\includegraphics[width=1\\textwidth]{Picture/dist_4_0.pdf}\n    \\caption{The bias distribution of all nodes on the Cora dataset \\wrt different augmentation strategies and encoder .\n    }\n    \\label{fig:biasdistri}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\bm{H}_1 = f_1 (\\tilde{\\bm{A}}_1, \\tilde{\\bm{X}}_1), \\cdots, \\bm{H}_k = f_k (\\tilde{\\bm{A}}_k, \\tilde{\\bm{X}}_k).\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\begin{aligned}\n    \\mathrm{GCN}_{l} (\\bm{X}, \\bm{A}) &=\\sigma\\left (\\hat{\\bm{D}}^{-\\frac{1}{2}} \\hat{\\bm{A}} \\hat{\\bm{D}}^{-\\frac{1}{2}} \\bm{X} \\bm{W}_l\\right), \\\\\n    f (\\bm{X}, \\bm{A}) &=\\mathrm{GCN}_{2}\\left (\\mathrm{GCN}_{1} (\\bm{X}, \\bm{A}), \\bm{A}\\right),\n    \\end{aligned}\n    \\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:mvcontrastiveloss}\n\\begin{aligned}\n&\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^{n} \\Big[{(\\bm{U}^\\top_{i:}, \\bm{V}_{i:})}/\\tau -\\log \\Big(\n\\sum_{j=1}^{n} e^{\\left (\\bm{U}_{i:}^\\top, \\bm{V}_{j:}\\right) / \\tau}+\\sum_{j=1}^{n} e^{\\left (\\bm{U}^\\top_{i:}, \\bm{U}_{j:}\\right) / \\tau}\\Big) \\Big].\n\\end{aligned}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n% \\begin{aligned}\n      \\text{Bias} (\\mathcal{T} (\\bm{x}_i))=\\Big\\| \\mathbb{E}_{\\tilde{\\bm{x}}_i \\sim \\tilde{\\mathcal{T}}(\\bm{x}_i)} (\\tilde{\\bm{x}}_i) - \\bm{x}_i \\Big\\|_2  \\approx \\Big\\|  \\frac{1}{|\\tilde{\\mathcal{X}}_i|}\\sum_{k=1}^{|\\tilde{\\mathcal{X}}_i|} \\tilde{x}^{(k)}_i\\!  - \\bm{x}_i \\Big\\|_2.\n% \\end{aligned}\n\\label{eq:bias}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\bm{P} = \\bm{U}^\\top, \\bm{X} = \\bm{U} \\bm{\\Sigma} \\bm{V}^\\top.\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\begin{aligned}\n    &\\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top \\tilde{\\bm{X}}\\|_F^2 \\leq \\frac{\\mu^{2}}{k}\\|\\bm{A}\\|_{F}^{4}, \\\\\n    \\Rightarrow &\\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top \\tilde{\\bm{X}}\\|_F \\leq \\frac{\\mu}{\\sqrt{k}}\\|\\bm{A}\\|_{F}^{2}  \\text{   (As $\\|\\bm{X}\\|_2 \\leq \\|\\bm{X}\\|_F$)},\\\\\n    \\Rightarrow & \\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top \\tilde{\\bm{X}}\\|_2 \\leq \\frac{\\mu}{\\sqrt{k}}\\operatorname{Tr}(\\bm{X}^\\top\\bm{X}),\n    \\end{aligned}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mathcal{P}(\\|\\bm{X}^\\top \\bm{X} - \\tilde{\\bm{X}}^\\top \\tilde{\\bm{X}}\\|_F \\leq \\varepsilon \\operatorname{Tr}(\\bm{X}^\\top\\bm{X})) \\geq 1 - e^{\\left( -\\frac{(\\varepsilon \\sqrt{k}-1)^2}{8}\\right)},\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\label{eq:normpre}\n    \\mathcal{P}\\left ( (1-\\varepsilon)\\|\\bm{x}\\|_2^{2} \\leq\\left\\|\\frac{1}{\\sqrt{k}} \\bm{P} \\bm{x}\\right\\|^{2} \\leq (1+\\varepsilon)\\|\\bm{x}\\|_2^{2}\\right) \\geq 1-e^{ \\left(-\\frac{k\\varepsilon^2}{8}\\right)}.\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\begin{aligned}\n          & \\mathbb{E}\\left[\\frac{1}{\\sqrt{k}} \\bm{P}\\bm{x}\\right] = \\mathbb{E}\\left[\\sum_i^k \\frac{1}{k}\\sum_{j}(P_{ij} x_j)^2\\right] = \\mathbb{E}\\left[\\sum_i^k\\frac{1}{k}\\sum_{j,j'}(P_{ij} P_{ij'} x_j x_{j'})\\right]\\\\\n          & = \\sum_i^k\\frac{1}{k}\\mathbb{E}\\left[\\sum_j \\bm{P}^2_{jj}\\bm{x}^2_j\\right] =\\sum_i^k\\frac{1}{k} \\sum_j x^2_j\\\\\n          & = \\|\\bm{x}\\|^2_2.\n\\end{aligned}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\begin{aligned}\n& \\mathcal{P}(\\| \\frac{1}{\\sqrt{k}}\\|\\bm{P}\\bm{x}\\|_2^2 > (1+\\varepsilon)\\|\\bm{x}\\|_2)  = \\mathcal{P}(\\sum_{j=1}^k X^2_j  > (1+\\varepsilon)k),\\\\\n&=\\mathcal{P}\\left(e^{t \\sum_{i=1}^{k} X_{j}^{2}}>e^{t(1+\\varepsilon) k}\\right)\\\\\n& \\leq \\frac{\\mathbb{E}\\left[e^{t \\sum_{j=1}^k X_{j}^{2}}\\right]}{e^{t(1+\\varepsilon) k}} \\quad\\text{  (apply Markovs inequality)}\\\\ \n&= \\frac{\\prod_{j=1}^{k} \\mathbb{E}\\left[e^{t X_{j}^{2}}\\right]}{e^{t(1+\\varepsilon) k}} \\quad\\text{      (as $X_j$ is i.i.d)}\\\\ \n&= \\frac{(\\mathbb{E}\\left[e^{t X_{j}^{2}}\\right])^k}{e^{t(1+\\varepsilon) k}}  \\text{      (for $1<t<1/2$, it holds that $\\mathbb{E}\\left[e^{t X_{i}^{2}}\\right] \\leq\\left(\\frac{1}{\\sqrt{1-2 t}}\\right)$)}\\\\\n& \\leq \\left(\\frac{1}{\\sqrt{1-2 t}}\\right)^{k}\\left(\\frac{1}{e^{t(1+\\varepsilon)}}\\right)^{k}\\\\\n& =\\left(e^{-\\varepsilon+\\ln (1+\\varepsilon)}\\right)^{k / 2} \\quad\\text{ (using $\\ln (1+\\varepsilon) \\leq \\varepsilon-\\frac{\\varepsilon^2}{4}$)}\\\\\n& \\leq e^{-\\varepsilon^2k/8}.\n\\end{aligned}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    \\mathcal{P}(\\|\\ \\frac{1}{\\sqrt{k}}\\|\\bm{P}\\bm{x}\\|_2^2 \\leq (1+\\varepsilon)\\|\\bm{x}\\|^2_2)) \\leq 1 - e^{-\\varepsilon^2k/8}.\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    \\mathcal{P}(\\|\\ \\frac{1}{\\sqrt{k}}\\|\\bm{P}\\bm{x}\\|_2^2 \\geq (1-\\varepsilon)\\|\\bm{x}\\|^2_2)) \\leq 1 - e^{-\\varepsilon^2k/8}.\n\\end{equation}",
            "eq:13": "\\begin{equation}\n\\mathcal{P}((1-\\varepsilon)\\|\\bm{x}\\|^2_2 \\leq \\frac{1}{\\sqrt{k}}\\|\\bm{P}\\bm{x}\\|_2^2 \\leq (1+\\varepsilon)\\|\\bm{x}\\|^2_2) \\geq 1 - e^{-\\varepsilon^2k/8}.\n\\end{equation}",
            "eq:14": "\\begin{align}\n    \\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 &= \\bm{x}^\\top (\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}) \\bm{x} = \\sigma_{\\max} \\\\\n    & = \\bm{x}^T \\bm{X}^T\\bm{X} \\bm{x} - \\bm{x}^T \\tilde{\\bm{X}}^T\\tilde{\\bm{X}} \\bm{x} \\\\\n    & = \\|\\bm{Xx}\\|_2^2 - \\|\\tilde{\\bm{X}}\\bm{x}\\|_2^2 \\\\\n    & = \\|\\bm{Xx}\\|_2^2 - \\|\\bm{PXx}\\|_2^2 .\n\\end{align}",
            "eq:15": "\\begin{equation}\n   \\begin{aligned}\n    \\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 & \\leq \\|\\bm{Xx}\\|_2^2 + (\\varepsilon-1) \\|\\bm{Xx}\\|_2^2\\\\\n   & = \\varepsilon \\|\\bm{Xx}\\|^2_2 \\\\\n   &\\leq \\varepsilon \\|\\bm{X}\\|^2_2\\|\\bm{x}\\|^2_2 \\text{  $(\\|\\bm{x}\\|^2_2 = 1)$}\\\\ \n%   &\\leq \\varepsilon \\|\\bm{X}\\|^2_F\\\\\n   & \\leq \\varepsilon \\operatorname{Tr}(\\bm{X}^\\top\\bm{X})\n\\end{aligned} \n\\end{equation}",
            "eq:16": "\\begin{equation}\n    \\mathcal{P}(\\|\\bm{X}^\\top\\bm{X} - \\tilde{\\bm{X}}^\\top\\tilde{\\bm{X}}\\|_2 \\leq \\varepsilon \\operatorname{Tr}(\\bm{X}^\\top\\bm{X})) \\geq  1 - e^{-\\varepsilon^2k/8},\n\\end{equation}",
            "eq:17": "\\begin{equation}\n    p_{ij}=\\left\\{\\begin{array}{rl}\n\\sqrt{s} &\\text { with probability } \\frac{1}{2s}, \\\\\n0 &\\text { with probability } 1-\\frac{1}{s}, \\\\\n-\\sqrt{s} &\\text { with probability } \\frac{1}{2s},\n\\end{array}\\right.\n\\label{eq:sij}\n\\end{equation}"
        }
    }
}