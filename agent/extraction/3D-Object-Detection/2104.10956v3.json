{
    "meta_info": {
        "title": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection",
        "abstract": "Monocular 3D object detection is an important task for autonomous driving\nconsidering its advantage of low cost. It is much more challenging than\nconventional 2D cases due to its inherent ill-posed property, which is mainly\nreflected in the lack of depth information. Recent progress on 2D detection\noffers opportunities to better solving this problem. However, it is non-trivial\nto make a general adapted 2D detector work in this 3D task. In this paper, we\nstudy this problem with a practice built on a fully convolutional single-stage\ndetector and propose a general framework FCOS3D. Specifically, we first\ntransform the commonly defined 7-DoF 3D targets to the image domain and\ndecouple them as 2D and 3D attributes. Then the objects are distributed to\ndifferent feature levels with consideration of their 2D scales and assigned\nonly according to the projected 3D-center for the training procedure.\nFurthermore, the center-ness is redefined with a 2D Gaussian distribution based\non the 3D-center to fit the 3D target formulation. All of these make this\nframework simple yet effective, getting rid of any 2D detection or 2D-3D\ncorrespondence priors. Our solution achieves 1st place out of all the\nvision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020.\nCode and models are released at https://github.com/open-mmlab/mmdetection3d.",
        "author": "Tai Wang, Xinge Zhu, Jiangmiao Pang, Dahua Lin",
        "link": "http://arxiv.org/abs/2104.10956v3",
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "additionl_info": "Camera-ready version of 3DODI workshop at ICCV 2021; Technical report  for the best vision-only method (1st place of the camera track) in the  nuScenes 3D detection challenge of NeurIPS 2020"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n% 2D detection\nObject detection is a fundamental problem in computer vision. \nIt aims to identify objects of interest in the image and predict their categories with corresponding 2D bounding boxes. \nWith the rapid progress of deep learning, 2D object detection has been well explored in recent years.\nVarious models such as Faster R-CNN~\\cite{FasterRCNN}, RetinaNet~\\cite{RetinaNet}, and FCOS~\\cite{FCOS} significantly promote the progress of the field and benefit various applications like autonomous driving.\n\n% 3D detection, LiDAR vs. mono3d\nHowever, 2D information is not enough for an intelligent agent to perceive the 3D real world. \nFor example, when an autonomous vehicle needs to run smoothly and safely on the road, it must have accurate 3D information of objects around it to make secure decisions. Therefore, 3D object detection is becoming increasingly important in these robotic applications. \nMost state-of-the-art methods~\\cite{VoxelNet,PointPillars,PointRCNN,reconfig_voxels,ssn,cylinder3d} rely on the accurate 3D information provided by LiDAR point clouds, but it is a heavy burden to install expensive LiDARs on each vehicle.\nSo monocular 3D object detection, as a simple and cheap setting for deployment, becomes a much meaningful research problem nowadays.\n\n% Problems of monocular 3D detection (the main difference between mono3D and 2D)\nConsidering monocular 2D and 3D object detection have the same input but different outputs, a straightforward solution for monocular 3D object detection is following the practices in the 2D domain but adding extra components to predict the additional 3D attributes of the objects.\nSome previous work~\\cite{MonoDIS,ROI10D} keeps predicting 2D boxes and further regresses 3D attributes on top of 2D centers and regions of interest. Others~\\cite{M3D-RPN,D4LCN,Kinematic3D} simultaneously predict 2D and 3D boxes with 3D priors corresponding to each 2D anchor. Another stream of methods based on redundant 3D information~\\cite{SS3D,RTM3D} predicts extra keypoints for optimized results ultimately.\nIn a word, the fundamental underlying problem is how to assign 3D targets to the 2D domain with the 2D-3D correspondence and predict them afterward.\n\nIn this paper, we adopt a simple yet efficient method to enable a 2D detector to predict 3D localization.\nWe first project the commonly defined 7-DoF 3D locations onto the 2D image and get the projected center point, which we name as 3D-center compared to the previous 2D-center.\nWith this projection, the 3D-center contains 2.5D information, \\emph{i.e.}, 2D location and its corresponding depth. The 2D location can be further reduced to the 2D offset from a certain point on the image, which serves as the only 2D attribute that can be normalized among different feature levels like in the 2D detection. In comparison, depth, 3D size, and orientation are regarded as 3D attributes after decoupling. In this way, we transform the 3D targets with a center-based paradigm and avoid any necessary 2D detection or 2D-3D correspondence priors.\n\nAs a practical implementation, we build our method on FCOS~\\cite{FCOS}, a simple anchor-free fully convolutional single-stage detector.\nWe first distribute the objects to different feature levels with consideration of their 2D scales.\nThen the regression targets of each training sample are assigned only according to the projected 3D centers.\nIn contrast to FCOS that denotes the center-ness with distances to boundaries, we represent the 3D center-ness with a 2D Gaussian distribution based on the 3D-center.\n\nWe evaluate our method on a popular large-scale dataset, nuScenes~\\cite{nuScenes}, and achieved \\emph{1st place} on the camera track of this benchmark without any prior information. Moreover, we only need 2x less computing resources to train a baseline model with performance comparable to the previous best open-source method, CenterNet~\\cite{CenterNet}, in one day, also 3x faster than it. Both show that our framework is simple and efficient. Detailed ablation studies show the importance of each component.\n\n\n\\label{sec:introduction}\n\n%% Background\n% !TEX root = ./arxiv.tex\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n\\noindent\\textbf{2D Object Detection}\\quad\nResearch on 2D object detection has made great progress with the breakthrough of deep learning approaches. According to the base of initial guesses, modern methods can be divided into two branches: anchor-based and anchor-free. Anchor-based methods~\\cite{FastRCNN,FasterRCNN,SSD,YOLOv2} benefit from the predefined anchors in terms of much easier regression while having many hyper-parameters to tune. In contrast, anchor-free methods~\\cite{DenseBox,YOLOv1,FCOS,CornerNet,CenterNet} do not need these prior settings and are thus neater with better universality. For simplicity, this paper takes FCOS, a representative anchor-free detector, as the baseline considering its capability of handling overlapped ground truths and scale variance problem.\n\nFrom another perspective, monocular 3D detection is a more difficult task closely related to 2D detection. But there is few work investigating the connection and difference between them, which makes them isolated and not able to benefit from the advancement of each other. This paper aims to adapt FCOS as the example and further build a closer connection between these two tasks.\n\n\\noindent\\textbf{Monocular 3D Object Detection}\\quad\nMonocular 3D detection is more complex than conventional 2D detection. The underlying key problem is the inconsistency of input 2D data modal and the output 3D predictions.\n\n\\noindent\\emph{Methods involving sub-networks}\\quad The first batch of works resorts to sub-networks to assist 3D detection. To mention only a few, 3DOP~\\cite{3DOP} and MLFusion~\\cite{MLFusion} use a depth estimation network, while Deep3DBox~\\cite{Deep3DBox} uses a 2D object detector. They heavily rely on the performance of sub-networks, even external data and pre-trained models, making the entire system complex and inconvenient to train.\n% Earlier work uses sub-networks to assist 3D detection. 3DOP~\\cite{3DOP} and MLFusion~\\cite{MLFusion} use a depth estimation network, while Deep3DBox~\\cite{Deep3DBox} uses a 2D object detector. They rely on the design and performance of sub-networks, even external data and pre-trained models, which makes the training inconvenient and introduces additional system complexity.\n\n\\noindent\\emph{Transform to 3D representations}\\quad Another category of methods converts the input RGB image to other 3D representations, such as voxels~\\cite{OFTNet} and point clouds~\\cite{PseudoLiDAR}. Recent work~\\cite{PseudoLiDAR++,End2EndPL,Foresee,CaDDN} has made great progress following this approach and shown promising performance. However, they still rely on dense depth labels and thus are not regarded as pure monocular approaches. There are also domain gaps between different depth sensors and LiDARs, making them hard to generalize to new practice settings smoothly. In addition, it is difficult to process a large number of point clouds when applying these methods to the real-world scenarios.\n% Another category is to convert the RGB input to other representations like OFTNet~\\cite{OFTNet} and Pseudo-Lidar~\\cite{PseudoLiDAR}. Although these methods have shown promising performance, they still rely on dense depth labels and thus are not regarded as pure monocular approaches.\n% There are also domain gaps between different depth sensors and LiDARs, making them hard to generalize to a new practice setting smoothly. Furthermore, the efficiency of processing a large number of point clouds is also a significant issue to deal with when applying these methods to cases in the real world.\n\n\\noindent\\emph{End-to-end design like 2D detection}\\quad Recent work notices these drawbacks and begins to design end-to-end frameworks like 2D detectors. For example, M3D-RPN~\\cite{M3D-RPN} proposes a single-stage detector with an end-to-end region proposal network and depth-aware convolution. SS3D~\\cite{SS3D} detects 2D key points and further predicts object characteristics with uncertainties. MonoDIS~\\cite{MonoDIS} improves the multi-task learning with a disentangling loss. These methods follow the anchor-based manners and are thus required to define consistent 2D and 3D anchors. Some of them also need multiple training stages or hand-crafted post-optimization phases. In contrast, anchor-free methods~\\cite{CenterNet,RTM3D,MonoPair} do not need to make statistics on the given data. It is more convenient to generalize their simple designs to more complex cases with more various classes or different intrinsic settings. Hence, we choose to follow this paradigm.\n\n% Recent work notices these drawbacks, and end-to-end frameworks are thus proposed. M3D-RPN~\\cite{M3D-RPN} implements a single-stage multi-class detector with an end-to-end region proposal network and depth-aware convolution. SS3D~\\cite{SS3D} proposes to detect 2D key points and further predicts object characteristics with uncertainties. MonoDIS~\\cite{MonoDIS} introduces a disentangling loss to reduce the instability of the training procedure. Some of them still have multiple training stages or hand-crafted post-optimization phases. These methods follow anchor-based manners; thus, the consistency of 2D and 3D anchors needed to be determined. In contrast, anchor-free methods~\\cite{CenterNet,RTM3D,MonoPair} do not need to make statistics on the given data and can be better generalized to more complicated cases with more various classes or different intrinsic settings, so we choose to follow this paradigm.\n\nNevertheless, these works hardly study the key difficulty when applying a general 2D detector to monocular 3D detection. What should be kept or adjusted therein is seldom discussed when proposing their new frameworks. In contrast, this paper concentrates on this point, which could provide a reference when applying a typical 2D detector framework to a closely related task. On this basis, a more in-depth understanding of the connection and difference between these two tasks will also benefit further research of both communities.\n%---------------original---------------------replace with ICCV version-----------------\n%\\noindent\\textbf{Anchor-Free Detectors}\\quad\n%The idea of anchor-free detection being used in the deep learning era can be traced back to DenseBox~\\cite{DenseBox}, which is a framework designed for face detection. This family of detectors like UnitBox~\\cite{UnitBox} typically utilize only single-scale feature map for per-pixel-wise detection, which leads to low recall and lack of capability to handle the overlapped ground truths. The same problem also existed in another popular anchor-free framework, YOLOv1~\\cite{YOLOv1}. Compared to DenseBox, YOLO is created for generic object detection with grid-cell based detection. However, due to the low recall problem, YOLOv2~\\cite{YOLOv2} exploited anchor boxes as well.\\\\\n%Afterwards, the proposal of FPN~\\cite{FPN} provides a possibility for anchor-free frameworks to resolve the low recall problem. FCOS~\\cite{FCOS} is a representative one which benefits from this design. It follows pixel-wise prediction for detection to make a better use of all the sampled points so that a higher recall can be achieved. Meanwhile, the concept of center-ness helps suppress those points far away the centers of objects, which usually leads to low-quality predictions. Furthermore, multi-level predictions with FPN could better handle objects with different scales. It also further alleviates the ambiguity when a single point is inside of multiple boxes. With all these technical designs, it achieved significant performance gains for anchor-free object detection in 2D cases. However, there is still not work exploring how to apply these designs to monocular 3D object detection, which is similarly formulated and closely related to 2D detection. What we do in this paper is to adjust several details of the overall approach according to the characteristic of data and this specific task.\\\\\n%Finally, there is also another stream of anchor-free pipelines based on key points detection, such as CornerNet~\\cite{CornerNet} and CenterNet~\\cite{CenterNet}. For CornerNet, the main problem is that it needs complicated post-processing, like matching, after the preliminary corner key points detection. In comparison, CenterNet gets rid of it but does not have great capability of handling the ambiguity when centers of two objects are overlapped even though it is a minor case. The lack of mechanism designed for multi-scale detection is also a weakness for these two frameworks. Therefore, we choose to follow FCOS for monocular object detection in this work.\n\n%\\noindent\\textbf{LiDAR-Based 3D Object Detection}\\quad\n%LiDAR is becoming a more and more important sensor for autonomous driving systems. It could provide accurate 3D information for the system to sense the environment, which is superior to conventional visual sensors like RGB cameras. Therefore, recent years have witnessed the rapid progress of the LiDAR-based perception algorithms. The methods using LiDAR data can be divided into two categories according to the data type used: those using multi-modal data and those using only LiDAR data. \\\\\n%For the first stream, the main problem lies in how to utilize these data. Most of the earliest methods, like MV3D~\\cite{MV3D} and AVOD~\\cite{AVOD}, resolve this problem by fusing features extracted from frontal-view RGB images and those extracted from projections of point clouds, which simplifies the problem from 3D to 2D case. Then as a representative backbone tailored to point set, namely PointNet~\\cite{PointNet}, is proposed, processing point cloud directly becomes possible. For example, Frustum PointNet~\\cite{F-PointNet} reduces the searching space via customized frustum proposals generated from 2D detections, and detects objects with features extracted from points inside of them. Afterwards, LiDAR-only methods dominate the benchmarks of 3D object detection due to its convenience of exploring 3D geometric patterns directly and data augmentation. To mention only a few of them here, point-based methods~\\cite{PointRCNN,STD} process the points individually and generate proposals on top of them while voxel-based methods~\\cite{VoxelNet,SECOND,PointPillars} group points at first and detect objects based on the quantized representation more efficiently. \\\\\n%However, although LiDAR-based 3D object detectors have achieved excellent performance, the high cost of LiDAR is always a problem for its practical use. Besides, there is still an urgent demand to guarantee the safety of self-driving systems from detectors based on other sensors. In this paper, we focus on how to design an efficient 3D detector using the conventional monocular RGB cameras.\n\n%\\noindent\\textbf{Monocular 3D Object Detection}\\quad\n%Monocular 3D object detection is an especially challenging task due to its inherent ill-posed property. The key problem lying in it is the lack of depth information. Next according to the similarity of methodologies, we present related work as follows.\\\\\n%The first batch of earlier methods utilize sub-networks to assist 3D detection. To mention a few, Chen et.al.~\\cite{3DOP} and Xu et.al.~\\cite{MLFusion} make use of a depth estimation network to aid the 3D detection procedure. Deep3DBox~\\cite{Deep3DBox} uses an external SOTA 2D object detector to generate 2D proposals and estimates 3D objects thereon. All of them rely on the design and performance of these sub-networks and some of them even rely on external data and pretrained models, which brings heavy burden to training a clean monocular 3D object detector.\\\\\n%Another sub-category is to convert the RGB input to other representations, in which OFTNet~\\cite{OFTNet} and Pseudo-Lidar~\\cite{PseudoLiDAR} are representative ones. OFTNet aims to convert the RGB input to 3D voxels with so-called Orthographic Feature Transform. This transformation further makes the problem could be reformulated as the detection on the bird view map. In contrast, Pseudo-Lidar converts the image into 3D point clouds with an off-the-shelf depth estimation network and further detects objects with LiDAR-based detection methods. Although this category of methods has shown promising results in terms of performance, it is difficult to use it in practice if depth ground truth or depth estimator is not available. There is also a domain gap between different depth sensors and LiDARs, which makes it hard to generalize to a new setting smoothly. Furthermore, the efficiency of processing a large amount of point clouds is also a significant issue to deal with when applying these methods to surrounding views, which is a common setting in practical use.\\\\\n%Recent work notices these drawbacks and several end-to-end frameworks are thus proposed. M3D-RPN~\\cite{M3D-RPN} is one of the earliest and popular frameworks to make the training procedure more elegant. It implements a single-stage multi-class detector with an end-to-end region proposal network and depth-aware convolutions. However, it still has two procedures when training including a warm-up step. SS3D~\\cite{SS3D} proposes to detect 2D object key points and further predicts object characteristics with the corresponding uncertainties. Both of them benefit from a post-optimization phase, which is also not elegant in practical use. For two-stage methods, MonoDIS~\\cite{MonoDIS} introduces a novel disentangling loss to reduce the instability of training procedure. These methods follow anchor-based manners, and the consistency of 2D and 3D anchors and detections need to be determined. In contrast, anchor-free methods do not need to make statistics on the given data and have a better capability and generalization to more complicated cases with more various classes. To our best of knowledge, only CenterNet~\\cite{CenterNet} uses this anchor-free manner for monocular object detection. Nevertheless, it is difficult to be trained possibly because the overall design is not efficient in fact.~\\footnote{A model needs to be trained with 8 V100 GPUs for 72 hours.} In this paper, we propose a more efficient framework based on FCOS, which could achieve end-to-end training and converge to a state-of-the-art model with only 24 hours, about 3$\\times$ faster and 2$\\times$ less computational resources.\n\n\\label{sec:related}\n% !TEX root = ./arxiv.tex\n\n\n"
            },
            "section 3": {
                "name": "Approach",
                "content": "\n\\label{sec:approach}\nObject detection is one of the most fundamental and challenging problems for scene understanding. The goal of conventional 2D object detection is to predict 2D bounding boxes and category labels for each object of interest. In comparison, monocular 3D detection needs us to predict 3D bounding boxes instead, which need to be decoupled and transformed to the 2D image plane. This section will first present an overview of our framework with our adopted reformulation of 3D targets, and then elaborate on two corresponding technical designs, 2D guided multi-level 3D prediction and 3D center-ness with 2D Gaussian distribution, tailored to this task. These technical designs work together to equip the 2D detector FCOS with the capability of detecting 3D objects.\n\n",
                "subsection 3.1": {
                    "name": "Framework Overview",
                    "content": "\nA fully convolutional one-stage detector typically consists of three components: a backbone for feature extraction, necks for multi-level branches construction and detection heads for dense predictions. Then we briefly introduce each of them.\n\n\\noindent\\textbf{Backbone}\\quad We use the pretrained ResNet101~\\cite{ResNet,ImageNet} with deformable convolutions~\\cite{DeformConv} for feature extraction. It achieves a good trade-off between accuracy and efficiency in our experiments. We fixed the parameters of the first convolutional block to avoid more memory overhead.\n\n\\noindent\\textbf{Neck}\\quad The second module is the Feature Pyramid Network~\\cite{FPN}, a primary component for detecting objects at different scales. For precise clarification, we denote feature maps from level 3 to 7 as P3 to P7, as shown in Fig.~\\ref{fig: overview}. We follow the original FCOS to obtain P3 to P5 and downsample P5 with two convolutional blocks to obtain P6 and P7. All of these five feature maps are responsible for predictions of different scales afterward.\n\n\\noindent\\textbf{Detection Head}\\quad Finally, for shared detection heads, we need to deal with two critical issues. The first is how to distribute targets to different feature levels and different points. It is one of the core problems for different detectors and will be presented in Sec.~\\ref{sec: target_assign}. The second is how to design the architecture. We follow the conventional design of RetinaNet~\\cite{RetinaNet} and FCOS~\\cite{FCOS}. Each shared head consists of 4 shared convolutional blocks and small heads for different targets. It is empirically more effective to build extra disentangled heads for \\emph{regression} targets with different measurements, so we set one small head for each of them (Fig.~\\ref{fig: overview}).\n\nSo far, we have introduced the overall design of our network architecture. Next, we will formulate this problem more formally and present the detailed training and inference procedure.\n\n\\noindent\\textbf{Regression Targets}\\quad To begin with, we first recall the formulation of anchor-free manners for object detection in FCOS. Given a feature map at layer $i$ of the backbone, denoted as $F_i\\in\\mathbb{R}^{H\\times W\\times C}$, we need to predict objects based on each point on this feature map, which corresponds to uniformly distributed points on the original input image. Formally, for each location $(x, y)$ on the feature map $F_i$, suppose the total stride until layer $i$ is $s$, then the corresponding location on the original image should be $(sx+\\lfloor\\frac{s}{2}\\rfloor, sy+\\lfloor\\frac{s}{2}\\rfloor)$. Unlike anchor-based detectors regressing targets by taking predefined anchors as a reference, we directly predict objects based on these locations. Moreover, because we do not rely on anchors, the criterion for judging whether a point is from the foreground or not will no longer be the IoU (Intersection over Union) between anchors and ground truths. Instead, as long as the point is near the box center enough, it could be a foreground point.\n\n\nIn the 2D case, the model needs to regress the distance of the point to the top/bottom/left/right side, denoted as $t, b, l, r$ in Fig.~\\ref{fig:teaser}. However, in the 3D case, it is non-trivial to regress the distance to six faces of the 3D bounding box. Instead, a more straightforward implementation is to convert the commonly defined 7-DoF regression targets to the 2.5D center and 3D size. The 2.5D center can be easily transformed back to 3D space with a camera intrinsic matrix. Regressing the 2.5D center could be further reduced to regressing the offset from the center to a specific foreground point, $\\Delta x, \\Delta y$, and its corresponding depth $d$ respectively. In addition, to predict the allocentric orientation of the object, we divide it into two parts: angle $\\theta$ with period $\\pi$ and 2-bin direction classification. The first component naturally models the IOU of our predictions with the ground truth boxes, while the second component focuses on the adversarial case where two boxes have opposite orientations. Benefiting from this angle encoding, our method surpasses another center-based framework, CenterNet, in terms of orientation accuracy, which will be compared in the experiments. The rotation encoding scheme is illustrated in Fig.~\\ref{fig: rot_encoding}.\n\nIn addition to these regression targets related to the location and orientation of objects, we also regress a binary target center-ness $c$ like FCOS. It serves as a soft binary classifier to determine which points are closer to centers, and helps suppress those low-quality predictions far away from object centers. More details are presented in Sec.~\\ref{sec:centerness}.\n\nTo sum up, the regression branch needs to predict $\\Delta x, \\Delta y, d, w, l, h, \\theta, v_x, v_y$, direction class $C_\\theta$ and center-ness $c$ while the classification branch needs to output the class label of the object and its attribute label (Fig.~\\ref{fig: overview}).\n\n\\noindent\\textbf{Loss}\\quad For classification and different regression targets, we define their loss respectively and take their weighted summation as the total loss. Firstly, for classification branch, we use the commonly used focal loss~\\cite{RetinaNet} for object classification loss:\n\\vspace{-1ex}\n\\begin{equation}\n    \\centering\n    L_{cls} = -\\alpha(1-p)^\\gamma logp\n    \\vspace{-1ex}\n\\end{equation}\nwhere $p$ is the class probability of a predicted box. We follow the settings, $\\alpha = 0.25$ and $\\gamma = 2$, of the original paper. For attribute classification, we use a simple softmax classification loss, denoted as $L_{attr}$.\\\\\nFor regression branch, we use smooth L1 loss for each regression targets except center-ness with corresponding weights considering their scales:\n\\vspace{-1ex}\n\\begin{equation}\n    \\label{eqn: loc_loss}\n    \\centering\n    L_{loc} = \\sum_{b\\in (\\Delta x, \\Delta y, d, w, l, h, \\theta, v_x, v_y)} SmoothL1(\\Delta b)\n    \\vspace{-1ex}\n\\end{equation}\nwhere the weight of $\\Delta x, \\Delta y, w, l, h, \\theta$ error is 1, the weight of $d$ is 0.2 and the weight of $v_x, v_y$ is 0.05. Note that although we employ $exp(x)$ for depth prediction, we still compute the loss in the original depth space instead of the log space. It empirically results in more accurate depth estimation ultimately. We use the softmax classification loss and binary cross entropy (BCE) loss for direction classification and center-ness regression, denoted as $L_{dir}$ and $L_{ct}$ respectively. Finally, the total loss is:\n\\begin{equation}\n    \\centering\\footnotesize\n    L = \\frac{1}{N_{pos}}(\\beta_{cls}L_{cls}+\\beta_{attr}L_{attr}+\\beta_{loc}L_{loc}+\\beta_{dir}L_{dir}+\\beta_{ct}L_{ct})\n\\end{equation}\nwhere $N_{pos}$ is the number of positive predictions and $\\beta_{cls} = \\beta_{attr} = \\beta_{loc} = \\beta_{dir} = \\beta_{ct} = 1$.\n\n\\noindent\\textbf{Inference}\\quad During inference, given an input image, we forward it through the framework and obtain bounding boxes with their class scores, attribute scores, and center-ness predictions. We multiply the class score and center-ness as the confidence for each prediction and conduct rotated Non-Maximum Suppression (NMS) in the bird view as most 3D detectors to get the final results.\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "2D Guided Multi-Level 3D Prediction",
                    "content": "\\label{sec: target_assign}\nAs mentioned previously, to train a detector with pyramid networks, we need to devise a strategy to distribute targets to different feature levels. FCOS~\\cite{FCOS} has discussed two crucial issues therein: 1) How to enable anchor-free detectors to achieve similar Best Possible Recall (BPR) compared to anchor-based methods, 2) Intractable ambiguity problem caused by overlaps of ground-truth boxes. The comparison in the original paper has well addressed the first problem. It shows that multi-level prediction through FPN can improve BPR and even achieve better results than anchor-based methods. Similarly, the conclusion of this problem is also applicable in our adapted framework. The second question will involve the specific setting of the regression target, which we will discuss next.\n\nThe original FCOS detects objects of different sizes in different levels of feature maps. Different from anchor-based methods, instead of assigning anchors with different sizes, it directly assigns ground-truth boxes with different sizes to different levels of feature maps. Formally, it first computes the 2D regression targets, $l^*$, $r^*$, $t^*$, $b^*$  for each location at each feature level. Then locations satisfying $max(l^*, r^*, t^*, b^*)>m_i$ or $max(l^*, r^*, t^*, b^*)<m_{i-1}$ would be regarded as a negative sample, where $m_i$ denotes the maximum regression range for feature level $i$~\\footnote{We set the regression range as (0, 48, 96, 192, 384, $\\infty$) for $m_2$ to $m_7$ in our experiments respectively.}. In comparison, we also follow this criterion in our implementation, considering that the scale of 2D detection is directly consistent with how large a region we need to focus on. However, we only use 2D detection for filtering meaningless targets in this assignment step. After completing the target assignment, our regression targets only include 3D-related ones. Here we generate the 2D bounding boxes by computing the exterior rectangle of projected 3D bounding boxes, so we do not need any 2D detection annotations or priors.\n\nNext, we will discuss how to deal with the ambiguity problem. Specifically, when a point is inside multiple ground truth boxes in the same feature level, which box should be assigned to it? The usual way is to select according to the area of the 2D bounding box. The box with a smaller area is selected as the target box for this point. We call this scheme the \\emph{area-based} criterion. This scheme has an obvious drawback: Large objects will be paid less attention by such processing, which is also verified by our experiments (Fig.~\\ref{fig: target_assign}). Taking this into account, we instead propose a \\emph{distance-based} criterion, \\emph{i.e.}, select the box with closer center as the regression target. This scheme is consistent with the adapted center-based mechanism for defining regression targets. Furthermore, it is also reasonable because the points closer to the object's center can obtain more comprehensive and balanced local region features, thus easily producing higher-quality predictions.\nThrough simple verification (Fig.~\\ref{fig: target_assign}), we find that this scheme significantly improves the best possible recall (BPR) and mAP of large objects and also improves the overall mAP (about 1\\%), which will be presented in the ablation study.\n\nIn addition to the center-based approach to deal with ambiguity, we also use the 3D-center to determine foreground points, \\emph{i.e.}, only the points near the center enough will be regarded as positive samples. We define a hyper-parameter, radius, to measure this central portion. The points with a distance smaller than radius$\\times$stride to the object center would be considered positive, where the radius is set to 1.5 in our experiments.\n\nFinally, we replace each output $x$ of different regression branches with $s_ix$ to distinguish shared heads for different feature levels. Here $s_i$ is a trainable scalar used to adjust the exponential function base for feature level $i$. It brings a minor improvement in terms of detection performance.\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "3D Center-ness with 2D Gaussian Distribution",
                    "content": "\\label{sec:centerness}\nIn the original design of FCOS, center-ness $c$ is defined by 2D regression targets, l*, r*, t*, b*:\n\\vspace{-1ex}\n\\begin{equation}\n    \\centering\n    c = \\sqrt{\\frac{min(l^*, r^*)}{max(l^*, r^*)}\\times \\frac{min(t^*, b^*)}{max(t^*, b^*)}}\n    \\vspace{-1ex}\n\\end{equation}\nBecause our regression targets are changed to the 3D center-based paradigm, we define the center-ness by 2D Gaussian distribution with the projected 3D-center as the origin. The 2D Gaussian distribution is simplified as:\n\\vspace{-1ex}\n\\begin{equation}\n    \\centering\n    c = e^{-\\alpha ((\\Delta x)^2+(\\Delta y)^2)}\n    \\vspace{-1ex}\n\\end{equation}\nHere $\\alpha$ is used to adjust the intensity attenuation from the center to the periphery and set to 2.5 in our experiments. We take it as the ground truth of center-ness and predict it from the regression branch for filtering low-quality predictions later. As mentioned earlier, this center-ness target ranges from 0 to 1, so we use the Binary Cross Entropy (BCE) loss for training that branch.% !TEX root = ./arxiv.tex\n\n"
                }
            },
            "section 4": {
                "name": "Experimental Setup",
                "content": "\n\\label{sec:experimental_setup}\n",
                "subsection 4.1": {
                    "name": "Dataset",
                    "content": "\nWe evaluate our framework on a large-scale, commonly used dataset, nuScenes~\\cite{nuScenes}. It consists of multi-modal data collected from 1000 scenes, including RGB images from 6 surround-view cameras, points from 5 Radars and 1 LiDAR. It is split into 700/150/150 scenes for training/validation/testing. There are overall 1.4M annotated 3D bounding boxes from 10 categories. Due to its variety of scenes and ground truths, it is becoming one of the authoritative benchmarks for 3D object detection. Therefore, we take it as the platform to validate the efficacy of our method.\n\n"
                },
                "subsection 4.2": {
                    "name": "Evaluation Metrics",
                    "content": "\nWe use the official metrics, distance-based mAP, and NDS for a fair comparison with other methods. Next, we briefly introduce these two kinds of metrics as follows.\n\n\\noindent\\textbf{Average Precision metric} \\quad\nThe Average Precision (AP) metric is generally used when evaluating the performance of object detectors. Instead of using 3D Intersection over Union (IoU) for thresholding, nuScenes defines the match by 2D center distance $d$ on the ground plane for decoupling detection from object size and orientation. On this basis, we calculate AP by computing the normalized area under the precision-recall curve for recall and precision over 10\\%. Finally, mAP is computed over all matching thresholds, $\\mathbb{D} = \\{0.5, 1, 2, 4\\}$ meters, and all categories $\\mathbb{C}$:\n\\vspace{-1ex}\n\\begin{equation}\n    \\centering\n    mAP = \\frac{1}{|\\mathbb{C}||\\mathbb{D}|}\\sum_{c\\in\\mathbb{C}}\\sum_{d\\in\\mathbb{D}}AP_{c,d}\n    \\vspace{-0.6ex}\n\\end{equation}\n\\noindent\\textbf{True Positive metrics} \\quad\nApart from Average Precision, we also calculate five kinds of True Positive metrics, Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE) and Average Attribute Error (AAE). To obtain these measurements, we firstly define that predictions with center distance from the matching ground truth $d \\le 2m$ will be regarded as true positives (TP). Then matching and scoring are conducted independently for each class of objects, and each metric is the average cumulative mean at each recall level above 10\\%. ATE is the Euclidean center distance in 2D ($m$). ASE is equal to $1-IOU$, $IOU$ is calculated between predictions and labels after aligning their translation and orientation. AOE is the smallest yaw angle difference between predictions and labels ($radians$). Note that different from other classes measured on the entire $360^{\\circ}$ period, barriers are measured on $180^{\\circ}$ period. AVE is the L2-Norm of the absolute velocity error in 2D ($m/s$). AAE is defined as $1-acc$, where $acc$ refers to the attribute classification accuracy. Finally, given these metrics, we compute the mean TP metric (mTP) overall all categories:\n\\vspace{-1ex}\n\\begin{equation}\n    \\centering\n    mTP = \\frac{1}{|\\mathbb{C}|}\\sum_{c\\in\\mathbb{C}}TP_c\n    \\vspace{-1ex}\n\\end{equation}\nNote that not well-defined metrics will be omitted, like AVE for cones and barriers, considering they are stationary.\n\n\\noindent\\textbf{NuScenes Detection Score} \\quad\nThe conventional mAP couples the evaluation of locations, sizes, and orientations of detections and also could not capture some aspects in this setting like velocity and attributes, so this benchmark proposes a more comprehensive, decoupled but simple metric, nuScenes detection score (NDS):\n\\vspace{-1ex}\n\\begin{equation}\n    \\centering\n    NDS = \\frac{1}{10}[5mAP+\\sum_{mTP\\in\\mathbb{TP}}(1-min(1, mTP))]\n    \\vspace{-1ex}\n\\end{equation}\nwhere mAP is mean Average Precision (mAP) and $\\mathbb{TP}$ is the set composed of five True Positive metrics. Considering mAVE, mAOE and mATE can be larger than 1, a bound is applied to limit them between 0 and 1.\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Implementation Details",
                    "content": "\n\\noindent\\textbf{Network Architectures}\\quad\nAs shown in Fig.~\\ref{fig: overview}, our framework follows the design of FCOS. Given the input image, we utilize ResNet101 as the feature extraction backbone followed by Feature Pyramid Networks (FPN) for generating multi-level predictions. Detection heads are shared among multi-level feature maps except that three scale factors are used to differentiate some of their final regressed results, including offsets, depths, and sizes, respectively. All the convolutional modules are made up of basic convolution, batch normalization, and activation layers, and normal distribution is leveraged for weights initialization. The overall framework is built on top of MMDetection3D~\\cite{mmdet3d}.\n\n\\noindent\\textbf{Training Parameters}\\quad\nFor all experiments, we trained randomly initialized networks from scratch following end-to-end manners. Models are trained with an SGD optimizer. Gradient clip and warm-up policy are exploited with the learning rate 0.002, the number of warm-up iterations 500, warm-up ratio 0.33, and batch size 32 on 16 GTX 1080Ti GPUs. We apply a weight of 0.2 for depth regression to train our baseline model to make the training more stable. For a more competitive performance and a more accurate detector, we finetune our model with this weight switched to 1. Related results are presented in the ablation study.\n\n\\noindent\\textbf{Data Augmentation}\\quad\nLike previous work, we only implement image flip for data augmentation both when training and testing. Note that only the offset is needed to be flipped as 2D attributes and 3D boxes need to be transformed correspondingly in 3D space when flipping images. For test time augmentation, we average the score maps output by the detection heads except rotation and velocity related scores due to their inaccuracy. It is empirically a more efficient approach for augmentation than merging boxes at last.\n%\\begin{figure*}\n%\\begin{center}\n%\\includegraphics[width=1.0\\linewidth]{./figures/precision_recall.png}\n%\\end{center}\n%   \\caption{Detailed precision vs. recall curve for each class of our model.}\n%\\label{fig: precision_recall}\n%\\end{figure*}\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Results",
                "content": "\nIn this section, we present quantitative and qualitative results and make a detailed ablation study on essential factors in pushing our method towards the state-of-the-art.\n\\label{sec:results}\n",
                "subsection 5.1": {
                    "name": "Quantitative Analysis",
                    "content": "\nFirst, we show the results of quantitative analysis in Tab.~\\ref{tab: quantitative}. We compare the results on the test set and validation set, respectively. We first compared all the methods using RGB images as the input data on the test set. We achieved the best performance among them with mAP 0.358 and NDS 0.428. In particular, our method exceeded the previous best one by more than 2\\% in terms of mAP. Benchmarks using LiDAR data as the input include PointPillars~\\cite{PointPillars}, which are faster and lighter, and CBGS~\\cite{CBGS} (MEGVII in the Tab.~\\ref{tab: quantitative}) with relatively high performance. For the approaches which use the input of RGB image and Radar data, we select CenterFusion~\\cite{CenterFusion} as the benchmark. It can be seen that although our method has a certain gap with the high-performance CBGS, it even surpasses PointPillars and CenterFusion on mAP. It shows that we can solve this ill-posed problem decently with enough data. At the same time, it can be seen that the methods using other modals of data have relatively better NDS, mainly because the mAVE is smaller. The reason is that other methods introduce continuous multi-frame data, such as point cloud data from consecutive frames, to predict the speed of objects. In addition, Radars can measure the velocity, so CenterFusion can achieve reasonable speed prediction even with a single frame image. However, these can not be achieved with only a single image, so how to mine the speed information from consecutive frame images will be one of the directions that can be explored in the future. For detailed mAP for each category, please refer to Tab.~\\ref{tab: ap_class} and the official benchmark.\n\nOn the validation set, we compare our method with the best open-source detector, CenterNet. Their method not only takes about three days to train (compared with our only one day to achieve comparable performance, possibly thanks to our pre-trained backbone) but also is inferior to our method except for mATE. In particular, thanks to our rotation encoding scheme, we achieved a significant improvement in the accuracy of angle prediction. The significant improvement of mAP reflects the superiority of our multi-level prediction. Based on all the improvements in these aspects, we finally achieved a gain of about 9\\% on NDS.\n\n"
                },
                "subsection 5.2": {
                    "name": "Qualitative Analysis",
                    "content": "\nThen we show some qualitative results in Fig.~\\ref{fig: qualitative} to give an intuitive understanding of the performance of our model. First of all, in Fig.~\\ref{fig: qualitative}, we draw the predicted 3D bounding boxes in the six-view images and the top-view point clouds. For example, the barriers in the camera at the rear right are not labeled but detected by our model. However, at the same time, we should also see that our method still has apparent problems in the depth estimation and identification of occluded objects. For example, it is difficult to detect the blocked car in the left rear image. Moreover, from the top view, especially in terms of depth estimation, results are not as good as those shown in the image. This is also in line with our expectation that depth estimation is still the core challenge in this ill-posed problem.\n\n"
                },
                "subsection 5.3": {
                    "name": "Ablation Studies",
                    "content": "\nFinally, we show some critical factors in the whole process of studying in Tab.~\\ref{tab: ablation}. It can be seen that in the prophase process, transforming depth back to the original space to compute loss is an essential factor to improve mAP, and distance-based target assignment is an essential factor to improve the overall NDS. The stronger backbone, such as replacing the original ResNet50 with ResNet101 and using DCN, is crucial in the later promotion process. At the same time, due to the difference in scales and measurements, using disentangled heads for different regression targets is also a meaningful way to improve the accuracy of angle prediction and NDS. Finally, we achieve the current state-of-the-art through simple augmentation, more training epochs, and a basic model ensemble.\n\\iffalse\n\nIn the Fig.~\\ref{fig: failure_case}, we show some failure cases, mainly focused on the detection of large objects and occluded objects. In the camera view and top view, the yellow dotted circle is used to mark the blocked object which has not been successfully detected, while the red dotted circle is used to mark the detected large object with obvious deviation. The former is mainly manifest in the failure to find the object behind, while the latter is mainly manifest in the inaccurate estimation of the size and orientation of the object. The reasons behind the two failure cases are also different. The former is due to the inherent property of the current setting, which is difficult to be solved; the latter may be due to the fact that the receptive field of convolution kernel of the current model is not large enough, resulting in low performance of large object detection. Therefore, the future research direction may be more focused on the solution of the latter.\n\\fi\n% !TEX root = ./arxiv.tex\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\nThis paper proposes a simple yet efficient one-stage framework, FCOS3D, for monocular 3D object detection without any 2D detection or 2D-3D correspondence priors. In the framework, we first transform the commonly defined 7-DoF 3D targets to the image domain and decouple them as 2D and 3D attributes to fit the 3D setting. On this basis, the objects are distributed to different feature levels considering their 2D scales and further assigned only according to the 3D centers. In addition, the center-ness is redefined with a 2D Gaussian distribution based on the 3D-center to be compatible with our target formulation.\nExperimental results with detailed ablation studies show the efficacy of our approach. For future work, a promising direction is how to better tackle the difficulty of depth and orientation estimation in this ill-posed setting.\n\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egbib}\n}\n\n% !TEX root = ./arxiv.tex\n\n\\clearpage\n\n\\twocolumn[{%\n\\renewcommand\\twocolumn[1][]{#1}%\n\\begin{center}\n    \\Large\n    \\textbf{Appendix}\n\\end{center}\n\\begin{center}\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{./failure_case.png}\n  \\vspace{-4ex}\n  \\captionof{figure}{Failure cases. As shown in this figure, our detectors perform poorly, especially for occluded and large objects. We use yellow dotted circles to mark the failure case caused by occlusion while use red dotted circles to mark the inaccurate large objects predictions. The former problem is intrinsic, considering the ill-posed property of this task itself. So a direction to improve our method would be how to enhance the detection performance for large objects.}\n  \\label{fig: failure_case}\n  \\vspace{1ex}\n\\end{center}%\n}]\n\n\\setcounter{section}{0}\n"
            },
            "section 7": {
                "name": "Failure Cases",
                "content": "\n\\vspace{-0.5ex}\nIn Fig.~\\ref{fig: failure_case}, we show some failure cases, mainly focused on the detection of large objects and occluded objects. In the camera view and top view, yellow dotted circles are used to mark the blocked objects that are not successfully detected. Red dotted circles are used to mark the detected large objects with noticeable deviation. The former is mainly manifest in the failure to find the objects behind, while the latter is mainly manifest in the inaccurate estimation of the size and orientation of the objects. The reasons behind the two failure cases are also different. The former is due to the inherent property of the current setting, which is difficult to solve; the latter may be because the receptive field of convolution kernel of the current model is not large enough, resulting in low performance of large object detection. Therefore, the future research direction may be more focused on the solution of the latter.\n\n\\newpage\n"
            },
            "section 8": {
                "name": "Results on the KITTI Benchmark",
                "content": "\n\\vspace{-0.5ex}\nWe provide FCOS3D baseline results on the KITTI benchmark in the follow-up work, PGD~\\cite{PGD}. Since the number of samples on KITTI is limited, vanilla FCOS3D cannot achieve outstanding performance. With the basic enhancement of local geometric constraints and customized designs for depth estimation, PGD (can also be termed as FCOS3D++) finally achieves state-of-the-art or competitive performance on various benchmarks under different evaluation metrics. Please refer to the paper~\\cite{PGD} for more details.\n"
            }
        },
        "tables": {
            "tab: quantitative": "\\begin{table*}\n\\small\n    \\caption{Results on the nuScenes dataset.}\n    \\vspace{-4ex}\n\t\\begin{center}\n\t\\begin{tabular}{c|c|c|c|c|c|c|c|c|c}\n\t\\hline\n\tMethods & Dataset & Modality & mAP & mATE & mASE & mAOE & mAVE & mAAE & NDS\\\\\n\t\\hline\\hline\n\tCenterFusion~\\cite{CenterFusion} & test & Camera \\& Radar & 0.326 & 0.631 & 0.261 & 0.516 & 0.614 & 0.115 & 0.449\\\\\n\t\\hline\n\tPointPillars~\\cite{PointPillars} & test & LiDAR & 0.305 & 0.517 & 0.290 & 0.500 & 0.316 & 0.368 & 0.453\\\\\n\t\\hline\n\tMEGVII~\\cite{CBGS} & test & LiDAR & \\textbf{0.528} & 0.300 & 0.247 & 0.379 & 0.245 & 0.140 & \\textbf{0.633}\\\\\n\t\\hline\\hline\n\tLRM0 & test & Camera & 0.294 & 0.752 & 0.265 & 0.603 & 1.582 & 0.14 & 0.371\\\\\n\t\\hline\n\tMonoDIS~\\cite{MonoDIS} & test & Camera & 0.304 & 0.738 & 0.263 & 0.546 & 1.553 & 0.134 & 0.384\\\\\n\t\\hline\n\tCenterNet~\\cite{CenterNet} (HGLS) & test & Camera & 0.338 & 0.658 & 0.255 & 0.629 & 1.629 & 0.142 & 0.4\\\\\n\t\\hline\n\tNoah CV Lab & test & Camera & 0.331 & 0.660 & 0.262 & 0.354 & 1.663 & 0.198 & 0.418\\\\\n\t\\hline\n\tFCOS3D (Ours) & test & Camera & \\textbf{0.358} & 0.690 & 0.249 & 0.452 & 1.434 & 0.124 & \\textbf{0.428}\\\\\n\t\\hline\\hline\n\tCenterNet~\\cite{CenterNet} (DLA) & val & Camera & 0.306 & 0.716 & 0.264 & 0.609 & 1.426 & 0.658 & 0.328\\\\\n\t\\hline\n\tFCOS3D (Ours) & val & Camera & \\textbf{0.343} & 0.725 & 0.263 & 0.422 & 1.292 & 0.153 & \\textbf{0.415}\\\\\n\t\\hline\n\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-5.0ex}\n\t\\label{tab: quantitative}\n\\end{table*}",
            "tab: ap_class": "\\begin{table*}\n\\small\n    \\caption{Average precision for each class on the nuScenes test benchmark. CV and TC are abbreviation of construction vehicle and traffic cone in the table.}\n    \\vspace{-3.5ex}\n\t\\begin{center}\n\t\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}\n\t\\hline\n\tMethods & car & truck & bus & trailer & CV & ped & motor & bicycle & TC & barrier & mAP\\\\\n\t\\hline\\hline\n\tLRM0 & 0.467 & 0.21 & 0.17 & 0.149 & 0.061 & 0.359 & 0.287 & 0.246 & 0.476 & 0.512 & 0.294\\\\\n\t\\hline\n\tMonoDIS~\\cite{MonoDIS} & 0.478 & 0.22 & 0.188 & 0.176 & 0.074 & 0.37 & 0.29 & 0.245 & 0.487 & 0.511 & 0.304\\\\\n\t\\hline\n\tCenterNet~\\cite{CenterNet} (HGLS) & 0.536 & 0.27 & 0.248 & 0.251 & 0.086 & 0.375 & 0.291 & 0.207 & 0.583 & 0.533 & 0.338\\\\\n\t\\hline\n\tNoah CV Lab & 0.515 & 0.278 & 0.249 & 0.213 & 0.066 & 0.404 & 0.338 & 0.237 & 0.522 & 0.49 & 0.331\\\\\n\t\\hline\n\tFCOS3D (Ours) & 0.524 & 0.27 & 0.277 & 0.255 & 0.117 & 0.397 & 0.345 & 0.298 & 0.557 & 0.538 & \\textbf{0.358}\\\\\n\t\\hline\n\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-3.0ex}\n\t\\label{tab: ap_class}\n\\end{table*}",
            "tab: ablation": "\\begin{table*}\n\\small\n    \\caption{Ablation studies on the nuScenes validation 3D detection benchmark.}\n    \\vspace{-3.5ex}\n\t\\begin{center}\n\t\\begin{tabular}{c|c|c|c|c|c|c|c}\n\t\\hline\n\tMethods & mAP & mATE & mASE & mAOE & mAVE & mAAE & NDS\\\\\n\t\\hline\\hline\n\tBaseline (FCOS + 3D targets) & 0.227 & 0.868 & 0.272 & 0.778 & 1.326 & 0.393 & 0.282\\\\\n\t\\hline\n\t+ Depth loss in original space & 0.25 & 0.838 & 0.268 & 0.892 & 1.33 & 0.413 & 0.284\\\\\n\t\\hline\n\t+ Flip augmentation & 0.248 & 0.85 & 0.267 & 1.016 & 1.358 & 0.268 & 0.286\\\\\n\t\\hline\n\t+ Dist-based target assign  \\& attr pred & 0.257 & 0.832 & 0.268 & 0.852 & 1.2 & 0.18 & 0.316\\\\\n\t\\hline\n\t+ NMS among predictions of six views & 0.26 & 0.828 & 0.267 & 0.85 & 1.371 & 0.18 & 0.317\\\\\n\t\\hline\n\t+ Stronger backbone (ResNet101) & 0.272 & 0.821 & 0.265 & 0.81 & 1.379 & 0.17 & 0.329\\\\\n\t\\hline\n\t+ Disentangled heads & 0.28 & 0.822 & 0.274 & 0.64 & 1.305 & 0.177 & 0.349\\\\\n\t\\hline\n\t+ DCN in backbone & 0.295 & 0.806 & 0.268 & 0.511 & 1.315 & 0.17 & 0.372\\\\\n\t\\hline\n\t+ Finetune w/ depth weight=1.0 & 0.316 & 0.755 & 0.263 & 0.458 & 1.307 & 0.169 & 0.393\\\\\n\t\\hline\n\t+ Test time augmentation & 0.326 & 0.743 & 0.259 & 0.441 & 1.341 & 0.163 & 0.402\\\\\n\t\\hline\n\t+ More epochs \\& ensemble & \\textbf{0.343} & 0.725 & 0.263 & 0.422 & 1.292 & 0.153 & \\textbf{0.415}\\\\\n\t\\hline\n\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-4.5ex}\n\t\\label{tab: ablation}\n\\end{table*}"
        },
        "figures": {
            "fig: overview": "\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{./fcos3d_overview.png}\n\\end{center}\n   \\vspace{-3ex}\n   \\caption{An overview of our pipeline. To leverage the well-developed 2D feature extractors, we basically follow the typical design of backbone and neck for 2D detectors. For detection head, we first reformulate the 3D targets with center-based paradigm to decouple it as multi-task learning. The strategies for multi-level target assignment and center sampling are further adjusted accordingly to equip this framework with the better capability of handling overlapped ground truths and scale variance problem.}\n   \\vspace{-2.5ex}\n\\label{fig: overview}\n\\end{figure*}",
            "fig: rot_encoding": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{./rot_encoding.png}\n\\end{center}\n   \\vspace{-3.5ex}\n   \\caption{Our exploited rotation encoding scheme. Two objects with opposite orientations share the same rotation offset based on the 2-bin boundary, thus have the same $sin$ value. To distinguish them, we predict an additional direction class from the regression branch.}\n   \\vspace{-4ex}\n\\label{fig: rot_encoding}\n\\end{figure}",
            "fig: target_assign": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{./target_assign.png}\n\\end{center}\n   \\vspace{-3ex}\n   \\caption{Our proposed distance-based target assignment for dealing with ambiguity case could significantly improve the best possible recall (BPR) for each class, especially for large objects like trailers. Construction vehicle and traffic cone are abbreviated as CV and TC in this figure.}\n   \\vspace{-3ex}\n\\label{fig: target_assign}\n\\end{figure}",
            "fig: qualitative": "\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{./qualitative.png}\n\\end{center}\n   \\vspace{-3.2ex}\n   \\caption{Qualitative analysis of detection results. 3D bounding boxes predictions are projected onto images from six different views and bird-view, respectively. Boxes from different categories are marked with different colors. We can see that the results are reasonable except for some detection with false class predictions from the left part. Moreover, a few small objects are detected by our model while not annotated as ground truth, like barriers in the back/back right camera. However, apart from the inherent occlusion problem in this setting, depth and orientation estimations of some objects are still inaccurate, which can be observed in the visualization from bird-view.}\n   \\vspace{-2.8ex}\n\\label{fig: qualitative}\n\\end{figure*}",
            "fig: failure_case": "\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{./failure_case.png}\n\\end{center}\n   \\vspace{-2ex}\n   \\caption{Failure cases. As shown in this figure, our detectors perform poorly especially for occluded and large objects. We use yellow dotted circle to mark the failure case caused by occlusion, while use red dotted circle to mark the inaccurate large objects predictions. The former problem is intrinsic considering the ill-posed property of this task itself. So a direction to improve our method would be how to enhance the detection performance for large objects.}\n\\label{fig: failure_case}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\centering\n    L_{cls} = -\\alpha(1-p)^\\gamma logp\n    \\vspace{-1ex}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\label{eqn: loc_loss}\n    \\centering\n    L_{loc} = \\sum_{b\\in (\\Delta x, \\Delta y, d, w, l, h, \\theta, v_x, v_y)} SmoothL1(\\Delta b)\n    \\vspace{-1ex}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\centering\\footnotesize\n    L = \\frac{1}{N_{pos}}(\\beta_{cls}L_{cls}+\\beta_{attr}L_{attr}+\\beta_{loc}L_{loc}+\\beta_{dir}L_{dir}+\\beta_{ct}L_{ct})\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\centering\n    c = \\sqrt{\\frac{min(l^*, r^*)}{max(l^*, r^*)}\\times \\frac{min(t^*, b^*)}{max(t^*, b^*)}}\n    \\vspace{-1ex}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\centering\n    c = e^{-\\alpha ((\\Delta x)^2+(\\Delta y)^2)}\n    \\vspace{-1ex}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\centering\n    mAP = \\frac{1}{|\\mathbb{C}||\\mathbb{D}|}\\sum_{c\\in\\mathbb{C}}\\sum_{d\\in\\mathbb{D}}AP_{c,d}\n    \\vspace{-0.6ex}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\centering\n    mTP = \\frac{1}{|\\mathbb{C}|}\\sum_{c\\in\\mathbb{C}}TP_c\n    \\vspace{-1ex}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\centering\n    NDS = \\frac{1}{10}[5mAP+\\sum_{mTP\\in\\mathbb{TP}}(1-min(1, mTP))]\n    \\vspace{-1ex}\n\\end{equation}"
        },
        "git_link": "https://github.com/open-mmlab/mmdetection3d"
    }
}