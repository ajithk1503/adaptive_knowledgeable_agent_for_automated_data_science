{
    "meta_info": {
        "title": "Reducing Exposure to Harmful Content via Graph Rewiring",
        "abstract": "Most media content consumed today is provided by digital platforms that\naggregate input from diverse sources, where access to information is mediated\nby recommendation algorithms. One principal challenge in this context is\ndealing with content that is considered harmful. Striking a balance between\ncompeting stakeholder interests, rather than block harmful content altogether,\none approach is to minimize the exposure to such content that is induced\nspecifically by algorithmic recommendations. Hence, modeling media items and\nrecommendations as a directed graph, we study the problem of reducing the\nexposure to harmful content via edge rewiring. We formalize this problem using\nabsorbing random walks, and prove that it is NP-hard and NP-hard to approximate\nto within an additive error, while under realistic assumptions, the greedy\nmethod yields a (1-1/e)-approximation. Thus, we introduce Gamine, a fast greedy\nalgorithm that can reduce the exposure to harmful content with or without\nquality constraints on recommendations. By performing just 100 rewirings on\nYouTube graphs with several hundred thousand edges, Gamine reduces the initial\nexposure by 50%, while ensuring that its recommendations are at most 5% less\nrelevant than the original recommendations. Through extensive experiments on\nsynthetic data and real-world data from video recommendation and news feed\napplications, we confirm the effectiveness, robustness, and efficiency of\nGamine in practice.",
        "author": "Corinna Coupette, Stefan Neumann, Aristides Gionis",
        "link": "http://arxiv.org/abs/2306.07930v1",
        "category": [
            "cs.SI",
            "cs.CY",
            "cs.DS"
        ],
        "additionl_info": "25 pages, 28 figures, accepted at KDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:introduction}\n\nRecommendation algorithms mediate access to content on digital platforms, \nand as such, they critically influence how individuals and societies perceive the world and form their opinions \\cite{spinelli2017closed,robertson2018auditing,papadamou2022just,hussein2020measuring,ferrara2022link}. \nIn recent years, \nplatforms have come under increasing scrutiny from researchers and regulators alike\ndue to concerns and evidence that their recommendation algorithms create filter bubbles \\cite{ledwich2022radical,srba2022auditing,kirdemir2022exploring,chitra2020analyzing} \nand fuel radicalization \\cite{hosseinmardi2021examining,whittaker2021recommender,ribeiro2020auditing,ledwich2020algorithmic,pescetelli2022bots}. \nOne of the main challenges in this context is dealing with content that is considered harmful \\cite{bandy2021problematic,yesilada2022systematic,costanza2022audits}.\nTo address this challenge while balancing the interests of creators, users, and platforms,\nrather than block harmful content, \none approach is to minimize the exposure to such content that is induced by algorithmic recommendations. \n\n\n\nIn this paper, \nwe study the problem of reducing the exposure to harmful content via \\emph{edge rewiring}, \ni.e., replacing certain recommendations by others. \nThis problem was recently introduced by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring},\nwho proposed to address it\nby modeling harmfulness as a \\emph{binary} node label and minimizing the \\emph{maximum} \\emph{segregation}, \ndefined as the largest expected number of steps of a random walk starting at a harmful node until it visits a benign node. \nHowever, while \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring} \nposed a theoretically interesting and practically important problem, \ntheir approach has some crucial limitations. \n\nFirst, treating harmfulness as dichotomous \nfails to capture the complexity of real-world harmfulness assessments.\nSecond, \nthe segregation objective\nignores completely all random-walk continuations that return to harmful content after the first visit to a benign node, \nbut \\emph{benign nodes do not act as absorbing states} in practice.\nThe consequences are illustrated in \\cref{fig:setting:bad}, \nwhere the segregation objective judges that the graph\nprovides minimal exposure to harmful content \n(the hitting time from any harmful node to a benign node is~1), \nwhile long random walks, \nwhich model user behavior more realistically, \noscillate between harmful and benign~content.\n\nIn this paper, \nwe remedy the above-mentioned limitations.\nFirst, we more nuancedly model harmfulness as \\emph{real-valued} node costs.\nSecond, we propose a novel minimization objective, \nthe \\emph{expected} \\emph{total} \\emph{exposure}, \ndefined as the sum of the costs of absorbing random walks starting at any node.\nNotably, in our model, \nno node is an absorbing state, \nbut any node can lead to absorption, \nwhich represents more faithfully how users cease to interact with a platform.\nOur exposure objective truly minimizes the exposure to harmful content. \nFor example, it correctly \nidentifies the graph in \\cref{fig:setting:good} as significantly less harmful than that in \\cref{fig:setting:bad}, \nwhile for the segregation objective by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring},\nthe two graphs are indistinguishable. \n\nOn the algorithmic side, \nwe show that although minimizing the expected total exposure is \\NP-hard and \\NP-hard to approximate to within an additive error, \nits maximization version is equivalent to a submodular maximization problem \nunder the assumption that the input graph contains a small number of \\emph{safe} nodes, \ni.e., nodes that cannot reach nodes with non-zero costs.\nIf these safe nodes are present---% \nwhich holds in $80\\%$ of the real-world graphs used in our experiments---%\nthe greedy method yields a $(1-\\nicefrac{1}{e})$-approximation.  \nBased on our theoretical insights, \nwe introduce \\ourmethod, \na fast greedy algorithm for reducing exposure to harmful content via edge rewiring. \n\\ourmethod leverages provable strategies for pruning unpromising rewiring candidates,\nand it works both with and without quality constraints on recommendations.\nWith just $100$ rewirings on YouTube graphs containing hundred thousands of edges, \n\\ourmethod reduces the exposure by $50\\%$, \nwhile ensuring that its recommendations are at least $95\\%$ as relevant as the originals.\n\n\nIn the following, \nwe introduce our problems, \\ourproblem and \\ourproblemtwo (\\cref{sec:problems}), \nand analyze them theoretically (\\cref{sec:theory}).\nBuilding on our theoretical insights, \nwe develop \\ourmethod as an efficient greedy algorithm for tackling our problems (\\cref{sec:algorithm}). \nHaving discussed related work (\\cref{sec:related}),\nwe demonstrate the performance of \\ourmethod through extensive experiments (\\cref{sec:experiments}) \nbefore concluding with a discussion (\\cref{sec:conclusion}).\nAll code, datasets, and results are publicly available,\\!\\footnote{\\oururl}\nand we provide further materials in \\cref{apx:datasets,apx:edits,apx:experiments,apx:hardness,apx:pseudocode,apx:reproducibility}.\n\n% vim:spell spelllang=en_us\n%!TEX root = ../main.tex\n"
            },
            "section 2": {
                "name": "Problems",
                "content": "\n\\label{sec:problems}\n\nWe consider a directed graph $\\graph = (\\nodes,\\edges)$ of content items ($\\nodes$) and what-to-consume-next recommendations ($\\edges$),\nwith $\\nnodes = \\cardinality{\\nodes}$ nodes and $\\nedges = \\cardinality{\\edges}$ edges. \nSince we can typically make a fixed number of recommendations for a given content item, \nsuch \\emph{recommendation graphs} are often $\\outregulardegree$-out-regular, \ni.e., all nodes have $\\outregulardegree = \\nicefrac{\\nedges}{\\nnodes}$ out-neighbors, \nbut we do not restrict ourselves to this setting.\nRather, each node $i$ has an out-degree $\\outdegree{i} = \\cardinality{\\outneighbors{i}}$, \nwhere~$\\outneighbors{i}$ is the set of out-neighbors of $i$, \nand a cost $\\labeling_i\\in\\range$,\nwhich quantifies the harmfulness of content item $i$, \nranging from $0$ (not harmful at all) to $1$ (maximally harmful).\nFor convenience, we define $\\maxoutdegree = \\max\\{\\outdegree{i}\\mid i \\in \\nodes\\}$\nand collect all costs into a vector $\\labelingvector\\in \\range^{\\nnodes}$. \nWe model user behavior as a random-walk process on the recommendation graph~$\\graph$. \nEach edge $\\edge{i}{j}$ in the recommendation graph\nis associated with a transition probability $\\probability{ij}$ \nsuch that $\\sum_{j\\in\\outneighbors{i}}\\probability{ij} = 1 - \\pabsorption_i$, \nwhere $\\pabsorption_i$ is the absorption probability of a random walk at node $i$ \n(i.e., the probability that the walk ends~at~$i$). Intuitively, one can interpret $\\alpha_i$ as the probability that a user stops using the service after consuming content~$i$.\nFor simplicity, we assume $\\pabsorption_i = \\pabsorption \\in (0,1]$ for all $i\\in\\nodes$.\nThus, we can represent the random-walk process on $\\graph$ \nby the transition matrix $\\transitionmatrix\\in[0,1-\\pabsorption]^{\\nnodes\\times\\nnodes}$, \nwhere \n\\begin{align}\n\t\\transitionmatrix[i,j] =\\begin{cases}\n\t\t \\probability{ij}& \\text{if }\\edge{i}{j}\\in \\edges\\;,\\\\\n\t\t 0&\\text{otherwise}\\;.\n\t\\end{cases}\n\\end{align}\nThis is an absorbing Markov chain, and the expected number of visits from a node $i$ \nto a node $j$ before absorption is given by the entry $(i,j)$ \nof the \\emph{fundamental matrix} \n$\\fundamental\\in\\reals_{\\geq 0}^{\\nnodes\\times\\nnodes}$, defined as\n\\begin{align}\\label{eq:fundamental}\n\t\\fundamental = \\sum_{i=0}^{\\infty} \\transitionmatrix^i = (\\identity - \\transitionmatrix)^{-1}\\;,\n\\end{align}\nwhere $\\identity$ is the $\\nnodes\\times\\nnodes$-dimensional identity matrix, \nand the series converges since $\\norm{\\transitionmatrix}_{\\infty} = \\max_i\\sum_{j=0}^{\\nnodes}\\transitionmatrix[i,j] = 1 - \\pabsorption < 1$.\nDenoting the $i$-th unit vector as~$\\unitvector_i$, \nobserve that the row vector $\\unitvector_i^T\\fundamental$ gives the expected number of visits, before absorption, from $i$ to any node, \nand the column vector $\\fundamental\\unitvector_i$ gives the expected number of visits from any node to $i$.\nHence, $\\unitvector_i^T\\fundamental\\labelingvector = \\sum_{j\\in\\nodes}\\fundamental[i,j]\\labelingvector_j$ gives the expected exposure to harmful content of users starting their random walk at node $i$, \nreferred to as the \\emph{exposure} of $i$. \nThe \\emph{expected total exposure} to harm in the graph $\\graph$, then, \nis given by the non-negative function\n\\begin{align}\n\t\\objective{\\graph} = \\onevector^T\\fundamental\\labelingvector\\;,\n\t\\label{eq:harm}\n\\end{align}\nwhere $\\onevector$ is the vector with each entry equal to~$1$.\n\nWe would like to \\emph{minimize} the exposure function given in \\cref{eq:harm} \nby making $\\budget$ edits to the graph $\\graph$, \ni.e., we seek an effective \\emph{post-processing strategy} for harm reduction. \nIn line with our motivating application, \nwe restrict edits to \\emph{edge rewirings} denoted as $\\rewiring{i}{j}{k}$, \nin which we replace an edge $\\edge{i}{j}\\in\\edges$ by an edge $\\edge{i}{k}\\notin\\edges$ with $i\\neq k$, \nsetting $\\probability{ik} = \\probability{ij}$ \n(other edits are discussed in \\cref{apx:edits}). \nSeeking edge rewirings to minimize the expected total exposure yields the following problem definition.\n\\begin{problem}[$\\budget$-rewiring exposure minimization {[}\\ourproblem{]}]\n\t\\label{problem}\n\tGiven a graph $\\graph$, \n\tits random-walk transition matrix $\\transitionmatrix$, \n\ta node cost vector $\\labelingvector$, \n\tand a budget $\\budget$, \n\tminimize $\\objective{\\graph_\\budget}$, \n\twhere $\\graph_\\budget$ is $\\graph$ after $\\budget$ rewirings.\n\\end{problem}\nEquivalently, we can \\emph{maximize} the \\emph{reduction} in the expected total exposure to harmful content,\n\\begin{align}\n\t\\maxobjective{\\graph,\\graph_\\budget} =\\objective{\\graph}-\\objective{\\graph_\\budget}\\;.\n\t\\label{eq:maxdelta}\n\\end{align}\nNote that while any set of rewirings minimizing $\\objective{\\graph_\\budget}$ also maximizes $\\maxobjective{\\graph,\\graph_\\budget}$, \nthe approximabilities of $\\objectivef$ and $\\maxobjectivef$ can differ widely.\n\nAs \\cref{problem} does not impose any constraints on the rewiring operations, \nthe optimal solution might contain rewirings $\\rewiring{i}{j}{k}$ such that node $k$ is unrelated to $i$. \nTo guarantee high-quality recommendations, \nwe need additional relevance information, \nwhich we assume to be given as a \\emph{relevance matrix} \n$\\relevancematrix\\in\\reals_{\\geq 0}^{\\nnodes\\times\\nnodes}$, \nwhere $\\relevancematrix[i,j]$ denotes the relevance of node~$j$ in the context of node~$i$. \nGiven such relevance information, \nand assuming that the out-neighbors of a node $i$ are ordered as\n$\\outseq{i}\\in \\nodes^{\\outdegree{i}}$, \nwe can define a \\emph{relevance function}~$\\qualityf$ with range $\\range$\nto judge the quality of the recommendation sequence at node~$i$,\ndepending on the relevance and ordering of recommended nodes, \nand demand that any rewiring retain $\\quality{\\outseq{i}} \\geq \\qualitythreshold$ for all $i\\in\\nodes$ and some \\emph{quality threshold} $\\qualitythreshold\\in\\range$. \nOne potential choice for $\\qualityf$\nis the normalized discounted cumulative gain ($\\ndcg$), \na popular ranking quality measure,\nwhich we use in our experiments and define in \\cref{apx:reproducibility:qualityf}.\nIntroducing $\\qualityf$ allows us to consider a variant of \\ourproblem with relevance constraints.\n\n\\begin{problem}[$\\qualitythreshold$-relevant $\\budget$-rewiring exposure minimization {[}\\ourproblemtwo{]}]\\label{problem2}\n\tGiven a graph $\\graph$, \n\tits random-walk transition matrix $\\transitionmatrix$, \n\ta node cost vector $\\labelingvector$, \n\ta budget $\\budget$, \n\ta relevance matrix $\\relevancematrix$,\n\ta relevance function $\\qualityf$,\n\tand a quality threshold $\\qualitythreshold$, \n\tminimize $\\objective{\\graph_\\budget}$\n\tunder the condition that $\\quality{\\outseq{i}} \\geq \\qualitythreshold$ for all $i\\in\\nodes$.\n\\end{problem}\n\nFor $\\qualitythreshold = 0$, \\ourproblemtwo is equivalent to \\ourproblem. \nCollecting our notation in Appendix~\\cref{tab:notation}, \nwe now seek to address both problems. \n\n% vim:spell spelllang=en_us\n%!TEX root = ../main.tex\n"
            },
            "section 3": {
                "name": "Theory",
                "content": "\n\\label{sec:theory}\n\nTo start with, we establish some theoretical properties of our problems, the functions $\\objectivef$ and $\\maxobjectivef$, \nand potential solution approaches.\n\n\\paragraph{Hardness}\nWe begin by proving that \\ourproblem (and hence, also \\ourproblemtwo) is an \\NP-hard problem.\n\n\\begin{proof}\n\tWe obtain this result by reduction from minimum vertex cover for cubic, \n\ti.e., 3-regular graphs (MVC-3), \n\twhich is known to be \\NP-hard \\cite{greenlaw1995cubic}.\n\tA full, illustrated proof is given in \\cref{apx:hardness:np}.\n\\end{proof}\n\nNext, we further show that \\ourproblem is hard to approximate under the Unique Games Conjecture (UGC) \\cite{khot2002power}, \nan influential conjecture in hardness-of-approximation theory.\n\n\\begin{proof}\n\tWe obtain this result via the hardness of approximation of MVC \n\tunder the UGC. \n\tA full proof is given in \\cref{apx:hardness:apx}.\n\\end{proof}\nBoth \\cref{thm:hardness} and \\cref{thm:apxhardness} extend from $\\objectivef$ to \\maxobjectivef (\\cref{eq:delta}). \n\n\\paragraph{Approximability}\nAlthough we cannot approximate $\\objectivef$ directly,  \nwe \\emph{can} approximate $\\maxobjectivef$ \\emph{with guarantees} \nunder mild assumptions, detailed below. \nTo formulate this result and its assumptions, \nwe start by calling a node \\emph{safe} \nif  $\\unitvector_i^T\\fundamental\\labelingvector = 0$, \ni.e., no node $j$ with $\\labeling_j > 0$ is reachable from $i$, \nand \\emph{unsafe} otherwise. \nNote that the existence of a safe node in a graph $\\graph$ containing at least one unsafe node (i.e.,~$\\labeling_i > 0$ for some $i\\in \\nodes$) \nimplies that $\\graph$ is not strongly connected. \nThe node safety property partitions $\\nodes$ into two sets of safe resp. unsafe nodes, \n$\\safenodes = \\{i\\in\\nodes\\mid \\unitvector_i^T\\fundamental\\labelingvector = 0\\}$~and~$\\unsafenodes = \\{i\\in\\nodes\\mid \\unitvector_i^T\\fundamental\\labelingvector > 0\\}$, \nand $\\edges$ into four sets, \n$\\edges_{\\safenodes\\safenodes}$, $\\edges_{\\safenodes\\unsafenodes}$, $\\edges_{\\unsafenodes\\safenodes}$, and $\\edges_{\\unsafenodes\\unsafenodes}$, \nwhere $\\edges_{AB} = \\{(i,j)\\in\\edges\\mid i\\in A, j\\in B\\}$, \nand $\\edges_{\\safenodes\\unsafenodes}=\\emptyset$ by construction. \nFurther, observe that if $\\safenodes\\neq\\emptyset$,  \nthen $\\objectivef$ is minimized, \nand $\\maxobjectivef$ is maximized, \nonce $\\edges_{\\unsafenodes\\unsafenodes} = \\emptyset$.\nThis allows us to state the following result.\n\n\\begin{proof}\n\tLeveraging the terminology introduced above, we obtain this result by applying the definitions of monotonicity and submodularity. \n\tA full proof is given in \\cref{apx:hardness:submod}.\n\\end{proof}\nOur motivating application, however, ideally prevents multi-edges.\nTo get a similar result without multi-edges, \ndenote by $\\maxunsafeoutdegree = \\max\\{\\outdegree{i}\\mid i \\in \\unsafenodes\\}$\nthe maximum out-degree of any \\emph{unsafe} node in~$\\graph$, \nand assume that $\\cardinality{\\safenodes} \\geq \\maxunsafeoutdegree$. \nNow, we obtain the following.\n\n\n\\begin{proof}\n\tFollowing the reasoning provided for \\cref{lem:submoddupes}, \n\twith the modification that we need $\\cardinality{\\safenodes} \\geq \\maxunsafeoutdegree$ to ensure that safe targets are always available for rewiring without creating multi-edges.\n\\end{proof}\n\nObserve that the larger the number of zero-cost nodes, \nthe smaller the number of edges, \nor the more homophilous the linking, \nthe higher the probability that safe nodes exist in a graph. \nNotably, the precondition of \\cref{thm:submodularity} holds for the graph constructed to prove \\cref{thm:hardness} (\\cref{apx:hardness:np}, \\cref{fig:hardness}) \nas well as for most of the real-world graphs used in our experiments (\\cref{apx:datasets}, \\cref{fig:safety}).\nHowever,\n\\cref{thm:submodularity} only applies to the maximization version of \\ourproblem (\\cref{eq:delta}) and not to the maximization version of \\ourproblemtwo, \nsince in the quality-constrained setting, \nsome safe nodes might not be available as rewiring targets for edges emanating from unsafe nodes. \nStill, for the maximization version of \\ourproblem,\ndue to \\cref{thm:submodularity}, \nusing a greedy approach to optimize $\\maxobjectivef$ \nprovides an approximation guarantee \nwith respect to the optimal solution~\\cite{nemhauser1978analysis}.\n\\begin{cor}\\label{cor:apxguarantee}\n\tIf the precondition of \\cref{thm:submodularity} holds, \n\tthen the greedy algorithm, \n\twhich always picks the rewiring $\\rewiring{i}{j}{k}$ that maximizes $\\maxobjective{\\graph,\\graph_1}$ for the current $\\graph$,\n\tyields a $(1-\\nicefrac{1}{e})$-approximation for $\\maxobjectivef$.\n\\end{cor}\nNote that \\cref{cor:apxguarantee} only applies to the \\emph{maximization} version of \\ourproblem, \nnot to its \\emph{minimization} version, \nas supermodular minimization is less well-behaved than submodular maximization \\cite{zhang2022fast,ilev2001approximation}.\n\n\\paragraph{Greedy Rewiring}\nGiven the quality assurance of a greedy approach at least for \\ourproblem, \nwe seek to design an efficient greedy algorithm to tackle both \\ourproblem and \\ourproblemtwo.\nTo this end, we analyze the mechanics of individual rewirings to understand how we can identify and perform greedily optimal rewirings efficiently.\nAs each greedy step constitutes a rank-one update of the \ntransition matrix~$\\transitionmatrix$, \nwe can express the new transition matrix $\\transitionmatrix'$ as \n\\begin{align}\n\t\\transitionmatrix' = \\transitionmatrix + \\ivector(-\\jkvector)^T \\;,\n\\end{align}\nwhere $\\ivector = \\probability{ij}\\unitvector_i$ and $\\jkvector = \\unitvector_j - \\unitvector_k$, \nand we omit the dependence on $i$, $j$, and $k$ for notational conciseness.\nThis corresponds to a rank-one update of $\\fundamental$, such that we obtain the new fundamental matrix $\\fundamental'$ as\n\\begin{align}\n\t\\fundamental' \n\t= (\\identity - (\\transitionmatrix + \\ivector(-\\jkvector)^T))^{-1}\n\t= (\\identity - \\transitionmatrix + \\ivector\\jkvector^T)^{-1} \\;.\n\\end{align}\n\nThe rank-one update allows us to use the \nSherman-Morrison formula \\cite{sherman1950adjustment} to compute \nthe updated fundamental matrix as\n\\begin{align}\\label{eq:sherman}\n\t\\fundamental'  = \n\t\t\\fundamental - \n\t\t\\frac{\\fundamental \\ivector \\jkvector^T\\fundamental}\n\t\t\t {1 + \\jkvector^T\\fundamental\\ivector} \\;.\n\\end{align}\n\nThe mechanics of an individual edge rewiring are summarized in \\cref{tab:rewiring}.\nThey will help us \\emph{perform} greedy updates efficiently.\n\nTo also \\emph{identify} greedily optimal rewirings efficiently, \nleveraging \\cref{eq:sherman},\nwe assess the impact \nof a rewiring on the value of our objective function, \nwhich will help us prune weak rewiring candidates. \nFor a rewiring $\\rewiring{i}{j}{k}$ represented by $\\ivector$ and $\\jkvector$, \nthe value of the exposure function~$\\objectivef$\nfor the new graph $\\graph'$ is\n\\begin{align}\n\t\\objective{\\graph'} \n\t&\t= \\onevector^T\\fundamental'\\labelingvector\n\t\t= \\onevector^T\\left(\\fundamental - \\frac{\\fundamental \\ivector \\jkvector^T\\fundamental}{1 + \\jkvector^T\\fundamental\\ivector}\\right)\\labelingvector\n\t\t= \\onevector^T\\fundamental\\labelingvector - \\onevector^T\\left(\\frac{\\fundamental \\ivector \\jkvector^T\\fundamental}{1 + \\jkvector^T\\fundamental\\ivector}\\right)\\labelingvector \\nonumber \\\\\n\t&\t= \\objective{\\graph} -  \\frac{(\\onevector^T\\fundamental \\ivector)(\\jkvector^T\\fundamental\\labelingvector)}{1 + \\jkvector^T\\fundamental\\ivector}\n\t\t= \\objective{\\graph} - \\frac{\\sigma\\tau}{\\rho}\n\t\t= \\objective{\\graph} - \\Delta \\;,  \n\t\\label{eq:fnew=fold-delta}\n\\end{align}\nwith \n$\\sigma = \\onevector^T\\fundamental \\ivector$, \n$\\tau = \\jkvector^T\\fundamental\\labelingvector$,\n$\\rho = 1 + \\jkvector^T\\fundamental\\ivector$,\nand \n\\begin{align}\\label{eq:delta}\n\t\\Delta = \\maxobjective{\\graph,\\graph'} = \\frac{\\sigma\\tau}{\\rho} = \\frac{(\\onevector^T\\fundamental \\ivector)(\\jkvector^T\\fundamental\\labelingvector)}{1 + \\jkvector^T\\fundamental\\ivector} \\;.\n\\end{align}\n\n\n\nThe interpretation of the above quantities is as follows:\n$\\sigma$~is the $\\probability{ij}$-scaled $i$-th column sum of $\\fundamental$ (expected number of visits to $i$),\n$\\tau$~is the cost-scaled sum of the differences between the $j$-th row and the $k$-th row of $\\fundamental$ (expected number of visits from $j$ resp. $k$), \nand $\\rho$~is a normalization factor scaling the update by $1$ plus the $\\probability{ij}$-scaled difference in the expected number of visits from $j$ to $i$ and from $k$ to $i$, \nensuring that $\\fundamental'\\onevector = \\fundamental\\onevector$.\nScrutinizing \\cref{eq:delta}, we observe:\n\n\\begin{proof}\n\tWe obtain this result by analyzing the definitions of $\\rho$, $\\sigma$, and $\\tau$. \n\tThe full proof is given in \\cref{apx:hardness:deltaanalysis}.\n\\end{proof}\n\nTo express when we can safely prune rewiring candidates,\nwe call a rewiring $\\rewiring{i}{j}{k}$ \\emph{greedily permissible} if $\\Delta > 0$, \ni.e., if it reduces our objective, \nand \\emph{greedily optimal} if it maximizes $\\Delta$. \nFor \\ourproblemtwo, we further call a rewiring $\\rewiring{i}{j}{k}$ \\emph{greedily $\\qualitythreshold$-permissible} if it ensures that $\\quality{\\outseq{i}} \\geq \\qualitythreshold$ under the given relevance function $\\qualityf$.\nWith this terminology, \nwe can confirm our intuition about rewirings \nas a corollary of \\cref{eq:fnew=fold-delta,eq:delta}, \ncombined with \\cref{lemma:signs}.\n\\begin{cor}\n\tA rewiring $\\rewiring{i}{j}{k}$ is greedily permissible if and only if $\\tau > 0$, i.e., \n\tif $j$ is more exposed to harm than $k$.\n\\end{cor}\n \nFor the greedily optimal rewiring, \nthat is, to maximize $\\Delta$, \nwe would like $\\sigma\\tau$ to be as \\emph{large} as possible, \nand $\\rho$ to be as \\emph{small} as possible.\nInspecting \\cref{eq:delta},\nwe find that to accomplish this objective, it helps if (in expectation)\n$i$~is visited more often (from~$\\sigma$),\n$j$~is more exposed and $k$ is less exposed to harm (from~$\\tau$),\nand $i$~is harder to reach from $j$ and easier to reach from $k$ (from~$\\rho$).\n\nIn the next section, we leverage these insights\nto guide our efficient implementation of the greedy method\nfor \\ourproblem and \\ourproblemtwo. \n\n% vim:spell spelllang=en_us\n%!TEX root = ../main.tex\n"
            },
            "section 4": {
                "name": "Algorithm",
                "content": "\n\\label{sec:algorithm}\n\nIn the previous section, we identified useful structure\nin the fundamental matrix $\\fundamental$, the exposure function $\\objectivef$, and our maximization objective \\maxobjectivef.\nNow, we leverage this structure to design an efficient greedy algorithm\nfor \\ourproblem and \\ourproblemtwo.\nWe develop this algorithm in three steps, \nfocusing on \\ourproblem in the first two steps, \nand integrating the capability to handle \\ourproblemtwo in the third step.\n\n\\paragraph{Na\\\"ive implementation}\n\nGiven a graph $\\graph$, its transition matrix~$\\transitionmatrix$, a cost vector $\\labelingvector$,\nand a budget $\\budget$,\na na\\\"ive greedy implementation for \\ourproblem \ncomputes the fundamental matrix \nand gradually fills up an initially empty set of rewirings \nby performing $\\budget$ greedy steps \nbefore returning the selected rewirings (\\cref{apx:pseudocode}, \\cref{alg:naivegreedy}). \nIn each greedy step, \nwe identify the triple $\\rewiring{i}{j}{k}$ that maximizes \\cref{eq:delta} \nby going through all edges $\\edge{i}{j}\\in\\edges$ \nand computing $\\Delta$ for rewirings to all potential targets $k$. \nWe then update $\\edges$, $\\transitionmatrix$, and $\\fundamental$ to reflect a rewiring replacing $\\edge{i}{j}$ by $\\edge{i}{k}$ (cf.~\\cref{tab:rewiring}), \nand add the triple $\\rewiring{i}{j}{k}$ to our set of rewirings.\n%\nComputing the fundamental matrix na\\\"ively takes time $\\bigoh{\\nnodes^3}$,\ncomputing $\\Delta$ takes time $\\bigoh{\\nnodes}$ and is done $\\bigoh{\\nedges\\nnodes}$ times, and \nupdating $\\fundamental$ takes time $\\bigoh{\\nnodes^2}$.\nHence, we arrive at a time complexity of $\\bigoh{\\budget\\nnodes^2(\\nnodes+\\nedges)}$.\nBut we can do better.\n\n\\paragraph{Forgoing matrix inversion}\n\nWhen identifying the greedy rewiring, \nwe never need access to $\\fundamental$ directly.\nRather, in \\cref{eq:delta}, we work with $\\onevector^T\\fundamental$, \ncorresponding to the column sums of $\\fundamental$, \nand with $\\fundamental\\labelingvector$, \ncorresponding to the cost-scaled row sums of $\\fundamental$.\nWe can approximate both via power iteration:\n\\begin{align}\n\t\\onevector^T\\fundamental \n\t=~& \\onevector^T\\sum_{i=0}^{\\infty}\\transitionmatrix^i \n\t= \\onevector^T \n\t+ \\onevector^T\\transitionmatrix \n\t+  (\\onevector^T\\transitionmatrix)\\transitionmatrix \n\t+ ((\\onevector^T\\transitionmatrix)\\transitionmatrix)\\transitionmatrix + \\dots\\\\\n\t\\fundamental\\labelingvector \n\t=~& \\left(\\sum_{i=0}^{\\infty}\\transitionmatrix^i\\right)\\labelingvector\n\t= \\labelingvector + \\transitionmatrix\\labelingvector + \\transitionmatrix(\\transitionmatrix\\labelingvector) + \\transitionmatrix(\\transitionmatrix(\\transitionmatrix\\labelingvector)) + \\dots\n\\end{align}\nFor each term in these sums, we need to perform $\\bigoh{\\nedges}$ multiplications, \nsuch that we can compute $\\onevector^T\\fundamental$ and $\\fundamental\\labelingvector$ in time $\\bigoh{\\niter\\nedges}$, \nwhere $\\niter$ is the number of power iterations.\nThis allows us to compute \n$\\onevector^T\\fundamental\\ivector$ for all $\\edge{i}{j}\\in\\edges$ in time $\\bigoh{\\nedges}$ and $\\jkvector^T\\fundamental\\labelingvector$ for all $j\\neq k\\in\\nodes$ in time $\\bigoh{\\nnodes^2}$.\nTo compute $\\Delta$ in time $\\bigoh{1}$,\nas $\\fundamental$ is now unknown, \nwe need to compute $\\fundamental\\ivector$ for all $(i,j)\\in\\edges$ via power iteration, \nwhich is doable in time $\\bigoh{\\niter\\nnodes^2}$.\nThis changes the running time from $\\bigoh{\\budget\\nnodes^2(\\nnodes+\\nedges)}$ to $\\bigoh{\\budget\\niter\\nnodes(\\nnodes+\\nedges)}$ (\\cref{apx:pseudocode}, \\cref{alg:greedy}).\nBut we can do better.\n\n\\paragraph{Reducing the number of candidate rewirings}\nObserve that to further improve the time complexity of our algorithm,\nwe need to reduce the number of rewiring candidates considered. \nTo this end, note that the quantity $\\tau$ is maximized for the nodes $j$ and $k$ with the largest difference in cost-scaled row sums.\nHow exactly we leverage this fact depends on our problem.\n\nIf we solve \\ourproblem, \ninstead of considering all possible rewiring targets, \nwe focus on the $\\maxoutdegree+2$ candidate targets $\\kcandidates$ with the smallest exposure, \nwhich we can identify in time $\\bigoh{\\nnodes}$ without sorting $\\fundamental\\labelingvector$.\nThis ensures that for each $\\edge{i}{j}\\in\\edges$, \nthere is at least one $k\\in\\kcandidates$ such that $k\\neq i$ and $k\\neq j$, \nwhich ascertains that despite restricting to $\\kcandidates$, \nfor each $i\\in\\nodes$, \nwe still consider the rewiring $\\rewiring{i}{j}{k}$ maximizing $\\tau$.\nWith this modification, we reduce the number of candidate targets from $\\bigoh{\\nnodes}$ to $\\bigoh{\\maxoutdegree}$\nand the time to compute all relevant $\\jkvector^T\\fundamental\\labelingvector$ values from $\\bigoh{\\nnodes^2}$ to $\\bigoh{\\maxoutdegree\\nnodes}$.\n%\nTo obtain a subquadratic complexity, however, we still need to eliminate the computation of $\\fundamental\\ivector$ for all $(i,j)\\in\\edges$.\nThis also means that we can no longer afford to compute~$\\rho$ for each of the now $\\bigoh{\\nedges\\maxoutdegree}$ rewiring candidates under consideration, \nas this can only be done in constant time if $\\fundamental\\ivector$ is already precomputed for the relevant edge $\\edge{i}{j}$.\nHowever, $\\rho$ is driven by the difference between two \\emph{entries} of $\\fundamental$, \nwhereas $\\tau$ is driven by the difference between two \\emph{row sums} of $\\fundamental$, \nand $\\sigma$ is driven by a single \\emph{column sum} of $\\fundamental$. \nThus, although $\\sigma\\tau > \\sigma\\tau'$ does not generally imply $\\nicefrac{\\sigma\\tau}{\\rho} > \\nicefrac{\\sigma\\tau'}{\\rho'}$,\nthe variation in $\\sigma\\tau$ is typically much larger than that in $\\rho$, \nand large $\\sigma\\tau$ values mostly dominate small values of $\\rho$.\nConsequently, as demonstrated in \\cref{apx:exp:heuristic}, \nthe correlation between $\\heuristic = \\Delta\\rho = \\sigma\\tau$ and  $\\Delta = \\nicefrac{\\sigma\\tau}{\\rho}$ is almost perfect.\nThus, instead of $\\Delta$, we opt to compute $\\heuristic$ as a heuristic, \nand we further hedge against small fluctuations without increasing the time complexity of our algorithm by computing $\\Delta$ \nfor the rewirings associated with the $\\bigoh{1}$ \\emph{largest} values of $\\heuristic$,\nrather than selecting the rewiring with the \\emph{best} $\\heuristic$ value directly. \nUsing $\\heuristic$ instead of $\\Delta$, \nwe obtain a running time of $\\bigoh{\\budget\\niter\\maxoutdegree(\\nnodes+\\nedges)}$ when solving \\ourproblem.\n\nWhen solving \\ourproblemtwo, \nwe are given a relevance matrix $\\relevancematrix$, \na relevance function $\\qualityf$,\nand a relevance threshold $\\qualitythreshold$ as additional inputs. \nInstead of considering the $\\maxoutdegree+2$ nodes $\\kcandidates$ with the smallest exposure as candidate targets for \\emph{all} edges, \nfor \\emph{each} edge $\\edge{i}{j}$, \nwe first identify the set of rewiring candidates $\\rewiring{i}{j}{k}$ \nsuch that $\\rewiring{i}{j}{k}$ is $\\qualitythreshold$-permissible, \ni.e., $\\quality{\\outseq{i}} \\geq \\qualitythreshold$ after replacing $\\edge{i}{j}$ by $\\edge{i}{k}$,\nand then select the node $k_{ij}$ with the smallest exposure to construct our most promising rewiring candidate $\\rewiring{i}{j}{k_{ij}}$ for edge $\\edge{i}{j}$.\nThis ensures that we can still identify the rewiring $\\rewiring{i}{j}{k}$ that maximizes $\\sigma\\tau$ \\emph{and} satisfies our quality constraints,\nand it leaves us to consider $\\bigoh{\\nedges}$ rewiring candidates. \nAgain using $\\heuristic$ instead of  $\\Delta$, \nwe can now solve \\ourproblemtwo in time \n$\\bigoh{\\budget\\niter\\nqpermissible\\qualitycomplexity\\nedges+\\qualitycomplexitystart}$, \nwhere \n$\\nqpermissible$ is the maximum number of targets $k$ such that $\\rewiring{i}{j}{k}$ is $\\qualitythreshold$-permissible,\n$\\qualitycomplexity$ is the complexity of evaluating $\\qualityf$, \nand $\\qualitycomplexitystart$ is the complexity of determining the initial set $\\qualityset$ of $\\qualitythreshold$-permissible rewirings.\n\n\\smallskip\nThus, we have arrived at our efficient greedy algorithm, \ncalled \\ourmethod (\\textsc{G}reedy \\textsc{a}pproximate \\textsc{min}imi\\-zation of \\textsc{e}xposure),\nwhose pseudocode we state as \n\\cref{alg:greedyrelevant:rem,alg:greedyrelevant} in \\cref{apx:pseudocode}.\n\\ourmethod solves \\ourproblem in time $\\bigoh{\\budget\\niter\\maxoutdegree(\\nnodes+\\nedges)}$\nand \\ourproblemtwo in time $\\bigoh{\\budget\\niter\\nqpermissible\\qualitycomplexity\\nedges+\\qualitycomplexitystart}$.\nIn realistic recommendation settings, \nthe graph $\\graph$ is $\\outregulardegree$-out-regular for $\\outregulardegree\\in\\bigoh{1}$, \nsuch that\n$\\maxoutdegree\\in\\bigoh{1}$ and $\\nedges=\\outregulardegree\\nnodes\\in\\bigoh{\\nnodes}$.\nFurther, for \\ourproblemtwo,\nwe can expect that $\\qualityf$ is evaluable in time $\\bigoh{1}$,\nand that only the $\\bigoh{1}$ nodes most relevant for $i$ will be considered as potential rewiring targets of any edge $\\edge{i}{j}$, \nsuch that $\\nqpermissible\\in\\bigoh{1}$ and $\\qualitycomplexitystart \\in \\bigoh{\\nedges} = \\bigoh{\\nnodes}$. \nAs we can also safely work with a number of power iterations $\\niter\\in\\bigoh{1}$ (\\cref{apx:reproducibility:poweriteration}), \nin realistic settings, \n\\ourmethod solves both \\ourproblem and \\ourproblemtwo in time $\\bigoh{\\budget\\nnodes}$, \nwhich, for $\\budget\\in\\bigoh{1}$, is linear in the order of the input graph~$\\graph$.\n\n% vim:spell spelllang=en_us\n%!TEX root = ../main.tex\n"
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n\\label{sec:related}\n\nOur work methodically relates to research on \\emph{graph edits} with distinct goals, \nsuch as improving robustness, reducing distances, or increasing centralities \\cite{chan2016optimizing,parotsidis2015selecting,medya2018group},\nand research leveraging \\emph{random walks} to rank nodes \\cite{wkas2019random,oettershagen2022temporal,mavroforakis2015absorbing}\nor recommend links \\cite{yin2010unified,paudel2021random}.\nThe agenda of our work, however, \naligns most closely with the literature studying harm reduction, bias mitigation, and conflict prevention in graphs.\nHere, the large body of research on shaping opinions or mitigating negative phenomena\nin \\emph{graphs of user interactions} (especially on social media)\n\\cite{gionis2013opinion,das2014modeling,abebe2021opinion,amelkin2019fighting,garimella2017reducing,garimella2018quantifying,tsioutsiouliklis2022link,zhu2021minimizing,zhu2022nearly,vendeville2023opening,minici2022cascade}\npursues goals \\emph{similar} to ours in graphs capturing \\emph{different} digital contexts.\n\nAs our research is motivated by recent work demonstrating how recommendations on digital media platforms like YouTube can fuel radicalization \\cite{ribeiro2020auditing,mamie2021anti}, \nthe comparatively scarce literature on harm reduction in \\emph{graphs of content items} is even more closely related.\nOur contribution is inspired by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}, \nwho study how edge rewiring can reduce \\emph{radicalization pathways} in recommendation graphs. \n\\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring} encode harmfulness in binary node labels, \nmodel benign nodes as absorbing states, \nand aim to minimize the \\emph{maximum} \\emph{segregation} of any node, \ndefined as the largest expected length of a random walk starting at a harmful node before it visits a benign node.\nIn contrast, we encode harmfulness in more nuanced, real-valued node attributes,\nuse an absorbing Markov chain model that more naturally reflects user behavior, \nand aim to minimize the \\emph{expected} \\emph{total} \\emph{exposure} to harm in random walks  starting at any node. \nThus, our work not only eliminates several limitations of the work by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}, \nbut it also provides a different perspective on harm mitigation in recommendation graphs. \n\nWhile \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}, \nlike us, \nconsider \\emph{recommendation graphs}, \n\\citeauthor{haddadan2022reducing} \\cite{haddadan2022reducing}\nfocus on polarization mitigation via \\emph{edge insertions}. \nTheir setting was recently reconsidered by \\citeauthor{adriaens2023minimizing}~\\cite{adriaens2023minimizing}, \nwho tackle the minimization objective directly instead of using the maximization objective as a proxy,\nproviding approximation bounds as well as speed-ups for the standard greedy method. \nBoth \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring} and the works on edge insertion employ with random-walk objectives \nthat---%\nunlike our exposure function---%\ndo not depend on random walks starting from \\emph{all} nodes.\nIn our experiments, \nwe compare with the algorithm introduced by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}, which we call \\fabbrialg. \nWe refrain from comparing with edge insertion strategies because they consider a different graph edit operation and are already outperformed by \\fabbrialg.\n\n% vim:spell spelllang=en_us\n%!TEX root = ../main.tex\n"
            },
            "section 6": {
                "name": "Experimental Evaluation",
                "content": "\n\\label{sec:experiments}\n\nIn our experiments, we seek to \n\\begin{enumerate}\n\t\\item establish the impact of modeling choices and input parameters\n\ton the performance of \\ourmethod;\n\t\\item demonstrate the effectiveness of \\ourmethod in reducing exposure to harm compared to existing methods and baselines;\n\t\\item ensure that \\ourmethod is scalable in theory \\emph{and practice}; \n\t\\item understand what features make reducing exposure to harm easier resp. harder on different datasets; and\n\t\\item derive general guidelines for reducing exposure to harm in recommendation graphs under budget constraints.\n\\end{enumerate}\nFurther experimental results are provided in \\cref{apx:experiments}.\n\n",
                "subsection 6.1": {
                    "name": "Setup",
                    "content": "\n\n",
                    "subsubsection 6.1.1": {
                        "name": "Datasets",
                        "content": "\nTo achieve our experimental goals, \nwe work with both synthetic and real-world data, \nas summarized in \\cref{tab:data}. \nBelow, we briefly introduce these datasets.\nFurther details, \nincluding on data generation and preprocessing, are provided in \\cref{apx:datasets}.\n\n\n\n\\paragraph{Synthetic data}\nAs our synthetic data, \nwe generate a total of $288$ synthetic graphs of four different sizes using two different edge placement models and various parametrizations.\nThe first model, \\synuni, \nchooses out-edges \\emph{uniformly} at random, \nsimilar to a directed Erd\\H{o}s-R\\'enyi model~\\cite{erdHos1959random}. \nIn contrast, the second model, \\synhom, \nchooses edges \\emph{preferentially} to favor small distances between the costs of the source and the target node, \nimplementing the concept of \\emph{homophily} \\cite{mcpherson2001birds}.\nWe use these graphs primarily to analyze the behavior of our objective function, \nand to understand the impact of using $\\heuristic$ instead of $\\Delta$ to select the greedily optimal rewiring (\\cref{apx:exp:heuristic}).\n\n\\paragraph{Real-world data}\nWe work with real-world data from two domains, \nvideo recommendations (\\yt) and news feeds (\\nf).\nFor our \\emph{video application}, \nwe use the YouTube data by \\citeauthor{ribeiro2020auditing} \\cite{mamie2021anti,ribeiro2020auditing}, \nwhich contains identifiers and ``Up Next''-recommendations for videos from selected channels categorized \nto reflect different degrees and directions of radicalization. \nFor our \\emph{news application}, \nwe use subsets of the NELA-GT-2021 dataset \\cite{gruppi2020nelagt2021}, \nwhich contains 1.8 million news articles published in 2021 from 367 outlets, \nalong with veracity labels from Media Bias/Fact Check. \nPrior versions of both datasets are used in the experiments reported by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}. \n\n\\paragraph{Parametrizations}\nTo comprehensively assess the effect of modeling assumptions regarding the input graph and its associated random-walk process on our measure of exposure as well as on the performance of \\ourmethod and its competitors, \nwe experiment with a variety of parametrizations expressing these assumptions.\nFor all datasets, \nwe distinguish three random-walk absorption probabilities $\\alpha\\in\\{0.05,0.1,0.2\\}$ \nand two probability shapes $\\probabilityshape\\in\\{\\mathbf{U},\\mathbf{S}\\}$ over the  out-edges of each node (\\textbf{U}ni\\-form and \\textbf{S}kewed).\nFor our synthetic datasets, \nwe further experiment with three fractions of latently harmful nodes $\\fractionbad\\in\\{0.3,0.5,0.7\\}$ and \ntwo cost functions $\\labeling \\in \\{\\labeling_{B},\\labeling_{R}\\}$, \none binary and one real-valued.\nLastly, for our real-world datasets,   \nwe distinguish three regular out-degrees $\\outregulardegree\\in\\{5,10,20\\}$,\nfive quality thresholds $\\qualitythreshold\\in\\{0.0,0.5,0.9,0.95,0.99\\}$ \nand four cost functions,\ntwo binary ($\\labeling_B1$, $\\labeling_B2$) and two real-valued ($\\labeling_{R1}, \\labeling_{R2}$),\nbased on labels provided with the original datasets, \nas detailed in \\cref{apx:data:costfunctions}.\n\n"
                    },
                    "subsubsection 6.1.2": {
                        "name": "Algorithms",
                        "content": "\n\\label{exp:alg}\nWe compare \\ourmethod, our algorithm for \\ourproblem and \\ourproblemtwo, \nwith four baselines (\\baselineone-\\baselinefour) \nand the algorithm by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring} for minimizing the maximum segregation, which we call \\fabbrialg. \nIn all \\ourproblemtwo experiments, \nwe use the $\\bigoh{1}$-computable normalized discounted cumulative gain (\\ndcg),\ndefined in \\cref{apx:reproducibility:qualityf} and also used by \\fabbrialg,\nas a relevance function $\\qualityf$, \nand consider the $100$ most relevant nodes as potential rewiring targets.\n\nAs \\fabbrialg can only handle binary costs, \nwe transform nonbinary costs $\\labeling$ into binary costs $\\labeling'$ by thresholding to ensure $\\labeling_i \\geq \\roundingthreshold \\Leftrightarrow \\labeling'_i = 1$ for some rounding threshold $\\roundingthreshold\\in(0,1]$ \n(cf. \\cref{apx:reproducibility:binarization}).\nSince \\fabbrialg requires access to relevance information, \nwe restrict our comparisons with \\fabbrialg to data where this information is available.\n\nOur baselines \\baselineone-\\baselinefour are ablations of \\ourmethod, \nsuch that outperforming them shows how each component of our approach is beneficial. \nWe order the baselines by the competition we expect from them, \nfrom no competition at all (\\baselineone) to strong competition (\\baselinefour). \nIntuitively, \\baselineone does not consider our objective at all,\n\\baselinetwo is a heuristic focusing on the $\\tau$ component of our objective, \n\\baselinethree is a heuristic focusing on the $\\sigma$ component of our objective, \nand \\baselinefour is a heuristic eliminating the iterative element of our approach. \n\\baselineone--\\baselinethree each run in $\\budget$ rounds, while \\baselinefour runs in one round. \nIn each round,\n\\baselineone randomly selects a permissible rewiring via rejection sampling.\n\\baselinetwo selects the rewiring $(i,j,k)$ with the node $j$ maximizing $\\unitvector_j^T\\fundamental\\labelingvector$ as its old target, \nthe node $i$ with $j\\in\\outneighbors{i}$ maximizing $\\onevector^T\\fundamental\\unitvector_i$ as its source,\nand the available node $k$ minimizing $\\unitvector_k^T\\fundamental\\labelingvector$ as its new target. \n\\baselinethree selects the rewiring $(i,j,k)$ with the node $i$ maximizing $\\onevector^T\\fundamental\\unitvector_i$ as its source, \nthe node $j$ with $j\\in\\outneighbors{i}$ maximizing $\\unitvector_j^T\\fundamental\\labelingvector$ as its old target, and the available node $k$ minimizing $\\unitvector_k^T\\fundamental\\labelingvector$ as its new target.\n\\baselinefour selects the $\\budget$ rewirings with the largest initial values of $\\heuristic$, \nwhile ensuring each edge is rewired at most once.\n\n"
                    },
                    "subsubsection 6.1.3": {
                        "name": "Implementation and reproducibility",
                        "content": "\nAll algorithms, including \\ourmethod, the baselines, and \\fabbrialg, \nare implemented in Python 3.10.\nWe run our experiments on a 2.9 GHz 6-Core Intel Core i9 with 32 GB RAM and report wall-clock time.\nAll code, datasets, and results are publicly available,\\!\\footnote{\\oururl}\nand we provide further reproducibility information in \\cref{apx:reproducibility}.\n\n"
                    }
                },
                "subsection 6.2": {
                    "name": "Results",
                    "content": "\n\n",
                    "subsubsection 6.2.1": {
                        "name": "Impact of modeling choices",
                        "content": "\nTo understand the impact of a particular modeling choice on the performance of \\ourmethod and its competitors, \nwe analyze groups of experimental settings that vary only the parameter of interest while keeping the other parameters constant,\nfocusing on the \\ytone datasets. \nWe primarily report the evolution of the ratio $\\nicefrac{\\objective{\\graph_{\\budget}}}{\\objective{\\graph}}=\\nicefrac{\\big(\\objective{\\graph}-\\maxobjective{\\graph,\\graph_{\\budget}}\\big)}{\\objective{\\graph}}$, \nwhich indicates what fraction of the initial expected total exposure is left after $\\budget$ rewirings, \nand hence is comparable across \\ourproblem instances with different starting values.\nOverall, we observe that \\ourmethod robustly reduces the expected total exposure to harm, \nand that it changes its behavior predictably under parameter variations. \nDue to space constraints, we defer the results showing this for variations in the regular out-degree $\\outregulardegree$, \nthe random-walk absorption probability $\\pabsorption$, \nthe probability shape $\\probabilityshape$,\nand the cost function $\\labeling$ \nto \\cref{apx:exp:modelingchoices}.\n\n\\paragraph{Impact of quality threshold $\\qualitythreshold$}\nThe higher the quality threshold $\\qualitythreshold$, \nthe more constrained our rewiring options. \nThus, under a given budget $\\budget$, \nwe expect \\ourmethod to reduce our objective more strongly for smaller $\\qualitythreshold$. \nAs illustrated in \\cref{fig:quality_threshold}, \nour experiments confirm this intuition, \nand the effect is more pronounced if the out-edge probability distribution is skewed.\nWe further observe that \\ourmethod can guarantee $\\qualitythreshold = 0.5$ with little performance impact, \nand it can strongly reduce the exposure to harm even under a strict $\\qualitythreshold = 0.95$:\nWith just $100$ edge rewirings, \nit reduces the expected total exposure to harm by $50\\%$, \nwhile ensuring that its recommendations are at most $5\\%$ less relevant than the original recommendations.\n\n"
                    },
                    "subsubsection 6.2.2": {
                        "name": "Performance comparisons",
                        "content": "\nHaving ensured that \\ourmethod robustly and predictably reduces the total exposure across the entire spectrum of modeling choices, \nwe now compare it with its competitors.\nOverall, we find that \\ourmethod offers more reliable performance and achieves stronger harm reduction than its contenders.\n\n\\paragraph{Comparison with baselines \\baselineone--\\baselinefour}\nFirst, we compare \\ourmethod with our four baselines, \neach representing a different ablation of our algorithm.  \nAs depicted in \\cref{fig:baselines}, \nthe general pattern we observe matches our performance expectations (from weak performance of \\baselineone to strong performance of \\baselinefour), \nbut we are struck by the strong performance of \\baselinethree \n(selecting based on $\\sigma$), \nespecially in contrast to the weak performance of \\baselinetwo \n(selecting based on $\\tau$). \nThis suggests that whereas the most \\emph{exposed} node does not necessarily have a highly visited node as an in-neighbor, \nthe most \\emph{visited} node tends to have a highly exposed node as an out-neighbor.\nIn other words, for some highly \\emph{prominent} videos, \nthe YouTube algorithm\nproblematically\nappears to recommend highly \\emph{harm-inducing} content to watch next. \nDespite the competitive performance of \\baselinethree and \\baselinefour,\n\\ourmethod consistently outperforms these baselines, too, \nand unlike the baselines, it \\emph{smoothly} reduces the exposure function.\nThis lends additional support to our reliance on $\\sigma\\tau$ (rewiring a highly visited $i$ away from a highly exposed $j$) as an \\emph{iteratively} evaluated heuristic.\n\n\\paragraph{Comparison with \\fabbrialg}\nHaving established that all components of \\ourmethod are needed to achieve its performance, \nwe now compare our algorithm with \\fabbrialg, the method proposed by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}. \nTo this end, we run both \\ourmethod and \\fabbrialg using their respective objective functions,\ni.e., the expected total exposure to harm of random walks starting at any node (\\emph{total exposure}, \\ourmethod)\nand the maximum expected number of random-walk steps from a harmful node to a benign node (\\emph{maximum segregation}, \\fabbrialg).\nReporting their performance under the objectives of \\emph{both} algorithms (as well as the \\emph{total segregation}, which sums the segregation scores of all harmful nodes) in \\cref{fig:gamine_vs_mms}, \nwe find that under strict quality control ($\\qualitythreshold\\in\\{0.9,0.95,0.99\\}$), \n\\ourmethod outperforms \\fabbrialg on \\emph{all} objectives, \nand \\fabbrialg stops early as it can no longer reduce its objective function.\nFor $\\qualitythreshold = 0.5$, \\fabbrialg outperforms \\ourmethod on the segregation-based objectives, \nbut \\ourmethod still outperforms \\fabbrialg on our exposure-based objective, sometimes at twice the margin (\\cref{fig:largemargin}).\nFurther, while \\ourmethod delivers consistent and predictable performance that is strong on exposure-based and segregation-based objectives, \nwe observe much less consistency in the performance of \\fabbrialg.\nFor example, it is counterintuitive that \\fabbrialg identifies $100$ rewirings on the smaller \\ytone data but stops early on the larger \\yttwo data. \nMoreover, \\fabbrialg delivers the results shown in \\cref{fig:gamine_vs_mms} under $\\labeling_{B1}$, \nbut it cannot decrease its objective at all on the same data under $\\labeling_{B2}$, \nwhich differs from $\\labeling_{B1}$ only in that it also assigns harm to anti-feminist content (\\cref{apx:datasets}, \\cref{tab:yt-costs}).\nWe attribute this brittleness to the reliance on the \\emph{maximum}-based \\emph{segregation} objective, \nwhich, by design, \nis less robust than our \\emph{sum}-based \\emph{exposure} objective.\n\n%\n\n%\n\n%\n\n%\n\n\n"
                    },
                    "subsubsection 6.2.3": {
                        "name": "Empirical scalability of \\ourmethod",
                        "content": "\n\nIn our previous experiments, we found that \\ourmethod robustly and reliably reduces the expected total exposure to harm. \nNow, we seek to ascertain that its practical scaling behavior matches our theoretical predictions, \ni.e., that under realistic assumptions on the input, \n\\ourmethod scales linearly in $\\nnodes$ and $\\nedges$.\nWe are also interested in comparing \\ourmethod's scalability to that of \\fabbrialg.\nTo this end, we measure the time taken to compute a single rewiring and report, \nin \\cref{fig:scalability},\nthe \\emph{average} over ten\nrewirings for each of our datasets.\nThis corresponds to the time taken by \\greedyname in \\ourmethod and by \\textsc{1-Rewiring} in \\fabbrialg, \nwhich drives the overall scaling behavior of both algorithms. \nWe find that \\ourmethod scales approximately linearly, \nwhereas \\fabbrialg scales approximately quadratically \n(contrasting with the empirical time complexity of $\\bigoh{\\nnodes\\log\\nnodes}$ claimed in \\cite{fabbri2022rewiring}).\nThis is because our implementation of \\fabbrialg follows the original authors', \nwhose evaluation of the segregation objective takes time  $\\bigoh{\\nnodes}$ and is performed $\\bigoh{\\nedges}$ times. \nThe speed of precomputations depends on the problem variant (\\ourproblem vs. \\ourproblemtwo), \nand for \\ourproblemtwo, also on the quality function $\\qualityf$. \nIn our experiments, precomputations add linear overhead for \\ourmethod and volatile overhead for \\fabbrialg, \nas we report in \\cref{apx:exp:scalability}.\n\n\n"
                    },
                    "subsubsection 6.2.4": {
                        "name": "Data complexity",
                        "content": "\nGiven that \\ourmethod strongly reduces the expected total exposure to harm with few rewirings on the YouTube data, \nas evidenced in \\cref{fig:quality_threshold,fig:baselines,fig:gamine_vs_mms}, \none might be surprised to learn that its performance seems much weaker on the NELA-GT data (\\cref{apx:exp:nela}): \nWhile it still reduces the expected total exposure and outperforms \\fabbrialg (which struggles to reduce its objective at all on the \\nf data), \nthe impact of individual rewirings is much smaller than on the YouTube datasets, \nand the value of the quality threshold $\\qualitythreshold$ barely makes a difference. \nThis motivates us to investigate how \\emph{data complexity} impacts our ability to reduce the expected total exposure to harm via edge rewiring: \nCould reducing exposure to harm be \\emph{intrinsically} harder on \\nf data than on \\yt data? \nThe answer is yes. \nFirst, \nthe in-degree distributions of the \\yt graphs are an order of magnitude more skewed than those of the \\nf graphs (\\cref{apx:datasets:statistics}, \\cref{fig:real-indegrees}). \nThis is unsurprising given the different origins of their edges (user interactions vs. cosine similarities), \nbut it creates opportunities for high-impact rewirings involving highly prominent nodes in \\yt graphs (which \\ourmethod seizes in practice, see below).\nSecond, as depicted in \\cref{fig:channelclasses},\nharmful and benign nodes are much more strongly interwoven in the \\nf data than in the \\yt data. \nThis means that harmful content is less siloed in the \\nf graphs, \nbut it also impedes strong reductions of the expected total exposure. \nThird, \nas a result of the two previous properties, \nthe initial node exposures are much more concentrated in the \\nf graphs than in the \\yt graphs, \nas illustrated in \\cref{fig:initialexposure}, \nwith a median sometimes twice as large as the median of the identically parametrized \\yt graphs, \nand a much higher average exposure (cf.~$\\nicefrac{\\objective{\\graph}}{\\nnodes}$ in \\cref{tab:data}). \nFinally, the relevance scores are much more skewed in the \\yt data than in the \\nf data (\\cref{apx:datasets:statistics}, \\cref{fig:relevancescores}). \nHence, while we are strongly constrained by $\\qualitythreshold$ on the \\yt data even when considering only the $100$ highest-ranked nodes as potential rewiring targets, \nwe are almost unconstrained in the same setting on the \\nf data, \nwhich explains the comparative irrelevance of $\\qualitythreshold$ on the \\nf data.\nThus, the performance differences we observe between the \\nf data and the \\yt data are due to intrinsic dataset properties: \n\\ourproblem and \\ourproblemtwo are simply more complex on the news data than on the video data.\n\n\n\n%\n\n%\n\n\n"
                    },
                    "subsubsection 6.2.5": {
                        "name": "General guidelines",
                        "content": "\nFinally, we would like to abstract the findings from our experiments into general guidelines for reducing exposure to harm in recommendation graphs, \nespecially under quality constraints.\nTo this end, we analyze the metadata associated with our rewirings. \nIn particular, for each set of rewirings $(i,j,k)$ obtained in our experiments, \nwe are interested in the channel resp. news outlet classes involved, \nas well as in the distributions of \ncost triples $(\\labeling_i,\\labeling_j,\\labeling_k)$ \nand in-degree tuples $(\\indegree{i},\\indegree{j})$.\nAs exemplified in \\cref{fig:yt-rewiringchannels-R1}, \nwhile we consistently rewire edges from harmful to benign targets in the quality-unconstrained setting ($\\qualitythreshold=0.0$), \nunder strict quality control ($\\qualitythreshold=0.99$),\nwe frequently see rewirings from harmful to equally or more harmful targets.\nMore generally, as illustrated in \\cref{fig:yt-rewiringtypes-B1},\nthe larger the threshold $\\qualitythreshold$, \nthe more we rewire \\emph{among} harmful, resp.\\ benign, nodes ($\\labeling_i=\\labeling_j=\\labeling_k=1$, resp.\\ $0$)---%\nwhich \\fabbrialg does not even allow.\nFurthermore, the edges we rewire typically connect nodes with large in-degrees (\\cref{apx:experiments:edgestats}, \\cref{fig:indegree-sums}).\nWe conclude that a simplified strategy for reducing exposure to harm under quality constraints \nis to identify edges that connect high-cost nodes with large in-degrees, \nand rewire them to the node with the lowest exposure among all nodes meeting the quality constraints.\n\n% vim:spell spelllang=en_us\n%!TEX root = ../main.tex\n"
                    }
                }
            },
            "section 7": {
                "name": "Discussion and Conclusion",
                "content": "\n\\label{sec:conclusion}\n\nWe studied the problem of reducing the exposure to harmful content in recommendation graphs by edge rewiring. \nModeling this exposure via absorbing random walks, \nwe introduced \\ourproblemtwo and \\ourproblem as formalizations of the problem with and without quality constraints on recommendations. \nWe proved that both problems are \\NP-hard and \\NP-hard to approximate to within an additive error, \nbut that under mild assumptions, \nthe greedy method provides a $(1-\\nicefrac{1}{e})$-approximation for the \\ourproblem problem.\nHence, we introduced \\ourmethod, \na greedy algorithm for \\ourproblem and \\ourproblemtwo running in linear time under realistic assumptions on the input, \nand we confirmed its effectiveness, robustness, and efficiency through extensive experiments on synthetic data \nas well as on real-world data from video recommendation and news feed applications.\n\nOur work improves over the state of the art (\\fabbrialg by \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}) in terms of performance, \nand it eliminates several limitations of prior work.\nWhile \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring} model benign nodes as \\emph{absorbing} states and consider a brittle \\emph{max}-objective that is minimized even by highly harm-exposing recommendation graphs, \nwe model benign nodes as \\emph{transient} states and consider a robust \\emph{sum}-objective that captures the overall consumption of harmful content by users starting at any node in the graph. \nWhereas \\fabbrialg can only handle \\emph{binary} node labels, \n\\ourmethod works with \\emph{real-valued} node attributes, \nwhich permits a more nuanced encoding of harmfulness.\n\nWe see potential for future work in several directions.\nFor example, \nit would be interesting to adapt our objective to mitigate \\emph{polarization}, \ni.e., the separation of content with opposing views, \nwith positions modeled as positive and negative node costs.\nMoreover, we currently assume that all nodes are equally likely as starting points of random walks, \nwhich is unrealistic in many applications. \nFinally, \nwe observe that harm reduction in recommendation graphs has largely been studied in separation from harm reduction in other graphs representing consumption phenomena, \nsuch as user interaction graphs.\nA framework for optimizing functions under budget constraints that includes edge rewirings, insertions, and deletions\ncould unify these research lines and facilitate future progress.\n\n\\vspace*{18pt}\n\n\n\\begin{acks}\nThis research is supported by \nthe Academy of Finland project MLDB (325117), \nthe ERC Advanced Grant REBOUND (834862), \nthe EC H2020 RIA project SoBigData++ (871042), and \nthe Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. \n\\end{acks}\n\n\\clearpage\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{bibliography.bib}\n\n\\appendix\n\n\\pdfbookmark[section]{Ethics Statement}{ethicsmark}\n% vim:spell spelllang=en_us\n%!TEX root = ../main.tex\n"
            },
            "section 8": {
                "name": "Ethics Statement",
                "content": "\n\\label{apx:ethics}\n\nIn this work, we introduce \\ourmethod, \na method to reduce the exposure to harm induced by recommendation algorithms on digital media platforms via edge rewiring, \ni.e., replacing certain recommendations by others. \nWhile removing harm-inducing recommendations constitutes a milder intervention than censoring content directly, \nit still steers attention away from certain content to other content, \nwhich, if pushed to the extreme, can have censorship-like effects.\nAlthough in its intended usage, \n\\ourmethod primarily counteracts the tendency of recommendation algorithms to overexpose harmful content as similar to other harmful content, \nwhen fed with a contrived cost function, \nit could also be used to discriminate against content considered undesirable for problematic reasons \n(e.g., due to political biases or stereotypes against minorities).\nHowever, \nas the changes to recommendations suggested by \\ourmethod could also be made by amending recommendation algorithms directly,\nthe risk of \\emph{intentional} abuse is no greater than that inherent in the recommendation algorithms themselves, \nand \\emph{unintentional} abuse can be prevented by rigorous impact assessments and cost function audits before and during deployment. \nThus, we are confident that overall, \\ourmethod can contribute to the health of digital platforms.\n\\pdfbookmark[section]{Appendix}{appendixmark}\n"
            },
            "section 9": {
                "name": "Appendix",
                "content": "\n\nIn addition to \\cref{tab:notation}, included below,\nthe written appendix to this work contains the following sections:\n\\begin{enumerate}[label=\\Alph*,align=left,labelwidth=7.5pt,leftmargin=15pt]\n\t\\item Omitted proofs\n\t\\item Other graph edits\n\t\\item Omitted pseudocode\n\t\\item Reproducibility information\n\t\\item Dataset information \n\t\\item Further experiments\n\\end{enumerate}\nThis appendix, along with the main paper, \nis available on arXiv and also deposited at the following DOI: \n\\href{https://doi.org/10.5281/zenodo.8002980}{10.5281/zenodo.8002980}. \nTo facilitate reproducibility, \nall code, data, and results are made available at the following DOI: \\oururl.\n\\balance\n\n\n\n\n% vim:spell spelllang=en_us\n%!TEX root = ../main.tex\n\\balance\n\n\\clearpage\n\n"
            },
            "section 10": {
                "name": "\\huge Appendix",
                "content": "\n\nIn this appendix, \nwe present the proofs omitted in the main paper (\\cref{apx:hardness}), \ndiscuss alternative graph edit operations (\\cref{apx:edits}), \nand state the pseudocode for \\ourmethod as well as the algorithms leading up to it (\\cref{apx:pseudocode}).  \nWe also provide further reproducibility information (\\cref{apx:reproducibility}), \nmore details on our datasets (\\cref{apx:datasets}), \nand additional experimental results (\\cref{apx:experiments}).\n\n\n\n\n\\begin{algorithm}[h]\n\t\\input{text/greedyrelevantrem}\n\t\\caption{%\n\t\tHeuristic greedy \\ourproblem with \\ourmethod. \n\t}\\label{alg:greedyrelevant:rem}\n\\end{algorithm}\n\n\\begin{algorithm}[h]\n\t\\input{text/greedyrelevant}\n\t\\caption{%\n\t\tHeuristic greedy \\ourproblemtwo with \\ourmethod. \n\t}\\label{alg:greedyrelevant}\n\\end{algorithm}\n\n\\clearpage\n\n\\begin{algorithm}[h]\n\t\\input{text/naivegreedy}\n\t\\caption{Na\\\"ive greedy \\ourproblem.}\\label{alg:naivegreedy}\n\\end{algorithm}\n\n\\begin{algorithm}[h]\n\t\\input{text/greedy}\n\t\\caption{Exact greedy \\ourproblem.}\\label{alg:greedy}\n\\end{algorithm}\n\n\n\n\n"
            }
        },
        "tables": {
            "thm:hardness": "\\begin{restatable}[\\NP-Hardness of \\ourproblem]{thm}{hardness}\\label{thm:hardness}\n\tThe $\\budget$-rewiring exposure minimization problem is \\NP-hard, even on 3-out-regular input graphs with binary costs $\\labelingvector\\in\\{0,1\\}^{\\nnodes}$.\n\\end{restatable}",
            "thm:apxhardness": "\\begin{restatable}{thm}{apxhardness}\\label{thm:apxhardness}\n\tAssuming the UGC, \n\t\\ourproblem is hard to approximate to within an additive error of both $\\Theta(\\nnodes)$ and $\\Theta(\\budget)$.\n\\end{restatable}",
            "lem:submoddupes": "\\begin{restatable}{lem}{submodularitywithduplicates}\\label{lem:submoddupes}\n\tIf there exists a safe node in $\\graph$ and we allow multi-edges,  \n\tmaximizing $\\maxobjectivef$ is equivalent to maximizing a monotone, submodular set function over $\\edges_{\\unsafenodes\\unsafenodes}$.\n\\end{restatable}",
            "thm:submodularity": "\\begin{restatable}{thm}{submodularity}\\label{thm:submodularity}\n\tIf $\\cardinality{\\safenodes} \\geq \\maxunsafeoutdegree$,\n\tthen maximizing $\\maxobjectivef$ is equivalent to maximizing a monotone and submodular set function over $\\edges_{\\unsafenodes\\unsafenodes}$.\n\\end{restatable}",
            "tab:rewiring": "\\begin{table}[t]\n\t\\centering\n\t\\caption{%\n\t\tSummary of an edge rewiring $\\rewiring{i}{j}{k}$\n\t\tin a~graph $\\graph = (\\nodes,\\edges)$\n\t\twith random-walk transition matrix $\\transitionmatrix$\n\t\tand fundamental matrix $\\fundamental = (\\identity - \\transitionmatrix)^{-1}$.\n\t}\n\t\\label{tab:rewiring}\n\t\\begin{tabular}{p{0.9625\\columnwidth}}\n\t\t\\toprule\n\t\t$\\graph'=(\\nodes,\\edges')$, for $E' = (E \\setminus \\{\\edge{i}{j}\\}) \\cup \\{\\edge{i}{k}\\}$, $\\edge{i}{j}\\in E$, $\\edge{i}{k}\\notin E$\\\\\n\t\t\\midrule\n\t\t$\\transitionmatrix'[x,y] = \\begin{cases}\n\t\t\t0&\\text{if } x = i \\text{ and } y = j\\;,\\\\\n\t\t\t\\transitionmatrix[i,j]&\\text{if } x = i \\text{ and } y = k\\;,\\\\\n\t\t\t\\transitionmatrix[x,y]&\\text{otherwise}\\;.\n\t\t\\end{cases}$\\\\\n\t\t\\midrule\n\t\t$\\fundamental'\n\t\t= \\fundamental - \\frac{\\fundamental\\ivector\\jkvector^T\\fundamental}{1 + \\jkvector^T\\fundamental\\ivector}$, with $\\ivector = \\probability{ij} \\unitvector_i$, $\\jkvector = \\unitvector_j-\\unitvector_k$, cf.~\\cref{eq:sherman}\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}",
            "lem:deltacomponents": "\\begin{restatable}{lem}{deltacomponents}\\label{lem:deltacomponents}\n\t\\label{lemma:signs}\n\tFor a rewiring $(i,j,k)$ represented by $\\ivector$ and $\\jkvector$,\n\t\\begin{enumerate*}[label=(\\roman*)]\n\t\t\\item $\\rho$ is always positive, \n\t\t\\item $\\sigma$ is always positive, and\n\t\t\\item $\\tau$ can have any sign.\n\t\\end{enumerate*}\n\\end{restatable}",
            "lemma:signs": "\\begin{restatable}{lem}{deltacomponents}\\label{lem:deltacomponents}\n\t\\label{lemma:signs}\n\tFor a rewiring $(i,j,k)$ represented by $\\ivector$ and $\\jkvector$,\n\t\\begin{enumerate*}[label=(\\roman*)]\n\t\t\\item $\\rho$ is always positive, \n\t\t\\item $\\sigma$ is always positive, and\n\t\t\\item $\\tau$ can have any sign.\n\t\\end{enumerate*}\n\\end{restatable}",
            "tab:data": "\\begin{table}[t]\n\t\\centering\n\t\\caption{%\n\t\tOverview of the datasets used in our experiments. \n\t\tFor each graph $\\graph$, \n\t\twe report the regular out-degree $\\outregulardegree$, \n\t\tthe number of nodes $\\nnodes$, and the number of edges $\\nedges$, \n\t\tas well as the range of the expected exposure $\\nicefrac{\\objective{\\graph}}{\\nnodes}$ under our various cost functions, edge wirings, and edge transition probabilities.\n\t\tDatasets with identical statistics are pooled in the same row.\n\t}\n\t\\label{tab:data}\n\t\\input{figures/datasets.tex}\n\\end{table}",
            "tab:notation": "\\begin{table*}[t]\t\n\t\\centering\\small\n\t\\caption{Most important notation used in this work.}\\label{tab:notation}\\pdfbookmark[section]{Notation}{notationmark}\n\t\\input{figures/notation}\\vspace*{-20em}\n\\end{table*}"
        },
        "figures": {
            "fig:setting": "\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\input{figures/badobjective.tex}\n\t\t\\vspace*{3pt}\n\t\t\\subcaption{Minimizing \\emph{segregation}}\n\t\t\\label{fig:setting:bad}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\input{figures/ourobjective.tex}\n\t\t\\vspace*{3pt}\n\t\t\\subcaption{Minimizing \\emph{exposure}}\n\t\t\\label{fig:setting:good}\n\t\\end{subfigure}\n\t\\caption{%\n\t\t3-out-regular directed graphs with four \\emph{good} nodes (white) and three \\emph{bad} nodes (gray). \n\t\tEdges running from good to bad nodes are drawn in red. \n\t\tThe left graph minimizes the \\emph{segregation} objective from \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}, \n\t\tbut random walks oscillate between good nodes and bad nodes. \n\t\tIn contrast, only the right graph minimizes our \\emph{exposure} objective.\n\t}\\vspace*{-12pt}\n\t\\label{fig:setting}\n\\end{figure}",
            "fig:quality_threshold": "\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/quality_algo-gamine_budg-100_cost-cost_binary_2_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_thre-100000}\n\t\t\\subcaption{Probability shape $\\probabilityshape = \\mathbf{U}$}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/quality_algo-gamine_budg-100_cost-cost_binary_2_data-youtube_degr-5_dist-nonuniform_n_it-150_p_ab-0-05_thre-100000_nolegend}\n\t\t\\subcaption{Probability shape $\\probabilityshape = \\mathbf{S}$}\n\t\\end{subfigure}\n\t\\caption{%\n\t\tPerformance of \\ourmethod for quality thresholds $\\qualitythreshold\\in\\{0.0,0.5,0.9,0.95,0.99\\}$ as measured by $c_{B2}$,\n\t\trun on \\ytone with $\\outregulardegree=5$ and $\\pabsorption=0.05$.\n\t\t\\ourmethod can ensure $\\qualitythreshold = 0.5$ with little loss in performance, \n\t\tand it can reduce our objective considerably even under a strict $\\qualitythreshold = 0.95$.\n\t}\\label{fig:quality_threshold}\n\\end{figure}",
            "fig:baselines": "\\begin{figure}[t] \n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/cost-cost_binary_1_data-youtube_degr-10_dist-uniform_n_it-150_p_ab-0-05_thre-100000}\n\t\t\\subcaption{Regular out-degree $\\outregulardegree = 10$}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_thre-100000_nolegend}\n\t\t\\subcaption{Regular out-degree $\\outregulardegree = 5$}\n\t\\end{subfigure}\n\t\\caption{%\n\t\tPerformance of \\ourmethod with $\\qualitythreshold = 0.0$, compared with the four baselines \\baselineone, \\baselinetwo, \\baselinethree, and \\baselinefour under $c_{B1}$,\n\t\trun on \\ytone with $\\pabsorption = 0.05$ and $\\probabilityshape = \\mathbf{U}$. \n\t\tAs \\baselinefour is roundless, we apply its rewirings in decreasing order of $\\Delta$ to depict its performance as a function of $\\budget$.\n\t\t\\ourmethod outcompetes all baselines, \n\t\tbut \\baselinethree and \\baselinefour also show strong performance. \n\t}\\label{fig:baselines}\n\\end{figure}",
            "fig:gamine_vs_mms": "\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/gamine-vs-mms_budg-100_cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-99_thre-100000_mms}\n\t\t\\subcaption{\\ytone, $\\qualitythreshold=0.99$}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/gamine-vs-mms_budg-100_cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-99_thre-10000_gamine}\n\t\t\\subcaption{\\yttwo, $\\qualitythreshold=0.99$}\n\t\\end{subfigure}\\vspace*{3pt}\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/gamine-vs-mms_budg-100_cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-95_thre-100000_nolegend}\n\t\t\\subcaption{\\ytone, $\\qualitythreshold=0.95$}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/gamine-vs-mms_budg-100_cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-95_thre-10000_nolegend}\n\t\t\\subcaption{\\yttwo, $\\qualitythreshold=0.95$}\n\t\\end{subfigure}\\vspace*{3pt}\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/gamine-vs-mms_budg-100_cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-9_thre-100000_mms}\n\t\t\\subcaption{\\ytone, $\\qualitythreshold=0.9$}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/gamine-vs-mms_budg-100_cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-9_thre-10000_gamine}\n\t\t\\subcaption{\\yttwo, $\\qualitythreshold=0.9$}\n\t\\end{subfigure}\\vspace*{3pt}\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/gamine-vs-mms_budg-100_cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-5_thre-100000_nolegend}\n\t\t\\subcaption{\\ytone, $\\qualitythreshold=0.5$}\\label{fig:largemargin}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/gamine-vs-mms_budg-100_cost-cost_binary_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-5_thre-10000_nolegend}\n\t\t\\subcaption{\\yttwo, $\\qualitythreshold=0.5$}\n\t\\end{subfigure}\n\t\\caption{%\n\t\tPerformance of \\ourmethod and \\fabbrialg \n\t\twhen measured under $c_{B1}$ by the maximum segregation or the total segregation from \\citeauthor{fabbri2022rewiring} \\cite{fabbri2022rewiring}, \n\t\tor by the total exposure as defined in \\cref{eq:harm},\n\t\trun on \\ytone (left) and \\yttwo (right) with $\\outregulardegree=5$, $\\pabsorption=0.05$, and $\\probabilityshape=\\mathbf{U}$.\n\t\tFor all but $\\qualitythreshold = 0.5$, \n\t\t\\ourmethod outperforms \\fabbrialg on \\emph{all} objectives, \n\t\tand \\fabbrialg stops early because it can no longer reduce the maximum segregation. \n\t}\\label{fig:gamine_vs_mms}\n\\end{figure}",
            "fig:scalability": "\\begin{figure}[t]\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/rem_scalability}\n\t\t\\subcaption{\\ourmethod on \\ourproblem}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/qrem_scalability}\n\t\t\\subcaption{\\ourmethod and \\fabbrialg on \\ourproblemtwo}\n\t\\end{subfigure}\n\t\\caption{%\n\t\tScaling of \\ourmethod and \\fabbrialg\n\t\t%on our four largest real-world datasets, \n\t\tunder $\\labeling_{B1}$ with \n\t\t$\\pabsorption = 0.05$,\n\t\t$\\probabilityshape = \\mathbf{U}$, and \n\t\t$\\qualitythreshold = 0.0$ (\\ourproblem) resp.\\ $0.99$ (\\ourproblemtwo).\n\t\t%For each dataset and algorithm, \n\t\tWe report the seconds $s$ to compute a single rewiring \n\t\tas a function of $\\nedges = \\outregulardegree\\nnodes$ \n\t\t(\\fabbrialg does not identify any rewirings on \\nelatwo and \\nelathree). \n\t\t\\ourmethod scales more favorably than \\fabbrialg. \n\t}\\label{fig:scalability}\n\\end{figure}",
            "fig:channelclasses": "\\begin{figure}[t]\n\t\\centering\n\t\\hspace*{-5pt}\\begin{subfigure}[b]{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.94\\linewidth]{figures/edge-channel-classes_nelagt_degr-5_vari-2021-01_all}\\vspace*{7pt}\n\t\t\\subcaption{\\nelathree}\n\t\\end{subfigure}~%\n\t\\hspace*{-11pt}\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/edge-channel-classes_youtube_degr-5_thre-10000}\n\t\t\\subcaption{\\yttwo}\n\t\\end{subfigure}~%\n\t\\hspace*{-7pt}\\begin{subfigure}{0.078\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[height=3cm]{figures/colorbar_viridis}\\vspace*{31.2pt}\n\t\\end{subfigure}\n\t\\caption{%\n\t\tFractions of edges running between news outlet resp. video channel categories for real-world graphs with $\\outregulardegree = 5$, \n\t\twith marginals indicating the fraction of sources (right) resp. targets (top) in each category. \n\t\tNews outlet categories are denoted as triples (veracity score,\n\t\tconspiracy-pseudoscience flag, questionable-source flag); \n\t\tfor video channel categories, $\\{\\text{left},\\text{right}\\}$-center is abbreviated as $\\{\\text{left},\\text{right}\\}$-c;\n\t\tand label colors are coarse indicators of harm. \n\t\tIn \\nelathree, harmful and benign nodes are more interconnected than in \\yttwo.\n\t}\n\t\\label{fig:channelclasses}\n\\end{figure}",
            "fig:initialexposure": "\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering \n\t\t\\includegraphics[width=\\linewidth]{figures/exposure_distribution_new_cost_binary_2_nonuniform}\n\t\t\\subcaption{$\\labeling_{B2}$, $\\probabilityshape=\\mathbf{S}$}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\linewidth]{figures/exposure_distribution_new_cost_real_1_uniform_with_legend}\n\t\t\\subcaption{$\\labeling_{R1}$, $\\probabilityshape=\\mathbf{U}$}\n\t\\end{subfigure}\n\t\\caption{%\n\t\tDistributions of initial node exposures $\\unitvector^T_i\\fundamental\\labelingvector$ in our real-world datasets, computed with $\\pabsorption=0.05$. \n\t\tNote that cost functions sharing a name are defined differently for the \\yt and \\nf datasets (based on their semantics).\n\t\tThe \\nf datasets generally exhibit more concentrated exposure distributions than the \\yt datasets and higher median exposures.\n\t}\n\t\\label{fig:initialexposure}\n\\end{figure}",
            "fig:yt-rewiringchannels-R1": "\\begin{figure}[t]\n\t\\centering\n\t\\hspace*{-10pt}\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=1.1\\linewidth]{figures/k_sankey_algo-gamine_budg-100_cost-cost_real_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_thre-100000\n\t\t} \n\t\t\\subcaption{Quality threshold $\\qualitythreshold=0.0$}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\hspace*{-3pt}\\includegraphics[width=1.1\\linewidth]{figures/k_sankey_algo-gamine_budg-100_cost-cost_real_1_data-youtube_degr-5_dist-uniform_n_it-150_p_ab-0-05_qual-0-99_thre-100000\n\t\t}\n\t\t\\subcaption{Quality threshold $\\qualitythreshold=0.99$}\n\t\\end{subfigure}\n\t\\caption{%\n\t\tChannel class of videos in rewirings $\\rewiring{i}{j}{k}$ on \\ytone with $\\outregulardegree=5$, \n\t\t$\\pabsorption=0.05$, \n\t\tand $\\probabilityshape=\\mathbf{U}$, \n\t\tcomputed using $\\labeling_{R1}$, for different quality thresholds. \n\t\tRewirings between classes are color-scaled by their count, \n\t\tusing blues if $c_{R1}(k) < c_{R1}(j)$, reds if $c_{R1}(k) > c_{R1}(j)$, and grays otherwise. \n\t\tFor $\\qualitythreshold = 0.0$, \n\t\twe only replace costly targets $j$ by less costly targets $k$,\n\t\tas expected, \n\t\tbut for $\\qualitythreshold = 0.99$, \n\t\twe see many rewirings with $c_{R1}(k) \\geq c_{R1}(j)$.\n\t}\n\t\\label{fig:yt-rewiringchannels-R1}\n\\end{figure}",
            "fig:yt-rewiringtypes-B1": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{figures/rewiring_types_cost-binary_1_data-youtube_degr-5_thres-both}\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\subcaption{\\ytone}\n\t\\end{subfigure}~%\n\t\\begin{subfigure}{0.5\\linewidth}\n\t\t\\centering\n\t\t\\subcaption{\\yttwo}\n\t\\end{subfigure}\n\t\\caption{%\n\t\tMapping the nodes in each rewiring $\\rewiring{i}{j}{k}$ to their costs $(\\labeling_i, \\labeling_j, \\labeling_k)$, \n\t\twe report the fraction of rewirings in each cost class \n\t\tunder $\\labeling_{B1}$ and $\\qualitythreshold\\in\\{0.0,0.5,0.9,0.95,0.99\\}$,\n\t\tfor \\yt graphs with \n\t\t$\\outregulardegree=5$, \n\t\t$\\pabsorption = 0.05$, \n\t\tand $\\probabilityshape = \\textbf{U}$.\n\t\tWhile most intuitively suboptimal classes occur rarely (e.g., 001, 011, 101), \n\t\tunder quality constraints,\n\t\twe often rewire \\emph{among} harmful nodes.\n\t}\n\t\\label{fig:yt-rewiringtypes-B1}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\t\\transitionmatrix[i,j] =\\begin{cases}\n\t\t \\probability{ij}& \\text{if }\\edge{i}{j}\\in \\edges\\;,\\\\\n\t\t 0&\\text{otherwise}\\;.\n\t\\end{cases}\n\\end{align}",
            "eq:eq:fundamental": "\\begin{align}\\label{eq:fundamental}\n\t\\fundamental = \\sum_{i=0}^{\\infty} \\transitionmatrix^i = (\\identity - \\transitionmatrix)^{-1}\\;,\n\\end{align}",
            "eq:2": "\\begin{align}\n\t\\objective{\\graph} = \\onevector^T\\fundamental\\labelingvector\\;,\n\t\\label{eq:harm}\n\\end{align}",
            "eq:3": "\\begin{align}\n\t\\maxobjective{\\graph,\\graph_\\budget} =\\objective{\\graph}-\\objective{\\graph_\\budget}\\;.\n\t\\label{eq:maxdelta}\n\\end{align}",
            "eq:4": "\\begin{align}\n\t\\transitionmatrix' = \\transitionmatrix + \\ivector(-\\jkvector)^T \\;,\n\\end{align}",
            "eq:5": "\\begin{align}\n\t\\fundamental' \n\t= (\\identity - (\\transitionmatrix + \\ivector(-\\jkvector)^T))^{-1}\n\t= (\\identity - \\transitionmatrix + \\ivector\\jkvector^T)^{-1} \\;.\n\\end{align}",
            "eq:eq:sherman": "\\begin{align}\\label{eq:sherman}\n\t\\fundamental'  = \n\t\t\\fundamental - \n\t\t\\frac{\\fundamental \\ivector \\jkvector^T\\fundamental}\n\t\t\t {1 + \\jkvector^T\\fundamental\\ivector} \\;.\n\\end{align}",
            "eq:6": "\\begin{align}\n\t\\objective{\\graph'} \n\t&\t= \\onevector^T\\fundamental'\\labelingvector\n\t\t= \\onevector^T\\left(\\fundamental - \\frac{\\fundamental \\ivector \\jkvector^T\\fundamental}{1 + \\jkvector^T\\fundamental\\ivector}\\right)\\labelingvector\n\t\t= \\onevector^T\\fundamental\\labelingvector - \\onevector^T\\left(\\frac{\\fundamental \\ivector \\jkvector^T\\fundamental}{1 + \\jkvector^T\\fundamental\\ivector}\\right)\\labelingvector \\nonumber \\\\\n\t&\t= \\objective{\\graph} -  \\frac{(\\onevector^T\\fundamental \\ivector)(\\jkvector^T\\fundamental\\labelingvector)}{1 + \\jkvector^T\\fundamental\\ivector}\n\t\t= \\objective{\\graph} - \\frac{\\sigma\\tau}{\\rho}\n\t\t= \\objective{\\graph} - \\Delta \\;,  \n\t\\label{eq:fnew=fold-delta}\n\\end{align}",
            "eq:eq:delta": "\\begin{align}\\label{eq:delta}\n\t\\Delta = \\maxobjective{\\graph,\\graph'} = \\frac{\\sigma\\tau}{\\rho} = \\frac{(\\onevector^T\\fundamental \\ivector)(\\jkvector^T\\fundamental\\labelingvector)}{1 + \\jkvector^T\\fundamental\\ivector} \\;.\n\\end{align}",
            "eq:7": "\\begin{align}\n\t\\onevector^T\\fundamental \n\t=~& \\onevector^T\\sum_{i=0}^{\\infty}\\transitionmatrix^i \n\t= \\onevector^T \n\t+ \\onevector^T\\transitionmatrix \n\t+  (\\onevector^T\\transitionmatrix)\\transitionmatrix \n\t+ ((\\onevector^T\\transitionmatrix)\\transitionmatrix)\\transitionmatrix + \\dots\\\\\n\t\\fundamental\\labelingvector \n\t=~& \\left(\\sum_{i=0}^{\\infty}\\transitionmatrix^i\\right)\\labelingvector\n\t= \\labelingvector + \\transitionmatrix\\labelingvector + \\transitionmatrix(\\transitionmatrix\\labelingvector) + \\transitionmatrix(\\transitionmatrix(\\transitionmatrix\\labelingvector)) + \\dots\n\\end{align}"
        }
    }
}