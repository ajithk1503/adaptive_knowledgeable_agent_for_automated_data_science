{
    "meta_info": {
        "title": "CF-GODE: Continuous-Time Causal Inference for Multi-Agent Dynamical  Systems",
        "abstract": "Multi-agent dynamical systems refer to scenarios where multiple units\ninteract with each other and evolve collectively over time. To make informed\ndecisions in multi-agent dynamical systems, such as determining the optimal\nvaccine distribution plan, it is essential for decision-makers to estimate the\ncontinuous-time counterfactual outcomes. However, existing studies of causal\ninference over time rely on the assumption that units are mutually independent,\nwhich is not valid for multi-agent dynamical systems. In this paper, we aim to\nbridge this gap and study how to estimate counterfactual outcomes in\nmulti-agent dynamical systems. Causal inference in a multi-agent dynamical\nsystem has unique challenges: 1) Confounders are time-varying and are present\nin both individual unit covariates and those of other units; 2) Units are\naffected by not only their own but also others' treatments; 3) The treatments\nare naturally dynamic, such as receiving vaccines and boosters in a seasonal\nmanner. We model a multi-agent dynamical system as a graph and propose\nCounterFactual GraphODE (CF-GODE), a causal model that estimates\ncontinuous-time counterfactual outcomes in the presence of inter-dependencies\nbetween units. To facilitate continuous-time estimation, we propose\nTreatment-Induced GraphODE, a novel ordinary differential equation based on\nGNN, which incorporates dynamical treatments as additional inputs to predict\npotential outcomes over time. To remove confounding bias, we propose two domain\nadversarial learning based objectives that learn balanced continuous\nrepresentation trajectories, which are not predictive of treatments and\ninterference. We further provide theoretical justification to prove their\neffectiveness. Experiments on two semi-synthetic datasets confirm that CF-GODE\noutperforms baselines on counterfactual estimation. We also provide extensive\nanalyses to understand how our model works.",
        "author": "Song Jiang, Zijie Huang, Xiao Luo, Yizhou Sun",
        "link": "http://arxiv.org/abs/2306.11216v1",
        "category": [
            "cs.LG",
            "stat.ME"
        ],
        "additionl_info": "13 pages, 8 figures"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nEstimating counterfactual outcomes \\emph{over time} is critical to gaining causal understanding for many useful practical applications, such as how to distribute the limited vaccines in the early days to maximize protection over time~\\cite{medlock2009optimizing}, or how to design proper scheduling of medical treatments to optimize the patient recovery process~\\cite{BicaAJS20}. Randomized controlled trials (RCTs) are the gold standard for causal inference, but they can be cost-prohibitive and ethically challenging, particularly when considering the dynamical settings described above. Therefore, estimating counterfactual outcomes from observational data is the key approach to answering causal questions in real-world scenarios. Existing research on observational causal inference over time has begun by utilizing basic linear regression~\\cite{robins2000marginal} and Gaussian processes~\\cite{xu2016bayesian} to capture the time-dependencies. Subsequently, advancements have been made by incorporating more advanced deep learning models such as recurrent neural networks (RNNs) ~\\cite{BicaAJS20,fujii2022estimating} and Transformers ~\\cite{MelnychukFF22}. \n\n\n\nDespite the progress, all aforementioned studies have relied on the assumption that units (e.g., people in the vaccine example) are independent of each other, i.e., each unit is solely influenced by its own treatment but not by others. In many realistic scenarios, however, this assumption is not valid. For instance, a person's vaccination not only protects themselves but also those close to them. This type of setting is referred to as a \\emph{multi-agent dynamical system}~\\cite{gazi2007coordination}, where units (also known as agents) interact with each other and evolve collectively over time. Many practical problems can be expressed as multi-agent dynamical systems, such as the long-term effects of vaccination where people mutually influence~\\cite{halloran2012causal}, brain network signals in which the regions of interest (ROI) in a brain are associated~\\cite{yu2022learning,cui2022braingb}, and molecular systems movements where the atoms are interconnected ~\\cite{durrant2011molecular}. Prior approaches for causal inference over time are not applicable to multi-agent dynamical systems since they are not capable of handling the interconnections between units. In this paper, we propose to study this novel problem \\emph{counterfactual estimation in multi-agent dynamical systems}, which has received limited attention in the literature.  \n\n\n%\n\nThe dynamic and interrelated nature of multi-agent dynamical systems poses unique and nontrivial challenges to causal inference. We illustrate them along with Fig.~\\ref{fig:intro}. 1) \\textbf{Multi-source confounders}. Confounders are variables that have an impact on both treatments and outcomes, leading to spurious correlations between them. Therefore, in observational data, the treatments are not balanced among units with different confounders values, resulting in biased counterfactual outcomes estimation. For example, old people are more likely to receive vaccines, but also face a higher risk of virus infection. If we train a standard supervised model using such imbalanced data, it may wrongly predict that vaccines may increase the infection risk for young people. In multi-agent dynamical systems, the confounders are multi-source, including \\emph{time-dependent confounders} and \\emph{neighbor confounders}. Time-dependent confounders refer to the fact that the confounders typically evolve over time and thus their impact on treatments and outcomes also changes dynamically~\\cite{platt2009time, BicaAJS20}. For instance, people's health conditions change over time, affecting their likelihood of getting vaccinated and future health status. Neighbor confounders mean that a unit's treatment and outcome could also be confounded by the covariates of its near units (neighbors)~\\cite{forastiere2021identification,arbour2016inferring}, For example, if family members are in poor health, a unit may be more likely to receive a vaccine. Compared to the independent setting, neighbor confounders are additional confounding factors in multi-agent dynamical systems. 2) \\textbf{Imbalance of interference}. As discussed in the previous example that vaccines protect not only a unit but also those in close proximity, the outcome of a unit can be influenced by others' treatments in multi-agent dynamical systems. In causal language, this phenomenon is referred to as \\emph{interference}. Similar to the treatments, interference is affected by the covariates and thus is not balanced across the units in observational data~\\cite{forastiere2021identification}. For instance, highly educated units are more likely to receive vaccines and typically have more highly educated friends. Therefore, they receive stronger protection through higher vaccination rates among their social networks. Such imbalanced interference causes additional bias in the estimation of counterfactual outcomes. 3) \\textbf{Continuous dynamics}. In realistic applications, a multi-agent dynamical system is continuous in nature~\\cite{porter2014dynamical}. However, most existing causal models are discrete, making them inappropriate for multi-agent dynamical systems. Modeling continuous-time observations (such as covariates and outcomes) and continuously estimating counterfactual outcomes over time remains an open challenge.  \n\n\nIn this paper, we address the above challenges and study how to estimate continuous-time counterfactual outcomes, in presence of multi-source confounders and interference, in multi-agent dynamical systems. This is a novel, yet challenging and under-explored problem with valuable real-world applications. \n\nTo this end, we model a multi-agent dynamical system as a \\emph{graph}, where nodes represent units and edges capture their interactions. Inspired by recent achievements in graph ordinary differential equations (GraphODE)~\\cite{huang2020learning}, we propose \\model, a novel \\emph{causal} model that estimates continuous-time \\textbf{\\underline{C}}ounter\\textbf{\\underline{F}}actual outcomes based on \\textbf{\\underline{G}}raph \\textbf{\\underline{O}}rdinary \\textbf{\\underline{D}}ifferential \\textbf{\\underline{E}}quations in multi-agent dynamical systems. Specifically, we use GraphODE as a backbone to model the continuous trajectory of each unit. However, in this case, traditional GraphODE can only model the pure dynamics of potential outcomes~\\cite{huang2020learning} and lacks the ability to incorporate additional inputs such as treatments, making it inappropriate for causal inference. To address this issue, in \\model, we propose \\emph{\\ode}, a new GraphODE model capable of handling treatments when predicting the future trajectory of potential outcomes. \\ode uses graph neural networks (GNNs)~\\cite{KipfW17} to formulate its differential equations, which can effectively capture the mutual dependencies between units including neighbor confounders and interference. This advantage makes it a natural fit for counterfactual estimation in multi-agent dynamical systems. Then a latent representation is learned for each unit from its observations as the solution to \\ode, which represents the continuous trajectory driven by treatments. The core of ensuring \\model is a causal model is to deal with the aforementioned estimation bias caused by imbalanced treatments and interference in the observational data. We solve this issue via domain adversarial learning~\\cite{ganin2016domain,WangHK20}, in which we treat the values of treatments (and interference) as domains and ensure the latent representation trajectories are invariant to them. We provide theoretical justification to demonstrate that the domain-adversarial balancing objective functions proposed in \\model can effectively achieve the balancing goal, thereby removing bias in counterfactual estimation and ensuring that \\model is causal.\n\nWe summarize our major \\textbf{contributions} as follows: 1) We study how to estimate counterfactual outcomes in multi-agent dynamical systems, which is a novel yet challenging problem with useful practical implications. 2) We propose \\model, a novel causal model for causal inference multi-agent dynamical systems based on GraphODE and domain-adversarial learning. 3) We provide theoretical analysis to show that \\model is able to handle the imbalanced treatments and interference, ensuring unbiased counterfactual estimation. 4)We conduct extensive experiments to evaluate \\model's performance on counterfactual outcomes estimation in multi-agent dynamical systems.\n\\vspace{-10pt}\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Causal Inference Over Time",
                    "content": "\nThe central challenge in estimating counterfactual outcomes in longitudinal settings is to remove the confounding bias from time-dependent covariates~\\cite{BicaAJS20,platt2009time}. The core solution to this in existing works is to cut off the association between covariates and the observed treatment assignments over time. To achieve this goal, statistical tools are widely used in traditional approaches. For example, marginal structural models (MSMs)~\\cite{robins2000marginal} use inverse probability of treatment weighting (IPTW)~\\cite{rosenbaum1983central,rosenbaum1987model} to balance the distribution of covariates over time between unit groups that are assigned to different treatments. By doing so, treatment assignment can no longer be predicted from the balanced covariates, thus breaking their correlation. A later work~\\cite{lim2018forecasting} further enhances MSMs by using recurrent neural networks (RNNs) to learn the inverse probability of treatment weights (IPTWs), which is more capable of modeling sequential data. However, IPTW based tools can result in high variances in practice~\\cite{BicaAJS20}. To overcome this limitation, recent studies~\\cite{BicaAJS20, MelnychukFF22} extend the representation learning based balancing approaches from static settings~\\cite{johansson2016learning,yoon2018ganite,shalit2017estimating,yao2018representation} to dynamic settings. Specifically, counterfactual recurrent network (CRN)~\\cite{BicaAJS20} uses RNNs to encode the time-varying covariates into latent embeddings over time, which are simultaneously optimized by two objectives: potential outcomes prediction and longitudinal distribution balancing w.r.t. treatment assignments. The learned balanced embeddings are not predictive of treatments, thus ensuring unbiased estimates of the potential outcomes. Since RNNs are less powerful in capturing long-range dependencies, ~\\cite{MelnychukFF22} improves CRN by using Transformer~\\cite{vaswani2017attention} that preserves long-range dependencies between time-dependent confounders. Despite the progress, all the above models can only predict counterfactual outcomes in discrete timestamps. However, practical longitudinal sequences are continuous in nature.\n\nOur work is most related to~\\cite{gwak2020neural,bellot2021policy,SeedatIBQS22,de2022predicting}, which estimate counterfactual outcomes in continuous dynamic settings using neural ordinary differential equations (ODEs)~\\cite{RubanovaCD19,chen2018neural} or neural controlled differential equations (CDEs)~\\cite{kidger2020neural}. Specifically, ~\\cite{SeedatIBQS22} infers continuous latent trajectories to represent the movement of potential outcomes and balance the distribution of this latent representation between treated and control groups via adversarial learning. However, ~\\cite{SeedatIBQS22} (and all aforementioned models) assume that units are mutually independent, which is usually not valid in many practical scenarios where the units affect each other, e.g., getting vaccinated provides long-term protection not only for oneself but also for their close ones. In contrast, our model is designed to estimate counterfactual outcomes in longitudinal settings where units are interdependent, i.e., the multi-agent dynamical systems.     \n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Continuous Modeling With Neural Ordinary Differential Equations (ODEs)",
                    "content": "\nMany dynamical systems are continuous in nature, which can be typically modeled using first-order ordinary differential equations (ODEs)~\\cite{porter2014dynamical}. ODEs describe a system's rate of change over time by a specific function, which is traditionally designed by domain experts~\\cite{qian2021integrating}, and more recently parameterized by neural networks~\\cite{RubanovaCD19,chen2018neural,massaroli2020dissecting}, as a closed-form ODE function may be unknown for some complex real-world systems. Given initial states, the solution of a NeuralODE can be easily computed using any ODE solver, such as the Runge-Kutta method~\\cite{schober2019probabilistic}. In multi-agent dynamical systems, such as the spread of infectious disease among people, units often interact with one another, yet standard NeuralODEs do not explicitly model these interactions. Recent works have sought to address this limitation by representing the interactions among multiple units as graphs, and then utilizing graph neural networks (GNNs)~\\cite{KipfW17,VelickovicCCRLB18} to parameterize the ODE function~\\cite{huang2021coupled,huang2020learning,ZangW20a,poli2019graph}. When predicting the dynamics of each unit, these GraphODE models not only take into account the unit's own latent state but also aggregate the latent states of its connected units along the interaction graph, to effectively capture the mutual influence between them. However, GraphODE models are standard statistical methods and therefore lack the capability for causal inference. Instead, our model aims to address the unique challenges present in multi-agent dynamical systems, i.e., time-dependent confounders and network interference, in order to make counterfactual predictions.\n\n  "
                }
            },
            "section 3": {
                "name": "Problem Setup",
                "content": "\n\n",
                "subsection 3.1": {
                    "name": "Problem Formulation",
                    "content": "\\label{sec:problem}\n\nWe study how to estimate counterfactual outcomes in the context of multi-agent dynamical systems, where the units engage in mutual interactions and evolve simultaneously over time. Throughout this paper, we use boldface uppercase letters to denote matrices or vectors, boldface uppercase letters with subscripts to signify elements of matrices or vectors, regular lowercase letters to represent values of variables, and calligraphic uppercase letters to indicate sets. We summarize all notations used in this paper in Appendix.~\\ref{sec:notations}.\n\nFormally, a multi-agent dynamical system can be represented by a dynamical graph $\\mathcal{G}^t=(\\mathcal{V},\\mathcal{E}^t)$, where $\\mathcal{V} = \\{v_1, v_2, ..., v_N\\}$ is the set of $N$ units (nodes) and $\\mathcal{E}^t$ denotes the edge set at time $t$. An edge in $\\mathcal{E}^t$ describes the intersection between the two units it connects at time $t$. In this paper, we present an early exploration of causal inference in multi-agent dynamical systems, and for the purpose of simplicity, we assume that the graph structure remains constant over time, i.e., $\\mathcal{G}^t = \\mathcal{G}$. Each unit is associated with time-varying variables, which are the causal quantities in our case. We introduce them together with the causal framework in the following.\n\nWe follow the longitudinal potential outcomes framework~\\cite{robins2009estimation,rubin1978bayesian} to formalize the counterfactual outcome estimation as in~\\cite{BicaAJS20, SeedatIBQS22}. The observational data $\\left(\\left( \\mathbf{X}^t, \\mathbf{A}^t, \\mathbf{Y}^t \\right) \\cup \\mathbf{V}\\right)$ in a multi-agent dynamical system contains time-dependent covariates $\\mathbf{X}^t$ (e.g., health condition), dynamical\ntreatments $\\mathbf{A}^t$ (e.g., vaccine allocation), and time-varying outcomes $\\mathbf{Y}^t$ (e.g., immunity to infectious disease). It is worth noting that $\\mathbf{Y}^t$ is essentially a part of $\\mathbf{X}^t$. $\\mathbf{V}$ denotes the static covariates of units such as ethnicity. Let the historical records of the multi-agent dynamical system up to time $t$ be represented by $\\mathcal{H}^t = \\{\\mathbf{\\Bar{X}}^t,\\mathbf{\\Bar{A}}^t,\\mathbf{\\Bar{Y}}^t, \\mathbf{V}\\}$, where $\\mathbf{\\Bar{X}}^t, \\mathbf{\\Bar{A}}^t,\\mathbf{\\Bar{Y}}^t$ are all the $\\mathbf{X}^{t^-}, \\mathbf{A}^{t^-}, \\mathbf{Y}^{t^-}$ until $t$ $(t^-\\leq t)$, respectively. In causal inference, we are focused on understanding the potential outcomes $\\mathbf{Y}^{t^+}(\\mathbf{A}^{t^+}=a)$\\footnote{The potential outcome can also be formalized using \\emph{do} operation~\\cite{pearl2009causality}.} that may occur in the future $(t^+>t)$ under a specific treatment $a$, which explains the impact of the treatment assignment on the dynamics of the system. Note that $a$ is a treatment trajectory that includes all treatments in the future time. Our goal is to estimate the future potential outcomes sequence driven by treatments in a multi-agent dynamical system, which is formalized as:\n\\begin{align}\n\\mathbb{E}\\left(\\mathbf{Y}^{t^+}(\\mathbf{A}^{t^+}=a)\\mid \\mathcal{H}^t, \\mathcal{G}\\right).\\label{eqn:formalization}\n\\end{align}\n"
                },
                "subsection 3.2": {
                    "name": "Causal Identification",
                    "content": "\nThe potential outcomes represented by $\\mathbf{Y}^{t^+}(\\mathbf{A}^{t^+}=a)$ are a causal quantity. To make it identifiable from observational data, we must adhere to the following necessary assumptions.\n\n\\textbf{Assumption 1: Positivity (Overlap)}. The future treatment trajectory is probabilistic regardless of the historical observation, i.e., $0 < P(\\mathbf{A}^{t^+}=a\\mid \\mathcal{H}^t) < 1, \\forall \\mathcal{H}^t$.\n\n\\textbf{Assumption 2: Consistency}. Under the same treatment trajectory $a$, the potential outcome is equal to the observed outcomes, i.e., $\\mathbf{Y^{t^+}}(\\mathbf{A}^{t^+}=a) = Y^{t^+}$.\n\nThe above two assumptions are standard for longitudinal counterfactual estimation. To identify the potential outcomes, it is also necessary to assume that there are no unobserved confounders, i.e., the strong ignorability assumption. However, the typical sequential strong ignorability assumption~\\cite{BicaAJS20,SeedatIBQS22,MelnychukFF22,lim2018forecasting} is not appropriate for multi-agent dynamical systems, because the graph structure $\\mathcal{G}$ introduces extra graph confounders and interference. A plausible strong ignorability assumption for graphs is first introduced by~\\cite{forastiere2021identification} and later validated in studies such as~\\cite{ma2021causal, ma2022learning, jiang2022estimating}. However, the assumption made in these works is limited to static settings. To address this, we extend it to longitudinal settings and adapt it to be applicable to multi-agent dynamical systems in the following.\n\n\n\n\n\nWe first introduce a summary function, denoted as $g(\\cdot)$, that captures the interference effects caused by the treatments of a node's neighboring units in the graph as in~\\cite{forastiere2021identification}. Formally, $\\mathbf{G}_i^t = g(\\mathbf{A}^t_{\\mathcal{N}_i},\\mathbf{A}^t_{\\mathcal{N}_{-i}})$, where $\\mathbf{A}^t_{\\mathcal{N}_i}$ denotes the treatments of node $i$'s immediate neighbors, and $\\mathbf{A}^t_{\\mathcal{N}_{-i}}$ is the treatments of all the remaining node that are not directly connected to node $i$. We refer to $\\mathbf{G}_i^t$ as \\emph{interference summary}. Here for simplicity, we adopt the assumption put forth in ~\\cite{forastiere2021identification,arbour2016inferring,jiang2022estimating} that a node is only influenced by the treatments of its immediate neighbors, i.e., $g(\\mathbf{A}^t_{\\mathcal{N}_i},\\mathbf{A}^t_{\\mathcal{N}_{-i}}) = g(\\mathbf{A}^t_{\\mathcal{N}_i},\\mathbf{A'}^t_{\\mathcal{N}_{-i}}) = g(\\mathbf{A}^t_{\\mathcal{N}_i}), \\forall \\mathbf{A}^t_{\\mathcal{N}_{-i}}, \\mathbf{A'}^t_{\\mathcal{N}_{-i}}$. $g(\\cdot)$ can be instantiated using any aggregation functions or models. As in previous studies~~\\cite{forastiere2021identification,ma2021causal,jiang2022estimating}, in this paper, we define $\\mathbf{G}_i^t$ as the proportion of treated units in unit $i$'s neighbors, i.e., $\\mathbf{G}_i^t \\vcentcolon=\\sum_{j\\in \\mathcal{N}_i}\\frac{\\mathbf{A}_i^t}{|N_i|}$.\nWith $\\mathbf{G}_i^t$, we present the strong ignorability assumption for multi-agent dynamical systems in the following:\n\n\n\n\n\n\\textbf{Assumption 3: Strong Ignorability for Multi-Agent Dynamical Systems}\\footnote{Note that similar to the strong ignorability assumptions in static or non-graph sequential settings, assumption 3 can not be verified only from data.}. Given the historical observations and the graph structure that describes the multi-agent dynamical system, the potential outcome trajectory is independent of the treatments and interference summary, i.e., $\\mathbf{Y^{t^+}}(\\mathbf{A}^{t^+}=a) \\CI \\mathbf{A}^{t^+}, \\mathbf{G}^{t^+} \\mid \\mathcal{H}^t, \\mathcal{G}. \\forall a, t$.\n\nWith these three assumptions, the potential outcome trajectory Eq.~(\\ref{eqn:formalization}) can be identifiable as:\n% \\begin{proof}\n\\begin{align}\n        &\\mathbb{E}\\left(\\mathbf{Y}^{t^+}(\\mathbf{A}^{t^+}=a)\\mid \\mathcal{H}^t, \\mathcal{G}\\right)\\nonumber \\\\\n        &= \\mathbb{E}\\left(\\mathbf{Y}^{t^+}(\\mathbf{A}^{t^+}=a)\\mid \\mathbf{A}^{t^+}, \\mathbf{G}^{t^+}, \\mathcal{H}^t, \\mathcal{G}\\right)\\label{eqn:identify_ignore}\\\\\n&= \\mathbb{E}\\left(\\mathbf{Y}^{t^+}\\mid \\mathbf{A}^{t^+}, \\mathbf{G}^{t^+}, \\mathcal{H}^t, \\mathcal{G}\\right)\\label{eqn:identify_consist}.\n\\end{align}\nEq.~(\\ref{eqn:identify_ignore}) is true because of assumption 3, while Eq.~(\\ref{eqn:identify_consist}) holds under the assumption 2.\n% \\end{proof}\nThe above causal identification enables us to estimate the potential outcomes in multi-agent dynamical systems using observational data. More specifically, we can train a machine learning model on observational data, which takes treatment trajectory $\\mathbf{A}^{t^+}$, interference summary $\\mathbf{G}^{t^+}$, historical observation $\\mathcal{H}^t$ and graph $\\mathcal{G}$ as inputs, and the observed (factual) outcome $\\mathbf{Y}^{t^+}$ as targets, to predict the counterfactual outcomes given new treatment trajectories. Our proposed model \\model is grounded in this and will be presented in detail in the subsequent section."
                }
            },
            "section 4": {
                "name": "Proposed Model: \\model",
                "content": "\n\n\n",
                "subsection 4.1": {
                    "name": "Overview",
                    "content": "\nOur proposed \\model is a causal model that predicts counterfactual outcomes in a multi-agent dynamical system by learning from observational data. We show an overview of our model in Fig.~\\ref{fig:frame}. Compared to most existing causal models designed for standard sequential settings that consider discrete time intervals and independent units~\\cite{BicaAJS20, MelnychukFF22}, multi-agent dynamical systems are more realistic and present two challenging properties: the dynamics are \\emph{continuous} in nature, and units are \\emph{influenced} by others. To address these, our proposed \\model takes the advantage of recent breakthroughs in graph ordinary differential equations (GraphODE)~\\cite{huang2020learning,huang2021coupled} and extends it to handle treatments and interference, enabling continuous estimation of counterfactual outcomes in multi-agent dynamical systems. We refer to our ODE model as \\emph{\\ode} (Sec.~\\ref{sec:graph_ode}). The time-dependent confounders lead the distribution of covariates to be quite discrepant between units assigned to different treatments, resulting in high variances in counterfactual outcome estimation~\\cite{johansson2016learning, shalit2017estimating,robins2000marginal}. This effect is further amplified by the imbalanced interference caused by the graph structure in multi-agent dynamical systems~\\cite{forastiere2021identification,jiang2022estimating}. \\model uses adversarial learning to alleviate this issue and guarantee unbiased estimates of counterfactual outcomes (Sec.~\\ref{sec:adversarial_learning}).\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "\\ode",
                    "content": "\\label{sec:graph_ode}\nTo facilitate continuous-time counterfactual outcome estimation, we propose to learn a continuous latent trajectory $\\mathbf{Z}_i^t$ for every node in multi-agent dynamical system that represents their movement. An ideal $\\mathbf{Z}_i^t$ should possess two characteristics: 1) the ability to predict observed outcomes, and 2) not to be predictive of the received treatment or interference in observational data\\footnote{The second characteristic is discussed in Sec.~\\ref{sec:adversarial_learning}.}. We implement such a $\\mathbf{Z}_i^t$ by a novel model called \\ode, which empowers the recent GraphODE~\\cite{huang2020learning,huang2021coupled} to deal with treatment and interference for counterfactual outcomes estimation. \n\nIn a multi-agent dynamical system, the future outcomes of node $i$ might be affected by not only its own past movement and current treatment, but also the movements and interference from neighbors (e.g., a unit's health condition and vaccination status have a significant impact on how likely others are to be infected). We model this process and formalize \\odelower as:\n\\begin{align}\n    \\mathbf{Z}_{i}^{t}=\\mathbf{Z}_{i}^{0}+\\int_{t'=0}^{t} \\phi\\left(\\mathbf{Z}^{t'},\\mathbf{A}^{t'}\\right) dt'\\label{eqn:ode}.\n\\end{align}\nIn Eq.~(\\ref{eqn:ode}), $\\mathbf{Z}^{t'}$ and $\\mathbf{A}^{t'}$ denote the latent trajectory representations and treatments of all nodes in the multi-agent dynamical system, respectively. $\\phi(\\cdot)$ is the ODE function. To comprehensively capture the effects from node $i$ and its connected neighbors, we parameterize $\\phi(\\cdot)$ using graph neural networks~\\cite{KipfW17} with self-loops. $\\mathbf{Z}_{i}^{0}$ is the initial state and can be encoded from the initial observations as  $\\mathbf{Z}_{i}^{0} = f(\\mathbf{X}_{i}^{0},\\mathbf{V}_i)$, where $f(\\cdot)$ is an encoder parameterized by neural networks. With $\\mathbf{Z}_{i}^{0}$, we can obtain the $\\mathbf{Z}_{i}^{t}$, which is the solution to \\odelower, by solving an ODE initial-value problem (IVP) in Eq.~(\\ref{eqn:ode}), formalized as:\n\\begin{align}\n     \\mathbf{Z}_{i}^{0},\\mathbf{Z}_{i}^{1}\\cdots \\mathbf{Z}_{i}^{T} = \\text{ODESolve}\\left(\\phi,[\\mathbf{Z}_1^0,\\mathbf{Z}_2^0\\cdots \\mathbf{Z}_N^0],\\left(t_0,t_1\\cdots t_T\\right)\\right)\\label{eqn:solver},\n\\end{align}\nwhere $T$ is the number of timestamps for the evaluation of Eq.~(\\ref{eqn:solver}). With the solution latent trajectory $\\mathbf{Z}_{i}^{t}$, we can then use a decoder $d_\\mathbf{Y}(\\cdot)$ to transform it to the predicted outcome $\\hat{\\mathbf{Y}}_i^t = d_\\mathbf{Y}(\\mathbf{Z}_i^t)$. We also use neural networks to instantiate $d_\\mathbf{Y}(\\cdot)$. We compare the prediction $\\hat{\\mathbf{Y}}_i^t$ to ground-truths $\\mathbf{Y}_i^t$ in all observed timestamps $\\left(t_0,t_1\\cdots t_T\\right)$ using a mean square error as objective, which is formalized as:\n\\begin{align}\n    L^{\\langle Y \\rangle} = \\frac{1}{N}\\frac{1}{T}\\sum_i^N\\sum_t^T\\left(\\hat{\\mathbf{Y}}_i^t - \\mathbf{Y}_i^t\\right)^2.\n\\end{align}\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Balancing via Adversarial Learning",
                    "content": "\\label{sec:adversarial_learning}\nIn the observational data, the treatments applied to each unit $\\mathbf{A}_i^t$ are affected by the time-dependent confounders present in the covariates (and thus in its latent representation trajectory $\\mathbf{Z}_i^t$). Consequently, the distribution of latent representation trajectory is not balanced among units with different treatment assignments, i.e., $P(\\mathbf{A}_i^t|\\mathbf{Z}_i^t)$ is not uniform, leading to high variances in the counterfactual outcome estimation~\\cite{johansson2016learning,shalit2017estimating}. In the context of multi-agent dynamical systems, this effect is further exacerbated by the presence of \\emph{imbalanced interference} among units. This is because a unit's interference is influenced by its covariates (also the latent representation) and treatments in the observational data, i.e., $P(\\mathbf{G}_i^t|\\mathbf{Z}_i^t,\\mathbf{A}_i^t)$ is not uniform~\\cite{forastiere2021identification,jiang2022estimating,ma2021causal}. Here we give an intuitive example of the imbalanced interference: consider that a highly educated person is more likely to be surrounded by other highly educated friends, who believe in science and are more likely to be vaccinated, thereby providing stronger protection for this person against infectious diseases, i.e., higher interference.\n\nA sufficient condition to remove the above bias is to ensure that the distribution of latent representation trajectories is invariant to treatments, and when combined with the corresponding treatments, is interference-invariant~\\cite{shalit2017estimating,BicaAJS20,SeedatIBQS22,forastiere2021identification,jiang2022estimating}. This condition is formalized as  $P(\\mathbf{Z}^t|\\mathbf{A}^t=0) = (\\mathbf{Z}^t|\\mathbf{A}^t=1)$ for treatment balancing, and $P(\\mathbf{Z}^t,\\mathbf{A}^t|\\mathbf{G}^t=g')$ is identical for any given value of $g'$ for interference balancing. The treatment $\\mathbf{A}^t$ is binary and interference $\\mathbf{G}^t$ is continuous as in~\\cite{forastiere2021identification,jiang2022estimating}. Note that the aforementioned conditions are over the unit groups. This guarantees that the treatment cannot be inferred from the latent representation trajectory, and that the interference is not predictable when the treatment is combined with latent representation. We implement this balancing goal through domain adversarial learning~\\cite{ganin2016domain}, in which the treatment is treated as binary domains and the interference is treated as continuous domains~\\cite{WangHK20}. Specifically, we use the gradient reversal layer proposed in~\\cite{ganin2016domain}, denoted as $r(\\cdot)$, to adversarially optimize the latent representation trajectory at every observed time, making it agnostic towards the treatments and interference. \n\n\\textbf{Treatment Balancing.} Formally, the predicted treatment is $\\hat{\\mathbf{A}}_i^t = d_\\mathbf{A}\\left(r(\\mathbf{Z}_i^t)\\right)$, where the $d_\\mathbf{A}$ is a neural network that attempts to recover the treatment from latent representation. The gradient reversal layer $r(\\cdot)$ does nothing in the forward pass, but reverses the gradients in the back-propagation. This way, a min-max game is created in which $d_\\mathbf{A}$ aims to minimize the treatment prediction loss, while the latent representation learner in \\odelower strives to maximize it, as formalized in the following:\n\\begin{align}\\label{eqn:a_pred}\n    L^{\\langle A \\rangle} = \\underset{d_\\mathbf{A}^j}{\\text{min}}\\ \\underset{f,\\phi}{\\text{max}}\\frac{1}{N}\\frac{1}{T}\\sum_i^N\\sum_t^T\\sum_{j\\in\\{0,1\\}}\\mathds{1}_{(\\mathbf{A}_i^t=j)}-\\log\\left(d_\\mathbf{A}^j\\left(r(\\mathbf{Z}_i^t)\\right)\\right),\n\\end{align}\nwhere $d_\\mathbf{A}^j$ represents the logits of $d_\\mathbf{A}(\\cdot)$ for predicting treatment $j$. We then provide a theoretical analysis to justify the capability of $L^{\\langle A \\rangle}$ to attain balanced representations in the following.\n\\begin{theorem}\\label{theorem:t1}\n Let $j\\in \\{0,1\\}$ be the binary treatment values, and let $N$ and $T$ denote the number of units and observed timestamp lengths, respectively. Let $P_j^t = P(\\mathbf{Z}^t\\mid \\mathbf{A}^t=j)$, be the distribution of latent representation $\\mathbf{Z}^t$ for the group of units with treatments $j$ at time $t$. Let $f$, $\\phi$, $d_\\mathbf{A}^j$ be the initial state encoder, the ODE function of \\odelower, and logits of predicting treatment $j$. The necessary and sufficient condition for the min-max game in Eq.~(\\ref{eqn:a_pred}) to be optimal is $P_0^t = P_1^t, \\forall t\\in\\left(t_0,t_1\\cdots t_T\\right)$.\n \\end{theorem}\nTheorem~\\ref{theorem:t1} suggests that the condition to obtain global optimum of Eq.~(\\ref{eqn:a_pred}) is $P(\\mathbf{Z}^t|\\mathbf{A}^t=0) = (\\mathbf{Z}^t|\\mathbf{A}^t=1)$. Therefore, by optimizing $L^{\\langle A \\rangle}$ in Eq.~(\\ref{eqn:a_pred}), we can ensure the latent representation trajectory $\\mathbf{Z}^t$ is balanced with respect to treatments. In other words, $\\mathbf{Z}^t$ is not predictive of  $\\mathbf{A}^t$.  We prove Theorem~\\ref{theorem:t1} in Appendix.~\\ref{sec:proof_t1}.\n\n\n\n\n\n\n \n\\textbf{Interference Balancing. } The interference $\\mathbf{G}_i^t$ is continuous, we thus adapt the continuous domain adversarial learning~\\cite{WangHK20} to achieve the interference balancing. Similar to the binary case, we consider the continuous interference as continuous domains, and use the gradient reversal layer $r(\\cdot)$ to build a min-max game on interference prediction as follows:\n\\begin{align}\\label{eqn:pred_g}\n    L^{\\langle G \\rangle} &= \\underset{d_\\mathbf{G}}{\\text{min}}\\ \\underset{f,\\phi}{\\text{max}}\\frac{1}{N}\\frac{1}{T}\\sum_i^N\\sum_t^T\\left(d_\\mathbf{G}\\left(r([\\mathbf{Z}_i^t,\\mathbf{A}_i^t])\\right)-\\mathbf{G}_i^t\\right)^2    \n\\end{align}\nwhere $d_\\mathbf{G}$ is the interference predictor which is parameterized by neural networks, and $[\\cdot,\\cdot]$ is the concatenation operation. In the following, we also theoretically demonstrate that $L^{\\langle G \\rangle}$ is able to achieve the interference balancing objective.\n\n\n\\begin{theorem}\\label{theorem:t2}\nLet $f$, $\\phi$, $d_\\mathbf{G}$ be the initial state encoder, the ODE function of \\odelower, and the interference predictor. The necessary and sufficient condition for min-max game in Eq.~(\\ref{eqn:pred_g}) to be optimal is $P(\\mathbf{Z}^t,\\mathbf{A}^t|\\mathbf{G}^t=g')$ is identical for any $g'$.\n\\end{theorem}\n\nTheorem.~\\ref{theorem:t2} indicates that if $\\mathbb{E}([\\mathbf{Z}^t,\\mathbf{A}^t]\\mid \\mathbf{G}^t)$ is identical for any $\\mathbf{G}^t=g'$, Eq.~(\\ref{eqn:pred_g}) achieves optimum. Therefore, it is sufficient to balance the combination of representations and treatments with respect to interference $\\mathbf{G}^t$ by optimizing the objective function $L^{\\langle G \\rangle}$. We show the proof of Theorem~\\ref{theorem:t2} in Appendix.~\\ref{sec:proof_t2}.\n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Training of \\model",
                    "content": "\n\\textbf{Objective Function. }The overall objective function of \\model is formalized in the following:\n\\begin{align}\n    L = L^{\\langle Y \\rangle} + \\alpha_{\\mathbf{A}} L^{\\langle A \\rangle} + \\alpha_{\\mathbf{G}} L^{\\langle G \\rangle},\n\\end{align}\nwhere coefficients $\\alpha_{\\mathbf{A}}$, $\\alpha_{\\mathbf{G}}$ are the strengths of the treatment balancing and interference balancing, respectively. By adversarially optimizing $L$, the latent representation trajectory $\\mathbf{Z}_i^t$ is able to predict the outcome trajectory $\\mathbf{Y}_i^t$ while remaining invariant to the treatments $\\mathbf{A}_i^t$ and interference $\\mathbf{G}_i^t$ (combined with treatments), which enables the unbiased counterfactual outcome estimation in multi-agent dynamical systems. \n\n\\textbf{Alternative Training as Trade-Off. } In practice, we find that directly training \\model with the overall loss function $L$ may not be stable as $L^{\\langle A \\rangle}$ and $L^{\\langle G \\rangle}$ could hinder the ability of latent representation trajectory $\\mathbf{Z}_i^t$ to predict the outcome. Therefore, we trade-off the training of \\model in an alternative manner between $L$ and $L^{\\langle Y \\rangle}$, to ensure that $\\mathbf{Z}_i^t$ is capable of predicting outcomes. Specifically, we switch the training iterations between $L$ and $L^{\\langle Y \\rangle}$ with a ratio of $K$, i.e., $\\frac{Iter_{L}}{Iter_{L^{\\langle Y \\rangle}}} = K$, where $Iter$ means the number of training iterations and $K$ is a tunable hyperparameter. We elaborate on the training procedure in Appendix.~\\ref{sec:pseudo}.\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\n",
                "subsection 5.1": {
                    "name": "Experimental Settings",
                    "content": "\n\\textbf{Dataset. } In observational data, we only have factual outcomes but not counterfactual outcomes. Therefore, we use semi-synthetics data to evaluate \\model as in~\\cite{ma2022learning,guo2020learning,veitch2019using}. That is, we use two real graphs Flickr and BlogCatalog~\\cite{guo2020learning,chu2021graph,ma2021deconfounding} and use a Pharmacokinetic-Pharmacodynamic (PK-PD) model~\\cite{goutelle2008hill} to simulate the continuous trajectory of treatments and potential outcomes~\\cite{BicaAJS20,SeedatIBQS22}. The data simulation mimics the vaccine example in the real world. We introduce  the data simulation process in detail in Appendix.~\\ref{sec:exp_setting}.  \n\n\\textbf{Metric. } We focus on counterfactual outcomes estimation in this paper, which is a continuous value. Therefore, we use mean square errors (MSE) as our metric to evaluate the performance of our model, which is formalized as $MSE:= \\frac{1}{N}\\frac{1}{T}\\sum_i^N\\sum_t^T\\left(\\hat{\\mathbf{Y}}_i^t - \\mathbf{Y}_i^t\\right)^2$.\n\n\\textbf{Baselines. } The scope of our model is in continuous-time causal inference, therefore we compare \\model with the following baselines: \\textbf{CDE}~\\cite{kidger2020neural}: Ordinary differential equations with external inputs to adjust the continuous trajectory. \\textbf{GraphODE}~\\cite{huang2020learning} Ordinary differential equations model with graph neural networks (GNNs) based ODE functions. \\textbf{TE-CDE}~\\cite{SeedatIBQS22}:  the state-of-the-art model for continuous-time counterfactual outcomes estimation based on neural controlled differential equations (NeuralCDE).\n\n\\textbf{Implementation. } The parameters of \\model are set as follows: the dimension of latent representations is $64$; the ODE solver is the Euler method; the balancing degrees are $\\alpha_{\\mathbf{A}}=\\alpha_{\\mathbf{G}}=0.5$. For training hyperparameters, the learning rate is $0.0001$; the default alternative training ratio $K$ is $4$. We train the model $5000$ epochs and select the best model according to the performance on the validation set. The parameters are optimized by Adam~\\cite{kingma2014adam}. We run all experiments on a Lambda Labs instance with one A100 GPU.\n\n\n\n\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Can \\model Deliver Accurate Estimations of Counterfactual Outcomes in Multi-Agent Dynamical Systems?",
                    "content": "\\label{sec:results}\nWe compare \\model to three lines of models: 1) Continuous-time dynamical prediction models CDE and GraphODE. Note that these baselines are not causal models since they only preserve the dynamical statistical associations.  2) Continuous-time causal inference model TE-CDE. But it is not capable of capturing the mutual dependencies between units in multi-agent dynamical systems. 3) Variants of \\model. We consider three variants: \\model-N means there is no any balancing ($\\alpha_{\\mathbf{A}}=\\alpha_{\\mathbf{G}}=0$); \\model-T denotes balancing only w.r.t. treatments ($\\alpha_{\\mathbf{A}}=1$, $\\alpha_{\\mathbf{G}}=0$); \\model-I means balancing only w.r.t. interference ($\\alpha_{\\mathbf{A}}=0$, $\\alpha_{\\mathbf{G}}=1$). For a multi-agent dynamical system with $N$ nodes and length-$A$ treatment trajectories, the total number of possible treatments for all nodes is $O(A\\cdot2^N)$. Therefore, it is intractable to enumerate all treatment combinations. To this end, we randomly flip $50\\%$ of all observed treatments in each experiment. We estimate five-step (timestamp) ahead counterfactual outcomes and report estimation errors in Table.~\\ref{tb:cf_in}. Generally, \\model and the variants outperform the baselines by substantial margins. It is noteworthy that, despite being a causal model, TE-CDE performs clearly worse than the family of \\model, because it ignores the mutual influence between units. This underscores our motivation to address this unique challenge in multi-agent dynamical systems. We also note \\model-N is generally the weakest estimator among all variants, confirming the effectiveness of our proposed balancing objectives.\n\n\n\n% \\vspace{-0.5cm}\n\n\n\n\\textbf{Why Does \\model-T Show Superior Performance Than \\model on BlogCatalog Dataset? } On BlogCatalog dataset, we observe that balancing solely with respect to treatments (\\model-T) yields the lowest estimation errors, even outperforming balancing both treatments and interference (\\model). To understand this phenomenon, we project all units' latent representations $\\mathbf{Z}^t$ into 2-D embeddings using T-SNE~\\cite{van2008visualizing} and color these 2-D points by their corresponding interference in Fig.~\\ref{fig:why_bad}. Specifically, we  compare the units' interference before and after flipping the treatments. Compared to Flickr, we notice that in BlogCatalog 1) the latent representations are already comparatively more balanced before flipping the treatments, and 2) the units' interference does not change significantly after flipping the treatments. This suggests that balancing solely with respect to treatments might be sufficient in the BlogCatalog dataset. Actually, since we use the same data simulation protocol for Flickr and BlogCatalog, this difference in interference distribution is expected to be caused by their distinct graph structures. Specifically, the average and standard derivation of node degrees of the two datasets are Flickr: $2.0\\pm1.7$; BlogCatalog: $30.7\\pm25.1$. Intuitively, the interference of high degrees nodes is more resistant to flipping a random portion of their neighbors, which is pretty common among nodes in BlogCatalog. We provide further breakdown studies to better understand how node degrees affect counterfactual outcomes estimation in Sec.~\\ref{sec:degree_exp}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "How Does \\model Respond to The Flipping of Counterfactual Treatments?",
                    "content": "\n\n\nIn the above experiments, the default treatment flipping ratio is set at $50\\%$. It's intriguing to investigate how \\model reacts to different flipping ratios, as this would indicate the degree of difference between factual and counterfactual outcomes in terms of treatments. To this end, we set the flip ratio as $[25\\%,50\\%,75\\%,100\\%]$, and present the results of \\model and its variants under these settings in Fig.~\\ref{fig:flip}. As expected, all models perform worse as the flip ratio increases, since the counterfactual treatments diverge further from the observed factual treatments. However, we observe that with balancing objectives, the error of \\model increases generally slowly, highlighting the need for balancing objectives.\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "How Does \\model Respond to Different Confounding Degrees?",
                    "content": "\\label{sec:degree}\n\n\nIn data simulation, we use coefficients $\\gamma_a$, $\\gamma_f$ to control the degree of the time-dependent and neighbor confounding bias. With larger values of these coefficients, the confounding bias is more severe, leading to increasingly imbalanced data. To study how \\model works under varying confounding degrees, we set $\\gamma_a=\\gamma_f = \\gamma$, where $\\gamma\\in[0,1,2,3,4,5,6,7,8,9,10]$, and present the counterfactual outcomes estimation errors of \\model under these conditions in Fig.\\ref{fig:confounding}. The errors increase as the confounding bias becomes more severe, but the rate of increase is relatively smooth, particularly on Flickr dataset. This implicates \\model's robustness against high degree confounding bias. Additionally, \\model produces low errors when there is no confounding bias ($\\gamma_a=\\gamma_f=0$), which demonstrates the compatibility of \\model with such settings.\n\n\n\n"
                },
                "subsection 5.5": {
                    "name": "How Does Graph Structure Impact Counterfactual Outcomes Estimation?",
                    "content": "\\label{sec:degree_exp}\nAs discussed in Sec.~\\ref{sec:results}, the graph structure affects \\model's performance on counterfactual outcomes estimation. To gain deeper insights into this phenomenon, we break down the estimation errors on BlogCatalog dataset according to node degrees in Table.~\\ref{tb:degree}. Interestingly, we find that \\model's counterfactual estimation errors decrease as the node degrees become higher. Intuitively, this might also be because the interference of high-degree nodes is more stable. However, in this paper, we do not have a theoretical understanding of the relationships between estimation errors and node degrees. We leave this line of research in future study.\n\n\n\\vspace{-0.5cm}\n"
                },
                "subsection 5.6": {
                    "name": "Can \\model Be Generalized to New Multi-Agent Dynamical Systems?",
                    "content": "\nStandard counterfactual outcome estimations are typically conducted on units whose factual outcomes have been observed. However, the estimation of the potential outcomes on \\emph{new} multi-agent dynamical systems is also of great importance. For instance, to predict the effects of an initial vaccine distribution strategy for a new community. To assess \\model's ability to generalize to new systems, i.e., new graphs, we split the original graph into three subgraphs, denoted as training/validation/testing graphs (details in Appendix.~\\ref{sec:exp_setting}). We train our model on the training graph and evaluate its potential outcome estimation on the testing graph. We report the results in Table.~\\ref{tb:cf_out}. We note that the performance of \\model and the variants on new graphs are also generally better than baselines, which is consistent with the estimation of the counterfactual outcomes within the same graph (Sec.~\\ref{sec:results}). This demonstrates our model's generalizability to new multi-agent dynamical systems.   \n\n\n\n\n% \\vspace{-20pt}\n\n\n\n\n\n\n\n"
                },
                "subsection 5.7": {
                    "name": "How Does Alternative Training Affect \\model?",
                    "content": "\n\n\\model uses an alternative training strategy to trade off the latent representation balancing and potential outcome prediction. We examine how this trade-off is performed under varying alternative ratios $K$, where $K=\\frac{Iter_{L}}{Iter_{L^{\\langle Y \\rangle}}}$ represents the alternating training between the overall loss $L$ and the outcome prediction loss $L^{\\langle Y \\rangle}$. Fig.~\\ref{fig:embed} shows the 2-D T-SNE projections of latent representations and their corresponding counterfactual estimation errors for different K values. We note that compared to solely training on $L^{\\langle Y \\rangle}$ (i.e., no balancing), training with $L$ is able to force the embeddings more balanced, suggesting the effectiveness of our proposed domain adversarial learning based balancing objectives. In addition, with a smaller $K$, \\model achieves better estimation errors, while a bigger $K$ leads to more balanced latent representations. These results confirm that our alternative training is able to trade off between latent representation balancing and potential outcome prediction. In practice, choosing an appropriate value of $K$ is expected to be determined through empirical analysis for each dataset.\n\n\n\n\n\n\n% \\vspace{-0.1cm}\n\n"
                },
                "subsection 5.8": {
                    "name": "Case Study: When \\model Is Good, and When It Is Not.",
                    "content": "\n\nTo intuitively understand how \\model works in estimating counterfactual outcomes and to study when \\model would fail, we sample one successful unit and one failure unit from Flickr dataset. We draw their factual outcomes, counterfactual outcomes, and the estimations made by \\model and \\model-N (without balancing) in Fig.~\\ref{fig:case_study}. In the successful case, the estimate by \\model is able to conform to the counterfactual treatment trajectory, while \\model-N still follows the factual trajectory. This shows the effectiveness of our proposed balancing objectives. However, \\model also makes mistakes. In the failure case, its estimate fails to catch up with the counterfactual outcome trajectory, yielding a non-trivial error. We speculate that this is because the counterfactual outcome of this unit is quite distinct from the factual one in terms of data scale, making counterfactual estimation more difficult.  \n\n\n\n\n\n"
                },
                "subsection 5.9": {
                    "name": "How Hyperparamters Affect \\model?",
                    "content": "\n\nThe two balancing objectives are core to making \\model causal. Therefore, we finally study the impact of their degrees, represented by $\\alpha_{\\mathbf{A}}$ and $\\alpha_{\\mathbf{G}}$ in the loss function, on the model performance. We test $\\alpha_{\\mathbf{A}}$ and $\\alpha_{\\mathbf{G}}$ values evenly ranging from $[0,0,1.0]$, and present the counterfactual outcomes estimation errors for each combination of $\\alpha_{\\mathbf{A}}$ and $\\alpha_{\\mathbf{G}}$ in Fig.~\\ref{fig:params}.  Our results show that the errors are relatively higher when both $\\alpha_{\\mathbf{A}}$ and  $\\alpha_{\\mathbf{G}}$ are in low values, i.e., light balancing. On the other hand, with larger values, the estimation errors generally become lower, but with high variance. This indicates the effectiveness of the balancing objectives but also highlights the instability of domain adversarial learning based balancing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\nIn this paper, we study continuous-time counterfactual outcomes estimation in multi-agent dynamical systems, where units interact with each other. To this end, we propose \\model, a novel causal model based on GraphODE to enable continuous potential outcomes prediction, and domain adversarial learning to remove confounding bias. We provide both theoretical justification and empirical analyses to demonstrate the effectiveness of our model. One limitation of \\model is it needs the assumption of strong ignorability for multi-agent dynamical systems, which is not testable in practice. Recent studies relax this assumption by inferring latent proxy variables~\\cite{wang2019blessings}, which could be a potential solution.\n% \\begin{acks}\nThis work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037,  NASA, SRC, Okawa Foundation Grant, Amazon Research Awards, Cisco research grant, Picsart Gifts, and Snapchat Gifts.\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\clearpage\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{reference}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\appendix\n% \\clearpage\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n\n\n\n",
                "subsection 7.1": {
                    "name": "Proofs of Theorems",
                    "content": "\n\n",
                    "subsubsection 7.1.1": {
                        "name": "theorem:t1",
                        "content": "\\label{sec:proof_t1}\n\n\\begin{theorem_copy}\n Let $j\\in \\{0,1\\}$ be the binary treatment values, and let $N$ and $T$ denote the number of units and observed timestamp lengths, respectively. Let $P_j^t = P(\\mathbf{Z}^t\\mid \\mathbf{A}^t=j)$, be the distribution of latent representation $\\mathbf{Z}^t$ for the group of units with treatments $j$ at time $t$. Let $f$, $\\phi$, $d_\\mathbf{A}^j$ be the initial state encoder, the ODE function of \\odelower, and logits of predicting treatment $j$. The necessary and sufficient condition for the min-max game in Eq.~(\\ref{eqn:a_pred}) to be optimal is $P_0^t = P_1^t, \\forall t\\in\\left(t_0,t_1\\cdots t_T\\right)$.\n \\end{theorem_copy}\n  The proof of Theorem~\\ref{theorem:t1} follows \\cite{BicaAJS20, MelnychukFF22} and consists of two steps: to find the optimal $d_\\mathbf{A}^{j}$ while fixing $f$ and $\\phi$, and then to prove the optimal $f$ and $\\phi$ while fixing $d_\\mathbf{A}^{j}$ can balance the latent representations, i.e., $P_0^t = P_1^t$. The first step is given by Proposition~\\ref{pro:1}.\n\\begin{proposition}\\label{pro:1}\n(Proposition 1 in~\\cite{MelnychukFF22}) Let $\\alpha_{j}=P(A^t=j)$. When the initial state encoder $f$ and ODE function $\\phi$ are fixed, the optimal $d_\\mathbf{A}^j$ at time $t$ is:\n\\begin{equation}\n    {d_\\mathbf{A}^{j}}^* = \\frac{\\alpha_{j} P_j^t}{\\sum_{j'\\in\\{0,1\\}} \\alpha_{j'} P_{j'}^t}.\n\\end{equation}\n\\end{proposition}\n\\begin{proof}\n When fixing $f$ and $\\phi$, ${d_\\mathbf{A}^{j}}^*$ is obtained by:\n \\begin{align}\n    {d_\\mathbf{A}^{j}}^* &= \\underset{d_\\mathbf{A}^j}{\\text{argmin}}\\sum_{j\\in\\{0,1\\}}\\mathds{1}_{(\\mathbf{A}^t=j)}-\\log\\left(d_\\mathbf{A}^j\\left(r(\\mathbf{Z}^t)\\right)\\right),\\label{eqn:optim_da}\\\\\n     &\\qquad\\qquad\\text{subject to } \\sum_{j\\in\\{0,1\\}} d_\\mathbf{A}^j\\left(r(\\mathbf{Z}^t)\\right)=1\\label{eqn:da_cons},\n \\end{align}\nwhere Eq.~(\\ref{eqn:optim_da}) is adapted from Eq.~(\\ref{eqn:a_pred}). Note Eq.~(\\ref{eqn:optim_da}) can be applied to any $i$ and $t$ in Eq.~(\\ref{eqn:a_pred}), so we disregard the expectation with respect to them. Let $\\alpha_{j}=P(A^t=j)$ and $P_j^t = P(\\mathbf{Z}^t\\mid \\mathbf{A}^t=j)$. Then Eq.~(\\ref{eqn:optim_da}) can be rewritten as: \n\\begin{align}\n        {d_\\mathbf{A}^{j}}^* &= \\underset{d_\\mathbf{A}^j}{\\text{argmin}}\\sum_{j\\in\\{0,1\\}}-\\mathbb{E}_{\\mathbf{Z}^t\\sim P_j^t}\\alpha_{j}\\log\\left(d_\\mathbf{A}^j\\left(r(\\mathbf{Z}^t)\\right)\\right)\\label{eqn:optim_da2}\\\\\n        & = \\underset{d_\\mathbf{A}^j}{\\text{argmin}}\\sum_{j\\in\\{0,1\\}}-\\int_{\\mathbf{Z'}^t}\\alpha_{j}\\log\\left(d_\\mathbf{A}^j\\left(r({\\mathbf{Z'}^t})\\right)\\right)P_j^t d\\mathbf{Z'}^t.\\label{eqn:optim_da3}\n\\end{align}\nWe can also take pointwise optimization for any $\\mathbf{Z'}^t$ in Eq.~(\\ref{eqn:optim_da3})  Then by combining Equation (\\ref{eqn:optim_da3}) with the constraint in Equation (\\ref{eqn:da_cons}) and using Lagrange multipliers, we have: \n\\begin{align}\n    {d_\\mathbf{A}^{j}}^* &= \\underset{d_\\mathbf{A}^j}{\\text{argmin}}\\sum_{j\\in\\{0,1\\}}-\\alpha_{j}\\log\\left(d_\\mathbf{A}^j\\left(r({\\mathbf{Z'}^t})\\right)\\right)P_j^t \\nonumber\\\\\n    &+ \\lambda\\left(\\sum_{j\\in\\{0,1\\}} d_\\mathbf{A}^j\\left(r(\\mathbf{Z}^t)\\right)-1\\right)\\label{eqn:lagrange}.\n\\end{align}\nLet $J=\\sum_{j\\in\\{0,1\\}}-\\alpha_{j}\\log(d_\\mathbf{A}^j(r({\\mathbf{Z'}^t})))P_j^t + \\lambda(\\sum_{j\\in\\{0,1\\}} d_\\mathbf{A}^j(r(\\mathbf{Z}^t))-1)$ be the objective in Eq.~(\\ref{eqn:lagrange}) The optimal values can be obtained by taking partial gradients $\\frac{\\partial J}{\\partial d_\\mathbf{A}^{j}} = 0$ and $\\frac{\\partial J}{\\lambda} = 0$, respectively. By computing them jointly we can obtain ${d_\\mathbf{A}^{j}}^* = \\frac{\\alpha_{j}P_j^t}{\\sum_{j'\\in\\{0,1\\}} \\alpha_{j'}P_{j'}^t}$.\n \\end{proof}\n\nThe second step is to prove Theorem.~\\ref{theorem:t1} that the optimal $f$ and $\\phi$ can obtain balanced representations with respect to treatments.\n\\begin{proof}\nWith Proposition~\\ref{pro:1}, we can fix the optimal ${d_\\mathbf{A}^{j}}^*$ and find the condition where Eq.~(\\ref{eqn:a_pred}) achieves optimum. Putting ${d_\\mathbf{A}^{j}}^*$ into the objective in Eq.~(\\ref{eqn:a_pred}) and applying similar simplifications as in Eq.~(\\ref{eqn:optim_da3}) and Eq.~(\\ref{eqn:lagrange}), we have:\n\\begin{align}\n    f^*,\\phi^* &= \\underset{f,\\phi}{\\text{argmax}}\\sum_{j\\in\\{0,1\\}}-\\mathbb{E}_{\\mathbf{Z}^t\\sim P_j^t}\\log\\left(\\frac{\\alpha_{j}P_j^t}{\\sum_{j'\\in\\{0,1\\}} \\alpha_{j'}P_{j'}^t}\\right)\\\\\n    &= \\underset{f,\\phi}{\\text{argmin}}\\sum_{j\\in\\{0,1\\}}\\mathbb{E}_{\\mathbf{Z}^t\\sim P_j^t}\\log\\left(\\frac{P_j^t}{\\sum_{j'\\in\\{0,1\\}} \\alpha_{j'}P_{j'}^t}\\right) + \\log(\\alpha_{j})\\\\\n    & = \\underset{f,\\phi}{\\text{argmin}}\\sum_{j\\in\\{0,1\\}}\\text{KL}\\left(P_j^t \\Bigg| \\Bigg| {\\sum_{j'\\in\\{0,1\\}} \\alpha_{j'}P_{j'}^t}\\right)+ \\log(\\alpha_{j}),\\\\\n\\end{align}\nwhere $\\text{KL}(\\cdot||\\cdot)$ is the KL divergence. Note that $\\sum_{j\\in\\{0,1\\}}\\log(\\alpha_{j})$ is constant in observation data. $\\text{KL}(\\cdot||\\cdot)\\geq0$ and it reaches $0$ when the two operands are equal. Therefore, to have $f^*,\\phi^*$, for $j\\in\\{0,1\\}$, we have $P_0^t = P_1^t ={\\sum_{j'\\in\\{0,1\\}} \\alpha_{j'}P_{j'}^t}$. Therefore, the optimal $f^*,\\phi^*$ are those who achieve $P_0^t = P_1^t$. We can apply this to all the $N$ units and all the $T$ observed timestamps to obtain the global optimum of Eq.~(\\ref{eqn:a_pred}), which concludes the proof of Theorem~\\ref{theorem:t1}.    \n\\end{proof}\n\n\n\n\n"
                    },
                    "subsubsection 7.1.2": {
                        "name": "theorem:t2",
                        "content": "\\label{sec:proof_t2}\n\\begin{theorem_copy}\nLet $f$, $\\phi$, $d_\\mathbf{G}$ be the initial state encoder, the ODE function of \\odelower, and the interference predictor. The necessary and sufficient condition for min-max game in Eq.~(\\ref{eqn:pred_g}) to be optimal is $P(\\mathbf{Z}^t,\\mathbf{A}^t|\\mathbf{G}^t=g')$ is identical for any $g'$.\n\\end{theorem_copy}\n\n\n\nThe proof of Theorem~\\ref{theorem:t2} follows~\\cite{WangHK20}. Similar to Theorem~\\ref{theorem:t1}'s proof, it first finds the optimum of $d_\\mathbf{G}$, and then proves that the optimal $f$ and $\\phi$ can balance the representations with respect to interference. We first restate the Lemma 4.1 in~\\cite{WangHK20} in Proposition.~\\ref{pro:2}.\n\n\\begin{proposition}\\label{pro:2}\n(Lemma 4.1 in~\\cite{WangHK20}) Let $\\mathbf{C}^t = [\\mathbf{Z}^t,\\mathbf{A}^t]$ be the concatenation of $\\mathbf{Z}^t$ and $\\mathbf{A}^t$. When fixing initial state encoder $f$ and ODE function $\\phi$, the optimal $d_\\mathbf{G}$ at time $t$ is:\n\\begin{equation}\n    d_\\mathbf{G}^* = \\mathbb{E}_{\\mathbf{G}^t\\sim p(\\mathbf{G}^t|\\mathbf{C}^t)}(\\mathbf{G}^t)\\label{eqn:find_dg}.\n\\end{equation}\n\\end{proposition}\n\\begin{proof}\n    Eq.~(\\ref{eqn:find_dg}) is adapted from Eq.~(\\ref{eqn:pred_g}). We disregard the expectation with respect to $i$ and $t$ since Eq.~(\\ref{eqn:find_dg}) is applicable to any $i$ and $t$. If fixing $f$ and $\\phi$, the optimal $d_\\mathbf{G}^*$ is given by:\n\\begin{align}\n    d_\\mathbf{G}^* &=\\underset{d_\\mathbf{G}}{\\text{argmin}} \\ \\mathbb{E}_{(\\mathbf{Z}^t,\\mathbf{A}^t,\\mathbf{G}^t)\\sim p(\\mathbf{Z}^t,\\mathbf{A}^t,\\mathbf{G}^t)}\\left(d_\\mathbf{G}\\left(r([\\mathbf{Z}^t,\\mathbf{A}^t])\\right)-\\mathbf{G}^t\\right)^2\\\\\n    &= \\underset{d_\\mathbf{G}}{\\text{argmin}} \\ \\mathbb{E}_{(\\mathbf{C}^t,\\mathbf{G}^t)\\sim p(\\mathbf{C}^t,\\mathbf{G}^t)}\\left(d_\\mathbf{G}\\left(r(\\mathbf{C}^t)\\right)-\\mathbf{G}^t\\right)^2\\\\\n    &= \\underset{d_\\mathbf{G}}{\\text{argmin}} \\ \\mathbb{E}_{\\mathbf{C}^t\\sim p(\\mathbf{C}^t)} \\mathbb{E}_{\\mathbf{G}^t\\sim p(\\mathbf{G}^t\\mid \\mathbf{C}^t)}\\left(d_\\mathbf{G}\\left(r(\\mathbf{C}^t)\\right)-\\mathbf{G}^t\\right)^2.\\label{eqn:optim_dg3}\n\\end{align}\nAs the quadratic expansion in~\\cite{WangHK20}, the optimal interference predictor is $d_\\mathbf{G}^* = \\mathbb{E}_{\\mathbf{G}^t\\sim p(\\mathbf{G}^t\\mid \\mathbf{C}^t)}(\\mathbf{G}^t)$. \n\\end{proof}\n\nHere we introduce and reformulate Theorem 4.1 in~\\cite{WangHK20} as Lemma.~\\ref{lemma:1}. \n\n\\begin{lemma}\\label{lemma:1}\n(Theorem 4.1 in~\\cite{WangHK20}) Given $\\mathbb{E}_{x}\\mathbb{V}(y\\mid x)$ where $\\mathbb{V}$ denotes variance, its global optimum can be achieved if and only if for any $x$, $\\mathbb{E}(y\\mid x) = \\mathbb{E}(y)$.\n\\end{lemma}\n\nWith Proposition.~\\ref{pro:2} and Lemma.~\\ref{lemma:1}, we can prove Theorem.~\\ref{theorem:t2}. \n\\begin{proof}\nFixing the optimal $d_\\mathbf{G}^*$ in Proposition.~\\ref{pro:2}, the optimal $f$ and $\\phi$ for objective Eq.~(\\ref{eqn:optim_dg3}) is:\n\\begin{align}\n    f^*,\\phi^* &= \\underset{f,\\phi}{\\text{argmax}}\\ \\mathbb{E}_{\\mathbf{C}^t\\sim p(\\mathbf{C}^t)} \\mathbb{E}_{\\mathbf{G}^t\\sim p(\\mathbf{G}^t\\mid \\mathbf{C}^t)}\\left(\\mathbb{E}_{\\mathbf{G}^t\\sim p(\\mathbf{G}^t|\\mathbf{C}^t)}(\\mathbf{G}^t)-\\mathbf{G}^t\\right)^2\\\\\n    & = \\underset{f,\\phi}{\\text{argmax}}\\ \\mathbb{E}_{\\mathbf{C}^t\\sim p(\\mathbf{C}^t)}\\mathbb{V}\\left(\\mathbf{G}^t\\mid \\mathbf{C}^t\\right)\\label{eqn:variance}.\n\\end{align}\nEq.~(\\ref{eqn:variance}) has the same form as the equation in Lemma.~\\ref{lemma:1}. Then substituting $x=\\mathbf{C}^t$ and $y=\\mathbf{G}^t$ in Lemma.~\\ref{lemma:1}, we have $\\mathbb{E}(\\mathbf{G}^t\\mid \\mathbf{C}^t) = \\mathbb{E}(\\mathbf{G}^t)$. In other words, $\\mathbf{G}^t \\ind \\mathbf{C}^t$. Therefore, we can also use the inverse form that $\\mathbb{E}(\\mathbf{C}^t) = \\mathbb{E}(\\mathbf{C}^t\\mid \\mathbf{G}^t) = \\mathbb{E}([\\mathbf{Z}^t,\\mathbf{A}^t]\\mid \\mathbf{G}^t)$ for any $\\mathbf{G}^t$, which concludes the proof of Theorem.~\\ref{theorem:t2}.\n\\end{proof}\n\n\n\n\n\n\n"
                    }
                },
                "subsection 7.2": {
                    "name": "Pseudo-Code of \\model Training ",
                    "content": "\\label{sec:pseudo}\nThe pseudo-code for training \\model is shown in Algorithm.~\\ref{algo:training}. Specifically, we use an alternative training trick to trade-off the outcome prediction and adversarial balancing. \n\n% \\vspace{-0.5cm}\n\\begin{algorithm}[H]\n\\caption{The optimization process of \\model}\n\\begin{algorithmic}\n    \\State \\textbf{Input:} Multi-agaent dynamical system ${\\mathcal{G}}$; the observational data $\\left(\\left( \\mathbf{X}^t, \\mathbf{A}^t, \\mathbf{Y}^t \\right) \\cup \\mathbf{V}\\right)$; observed timestamps $\\left(t_0,t_1\\cdots t_T\\right)$.\n    \\State \\textbf{Output:} Trained initial state encoder $f(\\cdot)$; \\odelower function $\\phi(\\cdot)$; outcome decoder $d_\\mathbf{Y}(\\cdot)$; treatment predictor $d_\\mathbf{A}(\\cdot)$; interference predictor $d_\\mathbf{G}(\\cdot)$.\n    \\State \\textbf{Training:}\n    \\State Initialize $f(\\cdot)$, $\\phi(\\cdot)$, $d_\\mathbf{Y}(\\cdot)$, $d_\\mathbf{A}(\\cdot)$, $d_\\mathbf{G}(\\cdot)$;\n\\For {w = 1, 2, ..., W } \\Comment{\\com{Train W iterations}}\n    \\If {w\\%(K+1) = 0}\n    \\Comment{\\com{Train with $L^{\\langle Y \\rangle}$ for 1 steps}}\n        \\State Compute $L^{\\langle Y \\rangle}$;\n        \\State One step optimization for $f(\\cdot)$, $\\phi(\\cdot)$, and $d_\\mathbf{Y}(\\cdot)$: \n        \\State $\\theta_{f}^{(w+1)} = \\theta_{f}^{(w)}-\\eta\\nabla_{\\theta_{f}} L^{\\langle Y \\rangle}$; \\Comment{\\com{$\\eta$ is learning rate}}\n        \\State $\\theta_{\\phi}^{(w+1)} = \\theta_{\\phi}^{(w)}-\\eta\\nabla_{\\theta_{\\phi}} L{\\langle Y \\rangle}$; \n        \\State $\\theta_{d_\\mathbf{Y}}^{(w+1)} = \\theta_{d_\\mathbf{Y}}^{(w)}-\\eta\\nabla_{\\theta_{d_\\mathbf{Y}}} L^{\\langle Y \\rangle}$; \n    \\Else \\Comment{\\com{Train with $L$ for $K$ steps}}\n        \\State Compute $L$;\n        \\State One step optimization for $f(\\cdot)$, $\\phi(\\cdot)$, $d_\\mathbf{Y}(\\cdot)$, $d_\\mathbf{A}(\\cdot)$, $d_\\mathbf{G}(\\cdot)$: \n        \\State $\\theta_{d_\\mathbf{A}}^{(w+1)} = \\theta_{d_\\mathbf{A}}^{(w)}-\\eta\\nabla_{\\theta_{d_\\mathbf{A}}} L$; \n        \\State $\\theta_{d_\\mathbf{G}}^{(w+1)} = \\theta_{d_\\mathbf{G}}^{(w)}-\\eta\\nabla_{\\theta_{d_\\mathbf{G}}} L$; \n        \\State $\\theta_{d_\\mathbf{Y}}^{(w+1)} = \\theta_{d_\\mathbf{Y}}^{(w)}-\\eta\\nabla_{\\theta_{d_\\mathbf{Y}}} L$\n        \\State $\\theta_{f}^{(w+1)} = \\theta_{f}^{(w)}-\\eta\\nabla_{\\theta_{f}} L^{\\langle Y \\rangle}+\\eta\\nabla_{\\theta_{f}} L^{\\langle A \\rangle}+\\eta\\nabla_{\\theta_{f}} L^{\\langle G \\rangle}$; \n        \\State \\Comment{\\com{Gradient reversal}}\n        \\State $\\theta_{\\phi}^{(w+1)} = \\theta_{\\phi}^{(w)}-\\eta\\nabla_{\\theta_{\\phi}} L^{\\langle Y \\rangle}+\\eta\\nabla_{\\theta_{\\phi}} L^{\\langle A \\rangle}+\\eta\\nabla_{\\theta_{\\phi}} L^{\\langle G \\rangle}$; \n        \\State \\Comment{\\com{Gradient reversal}}\n\\EndIf \n\\EndFor\n\\State \\textbf{Return} $f(\\cdot)$, $\\phi(\\cdot)$, $d_\\mathbf{Y}(\\cdot)$, $d_\\mathbf{A}(\\cdot)$, $d_\\mathbf{G}(\\cdot)$.\n\\end{algorithmic}\n\\label{algo:training}\n\\end{algorithm}\n\n\n"
                },
                "subsection 7.3": {
                    "name": "Experimental Settings",
                    "content": "\\label{sec:exp_setting}\n\n\\textbf{Datasets. } In observational data, we only have outcomes under one treatment trajectory but not the ground-truths of counterfactual outcomes. Therefore, we follow~\\cite{ma2022learning,guo2020learning,veitch2019using} to use semi-synthetic data to evaluate \\model. That is, the graph structure and node features are real, but treatments and potential outcomes are simulated. We use the social networks Flickr and BlogCatalog as in~\\cite{guo2020learning,chu2021graph,ma2021deconfounding} as the graph $\\mathcal{G}$. We follow these works to first encode the node features into low-dimensional embeddings ($10$-dimensional in this paper) via LDA~\\cite{blei2003latent}. We then follow~\\cite{jiang2022estimating} to use Metis~\\cite{karypis1998fast} to split the graph into training/validation/testing sets. To simulate treatment and potential outcomes over time, ~\\cite{geng2017prediction, BicaAJS20, SeedatIBQS22} use a longitudinal simulation environment, which, however, assumes the units are mutually independent. We extend it into our multi-agent dynamical systems setting by considering the neighbor confounders and interference. During the simulation, we are motivated by the vaccine's use case. Specifically, treatment $\\mathbf{A}_i^t$ denotes getting a vaccine or not at time $t$ of unit $i$. The trajectory of $\\mathbf{A}_i^t$ denotes the vaccine records over time, e.g., a unit may have a booster dose after the initial vaccine. In this case, the time-dependent covariates $\\mathbf{X}^t$ could be the health condition, static covariates $\\mathbf{V}$ could be race or educational background (assuming it does not change during the study) and potential outcome $\\mathbf{Y}^t$ could be immunity to the virus. As discussed in Sec.~\\ref{sec:problem}, the potential outcome $\\mathbf{Y}^t$ is essentially a part of $\\mathbf{X}^t$. This is a common setting in longitudinal causal inference studies~\\cite{BicaAJS20,SeedatIBQS22,MelnychukFF22}. During the simulation, we follow this protocol: the health condition $\\mathbf{X}^t_i$ has a value range $[0.1,10]$, in which a higher value means a better health condition. Meanwhile, a higher health condition means a lower probability to receive a vaccine (treatment).\n\n\n\\emph{Treatment simulation. } The treatment $\\mathbf{A}_i^t$ is affected by a unit's own time-dependent covariates $\\mathbf{X}_i^t$, static covariates $\\mathbf{V}_i$ and those of their neighbors. Let $\\mathbf{E}_i = w_a\\mathbf{V}_i$ denote the effects of static confounders on treatments, where $w_a$ is a generated parameter representing this mechanism. The treatment is then simulated by Bernoulli generator with probability the $p_i^t (a)$ of unit $i$ at time $t$:\n\n\n\\begin{align}\n\tp_i^t (a) = \\sigma \\Biggl(&\\underbrace{\\gamma_a (\\delta_a-\\bar{\\mathbf{X}}_i^t)}_{\\text {time-dependent covariates}} + \\underbrace{\\gamma_n \\Biggl(\\delta_n-(\\frac{1}{|N_i|}\\sum_{j\\in \\mathcal{N}_i}\\bar{\\mathbf{X}}_j^t)\\Biggr)}_{\\text {Neighbor time-dependent covariates}} \\nonumber \\\\\n &+\\underbrace{\\gamma_f\\mathbf{E}i}_{\\text {Static covariates}}  +\\underbrace{\\gamma_g(\\frac{1}{|N_i|}\\sum_{j\\in \\mathcal{N}_i}\\mathbf{E}_j)}_{\\text {Neighbor static covariates}}\\Biggr),\n\\end{align}\nwhere $\\sigma(\\cdot)$ is sigmoid function. $\\gamma_a$, $\\gamma_n$, $\\gamma_f$ and $\\gamma_g$ are degrees of time-dependent confounders, neighbor time-dependent confounders, static confounders and neighbor static confounders, respectively. The default values are $[\\gamma_a, \\gamma_n, \\gamma_f, \\gamma_g] = [10,3.3,10,3.3]$ ($\\frac{\\gamma_a}{\\gamma_n}=\\frac{\\gamma_f}{\\gamma_g}=3$), to mimic that a unit's own confounding factors should affect it more than neighbors. Note $\\bar{\\mathbf{X}}_i^t$ is the average time-dependent covariates until $t$. This reflects that past time-dependent covariates also affect the treatment. We set $\\delta_a = \\delta_n =5$ as adjustments. \n\n\\emph{Potential outcome simulation. } We follow~\\cite{geng2017prediction, BicaAJS20, SeedatIBQS22} to use a Pharmacokinetic-Pharmacodynamic (PK-PD) model~\\cite{goutelle2008hill} to simulate the continuous trajectory. PK-PD model is a popular bio-mathematical model and a natural fit for our vaccine use case. As mentioned above, $\\mathbf{Y}^t$ is essentially a part of $\\mathbf{X}^t$. Therefore we directly simulate the trajectory of $\\mathbf{X}_i^t$: \n\\begin{align}\n\\frac{d\\mathbf{X}^t_i}{dt}= \\mathbf{X}^t_i\\bigg(&\\underbrace{\\rho_u \\log \\left(\\frac{K}{\\mathbf{X}^t_i}\\right)}_{\\text {Time-dependt covariates}}+\\underbrace{\\rho_n \\log \\left(\\frac{K}{\\mathbf{X}^t_i}\\right)}_{\\text {Neighbor time-dependt covariates}}\\nonumber \\\\\n& + \\underbrace{\\rho_f\\mathbf{O}_i}_{\\text {Static covariates}} + \\underbrace{\\rho_g\\sum_{j\\in \\mathcal{N}_i}\\mathbf{O}_j)}_{\\text {Neighbor static covariates}}\\nonumber \\\\\n&+\\underbrace{\\beta_{a} \\mathbf{D}_i^t}_{\\text {Treatment }}+\\underbrace{\\frac{1}{|N_i|}\\sum_{j\\in \\mathcal{N}_i}\\beta_{n} \\mathbf{D}_j^t}_{\\text {Interference}}+\\underbrace{e_{i}^t}_{\\text {Noise }} \\bigg),\n\\label{eqn:pkpd}\n\\end{align}\nwhere $K$ controls the effects of time-dependent covariates on future potential outcomes. $\\mathbf{O}_i = w_x\\mathbf{V}_i$ denote the effects of static confounders on potential outcomes, where $w_x$ represents this mechanism. $\\rho_u$, $\\rho_n$, $\\rho_f$ and $\\rho_g$ are degrees of time-dependent covariates, neighbor time-dependent covariates, static covariates and neighbor static covariates, respectively. Their default values are $[\\rho_u, \\rho_n, \\rho_f, \\rho_g] = [-0.001, -00033,0.001,0.00033]$ ($\\frac{\\rho_u}{\\rho_n} = \\frac{\\rho_f}{\\rho_g}=3$). $\\beta_a$ and $\\beta_n$ control the strengths of treatment and interference. We set them as $[\\beta_a,\\beta_n] = [0.03, 0.01]$. The values also reflect that a unit's own covariates and treatment should have stronger effects on its future potential outcomes than neighbors. In reality, the effects of vaccines on providing protection decrease over time. To mimic this phenomenon, we use the following decay function to model the effects of treatments over time as in~\\cite{BicaAJS20,SeedatIBQS22}.\n\\begin{align}\n    \\mathbf{D}_i^t = \\tilde{\\mathbf{D}}_i^t + \\mathbf{D}_i^{(t-1)}/2,\n\\end{align}\nwhere $\\mathbf{D}_i^t$ denote the protection effect at time $t$, and $\\tilde{\\mathbf{D}}_i^t$ means a full protection of vaccines given at $t$. In other words, the unit receives a vaccine at time $t$, i.e., $\\mathbf{A}_i^t=1$. We set $\\tilde{\\mathbf{D}}_i^t=1$.\n\n\n\n"
                },
                "subsection 7.4": {
                    "name": "Notation Table",
                    "content": "\\label{sec:notations}\nThe notations and their corresponding description are in Table.~\\ref{tb:notation_table}.\n\n"
                }
            }
        },
        "tables": {
            "tb:cf_in": "\\begin{table}[!htp]\n\\centering\n\\scriptsize\n\\setlength\\tabcolsep{0.5pt}\n\\fontsize{6.5}{10}\\selectfont  \n\\caption{Counterfactual outcomes estimation errors on two datasets. ``BC'' is the abbreviation of the BlogCatalog dataset. The errors are broken down in x-step future estimation ($x\\in[1,2,3,4,5]$). MSE errors are reported. The best results are in boldface and the second best results are \\underline{underlined}. $\\model$-N is the variant of our model without any balancing; $\\model$-T means balance only w.r.t. treatments; $\\model$-I denotes balance only w.r.t. interference.}\\label{tb:cf_in}\n\\vspace{-10pt}\n\\begin{tabular}{ccccccccc}\\toprule[1.1pt]\nDataset &Model &1-step &2-step &3-step &4-step &5-step &Overall \\\\\\midrule\n\\multirow{6}{*}{Flickr} \n\n&CDE &0.134\\tiny{$\\pm$0.015} &0.164\\tiny{$\\pm$0.017} &0.198\\tiny{$\\pm$0.021} &0.237\\tiny{$\\pm$0.023} &0.281\\tiny{$\\pm$0.026} &0.203\\tiny{$\\pm$0.205} \\\\\n&GraphODE &0.237\\tiny{$\\pm$0.013} &0.276\\tiny{$\\pm$0.010} &0.313\\tiny{$\\pm$0.016} &0.347\\tiny{$\\pm$0.018} &0.379\\tiny{$\\pm$0.021} &0.310\\tiny{$\\pm$0.016} \\\\\n&TE-CDE &0.189\\tiny{$\\pm$0.025} &0.216\\tiny{$\\pm$0.021} &0.246\\tiny{$\\pm$0.027} &0.281\\tiny{$\\pm$0.041} &0.326\\tiny{$\\pm$0.063} &0.252\\tiny{$\\pm$0.031} \\\\\n\\cmidrule(l{4pt}){2-8}\n&\\model-N  &0.089\\tiny{$\\pm$0.006} &0.102\\tiny{$\\pm$0.007} &0.114\\tiny{$\\pm$0.009} &0.126\\tiny{$\\pm$0.010} &0.139\\tiny{$\\pm$0.012} &0.114\\tiny{$\\pm$0.008} \\\\\n&\\model-T &\\underline{0.058\\tiny{$\\pm$0.017}} &\\underline{0.066\\tiny{$\\pm$0.020}} &\\underline{0.075\\tiny{$\\pm$0.023}} &\\underline{0.084\\tiny{$\\pm$0.027}} &\\underline{0.098\\tiny{$\\pm$0.036}} &\\underline{0.076\\tiny{$\\pm$0.025}} \\\\\n&\\model-I &0.069\\tiny{$\\pm$0.007} &0.080\\tiny{$\\pm$0.008} &0.091\\tiny{$\\pm$0.009} &0.103\\tiny{$\\pm$0.011} &0.115\\tiny{$\\pm$0.012} &0.092\\tiny{$\\pm$0.009} \\\\\n&\\model &\\textbf{0.056\\tiny{$\\pm$0.009}} &\\textbf{0.060\\tiny{$\\pm$0.009 }}&\\textbf{0.067\\tiny{$\\pm$0.009}} &\\textbf{0.070\\tiny{$\\pm$0.010}} &\\textbf{0.077\\tiny{$\\pm$0.012}} &\\textbf{0.065\\tiny{$\\pm$0.010}} \\\\\\midrule[0.8pt]\n\\multirow{6}{*}{BC} \n&CDE &0.255\\tiny{$\\pm$0.120} &0.324\\tiny{$\\pm$0.178} &0.407\\tiny{$\\pm$0.263} &0.515\\tiny{$\\pm$0.383} &0.640\\tiny{$\\pm$0.549} &0.427\\tiny{$\\pm$0.296} \\\\\n&GraphODE &0.195\\tiny{$\\pm$0.018} &0.223\\tiny{$\\pm$0.023} &0.251\\tiny{$\\pm$0.028} &0.280\\tiny{$\\pm$0.033} &0.309\\tiny{$\\pm$0.040} &0.252\\tiny{$\\pm$0.028} \\\\\n&TE-CDE &0.316\\tiny{$\\pm$0.086} &0.351\\tiny{$\\pm$0.089} &0.399\\tiny{$\\pm$0.093} &0.493\\tiny{$\\pm$0.137} &0.725\\tiny{$\\pm$0.351} &0.457\\tiny{$\\pm$0.127} \\\\\n\\cmidrule(l{4pt}){2-8}\n&\\model-N &0.167\\tiny{$\\pm$0.012} &0.188\\tiny{$\\pm$0.015} &0.209\\tiny{$\\pm$0.018} &0.228\\tiny{$\\pm$0.023} &0.246\\tiny{$\\pm$0.028} &0.207\\tiny{$\\pm$0.019} \\\\\n&\\model-T &\\textbf{0.139\\tiny{$\\pm$0.015}} &\\textbf{0.154\\tiny{$\\pm$0.019}} &\\textbf{0.172\\tiny{$\\pm$0.025}} &\\textbf{0.189\\tiny{$\\pm$0.027}} &\\textbf{0.202\\tiny{$\\pm$0.031}} &\\textbf{0.171\\tiny{$\\pm$0.023}} \\\\\n&\\model-I &0.164\\tiny{$\\pm$0.016} &0.188\\tiny{$\\pm$0.020} &0.210\\tiny{$\\pm$0.024} &0.232\\tiny{$\\pm$0.029} &0.253\\tiny{$\\pm$0.035} &0.209\\tiny{$\\pm$0.025} \\\\\n&\\model &\\underline{0.148\\tiny{$\\pm$0.015}} &\\underline{0.166\\tiny{$\\pm$0.019}} &\\underline{0.186\\tiny{$\\pm$0.023}} &\\underline{0.205\\tiny{$\\pm$0.025}} &\\underline{0.229\\tiny{$\\pm$0.029}} &\\underline{0.186\\tiny{$\\pm$0.021}} \\\\\n\\bottomrule[1.1pt]\n\\end{tabular}\n\\vspace{-10pt}\n\\end{table}",
            "tb:degree": "\\begin{table}[!htp]\n\\vspace{-10pt}\n\\centering\n% \\scriptsize\n\\fontsize{8}{10}\\selectfont\n\\setlength{\\tabcolsep}{12pt}\n\\caption{The breakdown of counterfactual outcomes estimation errors by units (nodes) degrees in BlogCatalog dataset.}\\label{tb:degree}\n\\vspace{-10pt}\n\\begin{tabular}{ccccc}\\toprule[1.1pt]\nDegree &$\\#$Nodes &Percentage$\\%$ &Error \\\\\\midrule\n(0,5] &176 &10.2 &0.243\\scriptsize{$\\pm$0.337} \\\\\n(5,10] &185 &10.6 &0.235\\scriptsize{$\\pm$0.344} \\\\\n(10,20] &389 &22.5 &0.216\\scriptsize{$\\pm$0.296} \\\\\n(20,30] &312 &18.0 &0.218\\scriptsize{$\\pm$0.314} \\\\\n(20,30] &200 &11.5 &0.221\\scriptsize{$\\pm$0.295} \\\\\n(30,40] &165 &9.5 &0.195\\scriptsize{$\\pm$0.284} \\\\\n(40,50] &98 &5.7 &0.176\\scriptsize{$\\pm$0.269} \\\\\n$>$50 &207 &12.0 &0.187\\scriptsize{$\\pm$0.261} \\\\\n\\bottomrule[1.1pt]\n\\end{tabular}\n\\end{table}",
            "tb:cf_out": "\\begin{table}[!htp]\n\\centering\n% \\vspace{-0.5cm}\n\\scriptsize\n\\setlength\\tabcolsep{0.5pt}\n\\fontsize{6.5}{10}\\selectfont  \n\\caption{Generalization errors of potential outcomes prediction for new multi-agent dynamical systems (new graphs) on two datasets. ``BC'' is the abbreviation of the BlogCatalog dataset. The errors are broken down in x-step future estimation ($x\\in[1,2,3,4,5]$). MSE errors are reported. The best results are in boldface and the second best results are \\underline{underlined}. $\\model$-N is the variant of our model without any balancing; $\\model$-T means balance only w.r.t. treatments; $\\model$-I denotes balance only w.r.t. interference.}\\label{tb:cf_out}\n\\vspace{-10pt}\n\\begin{tabular}{ccccccccc}\\toprule[1.1pt]\nDataset &Model &1-step &2-step &3-step &4-step &5-step &Overall \\\\\\midrule\n\\multirow{7}{*}{Flickr} \n&CDE &0.134\\tiny{$\\pm$0.017} &0.166\\tiny{$\\pm$0.022} &0.201\\tiny{$\\pm$0.026} &0.241\\tiny{$\\pm$030} &0.285\\tiny{$\\pm$0.034} &0.205\\tiny{$\\pm$0.025} \\\\\n&GraphODE &0.209\\tiny{$\\pm$0.010} &0.243\\tiny{$\\pm$0.012} &0.275\\tiny{$\\pm$0.015} &0.306\\tiny{$\\pm$0.018} &0.335\\tiny{$\\pm$0.022} &0.274\\tiny{$\\pm$0.014} \\\\\n&TE-CDE &0.193\\tiny{$\\pm$0.023} &0.221\\tiny{$\\pm$0.021} &0.251\\tiny{$\\pm$0.027} &0.285\\tiny{$\\pm$0.040} &0.328\\tiny{$\\pm$0.061} &0.256\\tiny{$\\pm$0.030} \\\\\n\\cmidrule(l{4pt}){2-8}\n&\\model-N &0.087\\tiny{$\\pm$0.06} &0.099\\tiny{$\\pm$0.008} &0.111\\tiny{$\\pm$0.009} &0.122\\tiny{$\\pm$0.010} &0.134\\tiny{$\\pm$0.011} &0.111\\tiny{$\\pm$0.008} \\\\\n&\\model-T &\\textbf{0.057\\tiny{$\\pm$0.014}} &\\underline{0.064\\tiny{$\\pm$0.016}} &\\underline{0.072\\tiny{$\\pm$0.018}} &\\underline{0.081\\tiny{$\\pm$0.022}} &\\underline{0.092\\tiny{$\\pm$0.029}} &\\underline{0.738\\tiny{$\\pm$0.020}} \\\\\n&\\model-I &\\underline{0.071\\tiny{$\\pm$0.007}} &0.083\\tiny{$\\pm$0.008} &0.096\\tiny{$\\pm$0.009} &0.109\\tiny{$\\pm$0.010} &0.122\\tiny{$\\pm$0.011} &0.096\\tiny{$\\pm$0.009} \\\\\n&\\model &\\textbf{0.057\\tiny{$\\pm$0.008}} &\\textbf{0.062\\tiny{$\\pm$0.009}} &\\textbf{0.067\\tiny{$\\pm$0.010}} &\\textbf{0.073\\tiny{$\\pm$0.011}} &\\textbf{0.081\\tiny{$\\pm$0.013}} &\\textbf{0.069\\tiny{$\\pm$0.010}} \\\\\\midrule\n\\multirow{7}{*}{BC} \n&CDE &0.251\\tiny{$\\pm$0.113} &0.317\\tiny{$\\pm$0.174} &0.399\\tiny{$\\pm$0.259} &0.500\\tiny{$\\pm$0.380} &0.625\\tiny{$\\pm$0.548} &0.418\\tiny{$\\pm$0.294} \\\\\n&GraphODE &0.183\\tiny{$\\pm$0.021} &0.210\\tiny{$\\pm$0.026} &0.237\\tiny{$\\pm$0.032} &0.265\\tiny{$\\pm$0.039} &0.293\\tiny{$\\pm$0.046} &0.237\\tiny{$\\pm$0.033} \\\\\n&TE-CDE &0.328\\tiny{$\\pm$0.092} &0.372\\tiny{$\\pm$0.104} &0.440\\tiny{$\\pm$0.164} &0.582\\tiny{$\\pm$0.378} &0.933\\tiny{$\\pm$0.966} &0.457\\tiny{$\\pm$0.127} \\\\\n\\cmidrule(l{4pt}){2-8}\n&\\model-N &0.168\\tiny{$\\pm$0.008} &0.190\\tiny{$\\pm$0.009} &0.213\\tiny{$\\pm$0.012} &0.235\\tiny{$\\pm$0.015} &0.255\\tiny{$\\pm$0.021} &0.213\\tiny{$\\pm$0.013} \\\\\n&\\model-T &\\textbf{0.141\\tiny{$\\pm$0.011}} &\\textbf{0.157\\tiny{$\\pm$0.015}} &\\textbf{0.175\\tiny{$\\pm$0.021}} &\\textbf{0.192\\tiny{$\\pm$0.022}} &\\textbf{0.203\\tiny{$\\pm$0.028}} &\\textbf{0.174\\tiny{$\\pm$0.018}} \\\\\n&\\model-I &0.164\\tiny{$\\pm$0.014} &0.187\\tiny{$\\pm$0.018} &0.209\\tiny{$\\pm$0.022} &0.231\\tiny{$\\pm$0.028} &0.253\\tiny{$\\pm$0.034} &0.209\\tiny{$\\pm$0.023} \\\\\n&\\model &\\underline{0.143\\tiny{$\\pm$0.013}} &\\underline{0.162\\tiny{$\\pm$0.017}} &\\underline{0.180\\tiny{$\\pm$0.022}} &\\underline{0.199\\tiny{$\\pm$0.023}} &\\underline{0.214\\tiny{$\\pm$0.025}} &\\underline{0.180\\tiny{$\\pm$0.019}} \\\\\n\\bottomrule[1.1pt]\n\\end{tabular}\n\\end{table}",
            "tb:notation_table": "\\begin{table}[H]\n\\caption{Notations.}\n% \\fontsize{8}{10\n% }\\selectfont\n\\vspace{-10pt}\n\\setlength{\\tabcolsep}{2.1pt}\n\\begin{tabular}{@{}ll@{}}\n\\toprule[1.1pt]\nNotation & Description \\\\ \\midrule\n$\\mathcal{G}$ & Graph that represents multi-agent dynamical system\\\\\n$\\mathcal{V}$ & Node set in $\\mathcal{G}$\\\\\n$\\mathcal{E}$ & Edge set in $\\mathcal{G}$\\\\\n$\\mathbf{V}$ & Static unit features\\\\\n$\\mathbf{X}^t$ & Time-dependent covariates at time $t$\\\\\n$\\mathbf{A}^t$ & Treatment at time $t$\\\\\n$\\mathbf{Y}^t$ & Observed outcomes at time $t$\\\\\n$\\mathbf{Y}^t(A^t=a)$ & Potential outcomes under treatment $a$\\\\\n$\\mathbf{\\Bar{X}}^t$ & Time-dependent covariates collections up to $t$\\\\\n$\\mathbf{\\Bar{A}}^t$ & Treatment collections up to $t$\\\\\n$\\mathbf{\\Bar{Y}}^t$ & Observed outcomes collections up to $t$\\\\\n$\\mathcal{H}^t$ & Past observations\\\\\n$\\mathbf{A}^t_{\\mathcal{N}_i}$ & Treatments of node $i$'s first-order neighbors\\\\\n$\\mathbf{A}^t_{\\mathcal{N}_{-i}}$ & Treatments of nodes that are \\\\&beyond $i$'s first-order neighbors\\\\\n$\\mathbf{G}_i^t$ & Interference summary variable\\\\\n$\\mathbf{Z}_i^t$ & Continuous latent trajectory for node $i$\\\\\n$\\mathbf{C}_i^t$ & Concatenation of $\\mathbf{Z}_i^t$ and $\\mathbf{A}_i^t$\\\\\n$g(\\cdot)$ & Interference summary function\\\\\n$\\phi(\\cdot)$ & ODE function\\\\\n$f(\\cdot)$ & Initial state encoder function\\\\\n$r(\\cdot)$ & Gradient reversal layer\\\\\n$d_\\mathbf{Y}(\\cdot)$ & Outcome prediction layer\\\\\n$d_\\mathbf{A}(\\cdot)$ & Treatment prediction layer\\\\\n$d_\\mathbf{G}(\\cdot)$ & Interference prediction layer\\\\\n$N$ & Number of nodes in $\\mathcal{V}$\\\\\n$T$ & Number of observed timestamps\\\\\n$L^{\\langle Y \\rangle}$ & Loss function of outcome prediction\\\\\n$L^{\\langle A \\rangle}$ & Loss function of treatment prediction\\\\\n$L^{\\langle G \\rangle}$ & Loss function of interference prediction\\\\\n$\\alpha_{\\mathbf{A}}$& Weight of treatment balancing\\\\\n$\\alpha_{\\mathbf{G}}$& Weight of interference balancing\\\\\n\\bottomrule[1.1pt]\n\\end{tabular}\n\\label{tb:notation_table}\n% \\vspace{-15pt}\n\\end{table}"
        },
        "figures": {
            "fig:intro": "\\begin{figure}[t]\n \\centering\n \\includegraphics[width=1\\columnwidth]{figures/intro.pdf}\n  \\caption{Causal graph at time $t$ in a multi-agent dynamical system. The causal variables are represented by shapes, while their relationships are distinguished by colors.  }\\label{fig:intro}\n   \\vspace{-0.2cm}\n\\end{figure}",
            "fig:frame": "\\begin{figure}[t]\n \\centering\n \\includegraphics[width=1\\columnwidth]{figures/frame.pdf}\n  \\caption{Overview of \\model. The initial latent representation $\\mathbf{\\mathbf{Z}^0}$ is first learned from initial observations. Then the continuous latent representation trajectory $\\mathbf{\\mathbf{Z}^t}$ is learned as the solution to \\odelower, which is able to handle treatments as additional inputs. The graph neural network (GNN) based ODE function naturally models the mutual dependencies. The future potential outcomes can be decoded from $\\mathbf{\\mathbf{Z}^t}$ at any given time. To remove confounding bias, $\\mathbf{\\mathbf{Z}^t}$ is balanced with respect to 1) treatments, 2) interference when combined with corresponding treatments.}\\label{fig:frame}\n  \\vspace{-15pt}\n\\end{figure}",
            "fig:why_bad": "\\begin{figure}[h]\n \\centering\n \\includegraphics[width=1\\columnwidth]{figures/why_bad.pdf}\n  \\caption{T-SNE projections of latent representations ``before'' (factual) and ``after'' (counterfactual) flipping the treatments. Each point represents a unit's latent representation. The points are colored by the units' corresponding interference. Upper row: Flickr dataset; Lower Row: BlogCatalog dataset.}\\label{fig:why_bad}\n\\end{figure}",
            "fig:flip": "\\begin{figure}[h]\n \\centering\n \\includegraphics[width=1\\columnwidth]{figures/flip.pdf}\n  \\caption{Counterfactual outcomes estimation errors w.r.t. the percentage of units in the graph whose treatments are flipped. Left: Flickr dataset; Right: BlogCatalog dataset.}\\label{fig:flip}\n\\vspace{-15pt}\n\\end{figure}",
            "fig:confounding": "\\begin{figure}[h]\n \\centering\n \\includegraphics[width=1\\columnwidth]{figures/confounding.pdf}\n  \\caption{Counterfactual outcomes estimation errors w.r.t. 11 different confounding degrees $\\gamma_a$ and $\\gamma_f$. Note $\\gamma_a$ and $\\gamma_f$ are set as the same values in each experiment. The \\redtext{red} line points to the ``no confounding bias'' setting ($\\gamma_a = \\gamma_f=0$).} \\label{fig:confounding}\n  \\vspace{-10pt}\n\\end{figure}",
            "fig:embed": "\\begin{figure*}[h]\n \\centering\n \\includegraphics[width=2.\\columnwidth]{figures/embed.pdf}\n  \\caption{The T-SNE visualization of latent representations under different alternative training settings. Each point represents a unit's latent representation. The top line is colored by the observed treatments and the bottom line is colored by observed interference. Five columns from left to right: 1) only trained on $L^{\\langle Y \\rangle}$ (no balancing); 2) $\\frac{Iter_{L}}{Iter_{L^{\\langle Y \\rangle}}} = 1$; 3) $\\frac{Iter_{L}}{Iter_{L^{\\langle Y \\rangle}}} = 3$; 4) $\\frac{Iter_{L}}{Iter_{L^{\\langle Y \\rangle}}} = 5$; 5) only trained on $L$ (no alternative training). Each setting's corresponding counterfactual estimation error is marked in \\drtext{red}.    }\\label{fig:embed}\n\\end{figure*}",
            "fig:case_study": "\\begin{figure}[h]\n \\centering\n \\includegraphics[width=1\\columnwidth]{figures/case_study.pdf}\n  \\caption{The predicted counterfactual outcomes of \\model w/w.o balancing loss functions. The treatments of factual outcomes \\redtext{$\\mathbf{A}_i^t$} and treatments of counterfactual outcomes \\redtext{$\\mathbf{\\Tilde{A}}_i^t$} are attached around the corresponding curves at each timestamp. The estimation errors are noted in \\bluetext{blue}. Results are from Flickr dataset. Left: a successful case; right: a bad case.}\\label{fig:case_study}\n  \\vspace{-15pt}\n\\end{figure}",
            "fig:params": "\\begin{figure}[h]\n \\centering\n \\includegraphics[width=1\\columnwidth]{figures/parameter.pdf}\n  \\caption{The counterfactual estimation errors w.r.t. different combinations of $\\alpha_{\\mathbf{A}}$ and  $\\alpha_{\\mathbf{G}}$. The ``Light Balancing'' settings where $\\alpha_{\\mathbf{A}}$ and $\\alpha_{\\mathbf{G}}$ are both in small values  are circles in \\drtext{red}.  }\\label{fig:params}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\\mathbb{E}\\left(\\mathbf{Y}^{t^+}(\\mathbf{A}^{t^+}=a)\\mid \\mathcal{H}^t, \\mathcal{G}\\right).\\label{eqn:formalization}\n\\end{align}",
            "eq:2": "\\begin{align}\n        &\\mathbb{E}\\left(\\mathbf{Y}^{t^+}(\\mathbf{A}^{t^+}=a)\\mid \\mathcal{H}^t, \\mathcal{G}\\right)\\nonumber \\\\\n        &= \\mathbb{E}\\left(\\mathbf{Y}^{t^+}(\\mathbf{A}^{t^+}=a)\\mid \\mathbf{A}^{t^+}, \\mathbf{G}^{t^+}, \\mathcal{H}^t, \\mathcal{G}\\right)\\label{eqn:identify_ignore}\\\\\n&= \\mathbb{E}\\left(\\mathbf{Y}^{t^+}\\mid \\mathbf{A}^{t^+}, \\mathbf{G}^{t^+}, \\mathcal{H}^t, \\mathcal{G}\\right)\\label{eqn:identify_consist}.\n\\end{align}",
            "eq:3": "\\begin{align}\n    \\mathbf{Z}_{i}^{t}=\\mathbf{Z}_{i}^{0}+\\int_{t'=0}^{t} \\phi\\left(\\mathbf{Z}^{t'},\\mathbf{A}^{t'}\\right) dt'\\label{eqn:ode}.\n\\end{align}",
            "eq:4": "\\begin{align}\n     \\mathbf{Z}_{i}^{0},\\mathbf{Z}_{i}^{1}\\cdots \\mathbf{Z}_{i}^{T} = \\text{ODESolve}\\left(\\phi,[\\mathbf{Z}_1^0,\\mathbf{Z}_2^0\\cdots \\mathbf{Z}_N^0],\\left(t_0,t_1\\cdots t_T\\right)\\right)\\label{eqn:solver},\n\\end{align}",
            "eq:5": "\\begin{align}\n    L^{\\langle Y \\rangle} = \\frac{1}{N}\\frac{1}{T}\\sum_i^N\\sum_t^T\\left(\\hat{\\mathbf{Y}}_i^t - \\mathbf{Y}_i^t\\right)^2.\n\\end{align}",
            "eq:eqn:a_pred": "\\begin{align}\\label{eqn:a_pred}\n    L^{\\langle A \\rangle} = \\underset{d_\\mathbf{A}^j}{\\text{min}}\\ \\underset{f,\\phi}{\\text{max}}\\frac{1}{N}\\frac{1}{T}\\sum_i^N\\sum_t^T\\sum_{j\\in\\{0,1\\}}\\mathds{1}_{(\\mathbf{A}_i^t=j)}-\\log\\left(d_\\mathbf{A}^j\\left(r(\\mathbf{Z}_i^t)\\right)\\right),\n\\end{align}",
            "eq:eqn:pred_g": "\\begin{align}\\label{eqn:pred_g}\n    L^{\\langle G \\rangle} &= \\underset{d_\\mathbf{G}}{\\text{min}}\\ \\underset{f,\\phi}{\\text{max}}\\frac{1}{N}\\frac{1}{T}\\sum_i^N\\sum_t^T\\left(d_\\mathbf{G}\\left(r([\\mathbf{Z}_i^t,\\mathbf{A}_i^t])\\right)-\\mathbf{G}_i^t\\right)^2    \n\\end{align}",
            "eq:6": "\\begin{align}\n    L = L^{\\langle Y \\rangle} + \\alpha_{\\mathbf{A}} L^{\\langle A \\rangle} + \\alpha_{\\mathbf{G}} L^{\\langle G \\rangle},\n\\end{align}",
            "eq:7": "\\begin{align}\n\tp_i^t (a) = \\sigma \\Biggl(&\\underbrace{\\gamma_a (\\delta_a-\\bar{\\mathbf{X}}_i^t)}_{\\text {time-dependent covariates}} + \\underbrace{\\gamma_n \\Biggl(\\delta_n-(\\frac{1}{|N_i|}\\sum_{j\\in \\mathcal{N}_i}\\bar{\\mathbf{X}}_j^t)\\Biggr)}_{\\text {Neighbor time-dependent covariates}} \\nonumber \\\\\n &+\\underbrace{\\gamma_f\\mathbf{E}i}_{\\text {Static covariates}}  +\\underbrace{\\gamma_g(\\frac{1}{|N_i|}\\sum_{j\\in \\mathcal{N}_i}\\mathbf{E}_j)}_{\\text {Neighbor static covariates}}\\Biggr),\n\\end{align}",
            "eq:8": "\\begin{align}\n\\frac{d\\mathbf{X}^t_i}{dt}= \\mathbf{X}^t_i\\bigg(&\\underbrace{\\rho_u \\log \\left(\\frac{K}{\\mathbf{X}^t_i}\\right)}_{\\text {Time-dependt covariates}}+\\underbrace{\\rho_n \\log \\left(\\frac{K}{\\mathbf{X}^t_i}\\right)}_{\\text {Neighbor time-dependt covariates}}\\nonumber \\\\\n& + \\underbrace{\\rho_f\\mathbf{O}_i}_{\\text {Static covariates}} + \\underbrace{\\rho_g\\sum_{j\\in \\mathcal{N}_i}\\mathbf{O}_j)}_{\\text {Neighbor static covariates}}\\nonumber \\\\\n&+\\underbrace{\\beta_{a} \\mathbf{D}_i^t}_{\\text {Treatment }}+\\underbrace{\\frac{1}{|N_i|}\\sum_{j\\in \\mathcal{N}_i}\\beta_{n} \\mathbf{D}_j^t}_{\\text {Interference}}+\\underbrace{e_{i}^t}_{\\text {Noise }} \\bigg),\n\\label{eqn:pkpd}\n\\end{align}",
            "eq:9": "\\begin{align}\n    \\mathbf{D}_i^t = \\tilde{\\mathbf{D}}_i^t + \\mathbf{D}_i^{(t-1)}/2,\n\\end{align}"
        }
    }
}