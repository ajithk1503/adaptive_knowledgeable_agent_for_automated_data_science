{
    "meta_info": {
        "title": "An End-to-End Transformer Model for 3D Object Detection",
        "abstract": "We propose 3DETR, an end-to-end Transformer based object detection model for\n3D point clouds. Compared to existing detection methods that employ a number of\n3D-specific inductive biases, 3DETR requires minimal modifications to the\nvanilla Transformer block. Specifically, we find that a standard Transformer\nwith non-parametric queries and Fourier positional embeddings is competitive\nwith specialized architectures that employ libraries of 3D-specific operators\nwith hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and\neasy to implement, enabling further improvements by incorporating 3D domain\nknowledge. Through extensive experiments, we show 3DETR outperforms the\nwell-established and highly optimized VoteNet baselines on the challenging\nScanNetV2 dataset by 9.5%. Furthermore, we show 3DETR is applicable to 3D tasks\nbeyond detection, and can serve as a building block for future research.",
        "author": "Ishan Misra, Rohit Girdhar, Armand Joulin",
        "link": "http://arxiv.org/abs/2109.08141v1",
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "additionl_info": "Accepted at ICCV 2021"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n\n\n3D object detection aims to identify and localize objects in 3D scenes. Such scenes,  often represented using {\\em point clouds}, contain an unordered, sparse and irregular set of points captured using a depth scanner.\nThis set-like nature makes\npoint clouds significantly different from the traditional grid-like vision data like images and videos.\nWhile there are other 3D representations such as multiple-views~\\cite{su2015multi}, voxels~\\cite{adams2010fast} or meshes~\\cite{delaunay1934sphere}, they require additional post-processing to be constructed, and often loose information due to quantization.\nHence, point clouds have emerged as a popular 3D representation, and spurred the development of\nspecialized 3D architectures.\n \nMany recent 3D detection models directly work on the 3D points to produce the bounding boxes.\nOf particular interest, VoteNet~\\cite{qi2019votenet} casts 3D detection as a set-to-set problem, \\ie, transforming an unordered set of inputs (point cloud),\ninto an unordered set of outputs (bounding boxes).\nVoteNet uses an encoder-decoder architecture:\nthe encoder is a PointNet++ network~\\cite{qi2017pointnet} which converts the unordered point set into a unordered set of point features.\nThe point features are then input to a decoder that produces the 3D bounding boxes.\nWhile effective, such architectures have required years of\ncareful development\nby hand-encoding inductive biases, radii, and designing special 3D operators and loss functions.\n\nIn parallel to 3D, set-to-set encoder-decoder models have emerged as a competitive way to model 2D object detection.\nIn particular, the recent Transformer~\\cite{vaswani2017attention} based model, called DETR~\\cite{carion2020end}, casts 2D object detection as a set-to-set problem.\nThe self-attention operation in Transformers is designed to be permutation-invariant and capture long range contexts,\nmaking them a natural candidate for processing unordered 3D point cloud data.\nInspired by this observation, we ask the following question: can we leverage\nTransformers\nto learn a 3D object detector without relying on hand-designed inductive biases?\n\n\n\n\nTo that end, we develop 3D DEtection TRansformer (\\OURS) a simple to implement 3D detection method that uses fewer hand-coded design decisions and also casts detection as a set-to-set problem.\nWe explore the similarities between VoteNet and DETR, as well as between the core mechanisms of PointNet++ and the self-attention of Transformers to build our end-to-end Transformer-based detection model.\nOur model follows the general encoder-decoder structure that is common to both DETR and VoteNet.\nFor the encoder, we replace the PointNet++ by a standard Transformer applied directly on the point clouds.\nFor the decoder, we consider the parallel decoding strategy from DETR with Transformer layers making two important changes to adapt it to 3D detection, namely non-parametric query embeddings and Fourier positional embeddings~\\cite{tancik2020fourfeat}.\n\n \\OURS removes many of the hard coded design decisions in VoteNet and PointNet++ while being simple to implement and understand.\nUnlike DETR, \\OURS does not employ a ConvNet backbone, and solely relies on Transformers trained from scratch.\nOur transformer-based detection pipeline is flexible, and\nas in VoteNet, any component can be replaced by other existing modules.\nFinally, we show that 3D specific inductive biases can be easily incorporated in \\OURS to further improve its performance.\nOn two standard indoor 3D detection benchmarks, \\scannet and \\sunrgbd we achieve 65.0\\% AP and 59.0\\% AP respectively, outperforming an improved VoteNet baseline by $9.5\\%$ AP$_{50}$ on \\scannet.\n\n\n\n\n\n\\vspace{-0.1in}\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec:related}\n\\vspace{-0.1in}\n\n\nWe propose a 3D object detection model composed of Transformer blocks.\nWe build upon prior work in 3D architectures, detection, and Transformers.\n\n\\vspace{0.02in}\n\\par \\noindent \\textbf{Grid-based 3D Architectures.}\nConvolution networks can be applied to irregular 3D data after converting it into regular grids.\nProjection methods~\\cite{su2015multi,su2018splatnet,boulch2017unstructured,lawin2017deep,kanezaki2018rotationnet,lang2019pointpillars,tatarchenko2018tangent} project 3D data into 2D planes and convert it into 2D grids.\n3D data can also be converted into a volumetric 3D grid by voxelization~\\cite{tchapmi2017segcloud,riegler2017octnet,graham2015sparse,adams2010fast,hermosilla2018monte,li2018pointcnn,maturana2015voxnet,song2017semantic}.\nWe use 3D point clouds directly since they are suitable for \\emph{set} based architectures such as the transformer.\n\n\\par \\noindent \\textbf{Point cloud Architectures.}\n3D sensors often acquire data in the form of unordered point clouds.\nWhen using unordered point clouds as input, it is desirable to obtain permutation invariant features.\nPoint-wise MLP based architectures~\\cite{yang2019modeling,hu2020randla} such as PointNet~\\cite{qi2017pointnet} and PointNet++~\\cite{qi2017pointnet++} use permutation equivariant set aggregation (downsampling) and pointwise MLPs to learn effective representations. We use a single downsampling operation from~\\cite{qi2017pointnet++} to keep the number of input points tractable in our model.\n\nGraph-based models~\\cite{wang2019graph,li2019deepgcns} can operate on unordered 3D data.\nGraphs are constructed from 3D data in a variety of ways -- DGCNN~\\cite{wang2019dynamic} and PointWeb~\\cite{zhao2019pointweb} use local neighborhoods of points, SPG~\\cite{landrieu2018large} uses attribute and context similarity and Jiang \\etal~\\cite{jiang2019hierarchical} use point-edge interactions.\n\nFinally, continuous point convolution based architectures can also operate on point clouds.\nThe continuous weights can be defined using polynomial functions as in SpiderCNN~\\cite{xu2018spidercnn} or linear functions as in Flex-Convolutions~\\cite{groh2018flex}.\nConvolutions can also be applied by soft-assignment matrices~\\cite{verma2018feastnet} or specific ordering~\\cite{li2018pointcnn}.\nPointConv~\\cite{wu2019pointconv} and KPConv~\\cite{thomas2019kpconv} dynamically generate convolutional weights based on the input point coordinates, while InterpCNN~\\cite{mao2019interpolated} uses these coordinates to interpolate weights.\nWe build upon the Transformer~\\cite{vaswani2017attention} which is applicable for sets but not tailored for 3D.\n\n\\par \\noindent \\textbf{3D Object Detection} is a well studied research area where methods predict three dimensional bounding boxes from 3D input data~\\cite{qi2018frustum,pham2016geometrically,lahoud20193d,Song2014SlidingSF,Song_2016_CVPR,zhu2020ssn,simony2018complex,vora2020pointpainting,wang2015voting}.\nMany methods avoid expensive 3D operations by using 2D projection.\nMV3D~\\cite{chen2017multi}, VoxelNet~\\cite{zhou2018voxelnet} use a combination of 3D and 2D convolutions.\nYan \\etal~\\cite{yan2018second} simplify the 3D operation while~\\cite{yang2018pixor} uses a 2D projection, and~\\cite{wang2020pillar} uses `pillars' of voxels.\nWe focus on methods that directly use 3D point clouds~\\cite{yi2019gspn,shi2019pointrcnn,pham2019jsis3d,wang2019associatively}.\nPointRCNN~\\cite{shi2019pointrcnn} and PVRCNN~\\cite{shi2020pv} are 2-stage detection pipelines similar to the popular R-CNN framework~\\cite{Ren15fasterrcnn} for 2D images.\nWhile these methods are related to our work, for simplicity we build a single stage detection model as done in~\\cite{qi2019votenet,yang20203dssd,engelmann20203d,gwak2020gsdn}.\nVoteNet~\\cite{qi2019votenet} uses Hough Voting on sparse point cloud inputs and detects boxes by feature sampling, grouping and voting operations designed for 3D data.\nVoteNet is a building block for many follow up works.\n3D-MPA~\\cite{engelmann20203d} combines voting with a graph ConvNet for refining object proposals and uses specially designed 3D geometric features for aggregating detections.\nHGNet~\\cite{chen2020hierarchical} improves Hough Voting and uses a hierarchical graph network with feature pyramids.\nH3DNet~\\cite{zhang2020h3dnet} improves VoteNet by predicting 3D primitives and uses a geometric loss function.\nWe propose a simple detection method that can serve as a building block for such innovations in 3D detection.\n\\par \\noindent \\textbf{Transformers in Vision.} The Transformer architecture by Vaswani \\etal~\\cite{vaswani2017attention} has been immensely successful across domains like NLP~\\cite{radford2018improving,devlin2018bert}, speech recognition~\\cite{luscher2019rwth,synnaeve2019end}, image recognition~\\cite{parmar2018image,carion2020end,wang2018non,hu2018relation,dosovitskiy2020image}, and for cross-domain applications~\\cite{lu2019vilbert,su2019vl,tan2019lxmert}.\nTransformers are well suited for operating on 3D points since they are naturally permutation invariant.\nAttention based methods have been used for building 3D point representations for retrieval~\\cite{zhang2019pcan}, outdoor 3D detection~\\cite{liu2020tanet,yin2020lidar,paigwar2019attentional}, object classification~\\cite{yang2019modeling}.\nConcurrent work~\\cite{zhao2020point,pan20203d} also uses the Transformer architecture for 3D.\nWhile these methods use 3D specific information to modify the Transformer, we push the limits of the standard Transformer.\nOur work is inspired by the recent DETR model~\\cite{carion2020end} for object detection in images by Carion \\etal~\\cite{carion2020end}.\nDifferent from Carion \\etal, our model is an end-to-end transformer (no convolutional backbone) that can be trained from scratch and has important design differences such as non-parametric queries to enable 3D detection.\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.05in}\n"
            },
            "section 3": {
                "name": "Approach",
                "content": "\n\\label{sec:approach}\n\\vspace{-0.05in}\n\n\nWe briefly review prior work in 3D detection and their conceptual similarities to \\OURS.\nNext, we describe \\OURS,\nsimplifications in bounding box parametrization and the simpler set-to-set objective function.\n\n\\vspace{-0.05in}\n",
                "subsection 3.1": {
                    "name": "Preliminaries",
                    "content": "\n\\label{sec:preliminaries}\n\\vspace{-0.05in}\n\nThe recent VoteNet~\\cite{qi2019votenet} framework forms the basis for many detection models in 3D, and like our method, is a set-to-set prediction framework. VoteNet uses a specialized 3D encoder and decoder architecture for detection. It combines these models with a Hough Voting loss designed for sparse point clouds.\nThe encoder is a PointNet++~\\cite{qi2017pointnet++} model that uses a combination of multiple downsampling (set-aggregation) and upsampling (feature-propagation) operations that are specifically designed for 3D point clouds.\nThe VoteNet ``decoder'' predicts bounding boxes in three steps - 1) each point `votes' for the center coordinate of a box; 2) votes are aggregated within a fixed radius to obtain `centers'; 3) bounding boxes are predicted around `centers'.\n\\emph{BoxNet}~\\cite{qi2019votenet} is a non-voting alternative to VoteNet that randomly samples `seed' points from the input and treats them as `centers'.\nHowever, BoxNet achieves much worse performance than VoteNet as the voting captures additional context in sparse point clouds and yields better `center' points.\nAs noted by the authors~\\cite{qi2019votenet}, the multiple hand-encoded radii used in the encoder, decoder, and the loss function are important for detection performance and have been carefully tuned~\\cite{qi2017pointnet++,qi2017pointnet}.\n\nThe Transformer~\\cite{vaswani2017attention} is a generic architecture that can work on set inputs and capture large contexts by computing self-attention between all pairs of input points.\nBoth these properties make it a good candidate model for 3D point clouds.\nNext, we present our \\OURS model which uses a Transformer for both the encoder and decoder with minimal modifications and has minimal hand-coded information for 3D.\n\\OURS uses a simpler training and inference procedure.\nWe also highlight similarities and differences to the DETR model for 2D detection.\n\n\\vspace{-0.05in}\n"
                },
                "subsection 3.2": {
                    "name": "\\OURS: Encoder-decoder Transformer",
                    "content": "\n\\label{sec:pointdetr}\n\\vspace{-0.05in}\n\n\\OURS takes as input a 3D point cloud and predicts the positions of objects in the form of 3D bounding boxes.\nA point cloud is a unordered set of $N$ points where each point is associated with its $3$-dimensional XYZ coordinates.\nThe number of points is very large and we use the set-aggregation downsampling operation from~\\cite{qi2017pointnet++} to downsample the points and project them to $\\pN$ dimensional features.\nThe resulting subset of $\\pN$ features is passed through an encoder to also obtain a set of $\\pN$ features.\nA decoder takes these features as input and predicts multiple bounding boxes using a parallel decoding scheme inspired by~\\cite{carion2020end}.\nBoth encoder and decoder use standard Transformer blocks with `pre-norm'~\\cite{klein2017opennmt} and we refer the reader to Vaswani~\\etal~\\cite{vaswani2017attention} for details.\n~\\cref{fig:approach} illustrates our model.\n\\vspace{0.02in}\n\\par \\noindent \\textbf{Encoder.}\nThe downsample and set-aggregation steps provide a set of $\\pN$ features of $d=256$ dimensions using an MLP with two hidden layers of $64, 128$ dimensions.\nThe set of $\\pN$ features is then passed to a Transformer to also produce a set of $N'$ features of $d\\!=\\!256$ dimensions.\nThe Transformer applies multiple layers of self-attention and non-linear projections.\nWe do not use downsampling operations in the Transformer, and use the standard self-attention formulation~\\cite{vaswani2017attention}.\nThus, the Transformer encoder has no specific modifications for 3D data.\nWe omit positional embeddings of the coordinates from the encoder since the input already contains information about the XYZ coordinates.\n\n\n\n\n\\vspace{0.02in}\n\\par \\noindent \\textbf{Decoder.}\nFollowing Carion \\etal~\\cite{carion2020end}, we frame detection as a set prediction problem, \\ie, we simultaneously predict a set of boxes with no particular ordering.\nThis is achieved with a parallel decoder composed of Transformer blocks.\nThis decoder takes as input the $\\pN$ point features and a set of $B$ query embeddings $\\{\\bqe_1,\\dots,\\bqe_B\\}$ to produce a set of $B$ features that are then used to predict 3D-bounding boxes.\nIn our framework, the query embeddings $\\bqe$ represent locations in 3D space around which our final 3D bounding boxes are predicted.\nWe use positional embeddings in the decoder as it does not have direct access to the coordinates (operates on encoder features and query embeddings).\n\n\\vspace{0.02in}\n\\par \\noindent \\textbf{Non-parametric query embeddings.}\nInspired by seed points used in VoteNet and BoxNet~\\cite{qi2019votenet}, we use non-parametric embeddings computed from `seed' XYZ locations.\nWe sample a set of $B$ `query' points $\\{ \\bq_{i} \\}_{i=1}^{B}$ randomly from the $\\pN$ input points (see~\\cref{fig:approach}).\nWe use Farthest Point Sampling~\\cite{qi2017pointnet++} for the random samples as it\nensures a good coverage of the original set of points.\nWe associate each query point $\\bq_i$ with a query embedding $\\bqe_i$, by converting the coordinates of $\\bq_i$ into Fourier positional embeddings~\\cite{tancik2020fourfeat} followed by projection with a MLP.\n\n\\vspace{0.05in}\n\\par \\noindent \\textbf{\\OURSm: Inductive biases into \\OURS.}\nAs a proof of concept that our model is flexible, we modify our encoder to include inductive biases in 3D data, while keeping the decoder and loss fixed.\nWe leverage a weak inductive bias inspired by PointNet++,\n\\ie, local feature aggregation matters more than global aggregation.\nSuch an inductive bias can be easily implemented in Transformers by applying a mask to the self-attention~\\cite{vaswani2017attention}.\nThe resulting model, \\OURS-\\emph{m} has a \\emph{m}asked self-attention encoder with the same decoder and loss function as \\OURS.\n\\OURSm uses a three layer encoder which has an additional downsampling operation (from $\\pN\\!=\\!2048$ to $N''\\!=\\!1024$ points) after the first layer.\nEvery encoder layer applies a binary mask of $N''\\times N''$ to the self-attention operation.\nRow $i$ in the mask indicates which of the $N''$ points lie within the $\\ell_{2}$ radius of point $i$.\nWe use the radius values of $[0.16,0.64,1.44]$.\nCompared to PointNet++, \\OURSm does not rely on multiple layers of 3D feature aggregation and 3D upsampling.\n\n"
                },
                "subsection 3.3": {
                    "name": "Bounding box parametrization and prediction",
                    "content": "\n\\label{sec:bounding_box_param}\nThe encoder-decoder architecture produces a set of $B$ features, that are fed into prediction MLPs to predict bounding boxes.\nA 3D bounding box has the attributes\n\\textbf{(a)} its location, \\textbf{(b)} size, \\textbf{(c)} orientation, and \\textbf{(d)} the class of the object contained in it.\nWe describe the parametrization of these attributes and their associated prediction problems.\n\nThe prediction MLPs produce a box around every query coordinate $\\bq$.\n\\noindent\\textbf{(a) Location:} We use the XYZ coordinates of box's center $\\bc$.\nWe predict this in terms of an offset $\\bo$ that is added to the query coordinates, \\ie, $\\bc = \\bq + \\bo$.\n\n\n\\noindent\\textbf{(b) Size:} Every box is a 3D rectangle and we define its size around the center coordinate $\\bc$ using XYZ dimensions $\\bd$.\n\n\\noindent\\textbf{(c) Orientation:} In some settings~\\cite{song2015sun}, we must predict the orientation of the box, \\ie, the angle it forms compared to a given referential.\nWe follow~\\cite{qi2019votenet} and quantize the angles into $12$ bins from $[0, 2\\pi)$ and note the quantization residual.\nAngular prediction involves predicting the the quantized `class' of the angle and the residual to obtain the continuous angle $a$.\n\\noindent\\textbf{(d) Semantic Class:} We use a one-hot vector $\\bs$ to encode the object class contained in the bounding box.\nWe include a `background' or `not an object' class as some of the predicted boxes may not contain an object.\n\nPutting together the attributes of a box, we have two quantities: the predicted boxes $\\hbb$ and the ground truth boxes $\\bb$.\nEach predicted box $\\hbb = [ \\hbc, \\hbd, \\hba, \\hbs ]$ consists of (1) geometric terms $\\hbc, \\hbd \\in [0,1]^{3}$ that define the box center and dimensions respectively, $\\hba = [\\hba_c, \\hba_r ]$ that defines the quantized class and residual for the angle; (2) semantic term $\\hbs = [0, 1]^{K+1}$ that contains the probability distribution over the $K$ semantic object classes and the `background' class.\nThe ground truth boxes $\\bb$ also have the same terms.\n\n"
                },
                "subsection 3.4": {
                    "name": "Set Matching and Loss Function",
                    "content": "\n\\label{sec:set_loss}\nTo train the model, we first match the set of $B$ predicted 3D bounding boxes $\\{\\hbb\\}$ to the ground truth bounding boxes $\\{\\bb\\}$.\nWhile VoteNet uses hand-defined radii to do such set matching, we follow~\\cite{carion2020end} to perform a bipartite graph matching which is simpler, generic (see~\\cref{sec:components}) and robust to Non-Maximal Suppression.\nWe compute a loss for each predicted box using its matched ground truth box.\n\n\\par \\noindent \\textbf{Bipartite Matching.} We define a matching cost for a pair of boxes, predicted box $\\hbb$ and ground truth box $\\bb$, using a geometric and a semantic term.\n\n\\begin{multline}\nC_{\\bmatch}(\\hbb, \\bb) =  \\underbrace{- \\lambda_{1} \\mathrm{GIoU}(\\hbb, \\bb) + \\lambda_{2} \\|\\hbc - \\bc\\|_{1}}_\\text{geometric} \\\\\n- \\underbrace{\\lambda_{3} \\hbs[s_{\\mathrm{gt}}] + \\lambda_{4} (1 - \\hbs[s_{\\mathrm{bg}}]) }_\\text{semantic} \\\\\n\\end{multline}\n\n\nThese terms are similar to the loss functions used for training the model and $\\lambda$s are scalars used for a weighted combination.\nThe geometric cost measures the box overlap using GIoU~\\cite{Rezatofighi_2018_CVPR} and the distance between the centers of the boxes. Box overlap automatically accounts for the box dimensions, angular rotation and is scale invariant.\nThe semantic cost measures the likelihood of the ground truth class $s_{\\mathrm{gt}}$ under the predicted distribution $\\hbs$ and the likelihood of the box features belonging to a foreground class, \\ie, of not belonging to the background class $s_{\\mathrm{bg}}$.\n\nWe compute the optimal bipartite matching between all the predicted boxes $\\{\\hbb\\}$ and ground truth boxes $\\{\\bb\\}$ using the Hungarian algorithm~\\cite{kuhn1955hungarian} as in prior work~\\cite{stewart2016end,carion2020end}.\nAs we predict a larger number of boxes than the ground truth, the predicted boxes that do not get matched are considered matched to the `background' class.\nThis encourages the model to not over-predict, a property that helps our model be robust to Non-Maximal Suppression (see~\\cref{sec:ablations}).\n\n\n\\par \\noindent \\textbf{Loss function.} We use $\\ell_{1}$ regression losses for the center and box dimensions, normalizing them both in the range $[0, 1]$ for scale invariance.\nWe use Huber regression loss for the angular residuals and cross-entropy losses for the angular classification and semantic classification.\n\\begin{multline}\n\\mathcal{L}_{\\mathrm{\\OURS}} = \\lambda_{c} \\|\\hbc - \\bc\\|_{1} + \\lambda_{d} \\|\\hbd - \\bd\\|_{1} + \\lambda_{ar} \\|\\hba_r - \\ba_r\\|_{\\mathrm{huber}} \\\\\n-\\lambda_{ac} \\ba_c^\\intercal \\log \\hba_c -\\lambda_{s} \\bs_c^\\intercal \\log \\hbs_c\n\\end{multline}\n\nOur final loss function is a weighted combination of the above five terms and we provide the full details in the appendix.\nFor predicted boxes matched to the `background' class, we only compute the semantic classification loss with the background class ground truth label.\nFor datasets with axis-aligned 3D bounding boxes, we also use a loss directly on the GIoU as in~\\cite{carion2020end,Rezatofighi_2018_CVPR}.\nWe do not use the GIoU loss for oriented 3D bounding boxes as it is computationally involved.\n\\par \\noindent \\textbf{Intermediate decoder layers.} At training time, we use the same bounding box prediction MLPs to predict bounding boxes at every layer in the decoder.\nWe compute the set loss for each layer independently and sum all the losses to train the model.\nAt test time, we only use the bounding boxes predicted from the last decoder layer.\n\n\n \\vspace{-0.05in}\n"
                },
                "subsection 3.5": {
                    "name": "Implementation Details",
                    "content": "\n\\vspace{-0.05in}\nWe implement \\OURS using PyTorch~\\cite{NEURIPS2019_9015} and use the standard \\texttt{nn.MultiHeadAttention} module to implement the Transformer.\nWe use a single set aggregation operation~\\cite{qi2017pointnet++} to subsample $\\N'\\!=\\!2048$ points and obtain $256$ dimensional point features.\nThe \\OURS encoder has 3 layers where each layer uses multiheaded attention with four heads and a two layer MLP with a `bottleneck' of $128$ hidden dimensions.\nThe \\OURS decoder has 8 layers and closely follows the encoder, except that the MLP hidden dimensions are $256$.\nWe use Fourier positional encodings~\\cite{tancik2020fourfeat} of the XYZ coordinates in the decoder.\nThe bounding box prediction MLPs are two layer MLPs with a hidden dimension of $256$.\nFull architecture details in the appendix~\\cref{sec:supp_architecture}.\n\nAll the MLPs and self-attention modules in the model use a dropout~\\cite{srivastava2014dropout} of $0.1$ except in the decoder where we use a higher dropout of $0.3$.\n\\OURS is optimized using the AdamW optimizer~\\cite{loshchilov2017decoupled} with the learning rate decayed by a cosine learning rate schedule~\\cite{loshchilov2016sgdr} to $10^{-6}$, a weight decay of $0.1$, and gradient clipping at an $\\ell_{2}$ norm of $0.1$.\nWe train the model on a single V100 GPU with a batchsize of $8$ for 1080 epochs.\nWe use the RandomCuboid augmentation from~\\cite{zhang_depth_contrast} which reduces overfitting.\n\n\n\\vspace{-0.05in}\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:experiments}\n\\vspace{-0.05in}\n\n\\par \\noindent \\textbf{Dataset and metrics.}\nWe evaluate models on two standard 3D indoor detection benchmarks - \\scannet~\\cite{Dai_2017_CVPR_scannet} and \\sunrgbd-v1~\\cite{song2015sun}.\n\\sunrgbd has 5K single-view RGB-D training samples with oriented bounding box annotations for 37 object categories.\n\\scannet has 1.2K training samples (reconstructed meshes converted to point clouds) with axis-aligned bounding box labels for 18 object categories.\nFor both datasets, we follow the experimental protocol from~\\cite{qi2019votenet}:\nwe report the detection performance on the val set using mean Average Precision (mAP) at two different IoU thresholds of $0.25$ and $0.5$, denoted as AP$_{25}$ and AP$_{50}$.\nAlong with the metric, their protocol evaluates on the 10 most frequent categories for \\sunrgbd.\n\n",
                "subsection 4.1": {
                    "name": "\\OURS on 3D Detection",
                    "content": "\n\\label{sec:comparison}\n\n\n\n\nIn this set of experiments, we validate \\OURS for 3D detection.\nWe compare it to the BoxNet and VoteNet models since they are conceptually similar to \\OURS and are the foundations of many recent detection models.\nFor fair comparison, we use our own implementation of these models with the same optimization improvements used in \\OURS -- leading to a boost of +2-4\\% AP over the original paper (details in supplemental).\nWe also compare against a state-of-the-art method H3DNet~\\cite{zhang2020h3dnet} and provide a more detailed comparison against other recent methods in the appendix.\n\\OURS models use $256$ and $128$ queries for \\scannet and \\sunrgbd datasets.\n\n\n\\vspace{0.05in}\n\\par \\noindent \\textbf{Observations.}\nWe summarize results in~\\cref{tab:comparison}.\nThe comparison between BoxNet and \\OURS is particularly relevant since both methods predict boxes around location queries while VoteNet uses 3D Hough Voting to obtain queries.\nOur method significantly outperforms BoxNet on both the datasets with a gain of $+13\\%$ AP$_{25}$ on \\scannet and $+3.9\\%$ AP$_{25}$ on \\sunrgbd.\nEven when compared with VoteNet, our model achieves competitive performance, with $+2.3\\%$ AP$_{25}$ on \\scannet and $-1.5\\%$ AP$_{25}$ on \\sunrgbd.\n\\OURSm, which uses the masked Transformer encoder, achieves comparable performance to VoteNet on \\sunrgbd and a gain of $+4.6\\%$ AP$_{25}$ and $+9.5\\%$ AP$_{50}$ on \\scannet.\n\nCompared to a state-of-the-art method, H3DNet~\\cite{zhang2020h3dnet}, that builds upon VoteNet, \\OURSm is within a couple of AP$_{25}$ points on both datasets (more detailed comparison in~\\cref{sec:supp_experiments}).\nThese experiments validate that a encoder-decoder detection model based on the standard Transformer is competitive with similar models tailored for 3D data.\nJust as the VoteNet model was improved by the innovations of H3DNet~\\cite{zhang2020h3dnet}, HGNet~\\cite{chen2020hierarchical}, 3D-MPA~\\cite{engelmann20203d}, similar innovations could be integrated to our model in the future.\n\n\\vspace{0.05in}\n\\par \\noindent \\textbf{Qualitative Results.} In~\\cref{fig:detection_results}, we visualize a few detections and ground truth boxes from \\sunrgbd.\n\\OURS detects boxes despite the partial (single-view) depth scans and also predicts amodal bounding boxes or missing annotations on \\sunrgbd.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Analyzing \\OURS",
                    "content": "\n\\label{sec:analysis}\n\nWe conduct a series of experiments to understand \\OURS.\nIn~\\cref{sec:components}, we explore the similarities between \\OURS, VoteNet and BoxNet.\nNext, in~\\cref{sec:design_transformer}, we compare the design decisions in \\OURS that enable 3D detection to the original components in DETR.\n\n\\vspace{-0.1in}\n",
                    "subsubsection 4.2.1": {
                        "name": "Modules of VoteNet and BoxNet vs. \\OURS",
                        "content": "\n\\label{sec:components}\n\\vspace{-0.1in}\nThe encoder-decoder paradigm is flexible and we can test if the different modules in VoteNet, BoxNet and \\OURS are interchangeable.\nWe focus on the encoders, decoders and losses and report the detection performance in~\\cref{tab:components_encoder_loss,tab:components_decoder_loss}.\nFor simplicity, we denote the decoders and the losses used in BoxNet and VoteNet as Box and Vote respectively.\nWe use PointNet++ to refer to the modified PointNet++ architecture used in VoteNet~\\cite{qi2019votenet}.\n\n\n\n\n\\vspace{0.02in}\n\\par \\noindent \\textbf{Replacing the encoder.}\nWe train \\OURS with a PointNet++ encoder (\\cref{tab:components_encoder_loss}) and observe that the detection performance is unchanged or slightly worse compared to \\OURS with a transformer encoder.\nThis shows that the design decisions in \\OURS are broadly compatible with prior work, and can be used for designing better encoder models.\n\n\n\\vspace{0.02in}\n\\par \\noindent \\textbf{Replacing the decoder.}\nIn~\\cref{tab:components_decoder_loss}, we observe that replacing our Transformer-based decoders by Box or Vote decoders leads to poor detection performance on both benchmarks.\nAdditionally, the Box and Vote decoders work only with their respective losses and our preliminary experiments using set loss on these decoders led to worse results.\nThus, the drop of performance could be attributed to changing the decoder used with our transformer encoder. We inspect this next by replacing the loss in \\OURS while using the transformer encoder and decoder.\n\n\\vspace{0.02in}\n\\par \\noindent \\textbf{Replacing the loss.}\nWe train \\OURS, \\ie, both Transformer encoder and decoder with the Box and Vote losses.\nWe observe (\\cref{tab:components_decoder_loss} rows \\rownumber{4} and \\rownumber{5}) that this leads to similar degradation in performance, suggesting that the losses are not applicable to our model.\nThis is not surprising since the design decisions, \\eg, voting radius, aggregation radius \\etc in the Vote loss was specifically designed for radius parameters in the PointNet++ encoder~\\cite{qi2017pointnet++}.\nThis set of observations exposes that the decoder and loss function used in VoteNet depend greatly on the nature of the encoder (additional results in~\\cref{app:masked_encoder_vote_loss}).\nIn contrast, our set loss has no design decisions specific to our encoder-decoder.\n\n\n\n\n\\vspace{0.02in}\n\\par \\noindent \\textbf{Visualizing self-attention.}\nWe visualize the self-attention in the decoder in~\\cref{fig:encoder_attn}.\nThe decoder focuses on whole instances and groups points within instances.\nThis presumably makes it easier to predict bounding boxes for each instance.\nWe provide visualizations for the encoder self-attention in the supplemental.\n\n\n\\vspace{0.05in}\n\\par \\noindent \\textbf{Encoder applied to Shape classification.}\nTo verify that our encoder design is not specific to the detection task we test the encoder on shape classification of of models including 3D Warehouse~\\cite{wu20153d}.\n\nWe use the three layer encoder from \\OURS with vanilla self-attention (no decoder) or the three layer encoder from \\OURSm.\nTo obtain global features for the point cloud, we use the `\\texttt{CLS} token' formulation from Transformer, \\ie, append a constant point to the input and use this point's output encoder features as global features (see supplemental for details).\nThe global features from the encoder are input to a 2-layer MLP to perform shape classification.\n\\cref{tab:shape_cls} shows that both the \\OURS and \\OURSm encoders are competitive with state-of-the-art encoders tailored for 3D.\nThese results suggest that our encoder design is not specific to detection and can be used for other 3D tasks.\n\n\n\n\\vspace{-0.1in}\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "Design decisions in \\OURS",
                        "content": "\n\\label{sec:design_transformer}\n\\vspace{-0.1in}\n\n\n\nOur model is inspired by the DETR~\\cite{carion2020end} architecture but has major differences - (1) it is an end-to-end transformer without a ConvNet, (2) it is trained from scratch (3) uses non-parametric queries and (4) Fourier positional embeddings.\nIn~\\cref{tab:ablate_query_position}, we show the impact of the last two differences by evaluating various versions of our model on \\scannet.\nThe version with minimal modifications is a DETR model applied to 3D with our training and loss function.\n\nFirst, this version does not perform well on the \\scannet benchmark, achieving 15\\% AP$_{25}$.\nHowever, when replacing the parametric queries by non-parametric queries, we observe a significant improvement of +40\\% in AP$_{25}$ (\\cref{tab:ablate_query_position} rows \\rownumber{3} and \\rownumber{5}).\nIn fact, only using the non-parametric queries (row \\rownumber{4}) without positional embeddings doubles the performance.\nThis shows the importance of using non-parametric queries with 3D point clouds.\nA reason is that point clouds are irregular and sparse, making the learning of parametric queries harder than on a 2D image grids.\nNon-parametric queries are directly sampled from the point clouds and hence are less impacted by these irregularities.\nUnlike the fixed number of parametric queries in DETR, non-parametric queries easily enable the use different number of queries at train and test time (see~\\cref{sec:adaptive}).\n\nFinally, replacing the sinusoidal positional embedding by the low-frequency Fourier encodings of~\\cite{tancik2020fourfeat} provides an additional improvement of +5\\% in AP$_{25}$ (\\cref{tab:ablate_query_position} rows \\rownumber{2} and \\rownumber{3}).\nAs a side note, using positional encodings benefits the decoder more than the encoder because the decoder does not have direct access to coordinates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.05in}\n"
                    }
                }
            },
            "section 5": {
                "name": "Ablations",
                "content": "\n\\label{sec:ablations}\n\\vspace{-0.05in}\n\nWe conduct a series of ablation experiments to understand the components of \\OURS with settings from~\\cref{sec:experiments}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{0.05in}\n\\par \\noindent \\textbf{Effect of NMS.} \n\\OURS uses the set loss of DETR (\\cref{sec:set_loss}) that forces a 1-to-1 mapping between the ground truth box and the predicted box.\nThis loss penalizes models that predict too many boxes, since excess predictions are not matched to ground truth.\nIn contrast, the loss used in VoteNet~\\cite{qi2019votenet} does not discourage multiple predictions of the same object and thus relies on Non-Maximal Suppression to remove them as a post-processing step.\nWe compare \\OURS and VoteNet with and without NMS in~\\cref{tab:ablate_nms} with the detection AP metric, which penalizes duplicate detections.\nWithout NMS, \\OURS drops in performance by only 3\\% AP while VoteNet drops by 50\\%, showing our set loss works without NMS.\n\n\\vspace{0.06in}\n\\par \\noindent \\textbf{Effect of encoder/decoder layers.}\nWe assess the importance of the number of layers in the encoder and decoder in~\\cref{fig:enc_dec_layers}.\nWhile a higher number of layers improves detection performance in general, adding the layers in the decoder instead of the encoder has a greater impact on performance.\nFor instance, for a model with three encoder and three decoder layers, adding five decoder layers improves performance by +7\\% AP$_{50}$ while adding five encoder layers improves by +2\\%AP$_{50}$.\nThis preference toward the decoder arises because in our parallel decoder, each layer further refines the prediction quality of the bounding boxes.\n\n",
                "subsection 5.1": {
                    "name": "Adapting computation to inference constraints",
                    "content": "\n\\label{sec:adaptive}\nAn advantage of our model is that we can adapt its computation during inference by using less layers in the decoder or queries to predict boxes without retraining.\n\n\\vspace{0.06in}\n\\par \\noindent \\textbf{Adapting decoder depth.} \nThe parallel decoder of \\OURS is trained to predict boxes at each layer with the same bounding box prediction MLPs.\nThus far, in all our results we used the predictions only from the last decoder layer.\nWe now test the performance of the intermediate layers for a decoder with six layers in~\\cref{fig:dec_interim_queries} (left).\nWe compare this to training different models with a varying number of decoder layers.\nWe make two observations - (1) similar to~\\cref{fig:enc_dec_layers}, detection performance increases with the number of decoder layers; and (2) more importantly, the same model with reduced depth at test time performs as well or better than models trained from scratch with reduced depth.\nThis second property is shared with the DETR, but not with VoteNet.\nIt allows adapting the number of layers in the decoder to a computation budget during inference without retraining.\n\n\n\\vspace{0.06in}\n\\par \\noindent \\textbf{Adapting number of queries.}\nAs we increase the number of queries, \\OURS predicts more bounding boxes, resulting in better performance at a cost of longer running time.\nHowever, our non-parametric queries in \\OURS allow us to adapt the number of box predictions to trade performance for running time. Note that this is also possible with VoteNet, but not with DETR.\nIn~\\cref{fig:dec_interim_queries} (right), we compare changing the number of queries at test time to different models trained with varying number of queries.\nThe same \\OURS model can adapt to a varying number of queries at test time and performs comparably to different models.\nPerformance increases until the number of queries is enough to cover the point cloud well.\nWe found this adaptation to number of queries at test time works best with a \\OURS model trained with $128$ queries (see~\\cref{sec:supp_experiments} for other models).\nThis adaptive computation is promising and research into efficient self-attention should benefit our model.\nWe provide inference time comparisons to VoteNet in~\\cref{sec:supp_architecture} for different versions of the \\OURS model.\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.05in}\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\n\\vspace{-0.05in}\n\nWe presented \\OURS, an end-to-end Transformer model for 3D detection on point clouds.\n\\OURS requires few 3D specific design decisions or hyperparameters.\\\nWe show that using non-parametric queries and Fourier encodings is critical for good 3D detection performance.\nOur proposed design decisions enable powerful Transformers for 3D detection, and also benefit other 3D tasks like shape classification.\nAdditionally, our set loss function generalizes to prior 3D architectures.\nIn general, \\OURS is a flexible framework and can easily incorporate prior components used in 3D detection and can be leveraged to build more advanced 3D detectors.\nFinally, it also combines the flexibility of both VoteNet and DETR, allowing for a variable number of predictions at test time (like VoteNet) with a variable number of decoder layers (like DETR).\n\\vspace{0.05in}\n{\\small\n\\par \\noindent \\textbf{Acknowledgments:} We thank Zaiwei Zhang for helpful discussions and Laurens van der Maaten for feedback on the paper.\n}\n\n\n\\clearpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Supplemental Material",
                "content": "\n\n\n"
            },
            "section 8": {
                "name": "Implementation Details",
                "content": "\n\n",
                "subsection 8.1": {
                    "name": "Architecture",
                    "content": "\n\\label{sec:supp_architecture}\n\n\nWe describe the \\OURS architecture in detail.\n\n\n\n\\par \\noindent \\textbf{Architecture.}\nWe follow the dataset preprocessing from~\\cite{qi2019votenet} and obtain $N=20,000$ points and $N=40,000$ points respectively for each sample in \\sunrgbd and \\scannet datasets.\nThe $N\\times3$ matrix of point coordinates is then passed through one layer of the downsampling and set aggregation operation~\\cite{qi2017pointnet++} which uses Farthest-Point-Sampling to sample $2048$ points randomly from the scene.\nEach point is projected to a $256$ dimensional feature followed by the set-aggregation operation that aggregates features within a $\\ell_{2}$ distance of $0.2$.\nThe output is a $2048\\times256$ dimensional matrix of features for the $\\pN=2048$ points which is input to the encoder.\nWe now describe the encoder and decoder architectures (illustrated in~\\cref{fig:enc_dec_arch}).\n\n\\par \\noindent \\textbf{Encoder.}\nThe encoder has three layers of self-attention followed by an MLP.\nThe self-attention operation uses multi-headed attention with four heads.\nThe self-attention produces a $2048\\times2048$ attention matrix which is used to attend to the  features to produce a $256$ dimensional output.\nThe MLPs in each layer have a hidden dimension with $128$.\nAll the layers use LayerNorm~\\cite{ba2016layer} and the ReLU non-linearity.\n\n\\par \\noindent \\textbf{\\OURSm Encoder.}\nThe masked \\OURSm encoder has three layers of self-attention followed by an MLP.\nAt each layer the self-attention matrix of size \\texttt{\\#points}$\\times$\\texttt{\\#points} is multiplied with a binary mask $M$ of the same size.\nThe binary mask entry $M_{ij}$ is $1$ if the point coordinates for points $i$ and $j$ are within a radius $r$ of each other.\nWe use radius values of $[0.4, 0.8, 1.2]$ for the three layers.\nThe first layer operates on $2048$ points and is followed by a downsample + set aggregation operator that downsamples to $1024$ points using a radius of $0.4$, similar to PointNet++.\nThe encoder layers follow the same structure as the vanilla Encoder described above, \\ie, MLPs with hidden dimension of $128$, multi-headed attention with four heads \\etc.\nThe encoder produces $256$ dimensional features for $1024$ points.\n\n\\par \\noindent \\textbf{Decoder.}\nThe decoder operates on the $\\pN\\times256$ encoder features and $B\\times256$ location query embeddings.\nIt produces a $B\\times256$ matrix of box features as output.\nThe decoder has eight layers and uses cross-attention between the location query embeddings (Sec 3.2 main paper) and the encoder features, and self-attention between the box features.\nEach layer has the self-attention operation followed by a cross-attention operation (implemented exactly as self-attention) and an MLP with a hidden dimension of $256$.\nAll the layers use LayerNorm~\\cite{ba2016layer}, ReLU non-linearity and a dropout of $0.3$.\n\n\n\\par \\noindent \\textbf{Bounding box prediction MLPs.}\nThe box prediction MLPs operate on the $B\\times256$ box features from the decoder.\nWe use separate MLPs for the following five predictions - 1) center location offset $\\Delta \\bq \\in [0,1]^{3}$; 2) angle quantization class; 3) angle quantization residual $\\in \\mathbb{R}$; 4) box size $\\bs \\in [0,1]^{3}$; 5) semantic class of the object.\nEach MLP has $256$ hidden dimensions and uses the ReLU non-linearity.\nThe center location and size prediction MLP outputs are followed by a sigmoid function to convert them into a $[0,1]$ range.\n\n\n\\par \\noindent \\textbf{Inference speed}.\n\\OURS has very few 3D-specific tweaks and uses standard PyTorch.\nVoteNet relies on custom GPU CUDA kernels for 3D operations.\nWe measured the inference time of \\OURS (256 queries) and VoteNet (256 boxes) on a V100 GPU with a batchsize of 8 samples.\nBoth models downsample the pointcloud to $2048$ points.\n\\OURS needs 170 ms while VoteNet needs 132 ms.\nAs research into efficient self-attention becomes more mature (several recent works show promise), it will benefit the runtime and memory efficiency of our model.\n\n\n\n\n"
                },
                "subsection 8.2": {
                    "name": "Set Loss",
                    "content": "\nThe set matching cost is defined as:\n\\begin{equation*}\n  C_{\\bmatch}(\\hbb, \\bb) =  \\underbrace{- \\lambda_{1} \\mathrm{GIoU}(\\hbb, \\bb) + \\lambda_{2} \\|\\hbc - \\bc\\|_{1}}_\\text{geometric} -\\underbrace{\\lambda_{3} \\hbs[s_{gt}]}_\\text{semantic} \\\\\n\\end{equation*}\n\nFor $B$ predicted boxes and $G$ ground truth boxes, we compute a $B\\times G$ matrix of costs by using the above pairwise cost term.\nWe then compute an optimal assignment between each ground truth box and predicted box using the Hungarian algorithm.\nSince the number of predicted boxes is larger than the number of ground truth boxes, the remainder $B - G$ boxes are considered to match to background.\nWe set $\\lambda_{1}, \\lambda_{2}, \\lambda_{3}, \\lambda_{4}$ as $2, 1, 0, 0$ for \\scannet and $3,5,1, 5$ for \\sunrgbd.\n\n\nFor each predicted box that is matched to a ground truth box, our loss function is:\n\\begin{multline*}\n\\mathcal{L}_{\\mathrm{\\OURS}} = 5 * \\|\\hbc - \\bc\\|_{1} + \\|\\hbd - \\bd\\|_{1} + \\|\\hba_r - \\ba_r\\|_{\\mathrm{huber}} \\\\\n-0.1 * \\ba_c^\\intercal \\log \\hba_c - 5 * \\bs_c^\\intercal \\log \\hbs_c\n\\end{multline*}\nFor each unmatched box that is considered background, we compute only the semantic loss term.\nThe semantic loss is implemented as a weighted cross entropy loss with the weight of the `background' class as $0.2$ and a weight of $0.8$ for the K object classes.\n\n"
                }
            },
            "section 9": {
                "name": "Experiments",
                "content": "\n\\label{sec:supp_experiments}\nWe provide additional experimental details and hyperparameter settings.\n\n",
                "subsection 9.1": {
                    "name": "Improved baselines",
                    "content": "\nWe improve the VoteNet and BoxNet baselines by doing a grid search and improving the optimization hyperparameters.\nWe train the baseline models for $360$ epochs using the Adam optimizer~\\cite{KingmaB14} with a learning rate of $1\\times10^{-3}$ decayed by a factor of 10 after $160, 240, 320$ epochs and a weight decay of $0$.\nWe found that using a cosine learning rate schedule, even longer training than 360 epochs or the AdamW optimizer did not make a significant difference in performance for the baselines.\nThese improvements to the baseline lead to an increase in performance, summarized in~\\cref{tab:improved_baseline}.\n\n\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Per-class Results",
                    "content": "\nWe provide the per-class mAP results for \\scannet in~\\cref{tab:per_class_scannet} and \\sunrgbd in~\\cref{tab:per_class_sun}.\nThe overall results for these models were reported in the main paper (~\\cref{tab:comparison}).\n\n\n\n\n\n\n\n"
                },
                "subsection 9.3": {
                    "name": "Detailed state-of-the-art comparison",
                    "content": "\nWe provide a detailed comparison to state-of-the-art detection methods in~\\cref{tab:sota_comparison}.\nMost state-of-the-art methods build upon VoteNet.\nH3DNet~\\cite{zhang2020h3dnet} uses 3D primitives with VoteNet for better localization.\nHGNet~\\cite{chen2020hierarchical} improves VoteNet by using a hierarchical graph network with higher resolution output from its PointNet++ backbone.\n3D-MPA~\\cite{engelmann20203d} uses clustering based geometric aggregation and graph convolutions on top of the VoteNet method.\n\\OURS does not use Voting and has fewer 3D specific decisions compared to all other methods.\n\\OURS performs favorably compared to these methods and outperforms VoteNet.\nThis suggests that, like VoteNet, \\OURS can be used as a building block for future 3D detection methods.\n\n\n\n\n\n"
                },
                "subsection 9.4": {
                    "name": "\\OURSm with Vote loss",
                    "content": "\n\\label{app:masked_encoder_vote_loss}\nWe tuned the VoteNet loss with the \\OURSm encoder and our best tuned model gave 60.7\\% and 56.1\\% mAP on \\scannet and \\sunrgbd respectively (settings from~\\cref{tab:components_decoder_loss} of the main paper).\nThe VoteNet loss performs better with \\OURSm compared to the vanilla \\OURS encoder (gain of 6\\% and 3\\%), confirming that the VoteNet loss is dependent on the inductive biases/design of the encoder.\nUsing our set loss is still better than using the VoteNet loss for \\OURSm (~\\cref{tab:comparison} \\vs results stated in this paragraph).\nThus, our set loss design decisions are more broadly applicable than that of VoteNet.\n\n\n"
                },
                "subsection 9.5": {
                    "name": "Adapt queries at test time",
                    "content": "\nWe provide additional results for Section 5.1 of the main paper.\nWe change the number of queries used at test time for the same \\OURS model.\nWe show these results in~\\cref{fig:queries_test_time_extra} for two different \\OURS models trained with 64 and 256 queries respectively.\nWe observe that the model trained with $64$ queries is more robust to changing queries at test-time, but at its most optimal setting achieves worse detection performance than the model trained with $256$ queries.\nIn the main paper, we show results of changing queries at test time for a model trained with $128$ queries that achieves a good balance between overall performance and robustness to change at test-time.\n\n\n\n\n"
                },
                "subsection 9.6": {
                    "name": "Visualizing the encoder attention",
                    "content": "\nWe visualize the encoder attention for a \\OURS model trained on the \\sunrgbd dataset in~\\cref{fig:encoder_attention}.\nThe encoder focuses on parts of objects.\n\n\n\n"
                },
                "subsection 9.7": {
                    "name": "Shape Classification setup",
                    "content": "\n\n\\paragraph{Dataset and Metrics.}\nWe use the processed point clouds with normals from~\\cite{qi2017pointnet++}, and sample 8192 points as input for both training and testing our models.\nFollowing prior work~\\cite{zhao2019pointweb}, we report two metrics to evaluate shape classification performance: 1) Overall Accuracy (OA) evaluates how many point clouds we classify correctly; and 2) Class-Mean Accuracy (mAcc) evaluates the accuracy for each class independently, followed by an average over the per-class accuracy. This metric ensures tail classes contribute equally to the final performance.\n\n\\paragraph{Architecture Details.}\nWe use the base \\OURS and \\OURSm encoder architectures, followed by a 2-layer MLP with batch norm and a 0.5 dropout to transform the final features into a distribution over the 40 predefined shape classes. Differently from object detection experiments, our point features include the 3D position information concatenated with 3D normal information at each point, and hence the first linear layer is correspondingly larger, though the rest of the network follows the same architecture as the encoder used for detection. For the experiments with \\OURS, we prepend a \\texttt{[CLS]} token, output of which is used as input to the classification MLP. For the experiments with \\OURSm that involve masked transformers, we max pool the final layer features, which are then passed into the classifier.\n\n\\paragraph{Training Details.} All models are trained for 250 epochs with a learning rate of $4\\times10^{-4}$ and a weight decay of $0.1$, using the AdamW optimizer. We use a linear warmup from $4\\times10^{-7}$ to the initial LR over 20 epochs, and then decay to $4\\times10^{-5}$ over the remaining 230 epochs.\nThe models are trained on 4 GPUs with a batch size of 2 per GPU.\n\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{refs}\n}\n\n"
                }
            }
        },
        "tables": {
            "tab:comparison": "\\begin{table}[!t]\n          \\centering\n    \\begin{tabular}{@{}l|cccc@{}}\n    \\toprule\n    \\textbf{Method} & \\multicolumn{2}{c}{\\textbf{\\scannet}} & \\multicolumn{2}{c}{\\textbf{\\sunrgbd}}\\\\\n    & AP$_{25}$ & AP$_{50}$ & AP$_{25}$ & AP$_{50}$ \\\\\n    \\hline\n    BoxNet$^\\dagger$~\\cite{qi2019votenet} & 49.0 & 21.1 & 52.4 & 25.1 \\\\\n    \\colorrow \\OURS & 62.7 & 37.5 & 58.0 & 30.3 \\\\\n\\hline\n    VoteNet$^\\dagger$~\\cite{qi2019votenet} & 60.4 & 37.5 & 58.3 & 33.4 \\\\\n    \\colorrow \\OURSm & 65.0 & 47.0 & 59.1 & 32.7 \\\\\n\\hline\n    H3DNet~\\cite{zhang2020h3dnet} & 67.2 & 48.1 & 60.1 & 39.0 \\\\\n    \\bottomrule\n                \\end{tabular}\n\\vspace{-0.1in}\n\\caption{\\textbf{Evaluating \\OURS on 3D detection.}\nWe compare \\OURS with BoxNet and VoteNet methods and denote by $^\\dagger$ our improved implementation of these baselines.\n\\OURS achieves comparable or better performance to these improved baselines despite having fewer hand-coded 3D or detection specific decisions.\nWe report state-of-the-art performance from~\\cite{zhang2020h3dnet} that improves VoteNet by using 3D primitives. Detailed state-of-the-art comparison in~\\cref{sec:supp_experiments}.\n}\n\\label{tab:comparison}\n\\end{table}",
            "tab:components_encoder_loss": "\\begin{table}[!t]\n    \\mbox{}\\hfill\n      \\centering\n\\setlength{\\tabcolsep}{0.3em}\\resizebox{\\linewidth}{!}{\n    \\begin{threeparttable}\n    \\begin{tabular}{@{}ll|lll|cc|cc@{}}\n    \\toprule\n    & \\textbf{Method} & \\textbf{Encoder} & \\textbf{Decoder} & \\textbf{Loss}  & \\multicolumn{2}{c}{\\textbf{\\scannet}} & \\multicolumn{2}{c}{\\textbf{\\sunrgbd}}\\\\\n    &&&&& AP$_{25}$ & AP$_{50}$ & AP$_{25}$ & AP$_{50}$ \\\\\n    \\hline\n        \\colorrow &\\OURS & Tx. & Tx. & Set & 62.7 & 37.5 & 58.0 & 30.3 \\\\\n                        && PN++ & Tx.  & Set  & 61.4 & 34.7 & 56.8 & 26.9 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n    \\item {\\small \\hspace{-0.25in} PN++: PointNet++~\\cite{qi2017pointnet++}, Tx.: Transformer, Set loss~\\cref{sec:set_loss}}\n    \\end{tablenotes}\n    \\end{threeparttable}\n}\n\\vspace{-0.1in}\n\\caption{\\textbf{\\OURS with different encoders.}\nWe vary the encoder used in \\OURS and observe that the performance is unchanged or slightly worse when moving to a PointNet++ encoder.\nThis suggests that the decoder design and the loss function in \\OURS are compatible with prior 3D specific encoders.\n}\n\\label{tab:components_encoder_loss}\n\\end{table}",
            "tab:components_decoder_loss": "\\begin{table}[!t]\n    \\mbox{}\\hfill\n      \\centering\n\\setlength{\\tabcolsep}{0.3em}\\resizebox{\\linewidth}{!}{\n    \\begin{threeparttable}\n    \\begin{tabular}{@{}ll|lll|cc|cc@{}}\n    \\toprule\n    \\rownumber{\\#} & \\textbf{Method} & \\textbf{Encoder} & \\textbf{Decoder} & \\textbf{Loss}  & \\multicolumn{2}{c}{\\textbf{\\scannet}} & \\multicolumn{2}{c}{\\textbf{\\sunrgbd}}\\\\\n    &&&&& AP$_{25}$ & AP$_{50}$ & AP$_{25}$ & AP$_{50}$ \\\\\n        \\hline\n    \\multicolumn{8}{l}{\\textit{Comparing different decoders}} \\\\\n    \\hline\n        \\colorrow \\rownumber{1}&\\OURS & Tx. & Tx. & Set & 62.7 & 37.5 & 58.0 & 30.3 \\\\\n    \\rownumber{2}&& Tx. & Box  & Box & 31.0 & 10.2 & 36.4 & 14.4 \\\\\n    \\rownumber{3}&& Tx. & Vote  & Vote  & 46.1 & 23.4 & 47.5 & 24.9 \\\\\n\n     \\hline\n    \\multicolumn{8}{l}{\\textit{Comparing different losses}} \\\\\n    \\hline\n    \\rownumber{4}&& Tx. & Tx.  & Box & 49.6 & 20.5 & 49.5 & 21.1 \\\\\n    \\rownumber{5}&& Tx. & Tx.  & Vote & 54.0  & 31.9 & 53.4 & 28.3 \\\\\n            \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n    \\item {\\small \\hspace{-0.25in} Tx.: Transformer, Vote/Box loss~\\cite{qi2019votenet}, Set loss~\\cref{sec:set_loss} }\n    \\end{tablenotes}\n    \\end{threeparttable}\n}\n\\vspace{-0.1in}\n\\caption{\\textbf{\\OURS with different decoders and losses.}\nWe vary the decoder and losses used with our transformer encoder.\nAs the Box and Vote decoders are only compatible with their losses, we vary the loss function while using them.\nThe Vote loss is compatible with our Transformer encoder-decoder, however a simpler set loss performs the best.\n}\n\\label{tab:components_decoder_loss}\n\\end{table}",
            "tab:shape_cls": "\\begin{table}[!t]\n    \\centering\n    \\setlength{\\tabcolsep}{0.4em}\n        \\begin{tabular}{@{}l|ccc@{}}\n        \\toprule\n        {\\bf Method} & {\\bf input} & {\\bf mAcc} & {\\bf OA} \\\\\n        \\hline\n                \n                        \n                                                                                        \n\n        \\hline\n        PointNet++~\\cite{qi2017pointnet++} & point & -- & 91.9 \\\\\n        SpecGCN~\\cite{wang2018local} & point  & -- & 92.1 \\\\          DGCNN~\\cite{wang2019dynamic} & point & 90.2 & 92.2 \\\\\n        PointWeb~\\cite{zhao2019pointweb} & point  & 89.4 & 92.3 \\\\\n        SpiderCNN~\\cite{xu2018spidercnn} & point  & -- & 92.4 \\\\\n        PointConv~\\cite{wu2019pointconv} & point  & -- & 92.5 \\\\\n        KPConv~\\cite{thomas2019kpconv} & point  & -- & 92.9 \\\\\n        InterpCNN~\\cite{mao2019interpolated} & point & -- & 93.0 \\\\\n                \\hline\n                \\colorrow \\OURS encoder (Ours) & point & 89.1 & 92.1 \\\\          \\colorrow \\OURSm encoder (Ours) & point & 89.9 & 91.9 \\\\          \\bottomrule\n    \\end{tabular}\n        \\vspace{-0.1in}\n    \\caption{\\textbf{Shape classification.} We report shape classification results by training our Transformer encoder model.\n    Our model performs competitively with architectures designed for 3D suggesting that our design decisions can extend beyond detection and be useful for other tasks.\n    }\n\\label{tab:shape_cls}\n\\end{table}",
            "tab:ablate_query_position": "\\begin{table}[!t]\n    \\centering\n\\setlength{\\tabcolsep}{0.4em}\\resizebox{\\linewidth}{!}{\n    \\begin{threeparttable}\n    \\begin{tabular}{@{}llllc|cc@{}}\n    \\toprule\n    \\rownumber{\\#}&\\textbf{Method} & \\multicolumn{2}{c}{\\textbf{Positional Embedding}} & \\textbf{Query Type} & \\multicolumn{2}{c}{\\textbf{\\scannet}}  \\\\      && Encoder& Decoder& & AP$_{25}$ & AP$_{50}$ \\\\         \\hline\n    \\colorrow \\rownumber{1} &\\OURS & - & Fourier &  np + Fourier & 62.7 & 37.5 \\\\\n    \\rownumber{2}&& Fourier & Fourier &  np + Fourier & 61.8 & 37.0 \\\\\n    \\rownumber{3}&& Sine & Sine &  np + Sine & 55.8 & 30.9 \\\\\n            \\rownumber{4}&& - & - &  np + Sine & 31.3 & 10.8 \\\\\n    \\hline\n    \\rownumber{5} & DETR~\\cite{carion2020end}$^\\dagger$ & Sine & Sine & parametric~\\cite{carion2020end} & 15.4 & 5.3 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n    \\item {\\small \\hspace{-0.2in} np: non-parametric query (~\\cref{sec:pointdetr})}\n    \\end{tablenotes}\n    \\end{threeparttable}\n}\n\\vspace{-0.1in}\n\\caption{\\textbf{Decoder Query Type and Positional Embedding.} We how using non-parametric queries and Fourier positional embeddings~\\cite{tancik2020fourfeat} affect detection performance.\nDETR's parametric queries do not work well for 3D detection (rows \\rownumber{3}, \\rownumber{5}).\nThe standard choice~\\cite{vaswani2017attention,carion2020end} of sinusoidal positional embeddings is worse than Fourier embeddings (rows \\rownumber{2}, \\rownumber{3}).\n$^\\dagger$ - DETR is designed for 2D image detection and we adapt it for 3D detection.\n}\n\\label{tab:ablate_query_position}\n\\end{table}",
            "tab:ablate_nms": "\\begin{table}[!t]\n    \\centering\n    \\begin{tabular}{@{}l|cc@{}}\n    \\toprule\n    \\textbf{Method} & NMS & No NMS \\\\\n    \\hline\n    VoteNet~\\cite{qi2019votenet} & 60.4 & 10.7 \\\\\n    \\OURS (ours) & 62.7 & 59.5 \\\\\n    \\bottomrule\n    \\end{tabular}\n\\vspace{-0.1in}\n\\caption{\\textbf{Effect of NMS.} We report the detection performance (AP$_{25}$) for \\OURS and VoteNet on \\scannet.\n\\OURS works without NMS at test time because the set matching loss discourages excess predicted boxes.\n}\n\\label{tab:ablate_nms}\n\\end{table}",
            "tab:inference_speed": "\\begin{table}[!t]\n    \\centering\n  \\setlength{\\tabcolsep}{0.3em}\\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}cc|c@{}}\n    \\toprule\n    \\textbf{Encoder Layers} & \\textbf{Decoder Layers} & \\textbf{Inference time} \\\\\n    \\hline\n    3 & 3 & 153 \\\\\n    3 & 6 & 164 \\\\\n    3 & 8 & 170 \\\\\n    3 & 10 & 180 \\\\\n    \\hline\n    6 & 6 & 193 \\\\\n    6 & 8 & 213 \\\\\n    \\hline\n    8 & 8 & 219 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\vspace{-0.1in}\n\\caption{\\textbf{Inference Speed and Memory.}\nWe provide inference speed (in milliseconds) for different number of encoder and decoder layers in the \\OURS model.\nAll timings are measured on a single V100 GPU with a batchsize of 8 and using 256 queries.\n}\n\\label{tab:inference_speed}\n\\end{table}",
            "tab:improved_baseline": "\\begin{table}[!t]\n      \\centering\n    \\begin{tabular}{@{}l|cccc@{}}\n    \\toprule\n    \\textbf{Method} & \\multicolumn{2}{c}{\\textbf{\\scannet}} & \\multicolumn{2}{c}{\\textbf{\\sunrgbd}} \\\\\n    &AP$_{25}$ & AP$_{50}$ & AP$_{25}$ & AP$_{50}$ \\\\\n    \\hline\n    BoxNet~\\cite{qi2019votenet} & 45.4 & - & 53.0 & - \\\\\n    BoxNet$^\\dagger$~\\cite{qi2019votenet} & 49.0 & 21.1 & 52.4 & 25.1 \\\\\n    \\hline\n    VoteNet~\\cite{qi2019votenet} & 58.6 & 33.5 & 57.7 & - \\\\\n    VoteNet$^\\dagger$~\\cite{qi2019votenet} & 60.4 & 35.5 & 58.3 & 33.4 \\\\\n    \\bottomrule\n    \\end{tabular}\n\\vspace{-0.1in}\n\\caption{\\textbf{Improved baseline.}\nWe denote by $^\\dagger$ our improved implementation of the baseline methods and report the numbers from the original paper~\\cite{qi2019votenet}.\nOur improvements ensure that the comparisons in the main paper are fair.\n}\n\\label{tab:improved_baseline}\n\\end{table}",
            "tab:per_class_sun": "\\begin{table*}[!t]\n  \\centering\n  \\begin{tabular}{@{}l|cccccccccc@{}}\n    \\toprule\n    Model & bed & table & sofa & chair & toilet & desk & dresser & nightstand & bookshelf & bathtub\\\\\n    \\hline\n    \\OURS & 81.8 & 50.0 & 58.3 & 68.0 & 90.3 & 28.7 & 28.6 & 56.6 & 27.5 & 77.6 \\\\\n    \\OURS-m & 84.6 & 52.6 & 65.3 & 72.4 & 91.0 & 34.3 & 29.6 & 61.4 & 28.5 & 69.8 \\\\\n  \\bottomrule\n  \\end{tabular}\n\\vspace{-0.1in}\n\\caption{\\textbf{Per-class AP$_{25}$ for \\sunrgbd.}\n}\n\\label{tab:per_class_sun}\n\\end{table*}",
            "tab:per_class_scannet": "\\begin{table*}[!t]\n  \\centering\n\\setlength{\\tabcolsep}{0.3em}\\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{@{}l|cccccccccccccccccc@{}}\n    \\toprule\n    Model & cabinet & bed & chair & sofa & table & door & window & bookshelf & picture & counter & desk & curtain & refrigerator & showercurtrain & toilet & sink & bathtub & garbagebin \\\\\n    \\hline\n    \\OURS & 50.2 & 87.0 & 86.0 & 87.1 & 61.6 & 46.6 & 40.1 & 54.5 & 9.1 & 62.8 & 69.5 & 48.4 & 50.9 & 68.4 & 97.9 & 67.6 & 85.9 & 45.8 \\\\\n    \\OURSm & 49.4 & 83.6 & 90.9 & 89.8 & 67.6 & 52.4 & 39.6 & 56.4 & 15.2 & 55.9 & 79.2 & 58.3 & 57.6 & 67.6 & 97.2 & 70.6 & 92.2 & 53.0 \\\\\n  \\bottomrule\n  \\end{tabular}\n}\n\\vspace{-0.1in}\n\\caption{\\textbf{Per-class AP$_{25}$ for \\scannet.}\n}\n\\label{tab:per_class_scannet}\n\\end{table*}",
            "tab:sota_comparison": "\\begin{table}[!t]\n          \\centering\n\\setlength{\\tabcolsep}{0.3em}\\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l|lcccc@{}}\n    \\toprule\n    \\textbf{Method} & \\textbf{Arch.} & \\multicolumn{2}{c}{\\textbf{\\scannet}} & \\multicolumn{2}{c}{\\textbf{\\sunrgbd}}\\\\\n    && AP$_{25}$ & AP$_{50}$ & AP$_{25}$ & AP$_{50}$ \\\\\n    \\hline\n    BoxNet$^\\dagger$~\\cite{qi2019votenet} & BoxNet & 49.0 & 21.1 & 52.4 & 25.1 \\\\\n    \\OURS & Tx. & 62.7 & 37.5 & 56.8 & 30.1 \\\\\n\\hline\n    VoteNet$^\\dagger$~\\cite{qi2019votenet} & VoteNet & 60.4 & 37.5 & 58.3 & 33.4 \\\\\n    \\OURSm & Tx. & 65.0 & 47.0 & 59.0 & 32.7 \\\\\n\\hline\n    H3DNet~\\cite{zhang2020h3dnet} & VoteNet + 3D primitives & 67.2 & 48.1 & 60.1 & 39.0 \\\\\n    HGNet~\\cite{chen2020hierarchical} & VoteNet + GraphConv & 61.3 & 34.4 & 61.6 & 34.4 \\\\\n    3D-MPA~\\cite{engelmann20203d} & VoteNet + GraphConv & 64.2 & 49.2 & - & - \\\\\n  \\bottomrule\n    \\end{tabular}\n}\n\\vspace{-0.1in}\n\\caption{\\textbf{Detailed state-of-the-art comparison on 3D detection.}\n}\n\\label{tab:sota_comparison}\n\\end{table}"
        },
        "figures": {
            "fig:encoder_attn": "\\begin{figure}[!t]\n\\includegraphics[width=0.5\\textwidth]{viz_decoder_detections_v2.pdf}\n\\caption{\\textbf{\\OURS.} We train an end-to-end Transformer model for 3D object detection on point clouds.\nOur model has a Transformer encoder for feature encoding and a Transformer decoder for predicting boxes.\nFor an unseen input, we compute the self-attention from the reference point (blue dot) to all points in the scene and display the points with the highest attention values in red.\nThe decoder attention groups points within an instance which presumably makes it easier to predict bounding boxes.\n}\n\\label{fig:encoder_attn}\n\\end{figure}",
            "fig:approach": "\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=\\textwidth]{approach_reduced.pdf}\n\n\\vspace{-0.1in}\n\\caption{\\textbf{Approach.}\n\\emph{(Left)} \\OURS is an end-to-end trainable Transformer that takes a set of 3D points (point cloud) as input and outputs a set of 3D bounding boxes.\nThe Transformer encoder produces a set of per-point features using multiple layers of self-attention.\nThe point features and a set of `query' embeddings are input to the Transformer decoder that produces a set of boxes.\nWe match the predicted boxes to the ground truth and optimize a set loss.\nOur model does not use color information (used for visualization only).\n\\emph{(Right)}\nWe randomly sample a set of `query' points that are embedded and then converted into bounding box predictions by the decoder.\n}\n\\label{fig:approach}\n\\end{figure*}",
            "fig:enc_dec_layers": "\\begin{figure}[!t]\n\\includegraphics[width=0.45\\textwidth]{ablate_encdec_layersv2_scannet_final.pdf}\n\\vspace{-0.2in}\n\\caption{\\textbf{Varying number of layers for encoder and decoder.} \nWe train different models with varying number of encoder and decoder layers and analyze the impact on detection performance on \\scannet.\nIncreasing the number of layers in either the encoder or decoder has a positive effect, but a higher number of decoder layers matters more than the encoder layers.\n}\n\\label{fig:enc_dec_layers}\n\\end{figure}",
            "fig:dec_interim_queries": "\\begin{figure}[!t]\n\\centering\n\n\\includegraphics[width=0.23\\textwidth]{ablate_interim_dec_layers_train_test_v4.pdf}\n\\includegraphics[width=0.23\\textwidth]{ablate_queries_vs_ap_scannet_train_test.pdf}\n\\vspace{-0.15in}\n\\caption{\\textbf{Adapting compute at test time.} \nWe change the number of decoder layers or the number of queries used at test time for a \\OURS model (`same model').\nWe compare this to different models trained with reduced depth of the decoder (left) or with different number of queries (right).\n\\OURS can adapt to different test time conditions and performs favorably compared to different models trained for the test time conditions.\n}\n\n\n\n\\label{fig:dec_interim_queries}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation*}\n  C_{\\bmatch}(\\hbb, \\bb) =  \\underbrace{- \\lambda_{1} \\mathrm{GIoU}(\\hbb, \\bb) + \\lambda_{2} \\|\\hbc - \\bc\\|_{1}}_\\text{geometric} -\\underbrace{\\lambda_{3} \\hbs[s_{gt}]}_\\text{semantic} \\\\\n\\end{equation*}"
        }
    }
}