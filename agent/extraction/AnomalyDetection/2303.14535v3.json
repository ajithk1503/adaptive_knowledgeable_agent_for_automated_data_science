{
    "meta_info": {
        "title": "EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level  Latencies",
        "abstract": "Detecting anomalies in images is an important task, especially in real-time\ncomputer vision applications. In this work, we focus on computational\nefficiency and propose a lightweight feature extractor that processes an image\nin less than a millisecond on a modern GPU. We then use a student-teacher\napproach to detect anomalous features. We train a student network to predict\nthe extracted features of normal, i.e., anomaly-free training images. The\ndetection of anomalies at test time is enabled by the student failing to\npredict their features. We propose a training loss that hinders the student\nfrom imitating the teacher feature extractor beyond the normal images. It\nallows us to drastically reduce the computational cost of the student-teacher\nmodel, while improving the detection of anomalous features. We furthermore\naddress the detection of challenging logical anomalies that involve invalid\ncombinations of normal local features, for example, a wrong ordering of\nobjects. We detect these anomalies by efficiently incorporating an autoencoder\nthat analyzes images globally. We evaluate our method, called EfficientAD, on\n32 datasets from three industrial anomaly detection dataset collections.\nEfficientAD sets new standards for both the detection and the localization of\nanomalies. At a latency of two milliseconds and a throughput of six hundred\nimages per second, it enables a fast handling of anomalies. Together with its\nlow error rate, this makes it an economical solution for real-world\napplications and a fruitful basis for future research.",
        "author": "Kilian Batzner, Lars Heckler, Rebecca K\u00f6nig",
        "link": "http://arxiv.org/abs/2303.14535v3",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "Accepted as Oral to WACV 2024"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nIn the past years, deep learning methods have continued to improve the state of the art across a wide range of computer vision applications.\nThis progress has been accompanied by advances in making neural network architectures faster and more efficient \\cite{tan2019efficientnet, tan2020efficientdet, redmon2016you, yin2022vit}.\nModern classification architectures, for example, focus on characteristics such as latency, throughput, memory consumption, and the number of trainable parameters \\cite{tan2019efficientnet, tan2021efficientnetv2, liu2022convnet, yin2022vit, sandler2018mobilenetv2, liu2021swin}.\nThis ensures that as networks become more capable, their computational requirements remain suitable for real-world applications. \nThe field of visual anomaly detection has also seen rapid progress in the recent past, especially on industrial anomaly detection benchmarks \\cite{bergmann2021_mvtec_ad_ijcv, bergmann2019_mvtec_ad_cvpr, roth2022towards, rudolph2023asymmetric}.\nState-of-the-art anomaly detection methods, however, often sacrifice computational efficiency for an increased anomaly detection performance.\nCommon techniques are ensembling, the use of large backbones, and increasing the input image resolution to up to 768$\\times$768 pixels.\n\n\n\n\nReal-world anomaly detection applications frequently put constraints on the computational requirements of a method.\nThere are cases where detecting an anomaly too late can cause substantial economic damage, such as metal objects in a crop field entering the interior of a combine harvester.\nIn other cases, even human health is at risk, for example, if a limb of a machine operator approaches a blade.\nFurthermore, industrial settings commonly involve strict runtime limits caused by high production rates \\cite{bailey2012machine_vision_handbook_fpgas}.\nNot adhering to these limits would decrease the production rate of the respective application and thus its economic viability.\nIt is therefore essential to pay attention to the computational and economic cost of anomaly detection methods to keep them suitable for real-world applications.\n\n\nIn this work, we propose \\ourmethod, a method that sets new standards for both the anomaly detection performance and the inference runtime, as shown in \\Cref{fig:teaser}.\nWe first introduce an efficient network architecture for computing expressive features in less than a millisecond on a modern GPU\\@.\nTo detect anomalous features, we use a student--teacher approach \\cite{bergmann2020_uninformed_cvpr, rudolph2023asymmetric, wang2021student_teacher}.\nWe train a student network to predict the features computed by a pretrained teacher network on normal, i.e., anomaly-free training images.\nBecause the student is not trained on anomalous images, it generally fails to mimic the teacher on these.\nA large distance between the outputs of the teacher and the student thus enables the detection of anomalies at test time.\nTo further increase this effect, Rudolph \\etal \\cite{rudolph2023asymmetric} use \\textit{architectural} asymmetry between the teacher and the student.\nWe instead propose \\textit{loss-induced} asymmetry in the form of a training loss that hinders the student from imitating the teacher beyond the normal images.\nThis loss does not affect the computational cost at test time and does not restrict the architecture design.\nIt allows us to use our efficient network architecture for both the student and the teacher, while improving the detection of anomalous features.\n\nIdentifying anomalous local features enables the detection of anomalies that are \\textit{structurally} different from the normal images, for example, contaminations or stains on manufactured products.\nA challenging problem, however, are violations of \\textit{logical} constraints regarding the position, size, arrangement, etc.\\ of normal objects.\nTo address this, \\ourmethod\\ includes an autoencoder that learns the logical constraints of training images and detects violations at test time.\nWe show how to integrate the autoencoder efficiently with a student--teacher model.\nFurthermore, we present a method to improve the anomaly detection performance by calibrating the detection results of the autoencoder and the student--teacher model before combining their results.\n\nOur contributions are summarized as follows:\n\\begin{itemize}[topsep=0.2em]\n    \\itemsep-0.2em\n    \\item We substantially improve the state of the art for both the detection and the localization of anomalies on industrial benchmarks, at a latency of \\SI{2}{\\milli\\second} and a throughput of more than 600 images per second.\n    \\item We propose an efficient network architecture to speed up feature extraction by an order of magnitude in comparison to the feature extractors used by recent methods \\cite{roth2022towards, rudolph2023asymmetric, yu2021fastflow}.\n    \\item We introduce a training loss that significantly improves the anomaly detection performance of a student--teacher model without affecting its inference runtime.\n    \\item We achieve an efficient autoencoder-based detection of logical anomalies and propose a method for a calibrated combination of the detection results with those of a student--teacher model.\n\\end{itemize}\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Anomaly Detection Tasks",
                    "content": "\n\nVisual anomaly detection is a rapidly growing area of research with a diverse range of applications, including medical imaging \\cite{fernando2021_medical_ad_survey, armato2011lung, menze2015_brats_dataset}, autonomous driving \\cite{hendrycks2019scaling, lis2019_iccv_resynthesis, blum2019_fishyscapes_dataset}, and industrial inspection \\cite{bergmann2021_mvtec_ad_ijcv, ehret2019_ad_review_paper, pang2020_review_paper}.\nApplications often have specific characteristics, such as the availability of image sequences in surveillance datasets \\cite{li2013_ucsd_video_ad_dataset, zhang2016single, lu2013_avenue_video_ad_dataset} or the different modalities of medical imaging datasets (MRI \\cite{bakas2017_brats_dataset}, CT \\cite{armato2011lung}, X-ray \\cite{irvin2019chexpert}, etc.).\nThis work focuses on detecting anomalies in RGB or gray-scale images without conditioning the prediction on a sequence of images.\nWe use industrial anomaly detection datasets to benchmark our proposed method against existing ones.\n\n\nThe introduction of the MVTec AD dataset \\cite{bergmann2019_mvtec_ad_cvpr, bergmann2021_mvtec_ad_ijcv} has catalyzed the development of methods for industrial applications.\nIt comprises 15 separate inspection scenarios, each consisting of a training set and a test set.\nEach training set contains only normal images, for example, defect-free screws, while the test sets also contain anomalous images.\nThis represents a frequent challenge in real-world applications where the types and possible locations of defects are unknown during the development of the anomaly detection system.\nTherefore, it is a challenging yet crucial requirement that methods perform well when trained only on normal images.\n\nRecently, several new industrial anomaly detection datasets have been introduced \\cite{bergmann2022_mvtec_3dad, bergmann2021_mvtec_loco_ijcv, zou2022spot, mishra2021vt, huang2018_magnetic_tile_dataset, jezek2021deep}.\nThe Visual Anomaly (VisA) dataset \\cite{zou2022spot} and the MVTec Logical Constraints (MVTec LOCO) dataset \\cite{bergmann2021_mvtec_loco_ijcv} follow the design of MVTec AD and comprise twelve and five anomaly detection scenarios, respectively.\nThey contain anomalies that are empirically more challenging than those of MVTec AD\\@.\nFurthermore, MVTec LOCO contains not only structural anomalies, such as stains or scratches, but also logical anomalies.\nThese are violations of logical constraints, for example, a wrong ordering or a wrong combination of normal objects.\nWe refer to MVTec AD, VisA, and MVTec LOCO as dataset collections, as each scenario is a separate dataset consisting of a training and a test set.\nAll three provide pixel-precise defect segmentation masks for evaluating the anomaly localization performance of a method.\n\n"
                },
                "subsection 2.2": {
                    "name": "Anomaly Detection Methods",
                    "content": "\n\nTraditional computer vision algorithms have been applied successfully to industrial anomaly detection tasks for several decades \\cite{steger2018_mva_book}.\nThese algorithms commonly fulfill the requirement of processing an image within a few milliseconds.\nBergmann \\etal \\cite{bergmann2021_mvtec_ad_ijcv} evaluate some of these methods and find that they fail when requirements such as well-aligned objects are not met.\nDeep-learning-based methods have been shown to handle such cases more robustly \\cite{bergmann2021_mvtec_ad_ijcv, bergmann2021_mvtec_loco_ijcv}.\n\n\nA successful approach in the recent past has been to apply outlier detection and density estimation methods in the feature space of a pretrained and frozen convolutional neural network (CNN).\nIf feature vectors can be mapped to input pixels, assigning their outlier scores to the respective pixels yields a 2D anomaly map of pixel anomaly scores.\nCommon methods include multivariate Gaussian distributions \\cite{defard2021_PaDiM, rippel2021_Gaussian, li2021cutpaste}, Gaussian Mixture Models \\cite{mishra2021vt, zong2018deep}, Normalizing Flows \\cite{yu2021fastflow, rudolph2022_cross_flows, gudovskiy2022_CFLOW, rudolph2021_differnet, rezende2015variational}, and the k-Nearest Neighbor (kNN) algorithm \\cite{napoletano2018_cnn_feature_dictionary_nanofibres, cohen2020_subimage, roth2022towards, nazare2018_pretrained_cnns_for_ad}.\nA runtime bottleneck for kNN-based methods is the search for nearest neighbors during inference.\nWith PatchCore \\cite{roth2022towards}, Roth \\etal therefore perform kNN on a reduced database of clustered feature vectors.\nThey achieve state-of-the-art anomaly detection results on MVTec AD\\@.\nIn our experiments, we include PatchCore and FastFlow \\cite{yu2021fastflow}, a recent Normalizing-Flow-based method with a comparatively low inference runtime.\n\n\nBergmann \\etal \\cite{bergmann2020_uninformed_cvpr} propose a student--teacher (\\studteach) framework for anomaly detection, in which the teacher is a pretrained frozen CNN\\@.\nThey train student networks to mimic the output of the teacher on the training images.\nBecause the students have not seen anomalous images during training, they generally fail to predict the teacher's output on these images, which enables anomaly detection.\nVarious modifications of \\studteach\\ have been proposed \\cite{salehi2021_st_ad, wang2021student_teacher, rudolph2023asymmetric}.\nRudolph \\etal \\cite{rudolph2023asymmetric} reach a competitive anomaly detection performance on MVTec AD by restricting the teacher to be an invertible neural network.\nWe compare our method to their Asymmetric Student Teacher (AST) approach and to the original \\studteach\\ method \\cite{bergmann2020_uninformed_cvpr}. \n\n\nGenerative models such as autoencoders \\cite{bergmann2018_ssim_ae, park2020_mmnad, baur2019_ano_vae_gan, liu2020_visually_explaining_vaes, sakurada2014_aes_for_ad, bergmann2021_mvtec_loco_ijcv, gong2019_mem_ae_iccv} and GANs \\cite{goodfellow2014_gans, schlegl2017_anogan, schlegl2019_fast_anogan, perera2019_cvpr_ocgan, akcay2019ganomaly} have been used extensively for anomaly detection.\nRecent autoencoder-based methods rely on accurate reconstructions of normal images and inaccurate reconstructions of anomalous images \\cite{bergmann2018_ssim_ae, park2020_mmnad, bergmann2021_mvtec_loco_ijcv, gong2019_mem_ae_iccv}.\nThis enables detecting anomalies by comparing the reconstruction to the input image.\nA common problem are false-positive detections caused by inaccurate reconstructions of normal images, e.g., blurry reconstructions.\nTo avoid this, GCAD \\cite{bergmann2021_mvtec_loco_ijcv} lets an autoencoder reconstruct images in the feature space of a pretrained network.\nAnother recent reconstruction-based method is DSR \\cite{zavrtanik2022dsr}, which uses the latent space of a pretrained autoencoder and generates synthetic anomalies in it.\nSimilarly, the recently proposed SimpleNet \\cite{Liu_2023_CVPR} generates synthetic anomalies in a pretrained feature space to train a discriminator network for detecting anomalous features.\nIn our experiments, we include GCAD, DSR, and SimpleNet.\n\n"
                }
            },
            "section 3": {
                "name": "Method",
                "content": "\n\nWe describe the components of \\ourmethod\\ in the following subsections.\nIt begins with the efficient extraction of features from a pretrained neural network in \\cref{sec:method_pdn}.\nWe detect anomalous features at test time using a lightweight student--teacher model, as described in \\cref{sec:method_structural}.\nA key challenge is to achieve a competitive anomaly detection performance while keeping the overall runtime low.\nTo this end, we introduce a novel loss function for the training of a student--teacher model.\nIn \\cref{sec:method_logical}, we explain how to efficiently detect logical anomalies with an autoencoder-based approach.\nFinally, we provide a solution for calibrating and combining the detection results of the autoencoder with those of the student--teacher model in \\cref{sec:method_balancing}.\n\n",
                "subsection 3.1": {
                    "name": "Efficient Patch Descriptors",
                    "content": "\n\\label{sec:method_pdn}\n\nRecent anomaly detection methods commonly use the features of a deep pretrained network, such as a WideResNet-101 \\cite{zagoruyko2016wideresnet_wrn, roth2022towards}.\nWe use a network with a drastically reduced depth as a feature extractor.\nIt consists of only four convolutional layers and is visualized in \\Cref{fig:pdn}.\nEach output neuron has a receptive field of 33$\\times$33 pixels and thus each output feature vector describes a 33$\\times$33 patch.\nDue to this clear correspondence, we refer to the network as a patch description network (PDN).\nThe PDN is fully convolutional and can be applied to an image of variable size to generate all feature vectors in a single forward pass.\n\n\n\n\n\nThe \\studteach\\ method \\cite{bergmann2020_uninformed_cvpr} also uses features from networks with only few convolutional layers.\nThe computational cost of these networks is nevertheless high because of the lack of downsampling in convolutional and pooling layers.\nThe number of parameters of the networks used by \\studteach\\ is comparably low (between 1.6 and 2.7 million per network).\nYet, executing a single network takes longer and requires more memory in our experiments than a \\unet\\ \\cite{ronneberger2015_u_net} with 31 million parameters, an architecture used by the GCAD method \\cite{bergmann2021_mvtec_loco_ijcv}.\nThis demonstrates how the number of parameters can be a misleading proxy metric for the latency, throughput, and memory footprint of a method.\nModern classification architectures typically perform downsampling early to reduce the size of feature maps and thus the runtime and memory requirements \\cite{he2016_resnet_paper}.\nWe implement this in our PDN via strided average-pooling layers after the first and the second convolutional layer.\nWith the proposed PDN, we are able to obtain the features for an image of size 256$\\times$256 in less than \\SI{800}{\\micro\\second} on an NVIDIA RTX A6000 GPU.\n\n\nTo make the PDN generate expressive features, we distill a deep pretrained classification network into it.\nFor a controlled comparison, we use the same pretrained features as PatchCore \\cite{roth2022towards} from a WideResNet-101.\nWe train the PDN on images from ImageNet \\cite{russakovsky2015_alexnet} by minimizing the mean squared difference between its output and the features extracted from the pretrained network.\nWe provide the full list of training hyperparameters in Appendix \\ref{subsec:distillation}.\nBesides higher efficiency, the PDN has another benefit in comparison to the deep networks used by recent methods.\nBy design, a feature vector generated by the PDN only depends on the pixels in its respective 33$\\times$33 patch.\nThe feature vectors of pretrained classifiers, on the other hand, exhibit long-range dependencies on other parts of the image.\nThis is shown in \\Cref{fig:artifacts}, using PatchCore's feature extractors as an example.\nThe well-defined receptive field of the PDN ensures that an anomaly in one part of the image cannot trigger anomalous feature vectors in other, distant parts, which would impair the localization of anomalies.\n\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Lightweight Student--Teacher",
                    "content": "\n\\label{sec:method_structural}\n\nFor detecting anomalous feature vectors, we use a student--teacher (S--T) approach in which the teacher is given by our distilled PDN.\nSince we can execute the PDN in under a millisecond, we use its architecture for the student as well, resulting in a low overall latency.\nThis lightweight student--teacher pair, however, lacks techniques used by previous methods to increase the anomaly detection performance: ensembling multiple teachers and students \\cite{bergmann2020_uninformed_cvpr}, using features from a pyramid of layers \\cite{wang2021student_teacher}, and using architectural asymmetry between the student and the teacher network \\cite{rudolph2023asymmetric}.\nWe therefore introduce a training loss that substantially improves the detection of anomalies without affecting the computational requirements at test time.\n\nWe observe that in the standard \\studteach\\ framework, increasing the number of training images can improve the student's ability to imitate the teacher on anomalies.\nThis worsens the anomaly detection performance.\nAt the same time, deliberately decreasing the number of training images can suppress important information about normal images.\nOur goal is to show the student enough data so that it can mimic the teacher sufficiently on normal images while avoiding generalization to anomalous images.\nSimilar to Online Hard Example Mining \\cite{shrivastava2016training}, we therefore restrict the student's loss to the most relevant parts of an image.\nThese are the patches where the student currently mimics the teacher the least.\nWe propose a hard feature loss, which only uses the output elements with the highest loss for backpropagation.\n\nFormally, we apply a teacher $T$ and a student $S$ to a training image $I$, which yields $T(I) \\in \\mathbb{R}^{C \\times W \\times H}$ and $S(I) \\in \\mathbb{R}^{C \\times W \\times H}$.\nWe compute the squared difference for each tuple $(c, w, h)$ as $D_{c, w, h} = (T(I)_{c, w, h} - S(I)_{c, w, h})^2$.\nBased on a mining factor $p_\\mathrm{hard} \\in [0, 1]$, we then compute the $p_\\mathrm{hard}$-quantile of the elements of $D$.\nGiven the $p_\\mathrm{hard}$-quantile $d_\\mathrm{hard}$, we compute the training loss $L_\\mathrm{hard}$ as the mean of all $D_{c, w, h} \\geq d_\\mathrm{hard}$.\nSetting $p_\\mathrm{hard}$ to zero would yield the original \\studteach\\ loss.\nIn our experiments, we set $p_\\mathrm{hard}$ to $0.999$, which corresponds to using, on average, ten percent of the values in each of the three dimensions of $D$ for backpropagation.\n\\Cref{fig:ohem} visualizes the effect of the hard feature loss for $p_\\mathrm{hard} = 0.999$.\nDuring inference, the 2D anomaly score map $M \\in \\mathbb{R}^{W \\times H}$ is given by $M_{w, h} = C^{-1} \\sum_c D_{c, w, h}$, i.e., by $D$ averaged across channels.\nIt assigns an anomaly score to each feature vector.\nBy using the hard feature loss, we avoid outliers in the anomaly scores on normal images, i.e., false-positive detections.\n\n\n\n\n\n\n\n\nIn addition to the hard feature loss, we use a loss penalty during training that further hinders the student from imitating the teacher on images that are not part of the normal training images.\nIn the standard \\studteach\\ framework, the teacher is pretrained on an image classification dataset, or it is a distilled version of such a pretrained network.\nThe student is not trained on that pretraining dataset but only on the application's normal images.\nWe propose to also use the images from the teacher's pretraining during the training of the student.\nSpecifically, we sample a random image $P$ from the pretraining dataset, in our case ImageNet, in each training step.\nWe compute the loss of the student as $L_\\mathrm{ST} = L_\\mathrm{hard} + (CWH)^{-1}\\sum_c \\|S(P)_c\\|_F^2$.\nThis penalty hinders the student from generalizing its imitation of the teacher to out-of-distribution images.\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Logical Anomaly Detection",
                    "content": "\n\\label{sec:method_logical}\n\n\nThere are many types of logical anomalies, such as missing, misplaced, or surplus objects or the violation of geometrical constraints, for example, the length of a screw.\nAs recommended by the authors of the MVTec LOCO dataset \\cite{bergmann2021_mvtec_loco_ijcv}, we use an autoencoder for learning logical constraints of the training images and detecting violations of these constraints.\n\\Cref{fig:architecture} depicts the anomaly detection methodology for \\ourmethod.\nIt consists of the aforementioned student--teacher pair and an autoencoder.\nThe autoencoder is trained to predict the output of the teacher.\nFormally, we apply an autoencoder $A$ to a training image $I$, yielding $A(I) \\in \\mathbb{R}^{C \\times W \\times H}$, and compute the loss as $L_\\mathrm{AE} = (CWH)^{-1} \\sum_c \\|T(I)_c - A(I)_c\\|_F^2$.\nWe use a standard convolutional autoencoder comprising strided convolutions in the encoder and bilinear upsampling in the decoder.\nWe provide the detailed hyperparameters of its layers in in Appendix \\ref{sec:details_ours}.\n\nIn contrast to the patch-based student, the autoencoder must encode and decode the complete image through a bottleneck of 64 latent dimensions.\nOn images with logical anomalies, the autoencoder usually fails to generate the correct latent code for reconstructing the image in the teacher's feature space.\nHowever, its reconstructions are also flawed on normal images, as autoencoders generally struggle with reconstructing fine-grained patterns \\cite{brox2016_learned_visual_similarity_metrics, bergmann2018_ssim_ae}.\nThis is the case for the background grids in \\Cref{fig:architecture}.\nUsing the difference between the teacher's output and the autoencoder's reconstruction as an anomaly map would cause false-positive detections in these cases.\nInstead, we double the number of output channels of our student network and train it to predict the output of the autoencoder in addition to the output of the teacher.\nLet $S'(I) \\in \\mathbb{R}^{C \\times W \\times H}$ denote the additional output channels of the student.\nThe student's additional loss is then $L_\\mathrm{STAE} = (CWH)^{-1} \\sum_c \\|A(I)_c - S'(I)_c\\|_F^2$.\nThe total training loss is the sum of $L_\\mathrm{AE}$, $L_\\mathrm{ST}$, and $L_\\mathrm{STAE}$.\n\nThe student learns the systematic reconstruction errors of the autoencoder on normal images, e.g., blurry reconstructions.\nAt the same time, it does not learn the reconstruction errors for anomalies because these are not part of the training set.\nThis makes the difference between the autoencoder's output and the student's output well-suited for computing the anomaly map.\nAnalogous to the student--teacher pair, the anomaly map is the squared difference between the two outputs, averaged across channels.\nWe refer to this anomaly map as the global anomaly map and to the anomaly map generated by the student--teacher pair as the local anomaly map.\nWe average these two anomaly maps to compute the combined anomaly map and use its maximum value as the image-level anomaly score.\nThe combined anomaly map thus contains the detection results of the student--teacher pair and the detection results of the autoencoder--student pair.\nSharing the student's hidden layers in the computation of these detection results allows our method to maintain low computational requirements, while enabling the detection of structural and logical anomalies.\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Anomaly Map Normalization",
                    "content": "\n\\label{sec:method_balancing}\n\n\nThe local and the global anomaly map must be normalized to similar scales before averaging them to obtain the combined anomaly map.\nThis is important for cases where the anomaly is only detected in one of the maps, such as in \\Cref{fig:architecture}.\nOtherwise, noise in one map could make accurate detections in the other map indiscernible in the combined map.\nTo estimate the scale of the noise in normal images, we use validation images, i.e., unseen images from the training set.\nFor each of the two anomaly map types, we compute the set of all pixel anomaly scores across the validation images.\nWe then compute two $p$-quantiles for each set: $q_a$ and $q_b$, for $p=a$ and $p=b$, respectively.\nWe determine a linear transformation that maps $q_a$ to an anomaly score of $0$ and $q_b$ to a score of $0.1$.\nAt test time, the local and global anomaly maps are normalized with the respective linear transformation.\n\nBy using quantiles, the normalization becomes robust to the distribution of anomaly scores on normal images, which can vary between scenarios.\nWhether the scores between~$q_a$ and~$q_b$ are normally distributed or a mixture of Gaussians or follow another distribution has no influence on the normalization.\nOur experiments include an ablation study on the values of~$a$ and~$b$.\nThe choice of the mapping destination values $0$ and $0.1$ has no effect on anomaly detection metrics such as the area under the ROC curve (AU-ROC).\nThat is because the AU-ROC only depends on the ranking of scores, not on their scale.\nWe choose $0$ and $0.1$ because they yield maps that are suitable for a standard zero-to-one color scale.\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\n\nWe compare \\ourmethod\\ to AST \\cite{rudolph2023asymmetric}, DSR \\cite{zavrtanik2022dsr}, FastFlow \\cite{yu2021fastflow}, GCAD \\cite{bergmann2021_mvtec_loco_ijcv}, PatchCore \\cite{roth2022towards}, SimpleNet \\cite{Liu_2023_CVPR}, and \\studteach\\ \\cite{bergmann2020_uninformed_cvpr}, using official implementations where available.\nWe provide configuration details for all evaluated methods in Appendix \\ref{sec:details_others}.\nGCAD consists of an ensemble of two anomaly detection models that use different feature extractors.\nWe find that one of the two ensemble members performs better on average than the combined ensemble and therefore report the results for this member.\nThis reduces the latency reported for GCAD by a factor of two.\nFor SimpleNet, we are able to reproduce the official results but find that SimpleNet tunes the training duration on the test images of a scenario.\nDuring training, the model is repeatedly evaluated on all test images and the maximum of all obtained test scores is reported after training.\nWe disable this technique, since it overestimates the actual performance of the model on unseen images.\nIn practice, it would furthermore require a validation set with anomalous images.\n\\mbox{MVTec AD}, VisA, and MVTec LOCO do not include anomalous images in their training and validation sets to avoid defect-type-specific tuning of hyperparameters.\nFor SimpleNet, we therefore evaluate the final trained model, following common practice.\n\nFor PatchCore, we include two variants: the default single model variant, for which the authors report the lowest latency, and the ensemble variant, denoted by \\patchcoreens.\nWe are able to reproduce the official results but disable the cropping of the center \\SI{76.6}{\\percent} of input images for a fair comparison.\nIn the case of MVTec AD, \\SI{99.9}{\\percent} of the defects lie fully or partially within this cropped area.\nIn real-world applications, anomalies can occur outside of this area as well.\nWe disable custom cropping, as it implies knowledge about the anomalies in the test set.\nFor FastFlow, we use the version based on the WideResNet-50-2 feature extractor, as it is similar to the WideResNet used by PatchCore, SimpleNet, and our method.\nWe use the implementation provided by the Intel anomalib \\cite{akcay2022anomalib} but disable early stopping, i.e., the scenario-specific tuning of the training duration on test images, analogously to SimpleNet.\nWith early stopping enabled, EfficientAD itself achieves an image-level detection AU-ROC of \\SI{99.8}{\\percent} on MVTec AD.\n\n\nFor our method, we evaluate two variants: \\mbox{\\ourmethod-S} and \\ourmethod-M.\n\\ourmethod-S uses the architecture displayed in \\Cref{fig:pdn} for the teacher and the student.\nFor \\ourmethod-M, we double the number of kernels in the hidden convolutional layers of the teacher and the student.\nFurthermore, we insert a 1$\\times$1 convolution after the second pooling layer and after the last convolutional layer.\nWe provide a list of implementation details, such as the learning rate schedule, in Appendix \\ref{subsec:training_and_evaluation}.\n\n\nWe evaluate each method on the 32 anomaly detection scenarios of MVTec AD, VisA, and MVTec LOCO\\@.\nThe anomaly detection performance of a method is measured with the AU-ROC based on its predicted image-level anomaly scores.\nWe measure the anomaly localization performance using the AU-PRO segmentation metric up to a false positive rate of \\SI{30}{\\percent}, as recommended by \\cite{bergmann2021_mvtec_ad_ijcv}.\nFor MVTec LOCO, we use the AU-sPRO metric \\cite{bergmann2021_mvtec_loco_ijcv}, a generalization of the AU-PRO metric for evaluating the localization of logical anomalies.\nAppendix \\ref{sec:ad_metrics} provides the results for additional anomaly detection metrics, such as the area under the precision-recall curve and the pixel-wise AU-ROC computed on pixel anomaly scores.\n\n\\addtolength{\\tabcolsep}{-2pt}  \n\n\\addtolength{\\tabcolsep}{2pt}\n\n\\addtolength{\\tabcolsep}{-3pt}  \n\n\\addtolength{\\tabcolsep}{3pt}\n\n\n\\addtolength{\\tabcolsep}{-0.5pt}  \n\n\\addtolength{\\tabcolsep}{-0.5pt}\n\nWhen reporting the AU-ROC or AU-PRO for a dataset collection, we follow the policy of the dataset authors.\nFor each collection, we evaluate the respective metric for each scenario and then compute the mean across scenarios.\nFor MVTec LOCO, we use the official evaluation script, which gives logical and structural anomalies an equal weight in the computed metrics.\nWhen reporting the average AU-ROC or AU-PRO on the three dataset collections, we compute the average of the three dataset means.\nThus, an overall average score weights logical anomalies and structural anomalies by roughly one-sixth and five-sixths, respectively.\nWe provide the evaluation results for each of the 32 anomaly detection scenarios individually in the appendix to enable an evaluation with a custom weighting.\n\n\n\\Cref{tab:main} reports the overall anomaly detection performance for each method.\n\\ourmethod\\ achieves a strong image-level detection and pixel-level localization of anomalies.\nReliably localizing anomalies in an image provides explainable detection results and allows the discovery of spurious correlations in detections.\nIt also enables a flexible postprocessing, such as excluding defect segmentations based on their size.\n\n\\Cref{tab:per_dataset} breaks down the overall anomaly detection performance into the three dataset collections.\nIt shows that the lead of \\ourmethod\\ on MVTec LOCO is in equal parts due to its performance on logical and on structural anomalies.\nIn \\Cref{tab:hyperparams}, we assess the robustness of \\ourmethod\\ to varying hyperparameters.\n\n\n\nFurthermore, we measure the computational cost of each method during inference.\nAs explained above, the number of parameters can be a misleading proxy metric for the latency and throughput of convolutional architectures since it does not consider the resolution of a convolution's input feature map, i.e., how often a parameter is used in a forward pass.\nSimilarly, the number of floating point operations (FLOPs) can be misleading since it does not take into account how easily computations can be parallelized.\nFor transparency, we report the number of parameters, the number of FLOPs, and the memory footprint of each method in Appendix \\ref{sec:efficiency_metrics}.\nHere, we focus on the metrics that are most relevant in anomaly detection applications: the latency and the throughput.\nWe measure the latency with a batch size of 1 and the throughput with a batch size of 16.\n\\Cref{tab:main} reports the measurements for each method on an NVIDIA RTX A6000 GPU.\n\\Cref{fig:latency_per_gpu} shows the latency of each method on each of the GPUs in our experimental setup.\nAppendix \\ref{sec:efficiency_metrics} contains a detailed description of our timing methodology.\n\n\n\n\nIn \\Cref{fig:qual}, we show randomly sampled qualitative results of \\ourmethod\\ on the VisA dataset collection.\nAppendix \\ref{sec:qualitative_results} provides qualitative results for the other evaluated methods and dataset collections as well.\n\n\nWe examine the effects of the components of \\ourmethod\\ in the ablation study shown in \\Cref{tab:ablation_cumulative} and \\Cref{tab:ablation_isolated}.\nFor experiments without the proposed quantile-based map normalization, we use a Gaussian-based map normalization as a baseline instead.\nThere, we compute the linear transformation parameters such that pixel anomaly scores on the validation set have a mean of zero and a variance of one.\nThis baseline normalization is sensitive to the distribution of validation anomaly scores, which can vary between scenarios.\nThe quantile-based normalization is independent of how the scores between $q_a$ and $q_b$ are distributed and performs substantially better than the baseline.\n\nWe also evaluate the effect of the two proposed loss terms for training the student--teacher pair.\nThe hard feature loss increases the anomaly detection AU-ROC by \\SI{1.0}{\\percent} in \\Cref{tab:ablation_cumulative}.\nThis improvement alone is greater than or equal to each of the improvement margins between the consecutive rows of FastFlow, DSR, PatchCore, \\patchcoreens, and AST in \\Cref{tab:main}.\nThe student's penalty on pretraining images further improves the anomaly detection performance.\nNotably, the proposed map normalization, the hard feature loss, and the pretraining penalty keep the computational requirements of \\ourmethod\\ low, while creating a substantial margin w.r.t. the anomaly detection performance.\n\n\n\n\n\n\n\n\n\n"
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\nIn this paper, we introduce \\ourmethod, a method with a strong anomaly detection performance and a high computational efficiency.\nIt sets new standards for the detection of structural as well as logical anomalies.\nBoth \\ourmethod-S and \\ourmethod-M outperform other methods on the detection and the localization of anomalies by a large margin.\nCompared to AST, the second-best method, \\ourmethod-S reduces the latency by a factor of 24 and increases the throughput by a factor of 15.\nIts low latency, high throughput, and high detection rate make it suitable for real-world applications.\nFor future anomaly detection research, \\ourmethod\\ is an important baseline and a fruitful foundation.\nIts efficient patch description network, for instance, can be used as a feature extractor in other anomaly detection methods as well to reduce their latency.\n\n\\paragraph{Limitations.}\nThe student--teacher model and the autoencoder are designed to detect anomalies of different types.\nThe autoencoder detects logical anomalies, while the student--teacher model detects coarse and fine-grained structural anomalies.\nFine-grained logical anomalies, however, remain a challenge -- for example a screw that is two millimeters too long.\nTo detect these, practitioners would have to use traditional metrology methods \\cite{steger2018_mva_book}.\nAs for the limitations in comparison to other recent anomaly detection methods:\nIn contrast to kNN-based methods, our approach requires training, especially for the autoencoder to learn the logical constraints of normal images.\nThis takes twenty minutes in our experimental setup.\n\n\\onecolumn\n\n{\\small\n}\n\\vspace{-1mm}\n"
            },
            "section 6": {
                "name": "Appendices",
                "content": "\n\n\\appendices\n\nWe provide the following supplementary material:\n\n\\begin{itemize}[topsep=0.1em]\n    \\itemsep-1mm\n    \\item (\\ref{sec:details_ours}) Implementation details for \\ourmethod, including the training and inference procedure of \\ourmethod\\ on the dataset of an anomaly detection scenario (\\ref{subsec:training_and_evaluation}) and the distillation training of the patch description network (\\ref{subsec:distillation}).\n    \\item (\\ref{sec:details_others}) Implementation and configuration details for other evaluated methods.\n    \\item (\\ref{sec:backbones}) Evaluation of the anomaly detection performance of \\ourmethod\\ for different distillation backbones.\n    \\item (\\ref{sec:ad_metrics}) Results for additional anomaly detection metrics, such as the area under the precision recall curve.\n    \\item (\\ref{sec:efficiency_metrics}) Description of our timing methodology and results for additional computational efficiency metrics such as the number of parameters.\n    \\item (\\ref{sec:qualitative_results}) Qualitative results in the form of anomaly maps generated by \\ourmethod\\ and other methods on the evaluated datasets.\n    \\item Anomaly detection results for each method on each of the 32 scenarios from MVTec AD, VisA, and MVTec LOCO in the \\href{https://www.mydrive.ch/shares/79401/c41dcdb937972fb43d5cdd7bfa7072f8/download/449203483-1690527455/per_scenario_results.json}{\\texttt{per\\_scenario\\_results.json}} file \\footnote{\\url{https://www.mydrive.ch/shares/79401/c41dcdb937972fb43d5cdd7bfa7072f8/download/449203483-1690527455/per_scenario_results.json}}.\n\\end{itemize}\n\\vspace{0cm}\n\n"
            },
            "section 7": {
                "name": "Implementation Details for \\ourmethod",
                "content": "\n\\label{sec:details_ours}\n\n\n",
                "subsection 7.1": {
                    "name": "Training and Inference",
                    "content": "\n\\label{subsec:training_and_evaluation}\n\n\\Cref{alg:training} describes the training of \\ourmethod-S and \\Cref{alg:inference} explains the inference procedure.\nFor \\mbox{\\ourmethod-M}, replace the architecture of \\Cref{tab:arch_eads} with that of \\Cref{tab:arch_eadm}.\n\n\\begin{algorithm}[h!]\n  \\caption{\\ourmethod-S Training Algorithm} \n  \\label{alg:training} \n  \\begin{algorithmic} [1]\n    \\Require{A pretrained teacher network $T : \\mathbb{R}^{3 \\times 256 \\times 256} \\rightarrow \\mathbb{R}^{384 \\times 64 \\times 64}$ with an architecture as given in \\Cref{tab:arch_eads}}\n    \\Require{A sequence of training images $\\mathcal{I}_\\mathrm{train}$ with $I_\\mathrm{train} \\in \\mathbb{R}^{3 \\times 256 \\times 256}$ for each $I_\\mathrm{train} \\in \\mathcal{I}_\\mathrm{train}$}\n    \\Require{A sequence of validation images $\\mathcal{I}_\\mathrm{val}$ with $I_\\mathrm{val} \\in \\mathbb{R}^{3 \\times 256 \\times 256}$ for each $I_\\mathrm{val} \\in \\mathcal{I}_\\mathrm{val}$}\n    \\State{Randomly initialize a student network $S : \\mathbb{R}^{3 \\times 256 \\times 256} \\rightarrow \\mathbb{R}^{768 \\times 64 \\times 64}$ with an architecture as given in \\Cref{tab:arch_eads}}\n    \\State{Randomly initialize an autoencoder $A : \\mathbb{R}^{3 \\times 256 \\times 256} \\rightarrow \\mathbb{R}^{384 \\times 64 \\times 64}$ with an architecture as given in \\Cref{tab:arch_ae}}\n    \\For{$c \\in {1, \\dots, 384}$} \\Comment{Compute teacher channel normalization parameters $\\mu \\in \\mathbb{R}^{384}$ and $\\sigma \\in \\mathbb{R}^{384}$}\n    \\State{Initialize an empty sequence $X \\leftarrow (~)$}\n    \\For{$I_\\mathrm{train} \\in \\mathcal{I}_\\mathrm{train}$}\n        \\State{$Y'\\leftarrow T(I_\\mathrm{train})$}\n        \\State{$X \\leftarrow X^\\frown \\mathrm{vec}(Y'_c)$} \\Comment{Append the channel output to $X$}\n    \\EndFor\n    \\State{Set $\\mu_c$ to the mean and $\\sigma_c$ to the standard deviation of the elements of $X$}\n    \\algstore{alg_training}\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\begin{algorithm}[h]\n  \\begin{algorithmic} [1]\n    \\algrestore{alg_training}\n    \\EndFor\n    \\State{Initialize Adam \\cite{kingma2014_adam} with a learning rate of $10^{-4}$ and a weight decay of $10^{-5}$ for the parameters of $S$ and $A$}\n    \\For{iteration $= 1,\\dots, \\num{70000}$}\n        \\State{Choose a random training image $I_\\mathrm{train}$ from $\\mathcal{I}_\\mathrm{train}$}\n        \\State{$Y'\\leftarrow T(I_\\mathrm{train})$} \\Comment{Forward pass of the student--teacher pair}\n        \\State{Compute the normalized teacher output $\\hat{Y}$ given by $\\hat{Y}_c = (Y'_c - \\mu_c) \\sigma_c^{-1}$ for each $c \\in \\{1, \\dots, 384\\}$}\n        \\State{$Y^\\mathrm{S} \\leftarrow S(I_\\mathrm{train})$}\n        \\State{Set $Y^\\mathrm{ST} \\in \\mathbb{R}^{384 \\times 64 \\times 64}$ to the first 384 channels of $Y^\\mathrm{S} \\in \\mathbb{R}^{768 \\times 64 \\times 64}$}\n        \\State{Compute the squared difference between $\\hat{Y}$ and $Y^\\mathrm{ST}$ for each tuple $(c, w, h)$ as $D^\\mathrm{ST}_{c, w, h} = (\\hat{Y}_{c, w, h} - Y^\\mathrm{ST}_{c, w, h})^2$}\n        \\State{Compute the $0.999$-quantile of the elements of $D^\\mathrm{ST}$, denoted by $d_\\mathrm{hard}$}\n        \\State{Compute the loss $L_\\mathrm{hard}$ as the mean of all $D^\\mathrm{ST}_{c, w, h} \\geq d_\\mathrm{hard}$ }\n        \\State{Choose a random pretraining image $P \\in \\mathbb{R}^{3 \\times 256 \\times 256}$ from ImageNet \\cite{russakovsky2015_alexnet}}\n        \\State{Compute the loss $L_\\mathrm{ST} = L_\\mathrm{hard} + (384 \\cdot 64 \\cdot 64)^{-1}\\sum_{c=1}^{384} \\|S(P)_c\\|_F^2$}\n        \\State{Randomly choose an augmentation index $i_\\mathrm{aug} \\in \\{1, 2, 3\\}$} \\Comment{Augment $I_\\mathrm{train}$ for $A$ using torchvision \\cite{paszke2019_PyTorch}}\n        \\State{Sample an augmentation coefficient $\\lambda$ from the uniform distribution $U(0.8, 1.2)$}\n        \\If{$i_\\mathrm{aug} == 1$}\n        $I_\\mathrm{aug} \\leftarrow \\mathtt{torchvision.transforms.functional\\_pil.adjust\\_brightness}(I_\\mathrm{train}, \\lambda)$\n        \\ElsIf{$i_\\mathrm{aug} == 2$}\n            $I_\\mathrm{aug} \\leftarrow \\mathtt{torchvision.transforms.functional\\_pil.adjust\\_contrast}(I_\\mathrm{train}, \\lambda)$\n        \\ElsIf{$i_\\mathrm{aug} == 3$}\n            $I_\\mathrm{aug} \\leftarrow \\mathtt{torchvision.transforms.functional\\_pil.adjust\\_saturation}(I_\\mathrm{train}, \\lambda)$\n        \\EndIf\n        \\State{$Y^\\mathrm{A} \\leftarrow A(I_\\mathrm{aug})$} \\Comment{Forward pass of the autoencoder--student pair}\n        \\State{$Y'\\leftarrow T(I_\\mathrm{aug})$}\n        \\State{Compute the normalized teacher output $\\hat{Y}$ given by $\\hat{Y}_c = \\sigma_c^{-1}(Y'_c - \\mu_c)$ for each $c \\in \\{1, \\dots, 384\\}$}\n        \\State{$Y^\\mathrm{S} \\leftarrow S(I_\\mathrm{aug})$}\n        \\State{Set $Y^\\mathrm{STAE} \\in \\mathbb{R}^{384 \\times 64 \\times 64}$ to the last 384 channels of $Y^\\mathrm{S} \\in \\mathbb{R}^{768 \\times 64 \\times 64}$}\n        \\State{Compute the squared difference between $\\hat{Y}$ and $Y^\\mathrm{A}$ for each tuple $(c, w, h)$ as $D^\\mathrm{AE}_{c, w, h} = (\\hat{Y}_{c, w, h} - Y^\\mathrm{A}_{c, w, h})^2$}\n        \\State{Compute the squared difference between $Y^\\mathrm{A}$ and $Y^\\mathrm{STAE}$ for each tuple $(c, w, h)$ as $D^\\mathrm{STAE}_{c, w, h} = (Y^\\mathrm{A}_{c, w, h} - Y^\\mathrm{STAE}_{c, w, h})^2$}\n        \\State{Compute the loss $L_\\mathrm{AE}$ as the mean of all elements $D^\\mathrm{AE}_{c, w, h}$ of $D^\\mathrm{AE}$}\n        \\State{Compute the loss $L_\\mathrm{STAE}$ as the mean of all elements $D^\\mathrm{STAE}_{c, w, h}$ of $D^\\mathrm{STAE}$}\n        \\State{Compute the total loss $L_\\mathrm{total} = L_\\mathrm{ST} + L_\\mathrm{AE} + L_\\mathrm{STAE}$}  \\Comment{Backward pass}\n        \\State{Update the union of the parameters of $S$ and $A$, denoted by $\\phi$, using the gradient $\\nabla_\\phi L_\\mathrm{total}$ }\n        \\If{iteration $>$ \\num{66500}}\n        \\State{Decay the learning rate to $10^{-5}$}\n        \\EndIf\n    \\EndFor\n    \\State{Initialize empty sequences $X_\\mathrm{ST} \\leftarrow (~)$ and $X_\\mathrm{AE} \\leftarrow (~)$} \\Comment{Quantile-based map normalization on validation images}\n    \\For{$I_\\mathrm{val} \\in \\mathcal{I}_\\mathrm{val}$}\n        \\State{$Y'\\leftarrow T(I_\\mathrm{val}),\\;\\;Y^\\mathrm{S} \\leftarrow S(I_\\mathrm{val}),\\;\\;Y^\\mathrm{A} \\leftarrow A(I_\\mathrm{val})$}\n        \\State{Compute the normalized teacher output $\\hat{Y}$ given by $\\hat{Y}_c = (Y'_c - \\mu_c) \\sigma_c^{-1}$ for each $c \\in \\{1, \\dots, 384\\}$}\n        \\State{Split the student output into $Y^\\mathrm{ST} \\in \\mathbb{R}^{384 \\times 64 \\times 64}$ and $Y^\\mathrm{STAE} \\in \\mathbb{R}^{384 \\times 64 \\times 64}$ as above}\n        \\State{Compute the squared difference $D^\\mathrm{ST}_{c, w, h} = (\\hat{Y}_{c, w, h} - Y^\\mathrm{ST}_{c, w, h})^2$ for each tuple $(c, w, h)$}\n        \\State{Compute the squared difference $D^\\mathrm{STAE}_{c, w, h} = (Y^\\mathrm{A}_{c, w, h} - Y^\\mathrm{STAE}_{c, w, h})^2$ for each tuple $(c, w, h)$ }\n        \\State{Compute the anomaly maps $M_\\mathrm{ST} = 384^{-1}\\sum_{c=1}^{384} D^\\mathrm{ST}_c$ and $M_\\mathrm{AE} = 384^{-1}\\sum_{c=1}^{384} D^\\mathrm{STAE}_c$}\n        \\State{Resize $M_\\mathrm{ST}$ and $M_\\mathrm{AE}$ to $256 \\times 256$ pixels using bilinear interpolation}\n        \\State{$X_\\mathrm{ST} \\leftarrow {X_\\mathrm{ST}}^\\frown \\mathrm{vec}(M_\\mathrm{ST})$} \\Comment{Append to the sequence of local anomaly scores}\n        \\State{$X_\\mathrm{AE} \\leftarrow {X_\\mathrm{AE}}^\\frown \\mathrm{vec}(M_\\mathrm{AE})$} \\Comment{Append to the sequence of global anomaly scores}\n    \\EndFor\n    \\State{Compute the $0.9$-quantile $q_a^\\mathrm{ST}$ and the $0.995$-quantile $q_b^\\mathrm{ST}$ of the elements of $X_\\mathrm{ST}$.}\n    \\State{Compute the $0.9$-quantile $q_a^\\mathrm{AE}$ and the $0.995$-quantile $q_b^\\mathrm{AE}$ of the elements of $X_\\mathrm{AE}$.}\n    \\State \\Return{$T$, $S$, $A$, $\\mu$, $\\sigma$, $q_a^\\mathrm{ST}$, $q_b^\\mathrm{ST}$, $q_a^\\mathrm{AE}$, and $q_b^\\mathrm{AE}$ }\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\begin{algorithm}\n  \\caption{\\ourmethod\\  Inference Procedure} \n  \\label{alg:inference} \n  \\begin{algorithmic} [1]\n    \\Require{$T$, $S$, $A$, $\\mu$, $\\sigma$, $q_a^\\mathrm{ST}$, $q_b^\\mathrm{ST}$, $q_a^\\mathrm{AE}$, and $q_b^\\mathrm{AE}$, as returned by \\Cref{alg:training}}\n    \\Require{Test image $I_\\mathrm{test} \\in \\mathbb{R}^{3 \\times 256 \\times 256}$}\n    \\State{$Y'\\leftarrow T(I_\\mathrm{test}),\\;\\;Y^\\mathrm{S} \\leftarrow S(I_\\mathrm{test}),\\;\\;Y^\\mathrm{A} \\leftarrow A(I_\\mathrm{test})$}\n    \\State{Compute the normalized teacher output $\\hat{Y}$ given by $\\hat{Y}_c = (Y'_c - \\mu_c) \\sigma_c^{-1}$ for each $c \\in \\{1, \\dots, 384\\}$}\n    \\State{Split the student output into $Y^\\mathrm{ST} \\in \\mathbb{R}^{384 \\times 64 \\times 64}$ and $Y^\\mathrm{STAE} \\in \\mathbb{R}^{384 \\times 64 \\times 64}$ as above}\n    \\State{Compute the squared difference $D^\\mathrm{ST}_{c, w, h} = (\\hat{Y}_{c, w, h} - Y^\\mathrm{ST}_{c, w, h})^2$ for each tuple $(c, w, h)$}\n    \\State{Compute the squared difference $D^\\mathrm{STAE}_{c, w, h} = (Y^\\mathrm{A}_{c, w, h} - Y^\\mathrm{STAE}_{c, w, h})^2$ for each tuple $(c, w, h)$ }\n    \\State{Compute the anomaly maps $M_\\mathrm{ST} = 384^{-1}\\sum_{c=1}^{384} D^\\mathrm{ST}_c$ and $M_\\mathrm{AE} = 384^{-1}\\sum_{c=1}^{384} D^\\mathrm{STAE}_c$}\n    \\State{Resize $M_\\mathrm{ST}$ and $M_\\mathrm{AE}$ to $256 \\times 256$ pixels using bilinear interpolation}\n    \\State{Compute the normalized $\\hat{M}_\\mathrm{ST} = 0.1 (M_\\mathrm{ST} - q_a^\\mathrm{ST}) (q_b^\\mathrm{ST} - q_a^\\mathrm{ST})^{-1}$}\n    \\State{Compute the normalized $\\hat{M}_\\mathrm{AE} = 0.1 (M_\\mathrm{AE} - q_a^\\mathrm{AE}) (q_b^\\mathrm{AE} - q_a^\\mathrm{AE})^{-1}$}\n    \\State{Compute the combined anomaly map $M = 0.5 \\hat{M}_\\mathrm{ST} + 0.5 \\hat{M}_\\mathrm{AE}$}\n    \\State{Compute the image-level score as $m_\\mathrm{image} = \\max_{i, j} M_{i, j}$}\n    \\State \\Return{$M$ and $m_\\mathrm{image}$}\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n\n\\paragraph{Comments on \\Cref{alg:training} and \\Cref{alg:inference}:}\n\\begin{itemize}\n    \\item We use the default initialization method of PyTorch \\cite{paszke2019_PyTorch} (version 1.12.0) for the convolutional layers.\n    \\item We apply the teacher and the student to both the original and the augmented training image. \nThat is necessary because the student--teacher model is trained without augmentation, while the autoencoder is trained with augmentation.\nDuring inference, we do not need these second forward passes because images are not augmented at test time.\n    \\item The sizes of the images of MVTec AD \\cite{bergmann2021_mvtec_ad_ijcv, bergmann2019_mvtec_ad_cvpr}, VisA \\cite{zou2022spot}, and MVTec LOCO \\cite{bergmann2021_mvtec_loco_ijcv} differ.\nWe resize each input image to $256\\times256$ and resize the anomaly map $M$ back to the original image size using bilinear interpolation.\n    \\item We use a batch size of one.\n    \\item We use the image normalization of the pretrained models of torchvision \\cite{paszke2019_PyTorch}.\nThat means we subtract $0.485$, $0.456$, and $0.406$ from the R, G, and B channel, respectively, for each input image and divide the channels by $0.229$, $0.224$, and $0.225$, respectively.\nWe perform this normalization directly before applying a network to an image, i.e., after augmentation.\nAt test time, this can also be done by adjusting the weights and bias of the first convolutional layer of a network accordingly.\n    \\item The parameters of the autoencoder $A$ are not only affected by the gradient of $L_\\mathrm{AE}$, but also by the gradient of $L_\\mathrm{STAE}$.\n    \\item We obtain an image $P \\in \\mathbb{R}^{3 \\times 256 \\times 256}$ from ImageNet by choosing a random image, resizing it to $512\\times512$, converting it to gray scale with a probability of $0.3$, and cropping the center $256\\times256$ pixels.\n\\end{itemize}\n\n"
                },
                "subsection 7.2": {
                    "name": "Distillation",
                    "content": "\n\\label{subsec:distillation}\n\nIn the following, we describe how to distill the WideResNet-101 \\cite{zagoruyko2016wideresnet_wrn} features used by PatchCore \\cite{roth2022towards} into the teacher network $T$.\nThe distillation training algorithm is presented in \\Cref{alg:distillation}.\nThe process in analogous for other pretrained feature extractors.\n\nThere are only few requirements regarding the output shape of the feature extractor.\nThe feature extractors used by PatchCore output features of shape $384\\times64\\times64$ for an input image size of $512\\times512$ pixels.\nTherefore, the teacher and the autoencoder also output $384$ channels (as described in \\Cref{tab:arch_eads,tab:arch_eadm,tab:arch_ae}).\nIf a pretrained feature extractor outputs a different number of channels, this default of $384$ output channels of the teacher and the autoencoder can be adjusted flexibly.\nDuring distillation, we resize input images to $512\\times512$ for the pretrained feature extractor and to $256\\times256$ for the teacher network that is being trained.\nThis results in an output shape of $384\\times64\\times64$ for the teacher network as well.\nIf a feature extractor outputs feature maps of a size other than $64\\times64$, we can adjust its input image size to achieve an output feature map size of $64\\times64$.\nAlternatively, we can adjust the input image size of the teacher network because it is fully convolutional and operates separately on patches of size $33\\times33$.\nA feature map size of $53\\times71$, for example, can be achieved by applying the teacher network to images of size $212\\times284$.\n\nWe use a batch size of 16 for the distillation training and use ImageNet \\cite{russakovsky2015_alexnet} as the pretraining dataset.\nWe use the official implementation of PatchCore \\footnote{\\url{https://github.com/amazon-science/patchcore-inspection/tree/6a9a281fc34cb1b13c54b318f71e6f1f371536bb}} and its default values if not stated otherwise.\nWe use the feature postprocessing of PatchCore as well, which includes pooling features from two layers and projecting each feature vector to a reduced dimensionality of $384$ dimensions, as described in \\cite{roth2022towards}.\nThe features used for our distillation training are the final features used by PatchCore, i.e., those given to the coreset subsampling algorithm when training PatchCore.\nWe denote the WideResNet-101-based feature extractor, including the feature postprocessing, as $\\Psi : \\mathbb{R}^{3\\times512\\times512} \\rightarrow \\mathbb{R}^{384\\times64\\times64}$.\n\n\\begin{algorithm}\n  \\caption{Distillation Training Algorithm} \n  \\label{alg:distillation} \n  \\begin{algorithmic} [1]\n    \\Require{A pretrained feature extractor $\\Psi : \\mathbb{R}^{3\\times W \\times H} \\rightarrow \\mathbb{R}^{384\\times64\\times64}$.}\n    \\Require{A sequence of distillation training images $\\mathcal{I}_\\mathrm{dist}$}\n    \\State{Randomly initialize a teacher network $T : \\mathbb{R}^{3 \\times 256 \\times 256} \\rightarrow \\mathbb{R}^{384 \\times 64 \\times 64}$ with an architecture as given in \\Cref{tab:arch_eads} or \\ref{tab:arch_eadm}}\n    \\For{$c \\in {1, \\dots, 384}$} \\Comment{Compute feature extractor channel normalization parameters $\\mu^\\Psi \\in \\mathbb{R}^{384}$ and $\\sigma^\\Psi \\in \\mathbb{R}^{384}$}\n    \\State{Initialize an empty sequence $X \\leftarrow (~)$}\n    \\For{iteration $= 1,2,\\dots, \\num{10000}$}\n        \\State{Choose a random training image $I_\\mathrm{dist}$ from $\\mathcal{I}_\\mathrm{dist}$}\n        \\State{Convert $I_\\mathrm{dist}$ to gray scale with a probability of $0.1$}\n        \\State{Compute $I^\\Psi_\\mathrm{dist}$ by resizing $I_\\mathrm{dist}$ to $3\\times W \\times H$ using bilinear interpolation}\n        \\State{$Y^\\Psi\\leftarrow \\Psi(I^\\Psi_\\mathrm{dist})$}\n        \\State{$X \\leftarrow X^\\frown \\mathrm{vec}(Y^\\Psi_c)$} \\Comment{Append the channel output to $X$}\n    \\EndFor\n    \\State{Set $\\mu^\\Psi_c$ to the mean and $\\sigma^\\Psi_c$ to the standard deviation of the elements of $X$}\n    \\EndFor\n    \\State{Initialize the Adam \\cite{kingma2014_adam} optimizer with a learning rate of $10^{-4}$ and a weight decay of $10^{-5}$ for the parameters of $T$}\n    \\For{iteration $= 1,\\dots, \\num{60000}$}\n        \\State{$L_\\mathrm{batch} \\leftarrow 0$}\n        \\For{batch index $= 1, \\dots, 16$}\n        \\State{Choose a random training image $I_\\mathrm{dist}$ from $\\mathcal{I}_\\mathrm{dist}$}\n        \\State{Convert $I_\\mathrm{dist}$ to gray scale with a probability of $0.1$}\n        \\State{Compute $I^\\Psi_\\mathrm{dist}$ by resizing $I_\\mathrm{dist}$ to $3\\times W \\times H$ using bilinear interpolation}\n        \\State{Compute $I'_\\mathrm{dist}$ by resizing $I_\\mathrm{dist}$ to $3\\times 256 \\times 256$ using bilinear interpolation}\n        \\State{$Y^\\Psi\\leftarrow \\Psi(I^\\Psi_\\mathrm{dist})$}\n        \\State{Compute the normalized features $\\hat{Y}^\\Psi$ given by $\\hat{Y}^\\Psi_c = (Y^\\Psi_c - \\mu^\\Psi_c) (\\sigma^\\Psi_c)^{-1}$ for each $c \\in \\{1, \\dots, 384\\}$}\n        \\State{$Y' \\leftarrow T(I'_\\mathrm{dist})$}\n        \\State{Compute the squared difference between $\\hat{Y}^\\Psi$ and $Y'$ for each tuple $(c, w, h)$ as $D^\\mathrm{dist}_{c, w, h} = (\\hat{Y}^\\Psi_{c, w, h} - Y'_{c, w, h})^2$}\n        \\State{Compute the loss $L_\\mathrm{dist}$ as the mean of all elements $D^\\mathrm{dist}_{c, w, h}$ of $D^\\mathrm{dist}$}\n        \\State{$L_\\mathrm{batch} \\leftarrow L_\\mathrm{batch} + L_\\mathrm{dist}$}\n        \\EndFor\n        \\State{$L_\\mathrm{batch} \\leftarrow 16^{-1} L_\\mathrm{batch}$}\n        \\State{Update the parameters of $T$, denoted by $\\theta$, using the gradient $\\nabla_\\theta L_\\mathrm{batch}$ }\n    \\EndFor\n    \\State \\Return{$T$}\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\paragraph{Comments on \\Cref{alg:distillation}:}\nWe use the image normalization of the pretrained models of torchvision \\cite{paszke2019_PyTorch}.\nThat means we subtract $0.485$, $0.456$, and $0.406$ from the R, G, and B channel, respectively, for each input image and divide the channels by $0.229$, $0.224$, and $0.225$, respectively.\nWe perform this normalization directly before applying a network to an image, i.e., after augmentation.\n\n"
                }
            },
            "section 8": {
                "name": "Implementation Details for Other Evaluated Methods",
                "content": "\n\\label{sec:details_others}\n\nIn the following, we provide the implementation and configuration details for Asymmetric Student--Teacher (AST) \\cite{rudolph2023asymmetric}, DSR \\cite{zavrtanik2022dsr}, FastFlow \\cite{yu2021fastflow}, GCAD \\cite{bergmann2021_mvtec_loco_ijcv}, PatchCore \\cite{roth2022towards}, SimpleNet \\cite{Liu_2023_CVPR}, and Student--Teacher \\cite{bergmann2020_uninformed_cvpr}.\n\n",
                "subsection 8.1": {
                    "name": "AST",
                    "content": "\n\\label{sec:details_ast}\n\nWe use the official implementation of Rudolph \\etal \\cite{rudolph2023asymmetric} \\footnote{\\url{https://github.com/marco-rudolph/AST/tree/1a157973a0e2cb23b6fbb853db8ae43537ab2568}}.\nWe use the default configuration without modifications, but are not able to fully reproduce the results reported in the AST paper.\nThe AST paper reports a mean image-level detection AU-ROC of \\SI{99.2}{\\percent} on MVTec AD, averaged across five runs.\nWe obtain an AU-ROC of \\SI{98.9}{\\percent} across five runs.\n\n"
                },
                "subsection 8.2": {
                    "name": "DSR",
                    "content": "\n\\label{sec:details_dsr}\n\nWe use the official implementation of Zavrtanik \\etal \\cite{zavrtanik2022dsr} \\footnote{\\url{https://github.com/VitjanZ/DSR_anomaly_detection/tree/672bfb81434fd2a6c5ef00db858cef8834c54f28}}.\nWe use the default configuration without modifications for reproducing the results on MVTec AD.\nWe obtain a mean image-level detection AU-ROC of \\SI{98.1}{\\percent} on MVTec AD, which is close to the \\SI{98.2}{\\percent} reported by the authors.\nOn the scenarios from VisA, which contain more training images than those of MVTec AD, we change the number of epochs to 50 to keep the total number of training iterations in a similar range.\n\n"
                },
                "subsection 8.3": {
                    "name": "FastFlow",
                    "content": "\n\\label{sec:details_fastflow}\n\nWe use the implementation of Akcay \\etal \\cite{akcay2022anomalib} \\footnote{\\url{https://github.com/openvinotoolkit/anomalib/tree/e66a17c86489486f6bbd5099366383e8660923fd}}.\nWe use the FastFlow version based on the WideResNet-50-2 feature extractor, as it is similar to the WideResNet used by PatchCore, SimpleNet and our method.\nWe use the default configuration, but disable early stopping, i.e., the scenario-specific tuning of the training duration on test images.\nInstead, we choose a constant training duration (200 steps) that works well on average for all evaluated datasets.\n\n"
                },
                "subsection 8.4": {
                    "name": "GCAD",
                    "content": "\n\\label{sec:details_gcad}\n\nWe implement GCAD as described by Bergmann \\etal \\cite{bergmann2021_mvtec_loco_ijcv}.\nWe are able to reproduce the results reported by the authors, but adapt GCAD to a configuration that performs better in our experiments.\nGCAD consists of an ensemble of two anomaly detection models that use different feature extractors.\nThe first member uses a feature extractor that operates on patches of size $17\\times17$ while the feature extractor used by the second member operates on patches of size $33\\times33$.\nWe find that the second member performs better on average than the combined ensemble and therefore report the results for this member in the main paper.\nOn the logical anomalies of MVTec LOCO, the single model scores an image-level detection AU-ROC of \\SI{83.9}{\\percent}, while the AU-ROC of the ensemble model used by the authors is \\SI{86.0}{\\percent}.\nThe overall anomaly detection performance on MVTec LOCO, however, stays the same.\n\n"
                },
                "subsection 8.5": {
                    "name": "PatchCore",
                    "content": "\n\\label{sec:details_patchcore}\n\nWe use the official implementation of Roth \\etal \\cite{roth2022towards} \\footnote{\\url{https://github.com/amazon-science/patchcore-inspection/tree/6a9a281fc34cb1b13c54b318f71e6f1f371536bb}} and are able to reproduce the results reported for MVTec AD.\nAs described in the main paper, we disable the cropping of the center \\SI{76.6}{\\percent} of input images for a fair comparison.\nFor the single model variant of PatchCore, we use the configuration of PatchCore for which the authors report the lowest latency.\nSpecifically, this means setting the coreset subsampling ratio to \\SI{1}{\\percent}, the image size to $224\\times224$ pixels, and the feature extraction backbone to a WideResNet-101.\nFor the ensemble variant, we use the configuration for which the authors report the best detection AU-ROC on MVTec AD.\nWe use a WideResNet-101, a ResNeXT-101 \\cite{xie2017resnext}, and a DenseNet-201 \\cite{huang2017densely} as backbones, set the coreset subsampling ratio to \\SI{1}{\\percent}, and use images of size $320\\times320$ pixels.\n\n"
                },
                "subsection 8.6": {
                    "name": "SimpleNet",
                    "content": "\n\\label{sec:details_simplenet}\nWe use the official implementation of Liu \\etal \\cite{Liu_2023_CVPR} \\footnote{\\url{https://github.com/DonaldRR/SimpleNet/tree/35bf32292995842a4277a7c93431430129efccb5}} and are able to reproduce the reported results.\nAs explained in the main paper, we disable the scenario-specific tuning of the training duration on test images for a fair comparison.\n\n"
                },
                "subsection 8.7": {
                    "name": "Student--Teacher",
                    "content": "\n\\label{sec:details_st}\n\nWe implement the original multi-scale Student--Teacher (\\studteach) method as described by Bergmann \\etal \\cite{bergmann2020_uninformed_cvpr}.\nWe use the default hyperparameter settings without modification.\nOur implementation achieves better anomaly localization results on MVTec AD than those reported by the authors but matches those reported in \\cite{bergmann2021_mvtec_ad_ijcv}.\n\n"
                }
            },
            "section 9": {
                "name": "Robustness to the Distillation Backbone Architecture",
                "content": "\n\\label{sec:backbones}\nIn the main paper, we use the features from a WideResNet-101 for training a teacher network in \\Cref{alg:distillation}.\nThe default configuration of PatchCore uses the same features.\nIn \\Cref{tab:distillation_backbones}, we evaluate the anomaly detection performance for other backbones.\nSpecifically, we evaluate the two additional backbones that \\patchcoreens\\ uses, i.e., a ResNeXt-101 and a DenseNet-201.\nOn the three evaluated dataset collections, the anomaly detection performance of \\ourmethod\\ is similarly robust to the choice of the backbone in comparison to the robustness of PatchCore.\nOn MVTec AD, both methods perform very similarly across backbones, while their performance on MVTec LOCO varies more.\nOn VisA, the gap between the structural anomaly detection performance of PatchCore and that of \\ourmethod\\ becomes evident.\n\n\n\n\n\\clearpage\n\n"
            },
            "section 10": {
                "name": "Additional Anomaly Detection Metrics",
                "content": "\n\\label{sec:ad_metrics}\n\nIn this section, we report the results for additional anomaly detection metrics.\n\\Cref{sec:ad_metrics_classification} evaluates image-level anomaly detection metrics.\n\\Cref{sec:ad_metrics_localization} evaluates pixel-level anomaly localization metrics.\n\nFor per-scenario evaluation results, see the \\href{https://www.mydrive.ch/shares/79401/c41dcdb937972fb43d5cdd7bfa7072f8/download/449203483-1690527455/per_scenario_results.json}{\\texttt{per\\_scenario\\_results.json}} file \\footnote{\\url{https://www.mydrive.ch/shares/79401/c41dcdb937972fb43d5cdd7bfa7072f8/download/449203483-1690527455/per_scenario_results.json}}.\n\nFollowing the official MVTec LOCO evaluation script \\footnote{\\url{https://www.mvtec.com/company/research/datasets/mvtec-loco}}, we evaluate each performance metric separately on the structural and on the logical anomalies of MVTec LOCO.\nThen, we compute the mean of the two scores to compute the overall performance of a method on a scenario of MVTec LOCO.\n\n\n",
                "subsection 10.1": {
                    "name": "Anomaly Detection",
                    "content": "\n\\label{sec:ad_metrics_classification}\n\nIn the main paper, we evaluate the image-level anomaly detection performance with the area under the ROC curve (AU-ROC).\nHere, we report the results for the area under the precision recall curve (AU-PRC) as well.\nFor information on the differences between the AU-ROC and the AU-PRC, we refer to Davis and Goadrich \\cite{davis2006relationship}.\n\n\\Cref{tab:classification_roc} shows the anomaly detection performance of each method measured with the AU-ROC.\nThis table contains the results reported in the main paper.\n\\Cref{tab:classification_prc} shows the results for the image-level AU-PRC.\n\n\n\n\n\n"
                },
                "subsection 10.2": {
                    "name": "Anomaly Localization",
                    "content": "\n\\label{sec:ad_metrics_localization}\n\nTo evaluate the anomaly localization performance, we use the area under the PRO curve (AU-PRO) up to a false positive rate (FPR) of \\SI{30}{\\percent} in the main paper, as recommended by \\cite{bergmann2021_mvtec_ad_ijcv}.\nThe AU-PRO metric \\cite{bergmann2021_mvtec_ad_ijcv} is similar to the pixel-wise AU-ROC.\nThe difference is that the pixel-wise AU-ROC gives each ground truth defect \\textit{pixel} the same weight in its computation.\nThe AU-PRO gives each ground truth defect \\textit{region} the same weight.\nThe FPR limit of \\SI{30}{\\percent} is due to the fact that a method that segments, on average, more than \\SI{30}{\\percent} of defect-free pixels as anomalous is of limited use.\n\n\\Cref{tab:localization_pro_03} contains the results reported in the main paper.\nHere, we report the AU-PRO for an FPR limit of \\SI{5}{\\percent} as well in \\Cref{tab:localization_pro_005}.\nFor comparison, we also report the pixel-wise AU-ROC for an FPR limit of \\SI{5}{\\percent} in \\Cref{tab:localization_roc_005}.\nFurthermore, we evaluate the pixel-wise AU-PRC as an additional segmentation, and thus, localization performance metric in \\Cref{tab:localization_prc}.\nThe \\href{https://www.mydrive.ch/shares/79401/c41dcdb937972fb43d5cdd7bfa7072f8/download/449203483-1690527455/per_scenario_results.json}{\\texttt{per\\_scenario\\_results.json}} file \\footnote{\\url{https://www.mydrive.ch/shares/79401/c41dcdb937972fb43d5cdd7bfa7072f8/download/449203483-1690527455/per_scenario_results.json}} also contains the AU-PRO and pixel-wise AU-ROC results for an FPR limit of \\SI{100}{\\percent}.\n\n\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 11": {
                "name": "Timing Methodology and Additional Computational Efficiency Metrics",
                "content": "\n\\label{sec:efficiency_metrics}\n\n\n\nIn the following, we describe how we measure the latency and the throughput of each anomaly detection method.\nLatency refers to the inference runtime, i.e., how long it takes a method to generate the anomaly detection result for a single image.\nThroughput refers to how many images can be processed per second when allowing a batched processing of images.\nIn settings in which latency constraints are fulfilled or not present, a high throughput is relevant for using computational resources efficiently and thus for reducing the economic cost of an application.\n\nAll evaluated methods are implemented in PyTorch.\nAll of them, including the nearest neighbor search of PatchCore, run faster on each of the GPUs in our experimental setup than on a CPU.\nWe therefore execute each method on a GPU.\nFor a test image, our timing begins with the transfer of the image from the CPU to the GPU.\nWe include the transfer to regard the benefit of a method that would run exclusively on a CPU.\nOur timing stops when the anomaly detection result, which for all evaluated methods is an anomaly map, is available on the CPU.\nFor each method, we remove unnecessary parts for the timing, such as the computation of losses during inference, and use float16 precision for all networks.\nSwitching from float32 to float16 for the inference of \\ourmethod\\ does not change the anomaly detection results for the 32 anomaly detection scenarios evaluated in this paper.\nIn latency-critical applications, padding in the PDN architecture of \\ourmethod\\ can be disabled.\nThis speeds up the forward pass of the PDN architecture by \\SI{80}{\\micro\\second} without impairing the detection of anomalies.\nWe time \\ourmethod\\ without padding and therefore report the anomaly detection results for this setting in the experimental results of this paper.\nWe perform 1000 forward passes as warm up and report the mean runtime of the following 1000 forward passes.\nFor the latency, we report the average runtime of 1000 forward passes with a batch size of 1.\nWe compute the throughput by dividing \\num{16000} by the sum of the runtimes of 1000 forward passes with a batch size of 16.\nIn addition to the latency and the throughput, we report the number of parameters, the number of floating point operations (FLOPs), and the GPU memory consumption for each method in \\Cref{tab:efficiency}.\nAnalogously to the latency, we measure these metrics for the processing of one image during inference and report the mean of 1000 forward passes.\nThe number of parameters and the FLOPs remain constant across forward passes, while the GPU memory consumption varies slightly (less than one MB difference between forward passes).\n\n\\paragraph{Technical Details}\nFor methods that use features from hidden layers of a pretrained network, we exclude the layers that are not required for computing these features, i.e., classification heads etc.\nWe measure the number of FLOPs using the official profiling framework of PyTorch \\cite{paszke2019_PyTorch} (version 1.12.0).\nSpecifically, we wrap the inference function of a method into a call of \\texttt{with torch.profiler.profile(with\\_flops=True) as prof:}.\nFor measuring the GPU memory consumption, we also use the official profiling framework of PyTorch.\nWe obtain the peak of the reserved GPU memory during inference with \\texttt{torch.cuda.memory\\_stats()['reserved\\_bytes.all.peak']}.\n\n\\paragraph{Interpretability of Efficiency Metrics}\nIn the main paper, we focus on the latency and the throughput of the evaluated anomaly detection methods.\nThe number of parameters and the number of FLOPs are often used as proxy metrics for the runtime, but can be misleading.\nFor example, the number of parameters of FastFlow in \\Cref{tab:efficiency} is roughly 2.5 times larger than that of S--T.\nYet, the latency of FastFlow is substantially lower and its throughput is 6.5 times higher.\n\nWith 4.5 trillion FLOPs, S--T exceeds the FLOPs of other methods by a large margin.\nThe high number of FLOPs, however, comes from the fact that S--T uses convolutions that operate on large feature maps.\nThis means that these convolutions can be parallelized well on a GPU, while implementing them naively on a CPU would indeed cause a prohibitively long runtime.\nFLOPs measurements do not account for this, because they do not consider how well operations can be parallelized.\nThe number of FLOPs can therefore be an unreliable metric for efficiency.\nFor example, the number of FLOPs of S--T is more than \\SI{2000}{\\percent} higher than that of AST, but the latency is only \\SI{42}{\\percent} higher.\n\nThe GPU memory footprint of a method can theoretically be reduced drastically by freeing obsolete GPU memory segments after each layer's execution during a forward pass.\nIn the extreme case, one could even directly free the memory of individual input activation values directly after the output activation of a neuron in a convolutional layer has been computed.\nThis, however, would worsen the runtime of a forward pass, which generally improves when reserved GPU memory segments can be reused.\nTherefore, the GPU memory footprint of a method needs to be reported and analyzed jointly with the latency and throughput.\nWe focus on the GPU memory required for achieving the reported latency and throughput and therefore measure the peak of the reserved GPU memory during a forward pass.\n\n\\paragraph{PatchCore}\nFor PatchCore, we distinguish between the backbones used to compute features and the kNN algorithm itself.\nFor example, the part of the WideResNet-101 backbone until the layer used for computing features has 83 million parameters.\nDuring training, PatchCore computes the feature vectors of all training images.\nThe coreset subsampling phase of PatchCore reduces the number of feature vectors to \\SI{1}{\\percent} of the computed feature vectors.\nThese are then indexed and stored in GPU memory to enable a fast search for nearest neighbors during inference.\nThis, however, means that the number of parameters, the FLOPs, and the GPU memory footprint of PatchCore depend on the training images.\nWe therefore benchmark PatchCore on the ``cashew'' scenario of VisA, which contains 450 training images and is thus closest to the average 439 training images of the 32 scenarios of MVTec AD, VisA, and MVTec LOCO.\nWe do not report the FLOPs and the GPU memory consumption of the kNN search, as we were not able to measure it with the kNN library used by the official PatchCore implementation.\nThe number of parameters of the kNN search is given by the number of values stored in the GPU memory during inference.\nIn the case of \\patchcoreens, for example, the search database contains 8 million values.\n\n\\clearpage\n\n\\paragraph{Anomaly Detection and Throughput}\n\\Cref{fig:throughput} shows the anomaly detection performance together with the throughput of each evaluated method, analogous to \\Cref{fig:teaser} in the main paper. \nApart from \\ourmethod, the ranking of methods changes drastically between the image-level and the pixel-level detection of anomalies.\n\n\n\n\n\\paragraph{Latency per GPU}\nIn \\Cref{tab:latency_per_gpu}, we provide the values for \\Cref{fig:latency_per_gpu} in the main paper.\n\n \n\n\n\\clearpage\n\n"
            },
            "section 12": {
                "name": "Qualitative Results",
                "content": "\n\\label{sec:qualitative_results}\n\nIn \\Cref{fig:qualitative_1,fig:qualitative_2,fig:qualitative_3}, we display anomaly maps for each of the 32 scenarios of MVTec AD, VisA, and MVTec LOCO.\nFor MVTec LOCO, we show both logical and structural anomalies.\nWe visualize the anomaly maps using a different scale for each method, since the anomaly score scales differ between methods.\nAcross scenarios, however, we use the same color scale per anomaly detection method.\nA consistent anomaly score scale across applications is an important requirement for a method.\nOtherwise, the scale of scores on anomalies is hard to forecast if no or only few defect images are present during the development of the anomaly detection system.\nKnowing the scale is important for choosing a robust threshold value that ultimately determines whether an image or a pixel is anomalous or not.\nFurthermore, a consistent scale facilitates the interpretation of anomaly maps.\nFor the evaluated methods, we choose the start and end values of the color scales so that true positive and true negative detections become clearly visible.\nFor example, the color scale of AST ranges from 2 to 10.\nScores outside of this range are visualized with the minimum and maximum color value, respectively.\nFor PatchCore, choosing the range of the color scale is difficult.\nOn the one hand, scores of true positive detections are low, such as the contamination of the banana juice bottle in \\Cref{fig:qualitative_1}.\nOn the other hand, scores of false positive detections are similarly high, such as the predictions on the breakfast box in \\Cref{fig:qualitative_1}.\n\nOverall, the evaluated anomaly detection methods succeed on the anomalies of MVTec AD, but leave room for improvement on MVTec LOCO and VisA.\n\\begin{itemize}\n\\item EfficientAD responds to both logical and structural anomalies in the images.\nThe strength of its response sometimes leaves room for improvement, for example, on the logical anomalies of the breakfast box and the box of pushpins in \\Cref{fig:qualitative_1}.\n    \\item  AST detects some logical anomalies, but lacks an approach that detects logical anomalies by design.\nFor example, it detects that the additional blue cable connecting two splicing connectors in \\Cref{fig:qualitative_1} causes unseen features.\nYet, the missing pushpin in the box of pushpins in \\Cref{fig:qualitative_1} is also an unseen feature and does not cause a response in the anomaly map of AST.\nThis highlights the importance of a reliable approach to logical anomalies.\nFurthermore, it shows the dependence of anomaly detection methods on the choice of the feature extractor.\nAs shown in \\Cref{tab:distillation_backbones}, \\ourmethod\\ is robust to this choice.\n\\item DSR produces very precise segmentations, but also suffers from false positives, for example on the grid and the wood image of MVTec AD in \\Cref{fig:qualitative_2}.\nAt times, it furthermore shows no response at all to defects.\n\\item FastFlow's anomaly maps contain a large amount of noise, i.e. false positive detections. This hinders the interpretability of its detection results.\n\\item GCAD succeeds at detecting logical anomalies, but has difficulty with some structural anomalies that other methods detect reliably, such as the scratches on the metal nut in \\Cref{fig:qualitative_2} or the green capsules in \\Cref{fig:qualitative_3}.\n\\item \\patchcoreens\\ struggles with very small defects such as those of the printed circuit boards in \\Cref{fig:qualitative_3}.\nSmall defects are challenging, but highly relevant for practical applications.\nA small contamination can cause a high economic damage if it goes unnoticed, for example, in a pharmaceutical application.\n\\item SimpleNet performs similar to other methods on MVTec AD, but struggles with the more challenging anomalies of MVTec LOCO and VisA, for example the defective capsules and PCBs in \\Cref{fig:qualitative_3}.\n\\item S--T is a patch-based anomaly detection approach and therefore can only detect anomalies if they involve patches that are anomalous per se, i.e., without putting them in the global context of the respective image.\nWhile AST's feature vectors have a receptive field that spans across the entire image, S--T's receptive field is limited to 65$\\times$65 pixels.\nTherefore, it does not detect anomalies such as the missing transistor in \\Cref{fig:qualitative_2}.\n\\end{itemize}\n\nThe qualitative results show tendencies of each method regarding the behavior on anomalous images.\nWhile these results are informative, they should not be used exclusively for evaluating the anomaly detection performance of a method or for comparing methods.\n\\textbf{For that, metrics such as the AU-ROC and the AU-PRO are well-suited, since they are evaluated objectively on thousands of test images across dataset collections.}\n\n\n\n\n\n\n\n\n"
            }
        },
        "tables": {
            "tab:main": "\\begin{table}\n\\small\n\\begin{center}\n\\begin{tabular}{ccccc}\nMethod & \\specialcell[c]{Detect. \\\\ AU-ROC} & \\specialcell[c]{Segment. \\\\ AU-PRO} & \\specialcell[c]{Latency \\\\ {[ms]}} & \\specialcell[c]{Throughput \\\\ {[img / s]}} \\\\\n\\hline\nGCAD & 85.4 & 88.0 & 11 & 121 \\Tstrut\\\\\nSimpleNet & 87.9 & 74.4 & 12 & 194 \\\\\n\\studteach & 88.4 & 89.7 & 75 & 16\\\\\nFastFlow & 90.0 & 86.5 & 17 & 120\\\\\nDSR & 90.8 & 78.6 & 17 & 104 \\\\\nPatchCore & 91.1 & 80.9 & 32 & 76 \\\\\n\\patchcoreens & 92.1 & 80.7 & 148 & 13 \\\\\nAST & 92.4 & 77.2 & 53 & 41  \\Bstrut\\\\\n\\hline\n\\ourmethod-S & \\meanwithstd{95.4}{0.06} & \\meanwithstd{92.5}{0.05} & \\meanwithstd{\\textbf{2.2}}{0.01} & \\meanwithstd{\\textbf{614}}{2} \\rule{0pt}{3.6ex}\\\\\n\\ourmethod-M & \\meanwithstd{\\textbf{96.0}}{0.09} & \\meanwithstd{\\textbf{93.3}}{0.04} & \\meanwithstd{4.5}{0.01} & \\meanwithstd{269}{1} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Anomaly detection and anomaly localization performance in comparison to the latency and throughput. Each AU-ROC and AU-PRO percentage is an average of the mean AU-ROCs and mean AU-PROs, respectively, on MVTec AD, VisA, and MVTec LOCO.\nFor \\ourmethod, we report the mean and standard deviation of five runs.\n}\n\\label{tab:main}\n\\end{table}",
            "tab:per_dataset": "\\begin{table}\n\\small\n\\begin{center}\n\\begin{tabular}{ccccc||cc}\nMethod & MAD & LOCO & VisA & Mean & \\specialcell[c]{LOCO \\\\ Logic.} & \\specialcell[c]{LOCO \\\\ Struct.} \\\\\n\\hline\nGCAD & 89.1 & 83.3 & 83.7 & 85.4 & 83.9 & 82.7 \\Tstrut\\\\\nSimpleNet & 98.2 & 77.6 & 87.9 & 87.9 & 71.5 & 83.7 \\\\\n\\studteach & 93.2 & 77.4 & 94.6 & 88.4 & 66.5 & 88.3 \\\\\nFastFlow & 96.9 & 79.2 & 93.9 & 90.0 & 75.5 & 82.9 \\\\\nDSR & 98.1 & 82.6 & 91.8 & 90.8 & 75.0 & 90.2 \\\\\nPatchCore & 98.7 & 80.3 & 94.3 & 91.1 & 75.8 & 84.8 \\\\\n\\patchcoreens & \\textbf{99.3} & 79.4 & 97.7 & 92.1 & 71.0 & 87.7 \\\\\nAST & 98.9 & 83.4 & 94.9 & 92.4 & 79.7 & 87.1 \\Bstrut\\\\\n\\hline\n\\ourmethod-S & 98.8 & 90.0 & 97.5 & 95.4 & 85.8 & 94.1 \\Tstrut\\\\\n\\ourmethod-M & 99.1 & \\textbf{90.7} & \\textbf{98.1} & \\textbf{96.0} & \\textbf{86.8} & \\textbf{94.7} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly detection AU-ROC percentages per dataset collection (left) and on the logical and structural anomalies of MVTec LOCO (right). For \\ourmethod, we report the mean of five runs.\nPerforming method development solely on MVTec AD (MAD) becomes prone to overfitting design choices to the few remaining misclassified test images.}\n\\label{tab:per_dataset}\n\\end{table}",
            "tab:hyperparams": "\\begin{table}\n\\small\n\\begin{center}\n\\begin{tabular}{ccccccc}\n$a$ (for $q_a$) & 0.5 & 0.8 & \\textbf{0.9} & 0.95 & 0.98 & 0.99 \\Bstrut\\\\\n\\hline\nAU-ROC & 95.9 & 95.9 & 96.0 & 95.9 & 95.9 & 95.8 \\Tstrut\\\\\n\\rule{0pt}{5mm}\n$b$ (for $q_b$) & 0.95 & 0.98 & 0.99 & \\textbf{0.995} & 0.998 & 0.999 \\Bstrut\\\\\n\\hline\nAU-ROC & 95.8 & 95.9 & 96.0 & 96.0 & 95.9 & 95.9 \\Tstrut\\\\\n\\rule{0pt}{5mm} \n$p_\\mathrm{hard}$ & 0 & 0.9 & 0.99 & \\textbf{0.999} & 0.9999 & 0.99999 \\Bstrut\\\\\n\\hline\nAU-ROC & 94.9 & 94.9 & 95.7 & 96.0 & 95.8 & 95.7 \\Tstrut\\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly detection AU-ROC of \\mbox{\\ourmethod-M} on MVTec AD, VisA, and MVTec LOCO when varying the locations of quantiles.\nThese are the two sampling points $a$ and $b$ of the quantile-based map normalization and the mining factor $p_\\mathrm{hard}$.\nSetting $p_\\mathrm{hard}$ to zero disables the proposed hard feature loss.\nDefault values used in our experiments are highlighted in bold.\n}\n\\label{tab:hyperparams}\n\\vspace*{-8mm}\n\\end{table}",
            "tab:ablation_cumulative": "\\begin{table}\n\\small\n\\begin{center}\n\\begin{tabular}{lccc}\n & \\specialcell[c]{Detection \\\\ AU-ROC} & Diff. & \\specialcell[c]{Latency \\\\ {[ms]}} \\\\\n\\hline\nPDN & 93.2 & & 2.2 \\Tstrut\\\\\n$\\hookrightarrow$ with map normalization & 94.0 & + 0.8 & 2.2 \\\\\n\\, $\\hookrightarrow$ with hard feature loss & 95.0 & + 1.0  & 2.2 \\\\\n\\, \\, $\\hookrightarrow$ with pretraining penalty & 95.4 & + 0.4 & 2.2 \\Bstrut\\\\\n\\hline\n\\ourmethod-S & 95.4 & & 2.2 \\\\\n\\ourmethod-M & 96.0 & + 0.6 & 4.5 \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Cumulative ablation study in which techniques are gradually combined to form EfficientAD. Each AU-ROC percentage is an average of the mean AU-ROCs on MVTec AD, VisA, and MVTec LOCO.}\n\\label{tab:ablation_cumulative}\n\\end{table}",
            "tab:ablation_isolated": "\\begin{table}\n\\small\n\\begin{center}\n\\begin{tabular}{lccc}\n & \\specialcell[c]{Detection \\\\ AU-ROC} & Diff. & \\specialcell[c]{Latency \\\\ {[ms]}} \\\\\n\\hline\n\\ourmethod-S & 95.4 & & 2.2 \\Tstrut\\\\\nWithout map normalization \\, \\, & 94.7 & - 0.7 & 2.2 \\\\\nWithout hard feature loss & 94.7 & - 0.7 & 2.2 \\\\\nWithout pretraining penalty & 95.0 & - 0.4 & 2.2 \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Isolated ablation study in which techniques are separately removed from EfficientAD-S.}\n\\label{tab:ablation_isolated}\n\\vspace*{1mm}\n\\end{table}",
            "tab:arch_eads": "\\begin{table}\n\\begin{center}\n\\begin{tabular}{cccccc}\nLayer Name & Stride & Kernel Size & Number of Kernels & Padding & Activation \\\\\n\\hline\nConv-1 & 1$\\times$1 & 4$\\times$4 & 128 & 3 & ReLU \\\\\nAvgPool-1 & 2$\\times$2 & 2$\\times$2 & 128 & 1 & - \\\\\nConv-2 & 1$\\times$1 & 4$\\times$4 & 256 & 3 & ReLU \\\\\nAvgPool-2 & 2$\\times$2 & 2$\\times$2 & 256 & 1 & - \\\\\nConv-3 & 1$\\times$1 & 3$\\times$3 & 256 & 1 & ReLU \\\\\nConv-4 & 1$\\times$1 & 4$\\times$4 & 384 & 0 & - \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\caption{Patch description network architecture of the teacher network for \\ourmethod-S. The student network has the same architecture, but 768 kernels instead of 384 in the Conv-4 layer. A padding value of 3 means that three rows, or columns respectively, of zeros are appended at each border of an input feature map.}\n\\label{tab:arch_eads}\n\\end{table}",
            "tab:arch_eadm": "\\begin{table}\n\\begin{center}\n\\begin{tabular}{cccccc}\nLayer Name & Stride & Kernel Size & Number of Kernels & Padding & Activation \\\\\n\\hline\nConv-1 & 1$\\times$1 & 4$\\times$4 & 256 & 3 & ReLU \\\\\nAvgPool-1 & 2$\\times$2 & 2$\\times$2 & 256 & 1 & - \\\\\nConv-2 & 1$\\times$1 & 4$\\times$4 & 512 & 3 & ReLU \\\\\nAvgPool-2 & 2$\\times$2 & 2$\\times$2 & 512 & 1 & - \\\\\nConv-3 & 1$\\times$1 & 1$\\times$1 & 512 & 0 & ReLU \\\\\nConv-4 & 1$\\times$1 & 3$\\times$3 & 512 & 1 & ReLU \\\\\nConv-5 & 1$\\times$1 & 4$\\times$4 & 384 & 0 & ReLU \\\\\nConv-6 & 1$\\times$1 & 1$\\times$1 & 384 & 0 & - \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\caption{Patch description network architecture of the teacher network for \\ourmethod-M. The student network has the same architecture, but 768 kernels instead of 384 in the Conv-5 and Conv-6 layers. A padding value of 3 means that three rows, or columns respectively, of zeros are appended at each border of an input feature map.}\n\\label{tab:arch_eadm}\n\\end{table}",
            "tab:arch_ae": "\\begin{table}\n\\begin{center}\n\\begin{tabular}{cccccc}\nLayer Name & Stride & Kernel Size & Number of Kernels & Padding & Activation \\\\\n\\hline\nEncConv-1 & 2$\\times$2 & 4$\\times$4 & 32 & 1 & ReLU \\\\\nEncConv-2 & 2$\\times$2 & 4$\\times$4 & 32 & 1 & ReLU \\\\\nEncConv-3 & 2$\\times$2 & 4$\\times$4 & 64 & 1 & ReLU \\\\\nEncConv-4 & 2$\\times$2 & 4$\\times$4 & 64 & 1 & ReLU \\\\\nEncConv-5 & 2$\\times$2 & 4$\\times$4 & 64 & 1 & ReLU \\\\\nEncConv-6 & 1$\\times$1 & 8$\\times$8 & 64 & 0 & - \\\\\nBilinear-1 & \\multicolumn{5}{c}{Resizes the 1$\\times$1 input features maps to 3$\\times$3}  \\\\\nDecConv-1 & 1$\\times$1 & 4$\\times$4 & 64 & 2 & ReLU \\\\\nDropout-1 & \\multicolumn{5}{c}{Dropout rate = 0.2} \\\\\nBilinear-2 & \\multicolumn{5}{c}{Resizes the 4$\\times$4 input features maps to 8$\\times$8}  \\\\\nDecConv-2 & 1$\\times$1 & 4$\\times$4 & 64 & 2 & ReLU \\\\\nDropout-2 & \\multicolumn{5}{c}{Dropout rate = 0.2} \\\\\nBilinear-3 & \\multicolumn{5}{c}{Resizes the 9$\\times$9 input features maps to 15$\\times$15}  \\\\\nDecConv-3 & 1$\\times$1 & 4$\\times$4 & 64 & 2 & ReLU \\\\\nDropout-3 & \\multicolumn{5}{c}{Dropout rate = 0.2} \\\\\nBilinear-4 & \\multicolumn{5}{c}{Resizes the 16$\\times$16 input features maps to 32$\\times$32}  \\\\\nDecConv-4 & 1$\\times$1 & 4$\\times$4 & 64 & 2 & ReLU \\\\\nDropout-4 & \\multicolumn{5}{c}{Dropout rate = 0.2} \\\\\nBilinear-5 & \\multicolumn{5}{c}{Resizes the 33$\\times$33 input features maps to 63$\\times$63}  \\\\\nDecConv-5 & 1$\\times$1 & 4$\\times$4 & 64 & 2 & ReLU \\\\\nDropout-5 & \\multicolumn{5}{c}{Dropout rate = 0.2} \\\\\nBilinear-6 & \\multicolumn{5}{c}{Resizes the 64$\\times$64 input features maps to 127$\\times$127}  \\\\\nDecConv-6 & 1$\\times$1 & 4$\\times$4 & 64 & 2 & ReLU \\\\\nDropout-6 & \\multicolumn{5}{c}{Dropout rate = 0.2} \\\\\nBilinear-7 & \\multicolumn{5}{c}{Resizes the 128$\\times$128 input features maps to 64$\\times$64}  \\\\\nDecConv-7 & 1$\\times$1 & 3$\\times$3 & 64 & 1 & ReLU \\\\\nDecConv-8 & 1$\\times$1 & 3$\\times$3 & 384 & 1 & - \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\caption{Network architecture of the autoencoder for \\ourmethod-S and \\ourmethod-M. Layers named ``EncConv'' and ``DecConv'' are standard 2D convolutional layers.}\n\\label{tab:arch_ae}\n\\end{table}",
            "tab:distillation_backbones": "\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{ccccc}\n& Method & WideResNet-101 & ResNeXt-101 & DenseNet-201 \\\\\n\\hline\n\\multirow{3}{*}{\\rotatebox{90}{\\specialcell[c]{MVTec \\\\ AD}}} & \\text{PatchCore} & 98.7 & 98.8 & 98.7 \\\\\n& \\ourmethod-S & 98.8 & 98.9 & 98.8 \\\\\n& \\ourmethod-M & 99.1 & 99.0 & 99.2 \\\\\n&&&& \\\\ \n\\multirow{3}{*}{\\rotatebox{90}{\\specialcell[c]{MVTec \\\\ LOCO}}} & \\text{PatchCore} & 80.3  & 78.9 & 76.5 \\\\\n& \\ourmethod-S & 90.0 & 90.1 & 90.6 \\\\\n& \\ourmethod-M & 90.7 & 89.9 & 88.3 \\\\\n&&&& \\\\ \n\\multirow{3}{*}{\\rotatebox{90}{VisA}} & \\text{PatchCore} & 94.3 & 95.2 & 94.8 \\\\\n& \\ourmethod-S & 97.5 & 97.3 & 97.1 \\\\\n& \\ourmethod-M & 98.1 & 98.0 & 97.7 \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly detection AU-ROC percentages for different backbones. For \\ourmethod, each listed architecture is used as the distillation backbone in \\Cref{alg:distillation}. The ``WideResNet-101'' column contains the results reported in \\Cref{tab:per_dataset} in the main paper.}\n\\label{tab:distillation_backbones}\n\\end{table}",
            "tab:classification_roc": "\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{c|c|c|ccc||c}\nMethod & \\specialcell[c]{MAD \\\\ Mean} & \\specialcell[c]{VisA \\\\ Mean} & \\specialcell[c]{LOCO \\\\ Structural} & \\specialcell[c]{LOCO \\\\ Logical} & \\specialcell[c]{LOCO \\\\ Mean} & \\specialcell[c]{Overall \\\\ Mean} \\\\\n\\hline\nGCAD & 89.1 & 83.7 & 82.7 & 83.9 & 83.3 & 85.4 \\\\\nSimpleNet & 98.2 & 87.9 & 83.7 & 71.5 & 77.6 & 87.9 \\\\\n\\studteach & 93.2 & 94.6 & 88.3 & 66.5 & 77.4 & 88.4 \\\\\nFastFlow & 96.9 & 93.9 & 82.9 & 75.5 & 79.2 & 90.0 \\\\\nDSR & 98.1 & 91.8 & 90.2 & 75.0 & 82.6 & 90.8 \\\\\nPatchCore & 98.7 & 94.3 & 84.8 & 75.8 & 80.3 & 91.1 \\\\\n\\patchcoreens & \\textbf{99.3} & 97.7 & 87.7 & 71.0 & 79.4 & 92.1 \\\\\nAST & 98.9 & 94.9 & 87.1 & 79.7 & 83.4 & 92.4 \\\\\n\\hline\n\\ourmethod-S & 98.8 & 97.5 & 94.1 & 85.8 & 90.0 & 95.4 \\\\\n\\ourmethod-M & 99.1 & \\textbf{98.1} & \\textbf{94.7} & \\textbf{86.8} & \\textbf{90.7} & \\textbf{96.0} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly detection AU-ROC percentages per dataset collection. For \\ourmethod, we report the mean of five runs.}\n\\label{tab:classification_roc}\n\\vspace*{-5mm}\n\\end{table}",
            "tab:classification_prc": "\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{c|c|c|ccc||c}\nMethod & \\specialcell[c]{MAD \\\\ Mean} & \\specialcell[c]{VisA \\\\ Mean} & \\specialcell[c]{LOCO \\\\ Structural} & \\specialcell[c]{LOCO \\\\ Logical} & \\specialcell[c]{LOCO \\\\ Mean} & \\specialcell[c]{Overall \\\\ Mean} \\\\\n\\hline\nGCAD & 95.7 & 87.1 & 81.0 & 84.9 & 83.0 & 88.6 \\\\\nSimpleNet & 98.5 & 90.1 & 82.5 & 73.5 & 78.0 & 88.9 \\\\\n\\studteach & 95.7 & 94.6 & 87.9 & 70.7 & 79.3 & 89.9 \\\\\nFastFlow & 95.3 & 94.7 & 79.5 & 76.2 & 77.9 & 89.3 \\\\\nDSR & 98.1 & 93.8 & 88.2 & 76.6 & 82.4 & 91.4 \\\\\nPatchCore & 98.9 & 95.2 & 84.6 & 77.7 & 81.2 & 91.8 \\\\\n\\patchcoreens & \\textbf{99.0} & 97.8 & 88.3 & 74.7 & 81.5 & 92.8 \\\\\nAST & 98.9 & 95.3 & 84.5 & 80.5 & 82.5 & 92.2 \\\\\n\\hline\n\\ourmethod-S & 98.7 & 97.5 & 93.6 & 86.2 & 89.9 & 95.4 \\\\\n\\ourmethod-M & 98.9 & \\textbf{98.0} & \\textbf{93.9} & \\textbf{86.8} & \\textbf{90.3} & \\textbf{95.7} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly detection AU-PRC percentages per dataset collection. For \\ourmethod, we report the mean of five runs.}\n\\label{tab:classification_prc}\n\\vspace*{-5mm}\n\\end{table}",
            "tab:localization_pro_03": "\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{c|c|c|ccc||c}\nMethod & \\specialcell[c]{MAD \\\\ Mean} & \\specialcell[c]{VisA \\\\ Mean} & \\specialcell[c]{LOCO \\\\ Structural} & \\specialcell[c]{LOCO \\\\ Logical} & \\specialcell[c]{LOCO \\\\ Mean} & \\specialcell[c]{Overall \\\\ Mean} \\\\\n\\hline\nGCAD & 91.0 & 83.7 & 89.5 & 89.4 & 89.5 & 88.0 \\\\\nSimpleNet & 89.6 & 68.9 & 60.6 & 68.6 & 64.6 & 74.4 \\\\\n\\studteach & 92.4 & 93.0 & 90.8 & 76.4 & 83.6 & 89.7 \\\\\nFastFlow & 92.5 & 86.8 & 84.2 & 76.5 & 80.3 & 86.5 \\\\\nDSR & 90.8 & 68.1 & 81.3 & 72.3 & 76.8 & 78.6 \\\\\nPatchCore & 92.7 & 79.7 & 64.3 & 76.6 & 70.4 & 80.9 \\\\\n\\patchcoreens & \\textbf{95.6} & 79.3 & 62.0 & 72.6 & 67.3 & 80.7 \\\\\nAST & 81.2 & 81.5 & 75.4 & 62.6 & 69.0 & 77.2 \\\\\n\\hline\n\\ourmethod-S & 93.1 & 93.1 & 92.6 & 90.1 & 91.3 & 92.5 \\\\\n\\ourmethod-M & 93.5 & \\textbf{94.0} & \\textbf{93.7} & \\textbf{91.3} & \\textbf{92.5} & \\textbf{93.3} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly localization performance per method and dataset collection, measured with the AU-PRO up to a FPR of \\SI{30}{\\percent}. For \\ourmethod, we report the mean of five runs.}\n\\label{tab:localization_pro_03}\n\\vspace*{-5mm}\n\\end{table}",
            "tab:localization_pro_005": "\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{c|c|c|ccc||c}\nMethod & \\specialcell[c]{MAD \\\\ Mean} & \\specialcell[c]{VisA \\\\ Mean} & \\specialcell[c]{LOCO \\\\ Structural} & \\specialcell[c]{LOCO \\\\ Logical} & \\specialcell[c]{LOCO \\\\ Mean} & \\specialcell[c]{Overall \\\\ Mean} \\\\\n\\hline\nGCAD & 68.8 & 52.6 & 68.8 & 67.1 & 68.0 & 63.1 \\\\\nSimpleNet & 61.8 & 37.7 & 36.6 & 36.1 & 36.3 & 45.3 \\\\\n\\studteach & 73.4 & 75.0 & 75.6 & 49.7 & 62.6 & 70.4 \\\\\nFastFlow & 71.6 & 63.4 & 64.5 & 49.1 & 56.8 & 63.9 \\\\\nDSR & 78.9 & 49.5 & 67.1 & 49.8 & 58.5 & 62.3 \\\\\nPatchCore & 68.6 & 49.4 & 37.9 & 41.5 & 39.7 & 52.6 \\\\\n\\patchcoreens & \\textbf{79.5} & 55.1 & 37.8 & 35.3 & 36.5 & 57.1 \\\\\nAST & 42.1 & 48.0 & 50.1 & 35.3 & 42.7 & 44.3 \\\\\n\\hline\n\\ourmethod-S & 78.2 & 73.4 & 80.8 & 74.8 & 77.8 & 76.5 \\\\\n\\ourmethod-M & 78.4 & \\textbf{75.9} & \\textbf{83.2} & \\textbf{76.5} & \\textbf{79.8} & \\textbf{78.0} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly localization performance per method and dataset collection, measured with the AU-PRO up to a FPR of \\SI{5}{\\percent}. For \\ourmethod, we report the mean of five runs.}\n\\label{tab:localization_pro_005}\n\\vspace*{-5mm}\n\\end{table}",
            "tab:localization_roc_005": "\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{c|c|c|ccc||c}\nMethod & \\specialcell[c]{MAD \\\\ Mean} & \\specialcell[c]{VisA \\\\ Mean} & \\specialcell[c]{LOCO \\\\ Structural} & \\specialcell[c]{LOCO \\\\ Logical} & \\specialcell[c]{LOCO \\\\ Mean} & \\specialcell[c]{Overall \\\\ Mean} \\\\\n\\hline\nGCAD & 72.1 & 73.1 & 73.1 & 32.0 & 52.5 & 65.9 \\\\\nSimpleNet & 67.9 & 57.1 & 36.4 & 22.1 & 29.2 & 51.4 \\\\\n\\studteach & 74.3 & 82.7 & 69.8 & 20.6 & 45.2 & 67.4 \\\\\nFastFlow & 72.1 & 78.9 & 63.4 & 33.7 & 48.6 & 66.5 \\\\\nDSR & 76.1 & 66.5 & 66.0 & 25.5 & 45.7 & 62.8 \\\\\nPatchCore & 74.1 & 65.0 & 43.5 & 24.1 & 33.8 & 57.6 \\\\\n\\patchcoreens & 79.4 & 65.7 & 38.7 & 20.4 & 29.6 & 58.2 \\\\\nAST & 41.1 & 67.4 & 52.4 & 30.9 & 41.7 & 50.1 \\\\\n\\hline\n\\ourmethod-S & \\textbf{79.7} & 86.3 & 80.6 & 33.8 & 57.2 & 74.4 \\\\\n\\ourmethod-M & 79.4 & \\textbf{86.9} & \\textbf{82.1} & \\textbf{35.3} & \\textbf{58.7} & \\textbf{75.0} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly localization performance per method and dataset collection, measured with the AU-ROC up to a FPR of \\SI{5}{\\percent}. For \\ourmethod, we report the mean of five runs.}\n\\label{tab:localization_roc_005}\n\\vspace*{-5mm}\n\\end{table}",
            "tab:localization_prc": "\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{c|c|c|ccc||c}\nMethod & \\specialcell[c]{MAD \\\\ Mean} & \\specialcell[c]{VisA \\\\ Mean} & \\specialcell[c]{LOCO \\\\ Structural} & \\specialcell[c]{LOCO \\\\ Logical} & \\specialcell[c]{LOCO \\\\ Mean} & \\specialcell[c]{Overall \\\\ Mean} \\\\\n\\hline\nGCAD & 59.3 & 27.8 & 41.4 & 38.7 & 40.1 & 42.4 \\\\\nSimpleNet & 51.5 & 22.6 & 11.9 & 29.3 & 20.6 & 31.6 \\\\\n\\studteach & 59.9 & 36.2 & 43.5 & 27.4 & 35.4 & 43.8 \\\\\nFastFlow & 57.6 & 33.4 & 35.1 & 41.3 & 38.2 & 43.1 \\\\\nDSR & \\textbf{69.2} & \\textbf{41.1} & 50.4 & 32.7 & 41.5 & 50.6 \\\\\nPatchCore & 57.6 & 27.8 & 17.8 & 32.5 & 25.2 & 36.8 \\\\\n\\patchcoreens & 64.1 & 28.3 & 15.1 & 28.9 & 22.0 & 38.2 \\\\\nAST & 29.7 & 22.9 & 17.0 & 35.6 & 26.3 & 26.3 \\\\\n\\hline\n\\ourmethod-S & 65.9 & 40.4 & \\textbf{54.0} & 40.2 & \\textbf{47.1} & \\textbf{51.1} \\\\\n\\ourmethod-M & 63.8 & 40.8 & 51.9 & \\textbf{42.0} & 46.9 & 50.5 \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Mean anomaly localization performance per method and dataset collection, measured with the pixel-wise AU-PRC.  For \\ourmethod, we report the mean of five runs.}\n\\label{tab:localization_prc}\n\\end{table}",
            "tab:efficiency": "\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{cccccccc}\nMethod & \\specialcell[c]{Detect. \\\\ AU-ROC} & \\specialcell[c]{Segment. \\\\ AU-PRO} & \\specialcell[c]{Latency \\\\ {[ms]}} & \\specialcell[c]{Throughput \\\\ {[img / s]}} & \\specialcell[c]{Number of \\\\ Parameters [$\\times 10^6$]} & \\specialcell[c]{FLOPs \\\\ {[$\\times 10^9$]}} &  \\specialcell[c]{GPU Memory \\\\  {[MB]}} \\\\\n\\hline\nGCAD & 85.4 & 88.0 & 11 & 121 & 65 & 416 & 555 \\\\\nSimpleNet & 87.9 & 74.4 & 12 & 194 & 73 & \\textbf{38} & 508 \\\\\n\\studteach & 88.4 & 89.7 & 75 & 16 & 26 & 4468 & 1077 \\\\\nFastFlow & 90.0 & 86.5 & 17 & 120 & 92 & 85 & 404 \\\\\nDSR & 90.8 & 78.6 & 17 & 104 & 40 & 267 & 314 \\\\\nPatchCore & 91.1 & 80.9 & 32 & 76 & 83 + 3 & 41 + kNN & 637 + kNN \\\\\n\\patchcoreens & 92.1 & 80.7 & 148 & 13 & 150 + 8 & 159 + kNN & 1335 + kNN \\\\\nAST & 92.4 & 77.2 & 53 & 41 & 154 & 199 & 618 \\\\\n\\hline\n\\ourmethod-S & \\meanwithstd{95.4}{0.06} & \\meanwithstd{92.5}{0.05} & \\meanwithstd{\\textbf{2.2}}{0.01} & \\meanwithstd{\\textbf{614}}{2} & \\meanwithstd{\\textbf{8}}{0} & \\meanwithstd{76}{0} & \\meanwithstd{\\textbf{100}}{0} \\\\\n\\ourmethod-M & \\meanwithstd{\\textbf{96.0}}{0.09} & \\meanwithstd{\\textbf{93.3}}{0.04} & \\meanwithstd{4.5}{0.01} & \\meanwithstd{269}{1} & \\meanwithstd{21}{0} & \\meanwithstd{235}{0} & \\meanwithstd{161}{0} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{\nExtension of \\Cref{tab:main} in the main paper by additional computational efficiency metrics measured on an NVIDIA RTX A6000 GPU.\nFor \\ourmethod, we report the mean and standard deviation of five runs.\nFor PatchCore, we report the computational requirements of the feature extraction during inference separately from the nearest neighbor search.\n}\n\\label{tab:efficiency}\n\\end{table}",
            "tab:latency_per_gpu": "\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{cccccc}\nMethod & RTX A6000 & RTX A5000 & Tesla V100 & RTX 3080 & RTX 2080 Ti \\\\\n\\hline\n\\ourmethod-S & \\textbf{2.2} & \\textbf{2.5} & \\textbf{3.9} & \\textbf{3.8} & \\textbf{4.5} \\\\\n\\ourmethod-M & 4.5 & 5.3 & 6.3 & 7.0 & 7.6 \\\\\nGCAD & 10.7 & 11.7 & 12.9 & 13.7 & 18.0 \\\\\nSimpleNet & 12.0 & 13.3 & 19.2 & 18.1 & 21.9 \\\\\nFastFlow & 16.5 & 17.1 & 26.1 & 27.5 & 31.0 \\\\\nDSR & 17.2 & 18.0 & 24.8 & 24.6 & 34.5 \\\\\nPatchCore & 32.0 & 31.5 & 47.1 & 41.1 & 53.2 \\\\\nAST & 53.1 & 53.4 & 75.6 & 82.3 & 87.1 \\\\\n\\studteach & 74.7 & 81.0 & 82.2 & 99.6 & 121.7 \\\\\n\\patchcoreens & 147.6 & 145.0 & 229.2 & 189.0 & 216.9 \\\\\n\\end{tabular}\n\\end{center}\n\\caption{\nLatency in milliseconds per GPU, as plotted in \\Cref{fig:latency_per_gpu} in the main paper.\n}\n\\label{tab:latency_per_gpu}\n\\end{table}"
        },
        "figures": {
            "fig:teaser": "\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/teaser.pdf}\n\\end{center}\n   \\caption{Anomaly detection performance vs. latency per image on an NVIDIA RTX A6000 GPU\\@. Each AU-ROC value is an average of the image-level detection AU-ROC values on the MVTec AD \\cite{bergmann2021_mvtec_ad_ijcv, bergmann2019_mvtec_ad_cvpr}, VisA \\cite{zou2022spot}, and MVTec LOCO \\cite{bergmann2021_mvtec_loco_ijcv} dataset collections.}\n\\label{fig:teaser}\n\\end{figure}",
            "fig:pdn": "\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/pdn.pdf}\n\\end{center}\n   \\caption{Patch description network (PDN) architecture of \\ourmethod-S.\n   Applying it to an image in a fully convolutional manner yields all features in a single forward pass.\n   }\n\\label{fig:pdn}\n\\vspace*{2mm}\n\\end{figure}",
            "fig:artifacts": "\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/artifacts_reduced.pdf}\n\\end{center}\n   \\caption{Upper row: absolute gradient of a single feature vector, located in the center of the output, with respect to each input pixel, averaged across input and output channels.\n   Lower row: Average feature map of the first output channel across 1000 randomly chosen images from ImageNet \\cite{russakovsky2015_alexnet}.\n   The mean of these images is shown on the left.\n   The feature maps of the DenseNet \\cite{huang2017densely} and the WideResNet exhibit strong artifacts.\n   }\n\\label{fig:artifacts}\n\\vspace*{2mm}\n\\end{figure}",
            "fig:ohem": "\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/ohem.pdf}\n\\end{center}\n   \\caption{\n   Randomly picked loss masks generated by the hard feature loss during training.\n   The brightness of a mask pixel indicates how many of the dimensions of the respective feature vector were selected for backpropagation.\n   The student network already mimics the teacher well on the background and thus focuses on learning the features of differently rotated screws.\n   }\n\\label{fig:ohem}\n\\end{figure}",
            "fig:architecture": "\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/pipeline_reduced.pdf}\n\\end{center}\n   \\caption{\\ourmethod\\ applied to two test images from MVTec LOCO\\@.\n   Normal input images contain a horizontal cable connecting the two splicing connectors at an arbitrary height.\n   The anomaly on the left is a foreign object in the form of a small metal washer at the end of the cable.\n   It is visible in the local anomaly map because the outputs of the student and the teacher differ.\n   The logical anomaly on the right is the presence of a second cable.\n   The autoencoder fails to reconstruct the two cables on the right in the feature space of the teacher.\n   The student also predicts the output of the autoencoder in addition to that of the teacher.\n   Because its receptive field is restricted to small patches of the image, it is not influenced by the presence of the additional red cable.\n   This causes the outputs of the autoencoder and the student to differ.\n   ``Diff'' refers to computing the element-wise squared difference between two collections of output feature maps and computing its average across feature maps.\n   To obtain pixel anomaly scores, the anomaly maps are resized to match the input image using bilinear interpolation.\n   }\n\\label{fig:architecture}\n\\end{figure*}",
            "fig:latency_per_gpu": "\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/plot_per_gpu.pdf}\n\\end{center}\n\\vspace*{3mm}\n   \\caption{\n   Latency per GPU. The ranking of methods is the same on each GPU, except for two cases in which DSR is slightly faster than FastFlow.}\n\\label{fig:latency_per_gpu}\n\\vspace*{4mm}\n\\end{figure}",
            "fig:qual": "\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/qualitative_main_reduced.pdf}\n\\end{center}\n\\vspace*{-1mm}\n   \\caption{Non-cherry-picked qualitative results of \\ourmethod\\ on VisA. For each of its 12 scenarios, we show a randomly sampled defect image, the ground truth segmentation mask, and the anomaly map generated by EfficientAD-M.}\n\\label{fig:qual}\n\\end{figure}",
            "fig:throughput": "\\begin{figure}[h!]\n\\hfill\n\\subfigure[Anomaly detection]{\\includegraphics[width=.49\\linewidth]{figures/throughput_classification.pdf}}\n\\hfill\n\\subfigure[Anomaly localization]{\\includegraphics[width=.49\\linewidth]{figures/throughput_segmentation.pdf}}\n\\hfill\n\\caption{Anomaly detection performance vs. throughput on an NVIDIA RTX A6000 GPU\\@. We report the image-level anomaly detection performance on the left using the image-level AU-ROC. On the right, we report the anomaly localization performance using the pixel-level AU-PRO segmentation metric up to a FPR of \\SI{30}{\\percent}. Each AU-ROC and AU-PRO value is an average of the values on MVTec AD, VisA, and MVTec LOCO. We measure the throughput using a batch size of 16.\n}\n\\label{fig:throughput}\n\\end{figure}",
            "fig:qualitative_1": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/qualitative_1_reduced.pdf}\n\\end{center}\n\\caption{\nAnomaly maps on anomalous images from MVTec LOCO and MVTec AD.\nFor MVTec LOCO, we show a logical anomaly (upper row) and a structural anomaly (lower row) for each scenario.\nThe receptive field of AST's features is large enough to detect some logical anomalies, while \\patchcoreens\\ and S--T struggle with logical anomalies.\n}\n\\label{fig:qualitative_1}\n\\end{figure}",
            "fig:qualitative_2": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/qualitative_2_reduced.pdf}\n\\end{center}\n\\caption{\nAnomaly maps on anomalous images from MVTec AD.\nAlmost all anomalies are detected by every method, but the separability of pixel anomaly scores varies between methods.\nFor example, \\patchcoreens\\ detects the anomaly on the capsule in the first row but the pixel anomaly scores are in a similar range as the false positive detections in the background of the screw image.\n}\n\\label{fig:qualitative_2}\n\\end{figure}",
            "fig:qualitative_3": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/qualitative_3_reduced.pdf}\n\\end{center}\n\\caption{\nAnomaly maps on anomalous images from MVTec AD and VisA.\nVisA contains challenging, small anomalies, such as the defect on the non-aligned macaronis or the defect on the fryum two rows above.\n}\n\\label{fig:qualitative_3}\n\\end{figure}"
        },
        "git_link": "https://github.com/openvinotoolkit/anomalib/"
    }
}