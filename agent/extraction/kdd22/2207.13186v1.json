{
    "meta_info": {
        "title": "On Missing Labels, Long-tails and Propensities in Extreme Multi-label  Classification",
        "abstract": "The propensity model introduced by Jain et al. 2016 has become a standard\napproach for dealing with missing and long-tail labels in extreme multi-label\nclassification (XMLC). In this paper, we critically revise this approach\nshowing that despite its theoretical soundness, its application in contemporary\nXMLC works is debatable. We exhaustively discuss the flaws of the\npropensity-based approach, and present several recipes, some of them related to\nsolutions used in search engines and recommender systems, that we believe\nconstitute promising alternatives to be followed in XMLC.",
        "author": "Erik Schultheis, Marek Wydmuch, Rohit Babbar, Krzysztof DembczyÅ„ski",
        "link": "http://arxiv.org/abs/2207.13186v1",
        "category": [
            "cs.LG",
            "cs.IR"
        ],
        "additionl_info": "This is the author's version of the work accepted at KDD '22"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nExtreme multi-label classification (XMLC) is a supervised learning problem \nwhere only a few labels from an enormous label space, \nreaching orders of millions, are relevant per data point. \nNotable examples are \ntagging of text documents~\\citep{Agrawal_et_al_2013},\ncontent annotation for multimedia search~\\citep{Deng_et_al_2011}, \nand diverse types of recommendation, \nincluding webpages-to-ads~\\citep{Beygelzimer_et_al_2009b}, \nads-to-bid-words~\\citep{Prabhu_Varma_2014}, \nusers-to-items~\\citep{Zhuo_et_al_2020}, \nqueries-to-items~\\citep{Medini_et_al_2019}, \nor items-to-queries~\\citep{Chang_et_al_2020}.\nThese practical applications pose statistical challenges, including: \n1) long-tailed distribution of labels---infrequent (tail) labels are much harder to predict \nthan frequent (head) labels due to data imbalance, and a model completely ignoring the tail labels can get very high scores on standard performance metrics;  \n2) missing relevant labels in the observed training data---since it is nearly impossible to check the whole set of labels \nwhen it is so large.\n\nTo address the latter issue, \npropensity-scored versions of popular measures (i.e., precision$@k$ and nDCG$@k$) \nwere introduced by~\\citet{Jain_et_al_2016}. \nUnder the propensity model, it is assumed that an assignment of a label to an example is always correct,\nbut the supervision may skip some positive labels, and propensity of a label refers to the probability of not skipping that label.\nUnder the implicit assumption that the chance for a label to be missing is higher for tail than for head labels, the propensity-scored measures were used to evaluate the prediction performance on tail labels. \nDespite being originally introduced to study the phenomenon of missing labels in XMLC, over the years, they have found their way into the literature as default performance metrics on tail labels \\citep{You_et_al_2019,guo_breaking_2019,Babbar_Scholkopf_2019}.\n\nIn this work, we take a step back and thoroughly investigate the validity of the propensity model of~\\citet{Jain_et_al_2016}, \nfurther referred to as \\xmlcpropensitymodel{} (from the first letters of authors' names),\nfor the dual usage of missing and long-tail labels in XMLC.\nWe start our discussion by recalling the definition of the XMLC problem, stating the problem of missing labels, \nand bringing closer the issues with long-tail labels (Section~\\ref{sec:problem_statement}).\nWe recall the \\xmlcpropensitymodel{} propensity model and highlight its shortcomings in Section~\\ref{sec:critical-view-on-propensities}, both in terms of the model itself and in regard to its current usage in XMLC.\nIn particular, we demonstrate that this model: \n(i) does not fulfill natural conditions that may be desired of a reasonable propensity model, \n(ii) falls short on reliable and reproducible estimation of the model hyper-parameters,\nand (iii) leads to implausible results exceeding substantially the natural range of the metrics\n(e.g., precision@k $>$ 300\\%).\n\nAfter formally studying the above short-comings of the \\xmlcpropensitymodel{} model, \nwe propose a suite of alternatives (c.f. Section \\ref{sec:recipes-to-follow}) \nwhich are promising to follow for a more principled approach in \n(i) evaluating machine learning systems trained on data with incomplete user feedback, \nand (ii) disentangling the individual contribution of missing and tail labels in XMLC. \nWe suggest using unbiased sets for validating models designed to deal with missing labels.\nAlternatively, one should use a set with a controlled bias, \non which one can obtain unbiased estimates of performance metrics.\nThereafter, we discuss alternative propensity models, which possess desirable analytical properties, \nand compare them with the \\xmlcpropensitymodel{} model empirically, confirming its shortcomings.\nWe also show the efficacy of a framework \nin which label propensities and parameters of the learning model are learned jointly. \nTowards disambiguating the phenomenon of missing and long-tail labels in XMLC, \nwe finally highlight other metrics as possible options for measuring tail-label performance instead of conflating these with missing labels.\n\nIt should be noted that, unlike most contemporary advances in XMLC, our goal in this work is not algorithmic. \nInstead, we take a critical viewpoint and study the commonly-used propensity model, \nexplicating the consequences when it is used in real-world production environments. \n\n\n"
            },
            "section 2": {
                "name": "Problem statement",
                "content": "\n\\label{sec:problem_statement}\nWe first define the problem of XMLC, \nthen the problem of missing labels, \nand finally the problem of long-tail labels. \n\n",
                "subsection 2.1": {
                    "name": "Extreme multi-label classification",
                    "content": "\n\\label{subsec:xmlc}\nThe goal of XMLC is to find a mapping between instances $\\rinstance \\in \\instancespace$ \nand a finite set of $\\numlabels$ non-mutually-exclusive class labels.%\n\\footnote{We use capital letters for random variables, \nand calligraphic letters for sets.} \nThis means that any specific\nrealization $\\sinstance$ of $\\rinstance$ is associated with a (possibly empty)\nsubset $\\relevantlabels{\\sinstance} \\subset [\\numlabels]$ of the labels called\nthe \\emph{relevant} or \\emph{positive} labels, \nwith the complement, $[\\numlabels] \\setminus \\relevantlabels{\\sinstance}$, \nof the \\emph{irrelevant} or \\emph{negative} ones. \nWe identify the relevant labels with a binary vector\n$\\slabels \\in \\labelspace$ through $\\sblabel_j = \\indicator[j \\in\n\\relevantlabels{\\sinstance}]$,\\footnote{$\\indicator[\\cdot]$ is the indicator function.}\nwhere $\\labelspace \\coloneqq \\set{0,\n1}^\\numlabels$ is called the \\emph{label vector space}. In the classical setting, we\nassume that observations $(\\rinstance, \\rlabels)$ are generated independently\nand identically according to a probability distribution $\\probability$ on\n$\\instancespace \\times \\labelspace$.  \nIn case of XMLC we assume $\\numlabels$ to be a large number (e.g., $\\ge 10^5$), \nand $\\|\\rlabels\\|_1$ to be much smaller than $\\numlabels$, $\\|\\rlabels\\|_1 \\ll \\numlabels$.\n\n% Regret part\nThe problem of XMLC can be defined as finding a \\emph{classifier} \n$\\defmap{\\hypothesis}{\\instancespace}{\\reals^m}$ which minimizes the \\emph{task risk}:\n\\begin{equation}\n\\risk_{\\taskloss}[\\hypothesis; \\rinstance, \\rlabels] \\coloneqq \\expectation[\\taskloss(\\rlabels, \\hypothesis(\\rinstance))]\\,,\n\\end{equation}\nwhere $\\defmap{\\taskloss}{\\labelspace \\times \\reals^m}{\\nnreals}$ is the (\\emph{task}) \\emph{loss}.\nThe optimal (\\emph{Bayes}) classifier for a loss $\\taskloss$ is given by\n\\begin{equation}\n\\bayescls(\\sinstance) = \\argmin_{\\spreds \\in \\reals^m} \\expectation[\\taskloss(\\rlabels, \\spreds) \\mid \\rinstance = \\sinstance]\\,.\n\\end{equation}\nThe above definitions follow the standard statistical learning framework. \nLet us notice, however, that in XMLC, instead of loss functions, \none often uses performance metrics,\nwhich are rather maximized than minimized.\nMoreover, these definitions correspond to the most natural setting\nin which a decision is made based on a single $\\sinstance$. \nLater in the paper, we also consider more general metrics \nthat cannot be optimized with respect to individual instances.\n\nTypically, a task loss is hard to optimize and one chooses\ninstead a \\emph{surrogate loss} that is easier to cope with, e.g., because it is\ndifferentiable and convex. Furthermore, instead of a\nprobability distribution, a learning algorithm operates on a finite i.i.d.\nsample and minimizes the corresponding empirical risk.\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Missing labels",
                    "content": "\n\\label{subsec:missing_labels}\nIn XMLC, the observed data might not follow the distribution we want to learn about. \nAs an illustrative example, take the Wikipedia-500k dataset. \nThe content of a Wikipedia article should be matched with a set of categories the article belongs to. \nSuch a dataset can be easily created by scraping existing Wikipedia annotations. \nHowever, there are about \\num{500000} categories on Wikipedia, \nand it is clear that the original authors and curators have\nnever checked every single category for each article.%\n\\footnote{If it took a human one second to check a category for an article, \nthen annotating a single article fully would take almost 6 days.}\n%\nOn the other hand, each category that has been assigned to an article has been\nverified by a human to be relevant. \nTherefore, the labeling error can be assumed to be strongly one-sided: \nThere may be many missing labels, but spurious labels should be uncommon. \n\nTo contrast ground-truth labels $\\rlabels$ with those actually available, \nwe denote the \\emph{observed labels} $\\rolabels$ using a tilde.%\n\\footnote{Note that other papers, including~\\cite{Jain_et_al_2016}, \nuse often a slightly different notation.} \nMathematically, the setting studied in this paper is defined by\n%\n\\begin{equation}\n \\probability[\\rolabels \\preceq \\rlabels \\given \\rinstance] = 1\\,, \\qquad\n \\probability[\\rolabels \\npreceq \\rlabels \\given \\rinstance] = 0\\,,\n\\label{eqn:general_missing_labels_problem}\n\\end{equation}\nwhere $\\rolabels \\preceq \\rlabels$ means that $\\rolabel_j \\le \\rblabel_j$ for all $j \\in [m]$,\nand $\\rolabels \\npreceq \\rlabels$ means that \nthere is at least one label for which $\\rolabel_j > \\rblabel_j$. \nNotice that the above equations cover also the no noise case, \nas we may have $\\probability[\\rolabels = \\rlabels \\given \\rinstance] = 1$. \n\nReconstruction of the ground truth distribution from the observed one, in the general case,\nis not a trivial task from the statistical and computational perspective, \nas it requires an exponential number of parameters.\nLet $\\eta_{\\slabels}(\\sinstance) \\coloneqq \\probability[\\rlabels=\\slabels \\given \\rinstance = \\sinstance]$\nand $\\tilde{\\eta}_{\\slabels}(\\sinstance) \\coloneqq \\probability[\\rolabels=\\slabels \\given \\rinstance = \\sinstance]$.\nWe have then\n\\begin{equation}\n\\tilde{\\eta}_{\\solabels}(\\bx) = \n\\sum_{\\slabels} \np_{\\solabels}(\\slabels, \\sinstance)\n\\eta_{\\slabels}(\\bx)\\,,\n\\end{equation}\nwhere $p_{\\solabels}(\\slabels, \\sinstance) \\coloneqq \\probability[\\rolabels=\\solabels \\given \\rlabels=\\slabels, \\rinstance = \\sinstance]$ is \na propensity of observing $\\solabels$ for ground-truth labels $\\slabels$ and instance $\\sinstance$.\nNotice that from (\\ref{eqn:general_missing_labels_problem}) \nwe have $p_{\\solabels}(\\slabels, \\sinstance) = 0$ for $\\solabels \\npreceq \\slabels$.\n%\nFurthermore, let $\\vec{\\eta}_{\\calY}(\\sinstance)$ and $\\tilde{\\vec{\\eta}}_{\\calY}(\\sinstance)$\nbe vectors of $\\eta_{\\slabels}(\\sinstance)$ and $\\tilde{\\eta}_{\\slabels}(\\sinstance)$, respectively,\nfor all $\\slabels \\in \\labelspace$ given in some predefined order $\\pi$. % over vectors $\\slabels$.\nLet $\\mathsf{C}$ be a matrix containing all propensities $p_{\\solabels}(\\slabels, \\sinstance)$,\nwith rows and columns corresponding to $\\rolabels$ and $\\rlabels$, respectively, \nand organized according to $\\pi$.  \nThen, we get: \n\\begin{equation}\n\\tilde{\\vec{\\eta}}_{\\calY}(\\sinstance) = \\mathsf{C} \\vec{\\eta}_{\\calY}(\\sinstance) \\,,\n\\end{equation}\nand, finally:\n\\begin{equation}\n\\vec{\\eta}_{\\calY}(\\sinstance) = \\mathsf{C}^{-1}\\tilde{\\vec{\\eta}}_{\\calY}(\\sinstance) \\,,\n\\end{equation}\nwhere we need to assume that $\\mathsf{C}$ is invertible. \n\n\nBecause of the practical reasons, \na much simpler, label-wise, propensities  %model \nare commonly used that are defined for each label separately:\n\\begin{equation}\n \\propensity_j(\\rinstance) \\coloneqq \\probability[\\rolabel_j=1 \\given \\rblabel_j = 1, \\rinstance]\\,.\n\\label{eq:missing-model}\n\\end{equation}\n\nLet $\\tilde{\\eta}_{j}(\\sinstance) \\coloneqq \\probability[\\rolabel_j = 1 \\given \\rinstance = \\sinstance]$ \nand $\\eta_{j}(\\sinstance) \\coloneqq \\probability[\\rblabel_j = 1 \\given \\rinstance = \\sinstance]$.\nWe have then:\n\\begin{equation}\n\\olabeldist_j(\\sinstance) = \\propensity_j(\\sinstance) \\labeldist_j(\\sinstance)\\,, \\qquad\n\\labeldist_j(\\sinstance) = \\olabeldist_j(\\sinstance) / \\propensity_j(\\sinstance)\\,.\n\\label{eqn:prob_adjustment}\n\\end{equation}\n\n\nIf propensities are known, then they can be used to construct an unbiased, task or \nsurrogate, loss $\\ubloss$ \\citep{van_rooyen_theory_2017}\nin the sense that\n\\begin{equation}\n    \\forall \\hypothesis: \\; \\risk_{\\lossfn}[\\hypothesis; \\rinstance, \\rlabels] = \\risk_{\\ubloss}[\\hypothesis; \\rinstance, \\rolabels].\n    \\label{eq:unbiased-risk-property}\n\\end{equation}\nThe construction of the unbiased counterpart depends on the \nform of propensities,  \ne.g., the label-wise propensities (\\ref{eq:missing-model}) are sufficient\nfor losses decomposable over labels~\\citep{natarajan_cost-sensitive_2017}  \nlike Hamming loss or binary cross-entropy,\nbut might not be for more complex losses without additional assumptions~\\citep{Schultheis_Babbar_2021}.\nThe unbiased losses can be used in training procedures~\\citep{Jain_et_al_2016,Qaraei_et_al_2021}\nor for estimating the performance of classifiers.\nFor some losses, such as Hamming loss or precision@$k$, the Bayes\nclassifier can be written as a function of the conditional label distributions\n$\\labeldist_j(\\sinstance)$.\nIn this case, one can adjust existing inference procedures to use (\\ref{eqn:prob_adjustment}) \nto obtain estimates of $\\labeldist_j(\\sinstance)$ \nfrom estimates of $\\olabeldist_j(\\sinstance)$~\\citep{propensity-plt}.\n\n\n\n"
                },
                "subsection 2.3": {
                    "name": "Long-tailed label distribution",
                    "content": "\n\\label{subsec:long-tail-labels}\n\nA defining characteristic of extreme classification data is that the label\ndistribution is highly imbalanced. In the binary case, the amount of imbalance\nis completely determined by the imbalance ratio\n$\\frac{\\probability[\\rblabel=0]}{\\probability[\\rblabel=1]}$. \nIn this sense, almost every binary problem\ncorresponding to a label is highly imbalanced in XMLC, i.e., only a small\nfraction of training instances will be associated with that label. However,\nin XMLC, the data are also imbalanced when comparing different labels. In\nanalogy to the binary case, we can define an inter-label imbalance ratio\nthrough $\\operatorname{ILIR} = \\frac{\\max\\{\\probability[\\rblabel_i=1] : i \\in [\\numlabels]\\}}{\\min\\{ \\probability[\\rblabel_j = 1] : j \\in\n[\\numlabels] \\}}$.\\footnote{This concept is also used in \\emph{multiclass}\nclassification, e.g. the \\emph{imbalance factor} of \\citet{cui2019class}.}\nNevertheless, the imbalance factor does not cover an important\nproperty of the label distribution. It could be that most labels have a\nlarge number of positives, but some have very few, or vice-versa. The latter\ncase is what happens in XMLC, where the label distribution\nis said to be \\emph{long-tailed}~\\citep{Bhatia_et_al_2015, Babbar_Scholkopf_2017}.\n\n\n\n\nIn \\autoref{table:dataset-stats}, these imbalance measures are shown for\nseveral XMLC datasets. \nWe use \\textit{min IR} to denote the binary imbalance ratio of the head label,\ni.e., the label with the largest fraction of positive instances\n(therefore, its IR is the smallest),\nand Pos-$80\\%$ to indicate the minimum fraction of class labels that retain $80\\%$ of \npositive labels (i.e., $y_j = 1$) in the training set.\nFor example, in Delicious-200K, only four percent of the class labels contain $80\\%$ of the positive labels.\n \n\nIn addition to the number of positive instances of a sparse label, \ntheir distribution within the feature space can be very important. \nIf $\\rblabel_j$ has few positives, but in a small pocket $\\instancespace_{+}$ of\nthe feature space it still fulfills \n$\\probability[\\rblabel_j = 1 \\mid \\rinstance \\in \\instancespace_{+}]\\approx 1$, \nthen a learning algorithm might still learn a reasonable decision boundary, \nespecially if the overall number of samples is large enough \\citep[p. 23]{fernandez2018learning}. \nIn contrast, if $\\probability[\\rblabel_j = 1 \\mid \\rinstance] \\ll 1$ \neverywhere (called \\emph{uniform class imbalance} by \\citet{singh2021statistical}), \nlearning to recognize the given class might be infeasible.\n\n"
                }
            },
            "section 3": {
                "name": "Critical view on the current approach to sparse labels in XMLC",
                "content": "\n\\label{sec:critical-view-on-propensities}\n\nIn this section we present an overview on the current state of addressing the\nlong-tail and missing-labels problems in XMLC. This is in large parts based on\nthe work of \\citet{Jain_et_al_2016}, so we start by a recap of their findings.\n\n",
                "subsection 3.1": {
                    "name": "Current approach to missing labels and long tails",
                    "content": "\nThe goal of \\citet{Jain_et_al_2016} was to develop loss functions for XMLC that\n\\begingroup\n\\addtolength\\leftmargini{-0.1in}\n\\begin{quote}\n(a) prioritize predicting the few relevant labels over the large number of\nirrelevant ones; (b) do not erroneously treat missing labels as irrelevant\n[...] (c) promote the accurate prediction of infrequently occurring, hard to\npredict, but rewarding tail labels.\n\\end{quote}\n\\endgroup\n\nThere are two main contributions of the paper that are relevant for our discussion:\nFirst, the development of unbiased loss functions that allow compensating for missing\nlabels if their propensities are known, and second, an empirical model to estimate\nthese propensities on XMLC data.\n\n\n",
                    "subsubsection 3.1.1": {
                        "name": "Propensity-scored losses",
                        "content": "\n\nPopular XMLC performance metrics\nfocus on the highest scored labels by the prediction algorithm.  \nExamples of such metrics are \\emph{precision at k} ($\\precision[k]$), \\emph{recall at k} ($\\recall[k]$), \nor \\emph{(normalized) discounted cumulative gain} $\\ndcg[k]$.\n%\nFor these metrics, unbiased estimates \nin the sense of \\eqref{eq:unbiased-risk-property} can be calculated, \nwhich are called the \\emph{propensity-scored} (PS) variants of these metrics\n(more examples in \\citep[Table 1]{Jain_et_al_2016}).\nTable~\\ref{tab:psmetrics} gives the formal definitions,\nwhere $\\topk[k]$ maps a vector to the indices of its top-k components, \n$\\rankof{j}(\\spreds)$ gives the ranking of the $j$-th element in the vector,\nand $p_j$ is the propensity for label $j$. \nLet us notice that, in general, $p_j$ shall depend on $x$, \nbut \\citet{Jain_et_al_2016} practically assume $p_j$ to be a constant value for each label $j$.\nMoreover, of the above unbiased estimates, only $\\psprecision$ and $\\psndcg$ have found widespread use\n\\citep{Bhatia_et_al_2016}, because $\\psrecall$ still requires the knowledge of\nthe total number of relevant labels $\\|\\slabels\\|_1$.\n%\n\n\n\nBecause \\citet{Jain_et_al_2016} observed that the unbiased estimates\ncould results in values larger than one, they suggest \na normalized version of these metrics to be reported (cf. Section~\\ref{subsec:implausible_results}).\nIn subsequent literature, the distinction between the unbiased metrics\nand the normalized versions is not always preserved, e.g., \\citet{Bhatia_et_al_2016} present unbiased formulas\nbut lists normalized values.\n\n"
                    },
                    "subsubsection 3.1.2": {
                        "name": "Empirical propensity model",
                        "content": "\n\\label{subsec:empirical-model}\nIn order to use the propensity-scored loss functions, one needs to have\nthe propensities available for the individual labels. Since true propensities\nare unknown for the XMLC benchmark datasets,\n\\citet{Jain_et_al_2016} proposed to model propensities\nas a function of labels frequencies, \nresulting in propensities being a constant value for each label. \n\nLet $\\phi$ denote a propensity model.  \nThe model defined in~\\citep{Jain_et_al_2016}\ncan be expressed via label priors $\\olabelprior_j \\coloneqq \\probability[\\rolabel_j=1]$:\n\\begin{equation}\n\\label{eq:jain-prop-model}\n\\!\\!\\! \\propensity_j \\!=\\! \\phijain(\\olabelprior_j; \\numinstances, a, b) \\!\\coloneqq\\! \\frac{1}{1 + (\\log \\numinstances - 1)(b + 1)^a e^{-a \\log (\\numinstances \\olabelprior_j + b)}} \\,,\n\\end{equation}\nwhere $n$ is the number of training instances, and $a$ and $b$ are\ndataset-dependent parameters.\n\nIn order to arrive at this model and determine values for $a$ and $b$,\n\\citet{Jain_et_al_2016} investigated two datasets in which ancillary\ninformation could be used to identify some missing labels.\n\nFor a Wikipedia-based dataset, the parameters of the model have been\nestimated with the help of a label hierarchy. They assumed\nthat if a label is relevant to an article, then all its ancestors in the\nhierarchy should also be relevant. If not present, they are counted as\nmissing. This allows plotting the fraction of instances in which the label is\nmissing over the number of instances in which it appears. This seems to\nfollow a sigmoidal shape as described by \\eqref{eq:jain-prop-model}, see~Figure~\\ref{fig:wikipedia-ps-est}. \nThe\nparameters $a$ and $b$ were then determined by fitting the model against the\nestimated values, where only labels with more than 4 descendants were used to\nimprove robustness. The obtained values are $a = 0.5, b = 0.4$.\n%\n\nFor the Amazon data set, which is an item-to-item recommendation task, \nmissing labels have been approximated using ``also viewed'' and ``also bought`` information. \nIt was assumed that a label $j$ (an item) is relevant to all the items viewed \nalong with items that were also bought with label $j$, as proposed by \\citet{McAuley_et_al_2015}. \nThe obtained values are $a = 0.6$ and $b = 2.6$.\n\nFor other data sets the authors propose, if there is no other possibility of\nestimating parameters $a$ and $b$, to use averages of the values obtained for\nWikipedia and Amazon data sets (which are $a = 0.55$, $b = 1.5$). This, in fact,\nhas become a standard followed in many papers without questioning its rationality.\n\nThe above propensity model is then typically used in the metric of choice \nfor model selection and evaluation. %(i.e., for validation and testing of models).\nIt has also been incorporated into training procedures. \nFor example, decision tree methods can directly use the\npropensity-scored variants of metrics such as precision$@k$ or nDCG$@k$ \\citep{Jain_et_al_2016}.\nAlternatively, one can use unbiased or upper-bounded propensity-scored\nsurrogate losses~\\citep{Qaraei_et_al_2021}.\n\n"
                    },
                    "subsubsection 3.1.3": {
                        "name": "Propensity and long tails",
                        "content": "\n\\label{subsec:prop-long-tail}\nThe form of \\eqref{eq:jain-prop-model} implies that tail labels are assigned\nlower propensities, which means that in metrics like those in \\autoref{tab:psmetrics} these\ntail labels, if predicted correctly, will be weighted more strongly than head\nlabels. In particular, the resulting weightings resemble existing weighting\nschemes used for long-tailed learning tasks, leading the authors to conclude:\n\n\\begingroup\n\\addtolength\\leftmargini{-0.1in}\n\\begin{quote}\nSuch weights arise naturally as inverse propensities in the unbiased losses\ndeveloped in this paper. [...] This not only provides a sound theoretical\njustification of label weighting heuristics for recommending rare items but\nalso leads to a more principled setting of the weights.\n\\end{quote}\n\\endgroup\n\n\nAs a result, propensity-scored variants are also viewed as metrics in their own right, and are currently used both \nto counteract missing labels (as unbiased estimates) and to weigh tail labels (as independent metrics),\nbecoming established performance metrics commonly used in XMLC.%\n\\footnote{We list several examples of references to propensity-scored losses:\n``We examined the performance on tail labels by PSP@k''~\\citep{You_et_al_2019};\n``We achieve high precision and propensity scores, \nthus demonstrating the effectiveness of our method even on infrequent tail labels.''~\\citep{guo_breaking_2019};\n``capture prediction accuracy of a learning algorithm at top-k slots of prediction, and also the diversity of\nprediction by giving higher score for predicting rarely occurring\ntail-labels''~\\citep{Babbar_Scholkopf_2019};\n``propensity scored precision@k which has recently been shown to be an\nunbiased, and more suitable, metric''~\\citep{jain_slice_2019};\n``which leads to better performance on tail labels.''~\\citep{Yen_et_al_2017};\n``propensity scored variant which is unbiased and assigns higher rewards for accurate tail label predictions'', \n``evaluate prediction performance on tail labels using propensity scored variants''~\\citep{Khandagale_et_al_2019};\n``replacing the nDCG loss with its propensity scored variant and using additional classifiers designed for tail labels''~\\citep{Tagami_2017}.\n}\n\n\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Discussion of missing-labels assumptions",
                    "content": "\n\nIn order to derive unbiased loss functions, we need to impose assumptions on\nthe process of how labels go missing, as initially discussed in Section~\\ref{subsec:missing_labels}.\nUnfortunately, \\citet{Jain_et_al_2016} sent a potentially misleading message in this regard. \nTheir Theorem 4.1 proves\n\\begin{equation}\n    \\expectation[\\lossfn(\\rlabels, \\spreds)] = \\expectation[\\ubloss(\\rolabels, \\spreds)]\\,,\n\\end{equation}\nfor any \\textbf{fixed} prediction $\\spreds$ without a clear dependence on $X$. \nThis also implies that the assumptions behind the propensities are unclear.\nEven if we assume the propensities to be constant for label $j$,\nthe exact form of this assumption is necessary to properly prove unbiasedness \nin the sense of (\\ref{eq:unbiased-risk-property}).\nNotice that $\\probability[\\rolabel_j = 1 \\given \\rblabel_j = 1] = p_j$\ndoes not imply $\\probability[\\rolabel_j = 1 \\given \\rblabel_j = 1, X] = p_j$.\nMoreover, for more complex functions, such as recall$@k$, this assumption may take the form of \n$\\probability[\\rolabel_j = 1 \\given \\rblabel_j = 1, \\rlabels_{\\!\\neg j}, X] = p_j$,\nwhere  $\\rlabels_{\\!\\neg j}$ represents ground-truth labels without label $j$\n(see Appendix for an example).\n\nIn general, we cannot expect the independence of missing labels from the\ninstance's features to hold. Consider, for example, cases where the feature and\nlabel space are of a similar origin~\\citep{dahiya2021siamesexml}, such as matching\nWikipedia titles or articles to categories. \nIt seems unlikely that a label such as ``Italy'' would be missing\nfor articles containing the word ``Italy'' in the subject, \nbut it might be missing for articles that still pertain to Italy  \nbut do not feature the word ``Italy'' in the title.\nThe assumption that propensities are constant for each label simplifies the model \nsignificantly and leads to much simpler computational procedures. \nUnfortunately, if this assumption is not satisfied, \nthen one may get implausible results as discussed later. \n\nThe assumption that the propensities do not depend on other labels going missing \ndoes not need to hold in practice as well.\nFor example, a user that tagged the article for ``Italy'' with ``Member states\nof the European Union'' might be primed to think of more examples of\norganizations in which Italy is a member, and thus e.g., ``Current member\nstates of the United Nations'' might be less likely to be forgotten than if\nthe EU membership had been forgotten. \nFortunately, in many cases, the unbiased\nestimate does not actually require this dependence --- \nif the loss function can be written as a sum over contributions from each label individually, \nthen the labels do not interact with each other \nand the label-wise properties are sufficient to obtain unbiased losses.\nThis is the case for the popular $\\psprecision$ and $\\psndcg$ metrics.\n\n\n\n"
                },
                "subsection 3.3": {
                    "content": "\n\\label{subsec:shortcomings}\n\nLet us discuss several issues of the \\xmlcpropensitymodel{} model,\nconcerning theoretical and empirical shortcomings, \nas well as some problems in the way the parameters of the model have been established.\n\n\\paragraph{Scaling behavior}\nLet us first observe that \\eqref{eq:jain-prop-model} does not preserve propensity estimates \nif the amount of data is changed, without changing its characteristics, \ne.g., by sub- or over-sampling the dataset. \nIn particular, if one increases the amount of available data \nby making multiple copies of the dataset, \nwhich should not change the estimates of label priors $\\tilde \\pi_j$\ngiven by $\\tilde{\\numinstances}_j/\\numinstances$ \n(with  $\\tilde \\numinstances_j$ being the number of positive instances of label $j$ \nin the observed, noisy training set), \nthe \\xmlcpropensitymodel{} model will estimate propensities to be equal one, \ni.e., no missing labels, as the amount of data goes to infinity:\n\\begin{multline}\n    \\!\\!\\!\\!\\!\\lim_{\\numinstances \\rightarrow \\infty} \\phijain(\\tilde \\pi_j,  n) = \\frac{1}{1 + (b + 1)^a  \\lim_{\\numinstances \\rightarrow \\infty} (\\log \\numinstances - 1)e^{-a \\log (\\tilde\\pi_j \\numinstances + b)}} \\\\\n    = \\frac{1}{1 + (b + 1)^a \\lim_{\\numinstances \\rightarrow \\infty} (\\log \\numinstances) (\\tilde\\pi_j \\numinstances)^{-a}} = 1.\n\\end{multline}\nThis means that we cannot interpret $a$ and $b$ as parameters of some\nunderlying (unknown) process that describes the labeling process. As we cannot\neven have fixed $a$ and $b$ when the data come from the same process, this\nvery much calls into question the approach of using values for $a$ and $b$\nacross datasets as is current practice.\n\n\\paragraph{Estimation process}\nSetting aside structural concerns about \\eqref{eq:jain-prop-model}, the\nestimation of the parameters $a$ and $b$ still remains an issue. First, by\nidentifying missing labels based on meta-data as described in\nSection~\\ref{subsec:empirical-model}, only an upper-bound on the propensity is\nestimated, since labels may also be missing in other ways. \n%\nFor example, we tried to reproduce the procedure of propensity estimation on the Wikipedia dataset. \nWe have found that only around \\num{40000} out of \\num{500000}\nlabels meet the criteria of the sufficient number of descendants selected by the authors,\nand around \\num{300000} labels are without descendants, \nso they would never be considered missing by this protocol. \n\nFurther, one might argue that in cases in which missing labels can be\nidentified by some side-channel information such as label hierarchies, then\none can directly impute these missing labels and need not worry about training\nwith missing labels. \n\n\\paragraph{Propensity as a function of frequency}\nThis still leaves the question of whether such estimates are sensible.\nEven though there is clearly a trend that labels within a given range of\nfrequency have -- on average -- a certain propensity, for each individual label\nthe actual propensity can fluctuate widely around this mean, as shown in \n\\autoref{fig:wikipedia-ps-est} that we obtained following the original procedure for estimating propensities.\n\n\\paragraph{Reproducibility} \n\nThe description of the process of propensity estimation in~\\citep{Jain_et_al_2016} \nis rather sparse on details. \nWhile meta-data for Wikipedia is easily obtainable, \nit is not clear what is the source of ancillary information \nthat has been used for the Amazon dataset. \nAdditionally, depending on the preprocessing steps \nand criteria, such as the number of descendants in the label hierarchy, \none can achieve very different estimates of parameters $a$ and $b$.\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Implausible results and normalization",
                    "content": "\n\\label{subsec:implausible_results}\n\nDespite being unable to verify the correctness \nof the assumptions and the \\xmlcpropensitymodel{} model \nwithout actual clean ground truth data, \nwe are still able to show \nthat the approach of~\\citet{Jain_et_al_2016} leads to implausible results.\nFor example, $\\psprecision[k]$, \nas an unbiased estimate of $\\precision[k]$ on the ground-truth data,\nshould be bounded between zero and one.\nHowever, when calculating this measure for a real classifier, \nthe result may exceed this range substantially. \nOf course, for an individual instance or a small subset of them, \nthe unbiased estimate does not need to fall into that range, \nbut a large deviation from the true value becomes exceedingly unlikely \nwhen averaging over the entire dataset. \n\nTo circumvent this issue, \\citet{Jain_et_al_2016} suggest \nto report a normalized version of $\\psprecision[k]$, \nalso calling this measure ``propensity-scored precision''.\nThe normalization is realized by dividing the metrics value \nby the largest possible value that any prediction could have achieved on that data:\n\\begin{equation}\n\\label{eq:ps-loss-norm}\n\\textrm{Norm}\\psprecision[k] = \\frac{\\sum_{i=1}^{n} \\psprecision[k](\\solabels_i, \\spreds_i)}{\\sum_{i=1}^{n} \\max_{\\mathbf{z}} \\psprecision[k](\\solabels_i, \\mathbf{z})} \\,.\n\\end{equation}\nThe normalization introduces a factor \nthat is constant over the entire dataset, \nand thus does not influence model selection. \nHowever, it removes the interpretation of the received value \nas an unbiased estimate of the metric on clean data, \nand it hides the model misspecification.\nTable~\\ref{tab:norm-unnorm-psp-at-k} reports the values of both variants of $\\psprecision[k]$,\nshowing how severe this issue is. \n\n\n\n"
                },
                "subsection 3.5": {
                    "name": "The current use of propensity metrics",
                    "content": "\n\\label{subseq:usage-problems}\n\nIt seems that the current use of propensity metrics mixes up, \nin a not entirely clear way, two different issues, missing and tail labels.\nAs mentioned in Section~\\ref{subsec:prop-long-tail},\nthese metrics might be used for the purpose of giving \nmore weight to tail labels. \nIn this case, the normalization step seems to be a valid procedure. \nHowever, a propensity metric loses its original interpretation, \nand it is just one way of accounting for tail labels, \nwithout any concrete justification. \nFor this use case, it would be preferable to have a metric \nthat treats tail labels in a principled way. \nAs a first step towards that goal, \nSection~\\ref{sec:task-losses-for-long-tails} provides \nsome discussion on alternative task losses.\n\nOnly in the interpretation as a tail-performance promoting loss, \nit does make sense to speak of a trade-off in performance \nbetween vanilla and propensity-scored metrics, as these are conceptually different. \nIn the missing-labels interpretation, taking the propensities into account is not\ncalculating a different conceptual metric, but instead, the \\emph{correct} way\nof calculating the unweighted, but the true performance of a classifier.\n%\nOf course, in XMLC, both interpretations can be combined, \ni.e., one would like to have a task loss that is adapted to tail labels, \nbut calculate it in a way that takes missing labels into account. \nThe closest to this in the literature is \\citep{Qaraei_et_al_2021}, \nwhere training uses a loss that combines unbiased estimates and class-rebalancing, \nbut still, evaluation is performed using vanilla and propensity-scored metrics, \ninstead of a propensity-scored variant of a tail-weighted metric.\n\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Recipes to follow",
                "content": "\n\\label{sec:recipes-to-follow}\n\nIn this section, we present several recipes on how to conduct \nresearch on missing and tail labels in XMLC.\nWe start our discussion with a recommendation of using an additional dataset\nwhich is either unbiased or its bias is under control.\nWe then discuss several alternatives for the \\xmlcpropensitymodel{} model,\nshow how to fit the models to unbiased data, and how to compare them empirically.\nNext, we introduce methods that jointly train the prediction and the propensity model. \nDespite our critical remarks, we consider in this section only propensities being constant for each label.\nFinally, we discuss performance metrics for long-tail labels \nthat might be a better choice than propensity-scored metrics.%\n\\footnote{Repository with the code to reproduce all experiments: \\url{https://github.com/mwydmuch/missing-labels-long-tails-and-propensities-in-xmlc}}\n\n",
                "subsection 4.1": {
                    "name": "Bias-controlled validation and test sets",
                    "content": "\n\\label{sec:unbiased-sets}\n\nIf indeed our training data are biased by missing labels, \nthe best way to test, validate, and estimate the propensity model \nis to use unbiased data or data with controlled bias.\nIn the latter case, the bias is controlled in such a way that \nunbiased estimates can be easily computed.\nSuch data are definitely costly to get, but even a small set can be beneficial,\nhelping in selecting the right model to be used in production and in estimating its real performance. \nThis is a standard approach used in recommendation systems~\\citep{Saito_et_al_2020,Yang_et_al_2018} \nand search engines~\\citep{Joachims_et_al_2018}. \nEven if, in a given application, it is not possible to obtain an unbiased data due to some constraints,\nwe should investigate algorithms on benchmarks with unbiased or bias-controlled sets associated. \nWithout this investigation, it is hard to verify which methods are indeed working and which are misleading. \nIn many real-world applications, the final evaluation of the model \nis performed in A/B tests on real, unbiased data. \nWe should avoid situations in which the results of A/B tests are against \nour expectations coming from offline experiments.\n\nTo follow the above recommendation, \nwe have prepared several datasets of different types,\nwhich can be used for experimentation with missing labels in XMLC. \nThe first type contains fully synthetic datasets. \nThe second one is a modification of the standard XMLC benchmarks. \nThe final dataset is a variant of the Yahoo R3 dataset,\\footnote{\\url{https://webscope.sandbox.yahoo.com/}}\ntransformed from a recommendation problem to multi-label classification. \nStatistics of these datasets along with additional information are given in Appendix.\n\nThe synthetic datasets are generated in a similar way as in~\\citep{Jimena_et_al_2014_ml_datagen}.\nThey are parameterized by the number $m$ of labels. \nIn the experiments reported below, we use $m=100$, which is not \\textit{very} extreme\nbut suffices for investigating the propensity models.\nEach label $j$ is represented by a $d$-dimensional hyper-ball $S_j$,\nwhose radius and center are generated randomly.\nAll those hyper-balls lay in a feature space being itself a $d$-dimensional hyper-ball $S$,\nbig enough to contain all hyper-spheres $S_j$.\nInstances are uniformly generated in $S$ \nand each instance $\\sinstance$ is associated with labels \nwhose hyper-balls contain $\\sinstance$, i.e., $\\relevantlabels{\\sinstance} = \\set{j \\in [\\numlabels]: \\sinstance \\in S_j}$.\nThe popularity (or priors) of the labels is directly determined by the radius of $S_j$.\nWe separately generate training, validation, and test sets from the above model.\nWe then apply a propensity model of choice to the training set to generate missing labels.\nFor some experiments, we also generate missing labels in validation and test sets.\n\nFor the original XMLC benchmark datasets, we assume that there are no missing labels. \nWe then merge the original train and test sets, \nand take labels having at least $s$ positive instances.\nWe perform this step to select labels for which one can apply the noise models\nwithout removing all positive labels. \nWe then split the data again into training and test sets,\nand apply, similarly as above, a propensity model of choice to the training set to generate missing labels. \nFor some experiments, we extract a validation set from the test set.\n\nIn the original Yahoo R3 dataset, records are organized in a format of user-item ratings, \nand each record contains a user ID, an item ID, and the user's rating for the item (from 1 to 5). \nThe training set contains over 300K ratings from 15.4K users to 1K items. \nThis set is biased as users select items from a limited list of options recommended by some algorithm.\nThe bias-controlled test set is obtained by collecting ratings from a subset of 5.4K users \nto rate $r = 10$ randomly selected items. \nTo create a multi-label dataset we treat each item as a label ($\\numlabels=1K$).\nWe consider ratings greater or equal to 4 as positive feedback and others as irrelevant. \nWe take users unique to the original training set (10K of users) to create a biased multi-label training set. \nFor each user, we randomly split positive feedback into equal halves. \nWe take the first half as features $\\sinstance$ and the later half as labels $\\slabels$. \nNext, we use users present in both the original training and test set to create a test set with a controlled bias. \nWe again randomly select half of the positive feedbacks from the training set as features $\\sinstance$ (to keep the same distribution of features) \nand all positive feedback from the test set as labels $\\slabels$.\nWe can also extract a validation set from the test set,\nusually consisting of a half of users from the test set. \n\nFor a dataset created in such a way, \nwe can calculate estimates of training-set propensities $\\trainprop_j$ as:\n\\begin{equation}\n    \\label{eq:unbiased-propensities}\n    \\estimtrainprop_j = \\phi_{\\text{direct}} = \\trainlabelprior_j \\ctprop_j \\left (\\vallabelprior_j \\right )^{-1}\n    \\,,\n\\end{equation}\nwhere $\\trainlabelprior_j$ and $\\vallabelprior_j$ are, respectively, \ntraining- and validation-set estimates of the prior probability of label $j$, \nand $\\ctprop_j$ is the controlled propensity used for the validation and test set.\nTo estimate the label priors, one can use relative frequencies of labels in training and validation sets.\nFor $\\ctprop_j$ we use a ratio of $r$ labels used for labelling to all $m$ labels, i.e., $\\ctprop_j = \\frac{r}{\\numlabels} = 0.01$\nfor the Yahoo R3 dataset. \n\n"
                },
                "subsection 4.2": {
                    "name": "Alternative propensity models",
                    "content": "\n\\label{sec:alternative-propensity-models}\n\nThe \\xmlcpropensitymodel{} propensity model~(\\ref{eq:jain-prop-model}) is not\nthe only one to consider. In fact, many different forms have been introduced\nin other domains~\\citep{Saito_et_al_2020,Yang_et_al_2018,Joachims_et_al_2018}. \nWe express the propensity models as functions of observed label priors\n$\\olabelprior_j \\coloneqq \\probability[\\rolabel_j = 1]$, without direct relation to $n$, \nwhich by construction avoids convergence issues discussed in Section~\\ref{subsec:shortcomings}.%\n\\footnote{%\nThe simplest estimate of the priors are relative frequencies of labels, i.e.,\n$\\hnoisylabelprior_j = \\ocount_j/\\numinstances$. As we deal with many very\nsparse labels, we should rather use more robust estimates, for example,\n$\\hnoisylabelprior_j = (\\ocount_j + \\alpha)/(\\numinstances + \\alpha)$.%\n}\n\nA propensity model used frequently in recommendation systems is given by the following power-law formulation:\n\\begin{equation}\n\\propensity_j = \\phipower(\\olabelprior_j;\\beta,\\gamma) \\coloneqq \\left (\\beta \\olabelprior_j \\right )^\\gamma\\,.\n\\end{equation}\nWith $\\beta = \\max_j \\ocount_j/\\numinstances$ we receive a model used, \nfor example, in~\\citep{Yang_et_al_2018, Saito_et_al_2020},\nwhile for $\\beta = \\gamma= 1$ we get a very simple model which might be used,\nif estimation of the parameters is infeasible due to lack of unbiased or bias-controlled data.\nAnother solution could be to use the generalized logistic function, also called Richard's curve~\\citep{Richards_1959_curve},\nwhich is very flexible and its shape resembles~(\\ref{eq:jain-prop-model}):\n\\begin{equation}\n\\propensity_j = \\phirichards(\\olabelprior_j; c,d,e,f,g,h) \\coloneqq c+\\frac{d - c}{\\left (e+ f\\exp(-g \\olabelprior_j) \\right )^{1/h} }\\,.\n\\end{equation}\n\n\nThe parameters of the models can be either set up according to a domain\nknowledge or fit using an additional unbiased or bias-controlled dataset. In\nthe latter case, one can use standard non-linear optimization\nmethods~\\citep{Bazaraa_nonlinear_programming}. We fit the models to inverse\npropensities, minimizing squared errors $\\|\\estimprop_j^{-1} -\n\\phi(\\olabelprior_j)^{-1}\\|^2$ using the Levenberg-Marquardt\nmethod~\\citep{More_and_Watson_least_squares}. The errors are reported in the Appendix.\n\n\n\n%\n\\autoref{fig:yahoo-r3-ps} illustrates the results of fitting the different propensity models for the\nYahoo R3 dataset. The actual training-set propensities have been obtained\nusing~(\\ref{eq:unbiased-propensities}). The plot clearly shows that the\n\\xmlcpropensitymodel{} model with $a=0.55$ and $b=1.5$, suggested as default\nvalues, is not a good fit to the actual propensities. The same model, but with\n$a$ and $b$ fitted to the data gives a degenerated solution because many values are out of codomain of~(\\ref{eq:jain-prop-model}) when $n$ is that small. \nOn the other hand, $\\phipower$ and $\\phirichards$ seem to give a good fit, but still\nactual propensities are widely spread, suggesting that a model solely\ndepending on label priors might not be the best choice.\n\nWe have also trained prediction models using the above propensities to see \nwhether they help in improving (actual) precision$@k$ on the unbiased test set.\nWe use the one-vs-all approach in which probabilistic model $f_j(\\sinstance)$, for label $j$, \nis obtained by minimizing the unbiased variant of logistic loss~\\citep{Saito_et_al_2020, Qaraei_et_al_2021}:\n\\begin{equation}\n\\label{eq:un-log-loss}\n\\lossfn(\\solabel_j, \\propensity_j, f_j(\\sinstance)) =\n    -\\frac{\\solabel_j}{\\propensity_j} \\log(f_j(\\sinstance)) - \\left(1 -\\frac{\\solabel_j}{\\propensity_j}\\right) \\log(1 -  f_j(\\sinstance))\\,.\n\\end{equation}\n\n\n\n%\nThe results are given in \\autoref{tab:results-yahoo-r3}. \nAs a baseline we also use a vanilla logistic loss which corresponds to $\\phi_{1}(\\olabelprior_j) = 1$.\nWe can observe that all propensities models, \nexcept the degenerated variant of \\xmlcpropensitymodel{},\ngive slightly better results than the baseline, with $\\phirichards$ being clearly the best among them. \nOn the other hand, $\\phi_{\\text{direct}}$, \nwhich directly estimates propensity for each label using~(\\ref{eq:unbiased-propensities}),\nsignificantly improves the performance (particularly for $\\precision[3]$ and $\\precision[5]$).\nNevertheless, $\\phi_{\\text{direct}}$ can only work well if the unbiased or bias-controlled data are substantial. \nIf this is not the case, one might need to use a parametric model, \nbut the above results suggest \nthat the dependence on label priors might not be sufficient.\n\n\n%\n\\definecolor{color-good}{HTML}{28a928}\n\\definecolor{color-wrong}{HTML}{df2021}\n\n%\nFinally, we illustrate a problem of propensity mismatch on synthetic and modified benchmark datasets.\nWe introduce noise to training data according to either the $\\phijain$ or $\\phipower$ model,\ntrain prediction functions using both propensities models, and report actual precision$@k$, \ncomputed on the unbiased test set, along with propensity-based precision$@k$ for the same $\\phijain$ or $\\phipower$ model, \ncomputed on the biased test set (i.e., with the noise model applied).\nThe results in \\autoref{tab:cross-propensities-p@k} show that relying on propensity-based metrics can be misleading.\nAs it should be expected, in the majority of cases, models compatible with the metric are obtaining the best performance.\nHowever, selecting a model based on a chosen propensity-based metric can be wrong as the actual precision \nmight be driven by a completely different propensity model. \n\n\n"
                },
                "subsection 4.3": {
                    "name": "Propensity estimation via joint learning",
                    "content": "\n\\label{sec:ps-joint-learning}\n\nTo minimize an unbiased loss function, such as the unbiased logistic\nloss~(\\ref{eq:un-log-loss}), one needs to know propensities in advance. \nHowever, estimating them might be difficult in practice. \nAs demonstrated above, the use of inaccurate estimates can lead to results being far away from the optimal ones.\n\nTherefore it would be useful if propensities could be estimated directly from a biased training set. \nUnfortunately, this is an ill-defined problem because the absence of a label can be explained by \neither a small conditional probability of the label or a low propensity or both. \nThe additional assumption needed for the propensity to be identifiable were studied before, \nin the areas of learning from positive and unlabeled data~\\citep{elkan_learning_2008}, \nand novelty detection~\\citep{Blanchard_et_al_2010_SSND}.\nThe overview of the possible assumption is given by~\\citet{Bekker_and_Davis_2020_PAU_Survey}, \nwhere the weakest of the assumptions requires \nthat the true distribution of negative samples for a given label \ncannot contain the positive distribution~\\citep{Blanchard_et_al_2010_SSND}. \nIn these areas and under compatible assumptions, many methods for estimating the error ratio or labels priors, \nboth directly related to propensity estimates, were proposed~\\citep{Bekker_and_Davis_2020_PAU_Survey}.\nRecently, \\citep{Zhu_et_al_2020} and~\\citep{Teisseyre_et_al_2020} have introduced methods for estimating the unbiased conditional label probabilities and propensities jointly on the biased training set.  \nWe refer to such methods as Propensity Estimation via Joint Learning (PEJL).\n\nLet us briefly describe the method of \\citet{Teisseyre_et_al_2020} \n(cf. Appendix for description of the method of \\citet{Zhu_et_al_2020}).\nIt uses the fact \nthat minimization of logistic loss leads to estimation of the posterior probability. \nTherefore, we can define the loss in the following way:\n\\begin{equation}\n\\label{eq:log-loss-ps-plug}\n\\lossfn(\\solabel_j, \\propensity_j, f_j(\\sinstance)) \\!=\\!\n    -\\solabel_j \\log(\\propensity_j f_j(\\sinstance)) \\!-\\! \\left(1 \\!-\\!\\solabel_j\\right) \\log(1\\!-\\!\\propensity_j f_j(\\sinstance))\\,,\n\\end{equation}\nwhere $\\propensity_j f_j(\\sinstance)$ can be seen as an estimate of the actual, \nground-truth, conditional probability $\\eta_j(\\sinstance)$,\nwith $\\propensity_j$ being the propensity and $f_j(\\sinstance)$ the estimate of the observed conditional probability,\nanalogously to~(\\ref{eqn:prob_adjustment}). \nThis function can be optimized not only with respect to $f_j(\\sinstance)$, but also to $\\propensity_j$. The outline of the alternative method of \\citet{Zhu_et_al_2020} can be found in the appendix.\n\nWe evaluate this approach on Yahoo R3 dataset. The estimated values of $\\propensity_j$ are plotted on the \\autoref{fig:yahoo-r3-ps} and the last row of \\autoref{tab:results-yahoo-r3} presents the promising results of this approach. While the obtained estimates are overestimated, they capture the true trend. The predictive performance also looks promising, being only slightly worst than the best propensity model $\\phirichards$. This is indeed encouraging as this method does not have access to the unbiased or bias-controlled data.\n\\autoref{fig:yahoo-r3-ps} also plots the obtained propensities for each label.\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Task losses for long-tails",
                    "content": "\n\\label{sec:task-losses-for-long-tails}\n\nIt seems that \\citet{Jain_et_al_2016} have introduced the propensity-scored losses \nrather to \"promote\" long-tail labels than to deal with missing labels. \nAs such, the propensities can be seen as a kind of weighing approach that \ngives higher importance to less popular labels. \nUnfortunately, it is not clear why the weighing scheme used in~\\citep{Jain_et_al_2016} should be preferred over \nother ones. \nMoreover, a weighing scheme does not have to be interpreted in terms of propensities.\n%\nLet us consider a weighted variant of $\\precision[k]$:\n\\begin{equation}\n\\precision[k](\\slabels, \\spreds) = k^{-1} \\sum_{\\mathclap{j \\in \\topk[k](\\spreds)}} w_j \\solabel_j \\,.\n\\end{equation}\nThis boils down to $\\psprecision[k]$ when $w_j = \\frac{1}{p_j}$ which also implies $w_j \\ge 1$.\nBut one can use any weights that would represent the importance or gain of labels.\nIn such a case, the weighted $\\precision[k]$ has a natural interpretation of being an unbiased estimate of the expected gain.\nIf tail labels are of our interest, then they should get higher weights, \nbut actual values are rather domain-specific\nwithout a direct relation to propensities.\n\nTo finalize our discussion, let us mention several other task losses that can be used \nas metrics that pay special attention to long-tail labels.\nThe macro $F_\\beta$-measure defined as:\n\\begin{equation}\n\\fmacro[\\beta] \\left (\\set{\\slabels_i, \\spreds_i}_1^\\numinstances \\right ) = \n\\frac{1}{\\numlabels} \\sum_j \\frac{(1+\\beta^2) \\sum_i \\sblabel_{ij} \\spred_{ij}} { \\beta^2 \\sum_i \\sblabel_{ij} + \\sum_i \\spred_{ij}}\\,,\n\\end{equation}\nputs the same weight to each label and focuses on true positives. \nTherefore, positive predictions on long-tail labels are important to obtain high values on this metric.\nOne can also consider an $@k$ version of this metric, \nin which only top $k$ predictions are taken into account.\n\nAnother interesting metric, originally proposed for search engines~\\citep{Radlinski_et_al_2008},\n%used in search engines~\\citep{Radlinski_et_al_2008}, \nis abandonment$@k$ defined as:\n\\begin{equation}\n\\abandon[k](\\slabels, \\spreds) = \\indicator[\\forall j \\in \\topk[k](\\spreds): \\, \\sblabel_j \\ne 1 ]\\,,\n\\end{equation}\nwhich encounters no error if there is at least one correctly predicted label among the $k$ ones in the predicted set. \nThis untypical formulation enforces diversity in the predicted set. \nAlways predicting the two most popular but correlated, \nlabels might be less beneficial than predicting less popular but also non-overlapping labels.\n\nFinally, we mention the coverage metric, which directly reflects the diversity of correctly predicted labels.\nIt is defined as a fraction of labels with at least one correct positive prediction:\n\\begin{equation}\n\\!\\!\\!\\coverage\\!\\left( \\set{\\slabels_i, \\spreds_i}_1^\\numinstances \\right) = {\\numlabels}^{-1} \\left | \\{ j \\in [\\numlabels] \\!:\\! \\exists i \\in [\\numinstances] \\; \\text{s.t.}\\; \\sblabel_{ij} = \\spred_{ij} = 1 \\} \\right | \\,.\n\\end{equation}\nThis metric has already been suggested in the literature as an alternative~\\citep{Babbar_Scholkopf_2019, Wei_and_Li_2020}.\nIt has also been used in the original paper of~\\citet{Jain_et_al_2016},\nbut only to motivate the propensity-based metrics. \n\n\n\n"
                }
            },
            "section 5": {
                "name": "Conclusions",
                "content": "\n\nDespite our critical comments regarding~\\citep{Jain_et_al_2016}, \nwe still appreciate this contribution.\nIt was the first paper that brought direct attention to the problem of missing and long-tail labels in XMLC.\nThe original theoretical results concerning the propensity model have motivated a lot of research in this direction.\nNevertheless, we believe that missing labels and long-tail labels are rather orthogonal problems \nthat should be solved with different tools.\nObviously, labels gone missing may cause labels to be sparse, \nbut it does not mean that a blind propensity model may solve any of these problems.\n\n\\begin{acks}\nAcademy of Finland grant: Decision No. 347707. Computational experiments have been performed in Poznan Supercomputing and Networking Center.\n\\end{acks}\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{references_short}\n\n\\appendix\n\\pagebreak\n\n\n"
            },
            "section 6": {
                "name": "Label frequency in XMLC datasets",
                "content": "\n\nWe show in \\autoref{fig:labels-freqs} a log-log plot of the distribution of label frequencies\nin popular benchmark datasets from the XMLC repository~\\citep{Bhatia_et_al_2016}.\nAs also noted in other works~\\citep{Bhatia_et_al_2015, Babbar_Scholkopf_2017},\nthe label frequencies are characterized by the long-tail. \n\n\n\n"
            },
            "section 7": {
                "name": "Statistics of missing-label datasets",
                "content": "\n\nWe present in~\\autoref{tab:new-dataset-stats} the statistics of datasets created to experiment with propensities models and missing labels. The description of these datasets is given in Section~\\ref{sec:recipes-to-follow}. Because the process of generating the biased training sets contains randomness, for the mean number of labels per example, we report the average value from all generated variants of the datasets.\n\n\n\n\n"
            },
            "section 8": {
                "name": "Estimation errors of propensity models",
                "content": "\n\n\\autoref{tab:est-error} presents the mean squared errors (MSE) of inverse propensity estimates ($\\|\\estimprop_j^{-1} - \\phi(\\olabelprior_j)^{-1}\\|^2$) on the Yahoo R3 dataset\nobtained by different propensity models described in Section~\\ref{sec:recipes-to-follow}. Models marked as (fit.) have been fitted to the same data the error has been calculated on. \nSince we repeated the experiment with the PEJL model several times, we report the average error.\n\n\n\n\n"
            },
            "section 9": {
                "name": "Details of model training",
                "content": "\n\nWe describe here the implementation and training procedures of classifiers used in the experiments in Section~\\ref{sec:recipes-to-follow}.\n%\nFor all the experiments, except the one with the PEJL model described in Section~\\ref{sec:ps-joint-learning}, \nwe train a linear model using the unbiased binary-cross entropy loss (\\ref{eq:un-log-loss}) \nwith propensities coming from different models $\\phi$. \nThe weights of the linear models are initialized from the uniform distribution $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$, \nwhere $k = 1/d$ with d being the number of features.\n\nIn the case of the PEJL approach, the same linear architecture is used but trained using~(\\ref{eq:log-loss-ps-plug}). \nTo assure that estimated values of $p_j$ are in $[0, 1]$, they are modeled as a sigmoid transformation ($\\sigma(\\cdot)$) of $p'_j$ parameters (one per label). Parameters $p'_j$ are initialized from the uniform distribution $\\mathcal{U}(-e, e)$.\n\nFor each experiment, 10\\% of the biased training set serves as a validation set for the selection of hyperparameters and early stopping. All methods are implemented using PyTorch~\\citepappendix{pytorch}.\nOptimization is performed with the Adam optimizer~\\citepappendix{Kingma_et_al_2015}. \nOnly two hyperparameters are tuned on the biased validation test, the learning rate (from set \\{0.005, 0.01, 0.05, 0.1\\}) and the weight decay (from set \\{0, 1e-8, 1e-7, 1e-6\\}). Experiments have been performed on a single machine with Intel Xeon Gold 5115 and NVIDIA V100 16GB.\n\n\n"
            },
            "section 10": {
                "name": "An alternative PEJL method",
                "content": "\n\nBelow we briefly describe the main idea behind the method of~\\citet{Zhu_et_al_2020},\nwhich can serve as an alternative to the approach of \\citet{Teisseyre_et_al_2020}\ndescribe in Section~\\ref{sec:ps-joint-learning}.\n\nLet $\\masks \\in \\{0, 1\\}^\\numlabels$ be a mask random variable \nthat expresses relation between $\\rlabels$ and $\\rolabels$, \ni.e., $\\rolabels = \\masks \\odot \\rlabels$.\nWe then have $\\probability[\\rolabel_j = 1 | \\bx] = \\probability[\\mask_j = 1] \\cdot \\probability[\\rblabel = 1 | \\bx]$ \nand $\\probability[\\mask_j = 1] = p_j$ and $\\probability[\\rblabel = 1 | \\bx] = \\eta_j(\\bx)$~\\citep{Schultheis_Babbar_2021}. \n%\nBecause $\\mask_j$ is a Bernoulli variable as well as $\\rblabel_j$ and $\\rolabel_j$, \nwe end up with two models, the first one for $\\rblabel_j$ with known $\\propensity_j$, and the second for $\\mask_j$ with known $\\eta_j(\\bx)$.\nThe first one can be obtained by minimizing~(\\ref{eq:un-log-loss}). \nFor the second one, a parametric model $\\phi(\\theta_j)$ for $p_j$ can be learned by minimizing the following logistic loss:\n%\n\\begin{equation}\n\\label{eq:ps-un-log-loss}\n\\!\\lossfn(\\solabel_j, \\eta_j(\\bx), \\phi(\\theta_j)) \\!=\\! -\\frac{\\solabel_j}{\\eta_j(\\bx)} \\log(\\phi(\\theta_j)) \n    - \\left(1\\!-\\!\\frac{\\solabel_j}{\\eta_j(\\bx)}\\right) \\log(1 - \\phi(\\theta_j))\\,.\n\\end{equation}\n%\nWe can learn $f_j(\\bx)$ and $\\phi(\\theta_j)$ jointly by replacing $\\eta_j(\\bx)$ by $f_j(\\bx)$ in (\\ref{eq:ps-un-log-loss}),\nand $p_j$ by $\\phi(\\theta_j)$ in (\\ref{eq:un-log-loss}).\nSince both $f_j(\\bx)$ and $\\phi(\\theta_j)$ can be updated on a single example $\\bx$, to avoid estimation-training overlap problem, \nthe training data is split into two parts, and the training is performed with $\\phi(\\theta_j)$ fixed on one part and with $f_j(\\bx)$ fixed on the second one.\n\n"
            },
            "section 11": {
                "name": "Additional assumptions for complex metrics",
                "content": "\n\nHere we show an example which demonstrates that for a non-decomposable metric, additional assumptions on the\nprocess of labels going missing are required.\n\nConsider a setting with two class labels. Let the label-wise propensities for both labels be $\\propensity_1 = \\propensity_2 = 0.5$.\nThe desired unbiased loss function of $\\ell$ can be parametrized for each prediction $\\spreds$ by four real numbers, $v_{\\solabels} \\coloneqq\n\\tilde{\\ell}(\\solabels, \\spreds)$. We can consider two different scenarios. First, both labels always go missing at the\nsame time, second they go missing complementarily, corresponding to the probability distributions $\\probability$ and\n$\\probability^\\prime$. As the unbiasedness needs to hold for all potential distributions of true labels, it needs to hold in particular in the four cases in which the true label distribution is concentrated on a single point. By explicitly calculating expectations through reading off the probabilities from \\autoref{tab:correlated-missing}, we can state the unbiasedness\nrequirement which is $\\expectation[\\tilde{\\ell}(\\rolabels, \\rpreds)] = \\expectation[\\ell(\\rlabels, \\rpreds)] $:\n\\begin{equation}\n\\begin{aligned}\n\\ell((1,1), \\spreds) &= 0.5 (v_{11} + v_{00}) = 0.5(v_{10} + v_{01})\\,,\\\\\n  \\ell((1,0), \\spreds) &= 0.5 v_{00} + 0.5 v_{10}\\,,\\\\\n  \\ell((0,1), \\spreds) &= 0.5 v_{00} + 0.5 v_{01}\\,,\\\\\n  \\ell((0,0), \\spreds) &= v_{00}\\,.\n\\end{aligned}\n\\end{equation}\nThese are five linear equations with only four variables, so in general, there is no solution. If we additionally assume that\nthe labels go missing independently from each other, then the marginal propensities uniquely determine the full distribution.\nIn that case, unbiased estimates can be derived for general loss functions \\citep{Schultheis_Babbar_2021}.\n\n\n\n\\bibliographystyleappendix{ACM-Reference-Format}\n\\balance\n\\bibliographyappendix{references_short}\n\n"
            }
        },
        "tables": {
            "table:dataset-stats": "\\begin{table}\n\\small\n\\caption{Imbalance characteristics of typical XMLC datasets.}\n\\label{table:dataset-stats}\n\\pgfplotstabletypeset[\nevery head row/.style={\n  before row={\\toprule},\n  after row={\\midrule}},\n  columns={dataset,instances,head imbalance,class imbalance,80percent},\nevery last row/.style={after row=\\bottomrule},\ncolumns/dataset/.style={string type, column type={l|}, column name={Dataset}},\n%columns/obesity/.style={column name={OB}},\ncolumns/head imbalance/.style={column type = {r}, column name={min IR},zerofill,precision=1},\ncolumns/instances/.style={column type = {r}, column name={Instances}, precision=2, sci,zerofill},\ncolumns/class imbalance/.style={column type = {r}, column name={ILIR}, sci,zerofill, precision=2},\ncolumns/80percent/.style={column type = {r}, column name={Pos-80\\%},zerofill,precision=1}\n]{data/datasets.txt}\n\\end{table}",
            "tab:psmetrics": "\\begin{table}[h!]\n    \\caption{Definitions of popular XMLC performance metrics and their unbiased estimates on missing labels.}\n    \\label{tab:psmetrics}\n    \\begin{center}\n    \\resizebox{\\linewidth}{!}{\n        \\begin{tabular}{l|c|c}\n             \\toprule\n             Measure & Definition & Unbiased estimate \\\\\n             \\midrule\n             $\\precision[k](\\slabels, \\spreds)$  & \n             $k^{-1} \\sum_{j \\in \\topk[k](\\spreds)} \\sblabel_j$ \n             & $k^{-1} \\sum_{j \\in \\topk[k](\\spreds)} \\solabel_j / \\propensity_j, \\label{eq:def:psp}$ \\\\\n             \\midrule\n             $\\recall[k](\\slabels, \\spreds) $ \n             & $\\|\\slabels\\|_1^{-1}\\sum_{{j \\in \\topk[k](\\spreds)}} \\sblabel_j$ \n             & $\\|\\slabels\\|_1^{-1}\\sum_{j \\in \\topk[k](\\spreds)} \\solabel_j/ \\propensity_j$ \\\\\n             \\midrule\n             $\\ndcg[k] (\\slabels, \\spreds) $ \n             & $\\frac{\\sum_{{j \\in \\topk[k](\\spreds)}} \\frac{\\sblabel_j}{\\log(\\rankof{j}(\\spreds)+1)}}{\\sum_{j=1}^{k} \\frac{1}{\\log(j+1)}}$\n             & $\\frac{\\sum_{j \\in \\topk[k](\\spreds)} \\frac{\\solabel_j}{\\propensity_j \\log(\\rankof{j}(\\spreds)+1)}}{\n    \\sum_{j=1}^{k} \\frac{1}{\\log(j\\!+\\!1)}}$\\\\\n             \\bottomrule\n        \\end{tabular}\n    }\\end{center}\n\\end{table}",
            "tab:norm-unnorm-psp-at-k": "\\begin{table}[h]\n    \\caption{Normalized and unnormalized propensity-scored precision of PfastreXML~\\citep{Jain_et_al_2016}, when using the \\xmlcpropensitymodel{} model, with $a=0.5$, $b=0.4$ for WikiLSHTC-325K and $a=0.6$, $b=2.6$ for Amazon-670K.} \n    \\label{tab:norm-unnorm-psp-at-k}\n\n    \\centering\n    \\small\n%\\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{c|rrr|rrr}\n        \\toprule\n        & \\multicolumn{3}{c|}{WikiLSTHC-325K} & \\multicolumn{3}{c}{Amazon-670K} \\\\\n        \\midrule\n        PSP(\\%) & $@1$ & $@3$ & $@5$ & $@1$ & $@3$ & $@5$ \\\\\n        \\midrule\n             Normalized & 31.16 & 31.80 & 33.35 & 29.93 & 31.26 & 32.80 \\\\\n             Unnormalized & 196.96 & 118.54 & 85.28 & 326.47 & 282.28 & 250.57 \\\\\n             %Maximum & 632.10 & 372.78 & 255.70 & 1090.77 & 903.01 & 763.93 \\\\\n        \\bottomrule\n    \\end{tabular}\n%}\n\\end{table}",
            "tab:results-yahoo-r3": "\\begin{table}[t]\n\\caption{Actual precision@\\{1,3,5\\} (and their standard errors) on the Yahoo R3 dataset.\nThe best results are marked in bold.\nThe last row presents the results of the PEJL method from Section~\\ref{sec:ps-joint-learning}. \nEach experiment was repeated 25 times.}\n\\label{tab:results-yahoo-r3}\n\\begin{center}\n%\\resizebox{\\linewidth}{!}{\n\\small\n\\begin{tabular}{l|ccc}\n\\toprule\nMethod\n& $\\precision[1]$ (\\%)\n& $\\precision[3]$ (\\%)\n& $\\precision[5]$ (\\%)\n\\\\\n\\midrule\n$\\phi_{\\mathbf{1}}$ & 60.83 $\\pm$ 2.02 & 54.30 $\\pm$ 0.89 & 51.20 $\\pm$ 0.75 \\\\\n$\\phijpv$ & 66.03 $\\pm$ 1.70 & 56.17 $\\pm$ 0.91 & 52.02 $\\pm$ 0.70 \\\\\n$\\phijpv$ (fit.) & 48.58 $\\pm$ 2.13 & 43.26 $\\pm$ 0.86 & 40.47 $\\pm$ 0.68 \\\\\n$\\phipower$ (fit.) & 63.53 $\\pm$ 1.90 & 54.53 $\\pm$ 0.97 & 50.50 $\\pm$ 0.73 \\\\\n$\\phirichards$ (fit.) & 71.23 $\\pm$ 1.71 & 61.02 $\\pm$ 0.77 & 54.03 $\\pm$ 0.49 \\\\\n$\\phi_{\\text{direct}}$ & \\textbf{73.72 $\\pm$ 2.26} & \\textbf{66.14 $\\pm$ 0.97} & \\textbf{59.59 $\\pm$ 0.74} \\\\\n\\midrule\n$\\phi_{\\text{PEJL}}$ & 68.09 $\\pm$ 1.53 & 58.15 $\\pm$ 1.04 & 53.62 $\\pm$ 0.72\\\\\n\\bottomrule\n\\end{tabular}\n%}\n\\end{center}\n\\end{table}",
            "tab:cross-propensities-p@k": "\\begin{table}[b]\n\\caption{Mismatch of propensity models: actual P@$\\{1,3,5\\}$ (computed on unbiased test set)\nand PSP@$1$ (computed on biased test set) of prediction models trained on data biased by \n$\\phijpv$ or $\\phipower$ models.\nGreen highlights PSP@k compatible with the used propensity model, while red highlights incompatible PSP@k.\nThe best value in each column for a given dataset is marked in bold.\n}\n\\label{tab:cross-propensities-p@k}\n\n\n\\begin{center}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|rrr|rr}\n\\toprule\nDataset\n& Method\n& \\multicolumn{3}{c|}{P@k (\\%)}\n& \\multicolumn{2}{c}{PSP@k(\\%)}\n\\\\\n&\n& @1\n& @3\n& @5\n& ($\\phijpv$) @1\n& ($\\phipower$) @1\n\n\\\\\n\\midrule\n\\multirow{2}{*}{Artificial-$\\phijpv$} & $\\phijpv$ & \\textbf{78.49} & \\textbf{69.74} & \\textbf{58.86} & {\\color{color-good}\\textbf{78.66}} & {\\color{color-wrong}102.02} \\\\\n & $\\phipower$ & 70.22 & 64.89 & 56.66 & {\\color{color-good}70.34} & {\\color{color-wrong}\\textbf{133.60}} \\\\\n\\midrule\n\\multirow{2}{*}{Artificial-$\\phipower$} & $\\phijpv$ & 75.71 & 67.8 & 56.69 & {\\color{color-wrong}\\textbf{77.59}} & {\\color{color-good}75.71} \\\\\n & $\\phipower$ & \\textbf{77.79} & \\textbf{69.14} & \\textbf{58.04} & {\\color{color-wrong}72.02} & {\\color{color-good}\\textbf{77.20}} \\\\\n\\midrule\n\\multirow{2}{*}{EUR-Lex-$\\phijpv$} & $\\phijpv$ & 64.75 & 50.64 & 40.57 & {\\color{color-good}64.94} & {\\color{color-wrong}74.75} \\\\\n & $\\phipower$ & \\textbf{66.51} & \\textbf{51.91} & \\textbf{41.50} & {\\color{color-good}\\textbf{66.84}} & {\\color{color-wrong}\\textbf{80.63}} \\\\\n\\midrule\n\\multirow{2}{*}{EUR-Lex-$\\phipower$} & $\\phijpv$ & 54.23 & 41.72 & 32.99 & {\\color{color-wrong}\\textbf{44.33}} & {\\color{color-good}52.19} \\\\\n & $\\phipower$ & \\textbf{55.07} & \\textbf{42.07} & \\textbf{33.04} & {\\color{color-wrong}42.35} & {\\color{color-good}\\textbf{53.08}} \\\\\n\\midrule\n\\multirow{2}{*}{AmazonCat-$\\phijpv$} & $\\phijpv$ & \\textbf{86.32} & 64.58 & 48.53 & {\\color{color-good}\\textbf{86.60}} & {\\color{color-wrong}182.71} \\\\\n & $\\phipower$ & 78.87 & \\textbf{67.78} & \\textbf{53.31} & {\\color{color-good}79.38} & {\\color{color-wrong}\\textbf{389.35}} \\\\\n\\midrule\n\\multirow{2}{*}{AmazonCat-$\\phipower$} & $\\phijpv$ & 67.32 & 40.86 & 29.78 & {\\color{color-wrong}\\textbf{44.94}} & {\\color{color-good}64.03} \\\\\n & $\\phipower$ & \\textbf{82.80} & \\textbf{55.72} & \\textbf{40.33} & {\\color{color-wrong}44.31} & {\\color{color-good}\\textbf{82.22}} \\\\\n\\midrule\n\\multirow{2}{*}{Wiki10-$\\phijpv$} & $\\phijpv$ & \\textbf{82.57} & \\textbf{68.72} & \\textbf{59.18} & {\\color{color-good}\\textbf{82.97}} & {\\color{color-wrong}120.76} \\\\\n & $\\phipower$ & 78.85 & 65.53 & 57.19 & {\\color{color-good}80.01} & {\\color{color-wrong}\\textbf{228.02}} \\\\\n\\midrule\n\\multirow{2}{*}{Wiki10-$\\phipower$} & $\\phijpv$ & \\textbf{80.44} & 54.8 & 47.06 & {\\color{color-wrong}\\textbf{87.34}} & {\\color{color-good}\\textbf{80.43}} \\\\\n & $\\phipower$ & 79.18 & \\textbf{59.89} & \\textbf{49.05} & {\\color{color-wrong}71.98} & {\\color{color-good}78.61} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{center}\n\\end{table}",
            "tab:new-dataset-stats": "\\begin{table}[h!]\n\\caption{Characteristics of datasets used in the experiments. \nWe report the size of the \\textit{biased} train set ($n^\\text{tr.}$) and the size of the  test set ($n^\\text{ts.}$), the total number of labels ($m$), and the mean number of labels per example in the \\textit{biased} train set and the test set. Symbol $*$ denotes  the average value over all generated variants of the dataset, and $\\dagger$ the value corrected by $p_j^c = r / m$, where $r$ is a number of labels sampled for labeling for each datapoint.}\n\\label{tab:new-dataset-stats}\n\\resizebox{\\linewidth}{!}{\n\\pgfplotstabletypeset[\nevery head row/.style={\n  before row={\\toprule},\n  after row={\\midrule}},\n  columns={dataset,n train,n test,m,avg l bs,avg l true},\nevery last row/.style={after row=\\bottomrule},\ncolumns/dataset/.style={string type, column type={l|}, column name={Dataset}},\n%columns/obesity/.style={column name={OB}},\ncolumns/n train/.style={column name={$n^\\text{tr.}$}},\ncolumns/n test/.style={column name={$n^\\text{ts.}$}},\ncolumns/m/.style={column name={$m$}},\ncolumns/avg l bs/.style={string type, column name={$\\expectation[|\\relevantlabels{\\sinstance}|]^\\text{tr.}$}},\ncolumns/avg l true/.style={string type, column name={$\\expectation[|\\relevantlabels{\\sinstance}|]^\\text{ts.}$}}\n]{data/datasets-new.txt}\n}\n\\end{table}",
            "tab:est-error": "\\begin{table}[h!]\n    \\caption{MSE on inverse propensity estimates of different propensity models for the Yahoo-R3 dataset. Symbol $*$ denotes the average value over several runs.}\n    \\label{tab:est-error}\n    \\centering\n    \\small\n    \\begin{tabular}{l|c}\n        \\toprule\n        Model & MSE \\\\\n        \\midrule\n        $\\phi_{\\mathbf{1}}$ & 1663.94\\\\\n        $\\phijpv$ & 1557.04 \\\\\n        $\\phijpv$ (fit.) & 1259.78 \\\\\n        $\\phipower$ (fit.) & 512.94 \\\\\n        $\\phirichards$ (fit.) & 516.85 \\\\\n        \\midrule\n        $\\phi_{\\text{PEJL}}$ & $\\approx1236^*$ \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:correlated-missing": "\\begin{table}[h!] \n  \\caption{Different distributions with the same propensities.}\n  \\label{tab:correlated-missing}\n  \\centering\n  \\small\n  \\begin{tabular}{cc|cc|cc|c}\n    \\toprule\n    $\\rblabel_1$ & $\\rblabel_2$ & $\\rolabel_1$ & $\\rolabel_2$ & $\\probability[\\rolabel_1, \\rolabel_2]$ & \n    $\\probability^\\prime[\\rolabel_1, \\rolabel_2]$ & $\\tilde{\\ell}(\\solabels, \\spreds)$ \\\\ \\midrule\n    1 & 1 & 1 & 1 & 0.5 & 0.0 & $v_{11}$ \\\\\n    1 & 1 & 1 & 0 & 0.0 & 0.5 & $v_{10}$ \\\\\n    1 & 1 & 0 & 1 & 0.0 & 0.5 & $v_{01}$ \\\\\n    1 & 1 & 0 & 0 & 0.5 & 0.0 & $v_{00}$ \\\\ \\midrule\n    1 & 0 & 1 & 0 & 0.5 & 0.5 & $v_{10}$ \\\\\n    1 & 0 & 0 & 0 & 0.5 & 0.5 & $v_{00}$ \\\\\n    \\midrule\n    0 & 1 & 0 & 1 & 0.5 & 0.5 & $v_{01}$ \\\\\n    0 & 1 & 0 & 0 & 0.5 & 0.5 & $v_{00}$ \\\\ \\midrule\n    0 & 0 & 0 & 0 & 1.0 & 1.0 & $v_{00}$ \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:wikipedia-ps-est": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.75\\linewidth]{figures/wikipedia-ps-est+jpv.pdf}\n\\vspace{-0.4cm}\n\\caption{\nReproduced estimates of propensities for Wikipedia-500K dataset using labels hierarchy and propensity function $\\phi_{\\xmlcpropensitymodel}$ with $a=0.5$ and $b=0.4$ as estimated by \\citet{Jain_et_al_2016} for this dataset.}\n\\label{fig:wikipedia-ps-est}\n\\end{figure}",
            "fig:yahoo-r3-ps": "\\begin{figure}[t]\n\\centering\n%\\includegraphics[width=0.75\\linewidth]{figures/yahoo-r3-ps.pdf}\n\\includegraphics[width=0.75\\linewidth]{figures/yahoo-r3-ps+pejl.pdf}\n\\vspace{-0.4cm}\n\\caption{\nPropensity models on the Yahoo R3 dataset. \nAnnotation $\\mathrm{(fit.)}$ \ndenotes that the parameters have been fitted to the actual training-set propensities.\nThe $\\phi_{PEJL}$ model is described in Section~\\ref{sec:ps-joint-learning}.\n}\n\\label{fig:yahoo-r3-ps}\n\n\\end{figure}",
            "fig:labels-freqs": "\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=0.75\\linewidth]{figures/labels-freqs.pdf}\n\\caption{\nLabel frequency in XMLC datasets. \nThe X-axis shows the label rank when sorted by the frequency of positive instances \nand the Y-axis gives the number of the positive instances.}\n\\label{fig:labels-freqs}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\risk_{\\taskloss}[\\hypothesis; \\rinstance, \\rlabels] \\coloneqq \\expectation[\\taskloss(\\rlabels, \\hypothesis(\\rinstance))]\\,,\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\bayescls(\\sinstance) = \\argmin_{\\spreds \\in \\reals^m} \\expectation[\\taskloss(\\rlabels, \\spreds) \\mid \\rinstance = \\sinstance]\\,.\n\\end{equation}",
            "eq:3": "\\begin{equation}\n \\probability[\\rolabels \\preceq \\rlabels \\given \\rinstance] = 1\\,, \\qquad\n \\probability[\\rolabels \\npreceq \\rlabels \\given \\rinstance] = 0\\,,\n\\label{eqn:general_missing_labels_problem}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\tilde{\\eta}_{\\solabels}(\\bx) = \n\\sum_{\\slabels} \np_{\\solabels}(\\slabels, \\sinstance)\n\\eta_{\\slabels}(\\bx)\\,,\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\tilde{\\vec{\\eta}}_{\\calY}(\\sinstance) = \\mathsf{C} \\vec{\\eta}_{\\calY}(\\sinstance) \\,,\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\vec{\\eta}_{\\calY}(\\sinstance) = \\mathsf{C}^{-1}\\tilde{\\vec{\\eta}}_{\\calY}(\\sinstance) \\,,\n\\end{equation}",
            "eq:7": "\\begin{equation}\n \\propensity_j(\\rinstance) \\coloneqq \\probability[\\rolabel_j=1 \\given \\rblabel_j = 1, \\rinstance]\\,.\n\\label{eq:missing-model}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\olabeldist_j(\\sinstance) = \\propensity_j(\\sinstance) \\labeldist_j(\\sinstance)\\,, \\qquad\n\\labeldist_j(\\sinstance) = \\olabeldist_j(\\sinstance) / \\propensity_j(\\sinstance)\\,.\n\\label{eqn:prob_adjustment}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    \\forall \\hypothesis: \\; \\risk_{\\lossfn}[\\hypothesis; \\rinstance, \\rlabels] = \\risk_{\\ubloss}[\\hypothesis; \\rinstance, \\rolabels].\n    \\label{eq:unbiased-risk-property}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\label{eq:jain-prop-model}\n\\!\\!\\! \\propensity_j \\!=\\! \\phijain(\\olabelprior_j; \\numinstances, a, b) \\!\\coloneqq\\! \\frac{1}{1 + (\\log \\numinstances - 1)(b + 1)^a e^{-a \\log (\\numinstances \\olabelprior_j + b)}} \\,,\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    \\expectation[\\lossfn(\\rlabels, \\spreds)] = \\expectation[\\ubloss(\\rolabels, \\spreds)]\\,,\n\\end{equation}",
            "eq:12": "\\begin{equation}\n\\label{eq:ps-loss-norm}\n\\textrm{Norm}\\psprecision[k] = \\frac{\\sum_{i=1}^{n} \\psprecision[k](\\solabels_i, \\spreds_i)}{\\sum_{i=1}^{n} \\max_{\\mathbf{z}} \\psprecision[k](\\solabels_i, \\mathbf{z})} \\,.\n\\end{equation}",
            "eq:13": "\\begin{equation}\n    \\label{eq:unbiased-propensities}\n    \\estimtrainprop_j = \\phi_{\\text{direct}} = \\trainlabelprior_j \\ctprop_j \\left (\\vallabelprior_j \\right )^{-1}\n    \\,,\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\propensity_j = \\phipower(\\olabelprior_j;\\beta,\\gamma) \\coloneqq \\left (\\beta \\olabelprior_j \\right )^\\gamma\\,.\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\propensity_j = \\phirichards(\\olabelprior_j; c,d,e,f,g,h) \\coloneqq c+\\frac{d - c}{\\left (e+ f\\exp(-g \\olabelprior_j) \\right )^{1/h} }\\,.\n\\end{equation}",
            "eq:16": "\\begin{equation}\n\\label{eq:un-log-loss}\n\\lossfn(\\solabel_j, \\propensity_j, f_j(\\sinstance)) =\n    -\\frac{\\solabel_j}{\\propensity_j} \\log(f_j(\\sinstance)) - \\left(1 -\\frac{\\solabel_j}{\\propensity_j}\\right) \\log(1 -  f_j(\\sinstance))\\,.\n\\end{equation}",
            "eq:17": "\\begin{equation}\n\\label{eq:log-loss-ps-plug}\n\\lossfn(\\solabel_j, \\propensity_j, f_j(\\sinstance)) \\!=\\!\n    -\\solabel_j \\log(\\propensity_j f_j(\\sinstance)) \\!-\\! \\left(1 \\!-\\!\\solabel_j\\right) \\log(1\\!-\\!\\propensity_j f_j(\\sinstance))\\,,\n\\end{equation}",
            "eq:18": "\\begin{equation}\n\\precision[k](\\slabels, \\spreds) = k^{-1} \\sum_{\\mathclap{j \\in \\topk[k](\\spreds)}} w_j \\solabel_j \\,.\n\\end{equation}",
            "eq:19": "\\begin{equation}\n\\fmacro[\\beta] \\left (\\set{\\slabels_i, \\spreds_i}_1^\\numinstances \\right ) = \n\\frac{1}{\\numlabels} \\sum_j \\frac{(1+\\beta^2) \\sum_i \\sblabel_{ij} \\spred_{ij}} { \\beta^2 \\sum_i \\sblabel_{ij} + \\sum_i \\spred_{ij}}\\,,\n\\end{equation}",
            "eq:20": "\\begin{equation}\n\\abandon[k](\\slabels, \\spreds) = \\indicator[\\forall j \\in \\topk[k](\\spreds): \\, \\sblabel_j \\ne 1 ]\\,,\n\\end{equation}",
            "eq:21": "\\begin{equation}\n\\!\\!\\!\\coverage\\!\\left( \\set{\\slabels_i, \\spreds_i}_1^\\numinstances \\right) = {\\numlabels}^{-1} \\left | \\{ j \\in [\\numlabels] \\!:\\! \\exists i \\in [\\numinstances] \\; \\text{s.t.}\\; \\sblabel_{ij} = \\spred_{ij} = 1 \\} \\right | \\,.\n\\end{equation}",
            "eq:22": "\\begin{equation}\n\\label{eq:ps-un-log-loss}\n\\!\\lossfn(\\solabel_j, \\eta_j(\\bx), \\phi(\\theta_j)) \\!=\\! -\\frac{\\solabel_j}{\\eta_j(\\bx)} \\log(\\phi(\\theta_j)) \n    - \\left(1\\!-\\!\\frac{\\solabel_j}{\\eta_j(\\bx)}\\right) \\log(1 - \\phi(\\theta_j))\\,.\n\\end{equation}",
            "eq:23": "\\begin{equation}\n\\begin{aligned}\n\\ell((1,1), \\spreds) &= 0.5 (v_{11} + v_{00}) = 0.5(v_{10} + v_{01})\\,,\\\\\n  \\ell((1,0), \\spreds) &= 0.5 v_{00} + 0.5 v_{10}\\,,\\\\\n  \\ell((0,1), \\spreds) &= 0.5 v_{00} + 0.5 v_{01}\\,,\\\\\n  \\ell((0,0), \\spreds) &= v_{00}\\,.\n\\end{aligned}\n\\end{equation}"
        },
        "git_link": "https://github.com/mwydmuch/missing-labels-long-tails-and-propensities-in-xmlc"
    }
}