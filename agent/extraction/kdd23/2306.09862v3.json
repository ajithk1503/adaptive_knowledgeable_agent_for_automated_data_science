{
    "meta_info": {
        "title": "DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock  Trend Forecasting",
        "abstract": "Stock trend forecasting is a fundamental task of quantitative investment\nwhere precise predictions of price trends are indispensable. As an online\nservice, stock data continuously arrive over time. It is practical and\nefficient to incrementally update the forecast model with the latest data which\nmay reveal some new patterns recurring in the future stock market. However,\nincremental learning for stock trend forecasting still remains under-explored\ndue to the challenge of distribution shifts (a.k.a. concept drifts). With the\nstock market dynamically evolving, the distribution of future data can slightly\nor significantly differ from incremental data, hindering the effectiveness of\nincremental updates. To address this challenge, we propose DoubleAdapt, an\nend-to-end framework with two adapters, which can effectively adapt the data\nand the model to mitigate the effects of distribution shifts. Our key insight\nis to automatically learn how to adapt stock data into a locally stationary\ndistribution in favor of profitable updates. Complemented by data adaptation,\nwe can confidently adapt the model parameters under mitigated distribution\nshifts. We cast each incremental learning task as a meta-learning task and\nautomatically optimize the adapters for desirable data adaptation and parameter\ninitialization. Experiments on real-world stock datasets demonstrate that\nDoubleAdapt achieves state-of-the-art predictive performance and shows\nconsiderable efficiency.",
        "author": "Lifan Zhao, Shuming Kong, Yanyan Shen",
        "link": "http://arxiv.org/abs/2306.09862v3",
        "category": [
            "q-fin.ST",
            "cs.AI",
            "cs.CE",
            "cs.LG",
            "q-fin.CP"
        ],
        "additionl_info": "Accepted by KDD 2023. Code is at https://github.com/SJTU-Quant/qlib"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nStock trend forecasting, which aims at predicting future trends of stock prices, is a fundamental task of quantitative investment and has attracted soaring attention in recent years~\\cite{TRA, REST}. Due to the widespread success of deep learning, various neural networks have been developed to exploit intricate patterns of the stock market and infer future price trends. As an online application, new stock data arrive in a streaming way as time goes by. This gives rise to an increasingly growing dataset that is enriched with more underlying patterns. \nIt is of vital importance to continually learn new emerging patterns from incoming stock data, in order to avoid the model aging issue~\\cite{LLF} and pursue higher accuracy in future predictions. \n\n\nTo this end, a common practice named \\textit{Rolling Retraining} (RR) is used to periodically leverage the whole enlarged dataset to retrain the model parameters from scratch. However, RR usually leaves out abundant recent samples for validation and fails to retrain the model on the validation set, of which the patterns are often informative and valuable for future predictions~\\cite{DDGDA}. \n% In spite of benefiting from comprehensive data patterns of the large training set, a\nAnother fatal drawback of RR lies in its expensive time and space consumptions. The training time increases with the size of the enlarged training data, which further causes an unbearable duration of hyperparameter tuning and retraining algorithm selection. \nAn alternative way known as \\textit{Incremental Learning} (IL) is to fine-tune the model \\textit{only} with the latest incremental data. In each IL task, the model is initialized by inheriting parameters from the preceding IL task that are expected to memorize historical patterns, and then the model is consolidated with new knowledge in incremental data. The rationale behind it is that recent data may reveal some new patterns that did not appear before but will reoccur in the future. \n% IL could gain advantages by timely learning the latest emerging patterns, as the validation set is dispensable to IL with a predetermined number of training epochs. For example, we can leverage all of the incremental data in only one epoch and not care about convergence. \nMoreover, IL is not only dramatically faster but also occupies much smaller space than RR. \n\nDespite its considerable efficiency and potential effectiveness, IL is still under-explored in stock trend forecasting mainly due to the challenge of \\textbf{distribution shifts} (\\textit{a.k.a.} concept drifts).\n% two challenges: (i) \\textbf{limited data} and (ii) \\textbf{distribution shifts}. \n% \\textbf{First}, incremental data in time series domain is often small compared with other popular online services such as recommender systems and social networks. Though recent time series could reveal some new patterns that may not appear before but will continue in the future, incremental data of limited size may not contain abundant samples for the predictor to recognize, understand and memorize such patterns. On the other hand, repeatedly training on a few samples can easily lead to overfitting and inferior generalization ability. Therefore, it is challenging to reach a sweet spot between underfitting and overfitting.\nIL performs well only if there is always little difference between the distributions of incremental data and future data.\nHowever, it is well accepted that the stock market is in a non-stationary environment where data distribution irregularly shifts over time~\\cite{DDGDA, LLF, MASSER}. Such distribution shifts can vary in direction and degree. For example, in Figure~\\ref{fig:shift}, we visualize two real cases of distribution shifts in the Chinese stock market. Given historical samples in the last month (\\textit{e.g.}, 2019/09) as \\textit{incremental data}, we are meant to deploy a model online and do inference on the test samples in the next month (\\textit{e.g.}, 2019/10), termed as \\textit{test data}.\nIn the case of gradual shifts as shown in Figure~\\ref{fig:shift:a}, incremental data can reveal some future tendencies but its distribution still differs from the test data. \n% Learning from incremental data is profitable for future data prediction, which is the basic assumption of IL.  Nevertheless, \nA model that well fits the incremental data may not perfectly succeed in the future. Moreover, incremental data could even become misleading when distribution shifts abruptly appear, making a nonnegligible gap between the two data distributions, as shown in Figure~\\ref{fig:shift:b}. \n% There are more out-of-distribution samples in test data that are rather challenging to predict. \nAs long as distribution shifts exist, typical IL cannot consistently benefit from incremental data and may even suffer from inappropriate updates.\n% that are ineffective and even detrimental to future predictions. \nIn a nutshell, the discrepancy between the distributions of incremental data and test data could hinder the overall performance, posing the key challenge to IL for stock trend forecasting. \n\n% As the ubiquitous issue of limited data also exists in other research fields, a general approach is to adopt \\textit{meta-learning} (ML), the goal of which is to fast adapt to a new task only with a few training samples. \n% In ML settings, the training and test samples in each task are referred to as \\textit{support set} and \\textit{query set}, respectively. Among ML methods, MAML~\\cite{MAML} is the most widely adopted to learn how to fine-tune for good performance on query sets.\n% Due to its success in generalization over various tasks, some works~\\cite{FTML, MOLe, W&H, OSAKA} have extended MAML to online learning or continual learning, and more recent works~\\cite{LLF, MASSER} have introduced MAML to time series. Almost all of them assume that the support and query set of a specific task should come from the same context, \\textit{i.e.}, follow the same distribution or a locally stationary distribution. The meta-learner will quickly remember task-specific information of the support set and perform well on a similar query set. \n% However, such assumption cannot hold in time series under distribution shifts (\\textit{a.k.a.} concept drifts) where discrepancies between the two sets (\\textit{e.g.}, incremental data and test data) are non-negligible as mentioned before. Even though a predictor optimized by MAML is proved to generalize well against concept drifts~\\cite{LLF}, optimization of the meta-learner is delayed for unseen labels in online environment, and the predictions from task-specific parameters are still susceptible to distribution shifts. In light of such limitations, it is non-trivial to apply meta-learning into online time series prediction.\n\n% In our problem, each \\textit{incremental learning task} has the same target which expects the model parameters to quickly adapt to the incremental data of limited size, in order to make more precise predictions on the future distribution with similar patterns. \n\n\n\n\n\nConfronted with this challenge, it is noteworthy that the incremental updates stem from two factors: the incremental data and the initial parameters. Conventional IL blindly inherits the parameters learned in the previous task as initial parameter weights and conducts one-sided model adaptation on raw incremental data. To improve IL against distribution shifts, we propose to strengthen the learning scheme by performing two-fold adaptation, namely \\textbf{data adaptation} and \\textbf{model adaptation}. The data adaptation aims to close the gap between the distributions of incremental data and test data. For example, biased patterns that only exist in incremental data are equivalent to noise with respect to test data and could be resolved through proper data adaptation. Our model adaptation focuses on learning a good initialization of parameters for each IL task, which can appropriately adapt to incremental data and still retain a degree of robustness to distribution shifts. However, it is intractable to design optimal adaptation for each IL task. A proper choice of adaptation varies by forecast model, dataset, period, degree of distribution shifts, and so on. Hence, we borrow ideas from meta-learning~\\cite{MAML} to realize the two-fold adaptation, \\textit{i.e.}, to automatically find profitable data adaptation without human labor or expertise and to reach a sweet spot between adaptiveness and robustness for model adaptation.\n% Conventional IL blindly adapts the model parameters on the presumption that incremental data are always credible. Such one-sided model adaptation is susceptible to distribution shifts and can result in suboptimal updates along with inferior test performance. In pursuit of adaptiveness and robustness to various shifts, it is noteworthy that the incremental updates stem from both incremental data and initial parameters. \n% This fact implies that not only improper initial parameters but also imperfect incremental data hold accountable for the inappropriate updates and the following test errors. \n% The former factor motivates us to adapt data to close the gap between distributions of incremental data and test data. \n% Data preprocessing methods such as outlier clipping and normalization~\\cite{BN} cannot work consistently for all forecast models and require manually crafted heuristics to design. \n % Hence, it is desirable and interesting to automatically find optimal data adaptation in any case without human labor or expertise.\n% The latter factor motivates us to . This also requires us to automatically reach a sweet spot between adaptiveness and robustness.\n\nIn this work, we propose \\modelns, a meta-learning approach to incremental learning for stock trend forecasting. \n% \\textcolor{blue}{\\sout{We cast the IL problem as a sequence of meta-learning tasks with an explicit optimization objective, \\textit{i.e.}, to minimize test errors and avoid inappropriate incremental updates. We are devoted to automatically learning what input data and initial parameters are favorable for incremental learning.}}\nWe introduce two meta-learners, namely data adapter and model adapter, which adapt data towards a locally stationary distribution and equip the model with task-specific parameters that have quickly adapted to the incremental data and still generalize well on the test data. \nThe data adapter contains a multi-head feature adaptation layer and a multi-head label adaptation layer in order to obtain adapted incremental data and adapted test data that are profitable for incremental learning. Specifically, the feature adaptation layer transforms all features from the incremental data and the test data, while the label adaptation layer rectifies labels of the incremental data and its inverse function restores model predictions on the test data. By casting the problem of IL for stock trend forecasting as a sequence of meta-learning tasks, we perform each IL task by solving a bi-level optimization problem: \n(i) in the lower-level optimization, the parameters of the forecast model are initialized by the model adapter and fine-tuned on the adapted incremental data; \n(ii) in the upper-level optimization, the meta-learners are optimized by test errors \n% \\sout{for appropriate adaptation according to performance}\non the adapted test data.\nThroughout the online inference phase, both the two adapters and the forecast model will be updated continually over new IL tasks. \n% Also, it is necessary to warm up the meta-learners for accurate predictions at the early stage. Therefore, we organize offline training data into multiple meta-learning tasks and imitate online incremental learning to pretrain the two adapters.\n% \\sout{\\model achieves desirable adaptation in two granularities. \n% At a fine-grained \\textit{data level}, we propose a multi-head feature adaptation layer to transform features of the incremental data and the test data, along with a multi-head label adaptation layer to transform labels of the incremental data and transform model predictions on the test data. Both the meta-learners and the forecast model will be updated continuously in the online setting. \n% At a coarse-grained \\textit{parameter level}, the model adapter initializes a forecast model with profitable parameters that can fast adapt to the task-specific information and still retain generalization ability against distribution shifts. }\n\nThe main contributions of this work are summarized as follows.\n\n\\begin{itemize}[leftmargin=*]\n  \\item We propose \\modelns, an end-to-end incremental learning framework for stock trend forecasting, which adapts both the data and the model to cope with distribution shifts in the online environment.   \n  \\item We formulate each incremental learning task as a bi-level optimization problem. The lower level is the forecast model that is initialized by the model adapter and fine-tuned using the adapted incremental data. \n  The upper level includes the data adapter and the model adapter as two meta-learners that are optimized to minimize the forecast error on the adapted test data. \n  % , where we propose a data adapter to mitigate distribution shifts and employ a model adapter to consolidate the forecast model with task-specific information. We introduce meta-learning to automatically find profitable data adaptation and model adaptation.  \n  \\item We conduct experiments on real-world datasets and demonstrate that \\model performs effectively against different kinds of distribution shifts and achieves state-of-the-art predictive performance compared with RR and meta-learning methods. \\model also enjoys high efficiency compared with RR methods.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\nIn this section, we will introduce some definitions of our work and formulate the incremental learning problem. We also highlight the challenge of distribution shifts.\n\n\\begin{mydefinition}{Stock Price Trend}\\label{def:trend}\n  Following~\\cite{HAN2018, REST, HIST}, we define the stock price trend at date $t$ as the stock price change rate of the next day:\n  \\begin{equation}\n    y^{(t)} = \\frac{Price^{(t+1)} - Price^{(t)}}{Price^{(t)}},\n  \\end{equation}\n  where $Price^{(t)}$ is the closing price at date $t$ and could also be the opening price or volume-weighted average price (VWAP).\n\\end{mydefinition}\n\nLet $\\mathbf x^{(t)} \\in \\mathbb R^{D}$ represent the feature vector of a stock at date $t$, where $D$ is the feature dimension. For example, we can constitute $\\mathbf x$ with opening price, closing price, and other indicators in recent days. Its stock price trend $y$ is the corresponding label. Suppose the stock market comprises $S$ stocks. The collection of features and labels of the $S$ stocks at date $t$ can be denoted as $\\mathbf X^{(t)} \\in \\mathbb R^{S\\times D}$ and $\\mathbf Y^{(t)} \\in \\mathbb R^{S}$, respectively.\nThe goal of stock trend forecasting is to learn a forecast model $F$ on historical data $\\{(\\mathbf{X}^{(t)}, \\mathbf Y^{(t)})\\}_{t=1}^T$ and then forecast the labels of future data $\\{(\\mathbf{X}^{(t)}, \\mathbf Y^{(t)})\\}_{t=T+1}^{T'}$, where $T$ and $T'$ are the end time of the historical data and the future data, respectively. The parameters of $F$ are denoted as $\\theta$. \n\nIn online scenarios, new data are coming over time. Once the ground-truth labels of new samples are obtained, we can update the forecast model to learn new emerging patterns. \nIn this work, we focus on incremental learning (IL) for stock trend forecasting, where we periodically launch IL tasks to update the model \\textit{only} with incremental data, as illustrated in Figure~\\ref{fig:IL} and defined as follows.\n\n\\begin{mydefinition}{IL Task for Stock Trend Forecasting}\n  Supposing a pretrained forecast model is deployed online at date $T$+$1$, we launch an IL task every $r$ dates, where $r$ is predetermined by practical applications. For the $k$-th IL task at date $T$+$kr$+$1$, we fine-tune the model parameters $\\theta^{k-1}$ on incremental data ${\\mathcal D}^{k}_{\\text{train}}$ and predict labels on test data ${\\mathcal D}^{k}_{\\text{test}}$ in the following $r$ dates, where ${\\mathcal D}_{\\text{train}}^{k}=\\{(\\mathbf{X}^{(t)}, \\mathbf Y^{(t)})\\}_{t=T+(k-1)r+1}^{T+kr}$ and ${\\mathcal D}^{k}_{\\text{test}}=\\{(\\mathbf{X}^{(t)}, \\mathbf Y^{(t)})\\}_{t=T+kr+1}^{T+(k+1)r}$. \n  % Timespans of two data are the same and also equal to the interval between two consecutive tasks. \n  The outputs of the $k$-th task are updated parameters $\\theta^{k}$ and predictions $\\{ \\hat{\\mathbf Y}^{(t)}\\}_{i=T+kr+1}^{T+(k+1)r}$. \n  The IL task expects the model to quickly adapt to the incremental data, in order to make precise predictions on future data with similar patterns. We can evaluate the predictions by computing a loss function $\\mathcal L_{\\text{test}}$ on ${\\mathcal D}^{k}_{\\text{test}}$, \\textit{e.g.}, mean square error.\n\\end{mydefinition}\n\n\\noindent\\textbf{Problem Statement. }\n  Given a predefined task interval $r$, IL for stock trend forecasting is constituted by a sequence of IL tasks, \\textit{i.e.}, $\\mathcal{T} = $\n  $\\{({\\mathcal D}_{\\text{train}}^{1}, {\\mathcal D}_{\\text{test}}^{1})$, \n  $\\ ({\\mathcal D}_{\\text{train}}^{2}, {\\mathcal D}_{\\text{test}}^{2})$,\n  $\\ \\cdots, ({\\mathcal D}_{\\text{train}}^{k}, {\\mathcal D}_{\\text{test}}^{k}), \\cdots\\}$. \n  In each task, we update the model parameters, do online inference, and end up with performance evaluation on the ground-truth labels of the test data. The goal of IL is to achieve the best overall performance across all test dates, which can be evaluated by excess annualized returns or other ranking metrics of stock trend forecasting. \n  % Labeled samples that have not been learned in previous $k-1$ tasks are collected as the incremental data $\\mathcal D_{\\text{train}}^{k}=\\{(\\mathcal{X}^{(t)}, \\mathcal Y^{(t)})\\}_{i=T+(k-1)r+1}^{T+kr}$. We leverage $\\mathcal D_{\\text{train}}^{k}$ to fine-tune the predictor and predict labels on the test data $\\mathcal D_{\\text{test}}^{k}=\\{(\\mathcal{X}^{(t)}, \\mathcal Y^{(t)})\\}_{i=T+H+kr}^{T+H+(k+1)r-1}$.\n\n\\vspace{0.2em}\n% In effect, IL condenses historical patterns in accomplished tasks into model parameters and consolidates the parameters with new knowledge in the current task. \nTypically, IL holds a strong assumption that a model which fits recent data can perform well on the following data under the same distribution. However, as the stock market is dynamically evolving, its data distribution can easily shift over time.\n$\\mathcal D_{\\text{train}}^{k}$ and $\\mathcal D_{\\text{test}}^{k}$ are likely to have two different joint distributions, \\textit{i.e.}, $\\mathcal P_{\\text{train}}^{k}(\\mathbf x, y) \\ne \\mathcal P_{\\text{test}}^{k}(\\mathbf x, y)$, where $\\mathcal P_{\\text{train}}^{k}$ and $\\mathcal P_{\\text{test}}^{k}$ denote the distributions of $\\mathcal D_{\\text{train}}^{k}$ and $\\mathcal D_{\\text{test}}^{k}$, respectively. \nThe distribution shifts can be zoomed into the following two cases~\\cite{SurveyConcepDrift}: \n\\begin{itemize}[leftmargin=*]\n  \\item \\textit{Conditional distribution shift} when $\\mathcal P_{\\text{train}}^{k}(y\\mid\\mathbf x) \\ne \\mathcal P_{\\text{test}}^{k}(y\\mid\\mathbf x)$;\n  \\item \\textit{Covariate shift} when $\\mathcal P_{\\text{train}}^{k}(\\mathbf x) \\ne \\mathcal P_{\\text{test}}^{k}(\\mathbf x)$ and $\\mathcal P_{\\text{train}}^{k}(y\\mid\\mathbf x) = \\mathcal P_{\\text{test}}^{k}(y\\mid\\mathbf x)$.\n\\end{itemize}\nIn either case, excessive updates on incremental data would incur the overfitting issue while deficient updates may result in an underfit model. \nHence, distribution shifts pose challenges to incremental learning for stock trend forecasting.\n% In what follows, we present the opportunities and insights to overcome the distribution shift challenge towards effective IL performance. \n\n\n\n"
            },
            "section 3": {
                "name": "Key Insights",
                "content": "\n\nTo tackle the distribution shift challenge, there are two important directions to follow. One is to close the gap between incremental data and test data so as to perform IL on more stationary distributions. Another is to enhance the generalization ability of the model against distribution shifts. Following both directions, we propose to perform data adaptation and model adaptation for each IL task. We describe our key insights with the following details.\n\n% Incremental learning condenses historical patterns into the model parameters in previous tasks and consolidates the parameters with new knowledge in the current task. Suppose future data share some patterns with recent data, it is profitable to memorize the incoming context information. We thus require initial parameters to effectively adapt to the task-specific context without being trapped in past experience. Suppose the data adaptation cannot fully mitigate distribution shifts, inappropriate updates on limited data can incur overfitting issues. We also require the initial parameters to preserve more historical experience and retain generalization ability after incremental updates. To reach a sweet spot between adaptiveness and generalization, \\textit{i.e.}, between underfitting and overfitting,\n\n% In this section, we introduce our two key insights to address distribution shifts in each incremental learning task. \n% There are several opportunities for improving the performance of incremental learning.\n% Confronted with distribution shifts, it is noteworthy that the fine-tuned model of each task not only stems from incremental data but also originates from initial parameters, both of which indirectly affect the test performance. \n% There are several opportunities for improving the performance of incremental learning.\n% Therefore, there are two directions to address distribution shifts, \\textit{i.e.}, improving the data and improving the initial parameters.\n\n\\vspace{0.2cm}\n\\noindent\\textbf{Data Adaptation.} A critical yet under-explored direction is to adapt data into a locally stationary distribution so as to mitigate the effects of distribution shifts at the data level. Some RR methods resample all historical data (\\textit{e.g.}, 500 million samples) into a new training set that shares a similar distribution with future data~\\cite{DDGDA}. However, such a \\textit{coarse-grained} adaptation fails in IL where incremental data is of limited size (\\textit{e.g.}, one thousand samples) and contains deficient samples to reveal future patterns. To address this limitation, we propose to adapt all features and labels of the incremental data to mitigate the effects of distribution shifts in a \\textit{fine-grained} way. \nWe argue that some shift patterns repeatedly appear in the historical data and are learnable. \n% The rationale behind this is that some high-level patterns (\\textit{e.g.}, economic laws and global tendencies) change little even if financial factors vary in different distributions. \n% For example, stock prices may soar madly in the terminal stage of a bull market, while such a soaring trend is fragile and may well encounter a meltdown in the next period. \nFor example, stock prices can overreact to some bullish news and emotional investment, while the prices and the trend patterns tend to shift towards normal in future weeks.\nThus, it is often desirable to retract the overreacting features and labels to approach future tendencies. \n% Data adaptation is meaningful to imperfect incremental data with spurious information that will not exist in test data. For instance, a stock of extreme feature values is not likely to reappear. One straightforward way is to directly discard such a noise, or we can adjust the features and the label to normal.  \n% ~\\cite{DDGDA} notices that distribution shifts on stock data have some temporal tendencies that are predictable. This gives us the opportunity to proactively adapt the incremental data. \n% This motivates us to learn how to adapt input data in a fine-grained way for more profitable incremental data. \nIn addition, assuming the original incremental data is reliable, test data of a different distribution can be deemed as a biased dataset. \nAdapting test data towards the distribution of incremental data has a debiasing effect. \n% It is also important to adapt test data toward the distribution of incremental data. \nHence, we adapt both $\\mathcal {D}_{\\text {train}}^{k}$ and $\\mathcal {D}_{\\text {test}}^{k}$ so as to narrow the gap between their distributions. \n\n\n% \\noindent\\textbf{Opportunity for local stationarity.} Note that mitigating the distribution shifts at the data level is also critical to IL. The key idea is to adapt the input data into a locally stationary distribution in preparation for incremental updates. Though future data are unknown at the update time, ~\\cite{DDGDA} notices that distribution shifts on stock data have some temporal tendencies that are predictable. This gives us the opportunity to proactively adapt the data under either gradual shifts or abrupt shifts towards local stationarity.\n% Given that mitigating the distribution shifts is critical to incremental learning, we should not only adapt the model but also adapt the input data into a locally stationary distribution.\n\n\n\n\n% \\noindent\\textbf{Insights of data adaptation.} \nTechnically, we cannot directly align $\\mathcal P_{\\text{train}}^{k}(\\mathbf x, y)$ and $\\mathcal P_{\\text{test}}^{k}(\\mathbf x, y)$ because labels of test data are unknown at the inference time. We thus decouple distribution shifts into covariate shifts and conditional distribution shifts, and address them separately, as illustrated in Figure~\\ref{fig:flow}. \n\\textbf{First}, we require a mapping function $G$ to transform the features of $\\mathcal {D}_{\\text {train}}^{k}$ and $\\mathcal {D}_{\\text {test}}^{k}$ in a fine-grained way, and we expect $G$ to adapt $\\mathcal P_{\\text{train}}^{k}(\\mathbf x)$ and $\\mathcal P_{\\text{test}}^{k}(\\mathbf x)$ to an agent feature distribution $\\mathcal P_{\\text{agent}}^{k}(\\mathbf x)$, alleviating covariate shifts. \n% For instance, a stock of extreme feature values is not likely to reappear. One straightforward way is to directly discard such a noise, or we can adjust the features to normal. \n\\textbf{Second}, we apply another mapping function $H$ to adapt the labels of $\\mathcal {D}_{\\text {train}}^{k}$. We expect $H$ to adapt $\\mathcal {P}_{\\text{train}}^{k}(y|\\mathbf x)$ to a possible future distribution $\\mathcal P_{\\text{agent}}^{k}(y|\\mathbf x)$, dealing with conditional distribution shifts. \n% For example, given fancy features $\\mathbf x$, the stock price rises with a much higher $y$ than other stocks. But a turning point may well take place in the next period even if $\\mathbf x$ changes little. A prediction that the stock will keep its upward trend is too optimistic and could cause a great test error. To avoid memorizing such nonpersistent and misleading information, we can moderately lower the label $y$ as an adapted label $\\tilde y$. \nIdeally, $H$ could project the labels from $\\mathcal P_{\\text{test}}^{k}(y|\\mathbf x)$ into $\\mathcal P_{\\text{agent}}^{k}(y|\\mathbf x)$ so that a forecast model fitting $\\mathcal P_{\\text{agent}}^{k}(y|\\mathbf x)$ can precisely predict the adapted label for the test data. Finally, we need to inversely map the model outputs from $\\mathcal P_{\\text{agent}}^{k}(y| \\mathbf x)$ to $\\mathcal P_{\\text{test}}^{k}(y| \\mathbf x)$.\n% Note that the ultimate goal is to make an accurate prediction $\\hat y$ that is close to the raw label in the test data under $\\mathcal P_{\\text{test}}^{k}(y| \\mathbf x)$. We need to inversely map $\\check{y}$ from $\\mathcal P_{\\text{agent}}^{k}(y| \\mathbf x)$ to $\\mathcal P_{\\text{test}}^{k}(y| \\mathbf x)$. \n\\textbf{As such}, we narrow the gap between $\\mathcal P_{\\text{train}}^{k}(\\mathbf x)$ and $\\mathcal P_{\\text{test}}^{k}(\\mathbf x)$ via feature adaptation, and narrow the gap between $\\mathcal {P}_{\\text{train}}^{k}(y|\\mathbf x)$ and $\\mathcal {P}_{\\text{test}}^{k}(y|\\mathbf x)$ via label adaptation. This allows us to reduce the discrepancy between the joint distributions, alleviating the distribution shift issue.\n% Fine-tuned on the adapted incremental data, the forecast model will give predictions that follow $\\mathcal P_{\\text{agent}}^{k}(y|\\mathbf x)$ but still not the exact future distribution $\\mathcal P_{\\text{test}}^{k}(y|\\mathbf x)$. \n% Thus we still need to map the predictions on $\\mathcal {D}_{\\text{test}}^{k}$ from $\\mathcal P_{\\text{agent}}^{k}(y|\\mathbf x)$ to $\\mathcal P_{\\text{test}}^{k}(y|\\mathbf x)$. \n% Note that what we ultimately want are predictions that are close to the raw label of test data under $\\mathcal P_{\\text{test}}^{k}(y| \\mathbf x)$. We can achieve it by using $H^{-1}$ to inversely map the model outputs from $\\mathcal P_{\\text{agent}}^{k}(y| \\mathbf x)$ to $\\mathcal P_{\\text{test}}^{k}(y| \\mathbf x)$.\n\n\n% As incremental learning would inappropriately adapt the model under distribution shifts, one can turn to advanced model adaptation methods for more generalization ability. \n % As inappropriate updates on limited data can incur overfitting issues, \n \\vspace{0.2cm}\n\\noindent\\textbf{Model Adaptation.}  \n% For each IL task, the model parameters will be initialized first and then fine-tuned with incremental data. Conventionally, the initialization is performed by inheriting trained parameters from the previous task. Such a sequential initialization scheme may forget patterns of more previous tasks and suffer from ovefitting to incremental data that has a different distribution from the test data....\n% Ideally, a good initialization should...increase generalization??? and diminishing the impact of distribution shifts.\n% As updating the model parameters is the core of incremental learning, great efforts have been devoted to developing advanced model adaptation methods for better gradient descent~\\cite{BGD}, regularization~\\cite{EWC}, gating~\\cite{context-dependent-gating}, transfer~\\cite{SML}, and so on. It is worth mentioning that all these methods act on the basis of initial parameters before adaptation.\n% Typically, IL inherits initial parameters from the previous task and adapts the parameters by incremental data. \nTypically, IL initializes the model by the parameters learned in the previous task and updates the initial parameters on incremental data. Distribution shifts would hinder the test performance if the parameters after updates fall into a local optimum and overfit the incremental data. This motivates us to learn a good initialization of parameters for each IL task. \nOn the one hand, the initial parameters of each task is required to preserve historical experience and retain generalization ability against distribution shifts. On the other hand, the parameters in IL still need to effectively memorize task-specific information without being trapped in past experiences. Therefore, we emphasize another important direction where we optimize the initial parameters of each IL task for robustness and adaptiveness.\n% ~\\cite{LLF} also proves that a model pretrained by MAML~\\cite{MAML} can generalize well against concept drift. Inspired by this, we implement a model adapter by extending MAML to our incremental learning problem, where we introduce a differential optimization objective to optimize the initial parameters. \n\n% that is not vulnerable to either underfitting or overfitting. \n% Suppose future data share some patterns with recent data which are profitable to memorize, we still require initial parameters to effectively adapt to the task-specific context without being trapped in past experience. \n% It is tough to reach a sweet spot between generalization and adaptiveness, \\textit{i.e.}, between overfitting and underfitting. Inspired by ~\\cite{MAML}, we adopt meta-learning to learn the desirable initialization of parameters for each task, and then we adapt the initial parameters to incremental data for future predictions.\n\n \\vspace{0.2cm}\n\\noindent\\textbf{Optimization via Meta-learning.} Following our key insights, we aim to mitigate distribution shifts at the data level and enhance generalization ability at the parameter level.\nDesirable data adaptation should make the distributions of the two datasets more similar but still informative for stock trend forecasting.\nNevertheless, it is infeasible to manually design proper data adaptation as numerous factors should be considered, \\textit{e.g.}, forecast models, datasets, prediction time, degrees of distribution shifts, and so on. \nFor model adaptation, it is also tough to reach a sweet spot between robustness and adaptiveness. We thus introduce a meta-learning optimization objective to guide profitable data adaptation and parameter initialization. \nThough some normalization techniques ~\\cite{MetaNorm, InstanceNorm, Tasknorm} in meta-learning can reduce distribution divergences, they may destroy the original statistical indicators (\\textit{i.e.}, mean and standard deviation), which are critical task-specific information and deserve memorization in the online settings. In light of this, we pioneer mitigating the distribution shifts in meta-learning through neural networks rather than normalization.\n\n% Hence, we adopt meta-learning~\\cite{MAML, SML} to find optimal data adaptation in any case without human labor or expertise and provide good initialization of parameters.\n\n"
            },
            "section 4": {
                "name": "Methodology",
                "content": "\n\n% In this section, we first introduce our motivation of two-fold adaptation for distribution shifts. Next, we present the overview of our proposed \\model framework. Then, we elaborate on the design of two key components. Lastly, we propose a two-phase training procedure for our framework. \n% The key idea of \\model is to jointly learn how to adapt data for distribution shifts and learn how to fast adapt the model parameters to the adapted data distribution. \n% At the data level, we learn to transform the raw features and labels of each sample in both incremental data and test data to mitigate distribution shifts. At the parameter level, we update the model with task-specific parameters that have remembered recent patterns in incremental data in favor of promising performance in a similar test distribution. \n\n",
                "subsection 4.1": {
                    "name": "Overview",
                    "content": "\n\n\n\nFigure~\\ref{fig:overview} depicts the overview of our \\model framework, which consists of three key components: \\textit{forecast model} $F$ with parameters $\\theta$, \\textit{model adapter} ${MA}$ with parameters $\\phi$, and \\textit{data adapter} ${DA}$ with parameters $\\psi$. \n$DA$ contains a feature adaptation layer $G$ and a label adaptation layer $H$, along with its inverse function $H^{-1}$. In particular, the implementation of $F$ can be realized by any neural network for stock trend forecasting, such as GRU~\\cite{GRU} and ALSTM~\\cite{ALSTM}.\n\n% For the $k$-th incremental learning task, ${DA}$ and ${MA}$ inherit their parameters from the previous task, \\textit{i.e.}, $\\phi^{{k-1}}$ and $\\phi^{{k-1}}$. We extend the one-sided model adaptation in typical IL to five steps. \nWe cast IL for stock trend forecasting as a sequence of meta-learning tasks.\nFor each task $(\\mathcal D_{\\text{train}}^{k}, \\mathcal D_{\\text{test}}^{k})$, \\model involves four steps as listed below.\n\n\\begin{enumerate}[leftmargin=*]\n    \\item \\textbf{Incremental data adaptation.} Given incremental data $\\mathcal D_{\\text{train}}^{k}$, ${DA}$ transforms each feature vector $\\mathbf x$ via $G$ and transforms the corresponding label $y$ via $H$, generating an adapted incremental dataset $\\mathcal{\\widetilde D}_{\\text{train}}^{k}$.\n    \\item\\textbf{Model adaptation.\\label{step:ma}} ${MA}$ initializes the forecast model by parameter weights $\\phi^{k-1}$. Following typical IL, we fine-tune the forecast model on $\\widetilde{\\mathcal D}_{\\text{train}}^{k}$, generating task-specific parameters $\\theta^{k}$. Then, the updated forecast model $F(\\cdot;{\\theta^{k}})$ is deployed online.\n    \\item\\textbf{Online inference.} Given $\\mathbf x$ of each sample in $\\mathcal D_{\\text{test}}^{k}$, $DA$ transforms it via $G$ into $\\tilde{\\mathbf x}$. Then, $F(\\cdot;{\\theta^{k}})$ takes $\\tilde{\\mathbf{x}}$ and produces an intermediate prediction $\\check y$ which will be transformed via $H^{-1}$ into the final prediction $\\hat y$. We denote the adapted test dataset by $\\mathcal{\\widetilde D}_{\\text{test}}^{k}$ that is obtained by transforming raw features in $\\mathcal{D}_{\\text{test}}^{k}$.\n    % We calculate errors at each date once we obtain the corresponding ground truth $y$.  \n    \\item\\textbf{Optimization of meta-learners.\\label{step:opt}} We calculate the final forecast error $\\mathcal L_{\\text{test}}$ once we obtain all ground-truth labels in order to optimize our meta-learners (\\textit{i.e.}, ${DA}$ and ${MA}$) in the upper level. The parameters of meta-learners are updated from $\\phi^{k-1}$ and $\\psi^{k-1}$ to $\\phi^{k}$ and $\\psi^{k}$, which are used for the next IL task.\n\\end{enumerate}\n\n% \\begin{enumerate}[leftmargin=*]\n%     \\item ${DA}$ transforms each $\\mathbf x$ and $y$ of $\\mathcal D_{\\text{train}}^{k}$ to $\\tilde{\\mathbf x}$ and $\\tilde y$, respectively, generating an adapted incremental dataset $\\mathcal{\\widetilde D}_{\\text{train}}^{k}$.\n%     \\item ${MA}$ leverages $\\widetilde{\\mathcal D}_{\\text{train}}^{k}$ to update $F$ with task-specific parameters $\\theta^{k}$. We then deploy $F_{\\theta^{k}}$ online.\n%     \\item During inference, each $\\mathbf x$ in test data $\\mathcal D_{\\text{test}}^{k}$ are also transformed as $\\tilde{\\mathbf x}$ via ${DA}$ into an adapted test dataset $\\mathcal{\\widetilde D}_{\\text{test}}^{k}$ which is expected to close the distribution gap with $\\widetilde {\\mathcal D}_{\\text{train}}^{k}$.\n%     \\item $F_{\\theta^{k}}$ takes each $\\tilde x$ and outputs $\\check y$. We transform $\\check y$ to the final prediction $\\hat y$. We calculate errors at each date once we obtain the corresponding ground truth $y$.\n%     \\item Finally, we utilize the final test loss $\\mathcal L_{\\text{test}}$ to optimize our two meta-learners, ${DA}$ and ${MA}$.\n% \\end{enumerate}\n\n% From a separate perspective of the forecast model, the fine-tuning and inference procedures of $F$ are carried out under a locally stationary distribution, alleviating the distribution shift issue. \nThe meta-learning process is essentially a bi-level optimization problem, where the fine-tuning in Step (\\ref{step:ma}) is the lower-level optimization and Step (\\ref{step:opt}) is the upper-level optimization. Formally, since we desire a minimal forecast error on $\\mathcal {\\widetilde D}_{\\text {test}}^{k}$, the bi-level optimization of the $k$-th IL task is defined as\n\n\\begin{equation}\\label{eq:upper level}\n  \\phi^{k}, \\psi^{k}=\\underset{{\\phi, \\psi}}{\\arg \\min } \\mathcal L_{\\text{test}}(\\mathcal {\\widetilde D}_{\\text {test}}^{k}; \\theta^{k}),\n\\end{equation}\n\\begin{equation}\\label{eq:lower level}\n  \\text {s.t.} \\quad {\\theta}^{k}={MA}(\\mathcal {\\widetilde D}_{\\text{train}}^{k};\\phi^{k-1}), \n\\end{equation}\nwhere\n\\begin{equation}\\label{eq:adapted data}\n  \\mathcal{\\widetilde D}_{\\text {train }}^{k}={DA}({\\mathcal D}_{\\text {train }}^{k};\\psi^{k-1});\\quad\n  \\mathcal {\\widetilde D}_{\\text {test}}^{k}={DA}({\\mathcal D}_{\\text {test}}^{k};\\psi^{k-1}).\n\\end{equation}\n% The test loss $\\mathcal L_{\\text{test}}$ is used to evaluate performance of adapted parameters $\\theta^{k}$ on the adapted test dataset $\\mathcal{\\widetilde D}_{\\text {test}}^{k}$. \n% We can use mean square error (MSE) to calculate $\\mathcal L_{\\text{test}}$ in stock trend forecasting. \nNote that we only adapt features of ${\\mathcal D}_{\\text {test}}^{k}$ because test labels are unknown during online inference time.\n\nGenerally, the best incremental learning algorithm should lead to minimum accumulated errors on all test datasets throughout online inference. However, in the context of stock trend forecasting, we only focus on improving the test performance of the current task since past predictions cannot be withdrawn and future datasets are unseen. Therefore, we greedily optimize $\\phi$ and $\\psi$ task by task in a practical scenario. Furthermore, we approximate Eq.~(\\ref{eq:upper level}) by one-step gradient descent for the concern of training efficiency. \n\nIn the following subsections, we elaborate the two adapters and detail our upper-level optimization.\n% Note that we update $\\phi$ and $\\psi$ only once in order to prevent overfitting on the current task, \\textit{i.e.}, we replace the $\\arg$$\\min$ operator with one-step gradient descent in Eq.~(\\ref{eq:upper level}). We will detail our upper-level optimization in Section~\\ref{sec:optimization}.\n\n"
                },
                "subsection 4.2": {
                    "name": "Data Adapter",
                    "content": "\\label{sec:DA}\n\nData adapter ${DA}$ is a meta-learner to learn what data adaptation is favorable for appropriate updates and promising test performance. ${DA}$ consists of three mapping functions, \\textit{i.e.}, feature adaptation $G$, label adaptation $H$, and its inverse mapping $H^{-1}$.\n\n% Imperfect incremental data with either spurious information or biased features that will not exist in test data can lead to inappropriate updates. This motivates us to learn how to adapt input data for more profitable incremental data. \n% On the other hand, assuming the original incremental data is clean and reliable, test data of a different distribution can be deemed as a biased dataset. Accordingly, another direction is to adapt test data toward the distribution of incremental data. Hence, we adapt both $\\mathcal {D}_{\\text {train}}^{k}$ and $\\mathcal {D}_{\\text {test}}^{k}$ so as to narrow the gap between their distributions. \n\n\nFollowing our insights, we propose a mapping function $G$ to map $\\mathcal P_{\\text{train}}^{k}(\\mathbf x)$ and $\\mathcal P_{\\text{test}}^{k}(\\mathbf x)$ into an agent feature distribution $\\mathcal P_{\\text{agent}}^{k}(\\mathbf x)$, alleviating covariate shifts. We intuitively introduce a dense layer as a simple implementation of ${G}$, which is defined as follows:\n\\begin{equation}\n  \\tilde{\\mathbf{x}} = \\mathbf W\\mathbf x + \\mathbf b,\n\\end{equation}\nwhere $\\tilde{\\mathbf{x}}$ denotes the adapted feature vector, $\\mathbf W\\in \\mathbb R^{D\\times D}$ is the parameter matrix, and $\\mathbf b\\in \\mathbb R^{D}$ is the bias vector. \nFeatures from ${\\mathcal D}_{\\text {train }}^{k}$ and ${\\mathcal D}_{\\text {test}}^{k}$ are thereby transformed onto a new common hyperplane via the same affine transformation. \n% We optimize the dense layer via meta-learning to find an optimal data adapter that makes distributions of the two datasets more similar and still informative for stock trend forecasting. \n\nThe remaining concern is that one simple dense layer is not expressive enough.\n% and is not sensitive to outliers. \n% Another choice is to perform a personalized transformation as follows:\n% \\begin{equation}\n%   \\tilde{\\mathbf{x}} = W(\\mathbf x)\\mathbf x + b(\\mathbf x),\n% \\end{equation}\n% where $W$ and $b$ are parametrized functions to generate the transformation weight and transformation bias according to input features. But this sample-specific transformation may depart from our original intention to transform two distributions into a common one. It may also suffer from high complexity and limited data. \nDifferent types of feature vectors may require different transformations, for example, abnormal values need to be scaled or masked, trustworthy features just need identity mapping, and profitable signals need to be emphasized. Moreover, stock price trends tend to bear similar shift patterns when the stocks belong to the same concept (\\textit{e.g.}, sector, industry, and business), and vice versa. Accordingly, an appealing solution is to employ different transformation heads and decide which candidate head is more suitable for the input. We thus propose a \\textit{multi-head feature adaptation layer} with multiple \\textit{feature transformation heads}, which is formally defined as follows:\n\\begin{equation}\\label{eq:feature adaptation}\n  \\mathbf{\\tilde x} = G(\\mathbf x) := \\mathbf x+\\sum_{i=1}^{N} s_i\\cdot g_{i}(\\mathbf x),\n\\end{equation}\nwhere we add a residual connection~\\cite{ResNet} in case excessive transformation forgets most raw information; $N$ is the number of heads; $g_i$ is the $i$-th feature transformation head; $s_i$ is a normalized confidence score to decide the strength of the $i$-th transformation.\nWe implement each $g_i$ by a simple dense layer:\n\\begin{equation}\n  g_i(\\mathbf x)=\\mathbf W_i\\mathbf x+\\mathbf b_i,  \n\\end{equation}\nwhere $\\mathbf W_i$ and $\\mathbf b_i$ are head-specific transformation parameters. \nAs for confidence estimation, we use $N$ prototype vectors to prompt which head is more applicable. Specifically, we first calculate the cosine similarity $\\hat s_i$ between the feature vector and each prototype, which is formulated as:\n\\begin{equation}\n  \\hat s_i=\\frac{\\mathbf p_i^\\top \\mathbf x}{\\|\\mathbf p_i\\| \\| \\mathbf x \\|}, \n\\end{equation}\nwhere $\\mathbf p_i\\in \\mathbb R^{D}$ is the $i$-th prototype vector. Then, we derive the normalized score $s_i$ by\n\\begin{equation}\n  s_i=\\frac{\\exp \\hat s_i/\\tau}{\\sum_{j=1}^{N} \\exp \\hat s_j/\\tau},\n\\end{equation}\nwhere $\\tau$ is a positive hyperparameter to control softmax temperature. With the prototypes deemed as learnable embeddings of hidden concepts in the stock market, the multi-head feature adaptation layer can provide concept-oriented transformations for each input feature vector $\\mathbf x$.\n\nTo cope with conditional distribution shifts, we adapt labels of $\\mathcal {D}_{\\text {train}}^{k}$. We define a \\textit{label transformation head} as follows:\n\\begin{equation}\n  h_i(y) = {\\gamma}_i y + \\beta_i,\n\\end{equation}\nwhere ${\\gamma}_i\\in \\mathbb R$ and $\\beta_i\\in \\mathbb R$ are learnable meta-parameters. \nA \\textit{multi-head label adaptation layer}, termed as $H$, is formulated as\n\\begin{equation}\\label{eq:label adaptation}\n  \\tilde{y} = H(y) := \\sum_{i=1}^{N} s_i\\cdot h_{i}(y),\n\\end{equation}\nwhere $\\tilde y$ denotes the adapted label; $s_i$ has been calculated in the feature adaptation layer and is determined by $\\mathbf x$.\n% We believe this label adaptation layer can help mitigate conditional distribution shifts. For example, given fancy features $\\mathbf x$, the stock price rises with a much higher $y$ than other stocks. But a turning point may well take place in the next period even if $\\mathbf x$ changes little. A prediction that the stock will keep its upward trend is too optimistic and could cause a great test error. To avoid memorizing such nonpersistent and misleading information, we can moderately lower the label. We thus expect our label adaptation to project labels of $\\mathcal {D}_{\\text{train}}^{k}$ into a possible future distribution, denoising the training samples.\n\n\n\n% With regard to the test data $\\mathcal {D}_{\\text{test}}^{k}$, its posterior distribution should also approach the experienced distribution of training samples. \n% Similar to feature adaptation, we expect $H$ to map $\\mathcal P_{\\text{train}}^{k}(y| \\mathbf x)$ and $\\mathcal P_{\\text{test}}^{k}(y| \\mathbf x)$ into an agent conditional distribution $\\mathcal P_{\\text{agent}}^{k}(y| \\mathbf x)$. \n% The model output $\\check{y}$ on the two datasets will ideally follow a shared distribution, \\textit{i.e.}, $\\check{y} = F(\\mathbf x)\\sim \\mathcal P_{\\text{agent}}^{k}(y| \\mathbf x)$. Note that the ultimate goal is to make an accurate prediction $\\hat y$ that is close to the ground truth $y$ in the test data under $\\mathcal P_{\\text{test}}^{k}(y| \\mathbf x)$. \nDuring online inference, we need to inversely map the intermediate output $\\check{y}$ from $\\mathcal P_{\\text{agent}}^{k}(y| \\mathbf x)$ to $\\mathcal P_{\\text{test}}^{k}(y| \\mathbf x)$, where $\\check y = F(\\mathbf x)$. We define the inverse mapping function $H^{-1}$ by\n\\begin{equation}\n  \\hat y = H^{-1}(\\mathbf x, \\check y) := \\sum_{i=1}^{N} s_i\\cdot h^{-1}_{i}(\\check y),\n\\end{equation}\nwhere \n\\begin{equation}\n  h_i^{-1}(\\check y) = ({\\check y - \\beta_i})/{\\gamma_i}.\n\\end{equation}\nOne can seek further improvements on the implementation of each label adaptation head $h_i$ by various normalizing flows~\\cite{Normalizing_Flow_Survey}, which comprise a sequence of invertible mappings. Empirically, we show that the simple linear heads have already been effective in Sec.~\\ref{sec:ablation}.\n\nTo summarize, meta-parameters $\\psi$ of the data adapter ${DA}$ include the parameters of $G$ and $H$, \\textit{i.e.}, $\\psi = \\{\\mathbf W_i, \\mathbf b_i, \\mathbf p_i, \\gamma_i, \\beta_i\\}_{i=1}^{N}$. The data adapter transforms features of $\\mathcal {D}_{\\text {train}}^{k}$ and $\\mathcal {D}_{\\text {test}}^{k}$ by Eq.~(\\ref{eq:feature adaptation}), and transforms labels of $\\mathcal {D}_{\\text {train}}^{k}$ by Eq.~(\\ref{eq:label adaptation}). The adapted datasets $\\mathcal {\\widetilde D}_{\\text {train}}^{k}$ and $\\mathcal{\\widetilde D}_{\\text {test}}^{k}$ in Eq.~(\\ref{eq:adapted data}) can be formalized as\n\\begin{subequations}\\label{eq:final adapted data}\n  \\begin{align}\n  \\mathcal{\\widetilde D}_{\\text {train}}^{k}&=\\{(G(\\mathbf x), H(\\mathbf x, y))\\mid (\\mathbf x, y)\\in \\mathcal{D}_{\\text {train }}^{k}\\},\\label{eq:final adapted training data}\\\\ \n  \\mathcal{\\widetilde D}_{\\text {test }}^{k}&=\\{(G(\\mathbf x), y)\\mid (\\mathbf x, y)\\in \\mathcal{D}_{\\text {test}}^{k}\\}\\label{eq:final adapted test data},\n  \\end{align}\n\\end{subequations}\nwhere the ground-truth labels in $\\mathcal{\\widetilde D}_{\\text {test}}^{k}$ are known at the corresponding trading dates in the future.\n\n"
                },
                "subsection 4.3": {
                    "name": "Model Adapter",
                    "content": "\n\n% Model adapter ${MA}$ is another meta-learner to learn how to initialize a forecast model that can fast adapt to incremental data. \nModel adapter ${MA}$ is another meta-learner to provide a good initialization of model parameters for each IL task and then adapt the initial parameters to fit the incremental data. For the $k$-th IL task, ${MA}$ first assigns the forecast model $F$ with initial weights $\\phi^{k-1}$, and then fine-tunes $F({\\cdot;\\phi^{k-1}})$ on $\\mathcal{\\widetilde D}_{\\text{train}}^{k}$. The optimization objective is to reduce the training loss $\\mathcal{L}_{\\text{train}}$ which is defined as:\n\n\\begin{equation}\\label{eq:training loss}\n  \\mathcal{L}_{\\text{train}}=\\frac{1}{|\\mathcal{\\widetilde D}_{\\text{train}}^{k}|}\\sum_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}}^{k}} \\left(F(\\tilde{\\mathbf x};\\phi^{k-1})-\\tilde y\\right)^2.\n\\end{equation}\nWe adapt the initial parameters $\\phi^{k-1}$ and derive task-specific parameters $\\theta^{k}$ as follows:\n\\begin{equation}\\label{eq:task-specific param}\n  \\begin{aligned}  \n  {\\theta}^{k}&={MA}(\\mathcal {\\widetilde D}_{\\text{train}}^{k};\\phi^{k-1})\\\\\n                &=\\phi^{k-1} -\\eta_\\theta \\nabla_\\phi \\mathcal{L}_{\\text{train}}(\\mathcal {\\widetilde D}_{\\text {train}}^{k}; \\phi^{k-1}),\n  \\end{aligned}\n\\end{equation}\nwhere $\\eta_\\theta$ is the learning rate of the forecast model and we only perform one-step gradient updates for fast adaptation. Next, we deploy $F(\\cdot;{\\theta^{k}})$ online to make predictions on adapted test data $\\mathcal {\\widetilde D}_{\\text{test}}^{k}$. \n\nNote that \\model is a generic framework for any implementation of the model adapter. \nThe one-step gradient update can also be replaced by other model adaptation approaches, such as context-dependent gating~\\cite{context-dependent-gating} and transfer network~\\cite{SML}.\n\n"
                },
                "subsection 4.4": {
                    "name": "Optimization of Meta-learners",
                    "content": "\\label{sec:optimization}\nAt each test time, we can calculate the test error once the ground-truth labels of the test data are known. Formally, we obtain mean square error $\\mathcal L_{\\text{MSE}}$ to optimize our meta-learners before we start the next task, which is defined as follows:\n\\begin{equation}\n  \\mathcal L_{\\text{MSE}}=\\frac{1}{|\\mathcal{\\widetilde D}_{\\text {test}}^{k}|}\\sum_{(\\tilde{\\mathbf x}, y) \\in \\mathcal {\\widetilde D}_{\\text {test}}^{k}}\\left( H^{-1}\\circ F(\\tilde{\\mathbf x} ; \\theta^{k}, \\psi^{k-1}) - y\\right)^2.\n\\end{equation}\nAdditionally, we add a regularization term to avoid the adapted labels of $\\mathcal{\\widetilde D}_{\\text {train}}^{k}$ being abnormal and hard to learn. \nWe formulate the regularization loss $\\mathcal L_{\\text{reg}}$ by\n\\begin{equation}\n  \\mathcal L_{\\text{reg}}=\\frac{1}{|\\mathcal{D}_{\\text {train}}^{k}|}\\sum_{(\\mathbf{x}, y) \\in \\mathcal {D}_{\\text {train}}^{k}}\\left( H(\\mathbf{\\tilde x}, y; \\psi^{k-1}) - y\\right)^2.\n\\end{equation}\n% Under a normal distribution assumption~\\cite{DDGDA}, we can prove that $\\mathcal L_{\\text{MSE}}$ and $\\mathcal L_{\\text{reg}}$ are equivalent to $D_KL(\\mathcal Q^{k}(y|\\mathbf x)\\|\\mathcal P^{k}_{\\text{test}}(y|\\mathbf x))$.\nThe final test loss $\\mathcal L_{\\text{test}}$ is derived by\n\\begin{equation}\\label{eq:test loss}\n  \\mathcal L_{\\text{test}}=\\mathcal L_{\\text{MSE}}+\\alpha\\mathcal L_{\\text{reg}},\n\\end{equation}\nwhere $\\alpha$ is a hyperparameter to control the regularization strength. \nActually, $\\alpha\\mathcal L_{\\text{reg}}$ approximates the second-order gradients of the label adaptation layer (see derivations in Appendix ~\\ref{apx:approximation}).\n\nThe optimization of our model adapter ${MA}$ is formulated as:\n\\begin{equation}\n  {\\phi}^{k} = {\\phi}^{k-1}-\\eta_\\phi \\nabla_\\phi \\mathcal{L}_{\\text{test}}(\\mathcal {\\widetilde D}_{\\text {test}}^{k}; \\theta^{k}),\n\\end{equation}\nwhere $\\eta_\\phi$ is the learning rate of ${MA}$. The difference between traditional MAML~\\cite{MAML} and our method lies in that we only use one query set at each time to update the meta-learner. We are devoted to improving performance on the current test data because the stock market evolves and a large proportion of past data may not contain patterns that will reappear in the future. It is also practical in the IL setting where we only save samples of one task in memory.\n\nSimilarly, we optimize ${DA}$ according to actual test performance, using the following equation:\n% In the upper-level optimization, we also update parameters $\\psi$ of our data adapter ${DA}$ by\n\\begin{equation}\n  {\\psi}^{k} = {\\psi}^{k-1}-\\eta_\\psi \\nabla_\\psi \\mathcal{L}_{\\text{test}}(\\mathcal {\\widetilde D}_{\\text {test}}^{k}; \\theta^{k}),\n\\end{equation}\nwhere $\\eta_\\psi$ is the learning rate of ${DA}$. $\\mathcal {\\widetilde D}_{\\text {test}}^{k}$ and $\\theta^{k}$ are derived by Eq.~(\\ref{eq:final adapted training data}) and Eq.~(\\ref{eq:task-specific param}), respectively. \n\n\n"
                }
            },
            "section 5": {
                "name": "Training Procedure",
                "content": " \\label{sec:procedure}\nIn this section, we propose a two-phase training procedure of \\model including an offline training phase and an online training phase. Pseudo-codes are shown in Alg.~\\ref{alg:overall} and Alg.~\\ref{alg:task}.\n\n\\begin{algorithm}[b]\n  \\KwIn{Meta-train set $\\mathcal T_{\\text{train}}=\\{(\\mathcal D_{\\text {train }}^{k}, \\mathcal D_{\\text {test}}^{k})\\}_{k=1}^{K_0}$, meta-valid set $\\mathcal T_{\\text{valid}}=\\{(\\mathcal D_{\\text {train }}^{k}, \\mathcal D_{\\text {test}}^{k})\\}_{k=K_0+1}^{K}$ and meta-test set $\\mathcal T_{\\text{test}}=\\{(\\mathcal D_{\\text {train }}^{k}, \\mathcal D_{\\text {test}}^{k})\\}_{k=K+1}^{K^\\prime}$\n  % , where $\\operatorname{task}^{k}=\\left(\\mathcal D_{\\text {train }}^{k}, \\mathcal D_{\\text {test}}^{k})\\right)$\n  }\n  \\KwOut{Trained parameters $\\phi$, $\\psi$ and online predictions $\\{\\mathbf {\\hat Y}^{(t)}\\}_{t=T+1}^{T^\\prime}$}\n  \\BlankLine\n  \\caption{Overall training procedure}\\label{alg:overall}\n\n  Initialize $\\phi$ and $\\psi$\\;  \n  % Use $\\{D_{\\text {train }}^{(k\\tau)}\\}_{k=1}^{K_1}$ to warm up $\\theta$\\;\n  \n  \\tcc{Offline training phase}\n  \\Repeat{\\textnormal{the metric decreases for $\\zeta$ epochs}}{\n    $\\mathcal T_{\\text{train}}^\\prime \\leftarrow \\mathtt{shuffle}(\\mathcal T_{\\text{train}})$\\;\n    $\\phi$, $\\psi$, $\\{\\hat{\\mathbf Y}^{(t)}\\}_{\\scriptscriptstyle t=r+1}^{\\scriptscriptstyle (K_0+1)r}$$\\leftarrow \\mathtt{\\modelns}(\\phi, \\psi, \\mathcal T_{\\text{train}}^\\prime)$\\;\n    ${\\phi^\\prime}$, ${\\psi^\\prime} \\leftarrow$ make a temporary copy of $\\phi$ and $\\psi$\\label{alg:overall:copy}\\;\n    ${\\phi^\\prime}$, ${\\psi^\\prime}$, $\\{\\hat{\\mathbf Y}^{(t)}\\}_{\\scriptscriptstyle t=(K_0+1)r+1}^{\\scriptscriptstyle (K+1)r}$$\\leftarrow \\mathtt{\\modelns}({\\phi^\\prime}, {\\psi^\\prime}, \\mathcal T_{\\text{valid}})$\\label{alg:overall:eval}\\;\n    Calculate metric on $\\{\\hat {\\mathbf Y}^{(t)}, {\\mathbf Y}^{(t)}\\}_{\\scriptscriptstyle t=(K_0+1)r+1}^{\\scriptscriptstyle (K+1)r}$\\;\n  } \n  \\BlankLine\n  \\tcc{Online training phase from $T+1$ to $T^\\prime$}\n  $\\phi$, $\\psi$, $\\{\\hat{\\mathbf Y}^{(t)}\\}_{\\scriptscriptstyle t=H+(K+1)r}^{\\scriptscriptstyle T'}$$\\leftarrow \\mathtt{\\modelns}(\\phi, \\psi, \\mathcal T_{\\text{valid}}\\cup \\mathcal T_{\\text{test}})$\\label{alg:overall:online}\\; \n  Evaluate \\model by metric on $\\{\\hat{\\mathbf Y}^{(t)}, {\\mathbf Y}^{(t)}\\}_{t=T+1}^{T^\\prime}$\\;\n\n  \\Return $\\phi$, $\\psi$, $\\{\\hat{\\mathbf Y}^{(t)}\\}_{t=T+1}^{T^\\prime}$\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n  \\KwIn{$\\phi$, $\\psi$ and task set $\\mathcal T=\\{(\\mathcal D_{\\text {train }}^{k}, \\mathcal D_{\\text {test}}^{k})\\}_{k=m}^{n}$}\n  \\KwOut{$\\phi^{n}$, $\\psi^{n}$ and predictions $\\{\\hat{\\mathbf Y}^{(t)}\\}_{t=mr+1}^{(n+1)r}$}\n  \\BlankLine\n  \\caption{\\modelns}\\label{alg:task}\n\n  $\\phi^{(m-1)}, \\psi^{(m-1)} \\leftarrow \\phi$, $\\psi$\\;  \n  \\For{$k \\leftarrow m$ \\KwTo $n$}{\n    Assign forecast model $F$ with inital weights $\\phi^{k-1}$\\;\n\n    Predict on $\\widetilde{\\mathcal D}_{\\text {train }}^{k}$ defined by Eq.~(\\ref{eq:final adapted training data})\\;\n\n    Compute training loss by Eq.~(\\ref{eq:training loss})\\;\n    \n    Adapt the forecast model parameters by:\n    $$\n    {\\theta}^{k}=\\phi^{k-1}-\\eta_\\theta \\nabla_\\phi \\mathcal{L}_{\\text{train}}(\\mathcal {\\widetilde D}_{\\text {train}}^{k}; \\phi^{k-1});\\quad\n    $$\n\n    $\\{\\hat{\\mathbf Y}^{(t)}\\}_{t=kr+1}^{(k+1)r}$$\\leftarrow$ Predict on $\\widetilde{\\mathcal D}_{\\text {test}}^{k}$ defined by Eq.~(\\ref{eq:final adapted test data})\n\n    Compute test loss by Eq.~(\\ref{eq:test loss})\\;\n    \n    Update data adapter ${DA}$ and model adapter ${MA}$:\\label{alg:task:upper}\n    $$\n    {\\psi}^{k} = {\\psi}^{k-1}-\\eta_\\psi \\nabla_\\psi \\mathcal{L}_{\\text{test}}(\\mathcal {\\widetilde D}_{\\text {test}}^{k}; \\theta^{k}),\\quad\\quad\\quad\n    $$\n    $$\n    {\\phi}^{k} = {\\phi}^{k-1}-\\eta_\\phi \\nabla_\\phi \\mathcal{L}_{\\text{test}}(\\mathcal {\\widetilde D}_{\\text {test}}^{k}; \\theta^{k}).\\quad\\quad\\quad\n    $$\n  }\n  \\Return $\\phi^{n}$, $\\psi^{n}$, $\\{\\hat{\\mathbf Y}^{(t)}\\}_{t=mr+1}^{(n+1)r}$\n\\end{algorithm}\n\n\nGiven all historical data $\\{(\\mathbf X^{(t)}, \\mathbf Y^{(t)})\\}_{t=1}^{T}$ before deployment, we organize these offline data into $K$ incremental learning tasks to imitate online incremental learning. The $k$-th IL task consists of the $k$-th incremental data $\\mathcal D_{\\text{train}}^{k}=\\{(\\mathbf{X}^{(t)}, \\mathbf Y^{(t)})\\}_{t=(k-1)r+1}^{kr}$ and the $k$-th test data $\\mathcal D_{\\text{test}}^{k}=\\{(\\mathbf{X}^{(t)}, \\mathbf Y^{(t)})\\}_{t=kr+1}^{(k+1)r}$. We take the first $K_0$ tasks as meta-train set $\\mathcal T_{\\text{train}}$ and others as meta-valid set $\\mathcal T_{\\text{valid}}$. It is noteworthy that we can shuffle $\\mathcal T_{\\text{train}}$ to simulate arbitrary distribution shifts in pursuit of robustness against extreme non-stationarity.\n\n\\vspace{0.3em}\n\\noindent\\textbf{Offline training.} We pretrain the two meta-learners on $\\mathcal T_{\\text{train}}$ task by task for epochs. At the end of each epoch, we continue incremental learning on $\\mathcal T_{\\text{valid}}$ for evaluation (Alg.~\\ref{alg:overall}, L\\ref{alg:overall:eval}). The updates of the meta-learners on the meta-valid set are conducted on a temporary copy of the meta-learners (Alg.~\\ref{alg:overall}, L\\ref{alg:overall:copy}), in order to avoid information leakage. We perform early stopping when the evaluation metric on $\\mathcal T_{\\text{valid}}$ decreases for $\\zeta$ consecutive epochs, in order to avoid overfitting. The meta-learners get ready for online service after they have been incrementally updated on the whole validation set (Alg.~\\ref{alg:overall}, L\\ref{alg:overall:online}).\n\n\\vspace{0.3em}\n\\noindent\\textbf{Online training.} \n% \\sout{We deploy \\model at the start date of $\\mathcal T_{\\text{valid}}$ to further leverage the whole validation set. The online training phase commerces at date $T+1$ and may end at date $T^\\prime$, where we} \nWe deploy \\model online at date $T$ and incrementally update the meta-learners at the end of each online task (Alg.~\\ref{alg:task}, L\\ref{alg:task:upper}). As a special case, the meta-learners could keep the original adaptation ability of traditional MAML when the learning rates $\\eta_\\psi$ and $\\eta_\\phi$ are always set to zero during the online training phase. However, as the stock market is dynamically evolving, it is critical to continually consolidate the meta-learners with new knowledge, which is also empirically confirmed by our experimental results in Appendix~\\ref{sec:hyperparam}.\n\nIn case the meta-learners encounter the catastrophic forgetting problem, we can also restart offline training after a much larger interval. For example, we incrementally update the meta-learners every week and fully retrain them on an enlarged meta-train set after one-year incremental learning. Integrating the advantages of IL and RR, \\model is expected to achieve better performance. In this work, we focus on improving IL performance against distribution shifts and leave the combination of \\model and RR as future work. According to our experiments on two real-world stock datasets, \\model alone can outperform RR methods for a long period (\\textit{e.g.}, over 2.5 years) and hence it is unnecessary to frequently perform full retraining.\n\n\\vspace{0.3em}\n\\noindent\\textbf{Complexity analysis.} When optimizing the meta-learners (Alg.~\\ref{alg:task}, L\\ref{alg:task:upper}), we adopt the first-order approximation version of MAML ~\\cite{MAML} to avoid the expensive computation of Hessian matrices. Therefore, both the time and memory costs per IL task are linearly proportional to the size of incremental data. Formally, let $S$ denote the number of stocks in the market and $E$ denote the number of training epochs for RR methods till convergence. For the $k$-th online task, both the incremental data and the test data have $rS$ samples, while RR takes $(T+kr)S$ historical samples to train the model from scratch. Hence, the time complexity of DoubleAdapt and RR is $O(rS)$ and $O(E(T+kr)S)$, respectively. The scalability of \\model allows frequent updates on the forecast model, \\textit{e.g.}, on a weekly basis.\n\n"
            },
            "section 6": {
                "name": "Experiments",
                "content": "\n\nIn this section, we study our \\model framework with experiments, aiming to answer the following research questions:\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{RQ1}: How does our proposed \\model approach perform compared with the state-of-the-art methods?\n\\item \\textbf{RQ2}: How is the effect of different components in \\modelns?\n% \\item \\textbf{RQ3}: How does the proposed feature adaptation layer perform compared with the existing normalization method?\n\\item \\textbf{RQ3}: What is the empirical time cost of \\modelns?\n% \\item \\textbf{RQ4}: How does the performance of \\model vary with different settings of the hyperparameters?\n\\end{itemize}\nAn additional hyperparameter study is provided in Appendix~\\ref{sec:hyperparam}.\n\n",
                "subsection 6.1": {
                    "name": "Experimental Settings",
                    "content": "\n",
                    "subsubsection 6.1.1": {
                        "name": "Datasets",
                        "content": "\nWe evaluate our \\model framework on two popular real-world stock sets: CSI 300~\\cite{REST, Yoo2021, CMLF, HIST} and CSI 500~\\cite{REST, DDGDA} in the China A-share market. CSI 300 consists of the 300 largest stocks, reflecting the overall performance of the market. CSI 500 comprises the largest remaining 500 stocks after excluding the CSI 300 constituents, reflecting the small-mid cap stocks.\n\nWe use the stock features of Alpha360 in the open-source quantitative investment platform Qlib~\\cite{Qlib}. Alpha360 contains 6 indicators on each day, which are \\textit{opening price, closing price, highest price, lowest price, volume weighted average price} (VWAP) and \\textit{trading volume}. For each stock at date $t$, Alpha360 looks back 60 days to construct a 360-dimensional vector as the raw feature of this stock. We use the stock price trend defined in Definition~\\ref{def:trend} as the label for each stock. \n% Following Qlib, we use the stock data in CSI 300 and CSI 500 both from 01/01/2008 to 07/31/2020, and \nFollowing Qlib, we split stock data into training set (from 01/01/2008 to 12/31/2014), validation set (from 01/01/2015 to 12/31/2016), and test set (from 01/01/2017 to 07/31/2020). The features are normalized by moments of the whole training set, and the labels grouped by date are normalized by moments of data at the same date~\\cite{REST, HIST}. \n\n\n"
                    },
                    "subsubsection 6.1.2": {
                        "name": "Evaluation Metrics",
                        "content": "\nWe use four widely-used evaluation metrics: IC~\\cite{TRA}, ICIR~\\cite{AdaRNN}, Rank IC~\\cite{Li2019b}, and Rank ICIR~\\cite{HIST}. At each date $t$, $IC^{(t)}$ could be measured by\n\\begin{equation}\n    IC^{(t)} = \\frac{1}{N}\\frac{(\\hat{\\mathbf{Y}}^{(t)} - mean(\\hat{\\mathbf{Y}}^{(t)}))^{\\top} (\\mathbf{Y}^{(t)} - mean(\\mathbf{Y}^{(t)}))}{std(\\hat{\\mathbf{Y}}^{(t)})\\cdot std(\\mathbf{Y}^{(t)})},\n\\end{equation}\nwhere ${\\mathbf{Y}^{(t)}}$ are the raw stock price trends and $\\hat{\\mathbf{Y}}^{(t)}$ are the model predictions at each date. We report the average IC over all test dates.\nICIR is calculated by dividing the average by the standard deviation of IC. Rank IC and Rank ICIR are calculated by ranks of labels and ranks of predictions.\n% Following ~\\cite{DDGDA}, we use IC and ICIR as ranking metrics. At each date $t$, $IC^{(t)}$ could be measured by $IC^{(t)}=corr(rank(\\mathbf{Y}^{(t)}), rank(\\mathbf{\\hat Y}^{(t)}))$, where $\\mathbf{Y}^{(t)}$ are the ground truth of price trends, $\\hat{\\mathbf{Y}}^{(t)}$ are the predictions, $rank$ returns the rank of a given value within an array, and $corr$ is defined as\n% \\begin{equation}\n%     corr(\\mathbf Y_1, {\\mathbf{Y_2}}) = \\frac{1}{N}\\frac{({\\mathbf{Y_1}} - mean({\\mathbf{Y_1}}))^{\\top} (\\mathbf{Y_2} - mean(\\mathbf{Y_2}))}{std(\\mathbf{Y_1})\\cdot std(\\mathbf{Y_2})}.\n% \\end{equation}\n% We report the average IC over all test dates. ICIR is calculated by dividing the average by the standard deviation of IC. \nBesides, we also use two portfolio metrics, including the excess annualized return (Return) and its information ratio (IR). IR is calculated by dividing the excess annualized return by its standard deviation. \n% The benchmark index is SH000300 for CSI 300 and SH000905 for CSI 500. \nOur backtest settings adhere to Qlib's default strategy. \n\nWe ran each experiment 10 times and report the average results. \n% We provide the standard deviations in Appendix~\\ref{apx:std}. \nFor all six metrics, a higher value reflects better performance.\n\n\n"
                    },
                    "subsubsection 6.1.3": {
                        "name": "Baseline",
                        "content": "\nWe consider two kinds of model-agnostic retraining approaches as the comparison methods:\n\n\\vspace{0.3em}\n\\noindent\\textit{(1) Rolling retraining methods}:\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{RR}~\\cite{DDGDA}: RR, short for rolling retraining, periodically retrains a model on all available data with equal weights. \n%RR is analogous to TOE (training-on-everything) in online learning problems.\n\\item \\textbf{DDG-DA}~\\cite{DDGDA}: This method predicts the data distribution of the next time-step sequentially and re-weights all historical samples to generate a training set, of which the distribution is similar to the predicted future distribution. \n%DDG-DA improves RR by adapting to the new concept \\textit{before} concept drift happens.\n\\end{itemize}\n\\textit{(2) Incremental learning methods}:\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{IL}: This method is a na\\\"ive incremental learning baseline to fine-tune the model only with the recent incremental data by gradient descent~\\cite{OGD}.\n\\item \\textbf{MetaCoG}~\\cite{CML}: This method introduces a per-parameter mask to select task-specific parameters according to the context. \nMetaCoG updates the masks rather than the model parameters to avoid catastrophic forgetting.\n\\item \\textbf{C-MAML}~\\cite{OSAKA}: This method follows MAML~\\cite{MAML} to pretrain slow weights that can produce fast weights to accommodate new tasks. At the online time, C-MAML keeps fine-tuning the fast weights until a distribution shift is detected, and then the slow weights are updated and used to initialize new fast weights.\n%by monitoring the difference in loss. C-MAML updates the slow weights with new knowledge only \\textit{after} a shift has been detected. \n\\item \\textbf{\\modelns}: Our proposed method. \\model learns to initialize parameters and, notably, learns to adapt features and labels, alleviating the distribution shift issue.\n\\end{itemize}\nNote that there are few studies on incremental learning for stock trend forecasting. We borrow MetaCoG and C-MAML from the continual learning problem as IL-based baselines.\n\n"
                    },
                    "subsubsection 6.1.4": {
                        "name": "Implementation Details",
                        "content": "\\label{sec:details}\n%For all the baselines, we employ a specific forecast model selected from Qlib's model zoo, which takes the recommended hyperparameters for a fair comparison.  \nThe time interval of two consecutive tasks is 20 trading days~\\cite{HIST}. The batch size of RR-based methods approximates the number of samples in incremental data, \\textit{i.e.}, 5000 for CSI 300 and 8000 for CSI 500. We apply Adam optimizer with an initial learning rate of 0.001 for the forecast model of all baselines, 0.001 for our model adapter, and 0.01 for our data adapter. We perform early stopping when IC decreases for 8 consecutive epochs. The regularization strength $\\alpha$ of \\model is 0.5. The head number $N$ is 8 and the temperature $\\tau$ is 10. Other hyperparameters of the forecast model (\\textit{e.g.}, dimension of hidden states) keep the same for a fair comparison. We use the \\textbf{first-order approximation} version of MAML~\\cite{MAML} for all meta-learning methods. \n\n\n\n"
                    }
                },
                "subsection 6.2": {
                    "name": "Performance Comparison (RQ1)",
                    "content": "\n\nWe instantiate the forecast model by four deep neural networks, including Transformer~\\cite{Transformer}, LSTM~\\cite{LSTM}, ALSTM~\\cite{ALSTM}, and GRU~\\cite{GRU}. Table~\\ref{tab:main_result} compares the overall performance of all baselines. Though RR methods are strong baselines and often beat simple IL methods, our proposed \\model framework achieves the best results in almost all the cases, demonstrating that \\model can make more precise predictions in stock trend forecasting. Exceptionally, C-MAML sometimes achieves higher ICIR or Rank ICIR than \\model on CSI 500. During the online training phase, C-MAML modulates the learning rate of its meta-learner according to the test error and thus achieves more stable performance on daily IC. Note that this update modulation is orthogonal to our work and can also be integrated into \\modelns. We also observe that MetaCoG is nearly the worst method that even performs worse than na\\\"ive IL. MetaCoG focuses on catastrophic forgetting issues and selectively masks the model parameters, instead of consolidating the parameters with new knowledge acquired online. This observation confirms the significance of online updates for stock trend forecasting. \nAs for the implementation of the forecast model, \\model can consistently achieve better performance with a stronger backbone.\n% \\model also achieves the best portfolio metrics in all the cases except for ALSTM on CSI 300. \n% We would like to clarify that the portfolio results also depend on the investment strategy. \n% C-MAML sometimes shows poor performance due to one-sided model adaptation which is susceptible to distribution shifts. \n% We also notice that RR often lags behind IL, verifying our assumption that a large proportion of past data may not contain patterns that will reoccur in the future.\n% Second, IL outperforms RR on CSI 500 but fails on CSI 300. We conjecture that the trends of small-caps stocks in CSI 500 are less volatile and the incremental data contain most future patterns. Moreover, DDG-DA which is based on RR performs worse than IL-based methods and even worse than RR on CSI 500. We explain that DDG-DA uses a proxy linear model which highly relies on the data quality, while the input features are the simplest indicators and the raw information is hard for a linear model to understand. \n% Second, MetaCoG and C-MAML are not competent in stock trend forecasting because they focus on catastrophic forgetting which is not the main issue in our problem. \n% MetaCoG can even perform worse than IL due to the limitation of merely selectively masking the parameters rather than consolidating the pretrained parameters with new knowledge.\n\n\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Ablation Study (RQ2)",
                    "content": "\\label{sec:ablation}\n\nIn this section, we investigate the effects of the key components in \\modelns. Note that IL is also a special case of \\model when ${DA}$ always provides identity mappings and $\\phi^{k-1}$ is directly updated into $\\theta^{k}$ instead of performing gradient descent. We introduce some variants by equipping IL with one or several components. Besides, as the performance varies in different degrees of distribution shifts, we also evaluate the online predictions under different kinds of shifts. To this end, we pretrain the model $F$ on the training set and adopt na\\\"ive IL throughout the tasks. In the $k$-th task of the test set, we first use $F$ to do inference on $\\mathcal D^{k}_{\\text{test}}$ without training on $\\mathcal D^{k}_{\\text{train}}$, resulting in a mean square error $\\mathcal L^{k}_{1}$. Then we update $F$ on $\\mathcal D^{k}_{\\text{train}}$ and infer the same test samples, resulting in a new mean square error $\\mathcal L^{k}_{2}$. The distribution shift in this task can be measured by $\\Delta \\mathcal L^{k} = \\mathcal L^{k}_2 - \\mathcal L^{k}_1$ which can reflect whether $F$ benefits from the incremental update. When $\\mathcal P^{k}_{\\text{test}}$ is similar to $\\mathcal P^{k}_{\\text{train}}$, $\\mathcal L^{k}_2$ should be smaller than $\\mathcal L^{k}_1$, and vice versa. With all online tasks sorted by $\\Delta \\mathcal L^{k}$ in an ascending order, we take the first 25\\% tasks as cases of gradual shifts and the last 25\\% tasks as cases of abrupt shifts. \n\n\nTable~\\ref{tab:ablation} shows the average results over the two kinds of distribution shifts on CSI 300, \\textit{i.e.}, gradual shifts and abrupt shifts. Applying both ${MA}$ and ${DA}$ into IL, \\model achieves the best results against different kinds of distribution shifts.\nIn the cases of gradual shifts, the multi-head version with better expressiveness significantly outperforms the single-head one. Nevertheless, the multi-head version performs worse in the cases of abrupt shifts because its high complexity is a double-edged sword and may incur overfitting issues. As gradual shift is the major issue in stock data~\\cite{DDGDA}, the multi-head version still achieves the best overall performance. \nAlso, it is noteworthy that our proposed data adaptation effectively facilitates model adaptation, and data adaptation alone also beats one-sided model adaptation. On the other hand, the improvement of \\model over $IL$+$DA$, especially under abrupt shifts, indicates that the initial parameters of each task are also critical to generalization ability. \n\nMoreover, either the data adaptation or the model adaptation outperforms the most competitive methods (\\textit{i.e.,} DDG-DA and C-MAML) in Table~\\ref{tab:main_result}. Specifically, IL+DA outperforms DDG-DA by 5.6\\% improvement on IC, and IL+MA outperforms C-MAML by 3.3\\% improvement on IC. In terms of model adaptation, C-MAML proposes additional modules to deal with catastrophic forgetting in general continual learning problems. However, future stock trends are mainly affected by recent stock trends, and it is more beneficial to learn new patterns from recent data rather than memorize long-term historical patterns. Thus, our simple variant IL+MA achieves better performance than C-MAML.\n% Besides, our multi-head version defeats the single-head version in most cases, especially against gradual shifts, demonstrating the expressiveness of our proposed multi-head adaptation layers. Nevertheless, the single-head version achieves the highest metrics against abrupt shifts\n\n% It will be desirable to apply the single-head version in a more non-stationary environment.\n\n\n"
                },
                "subsection 6.4": {
                    "name": "Time Cost Study (RQ3)",
                    "content": "\n\n\nTable~\\ref{tab:time cost} compares the total time costs in offline training and online training of different methods. IL methods show superior efficiency compared with RR methods in either offline training or online training. As we adopt the first-order approximation of MAML, we avoid the expensive computation of Hessian matrices. Thus the meta-learning methods are not much slower than na\\\"ive IL, and the excess time cost is small enough to omit. It is practical for model selection, hyperparameter tuning, and retraining algorithm selection. Moreover, the low time cost of \\model in offline training paves the way for collaboration with RR, \\textit{e.g.}, periodically retraining the meta-learners once a year.\n\n\n"
                }
            },
            "section 7": {
                "name": "Related Work",
                "content": "\n\n",
                "subsection 7.1": {
                    "name": "Stock Trend Forecasting",
                    "content": "\nProfitable quantitative investment usually depends on precise predictions of stock price trends. In recent years, great efforts~\\cite{SFM, Li2018StockPP, Zhao2018, HAN2018, Yoo2021, REST, HIST} have been devoted to designing deep-learning approaches to capture intricate finance patterns, while only a few works study online learning for stock data. \n% It is well accepted that time series including stock data are in a non-stationary environment where the data distribution shifts over time. \nMASSER~\\cite{MASSER}, which mainly focuses on stock movement prediction in the offline setting, introduces Bayesian Online Changepoint Detection in online experiments to detect distribution shifts and updates its meta-learner \\textit{after} a detection. Such delayed updates can easily lead to inferior performance in online inference. DDG-DA~\\cite{DDGDA}, an advanced RR method, copes with distribution shifts by predicting the future data distribution and resampling the training data for a similar distribution. DDG-DA adapts the training data by assigning samples grouped by periods with different weights \\textit{before} a distribution shift happens. By contrast, our data adaptation transforms the features and the label of each sample in a fine-grained way.\n% Among them, ~\\cite{SFM} proposes a variant of LSTM to discover multiple frequency patterns in stock market data,\n% ~\\cite{Li2018StockPP} proposes a multi-input LSTM to denoise information of low-correlated factors by convincing factors, \n% ~\\cite{HAN2018} proposes a hybrid attention network to predict the stock trend based on the sequence of recent related news, and ~\\cite{REST} proposes a relational event-driven stock trend forecasting framework to learn the event effect under different contexts and propagate the effect via a stock graph. \n% ~\\cite{HIST} considers the cross-stock relationships and mine the concept-oriented shared information based on a graph neural network.\n\n"
                },
                "subsection 7.2": {
                    "name": "Meta-Learning",
                    "content": "\nMeta-learning aims to fast adapt to new tasks only with a few training samples, dubbed \\textit{support set}, and generalize well on test samples, dubbed \\textit{query set}. \n% In meta-learning settings, the training samples and the test samples of each task are referred to as \\textit{support set} and \\textit{query set}, respectively. \nMAML~\\cite{MAML} is the most widely adopted to learn how to fine-tune for good performance on query sets. \n% Due to its success in generalization over various tasks, \nSome works~\\cite{FTML, MOLe, W&H} extend MAML to online settings on the assumption that the support set and the corresponding query set come from the same context, \\textit{i.e.}, following the same distribution. As such, the meta-learner will quickly remember task-specific information and perform well on a similar query set. However, this assumption cannot hold when discrepancies between the two sets are non-negligible. LLF~\\cite{LLF} studies MAML in an offline setting, proving that a predictor optimized by MAML can generalize well against concept drifts. However, the query sets are unlabeled in online settings, and one can only retrain the meta-learner after detecting a shift~\\cite{OSAKA}. Consequently, the predictions are still susceptible to distribution shifts. Some methods~\\cite{SML, ROLAND} combine incremental learning and meta-learning for recommender systems and dynamic graphs but ignore distribution shifts.\nSML~\\cite{SML} focuses on model adaptation and proposes a transfer network to convert model parameters on incremental data, which is orthogonal to our work.\n\n% Although existing meta-learning methods for continual learning can succeed in the computer vision area, their assumption cannot hold in the stock data under distribution shifts where discrepancies between the support set and the query set \n% (\\textit{i.e.}, incremental data and test data in our study) \n\n% In light of such limitations, it is non-trivial to apply meta-learning to stock trend forecasting.\n\n% Other existing works turn to normalization techniques to erase the distribution difference, such as batch normalization~\\cite{BN}, instance normalization~\\cite{InstanceNorm}, and TaskNorm~\\cite{Tasknorm}. However, in the meanwhile time, they also discard the valuable context information which is crucial to online updating. A manually crafted normalization manner cannot work consistently for all forecast models. It is also hard to find a sweet spot between reserving and discarding all the context information. This limitation motivates us to automatically learn how to adapt data to reduce distribution shifts and simultaneously reserve critical information.\n\n"
                }
            },
            "section 8": {
                "name": "Conclusion",
                "content": "\nIn this work, we propose \\modelns, a meta-learning approach to incremental learning for stock trend forecasting. We give two key insights to handle distribution shifts. First, we learn to adapt data into a locally stationary distribution in a fine-grained way. Second, we learn to assign the forecast model with initial parameters which can fast adapt to incremental data and still generalize well against distribution shifts. Experiments on real-world datasets demonstrate that \\model is generic and efficient, achieving state-of-the-art predictive performance in stock trend forecasting. \n\nIn the future, we will try to combine\n% apply our generic framework to a wide range of forecast models, \\textit{e.g.}, graph neural networks~\\cite{HIST}, and event-driven methods~\\cite{REST}. \nour incremental learning algorithm and rolling retraining to avoid catastrophic forgetting issues after a long-period online incremental learning. \nWe also believe the idea of the two-fold adaptation can inspire other applications that encounter the challenge of complex distribution shifts.\n% \\model can also be extended to other non-stationary environments, such as time series prediction.\n  \n% \\section{Acknowledgments}\n\\begin{acks}\nThis  work is supported by the National Key Research and Development Program of China  (2022YFE0200500), Shanghai Municipal Science and Technology Major Project  (2021SHZDZX0102), and SJTU Global Strategic Partnership Fund (2021 SJTU-HKUST).\n\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{online.bib}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\clearpage\n\\appendix\n\n\n% \\section{Pseudo Codes}\\label{apx:pseudo}\n% In this section, we provide our pseudo codes of the overall training procedure and \\model in Alg.~\\ref{alg:overall} and Alg.~\\ref{alg:task}, respectively.\n\n\n"
            },
            "section 9": {
                "name": "Implementation Details",
                "content": "\nIn practice, we employ shared transformation parameters for all time steps of the features when the stock data are taken as time series. Formally, let $\\mathbf x = \\{x_1, x_2, \\cdots, x_L\\}$ represent the stock feature, where $L$ is the length of time series. Each element $x_j\\in \\mathbb R^{d}$ is the feature at step $j$, where $d$ is the number of indicators at each time step. The parameters of the $i$-th transformation head is $\\mathbf W_i\\in \\mathbb R^{d\\times d}$ and $\\mathbf{b}_i \\in \\mathbb R^{d}$. At each time step $j$, we first calculate the cosine similarity $\\hat s_{ij}$ between each pair of $x_j$ and $\\mathbf p_i$, where $\\mathbf p_i \\in \\mathbb R^{d}$. Then, we perform softmax operation over the $N$ heads to obtain the confidence score $s_{ij}$. As such, features of $\\mathbf x$ at all time steps are fed into the shared feature adaptation layer with different scores. It is practical to avoid gradient vanishing issues in RNNs. Otherwise, the gradients of more previous steps are too small to train unshared adaptation parameters. \n\nAs for the label adaptation layer, we first employ a linear transformation matrix to cast $\\mathbf x\\in \\mathbb{R}^{Ld}$ into a low-dimensional vector $\\mathbf v$. The confidence score for head selection is calculated by the cosine similarity between $\\mathbf v$ and $\\mathbf p^{\\prime}_i$, where $\\mathbf p^{\\prime}_i$ is a specific prototype for label adaptation and has the same dimension with $\\mathbf v$.\n\n\n"
            },
            "section 10": {
                "name": "Hyperparameter Study",
                "content": "\\label{sec:hyperparam}\n\n\nIn this section, we evaluate the performance of \\model on different values of hyperparameters. We only provide the results on CSI 300 with the forecast model implemented by GRU. The results on CSI 300 are similar.\n\n\\vspace{0.3em}\\noindent\\textbf{Task interval $r$. } \nFigure~\\ref{fig:hyper:r} shows that more frequent incremental learning with a smaller interval $r$ (\\textit{e.g.}, 5 trading days) can lead to better performance. This is because up-to-date patterns are more informative to future trends than previous ones, and it is beneficial to consolidate the forecast model and the meta-learners with new knowledge more timely. It is noteworthy that RR methods with a smaller $r$ will suffer from much more expensive time consumption. In contrast, \\model shows superiority in its scalability and is applicable to a small $r$. \n\n\\vspace{0.3em}\\noindent\\textbf{Softmax temperature $\\tau$.}\nAs shown in Figure~\\ref{fig:hyper:tau}, DoubleAdapt achieves the worst performance when we set the softmax temperature $\\tau$ to 0.5. \nWe reason that the logits from softmax with a small temperature approximate a one-hot vector, \\textit{i.e.}, one sample is transformed by merely one head. Thereby, the adaptation layers are trained by fewer samples and can suffer from underfitting issues. Fortunately, it is always safe to set the temperature with a great value and even towards infinity, since a single-head version of \\model still achieves outstanding performance in Table~\\ref{tab:ablation}. \n\n\\vspace{0.3em}\\noindent\\textbf{Number of transformation heads $N$.} \nAs shown in Figure~\\ref{fig:hyper:head}, almost all the multi-head versions of \\model outperform the single-head one. Exceptionally, the two-head version shows lower average IC but achieves the highest ceiling value. With a carefully selected temperature (\\textit{e.g.}, 1.0), the multi-head version can outperform the single-head one by a larger margin. Note that our incremental learning framework is efficient, and the time cost for hyperparameter tuning is durable. Besides, it is not recommended to use too many heads which can result in more time cost and may also cause overfitting issues. \n% In the case of 10 transformation heads, $DA$ of high complexity cannot achieve stable performance due to overfitting issues. In general, our data adaptation layers are simple yet effective.\n\n\\vspace{0.3em}\\noindent\\textbf{Regularization strength $\\alpha$.}\nFigure~\\ref{fig:hyper:lr} shows that \\model without regularization achieves inferior performance, which verifies our proposed regularization technique. Nevertheless, the IC performance gradually decreases with a larger regularization strength which hinders the proposed label adaptation. Thus, it is desirable to set moderate regularization to train \\model smoothly. Generally, \\model is relatively not sensitive to the regularization strength $\\alpha$.\n% It is desirable to find an appropriate regularization strength for a higher ceiling value of IC.\n\n\n\n\\vspace{0.3em}\\noindent\\textbf{Online learning rates $\\eta_\\phi$ and $\\eta_\\psi$.} As shown in Figure~\\ref{fig:hyper:tau}, we evaluate the performance on different learning rates of the two meta-learners during the online training phase, while we use the same pretrained meta-learners before online deployment. \nWhen we freeze the meta-learners online with both $\\eta_\\phi$ and $\\eta_\\psi$ set to zero, \\model achieves the worst results. This confirms that it is critical to continually consolidate the meta-learners with new knowledge acquired online. \nNevertheless, \\model still keeps the best performance when we freeze the data adapter with a zero $\\eta_\\psi$ but fine-tune the model adapter with an appropriate $\\eta_\\phi$ (0.0005 or 0.001). We conjecture that the pretrained data adapter has learned high-level patterns of distribution shifts, which are shared by the majority of the meta-test set. By contrast, the forecast model should accommodate the up-to-date trends of the stock markets which may not exist in the historical data. This requires the model adapter to be continually fine-tuned on recent incremental data in the online training phase. Otherwise, \\model shows inferior performance due to a zero $\\eta_\\phi$, even if we fine-tune the data adapter with a positive $\\eta_\\psi$. Besides, as shown in the rightmost column and the bottom row of Figure~\\ref{fig:hyper:lr}, the performance degrades with too large learning rates due to catastrophic forgetting issues. It is suggested to tune the hyperparameters of online learning rates on the meta-valid set by using the same pretrained meta-learners for efficiency.\n\n\n\n\\eat{\n\\begin{figure*}[ht]\n\t\\centering\n  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t\\subcaptionbox{Task interval\\label{fig:hyper:r}}\n\t{\n    \\includegraphics[width=.17\\linewidth]{step.pdf}\n  }\n  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t\\subcaptionbox{Number of heads\\label{fig:hyper:head}}\n\t{\n    \\includegraphics[width=.18\\linewidth]{head.pdf}\n  }\n  % \\\\\\vspace{0.5cm}\n  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t\\subcaptionbox{Temparature\\label{fig:hyper:tau}}\n\t{\n    \\includegraphics[width=.18\\linewidth]{tau.pdf}\n  }\n  % \\vspace{5pt}\n  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t\\subcaptionbox{Regularization\\label{fig:hyper:reg}}\n\t{\n    \\includegraphics[width=.18\\linewidth]{reg.pdf}\n  }\n  % \\vspace{5pt}\n  % \\\\\\vspace{0.5cm}\n  \\captionsetup[subfigure]{margin={0pt, 0pt}}\n\t\\subcaptionbox{Online learning rates\\label{fig:hyper:lr}}\n\t{\n    \\includegraphics[width=.18\\linewidth]{lr.pdf}\n  }\n\t\\caption{Performance comparison under different hyperparameters on CSI 300. Outliers are marked in diamonds.}\n  \\label{fig:hyper}\n\\end{figure*}\n}\n\n\n\n"
            },
            "section 11": {
                "name": "Approximation of Meta-gradients",
                "content": "\nConsidering efficiency, we apply first-order approximation in the upper-level optimization of the meta-learners, avoiding the expensive computation of high-order gradients.\n\nTo simplify notations, we omit the superscript of the parameters in the following derivations, \\textit{i.e.}, $\\theta$ for $\\theta^{k}$, $\\phi$ for $\\phi^{k-1}$, and $\\psi$ for $\\psi^{k-1}$. We further distinguish the parameters of the feature adaptation layer and the label adaptation layer as $\\psi_x$ and $\\psi_y$, respectively. We also use $\\eta$ to represent $\\eta_\\theta$. \n\n\n\n",
                "subsection 11.1": {
                    "name": "Gradients of Model Adapter",
                    "content": "\n\nThe gradients of $\\phi$ can be derived by the chain rule:\n\\begin{equation}\n  \\begin{aligned}\n  \\nabla_\\phi \\mathcal{L}_{\\text{test}}(\\theta )\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\frac{\\partial \\theta }{\\partial \\phi}\\\\\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\frac{\\partial(\\phi -\\eta \\nabla_\\phi \\mathcal{L}_{\\text{train}}(\\phi ))}{\\partial \\phi}\\\\\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\left(1-\\eta\\frac{\\partial }{\\partial \\phi}\\nabla_\\phi \\mathcal{L}_{\\text{train}}(\\phi )\\right)\\\\\n  \\end{aligned}\n\\end{equation}\n\nFollowing MAML~\\cite{MAML}, we assume that the second-order gradient is small enough to omit. Thus, we can derive the first-order approximation by\n\\begin{equation}\n\\nabla_\\phi \\mathcal{L}_{\\text{test}}(\\theta)\\approx \\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta)}{\\partial \\theta}=\\nabla_\\theta \\mathcal{L}_{\\text{test}}(\\theta).\n\\end{equation}\n\n"
                },
                "subsection 11.2": {
                    "name": "Gradients of Data Adapter",
                    "content": "\\label{apx:approximation}\n\nAs shown in Figure~\\ref{fig:computation graph}, the adaptation on features and predictions in the test data involves first-order gradients which directly optimize $\\psi_x$ and $\\psi_y$. Inspired by~\\cite{MPL}, we can also estimate the second-order gradient of $\\psi_y$ introduced by $\\mathcal L_{\\text{train}}$. As we now focus on the second-order gradient, we leave out the first-order one in $\\nabla_{\\psi_y} \\mathcal{L}_{\\text{test}}(\\theta )$ in the following derivations. \n\nFirst, we refomulate the task-specific parameter $\\theta$ (\\textit{i.e.}, $\\theta^{k}$) by\n\\begin{equation}\n\\theta = \\phi - \\eta \\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y).\n\\end{equation}\nAs $\\phi$ is independent of $\\psi_y$, we derive the second-order gradients of $\\psi_y$ by\n\n\\begin{equation}\\label{eq:psi_y gradient}\n  \\begin{aligned}\n  \\nabla_{\\psi_y} \\mathcal{L}_{\\text{test}}(\\theta )\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\frac{\\partial \\theta }{\\partial \\psi_y}\\\\\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\frac{\\partial(\\phi -\\eta \\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))}{\\partial {\\psi_y}}\\\\\n  =&-\\eta\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\left(\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\right)\\\\\n  \\end{aligned}\n\\end{equation}\nNote that $\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }$ is actually a Monte Carlo approximation of $\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\sim \\mathcal{P}_{\\text{agent}}(\\mathbf x, y) }$. We have\n\\begin{equation} \\label{eq:second-order gradient}\n\\begin{aligned}\n  &\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\\\\n  =&\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\sim \\mathcal{P}_{\\text{agent}}(\\mathbf x, y) }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\\\\n  =&\\mathbb E_{\\tilde{\\mathbf x}\\sim\\mathcal P_{\\text{agent}}(\\mathbf x)}\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{\\tilde y\\sim \\mathcal P_{\\text{agent}}(y|\\mathbf x)}\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)),\n  \\end{aligned}\n\\end{equation}\nwhere we extract the expectation on $\\tilde{\\mathbf x}$ out of the gradient on $\\psi_y$ as $\\tilde{\\mathbf x}$ is independent of $\\psi_y$.\n\nSince $\\nabla_\\phi\\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))$ has no dependency on $\\psi_y$, except for via $\\tilde y$, we can apply the REINFORCE rule~\\cite{REINFORCE} to achieve\n\\begin{equation} \\label{eq:REINFORCE}\n\\begin{aligned}\n&\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{\\tilde y\\sim \\mathcal P_{\\text{agent}}(y|\\mathbf x)}\\left[\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))\\right]\\\\\n=&\\mathbb E_{\\tilde y\\sim \\mathcal P_{\\text{agent}}(y|\\mathbf x)}\\left[\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\cdot \\frac{\\partial \\log P(\\tilde y\\mid\\mathbf x,y;\\psi_y)}{\\partial \\psi_y}\\right],\n\\end{aligned}\n\\end{equation}\nwhere $P(\\tilde y\\mid\\mathbf x,y;\\psi_y)$ is the probability density of $\\mathcal P_{\\text{agent}}(y|\\mathbf x)$. \n\nAssume the conditional probability follows a normal distribution, \\textit{i.e.}, $P(\\tilde y \\mid \\mathbf x,y) = \\mathcal N(y,\\sigma^2)$, where $\\sigma$ is a constant. We have\n\\begin{equation} \\label{eq:logMSE}\n\\begin{aligned}\n\\frac{\\partial \\log P(\\tilde y|\\mathbf x,y;\\psi_y)}{\\partial \\psi_y}\n=&\\frac{\\partial \\left(-\\frac{1}{2} \\log \\left(2 \\pi \\sigma^2\\right)-\\frac{1}{2 \\sigma^2}\\left(\\tilde y-y\\right)^2\\right)}{\\partial \\psi_y}\\\\\n=&\\frac{\\partial \\left(-\\frac{1}{2} \\log \\left(2 \\pi \\sigma^2\\right)-\\frac{1}{2 \\sigma^2}\\mathcal{L}_{\\text{MSE}}(\\tilde y, y)\\right)}{\\partial \\psi_y}\\\\\n=&-\\frac{1}{2\\sigma^2}\\frac{\\partial \\mathcal{L}_{\\text{MSE}}(\\tilde y, y)}{\\partial \\psi_y}\n\\end{aligned}\n\\end{equation}\nThen Eq.~(\\ref{eq:second-order gradient}) becomes\n\\begin{equation} \\label{eq:flatten second-order gradient} \n\\begin{aligned}\n&\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\\\\n=&\\frac{-1}{2\\sigma^2}\\mathbb E_{\\tilde{\\mathbf x}\\sim\\mathcal P_{\\text{agent}}(\\mathbf x)}\\mathbb E_{\\tilde y\\sim \\mathcal P_{\\text{agent}}(y|\\mathbf x)}\\left[\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))\\frac{\\partial \\mathcal{L}_{\\text{MSE}}(\\tilde y, y)}{\\partial \\psi_y}\\right]\\\\\n=&\\frac{-1}{2\\sigma^2}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\left[\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))\\frac{\\partial \\mathcal{L}_{\\text{MSE}}(\\tilde y, y)}{\\partial \\psi_y}\\right]\\\\\n=&\\frac{-1}{2\\sigma^2}\\nabla_\\phi \\mathcal L_{\\text{train}}(\\phi)\\frac{\\partial\\ \\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\mathcal{L}_{\\text{MSE}}(\\tilde y, y)}{\\partial \\psi_y},\\\\\n=&\\frac{-1}{2\\sigma^2}\\nabla_\\phi \\mathcal L_{\\text{train}}(\\phi)\\frac{\\partial\\mathcal{L}_{\\text{reg}}}{\\partial \\psi_y},\\\\\n\\end{aligned}\n\\end{equation}\nwhere $\\mathcal{L}_{\\text{MSE}}(\\tilde y, y)$ is only calculated on the training samples, and the expectation is actually the regularization loss $\\mathcal L_{\\text{reg}}$ in our paper. Then, we substitute Eq.~(\\ref{eq:flatten second-order gradient}) into Eq.~(\\ref{eq:psi_y gradient}) to obtain\n\\begin{equation}\n\\begin{aligned}\n    \\nabla_{\\psi_y} \\mathcal{L}_{\\text{test}}(\\theta )\n  =\\frac{\\eta}{2\\sigma^2}[\\nabla_\\theta\\mathcal{L}_{\\text{test}}(\\theta)]^\\top\\cdot\\nabla_\\phi \\mathcal L_{\\text{train}}(\\phi)\\cdot \\frac{\\partial \\mathcal{L}_{\\text{reg}}}{\\partial \\psi_y}.\n\\end{aligned}\n\\end{equation}\nWith Taylor expansion, we have\n\\begin{equation} \n\\mathcal L_{\\text{test}}(\\phi)\\approx\\mathcal L_{\\text{test}}(\\theta)+[\\nabla_\\theta\\mathcal{L}_{\\text{test}}(\\theta )]^\\top(\\phi-\\theta).\n\\end{equation}\nNote that\n\\begin{equation} \n\\theta=\\phi-\\eta\\nabla_\\phi\\mathcal L_{\\text{train}}(\\phi).\n\\end{equation}\nThus, \n\\begin{equation} \n\\mathcal L_{\\text{test}}(\\phi)-\\mathcal L_{\\text{test}}(\\theta)\\approx\\eta[\\nabla_\\theta\\mathcal{L}_{\\text{test}}(\\theta)]^\\top\\nabla_\\phi \\mathcal L_{\\text{train}}(\\phi)\n\\end{equation}\nFinally, the second-order gradient of $\\psi_y$ is approximated by\n\\begin{equation} \n\\nabla_{\\psi_y} \\mathcal{L}_{\\text{test}}(\\theta )=\\frac{\\mathcal L_{\\text{test}}(\\phi)-\\mathcal L_{\\text{test}}(\\theta)}{2\\sigma^2}\\cdot\\frac{\\partial \\mathcal{L}_{\\text{reg}}}{\\partial \\psi_y},\n\\end{equation}\nwhere we set $\\sigma$ as a hyperparameter. Thus the coefficient of gradients by $\\mathcal L_{\\text{reg}}$ is similar to $\\alpha$ which controls the regularization strength when $\\mathcal L_{\\text{test}}(\\phi)\\ge\\mathcal L_{\\text{test}}(\\theta)$. If otherwise, the updated parameters result in a greater loss, meaning that the incremental data is noisy and perhaps we should make the adapted labels $\\tilde y$ more different from the original $y$.\n\n% \\begin{figure*}[!htbp]\n\n%   \\centering\n%   % \\setlength{\\abovecaptionskip}{5pt}\n%   % \\setlength{\\belowcaptionskip}{-10pt}\n%   \\includegraphics[width=\\linewidth]{workflow.pdf}\n%   \\caption{Detailed illustration of data adaptation.}\n%   \\label{fig:workflow}\n% \\end{figure*}\n\n% }\n% \\section{Additional Figures}\n% As shown in Figure~\\ref{fig:workflow}, we also provide a detailed illustration of our proposed data adaptation.\n\n\n\n\n\\eat{\n\\section{Further Experimental Results}\\label{apx:exp}\n% \\subsection{Hyperparameter Sensitivity}\n\n\n\\subsection{Standard Deviations}\\label{apx:std}\nIn this section, we provided the standard deviations of performance comparison and ablation study in Table~\\ref{tab:main_result_std} and Table \\ref{tab:ablation_std}, respectively. RR methods often achieve smaller standard deviations while IL methods encounter more randomness.\n\n\n\n\\begin{table*}[ht]\n\t\\centering\n\t\\caption{Standard deviations of Table~\\ref{tab:main_result} (RQ1). \n % All improvements are statistically significant with $p$-value<0.01.\n }\\label{tab:main_result_std}\n  \\resizebox{\\textwidth}{!}{\n\\begin{tabular}{cc|cccccc|cccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{6}{c|}{\\textbf{CSI 300}} & \\multicolumn{6}{c}{\\textbf{CSI 500}} \\\\\n &  & \\textbf{\\quad IC \\quad} & \\textbf{\\quad ICIR\\quad } & \\textbf{RankIC} & \\textbf{RankICIR} & \\textbf{Return} & \\textbf{\\quad \\ IR\\ \\quad } & \\textbf{\\quad IC\\quad } & \\textbf{\\quad ICIR\\quad } & \\textbf{RankIC} & \\textbf{RankICIR} & \\textbf{Return} & \\textbf{\\quad \\ IR\\ \\quad } \\\\\\midrule\n \\multirow{6}{*}{\\makecell{Trans-\\\\former}} & RR & 0.003 & 0.019 & 0.002 & 0.017 & 0.011 & 0.113 & 0.002 & 0.014 & 0.002 & 0.018 & 0.034 & 0.459 \\\\\n & DDG-DA & 0.002 & 0.012 & 0.001 & 0.011 & 0.023 & 0.239 & 0.001 & 0.006 & 0.001 & 0.007 & 0.020 & 0.247 \\\\\n & IL & 0.011 & 0.088 & 0.008 & 0.068 & 0.031 & 0.370 & 0.002 & 0.016 & 0.002 & 0.017 & 0.012 & 0.161 \\\\\n & MetaCoG & 0.011 & 0.103 & 0.007 & 0.079 & 0.035 & 0.410 & 0.003 & 0.026 & 0.002 & 0.022 & 0.020 & 0.270 \\\\\n & C-MAML & 0.005 & 0.052 & 0.005 & 0.053 & 0.030 & 0.392 & 0.006 & 0.048 & 0.004 & 0.045 & 0.027 & 0.373 \\\\\n & DoubleAdapt & 0.006 & 0.055 & 0.006 & 0.058 & 0.023 & 0.264 & 0.006 & 0.049 & 0.002 & 0.025 & 0.024 & 0.380 \\\\\\midrule\n\\multirow{6}{*}{LSTM} & RR & 0.001 & 0.009 & 0.001 & 0.013 & 0.008 & 0.112 & 0.001 & 0.013 & 0.001 & 0.023 & 0.028 & 0.440 \\\\\n & DDG-DA & 0.001 & 0.009 & 0.002 & 0.011 & 0.014 & 0.171 & 0.001 & 0.015 & 0.001 & 0.013 & 0.017 & 0.288 \\\\\n & IL & 0.006 & 0.054 & 0.005 & 0.044 & 0.034 & 0.393 & 0.004 & 0.034 & 0.002 & 0.026 & 0.014 & 0.223 \\\\\n & MetaCoG & 0.006 & 0.061 & 0.006 & 0.056 & 0.028 & 0.334 & 0.004 & 0.039 & 0.002 & 0.025 & 0.014 & 0.224 \\\\\n & C-MAML & 0.006 & 0.049 & 0.006 & 0.052 & 0.020 & 0.277 & 0.003 & 0.027 & 0.002 & 0.031 & 0.018 & 0.254 \\\\\n & DoubleAdapt & 0.002 & 0.019 & 0.002 & 0.021 & 0.019 & 0.214 & 0.002 & 0.023 & 0.002 & 0.018 & 0.015 & 0.261 \\\\\\midrule\n\\multirow{6}{*}{ALSTM} & RR & 0.001 & 0.009 & 0.001 & 0.009 & 0.013 & 0.168 & 0.001 & 0.007 & 0.002 & 0.014 & 0.010 & 0.177 \\\\\n & DDG-DA & 0.001 & 0.010 & 0.001 & 0.013 & 0.017 & 0.220 & 0.001 & 0.016 & 0.001 & 0.020 & 0.025 & 0.356 \\\\\n & IL & 0.002 & 0.021 & 0.001 & 0.017 & 0.018 & 0.217 & 0.003 & 0.030 & 0.002 & 0.031 & 0.012 & 0.194 \\\\\n & MetaCoG & 0.003 & 0.027 & 0.003 & 0.025 & 0.012 & 0.137 & 0.003 & 0.030 & 0.002 & 0.026 & 0.017 & 0.250 \\\\\n & C-MAML & 0.003 & 0.031 & 0.002 & 0.023 & 0.012 & 0.160 & 0.001 & 0.014 & 0.002 & 0.022 & 0.018 & 0.274 \\\\\n & DoubleAdapt & 0.001 & 0.018 & 0.002 & 0.018 & 0.019 & 0.252 & 0.002 & 0.018 & 0.001 & 0.017 & 0.017 & 0.258 \\\\\\midrule\n\\multirow{6}{*}{GRU} & RR & 0.001 & 0.012 & 0.001 & 0.008 & 0.014 & 0.184 & 0.001 & 0.015 & 0.001 & 0.010 & 0.018 & 0.306 \\\\\n & DDG-DA & 0.001 & 0.005 & 0.001 & 0.005 & 0.014 & 0.158 & 0.001 & 0.018 & 0.002 & 0.018 & 0.013 & 0.158 \\\\\n & IL & 0.002 & 0.023 & 0.001 & 0.022 & 0.034 & 0.402 & 0.003 & 0.028 & 0.002 & 0.027 & 0.010 & 0.142 \\\\\n & MetaCoG & 0.006 & 0.055 & 0.005 & 0.052 & 0.024 & 0.318 & 0.004 & 0.047 & 0.002 & 0.034 & 0.019 & 0.311 \\\\\n & C-MAML & 0.002 & 0.019 & 0.001 & 0.014 & 0.013 & 0.141 & 0.001 & 0.014 & 0.001 & 0.021 & 0.021 & 0.318 \\\\\n & DoubleAdapt & 0.002 & 0.016 & 0.001 & 0.012 & 0.014 & 0.158 & 0.001 & 0.017 & 0.001 & 0.011 & 0.016 & 0.255 \\\\\n \\bottomrule\n  \\end{tabular}\n  }\n\\end{table*}\n\n\\begin{table*}[]\n\t\\centering\n\t\\caption{Standard deviations of Table~\\ref{tab:ablation} (RQ2). The head number $N$ is 6 unless otherwise stated. $IL+{MA}+{DA}$ is our proposed method. \n % * indicates a significant improvement over the most competitive method with $p$-value<0.01.\n }\\label{tab:ablation_std}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{l|cccc|cccc|cccc}\n\\toprule\n\\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{Method}}} & \\multicolumn{4}{c|}{\\textbf{Gradual Shifts}}                                        & \\multicolumn{4}{c|}{\\textbf{Overall Performance}}                                   & \\multicolumn{4}{c}{\\textbf{Abrupt Shifts}}                                         \\\\\n\\multicolumn{1}{c|}{}                                 & \\textbf{IC}     & \\textbf{ICIR}   & \\textbf{RankIC} & \\textbf{RankICIR} & \\textbf{IC}     & \\textbf{ICIR}   & \\textbf{RankIC} & \\textbf{RankICIR} & \\textbf{IC}     & \\textbf{ICIR}   & \\textbf{RankIC} & \\textbf{RankICIR} \\\\\\midrule\nIL                                                                              & 0.010 & 0.073 & 0.007 & 0.068 & 0.002 & 0.023 & 0.001 & 0.022 & 0.012 & 0.084 & 0.010 & 0.065 \\\\\n$+{DA}$                                                                         &0.009 & 0.070 & 0.006 & 0.060 & 0.002 & 0.022 & 0.001 & 0.022 & 0.012 & 0.074 & 0.009 & 0.060 \\\\\n$+{MA}$                                                                        & 0.009 & 0.062 & 0.005 & 0.055 & 0.001 & 0.012 & 0.001 & 0.014 & 0.011 & 0.066 & 0.009 & 0.051 \\\\\n$+{MA}$+$G$                                                                     & 0.010 & 0.088 & 0.006 & 0.067 & 0.002 & 0.027 & 0.001 & 0.020 & 0.011 & 0.071 & 0.008 & 0.048 \\\\\n$+{MA}$+$H$+$H^{-1}$                                                            & 0.009 & 0.065 & 0.009 & 0.063 & 0.002 & 0.017 & 0.002 & 0.019 & 0.012 & 0.071 & 0.009 & 0.054 \\\\\n$+{MA}$+${DA}$ ($N$=1)                                                          & 0.006 & 0.046 & 0.005 & 0.042 & 0.002 & 0.023 & 0.002 & 0.021 & 0.013 & 0.086 & 0.009 & 0.063 \\\\\n\\textbf{+\\textit{MA+DA} ($\\boldsymbol{N}$=6)}\n& 0.008 & 0.066 & 0.005 & 0.046 & 0.002 & 0.016 & 0.001 & 0.012 & 0.013 & 0.083 & 0.010 & 0.066\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table*}\n}\n\n% \\section{Additional Experiments}\n% \\subsection{Comparison with Normalization}\n\n"
                }
            }
        },
        "tables": {
            "tab:main_result": "\\begin{table*}[t]\n\t\\centering\n\t\\caption{Overall performance comparison on CSI 300 and CSI 500, where the bold values are the best results and the underlined values are the most competitive results (RQ1).\n % All improvements are statistically significant with $p$-value<0.01.\n }\\label{tab:main_result}\n  \\resizebox{\\textwidth}{!}{\n\\begin{tabular}{cc|cccccc|cccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{6}{c|}{\\textbf{CSI 300}} & \\multicolumn{6}{c}{\\textbf{CSI 500}} \\\\\n &  & \\textbf{\\quad IC \\quad} & \\textbf{\\quad ICIR\\quad } & \\textbf{RankIC} & \\textbf{RankICIR} & \\textbf{Return} & \\textbf{\\quad \\ IR\\ \\quad } & \\textbf{\\quad IC\\quad } & \\textbf{\\quad ICIR\\quad } & \\textbf{RankIC} & \\textbf{RankICIR} & \\textbf{Return} & \\textbf{\\quad \\ IR\\ \\quad } \\\\\\midrule\n \\multirow{6}{*}{\\makecell{Trans-\\\\former}} & RR & 0.0449 & 0.3410 & \\underline{0.0462} & \\textbf{0.3670} & 0.0881 & 1.0428 & 0.0452 & 0.4276 & \\underline{0.0469} & 0.4732 & 0.0639 & 0.9879 \\\\\n & DDG-DA & 0.0420 & 0.3121 & 0.0441 & 0.3420 & 0.0823 & 1.0018 & 0.0450 & 0.4223 & 0.0465 & 0.4634 & 0.0681 & 1.0353 \\\\\n & IL & 0.0431 & 0.3108 & 0.0411 & 0.2944 & 0.0854 & 0.9215 & 0.0428 & 0.3943 & 0.0453 & 0.4475 & 0.1014 & \\underline{1.5108} \\\\\n & MetaCoG & 0.0463 & 0.3493 & 0.0434 & 0.3133 & 0.0952 & 0.9921 & 0.0449 & \\underline{0.4643} & 0.0469 & 0.4629 & \\underline{0.1053} & 0.8945 \\\\\n  & C-MAML & \\underline{0.0479} & \\underline{0.3560} & 0.0448 & 0.3405 & \\underline{0.0986} & \\underline{1.0537} & \\underline{0.0477} & 0.4620 & 0.0468 & \\underline{0.4861} & 0.0930 & 1.4923 \\\\\n & DoubleAdapt & \\textbf{0.0516} & \\textbf{0.3889} & \\textbf{0.0475} & \\underline{0.3585} & \\textbf{0.1041} & \\textbf{1.1035} & \\textbf{0.0492} & \\textbf{0.4653} & \\textbf{0.0490} & \\textbf{0.4970} & \\textbf{0.1330} & \\textbf{1.9761} \\\\\\midrule\n\\multirow{6}{*}{LSTM} & RR & 0.0592 & \\underline{0.4809} & 0.0536 & \\underline{0.4526} & 0.0805 & 0.9578 & \\underline{0.0642} & \\underline{0.6187} & 0.0543 & 0.5742 & 0.0980 & 1.5220 \\\\\n & DDG-DA & 0.0572 & 0.4622 & 0.0528 & 0.4415 & 0.0887 & 1.0583 & 0.0636 & 0.6181 & 0.0540 & 0.5783 & 0.1061 & 1.6673 \\\\\n & IL & \\underline{0.0594} & 0.4664 & \\underline{0.0546} & 0.4362 & \\underline{0.1089} & \\underline{1.2553} & 0.0576 & 0.5550 & \\underline{0.0553} & 0.5660 & 0.1249 & 1.8461 \\\\\n & MetaCoG & 0.0515 & 0.4131 & 0.0505 & 0.4197 & 0.1013 & 1.1133 & 0.0573 & 0.5673 & 0.0549 & \\underline{0.5908} & \\underline{0.1384} & \\underline{2.0546} \\\\\n & C-MAML & 0.0568 & 0.4601 & 0.0517 & 0.4381 & 0.0963 & 1.1145 & 0.0582 & 0.5863 & 0.0550 & 0.5898 & 0.1315 & 1.9770 \\\\\n & DoubleAdapt & \\textbf{0.0632} & \\textbf{0.5126} & \\textbf{0.0567} & \\textbf{0.4669} & \\textbf{0.1117} & \\textbf{1.3029} & \\textbf{0.0648} & \\textbf{0.6331} & \\textbf{0.0594} & \\textbf{0.6087} & \\textbf{0.1496} & \\textbf{2.2220} \\\\\\midrule\n\\multirow{6}{*}{ALSTM} & RR & 0.0630 & \\underline{0.5084} & \\underline{0.0589} & \\textbf{0.4892} & 0.0947 & 1.1785 & \\underline{0.0649} & 0.6331 & 0.0575 & 0.6030 & 0.1211 & 1.8726 \\\\\n & DDG-DA & 0.0609 & 0.4915 & 0.0581 & 0.4823 & 0.0966 & 1.2227 & 0.0645 & 0.6298 & 0.0573 & 0.6029 & 0.1042 & 1.6091 \\\\\n & IL & 0.0626 & 0.4762 & 0.0585 & 0.4489 & \\underline{0.1171} & \\underline{1.3349} & 0.0596 & 0.5705 & 0.0579 & 0.5712 & 0.1501 & 2.1468 \\\\\n & MetaCoG & 0.0581 & 0.4676 & 0.0570 & 0.4695 & 0.1140 & 1.3228 & 0.0576 & 0.5874 & 0.0571 & 0.6086 & 0.1403 & 2.0857 \\\\\n & C-MAML & \\underline{0.0636} & 0.5064 & 0.0588 & 0.4765 & 0.1085 & 1.2432 & 0.0647 & \\textbf{0.6490} & \\underline{0.0598} & \\textbf{0.6330} & \\underline{0.1644} & \\underline{2.4636} \\\\\n & DoubleAdapt & \\textbf{0.0679} & \\textbf{0.5480} & \\textbf{0.0594} & \\underline{0.4882} & \\textbf{0.1225} & \\textbf{1.4717} & \\textbf{0.0653} & \\underline{0.6404} & \\textbf{0.0607} & \\underline{0.6170} & \\textbf{0.1738} & \\textbf{2.5192} \\\\\\midrule\n\\multirow{6}{*}{GRU} & RR & 0.0629 & \\underline{0.5105} & 0.0581 & 0.4856 & 0.0933 & 1.1428 & \\underline{0.0669} & \\underline{0.6588} & 0.0586 & 0.6232 & 0.1200 & 1.8629 \\\\\n & DDG-DA & 0.0623 & 0.5045 & 0.0589 & \\underline{0.4898} & 0.0967 & 1.1606 & 0.0666 & 0.6575 & 0.0582 & 0.6234 & 0.1264 & 1.9963 \\\\\n & IL & 0.0633 & 0.4818 & \\underline{0.0596} & 0.4609 & \\underline{0.1166} & 1.3196 & 0.0637 & 0.6093 & \\underline{0.0617} & 0.6291 & 0.1626 & 2.3352 \\\\\n & MetaCoG & 0.0560 & 0.4443 & 0.0545 & 0.4503 & 0.0992 & 1.1014 & 0.0603 & 0.5741 & 0.0585 & 0.5720 & 0.1587 & 2.2635 \\\\\n & C-MAML & \\underline{0.0638} & 0.5085 & 0.0595 & 0.4865 & 0.1121 & \\underline{1.3210} & 0.0646 & 0.6498 & 0.0600 & \\textbf{0.6494} & \\underline{0.1693} & \\textbf{2.5064} \\\\\n & DoubleAdapt & \\textbf{0.0687} & \\textbf{0.5497} & \\textbf{0.0621} & \\textbf{0.5110} & \\textbf{0.1296} & \\textbf{1.5123} & \\textbf{0.0686} & \\textbf{0.6652} & \\textbf{0.0632} & \\underline{0.6445} & \\textbf{0.1748} & \\underline{2.4578} \\\\\n \\bottomrule\n  \\end{tabular}\n  }\n\\end{table*}",
            "tab:ablation": "\\begin{table*}[t]\n\t\\centering\n\t\\caption{Ablation study on CSI 300 (RQ2). The forecast model is GRU. The head number $N$ is 8 unless otherwise stated. $IL+{MA}+{DA}$ is our proposed method. The column \\textit{Overall Performance} corresponds to the results on CSI 300 in Table~\\ref{tab:main_result}.\n % * indicates a significant improvement over the most competitive method with $p$-value<0.01.\n }\\label{tab:ablation}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{l|cccc|cccc|cccc}\n\\toprule\n\\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{Method}}} & \\multicolumn{4}{c|}{\\textbf{Overall Performance}}                                        & \\multicolumn{4}{c|}{\\textbf{Gradual Shifts}}                                   & \\multicolumn{4}{c}{\\textbf{Abrupt Shifts}}                                         \\\\\n\\multicolumn{1}{c|}{}                                 & \\textbf{IC}     & \\textbf{ICIR}   & \\textbf{RankIC} & \\textbf{RankICIR} & \\textbf{IC}     & \\textbf{ICIR}   & \\textbf{RankIC} & \\textbf{RankICIR} & \\textbf{IC}     & \\textbf{ICIR}   & \\textbf{RankIC} & \\textbf{RankICIR} \\\\\\midrule\nIL & 0.0633 & 0.4818 & 0.0596 & 0.4609 & 0.0643 & 0.4936 & 0.0652 & 0.5161 & 0.0690 & 0.5134 & 0.0619 & 0.4581 \\\\\n$+{DA}$ & 0.0659 & 0.5279 & 0.0615 & 0.4993 & 0.0708 & 0.5938 & 0.0692 & 0.5897 & 0.0690 & 0.5271 & 0.0620 & 0.4677 \\\\\n$+{MA}$ & 0.0658 & 0.5160 & 0.0610 & 0.4910 & 0.0703 & 0.5703 & 0.0680 & 0.5686 & 0.0681 & 0.5085 & 0.0618 & 0.4594 \\\\\n$+{MA}$+$G$ & 0.0678 & 0.5360 & 0.0619 & 0.4978 & 0.0740 & 0.6155 & \\underline{0.0709} & \\underline{0.6060} & 0.0694 & 0.5224 & 0.0626 & 0.4672 \\\\\n$+{MA}$+$H$+$H^{-1}$ & 0.0660 & 0.5207 & 0.0614 & 0.4995 & 0.0714 & 0.5846 & 0.0701 & 0.5958 & 0.0680 & 0.5093 & 0.0616 & 0.4615  \\\\\n$+{MA}$+${DA}$ ($N$=1) & \\underline{0.0684} & \\underline{0.5462} & \\textbf{0.0622} & \\underline{0.5074} & \\underline{0.0744} & \\underline{0.6227} & 0.0703 & 0.6029 & \\textbf{0.0710} & \\textbf{0.5430} & \\textbf{0.0634} & \\textbf{0.4822} \\\\\n\\textbf{+\\textit{MA}+\\textit{DA} ($\\boldsymbol{N}$=8)} & \\textbf{0.0687} & \\textbf{0.5497} & \\underline{0.0621} & \\textbf{0.5110} & \\textbf{0.0755} & \\textbf{0.6390} & \\textbf{0.0713} & \\textbf{0.6243} & \\underline{0.0699} & \\underline{0.5323} & \\underline{0.0620} & \\underline{0.4730}\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table*}",
            "tab:time cost": "\\begin{table}[t]\n\t\\centering\n\t\\caption{Empirical time cost (in second) comparison.}\\label{tab:time cost}\n  \\resizebox{0.95\\linewidth}{!}{\n\\begin{tabular}{cccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{2}{c}{\\textbf{CSI 300}} & \\multicolumn{2}{c}{\\textbf{CSI 500}} \\\\\n                                &                                   & \\textbf{Offline}  & \\textbf{Online} & \\textbf{Offline}  & \\textbf{Online} \\\\\n                                \\midrule\n\\multirow{6}{*}{GRU}            & RR                                & -                 & 6064            & -                 & 10793           \\\\\n                                & DDG-DA                            & 1862              & 6719            & 2360              & 10713           \\\\\n                                & IL                                & \\textbf{256}      & \\textbf{58}     & \\textbf{394}      & \\textbf{75}     \\\\\n                                & MetaCoG                           & 457               & 59              & 784               & 76              \\\\\n                                & C-MAML                            & 314               & 62              & 533               & 77             \\\\\n                                & \\textbf{DoubleAdapt}                              & 356               & 61              & 677               & 79            \\\\\\bottomrule\n\\end{tabular}\n}\n\\end{table}"
        },
        "figures": {
            "fig:shift": "\\begin{figure}[t]\n  % \\setlength{\\abovecaptionskip}{2pt}\n  % \\setlength{\\belowcaptionskip}{-6pt}\n  \\vspace{4pt}\n\t\\centering\n  \\captionsetup[subfigure]{skip=1pt}\n\t\\subcaptionbox{Gradual shift\\label{fig:shift:a}}\n\t{\n    \\includegraphics[width=.98\\linewidth]{gradual_shift.pdf}\n  }\\\\\n  \\vspace{4pt}\n\t\\subcaptionbox{Abrupt shift\\label{fig:shift:b}}\n\t{\n    \\includegraphics[width=0.98\\linewidth]{abrupt_shift.pdf}\n  }\\\\\n  % \\vspace{5pt}\n %  \\subcaptionbox{Reoccurring distribution\\label{fig:shift:c}}\n\t% {\n %    \\includegraphics[width=.98\\linewidth]{reoccuring_distribution.pdf}\n %  }\n\t\\caption{{Illustration of distribution shifts in CSI 300 stock set. The vector of each stock including dozens of technical indicators and the corresponding label is mapped via t-SNE~\\cite{t-SNE} to a 1-D point on the horizontal axis. We estimate the distributions with a kernel density estimator. We plot the distribution of the incremental data in one month in purple, the distribution of all the previous data in black, and the distribution of the next month's data in green.}}\n  \\label{fig:shift}\n\\end{figure}",
            "fig:IL": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{IL_short.pdf}\n  \\caption{Illustration of IL for stock trend forecasting. $r$ is the timespan of incremental data or test data. \n  %The test data of two consecutive incremental tasks are closely adjacent so that we make predictions at all test dates.\n  }\n  \\label{fig:IL}\n\\end{figure}",
            "fig:flow": "\\begin{figure}[ht]\n\n  \\centering\n  \\setlength{\\abovecaptionskip}{6pt}\n  \\setlength{\\belowcaptionskip}{-15pt}\n  \\includegraphics[width=\\linewidth]{after_da.pdf}\n  \\caption{Illustration of data adaptation. The function $G$ adapts the feature distributions of the incremental data and the test data to an agent feature distribution. Similarly, the function $H$ adapts the two posterior distributions to an agent one. Its inverse function $H^{-1}$ restores the model outputs to the original posterior distribution of the test data.}\n  \\label{fig:flow}\n\\end{figure}",
            "fig:overview": "\\begin{figure}[t]\n  \\setlength{\\belowcaptionskip}{-8pt}\n  \\centering\n  \\includegraphics[width=\\linewidth]{overview_new.pdf}\n  \\caption{Overview of \\model with a data adapter ${DA}$ and a model adapter ${MA}$. The parameters are shown in red.}\n  \\label{fig:overview}\n\\end{figure}",
            "fig:hyper": "\\begin{figure*}[t]\n\t\\centering\n  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t\\subcaptionbox{Task interval\\label{fig:hyper:r}}\n\t{\n    \\includegraphics[width=.23\\linewidth]{step.pdf}\n  }\n  % \\\\\\vspace{0.5cm}\n  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t\\subcaptionbox{Softmax temparature\\label{fig:hyper:tau}}\n\t{\n    \\includegraphics[width=.23\\linewidth]{tau.pdf}\n  }\n  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t\\subcaptionbox{Number of heads\\label{fig:hyper:head}}\n\t{\n    \\includegraphics[width=.23\\linewidth]{head.pdf}\n  }\n  % \\vspace{5pt}\n  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t\\subcaptionbox{Regularization strength\\label{fig:hyper:reg}}\n\t{\n    \\includegraphics[width=.23\\linewidth]{reg.pdf}\n  }\n  % \\vspace{5pt}\n %  \\captionsetup[subfigure]{margin={0pt, 15pt}}\n\t% \\subcaptionbox{Online learning rates\\label{fig:hyper:lr}}\n\t% {\n %    \\includegraphics[width=.18\\linewidth]{lr.pdf}\n %  }\n\t\\caption{Performance comparison under different hyperparameters on CSI 300. Outliers are marked in diamonds.}\n  \\label{fig:hyper}\n\\end{figure*}",
            "fig:hyper:lr": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{lr.pdf}\n    \\caption{Performance comparison under different online learning rates of the meta-learners on CSI 300. The learning rate $\\eta_\\phi$ of the model adapter varies on the horizontal axis. The learning rate $\\eta_\\psi$ of the data adapter varies on the vertical axis.}\n    \\label{fig:hyper:lr}\n\\end{figure}",
            "fig:computation graph": "\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\linewidth]{computation_graph.pdf}\n    \\caption{Illustration of our computation graph}\n    \\label{fig:computation graph}\n\\end{figure}"
        },
        "equations": {
            "eq:eq:upper level": "\\begin{equation}\\label{eq:upper level}\n  \\phi^{k}, \\psi^{k}=\\underset{{\\phi, \\psi}}{\\arg \\min } \\mathcal L_{\\text{test}}(\\mathcal {\\widetilde D}_{\\text {test}}^{k}; \\theta^{k}),\n\\end{equation}",
            "eq:eq:lower level": "\\begin{equation}\\label{eq:lower level}\n  \\text {s.t.} \\quad {\\theta}^{k}={MA}(\\mathcal {\\widetilde D}_{\\text{train}}^{k};\\phi^{k-1}), \n\\end{equation}",
            "eq:eq:adapted data": "\\begin{equation}\\label{eq:adapted data}\n  \\mathcal{\\widetilde D}_{\\text {train }}^{k}={DA}({\\mathcal D}_{\\text {train }}^{k};\\psi^{k-1});\\quad\n  \\mathcal {\\widetilde D}_{\\text {test}}^{k}={DA}({\\mathcal D}_{\\text {test}}^{k};\\psi^{k-1}).\n\\end{equation}",
            "eq:1": "\\begin{equation}\n  \\tilde{\\mathbf{x}} = \\mathbf W\\mathbf x + \\mathbf b,\n\\end{equation}",
            "eq:eq:feature adaptation": "\\begin{equation}\\label{eq:feature adaptation}\n  \\mathbf{\\tilde x} = G(\\mathbf x) := \\mathbf x+\\sum_{i=1}^{N} s_i\\cdot g_{i}(\\mathbf x),\n\\end{equation}",
            "eq:2": "\\begin{equation}\n  g_i(\\mathbf x)=\\mathbf W_i\\mathbf x+\\mathbf b_i,  \n\\end{equation}",
            "eq:3": "\\begin{equation}\n  \\hat s_i=\\frac{\\mathbf p_i^\\top \\mathbf x}{\\|\\mathbf p_i\\| \\| \\mathbf x \\|}, \n\\end{equation}",
            "eq:4": "\\begin{equation}\n  s_i=\\frac{\\exp \\hat s_i/\\tau}{\\sum_{j=1}^{N} \\exp \\hat s_j/\\tau},\n\\end{equation}",
            "eq:5": "\\begin{equation}\n  h_i(y) = {\\gamma}_i y + \\beta_i,\n\\end{equation}",
            "eq:eq:label adaptation": "\\begin{equation}\\label{eq:label adaptation}\n  \\tilde{y} = H(y) := \\sum_{i=1}^{N} s_i\\cdot h_{i}(y),\n\\end{equation}",
            "eq:6": "\\begin{equation}\n  \\hat y = H^{-1}(\\mathbf x, \\check y) := \\sum_{i=1}^{N} s_i\\cdot h^{-1}_{i}(\\check y),\n\\end{equation}",
            "eq:7": "\\begin{equation}\n  h_i^{-1}(\\check y) = ({\\check y - \\beta_i})/{\\gamma_i}.\n\\end{equation}",
            "eq:eq:final adapted data": "\\begin{subequations}\\label{eq:final adapted data}\n  \\begin{align}\n  \\mathcal{\\widetilde D}_{\\text {train}}^{k}&=\\{(G(\\mathbf x), H(\\mathbf x, y))\\mid (\\mathbf x, y)\\in \\mathcal{D}_{\\text {train }}^{k}\\},\\label{eq:final adapted training data}\\\\ \n  \\mathcal{\\widetilde D}_{\\text {test }}^{k}&=\\{(G(\\mathbf x), y)\\mid (\\mathbf x, y)\\in \\mathcal{D}_{\\text {test}}^{k}\\}\\label{eq:final adapted test data},\n  \\end{align}\n\\end{subequations}",
            "eq:eq:training loss": "\\begin{equation}\\label{eq:training loss}\n  \\mathcal{L}_{\\text{train}}=\\frac{1}{|\\mathcal{\\widetilde D}_{\\text{train}}^{k}|}\\sum_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}}^{k}} \\left(F(\\tilde{\\mathbf x};\\phi^{k-1})-\\tilde y\\right)^2.\n\\end{equation}",
            "eq:eq:task-specific param": "\\begin{equation}\\label{eq:task-specific param}\n  \\begin{aligned}  \n  {\\theta}^{k}&={MA}(\\mathcal {\\widetilde D}_{\\text{train}}^{k};\\phi^{k-1})\\\\\n                &=\\phi^{k-1} -\\eta_\\theta \\nabla_\\phi \\mathcal{L}_{\\text{train}}(\\mathcal {\\widetilde D}_{\\text {train}}^{k}; \\phi^{k-1}),\n  \\end{aligned}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n  \\mathcal L_{\\text{MSE}}=\\frac{1}{|\\mathcal{\\widetilde D}_{\\text {test}}^{k}|}\\sum_{(\\tilde{\\mathbf x}, y) \\in \\mathcal {\\widetilde D}_{\\text {test}}^{k}}\\left( H^{-1}\\circ F(\\tilde{\\mathbf x} ; \\theta^{k}, \\psi^{k-1}) - y\\right)^2.\n\\end{equation}",
            "eq:9": "\\begin{equation}\n  \\mathcal L_{\\text{reg}}=\\frac{1}{|\\mathcal{D}_{\\text {train}}^{k}|}\\sum_{(\\mathbf{x}, y) \\in \\mathcal {D}_{\\text {train}}^{k}}\\left( H(\\mathbf{\\tilde x}, y; \\psi^{k-1}) - y\\right)^2.\n\\end{equation}",
            "eq:eq:test loss": "\\begin{equation}\\label{eq:test loss}\n  \\mathcal L_{\\text{test}}=\\mathcal L_{\\text{MSE}}+\\alpha\\mathcal L_{\\text{reg}},\n\\end{equation}",
            "eq:10": "\\begin{equation}\n  {\\phi}^{k} = {\\phi}^{k-1}-\\eta_\\phi \\nabla_\\phi \\mathcal{L}_{\\text{test}}(\\mathcal {\\widetilde D}_{\\text {test}}^{k}; \\theta^{k}),\n\\end{equation}",
            "eq:11": "\\begin{equation}\n  {\\psi}^{k} = {\\psi}^{k-1}-\\eta_\\psi \\nabla_\\psi \\mathcal{L}_{\\text{test}}(\\mathcal {\\widetilde D}_{\\text {test}}^{k}; \\theta^{k}),\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    IC^{(t)} = \\frac{1}{N}\\frac{(\\hat{\\mathbf{Y}}^{(t)} - mean(\\hat{\\mathbf{Y}}^{(t)}))^{\\top} (\\mathbf{Y}^{(t)} - mean(\\mathbf{Y}^{(t)}))}{std(\\hat{\\mathbf{Y}}^{(t)})\\cdot std(\\mathbf{Y}^{(t)})},\n\\end{equation}",
            "eq:13": "\\begin{equation}\n  \\begin{aligned}\n  \\nabla_\\phi \\mathcal{L}_{\\text{test}}(\\theta )\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\frac{\\partial \\theta }{\\partial \\phi}\\\\\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\frac{\\partial(\\phi -\\eta \\nabla_\\phi \\mathcal{L}_{\\text{train}}(\\phi ))}{\\partial \\phi}\\\\\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\left(1-\\eta\\frac{\\partial }{\\partial \\phi}\\nabla_\\phi \\mathcal{L}_{\\text{train}}(\\phi )\\right)\\\\\n  \\end{aligned}\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\nabla_\\phi \\mathcal{L}_{\\text{test}}(\\theta)\\approx \\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta)}{\\partial \\theta}=\\nabla_\\theta \\mathcal{L}_{\\text{test}}(\\theta).\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\theta = \\phi - \\eta \\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y).\n\\end{equation}",
            "eq:eq:psi_y gradient": "\\begin{equation}\\label{eq:psi_y gradient}\n  \\begin{aligned}\n  \\nabla_{\\psi_y} \\mathcal{L}_{\\text{test}}(\\theta )\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\frac{\\partial \\theta }{\\partial \\psi_y}\\\\\n  =&\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\frac{\\partial(\\phi -\\eta \\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))}{\\partial {\\psi_y}}\\\\\n  =&-\\eta\\frac{\\partial \\mathcal{L}_{\\text{test}}(\\theta )}{\\partial \\theta }\\left(\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\right)\\\\\n  \\end{aligned}\n\\end{equation}",
            "eq:16": "\\begin{equation} \\label{eq:second-order gradient}\n\\begin{aligned}\n  &\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\\\\n  =&\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\sim \\mathcal{P}_{\\text{agent}}(\\mathbf x, y) }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\\\\n  =&\\mathbb E_{\\tilde{\\mathbf x}\\sim\\mathcal P_{\\text{agent}}(\\mathbf x)}\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{\\tilde y\\sim \\mathcal P_{\\text{agent}}(y|\\mathbf x)}\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)),\n  \\end{aligned}\n\\end{equation}",
            "eq:17": "\\begin{equation} \\label{eq:REINFORCE}\n\\begin{aligned}\n&\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{\\tilde y\\sim \\mathcal P_{\\text{agent}}(y|\\mathbf x)}\\left[\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))\\right]\\\\\n=&\\mathbb E_{\\tilde y\\sim \\mathcal P_{\\text{agent}}(y|\\mathbf x)}\\left[\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\cdot \\frac{\\partial \\log P(\\tilde y\\mid\\mathbf x,y;\\psi_y)}{\\partial \\psi_y}\\right],\n\\end{aligned}\n\\end{equation}",
            "eq:18": "\\begin{equation} \\label{eq:logMSE}\n\\begin{aligned}\n\\frac{\\partial \\log P(\\tilde y|\\mathbf x,y;\\psi_y)}{\\partial \\psi_y}\n=&\\frac{\\partial \\left(-\\frac{1}{2} \\log \\left(2 \\pi \\sigma^2\\right)-\\frac{1}{2 \\sigma^2}\\left(\\tilde y-y\\right)^2\\right)}{\\partial \\psi_y}\\\\\n=&\\frac{\\partial \\left(-\\frac{1}{2} \\log \\left(2 \\pi \\sigma^2\\right)-\\frac{1}{2 \\sigma^2}\\mathcal{L}_{\\text{MSE}}(\\tilde y, y)\\right)}{\\partial \\psi_y}\\\\\n=&-\\frac{1}{2\\sigma^2}\\frac{\\partial \\mathcal{L}_{\\text{MSE}}(\\tilde y, y)}{\\partial \\psi_y}\n\\end{aligned}\n\\end{equation}",
            "eq:19": "\\begin{equation} \\label{eq:flatten second-order gradient} \n\\begin{aligned}\n&\\frac{\\partial }{\\partial \\psi_y}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y)\\\\\n=&\\frac{-1}{2\\sigma^2}\\mathbb E_{\\tilde{\\mathbf x}\\sim\\mathcal P_{\\text{agent}}(\\mathbf x)}\\mathbb E_{\\tilde y\\sim \\mathcal P_{\\text{agent}}(y|\\mathbf x)}\\left[\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))\\frac{\\partial \\mathcal{L}_{\\text{MSE}}(\\tilde y, y)}{\\partial \\psi_y}\\right]\\\\\n=&\\frac{-1}{2\\sigma^2}\\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\left[\\nabla_\\phi \\mathcal L_{\\text{MSE}}(F(\\mathbf{\\tilde x}), \\tilde y))\\frac{\\partial \\mathcal{L}_{\\text{MSE}}(\\tilde y, y)}{\\partial \\psi_y}\\right]\\\\\n=&\\frac{-1}{2\\sigma^2}\\nabla_\\phi \\mathcal L_{\\text{train}}(\\phi)\\frac{\\partial\\ \\mathbb E_{(\\tilde{\\mathbf x},\\tilde y)\\in \\mathcal{\\widetilde D}_{\\text{train}} }\\mathcal{L}_{\\text{MSE}}(\\tilde y, y)}{\\partial \\psi_y},\\\\\n=&\\frac{-1}{2\\sigma^2}\\nabla_\\phi \\mathcal L_{\\text{train}}(\\phi)\\frac{\\partial\\mathcal{L}_{\\text{reg}}}{\\partial \\psi_y},\\\\\n\\end{aligned}\n\\end{equation}",
            "eq:20": "\\begin{equation}\n\\begin{aligned}\n    \\nabla_{\\psi_y} \\mathcal{L}_{\\text{test}}(\\theta )\n  =\\frac{\\eta}{2\\sigma^2}[\\nabla_\\theta\\mathcal{L}_{\\text{test}}(\\theta)]^\\top\\cdot\\nabla_\\phi \\mathcal L_{\\text{train}}(\\phi)\\cdot \\frac{\\partial \\mathcal{L}_{\\text{reg}}}{\\partial \\psi_y}.\n\\end{aligned}\n\\end{equation}",
            "eq:21": "\\begin{equation} \n\\mathcal L_{\\text{test}}(\\phi)\\approx\\mathcal L_{\\text{test}}(\\theta)+[\\nabla_\\theta\\mathcal{L}_{\\text{test}}(\\theta )]^\\top(\\phi-\\theta).\n\\end{equation}",
            "eq:22": "\\begin{equation} \n\\theta=\\phi-\\eta\\nabla_\\phi\\mathcal L_{\\text{train}}(\\phi).\n\\end{equation}",
            "eq:23": "\\begin{equation} \n\\mathcal L_{\\text{test}}(\\phi)-\\mathcal L_{\\text{test}}(\\theta)\\approx\\eta[\\nabla_\\theta\\mathcal{L}_{\\text{test}}(\\theta)]^\\top\\nabla_\\phi \\mathcal L_{\\text{train}}(\\phi)\n\\end{equation}",
            "eq:24": "\\begin{equation} \n\\nabla_{\\psi_y} \\mathcal{L}_{\\text{test}}(\\theta )=\\frac{\\mathcal L_{\\text{test}}(\\phi)-\\mathcal L_{\\text{test}}(\\theta)}{2\\sigma^2}\\cdot\\frac{\\partial \\mathcal{L}_{\\text{reg}}}{\\partial \\psi_y},\n\\end{equation}"
        },
        "git_link": "https://github.com/SJTU-Quant/qlib/"
    }
}