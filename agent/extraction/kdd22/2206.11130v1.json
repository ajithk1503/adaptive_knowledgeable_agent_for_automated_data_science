{
    "meta_info": {
        "title": "Multi-View Clustering for Open Knowledge Base Canonicalization",
        "abstract": "Open information extraction (OIE) methods extract plenty of OIE triples <noun\nphrase, relation phrase, noun phrase> from unstructured text, which compose\nlarge open knowledge bases (OKBs). Noun phrases and relation phrases in such\nOKBs are not canonicalized, which leads to scattered and redundant facts. It is\nfound that two views of knowledge (i.e., a fact view based on the fact triple\nand a context view based on the fact triple's source context) provide\ncomplementary information that is vital to the task of OKB canonicalization,\nwhich clusters synonymous noun phrases and relation phrases into the same group\nand assigns them unique identifiers. However, these two views of knowledge have\nso far been leveraged in isolation by existing works. In this paper, we propose\nCMVC, a novel unsupervised framework that leverages these two views of\nknowledge jointly for canonicalizing OKBs without the need of manually\nannotated labels. To achieve this goal, we propose a multi-view CH K-Means\nclustering algorithm to mutually reinforce the clustering of view-specific\nembeddings learned from each view by considering their different clustering\nqualities. In order to further enhance the canonicalization performance, we\npropose a training data optimization strategy in terms of data quantity and\ndata quality respectively in each particular view to refine the learned\nview-specific embeddings in an iterative manner. Additionally, we propose a\nLog-Jump algorithm to predict the optimal number of clusters in a data-driven\nway without requiring any labels. We demonstrate the superiority of our\nframework through extensive experiments on multiple real-world OKB data sets\nagainst state-of-the-art methods.",
        "author": "Wei Shen, Yang Yang, Yinan Liu",
        "link": "http://arxiv.org/abs/2206.11130v1",
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "additionl_info": "Accepted by SIGKDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\tClosed information extraction (CIE) \\cite{chang2006survey} requires pre-specified ontology, \n\tand has made a contribution to the development of curated knowledge bases (CKBs) \n\t(e.g., YAGO \\cite{yago} and Freebase \\cite{bollacker2008freebase}). \n\t%(e.g., YAGO \\cite{yago}, DBpedia \\cite{lehmann2015dbpedia}, and Freebase \\cite{bollacker2008freebase}). \n\tLarge CKBs contain millions of entities and hundreds of millions of relational facts about them, \n\twhich are usually stored in the form of triples. \n\tEntities and relations in CKBs are generally canonicalized and well defined with unique identifiers. \n\tCKBs play a fundamental role in many real knowledge-driven applications including \n\tsemantic search \\cite{xiong2017explicit} and knowledge reasoning \\cite{liu2021kompare}. \n\tHowever, the construction of CKBs usually requires significant human involvement to pre-define the ontology, \n\twhich has limitations in its adaptability to new domains or new data.\n\t\n\tOpen information extraction (OIE) has been proposed to solve this problem, such as Standford OIE \\cite{angeli2015leveraging} and MinIE \\cite{gashteovski2017minie}. \n\t%ClausIE \\cite{del2013clausie}, and MinIE \\cite{gashteovski2017minie}. \n\tWithout any pre-specified ontology,\n\tOIE methods extract a large number of OIE triples <noun phrase, relation phrase, noun phrase> from unstructured text \n\tto compose large open knowledge bases (OKBs), which makes them highly adaptable.\n\tSuch kind of notable OKBs include ReVerb \\cite{fader2011identifying} and TextRunner \\cite{Banko2007Open}.\n\tIn general, the coverage and diversity of OKBs are much higher than CKBs.\n\t\n\tHowever, unlike CKBs, noun (relation) phrases in OKBs are not well canonicalized and lack unique identifiers for them.\n\tThis may result in storage of scattered and redundant facts, which hinder their subsequent utilization in the downstream applications.\n\tHere, we show three sentences as an example.\\vspace{0.2em}\n\t\n\t\\textit{Donald J. Trump is Chairman of Trump University.}\n\t\n\t\\quad \\textit{Donald Trump is the CEO of Trump Organization.}\n\t\n\t\\qquad \\textit{Trump is currently the CEO of Trump Organization.} \\vspace{0.2em}\n\t\n\tAn OIE system can extract the following three OIE triples from these three sentences.\\vspace{0.2em}\n\t\n\t\\textit{<Donald J. Trump, is Chairman of, Trump University> }\n\t\n\t\\quad \\textit{<Donald Trump, is the CEO of, Trump Organization> }\n\t\n\t\\qquad \\textit{<Trump, is currently the CEO of, Trump Organization> }\\vspace{0.2em}\n\t\n\tUnfortunately, it is unknown for machines that \\textit{Donald J. Trump}, \\textit{Donald Trump} and \\textit{Trump} refer to the same entity. \n\tThis means that machines would not return all the available facts about this entity\t\n\twhen querying the term \\textit{Donald Trump} over the above OIE triples.\n\tAdditionally, it can be seen that the last two OIE triples are redundant facts, only one of which needs to be stored in practice. \n\t\n\tTo deal with this crucial issue, much attention has been paid to the task of OKB canonicalization, that is, \n\tconvert OIE triples in OKBs to their canonicalized form, which clusters synonymous noun (relation) phrases into a group. \n\tIntuitively, OKB canonicalization helps to integrate knowledge and eliminate redundancy of OKBs, which could greatly benefit the downstream applications. \n\t\n\tIn spite of its importance, canonicalization of OKBs is still an unsolved problem.\n\tTo tackle it, two categories of models have been explored, and they all regard it as a clustering problem and \n\tcluster synonymous noun (relation) phrases into the same group. \n\tSpecifically, the first line of models \\cite{galarraga2014canonicalizing, vashishth2018cesi, dash2021open, liu2021joint} mainly utilizes the fact view, i.e., knowledge embedded in the OIE fact triple, to cluster noun (relation) phrases with the same semantics.\n\tAlthough some of them \\cite{vashishth2018cesi, dash2021open} need the context as their input, they use the context as the input of some third-party tools to generate side information utilized in the fact view, rather than leveraging the context as an independent view for canonicalizing OKBs.\n\tThe second category of method \\cite{lin2019canonicalization} primarily employs the context view, i.e., knowledge embedded in the source context where the OIE fact triple is extracted.\n\tIn reality, these two views of knowledge characterize different aspects of noun (relation) phrases and are complementary to each other.\n\tMaking use of them together would improve the model accuracy and robustness.\n\tHowever, these two views of knowledge have been leveraged in isolation so far.\n\t\n\tIn this paper, we propose CMVC, a novel unsupervised framework for \\underline{\\textbf{C}}anonicalizing OKBs based on \\underline{\\textbf{M}}ulti-\\underline{\\textbf{V}}iew \\underline{\\textbf{C}}lustering\n\tby leveraging these two views of knowledge jointly. \n\tThe fact view characterizes the relational structural information of fact triples, in which entities are linked by relations.\n\tThe context view captures the semantic distributional information of the source context where the fact triple occurs.\n\tThese two views provide complementary information both of which are vital to OKB canonicalization.\n\tTo combine knowledge from both views, we propose a multi-view CH K-Means clustering algorithm to mutually reinforce the clustering of view-specific embeddings learned \n\tfrom each view by considering their different clustering qualities. \n\t\n\tIntuitively, better embeddings learned from multiple views lead to better canonicalization result. \n\tConsequently, the goal of each view is to extract high-quality embeddings based on the view-specific information. \n\tInspired by a prior work \\cite{vashishth2018cesi}, we could collect some seed pairs of synonymous noun (relation) phrases automatically from external resources \n\twithout any human involvement and then leverage them as prior knowledge for high-quality encoding. \n\tHowever, these automatically collected seed pairs are often limited and error-prone.\n\tTo address this issue, we propose a training data optimization strategy in terms of data quantity and data quality respectively in each particular view \n\tto further promote the learned view-specific embeddings iteratively. \n\tIn the fact view, we focus on optimizing the quantity of the training data and propose a data augmentation operator, \n\ti.e., swap counterparts of the seed pairs in their involved fact triples, to derive additional augmented training data. \n\tIn the context view, we focus on optimizing the quality of the training data and propose an iterative clustering procedure, \n\twhich alternately performs clustering and embedding learning, to transfer prior knowledge from seed pairs to other pairs to construct more accurate training data. \n\tThese two training data optimization strategies in both views boost the quality of the learned view-specific embeddings significantly.\n\t\n\tOur major contributions can be summarized as follows:\n\t\n\t$\\bullet$ This is the first unsupervised framework that combines knowledge from both the fact view and the context view for canonicalizing OKBs \n\tvia a novel multi-view CH K-Means clustering algorithm. \n\t\n\t$\\bullet$ In each view, we propose a training data optimization strategy in terms of data quantity and data quality respectively \n\tto refine the learned view-specific embeddings in an iterative manner.\n\t\n\t$\\bullet$ We propose a Log-Jump algorithm to predict the optimal number of clusters in a data-driven way without requiring any labels.\n\t\n\t$\\bullet$ A thorough experimental study over three real-world OKB data sets shows that our framework outperforms all the baseline methods \n\tfor the task of OKB canonicalization. \n\t\n\t%\\vspace{-1.5mm}\n\t"
            },
            "section 2": {
                "name": "PRELIMINARIES AND NOTATIONS",
                "content": "\n\tIn this section, we introduce some basic concepts and define the task of OKB canonicalization.\n\t\n\tIn an OKB, an OIE triple is denoted by $t_i = <sub_i, rel_i, obj_i > $, \n\twhere $sub_i$ and $obj_i$ are noun phrases (NPs) and $rel_i$ is a relation phrase (RP). \n\tThe source text where the OIE triple $t_i$ is extracted, is denoted by $s_{t_i}$, which could be a sentence or a paragraph in the source article. \n\tFor an NP $sub_{i}$, we define its source context as $c_{sub_i}$, i.e., the source text $s_{t_i}$ with the NP $sub_i$ itself removed. \n\tThe source context of $rel_i$ ($obj_i$) could be defined in a similar manner.\n\t\n\t\\vspace{-1mm}\n\t\\begin{definition}\n\t\t[\\textbf{OKB Canonicalization}] \n\t\tGiven a set of OIE triples $t_{i}$'s in an OKB and their corresponding source texts $s_{t_{i}}$'s, \n\t\tthe goal of this task is to cluster synonymous NPs referring to the same entity and synonymous RPs having the same semantic meaning into a group, \n\t\twhich converts these OIE triples to the canonicalized form.\n\t\\end{definition}\n\t\\vspace{-1mm}\n\tIt can be seen from the task definition above that two views \n\t(i.e., a fact view based on the OIE fact triples $t_{i}$'s and a context view based on their corresponding source texts $s_{t_{i}}$'s) \n\tare provided as input and our target is to leverage these two views of knowledge jointly. \n\tBased on the knowledge from a specific view $v$, we could learn the view-specific embedding $\\bm{sub}_i^{(v)}$ for the NP $sub_i$, \n\twhich is introduced in details in Section \\ref{fact_view} (i.e., the fact view) and Section \\ref{context_view} (i.e., the context view), respectively. \n\tFor notation, we use the bold lowercase letter to represent the embedding in this paper. \n\tThe view-specific embedding $\\bm{rel}_i^{(v)}$ ($\\bm{obj}_i^{(v)}$) of ${rel}_i$ (${obj}_i $) could be derived in a similar manner. \n\tThere are totally two views in our framework, i.e., $v \\in \\{ 1, 2 \\}$, where view $1$ represents the fact view and view $2$ represents the context view. \n\n\t"
            },
            "section 3": {
                "name": "sec:cmvc",
                "content": "\n\t%\\vspace{-1mm}\n\tThe overall framework of our proposed CMVC is shown in Figure \\ref{figure_1}. \n\tWe begin with the introduction of our proposed multi-view clustering algorithm and thereafter describe the fact view and the context view subsequently. \n\t\\vspace{-2mm}\n\t",
                "subsection 3.1": {
                    "name": "sec:mvc",
                    "content": "\n\t\\vspace{-1mm}\n\tMulti-view clustering has been widely studied in machine learning, \n\twhich aims to provide a more accurate and stable clustering result than single view clustering by considering complementary information of multiple views.  \n\tIn this paper, we propose a novel multi-view CH K-Means algorithm for OKB canonicalization to integrate knowledge from two views (i.e., a fact view and a context view) \n\tby considering their different clustering qualities. \n\tIn the following, first, we describe this proposed multi-view CH K-Means algorithm. \n\tNext, we introduce our proposed novel Log-Jump algorithm, to predict the parameter (i.e., the number of clusters) of the clustering algorithm in a data-driven manner. \n\tFor simplicity, we take the NP $sub_{i}$ as an example for illustration. \n\tThe process for the RP $rel_{i}$ and NP $obj_{i}$ is similar and omitted for saving space. \n\t\n\t\\setlength{\\textfloatsep}{0.7mm}\n\t\\setlength{\\floatsep}{0.7mm}\n\t%\\begin{figure}[htbp] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形\n\t\n\t\n\t\\vspace{-1.5mm}\n\t",
                    "subsubsection 3.1.1": {
                        "name": "sec:mvch",
                        "content": "\n\tInspired by the multi-view spherical K-Means \\cite{bickel2004multi}, our multi-view CH K-Means is based on the co-EM algorithm. \n\tIn order to quantify the quality of the clustering result for each view, we leverage Cali$\\rm{\\acute{n}}$ski-Harabasz (CH) index \\cite{calinski1974dendrite} as the weight of each view, instead of treating different views equally. \n\tTo mutually reinforce the clustering of each view, steps M, E in view 1 and steps M, E in view 2 are performed by turn \n\tand the clustering result is transferred between two views. \n\tWe introduce the M-step and the E-step as follows.\n\t\n\t\\textbf{\\textit{M-step}}. The input of the maximization step is the clustering result $\\pi^{(\\bar{v})} = \\{ \\pi_{1}^{(\\bar{v})}, ..., \\pi_{j}^{(\\bar{v})}, ..., \\pi_{K}^{(\\bar{v})} \\}$ from the \\textbf{\\textit{other}} view $\\bar{v}$ ($\\bar{v}$ is a different view from view $v$), where $\\pi_{j}^{(\\bar{v})}$ is a cluster of NPs and $K$ is the desired number of clusters. \n\tThen we calculate the cluster center embedding $\\pmb{\\xi}_{j}^{(v)}$ for the $j$-th cluster of view $v$ as follows: \n\t\\vspace{-2mm}\n\t\\begin{equation}\n\t\\vspace{-2mm}\n\t\\pmb{\\xi}_{j}^{(v)} = \\frac { \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(\\bar{v})}} \\bm{sub}_{i}^{(v)} }{ \\| \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(\\bar{v})}} \n\t\\bm{sub}_{i}^{(v)}\\| } \\label{mvc_m}, \n\t\\vspace{-2mm}\n\t\\end{equation}\n\twhere $\\bm{sub}_{i}^{(v)}$ is the view-specific embedding of view $v$ for the NP $sub_{i}$ and $\\| \\cdot \\|$ denotes the $L_1$ vector norm. \n\t\n\t\\textbf{\\textit{E-step}}. In the expectation step, given the cluster center embeddings $\\pmb{\\xi}^{(v)} = \\{ \\pmb{\\xi}_{1}^{(v)}, ..., \\pmb{\\xi}_{j}^{(v)}, ..., \\pmb{\\xi}_{K}^{(v)} \\}$ \n\tof view $v$ calculated in the M-step, we assign an NP $sub_{i}$ to its corresponding cluster $\\pi_{j}^{(v)}$ of view $v$ \n\tby finding its closest cluster center embedding $ \\pmb{\\xi}_{j}^{(v)} $ based on the following formula: \n\t\\vspace{-2mm}\n\t\\begin{equation}\n\t\\begin{aligned}\n\t\\pi_{j}^{(v)} = \\{ sub_{i} \\in S : sim( \\bm{sub}_{i}^{(v)}, \\pmb{\\xi}_{j}^{(v)} ) \\geq sim( \\bm{sub}_{i}^{(v)}, \\pmb{\\xi}_{l}^{(v)} ) \\}, \\label{mvc_e}\n\t\\end{aligned}\n\t\\end{equation}\n\twhere $l$, $j \\in \\{ 1, ... , K \\} $, $l \\neq j $, $sim( \\cdot )$ is the cosine similarity, \n\tand $S = \\{ sub_1, ... , sub_i, ...  \\} $ is the whole set of NPs. \n\tIt is noted that all the NP embeddings for the NP $sub_{i}$ in $S$ have unit length (i.e., $ \\| \\bm{sub}_{i}^{(v)} \\| = 1 $). \n\t\n\tAfter performing an M-step and an E-step in one view, \n\tthe clustering result will get interchanged for an M-step and an E-step in the other view, and so on, \n\twhich is shown in the right part of Figure \\ref{figure_1}. \n\tThis iteration procedure is expected to minimize the loss function $\\mathcal{L}_{mvc}$ as follows: \t\n\t\\vspace{-1.5mm}\n\t\\begin{equation}\n\t\\mathcal{L}_{mvc} = \\sum_{v = 1} ^{2} \\sum_{j = 1} ^{K} \\sum_{sub_{i} \\in \\pi_{j}^{(v)}} 1 - sim( \\bm{sub}_{i}^{(v)}, \\pmb{\\xi}_{j}^{(v)} ), \\label{mvc_loss} \n\t\\vspace{-1.5mm}\n\t\\end{equation}\n\tWhen this iteration procedure converges, for each view $v$, we could yield a clustering result $\\pi^{(v)}$. \n\tHowever, there are still some conflicts between the clustering results of different views. \n\tIn order to eliminate conflicts and establish the final clustering result, \n\twe assign each NP to one distinct cluster based on its closest cluster center embedding. \n\tFor this purpose, we calculate a consensus mean $\\bm{m}_{j}^{(v)}$ for the $j$-th cluster of view $v$ as follows:\n\t\\vspace{-2mm}\n\t\\begin{equation}\n\t\\vspace{-1mm}\n\t\\bm{m}_{j}^{(v)} = \\frac { \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(1)} \\land sub_{i} \\in \\pi_{j}^{(2)}} \\bm{sub}_{i}^{(v)} }\n\t{ \\| \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(1)} \\land sub_{i} \\in \\pi_{j}^{(2)}} \\bm{sub}_{i}^{(v)}\\| }, \\label{mvc_mean}\n\t\\vspace{-1mm}\n\t\\end{equation}\n\t%\\vspace{-1mm}\n\tIt can be seen that the calculation of the consensus mean $\\bm{m}^{(v)}_j$ only considers those NPs that both views agree on. \n   Due to the fact that different views have different clustering qualities, we should treat them differently in the process of integrating their clustering results. \n   In order to quantify the quality of the clustering result for a view, we leverage a well-known statistics-based measure, i.e., Cali$\\rm{\\acute{n}}$ski-Harabasz (CH) index \\cite{calinski1974dendrite}. \n   To be specific, we calculate the CH index of the clustering result $\\pi^{(v)}$ of view $v$ as follows:\n\t\\vspace{-1mm}\n\t\\begin{equation}\t\n\t\\vspace{-1mm}\n\t{CH}^{(v)} = \\frac { \\sum_{j=1}^{K} \\left| \\pi_{j}^{(v)} \\right| \\cdot \\| \\pmb{\\xi}_{j}^{(v)} - \\pmb{\\xi}^{(v)} \\|^2 }\n\t{ \\sum_{j=1}^{K} \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(v)}} \\| \\bm{sub}_{i}^{(v)} - \\pmb{\\xi}_{j}^{(v)}\\|^2 } \\times \\frac { \\left| S \\right| -K }{ K-1 }  \\label{ch}, \n\t\\vspace{-1mm}\n\t\\end{equation}\n\twhere $\\left| \\pi_{j}^{(v)} \\right|$ is the number of NPs in the cluster $\\pi_{j}^{(v)}$, \n\t$\\pmb{\\xi}^{(v)}$ is the global center embedding (i.e., the mean embedding) of all NP embeddings in view $v$, and $\\left| S \\right|$ is the total number of NPs. \n\tFrom Formula \\eqref{ch}, it can be seen that high value of the CH index means the clusters are well separated to each other and dense in each intra-cluster, \n\tdemonstrating that this CH index is a reasonable indicator for the clustering quality. \n\tTherefore, we regard the CH index as the weight of each view and assign an NP $sub_{i}$ to the final cluster $ \\hat{\\pi_{j}}$ \n\tby averaging over the weighted cosine similarities in both views based on the following formula:\n\t\\vspace{-1mm}\n\t\\begin{equation}\n\t\\vspace{-1mm}\n\t\\begin{aligned}\n\t\\hat{\\pi_{j}} = \\{ sub_{i} \\in S : \\sum_{v = 1} ^{2} {CH}^{(v)} \\cdot sim ( \\bm{sub}_{i}^{(v)}, \\bm{m}_{j}^{(v)} ) \\\\\n\t\\geq \\sum_{v = 1} ^{2} {CH}^{(v)} \\cdot sim( \\bm{sub}_{i}^{(v)}, \\bm{m}_{l}^{(v)} ) \\}, \\label{mvc_pi}\n\t\\end{aligned}\n\t\\vspace{-1mm}\n\t\\end{equation}\n\twhere $l$, $j \\in \\{ 1, ... , K \\} $ and $l \\neq j $. \n\tWe define the final clustering result as $\\hat{\\pi} = \\{ \\hat{\\pi_1}, ... ,  \\hat{\\pi_K} \\} $. \n\tThis multi-view CH K-Means algorithm is summarized in Algorithm \\ref{alg1} of Appendix \\ref{sec:multi-view_ch}. \n\tFor the initialization, we randomly initialize the cluster center embeddings $\\pmb{\\xi}^{(2)}$ of view 2 as shown in Figure \\ref{figure_1}. \n\t\n\t\\vspace{-1.5mm}\n\t"
                    },
                    "subsubsection 3.1.2": {
                        "name": "sec:inverse-jump",
                        "content": "\n\tThere is an important issue still unresolved, that is, the number of clusters (i.e., the parameter $K$) is unknown, which is an input of Algorithm \\ref{alg1}. \n\tFor OKB canonicalization, data is large-scale and sparse, \n\tand the number of clusters is usually large, which makes this issue challenging. \n\t\n\tPrevious OKB canonicalization studies \\cite{galarraga2014canonicalizing, vashishth2018cesi, dash2021open} utilized the validation data set to \n\tfind the optimal clustering threshold in a semi-supervised manner. \n\tLin et al. \\cite{lin2019canonicalization} set the clustering threshold of different data sets to the same fixed value in an unsupervised manner. \n\tTo remedy the issue in a more flexible and unsupervised manner, \n\twe propose a novel algorithm called Log-Jump to predict the number of clusters in a data-driven way, \n\twhich only depends on the input embeddings of data without requiring any labels. \n\tThis Log-Jump algorithm is summarized in Algorithm \\ref{alg2}, and we elaborate it as follows. \n\t\n\tGiven a set of input embeddings $X = \\{ \\pmb{x}_1, \\pmb{x}_2, ... ,  \\pmb{x}_n \\} $ that needs to be grouped into clusters, $n$ is the number of input embeddings \n\tand also the possible maximum number of clusters. \n\tFirst, for each possible $K$, we derive the cluster center embeddings $\\pmb{\\xi} = \\{ \\pmb{\\xi}_{1}, ..., \\pmb{\\xi}_{K} \\}$ \n\tvia clustering the input embeddings $X$ using K-Means (w.r.t. lines 2 in Algorithm \\ref{alg2}). \n\tSubsequently, we calculate the distortion $d_{K}$ \\cite{sugar2003findingtn} (w.r.t. lines 3 in Algorithm \\ref{alg2}), \n\twhich is a quantity that measures the average distance, per dimension, \n\tbetween each input embedding and its closest cluster center embedding and defined as follows:\n\t\\vspace{-2mm}\n\t\\begin{equation}\n\t\\vspace{-1mm}\n\td_{K} = \\frac{1}{n \\cdot p} \\sum_{ \\pmb{x}_{i} \\in X } \\min\\limits_{j \\in \\{ 1,..,K \\} } (1 - sim( \\pmb{x}_{i}, \\pmb{\\xi}_{j} )), \\label{inverse-jump-dk}\n\t\\vspace{-2mm}\n\t\\end{equation}\n\twhere $p$ is the dimension of the input embedding, and $1 - sim( \\pmb{x}_{i}, \\pmb{\\xi}_{j} )$ is the cosine distance between $\\pmb{x}_{i}$ and $\\pmb{\\xi}_{j}$. \n\tNext, for each possible $K$, we calculate its Log-Jump measure $LJ_{K}$ using a logarithmic function based on the calculated distortion \n\t(w.r.t. line 6 in Algorithm \\ref{alg2}). \n\tIn the end, we output the predicted number of clusters $K^{*}$, whose Log-Jump measure $LJ_{K}$ is the largest. \n\t\n\tOverall, compared with the existing Jump method \\cite{sugar2003findingtn}, our proposed Log-Jump algorithm not only \n\tpredicts the number of clusters based on the Log-Jump measure, \n\tbut also changes the manner of calculating the distortion $d_{K}$ from Mahalanobis distance to cosine distance. \n\tWe will give a theoretical justification for this method based on information theoretic ideas in Appendix \\ref{A1} and \n\tintroduce a heuristic method to generate a small candidate range of possible $K$ for the sake of efficiency in Appendix \\ref{A2}.\n\t%\\iffalse\n\t\\begin{algorithm}[t]\n\t\t%\\textsl{}\\setstretch{1.8}\n\t\t\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n\t\t\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n\t\t\\caption{Log-Jump Algorithm}\n\t\t\\label{alg2}\n\t\t\\begin{algorithmic}[1]\n\t\t\t\\REQUIRE A set of input embeddings $X = \\{ \\pmb{x}_1, ... ,  \\pmb{x}_n \\} $, the number of input embeddings $ n $. \n\t\t\t\\FOR{$K = 1$ to $ n $}{\n\t\t\t\t\\STATE Compute the cluster center embeddings $\\pmb{\\xi} = K$-$Means(X, K)$\\vspace{-3.5mm}\n\t\t\t\t\\STATE Compute the distortion $ d_{K} $ by Formula \\eqref{inverse-jump-dk}\n\t\t\t}\n\t\t\t\\ENDFOR\n\t\t\t\\FOR{$K = 1$ to $n - 1$}{\n\t\t\t\t\\STATE $ LJ_{K} = \\log d_{K+1} - \\log d_{K} $ \n\t\t\t}\n\t\t\t\\ENDFOR\n\t\t\t\\ENSURE  the predicted number of clusters $ K^{*} = \\arg\\max\\limits_{K} {LJ_{K}} $\n\t\t\\end{algorithmic}\n\t\\end{algorithm}\n\t%\\fi\n\t\\vspace{-2mm}\n\t"
                    }
                },
                "subsection 3.2": {
                    "name": "fact_view",
                    "content": "\n\t\\vspace{-1mm}\n\tBased on the information from the fact view, we introduce how to learn view-specific embeddings of the fact view, called fact embeddings. \n\tNext, we introduce how to promote the learned fact embeddings via the proposed training data optimization strategy (i.e., data augmentation operator).\n\t\\vspace{-1.5mm}\n\t",
                    "subsubsection 3.2.1": {
                        "name": "Fact Embedding Learning",
                        "content": "\n\tTo capture the relational structural information of fact triples in an OKB, we could learn fact embeddings via a kind of KB embedding model (KEM), \n\tsuch as TransE \\cite{bordes2013translating} and RotatE \\cite{ sun2018rotate}. \n\t%such as TransE \\cite{bordes2013translating}, HolE \\cite{ nickel2016holographic} and RotatE \\cite{ sun2018rotate}.\n\tIt is noted that any KEM could be employed in this framework, as long as it could encode fact triples into fact embeddings. \n\tThat is to say, given a fact triple $t_{i} = < sub_{i} , rel_{i} , obj_{i} >$ in an OKB, KEM can project $sub_{i}$, $rel_{i}$, and $obj_{i}$ \n\tinto fact embeddings $\\bm{sub}_{i}^{(1)}$, $\\bm{rel}_{i}^{(1)}$, and $\\bm{obj}_{i}^{(1)}$ respectively. \n\tHere, we assume the score function of the KEM is $f(\\cdot)$, which measures the plausibility of a fact triple \\cite{ji2021survey}. \n\tTo learn fact embeddings, KEM usually minimizes a margin-based loss function $\\mathcal{L}_{fact}$ based on training data (i.e., fact triples): \n\t\\begin{equation}\n\t\\vspace{-1mm}\n\t\\mathcal{L}_{fact} = \\sum_{t_i \\in T^+} \\sum_{t'_i \\in T^-} \\left[ \\gamma + f(t_i) - f(t'_i) \\right]_+, \\label{transe_loss}\n\t\\vspace{-1mm}\n\t\\end{equation}\n\twhere $ \\left[ \\alpha \\right]_+ $ represents the maximum between $0$ and $\\alpha$, \n\t$ \\gamma > 0 $ is a margin hyperparameter, \n\t$ T^{+} $ denotes all the available fact triples of the OKB which are regarded as the positive training data, and\n\t$ T^{-} $ denotes the negative training data, which is usually generated via negative sampling and defined as follows:\n\t\\begin{equation}\n\t\\vspace{-0.5mm}\n\t\\begin{aligned}\n\tT^{-} = \\{ <sub'_i, rel_i, obj_i> | sub'_i \\in N \\} \\cup \\{ <sub_i, \\\\\n\trel_i, obj'_i> | obj'_i \\in N \\}, \n\t<sub_i, rel_i, obj_i> \\in T^{+}, \\label{negative_sample}\n\t\\end{aligned}\n\t\\vspace{-0.5mm}\n\t\\end{equation}\n\twhere $ N $ denotes the set of NPs in $ T^{+} $. \n\tFormula \\eqref{negative_sample} indicates that one of two NPs in a positive fact triple is replaced by another random NP (but not both at the same time). \n\t\n\t%\\vspace{-1.5mm}\n\t"
                    },
                    "subsubsection 3.2.2": {
                        "name": "sec:dao",
                        "content": "\n\tIn general, fact triples of an OKB are usually sparse, which prevents the KEM from learning high-quality fact embeddings. \n\tTo make the training data dense and augment it with more instances, \n\twe leverage prior knowledge embedded in seed pairs and propose a data augmentation operator, \n\ti.e., swap counterparts of the seed pairs in their involved fact triples, \n\tto derive additional augmented training data, \n\twhich could be regarded as a training data optimization strategy \n\tin terms of data quantity. \n\tFor an OKB, the original training data (i.e., all the available fact triples in the OKB) is denoted by $ T^{O} $, \n\tand the augmented training data is denoted by $ T^{A} $, \n\twhich is generated via the data augmentation operator. \n\tAs aforementioned, \n\twe could collect some seed pairs of synonymous NPs automatically from external resources \n\twithout any human involvement \n\tand thereafter leverage them as prior knowledge for high-quality encoding. \n\tLet $(u_i, u_j)$ denote a seed pair, where $u_i$ and $u_j$ are synonymous NPs that refer to the same entity. \n\tThe set of collected seed pairs is denoted by $U$. Given the seed pair set $U= \\{(u_{i}, u_{j}) \\}$ \n\tand the original training data $T^{O} = \\{ <sub_{i},rel_{i},obj_{i}> \\}$, \n\tthe augmented training data $ T^{A} $ is generated via swapping counterparts of the seed pairs in their involved fact triples as follows:  \n\t\\vspace{-1.5mm}\n\t\\begin{equation}\n\t\\vspace{-1mm}\n\t\\begin{aligned}\n\tT^{A} = \\{ <u_j, rel_i, obj_i> | sub_i = u_i \\} \\cup \\{ <sub_i, rel_i, u_j> | obj_i = u_i \\} \\\\\n\t\\cup \\{ <u_i, rel_i, obj_i> | sub_i = u_j \\} \\cup \\{ <sub_i, rel_i, u_i> | obj_i = u_j \\}. \\label{DAO}\n\t\\end{aligned}\n\t\\vspace{-1mm}\n\t\\end{equation}\n\t\\vspace{-1.5mm}\n\t\n\tIntuitively, a heuristic way to leverage the generated augmented training data $ T^{A} $ for encoding is combining $ T^{A} $ and $ T^{O} $ \n\tinto one positive training data set. \n\tHowever, these automatically collected seed pairs are error-prone. \n\tTherefore, triples in $ T^{A} $ are not entirely correct and the disagreement between $ T^{A} $ and $ T^{O} $ would not be helpful for learning high-quality embeddings. \n\tTo minimize the impact of the disagreement and further promote the learned fact embeddings, inspired by the idea of iterative training \\cite{sun2018bootstrapping}, \n\twe select $ T^{A} $ or $ T^{O} $ as the positive training data iteratively. \n\tThis way, KEM could learn superior fact embeddings based on the original training data $T^{O}$ and the augmented training data $ T^{A} $ alternately. \n\tIt should be noted that negative sampling also needs to be performed when $ T^{A} $ is selected as the positive training data. \n\t\\vspace{-2mm}\n\t"
                    }
                },
                "subsection 3.3": {
                    "name": "context_view",
                    "content": "\n\t\\vspace{-1mm}\n\tBased on the information from the context view, we introduce how to extract view-specific embeddings of the context view, called context embeddings. \n\tNext, we introduce how to refine the learned context embeddings \n\tvia the proposed training data optimization strategy \n\t(i.e., iterative clustering procedure). \n\t\\vspace{-2mm}\n\t",
                    "subsubsection 3.3.1": {
                        "name": "Context Embedding Learning",
                        "content": "\n\tTo exploit the semantic distributional information of the source context where the fact triple occurs, \n\twe extract context embeddings resorting to a kind of pre-trained language model (PLM), such as BERT \\cite{kenton2019bert} and ELMo \\cite{Peters:2018}. \n\t%we extract context embeddings resorting to a kind of pre-trained language model (PLM), such as BERT \\cite{kenton2019bert}, XLNet \\cite{ yang2019xlnet} and ELMo \\cite{Peters:2018}. \n\tFor an NP $sub_i$, we leverage its source context $ c_{sub_i} $ as the input of the PLM and regard the output embedding as its context embedding $ \\bm{sub}_{i}^{(2)} $. \n\tIt is also worth mentioning that any PLM could be leveraged in this framework, as long as it could encode source contexts into context embeddings.\n\t\n\t\\vspace{-2.5mm}\n\t"
                    },
                    "subsubsection 3.3.2": {
                        "name": "sec:icp",
                        "content": "\n\tWhen fine-tuned over enough task-specific labeled training data, PLM could achieve excellent performance on the specific task, \n\tbut these task-specific labeled training data usually require large amounts of manual annotation. \n\tAs source contexts in this task are unlabeled,\n\twe derive their pseudo-labels that represent the referring entities of their corresponding NPs as follows. \n\tFirst, we cluster source contexts based on the seed pair set $U$ by grouping two source contexts into a cluster, \n\tif their corresponding NPs form a pair in $U$, which means these two NPs are synonymous and refer to the same entity. \n\tIf an NP does not occur in any seed pair of $U$, its source context will be assigned into a singleton cluster. \n\tIn this case, source contexts grouped into the same group should have the same pseudo-label. \n\tBased on this clustering result of source contexts, a distinct pseudo-label is assigned to each cluster, \n\tand the source context in a cluster is given the corresponding pseudo-label of that cluster. \n\tThereafter, we could obtain the pseudo-label set $ Y_{sub} = \\{ y_{sub_1}, ... , y_{sub_i}, ... \\} $,\n\twhere $ y_{sub_i}$ is the pseudo-label of the source context $ c_{sub_i} $. \n\tUltimately, we leverage these pseudo-labels $ Y_{sub} $ of source contexts \n\tto fine-tune the PLM with a classifier MLP jointly using the Cross Entropy loss function. \n\n\tIt is noted that pseudo-labels generated in the aforementioned way are not perfect. \n\tSome source contexts whose corresponding NPs refer to the same entity may not have the same pseudo-label, \n\tsince the coverage of the collected seed pairs is very limited and many real synonymous NPs do not appear as a seed pair in the collected seed pair set. \n\tIn order to yield high-quality context embeddings, \n\twe require superior pseudo-labels of source contexts for fine-tuning the PLM to further refine its output context embeddings.\n\t\n\tTo achieve this goal, we propose an iterative clustering procedure to transfer the prior knowledge from limited seed pairs to other pairs, \n\tto generate more accurate pseudo-labels, which could be regarded as a training data optimization strategy in terms of data quality. \n\tSpecifically, in each iteration, we first cluster source contexts via the hierarchical agglomerative clustering (HAC) algorithm \n\tbased on their context embeddings output by the PLM. \n\tHere, we have the assumption that synonymous NPs referring to the same entity may occur in semantically similar source contexts. \n\tConsequently, source contexts grouped into the same cluster are expected to mention the same entity and thus should be assigned the same pseudo-label. \n\tThen, we yield new pseudo-labels of source contexts based on the new clustering result. \n\tFinally, we fine-tune the PLM with a classifier MLP by leveraging these newly generated pseudo-labels. \n\tAfter fine-tuning, PLM is expected to output better context embeddings, \n\twhich are applied to a new round of iteration to establish a better clustering result and then better pseudo-labels. \n\tIt can be seen that this iterative clustering procedure performs clustering and fine-tuning alternately to enhance the quality of context embeddings iteratively. \n\tMoreover, clustering is the key step to transfer the synonymous relationship of seed pairs to other pairs, \n\tvia grouping semantically similar source contexts that may mention the same entity into the same cluster, so that they are assigned the same pseudo-label. \n\tIt is also worth mentioning that we predict the number of clusters for the hierarchical agglomerative clustering algorithm \n\tusing the Log-Jump algorithm (i.e., Algorithm \\ref{alg2}) introduced in Section \\ref{sec:inverse-jump} in a data-driven way as well.\n\t\n\t\\vspace{-1mm}\n\t"
                    }
                }
            },
            "section 4": {
                "name": "Experimental Study",
                "content": "\n\t%\\vspace{-1mm}\n\t",
                "subsection 4.1": {
                    "name": "Experimental Setting",
                    "content": "\n\t%\\vspace{-1mm}\n\t\\textbf{Data sets}. \n\tStatistics of the three real-world OKB data sets used in the experiments are shown in Table \\ref{data sets}.\n\tWe present below a brief summary of each data set.\n\t\n\t$\\bullet$ ReVerb45K. \n\tThis OKB data set contains 45k OIE triples extracted by ReVerb \\cite{fader2011identifying} from the source text in Clueweb09 \\cite{callan2009clueweb09}.\n\tAll NPs in these OIE triples are linked to their corresponding Freebase entities via an entity linking tool \\cite{gabrilovich2013facc}, \n\tand each entity has at least two aliases occurring as NPs. \n\tUnfortunately, this automatic linking process inevitably produces some mistakes. \n\tFor example, the subject \\textit{Google} of the OIE triple \\textit{<Google, just bought, Pyra Labs> } and \n\tthe subject \\textit{Yahoo} of the OIE triple \\textit{<Yahoo, has relationships with, MSN> } are both linked to the same entity, which is obviously wrong. \n\tTo mitigate this issue, we manually amended such mistakes existing in 947 OIE triples.\n\t\n\t$\\bullet$ NYTimes2018. \n\tThis OKB data set contains 34k OIE triples which are extracted by Standford OIE \\cite{angeli2015leveraging} over 1500 articles from nytimes.com in 2018, \n\tand all NPs in these OIE triples are not linked to any CKB.\n\t\n\t$\\bullet$ OPIEC59K. \n\tThis OKB data set contains 59k OIE triples, \n\tdistilled from the OPIEC-Linked corpus \\cite{gashteovski2019opiec} that contains 5.8M OIE triples. \n\tThe OPIEC-Linked corpus is extracted by MinIE \\cite{gashteovski2017minie} from English Wikipedia articles \n\tand contains only those OIE triples in which both NPs (i.e., subject and object) \n\thave Wikipedia links. \n\tWikipedia links are anchor texts existing in Wikipedia articles and provided by editors to link NPs in Wikipedia articles with \n\ttheir corresponding Wikipedia entities, which could be regarded as gold disambiguation links. \n\tBased on these Wikipedia links, we could group NPs that are linked to the  same Wikipedia entity into the same cluster. \n\tLike ReVerb45K, each subject of OIE triples in this data set has at least two aliases. \n\tIt is worthy to notice that this is an OKB canonicalization data set with the most accurate annotation so far to the best of our knowledge. \n\t\n\tFor these three OKB data sets, no training set is given.\n\tFor ReVerb45K, we leverage the triples associated with 20\\% selected Freebase entities of ReVerb45K as the validation set, \n\tand the rest triples of ReVerb45K as the test set following the previous study \\cite{vashishth2018cesi}.  \n\tFor OPIEC59K, we select 10\\% OIE triples as the validation set, and the rest triples are regarded as the test set. \n\tIn this experiment, we use the validation set to search the optimal parameters of semi-supervised baseline methods, \n\tand the test set to evaluate the performance of all the methods including both semi-supervised and unsupervised ones. \n\tSpecially, for the NYTimes2018 data set which is not linked to any CKB, we randomly sample 100 non-singleton NP groups and manually label them \n\tas the ground truth for NP canonicalization like the previous study \\cite{lin2019canonicalization}.\n\tFor evaluating RP canonicalization on each of the three data sets,\n\twe randomly sample 35 non-singleton RP groups and manually label them as the ground truth, which is the same as the previous study \\cite{lin2019canonicalization}. \n\t\n\t\\noindent\\textbf{Evaluation measures}. \n\tFollowing the previous OKB canonicalization studies \\cite{galarraga2014canonicalizing, vashishth2018cesi, lin2019canonicalization, dash2021open, liu2021joint}, \n\twe utilize macro, micro, and pairwise metrics for evaluating the performance of OKB canonicalization methods. \n\tDue to the limited space, we omit the detailed computing methods of these metrics and you could refer to \n\t\\cite{galarraga2014canonicalizing, vashishth2018cesi, lin2019canonicalization, dash2021open, liu2021joint} for more details. \n\tMacro, micro, and pairwise metrics evaluate results from different perspectives. \n\tTo give an overall evaluation of each method, we calculate the average of macro F1, micro F1, and pairwise F1 as \\textbf{average F1}, \n\twhich is a standard comprehensive metric for the task of OKB canonicalization.\n\t\n\t\\noindent\\textbf{Setting details}. \n\tFor all baselines, we perform grid search over hyperparameter spaces and report their results under the best performing setting. \n\tTo instantiate our framework CMVC, we need to choose a specific KEM and PLM to work with our framework. \n\tIn the experiment, we choose the well-known TransE \\cite{bordes2013translating} and BERT \\cite{kenton2019bert} as the KEM and PLM respectively, which have achieved satisfactory results. \n\tWe use these two simple models to give prominence to the effects of our framework.\n\tNote that the performance of choosing different KEMs and PLMs is not the focus of this paper and left for future exploration.\n\tFor the fact view, all input embeddings are initialized via Crawl embeddings, \n\twhich is a common pre-trained word embeddings by using fastText \\cite{bojanowski2017enriching} trained on Common Crawl\\footnote{\\href{https://commoncrawl.org/2017/06}{https://commoncrawl.org/2017/06}}.\n\tSpecifically, for an NP which contains multiple words, we average the Crawl embeddings of all single words as its embedding for simplicity. \n\tThe learning rate and the margin hyperparameter are set to $0.0001$ and $12$ respectively.\n\tFor the context view, the learning rate is set to $0.005$. \n\tFor the multi-view clustering, the number of iterations and the tolerance are set to $10$ and $10^{-4}$ respectively. \n\tFor the Log-Jump algorithm, we initialize the input embeddings via Crawl embeddings as well. \n\tAll experiments are implemented by MindSpore Framework\\footnote{https://www.mindspore.cn/en}. \n\tWe make the datasets and the source code used in this paper publicly available for future research\\footnote{\\href{https://github.com/Yang233666/cmvc}{https://github.com/Yang233666/cmvc}}.\n\t\n\t\\vspace{-2mm}\n\t"
                },
                "subsection 4.2": {
                    "name": "Effectiveness Study",
                    "content": "\n\t\\vspace{-1mm}\n\t",
                    "subsubsection 4.2.1": {
                        "name": "NP canonicalization",
                        "content": "\n\tAll baselines are listed as follows.\n\t\n\t$\\bullet$ Morph Norm \\cite{fader2011identifying}\n\tapplies several simple normalization operations (e.g., removing tenses, pluralization, and capitalization) over NPs \n\tand groups the same NPs after normalization into a group.\n\t\n\t$\\bullet$ Text Similarity \\cite{lin2019canonicalization}\n\tcalculates the Jaro-Winkler similarity \\cite{winkler1999state} between two NPs and employs the HAC method to cluster them.\n\t\n\t$\\bullet$ IDF Token Overlap \\cite{galarraga2014canonicalizing}\n\tmeasures the similarity between two NPs based on the inverse document frequency (IDF) of their tokens, and leverages the HAC method for clustering.\n\t\n\t$\\bullet$ Attribute Overlap \\cite{galarraga2014canonicalizing}\n\tleverages the Jaccard similarity of attributes between two NPs for NP canonicalization and clusters NPs via HAC. \n\t\n\t$\\bullet$ CESI \\cite{vashishth2018cesi}\n\tis a deep learning based method for OKB canonicalization, which learns the embeddings of NPs and RPs \n\tby leveraging relational information in the fact triples and side information, \n\tand then clusters the learned embeddings to obtain canonicalized NP (RP) groups via HAC.\n\t\n\t$\\bullet$ SIST \\cite{lin2019canonicalization}\n\tis an unsupervised method that leverages knowledge embedded in the source text to cluster NPs and RPs jointly for OKB canonicalization. \n\t\n\t$\\bullet$ CUVA \\cite{dash2021open}\n\tis a semi-supervised method for OKB canonicalization, which uses variational deep autoencoders to jointly learn both the embeddings and cluster assignments \n\tby leveraging relational information in the fact triples and side information. \n\t\n\t$\\bullet$ JOCL$_{cano}$ \\cite{liu2021joint}\n\tis an OKB canonicalization framework based on factor graph model, which leverages diverse signals including word embedding, \n\tPPDB \\cite{pavlick2015ppdb}, AMIE \\cite{galarraga2013amie} and transitive relation signals. \n\t\n\tThe experimental results of all the methods for NP canonicalization are shown in Table \\ref{np_all}.\n\tExcept JOCL$_{cano}$ \\cite{liu2021joint}, all the baseline results over the NYTimes2018 data set are directly taken from SIST \\cite{lin2019canonicalization}. \n\tAs we have amended some mistakes in the ReVerb45K data set, ReVerb45K used in this experiment is a little different from it in SIST \\cite{lin2019canonicalization}. \n\tTherefore, we executed all the baselines over the new version of ReVerb45K as well as the newly constructed OPIEC59K. \n\tWe fail to obtain the experimental results of some baselines over some data sets, since the source codes obtained from their authors do not work well over these data sets. \n\t\n\tOverall, it can be seen from Table \\ref{np_all} that our proposed framework CMVC consistently outperforms all competitive baselines \n\tin terms of average F1 over the three data sets.\n\tThe four simple baselines (i.e., Morph Norm, Text Similarity, IDF Token Overlap, and Attribute Overlap) perform poorly over all these three data sets. \n\tThis may be due to the fact that they mainly rely on the surface forms of NPs and often fail to deal with the cases when NPs with different surface forms refer to the same entity \n\tand NPs with similar surface forms refer to different entities. \n\tThe four recent advanced baselines (i.e., CESI, SIST, CUVA and JOCL$_{cano}$) yield much better performance than the four simple baselines. \n\tTo be specific, CESI improves the quality of the NP canonicalization via learning NP embeddings using relational information in the fact view \n\tand various side information. \n\tSIST exceeds CESI on NYTimes2018 by leveraging knowledge embedded in the source context where the OIE fact triple is extracted. \n\tCUVA achieves satisfactory performance on ReVerb45K, but performs poorly on OPIEC59K, probably because the performance of variational deep autoencoders is not stable. \n\tThe performance of JOCL$_{cano}$ is limited as it mainly leverages string and word embedding similarities of NPs. \n\tCompared with all these baselines each of which only leverages the knowledge from a single view, \n\tour framework CMVC surpasses all of them by integrating the complementary knowledge from both views, \n\twhich validates the effectiveness of CMVC in NP canonicalization. \n\t\\vspace{-2.5mm}\n\t\n\t"
                    },
                    "subsubsection 4.2.2": {
                        "name": "RP canonicalization",
                        "content": "\n\tIn addition to CESI \\cite{vashishth2018cesi}, JOCL$_{cano}$ \\cite{liu2021joint} and SIST \\cite{lin2019canonicalization}, \n\twe add AMIE \\cite{galarraga2013amie} and PATTY \\cite{nakashole2012patty} as baselines on the task of RP canonicalization. \n\t\n\t$\\bullet$ AMIE \\cite{galarraga2013amie}\n\tjudges whether two given RPs represent the same semantc meaning by learning Horn rules and requires NPs canonicalized already. \n\tTherefore, we only perform AMIE on Reverb45K and OPIEC59K, both of which contain gold NP canonicalization results. \n\tIf there is a rule $p_i \\rightarrow p_j$ output by AMIE, we consider RPs $p_i$ and $p_j$ to be synonymous, and put them into a cluster. \n\t\n\t$\\bullet$ PATTY \\cite{nakashole2012patty}\n\tputs OIE triples with the same pairs of NPs as well as RPs belonging to the same synset in PATTY into one cluster.\n\t\n\tExperimental results for RP canonicalization are shown in Table \\ref{rp_can}.\n\tIt can be seen that CMVC surpasses all baselines in terms of average F1 on all these three data sets.\n\tAMIE obtains unsatisfactory performance, which may be attributed to the fact that it only covers very few RPs, which leads to most RPs discarded.\n\tCESI shows superiority over AMIE, since it employs the knowledge embedded in the fact view to cluster RPs with the same semantics.\n\tPATTY performs well on ReVerb45K since it leverages the synset of each RP for clustering. \n\tJOCL$_{cano}$ performs well on OPIEC59K, but performs unsatisfactory over the other two data sets. \n\tSIST further improves the performance by utilizing the knowledge embedded in the context view. \n\tCompared with SIST, CMVC promotes by over 1 percentage in terms of average F1\n\ton both ReVerb45K and NYTimes2018 via jointly leveraging the knowledge from the fact view and the context view, \n\tindicating the superiority of CMVC in RP canonicalization.\n\t\n\t\n\t\n\t\\vspace{-1.5mm}\n\t"
                    }
                },
                "subsection 4.3": {
                    "name": "Ablation Study",
                    "content": "\n\t\\vspace{-1mm}\n\tTo examine the effectiveness of our framework CMVC in combining the complementary knowledge from both views, \n\twe remove the multi-view clustering algorithm described in Section \\ref{sec:mvc} from CMVC and make two views work alone. \n\tWe present the performance of three variants, namely, CMVC$_{fact}$ (i.e., CMVC working on the fact view alone), \n\tCMVC$_{cnt}$ (i.e., CMVC working on the context view alone), \n\tand CMVC (i.e., the whole framework) on OPIEC59K for NP canonicalization in Table \\ref{np_ablation}. \n\tIn addition, to verify the crucial importance of our proposed training data optimization \n\tstrategy in each view, we remove data augmentation operator (DAO) described in Section \\ref{sec:dao} from the variant CMVC$_{fact}$ \n\tand remove iterative clustering procedure (ICP) described in Section \\ref{sec:icp} from the variant CMV$C_{cnt}$. \n\tFurthermore, we remove the CH index described in Section \\ref{sec:mvch} from the variant CMVC to verify its importance. \n\tWe show the experimental results of these three variants in Table \\ref{np_ablation} as well as another variant called Seed Pairs \n\tthat just utilizes the collected seed pairs to generate the canonicalization result. \n\tThe seed pair collection process is described in Appendix \\ref{sec:seed_pairs}. \n\tNote that the four variants (i.e., CMVC$_{fact}-$DAO, CMVC$_{fact}$, CMVC$_{cnt}-$ICP and CMVC$_{cnt}$) that output view-specific embeddings \n\tare combined with the HAC method to generate the final canonicalization result.\n\t\n\tFrom the experimental results in Table \\ref{np_ablation}, we can see that \n\t(1) CMVC surpasses CMVC$_{fact}$ and CMVC$_{cnt}$ by 3.0 and 6.1 percentages in terms of average F1, respectively, \n\twhich validates the point that our proposed multi-view clustering algorithm could indeed mutually reinforce the clustering of view-specific embeddings \n\textracted from each view and harness the complementary knowledge from both views for better canonicalization; \n\t(2) compared with CMVC$_{fact}-$DAO (CMVC$_{cnt}-$ICP), CMVC$_{fact}$ (CMVC$_{cnt}$) promotes by an average F1 of 9.8 (16.1) percentages. \n\tThis confirms that the proposed training data optimization strategy in each view boosts the quality of \n\tthe learned view-specific embeddings significantly and thus enhances the canonicalization performance obviously; \n\t(3) compared with CMVC$-$CH, CMVC promotes by an average F1 of 1.7 percentage, \n\twhich validates that it is meaningful to consider the different clustering qualities of different views in multi-view clustering rather than treating them equally; \n\t(4) in comparison to Seed Pairs, CMVC$_{fact}$ (CMVC$_{cnt}$) improves by an average F1 of 4.6 (1.5) percentages, \n\tindicating that both views effectively transfer prior knowledge from seed pairs to other pairs via high-quality encoding. \n\tIn summary, each component in our proposed framework CMVC has a positive contribution to the canonicalization performance, \n\tand when all the components are consolidated together in CMVC, it yields the best performance. \n\t\n\n\t\\vspace{-2mm}\n\t"
                },
                "subsection 4.4": {
                    "name": "Convergence Study",
                    "content": "\n\t%\\vspace{-1mm}\n\tTo investigate the convergence and effectiveness of our proposed framework CMVC, we show how the loss and average F1 of the fact view, \n\tthe context view and the whole framework CMVC respectively change with respect to the number of iterations \n\ton OPIEC59K in Figure 2. \n\tFrom this figure, we can draw the following observations: \n\t(1) the loss decreases rapidly with the increase of the number of iterations in the fact view, the context view, and CMVC, \n\tdemonstrating that they could achieve a very rapid convergence; \n\t(2) the average F1 promotes rapidly as the number of iterations increases in these two single views and CMVC; \n\t(3) the increasing speed of the average F1 of the fact view and CMVC slows down as the number of iterations increases. \n\t\\iffalse\n\t\n\t\\fi\n\t%\\iffalse\n\t\n\t%\\fi\n\t\\vspace{-2mm}\n\t"
                },
                "subsection 4.5": {
                    "name": "Effect Analysis of Data-Driven Parameter Prediction",
                    "content": "\n\t%\\vspace{-1mm}\n\tTo validate the effectiveness of our proposed Log-Jump algorithm (introduced in Section \\ref{sec:inverse-jump}) in predicting the parameter (i.e., the number of clusters), \n\twe compare it with twenty seven existing cluster number prediction methods on a total of thirteen real-world data sets. \n\tWe obtain six data sets from \\cite{gupta2018fast} and five data sets from OpenML \\cite{OpenML2013}, and show the number of input embeddings, \n\tthe dimension of the input embedding and the number of clusters for each data set in Table \\ref{predict_k_data set} of Appendix \\ref{sec:experiments_details}. \n\tIn addition to these eleven data sets, we leverage two OKB canonicalization data sets (i.e., ReVerb45K and OPIEC59K) \n\tboth of which have been annotated for NP canonicalization. \n\tNote that the numbers of clusters in these two OKB data sets are much larger than the other eleven data sets, which brings a new challenge. \n\tTo give an overall evaluation of each cluster number prediction method, besides the average of rank (AR) over all data sets used in \\cite{gupta2018fast}, \n\twe calculate the average of relative error (ARE) as well. \n\tThe metric of relative error is defined as $\\frac{\\left\\vert k - k^{*} \\right\\vert}{k^{*}} \\label{re}$, \n\twhere $k$ and $k^*$ denote the predicted number of clusters and the gold number of clusters of a data set, respectively.\n\t\n\tFrom the results shown in Table \\ref{predict_k_result}, it can be seen that our proposed method Log-Jump \n\tsignificantly outperforms all the twenty seven baselines in terms of AR and ARE, exhibiting the superiority \n\tof Log-Jump for predicting the number of clusters. \n\tMoreover, our proposed Log-Jump algorithm could be applied to not only the OKB canonicalization task addressed in this paper, \n\tbut also other clustering tasks that need to estimate the number of clusters. \n\tThe detailed experimental results of all methods over each data set are shown in Tables \\ref{predict_k_all_rank_result} and \\ref{predict_k_all_relative_error_result} of Appendix \\ref{sec:experiments_details}. \n\t\n\t\\vspace{-3mm}\n\t"
                }
            },
            "section 5": {
                "name": "RELATED WORK",
                "content": "\n\t%\\vspace{-1mm}\n\tTwo aspects of research are related to our work: OKB canonicalization and multi-view clustering, which are introduced as follows. \n\t\n\tFor the task of OKB canonicalization, previous methods can be divided into two categories: \n\tsemi-supervised methods \\cite{galarraga2014canonicalizing, wu2018towards, vashishth2018cesi, dash2021open, liu2021joint} and unsupervised method \\cite{lin2019canonicalization}.\n\tOur proposed framework CMVC belongs to the latter without the requirement of any manually annotated label. \n\tIn addition, according to the view of knowledge leveraged for OKB canonicalization, previous methods could also be classified into two types: \n\tfact view based methods \\cite{galarraga2014canonicalizing, wu2018towards, vashishth2018cesi, dash2021open, liu2021joint} and context view based method \\cite{lin2019canonicalization}. \n\tSpecifically, the first OKB canonicalization model \\cite{galarraga2014canonicalizing} performs the HAC method over manually-defined features, \n\tsuch as IDF token overlap and attribute overlap, to cluster synonymous NPs, and then clusters RPs by leveraging AMIE \\cite{galarraga2013amie}. \n\tBased on this canonicalization model \\cite{galarraga2014canonicalizing}, \n\tFAC \\cite{wu2018towards} proposes a more efficient graph-based clustering method, \n\twhich utilizes pruning and bounding techniques to reduce similarity computation. \n\tCESI \\cite{vashishth2018cesi} clusters the embeddings of NPs (RPs), \n\twhich are learned by leveraging a KB embedding model based on the fact view and various side information \n\t(i.e., Entity Linking, PPDB, WordNet, IDF token overlap, morph normalization, AMIE, and KBP), to obtain canonicalized NP (RP) groups. \n\tSIST \\cite{lin2019canonicalization} leverages knowledge from the original source text (i.e., the context view), \n\tto cluster NPs and RPs jointly using an efficient clustering method. \n\tCUVA \\cite{dash2021open} uses variational deep autoencoders to jointly learn both embeddings and cluster assignments in a semi-supervised way. \n\tJOCL$_{cano}$ \\cite{liu2021joint} is based on factor graph model by leveraging diverse signals including word embedding, \n\tPPDB, AMIE and transitive relation signals. \n\tAll aforementioned works only leverage the knowledge from a single view. \n\tBeyond that, our framework CMVC harnesses the complementary knowledge from both views effectively for OKB canonicalization. \n\t\n\tMulti-view clustering aims to provide more accurate and stable partitions than single view clustering \n\tby considering complementary information embedded in multiple views,\n\twhich has achieved prominent success in diverse tasks, such as graph clustering \\cite{ fan2020one2multi} and dialog intent induction \\cite{ perkins2019dialog}.  \n\t%which has achieved prominent success in diverse tasks, such as graph clustering \\cite{ fan2020one2multi}, dialog intent induction \\cite{ perkins2019dialog} and image clustering \\cite{ xie2020joint}. \n\tTo the best of our knowledge, this paper is the first work that leverages multi-view clustering to investigate the task of OKB canonicalization. \n\t\n\t\\vspace{-1.5mm}\n\t"
            },
            "section 6": {
                "name": "CONCLUSION",
                "content": "\n\t\\vspace{-0.5mm}\n\tThe complementarity between the fact view and the context view is vital to the task of OKB canonicalization, but previous works ignore it. \n\tIn this paper, we propose a novel unsupervised framework CMVC that integrates the complementary knowledge delivered by both views \n\tvia a multi-view CH K-Means algorithm by considering their different clustering qualities. \n\tIn order to further enhance the canonicalization performance, we propose a training data optimization strategy in terms of \n\tdata quantity and data quality respectively in each particular view to refine the learned view-specific embeddings in an iterative manner. \n\tTo demonstrate the effectiveness of CMVC, we conduct extensive experiments over three real-world OKB data sets, \n\tand the experimental results show that our framework surpasses all baselines in terms of average F1. \n\t\\vspace{-2mm}\n\t%\\section{ACKNOWLEDGMENTS}\n\t\\begin{acks}\n\tThis work was supported in part by National Natural Science Foundation of China (No. U1936206), YESS by CAST (No. 2019QNRC001), and CAAI-Huawei MindSpore Open Fund. \n\t\\end{acks}\n\t%Since the scope of possible attribute values may be open-ended in nature, we will consider to obtain more general Web documents via a Web search engine using some attribute-specific patterns.\n\t\\vspace{-2mm}\n\t\n\t%%\n\t%% The next two lines define the bibliography style to be used, and\n\t%% the bibliography file.\n\t%% \\clearpage\n\t\\bibliographystyle{ACM-Reference-Format}\n\t\\bibliography{sample-base}\n\t\n\t%%\n\t%% If your work has an appendix, this is the place to put it.\n\t\\clearpage\n\t\\normalsize\n\t\\appendix\n\t\n\t"
            },
            "section 7": {
                "name": "sec:multi-view_ch",
                "content": "\n\t\\vspace{-3.5mm}\n\t\\begin{algorithm}[htbp]\n\t\t%\\textsl{}\\setstretch{1.8}\n\t\t\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n\t\t\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n\t\t\\caption{Multi-View CH K-Means Algorithm}\n\t\t\\label{alg1}\n\t\t\\begin{algorithmic}[1]\n\t\t\t\\REQUIRE A set of NPs $S = \\{ sub_1, ... , sub_i, ...  \\} $ and their view-specific embeddings of two views $ \\{(\\bm{sub}_{1}^{1}, \\bm{sub}_{1}^{2}), ..., (\\bm{sub}_{i}^{1}, \\bm{sub}_{i}^{2}), ...\\}$, the number of iterations $T$, the tolerance $tol$, the number of clusters $K$. \n\t\t\t\\STATE Initialization: randomly initialize the cluster center embeddings $ \\pmb{\\xi}^{(2)} $ of view 2. \n\t\t\t\\STATE $\\bm{E}$-$\\bm{step}$ in view $2$: compute the clustering result $ \\pi^{(2)} $ by Formula~(\\ref{mvc_e})\n\t\t\t\\STATE $ t = 0 $\n\t\t\t\\REPEAT\t\n\t\t\t\\FOR{$v=1$ to $2$}{\n\t\t\t\t\\STATE $t \\leftarrow t + 1$\n\t\t\t\t\\STATE $\\bm{M}$-$\\bm{step}$ in view $v$: compute the cluster center embeddings $ \\pmb{\\xi}^{(v)} $ by Formula~(\\ref{mvc_m}) based on the clustering result $ \\pi^{(\\bar{v})}$ from the other view $\\bar{v}$ \n\t\t\t\t\\STATE $\\bm{E}$-$\\bm{step}$ in view $v$: compute the clustering result $ \\pi^{(v)} $ by Formula~(\\ref{mvc_e})\n\t\t\t}\n\t\t\t\\ENDFOR\n\t\t\t\\STATE Compute the loss function $\\mathcal{L}_{mvc}$ by Formula~(\\ref{mvc_loss})\n\t\t\t\\UNTIL $ t == T $ or $\\mathcal{L}_{mvc}$ < $tol$\n\t\t\t\\STATE Compute consensus means $\\bm{m}^{(1)}_j$ and $\\bm{m}^{(2)}_j$ by Formula \\eqref{mvc_mean}\n\t\t\t\\STATE Compute CH indexes ${CH}^{(1)}$ and ${CH}^{(2)}$ by Formula \\eqref{ch}\n\t\t\t\\STATE Compute the final clustering result $\\hat{\\pi}$ by Formula \\eqref{mvc_pi}\n\t\t\t\\ENSURE the final clustering result $ \\hat{\\pi} $\n\t\t\\end{algorithmic}  \n\t\\end{algorithm}\n\t\\vspace{-4.5mm}\n\t"
            },
            "section 8": {
                "name": "Supplementary Materials for the Log-Jump Algorithm",
                "content": "\n\t",
                "subsection 8.1": {
                    "name": "A1",
                    "content": "\n\tTo give a theoretical justification for the Log-Jump algorithm based on information theoretic ideas, \n\tinspired by the proof in \\cite{sugar2003findingtn}, we give Theorem 1 and its proof as follows:\n\t\n\t\\textbf{Theorem 1} \\emph{Suppose that the distribution of the input X is a mixture of G Gaussian clusters with equal priors and common covariance $\\Gamma_{p}$. \n\t\tLet $\\Delta \\sqrt{p}$ be the minimum cosine distance between cluster means after standardizing the space by multiplying by $\\Gamma_{p}^{-1/2}$. \n\t\tThereafter, for $ K < G $}\n\t\\vspace{-0.7mm}\n\t\\begin{equation}\n\t\\lim_{p \\to \\infty} d_{K} = c \\quad (\\frac{p \\Delta^2}{9G}W < c \\le 1) \\label{Theorem 1}\n\t\\end{equation}\n\t\\vspace{-0.7mm}\n\twhere $W = 1 - \\frac{6^4 V_X}{{( \\Delta^2 - 36 )}^2}$, \n\t%$ V_X = Var {( \\frac{1}{p} \\| X - \\mu_{j} \\|_{\\Gamma^{-1}}^2 |X}$ in $j$-th cluster), \n\t$ V_X = \\min\\limits_{j \\in \\{ 1,..,K \\} } \\frac{1}{p} \\sum_{l = 1} ^{p} { ( X_{jl} - \\mu_{jl} )^2 }$, \n\t$\\mu_{j}$ is the cluster center embedding of $j$-th cluster, and $\\Delta > 6$. \n\t\n\t\\textbf{Proof}. According to Theorem 2 in \\cite{sugar2003findingtn}, \n\t\\vspace{-0.7mm}\n\t\\begin{equation}\n\td_{K} \\ge \\frac{p \\Delta^2}{9G}W, \n\t\\end{equation}\n\t\\vspace{-0.7mm}\n\tAs for $G > p$ and ${p \\to \\infty}$, thus $\\frac{p \\Delta^2}{9G}W > 0$. $d_{K}$ is calculated by cosine distance, thus $ d_{K} \\le 1 $. \n\t%Let $\\frac{p \\Delta^2}{9G}W \\le c \\le 1$, then we could obtain Formula \\eqref{Theorem 1}. \n\tLet $d_{K} = c$ when ${p \\to \\infty}$, then we could obtain Formula \\eqref{Theorem 1}. \n\t\n\tTheorem 1 implies that for large enough $p$ and $G$, $K<G$, then $d_{K}^{-p/2} \\approx c, (\\frac{p \\Delta^2}{9G}W \\le c \\le 1)$. \n\tWhile for $K>G$, thus $d_{K}^{-p/2} \\varpropto k^{p} \\approx K $. \n\tIt is easy to obtain $d_{K}$ as follows:\n\t\\vspace{-0.7mm}\n\t\\begin{equation}\n\td_{K} = \\begin{cases}\n\t{( \\frac{G}{aK} )} ^{2/p}, & \\text {$K > G$} \\quad (0 < a < 1) \\\\\n\tc, &\\text{$K \\le G$} \\quad (\\frac{p \\Delta^2}{9G}W \\le c \\le 1) \n\t\\end{cases}  \\label{d_k}\n\t\\end{equation}\n\t\\vspace{-0.7mm}\n\tNext, we could calculate the Log-Jump measure $LJ_K$ based on the logarithm of the distortion as follows: \n\t\\vspace{-0.7mm}\n\t\\begin{equation}\n\tLJ_{K} = \\log d_{K+1} - \\log d_{K} = \\begin{cases}\n\tc - c = 0, & \\text {K < G} \\\\\n\t\\log \\frac{1}{a} > 0, &\\text{K = G}  \\\\\n\t\\frac{2}{p} \\log \\frac{K}{K+1} < 0, &\\text{K > G}\n\t\\end{cases}  \\label{log_d_k}\n\t\\end{equation}\n\t\\vspace{-0.7mm}\n\tFinally, according to Formula \\eqref{log_d_k}, the number of clusters $G$ can be estimated as follows: \n\t\\vspace{-0.7mm}\n\t\\begin{equation}\n\t%G = \\arg\\max\\limits_{K} {\\left[ \\log d_{K+1} - \\log d_{K}  \\right]}\n\tG = \\arg\\max\\limits_{K} { LJ_{K} }\n\t\\end{equation}\n\t\n\t\\vspace{-1.5mm}\n\t"
                },
                "subsection 8.2": {
                    "name": "A2",
                    "content": "\n\tIn this section, we introduce a heuristic method to generate a small candidate range of possible $K$ for the sake of efficiency. \n\tOtherwise, we have to test each possible $K$ from $1$ to $n$ in Algorithm \\ref{alg2}, which is time-consuming. \n\t\n\tFor traditional clustering tasks, the number of clusters is usually small, and the candidate range of possible $K$ could be set as \n\t$ \\left[ 1, \\left \\lfloor \\frac{\\sqrt{n}}{2} \\right \\rfloor \\right] $. \n\tFor example, if the number of input embeddings $n = 100$, the candidate range of possible $K$ changes from \n\tthe original range $ \\left[1, 100\\right] $ to a small range $ \\left[1, 5\\right] $. \n\t\n\tConsidering the large number of clusters for the task of OKB canonicalization, \n\twe need to extend $\\log d_{K+1} - \\log d_{K}$ to $\\log d_{K+gap} - \\log d_{K}$, where $gap$ is a positive integer. \n\tSpecifically, we change the step size from $1$ to $gap$, which could be calculated as follows:\n\t\\begin{equation}\n\tgap = 10^{len(string(n)) - 2}, \\label{inverse-jump-max} \n\t\\end{equation}\n\twhere function $string(\\cdot)$ maps an integer $n$ to its string style, and $len(\\cdot)$ calculates the length of a string.\n\tFor example, if the number of input embeddings $n = 20000$, then the $gap$ will be $10^{len(``20000\")-2}=10^{5-2} = 1000$. \n\tIf the number of input embeddings $n > 10000$, the number of clusters may be large, and thus we generate the candidate range of possible $K$ as \n\t$ \\left[ 4gap, 9gap \\right] $. \n\tOtherwise, the number of clusters may be small, and then we generate the candidate range of possible $K$ as \n\t$ \\left[ 2gap, 9gap \\right] $. \n\tFor example, if the number of input embeddings $n = 20000$, then the step size changes from $1$ to $1000$, \n\tand the candidate range of possible $K$ changes from the original range $ \\left[1, 20000\\right] $ to a small range $ \\left[4000, 9000\\right] $. \n\tIn order to get a more accurate result, we could set $gap = gap / 10$, and perform the next round of iteration to predict the number of clusters \n\taccording to the rules introduced above. \n\t\n\t%\\noindent\\textbf{Seed pair collection}. \n\t\\vspace{-1.5mm}\n\t"
                }
            },
            "section 9": {
                "name": "sec:seed_pairs",
                "content": "\n\tWe collect seed pairs automatically from three external resources \n\t(i.e., mention entity dictionary \\cite{Shen2012linden, shen2021entity}, search engine, and AMIE \\cite{galarraga2013amie}) without the need of any human involvement. \n\t\n\t$\\bullet$ Mention entity dictionary \\label{sec:ep} contains information about possible mapping entities of various mentions \n\tand their corresponding prior mapping probabilities calculated based on the count information in Wikipedia. \n\tGiven an NP, we look up the dictionary and obtain its most likely mapping entity which has the maximum prior mapping probability. \n\tFor two NPs, if their most likely mapping entities are the same, they will form a seed pair. \n\t\n\t$\\bullet$ Search engine would return a collection of relevant Web pages for a given query. We query an NP (a RP) \n\tvia Bing\\footnote{\\href{https://www.bing.com/}{https://www.bing.com/}} and collect the URLs of the returned Web pages in the first ten search result pages. \n\tFor two NPs (RPs), if the Jaccard similarity of their corresponding URL collections is greater than a threshold, they will form a seed pair. \n\tIn the experiment, this threshold is set to 0.015.\n\t\n\t$\\bullet$ AMIE\\label{sec:amie} judges whether two given RPs are synonymous by learning Horn rules, and it could be used to generate seed pairs of RPs directly. \n\tIt requires the input data to be semi-canonicalized (i.e., NPs are canonicalized already), so we normalize NPs morphologically and then apply AMIE over the NP-canonicalized OKB. \n\t\n\t"
            },
            "section 10": {
                "name": "sec:experiments_details",
                "content": " \n\tFor the task of cluster number prediction, we show the statistics of the eleven data sets in Table \\ref{predict_k_data set} and the detailed experimental results \n\tof all methods over all data sets in Tables \\ref{predict_k_all_rank_result} and \\ref{predict_k_all_relative_error_result}. \n\t%\\iffalse\n\t\\vspace{-6.5mm}\n\t\n\t\\vspace{-6.5mm}\n\t\n\t\\vspace{-6.5mm}\n\t\n\t\\vspace{-6.5mm}\n\t%\\fi\n\t\n\t\n"
            }
        },
        "tables": {
            "data sets": "\\begin{table}[]\n\t\t%\\vspace{-3mm}\n\t\t\\caption{Statistics of the used OKB data sets. }\n\t\t\\vspace{-4.5mm}\n\t\t\\label{data sets}\n\t\t\\centering\n\t\t%\\small\n\t\t\\resizebox{0.42\\textwidth}{!}{%\n\t\t\t\\begin{tabular}{ccccc}\n\t\t\t\t\\hline\n\t\t\t\t\\textit{\\textbf{Data set}}    & \\textit{\\textbf{\\#Gold entities}} & \\textit{\\textbf{\\#NPs}} & \\textit{\\textbf{\\#RPs}} & \\textit{\\textbf{\\#OIE triples}} \\\\ \\hline\n\t\t\t\t\\hline\n\t\t\t\tReVerb45K   & 7.5k            & 15.5k & 22k       & 45k       \\\\\n\t\t\t\tNYTimes2018 & /               & 10.6k & 14k       & 34k       \\\\\n\t\t\t\tOPIEC59K    & 18.4k           & 22.8k & 17k       & 59k     \\\\ \\hline\n\t\t\t\\end{tabular}%\n\t\t}\n\t\\end{table}",
            "np_all": "\\begin{table*}[t]\n\t\t\\caption{Performance on the NP canonicalization task. }\n\t\t\\vspace{-4.5mm}\n\t\t\\label{np_all}\n\t\t\\centering\n\t\t%\\small\n\t\t\\resizebox{0.934\\textwidth}{!}{%\n\t\t\t\\begin{tabular}{c|ccc|c|ccc|c|ccc|c}\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{2}{*}{\\textit{\\textbf{Method}}} & \\multicolumn{4}{c|}{\\textit{\\textbf{ReVerb45K}}}                                  & \\multicolumn{4}{c|}{\\textit{\\textbf{NYTimes2018}}}                                  & \\multicolumn{4}{c}{\\textit{\\textbf{OPIEC59K}}}  \\\\ \\cline{2-13}\n\t\t\t\t& \\textit{Macro F1} & \\textit{Micro F1} & \\textit{Pairwise F1} & \\textit{\\textbf{Average F1}} & \\textit{Macro F1} & \\textit{Micro F1} & \\textit{Pairwise F1} & \\textit{\\textbf{Average F1}} & \\textit{Macro F1} & \\textit{Micro F1} & \\textit{Pairwise F1} & \\textit{\\textbf{Average F1}} \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\tMorph Norm \\cite{vashishth2018cesi}                                & 0.627                       & 0.558                    & 0.334                & 0.506                       & 0.471                    & 0.658                & 0.643                       & 0.590                    & 0.476                & 0.222             & 0.186                & 0.294                            \\\\ %\\hline\n\t\t\t\tText Similarity \\cite{lin2019canonicalization}                     & 0.625              \t\t\t& 0.566                    & 0.394                & 0.528                       & 0.581                    & 0.796                & 0.658                       & 0.678                    & 0.480                & 0.228             & 0.192                & 0.300                            \\\\ %\\hline\n\t\t\t\tIDF Token Overlap \\cite{galarraga2014canonicalizing}               & 0.603                       & 0.551                    & 0.338                & 0.497                       & 0.551                    & 0.612                & 0.527                       & 0.563                    & 0.457                & 0.225             & 0.190                & 0.290                            \\\\ %\\hline\n\t\t\t\tAttribute Overlap \\cite{galarraga2014canonicalizing}               & 0.621                       & 0.558                    & 0.342                & 0.507                       & 0.538                    & 0.593                & 0.561                       & 0.564                    & 0.474                & 0.226             & 0.187                & 0.295                            \\\\ %\\hline\n\t\t\t\tCESI \\cite{vashishth2018cesi}                                      & 0.640                       & 0.855                    & 0.842                & 0.779                       & 0.586                    & 0.842                & 0.778                       & 0.735                    & 0.328                & 0.807             & 0.667                & 0.600                            \\\\ %\\hline\n\t\t\t\tSIST \\cite{lin2019canonicalization}                                & /                           & /                        & /                    & /                           & 0.675           & 0.816                & 0.838                       & 0.776                    & /                    & /                 & /                    & /                                \\\\ %\\hline\n\t\t\t\tCUVA \\cite{dash2021open}                                      & \\textbf{0.682}                       & 0.872                    & 0.878                & 0.810                       & /                    & /  & /                       & /                    & 0.128                & 0.789             & 0.686                & 0.534                            \\\\ %\\hline\n\t\t\t\tJOCL$_{cano}$ \\cite{liu2021joint}                                      & 0.537                       & 0.854                    & 0.823                & 0.738                       & \\textbf{0.876}                    & 0.865  & 0.459                       & 0.733                    & 0.465                & 0.790             & 0.776                & 0.677                            \\\\ %\\hline\n\t\t\t\t\\hline\n\t\t\t\tCMVC              & 0.662              & \\textbf{0.881}           & \\textbf{0.893}\t\t   & \\textbf{0.812}              & 0.635           \t\t   & \\textbf{0.874}       & \\textbf{0.893}              & \\textbf{0.800}           & \\textbf{0.521}       & \\textbf{0.909}    & \\textbf{0.878}        & \\textbf{0.769}                   \\\\ \\hline\n\t\t\t\\end{tabular}%\n\t\t}\n\t\t\\vspace{-4mm}\n\t\\end{table*}",
            "rp_can": "\\begin{table}[t]\n\t\t\\caption{Performance on the RP canonicalization task. }\n\t\t\\vspace{-4.5mm}\n\t\t\\label{rp_can}\n\t\t\\centering\n\t\t\\resizebox{0.38\\textwidth}{!}{%\n\t\t\t\\begin{tabular}{cccc|c}\n\t\t\t\t\\hline\n\t\t\t\t\\textit{\\textbf{Method}} & \\textit{Macro F1} & \\textit{Micro F1} & \\textit{Pairwise F1} & \\textit{\\textbf{Average F1}} \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\t%\\multicolumn{5}{c}{\\textit{\\textbf{ReVerb45K}}}                                                                                                   \\\\\n\t\t\t\t\\multicolumn{4}{c|}{\\textit{\\textbf{ReVerb45K}}}                                                                                                   \\\\\n\t\t\t\tAMIE \\cite{galarraga2013amie}                     & 0.735                      & 0.863                      & 0.735                         & 0.777                        \\\\\n\t\t\t\tPATTY \\cite{nakashole2012patty}                   & 0.782                      & 0.872                      & 0.802                         & 0.818                        \\\\\n\t\t\t\tCESI \\cite{vashishth2018cesi}                     & \\textbf{0.923}             & 0.842                      & 0.620                         & 0.795                        \\\\\n\t\t\t\tJOCL$_{cano}$ \\cite{liu2021joint}                 & 0.918                      & 0.836                      & 0.614                         & 0.789                        \\\\\n\t\t\t\tSIST \\cite{lin2019canonicalization}               & 0.875                      & 0.872                      & 0.845                         & 0.864                        \\\\\n\t\t\t\tCMVC                                              & 0.853                      & \\textbf{0.928}             & \\textbf{0.856}                & \\textbf{0.879}               \\\\ \\hline\n\t\t\t\t\\multicolumn{4}{c|}{\\textit{\\textbf{NYTimes2018}}}                                                                                                  \\\\\n\t\t\t\tPATTY \\cite{nakashole2012patty}                   & 0.775                      & 0.802                      & 0.617                         & 0.731                        \\\\\n\t\t\t\tJOCL$_{cano}$ \\cite{liu2021joint}                 & \\textbf{0.885}                      & 0.885                      & 0.522                         & 0.764       \\\\\n\t\t\t\tSIST \\cite{lin2019canonicalization}               & 0.853             & 0.844                      & 0.722                         & 0.806                        \\\\\n\t\t\t\tCMVC                                              & 0.766                      & \\textbf{0.905}             & \\textbf{0.781}                & \\textbf{0.817}               \\\\ \\hline\n\t\t\t\t\\multicolumn{4}{c|}{\\textit{\\textbf{OPIEC59K}}}                                                                                                    \\\\\n\t\t\t\tAMIE \\cite{galarraga2013amie}                     & 0.595                      & 0.800                      & 0.631                         & 0.675                        \\\\\n\t\t\t\tCESI \\cite{vashishth2018cesi}                     & \\textbf{0.699}             & 0.752                      & 0.628                         & 0.693                        \\\\\n\t\t\t\tJOCL$_{cano}$ \\cite{liu2021joint}                 & 0.622                      & 0.775                      & 0.724                         & 0.707                        \\\\\n\t\t\t\tCMVC                                              & 0.542                      & \\textbf{0.854}             & \\textbf{0.770}                & \\textbf{0.722}               \\\\ \\hline\n\t\t\t\t\n\t\t\t\\end{tabular}%\n\t\t}\n\t\t%\\vspace{-2mm}\n\t\\end{table}",
            "np_ablation": "\\begin{table}[t]\n\t\t\\caption{Performance of different variants in CMVC. }\n\t\t\\vspace{-4.5mm}\n\t\t\\label{np_ablation}\n\t\t\\centering\n\t\t%\\small\n\t\t\\resizebox{0.47\\textwidth}{!}{%\n\t\t%\\resizebox{0.44\\textwidth}{!}{%\n\t\t\t\\begin{tabular}{ccccc|c}\n\t\t\t\\hline\n\t\t\t\\multicolumn{2}{c}{\\textit{\\textbf{Method}}}                                    & \\textit{Macro F1} & \\textit{Micro F1} & \\textit{Pairwise F1} & \\textit{\\textbf{Average F1}} \\\\ \\hline\n\t\t\t\\multicolumn{2}{c}{Seed Pairs}                                                  & 0.308             & 0.900             & 0.872                & 0.693                        \\\\ \\hline\n\t\t\t\\multicolumn{1}{c|}{\\multirow{2}{*}{Fact View}}    & CMVC$_{fact}-$DAO       & 0.331             & 0.842             & 0.752                & 0.641                        \\\\\n\t\t\t\\multicolumn{1}{c|}{}                              & CMVC$_{fact}$              & 0.516             & 0.888             & 0.815                & 0.739                        \\\\ \\hline\n\t\t\t\\multicolumn{1}{c|}{\\multirow{2}{*}{Context View}} & CMVC$_{cnt}-$ICP        & 0.020             & 0.817             & 0.806                & 0.547                        \\\\\n\t\t\t\\multicolumn{1}{c|}{}                              & CMVC$_{cnt}$               & 0.340             & 0.905             & \\textbf{0.879}       & 0.708                        \\\\ \\hline\n\t\t\t\\multicolumn{1}{c|}{\\multirow{2}{*}{Multi-View}}   & CMVC$-$CH                & 0.518             & 0.894             & 0.846                & 0.752                        \\\\\n\t\t\t\\multicolumn{1}{c|}{}                              & CMVC                       & \\textbf{0.521}    & \\textbf{0.909}    & 0.878                & \\textbf{0.769}               \\\\ \\hline\n\t\t\t\\end{tabular}%\n\t\t}\n\t\t%\\vspace{-3mm}\n\t\\end{table}",
            "predict_k_data set": "\\begin{table}[]\n\t\t\\caption{Statistics of the data sets for cluster number prediction. }\n\t\t\\vspace{-4.5mm}\n\t\t\\label{predict_k_data set}\n\t\t\\centering\n\t\t\\resizebox{0.45\\textwidth}{!}{%\n\t\t\t\\begin{tabular}{cccc|cccc}\n\t\t\t\t\\hline\n\t\t\t\t\\textit{\\textbf{Data set}} & \\textit{\\textbf{\\#Emb}} & \\textit{\\textbf{\\#Dim}} & \\textit{\\textbf{\\#Clu}} & \\textit{\\textbf{Data set}} & \\textit{\\textbf{\\#Emb}} & \\textit{\\textbf{\\#Dim}} & \\textit{\\textbf{\\#Clu}} \\\\ \\hline\n\t\t\t\t\\hline\n\t\t\t\tEchocardiogram    & 106       & 9     & 3     & Mice Protein      & 552       & 77      & 8       \\\\\n\t\t\t\tIris              & 150       & 4     & 3     & Abalone           & 4177      & 8       & 28        \\\\\n\t\t\t\tSeeds             & 210       & 7     & 3     & Cnae-9            & 1080      & 856     & 9         \\\\\n\t\t\t\tWine              & 178       & 13    & 3     & Vowel             & 990       & 12      & 11        \\\\\n\t\t\t\tColon Cancer      & 62        & 2000  & 2     & Synthetic Control & 600       & 60      & 6         \\\\\n\t\t\t\tProstate Cancer   & 102       & 6033  & 2     \\\\ \\hline\n\t\t\t\\end{tabular}%\n\t\t}\n\t\t%\\vspace{-3mm}\n\t\\end{table}",
            "predict_k_result": "\\begin{table}[t]\n\t\t\\caption{Performance on the cluster number prediction task. }\n\t\t\\vspace{-4.5mm}\n\t\t\\label{predict_k_result}\n\t\t\\centering\n\t\t%\\small\n\t\t\\resizebox{0.43\\textwidth}{!}{%\n\t\t%\\resizebox{0.38\\textwidth}{!}{%\n\t\t\t\\begin{tabular}{ccc|ccc}\n\t\t\t\t\\hline\n\t\t\t\t\\textit{\\textbf{Method}}     & \\textit{\\textbf{AR}}     & \\textit{\\textbf{ARE}}   & \\textit{\\textbf{Method}}          & \\textit{\\textbf{AR}}     & \\textit{\\textbf{ARE}}   \\\\ \\hline\n\t\t\t\t\\hline\n\t\t\t\tAIC \\cite{mehrjou2016improved}        & 6.154  & 0.456 & MPC \\cite{rajesh1996fuzzy}             & 6.769  & 0.522 \\\\\n\t\t\t\tBIC \\cite{zhao2008knee}        & 15.077 & 0.724 & PC \\cite{james1973cluster}              & 7.385  & 0.533 \\\\\n\t\t\t\tCH Index \\cite{calinski1974dendrite}   & 6.462  & 0.422 & PI \\cite{bensaid1996validity}              & 5.385  & 0.436 \\\\\n\t\t\t\tCE \\cite{bezdek1975classe}         & 15.385 & 0.736 & PBMF \\cite{pakhira2004validity}            & 5.692  & 0.436 \\\\\n\t\t\t\tCWB \\cite{rezaee1998new}        & 14.154 & 0.655 & PCAES \\cite{wu2005cluster}           & 4.692  & 0.391 \\\\\n\t\t\t\tDB Index \\cite{davies1979cluster}   & 5.462  & 0.440 & RLWY Index \\cite{ren2016self}      & 15.385 & 0.736 \\\\\n\t\t\t\tDunn Index \\cite{dunn1973fuzzy} & 10.154 & 0.644 & Rezaee \\cite{rezaee2010cluster}          & 16.000 & 0.753 \\\\\n\t\t\t\tKnee-point \\cite{salvador2004knee} & 2.538  & 0.341 & SIL Index \\cite{rousseeuw1987silhouettes}       & 6.462  & 0.550 \\\\\n\t\t\t\tFS Index \\cite{fukuyama1989ANM}   & 3.846  & 0.389 & Slope Statistic \\cite{fujita201427} & 5.923  & 0.553 \\\\\n\t\t\t\tFHV \\cite{rajesh1996fuzzy}        & 13.923 & 0.710 & XB Index \\cite{xie1991validity}        & 15.923 & 0.750 \\\\\n\t\t\t\tHV Index \\cite{halkidi2001clustering}   & 16.000 & 0.753 & Xu Index \\cite{xu1997bayesian}        & 11.923 & 0.611 \\\\\n\t\t\t\tI Index \\cite{maulik2002performance}    & 5.538  & 0.414 & ZXF Index \\cite{zhao2009sum}       & 5.077  & 0.422 \\\\\n\t\t\t\tLL \\cite{gupta2018fast}         & 7.923  & 0.558 & Jump \\cite{sugar2003findingtn}            & 8.538  & 0.415 \\\\\n\t\t\t\tLML \\cite{gupta2018fast}        & 7.364  & 0.560 & Log-Jump        & \\textbf{1.769}  & \\textbf{0.217} \\\\ \\hline\n\t\t\t\\end{tabular}%\n\t\t}\n\t\t%\\vspace{-3mm}\n\t\\end{table}",
            "predict_k_all_rank_result": "\\begin{table*}[]\n\t\t\\caption{The rank metrics of all methods over all data sets on the cluster number prediction task. }\n\t\t\\vspace{-4.5mm}\n\t\t\\label{predict_k_all_rank_result}\n\t\t\\centering\n\t\t\\resizebox{0.872\\textwidth}{!}{%\n\t\t\t\\begin{tabular}{ccccccccccccccc}\n\t\t\t\t\\hline\n\t\t\t\t\\textit{\\textbf{Method}}       & \\textit{\\textbf{Echocardiogram}} & \\textit{\\textbf{Iris}} & \\textit{\\textbf{Seeds}} & \\textit{\\textbf{Wine}} & \\textit{\\textbf{Colon}} & \\textit{\\textbf{Prostate}} & \\textit{\\textbf{Mice}} & \\textit{\\textbf{Abalone}} & \\textit{\\textbf{Cnae-9}} & \\textit{\\textbf{Vowel}} & \\textit{\\textbf{Synthetic}} & \\textit{\\textbf{OPIEC59K}} & \\textit{\\textbf{ReVerb45K}} & {\\textit{\\textbf{AR}}}                                                  \\\\ \\hline\n\t\t\t\t\\hline\n\t\t\t\tAIC \\cite{mehrjou2016improved}   & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 11                             & 14                        & 11                       & 9                       & 10                                  & 10                         & 9                           & 6.154                                               \\\\\n\t\t\t\tBIC \\cite{zhao2008knee} & 19                               & 20                     & 18                      & 16                     & 1                              & 1                                 & 19                             & 21                        & 18                       & 21                      & 19                                  & 12                         & 11                          & 15.077                                              \\\\\n\t\t\t\tCH Index \\cite{calinski1974dendrite}             & 1                                & 1                      & 1                       & 16                     & 1                              & 1                                 & 11                             & 3                         & 11                       & 9                       & 2                                   & 13                         & 14                          & 6.462                                               \\\\\n\t\t\t\tCE \\cite{bezdek1975classe}         & 19                               & 20                     & 18                      & 16                     & 1                              & 1                                 & 19                             & 21                        & 18                       & 21                      & 19                                  & 13                         & 14                          & 15.385                                              \\\\\n\t\t\t\tCWB \\cite{rezaee1998new}         & 19                               & 20                     & 18                      & 16                     & 1                              & 1                                 & 19                             & 21                        & 18                       & 21                      & 19                                  & 4                          & 7                           & 14.154                                              \\\\\n\t\t\t\tDB Index \\cite{davies1979cluster}           & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 11                             & 14                        & 11                       & 9                       & 10                                  & 6                          & 4                           & 5.462                                               \\\\\n\t\t\t\tDunn Index \\cite{dunn1973fuzzy}                     & 1                                & 1                      & 26                      & 16                     & 1                              & 27                                & 3                              & 5                         & 4                        & 4                       & 10                                  & 21                         & 13                          & 10.154                                              \\\\\n\t\t\t\tKnee-point \\cite{salvador2004knee}              & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 7                              & 5                         & 3                        & 7                       & 2                                   & 2                          & 1                           & 2.538                                               \\\\\n\t\t\t\tFS Index \\cite{fukuyama1989ANM}          & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 7                              & 8                         & 7                        & 7                       & 2                                   & 5                          & 8                           & 3.846                                               \\\\\n\t\t\t\tFHV \\cite{rajesh1996fuzzy}              & 19                               & 1                      & 18                      & 16                     & 1                              & 1                                 & 19                             & 21                        & 18                       & 21                      & 19                                  & 13                         & 14                          & 13.923                                              \\\\\n\t\t\t\tHV Index \\cite{halkidi2001clustering}     & 19                               & 20                     & 18                      & 16                     & 1                              & 1                                 & 19                             & 21                        & 18                       & 21                      & 19                                  & 21                         & 14                          & 16.000                                              \\\\\n\t\t\t\tI Index \\cite{maulik2002performance}                        & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 11                             & 9                         & 8                        & 9                       & 2                                   & 13                         & 14                          & 5.538                                               \\\\\n\t\t\t\tLL \\cite{gupta2018fast}                      & 19                               & 1                      & 1                       & 1                      & 1                              & 1                                 & 19                             & 14                        & 18                       & 9                       & 2                                   & 7                          & 10                          & 7.923                                               \\\\\n\t\t\t\tLML \\cite{gupta2018fast}                & 19                               & 1                      & 1                       & 1                      & 1                              & 1                                 & 19                             & 14                        & 18                       & 4                       & 2                                   & /                          & /                           & 7.364                                               \\\\\n\t\t\t\tMPC \\cite{rajesh1996fuzzy} & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 11                             & 9                         & 8                        & 9                       & 10                                  & 21                         & 14                          & 6.769                                               \\\\\n\t\t\t\tPC \\cite{james1973cluster}          & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 11                             & 14                        & 11                       & 9                       & 10                                  & 21                         & 14                          & 7.385                                               \\\\\n\t\t\t\tPI \\cite{bensaid1996validity}                & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 11                             & 14                        & 11                       & 9                       & 10                                  & 7                          & 2                           & 5.385                                               \\\\\n\t\t\t\tPBMF \\cite{pakhira2004validity}                           & 1                                & 1                      & 1                       & 16                     & 1                              & 1                                 & 6                              & 9                         & 5                        & 4                       & 2                                   & 13                         & 14                          & 5.692                                               \\\\\n\t\t\t\tPCAES \\cite{wu2005cluster}                          & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 1                              & 9                         & 1                        & 9                       & 2                                   & 19                         & 14                          & 4.692                                               \\\\\n\t\t\t\tRLWY Index \\cite{ren2016self}          & 19                               & 20                     & 18                      & 16                     & 1                              & 1                                 & 19                             & 21                        & 18                       & 21                      & 19                                  & 13                         & 14                          & 15.385                                              \\\\\n\t\t\t\tRezaee \\cite{rezaee2010cluster}                         & 19                               & 20                     & 18                      & 16                     & 1                              & 1                                 & 19                             & 21                        & 18                       & 21                      & 19                                  & 21                         & 14                          & 16.000                                              \\\\\n\t\t\t\tSIL Index \\cite{rousseeuw1987silhouettes}               & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 7                              & 9                         & 8                        & 9                       & 10                                  & 21                         & 14                          & 6.462                                               \\\\\n\t\t\t\tSlope Statistic \\cite{fujita201427}                & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 7                              & 5                         & 18                       & 9                       & 10                                  & 11                         & 11                          & 5.923                                               \\\\\n\t\t\t\tXB Index \\cite{xie1991validity}                 & 19                               & 20                     & 18                      & 16                     & 1                              & 1                                 & 19                             & 21                        & 18                       & 21                      & 19                                  & 20                         & 14                          & 15.923                                              \\\\\n\t\t\t\tXu Index \\cite{xu1997bayesian}                       & 1                                & 20                     & 26                      & 16                     & 1                              & 27                                & 3                              & 1                         & 5                        & 1                       & 19                                  & 21                         & 14                          & 11.923                                              \\\\\n\t\t\t\tZXF Index \\cite{zhao2009sum}                      & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 11                             & 14                        & 11                       & 9                       & 10                                  & 3                          & 2                           & 5.077                                               \\\\\n\t\t\t\tJump \\cite{sugar2003findingtn}                    & 1                                & 20                     & 26                      & 16                     & 1                              & 1                                 & 3                              & 1                         & 11                       & 1                       & 19                                  & 7                          & 4                           & 8.538                                               \\\\ \\hline\n\t\t\t\tLog-Jump            & 1                                & 1                      & 1                       & 1                      & 1                              & 1                                 & 1                              & 4                         & 1                        & 3                       & 1                                   & 1                          & 6                           & {\\textbf{1.769}}                                    \\\\ \\hline\n\t\t\t\\end{tabular}%\n\t\t}\\vspace{-3mm}\n\t\\end{table*}",
            "predict_k_all_relative_error_result": "\\begin{table*}[]\n\t\t\\caption{The relative error metrics of all methods over all data sets on the cluster number prediction task. }\n\t\t\\vspace{-4.5mm}\n\t\t\\label{predict_k_all_relative_error_result}\n\t\t\\centering\n\t\t\\resizebox{0.872\\textwidth}{!}{%\n\t\t\t\\begin{tabular}{ccccccccccccccc}\n\t\t\t\t\\hline\n\t\t\t\t\\textit{\\textbf{Method}}       & \\textit{\\textbf{Echocardiogram}} & \\textit{\\textbf{Iris}} & \\textit{\\textbf{Seeds}} & \\textit{\\textbf{Wine}} & \\textit{\\textbf{Colon}} & \\textit{\\textbf{Prostate}} & \\textit{\\textbf{Mice}} & \\textit{\\textbf{Abalone}} & \\textit{\\textbf{Cnae-9}} & \\textit{\\textbf{Vowel}} & \\textit{\\textbf{Synthetic}} & \\textit{\\textbf{OPIEC59K}} & \\textit{\\textbf{ReVerb45K}} & \\textit{\\textbf{ARE}} \\\\ \\hline\n\t\t\t\t\\hline\n\t\t\t\tAIC \\cite{mehrjou2016improved}   & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.000                      & 0.750                  & 0.929                     & 0.778                    & 0.818                   & 0.667                       & 0.490                      & 0.170                       & 0.456                           \\\\\n\t\t\t\tBIC \\cite{zhao2008knee} & 0.667                            & 0.667                  & 0.667                   & 0.667                  & 0.500                   & 0.500                      & 0.875                  & 0.964                     & 0.889                    & 0.909                   & 0.833                       & 0.776                      & 0.506                       & 0.724                           \\\\\n\t\t\t\tCH Index \\cite{calinski1974dendrite}             & 0.333                            & 0.000                  & 0.000                   & 0.667                  & 0.000                   & 0.000                      & 0.750                  & 0.214                     & 0.778                    & 0.818                   & 0.500                       & 0.796                      & 0.632                       & 0.422                           \\\\\n\t\t\t\tCE \\cite{bezdek1975classe}         & 0.667                            & 0.667                  & 0.667                   & 0.667                  & 0.500                   & 0.500                      & 0.875                  & 0.964                     & 0.889                    & 0.909                   & 0.833                       & 0.796                      & 0.632                       & 0.736                           \\\\\n\t\t\t\tCWB \\cite{rezaee1998new}         & 0.667                            & 0.667                  & 0.667                   & 0.667                  & 0.500                   & 0.500                      & 0.875                  & 0.964                     & 0.889                    & 0.909                   & 0.833                       & 0.265                      & 0.110                       & 0.655                           \\\\\n\t\t\t\tDB Index \\cite{davies1979cluster}           & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.000                      & 0.750                  & 0.929                     & 0.778                    & 0.818                   & 0.667                       & 0.347                      & 0.094                       & 0.440                           \\\\\n\t\t\t\tDunn Index \\cite{dunn1973fuzzy}                     & 0.333                            & 0.333                  & 1.000                   & 0.667                  & 0.500                   & 1.000                      & 0.250                  & 0.821                     & 0.556                    & 0.636                   & 0.667                       & 1.020                      & 0.582                       & 0.644                           \\\\\n\t\t\t\tKnee-point \\cite{salvador2004knee}              & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.000                      & 0.625                  & 0.821                     & 0.333                    & 0.727                   & 0.500                       & 0.082                      & 0.005                       & 0.341                           \\\\\n\t\t\t\tFS Index \\cite{fukuyama1989ANM}          & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.000                      & 0.625                  & 0.857                     & 0.556                    & 0.727                   & 0.500                       & 0.306                      & 0.154                       & 0.389                           \\\\\n\t\t\t\tFHV \\cite{rajesh1996fuzzy}              & 0.667                            & 0.333                  & 0.667                   & 0.667                  & 0.500                   & 0.500                      & 0.875                  & 0.964                     & 0.889                    & 0.909                   & 0.833                       & 0.796                      & 0.632                       & 0.710                           \\\\\n\t\t\t\tHV Index \\cite{halkidi2001clustering}     & 0.667                            & 0.667                  & 0.667                   & 0.667                  & 0.500                   & 0.500                      & 0.875                  & 0.964                     & 0.889                    & 0.909                   & 0.833                       & 1.020                      & 0.632                       & 0.753                           \\\\\n\t\t\t\tI Index \\cite{maulik2002performance}                        & 0.000                            & 0.000                  & 0.000                   & 0.333                  & 0.000                   & 0.000                      & 0.750                  & 0.893                     & 0.667                    & 0.818                   & 0.500                       & 0.796                      & 0.632                       & 0.414                           \\\\\n\t\t\t\tLL \\cite{gupta2018fast}                      & 0.667                            & 0.333                  & 0.333                   & 0.333                  & 0.500                   & 0.500                      & 0.875                  & 0.929                     & 0.889                    & 0.818                   & 0.500                       & 0.367                      & 0.209                       & 0.558                           \\\\\n\t\t\t\tLML \\cite{gupta2018fast}                & 0.667                            & 0.000                  & 0.333                   & 0.333                  & 0.500                   & 0.500                      & 0.875                  & 0.929                     & 0.889                    & 0.636                   & 0.500                       & /                          & /                           & 0.560                           \\\\\n\t\t\t\tMPC \\cite{rajesh1996fuzzy} & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.000                      & 0.750                  & 0.893                     & 0.667                    & 0.818                   & 0.667                       & 1.020                      & 0.632                       & 0.522                           \\\\\n\t\t\t\tPC \\cite{james1973cluster}          & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.000                      & 0.750                  & 0.929                     & 0.778                    & 0.818                   & 0.667                       & 1.020                      & 0.632                       & 0.533                           \\\\\n\t\t\t\tPI \\cite{bensaid1996validity}                & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.000                      & 0.750                  & 0.929                     & 0.778                    & 0.818                   & 0.667                       & 0.367                      & 0.028                       & 0.436                           \\\\\n\t\t\t\tPBMF \\cite{pakhira2004validity}                           & 0.000                            & 0.000                  & 0.000                   & 0.667                  & 0.000                   & 0.500                      & 0.375                  & 0.893                     & 0.667                    & 0.636                   & 0.500                       & 0.796                      & 0.632                       & 0.436                           \\\\\n\t\t\t\tPCAES \\cite{wu2005cluster}                          & 0.333                            & 0.333                  & 0.000                   & 0.000                  & 0.500                   & 0.000                      & 0.000                  & 0.893                     & 0.111                    & 0.818                   & 0.500                       & 0.959                      & 0.632                       & 0.391                           \\\\\n\t\t\t\tRLWY Index \\cite{ren2016self}          & 0.667                            & 0.667                  & 0.667                   & 0.667                  & 0.500                   & 0.500                      & 0.875                  & 0.964                     & 0.889                    & 0.909                   & 0.833                       & 0.796                      & 0.632                       & 0.736                           \\\\\n\t\t\t\tRezaee \\cite{rezaee2010cluster}                         & 0.667                            & 0.667                  & 0.667                   & 0.667                  & 0.500                   & 0.500                      & 0.875                  & 0.964                     & 0.889                    & 0.909                   & 0.833                       & 1.020                      & 0.632                       & 0.753                           \\\\\n\t\t\t\tSIL Index \\cite{rousseeuw1987silhouettes}               & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.500                   & 0.000                      & 0.625                  & 0.893                     & 0.667                    & 0.818                   & 0.667                       & 1.020                      & 0.632                       & 0.550                           \\\\\n\t\t\t\tSlope Statistic \\cite{fujita201427}                & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.500                   & 0.500                      & 0.625                  & 0.821                     & 0.889                    & 0.818                   & 0.667                       & 0.531                      & 0.506                       & 0.553                           \\\\\n\t\t\t\tXB Index \\cite{xie1991validity}                 & 0.667                            & 0.667                  & 0.667                   & 0.667                  & 0.500                   & 0.500                      & 0.875                  & 0.964                     & 0.889                    & 0.909                   & 0.833                       & 0.980                      & 0.632                       & 0.750                           \\\\\n\t\t\t\tXu Index \\cite{xu1997bayesian}                       & 0.333                            & 0.667                  & 1.000                   & 0.667                  & 0.500                   & 1.000                      & 0.250                  & 0.107                     & 0.667                    & 0.273                   & 0.833                       & 1.020                      & 0.632                       & 0.611                           \\\\\n\t\t\t\tZXF Index \\cite{zhao2009sum}                      & 0.333                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.000                      & 0.750                  & 0.929                     & 0.778                    & 0.818                   & 0.667                       & 0.184                      & 0.028                       & 0.422                           \\\\\n\t\t\t\tJump \\cite{sugar2003findingtn}                    & 0.333                            & 0.667                  & 1.000                   & 0.667                  & 0.000                   & 0.000                      & 0.250                  & 0.107                     & 0.778                    & 0.273                   & 0.833                       & 0.388                      & 0.094                       & 0.415                           \\\\ \\hline\n\t\t\t\tLog-Jump            & 0.000                            & 0.333                  & 0.333                   & 0.333                  & 0.000                   & 0.500                      & 0.125                  & 0.464                     & 0.000                    & 0.455                   & 0.167                       & 0.000                      & 0.104                       & \\textbf{0.217}                  \\\\ \\hline\n\t\t\t\\end{tabular}%\n\t\t}\\vspace{-3mm}\n\t\\end{table*}"
        },
        "figures": {
            "figure_1": "\\begin{figure}[t] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形\n\t\t\\centering %图片居中\n\t\t\\includegraphics[width=0.49\\textwidth]{fig-1-1.eps} %插入图片，[]中设置图片大小，{}中是图片文件名\n\t\t\\vspace{-5.5mm}\n\t\t\\caption{An illustration of the proposed CMVC framework. }\n\t\t%\\vspace{-2.5mm}\n\t\t\\vspace{-0.5mm}\n\t\t\\label{figure_1} %用于文内引用的标签\n\t%\\end{figure}\n\t\\end{figure}",
            "figure_2": "\\begin{figure*}[t] %需要subfigure宏包\n\t\t\\subfigcapskip=-10pt\n\t\t\\vspace{-8mm}\n\t\t\\begin{minipage}[t]{\\textwidth}\n\t\t\t\\begin{minipage}[t]{\\textwidth}\n\t\t\t\t\\begin{figure}[H]\n\t\t\t\t\t\\centering\n\t\t\t\t\t\\subfigure[Fact view.]{ \n\t\t\t\t\t\t\\begin{minipage}[h]{.3\\textwidth}\n\t\t\t\t\t\t\t\\centering\n\t\t\t\t\t\t\t\\includegraphics[width=1\\textwidth]{fig-2-1.eps}\n\t\t\t\t\t\t\t\\label{fig-2-1}\n\t\t\t\t\t\t\\end{minipage}\n\t\t\t\t\t}\n\t\t\t\t\t\\subfigure[Context view.]{    \n\t\t\t\t\t\t\\begin{minipage}[h]{.3\\textwidth}\n\t\t\t\t\t\t\t\\centering\n\t\t\t\t\t\t\t\\includegraphics[width=1\\textwidth]{fig-2-2.eps}\n\t\t\t\t\t\t\t\\label{fig-2-2}\n\t\t\t\t\t\t\\end{minipage}\n\t\t\t\t\t}\n\t\t\t\t\t\\subfigure[CMVC.]{    \n\t\t\t\t\t\t\\begin{minipage}[h]{.3\\textwidth}\n\t\t\t\t\t\t\t\\centering\n\t\t\t\t\t\t\t\\includegraphics[width=1\\textwidth]{fig-2-3.eps}\n\t\t\t\t\t\t\t\\label{fig-2-3}\n\t\t\t\t\t\t\\end{minipage}\n\t\t\t\t\t}\n\t\t\t\t\t\\vspace{-6mm}\n\t\t\t\t\t\\caption{Convergence study. }\n\t\t\t\t\t\\vspace{-6mm}\n\t\t\t\t\\end{figure}\n\t\t\t\\end{minipage}\n\t\t\\end{minipage}\n\t\t\\label{figure_2}\n\t\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\t\\vspace{-2mm}\n\t\\pmb{\\xi}_{j}^{(v)} = \\frac { \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(\\bar{v})}} \\bm{sub}_{i}^{(v)} }{ \\| \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(\\bar{v})}} \n\t\\bm{sub}_{i}^{(v)}\\| } \\label{mvc_m}, \n\t\\vspace{-2mm}\n\t\\end{equation}",
            "eq:2": "\\begin{equation}\n\t\\begin{aligned}\n\t\\pi_{j}^{(v)} = \\{ sub_{i} \\in S : sim( \\bm{sub}_{i}^{(v)}, \\pmb{\\xi}_{j}^{(v)} ) \\geq sim( \\bm{sub}_{i}^{(v)}, \\pmb{\\xi}_{l}^{(v)} ) \\}, \\label{mvc_e}\n\t\\end{aligned}\n\t\\end{equation}",
            "eq:3": "\\begin{equation}\n\t\\mathcal{L}_{mvc} = \\sum_{v = 1} ^{2} \\sum_{j = 1} ^{K} \\sum_{sub_{i} \\in \\pi_{j}^{(v)}} 1 - sim( \\bm{sub}_{i}^{(v)}, \\pmb{\\xi}_{j}^{(v)} ), \\label{mvc_loss} \n\t\\vspace{-1.5mm}\n\t\\end{equation}",
            "eq:4": "\\begin{equation}\n\t\\vspace{-1mm}\n\t\\bm{m}_{j}^{(v)} = \\frac { \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(1)} \\land sub_{i} \\in \\pi_{j}^{(2)}} \\bm{sub}_{i}^{(v)} }\n\t{ \\| \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(1)} \\land sub_{i} \\in \\pi_{j}^{(2)}} \\bm{sub}_{i}^{(v)}\\| }, \\label{mvc_mean}\n\t\\vspace{-1mm}\n\t\\end{equation}",
            "eq:5": "\\begin{equation}\t\n\t\\vspace{-1mm}\n\t{CH}^{(v)} = \\frac { \\sum_{j=1}^{K} \\left| \\pi_{j}^{(v)} \\right| \\cdot \\| \\pmb{\\xi}_{j}^{(v)} - \\pmb{\\xi}^{(v)} \\|^2 }\n\t{ \\sum_{j=1}^{K} \\sum\\limits_{sub_{i} \\in \\pi_{j}^{(v)}} \\| \\bm{sub}_{i}^{(v)} - \\pmb{\\xi}_{j}^{(v)}\\|^2 } \\times \\frac { \\left| S \\right| -K }{ K-1 }  \\label{ch}, \n\t\\vspace{-1mm}\n\t\\end{equation}",
            "eq:6": "\\begin{equation}\n\t\\vspace{-1mm}\n\t\\begin{aligned}\n\t\\hat{\\pi_{j}} = \\{ sub_{i} \\in S : \\sum_{v = 1} ^{2} {CH}^{(v)} \\cdot sim ( \\bm{sub}_{i}^{(v)}, \\bm{m}_{j}^{(v)} ) \\\\\n\t\\geq \\sum_{v = 1} ^{2} {CH}^{(v)} \\cdot sim( \\bm{sub}_{i}^{(v)}, \\bm{m}_{l}^{(v)} ) \\}, \\label{mvc_pi}\n\t\\end{aligned}\n\t\\vspace{-1mm}\n\t\\end{equation}",
            "eq:7": "\\begin{equation}\n\t\\vspace{-1mm}\n\td_{K} = \\frac{1}{n \\cdot p} \\sum_{ \\pmb{x}_{i} \\in X } \\min\\limits_{j \\in \\{ 1,..,K \\} } (1 - sim( \\pmb{x}_{i}, \\pmb{\\xi}_{j} )), \\label{inverse-jump-dk}\n\t\\vspace{-2mm}\n\t\\end{equation}",
            "eq:8": "\\begin{equation}\n\t\\vspace{-1mm}\n\t\\mathcal{L}_{fact} = \\sum_{t_i \\in T^+} \\sum_{t'_i \\in T^-} \\left[ \\gamma + f(t_i) - f(t'_i) \\right]_+, \\label{transe_loss}\n\t\\vspace{-1mm}\n\t\\end{equation}",
            "eq:9": "\\begin{equation}\n\t\\vspace{-0.5mm}\n\t\\begin{aligned}\n\tT^{-} = \\{ <sub'_i, rel_i, obj_i> | sub'_i \\in N \\} \\cup \\{ <sub_i, \\\\\n\trel_i, obj'_i> | obj'_i \\in N \\}, \n\t<sub_i, rel_i, obj_i> \\in T^{+}, \\label{negative_sample}\n\t\\end{aligned}\n\t\\vspace{-0.5mm}\n\t\\end{equation}",
            "eq:10": "\\begin{equation}\n\t\\vspace{-1mm}\n\t\\begin{aligned}\n\tT^{A} = \\{ <u_j, rel_i, obj_i> | sub_i = u_i \\} \\cup \\{ <sub_i, rel_i, u_j> | obj_i = u_i \\} \\\\\n\t\\cup \\{ <u_i, rel_i, obj_i> | sub_i = u_j \\} \\cup \\{ <sub_i, rel_i, u_i> | obj_i = u_j \\}. \\label{DAO}\n\t\\end{aligned}\n\t\\vspace{-1mm}\n\t\\end{equation}",
            "eq:11": "\\begin{equation}\n\t\\lim_{p \\to \\infty} d_{K} = c \\quad (\\frac{p \\Delta^2}{9G}W < c \\le 1) \\label{Theorem 1}\n\t\\end{equation}",
            "eq:12": "\\begin{equation}\n\td_{K} \\ge \\frac{p \\Delta^2}{9G}W, \n\t\\end{equation}",
            "eq:13": "\\begin{equation}\n\td_{K} = \\begin{cases}\n\t{( \\frac{G}{aK} )} ^{2/p}, & \\text {$K > G$} \\quad (0 < a < 1) \\\\\n\tc, &\\text{$K \\le G$} \\quad (\\frac{p \\Delta^2}{9G}W \\le c \\le 1) \n\t\\end{cases}  \\label{d_k}\n\t\\end{equation}",
            "eq:14": "\\begin{equation}\n\tLJ_{K} = \\log d_{K+1} - \\log d_{K} = \\begin{cases}\n\tc - c = 0, & \\text {K < G} \\\\\n\t\\log \\frac{1}{a} > 0, &\\text{K = G}  \\\\\n\t\\frac{2}{p} \\log \\frac{K}{K+1} < 0, &\\text{K > G}\n\t\\end{cases}  \\label{log_d_k}\n\t\\end{equation}",
            "eq:15": "\\begin{equation}\n\t%G = \\arg\\max\\limits_{K} {\\left[ \\log d_{K+1} - \\log d_{K}  \\right]}\n\tG = \\arg\\max\\limits_{K} { LJ_{K} }\n\t\\end{equation}",
            "eq:16": "\\begin{equation}\n\tgap = 10^{len(string(n)) - 2}, \\label{inverse-jump-max} \n\t\\end{equation}"
        },
        "git_link": "https://github.com/Yang233666/cmvc"
    }
}