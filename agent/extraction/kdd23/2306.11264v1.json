{
    "meta_info": {
        "title": "GraphGLOW: Universal and Generalizable Structure Learning for Graph  Neural Networks",
        "abstract": "Graph structure learning is a well-established problem that aims at\noptimizing graph structures adaptive to specific graph datasets to help message\npassing neural networks (i.e., GNNs) to yield effective and robust node\nembeddings. However, the common limitation of existing models lies in the\nunderlying \\textit{closed-world assumption}: the testing graph is the same as\nthe training graph. This premise requires independently training the structure\nlearning model from scratch for each graph dataset, which leads to prohibitive\ncomputation costs and potential risks for serious over-fitting. To mitigate\nthese issues, this paper explores a new direction that moves forward to learn a\nuniversal structure learning model that can generalize across graph datasets in\nan open world. We first introduce the mathematical definition of this novel\nproblem setting, and describe the model formulation from a probabilistic\ndata-generative aspect. Then we devise a general framework that coordinates a\nsingle graph-shared structure learner and multiple graph-specific GNNs to\ncapture the generalizable patterns of optimal message-passing topology across\ndatasets. The well-trained structure learner can directly produce adaptive\nstructures for unseen target graphs without any fine-tuning. Across diverse\ndatasets and various challenging cross-graph generalization protocols, our\nexperiments show that even without training on target graphs, the proposed\nmodel i) significantly outperforms expressive GNNs trained on input\n(non-optimized) topology, and ii) surprisingly performs on par with\nstate-of-the-art models that independently optimize adaptive structures for\nspecific target graphs, with notably orders-of-magnitude acceleration for\ntraining on the target graph.",
        "author": "Wentao Zhao, Qitian Wu, Chenxiao Yang, Junchi Yan",
        "link": "http://arxiv.org/abs/2306.11264v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "Published as a conference paper at KDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\\label{sec-intro}\n\tGraph neural networks (GNNs)~\\cite{gcn, gat, graphsage}, as a de facto model class based on the message passing principle, have shown promising efficacy for learning node representations for graph-structured data, with extensive applications to, e.g., physics simulation~\\cite{sanchez2020learning}, traffic prediction~\\cite{jiang2022graph}, drug recommendation~\\cite{yang2023molerec}. However, due to the inevitable error-prone data collection~\\cite{idgl}, the input graph may contain spurious and unobserved edges that lead to sub-optimal results of GNNs and degrade the downstream performance.\n \n Graph structure learning~\\cite{structure_learning_survey} serves as a plausible remedy for such an issue via optimizing graph structures and GNN classifiers at the same time. To this end, recent endeavors explore different technical aspects, e.g., parameterizing each potential edge between any pair of nodes~\\cite{lds,variational_inference_2} or estimating potential links through a parameterized network~\\cite{idgl,wunodeformer,rbf},  etc. \nHowever, existing models limit their applicability within a closed-world hypothesis: the training and testing of structure learning models, which optimize the graph structures, are performed on the same graph. The issue, however, is that since structure learning is often heavy-weighted and requires sophisticated optimization, it can be prohibitively resource-consuming to train structure learning models from scratch for each graph dataset. Moreover, due to limited labels in common graph-based predictive tasks, structure learning models are prone to over-fitting given that they cannot utilize the common knowledge shared across different graph datasets.\n\nTo resolve the above dilemma, this paper attempts to explore a novel problem setting termed \\textit{Open-World Graph Structure Learning}. Specifically, we target learning a generalizable graph structure learning model which is trained with multiple source graphs and can be directly adapted for inference (without re-training or fine-tuning) on new unseen target graphs. We formulate the problem as a bi-level optimization target that jointly learns a single dataset-shared structure learner and multiple dataset-specific GNNs tailored for particular graph datasets, as shown in Fig. \\ref{fig:glow}. Under such a framework, the well-trained structure learner can leverage the common transferrable knowledge across datasets for enhancing generalization and more critically, be readily utilized to yield adaptive message-passing topology for arbitrarily given target graphs.\n\nWith the guidance of the aforementioned general goal, we propose \\mymodel (short for A Graph Structure Learning Model for Open-World Generalization) that aims at learning the generalizable patterns of optimal message-passing topology across source graphs. Specifically, we first take a bottom-up perspective and formulate the generative process for observed data in a probabilistic manner. On top of this, we derive a tractable and feasible learning objective through the lens of variational inference. The structure learner is specified as a multi-head weighted similarity function so as to guarantee enough expressivity for accommodating diverse structural information, and we further harness an approximation scheme to reduce the quadratic complexity overhead of learning potential edges from arbitrary node pairs. \n\n\n\nTo reasonably and comprehensively evaluate the model, we devise experiments with a diverse set of protocols that can measure the generalization ability under different difficulty levels (according to the intensity of distribution shifts between source graphs and target graphs).\nConcretely, we consider: 1) In-domain generalization, in which we generalize from some citation (social) networks to other citation (social) networks. 2) Cross-domain networks generalization between citation and social networks.\n% three cross-graph generalization tasks based on seven real-world datasets: 1) generalization among homophilous graphs from different distributions, 2) generalization among non-homophilous graphs from different distributions, and 3) generalization from homophilous (resp. non-homophilous) graphs to non-homophilous (resp. homophilous) graphs. \nThe results, which are consistent across various combinations of source and target graph datasets, demonstrate that when evaluated on the target graphs, our approach i) consistently outperforms directly training the GNN counterpart on original non-optimized graph structures of the target datasets and ii) performs on par with state-of-the-art structure learning methods~\\cite{lds,idgl,variational_inference_2} trained on target graphs from scratch with up to $25\\times$ less training time consumed. \nOur code is available at \\href{https://github.com/WtaoZhao/GraphGLOW}{https://github.com/WtaoZhao/GraphGLOW}.\n% \\textbf{The contributions of this work are:}\n\n% \\textbf{1) Aspects:} We propose a new problem setting that aims at learning a universal graph structure learning model that can generalize across graph datasets. The new problem relaxes the closed-world assumption of prior art and enables the structure learning model to i) capture common knowledge across graphs to overcome the limited supervision in each dataset and ii) significantly reduce the computation costs required to train particular structure learning models for any specific graphs. Despite the recognized practical significance, this problem has remained largely under-explored and our work serves as an initiative attempt that could inspire future works.\n\n% \\textbf{2) Methodology:} We devise an end-to-end learning framework based on the data-generating process from a Bayesian perspective. We harness the scalable inference technique to derive a tractable objective that integrates the training for a shared structure learner and multiple dataset-specific GNNs. Our model is expressive for learning diverse structures, efficient with nearly linear complexity w.r.t. node numbers, and flexible for handling new unseen graphs without re-training. As far as we know, our work is the first for generalizable structure learning under the open-world assumption. \n\n% \\textbf{3) Evaluation:} We design a comprehensive set of experiments for reasonable evaluation under our new setting. The results show that even compared with training SOTA structure learning models from scratch on target graphs, our model still achieves competitive results without any training on the target graphs with orders-of-magnitude time cost saved. In particular, our structure learner shows promising efficacy for not only transferring across graphs with different properties (e.g., feature/label space, graph sizes) but also extrapolate to graphs under distribution shifts (e.g., disparate domains or homophily ratios). We believe these results serve as evidence that sheds lights on a new promising direction. \n% Our code is provided in https://anonymous.4open.science/r/GraphGLOW-A84F.\n\n\n\n\n\n\t\n\n"
            },
            "section 2": {
                "name": "Preliminary and Problem Definition",
                "content": "\\label{sec-problem}\n\n% We first review the technical background as building blocks and describe the problem formulation of closed-world graph structure learning studied by prior art. On top of them, we present the formal definition for our introduced problem under open-world assumptions.\n\n\\textbf{Node-Level Predictive Tasks.} Denote a graph with $N$ nodes as $\\mathcal G = (\\mathbf A, \\mathbf X, \\mathbf Y)$ where $\\mathbf A = \\{a_{uv}\\}_{N\\times N}$ is an adjacency matrix ($a_{uv}=1$ means the edge between node $u$ and $v$ exists and 0 otherwise), $\\mathbf X = \\{\\mathbf x_u\\}_{N\\times D}$ is a feature matrix with $\\mathbf x_u$ a $D$-dimensional node feature vector of node $u$, and $\\mathbf Y = \\{y_u\\}_{N\\times C}$ with $y_u$ the label vector of node $u$ and $C$ class number. The node labels are partially observed as training data, based on which the node-level prediction aims to predict the unobserved labels for testing nodes in the graph using node features and graph structures. The latter is often achieved via a GNN model, denoted as $h_w$, that yields predicted node labels $\\hat{\\mathbf Y} = h_w(\\mathbf A, \\mathbf X)$ and is optimized with the classification loss $w^* = \\argmin_{w} = \\mathcal L(\\hat{\\mathbf Y}, \\mathbf Y)$ using observed labels from training nodes.\n\n\\textbf{Closed-World Graph Structure Learning (GLCW).}\n The standard graph structure learning for node-level predictive tasks trains a graph structure learner $ g_\\theta $ to refine the given structure, i.e., $ \\hat{\\mathbf{A}}=g_\\theta (\\mathbf{A}, \\mathbf{X}) $, over which the GNN classifier $ h_w $ conducts message passing for producing node representations and predictions. The $g_\\theta$ is expected to produce optimal graph structures that can give rise to satisfactory downstream classification performance of the GNN classifier. Formally speaking, the goal for training $ g_\\theta $ along with $ h_w $ can be expressed as a nested optimization problem:\n\\begin{equation}\\label{eqn-standard}\n\t\\theta^* = \\argmin_{w}  \\min_{\\theta}   \\mathcal L \\left (h_{w}(g_\\theta(\\mathbf A, \\mathbf X), \\mathbf X),\\mathbf Y \\right ).\n\\end{equation}\nThe above formulation of graph structure learning under closed-world assumptions constrains the training and testing nodes in the same graph, which requires $g_\\theta$ to be trained from scratch on each graph dataset. Since $g_\\theta$ is often much more complicated (e.g., with orders-of-magnitude more trainable parameters) and difficult for optimization (due to the bi-level optimization \\eqref{eqn-standard}) than the GNN $h_w$, the GLCW would lead to undesired inefficiency and vulnerability for serious over-fitting (due to limited labeled information). \n\n\\textbf{Open-World Graph Structure Learning (GLOW).}\nIn this work, we turn to a new learning paradigm that generalizes graph structure learning to open-world assumptions, borrowing the concepts of domain generalization~\\cite{dg-1} and out-of-distribution generalization~\\cite{wu2022handling}, more broadly. Specifically, assume that we are given multiple source graphs, denoted as $\\{\\mathcal G^s_m\\}_{m=1}^M = \\{(\\mathbf A^s_m, \\mathbf X^s_m, \\mathbf Y^s_m)\\}_{m=1}^M$, and a target graph $\\mathcal G^t = (\\mathbf A^t, \\mathbf X^t, \\mathbf Y^t)$, whose distribution is often different from any source graph. The goal is to train a universal structure learner $ g_\\theta $ on source graphs which can be directly used for inference on the target graph without any re-training or fine-tuning. The trained structure learner is expected to produce desired graph structures that can bring up better downstream classification of a GNN classifier optimized for the target graph.\n\nMore specifically, we consider a one-to-many framework that coordinates a shared graph structure learner $ g_\\theta $ and multiple dataset-specific GNNs $ \\{h_{w_m} \\}_{m=1}^M$, where $h_{w_m}$ with independent parameterization $w_m$ is optimized for a given source graph $ \\mathcal{G}_m^s $. With the aim of learning a universal $ g_\\theta $ that can generalize to new unseen target graphs, our training goal can be formulated as the following bi-level optimization problem:\n\\begin{equation}\\label{eqn-obj}\n\t\\theta^* = \\argmin_{\\theta}  \\min_{w_1, \\cdots, w_M} \\sum_{m=1}^M  \\mathcal L \\left (h_{w_m}(g_\\theta(\\mathbf A^s_m, \\mathbf X^s_m), \\mathbf X^s_m),\\mathbf Y^s_m \\right ),\n\\end{equation}\nwhere the inner optimization is a multi-task learning objective. Generally, \\eqref{eqn-obj} aims at finding an optimal $g_\\theta$ that can jointly minimize the classification loss induced by $M$ GNN models, each trained for a particular source graph.\nAfter training, we can directly adapt $g_{\\theta^*}$ to the target graph for testing purpose, and only need to train a GNN $h_w$ on the target graph:\n\\begin{equation}\n\tw^* = \\argmin_{w} \\mathcal L \\left (h_{w}(g_{\\theta^*}(\\mathbf A^t, \\mathbf X^t), \\mathbf X^t),\\mathbf Y^t \\right ).\n\\end{equation}\n\n"
            },
            "section 3": {
                "name": "Proposed Model",
                "content": "\n\nTo handle the above problem, we present an end-to-end learning framework \\mymodel that guides the central graph structure learner to learn adaptive message-passing structures exploited by multiple GNNs. The overview of \\mymodel is shown in Fig.~\\ref{fig:precis}.\n\nThe fundamental challenge of GLOW lies in how to model and capture the generalizable patterns among adaptive structures of different graphs. To this end, we first take a data-generative perspective that treats the inputs and inter-mediate results as random variables and investigate into their dependency, based on which we present the high-level model formulation in a probabilistic form (Sec.~\\ref{sec-model-for}). Then we proceed to instantiate the model components (Sec.~\\ref{sec-model-ins}). Finally, we discuss differentiable training approaches for optimization (Sec.~\\ref{sec-model-training}).\n\n",
                "subsection 3.1": {
                    "name": "Model Formulation",
                    "content": "\\label{sec-model-for}\n\nTo commence, we characterize the data generation process by a latent variable model, based on which we derive the formulation of our method. We treat the latent graph $\\hat{\\mathbf A}$ (given by $g_\\theta$) as a latent variable whose prior distribution is given by $p(\\hat{\\mathbf A} | \\mathbf A, \\mathbf X)$. The prior distribution reflects how one presumed on the latent structures before observed labels arrive. Then, the prediction is given by a predictive distribution $p(\\mathbf Y| \\hat{\\mathbf A}, \\mathbf A, \\mathbf X)$. \nThe learning objective aims at maximizing the log-likelihood of observed labels, which can be written as:\n\t$ \\log p(\\mathbf{Y} | \\mathbf{A} , \\mathbf{X}  )\n\t=\\log \\int_{\\hat{\\mathbf{A} }} p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   ) p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) d \\hat{\\mathbf{A} } $.\nTo estimate latent graphs that could enhance message passing for downstream tasks, one plausible way is to sample from the posterior, i.e., $ p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X}) $,  conditioned on the labels from downstream tasks. Using the Bayes' rule, we have\n\\begin{equation}\n    p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X}) = \\frac{ p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   ) p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   )}{\\int_{\\hat{\\mathbf{A} }} p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   ) p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) d \\hat{\\mathbf{A} }}.\n\\end{equation}\nHowever, the integration over $\\hat{\\mathbf A}$ in the denominator is intractable  for computation due to the exponentially large space of $\\hat{\\mathbf{A}}$. \n\nTo circumvent the difficulty, we can introduce a variational distribution $ q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) $ over $ \\hat{\\mathbf{A} } $ as an approximation to $p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X})$. We can sample latent graphs from $ q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) $, i.e., instantiate it as the structure learner $g_\\theta$, and once $q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) = p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X})$, we could have samples from the posterior that ideally generates the optimal graph structures for downstream prediction. By this principle, we can start with minimizing the Kullback-Leibler divergence between $q$ and $p$ and derive the learning objective as follows:\n\\begin{equation}\n\t\\begin{split}\n\t\t& \\mathcal D_{KL}(q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) \\| p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X})) \\\\\n\t\t= & - \\underbrace{\\mathbb{E}_{\\hat{\\mathbf{A} } \\sim q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   )} \\left [\\log  \\frac{ p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   )  p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) }{ q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) } \\right ] }_{\\mbox{Evidence Lower Bound}} + \\log p(\\mathbf Y|\\mathbf A, \\mathbf X).\n\t\\end{split}\n\\end{equation}\nBased on this equation, we further have the inequality which bridges the relationship between the Evidence Lower Bound (ELBO) and observed data log-likelihood:\n\\begin{equation}\n    \\log p(\\mathbf Y|\\mathbf A, \\mathbf X) \\\\\n    \\geq  \\mathbb{E}_{\\hat{\\mathbf{A} } \\sim q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   )} \\left [\\log  \\frac{ p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   )  p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) }{ q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) } \\right ].\n\\end{equation}\nThe equality holds if and only if $\\mathcal D_{KL}(q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}) \\| p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X})) = 0$. The above fact suggests that we can optimize the ELBO as a surrogate for $\\log p(\\mathbf Y|\\mathbf A, \\mathbf X)$ which involves the intractable integration.\nMore importantly, when the ELBO is optimized w.r.t. $q$ distribution, the variational bound is lifted to the original log-likelihood and one has $q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X})=p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X})$, i.e., the variational distribution equals to the true posterior, which is what we expect.  \n\nPushing further and incorporating source graphs $\\mathcal G_m$ (we omit the superscript for simplicity), we arrive at the following objective:\n\\begin{equation}\\label{eqn-elbo}\n\\begin{split}\n    \\mathbb E_{\\mathcal G_m \\sim p(\\mathcal G)} \\left [ \n    \\mathbb{E}_{\\hat{\\mathbf{A} } \\sim q_\\theta(\\hat{\\mathbf{A} } | \\mathbf{A} = \\mathbf A_m, \\mathbf{X} = \\mathbf X_m   )} \\left [\\log p_{w_m }(\\mathbf{Y} | \\mathbf{A} = \\mathbf A_m, \\mathbf{X} = \\mathbf X_m, \\hat{\\mathbf{A} }  ) \\right. \\right. \\\\\n    \\left. \\left. + \\log p_{0}(\\hat{\\mathbf{A} } | \\mathbf{A} = \\mathbf A_m, \\mathbf{X} = \\mathbf X_m   ) - \\log q_\\theta(\\hat{\\mathbf{A} } | \\mathbf{A} = \\mathbf A_m, \\mathbf{X} = \\mathbf X_m   )  \\right ]\n    \\right ].\n\\end{split}\n\\end{equation}\nHere we instantiate $q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   )$ as the shared structure learner $g_\\theta$, $p(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)$ as a (shared) non-parametric prior distribution $p_0$ for latent structures, and $p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A}})$ as the dataset-specific GNN model $h_{w_m}$, to suit the framework for our formulated problem in Section~\\ref{sec-problem}. The formulation of \\eqref{eqn-elbo} shares the spirits with Bayesian meta learning~\\cite{maml-bayes}. We can treat the GNN training as a dataset-specific learning task and latent graph as a certain `learning algorithm' or `hyper-parameter', so \\eqref{eqn-elbo} essentially aims at learning a structure learner that can yield desirable `learning algorithm' for each specific learning task on graphs. Furthermore, the three terms in \\eqref{eqn-elbo} have distinct effects: i) the predictive term $\\log p_{w_m}$ acts as a supervised classification loss; ii) the prior term $\\log p_0$ serves for regularization on the generated structures; iii) the third term, which is essentially the entropy of $q_\\theta$, penalizes high confidence on certain structures. \n\nTo sum up, we can optimize \\eqref{eqn-elbo} with joint learning of the structure learner $g_\\theta$ and GNN models $\\{h_{w_m}\\}_{m=1}^M$ on source graphs $\\{\\mathcal G_m\\}_{m=1}^M$ for training the structure learner. After that, we can generalize the well-trained $g_{\\theta^*}$ to estimate latent graph structures for a new target graph $\\mathcal G^t = (\\mathbf A^t, \\mathbf X^t)$ and only need to train the GNN model $h_w$ w.r.t. the predictive objective with fixed $\\theta^*$:\n\\begin{equation}\n    \\mathbb{E}_{\\hat{\\mathbf{A} } \\sim q_{\\theta^*}(\\hat{\\mathbf{A} } | \\mathbf{A} = \\mathbf A^t, \\mathbf{X} = \\mathbf X^t   )} \\left [\\log p_{w}(\\mathbf{Y} | \\mathbf{A} = \\mathbf A^t, \\mathbf{X} = \\mathbf X^t, \\hat{\\mathbf{A} }   ) \\right ].\n\\end{equation}\n\nWe next discuss how to specify $g_\\theta$, $h_{w_m}$ and $p_0$ with special focus on their expressiveness and efficiency in Section~\\ref{sec-model-ins}. Later, we present the details for loss computation and model training based on the formulation stated above in Section~\\ref{sec-model-training}. \n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Model Instantiations",
                    "content": "\\label{sec-model-ins}\n\n",
                    "subsubsection 3.2.1": {
                        "name": "\\mathbf A",
                        "content": "\n\nThe variational distribution aims at learning the conditional distribution that generates suitable latent structures for message passing based on input observations. A natural means is to assume each edge of the latent graph as a Bernoulli random variable and the distribution $q$ is a product of $N\\times N$ independent Bernoulli random variables~\\cite{variational_inference_2,GIB-neurips20}.\n\nThe graph structure learner $g_\\theta$ can be used for predicting the Bernoulli parameter matrix. To accommodate the information from node features and graph structure, we can use the node representation, denoted as $\\mathbf z_u \\in \\mathbb R^d$, where $d$ is the embedding dimension, to compute the edge probability $\\alpha_{uv}$ for edge $(u,v)$ as\n\\begin{equation}\\label{eqn-sim}\n    \\alpha_{uv}= \\delta\\left (\\frac{1}{H}\\sum_{h=1}^H s(\\mathbf {w}_{h}^1 \\odot \\mathbf {z}_u, \\mathbf {w}_{h}^2 \\odot \\mathbf {z}_v) \\right ),\n\\end{equation}\nwhere $s(\\cdot, \\cdot)$ is a similarity function for two vectors, $\\odot$ denotes Hadamard product, $\\delta$ is a function that converts the input into values within $[0,1]$ and $\\mathbf {w}_{h}^1, \\mathbf {w}_{h}^2 \\in \\mathbb R^d$ are two weight vectors of the $h$-th head. Common choices for $s(\\cdot, \\cdot)$ include simple dot-product, cosine distance \\cite{cosine}, RBF kernel \\cite{rbf}, etc. Here we introduce $H$ heads and aggregate their results to enhance model's expressiveness for capturing the underlying influence between nodes from multifaceted causes, following the spirit of multi-head attention~\\cite{transformer,gat}. Besides, the weight vectors in \\eqref{eqn-sim} could learn to element-wisely scale the input vectors, i.e., node representations, and adaptively attend to dominant features. Apart from these, two weight vectors $\\mathbf {w}_{h}^1, \\mathbf {w}_{h}^2$ with independent parameterization could potentially have the same or distinct directions, which makes the model capable of connecting similar or dissimilar nodes and expressive enough to handle both homophilous and non-homophilous graphs. \n\nTo obtain discrete latent graph $\\hat{\\mathbf A} = \\{\\hat a_{uv}\\}_{N\\times N}$, one can sample from $\\hat a_{uv} \\sim Bernoulli(\\alpha_{uv})$ to obtain each latent edge. \nHowever, such an approach induces the quadratic algorithmic complexity $O(N^2)$ for computing and storing an estimated structure that entails potential links between any node pair, which could be prohibitive for large graphs. To reduce space and time complexity, we adopt a pivot-based structure learning method, as shown in Fig.~\\ref{fig-pivot}. Concretely, we randomly choose $P$ nodes in the graph as \\textit{pivots}, where\n$P$ is a hyperparameter much smaller than $N$ (e.g., $P\\approx \\frac{1}{10}N$). We then leverage pivot nodes as intermediates and convert the $N\\times N$ graph $\\hat {\\mathbf A}$, which can be prohibitively large with dense edges, into a cascade of one $N\\times P$ node-pivot bipartite graph $\\hat{\\mathbf B}_1$ and one $P\\times N$ pivot-node bipartite graph $\\hat{\\mathbf B}_2 = \\hat{\\mathbf B}_1^\\top$, which can effectively control the computational cost with proper $P$. In this way, we can compute a node-pivot similarity matrix $\\mathbf \\Gamma = \\{\\alpha_{up}\\}_{N\\times P}$ based on \\eqref{eqn-sim}, to parameterize the distribution of latent graph structures. This only requires $O(NP)$ time and space complexity, and one can sample from each $Bernoulli(\\alpha_{up})$ to obtain $\\hat{\\mathbf B}_1$ and $\\hat{\\mathbf B}_2$. In the meanwhile, the original $N\\times N$ adjacency matrix could be retrieved by $\\hat{\\mathbf A} = \\hat{\\mathbf B}_1 \\hat{\\mathbf B}_2$, which suggests that one can execute message passing on $\\hat{\\mathbf B}_1$ and $\\hat{\\mathbf B}_2$ to approximate that on $\\hat{\\mathbf A}$ (see more details in Section~\\ref{sec-method-ins-p}). In terms of the acceleration of structure learning, other strategies like the all-pair message passing schemes with linear complexity explored by \\cite{wunodeformer,wudifformer} can also be utilized to achieve the purpose.\n\n% Note that the elements in $\\mathbf S$ lie in $[-1,1]$. To obtain parameter matrix for Bernoulli distribution, we zero the elements in $\\mathbf{S}$ smaller than a threshold $\\epsilon$. Intuitively, we ignore the potential connection between a node-pivot pair if their similarity score is below the threshold. \n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "\\mathbf A",
                        "content": "\\label{sec-method-ins-p}\n\nThe predictive distribution, parameterized by the GNN network $h_{w_m}$, aims at recursively propagating features along the latent graph to update node representations and producing the prediction result for each node. We then present the details of GNN's message passing on the latent graph in order for enough expressiveness, stability and efficiency.\n\nTo begin with, we review the message-passing rule in common GNN models, like GCN~\\cite{gcn}, that operates on the original graph $\\mathbf A$:\n\\begin{equation}\\label{eqn-mp-gcn}\n\t\\mathbf Z^{(l+1)} = \\sigma \\left (\\operatorname{MP}_1(\\mathbf{Z}^{(l)},\\mathbf{A}) \\mathbf W^{(l)} \\right )= \\sigma \\left( \\mathbf D^{-\\frac{1}{2}} \\mathbf A \\mathbf D^{-\\frac{1}{2}} \\mathbf{Z}^{(l)} \\mathbf W^{(l)} \\right ), \n\\end{equation}\nwhere $\\mathbf W^{(l)}\\in \\mathbb R^{d\\times d}$ is a weight matrix, $\\sigma$ is non-linear activation, and $\\mathbf D$ denotes a diagonal degree matrix from input graph $\\mathbf A$ and $\\mathbf Z^{(l)} = \\{\\mathbf z_u^{(l)}\\}_{N\\times d}$ is a stack of node representations at the $l$-th layer. %In fact, \\eqref{eqn-mp-gcn} is the equivalent matrix-form of \\eqref{eqn-gnn}. \n\n \n\nWith the estimated latent graph $\\hat{\\mathbf A} = \\hat{\\mathbf B}_1 \\hat{\\mathbf B}_2$, we perform message passing $\\mathrm{MP}_2(\\cdot)$ in a two-step fashion to update node representations:\n\\begin{equation}\n    \\begin{split}\n        & \\mbox{i) node-to-pivot passing:} ~~~ \\mathbf C^{(l+\\frac{1}{2})} = \\mathrm{RowNorm} (\\mathbf \\Gamma^\\top) \\mathbf Z^{(l)}, \\\\\n        & \\mbox{ii) pivot-to-node passing:} ~~~ \\mathbf C^{(l+1)} = \\mathrm{RowNorm} ( \\mathbf \\Gamma) \\mathbf C^{(l+\\frac{1}{2})},\n    \\end{split}\n\\end{equation}\nwhere $\\mathbf C^{(l+\\frac{1}{2})}$ is an intermediate node representation and $\\mathbf \\Gamma = \\{\\alpha_{uv}\\}_{N\\times P}$ is the node-pivot similarity matrix calculated by \\eqref{eqn-sim}.  Such a two-step procedure can be efficiently conducted within $O(NP)$ time and space complexity. \n\n% Also, we can rigorously prove that the two-step message passing can update node representations in the same manner as the message-passing rule defined over original $N\\times N$ graph $\\hat{\\mathbf A}$.  \n% \\begin{proposition}\\label{prop-mp}\n% \tThe two-step message passing on $ n\\times p $ node-pivot adjacency matrix is equivalent to restoring $ n\\times n $ node adjacency matrix and performing vanilla message passing on it.\n% \\end{proposition}\n\nDespite that the feature propagation on the estimated latent structure could presumably yield better node representations, the original input graph structures also contain useful information, such as effective inductive bias~\\cite{geometriclearning-2016}. Therefore, we integrate two message-passing functions to compute layer-wise updating for node representations:\n\\begin{equation}\n    \\mathbf Z^{(l+1)} = \\sigma \\left( \\lambda \\mbox{MP}_1(\\mathbf Z^{(l)}, \\mathbf A) \\mathbf W^{(l)} + (1 - \\lambda) \\mbox{MP}_2(\\mathbf Z^{(l)}, \\hat{\\mathbf A}) \\mathbf W^{(l)} \\right ),\n\\end{equation}\nwhere $\\lambda$ is a trading hyper-parameter that controls the concentration weight on input structures. Such design could also improve the training stability by reducing the impact from large variation of latent structures through training procedure. \n\nWith $L$ GNN layers, one can obtain the prediction $\\hat {\\mathbf Y}$ by setting $\\hat {\\mathbf Y} = \\mathbf Z^{(L)}$ and $\\mathbf W^{(L-1)} \\in \\mathbb R^{d\\times C}$ where $C$ is the number of classes. Alg.~\\ref{alg:GNN} shows the feed-forward computation of message passing.\n\n"
                    },
                    "subsubsection 3.2.3": {
                        "name": "\\mathbf A",
                        "content": "\n\nThe prior distribution reflects how we presume on the latent graph structures without the information of observed labels. In other words, it characterizes how likely a given graph structure could provide enough potential for feature propagation by GNNs. The prior could be leveraged for regularization on the estimated latent graph $\\hat{\\mathbf A}$. In this consideration, we choose the prior as an energy function that quantifies the smoothness of the graph:\n\\begin{equation}\\label{eqn-reg}\n    p_0(\\hat{\\mathbf A}| \\mathbf X, \\mathbf A) \\propto \\exp{\\left( -\\alpha \\sum_{u, v} \\hat{\\mathbf A}_{uv} \\| \\mathbf x_u - \\mathbf x_v \\|_2^2 \n    %+ \\beta \\mathbf 1^\\top \\log (\\hat{\\mathbf A} \\mathbf 1) \n    - \\rho \\|\\hat{\\mathbf A}\\|_F^2 \\right ) },\n\\end{equation}\nwhere %$\\mathbf 1$ denotes an all-one column vector and \n$\\| \\cdot \\|_{F}$ is the Frobenius norm. The first term in \\eqref{eqn-reg} measures the smoothness of the latent graph~\\cite{dirichlet}, with the hypothesis that graphs with smoother feature has lower energy (i.e., higher probability).\nThe \nsecond term helps avoiding too large node degrees~\\cite{connectivity}. The hyperparameters\n$ \\alpha $ and $ \\rho $ control the strength for regularization effects.\n\nWhile we can retrieve the latent graph via $\\hat{\\mathbf A} = \\hat{\\mathbf B}_1 \\hat{\\mathbf B}_2$, the computation of \\eqref{eqn-reg} still requires $ O(N^2) $ cost. To reduce the overhead, we apply the regularization on the $ P \\times P $ pivot-pivot adjacency matrix $\\hat{\\mathbf{E}} = \\hat{\\mathbf B}_2\\hat{\\mathbf B}_1$ as a proxy regularization: %which can be calculated from sampled node-pivot adjacency matrix $\\bar{\\mathbf{G}} $ by Eq. \\eqref{eq:recover_B}, whose proof is in Appendix \\ref{append:recover adj}. $ \\mathbf{\\Omega} $ is defined in Eq. \\eqref{eq:lambda def}.\n\\begin{equation}\\label{eqn-reg1}\n\\begin{split}\n    \\mathcal R (\\hat{\\mathbf E}) &= \\log p_0(\\hat{\\mathbf A}| \\mathbf X, \\mathbf A) \\\\\n    & \\approx -\\alpha \\sum_{p, q} \\hat{\\mathbf E}_{pq} \\| \\mathbf x'_p - \\mathbf x'_q \\|_2^2 - \\rho \\|\\hat{\\mathbf E}\\|_F^2,\n\\end{split}\n\\end{equation}\nwhere $\\mathbf x'_p$ denotes the input feature of the $p$-th pivot node.\n\n"
                    }
                },
                "subsection 3.3": {
                    "name": "Model Training",
                    "content": "\\label{sec-model-training}\n\nFor optimization with \\eqref{eqn-elbo}, we proceed to derive the loss functions and updating gradients for $\\theta$ and $w_m$ based on the three terms $\\mathbb E_{q_\\theta}[\\log p_{w_m}]$, $\\mathbb E_{q_\\theta}[\\log p_0]$ and $\\mathbb E_{q_\\theta}[\\log q_\\theta]$. \n\n",
                    "subsubsection 3.3.1": {
                        "name": "w_m",
                        "content": "\\label{sec-model-training-p}\nThe optimization difficulty stems from the expectation over $q_\\theta$, where the sampling process is non-differentiable and hinders back-propagation. \nCommon strategies for approximating the sampling for discrete random variables include Gumbel-Softmax trick~\\cite{gumbel-iclr17} and REINFORCE trick~\\cite{reinforce}. However, both strategies yield a sparse graph structure each time of sampling, which could lead to high variance for the prediction result $\\log p_{w_m}(\\mathbf Y| \\mathbf A, \\mathbf X, \\hat{\\mathbf A})$ produced by message passing over a sampled graph. To mitigate the issue, we alternatively adopt the Normalized Weighted Geometric Mean (NWGM)~\\cite{NWGM} to move the outer expectation into the feature-level. Specifically, we have (see Appendix~\\ref{appx-deri} for detailed derivations)\n\\begin{equation}\\label{eqn-loss-sup-grad}\n    \\begin{split}\n        & \\nabla_{\\theta} \\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [ \\log p_{w_m} (\\mathbf Y| \\mathbf A, \\mathbf X, \\hat{\\mathbf A})] \\\\ \n        \\approx & \\nabla_{\\theta}  \\log p_{w_m} (\\mathbf Y| \\mathbf A, \\mathbf X, \\hat{\\mathbf A} = \\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [\\hat{\\mathbf A}]).\n    \\end{split}\n\\end{equation}\nWe denote the opposite of the above term  as $ \\nabla_\\theta \\mathcal{L}_s(\\theta) $.  The gradient w.r.t. $w_m$ can be similarly derived. The above form is a biased estimation for the original objective, yet it can reduce the variance from sampling and also improve training efficiency (without the need of message passing over multiple sampled graphs).\\eqref{eqn-loss-sup-grad} induces the supervised cross-entropy loss.\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "q_\\theta",
                        "content": "\nAs for the second term in \\eqref{eqn-elbo}, we adopt the REINFORCE trick, i.e., policy gradient, to tackle the non-differentiability of sampling from $q_\\theta$. Specifically, for each feedforward computation, we sample from the Bernoulli distribution for each edge given by the estimated node-pivot similarity matrix, i.e., $Bernoulli(\\alpha_{up})$, and obtain the sampled latent bipartite graph $\\hat{\\mathbf B}_1$ and subsequently have $\\hat{\\mathbf E} = \\hat{\\mathbf B}_1 \\hat{\\mathbf B}_2 = \\hat{\\mathbf B}_1 \\hat{\\mathbf B}_1^\\top$. The probability for the latent structure could be computed by \n\\begin{equation}\\label{eqn-pi}\n\\pi_\\theta(\\hat{\\mathbf E}) = \\prod_{u, p} \\left( \\hat{\\mathbf B}_{1,up}  \\alpha_{up} + (1 - \\hat{\\mathbf B}_{1,up}) \\cdot(1 - \\alpha_{up})    \\right).\n\\end{equation}\nDenote $\\hat{\\mathbf E}_k$ as the sampled result at the $k$-th time, we can independently sample $K$ times and obtain $\\{\\hat{\\mathbf E}_k\\}_{k=1}^K$ and $\\{\\pi_\\theta(\\hat{\\mathbf E}_k)\\}_{k=1}^K$. Recall that the regularization reward from $\\log p_0$ has been given by \\eqref{eqn-reg1}. \nThe policy gradient~\\cite{reinforce} yields the gradient of loss for $\\theta$ as\n\\begin{equation}\\label{eqn-grad-reg}\n\\begin{split}\n    \\nabla_{\\theta} \\mathcal L_r(\\theta) &= -\\nabla_{\\theta} \\mathbb E_{\\hat{\\mathbf{A}}\\sim q(\\hat{\\mathbf{A}}  |  \\mathbf{X},\\mathbf{A} )} [\\log p_0(\\hat{\\mathbf{A}}  |  \\mathbf{X},\\mathbf{A})] \\\\\n    &\\approx -\\nabla_{\\theta} \\frac{1}{K} \\sum_{k=1}^K \\log \\pi_\\theta(\\hat {\\mathbf E}_k) \\left(\\mathcal R(\\hat{\\mathbf E}_k) - \\overline{\\mathcal R}\\right),\n\\end{split}\n\\end{equation}\nwhere $\\overline{\\mathcal R}$ acts as a baseline function by averaging the regularization rewards $\\mathcal R(\\hat{\\mathbf E}_k)$ in one feed-forward computation, which helps to reduce the variance during policy gradient training~\\cite{policy}. %Notice that here $\\mathcal R(\\hat{\\mathbf B}^k)$ does not contribute to gradient w.r.t. $\\theta$ and in practice we can truncate its gradient backpropagation.\n\n"
                    },
                    "subsubsection 3.3.3": {
                        "name": "q_\\theta",
                        "content": "\n\nThe last entropy term for $q_\\theta$ could be directly computed by\n\\begin{equation}\\label{eqn-loss-ent}\n\\begin{split}\n    \\mathcal L_{e} (\\theta) &= \\mathbb E_{\\hat{\\mathbf{A}}\\sim q(\\hat{\\mathbf{A}}  |  \\mathbf{X},\\mathbf{A} )} [\\log q(\\hat{\\mathbf{A}}  |  \\mathbf{X},\\mathbf{A} )] \\\\\n    &\\approx\n\t\t\\frac{1}{NP} \\sum_{u=1}^{N} \\sum_{p=1}^{P} \\left[ \\alpha_{up}\\log \\alpha_{up}  + (1- \\alpha_{up}  ) \\log (1-\\alpha_{up} )  \\right],\n\\end{split}\n\\end{equation}\nwhere we again adopt the node-pivot similarity matrix as a proxy for the estimated latent graph.\n\n\n\n"
                    },
                    "subsubsection 3.3.4": {
                        "name": "Iterative Structure Learning for Acceleration",
                        "content": "\nA straightforward way is to consider once structure inference and once GNN's message passing for prediction in each feed-forward computation. To enable structure learning and GNN learning mutually reinforce each other~\\cite{idgl}, we consider multiple iterative updates of graph structures and node representations before once back-propagation. More specifically, in each epoch, we repeatedly update node representations $\\mathbf{Z}^{t} $ (where the superscript $t$ denotes the $t$-th iteration) and latent graph $\\hat{\\mathbf{A}}^{t} $ until a given maximum budget is achieved. To accelerate the training,\nwe aggregate the losses $\\mathcal L^{t}$ in each iteration step for parameter updating.\nAs different graphs have different feature space, we utilize the first layer of GNN as an encoder at the very beginning and then feed the encoded representations to structure learner.\nThe training algorithm for structure learner $g_\\theta$ on source graphs is described in Alg.~\\ref{alg:meta-learning} (in the appendix) where we train structure learner for multiple episodes and in each episode, we train $g_\\theta$ on each source graph for several epochs.\nIn testing, the well-trained $g_\\theta$ is fixed and we train a GNN $h_w$ on the target graph with latent structures inferred by $g_\\theta$, as described in Alg.~\\ref{alg:sup-learning}. \n\n"
                    }
                }
            },
            "section 4": {
                "name": "Related Works",
                "content": "\n\n\\paragraph{Graph Neural Networks} \nGraph neural networks (GNNs)~\\cite{gcn,gat,graphsage,jknet-icml18,mixhop-icml19,geng2023pyramid} have achieved impressive performances in modeling graph-structured data. Nonetheless, there is increasing evidence suggesting GNNs' deficiency for graph structures that are inconsistent with the principle of message passing.\nOne typical situation lies in non-homophilous graphs~\\cite{zhu2020beyond}, where adjacent nodes tend to have dissimilar features/labels. Recent studies devise adaptive feature propagation/aggregation to tackle the heterophily~\\cite{spectral_filter,zhu2020graph,zhu2020beyond,gpr}. Another situation stems from graphs with noisy or spurious links, for which several works propose to purify the observed structures for more robust node representations~\\cite{sparsification,robust-topo}. Our work is related to these works by searching adaptive graph structures that is suitable for GNN's message passing. Yet, the key difference is that our method targets learning a new graph out of the scope of input one, while the above works focus on message passing within the input graph.\n\n\\paragraph{Graph Structure learning} To effectively address the limitations of GNNs' feature propagation within observed structures, many recent works attempt to jointly learn graph structures and the GNN model. For instance, \\cite{lds} models each edge as a Bernoulli random variable and optimizes graph structures along with the GCN. To exploit enough information from observed structure for structure learning, \\cite{rbf} proposes a metric learning approach based on RBF kernel to compute edge probability with node representations, while \\cite{attention} adopts attention mechanism to achieve the similar goal. Furthermore,\n\\cite{idgl} considers an iterative method that enables mutual reinforcement between learning graph structures and node embeddings. Also,\n\\cite{bgcnn} presents a probabilistic\nframework that views the input graph as a random sample from a collection modeled by a parametric random graph model.\n% Based on \\cite{bgcnn}, \\cite{bgcnn-non-param} formulates the posterior inference of the\n% graph in a non-parametric fashion.\n\\cite{lao2022variational, variational_inference_2} harnesses variational inference to estimate a posterior of graph structures and GNN parameters. While learning graph structures often requires $O(N^2)$ complexity, a recent work \\cite{wunodeformer} proposes an efficient Transformer that achieves latent structure learning in each layer with $O(N)$ complexity. However, though these methods have shown promising results, they assume training nodes and testing nodes are from the same graph and consider only one graph.\nBy contrast, we consider graph structure learning under the cross-graph setting and propose a general framework to learn a shared structure learner which can generalize to target graphs without any re-training.\n\n\\paragraph{Out-of-Distribution Generalization on Graphs.} Due to the demand for handling testing data in the wild, improving the capability of the neural networks for performing satisfactorily on out-of-distribution data has received increasing attention~\\cite{wu2022handling,ma2021subgroup,ligraphde,wu2023energybased,wu2021towards2,yang2023pmlp}. Recent studies, e.g., \\cite{zhu2021shift,wu2022handling,sui2022causal} explore effective treatments for tackling general distribution shifts on graphs, and there are also works focusing on particular categories of distribution shifts like size generalization~\\cite{bevilacqua2021size}, molecular scaffold generalization~\\cite{yang2022learning}, feature/attribute shifts~\\cite{wu2021towards,ood2023bi}, topological shifts~\\cite{yang2022geometric}, etc. To the best of our knowledge, there is no prior works considering OOD generalization in the context of graph structure learning. In our case, the target graph, where the structure learner is expected to yield adaptive structures, can have disparate distributions than the source graphs. The distribution shifts could potentially stem from feature/label space, graph sizes or domains (e.g., from social networks to citation networks). As the first attempt along this path, our work can fill the research gap and enable the graph structure learning model to deal with new unseen graphs in an open world.\n\n\n"
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\nWe apply \\mymodel to real-world datasets for node classification to test the efficacy of proposed structure learner for boosting performance of GNN learning on target graphs with distribution shifts from source graphs. We specify the backbone GNN network for \\mymodel as a two-layer GCN~\\cite{gcn}. We focus on the following research questions: \n\n\\noindent\\bb\\;\\textbf{1)} How does \\mymodel perform compared with directly training GNN models on input structure of target graphs? \n\n\\noindent\\bb\\;\\textbf{2)} How does \\mymodel perform compared to state-of-the-art structure learning models that are directly trained on target datasets in terms of both accuracy and training time? \n\n\\noindent\\bb\\;\\textbf{3)} Are the proposed components of \\mymodel effective and necessary for the achieved performance? \n\n\\noindent\\bb\\;\\textbf{4)} What is the impact of hyper-parameter on performance and what is the impact of attack on observed edges? \n\n\\noindent\\bb\\;\\textbf{5)} What is the property of inferred latent graphs and what generalizable pattern does the structure learner capture?\n% \\begin{itemize}\n%     \\item \n% \\end{itemize}\n\n\n% We probe into three tasks with increasing difficulty: 1) transfer within graphs from one domain; 2) transfer among graphs from different domains yet with similar properties; 3) transfer among graphs from different domains with different properties. Moreover, we conduct ablation studies and more discussions to shed more insights on our approach.\n\n",
                "subsection 5.1": {
                    "name": "Experimental Protocols",
                    "content": "\n\n\\textbf{Datasets.} Our experiments are conducted on several public graph datasets. First we consider three commonly used citation networks \\textsc{Cora}, \\textsc{CiteSeer} and \\textsc{PubMed}. We use the same splits as in \\cite{planetoid}. These three datasets have high homophily ratios (i.e., adjacent nodes tend to have similar labels) \\cite{lim2021new}. Apart from this, we also consider four social networks from \\textsc{Facebook-100} \\cite{traud2012social}, which have low homophily ratios.\nReaders may refer to Appendix \\ref{appx-implement} for more dataset information like splitting ratios.\n%We apply feature normalization to all the datasets in data preprocessing.\n\n\\textbf{Competitors.} We mainly compare with GCN \\cite{gcn}, the GNN counterpart trained on input structure, for testing the efficacy of produced latent graphs by \\mymodel. As further investigation, we also compare with other advanced GNN models: GraphSAGE \\cite{graphsage}, GAT \\cite{gat}, APPNP \\cite{appnp}, $\\mbox{H}_2$GCN \\cite{zhu2020beyond} and GPRGNN \\cite{gpr}. Here APPNP, $\\mbox{H}_2$GCN and GPRGNN are all strong GNN models equipped with adaptive feature propagation and high-order aggregation. For these pure GNN models, the training and testing are considered on (the same) target graphs. \nFurthermore, we compete \\mymodel with state-of-the-art graph structure learning models, IDS~\\cite{lds}, IDGL~\\cite{idgl} and VGCN~\\cite{variational_inference_2}. Since these models are all designed for training on one dataset from scratch, we directly train them on the target graph and they in principle could yield better performance than \\mymodel.\n\nWe also consider variants of \\mymodel as baselines. We replace the similarity function $s$ with attention-based structure learner, denoted as \\mymodelgat, which follows the same training scheme as \\mymodel. Besides, we consider some non-parametric similarity functions like dot-product, KNN and cosine distance (denoted as $\\mathrm{\\mymodel_{dp}}$, $\\mathrm{\\mymodel_{knn}}$ and $\\mathrm{\\mymodel_{cos}}$, respectively). For these models, we only need to train the GNN network on target graphs with the non-parametric structure learners yielding latent structures. In addition, we introduce a variant \\mymodelvar that shares the same architecture as \\mymodel and is directly trained on target graphs. Also, \\mymodelvar in principle could produce superior results than \\mymodel. We report the test accuracy given by the model that produces the highest validation accuracy within 500 training epochs.\n\n"
                },
                "subsection 5.2": {
                    "name": "In-domain Generalization",
                    "content": "\n\n\n\n\n\nWe first consider transferring within social networks or citation networks. The results are reported in Table~\\ref{tab-res1} where for each social network (resp. citation network) as the target, we use the other social networks (resp. citation networks) as the source datasets.  \n%The results are shown in Table~\\ref{tab-res} where we use each graph as the target and the others as source graphs. As we can seen, \n\\mymodel performs consistently better than GCN, i.e., the counterpart using observed graph for message passing, which proves that \\mymodel can capture generalizable patterns for desirable message-passing structure for unseen datasets that can indeed boost the GCN backbone's performance on downstream tasks. In particular, the improvement over GCN is over 5\\% on \\textsc{Cornell5} and \\textsc{Reed98}, two datasets with low homophily ratios (as shown in Table~\\ref{tab:dataset_statistic}). The reason is that for non-homophilous graphs where the message passing may propagate inconsistent signals (as mentioned in Section~\\ref{sec-intro}), the GNN learning could better benefits from structure learning than homophilous graphs. Furthermore, compared to other strong GNN models, \\mymodel still achieves slight improvement than the best competitors though the backbone GCN network is less expressive. One could expect further performance gain by \\mymodel if we specify the GNN backbone as other advanced architectures.\n\nIn contrast with non-parametric structure learning models and \\mymodelgat, \\mymodel outperforms them by a large margin throughout all cases, which verifies the superiority of our design of multi-head weighted similarity function that can accommodate multi-faceted diverse structural information. Compared with \\mymodelvar, \\mymodel performs on par with and even exceeds it on \\textsc{Cornell5} and \\textsc{Amherst41}. The possible reasons are two-fold. First, there exist sufficient shared patterns among citation networks (resp. social networks), which paves the way for successful generalization of \\mymodel. Second, \\mymodelvar could sometimes overfit specific datasets, since the amount of free parameters are regularly orders-of-magnitude more than the number of labeled nodes in the dataset. The results also imply that our transfer learning approach can help to mitigate over-fitting on one dataset. %Notably, other non-parametric graph learning methods like $\\mathrm{GSL_{dt}}$, $\\mathrm{GSL_{knn}}$ and $\\mathrm{GSL_{cosine}}$ perform much worse than \\mymodel, so we can see a clear advantage of \\mymodel over these methods when it comes to non-homophilous graphs.\nMoreover, \\mymodel can generalize structure learner to unseen graphs that is nearly three times larger than training graphs, i.e., \\textsc{Cornell5}. \n\n"
                },
                "subsection 5.3": {
                    "name": "Cross-domain Generalization",
                    "content": "\nWe next consider a more difficult task, transferring between social networks and citation networks. The difficulty stems from two aspects: 1) social networks and citations graphs are from distinct categories thus have larger underlying data-generating distribution gaps; 2) they have varied homophily ratios, which indicates that the observed edges play different roles in original graphs. \n%Therefore, such a task requires the model to extrapolate to out-of-distribution target graphs. %We use $S_3$ as the similarity metric.\nIn Table~\\ref{tab-res2} we report the results.\n%we first use \\textsc{Cora}, \\textsc{Citeseer} and \\textsc{Pubmed} as source graphs, and four \\textsc{FB-100} datasets as target graphs. Conversely, we consider \\textsc{FB-100} datasets as source graphs, and three citation networks as the target. \nDespite the task difficulty, \\mymodel manages to achieve superior results than GCN and also outperforms other non-parametric graph structure learning methods throughout all cases. This suggests \\mymodel's ability for handling target graphs with distinct properties. \n\nIn Fig.~\\ref{fig-res} we further compare \\mymodel with three state-of-the-art graph structure learning models that are directly trained on target graphs. Here we follow the setting in Table~\\ref{tab-res2}. The results show that even trained on source graphs that are different from the target one, \\mymodel still performs on par with the competitors that are trained and tested on (the same) target graphs. Notably, \\mymodel significantly reduces training time. \nFor instance, in \\textsc{John Hopkins55}, \\mymodel is 6x, 9x and 40x faster than IDGL, LDS and VGCN, respectively. This shows one clear advantage of \\mymodel in terms of training efficiency and also verifies that our model indeed helps to reduce the significant cost of training time for structure learning on target graphs. \n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Ablation Studies",
                    "content": "\n\n%In Table~\\ref{tab-res1} and \\ref{tab-res2} we have verified the effectiveness of our design for the multi-head weighted similarity functions. \nWe conduct ablation studies to test the effectiveness of iterative learning scheme and regularization on graphs.\n\n\\textbf{Effect of Iterative Learning.}\nWe replace the iterative learning process as a one-step prediction (i.e., once structure estimation and updating node representations in once feed-forward computation) and compare its test accuracy with \\mymodel. The results are shown in Fig. ~\\ref{fig:ablation} where we follow the setting of Table~\\ref{tab-res1}. \nThe non-iterative version exhibits a considerable drop in accuracy (as large as 5.4\\% and 8.8\\% when tested on target graphs \\textsc{Cornell5} and \\textsc{Amherst41}, respectively).\nTherefore, the iterative updates indeed help to learn better graph structures and node embeddings, contributing to higher accuracy for downstream prediction.\n\n\\textbf{Effect of Regularization on structures.}\nWe remove the regularization on structures (i.e., setting $\\alpha = \\rho = 0$) and compare with \\mymodel. As shown in Fig.~\\ref{fig:ablation}, there is more or loss performance degradation. In fact, the regularization loss derived from the prior distribution for latent structures could help to provide some guidance for structure learning, especially when labeled information is limited.\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.5": {
                    "name": "Hyper-parameter Sensitivity",
                    "content": "\n\nIn Fig.~\\ref{fig-para} (in the appendix), we study the variation of model's performance w.r.t. $\\lambda$ (the weight on input graphs) and $P$ (the number of pivots) on target datasets \\textsc{Cora} and \\textsc{CiteSeer}. \nOverall, the model is not sensitive to $\\lambda$'s.\nFor \\textsc{Cora}, larger $\\lambda$ contributes to higher accuracy, while for \\textsc{CiteSeer}, smaller $\\lambda$ yields better performance. The possible reason is that the initial graph of \\textsc{Cora} is more suitable for message passing (due to higher homophily ratio).\nFor the impact of pivot number, as shown in Fig.~\\ref{fig-para}(b), a moderate value of $P$ could provide decent downstream performance. \n\n"
                },
                "subsection 5.6": {
                    "name": "Robustness Analysis",
                    "content": "\n\nIn addition, we find that \\mymodel is more immune to edge deletion attack than GCN. We randomly remove 10-50\\% edges of target graphs respectively, and then apply \\mymodel and GCN. \nWe present the results in Johns Hopkins55 in Fig.~\\ref{fig:edge_delete} and leave more results in Appendix \\ref{appx-moreres}. When the drop ratio increases, the performance gap between two models becomes more significant.\nThis is due to our structure learner's ability for learning new graph structures from node embeddings, making it less reliant on initial graph structures and more robust to attack on input edges. \n\n"
                },
                "subsection 5.7": {
                    "name": "Case Study",
                    "content": "\n%One great property of our method is to improve the homophily of graph,  making it easier to perform message passing. This effect is most conspicuous on non-homophilous graphs.\nWe further probe into why our approach is effective for node classification by dissecting the learnt graph structures. \nSpecifically, we measure the homophily ratios of learnt structures and their variance of neighborhood distributions of nodes with same labels.\nAs nodes receive messages from neighbors in message passing, the more similar the neighborhood patterns of nodes within one class are, the easier it is for GNNs to correctly classify them \\cite{ma2022is}.\nWe use homophily metric proposed in \\cite{lim2021new}  to measure homophily ratios.\nFor calculation of variance of neighborhood distribution, we first calculate variance for each class, and then take weighted sum to get the final variance, where the weight is proportional to the number of nodes within corresponding class.\n\n\\textbf{Homophily Ratio.} We choose \\textsc{Amherst41}, \\textsc{Johns Hopkins55} and \\textsc{Reed98} as target graphs, and record the homophily ratios of inferred latent structures every five epochs during training. As shown in Fig. \\ref{fig:homo}. the homophily ratios of inferred latent graphs exhibit a clear increase as the training epochs become more and the final ratio is considerably larger than that of input graph. The results indicate that the trained structure learner incline to output more homophilous latent structures that are reckoned to be more suitable for message passing. \n\n\\textbf{Neighborhood Distribution Variance.} As shown in Fig. \\ref{fig:variance}, the variance of neighborhood distribution of nodes with the same label is  significantly smaller in our learnt structure, making it easier to classify nodes through message passing.\nThe results also imply that  high homophily ratio and similar intra-class neighborhood patterns could be two of the underlying transferrable patterns of optimal message-passing structure, identified by \\mymodel.\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nThis paper proposes \\textit{Graph Structure Learning Under Cross-Graph Distribution Shift}, a new problem that requires structure learner to transfer to new target graphs without re-training and handles distribution shift.\nWe develop a transfer learning framework that guides the structure learner to discover shared knowledge across source datasets with respect to optimal message-passing structure for boosting downstream performance. We also carefully design the model components and training approach in terms of expressiveness, scalability and stability.\nWe devise experiments with various difficulties and  demonstrate the efficacy and robustness of our approach.\nAlthough our framework is pretty general, we believe their are other potential methods that can lead to equally competitive results, which we leave as future work.\n\n\\begin{acks}\nThe work was supported in part by National Key Research and Development Program of China (2020AAA0107600), NSFC (62222607), Science and Technology Commission of Shanghai Municipality (22511105100), and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).\n\\end{acks}\n\t\n\\clearpage\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{ref}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\n\n%\\newpage\n\\appendix\n\n\\clearpage\n\\nobalance\n\n\\begin{algorithm}[t]\n    \\caption{Message Passing over Latent Graphs $[\\mathbf Z, \\hat{\\mathbf Y}] = \\mbox{GNN}(\\mathbf A, \\mathbf X, \\mathbf \\Gamma; w)$}\n    \\label{alg:GNN}\n    \\KwIn{node features $\\mathbf X$, input adjacency $\\mathbf A$, latent graph node-pivot similarity matrix $\\mathbf \\Gamma = \\{\\alpha_{up}\\}_{N\\times P}$.}\n%    Compute $\\mathbf \\Lambda$ and $\\mathbf \\Omega$ by $\\mathbf \\Lambda_{pp}=\\sum_{u=1}^N \\alpha_{up}, ~~\n%\t\\mathbf \\Omega_{uu}=\\sum_{p=1}^P \\alpha_{up}$\\;\n\t$\\mathbf Z^{(0)} \\leftarrow \\mathbf X$\\;\n\t\\For {$l = 0,1,\\cdots,L-1$}{\n\t$\\mathbf Z^{(l+1)} \\leftarrow \\mathbf D^{-\\frac{1}{2}} \\mathbf A \\mathbf D^{-\\frac{1}{2}} \\mathbf Z^{(l)} \\mathbf W^{(l)}$\\;\n\t$\\mathbf C^{(l+\\frac{1}{2})} = \\mathrm{RowNorm}  (\\mathbf \\Gamma^\\top) \\mathbf Z^{(l)}$\\;\n\t$\\mathbf C^{(l+1)} = \\mathrm{RowNorm} (\\mathbf \\Gamma) \\mathbf C^{(l+\\frac{1}{2})}$ \\;\n\t$\\mathbf Z^{(l+1)} \\leftarrow \\sigma \\left((\\lambda \\mathbf Z^{(l+1)}+ (1-\\lambda)\\mathbf C^{(l+1)} \\mathbf W^{(l)} \\right) $\\;\n\t}\n\t\\KwOut{node representations $\\mathbf Z = \\mathbf Z^{(L-1)}$, node-level prediction $\\hat{\\mathbf Y} = \\mathbf Z^{(L)}$.}\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\t\\caption{Training Graph Structure Learner}\n\t\\label{alg:meta-learning}\n\t\\KwIn{observed source graphs $\\{\\mathcal G_m\\}_{m=1}^M = \\{(\\mathbf A_m, \\mathbf X_m, \\mathbf Y_m)\\}_{m=1}^M$, maximum training episode $M$, maximum iteration $T$, a shared graph structure learner $g_\\theta$, GNN networks $ \\{ h_{w_m} \\}_{m=1}^M$ for each source graph.}\n\tInitialize $\\theta$ and $\\{w_m\\}_{m=1}^M$\\;\n\t\\For {episode: $1,2,\\dots,E$}{\n\t\t\\For {each source graph $\\mathcal G_m$}{\n\t\t\t\\For {$e=1:T$}{\n\t\t\t\t$\\Delta_\\theta \\leftarrow 0$, $\\Delta_w \\leftarrow 0$, $ t \\leftarrow 0 $\\;\n\t\t\t\t% Calculate normalized adjacency matrix $L^{(0)}$ \\;\n\t\t\t\t$\\mathbf Z^0 = \\mbox{MLP}(\\mathbf X; w_m)$ or $\\mathbf Z^0 = \\mbox{GCN}(\\mathbf A, \\mathbf X; w_m)$\\;\n\t\t\t\t\\While{not converged}{\n\t\t\t\t\t$ t \\leftarrow t+1 $\\;\n\t\t\t\t\tCompute $\\mathbf \\Gamma^t = \\{\\alpha_{up}\\}$ using \\eqref{eqn-sim} with $\\mathbf Z^{t-1}$\\;\n\t\t\t\t\tSample $K$ times over $\\mathbf \\Gamma^t$ to obtain $\\{\\hat{\\mathbf E}_k^t\\}_{k=1}^K$\\;\n\t\t\t\t\tCompute $\\{\\pi_\\theta(\\hat{\\mathbf E}_k^t)\\}_{k=1}^K$ using \\eqref{eqn-pi}\\;\n\t\t\t\t\t$[\\mathbf Z^t, \\hat{\\mathbf Y}^t] = \\mbox{GNN}(\\mathbf A, \\mathbf X, \\mathbf \\Gamma^t; w_m)$\\;\n\t\t\t\t\tCompute $\\nabla_{\\theta} \\mathcal L_s, \\nabla_{\\theta} \\mathcal L_r, \\nabla_{\\theta} \\mathcal L_e$ using \\eqref{eqn-loss-sup-grad}, \\eqref{eqn-grad-reg}, \\eqref{eqn-loss-ent}, respectively \\;\n\t\t\t\t\t% calculate $\\hat{\\mathbf{Y}}^{(1)}$ using Eq.~\\eqref{eq_pred}\\;\n\t\t\t\t\t% calculate $ \\mathcal{L}^{(1)} $ using Eq.~\\eqref{eq_loss}\\;\n\t\t\t\t\t%stopCond$\\leftarrow \\| \\mathbf S^{(t)}- \\mathbf S^{(t-1)} \\|_{F}^2> \\delta  \\| \\mathbf S^{(1)} \\|_{F}^2  $\\;\n\t\t\t\t\t$\\Delta_\\theta^{(t)} \\leftarrow \\nabla_{\\theta} \\mathcal L_s + \\nabla_{\\theta} \\mathcal L_r + \\nabla_{\\theta} \\mathcal L_e$\\;\n\t\t\t\t\t$\\Delta_w^{(t)} \\leftarrow \\nabla_{w_m} \\mathcal L_s$\\;\n\t\t\t\t}\n\t\t\t\t$\\Delta_\\theta \\leftarrow \\Delta_\\theta^{(0)} + \\sum_{i=2}^t \\Delta_\\theta^{(i)} / (t-1)  $\\;\n\t\t\t\t$\\Delta_w \\leftarrow \\Delta_w^{(0)} +  \\sum_{i=2}^t\\Delta_w^{(i)} / (t-1)  $\\;\n\t\t\t\tUse gradient $\\Delta_\\theta$ to update $\\theta$\\;\n\t\t\t\tUse gradient $\\Delta_w$ to update $w_m$\\;\n\t\t\t}\n\t\t}\n\t}\n\t\\KwOut{trained graph structure learner $g_{\\theta^*}$.}\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n    \\caption{Supervised Learning for target GNN}\n    \\label{alg:sup-learning}\n    \\KwIn{observed target graphs $\\mathcal G = (\\mathbf A, \\mathbf X, \\mathbf Y)$, GNN network $h_w$ for the target graph, maximum iteration $T$.}\n\tInitialize $w$\\;\n\t\\For {$e=1:T$}{\n\t$t \\leftarrow 0$, $\\Delta_w \\leftarrow 0$, $\\mathbf Z^{(0)} \\leftarrow \\mathbf X_m$, $\\mathbf A \\leftarrow \\mathbf A_m$\\;\n\t$\\mathbf Z^0 = \\mbox{MLP}(\\mathbf X; w)$ or $\\mathbf Z^0 = \\mbox{GCN}(\\mathbf A, \\mathbf X; w)$\\;\n\t\\While{not converged}{\n\t\t\t\t$ t \\leftarrow t+1 $\\;\n\t\t\t    Compute $\\mathbf \\Gamma^t = \\{\\alpha_{up}\\}$ using \\eqref{eqn-sim} with $\\mathbf Z^{t-1}$\\;\n\t\t\t\tSample $K$ times over $\\mathbf \\Gamma^t$ to obtain $\\{\\hat{\\mathbf E}^t_k\\}_{k=1}^K$\\;\n\t\t\t\tCompute $\\{\\pi_\\theta(\\hat{\\mathbf E}^t_k)\\}_{k=1}^K$ using \\eqref{eqn-pi}\\;\n\t\t\t\t$[\\mathbf Z, \\hat{\\mathbf Y}] = \\mbox{GNN}(\\mathbf A, \\mathbf X, \\mathbf \\Gamma^t; w)$\\;\n\t\t\t\tCompute $\\nabla_{\\theta} \\mathcal L_s$ using \\eqref{eqn-loss-sup-grad} \\;\n\t\t\t\t% calculate $\\hat{\\mathbf{Y}}^{(1)}$ using Eq.~\\eqref{eq_pred}\\;\n\t\t\t\t% calculate $ \\mathcal{L}^{(1)} $ using Eq.~\\eqref{eq_loss}\\;\n\t\t\t\t%stopCond$\\leftarrow \\| \\mathbf S^{(t)}- \\mathbf S^{(t-1)} \\|_{F}^2> \\delta  \\| \\mathbf S^{(1)} \\|_{F}^2  $\\;\n\t\t\t\t$\\Delta_w^{(t)} \\leftarrow \\nabla_{w_m} \\mathcal L_s$\\;\n\t\t\t}\n\t\t$\\Delta_w \\leftarrow \\Delta_w^{(0)} +  \\sum_{i=2}^t\\Delta_w^{(i)} / (t-1)  $\\;\n\t\tUse gradient $\\Delta_w$ to update $w$\\;\n\t\t}\n\t\\KwOut{trained GNN network $h_{w^*}$ for the target graph.}\n\\end{algorithm}\n\n% \\section{Proofs of Theorem and More Experiment Details} \n\n% \\subsection{Proof of Recovering Adjacency Matrix the Correctness of Two-step Message Passing} \\label{append:recover adj}\n% The row-normalized adjacency matrix used in vanilla message passing is  $ n \\times n $ node-to-node transition matrix. \n% Using Markov chain theory, we can restore the node-to-node adjacency matrix $ \\mathbf{A} $ by matrix multiplication of node-to-pivot transition matrix $ \\mathbf{T}_1 $ and pivot-to-node transition matrix $ \\mathbf{T}_2 $:\n% \\begin{align}\n% \t\\mathbf{T}_1&=\\mathbf{\\Lambda}^{-1}\\bar{\\mathbf{G}}^\\top\\\\\n% \t\\mathbf{T}_2&=\\mathbf{\\Omega}^{-1}\\bar{\\mathbf{G}}\\\\\n% \t\\mathbf{A}&=\t\\mathbf{T}_1\t\\mathbf{T}_2\n% \\end{align}\n% the definition of $ \\mathbf{\\Lambda} $ and $ \\mathbf{\\Omega} $ is in Eq. \\eqref{eq:lambda def}.\n% Therefore, given input node feature $ \\mathbf{F} $, vanilla message passing result on $ \\mathbf{A} $ is:\n% \\begin{align}\n% \t\\mathbf{\\Omega}^{-1}\\bar{\\mathbf{G}}  \\mathbf{\\Lambda}^{-1}\\bar{\\mathbf{G}}^\\top \\mathbf{F},\n% \\end{align} \n% which is the same as performing two-step message passing in Eq. \\eqref{eq:2-step mp}.\n% But the time complexity of performing our two-step message passing is $ O(npd) $ ($ d $ is the feature dimension), whereas the time complexity of performing vanilla message passing is $ O(n^2p+n^2d) $.  Therefore, the two-step message passing is much more efficient.\n\n% The row-normalized pivot adjacency matrix $\\tilde{\\mathbf{P}} $ can be recovered in the same manner:\n% \\begin{align}\n% \t\\tilde{\\mathbf{P}}=\t\\mathbf{\\Lambda}^{-1}\\bar{\\mathbf{G}}^\\top  \\mathbf{\\Omega}^{-1}\\bar{\\mathbf{G}}.\n% \\end{align}\n% The unnormalized pivot adjacency matrix $ \\mathbf{P} $ can be obtained by multiplying normalizing matrix $ \\mathbf{\\Lambda} $ with $  \\tilde{\\mathbf{P}} $:\n% \\begin{align}\n% \t\\mathbf{P} &= \\mathbf{\\Lambda} \\tilde{\\mathbf{P}} \\\\\n% \t&=\\bar{\\mathbf{G}}^\\top  \\mathbf{\\Omega}^{-1}\\bar{\\mathbf{G}}\n% \\end{align}\n\n"
            },
            "section 7": {
                "name": "Derivations for NWGM",
                "content": "\\label{appx-deri}\n\nFirst, when taking the gradient, we have $\\nabla_\\theta \\mathbb E_{q_\\theta} [ \\log p_{w_m}(\\mathbf Y| \\mathbf A, \\mathbf X, \\hat{\\mathbf A})]$ $ \\approx c \\nabla_\\theta   \\log  \\mathbb E_{q_\\theta} [ p_{w_m}(\\mathbf Y| \\mathbf A, \\mathbf X, \\hat{\\mathbf A})]$ with basic applications of the chain rule. We then adopt Normalized Weighted Geometric Mean (NWGM)~\\cite{NWGM}:\n\\begin{equation}\n    \\begin{split}\n        & \\nabla_\\theta   \\log \\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [ p_{w_m}(\\mathbf Y_{u, c} = 1| \\mathbf A, \\mathbf X, \\hat{\\mathbf A})] \\\\ \n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\frac{\\exp(s_1(\\hat{\\mathbf A}))}{\\exp(s_1(\\hat{\\mathbf A})) + \\exp(s_2(\\hat{\\mathbf A}))} q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X) \\\\\n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\mbox{Softmax}(s_1(\\hat{\\mathbf A})) q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)  \\\\\n        \\approx &  \\nabla_\\theta \\log \\mbox{NWGM}(\\mbox{Softmax}(s_1(\\hat{\\mathbf A}))) \\\\\n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\frac{\\prod_{\\hat{\\mathbf A}} [\\exp (s_1(\\hat{\\mathbf A}))]^{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)}}{ \\prod_{\\hat{\\mathbf A}}\\exp (s_1(\\hat{\\mathbf A}))]^{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} + \\prod_{\\hat{\\mathbf A}}\\exp (s_2(\\hat{\\mathbf A}))]^{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} } \\\\\n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\frac{\\exp(\\sum_{\\hat{\\mathbf A}} s_1(\\hat{\\mathbf A})q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X))}{ \\exp(\\sum_{\\hat{\\mathbf A}} s_1(\\hat{\\mathbf A})q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)) + \\exp(\\sum_{\\hat{\\mathbf A}} s_2(\\hat{\\mathbf A})q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X))}\\\\\n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\frac{\\exp(\\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [s_1(\\hat{\\mathbf A})] )}{ \\exp(\\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [s_1(\\hat{\\mathbf A})] ) + \\exp(\\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [s_2(\\hat{\\mathbf A})] ) } \\\\\n        = & \\nabla_{\\theta}  \\log p_{w_m} (\\mathbf Y_{u,c} = 1| \\mathbf A, \\mathbf X, \\hat{\\mathbf A} = \\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [\\hat{\\mathbf A}]),\n    \\end{split}\n\\end{equation}\nwhere $s_1$ denotes the positive predicted score for class $c$ which is indeed associated with node $u$, and $s_2(\\hat{\\mathbf A}) = 0$ in our case. We thus conclude the proof for \\eqref{eqn-loss-sup-grad}.\n\n\n\n"
            },
            "section 8": {
                "name": "Datasets and Experimental Details",
                "content": "\\label{appx-implement}\nThe statistical information on datasets is displayed in Table \\ref{tab:dataset_statistic}. For the splitting of Cora, CiteSeer and PubMed, we follow \\cite{planetoid} to randomly select 20 instances per\nclass for training, 500/1000 instances for validation/testing in each dataset. In the remaining datasets, \nwe employ random train/valid/test splits of 50\\%/25\\%/25\\%. \n\nThe backbone GNN network is specified as a two-layer GCN model. \nWe set the similarity function $s$ in \\eqref{eqn-sim} as cosine similarity and $\\delta$ as a threshold-based truncation. \nBesides, since the dimensions of input node features are different across datasets, we adopt a transformation network that converts input features into a $d$-dimensional node representations before the structure learning module as shown in Alg.~\\ref{alg:meta-learning} ($\\mathbf Z^0 = \\mbox{MLP}(\\mathbf X; w_m)$ or $\\mathbf Z^0 = \\mbox{GCN}(\\mathbf A, \\mathbf X; w_m)$). We can specify the transformation as a one-layer MLP or a one-layer GCN network (what we adopt). \n% and in Appendix~\\ref{appx-moreres} we empirically compare these two methods.\nMost of the experiments were conducted on an NVIDIA GeForce RTX 2080 Ti with 11GB memory. For experiments involving two larger datasets, PubMed and Cornell5, we utilized an NVIDIA GeForce RTX 3090 with 24 GB memory.\n\n"
            },
            "section 9": {
                "name": "Hyperparameters",
                "content": "\\label{appx-hyper}\n\\label{append:hyper}\nWe use grid search on validation set to tune the hyperparameters.\n% Unless otherwise stated, the GNN network in our experiment uses no BatchNorm. \nThe learning rate is searched in $\\{0.001, 0.005, 0.01,0.05 \\}$; Dropout is searched in $\\{0, 0.2, 0.3, 0.5, 0.6 \\}$; Hidden channels is searched in $\\{16, 32, 64, 96\\}$.\nOther hyperparameters for specific models are stated below.\n\nFor GCN, GraphSAGE and H$^2$GCN, we use 2 layers. For GAT, we search gat head number in $\\{2, 4 \\}$ and use 2 layers. For APPNP and GPR, we search $\\alpha$ in $\\{0.1, 0.2, 0.5\\}$ and set $K$ to 10. We list the searching space of structure learning methods below.\n\\begin{itemize}\t\n\t\\item\n\t\\mymodel and its variants:\n\tpivot number $P \\in$ \\{800, 1000, 1200, 1400\\}, embedding size $d \\in$ \\{16, 32, 64, 96\\},\n\t%Graph learning similarity metric $\\in$ $\\{S_1,S_2,S_3\\}$.\n\t$\\lambda \\in$ [0.1, 0.9], \n\t%$\\eta \\in$ \\{0.1,0.2,0.3\\},\n\t$\\alpha \\in$ \\{0, 0.1, 0.15, 0.2, 0.25, 0.3\\}, $\\rho \\in $ \\{0, 0.1, 0.15, 0.2, 0.25, 0.3\\}, threshold $\\in $ \\{4e-5, 8.5e-5\\}, $H \\in $ \\{4, 6\\}, $T=10$,  $E\\in \\{1, 2, 3\\}$. \n\n\t\\item \n\tLDS: the sampling time $S=16$, the patience window size $\\rho \\in $\\{10, 20\\}, the hidden size $\\in$ \\{8, 16, 32, 64\\}, \n    %the inner learning rate $\\gamma \\in$ \\{1e-4, 2e-2, 5e-2\\}, \n    the inner learning rate $\\gamma \\in$ \\{1e-4, 1e-3, 1e-2, 1e-1\\}, \n    and the number of updates used to compute the truncated hypergradient $\\tau \\in$ \\{5, 10, 15\\}. \n\t\\item\n\tIDGL: $\\epsilon=0.01$, hidden size $\\in$ \\{16, 64, 96,128\\}, $\\lambda \\in$ \\{0.5, 0.6, 0.7, 0.8\\}, $\\eta \\in$ \\{0, 0.1, 0.2\\}, $\\alpha \\in$ \\{0, 0.1, 0.2\\}, $\\beta \\in$ \\{0, 0.1\\}, $\\gamma \\in$ \\{0.1, 0.2\\}, $m \\in$ \\{6, 9, 12\\}.\n\t\\item\n\tVGCN: $\\bar{\\rho}_1 \\in$ \\{0.25, 0.5, 0.75, 0.99\\}, $\\bar{\\rho}_0=10^{-5}$, $\\tau_0 \\in $ \\{0.1, 0.5\\}, $\\tau \\in$ \\{0.1, 0.5\\}, $\\beta \\in$ \\{$10^{-4}$, $10^{-3}$, $10^{-2}$, 1\\}. Sampling time is 3. Maximum number of training epochs is 5000.\n\\end{itemize}\n\n\n\n\n\n\n\n\n"
            },
            "section 10": {
                "name": "More Experimental Results",
                "content": "\\label{appx-moreres}\n\n% \\begin{table}[tb!]\n% \t\\centering\n% \t\\caption{Impacts of edge deletion on model performance.}\n% \t\\label{tab:edge_del}\n% \t\\begin{tabular}{|c|l|l|l|}\n% \t\t\\hline\n% \t\tDataset & Deletion & GCN~~~~~ & GSTL~~~~ \\\\  \n% \t\t\\hline\n% \t\t\\multirow{6}{*}{Citeseer} & 0 & 70.3 & 73.6 \\\\ \n% \t\t\\cline{2-4} & 10 & 69.4 & 72.4 \\\\\n% \t\t\\cline{2-4} & 20 & 69.6 & 72.7 \\\\\n% \t\t\\cline{2-4} & 30 & 68.7 & 72.5 \\\\\n% \t\t\\cline{2-4} & 40 & 68.2 & 72.3 \\\\\n% \t\t\\cline{2-4} & 50 & 67.2 & 72 \\\\\n% \t\t\\hline\n% \t\t\\multirow{6}{*}{Johns Hopkins55} & 0 & 70.8 & 71.5 \\\\ \n% \t\t\\cline{2-4} & 10 & 68.6 & 70.7 \\\\\n% \t\t\\cline{2-4} & 20 & 67.4 & 71.7 \\\\\n% \t\t\\cline{2-4} & 30 & 68.0 & 71.1 \\\\\n% \t\t\\cline{2-4} & 40 & 67.0 & 71.0 \\\\\n% \t\t\\cline{2-4} & 50 & 67.1 & 70.8 \\\\\n% \t\t\\hline\n% \t\\end{tabular}\n% \\end{table}\n\nWe compare with using MLP and GCN, respectively, as the transformation network before structure learning module and report the results in Table~\\ref{tab:linear}.\nIn summary, these two methods are of equal competence, which suggests that \\mymodel is not sensitive to the transformation network used for converting node features with various dimensions into embeddings with a shared dimension. This also implies that simple neural architectures, e.g. MLP and GCN, could provide enough capacity for extracting the information in input observation, which is leveraged by the shared graph structure learner to discover generalizable patterns in optimal message-passing structure.\n\nWe also provide more results of edge deletion experiments in Fig. \\ref{fig:edge}. We randomly remove 10-50\\% edges of target graphs respectively, and then apply \\mymodel and GCN. The results demonstrate that \\mymodel is more immune to edge deletion. This is due to our structure learner's ability for learning new structures, making it less reliant on initial graph structures and more robust to attack on input edges. \n\n\n\n"
            }
        },
        "tables": {
            "tab-res2": "\\begin{table*}[tb!]\n\t\\caption{Test accuracy (\\%) on target graphs for cross-domain generalizations. For each social network (resp. citation network) as target dataset, we consider citation networks (resp. social networks) as source graphs.}\n\t\\label{tab-res2}\n\t\\begin{tabular}{| c| lccccccc |}\n\t\t\\hline\n\t\t\\textbf{Type} & \\textbf{Method }& \\textbf{Cornell5} & \\textbf{Johns.55} & \\textbf{Amherst41} & \\textbf{Reed98} & \\textbf{Cora} & \\textbf{CiteSeer} & \\textbf{PubMed} \\\\\n\t\t\\hline\n\t\t\\multirow{7}{*}{\\specialcell[t]{\\textbf{Pure}\\\\\\textbf{GNN}} }& GCN & 68.6  0.5 & 70.8  1.0 & 65.8  1.6 & 60.8  1.6 & 81.6  0.4 & 71.6  0.3 & 78.8  0.6 \\\\\n\t\t& SAGE & 68.7  0.8 & 67.5  0.9 & 66.3  1.8 & 63.9  1.9 & 81.4  0.6 & 71.6  0.5 & 78.6  0.7 \\\\\n\t\t& GAT & 69.6  1.2 & 69.4  0.7 & 68.7  2.1 & 64.5  2.5 & 83.0  0.7 & 72.1  1.1 & 79.0  0.4 \\\\\n\t\t& GPR & 68.8  0.7 & 69.6  1.3 & 66.2  1.5 & 62.7  2.0 & 83.1  0.7 & 72.4  0.8 & 79.6  0.5 \\\\\n\t\t& APPNP & 68.5  0.8 & 69.1  1.4 & 65.9  1.3 & 62.3  1.5 & 82.7  0.5 & 71.9  0.5 & 79.2  0.3 \\\\\n\t\t& H$_2$GCN & 71.4  0.5 & 68.3  1.0 & 66.5  2.2 & 65.4  1.3 & 82.5  0.8 & 71.4  0.7 & 79.4  0.4 \\\\\n\t\t& CPGNN & 71.1  0.5 & 68.7  1.3 & 66.7  0.8 & 63.6  1.8 & 80.8  0.4 & 71.6  0.4 & 78.5  0.7 \\\\\n\t\t\\hline\n\t\t\\multirow{6}{*}{\\specialcell[t]{\\textbf{Graph}\\\\\\textbf{Structure}\\\\\\textbf{Learning}}} & $\\mathrm{\\mymodel_{dp}}$ & 71.5  0.7 & 71.3  1.2 & 68.5  1.6 & 63.2  1.2 & 83.1  0.8 & 71.7  1.0 & 77.3  0.8 \\\\\n\t\t& $\\mathrm{\\mymodel_{knn}}$ & 69.4  0.8 & 71.0  1.3 & 64.8  1.2 & 63.6  1.6 & 81.7  0.8 & 71.5  0.8 & 79.4  0.6 \\\\\n\t\t& $\\mathrm{\\mymodel_{cos}}$ & 69.9  0.7 & 70.8  1.4 & 65.2  1.8 & 62.7  1.3 & 82.0  0.7 & 71.9  0.9 & 78.7  0.8 \\\\\n\t\t& $\\mathrm{\\mymodel_{at}}$ & 69.9  1.0 & 70.4  1.5 & 64.4  1.2 & 65.0  1.7 & 82.5  0.9 & 71.8  0.8 & 78.5  0.7 \\\\\n\t\t& \\mymodel & \\cellcolor{lightgrey}\\textbf{72.0  1.0} & 71.8  0.7 & 69.8  1.3 & \\cellcolor{lightgrey}\\textbf{67.3  1.2} & 83.2  0.4 & 73.8  0.9 & 79.6  0.7 \\\\\n\t\t& \\mymodelvar & 71.1  0.3 & \\cellcolor{lightgrey}\\textbf{72.2  0.5} & \\cellcolor{lightgrey}\\textbf{70.3  0.9} & 66.8  1.4 & \\cellcolor{lightgrey}\\textbf{83.5  0.6} & \\cellcolor{lightgrey}\\textbf{73.9  0.7} & \\cellcolor{lightgrey}\\textbf{79.9  0.5} \\\\\n\t\t\\hline\n\t\\end{tabular}\t\n\\end{table*}",
            "tab:dataset_statistic": "\\begin{table}[tb!]\n\t\\caption{Statistic information on experimental datasets. The column \\textit{Homo.} reports the homophily ratios, measured by the metric proposed in \\cite{lim2021new}.}\n\t\\label{tab:dataset_statistic}\n\t\\begin{center}\n\t\t\\small\n\t\t\\begin{tabular}{lcccc} \n\t\t\t\\toprule % <-- Toprule here\n\t\t\t\\textbf{Dataset}&\n\t\t\t\\textbf{Type}&\n\t\t\t\n\t\t\t\\textbf{\\# Node }&\n\t\t\t\\textbf{\\# Edge }&\n\t\t\t\\textbf{Homo.}\\\\\n\t\t\t\\midrule % <-- Midrule here\n\t\t\tCora & citation & \n\t\t\t2,708 & 5,429   & 0.77 \\\\ \n\t\t\tCiteSeer & citation &\n\t\t\t3,327 & 4,732  &  0.63 \\\\\n\t\t\tPubMed& citation &\n\t\t\t\n\t\t\t19,717 & 44,338   &  0.66 \\\\\n\t\t\tAmherst41& social & \n\t\t\t2,235 & 90,964   & 0.06  \\\\\n\t\t\tCornell5& social & \n\t\t\t18,660 & 790,777   & 0.09  \\\\\n\t\t\tJohns Hopkins55& social  &\n\t\t\t5,180 & 186,586   & 0.10  \\\\\n% \t\t\tPenn94& PN  &\n% \t\t\t41,554 & 1,362,229   & 0.05  \\\\\n\t\t\tReed98& social  &\n\t\t\t962 & 18,812   & 0.04 \\\\\n\t\t\t\\bottomrule % <-- Bottomrule here\n\t\t\\end{tabular}\n\t\\end{center}\n\\end{table}"
        },
        "figures": {
            "fig:glow": "\\begin{figure}[tb!]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figs/data_flow.pdf}\n  \\caption{Illustration of Open-World Graph Structure Learning. In a diverse set of source graphs, we train multiple dataset-specific GNNs and a shared structure learner. In the target graph, we directly utilize the learned structure learner and only need to train a new GNN.}\n  \\label{fig:glow}\n\\end{figure}",
            "fig:precis": "\\begin{figure*}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.86\\linewidth]{figs/frame.pdf}\n\t\t\\caption{Illustration of the proposed framework \\mymodel targeting open-world graph structure learning. The middle part of the figure presents the training process for the structure learner together with multiple dataset-specific GNNs on source graphs. In (a)-(e) we illustrate the details of graph structure learner, backbone GNN, iterative training process, training procedure and transferring procedure.\n\t\tWhen the training is finished, the structure learner is fixed and we only need to train a dataset-specific GNN network on new target graph with latent structures inferred by the well-trained structure learner.\n\t\t} \n\t\t\\label{fig:precis}\n\t\\end{figure*}",
            "fig-res": "\\begin{figure*}[t]\n\\subfigure[Cora]{\n\\includegraphics[width=0.19\\textwidth]{figs/efficiency_cora.pdf}}\n\\subfigure[CiteSeer]{\n\\includegraphics[width=0.19\\textwidth]{figs/efficiency_citeseer.pdf}}\n\\subfigure[Amherst41]{\n\\includegraphics[width=0.19\\textwidth]{figs/efficiency_amherst41.pdf}}\n\\subfigure[Johns Hopkins55]{\n\\includegraphics[width=0.187\\textwidth]{figs/efficiency_johns.pdf}}\n\\subfigure[Reed98]{\n\\includegraphics[width=0.19\\textwidth]{figs/efficiency_reed.pdf}}\n\\caption{Comparison of test accuracy and training time with SOTA structure learning models (LDS~\\cite{lds}, IDGL~\\cite{idgl} and VGCN~\\cite{variational_inference_2}). The radius of circle is proportional to standard deviation. The experiments are run on one Tesla V4 with 16 GPU memory. We adopt the same setting as Table~\\ref{tab-res2} and report the results on target datasets. For \\textsc{Cornell5} and \\textsc{PubMed}, the competitor models suffer out-of-memory.}\n\\label{fig-res}\n\\end{figure*}",
            "fig": "\\begin{figure}[tb!]\n\t\\centering\n\t\\subfigure[]{\n\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=0.9\\textwidth,angle=0]{figs/homo.pdf}\n\t\t\t\\label{fig:homo}\n\t\t\\end{minipage}%\n\t}%\n\t\\subfigure[]{\n\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=0.9\\textwidth,angle=0]{figs/variance.pdf}\n\t\t\t\\label{fig:variance}\n\t\t\\end{minipage}%\n\t}%\n\t\\caption{(a) The curves of homophily ratios for latent structures during the learning process. (b)  The variance of neighborhood distribution of nodes with the same label in original graphs and learnt structure. }\n\t\\label{fig}\n\\end{figure}",
            "fig-para": "\\begin{figure}[tb!]\n\\centering\n\\subfigure[$\\lambda$]{\n\\begin{minipage}[t]{0.45\\linewidth}\n\\centering\n\\includegraphics[width=0.99\\textwidth,angle=0]{figs/param1.pdf}\n\\label{fig-exp-abl-d}\n\\end{minipage}%\n}%\n\\subfigure[$P$]{\n\\begin{minipage}[t]{0.45\\linewidth}\n\\centering\n\\includegraphics[width=0.99\\textwidth,angle=0]{figs/param2.pdf}\n\\label{fig-exp-abl-D}\n\\end{minipage}%\n}%\n\\caption{Hyper-parameter sensitivity analysis on the weight of input graphs $\\lambda$ and pivot number $P$.}\n\\label{fig-para}\n\\end{figure}",
            "fig:edge": "\\begin{figure}[t]\n\t\\centering\n\t\\subfigure[CiteSeer]{\n\t\t\\includegraphics[width=0.23\\textwidth]{figs/e_citeseer.pdf}}\n\t\\subfigure[Johns Hopkins55]{\n\t\t\\includegraphics[width=0.23\\textwidth]{figs/e_jh.pdf}}\n\t\\caption{Performance comparison of \\mymodel and GCN w.r.t. randomly removing certain ratios of edges in input graphs.}\n\t\\label{fig:edge}\n\\end{figure}"
        },
        "equations": {
            "eq:eqn-standard": "\\begin{equation}\\label{eqn-standard}\n\t\\theta^* = \\argmin_{w}  \\min_{\\theta}   \\mathcal L \\left (h_{w}(g_\\theta(\\mathbf A, \\mathbf X), \\mathbf X),\\mathbf Y \\right ).\n\\end{equation}",
            "eq:eqn-obj": "\\begin{equation}\\label{eqn-obj}\n\t\\theta^* = \\argmin_{\\theta}  \\min_{w_1, \\cdots, w_M} \\sum_{m=1}^M  \\mathcal L \\left (h_{w_m}(g_\\theta(\\mathbf A^s_m, \\mathbf X^s_m), \\mathbf X^s_m),\\mathbf Y^s_m \\right ),\n\\end{equation}",
            "eq:1": "\\begin{equation}\n\tw^* = \\argmin_{w} \\mathcal L \\left (h_{w}(g_{\\theta^*}(\\mathbf A^t, \\mathbf X^t), \\mathbf X^t),\\mathbf Y^t \\right ).\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X}) = \\frac{ p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   ) p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   )}{\\int_{\\hat{\\mathbf{A} }} p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   ) p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) d \\hat{\\mathbf{A} }}.\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\t\\begin{split}\n\t\t& \\mathcal D_{KL}(q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) \\| p(\\hat{\\mathbf{A} } | \\mathbf Y, \\mathbf{A}, \\mathbf{X})) \\\\\n\t\t= & - \\underbrace{\\mathbb{E}_{\\hat{\\mathbf{A} } \\sim q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   )} \\left [\\log  \\frac{ p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   )  p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) }{ q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) } \\right ] }_{\\mbox{Evidence Lower Bound}} + \\log p(\\mathbf Y|\\mathbf A, \\mathbf X).\n\t\\end{split}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\log p(\\mathbf Y|\\mathbf A, \\mathbf X) \\\\\n    \\geq  \\mathbb{E}_{\\hat{\\mathbf{A} } \\sim q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   )} \\left [\\log  \\frac{ p(\\mathbf{Y} | \\mathbf{A}, \\mathbf{X}, \\hat{\\mathbf{A} }   )  p(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) }{ q(\\hat{\\mathbf{A} } | \\mathbf{A}, \\mathbf{X}   ) } \\right ].\n\\end{equation}",
            "eq:eqn-elbo": "\\begin{equation}\\label{eqn-elbo}\n\\begin{split}\n    \\mathbb E_{\\mathcal G_m \\sim p(\\mathcal G)} \\left [ \n    \\mathbb{E}_{\\hat{\\mathbf{A} } \\sim q_\\theta(\\hat{\\mathbf{A} } | \\mathbf{A} = \\mathbf A_m, \\mathbf{X} = \\mathbf X_m   )} \\left [\\log p_{w_m }(\\mathbf{Y} | \\mathbf{A} = \\mathbf A_m, \\mathbf{X} = \\mathbf X_m, \\hat{\\mathbf{A} }  ) \\right. \\right. \\\\\n    \\left. \\left. + \\log p_{0}(\\hat{\\mathbf{A} } | \\mathbf{A} = \\mathbf A_m, \\mathbf{X} = \\mathbf X_m   ) - \\log q_\\theta(\\hat{\\mathbf{A} } | \\mathbf{A} = \\mathbf A_m, \\mathbf{X} = \\mathbf X_m   )  \\right ]\n    \\right ].\n\\end{split}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\mathbb{E}_{\\hat{\\mathbf{A} } \\sim q_{\\theta^*}(\\hat{\\mathbf{A} } | \\mathbf{A} = \\mathbf A^t, \\mathbf{X} = \\mathbf X^t   )} \\left [\\log p_{w}(\\mathbf{Y} | \\mathbf{A} = \\mathbf A^t, \\mathbf{X} = \\mathbf X^t, \\hat{\\mathbf{A} }   ) \\right ].\n\\end{equation}",
            "eq:eqn-sim": "\\begin{equation}\\label{eqn-sim}\n    \\alpha_{uv}= \\delta\\left (\\frac{1}{H}\\sum_{h=1}^H s(\\mathbf {w}_{h}^1 \\odot \\mathbf {z}_u, \\mathbf {w}_{h}^2 \\odot \\mathbf {z}_v) \\right ),\n\\end{equation}",
            "eq:eqn-mp-gcn": "\\begin{equation}\\label{eqn-mp-gcn}\n\t\\mathbf Z^{(l+1)} = \\sigma \\left (\\operatorname{MP}_1(\\mathbf{Z}^{(l)},\\mathbf{A}) \\mathbf W^{(l)} \\right )= \\sigma \\left( \\mathbf D^{-\\frac{1}{2}} \\mathbf A \\mathbf D^{-\\frac{1}{2}} \\mathbf{Z}^{(l)} \\mathbf W^{(l)} \\right ), \n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\begin{split}\n        & \\mbox{i) node-to-pivot passing:} ~~~ \\mathbf C^{(l+\\frac{1}{2})} = \\mathrm{RowNorm} (\\mathbf \\Gamma^\\top) \\mathbf Z^{(l)}, \\\\\n        & \\mbox{ii) pivot-to-node passing:} ~~~ \\mathbf C^{(l+1)} = \\mathrm{RowNorm} ( \\mathbf \\Gamma) \\mathbf C^{(l+\\frac{1}{2})},\n    \\end{split}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mathbf Z^{(l+1)} = \\sigma \\left( \\lambda \\mbox{MP}_1(\\mathbf Z^{(l)}, \\mathbf A) \\mathbf W^{(l)} + (1 - \\lambda) \\mbox{MP}_2(\\mathbf Z^{(l)}, \\hat{\\mathbf A}) \\mathbf W^{(l)} \\right ),\n\\end{equation}",
            "eq:eqn-reg": "\\begin{equation}\\label{eqn-reg}\n    p_0(\\hat{\\mathbf A}| \\mathbf X, \\mathbf A) \\propto \\exp{\\left( -\\alpha \\sum_{u, v} \\hat{\\mathbf A}_{uv} \\| \\mathbf x_u - \\mathbf x_v \\|_2^2 \n    %+ \\beta \\mathbf 1^\\top \\log (\\hat{\\mathbf A} \\mathbf 1) \n    - \\rho \\|\\hat{\\mathbf A}\\|_F^2 \\right ) },\n\\end{equation}",
            "eq:eqn-reg1": "\\begin{equation}\\label{eqn-reg1}\n\\begin{split}\n    \\mathcal R (\\hat{\\mathbf E}) &= \\log p_0(\\hat{\\mathbf A}| \\mathbf X, \\mathbf A) \\\\\n    & \\approx -\\alpha \\sum_{p, q} \\hat{\\mathbf E}_{pq} \\| \\mathbf x'_p - \\mathbf x'_q \\|_2^2 - \\rho \\|\\hat{\\mathbf E}\\|_F^2,\n\\end{split}\n\\end{equation}",
            "eq:eqn-loss-sup-grad": "\\begin{equation}\\label{eqn-loss-sup-grad}\n    \\begin{split}\n        & \\nabla_{\\theta} \\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [ \\log p_{w_m} (\\mathbf Y| \\mathbf A, \\mathbf X, \\hat{\\mathbf A})] \\\\ \n        \\approx & \\nabla_{\\theta}  \\log p_{w_m} (\\mathbf Y| \\mathbf A, \\mathbf X, \\hat{\\mathbf A} = \\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [\\hat{\\mathbf A}]).\n    \\end{split}\n\\end{equation}",
            "eq:eqn-pi": "\\begin{equation}\\label{eqn-pi}\n\\pi_\\theta(\\hat{\\mathbf E}) = \\prod_{u, p} \\left( \\hat{\\mathbf B}_{1,up}  \\alpha_{up} + (1 - \\hat{\\mathbf B}_{1,up}) \\cdot(1 - \\alpha_{up})    \\right).\n\\end{equation}",
            "eq:eqn-grad-reg": "\\begin{equation}\\label{eqn-grad-reg}\n\\begin{split}\n    \\nabla_{\\theta} \\mathcal L_r(\\theta) &= -\\nabla_{\\theta} \\mathbb E_{\\hat{\\mathbf{A}}\\sim q(\\hat{\\mathbf{A}}  |  \\mathbf{X},\\mathbf{A} )} [\\log p_0(\\hat{\\mathbf{A}}  |  \\mathbf{X},\\mathbf{A})] \\\\\n    &\\approx -\\nabla_{\\theta} \\frac{1}{K} \\sum_{k=1}^K \\log \\pi_\\theta(\\hat {\\mathbf E}_k) \\left(\\mathcal R(\\hat{\\mathbf E}_k) - \\overline{\\mathcal R}\\right),\n\\end{split}\n\\end{equation}",
            "eq:eqn-loss-ent": "\\begin{equation}\\label{eqn-loss-ent}\n\\begin{split}\n    \\mathcal L_{e} (\\theta) &= \\mathbb E_{\\hat{\\mathbf{A}}\\sim q(\\hat{\\mathbf{A}}  |  \\mathbf{X},\\mathbf{A} )} [\\log q(\\hat{\\mathbf{A}}  |  \\mathbf{X},\\mathbf{A} )] \\\\\n    &\\approx\n\t\t\\frac{1}{NP} \\sum_{u=1}^{N} \\sum_{p=1}^{P} \\left[ \\alpha_{up}\\log \\alpha_{up}  + (1- \\alpha_{up}  ) \\log (1-\\alpha_{up} )  \\right],\n\\end{split}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\begin{split}\n        & \\nabla_\\theta   \\log \\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [ p_{w_m}(\\mathbf Y_{u, c} = 1| \\mathbf A, \\mathbf X, \\hat{\\mathbf A})] \\\\ \n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\frac{\\exp(s_1(\\hat{\\mathbf A}))}{\\exp(s_1(\\hat{\\mathbf A})) + \\exp(s_2(\\hat{\\mathbf A}))} q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X) \\\\\n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\mbox{Softmax}(s_1(\\hat{\\mathbf A})) q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)  \\\\\n        \\approx &  \\nabla_\\theta \\log \\mbox{NWGM}(\\mbox{Softmax}(s_1(\\hat{\\mathbf A}))) \\\\\n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\frac{\\prod_{\\hat{\\mathbf A}} [\\exp (s_1(\\hat{\\mathbf A}))]^{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)}}{ \\prod_{\\hat{\\mathbf A}}\\exp (s_1(\\hat{\\mathbf A}))]^{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} + \\prod_{\\hat{\\mathbf A}}\\exp (s_2(\\hat{\\mathbf A}))]^{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} } \\\\\n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\frac{\\exp(\\sum_{\\hat{\\mathbf A}} s_1(\\hat{\\mathbf A})q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X))}{ \\exp(\\sum_{\\hat{\\mathbf A}} s_1(\\hat{\\mathbf A})q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)) + \\exp(\\sum_{\\hat{\\mathbf A}} s_2(\\hat{\\mathbf A})q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X))}\\\\\n        = & \\nabla_\\theta \\log \\sum_{\\hat{\\mathbf A}} \\frac{\\exp(\\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [s_1(\\hat{\\mathbf A})] )}{ \\exp(\\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [s_1(\\hat{\\mathbf A})] ) + \\exp(\\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [s_2(\\hat{\\mathbf A})] ) } \\\\\n        = & \\nabla_{\\theta}  \\log p_{w_m} (\\mathbf Y_{u,c} = 1| \\mathbf A, \\mathbf X, \\hat{\\mathbf A} = \\mathbb E_{q_\\theta(\\hat{\\mathbf A}|\\mathbf A, \\mathbf X)} [\\hat{\\mathbf A}]),\n    \\end{split}\n\\end{equation}"
        },
        "git_link": "https://github.com/WtaoZhao/GraphGLOW"
    }
}