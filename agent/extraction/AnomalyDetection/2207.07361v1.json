{
    "meta_info": {
        "title": "Registration based Few-Shot Anomaly Detection",
        "abstract": "This paper considers few-shot anomaly detection (FSAD), a practical yet\nunder-studied setting for anomaly detection (AD), where only a limited number\nof normal images are provided for each category at training. So far, existing\nFSAD studies follow the one-model-per-category learning paradigm used for\nstandard AD, and the inter-category commonality has not been explored. Inspired\nby how humans detect anomalies, i.e., comparing an image in question to normal\nimages, we here leverage registration, an image alignment task that is\ninherently generalizable across categories, as the proxy task, to train a\ncategory-agnostic anomaly detection model. During testing, the anomalies are\nidentified by comparing the registered features of the test image and its\ncorresponding support (normal) images. As far as we know, this is the first\nFSAD method that trains a single generalizable model and requires no\nre-training or parameter fine-tuning for new categories. Experimental results\nhave shown that the proposed method outperforms the state-of-the-art FSAD\nmethods by 3%-8% in AUC on the MVTec and MPDD benchmarks.",
        "author": "Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, Yan-Feng Wang",
        "link": "http://arxiv.org/abs/2207.07361v1",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "ECCV 2022 Oral; Code is available at  https://github.com/MediaBrain-SJTU/RegAD"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nAnomaly detection (AD), with a wide range of applications such as defect detection~\\cite{matsubara2018anomaly}, medical diagnosis~\\cite{zhang2020viral}, and autonomous driving~\\cite{eykholt2018robust}, has received quite some attention in the computer vision community over the last decades. With the ambiguous definition of ``anomaly'', \\emph{i.e.,} samples that do not conform to the ``normal'', it is impossible to train with an exhaustive set of anomalous samples. As a result, recent studies on anomaly detection have largely been devoted to unsupervised learning, \\emph{i.e.,} learning with only the ``normal'' samples. Through modeling the normal distribution with one-class classification~\\cite{scholkopf2001estimating,ruff2018deep,yi2020patch}, reconstruction~\\cite{zong2018deep,gong2019memorizing,metaformer,huang2022esad}, or self-supervised learning tasks~\\cite{golan2018deep,ARNet,MKD,focus}, many AD methods detect anomalies by identifying samples with different distributions than the model.\n\nMost existing AD methods have focused on training a dedicated model for each category (Fig.~\\ref{img:intro} (a)). However, in real-world scenarios such as defect detection, given hundreds of industrial products to handle, it is not cost-effective to collect a large training set for each product, not to mention the need for many time-sensitive applications. A couple of studies~\\cite{TDG,DiffNet} have recently explored a special, yet practical, setting of AD, \\emph{i.e.,} few-shot anomaly detection (FSAD), where only a limited number of normal images are provided for each category at training (Fig.~\\ref{img:intro} (b)). The few-shot learning of anomaly detection has been approached with strategies to reduce the demand on training samples, such as radical data augmentation with multiple transformations~\\cite{TDG} or a lighter estimator for the normal distribution estimation~\\cite{DiffNet}. \\emph{However, such approaches still follow the one-model-per-category learning paradigm and fail to leverage the inter-category commonality.}\n\nThis paper aims to explore a new paradigm for FSAD, by learning a common model shared among multiple categories and also generalizable to novel categories, and inspired by how human beings detect anomalies. In fact, when a human is asked to search for the anomaly in an image, a simple strategy one may adopt is to compare the sample to a normal one to find the difference. As long as one knows how to compare two images, the actual semantics of the images does not matter anymore. To achieve such a human-like comparison process, we resort to registration, a process of transforming different images into one coordinate system in order to better enable comparison~\\cite{brown1992survey,Barbara2003Image,peng2011brainaligner}. Registration is particularly suitable for FSAD, as \\emph{registration is expected to be category-agnostic and thus generalizable across categories, allowing the model to be adaptable to novel categories without the necessity of parameter fine-tuning.}\n\nFig.~\\ref{img:intro} (c) provides an overview of the proposed \\underline{Reg}istration based few-shot \\underline{A}nomaly \\underline{D}etection (RegAD) framework. To train a category-agnostic anomaly detection model, we leverage registration, a task that is inherently generalizable across categories, as the proxy task. A Siamese network~\\cite{chen2021exploring} with three spatial transformer network~\\cite{STN} blocks is employed as the registration network (see Fig.~\\ref{img:RegAD}). For better robustness, instead of registering the images pixel-by-pixel as typical registration methods~\\cite{peng2011brainaligner}, here we propose a feature-level registration loss by maximizing the cosine similarity of features from the same category, which may be deemed as a relaxed version of the pixel-wise registration loss. Normal images from different categories are used together to aggregately train the model, with two images from the same category randomly selected as a training pair. Such aggregated training procedure is adopted so as to enable the trained registration model to be category-agnostic. At test time, a support set of a few normal samples is provided for the target category, together with each test sample. It is straightforward to identify anomalies by comparing the registered features of the test image and the corresponding support (normal) images. Given the support set, the normal distribution of registered features for the target category is estimated with a statistical-based distribution estimator~\\cite{defard2021padim}. Test samples that are out of the statistical normal distribution are considered anomalies. In this way, the model quickly adapts to novel categories by simply estimating its normal feature distribution without any parameter fine-tuning.\n\nTo validate the effectiveness of RegAD, we experiment with two challenging benchmark datasets for industrial defect detection, MVTec AD~\\cite{bergmann2019mvtec} and MPDD~\\cite{jezek2021deep}. Our experimental results have shown that RegAD outperforms the state-of-the-art FSAD methods~\\cite{TDG,DiffNet}, achieving improvements of 5.1\\%, 6.9\\%, and 8.0\\% in AUC on MVTec, and improvements of 3.2\\%, 5.0\\%, and 3.4\\% in AUC on MPDD, for 2-shot, 4-shot, and 8-shot scenarios, respectively.\n\nThe main contributions of the paper are summarised as follows:\n\\begin{itemize}\n  \\item We introduce feature registration as a category-agnostic approach for few-shot anomaly detection (FSAD). To our best of knowledge, it is the first FSAD method that trains a single generalizable model and requires no re-training or parameter fine-tuning for new categories.\n  \\item Extensive experiments on recent benchmark datasets have shown that the proposed RegAD outperforms the state-of-the-art FSAD methods on both the anomaly detection and anomaly localization tasks.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n",
                "subsection 2.1": {
                    "name": "Anomaly Detection",
                    "content": " \nAD is a task where training datasets contain only normal data. To better estimate the normal distributions, one-class classification based approaches tend to depict the normal data directly with statistical approaches~\\cite{Eskin2000Anomaly,scholkopf2001estimating,Rahmani2017Coherence,ruff2018deep}. Self-supervised based approaches are trained using only normal data, and then make inferences by assuming that anomalous data performs differently. In this domain, reconstruction~\\cite{xia2015learning,schlegl2017unsupervised,zong2018deep,Sabokrou2018Adversarially,ganomaly,gong2019memorizing,metaformer,huang2022ssm} is the most popular self-supervision. Some approaches~\\cite{golan2018deep,ARNet,MKD} introduce other self-supervisions, \\emph{e.g.}, \\cite{golan2018deep} applies dozens of image geometric transforms for transformation classification; \\cite{ARNet} proposes a restoration framework for attribute restoration. Recent AD methods usually use feature embeddings extracted from a pre-trained deep neural network. Feature embedding is mostly used as an input for a traditional machine learning algorithm or statistical metrics such as the Mahalanobis distance~\\cite{defard2021padim}. The network used as a feature extractor can be trained from scratch~\\cite{yi2020patch}, while several methods~\\cite{cutpaste,defard2021padim,focus,patchcore,cflow} have also achieved state-of-the-art results using models pre-trained on the ImageNet dataset~\\cite{imagenet}. This paper differs from these previous works by focusing on FSAD, where only a few normal images are available.\n\n"
                },
                "subsection 2.2": {
                    "name": "Few-shot Learning",
                    "content": "\nFew-shot learning (FSL) aims to adapt to novel classes with a few annotated examples. Representative FSL methods can be categorized into metric learning, generation, and optimization. Metric learning approaches~\\cite{snell2017prototypical,sung2018learning,he2021revisiting} learn to calculate a feature space that classifies an unseen sample based on its nearest example category. Generation methods \\cite{liu2020deep,yang2021free,chen2019multi} enhance the novel class performance by generating its images or features. Optimization methods \\cite{Ravi2017OptimizationAA,finn2017model} learn commonalities among different categories and explore efficient optimization strategies for novel classes based on these commonalities. In this paper, the proposed method predicts `normal' or `anomaly' for a new category. In contrast to previous work on FSL, both training data and support set only have positive (normal) examples without any negative (anomaly) samples. \n\n"
                },
                "subsection 2.3": {
                    "name": "Few-shot Anomaly Detection",
                    "content": "\nFSAD aims to indicate anomalies with only a few normal samples as the support images for target categories. TDG~\\cite{TDG} proposes a hierarchical generative model that captures the multi-scale patch distribution of each support image. They use multiple image transformations and optimize discriminators to distinguish between real and fake patches, as well as between different transformations applied to the patches. The anomaly score was obtained by aggregating the patch-based votes of the correct transformations.\nDiffNet~\\cite{DiffNet} leverages the descriptiveness of features extracted by convolutional neural networks to estimate their density using a normalizing flow, which is a tool well-suited to estimate distributions from a few support samples. Metaformer~\\cite{metaformer} can be applied to the FSAD, although an additional large-scale dataset, MSRA10K~\\cite{msra10k}, should be used during its entire meta-training procedure (beyond parameter pre-training), together with additional pixel-level annotations. In this paper, we design registration based FSAD to learn the category-agnostic feature registration, enabling the model to detect anomalies in new categories given a few normal images without fine-tuning. \n\n\n\n"
                }
            },
            "section 3": {
                "name": "Problem Setting",
                "content": "\n\\label{sec:PS}\nWe first formally define the problem setting for the proposed few-shot anomaly detection. Given a training set consisting of only normal samples of $n$ categories, \\emph{i.e.,} $\\mathcal{T}_{train}=\\bigcup_{i=1}^{n}\\mathcal{T}_{i}$, where the subset $\\mathcal{T}_i$ consists of normal samples from the category $c_i$, ($i=1,2,\\cdots,n$), we want to train a category-agnostic anomaly detection model. At test time, given a normal or anomalous image from a target category $c_t\\ (t \\notin \\{1,2,\\cdots,n\\})$ and its associated support set $\\mathcal{S}_t$ consisting of $k$ normal samples from the target category $c_t$, the trained category-agnostic anomaly detection model should predict whether the image is anomalous or not.\n\nFor FSAD, we attempt to detect anomalies from test samples of unseen/novel categories using only a few normal images as the support set. The key challenges lie in: (i) $\\mathcal{T}_{train}$ has only access to normal samples from multiple known categories (\\emph{e.g.}, different objects or textures), without any image-level or pixel-level annotations, (ii) the test data is from an unseen/novel category, and (iii) only a few normal samples from the target category $c_t$ are available, making it hard to estimate the normal distribution of the target category $c_t$.\n\n"
            },
            "section 4": {
                "name": "Method",
                "content": "\\label{sec:method}\nMotivated by how humans detect anomalies, the feature registration is used as a generalization paradigm for FSAD. During the training procedure, we leverage an anomaly-free feature registration network to learn category-agnostic feature registration. During testing, given the support set of a few normal images, the normal distribution of registered features for the target category is estimated with a statistical-based distribution estimator. Test samples that are out of the learned statistical normal distribution are considered anomalies.\n\n",
                "subsection 4.1": {
                    "name": "Feature Registration Network",
                    "content": "\nGiven a pair of images $I_a$ and $I_b$ randomly selected from a same category in the training set $\\mathcal{T}_{train}$, a ResNet-type convolutional network~\\cite{he2016deep} is leveraged as the feature extractor. Specifically, as shown in Fig.~\\ref{img:RegAD},  the first three convolutional residual blocks of ResNet, $C_1$, $C_2$, and $C_3$, are adopted, and the last convolution block in ResNet's original design is discarded, in order to ensure that final features still retain spatial information. A spatial transformer network (STN)~\\cite{STN} is inserted into each block as a feature transformation module, so as to enable the model to learn feature registration flexibly, inspired by~\\cite{focus}. Specifically, a transformation function $S_i$ ($i=1,2,3$) is applied on an input feature $f_{i}^s$:\n\\begin{equation}\n\\begin{pmatrix}\nx_i^t \\\\ y_i^t\n\\end{pmatrix}\n= S_i(f_{i}^s)\n=A_i\\begin{pmatrix}\nx_{i}^s \\\\ y_{i}^s \\\\ 1\n\\end{pmatrix}\n=\\begin{bmatrix}\n\\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n\\theta_{21} & \\theta_{22} & \\theta_{23} \\\\\n\\end{bmatrix} \n\\begin{pmatrix}\nx_{i}^s \\\\ y_{i}^s \\\\ 1\n\\end{pmatrix},\n\\end{equation}\nwhere $(x_i^t,y_i^t)$ are the target coordinates of output feature $f_i^t$, $(x_i^s,y_i^s)$ are the same points in the source coordinates of input feature $f_i^s$ and $A_i$ is the affine transformation matrix. The module $S_i$ is used to learn the mappings from features of convolutional block $C_i$ with the same tiny architecture as used in~\\cite{STN}. \n\nGiven paired extracted features $f_{3,a}^t$ and $f_{3,b}^t$ as the final transformation outputs, we design the feature encoder as a Siamese network~\\cite{bromley1993signature}. A Siamese network is a parameter-sharing neural network applied on multiple inputs. To avoid the collapsing problem when optimized without negative pairs, inspired by SimSiam~\\cite{chen2021exploring}, features are processed by the same encoder network $E$ followed by a prediction head $P$ applied on one branch. A stop-gradient operation is applied on the other branch, as shown in Fig.~\\ref{img:RegAD}, which is critical to prevent such collapsing solutions. Denote $p_a\\triangleq P(E(f_{3,a}))$ and $z_b\\triangleq E(f_{3,b})$, a negative cosine similarity loss is applied:\n\\begin{equation}\n    \\mathcal{D}(p_a,z_b)=-\\frac{p_a}{||p_a||_2}\\cdot \\frac{z_b}{||z_b||_2},\n\\end{equation}\nwhere $||\\cdot||_2$ is a $L_2$ norm. \nInstead of registering the images pixel-by-pixel, here we use a feature-level registration loss which may be deemed as a relaxed version of the pixel-wise registration constraints for better robustness. Finally, following SimSiam~\\cite{chen2021exploring}, a symmetrized feature registration loss is defined as:\n\\begin{equation}\\label{eq:loss}\n    \\mathcal{L}=\\frac{1}{2}(\\mathcal{D}(p_a,z_b)+\\mathcal{D}(p_b,z_a)).\n\\end{equation}\n\n\\textbf{Discussion.}  \nFeatures from the proposed method retain relatively complete spatial information, since we adopt the first three convolutional blocks of ResNet as the backbone without global average pooling, followed by a convolutional encoder and predictor architecture, but not the MLP architecture in SimSiam~\\cite{chen2021exploring}. Thus Eq.~\\eqref{eq:loss} should be computed by averaging cosine similarity scores at every spatial pixel. Features containing spatial information are beneficial for the AD task, which needs to provide anomaly score maps as prediction results. Different from SimSiam~\\cite{chen2021exploring}, which defines the inputs as two augmentations of one image and maximizes their similarity to enhance the model representation, the proposed feature registration leverages two different images as inputs and maximizes the similarity between the features to learn the registration. \n\n"
                },
                "subsection 4.2": {
                    "name": "Normal Distribution Estimation",
                    "content": "\nTo perform testing, it is assumed that the feature registration ability can generalize to the target category, and the learned feature registration model is applied to the support set $\\mathcal{S}_t$ for the target category without parameter fine-tuning. Multiple data augmentations are applied to the support images, consistent with~\\cite{TDG}. As the two branches of the Siamese network are exactly the same, only one branch feature is used for the normal distribution estimation. After achieving the registered features, a statistical-based estimator~\\cite{defard2021padim} is used to estimate the normal distribution of target category features, which uses multivariate Gaussian distributions to get a probabilistic representation of the normal class. Suppose an image is divided into a grid of $(i,j)\\in [1,W]\\times [1,H]$ positions where $W\\times H$ is the resolution of features used to estimate the normal distribution. At each patch position $(i,j)$, let $F_{ij} = \\{ f_{ij}^k,k\\in [1,N]\\}$ be the registered features from $N$ augmented support images. $f_{ij}$ is the aggregated features at patch position $(i,j)$, achieved by concatenating the three STN outputs at the corresponding position with upsampling operations to match their sizes. By the assumption that $F_{ij}$ is generated by $\\mathcal{N}(\\mu_{ij}, \\Sigma_{i j})$, the sample covariance is:\n\\begin{equation}\n    \\Sigma_{i j}=\\frac{1}{N-1} \\sum_{k=1}^{N}\\left(f_{ij}^k-\\mu_{i j}\\right)\\left(f_{ij}^k-\\mu_{i j}\\right)^{\\mathrm{T}}+\\epsilon I,\n\\end{equation}\nwhere $\\mu_{ij}$ is the sample mean of $F_{ij}$, and the regularization term $\\epsilon I$ makes the sample covariance matrix full rank and invertible. Finally, each possible patch position is associated with a multivariate Gaussian distribution.\n\n\\textbf{Discussion.} Data augmentations are widely adopted in AD, and especially in FSAD, including TDG~\\cite{TDG} and DiffNet~\\cite{DiffNet}. However, most methods simply apply the data augmentations on both the support and test images without any exploration of the impact. In this paper, we emphasize that data augmentation plays a very important role in expanding the support set, which is beneficial for the normal distribution estimation. Specifically, we adopt augmentations including rotation, translation, flipping, and graying for each image in the support set $\\mathcal{S}_t$. Other augmentations like mixup and cutpaste are not considered since they seem more suitable for simulating anomalies~\\cite{cutpaste}. We conduct the possible combinations of all these augmentations for each sample in the support set, which jointly combine into a larger support set. We conduct the normal distribution estimation on such an augmented support set. We study the impacts of different augmentations in the supplementary material.\n\n"
                },
                "subsection 4.3": {
                    "name": "Inference",
                    "content": " \nDuring inference, test samples that are out of the normal distribution are considered anomalies. For each test image in $\\mathcal{T}_{test}$, we use the Mahalanobis distance $\\mathcal{M}\\left(f_{i j}\\right)$ to give an anomaly score to the patch in position $(i,j)$, where\n\\begin{equation}\n    \\mathcal{M}\\left(f_{i j}\\right)=\\sqrt{\\left(f_{i j}-\\mu_{i j}\\right)^{T} \\Sigma_{i j}^{-1}\\left(f_{i j}-\\mu_{i j}\\right)}.\n\\end{equation}\nThe matrix of Mahalanobis distances $\\mathcal{M}=\\left(\\mathcal{M}\\left(f_{i j}\\right)\\right)_{1\\leqslant i\\leqslant W, 1\\leqslant j\\leqslant H}$ forms an anomaly map. Three inverse affine transformations corresponding to the three STN modules are applied to this anomaly map to get the final anomaly score map $\\mathcal{M}_{final}$ aligned with the original image. High scores in this map indicate the anomalous areas. The final anomaly score of the entire image is the maximum of anomaly map $\\mathcal{M}_{final}$. Compared with~\\cite{TDG,DiffNet}, RegAD cancels the data augmentation of the test images which reduces the inference computational costs.\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n",
                "subsection 5.1": {
                    "name": "Experimental Setups",
                    "content": "\n",
                    "subsubsection 5.1.1": {
                        "name": "Datasets.",
                        "content": "\nWe experiment on two challenging real-world benchmark datasets for AD~\\cite{bergmann2019mvtec,jezek2021deep}, which are both related to industrial defect detection.\n\\begin{itemize}\n    \\item \\textbf{MVTec}~\\cite{bergmann2019mvtec}: MVTec comprises 15 categories with 3629 images for training and validation and 1725 images for testing. The training set contains only of normal images without defects. The test set contains both images with various kinds of defects (anomaly) and defect-free images (normal). On average five per category, 73 different defect types are given. \n    All images are in the resolution range between $700 \\times 700$ and $1024 \\times 1024$ pixels. Pixel-wise ground truth labels for each defective image region are provided. \n    \\item \\textbf{MPDD}~\\cite{jezek2021deep}: MPDD is a newly proposed dataset focused specifically on defect detection during painted metal part fabrication, containing 6 classes of metal parts. Images are captured under the conditions of various spatial orientations, positions, and distances of multiple objects, concerning different light intensities and a non-homogeneous background. \n\\end{itemize}\n\nFor each dataset, we conduct experiments on two different experimental settings. (i) \\textbf{Aggregated training} on multiple categories and then adapting to unseen categories, and (ii) \\textbf{Individual training} only with the support set for each category.\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Competing Methods.",
                        "content": " \nWe consider two state-of-the-art FSAD approaches, TDG~\\cite{TDG} and DiffNet~\\cite{DiffNet}. These two methods both train models individually for each category (setting (ii)). Results are reproduced using the official source code. Considering that our method uses data from multiple categories, for fairness of comparison, we extend them to leverage the same amount of data (setting (i)). A pre-training procedure is added to these methods, where data from multiple categories are used to pre-train the transformation classifier for TDG or initialize the normalizing flow-based estimator for DiffNet. The corresponding methods are TDG+ and DiffNet+. We also evaluate RegAD under the individual training setting, and denote the corresponding method as RegAD-L. We compare with some state-of-the-art vanilla AD methods, such as GANomaly~\\cite{ganomaly}, ARNet~\\cite{ARNet}, MKD~\\cite{MKD}, CutPaste~\\cite{cutpaste}, FYD~\\cite{focus}, PaDiM~\\cite{defard2021padim}, PatchCore~\\cite{patchcore} and CflowAD \\cite{cflow}. These methods use the whole normal dataset for their training, so they can be deemed as the upper bound on FSAD performance.\n\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Evaluation Protocols.",
                        "content": " \nWe quantify the model performance using the area under the Receiver Operating Characteristic (ROC) curve metric (AUC), which is commonly adopted as the performance measurement for AD tasks. The image-level AUC and the pixel-level AUC are used for anomaly detection and anomaly localization respectively.\n\n"
                    },
                    "subsubsection 5.1.4": {
                        "name": "Model Configuration and Training Details.",
                        "content": "\nAn ImageNet pre-trained ResNet-18~\\cite{he2016deep} is used as the backbone, followed by a convolutional-based encoder and predictor. To retain the spatial information, the encoder contains three $1\\times 1$ convolutional layers, while the predictor contains two $1\\times 1$ convolutional layers, without any pooling operation. We train models on $224 \\times 224$ images on one NVIDIA GTX 3090. We update the parameters using momentum SGD with a learning rate of 0.0001 for 50 epochs, with a batch size of 32. A single cycle of cosine learning rate is used as the decay schedule. \n\n\n\n\n\n\n\n\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Comparison with State-of-the-art Methods",
                    "content": "\n",
                    "subsubsection 5.2.1": {
                        "name": "Comparison with Few-Shot Anomaly Detection Methods.",
                        "content": " \nExperiments were conducted using the leave-one-out setting, \\emph{i.e.}, a target category was chosen to be tested, while other categories in the dataset are used for training. Table~\\ref{tal:mvtec} and Table~\\ref{tal:mpdd} show the comparison results on MVTec and MPDD, respectively, under the experimental setting (i). RegAD achieves an improvement of 5.1\\%, 6.9\\%, 8.0\\% in average AUC on MVTec, and an improvement of 3.2\\%, 5.0\\%, 3.4\\% in average AUC on MPDD, over DiffNet+~\\cite{DiffNet}, with 2-shot, 4-shot, and 8-shot scenarios, respectively. Also, with one-shot, RegAD achieves 82.4\\% and 57.8\\% AUC on MVtec and MPDD respectively.\n\nRegAD is tested without any parameter fine-tuning, which may not guarantee the best performance for every category, while other baselines have unfair advantages in that they tune the parameters for each category. In 9 out of the 15 categories, RegAD outperforms all the other baselines. RegAD also achieves the least standard deviation (10.94) for the 15 categories when k=8, compared to TDG+ (15.20) and DiffNet+ (13.11), suggesting its better generalizability across different categories. Also, although using different training settings, for MVTec (k=8), RegAD achieves 91.2\\% AUC, with an $\\approx$3\\% improvement compared with Metaformer~\\cite{metaformer} which uses an additional large-scale dataset, MSRA10K~\\cite{msra10k}, during its entire training procedure.\n\n\\textbf{Discussion.} Adaptation time is important for real-world applications of FSAD. The procedures of fine-tuning for both TDG+ and DiffNet+ are time-consuming since they update the models for many epochs, while RegAD has the fastest adaptation speed since it is based on a statistical estimator which needs only one inference for each support image. In Table~\\ref{tal:fewshot}, we report the adaptation times for each method, by averaging the results for $k=2,4,8$ on both the MVTec and MPDD datasets. Compared with TDG+ (1559.76s) and DiffNet+ (357.75s), the proposed RegAD has the fastest adaptation speed (4.47s).\n\nTable~\\ref{tal:fewshot} also compares these methods under experimental setting (ii), where we train the models individually using the support images for each category. RegAD-L means RegAD with individual training on one category only. Assuming that features pre-trained by ImageNet are fully representative, we simply fine-tune features using limited support images. Thus, we conduct the fine-tuning procedures directly under an ImageNet pre-training backbone for all methods. All methods use the same ImageNet pre-training backbone to have a fair comparison. In this setting, RegAD-L outperforms both TDG and DiffNet on the MVTec dataset. DiffNet performs better than the proposed method on the MPDD dataset. However, compared with RegAD-L, the proposed RegAD improves a lot, showing the effectiveness of the proposed feature registration aggregated training procedure on multiple categories.\n\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "Comparison with Vanilla Anomaly Detection Methods.",
                        "content": " \nThe state-of-the-art vanilla AD methods use the whole normal dataset for their training and train a separate model for each category, so their performance can be seen as the upper bound for FSAD. We consider methods including GANomaly~\\cite{ganomaly}, ARNet~\\cite{ARNet}, MKD~\\cite{MKD}, CutPaste~\\cite{cutpaste}, FYD~\\cite{focus}, PaDiM~\\cite{defard2021padim}, PatchCore~\\cite{patchcore} and CflowAD~\\cite{cflow}. Results in Table~\\ref{tal:vanilla} show that the proposed RegAD reaches competitive performance even compared with vanilla AD methods that are based on extensive normal data. For example, with only 4 support images, the proposed method (88.2\\% AUC) outperforms MKD (87.7\\%) with the same ImageNet pre-trained backbone, and with 32 support images its AUC increases to 94.6\\%.\n\n"
                    }
                },
                "subsection 5.3": {
                    "name": "Ablation Studies",
                    "content": "\\label{sec:abl}\nExperiments were performed to evaluate the contribution made by individual components of the proposed method. Results of ablation studies for k-shot anomaly detection and localization on the MVTec and MPDD datasets are shown in Table~\\ref{tal:abl_all}. Modules of \u2018A\u2019, \u2018F\u2019, and \u2018S\u2019 mean the augmentations for support sets, the feature registration aggregated training on multiple categories, and the spatial transformer networks (STN), respectively. Results in Table~\\ref{tal:abl_all} show that: \n\n\n\n\n\n\n\n\n\n\\textbf{(i) Augmentations.} The proposed support set augmentations are shown to be essential for both detection and localization. With $k=\\{2,4,8\\}$, the AUC is improved for 6.8\\%, 6.9\\%, 6.9\\% on MVTec and for 1.2\\%, 0.5\\%, 0.6\\% on MPDD, respectively. We further presents the ablation studies of comparing different augmentation methods for support images in the supplementary material.\n\n\\textbf{(ii) Feature Registration Aggregated Training.} The feature registration aggregated training on multiple categories is effective both with and without support image augmentations. It shows that the proposed feature registration is beneficial for estimating the normal distribution. As shown in Table~\\ref{tal:abl_all}, with $k=\\{2,4,8\\}$, the proposed anomaly-free feature registration can improve the AUC by 3.3\\%, 2.9\\%, 2.6\\% on MVTec, respectively.\n\n\\textbf{(iii) Spatial Transformer Modules.} The proposed STN module is good for improving the ability of the feature registration and thus beneficial for AD. For example, as shown in Table~\\ref{tal:abl_all}, when $k=8$, the STN module can further improve the performance from 89.3\\% to 91.2\\% on MVTec and from 64.8\\% to 71.9\\% on MPDD. However, models with STN modules show similar pixel-level localization performance with models without STN modules. The reason comes from the information lost of the inverse transformation operation and its imprecision. These inverse transformations are designed as post-processing operations to rematch the spatial location of transformed features and the original images.\n\nWe further conduct ablation studies on different transformation versions of STN modules on MVTec and MPDD for AD, as shown in Table~\\ref{tal:abl_stn}. The best performing STN version is rotation+scale on MVTec, which matches the observation that samples in this dataset are all aligned to the center, and thus, there is no need for translation. While for the MPDD dataset, since the samples are not well be centered, the version of STN with affine transformations shows the best performance. STN is used as a feature transformation module, enabling the model to implicitly transform the images to facilitate feature registration. Images in MPDD are captured under various spatial orientations and positions, thus aligning the features is expected to be helpful. For MVTec, objects are well centralized and have similar orientations, so STN is less helpful to MVTec.\n\n"
                },
                "subsection 5.4": {
                    "name": "Visualization Analysis",
                    "content": "\nTo qualitatively analyze how the proposed feature registration approach improves the anomaly localization performance, we visualize the results of some cases from the MVTec and MPDD datasets. It can be seen from the results in Fig.~\\ref{img:result} that the localization produced by RegAD using aggregated training (column e) is closer to the ground truth (column f) than that produced by the individual training baseline (column c). This illustrates the effectiveness of the proposed feature registration training procedure on multiple categories.\n\nWe also use t-SNE~\\cite{maaten2008visualizing} to visualize the features learned on the MVTec dataset, as shown in Fig.~\\ref{img:tsne}. Each dot here represents an augmented normal sample from the test set. It can be seen that the proposed feature registration makes the features more compact within each category, and pushes away features of different categories, which is desirable for the benefit of estimating the normal distribution for each category.\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nThis paper proposes an FSAD method utilizing registration, a task inherently generalizable across categories, as the proxy task. Given only a few normal samples for each category, we trained a category-agnostic feature registration network with the aggregated data. This model is shown to be directly generalizable to new categories, requiring no re-training or parameter fine-tuning. The anomalies are identified by comparing the registered features of the test image and its corresponding support (normal) images.  For both anomaly detection and anomaly localization, the method is shown to be competitive, even compared with vanilla AD methods that are trained with much larger volumes of data. The impressive results suggest a high potential for the proposed method to be applicable in real-world anomaly detection environments.\n\n\\textbf{Acknowledgments.}\nThis work is supported by the National Key Research and Development Program of China (No. 2020YFB1406801), 111 plan (No. BP0719010),  and STCSM (No. 18DZ2270700), and State Key Laboratory of UHD Video and Audio Production and Presentation.\n\n\\bibliographystyle{splncs04}\n\\bibliography{RegAD}\n\n\\appendix\n\n"
            },
            "section 7": {
                "name": "Main Contributions",
                "content": "\n\nThis paper targets a challenging yet practical setting for anomaly detection, with 1) a single model for all categories (\\emph{i.e., generalizable without fine-tuning}), 2) only a few images for each novel category (\\emph{i.e., few shot}), and 3) only normal samples available (\\emph{i.e., unsupervised setting}). To our best knowledge, it is the first attempt to explore such a setting, as a critical step toward practical large-scale industrial applications, a point appreciated by the other reviewers. To learn a category-agnostic model, we further propose a novel comparison-based solution, which is quite different from the popular reconstruction-based or classification-based methods. We adopt STN to align the images and Siamese network to implement the comparison. The SOTA results achieved on MVTec and MPDD show the effectiveness of our method.\n\n"
            },
            "section 8": {
                "name": "Experiments",
                "content": "\n\n",
                "subsection 8.1": {
                    "name": "Experiments with a Large k.",
                    "content": " \nTo decrease the training burden, RegAD is designed to be adaptable to unseen categories without parameter fine-tuning. Without fine-tuning on the few-shot support examples, simply increasing the shot number, the performance gain saturates very soon. We further experiment with k=64 and k=128. As shown in Table~\\ref{tab:largeK}, when k increases from 64 to 128, a limited performance gain is observed. But the results are still competitive, though with a shallow backbone, compared to those of the AD methods trained by full data.\n\n\n\n\n\n"
                },
                "subsection 8.2": {
                    "name": "Ablation Studies on Support Set Augmentations.",
                    "content": "\nThe proposed support set augmentations are shown to be essential for both detection and localization. Table~\\ref{tal:abl_aug} further presents the ablation studies of comparing different augmentation methods for support images with $k=2$. The experimental results have validated the effectiveness of all the proposed augmentation methods. In particular, rotation and translation are shown to perform better on MVTec, while flipping and rotation seem to perform better on MPDD.\n\n"
                },
                "subsection 8.3": {
                    "name": "metaformer",
                    "content": "\nAlthough using different training settings, according to the reported results in~\\cite{metaformer}, Metaformer achieves about 88\\% AUC for MVTec when k=8, while RegAD achieves 91.2\\% AUC, an $\\approx$3\\% improvement, with the same test set and evaluation protocol. Metaformer achieves worse results despite three unfair advantages: (i) an additional large-scale dataset, MSRA10K, is used during its entire meta-training procedure (beyond parameter pre-training), together with additional pixel-level annotations; (ii) it performs additional fine-tuning on each novel category; (iii) it is trained with a deep transformer architecture for more epochs (100 vs. 50), with a larger batch size (64 vs. 32).\n\n"
                }
            }
        },
        "tables": {
            "tal:mvtec": "\\begin{table}[t]\n\\centering\n\\caption{Results of k-shot anomaly detection on the MVTec dataset, comparing with state-of-the-art methods. Results are listed as the average AUC in \\% of 10 runs and are marked individually for each category. A macro-average score over all categories is also reported in the last row. The best-performing method is in bold.}\n\\label{tal:mvtec}\n\\scriptsize\n\\setlength{\\tabcolsep}{1.2pt}{\n\\begin{tabular}{C{1.3cm}C{1.1cm}C{1.2cm}C{0.9cm}C{1.1cm}C{1.2cm}C{0.9cm}C{1.1cm}C{1.2cm}C{0.9cm}}\n\\toprule\n\\multirow{3}{*}{Category} & \\multicolumn{3}{c}{k=2} & \\multicolumn{3}{c}{k=4} & \\multicolumn{3}{c}{k=8}\\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10}\n& \\makecell[c]{TDG+\\\\\\cite{TDG}} & \\makecell[c]{DiffNet+\\\\\\cite{DiffNet}} & \\makecell[c]{RegAD\\\\(ours)} & \\makecell[c]{TDG+\\\\\\cite{TDG} } & \\makecell[c]{DiffNet+\\\\\\cite{DiffNet}} & \\makecell[c]{RegAD\\\\(ours)} & \\makecell[c]{TDG+\\\\\\cite{TDG}} & \\makecell[c]{DiffNet+\\\\\\cite{DiffNet} } & \\makecell[c]{RegAD\\\\(ours)}\\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10}\nBottle     & 69.3 & 99.3 & \\textbf{99.4} & 69.6 & 99.3 & \\textbf{99.4} & 70.3 & 99.4 & \\textbf{99.8} \\\\\nCable      & 68.3 & \\textbf{85.3} & 65.1 & 70.3 & \\textbf{85.2} & 76.1 & 74.7 & \\textbf{87.9} & 80.6 \\\\\nCapsule    & 55.1 & \\textbf{73.0} & 67.5 & 47.6 & \\textbf{80.3} & 72.4 & 44.7 & \\textbf{78.6} & 76.3 \\\\\nCarpet     & 66.2 & 78.4 & \\textbf{96.5} & 68.7 & 78.6 & \\textbf{97.9} & 78.2 & 78.5 & \\textbf{98.5} \\\\\nGrid       & 83.8 & 62.1 & \\textbf{84.0} & 86.2 & 60.5 & \\textbf{91.2} & 87.6 & 78.5 & \\textbf{91.5} \\\\\nHazelnut   & 67.2 & 94.9 & \\textbf{96.0} & 71.2 & \\textbf{95.8} & 95.8 & 82.8 & \\textbf{97.9} & 96.5 \\\\\nLeather    & 93.6 & 90.7 & \\textbf{99.4}  & 93.2 & 91.2 & \\textbf{100} & 93.5 & 92.2 & \\textbf{100}  \\\\\nMetal Nut  & 67.1 & 61.9 & \\textbf{91.4} & 69.2 & 67.3 & \\textbf{94.6} & 68.7 & 67.6 & \\textbf{98.3} \\\\\nPill       & 69.2 & \\textbf{83.2} & 81.3 & 64.7 & \\textbf{84.0} & 80.8 & 67.9 & \\textbf{82.1} & 80.6 \\\\\nScrew      & \\textbf{98.8} & 73.4 & 52.5 & \\textbf{98.8} & 72.5 & 56.6 & \\textbf{99.0} & 75.0 & 63.4 \\\\\nTile       & 86.3 & \\textbf{97.0} & 94.3 & 87.2 & \\textbf{98.0} & 95.5 & 87.4 & \\textbf{99.6} & 97.4 \\\\\nToothbrush & 54.4 & 60.8 & \\textbf{86.6} & 57.8 & 62.5 & \\textbf{90.9} & 57.6 & 60.8 & \\textbf{98.5} \\\\\nTransistor & 55.9 & 61.8 & \\textbf{86.0} & 67.7 & 62.2 & \\textbf{85.2} & 71.5 & 63.3 & \\textbf{93.4} \\\\\nWood       & 98.4 & 98.1 & \\textbf{99.2} & 98.3 & 96.4 & \\textbf{98.6} & 98.4 & \\textbf{99.4} & \\textbf{99.4} \\\\\nZipper     & 64.4 & \\textbf{89.2} & 86.3 & 65.3 & 84.8 & \\textbf{88.5} & 66.3 & 87.3 & \\textbf{94.0} \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10}\nAverage    & 73.2 & 80.6 & \\textbf{85.7} & 74.4 & 81.3 & \\textbf{88.2} & 76.6 & 83.2 & \\textbf{91.2} \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}",
            "tal:mpdd": "\\begin{table}[t]\n\\centering\n\\caption{Results of k-shot anomaly detection on the MPDD dataset, comparing with state-of-the-art methods. Results are listed as the average AUC in \\% of 10 runs and are marked individually for each category. A macro-average score over all categories is also reported in the last row. The best-performing method is in bold.}\n\\label{tal:mpdd}\n\\scriptsize\n\\setlength{\\tabcolsep}{0.8pt}{\n\\begin{tabular}{C{1.8cm}C{1.1cm}C{1.2cm}C{0.9cm}C{1.1cm}C{1.2cm}C{0.9cm}C{1.1cm}C{1.2cm}C{0.9cm}}\n\\toprule\n\\multirow{3}{*}{Category} & \\multicolumn{3}{c}{k=2} & \\multicolumn{3}{c}{k=4} & \\multicolumn{3}{c}{k=8}\\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10}\n& \\makecell[c]{TDG+\\\\\\cite{TDG} } & \\makecell[c]{DiffNet+\\\\\\cite{DiffNet} } & \\makecell[c]{RegAD\\\\(ours)} & \\makecell[c]{TDG+\\\\\\cite{TDG} } & \\makecell[c]{DiffNet+\\\\\\cite{DiffNet} } & \\makecell[c]{RegAD\\\\(ours)} & \\makecell[c]{TDG+\\\\\\cite{TDG} } & \\makecell[c]{DiffNet+\\\\\\cite{DiffNet} } & \\makecell[c]{RegAD\\\\(ours)}\\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10}\nbracket black & 46.4 & 56.7 & \\textbf{63.3} & 48.8 & 59.9 & \\textbf{63.8} & 51.0 & \\textbf{69.7} & 67.3 \\\\\nbracket brown & 54.9 & \\textbf{61.3} & 59.4 & 57.5 & 64.2 & \\textbf{66.1} & 65.4 & 66.3 & \\textbf{69.6} \\\\\nbracket white & \\textbf{64.0} & 42.2 & 55.6 & \\textbf{65.4} & 51.8 & 59.3 & 66.8 & \\textbf{69.1} & 61.4 \\\\\nconnector     & 53.1 & 54.1 & \\textbf{73.0} & 55.8 & 54.8 & \\textbf{77.2} & 62.9 & 54.5 & \\textbf{84.9} \\\\\nmetal plate   & 91.8 & \\textbf{96.8} & 61.7 & 95.1 & \\textbf{98.2} & 78.6 & 98.4 & \\textbf{98.8} & 80.2 \\\\\ntubes         & 51.8 & 49.8 & \\textbf{67.1} & 58.5 & 50.7 & \\textbf{67.5} & 64.9 & 52.6 & \\textbf{67.9} \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10}\nAverage       & 60.3 & 60.2 & \\textbf{63.4} & 63.5 & 63.3 & \\textbf{68.3} & 68.2 & 68.5 & \\textbf{71.9} \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}",
            "tal:fewshot": "\\begin{table}[t]\n\\centering\n\\caption{Results of anomaly detection on the MVTec and MPDD datasets under two different experimental settings (i) and (ii), comparing with state-of-the-art few-shot anomaly detection methods on $k=2, 4, 8$. Results are listed as the macro-average AUC in \\% over all categories in each dataset of 10 runs. The best-performing method for each experimental setting is in bold.}\n\\label{tal:fewshot}\n\\scriptsize\n\\setlength{\\tabcolsep}{0.6pt}{\n\\begin{tabular}{C{2.2cm}C{1.3cm}C{1.5cm}C{1.8cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.8cm}}\n\\toprule\n\\multicolumn{1}{c}{\\multirow{2}{*}{Methods}}\n& ImageNet & Aggregated & Time of & \\multicolumn{3}{c}{MVTec} & \\multicolumn{3}{c}{MPDD} \\\\\n& Pretrain & Training &  Adaptation & k=2 & k=4 & k=8 & k=2 & k=4 & k=8\\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10}\nTDG~\\cite{TDG} & \\checkmark & \\ding{55} & - & 71.2 & 72.7 & 75.2 & 57.3 & 60.4 & 64.4\\\\\nDiffNet~\\cite{DiffNet} & \\checkmark & \\ding{55} & - & 80.5 & 80.8 & 82.9 & \\textbf{58.4} & \\textbf{61.2} & \\textbf{66.5}\\\\\nRegAD-L (ours) & \\checkmark & \\ding{55} & - & \\textbf{81.5} & \\textbf{84.9} & \\textbf{87.4} & 50.8 & 54.2 & 61.1\\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10}\nTDG+~\\cite{TDG}   & \\checkmark & \\checkmark & 1559.76s & 73.2 & 74.4 & 76.6 & 60.3 & 63.5 & 68.2\\\\\nDiffNet+~\\cite{DiffNet}   & \\checkmark & \\checkmark & 357.75s & 80.6 & 81.3 & 83.2 & 60.2 & 63.3 & 68.5\\\\\nRegAD (ours) & \\checkmark & \\checkmark & 4.47s & \\textbf{85.7} & \\textbf{88.2} & \\textbf{91.2} & \\textbf{63.4} & \\textbf{68.3} & \\textbf{71.9}\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}",
            "tal:vanilla": "\\begin{table}[t]\n\\centering\n\\caption{Results of anomaly detection and anomaly localization on the MVTec and MPDD datasets, comparing with state-of-the-art vanilla AD methods. Results are listed as AUC in \\% as the macro-average score over all categories in each dataset.}\n\\label{tal:vanilla}\n\\scriptsize\n\\setlength{\\tabcolsep}{1.2pt}{\n\\begin{tabular}{C{2.2cm}C{1.5cm}C{1.6cm}C{1.8cm}C{1.1cm}C{1.1cm}C{1.1cm}C{1.1cm}}\n\\toprule\n\\multicolumn{1}{c}{\\multirow{2}{*}{Methods}}\n& \\multicolumn{1}{c}{\\multirow{2}{*}{Data}} & ImageNet & \\multicolumn{1}{c}{\\multirow{2}{*}{Backbone}} & \\multicolumn{2}{c}{MVTec} & \\multicolumn{2}{c}{MPDD} \\\\\n& & Pretrain & & image & pixel & image & pixel\\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8}\nRegAD (k=4) & 4 images & \\checkmark & Res18 & 88.2 & 95.8 & 68.8 & 93.9\\\\\nRegAD (k=8) & 8 images & \\checkmark & Res18 & 91.2 & 96.7 & 71.9 & 95.1\\\\\nRegAD (k=16) & 16 images & \\checkmark & Res18 & 92.7 & 96.6 & 75.3 & 96.3\\\\\nRegAD (k=32) & 32 images & \\checkmark & Res18 & 94.6 & 96.9 & 76.8 & 96.3\\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3}\\cmidrule(lr){4-4} \\cmidrule(lr){5-6} \\cmidrule(lr){7-8}\nGANomaly~\\cite{ganomaly} & full data & \\ding{55} & UNet & 80.5 & - & 64.8 & -\\\\\nARNet~\\cite{ARNet} & full data & \\ding{55} & UNet & 83.9 & - & 69.7 & -\\\\\nMKD~\\cite{MKD} & full data & \\checkmark & Res18 & 87.7 & 90.7 & - & - \\\\\nCutPaste~\\cite{cutpaste} & full data & \\checkmark & Res18 & 95.2 & 96.0 & - & -\\\\\nFYD~\\cite{focus} & full data & \\checkmark & Res18 & 97.3 & 97.4 & - & -\\\\\nPaDiM~\\cite{defard2021padim} & full data & \\checkmark & WRN50 & 97.9 & 97.5 & 74.8 & 96.7\\\\\nPatchCore~\\cite{patchcore} & full data & \\checkmark & WRN50 & 99.1 & 98.1 & 82.1 & 95.7\\\\\nCflowAD~\\cite{cflow} & full data & \\checkmark & WRN50 & 98.3 & 98.6 & 86.1 & 97.7\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}",
            "tal:abl_all": "\\begin{table}[t]\n\\centering\n\\caption{Ablation studies of k-shot anomaly detection and localization on the MVTec and MPDD datasets. Modules of `A', `F', and `S' mean the augmentations for the support set, the feature registration aggregated training, and the spatial transformer networks (STN), respectively. Results are listed as the macro-average AUC in \\% over all categories in each dataset of 10 runs. The best-performing method is in bold.}\n\\label{tal:abl_all}\n\\scriptsize\n\\setlength{\\tabcolsep}{1.1pt}{\n\\begin{tabular}{C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}}\n\\toprule\n\\multicolumn{3}{c}{\\multirow{2}{*}{Modules}}\n& \\multicolumn{6}{c}{MVTec}                                        & \\multicolumn{6}{c}{MPDD}                                          \\\\\n\\cmidrule(lr){4-9} \\cmidrule(lr){10-15}\n& & & \\multicolumn{3}{c}{image} & \\multicolumn{3}{c}{pixel} & \\multicolumn{3}{c}{image} & \\multicolumn{3}{c}{pixel}  \\\\\n\\cmidrule(lr){1-3} \\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \\cmidrule(lr){10-12} \\cmidrule(lr){13-15}\n A & F & S & k=2 & k=4 & k=8               & k=2 & k=4 & k=8                  & k=2 & k=4 & k=8               & k=2 & k=4 & k=8                   \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3}\\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10} \\cmidrule(lr){11-11} \\cmidrule(lr){12-12} \\cmidrule(lr){13-13} \\cmidrule(lr){14-14} \\cmidrule(lr){15-15}\n            &            &            & 74.7 & 78.0 & 80.5 & 88.6 & 90.5 & 92.1 & 49.6 & 53.7 & 55.5 & 89.5 & 91.2 & 92.0 \\\\\n \\checkmark &            &            & 81.5 & 84.9 & 87.4 & 93.3 & 94.7 & 95.5 & 50.8 & 54.2 & 61.1 & 92.4 & 93.3 & 93.9\\\\\n            & \\checkmark &            & 78.0 & 80.9 & 83.1 & 90.8 & 92.5 & 94.0 & 53.9 & 55.5 & 57.2 & 91.5 & 92.2 & 93.0 \\\\\n            & \\checkmark & \\checkmark & 79.1 & 82.9 & 84.9 & 90.5 & 93.3 & 94.3 & 57.6 & 60.9 & 62.7 & 91.0 & 91.8 & 93.0\\\\\n \\checkmark & \\checkmark &            & 83.0 & 86.4 & 89.3 & \\textbf{94.7} & \\textbf{95.9} & 96.6 & 52.8 & 57.7 & 64.8 & \\textbf{93.3} & \\textbf{94.1} & 94.4\\\\\n \\checkmark & \\checkmark & \\checkmark & \\textbf{85.7} & \\textbf{88.2} & \\textbf{91.2} & 94.6 & 95.8 & \\textbf{96.7} & \\textbf{63.4} & \\textbf{68.8} & \\textbf{71.9} & 93.2 & 93.9 & \\textbf{95.1} \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}",
            "tal:abl_stn": "\\begin{table}[t]\n\\centering\n\\caption{Ablation studies of different transformation versions of STN modules on MVTec and MPDD for anomaly detection with $k=2$. T, R means translation, and rotation, respectively. Results are listed as the macro-average AUC in \\% over all categories in each dataset of 10 runs. The best-performing method is in bold.}\n\\label{tal:abl_stn}\n\\scriptsize\n\\setlength{\\tabcolsep}{0.4pt}{\n\\begin{tabular}{C{1.3cm}C{1.3cm}C{0.8cm}C{0.8cm}C{0.9cm}C{0.9cm}C{1.1cm}C{1.4cm}C{1.1cm}C{1.3cm}C{1.0cm}}\n\\toprule\nData & no STN & T & R & scale & shear & \\makecell[c]{R\\\\+scale} & \\makecell[c]{T\\\\+scale} & \\makecell[c]{T+R} & \\makecell[c]{T+R\\\\+scale} & affine \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10} \\cmidrule(lr){11-11}\nMVTec & 83.0 & 84.5 & 85.0 & 84.9 & 84.9 & \\textbf{85.7} & 84.9 & 84.2 & 84.9 & 84.5 \\\\\nMPDD & 52.8 & 62.3 & 57.7 & 59.2 & 59.0 & 61.5 & 61.8 & 61.0 & 61.7 & \\textbf{63.4} \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}",
            "tab:largeK": "\\begin{table}[h]\n    \\centering\n    \\scriptsize\n    \\caption{Comparison with AD method trained by full data.}\n    \\begin{tabular}{C{1.7cm}|C{1.5cm}C{1.5cm}C{1.5cm}C{2.1cm}C{2.1cm}}\n        \\toprule\n         &  \\multicolumn{3}{c}{RegAD} & PatchCore~\\cite{patchcore} & CflowAD~\\cite{cflow}\\\\\n         \\hline\n       k  &  32 & 64 & 128 & full data & full data\\\\\n       \\hline\n       MVTec & 94.6\\% & 95.5\\% & 95.9\\% & 99.1\\% & 98.3\\%\\\\\n       MPDD & 76.8\\% & 82.3\\% & 83.2\\% & 82.1\\% & 86.1\\%\\\\\n       \\bottomrule\n    \\end{tabular}\n    \\label{tab:largeK}\n\\end{table}",
            "tal:abl_aug": "\\begin{table}[t]\n\\centering\n\\caption{Ablation studies of different versions of support set augmentations on the MVTec and MPDD datasets with $k=2$. Besides the full version of RegAD, We also provide the individual training version to reduce the influence of data augmentations on multiple categories. G, F, T, R means graying, flipping, translation, and rotation, respectively. Results are listed as the macro-average AUC in \\% over all categories in each dataset of 10 runs. The best-performing method is in bold.}\n\\label{tal:abl_aug}\n\\scriptsize\n\\setlength{\\tabcolsep}{0.6pt}{\n\\begin{tabular}{C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{1.1cm}C{1.1cm}C{1.1cm}C{1.1cm}C{1.1cm}C{1.1cm}C{1.1cm}C{1.1cm}}\n\\toprule\n\\multicolumn{4}{c}{\\multirow{2}{*}{Augmentations}} & \\multicolumn{4}{c}{Individual Training} & \\multicolumn{4}{c}{Aggregated Training}\\\\\n\\cmidrule(lr){5-8} \\cmidrule(lr){9-12}\n&&&& \\multicolumn{2}{c}{MVTec} & \\multicolumn{2}{c}{MPDD} & \\multicolumn{2}{c}{MVTec} & \\multicolumn{2}{c}{MPDD}\\\\\nG & F & T & R & image & pixel & image & pixel & image & pixel & image & pixel\\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10} \\cmidrule(lr){11-11} \\cmidrule(lr){12-12}\n & & & & 74.7 & 88.6 & 49.6 & 89.5 & 79.1 & 90.5 & 57.6 & 91.0\\\\\n\\checkmark& & & & 74.9 & 88.6 & 49.4 & 89.4 & 79.5 & 90.7 & 58.0 & 91.3\\\\\n &\\checkmark& & & 75.5 & 90.0 & 50.1 & 90.2 & 79.8 & 92.6 & 58.6 & 92.6\\\\\n & &\\checkmark& & 77.4 & 90.9 & 49.8 & 91.5 & 81.3 & 92.4 & 58.3 & 90.9\\\\\n & & &\\checkmark& 79.6 & 92.7 & 50.0 & 91.7 & 82.2 & 93.6 & 59.8 & 90.8\\\\\n \\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10} \\cmidrule(lr){11-11} \\cmidrule(lr){12-12}\n\\checkmark&\\checkmark& & & 75.6 & 89.9 & 50.0 & 90.1 & 80.5 & 92.7 & 59.7 & 91.7\\\\\n\\checkmark& &\\checkmark& & 77.5 & 90.9 & 49.8 & 91.4 & 81.0 & 92.4 & 58.5 & 92.6\\\\\n\\checkmark& & &\\checkmark& 79.7 & 92.6 & 50.0 & 91.6 & 83.8 & 93.7 & 60.9 & 92.6\\\\\n &\\checkmark&\\checkmark& & 77.7 & 91.5 & 50.1 & 91.7 & 81.6 & 93.2 & 58.2 & 92.3\\\\\n &\\checkmark& &\\checkmark& 79.7 & 92.9 & 51.3 & 91.8 & 82.3 & 94.0 & 59.7 & 92.9\\\\\n & &\\checkmark&\\checkmark& 81.3 & 93.1 & 49.9 & 92.2 & 84.2 & 94.6 & 60.6 & 91.7\\\\\n \\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10} \\cmidrule(lr){11-11} \\cmidrule(lr){12-12}\n\\checkmark&\\checkmark&\\checkmark& & 77.8 & 91.5 & 50.2 & 91.6 & 81.7 & 93.5 & 59.9 & 92.6\\\\\n\\checkmark&\\checkmark& &\\checkmark& 79.8 & 92.9 & 51.2 & 91.7 & 83.9 & 94.2 & 63.0 & 93.0\\\\\n\\checkmark& &\\checkmark&\\checkmark& 81.4 & 93.1 & 49.9 & 92.3 & 84.9 & \\textbf{94.7} & 61.2 & 92.8\\\\\n&\\checkmark&\\checkmark&\\checkmark& \\textbf{81.5} & \\textbf{93.3} & 50.7 & \\textbf{92.4} & 85.4 & 94.6 & 61.2 & \\textbf{93.2}\\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10} \\cmidrule(lr){11-11} \\cmidrule(lr){12-12}\n\\checkmark&\\checkmark&\\checkmark&\\checkmark& \\textbf{81.5} & \\textbf{93.3} & \\textbf{50.8} & \\textbf{92.4} & \\textbf{85.7} & 94.6 & \\textbf{63.4} & \\textbf{93.2}\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}"
        },
        "figures": {
            "img:intro": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{img/intro_final.pdf}\n\\caption{Different from (a) vanilla AD, and (b) existing FSAD methods under the one-model-per-category learning paradigm, the proposed method (c) leverages feature registration as a category-agnostic approach for FSAD, under the one-model-all-category learning paradigm. Trained with aggregated data of multiple categories, the model is directly applicable to novel categories without any parameter fine-tuning, with the only need to estimate the normal feature distribution given the corresponding support set.}\n\\label{img:intro}\n\\end{figure}",
            "img:RegAD": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{img/architecture.pdf}\n\\caption{The model architecture of the proposed RegAD. Given paired images from the same category, features are extracted by three convolutional residual blocks each followed by a spatial transformer network. A Siamese network acts as the feature encoder, supervised by a registration loss for feature similarity maximization.}\n\\label{img:RegAD}\n\\end{figure}",
            "img:result": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.75\\textwidth]{img/results.pdf}\n\\caption{Qualitative results of anomaly localization for RegAD on the MVTec dataset (top three rows) and the MPDD dataset (bottom two rows) for several cases, including localization results with individual training and aggregated training. Results from (e) show better performance than results from (c), showing the effectiveness of the proposed feature registration aggregated training procedure.}\n\\label{img:result}\n\\end{figure}",
            "img:tsne": "\\begin{figure}[t]\n  \\begin{minipage}[t]{0.5\\textwidth}\n\\centering\n\\includegraphics[width=5.0cm]{img/tsne.pdf}\n\\footnotesize\n\n(a) Without Feature Registration\n\\end{minipage}\n\\begin{minipage}[t]{0.37\\textwidth}\n\\centering\n\\includegraphics[width=5.0cm]{img/tsne2.pdf}\n\\footnotesize\n(b) With Feature Registration\n\\end{minipage}\n\\caption{Visualization, using t-SNE, of the features learned from the MVTec dataset, using (a) the baseline without the feature registration, and (b) the proposed method with the feature registration. The same t-SNE optimization iterations are used in each case. Results show that features with registration are more compact within each category, and more separated from different categories.}\n\\label{img:tsne}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\begin{pmatrix}\nx_i^t \\\\ y_i^t\n\\end{pmatrix}\n= S_i(f_{i}^s)\n=A_i\\begin{pmatrix}\nx_{i}^s \\\\ y_{i}^s \\\\ 1\n\\end{pmatrix}\n=\\begin{bmatrix}\n\\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n\\theta_{21} & \\theta_{22} & \\theta_{23} \\\\\n\\end{bmatrix} \n\\begin{pmatrix}\nx_{i}^s \\\\ y_{i}^s \\\\ 1\n\\end{pmatrix},\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\mathcal{D}(p_a,z_b)=-\\frac{p_a}{||p_a||_2}\\cdot \\frac{z_b}{||z_b||_2},\n\\end{equation}",
            "eq:eq:loss": "\\begin{equation}\\label{eq:loss}\n    \\mathcal{L}=\\frac{1}{2}(\\mathcal{D}(p_a,z_b)+\\mathcal{D}(p_b,z_a)).\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\Sigma_{i j}=\\frac{1}{N-1} \\sum_{k=1}^{N}\\left(f_{ij}^k-\\mu_{i j}\\right)\\left(f_{ij}^k-\\mu_{i j}\\right)^{\\mathrm{T}}+\\epsilon I,\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\mathcal{M}\\left(f_{i j}\\right)=\\sqrt{\\left(f_{i j}-\\mu_{i j}\\right)^{T} \\Sigma_{i j}^{-1}\\left(f_{i j}-\\mu_{i j}\\right)}.\n\\end{equation}"
        },
        "git_link": "https://github.com/MediaBrain-SJTU/RegAD"
    }
}