{
    "meta_info": {
        "title": "FreeKD: Free-direction Knowledge Distillation for Graph Neural Networks",
        "abstract": "Knowledge distillation (KD) has demonstrated its effectiveness to boost the\nperformance of graph neural networks (GNNs), where its goal is to distill\nknowledge from a deeper teacher GNN into a shallower student GNN. However, it\nis actually difficult to train a satisfactory teacher GNN due to the well-known\nover-parametrized and over-smoothing issues, leading to invalid knowledge\ntransfer in practical applications. In this paper, we propose the first\nFree-direction Knowledge Distillation framework via Reinforcement learning for\nGNNs, called FreeKD, which is no longer required to provide a deeper\nwell-optimized teacher GNN. The core idea of our work is to collaboratively\nbuild two shallower GNNs in an effort to exchange knowledge between them via\nreinforcement learning in a hierarchical way. As we observe that one typical\nGNN model often has better and worse performances at different nodes during\ntraining, we devise a dynamic and free-direction knowledge transfer strategy\nthat consists of two levels of actions: 1) node-level action determines the\ndirections of knowledge transfer between the corresponding nodes of two\nnetworks; and then 2) structure-level action determines which of the local\nstructures generated by the node-level actions to be propagated. In essence,\nour FreeKD is a general and principled framework which can be naturally\ncompatible with GNNs of different architectures. Extensive experiments on five\nbenchmark datasets demonstrate our FreeKD outperforms two base GNNs in a large\nmargin, and shows its efficacy to various GNNs. More surprisingly, our FreeKD\nhas comparable or even better performance than traditional KD algorithms that\ndistill knowledge from a deeper and stronger teacher GNN.",
        "author": "Kaituo Feng, Changsheng Li, Ye Yuan, Guoren Wang",
        "link": "http://arxiv.org/abs/2206.06561v4",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "Accepted to KDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nGraph data is becoming increasingly prevalent and ubiquitous with the rapid development of the Internet, such as social networks \\cite{hamilton2017inductive},  citation networks \\cite{sen2008collective}, etc. To better handle graph-structured data, graph neural networks (GNNs) provide an effective means to learn node embeddings by aggregating feature information of neighborhood nodes \\cite{velivckovic2017graph}. \nBecause of the powerful ability in modeling relations of data, various graph neural networks have been proposed in the past decade \\cite{kipf2016semi,hamilton2017inductive,velivckovic2017graph,chiang2019cluster,pei2020geom}. The representative works include GraphSAGE \\cite{hamilton2017inductive}, GAT \\cite{velivckovic2017graph}, GCN \\cite{kipf2016semi}, etc.\n\n%widely used for solving various tasks, including recommendation \\cite{fan2019graph}, molecular property prediction   \\cite{hao2020asgn}, text classification \\cite{yao2019graph}, link prediction \\cite{zhang2018link}, etc.\n\n\nRecently, some researchers extend an interesting learning scheme, called knowledge distillation (KD) , into GNNs to further improve the performance of  GNNs \\cite{yang2020distilling,yang2021extract,deng2021graph}.  \nThe basic idea among these methods is to optimize a shallower student  GNN model by distilling knowledge from a deeper teacher  GNN model. \nFor instance, LSP \\cite{yang2020distilling} proposed a local structure preserving module to transfer the topological structure information of a  GNN teacher model. \n% CPF \\cite{yang2021extract} distilled the knowledge of a GNN teacher model by utilizing a parameterized label propagation to absorb structure-based knowledge and using MLP to encode the information of node attributes.\nThe work in \\cite{yan2020tinygnn} proposed a light GNN architecture, called TinyGNN, and attempted to distill knowledge from a deep GNN teacher model to the light GNN model. \nGFKD \\cite{deng2021graph} designed a data-free knowledge distillation strategy for GNNs, enabling to transfer knowledge from a GNN teacher model by generating fake graphs. \n\n\nThe above methods follow the same teacher-student architecture as the traditional knowledge distillation methods \\cite{bucilu2006model,hinton2015distilling}, and resort to a deeper well-optimized teacher GNN for distilling knowledge. However, when applying such an architecture to GNNs, it often suffers from the following limitations:\nfirst, it is often difficult and inefficient to train a satisfactory teacher GNN. As we know, the existing over-parameterized and  over-smoothing issues often degrade the performance of the deeper GNN model. Moreover,  training a deeper well-optimized model usually needs plenty of data and high computational costs. %Meanwhile, the over-parameterized GNN model increases the risk of overfitting \\cite{li2018deeper} and the large number of layers in GNN often leads to the issue of over-smoothing \\cite{rong2019dropedge}, degrading the performance of the teacher ;\nSecond, according to \\cite{yan2020tinygnn,mirzadeh2020improved,yuan2021reinforced}, we know that a stronger teacher model may not necessarily lead to a better student model. This may be because the mismatching of the representation capacities between a teacher model and a student model makes the student model hard to mimic the outputs of a too strong teacher model. Thus, it is  difficult to find an optimal teacher GNN for a student GNN in practical applications. \nConsidering that  many powerful GNN models  have been proposed in the past decade \\cite{wu2020comprehensive}, this gives rise to one intuitive  thought: \\emph{whether we can explore a new knowledge distillation architecture to boost the performance of GNNs, avoiding the obstacle involved by training a deeper well-optimized teacher GNN?}\n\n\n\n\n\n\n\nIn light of these, we propose a new knowledge distillation framework, \\textbf{Free}-direction \\textbf{K}nowledge \\textbf{D}istillation based on Reinforcement learning tailored for GNNs, called \\textbf{FreeKD}. Rather than requiring a deeper well-optimized teacher GNN for unidirectional knowledge transfer, we collaboratively learn two  shallower  GNNs in an effort to distill knowledge from each other via reinforcement learning in a hierarchical way. This idea stems from our observation that one typical GNN model often has better and worse performances at different nodes during training. \nAs shown in Figure \\ref{cross}, GraphSAGE \\cite{hamilton2017inductive} has lower cross entropy losses at nodes with ID$=\\{1,4,5,7,9\\}$, while GAT \\cite{velivckovic2017graph} has better performances at the rest nodes. \n% Based on this observation, we design a reinforcement learning based node judge strategy to dynamically distinguish which model should be used to distill knowledge for each node in each training iteration, in order to capture the complementary information.\nBased on this observation, we design a free-direction knowledge distillation strategy to dynamically exchange knowledge between two shallower GNNs  to benefit from each other.\nConsidering that the direction of distilling knowledge for each node will have influence on the other nodes, we thus regard determining the directions for different nodes as a sequential decision making problem. Meanwhile, since the selection of the directions is a discrete problem, we can not optimize it by stochastic gradient descent based methods \\cite{wang2019minimax}. Thus, we address this problem via reinforcement learning in a hierarchical way. Our hierarchical reinforcement learning algorithm consists of  two levels of actions: Level 1, called  node-level action, is used to distinguish which GNN is chosen to distill knowledge to the other GNN for each node.\nAfter determining the direction of knowledge transfer for each node, we expect to propagate not only the soft label of the node, but also its neighborhood relations.\nThus level 2, called  structure-level action, decides which of the local structures generated by our node-level actions to be propagated.\nOne may argue that we could directly use the loss, e.g., cross entropy, to decide the directions of  node-level knowledge distillation.\nHowever, this heuristic strategy only considers the performance of the node itself, but neglects its influence on other nodes, thus might lead to a sub-optimal solution.\nOur experimental results also verify  our reinforcement learning based strategy significantly outperforms the above heuristic one.\n% After determining the direction of knowledge transfer, we expect to propagate not only the soft label of the node, but also its neighborhood relations which are generated by our node judge module. \n%In the two GNN models, each model  plays the role of either teacher or student at different nodes and {\\color{blue} structures}, enabling  the two models to  flexibly learn from each other. \n    \nThe contributions of this paper can be summarized as:\n\\begin{itemize}\n\\item We propose a new knowledge distillation architecture for GNNs, avoiding requiring a deeper well-optimized teacher model for distilling knowledge. The proposed framework is general and principled, which can be naturally compatible with GNNs of different architectures.\n% \\item We devise an distillation mechanism via reinforcement learning, which can dynamically determine the direction of knowledge transfer at each node. In addition, we simultaneously transfer knowledge from both node-level and structure-level aspects to boost the performance of each model.\n\n\\item We devise a  free-direction knowledge distillation strategy via a hierarchical reinforcement learning  algorithm, which can dynamically manage the directions of knowledge transfer from both node-level and structure-level aspects.\n\n\\item Extensive experiments on five benchmark datasets demonstrate the proposed framework promotes the performance of two shallower GNNs in a large margin, and is valid to various GNNs. \tMore surprisingly, the performance of our FreeKD is comparable to or even better than traditional KD algorithms distilling knowledge from a deeper and stronger teacher GNN.\n\\end{itemize}\n\n\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\nThis work is related to graph neural networks, graph-based knowledge distillation, and reinforcement learning. \n\n",
                "subsection 2.1": {
                    "name": "Graph Neural Networks",
                    "content": "\n\nGraph neural networks have achieved promising results in processing graph data, whose goal is to learn node embeddings by aggregating nodes' neighbor information.\n    In recent years, lots of GNNs have been proposed \\cite{kipf2016semi,hamilton2017inductive,velivckovic2017graph}. For instance, GCN \\cite{kipf2016semi} designed a convolutional neural network architecture for graph data.\n    GraphSAGE \\cite{hamilton2017inductive} proposed an efficient sample strategy  to aggregate neighbor nodes.\n    % , and can perform inductive learning in large-scale graphs with limited computational resources. \n    GAT \\cite{velivckovic2017graph} applied a self-attention mechanism to GNN to assign different weights to different neighbors. \n%   The work in \\cite{xu2018powerful} analyzed the upper bound  of the representation ability of GNN is the Weisfeiler-Lehman isomorphism test \\cite{weisfeiler1968reduction}, and built a GNN model that could reach to this upper bound. \n   SGC \\cite{wu2019simplifying} simplified GCN by removing nonlinearities and weight matrices between consecutive convolutional layers.\n   ROD \\cite{zhang2021rod} proposed an ensemble learning based GNN model to fuse the knowledge in multiple hops.\n    APPNP \\cite{klicpera2018predict} analyzed the relationship between GCN and PageRank \\cite{page1999pagerank}, and proposed a propagation model combined with a personalized PageRank.\n    Cluster-GCN \\cite{chiang2019cluster} built an efficient model based on graph clustering \\cite{schaeffer2007graph}. \n    Being orthogonal to the above approaches developing different powerful  GNN models, we concentrate on developing a new knowledge distillation framework     on the basis of various GNNs.\n\n"
                },
                "subsection 2.2": {
                    "name": "Knowledge Distillation for GNNs",
                    "content": "\n\n    Knowledge distillation (KD) has been widely studied in computer vision \\cite{chen2017learning,liu2018multi}, natural language processing \\cite{kim2016sequence,liu2019improving}, etc. \n    Recently, a few KD methods have proposed for GNNs. \n    LSP \\cite{yang2020distilling} transferred the topological structure knowledge from a pre-trained deeper teacher GNN to a shallower student GNN.\n    CPF \\cite{yang2021extract} designed a student architecture that is a combination of a parameterized label propagation and MLP layers.\n    GFKD \\cite{deng2021graph} proposed a method to generate fake graphs and distilled knowledge from a teacher GNN model without any training data involved.\n    The authors in \\cite{yan2020tinygnn} designed an efficient GNN model by utilizing the information from peer nodes to model the local structure explicitly and distilling the neighbor structure information from a deeper GNN implicitly.\n    The work in \\cite{chen2020self} studied a self-distillation framework, and proposed an adaptive discrepancy retaining  regularizer to empower the transferability of knowledge.\n        % GNN-SD \\cite{chen2020self} proposed to distill knowledge from the shallow layers to the deep layers in one GNN model.\n    RDD \\cite{zhang2020reliable} was a semi-supervised knowledge distillation method for GNNs. It online learnt a complicated teacher GNN model by ensemble learning, and distilled  knowledge from the generated teacher model into the student model. \n    Different from them, we focus on studying a new free-direction knowledge distillation architecture, with the purpose of dynamically exchanging knowledge between two shallower GNNs. \n% {\\color{blue}\n% \\subsection{Ensemble learning}\n% The basic idea of ensemble learning is to ensemble multiple peer models to derive a powerful model. The representative methods include Bagging \\cite{oza2001online}, Boosting \\cite{schapire2013explaining},  Stacking \\cite{pavlyshenko2018using}, etc. \n% In common, most of ensemble learning methods derive a more accurate output by fusing the outputs of a huge number of peer models.\n% Thus, despite the derived ensembled model has good performance, the high computational and storage cost is inevitable. \n% Our method is different from ensemble learning, we don't ensemble many GNNs and use their outputs to derive a powerful model. Instead, we use two GNNs to exchange learned knowledge for mutual improvement. \n% }\n\n"
                },
                "subsection 2.3": {
                    "name": "Reinforcement Learning",
                    "content": "\n    Reinforcement learning aims at training agents to make optimal decisions by learning from interactions with the environment \\cite{arulkumaran2017deep}. Reinforcement learning mainly has two genres \\cite{arulkumaran2017deep}: value-based methods and policy-based methods. Value-based methods estimate the expected reward of actions \\cite{mnih2015human}, while policy-based methods take actions according to the output probabilities of the agent \\cite{williams1992simple}. There is also a hybrid of this two genres,  called the actor-critic architecture \\cite{haarnoja2018soft}. The actor-critic architecture utilizes the value-based method as a value function to estimate the expected reward, and employs the policy-based method as a policy search strategy to take actions. \n    Until now,  reinforcement learning has been taken as a popular tool to solve various tasks, such as recommendation systems \\cite{zheng2018drn}, anomaly detection \\cite{oh2019sequential}, multi-label learning \\cite{chen2018recurrent}, etc.\n    In this paper, we explore  reinforcement learning for graph data based knowledge distillation. \n\n\n"
                }
            },
            "section 3": {
                "name": "METHODOLOGY",
                "content": "\n    In this section, we elaborate the details of our FreeKD framework that is shown in Figure \\ref{main}.\n    % As shown in Figure \\ref{main}, our framework mainly consists of three modules: A  node judge module aims at dynamically deciding the directions of knowledge distillation via reinforcement learning to capture the complementary information for each node;\n    % After determining the direction for each node, an agent-guided node knowledge transfer module attempts to propagate the soft label from one node of one network to the corresponding node of the other network; An agent-guided structure knowledge transfer module aims to transfer the neighborhood relations of the nodes which are obtained based on our node judge module.\n    Before introducing it, we first give some notations and  preliminaries.\n\n",
                "subsection 3.1": {
                    "name": "Preliminaries",
                    "content": "\n\tLet $\\mathbf{G}=(\\mathbf{V},\\mathbf{E},\\mathbf{X})$ denote a graph, where $\\mathbf{V}$ is the set of nodes and $\\mathbf{E}$ is the set of edge. $\\ \\mathbf{X}\\in\\mathbb{R}^{N\\times d}$ is the feature matrix of nodes, where $N$ is the number of nodes and $d$ is the dimension of node features.  Let $\\mathbf{x}_i$ be the feature representation of node $i$ and $y_i$ be its class label.\n\tThe neighborhood set of node $i$ is ${\\mathcal{N}(i)}=\\{\\ j\\in \\mathbf{V}\\ |\\ (i,j)\\in \\mathbf{E}\\ \\}$.\n\t%The node classification task aims to map every node to a class label. \n\tCurrently, graph neural networks (GNNs) have become one of the most popular models for handling graph data.\n\t GNNs can learn the embedding $\\mathbf{h}_i^{(l)}$  for node $i$ at the $l$-th layer  by the following formula:\n\\begin{align}\n  \\mathbf{h}_i^{(l)}=AGGREGATOR(\\mathbf{h}_i^{(l-1)},\\{\\mathbf{h}_j^{(l-1)}|\\ j\\in{\\mathcal{N}(i)}\\},{\\ \\mathbf{W}^{(l)}}),\n\\end{align}\nwhere $AGGREGATOR$ is an aggregation function, and it can be defined in many forms, e.g., mean aggregator \\cite{hamilton2017inductive}.  $\\mathbf{W}^{(l)}$ is the learnt parameters in the  $l$-th layer of the network. The initial feature of each node $i$ can be used as the input of the first layer, i.e., $\\mathbf{h}_i^{(0)}=\\mathbf{x}_i$.\n\n%After learning the node representations, several  fully-connected layers can serve as a projection head to map the node representations to the probability distributions of the classes. Finally, the cross entropy loss is often used for optimizing the network.\n\n% The cross entropy loss is often used for optimizing the network,  defined as:\n\n% \\begin{equation}\n%     L_{CE}=-\\sum_{i=1}^{N}\\sum_{j=1}^{M}{I(y_i,j)log(p^j_{i})},\n% \\end{equation}\n% where $M$ is the number of classes, and $p^j_{i}$ denotes the predicted probability of node $i$ belonging to class $j$. $I(y_i,j)$ is the indicator function defined as:\n% \\begin{equation}\n% I(y_i,j)=\\begin{cases}\n% 0,& \\text{$y_i\\neq j$}\\\\\n% 1,& \\text{$y_i=j$}.\n% \\end{cases}\n% \\end{equation}\n\nBeing orthogonal to those works developing various GNN models, our goal is to explore a new knowledge distillation framework for promoting the performance of GNNs, while addressing the issue involved because of producing a deeper teacher GNN model in the existing KD methods. \n% Next, we will first introduce the node judge module which can dynamically determine the directions of knowledge distillation via reinforcement learning for each node.\n% {\\color{blue}\n% Next, we will first introduce our two-level reinforced knowledge judge module which can dynamically determine the knowledge to transfer between two GNNs.\n% }\n\n"
                },
                "subsection 3.2": {
                    "name": "Overview of Framework",
                    "content": "\nAs shown in Figure \\ref{cross}, we observe typical GNN models often have different performances at different nodes during training.\n% , i.e., one GNN obtains better performances at certain nodes but the other is superior at other nodes. \nBased on this observation, we intend to dynamically exchange useful knowledge between two shallower GNNs, so as to benefit from each other. \nHowever, a challenging problem is attendant upon that: how to decide the directions of knowledge distillation for different nodes during training. \nTo address this, we propose to manage the directions of knowledge distillation via reinforcement learning, where we regard the directions of knowledge transfer for different nodes as a sequential decision making problem \\cite{pednault2002sequential}. Consequently, we propose a free-direction knowledge distillation framework via a hierarchical reinforcement learning, as shown in Figure \\ref{main}.\nIn our framework, the hierarchical reinforcement learning can be taken as a reinforced knowledge judge that consists of two levels of actions: 1) Level 1, called node-level action, is used to decide the distillation direction of each node for propagating the soft label; 2) Level 2, called structure-level action, is used to determine which of  the local structures generated via node-level actions to be propagated. \n\n\t\n Specifically, the reinforced knowledge judge (we call it agent for convenience) interacts with the environment constructed by two GNN models in each iteration, as in Figure \\ref{main}.\n\tIt receives the soft labels and cross entropy losses for a batch of nodes, and regards them as its node-level states. \n\tThe agent then samples sequential node-level actions for nodes according to a learned policy network, where each action decides the direction of knowledge distillation for propagating node-level knowledge. \n\tThen, the agent receives the structure-level states and produces structure-level actions to decide which of the local structures generated on the basis of node-level actions to be propagated.\n\tAfter that, the two GNN models are trained based to the agent's actions with a new loss function.\n\tFinally, the agent calculates the reward for each action to train the policy network, where the agent's target is to maximize the expected reward.\n\tThis process is repeatedly iterated until convergence.\n% \tNote that our method is different from  traditional reinforcement learning \\cite{arulkumaran2017deep}, which samples one action based on one state. Similar to \\cite{yuan2021reinforced,wang2019minimax}, we sample a batch of actions based on a batch of states, and received the delayed reward after two GNNs being updated.\n\t\n\n\tWe first  give some notations for convenient presentation, before introducing how to distill both node-level and structure-level knowledge.\n\tLet $\\Phi$ and $\\Psi$ denote two GNN models, respectively.  $\\mathbf{h}_i^\\Phi$ and $\\mathbf{h}_i^\\Psi$ denote the learnt representations of node $i$ obtained by $\\Phi$ and $\\Psi$, respectively.\n\tLet $\\mathbf{p}_i^\\Phi$ and $\\mathbf{p}_i^\\Psi$ be the predicted probabilities of the two GNN models for node $i$ respectively. We regard them as the soft labels.\n\tIn addition,  $L_{CE}^\\Phi(i)$ and $L_{CE}^\\Psi(i)$ denote the cross entropy losses of node $i$ in $\\Phi$ and $\\Psi$, respectively.\n\n\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Agent-guided Node-level Knowledge Distillation",
                    "content": "\nIn this section, we introduce our reinforcement learning based strategy  to dynamically distill node-level knowledge  between two GNN models. \n\n",
                    "subsubsection 3.3.1": {
                        "name": "Node-level State",
                        "content": "  We concatenate the following features as the node-level state vector $s_i^{[1]}$ for node $i$: \n\n(1) Soft label vector of node $i$ in GNN $\\Phi$.\n\n(2) Cross entropy loss of node $i$ in GNN $\\Phi$.\n\n(3) Soft label vector of node $i$ in GNN $\\Psi$.\n\n(4) Cross entropy loss of node $i$ in GNN $\\Psi$.\n\n\tThe first two kinds of features are based on the intuition that the cross entropy loss and soft label can quantify the useful knowledge for node $i$ in GNN $\\Phi$ to some extent. \n\tThe last two kinds of features have the same function for $\\Psi$.\n\tSince these features can measure the knowledge each node contains to some extent, we use them as the feature of the node-level state for predicting the node-level actions.\n \n\tFormally, the state $s_i^{[1]}$ for the node $i$ is expressed as:\n\\begin{equation}\\label{n-state}\n    \\mathbf{s}_i^{[1]}=CONCAT(\\mathbf{p}_i^\\Phi,L_{CE}^\\Phi(i),\\mathbf{p}_i^\\Psi,L_{CE}^\\Psi(i)),\n\\end{equation}\nwhere $CONCAT$ is the concatenation operation.\n\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Node-level Action",
                        "content": " \n% \tThe agent makes a decision by outputting the action $a_i$ which is 1 or 0. \n\tThe node-level action $a_i^{[1]}\\in\\{0,1\\}$ decides the direction of knowledge distillation for node $i$.\n\t$a_i^{[1]}=0$ means transferring knowledge from GNN $\\Phi$ to GNN $\\Psi$ at node $i$, while $a_i^{[1]}=1$ means the distillation direction from $\\Psi$ to  $\\Phi$. \n\tIf $a_i^{[1]}=0$, we define node $i$ in $\\Phi$ as \\emph{agent-selected node}, otherwise,  we define node $i$ in $\\Psi$ as \\emph{agent-selected node}.\n\tThe actions are sampled from the probability distributions produced by a node-level policy function $\\pi_\\mathbf{\\theta}$, where $\\mathbf{\\theta}$ is the trainable parameters in the policy network and $\\pi_\\mathbf{\\theta}\\left(s_i^{[1]},a_i^{[1]} \\right)$ means the probability to take action $a_i^{[1]}$ over the state $s_i^{[1]}$. In this paper, we adopt a three-layer MLP with the $tanh$ activation function as our node-level policy network.\n\n\n\n"
                    },
                    "subsubsection 3.3.3": {
                        "name": "Node-level Knowledge Distillation",
                        "content": " \n    After determining the direction of knowledge distillation for each node, the two GNN models can exchange beneficial node-level knowledge. We take Figure \\ref{transfer}(a) as an example to illustrate our idea. In Figure \\ref{transfer}(a), the agent-selected nodes $\\{v_1, v_4, v_5\\}$ in GNN $\\Phi$ will serve as the distilled nodes to transfer knowledge to the nodes  $\\{v_1, v_4, v_5\\}$  in GNN $\\Psi$. In the meantime, the agent-selected nodes $\\{v_2, v_3, v_6\\}$ in $\\Psi$ will be used as the distilled nodes to distill knowledge for the nodes $\\{v_2, v_3, v_6\\}$ in $\\Phi$. \n    In order to transfer node-level knowledge, we utilize the KL divergence to measure the distance between the soft labels of the same node in the two GNN models, and  propose to minimize a new loss function for each GNN model:  \n\\begin{align}\nL_{node}^\\Psi=\\sum_{i=1}^{N}{(1-a_i^{[1]}) KL(\\mathbf{p}_i^\\Phi||\\mathbf{p}_i^\\Psi)}\\label{ce1}\\\\\nL_{node}^\\Phi=\\sum_{i=1}^{N}{a_i^{[1]}KL(\\mathbf{p}_i^\\Psi||\\mathbf{p}_i^\\Phi)}\\label{ce2},\n\\end{align}\nwhere the value of $a_i^{[1]}$ is 0 or 1. \nWhen $a_i^{[1]}=0$, we use the $KL$ divergence to make the probability distribution $\\mathbf{p}_i^\\Psi$ match $\\mathbf{p}_i^\\Phi$ as much as possible, enabling the knowledge from $\\Phi$ to be transferred to $\\Psi$ at node $i$, and vice versa for $a_i^{[1]}=1$.\nThus, by minimizing the two loss functions $L_{node}^\\Phi$ and $L_{node}^\\Psi$, we can reach the goal of dynamically exchanging useful node-level knowledge between two GNN models and thus obtaining gains from each other. \n\n"
                    }
                },
                "subsection 3.4": {
                    "name": "Agent-guided Structure-level Knowledge Distillation",
                    "content": "\n\nAs we know, the structure information is important for graph learning. Thus, we attempt to dynamically transfer structure-level knowledge between two GNNs.\nIt is worth noting that we don't propagate all neighborhood information of one node as structure-level knowledge. Instead, we propagate a neighborhood subset of the node, which is comprised of agent-selected nodes. This is because we think agent-selected nodes contain more useful knowledge. \nWe take Figure \\ref{transfer}(b) as an example to illustrate it. $v_1$ is an agent-selected node in $\\Phi$. When transferring its local structure information to $\\Psi$, we only transfer the local structure composed of $\\{v_1, v_4, v_5\\}$.\nIn other words, the local structure of node $v_1$ we consider to transfer is made up of agent-selected nodes.  We call it agent-selected neighborhood set.\nMoreover, considering the knowledge of the local structure in graphs is not always reliable \\cite{zhang2020reliable,chen2021topology}, we design a reinforcement learning based strategy to distinguish which of the local structures to be propagated.\nNext, we introduce it in detail.\n\n",
                    "subsubsection 3.4.1": {
                        "name": "Structure-level State",
                        "content": " \nWe adopt the following features as the structure-level state vector $s_i^{[2]}$ for the local structure of node $i$:\n\n(1) Node-level state of node $i$.\n\n(2) Center similarity of node $i$'s agent-selected neighborhood set in the distilled network. \n\n(3) Center similarity of the same node set as node $i$'s agent-selected neighborhood set in the guided network. \n\nSince the node-level state contains much information for measuring the information of local structures, we use the node-level state as the first feature of structure-level state. \nAs \\cite{xie2020gnns} points out, the center similarity can indicate the performance of GNNs, where the center similarity measures the degree of similarity between the node and its neighbors. In other words, if center similarity  is high, the structure information should be more reliable. Thus, we also take center similarity as another feature.\nMotivated by \\cite{xie2020gnns}, we present a similar strategy to calculate the center similarity as:\n\n\n    First, let $\\mathbf{M}_i^\\Phi$ and $\\mathbf{M}_i^\\Psi$ denote the  agent-selected neighborhood set of node $i$ in $\\Phi$ and $\\Psi$, respectively. Formally, \n    \\begin{align}\n        &\\mathbf{M}_i^\\Phi=\\{v\\ |\\ a_i^{[1]}=0\\ , a_v^{[1]}=0, and\\ \\left(i,v\\right)\\in \\mathbf{E}\\}\\\\\n        &\\mathbf{M}_i^\\Psi=\\{v\\ |\\ a_i^{[1]}=1\\ , a_v^{[1]}=1, and\\ \\left(i,v\\right)\\in \\mathbf{E}\\}.\n    \\end{align}\nThen, we calculate the center similarity as:\n    \\begin{equation}\n    \\mathbf{u}_i\\!=\\!\n    \\begin{cases}\n    (\\frac{1}{|\\mathbf{M}_i^\\Phi|}\\sum\\limits_{v\\in \\mathbf{M}_i^\\Phi}{g(\\mathbf{h}_i^\\Phi,\\mathbf{h}_v^\\Phi)},\n    \\frac{1}{|\\mathbf{M}_i^\\Phi|}\\sum\\limits_{v\\in \\mathbf{M}_i^\\Phi}{g(\\mathbf{h}_i^\\Psi,\\mathbf{h}_v^\\Psi)})\n    ,\\emph{if} \\ \\  \\text{$a_i^{[1]}=0$}\\\\\n    (\\frac{1}{|\\mathbf{M}_i^\\Psi|}\\sum\\limits_{v\\in \\mathbf{M}_i^\\Psi}{g(\\mathbf{h}_i^\\Psi,\\mathbf{h}_v^\\Psi)},\n    \\frac{1}{|\\mathbf{M}_i^\\Psi|}\\sum\\limits_{v\\in \\mathbf{M}_i^\\Psi}{g(\\mathbf{h}_i^\\Phi,\\mathbf{h}_v^\\Phi)}),\n    \\emph{if} \\ \\  \\text{$a_i^{[1]}=1$},\\\\\n    \\end{cases}\\nonumber\n    \\end{equation}\nwhere $g$ can be an arbitrary similarity  function. Here we use the cosine similarity function $g(\\mathbf{x},\\mathbf{y})=cos(\\mathbf{x},\\mathbf{y})$. \n$\\mathbf{u}_i$ is a two-dimension vector. In order to better present what $\\mathbf{u}_i$ stands for, we take $v_1$ and $v_3$ in Figure \\ref{transfer}(b) as an example. \n$v_1$ is an agent-selected node in $\\Phi$, i.e., $a_1^{[1]}=0$, and $v_3$ is an agent-selected node in $\\Psi$, i.e., $a_3^{[1]}=1$. \nFor $\\mathbf{u}_1$, \nits first element $u_1^{(1)}$ is the center similarity between $v_1$ and \\{$v_4$, $v_5$\\} in the distilled network $\\Phi$, \nwhile its second element $u_1^{(2)}$ is the center similarity between $v_1$ and \\{$v_4$, $v_5$\\} in the guided network $\\Psi$.\nSimilarly, for $\\mathbf{u}_3$, $u_3^{(1)}$ measures the center similarity between $v_3$ and \\{$v_2$, $v_6$\\} in  $\\Psi$, and $u_3^{(2)}$ is  the center similarity between $v_3$ and \\{$v_2$, $v_6$\\} $\\Phi$.\nIn a word, the first element in $\\mathbf{u}_i$ measures the center similarity in the distilled network, and the second element measures the center similarity in the guided network.\n\n\n    \n    % The motivation to use the similarity between the representation of central node and its neighbors is based on homophily assumption \\cite{wang2021hagen,wang2021powerful}. ( i.e. nodes with same or similar class are tend to connect to each other, connected nodes are prone to have similar representations). If the similarity value between the embeddings of node $i$ and its neighbors in one GNN is low, that could indicate this GNN is uncertain and unstable for the local structure of node $i$ to a certain degree. In this case, the structure-level information might be not reliable enough to distill. Thus, the last two features could help to judge whether to transfer the structure-level knowledge.\n\n\tFinally, the structure-level state $s_i^{[2]}$ for the the local structure of node $i$ is expressed as:\n\\begin{equation}\\label{s-state}\n    \\mathbf{s}_i^{[2]}=\n    CONCAT(\\mathbf{s}_i^{[1]},\\mathbf{u}_i),\n\\end{equation}\nwhere $CONCAT$ is the concatenation operation.\n\n    "
                    },
                    "subsubsection 3.4.2": {
                        "name": "Structure-level Action",
                        "content": " \n    Structure-level action $a_i^{[2]}\\in\\{0,1\\}$ is the second level action that determines which of the structure-level knowledge to be propagated. \n    If $a_i^{[2]}=1$, the agent decides to transfer the knowledge of the local structure encoded in the agent-selected neighborhood set of node $i$, otherwise it will not be transferred.\n    Similar to the node-level policy network, the structure-level policy network $\\pi_\\mathbf{\\delta}$ that produces structure-level actions is also comprised of a three-layer MLP with the $tanh$ activation function.\n\n"
                    },
                    "subsubsection 3.4.3": {
                        "name": "Structure-level Knowledge Distillation",
                        "content": "\nWe first introduce how to distill structure-level knowledge from $\\Phi$ to $\\Psi$. The method for distilling from $\\Psi$ to $\\Phi$ is the same. First, we define the similarity between two agent-selected nodes $i$ and $j$ by:\n\t\\begin{align}\n    \\hat{s}_{ij}^\\Phi=\\frac{e^{g\\left(\\mathbf{h}_i^\\Phi,\\mathbf{h}_j^\\Phi\\right)}}{\\sum_{v\\in \\mathbf{M}_i^\\Phi} e^{g\\left(\\mathbf{h}_i^\\Phi,\\mathbf{h}_v^\\Phi\\right)}},\\ \\ \n    \\hat{s}_{ij}^\\Psi=\\frac{e^{g\\left(\\mathbf{h}_i^\\Psi,\\mathbf{h}_j^\\Psi\\right)}}{\\sum_{v\\in \\mathbf{M}_i^\\Phi} e^{g\\left(\\mathbf{h}_i^\\Psi,\\mathbf{h}_v^\\Psi\\right)}},\n\\end{align}\nwhere $g$ is the cosine similarity function.\n\nTo transfer structure-level knowledge, we propose a new loss function to be minimized as:\n\\begin{align}\\label{struct1}\n    L_{struct}^\\Psi=\\sum_{i=1}^{N}{(1-a_i^{[1]})a_i^{[2]} KL(\\mathbf{\\hat{s}}_i^\\Phi||\\mathbf{\\hat{s}}_i^\\Psi)},\n\\end{align}\nwhere $\\mathbf{\\hat{s}}_i^\\Phi=[\\hat{s}_{i1}^\\Phi, \\cdots, \\hat{s}_{iC_i^{\\Phi}}^\\Phi]$ and $\\mathbf{\\hat{s}}_i^\\Psi=[\\hat{s}_{i1}^\\Psi, \\cdots, \\hat{s}_{iC_i^{\\Phi}}^\\Psi]$, $C_i^{\\Phi}$ is the size of  $\\mathbf{M}_i^\\Phi$. $\\mathbf{\\hat{s}}_i^\\Phi$ represents the distribution of the similarities between node $i$ and its agent-selected neighborhoods in $\\Phi$, while $\\mathbf{\\hat{s}}_i^\\Psi$ represents the distribution of the similarities between node $i$ and its corresponding neighborhoods in $\\Psi$.\nIf the local structure of node $i$ is decided to transfer, we adopt the $KL$ divergence to make $\\mathbf{\\hat{s}}_i^\\Psi$ match $\\mathbf{\\hat{s}}_i^\\Phi$, so as to transfer structure-level knowledge.\nSimilarly, we can propose another new loss function for distilling knowledge from $\\Psi$ to $\\Phi$ as:\n\\begin{equation}\\label{struct2}\n    L_{struct}^\\Phi=\\sum_{i=1}^{N}{a_i^{[1]}a_i^{[2]}KL(\\mathbf{\\bar{s}}_i^\\Psi||\\mathbf{\\bar{s}}_i^\\Phi)},\n\\end{equation}\nwhere $\\mathbf{\\bar{s}}_i^\\Phi=[\\bar{s}_{i1}^\\Phi, \\cdots, \\bar{s}_{iC_i^{\\Psi}}^\\Phi]$ and $\\mathbf{\\bar{s}}_i^\\Psi=[\\bar{s}_{i1}^\\Psi, \\cdots, \\bar{s}_{iC_i^{\\Psi}}^\\Psi]$, $C_i^{\\Psi}$ is the size of  $\\mathbf{M}_i^\\Psi$. $\\bar{s}_{ij}^\\Phi$ and $\\bar{s}_{ij}^\\Psi$ are defined as:\n\\begin{align}\n    \\bar{s}_{ij}^\\Phi=\\frac{e^{g\\left(\\mathbf{h}_i^\\Phi,\\mathbf{h}_j^\\Phi\\right)}}{\\sum_{v\\in \\mathbf{M}_i^\\Psi} e^{g\\left(\\mathbf{h}_i^\\Phi,\\mathbf{h}_v^\\Phi\\right)}}, \\ \\ \n    \\bar{s}_{ij}^\\Psi=\\frac{e^{g\\left(\\mathbf{h}_i^\\Psi,\\mathbf{h}_j^\\Psi\\right)}}{\\sum_{v\\in \\mathbf{M}_i^\\Psi} e^{g\\left(\\mathbf{h}_i^\\Psi,\\mathbf{h}_v^\\Psi\\right)}}.\n\\end{align}\n\n\t\n\t\n\n\nBy jointly minimizing (\\ref{struct1}) and (\\ref{struct2}), we can dynamically exchange structure-level knowledge  between $\\Phi$ and $\\Psi$.\n\n\n\n\n"
                    }
                },
                "subsection 3.5": {
                    "name": "Optimizations",
                    "content": "\nIn this section, we introduce the optimization procedure of our method.\nThe detailed training procedure is in Appendix \\ref{procedure}. \n% \\subsubsection{Reward} \n% Since distilling knowledge from one node will have a global influence on the update of the whole network, \n% we  utilize the model performance different before and after update as the reward. The motivation of adopting this reward is to encourage the agent to achieve better performance compared to previous training epoch. The reward $R_i$ for the action taken at node $i$ is defined as:\n%  \\begin{equation}\\label{reward}\n%      R_i\\!=\\!-\\frac{\\sum_{u\\in \\mathbf{B}}{\\!(\\!\\Delta L_{CE}^\\Phi(u) \\!+\\!\\Delta L_{CE}^\\Psi(u))}}{\\left|\\mathbf{B}\\right|}\n%      \\!-\\!\n%      \\gamma\\frac{\\sum_{v\\in {\\mathcal{N}_i}}{\\!(\\!\\Delta L_{CE}^\\Phi(v)\\!+\\!\\Delta L_{CE}^\\Psi(v))}}{\\left|{\\mathcal{N}_i}\\right|},\\nonumber\n%  \\end{equation}\n% where $\\Delta L_{CE}^\\Phi(i)$ ,$\\Delta L_{CE}^\\Psi(i)$ are the performance difference of $\\Phi$ and $\\Psi$ at node $i$ respectively. $\\mathbf{B}$ is the batch consisting of nodes. %Here we use the batch instead of all nodes to approximately measure the influence of updating node $i$, and \n% Following \\cite{yuan2021reinforced, liang2021reinforced},\n% we use the negative value of the cross entropy loss as the performance of the model. The reward for an action  $a_i$ consists of two parts: The first part is the average performance difference for a batch of nodes, which measures global effects that the action $a_i$ brings on the GNN model; The second part is the average performance  difference of node $i$'s neighborhoods, which models local effects of $a_i$. $\\gamma$ is a trade-off parameter to balance the contributions between them. \n\n\n% \\subsubsection{Optimization for Policy Networks} \n% Following previous studies \\cite{liu2021rmm}, the gradient of expected cumulative reward $\\mathrm{\\nabla}_{\\theta,\\delta}J$ could be computed as follows:\n% \\begin{equation}\\label{grad}\n% \\mathrm{\\nabla}_{\\theta,\\delta}J\\!=\\!\n% \\frac{1}{|\\mathbf{B}|}\\!\n% \\sum_{i\\in\\mathbf{B}\\!}{R_i \\mathrm{\\nabla}_{\\theta,\\delta}\\!\\log(\\pi_\\theta(\\mathbf{s}_i^{[1]}\\!,a_i^{[1]})\\pi_\\delta(\\mathbf{s}_i^{[2]}\\!,a_i^{[2]})}),\n% \\end{equation}\n% where $\\theta$ and $\\delta$ is the learned parameters of the node-level policy network and structure-level policy network, respectively. \n% Finally, we update the parameters of policy networks by gradient ascent: %\\cite{williams1992simple} as follows:\n% \\begin{equation}\\label{update}\n%     \\theta\\gets\\theta+\\eta\\mathrm{\\nabla}_\\theta J,\\ \\   \\delta\\gets\\delta+\\eta\\mathrm{\\nabla}_\\delta J,\n% \\end{equation}\n% where $\\eta$ is the learning rate for reinforcement learning.\n\n",
                    "subsubsection 3.5.1": {
                        "name": "Reward",
                        "content": " \nFollowing \\cite{wang2019minimax}, our actions are sampled in batch, and obtain the delayed reward after two GNNs being updated according to a batch of sequential actions.\nSimilar to \\cite{ liang2021reinforced}, we utilize the performance of the models after being updated as the reward.\nWe use the negative value of the cross entropy loss to measure the performance of the models as in \\cite{yuan2021reinforced, liang2021reinforced}, defined as:\n \\begin{equation}\\label{reward}\n     R_i=-\\frac{\\sum\\limits_{u\\in \\mathbf{B}}{(L_{CE}^\\Phi(u){+L}_{CE}^\\Psi(u))}}{\\left|\\mathbf{B}\\right|}-\\gamma\\frac{\\sum\\limits_{v\\in {\\mathcal{N}_i}}{(L_{CE}^\\Phi(v){+L}_{CE}^\\Psi(v))}}{\\left|{\\mathcal{N}_i}\\right|},\n \\end{equation}\nwhere $\\gamma$ is a hyper-parameter.\n$R_i$ is the reward for the action taken at node $i$, and $\\mathbf{B}$ is a batch set of nodes from the training set.\nThe reward for an action  $a_i$ consists of two parts: The first part is the average performance for a batch of nodes,  measuring the global effects that the action $a_i$ brings on the GNN model; The second part is the average performance of the neighborhoods of node $i$,  in order to model the local effects of $a_i$. \n\n"
                    },
                    "subsubsection 3.5.2": {
                        "name": "Optimization for Policy Networks",
                        "content": " \nFollowing previous studies about hierarchical reinforcement learning \\cite{liu2021rmm}, the gradient of expected cumulative reward $\\mathrm{\\nabla}_{\\theta,\\delta}J$ could be computed as follows:\n\\begin{equation}\\label{grad}\n\\mathrm{\\nabla}_{\\theta,\\delta}J\\!=\\!\n\\frac{1}{|\\mathbf{B}|}\\!\n\\sum_{i\\in\\mathbf{B}\\!}{\\!(R_i\\!-\\!b_i)\\mathrm{\\nabla}_{\\theta,\\delta}\\!\\log(\\pi_\\theta(\\mathbf{s}_i^{[1]}\\!,a_i^{[1]})\\pi_\\delta(\\mathbf{s}_i^{[2]}\\!,a_i^{[2]})}),\n\\end{equation}\nwhere $\\theta$, $\\delta$ is the learned parameters of the node-level policy network and structure-level policy network, respectively. Similar to \\cite{lai2020policy}, to speed up convergence and reduce variance ,  we also add a baseline reward $b_i$ that is the rewards at node $i$ in the last epoch. The motivation behind this is to encourage the agent to achieve better performance than that of the last epoch. Finally, we update the parameters of policy networks by gradient ascent \\cite{williams1992simple} as:\n\\begin{equation}\\label{update}\n    \\theta\\gets\\theta+\\eta\\mathrm{\\nabla}_\\theta J,\\ \\   \\delta\\gets\\delta+\\eta\\mathrm{\\nabla}_\\delta J,\n\\end{equation}\nwhere $\\eta$ is the learning rate for reinforcement learning.\n\n\n"
                    },
                    "subsubsection 3.5.3": {
                        "name": "Optimization for GNNs",
                        "content": "\nWe minimize the following loss functions for optimizing  $\\Phi$ and $\\Psi$, respectively:\n\\begin{equation}\\label{loss1}\n    L^\\Phi=L_{CE}^\\Phi+\\mu L_{node}^\\Phi+\\rho L_{struct}^\\Phi\n\\end{equation}\n\\begin{equation}\\label{loss2}\n    L^\\Psi=L_{CE}^\\Psi+\\mu L_{node}^\\Psi+\\rho L_{struct}^\\Psi,   \n\\end{equation}\nwhere $L_{CE}^\\Phi$, $L_{CE}^\\Psi$ are the cross entropy losses for  $\\Phi$ and  $\\Psi$, respectively. $L_{node}^\\Phi$ and $L_{node}^\\Psi$ are two node-level knowledge distillation losses. $L_{struct}^\\Phi$ and $L_{struct}^\\Psi$ are two structure-level distillation losses. \n$\\mu$ and $\\rho$ are two trade-off parameters.\n% By jointly minimizing them, we can mutually distill useful knowledge from the two GNN models, so as to benefit from each other. \n\n\n\n\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "EXPERIMENTS",
                "content": "\nTo verify the effectiveness of our proposed FreeKD, we perform the experiments on five benchmark datasets of different domains and on GNN models of different architectures. More implementation details are given in Appendix \\ref{details}.\n\t\n",
                "subsection 4.1": {
                    "name": "Experimental Setups",
                    "content": "\n",
                    "subsubsection 4.1.1": {
                        "name": "Datasets",
                        "content": " \n We use five widely used benchmark datasets to evaluate our methods.\n Cora \\cite{sen2008collective} and Citeseer \\cite{sen2008collective} are two citation datasets where  nodes represent documents and   edges represent citation relationships. \nChameleon \\cite{rozemberczki2021multi} and Texas \\cite{pei2020geom} are two web network datasets where  nodes stand for web pages and  edges show their hyperlink relationships. \nThe PPI dataset \\cite{hamilton2017inductive} consists of 24 protein–protein interaction graphs, corresponding to different human tissues.\nIn Appendix \\ref{datasets}, we give more information about these datasets.\nFollowing \\cite{chen2018fastgcn} and \\cite{huang2018adaptive}, we use 1000 nodes for testing, 500 nodes for validation, and the rest for training on the Cora and Citeseer datasets.\nFor Chameleon and Texas datasets, we randomly split nodes of each class into 60\\%, 20\\%, and 20\\% for training, validation and testing respectively, following \\cite{pei2020geom} and \\cite{chen2020simple}.\nFor the PPI dataset, we use 20 graphs for training, 2 graphs for validation, and 2 graphs for testing, as in  \\cite{chen2020simple}.\nFollowing previous works \\cite{velivckovic2017graph,pei2020geom} , we study the transductive setting on the first four datasets, and the inductive setting on the PPI dataset.\nIn the tasks of transductive setting, we predict the labels of the nodes observed during training, whereas in the task of inductive setting, we predict the labels of nodes in never seen graphs before. \n\n\n\n\n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Baselines",
                        "content": " In the experiment, we adopt three popular GNN models, GCN \\cite{kipf2016semi}, GAT \\cite{velivckovic2017graph}, GraphSAGE \\cite{hamilton2017inductive}, as our basic models in our method.\nOur framework aims to promote the performance of these GNN models. Thus, these three GNN models can be used as our baselines.\nSince we propose a free-direction knowledge distillation framework, we also compare  with five typical knowledge distillation approaches proposed recently, including KD \\cite{hinton2015distilling}, LSP \\cite{yang2020distilling}, CPF \\cite{yang2021extract}, RDD \\cite{zhang2020reliable}, and GNN-SD \\cite{chen2020self},  to further verify the effectiveness of our method.\nFollowing \\cite{chen2018fastgcn,hamilton2017inductive}, we use the Micro-F1 score as the evaluation measure throughout the experiment. \n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Overall Evaluations on Our Method",
                    "content": "\n\tIn this subsection, we evaluate our method using three popular GNN models, GCN \\cite{kipf2016semi}, GAT \\cite{velivckovic2017graph}, and GraphSAGE \\cite{hamilton2017inductive}. \n\tWe arbitrarily select two networks from the above three models as our basic models $\\Phi$ and $\\Psi$, and perform our method FreeKD\n\t, enabling them to learn from each other. \n\tNote that we do not perform GCN on the PPI dataset, because of the inductive setting.\n\n    \n    %Table 2 shows the experiment results. We denote the GNN $\\Phi$ as net 1 and GNN $\\Psi$ and net 2. The “Single” represents the original training method which trains a single GNN model independently. The “BKDR” represent our training method which makes two GNNs learning from the good aspects of each other. We keep all the settings and parameters consistent for training independently and training by our method for fairness. And we explore BKDR can bring how much improvements to GNNs.\n\t\n\tTable \\ref{trasductive} and Table \\ref{inductive} report the experimental results.\n\tAs shown in Table \\ref{trasductive} and Table \\ref{inductive}, our FreeKD can consistently promote the performance of the basic GNN models in a large margin on all the datasets.\n\tFor instance, our method can achieve more than $4.5\\%$ improvement by mutually learning from two GCN models on the Chameleon dataset, compared with the single GCN model. In summary, for the transductive learning tasks,  our method improves the performance by 1.01\\% $\\sim$ 1.97\\% on the Cora and Citeseer datasets and 1.08\\% $\\sim$ 4.61\\% on the Chameleon and Texas datasets, compared with the corresponding GNN models. \n\tFor the inductive learning task, our method improves the performance by 1.31\\% $\\sim$ 3.11\\% on the PPI dataset dataset.\n\tIn addition, we observe that two GNN models either sharing the same architecture or using different architectures can both benefit from each other by using our method, which shows the efficacy to various GNN models.\n\n"
                },
                "subsection 4.3": {
                    "name": "Comparison with Knowledge Distillation",
                    "content": "\n% Since our method is related to knowledge distillation to some extent, we also compare with three existing knowledge distillation methods, LSP, CPF, and RDD, to further verify the effectiveness of our method. For these methods, we respectively use the following GNN models as the teacher model:  GCN \\cite{kipf2016semi}, GraphSAGE \\cite{hamilton2017inductive}, GAT \\cite{velivckovic2017graph}, GIN \\cite{xu2018powerful}, SGC \\cite{wu2019simplifying}, APPNP \\cite{klicpera2018predict}, and Cluster-GCN \\cite{chiang2019cluster}. For LSP, the student network is the same with the teacher network; For CPF, we use the student network proposed in the original paper \\cite{yang2021extract}, which is a combination of parameterized label propagation and feature transformation MLP; For RDD, we use two teacher networks to form one new teacher network by ensemble learning for knowledge distillation; For our method FreeKD, we use the models sharing the same architecture as our two basic models.\n\n% Table \\ref{kd} lists the results.\n% As shown in Table \\ref{kd}, our FreeKD consistently outperforms all the other knowledge distillation approaches for node classification.\n% We notice that all knowledge distillation approaches achieve better performance than the single GNN model, which demonstrates the effectiveness of knowledge distillation.\n% An interesting point is that our method obtains better performance than LSP that uses the same model as the teacher network and the student network. This illustrates that our bi-directional knowledge distillation can obtain more gains to boost the performance of GNN models, compared to that distilling knowledge from one single direction.\n% In addition, FreeKD is better than CPF and RDD which are two comprehensively designed teacher-student architectures. It further verifies the effectiveness of FreeKD. \n\n\nSince our method is related to knowledge distillation, we also compare with the existing knowledge distillation methods to further verify effectiveness of our method. In this experiment, we first compare with three traditional knowledge distillation methods, KD \\cite{hinton2015distilling}, LSP \\cite{yang2020distilling}, CPF \\cite{yang2021extract} distilling knowledge from a deeper and stronger  teacher GCNII model \\cite{chen2020simple} into a shallower student  GAT model. \nThe structure details of GCNII and GAT could be found in the Appendix \\ref{details}.\n%Note that for CPF, we use the student network proposed in the original paper \\cite{yang2021extract}, which is a combination of parameterized label propagation and feature transformation MLP.\nIn addition, we also compare with an ensemble learning method, RDD \\cite{zhang2020reliable}, where a complex teacher network is generated by ensemble learning for distilling knowledge. Finally, we take GNN-SD \\cite{chen2020self} as another baseline, which distills knowledge  from shallow layers into deep layers in one GNN.\nFor our FreeKD, we take two GAT sharing the same structure as the basic models.\n\n\nTable \\ref{kd} lists the experimental results. Surprisingly, our FreeKD perform comparably or even better than the traditional knowledge distillation methods (KD, LSP, CPF) on all the datasets. This demonstrates the effectiveness of our method, as they  distill knowledge from the stronger teacher GCNII while we only mutually distill knowledge between two shallower GAT.\n%do not require a large well-optimized teacher GNN for distillation. \n% An interesting point is that our method obtains better performance than KD and LSP. This illustrates that our free-directional knowledge distillation can obtain more gains to boost the performance of GNN models, compared to that distilling knowledge from one single direction.\nIn addition, our FreeKD consistently outperforms GNN-SD and RDD, which further illustrates the effectiveness of our proposed  FreeKD.\n\n\t\n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study",
                    "content": "\nWe perform ablation study to verify the effectiveness of the components in our method. We use GCN as the basic models $\\Phi$ and $\\Psi$ in our method, and conduct the experiments on two datasets of different domains, Chameleon and Cora. \nWhen setting $\\rho=0$, this means that we only transfer the node-level knowledge. We denote it FreeKD-node for short.\nTo evaluate our reinforcement learning based node judge module, we design three variants:\n\\begin{itemize}\n    \\item FreeKD-w.o.-judge: our FreeKD  without using the agent.\n     $\\Phi$ and $\\Psi$ distills knowledge for each node from each other.\n    \\item FreeKD-loss: our FreeKD  without using the reinforced knowledge judge. It determines the directions of knowledge distillation only relying on the cross entropy loss.\n    \\item FreeKD-all-neighbors: our FreeKD selecting the directions of node-level knowledge distillation via node-level actions, but using all neighborhood nodes as the local structure.\n    \\item FreeKD-all-structures: our FreeKD selecting the directions of node-level knowledge distillation, but without using structure-level actions for  structure-level knowledge distillation.\n\\end{itemize}\n\n\n\nTable \\ref{ablation_study} shows the  results. \nFreeKD-node is better than GCN, showing that mutually transferring node-level knowledge via reinforcement learning is useful for boosting the performance of GNNs.\nFreeKD obtain better results than FreeKD-node. It illustrates distilling structure knowledge by our method is beneficial to GNNs.\nFreeKD achieves better performance than FreeKD-w.o.-judge, illustrating dynamically determining the knowledge distillation direction is important.\nIn addition, FreeKD outperforms FreeKD-loss. This shows that directly using the cross entropy loss to decide the directions of knowledge distillation is sub-optimal. As stated before, this heuristic strategy only considers the performance of the node itself, but neglects the influence of the node on other nodes.\nAdditionally, FreeKD has superiority over FreeKD-all-neighbors, demonstrating that transferring part of neighborhood information selected by our method is more effective than transferring all neighborhood information for GNNs. \nFinally, FreeKD obtains better performance than FreeKD-all-structures, which indicates our reinforcement learning based method can transfer more reliable structure-level knowledge. \nIn summary, these results demonstrate our proposed knowledge distillation framework with a hierarchical reinforcement learning strategy is effective.\n\n\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Visualizations",
                    "content": "\n    We further intuitively show the effectiveness of the reinforced knowledge judge to dynamically decide the directions of knowledge distillation.\n    We set GCN as $\\Phi$ and GraphSAGE as $\\Psi$, and train our FreeKD on the Cora dataset.\n    Then, we poison  $\\Phi$ by adding random Gaussian noise with a standard deviation $\\sigma$ to its model parameters. Finally, we visualize the agent’s output, i.e., node-level policy probabilities $\\pi_\\mathbf{\\theta}\\left(\\mathbf{s}_i^{[1]},0\\right)$  and $\\pi_\\mathbf{\\theta}\\left(\\mathbf{s}_i^{[1]},1\\right)$ at node $i$ for $\\Phi$ and $\\Psi$, respectively. To better visualize, we show a subgraph composed of the first 30 nodes and their neighborhoods. \n    \n\n\n\n\n\n    Figure \\ref{vis} shows the results using different standard deviations $\\sigma$. \n    In Figure \\ref{vis} (a), (b), and (c), the higher the probability output by the agent is, the redder the node is. And this means that the probability for the node in this network to serve as a distilled node to transfer knowledge to the corresponding node of the other network is higher.\n\tAs shown in Figure \\ref{vis} (a), when without adding noise, the degrees of the red color in $\\Phi$ and $\\Psi$ are comparable. As the noise is gradually increased in $\\Phi$, the red color becomes more and more light in $\\Phi$, but an opposite case happens in $\\Psi$, as shown in Figure \\ref{vis} (b) and (c). This is because the noise brings negative influence on the outputs of the network, leading to inaccurate soft labels and large losses. In such a case, our agent can output low probabilities for the network $\\Phi$.  Thus, our agent can effectively determine the direction of knowledge distillation for each node.\n\t\n\n\t\n% \tFigure \\ref{vis}(a) visualize the result of add no Gaussian noise to net 1 GCN, where the degree of red for the whole subgraph are comparable. And for different nodes and local structures, GCN and GraphSAGE show different degree of red, because the agent could judge which GNN learns better and guide the bidirectional knowledge distillation between two GNNs at node-level and structure-level. Figure 3(b) and Figure 3(c) show the results of add Gaussian noise only to the parameters of net 1 GCN with standard deviation $\\sigma=0.5$ and $\\sigma=1.0$, respectively. We can observe that after adding the perturbations to poison net 1 GCN, net 2 GraphSAGE get redder and net 1 GCN become less red for the whole subgraph. The bigger the standard deviation $\\sigma$ is, the redder net 2 GraphSAGE is. And when $\\sigma=1.0$, net 1 GCN become almost totally yellow for the whole subgraph, since in this circumstance knowledge from net 1 GCN are almost harmful and not reliable. These visualizations indicates that the agent could commendably judge between beneficial knowledge and harmful knowledge, and measure how much one GNN learned relative better for node $i$ than another GNN, thus could effectively guide the bidirectional knowledge distillation between two GNNs.\n\n\n\n"
                },
                "subsection 4.6": {
                    "name": " Sensitivity and Convergence Analysis",
                    "content": "\nIn our method, there are three main hyper-parameters, i.e., $\\gamma$ in the reward function (\\ref{reward}), $\\mu$ and $\\rho$ in the loss function (\\ref{loss1}) and (\\ref{loss2}). We study the sensitivity of our method to these hyper-parameters on the Cora dataset.\nFirst, we investigate the impact of $\\gamma$ in the agent’s reward function on the performance of our method. \nAs shown in Table \\ref{gamma}, with the values of $\\gamma$ increasing, \nthe performance of our method will fall after rising. In the meantime, our method is not sensitive to $\\gamma$ in a relatively large range.\nWe also study the parameter sensitiveness of our method to $\\mu$ and $\\rho$. \nFigure \\ref{param}(a) shows the results. Our method is still not sensitive to these two hyper-parameters in a relatively large range.\nAdditionally, we analyze the convergence of our method. Figure \\ref{param}(b) shows the reward convergence curve. Our method is convergent after 100 epochs.\n% Thus, it is easy to set the values of these hyper-parameters in real-world applications. \n\n\n\n% \\begin{figure}[h]\n%   \\centering\n%   \\includegraphics[width=0.45\\linewidth]{figure/param_cora3}\n%   \\vspace{-0.1in}\n%   \\caption{Sensitivity study of $\\mu$ and $\\rho$ on the Cora dataset. \n%   }\n%   \\label{param}\n%   \\vspace{-0.18in}\n% \\end{figure}\n\n\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n% \\usepackage{multirow}\n% \\usepackage{graphicx}\n\n% \tRecall that $\\mu$ denotes the contribution factor of node-level distillation loss to the whole loss, while $\\rho$ denotes the contribution factor of structure-level distillation loss to the whole loss. \n%\tWe visualize the mean F1-score of two GCN models training by BKDR with different hyper-parameters $\\mu$ and $\\rho$. \n% \tWe choose the $\\mu$ and $\\rho$ ranging from 0 to 4.5 with a interval of 0.3. Figure 4 shows the results. The darker the grid is, the higher mean F1-score the grid has. \n% \tAs shown in Figure \\ref{param}, we observe that increasing $\\mu$ and $\\rho$ in appropriate range improve the performance of GNNs, indicating that the agent-guided knowledge transfer at node-level and at structure-level both play significant roles in BKDR. \n%Tthe results. Our method is still not sensitive to these two hyper-parameters in a relatively large range. Thus, it is easy to set the values of these hyper-parameters in real-world applications. \n% \tHowever, too large or too small values of $\\mu$ and $\\rho$ should be avoided in practice, which could result in bad performance. Because too small values could make the bidirectional distillation contribute too little during training, while too large values could lead to the ineffectiveness of cross entropy loss with ground-truth label. \n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\nIn this paper, we proposed a free-direction  knowledge distillation framework to enable two shallower GNNs to learn from each other, without requiring  a deeper well-optimized teacher GNN. Meanwhile, we devised a hierarchical reinforcement learning  mechanism to manage the directions of knowledge transfer, so as to distill knowledge from both node-level and structure-level aspects. Extensive experiments demonstrated the effectiveness of our method.\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\\begin{acks}\nThis  work  was  supported  by  the National Natural Science Foundation of China (NSFC) under Grants 62122013, U2001211.\n\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{myreference}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\clearpage\n\\appendix\n\n"
            },
            "section 6": {
                "name": "APPENDIX",
                "content": "\nTo support the reproducibility of our work, we introduce more details about training and experiments in the appendix.\n",
                "subsection 6.1": {
                    "name": "Training procedure",
                    "content": "\n\\label{procedure}\nAlgorithm 1 is the pseudo-code of the our FreeKD training procedure. \nThe GNNs and the agent closely interact with each other when training. For each batch, we first calculate the cross entropy loss for training GNNs. The node-level states for a batch of nodes are then calculated and feed into the node-level policy network. After that, we sample node-level actions from the policy probabilities produced by the agent to decide the directions of knowledge distillation between two GNNs. Then, the agent receives structure-level states from environment and produces structure-level actions to decide which of the local structures to be propagated.\nThe two-level states and actions are stored in the history buffer. Next, we train the two GNNs with the overall loss. After that, for the stored states and actions, we calculate the delayed rewards according to the performance of GNNs and update the policy network with gradient ascent. The GNNs and the agent are learned together and mutually improved.\n\\begin{algorithm}[h]\n  \\caption{Free-direction Knowledge Distillation Framework via Reinforcement Learning for GNNs}  \n  \\begin{algorithmic}[1]  \n    \\Require \n    graph $\\mathbf{G=(V,E,X)}$, label set $\\mathbf{\\mathcal{Y}}$, epoch number $L$;\n    \\Ensure\n    the predicted classes of nodes in the GNN models $\\Phi$ and $\\Psi$, the trained parameters of  $\\Phi$ and  $\\Psi$;\n    \\State Initialize  $\\Phi$ and  $\\Psi$;\n    \\State Initialize the policy networks in reinforcement learning;\n    \\For{each epoch $k$ in $1$ to $L$}  \n        \\For{each batch in epoch $k$}  \n\t\t    \\State calculate the cross entropy losses $L_{CE}^\\Phi$, $L_{CE}^\\Psi$;\n\t\t    \\State calculate node-level states for a batch of nodes by (\\ref{n-state});\n\t\t    \\State  sample node-level actions by $a_i^{[1]}\\sim\\pi_\\mathbf{\\theta}(\\mathbf{s}_i^{[1]},a_i^{[1]})$;\n\t\t    \\State derive structure-level states by (\\ref{s-state});\n\t\t    \\State  sample structure-level actions by $a_i^{[2]}\\sim\\pi_\\mathbf{\\delta}(\\mathbf{s}_i^{[2]},a_i^{[2]})$;\n\t\t    \\State store states and actions to history buffer $H$;\n\t\t    \\State  calculate $L_{node}^\\Phi$ and $L_{node}^\\Psi$  by (\\ref{ce1}) and (\\ref{ce2});\n\t\t    \\State calculate $L_{struct}^\\Phi$ and $L_{struct}^\\Psi$  by (\\ref{struct1}) and (\\ref{struct2});\n\t\t    \\State  calculate the overall losses $L^\\Phi$, $L^\\Psi$ by (\\ref{loss1}) and (\\ref{loss2});\n\t\t    \\State update the parameters of  $\\Phi$ by minimizing $L^\\Phi$;\n\t\t    \\State update the parameters of $\\Psi$ by minimizing $L^\\Psi$;\n\t\t    \\For{each state and action in buffer $H$}\n            \t\\State calculate the delayed rewards by (\\ref{reward});\n        \t\t\\State update parameters of the policy networks by (\\ref{update});\n            \\EndFor\n        \\EndFor  \n    \\EndFor     \n  \\end{algorithmic}  \n\\end{algorithm}  \n\n"
                },
                "subsection 6.2": {
                    "name": "Datasets Description and Statistics",
                    "content": "\n\\label{datasets}\nIn our experiments, we use five widely used public datasets of different domains to evaluate our method. Table \\ref{dataset} summarizes the statistics of the five datasets. In the following we introduce more information about these five datasets.\n\n\\textbf{Cora \\cite{sen2008collective} and Citeseer \\cite{sen2008collective}} are two citation networks. Cora is composed of papers in the machine learning domain, and Citeseer is about computer science publications. In these two datasets, nodes stand for documents while edges represent the citation relationship between documents. The node feature is the bag-of-words representation of document and the node label is the corresponding research domain.\n\n\\textbf{Chameleon \\cite{rozemberczki2021multi}  and Texas \\cite{pei2020geom}} are two web networks. In these two datasets, the nodes represent web pages and the edges are the hyperlinks between web pages. The node feature is the bag-of-words vector that represents the corresponding web page and the node label is its corresponding category.\n\n\\textbf{PPI \\cite{hamilton2017inductive}} describes the protein-protein interactions in human tissues. PPI is a widely used inductive learning dataset,  containing 24 protein-protein interactions graphs. Every graph is about a specific human tissue. PPI takes positional gene sets, motif gene sets and immunological signatures as the node feature, and takes the corresponding gene ontology set as the node label. \n\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Implementation Details",
                    "content": "\n\\label{details}\n All the results are averaged above 10 times and we run our experiments on GeForce RTX 2080 Ti GPU.\n We use the Adam optimizer \\cite{kingma2014adam} for training and adopt early stopping with a patience on validation sets of $150$ epochs. The initial learning rate is $0.05$ for GAT and  $0.01$ for GCN, GraphSAGE, and is decreased by multiplying $0.1$ every $100$ epochs. For the reinforced knowledge judge module, we set a fixed learning rate of $0.01$. We set the dropout rate to $0.5$ and the $l_2$ norm regularization weight decay to $0.0005$. The parameters of all GNN models are randomly initialized.  \nThe hyper-parameters  $\\mu$ and $\\rho$ in our method are searched from $\\{0.5, 1.0,1.5,2.0\\}$, and $\\gamma$ is searched from $\\{0.1,0.3,0.5\\}$. The node-level policy network and structure-level policy network are both 3-layer MLP with tanh activation function and the size of hidden layer is set to $\\{64,32\\}$.\nFor the transductive setting, the number of layers in GNNs is set to $2$ and\nthe hidden size is set to $64$. For the inductive setting, the number\nof layers in GNNs is set to $3$ and the hidden size is set to $256$. For GAT, the attention dropout probability is set to $0.5$ and the number of attention heads is set to $8$. For GraphSAGE, we use the mean aggregator to sample neighbors. \n% For GIN, the training epsilon is fixed to $0$ for better generalization as the original paper suggests. For APPNP, the propagation iteration steps is set to $8$ and the teleport probability is set to $0.2$. For SGC, the propagation iteration steps is set to $4$.\n\nIn the experiments of comparison with other knowledge distillation methods, the student model, i.e., GAT, is set to 2-layer with 64 hidden size in the transductive setting and 3-layer with 256 hidden size in the inductive setting.\nFor the teacher model GCNII, the number of layers is set to $32$ and the hidden size is set to $128$ in the transductive setting; in the inductive setting, the number of layers is set to $9$ and the hidden size is set to $2048$.\nFor all the compared knowledge distillation methods, we use the parameters as their original papers suggest and report their best results.\n\n\n"
                }
            }
        },
        "tables": {
            "trasductive": "\\begin{table*}\n\\caption{Results (\\%) of the compared approaches for node classification in the transductive settings on the Cora, Chameleon, Citeseer, and Texas datasets.\nThe values in the brackets denote the performance improvement of our FreeKD over the corresponding baselines.\nHere, we denote GraphSAGE as GSAGE for short.}\n\\vspace{-0.1in}\n\\setlength{\\tabcolsep}{0.8mm}{%\n\\begin{tabular}{@{}ccccccccccc@{}}\n\\toprule\n                            & \\multicolumn{2}{c}{}             & \\multicolumn{2}{c}{Cora}                         & \\multicolumn{2}{c}{Chameleon}                     & \\multicolumn{2}{c}{Citeseer}                    & \\multicolumn{2}{c}{Texas}             \\\\ \\midrule\n\\multicolumn{1}{c|}{Method} & \\multicolumn{2}{c|}{Basic Model}     & \\multicolumn{2}{c|}{F1 Score ($\\uparrow$Impv.)}           & \\multicolumn{2}{c|}{F1 Score ($\\uparrow$Impv.)}            & \\multicolumn{2}{c|}{F1 Score ($\\uparrow$Impv.)}           & \\multicolumn{2}{c}{F1 Score ($\\uparrow$Impv.)} \\\\\n\\multicolumn{1}{c|}{}       &  $\\Phi$ & \\multicolumn{1}{c|}{ $\\Psi$} & $\\Phi$         & \\multicolumn{1}{c|}{$\\Psi$}         & $\\Phi$         & \\multicolumn{1}{c|}{$\\Psi$}         & $\\Phi$         & \\multicolumn{1}{c|}{$\\Psi$}         & $\\Phi$              & $\\Psi$              \\\\\n\\hline\n\\multicolumn{1}{c|}{GCN} & -  & \\multicolumn{1}{c|}{-}     & 85.12        & \\multicolumn{1}{c|}{-}             & 33.09        & \\multicolumn{1}{c|}{-}             & 75.42        & \\multicolumn{1}{c|}{-}             & 57.57             &       -            \\\\\n\\multicolumn{1}{c|}{GSAGE} & - & \\multicolumn{1}{c|}{-}     & 85.36        & \\multicolumn{1}{c|}{-}             & 48.77        & \\multicolumn{1}{c|}{-}             & 76.56        & \\multicolumn{1}{c|}{-}             & 76.22             &          -         \\\\\n\\multicolumn{1}{c|}{GAT} & -  & \\multicolumn{1}{c|}{-}     & 85.45        & \\multicolumn{1}{c|}{-}             & 40.29       & \\multicolumn{1}{c|}{-}             & 75.66         & \\multicolumn{1}{c|}{-}             & 57.84             &        -           \\\\\n\\hline\n\\multicolumn{1}{c|}{FreeKD}  & GCN  & \\multicolumn{1}{c|}{GCN}  & 86.53(\\textbf{$\\uparrow$1.41}) & \\multicolumn{1}{c|}{86.62(\\textbf{$\\uparrow$1.50})} & 37.61(\\textbf{$\\uparrow$4.52})  & \\multicolumn{1}{c|}{37.70(\\textbf{$\\uparrow$4.61})} & 77.28(\\textbf{$\\uparrow$1.86}) & \\multicolumn{1}{c|}{77.33(\\textbf{$\\uparrow$1.91})} & 60.28(\\textbf{$\\uparrow$2.71})      & 60.55(\\textbf{$\\uparrow$2.98})      \\\\\n\\multicolumn{1}{c|}{FreeKD}  & GSAGE & \\multicolumn{1}{c|}{GSAGE} & 86.41(\\textbf{$\\uparrow$1.05}) & \\multicolumn{1}{c|}{86.55(\\textbf{$\\uparrow$1.19})} & 49.89(\\textbf{$\\uparrow$1.12}) & \\multicolumn{1}{c|}{49.85(\\textbf{$\\uparrow$1.08})} &  77.78(\\textbf{$\\uparrow$1.22}) & \\multicolumn{1}{c|}{ 77.58(\\textbf{$\\uparrow$1.02})} & 78.76(\\textbf{$\\uparrow$2.54})      & 77.85(\\textbf{$\\uparrow$1.63})      \\\\\n\\multicolumn{1}{c|}{FreeKD}  & GAT  & \\multicolumn{1}{c|}{GAT}  & 86.46(\\textbf{$\\uparrow$1.01}) & \\multicolumn{1}{c|}{86.68(\\textbf{$\\uparrow$1.23})} & 43.96(\\textbf{$\\uparrow$3.67}) & \\multicolumn{1}{c|}{44.42(\\textbf{$\\uparrow$4.13})} &  77.13(\\textbf{$\\uparrow$1.47}) & \\multicolumn{1}{c|}{77.42(\\textbf{$\\uparrow$1.76})} & 61.18(\\textbf{$\\uparrow$3.34})      & 61.36(\\textbf{$\\uparrow$3.52})      \\\\\n\\multicolumn{1}{c|}{FreeKD}  & GCN  & \\multicolumn{1}{c|}{GAT}  & 86.65(\\textbf{$\\uparrow$1.53}) & \\multicolumn{1}{c|}{86.72(\\textbf{$\\uparrow$1.27})} & 35.58(\\textbf{$\\uparrow$2.49}) & \\multicolumn{1}{c|}{43.79(\\textbf{$\\uparrow$3.53})} &  77.39(\\textbf{$\\uparrow$1.97}) & \\multicolumn{1}{c|}{77.58(\\textbf{$\\uparrow$1.92})} & 61.06(\\textbf{$\\uparrow$3.49})      & 60.38(\\textbf{$\\uparrow$2.54})      \\\\\n\\multicolumn{1}{c|}{FreeKD}  & GCN  & \\multicolumn{1}{c|}{GSAGE} & 86.26(\\textbf{$\\uparrow$1.14}) & \\multicolumn{1}{c|}{86.76(\\textbf{$\\uparrow$1.40})} & 35.39(\\textbf{$\\uparrow$2.30}) & \\multicolumn{1}{c|}{49.89(\\textbf{$\\uparrow$1.12}) } &  77.08(\\textbf{$\\uparrow$1.66}) & \\multicolumn{1}{c|}{77.68(\\textbf{$\\uparrow$1.12})} & 60.61(\\textbf{$\\uparrow$3.04})      & 77.58(\\textbf{$\\uparrow$1.36})      \\\\\n\\multicolumn{1}{c|}{FreeKD}  & GAT  & \\multicolumn{1}{c|}{GSAGE} & 86.67(\\textbf{$\\uparrow$1.22}) & \\multicolumn{1}{c|}{86.84(\\textbf{$\\uparrow$1.48})} & 43.96(\\textbf{$\\uparrow$3.67})  & \\multicolumn{1}{c|}{49.87(\\textbf{$\\uparrow$1.10})} & 77.24(\\textbf{$\\uparrow$1.58}) & \\multicolumn{1}{c|}{77.62(\\textbf{$\\uparrow$1.06}) } & 62.45(\\textbf{$\\uparrow$4.61})      & 78.36(\\textbf{$\\uparrow$2.14})      \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{trasductive}\n\\end{table*}",
            "inductive": "\\begin{table}\n\\caption{Results (\\%) of the compared approaches for node classification in the inductive setting on the PPI dataset.\n% The values in the brackets denote the performance improvement of our FreeKD over the corresponding baselines.\n}\n\\vspace{-0.1in}\n\\setlength{\\tabcolsep}{1.2mm}{%\n\\begin{tabular}{@{}ccccc@{}}\n\\toprule\n                            & \\multicolumn{2}{c}{}             & \\multicolumn{2}{c}{PPI}               \\\\ \\midrule\n\\multicolumn{1}{c|}{Method} & \\multicolumn{2}{c|}{Basic Model}     & \\multicolumn{2}{c}{F1 Score ($\\uparrow$Impv.)} \\\\\n\\multicolumn{1}{c|}{}       & $\\Phi$ & \\multicolumn{1}{c|}{$\\Psi$} & $\\Phi$              & $\\Psi$              \\\\\n\\hline\n\\multicolumn{1}{c|}{GSAGE} & - & \\multicolumn{1}{c|}{-}     & 69.28             &       -            \\\\\n\\multicolumn{1}{c|}{GAT} & -  & \\multicolumn{1}{c|}{-}     & 97.30              &        -           \\\\\n\\hline\n\\multicolumn{1}{c|}{FreeKD}  & GSAGE & \\multicolumn{1}{c|}{GSAGE} & 71.72(\\textbf{$\\uparrow$2.44})      & 71.56(\\textbf{$\\uparrow$2.28})      \\\\\n\\multicolumn{1}{c|}{FreeKD}  & GAT  & \\multicolumn{1}{c|}{GAT}  & 98.79(\\textbf{$\\uparrow$1.49})      & 98.73(\\textbf{$\\uparrow$1.43})      \\\\\n\\multicolumn{1}{c|}{FreeKD}  & GAT  & \\multicolumn{1}{c|}{GSAGE} & 98.61(\\textbf{$\\uparrow$1.31})      & 72.39(\\textbf{$\\uparrow$3.11})      \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\vspace{-0.1in}\n\\label{inductive}\n\\end{table}",
            "kd": "\\begin{table}\n\\caption{Results (\\%) of  different knowledge distillation methods. '-' means not available.\n}\n\\vspace{-0.1in}\n\\setlength{\\tabcolsep}{2mm}{\n\\begin{tabular}{c|c|c|c|c|c}\n \\toprule\n        & Cora                  & Chameleon              & Citeseer              & Texas                  & PPI                    \\\\ \\hline\nTeacher & \\multirow{2}{*}{87.80} & \\multirow{2}{*}{46.79} & \\multirow{2}{*}{78.60} & \\multirow{2}{*}{65.14} & \\multirow{2}{*}{99.41} \\\\\nGCNII   &                       &                        &                       &                        &                        \\\\ \\hline\nKD      & 86.13                 & 43.69                  & 77.03                 & 59.46                  & 97.81                  \\\\ \nLSP     & 86.25                 & 44.01                  & 77.21                 & 59.73                  & 98.25                  \\\\\nCPF     & 86.41                 & 41.40                   & \\textbf{77.80}                  & 60.81                  & -                      \\\\ \\hline\nGNN-SD  & 85.75                 & 40.79                  & 75.96                 & 58.65                  & 97.73                  \\\\\nRDD     & 85.84                 & 41.15                  & 76.02                 & 58.92                  & 97.66                  \\\\\nFreeKD    & \\textbf{86.68}    & \\textbf{44.42}  & 77.42                 & \\textbf{61.36}      & \\textbf{98.79}    \\\\ \\bottomrule             \n\\end{tabular}%\n}\n\\vspace{-0.2in}\n\\label{kd}\n\\end{table}",
            "ablation_study": "\\begin{table}\n\\caption{Ablation Study on the Chameleon and Cora dataset.}\n\\vspace{-0.1in}\n \\setlength{\\tabcolsep}{1.2mm}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n& & &\\multicolumn{2}{c}{Chameleon} &\\multicolumn{2}{c}{Cora} \\\\ \n\\hline\n\\multicolumn{1}{c|}{Method}                 & \\multicolumn{2}{c|}{Network}     & \\multicolumn{2}{c|}{F1 Score}     & \\multicolumn{2}{c}{F1 Score}  \\\\\n\\multicolumn{1}{c|}{}                       & $\\Phi$ & \\multicolumn{1}{c|}{$\\Psi$} & $\\Phi$  & \\multicolumn{1}{c|}{$\\Psi$}    & $\\Phi$  & \\multicolumn{1}{c}{$\\Psi$}   \\\\\n\\hline\n\\multicolumn{1}{c|}{GCN} & - & \\multicolumn{1}{c|}{-} & 33.09 &\\multicolumn{1}{c|}{-}  & 85.12 & - \\\\\n\\multicolumn{1}{c|}{FreeKD-node} & GCN & \\multicolumn{1}{c|}{GCN} & 36.35 & \\multicolumn{1}{c|}{36.42}  & 86.17 & 86.03 \\\\\n\\hline\n\\multicolumn{1}{c|}{FreeKD-w.o.-judge}     & GCN  & \\multicolumn{1}{c|}{GCN}  & 35.33 & \\multicolumn{1}{c|}{35.27}          & 85.83 & 85.76  \\\\\n\\multicolumn{1}{c|}{FreeKD-loss} & GCN  & \\multicolumn{1}{c|}{GCN}  & 35.86 & \\multicolumn{1}{c|}{35.79}           & 85.89 & 85.97 \\\\\n\\multicolumn{1}{c|}{FreeKD-all-neighbors} & GCN  & \\multicolumn{1}{c|}{GCN}  & 36.85 & \\multicolumn{1}{c|}{36.73}      & 86.21 & 86.26 \\\\\n\\multicolumn{1}{c|}{FreeKD-all-structures} & GCN  & \\multicolumn{1}{c|}{GCN}  & 36.53 & \\multicolumn{1}{c|}{36.62}     & 86.13 & 86.07  \\\\\n\\multicolumn{1}{c|}{FreeKD}                  & GCN  & \\multicolumn{1}{c|}{GCN}  & 37.61 & \\multicolumn{1}{c|}{37.70}    & 86.53 & 86.62 \\\\ \\bottomrule\n\\end{tabular}\n}\n \\label{ablation_study}\n  \\vspace{-0.15in}\n\\end{table}",
            "gamma": "\\begin{table}\n\\caption{Sensitivity study of $\\gamma$ on the Cora dataset.}\n\\vspace{-0.1in}\n\\setlength{\\tabcolsep}{0.9mm}{%\n\\begin{tabular}{@{}c|cc|cccccc@{}}\n\\toprule\n\\multirow{2}{*}{Dataset} & \\multicolumn{2}{c|}{Network} & \\multirow{2}{*}{$\\gamma$=0.0} & \\multirow{2}{*}{$\\gamma$=0.1} & \\multirow{2}{*}{$\\gamma$=0.3} & \\multirow{2}{*}{$\\gamma$=0.5} & \\multirow{2}{*}{$\\gamma$=0.7} & \\multirow{2}{*}{$\\gamma$=0.9} \\\\\n                         & $\\Phi$          & $\\Psi$         &                        &                        &                        &                        &                        &                        \\\\ \\midrule\n\n\\multirow{2}{*}{Cora}    & GCN           & GCN          & 86.32                  & 86.39                  & \\textbf{86.57}         & 86.31                  & 86.12                  & 85.23                   \\\\\n                         & GAT           & GAT          & 86.21                  & 86.45                  & 86.41                  & \\textbf{86.57}         & 86.32                  & 86.22                  \\\\ \\bottomrule\n\\end{tabular}%\n}\n \\label{gamma}\n \\vspace{-0.1in}\n\\end{table}",
            "dataset": "\\begin{table}[h]\n\\vspace{-0.1in}\n\\caption{Dataset statistics.}\n\\vspace{-0.1in}\n \\setlength{\\tabcolsep}{1.3mm}{\n\\begin{tabular}{@{}cccccc@{}}\n\\toprule\nDataset   & \\# Graphs & \\# Nodes & \\# Edges & \\# Features & \\# Classes \\\\ \\midrule\nCora      & 1         & 2708     & 5429     & 1433        & 7          \\\\\nCiteseer  & 1         & 3327     & 4732     & 3703        & 6          \\\\\nChameleon & 1         & 2277     & 36101    & 2325        & 4          \\\\\nTexas     & 1         & 183      & 309      & 1703        & 5          \\\\\nPPI       & 24        & 56944    & 818716   & 50          & 121        \\\\ \\bottomrule\n\\end{tabular}\n}\n\\vspace{-0.2in}\n \\label{dataset}\n\\end{table}"
        },
        "figures": {
            "cross": "\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{figure/vis_1}\n  \\caption {Cross entropy losses for nodes with ID  from $1$ to $10$ on the Cora dataset obtained by two typical GNN models, GraphSAGE \\cite{hamilton2017inductive} and GAT \\cite{velivckovic2017graph}, after training 20 epochs. The value in each block denotes the corresponding loss.}\n  \\Description{}\n  \\label{cross}\n  \\vspace{-0.15in}\n\\end{figure}",
            "main": "\\begin{figure*}[htbp]\n  \\centering\n  \\includegraphics[width=0.92\\linewidth]{figure/new3}\n  \\caption{\n    % An illustration of the BKDR framework. BKDR is mainly made up of three modules: 1) reinforced node judge module based on reinforcement learning is designed to dynamically decide the directions of knowledge distillation; 2) An agent-guided node knowledge transfer module is proposed to transfer the soft label between corresponding nodes in two GNNs; 3) An agent-guided structure knowledge transfer module intends to transfer the neighborhood relation which is generated according to the agent's actions between two GNNs.\n    An illustration of the FreeKD framework. FreeKD can manage the knowledge distillation directions between two GNN models via a  hierarchical reinforcement learning that contains two-level actions. The first level of actions are designed to determine the distillation direction for each node, in order to propagate the node's soft label. And then the second level of actions are used to decide which of   the local structures generated based on node-level actions to be propagated.\n  }\n  \\Description{Main architecture of FreeKD}\n  \\label{main}\n\\end{figure*}",
            "transfer": "\\begin{figure}\n\\centering\n\\subfigure[Node-level distillation]{\n\\begin{minipage}[t]{0.49\\linewidth}\n\\centering\n\\includegraphics[width=1.46in]{figure/p1}\n\\end{minipage}%\n}%\n\\subfigure[Structure-level distillation]{\n\\begin{minipage}[t]{0.49\\linewidth}\n\\centering\n\\includegraphics[width=1.2in]{figure/p2}\n\\end{minipage}%\n}%\n\\centering\n\\vspace{-0.2in}\n\\caption{Illustration of both node-level and structure-level knowledge distillation.}\n\\label{transfer}\n\\vspace{-0.15in}\n\\end{figure}",
            "vis": "\\begin{figure*}\n\\centering\n\\subfigure[$\\Phi$: without noise; $\\Psi$: without noise]{\n\\begin{minipage}[t]{0.3\\linewidth}\n\\centering\n\\includegraphics[width=2in]{figure/vis_a4}\n\\end{minipage}%\n}%\n\\subfigure[$\\Phi$: noise $\\sigma=0.5$; $\\Psi$: without noise]{\n\\begin{minipage}[t]{0.3\\linewidth}\n\\centering\n\\includegraphics[width=2in]{figure/vis_b4}\n%\\caption{fig2}\n\\end{minipage}%\n\\label{vis2}\n}%\n\\subfigure[$\\Phi$: noise $\\sigma=1.0$; $\\Psi$: without noise]{\n\\begin{minipage}[t]{0.3\\linewidth}\n\\centering\n\\includegraphics[width=2in]{figure/vis_c4}\n%\\caption{fig2}\n\\end{minipage}\n\\label{vis3}\n}%\n\\centering\n\\vspace{-0.15in}\n  \\caption{The output probabilities of our node-level policy reinforced knowledge judge module by adding different degrees of noise to the network $\\Phi$. \n  The redder the node in a network is, the higher the probability for the node in this network to serve as a teacher node to transfer knowledge to the corresponding node of the other network is.}\n  \\label{vis}\n  \\vspace{-0.12in}\n\\end{figure*}",
            "param": "\\begin{figure}\n\\centering\n\\subfigure[Sensitivity study of $\\mu$ and $\\rho$.]{\n\\begin{minipage}[t]{0.49\\linewidth}\n\\centering\n\\includegraphics[width=1.56in]{figure/param_cora3}\n\\end{minipage}%\n}%\n\\subfigure[Convergence curve.]{\n\\begin{minipage}[t]{0.49\\linewidth}\n\\centering\n\\includegraphics[width=1.56in]{figure/convergence}\n\\end{minipage}%\n}%\n\\centering\n\\vspace{-0.2in}\n\\caption{Sensitivity study of $\\mu$,$\\rho$ and convergence analysis on the Cora dataset.}\n\\label{param}\n\\vspace{-0.17in}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n  \\mathbf{h}_i^{(l)}=AGGREGATOR(\\mathbf{h}_i^{(l-1)},\\{\\mathbf{h}_j^{(l-1)}|\\ j\\in{\\mathcal{N}(i)}\\},{\\ \\mathbf{W}^{(l)}}),\n\\end{align}",
            "eq:n-state": "\\begin{equation}\\label{n-state}\n    \\mathbf{s}_i^{[1]}=CONCAT(\\mathbf{p}_i^\\Phi,L_{CE}^\\Phi(i),\\mathbf{p}_i^\\Psi,L_{CE}^\\Psi(i)),\n\\end{equation}",
            "eq:2": "\\begin{align}\nL_{node}^\\Psi=\\sum_{i=1}^{N}{(1-a_i^{[1]}) KL(\\mathbf{p}_i^\\Phi||\\mathbf{p}_i^\\Psi)}\\label{ce1}\\\\\nL_{node}^\\Phi=\\sum_{i=1}^{N}{a_i^{[1]}KL(\\mathbf{p}_i^\\Psi||\\mathbf{p}_i^\\Phi)}\\label{ce2},\n\\end{align}",
            "eq:3": "\\begin{align}\n        &\\mathbf{M}_i^\\Phi=\\{v\\ |\\ a_i^{[1]}=0\\ , a_v^{[1]}=0, and\\ \\left(i,v\\right)\\in \\mathbf{E}\\}\\\\\n        &\\mathbf{M}_i^\\Psi=\\{v\\ |\\ a_i^{[1]}=1\\ , a_v^{[1]}=1, and\\ \\left(i,v\\right)\\in \\mathbf{E}\\}.\n    \\end{align}",
            "eq:4": "\\begin{equation}\n    \\mathbf{u}_i\\!=\\!\n    \\begin{cases}\n    (\\frac{1}{|\\mathbf{M}_i^\\Phi|}\\sum\\limits_{v\\in \\mathbf{M}_i^\\Phi}{g(\\mathbf{h}_i^\\Phi,\\mathbf{h}_v^\\Phi)},\n    \\frac{1}{|\\mathbf{M}_i^\\Phi|}\\sum\\limits_{v\\in \\mathbf{M}_i^\\Phi}{g(\\mathbf{h}_i^\\Psi,\\mathbf{h}_v^\\Psi)})\n    ,\\emph{if} \\ \\  \\text{$a_i^{[1]}=0$}\\\\\n    (\\frac{1}{|\\mathbf{M}_i^\\Psi|}\\sum\\limits_{v\\in \\mathbf{M}_i^\\Psi}{g(\\mathbf{h}_i^\\Psi,\\mathbf{h}_v^\\Psi)},\n    \\frac{1}{|\\mathbf{M}_i^\\Psi|}\\sum\\limits_{v\\in \\mathbf{M}_i^\\Psi}{g(\\mathbf{h}_i^\\Phi,\\mathbf{h}_v^\\Phi)}),\n    \\emph{if} \\ \\  \\text{$a_i^{[1]}=1$},\\\\\n    \\end{cases}\\nonumber\n    \\end{equation}",
            "eq:s-state": "\\begin{equation}\\label{s-state}\n    \\mathbf{s}_i^{[2]}=\n    CONCAT(\\mathbf{s}_i^{[1]},\\mathbf{u}_i),\n\\end{equation}",
            "eq:5": "\\begin{align}\n    \\hat{s}_{ij}^\\Phi=\\frac{e^{g\\left(\\mathbf{h}_i^\\Phi,\\mathbf{h}_j^\\Phi\\right)}}{\\sum_{v\\in \\mathbf{M}_i^\\Phi} e^{g\\left(\\mathbf{h}_i^\\Phi,\\mathbf{h}_v^\\Phi\\right)}},\\ \\ \n    \\hat{s}_{ij}^\\Psi=\\frac{e^{g\\left(\\mathbf{h}_i^\\Psi,\\mathbf{h}_j^\\Psi\\right)}}{\\sum_{v\\in \\mathbf{M}_i^\\Phi} e^{g\\left(\\mathbf{h}_i^\\Psi,\\mathbf{h}_v^\\Psi\\right)}},\n\\end{align}",
            "eq:struct1": "\\begin{align}\\label{struct1}\n    L_{struct}^\\Psi=\\sum_{i=1}^{N}{(1-a_i^{[1]})a_i^{[2]} KL(\\mathbf{\\hat{s}}_i^\\Phi||\\mathbf{\\hat{s}}_i^\\Psi)},\n\\end{align}",
            "eq:struct2": "\\begin{equation}\\label{struct2}\n    L_{struct}^\\Phi=\\sum_{i=1}^{N}{a_i^{[1]}a_i^{[2]}KL(\\mathbf{\\bar{s}}_i^\\Psi||\\mathbf{\\bar{s}}_i^\\Phi)},\n\\end{equation}",
            "eq:6": "\\begin{align}\n    \\bar{s}_{ij}^\\Phi=\\frac{e^{g\\left(\\mathbf{h}_i^\\Phi,\\mathbf{h}_j^\\Phi\\right)}}{\\sum_{v\\in \\mathbf{M}_i^\\Psi} e^{g\\left(\\mathbf{h}_i^\\Phi,\\mathbf{h}_v^\\Phi\\right)}}, \\ \\ \n    \\bar{s}_{ij}^\\Psi=\\frac{e^{g\\left(\\mathbf{h}_i^\\Psi,\\mathbf{h}_j^\\Psi\\right)}}{\\sum_{v\\in \\mathbf{M}_i^\\Psi} e^{g\\left(\\mathbf{h}_i^\\Psi,\\mathbf{h}_v^\\Psi\\right)}}.\n\\end{align}",
            "eq:reward": "\\begin{equation}\\label{reward}\n     R_i=-\\frac{\\sum\\limits_{u\\in \\mathbf{B}}{(L_{CE}^\\Phi(u){+L}_{CE}^\\Psi(u))}}{\\left|\\mathbf{B}\\right|}-\\gamma\\frac{\\sum\\limits_{v\\in {\\mathcal{N}_i}}{(L_{CE}^\\Phi(v){+L}_{CE}^\\Psi(v))}}{\\left|{\\mathcal{N}_i}\\right|},\n \\end{equation}",
            "eq:grad": "\\begin{equation}\\label{grad}\n\\mathrm{\\nabla}_{\\theta,\\delta}J\\!=\\!\n\\frac{1}{|\\mathbf{B}|}\\!\n\\sum_{i\\in\\mathbf{B}\\!}{\\!(R_i\\!-\\!b_i)\\mathrm{\\nabla}_{\\theta,\\delta}\\!\\log(\\pi_\\theta(\\mathbf{s}_i^{[1]}\\!,a_i^{[1]})\\pi_\\delta(\\mathbf{s}_i^{[2]}\\!,a_i^{[2]})}),\n\\end{equation}",
            "eq:update": "\\begin{equation}\\label{update}\n    \\theta\\gets\\theta+\\eta\\mathrm{\\nabla}_\\theta J,\\ \\   \\delta\\gets\\delta+\\eta\\mathrm{\\nabla}_\\delta J,\n\\end{equation}",
            "eq:loss1": "\\begin{equation}\\label{loss1}\n    L^\\Phi=L_{CE}^\\Phi+\\mu L_{node}^\\Phi+\\rho L_{struct}^\\Phi\n\\end{equation}",
            "eq:loss2": "\\begin{equation}\\label{loss2}\n    L^\\Psi=L_{CE}^\\Psi+\\mu L_{node}^\\Psi+\\rho L_{struct}^\\Psi,   \n\\end{equation}"
        }
    }
}