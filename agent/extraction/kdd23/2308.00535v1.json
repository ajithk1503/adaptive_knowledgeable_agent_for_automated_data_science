{
    "meta_info": {
        "title": "Graph Contrastive Learning with Generative Adversarial Network",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated promising results on\nexploiting node representations for many downstream tasks through supervised\nend-to-end training. To deal with the widespread label scarcity issue in\nreal-world applications, Graph Contrastive Learning (GCL) is leveraged to train\nGNNs with limited or even no labels by maximizing the mutual information\nbetween nodes in its augmented views generated from the original graph.\nHowever, the distribution of graphs remains unconsidered in view generation,\nresulting in the ignorance of unseen edges in most existing literature, which\nis empirically shown to be able to improve GCL's performance in our\nexperiments. To this end, we propose to incorporate graph generative\nadversarial networks (GANs) to learn the distribution of views for GCL, in\norder to i) automatically capture the characteristic of graphs for\naugmentations, and ii) jointly train the graph GAN model and the GCL model.\nSpecifically, we present GACN, a novel Generative Adversarial Contrastive\nlearning Network for graph representation learning. GACN develops a view\ngenerator and a view discriminator to generate augmented views automatically in\nan adversarial style. Then, GACN leverages these views to train a GNN encoder\nwith two carefully designed self-supervised learning losses, including the\ngraph contrastive loss and the Bayesian personalized ranking Loss. Furthermore,\nwe design an optimization framework to train all GACN modules jointly.\nExtensive experiments on seven real-world datasets show that GACN is able to\ngenerate high-quality augmented views for GCL and is superior to twelve\nstate-of-the-art baseline methods. Noticeably, our proposed GACN surprisingly\ndiscovers that the generated views in data augmentation finally conform to the\nwell-known preferential attachment rule in online networks.",
        "author": "Cheng Wu, Chaokun Wang, Jingcao Xu, Ziyang Liu, Kai Zheng, Xiaowei Wang, Yang Song, Kun Gai",
        "link": "http://arxiv.org/abs/2308.00535v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "KDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n% Graph representation learning, which aims at learning node representations by preserving the network information for downstream tasks, has attracted increasing attention from both academia and industry in recent years~\\citep{perozzi2014deepwalk, tang2015line, node2vec-kdd2016}. % to learn node representations which preserve the network information for downstream tasks.\nIn recent years, graph representation learning has attracted increasing attention from both academia and industry to deal with network-based data~\\citep{perozzi2014deepwalk, tang2015line, node2vec-kdd2016, gu2022hybridgnn, wu2023SUPA}.\n% Graph Neural Networks (GNNs)~\\citep{kipf2016semi, hamilton2017inductive, he2020lightgcn} have been proved to be effective in supervised end-to-end training.\nGraph Neural Networks (GNNs)~\\citep{kipf2016semi, hamilton2017inductive, he2020lightgcn} have shown effectiveness in supervised end-to-end training.\nHowever, task-specific labels can be extremely scarce for graph datasets~\\citep{zitnik2018prioritizing,hu2019strategies}.\nTo this end, research efforts start exploring self-supervised learning for GNNs, where only limited or even no labels are needed~\\citep{wu2021self}.\n\n% \\begin{figure}[ht]\n% \\centering\n% \\includegraphics[width=0.75\\linewidth]{figures/ner.pdf} \n% \\caption{\n% \\review{The y-axis is the MRR rate $\\eta = \\frac{\\mbox{MRR}}{\\mbox{MRR w/o new edges}}$, and $\\eta > 1$ means that the performance is improved.\n% It is observed that adding new edges randomly to one of the augmented views can improve Simple-GCL on most datasets.\n% However, different rates of new edges are required to get the best performance on different datasets.}\n% }\n% \\label{fig:motivation}\n% \\end{figure}\n\nRecently, graph contrastive learning (GCL)~\\citep{velickovic2019deep,you2020graph,zhu2020deep,wu2021self} has become one of the most popular self-supervised approaches, which leverages the mutual information maximization principle (InfoMax)~\\citep{linsker1988self} to maximize the correspondence between the representations of a graph (or a node) in its different augmented views.\nThere are a large amount of view augmentation strategies explored by different GCL methods, including node dropping, edge perturbation, subgraph sampling and feature masking.\nFurthermore, views can be generated by random sampling~\\citep{you2020graph}, or under the guide of domain knowledge~\\cite{hassani2020contrastive,zhu2021graph}, or by a view learner~\\cite{suresh2021adversarial}.\n\n\n\n% Although there are a large amount of view augmentation strategies explored by different GCL methods, the evolution and the distribution of graphs remains unconsidered in view generation.\nHowever, the evolution and the distribution of graphs remains unconsidered in existing view generation strategies.\nIntuitively, a graph is formed with nodes and edges created in succession, and the observed graph is a snapshot of this procedure.\nThus, the non-connected node pairs are possible to form new edges in future.\n\\change{Case 1 in Figure~\\ref{fig:example} demonstrates this intuition. Furthermore, the evolution of graphs varies (e.g., Case1, Case2, or other possible cases in Figure~\\ref{fig:example}) based on the distribution of graphs. Then, exploring the distribution of graphs helps to search unseen but should existing edges, which benefits the variety of generated views and boosts the performance of GCL.}\n% \\change{Figure~\\ref{fig:example} provides an example to demonstrate this intuition.}\n% \\change{For example, on a video-watching platform, when a new video is uploaded at 09:00, it has no interactions. Then, Alice watches the video at 09:10 and Bob watches it at 09:20. If the observed graph is obtained at 09:15, the watching behavior from Bob is unseen.}\n% Furthermore, the evolution of graphs is not determined \\change{(e.g., Case 1 and Case 2 in Figure~\\ref{fig:example})} and exploring the distribution of graphs helps to search unseen but should existing edges, which benefits the variety of generated views and boosts the performance of GCL.\n\n% As illustrated in Figure~\\ref{fig:motivation}, we consider adding new edges randomly to one of the augmented views of a graph contrastive learning method (i.e., Simple-GCL in Sec~\\ref{sec:baseline}) and evaluate the link prediction performance.\nAs illustrated in Figure~\\ref{fig:motivation}, we consider replacing some existing edges with new edges randomly in one of the augmented views of a graph contrastive learning method (i.e., Simple-GCL in Sec~\\ref{sec:baseline}) and evaluate the link prediction performance.\n% Note that the y-axis is the Mean Reciprocal Rank (MRR) ratio $\\eta = \\frac{\\mbox{MRR of Simple-GCL}}{\\mbox{MRR of GACN}}$, where the performance of GACN is consistent w.r.t. the rate of new edges.\nIt is observed that replacing a certain amount of existing edges with new edges can improve Simple-GCL on most datasets, showing the benefit of the supplement of unseen edges.\nHowever, different rates of new edges are required to get the best performance on different datasets due to different graph distributions.\n\n% However, GCL methods rely on predefined augmentation strategies or domain knowledge to generate augmented views, which have be found hard to preserve the most relevant information for contrastive learning~\\citep{you2021graph,suresh2021adversarial}.\n% To avoid capturing redundant information, AD-GCL~\\citep{suresh2021adversarial} explores adaptive graph augmentation strategies through adversarial training, and achieve significant performance gains in graph-level tasks via learnable edge dropping, which lights the way to learn augmentation strategies automatically.\n\n% \\underline{\\textbf{Motivation.}} Although edge dropping is widely used in many GCL methods, we find it not optimal in node-level tasks.\n% As illustrated by Figure~\\ref{fig:motivation}, we consider adding new edges to one of the augmented view of a graph contrastive learning method (i.e., Simple-GCL in Sec~\\ref{sec:baseline}) and evaluate the link prediction performance. \n\n% In this work, to automatically learn graph distribution and rationally add new edges, we propose to leverage graph generative adversarial networks (GANs) to learn and generate augmented views for GCL.\nIn this work, we argue that the process of data augmentation for GCL should systematically consider graph evolution, and then propose to leverage graph generative adversarial networks (GANs) for graph distribution learning.\n% to add new edges rationally (e.g., following the preferential attachment rule~\\citep{barabasi1999emergence}).\nClearly, it is not a trivial task considering the following two major challenges:\n% It is not a trivial task and there are two major challenges:\n% In this work, inspired by the development of graph generative adversarial networks (GANs)~\\citep{wang2018graphgan}, we direct towards leveraging graph GANs to generate augmented views for GCL. Nevertheless, it is not a trivial task and there are two major challenges:\n\n\\noindent\\textbf{Automatically capturing the characteristic of graphs for augmentations.}\nOn the one hand, compared to image data and text data, graph data are more abstract~\\citep{suresh2021adversarial}, making it hard to characterize the distribution of graph.\nOn the other hand, graph data are discrete (e.g., the value of the adjacent matrix is binary), and sampling-based graph generators are usually hard to be trained end-to-end.\nThus, more explorations are required to design a graph GAN to generate high-quality augmented views.\n\n\\noindent\\textbf{Jointly training the graph GAN model and the GCL model.}\nA simple idea to combine graph GANs with GCL is to train the two models separately.\nHowever, in this way, the connection between the two models is weak and there is no guarantee that the generated views, which deceive the discriminator of the GAN model well, can be well encoded by the GCL model.\n% \\review{Furthermore, there are GNNs in both the two models and training separately results in two group of parameters, which is not necessary.}\nFurthermore, training graph GAN and GCL separately needs to maintain two groups of GNN parameters, which is unnecessary.\nThus, it is better to explore a parameter sharing strategy and a jointly learning framework for the sake of effectiveness and efficiency.\n\n\n\nTo tackle the above challenges, this paper proposes GACN, a graph \\textbf{G}enerative \\textbf{A}dversarial \\textbf{C}ontrastive learning \\textbf{N}etwork.\nSpecifically, GACN develops a graph generative adversarial network with a view generator and a view discriminator to learn generating augmented views through a minimax game. Then, GACN adapts a GNN as the graph encoder and designs two self-supervised learning losses to optimize the parameters. To train GACN, a jointly learning framework is proposed to optimize the view generator, the view discriminator and the graph encoder sequentially and iteratively.\n\nThe main contributions of this paper are highlighted as follows:\n% \\begin{itemize}[leftmargin=*]\n%     \\item \\review{We explore the benefit of leveraging missing edges to boost GCL, and first propose to incorporate graph GANs to learn and generate views for GCL.} \n%     % \\item We first leverage graph generative adversarial networks to generate augmented views for graph contrastive learning w.r.t. node-level tasks. \n%     \\item We present GACN, a new graph neural network that develops a view generator (Section~\\ref{sec:generator}) and a view discriminator (Section~\\ref{sec:discriminator}) to learn generating views for the graph encoder (Section~\\ref{sec:encoder}). All these modules are trained jointly with a novel framework (Section~\\ref{sec:optimization}).\n%     \\item We conduct comprehensive experiments to evaluate GACN with twelve state-of-the-art baseline methods. The experimental results show that GACN is able to learn the preferential attachment rule automatically and is superior to other methods (Section~\\ref{sec:exp}).\n% \\end{itemize}\n\\begin{itemize}[leftmargin=*]\n    \\item We explore the benefit of leveraging \\change{unseen} edges to boost GCL, and first propose to incorporate graph GANs to learn and generate views for GCL. \n    % \\item We first leverage graph generative adversarial networks to generate augmented views for graph contrastive learning w.r.t. node-level tasks. \n    \\item We present GACN, a new graph neural network that develops a view generator and a view discriminator to learn generating views for the graph encoder. All these modules are trained jointly with a novel framework (Section~\\ref{sec:methods}).\n    \\item We conduct comprehensive experiments to evaluate GACN with twelve state-of-the-art baseline methods. The experimental results show that GACN is superior to other methods, and is able to generate views satisfying the famous preferential attachment rule (Section~\\ref{sec:exp}).\n    % in the Web community.\n\\end{itemize}\n\n% The rest of this paper is organized as follows. The related work are presented in Section~\\ref{sec:related_work}. In Section~\\ref{sec:preliminaries}, we introduce several preliminary concepts. We propose our GACN model in Section~\\ref{sec:methods}. The experimental settings and results are demonstrated in Section~\\ref{sec:exp}. At last, Section~\\ref{sec:conclusion} concludes this paper.\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec:related_work}\nIn this section, the related work to this study is briefly summarized, including graph contrastive learning and graph generative adversarial network.\n% In this section, we review related work relevant to our study, including graph contrastive learning (GCL) and graph generative adversarial network (GGAN)~\\citep{gao2022generative}.\n\n",
                "subsection 2.1": {
                    "name": "Graph Contrastive Learning",
                    "content": "\n% Contrastive Learning (CL)~\\citep{linsker1988self,van2018representation,tian2020contrastive,1992Self,henaff2020data,hjelm2018learning} was initially proposed to train CNNs for image representation learning and has recently achieved great success~\\citep{chen2020simple,chen2020big}.\n% GCL applies the idea of CL on GNNs.\n% In contrast to the case of CNNs, GCL trained using GNNs posts us new fundamental challenges.\n% An image oftern has multiple natural views, say by imposing different color filters and so on.\n% Hence, different views of an image give natural contrastive pairs for CL to train CNNs.\n% However, graphs are more abstract and the irregularity of graph structures typically provides crucial information.\n% Thus, designing contrastive pairs for GCL must play with irregular graph structures and thus becomes more challenging.\n% Some works use different parts of a graph to build contrastive pairs, including nodes v.s. whole graphs~\\citep{velickovic2019deep,sun2020infograph}, nodes v.s. nodes~\\citep{peng2020graph}, nodes v.s. subgraphs~\\citep{jiao2020sub,hu2020strategies}.\n% Other works adopt graph data augmentations (GDA) such as edge perturbation~\\citep{qiu2020gcc} to generate contrastive pairs.\n% Recently, GraphCL~\\citep{you2020graph} gives an extensive study on different combinations of GDAs including node dropping, edge perturbation, subgraph sampling and feature masking. Extensive evaluation is required to determine good combinations.\n% MVGRL~\\citep{hassani2020contrastive} and GCA~\\citep{zhu2021graph} leverage the domain knowledge of network science and adopt network centrality to perform GDAs.\n% Very recently, JOAO~\\citep{you2021graph} adopts a bi-level optimization framework to learn GDAs.\nContrastive Learning (CL)~\\citep{linsker1988self,van2018representation,tian2020contrastive,1992Self,henaff2020data,hjelm2018learning} is an emerging paradigm to learn quality discriminative representations based on augmented ground-truth samples.\nIt initially showed the promising capability in the field of computer vision (CV) and natural language processing (NLP) while recently researchers have applied CL to graph domains to fully exploit graph structure information and rich unlabeled data.\nThe core idea of GCL is to maximize the mutual information between instances (e.g., node, subgraph, and graph) of different views augmented from the original graph.\n\nSimilar to the visual domain ~\\citep{chen2020simple, tian2020contrastive}, there are various augmentation techniques on attributes or topologies and contrastive pretext tasks on different granularities.\nFor example, DGI ~\\citep{velickovic2019deep} performs the row-wise shuffling on the attribute matrix and conducts node-graph level contrast while MVGRL~\\citep{hassani2020contrastive} applies an edge diffusion augmentation to obtain contrasting views.\nOn top of attribute masking, GraphCL ~\\citep{you2020graph} proposes several topology-based augmentations including node dropout, edge dropout and subgraph sampling to incorporate various priors.\nRather than contrasting views at the graph level, GRACE ~\\citep{zhu2020deep}, GCA ~\\citep{zhu2021graph} and GROC ~\\citep{jovanovic2021towards} conduct node-level same-scale contrast, which is the most adopted method to learn node-level representations.\nVery recently, JOAO~\\citep{you2021graph} adopts a bi-level optimization framework to learn graph data augmentations.\n\nHowever, many GCL methods require a trial-and-error selection or domain knowledge to augment views, which limits the application of GCL.\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Graph Generative Adversarial Network",
                    "content": "\nBy designing a game-theoretical minimax game, Generative Adversarial Networks (GAN)~\\citep{goodfellow2014generative} have achieved success in various applications, such as image generation~\\citep{denton2015deep}, sequence generation~\\citep{yu2017seqgan}, dialogue generation~\\citep{li2017adversarial}, information retrieval~\\citep{wang2017irgan}, and domain adaption~\\citep{zhang2017aspect}. More recently, GANs have been applied on graph-based tasks.\n\nIn terms of the graph generation task, ~\\citet{liu2019learning} stack multiple GANs to form a hierarchical architecture for preserving topological features of training graphs. \nTo preserve the distribution of links with minimal risk of privacy breaches,  ~\\citet{tavakoli2017learning} utilize GANs to learn the probability of link formation. \nAiming at better capturing the essential properties and preserving the patterns of real graphs, \\citet{bojchevski2018netgan} introduce NetGAN to learn a distribution of network via the random walks.\n\nAnother line of applications is graph embedding.\nANE ~\\citep{dai2018adversarial} treats GANs as an regularization term to learn robust representations.\nGraphGAN ~\\citep{wang2018graphgan} designs a generator to learn node embeddings and a discriminator to predict link probabilities.\nNetRA~\\citep{yu2018learning} and ProGAN~\\citep{gao2019progan} preserve and learn the underlying node similarity in the model of GAN.\n% Despite effectiveness towards static network embedding, researchers have implemented GANs to link prediction tasks on dynamic graphs or multi-view graphs.\n% For instance,\nBesides, \\citet{lei2019gcn} and \\citet{yang2019advanced} combine GANs with various encoders\n%(e.g., graph convolutional network (GCN), long short-term memory (LSTM))\nto refine the performance of temporal link prediction.\n\\citet{sun2019megan} introduce MEGAN for multi-view network embedding, which accounts for the information from individual views and correlations among different views.\n% \\citet{sun2019megan} introduced MEGAN for multi-view network embedding, which accounts for the information from individual network views and correlations among different views at the same time.\n\n% Besides, a few works on visual domains ~\\citep{ho2020contrastive, jiang2020robust, kim2020adversarial} propose adversarial contrastive learning, which uses the adversarial sample as a form of data augmentation to bring a better downstream task performance and higher robustness. However, adversarial contrastive learning on graphs is to be explored. AD-GCL \\citep{suresh2021adversarial} is a very recent work to incorporate adversarial learning with graph contrastive learning, which enables\n% GNNs to avoid capturing redundant information by optimizing adversarial graph augmentation strategies.\nRecently, GASSL~\\citep{yang2021graph} and AD-GCL~\\citep{suresh2021adversarial} incorporate adversarial learning with graph contrastive learning to avoid capturing redundant information by optimizing adversarial graph augmentation strategies.\n\\change{In computer vision, researchers have tried to make a combination of GAN and CL to boost the performance of GAN~\\citep{lee2021infomax} or CL~\\citep{pan2021videomoco}.}\nHowever, \\change{in graph mining}, adapting GANs to learn graph distribution and generate views for GCL remains unexplored.\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Preliminaries",
                "content": "\n\\label{sec:preliminaries}\nIn this section, we introduce some preliminary concepts and notations.\nIn this work, we denote a graph as $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, where $\\mathcal{V}$ is a node set and $\\mathcal{E}$ is an edge set.\n$\\mathcal{G}$ may have node attributes $\\{\\mathcal{X}_v \\in \\mathbb{R}^F | v \\in \\mathcal{V}\\}$. The adjacent matrix of $\\mathcal{G}$ is denoted as $A\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$, where:\n\\begin{align}\n    A_{i,j} = \\left\\{\n         \\begin{array}{lr}\n         1, \\quad \\mbox{if $(v_i, v_j)\\in\\mathcal{E}$;} \\\\\n         0, \\quad \\mbox{otherwise.}\n         \\end{array}\n    \\right.\n\\end{align}\n\n\\noindent \\underline{\\textbf{Graph Representation Learning.}}\nGiven a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$,  the aim of graph representation learning is to learn an encoder $f: \\mathcal{V}\\to\\mathbb{R}^D$, where $\\{f(v)|v\\in\\mathcal{V}\\}$ can be further used in downstream tasks, such as node classification and link prediction.\n\n\\noindent \\underline{\\textbf{Graph Neural Networks (GNNs).}}\nIn this work, we focus on using GNNs as the encoder $f$. For a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, each node $v\\in\\mathcal{V}$ is paired with a representation $d_v$ initialized as $d_v^{(0)} = \\mathcal{X}_v$.\nThe idea is to apply the neighborhood aggregation scheme on $\\mathcal{G}$, updating the representation of node by aggregating the representations of neighbor nodes:\n\\begin{align}\n    d_v^{(l)} = AGG(d_v^{(l-1)}, \\{d_u^{(l-1)}|u\\in \\mathcal{N}_v\\}),\n\\end{align}\nwhere $d_v^{(l)}$ denotes the representation of node $v$ in the $l$-th layer, $\\mathcal{N}_v$ is the set of neighbors of node $v$, and $AGG$ is the aggregation function. After obtaining $L$ layers presentations, a readout function is adopted to generate the final representation of node $i$:\n\\begin{align}\n    d_v = readout(\\{d_v^{(l)}|l=\\{0,\\cdots,L\\}\\}).\n\\end{align}\n\n\\noindent \\change{\\underline{\\textbf{Graph Contrastive Learning (GCL).}}\nGCL aims to maximize the mutual information between instances (e.g., node, subgraph and graph) of different views augmented from the original graph. Typically, GCL methods adopt graph augmentation strategies to construct positive pairs and negative pairs, and utilize GNNs to encode them into representations. Then, a contrastive loss function is defined to enforce maximizing the consistency between positive pairs compared with negative pairs.}\n\n\\change{In this paper, we use LightGCN~\\citep{he2020lightgcn} as the GNN encoder and focus on node-level GCL.}\nNote that several important notations used in this paper are summarized in Table~\\ref{tab:notations}.\n\n"
            },
            "section 4": {
                "name": "Methods",
                "content": "\n\\label{sec:methods}\n% \\review{In this section, we present the overview of the proposed model GACN (see Figure~\\ref{fig:architure}). GACN consists of three modules, namely \\textit{View Generator}, \\textit{View Discriminator} and \\textit{Graph Encoder}.\n% Specifically, the view generator learns the distributions of edges and generates augmented views by edge sampling.\n% Then the view discriminator is designed to distinguish views generated by the generator from those generated by predefined augmentation strategies (e.g., edge dropout).\n% The view generator and the view discriminator are trained in an adversarial style to generate high-quality views.\n% These views are used to train robust node representations in the graph encoder. Note that the view discriminator and the graph encoder share the same node representations.}\n\nIn this section, we first present the overview of the proposed GACN model (see Figure~\\ref{fig:architure}), and then bring forward the details of its three modules. \nFinally, we propose the optimization framework of GACN.\n\n",
                "subsection 4.1": {
                    "name": "Overview",
                    "content": "\nAs shown in Figure~\\ref{fig:architure}a, GACN consists of three modules, namely \\textit{View Generator}, \\textit{View Discriminator} and \\textit{Graph Encoder}.\nSpecifically, the view generator learns the distributions of edges and generates augmented views by edge sampling.\nThen the view discriminator is designed to distinguish views generated by the generator from those generated by predefined augmentation strategies (e.g., edge dropout).\nThe view generator and the view discriminator are trained in an adversarial style to generate high-quality views.\nThese views are used to train robust node representations in the graph encoder, which shares the same node representations with the view discriminator.\n\nNote that we do not explicitly encode any graph generative principles into the model design.\nHowever, surprisingly our proposed GACN learns the graph distribution to generate views that follow the well-known preferential attachment rule~\\citep{barabasi1999emergence} (see Section~\\ref{sec:case}).\n\n"
                },
                "subsection 4.2": {
                    "name": "View Generator",
                    "content": "\n\\label{sec:generator}\nGiven a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, the view generator is designed to generate a set of augmented views. For a specific view $\\mathcal{G}_g$, we assume that each edge $(v_i, v_j)$ in $\\mathcal{G}_g$ is associated with a random variable $P_{i,j}\\sim\\mbox{Bernoulli}(\\mathcal{W}_{i,j})$, where $\\mathcal{W}\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$ is a learnable matrix, $P$ is a binary matrix with size $|\\mathcal{V}|\\times|\\mathcal{V}|$, $(v_i, v_j)$ is in $\\mathcal{G}_g$ if $P_{i,j} = 1$ and is dropped otherwise. To train the view generator in an end-to-end fashion, we relax the discrete $P_{i,j}$ to be a continuous variable in $(0, 1)$ as follows:\n\\begin{align}\n\\label{eq:gen}\n    P = \\sigma(\\frac{\\mathcal{W} - X_g}{\\tau_g}),\n\\end{align}\nwhere $X_g\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$ is a random matrix with each element sampled from a uniform distribution: $X_{i,j}\\sim U(0, 1)$, $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function, and $\\tau_g\\in(0,1]$ is a hyper-parameter to make $P_{i,j}$ close to $0$ or $1$. Here, $P$ can be treated as an approximation of the generated adjacent matrix.\n\nTo constrain the structure of the generated views, we propose two regularization losses, namely the \\textbf{Edge Count Loss} and the \\textbf{New Edge Loss}, to train the parameters of the generator, i.e., $\\Theta_g = \\{\\mathcal{W}\\}$.\n\n\\noindent \\underline{\\textbf{Edge Count Loss.}} This loss is designed to limit the number of edges in $\\mathcal{G}_g$. Inspired by the edge-dropout strategy~\\citep{zhu2020deep}, we set a ratio $\\lambda_g$ and train $\\mathcal{W}$ to generate views with $\\lambda_g\\cdot|\\mathcal{E}|$ edges. Formally, the edge count loss is computed as:\n\\begin{align}\n\\mathcal{L}_{cnt} = |\\lambda_g\\cdot|\\mathcal{E}| - \\sum_{i,j}{P_{i,j}}|.\n\\end{align}\n\n\\noindent \\underline{\\textbf{New Edge Loss.}} This loss is proposed to avoid generating views that are aggressively different from $\\mathcal{G}$. Specifically, we calculate a penalty for each new edge in $\\mathcal{G}_g$, i.e., edge $(v_i, v_j)$ with $A_{i,j} = 0$ and $P_{i,j} = 1$. Then, the new edge loss is the sum of all the penalties:\n\\begin{align}\n\\mathcal{L}_{new} = \\sum_{i,j}(1 - A_{i,j})\\cdot P_{i,j}\n\\end{align}\n\nThen, the regularization loss is the combination of the above two losses:\n\\begin{align}\n\\label{eq:reg}\n    \\mathcal{L}_{reg} = \\lambda_{cnt}\\cdot\\mathcal{L}_{cnt} + \\lambda_{new}\\cdot\\mathcal{L}_{new},\n\\end{align}\nwhere $\\lambda_{cnt}$ and $\\lambda_{new}$ are hyper-parameters to balance the influences of $\\mathcal{L}_{cnt}$ and $\\mathcal{L}_{new}$, respectively.\n\nBesides, for the sake of efficiency, we initialize $\\mathcal{W}$ instead of training from scratch. Specifically, we set an initialization rate $\\gamma \\in [0, 1]$ to constrain the number of new edges at the beginning, i.e., $\\gamma\\cdot\\lambda_{g}\\cdot|\\mathcal{E}|$ new edges and $(1-\\gamma)\\cdot\\lambda_{g}\\cdot|\\mathcal{E}|$ existing edges are expected in the generated views. Thus, $\\mathcal{W}$ is initialized as follows:\n\\begin{align}\n    \\mathcal{W}_{i,j} = \\left\\{\n         \\begin{array}{cl}\n         \\frac{(1-\\gamma)\\cdot\\lambda_{g}\\cdot|\\mathcal{E}|}{|\\mathcal{E}|}&, \\quad \\mbox{if $(v_i, v_j) \\in \\mathcal{E}$;} \\\\\n         \\frac{\\gamma\\cdot\\lambda_{g}\\cdot|\\mathcal{E}|}{|\\mathcal{C}|}&, \\quad \\mbox{if $(v_i, v_j) \\in \\mathcal{C}$;} \\\\\n         0 &, \\quad \\mbox{otherwise,}\n         \\end{array}\n    \\right.\n\\end{align}\nwhere $\\mathcal{C}\\subseteq (\\mathcal{V}\\times\\mathcal{V} - \\mathcal{E})$ is a candidate set of new edges. Note that we do not consider all the possible new edges as candidates because maintaining a dense $\\mathcal{W} \\in \\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$ for large graphs is memory-unfriendly. In our implementation, we choose edges related to nodes with top-$2,000$ degrees as the candidate set. Note that we also try top-$5,000$ and top-$10,000$. However, the performance gain of top-$5,000$ or top-$10,000$ over that of top-$2,000$ is almost nothing with a huge increment of training time.\n\n"
                },
                "subsection 4.3": {
                    "name": "View Discriminator",
                    "content": "\n\\label{sec:discriminator}\nThe view discriminator is a graph-level classifier to  recognize the generated views.\nMore precisely, the discriminator takes an adjacent matrix as input and judges whether the matrix is a true matrix (i.e., a matrix generated by predefined augmentation strategies) or a false matrix (i.e., a matrix generated by the view generator).\nFormally, given a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, we denote the set of views generated by predefined augmentation strategies (i.e., edge dropout in this work) as $\\vec{\\mathbf{\\mathcal{G}}}_p$, and the set of views generated by the view generator as $\\vec{\\mathbf{\\mathcal{G}}}_g$. Thus, for each $G \\in \\vec{\\mathbf{\\mathcal{G}}}_p\\cup\\vec{\\mathbf{\\mathcal{G}}}_g$, a GNN encoder $f$ is used to encode the representations of each node:\n\\begin{align}\n\\label{eq:enc}\n    \\{d_v|v\\in\\mathcal{V}\\} = f(G).\n\\end{align}\n\nThen, we calculate the graph representation by concatenating the mean pooling and the maximum pooling of the node representations:\n\n\\begin{align}\n\\label{eq:pool}\n    d_G = (\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}{d_v}) \\oplus \\mbox{MaxPool}(\\{d_v|v\\in\\mathcal{V}\\}),\n\\end{align}\nwhere $\\oplus$ is the concatenate operation. With the graph representation, we compute the probability of $G$ using an $L_h$-layer Multilayer Perceptron (MLP) $h$:\n\\begin{align}\n\\label{eq:mlp}\n    p_G = h(d_G),\n\\end{align}\nwhere $\\Theta_h = \\{(W_i, b_i)|i = 1,2,\\ldots,L_h\\}$ is the set of parameters in $h$.\n\nTo train the discriminator, we label the views in $\\vec{\\mathbf{\\mathcal{G}}}_p$ with $1$ and the views in $\\vec{\\mathbf{\\mathcal{G}}}_g$ with $0$. Suppose that the label of $G$ is $y_G$. Then, the classification loss is defined as follows:\n\\begin{align}\n\\label{eq:clf}\n    \\mathcal{L}_{clf}=-y_G\\cdot\\log(p_G)-(1-y_G)\\cdot\\log(1-p_G).\n\\end{align}\n\n"
                },
                "subsection 4.4": {
                    "name": "Graph Encoder",
                    "content": "\n\\label{sec:encoder}\nThe graph encoder is designed to learn the node representations, i.e., the set of parameters of the encoder is $\\Theta_f = \\{d_v^{(0)}|v\\in\\mathcal{V}\\}$, and is trained by two self-supervised losses, including the \\textbf{Graph Contrastive Loss} and the pairwise \\textbf{Bayesian Personalized Ranking (BPR) Loss}.\n\n\\noindent \\underline{\\textbf{Graph Contrastive Loss.}}\nThis loss is proposed to learn robust node representations through maximizing the agreement between different views of the same node compared to that of other nodes.\nSpecifically, we generate two views $\\mathcal{G}_p$ and $\\mathcal{G}_g$ using the predefined augmentation strategies and the view generator, respectively. Encoding $\\mathcal{G}_p$ and $\\mathcal{G}_g$, we have two set of node representations:\n\\begin{align}\n    \\{d_v^p|v\\in\\mathcal{V}\\} = f(\\mathcal{G}_p),\n    \\notag \\\\\n    % \\quad\n    \\{d_v^g|v\\in\\mathcal{V}\\} = f(\\mathcal{G}_g).\n\\end{align}\nThen the graph contrastive loss is defined as :\n\\begin{align}\n% \\label{eq:gcl}\n%     \\mathcal{L}_{gcl} = -\\sum_{v\\in\\mathcal{V}}\\log{\\frac{\\exp(({d_v^p}^\\top d_v^g)/\\tau_f)}{\\sum_{u\\in\\mathcal{V}}{\\exp(({d_u^p}^\\top d_v^g)/\\tau_f)}}},\n% \\end{align}\n\\label{eq:gcl}\n    \\mathcal{L}_{gcl} = -\\sum_{v\\in\\mathcal{V}}\\log{\\frac{\\exp(\\frac{{d_v^p}^\\top d_v^g}{\\tau_f})}{\\sum_{u\\in\\mathcal{V}}{\\exp(\\frac{{d_u^p}^\\top d_v^g}{\\tau_f})}}},\n\\end{align}\nwhere $\\tau_f$ is the temperature hyper-parameter in softmax.\n\n\\noindent \\underline{\\textbf{Bayesian Personalized Ranking Loss.}}\nThis loss is introduced to learn representations that are suitable for downstream tasks, especially for link prediction, and the intuition is to maximize the similarity of connected nodes, while minimize the similarity of disconnected nodes.\nFormally, the bpr loss is defined as:\n\\begin{align}\n\\label{eq:bpr}\n    \\mathcal{L}_{bpr} = -\\frac{1}{|\\mathcal{O}|}\\sum_{(i,j,k)\\in\\mathcal{O}}{\\log\\sigma(d_i^\\top d_j - d_i^\\top d_k)},\n\\end{align}\nwhere $\\mathcal{O} = \\{(i,j,k)|A_{i,j} = 1, A_{i,k} = 0\\}$ is the training data.\n\nThen, the self-supervised loss is the combination of the above two losses:\n\\begin{align}\n\\label{eq:ssl}\n    \\mathcal{L}_{ssl} = \\lambda_{gcl}\\cdot\\mathcal{L}_{gcl} + \\lambda_{bpr}\\cdot\\mathcal{L}_{bpr},\n\\end{align}\nwhere $\\lambda_{gcl}$ and $\\lambda_{bpr}$ are hyper-parameters to balance the influences of $\\mathcal{L}_{gcl}$ and $\\mathcal{L}_{bpr}$, respectively.\n\n\\begin{algorithm}[t]\n\\footnotesize\n\\caption{GACN Framework.}\n\\label{alg:training_framework}\n\\LinesNumbered \n\\KwIn{\ngraph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, dimension of embedding $s$, hyper-parameters $\\tau_g, \\tau_f, \\lambda_{g}, \\lambda_{cnt}, \\lambda_{new}, \\lambda_{gcl}, \\lambda_{bpr}$\n}\n\\KwOut{node representations $\\{d_v|v\\in\\mathcal{V}\\}$}\nInitialize $\\Theta_g, \\Theta_h, \\Theta_f$\\;\n\\While{GACN not converge}{\n    \\For{G-Steps}{\n        Sample $X_g$ and calculate $P$ according to Eq.~(\\ref{eq:gen}) \\;\n        Compute $p_{\\mathcal{G}_g}$ using $P$ according to Eq.~(\\ref{eq:enc}), (\\ref{eq:pool}) and (\\ref{eq:mlp})\\;\n        Update $\\Theta_g$ according to Eq.~(\\ref{eq:reg}) and (\\ref{eq:clf_g})\\;\n    }\n    \\For{D-Steps}{\n        Generate and label $\\vec{\\mathbf{\\mathcal{G}}}_p, \\vec{\\mathbf{\\mathcal{G}}}_g$ \\;\n        Encode $G \\in \\vec{\\mathbf{\\mathcal{G}}}_p\\cup\\vec{\\mathbf{\\mathcal{G}}}_g$ using Eq.~(\\ref{eq:enc}), (\\ref{eq:pool}) and (\\ref{eq:mlp})\\;\n        Update $\\Theta_h$ according to Eq.~(\\ref{eq:clf})\\;\n    }\n    \\For{E-Steps}{\n        Generate $\\mathcal{G}_p, \\mathcal{G}_g$ for Eq.~(\\ref{eq:gcl}) and sample $\\mathcal{O}$ for Eq.~(\\ref{eq:bpr}) \\;\n        Update $\\Theta_f$ according to Eq.~(\\ref{eq:ssl})\\;\n    }\n}\n$\\{d_v|v\\in\\mathcal{V}\\} = f(\\mathcal{G})$ \\;\n\\Return{$\\{d_v|v\\in\\mathcal{V}\\}$}.\n\\end{algorithm}\n\n"
                },
                "subsection 4.5": {
                    "name": "Model Optimization",
                    "content": "\n\\label{sec:optimization}\nIn this subsection, we present the parameter optimization procedure of GACN. As shown in Algorithm~\\ref{alg:training_framework}, the view generator, the view discriminator and the graph encoder are optimized sequentially and iteratively.\n\n\\underline{\\textbf{G-Steps (Lines 3--7)}} (see Figure~\\ref{fig:architure}b) optimize the parameters of the view generator. Specifically, in each iteration, an augmented view $\\mathcal{G}_g$ is generated and then the regularization loss is computed. In consideration of generating high-quality views, an adversarial classification loss is incorporated to cheat the view discriminator by labeling $\\mathcal{G}_g$ with $1$. According to Eq.~(\\ref{eq:clf}), we have:\n\\begin{align}\n\\label{eq:clf_g}\n    \\mathcal{L}'_{clf}=-\\log(p_{\\mathcal{G}_g}).\n\\end{align}\n\n\\underline{\\textbf{D-Steps (Lines 8--12)}} (see Figure~\\ref{fig:architure}c) optimize the parameters of the view discriminator by generating $\\vec{\\mathbf{\\mathcal{G}}}_p, \\vec{\\mathbf{\\mathcal{G}}}_g$ and training the discriminator to classify them.\n\n\\underline{\\textbf{E-Steps (Lines 13--17)}} (see Figure~\\ref{fig:architure}d) first prepare the training data for the self-supervised losses and then update the parameters of the graph encoder.\n\nNote that all the parameters are optimized using the back propagation algorithm. After converging, we obtain the learned node representations $\\{d_v|v\\in\\mathcal{V}\\}$ by encoding graph $\\mathcal{G}$ (Lines 18--19).\n\n\\underline{\\textbf{Time Complexity Analysis.}}\nFor the view generator, the time complexity to generate a single view and calculate the regularization loss is $O(|\\mathcal{V}|^2)$.\nFor the view discriminator, the time complexity to encode a graph using LightGCN~\\citep{he2020lightgcn} is $O(|\\mathcal{V}|^2\\cdot N_lD)$, where $N_l$ is the number of GCN layers. The time complexity to pool the graph and calculate $\\mathcal{L}_{clf}$ is $O(|\\mathcal{V}|\\cdot D)$.\nFor the graph encoder, the time complexity to encode the two augmented views is $O(|\\mathcal{V}|^2\\cdot N_lD)$ and the time complexity to compute $\\mathcal{L}_{ssl}$ is $O(|\\mathcal{V}|^2\\cdot D) + |\\mathcal{O}|\\cdot D)$.\nThus, the overall time complexity of Algorithm~\\ref{alg:training_framework} is\n% $O(N_{iter}[N_G\\cdot(|V|^2 + |V|^2\\cdot N_ld + |V|\\cdot d) + N_D\\cdot(|\\vec{\\mathbf{\\mathcal{G}}}_p| + |\\vec{\\mathbf{\\mathcal{G}}}_g|)(|V|^2\\cdot N_ld + |V|\\cdot d) + N_E\\cdot(|V|^2\\cdot N_ld + |V|^2\\cdot d + |\\mathcal{O}|\\cdot d)])$\n$O(N_{iter}[N_G + N_D\\cdot(|\\vec{\\mathbf{\\mathcal{G}}}_p| + |\\vec{\\mathbf{\\mathcal{G}}}_g|) + N_E]\\cdot|\\mathcal{V}|^2\\cdot N_lD)$\n, where $N_{iter}$ is the number of iteration round, $N_G$, $N_D$ and $N_E$ are the number of G-Steps, D-Steps and E-Steps, respectively.\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\\label{sec:exp}\nIn this section, we conduct extensive experiments and answer the following research questions:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{RQ1}: How does GACN perform w.r.t. node classification task?\n    \\item \\textbf{RQ2}: How does GACN perform w.r.t. link prediction task?\n    \\item \\textbf{RQ3}: What are the benefits of the proposed modules of GACN?\n    \\item \\textbf{RQ4}: Can the generator of GACN generate high-quality graphs for contrastive learning?\n    \\item \\textbf{RQ5}: How do different settings influence the effectiveness of GACN?\n\\end{itemize}\n\n\n\n\n",
                "subsection 5.1": {
                    "name": "Experimental Settings",
                    "content": "\n",
                    "subsubsection 5.1.1": {
                        "name": "Datasets",
                        "content": "\nWe evaluate the performance of GACN on seven real-world datasets, including two datasets for node classification namely Cora and Citeseer, and five datasets for link prediction namely UCI, Taobao, Amazon, Lastfm and Kuaishou. We summarize the statistics of all the datasets in Table \\ref{tab:datasets}. The detailed information of these datasets is listed as follows.\n\n\\noindent \\textbf{Datasets for Node Classification.}\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Cora}~\\citep{mccallum2000automating} consists of 2,708 scientific publications classified into one of seven classes. The citation network consists of 5,429 edges. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1,433 unique words.\n    \\item \\textbf{Citeseer}~\\citep{giles1998citeseer} is similar to Cora. It  consists of 3,312 nodes and 4,714 edges. The nodes are classified into one of six classes and the dimension of node feature is 3,703.\n\\end{itemize}\n\n% \\begin{table*}\n% \\footnotesize\n% \\centering\n% \\caption{The experimental results of link prediction. The best results are illustrated in bold and the number underlined is the runner-up.}\n% \\begin{tabular}{c|cc|cc|cc|cc|cc}\n% \\toprule\n% Dataset & \\multicolumn{2}{c|}{UCI} & \\multicolumn{2}{c|}{Taobao} & \\multicolumn{2}{c|}{Amazon} & \\multicolumn{2}{c|}{Last.fm} & \\multicolumn{2}{c}{Kuaishou}\\\\\n% Metric  & H@50 & MRR & H@50 & MRR & H@50 & MRR & H@50 & MRR & H@50 & MRR \\\\\n% \\midrule\n% DeepWalk & \\underline{0.2550} & 0.0474 & 0.3522 & \\underline{0.1764} & \\underline{0.4496} & 0.0744 & 0.1344 & 0.0180 & 0.0486 & 0.0055 \\\\\n% LINE & 0.1086 & 0.0249 & 0.3117 & 0.1758 & 0.3327 & 0.0661 & 0.0857 & 0.0112 & 0.0223 & 0.0031 \\\\\n% node2vec & 0.1808 & 0.0312 & 0.3533 & 0.1754 & 0.3026 & 0.0619 & 0.1184 & 0.0181 & 0.0623 & 0.0073 \\\\\n% LightGCN & 0.1093 & 0.0256 & 0.3433 & 0.1692 & 0.4359 & 0.0818 & 0.1368 & \\underline{0.0203} & \\underline{0.0745} & \\underline{0.0093} \\\\\n% \\midrule\n% Simple-GCL & 0.2549 & \\underline{0.0574} & 0.3330 & 0.1355 & 0.4079 & 0.0818 & 0.0581 & 0.0100 & 0.0511 & 0.0060 \\\\\n% DGI & 0.1972 & 0.0310 & 0.1657 & 0.0388 & 0.1463 & 0.0258 & 0.0972 & 0.0151 & 0.0485 & 0.0060 \\\\\n% GraphCL & 0.1669 & 0.0291 & 0.1659 & 0.0348 & 0.1692 & 0.0334 & 0.1012 & 0.0145 & 0.0468 & 0.0061 \\\\\n% GRACE & 0.1915 & 0.0270 & 0.2006 & 0.1056 & 0.3127 & 0.0553 & \\underline{0.1385} & 0.0198 & 0.0439 & 0.0055 \\\\\n% SGL & 0.2545 & \\underline{0.0574} & \\underline{0.3654} & 0.1741 & 0.4014 & 0.0811 & 0.0981 & 0.0150 & 0.0702 & 0.0086 \\\\\n% \\midrule\n% GraphGAN & 0.2543 & 0.0374 & 0.3538 & 0.1390 & 0.4380 & \\underline{0.0882} & 0.0781 & 0.0115 & 0.0544 & 0.0067 \\\\\n% AD-GCL & 0.1819 & 0.0323 & 0.1008 & 0.0214 & 0.0843 & 0.0118 & 0.0822 & 0.0111 & 0.0184 & 0.0024 \\\\\n% GraphMAE & 0.0170 & 0.0052 & 0.1366 & 0.0441 & 0.2660 & 0.0348 & 0.0307 & 0.0043 & 0.0206 & 0.0031 \\\\\n% \\midrule\n% % \\textit{w/o} REG & 0.2082 & 0.0523 & 0.3691 & 0.1761 & 0.4881 & 0.0999 &  0.1478 & 0.0232 & 0.0811 & 0.0100 \\\\\n% \\textit{w/o} REG & 0.0685 & 0.0127 & 0.3159 & 0.1308 & 0.4850 & 0.0997 & 0.1486 & 0.0230 & 0.0840 & 0.0103  \\\\\n% \\textit{w/o} GAN & 0.2389 & 0.0627 & 0.3654 & 0.1741 & 0.4025 & 0.0818 & 0.0982 & 0.0150 & 0.0693 & 0.0086 \\\\\n% \\textit{w/o} SSL & 0.0871 & 0.0160 & 0.1743 & 0.0553 & 0.0380 & 0.0074 &  0.0023 & 0.0004 & 0.0007 & 0.0001 \\\\\n% \\textit{w/o} GCL & 0.0817 & 0.0171 & 0.3775 & 0.1850 & 0.5064 & 0.1036 & 0.1515 & 0.0247 & 0.1001 & 0.0122  \\\\\n% \\textit{w/o} BPR & 0.2727 & 0.0672 & 0.2326 & 0.1270 & 0.3587 & 0.0718 & 0.0497 & 0.0081 & 0.0554 & 0.0066 \\\\\n% \\midrule\n% GACN & \\textbf{0.2836} & \\textbf{0.0692} & \\textbf{0.3794} & \\textbf{0.1895} & \\textbf{0.5593} & \\textbf{0.1158} & \\textbf{0.1568} & \\textbf{0.0263} & \\textbf{0.1067} & \\textbf{0.0132} \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\label{tab:link_prediction}\n% \\end{table*}\n\n\n\n\\noindent \\textbf{Datasets for Link Prediction.}\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{UCI}~\\citep{konect:2016:opsahl-ucsocial} contains the message communications between the students of the University of California, Irvine in an online community.\n    \\item \\textbf{Taobao}~\\citep{zhu2018learning} is offered by Alibaba with user behaviors collected from Taobao\\footnote{https://www.taobao.com/}. There are 1,000 users with all the corresponding interactive items in this dataset.\n    \\item \\textbf{Amazon}~\\citep{he2016ups} is a network that includes product metadata and links between products. We use the data provided by~\\citep{cen2019representation} which contains the product metadata of \\textit{Electronic} category.\n    \\item \\textbf{Last.fm}\\footnote{https://www.last.fm/} contains $<$user, artist, song$>$ tuples collected from Last.fm API, which represents the whole listening habits of nearly 1,000 users. We use the $<$user, artist$>$ pairs to construct a network.\n    \\item \\textbf{Kuaishou}\\footnote{https://www.kuaishou.com/} is collected from the Kuaishou online video-watching platform. This dataset includes the interactions of 6,840 users and 131,972 videos.\n\\end{itemize}\n\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Baseline Methods",
                        "content": "\n\\label{sec:baseline}\nTo demonstrate the effectiveness and efficiency of GACN, we choose twelve state-of-the-art baseline methods, categorized into three groups.\nGraph representation learning models include DeepWalk~\\citep{perozzi2014deepwalk}, LINE~\\citep{tang2015line}, node2vec~\\citep{node2vec-kdd2016} and LightGCN~\\citep{he2020lightgcn}. \nGraph contrastive learning models include DGI~\\citep{velickovic2019deep}, GraphCL~\\citep{you2020graph}, GRACE~\\citep{zhu2020deep} and SGL~\\citep{wu2021self}.\nGraph generative and adversarial learning models include GraphGAN~\\citep{wang2018graphgan} , AD-GCL~\\citep{suresh2021adversarial} and GraphMAE~\\citep{10.1145/3534678.3539321}.\nThe details of the baseline methods are listed as follows.\n\n\n\n\\noindent \\textbf{Graph Representation Learning Models.}\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{DeepWalk}~\\citep{perozzi2014deepwalk} is an embedding method for static homogeneous networks. It exploits the random walk strategy and the skip-gram model to learn node vector representations.\n    \\item \\textbf{LINE}~\\citep{tang2015line} learns node representations by modelling the first- and second-order proximity between node pairs.\n    \\item \\textbf{node2vec}~\\citep{node2vec-kdd2016} adds two parameters to control the random walk process based on DeepWalk.\n    \\item \\textbf{LightGCN}~\\citep{he2020lightgcn} is a light-weight graph convolution network, which is easy to train and has good generalization ability.\n\\end{itemize}\n\n\\noindent \\textbf{Graph Contrastive Learning Models.}\n\\begin{itemize}[leftmargin=*]\n    % \\item \\textbf{Simple-GCL}~\\citep{wu2021self} is a graph contrastive learning method implemented by us and is trained using the InfoNCE~\\cite{oord2018representation} loss upon two views generated via edge-dropping.\n    \\item \\textbf{Simple-GCL} is \\change{a variant of SGL-ED~\\citep{wu2021self}} implemented by us and is trained using the InfoNCE~\\cite{oord2018representation} loss upon two views generated via edge-dropping.\n    \\item \\textbf{DGI}~\\citep{velickovic2019deep} relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs to learn node representations in an unsupervised manner.\n    \\item \\textbf{GraphCL}~\\citep{you2020graph} is a graph contrastive learning framework for learning unsupervised representations of graph data, which designs four types of graph augmentations to incorporate various priors.\n    \\item \\textbf{GRACE}~\\citep{zhu2020deep} generates two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views.\n    \\item \\textbf{SGL}~\\citep{wu2021self} explores self-supervised learning on graph structure and accordingly devises three unified augmentation operators including node dropout, edge dropout and random walk.\n\\end{itemize}\n\n\\noindent \\textbf{Graph Generative and Adversarial Learning Models.}\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{GraphGAN}~\\citep{wang2018graphgan} is a graph representation learning framework unifying the generative model and the discriminative model, in which these two models play a game-theoretical minimax game.\n    \\item \\textbf{AD-GCL}~\\citep{suresh2021adversarial} proposes a principle to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in GCL.\n    \\item \\textbf{GraphMAE}~\\citep{10.1145/3534678.3539321} explores generative self-supervised learning in graphs and designs a state-of-the-art graph autoencoder using the masked feature reconstruction strategy with a scaled cosine error as the reconstruction criterion. \n\\end{itemize}\nNote that we focus on node-level tasks in this paper, and methods designed for graph-level tasks such as GCA~\\citep{zhu2021graph}, JOAO~\\citep{you2021graph}, MVGRL~\\citep{hassani2020contrastive} and GASSL~\\citep{yang2021graph} are not chosen as baselines. \n\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Parameter Settings",
                        "content": "\nWe implement GACN with Pytorch and the model is optimized using the Adam optimizer with learning rate $0.001$ during the training phase.\nBy default, $\\tau_g$ is set to $0.0001$, $\\tau_f$ is set to $0.5$, $\\lambda_{g}$ is set to $0.5$, $\\lambda_{cnt}$ is set to $1$, $\\lambda_{new}$ is set to $0.5$, $\\gamma$ is set to $0.75$.\nFor Cora, Citeseer and UCI, $\\lambda_{gcl}$ is set to $1$ and $\\lambda_{bpr}$ is set to $0.0001$, while for the other datasets, $\\lambda_{gcl}$ is set to $0.0001$ and $\\lambda_{bpr}$ is set to $1$. \nFor all the baseline methods, we tune the parameters according to the validation set and report the best results.\nThe dimension of embedding $s$ of all the model is set to $128$, and all the experiments are conducted on a single GTX 1080Ti GPU.\n\n"
                    },
                    "subsubsection 5.1.4": {
                        "name": "Metrics",
                        "content": "\nFor the node classification task, we choose three widely-used metrics, namely \\textit{P(recision)}, \\textit{R(ecall)} and \\textit{F1}.\nFor the link prediction task, we adopt two ranking metrics, include \\textit{MRR} and \\textit{H(it rate)@k}. In this paper, we report \\textit{H@50} and similar results are observed when $k=20$ and $k=100$.\n\n% \\begin{table*}\n% % \\footnotesize\n% \\centering\n% \\caption{The experimental results of node classification. The best results are illustrated in bold and the number underlined is the runner-up.}\n% \\begin{tabular}{c|ccc|ccc}\n% \\toprule\n% Dataset & \\multicolumn{3}{c|}{Cora} & \\multicolumn{3}{c}{Citeseer} \\\\\n% Metric & Precision & Recall & F1 & Precision & Recall & F1 \\\\\n% \\midrule\n% DeepWalk & 0.775 & 0.701 & 0.729 & 0.558 & 0.496 & 0.487 \\\\\n% LINE & 0.787 & 0.697 & 0.728 & 0.599 & 0.444 & 0.441 \\\\\n% node2vec & 0.774 & 0.735 & 0.752 & 0.472 & 0.458 & 0.455 \\\\\n% LightGCN& 0.761 & 0.734 & 0.745 & 0.443 & 0.451 & 0.436 \\\\\n% \\midrule\n% DGI &  0.832 & 0.813 & 0.821 & 0.643 & 0.636 & 0.628 \\\\\n% GraphCL & 0.799 & 0.750 & 0.770 & 0.655 & 0.616 & 0.611 \\\\\n% GRACE & \\underline{0.855} & \\underline{0.838} & \\underline{0.844} & \\underline{0.723} & \\underline{0.696} & \\underline{0.695} \\\\\n% SGL & 0.803 & 0.779 & 0.789 & 0.533 & 0.524 & 0.508 \\\\\n% \\midrule\n% GraphGAN & 0.529 & 0.205& 0.164 & 0.316 & 0.296 & 0.265 \\\\\n% AD-GCL & 0.418 & 0.367 & 0.380 & 0.284 & 0.281 & 0.276 \\\\\n% GraphMAE & \\\\\n% \\midrule\n% GACN & \\textbf{0.869} & \\textbf{0.855} & \\textbf{0.861} & \\textbf{0.726} & \\textbf{0.716} & \\textbf{0.717} \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\label{tab:node_classification}\n% \\end{table*}\n\n% \\begin{table*}\n% % \\footnotesize\n% \\centering\n% \\caption{The experimental results of link prediction where ``OOM'' means that the method run out of GPU memory when training. The best results are illustrated in bold and the number underlined is the runner-up.}\n% \\begin{tabular}{c|cc|cc|cc|cc|cc}\n% \\toprule\n% Dataset & \\multicolumn{2}{c|}{UCI} & \\multicolumn{2}{c|}{Taobao} & \\multicolumn{2}{c|}{Amazon} & \\multicolumn{2}{c|}{Last.fm} & \\multicolumn{2}{c}{Kuaishou}\\\\\n% Metric  & H@50 & MRR & H@50 & MRR & H@50 & MRR & H@50 & MRR & H@50 & MRR \\\\\n% \\midrule\n% DeepWalk & \\underline{0.2550} & 0.0474 & 0.3522 & \\underline{0.1764} & \\underline{0.4496} & 0.0744 & 0.1344 & 0.0180 & 0.0486 & 0.0055 \\\\\n% LINE & 0.1086 & 0.0249 & 0.3117 & 0.1758 & 0.3327 & 0.0661 & 0.0857 & 0.0112 & 0.0223 & 0.0031 \\\\\n% node2vec & 0.1808 & 0.0312 & 0.3533 & 0.1754 & 0.3026 & 0.0619 & 0.1184 & 0.0181 & 0.0623 & 0.0073 \\\\\n% LightGCN & 0.1093 & 0.0256 & 0.3433 & 0.1692 & 0.4359 & 0.0818 & \\underline{0.1368} & \\underline{0.0203} & \\underline{0.0745} & \\underline{0.0093} \\\\\n% \\midrule\n% Simple-GCL & 0.2549 & \\underline{0.0574} & 0.3330 & 0.1355 & 0.4079 & 0.0818 & 0.0581 & 0.0100 & 0.0511 & 0.0060 \\\\\n% DGI & 0.1607 & 0.0226 & 0.1326 & 0.0454 & 0.1463 & 0.0258 & 0.0526 & 0.0070 & 0.0011 & 0.0002 \\\\\n% GraphCL & 0.1251 & 0.0218 & 0.1304 & 0.0188 & 0.1547 & 0.0249 & 0.0402 & 0.0064 & 0.0167 & 0.0020 \\\\\n% GRACE & 0.1915 & 0.0270 & 0.1473 & 0.0200 & 0.0835 & 0.0121 & 0.0894 & 0.0126 & \\multicolumn{2}{c}{OOM} \\\\\n% SGL & 0.2545 & \\underline{0.0574} & \\underline{0.3654} & 0.1741 & 0.4014 & 0.0811 & 0.0981 & 0.0150 & 0.0702 & 0.0086 \\\\\n% \\midrule\n% GraphGAN & 0.2543 & 0.0374 & 0.3474 & 0.1355 & 0.4326 & \\underline{0.0879} & \\multicolumn{2}{c|}{OOM} & \\multicolumn{2}{c}{OOM}\\\\\n% AD-GCL & 0.1819 & 0.0323 & 0.1008 & 0.0214 & 0.0843 & 0.0118 & 0.0822 & 0.0111 & 0.0184 & 0.0024 \\\\\n% GraphMAE & \\\\\n% \\midrule\n% % \\textit{w/o} REG & 0.2082 & 0.0523 & 0.3691 & 0.1761 & 0.4881 & 0.0999 &  0.1478 & 0.0232 & 0.0811 & 0.0100 \\\\\n% \\textit{w/o} REG & 0.0685 & 0.0127 & 0.3159 & 0.1308 & 0.4850 & 0.0997 & 0.1486 & 0.0230 & 0.0840 & 0.0103  \\\\\n% \\textit{w/o} GAN & 0.2389 & 0.0627 & 0.3654 & 0.1741 & 0.4025 & 0.0818 & 0.0982 & 0.0150 & 0.0693 & 0.0086 \\\\\n% \\textit{w/o} SSL & 0.0871 & 0.0160 & 0.1743 & 0.0553 & 0.0380 & 0.0074 &  0.0023 & 0.0004 & 0.0007 & 0.0001 \\\\\n% \\textit{w/o} GCL & 0.0817 & 0.0171 & 0.3775 & 0.1850 & 0.5064 & 0.1036 & 0.1515 & 0.0247 & 0.1001 & 0.0122  \\\\\n% \\textit{w/o} BPR & 0.2727 & 0.0672 & 0.2326 & 0.1270 & 0.3587 & 0.0718 & 0.0497 & 0.0081 & 0.0554 & 0.0066 \\\\\n% \\midrule\n% GACN & \\textbf{0.2836} & \\textbf{0.0692} & \\textbf{0.3794} & \\textbf{0.1895} & \\textbf{0.5593} & \\textbf{0.1158} & \\textbf{0.1568} & \\textbf{0.0263} & \\textbf{0.1067} & \\textbf{0.0132} \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\label{tab:link_prediction}\n% \\end{table*}\n\n% \\begin{table}\n% % \\footnotesize\n% \\centering\n% \\caption{The experimental results of ablation study.}\n% \\begin{tabular}{c|cc|cc}\n% \\hline\n% Dataset & \\multicolumn{2}{c|}{UCI} & \\multicolumn{2}{c}{Amazon} \\\\\n% Metric  & H@50 & MRR & H@50 & MRR  \\\\\n% \\hline\n% \\textit{w/o} REG &  &  \\\\\n% \\textit{w/o} GAN & \\todo{0.2558} & \\todo{0.0569} & \\todo{0.4097} & \\todo{0.0828} \\\\\n% \\textit{w/o} SSL &  &  & \\\\\n% \\hline\n% GACN & \\textbf{0.2696} & \\textbf{0.0686} & \\textbf{0.4976} & \\textbf{0.1019} \\\\\n% \\hline\n% \\end{tabular}\n% \\label{tab:ablation}\n% \\end{table}\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Node Classification (RQ1)",
                    "content": "\n% \\review{Add a larger dataset.}\n% In the subsection, we evaluate the performance of GACN w.r.t. the node classification task on the Cora and the Citeseer datasets.  Based on the results reported in Table~\\ref{tab:node_classification}, we have the following observations: \n\nWe evaluate the performance of GACN w.r.t. the node classification task on the Cora and the Citeseer datasets.\nFollowing the experimental setting as \\citet{hassani2020contrastive} and \\citet{velickovic2019deep}, we first run different methods without supervision to generate all the nodes' embeddings.\nThen we train a linear classifier and report the mean accuracy on the test nodes through 10 random initialization.\nAs shown in Table~\\ref{tab:node_classification}, GACN achieves the best results compared to the SOTA baseline methods in all benchmarks.\nNotably, GACN outperforms existing GCL methods by a large margin on two node classification datasets. Notice that AD-GCL is designed for graph-level tasks and has poor performances on node-level tasks.\n\n"
                },
                "subsection 5.3": {
                    "name": "Link Prediction (RQ2)",
                    "content": "\nIn the subsection, we conduct the link prediction on the UCI, Taobao, Amazon, Last.fm and the Kuaishou datasets.\nTable~\\ref{tab:link_prediction} reports the experimental results and it is observed that:\n1) GACN consistently performs the best on all datasets compared to other methods. We attribute these results to the fact that GACN is able to explore unseen edges and generate high-quality views for graph contrastive learning.\n2) Although SGL leverages the BPR loss and the contrastive loss for self-supervise learning, GACN still outperforms SGL, showing the effectiveness of incorporating GCL with graph GANs.\n% 3) GRACE and GraphGAN run out of GPU memory due to the requirement of maintaining the dense adjacent matrix in memory.\n\n\n% Dataset & \\multicolumn{3}{c|}{Cora} & \\multicolumn{3}{c}{Citeseer} \\\\\n% Metric & Precision & Recall & F1 & Precision & Recall & F1 \\\\\n% \\change{\\textit{w/o} REG} & 0.8650 & 0.8385 & 0.8498 & 0.7250 & 0.7136 & 0.7155 \\\\\n% \\change{\\textit{w/o} GAN} & 0.8670 & 0.8497 & 0.8571 & 0.7286 & 0.7170 & 0.7191 \\\\\n% \\change{\\textit{w/o} SSL} & 0.8608 & 0.8368 & 0.8470 & 0.7265 & 0.7151 & 0.7168 \\\\\n% \\change{\\textit{w/o} GCL} & 0.7226 & 0.3442 & 0.3565 & 0.7305 & 0.7175 & 0.7194 \\\\\n% \\change{\\textit{w/o} BPR} & 0.8632 & 0.8392 & 0.8494 & 0.7307 & 0.7181 & 0.7207 \\\\\n\n\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Ablation Study (RQ3)",
                    "content": "\nIn this subsection, we evaluate the effectiveness of different components of GACN, with five variants as follows:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{\\textit{w/o} REG}: The view generator is trained without the regularization loss $\\mathcal{L}_{reg}$.\n    \\item \\textbf{\\textit{w/o} GAN}: The contrastive loss $\\mathcal{L}_{gcl}$ is optimized using views generated by predefined augmentation strategies only.\n    \\item \\textbf{\\textit{w/o} SSL}: The self-supervised learning losses are ignored and the model is optimized using the G-Steps and D-Steps only.\n    \\item \\textbf{\\textit{w/o} GCL}: The graph contrastive loss $\\lambda_{gcl}$ is ignored during E-Steps.\n    \\item \\textbf{\\textit{w/o} BPR}: The bayesian personalized ranking loss $\\lambda_{bpr}$ is ignored during E-Steps.\n\\end{itemize}\n\n% \\change{Table~\\ref{tab:node_classification} and} Table~\\ref{tab:link_prediction} shows the experimental results.\n\\change{Table~\\ref{tab:ablation}} shows the experimental results.\nWe can find that:\n1) The regularization loss plays as an assistant role in performance, which indicates that $\\mathcal{L}_{reg}$ helps to generate rational views for contrastive learning.\n2) The graph GAN is important in GACN, showing the benefits of utilizing GAN to generate views and the joint learning framework.\n3) The self-supervised learning losses are essential to GACN, because the GAN in GACN is based on graph-level classification and does not focus on learning node representations.\n\n% \\begin{itemize}[leftmargin=*]\n%     \\item The regularization loss plays as an assistant role in performance, which indicates that $\\mathcal{L}_{reg}$ helps to generate rational views for contrastive learning.\n%     \\item The graph GAN is important in GACN, showing the benefits of utilizing GAN to generate views and the joint learning framework.\n%     \\item The self-supervised learning losses are essential to GACN, because the GAN in GACN is based on graph-level classification and does not focus on learning node representations.\n% \\end{itemize}\n\n\n\n\n"
                },
                "subsection 5.5": {
                    "name": "Quality of Generated Graphs (RQ4)",
                    "content": "\n\\label{sec:case}\nIn this subsection, we investigate the quality of graphs generated by the view generator w.r.t. \\textit{Impact of $\\lambda_{cnt}$ and $\\lambda_{new}$}, \\textit{New Edge Distribution} and \\textit{Case Study}.\n\n",
                    "subsubsection 5.5.1": {
                        "name": "new",
                        "content": "\nIn this part, we run the link prediction task on UCI under different settings of $\\lambda_{cnt}$ and $\\lambda_{new}$, while in each training epoch, we generate ten views and calculate the average amount of edges and the new edges.\nAs shown in Figure~\\ref{fig:new_count}, $\\lambda_{cnt}$ contributes to the stability of the edge count, while $\\lambda_{new}$ helps to limit the number of new edges.\nSpecifically, when $\\lambda_{cnt} = 1$ and $\\lambda_{new} = 0.5$, the number of new edges decreases gradually as the training goes on while the edge count remains stable, which obtains the highest MRR compared to other settings.\n\n\n\n\n\n"
                    },
                    "subsubsection 5.5.2": {
                        "name": "New Edge Distribution",
                        "content": "\nTo further investigate the distribution of new edges, we count the number of new edges in groups divided by node degree.\nFigure~\\ref{fig:degree} illustrates that compared to randomly adding new edges, GACN 1) is able to adjust the number of new edges during training, and 2) generates more edges for high degree nodes (i.e., the color of high degree nodes is more similar to low degree nodes in Figure~\\ref{fig:degree_best} compared to that in Figure~\\ref{fig:degree_random}), which is in agreement with the preferential attachment rule~\\citep{barabasi1999emergence}. \n\n"
                    },
                    "subsubsection 5.5.3": {
                        "name": "Case Study",
                        "content": "\nFor better insight of the views generated by GACN, we randomly sample some nodes in UCI and visualize their neighborhoods within two hops. \nFigure~\\ref{fig:case_study} shows that GACN tends to attach nodes to those with high degree and removes other edges, which confirms that GACN indeed learns the preferential attachment rule~\\citep{barabasi1999emergence} and is able to generate reasonable alternative views for contrastive learning.\n\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\subfigure[Impact of $\\lambda_{cnt}$]{\n%     \\label{fig:ps_lambda_cnt}\n%     \\begin{minipage}[]{0.45\\linewidth}\n%       \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_lambda_cnt.pdf}\n%     \\end{minipage}\n%   }\n%   \\subfigure[Impact of $\\lambda_{new}$]{\n%     \\label{fig:ps_lambda_new}\n%     \\begin{minipage}[]{0.45\\linewidth}\n%       \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_lambda_new.pdf}\n%     \\end{minipage}\n%   }\n%   \\subfigure[Impact of $\\gamma$]{\n%     \\label{fig:ps_gamma}\n%     \\begin{minipage}[]{0.45\\linewidth}\n%       \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_init_rate.pdf}\n%     \\end{minipage}\n%   }\n%   \\caption{The experimental results of parameter sensitivity w.r.t. $\\lambda_{cnt}, \\lambda_{new}, \\gamma$.}\n%   \\label{fig:parameter_sensitivity_appendix}\n% \\end{figure}\n\n"
                    }
                },
                "subsection 5.6": {
                    "name": "Parameter Sensitivity (RQ5)",
                    "content": "\nIn this subsection, we analyze the sensitivity of hyper-parameters in GACN. Specifically, we first examine the impact of the dimension of embedding $s$, the hyper-parameter of the view generator $\\tau_g$ and $\\lambda_{g}$, and the temperature of the contrastive learning $\\tau_f$.\nWe report the MRR rate $\\eta = \\frac{\\mbox{MRR with current settings}}{\\mbox{MRR with default settings}}$\n% over the default settings\nw.r.t. the link prediction task in Figure~\\ref{fig:parameter_sensitivity}.\n\nAs shown in Figure \\ref{fig:ps_s}, the larger dimension of embedding yields the better performance due to the strengthened expression capability of the GACN model.\n% From Figure \\ref{fig:ps_s}, we observe that larger dimension of embedding yield better performance due to the strengthened expression capability of the model.\nFigure~\\ref{fig:ps_tau_g} shows that GACN is insensitive to $\\tau_g$. However, a large $\\tau_g$ may result in poor performance.\nAs shown in Figure~\\ref{fig:ps_lambda_g}, GACN is sensitive to $\\lambda_g$. Specifically, a small $\\lambda_g$ results in sparse views, which are uninformative for contrastive learning, while a large $\\lambda_g$ yields dense views, which do harm to robust node representation learning. Generally, setting $\\lambda_g$ to $[0.5, 0.7]$ is a good choice.\nFrom Figure~\\ref{fig:ps_tau_f}, it is observed that different datasets require different $\\tau_f$ for best performance. \nIn general, setting $\\tau_f$ to $0.5$ yields competitive performances.\n\nWe also analyze how the view generator influences GCL, i.e., the sensitivity of $\\lambda_{cnt}, \\lambda_{new}$ and $\\gamma$. It is observed that GACN setting $\\lambda_{cnt}$ to $1$ can obtain competitive results (see Figure~\\ref{fig:ps_lambda_cnt}), while a small $\\lambda_{new}$ is preferred (see Figure~\\ref{fig:ps_lambda_new}). However, setting $\\lambda_{new}$ to $0$ generates a large amount of unseen edges, and can result in poor performance in some datasets. Thus setting $\\lambda_{new}$ to $0.25$ is a better choice. In contrast, different datasets are sensitive to $\\gamma$ (see Figure~\\ref{fig:ps_gamma}). However, setting $\\gamma$ to $0.75$ produces satisfying results.\n\n% \\subsubsection{Effect of $s$}\n% From Figure \\ref{fig:ps_s}, we observe that larger dimension of embedding yield better performance due to the strengthened expression capability of the model.\n\n% \\subsubsection{Effect of $\\tau_g$}\n% Figure \\ref{fig:ps_tau_g} shows that GACN is insensitive to $\\tau_g$. However, a large $\\tau_g$ may result in poor performance.\n\n% \\subsubsection{Effect of $\\lambda_{g}$}\n% As shown in Figure \\ref{fig:ps_lambda_g}, GACN is sensitive to $\\lambda_g$. Specifically, a small $\\lambda_g$ results in sparse views, which are uninformative for contrastive learning, while a large $\\lambda_g$ yield dense views, which do harm to robust node representation learning. Generally, setting $\\lambda_g$ to $[0.5, 0.7]$ is a good choice.\n\n% \\subsubsection{Effect of $\\tau_f$}\n% From Figure \\ref{fig:ps_tau_f}, it is observed that different datasets requires different $\\tau_f$ for best performance. In general, setting $\\tau_f$ to $0.5$ yield competitive performances.\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\nIn this paper, we incorporated graph GANs with GCL w.r.t. node-level tasks, and presented GACN, a new GNN model that leveraged a graph GAN to generate augmented views for GCL.\nSpecifically, GACN developed a view generator, a view discriminator and a graph encoder to learn node representations in a self-supervised learning style.\nBesides, a novel optimization framework was proposed to train the modules of GACN jointly.\nThrough comprehensive experiments on seven real-world datasets, we empirically showed the superiority of GACN.\nIn the future, GACN will be developed to deal with heterogeneous and dynamic graphs.\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\\begin{acks}\n\\change{This work is supported in part by the National Natural Science Foundation of China (No.~61872207) and Kuaishou Inc.\nChaokun Wang is the corresponding author.}\n\\end{acks}\n\n% \\clearpage\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{GACoN}\n\n% \\clearpage\n\n%%\n%% If your work has an appendix, this is the place to put it.\n% \\appendix\n% \\section{Appendix}\n% \\subsection{How the View Generator influences GCL}\n% \\begin{figure}[ht]\n%   \\centering\n%   \\subfigure[Impact of $\\lambda_{cnt}$]{\n%     \\label{fig:ps_lambda_cnt}\n%     \\begin{minipage}[]{0.45\\linewidth}\n%       \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_lambda_cnt.pdf}\n%     \\end{minipage}\n%   }\n%   \\subfigure[Impact of $\\lambda_{new}$]{\n%     \\label{fig:ps_lambda_new}\n%     \\begin{minipage}[]{0.45\\linewidth}\n%       \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_lambda_new.pdf}\n%     \\end{minipage}\n%   }\n%   \\subfigure[Impact of $\\gamma$]{\n%     \\label{fig:ps_gamma}\n%     \\begin{minipage}[]{0.45\\linewidth}\n%       \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_init_rate.pdf}\n%     \\end{minipage}\n%   }\n%   \\caption{The experimental results of parameter sensitivity w.r.t. $\\lambda_{cnt}, \\lambda_{new}, \\gamma$.}\n%   \\label{fig:parameter_sensitivity_appendix}\n% \\end{figure}\n\n% Figure~\\ref{fig:parameter_sensitivity_appendix} demonstrates the sensitivity of the hyper-parameters in the view generator, i.e., $\\lambda_{cnt}, \\lambda_{new}$ and $\\gamma$. It is observed that GACN setting $\\lambda_{cnt}$ to $1$ can obtain competitive results, while a small $\\lambda_{new}$ is preferred. However, setting $\\lambda_{new}$ to $0$ generates a large amount of unseen edges, and can result in poor performance in some datasets. Thus setting $\\lambda_{new}$ to $0.25$ is a better choice. In contrast, different datasets are sensitive to $\\gamma$. However, setting $\\gamma$ to $0.75$ produces satisfying results.\n"
            }
        },
        "tables": {
            "tab:notations": "\\begin{table}[t]\n\\footnotesize\n\\centering\n\\caption{Notations in this paper.}\n\\begin{tabular}{c|l}\n\\toprule\nNotations & Definitions \\\\\n\\midrule\n$\\mathcal{V}$ & the set of nodes of a graph \\\\\n$\\mathcal{E}$ & the set of edges of a graph, $\\mathcal{E}\\subseteq\n\\mathcal{V}\\times\\mathcal{V}$\\\\\n$\\mathcal{G}$ & a graph $\\mathcal{G}= (\\mathcal{V}, \\mathcal{E})$ \\\\\n$\\mathcal{X}$ & the node attributes that $\\mathcal{G}$ may have \\\\\n$F$ & the dimension of node attributes \\\\\n$D$ & the dimension of node representations \\\\\n$A$ & the adjacent matrix of $\\mathcal{G}$, $A\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$ \\\\\n$P$ & the approximation of the generated adjacent matrix\\\\\n% , $P\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$ \\\\\n$d_v$ & the node representation of node $v\\in\\mathcal{V}$ \\\\\n$\\theta_g$ & the parameters of the view generator  \\\\\n$\\theta_h$ & the parameters of the view discriminator  \\\\\n$\\theta_f$ & the parameters of the graph encoder  \\\\\n% \\midrule\n% $\\tau_g$ & \\\\\n% $\\lambda_g$ & \\\\\n% $\\lambda_{cnt}$ & \\\\\n% $\\lambda_{new}$ & \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:notations}\n\\end{table}",
            "tab:datasets": "\\begin{table}\n\\footnotesize\n\\centering\n\\caption{The statistics of datasets.}\n\\begin{tabular}{cccc}\n\\toprule\nDatasets & $|\\mathcal{V}|$ & $|\\mathcal{E}|$ & Task \\\\\n\\midrule\nCora & 2,708 & 5,429 & Node Classification\\\\\nCiteseer & 3,312 & 4,714 & Node Classification\\\\\n% ogbn-arxiv & 169,343 & 1,186,243 & Node Classification\\\\\n\\midrule\nUCI & 1,677 & 56,617 & Link Prediction \\\\\nTaobao & 12,611 & 20,890 & Link Prediction \\\\\nAmazon & 10,099 & 148,659 & Link Prediction \\\\\nLast.fm & 127,786 & 720,537 & Link Prediction \\\\\nKuaishou & 138,812 & 1,779,639 & Link Prediction \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:datasets}\n\\end{table}",
            "tab:node_classification": "\\begin{table}\n\\footnotesize\n\\centering\n\\caption{The experimental results of node classification. The best results are illustrated in bold and the number underlined is the runner-up.}\n\\begin{tabular}{c|ccc|ccc}\n\\toprule\nDataset & \\multicolumn{3}{c|}{Cora} & \\multicolumn{3}{c}{Citeseer} \\\\\nMetric & P & R & F1 & P & R & F1 \\\\\n\\midrule\nDeepWalk & 0.7753 & 0.7012 & 0.7292 & 0.5579 & 0.4962 & 0.4869 \\\\\nLINE & 0.7873 & 0.6970 & 0.7281 & 0.5992 & 0.4437 & 0.4413 \\\\\nnode2vec & 0.7744 & 0.7352 & 0.7516 & 0.4717 & 0.4581 & 0.4552 \\\\\nLightGCN& 0.7615 & 0.7342 & 0.7453 & 0.4434 & 0.4513 & 0.4361 \\\\\n\\midrule\nSimple-GCL & 0.8491 & 0.8154 & 0.8287 & 0.7019 & 0.6930 & 0.6946 \\\\\nDGI &  0.8320 & 0.8129 & 0.8212 & 0.6427 & 0.6357 & 0.6278 \\\\\nGraphCL & 0.7993 & 0.7500 & 0.7689 & 0.6547 & 0.6156 & 0.6112 \\\\\nGRACE & 0.8546 & \\underline{0.8377} & 0.8445 & 0.7232 & 0.6963 & 0.6948 \\\\\nSGL & 0.8029 & 0.7769 & 0.7887 & 0.7177 & 0.7045 & 0.7063 \\\\\n\\midrule\nGraphGAN & 0.4195 & 0.2177 & 0.1745 & 0.3148 & 0.2994 & 0.2696 \\\\\nAD-GCL & 0.4176 & 0.3672 & 0.3800 & 0.2842 & 0.2809 & 0.2763 \\\\\nGraphMAE & \\underline{0.8667} & 0.8287 & \\underline{0.8447} & \\underline{0.7278} & \\underline{0.7048} & \\underline{0.7064} \\\\\n\\midrule\n% \\change{\\textit{w/o} REG} & 0.8650 & 0.8385 & 0.8498 & 0.7250 & 0.7136 & 0.7155 \\\\\n% \\change{\\textit{w/o} GAN} & 0.8670 & 0.8497 & 0.8571 & 0.7286 & 0.7170 & 0.7191 \\\\\n% \\change{\\textit{w/o} SSL} & 0.8608 & 0.8368 & 0.8470 & 0.7265 & 0.7151 & 0.7168 \\\\\n% \\change{\\textit{w/o} GCL} & 0.7226 & 0.3442 & 0.3565 & 0.7305 & 0.7175 & 0.7194 \\\\\n% \\change{\\textit{w/o} BPR} & 0.8632 & 0.8392 & 0.8494 & 0.7307 & 0.7181 & 0.7207 \\\\\n% \\midrule\nGACN & \\textbf{0.8705} & \\textbf{0.8545} & \\textbf{0.8614} & \\textbf{0.7311} & \\textbf{0.7187} & \\textbf{0.7212} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:node_classification}\n\\end{table}",
            "tab:link_prediction": "\\begin{table*}\n\\footnotesize\n\\centering\n\\caption{The experimental results of link prediction. The best results are illustrated in bold and the number underlined is the runner-up.}\n\\begin{tabular}{c|cc|cc|cc|cc|cc}\n\\toprule\nDataset & \\multicolumn{2}{c|}{UCI} & \\multicolumn{2}{c|}{Taobao} & \\multicolumn{2}{c|}{Amazon} & \\multicolumn{2}{c|}{Last.fm} & \\multicolumn{2}{c}{Kuaishou}\\\\\nMetric  & H@50 & MRR & H@50 & MRR & H@50 & MRR & H@50 & MRR & H@50 & MRR \\\\\n\\midrule\nDeepWalk & \\underline{0.2550} & 0.0474 & 0.3522 & \\underline{0.1764} & \\underline{0.4496} & 0.0744 & 0.1344 & 0.0180 & 0.0486 & 0.0055 \\\\\nLINE & 0.1086 & 0.0249 & 0.3117 & 0.1758 & 0.3327 & 0.0661 & 0.0857 & 0.0112 & 0.0223 & 0.0031 \\\\\nnode2vec & 0.1808 & 0.0312 & 0.3533 & 0.1754 & 0.3026 & 0.0619 & 0.1184 & 0.0181 & 0.0623 & 0.0073 \\\\\nLightGCN & 0.1093 & 0.0256 & 0.3433 & 0.1692 & 0.4359 & 0.0818 & 0.1368 & \\underline{0.0203} & \\underline{0.0745} & \\underline{0.0093} \\\\\n\\midrule\nSimple-GCL & 0.2549 & \\underline{0.0574} & 0.3330 & 0.1355 & 0.4079 & 0.0818 & 0.0581 & 0.0100 & 0.0511 & 0.0060 \\\\\nDGI & 0.1972 & 0.0310 & 0.1657 & 0.0388 & 0.1463 & 0.0258 & 0.0972 & 0.0151 & 0.0485 & 0.0060 \\\\\nGraphCL & 0.1669 & 0.0291 & 0.1659 & 0.0348 & 0.1692 & 0.0334 & 0.1012 & 0.0145 & 0.0468 & 0.0061 \\\\\nGRACE & 0.1915 & 0.0270 & 0.2006 & 0.1056 & 0.3127 & 0.0553 & \\underline{0.1385} & 0.0198 & 0.0439 & 0.0055 \\\\\nSGL & 0.2545 & \\underline{0.0574} & \\underline{0.3654} & 0.1741 & 0.4014 & 0.0811 & 0.0981 & 0.0150 & 0.0702 & 0.0086 \\\\\n\\midrule\nGraphGAN & 0.2543 & 0.0374 & 0.3538 & 0.1390 & 0.4380 & \\underline{0.0882} & 0.0781 & 0.0115 & 0.0544 & 0.0067 \\\\\nAD-GCL & 0.1819 & 0.0323 & 0.1008 & 0.0214 & 0.0843 & 0.0118 & 0.0822 & 0.0111 & 0.0184 & 0.0024 \\\\\nGraphMAE & 0.0170 & 0.0052 & 0.1366 & 0.0441 & 0.2660 & 0.0348 & 0.0307 & 0.0043 & 0.0206 & 0.0031 \\\\\n\\midrule\n% % \\textit{w/o} REG & 0.2082 & 0.0523 & 0.3691 & 0.1761 & 0.4881 & 0.0999 &  0.1478 & 0.0232 & 0.0811 & 0.0100 \\\\\n% \\textit{w/o} REG & 0.0685 & 0.0127 & 0.3159 & 0.1308 & 0.4850 & 0.0997 & 0.1486 & 0.0230 & 0.0840 & 0.0103  \\\\\n% \\textit{w/o} GAN & 0.2389 & 0.0627 & 0.3654 & 0.1741 & 0.4025 & 0.0818 & 0.0982 & 0.0150 & 0.0693 & 0.0086 \\\\\n% \\textit{w/o} SSL & 0.0871 & 0.0160 & 0.1743 & 0.0553 & 0.0380 & 0.0074 &  0.0023 & 0.0004 & 0.0007 & 0.0001 \\\\\n% \\textit{w/o} GCL & 0.0817 & 0.0171 & 0.3775 & 0.1850 & 0.5064 & 0.1036 & 0.1515 & 0.0247 & 0.1001 & 0.0122  \\\\\n% \\textit{w/o} BPR & 0.2727 & 0.0672 & 0.2326 & 0.1270 & 0.3587 & 0.0718 & 0.0497 & 0.0081 & 0.0554 & 0.0066 \\\\\n% \\midrule\nGACN & \\textbf{0.2836} & \\textbf{0.0692} & \\textbf{0.3794} & \\textbf{0.1895} & \\textbf{0.5593} & \\textbf{0.1158} & \\textbf{0.1568} & \\textbf{0.0263} & \\textbf{0.1067} & \\textbf{0.0132} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:link_prediction}\n\\end{table*}",
            "tab:ablation": "\\begin{table*}\n\\footnotesize\n\\centering\n\\caption{\\change{Ablation Study. The best results are illustrated in bold and the number underlined is the runner-up.}}\n\\begin{tabular}{c|ccc|ccc|cc|cc|cc|cc|cc}\n\\toprule\nDataset & \\multicolumn{3}{c|}{Cora} & \\multicolumn{3}{c|}{Citeseer} & \\multicolumn{2}{c|}{UCI} & \\multicolumn{2}{c|}{Taobao} & \\multicolumn{2}{c|}{Amazon} & \\multicolumn{2}{c|}{Last.fm} & \\multicolumn{2}{c}{Kuaishou}\\\\\nMetric & P & R & F1 & P & R & F1 & H@50 & MRR & H@50 & MRR & H@50 & MRR & H@50 & MRR & H@50 & MRR \\\\\n\\midrule\n% \\textit{w/o} REG & 0.2082 & 0.0523 & 0.3691 & 0.1761 & 0.4881 & 0.0999 &  0.1478 & 0.0232 & 0.0811 & 0.0100 \\\\\n\\textit{w/o} REG & 0.8650 & 0.8385 & 0.8498 & 0.7250 & 0.7136 & 0.7155 & 0.0685 & 0.0127 & 0.3159 & 0.1308 & 0.4850 & 0.0997 & 0.1486 & 0.0230 & 0.0840 & 0.0103  \\\\\n\\textit{w/o} GAN & \\underline{0.8670} & \\underline{0.8497} & \\underline{0.8571} & 0.7286 & 0.7170 & 0.7191 & 0.2389 & 0.0627 & 0.3654 & 0.1741 & 0.4025 & 0.0818 & 0.0982 & 0.0150 & 0.0693 & 0.0086 \\\\\n\\textit{w/o} SSL & 0.8608 & 0.8368 & 0.8470 & 0.7265 & 0.7151 & 0.7168 & 0.0871 & 0.0160 & 0.1743 & 0.0553 & 0.0380 & 0.0074 &  0.0023 & 0.0004 & 0.0007 & 0.0001 \\\\\n\\textit{w/o} GCL & 0.7226 & 0.3442 & 0.3565 & 0.7305 & 0.7175 & 0.7194 & 0.0817 & 0.0171 & \\underline{0.3775} & \\underline{0.1850} & \\underline{0.5064} & \\underline{0.1036} & \\underline{0.1515} & \\underline{0.0247} & \\underline{0.1001} & \\underline{0.0122}  \\\\\n\\textit{w/o} BPR & 0.8632 & 0.8392 & 0.8494 & \\underline{0.7307} & \\underline{0.7181} & \\underline{0.7207} & \\underline{0.2727} & \\underline{0.0672} & 0.2326 & 0.1270 & 0.3587 & 0.0718 & 0.0497 & 0.0081 & 0.0554 & 0.0066 \\\\\n\\midrule\nGACN & \\textbf{0.8705} & \\textbf{0.8545} & \\textbf{0.8614} & \\textbf{0.7311} & \\textbf{0.7187} & \\textbf{0.7212} & \\textbf{0.2836} & \\textbf{0.0692} & \\textbf{0.3794} & \\textbf{0.1895} & \\textbf{0.5593} & \\textbf{0.1158} & \\textbf{0.1568} & \\textbf{0.0263} & \\textbf{0.1067} & \\textbf{0.0132} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:ablation}\n\\end{table*}"
        },
        "figures": {
            "fig:example": "\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=1.0\\linewidth]{figures/Example.pdf} \n\\caption{\\change{\n% % Take a video-watching platform as an example.\n% Suppose a new video is uploaded at 09:00. % in a video-watching platform.\n% Alice watches the video at 09:10 and Bob watches it at 09:20 (Case 1).\n% If the observed graph is obtained at 09:15, the watching behavior from Bob is unseen.\n% Besides, it is possible that Bob watches the video at 09:10 and Alice watches it at 09:20 (Case 2). In other words, the observed graph is sampled from a graph distribution where both Bob and Alice watch the video.\nA toy example of the evolution and the distribution of graphs. In detail, suppose a new video is uploaded at 09:00. Next, different observed graphs, such as Case 1 and Case 2, are sampled from a\ngraph distribution and then evolve respectively. For instance, in Case 1 (Case 2), Alice (Bob) watches the video at 09:10 and Bob (Alice) watches it at 09:20. If the observed graph is obtained at 09:15 (09:17), the watching behavior from Bob (Alice) is unseen.\n}}\n\\label{fig:example}\n\\end{figure}",
            "fig:motivation": "\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{figures/ner_bar.pdf} \n\\caption{\n% Empirical experiments show that adding random new edges to one of the augmented views can improve Simple-GCL on most datasets.\nEmpirical experiments show that replacing some existing edges with random new edges in one of the augmented views can improve Simple-GCL on most datasets.\nHowever, it requires a \\change{trial-and-error} selection of the new edge rate to get the best performance.\nIn contrast, \\change{our} GACN can automatically learn the graph distribution and precisely add edges for better graph representation learning.}\n\\label{fig:motivation}\n\\end{figure}",
            "fig:architure": "\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figures/GACoN.pdf} \n\\caption{\\change{The architecture and the training steps of GACN. There are three modules in GACN, including the view generator, the view discriminator and the graph encoder, which are optimized by the G-Steps, the D-Steps and the E-Steps, respectively.}}\n\\label{fig:architure}\n\\end{figure*}",
            "fig:new_count": "\\begin{figure}[ht]\n  \\centering\n  \\subfigure[$\\lambda_{cnt} = 0, \\lambda_{new} = 0$]{\n    \\label{fig:new_0_count_0}\n    \\begin{minipage}[]{0.475\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/quality/new_0_count_0_init_rate_0_6.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[$\\lambda_{cnt} = 1, \\lambda_{new} = 0$]{\n    \\label{fig:new_0_count_1}\n    \\begin{minipage}[]{0.475\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/quality/new_0_count_1_init_rate_0_6.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[$\\lambda_{cnt} = 0, \\lambda_{new} = 0.5$]{\n    \\label{fig:new_0_5_count_0}\n    \\begin{minipage}[]{0.475\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/quality/new_0_5_count_0_init_rate_0_6.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[$\\lambda_{cnt} = 1, \\lambda_{new} = 0.5$]{\n    \\label{fig:new_0_5_count_1}\n    \\begin{minipage}[]{0.475\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/quality/new_0_5_count_1_init_rate_0_6.pdf}\n    \\end{minipage}\n  }\n  % \\subfigure[$\\lambda_{cnt} = 1, \\lambda_{new} = 0.1$]{\n  %   \\label{fig:new_0_1_count_1}\n  %   \\begin{minipage}[]{0.23\\linewidth}\n  %     \\includegraphics[width=1.0\\linewidth]{figures/quality/new_0_1_count_1_init_rate_0_6.pdf}\n  %   \\end{minipage}\n  % }\n  \\caption{\\change{The impact of $\\lambda_{cnt}$ and $\\lambda_{new}$.}}\n  \\label{fig:new_count}\n\\end{figure}",
            "fig:degree": "\\begin{figure}[t]\n  \\centering\n  \\subfigure[Random]{\n    \\label{fig:degree_random}\n    \\begin{minipage}[]{0.475\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/quality/degree_random.pdf}\n    \\end{minipage}\n  }\n%   \\subfigure[$\\gamma = 0.6$]{\n  \\subfigure[GACN]{\n    \\label{fig:degree_best}\n    \\begin{minipage}[]{0.475\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/quality/degree_best.pdf}\n    \\end{minipage}\n  }\n%   \\subfigure[$\\gamma = 0$]{\n%     \\label{fig:degree_zero_init}\n%     \\begin{minipage}[]{0.23\\linewidth}\n%       \\includegraphics[width=1.0\\linewidth]{figures/quality/degree_zero_init.pdf}\n%     \\end{minipage}\n%   }\n%   \\subfigure[w/o $\\gamma$]{\n%     \\label{fig:degree_no_init}\n%     \\begin{minipage}[]{0.23\\linewidth}\n%       \\includegraphics[width=1.0\\linewidth]{figures/quality/degree_no_init.pdf}\n%     \\end{minipage}\n%   }\n  \\caption{The experimental results of the distribution of new edges.}\n  \\label{fig:degree}\n\\end{figure}",
            "fig:case_study": "\\begin{figure*}[ht]\n  \\centering\n  \\subfigure[Case 1]{\n    \\label{fig:case_1}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/case/case_study_0.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Case 2]{\n    \\label{fig:case_2}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/case/case_study_1.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Case 3]{\n    \\label{fig:case_3}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/case/case_study_2.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Case 4]{\n    \\label{fig:case_4}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/case/case_study_3.pdf}\n    \\end{minipage}\n  }\n  \\caption{\n  % Edges existing in the original graph are drawn in black, new edges generated by GACN are drawn in red, and dropped edges are drawn in dashed and gray.\n  Each case is the neighborhood of a sampled node in the generated view.\n  Edges in red are generated by GACN. Edges in black are those existing in the original graph and preserved by GACN. \n  Edges in gray and dashed are those existing in the original graph but dropped by GACN.\n  It is observed that GACN learns the preferential attachment rule and tends to attach nodes to those with high degree.}\n  \\label{fig:case_study}\n  % \\vspace{-10pt}\n\\end{figure*}",
            "fig:parameter_sensitivity": "\\begin{figure*}[ht]\n  \\centering\n  \\subfigure[Impact of $s$]{\n    \\label{fig:ps_s}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_s.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Impact of $\\tau_g$]{\n    \\label{fig:ps_tau_g}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_tau_g.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Impact of $\\lambda_g$]{\n    \\label{fig:ps_lambda_g}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_lambda_g.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Impact of $\\tau_f$]{\n    \\label{fig:ps_tau_f}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_tau_f.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Impact of $\\lambda_{cnt}$]{\n    \\label{fig:ps_lambda_cnt}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_lambda_cnt.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Impact of $\\lambda_{new}$]{\n    \\label{fig:ps_lambda_new}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_lambda_new.pdf}\n    \\end{minipage}\n  }\n  \\subfigure[Impact of $\\gamma$]{\n    \\label{fig:ps_gamma}\n    \\begin{minipage}[]{0.22\\linewidth}\n      \\includegraphics[width=1.0\\linewidth]{figures/ps/ps_init_rate.pdf}\n    \\end{minipage}\n  }\n  \\caption{The experimental results of parameter sensitivity.}\n  \\label{fig:parameter_sensitivity}\n  % \\vspace{-11pt}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n    A_{i,j} = \\left\\{\n         \\begin{array}{lr}\n         1, \\quad \\mbox{if $(v_i, v_j)\\in\\mathcal{E}$;} \\\\\n         0, \\quad \\mbox{otherwise.}\n         \\end{array}\n    \\right.\n\\end{align}",
            "eq:2": "\\begin{align}\n    d_v^{(l)} = AGG(d_v^{(l-1)}, \\{d_u^{(l-1)}|u\\in \\mathcal{N}_v\\}),\n\\end{align}",
            "eq:3": "\\begin{align}\n    d_v = readout(\\{d_v^{(l)}|l=\\{0,\\cdots,L\\}\\}).\n\\end{align}",
            "eq:4": "\\begin{align}\n\\label{eq:gen}\n    P = \\sigma(\\frac{\\mathcal{W} - X_g}{\\tau_g}),\n\\end{align}",
            "eq:5": "\\begin{align}\n\\mathcal{L}_{cnt} = |\\lambda_g\\cdot|\\mathcal{E}| - \\sum_{i,j}{P_{i,j}}|.\n\\end{align}",
            "eq:6": "\\begin{align}\n\\mathcal{L}_{new} = \\sum_{i,j}(1 - A_{i,j})\\cdot P_{i,j}\n\\end{align}",
            "eq:7": "\\begin{align}\n\\label{eq:reg}\n    \\mathcal{L}_{reg} = \\lambda_{cnt}\\cdot\\mathcal{L}_{cnt} + \\lambda_{new}\\cdot\\mathcal{L}_{new},\n\\end{align}",
            "eq:8": "\\begin{align}\n    \\mathcal{W}_{i,j} = \\left\\{\n         \\begin{array}{cl}\n         \\frac{(1-\\gamma)\\cdot\\lambda_{g}\\cdot|\\mathcal{E}|}{|\\mathcal{E}|}&, \\quad \\mbox{if $(v_i, v_j) \\in \\mathcal{E}$;} \\\\\n         \\frac{\\gamma\\cdot\\lambda_{g}\\cdot|\\mathcal{E}|}{|\\mathcal{C}|}&, \\quad \\mbox{if $(v_i, v_j) \\in \\mathcal{C}$;} \\\\\n         0 &, \\quad \\mbox{otherwise,}\n         \\end{array}\n    \\right.\n\\end{align}",
            "eq:9": "\\begin{align}\n\\label{eq:enc}\n    \\{d_v|v\\in\\mathcal{V}\\} = f(G).\n\\end{align}",
            "eq:10": "\\begin{align}\n\\label{eq:pool}\n    d_G = (\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}{d_v}) \\oplus \\mbox{MaxPool}(\\{d_v|v\\in\\mathcal{V}\\}),\n\\end{align}",
            "eq:11": "\\begin{align}\n\\label{eq:mlp}\n    p_G = h(d_G),\n\\end{align}",
            "eq:12": "\\begin{align}\n\\label{eq:clf}\n    \\mathcal{L}_{clf}=-y_G\\cdot\\log(p_G)-(1-y_G)\\cdot\\log(1-p_G).\n\\end{align}",
            "eq:13": "\\begin{align}\n    \\{d_v^p|v\\in\\mathcal{V}\\} = f(\\mathcal{G}_p),\n    \\notag \\\\\n    % \\quad\n    \\{d_v^g|v\\in\\mathcal{V}\\} = f(\\mathcal{G}_g).\n\\end{align}",
            "eq:14": "\\begin{align}\n% \\label{eq:gcl}\n%     \\mathcal{L}_{gcl} = -\\sum_{v\\in\\mathcal{V}}\\log{\\frac{\\exp(({d_v^p}^\\top d_v^g)/\\tau_f)}{\\sum_{u\\in\\mathcal{V}}{\\exp(({d_u^p}^\\top d_v^g)/\\tau_f)}}},\n% \\end{align}\n\\label{eq:gcl}\n    \\mathcal{L}_{gcl} = -\\sum_{v\\in\\mathcal{V}}\\log{\\frac{\\exp(\\frac{{d_v^p}^\\top d_v^g}{\\tau_f})}{\\sum_{u\\in\\mathcal{V}}{\\exp(\\frac{{d_u^p}^\\top d_v^g}{\\tau_f})}}},\n\\end{align}",
            "eq:15": "\\begin{align}\n\\label{eq:bpr}\n    \\mathcal{L}_{bpr} = -\\frac{1}{|\\mathcal{O}|}\\sum_{(i,j,k)\\in\\mathcal{O}}{\\log\\sigma(d_i^\\top d_j - d_i^\\top d_k)},\n\\end{align}",
            "eq:16": "\\begin{align}\n\\label{eq:ssl}\n    \\mathcal{L}_{ssl} = \\lambda_{gcl}\\cdot\\mathcal{L}_{gcl} + \\lambda_{bpr}\\cdot\\mathcal{L}_{bpr},\n\\end{align}",
            "eq:17": "\\begin{align}\n\\label{eq:clf_g}\n    \\mathcal{L}'_{clf}=-\\log(p_{\\mathcal{G}_g}).\n\\end{align}"
        }
    }
}