{
    "meta_info": {
        "title": "Continuous-Time and Multi-Level Graph Representation Learning for  Origin-Destination Demand Prediction",
        "abstract": "Traffic demand forecasting by deep neural networks has attracted widespread\ninterest in both academia and industry society. Among them, the pairwise\nOrigin-Destination (OD) demand prediction is a valuable but challenging problem\ndue to several factors: (i) the large number of possible OD pairs, (ii)\nimplicitness of spatial dependence, and (iii) complexity of traffic states. To\naddress the above issues, this paper proposes a Continuous-time and Multi-level\ndynamic graph representation learning method for Origin-Destination demand\nprediction (CMOD). Firstly, a continuous-time dynamic graph representation\nlearning framework is constructed, which maintains a dynamic state vector for\neach traffic node (metro stations or taxi zones). The state vectors keep\nhistorical transaction information and are continuously updated according to\nthe most recently happened transactions. Secondly, a multi-level structure\nlearning module is proposed to model the spatial dependency of station-level\nnodes. It can not only exploit relations between nodes adaptively from data,\nbut also share messages and representations via cluster-level and area-level\nvirtual nodes. Lastly, a cross-level fusion module is designed to integrate\nmulti-level memories and generate comprehensive node representations for the\nfinal prediction. Extensive experiments are conducted on two real-world\ndatasets from Beijing Subway and New York Taxi, and the results demonstrate the\nsuperiority of our model against the state-of-the-art approaches.",
        "author": "Liangzhe Han, Xiaojian Ma, Leilei Sun, Bowen Du, Yanjie Fu, Weifeng Lv, Hui Xiong",
        "link": "http://arxiv.org/abs/2206.15005v1",
        "category": [
            "cs.LG"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nIn recent years, deep learning techniques have been extensively extended into intelligent transportation systems.\nAmong all these applications, traffic prediction is the most attractive problem demonstrating its significance for urban construction, traffic controlling and route planning \\cite{STGCN, DCRNN, ACGRN, MTGNN}.\n\n\nAmong existing work, most of them focus on forecasting how many people flow in or out an area.\nInstead of merely forecasting the amount of passengers, Origin-Destination (OD) demand prediction also aims at predicting their destinations, which is rather important to understanding human mobility patterns.\nMoreover, as there is a time series for each node pair, this is a distinctive and challenging problem due to much higher complexity than the most studied node-level prediction.\nHowever, with the availability of large-scale transaction data, the problem attracts an increasing number of researchers to solve it.\nSome recent studies split an area into regularly shaped grids and leverage Convolution Neural Networks (CNN) to capture spatial dependency of grids \\cite{Contextualized, cityscale, AdversarialOD, MultiScale, multiresoluton}.\nHowever, the demand is associated with stations or irregular traffic zones in many cases, which can not be solved with convolution filters.\nSome studies generate representations for each OD pair, which can fit traffic graph topology but will significantly enlarge the complexity \\cite{linegraph, zhang2021short, DNEAT}.\nIn addition, some studies focus on node representations making the complexity acceptable, while these methods are usually based on explicit but incomplete node relations \\cite{GEML, MultiPerspective}.\n\n\nAlthough there are some attempts on OD demand prediction, two important issues have rarely been discussed.\nFirst, historical transactions are generally aggregated into demand snapshots, each of which contains demand in a fixed time window.\nThis operation will result in inevitable information loss.\nSecond, the spatial dependency in prior studies is always manually designed, which is intuitive but incomplete.\nIt has been demonstrated that there exists multiple types of relations in traffic \\cite{multigraph}.\nAs shown in Figure \\ref{fig: motivation}, the station in the west residential area is located close to other stations in the west.\nBut its demand can also be related to stations in the east residential area, as people from these stations may share a similar pattern going to central area.\nHowever, hand designing is impossible to enumerate all potential relation types.\nIn summary, three challenges still remain:\n(1) The time of transactions is a continuous feature which is unsatisfactory to process with fixed time windows.\nIt is challenging to model the complex temporal feature.\n(2) There is implicit spatial dependency among traffic nodes. \nHowever, it is challenging to manually design optimal spatial dependency by hand.\n(3) Each pair of nodes has its own time series. The quadratic amount of predicted values leads to the data sparsity problem. \n\nTo address the above issues, this study proposes a novel \\textbf{C}ontinuous-time and \\textbf{M}ulti-level dynamic graph representation learning framework for \\textbf{O}rigin-\\textbf{D}estination demand prediction (\\textbf{CMOD}).\nThe basic idea is shown in Figure \\ref{fig: motivation}.\nThe framework maintains continuously updating node memories, which are vectors compressing and keeping historical transaction information to represent node status.\nDuring each time of prediction, memories from last time are taken as input and updated according to the newly happened transactions.\nSpecially, the newly happened transactions are first leveraged to generate messages and update station-level node memories.\nNext, we establish an adaptive multi-level structure by attention mechanism.\nThen, the station-level messages are projected to cluster-level and area-level through their relations to update the corresponding memories.\nLast, updated multi-level memories are fused for the final prediction, and they also act as the context of the next prediction.\nMoreover, an objective function is designed to alleviate the data sparsity problem.\nThe main contributions are three folds:\n\\begin{itemize}\n    \\item \\textit{A continuous-time dynamic graph representation learning framework is proposed for OD demand forecasting.}\n    Different from the previous research, our method maintains continuous-time node representations and updated them continuously once a number of transactions are available. \n    As the evolutionary dynamics of stations could be learned in an extremely fine time scale, our method is promising to achieve higher prediction accuracy.\n    \\item  \\textit{A hierarchical message passing module is proposed to model the spatial interactions of stations.}\n    By sharing messages via the virtual cluster-level and area-level nodes, CMOD could exploit multi-level spatial dependence among stations.\n    \\item Extensive experiments have been conducted on two real-world datasets. The results not only demonstrate the superiority of our method over baselines, but also illustrate the ability of our method to capture the continuous evolving trajectory of station status.\n\\end{itemize}\n\nWhat's more, the proposed method has potential to be further extended to other applications predicting features on edges, such as inter-country trade amount and network usage prediction.\nThe ideas to eliminate the effect of fixed time windows and exploit implicit node relations could also work in those scenarios.\n% this will be explored in the future work.\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\begin{mydef}[Dynamic Transaction Graph]\nPassenger demand from one location to another can be reflected in historical transaction records, such as taxi orders or metro records.\nThese records contain the origin, the destination and the departure time of passengers.\nIn this study, these transactions are organized as a continuous-time dynamic graph $\\mathcal{G}=(\\mathbb{V}, \\mathbb{E})$,  where $\\mathbb{V}=\\{v_1, v_2, \\cdots, v_N\\}$ is a finite set of $N$ traffic nodes; $\\mathbb{E}=\\{e_1, e_2, \\cdots, e_M\\}$ is the set of $M$ timestamped transactions.\nEach node represents a fixed station or a zone and an edge $e_m=(v_m^o, v_m^d, t_m)$ represents a passenger from $v_m^o$ to $v_m^d$ at time $t_m$.\nEach node has a feature vector, and features of all nodes are denoted as $\\mathbf{F}\\in \\mathbb{R}^{N\\times d^F}$, where $d^F$ is the dimension of feature vectors. \nMoreover, the dynamic graph at a certain time $t$ is denoted as $\\mathcal{G}_t=(\\mathbb{V},\\{e_k|t_k<t\\})$, which contains all transactions before $t$.\n\\end{mydef}\n\n\\begin{mydef}[OD Demand Matrix]\nOD demand matrix is a compressed format of passenger demand.\nIt contains the amount of demand between each pair of nodes in a period of time.\nFormally, the OD demand matrix between $t$ and $t+\\tau$ is denoted as $\\mathbf{Y}^{t:t+\\tau}\\in \\mathbb{R}^{N\\times N}$.\nThe $(i,j)$-entry of $\\mathbf{Y}^{t:t+\\tau}$ represents how many passengers travel from $v_i$ to $v_j$ between $t$ and $t+\\tau$: $\\mathbf{Y}^{t:t+\\tau}_{i,j}=|\\{e_k|v_k^o = v_i \\wedge v_k^d = v_j \\wedge t\\leq t_k< t+\\tau\\}|$, where $|\\cdot|$ is the size of a set.\n\\end{mydef}\n\n\\begin{mydef}[OD Demand Prediction Problem]\nCompared to demand of nodes, OD demand from node to node can better reveal human mobility.\nOD demand prediction can not only estimate the amount of passengers in the future, but also gives an illustration about how they move, which is extremely helpful to transportation management.\nFormally, given historical transaction records, the aim of OD demand prediction is to estimate OD demand matrix in the next period of time:\n\\begin{equation}\\nonumber\n    \\hat{\\mathbf{Y}}^{t:t+\\tau} = f(\\mathcal{G}_t, \\mathbf{F}, \\mathbb{W}),\n\\end{equation}where $\\mathbb{W}$ is the set of learnable parameters. \n\\end{mydef}\n\n\n"
            },
            "section 3": {
                "name": "Methodology",
                "content": "\nThe overall framework of CMOD is shown in Figure \\ref{fig: archtecture}.\nThe core idea of the framework is to maintain multi-level memories for nodes.\nThe multi-level structure is designed to capture spatial dependency between traffic nodes.\nWhen transitions happen, this framework will update these memories with these streaming events.\nAnd the updated memories, which compress all historical transactions and represent real-time node status, are utilized for the final prediction.\n",
                "subsection 3.1": {
                    "name": "Continuous-Time Node Representation",
                    "content": "\nTo predict future OD demand, a general framework adopted by all previous work is based on discrete-time aggregation.\nIn another word, they all aggregate demand into demand snapshots with a fixed time window.\nAnd sequence learning modules (e.g. GRU, LSTM) are leveraged to capture temporal dynamic on multiple demand snapshots.\nOne main drawback is that this process simply counts transactions in a time window and discards original continuous time information.\nThis will make it hard to capture important features.\nFor example, there are two time windows having similar amount of demand, but the first time window indicates demand increasing while another indicates demand decreasing.\nHowever, simply counting transactions in a time window is incapable to distinguish them from each other.\n\nDifferent from previous discrete-time OD demand prediction methods, CMOD is directly built on raw transition records, which views time information as continuous features and maintains continuous-time evolving representation for each traffic node (e.g. metro stations, taxi zones).\nThe raw records are represented as timestamped events, which are also streaming interactions between traffic nodes.\nTo get the node representations, there are two basic assumptions: \n1) the more recently an interaction happens, the more important it will be; \n2) the more frequently node i interact with node j, the more effect node j should have on node i. \nTo this end, we expect representation of node i at time $t$ to be\n\\begin{equation}\n  \\mathbf{r}_i^t=\\frac{\\sum\\limits_{(v_i, v_j, t^\\prime)\\in E^t}exp(-\\lambda (t-t^\\prime))\\mathbf{r}_j^{t^\\prime}}{\\sum\\limits_{(v_i, v_j, t^\\prime)\\in E^t}exp(-\\lambda (t-t^\\prime))},\n  \\label{node_rep}\n\\end{equation}\nwhere $\\mathbf{r}_i^t$ is representation of node $i$ at time $t$ and $\\lambda$ is a hyperparameter which controls how fast the weights decay.\nHowever, directly calculating node representations according to Equation \\ref{node_rep} at time $t$ requires recomputing weights for all previous interaction when new transactions happen.\nHere, we can choose to maintain two accumulators $\\mathbf{a}$ and $b$ as node memory to update the representation in an online manner:\n\\begin{equation}\n\\begin{split}\n  \\mathbf{a}_i^t=exp(-\\lambda (t-t^-))\\mathbf{a}_i^{t^-}+\\mathbf{r}_j^{t^-},\\\\\n%   \\label{cal_a}\n% \\end{equation}\n% \\begin{equation}\n  b_i^t=exp(-\\lambda (t-t^-))b_i^{t^-}+1,\n  \\end{split}\n   \\label{cal_b}\n\\end{equation}\nwhere $\\mathbf{a}_i^0=\\mathbf{0}$, $b_i^0=1$ and $t^-$ is last update time of node $i$.\n$\\mathbf{a}_i^t$ is temporally weighted sum of neighbor node representations and acts as numerator in Equation \\ref{node_rep}; and $b_i^t$ is temporal normalizer to acts as denominator in Equation \\ref{node_rep}.\nAnd node status at time $t$ is represented as\n\\begin{equation}\n  \\mathbf{r}_i^t=\\frac{\\mathbf{a}_i^t}{b_i^t}.\n\\end{equation}\n\nThe above procedure provides an inspiration to maintain dynamic node representations based on timestamped events.\nHowever, in OD prediction, the amount of events is extremely huge, which makes it impossible to update node representation for each event.\nMeanwhile, the procedure described above is lack of expressive power; it has no trainable parameters or meaningful initial features.\nTo address these issues, our dynamic node representation procedure is formally designed as following:\nFirst, given a batch of newly happened events, we calculate an representation for each event; it combines node representation with some inherent features for a meaningful start:\n\\begin{equation}\n  \\mathbf{s}_k=[\\mathbf{r}_j^{t^-};F_j], e_k=(v_i, v_j, t_k),\n\\end{equation}\nwhere $t^-$ is last update time of node representations and $[\\cdot;\\cdot]$ is concatenation of two vectors.\nThen for each node, we aggregate event representation involving it to get station-level messages, which are used to update station-level node status:\n\\begin{equation}\n\\begin{split}\n  \\mathbf{p}_{i}^t = \\sum\\limits_{(v_i, v_j, t_k)\\in E^t-E^{t^-}}exp(-\\lambda(t-t_k))\\mathbf{s}_k,\n\\\\\n  q_{i}^t = \\sum\\limits_{(v_i, v_j, t_k)\\in E^t-E^{t^-}}exp(-\\lambda(t-t_k)).\n  \\end{split}\n  \\label{cal_q}\n\\end{equation}\nNext, node memories, which consists of two accumulators similar as Equation \\ref{cal_b}, are updated by the messages:\n\\begin{equation}\n  \\begin{split}\n  \\mathbf{a}_{i}^t = exp(-\\lambda (t-t^-))\\mathbf{a}_{i}^{t^-}+MLP(\\mathbf{p}_{i}^t),\\\\\n  b_{i}^t=exp(-\\lambda (t-t^-))b_{i}^{t^-}+q_{i}^t.\n  \\end{split}\n\\end{equation}\nAnd the updated node representations could be obtained as:\n\\begin{equation}\n  \\mathbf{r}_i^t=\\frac{\\mathbf{a}_{i}^t}{b_{i}^t}.\n  \\label{node_emb}\n\\end{equation}\n\nNote that the message can also be extended with edge features such as user information and the transaction price, which is hard for previous snapshot-based methods.\n"
                },
                "subsection 3.2": {
                    "name": "Multi-level Structure",
                    "content": "\nIt is widely known that there exist multiple types of spatial dependency among nodes in traffic including geographical distance and functional similarity.\nFor example, in the morning peak, people come from different residual areas to a business area; moreover, adjacent subway stations covered by a big community may perform similarly.\nHowever, the spatial dependency is hard to enumerate by hand.\nTherefore, this study establishes an adaptive multi-level structure by the attention mechanism to automatically exploit the spatial dependency among nodes.\nAs shown in top-left of Figure \\ref{fig: archtecture}, station-level nodes are aggregated to virtual cluster-level nodes and cluster-level nodes are aggregated to the virtual area-level node.\nThe rationale is that the attention mechanism can assign weights to determine how much a station belongs to a cluster and how much a cluster affects the global area status.\nStation-level nodes that have similar OD demand patterns could be highly related to the same cluster and share useful information.\n\nFormally, for the relations between stations and clusters, the relation matrix $\\mathbf{A}^c_h\\in \\mathbf{R}^{N\\times N^c}$ is computed as:\n\\begin{equation}\n    \\mathbf{A}^c_h=(\\mathbf{W}^{c1}_h (\\mathbf{r}^{t^-})^T)^T(\\mathbf{W}^{c2}_h (\\mathbf{r}^{c,t^-})^T), h=1,2,\\cdots,H,\n\\end{equation}\nwhere $\\mathbf{r}^{t^-}\\in \\mathbf{R}^{N\\times d}$ is station-level nodes representations, $\\mathbf{r}^{c,t^-} \\in \\mathbf{R}^{N^c\\times d}$ is cluster-level nodes representations, $\\mathbf{W}_{\\cdot}\\in \\mathbf{R}^{d^\\prime\\times d}$ is learnable parameters. \nFurthermore, multiple identical heads are leveraged here to capture relations in different aspects.\nMeanwhile, relations between clusters and the whole area $\\mathbf{A}^g_h\\in \\mathbf{R}^{N^c\\times N^g}$ are computed similarly.\nHere, $(i,j)$-entry of $\\mathbf{A}^c_h$ represents the strength that node $i$ belongs to cluster $j$; similarly, $(i, 1)$-entry of $\\mathbf{A}^g_h$ represents the strength that cluster $i$ affects the state of the whole area.\nThe first advantage of this multi-level structure is that unlike designing by hand, the attention mechanism is parameterized and can be optimized to obtain a proper clustering relation for OD demand prediction.\nThe second advantage is that based on dynamic representations of different levels, the spatial dependency can change adaptively in different situations.\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Multi-level Memory Updater",
                    "content": "\n\nSince we aim to maintain dynamic memories in three levels, it is essential to calculate corresponding messages to update them.\nFor example, if there are strong relations between a cluster-level node and several metro stations in business areas, its representation is supposed to be updated by transactions from these stations.\nHowever, the raw transactions are only associated with station-level nodes.\nTherefore, a module is proposed here to generate multi-level messages from station-level messages in Equation \\ref{cal_q}. \n\nAs shown in Figure \\ref{fig: multimseeage}, with above relation matrices, messages are computed to update memories of multiple levels.\nTo be specific, messages for cluster $i$ are computed as:\n\\begin{equation}\n\\begin{split}\n    \\mathbf{A}^{c,m}_{h, j, i} = \\frac{exp(\\mathbf{A}^{c}_{h, j, i})}{\\sum_{j=1}^Nexp(\\mathbf{A}^{c}_{h, j, i})},\\\\\n    \\mathbf{p}^{c,t}_{i}=\\bigg\\|_{h=1}^{H}\\sum_{j=1}^{N}\\mathbf{A}^{c,m}_{h, j, i}(\\mathbf{W}^{c3}_h\\frac{\\mathbf{p}_{i}^t}{q_{i}^t}),\n    \\end{split}\n    \\label{cluster_msg}\n\\end{equation}\nwhere $\\|$ is concatenation operation.\nSimilarly, messages for the graph memory are computed as:\n\\begin{equation}\n\\begin{split}\n    \\mathbf{A}^{g,m}_{h, i} = \\frac{exp(\\mathbf{A}^{g}_{h, i})}{\\sum_{i=1}^{N_c}exp(\\mathbf{A}^{g}_{h, i})},\\\\\n    \\mathbf{p}^{g,t}=\\bigg\\|_{h=1}^{H}\\sum_{i=1}^{N^c}\\mathbf{A}^{g,m}_{h, i}(\\mathbf{W}^{g3}_h \\mathbf{p}^{c,t}_{i}).\n    \\end{split}\n\\end{equation}\nThen cluster-level and area-level node memories are updated:\n\\begin{equation}\n  \\mathbf{a}_{i}^{c,t} = exp(-\\lambda (t-t^-))\\mathbf{a}_{i}^{c,t^-}+MLP(\\mathbf{p}^{c,t}_{i}),\n\\label{cluster_emb}\n\\end{equation}\n\\begin{equation}\n \\mathbf{a}^{g,t} = exp(-\\lambda (t-t^-))\\mathbf{a}^{g,t^-}+MLP(\\mathbf{p}^{g,t}).\n\\label{area_emb}\n\\end{equation}\n\nIn the above procedure, we discard normalizer $b$ for clusters and the area due to the fact that relations between levels are evolving all the time.\nFor example, in morning peak and evening peak, business cluster may weigh more to the whole area status, which makes it hard to aggregate the historical normalizer item.\nThus, we put the normalization procedure in Equation \\ref{cluster_msg}.\nAnd cluster-level and area-level node representations are directly set as their memories.\n\n"
                },
                "subsection 3.4": {
                    "name": "Cross-level Fusion Module",
                    "content": "\nWith previous modules, memories are updated to keep historical information for different levels.\nTo predict the OD demand matrix, updated node representations need to be extracted from those updated memories.\nIn the message generation part, the attention mechanism is utilized to model relations between different levels.\nA stronger relation between a cluster and a station can not only indicate that the cluster should receive more messages from the station, but it also means the final station-level node representation should contain more information from the cluster.\nFor example, if one station belongs to a business cluster with a high weight, transactions involving the station should update memory of the cluster and memory of the cluster is also helpful to stations in it.\n\nSince memories of different levels contain different parts of the information, a cross-level fusion module is proposed here to fuse memories from multiple levels.\nStation-level node representations are calculated in Equation \\ref{node_emb} and other-level representations are calculated in Equation \\ref{cluster_emb} and \\ref{area_emb}.\nTo fuse representations of clusters to station-level, relations between them are reutilized:\n\\begin{small}\n\\begin{equation}\n    \\mathbf{A}^{c,e}_{h, i, j} = \\frac{exp(\\mathbf{A}^{c}_{h, i, j})}{\\sum_{j=1}^{N^c}exp(\\mathbf{A}^{c}_{h, i, j})}, \n    \\mathbf{r}_{i}^{c,t\\prime} = \\frac{1}{H}\\sum\\limits_{h=1}^{H}\\sum\\limits_{j=1}^{N^c}\\mathbf{A}^{c,e}_{h, i, j}\\mathbf{a}_j^{c,t}.\n\\end{equation}\n\\end{small}\nThe global area status are projected to station-level as:\n\\begin{small}\n\\begin{equation}\n    \\mathbf{A}^{g,e}_{h, i, j} = \\frac{exp(\\mathbf{A}^{g}_{h, i, j})}{\\sum_{j=1}^{N^g}exp(\\mathbf{A}^{g}_{h, i, j})}, \n    \\mathbf{r}_{i}^{g,t\\prime} = \\frac{1}{H}\\sum\\limits_{h=1}^{H}\\sum\\limits_{j=1}^{N^c}\\sum\\limits_{k=1}^{N^g}\\mathbf{A}^{g,e}_{h, i, j}\\mathbf{A}^{g,e}_{h, j, k}\\mathbf{a}_k^{g,t}.\n\\end{equation}\n\\end{small}\nAnd the fused node representations are computed as following:\n\\begin{equation}\n    \\mathbf{Z}^t = [\\mathbf{r}^{t};\\mathbf{r}^{c,t\\prime};\\mathbf{r}^{g,t\\prime}].\n\\end{equation}\n\nHow the attention mechanism captures the spatial dependency can be explained in two angles.\nFirst, stations receive the same cluster information during cross-level fusion, which makes them partly similar.\nSecond, records involving a station send messages to clusters; when fusing representations for other stations, the fusion module makes them receive these messages through a bipartite-graph-like structure.\nIn both views, the more similar relations to clusters of two stations are, the more information can be shared.\n"
                },
                "subsection 3.5": {
                    "name": "Output and Training",
                    "content": "\nThe final prediction is then obtained based on the fused node representations.\nFor demand from node $i$ to node $j$, the prediction is calculated with concatenation of $\\mathbf{Z}_{i}^t$ and $\\mathbf{Z}_{j}^t$:\n\\begin{equation}\n    \\hat{\\mathbf{Y}}_{i,j}^{t:t+\\tau} = MLP(\\mathbf{Z}_{i}^t;\\mathbf{Z}_{j}^t).\n\\end{equation}\n\nTo predict OD demand for each pair of nodes, how to handle the situation that many pairs have no demand at a single time is important.\nHere, a loss is customized for OD demand prediction.\nThe motivation is that more attention should be paid to non-zero demand and if one pair is unlikely to have demand, it is tolerable to predict a negative number.\nAnd the loss is defined as:\n\\begin{equation}\n    \\begin{split}\n        \\mathcal{L}=\\frac{1}{|\\mathbf{Y}|}\\sum\\limits_{y\\in \\mathbf{Y}}((I_1(y)I_2(\\hat{y}) + 1 - I_1(y))(y-\\hat{y})^2),\n        \\\\\n        I_1(y)=\\begin{cases}\n            1, y=0\\\\\n            0, y>0\n        \\end{cases},\n        I_2(\\hat{y})=\\begin{cases}\n            1, \\hat{y}>0\\\\\n            0, \\hat{y}\\le 0\n        \\end{cases}.\n    \\end{split}\n\\end{equation}\nAnd the negative values in final prediction are replaced as zeros.\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\nThe performance of the proposed model is evaluated on two real-world datasets.\n\\textbf{BJSubway} contains transaction records generated in Beijing Subway from June to July in 2017.\n\\textbf{NYTaxi} contains taxi orders generated in Manhattan from January to June in 2019 \\footnote{Data is avaiable at \\href{https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page}{https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page}}.\nMore detailed statistic information of these datasets is shown in Table \\ref{table: datasets}. \nOur code is available at \\href{https://github.com/liangzhehan/CMOD}{https://github.com/liangzhehan/CMOD}.\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Baselines",
                    "content": "\nThe detailed introduction for baselines are as following:\n\\begin{itemize}\n    \\item \\textbf{HA} (Historical Average) computes historical average of OD demand matrix as prediction.\n    \\item \\textbf{LR} (Linear Regression) is a regression model which exploits linear correlations between input and output. \n    \\item \\textbf{XGBoost} \\cite{XGBoost} is a method based on gradient boosting tree. \n    \\item \\textbf{GEML} \\cite{GEML} is an OD demand prediction model based on snapshots and pre-defined neighborhoods. Geographical neighborhood of GEML is defined by distance. \n    \\item \\textbf{DNEAT} \\cite{DNEAT} is another OD demand prediction model based on snapshots and node-edge attention. The neighborhood definition of DNEAT is the same as GEML. \n    \\item \\textbf{TGN} \\cite{tgn} is a dynamic graph representation learning model using graph attention network to obtain node representation. \n    Note that TGN is not originally designed for OD demand prediction, some modules limit their performance. \n    In comparison, its output module is set as the same as the proposed model. \n    One-hot encoding is used as node features. \n    \\item \\textbf{DyRep} \\cite{DyRep} is a dynamic graph representation learning model using graph attention to aggregate neighbor messages. \n    One-hot encoding is also used as its node features. \n\\end{itemize}\n\n% The following metrics are selected to evaluate the performance:\n% \\begin{itemize}\n%     \\item \\textbf{Mean Average Error (MAE)} is the average distance between the prediction and true value, and the lower MAE is better.\n%     \\item \\textbf{Root Mean Square Error (RMSE)} describes standard deviation of the difference between prediction and true value, and the lower RMSE is better.\n%     \\item \\textbf{Pearson Correlation Coefficient (PCC)} reflects the correlation between prediction and true value, and the higher PCC is better.\n% \\end{itemize}\n"
                },
                "subsection 4.3": {
                    "name": "Experiment Setups",
                    "content": "\nFor both datasets, $\\tau$ in prediction is set as 30 minutes.\nThe amount of cluster nodes $N_c$ is $\\sqrt{N}$.\nOne-hot encoding is used as node features.\nThe proposed method is implemented with Pytorch toolbox on a machine with 4 Tesla T4 GPUs.\nAdam optimizer with initial learning rate 0.0001 and early stopping strategy with patience 10 are utilized to train the proposed model in an end-to-end manner.\nIn dynamic representation part, the memory dimension is set as 256 and the representation dimension is set as 256.\nIn multi-level message part, the attention head number $H$ is set as 8 and the message dimension is set as 256.\nThe learning rate of all deep learning methods is chosen from [0.01, 0.001, 0.0001, 0.00001] according to the best performance on the validation set.\nThe best parameters on the validation set are selected to evaluate the performance.\nAll deep learning methods are repeated with different seeds for 5 times and the average value and the standard deviation are reported. \nMean Average Error (MAE), Root Mean Square Error (RMSE) and Pearson Correlation Coefficient (PCC) are selected as metrics to compare.\n\n"
                },
                "subsection 4.4": {
                    "name": "Comparison Results",
                    "content": "\n\nTable \\ref{table: Comparison} summarizes the performance of all baselines. It can be observed:\n\n(1) Overall, CMOD outperforms other methods in all cases, especially on BJSubway dataset, which suggests the effectiveness of our method to learn meaningful node representations for OD demand prediction.\n(2) Though continuous-time dynamic graph representation learning methods (TGN and DyRep) are not originally designed for OD demand prediction, they perform better than other baselines on NYTaxi.\nHowever, on BJSubway, TGN performs worse than other baselines and DyRep even encounters the out of memory problem.\nOne potential reason may be the intrinsic characteristic of OD demand prediction.\nIn this task, a large amount of edges (e.g., there are more than 200 million transactions in BJSubway) will make it hard for some designs to discover real demand patterns.\nFor example, TGN and DyRep both aggregate information from dozens of sampled neighbors.\nIn the context of dense edges, the sampling ratio is so small that brings more randomness and noises.\n(3) Deep learning based methods perform better than simple statistic methods and traditional machine learning methods. \nThe reason is that deep learning methods do not need designed features and have more expressive power to exploit complex and useful information from data.\nOne exception is that DNEAT only performs better on MAE.\nThe reason may be that its output module first predicts if there is demand as a probability from 0 to 1 and then multiples the probability with a predicted value as the final prediction.\nThis procedure will help when demand is low but will make it more unstable when demand is high.\nThus, compared to other deep learning methods, it performs better on MAE, where low-demand situations and high demand situations weigh the same, but performs worse on RMSE and PCC.\n\n"
                },
                "subsection 4.5": {
                    "name": "Ablation Study",
                    "content": "\n\nTo demonstrate the effectiveness of each proposed component, an ablation study is conducted on BJSubway dataset with three variants:\n\\textbf{CMOD w/o ML} removes multi-level structure including memories of different levels, messages of different levels and cross-level fusion module.\n\\textbf{CMOD w/o MU} removes weighted memory updater and average messages and memories directly.\n\\textbf{CMOD w/o ODLoss} trains the model with MSE (Mean Square Error) loss.\nThe result is shown in Figure \\ref{fig: Ablation}; it can be observed that:\n(1) CMOD performs better than CMOD w/o ML; this demonstrates that the proposed attention-based multi-level structure can leverage spatial dependency for better prediction.\n(2) The outperformance over CMOD w/o MU demonstrates that the memory updater which weigh transactions differently by time can provide valuable temporal information.\n(3) After handling zeros by ODLoss, the model suits the OD demand prediction better than CMOD w/o ODLoss.\n\n"
                },
                "subsection 4.6": {
                    "name": "Illustration of Evolving Representations",
                    "content": "\nTo avoid information loss during generating snapshots, the proposed CMOD is based on maintaining station representations.\nHere, we conduct a case study on BJSubway to illustrate evolving patterns of station representations.\nSpecifically, representations of 8 stations from 6AM on first day to 6AM on second day are compressed to 2-dimensional vectors via Principal Component Analysis (PCA) and illustrated in Figure \\ref{fig: rep}.\nWe represent station representations as points and connect them in chronological order.\nThe first point is marked as red and the final point is marked as orange.\nFrom Figure \\ref{fig: rep} (a, b, c), it can be observed that though metro stations at three railway stations in Beijing are nonadjacent, evolving patterns of their representations are similar.\nThis phenomenon demonstrates that the model discovers similar feature among them via transaction data.\nFigure \\ref{fig: rep} (d, h) show evolving patterns of Tiananmen West Station and Wangfujing Station.\nThese two stations are both famous tourist spots and their similar shape indicates their representations share another common evolving pattern.\nIt indicates that CMOD can automatically discover similar patterns from stations with similar functions.\nFigure \\ref{fig: rep} (e, f, g) are from representation of three metro stations around Tiantongyuan, the largest community in Asia.\nIt demonstrates that CMOD can automatically discover similar patterns from adjacent stations without predefined geographical information.\nThus, compared to designing by hand, the idea to establish relations from data is more powerful and helpful.\n\n\n"
                },
                "subsection 4.7": {
                    "name": "Interpretation of Multi-level Structure",
                    "content": "\n\nTo demonstrate how the multi-level structure works, another case study is further conducted on BJSubway.\nTo be specific, the highest weighted station-level nodes in $\\mathbf{A}^c$ are selected to illustrate their locations in Figure \\ref{fig: inter_attention}.\nIn Figure \\ref{fig: inter_attention}, locations of nodes having highest weights with four clusters are illustrated on the map.\nIn cluster A (red marks), most of the stations are from two lines stretching out the center area; this is reasonable as people live in these places for a cheaper rent and have similar demands to travel to other working areas of the city.\nIn cluster B (blue marks) and cluster C (brown marks), most of the highly weighted stations are also distributed in a local area showing a similar pattern as cluster A.\nIn cluster D (black marks), the top weighted stations show another pattern; they don't gather in a small area.\nThough these stations are nonadjacent directly, it is still reasonable.\nThese stations are all located around airports and tourist spots including zoo, the Palace Museum, Olympic Park and some shopping areas.\nThis indicates that cluster D discovers an implicit travelling pattern behind metro passenger data.\nThus, the results demonstrate that the multi-level structure can adaptively aggregate station-level nodes to clusters, establish relations among traffic nodes and benefit the final prediction.\n\n\n"
                },
                "subsection 4.8": {
                    "name": "Prediction with Input of Varied Timespan",
                    "content": "\n\nIn the above experiments, input of CMOD is transaction sets of every 30 minutes, which is for the comparison with previous OD demand prediction models.\nActually, the demand evolves continuously and it will be helpful to predict more frequently when it is in peak hour. \nFortunately, another inherent advantage of CMOD is that unlike methods based on snapshots, the time granularity of input need not to set explicitly.\nTo demonstrate this, a case study is designed on BJSubway by varying the input timespan.\nSpecifically, if there are more than 200 thousand transactions during 30 minutes, they are split into several parts, each of which contains 200 thousand transactions at maximum.\nAs a result, memories are updated by different timespans.\nAs shown in Figure \\ref{fig: varied} (a), if the memories are updated every 30 minutes, we can only obtain a sparse result.\nHowever, when CMOD updates memories by varied timespans, it can obtain prediction result in Figure \\ref{fig: varied} (b).\nDuring 7:30 to 9:00, CMOD can update memories and predict OD demand more frequently.\nIt can be observed that the prediction with varied input timespan can fit denser real demand.\nThis demonstrates that CMOD is able to update memories with varied timespans and predict demand whenever the memories are updated. \n\n"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n\n",
                "subsection 5.1": {
                    "name": "OD Demand Prediction",
                    "content": "\nRecently, there have been some attempts in three directions to introduce powerful deep learning into OD demand prediction problem.\nIn the first direction, researchers divided an area into regular-shaped grids and leveraged convolutional neural networks to capture their spatial dependency \\cite{Contextualized, cityscale, AdversarialOD, MultiScale, multiresoluton}.\nHowever, in many cases, OD demand are associated with irregular-shaped zones or stations on graph topology.\nAlthough these methods could handle spatial dependency and temporal dynamics simultaneously, the requirement of grids limits their application.\nResearchers in the second direction transferred edges to nodes in line graphs and leveraged methods for nodes to solve this problem \\cite{linegraph, zhang2021short, DNEAT}.\nThey established relations based on underlying topology, while the complexity makes it difficult for a larger graph.\nThe third direction is to keep representations for nodes and reduced the complexity: researchers mainly utilized GCNs for each snapshot to capture spatial dependency and RNNs for multiple snapshots to capture temporal dynamics \\cite{GEML, MultiPerspective}.\nHowever, spatial dependency among traffic nodes was always designed by hand (e.g., geographical distance, last time demand); an improper relation may even hinder the prediction.\nMoreover, to the best of our knowledge, all previous studies predicted OD demand based on snapshots, which will omit useful tendency information and bring ambiguity in choosing time granularity.\n\n"
                },
                "subsection 5.2": {
                    "name": "Dynamic Graph Representation Learning",
                    "content": "\nTraditionally, research on graphs focused on static ones where nodes and edges are fixed \\cite{GCN, GraphSage, GAT, GIN}.\nThey would fail for many applications involving dynamic graphs including streaming communication events in social media and streaming transactions in traffic.\nRecent few years have witnessed a bunch of studies on dynamic graphs, and they fall into two categories: methods on discrete-time dynamic graphs (DTDGs) and methods on continuous-time dynamic graphs (CTDGs).\nMethods on DTDGs represented a dynamic graph as multiple static graphs at different times; each static graph contains graph information in a period of time \\cite{DynGEM, cc, DySAT, EvolveGCN}.\nThese studies took the temporal information into consideration but were still inflexible to fit more general cases.\nMethods on CTDGs viewed edges in dynamic graphs as streaming timestamped events \\cite{CTDNE, DyRep, JODIE, STREAMING, TGAT, tgn, TagGen}.\nMost of them updated node representations after an event involving the node.\nAlthough this architecture can handle temporal dependency and keep tendency information, these studies were lack of components to handle challenges in traffic including implicit spatial dependency and much denser events.\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nThis study proposed a novel framework for the OD demand prediction problem.\nThe framework is based on modeling demand as a continuous-time dynamic graph.\nIt breakthroughs the limit of traditional snapshot-based models and can capture more useful information.\nMoreover, a multi-level structure was established to adaptively exploit spatial dependency between traffic nodes.\nExperiments on two real-world datasets demonstrated that the proposed model achieved the state-of-the-art performance. \nCase studies were also conducted to demonstrate the capability to handle input of arbitrary timespan and the effectiveness of the multi-level structure.\nThis work bonds promising CTDG methods and OD demand prediction for the first time, and it's also a new scenario for CTDG methods.\nAnd the idea could also be further applied on more applications for edge-level prediction such as inter-country trade amount prediction and network usage prediction, where time information and node relations are also supposed to be handled carefully.\n\\begin{acks}\nThis work was supported by the National Natural Science Foundation of China (71901011, U1811463, 51991391, 51991395, U21A20516).\n\\end{acks}\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{reference.bib}\n"
            }
        },
        "tables": {
            "table: datasets": "\\begin{table}[!h]\n  \\caption{Statistic information of datasets}\n  \\label {table: datasets}\n  \\begin{tabular}{c|cc}\n   \\toprule\n   Dataset & BJSubway & NYTaxi\\\\\n   \\midrule\n   \\#Nodes & 268 & 63 \\\\\n   \\#Orders & 279,227,618 & 38,498,427 \\\\\n   \\#Train Days & 42 & 139 \\\\\n   \\#Validation Days & 7 & 21 \\\\\n   \\#Test Days & 7 & 21 \\\\\n   Average Demand  & 2.1694 & 1.1164 \\\\\n   Zero Order Ratio & 54.84\\% & 66.15\\%\\\\\n   \\bottomrule\n  \\end{tabular}\n\\end{table}",
            "table: Comparison": "\\begin{table*}\n  \\caption{Comparison results with baselines.}\n  \\label{table: Comparison}\n  \\begin{tabular}{cc|ccc|ccc}\n    \\toprule\n    \\multirow{2}{*}{Dataset} & \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{All OD Pairs} & \\multicolumn{3}{c}{OD Pairs with Demand above Average} \\\\\n    \\cline{3-8}\n    & & MAE $\\downarrow$ & RMSE $\\downarrow$ & PCC $\\uparrow$ & MAE $\\downarrow$ & RMSE $\\downarrow$ & PCC $\\uparrow$ \\\\\n    \\midrule\n    \\multirow{9}{*}{BJSubway} \n    & HA & 2.9003 & 8.1266 & 0 & 8.0378 & 18.3277  & 0 \\\\\n    & LR & 1.9396 & 5.3547 & 0.7521 & 6.0566 & 11.7181 & 0.7322\\\\\n    & XGBoost & 1.8048 & 5.7709 & 0.7040 & 5.9098 & 12.9627 & 0.6449\\\\\n    \\cline{2-8}\n    & GEML & 1.7291±0.0123 & 4.6018±0.1138 & 0.8279±0.0075 & 5.3002±0.0982 & 10.1491±0.2983 & 0.8083±0.0086 \\\\\n    & DNEAT & 1.4706±0.0099 & 5.7384±0.0311 & 0.7237±0.0033 & 5.4476±0.0365 &13.0661±0.0798  & 0.6488±0.0055 \\\\\n    & TGN & 2.1031±0.1629 & 5.8927±0.5148 & 0.6755±0.0659 &  6.5592±0.3331& 13.0607±1.1772 & 0.6455±0.0781 \\\\\n    & DyRep & - & - & - & - & - & -\\\\\n    \\cline{2-8}\n    & CMOD & \\textbf{1.4475±0.0202} & \\textbf{3.6890±0.0319} & \\textbf{0.8911±0.0020} & \\textbf{4.5068±0.0437} & \\textbf{8.1441±0.0697} & \\textbf{0.8773±0.0021}\\\\\n    \\hline\n    \\multirow{9}{*}{NYTaxi} \n    & HA & 1.4593 & 2.6569 & 0 & 3.6041 & 5.7289 & 0 \\\\\n    & LR & 0.6907 & 1.3611 & 0.8586 & 1.9939 & 2.8069 & 0.8164\\\\\n    & XGBoost & 0.6881 & 1.3555 & 0.8599 & 1.9895 & 2.8052 & 0.8185\\\\\n    \\cline{2-8}\n    & GEML & 0.6476±0.0033 & 1.3432±0.0093 & 0.8662±0.0015 & 1.8867±0.0138 & 2.7587±0.0198 & 0.8201±0.0025\\\\\n    & DNEAT & 0.6495±0.0025 & 1.5179±0.0172 & 0.8252±0.0040 & 2.1922±0.0292 & 3.2834±0.0685 & 0.7581±0.0104 \\\\\n    & TGN & 0.6516±0.0142 & 1.2947±0.0330 & 0.8747±0.0057 & 1.8387±0.0235 & 2.6435±0.0503  & 0.8311±0.0094 \\\\\n    & DyRep & 0.6094±0.0032 & 1.2164±0.0089 & 0.8892±0.0013 & 1.7844±0.0296 & 2.5074±0.0398 & 0.8528±0.0017 \\\\\n    \\cline{2-8}\n    & CMOD & \\textbf{0.5926±0.0026} & \\textbf{1.1795±0.0023} & \\textbf{0.8959±0.0004} & \\textbf{1.7244±0.0091} & \\textbf{2.4263±0.0089} & \\textbf{0.8618±0.0006} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table*}"
        },
        "figures": {
            "fig: motivation": "\\begin{figure}[!htbp]\n    \\centering\n    \\includegraphics[width=0.9\\columnwidth]{figures/od_figure_moti.jpg}\n    \\caption{OD demand prediction with continuously evolving node representations.}\n    \\label{fig: motivation}\n\\end{figure}",
            "fig: archtecture": "\\begin{figure*}[!htbp]\n  \\centering\n  \\includegraphics[width=0.9\\textwidth]{figures/od_figure_architecture.jpg}\n  \\caption{The overall framework of CMOD. It maintains continuously updated memory for each traffic node. When new transitions happen, the latest memories are used to represent these transitions. Then the representations are aggregated as messages for each node to update memories of corresponding station-level nodes. Meanwhile, a multi-level structure is established. Then the station-level messages are projected to cluster-level and area-level messages to update memory of nodes in these levels. Last, dynamic node representation is generated by cross-level fusion for the final prediction. \n%   Finally, future OD demand matrix is calculated with these continuously updated node representations.\n  }\n  \\label{fig: archtecture}\n\\end{figure*}",
            "fig: multimseeage": "\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.98\\columnwidth]{figures/od_figure_mlupdater.jpg}\n  \\caption{Illustration of multi-level memory updater.}\n  \\label{fig: multimseeage}\n\\end{figure}",
            "fig: Ablation": "\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\columnwidth]{figures/od_figure_ablation.jpg}\n  \\caption{Ablation study.}\n  \\label{fig: Ablation}\n\\end{figure}",
            "fig: rep": "\\begin{figure*}\n  \\centering\n  \\includegraphics[width=0.9\\textwidth]{figures/od_figure_rep.jpg}\n  \\caption{Illustration of evolving dynamic station representations.}\n  \\label{fig: rep}\n\\end{figure*}",
            "fig: inter_attention": "\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.89\\columnwidth]{figures/od_figure_cluster.jpg}\n  \\caption{Interpretation of discovered clusters.}\n  \\label{fig: inter_attention}\n\\end{figure}",
            "fig: varied": "\\begin{figure*}\n  \\centering\n  \\includegraphics[width=0.95\\textwidth]{figures/od_figure_varied.jpg}\n  \\caption{Prediction in two different settings.}\n  \\label{fig: varied}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n  \\mathbf{r}_i^t=\\frac{\\sum\\limits_{(v_i, v_j, t^\\prime)\\in E^t}exp(-\\lambda (t-t^\\prime))\\mathbf{r}_j^{t^\\prime}}{\\sum\\limits_{(v_i, v_j, t^\\prime)\\in E^t}exp(-\\lambda (t-t^\\prime))},\n  \\label{node_rep}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\begin{split}\n  \\mathbf{a}_i^t=exp(-\\lambda (t-t^-))\\mathbf{a}_i^{t^-}+\\mathbf{r}_j^{t^-},\\\\\n%   \\label{cal_a}\n% \\end{equation}\n% \\begin{equation}\n  b_i^t=exp(-\\lambda (t-t^-))b_i^{t^-}+1,\n  \\end{split}\n   \\label{cal_b}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n  \\mathbf{r}_i^t=\\frac{\\mathbf{a}_i^t}{b_i^t}.\n\\end{equation}",
            "eq:4": "\\begin{equation}\n  \\mathbf{s}_k=[\\mathbf{r}_j^{t^-};F_j], e_k=(v_i, v_j, t_k),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\begin{split}\n  \\mathbf{p}_{i}^t = \\sum\\limits_{(v_i, v_j, t_k)\\in E^t-E^{t^-}}exp(-\\lambda(t-t_k))\\mathbf{s}_k,\n\\\\\n  q_{i}^t = \\sum\\limits_{(v_i, v_j, t_k)\\in E^t-E^{t^-}}exp(-\\lambda(t-t_k)).\n  \\end{split}\n  \\label{cal_q}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n  \\begin{split}\n  \\mathbf{a}_{i}^t = exp(-\\lambda (t-t^-))\\mathbf{a}_{i}^{t^-}+MLP(\\mathbf{p}_{i}^t),\\\\\n  b_{i}^t=exp(-\\lambda (t-t^-))b_{i}^{t^-}+q_{i}^t.\n  \\end{split}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n  \\mathbf{r}_i^t=\\frac{\\mathbf{a}_{i}^t}{b_{i}^t}.\n  \\label{node_emb}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\mathbf{A}^c_h=(\\mathbf{W}^{c1}_h (\\mathbf{r}^{t^-})^T)^T(\\mathbf{W}^{c2}_h (\\mathbf{r}^{c,t^-})^T), h=1,2,\\cdots,H,\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\begin{split}\n    \\mathbf{A}^{c,m}_{h, j, i} = \\frac{exp(\\mathbf{A}^{c}_{h, j, i})}{\\sum_{j=1}^Nexp(\\mathbf{A}^{c}_{h, j, i})},\\\\\n    \\mathbf{p}^{c,t}_{i}=\\bigg\\|_{h=1}^{H}\\sum_{j=1}^{N}\\mathbf{A}^{c,m}_{h, j, i}(\\mathbf{W}^{c3}_h\\frac{\\mathbf{p}_{i}^t}{q_{i}^t}),\n    \\end{split}\n    \\label{cluster_msg}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\begin{split}\n    \\mathbf{A}^{g,m}_{h, i} = \\frac{exp(\\mathbf{A}^{g}_{h, i})}{\\sum_{i=1}^{N_c}exp(\\mathbf{A}^{g}_{h, i})},\\\\\n    \\mathbf{p}^{g,t}=\\bigg\\|_{h=1}^{H}\\sum_{i=1}^{N^c}\\mathbf{A}^{g,m}_{h, i}(\\mathbf{W}^{g3}_h \\mathbf{p}^{c,t}_{i}).\n    \\end{split}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n  \\mathbf{a}_{i}^{c,t} = exp(-\\lambda (t-t^-))\\mathbf{a}_{i}^{c,t^-}+MLP(\\mathbf{p}^{c,t}_{i}),\n\\label{cluster_emb}\n\\end{equation}",
            "eq:12": "\\begin{equation}\n \\mathbf{a}^{g,t} = exp(-\\lambda (t-t^-))\\mathbf{a}^{g,t^-}+MLP(\\mathbf{p}^{g,t}).\n\\label{area_emb}\n\\end{equation}",
            "eq:13": "\\begin{equation}\n    \\mathbf{Z}^t = [\\mathbf{r}^{t};\\mathbf{r}^{c,t\\prime};\\mathbf{r}^{g,t\\prime}].\n\\end{equation}",
            "eq:14": "\\begin{equation}\n    \\hat{\\mathbf{Y}}_{i,j}^{t:t+\\tau} = MLP(\\mathbf{Z}_{i}^t;\\mathbf{Z}_{j}^t).\n\\end{equation}",
            "eq:15": "\\begin{equation}\n    \\begin{split}\n        \\mathcal{L}=\\frac{1}{|\\mathbf{Y}|}\\sum\\limits_{y\\in \\mathbf{Y}}((I_1(y)I_2(\\hat{y}) + 1 - I_1(y))(y-\\hat{y})^2),\n        \\\\\n        I_1(y)=\\begin{cases}\n            1, y=0\\\\\n            0, y>0\n        \\end{cases},\n        I_2(\\hat{y})=\\begin{cases}\n            1, \\hat{y}>0\\\\\n            0, \\hat{y}\\le 0\n        \\end{cases}.\n    \\end{split}\n\\end{equation}"
        },
        "git_link": "https://github.com/liangzhehan/CMOD"
    }
}