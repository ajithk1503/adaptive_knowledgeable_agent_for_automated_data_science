{
    "meta_info": {
        "title": "Scalable Differentially Private Clustering via Hierarchically Separated  Trees",
        "abstract": "We study the private $k$-median and $k$-means clustering problem in $d$\ndimensional Euclidean space. By leveraging tree embeddings, we give an\nefficient and easy to implement algorithm, that is empirically competitive with\nstate of the art non private methods. We prove that our method computes a\nsolution with cost at most $O(d^{3/2}\\log n)\\cdot OPT + O(k d^2 \\log^2 n /\n\\epsilon^2)$, where $\\epsilon$ is the privacy guarantee. (The dimension term,\n$d$, can be replaced with $O(\\log k)$ using standard dimension reduction\ntechniques.) Although the worst-case guarantee is worse than that of state of\nthe art private clustering methods, the algorithm we propose is practical, runs\nin near-linear, $\\tilde{O}(nkd)$, time and scales to tens of millions of\npoints. We also show that our method is amenable to parallelization in\nlarge-scale distributed computing environments. In particular we show that our\nprivate algorithms can be implemented in logarithmic number of MPC rounds in\nthe sublinear memory regime. Finally, we complement our theoretical analysis\nwith an empirical evaluation demonstrating the algorithm's efficiency and\naccuracy in comparison to other privacy clustering baselines.",
        "author": "Vincent Cohen-Addad, Alessandro Epasto, Silvio Lattanzi, Vahab Mirrokni, Andres Munoz, David Saulpic, Chris Schwiegelshohn, Sergei Vassilvitskii",
        "link": "http://arxiv.org/abs/2206.08646v1",
        "category": [
            "cs.DS",
            "cs.CR",
            "cs.LG"
        ],
        "additionl_info": "To appear at KDD'22"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nClustering is a central problem in unsupervised learning with many applications\nsuch as duplicate detection, community detection, computational\nbiology and many others. Several formulations of clustering problems\nhave been studied throughout the years. Among these,\nthe geometric versions of the problem have attracted a lot of\nattention for their theoretical and practical importance.\nIn those problems we are given as input $n$ points and the objective is to \nput points that are close in the same cluster and far away points in different clusters.\nClassic formulations of geometric clustering problem are $k$-means, \n$k$-median, and $k$-center.\nDue to their relevance and their many practical applications, the \nproblems have been extensively\nstudied and many algorithms~\\cite{ahmadian2017better, arthur2007k,  byrka2014improved,  kanungo2004local, jain2003greedy, li20111} and\nheuristics~\\cite{lloyd1982least} have been proposed to solve\nthe classic version of these problems.\n\nIn this paper, we study these problems through a differential privacy lens. {\\em Differential Privacy (DP)} has emerged as a de facto standard for capturing user privacy~\\cite{dwork}. It is characterized by the notion of neighboring datasets, $X$ and $X'$, generally assumed to be differing on a single user's data.  An algorithm $\\calA$ is $\\epsilon$-differentially private if the probability of observing any particular outcome $S$ when run on $X$ vs $X'$ is bounded:\n$Pr[\\calA(X) = S] \\leq$ $ e^{\\epsilon} Pr[\\calA(X') = S].$\n\nFor an introduction, see the book by Dwork and Roth~\\cite{dwork}. At a high level, DP forces an algorithm to not focus on any individual training example, rather capturing global trends present in the data. \n\n\\paragraph{Private Clustering}\nDifferentially private clustering has been well studied, with a number of works giving polynomial time approximately optimal algorithms for different versions of the problem, including  $k$-median and $k$-means~\\cite{balcan,anamayclustering,badih_approximation,badih_local, clustering_with_convergence}. From an analysis standpoint, any approximately optimal differentially private algorithm must pay both a multiplicative as well as an additive approximation. In other words, the cost of any algorithm solution, $\\textsc{Alg}$, will satisfy  $\\textsc{Alg} \\leq \\alpha \\textsc{OPT} + \\beta$, for some $\\alpha, \\beta > 0$, where $\\textsc{OPT}$ denotes the optimal solution cost. \n\nAll else being equal, we aim for algorithms that minimize $\\alpha$ and $\\beta$, and recent work ~\\cite{badih_approximation} has made a lot of progress in that direction. However, in a push to minimize $\\alpha$ and $\\beta$, algorithms often pay in added complexity and running time. In fact, all known differentially private clustering algorithms with theoretical guarantees have this shortcoming: they have superlinear running times and do not scale to large datasets, even though the large data regime is precisely the one for which using private methods is particularly important. Hence there is a big gap between optimal algorithms in theory and those that can be used in practice. \n\n\\paragraph{Previous Work.} \nIn terms of approximation guarantee, the result of Ghazi et al.~\\cite{badih_approximation} is impressive: they show that it is possible to get privately the same approximation factor as the best non-private algorithm. This concludes a long line of work (see e.g. \\cite{FeldmanFKN09, balcan, StemmerK18}) that focused on approximation guarantee, but not really on practical algorithms. Furthermore, those algorithm attempt to minimize the multiplicative approximation factor, dropping the additive term: instead, in the hope to improve the practical guarantee, Jones et al.~\\cite{JonesNN21} and later Nguyen et al~\\cite{anamayclustering} proposed an algorithm with (large) constant multiplicative error, but with additive error close to the optimal one ($O(k\\log n)$ for $k$-median, $O(k\\log n + k\\sqrt d)$ for $k$-means). They implement their algorithm for $k$-means, showing guarantees comparable to \\cite{balcan}, and quite far from the results of the non-private Lloyd's algorithm. Further, those algorithms all have super linear running time, and do not scale nicely to large datasets. \n\nOn the opposite side, an algorithm for was recently described by Chang and Kamath.\\footnote{\\url{https://ai.googleblog.com/2021/10/practical-differentially-private.html}} This algorithm seems to perform extremely well in practice, with result close to the non-private $k$-means++, and can be implemented in a distributed environment to handle large scale dataset. However, this algorithm has no theoretical guarantee. \n\n\\paragraph{Our Results and Techniques.} \nIn this paper, we aim to address the above shortcomings, and design practical differentially private clustering algorithms, with provable approximation guarantees, that are fast, and amenable to scale up via parallel implementation. \nToward this goal, we take an approach that has been successful in the non-private clustering literature. While there are constant-approximate algorithms for $k$-median and $k$-means, see \\cite{ahmadian2017better,kanungo2004local, jain2003greedy}, most practical implementations use the $k$-means++ algorithm of~\\cite{arthur2007k}, which has a $\\Theta(\\log k)$ approximation ratio. The reason for the success of $k$-means++ is two-fold. First, it is fast, running in linear time, and second, it performs well empirically despite the logarithmic worst-case guarantee. The methods we introduce in this work, while different from $k$-means++, have the same characteristics: they are fast, and perform {\\em much} better than their worst-case guarantees, significantly outperforming all other implementations. In particular, they run in near-linear time, are amenable to parallel implementation in logarithmic number of rounds, and output high-quality private clusters in practice.\n\n\n Our first contribution is an efficient and scalable algorithm for differentially private $k$-median. Our starting point is an embedding of the input points into a tree using a randomly-shifted quadtree (sometimes called HST for Hierarchically Separated Tree)\\footnote{We note here that this technique has already been used in prior work for private clustering~\\cite{balcan} to find a  $\\tilde O(n)$-sized set of candidate centers, to then run a polynomial time local search algorithm. In contrast, we use this structure to directly compute a solution.}. It is well known~\\cite{FakcharoenpholRT03} that such tree embeddings can approximately preserve pairwise distances. Our key insight is that it is possible to truncate the tree embedding so that leaves represent sets of points of large enough cardinality and then use them to compute a solution for the $k$-median problem. In fact, by using this insight and by carefully adding Laplace noise to the cardinality of the sets considered by the algorithm, we obtain our differentially private $k$-median algorithm.  \n\nOur second contribution is a parallel implementation of our algorithm in the classic massively parallel computing (MPC) model. This model is a theoretical abstraction of real-world systems like MapReduce \\cite{dean2008mapreduce}, Hadoop \\cite{white2012hadoop}, Spark \\cite{zaharia2010spark} and Dryad \\cite{isard2007dryad} and it is the standard for analyzing algorithms for large-scale parallel computing~\\cite{karloff2010model, goodrich2011sorting, beame2013communication}. Interestingly we show that our algorithm can be efficiently implemented using a logarithmic number of MPC parallel rounds for $k$-median clustering. To the best of our knowledge, our algorithms are the first differentially private algorithms for $k$-median that can be efficiently parallelized.\n\nThird, we complement our theoretical results with an in-depth experimental analysis of the performance of our $k$-median algorithm. We demonstrate that not only our algorithms scale to large datasets where, until now, differential private clustering remained elusive, but also we outperform other state-of-the-art private clustering baselines in medium-sized datasets. \nIn particular, we show that in practice compared to prior work with theoretical guarantees such as~\\cite{balcan}, our parallel algorithm can scale to more $40$ times larger datasets, improve the cost by up to a factor of $2$, and obtain solutions within small single digit constant factor of non-private baselines.\n\nFinally, we adapt those techniques to the $k$-means problem. This poses an additional challenge because randomly-shifted quadtrees do not preserve squared distances accurately and so we cannot apply our approach directly. We adapt a technique first introduced by~\\cite{CohenAddadFS19} to our setting. The key observation behind our approach is that even if randomly-shifted quadtrees do not preserve all the squared distances well, they accurately preserve most of them. \n\nSuppose that we are given in input some solution $S$ for our problem (later will clarify how to obtain it), then we show that for most centers in $S$ it is true that the distances to these centers are approximately preserved by the quadtree. So points living in the clusters of these centers can be clustered using  the tree embedding, and the remaining points can be clustered using the few centers that are not preserved in the solution $S$. \nInterestingly, we  prove that we can use this approach iteratively starting with a solution to the differentially private $1$-means problem as $S$. In Section~\\ref{sec:kmeans} we show that this approach leads to an efficient differentially private algorithm for $k$-means.\n\n%\n%\n\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\\label{sec:prelim}\n\\paragraph{Notations.}\nFor two points $p$ and $q$ in $\\mathbb{R}^d$, we let $\\dist(p, q):=\\|p-q\\| = \\sqrt{\\sum_{i=1}^d (p_i-q_i)^2}$ be the Euclidean distance between $p$ and $q$. Given $r \\ge 0$, we define $B(x,r) = \\{y \\in \\Re^d | \\; \\dist(x, y) \\le r\\}$ as the closed ball around $x$ of radius $r$. \n\nWe are given a set of points $P$ as input, and assume that $P$ is contained in the open ball $B(0, \\Lambda)$. \n\n\nWe seek to find $k$ centers $C = \\{c_1, \\ldots c_k\\}$, that approximately minimize the $(k,z)$-clustering where distances of every point to their closest center are raised to the power of $z\\geq 1$\n$$\n\\cost(P, C) = \\sum_{p \\in P} \\min_{c \\in C} \\dist(p, c)^z \\mathrm{.}\n$$\nWe use $\\opt$ to indicate an optimal solution to the problem. We define $\\dist(p, C)  = \\min_{c \\in C} \\dist(p, c)$. In this article, we focus on $k$-median ($z = 1$) and $k$-means ($z = 2$).\n\n\nWe say that a solution $C$ is $(\\alpha,\\beta)$-approximate if the $\\cost(P,C) \\le \\alpha \\cost(P, \\opt) + \\beta$. We will seek solutions where $\\alpha$ is $\\tilde O (\\poly \\log(k))$ and $\\beta$ is $\\tilde O (\\poly(k, d, \\log(n))\\Lambda^z)$.\n\nThe goal of this paper is to have private algorithms that are easy to parallelize and that run in {\\it near-linear} running time, where by near linear we mean time $\\tilde O(n \\cdot \\poly(\\log(n),k,d,\\log(\\Lambda)))$. Notice that celebrated k-means++~\\cite{arthur2007k} satisfies all the requirements\nwith its $O(nkd)$ running time and $O(\\log(k))$ approximation, except that it is not private. \n\n\\paragraph{Differential privacy.}\n\\\nWe will make use of standard composition properties of differentially private algorithms, described in \\cite{dwork}. The algorithm $\\mathcal{A}$ that applies successively two algorithm $\\mathcal{A}_1$ and $\\mathcal{A}_2$ that are respectively $\\eps_1$-DP and $\\eps_2$-DP is itself $(\\eps_1 + \\eps_2)$-DP. If the $\\mathcal{A}_1$ and $\\mathcal{A}_2$ run on two distinct parts of the dataset, then $\\mathcal{A}$ is $\\max(\\eps_1, \\eps_2)$-DP. \n\nLastly, if $\\mathcal{A} : D_1 \\times D_2 \\rightarrow Z$ satisfies that for all $X \\in D_1$, the algorithm $ \\mathcal{A}(X, \\cdot)$ is $\\eps_1$-DP, and some algorithm\n$\\mathcal{B} : D_2 \\rightarrow D_1$ is $\\eps_2$-DP, then the algorithm $X \\rightarrow \\mathcal{A}(\\mathcal{B}(X), X)$ is $(\\eps_1 + \\eps_2)$-DP. \n\nA standard differentially private algorithm is the Laplace Mechanism (see \\cite{dwork}). We say that a random variable follows distribution $\\laplacenoise(b)$ if its probability density function is $\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)$. With a slight abuse of notation, we use $\\laplacenoise(b)$ to denote a variable that follows such a distribution. Example 3.1 in \\cite{dwork} shows that the following algorithm for counting queries is $\\eps$-DP :\n$\\mathcal{A}(X) = |X| + \\laplacenoise(1/\\eps)$.\n\nNote that the notion of differential privacy is only a model, and our result should not be used blindly to preserve privacy of users.\nWe emphasize in particular that the privacy notion is with respect to a single user's data: hence, this model does not necessarily ensure privacy for a \\emph{group} of people.\n\n\\paragraph{Randomly-shifted Quadtrees.}\nA quadtree is a binary tree $T$, such that each node $x$ in the tree corresponds to a region $T(x)$ of $\\mathbb{R}^d$. To distinguish from the input, we call tree nodes \\emph{cells}. Each cell is a hyper-rectangle. For a cell $c$ with children $c_1, c_2$, the region spanned by $c$ is the union of those spanned by $c_1$ and $c_2$, i.e., $T(c) = T(c_1) \\cup T(c_2)$. \n\nA shifted quadtree is constructed as follows. Start from a root cell containing the entire $d$-dimensional hypercube $[-\\Lambda, \\Lambda]^d$ at depth $0$, and proceed recursively. \nLet $c$ be a cell at depth $d\\cdot i + j$, with $0 \\leq j < d$. The $j$-th coordinate of the region spanned by $c$ is comprised in $[m, M]$. The children of $c$ are constructed as follows: let $x$ be some random number in $[m + \\frac{M-m}{3}, M - \\frac{M-m}{3}]$. $c_1$ comprises all points of $c$ that have their $j$-th coordinate at most $x$, and $c_2$ the remaining points\\footnote{Another standard way of defining quadtree is to have $2^d$-regular trees, and to split along the $d$-dimensions at each step. We are more comfortable working with binary trees, which allows for a simpler dynamic program.}. Note that the diameter of the cells is divided by at least $3/2$ every $d$ levels. Denote by $\\diam(c)$ the diameter of cell $c$. \n\n\n\\begin{algorithm}\\caption{DP-kMedian($P$)}\n\\label{alg:kmed}\n\\begin{algorithmic}[1]\n\\State Compute a shifted quadtree $T$. Let $r$ be the root of $T$.\n\\State $w = $ MakePrivate($T, P$). \n\\State Compute $v, S = $DynamicProgram-kMedian($T, w, r$)\n\\State \\textbf{Return} $S_k$\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\begin{algorithm}\n\\caption{MakePrivate($T, P$)}\\label{alg:makePrivate}\n\\begin{algorithmic}[1]\n\\State Input: a quadtree $T$, a set of points $P$\n\\State let $Q$ be a queue, initially containing only the root of $T$.\n\\State Let $w : T \\rightarrow \\mathbb{N}$, initiated with $\\forall c,~w(c) = 0$.\n\\While{$Q$ is not empty}\n\\State Let $c = Q.$pop()\n\\If{$\\diam(c) > \\Lambda/n$}\n\\State let $w(c) = |T(c) \\cap P| + \\laplacenoise(d \\log n / \\eps)$\n\\If{$w(c) > 2 d \\log n / \\eps$}\n\\State Add $c$'s children to $Q$\n\\EndIf\n\\EndIf\n\\EndWhile\n\n\\State \\textbf{Return} $w$\n\\end{algorithmic}\n\\end{algorithm}\n\n\nA quadtree $\\calD$ induces a metric: for two points $p$, $q$, we define $\\dist_\\calD(p, q) = \\diam(c)$ where $c$ is the smallest cell that contains both $p$ and $q$. We will frequently use that the expected distortion between two points $p$ and $q$ in a shifted quadtree of depth $d\\cdot \\alpha$, for any $\\alpha$, is $\\E_\\calD[\\dist_\\calD(p, q)] \\leq d^{3/2} \\alpha \\dist(p, q).$ \n\n\n\\begin{lemma}\\label{lem:ballCut}[Reformulation of Lemma 11.3 \\cite{har2011geometric}]\n  For any $i$, radius $r$ and point $p$, we have that $\\Pr[B(p, r) \\text{ is cut at level } i] = O\\left(\\frac{d r}{2^i}\\right)$.\n\\end{lemma}\n\n\nIn our case, we stop the construction when reaching cells of diameter $\\Lambda / n$. Hence, $\\alpha = \\log n$\nand the expected distortion is $O(d^{3/2} \\log n)$. \nSuch trees are often called Hierarchically Separated Trees (HST) in the literature.\n\n\n\\paragraph{Dimension Reduction.} \nFor clustering problems, it is possible to use the Johnson-Lindenstrauss Lemma to reduce the dimension to $O(\\log k)$ (see \\cite{makarychev2019performance}). Hence, we can apply all our algorithms in such a dimension, replacing dependency in $d$ by $\\log k$. \nTo compute centers in the original space, we can extract the clusters from the low-dimensional solution and compute privately the $1$-median (or $1$-mean) of the cluster in the original $d$-dimensional space. This adds an additive error $O(kd)$. Later we describe how to implement this procedure in MPC.\n"
            },
            "section 3": {
                "name": "Simple algorithm for $k$-Median",
                "content": "\n\n\n\\paragraph{Algorithm}\n\nA simple way of solving $k$-median is to embed the input points into a tree metric. Tree metrics are sufficiently simple to admit a dynamic program for computing an optimum. The approximation factor of this algorithm is therefore the distortion incurred by the embedding. \nWe adapt this approach to incorporate privacy as follows. First, we embed the input into a quadtree, which is a hierarchical decomposition of $\\mathbb{R}^d$ via axis-aligned hyperplanes. We then add noise on the quadtree to enforce privacy.\nSubsequently, we run the dynamic program on the quadtree.\nUnfortunately, a naive implementation of the dynamic program falls short of the nearly linear time algorithm we are hoping for.\nWe speed this up by trimming the recursion of the dynamic program for quadtree cells containing few points. \nTo do this, we require a private count of the number of points in each cell, that guides the dynamic program.\nSuch a private count can be obtained from the Laplace mechanism (see the preliminaries), and the error incurred by the privacy is charged to the additive term of the approximation. The result we aim to show is the following theorem:\n\\begin{theorem}\\label{thm:kmed}\nAlgorithm~\\ref{alg:kmed} is $\\eps$-DP and computes a solution with expected cost at most \n$O(d^{3/2}\\log n)\\cdot \\opt + \\frac{d^2 \\cdot \\log^2 n \\cdot k}{\\eps}\\cdot \\Lambda$. Furthermore, it runs in time $\\tilde O(n d k^2)$.\n\\end{theorem}\n\n\n\n\\begin{algorithm}\n\\caption{DynamicProgram-kMedian($T, w, c$)}\\label{alg:dynKMed}\n\\begin{algorithmic}[1]\n\\State Input: A quadtree $T$, a weight function on $T$'s node $w$, and a cell $c$.\n\\State Ouput: For each $k' \\leq k$, a value $v_{k'}$ and a solution $S_{k'}$ for $k'$-median on $T(c)$\n\\State set $v_0 \\gets w(c)\\cdot \\diam(c)$ and $S_0 \\gets \\emptyset$. \n\\If{$w(c) < \\frac{2 d \\log n}{\\eps}$} \n\\State For all $k'$, set $v_{k'} \\gets 0$ and $S_{k'} \\gets k'$ copies of the center of $c$.\n\\Else \n\\State Let $c_1, c_2$ be the two children of cell $c$\n\\State Let $v^i, S^i$ be the output of  DynamicProgram-kMedian($T, w, c_i$).\n\\State for all $k'$, let $(k_1, k_2) = \\text{argmin}_{k_1 + k_2 = k'} v^1_{k_1} + v^2_{k_2}$, and do $v_{k'} \\gets v^1_{k_1} + v^2_{k_2}$, $S_{k'} \\gets S^1_{k_1} \\cup S^2_{k_2}$.\n\\EndIf\n\n\\State \\textbf{Return} $v, S$.\n\\end{algorithmic}\n\\end{algorithm}\n\n\\paragraph{Analysis}\n\\begin{lemma}\\label{lem:dynKMed}\nStep 3 of \\cref{alg:kmed} computes a solution with expected  cost $\\opt_T + k\\cdot d^2 \\log^2 n/ \\eps \\cdot \\Lambda$, where $\\opt_T$ is the optimal solution on the metric induced by the tree $T$, and the expectation is taken over the realization of the variables $\\laplacenoise$. \n\\end{lemma}\n\\begin{proof}\nIn an HST metric, we have the following property. For a cell $c$, three points $x, y \\in T(c)$ and $z \\notin T(c)$, it holds that $\\dist(x, y) \\leq \\diam(T(c)) \\leq \\dist(x, z)$. Hence, if there is a center in $T(c)$, then all clients of $T(c)$ can be served by some center in $T(c)$. Moreover, if there is no center in $T(c)$, points in $T(c)$ are at distance of at least $\\diam(T(c))$ from a center.\n\nLet $F(c, k')$ be the expected cost of Dynamic\\-Program-\\-k\\-Median($T, k', c$), \nwhere the probability is taken over the privacy randomness.\nand $S$ be any solution. We show by induction that\nfor any cell $c$ of height $h$ in the tree with $T(c) \\cap S \\neq \\emptyset$, \n{\\setlength{\\emergencystretch}{2.5em}\\par}\n$$F(c, |S \\cap T(c)|) \\leq \\cost_T(T(c), S) + \\frac{d \\log n}{\\eps} \\cdot \\Lambda \\cdot |S \\cap T(c)| \\cdot h.$$\nThis is true by design of DynamicProgram-kMedian for any leaf that contains a center of $S$.\n\nFor an internal node $c$ with two children $c_1, c_2$ such that $T(c_1) \\cap S \\neq \\emptyset$ and $T(c_2) \\cap S \\neq \\emptyset$:\nit holds that $F(c, S \\cap T(c)) \\leq F(c_1, |S \\cap T(c_1)|) + F(c_2, |S \\cap T(c_2)|)$. Hence, by induction:\n\\begin{align*}\n    &F(c, |S \\cap T(c)|) \\leq F(c_1, |S \\cap T(c_1)|) + F(c_2, |S \\cap T(c_2)|)\\\\\n    &\\leq \\cost_T(T(c_1), S) + \\frac{d \\log n}{\\eps} \\cdot \\Lambda \\cdot |S \\cap T(c_1)| \\cdot (h-1)\\\\ \n    &\\qquad + \\cost_T(T(c_2), S) + \\frac{d \\log n}{\\eps} \\cdot \\Lambda \\cdot |S \\cap T(c_2)| \\cdot (h-1)\\\\\n    &\\leq \\cost_T(T(c), S) + \\frac{d \\log n}{\\eps} \\cdot \\Lambda \\cdot |S \\cap T(c)| \\cdot h,\n\\end{align*}\nwhere the last lines use $\\cost(T(c), S) = \\cost(T(c_1), S) + \\cost(T(c_2), S)$.\n{\\setlength{\\emergencystretch}{2.5em}\\par}\n\nThe last case is when $S \\cap T(c_1) = \\emptyset, S \\cap T(c_2) \\neq \\emptyset$. Let $h$ be the height of $c$. We have $S \\cap T(c_2) = S \\cap T(c)$, and so: \n\\begin{align*}\n    F(&c, |S \\cap T(c)|) \\leq F(c_1, 0) + F(c_2,  |T(c_2) \\cap S|)\\\\\n    &\\leq T(c_1) \\cdot \\diam(c_1) + \\E[\\laplacenoise(\\eps / (d \\log n)) \\cdot \\diam(c_1)] \\\\\n    &\\qquad + \\cost_T(T(c_2), S) + \\frac{d \\log n}{\\eps} \\cdot \\Lambda \\cdot |S \\cap T(c_2)| \\cdot (h-1)\\\\\n    &\\leq \\cost_T(T(c), S) + \\frac{d \\log n}{\\eps} \\cdot |S \\cap T(c)| \\cdot \\Lambda \\cdot h.\n\\end{align*}\n\n\nThis shows that the value computed by the algorithm is at most $\\cost_T(\\opt) + \\frac{d^2 \\log^2 n}{\\eps} \\cdot \\Lambda \\cdot k$. Now, we need to show the converse: the value computed corresponds to an actual solution. \n\nThis is done inductively as well. For any $k'$ and cell $c$ one can compute a solution $S$ for $T(c)$ with $k'$ centers and expected cost at most $F(c, k') + \\frac{d \\log n}{\\eps} \\cdot k'$. For that, the base cases are when $k' = 0$, and then $\\emptyset$ works, or when $w(c) \\leq \\frac{2d\\log n}{\\eps}$, where the center of the cell works. Otherwise,\nit is enough to find $k_1, k_2$ such that $k_1+k_2 = k'$ and $F(c, k') = F(c_1, k_1) + F(c_2, k_2)$. Let $S_i$ be the solution computed for $T(c_i)$ with $k_i$ centers: the solution for $T(c)$ is simply $S_1 \\cup S_2$. By induction, its cost is at most \n\\begin{align*}\n    F(c_1, k_1) + F(c_2, k_2) + \\frac{d \\log n}{\\eps} \\cdot \\Lambda \\cdot (k_1 + k_2) \\\\\n    = F(c, k') +  \\frac{d \\log n}{\\eps}\\cdot \\Lambda  \\cdot k'. \\qquad \\qedhere\n\\end{align*}\n\\end{proof}\n\n\n\\begin{proof}\nWe show by induction that for a tree $T$ of depth $h$, MakePrivate($T, P$) is $\\left(\\frac{\\eps}{d \\log n} \\cdot h\\right)$-DP.\n\nWhen the root of the tree has diameter at most $\\Lambda/n$, the algorithm returns the zero function, which is $0$-DP.\nLet $\\calD$ be a tree of depth $h$ rooted at $r$ with $\\diam(r) > \\Lambda/n)$, and let $r_1, r_2$ be the two children of $r$. Computing $w(r)$ is $\\frac{\\eps}{d \\log n}$-DP, by property of the Laplace Mechanism.\n\nNow, by induction hypothesis, MakePrivate($T(r_1), P$) and\nMakePrivate($T(r_2), P$) are $\\left(\\frac{\\eps}{d \\log n} \\cdot (h-1)\\right)$-DP. Since they are computed on two disjoint sets, the union of the two results is  $\\left(\\frac{\\eps}{d \\log n} \\cdot (h-1)\\right)$-DP as well. \nNotice that the algorithm MakePrivate($T, P$) boils down to computing $w(r)$,  MakePrivate($T(r_1), P$) and  MakePrivate($T(r_2), P$).\nHence, by composition MakePrivate($T, P$) is $\\left(\\frac{\\eps}{d \\log n} \\cdot h\\right)$-DP.{\\setlength{\\emergencystretch}{2.5em}\\par}\n\\end{proof}\n\nCombining Lemmas~\\ref{lem:dynKMed}, \\ref{lem:makePrivateDP} and properties of quadtrees, we conclude the proof of \\cref{thm:kmed}:\n\\begin{proof}[Proof of \\cref{thm:kmed}]\nWe start by proving the approximation guarantee. For this, note that the key property of quadtrees is that  \n$\\E_\\calD[\\dist_\\calD(p, q)] \\leq O\\left(d^{3/2} \\log n\\right) \\dist(p, q)$, where the expectation is taken on the tree randomness.\nHence, the optimal $k$-median solution is only distorted by a $O\\left(d^{3/2} \\log n\\right)$ factor: $\\opt_T \\leq O\\left(d^{3/2} \\log n\\right) \\opt$. \n\nCombined with Lemma \\ref{lem:dynKMed}, this shows the approximation guarantee of the whole algorithm.\nLemma \\ref{lem:makePrivateDP} shows the privacy guarantee. \nWhat therefore remains is to bound the running time. \n\nComputing the cells of the quadtree containing some points of $P$ can be done in a top-down manner in time $O(nd \\log n)$ as follows. \nLet $c$ be a cell at depth $d\\cdot i + j$ with  $j < d$, and $c_1, c_2$ be the two children of $c$. Given a $T(c) \\cap P$, it is easy to compute $T(c_1) \\cap P$ and $T(c_2) \\cap P$ in time $O(|T(c) \\cap P|)$, by partitioning $T(c) \\cap P$ according to the value of their $j$-th coordinate. Since there are $O(d \\log n)$ levels in the tree, this is done in time $O(nd \\log n)$.\n\nHence, the running time of \\cref{alg:makePrivate} is bounded by $\\tilde O(nd)$ plus the time to process empty cells added to $Q$.\nThere are at most $nd \\log n$ empty cells with a non-empty parent added -- one per level of the tree and per point of $P$. Each of them gives rise to a Galton-Watson process: each node adds its two children with probability $\\Pr[\\laplacenoise(d \\log n / \\eps) > 2d \\log n / \\eps] = e^{-2} < 1/2$. By standard properties of a Galton-Watson process, this goes on for a constant number of steps. \nTherefore, there are at most $\\tilde O(n d)$ empty cells added to $Q$, which concludes the running time bound for \\cref{alg:makePrivate}. \n\nLet $N$ be the number of cells that have a non-zero value of $w$. We claim that $N = \\tilde O(nd)$ and that the running time of \\cref{alg:dynKMed} is $O(Nk^2)$. \nFor the first claim, note that $N$ is equal to the number of cells added to $Q$, which is $\\tilde O(nd)$ as explained previously. For the second claim, notice that there are at most $kN$ different calls to DynamicProgram-kMedian, each being treated in time $O(k)$. Hence, the complexity of \\cref{alg:dynKMed} is $O(Nk^2) = \\tilde O(ndk^2)$. This concludes the proof.\n\\end{proof}\n\n\n\n"
            },
            "section 4": {
                "name": "MPC Implementation",
                "content": "\n\\paragraph{Brief description of MPC}\\label{sec:mpc}\n\nWe briefly summarize the MPC model \\cite{beame2017communication}. The input data has size $N = nd$, where $n$ is the number of points, and $d$ the dimension. We have $m$ machines, each with local memory $s$ in terms of words (of $O(\\log(ms))$ bits). We assume that each word can store one input point dimension. We work in the {\\it fully-scalable} MPC framework~\\cite{andoni2018parallel} where the memory is sublinear in $N$. \nMore precisely, the machine memory is $s=\\Omega\\left(N^{\\delta}\\right)$ for some constant $\\delta\\in (0,1)$, and the number of machines $m$ is such that $m\\cdot s=\\Omega\\left(N^{1+\\gamma}\\right)$, for some $\\gamma > 0$.\n\nThe communication between machines is as follows.\nAt the beginning of the computation, the input data is distributed arbitrarily in the local memory of machines, the computation proceeds in parallel rounds where each machine can send (and receive) arbitrary messages to any machine, subject to the total messages space of the messages received (or sent) is less than $s$. In case some machine receives more than $s$ messages, the whole algorithm fails.\n\nFor our MPC algorithm we assume that $k\\ll s$. This ensures that the final solution of size $kd$ fits in the memory of one machine, which is common for real world applications.  \n\nMore formally we assume that there are $m =  \\Omega(n^{1-\\delta+\\gamma})$ machines each with memory $s = \\Omega(n^{\\delta} d\\log(n))$, and $k \\leq n^{\\gamma}$, with $\\delta - \\gamma > \\eps$ for some constant $\\eps$. \n\nIn that section, we first show the following \\emph{low dimensional} theorem:\n\\begin{theorem}\n\\mbox{}\\label{thm:kmedMPC}\nAssuming  $k \\leq n^{\\gamma}$, there exists a $O(d \\log n)$ rounds MPC algorithm using $m =  O(n^{1-\\delta+\\gamma})$ machines each with memory $s = O(n^{\\delta} d\\log(n))$ that simulates exactly the private $k$-median from Theorems~\\ref{thm:kmed}.\n\\end{theorem}\n\nThis algorithm is suited for low dimensional spaces, as the number of rounds depends on $d$. We show in Section~\\ref{sec:kmedMPC} how to replace this dependency by a $O(\\log k)$, both in the number of rounds and in the approximation ratio.\n\nWe then show how to use dimension reduction, to replace dependencies in $d$ by $\\log k$:\n\\begin{theorem}\n\\mbox{}\\label{thm:kmedMPCHD}\nAssuming  $k \\leq n^{\\gamma}$, there exists a $O(\\log k \\cdot \\log n)$ rounds MPC algorithm using $m =  O(n^{1-\\delta+\\gamma})$ machines each with memory $s = O(n^{\\delta} d\\log(n))$ that computes a solution to $k$-median with cost at most\n\\[O\\left(\\log^{3/2} k \\log n \\cdot \\opt + \\frac{\\log^{3} k \\log^{3} n \\cdot k}{\\eps} \\cdot \\Lambda + \\frac{kd \\log k}{\\eps}\\cdot \\Lambda \\right).\\]\n\\end{theorem}\n\n\n",
                "subsection 4.1": {
                    "name": "Algorithm for Low Dimensional Inputs",
                    "content": "\nWe now describe a high level view of our algorithm which as we can prove simulates exactly (with high probability) our private $k$-median algorithm. The algorithm uses a shared hash function $h$ to compute the quadtree consistently over the machines. \nInformally, first, each machine computes over the points stored, all the cells which the points belong to in the tree at each level. \nTo compute the total count of each cell, one can use the algorithm from Andoni et al.~\\cite{andoni2018parallel} (section E.3 of the arxiv version), that computes in a constant number of rounds the number of points in each cell. At the end of that algorithm, the size of each cell is stored in some unspecified machine. To organize the quadtree data in order to be able to process it, we use a shared function $r$ such that a machine $r(c)$ is responsible for all computations related to cell $c$. We will need care to ensure that no machine is responsible for more cell than what its memory allows.\n\nThen the computation proceeds bottom-up solving the dynamic programming problem in $O(d \\log n)$ rounds.\\footnote{We note that a more careful and intricate implementation of the dynamic program that requires only $O(d)$ rounds can be achieved. We decided to chose simplicity rather than saving one log factor.} Finally, the computation proceeds over the tree top-down in other  $O(d \\log n)$ rounds to extract the solution. \n\n\n\\begin{algorithm}\n\\caption{MPC-quadtree($P$)}\\label{alg:mpc-aggregation}\n\\begin{algorithmic}[1]\n\\State Each machine receives an arbitrary set of $n^\\delta$ points of $P$.\n\\State Each machine, for each point $p$ received, computes, using $h_s$, the quadtree cell $c_i(p)$ in which the point $p$ is at level $i \\in [d\\log(n)]$. \n\\State Compute the count of every cell.\n\\State Send the count of cell $c$ to machine $r(c)$, for all $c$.\n\\end{algorithmic}\n\\end{algorithm}\n\n\nUsing the algorithm from Andoni et al.~\\cite{andoni2018parallel}, one can compute in $O(1)$ steps the count for each cell of the quadtree. Hence, we have the following result:\n\n\\begin{fact}\n\\cref{alg:mpc-aggregation} runs in $O(1)$ many rounds.\n\\end{fact}\n\n\nAt the end of \\cref{alg:mpc-aggregation}, we are given a quadtree, represented as follows: each cell $c$ is represented by a machine $r(c)$, which stores a count of input nodes in the cell and pointers towards each children. $r$ is a surjection from a set of $O(nd \\log n)$ cells to $m$ machine: we chose it in order to ensure that for any machine $\\mathcal{M}$, $|r^{-1}(\\mathcal{M})| \\leq \\frac{O(nd \\log n)}{m}$.\n\nWe now explain in more details how to implement the algorithm from \\cref{thm:kmed}, given that representation of the quadtree. \n\nFirst, it is straightforward to implement \\cref{alg:makePrivate} in $1$ rounds -- as each cell only needs to compute the DP count of points in the cell. Next, \\cref{alg:dynKMed} is straightforwardly implemented  in $O(d \\log n)$ rounds, as computing the output vector $v$ of the dynamic program for a cell only requires knowing those of its children -- and it is therefore easy to simulate bottom-up the dynamic program.\n\nWhat remains to be proven is that no machine gets responsible for more cell than it can afford in memory. More precisely, every time a machine is responsible for a cell, it stores $O(k)$ memory words, for the simulation of the dynamic program.\nHence, we need to show that no machine is responsible for more than $s/k$ many cells.\n\n\n\\begin{fact}\nNo machine is responsible for more than $s/k$ many cell.\n\\end{fact}\n\\begin{proof}\nOur choice of $m$ and mapping $r$ ensures that a given machine gets responsible for at most $\\frac{O(nd \\log n)}{m} = O\\left(n^{\\delta - \\gamma} d \\log n\\right)$. Similarly, our constraints on $k$ and $s$ ensures $\\frac{s}{k} = \\Omega(n^{\\delta - \\gamma} d \\log n)$, which concludes the proof.\n\\end{proof}\n\nCombining those two facts concludes \\cref{thm:kmedMPC}.\n\n\\newcommand{\\coreset}{\\Omega}\n\\newcommand{\\subApprox}{\\mathfrak{a}}\n\\newcommand{\\calC}{\\mathcal{C}}\n\\newcommand{\\calS}{\\mathcal{S}}\n\\newcommand{\\polylog}{\\text{polylog}}\n\n"
                },
                "subsection 4.2": {
                    "name": "$k$-Median in $O(\\log n)$-MPC rounds via dimension reduction",
                    "content": "\\label{sec:kmedMPC}\nThe goal of this section is to use standard dimension-reduction techniques to remove the dependency in the dimension from \\cref{thm:kmedMPC} and show \\cref{thm:kmedMPCHD}.\n\nFor that, one can use dimension reduction techniques to project the dataset onto $O(\\log k)$ dimensions, while preserving the cost of any clustering.\n\nHowever, the output of our algorithm should be a set of centers in $\\R^d$, and not a clustering:\nan additional step is therefore needed, once clusters have been computed in $\\R^{O(\\log k)}$, to \\textit{project back} and find centers in the original space. For $k$-means, this can easily be done using differentially-private mean \\cite{badih_approximation}. We show how to perform the equivalent for $k$-median.\n\nWe draw here a connection with the coreset literature. More precisely, we leverage results from Cohen-Addad et al.~\\cite{Cohen-AddadSS21}, who showed how to compute an approximate solution to $1$-median by only considering an uniform sample of constant size. Therefore, in the MPC setting it is enough to sample a constant number of points from each cluster computed in low dimension, and send them to a machine that can compute a median for them in the original high dimensional space.\n\nFor that last step, we rely on the following result.\n\\begin{lemma}[Corollary 54 in \\cite{badih_approximation}]\n\\mbox{}\\label{lem:DP1median}\nFor every $\\eps >  0$, there is an $\\eps$-DP polynomial time algorithm for $1$-median such that, with probability $1-\\beta$, the additive error is $O\\left(\\frac{d \\Lambda}{\\eps} \\text{polylog}\\left(\\frac{1}{\\beta}\\right)\\right)$\n\\end{lemma}\n\nWe consider the following algorithm, a simplified variant of Algorithm 1 in Cohen-Addad et al.~\\cite{Cohen-AddadSS21}.\n\n\\begin{algorithm}\n\\begin{algorithmic}\n\\State \\textbf{Input:} A dataset $P$, an $\\alpha$-approximate median $\\subApprox$ for $P$ with cost $\\calC$, and parameters $t, d_{close}, r_{small}$.\n\\State 1. Sample a set $\\coreset$ of $t$ points uniformly at random.\n\\State 2. Remove from $\\coreset$ all points at distance less than $\\Delta = \\frac{d_{close}}{\\alpha}\\cdot \\frac{\\calC}{|P|}$, and add to $\\coreset$ the point $\\subApprox$ with multiplicity equal to the number of removed points.\n\\State 3. Define rings $R_i$ such that $R_i\\cap \\coreset$ contains all the points at distance $(2^{i} \\cdot \\Delta, 2^{i+1}\\cdot \\Delta]$ from $\\subApprox$, for $i \\in \\{1,..., \\log(|P| \\alpha / \\mu_2)\\}$.  Let $R_0$ be $\\{\\subApprox\\}$, with multiplicity defined in step 2.\n\\State 4. If $|R_i\\cap \\coreset|< r_{small}\\cdot |\\coreset| + \\laplacenoise(1 / \\eps)$, remove all points in $R_i\\cap \\coreset$ from $\\coreset$.\n\\State 5. Solve the problem on the  set $\\coreset$, using the algorithm given by \\cref{lem:DP1median} with $\\beta = 1/k$.\n\\end{algorithmic}\n \\caption{Finding the median via uniform sampling}\n \\label{alg:coreset}\n\\end{algorithm}\n\n\\begin{lemma}\nAlgorithm \\cref{alg:coreset} is $2\\eps$-DP.\n\\end{lemma}\n\\begin{proof}\nFirst, the set of rings selected at step 4 is $\\eps$-DP: the selection of one ring is $\\eps$-DP, by Laplace mechanism, and since the rings are disjoint the composition of DP mechanisms ensures that the full set of selected rings is $\\eps$-DP.\n\nNow, given a selected set of rings, the set $\\coreset$ varies by at most one point when the input $P$ varies by a single point.\nSince the algorithm used in step 5 is $\\eps$-DP, by composition, the whole algorithm is $2\\eps$-DP.\n\\end{proof}\n\nAs shown by Cohen-Addad et al.~\\cite{Cohen-AddadSS21}, this algorithm computes an $O(1)$-approximation to $1$-median on $P$, with $t = \\polylog(|P|)$. \nHence, we can easily use it to \\emph{project back} the centers, and conclude the proof of \\cref{thm:kmedMPCHD}.\n\n\\begin{proof}[Proof of \\cref{thm:kmedMPCHD}.]\nUsing Johnshon-Lindenstrauss lemma, it is possible to project the points onto a space of dimension $\\tilde d = O(\\log k)$, preserving the cost of any clustering up to a constant factor (see Makarychev et al.~\\cite{makarychev2019performance}). \nIn that projected space, the algorithm from \\cref{thm:kmedMPC} computes privately a solution with cost $O(\\log^{3/2} k \\log n) \\cdot \\opt + \\frac{\\log^{3} k \\log^{3} n \\cdot k}{\\eps} \\cdot \\Lambda$, but centers are not points in $\\R^d$ -- they are nodes of the quadtree.\n\nTo compute good centers in $\\R^d$ from the quadtree solution, we use \\cref{alg:coreset}: in each cluster induced by the quadtree solution, sample the set $\\coreset$. Since $\\coreset$ has size $O(\\log^{3} k \\log^2 n)$, it can be sent it to a centralizing machine, that in turn can run \\cref{alg:coreset}. The additional additive error is $O\\left(\\frac{d \\Lambda \\log k}{\\eps}\\right)$ in any cluster, hence in total $O\\left(\\frac{kd\\Lambda \\log k}{\\eps}\\right)$. To sample the set $\\coreset$, each machine can send to the centralizing one the number of points it stores from $P$, and the centralizing computes the number of points to be sampled in each machine.\\footnote{For instance, the centralizing machine can sample as set $R$ of $\\mu_1$ points from $\\{1,...,|P|\\}$. Then, if machine $i$ stores $n_i$ points from $P$, it computes a uniform sample $R \\cap (\\sum_{j < i} n_i, \\sum_{j \\leq i} n_i]$ many points. The union of those sample is uniform.}\n\\end{proof}\n"
                }
            },
            "section 5": {
                "name": "Extension to $k$-Means",
                "content": "\\label{sec:kmeans}\nThe main focus in the paper is on k-median, however we can also show an extension of our result for $k$-means: \n\n\nWe give an in-depth description with full proofs in \\cref{ap:kmeans}. Here, we outline the high-level ideas, where we show as well how to remove the extra $\\alpha k$ centers, to get an approximate solution with exactly $k$ centers. \nAs explained in the introduction, we establish the following lemma, that shows how we can improve a solution given as input.\n\n\n\n\nAlthough the quadtree decomposition approximates distances well in expectation, it works poorly for squared distances. Indeed, two points $p, q$ have probability $\\frac{d \\cdot \\dist(p, q)}{2^i}$ to be cut at level $i$: hence, the expected distance squared between $p$ and $q$ is \n$d \\cdot \\dist(p, q) \\cdot \\sum_i \\sqrt d 2^i$, which means that the distance squared can be distorted by an arbitrarily large factor in expectation.\n\n However, observe that $p$ and $q$ have tiny probability to be cut at a level way higher than $\\log (d\\cdot \\dist(p, q))$. Hence, there is a tiny probability that points are cut from their optimal center at a high-level. The question is then: what to do when this happens? Here we want to avoid routing in the tree since the squared distance could be arbitrarily large and we may want to deal with such points in a different way.\n To do so, we use a baseline solution $L$ to guide our decisions on points for which the tree distance to their closest center in the optimum solution badly approximates the true distance, let call them \\emph{bad points}.\nSince we don't know the optimum solution, we don't know the bad points and so we will use $L$ as a proxy for finding the potential bad points.\n\n\nWe show that the solution computed by our algorithm is good w.r.t. to a solution that contains all facilities of $L$  for which the quadtree distances are not a good approximation of the true distances. We call those facilities \\emph{badly-cut}. To bound the cost of a client $c$, we distinguish three cases. Either the distance from a point to the optimal center is good in the tree, and we are happy because we can serve it nicely in the tree. Or its closest center of $L$ is not badly-cut, in which case we argue that the distance to the optimal center cannot be too high compared to its optimal cost. In the last case, where the closest center of $L$ is badly-cut, we simply assign the point to $L$ since we are working with a solution containing all centers of $L$. This happens with some tiny probability, and will not be too costly overall, i.e.: only a tiny fraction of the cost of $L$.\n\\\n\n"
            },
            "section 6": {
                "name": "Empirical Evaluation",
                "content": "\n\\label{sec:exp}\n\n\n\n\n\n\n\n\nIn this section, we present an empirical evaluation of our algorithm for the $k$-median objective. To the best of our knowledge, this is the first comprehensive empirical evaluation of private k-median algorithms as the majority of experimental results has previously focused on $k$-means. All datasets used here are  publicly-available, and the code accompanying our paper can be found at this page: \\url{https://github.com/google-research/google-research/tree/master/hst_clustering}\n\n\\textbf{Datasets.} \nWe used the following well known, real-world datasets from the UCI Repository~\\cite{Dua:2019} that are standard in clustering experiments \\skin~\\cite{skindataset} ($n=245057, d=4$),  \\shuttle~\\cite{Dua:2019} ($n=58000, d=9$), \\covtype~\\cite{blackard1999comparative} ($n=581012, d=54$)\nand \\higgs~\\cite{higgs} ($n=11000000$, $d=28$). Finally, we use a  publicly available synthetic datasets \\synth ($n=5000$, $d=2$)~\\cite{ClusteringDatasets} for visualizing clustering results.\n\n\\textbf{Experimental details.}\nTo simplify the stopping condition of Algorithm~\\ref{alg:makePrivate} we parameterize our algorithm by a depth parameter $\\alpha$ and weight parameter $\\beta$. We grow all of our trees to a max depth of $\\alpha d$ and stop splitting the tree when $w(c) < \\frac{10 \\beta d}{\\epsilon}$ instead of $2 d \\log n/\\epsilon$. This threshold was chosen to decrease the chance of potentially splitting empty cells multiple times and does not affect the privacy properties of the mechanism. The implementation for building the tree embedding was done using C++ in a large-scale distributed infrastructure. The dynamic program for solving the optimization problem in the tree was done in a single machine. \n\n\\paragraph{Non-private baseline}\nWe compare the results of our algorithm against a non-private implementation of $k$-median++~\\cite{arthur2007k} (\\textbf{kmed++}) with 10 iterations of Lloyd's algorithm. Each iteration was done by optimizing the $k$-median objective exactly using Python's BFGS optimizer. \n\n\\paragraph{Private baselines}\nTo the best of our knowledge all private baselines for clustering algorithms have focused on the k-means problem. However, using a private $1$-median algorithm it is possible to adapt some of the prior work to solve the private k-median problem.\n\nAs a first step we implement a subroutine of the $1$-median problem using the objective perturbation framework of \\cite{TPDPCO}. The algorithm described in \\cite{TPDPCO} requires a smooth loss function. We therefore modified the k-median objective to the  $\\frac{1}{\\lambda}$-smooth k-median objective $f_\\lambda \\colon x \\mapsto \\|x\\| + 2 \\lambda \\log \\big((1 + e^{-\\frac{\\|x\\|}{\\lambda}})/2\\big)$ which converges to $\\|x\\|$ as $\\lambda \\mapsto 0$. \nGiven this tool, we implemented the following algorithms.\n\n    $\\bullet$ {\\bf HST }: The MPC version of Algorithm~\\ref{alg:kmed}. After finding the centers using the tree, we ran 4 iterations of the Lloyd algorithm using the private $1$-median implementation described above using at most 20k points from each cluster for the optimization step to allow it to fit in memory. We split the privacy budget $\\epsilon$ uniformly: using $\\epsilon/5$ to build the tree and $\\epsilon/5$ per Lloyd's iteration. We tune the parameters $\\alpha \\in \\{10, 12, 14\\}$ and $\\beta \\in \\{6, 8, 10\\}$. The hyper-parameters for the $1$-median solver were set to $\\lambda = 0.2$ and $\\gamma$ to $0.01*\\sqrt{d}/n$ ($\\gamma$ is a bound on the gradient norm of the optimizer defined in \\cite{TPDPCO}).\n\n$\\bullet$ \\textbf{Private Lloyd}: a private implementation of Lloyd's algorithm. This algorithm has no approximation guarantee. The initial centers are chosen randomly in the space, and at each iteration, each point is assigned to the nearest center, and centers are recomputed using the private 1-median algorithm. We chose the number of iteration to be 7, as a tradeoff between the quality of approximation found and the privacy noise added. Here, the hyper-parameters for the $1$-median solver were $\\lambda = 1$ and $\\gamma = 0.01 \\sqrt{d}/n$.\n\n$\\bullet$ \\textbf{Balcan et al}: the private algorithm of~\\cite{balcan}. The solution computed has a worst case cost of at most $\\log(n)^{3/2} \\cdot \\opt + \\poly(d, k, \\log n)$. We modified the code available online~\\cite{balcanCode} to adapt it to $k$-median, by using our $1$-median implementation with $\\lambda = 1$ and $\\gamma = 0.01 \\sqrt{d}/n$.\n\n$\\bullet$ \\textbf{kvars}: A private instantiation of the kvariates heuristic algorithm of~\\cite{kvariates}. The algorithm uses a sub-routine that splits data into computation nodes. We hash each point using  SimHash \\cite{simhash} to assign them to one of 500 computation nodes.\n \n$\\bullet$  \\textbf{Coreset}\\footnote{\\url{https://ai.googleblog.com/2021/10/practical-differentially-private.html}}: A  heuristic algorithm for private k-means clustering that creates a coreset via  recursive partitioning using locality sensitive hashing. We modified the heuristic to handle k-median with our private 1-median implementation, with $\\lambda = 0.2$ and $\\gamma=0.01\\sqrt{d}/n$.\n\n\n\\paragraph{Other baselines not evaluated}\nWe describe here other potential candidate baselines which  we found not feasible to compare against. \nSince our work focuses on scalability, we do not compare against algorithms with impractically large running times like the algorithm of ~\\cite{StemmerK18, badih_approximation} which have state-of-the-art theoretical approximations but that have not previously been implemented.\\footnote{Private communication with the authors of \\cite{badih_approximation} confirmed that there is no practical implementation of this algorithm available.}\nWe also did not compare against \\cite{anamayclustering}, as it lacks guarantees for $k$-median and the baseline \\cite{balcan} showed comparable performance with their algorithm. Finally, we do not compare with the heuristic GUPT \\cite{gupt} as it does not provide an explicit aggregation procedure for k-median.\n\nFor all algorithms we report the average of $10$ runs. We varied the number of centers, $k$, from $5$ to $40$ and,  $\\epsilon$,  from $0.25$ to $1$.\n\n\\paragraph{Results} We begin by showing a visualization of our algorithm on the \\synth  dataset of 2 dimensions to give intuition on the effect of privacy on constructing the tree embedding. Figure~\\ref{fig:visualization}(a) shows the centers returned by our algorithm for $\\epsilon=0.25$ and $\\epsilon=1$. It is immediate to see that as $\\epsilon$ increases our tree embedding captures the geometry of the dataset correctly.\n\n\n\nWe now discuss the quality of the clusterings returned by each algorithm. We begin evaluating all baselines on the small datasets \\skin and \\shuttle. Figure~\\ref{fig:smallscale} shows the quality of each algorithm for $\\epsilon=0.5$. The plots are normalized by the best clustering objective. There are several points worth noting in this plot. First, the performance of the Balcan et al. algorithm which has the best approximation guarantees is consistently outperformed by our algorithm and the coreset algorithm. Second, notice that on \\skin our algorithm achieves a performance that is essentially the same as the non-private baseline. \n\nFor the large datasets \\covtype and \\higgs, it was impossible for us to run the Balcan et al.~approach. Therefore, we only compare our algorithm against the coreset and kvars baselines. Figure~\\ref{fig:largescale} shows the results. Here we see that  our algorithm has the strongest performances on HIGGS while on \\covtype it is comparable to the coreset heuristic and slightly worse for large $k$. \n\nWe compare only the quality of the solutions computed and not the running time, as the parallel implementation has a large overhead and it never runs really fast. However, our implementation does run and provide apparently good results on large scale datasets on which other private algorithms do not terminate or give really poor results -- to the notable exception of the Coreset algorithm, which does not enjoy theoretical guarantees.\n\n\n\nIn summary, our empirical evaluation confirms that our approach, which is the only method that has both theoretical performance guarantees and can be made to scale to large datasets consistently performs well on a wide variety of examples, achieving accuracy much higher than the worst case analysis would indicate. \n\n\n"
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\nWe present practical and scalable differentially private algorithms for $k$-median with worst case approximation guarantees. Although their worst-case performance is worse than state of the art methods, they are parallelizable, easy to implement in distributed settings, and empirically perform better than any other algorithm with approximation guarantees.\nFurthermore, we present an extension of those algorithms to the $k$-means objective, with a theoretical analysis.\nA natural open question is to  close this gap between theory and practice: finding scalable methods that have even better worst-case guarantees.\n\n\\bibliographystyle{plain}\n\\bibliography{references}\n\n\n\\newpage\n\\appendix\n\n"
            },
            "section 8": {
                "name": "Supplementary Material -- Extension to $k$-Means",
                "content": "\\label{ap:kmeans}\n\nIn this section we prove \\cref{thm:km}. \n\\begin{proof}[Proof of \\cref{thm:km}]\nWe consider the following algorithm: \n\n\\begin{algorithm}\n\\caption{$k$-means algorithm with extra centers}\n\\begin{algorithmic}[1]\n\\State \\textbf{Input:} A set of clients $X$.\\textbf{Output:} A set of $k$-means centers $C$.\n\\State $L \\gets {0}$, $\\eps' \\gets \\eps/\\log n$ \n\\For{$\\log n$ steps} \n \\State $L' \\gets$ Solution computed by $A$ as described by Lemma~\\ref{lem:main:kmeans}, setting $\\pi = \\frac{1}{4\\log n}$, with privacy parameter $\\eps'$.\\footnote{Instead, one could apply $\\log \\log n$ times the algorithm of Lemma~\\ref{lem:main:kmeans} with a constant probability $\\pi$ and take the outcome of the best run. This slightly changes the parameters -- it saves a few $\\log n$ in the approximation -- but for the proof we opted for simplicity rather than performances.} \n    \\State $L \\gets L'$\n\\EndFor\n\\State Return $L$\n\\end{algorithmic}\n\\end{algorithm}\n\nWe argue that the above algorithm produces a solution satisfying the claims\nof Theorem~\\ref{thm:km}. We have that the initial solution has cost at most $n \\Lambda^2$.\nThen, by repeatedly applying Lemma~\\ref{lem:main:kmeans}, we obtain \nsolutions of geometrically decreasing cost. \nMore precisely, after $i$ iterations, we claim that with probability $1 - \\frac{i}{4 \\log n}$, $L$ has size at most $k \\cdot \\sum_{j = 0}^i \\alpha^i$, and  the cost of $L$ is at most  $\\poly(d, \\log n, 1/\\alpha)\\cdot \\opt + \\alpha^i n \\Lambda + kd^2 \\log^2 n /\\eps' \\cdot \\Lambda^2$ . This is true when $i = 0$, and follows directly from applying Lemma~\\ref{lem:main:kmeans}.\n\nIt follows that the final solution computed after $\\log n$ steps \nhas cost at most $\\poly(d, \\log n)$ times $\\opt$ plus additive\n$kd^2 \\log^3 n /\\eps \\cdot \\Lambda^2$. \nMoreover, since $\\eps' = \\eps / \\log n$, the algorithm is by composition $\\eps$-DP.\nFinally, the number of centers is at most \n$k(1+\\alpha)$ as desired.\n\\end{proof}\n\n\nHence, the key is to prove \\cref{lem:main:kmeans}. Before describing the ideas behind the extension to $k$-means, we introduce some notations.\nWe say that two points $p, q$ are \\emph{cut at level $i$} when their lowest common ancestor\nin the tree is at level in $(d\\cdot (i-1), d \\cdot i]$, i.e., the diameter of that common ancestor is in $(\\sqrt d \\cdot 2^{i-1}, \\sqrt{d} \\cdot 2^i]$. In that case, the \ndistance in the quadtree metric between $p$ and $q$ is at most $\\sqrt d 2^i$.\nWe say that a ball $B(p, r)$ is cut at level $i$ if $i$ is the largest integer such that\nthere exists a point $q$ with $\\dist(p, q) \\leq r$ and $p$ and $q$ are cut at level $i$.\n\nRecall \\cref{lem:ballCut}: \n  For any $i$, radius $r$ and point $p$, it holds that $\\Pr[B(p, r) \\text{ is cut at level } i] = O\\left(\\frac{d r}{2^i}\\right)$.\n\n\\paragraph{Formalization} Let $P \\subseteq \\R^d$ be an instance of the \n$k$-means problem in $\\R^d$.\nLet $\\globalS$ be an optimal solution to $P$ and $L$ be an arbitrary solution. \nFor a given client $c$, we let\n$L(c)$ (resp. $\\globalS(c)$) denote the center of $L$ (resp. $\\globalS$) \nthat is the closest to $c$ in solutions\n$L$ (resp. $\\globalS$).\n\nFor a quadtree decomposition $\\calD$, we say that a client $c$ is \\emph{badly-cut} if the ball $B(c, \\dist(c,$ $\\opt))$ is cut at a level higher than $\\log (\\dist(c,$ $\\opt) \\cdot d/\\badcut)$ -- not that this is for the analysis only, since we don't know this algorithmically.\nWe say that a center $f \\in L$ is \\emph{badly-cut} if for some $i$, the ball $B(f, 2^i)$ is cut at a level higher than $i+ \\log (d \\log n/\\badcutF)$.  \nAs $L$ and $\\calD$ will be fixed all along that section, we simply say that a point or a center is badly-cut.\nNotice that we do not know which clients are badly-cut. It is however possible to compute the badly-cut centers, since it depends only on $L$ and $\\calD$. It is explained how to perform this step in time $\\tilde O(nd)$ in \\cite{CohenAddadFS19}.\n\n\n\nOur algorithm computes a randomized quadtree $\\calD$, and finds the badly-cut center $\\calB_\\calD$.\nIt removes from the input each cluster associated with a center of $\\calB_\\calD$. Let $P_\\calD$ be the remaining points.\n $P_\\calD$ is a random variable that depends on the randomness of~$\\calD$.\nGiven a solution $S$ for $k$-means on $P_\\calD$, the algorithm's output is $S \\cup \\calB_\\calD$.\n\nWe call $\\cost(P, S)$ the cost of any  solution $S$ in the original input \n$P$, and $\\cost(P_\\calD, S)$ its cost in $P_\\calD$. \n\n\nA key property for our analysis is a bound on the probability of being badly-cut.\n\\begin{lemma}\\label{lem:badcut}\n  Any client $p$ has probability at most $\\badcut$ to be badly-cut.\n  Similarly, a center $f \\in L$ has probability at most $\\badcutF$ to be badly-cut.\n\\end{lemma}\n\\begin{proof}\n  Consider first a point $p \\in P$. By \\cref{lem:ballCut}, the \nprobability that a ball $B(p, r)$ is cut at level at least $j$ is \nat most $d r /2^j$. Hence the probability that a \nball $B(p,\\dist(p, \\opt))$\nis cut at a level $j$ greater than $\\log (\\dist(p, \\opt)) + \\log (d/\\badcut)$ is at most $\\badcut$.\nThe proof for $f \\in F$ is identical.\n\\end{proof}\n\nUsing that lemma, one can bound the cost of the clusters of facilities from $\\calB_\\calD$, as well as the cost of badly-cut clients:\n\\begin{lemma}\\label{lem:boundBC}\nFor any $\\pi \\in (0, 1)$, it holds with probability $1 - \\pi$ that:\n\\begin{align*}\n  \\sum_{f \\in \\calB_\\calD} \\sum_{p : L(p) = f} \\cost(p, f) \\leq 3/\\pi \\cdot \\badcutF \\cost(P, L), \\\\\n  \\sum_{p \\bcc} \\cost(p, L) \\leq 3/\\pi \\cdot \\badcut \\cost(P, L)\n  \\ \\text{and} \\\n  |\\calB_\\calD| \\leq 3/\\pi \\cdot  \\badcutF |L|\n  \\end{align*}\n\\end{lemma}\n\\begin{proof}\nUsing \\cref{lem:badcut}, we have \n$$\\E[\\sum_{f, p :  L(p) = f} \\cost(p, f)] = \\sum_{p \\in P} \\Pr[L(p) \\in \\calB_\\calD] \\cost(p, L) \\leq \\badcutF \\cost(P, L).$$\nUsing Markov's inequality, with probability $1-\\pi/3$ the first bullet of the lemma holds. For the same reason, the second bullet holds with probability $1-\\pi/3$ as well. Similarly,\n$\\E[|\\calB_\\calD|] = \\sum_{f \\in L} \\Pr[f \\in \\calB_\\calD] = \\badcutF |L|,$ so applying again Markov's inequality gives that the third bullet holds with probability $1-\\pi/3$. A union-bound concludes the proof.\n\\end{proof}\n\n\n\\begin{lemma}\\label{lem:goodInstance}\nWhen $\\badcutF = \\frac{\\pi \\alpha}{6}$, $\\badcut = \\frac{\\alpha^3 \\pi^3}{144 d^3 \\log^2 n}$ and \\cref{lem:boundBC} holds:\n$$\\cost_\\calD(P_\\calD, \\opt) \\leq \\frac{\\alpha}{2} \\cost(P, L) +  \\frac{O(d^9 \\log^4 n)}{\\alpha^6 \\pi^6} \\cdot \\cost(P, \\opt).$$\n\\end{lemma}\n\\begin{proof}\nWe start by showing different bounds for $\\cost_\\calD(c, \\opt)$, according to whether $c$ is badly-cut or not.\n\nWhen a client $p$ is not badly-cut, we directly have that:\n$\\dist_\\calD(p, \\opt) \\leq \\frac{d \\sqrt d\\cdot \\dist(p, \\opt)}{\\badcut}$, since the lowest common ancestor of $p$ and $\\opt(p)$ has diameter at most $\\frac{d \\sqrt d\\cdot \\dist(p, \\opt)}{\\badcut}$.{\\setlength{\\emergencystretch}{2.5em}\\par}\n\n\nIn the case where $p \\in P_\\calD$ is badly-cut, we proceed differently. We use that $L(p)$ is not badly-cut as follows.  \nBoth $p$ and $\\opt(p)$ are contained in the ball $B(L(p), \\dist(p, L) + \\dist(p, \\opt))$, since $\\dist(L(p), \\opt) \\leq \\dist(p, L) + \\dist(p, \\opt)$. \nLet $i = \\lceil \\log(\\dist(p, L) + \\dist(p, \\opt))\\rceil$. Since $L(p)$ is not badly-cut, the ball $B(L(p), 2^{i})$ contains $p$ and $\\opt(p)$ and is cut at level at most $i + \\log(d \\log n/ \\badcutF)$. \nHence, $\\dist_\\calD(p, \\opt) \\leq \\frac{d^{3/2} \\log n}{\\badcutF} 2^{i} \\leq \\frac{2 d^{3/2} \\log n}{\\badcutF} \\cdot \\left(\\dist(p, L) + \\dist(p, \\opt)\\right)$.{\\setlength{\\emergencystretch}{2.5em}\\par}\n\n\nSince $\\cost_\\calD(p, \\opt) = \\dist_\\calD(p, \\opt)^2$, this implies that \n\n$$\\cost_\\calD(p, \\opt) \\leq \\frac{2 d^3 \\log^2 n}{\\badcutF^2} \\cdot \\left(2\\cost(p, L) + 2\\cost(p, \\opt)\\right) $$\n$$\\leq \\frac{4 d^3 \\log^2 n}{\\badcutF^2} \\cdot \\left(\\cost(p, L) + \\cost(p, \\opt)\\right)$$\n\nHence, we have that:\n\\begin{flalign*}\n    &\\cost_\\calD(P_\\calD, \\opt) =\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\sum_{p \\in P_\\calD: p \\bcc}\\!\\!\\!\\!\\!\\!\\!\\!\\! \\cost_\\calD(p, \\opt) \n+ \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{p \\in P_\\calD:  p \\text{ not badly-cut}} \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\cost_\\calD(p, \\opt)&\\\\\n    &\\leq \\sum_{p \\in P_\\calD: \\text{ badly-cut}} \\frac{4 d^3 \\log^2 n}{\\badcutF^2} \\cdot (\\cost(p, \\opt) + \\cost(p, L)) \\\\\n&\\phantom{xxx} + \\sum_{p \\in P_\\calD:  p \\text{ not badly-cut}} \\frac{d^3 \\cost(p, \\opt)}{\\badcut^2}\\\\\n    \\leq& \\left(\\frac{4 d^3 \\log^2 n}{\\badcutF^2} + \\frac{d^3}{\\badcut^2}\\right)\\cdot \\cost(P, \\opt) + \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{p \\in P: \\text{ badly-cut}} \\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{4 d^3 \\log^2 n}{\\badcutF^2} \\cdot \\cost(p, L)\n\\end{flalign*}\n\nUsing now \\cref{lem:boundBC}, we get: \n\\begin{flalign*}\n    &\\cost_\\calD(P_\\calD, \\opt \\cup \\calB_\\calD)  \\leq \\left(\\frac{4 d^3 \\log^2 n}{\\badcutF^2} + \\frac{d^3}{\\badcut^2}\\right)\\cdot \\cost(P, \\opt)+& \\\\\n    &\\frac{4 d^3 \\log^2 n}{\\badcutF^2} \\cdot \\frac{3\\badcut}{\\pi} \\cost(P, L)\n    \\leq  \\frac{\\alpha}{2} \\cost(P, L) +  \\frac{O(d^9 \\log^4) n}{\\alpha^6 \\pi^6}  \\cdot \\cost(P, \\opt).\n\\end{flalign*}\n\n\\end{proof}\n\nThat lemma shows that it is enough to compute the optimal solution on $P_\\calD$, and add to it the centers of $\\calB_\\calD$ which can be done by an algorithm similar to the one for $k$-Median:\n\\begin{algorithm}\n\\caption{DP-kMeans($P, L$)}\\label{alg:kmeans}\n\\begin{algorithmic}[1]\n\\State Compute a shifted quadtree $\\calD$. Let $r$ be the root of $\\calD$.\n\\State Compute the instance $P_\\calD$, $w = $ MakePrivate($\\calD, P_\\calD$). \n\\State Compute $s = $DynamicProgram-KMeans($\\calD, w, k, r$)\n\\State Use the dynamic program table to find a solution $S$ with cost $s$\n\\State \\textbf{Return} $S \\cup \\calB_\\calD$\n\\end{algorithmic}\n\\end{algorithm}\n\nThe algorithm DynamicProgram-KMeans is exactly the same as DynamicProgram-KMedian \\cref{alg:dynKMed}, except that it returns $w(c) \\cdot \\diam(c)^2$ at step 3, to fit the $k$-means cost. We can now turn to the proof of \\cref{lem:main:kmeans}, to show the guarantees ensured by this algorithm.\n\n\n\\begin{proof}[Proof of \\cref{lem:main:kmeans}]\nWe start by showing the quality of approximation. As for $k$-median, the solution $S$ computed at step 5 of \\cref{alg:kmeans} is optimal for $P_\\calB$ in the quadtree metric with the additional noise. Hence, its cost verifies $\\cost(P_\\calD, S) \\leq \\cost_\\calD(P_\\calD, S) \\leq \\cost_\\calD(P_\\calD, \\opt) + \\frac{k d^2 \\log^2 n}{\\eps} \\cdot \\Lambda^2$. \n\nNow, with probability $1-\\pi$ \\cref{lem:boundBC} holds. \nIn that case, using \\cref{lem:goodInstance}, the cost of $P_\\calD$ is at most \n$$\\cost(P_\\calD, S) \\leq \\frac{\\alpha}{2} \\cdot \\cost(P, L) + \\frac{O(d^9 \\log^4 n)}{\\alpha^6 \\pi^6} \\cdot \\cost(P, \\opt)  + \\frac{k d^2 \\log^2 n}{\\eps} \\cdot \\Lambda^2.$$\n\nMoreover, \\cref{lem:boundBC} ensures that $\\sum_{f \\in \\calB_\\calD} \\sum_{c : L(c) = f} \\cost(c, f) \\leq \\frac{\\alpha}{2} \\cost(P, L)$. Hence, combining those bounds concludes the lemma.\n\nWe now turn to the privacy guarantee. $\\calD$ is computed oblivious to the data. Hence, when $P$ changes by one point, $P_\\calD$ changes by at most one point as well -- depending whether this point is served by a badly-cut center in $L$. As for $k$-median, Step 3 of the algorithm therefore ensures that the solution computed at step 5 is $\\eps$-DP. \n\\end{proof}\n\n\\paragraph{Going from $(1+\\alpha)k$ centers to $k$}\nIn this last section, we show how to get a true solution to $k$-means, removing the extra $\\alpha k$ centers.\n\nFor that, we can use the reverse greedy algorithm of Chrobak et al.~\\cite{ChrobakKY06}.\nThis algorithm starts from a set of centers, and iteratively removes the one leading to the smallest cost increase, until there are $k$ centers remaining.\nIt can be implemented in a private manner as follows:\nlet $S$ be a set of $O(k)$ centers computed privately. For any center $s$ of $S$, let $w(s)$ be the size of $S$'s cluster, plus a Laplace noise $\\laplacenoise(1/\\eps)$.\nLet $P_S$ be the resulting instance. Informally, any solution on $P_S$ induces a solution with similar cost on $P$, with an additive error $\\pm~\\cost(P, S) + k\\cdot \\Lambda^2 / \\eps$ -- see \\cref{lem:reverseApprox}. Further, since $S$ is private, $P_S$ is private as well -- see \\cref{lem:reversePrivate}.\n\nOn the weighted instance $P_S$, the reverse greedy algorithm finds a solution with $k$ centers that is an $O(\\log k)$-approximation of the optimal solution $\\mathcal A$ for that instance, using that $P_S$ contains $O(k)$ distinct points. This is Theorem 2.2 in \\cite{ChrobakKY06}.\n\nNow, the optimal solution  $\\calA$ on $P_S$ has cost in $P$ at most $\\opt + \\cost(P, S) + k \\Lambda^2 / \\eps$, by \\cref{lem:reverseApprox}. Hence, combined with \\cref{thm:km}, $\\mathcal{A}$ has cost $\\cost(P, \\mathcal{A}) \\leq \\poly(d, \\log n, 1/\\alpha) \\cdot \\opt + k d^2 \\log^2 n \\log k/ \\eps^2 \\Lambda^2$. The solution computed by the reversed greedy has therefore cost at most $\\poly(d, \\log n, 1/\\alpha) \\cdot \\opt + k d^2 \\log^2 n \\log^2 k/ \\eps^2 \\Lambda^2$. {\\setlength{\\emergencystretch}{2.5em}\\par}\n\n\nBefore formalizing the argument, we show the two crucial lemmas.\n\\begin{lemma}\\label{lem:reversePrivate}\nIf solution $S$ is computed via an $\\eps$-DP algorithm, then the algorithm computing $P_S$ is $2\\eps$-DP.\n\\end{lemma}\n\\begin{proof}\nFix some solution $S$. By properties of the Laplace mechanism (see \\ref{sec:prelim}), for any center $s$ of $S$ the value of $w(s)$ is computed  on $s$'s cluster is $\\eps$-DP. Since all clusters are disjoint, the instance $P_S$ is computed in an $\\eps$-DP way. \n\nNow, $S$ is not fixed but given privately to the algorithm. By composition, the algorithm that computes $P_S$ is $2\\eps$-DP.\n\\end{proof}\n\n\\begin{lemma}\\label{lem:reverseApprox}\nLet $S$ be any solution, and $P_S$ computed as described previously. With high probability, for any set of $k$ centers $T$, $|\\cost(P, T) - \\cost(P_S,T)| \\leq \\frac{1}{2} \\cdot \\cost(P_S, T) + 10 \\cost(P, S) + k \\log n \\Lambda^2 / \\eps$. \n\\end{lemma}\n\\begin{proof}\nWithout the addition of a Laplace noise, the cost difference between the two solution can be bounded using the following generalization of the triangle inequality (see Lemma 1 in Cohen-Addad et al.~\\cite{stoc21}): for any $\\eps >0$, any points $p,s$ and set $T$, \n\\[|\\cost(p, T) - \\cost(s, T)| \\leq \\eps \\cost(s, T) + \\frac{4+\\eps}{\\eps} \\cost(p, s).\\]\n\n\n\n\nFor any point $p \\in P$ served by some center $s \\in S$, we can apply this inequality with $\\eps = 1/2$ to get:\n\\[|\\cost(p, T) - \\cost(s, T)| \\leq\\frac{1}{2}\\cdot \\cost(s, T) + 10 \\cost(p, s).\\]\n\n\nMoreover, w.h.p the total noise added is smaller than $k \\log n / \\eps$, hence contributes at most $k \\log n \\Lambda^2 / \\eps$ to the cost. Summing over all $p$ concludes the proof.\n\\end{proof}\n\n\n\\begin{lemma}\nLet $\\calS$ be the solution computed by \\cref{thm:km}, and $P_\\calS$ the instance computed as described previously.\nApplying the reverse greedy algorithm on instance $P_\\calS$ is $2\\eps$-DP and yields a solution with cost at most $\\poly(d, \\log n, 1/\\alpha) \\cdot \\opt + k d^2 \\log^2 n \\log^2 k/ \\eps^2 \\Lambda^2$ \n\\end{lemma}\n\\begin{proof}\nFirst, the algorithm is $2\\eps$-DP, as shown by \\cref{lem:reversePrivate}.\n\nSecond, let $\\calA$ be the optimal solution on the instance $P_\\calS$, and $\\opt$ be the optimal cost for the full set of points $P$. Applying \\cref{lem:reverseApprox}, we get:\n\\begin{align*}\n\\cost(P_\\calS, \\calA) &\\leq \\cost(P_\\calS, \\opt) \\\\\n&\\leq 2 \\cost(P, \\opt) + 20 \\cost(P, \\calS) + 2k \\log n \\Lambda^2 / \\eps\\\\\n&= \\poly(d, \\log n, 1/\\alpha) \\cdot \\opt + k d^2 \\log^2 n \\log k/ \\eps^2 \\Lambda^2.\n\\end{align*}\n\nWe can now bound the cost of the solution computed by the reverse greedy algorithm. As $P_\\calS$ is made of $O(k)$ many distinct points, the reverse greedy computes a solution $\\tilde \\calS$ with cost at most $\\cost(P_\\calS, \\tilde \\calS) = O(\\log k) \\cost(P_S, \\calA)$.\nApplying again \\cref{lem:reverseApprox}, we get\n\\begin{align*}\n\\cost(P, \\tilde \\calS) &\\leq \\frac{3}{2}\\cdot \\cost(P_S, \\tilde \\calS) + 10 \\cost(P, \\calS) + k \\log n \\Lambda^2 / \\eps\\\\\n&= \\poly(d, \\log n, 1/\\alpha) \\cdot \\opt + k d^2 \\log^2 n \\log k/ \\eps^2 \\Lambda^2,\n\\end{align*}\nwhich concludes the proof.\n\\end{proof}\n\n\n"
            }
        },
        "tables": {
            "lem:makePrivateDP": "\\begin{restatable}{lemma}{makeprivateDP}\n\\label{lem:makePrivateDP}\nAlgorithm~\\ref{alg:makePrivate} is $\\eps$-DP.\n\\end{restatable}",
            "thm:km": "\\begin{restatable}{theorem}{km}\n\\label{thm:km}\nThere exists an $\\eps$-DP algorithm $A$ that takes as input\n  a set of points and computes a solution for $k$-means with at most $(1+\\alpha)k$ centers and, with probability $3/4$, costs at most $\\poly(d, \\log n, 1/\\alpha) \\cdot \\opt + k d^2 \\log^2 n / \\eps \\cdot \\Lambda^2$.\n\\end{restatable}",
            "lem:main:kmeans": "\\begin{restatable}{lemma}{mainkmeans}\n\\label{lem:main:kmeans}\n  Given an arbitrary solution $L$, there exist an $\\eps$-DP algorithm $A$ that takes as input a set of points and computes a solution for $k$-means with at most $k + \\frac{\\alpha}{2}\\cdot|L|$ centers and, with probability $1-\\pi$, costs at most $\\frac{O(d^9 \\log^2 n)}{\\alpha^6 \\pi^6} \\cdot \\opt + \\alpha \\cdot \\cost(L) + k d^2 \\log^2 n / \\eps \\cdot \\Lambda^2$.\n\\end{restatable}"
        },
        "figures": {
            "fig:visualization": "\\begin{figure}[t]\n\\centering \n\\begin{tabular}{cc}\n\\includegraphics[width=.44\\linewidth]{visualization_eps_0.25.pdf}\n&\\includegraphics[width=.44 \\linewidth]{visualization_eps_1.pdf}\\\\\n\\vspace{-0.1in}\n(a) & (b)  \n\\end{tabular}\n\\caption{Visualization of our algorithm. Original dataset in green. Leaves of the tree scaled by their weight in blue and centers found by our algorithm in red. (a) $\\epsilon = 0.25$ and (b) $\\epsilon = 1.0$.}\n\\label{fig:visualization}\n\\end{figure}",
            "fig:smallscale": "\\begin{figure}[t!]\n\\centering \n\\begin{tabular}{cc}\n\\includegraphics[width=.44 \\linewidth]{skin-e-0.50.pdf} \n&\\includegraphics[width=.44 \\linewidth]{shuttle-e-0.50.pdf}\\\\\n\\vspace{-0.1in}\n(a) & (b) \n\\end{tabular}\n\\caption{Comparison of algorithms on (a) \\skin and (b) \\shuttle}\n\\label{fig:smallscale}\n\\end{figure}",
            "fig:largescale": "\\begin{figure}[t] \n\\centering\n\\begin{tabular}{cc}\n\\includegraphics[width=0.44\\linewidth]{covtype-e-0.50.pdf} & \n\\includegraphics[width=0.44\\linewidth]{HIGGS-e-0.50.pdf}\\\\\n\\vspace{-0.1in}\n(a) & (b)\n\\end{tabular}\n\\caption{Objective function as a function of $k$ datasets (a) \\covtype and (b) \\higgs.}\n\\label{fig:largescale}\n\\end{figure}"
        }
    }
}