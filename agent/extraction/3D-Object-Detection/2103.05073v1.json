{
    "meta_info": {
        "title": "Offboard 3D Object Detection from Point Cloud Sequences",
        "abstract": "While current 3D object recognition research mostly focuses on the real-time,\nonboard scenario, there are many offboard use cases of perception that are\nlargely under-explored, such as using machines to automatically generate\nhigh-quality 3D labels. Existing 3D object detectors fail to satisfy the\nhigh-quality requirement for offboard uses due to the limited input and speed\nconstraints. In this paper, we propose a novel offboard 3D object detection\npipeline using point cloud sequence data. Observing that different frames\ncapture complementary views of objects, we design the offboard detector to make\nuse of the temporal points through both multi-frame object detection and novel\nobject-centric refinement models. Evaluated on the Waymo Open Dataset, our\npipeline named 3D Auto Labeling shows significant gains compared to the\nstate-of-the-art onboard detectors and our offboard baselines. Its performance\nis even on par with human labels verified through a human label study. Further\nexperiments demonstrate the application of auto labels for semi-supervised\nlearning and provide extensive analysis to validate various design choices.",
        "author": "Charles R. Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, Dragomir Anguelov",
        "link": "http://arxiv.org/abs/2103.05073v1",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "18 pages, 7 figures, 19 tables"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n% \\todo{1st version ddl: 10/31 6PM; 2nd version ddl: 11/3 6PM; Owners: rqi, yinzhou}\n\n\n\n\nRecent years have seen a rapid progress of 3D object recognition with advances in 3D deep learning and strong application demands. However, most 3D perception research has been focusing on real-time, onboard use cases and only considers sensor input from the current frame or a few history frames. Those models are sub-optimal for many \\emph{offboard} use cases where the best perception quality is needed.\n% while constraints on input and computation are much more relaxed.\n% Among the offboard use cases, one important direction is to have machines ``auto label'' the data to save the high cost of human labeling. High quality perception can also be used for simulation or to build datasets to supervise or evaluate downstream modules such as behavior prediction and trajectory planning.\nAmong them, one important direction is to have machines ``auto label'' the data to save the cost of human labeling. High quality perception can also be used for simulation or to build datasets to supervise or evaluate downstream modules such as behavior prediction.% and trajectory planning.\n\n% Directly using the current 3D object detectors for the quality sensitive offboard use cases are not ideal as shown in the Fig.~\\ref{fig:teaser}. While the existing detector get reasonable average precision (AP) at the common standard (IoU larger than 70\\%), the AP quickly drops at higher standard (e.g. IoU threshold of 80\\%).\n\nIn this paper, we propose a novel pipeline for offboard 3D object detection with a modular design and a series of tailored deep network models. The offboard pipeline makes use of the whole sensor sequence input (such video data is common in applications of autonomous driving and augmented reality). With no constraints on the model causality and little constraint on model inference speed, we are able to greatly expand the design space of 3D object detectors and achieve significantly higher performance. %Due to its impressive results, we name our pipeline \\emph{3D Auto Labeling}.\n\nWe design our offboard 3D detector based on a key observation: different viewpoints of an object, within a point cloud sequence, contain complementary information about its geometry (Fig.~\\ref{fig:point_aggregation}). An immediate baseline design is to extend the current detectors to use multi-frame inputs. \nHowever, as multi-frame detectors are effective they are still limited in the amount of context they can use and are not naively scalable to more frames – gains from adding more frames diminish quickly (Table~\\ref{tab:detection_AP_vs_frames}).\n\nIn order to fully utilize temporal point clouds (\\eg 10 or more seconds), we step away from the common frame-based input structure where the entire frames of point clouds are merged. Instead, we turn to an \\emph{object-centric} design. We first leverage a top-performing multi-frame detector to give us initial object localization. Then, we link objects detected at different frames through multi-object tracking. Based on the tracked boxes and the raw point cloud sequences, we can extract the entire track data of an object, including all of its sensor data (point clouds) and detector boxes, which is 4D: 3D spatial plus 1D temporal. We then propose novel deep network models to process such 4D object track data and output temporally consistent and high-quality boxes of the object. As they are similar to how a human labels an object and because of their high-quality output, we call those models processing the 4D track data as ``object-centric auto labeling models'' and the entire pipeline ``3D Auto Labeling'' (Fig.~\\ref{fig:pipeline}).\n\nWe evaluate our proposed models on the Waymo Open Dataset (WOD)~\\cite{sun2020scalability} which is a large-scale autonomous driving benchmark containing 1,000+ Lidar scan sequences with 3D annotations for every frame. Our 3D Auto Labeling pipeline dramatically lifts the perception quality compared to existing 3D detectors designed for the real-time, onboard use cases (Fig.~\\ref{fig:teaser} and Sec.~\\ref{sec:exp:sota}). The gains are even more significant at higher standards.\n% The pipeline also further improves upon our multi-frame detector baseline, showing the effectiveness of the newly proposed module for object-centric object auto labeling. %\\todo{give concrete nubmers.}\n% As one of our goals is to automatically generate high-quality labels, a natural question is that: How far are we from expert human performance in 3D object detection? To answer the question we have conducted a human label study (Sec.~\\ref{sec:exp:human}).\n% To understand how far we are from expert human performance in 3D object detection, we have conducted a human label study (Sec.~\\ref{sec:exp:human}).\n% We asked several experienced 3D annotators to re-label several runs from the WOD and compared the statistics derived from human labels v.s. those from auto labels (output from our pipeline).\nTo understand how far we are from human performance in 3D object detection, we have conducted a human label study to compare auto labels with human labels (Sec.~\\ref{sec:exp:human}). To our delight, we found that auto labels are already on par or even slightly better compared to human labels on the selected test segments.\n\n% In Sec.~\\ref{sec:exp:semi_supervised}, we investigate the applicability and quality of our auto labels by training student models in the semi-supervised manner.\nIn Sec.~\\ref{sec:exp:semi_supervised}, we demonstrate the application of our pipeline for semi-supervised learning and show significantly improved student models trained with auto labels. We also conduct extensive ablation and analysis experiments to validate our design choices in Sec.~\\ref{sec:exp:multi_frame_detector} and Sec.~\\ref{sec:exp:analysis} and provide visualization results in Sec.~\\ref{sec:exp:visualization}.\n% shows the benefits of the offboard model compared to single-frame detectors and hints remaining places for improvements.\n\n\nIn summary, the contributions of our work are:\n\\begin{itemize}\n    \\item Formulation of the offboard 3D object detection problem and proposal of a specific pipeline (3D Auto Labeling) that leverages our multi-frame detector and novel object-centric auto labeling models.\n    \\item State-of-the-art 3D object detection performance on the challenging Waymo Open Dataset.\n    \\item The human label study on 3D object detection with comparisons between human and auto labels.\n    \\item Demonstrated the effectiveness of auto labels for semi-supervised learning.\n\\end{itemize}\n\n% As a result, current 3D object detector models are not able to fully exploit the available input, which are often video data of camera views and point clouds. Those models are therefore sub-optimal for many offboard use cases of 3D perception. \n\n% While we have seen a rapid progress of 3D object recognition thanks to advances in 3D deep learning, public datasets and strong demands from industry, the research community has been mainly focusing on the real-time, onboard use cases which only uses sensor input from the current moment or very few history frames. \n\n% The story:\n% \\begin{itemize}\n%     \\item While the current 3D perception research focus on onboard/real-time use cases, there are also many offboard use cases of perception (3D object detection) in robotics, autonomous driving and AR. For example, in autonomous driving: semi-supervised learning, labeling assistance, simulation, BP, planner evaluation.\n%     \\item However, the current 3D object detectors focus on the real-time onboard use cases and cannot satisfy the high precision requirements. See the Fig.~\\ref{fig:teaser}.\n%     \\item In this paper, we propose the \\emph{offboard} 3D object detection problem. Given all sensor data, very loose constraint on compute and non-causality (being able to see the data in the future), how much better can the 3D detector become?\n%     \\item We design offboard 3d detectors based on a key observation (Fig.~\\ref{fig:point_aggregation}: There is complementary information of objects from different frames in a point cloud video, as we can see the objects from different viewpoints. A baseline design is to extend the current detectors to multi-frame. While multi-frame detectors are effective they are still limited in how much contexts it can use -- they are not naively scalable to more frames -- diminishing returns after 5 frames.\n%     \\item In order to make use of longer contexts, to fully utilize the point cloud video, we proposed the \\emph{object-centric} track refinement. We first leverages the best multi-frame detector to give us initial localizations of objects. Then, we link objects detected at different frames through multi-object tracking. Next, the key step is that we collect all sensor data (point clouds) and detector boxes from each object, which is the 4D data of the object (3D spatial plus 1D temporal). We feed such 4D object data to a novel deep net to output refined, consistent and high-quality boxes of the object. As it is similar to how human labels an object and because of its high quality output, we call this pipeline \"3D Auto Labeling\" (Fig.~\\ref{fig:pipeline}).\n%     \\item A related question: What is the “upper-bound”/realistic goal for the offboard detection? To answer the question we conduct the human label study to give us the \"north star\". Due to the partiality of point cloud data, even humans may disagree in labeling. Having the human label study sets the right expectation for our modeling. We also compared our final model with the human labels. \\rqi{Shall we introduce the human label study earlier?}\n%     \\item To show the usefulness of the auto labels, we follow the semi-supervised learning set up to use the auto labels to train a student model and show that we can get significantly improved student performance in various scenarios.\n% \\end{itemize}\n\n% In summary, the contributions of our work are (\\rqi{to be expanded and elaborated.}):\n% \\begin{itemize}\n%     \\item Proposal of the offboard 3D detection problem.\n%     \\item The pipeline design of the 3D auto labeling.\n%     \\item The object-centric refinement algorithms that are able to make use of the full object information.\n%     \\item The human label study on 3D object detection.\n%     \\item Demonstrated use of auto labels for semi-supervised learning.\n% \\end{itemize}\n\n\n\n\n% \\begin{figure*}[h]\n%     \\centering\n%     \\includegraphics[width=0.9\\linewidth]{./fig/point_aggregation}\n%     \\caption{\\todo{1st version ddl: 10/29 11PM; 2nd version ddl: 11/1 6PM; Owners: rqi} Visualization of object points from the point cloud video. Top: for static objects; Right: for dynamic objects \\rqi{TBD}. Points are transformed to the world coordinate (removed the sensor ego-motion). For static objects, aggregated points make the viewpoint more complete thus enable accurate geometry estimation. For moving objects, the trajectory provide strong cues for the object heading. \\todo{make the figures bigger and remove the bounding boxes?}}\n%     \\label{fig:point_aggregation}\n% \\end{figure*}\n\n\n\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n% \\todo{1st version ddl: 10/30 1PM; 2nd version ddl: 10/31 1PM; Owners: rqi, yinzhou, najibi?}\n\n% \\paragraph{3D object detection.}\n\n% MV3D~\\cite{cvpr17chen}\n% Frustum Pointnet~\\cite{qi2018frustum}\n% ImVoteNet~\\cite{qi2020imvotenet}\n\n\\paragraph{3D object detection} Most work has been focusing on using single-frame input. In terms of the representations used, they can be categorized into voxel-based~\\cite{REF:VotingforVoting_RSS2015,REF:Vote3Deep_ICRA2017,REF:3DFCN_RSJ2017,song2016deep,REF:ku2018joint,REF:yang2018pixor,simony2018complex,zhou2018voxelnet,REF:second_2018,lang2019pointpillars,REF:HVNet2020,REF:PillarNet_ECCV2020}, point-based~\\cite{shi2018pointrcnn,REF:yang2018ipod,REF:StarNet_2019,qi2019deep,yang20203dssd,REF:Point-GNN_CVPR2020}, perspective-view-based~\\cite{REF:VeloFCN2016,REF:lasernet_CVPR2019,REF:bewley2020range} as well as hybrid strategy~\\cite{zhou2020end,REF:STD_ICCV2019,REF:FastPointRCNN_Jiaya_ICCV2019,REF:SA_SSD_He_2020_CVPR,shi2020pv}.\n% In this work, we employ one hybrid method, ~\\cite{zhou2020end}, for its promising performance on public benchmarks and efficient implementation.\nSeveral recent works explored temporal aggregation of Lidar scans for point cloud densification and shape completion. \\cite{REF:FaF_Luo_2018_CVPR} fuses multi-frame information by concatenating feature maps from different frames. \\cite{REF:you_see_Hu_2020_CVPR} aggregates (motion-compensated) points from different Lidar sweeps into a single scene. \\cite{REF:Spatiotemporal_transformer_CVPR2020} uses graph-based spatiotemporal feature encoding to enable message passing among different frames. \\cite{REF:ConvLSTM_ECCV2020} encodes previous frames with a LSTM to assist detection in the current frame.\nUsing multi-modal input (camera views and 3D point clouds)~\\cite{REF:ku2018joint,cvpr17chen,qi2018frustum,REF:pointfusion_CVPR2018,REF:ContFuse_ECCV2018,REF:Multi_task_multisensor_fusion_CVPR2019,REF:LaserNet++_CVPRW2019,REF:MVX-Net_ICRA2019,qi2020imvotenet} has shown improved 3D detection performance compared to point-cloud-only methods, especially for small and far-away objects.\n% However the need for an additional camera can yield diminishing returns when LiDAR quality can be compensated from multiple scans or improved by deploying higher-quality sensors.\nIn this work, we focus on a point-cloud-only solution and on leveraging data over a long temporal interval. \n\n% \\subsection{3D Ojbect Detection}\n% % \\yinzhou{First pass tries to be comprehensive. May shrink depending on the paper space.}\n% \\paragraph{3D Detection in Single-frame Point Clouds} \n% Currently, there are mainly three streams of methods in processing the point clouds, voxel-based, point-based as well as the hybrid strategy. On the voxel-based 3D detection research, traditional approaches rasterize point clouds into a 3D voxel grid and encode each voxel with handcrafted features~\\cite{REF:VotingforVoting_RSS2015,REF:Vote3Deep_ICRA2017,REF:3DFCN_RSJ2017,song2016deep,REF:ku2018joint,REF:yang2018pixor,REF:simony2018complex}. Along this line, modern approaches~\\cite{zhou2018voxelnet,REF:second_2018,lang2019pointpillars,REF:HVNet2020,REF:PillarNet_ECCV2020} employ PointNet-like network~\\cite{qi2017pointnet} to encode the points feature inside each voxel and build on the advancements from 2D images~\\cite{he2016deep,lin2017focal,REF:tian2019fcos}. Perspective view is another widely used representation for LiDAR. Some representative works are~\\cite{REF:VeloFCN2016,REF:lasernet_CVPR2019,REF:bewley2020range}. These voxel-based methods are efficient and can leverage the optimized libraries on modern GPUs/TPUs. \n% On the point-based 3D detection, methods~\\cite{shi2018pointrcnn,REF:yang2018ipod,REF:StarNet_2019,qi2019deep,yang20203dssd,REF:Point-GNN_CVPR2020} typically take raw point clouds as input and yields predictions based on each point. Compared to voxel-based approaches, these methods can more effectively exploit local structures of the point cloud but suffers from high run-time latency. \n% More recently, hybrid strategies~\\cite{zhou2020end,REF:STD_ICCV2019,REF:FastPointRCNN_Jiaya_ICCV2019,REF:SA_SSD_He_2020_CVPR,shi2020pv} have demonstrated more favorable detection performance by synergizing both the voxel- and point-based approaches. In this work, we employ one such hybrid method, ~\\cite{zhou2020end}, for its promising performance on public benchmarks and efficient implementation.\n\n% \\paragraph{3D Detection with Multi-frame LiDAR Scans} Temporal aggregation of LiDAR scans is an effective for point cloud densification and shape completion. \\cite{REF:FaF_Luo_2018_CVPR} fuses multi-frame information by concatenating feature maps from different time stamps. \\cite{REF:you_see_Hu_2020_CVPR} aggregates (motion-compensated) points from different LiDAR sweeps into a single scene. \\cite{REF:Spatiotemporal_transformer_CVPR2020} uses graph-based spatiotemporal feature encoding to enable message passing among different frames. \\cite{REF:ConvLSTM_ECCV2020} encodes previous frames with LSTM to assist detection in the current frame.\n\n% \\paragraph{3D Detection with Multi-Modalities} Several works~\\cite{REF:ku2018joint,cvpr17chen,qi2018frustum,REF:pointfusion_CVPR2018,REF:ContFuse_ECCV2018,REF:Multi_task_multisensor_fusion_CVPR2019,REF:LaserNet++_CVPRW2019,REF:MVX-Net_ICRA2019,qi2020imvotenet} have shown that combining camera information with LiDAR can improve performance compared to LiDAR-only 3D detection, especially for small and far-away objects. However the need for an additional camera can yield diminishing returns when LiDAR quality can be compensated from multiple scans or improved by deploying higher-quality sensors. In this work, we focus on\n% LiDAR-only solution.\n\n% \\paragraph{Key Differences} Compared to prior works using a single-frame or a few past frames, we propose 3D object detection in the offboard setting to leverage the full temporal extension (\\textit{history, present and future}) of a point cloud video. While existing works have been focusing on a real-time detector for computationally constrained platforms, we aim to explore the head room of 3D detection, removing the causality constraint and relaxing the budget for latency and computation resources.\n\n% \\subsection{Sequence Modeling??}\n\\paragraph{Learning from point cloud sequences} Several recent works~\\cite{liu2019flownet3d, gu2019hplflownet, mittal2020just} proposed to learn to estimate scene flow from dynamic point clouds using end-to-end trained deep neural networks (from a pair of consecutive point clouds). Extending such ideas, MeteorNet~\\cite{liu2019meteornet} showed that longer sequences input can lead to performance gains for tasks such as action recognition, semantic segmentation and scene flow estimation. There are also other applications of learning in point cloud sequences, like point cloud completion~\\cite{prantl2019tranquil}, future point cloud prediction~\\cite{weng20204d} and gesture recognition~\\cite{owoyemi2018spatiotemporal}. We also see more released datasets with sequence point cloud data such as the Waymo Open Dataset~\\cite{sun2020scalability} for detection and the SemanticKITTI dataset~\\cite{behley2019semantickitti} for 3D semantic segmentation.\n\n% In this paper, we addresses a different problem (3D object detection from point cloud sequences) and proposed a tailored solution to leverage long temporal contexts (object-centric track auto labeling).\n\n% FlowNet3D~\\cite{liu2019flownet3d}\n\n% HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-Scale Point Clouds~\\cite{gu2019hplflownet}\n\n% Just Go With the Flow: Self-Supervised Scene Flow Estimation~\\cite{mittal2020just}\n\n% Meteornet: Deep learning on dynamic 3d point cloud sequences~\\cite{liu2019meteornet} for action recognition, semantic segmentation and scene flow estimation\n\n% Tranquil clouds: Neural networks for learning temporally coherent features in point clouds~\\cite{prantl2019tranquil} for dynamic point cloud upsampling.\n\n% 4D Forecasting: Sequential Forecasting of 100,000 Points~\\cite{weng20204d} for next-frame point cloud prediction.\n\n% Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data~\\cite{owoyemi2018spatiotemporal} using 3D CNNs.\n\n% SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences~\\cite{behley2019semantickitti} semantic segmentation using multiple Lidar frames.\n\n% Key difference: Prior work using point cloud sequences has not touched the 3D detection problem, which is the focus of our work.\n\n% \\paragraph{3D multi-object tracking.} A traditional way to smooth object bounding boxes is through object tracking. Tracking by detection is a recently popular trend in object tracking method thanks to the robustness of deep net based detectors. In~\\cite{weng2019baseline}, detector boxes were used to link object boxes across frames and box states (geometries and velocities) were updated through Kalman filters. There are also attempts to jointly track and predict future trajectories~\\cite{weng2020joint} or optimize the tracked boxes by graph neural networks~\\cite{weng2020gnn3dmot}. A recent work~\\cite{li2020joint} jointly optimizes the spatial locations and the geometries (size and heading) of the 3D object boxes using the Gauss-Newton method. Nearly all prior work took detected object bounding boxes as input and did not leverage the raw sensor data (point clouds) for improving the boxes. In comparison, our work uses both the raw object points and the detector boxes to get the most accurate boxes possible.\n\n\n% Joint 3D Tracking and Forecasting with Graph Neural Network and Diversity Sampling~\\cite{weng2020joint}\n% GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning~\\cite{weng2020gnn3dmot}\n% Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking~\\cite{li2020joint} (jointly optimize for localization accuracy\n% and motion consistency).\n\n% Key difference: Tracking is also able to refine and smooth the boxes. While previous work focus on using the low-dimensional boxes to track or just use one or a few frames, our offboard 3d detection pipeline utilizes sensor data (point clouds) and the whole trajectory of the object.\n\n\\paragraph{Auto labeling}\n% \\najibi{we can shorten this section if we need space by removing this paragraph:}\n% The large datasets required for training state-of-the-art data-hungry models have increased the annotation costs noticeably in recent years, especially when it comes to labeling 3D scenes with thousands of points. Accurate auto labeling can dramatically reduce annotation time and cost, opening the way to further increase the scale of such datasets.\n\nThe large datasets required for training data-hungry models have increased the annotation costs noticeably in recent years. Accurate auto labeling can dramatically reduce annotation time and cost. %, opening the way to further increase the scale of such datasets.\nPrevious works on auto labeling were mainly focused on 2D applications. Lee \\etal proposed pseudo-labeling~\\cite{lee2013pseudo} to use the most confident predicted category of an image classifier as labels to train it on the unlabeled part of the dataset. More recent works~\\cite{iscen2019label, zou2019confidence, yalniz2019billion, xie2020self} have further improved the procedures to use pseudo labels and demonstrated wide success including state-of-the-art results on ImageNet~\\cite{deng2009imagenet}.\n% label propagation was utilized to refine pseudo-labels while taking the underlying manifold of the unlabeled data into account. Since pseudo-labels are noisy, \\cite{zou2019confidence} proposed confidence regularization to define soft pseudo-labels. Yalniz \\etal \\cite{yalniz2019billion} improved classification performance on ImageNet by using the most confident predictions of a teacher to annotate 1 Billion additional unlabeled images for training a student. More recently, \\cite{xie2020self} achieved state-of-the-art results on ImageNet classification while using 300M additional unlabeled images and deploying an iterative student-teacher framework to gradually improve the quality of the pseudo-labels. \n\n% Previous works on auto labeling were mainly focused on 2D applications. Lee \\etal proposed pseudo-labeling \\cite{lee2013pseudo} to use the most confident predicted category of an image classifier as labels to train it on the unlabeled part of the dataset. In \\cite{iscen2019label}, label propagation was utilized to refine pseudo-labels while taking the underlying manifold of the unlabeled data into account. Since pseudo-labels are noisy, \\cite{zou2019confidence} proposed confidence regularization to define soft pseudo-labels. Yalniz \\etal \\cite{yalniz2019billion} improved classification performance on ImageNet by using the most confident predictions of a teacher to annotate 1 Billion additional unlabeled images for training a student. More recently, \\cite{xie2020self} achieved state-of-the-art results on ImageNet classification while using 300M additional unlabeled images and deploying an iterative student-teacher framework to gradually improve the quality of the pseudo-labels. \n\nFor 3D object detection, recently, Zakharov \\etal~\\cite{zakharov2020autolabeling} proposed an auto labeling framework using pre-trained 2D detectors to annotate 3D objects. While effective for loose localization (\\ie IoU of 0.5), there is a considerable performance gap for applications requiring higher precision. \\cite{meng2020weakly} tried to leverage weak center-click supervision to reduce 3D labels needed.\n% Simplifying the annotation process was studied in~\\cite{meng2020weakly}, where only a small set of fully annotated 3D objects were used together with a larger collection of center-click supervision.\nSeveral other works~\\cite{castrejon2017annotating, acuna2018efficient, lee2018leveraging, ling2019fast, feng2019deep} have also proposed methods to assist human annotators and consequently reducing the annotation cost.\n% In contrast to these works, we design a novel framework for training a strong offboard model for the task of 3D object detection. \n\n% In contrast to the previous works, our method does not focus on reducing the noise of a weak teacher or relying on large collections of additional unlabeled data for boosting the performance. Instead, considering the offboard requirements, we design a novel framework for training a strong teacher for the task of 3D object detection in point clouds on the existing fully annotated datasets.\n% Our offline auto labeler achieves on-par performance with the human annotators on the challenging Waymo open dataset even for high localization thresholds. \n\n\n% Autolabeling 3D Objects with Differentiable Rendering of SDF Shape Priors~\\cite{zakharov2020autolabeling}\n\n% Weakly Supervised 3D Object Detection from Lidar Point Cloud~\\cite{meng2020weakly} using center click supervision.\n\n% Pseudo labeling in the semi-supervised learning set up. A strong teacher model.\n% FB paper~\\cite{yalniz2019billion}, Google paper~\\cite{xie2020self}\n\n% 2D labeling assistance papers. e.g. polygon RNN~\\cite{castrejon2017annotating}\n\n% Key differences: While prior work are mostly assistive or the focus is not on getting the strongest teacher model, our offboard setting and novel model design makes our 3D auto labeling pipeline achieve on par performance compared with humans, which can be served as a very strong teacher.\n% \\section{Problem Statement}\n"
            },
            "section 3": {
                "name": "Offboard 3D Object Detection",
                "content": "\n\\label{sec:problem}\n% Given a sequence of sensor data input of a dynamic environment, our goal is to localize and classify objects in the 3D scene.\n\n% Specifically, the input is a point cloud video $\\{\\mathcal{P}_i \\in \\mathbf{R}^{n_i \\times C}\\}$, $i=1,2,...,N$ with the point cloud data $\\mathcal{P}_i$ ($n_i$ points with $C$ channels for each point) of each of the $N$ total frames in the sequence. The point cloud can be obtained from depth cameras or Lidars. The point channels include the $XYZ$ coordinates of the points in the sensor coordinate (the sensor may move from frame to frame) and may contain other supplementary information such as the intensity from a Lidar sensor. We also assume known sensor poses $\\{\\mathcal{M}_i = [R_i | t_i] \\in \\mathbf{R}^{3\\times 4}\\}$, $i=1,2,...,N$ at each frame in the world coordinate, such that we can transform points to a shared coordinate system to remove the ego-motion of the sensor. Such pose information can be obtained by a localization system via SLAM or inertial measurement units (IMU).\n% In the general case, we may have other sensor input for each frame such as camera views, radar returns or maps (e.g. roadgraph). However for simplicity, we focus on just using the point cloud input in this paper. \n\n% The offboard 3D object recognition system taken such sequence input will output objects $\\{o_j\\}$, $j=1,2,...,M$ ($M$ is the total number of unique objects in the sequence) in the dynamic scene. Here, each \\emph{object} is a unique instance that may appear across a time horizon in multiple consecutive or non-consecutive frames. We define an object by its semantic class $c$ (one among the $k$ predefined classes), location and geometry. The location and geometry are represented by an amodal 3D bounding box $\\mathcal{B}$ parameterized by its size ($l, w, h$), center ($cx, cy, cz$) and orientation ($\\theta, \\phi, \\psi$). In our implementation, we only consider the heading angle $\\theta$ around the up-axis, so each box has 7-dimension of freedom. Then overall, each object $o_j$ can be represented as a sequence of 3D bounding boxes with a semantic class $o_j = (\\{\\mathcal{B}_{j,k}\\}, c_j)$, $k \\in S_j$ where $S_j \\subseteq \\{1,2,...,N\\}$ are the indices of frames this object appears in the sensor data.\n\n% Note that compared to simply detecting objects as each frame, the output of our problem formulation is aware of the identity of boxes at each frame. In another world, we output \\emph{tracked} object bounding boxes with object IDs. Such object identies are needed for the offboard use cases such as assisting human labeling or serve as auto label ground truth for prediction problems. \n\n% The offboard 3D detection system taken such sequence input recognizes objects at each frame of the sequence. Each object is represented by a class type $c$ (one among the $k$ predefined classes) and an amodal 3D bounding box $\\mathcal{B}$ parameterized by its size ($l, w, h$), center ($cx, cy, cz$) and orientation ($\\theta, \\phi, \\psi$). In our implementation, we only consider the heading angle $\\theta$ around the up-axis, so each box has 7-dimension of freedom. Besides the boxes and the their semantic class types, the system also outputs a score $s \\in [0,1]$ for each box representing the confidence of the estimation. The system outputs boxes for all recognized objects at every frame $\\{\\{(\\mathcal{B}_j, c_j, s_j)\\}\\}$.\n\n% In this section, we describe the formulation of the offboard 3D object detection problem and discuss its design space, before we dive into our proposed solution in Sec.~\\ref{sec:method}.\n\n\\paragraph{Problem statement}\nGiven a sequence of sensor inputs (temporal data) of a dynamic environment, our goal is to localize and classify objects in the 3D scene for every frame.\nSpecifically, we consider the input of a sequence of point clouds $\\{\\mathcal{P}_i \\in \\mathbf{R}^{n_i \\times C}\\}$, $i=1,2,...,N$ with the point cloud $\\mathcal{P}_i$ ($n_i$ points with $C$ channels for each point) of each of the $N$ total frames.\n% (for example $N = 200$ for a 20-second video at 10Hz) .\nThe point channels include the $XYZ$ in the sensor's coordinate (at each frame) and other optional information such as color and intensity.\nWe also assume known sensor poses $\\{\\mathcal{M}_i = [R_i | t_i] \\in \\mathbf{R}^{3\\times 4}\\}$, $i=1,2,...,N$ at each frame in the world coordinate, such that we can compensate the ego-motion.\n% The sensor poses can be obtained by a localization system via SLAM or using inertial measurement units (IMU).\nFor each frame, we output amodal 3D bounding boxes (parameterized by its center, size and orientation), class types (\\eg vehicles) and unique object IDs for all objects that appear in the frame.\n% Object IDs for appearances of the same physical object should be the same.\n\n\\paragraph{Design space} Access to temporal data (history and future) has led to a much larger design space of detectors compared to just using single frame input. \n% Here we discuss two baseline, frame-centric designs to our solution and show how we can develop based on them.\n\n% Before we introduce our 3D Auto Labeling pipeline, let's look at two alternative (frame-centric) designs for offboard 3D object recognition.\n\n% The first baseline idea to achieve more accurate perception from point cloud video is to extend the single-frame 3D object detectors to use multi-frame input. The boxes can then be tracked via a separate tracking model. Although previous works [CITE] has shown its effectiveness, we observe that its return becomes diminishing as we stack more and more frames as input (See Fig.[REF] in the experiment section). As the method stacks the point clouds from the entire scene, using a large number of frames (e.g. 10) also significantly increases the model size and computation which limits the number of frames to be used.\n\nOne baseline design is to extend the single-frame 3D object detectors to use multi-frame input. Although previous works~\\cite{REF:FaF_Luo_2018_CVPR, REF:you_see_Hu_2020_CVPR, REF:ConvLSTM_ECCV2020, REF:Spatiotemporal_transformer_CVPR2020} have shown its effectiveness, a multi-frame detector is hard to scale up to more than a few frames and cannot compensate the object motions since frame stacking is done for the entire scene. We observe that the contributions of multi-frame input to the detector quality diminish as we stack more frames (Table~\\ref{tab:detection_AP_vs_frames}).\nAnother idea is to extend the second stage of two-stage detectors~\\cite{qi2018frustum, shi2018pointrcnn} to take object points from multiple frames. Compared to taking multi-frame input of the whole scene, the second-stage only processes proposed object regions. However, it is not intuitive to decide how many context frames to use. Setting a fixed number may work well for some objects but suboptimal for others.\n\n% Instead of processing the entire scene point clouds from the sequence, a natural idea is to have a second stage to refine the object boxes just using the multi-frame points of the object. We still refine each object for every frame. Similar to a two-stage detector such as Fast R-CNN or Faster R-CNN, after the first-stage 3D object localization, we can have an extra sub-network to crop the multi-frame object points (transformed to box coordinate) from the region of interests and refine the box. Here because we only process object points, we can potentially aggregate more frames with less burden on memory usage. However, in this design it's hard to adaptively decide how many context frames to use. If both the sensor and the object are static, adding a few nearby frames would not increase the object visibility. Always using a maximum number of context frames may waste resources too as many objects may just appear in a few frames. \\rqi{Improve the logic here as such design looks similar to ours.}\n\nCompared to the \\emph{frame-centric} designs above, where input is always from a fixed number of frames, we recognize the necessity to adaptively choose the temporal context size for each object independently, leading to an \\emph{object-centric} design. As shown in Fig.~\\ref{fig:pipeline}, we can leverage the powerful multi-frame detector to give us the initial object localizations. Then for each object, through tracking, we can extract all relevant object point clouds and detection boxes from all frames that it appears in. Subsequent models can take such object track data to output the final track-level refined boxes of the objects. As this process emulates how a human labeler annotates a 3D object in the point cloud sequence (localize, track and refine the track over time), we chose to refer to our pipeline as \\emph{3D Auto Labeling}.\n\n% Our proposed pipeline extends the two baseline ideas above. First, it leverages the powerful multi-frame detector to give initial detection of objects in each frame. Then, we link detected objects across frames through multi-object tracking. Instead of taking a frame-centric approach in the second idea, we propose an \\emph{object track extraction} step to extract all relevant information of an object i.e. its point clouds and detection boxes at all frames that it appears. For static objects, we will then get the most complete viewpoints of the object. For moving objects, we will get the complete trajectory of the object along with its point clouds at every frame. Then using such object track data, we propose object-centric models (both input and output are of a specific object) to estimate high-precision and temporally consistent object boxes. As this process emulate how a human labeler annotates a 3D object in the point cloud sequence (localize, track and refine based on the object sequence), we name our pipeline as \\emph{3D Auto Labeling}.\n"
            },
            "section 4": {
                "name": "3D Auto Labeling Pipeline",
                "content": " % name to be changed.\n\\label{sec:method}\n% \\todo{1st version ddl: 10/30 11PM; 2nd version ddl: 11/2 11PM; Owners: rqi, yinzhou}\n\n% \\yinzhou{Consider separating the detection and tracking from the section 3 into the implementation section 4.}\n% \\rqi{Shall we introduce the modules one by one: detector, tracker, motion classification, object-centric track refinement, auto quality control (IoU prediction). or shall we introduce the novel designs/modules first?}\n\n% Overview (introduce the whole pipeline). In the subsection below, for each section, we will describe its input and output format and then explains its design while discussing alternatives and related work (the why part).\n\n% Fig.~\\ref{fig:pipeline} illustrates our proposed 3D Auto Labeling pipeline, which involves four key modules: 3D object detector, 3D multi-object tracker, as well as the novel object track data extraction and object-centric auto labeling modules. We will introduce each module in the following sub-sections.\nFig.~\\ref{fig:pipeline} illustrates our proposed 3D Auto Labeling pipeline. We will introduce each module of the pipeline in the following sub-sections.\n\n\n% \\subsection{Overall Design}\n% Before we introduce our 3D Auto Labeling pipeline, let's look at two alternative (frame-centric) designs for offboard 3D object recognition.\n\n% The first baseline idea to achieve more accurate perception from point cloud video is to extend the single-frame 3D object detectors to use multi-frame input. The boxes can then be tracked via a separate tracking model. Although previous works [CITE] has shown its effectiveness, we observe that its return becomes diminishing as we stack more and more frames as input (See Fig.[REF] in the experiment section). As the method stacks the point clouds from the entire scene, using a large number of frames (e.g. 10) also significantly increases the model size and computation which limits the number of frames to be used.\n\n% Instead of processing the entire scene point clouds from the sequence, a natural idea is to have a second stage to refine the object boxes just using the multi-frame points of the object. We still refine each object for every frame. Similar to a two-stage detector such as Fast R-CNN or Faster R-CNN, after the first-stage 3D object localization, we can have an extra sub-network to crop the multi-frame object points (transformed to box coordinate) from the region of interests and refine the box. Here because we only process object points, we can potentially aggregate more frames with less burden on memory usage. However, in this design it's hard to adaptively decide how many context frames to use. If both the sensor and the object are static, adding a few nearby frames would not increase the object visibility. Always using a maximum number of context frames may waste resources too as many objects may just appear in a few frames. \\rqi{Improve the logic here as such design looks similar to ours.}\n\n% Our proposed pipeline extends the two baseline ideas above. First, it leverages the powerful multi-frame detector to give initial detection of objects in each frame. Then, we link detected objects across frames through multi-object tracking. Instead of taking a frame-centric approach in the second idea, we propose an \\emph{object track extraction} step to extract all relevant information of an object i.e. its point clouds and detection boxes at all frames that it appears. For static objects, we will then get the most complete viewpoints of the object. For moving objects, we will get the complete trajectory of the object along with its point clouds at every frame. Then using such object track data, we propose object-centric models (both input and output are of a specific object) to estimate high-precision and temporally consistent object boxes. As this process emulate how a human labeler annotates a 3D object in the point cloud sequence (localize, track and refine based on the object sequence), we name our pipeline as \\emph{3D Auto Labeling}.\n\n",
                "subsection 4.1": {
                    "name": "Multi-frame 3D Object Detection",
                    "content": "\n\\paragraph{MVF++}\nAs the entry point to our pipeline, accurate object detection is essential for the downstream modules. % to generate high-quality outcomes.\n% In this work, we propose to use the MVF++ 3D detector by extending the top-performing detector, Multi-View Fusion~\\cite{zhou2020end} (MVF), in three aspects: 1) to enhance the discriminative ability of point-level features, we add an auxiliary loss for 3D semantic segmentation, where points are labeled as positives/negatives if they lie inside/outside of a ground truth 3D box; 2) for obtaining more accurate training targets and improving training efficiency, we eliminate the anchor matching step in the MVF paper and adopt the popular anchor-free design as in~\\cite{REF:tian2019fcos}; 3) to leverage ample computational resources available in the offboard setting, we redesign the network architecture and increase the model capacity. Please see the supplementary material for details.\nIn this work, we propose the MVF++ 3D detector by extending the top-performing Multi-View Fusion~\\cite{zhou2020end} (MVF) detector in three aspects: 1) to enhance the discriminative ability of point-level features, we add an auxiliary loss for 3D semantic segmentation, where points are labeled as positives/negatives if they lie inside/outside of a ground truth 3D box; 2) for obtaining more accurate training targets and improving training efficiency, we eliminate the anchor matching step in the MVF paper and adopt the anchor-free design as in~\\cite{REF:tian2019fcos}; 3) to leverage ample computational resources available in the offboard setting, we redesign the network architecture and increase the model capacity. Please see Sec.~\\ref{sec:supp:mvf} in the Appendix for details.\n\n% In this work, we propose MVF++ 3D detector by introducing a series extensions to Multi-View Fusion~\\cite{zhou2020end} (MVF). \n\n% MVF is an end-to-end trainable, single-stage LiDAR-only 3D detector. On point-wise feature learning, it utilizes a hybrid strategy leveraging the merits from both point-based and voxel-based representations. In addition, the method employs fully convolution backbone for efficient prediction. We derive MVF++ by introducing improvements to MVF from two aspects: 1) To enhance the discriminative ability of point-level features, we add an auxiliary loss on 3D semantic segmentation, where positive and negative samples are determined whether a point is within a ground truth 3D box; 2) For more robust detection of objects with dictint sizes (\\textit{e.g.,} motorcycles, cars, trucks) and efficient training, we eliminates the original anchor matching in the MVF paper and adopt the popular anchor-free design as in~\\cite{REF:tian2019fcos} \n\n% It consists of a point-level feature fusion network and a fully convolutional backbone. Specifically, given a raw LiDAR point cloud as input, the feature fusion network learns to extract complementary information for each point from three sources, \\textit{i.e.,} contextual features in the Bird's Eye View~\\cite{zhou2018voxelnet,lang2019pointpillars}, contextual features in the Perspective View~\\cite{REF:VeloFCN2016,REF:lasernet_CVPR2019} and PointNet~\\cite{qi2017pointnet} features based on the original point set. Then, the point-level features are projected to the Bird's Eye View and transformed into a 2D feature map, which is further processed by the fully convolutional backbone. \n\n% We derive MVF++ by introducing improvements to MVF from two aspects: 1) To enhance the discriminative ability of point-level features, we add an auxiliary loss on 3D semantic segmentation, where positive and negative samples are determined whether a point is within a ground truth 3D box; 2) For more robust detection of objects with dictint sizes (\\textit{e.g.,} motorcycles, cars, trucks) and efficient training, we eliminates the original anchor matching in the MVF paper and adopt the popular anchor-free design as in~\\cite{REF:tian2019fcos} \n\n\\paragraph{Multi-frame MVF++}\nWe extend the MVF model to use multiple LiDAR scans. Points from multiple consecutive scans are transformed to the current frame based on ego-motion. Each point is extended by one additional channel, encoding of the relative temporal offset, similar to~\\cite{REF:you_see_Hu_2020_CVPR}. The aggregated point cloud is used as the input to the MVF++.\n\n\\paragraph{Test-time augmentation}\n\\label{sec:test_time_augmentation}\nWe further boost the 3D detection through test-time augmentation (TTA)~\\cite{REF:AlexNet:2017}, by rotating the point cloud around Z-axis by 10 different angles (\\ie [0, $\\pm 1/8\\pi$, $\\pm 1/4\\pi$, $\\pm 3/4\\pi$, $\\pm 7/8\\pi$, $\\pi$]), and ensembling predictions with weighted box fusion~\\cite{REF:wbf2019}. While it may lead to excessive computational complexity for onboard uses, in the offboard setting TTA can be parallelized across multiple devices for fast execution. \n\n% \\paragraph{Discussion on Alternatives}\n\n% Introduce the multi-view fusion detector and why we use it. Introduce how we extends the single-frame model to use multi-frame points. Maybe discuss a few ablation methods to use multi-frame input. Briefly mention the test time augmentation?\n\n"
                },
                "subsection 4.2": {
                    "name": "Multi-object Tracking",
                    "content": "\n\nThe multi-object tracking module links detected objects across frames.\n% , which is necessary to achieve highly accurate object-centric box regression. \n% As described in the problem statement (Sec.~\\ref{sec:problem}), we care about recognizing unique objects. Therefore we need to understand the relations of detected boxes at different frames, which requires multi-object tracking.\n% While there are several methods that do joint detection and tracking~\\cite{REF:FaF_Luo_2018_CVPR,weng2020joint,li2020joint}, we observe that the performance of the current (tracking-by-detection based) tracking algorithms is still largely dependent on the detection quality.\n% While there are several methods that do joint detection and tracking~\\cite{REF:FaF_Luo_2018_CVPR,weng2020joint,li2020joint}, we intentionally choose a modular design to separate detectors and trackers for the flexibility to switch detectors easily.\nGiven the powerful multi-frame detector, we choose to take the tracking-by-detection path and have a separate non-parametric tracker. This leads to a simpler and more modular design compared to the joint detection and tracking methods~\\cite{REF:FaF_Luo_2018_CVPR,weng2020joint,li2020joint}.\nOur tracker is an implementation variant of the~\\cite{weng2019baseline}, using detector boxes for associations and Kalman filter for state updates.\n% Empirically, this simple design is already able to achieve top tracking performance (Table~\\ref{tab:tracking_results}).\n\n% Specifically, given the detector output $\\{(\\mathcal{B}^{(i)}_{j}, c_j, s_j)\\}$, $j=1,2,...,m_i$ ($m_i$ boxes) at frame $i=1,2,...,N$ with detector  boxes $\\mathcal{B}$, class type $c$ and confidence score $s$, the tracking module associates the boxes and assigns a track ID to each detector box. If we group all the boxes with the same track ID, we get a set of boxes for the object $j$ as $o_j = (\\{B_{j,k}\\}, c_j)$, $k \\in S_j$ where $S_j \\subseteq \\{1,2,...,N\\}$.\n\n% Specifically, the output of the tracking step is a set of tracklets/objects. Each tracklet corresponds to one object, which is represented as $o_j = (\\{\\mathcal{B}_{j,k}\\}, c_j)$, $k \\in S_j$ where $S_j \\subseteq \\{1,2,...,N\\}$ are the indices of the frames the object is tracked. $j$ is the object index. The $\\mathcal{B}_{j,k}$ is the object $j$'s 3D bounding box at frame $k$ (in the frame coordinate) and $c_j$ is the class type of the object. These recognized objects will be further refined through the next object-centric object auto labeling models to have more accurate and temporally consistent 3D boxes.\n\n% to get $\\{(\\mathcal{B}^{(i)}_{j}, c_j, s_j, \\text{id}_j)\\}$ for the frame $i=1,...,N$ and box $j=j_1,...,j_{m'_i}$ ($m'_i \\leq m_i$ as some boxes may not be tracked). Note that we do not update the detector boxes through the tracker. For more details of the tracking, please refer to the supplementary.\n\n% (this should be a short paragraph) we are using the baseline tracker~\\cite{weng2019baseline} with some careful implementations.\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Object Track Data Extraction",
                    "content": "\nGiven tracked detection boxes for an object, we can extract object-specific LiDAR point clouds from the sequence. We use the term \\emph{object track data} to refer to such 4D (3D spatial and 1D temporal) object information.\n\nTo extract object track data, we first transform all boxes and point clouds to the world coordinate through the known sensor poses to remove the ego-motion. For each unique object (according to the object ID), we crop its object points within the estimated detector boxes (enlarged by $\\alpha$ meters in each direction to include more contexts). %we also add a buffer to the box in the extraction by expanding the sizes of the box by $\\alpha$ meter in each dimension.\nSuch extraction gives us a sequence of object point clouds $\\{\\mathcal{P}_{j, k}\\}$, $k \\in S_j$ for each object $j$ and its visible frames $S_j$. Fig.~\\ref{fig:pipeline} visualizes the object points for several vehicles. Besides the raw point clouds, we also extract the tracked boxes for each object and every frame $\\{\\mathcal{B}_{j,k}\\}$, $k \\in S_j$ in the world coordinate.\n% Both the object point clouds and the boxes are in the world coordinate frame.\n\n% We see that for a static object, we get a dense point cloud of the object with much more complete view coverage than a single frame can possibly get. The points can come from more than 100 frames, which multi-frame detector can hardly handle. For a moving object, we get a complete trajectory of the object which provides strong cues to the object heading estimation. We also have its points at every frame.\n\n% We note the points in the world coordinate as $\\mathcal{P}'$ and the box in the world coordinate as $\\mathcal{B}'$. For each unique object $o_j = (\\{\\mathcal{B}'_{j,k}\\}, c_j)$, $k \\in S_j$ where $S_j \\subseteq \\{1,2,...,N\\}$ (indices of frames where the object is tracked), we crop its object points within its boxes at each frame. To preserve some contexts around the object and to handle the case where detection boxes are inaccurate, we also add a buffer to the box in the extraction by expanding the sizes of the box by $\\alpha$ meter in each dimension (in implementation we set a fixed $\\alpha = 1m$). Such extraction gives us a sequence of object point clouds $\\{\\mathcal{P}'_{j, k}\\}$, $k \\in S_j$ for each object $j$ and its visible frame $k$. Fig.~\\ref{fig:pipeline} visualizes the object points for a moving car and a static car respectively. We see that for a static object, we get a dense point cloud of the object with much more complete view coverage than a single frame can possibly get. The points can come from more than 100 frames, which multi-frame detector can hardly handle. For a moving object, we get a complete trajectory of the object which provides strong cues to the object heading estimation. We also have its points at every frame.\n\n% Besides the object points in the object track, we also get the tracked boxes for each object $\\{\\mathcal{B}'_{j,k}\\}$, $k \\in S_j$. Combining the object points and the object (initial) boxes, we get the full object track data for an object $j$: $(\\{\\mathcal{P}'_{j, k}\\}, \\{\\mathcal{B}'_{j,k}\\})$ which will be used for the next modules.\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Object-centric Auto Labeling",
                    "content": "\nIn this section, we describe how we take the object track data to ``auto label'' the objects. As illustrated in Fig.~\\ref{fig:pipeline}, the process includes three sub-modules: the track-based motion state classification, static object auto labeling and dynamic object auto labeling, which are described in detail below.\n\n\n\\paragraph{Divide and conquer: motion state estimation}\n\n% (this paragraph will focus on why we want to divide and conquer, to process static and dynamic objects differently. The algorithm for the state estimation will be briefly introduced -- currently it is a linear model, not deep net based.)\n\nIn the real world, lots of objects are completely static during a period of time. For example, parked cars or furniture in a room do not move within a few minutes or hours. In terms of offboard detection, it is preferred to assign a single 3D bounding box to a static object rather than separate boxes in different frames to avoid jittering.\n\nBased on this observation, we take a divide-and-conquer approach to handle static and moving objects differently, introducing a module to classify an object's motion state (static or not) before the auto labeling.\n% Note that here ``static'' is defined to be completely static in the entire input sequence, so that we can assign a single box for it.\nWhile it could be hard to predict an object's motion state from just a few frames (due to the perception noise), we find it relatively easy if all object track data is used. As the visualization in Fig.~\\ref{fig:pipeline} shows, it is often obvious to tell whether an object is static or not from its trajectory. A linear classifier using a few heuristic features from the object track's boxes can already achieve $99\\%+$ motion state classification accuracy for vehicles. More details are in Sec.~\\ref{sec:supp:motion}.\n% (for example, the variance of the box centers in the world coordinate).\n% For vehicles we can achieve more than $99\\%$ accuracy in motion status classification (we preferred a high precision of static classification).\n% More details can be found in the supplementary.\n\n% \\begin{figure*}[h]\n%     \\centering\n%     \\includegraphics[width=0.8\\linewidth]{./fig/object_auto_labeling}\n%     \\caption{Object Auto Labeling. \\rqi{Consider merge this figure with the overall pipeline figure. The texts are too small here.}}\n%     \\label{fig:object_auto_labeling}\n% \\end{figure*}\n\n\n\\paragraph{Static object auto labeling}\n\n% Based on the motion status classification, we can design separate modules for the static object auto labeling (or refinement) and moving object auto labeling. For a static object, the model outputs a single 3D bounding box and a single score for the track in the world coordinate. To get boxes at each frame, we just need to transform the box to each frame's coordinate through the known frame poses.\n% For a moving object, the model takes the sequential track data and outputs a 3D bounding box and a score for each frame.\n\n% \\paragraph{Modeling.}\nFor a static object, the model takes the merged object point clouds ($\\mathcal{P}_j = \\cup \\{\\mathcal{P}_{j, k}\\}$ in the world coordinate) from points at different frames and predicts a single box. The box can then be transformed to each frame through the known sensor poses.\n\nFig.~\\ref{fig:static_model} illustrates our proposed model for static object auto labeling.\n% For a static object, its  through a union $\\mathcal{P}_j = \\cup \\{\\mathcal{P}_{j, k}\\}$.\n% The static object auto labeling model is as shown in the Fig.~\\ref{fig:static_model}) which involves a few non-parametric transformations and a few sub-networks that are based on the PointNet~\\cite{qi2017pointnet} architecture.\nSimilar to~\\cite{qi2018frustum, shi2018pointrcnn}, we first transform (through rotation and translation) the object points to a \\emph{box coordinate} before the per-object processing, such that the point clouds are more aligned across objects. In the box coordinate, the $+X$ axis is the box heading direction, the origin is the box center.\n% There are multiple choices in which box to use for the transform, which can have a significant impact (Table~\\ref{tab:static_frame_selection}).\n% In the box coordinate, the origin is the box center and the $+X$ axis is the heading direction of the box.\nSince we have the complete sequence of the detector boxes, we have multiple options on which box to use as the initial box. The choice actually has a significant impact on model performance. Empirically, using the box with the highest detector score leads to the best performance (see Sec.~\\ref{sec:supp:analysis} for an ablation study).\n\nTo attend to the object, the object points are passed through an instance segmentation network to segment the foreground ($m$ foreground points are extracted by the mask). Inspired by the Cascade-RCNN~\\cite{cai2018cascade}, we iteratively regress the object's bounding box.\n% For each iteration, we first transform the object point cloud to the box coordinate of the last predicted box (for the first iteration it's the initial box) and then regress a new box.\nAt test time, we can further improve box regression accuracy by test-time-augmentation (similar to Sec. \\ref{sec:test_time_augmentation}).\n\nAll networks are based on the PointNet~\\cite{qi2017pointnet} architecture. The model is supervised by the segmentation and box estimation ground truths. Details of the architecture, losses and the training process are described in Sec.~\\ref{sec:supp:object_auto_labeling}.\n\n% Next, as in~\\cite{qi2018frustum}, we have an instance segmentation network to segment the foreground object points. Then we have a cascade structure similar to the Cascade-RCNN~\\cite{cai2018cascade} to predict the refined 3D bounding box. Specifically, the first box regression network takes in the segmented foreground points and output a 3D box $\\mathcal{B}_1$. Then similar to the first geometric transformation, we transform the foreground points to the box coordinate of $\\mathcal{B}_1$ and runs another box regression network to output $\\mathcal{B}_2$. We found that not sharing weights (cascade) between the two box regression networks lead to slightly better results than sharing the weights (iterative). As the predicted $\\mathcal{B}_2$ is in the box coordinate of $\\mathcal{B}_1$, we need to transform it back to the input point coordinate (the world coordinate), so we have a reverse geometric transformation. \n% \\rqi{mention the AutoQC?}\n\n\n% \\paragraph{Losses.}\n% The model is trained with supervision of the segmentation masks and the ground truth 3D bounding boxes. For the segmentation, the sub-network predicts two scores for each point as foreground or background and is supervised with a cross-entropy loss $L_{seg}$. For the box regression, we implement a process similar to~\\cite{qi2018frustum}, where each box regression network regresses the box by predicting its center $cx, cy, cz$, its size classes (among a few pre-defined template size classes) and residual sizes for each size class, as well as the heading bin class and a residual heading for each bin. More precisely, the box regression loss is defined as $L_{\\text{box}_i} = L_{\\text{c-reg}_i} + w_1 L_{\\text{s-cls}_i} + w_2 L_{\\text{s-reg}_i} + w_3 L_{\\text{h-cls}_i} + w_4 L_{\\text{h-reg}_i}$ where $i \\in \\{1,2\\}$ represents the cascade box estimation step. The total box regression loss is $L = L_{seg} + w (L_{\\text{box}_1} + L_{\\text{box}_2})$. The $w_i$ and $w$ are hyperparameter weights of the losses.\n\n% \\paragraph{Training and data augmentation.}\n% We train our models using the extracted object tracks from the Waymo Open Dataset for each class type separately. Ground truth boxes are assigned to every frame of the track (frames with no matched ground truth are skipped). \n% % \\todo{how to assign ground truth with filtering of wrong track ID or wrong class type}\n\n% During training, for each static object track, we randomly select an initial box from the sequence. We also randomly sub-sample $\\text{Uniform}[1, |S_j|]$ frames from all the visible frames $S_j$ of an object $j$. Note that at test time we always select the initial box with the highest score and use all frames. The merged points are randomly sub-sampled to $4,096$ points and randomly flipped along the $X, Y$ axes with $50\\%$ chance respectively and randomly rotated around the up-axis ($Z$) by $\\text{Uniform}[-10, 10]$ degrees.\n% To increase the data quantity, we also turn the dynamic object track data to pseudo static track by using the ground truth object boxes to align the object points to a specific frame's coordinate. More details for training can be found in supplementary.\n\n\n% \\begin{itemize}\n%     \\item Discuss the baselines: single-frame, fixed context frames.\n%     \\item The model: segmentation->cascade refinement using the PointNet~\\cite{qi2017pointnet}.\n%     \\item Optimization losses.\n%     \\item Data augmentation.\n% \\end{itemize}\n\n\n\n\n% \\begin{table*}[t!]\n%     \\centering\n%     \\setlength{\\tabcolsep}{3pt}\n%     \\small\n%     \\begin{tabular}{l|c|c|c|c|c}\n%     \\toprule\n%          Method & #frames & \\multicolumn{2}{|c|}{3D AP} & \\multicolumn{2}{c}{BEV AP} \\\\ \n%          & & IoU=0.7 & IoU=0.8 & IoU=0.7 & IoU=0.8 \\\\ \n%          \\toprule\n%          StarNet~\\cite{REF:StarNet_2019} & 1 & 53.70 & - & - & - \\\\\n%          PointPillar~\\cite{lang2019pointpillars} & 1 & 56.62 & - & 75.57 & - \\\\\n%          Multi-view fusion (MVF)~\\cite{zhou2020end} & 1 & 62.93 & - & 80.40 & - \\\\\n%          AFDET~\\cite{REF:AFDET_CVPRW2020} & 1 & 63.69 & - & - & - \\\\\n%          ConvLSTM~\\cite{REF:ConvLSTM_ECCV2020} & 4 & 63.60  & - & - & - \\\\\n%          RCD~\\cite{REF:bewley2020range} & 1 & 68.95 & - & 82.09 & - \\\\\n%          PillarNet~\\cite{REF:PillarNet_ECCV2020} & 1 & 69.80 & - & 87.11 & - \\\\\n%          PV-RCNN~\\cite{shi2020pv}$^{\\star}$ & 1 & 70.47  & 39.16 & 83.43  & 69.52 \\\\\n%          \\midrule\n%         %  Single-frame MVF++ (Ours) & 1 & 71.99 / 68.22 & 39.35 / 37.41 & 86.38 / 81.60 & 72.17 / 68.39 \\\\ (old)\n%          Single-frame MVF++ (Ours) & 1 & 74.64  & 43.30  & 87.59  & 75.30  \\\\ \n%          Multi-frame MVF++ w. TTA (Ours) & 5 & 79.73  & 49.43  & 91.93  & 80.33  \\\\\n%          3D Auto Labeling (Ours) & $\\infty$ & \\textbf{83.99} & \\textbf{56.97}  & \\textbf{93.22}  & \\textbf{84.61} \\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\textbf{3D object detection results for vehicles on the Waymo Open Dataset \\emph{val} set.} Methods in comparison include prior state-of-the-art single-frame based 3D detectors as well as our single-frame MVF++, our multi-frame MVF++ (5 frames) and our full 3D Auto Labeling pipeline. The metrics are L1 3D AP and bird's eye view (BEV) AP at two IoU thresholds: the common standard IoU=0.7 and a high standard IoU=0.8.} %\\yinzhou{Consider differentiate with and with TTA.}}\n%     \\label{tab:main_detection_results}\n% \\end{table*}\n\n\n% \\begin{table}[t]\n%     \\centering\n%     \\setlength{\\tabcolsep}{3pt}\n%     \\small\n%     \\begin{tabular}{l|c|c|c|c}\n%     \\toprule\n%          Method & \\multicolumn{2}{|c|}{3D AP} & \\multicolumn{2}{c}{BEV AP} \\\\ \n%          & IoU=0.5 & IoU=0.6 & IoU=0.5 & IoU=0.6 \\\\ \n%          \\midrule\n%          PointPillar~\\cite{lang2019pointpillars} &60.11 & 40.35 & 65.42 & 51.71 \\\\\n%          MVF~\\cite{zhou2020end} & 65.33 & - & 74.38 & - \\\\\n%          PV-RCNN~\\cite{shi2020pv}$^{\\star}$ & 65.34 & 45.12 & 70.35 & 56.63 \\\\\n%          \\midrule\n%          S-MVF++ (Ours) & 71.76 & 46.86 & 79.19  & 62.60 \\\\ % TODO: may need update.\n%          M-MVF++ w. TTA (Ours) & 81.83 & 60.56 & 85.90 & 73.00 \\\\\n%          3DAL (Ours) & \\textbf{82.88} & \\textbf{63.69} & \\textbf{86.32} & \\textbf{75.60} \\\\ % TODO: update to the newest results.\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\textbf{3D object detection results for pedestrians on the Waymo Open Dataset \\emph{val} set.} The metrics are L1 3D AP and bird's eye view (BEV) AP at two IoU thresholds: the common standard IoU=0.5 and a high standard IoU=0.6.}\n%     \\label{tab:ped_ap}\n% \\end{table}\n\n\n\n\n\\paragraph{Dynamic object auto labeling}\n\nFor a moving object, we need to predict different 3D bounding boxes for each frame.\nDue to the sequence input/output, the model design space is much larger than that for static objects.\n% Several design choices exist: 1) to estimate bounding boxes from the multi-frame point clouds; 2) to refine boxes from the detection boxes sequence;\nA baseline is to re-estimate the 3D bounding box with cropped point clouds.\n% at each frame or by the point clouds of the object in the current frame and a few nearby frames;\nSimilar to the smoothing in tracking, we can also refine boxes based on the sequence of the detector boxes.\n% The first approach, although having the advantage of using raw sensor data, can hardly use the longer trajectory shape.\n%esp. when the object moves slowly (a few nearby frames tell less about the heading of the object).\n% On the other hand, while the second approach is capable of using longer or even the full sequences, it completely ignores the point cloud data.\nAnother choice is to ``align'' or register object points with respect to a keyframe (\\eg the current frame) to obtain a denser point cloud for box estimation.\n% A naive alignment approach is to use the detector boxes while one can also estimate the relative motion between frames.\nHowever, the alignment can be a harder problem than box estimation especially for occluded or faraway objects with fewer points. Besides, it is challenging to align deformable objects like pedestrians.\n% \\rqi{There is also a more fancier design to model the point cloud sequence with a convnet/pointnet/pointnet++ where we compute object embedding for each frame and then use a model to learn a sequence embedding using edge features (from relative box positions) and object features. This however does not (yet) play out well in the experimental results. More experiments to be done.}\n\nWe propose a design (Fig.~\\ref{fig:dynamic_model}) that leverages both the point cloud and the detector box sequences without aligning points to a keyframe explicitly.\n% For moving objects (Fig.~\\ref{fig:dynamic_model}):\n% \\begin{itemize}\n%     \\item Discuss the baselines: single-frame, refining without sensor data, align and refine.\n%     \\item The model: context point clouds + whole trajectory boxes, cascade, sequence modeling, multi-head losses.\n%     \\item Data augmentation.\n% \\end{itemize}\n% \\paragraph{Modeling.}\n% \\rqi{The section describes the current design, which may be changed depending on new experiment results.}\n% \\todo{update the figure to be consistent with the symbols in the text e.g. the frame k v.s. T.}\nGiven a sequence of object point clouds $\\{\\mathcal{P}_{j, k}\\}$ and a sequence of detector boxes $\\{\\mathcal{B}_{j, k}\\}$ for the object $j$ at frames $k \\in S_j$, the model predicts the object box at each frame $k$ in a sliding window form. It consists of two branches, one taking the point sequence and the other taking the box sequence.\n\n% The model has two branches with one taking the sequence points and the other taking the sequence boxes. For each frame $T$ the object is visible, in the first branch, the model takes the object point clouds at frames $T-m, T-m+1, ..., T, T+1, ..., T+m$. This represents a sub-sequence of the object similar to the input of the multi-frame detector. We take the union of the points $\\mathcal{Q}'_{j,T} = \\cup \\{\\mathcal{P}'_{j,k}\\}_{k=T-m}^{T+m}$. The the merged points are transformed to the box coordinate of the detector box $\\mathcal{B}'_{j,T}$ at frame $T$. Following that, we have a PointNet~\\cite{qi2017pointnet} based segmentation network to segment the foreground points (of the $2m+1$ frames) and then encode the object points into an embedding through another point encoder network.\n\nFor the point cloud branch, the model takes a sub-sequence of the object point clouds $\\{\\mathcal{P}_{j,k}\\}_{k=T-r}^{T+r}$. After adding a temporal encoding channel to each point (similar to~\\cite{REF:you_see_Hu_2020_CVPR})\n% ($0$ for the current frame, $-0.1r$ for the $r$-th frame before and $+0.1r$ for the $r$-th frame ahead)\n, the sub-sequence points are merged through union and transformed to the box coordinate of the detector box $\\mathcal{B}_{j,T}$ at the center frame. Following that, we have a PointNet~\\cite{qi2017pointnet} based segmentation network to classify the foreground points (of the $2r+1$ frames) and then encode the object points into an embedding through another point encoder network.\n\nFor the box sequence branch, the box sequences $\\{\\mathcal{B}'_{j,k}\\}_{k=T-s}^{T+s}$ of $2s+1$ frames are transformed to the box coordinate of the detector box at frame $T$. Note that the box sub-sequence can be longer than the point sub-sequence to capture the longer trajectory shape. A box sequence encoder network (a PointNet variant) will then encode the box sequence into a trajectory embedding, where each box is a point with 7-dim geometry and 1-dim time encoding.\n\nNext, the computed object embedding and the trajectory embedding are concatenated to form the joint embedding which will then be passed through a box regression network to predict the object box at frame $T$.\n% To encourage contributions from both branches, we also follow~\\cite{wang2019makes} and pass the trajectory embedding and the object embedding to two additional box regression sub-networks to predict boxes independently (not shown in the illustration).\n% Details of the architecture, losses and model training are described in the supplementary.\n\n% As the predicted box is in the box coordinate at from the detector box at frame $T$, we finally reverse the transformation and turns it back to the world coordinate.\n\n% \\paragraph{Losses.}\n% Similar to the static auto labeling model, we have two types of losses, the segmentation loss and the box regression loss. The box regression outputs are defined in the same way as that for the static objects. The final loss $L = L_{\\text{seg}} + v_1 L_\\text{box-traj} + v_2 L_\\text{box-obj-pc} + v_3 L_\\text{box-joint}$ where we have three box losses from the trajectory head, the object point cloud head and the joint head respectively. The $v_i$, $i=1,2,3$ are the weights for the loss terms to achieve a balanced learning of the three types of embeddings.\n\n% \\paragraph{Training and data augmentation.}\n% During training, we randomly select the center frame from each dynamic object track. If the context size is less than the required sequence length $2r+1$ or $2s+1$, we use placeholder points and boxes (all zeros). Both points and boxes are randomly flipped along the $X$ and $Y$ axis with a $50\\%$ chance and randomly rotated around the $Z$ axis by $\\text{Uniform}[-10, 10]$ degrees.\n\n\n% \\subsubsection{Auto Quality Control through IoU Prediction}\n\n% For many offboard perception use cases, it is important to accurately rank the predictions by their quality. While the detector outputs confidence scores for the boxes, those scores have two limitations. First, the scores usually have a low correlation with the box quality (the closeness to the ground truth box) as they are often supervised in the region proposal network to get a high recall (as long as the region appears to be an object, the score will be high). Second, the detector score is estimated based on the single frame point or just a very few frames of points, which cannot accurately reflect the quality of the box in the object-centric auto labeling.\n\n% To get an object score that is more correlated with the box quality, we design a Auto Quality Check (AutoQC) model (it is named this way as it emulates the process how a human verifier examines the labels). The model takes both the predicted 3D bounding box and the object track point clouds as input. It transforms the object points to the normalized object coordinate system (NOCS) [CITE] according to the predicted box. Specifically, we transform the coordinate such that the origin is the box center and the box heading is along the $+X$ axis and the $X,Y,Z$ are scaled by the box's length, width and height respectively. After the transformation, if the object point cloud extrudes the $[-0.5, 0.5]^3$ space or looks tilted along any axis, it indicates the box is of low quality and may not be a good fit to the point cloud.\n\n% Taking such transformed point cloud in the NOCS, the AutoQC model regresses the IoU of the box with the ground truth box using $L_1$ loss. The regressed IoU becomes the AutoQC score for the object, which can be used for downstream applications or be used in the detection AP metric evaluation.\n\n% (this will be a short section describing the IoU prediction head)\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n% \\todo{1st version ddl: 10/30 11PM; 2nd version ddl: 11/4 11PM; Owners: rqi, yinzhou, najibi (Overview: introduce the dataset and the outline of the experimental section.)}\n\n\nWe start the section by comparing our offboard 3D Auto Labeling with state-of-the-art 3D object detectors in Sec.~\\ref{sec:exp:sota}. In Sec.~\\ref{sec:exp:human} we compare the auto labels with the human labels. In Sec.~\\ref{sec:exp:semi_supervised}, we show how the auto labels can be used to supervise a student model to achieve improved performance under low-label regime or in another domain. We provide analysis of the multi-frame detector in Sec.~\\ref{sec:exp:multi_frame_detector} and analysis experiments to validate our designs of the object-centric auto labeling models in Sec.~\\ref{sec:exp:analysis} and finally visualize the results in Sec.~\\ref{sec:exp:visualization}.\n\n\\paragraph{Dataset}\n\\label{sec:exp:dataset}\nWe evaluate our approach using the challenging Waymo Open Dataset (WOD)~\\cite{sun2020scalability}, as it provides a large collection of LiDAR sequences, with 3D labels available for each frame. The dataset includes a total number of 1150 sequences with 798 for training, 202 for validation and 150 for testing. Each LiDAR sequence lasts around 20 seconds with a sampling frequency at 10Hz.\n% The effective 3D annotation range is 75m.\nFor our experiments, we evaluate both 3D and bird's eye view (BEV) object detection metrics for vehicles and pedestrians.\n% Without extra explanation, our models are trained using the sequences in the train set and evaluate the models on the val set with a focus on the vehicle class as it contains the most number of instances.\n\n% WOD~\\cite{sun2020scalability} offers information collected from a set of well-calibrated sensors on a self-driving car, including multiple LiDARs and cameras. It captures multiple major cities (Mountain View, San Francisco, Phoenix) in the U.S., under a variety of weather conditions (dry, rain) and across different times of the day (day, night). Specifically, the dataset includes a total number of 1150 sequences, where 798 sequences are provided as training split, 202 sequences are allocated as validation split and the remaining 150 sequences are used for testing. Each LiDAR sequence lasts about 20 seconds and the sampling frequency is 10Hz. The effective 3D annotation radius is 75m for all object classes. For our experiments, we evaluate both 3D and BEV object detection metrics for vehicles, pedestrians and cyclists \\najibi{Remember to remove cyclists in case we decided to not report}. \n\n% Without extra explanation, we will be focusing on the metrics of the vehicle category as it contains the most number of instances.\n\n\n",
                "subsection 5.1": {
                    "name": "Comparing with State-of-the-art Detectors",
                    "content": "\n\\label{sec:exp:sota}\n\n% \\todo{the title of the section to be discussed -- depending on how we claim the contribution.}\n% \\rqi{Emphasize the input are not the same.}\n% \\rqi{Explain why we use 0.8 IoU threshold.}\n\n% In Table~\\ref{tab:main_detection_results}, we show comparisons of our 3D object detectors and the 3D Auto Labeling output using the 3D detection metrics on the Waymo Open Dataset val set. Note that we not only evaluate using the standard metrics: 3D AP and bird's eye view (BEV) AP at the 0.7 IoU threshold (for 3D IoU and BEV 2D IoU), but also evaluate at a higher standard: 3D AP and BEV AP at the 0.8 IoU threshold. The selection of 0.8 is backed by our human label study~\\ref{sec:exp:human} where we found human labelers are able to achieve an average IoU around 0.8 or higher. This higher standard is stress testing the models' detection quality.\n\nIn Table~\\ref{tab:main_detection_results}, we show comparisons of our 3D object detectors and the 3D Auto Labeling with various single-frame and multi-frame based detectors, under both the common standard IoU threshold and a higher standard IoU threshold to pressure test the models.\n\n% We compare with three representative Lidar-based single-frame detectors: PointPillar~\\cite{lang2019pointpillars} which is a real-time detector based on BEV images (with PointNet encoded features); Multi-view Fusion (MVF)~\\cite{zhou2020end} that extends PointPillar with multi-view point cloud features; and a recent state-of-the-art detector PVRCNN~\\cite{shi2020pv} that makes use of a few different types of point cloud feature learning operators.\n\nWe show that our single-frame MVF++ has already outperformed the prior art single-frame detector PVRCNN~\\cite{shi2020pv}. The multi-frame version of the MVF++, as a baseline of the offboard 3D detection methods, significantly improves upon the single-frame MVF++ thanks to the extra information from the context frames.\n\nFor vehicles, comparing the last three rows, our complete 3D Auto Labeling pipeline, which leverages the multi-frame MVF++ and the object-centric auto labeling models, further improves the detection quality especially in the higher standard at IoU threshold of 0.8. It improves the 3D AP@0.8 significantly by $\\textbf{14.52}$ points compared to the single-frame MVF++ and by $\\textbf{8.39}$ points compared to the multi-frame MVF++, which is already very powerful by itself. These results show the great potential of leveraging the long sequences of point clouds for offboard perception.\n\nWe also show the detection AP for the pedestrian class, where we consistently observe the leading performance of the 3D Auto Labeling pipeline especially at the higher localization standard (IoU=0.6) with $\\textbf{7.67}$ points gain compared to the single-frame MVF++ and $\\textbf{3.13}$ points gain compared to the multi-frame MVF++.\n\n% As a reference, we also report the APs at the test set in Table~\\ref{tab:all_category_detection_test}.\n% More detailed breakdowns of the APs can be found in the supplementary.\n\n% Main message: our single-frame detector, multi-frame detector and the 3D auto labeling are all their best in its own input type. We significantly outperform all existing detector and our strong multi-frame detector baselines, esp. in AP with a higher IoU standard.\n\n% Main results vehicle only Tab.~\\ref{tab:main_detection_results} \\rqi{An alternative is that we only compare with a few detector and show the results as bar plots instead of as a table.}\n\n% All category results val set Tab.~\\ref{tab:all_category_detection_val}\n% All category results test set Tab.~\\ref{tab:all_category_detection_test}\n\n% \\begin{figure*}[h]\n%     \\centering\n%     \\includegraphics[width=\\linewidth, height=2in]{./fig/placeholder}\n%     \\caption{\\todo{1st version ddl: 10/29 11PM; 2nd version ddl: 10/30 6PM; Owners: rqi, yinzhou} The table for comparing with published papers and our own baselines on detector APs of vehicles (on the OD val set). Methods in comparison include: single-frame PointPillar, 3DSSD, PointPillar, PointRCNN, PVRCNN etc. Our single-frame improved MVF, our multi-frame MVF (5 frames) and our full pipeline 3D auto labeling. The metrics are 3D AP@0.7, 3D AP@0.8 and APH as well. Maybe add BEV AP as well.}\n%     \\label{tab:main_detection_results}\n% \\end{figure*}\n\n% \\begin{table*}[t]\n%     \\centering\n%     \\setlength{\\tabcolsep}{3pt}\n%     \\small\n%     \\begin{tabular}{l|c|c|c|c|c}\n%     \\toprule\n%          Method & #frames & \\multicolumn{2}{|c|}{3D AP/APH} & \\multicolumn{2}{c}{BEV AP/APH} \\\\ \n%          & & IoU=0.7 & IoU=0.8 & IoU=0.7 & IoU=0.8 \\\\ \n%                 %   Method & #frames & \\multicolumn{2}{c}{3D AP/APH}3D AP/APH (IoU=0.7) & 3D AP/APH (IoU=0.8) & BEV AP/APH (IoU=0.7) & BEV AP/APH (IoU=0.8) \\\\\n%          \\toprule\n%          StarNet~\\cite{REF:StarNet_2019} & 1 & 53.70 / - & & & \\\\\n%          PointPillar~\\cite{lang2019pointpillars} & 1 & 56.62 / - & & 75.57 / - & \\\\\n%          Multi-view fusion (MVF)~\\cite{zhou2020end} & 1 & 62.93 / - & & 80.40 / - & \\\\\n%          AFDET~\\cite{REF:AFDET_CVPRW2020} & 1 & 63.69 / - & & & \\\\\n%          ConvLSTM~\\cite{REF:ConvLSTM_ECCV2020} & 4 & 63.60 / - & & & \\\\\n%          RCD~\\cite{REF:bewley2020range} & 1 & 68.95 / 68.52 & & 82.09 / 81.39 & \\\\\n%          PillarNet~\\cite{REF:PillarNet_ECCV2020} & 1 & 69.8 / - & & 87.11 / - & \\\\\n%          PV-RCNN~\\cite{shi2020pv}$^{\\star}$ & 1 & 70.47 / 69.78 & 39.16 / 38.90 & 83.43 / 82.34 & 69.52 / 68.85\\\\\n%          \\midrule\n%         %  Single-frame MVF++ (Ours) & 1 & 71.99 / 68.22 & 39.35 / 37.41 & 86.38 / 81.60 & 72.17 / 68.39 \\\\ (old)\n%          Single-frame MVF++ (Ours) & 1 & 74.64 / 74.21 & 43.30 / 43.10 & 87.59 / 86.98 & 75.30 / 74.87 \\\\ \n%          Multi-frame MVF++ w. TTA (Ours) & 5 & 79.73 / 79.33 & 49.43 / 49.23 & 91.93 / 91.38 & 80.33 / 79.92 \\\\\n%          3D Auto Labeling (Ours) & $\\infty$ & \\textbf{83.99} / \\textbf{83.50} & \\textbf{56.97} / \\textbf{56.68} & \\textbf{93.22} / \\textbf{92.65} & \\textbf{84.61} / \\textbf{84.15}\\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\todo{1st version ddl: 10/29 11PM; 2nd version ddl: 10/30 6PM; Owners: rqi, yinzhou} The table for comparing with published papers and our own baselines on detector APs of vehicles (on the WOD val set). Methods in comparison include: single-frame PointPillar, 3DSSD, PointPillar, PointRCNN, PVRCNN etc. Our single-frame improved MVF, our multi-frame MVF (5 frames) and our full pipeline 3D auto labeling. The metrics are 3D AP@0.7, 3D AP@0.8 and APH as well. $^{\\star}$ indicates our reimplementation. Results are sorted based on 3D AP@0.7. \\yinzhou{Consider differentiate with and with TTA.}}\n%     \\label{tab:main_detection_results}\n% \\end{table*}\n\n\n% \\begin{figure}[h]\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{./fig/placeholder}\n%     \\caption{\\todo{1st version ddl: 10/29 11PM; 2nd version ddl: 10/30 6PM; Owners: rqi} A smaller table showing AP on OD \\emph{val} set for all categories with AP and APH at standard IoU thresholds.}\n%     \\label{tab:all_category_detection_val}\n% \\end{figure}\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{l|c|c|c}\n%     \\toprule\n%          Method & Vehicle & Pedestrian & Cyclist  \\\\ \\toprule\n%          PointPillar~\\cite{lang2019pointpillars} & & & \\\\ \n%          MVF~\\cite{zhou2020end} & 62.93 / - & 65.33 / - & \\\\\n%          PV-RCNN~\\cite{shi2020pv} & 70.47 & 65.34 & \\\\ \\midrule\n%          3DAL (Ours) & 83.99 / 83.50 & 82.9 / 79.8 & 74.8 / 74.2\\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     % \\caption{\\todo{1st version ddl: 10/29 11PM; 2nd version ddl: 10/30 6PM; Owners: rqi} A smaller table showing AP on OD \\emph{val} set for all categories with AP and APH at standard IoU thresholds (3D IoU=0.5).}\n%         \\caption{\\todo{1st version ddl: 10/29 11PM; 2nd version ddl: 10/30 6PM; Owners: rqi} A smaller table showing AP on OD \\emph{val} set for all categories with AP at \\textcolor{red}{at different IoU thresholds} at standard IoU thresholds (3D IoU=0.5).}\n%     \\label{tab:all_category_detection_val}\n% \\end{table}\n\n\n% \\begin{figure}[h]\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{./fig/placeholder}\n%     \\caption{\\todo{1st version ddl: 10/30 6PM; 2nd version ddl: 11/1 6PM; Owners: rqi} A smaller table showing AP on OD \\emph{test} set for all categories with AP and APH at standard IoU thresholds.}\n%     \\label{tab:all_category_detection_test}\n% \\end{figure}\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{l|c|c|c}\n%     \\toprule\n%          Method & Vehicle & Pedestrian & Cyclist  \\\\ \\toprule\n%          PointPillar~\\cite{lang2019pointpillars} & & & \\\\ \n%          MVF~\\cite{zhou2020end} & & & \\\\\n%          PVRCNN~\\cite{shi2020pv} & & & \\\\ \\midrule\n%          3DAL (Ours) & 85.47 / 85.07 &  & \\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\todo{1st version ddl: 10/30 6PM; 2nd version ddl: 11/1 6PM; Owners: rqi} A smaller table showing AP on OD \\emph{test} set for all categories with AP and APH at standard IoU thresholds.}\n%     \\label{tab:all_category_detection_test}\n% \\end{table}\n\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Comparing with Human Labels",
                    "content": "\n\\label{sec:exp:human}\n\n% \\paragraph{The human label study.}\nIn many perception domains such as image classification and speech recognition, researchers have collected data to understand humans' capability~\\cite{russakovsky2015imagenet,deshmukh1996benchmarking,lippmann1997speech}. However, to the best of our knowledge, no such study exists for 3D recognition especially for 3D object detection.\nTo fill this gap, we conducted a small-scale human label study on the Waymo Open Dataset to understand the capability of human in recognizing objects in a dynamic 3D scene. We randomly selected 5 sequences from the Waymo Open Dataset \\emph{val} set and asked three experienced labelers to re-label each sequence independently (with the same labeling protocol as WOD).\n% The labeling followed the same protocol as that for the Waymo Open Dataset so the results are directly comparable to the public ground truth. More details about the label collection are in the supplementary.\n\n% (different runs may be labeled by a different set of 3 labelers, there are in total 12 labelers involved)\n% The labelers have all been trained and are experienced in annotating 3D object bounding boxes in video sequences of the sensor data, so they should represent the fair human performance. Besides Lidar point cloud video, they also have access to the camera images in their annotations.\n\n% In total, there are 15 labeling tasks. Note that 3D labeling is a very costly process, completing these 15 tasks takes several hundred human hours even when the labelers are already fluent in such tasks.\n\n% \\paragraph{How consistent are human labels?} Even for humans, 3D bounding box labeling can be challenging as the input point clouds are often partial and occluded. To understand how consistent human labels are, we can compare labels from one labeler with the label from the other and measure the 3D box consistency by their IoU. Since we already have the verified public ground truth, we can compare the 3 sets of labels with the public ground truth and get the average box IoU for all objects that are matched (due to the occlusion, some objects may be labeled or not labeled by a specific labeler). Specifically, for human boxes for which we cannot find a ground truth box with more than 0.1 IoU overlap with them (false positive or false negative), they are ignored and not counted in the computation.\n\n% The statistics are summarized in Table~\\ref{tab:human_mean_iou}. Surprisingly, human labels do not have the consistency some of us may expect (e.g. 95\\% IoU). Due to the inherent uncertainty of the problem, even humans can only achieve around XX\\% 3D IoU in their box consistency. What is intuitive is that for objects that are close by the mean IoU is significantly higher as we observe more points and a more complete viewpoints for them. The BEV 2D IoU is also higher than the 3D IoU we do not require the correct height estimation in the BEV box, which simplifies the problem.\n\n% To have a rough understanding of how the boxes generated by our 3D Auto Labeling pipeline compare with human labels, we also compute the average IoU of auto labels with the Open Dataset ground truth. Note that those numbers are not directly comparable to the average human IoUs as they are computed on a slightly different set of objects (due to the false positives and false negatives). However, it still gives us an understanding that the auto labels are already pretty close in quality to human labels. For a more fair comparison, in the next paragraph we will evaluate the human labels and auto labels using the detector metric: average precision.\n\n% \\paragraph{What is the Average Precision (AP) of human labels?} As nearly all 3D object detection papers are comparing their models using average precision. We are curious to know how well human labelers perform at this metric.\n% In AP calculation, we rank the detection boxes by their ``scores'' and set various score thresholds to compute the precision and recall at each operation point. However, human annotations don't have any scores therefore we cannot directly evaluate human AP based on the labels. To solve this issue, we experiment with a few heuristic ways to assign a score for each human label including (1) the inverse distance of the box to the sensor; (2) the number of points within the box; (3) the visibility ratio of the object (by computing the angular distributions of the rays from the points to the box center) and a few others. We found that using the number of points as human scores leads to the highest human AP among all the choices, so we will use such score generation for our reported numbers.\n% Note that to verify the effectiveness of such score assignment, we also use the same rule to assign scores to the machine generated labels and observe very little change in the AP \\rqi{shall we report?}.\n\n% \\paragraph{Comparing human labels and auto labels.}\nIn Table~\\ref{tab:human_label_study}, we report the mean AP of human labels and auto labels across the 5 sequences.\n% Since we have different labelers annotate each segment, we cannot compute an overall AP for each labeler, instead we compute the average precision for each segment and for each labeler separately. The final reported AP are the average of the 15 AP numbers.\nWith the common 3D AP@0.7 (L1) metric, the auto labels are only around 1 point lower than the average labeler, although the gap is slightly larger in the more strict 3D AP@0.8 metric. With some visualization, we found the larger gap is mostly caused by inaccurate heights. The comparisons with the BEV AP@0.8 metric verifies our observation: when we don't consider height, the auto labels even outperform the average human labels by $1.28$ points.\n\nWith such high quality, we believe the auto labels can be used to pre-label point cloud sequences to assist and accelerate human labeling, or be used directly to train light-weight student models as shown in the following section.\n% In the next section, we will also see how we can directly use auto labels to train light-weight student models.\n\n% \\rqi{Shall we say something more to hint auto labels can be used for labeling assistance?}\n% Main message: our 3D auto labeling results are on par with human label quality!\n\n% \\begin{figure}[h]\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{./fig/placeholder}\n%     \\caption{\\todo{1st version ddl: 10/29 11PM; 2nd version ddl: 10/30 6PM; Owners: rqi} The table for the human label study (comparing human label mIoU and AP in vehicles with auto labels).}\n%     \\label{tab:human_label_study}\n% \\end{figure}\n\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{c|c|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}}\n%     \\toprule\n%          IoU type & Label type & all & 0-30m & 30-50m & 50m+ \\\\ \\midrule\n%          \\multirow{2}{*}{valid boxes} & human & 25,641 & 11,543 & 7,963 & 6,135  \\\\\n%          & auto & 24,146 & 11,360 & 7,448 & 5,338 \\\\\n%          \\midrule\n%          \\multirow{2}{*}{3D mIoU} & human & 80.92 & 85.78 & 80.29 & 72.59 \\\\\n%          & auto & 80.29 & 84.04 & 77.45 & 76.28 \\\\\n%          \\midrule\n%          \\multirow{2}{*}{BEV mIoU} & human & 87.98 & 91.26 & 87.31 & 82.68 \\\\ \n%          & auto & 87.50 & 90.36 & 85.09 & 84.78 \\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{The mean IoU of human labels and auto labels compared with the Waymo Open Dataset ground truth. Note that since different labels (human or machine) annotate different number of objects for each frame. So those numbers are not directly comparable. They are summarized here for a reference. For a more fair comparison between human and auto labels, see the Average Precision comparison table. We only evaluate using ground truth boxes with at least one point in it. valid boxes are the human or auto labels that have an BEV IoU larger than 0.03 with any GT box. \\rqi{Note that due to the lower recall of auto labels in distant object detection, the mIoU of auto label looks higher. This could be misleading.}}\n%     \\label{tab:human_mean_iou}\n% \\end{table}\n\n\n% \\begin{table}[t!]\n%     \\centering\n%     \\resizebox{\\columnwidth}{!}{\n%     \\begin{tabu}{c|c|c|c}\n%     \\toprule\n%         Training Data & Test Data & 3D AP & BEV AP \\\\\n%         \\toprule\n%         100\\% OD \\textit{train} (Human) & OD \\textit{val} & 71.2  & 86.9  \\\\\n%         \\tabucline [0.1pt on 4pt off 2pt]{1-4} \n%         10\\% OD \\textit{train} (Human) & OD \\textit{val} & 64.3 & 81.2 \\\\\n%         \\tabucline [0.1pt on 4pt off 2pt]{1-4} \n%         % 90\\% MVF++ & 68.3 & 59.5 \\\\\n        \n%          10\\% OD \\textit{train} (Human) & \\multirow{2}{*}{OD \\textit{val}} & \\multirow{2}{*}{70.0} & \\multirow{2}{*}{86.4} \\\\\n%          + 90\\% OD \\textit{train} (\\textbf{3DAL}) & & & \\\\\n         \n%          \\midrule\n         \n%          100 \\% OD \\textit{train} (Human) & Domain \\textit{test} & 59.4 & \\textit{N/A} \\\\ \n%          \\tabucline [0.1pt on 4pt off 2pt]{1-4}\n%          100 \\% OD \\textit{train} (Human) & \\multirow{2}{*}{Domain \\textit{test}} & \\multirow{2}{*}{60.3} & \\multirow{2}{*}{\\textit{N/A}} \\\\\n%          + New domain (Self Anno.) & & & \\\\\n%          \\tabucline [0.1pt on 4pt off 2pt]{1-4}\n%          100 \\% OD \\textit{train} (Human) & \\multirow{2}{*}{Domain \\textit{test}} & \\multirow{2}{*}{64.2} & \\multirow{2}{*}{\\textit{N/A}} \\\\\n%          + New domain (\\textbf{3DAL}) & & & \\\\\n%          \\bottomrule\n%     \\end{tabu}\n%     }\n%     \\caption{\\textbf{Results of semi-supervised learning with auto labels.} Metrics are 3D and BEV AP for vehicles on the Waymo Open Dataset. The type of annotation is reported in parenthesis. Please note, test set BEV AP is not provided by the submission server.}\n%     \\label{tab:semi_supervisedv2}\n% \\end{table}\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Applications to Semi-supervised Learning",
                    "content": "\n\\label{sec:exp:semi_supervised}\n% In the previous sections, we showed that our 3D auto-labeling pipeline improves the performance of the state-of-the-art 3D detectors and achieves on-par performance with human annotators. Here, we further study the effectiveness of our auto labeling pipeline in the task of semi-supervised learning to train a real-time onboard student under two settings: having additional data in the same domain as our auto labeling training set, or additional data from a new domain.\nIn this section, we study the effectiveness of our auto labeling pipeline in the task of semi-supervised learning to train a student model under two settings: intra-domain and cross-domain. We choose the student model as a single-frame MVF++ detector that can run in real-time.\n\n% \\paragraph{Inter-domain semi-supervised learning.}\n% For this task,\nFor the \\emph{intra-domain semi-supervised learning},\nwe randomly select 10\\% sequences (79 ones) in the main WOD training set to train our 3D Auto Labeling (3DAL) pipeline. Once trained, we apply it to the rest 90\\% sequences (719 ones) in the main training set to generate ``auto labels'' (we only keep boxes with scores higher than $0.1$).\n% and boxes with scores higher than a threshold are kept as ground-truth objects (we use a score threshold of 0.1, but the results are fairly robust to this choice).\n% As the student, we choose a single-frame MVF detector that can be deployed on the car in real-time.\n% The results are presented in the first three rows of Table~\\ref{tab:semi_supervisedv2}.\n% By reducing the human annotations to 10\\%, the 3D AP and BEV AP drop by $\\sim$ 7\\% and $\\sim$ 5.5\\% respectively.\n% The third row shows the results for the semi-supervised setting, where our 3DAL pipeline is deployed to generate pseudo labels for the remaining 90\\% unlabeled data.\n% Using auto labels (third row) improves the performance of the student by more than 5\\% with respect to both metrics.\n% More remarkably, our pipeline with only 10\\% annotations reduces the BEV AP gap to 0.5\\% and pushes the 3D AP to the $\\sim$ 1\\% margin of fully annotated training, demonstrating the quality of our pseudo labels.\nIn Table~\\ref{tab:semi_supervisedv2} (first two rows), we see that reducing the human annotations to 10\\% significantly lowers the student model's performance. However, when we use auto labels, the student model trained on 10\\% human labels and 90\\% auto labels can get similar performance compared to using 100\\% human labels (AP gaps smaller than 1 point), demonstrating superb data efficiency auto labels can provide.\n\n% \\paragraph{Cross-domain semi-supervised learning.}\n% Compared to inter-domain, in this experiment,\nFor the \\emph{cross-domain semi-supervised learning},\nthe teacher auto labels data from an unseen domain. The teacher is trained on the main WOD \\emph{train} set, and auto labels the domain adaptation WOD \\emph{train} and \\emph{unlabeled} sets (separate 680 sequences from the main WOD). The student is then trained on the union of these three sets. Evaluations are on the domain adaptation \\emph{test} set.\nThe last three rows of Table~\\ref{tab:semi_supervisedv2} show the results.\nWithout using any data from the new domain, the student gets an AP of $59.4$.\n% Using the student itself to label additional data in the new domain and adding it to the training set only improves the results by $\\sim 1$ point. However, as shown in the last row, using our 3DAL to label the new domain data for the student improves its 3D AP by $\\sim 5$ points, showing the superiority of 3DAL labels even for a domain which it was not trained for.\nWhile using the student to self-label slightly helps (improves the results by $\\sim 1$ point), using our 3DAL to auto label the new domain significantly improves the student AP by $\\sim 5$ points.\n\n%More precisely, out of 798 segments in the training set, 79 run segments ($\\sim$ 15K frames) are human-annotated and 719 segments ($\\sim$ 142K frames) are auto labeled.  \n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{c|c|c|c}\n%     \\toprule\n%           Annotation & AP/APH L1 & AP/APH L2  \\\\ \\toprule\n%         100\\% human & 71.2 & 62.2 \\\\\n%         \\midrule\n%         10\\% human & 64.3 & 55.6 \\\\\n         \n%         % 90\\% MVF++ & 68.3 & 59.5 \\\\\n         \n%         10\\% human + 90\\% 3DAL & 69.9  & 61.0 \\\\\n    \n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\textbf{Results of semi-supervised learning with auto labels.}}\n%     \\label{tab:semi_supervisedv2}\n% \\end{table}\n\n\n%The results are presented in Table \\ref{tab:semi_supervisedv2}. As reported in the first two rows, by reducing the annotated part of the dataset to 10\\%, the 3D AP and BEV AP drop by $\\sim$ 7\\% and $\\sim$ 5.5\\% respectively. The last row shows the results for the semi-supervised setting, where our 3DAL pipeline is deployed to generate pseudo labels for the remaining 90\\% unlabeled data. Our pseudo labels improve the performance of the student by more than 5\\% with respect to both metrics. Moreover, our pipeline with only 10\\% annotations reduces the BEV AP gap to 0.5\\% and pushes the 3D AP to the $\\sim$ 1\\% margin of fully annotated training, demonstrating the quality of our pseudo labels. \n\n%The results are presented in Table \\ref{tab:semi_supervisedv2}. When using only 10\\% of the training set, we improve the baseline by {\\color{red} X\\%}. Interestingly, using only 10\\% human labels and 90\\% auto labels, the student is able to achieve {\\color{red} Y\\%} better L2 AP compared to a model trained with 100\\% human labels. We attribute this to removing hard examples from the training set that even our strong teacher is not able to detect, letting the student focusing on the rest. It should be noted that by increasing the capacity of the student or reducing the strength of the teacher (\\eg by decreasing the number of annotated examples), this improvement diminishes. When we reduce the human annotations to 5\\%, we still match the performance of the student trained on 100\\% human-annotated dataset, but we do not see improvements. \\najibi{I included the last couple of sentences to avoid possible reviewer complaints, but we may remove them.}\n\n%Main message: the auto labels can be effectively used to train student models.\n\n\n\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabu}{c|c||c|c}\n%     \\toprule\n%          Model & Annotation & AP/APH L1 & AP/APH L2  \\\\ \\toprule\n%          PointPillar & 100\\% human & 60.0 / 59.2 & 53.2 / 52.5 \\\\\n%         \\midrule\n%          MVF++ (Ours) & 10\\% human & 70.7 / 70.3 & 63.5 / 63.1\\\\\n         \n%          3DAL (Ours)  & 10\\% human & 78.1 / 77.6  & 69.5 / 69.0 \\\\\n         \n%          PointPillar & 10\\% human & 58.4 / 57.7  & 51.5 / 50.8 \\\\\n         \n%          \\midrule\n         \n%          %\\tabucline[0.4pt off 2pt]{-}\n%          PointPillar & 10\\% human + &  \\multirow{2}{*}{63.2 / 62.6} & \\multirow{2}{*}{54.6 / 54.1}\\\\\n%          (student) & 90\\% MVF++ & &\\\\\n         \n         \n%          \\tabucline[0.4pt off 2pt]{-}\n%          PointPillar & 10\\% human + &  \\multirow{2}{*}{ X / Y} & \\multirow{2}{*}{X / Y}\\\\\n%          (student) & 90\\% 3DAL & &\\\\\n%          \\bottomrule\n%     \\end{tabu}\n%     \\caption{\\todo{1st version ddl: 10/30 11PM (table design and baselines); 2nd version ddl: 11/4 6PM; Owners: najibi} \\najibi{We may remove numbers for MVF as the teacher} The table for the semi-supervised learning.}\n%     \\label{tab:semi_supervised}\n% \\end{table}\n\n"
                },
                "subsection 5.4": {
                    "name": "Analysis of the Multi-frame Detector",
                    "content": "\n\\label{sec:exp:multi_frame_detector}\n\nTable~\\ref{tab:detector_ablation} shows the ablations of our proposed MVF++ detectors. We see that the offboard techniques such as the model capacity increase ($+3.08$ AP@0.7), using point clouds from 5 frames as input ($+1.70$ AP@0.7) and test time augmentation ($+3.39$ AP@0.7) are all very effective in improving the detection quality.\n\nTable~\\ref{tab:detection_AP_vs_frames} shows how the number of consecutive input frames impacts the detection APs. The gains of adding frames quickly diminishes as the number of frames increases: \\eg while the AP@0.8 improves by $0.81$ from 1 to 2 frames, the gain from 4 to 5 frames is only $0.14$ point.\n\n% Main message: Multi-frame detectors are effective but are still limited in how much contexts it can use (not naively scalable to more frames → diminishing returns after 5 frames).\n% Fig.~\\ref{fig:multi_frame_detection}\n\n% \\begin{figure}[h]\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{./fig/placeholder}\n%     \\caption{\\todo{1st version ddl: 10/30 11PM; 2nd version ddl: 11/1 6PM; Owners: yinzhou} A plot showing the diminishing returns of the number of frames for the multi-frame detector. X-axis: number of frames of 1, 2, 3, 4, 5 and 10. Y-axis: 3D AP of vehicles on OD val set. Two curves, one for AP@0.7 and another for AP@0.8.}\n%     \\label{fig:multi_frame_detection}\n% \\end{figure}\n\n% \\paragraph{Analysis of the multi-frame detector.}\n% \\begin{itemize}\n%     \\item The effect of test time augmentation.\n%     \\item The effect of the anchor free design.\n%     \\item Large detector model.\n%     \\item Non-causal detector.\n% \\end{itemize}\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.5": {
                    "name": "Analysis of Object Auto Labeling Models",
                    "content": "\n\\label{sec:exp:analysis}\n\n% \\todo{1st version ddl: 10/30 11PM (table design and outline); 2nd version ddl: 11/1 11PM; Owners: rqi}\n\nWe evaluate the object auto labeling models using the box accuracy metric under two IoU thresholds 0.7 and 0.8 on the Waymo Open Dataset \\emph{val} set. The predicted box is considered correct if its IoU with the ground truth is higher than the threshold. More analysis is in Sec.~\\ref{sec:supp:analysis}.\n% passes the 3D IoU threshold compared with the ground truth box.\n% ~\\footnote{More analysis experiments on the impact of context sizes and hyperparameters can be found in the supplementary.}\n\n\\paragraph{Ablations of the static object auto labeling}\nIn table~\\ref{tab:static_ablation} we can see the importance of the initial coordinate transform (to the box coordinate), and the foreground segmentation network in the first 3 rows. In the 4th and the 5th rows, we see the gains of using iterative box re-estimation and test time augmentation respectively.\n\n\n\n% \\begin{table}[t!]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{l|c|c}\n%          \\toprule\n%          method & context frames & Acc@0.7/0.8 \\\\\n%          \\midrule\n%         Single-frame MVF++ & $[-0,+0]$ & 67.17 / 36.61 \\\\\n%         Multi-frame MVF++ & $[-4,+0]$  & 73.96 / 43.56 \\\\\n%         \\midrule\n%          \\multirow{4}{*}{Auto labeling (non-causal)} & $[-0,+0]$ & 78.13 / 50.30 \\\\\n%          & $[-2,+2]$ & 79.60 / 52.52 \\\\\n%          & $[-5,+5]$ & 80.48 / 55.02 \\\\\n%          & all & \\textbf{82.28} / \\textbf{56.92} \\\\\n%          \\midrule\n%          Auto labeling (causal) & all history & 77.56 / 49.21\\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\textbf{Effects of context sizes for static object auto labeling.} Note that for a causal model, we cannot output a single box for a static object -- we have to output the best estimation for every frame using the current frame and the history frames. We compute the average accuracy across all frames for the causal case.}\n%     \\label{tab:static_context_size}\n% \\end{table}\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{c|c|c}\n%          \\toprule\n%          ref frame & context frames & Acc@0.7/0.8 \\\\\n%          \\midrule\n%          \\multirow{3}{*}{highest score} & $\\{0\\}$ & \\\\\n%          & $\\{-2,-1,0,1,2\\}$ & \\\\\n%          & $\\{-5,...,-1,0,1,...,5\\}$ & \\\\\n%          & $\\infty$ & \\\\\n%          \\midrule\n%          every frame & $\\{-\\infty, ..., 0\\}$ (causal) & \\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{Effects of the context size for static object auto labeling. Note that for a causal model, we cannot output a single box for a static object -- we have to output the best estimation for every frame using the frame and history frames. We compute the average accuracy across all frames for the causal case.}\n%     \\label{tab:static_context_size}\n% \\end{table}\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{c|c|c}\n%          \\toprule\n%          ref frame & context frames & Acc@0.7/0.8 \\\\\n%          \\midrule\n%          \\multirow{3}{*}{all highest score} & $[-0,+0]$ & 78.13 / 50.30 \\\\\n%          & $[-2,+2]$ & 79.60 / 52.52 \\\\\n%          & $[-5,+5]$ & 80.48 / 55.02 \\\\\n%          & all & \\textbf{81.67} / \\textbf{57.12} \\\\\n%          \\midrule\n%          past highest score & all history & 77.56 / 49.21\\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\textbf{Effects of context sizes for static object auto labeling.} Note that for a causal model, we cannot output a single box for a static object -- we have to output the best estimation for every frame using the current frame and the history frames. We compute the average accuracy across all frames for the causal case.}\n%     \\label{tab:static_context_size}\n% \\end{table}\n\n% Auto labeling models:\n% \\begin{itemize}\n%     \\item Naive box size selection/averaging.\n%     \\item Different design choices and ablations of auto labeling models\n%     \\item The impact of context sizes\n%     \\item Causal models?\n% \\end{itemize}\n\n\\paragraph{Alternative designs of the dynamic object auto labeling}\nTable~\\ref{tab:dynamic_alternative} ablates the design of the dynamic object auto labeling model. For the align \\& refine model, we use the multi-frame MVF++ detector boxes to ``align'' the object point clouds from the nearby frames ($[-2,+2]$) to the center frame. For each context frame, we transform the coordinate by aligning the center and heading of the context frame boxes to the center frame box. The model using un-aligned point clouds (in the center frame's coordinate, from $[-2,+2]$ context frames), second row, actually gets higher accuracy (second row) than the aligned one.\nThe model taking only the box sequence (third row) as input performs reasonably as well, by leveraging the trajectory shape and the box sizes. Our model jointly using the multi-frame object point clouds and the box sequences gets the best accuracy.\n\n\n\n\n\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{l|c}\n%     \\toprule\n%         method & Acc@0.7/0.8 \\\\\n%         \\midrule\n%         Single-frame MVF++ & 80.07 / 57.71 \\\\\n%         Multi-frame MVF++ & 82.21 / 59.52 \\\\\n%         \\midrule\n%          Align \\& refine (5 frames)&  83.33 / 60.69 \\\\\n%          Box sequence only (11 frames)& 83.13 / 58.96 \\\\\n%          Points only (1 frame) & 82.01 / 58.61 \\\\\n%          Points only (5 frames) & 83.79 / 61.95 \\\\\n%          \\midrule\n%          Box sequence and points joint & \\textbf{85.51} / \\textbf{64.26} \\\\ % TODO: to be updated with the latest model. Consider adding one row for cascade.\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\textbf{Comparing with alternative designs of dynamic object auto labeling.}\n%     % Metrics are box accuracy with 3D IoU thresholds 0.7 and 0.8 for vehicles on the Waymo Open Dataset \\emph{val} set.\n%     The joint model uses points from 5 frames and boxes from 21 frames.}\n%     \\label{tab:dynamic_alternative}\n% \\end{table}\n\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{l|c|c}\n%     \\toprule\n%         method & context size & Acc@0.7/0.8 \\\\\n%         \\midrule\n%         Single-frame MVF++ & $[-0,+0]$ & 80.07 / 57.71 \\\\\n%         Multi-frame MVF++ & $[-4,+0]$& 82.21 / 59.52 \\\\\n%         \\midrule\n%          Align \\& refine &  $[-2,+2]$ & 83.33 / 60.69 \\\\\n%          Box sequence only & $[-5,+5]$ & 83.13 / 58.96 \\\\\n%          Points only & $[-0,+0]$ & 82.01 / 58.61 \\\\\n%          Points only & $[-2,+2]$ & 83.79 / 61.95 \\\\\n%          \\midrule\n%          Box sequence and points joint & $[-10,+10]$ & \\textbf{85.51} / \\textbf{64.26} \\\\ % TODO: to be updated with the latest model. Consider adding one row for cascade.\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{\\textbf{Comparing with alternative designs of dynamic object auto labeling.}\n%     % Metrics are box accuracy with 3D IoU thresholds 0.7 and 0.8 for vehicles on the Waymo Open Dataset \\emph{val} set.\n%     The joint model uses points from 5 frames and boxes from 21 frames.}\n%     \\label{tab:dynamic_alternative}\n% \\end{table}\n\n\\paragraph{Effects of temporal context sizes for object auto labeling}\nTable~\\ref{tab:context_size} studies how the context frame sizes influence the box prediction accuracy. We also compare with our single-frame (S-MVF++) and multi-frame detectors (M-MVF++) to show extra gains the object auto labeling can bring. We can clearly see that using large temporal contexts improves the performance while using the entire object track (the last row) leads to the best performance. Note that for the static object model, we use the detector box with the highest score for the initial coordinate transform, which gives our auto labeling an advantage over frame-based method.\n\n% We also trained a model using the causal input (the last row).\n% However, the causal model has a relatively low accuracy probably due to two reasons. First, it has limited contexts esp. for the beginning frames of the track. Second, the pool of initial boxes are much more restricted as we have to be causal. Such results indicate the benefit of having a non-causal model for the offboard 3D detection.\n\n\n\n\n\n% \\paragraph{The effects of the AutoQC model.} \\todo{TBD when the experiment results are mature.}\n\n% \\paragraph{The effects of offboard detection for 3D tracking.} In \n\n\n"
                },
                "subsection 5.6": {
                    "name": "Qualitative Analysis",
                    "content": "\n\\label{sec:exp:visualization}\n\nIn Fig.~\\ref{fig:visualization}, we visualize the auto labels for two representative scenes in autonomous driving: driving on a road with parked cars, and passing a busy intersection. Our model is able to accurately recognize vehicles and pedestrians in challenging cases with occlusions and very few points. The busy intersection scene also shows a few failure cases including false negatives of pedestrians in rare poses (sitting), false negatives of severely occluded objects and false positive for objects with similar geometry to cars. Those hard cases can potentially be solved with added camera information with multi-modal learning.\n\n% \\begin{figure}[h]\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{./fig/placeholder}\n%     \\caption{The table of 3D tracking results.}\n%     \\label{tab:tracking_results}\n% \\end{figure}\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nIn this work we have introduced 3D Auto Labeling, a state-of-the-art offboard 3D object detection solution using point cloud sequences as input. The pipeline leverages the long-term temporal data of objects in the 3D scene. Key to our success are our object-centric formulation, powerful offboard multi-frame detector and novel object auto labeling models.\nEvaluated on the Waymo Open Dataset, our solution has shown significant gains over prior art onboard 3D detectors, especially with high standard metrics. A human label study has further shown the high quality of the auto labels reaching comparable performance as experienced humans. Moreover, the semi-supervised learning experiments have demonstrated the usefulness of the auto labels for student training in cases of low-label and unseen domains.\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{pcl,yin_ref}\n}\n\n\\newpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n"
            },
            "section 8": {
                "name": "Overview",
                "content": "\nIn this document, we provide more details of models, experiments and show more analysis results. Sec.~\\ref{sec:supp:eval} presents more evaluation results on the Waymo Open Dataset test set and shows how our offboard 3D detection can help domain adaptation and 3D tracking. Sec.~\\ref{sec:supp:mvf} explains more details of MVF++ detectors. Sec.~\\ref{sec:supp:tracker} and Sec.~\\ref{sec:supp:motion} describe implementation details of our multi-object tracker and track-based motion state classifier respectively. Sec.~\\ref{sec:supp:object_auto_labeling} covers network architectures, losses and training details of object auto labeling models. Sec.~\\ref{sec:supp:human} describes the specifics of the human label study for 3D object detection and provides more statistics. Sec.~\\ref{sec:supp:semi} provides more information about the semi-supervised learning experiments. Lastly Sec.~\\ref{sec:supp:analysis} gives more analysis results supplementary to the main paper.\n\n"
            },
            "section 9": {
                "name": "More Evaluation Results",
                "content": "\n\\label{sec:supp:eval}\n\n",
                "subsection 9.1": {
                    "name": "3D Detection Results on the Test Set",
                    "content": "\nIn Table.~\\ref{tab:detection_test_set} we report detection results on the Waymo Open Dataset \\emph{test} set comparing our pipeline with a few leading methods in the leaderboard~\\cite{wod_3d_detection_website}. Note that our pipeline achieves the best results among Lidar-only methods. It also outperforms the HorizonLidar3D which uses both camera and Lidar input in the L1 metrics. We expect that adding camera input to our pipeline can further improve our pipeline in hard cases (L2).\n\n\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Domain Adaptation Results",
                    "content": "\nIn Table~\\ref{tab:domain_adaptation} we report detection results in another domain and compare our 3D Auto Labeling (3DAL) pipeline with two baselines: the popular PointPillars~\\cite{lang2019pointpillars} detector and our offboard multi-frame MVF++ detector. We see that our 3D Auto Labeling pipeline achieves significantly higher detection APs compared to the baselines (\\textbf{32.56} higher 3D AP than the PointPillars and \\textbf{8.03} higher 3D AP than the multi-frame MVF++), showing the strong generalization ability of our models. Compared to a few leading methods on the leaderboard~\\cite{wod_domain_adaptation_website} our method also shows significant gains. These large gains are probably due to the temporal information aggregation, which compensates the lower point densities in the WOD domain adaptation set (collected in Kirkland with mostly rainy weather). \n\n\n\n"
                },
                "subsection 9.3": {
                    "name": "3D Tracking Results",
                    "content": "\nIn table~\\ref{tab:tracking_results} we show how our improved box estimation from the offboard 3D Auto Labeling enhances the tracking performance, compared to using the boxes from the single-frame or multi-frame detectors. All methods used the same tracker (Sec.~\\ref{sec:supp:tracker}). This reflects that in the tracking-by-detection paradigm, the localization accuracy plays an important role in determining tracking quality in terms of MOTA and MOTP.\n\n\n\n"
                }
            },
            "section 10": {
                "name": "Implementation Details of the MVF++ Detectors",
                "content": "\n\\label{sec:supp:mvf}\n\\paragraph{Network Architecture}\n\nFigure~\\ref{fig:mvfpp_pointfusion_pipeline} illustrates the point-wise feature fusion network within the proposed MVF++. Given $C$-dimensional input encoding of $N$ points~\\cite{zhou2020end}, the network first projects the points into a $128$-D feature space via a multi-layer perceptron (MLP), where shape information can be better described. The MLP is composed of a linear layer, a batch normalization(BN) layer and a rectified linear unit (ReLU) layer. Then it processes the features by two separate MLPs for view-dependent information extraction, \\ie one for the Bird's Eye View and one for the Perspective View~\\cite{zhou2020end}. Next, the network employs voxelization~\\cite{zhou2020end} to transform view-dependent point features into the corresponding 2D feature maps, which are fed to view-dependent ConvNets (\\ie ConvNet$_{b}$ and ConvNet$_{p}$) to further extract contextual information within an enlarged receptive field. Different from MVF~\\cite{zhou2020end} using one ResNet~\\cite{he2016deep} layer in obtaining each down-sampled feature maps, we increase the depth of ConvNet$_{b}$ and ConvNet$_{p}$ by applying one more ResNet block in each down-sampling branch. At the end of view-dependent processing, it applies devoxelization to transform the 2D feature map back to point-wise features. The model fuses point-wise features by concatenating three sources of information. To reduce computational complexity, it applies two MLPs consecutively, reducing the feature dimension to 128. For improving the discriminative capability of features, it introduces 3D segmentation auxiliary loss and augments the dimension-reduced features with segmentation features. The output of point-wise feature fusion network has shape $N \\times 144$. \n\nUpon obtaining point-wise features, we voxelize them into a 2D feature map and employ a backbone network to generate detection results. Specifically, we adopt the same architecture as in~\\cite{lang2019pointpillars,zhou2020end}. To further boost detection performance in the offboard setting, we replace each plain convolution layer with a ResNet~\\cite{he2016deep} layer maintaining the same output feature dimension and feature map resolution. \n\n\\paragraph{Loss Function}\nWe train MVF++ by minimizing a loss function, defined as $L = L_{\\textrm{cls}} + w_1 L_{\\textrm{centerness}} + w_2 L_{\\textrm{reg}} + w_3 L_{\\textrm{seg}}$. $L_{\\textrm{cls}}$ and $L_{\\textrm{centerness}}$ are focal loss and centerness loss as in~\\cite{REF:tian2019fcos}. $L_{\\textrm{reg}}$ represents Smooth L1 loss learning to regress x, y, z center locations, length, width, height and heading orientation at foreground pixels, as in~\\cite{lang2019pointpillars,zhou2020end}. $L_{\\textrm{seg}}$ is the auxiliary 3D segmentation loss for distinguishing foreground from background points (points are labeled as foreground/background if they lie inside/outside of a ground truth 3D box)~\\cite{shi2018pointrcnn,qi2018frustum}. In our experiments, we set $w_1=1.0$, $w_2=2.0$, $w_3=1.0$. At inference time, the final score for ranking all detected boxes is computed as the multiplication of the classification score and the centerness score. By doing so, the centerness score can downplay the boxes far away from an object center and thus encourage non-maximum suppression (NMS) to yield high-quality boxes, as recommended in~\\cite{REF:tian2019fcos}.  \n\n\\paragraph{Data Augmentation}\nWe perform three global augmentations that are applied to the LiDAR point cloud and ground truth boxes simultaneously~\\cite{zhou2018voxelnet}. First, we apply random flip along the x axis, with probability 0.5. Then, we employ a random global rotation and scaling, where the rotation angle and the scaling factor are randomly drawn uniformly from $[-\\pi/4,+\\pi/4]$ and $[0.9, 1.1]$, respectively. Finally, we add a global translation noise to x, y, z drawn from $\\mathcal{N}(0, 0.6)$.\n\n\\paragraph{Hyperparameters} For vehicles, we set voxel size to $[0.32, 0.32, 6.0]$m and detection range to $[-74.88, 74.88]$m along the X and Y axes and $[-2, 4]$m along Z axis, which results in a $468 \\times 468$ 2D feature map in the Bird's Eye View. For pedestrians, we set voxel size to $[0.24, 0.24, 4.0]$m and detection range to $[-74.88, 74.88]$m along the X and Y axes and $[-1, 3]$m along Z axis, which corresponds to a $624 \\times 624$ 2D feature map in the Bird's Eye View. During test-time augmentation, we set (IoU threshold, box score) to be (0.275, 0.5) for vehicles and (0.2, 0.5) for pedestrians, to trigger weighted box fusion~\\cite{REF:wbf2019}.\n\n\\paragraph{Training} During training, we use the Adam optimizer~\\cite{REF:Adam} and apply cosine decay to the learning rate. The initial learning rate is set to $1.33 \\times 10^{-3}$ and ramps up to $3.0 \\times 10^{-3}$ after 1000 warm-up steps. The training used 64 TPUs with a global batch size of 128 and finished after 43,000 steps.\n\n% \\paragraph{Causal vs. Non-causal Detector} We have  \n\n"
            },
            "section 11": {
                "name": "Implementation Details of the Tracker",
                "content": "\n\\label{sec:supp:tracker}\nOur multi-object tracker is a similar implementation to~\\cite{weng2019baseline}.  To reduce the impact of sensor ego-motion in tracking, we transformed all the boxes to the world coordinate for tracking. To reduce false positives, we also filter out all detections with scores less than 0.1 before the tracking. We used Bird's Eye View (BEV) boxes for detection and track association, using the Hungarian algorithm with an IoU threshold of 0.1. During the states update, the heading is handled specially as there can be flips and cyclic patterns. Before updating the heading state, we first adjust the detection heading to align with the track state heading -- if the angle difference is obtuse, we add $\\pi$ to the detection angle before the update; we also average the angles in the cyclic space (\\eg the average of 6 rad and 0.5 rad is 0.1084 rather than 3.25).\n\n"
            },
            "section 12": {
                "name": "Implementation Details of the Motion State Estimator",
                "content": "\n\\label{sec:supp:motion}\nAs we introduced in the main paper, we use the object track data for motion state estimation, which is much easier compared to classifying the static/non-static state from a single or a few frames. Note that we define an object as static only if it is stationary in the entire sequence. Specifically, we extract two heuristic-based features and fit a linear classifier to estimate the motion state. The two features are: the detection box centers' variance and the begin-to-end distance of the tracked boxes (the distance from the center of the first box of the track to the center of the last box of the track), with boxes all in the world coordinate. To ensure that the statistics are reliable we only consider tracks with at least 7 valid measurements. For tracks that are too short, we do not run the classification nor the auto labeling models. The boxes of those short tracks are merged directly to the final auto labels.\n\nThe ground truth motion states are computed from ground truth boxes with pre-defined thresholds of begin-to-end distance (1.0m) and max speed (1m/s). The thresholds are needed because there could be small drifts in sensor poses, such that the ground truth boxes in the world coordinate are not exactly the same for a static object.\n\nFor vehicles, such a simple linear model can achieve more than $99\\%$ classification accuracy. The remaining rare error cases usually happen in short tracks with noisy detection boxes, or for objects that are heavily occluded or far away. For pedestrians, as most of them are moving and even the static ones tend to move their arms and heads, we consider all pedestrian tracks as dynamic.\n\n"
            },
            "section 13": {
                "name": "Details of the Object Auto Labeling Models",
                "content": "\n\\label{sec:supp:object_auto_labeling}\n\n",
                "subsection 13.1": {
                    "name": "Static Object Auto Labeling",
                    "content": "\n\\paragraph{Network architecture.}\nIn the static object auto labeling model, the foreground segmentation is a PointNet~\\cite{qi2017pointnet} segmentation network, where each point is firstly processed by an multi-layer perceptron (MLP) with 5 layers with output channel sizes of $64, 64, 64, 128, 1024$. For every layer of the MLP, we have batch normalization and ReLU. The $1024$-dim per point embeddings are pooled with a max pooling layer and concatenated with the output of the 2nd layer of the per-point MLP ($64$-dim). The concatenated $1088$-dim features are further processed by an MLP of 5 layers with output channel sizes $512,256,128,128,2$, where the last layer does not have non-linearity or batch normalization. The predicted foreground logit scores are used to classify each point as foreground or background. All the foreground points are extracted.\n\nThe box regression network is also a PointNet~\\cite{qi2017pointnet} variant that takes the foreground points and outputs the 3D box parameters. It has a per-point MLP with output sizes of $128, 128, 256, 512$, a max pooling layer and a following MLP with output sizes $512, 256$ on the max pooled features. There is a final linear layer predicting the box parameters. We parameterize the boxes in a way similar to~\\cite{qi2018frustum} as the box center regression ($3$-dim), the box heading regression and classification (to each of the heading bins) and the box size regression and classification (to each of the template sizes). For iterative refinement, we apply the same box regression network one more time on the foreground points transformed to the estimated box's coordinate. We found that if we use multi-frame MVF++ boxes, using shared weights for the two box regression networks works better than not sharing the weights; while if we use the single-frame MVF++, the cascaded design without sharing the weights works better. The numbers in the main paper are from the iterative model (shared weights).\n\nFor simplicity and higher generalizability of the model, we only used $XYZ$ coordinates of the points in the segmentation and box regression networks. Intensities and other point channels were not used. We have also tried to use the more powerful PointNet++~\\cite{qi2017pointnetplusplus} models but did not see improvement compared to the PointNet-based models in this problem.\n\n\n\n\\paragraph{Losses.}\nThe model is trained with supervision of the segmentation masks and the ground truth 3D bounding boxes. For the segmentation, the sub-network predicts two scores for each point as foreground or background and is supervised with a cross-entropy loss $L_{seg}$. For the box regression, we implement a process similar to~\\cite{qi2018frustum}, where each box regression network regresses the box by predicting its center $cx, cy, cz$, its size classes (among a few pre-defined template size classes) and residual sizes for each size class, as well as the heading bin class and a residual heading for each bin. We used $12$ heading bins (each bin account for 30 degrees) and $3$ size clusters: $(4.8, 1.8, 1.5), (10.0, 2.6, 3.2), (2.0, 1.0, 1.6)$, where the dimensions are length, width, height. The box regression loss is defined as $L_{\\text{box}_i} = L_{\\text{c-reg}_i} + w_1 L_{\\text{s-cls}_i} + w_2 L_{\\text{s-reg}_i} + w_3 L_{\\text{h-cls}_i} + w_4 L_{\\text{h-reg}_i}$ where $i \\in \\{1,2\\}$ represents the cascade/iterative box estimation step. The total box regression loss is $L = L_{seg} + w (L_{\\text{box}_1} + L_{\\text{box}_2})$. The $w_i$ and $w$ are hyperparameter weights of the losses. Empirically, we use $w_1 = 0.1$, $w_2 = 2$, $w_3 = 0.1$, $w_4 = 2$ and $w = 10$.\n\n\\paragraph{Training and data augmentation.}\nWe train our models using the extracted object tracks (with the proposed multi-frame MVF++ model and our multi-object tracker) from the Waymo Open Dataset for each class type separately. Ground truth boxes are assigned to every frame of the track (frames with no matched ground truth are skipped). \n% \\todo{how to assign ground truth with filtering of wrong track ID or wrong class type}\n\nDuring training, for each static object track, we randomly select an initial box from the sequence. We also randomly sub-sample $\\text{Uniform}[1, |S_j|]$ frames from all the visible frames $S_j$ of an object $j$. This naturally leads to a data augmentation effect. Note that at test time we always select the initial box with the highest score and use all frames. The merged points are randomly sub-sampled to $4,096$ points and randomly flipped along the $X, Y$ axes with $50\\%$ chance respectively and randomly rotated around the up-axis ($Z$) by $\\text{Uniform}[-10, 10]$ degrees.\nTo increase the data quantity, we also turn the dynamic object track data to pseudo static track. To achieve that, we use the ground truth object boxes to align the dynamic object points to a specific frame's ground truth box coordinate. This increases the number of object tracks of vehicles by 30\\%.\n\nIn total, we have extracted around 50K (vehicle) object tracks for training (including the augmented ones from dynamic objects) and around 10K object tracks for validation (static only). We trained the model using the Adam optimizer with a batch size of 32 and an initial learning rate of 0.001. The learning rate was decayed by 10X at the 60th, 100th and 140th epochs. The model was trained with 180 epochs in total, which took around 20 hours with a V100 GPU.\n\n"
                },
                "subsection 13.2": {
                    "name": "Dynamic Object Auto Labeling",
                    "content": "\n\\paragraph{Network architecture.}\nFor the foreground segmentation network, we adopt a similar architecture as that for the static auto labeling model except that the input points have one more channel besides the $XYZ$ coordinate, the time encoding channel. The temporal encoding is $0$ for points from the current frame, $-0.1r$ for the $r$-th frame prior to the current frame and $+0.1r$ for the $r$-th frame after the current frame. In our implementation we take 5 frames of object points with each frame's points subsampled to $1,024$ points, so in total there are $5,120$ points input to the segmentation network. The point sequence encoder network takes the segmented foreground points and uses a PointNet~\\cite{qi2017pointnet}-like architecture with a per-point MLP of output sizes $64,128,256,512$, a max-pooling layer and another MLP with output sizes $512, 256$ on the max-pooled features. The output is a $256$-dim feature vector.\n\nFor the box sequence encoder network, we consider each box (in the center frame's box coordinate) as a parameterized point with channels of box center (cx, cy, cz), box size (length, width, height), box heading $\\theta$ and a temporal encoding. We use nearly the entire box sequence (setting $s$ in the main paper to 50, leading to a sequence length of 101).\n\nThe box sequence can be considered as a point cloud and processed by another PointNet. Note, such a sequence can also be processed by a 1D ConvNet, or be concatenated and processed by a fully connected network, or we can even use a graph neural network. Through empirical study we found using a PointNet to encode the box sequence feature is both effective (compared with ConvNet and fully connected layers) and simple (compared with graph neural networks). The box sequence encoding PointNet has a per-point MLP with output sizes $64, 64, 128, 512$, a max-pooling layer and another MLP with output sizes $128, 128$ on the max-pooled features. The final output is a $128$-dim feature vector, which we call the trajectory embedding.\n\nThe point embedding and the trajectory embedding are concatenated and passed through a final box regression network, which is a MLP with two layers with output sizes $128, 128$ and a linear layer to regress the box parameters (similar to that of the static object auto labeling model).\n\nTo encourage contributions from both branches, we follow~\\cite{wang2019makes} and also pass the trajectory embedding and the object embedding to two additional box regression sub-networks to predict boxes independently. The sub-networks have the same structure as the one for the joint-embedding, but with non-shared weights.\n\n\n\\paragraph{Losses.}\nSimilar to the static auto labeling model, we have two types of loss, the segmentation loss and the box regression loss. The box regression outputs are defined in the same way as that for the static objects. We used $12$ heading bins (each bin account for 30 degrees) and the same size clusters as those for the static vehicle auto labeling. For pedestrians we use a single size cluster: $(0.9, 0.9, 1.7)$ of length, width, height. The final loss is $L = L_{\\text{seg}} + v_1 L_\\text{box-traj} + v_2 L_\\text{box-obj-pc} + v_3 L_\\text{box-joint}$ where we have three box losses from the trajectory head, the object point cloud head and the joint head respectively. The $v_i$, $i=1,2,3$ are the weights for the loss terms to achieve a balanced learning of the three types of embeddings. Empirically, we use $v_1 = 0.3, v_2 = 0.3, v_3 = 0.4$.\n\n\\paragraph{Training and data augmentation.}\nDuring training, we randomly select the center frame from each dynamic object track. If the context size is less than the required sequence length $2r+1$ or $2s+1$, or when the frames in the sequence are not consecutive (\\eg the object is occluded for a few frames), we use placeholder points and boxes (all zeros) for the empty frames. As there may be tracking errors, we match our object track with ground truth tracks and avoid training on the ones with switched track IDs.\n\nAs to augmentation, both points and boxes are randomly flipped along the $X$ and $Y$ axis with a $50\\%$ chance and randomly rotated around the $Z$ axis by $\\text{Uniform}[-10, 10]$ degrees. We also add a light random shift and a random scaling to the point clouds. Point cloud from each frame is also randomly sampled to $1,024$ points from the full point cloud observed.\n\n We train vehicle and pedestrian models separately. For vehicles we extracted around 15.7K dynamic tracks for training and 3K for validation. For pedestrians, we extracted around 22.9K dynamic tracks for training and around 5.1K for validation. We train the model using the Adam optimizer with batch size 32 and an initial learning rate 0.001. The learning rate is decayed by 10 times at the 180th, 300th and 420th epochs. The model is trained with 500 epochs in total, which takes 1-2 days with a V100 GPU.\n\n"
                }
            },
            "section 14": {
                "name": "Details of the Human Label Study",
                "content": "\n\\label{sec:supp:human}\n\nWe randomly selected 5 sequences from the Waymo Open Dataset \\emph{val} set as listed in Table~\\ref{tab:human_label_segments} to run the human label study. The 15 labeling tasks (3 sets of re-labels for each run segment) involved 12 labelers with experiences in labeling 3D Lidar point clouds. In total we collected around 2.3K labels (one label for one object track) for the 3 repeated labelings.\n\n\n\n\n% \\paragraph{What is the Average Precision (AP) of human labels?} As nearly all 3D object detection papers are comparing their models using average precision. We are curious to know how well human labelers perform at this metric.\n% In AP calculation, we rank the detection boxes by their ``scores'' and set various score thresholds to compute the precision and recall at each operation point. However, human annotations don't have any scores therefore we cannot directly evaluate human AP based on the labels. To solve this issue, we experiment with a few heuristic ways to assign a score for each human label including (1) the inverse distance of the box to the sensor; (2) the number of points within the box; (3) the visibility ratio of the object (by computing the angular distributions of the rays from the points to the box center) and a few others. We found that using the number of points as human scores leads to the highest human AP among all the choices, so we will use such score generation for our reported numbers.\n\n\\paragraph{How consistent are human labels?} Auxiliary to the AP results in the main paper, we also analyze the IoUs between human labels. We found that even for humans, 3D bounding box labeling can be challenging as the input point clouds are often partial and occluded. To understand how consistent human labels are, we compare labels from one labeler with the labels from the other and measure the 3D box consistency by their IoUs. Since we already have the verified public ground truth, we can compare the 3 sets of labels with the public WOD ground truth and get the average box IoU for all objects that are matched (due to occlusions, some objects may be labeled or not labeled by a specific labeler). Specifically, for human boxes that we cannot find a ground truth box with more than 0.03 BEV IoU overlap (false positive or false negative), they were ignored and not counted in the computation.\n\nThe statistics are summarized in Table~\\ref{tab:human_mean_iou}. Surprisingly, human labels do not have the consistency one may expect (\\eg 95\\% IoU). Due to the inherent uncertainty of the problem, even humans can only achieve around 81\\% 3D IoU or around 88\\% BEV IoU in their box consistency. As we break down the numbers by distance we see, intuitively, that nearby objects have a significantly higher mean IoU as they have more visible points and more complete viewpoints. The BEV 2D IoU is also higher than the 3D IoU as we do not require the correct height estimation in the BEV box, which simplifies the problem.\n\nTo have a rough understanding of how the boxes generated by our 3D Auto Labeling pipeline compare with human labels, we compute the average IoU of auto labels with the WOD ground truth. Note that those numbers are not directly comparable to the average human IoUs as they cover different sets of objects (due to the false positives and false negatives). However, it still gives us an understanding that the auto labels are already on par in quality to human labels.\n% For a more fair comparison, in the next paragraph we will evaluate the human labels and auto labels using the detector metric: average precision.\n\n\n"
            },
            "section 15": {
                "name": "More Details about the Semi-supervised Learning Experiment",
                "content": "\n\\label{sec:supp:semi}\n% \\todo{Owner: najibi, ddl: 11/19?}\nIn the semi-supervised experiments, we use an onboard single-frame \\emph{MVF++} detector as the student. We train all networks with an effective batch size of 256 scenes per iteration. The training schedule starts with a warmup period where the learning rate is gradually increased to 0.03 in 1000 iterations. Afterward, we use a cosine decay learning rate schedule to drop the learning rate from 0.03 to 0. \n\nFor the intra-domain semi-supervised learning, we randomly select 10\\% of the sequences (around 15K frames from 79 sequences) to train the 3DAL pipeline which gets an AP of 78.11\\% on the \\emph{validation} set. Then, the 3DAL annotates the rest of the training set (around 142K frames from 719 sequences). Finally, the student is trained on the union of these sets (798 sequences). We train the models for a total of 43K iterations. \n\nFor the cross-domain semi-supervised learning, we first train 3DAL on the regular Waymo Open \\emph{training} set. Since the domain adaptation validation set is relatively small (\\ie only contains 20 sequences), we submit the results to the submission server and report on the domain adaptation \\emph{test} set (containing 100 sequences). The 3DAL gets an AP of 78.0\\% on the domain adaptation \\emph{test} set without using any data from that domain. Then, we use the trained pipeline to annotate the domain adaptation \\emph{training} and \\emph{unlabeled} sets of the Waymo Open Dataset. Finally, the student is trained on the union of the regular \\emph{training} set annotated by humans, and domain adaptation \\emph{training+unlabeled} sets annotated by 3DAL. Since the data used for training the student is around 2X larger compared to the intra-domain experiment, we also increase the training iterations to 80K. \n\n\n\n\n\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n%     \\begin{tabular}{c|c|c|c}\n%     \\toprule\n%          Trajectory & Object Points & Joint & Acc@0.7/0.8 \\\\\n%          \\midrule\n%         %  1.0 & - & - & \\\\\n%         %  - & 1.0 & - & \\\\\n%          - & - & 1.0 & \\\\\n%          0.3 & 0.3 & 0.4 & \\\\\n%          \\bottomrule\n%     \\end{tabular}\n%     \\caption{Effects of loss weights $v_i$, $i=1,2,3$ for the dynamic object auto labeling.}\n%     \\label{tab:dynamic_multi_head}\n% \\end{table}\n\n"
            },
            "section 16": {
                "name": "More Analysis Experiments for Object Auto Labeling",
                "content": "\n\\label{sec:supp:analysis}\n% \\todo{Owner: bydeng, rqi; ddl: 11/22 Sunday}\n\nIn this section, we provide more analysis results auxiliary to the main paper.\n\n\\paragraph{Effects of key frame selection for static object auto labeling.}\nTable~\\ref{tab:static_frame_selection} compares the effects of using different initial boxes (from the detectors) for the model (for the coordinate transform before foreground segmentation): a uniformly chosen random box, the average box and the box with the highest score. We also show the box accuracy of the detectors as a reference (\\ie the accuracy of those initial boxes).\n\nChoosing a uniformly random box from the sequence is equivalent to the setting of a frame-centric approach. As for the detector baselines (row 1 and row 2), it directly evaluates the average accuracy of the detector boxes. As for the auto labeling, it means the box estimation is running for every frame, similar to a two-stage refinement step in two-stage detectors. We see that such a frame-centric box estimation achieves the most unfavorable results, as it is not able to leverage the best viewpoint in the sequence (in the object-centric way).\n\nIn the average box setting, we average all the boxes from the sequence (in the world coordinate) and use the averaged box for the transformation. For the highest score setting, we select the box with the highest confidence score as the initial box, which is similar to choosing the \\emph{best} viewpoint of the object. We see that the strategy to choose the initial box has a great impact and can cause a 4.46 Acc@0.8 difference for the auto labeling model (the highest score box \\vs a random box).\n\n\n\n\n\n\n\n\n\n\\paragraph{Causal model performance.}\nTable~\\ref{tab:static_context_size} compares non-causal models and causal models (for static object auto labeling). The causal model was trained using the causal input (the last row) and only used causal input for inference at every frame. We see the causal model has a relatively lower accuracy compared to non-causal ones, probably due to two reasons. First, it has limited contexts especially for the beginning frames of the track. Second, the pool of initial boxes are much more restricted if the input has to be causal. For non-causal models, we can select the frame with the highest confidence as the key frame and use the box from that frame for the initial coordinate transform. However, for the causal model, it can only select the highest confidence box from the \\emph{history} frames, which are not necessarily the well visible ones.\n\nAs the causal model's accuracy is even inferior to the one that just uses a single frame's points for refinement (the first row in Table~\\ref{tab:static_context_size}), the ability to select the best key frame weighs more than the added points from a few history frames. Note that the performance is still better than the detector boxes without refinement (row 2 in Table~\\ref{tab:static_frame_selection}) Such results indicate the benefit of having a non-causal model for the offboard 3D detection.\n\nTable~\\ref{tab:dynamic_context_size} reports a similar study for dynamic object auto labeling. We also see that the causal model's performance is inferior to the non-causal one, although it still improves upon the raw detector accuracy (Table 8 in the main paper row 2, where the multi-frame MVF++ gets 82.21 / 59.52 accuracy).\n\n\n\n\n% \\begin{table}[]\n%     \\centering\n%     \\small\n% \\begin{tabular}{c|cc} \n% \\toprule\n% \\multirow{2}{*}{Configuration} & \\multicolumn{2}{c}{Acc.@0.7/0.8}               \\\\\n%                               & Static                & Dynamic                \\\\ \n% \\midrule\n% Original                       & \\textbf{82.28/56.92}  & \\textbf{85.67/65.77}   \\\\\n% Larger Batch ($\\times$ 8)      & 81.94/55.98           & 84.46/64.48            \\\\\n% More Points ($\\times$ 2)       & 80.67/54.01           & 84.87/61.94            \\\\\n% Deeper Network ($\\times$ 2)    & 79.96/52.58           & 79.19/57.08            \\\\\n% Wider Network ($\\times$ 2)     & 80.91/54.48           & 84.26/62.42            \\\\\n% \\bottomrule\n% \\end{tabular}\n%     \\caption{\\textbf{Ablations of training hyper parameters.}}\n%     \\label{tab:hyperparam}\n% \\end{table}\n\n\n\n\n\\paragraph{Ablations of data augmentation.}\nTable~\\ref{tab:data_aug} compares the performance of our auto labeling pipeline when trained with different data augmentations.\n%\nThe most accurate models are consistently trained with all the proposed augmentations. \n%\nFor static objects, all the augmentations contribute similarly in terms of accuracy, while for dynamic objects, random rotation around Z-axis appears to be the most critical.\n%\n% An intuitive explanation is that since dynamic objects are mostly moving on the X-Y plane, rotating them around Z can generate new motion types without violating physical rules. Therefore, it can increase the motion variance in the training set, which is more important in dynamic objects.\n\n% \\paragraph{Ablations of training hyper parameters.}\n% In Table~\\ref{tab:hyperparam}, we explore the effects of different hyper parameters on object auto labeling model performances.\n% %\n% With increased the batch size, the point cloud size, or the neural network size, the results did not get better.\n% %\n% When trained with a much larger batch size, the model performs slightly worse. We can connect this phenomenon to the implicit regularization effect of mini-batch gradient descent. When large batches are used, this effect is less powerful.\n% %\n% Moreover, the performance gap is most significant when the number of parameters (Deeper or wider Network) or the computation complexity (More Points) is increased.\n% %\n% We conjecture that the optimization difficulty is the major obstacle in these cases. Especially, when the network becomes deeper, vanishing gradients are likely to appear, hence making the parameters hard to optimize.\n\n\\paragraph{Effects of the tracking accuracy.}\nTo study how tracking (association) accuracy affects our offboard 3D detection, we compare results using our Kalman filter tracker and an ``oracle'' tracker (they share the detection and object auto labeling models, just the tracker is different). For the ``oracle'' tracker, we associate detector boxes using the ground truth boxes. Specifically, for each detector box, we find its closest ground truth box and assign the ground truth box's object ID to it.\n%\nIn Table.~\\ref{tab:different_tracker}, we observe that better performances can be obtained when a more reliable tracker is used, although the difference is subtle.\n%\nIn particular, the ``oracle'' tracker introduces more improvement for vehicles than pedestrians.\n%\nThis difference can imply that, for pedestrians, there is more space for improvement in detecting targets than associating detected boxes.\n\n\n\n\\paragraph{Effects of the motion state estimation accuracy.}\nIn Table~\\ref{tab:motion_state} we study how motion state classification accuracy affects the offboard 3D detection AP. We replace the motion state classifier (``Pred'') with a new one using the ground truth boxes (``GT'') for classification and see how much AP improvements it can bring us. We see that while there are some gains, they are not significant. This is understandable as our linear classifier can already achieve a 99\\%+ accuracy.\n\n\n\n% \\section{More Visualizations}\n% \\label{sec:supp:vis}\n% We provide a MP4 video file (named \\textbf{3dal\\_video.mp4}) showcasing the 3D auto labeling results of vehicles and pedestrian in point cloud sequences. The sequences are from the Waymo Open Dataset \\emph{val} set.\n\n\\paragraph{Inference speed.}\nProcessing a 20-second sequence (200 frames with the 10Hz sensor input) using a V100 GPU, the detector takes the majority time (around 15 minutes) due to the multi-frame input and test-time augmentation. The tracking takes around 3s and the object-centric refinement takes around 25s, which is 28s in total, 0.14s per frame, or a 3\\% extra time over the detection. In the offboard setting, we can run detection or the refinement steps in parallel to further reduce the processing latency.\n% {\\small\n% \\bibliographystyle{ieee_fullname}\n% \\bibliography{pcl,yin_ref}\n% }\n\n"
            }
        },
        "tables": {
            "tab:main_detection_results": "\\begin{table*}[t!]\n    \\centering\n    \\setlength{\\tabcolsep}{3pt}\n    \\small\n    \\begin{tabular}{l|c|c|c|c|c||c|c|c|c}\n    \\toprule\n         \\multirow{3}{*}{Method} & \\multirow{3}{*}{frames} & \\multicolumn{4}{c||}{\\emph{Vehicles}} & \\multicolumn{4}{c}{\\emph{Pedestrians}} \\\\ %\\cmidrule{3-10}\n         & & \\multicolumn{2}{c|}{3D AP} & \\multicolumn{2}{c||}{BEV AP} & \\multicolumn{2}{c|}{3D AP} & \\multicolumn{2}{c}{BEV AP} \\\\  %\\cmidrule{3-10} %\\cline{3-10} \n         & & IoU=0.7 & IoU=0.8 & IoU=0.7 & IoU=0.8 & IoU=0.5 & IoU=0.6 & IoU=0.5 & IoU=0.6\\\\ \n         \\toprule\n         StarNet~\\cite{REF:StarNet_2019} & 1 & 53.70 & - & - & - & 66.80 & - & - & -\\\\\n         PointPillar~\\cite{lang2019pointpillars}$^{\\star}$ & 1 & 60.25 & 27.67 & 78.14 & 63.79 &60.11 & 40.35 & 65.42 & 51.71 \\\\\n        %  PointPillar~\\cite{lang2019pointpillars} & 1 & 56.62 & - & 75.57 & - &60.11 & 40.35 & 65.42 & 51.71 \\\\\n         Multi-view fusion (MVF)~\\cite{zhou2020end} & 1 & 62.93 & - & 80.40 & - & 65.33 & - & 74.38 & -\\\\\n         AFDET~\\cite{REF:AFDET_CVPRW2020} & 1 & 63.69 & - & - & - & - & - & - & - \\\\\n         ConvLSTM~\\cite{REF:ConvLSTM_ECCV2020} & 4 & 63.60  & - & - & - & - & - & - & -\\\\\n         RCD~\\cite{REF:bewley2020range} & 1 & 68.95 & - & 82.09 & - & - & - & - & -\\\\\n         PillarNet~\\cite{REF:PillarNet_ECCV2020} & 1 & 69.80 & - & 87.11 & - & 72.51 & - & 78.53 & -\\\\\n         PV-RCNN~\\cite{shi2020pv}$^{\\star}$ & 1 & 70.47  & 39.16 & 83.43  & 69.52 & 65.34 & 45.12 & 70.35 & 56.63\\\\\n         \\midrule\n        %  Single-frame MVF++ (Ours) & 1 & 71.99 / 68.22 & 39.35 / 37.41 & 86.38 / 81.60 & 72.17 / 68.39 \\\\ (old)\n        %  Single-frame MVF++ (Ours) & 1 & 74.64  & 43.30  & 87.59  & 75.30 & 71.76 & 46.86 & 79.19  & 62.60 \\\\ \n        %  Single-frame MVF++ (Ours) & 1 & 74.64  & 43.30  & 87.59  & 75.30 & 77.37 & 55.48 & 82.61  & 67.50 \\\\ \n         Single-frame MVF++ (Ours) & 1 & 74.64  & 43.30  & 87.59  & 75.30 & 78.01 & 56.02 & 83.31  & 68.04 \\\\\n         Multi-frame MVF++ w. TTA (Ours) & 5 & 79.73  & 49.43  & 91.93  & 80.33 & 81.83 & 60.56 & 85.90 & 73.00 \\\\\n        %  3D Auto Labeling (Ours) & all & \\textbf{83.99} & \\textbf{56.97}  & \\textbf{93.22}  & \\textbf{84.61} & \\textbf{82.88} & \\textbf{63.69} & \\textbf{86.32} & \\textbf{75.60} \\\\\n        3D Auto Labeling (Ours) & all & \\textbf{84.50} & \\textbf{57.82}  & \\textbf{93.30}  & \\textbf{84.88} & \\textbf{82.88} & \\textbf{63.69} & \\textbf{86.32} & \\textbf{75.60} \\\\ % updated results on 11/14/2020.\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{3D object detection results for vehicles and pedestrians on the Waymo Open Dataset \\emph{val} set.} Methods in comparison include prior state-of-the-art single-frame based 3D detectors as well as our single-frame MVF++, our multi-frame MVF++ (5 frames) and our full 3D Auto Labeling pipeline. The metrics are L1 3D AP and bird's eye view (BEV) AP at two IoU thresholds: the common standard IoU=0.7 and a high standard IoU=0.8 for vehicles; and IoU = 0.5, 0.6 for pedestrians.\n    % $^{\\star}$ represents results based on our reimplementation.}\n    $^{\\star}$ reproduced results using author's released code.}\n    %\\yinzhou{Consider differentiate with and with TTA.}}\n    \\label{tab:main_detection_results}\n\\end{table*}",
            "tab:human_label_study": "\\begin{table}[t!]\n    \\centering\n    \\small\n    \\begin{tabular}{l|cc|cc}\n    \\toprule\n    & \\multicolumn{2}{c|}{3D AP} & \\multicolumn{2}{c}{BEV AP} \\\\\n          & IoU=0.7 & IoU=0.8 & IoU=0.7 & IoU=0.8 \\\\ \\toprule\n         Human & \\textbf{86.45} & \\textbf{60.49} & \\textbf{93.86} & 86.27 \\\\ \n         3DAL (Ours) & 85.37 & 56.93 & 92.80 & \\textbf{87.55} \\\\ %\\midrule\n        %  \\emph{diff} & \\emph{-1.08} & \\emph{-3.56} & \\emph{-1.06} & \\emph{+1.28} \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Comparing human labels and auto labels in 3D object detection.} The metrics are 3D and BEV APs for vehicles on the 5 sequences from the Waymo Open Dataset \\emph{val} set. Human APs are computed by comparing them with the WOD's released ground truth and using number of points in boxes as human label scores.}\n    \\label{tab:human_label_study}\n\\end{table}",
            "tab:semi_supervisedv2": "\\begin{table}[t!]\n    \\centering\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabu}{c|c|c|c}\n    \\toprule\n        Training Data & Test Data & 3D AP & BEV AP \\\\\n        \\toprule\n        100\\% main \\textit{train} (Human) & main \\textit{val} & 71.2  & 86.9  \\\\\n        \\tabucline [0.1pt on 4pt off 2pt]{1-4} \n        10\\% main \\textit{train} (Human) &  main \\textit{val} & 64.3 & 81.2 \\\\\n        \\tabucline [0.1pt on 4pt off 2pt]{1-4} \n        % 90\\% MVF++ & 68.3 & 59.5 \\\\\n        \n         10\\% main \\textit{train} (Human) & \\multirow{2}{*}{main \\textit{val}} & \\multirow{2}{*}{70.0} & \\multirow{2}{*}{86.4} \\\\\n         + 90\\% main \\textit{train} (\\textbf{3DAL}) & & & \\\\\n         \n         \\midrule %\\midrule\n         \n         100 \\% main \\textit{train} (Human) & domain \\textit{test} & 59.4 & \\textit{N/A} \\\\ \n         \\tabucline [0.1pt on 4pt off 2pt]{1-4}\n         100 \\% main \\textit{train} (Human) & \\multirow{2}{*}{domain \\textit{test}} & \\multirow{2}{*}{60.3} & \\multirow{2}{*}{\\textit{N/A}} \\\\\n         + domain  (Self Anno.) & & & \\\\\n         \\tabucline [0.1pt on 4pt off 2pt]{1-4}\n         100 \\% \\textit{train} (Human) & \\multirow{2}{*}{domain \\textit{test}} & \\multirow{2}{*}{64.2} & \\multirow{2}{*}{\\textit{N/A}} \\\\\n         + domain  (\\textbf{3DAL}) & & & \\\\\n         \\bottomrule\n    \\end{tabu}\n    }\n    \\caption{\\textbf{Results of semi-supervised learning with auto labels.} Metrics are 3D and BEV AP for vehicles on the Waymo Open Dataset. The type of annotation is reported in parenthesis. Please note, test set BEV AP is not provided by the submission server.}\n    \\label{tab:semi_supervisedv2}\n\\end{table}",
            "tab:detector_ablation": "\\begin{table}[]\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{c|c|c|c|c|c}\n    \\toprule\n         anchor-free & cap. increase & seg loss & 5-frame & TTA & AP@0.7/0.8 \\\\\n         \\midrule\n         \\checkmark & - & - & - & - & 71.20 / 39.70 \\\\\n         \\checkmark & \\checkmark & - & - & - & 74.28 / 42.91 \\\\\n         \\checkmark & \\checkmark & \\checkmark & - & - & 74.64 / 43.30 \\\\\n         \\checkmark & \\checkmark & \\checkmark & \\checkmark & - & 76.34 / 45.57 \\\\ % TODO: to be updated with the latest model.\n         \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & 79.73 / 49.43 \\\\ % TODO: to be updated with the latest model.\n         \n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Ablation studies on the improvements to 3D detector MVF}~\\cite{zhou2020end}. Metrics are 3D AP (L1) at IoU thresholds 0.7 and 0.8 for vehicles on the Waymo Open Dataset \\emph{val} set.}\n    \\label{tab:detector_ablation}\n\\end{table}",
            "tab:detection_AP_vs_frames": "\\begin{table}[]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|c|c|c|c|c}\n    \\toprule\n         \\# frames & 1 & 2 & 3 & 4 & 5 & 10 \\\\\n         \\midrule\n         AP@0.7 & 74.64 & 75.32 & 75.63 & 76.17 & 76.34 & 76.96\\\\\n        %  $\\Delta$/fr.  & - & +0.68 & +0.50 & +0.51 & +0.43 & +0.26\\\\ \\midrule\n         AP@0.8 & 43.30 & 44.11 & 44.80 & 45.43 & 45.57 & 46.20\\\\\n        %  $\\Delta$/fr. & - & +0.81 & +0.75 & +0.71 & +0.57 & +0.32\\\\ \n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Ablation studies on 3D detection AP \\vs temporal contexts.} Metrics are 3D AP (L1) for vehicles on the Waymo Open Dataset \\emph{val} set. We used the 5-frame model in 3D Auto Labeling.\n    % The $\\Delta$/fr. shows the incremental gain per added frame compared to the single-frame baseline.\n   % \\yinzhou{fill in results after experiments.}\n    }\n    \\label{tab:detection_AP_vs_frames}\n\\end{table}",
            "tab:static_ablation": "\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|c|c|c}\n    \\toprule\n         transform & segmentation & iterative & tta & Acc@0.7/0.8 \\\\\n         \\midrule\n         - & - & - & - & 78.82 / 50.90 \\\\\n         \\checkmark & - & - & - & 81.35 / 54.76 \\\\\n         \\checkmark & \\checkmark & - & - & 81.37 / 55.67 \\\\ \\midrule\n         \\checkmark & \\checkmark &  \\checkmark & - & 82.02 / 56.77\\\\\n        %  \\checkmark & \\checkmark & cascade & - & 81.69 / 56.33 \\\\  \\midrule\n         \\checkmark & \\checkmark &  \\checkmark & \\checkmark & \\textbf{82.28} / \\textbf{56.92} \\\\\n        %  \\checkmark & \\checkmark & cascade & \\checkmark & 81.67 / \\textbf{57.12} \\\\ % TODO: update to the newest results.\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Ablation studies of the static auto labeling model.} Metrics are the box accuracy at 3D IoU=0.7 and IoU=0.8 for vehicles in the Waymo Open Dataset \\emph{val} set.}\n    \\label{tab:static_ablation}\n\\end{table}",
            "tab:dynamic_alternative": "\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{l|c}\n    \\toprule\n        Method & Acc@0.7/0.8 \\\\\n        % \\midrule\n        % Single-frame MVF++ & 80.07 / 57.71 \\\\\n        % Multi-frame MVF++ & 82.21 / 59.52 \\\\\n        \\midrule\n         Align \\& refine&  83.33 / 60.69 \\\\\n         %  Points only (1 frame) & 82.01 / 58.61 \\\\\n         Points only & 83.79 / 61.95 \\\\\n         Box sequence only & 83.13 / 58.96 \\\\ % todo: update with using 101 frames for the box sequence.\n         \\midrule\n         Points and box sequence joint & \\textbf{85.67} / \\textbf{65.77} \\\\ % TODO: to be updated with the latest model. Consider adding one row for cascade.\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Comparing with alternative designs of dynamic object auto labeling.}\n    Metrics are box accuracy with 3D IoU thresholds 0.7 and 0.8 for vehicles on the Waymo Open Dataset \\emph{val} set.\n    % The joint model uses points from 5 frames and boxes from 21 frames.\n    }\n    \\label{tab:dynamic_alternative}\n\\end{table}",
            "tab:context_size": "\\begin{table}[t!]\n    \\centering\n    \\small\n    \\begin{tabular}{l|c|c|c}\n         \\toprule\n         \\multirow{2}{*}{Method} & \\multirow{2}{*}{Context frames} & \\emph{static} & \\emph{dynamic} \\\\\n         & & Acc@0.7/0.8 & Acc@0.7/0.8 \\\\\n         \\midrule\n        S-MVF++ & $[-0,+0]$ & 67.17 / 36.61 & 80.07 / 57.71 \\\\\n        M-MVF++ & $[-4,+0]$  & 73.96 / 43.56 & 82.21 / 59.52 \\\\\n        \\midrule\n         \\multirow{4}{*}{3DAL} & $[-0,+0]$ & 78.13 / 50.30 & 80.65 / 57.97 \\\\ % todo: update with using just 1 frame points.\n         & $[-2,+2]$ & 79.60 / 52.52 & 84.34 / 63.60 \\\\\n         & $[-5,+5]$ & 80.48 / 55.02 & 85.10 / 64.51 \\\\\n         & all & \\textbf{82.28} / \\textbf{56.92} & \\textbf{85.67} / \\textbf{65.77} \\\\\n        %  \\midrule\n        %  3DAL & all history & 77.56 / 49.21 & \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Effects of temporal context sizes for object auto labeling.} Metrics are the box accuracy at 3D IoU=0.7, 0.8 for vehicles in the WOD \\emph{val} set. Dynamic vehicles have a higher accuracy because they are closer to the sensor than static ones.\n    % Note that for a causal model, we cannot output a single box for a static object -- we have to output the best estimation for every frame using the current frame and the history frames. We compute the average accuracy across all frames for the causal case.\n    }\n    \\label{tab:context_size}\n\\end{table}",
            "tab:detection_test_set": "\\begin{table}[h]\n    \\centering\n    \\small\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabu}{l|c|cccc}\n    \\toprule\n        Method & Sensor & AP L1 & APH L1 & AP L2 & APH L2 \\\\ \\midrule\n        PV-RCNN & L & 81.06 & 80.57 & 73.69 & 73.23 \\\\\n        CenterPoint & L & 81.05 & 80.59 & 73.42 & 72.99 \\\\\n        HorizonLidar3D & CL & 85.09 & 84.68 & \\textbf{78.23} & \\textbf{77.83} \\\\\n        \\midrule\n        3DAL (ours) & L & \\textbf{85.84} & \\textbf{85.46} & 77.24 & 76.91 \\\\\n    \\bottomrule\n    \\end{tabu}\n    }\n    \\caption{\\textbf{3D detection AP on the Waymo Open Dataset main \\emph{test} set for vehicles.} Evaluation results were obtained from submitting to the test server. For the sensor the `L` means Lidar-only; the `CL` means camera and Lidar. Note that our method peeks into the future for object-centric refinement, which is feasible in the offboard setting.}\n    \\label{tab:detection_test_set}\n\\end{table}",
            "tab:domain_adaptation": "\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{l|c|c|c|c}\n    \\toprule\n        Method & 3D AP & 0-30m & 30-50m & 50+m \\\\ \\midrule\n        PointPillar & 45.48 & 74.02 & 36.49 & 14.94\\\\\n        Multi-frame MVF++ & 70.01 & 86.54 & 67.72 & 43.25 \\\\\n        \\midrule\n        PV-RCNN-DA & 71.40 & 90.00 & 66.45 & 45.92 \\\\\n        CenterPoint & 67.04 & 86.62 & 60.95 & 38.59 \\\\\n        HorizonLidar3D & 72.48 & 90.65 & 67.26 & 47.89 \\\\\n        \\midrule\n        3DAL (ours) & \\textbf{78.04} & \\textbf{91.90} & \\textbf{73.47} & \\textbf{52.53} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{3D detection AP on the Waymo Open Dataset domain adaptation \\emph{test} set for vehicles.} The PointPillar, MVF++ and 3DAL models were trained by us on the Waymo Open Dataset main \\emph{train} set. Evaluation results were obtained from submitting to the test server. The PV-RCNN-DA, CenterPoint and HorizonLidar3D results are leading entries from the leaderboard~\\cite{wod_domain_adaptation_website}.}\n    \\label{tab:domain_adaptation}\n\\end{table}",
            "tab:tracking_results": "\\begin{table}[h]\n    \\centering\n    \\small\n    \\begin{tabular}{l|c|c}\n         \\toprule\n         Method & MOTA$\\uparrow$ & MOTP$\\downarrow$ \\\\\n         \\midrule\n         Single-frame MVF++ with KF & 52.20 & 17.08 \\\\\n         Multi-frame MVF++ with KF & 61.92 & 16.31 \\\\\n         \\midrule\n        %  3D Auto Labeling & \\textbf{65.92} & \\textbf{15.51} \\\\\n         3D Auto Labeling & \\textbf{66.90} & \\textbf{15.45} \\\\ % updated on 11/14/2020.\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{3D tracking results for vehicles on the Waymo Open Dataset \\emph{val} set.} The metrics are L1 MOTA and MOTP for vehicles on the Waymo Open Dataset \\emph{val} set. KF stands for using Kalman Filtering for the track state update. The arrows indicate whether the metric is better when it is higher (up-ward arrow) or is better when it is lower (down-ward arrow).}\n    \\label{tab:tracking_results}\n\\end{table}",
            "tab:human_label_segments": "\\begin{table}[]\n    \\centering\n    \\begin{tabular}{l}\n    \\toprule\n         segment-17703234244970638241\\_220\\_000\\_240\\_000 \\\\\n         segment-15611747084548773814\\_3740\\_000\\_3760\\_000 \\\\\n         segment-11660186733224028707\\_420\\_000\\_440\\_000 \\\\\n         segment-1024360143612057520\\_3580\\_000\\_3600\\_000 \\\\\n         segment-6491418762940479413\\_6520\\_000\\_6540\\_000 \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Sequence (run segment) list for the human label study.} The sequences are all from the Waymo Open Dataset \\emph{val} set.}\n    \\label{tab:human_label_segments}\n\\end{table}",
            "tab:human_mean_iou": "\\begin{table}[]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}}\n    \\toprule\n         IoU type & Label type & all & 0-30m & 30-50m & 50m+ \\\\ \\midrule\n         \\multirow{2}{*}{valid boxes} & human & 25,641 & 11,543 & 7,963 & 6,135  \\\\\n         & auto & 24,146 & 11,360 & 7,448 & 5,338 \\\\\n         \\midrule\n         \\multirow{2}{*}{3D mIoU} & human & 80.92 & 85.78 & 80.29 & 72.59 \\\\\n         & auto & 80.29 & 84.04 & 77.45 & 76.28 \\\\\n         \\midrule\n         \\multirow{2}{*}{BEV mIoU} & human & 87.98 & 91.26 & 87.31 & 82.68 \\\\ \n         & auto & 87.50 & 90.36 & 85.09 & 84.78 \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{The mean IoU of human labels and auto labels compared with the Waymo Open Dataset ground truth for vehicles.} Note that since different labels (human or machine) annotate different number of objects for each frame, those numbers are not directly comparable. They are summarized here for a reference. For a more fair comparison between human and auto labels, see the Average Precision comparison table in the main table. We only evaluate using ground truth boxes with at least one point in it and only evaluate boxes that have a BEV IoU larger than 0.03 with any ground truth box.}\n    \\label{tab:human_mean_iou}\n\\end{table}",
            "tab:static_frame_selection": "\\begin{table}[]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|c}\n    \\toprule\n         Model &  detector box & Acc@0.7/0.8 \\\\\n         \\midrule\n         Single-frame MVF++ & random & 67.17 / 36.61 \\\\\n         \\midrule\n         \\multirow{3}{*}{Multi-frame MVF++} & random & 73.96 / 43.56 \\\\\n          & average & 79.29 / 48.67 \\\\\n          & highest score & 78.67 / 52.42 \\\\\n        %   & most points & \\\\\n         \\midrule\n         \\multirow{3}{*}{Auto labeling model} & random & 79.66 / 52.46\\\\\n          & average & 81.22 / 53.96 \\\\\n          & highest score & \\textbf{82.28} / \\textbf{56.92} \\\\\n        %   & most points & \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Effects of initial box selection in static object auto labeling.} Numbers are averaged over 3 runs for the random boxes.}\n    \\label{tab:static_frame_selection}\n\\end{table}",
            "tab:static_context_size": "\\begin{table}[b]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|c}\n         \\toprule\n         ref frame & context frames & Acc@0.7/0.8 \\\\\n         \\midrule\n         \\multirow{2}{*}{all highest score} & $[-0,+0]$ & 78.13 / 50.30 \\\\\n         & all & \\textbf{82.28} / \\textbf{56.92} \\\\\n         \\midrule\n         past highest score & all history & 77.56 / 49.21\\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Effects of temporal contexts for static object auto labeling.} Note that for a causal model, we cannot output a single box for a static object -- we have to output the best estimation for every frame using the current frame and the history frames. We compute the average accuracy across all frames for the causal case.}\n    \\label{tab:static_context_size}\n\\end{table}",
            "tab:dynamic_context_size": "\\begin{table}[b]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|c}\n    \\toprule\n         Point cloud context & Box context & Acc@0.7/0.8 \\\\\n         \\midrule\n         $[-2,+2]$ & all & \\textbf{85.67} / \\textbf{65.77} \\\\\n         \\midrule\n         \\multirow{1}{*}{$[-4,0]$} & all history & 84.30 / 62.68\\\\\n        %  & all history & \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Effects of temporal contexts for dynamic object auto labeling.} For causal models, we only use the causal point and box sequence input.}\n    \\label{tab:dynamic_context_size}\n\\end{table}",
            "tab:data_aug": "\\begin{table}[]\n    \\centering\n    \\small\n\\begin{tabular}{cc|cc} \n\\toprule\n\\multicolumn{2}{c|}{Static}        & \\multicolumn{2}{c}{Dynamic}         \\\\\nAug.        & Acc.@0.7/0.8         & Aug.        & Acc@0.7/0.8           \\\\ \n\\midrule\nAll         & \\textbf{82.28/56.92} & All         & 85.67/\\textbf{65.77}  \\\\\n$-$D2S      & 81.72/55.96          & $-$Shift    & 85.15/65.23           \\\\\n$-$FlipX    & 81.42/55.98          & $-$Scale    & 85.15/65.91           \\\\\n$-$FlipY    & 81.50/55.49          & $-$FlipY    & \\textbf{85.76}/63.66  \\\\\n$-$RotateZ  & 81.72/56.52          & $-$RotateZ  & 84.94/64.20           \\\\\n\\bottomrule\n\\end{tabular}\n    \\caption{\\textbf{Ablations of data augmentation.} We use different data augmentations for static objects and dynamic objects. ``All'' means all the augmentations are used. ``$-$X'' means removing a specific augmentation, ``X'' from the augmentation set. ``D2S'' represents the dynamic-to-static augmentation. Best results in each column are in bold.}\n    \\label{tab:data_aug}\n\\end{table}",
            "tab:different_tracker": "\\begin{table}[]\n    \\centering\n    \\small\n\\begin{tabular}{lc|cc|cc} \n\\toprule\n                            &         & \\multicolumn{2}{c|}{3D AP} & \\multicolumn{2}{c}{BEV AP}         \\\\\n\\multicolumn{2}{c|}{Tracker}          & MOT   & GT                & MOT             & GT               \\\\ \n\\midrule\n\\multirow{2}{*}{Vehicle}    & IoU=0.7 & 84.50 & 85.77    & 93.30           & 96.74   \\\\\n                            & IoU=0.8 & 57.82 & 58.81    & 84.88           & 86.18   \\\\ \n\\midrule\n\\multirow{2}{*}{Pedestrian} & IoU=0.5 & 82.88 & 83.02    & 86.32  & 86.24            \\\\\n                            & IoU=0.6 & 63.69 & 64.80    & 75.60           & 75.65   \\\\\n\\bottomrule\n\\end{tabular}\n    \\caption{\\textbf{Effects of the tracking accuracy.} ``MOT'' stands for Multi Object Tracker. ``GT'' represents Ground Truth Tracker where the ground truth boxes are used. Best results of each comparable pairs are in bold.}\n    \\label{tab:different_tracker}\n\\end{table}",
            "tab:motion_state": "\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|c|c|c}\n    \\toprule\n         Motion & \\multicolumn{2}{c}{3D AP} & \\multicolumn{2}{c}{BEV AP} \\\\\n         State & IoU=0.7 & IoU = 0.8 & IoU = 0.7 & IoU = 0.8 \\\\ \\midrule\n         Pred & 84.50 & 57.82 & 93.30 & 84.88 \\\\ \\midrule\n         GT & 84.98 & 57.95 & 93.36 & 85.13 \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Effects of the motion state estimation on the offboard 3D detection.} The metric is AP for vehicles on the Waymo Open Dataset \\emph{val} set. ``Pred'' means we are classifying the motion state (static or not) using our linear classifier. ``GT'' means we are using the ground truth boxes to classify the motion state.}\n    \\label{tab:motion_state}\n\\end{table}"
        },
        "figures": {
            "fig:teaser": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{./fig/teaser_v6}\n    \\caption{\\textbf{Our offboard 3D Auto Labeling achieved significant gains over two representative onboard 3D detectors} (the efficient PointPillar~\\cite{lang2019pointpillars} and the top-performing PVRCNN~\\cite{shi2020pv}). The relative gains (the percentage numbers) are higher under more strict standard (higher IoU thresholds). The metric is 3D AP (L1) for vehicles on the Waymo Open Dataset~\\cite{sun2020scalability} \\emph{val} set.}\n    \\label{fig:teaser}\n\\end{figure}",
            "fig:point_aggregation": "\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{./fig/point_aggregation_v2}\n    \\caption{\\textbf{Illustration of the complementary views of an object from the point cloud sequence.}\n    % Overlaying points of the object (a minivan) from different frames leads to a more complete coverage of its geometry.\n    % Since the sensor captured the object (a parked minivan) at different viewpoints across time, overlaying object points from frames leads to a much more complete coverage of its geometry.\n    Point clouds (aggregated from multiple frames) visualized in a top-down view for a mini-van.}\n    \\label{fig:point_aggregation}\n\\end{figure}",
            "fig:pipeline": "\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{./fig/3dal_pipeline_v2}\n    \\caption{\\textbf{The 3D Auto Labeling pipeline.} Given a point cloud sequence as input, the pipeline first leverages a 3D object detector to localize objects in each frame. Then object boxes at different frames are linked through a multi-object tracker. Object track data (its point clouds at every frame as well as its 3D bounding boxes) are extracted for each object and then go through the object-centric auto labeling (with a divide-and-conquer for static and dynamic tracks) to generate the final ``auto labels'', \\ie refined 3D bounding boxes.}\n    \\label{fig:pipeline}\n\\end{figure*}",
            "fig:static_model": "\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{./fig/static_model_v3}\n    \\caption{\\textbf{The static object auto labeling model.} Taking as input the merged object points in the world coordinate, the model outputs a single box for the static object.}\n    \\label{fig:static_model}\n\\end{figure}",
            "fig:dynamic_model": "\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{./fig/dynamic_model_v4}\n    \\caption{\\textbf{The dynamic object auto labeling model.} Taking a sequence of object points and a sequence of object boxes, the model runs in a sliding window fashion and outputs a refined 3D box for the center frame. Input point and box colors represent frames.}\n    \\label{fig:dynamic_model}\n\\end{figure}",
            "fig:visualization": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{./fig/visualization}\n    \\caption{\\textbf{Visualization of 3D auto labels on the Waymo Open Dataset \\emph{val} set} (best viewed in color with zoom in). Object points are colored by object types with \\textcolor{blue}{blue} for static vehicles, \\textcolor{red}{red} for moving vehicles and \\textcolor{YellowOrange}{orange} for pedestrians. Boxes are colored as: \\textcolor{green}{green} for true positive detections, \\textcolor{red}{red} for false positives and \\textcolor{cyan}{cyan} for ground truth boxes in the cases of false negatives.}\n    \\label{fig:visualization}\n\\end{figure*}",
            "fig:mvfpp_pointfusion_pipeline": "\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{./fig/pointfusion_MVF++_v6}\n    \\caption{\\textbf{Point-wise feature fusion network of MVF++.} Given an input point cloud encoding of shape $N \\times C$, the network maps it to high-dimensional feature space and extracts contextual information from different views \\ie the Bird's Eye View and the Perspective View. It fuses view-dependent features by concatenating information from three sources. The final output has shape $N \\times 144$, as a result of concatenating dimension-reduced point features of shape $N \\times 128$ with 3D segmentation features of shape $N \\times 16$.} \n    \\label{fig:mvfpp_pointfusion_pipeline}\n\\end{figure*}"
        }
    }
}