{
    "meta_info": {
        "title": "Sampling-based Estimation of the Number of Distinct Values in  Distributed Environment",
        "abstract": "In data mining, estimating the number of distinct values (NDV) is a\nfundamental problem with various applications. Existing methods for estimating\nNDV can be broadly classified into two categories: i) scanning-based methods,\nwhich scan the entire data and maintain a sketch to approximate NDV; and ii)\nsampling-based methods, which estimate NDV using sampling data rather than\naccessing the entire data warehouse. Scanning-based methods achieve a lower\napproximation error at the cost of higher I/O and more time. Sampling-based\nestimation is preferable in applications with a large data volume and a\npermissible error restriction due to its higher scalability. However, while the\nsampling-based method is more effective on a single machine, it is less\npractical in a distributed environment with massive data volumes. For obtaining\nthe final NDV estimators, the entire sample must be transferred throughout the\ndistributed system, incurring a prohibitive communication cost when the sample\nrate is significant. This paper proposes a novel sketch-based distributed\nmethod that achieves sub-linear communication costs for distributed\nsampling-based NDV estimation under mild assumptions. Our method leverages a\nsketch-based algorithm to estimate the sample's {\\em frequency of frequency} in\nthe {\\em distributed streaming model}, which is compatible with most classical\nsampling-based NDV estimators. Additionally, we provide theoretical evidence\nfor our method's ability to minimize communication costs in the worst-case\nscenario. Extensive experiments show that our method saves orders of magnitude\nin communication costs compared to existing sampling- and sketch-based methods.",
        "author": "Jiajun Li, Zhewei Wei, Bolin Ding, Xiening Dai, Lu Lu, Jingren Zhou",
        "link": "http://arxiv.org/abs/2206.05476v1",
        "category": [
            "cs.DB"
        ],
        "additionl_info": "11 pages"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nEstimating the number of distinct values (NDV) in data mining is a fundamental but critical problem. NDV estimation can improve the efficiency of database tasks~\\cite{charikar2000towards,haas1995sampling,ozsoyoglu1991estimating,hou1989processing,naughton1990estimating}, and it also has many applications in other areas. These studies include estimating the unseen species in ecological studies~\\cite{bunge1993estimating,valiant2013estimating}, data compression~\\cite{lemire2011reordering}, network security~\\cite{cohen2019cardinality}, and statistics~\\cite{hou1988statistical}. A standard method of calculating NDV involves scanning the table, followed by sorting or hashing. When calculating the exact NDV for any distribution, which takes $O(N\\log(N))$ time, where $N$ is the size of the data, sorting is unavoidable. With the help of the streaming algorithms such as the FM Sketch~\\cite{flajolet1983probabilistic} and the HyperLogLog Sketch~\\cite{flajolet2007hyperloglog}, it is sufficient to scan the table once to provide a high-precision estimation for distinct values. However, scanning the entire table incurs high I/O and time costs as the data grows in size. Therefore, sampling is a frequently used approach for obtaining an acceptable approximate solution in applications where scalability is the primary concern.\n\n\\header{\\bf Motivation. }The motivation comes from estimating the NDV in large-scale distributed environments. First of all, scanning-based methods incur high I/O costs, limiting their scalability. On the other hand, while sampling-based NDV estimators reduce the I/O cost on a single machine~\\cite{brutlag2002block}, they may lead to high communication costs while transmitting the samples. For example, we consider the GEE~\\cite{charikar2000towards} estimator. GEE uses the equation $\\hat{D}_{GEE}=\\sqrt{\\frac{1}{q}}f_1+\\sum_{i\\geq 2}f_i$ to estimate the NDV of data, where $q$ is the sample rate and $f_i$ is the {\\em frequency of frequency} of the sample (i.e. $f_i$ is the number of elements that appear exactly $i$ times in the sample). To compute the {\\em frequency of frequency} of the sample, we need the frequency dictionaries of items on each machine. Most items are different, which means that the total size of frequency dictionaries is close to the sample size. ~\\cite{charikar2000towards} provides theoretical evidence that an adequate sample rate ($\\ge 0.1\\%$) is needed to obtain a reasonable NDV estimator. Therefore, if the data set has a large number of distinct values, we will end up with a massive dictionary to store the frequency of the samples. If the raw data is at the terabyte level, the sample is at the gigabyte level. In distributed systems where we need to transfer the dictionaries of the samples across the machines, such large dictionaries will lead to high communication costs, limiting the system's scalability. \n%For CRC\n%Our experiments find that merging 10 GB samples from different machines (about 500 million pieces of samples) needs to transmit about 1 GB, just as shown in Figure~\\ref{fig:communication}. With the growth of data, efficiency has become a problem.\n\n\n\n\n% First, the traditional methods do not work well. Scanning-based methods are costly when table is very large and sampling-based methods do not consider the distributed setting. \n\n\n% Although \n\n%  Besides, high communication cost means more I/O cost for distributed environments, just as shown in Figure~\\ref{fig:spark}. It is difficult to estimate NDV when the data set has a large number of distinct values and the data scale is particularly large.\n\n\n%The intuition is that the fewer times an element appears, the more difficult it appears in the sample. Thus, the elements that appear less frequently in the sample can be used to infer the number of elements that have not been sampled. This is why computing $f_1$ is critical for NDV estimation. To compute $f_1$ in a distributed system where samples are scattered over multiple machines, we must first identify the items that appear only once on all machines, which requires the computation of the sample's whole frequency dictionary. In fact, as we shall prove in Section 3.1, computing $f_1$ with relative error in a distributed system with only two machines requires a communication cost that is linear to the size of the sample. \n \n\n\n% For example, we assume data is stored in 2 machines. Only when an element appears once in one machine and does not appear in another machine, it will contribute to $f_1$. So only when we have the frequency dictionaries in two machines can we return the true $f_1$. This also works with more complicated cases. \n\n\n% In the worst case, all the elements are different. Transmitting the dictionaries is equal to transmitting the whole samples, which will result in a vast communication cost.\n% In actual applications, we will be faced with calculating the data of different elements\n% In a real-world application where the raw data volume exceeds 100 TB and the sample rate is 0.01, the dictionary of the sample in each machine may has a size of over 1TB. Even if the speed of transferring the dictionaries is 100MB/s, it will take 3 hours just for one query, which is unacceptable for actual applications.\n\n%   and thus incurs a significant communication cost.\n  \n\n \n\\header{\\bf Distributed Model for Sampling-based NDV Estimation.} In this paper, we consider the problem of extending various classic sampling-based NDV estimators~\\cite{ozsoyoglu1991estimating,shlosser1981estimation,chao1992estimating,charikar2000towards} to distributed settings with sub-linear communication costs.\nWe assume the target data is partitioned and stored on multiple machines. Rather than directly computing the entire sampling distribution, we provide a novel sketch-based algorithm to avoid transferring the frequency dictionaries of the samples. Additionally, we will provide theoretical demonstrations and experiments to prove that our method will maintain high accuracy while reducing transmission costs. \n\n% In particular, we combines various techniques from the following three areas to obtain the desirable communication cost.\n\nOur computation framework is shown in Figure~\\ref{fig:model}. We assume that the data is stored in a file system and uniformly distributed across multiple machines. Each machine may store terabytes of data, and the objective is to compute the NDV over the data union. One approach is to scan each database using sketching algorithms such as HyperLogLog\\cite{flajolet2007hyperloglog} and then merge the sketches on a master node to avoid high communication costs. However, this approach necessitates scanning the entire database, which results in a high I/O cost. Another possibility is to collect a sample from each machine and merge their dictionaries based on their frequency distributions in the master node. If data is not uniformly distributed, ~\\cite{brutlag2002block} also suggests using block sampling to avoid scanning the whole table. While sampling reduces I/O costs, the frequency dictionaries may be significant due to the large data volume and high sampling rate, resulting in a high communication cost. The purpose of this paper is to develop a method that combines the best of both worlds: a sampling-based NDV estimation algorithm that does not require a cross-machine transfer of sample frequencies.\n\nAt first glance, the above goal can be achievable by scanning each sample with a HyperLogLog sketch and merging the sketches in the master node. However, this approach yields an estimator for the NDV of the union of samples, not the raw data. Sampling-based NDV estimators, such as GEE, require the sample's $f_1$, which is difficult to estimate with HyperLogLog sketches. As a result, the fundamental challenge is estimating the $f_1$ of the samples without transferring the sample dictionaries between machines.\n\n% Due to the large data volume and a high sample rate, each sample may consume hundreds of gigabytes of space. \n\n% We should calculate the frequency of each machine's elements. It is impossible to return the total frequency dictionary if we do not calculate the entire frequency dictionary. Simultaneously, if the data is skewed, the frequency dictionaries will be quite large. We will not transmit the frequency dictionaries directly in our framework, preferring to thinking of them as data streams. We input the frequency into stream model sketches. By substituting sketches for transmitting frequencies, the cost of communication is significantly reduced. The remaining issue is determining the total frequency or frequency summary. \n\n\n\\header{\\bf Our contributions.} This paper estimates the NDV with low communication costs in the distributed environment and makes the following contributions. \n\\begin{itemize}[leftmargin = *]\n\t\\item We prove a lower bound that states, in the worst case, it is impossible to estimate the sample's $f_i$ in the distributed environment unless a linear communication cost is allowed. \n\t\\item We propose a distributed streaming algorithm that achieves sub-linear communication cost for computing the unique values of the sample(i.e., the unique values is the number of elements that appear once), under a mild assumption on the distribution.\n\t\\item We show how our framework can incorporate classic NDV estimators, including GEE, AE, Shlosser, CL1. We also propose several modified estimators to better cope with our framework. \n    \\item We conduct extensive experimental studies to demonstrate the scalability of our method. The proposed methods reduce the communication costs by orders of magnitude.\n\\end{itemize}"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n%We begin this section with some traditional sampling-based NDV estimators from the literature. We give the definition of some commonly used concept. Based on the simpliest estimator, GEE, we propose the formal problem definition. After the basic concept of sampling-based NDV estimation, we also introduce some other related work about streaming algorithm. \nThis section will give a brief review of the related concepts. Table~\\ref{tab:notation} summarizes the notations frequently used throughout the remainder of the work.\n%Table~\\ref{tab:notation} summarizes the notations used in this paper.\n%we will give some definition of concept in this paper. \n\n%We consider the problem of estimating the number of distinct values in a data stream with $N$ elements.\n\n\n%We begin this section by defining our computational model: the {\\em distributed model}. Then we will review some classic NDV estimators as well as some streaming algorithms  from the literature. Table\\ref{tab:notation} summarizes the notations that are frequently used throughout the remainder of the work.\n\n% study challenge and then estimating the problem's communication difficulty. Additionally, we will discuss current estimators from the literature, which will be utilized to validate the efficacy of our method. \n\n\n",
                "subsection 2.1": {
                    "name": "Sampling-based NDV Estimators",
                    "content": "\n% To ensure the accuracy of NDV estimation, using sketches to scan the full table with a relative error estimation is a general method~\\cite{flajolet2007hyperloglog}. Besides, \n% Sampling-based NDV estimator is a frequently used method for estimating NDV in large databases. \n\n% The work of \nThe NDV estimation problem aims to estimate the number of distinct values $D$ for a given data set with $N$ elements. \nSampling-based NDV estimators~\\cite{haas1998estimating,charikar2000towards,motwani2006distinct} take a small random sample of size $n$ from the data and approximate $D$ with pre-defined estimators. Under various assumptions on the data distribution, these methods provide empirical or theoretical guarantees for the quality of the estimators. For example, Motwani et al.~\\cite{motwani2006distinct} propose an estimator that works well on data that follows the Zipfian distribution. In general, these estimators take the form of a function of the {\\em frequency of frequency}, which is defined as follows. \n\n% is evaluated with a small proportion of samples from the data. These works, under the assumption of data distribution, can return the estimation of NDV by well-designed statistics in advance~\\cite{haas1998estimating,charikar2000towards,motwani2006distinct}. \n\n\\header{\\bf {\\em Frequency of frequency}. }\nThe {\\em frequency of frequency} of the raw data is composed of $F_i$, where $F_i$ is the number of elements that appear $i$ times in the raw data.\n\n% For CRC\n% \\begin{example}[frequency of frequency]\\label{ex1}\n% \tGiven a data stream $X=\\{A,B,A,C,B\\}$, the frequency dictionary of $X$ is $\\{ A: 2, B: 2. C: 1 \\}$ and the {\\em frequency of frequency} of $X$ is $\\{ 2:2,1:1\\}$. As a result, we have $F_1=1,F_2=2$. According to the  {\\em frequency of frequency} of $X$, we can calculate the distinct values of $X$, $D(X)=2+1=3$.\n% \\end{example}\n\n\\header{\\bf The number of unique values. }\nWe also define a sample's {\\em frequency of frequency} as the set of $f_i$'s, where $f_i$ is the number of elements that appear $i$ times in the sample. For convenience, $f_1$ is frequently used, naming it the number of unique values.\n%$f_1$ is the most important values of $f_i$'s.\n%Let $d$ denote the NDV of the sample, and we have $d = \\sum f_i$. Normally, the sample's {\\em frequency of frequency} can be integrated, except $f_1$, such as $D_{GEE}$. \n%Given the {\\em frequency of frequency} of the raw data, we can exactly compute the distinct values $D$ of the raw data, draw samples from the distribution of data or generate the {\\em frequency of frequency} of samples. \n %Therefore, in the following section, estimating $f_1$ is so crucial that we give it a special name, the (number of) unique values.\n\n\\header{\\bf Estimators from the Literature. }\nGiven the {\\em frequency of frequency} of samples, one can estimate the NDV of the raw data with various sampling-based NDV estimators. We review some of the estimators that will be extended in our distributed framework, including GEE~\\cite{charikar2000towards}, Shlosser~\\cite{shlosser1981estimation}, Chao~\\cite{ozsoyoglu1991estimating}, CL1~\\cite{chao1992estimating}. These estimators were proposed in the database and statistical literature. We also modify some of them to better fit our framework in the following section. The formulas of these estimators can also be found in Table~\\ref{tab:esti}. \n\\begin{itemize}[leftmargin = *]\n\\item  {\\em Guaranteed Error Estimator. }Charikar et al.~\\cite{charikar2000towards} give a hard case for estimating the distinct value in a table column.~\\cite{charikar2000towards} analyze the optimal error for the hard case and obtain the estimator as follows,\n\\begin{equation}\\label{eq:gee}\n\\hat{D}_{GEE} = \\sqrt{N/n} f_1 + \\sum_{i=2} f_i. \n\\end{equation}\nIn order to facilitate understanding and calculation, the GEE estimator can be rewritten by $d$ and $f_1$:\n\\begin{equation}\\label{eq:gee2}\n\\hat{D}_{GEE} =d+\\left(\\sqrt{N/n}-1\\right)f_1,\n\\end{equation}\nwhere $d$ is the NDV of the sample.\n\\item {\\em Chao's Estimator. }According to~\\cite{chao1984nonparametric}, Ozsoyoglu et al.~\\cite{ozsoyoglu1991estimating} apply the estimator $\\hat{D}_{Chao}=d+\\frac{f_1^2}{2f_2}$ to database NDV estimation. While $f_2 = 0$, $\\hat{D}_{Chao}$ will blow up. %According to appendix B of~\\cite{chao2010user}, we have a bias-corrected estimator for Chao's Estimator as follows.\n% \\begin{equation}\\label{eq:chao2}\n% \\hat{D}_{Chao2}=d+\\frac{f_1(f_1-1)}{2(f_2+1)}. \n% \\end{equation}\n%When $f_2$ is close to zero, however, Equation~\\ref{eq:chao2} cannot completely solve the problem of overestimation. \nWith the Assumption\\ref{ass-1} in the analysis part, the majority of elements appear once in the sample. Except for $f_1$, $f_2$ accounts for the vast majority of distinct values. It is reasonable for us to replace $f_2$ with $d-f_1$. Then we have a new estimator of Chao as:\n\\begin{equation}\\label{eq:chao3}\n\\hat{D}_{Chao3}=d+\\frac{1}{2}f_1^2/(d-f_1).     \n\\end{equation}\n%Because $\\hat{D}_{Chao2}$ is more stable than $\\hat{D}_{Chao}$ and the results are close when $f_1$ and $f_2$ are large, we will use $\\hat{D}_{Chao2}$ as Chao's estimator and do not consider $\\hat{D}_{Chao}$ further.\n%This estimator usually underestimates the actual number of distinct values. When $f_2=0$, this estimator can not be used. With the framework of our distributed profile estimation, we can use  $\\hat{f}_1$ and $\\hat{f}_2$ to estimate the Chao's estimator.\n\\item {\\em Chao Lee's Estimator. }Chao Lee's Estimators are the extension of Chao's estimator. We take the simplest one in Chao Lee's estimation family. The complete derivation process can be found in Appendix~\\ref{A-Esti}. Chao Lee's first estimator can be written as:\n\\begin{equation}\\label{eq:CL1}\n\\hat{D}_{CL1}=\\frac{d+f_1\\cdot \\max\\left\\{ \\frac{d\\sum_i i(i-1)f_i}{ (1-f_1/n)(n^2-n-1) } ,0 \\right\\} }{1-f_1/n}. \n\\end{equation}\n%Sample coverage C is defined as the fraction of classes in the population that appears in the sample:\n%$$\n%C=\\sum_{j:n_j>0} \\frac{N_j}{N}\n%$$\n%According to Turing \\cite{good1953population}, the following estimator is used for sample coverage.\n%\n%$$\n%\\hat{C}=1-f_1/n\n%$$\n%\n%To deal with the skew in the data, Chao and Lee\\cite{chao1992estimating} combine this coverage estimator with a correction term and obtain the estimator\n%\\begin{equation}\n%\\hat{D}_{CL}=\\frac{d}{\\hat{C}}+\\frac{n(1-\\hat{C})}{\\hat{C}}\\hat{\\gamma}^2 \\label{eq:cl}\n%\\end{equation}\n%\n%where $\\hat{\\gamma}^2$ is an estimator of $\\gamma^2$, the squared coefficient of variation of the frequencies as following:\n%$$\n%\\gamma^2=\\frac{(1/D)\\sum_{j=1}^D (N_j-\\bar{N})^2 }{\\bar{N}^2}\n%$$\n%According to \\cite{chao1992estimating}, there are following estimator of $\\gamma^2$:\n%\n%$$\n%\\hat{\\gamma^2}=\\max\\left\\{  \\frac{\\hat{D}_1 \\sum i(i-1) f_i}{n^2-n-1},0 \\right\\},\n%$$\n%where $\\hat{D}_1$ is an initial estimator $\\hat{D}_1=d/\\hat{C}$. When the variation coefficient of class distribution is relatively large, \\cite{chao1992estimating} suggests using the following bias-corrected version of $\\hat{\\gamma}^2$:\n%$$\n%\\tilde{\\gamma^2}=\\max\\left\\{\\hat{\\gamma^2}\\left(\\frac{1+n(1-\\hat{C})\\sum i(i-1)f_i}{n(n-1)\\hat{C}} \\right),0 \\right\\}\n%$$\n%From the above estimators of $\\gamma^2$ and \\eqref{eq:cl}, \\cite{chao1992estimating} constructs the following two estimators:\n%\\begin{equation}\n%\\hat{D}_{CL1}=\\frac{d}{\\hat{C}}+\\frac{n(1-\\hat{C})}{\\hat{C}}\\tilde{\\gamma^2} \\label{eq:cl1}\n%\\end{equation}\n%\\begin{equation}\n%\\hat{D}_{CL2}=\\frac{d}{\\hat{C}}+\\frac{n(1-\\hat{C})}{\\hat{C}}\\hat{\\gamma^2} \\label{eq:cl2}\n%\\end{equation}\n%\n%\\header{\\bf Jackknife Estimator}\n%\n%Haas et al. \\cite{haas1998estimating} propose a family of estimators, with the generalized jackknife approach\\cite{gray1972generalized}. It is of the form\n%\\begin{equation} \n%\\hat{D}=d+K\\frac{f_1}{n} \\label{eq:JE}\n%\\end{equation}\n%Different approximations for K results different estimators for D. At first, \\cite{haas1998estimating} obtain the first-order estimator:\n%\\begin{equation}\n%\\hat{D}_{uj1}=\\left( 1-\\frac{(1-q)f_1}{n}  \\right)^{-1} d \\label{eq:uj1}\n%\\end{equation}\n%\n%The second order estimator $\\hat{D}_{uj2}$ is derived with the approximation of $\\gamma^2$. Following but different form Chao and Lee\\cite{chao1992estimating}, \\cite{haas1998estimating} uses a natural method-of-moments estimator $\\hat{\\gamma}^2_{Hass}(D)$ of $\\gamma^2$ as follow:\n%\n%\\begin{equation}\n%\\hat{\\gamma}^2_{Haas}(\\hat{D})=\\max\\left(0,\\frac{\\hat{D}}{n^2}\\sum_{i=1}^n i(i-1)f_i +\\frac{\\hat{D}}{N}-1 \\right) \\label{eq:hass}\n%\\end{equation}\n%With Taylor approximations for K and \\eqref{eq:hass}, \\cite{haas1998estimating} obtain the second order jackknife estimator,\n%\\begin{equation}\n%\\hat{D}_{uj2}=\\left( 1-\\frac{(1-q)f_1}{n}  \\right)^{-1} \\left(d-\\frac{f_1(1-q)\\ln(1-q) \\hat{\\gamma}^2(\\hat{D}_{uj1}) }{q} \\right)  \\label{eq:uj2}\n%\\end{equation}\n%By replacing the expression $f_1/n$ with approximation to $E[f_1]/n$, \\cite{haas1998estimating} obtain a smoothed second-order jackknife estimator\n%\\begin{equation}\n%\\hat{D}_{sj2}=(1-(1-q)^{\\tilde{N}})^{-1}(d-(1-q)^{\\tilde{N}}\\ln(1-q)N\\hat{\\gamma^2}(\\hat{D}_{uj1})  \\label{eq:sj2}\n%\\end{equation}\n%where $\\tilde{N}$ is an estimate of the average class size, and is set to $N/\\hat{D}_{uj1}$\n\n\\item {\\em Shlosser's Estimator. }Shlosser~\\cite{shlosser1981estimation} derives the estimator,\n\\begin{equation}\\label{eq:sh}\n\\hat{D}_{Sh} = d + \\frac{f_1\\sum_i (1-q)^i f_i }{ \\sum_i iq(1-q)^{i-1} f_i }. \n\\end{equation}\nThis estimator was constructed for language dictionaries. It assumes that the population is large, the sampling fraction is non-negligible, and the proportions of classes in the sample reflect the population, namely $\\frac{E[f_i]}{E[f_1]}\\approx \\frac{F_i}{F_1}$.\n\\end{itemize}\n\n%\\begin{equation}\n%\\hat{D}_{Approx-Shlosser}=d+\\frac{E[f_0]}{E[f_1]}\\cdot f_1=d+\\frac{d-d'}{f'_1}\\cdot f_1\n%\\end{equation}\n%where $f'_1$ is the number of items which appear once in the sample of sample and d is the distinct values about sample of sample with the same sample rate.\n\n%Shlosser's estimator also conforms the model \\eqref{eq:JE} with parameter\n%$$\n%K=K_{Sh}= n \\frac{f_1\\sum_i (1-q)^i f_i }{ \\sum_i iq(1-q)^{i-1} f_i }\n%$$\n%\n%Replacing $K_{Sh}$ with approximation form, \\cite{haas1998estimating} obtain the following two estimators:\n%\n%\\begin{equation}\n%\\hat{D}_{Sh2}=d+f_1\\left(\\frac{q(1+q)^{\\tilde{N}-1}}{(1+q)^{\\tilde{N}}-1}\\right)\\left( \\frac{ \\sum_{i=1}^n (1-q)^i f_i }{\\sum_{i=1}^n iq(1-q)^{i-1}f_i}\\right) \\label{eq:sh2}\n%\\end{equation}\n%where $\\tilde{N}$ is an estimate of the average class size, and is set to $N/\\hat{D}_{uj1}$\n%\\begin{equation}\n%\\hat{D}_{Sh3}=d+f_1\\left(\\frac{\\sum_{i=1}^n iq^2(1-q^2)^{i-1}f_i}{\\sum_{i=1}^n(1-q)^i((1+q)^i-1)f_i}\\right) \\left(\\frac{\\sum_{i=1}^n(1-q)^if_i}{\\sum_{i=1}^n iq(1-q)^{i-1}f_i}\\right). \\label{eq:sh3}\n%\\end{equation}\n%\n%When calculating $\\hat{D}_{Sh2}$ for large N, it will result floating point errors. Follow \\cite{deolalikar2016extensive}, we will use $q/(1+q)$ to approximate the term in the first parentheses in \\eqref{eq:sh2}\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Sampling-based NDV Estimation in Communication Complexity Model",
                    "content": "\n%We begin by defining the distribution and sampling procedure. \n%\n%\\begin{definition}\n%\tLet $\\Delta$ be the collection of distribution of all discrete distributions. Given an unknown distribution $\\mathcal{D}\\in \\Delta$ and an independent sample $S^n$ from $\\mathcal{D}$, we aim to estimate the properties of the distribution.\n%\\end{definition}\nThis subsection will give the formal definition of the distributed sampling-based NDV estimation problem. We assume that amounts of data are dispersed over many devices. The communication cost becomes the most crucial complexity parameter in such a complicated processing system.\nTo model the distributed environment, we adapt the {\\em communication complexity model}~\\cite{yao1979some,kushilevitz1997communication}, which solely focus the communication cost. We first define the frequency vector, which is used to tally the number of times objects appear.\n\n\n% The population distribution is determined by the {\\em frequency of frequency} of data. The sampling-based estimation tries to use the {\\em frequency of frequency} of samples to estimate the distinct values of the data. \n\n\n\n\\begin{definition}[Frequency vector]\nThe associated frequency vector for a sample $S=(s_1,s_2,\\ldots,s_n)$ is $X=( x_{s_1},x_{s_2},\\ldots,x_{s_d} )$, where $x_{s_i}$ denotes the frequency of element $s_i$. To simplify, we denote $X=( x_{s_1},x_{s_2},\\ldots,x_{s_d})$ as $X=(x_1,\\ldots,x_d)$. $f^X$ denotes the {\\em frequency of frequency} of $X$.\n\\end{definition}\n%  At the heart of this paper is merging the frequency vectors and returning the {\\em frequency of frequency} of samples.\nLet $\\ell_p$ denote the $\\ell_p$ norm of the frequency vector (for the sample), and we have the following correlation between vector norms and frequency of frequency. \n\\begin{equation}\\label{eq:fan}\n    \\|X\\|_p^p = \\sum_{i=1} i^p f^{X}_i.\n\\end{equation}\n\nCombining data from multiple machines can be regarded as merging the frequency vectors across these machines. We can only transmit the frequency dictionaries of samples from multiple machines.\n\\begin{definition}[Frequency dictionaries]\nThe associated frequency dictionary for a frequency vector $X=( x_{s_1},x_{s_2},\\ldots,x_{s_d} )$ is $FD_{X}=\\{s_1:x_{s_1},s_2:x_{s_2},\\ldots,s_d:x_{s_d}\\mid x_{s_i}>0\\}$.\n\\end{definition}\n\nThe communication complexity model provides a computational model that characterizes the communication cost of such operations. In particular, we first present a simplified two-party communication model that will serve as a particular case of the communication complexity model. \n\n% merge the data stream and we attempt to merge the frequency of samples. When there are too many different elements, merging may result in significant communication costs. Meanwhile, scanning the whole table is intolerable, which can result in high IO costs and time. So our distributed sampling-based NDV estimation generates the frequencies of items on different machines and puts them into sketches. Sampling can avoid scanning the whole table. Transmitting the sketches can avoid transmitting the entire frequencies of items. To make the communication between two machines easier to understand, we define a stream model to explain the problem. \n\n\\begin{definition}[Two-party communication complexity model]\n\tAlice has a frequency vector $X=( x_{1},x_{2},\\ldots,x_{m} )$ of her sample, and Bob has a frequency vector $Y=( y_{1},y_{2},\\ldots,y_{m})$ of his sample. The goal is to compute $f=(f_1,f_2,\\ldots)$, the frequency of frequency for the union of Alice and Bob's samples, with the lowest possible communication between Alice and Bob. \n\\end{definition}\n\n% For CRC\n% \\begin{example}[Example of two-party communication complexity model]\n% \tGiven a frequency vector $X=(0,1,0,2,0,1)$ of Alice's sample and $Y=( 1,0,1,1,0,1)$ of Bob's sample, respectively. We have: $X+Y=( 1,1,1,3,0,2)$. Then the {\\em frequency of frequency} of union sample is $\\{1:3,2:1,3:1\\}$.\n% \\end{example}\n\nIn general, we assume that there are $k$ machines. A broader definition of our problem is given as follows.\n\n\\begin{definition}[Multi-party communication complexity model]\n\tLet $M_1,M_2,\\ldots,M_k$ be a series of machines. Each machine has a frequency vector $X_{M}=(x_1,x_2,\\ldots,x_m)$ of its sample. The goal is to return the {\\em frequency of frequency} of the sample union in $k$ machines with the lowest possible communication cost. \n\\end{definition}\nAfter we obtain the frequency of frequency of the sample union, we can employ NDV estimators such as Equation~\\eqref{eq:CL1} to approximate the NDV of the raw data union. \nFigure~\\ref{fig:model2} shows how to use the communication complexity model to obtain NDV estimators in real-world applications. \nNote that after accepting the input sample from each machine, specific data actions such as hashing will be performed to extract sketches. Then, rather than receiving the complete data, the model will receive small sketches to save high transmission costs. Because sketches are mergeable, the mater node will combine all the sketches and deliver the estimation.\n\n% \\subsection{Problem Definition}\n% When dealing with skew-frequency data with a large number of distinct values, conveying {\\em frequency of frequency} of samples results in a high communication cost. The purpose of this paper is to reduce the communication cost associated with determining the population's normal distribution value in a distributed setting. The following is the formal problem definition:\n% \\begin{definition}\n% \tThe data consists $N$ items and $D$ distinct values distributed across over $k$ machines. There are $n$ items with $d$ different values in total samples. The distributed sampling-based models estimate $D$ with the {\\em frequency of frequency} of samples on each machine as well as additional information, such as $\\ell_p$-norms. Our objective is to estimate $D$ with the lowest possible communication cost given a relative error rate $\\varepsilon$.\n% \\end{definition}\n\n\n\n% Significant amounts of data are usually dispersed over a large number of devices. The communication cost becomes the most crucial complexity parameter in such a complicated processing system. However, the majority of sketch approaches offer combining. The key to minimizing communication costs across several machines is to run the sketch methods on each machine and combine the sketches to produce the estimation. The merge operation is supported by the majority of stream types. It will save time and space by combining multiple stream models. As a result, the stream model is prevalent in database applications.\n\n\n% There is a strong connection between the stream model and the distributed model. \n\n"
                },
                "subsection 2.3": {
                    "name": "Streaming algorithms",
                    "content": "\nStreaming models and sketching techniques are frequently used for processing massive amounts of data. Sketch techniques enable a low-memory single-pass estimation of the attributes of data items in a stream, such as heavy hitters~\\cite{misra1982finding} and $\\ell_p$ norms~\\cite{indyk2006stable}. We now review some commonly utilized sketches in the streaming model.\n\n\\header{\\bf $\\ell_0$ norms estimation. }We employ an estimated distinct counting function int the data mining to return the NDV estimation. The HyperLogLog~\\cite{flajolet2007hyperloglog} is used as the basic algorithm. HyperLogLog is a near-optimal technique for estimating distinct values. In order to deliver an exact answer, all the distinct values must be saved, which can consume a large amount of space. However, if we allow for a relative error to estimate the distinct values, we can still scan the data once and save a great deal of space in the process. Estimation's extremely compact data structure is referred to as sketches. There are numerous techniques for estimating unique values using sketches, including KMV~\\cite{bar2002counting} and HyperLogLog~\\cite{flajolet2007hyperloglog,heule2013hyperloglog}. These algorithms scan the data once, use a hashing mechanism and take up relatively little space. %Because the KMV algorithm is relatively easy to understand, we use it as a running example. On the other hand, \nHyperLogLog is more accurate than KMV and will be used in the trials. %In comparison to sampling techniques, $\\ell_0$ sketch is more precise because it is theoretically guaranteed. However, as the data scale grows, scanning the columns of tables incurs a high cost in terms of I/O. As a result, sampling techniques are advantageous for reducing the size required to deal with the data.\n\n\\header{\\bf $\\ell_2$-norms estimation. }The estimator~\\eqref{eq:CL1} has an item $\\sum_i i^2f_i$ and computing all the $f_i$ while maintaining accuracy is difficult. Fortunately, $\\sum_i i^2f_i$ is the $\\ell_2$ norms of frequency vector $X$, and we have several algorithms to estimate $\\ell_2$ norms in the streaming model. Except for $\\ell_0$ estimation, $\\ell_2$ estimation is the most extensively studied problem in $\\ell_p$ estimation. Similar to $\\ell_0$ estimation, $\\ell_p$ estimation employs hashing to project the frequency and summarize the frequency moments using hash values, such as~\\cite{coppersmith2004improved}. Indeed, these estimations of frequency moments concentrate on those elements that always exist, such as~\\cite{alon1999space}. We primarily employ the $\\ell_2$ norms in this research to estimate the distinct values of scaled data. The AMS Sketch~\\cite{alon1999space} is the standard method for estimating the $\\ell_2$ norms. Given parameters $\\varepsilon$ and $\\delta$, the summary employs space $\\mathcal{O}(\\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta})$ and ensures that its estimate is within the relative $\\varepsilon$ error of the actual $\\ell_2$ norms with a probability of at least $1-\\delta$. The Count Sketch~\\cite{charikar2002finding} is thought of as an extension of the earlier AMS Sketch~\\cite{alon1999space}. We use Count Sketch to calculate $\\sum_i i^2f_i$ in experiments.\n"
                }
            },
            "section 3": {
                "name": "Distributed sampling-based NDV estimation",
                "content": "\nIn this section, we concentrate on the sampling-based estimation of NDV in a distributed environment. We first present the negative results, which inspire us to consider additive error and introduce the mild assumption in Section 4.\n\n",
                "subsection 3.1": {
                    "name": "Negative Result",
                    "content": "\n\\header{\\bf Exact computation of $f_1$'s} Almost all estimators based on sampling requires $f_1$ to estimate NDV. For instance, GEE estimator requires simply $f_1$ and $d$. Therefore, the natural idea is to compute $f_1$'s of the sample union by delivering the dictionaries of frequency vectors across the machines. However, as data volume increases, transmitting the dictionary of frequency vectors incurs a high communication cost. For the data with many different values, each machine will have a massive frequency dictionary. We will incur high costs in sending the dictionaries. \n\n% Take GEE as an example.\n% \\begin{example}[Hardness]\n% \tThe data is distributed on machines X and Y. Assume there are N items and 90\\% of items only appear once, and 10\\% of items are given items. $D\\approx 0.9N$. If we fix the sample rate as 0.01, the sample size will be 0.01N, which combined with 90\\% distinct values and 10\\% the same item. The {\\em frequency of frequency} of origin data is $\\{F_1=0.9N, F_{0.1N}=1 \\}$. The {\\em frequency of frequency} of samples is $\\{f_1=0.009N,f_{0.001N}=1$\\}. GEE estimator uses $f_1$ and $d$. $\\hat{D}_{GEE}\\approx 0.09N (N\\rightarrow \\infty)$.\n% \\end{example}\n\n% If using the scanning method, we have to read all the N elements, which may result in high IO costs. If using a sampling method, we only need to deal with 1\\% or 0.1\\% of the data. Because the data is distributed on two machines, we have to transmit at least 90\\% of the data to ensure computing the merge frequency of frequency of data, which will result in almost a communication cost of 0.9\\%N. With the growth of data, 0.9\\%N can be hundreds of gigabytes, which is a hard problem. We not only want to reduce the IO costs of scanning all of the data, but also want to control the cost of communication. \n\n\n\\header{\\bf Approximating $f_1$'s with relative error. } In a distributed context, we can estimate $d$ with a relative error using the notion of $\\ell_0$ sketch, such as HyperLogLog. The operations of HyperLogLog are like the operations of the set, except for intersections. Set intersection returns the common elements of two sets. Computing $f_1$ is equivalent to computing the size of the intersection of two sets. Calculating the $f_1$ of the two devices is the simplest case of merging two frequency dictionaries. We prove and establish Theorem~\\ref{thm:1} that estimating the size of the set intersections can be reduced to computing the merging frequency dictionary of two machines.\n\n\\begin{theorem}\\label{thm:1}\n\tCalculating the size of set intersection can be reduced to calculating the merging frequency dictionaries of two machines. Assume $m$ is the size of $A$ and $B$'s frequency vectors $Z$. Estimating $f_1(Z)$ with relative error $\\varepsilon$, the communication complexity of estimation is $\\Omega(m)$.\n\\end{theorem}\n\n% \\begin{proof}\n% We begin with the problem of set intersection. Given two sets, $A=(a_1,a_2,\\ldots, a_m)$ and $B=(b_1,b_2,\\ldots,b_m)$, the size of $A\\cap B$, represented by $|A\\cap B|$ must be calculated. The sets can be expressed as frequency vectors. Assume the initial configuration, where $X$ and $Y$ denote the frequency vectors of $A$ and $B$, respectively. If element $a$ is a member of set $A$, we have key $a$ and the associated value $1$ in frequency vector $X$. If element $a$ does not belong to set $A$, nothing of $a$ can be stored in $X$. If an element is a member of $A\\cap B$, it will appear more than once in the final merging frequency vector. We convert the size of two sets' intersection, $A\\cap B$, into calculating $d-f_1$ of $X+Y$. Thus, the set intersection problem can reduce to the problem of calculating the merge frequency dictionaries of two machines, i.e., calculating the $f_1$ and $d$ of $X+Y$. According to~\\cite{tamm1995deterministic}, the communication complexity of $C(m_n)$ of the cardinality of the set intersection will be determined up to one bit:\n% $$\n% n+\\lceil \\log_2(n+1) \\rceil -1 \\leq C(m_n) \\leq n + \\lceil \\log_2(n+1)\\rceil .\n% $$\n% The set intersection has a communication complexity of $\\Omega(m)$. Now, we demonstrate that estimating $f_1$ with relative error is equivalent to using $f_1$ to detect if an element is in the intersection of two sets. The essence of the set intersection problem is to determine whether an element takes a value of 0 or 1 in the intersection results. If we solve the set intersection problem using the relative-error estimation of $f_1$, the element takes either 0 or $v\\in(\\varepsilon,\\frac{1}{\\varepsilon})$. Thus, even if we estimate $f_1$ with relative error, we can still solve the set intersection problem. Assume that there is an algorithm that can accurately predict $f_1(X+Y)$ with relative error but has a communication complexity of $o(m)$. Then we can use this algorithm to solve the set intersection problem with communication complexity, a contradiction. As a result, the communication complexity associated with estimating $f_1(X+Y)$ with relative error is $\\Omega(m)$\n% \\end{proof}\n\nIn other words, if we do not transmit the complete dictionary of samples, we cannot guarantee the relative error $f_1$ in all situations. \n\n%\\section{Algorithm}\n%\\vspace{+0.5em}\n"
                },
                "subsection 3.2": {
                    "name": "Estimate $f_1$ with Small Communication",
                    "content": "\n\n%Computing $|f_A|$ or $|f_B|$ happens on one machines, which has no communication cost or error. With the principle of inclusion-exclusion, we have %following equation:\n%$$\n%|A\\cap B| = |A| + |B| - |A\\cup B|\n%$$\n%We have $\\|f_A\\|_1=|A|$, and only need consider $|A\\cup B|$. If an element is in $A\\cup B$, it will contribute \n\nWe present an algorithm for reducing the communication cost of estimating $f_1$'s under mild assumptions. Before diving into technical details, we first present the high-level ideas of our method via a simple two machines distributed model.\n\n\n\\header{\\bf Estimate $f_1$ on two machines. }\n% We begin with a fundamental problem, that of computing the $f_1$ of the merge frequency vector using $\\ell_0$ norms.\nWe now consider a two-party communication model where Alice and Bob each possess a sample with frequency vectors $X=( x_1,\\ldots,x_m)$, and $Y=( y_1,\\ldots,y_m)$, respectively. The goal is to estimate $f_1$ for the union $X+Y$ without transferring the entire sample $X+Y$. By sorting, Alice and Bob can obtain a frequency dictionary. According to Alice's dictionary, Alice can acquire vector $X_{=1}$ which contains the elements that appear just once, and the vector $X_{\\neq 0}$, which has all of Alice's elements. The number of distinct items in the distributed environment will be maintained using the $\\ell_0$ sketch, such as HyperLogLog. The precise count of distinct elements in vector $X$ can be expressed as $\\| X \\|_0$. Because the $\\ell_0$ sketch supports the merge operation, $\\|X+Y\\|_0$ represents the number of distinct elements in the merge of Alice's and Bob's $\\ell_0$ sketch. To calculate $f_1$, we make use of the following expression:\n\\begin{equation}\\label{eq:f1}\nf_1(X+Y) = \\|X_{=1}+Y\\|_0-\\|Y\\|_0 + \\|X+Y_{=1}\\|_0 - \\|X\\|_0.\n\\end{equation}\nFor proof, note that Equation~\\eqref{eq:f1} is divided into two parts: $\\|X_{=1}+Y\\|_0-\\|Y\\|_0$ and $\\|X+Y_{=1}\\|_0 - \\|X\\|_0$. If we exchange $x$ and $y$, the two parts just swap values, and the formula remains the same. Thus, the analysis for the two parts is similar. We analyze the former first. With the definition of $\\|\\cdot\\|_0$, we have \\begin{equation}\\label{eq:bridge}\n\\|X_{=1}+Y\\|_0-\\|Y\\|_0=\\#\\{ X_{=1} \\lor Y_{\\neq 0}\\} -\\#\\{Y_{\\neq 0}\\}. \n\\end{equation}\nWith the set operations, we can derive\n\\begin{equation}\\label{eq:set}\n%\\begin{aligned}\n\\#\\{ X_{=1} \\lor Y_{\\neq 0}\\} -\\#\\{Y_{\\neq 0}\\}=\\#\\{ (X_{=1} \\lor Y_{\\neq 0})\\land Y_{=0}\\}= \\#\\{ X_{=1} \\land Y_{=0}\\}.\n%\\end{aligned}\n\\end{equation} \n$\\#\\{ X_{=1} \\land Y_{=0}\\}$ denotes the number of elements that appear once in $X$ but not in $Y$. So $f_1$ is equal to the sum of $\\#\\{ X_{=1} \\land Y_{=0}\\}$ and $\\#\\{ Y_{=1} \\land X_{=0}\\}$.\nTherefore, combining Equation~\\eqref{eq:bridge}~\\eqref{eq:set}, we can derive\n$$\n\t%     \\begin{aligned}\n\\|X_{=1}+Y\\|_0-\\|Y\\|_0 + \\|X+Y_{=1}\\|_0 - \\|X\\|_0=f_1(X+Y).\n%=& \\#\\{ X_{=1} \\land Y_{=0}\\} + \\#\\{Y_{=1} \\land X_{=0}\\}\\\\\n%\\end{aligned}\n$$\nThe calculated expression of $f_1$ is not unique. However, the expression above shows the connection between the set operation and the calculation of $f_1$. Next, we extend the process of calculating $f_1$ from two machines to multi-machines.\n\n\n\n%\\begin{equation}\n%\\begin{aligned}\n%\t&\\|X_{=1}+Y\\|_0-\\|Y\\|_0 + \\|X+Y_{=1}\\|_0 - \\|X\\|_0\\\\\n%=& \\#\\{ X_{=1} \\lor Y_{\\neq 0}\\} -\\#\\{Y_{\\neq 0}\\} + \\#\\{ Y_{=1} \\lor X_{\\neq 0}\\} -\\#\\{X_{\\neq 0}\\}\\\\\n%=&  \\#\\{ (X_{=1} \\lor Y_{\\neq 0})\\land Y_{=0}\\} +\\#\\{ (Y_{=1} \\lor X_{\\neq 0}) \\land X_{= 0} \\}\\\\\n%=& \\#\\{ X_{=1} \\land Y_{=0}\\} + \\#\\{Y_{=1} \\land X_{=0}\\} =f_1(X+Y). \n%\\end{aligned}\n%\\end{equation}\n%{\\hz HZ: For the above proof, please add some connecting descriptions instead of pure mathematical derivations. For example: Then we can derive ...; Thus, we have:  ...; Therefore, ...; Consequently, ...}. \n%To return the entire histogram of streams, the cost of communication between machines will be $\\mathcal{O}(N)$. N is the number of elements. However, for a determinate frequency such as $f_1$, we only need small communication cost.\n\n%For most distinct value estimator, we only need the $f_1$ and $f_2$\n\n% The calculation expression of $f_1$ is not unique. However, the expression above is a clear way to see the connection between the set operation and the $f_i$ calculation. Besides, the calculation only uses the $\\ell_0$ norms. \n\n% \\header{\\bf Extension: Estimate $f_i$ on Two Machines. }Inspired by the idea of calculating $f_1$, we separate the calculation of $f_i$ on two machines into multi-steps as follows.\n% \\begin{itemize}[leftmargin = *]\n% \t\\item Make a list of all the combinations of $f_i=f_{a+b}$, where $a$ and $b$ are integers and $a+b=i$. $a$ and $b$ denote elements appear $a$ and $b$ times in Alice and Bob's stream, respectively.\n% \t\\item According to the combination in step (1), calculate $\\#\\{X_{=a} \\land Y_{=b}\\}$\n% \t\\item Sum all the results in different combinations.\n% \\end{itemize}\n\n% We can use the combination of different norms and $X,Y$ to compute the number of elements that only appear once in the merge of $X$ and $Y$. The key to calculating $f_i$ is to calculate any combination of two machines. We show that any two-machine combination can be expressed by $\\ell_0$ of $X$ and $Y$ as follows. \n\n% {\\em Head Idea. }Alice has a frequency stream $X=( x_1,\\ldots,x_m)$, and Bob similarly has a frequency stream $Y=( y_1,\\ldots,y_m)$. We can count the elements which appear $a$ times in $X$ and do not appear in $Y$, with the following expression:\n% \\begin{equation}\\label{eq:fa}\n% \\#\\{ X_{=a} \\land Y_{=0} \\} = \\|X_{=a}+Y\\|_0-\\|Y\\|_0. \n% \\end{equation}\n% With the following expression, we can count the elements that appear $a$ times in $X$ and $b$ times in $Y$($a,b\\neq 0$), \n% \\begin{equation}\\label{eq:fab}\n% \\#\\{ X_{=a} \\land Y_{=b} \\} = \\|X_{=a}\\|_0 + \\|Y_{=b}\\|_0 - \\|X_{=a}+Y_{=b}\\|_0. \n% \\end{equation}\n\t\n% The proof of Equation~\\eqref{eq:fa} is the same as Equation~\\eqref{eq:f1}. Its essence reflects the set invariant equation \n% \\begin{equation}\\label{eq:rest}\n% |A\\cap B^c|=|A\\cup B|-|B|.\n% \\end{equation}\n% The proof of Equation~\\eqref{eq:fab} follows the principle of inclusion-exclusion. \n\n\\begin{algorithm}[t]\n\t\\caption{ Construction of Pre-Merged Sketches  \\label{alg-Pre} }\n\t\\KwIn{ NDVSketch: Array of the sketches to store the distinct values of k machines.}\n\t\\KwOut{PMSketch: The Pre-Merged sketches array to store the intermediate merged sketches from different machines.}\n\tPMSketch[$0$]$=$NDVSketch \\;\n\t$n$ $\\leftarrow$ NDVSketch.length\\;\n\t$l$ $\\leftarrow$ 0\\;\n\t\\While{$n>2$}{\n\t    $n \\leftarrow n$/2\\;\n\t    \\For{$i$ from 0 to $n$}{\n\t        Initialize $\\ell_0$ sketch tempS with PMSketch[$l$][$2*i$] \\;\n\t        tempS.merge(PMSketch[$l$][$2*i+1$])\\;\n\t        PMSketch[$l+1$].append(tempS)\\;\n\t    }\n\t}\n\t\\Return\\;\n\\end{algorithm}\n\n\\header{\\bf Extension: Estimate $f_1$ on Multi-Machines. }Consider the extension of estimating $f_1$ about two machines. If we have more than two machines, we can still use the combination of $\\ell_0$ norms to calculate the $f_1$ of these machines. $\\mathcal{M}=[M_1,M_2,\\ldots,M_k]$ is a machine list. $\\ell_0$ sketches are similar to sketches of sets that only support union operations in our framework. Considering arbitrary machine $M_j$, we just need to count the number of items that only appear in this machine. Then the sum of all items that only appear in one machine is $f_1$. \n\nFor an arbitrary machine $X$, we summarize the frequency vectors of all the machines except $X$ as $Y$. Then we can use the Equation~\\eqref{eq:bridge} to count the number of items that only appear in machine $X$. Equation~\\eqref{eq:bridge}'s left part represents the number of items that only appear in machine $X$. In the right part, $\\|X_{=1}+Y\\|_0$ represents the number of items that appear once in $X$ or other machines, and $\\|Y\\|_0$ represents the number of items in other machines. We are both using the $\\ell_0$ sketch for estimation.\n\nHowever, if we traverse all the machine cases and merge the other machines for each machine, the time complexity is $O(k^2)$, limiting the system's scalability. Fortunately, many merge operations are duplicated. We use a binary tree to store the intermediate merged sketches, constructed with $O(k)$ space and $O(k)$ time complexity. Algorithm~\\ref{alg-Pre} provides the algorithm and pseudocode for constructing the intermediate merged sketches from different machines. \n\n\n% \\begin{figure}[t]\n%  \\centering\n%  \\includegraphics[width=0.5\\textwidth]{fig/construct.pdf}\n%  \\vspace{-1em}\n%  \\caption{Example of Pre-Merged Sketches (Algorithm \\ref{alg-Pre}).}\n%  \\label{fig:cons}\n%  \\vspace{-1em}\n% \\end{figure}\nAccording to the Pre-Merged Sketches, we give the algorithm~\\ref{alg-Estif1} and pseudocode to estimate $f_1$. The level of sketches is $O(\\log k)$ and we need at most 2 sketches per level to cover all machines. \n\n% For CRC\n%At Figure~\\ref{fig:query} we give an example of query machine 2, i.e. counting the number of items that only appear once in machine 2. The orange block denotes the sketch that maintains the elements that appear once, and the blue blocks represent the sketches that maintain the elements that appear in machines, respectively. The numbers on the blocks denote the index of machines.\n% \\begin{figure}[t]\n%  \\centering\n%  \\includegraphics[width=0.4\\textwidth]{fig/query.pdf}\n%  \t\\vspace{-1em}\n%  \\caption{Example of estimating the number of items that appear once in machine2 and not appear in the other.(index=2 in Algorithm \\ref{alg-Estif1}).}\n%  \\label{fig:query}\n% \\vspace{-1em}\n% \\end{figure}\n\n\n\n\n\n%We summarize the steps of calculating $f_1$ on multi-machine as follows.\n%\\begin{itemize}[leftmargin = *]\n%\t\\item Calculate all possible combinations of $f_i=f_{a_1+a_2+\\cdots+a_k}$, where $a_1,\\cdots,a_k$ are integers and $\\sum_{j=1}^k a_j=i$. $a_j$ means elements appear $a_j$ times in machine $M_j$ 's stream, respectively.\n%\t\\item According to the combinations in step (1), calculate \n%\t$\\#\\{X_{=a_1} \\land X_{=a_2} \\land \\cdots \\land X_{=a_k}\\}$\n%\t\\item Sum all the results in different combinations.\n%\\end{itemize}\n\n% \\begin{algorithm}[t]\n% \t%\\caption{ Combination of $f_i$ on $k$ machines}\n% \t\\caption{ getCombination\\label{alg-com} }\n% \t\\KwIn{$arr$: a temporary vector to store the combination, $left$: the begin index of $arr$, $right$: the end index of $arr$, $aim$: the aim values satisfied that $arr[left]+\\cdots+arr[right-1]=aim$, $ans$ to store the result.}\n% \t\\KwOut{Using $ans$ to store the combination.}\n% \t\\SetKwFunction{getCombination}{getCombination}\n% \t\\If{$left==right-1$}{\n% \t    $ans[right-1]=aim$\\;\n% \t\t$ans.append( arr )$\\;\n% \t\t\\Return\\;\n% \t}\n% \t\\For{i from 0 to res}{\n% \t\t$arr[left]=i$ \\;\n% \t\t\\getCombination{left+1,right,aim-i,$ans$}\\;\n% \t}\n% \t\\Return\\;\n% \\end{algorithm}\n\n\n\\begin{algorithm}[t]\n\t%\\caption{ Combination of $f_/i$ on $k$ machines}\n\t\\caption{ $Esti F_1$  \\label{alg-Estif1} }\n\t\\KwIn{ PMSketch: The Pre-Merged sketches array to store the intermediate merged sketches from different machines.\n\t    F1Sketch: Array of the sketches to store the unique values of $k$ machines, namely $f_1$ of each machine.\n\t    $k$: The number of machines}\n\t\\KwOut{The estimation of $f_1$}\n\t\\SetKw{XOR}{XOR}\n    Initialize $f_1$ $\\leftarrow$ $0$\\;\n\t\\For{$index$ from $0$ to $k$}{\n\t   Initialize $B$ with empty $\\ell_0$ Sketch \\; \n\t  \\For{$l$ from $0$ to PMSketch.height-1}{\n\t  // From down to top, merge the PMSketch\\;\n\t        \\If{$l=0$}{\n\t            $B$ Merge PMSketch[$l$][$index$ \\XOR 1] \\;\n\t            $Next$ $\\leftarrow$ (($index$ \\XOR 1)/2) \\XOR 1  \\;\n\t        }\\Else{\n\t            $B$ Merge PMSketch[$l$][$Next$] \\;\n\t            $Next$ $\\leftarrow$ ($Next$/2) \\XOR 1 \\;\n\t        } \n\t  }\n\t  $B$ merges the rest PMSketch's sketches which have not merged PMSketch[0][$index$] and have not been merged by other PMSketch's sketches\\;\n\t  $f_1$ $\\leftarrow$ $f_1$-$B$.estimate() \\;\n\t  $B$ merges F1Sketch[$index$]\\;\n\t  $f_1$ $\\leftarrow$ $f_1$+$B$.estimate() \\;\n}\n\t\\Return $f_1$ \\;\n\\end{algorithm}\n\n\n% At Algorithm~\\ref{alg-com}, we provide the algorithm and pseudocode for calculating all possible combinations of $f_i=f_{a_1+a_2+\\cdots+a_k}$. Then our problem can be reduced to the following:\n% Given a sequence of $a_1, a_2, \\cdots, a_k$ of machines $M_1, M_2, \\cdots, M_k$, how do we count the items that appear $a_1, a_2, \\cdots, a_k$  times on machine $M_1, M_2, \\cdots, M_k$ respectively?\n% Indeed, we can follow the Equation~\\eqref{eq:fa} and Equation~\\eqref{eq:fab}. We categorize machines according to whether $a_j$ is zero or not. If $a_j$ is zero, index $j$ is denoted by $b_{j'}$, otherwise, it is denoted by $c_{j'}$. $k_p,k_z$ denote the number of computers for which $a_j$ is greater than or equal zero, respectively. The intersection of the machines $b_1, b_2, \\cdots, b_{k_p}$ is calculated as follows using the principle of inclusion-exclusion.\n% \\begin{equation}\n% \\begin{aligned}\n% \\#&\\Big\\{X_{=a_{b_1}} \\land X_{=a_{b_2}} \\land \\cdots \\land X_{=a_{b_{k_p}}}\\Big\\} =\\sum_{j=1}^{k_p} \\Big\\| X_{=a_{b_j}}^{(b_j)}\\Big\\|_0\\\\\n% &-\\sum_{ 1\\leq j<l\\leq n }\\Big\\|X_{=a_{b_j}}^{(b_j)}+X_{=a_{b_l}}^{(b_l)} \\Big\\|_0\n% +\\cdots+(-1)^{k_p-1}\\Big\\|\\sum_{j=1} X_{=a_{b_j}}^{(b_j)} \\Big\\|_0. \n% \\end{aligned}\n% \\end{equation}\n% $\\Big\\{X_{=a_1} \\land X_{=a_2} \\land \\cdots \\land X_{=a_k}\\Big\\}$ is composed of two distinct types of elements. One category contains elements that appear in other machines, whereas the other category contains elements that do not appear in other machines. To illustrate our algorithm, we will utilize an extension of Equation~\\eqref{eq:rest} as follows.\n% \\begin{equation}\n% \\begin{aligned}\n% |A\\cap B_1^c\\cap\\cdots B_k^c|&=|A\\cap (B_1\\cup \\cdots B_k)^c|\\\\\n% &=|A\\cup (B_1\\cup \\cdots B_k)| - | B_1\\cup \\cdots B_k| \\label{eq:bu}\n% \\end{aligned}\n% \\end{equation}\n% To divide two types of elements, we employ the principle of Equation~\\eqref{eq:bu}. The invariant is as follows.\n% \\begin{equation}\\label{eq:final}\n% \\begin{aligned}\n% \\#&\\Big\\{X_{=a_1} \\land X_{=a_2} \\land \\cdots X_{=a_k}\\Big\\}=\\#\\Big\\{X_{=a_{b_1}}  \\land \\cdots X_{=a_{b_k}} \\land \\lnot ( X_{c_1} \\lor\\cdots X_{c_{k_z}} )   \\Big\\}\\\\\n% =&\\#\\Big\\{(X_{=a_{b_1}}  \\land \\cdots X_{=a_{b_k}}) \\lor X_{c_1} \\lor\\cdots X_{c_{k_z}}\\Big\\}-\\#\\Big\\{  X_{c_1} \\lor\\cdots X_{c_{k_z}}  \\Big\\}.\n% \\end{aligned}\n% \\end{equation}\n% We transform the problem of estimating $f_i$ on $k$ machines using Equation~\\eqref{eq:final} to the problem of estimating the size of some sets on $k$ machines, namely $\\ell_0$ estimation. On the streaming model, the most famous method of estimating $\\ell_0$ is HyperLogLog~\\cite{flajolet2007hyperloglog}. HyperLogLog uses an auxiliary memory of $m$ units with the $1.04/\\sqrt{m}$ relative accuracy. That is to say, if we want a relative error $\\varepsilon$, we need to use the space $O(\\frac{\\log\\log D}{\\varepsilon^2})$ for distinct value estimation, which is almost a constant. For $k$ machines, the communication cost of our algorithm will only be $O(\\frac{k\\log\\log d}{\\varepsilon^2})$. So in the rest of the paper, the $\\ell_0$ sketches will always be HyperLogLog.\n% We give the algorithm and pseudocode to estimate the number of elements of a given combination of machines at Algorithm~\\ref{alg-eva}. We propose Algorithm~\\ref{alg-fi} to calculate any $f_i$ on $k$ machines by combining Algorithm~\\ref{alg-com} and Algorithm~\\ref{alg-eva}.\n\n% \\vspace{+1em}\n% \\begin{algorithm}[t]\n% \t\\caption{Evaluate(FiSketch,NDVSketch,Comb) \\label{alg-eva}}\n% \t\\KwIn{FiSketch: array of the sketch to store the elements that appear in each machine. NDVSketch: array of the sketch to store the elements in each machine, Comb: the required combination of each machine.}\n% \t\\KwOut{Estimation of the given combination}\n% \t\\SetKw{AND}{AND}\n% \t\\SetKw{XOR}{XOR}\n% \t\\SetKw{NOT}{NOT}\n% \tInitialize $\\ell_0$ Sketch ZeroSketch\\;\n% \t// merge the NDV sketch of Machines whose value is 0 in the combination.\n% \tInitialize IntersetID$\\leftarrow$ []\\;\n% \tk$\\leftarrow$ Comb.length\\;\n% \t\\For{ $j$ from 1 to $k$}{\n% \t\t\\If{Comb[j]=0}{\n% \t\t\tZeroSketch.merge(NDVSketch[$j$])\\;\n% \t\t}\\Else{\n% \t\t\tIntersectID.append($j$)\\;\n% \t\t}\n% \t}\n% \tm$\\leftarrow$ $2^k$\\;\n% \tans$\\leftarrow$ 0\\;\n% \t// principle of inclusion-exclusion\\\\\n% \t\\For{ $j$ from 1 to $k$}{\n% \t\tsign[$j$] $\\leftarrow$ 0\\;\n% \t\tUnionList[$2^j$].merge( FiSketch[ IntersectID[$j$][ Comb[IntersectID[$j$]]] ] )\\;\n% \t}\n% \t\\For{stat from 1 to $m$}{\n% \t\tlowbit $\\leftarrow$ stat \\AND (-stat)\\;\n% \t\tUnionList[stat] $\\leftarrow$ $\\ell_0$ Sketch\\;\n% \t\tUnionList[stat].merge(UnionList[stat \\XOR lowbit])\\;\n% \t\tUnionList[stat].merge(UnionList[lowbit])\\;\n% \t\tsign[stat]=sign[stat \\XOR lowbit] \\XOR 1\\;\n% \t}\n% \t\\For{stat form 1 to $m$}{\n% \t\tUnionList[stat].merge(ZeroSketch)\\;\n% \t\t\\If{sign[stat] \\AND 1}{\n% \t\t\tans$\\leftarrow$ ans + UnionList[stat].card()\\;\n% \t\t}\\Else{\n% \t\t\tans$\\leftarrow$ ans - UnionList[stat].card()\\;\n% \t\t}\n% \t}\n% \tans $\\leftarrow$ ans - ZeroSketch.card()\\;\n% \t\\Return{ans}\n% \\end{algorithm}\n% \\vspace{+1em}\n% \\begin{algorithm}[t]\n% \t\\caption{Estimate $f_i (X_1,X_2,\\cdots,X_k)$ \\label{alg-fi}}\n% \t\\KwIn{ $X_1,X_2,\\cdots,X_k$ the frequency dictionary of Machines $M_1,M_2,\\cdots,M_k$.}\n% \t\\KwOut{Estimation of $f_i$}\n% \t\\SetKwFunction{getCombination}{getCombination}\n% \t\\SetKwFunction{Evaluate}{Evaluate}\n% \tInitialize $f_i\\leftarrow 0$\\;\n% \tInitialize Array of $\\ell_0$ Sketch Array FiSketch$\\leftarrow$ []\\;\n% \tInitialize Array of $\\ell_0$ Sketch NDVSketch$\\leftarrow$ []\\;\n% \t\\For{ each Machine $M_j$}{\n% \t\tInitialize $\\ell_0$ Sketch Array $M_j$.S[$k$]\\;\n% \t\tInitialize $\\ell_0$ Sketch  $M_j$.NDV\\;\n% \t\t\\For{ item, value in $M_j$.elems}{\n% \t\t\t\\If{value <= $k$}{\n% \t\t\t\t$M_j$.S[value].add(item)\\;\n% \t\t\t}\n% \t\t\t$M_j$.NDV.add(item)\\;\n% \t\t}\n% \t\tFiSketch.append($M_j$.S[k])\\;\n% \t\tNDVSketch.append($M_j$.NDV[k])\n% \t}\n% \tInitialize Combination Array Carr$\\leftarrow$ [] \\;\n% \t\\getCombination(1,$k+1$,$i$,Carr)\\;\n% \t\\For{ Comb in Carr }{\n% \t\t$f_i  \\leftarrow f_i$ + Evaluate( FiSketch,NDVSketch, Comb)\\;\n% \t}\n% \t\\Return{$f_i$} \n% \\end{algorithm}\n\n\n% \\header{\\bf A Running Example. }\n% Figure~\\ref{fig:example} shows a running example of our algorithm. In the beginning, Three machines receive three data streams, respectively. In each machine, we count the data stream as a dictionary. It is necessary to transmit the total dictionary for returning $f_1$ and $d$ of the data stream accurately. If allowing an approximate solution, we can use $\\ell_0$ sketch to approximate $d$, $f_1$. To simplify the procedure, we use KMV algorithm to show our algorithms. KMV's $k'$ is set to 3. $S_{f_1^X}$ represents the $\\ell_0$ sketch which maintain the elements which appear once in machine X and $S_{d^X}$ represents the $\\ell_0$ sketch which maintain the elements which appear in machine X. KMV's hashing function is $(5*x)\\mod 7$, and the hashing results are shown in figure~\\ref{fig:example}. We also show the merging results of different machines in figure~\\ref{fig:example}. According to Equation~\\ref{eq:bridge}, we can estimate $f_1\\approx 2.33$. The accurate value of $f_1$ is 2. We can also estimate $d=7$ and the true value of $d$ is 7. \n\n% In the same way, we can separate $f_2$ into two situations:\n% \\begin{itemize}[leftmargin = *]\n%     \\item The number of elements that appear once in A and B each. We can use Equation~\\eqref{eq:fab} to estimate, and the expression is $\\big|S_{f_1^A}\\big| + \\big| S_{f_1^B}\\big| - \\big| S_{f_1^B}\\cup S_{f_1^A} \\big|$.\n%     \\item The number of elements appear twice in $A(B)$ and does not appear in $B(A)$. According to Equation~\\eqref{eq:fa}, the expression is $\\big| S_{f_2^A}\\cup S_{d^B}\\big| - \\big| S_{d^B}\\big|$($\\big| S_{f_2^B}\\cup S_{d^A}\\big| - \\big| S_{d^A}\\big|$).\n% \\end{itemize}\n% To compute $f_2$, we sum all the situations above: \n% $$f_2=\\big| S_{f_2^A}\\cup S_{d^B}\\big| - \\big| S_{d^B}\\big|+\\big| S_{f_2^B}\\cup S_{d^A}\\big| - \\big| S_{d^A}\\big|+\\big|S_{f_1^A}\\big| + \\big| S_{f_1^B}\\big| - \\big| S_{f_1^B}\\cup S_{f_1^A} \\big|=4.9.$$\n% The accurate value of $f_2$ is 4.\n\n%\\begin{figure*}[h]\n%\t\\begin{small}\n%\t\t\\centering\n%\t\t%\\vspace{-2mm}\n%\t\t\\includegraphics[width=150mm]{fig/example.png}\n%\t\t%\\includepdf[pages={1}]{fig/example.pdf}\n%\t\t%\\vspace{5mm}\n%\t\t%\\caption{A running example of our profile estimation algorithm on two machines with KMV} \\label{fig:example}\n%\t\t%\\vspace{-3mm}\n%\t\\end{small}\n%\\end{figure*}\n\n% \\begin{figure*}[t]\n% \\centering\n% \\includegraphics[width=0.8\\textwidth]{fig/running.pdf}\n% %\\vspace{-5mm}\n% \\caption{A running example of our profile estimation algorithm on three machines with KMV. }\n% \\label{fig:example}\n% \\vspace{1em}\n% \\end{figure*}\n\n"
                },
                "subsection 3.3": {
                    "name": "Coupling with Existing NDV Estimation",
                    "content": "\nIn this section, we present how our framework can be coupled with the different traditional estimators. When the order of $f_i$ is high, the computational complexity cannot be linear. However, we can avoid calculating the high-frequency term by modifying the estimators.\n\n%Before given the new form of estimators, we have to limit the conditions to use our method. We assume our data meet the follow three conditions,\n%\\begin{itemize}[leftmargin = *]\n%\t\\item The amount of data is very large. Otherwise, we could transmit all the data at a tolerable cost.\n%\t\\item The range of data is very wide.Otherwise, we could transmit the dictionary of data at a tolerable cost.\n%\t\\item Low-frequency elements occupy the main part. Actually, most estimators focus on the relationship between $f_0$ and $f_1$. That is because unseen parts normally are generated by low-frequency elements, especially $f_1$. \n%\\end{itemize}\n\n%If the data does not meet condition 2, the dictionary of data only occupy a small space. For the application in reality, people can perform our methods based on the size of frequency dictionary size. We will give the more assumation detail in next section. Before that, we transform the classical estimators to fit our computation framework. \n\n\\header{\\bf Estimate GEE and Chao with $f_1$ and $d$. }For some basic estimators, according to Equation~\\eqref{eq:gee2} and Equation~\\eqref{eq:chao3}, it is enough to estimate NDV with $f_1$ and $d$. \n\n\\header{\\bf Estimate Chao Lee's Estimator with $\\ell_2$ Norms. }For some complicated estimators, they consider the influence of all $f_i$. Estimating all of $f_i$ with high precision is difficult. However, we can estimate the $\\ell_p$ norms of frequency vector with some stream models. We observe that $\\sum i^p f_i$ can be summarized as $\\ell_p$ norms of $f$ for these complex estimators. We use the $\\hat{D}_{CL1}$ as an example to show how to adjust the estimators. The relationship between $\\|X\\|_p^p$ and $\\sum_i i^p f_i$ is given by Equation~\\eqref{eq:fan}. Targeted at Equation~\\eqref{eq:CL1}, we have a new expression for $\\hat{D}_{CL1}$\n\\begin{equation}\\label{eq:CL1ad}\n\\hat{D}_{CL1-Adjust}=\\frac{d+f_1\\cdot \\max\\left\\{ \\frac{d\\cdot(\\|f\\|_2^2-n) }{ (1-f_1/n)(n^2-n-1) } ,0 \\right\\} }{1-f_1/n}.\n\\end{equation}\nWe use Count Sketch with a relative error to estimate $\\|X\\|_2^2$. $\\|X\\|_2^2$ is sufficient to deal with the majority of complicate estimators.\n\n\\header{\\bf Estimate Shlosser's Estimator with resampling. }We can deduce the meaning of estimators and propose the equivalent forms of estimators that cannot be transformed into $\\ell_p$ norms. Shlosser's estimator assumes that the population size is large and $\\frac{E[f_i]}{E[f_1]}\\approx \\frac{F_i}{F_1}.$ Then we have $F_0\\approx\\frac{F_1\\cdot E[f_0]}{E[f_1]}$. When the population size is large, the sampling procedure can be seen as sampling from a Binomial distribution at a fixed sample rate $q$. Then we have $E[f_0]=\\sum_i (1-q)^if_i$ and $E[f_1]=\\sum_iiq(1-q)^{i-1}f_i$.  There is no doubt that it is difficult to calculate each $f_i$ and then return Shlosser's estimator. Nevertheless, we just need the expectation of $f_0$ and $f_1$, which can be computed by resampling. So we have the adjusted estimator of Shlosser as follows.\n$$\\hat{D}_{Sh-Adjust}=d+\\frac{f_1 \\cdot E[f_0]}{E[f_1]}=d+\\frac{f_1\\cdot (d-d^{resample})}{f_1^{resample}}.$$\nWe summarize the original and adjusted forms of NDV estimators in Table~\\ref{tab:esti}. According to Table~\\ref{tab:esti}, we only require $f_1$, $d$ and $\\|f\\|_2^2$ for calculating sampling-based NDV.\n\n% \\begin{table*}[h]\n% \t%\\begin{small}\n% \t\t\\centering\n% \t\t\\vspace{+1mm}\n% \t\t\\caption{Estimators and their Approximation.}\n% \t\t%\\vspace{-2mm}\n% \t\t\\begin{tabular}{r c c l}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\tEstimator & Original Expression & Adjusted Expression & Required Variables\\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}$ & $ \\sqrt{\\frac{N}{n}} f_1 + \\sum_{i=2} f_i $&$\\hat{d}+\\left(\\sqrt{\\frac{N}{n}-1}\\right)\\hat{f}_1$  &$\\hat{d} , \\hat{f}_1$\\\\\n% \t\t\t$\\hat{D}_{Chao2}$ & $d+\\frac{f_1(f_1-1)}{2(f_2+1)}$&$\\hat{d}+\\frac{\\hat{f}_1^2}{2(\\hat{d}-\\hat{f}_1)}$&$\\hat{d},\\hat{f}_1$\\\\ \n% \t\t\t$\\hat{D}_{CL1}$ & $\\frac{d+f_1\\cdot \\max\\left\\{ \\frac{d\\sum_i i(i-1)f_i}{ (1-f_1/n)(n^2-n-1) } ,0 \\right\\} }{1-f_1/n}$&$\\frac{\\hat{d}+\\hat{f}_1\\cdot \\max\\left\\{ \\frac{\\hat{d}\\cdot(\\|\\hat{f}\\|_2^2-n) }{ (1-\\hat{f}_1/n)(n^2-n-1) } ,0 \\right\\} }{1-\\hat{f}_1/n}$& $\\hat{d},f_1,\\|\\hat{f}\\|_2^2,n$ \\\\\n% \t\t\t$\\hat{D}_{Sh}$ &$\\hat{d} + \\frac{f_1\\sum_i (1-q)^i f_i }{ \\sum_i iq(1-q)^{i-1} f_i }$&$d+\\frac{\\hat{f}_1\\cdot (\\hat{d}-d^{resample})}{f_1^{resample}}$ &$\\hat{d},f_1,d^{resample},f_1^{resample}$\\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:esti}\n% \t%\\end{small}\n% \\end{table*}\n%\\vspace{-1em}\n  \n \n\n\n"
                }
            },
            "section 4": {
                "name": "Analysis",
                "content": "\nIn this section, we first give a mild assumption about the data distribution. Next, we will provide a theoretical demonstration that we will have a relative error under this assumption. Besides, we analyze the communication cost of our algorithm to evaluate the GEE estimator. We will also give a theoretical analysis of the error in GEE. \n\n\\header{\\bf Assumption on Distribution. }If we want to count $f_1$ of samples accurately, the communication cost is determined by the distinct values of samples. Without any prior hypothesis, we suppose that data is distributed evenly among each machine. Our estimators focus on estimating $d$ and $f_1$, according to Table~\\ref{tab:esti}. The following is the mild assumption.\n%As long as we keep a HyperLogLog sketch on each machine and merge them into the master node, we can guarantee relative error of estimating. The next subsection will prove that it is impossible to estimate $f_1$ with a relative error in any situation. When $f_1\\geq c\\cdot d,c\\in (0,1)$, we can still calculate $f_1$ with a relative error. So we give the assumption as follows and show that it is a mild assumption.\n\n\\newtheorem{assumption}{Assumption}[section]\n\\begin{assumption}\\label{ass-1}\nIf we uniformly sample data from some known distributions or real-world datasets with a small sample ratio $q$, we have $f_1 \\geq c \\cdot d, c\\in (0,1)$, where $c$ will not be too small, implying that the majority of elements in the sample will appear only once.\n\\end{assumption}\n\nBecause some distribution's $f_1/d$ is close to zero, implying that $f_1$ and $d$ are small, we can directly transfer the frequency dictionaries. In fact, in any case, we should calculate the frequency dictionaries at first. So we can find out whether the sample meets the Assumption~\\ref{ass-1} when obtaining the frequency dictionary on each machine.\n\nBesides, under different distributions with fixed parameters, we can adjust the sample rates to achieve $f_1\\geq c\\cdot d$. To simplify the calculation, we use $Poi(iq)$ to approximate $Binomial(i,q)$, where $q$ is the sample ratio because we have a large population. Then, for a series of $F_i$, we have the following expression for $f_1$.\n$$\nE[f_1]=\\sum_i iq e^{-iq}\\cdot F_i.\n$$\nTo compute the distinct values of the sample, we have the expression $E[d]=E[D-f_0]=\\sum_i F_i(1-e^{iq})$. Combining the calculation expressions of $E[f_1]$, $E[d]$, and $f_1/d\\geq c$, we should solve the inequation:\n\\begin{equation}\\label{eq:error}\n\\frac{\\sum iq\\cdot e^{-iq}F_i }{\\sum F_i (1-e^{-iq}) }\\geq c.\n\\end{equation}\n% We give some well-known distributions as examples. Consider the frequency of data that follows the Poisson distribution, with a probability density function, $p(k;\\lambda)=\\frac{\\lambda^k}{k!}e^{-\\lambda}$, i.e., $F_i=\\frac{\\lambda^i}{i!}e^{-\\lambda}\\cdot N$. We get the following inequation by plugging the $F_i$ of the Poisson distribution into Equation~\\eqref{eq:error}.\n% $$\n% \\frac{\\sum_i iq\\cdot e^{-iq} \\cdot \\frac{\\lambda^i}{i!}\\cdot e^{-\\lambda} }{\\sum_i \\frac{\\lambda^i}{i!}\\cdot e^{-\\lambda} (1-e^{-iq}) }\\geq c.\n% $$ \n% With the invariant $\\sum_k^\\infty \\frac{\\lambda^k }{k!}=e^{\\lambda}$,  we simplify the expression above as follows,\n% \\begin{equation}\\label{eq:limit}\n% \\frac{\\lambda q e^{ \\lambda e^{-q}-\\lambda-q}}{1-e^{\\lambda(e^{-q}-1)}}\\geq c. \n% \\end{equation}\n% The relationship between $\\lambda$, $c$ and sample rate $q$ is shown in Equation~\\eqref{eq:limit}. Fixed sample rate $q$, the left term of Equation~\\eqref{eq:limit} decreases monotonously with $\\lambda$. In other words, if we know the Poisson distribution parameters and ratio of $f_1$ and $d$, we can determine the sample rate $q$. \nFor any distribution, we can use Equation~\\eqref{eq:error} to determine $q$ or $c$. Even when $c$ takes a relatively small value, we still can guarantee the accuracy. For example, as our experiment has shown, in realistic data, $f_1/d\\approx 0.14$(LO\\_ORDERKEY), 0.0028(LO\\_ORDTOTALPRICE), and 0.0018(LO\\_REVENUE) are also acceptable. We need to control the accuracy of HyperLogLog. After this preparation, we will analyze the error of estimating $f_1$ in the next part.\n\n\\header{\\bf Error of $f_1$ Estimation.} %Flajolet et al.~\\cite{flajolet2007hyperloglog} argue that the estimator of HyperLogLog follows a Gaussian distribution when the central limit theorem is used. If the estimator of $d$ in each machine achieves $\\varepsilon'$ relative error with constant probability provided $m\\geq 1/\\varepsilon'^2$~\\cite{cormode2020small}. $\\varepsilon_d$ denotes the standard error for $d$. $\\varepsilon$ denotes the standard error for $f_1$. \nAll the HyperLogLog estimators will be bound by $\\varepsilon_d$. One HyperLogLog requires $O(1/\\varepsilon_d^2\\log(1/\\delta)\\log\\log n )$ bits with the median trick according to~\\cite{cormode2020small}, where $\\delta$ is the probability of exceeding the bound. The theorem is as follows.\n\\begin{theorem}\\label{the-1}\nThe given data uniformly distributed in $k$ machines meets the Assumption~\\ref{ass-1} with parameter $c$. Using the HyperLogLog($\\delta,n$) with relative error $\\epsilon_d$ as $\\ell_0$ sketch, we only need to send two sketches for each machine. The total communication cost of Algorithm~\\ref{alg-Estif1} is $$O(k/\\varepsilon^2_d \\log(1/\\delta^2)\\log\\log(n),$$ which takes $O(k\\log k)$ merge operations and $k$ HyperLogLog($\\delta,n$).\n\\end{theorem}\n\n\n% \\begin{proof}\n% Calculating the elements that exist only once in a given machine $A$ is expressed as $f_1^{(A)} = \\big|S_{f_1^A} \\cup S_{d^{\\lnot A}} \\big| - \\big| S_{d^{\\lnot A}} \\big| $. The error of $f_1^{(A)} $ is equal to the sum of the errors of $S_{f_1^A} \\cup S_{d^{\\lnot A}}$ and $S_{d^{\\lnot A}} $, which are constrained by $\\varepsilon_d$. Applying the Union bound to all the errors, we have\n% $$\n% \\Big| \\hat{f}_1-f_1 \\Big| \\leq 2k \\varepsilon_d d \\leq 2kc \\varepsilon_d f_1,\n% $$\n% where $c$ is the ratio between $f_1$ and $d$. When we set $\\varepsilon=2kc\\varepsilon_d$, we obtain a relative error estimation of $f_1$. Since we have $k$ machines and calculating $f_1$ requires $\\ell_0$ sketches about $S_{f_1^{(X)}}$ and $S_{d^{X}}$, calculating $f_1$ takes a total of $O(k/\\varepsilon^2\\log(1/\\delta)\\log\\log n )$. %Even for a more complicated target, $f_i$, the communication cost will only be $O(ik/\\varepsilon^2\\log(1/\\delta)\\log\\log n )$. \n% \\end{proof}\n\n\\header{\\bf Error of GEE Estimator. }Charikar et al.~\\cite{charikar2000towards} provide a lower bound on the ratio error for sample-based NDV estimators. Formally, the {\\em ratio error} of an estimation $\\hat{D}$ w.r.t the genuine $D$ is\n\\begin{equation}\n    Error(\\hat{D},D)=\\max\\{\\hat{D}/D,D/\\hat{D}\\}.\n\\end{equation}\n\nCharikar et al.~\\cite{charikar2000towards} also give and prove the fact as follows.\n\\newtheorem{fact}{Fact}[section]\n\\begin{fact}\\label{the-2}\nThe expected ratio error of GEE is $O(\\sqrt{n/r})$ when it samples $r$ values from any possible input of size $n$.\n\\end{fact}\nAccording to Theorem~\\ref{the-1} and Fact~\\ref{the-2}, we also have the ratio error guarantee for $\\hat{D}_{GEE}$ computed by our algorithms as follows.\n\n\\begin{theorem}\\label{the-3}\nThe expected ratio error of GEE is $O(\\varepsilon\\sqrt{n/r})$ when it samples $r$ values from any possible input of size $n$ and is computed by Algorithm~\\ref{alg-Estif1} with relative error $\\varepsilon$.\n\\end{theorem}\n% \\begin{proof}\n% Following the proof of~\\cite{charikar2000towards}, the original expected value of estimator GEE is within a factor of $e\\sqrt{n/r}(1+o(1))$ of correct answer. According to Theorem~\\ref{the-2}, the relative error of $d$ and $f_1$ is less than $\\epsilon$. $\\hat{D}_{GEE}$ has a positive linear relationship with $f_1$ and $d$. So $f_1$ and d will contribute a factor $\\varepsilon$ to the correct answer. Following the proof of~\\cite{charikar2000towards}, the expected ratio error of $\\hat{D}_{GEE}$ based on our algorithm will be $O(\\varepsilon\\sqrt{n/r})$.\n% \\end{proof}\n\n\\header{\\bf Remark. }The reason we require Assumption\\ref{ass-1} is that, without any assumption, there is no relative-error estimation for $f_1$. We have proved that the set intersection problem can be reduced to the problem of merging frequency dictionaries. We only need to show that there is no relative error estimation for the size of the set intersection.\n\nFor the size of set intersection, we have the invariant as follows,\n\\begin{equation}\n\\big| A\\cap B\\big|=\\big|A\\big| + \\big| B\\big| - \\big| A\\cup B \\big|.  \\label{eq-inter}\n\\end{equation}\n\nAccording to Equation~\\eqref{eq-inter}, if we return $\\big| A\\cup B \\big|$ with a relative error $\\varepsilon\\big| A\\cup B \\big|$, the lower bound on $\\big| A\\cap B \\big|$ estimation error is $\\big|A\\big| + \\big| B\\big|-\\varepsilon \\big| A\\cup B \\big|$. As a result, we will not know the relative inaccuracy of all possible scenarios. For instance, consider the case where we control the relative error rate $\\varepsilon'$, and there is no intersection between $A$ and $B$. As long as $\\varepsilon'$ is not equal to 0, for $\\big| A\\cup B \\big|$, then the intersection of $A$ and $B$ will always have a relative infinity error. While set intersection does not support relative error estimation, it appears that we can still manage the absolute error in some range for set intersection problems and $f_1$ estimation. The key to minimizing the relative error of set intersections is ensuring that the size of the intersection is sufficiently large. For example, if the size of the set intersection $c\\dot | A \\cap B|$ equals $| A \\cup B|$, the set intersection error will be $c\\cdot | A \\cup B|=c\\varepsilon' |A\\cap B|$. The same holds for the assessment of $f_1$.\n\n% \\header{\\bf Remark. }This subsection will supplement the reason why we should construct Assumption\\ref{ass-1}. If we do not have any assumption, there is no such a relative-error estimation for estimating $f_1$. Given that we have established that the set intersection problem can be reduced to the problem of calculating the merge frequency dictionaries of two machines, all that remains is to demonstrate that the set intersection problem does not involve the estimation of relative error.\n\n% For the set intersection size estimation problem, we have the invariant as follows,\n% \\begin{equation}\n% \\big| A\\cap B\\big|=\\big|A\\big| + \\big| B\\big| - \\big| A\\cup B \\big|.  \\label{eq-inter}\n% \\end{equation}\n\n% According to Equation~\\eqref{eq-inter}, if we return $\\big| A\\cup B \\big|$ with a relative error $\\varepsilon\\big| A\\cup B \\big|$, the lower bound on $\\big| A\\cap B \\big|$ estimation error is $\\big|A\\big| + \\big| B\\big|-\\varepsilon \\big| A\\cup B \\big|$. As a result, we will not know the relative inaccuracy of all possible scenarios. For instance, consider the case where we control the relative error rate $\\varepsilon$, and there is no intersection between $A$ and $B$. As long as $\\varepsilon$ is not equal to 0, for $\\big| A\\cup B \\big|$, then the intersection of $A$ and $B$ will always have a relative error of infinity. While set intersection does not support relative error estimation, it appears that we can still manage the absolute error in some range for set intersection problems and $f_i$ estimation. The key to minimizing the relative error of set intersection is ensuring that the intersection area is sufficiently large. For example, if the size of the set intersection $c\\dot | A \\cap B|$ equals $| A \\cup B|$, the set intersection error will be $c\\cdot | A \\cup B|=c\\varepsilon |A\\cap B|$. The same holds true for the assessment of $f_i$ \n% estimation. We can also get a relative error for $f_i$ estimation when the elements which appear $i$ times domains a significant proportion of all different elements.\n\n%\n%\n%Consider the probability density function of Zipf's distribution as follow:\n%$$\n%p(k;s)=\\frac{1/k^s}{\\zeta(s)}\n%$$\n%where $\\zeta(s)$ is Riemann's Zeta function and $\\zeta(s) =\\sum_{n=1}^\\infty \\frac{1}{n^s}$. Under the Zipf distribution, there will be a theoretical guarantee. First, the radio between $f_1$ and $d$ can be computed as follow,\n%Given sample size n, we have the expectation of $f_1$\n%$$\n%E[f_1|n] = p(1;s)\\cdot n = \\frac{n}{\\zeta(s)}\n%$$\n%\n%$$\n%\\begin{aligned}\n%E[d|n] & = \\sum_{i=1}^\\infty p(i;s)\\cdot \\frac{n}{i} \\\\\n%&=\\frac{n}{\\zeta(s)} \\sum_{i=1}^\\infty  [\\frac{1}{i} 2^{-s}]\\\\\n%&=\\frac{n\\zeta(s+1)}{\\zeta(s)}\n%\\end{aligned}\n%$$\n%\n%For Zipf distribution, the radio between $f_1$ and d will be $\\frac{1}{\\zeta(s+1)}$. Because the parameter s of Zipf distribution should be greater than 1. Then we have:\n%$$\n%\\frac{E[f_1]}{E[d]}=\\frac{1}{\\zeta(s+1)}>\\frac{1}{zeta(2)}>\\frac{6}{\\pi^2}\n%$$\n%\n%Consider the simplest case, the discrete distribution consists of a single $F_i$. For example, there are 1000 elements which appear 10 times for each so $F_{10}$ is equal to 1000. Consider the $\\frac{f_1}{d}$ of such a distribution.\n%\n%$$\n%E[f_1]= F_i \\cdot \\binom{i}{1}q(1-q)^{i-1}\n%$$\n%\n%$$\n%E[d]=D-E[f_0]=F_i-F_i\\cdot (1-q)^i\n%$$\n%\n%Then we have\n%$$\n%\\frac{E[f_1]}{E[d]}=\\frac{iF_i q(1-q)^{i-1}}{F_i-F_i(1-q)^i}=\\frac{iq(1-q)^{i-1}}{1-(1-q)^i}\n%$$\n%\\input{rela.tex}\n%\\vspace{+1em}\n"
            },
            "section 5": {
                "name": "Experiment",
                "content": "\nIn this section, we experimentally evaluate our method in different distributed environments. Our code is publicly available.\\footnote{\\url{https://github.com/llijiajun/NDV_Estimation_in_distributed_environment.git}}\n%\\vspace{+0.5em}\n\n\\header{\\bf Datasets. } We evaluate our experiments on  two synthetic datasets and one real-world dataset, the star schema benchmark(SSB)~\\cite{o2009star}. One of the synthetic datasets follows the Poisson distribution with different mean (e.g., [50, 100, 200]), and the other follows the Zipfian distribution with different skewness factors (e.g., [1.2, 1.5 and 2]). Additionally, We modify the population size of synthetic data to manipulate the data's scale. The size of the population ranges from $10^{9}$ to $10^{12}$. We employ three columns of the star schema benchmark(SSB) dataset: LO\\_ORDERKEY, LO\\_ORDTOTALPRICE, and LO\\_REVENUE, abbreviations for orderkey, totalprice and revenue. We use the fact table with a scaling factor of 15,000, resulting in about 900M rows of sample data. Figure~\\ref{fig:all}(a), (c), and (e) depict the $F_i$'s values of three columns. we do not generate or store the raw data to simulate a large-scale dataset. Instead, we produce $F_i$'s, the {\\em frequency of frequency} of the raw data, and sample from them. %Note that we can sample a series of Binomial distributed variables to determine $f_i$'s, the sample's {\\em frequency of frequency}. For example, assuming the raw data's frequency of frequency only consists of a single $F_{10}=1000$, which means there are 1000 elements appearing tenth in population. Given a sample rate $q$, the appear times $i$ of sample's elements will follow $Binomial(10,q)$, $P(i)=\\binom{10}{i}q^i(1-q)^{10-i}$. \nTo imitate distributed environments on a single workstation, we divide the sample data into 1,024 files as the different workers. The partition of data determines the worker on Spark.\n\n",
                "subsection 5.1": {
                    "name": "Simulated Experiment",
                    "content": "\n% We compare it with the origin sampling-based method with Map Reduce to four estimators. We perform our algorithm to prove that we reduce the communication cost by two aspects:\n% \\begin{itemize}[leftmargin = *]\n%     \\item {\\em Simulated Communication Cost.} We compare the size of frequency dictionaries and sketches sent.\n%     \\item {\\em Time Cost on real-world distributed platform.} We evaluate algorithms in real-world distributed environments. We compare the time of estimation of NDV by our method with Map Reduce.\n% \\end{itemize}\nWe begin by simulating the experiments on a single machine to evaluate the communication cost incurred by various distributed NDV methods. The experiment is performed on a Linux machine with an Intel Xeon CPU clocked at 2.70GHz and 500 GB memory. All sketch algorithms are implemented in C++, including HyperLogLog and Count Sketch. \n\n\\header{\\bf Methods. } We compare our Algorithms~\\ref{alg-Estif1}, $Estif_1$ with $Exactf_i$, which corresponds to directly merging the frequency dictionaries to obtain the precise $f_i$'s. We use the $\\hat{f}_1$ and $f_i$'s calculate by two methods to form four classic NDV estimators, including Chao's~\\cite{ozsoyoglu1991estimating}, Shlosser's~\\cite{shlosser1981estimation}, CL1's~\\cite{chao1992estimating} estimator, and GEE~\\cite{charikar2000towards}. \n\n% We show that using the approximate {\\em frequency of frequency} of sample does not reduce the precision significantly.\n\n\n\n% store the raw data in our experiments to test the larger data. Because sampling results follow a binomial distribution, they can generate samples with the {\\em frequency of frequency} of population, namely $F_i$. We test the communication costs on samples rather than truly performing sample operations. For example, if the population is $F_{10}=1000$, which means there are 1000 elements appear tenth in population, and the sample rate is $q$, the {\\em frequency of frequency} of sample $f_i$ will obey $Binomial(10,q)$ and size is 1000.\n\n% Our theoretical analysis shows that our algorithms work on more skewed data. We use two kinds of synthetic datasets with different distributions. One is the Poisson distribution, and the other is following Zipfian distribution. To validate that in skewed data, the normal way of computing the profile of data will result in a high IO cost.\n\n\n\\header{\\bf Parameter Setup.} We evaluate all sampling methods with a sample rate of 0.01. Our experiments explore a variety of parameter $b$ (e.g. [10, 12, 14, 16, 18]) of HyperLogLog's. We use $\\gamma=0.1$ and $\\epsilon_{CS}=0.01$ to implement the Count Sketch. \n\n\n\n\\header{\\bf Results. }We progressively merge files on a single machine to simulate a multi-party communication complexity model. We make the following observations:\n\\begin{itemize}[leftmargin = *]\n\\vspace{0 mm}\n\\item {\\em Communication. }We record simulated communication costs, which are the size of sketches or dictionaries transferred from a workers. Figure~\\ref{fig:communication} and Figure~\\ref{fig:all}(b) show the simulated communication cost obtained by $Exact f_i$ and our method $Esti f_1$. \n% For CRC\n%As the data grows, the cost of transmitting the frequency dictionaries increases significantly. \nThe communication cost of our method is unaffected by data size or distribution parameters, whether synthetic or real-world data. The communication cost of our method is only determined by the sketch's parameters and the number of machines. %Therefore, we only show the data with the least amount of communication and the most difficult to estimation. \nWe show the communication cost of revenue in Figure~\\ref{fig:all} because revenue's $f_1/d$ is the smallest of three real-world data. The rest results will be shown in Appendix~\\ref{app-exp}. %Besides, when we take a high precision of HyperLogLog(b=18), the communication costs are much smaller the exact computation. \n%Consequently, the communication cost of our method scales perfectly with the data size. Figure~\\ref{fig:communication} also shows that our algorithm achieves a significantly lower communication cost across datasets with different distributions.\n\\item {\\em Performance. }The performance of $Esti f_1$ and the adjusted estimators vary on different datasets. Figure~\\ref{fig:all}(d) shows that using HyperLogLog with parameter $b\\leq 12$, which corresponds to an error ratio, 0.016, we can estimate $f_1$ with a relative error less than 0.1. The values without parentheses in Table~\\ref{tab:exp1.1} are the relative error between our estimators implemented by $Esit f_1$ and the original estimators implemented by $Exact f_i$. With the increase of b, we have a more accurate estimation of the original estimators, but we do not necessarily get closer to the actual values of $D$. Because some estimators, such as $\\hat{D}_{Shlosser}$ for Poisson distribution, do not perform well, our adjusted estimators also do not perform well. In the case of our experiments, $\\hat{D}_{GEE}$ and $\\hat{D}_{CL1}$ are more stable and accurate for NDV estimation. In real applications, a more appropriate estimator can eventually be used, such as $D_{Chao}$. As long as the relatively better estimator uses $f_1$, we can approximate $D$ with a tolerable error.\n\\item {\\em Scalability. }To make sure our algorithm is scalable, we count the time of $Esti f_1$ with the growth of machines. When we have thousands of machines and HyperLoglog's parameter $b=18$, the time of $Esti f_1$ will not be less than 10 seconds, as shown in Figure~\\ref{fig:all}(f). Figure~\\ref{fig:all}(f) also shows the consuming time of $Esti f_1$ with and without Pre-Merge. The blue line in Figure~\\ref{fig:all}(f) denotes the time of $Esti f_i$ without Pre-Merge which complexity is $O(k^2)$. The red line in Figure~\\ref{fig:all}(f) denotes the time of $Esti f_i$ with Pre-Merge, which argues that Algorithm~\\ref{alg-Pre} is necessary.\n\\end{itemize}\n\n\n\n\n\n\n% \\begin{table}[t]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t%\\vspace{-2mm}\n% \t\t\\caption{Exact Communication Cost. $q$=0.01,$k$=8}\n% \t\t%\\vspace{-1em}\n% \t\t\\begin{tabular}{r |l l |l l | l}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\t\\multicolumn{1}{l|}{Variable} & \\multicolumn{2}{c|}{Zipfian($s=2$)}                      & \\multicolumn{2}{c|}{Poisson($\\lambda=50$)}                      &       \\\\ \n% \t\t\tN & \\multicolumn{2}{c|}{99,981,084,038} & \\multicolumn{2}{c|}{99,999,976,360} &   \\\\\n% \t\t\tD & \\multicolumn{2}{c|}{73,076,293,095} & \\multicolumn{2}{c|}{2,041,704,130} &   \\\\\n% \t\t\t\\hline\n% \t\t\tVariable & True & Estimation & True & Estimation & Comm. Cost  \\\\\n% \t\t\t\\hline\n% \t\t\t$d$ & 9.86e+08 & 9.87e+08& 7.91e+08 & 7.91e+08  & 3MB\\\\\n% \t\t\t$f_1$ & 9.75+08 & 9.77e+08 & 6.12+08 & 6.13e+08& 3MB\\\\\n% \t\t\t$f_2$ & 9.35+06 & 9.05e+06  & 1.50+08 & 1.51e+08& 3MB\\\\\n% \t\t\t$\\|f\\|_2^2$ & 1.04e+09 & 1.27e+09 & 1.49e+09& 1.74e+09 & 19KB\\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp1.1}\n% \t\\end{small}\n% \\end{table}\n\n%Table~\\ref{tab:exp1.2}, Table~\\ref{tab:exp1.3}, and Table~\\ref{tab:exp1.4} show that replacing the exact $f_i$ with our approximate $\\hat{f}_1$ does not hurt the precision of the estimators. \n%Take the GEE estimator as an example. Our method reduces the communication cost by three orders of magnitudes while retaining relative error $\\max\\{\\frac{\\hat{D}}{D},\\frac{D}{\\hat{D}}\\}$and a ratio error   $\\frac{|D-\\hat{D}|}{D}$ that are almost identical to those of the exact method. \n%Table~\\ref{tab:exp1.2} and Table~\\ref{tab:exp1.3} also show that some approximate estimators outperform the exact ones when applied to a particular distribution. For instance, our Shlosser's estimator outperforms the exact one on the Poisson distribution with a parameter $\\lambda=50$. \n\n\n% We take the GEE estimator as an example. We reduce the cost of communication a thousand of times, but the relative error and ratio error of the estimator is almost the same. Additionally, as data expands, the disparity between the traditional approach and our method will get greater, as long as we preserve the data on the same number of machines. \n\n"
                },
                "subsection 5.2": {
                    "name": "Experiments on Spark",
                    "content": "\n%In the real distribution environments, We first evaluate the time cost between approximate distinct count on Spark and the sampling-based NDV estimation implemented by Map Reduce on Spark in Figure\\ref{fig:time}. \n\nIn this section, we deploy the sampling-based estimation of NDV in a real-world distributed environment and test the practical running time efficiency and I/O cost in the distributed environment. Because we use the same data, the performance on Spark is the same as in simulated experiments. We mainly compare the time cost and the I/O cost. \n\n\\header{\\bf Experimental setting.} The distributed environment includes sixteen machines with Intel(R) Xeon(R) CPUs clocked at 3.10GHz, 64 GB memory and 500 GB disk space. We implement the $Exact f_i$ algorithm, which calculates the {\\em frequency of frequency} of sample with the MapReduce framework. We also implement our algorithm, which estimates the {\\em frequency of frequency} of a sample with a sketch based on Map Reduce. The source codes on the distributed machines are implemented by PySpark and Cython on Spark version 3.1 and Hadoop version 3.3. \n\n\n\n\\header{\\bf Results.} The experiment results are shown in Figure~\\ref{fig:spark} and Table~\\ref{tab:exp2.3}. We roughly observe the I/O cost of $Exact f_i$ implemented by MapReduce on Spark's Web UI and draw Figure~\\ref{fig:spark}(a). Figure~\\ref{fig:spark}(a) illustrates that $Exact f_i$ implemented by MapReduce can cause a high I/O pressure for distributed systems. The blue line in Figure~\\ref{fig:spark}(a) is the I/O caused by reading data. The red line in Figure~\\ref{fig:spark}(a) is the shuffle I/O caused by MapReduce. With the amount of data increasing, I/O costs grow linearly. Our algorithm, $Esti f_i$ reads and processes the data in each machine's memory, so we do not have shuffle I/O. Figure~\\ref{fig:spark}(b) takes $\\hat{D}_{GEE}$ as an example and shows that our algorithm is almost ten times faster than the original MapReduce algorithm.\n\n% \\begin{table}[t]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t\\vspace{+1em}\n% \t\t\\caption{Time Cost of Poi(50), $N$=5e+11, $D$=1e+1.02e+10, $q$=0.01}\n% \t\t%\\vspace{-1em}\n% \t\t\\begin{tabular}{r c c c c}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\tEstimator & $\\hat{D}$ & $\\max\\{\\frac{\\hat{D}}{D},\\frac{D}{\\hat{D}}\\}$ & $\\frac{|D-\\hat{D}|}{D}$ & time(s)  \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exactf_i)$ &3.15e+10&3.09& 2.09  &1265\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exactf_i)$ & 1.02e+10&1.00  &0.00 & 1265\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exactf_i)$ & 2.44e+11 &23.93 &22.93 &1265\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exactf_i)$ & 1.81e+10 &1.77 &0.77  &1265 \\\\\n% \t\t\t\\hdashline\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ &3.19+10& 3.13 &2.13 & 142 \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ & 9.53e+09&0.93&0.07 &142\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$ &2.49e+11&24.35 & 23.35&205 \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ &1.05e+10 &1.03 &0.03 & 136\\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp2.1}\n% \t\\end{small}\n% \\end{table}\n\n% \\begin{table}[t]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t\\vspace{+1em}\n% \t\t\\caption{Time Cost of Zipf(2), $N$=1e+12, $D$=7.31e+11, $q$=0.01}\n% \t\t%\\vspace{-1em}\n% \t\t\\begin{tabular}{r c c c c}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\tEstimator & $\\hat{D}$& $\\hat{D}/D$ & $\\frac{|D-\\hat{D}|}{D}$ & time(s)  \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exactf_i)$ &9.70e+10& 0.13 & 0.87  &2124\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exactf_i)$ & 5.17e+11& 0.71  &0.13 & 2124\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exactf_i)$ & 9.57e+11 &1.31 &0.31 &2124\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exactf_i)$ & 1.04e+12 &1.43 &0.43  &2124 \\\\\n% \t\t\t\\hdashline\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ & 9.74e+10 & 0.13&0.87 & 262 \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ & 5.38e+11 &0.74 &0.26 &262\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$&9.63e+11&1.32 &0.32&370 \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ &4.96e+11 &0.68 &0.32 &270 \\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp2.2}\n% \t\\end{small}\n% \\end{table}\n\n\n\n\n% \\begin{table}[h]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t\\caption{Poi(50) N= 99,999,976,360,D=2,041,704,130,sample rate 0.01}\n% \t\t\\begin{tabular}{r l l l l}\n% \t\t\t\\toprule\n% \t\t\tEstimator & $\\hat{D}$ & $f_1$  & d & time(s)  \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exactf_i)$ &6.30e+09& 6.13e+08  & 7.91e+08 &362\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exactf_i)$ & 2.04e+09& 6.13e+08 &7.91e+08 & 362\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exactf_i)$ & 4.88e+10 & 6.13e+08&7.91e+08 &362\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exactf_i)$ & 3.62e+09 &6.13e+08& 7.91e+08 &362 \\\\\n\t\t\t\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ &6.23e+09& 6.00e+08 &7.85e+08& 42 \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ & 1.76e+09& 6.00e+08&7.85e+08&42\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$ &4.81e+10& 6.00e+08 &7.85e+08&63 \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ & 1.99e09& 6.00e+08& 7.85e+08& 44\\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp2.1}\n% \t\\end{small}\n% \\end{table}\n\n\\header{\\bf Sensitive Analysis. }Recall that the main idea of our algorithm is to estimate the {\\em frequency of frequency} of sample. We perform our algorithm on different scales and distributions to evaluate the efficiency of estimating $f_1$. Table~\\ref{tab:exp2.3} shows the relative error of $\\hat{f}_1$ based on our algorithm. When data size grows, the data will be distributed into more partitions. We can still guarantee that the relative error of $f_1$ is small. We also show that our algorithm has a negligible effect on the accuracy of sampling-based estimators in Appendix~\\ref{app-exp2}.\n\n\n\n%When estimating $D$ using our sampling-based method, there are two types of errors. One comes from sampling, and another comes from sketches. In Figure~\\ref{fig:rerror}, we evaluate the relative error w.r.t. estimators and $D$ to demonstrate how HyperLogLog parameter influence to estimation. On the basis of the Poi(50) distribution, we set the population size as $10^{11}$. The left of Figure~\\ref{fig:rerror} shows that with the increase of bucket number, the precision rises, and the estimators get close to their true values. It also shows that when the number of buckets is $2^8$ it is sufficient to obtain the estimations of estimators. However, the right side of Figure~\\ref{fig:rerror} shows that using fewer buckets could reduce the error of estimating NDV for some estimators. \n\n% \\begin{figure}[t]\n% \t\\centering\n% \t\\hspace{-4mm}\\includegraphics[width=47mm]{fig/fig1.pdf}\n% \t\\hspace{-6mm}\\includegraphics[width=47mm]{fig/fig2.pdf}\n% \t%\\vspace{-1em}\n% \t\\caption{Relative Error of Estimators under Different Parameters of HyperLogLog.}\n% \t\\label{fig:rerror}\n% \t%\\vspace{-1em}\n% \\end{figure}\n\n%\n%\\begin{table*}[h]\n%\t\\begin{small}\n%\t\t\\centering\n%\t\t\\caption{Estimators and their Approximation. N= 9,999,863,800}\n%\t\t\\begin{tabular}{r l l l l l}\n%\t\t\t\\toprule\n%\t\t\tEstimator & NDV & time(s) & $f_1$ & $f_2$&sample fraction \\\\\n%\t\t\t\\hline\n%\t\t\tcountApproxDistinct(spark) & 452,729,783 & 6187.97 & & &\\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-take & 142,074,165 & 125.58   &13,920,095 &&0.01\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-take & 90,289,636& 126.14 &13,920,095 &1,318,221& 0.01 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-take & 853,815,508 & 132.40 & 13,920,095 & &0.01 \\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-sample & 143,294,008& 1972.28   & &&0.01\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-sample & &&&& 0.01 \\\\\n%\t\t\t$\\hat{D}_{GEE}(MapReduce)$-take& 141,635,711  &  133.23 &   13,871,895& &0.01\\\\\n%\t\t\t$\\hat{D}_{Chao}(MapReduce)$-take& 83,869,782 & 133.23 & 13,871,895 & 1,434,303&0.01\\\\\n%\t\t\t$\\hat{D}_{Shlosser}(MapReduce)$-take& 849,813,564& 133.23&13,871,895& &0.01\\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-take & 64,821,779 & 19.08   &2,039,926 &&0.001\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-take & 13,645,680& 19.63 &2,039,926 &184,255& 0.001 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-take & 1,153,987,292 & 19.92 &2,039,926  & &0.001 \\\\\n%\t\t\t$\\hat{D}_{GEE}((MapReduce))$-take & 65,541,628 & 19.66   &2,063,226 &&0.001\\\\\n%\t\t\t$\\hat{D}_{Chao}((MapReduce))$-take & 16,703,733& 19.66 &2,063,226 &148,387& 0.001 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}((MapReduce))$-take & 1,182,766,785 & 19.66 &2,063,226 & &0.001 \\\\\n%\t\t\t\\bottomrule\n%\t\t\\end{tabular}\n%\t\t\\label{tab:esti3}\n%\t\\end{small}\n%\\end{table*}\n%\n%\\begin{table*}[h]\n%\t\\begin{small}\n%\t\t\\centering\n%\t\t\\caption{Estimators and their Approximation. N= 99,981,083,245,D=73,076,292,457}\n%\t\t\\begin{tabular}{r l l l l l}\n%\t\t\t\\toprule\n%\t\t\tEstimator & NDV & time(s) & $f_1$ & $f_2$&sample fraction \\\\\n%\t\t\t\\hline\n%\t\t\t$\\hat{D}_{GEE}(MR)$-take & 3,150,118,298& 106.53  & 99,610,036&&0.001\\\\\n%\t\t\t$\\hat{D}_{Chao}(MR)$-take & 31,175,868,349& 106.53 &99,610,036 &159,642& 0.001 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(MR)$-take & 99,421,449,061 & 106.53 & 99,610,036 & &0.001 \\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-16 & 3,023,674,065& 97.21  & 95,533,370&&0.001\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-16 & 1,346,697,672& 98.76 &95,533,370 &3,654,974& 0.001 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-16 & 93,533,486,320 & 104.89 & 95,533,370 & &0.001 \\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-32 & 3,012,951,774& 100.64 & 95,183,229&&0.001\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-32 & 1,145,223,831& 109.14&95,183,229 &4,326,380& 0.001 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-32 & 93,767,014,292 & 108.49 & 95,183,229 & &0.001 \\\\\n%\t\t\t\n%\t\t\t$\\hat{D}_{GEE}(our)$-32-H:0.005 & 3,106,893,155& 100.64 & 98,215,144&&0.001\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-32-H:0.005 & 2,710,356,484& 109.14&98,215,144 &1,847,165& 0.001 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-32-H:0.005 & 97,831,611,027 & 108.49 & 98,215,144 & &0.001 \\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-16-H:0.005 & 3,105,320,130& 99.76 & 98,163,776&&0.001\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-16-H:0.005 & 2,306,623,868& 105.82&98,163,776&2,182,734& 0.001 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-16-H:0.005 & 97,179,850,498& 107.26 & 98,163,776 & &0.001 \\\\\n%\t\t\t\n%\t\t\t$\\hat{D}_{GEE}(our)$-400-H:0.005&9,661,403,265&1150.26&96,439,468&&0.01\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-400-H:0.005&15,552,187,766&46302.45&96,439,468&31,916,058&0.01\\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-400-H:0.005&94,794,155,218&1241.27&96,439,468&&0.01\\\\\n%\t\t\t\\bottomrule\n%\t\t\\end{tabular}\n%\t\t\\label{tab:esti4}\n%\t\\end{small}\n%\\end{table*}\n%\n%\\begin{table*}[h]\n%\t\\begin{small}\n%\t\t\\centering\n%\t\t\\caption{Poi(100) c++. N= 999,994,437,D=10,102,009,sample rate 0.01}\n%\t\t\\begin{tabular}{r l l l l l}\n%\t\t\t\\toprule\n%\t\t\tEstimator & NDV & space (byte) & $f_1$ & $f_2$&d  \\\\\n%\t\t\t\\hline\n%\t\t\t$\\hat{D}_{GEE}(Real)$ &3.98e+07 &\n%\t\t\t%111,300,778\n%\t\t\t1.11e+08 & 3.72e+06&&6.35e+06\\\\\n%\t\t\t$\\hat{D}_{Chao}(Real)$ & 1.01e+07& 1.11e+08 &3.72e+06 &1.84e+06& 6.35e+06 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(Real)$ & 2.41e+08 & 1.11e+08& 3.72e+06 & &6.35e+06 \\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-16 &3.98e+07& 248  &3.72e+06&&6.35e+06\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-16 & 1.01e+07& 248 &3.72e+06&1.84e+06& 6.35e+06 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-16 & 6.35e+06 & 248 & 3.72e+06 & &6.35e+06 \\\\\n%\t\t\t\\bottomrule\n%\t\t\\end{tabular}\n%\t\t\\label{tab:esti5}\n%\t\\end{small}\n%\\end{table*}\n%\n%\\begin{table*}[h]\n%\t\\begin{small}\n%\t\t\\centering\n%\t\t\\caption{Poi(50) c++. N= 9,999,997,636,D=204,170,413,sample rate 0.01}\n%\t\t\\begin{tabular}{r l l l l l}\n%\t\t\t\\toprule\n%\t\t\tEstimator & NDV & space (byte) & $f_1$ & $f_2$&d  \\\\\n%\t\t\t\\hline\n%\t\t\t$\\hat{D}_{GEE}(Real)$ &6.30e+08 &1.27e+09 & 6.13e+07&&7.91e+07\\\\\n%\t\t\t$\\hat{D}_{Chao}(Real)$ & 2.04e+08& 1.27e+09 &6.13e+07 &1.50e+07&7.91e+07 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(Real)$ & 4.89e+09 & 1.27e+09 & 6.13e+07 & &7.91e+07\\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-16 &6.30e+08& 496  &6.13e+07&&7.91e+07\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-16 & 2.04e+08& 496 &6.13e+07&1.50e+07& 7.91e+07 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-16 & 7.91e+07 & 496 & 6.13e+07 & &7.91e+07 \\\\\n%\t\t\t\\bottomrule\n%\t\t\\end{tabular}\n%\t\t\\label{tab:esti6}\n%\t\\end{small}\n%\\end{table*}\n%\n%\\begin{table*}[h]\n%\t\\begin{small}\n%\t\t\\centering\n%\t\t\\caption{Poi(100) c++. N= 99,999,993,705,D=1,010,206,193,sample rate 0.01}\n%\t\t\\begin{tabular}{r l l l l l}\n%\t\t\t\\toprule\n%\t\t\tEstimator & NDV & space (byte) & $f_1$ & $f_2$&d  \\\\\n%\t\t\t\\hline\n%\t\t\t$\\hat{D}_{GEE}(Real)$ &3.98e+09 &5.31e+08 & 3.72e+08&&6.35e+08\\\\\n%\t\t\t$\\hat{D}_{Chao}(Real)$ &1.01e+09 & 5.31e+08  &3.72e+08 &1.84+e08& 6.35e+08\\\\\n%\t\t\t$\\hat{D}_{Shlosser}(Real)$ & 2.41e+10 & 5.31e+08 & 3.72e+08 & &6.35e+08 \\\\\n%\t\t\t$\\hat{D}_{GEE}(our)$-16 &5.99e+10& 496  &3.72e+08&&1.27e+09\\\\\n%\t\t\t$\\hat{D}_{Chao}(our)$-16 & 1.97e+09& 496 &3.72e+08&1.84+e08& 1.27e+09 \\\\\n%\t\t\t$\\hat{D}_{Shlosser}(our)$-16 & 1.24e+10 & 496 & 3.72e+08& &1.27e+09 \\\\\n%\t\t\t\\bottomrule\n%\t\t\\end{tabular}\n%\t\t\\label{tab:esti7}\n%\t\\end{small}\n%\\end{table*}\n\n\n%\\begin{figure}[h]\n%\\begin{small}\n% \\centering\n%   \\vspace{-2mm}\n%   % \\begin{footnotesize}\n% %\\includegraphics[height=31mm]{./Figs/alpha-nr.eps}\n% \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/01.eps}\n% \\end{minipage}\n% \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/03.eps}\n%  \\end{minipage}\n%  \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/04.eps}\n%  \\end{minipage}\n%  \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/05.eps}\n%  \\end{minipage}\n%  \\vspace{5mm}\n% \\caption{Different frequency Distributions} \\label{fig:exp1}\n%\\vspace{-5mm}\n%\\end{small}\n%\\end{figure}\n%\n%\n%\n%\\begin{figure}[h]\n%\\begin{small}\n% \\centering\n%   \\vspace{-2mm}\n%   % \\begin{footnotesize}\n% %\\includegraphics[height=31mm]{./Figs/alpha-nr.eps}\n% \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/02.eps}\n%  \\end{minipage}\n%  \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/06.eps}\n%  \\end{minipage}\n%  \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/07.eps}\n%  \\end{minipage}\n%  \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/08.eps}\n%  \\end{minipage}\n%  \\vspace{5mm}\n% \\caption{Shlosser on different frequency distributions} \\label{fig:exp1}\n%\\vspace{-5mm}\n%\\end{small}\n%\\end{figure}\n%\n%\\begin{table}[ht]\n%\\vspace{-2mm}\n%    \\centering\n%    \\begin{tabular}{c c c c}\n%    \\hline\n%        Datasets &Total Dict & Total Array &  base on hyperloglog\\\\\n%   \\hline\n%        Uniform&5,242,984 & 1,618,480 & 4,408 \\\\\n%        Poisson&2,621,552& 1,555,184&4,408\\\\\n%        zipf&1,310,824&702,816&4,408\\\\\n%        binomial&5,242,984&1,781,888&4,408\\\\\n%        tpch-loorderkey & 83,886,184 &26,470,128  &656\\\\\n%        tpch-loordtotalprice& 83,886,184 & 25,835,808 &1,136\\\\\n%    \\hline\n%    \\end{tabular}\n%    \\vspace{2mm}\n%    \\caption{$f_i$ Memory Cost(size of class)}\n%    \\label{tab:my_label}\n%\\end{table}\n%\n%\\subsubsection{Real Datasets}\n%\n%\\begin{figure}[h]\n%\\begin{small}\n% \\centering\n%   \\vspace{-2mm}\n%   % \\begin{footnotesize}\n% %\\includegraphics[height=31mm]{./Figs/alpha-nr.eps}\n% \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/lo_orderkey.eps}\n% \\end{minipage}\n% \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/lo_ordtotalprice.eps}\n%  \\end{minipage}\n%  \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/address.eps}\n%  \\end{minipage}\n%  \\begin{minipage}[t]{0.48\\textwidth}\n%  \\includegraphics[width=75mm]{fig/attributes.eps}\n%  \\end{minipage}\n%  \\vspace{5mm}\n% \\caption{Different datasets Distribution} \\label{fig:exp1}\n%\\vspace{-5mm}\n%\\end{small}\n%\\end{figure}\n%\n%\\begin{table}[ht]\n%\\vspace{-2mm}\n%    \\centering\n%    \\begin{tabular}{c c c c}\n%    \\hline\n%    Method & D & real fi  & ours.\\\\\n%   \\hline\n%   \\multicolumn{4}{c}{tpch-lo\\_orderkey}\\\\\n%    Chao  & 75,000,000& 74,970,518(0.99) & 176,543,248 (2.35) \\\\\n%    GEE &75,000,000 &31,765,035(0.42) & 32,030,413(0.43)\\\\\n%    Shlosser &75,000,000 &282,724,498(3.77) & 285,674,228(3.81)\\\\\n%    \\hline\n%    \\multicolumn{4}{c}{tpch-lo\\_ordtotalprice}\\\\\n%    Chao  & 29,799,227& 22,434,046(0.75) & 23,326,927(0.78) \\\\\n%    GEE &29,799,227 &28,992,048(0.97) & 28,726,335(0.96)\\\\\n%    Shlosser &29,799,227 &245,283,782(8.23) & 241,208,700(8.09)\\\\\n%    \\hline\n%    \\multicolumn{4}{c}{s\\_tc\\_logistics\\_order-attributes}\\\\\n%    Chao &6,478,325 &89,483,964(13.81)&4,264,165(0.65) \\\\\n%    GEE &6,478,325 &360,414(0.05)&357,913(0.05)\\\\\n%    Shlosser&6,478,325 & 3,258,751(0.50)&3,240,479(0.50)\\\\\n%    \\hline\n%    \\multicolumn{4}{c}{s\\_tc\\_logistics\\_order-address}\\\\\n%    Chao&7,358,035 & 29,803,851(4.05)&29,825,469(4.05)\\\\\n%    GEE&7,358,035&424390(0.05)&424,540(0.05)\\\\\n%    Shlosser&7,358,035&3,851,105(0.52)&3,856,780(0.52)\\\\\n%    \\hline\n%    \\end{tabular}\n%    \\vspace{2mm}\n%    \\caption{ sample rate=0.01}\n%    \\label{tab:my_label}\n%\\end{table}\n%\n%\n%\\begin{table}[ht]\n%\\vspace{-2mm}\n%    \\centering\n%    \\begin{tabular}{c c c c}\n%    \\hline\n%        $f_i$ &Real &  Estimation & time(s)\\\\\n%   \\hline\n%        $f_1$&209,892 & 206,076 & 0.66 \\\\\n%        $f_2$&189,889& 197,417& 8.22\\\\\n%        $f_3$&143,028&130,147&66.81\\\\\n%        $f_4$&98,853&102,504&466.53\\\\\n%    \\hline\n%    \\end{tabular}\n%    \\vspace{2mm}\n%    \\caption{16 $\\times$ zipf(3.5)}\n%    \\label{tab:exp16}\n%\\end{table}\n%\n%\\begin{table}[ht]\n%\\vspace{-2mm}\n%    \\centering\n%    \\begin{tabular}{c c c c}\n%    \\hline\n%        $f_i$ &Real &  Estimation & time(s)\\\\\n%   \\hline\n%        $f_1$&272,589 & 272,826 & 0.22 \\\\\n%        $f_2$&150,073& 145,993& 1.57\\\\\n%        $f_3$&79,504&79,098&7.47\\\\\n%        $f_4$&43,540&42,802&27.06\\\\\n%        $f_5$&24,200&25,858&88.71\\\\\n%        $f_6$&14,542&12,678&249.56\\\\\n%    \\hline\n%    \\end{tabular}\n%    \\vspace{2mm}\n%    \\caption{8 $\\times$ zipf(3.5)}\n%    \\label{tab:exp8}\n%\\end{table}\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\nIn this paper, we focus on a fundamental problem: extending various classic NDV estimators to distributed models with low communication costs. We propose a computation framework to estimate the number of the unique values of data in the distributed model. We also provide theoretical analysis for the communication cost of $f_1$ and GEE's estimators. Experiments on different synthetic datasets and the distributed environment demonstrate the efficiency of our algorithm. Based on the measurement of simulated and real-world experiments, we show that our method reduces communication costs. In future work, we intend to extend our algorithm to incorporate other estimations, such as entropy, by utilizing various types of sketches in the distributed environment.\n"
            },
            "section 7": {
                "name": "ACKNOWLEDGEMENTS",
                "content": "\nThis research was supported in part by Beijing Natural Science Foundation (No. 4222028), by National Natural Science Foundation of China (No. 61972401, No. 61932001), by the major key project of PCL (PCL2021A12), by Beijing Outstanding Young Scientist Program No. BJJWZYJH012019100020098 and by Alibaba Group through Alibaba Innovative Research Program. We also wish to acknowledge the support provided by Intelligent Social Governance Interdisciplinary Platform, Major Innovation and Planning Interdisciplinary Platform for the “Double-First Class” Initiative, Public Policy and Decision-making Research Lab, Public Computing Cloud, Renmin University of China.\n\n\n%\\clearpage\n\n\\bibliographystyle{ACM-Reference-Format}\n\n%\\bibliographystyle{IEEEtran}\n%\\bibliographystyle{plain}\n\\bibliography{ndv}\n\n%\\newpage\n\n\\appendix\n\n"
            },
            "section 8": {
                "name": "Proof Of Theorem",
                "content": "\n\n",
                "subsection 8.1": {
                    "name": "thm:1",
                    "content": "\n\n% \\begin{theorem}\n% \tThe problem of calculating the size of set intersection can be reduced to calculating the merging frequency dictionaries of two machines. Assume $m$ is the size of $A$ and $B$'s frequency vectors $Z$. Estimating $f_1(Z)$ with relative error $\\varepsilon$, the communication complexity of estimation is $\\Omega(m)$.\n% \\end{theorem}\n\n\\begin{proof}\nWe begin with the problem of set intersection. Given two sets, $A=(a_1,a_2,\\ldots, a_m)$ and $B=(b_1,b_2,\\ldots,b_m)$, the size of $A\\cap B$, represented by $|A\\cap B|$ must be calculated. The sets can be expressed as frequency vectors. Assume the initial configuration, where $X$ and $Y$ denote the frequency vectors of $A$ and $B$, respectively. If element $a$ is a member of set $A$, we have key $a$ and the associated value $1$ in frequency vector $X$. If element $a$ does not belong to set $A$, nothing of $a$ can be stored in $X$. If an element is a member of $A\\cap B$, it will appear more than once in the final merging frequency vector. We convert the size of two sets' intersection, $A\\cap B$, into calculating $d-f_1$ of $X+Y$. Thus, the set intersection problem can reduce to the problem of calculating the merge frequency dictionaries of two machines, i.e., calculating the $f_1$ and $d$ of $X+Y$. According to~\\cite{tamm1995deterministic}, the communication complexity of $C(m_n)$ of the cardinality of the set intersection will be determined up to one bit:\n$$\nn+\\lceil \\log_2(n+1) \\rceil -1 \\leq C(m_n) \\leq n + \\lceil \\log_2(n+1)\\rceil .\n$$\nThe set intersection has a communication complexity of $\\Omega(m)$. Now, we demonstrate that estimating $f_1$ with relative error is equivalent to using $f_1$ to detect if an element is in the intersection of two sets. The essence of the set intersection problem is to determine whether an element takes a value of 0 or 1 in the intersection results. If we solve the set intersection problem using the relative-error estimation of $f_1$, the element takes either 0 or $v\\in(\\varepsilon,\\frac{1}{\\varepsilon})$. Thus, even if we estimate $f_1$ with relative error, we can still solve the set intersection problem. Assume that there is an algorithm that can accurately predict $f_1(X+Y)$ with relative error but has a communication complexity of $o(m)$. Then we can use this algorithm to solve the set intersection problem with communication complexity, a contradiction. As a result, the communication complexity associated with estimating $f_1(X+Y)$ with relative error is $\\Omega(m)$\n\\end{proof}\n\n"
                },
                "subsection 8.2": {
                    "name": "the-1",
                    "content": "\n\n% \\begin{theorem}\n% The given data uniformly distributed in $k$ machines meet the Assumption~\\ref{ass-1}. Algorithm~\\ref{alg-Estif1} return an estimation $\\hat{f}_1$ for $f_1$ with a relative error $\\varepsilon$ by $\\ell_0$ sketch. If using the HyperLogLog($\\delta,n$) as $\\ell_0$ sketch, the communication cost of Algorithm~\\ref{alg-Estif1} is $$O(k/\\varepsilon^2 \\log(1/\\delta)\\log\\log(n) bits,$$ which takes $O(k\\log k)$ merge operations and $O(k)$ HyperLogLog($\\delta,n$).\n% \\end{theorem}\n\n\\begin{proof}\nCalculating the elements that exist only once in a given machine $A$ is expressed as $f_1^{(A)} = \\big|S_{f_1^A} \\cup S_{d^{\\lnot A}} \\big| - \\big| S_{d^{\\lnot A}} \\big| $. The error of $f_1^{(A)} $ is equal to the sum of the errors of $S_{f_1^A} \\cup S_{d^{\\lnot A}}$ and $S_{d^{\\lnot A}} $, which are constrained by $\\varepsilon_d d$. With Assumption~\\ref{ass-1}, we obtain a relative error estimation of $f_1$. Since we have $k$ machines and calculating $f_1$ requires $\\ell_0$ sketches about $S_{f_1^{(X)}}$ and $S_{d^{\\lnot X}}$, which cost $O(k)$ HyperLogLog($\\delta,n$). One HyperLogLog($\\delta,n$) costs \n\\begin{equation}\\label{hugeerror}\nO(1/\\varepsilon_d^2\\log(1/\\delta)\\log\\log n )\n\\end{equation}\nbits. Calculating $f_1$ takes $2k$ HyperLogLog, so the communication cost is $$\nO(\\frac{k}{\\varepsilon_d^2}\\log(1/\\delta)\\log\\log n) bits.\n$$\n\n\nAlgorithms~\\ref{alg-Pre} needs extra $O(k)$ HyperLogLog and $O(k)$ merge operations. Pre-Merged Sketches have $\\log k$ levels so Algorithms~\\ref{alg-Estif1} need $O(k\\log k)$ merge operations in total.  %Even for a more complicated target, $f_i$, the communication cost will only be $O(ik/\\varepsilon^2\\log(1/\\delta)\\log\\log n )$. \n\\end{proof}\n\n"
                },
                "subsection 8.3": {
                    "name": "the-3",
                    "content": "\n% \\begin{theorem}\n% The expected ratio error of GEE is $O(\\varepsilon\\sqrt{n/r})$ when it samples $r$ values from any possible input of size $n$ and is computed by Algorithm~\\ref{alg-Estif1} with relative error $\\varepsilon$.\n% \\end{theorem}\n\\begin{proof}\nFollowing the proof of~\\cite{charikar2000towards}, the original expected value of estimator GEE is within a factor of $e\\sqrt{n/r}(1+o(1))$ of correct answer. According to Theorem~\\ref{the-2}, the relative error of $d$ and $f_1$ is less than $\\epsilon$. $\\hat{D}_{GEE}$ has a positive linear relationship with $f_1$ and $d$. So $f_1$ and d will contribute a factor $\\varepsilon$ to the correct answer. Following the proof of~\\cite{charikar2000towards}, the expected ratio error of $\\hat{D}_{GEE}$ based on our algorithm will be $O(\\varepsilon\\sqrt{n/r})$.\n\\end{proof}\n\n"
                }
            },
            "section 9": {
                "name": "Other Related Works",
                "content": "\n%{\\hz {HZ: Maybe more intuitive descriptions instead of mathematical formulas. }}\n\n",
                "subsection 9.1": {
                    "name": "Other Related Sketch",
                    "content": "\n\\header{\\bf $\\ell_p$ norms estimation. }Considering the norms of frequency, $\\ell_0$ represent the distinct values of data. $\\ell_p$ norms estimation is also a widely studied problem in stream models. Equation~\\eqref{eq:fan} gives the relationship between $\\ell_p$-norms with {\\em frequency of frequency}. We just use $\\ell_2$ norms of $f$ to estimate $\\hat{D}_{CL1}$. If the estimators become more complicated than $\\hat{D}_{CL1}$, we may use others $\\ell_p$-norms to approximate. In other words, it is possible to derive an estimator by $\\ell_p$ Sketch. $\\ell_p$ sketch gives an estimation of the $\\ell_p$ norm of vector $v$ in the communication complexity model. $\\ell_p$ sketch utilizes a stable distribution proposed by Chambers et al.~\\cite{chambers1976method} to estimate $\\ell_p$ norms with a relative error. Limited by the updating time of the original $\\ell_p$ sketch, Li~\\cite{li2007very} proposes a faster updating method with a sparse random projection. \n\n\n"
                },
                "subsection 9.2": {
                    "name": "Other Estimates ",
                    "content": "\\label{A-Esti}\nIn this section, we will give some detail for complicated estimators. We also give the reason why our frameworks can handle different estimators. \n\n\\header{\\bf Chao Lee's Estimator. }\nWe have introduced the final expression of the first Chao Lee's Estimator before. In this part, we will give more details of Chao Lee's estimators.\n\nWe begin with some definitions of statistics. Sample coverage C is defined as the fraction of classes in the population that appears in the sample:\n\\begin{align*}\nC=\\sum_{j:n_j>0} \\frac{N_j}{N}. \n\\end{align*}\nAccording to Turing et al.~\\cite{good1953population}, $\\hat{C}=1-f_1/n$ is used for sample coverage. To deal with the skew in the data, Chao and Lee~\\cite{chao1992estimating} combine this coverage estimator with a correction term and obtain the estimator\n\\begin{equation}\\label{eq:cl}\n\\hat{D}_{CL}=\\frac{d}{\\hat{C}}+\\frac{n(1-\\hat{C})}{\\hat{C}}\\hat{\\gamma}^2,\n\\end{equation}\nwhere $\\hat{\\gamma}^2$ is an estimator of $\\gamma^2$, the squared coefficient of variation of the frequencies as follow.\n\\begin{align*}\n\\gamma^2=\\frac{(1/D)\\sum_{j=1}^D (N_j-\\bar{N})^2 }{\\bar{N}^2}. \n\\end{align*}\nAccording to~\\cite{chao1992estimating}, there are following estimator of $\\gamma^2$:\n\\begin{align*}\n\\hat{\\gamma^2}=\\max\\left\\{  \\frac{\\hat{D}_1 \\sum i(i-1) f_i}{n^2-n-1},0 \\right\\},\n\\end{align*}\nwhere $\\hat{D}_1$ is an initial estimator $\\hat{D}_1=d/\\hat{C}$. From the above estimator of $\\gamma^2$ and~\\eqref{eq:cl},~\\cite{chao1992estimating} constructs the following estimators:\n\\begin{equation}\\label{eq:cl11}\n\\hat{D}_{CL1}=\\frac{d}{\\hat{C}}+\\frac{n(1-\\hat{C})}{\\hat{C}}\\hat{\\gamma^2}.\n\\end{equation}\nChao and Lee~\\cite{chao1992estimating} also introduce some improved estimators based on Equation~\\eqref{eq:cl11} and other assumptions, but for these Chao Lee's estimators, $\\|f\\|_2$ is enough to estimate.\n\n\\header{\\bf Jackknife Estimator. }Haas et al.~\\cite{haas1998estimating} propose a family of estimators, with the generalized jackknife approach~\\cite{gray1972generalized}. It is of the form\n\\begin{equation}\\label{eq:JE}\n\\hat{D}=d+K\\frac{f_1}{n}.\n\\end{equation}\nDifferent approximations for $K$ results in other estimators for $D$. At first, ~\\cite{haas1998estimating} obtain the first-order estimator:\n\\begin{equation}\\label{eq:uj1}\n\\hat{D}_{uj1}=\\left( 1-\\frac{(1-q)f_1}{n}  \\right)^{-1} d. \n\\end{equation}\n\nThe second-order estimator $\\hat{D}_{uj2}$ is derived with the approximation of $\\gamma^2$. Following but different form Chao and Lee~\\cite{chao1992estimating},~\\cite{haas1998estimating} uses a natural method-of-moments estimator $\\hat{\\gamma}^2_{Hass}(D)$ of $\\gamma^2$ as follows.\n\n\\begin{equation}\n\\hat{\\gamma}^2_{Haas}(\\hat{D})=\\max\\left(0,\\frac{\\hat{D}}{n^2}\\sum_{i=1}^n i(i-1)f_i +\\frac{\\hat{D}}{N}-1 \\right). \\label{eq:hass}\n\\end{equation}\nWith Taylor approximations for $K$ and~\\eqref{eq:hass},~\\cite{haas1998estimating} obtain the second-order jackknife estimator,\n\\begin{equation}\\label{eq:uj2}\n\\hat{D}_{uj2}=\\left( 1-\\frac{(1-q)f_1}{n}  \\right)^{-1} \\left(d-\\frac{f_1(1-q)\\ln(1-q) \\hat{\\gamma}^2(\\hat{D}_{uj1}) }{q} \\right). \n\\end{equation}\nBy replacing the expression $f_1/n$ with approximation to $E[f_1]/n$,~\\cite{haas1998estimating} obtain a smoothed second-order jackknife estimator\n\\begin{equation}\\label{eq:sj2}\n\\hat{D}_{sj2}=(1-(1-q)^{\\tilde{N}})^{-1}(d-(1-q)^{\\tilde{N}}\\ln(1-q)N\\hat{\\gamma^2}(\\hat{D}_{uj1}),\n\\end{equation}\nwhere $\\tilde{N}$ is an estimate of the average class size and is set to $N/\\hat{D}_{uj1}$\n\nAlthough Jackknife estimator is very complicated, $\\|f\\|_2$ will still be enough to estimate. Since $\\hat{D}$ has a more concise form, we no longer put Jackknife estimator in the experimental comparison. \n\n\\header{\\bf Shlosser's Estimator. }Shlosser's estimator also conforms to the model~\\eqref{eq:JE} with parameter\n\\begin{align*}\nK=K_{Sh}= n \\frac{f_1\\sum_i (1-q)^i f_i }{ \\sum_i iq(1-q)^{i-1} f_i }. \n\\end{align*}\nReplacing $K_{Sh}$ with approximation form,~\\cite{haas1998estimating} obtain the following two estimators:\n\\begin{equation}\\label{eq:sh2}\n\\hat{D}_{Sh2}=d+f_1\\left(\\frac{q(1+q)^{\\tilde{N}-1}}{(1+q)^{\\tilde{N}}-1}\\right)\\left( \\frac{ \\sum_{i=1}^n (1-q)^i f_i }{\\sum_{i=1}^n iq(1-q)^{i-1}f_i}\\right),\n\\end{equation}\nwhere $\\tilde{N}$ is an estimate of the average class size and is set to $N/\\hat{D}_{uj1}$\n%\\begin{equation}\\label{eq:sh3}\n%\\hat{D}_{Sh3}=d+f_1\\left(\\frac{\\sum_{i=1}^n iq^2(1-q^2)^{i-1}f_i}{\\sum_{i=1}^n(1-q)^i((1+q)^i-1)f_i}\\right) \\left(\\frac{\\sum_{i=1}^n(1-q)^if_i}{\\sum_{i=1}^n iq(1-q)^{i-1}f_i}\\right). \n%\\end{equation}\n\nWhen calculating $\\hat{D}_{Sh2}$ for large $N$, it will result in floating point errors. Following~\\cite{deolalikar2016extensive}, we can use $q/(1+q)$ to approximate the term in the first parentheses in~\\eqref{eq:sh2}. For more complicated Shlosser's estimators, we still can resample to approximate, which also proves the applicability of our algorithm.\n\n%For example, if we fix the resample ratio as $q^2$, we can solve the item $\\sum_{i=1}^{n} i(1-q^2)^{i-1}f_i$.\n\n\n"
                }
            },
            "section 10": {
                "name": "Additional Experimental Results",
                "content": "\n",
                "subsection 10.1": {
                    "name": "Simulated Experiments",
                    "content": "\\label{app-exp}\nWe also evaluate the $Exact f_i$ and $Esti f_i$ on orderkey and totalprice. Figure~\\ref{fig:order} shows the communication costs and the relative error of two data. The conclusion is the same as revenue's. The communication cost of $Esti f_i$ is lower than $Exact f_i$'s on both synthetic data and real-world data. When the parameter $b$ of HyperLogLog is small than $12$, we can have a relative error estimation for $f_1$.\n\n\n"
                },
                "subsection 10.2": {
                    "name": "Experiments on Spark",
                    "content": "\\label{app-exp2}\n\nWe also evaluate the different estimators implemented by our method on Spark.  Table~\\ref{tab:exp2.2} shows the performance of different estimators. The conclusion is the same as the simulated experiments. Our method indeed has a negligible effect on the accuracy of sampling-based estimators. \n\n\n\n\n% \\begin{table}[t]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t\\caption{Communication Cost of Poi(50), $N$=1e+11, $D$=2.04e+09, $q$=0.01, $k$=8}\n% \t\t%\\vspace{-1em}\n% \t\t\\begin{tabular}{r c c c c}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\tEstimator & $\\hat{D}$ & $\\max\\{\\frac{\\hat{D}}{D},\\frac{D}{\\hat{D}}\\}$ & $\\frac{|D-\\hat{D}|}{D}$ & Space \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exact f_i)$ &6.30e+09 & 3.04  & 2.04 & 1.57GB\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exact f_i)$ & 2.04e+09 & 1.00 & 0.00 &1.57GB\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exact f_i)$ & 4.88e+10 &23.92 & 22.92& 1.57GB\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exact f_i)$ & 3.62e+09 & 1.77 & 0.77 & 1.57GB\\\\\n% \t\t\t\\hdashline\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ & 6.30e+09 & 3.04 & 2.04 & 3MB \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ &1.84e+09  & 1.11  & 0.10 & 3MB \\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$ & 7.91e+08 & 2.58& 0.61& 3MB \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ & 4.43e+09 &  2.17 &1.17 & 3MB \\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp1.2}\n% \t\\end{small}\n% \\end{table}\n\n% \\begin{table}[t]\n% \t\\begin{small}\n% \t\\vspace{+1em}\n% \t\t\\centering\n% \t\t\\caption{Communication Cost of Zipfian(2), $N$=1e+11, $D$=7.31e+10,  $q$=0.01, $k$=8}\n% \t\t%\\vspace{-1em}\n% \t\t\\begin{tabular}{r c c c c}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\tEstimator & $\\hat{D}$ & $\\max\\{\\frac{\\hat{D}}{D},\\frac{D}{\\hat{D}}\\}$ & $\\frac{|D-\\hat{D}|}{D}$ & Space \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exact f_i)$ & 9.77e+09 & 1.34  & 0.34 & 1.61GB\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exact f_i)$ & 5.18e+10 & 1.41 & 0.29 &1.61GB\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exact f_i)$ & 9.63e+10 &1.32 & 0.32& 1.61GB\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exact f_i)$ & 1.05e+11 & 1.37 & 0.37 & 1.61GB\\\\\n% \t\t\t\\hdashline\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ & 9.78e+09 & 1.34 & 0.34 & 3MB \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ & 4.84e+10 & 1.51 & 0.34 & 3MB \\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$ & 9.87e+08 & 1.35& 0.35& 3MB \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ & 5.40e+11 &  7.39 & 6.39 & 3MB \\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp1.3}\n% \t\\end{small}\n% \\end{table}\n\n% \\begin{table}[t]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t\\vspace{+1em}\n% \t\t\\caption{Communication Cost of Poi(50), $N$=5e+11, $D$=1.02e+10, $q$=0.01, $k$=8}\n% \t\t%\\vspace{-1em}\n% \t\t\\begin{tabular}{r c c c c}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\tEstimator & $\\hat{D}$ & $\\max\\{\\frac{\\hat{D}}{D},\\frac{D}{\\hat{D}}\\}$ & $\\frac{|D-\\hat{D}|}{D}$ & Space \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exact f_i)$ &3.15e+10 & 3.09  & 2.09 & 8.31GB\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exact f_i)$ & 1.02e+09 & 1.00 & 0.00 &8.31GB\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exact f_i)$ & 2.44e+11 &23.92 & 22.92& 8.31GB\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exact f_i)$ & 1.81e+10 & 1.77 & 0.77 & 8.31GB\\\\\n% \t\t\t\\hdashline\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ & 3.15e+10 & 3.09 & 2.09 & 3MB \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ & 9.21e+09  & 1.11 & 0.10 & 3MB \\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$ & 8.26e+09 & 1.23& 0.19& 3MB \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ & 3.72e+10 &  3.65 & 2.65 & 3MB \\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp1.4}\n% \t\\end{small}\n% \\end{table}\n\n%\\subsection{Experiments on Spark}\n\n\n% \\begin{table}[t]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t\\vspace{+1em}\n% \t\t\\caption{Time Cost of Poi(50), $N$=5e+11, $D$=1e+1.02e+10, $q$=0.01}\n% \t\t%\\vspace{-1em}\n% \t\t\\begin{tabular}{r c c c c}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\tEstimator & $\\hat{D}$ & $\\max\\{\\frac{\\hat{D}}{D},\\frac{D}{\\hat{D}}\\}$ & $\\frac{|D-\\hat{D}|}{D}$ & time(s)  \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exactf_i)$ &3.15e+10&3.09& 2.09  &1265\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exactf_i)$ & 1.02e+10&1.00  &0.00 & 1265\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exactf_i)$ & 2.44e+11 &23.93 &22.93 &1265\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exactf_i)$ & 1.81e+10 &1.77 &0.77  &1265 \\\\\n% \t\t\t\\hdashline\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ &3.19+10& 3.13 &2.13 & 142 \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ & 9.53e+09&0.93&0.07 &142\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$ &2.49e+11&24.35 & 23.35&205 \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ &1.05e+10 &1.03 &0.03 & 136\\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp2.1}\n% \t\\end{small}\n% \\end{table}\n\n% \\begin{table}[t]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t\\vspace{+1em}\n% \t\t\\caption{Time Cost of Zipf(2), $N$=1e+12, $D$=7.31e+11, $q$=0.01}\n% \t\t%\\vspace{-1em}\n% \t\t\\begin{tabular}{r c c c c}\n% \t\t\t\\toprule\n% \t\t\t\\hline\n% \t\t\tEstimator & $\\hat{D}$& $\\hat{D}/D$ & $\\frac{|D-\\hat{D}|}{D}$ & time(s)  \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exactf_i)$ &9.70e+10& 0.13 & 0.87  &2124\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exactf_i)$ & 5.17e+11& 0.71  &0.13 & 2124\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exactf_i)$ & 9.57e+11 &1.31 &0.31 &2124\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exactf_i)$ & 1.04e+12 &1.43 &0.43  &2124 \\\\\n% \t\t\t\\hdashline\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ & 9.74e+10 & 0.13&0.87 & 262 \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ & 5.38e+11 &0.74 &0.26 &262\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$&9.63e+11&1.32 &0.32&370 \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ &4.96e+11 &0.68 &0.32 &270 \\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp2.2}\n% \t\\end{small}\n% \\end{table}\n\n\n\n\n% \\begin{table}[h]\n% \t\\begin{small}\n% \t\t\\centering\n% \t\t\\caption{Poi(50) N= 99,999,976,360,D=2,041,704,130,sample rate 0.01}\n% \t\t\\begin{tabular}{r l l l l}\n% \t\t\t\\toprule\n% \t\t\tEstimator & $\\hat{D}$ & $f_1$  & d & time(s)  \\\\\n% \t\t\t\\hline\n% \t\t\t$\\hat{D}_{GEE}(Exactf_i)$ &6.30e+09& 6.13e+08  & 7.91e+08 &362\\\\\n% \t\t\t$\\hat{D}_{Chao}(Exactf_i)$ & 2.04e+09& 6.13e+08 &7.91e+08 & 362\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Exactf_i)$ & 4.88e+10 & 6.13e+08&7.91e+08 &362\\\\\n% \t\t\t$\\hat{D}_{CL1}(Exactf_i)$ & 3.62e+09 &6.13e+08& 7.91e+08 &362 \\\\\n\t\t\t\n% \t\t\t$\\hat{D}_{GEE}(Ours)$ &6.23e+09& 6.00e+08 &7.85e+08& 42 \\\\\n% \t\t\t$\\hat{D}_{Chao}(Ours)$ & 1.76e+09& 6.00e+08&7.85e+08&42\\\\\n% \t\t\t$\\hat{D}_{Shlosser}(Ours)$ &4.81e+10& 6.00e+08 &7.85e+08&63 \\\\\n% \t\t\t$\\hat{D}_{CL1}(Ours)$ & 1.99e09& 6.00e+08& 7.85e+08& 44\\\\\n% \t\t\t\\bottomrule\n% \t\t\\end{tabular}\n% \t\t\\label{tab:exp2.1}\n% \t\\end{small}\n% \\end{table}\n\n% \\begin{figure}[t]\n% \t\\centering\n% \t\\hspace{-4mm}\\includegraphics[width=47mm]{fig/fig1.pdf}\n% \t\\hspace{-6mm}\\includegraphics[width=47mm]{fig/fig2.pdf}\n% \t%\\vspace{-1em}\n% \t\\caption{Relative Error of Estimators under Different Parameters of HyperLogLog.}\n% \t\\label{fig:rerror}\n% \t%\\vspace{-1em}\n% \\end{figure}\n"
                }
            }
        },
        "tables": {
            "tab:esti": "\\begin{table}[t]\n\t%\\begin{small}\n\t\t\\centering\n\t\t\\caption{Estimators and their Approximation.}\n\t\t\\vspace{-1em}\n\t\t\\scalebox{0.9}{\n\t\t\\begin{tabular}{r c c}\n\t\t\t\\toprule\n\t\t\t\\hline\n\t\t\tEstimator & Original Expression & Adjusted Expression \\\\\n\t\t\t\\hline\n\t\t\t$\\hat{D}_{GEE}$ & $ \\sqrt{\\frac{N}{n}} f_1 + \\sum_{i=2} f_i $&$\\hat{d}+\\left(\\sqrt{\\frac{N}{n}-1}\\right)\\hat{f}_1$  \\\\\n\t\t\t$\\hat{D}_{Chao2}$ & $d+\\frac{f_1(f_1-1)}{2(f_2+1)}$&$\\hat{d}+\\frac{\\hat{f}_1^2}{2(\\hat{d}-\\hat{f}_1)}$\\\\ \n\t\t\t$\\hat{D}_{CL1}$ & $\\frac{d+f_1\\cdot \\max\\left\\{ \\frac{d\\sum_i i(i-1)f_i}{ (1-f_1/n)(n^2-n-1) } ,0 \\right\\} }{1-f_1/n}$&$\\frac{\\hat{d}+\\hat{f}_1\\cdot \\max\\left\\{ \\frac{\\hat{d}\\cdot(\\|X\\|_2^2-n) }{ (1-\\hat{f}_1/n)(n^2-n-1) } ,0 \\right\\} }{1-\\hat{f}_1/n}$ \\\\\n\t\t\t$\\hat{D}_{Sh}$ &$\\hat{d} + \\frac{f_1\\sum_i (1-q)^i f_i }{ \\sum_i iq(1-q)^{i-1} f_i }$&$d+\\frac{\\hat{f}_1\\cdot (\\hat{d}-d^{resample})}{f_1^{resample}}$ \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\t}\n\t\t\\vspace{-1em}\n\t\t\\label{tab:esti}\n\t%\\end{small}\n\\end{table}"
        },
        "figures": {
            "fig:model": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=60mm]{model.pdf}\n    \\vspace{-1em}\n\t\\caption{Sampling-based  estimation of NDV model in the distributed environment}\n\t\\vspace{-2em}\n\t\\label{fig:model}\n\\end{figure}",
            "fig:model2": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{stream.pdf}\n\\caption{Estimating with Streaming in Distributed Environment}\n\\label{fig:model2}\n\\vspace{-2em}\n\\end{figure}",
            "fig:all": "\\begin{figure}[t]\n\t\\centering\n\t\\vspace{-1em}\n\t\\includegraphics[width=80mm]{communicate2.pdf}\n\t\\vspace{-1em}\n\t\\caption{Distribution of Real-World datasets and Communication Cost of Different Methods with Different Parameter. b is the parameter of HyperLogLog.}\n\t\\label{fig:all}\n\t\\vspace{-1em}\n\\end{figure}",
            "fig:communication": "\\begin{figure}[t]\n\t\\centering\n    \\hspace{-4mm}\\includegraphics[width=45mm]{communicate.pdf}\n    \\hspace{-6mm}\\includegraphics[width=45mm]{communicate1.pdf}\n    \t\\vspace{-1em}\n\t\\caption{Communication Cost of Different Methods with Different Parameter. $\\lambda$ is the parameter of Poisson distribution; s is the parameter of Zipfian distribution; $b$ is the parameter of HyperLogLog.}\n\t\t\\vspace{-1em}\n\t\\label{fig:communication}\n\\end{figure}",
            "fig:spark": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=86mm]{communicate3.pdf}\n\t\\vspace{-1em}\n\t\\caption{I/O cost and Time Cost of $\\hat{D}_{GEE}$}\n\t\t\\vspace{-1em}\n\t\\label{fig:spark}\n\n\\end{figure}",
            "fig:order": "\\begin{figure}[t] \n\t\\centering\n\t\\includegraphics[width=85mm]{communicate4.pdf}\n\t\\caption{Communication Cost of Different Methods with Different Parameter. $\\lambda$ is the parameter of Poisson distribution; s is the parameter of Zipfian distribution; b is the parameter of HyperLogLog.}\n\t\\label{fig:order}\n\t\\vspace{-1em}\n\\end{figure}"
        },
        "equations": {
            "eq:eq:fan": "\\begin{equation}\\label{eq:fan}\n    \\|X\\|_p^p = \\sum_{i=1} i^p f^{X}_i.\n\\end{equation}",
            "eq:eq:f1": "\\begin{equation}\\label{eq:f1}\nf_1(X+Y) = \\|X_{=1}+Y\\|_0-\\|Y\\|_0 + \\|X+Y_{=1}\\|_0 - \\|X\\|_0.\n\\end{equation}",
            "eq:eq:bridge": "\\begin{equation}\\label{eq:bridge}\n\\|X_{=1}+Y\\|_0-\\|Y\\|_0=\\#\\{ X_{=1} \\lor Y_{\\neq 0}\\} -\\#\\{Y_{\\neq 0}\\}. \n\\end{equation}",
            "eq:eq:set": "\\begin{equation}\\label{eq:set}\n%\\begin{aligned}\n\\#\\{ X_{=1} \\lor Y_{\\neq 0}\\} -\\#\\{Y_{\\neq 0}\\}=\\#\\{ (X_{=1} \\lor Y_{\\neq 0})\\land Y_{=0}\\}= \\#\\{ X_{=1} \\land Y_{=0}\\}.\n%\\end{aligned}\n\\end{equation}",
            "eq:eq:CL1ad": "\\begin{equation}\\label{eq:CL1ad}\n\\hat{D}_{CL1-Adjust}=\\frac{d+f_1\\cdot \\max\\left\\{ \\frac{d\\cdot(\\|f\\|_2^2-n) }{ (1-f_1/n)(n^2-n-1) } ,0 \\right\\} }{1-f_1/n}.\n\\end{equation}",
            "eq:eq:error": "\\begin{equation}\\label{eq:error}\n\\frac{\\sum iq\\cdot e^{-iq}F_i }{\\sum F_i (1-e^{-iq}) }\\geq c.\n\\end{equation}",
            "eq:1": "\\begin{equation}\n    Error(\\hat{D},D)=\\max\\{\\hat{D}/D,D/\\hat{D}\\}.\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\big| A\\cap B\\big|=\\big|A\\big| + \\big| B\\big| - \\big| A\\cup B \\big|.  \\label{eq-inter}\n\\end{equation}",
            "eq:3": "\\begin{align*}\nC=\\sum_{j:n_j>0} \\frac{N_j}{N}. \n\\end{align*}",
            "eq:eq:cl": "\\begin{equation}\\label{eq:cl}\n\\hat{D}_{CL}=\\frac{d}{\\hat{C}}+\\frac{n(1-\\hat{C})}{\\hat{C}}\\hat{\\gamma}^2,\n\\end{equation}",
            "eq:4": "\\begin{align*}\n\\gamma^2=\\frac{(1/D)\\sum_{j=1}^D (N_j-\\bar{N})^2 }{\\bar{N}^2}. \n\\end{align*}",
            "eq:5": "\\begin{align*}\n\\hat{\\gamma^2}=\\max\\left\\{  \\frac{\\hat{D}_1 \\sum i(i-1) f_i}{n^2-n-1},0 \\right\\},\n\\end{align*}",
            "eq:eq:cl11": "\\begin{equation}\\label{eq:cl11}\n\\hat{D}_{CL1}=\\frac{d}{\\hat{C}}+\\frac{n(1-\\hat{C})}{\\hat{C}}\\hat{\\gamma^2}.\n\\end{equation}",
            "eq:eq:JE": "\\begin{equation}\\label{eq:JE}\n\\hat{D}=d+K\\frac{f_1}{n}.\n\\end{equation}",
            "eq:eq:uj1": "\\begin{equation}\\label{eq:uj1}\n\\hat{D}_{uj1}=\\left( 1-\\frac{(1-q)f_1}{n}  \\right)^{-1} d. \n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\hat{\\gamma}^2_{Haas}(\\hat{D})=\\max\\left(0,\\frac{\\hat{D}}{n^2}\\sum_{i=1}^n i(i-1)f_i +\\frac{\\hat{D}}{N}-1 \\right). \\label{eq:hass}\n\\end{equation}",
            "eq:eq:uj2": "\\begin{equation}\\label{eq:uj2}\n\\hat{D}_{uj2}=\\left( 1-\\frac{(1-q)f_1}{n}  \\right)^{-1} \\left(d-\\frac{f_1(1-q)\\ln(1-q) \\hat{\\gamma}^2(\\hat{D}_{uj1}) }{q} \\right). \n\\end{equation}",
            "eq:eq:sj2": "\\begin{equation}\\label{eq:sj2}\n\\hat{D}_{sj2}=(1-(1-q)^{\\tilde{N}})^{-1}(d-(1-q)^{\\tilde{N}}\\ln(1-q)N\\hat{\\gamma^2}(\\hat{D}_{uj1}),\n\\end{equation}",
            "eq:7": "\\begin{align*}\nK=K_{Sh}= n \\frac{f_1\\sum_i (1-q)^i f_i }{ \\sum_i iq(1-q)^{i-1} f_i }. \n\\end{align*}",
            "eq:eq:sh2": "\\begin{equation}\\label{eq:sh2}\n\\hat{D}_{Sh2}=d+f_1\\left(\\frac{q(1+q)^{\\tilde{N}-1}}{(1+q)^{\\tilde{N}}-1}\\right)\\left( \\frac{ \\sum_{i=1}^n (1-q)^i f_i }{\\sum_{i=1}^n iq(1-q)^{i-1}f_i}\\right),\n\\end{equation}"
        },
        "git_link": "https://github.com/llijiajun/NDV_Estimation_in_distributed_environment.git"
    }
}