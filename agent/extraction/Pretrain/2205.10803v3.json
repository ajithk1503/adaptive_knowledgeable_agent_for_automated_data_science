{
    "meta_info": {
        "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
        "abstract": "Self-supervised learning (SSL) has been extensively explored in recent years.\nParticularly, generative SSL has seen emerging success in natural language\nprocessing and other AI fields, such as the wide adoption of BERT and GPT.\nDespite this, contrastive learning-which heavily relies on structural data\naugmentation and complicated training strategies-has been the dominant approach\nin graph SSL, while the progress of generative SSL on graphs, especially graph\nautoencoders (GAEs), has thus far not reached the potential as promised in\nother fields. In this paper, we identify and examine the issues that negatively\nimpact the development of GAEs, including their reconstruction objective,\ntraining robustness, and error metric. We present a masked graph autoencoder\nGraphMAE that mitigates these issues for generative self-supervised graph\npretraining. Instead of reconstructing graph structures, we propose to focus on\nfeature reconstruction with both a masking strategy and scaled cosine error\nthat benefit the robust training of GraphMAE. We conduct extensive experiments\non 21 public datasets for three different graph learning tasks. The results\nmanifest that GraphMAE-a simple graph autoencoder with careful designs-can\nconsistently generate outperformance over both contrastive and generative\nstate-of-the-art baselines. This study provides an understanding of graph\nautoencoders and demonstrates the potential of generative self-supervised\npre-training on graphs.",
        "author": "Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, Jie Tang",
        "link": "http://arxiv.org/abs/2205.10803v3",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "11 pages; Accepted to KDD'22"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "%\n\\label{sec:introduction}\n\n\n\n\nSelf-supervised learning (SSL), which can be generally categorized into generative and contrastive methods~\\cite{liu2021self}, has found widespread adoption in computer vision (CV) and natural language processing (NLP). \n%and graph machine learning. \nWhile contrastive SSL methods have %\\zfj{been blossoming, }\n experienced an emergence in the past two years, \nsuch as MoCo~\\cite{he2020momentum}, generative SSL has been gaining steadily increasing significance thanks to several groundbreaking practices, such as the well-established BERT~\\cite{devlin2019bert} and GPT~\\cite{Radford2019LanguageMA} in NLP as well as the very recent MAE~\\cite{he2021masked}  %iBOT~\\cite{zhou2021ibot} \nin CV. \n\n\n\nHowever, in the context of graph learning, % albeit prior efforts on GAEs, \ncontrastive SSL has been the dominant approach, especially for two important tasks---node and graph classifications~\\cite{velivckovic2018deep, qiu2020gcc,hassani2020contrastive}. \nIts success has been largely built upon relatively complicated training strategies. \nFor example, the bi-encoders with momentum updates and exponential moving average are usually indispensable to stabilize the training of GCC~\\cite{qiu2020gcc} and BGRL~\\cite{thakoor2021bootstrapped}. \nAdditionally, negative samples are  necessary for most contrastive objectives, often requiring arduous labor to sample or construct from graphs, e.g., GRACE~\\cite{zhu2020deep}, GCA~\\cite{zhu2021graph}, and DGI~\\cite{velivckovic2018deep}. \nFinally, its heavy reliance on high-quality %and label-invariant \ndata augmentation proves to be the \npain point  \n%Achilles heel \nof contrastive SSL, e.g., CCA-SSG~\\cite{zhang2021canonical}, as graph augmentation is mostly based on heuristics whose effectiveness varies drastically from graph to graph. \n%, and some of them can be unscalable at larger scales (e.g., diffusion in MVGRL~\\cite{hassani2020contrastive}).\n\n%data augmentation, negative sampling, bi-encoders and other techniques to stabilize training\n\n\nSelf-supervised graph autoencoders (GAEs) can naturally avoid the aforementioned issues in contrastive methods, as its learning objective is to directly reconstruct the input graph data~\\cite{kipf2016variational,garcia2017learning}.   \n%Generative SSL in graphs, including graph autoregressive model (GAR) and graph autoencoders (GAEs), has also been studied for long. \n%As a typical GAR model, \nTake VGAE for example, it~\\cite{kipf2016variational} targets at predicting missing edges. \nEP~\\cite{garcia2017learning} instead proposes to recover vertex features. \nGPT-GNN~\\cite{hu2020gpt} proposes an autoregressive framework to perform node and edge reconstruction iteratively. \nLater GAEs, \nincluding ARVGA~\\cite{pan2018adversarially}, MGAE~\\cite{wang2017mgae}, GALA~\\cite{park2019symmetric}, GATE~\\cite{amin2020gate}, and AGE~\\cite{cui2020adaptive},\nmajorly focus on the objectives of link prediction and graph clustering. \n%Compared to GAR's impractical assumption on graph generating orders, GAEs holds little assumption and thus find more widespread applications.\n\n\n\\vpara{Dilemmas.} \nDespite their simple forms and various recent attempts \n% on them ~\\cite{you2020does,manessi2021graph,zhu2020self,jin2020self,hu2020strategies}, the development of self-supervised GAEs has been \non them ~\\cite{you2020does,jin2020self,hu2020strategies,park2019symmetric,amin2020gate,cui2020adaptive}, the development of self-supervised GAEs has been \nthus far lagged behind contrastive learning. \n%Despite a number of recent attempts~\\cite{you2020does,manessi2021graph,zhu2020self,jin2020self,hu2020strategies} to handle such challenges with autoencoders, \nTo date, there have been no GAEs succeeding to achieve a comprehensive outperformance over contrastive SSL methods, especially on node and graph classifications, which have been significantly advanced by neural encoders, \n% e.g., graph neural networks (GNNs); \ne.g., graph neural networks. \n%the progress of GAEs in graph has lagged behind NLP and CV. \nTo bridge the gap, we  analyze %the deficiency of \nexisting GAEs and identify the  issues that may negatively affect the progress of GAEs. \nNote that though previous GAEs may have individually tackled one or two of these issues below, \n%defects, \nnone of them collectively deals with the four challenges. \n%We try to shed some lights on this gap by analyzing the following problems of existing GAEs:\n\n\n%\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.3em,partopsep=0.3em]\n%(i) \n%\\item \nFirst, the structure information may be over-emphasized. \nMost  GAEs leverage  link reconstruction as the objective to encourage the topological closeness between neighbors~\\cite{kipf2016variational,pan2018adversarially,wang2017mgae,amin2020gate,hu2020gpt,cui2020adaptive}. \n% \\dong{reference all methods in fig 1a that fits here, also do this for the next three} \nThus, prior GAEs are usually good at link prediction and node clustering, but unsatisfactory on node and graph classifications. \n\n%(ii) \n%\\item \nSecond, feature reconstruction without corruption may not be robust. \nFor GAEs~\\cite{kipf2016variational,pan2018adversarially,park2019symmetric,amin2020gate,cui2020adaptive} that leverage feature reconstruction, most of them still employ the vanilla architecture that risks learning trivial solutions. \nHowever, the denoising autoencoders~\\cite{vincent2008extracting} that corrupt input and then attempt to recover it have been widely adopted in NLP~\\cite{devlin2019bert}, which might be applicable to graphs as well.\n\n%(iii) \n%\\item \nThird, \nthe mean square error (MSE) can be sensitive and unstable. \nTo the best of our knowledge, \nall existing GAEs with feature reconstruction~\\cite{wang2017mgae,park2019symmetric,amin2020gate,hu2020gpt,jin2020self} have adopted MSE as the criterion without additional precautions. \nHowever, MSE is known to suffer from varied feature vector norms and the curse of dimensionality~\\cite{Friedman2004OnBV} \n% \\dong{xx}, \nand thus can cause the collapse of training for autoencoders. \n%\\todo{polish the reason}\n\n%(iv) \n%\\item \nFourth, \nthe decoder architectures are of little expressiveness. \nMost GAEs~\\cite{kipf2016variational,pan2018adversarially,wang2017mgae,hu2020strategies,hu2020gpt,cui2020adaptive,jin2020self} leverage  MLP as their decoders. % by following BERT. \nHowever, the targets in language are one-hot vectors containing rich semantics, while in graphs, most of our targets are less informative feature vectors (except for some discrete attributes such as those in chemical graphs). \nIn this case, this trivial decoder (MLP) may not be strong enough to bridge the gap between encoder representations and decoder targets for graph features.\n\n%\\end{itemize}\n\n% Though previous GAEs may have individually tackled one or two of the above \n% issues, \n%defects, \n% none of them jointly deals with the four challenges as a whole. \n%The lack of a unified GAE solution to supersede contrastive self-supervised graph methods across extensive benchmarks.\n\n\n\n\n\\vpara{Contributions.}\nIn light of the above observations, the goal of this work is to examine to what extent we can 1) mitigate the issues faced by existing GAEs and 2) further enable GAEs to match or outperform contrastive graph learning techniques. \nTo this end, we present a masked graph autoencoder \\model for self-supervised graph representation learning. \nBy identifying the critical components in GAEs, we add new designs and also improve existing strategies for \\model, unleashing the power of autoencoders for graph learning. \n%\\dong{to update this}\nFigure \\ref{fig:cmp_tab} summarizes the different design choices between GAEs and \\model. \nSpecifically, the performance of \\model largely benefits from the following critical designs  (See Figure \\ref{fig:design_eff} for their contributions to performance improvements):\n%In light of the above observations, we endeavor to revival the idea of generative GAEs by optimizing its design and assimilating the advantages individually appear in prior GAEs. \n%We propose \\model, a masked graph autoencoder with decoding enhancement, and find that the following crucial designs of \\model can lead to substantially boosted performance on node classification, graph classification, and transfer learning: \n\n\\textbf{Masked feature reconstruction.}\nDifferent from most GAEs' efforts in structure reconstruction, \\model only focuses on reconstructing features with masking, whose effectiveness has been extensively verified in CV~\\cite{he2021masked} and  NLP~\\cite{devlin2019bert,Radford2019LanguageMA}.\nOur empirical studies suggest that with a proper error design, masked feature reconstruction can  substantially benefit GAEs. \n\n\n\n\\textbf{Scaled cosine error.}\nInstead of using MSE as existing GAEs, \\model employs the cosine error,  which is beneficial when feature vectors vary in their magnitudes (as is often the case for node attributes in graphs). \nOn top of it, we further introduce a scaled cosine error to tackle the issue of imbalance between easy and hard samples during reconstruction.\n\n\\textbf{Re-mask decoding.}\n%the architecture of decoders is considered to matter in the success of graph autoencoders in this work. \n%\\model leverage a more expressive single-layer GNN instead of traditional MLP. \nWe leverage a re-mask decoding strategy that re-masks the encoder's output embeddings of masked nodes \n% in the encoding phase \nbefore they are fed into the decoder. \n%to further enhance the encoder's capacity. \n% to create challenging learning task.\nIn addition,  \\model proposes to leverage more expressive graph neural nets (GNNs) as its decoder in contrast to previous GAEs' common usage of MLP. \n\n\n%\\qinkai{The naming is not consistent. \"Re-mask decoding\" does not indicate the structure of decoder, and why it \"enhances the encoder's capacity\"?}\n\n\nOverall, \\model is a simple generative self-supervised  method for graphs without additional cost. \nWe conduct extensive experiments on 21 datasets for three different graph learning tasks, including node classification, graph classification, and transfer learning.\n% : node classification (6 datasets), graph classification (7), and transfer learning (8). \nThe results suggest that equipped with the simple designs above, \\model can generate performance advantages over state-of-the-art contrastive SSL approaches across three tasks. \nMoreover, in many cases, \\model can match or sometimes outperform supervised baselines, further demonstrating the potential of self-supervised graph learning, particularly generative methods. \n\n\n\n\n%Surprisingly, without using any complicated tricks including data augmentation, negative sampling, bi-encoders and other techniques to stabilize training, \\model achieves competitive and even much better performance on a wide range of node classification and graph classification benchmarks compared to state-of-the-art contrastive SSL approaches. \n%It also significantly pushes the boundary of graph autoencoders, showing that generative SSL can offer great potential to graph representation learning in the future.\n\n\n% -----------------\n\n% \\begin{table}[t]\n%     \\centering\n%     \\caption{Technical comparison between generative SSL methods. \\textmd{\\textit{AE}: autoencoder methods. \\textit{No Struct.}: no structure reconstruction objective. \\textit{Mask Feat.}: use masking to corrupt input features. \\textit{GNN Decoder}: use GNN as the decoder. \\textit{Space} refers to run-time memory consumption. \\textit{CE} denotes Cross-Entropy Error.}}\n%     \\vspace{-2mm}\n%     \\small\n%     \\input{tables/comparsion}\n%     \\label{tab:ae_compare}\n% \\end{table}\n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=0.48\\textwidth]{imgs/GraphMAEXT.pdf}\n%     \\caption{ the effect of \\model design. }\n%     \\label{fig:design_xt}\n% \\end{figure}\n\n\n% -----------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hide{ %%%%%==============================\n\n\\section{Introduction}%\n\\label{sec:introduction}\nSelf-supervised learning (SSL), which can be generally categorized into generative and contrastive methods~\\cite{liu2021self}, has found widespread adoption in computer vision (CV), natural language processing (NLP), and graph learning communities. \nWhile contrastive SSL methods have experienced a sudden emergence in the past two years, \ngenerative SSL is gaining steadily increasing significance thanks to several successful practices, including well-established BERT~\\cite{devlin2019bert}, GPT~\\cite{radford2019language} in NLP and very recent MAE~\\cite{he2021masked}, iBOT~\\cite{zhou2021ibot} in CV.\n\n\n\nGenerative SSL in graph, including graph autoregressive (GAR) and graph autoencoders (GAEs), has also been studied for long. \nAs a typical GAR model, GPT-GNN~\\cite{hu2020gpt} proposes an autoregressive framework to perform node and edge reconstruction iteratively. \nFor GAEs, VGAE~\\cite{kipf2016variational} is a pioneer which targets predicting missing edges.\nEP~\\cite{garcia2017learning} first propose to recover vertex features. \nLater GAEs, including ARVGA~\\cite{pan2018adversarially}, MGAE~\\cite{wang2017mgae}, GALA~\\cite{park2019symmetric}, GATE~\\cite{amin2020gate}, and AGE~\\cite{cui2020adaptive} majorly focus on challenges of link prediction and graph clustering. \nCompared to GAR's impractical assumption on graph generating orders, GAEs holds little assumption and thus find more widespread applications.\n\nHowever, in the context of graph learning, albeit prior efforts on GAEs, \ncontrastive SSL is still the dominant approach, especially for node and graph classification. \nIts success has been built upon complex designs and architectures. \nFor example, bi-encoders with momentum update, exponential moving average (EMA), and Stop-Gradient are usually indispensable to stabilize the training (e.g., GCC~\\cite{qiu2020gcc}, BGRL~\\cite{thakoor2021bootstrapped}). \nNegative samples are necessary for most contrastive training objectives, but often require arduous labour to sample or construct from graphs (e.g., GRACE~\\cite{zhu2020deep}, GCA~\\cite{zhu2021graph}). \nSometimes, further transformation are needed to create even harder negatives (e.g., DGI~\\cite{velivckovic2018deep}). \nFinally, heavy reliance on high-quality and label-invariant data augmentation proves to be the Achilles heel of contrastive SSL (e.g., CCA-SSG~\\cite{zhang2021canonical}). \nThese augmentation techniques are mostly based on heuristics whose effectiveness vary drastically from graph to graph, and some of them can be unscalable at larger scales (e.g., diffusion in MVGRL~\\cite{hassani2020contrastive}).\n\n\\vpara{Dilemma.} \nSelf-supervised GAEs can naturally avoid aforementioned conflicts of contrastive methods, but suffer from poor performance on node and graph classification. \nDespite a number of recent attempts~\\cite{you2020does,manessi2021graph,zhu2020self,jin2020self,hu2020strategies} to handle such challenges with autoencoders, none of them succeed in achieving a comprehensive outperform over state-of-the-art contrastive SSL methods; \nthe progress of GAEs in graph has lagged behind NLP and CV. To bridge the gap, we systematically analysis the deficiency of existing GAEs:\n%We try to shed some lights on this gap by analyzing the following problems of existing GAEs:\n\n(i) Proximity information is over-emphasized. \nMost existing GAEs leverage the structure reconstruction as the main task to encourage topological closeness between neighbour nodes.\nConsequently, prior GAEs are usually good at link prediction and clustering, but unsatisfactory on classification.\n\n(ii) Feature reconstruction without corruption is not robust. \nFor GAEs that introduce the task of feature reconstruction, \nmost of them still employ the vanilla architecture that risks to learn trivial solutions, rather than the more efficient denoising autoencoders~\\cite{vincent2008extracting} which corrupt input and then recover.\n\n(iii) Mean Square Error (MSE) criterion can be sensitive and unstable. \nTo the best of our knowledge, all the existing GAEs with feature reconstruction have adopted MSE as their criterion. \nHowever, MSE is known to suffer from varied feature vector norms and curse of dimensionality, and thus can cause the collapse of training. \n%\\todo{polish the reason}\n\n(iv) Decoder architectures are of little expressiveness. \nMost GAEs leverage simple MLP as their decoders following BERT. \nHowever, while in language the targets are one-hot vectors containing rich semantics, in graphs most of our targets are less informative feature vectors (except for some discrete attributes such as in chemical graphs). \nIn this case, trivial decoder design may not be strong enough to bridge the gap between encoder representations and decoder targets.\n\nNoted that while previous works on GAEs may have individually tackled one or two of the above defects, none of them jointly deals with the four challenges as a whole to consequently present a unified GAE solution to supersede contrastive ones over extensive benchmarks.\n\n\n\\vpara{Contributions.}\nIn light of the above observations, we endeavor to revival the idea of generative GAEs by optimizing its design and assimilating the advantages individually appear in prior GAEs. \nWe propose \\model, a masked graph autoencoder with decoding enhancement, and find that the following crucial designs of \\model can lead to substantially boosted performance on node classification, graph classification, and transfer learning: \n\n\\textbf{Masked Feature Reconstruction}: \n\\model focuses on feature reconstruction with the more challenging masked prediction objective, \nwhose effectiveness and superiority over vanilla architecture has been verified in CV and NLP for long. \n\n\\textbf{Cosine Error}: \n\\model employs Cosine Error, which is beneficial when feature vectors vary in their magnitudes (as is often the case for node attributes in graphs), rather than traditional MSE. \nOn top of it, we introduce a scaling factor $\\gamma\\ge 1$ to conquer the problem of imbalance between easy and hard to reconstruct samples.\n\n\\textbf{Re-mask Decoding}: \n\\model enhance the design of decoder using a more expressive single-layer GNN instead of traditional MLP. \nA novel Re-mask Decoding strategy which re-masks encoder's output embeddings of masked input nodes brings additional improvements.\n\nSurprisingly, without using any complicated tricks including data augmentation, negative sampling, bi-encoders and other techniques to stabilize training, \n\\model achieves competitive and even much better performance on a wide range of node classification and graph classification benchmarks compared to state-of-the-art contrastive SSL approaches. \nIt also significantly pushes the boundary of graph autoencoders, showing that generative SSL can offer great potential to graph representation learning in the future.\n\n\n\\begin{table}\n    \\centering\n    \\caption{Technical comparison between generative SSL methods. \\textmd{\\textit{AE}: autoencoder methods. \\textit{No Struct.}: no structure reconstruction objective. \\textit{Mask Feat.}: use masking to corrupt input features. \\textit{GNN Decoder}: use GNN as the decoder. \\textit{Space} refers to run-time memory consumption. \\textit{CE} denotes Cross-Entropy Error.}}\n    \\vspace{-2mm}\n    \\small\n    \\input{tables/comparsion}\n    \\label{tab:ae_compare}\n\\end{table}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{imgs/GraphMAEXT.pdf}\n    \\caption{ the effect of \\model design. }\n    \\label{fig:design_xt}\n\\end{figure}\n\n\n\n}%========================================\n\n\n\n\n\n\n\\hide{\n\\begin{table}[t]\n    \\centering\n    \\caption{Technical comparison between \\model and contrastive SSL methods. \\textmd{Contrastive methods' performance relies on complicated bi-encoder architecture, negative sampling and strong data augmentation; \\model do not need them at all.}}\n    \\vspace{-2mm}\n    \\small\n    \\renewcommand\\tabcolsep{3pt}\n    \\input{tables/contrastive_cmp}\n    \\label{tab:contrastive_compare}\n    \\vspace{-5mm}\n\\end{table}\n}\n\n\n\\hide{\n%\\clearpage\nThe idea of masked autoencoders, a form of more general denoising autoencoders, is natural and applicable in computer vision as well. Indeed, closely related research in vision preceded BERT. However, despite signif- icant interest in this idea following the success of BERT, progress of autoencoding methods in vision lags behind NLP. We ask: what makes masked autoencoding different between graph and vision? We attempt to answer this question.\n\n\nInspired by the success of contrastive learning in vision, similar methods have been adapted to learning graph neural networks. Although these models have achieved impressive performance, they require complex designs and architectures. For example, DGI and MVGRL rely on a parameterized mutual information estimator to discriminate positive node-graph pairs from negative ones; GRACE and GCA harness an additional MLP-projector to guarantee sufficient capacity. Moreover, negative pairs sampled or constructed from data often play an indispensable role in providing effective contrastive signals and have a large impact on performance. Selecting proper negative samples is often nontrivial for graph-structured data, not to mention the extra storage cost for prohibitively large graphs. BGRL is a recent endeavor on targeting a negative-sample-free approach for GNN learning through asymmetric architectures. However, it requires additional components, e.g., an exponential moving average (EMA) and Stop- Gradient, to empirically avoid degenerated solutions, leading to a more intricate architecture.\n}%\\clearpage\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "%\n\\label{sec:related}\nAccording to model architectures and objective designs, self-supervised methods on graphs can be naturally divided into contrastive and generative domains. \n% Traditionally, random-walk-based network embeddings~\\cite{perozzi2014deepwalk,grover2016node2vec, tang2015line} can be regarded as a simple form of generative methods with shallow neural networks to recover proximity; \n% graph kernels~\\cite{shervashidze_2009_ais,shervashidze2011weisfeiler,yanardag2015deep} with designed features andmeasures can be viewed as ancestors of recent contrastive methods.\n\n%In this section, we focus on more recent self-supervised graph learning methods based on graph neural networks (GNNs).\n\n",
                "subsection 2.1": {
                    "name": "Contrastive Self-Supervised Graph Learning",
                    "content": "\nContrastive self-supervised learning\n, which encourages alignment between label-invariant distributions and uniformity across other distributions, \nhas been the prevalent paradigm for graph representation learning in the last two years. \nIts success relies heavily \non the elaborate designs of the following components:\n\n\\vpara{Negative sampling.}\nIn pursuit of uniformity, negative sampling is a must for most contrastive methods. \nMutual information based DGI~\\cite{velivckovic2018deep} and InfoGraph~\\cite{sun2019infograph} leverage corruptions to construct negative pairs.\nGCC~\\cite{qiu2020gcc} follows the MoCo-style~\\cite{he2020momentum} negative queues. \nGRACE~\\cite{zhu2020deep}, GCA~\\cite{zhu2021graph} and GraphCL~\\cite{you2020graph} use in-batch negatives. \nDespite recent attempts for negative-sample-free contrastive learning, strong regularization from architecture designs (e.g., BGRL ~\\cite{thakoor2021bootstrapped}) or \nin-batch feature decorrelation\n(e.g. CCA-SSG~\\cite{zhang2021canonical}) is necessary in their practice.\n\n\\vpara{Architectures.}\nContrastive methods can be unstable in early-stage training, and thus architecture constraints are important to tackle the challenge. \nAsymmetric bi-encoder designs including momentum update~\\cite{qiu2020gcc}, EMA, and Stop-gradient~\\cite{thakoor2021bootstrapped} are widely adopted.\n\n\\vpara{Data augmentation.}\nHigh-quality and informative data augmentation plays a central role in the success of contrastive learning, \nincluding feature-oriented (partial masking~\\cite{jin2020self,hu2020strategies,you2020graph,zhu2020deep,thakoor2021bootstrapped}, shuffling~\\cite{velivckovic2018deep}), proximity-oriented (diffusion~\\cite{hassani2020contrastive,kefato2021self}, perturbation~\\cite{you2020graph,hu2020gpt,zeng2020contrastive}), and graph-sampling-based (random-walk~\\cite{qiu2020gcc,hassani2020contrastive,you2020graph}, uniform~\\cite{zeng2020contrastive}, ego-network~\\cite{sterling2015zinc}) augmentations. \n\nHowever, while augmentation in CV is usually human comprehensible, it is difficult to interpret in graphs. \nWithout a theoretical understanding of handcrafted graph augmentation strategies, it remains unverified whether they are label-invariant and optimal. \n\n"
                },
                "subsection 2.2": {
                    "name": "Generative Self-Supervised Graph Learning",
                    "content": "\nGenerative self-supervised learning aims to recover missing parts of the input data. \nIt can be further classified into autoregressive and autoencoding two families. \nIn previous literature, generative methods' performance on graph representation learning falls behind contrastive methods by a large margin.\n\n\\vpara{Graph autoregressive models.}\nAutoregressive models decompose joint probability distributions as a product of conditionals. \nIn supervised graph generation, previous researchers have proposed GraphRNN~\\cite{you2018graphrnn}, GCPN~\\cite{you2018graph}. \nFor graph representation learning, GPT-GNN~\\cite{hu2020gpt} is a recent attempt to leverage graph generation as the training objective. \nHowever, since most graphs do not present inherent orders, autoregressive methods make little sense on them.\n\n\\vpara{Graph autoencoders (GAEs).}\nAutoencoders~\\cite{hinton1994autoencoders} are designed to reconstruct certain inputs given the contexts and do not enforce any decoding orders as autoregressive methods do. \nThe earliest works trace back to GAE and VGAE~\\cite{kipf2016variational} which take 2-layer GCN as encoder and dot-product for link prediction decoding. EP~\\cite{garcia2017learning} proposes to recover vertex features using mean squared error without input corruption. \nLater GAEs mostly adopt the structural reconstruction (e.g., ARVGA~\\cite{pan2018adversarially}) following VGAE, or a combination of structural and feature reconstruction (e.g., MGAE~\\cite{wang2017mgae}, GALA~\\cite{park2019symmetric} and GATE~\\cite{amin2020gate}) as their objectives. And NWR-GAE~\\cite{tang2022graph} jointly predicts the node degree and neighbor feature distribution.\n\nRegardless of the successful applications in link prediction and graph clustering, \ndue to existing GAEs' reconstruction of structure or/and features without masking, \ntheir results on node/graph classification benchmarks are usually unsatisfactory.\nTherefore, \nour goal in this work is \n% this work is \nto identify the weaknesses of existing GAE designs and rejuvenate the idea of self-supervised GAEs on graph representation learning for classification.\n\n\\vpara{\\model v.s. Attribute-Masking.}\nRecently, some works~\\cite{you2020does,manessi2021graph,zhu2020self,jin2020self} have been dedicated to surveying a wide range of many self-supervision objectives' effectiveness on GNNs, including masked feature reconstruction (namely Attribute-Masking).\nHowever, their performance lags far behind state-of-the-art contrastive methods because other critical defects of existing GAEs are not handled. \nWe present a rough comparison of algorithms and results between \\model and them in Appendix~\\ref{app:appendix}.%\\clearpage\n"
                }
            },
            "section 3": {
                "name": "raph",
                "content": "\n\nIn this section, we present the self-supervised masked graph autoencoder framework---\\model---to learn graph representations without supervision based on graph neural networks (GNNs). \nWe introduce the critical components that differ \\model from previous attempts on designing graph autoencoders (GAEs). \n%We couple the introduction with \n\n\n\n\n\n",
                "subsection 3.1": {
                    "name": "The GAE Problem and \\model",
                    "content": "\n\nBriefly, an autoencoder usually comprises an encoder, code (hidden states), and a decoder. %, has been popular in representation learning for decades. \nThe encoder maps the input data to code, and the decoder maps the code to reconstruct the input under the supervision of a reconstruction criterion. \n%To acquire a compressed knowledge representation, autoencoders usually impose a bottleneck in their architecture by enforcing a smaller dimension in code than the input.\nFor graph autoencoders, they can be formalized as follows. \n%We formally introduce the concept of graph autoencoders (GAEs) in the context of graphs. \n\nLet $\\mathcal{G}=(\\mathcal{V}, \\mA, \\mX)$ denote a graph, where $\\mathcal{V}$ is the node set, $N=|\\mathcal{V}|$ is the number of nodes, $\\mA \\in \\{0,1\\}^{N\\times N}$ is the adjacency matrix, and $\\mX \\in \\mathbb{R}^{N\\times d}$ is the input node feature matrix. \nFurther, given $f_E$ as the graph encoder, \n$f_D$ as the graph decoder, \nand $\\mH \\in \\mathbb{R}^{N\\times d_h}$ denoting the code encoded by the encoder, \nthe goal of general GAEs is to reconstruct the input as \n\\begin{equation}\n    \\begin{split}\n        \\mH = f_E(\\mA, \\mX), \\ \\mathcal{G}^{\\prime} = f_D(\\mA, \\mH),\n    \\end{split}\n\\end{equation}\nwhere $\\mathcal{G}^{\\prime}$ denotes the reconstructed graph, which could be either reconstructed features or structures or both.  \n%The backbones for $f_E$ and $f_D$ can be any type of graph neural networks (GNNs), such as GCN~\\cite{thomas2017gcn}, GAT~\\cite{petar2018gat}, or GIN~\\cite{xu2019powerful}. \n\n\nDespite their versatile applications in NLP and CV, autoencoders' progress in graphs, especially for classification tasks, is relatively insignificant. \nTo bridge the gap, in this work, we aim to identify and rectify the  deficiencies of existing GAE approaches, and subsequently present the \\model---a masked graph autoencoder---to further the idea and design of GAEs and generative SSL in graphs. \n\n\n\\vpara{GraphMAE.}\nThe overall architecture of \\model is illustrated in Figure~\\ref{fig:overview}. \nIts core idea lies in the reconstruction of masked node features. %---defined as $\\widetilde{\\mX}$.  \\todo{}\n% To have a robust reconstruction, we propose to use a scaled cosine error as the criterion. \n% In addition, we introduce a re-mask decoding strategy with GNNs, rather than the widely-used MLP decoder in GAEs, to empower \\model.\nAnd we introduce a re-mask decoding strategy with GNNs, rather than the widely-used MLP in GAEs, as the decoder to empower \\model.\n% Furthermore, \nTo have a robust reconstruction, we also propose to use a scaled cosine error as the criterion. \n% to enable \\model with capacity. \nFigure \\ref{fig:cmp_tab} summarizes the technical differences between % the proposed \n\\model and existing GAEs. \n\n\nIn detail, the backbones for $f_E$ and $f_D$ can be any type of GNNs, such as GCN~\\cite{thomas2017gcn}, GAT~\\cite{petar2018gat}, or GIN~\\cite{xu2019powerful}. \nAs our encoder $f_E$ processes the whole graph $\\mA$ with partially observed node features $\\widetilde{\\mX}$, resonating to the backbones in other generative SSL methods (e.g., BERT and MAE), \n\\model prefers a \nmore expressive GNN encoder on features for different tasks. \n% more expressive GNN encoder based on different graph learning tasks\n% \\qinkai{Not clear. \"more expressive GNN encoder based on different graph learning tasks\"?}\nFor instance, GAT is more expressive in node classification, and GIN provides a better inductive bias for graph-level applications (See Tables~\\ref{tab:backbone} and ~\\ref{tab:ablation}). \n\n \n\n\n\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "The   Design of \\model",
                    "content": "\n\n%\\qinkai{It would be better if the entire process of graphMAE can be presented by pseudo codes.}\nIn this part, we explore how to design a generative self-supervised graph pre-training framework \n%such that it can leverage lessons learned from existing GAE efforts\nthat can match and outperform state-of-the-art contrastive models. \nSpecifically, we discuss different strategies via answering the following four questions: \n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.3em,partopsep=0.3em]\n    \\item \\textbf{Q1}: What to reconstruct in GAEs? %, particularly \n    %for node/graph classification?\n%    ?\n    \\item \\textbf{Q2}: How to train robust GAEs to avoid trivial solutions? \n    %\\qinkai{It's not clear how robustness is guaranteed from the writing and the results. }\n    \\item \\textbf{Q3}: How to arrange the decoder for GAEs?\n    \\item \\textbf{Q4}: What error function to use for reconstruction?\n\\end{itemize}\nThese questions concern the designs of the reconstruction objective, robust learning, loss function, and model architecture in GAEs, the answers to which enable us to develop \\model. \n\n\n\\hide{\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]\n    \\item \\textbf{Q1}: What should we reconstruct for node/graph classification?\n    \\item \\textbf{Q2}: How to avoid the notorious trivial solution in training GAEs?\n    \\item \\textbf{Q3}: Which criterion to use for feature reconstruction in graphs?\n    \\item \\textbf{Q4}: Why and how do we choose the decoder for GAEs?\n\\end{itemize}\n}\n\n\n\\vpara{Q1: Feature reconstruction as the objective.}\nGiven a graph $\\mathcal{G}=(\\mathcal{V}, \\mA, \\mX)$, a GAE could target reconstructing either  the structure $\\mA$ or the features $\\mX$, or both of them. \nMost classical GAEs~\\cite{kipf2016variational,pan2018adversarially} focus on the tasks of  link prediction and graph clustering, and thus usually choose to reconstruct $\\mA$---a target commonly used in network embeddings~\\cite{perozzi2014deepwalk,grover2016node2vec,tang2015line}. \nMore recent GAEs~\\cite{wang2017mgae,park2019symmetric,amin2020gate} tend to adopt a combined objective of reconstructing both features and structure, which unfortunately does not empower GAEs to produce as significant progress in node and graph classifications as autoencoders have done in NLP and CV. \n%but still fail to compete with modern contrastive learning approaches on tasks related to classification.\n\n%when research focus was on the topological closeness and attributes seldom appeared in acknowledged benchmarks; thus, no feature was needed to be reconstructed. Besides, designing proper criterion to predict binary linking is much easier than node features, which can be potentially multi-dimensional and continuous.\n\n%Fundamentally speaking, this is because the node feature along with its entanglement with topology may count more for classification. \nA very recent study shows that simple MLPs distilled from trained GNN teachers can work comparably to advanced GNNs on node classification~\\cite{zhang2022graphless}, indicating the vital role of features in such tasks. \nThus, to enable \\model to achieve a good performance on classification, we adopt feature reconstruction as the  training objective. \nOur empirical examination also shows that the explicit prediction of structural proximity has no contributions to the downstream classification tasks in \\model  \n%masked training \n(See Figure~\\ref{fig:design_eff}).\n\n\n\n%\\vpara{Q2: More challenging masked feature reconstruction.}~\\\\\n\\vpara{Q2: Masked feature reconstruction.}\nWhen the code's dimension size is larger than input's, the vanilla autoencoders have risks to learn the notorious ``identity function''---the trivial solution---that makes the learned code useless~\\cite{vincent2008extracting}. \nRelatively speaking, it is not a severe problem in CV since the image input is usually  high-dimensional. \nHowever, in graphs, the node feature dimension size is typically quite small, making it a real challenge to train powerful feature-oriented GAEs. \nUnfortunately, existing GAEs that incorporate the reconstruction of features as their objective commonly ignore the threat~\\cite{kipf2016variational, pan2018adversarially,park2019symmetric,cui2020adaptive,amin2020gate}. \n% \\dong{cite at least 2/3 such GAEs here}\n\nThe denoising autoencoder~\\cite{vincent2008extracting}, which corrupts the input data on purpose, is a natural option to eliminate the trivial solution. \nActually, the idea of employing masking as the corruption in masked autoencoders has found wide applications in CV~\\cite{he2021masked,Bao2021BEiTBP} and NLP~\\cite{devlin2019bert}.  \n% \\dong{add refs}\nInspired by their success, we propose to adopt masked autoencoders as the backbone of \\model.\n\nFormally, we sample a subset of nodes $\\widetilde{\\mathcal{V}}\\subset\\mathcal{V}$ and mask each of their features with a mask token [MASK], i.e., a learnable vector $\\vx_{[M]}\\in \\mathbb{R}^{d}$.\n%\\qinkai{It's not clear what the mask token refers to. Need more explanation like \"adopting the concept of mask token in NLP...\".}\nThus, the node feature $\\widetilde{\\vx}_{i}$ for $v_i\\in\\mathcal{V}$ in the masked feature matrix $\\widetilde{\\mX}$ can be defined as:\n% \\begin{equation}\n$$\n\\widetilde{\\vx}_{i}=\n\\begin{cases}\n\\vx_{[M]} & v_i\\in\\widetilde{\\mathcal{V}} \\\\\n\\vx_i & v_i\\notin\\widetilde{\\mathcal{V}} \n\\end{cases}\n% \\end{equation}\n$$\n% \\qinkai{Is $x_m$ different among masked nodes? Should be $x_{i, m}$?}\nThe objective of \\model is to reconstruct the masked features of nodes in $\\widetilde{\\mathcal{V}}$ given the partially observed node signals $\\widetilde{\\mX}$ and the input adjacency matrix $\\mA$.\n% \\dong{to zy: fix qk's 2 commets}\n%\\qinkai{Can this statement be formalized?}\n\nWe apply a uniform random sampling strategy without replacement to obtain masked nodes. \nIn GNNs, each node relies on its neighbor nodes to enhance/recover its features~\\cite{gilmer2017neural}. \n% \\dong{add message passing  references} \nRandom sampling with a uniform distribution helps prevent a potential bias center, i.e., one's neighbors are neither all masked nor all visible. \nAdditionally, \nsimilar to MAE~\\cite{he2021masked}, \na relatively large mask ratio (e.g., 50\\%) is necessary to reduce redundancy in the attributed graphs in most cases\n% ~\\cite{feng2020graph} \nand thus form a challenging self-supervision to learn meaningful node representations. \n\n% A downside of using \\small{[MASK]} are \nThe use of [MASK], on the other hand, potentially creates a mismatch between training and inference since the [MASK] token does not appear during inference~\\cite{yang2019xlnet}. \nTo mitigate the discrepancy, \n%and strength \\model's robustness, \nBERT proposes to not always replace ``masked'' words with the actual [MASK] token, but with a small probability (i.e., 15\\% or smaller) to leave it unchanged or to substitute it with another random token. \nOur experiments find that the ``leave-unchanged'' strategy actually harms \\model's learning, while the ``random-substitution'' method could \nhelp form more high-quality representations. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vpara{Q3: GNN decoder with re-mask decoding.}\nThe decoder $f_D$ maps the latent code $\\mH$ back to the input $\\mX$, \nand its design \n% can highly \nwould\ndepend on the semantic level~\\cite{he2021masked} of target $\\mX$. \n% \\dong{add refs?} \n% and the design plays a key role in determining the semantic level of the learned latent representations.\nFor example, in language, since targets are one-hot missing words with rich semantics, usually a trivial decoder such as MLP is sufficient~\\cite{devlin2019bert}. \nBut in vision,  previous studies~\\cite{he2021masked} discover that a more advanced decoder (e.g., the Transformer model~\\cite{vaswani2017attention}) is necessary to recover pixel patches with low-level semantics.\n\n\nIn graphs, the decoder reconstructs relatively less informative multi-dimensional node features. \nTraditional GAEs employ either no neural decoders or a simple MLP for decoding with less expressiveness, causing the latent code $\\mH$ to be nearly identical to input features. \nHowever, it has no merit to learn such trivial latent representations because the goal is to embed input features with  meaningful compressed knowledge. \nTherefore, \\model resorts to a more expressive single-layer GNN as its decoder. \nThe GNN decoder can recover the input features of one node based on a set of nodes instead of only the node itself, and it consequently helps the encoder learn high-level latent code. \n% This aims to force the decoder to reconstruct input features from unmasked latent representations.\n\n\nTo further encourage the encoder to learn compressed representations, we propose a \\textit{re-mask decoding} technique to process the latent code $\\mH$ for decoding. \nWe replace $\\mH$ on masked node indices again with another mask token [DMASK], i.e., the decoder mask, with $\\vh_{[M]} \\in \\mathbb{R}^{d_h}$. \nSpecifically, the re-masked code $\\widetilde{\\vh}_i$ in $\\widetilde{\\mH} = \\mathrm{REMASK}(\\mH)$ can be denoted as \n$$\n\\widetilde{\\vh}_i = \n    \\begin{cases}\n        \\vh_{[M]} &  v_i \\in \\widetilde{\\mathcal{V}} \\\\\n        \\vh_i  &   v_i \\notin \\widetilde{\\mathcal{V}} \n    \\end{cases}\n$$\n% The [DMASK] is shared over all nodes and is different from the mask token used in the encoding. \\zhenyu{remove this sentence?}\n%Then the input to the decoder consists of (i) encoded embeddings for unmasked nodes, and (ii) masked tokens. \nWith the GNN decoder, a masked node is forced to reconstruct its input feature from the neighboring unmasked latent representations. \nSimilar to encoders, our empirical examination suggests that the GAT and GIN encoders are good options for node classification and  graph classification, respectively. \n%\\todo{do we need more intuition here?}\nNote that the decoder is only used during the self-supervised training stage to perform the node feature reconstruction task. \nTherefore, the decoder architecture is independent of the encoder choice and can use any type of GNN.\n\n\n\n\n\n\n\n\n\n\\vpara{Q4: Scaled cosine error as the criterion.}\nThe feature reconstruction criterion varies % counts \nfor masked autoencoders~\\cite{devlin2019bert,he2021masked} in different domains. \n% \\dong{can we add several refs here? this is a strong statement} \nIn NLP and CV, the \\textit{de facto} criterion is to predict discrete token indices derived from tokenizers using cross entropy error. \nAn exception is the MAE work~\\cite{he2021masked} in CV, which directly predicts pixels in the masked patches using the mean square error (MSE); \nNevertheless in fact, pixels are naturally normalized to 0--255, functioning similarly to tokenizers. \nBut in graphs, it remains unexplored how to define a universal tokenizer. \n\nIn \\model, we propose to directly reconstruct the raw features for each masked node, which can be challenging due to the multi-dimensional and continuous nature of node features. \nExisting GAEs with feature reconstruction have adopted MSE as their criterion~\\cite{wang2017mgae,park2019symmetric,jin2020self}. \n% \\dong{to add >2 refs}\nBut in preliminary experiments, we discover that \n%without effort to lift a finger, \nthe MSE loss can be minimized to nearly zero\n% Therefore, the MSE loss \nand may not be enough for feature reconstruction, which may (partly) explain why few existing GAEs use feature reconstruction as their only training objective. \n%After analysis,\n% To this end, we identify two issues of MSE  for self-supervised graph learning:\n% \\dong{xxx}\n% \\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]\n%     \\item \\textbf{Sensitivity}: MSE, or Euclidean Distance, is sensitive to vector norms and dimensionality. Extreme values in certain feature dimensions can also lead to MSE's overfit on them.\n%     \\item \\textbf{Low selectivity}: we find majority of node features are easy to recover because of their high expressiveness, which results in an extreme imbalance between easy and hard samples. MSE is not selective enough to focus \\model on hard ones.\n% \\end{itemize}\nTo this end, we found that MSE could suffer from the issues of \\textit{sensitivity} and \\textit{low selectivity}. Sensitivity means that MSE is sensitive to vector norms and dimensionality~\\cite{Friedman2004OnBV}. Extreme values in certain feature dimensions can also lead to MSE\u2019s overfit on them. Low selectivity represents that MSE is not selective enough to focus on those harder ones among imbalanced easy-and-hard samples. \n\nTo handle its sensitivity, we leverage the cosine error as the criterion to reconstruct original node features, which gets rid of the impact of dimensionality and vector norms. \nThe $l_2$-normalization in the cosine error maps vectors to a unit hyper-sphere and substantially improves the training stability of representation learning. \nThis benefit is also observed by some contrastive learning methods like BYOL~\\cite{grill2020bootstrap}. \n\nTo improve its selectivity, we further the cosine error by introducing the scaled cosine error (SCE) for \\model. \nThe intuition is that we can down-weight easy samples' contribution in training by scaling the cosine error with a power of $\\gamma\\ge 1$. \nFor predictions with high confidence, their corresponding cosine errors are usually smaller than 1 and decay faster to zero when the scaling factor $\\gamma>1$. \nFormally speaking, given the original feature $\\mX$ and reconstructed output $\\mZ=f_D(\\mA, \\widetilde{\\mH})$, we define SCE for \\model as \n% \\dong{vector bold? or not, h/H is bold in next eq.}\n\\begin{equation}\n    \\mathcal{L}_{\\textrm{SCE}} = \\frac{1}{|\\widetilde{\\mathcal{V}}|} \\sum_{v_i \\in \\widetilde{\\mathcal{V}}} (1 - \\frac{\\vx^T_i \\vz_i}{\\| \\vx_i\\| \\cdot \\| \\vz_i\\|})^\\gamma,~ \\gamma \\ge 1,\n    \\label{eq:loss}\n\\end{equation}\nwhich is averaged over all masked nodes. \nThe scaling factor $\\gamma$ is a hyper-parameter adjustable over different datasets.\nThis scaling technique could also be viewed as an adaptive sample reweighing, and the weight of each sample is adjusted with the reconstruction error. \nThis error is also famous in the field of supervised object detection as the focal loss~\\cite{lin2017focal}.\n\n\n\n% \\hide{\n% The gradient of SCE is:\n% \\begin{equation}\n% \\nabla L^\\alpha_{\\theta, \\phi} = \\alpha L^{\\alpha-1} \\nabla L_{\\theta, \\phi}\n% \\label{eq:grad}\n% \\end{equation}\n% The above equation indicates that, on the one hand, $L^{\\alpha-1}$ scales the gradient based on the the reconstruction error of ($\\vx$, $\\vz$). A sample would be assigned higher weight relatively if its reconstruction error $L$ is high. On the other hand, if $L < 1 (\\vx^\\vz > 0)$, that is, the reconstructed $\\vz$ tends to be similar to node feature $\\vx$, SCE would lower it weight during training. This makes optimization focus more on hard samples and helps convergence. \n\n% Eq \\ref{eq:loss} looks like the criterion in BYOL~\\cite{grill2020bootstrap}. But there exists obvious differences. (i) Learning objective. BYOL contrasts representations of different augmented views, which are from two different networks (online network and target network). \\model aims to reconstruct the input from the output of the decoder. (ii)  The targte network and representation is constantly updating as the training proceeds, while the reconstructed target of \\model is fixed (the original node feature). \n% }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%\\dong{to add a summary para to summarize what decisions graphMAE makes, either here or after 3.3}\n\n\n\n\\hide{Intuitively, using an MLP decoder is more likely to reconstruct a node's input feature from its encoded embedding. As a method of self-supervised learning, feature reconstruction aims to help learn expressive node representations, instead of making the encoded representations similar to raw node features. To achieve this target, we found that the decoder design plays an important role in learning meaningful representations. }\n\n\nIn summary, \\model is a simple and scalable self-supervised graph learning framework. \nFigure \\ref{fig:cmp_deg} illustrates how each of its design choices directly impacts the performance of the self-supervised \\model framework. \nBy identifying the negative components and designing new strategies, \\model unleashes the power of autoencoders for self-supervised graph pre-training. \n\n\n%It addresses the issues that limit the development of existing GAEs.\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Training and Inference",
                    "content": "\nThe overall training flow of \\model  is summarized by Figure \\ref{fig:overview}.\nFirst, given an input graph, we randomly select a certain proportion of nodes and replace their node features with the mask-token [MASK]. \nWe feed the graph with partially observed features into the encoder to generate the encoded node representations. \nIn decoding, we re-mask the selected nodes and replace their features with another token [DMASK]. Then the decoder is applied to the re-masked graph to reconstruct the original node features with the proposed scaled cosine error. \n\n\nFor downstream applications, the encoder is applied to the input graph without any masking in the inference stage. \nThe generated node embeddings can be used for various graph learning tasks, such as node classification and graph classification. \nFor graph-level tasks, we use a non-parameterized graph pooling (readout) function, e.g., MaxPooling and  MeanPooling, to obtain the graph-level representation \n$ \\vh^g = \\mathrm{READOUT}(\\{\\vh_i, v_i \\in \\mathcal{G}_g \\}) $.\n% \\dong{transfer}\nIn addition, similar to ~\\cite{hu2020strategies}, \\model also enables robust transfer of pre-trained GNN models to various downstream tasks.\nIn the experiments, we show that \\model achieves competitive performance in both node-level and graph-level applications. \n\n \n\n%\\todo{add a algorithmic pipeline}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hide{%===========================================================================================\n\n\n\\section{\\model: Self-supervised Masked Graph Autoencoder}\n\\begin{figure*}[htbp]\n    \\centering\n    \\includegraphics[width=\\textwidth]{imgs/overview_small.pdf}\n    \\caption{Illustration of \\model and comparison with GAE. \\model corrupts input graph by masking node features to create a challenging objective. The graph is encoded into latent representation by GNN encoder.  \\model would re-mask the encoding of nodes before fed into the decoder, and employ a GNN, e.g. GAT, GIN, as the decoder. \n    The output of decoding is used to reconstruct input node features of masked nodes, with \\textit{scaled cosine error} as the criterion.\n    Previous GAEs usually use a linear transformation or MLP in the decoding, and more focus on restoring graph structure.}\n    \\label{fig:overview}\n\\end{figure*}\n\nThe idea of autoencoders, which comprise the encoder, code, and the decoder, has been popular in representation learning for decades. The encoder maps input data to code, and the decoder maps the code back for input reconstruction under the supervison of a reconstruction creterion. To acquire a compressed knowledge representation, autoencoders usually impose a bottleneck in their architecture by enforcing a smaller dimension in code than the input.\n\nDespite versatile applications in NLP and CV, autoencoders' progress in graphs (especially for classification) is slow in recent years. To bridge the gap, in this work we want to identify and rectify crucial deficiencies of existing approaches, and consequently present \\model, a masked graph autoencoder framework, to renew the idea and design of GAEs and generative SSL in graphs.\n\nConsider a graph $\\mathcal{G}=(\\mathcal{V}, \\mA, \\mX)$, $\\mathcal{V}$ is node set, $\\mA \\in \\{0,1\\}^{N\\times N}$ is the adjacency matrix, and $\\mX \\in \\mathbb{R}^{N\\times d}$ is the original node feature matrix. Let $f_E$ refers to the graph encoder, $f_D$ denotes the graph decoder, $\\mH \\in \\mathbb{R}^{N\\times d_h}$ denotes to the code (or hidden states) encoded by the encoder, and $\\mZ \\in \\mathbb{R}^{N\\times d}$ denotes reconstructed features. Given masked feature matrix $\\widetilde{\\mX}$, \\model reconstructs inputs as\n\\begin{equation}\n    \\begin{split}\n        \\mH = f_E(\\mA, \\widetilde{\\mX}), \\ \\mZ = f_D(\\mA, \\mH)\n    \\end{split}\n\\end{equation}\n\n\\noindent where the backbone for $f_E,f_D$ can be any type of GNN, such as GCN~\\cite{thomas2017gcn}, GAT~\\cite{petar2018gat}, or GIN~\\cite{xu2019powerful}. As our encoder $f_E$ processes the whole graph $\\mA$ with partially observed node features $\\widetilde{\\mX}$, resonating to the backbones in other generative SSL methods (e.g., BERT and MAE), \\model may prefer a more expressive encoder on features. For instance, GAT is more expressive in node classification, and GIN provides a better inductive bias for graph level applications. We present extensive studies on backbones for encoder in Table~\\ref{tab:backbone} and decoder in Table~\\ref{tab:ablation}. \n\n\n\\subsection{Critical Designs of \\model}\nIn this part, we delve into the why and wherefores of \\model being more efficient and effective compared to previous GAEs. Particularly, we will introduce our ideas via answering the following four questions\n\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]\n    \\item \\textbf{Q1}: What should we reconstruct for node/graph classification?\n    \\item \\textbf{Q2}: How to avoid the notorious trivial solution in training GAEs?\n    \\item \\textbf{Q3}: Which criterion to use for feature reconstruction in graphs?\n    \\item \\textbf{Q4}: Why and how do we choose the decoder for \\model?\n\\end{itemize}\n\nwhich concerns the designs of objective, learning, loss function, and model architecture.\n\n\\vpara{Q1: Feature reconstruction as the objective.}~\\\\\n\nFor a graph $\\mathcal{G}=(\\mathcal{V}, \\mA, \\mX)$, $\\mathcal{V}$, a GAE could target reconstructing either the $\\mA$, $\\mX$, or both of them. Classical GAEs usually choose to reconstruct $\\mA$ following the fashion in early era of network embedding~\\cite{perozzi2014deepwalk,grover2016node2vec,tang2015line}, when research focus was on the topological closeness and attributes seldom appeared in acknowledged benchmarks; thus, no feature was needed to be reconstructed. Besides, designing proper criterion to predict binary linking is much easier than node features, which can be potentially multi-dimensional and continuous.\n\nHowever, in the era of GNNs, features and their entanglement with structures may count more, especially for node and graph classification challenges. Considering this paradigm shift, we argue that for a better performance on classification, feature reconstruction should be alternatively chosen as the GAE training objectives. Based on this intuition, next we introduce how we prevent trivial solution in the training of feature-oriented GAEs.\n\n\\vpara{Q2: More challenging masked feature reconstruction.}~\\\\\nWhen code's dimension is larger than input's, vanilla autoencoders are risking to learn the notorious ``identity function'' that makes the learned code useless. It is not an extremely severe problem in CV since the image input is high-dimensional; but in graphs, feature dimension is quite low and thus making it a real challenge to feature-oriented GAEs. Unfortunately, existing GAEs that incorporates feature reconstruction as objectives usually ignore the threat.\n\nDenoising autoencoder~\\cite{vincent2008extracting}, which corrupts the input data on purpose, is a natural solution to the challenge of trivial solution. Typically, the idea of masked autoencoders to employ masking as the corruption has found wide applications in CV and NLP. Inspired by its success, we propose to adopt masked autoencoders as the backbone for self-supervised representation learning on graphs, namely the \\model.\n\n Formally, we sample a subset of nodes $\\widetilde{\\mathcal{V}}\\subset\\mathcal{V}$ and mask the features of nodes in $\\widetilde{\\mathcal{V}}$ with a learnable mask token [MASK] (i.e., $\\vx_m\\in \\mathbb{R}^{d}$). Thus, we define node feature $\\widetilde{\\vx}_{i}$ for $v_i\\in\\mathcal{V}$ in masked feature matrix $\\widetilde{\\mX}$ as:\n\\begin{equation}\n\\widetilde{\\vx}_{i}=\n\\begin{cases}\nx_m & v_i\\in\\widetilde{\\mathcal{V}} \\\\\nx_i & v_i\\notin\\widetilde{\\mathcal{V}} \n\\end{cases}\n\\end{equation}\n \n\\noindent Our objective is to reconstruct the masked node features given the partial observed node signals and adjacency matrix.  \n\nWe apply a uniform random sampling without replacement strategy to obtain masked nodes. In GNNs, each node relies on neighboring nodes to enhance/recover its feature. Random sampling follows a uniform distribution and helps prevents a potential bias center (i.e. one's neighbors are neither all masked nor all visible). Additionally, unlike BERT~\\cite{devlin2019bert} in NLP but resonating to MAE~\\cite{he2021masked} in CV, in most cases a relatively large mask ratio (i.e., 50\\%) is necessary to reduce redundancy in attributed graph and create a challenging self-supervision to learn meaningful node representations. \n\n% A downside of using \\small{[MASK]} are \nThe use of [MASK], on the other hand, potentially creates a mismatch between training and\ninference, since the [MASK] token does not appear during inference~\\cite{yang2019xlnet}. To mitigate the discrepancy and strength \\model's robustness, BERT propose to not always replace ``masked'' nodes with the actual [MASK] token, but with a small probability (i.e., 15\\% or smaller) to leave it unchanged, or substitute it with another random token. In our experiments, we find that ``leaving-unchanged'' strategy harms \\model's learning, while the ``random-substitution'' can sometimes help to form a more robust and better-performed representation. \n\n\\vpara{Q3: Scaled Cosine Error as the criterion.}~\\\\\nReconstruction criterion counts for masked autoencoders. In NLP and CV, the de facto criterion is to predict discrete token indices derived from tokenizers using CrossEntropy Error. An exception would be MAE~\\cite{he2021masked} in CV, which directly predict pixels in the masked patches using Mean Square Error (MSE); but it highly depends on the fact that pixels are naturally normalized to 0-255, functioning similarly to tokenizers. But in graph, it remains unexplored how to define a universal tokenizer. \n\nAs a result, we propose to directly reconstruct the raw features for each masked node, which can be challenging owing to the multi-dimensional and continuous nature of node features. \nExisting GAEs with feature reconstruction have all adopted MSE as their criterion. But in preliminary experiments, we discover that without effort to lift a finger, MSE loss can be minimized to nearly zero. Therefore, the MSE is not enough for feature reconstruction, which explains why none of existing GAEs use feature reconstruction as their only training objective. After analysis, we believe that MSE has two severe shortcomings for self-supervised graph learning:\n%Nevertheless, it is known to suffer from varied vector norms and curse of dimensionality. \n\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]\n    \\item \\textbf{Sensitivity}: MSE, or Euclidean Distance, is sensitive to vector norms and dimensionality. Extreme values in certain feature dimensions can also lead to MSE's overfit on them.\n    \\item \\textbf{Low selectivity}: we find majority of node features are easy to recover because of their high expressiveness, which results in an extreme imbalance between easy and hard samples. MSE is not selective enough to focus \\model on hard ones.\n\\end{itemize}\n\nTo handle the sensitivity, we leverage Cosine Error as the criterion to reconstruct original node features, which gets rid of impact from dimensionality and vector norms. \n$l_2$-normalization in cosine error maps vectors to a unit hyper-sphere and substantially improves training stability of representation learning. This benefit is also observed by some contrastive learning methods like BYOL~\\cite{grill2020bootstrap}. \n\nOn top of Cosine Error, we propose Scaled Cosine Error (SCE) to improve the selectivity. The intuition is that we can down-weight easy samples' contribution in the training by scaling Cosine Error with a power of $\\gamma\\ge 1$. For those prediction with high confidence, their corresponding cosine error is usually smaller than 1 and decays faster to zero when scaling factor $\\gamma>1$. Formally speaking, given original input feature $\\vx$ and reconstructed output $\\vz$, we define SCE for \\model as\n\\begin{equation}\n    \\mathcal{L} = -\\frac{1}{|\\widetilde{\\mathcal{V}}|} \\sum_{v_i \\in \\widetilde{\\mathcal{V}}} (1 - \\frac{\\vx^T_i \\vz_i}{\\| \\vx_i\\| \\cdot \\| \\vz_i\\|})^\\gamma,~ \\gamma \\ge 1\n    \\label{eq:loss}\n\\end{equation}\n\\noindent which is averaged over all masked nodes. Scaling factor $\\gamma$ is a hyper-parameter adjustable over different datasets.\nThis scaling technique could be viewed as adaptive sample reweighing, and the weight of each sample is adjusted with the reconstruction error. It is also famous in the field of supervised object detection as the focal loss~\\cite{lin2017focal}.\n\n\\hide{\nThe gradient of SCE is:\n\\begin{equation}\n\\nabla L^\\alpha_{\\theta, \\phi} = \\alpha L^{\\alpha-1} \\nabla L_{\\theta, \\phi}\n\\label{eq:grad}\n\\end{equation}\nThe above equation indicates that, on the one hand, $L^{\\alpha-1}$ scales the gradient based on the the reconstruction error of ($\\vx$, $\\vz$). A sample would be assigned higher weight relatively if its reconstruction error $L$ is high. On the other hand, if $L < 1 (\\vx^\\vz > 0)$, that is, the reconstructed $\\vz$ tends to be similar to node feature $\\vx$, SCE would lower it weight during training. This makes optimization focus more on hard samples and helps convergence. (??)\n\nEq \\ref{eq:loss} looks like the criterion in BYOL~\\cite{grill2020bootstrap}. But there exists obvious differences. (i) Learning objective. BYOL contrasts representations of different augmented views, which are from two different networks (online network and target network). \\model aims to reconstruct the input from the output of the decoder. (ii)  The targte network and representation is constantly updating as the training proceeds, while the reconstructed target of \\model is fixed (the original node feature). }\n\n\n\n\n\n\n\\vpara{Q4: GNN Decoder with Re-mask Decoding.}~\\\\\nThe decoder $f_D$ maps the latent code $\\mH$ back to the input $\\mX$, and its complexity can highly depends on targets' semantic level. In language, since targets are one-hot missing words with rich semantics, usually a trivial decoder such as MLP is sufficient~\\cite{devlin2019bert}. But in vision, when previous works~\\cite{he2021masked} discover that a more complicated decoder based on transformers is necessary to recover pixel patches with low-level semantics.\n\nIn graph, the decoder reconstructs less informative multi-dimensional node features. \nTraditional GAEs employ no decoder or a simple MLP for decoding with little expressiveness, causing the latent code $\\mH$ to be nearly identical to input features. However, it has no merit to learn such trivial representations because what we want are embeddings with meaningful compressed knowledge. Therefore, \\model resorts to a more expressive single-layer GNN as its decoder. The GNN decoder recovers input features based on a set of nodes instead a node itself, and consequently helps the encoder learn high-level latent code.\n% This aims to force the decoder to reconstruct input features from unmasked latent representations.\n\nTo further encourage encoder learning compressed representation, we propose a novel technique \\textit{Re-mask Decoding} to process latent code $\\mH$ for decoding. We replace the $\\mH$ again on masked node indices with another mask token [DMASK] (i.e., decoder mask) with $\\vh_m \\in \\mathbb{R}^{d_h}$. Denote the re-masked results as $\\widetilde{\\mH} = \\mathrm{REMASK}(\\mH)$\n$$\n\\widetilde{\\vh}_i = \n    \\begin{cases}\n        \\vh_m, &  v_i \\in \\widetilde{\\mathcal{V}} \\\\\n        \\vh_i &   v_i \\notin \\widetilde{\\mathcal{V}} \n    \\end{cases}\n$$\nThe [DMASK] is shared over all nodes and different from the mask token used in encoding. \n%Then the input to the decoder consists of (i) encoded embeddings for unmasked nodes, and (ii) masked tokens. \nWith the GNN decoder, a masked node is forced to reconstruct its input feature from the neighboring unmasked latent representations. In our experiments, we find that GAT decoder performs better for node classification and GIN decoder is a good choice for graph classification. \n%\\todo{do we need more intuition here?}\n\n\\hide{Intuitively, using an MLP decoder is more likely to reconstruct a node's input feature from its encoded embedding. As a method of self-supervised learning, feature reconstruction aims to help learn expressive node representations, instead of making the encoded representations similar to raw node features. To achieve this target, we found that the decoder design plays an important role in learning meaningful representations. }\n\n\n\n\\subsection{Training and Inference}\n\\model is a simple and scalable self-supervised graph learning framework without complex designs and operations.\nFirst, we randomly select a certain proportion of nodes, and replace their node features with the mask-token [MASK]. We feed the partial observed graph into the encoder to generate encoding node representations. Before the decoding, we re-mask the selected nodes and replace them with another token [DMASK]. The decoder is applied to the re-masked graph to reconstruct original node features.\n\nFor downstream applications, the encoder is applied to input graph without any masking in the inference stage. The generated node embeddings could be used for downstream tasks, such as node classification. For graph-level tasks, we use a non-parameterized graph pooling (readout) function, i.e., MaxPooling, MeanPooling, to obtain the graph representation\n$ \\vh^g = \\mathrm{READOUT}(\\{\\vh_i, v_i \\in \\mathcal{G}_g \\}) $.\nIn the experiments, we show that \\model achieve competitive performance in both node-level and graph-level applications. \n\nThe decoder is only used during self-supervised learning to perform node feature reconstruction task. There, the decoder architecture is independent of the encoder design and can use any GNN layer. \n\n\\todo{add a algorithmic pipeline}\n\n\n\n}%end of hide%\\clearpage\n%\\input{3.method}\n%\\clearpage\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "%\n\\label{sec:experiments}\n\nIn this section, we demonstrate that \\model  is a general self-supervised framework for various graph learning tasks, including:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]\n    \\item Unsupervised representation learning for \\textit{node classification};\n    \\item Unsupervised representation learning for \\textit{graph classification};\n    \\item \\textit{Transfer learning} on molecular property prediction.\n\\end{itemize}\n% \\noindent We compare \\model with state-of-the-art methods on these three tasks. \nExtensive experiments on various datasets are conducted to evaluate \n%\\noindent We comprehensively evaluate \nthe performance of \\model against state-of-the-art (SOTA) contrastive and generative methods on these three tasks. \nIn each task, we follow exactly the same experimental procedure, e.g., data splits, evaluation protocol, as the  standard settings~\\cite{velivckovic2018deep,zhang2021canonical,sun2019infograph,hu2020strategies}. \n% \\dong{to ZY: add at least 1 for each task}\n\n%For reproducibility,  our code is available in \\url{https://anonymous.4open.science/r/GraphMAE}.\n\n",
                "subsection 4.1": {
                    "name": "Node Classification",
                    "content": "\n\n%\\subsubsection{Node classification}\\hfill\n\n\n\\vpara{Setup.}\nThe node classification task is to predict the unknown node labels in networks. \nWe test the performance of \\model on 6 standard benchmarks: Cora, Citeseer, PubMed~\\cite{yang2016revisiting}, Ogbn-arxiv~\\cite{hu2020open}, PPI, and Reddit. \n% are citation networks where nodes correspond to documents and edges represent citations. \n% PPI is a protein-protein interaction dataset and  Reddit contains posts belonging to different communities with user comments. \nFollowing the inductive setup in GraphSage~\\cite{hamilton2017inductive}, the testing for Reddit and PPI is carried out on unseen nodes and graphs, while the other networks are used for transductive learning.\n\nFor the evaluation protocol, we follow the experimental setting in ~\\cite{velivckovic2018deep,hassani2020contrastive,thakoor2021bootstrapped,zhang2021canonical}. \nFirst, we train a GNN encoder by the proposed \\model without supervision. \nThen we freeze the parameters of the encoder and generate all the nodes' embeddings. \nFor evaluation, we train a linear classifier and report the mean accuracy on the test nodes through 20 random initializations. \nWe follow the public data splits ~\\cite{velivckovic2018deep,hassani2020contrastive,zhang2021canonical}  of Cora, Citeseer, and PubMed. \nThe graph encoder $f_E$ and decoder $f_D$ are both specified as standard GAT. \nDetailed hyper-parameters can be found in  Appendix \\ref{app:appendix}.\n\n\n\\vpara{Results.} \nWe compare  \\model with SOTA  contrastive self-supervised models, DGI~\\cite{velivckovic2018deep}, MVGRL~\\cite{hassani2020contrastive}, GRACE~\\cite{zhu2020deep}, BGRL~\\cite{thakoor2021bootstrapped}, InfoGCL~\\cite{Xu2021InfoGCLIG}, and CCA-SSG~\\cite{zhang2021canonical}, as well as supervised baselines GCN and GAT. \nWe also report the results of previous generative self-supervised models, GAE~\\cite{kipf2016variational}, GPT-GNN~\\cite{hu2020gpt}, and GATE~\\cite{amin2020gate}.\nWe report results from previous works with the same experimental setup if available.\nIf results are not previously reported and codes are provided, we implement them based on the official codes and conduct a hyper-parameter search. \nTable~\\ref{tab:node_clf} lists the results. \n\\model achieves the best or competitive results compared to the SOTA self-supervised approaches in all benchmarks. \nNotably, \\model outperforms existing generative methods by a large margin. \nThe results in the inductive setting of PPI and Reddit suggest the self-supervised \\model technique provides strong generalization to unseen nodes. \n% Previous studies often report a large gap between supervised and self-supervised results on PPI. \n% We find that increasing the model size could significantly boost the performance in the self-supervised setting and help fill the gap, as shown in Figure ~\\ref{fig:ppi_hidden} of Appendix \\ref{app:ppi}. \n\n% \\dong{zy to fix, something missing}\n% As shown in Figure~\\ref{fig:ppi_hidden}, \\model even outperforms supervised counter-part, although the model would be too much larger. This might indicate the potential of large-scale GNNs in pre-training.\n\n\n% Generally, the performance in PPI gets better with higher model capacity. But even with a similar or much larger hidden dimension, previous SOTA methods couldn't achieve competitive results as \\model. \n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Graph Classification",
                    "content": "\n\n\n\\vpara{Setup.}\nFor graph classification,  we conduct experiments on 7 benchmarks: MUTAG, IMDB-B, IMDB-M, PROTEINS, COLLAB, REDDIT-B, and NCI1~\\cite{yanardag2015deep}.\n% which are widely used in recent graph classification models. \nEach dataset is a collection of graphs where each graph is associated with a label.\n% MUTAG, PROTEINS, and NCI1 have node labels as input features, while IMDB-B, IMDB-M, REDDIT-B, and COLLAB use node degrees as input features.\nNode labels are used as input features in MUTAG, PROTEINS, and NCI1, whereas node degrees are used in IMDB-B, IMDB-M, REDDIT-B, and COLLAB.\n\nFor the evaluation protocol, after generating graph embeddings with \\model's encoder and readout function, we feed encoded graph-level representations into a downstream LIBSVM~\\cite{chang2011libsvm} classifier to predict the label, and report the mean 10-fold cross-validation accuracy with standard deviation after 5 runs.  We adopt GIN~\\cite{xu2019powerful}, which is commonly used in previous graph classification works, as the backbone of encoder and decoder.\n% Detailed hyper-parameters can be found in Table 6 in the Appendix.\n\n\\vpara{Results.}\nIn addition to classical graph kernel methods---Weisfeiler-Lehman sub-tree kernel (WL)~\\cite{shervashidze2011weisfeiler} and deep graph kernel (DGK)~\\cite{yanardag2015deep}, we also compare \\model with SOTA unsupervised and contrastive methods, \nGCC~\\cite{qiu2020gcc}, graph2vec~\\cite{narayanan2017graph2vec}, Infograph~\\cite{sun2019infograph}, GraphCL~\\cite{you2020graph}, JOAO~\\cite{you2021graph},  MVGRL~\\cite{hassani2020contrastive}, and InfoGCL~\\cite{Xu2021InfoGCLIG}. \nThe supervised baselines, GIN~\\cite{xu2019powerful} and DiffPool~\\cite{ying2018hierarchical}, are also included. \nPer graph classification research tradition, we report results from previous papers if available.\nThe results are shown in Table~\\ref{tab:graph_clf}.\n% We find that \\model outperforms all unsupervised baselines on 5 out of 7 of the datasets and has competitive results in two of the others.\nWe find that \\model outperforms all self-supervised baselines on five out of seven datasets and has competitive results on the other two.\nThe node features of these seven datasets are all one-hot vectors representing node-labels or degrees, which are considered to be less informative than node features in node classification. The results manifest that generative auto-encoders could learn meaningful information and offer potentials in graph-level tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Transfer Learning",
                    "content": "\n\n\n\n\\vpara{Setup.} To evaluate the transferability of the proposed method, we test the performance on transfer learning on molecular property prediction, following the setting of ~\\cite{hu2020strategies,you2020graph,you2021graph}. The model is first pre-trained in  2 million unlabeled molecules sampled from the ZINC15~\\cite{sterling2015zinc}, and then finetuned in 8 classification benchmark datasets contained in MoleculeNet~\\cite{wu2018moleculenet}. The downstream datasets are split by scaffold-split to mimic real-world use cases. Input node features are the atom number and chirality tag, and edge features are the bond type and direction. \n% In our experiments, we only consider reconstructing node features.\n\nFor the evaluation protocol,  we run experiments for 10 times and report the mean and standard deviation of ROC-AUC scores (\\%). Following the default setting in ~\\cite{hu2020strategies},  we adopt a 5-layer GIN as the encoder and a single-layer GIN as the decoder.\n\n\n\\vpara{Results.}\nWe evaluate \\model against methods including Infomax, AttrMasking and ContextPred~\\cite{hu2020strategies} , and SOTA contrastive learning methods, GraphCL~\\cite{you2020graph}, JOAO~\\cite{you2021graph}, and GraphLoG~\\cite{xu2021self}. Table \\ref{tab:graph_clf} shows that the performance on downstream tasks is comparable to SOTA methods, in which \\model achieves the best average scores and has a small edge over previous best results in two tasks. \n% This shows the robust transfer of \\model. \nThis demonstrates the robust transferability of \\model.\n\n\n% it is noteworthy \nTo summarize,  the self-supervised \\model  method achieves  competitive performance on node classification, graph classification, and transfer learning across 21 benchmarks. % using a unified learning approach. \nNote that we do not customize a dedicated \\model for each task. \nThe consistent results on the three tasks demonstrate that \\model is an effective and universal self-supervised graph pre-training framework for various applications. \n\n\n\n\n\n\n\n\n\n\n% \\begin{table}\n%     \\centering\n%     \\caption{Performance on PPI using GAT with 4 attention heads, compared to other unsupervised and supervised baselines. \\model could ourperform supervised }\n%     \\begin{tabular}{c|c|c|c|c}\n%     \\toprule\n%                 &  hd=256*4    &   hd=512*4    &   hd=1024*4   &   hd=2048*4   \\\\\n%     \\midrule\n%         BGRL    & 73.63$\\pm$0.16 & 81.94$\\pm$0.24 & 89.75$\\pm$0.69 & 93.51$\\pm$1.05 \\\\\n%         CCA-SSG & 73.34$\\pm$0.17 & 83.75$\\pm$0.64 & 92.16$\\pm$0.63 & 96.99$\\pm$0.16 \\\\\n%         \\model  &  74.50\u00b10.24  & 85.81\u00b10.26    & 94.14\u00b10.30    &   97.79\u00b10.06 \\\\\n%     \\bottomrule\n%     \\end{tabular}\n%     \\label{tab:ppi_hd}\n% \\end{table}\n\n\n\\hide{\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.43\\textwidth]{imgs/ppi_hidden.pdf}\n    \\vspace{-4mm}\n    \\caption{Performance on PPI using GAT with 4 attention heads, compared to other baselines. Self-supervised methods benefit much from larger model size, and \\model could outperform supervised model. }\n    \\label{fig:ppi_hidden}\n\\end{figure}\n}\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Studies",
                    "content": "\nTo verify the effects of the main components in \\model, we further conduct several ablation studies. Without loss of generalization, we choose three datasets from node classification and two datasets from graph classification for experiments.\n\n\n\\vpara{Effect of reconstruction criterion.} \nWe study the influence of reconstruction criterion, and Table \\ref{tab:ablation} shows the results of MSE and the SCE loss function.\n% The observation from Table \\ref{tab:ablation} varies between node classification and graph classification.\nGenerally, the input features in node classification lie in continuous or high-dimensional discrete space, containing more discriminative information.\n% The performance using MSE degrades after training, which indicates that using MSE as criterion may mislead learning and harm performance. \nThe results manifest that SCE has a significant advantage over MSE,\n% , as it brings 3.1\\% $\\sim$ 6.0\\% absolute gain.\nwith an absolute gain of 1.5\\% $\\sim$ 8.0\\%.\n%SCE forces the reconstructed embedding and node feature to be aligned in the unit sphere, while MSE measures the reconstruction quality in Euclidean space. \nIn graph classification, pre-training with either MSE or SCE improves accuracy. \nThe input features % lie in discrete value space, and \nare discrete one-hot encoding in these benchmarks, \nrepresenting either node degrees or node labels. \nReconstructing one-hot encoding with MSE is\nsimilar to classification tasks, thus MSE also works. Nevertheless, SCE offers a performance edge (though limited) over MSE. %, as shown in the Table.\n% This suggests that the a proper criterion is critical to the success of generative SSL.\n% These experiments manifest that SCE could be a better choice for both both discrete and continous vector input features. \n\n\nFigure \\ref{fig:ablation} shows the influence of scaling factor $\\gamma$.  We observe that $\\gamma > 1$ offers benefits in most cases, especially in node classification. However, in MUTAG, a larger $\\gamma$ value harms the performance. In our experiments, we notice that the training loss in node classification is much higher than that in graph classification. \nThis further demonstrates that aligning continuous vectors in a unit sphere is more challenging. Therefore, scaling $\\gamma$ brings improvements. \n\n\n\n\n\n\n\n\n\n\n\n\\vpara{Effect of mask and re-mask.} Masking plays an important role in the proposed \\model method. \\model employs two masking strategies --- masking input feature before encoder, and re-masking encoded code before decoder. Table \\ref{tab:ablation} studies the designs. We observe a significant drop in performance if not masking input features, indicating that masking inputs is vital to avoid  the trivial solution. For the re-mask strategy, the accuracy drops by 0.1\\%$\\sim$1.9\\% without it. \n% In such case, it behaves like training a $K+1$ layer GNN and adopting the output of the $K$-th layer as the encoding representations. \nRe-mask is designed for the GNN decoder and can be regarded as regularization, which makes the self-supervised task more challenging.\n\n\n\n\n\n\n\\hide{\n\\begin{table}\n\\centering\n\\caption{Experiment results using different encoder backbones in node classification.}\n\\renewcommand\\tabcolsep{3pt}\n\\begin{tabular}{c|cccc}\n\\toprule[1.2pt]\n          & Cora & Citeseer & Pubmed & Ogbn-arxiv \\\\\n\\midrule\nBGRL (GCN)    &   82.7$\\pm$0.6    &  71.1$\\pm$0.7        & 79.4$\\pm$0.6       &  71.64$\\pm$0.12      \\\\\nBGRL (GAT)    &   82.8$\\pm$0.5    &   71.1$\\pm$0.8       &  79.6$\\pm$0.5      &   70.07$\\pm$0.02 \\\\\nCCA-SSG (GCN) &   84.0$\\pm$0.4      &  73.1$\\pm$0.3         &  81.0$\\pm$0.4        &  70.81$\\pm$0.13  \\\\\nCCA-SSG (GAT) &   83.8$\\pm$0.5    &  72.6$\\pm$0.7        &   79.9$\\pm$1.1     &  71.24$\\pm$0.20  \\\\\n\\model (GCN)  &   82.9$\\pm$0.6             &  72.5$\\pm$0.5                 &  81.0$\\pm$0.5               &  \\bf 71.87$\\pm$0.21\\\\   \n\\model (GAT)  &   \\bf 84.2$\\pm$0.4        &  \\bf 73.4$\\pm$0.4       & \\bf 81.1$\\pm$0.4      &  71.75$\\pm$0.17 \\\\\n\\bottomrule[1.2pt]\n\\end{tabular}\n\\vspace{-2mm}\n\\label{tab:backbone}\n\\end{table}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\\vpara{Effect of mask ratio.} \nFigure \\ref{fig:ablation} shows the influence of mask ratio. \n% Similar to the observation in ~\\cite{he2021masked}, \n% In most cases, the reconstruction task with a low mask ratio (0.1) is not challenging enough to learn useful features.\nIn most cases, the reconstruction task with a low mask ratio (0.1) is not challenging enough to learn useful features.\n% while it would be too difficult if the ratio is too high (0.9). \n% Different from the behavior in BERT and MAE,\nThe optimal ratio varies across graphs. \nResults on Ogbn-arxiv and IMDB-B can be found in Appendix \\ref{app:appendix}. \nIn Cora, increasing the mask ratio by more than 0.5 degrades the performance, while \\model still works with a surprisingly high ratio (0.7$\\sim$0.9) in PubMed and MUTAG. \nThis can be connected with the information redundancy in graphs. \nLarge node degrees or high homogeneity may lead to heavy information redundancy, in which missing node features may be recovered from very few neighboring nodes with little high-level understanding of\nfeatures and local context. \n% Masking a very high portion of node features is necessary to learn useful representations. \nIn contrast, lower redundancy means that an excessively high mask ratio would make it impossible to recover features, thus degrading the performance. \n% As for how to measure information redundancy, we leave it for future work. \n\n\n\n\n\n\n\n\n\\vpara{Effect of decoder type.} \nIn Table \\ref{tab:ablation}, we compare different decoder types, including MLP, GCN, GIN, and GAT. The re-mask strategy is only used for GNN decoders. As the results show, using GNN decoder typically boosts the performance. Compared to MLP, which reconstructs original features from latent representations, GNN enforces masked nodes to extract information relevant to their original features from the neighborhood. One reasonable assumption is that GNN avoids the representation tending to be the same as original features.\n% as the learned representations are expected to be discriminative in downstream tasks. \nMLP also works in \\model, which might be partly attribute to the usage of the SCE metric.\n% The type of decoder matters in training, and \n\nAmong different GNNs, GIN performs better in graph-level tasks and GAT is a more reasonable option for node classification. It is observed that replacing GAT with GCN causes a significant drop, especially in Cora ($\\sim$2.9\\%) and PubMed ($\\sim$2.0\\%). \nWe speculate that the attention mechanism matters in reconstructing continuous features with the re-mask strategy.\n% when the latent representations vary in neighbors with re-masked ones.\n\n\n\n\n\n\n\n\n\n\n\n\\hide{\n\\vpara{Effect of encoder architecture.}  In node classification, \\model uses GAT as the encoder. To have a fair comparison and investigate the influence of different GNN backbones, we compare the best baselines, BGRL and CCA-SSG, in node classification datasets using GCN and GAT as the encoder. The results are shown in Table \\ref{tab:backbone}. We observe that \\model still outperforms the baselines with the same GAT backbone. \nIn addition, the results manifest that attention mechanism would not always benefit graph self-supervised learning, as GCN is inferior to GAT, for instance, in (Cora, CCA-SSG) and (Ogbn-arxiv, \\model). Under the training setting of \\model, GAT could be a better option in most cases.\n}\n\n% To we also implement previous SOTA contrastive methods based on GAT and , as shown in Table \\ref{tab:node_clf}. GAT doesn't always outperform GCN, indicating that GAT is not a absolute winner in self-supervised learning. \n\n% \\vpara{Effect of scaling factor $\\gamma$.} \n\n\n\n\n% \\begin{table}\n% \\centering\n% \\small\n% \\caption{Experiment results using different encoder backbones in node classification.}\n% \\renewcommand\\tabcolsep{3pt}\n% \\begin{tabular}{c|cccc}\n% \\toprule\n%           & Cora & Citeseer & Pubmed & Ogbn-arxiv \\\\\n% \\midrule\n% BGRL (GCN)    &   82.66$\\pm$0.60    &  71.06$\\pm$0.66        & 79.44$\\pm$0.58       &  71.64$\\pm$0.12      \\\\\n% BGRL (GAT)    &   82.75$\\pm$0.52    &   71.08$\\pm$0.79       &  79.55$\\pm$0.50      &   70.07$\\pm$0.02 \\\\\n% CCA-SSG (GCN) &   84.0$\\pm$0.4      &  73.1$\\pm$0.3          &  80.9$\\pm$0.4        &  70.81$\\pm$0.13  \\\\\n% CCA-SSG (GAT) &   83.78$\\pm$0.48    &  72.59$\\pm$0.66        &   79.91$\\pm$1.09     &  71.24$\\pm$0.20  \\\\\n% \\model (GCN)  &   82.91$\\pm$0.56             &  72.50$\\pm$0.45                 &  81.05$\\pm$0.54               &  \\bf 71.87$\\pm$0.21\\\\   \n% \\model (GAT)  &   \\bf 84.16\u00b10.44        &  \\bf 73.35$\\pm$0.42       & \\bf 81.10$\\pm$0.41      &  71.75$\\pm$0.17 \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\label{tab:backbone}\n% \\end{table}\n\n\n\n\n\n% \\begin{table}\n%     \\centering\n%     \\caption{Re-mask ablation.}\n%     \\label{tab:abl_remask}\n%     \\begin{tabular}{c|c|c|c|c|c}\n%     \\toprule\n%                     & Cora       & PubMed     & \\textcolor{gray}{Ogbn-arxiv} & MUTAG & IMDB-B \\\\\n%     \\midrule\n%         w/ re-mask  & 84.16\u00b10.44 & 81.10\u00b10.41 & 71.75\u00b10.17 & 88.19$\\pm$1.26 & 75.52$\\pm$ 0.35 \\\\\n%         w/o re-mask & 81.84\u00b10.58 & 80.02\u00b10.40 & 71.61\u00b10.13 & 86.29$\\pm$2.32 & 74.42$\\pm$0.32 \\\\\n%     \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}\n%     \\centering\n%     \\caption{Decoder ablation.}\n%     \\begin{tabular}{c|c|c|c|c|c}\n%         \\toprule\n%         Decoder     & Cora       & PubMed     & \\textcolor{gray}{Ogbn-arxiv} & MUTAG   & IMDB-B \\\\\n%         \\midrule\n%         MLP         & 81.47\u00b10.65 & 80.39\u00b10.68 & 71.54\u00b10.17 & 87.01\u00b11.28 & 73.94\u00b10.45 \\\\\n%         GCN(GIN)    & 81.25\u00b10.93 & 79.06\u00b10.77 & 71.59\u00b10.27 & 88.19\u00b11.26 & 75.52\u00b10.35 \\\\\n%         GAT         & 84.16\u00b10.44 & 81.10\u00b10.41 & 71.73\u00b10.24 & 85.75\u00b11.74 & 74.04\u00b10.38 \\\\\n%         \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}[]\n%     \\centering\n%     \\caption{Reconstruction ablation. }\n%     \\label{tab:loss}\n%     \\begin{tabular}{c|c|c|c|c|c}\n%         \\toprule\n%                   & Cora       & PubMed     & \\textcolor{gray}{Ogbn-arxiv} & MUTAG      & IMDB-B     \\\\\n%         \\midrule\n%         SCE(ours) & 84.16\u00b10.44 & 81.10\u00b10.41 & 71.75\u00b10.17 & 88.19\u00b11.26 & 75.52\u00b10.35 \\\\\n%         MSE & 68.49\u00b13.65 & 73.07\u00b11.41 & 67.44\u00b10.38 & 86.30\u00b11.54 & 74.04\u00b10.32 \\\\\n%         \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}[htbp]\n% \\begin{tabular}{cccc|cc}\n% \\toprule\n%                       & \\multicolumn{3}{c}{Node-classification} & \\multicolumn{2}{l}{Graph-classification} \\\\\n% \\cmidrule{2-6}\n%                       & Cora        & PubMed      & Ogbn-arxiv  & MUTAG              & IMDB-B              \\\\\n% \\cmidrule{2-6}\n% \\model & 84.16\u00b10.44  & 81.10\u00b10.41  & 71.75\u00b10.17  & 88.19\u00b11.26         & 75.52\u00b1 0.35         \\\\\n% w/o re-mask           & 81.84\u00b10.58  & 80.02\u00b10.40  & 71.61\u00b10.13  & 86.29\u00b12.32         & 74.42\u00b10.32          \\\\\n% w/ MSE                & 68.49\u00b13.65  & 73.07\u00b11.41  & 67.44\u00b10.38  & 86.30\u00b11.54         & 74.04\u00b10.32          \\\\\n% \\midrule\n% MLP                 & 81.47\u00b10.65  & 80.39\u00b10.68  & 71.54\u00b10.17  & 87.01\u00b11.28         & 73.94\u00b10.45          \\\\\n% GCN                 & 81.25\u00b10.93  & 79.06\u00b10.77  & 71.59\u00b10.27  &                    &                     \\\\\n% GIN                 &             &             &             & 88.19\u00b11.26         & 75.52\u00b10.35          \\\\\n% GAT                 & 84.16\u00b10.44  & 81.10\u00b10.41  & 71.73\u00b10.24  & 85.75\u00b11.74         & 74.04\u00b10.38           \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}[htbp]\n% \\small\n% \\begin{tabular}{cccc|cc}\n% \\toprule\n%                       & \\multicolumn{3}{c}{Node-classification} & \\multicolumn{2}{l}{Graph-classification} \\\\\n% \\cmidrule{2-6}\n%                       & Cora        & PubMed      & Ogbn-arxiv  & MUTAG              & IMDB-B              \\\\\n% \\cmidrule{2-6}\n% \\model                & 84.2\u00b10.4  & 81.1\u00b10.4  & 71.8\u00b10.2    & 88.2\u00b11.3         & 75.5\u00b1 0.4         \\\\\n% w/o re-mask           & 81.8\u00b10.6  & 80.0\u00b10.4  & 71.6\u00b10.1    & 86.3\u00b12.3         & 74.4\u00b10.3          \\\\\n% w/ MSE                & 68.5\u00b13.6  & 73.1\u00b11.4  & 67.4\u00b10.2    & 86.3\u00b11.5         & 74.0\u00b10.3          \\\\\n% \\midrule\n% MLP                   & 81.5\u00b10.7  & 80.4\u00b10.7  & 71.5\u00b10.2    & 87.0\u00b11.3         & 73.9\u00b10.5          \\\\\n% GCN                   & 81.2\u00b10.9  & 79.1\u00b10.8  & 71.6\u00b10.27   &                  &                     \\\\\n% GIN                   &           &           &             & 88.2\u00b11.3         & 75.5\u00b10.4          \\\\\n% GAT                   & 84.2\u00b10.4  & 81.1\u00b10.4  & 71.75\u00b10.2   & 85.8\u00b11.7         & 74.0\u00b10.4           \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\end{table}\n\n\n\n% \\begin{figure}\n%     \\subfigure[name of the subfigure]{   %\u7b2c\u4e00\u5f20\u5b50\u56fe\n%     % \\begin{minipage}[t]\n%     % \\centering    %\u5b50\u56fe\u5c45\u4e2d\n%     \\includegraphics[width=0.22\\textwidth]{imgs/gmae_mask_ratio.pdf}  \n%     % \\end{minipage}\n%     }\n%     \\subfigure[name of the subfigure]{   %\u7b2c\u4e00\u5f20\u5b50\u56fe\n%     % \\begin{minipage}[t]\n%     % \\centering    %\u5b50\u56fe\u5c45\u4e2d\n%     \\includegraphics[width=0.22\\textwidth]{imgs/gmae_mask_ratio.pdf}  \n%     % \\end{minipage}\n%     }\n%     \\caption{Ablation study of mask ratio and scale coefficient $\\gamma$.}\n%     \\label{fig:my_label}\n% \\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hide{%start of hide================================\n\n\\section{Experiments}%\n\\label{sec:experiments}\n\nIn this section, we show our proposed \\model is a general self-supervised framework for a wide range of graph tasks, including:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]\n    \\item Unsupervised representation learning for \\textit{node classification}\n    \\item Unsupervised representation learning for \\textit{graph classification}\n    \\item \\textit{Transfer learning} on molecular property prediction.\n\\end{itemize}\n% \\noindent We compare \\model with state-of-the-art methods on these three tasks. \n\\noindent We comprehensively evaluate the performance of \\model against state-of-the-art methods on these three tasks. \n\n\\subsection{Task Evalution}\n\n\\subsubsection{Node classification}\\hfill\n\n\n\\vpara{Setup.}\nThe node classification task is to predict unknown node labels in  network. We analyze the performance of \\model on a set of 6 standard transductive and inductive benchmarks. Cora, Citeseer, PubMed, and Ogbn-arxiv are citation networks where nodes correspond to documents and edges\nrepresent citations. \nPPI is a protein-protein interaction dataset and  Reddit contains posts belonging to different communities with user comments. \nFollowing the inductive setup in ~\\cite{hamilton2017inductive}, the testing for Reddit and PPI is carried out on unseen (untrained) nodes and graphs, while the citation networks are used for transductive learning.\n\nFor evaluation protocol, we follow experiment settings in ~\\cite{velivckovic2018deep,hassani2020contrastive,thakoor2021bootstrapped,zhang2021canonical}. We first train the model using the method proposed in this paper, without supervision. Then we freeze the parameters of the encoder and obtain all the nodes' embeddings. For the evaluation, we train a linear classifier and report the mean classification accuracy with standard deviation on the test nodes through 20 random initialization. We follow the public data split ~\\cite{velivckovic2018deep,hassani2020contrastive,zhang2021canonical}  of Cora/Citeseer/PubMed. The graph encoder $f_E(\u00b7|\\theta)$ is specified as a standard GAT model and the decoder $f_D(\u00b7|\\phi)$ is a single-layer GAT. To have a fair comparison, we also implement previous SOTA contrastive methods based on GAT and conduct hyper-parameter search, as shown in Table \\ref{tab:node_clf}. GAT doesn't always outperform GCN, indicating that GAT is not a absolute winner in self-supervised learning. Under the training setting of \\model, GAT could be a better option.\nDetailed hyper-parameters can be found in the Appendix.\n\n\n\\vpara{Experiment results.} \nWe compare our \\model with state-of-the-art (SOTA) contrastive self-supervised models, DGI~\\cite{velivckovic2018deep}, MVGRL~\\cite{hassani2020contrastive}, GRACE~\\cite{zhu2020deep}, BGRL~\\cite{thakoor2021bootstrapped}, and CCA-SSG~\\cite{zhang2021canonical}, and supervised baselines GCN and GAT. We also report the results of previous generative self-supervised models, GAE~\\cite{kipf2016variational}, GPT-GNN~\\cite{hu2020gpt}, and GATE~\\cite{amin2020gate}.\nWe report results from previous papers with the same experimental setup if available. If results are not previously reported and codes are provided, we implement them based on the official codes and conduct a hyper-parameter search. Otherwise we only report the results on Cora/Citeseer/PubMed using our re-implemented codes according to the paper. Table~\\ref{tab:node_clf} represents the results, and it shows that our approach achieves the best or competitive results compared to the state-of-the-art self-supervised approaches. To test the influence of different GNN backbones, we also compare the best baselines, BGRL and CCA-SSG, in citation networks using GCN and GAT as the backbone. The results is shown in Table \\ref{tab:backbone}. On the one hand, we observe that \\model still outperforms the baselines with the same GAT backbone. \nOn the other hand, the results manifest that, attention mechanism would not always bring benefits in graph self-supervised learning, as GCN is inferior to GAT, for instance, in (Cora, CCA-SSG) and (Ogbn-arxiv, \\model).\n% as indicated in NLP and CV, graph neural networks with attention mechanism would gain excellent performance in generative self-supervised learning. \nBesides, the results in the inductive setting of PPI and Reddit substantiates the ability of generalization to unseen nodes. \\model successfully outperformed all the competing self-supervised methods.  \n% It is observed that \\model achieves a significant improvement ($\\sim$10\\%) over previous unsupervised  methods in PPI dataset ($\\sim$4\\% absolute gain). \nPrevious studies often reports a large gap between supervised and self-supervised results on PPI. We found that increasing the number of model parameters helps little in supervised setting, but could highly boost the performance in self-supervised setting. As show in Table~\\ref{tab:ppi_hd}, \\model even outperforms supervised counter-part, although the model would be too much larger. This might indicate the potential of large-scale GNNs in pre-training.\n\n% Generally, the performance in PPI gets better with higher model capacity. But even with a similar or much larger hidden dimension, previous SOTA methods couldn't achieve competitive results as \\model. \n\n\n\n\\begin{table}\n\\centering\n\\small\n\\caption{Experiment results using different encoder backbones in node classification.}\n\\renewcommand\\tabcolsep{3pt}\n\\begin{tabular}{c|cccc}\n\\toprule\n          & Cora & Citeseer & Pubmed & Ogbn-arxiv \\\\\n\\midrule\nBGRL (GCN)    &   82.66$\\pm$0.60    &  71.06$\\pm$0.66        & 79.44$\\pm$0.58       &  71.64$\\pm$0.12      \\\\\nBGRL (GAT)    &   82.75$\\pm$0.52    &   71.08$\\pm$0.79       &  79.55$\\pm$0.50      &   70.07$\\pm$0.02 \\\\\nCCA-SSG (GCN) &   84.0$\\pm$0.4      &  73.1$\\pm$0.3          &  80.9$\\pm$0.4        &  70.81$\\pm$0.13  \\\\\nCCA-SSG (GAT) &   83.78$\\pm$0.48    &  72.59$\\pm$0.66        &   79.91$\\pm$1.09     &  71.24$\\pm$0.20  \\\\\n\\model (GCN)  &   82.91$\\pm$0.56             &  72.50$\\pm$0.45                 &  81.05$\\pm$0.54               &  \\bf 71.87$\\pm$0.21\\\\   \n\\model (GAT)  &   \\bf 84.16\u00b10.44        &  \\bf 73.35$\\pm$0.42       & \\bf 81.10$\\pm$0.41      &  71.75$\\pm$0.17 \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:backbone}\n\\end{table}\n\n\n\n\n\\subsubsection{Graph classification}\\hfill\n\n\n\\vpara{Setup.}\nFor unsupervised learning for graph classification,  we conduct experiments on 8 well-known benchmarks: MUTAG, IMDB-B, IMDB-M, PROTEINS, COLLAB, REDDIT-B, DD, and NCI1~\\cite{yanardag2015deep}, which are widely used in recent graph classification models. Each dataset is a set of graphs where each graph is associated with a label. MUTAG, PROTEINS, DD, and NCI1 have node labels as input features, while IMDB-B, IMDB-M, REDDIT-B, and COLLAB use node degrees as input features. \n\nFor evaluation protocol, after generating graph embeddings with \\model's encoder, we feed encoded graph-level representation into a down-stream LIBSVM~\\cite{chang2011libsvm} classifier to predict the label, and report the mean 10-fold cross validation accuracy with standard deviation after 5 runs.  We adopt GIN~\\cite{xu2019powerful}, which is commonly used in previous graph classification works, as the backbone of encoder and decoder.\n% Detailed hyper-parameters can be found in Table 6 in the Appendix.\n\n\\vpara{Experiment results.}\nAside from classical graph kernel methods, Weisfeiler-Lehman sub-tree kernel (WL)~\\cite{shervashidze2011weisfeiler} and deep graph kernel (DGK)~\\cite{yanardag2015deep}, we also compare \\model with SOTA unsupervised and contrastive learning methods, graph2vec~\\cite{narayanan2017graph2vec}, Infograph~\\cite{sun2019infograph}, GraphCL~\\cite{you2020graph}, JOAO~\\cite{you2021graph}, and InfoGCL~\\cite{Xu2021InfoGCLIG}. The performance of supervised baselines, GIN~\\cite{xu2019powerful} and DGCNN~\\cite{Zhang2018AnED} is also included. We report results from previous papers if available.\nThe results are shown in Table~\\ref{tab:graph_clf}.\nWe find that \\model outperforms all unsupervised baselines on 4 out of 7 of the datasets, and has competitive results in two of the other. In these benchmarks, node features are all one-hot vectors, representing node-labels or degrees, which are often considered to be less informative compared with node features in node classification.  The results manifest that generative auto-encoding could also learn meaningful information and be potentional in graph-level tasks.\n\n\n\n\n\n\n\n\n\n\\begin{table*}[htbp]\n    \\centering\n    \\caption{\\textmd{Experiment results in unsupervised representation learning for \\textbf{\\textit{node classification}}. We report Micro-F1(\\%) score for PPI and accuracy(\\%) for the other datasets. }}\n    \\begin{threeparttable}\n    \\renewcommand\\tabcolsep{8pt}\n    \\begin{tabular}{c|c|cccccc}\n        \\toprule[1.2pt]\n            & Dataset &   Cora      & CiteSeer      & PubMed                & Ogbn-arxiv        & PPI               & Reddit        \\\\\n        % \\midrule\n        % \\multirow{3}{*}{Statistics}\n\n         \\midrule\n        \\multirow{2}{*}{Supervised} \n        & GCN     &  81.5          & 70.3          & 79.0                   & 71.74$\\pm$0.29    & 75.7$\\pm$0.1    & 95.4           \\\\\n        & GAT     &  83.0$\\pm$0.7  & 72.5$\\pm$0.7  & 79.0$\\pm$0.3           & 72.1$\\pm$0.13     & 97.30$\\pm$0.20    & 96.5           \\\\\n        \\midrule\n        \\multirow{10}{*}{Self-supervised} \n        & GAE     &  71.5$\\pm$0.4  & 65.8$\\pm$0.4  & 72.1$\\pm$0.5           & OOM               & OOM           & OOM \\\\\n        & GPT-GNN &  80.1$\\pm$1.0  & 68.4$\\pm$1.6  & 76.3$\\pm$0.8 & - & - & -\\\\\n        & GATE    &  83.2$\\pm$0.6  & 71.8$\\pm$0.8  & \\underline{80.9$\\pm$0.3}           & -                 & -             & -   \\\\ \n        & DGI     &  82.3$\\pm$0.6  & 71.8$\\pm$0.7  & 76.8$\\pm$0.6           & 70.34$\\pm$0.16 & 63.8$\\pm$0.20     & 94.0$\\pm$0.10 \\\\\n        & MVGRL   & 83.5$\\pm$0.4   & 73.3$\\pm$0.5  & 80.1$\\pm$0.7           & OOM               & OOM & OOM \\\\\n        & GRACE$^{1}$   & 81.9$\\pm$0.4   & 71.2$\\pm$0.5  & 80.6$\\pm$0.4           & 71.51$\\pm$0.11  & 69.71$\\pm$0.17  &    94.72$\\pm$0.04\\\\  \n        & BGRL$^{1}$    & 82.7$\\pm$0.6   & 71.1$\\pm$0.8  & 79.6$\\pm$0.5           & \\underline{71.64$\\pm$0.12}   & \\underline{73.63$\\pm$0.16}  & 94.22$\\pm$0.03         \\\\\n        & InfoGCL$^2$  & 83.5$\\pm$0.3   & \\bf 73.5$\\pm$0.4  & 79.1$\\pm$0.2  & - & - & - \\\\\n        & CCA-SSG$^{1}$ & \\underline{84.0$\\pm$0.4}   & 73.1$\\pm$0.3  & \\underline{80.9$\\pm$0.4}  & 71.24$\\pm$0.20  & 73.34$\\pm$0.17  & \\underline{95.07$\\pm$0.02}   \\\\\n        \\cmidrule{2-8}\n         & \\model  & \\bf 84.16$\\pm$0.44  & \\underline{73.35$\\pm$0.42}  & \\bf 81.10$\\pm$0.41  & \\bf 71.75$\\pm$0.17 & \\bf 74.50$\\pm$0.29    & \\bf 96.01$\\pm$0.08    \\\\\n        \\bottomrule[1.2pt]\n    \\end{tabular}\n     \\begin{tablenotes}\n        \\footnotesize\n        \\item[1] Results are from our reproducing with authors\u2019 public code, as they didn't report the results in part of these datasets. The result of PPI is a bit different from that the authers' reported. This is because we use the authors' public code, and train the linear classifier until convergence, instead for a fixed number of epochs, in downstream evaluation.\n        \\item[2] The code is not publicly available.\n    \\end{tablenotes}\n\n    \\end{threeparttable}\n    \\label{tab:node_clf}\n\\end{table*}\n\n\\begin{table*}[htbp]\n    \\centering\n    \\caption{\\textmd{Experiment results in unsupervised representation learning for \\textbf{\\textit{graph classification}}. We report accuracy(\\%) for all datasets.}}\n    \\begin{threeparttable}\n    \\begin{tabular}{c|c|ccccccc}\n        \\toprule[1.2pt]\n              & Dataset  & IMDB-B     & IMDB-M     & PROTEINS   & COLLAB     & MUTAG      & REDDIT-B   & NCI1     \\\\ % & DD         \\\\ \n        % \\midrule\n        % \\multirow{3}{*}{Statistics}\n\n        \\midrule\n        \\multirow{2}{*}{Supervised}\n        & GIN         & 75.1$\\pm$5.1   & 52.3$\\pm$2.8   & 76.2$\\pm$2.8   & 80.2$\\pm$1.9   & 89.4$\\pm$5.6   & 92.4$\\pm$2.5   & 82.7$\\pm$1.7 \\\\ % & -   \\\\\n        & DiffPool \\\\\n        \\midrule\n        \\multirow{2}{*}{Graph Kernels}\n        & WL          & 72.30$\\pm$3.44 & 46.95$\\pm$0.46 & 72.92$\\pm$0.56 & - & 80.72$\\pm$3.00 & 68.82$\\pm$0.41 & 80.31$\\pm$0.46 \\\\ % & - \\\\\n        & DGK         & 66.96$\\pm$0.56 & 44.55$\\pm$0.52 & 73.30$\\pm$0.82 & - & 87.44$\\pm$2.72 & 78.04$\\pm$0.39 & 80.31$\\pm$0.46 \\\\ % & -  \\\\\n        \\midrule\n        \\multirow{8}{*}{Self-supervised}\n        & graph2vec   & 71.10$\\pm$0.54 & 50.44$\\pm$0.87 & 73.30$\\pm$2.05 & -              & 83.15$\\pm$9.25 & 75.78$\\pm$1.03 & 73.22$\\pm$1.81 \\\\ % & -        \\\\\n        & Infograph   & 73.03$\\pm$0.87 & 49.69$\\pm$0.53 & 74.44$\\pm$0.31 & 70.65$\\pm$1.13 & 89.01$\\pm$1.13 & 82.50$\\pm$1.42 & 76.20$\\pm$1.06 \\\\%& 72.85$\\pm$1.78 \\\\\n        & GraphCL     & 71.14$\\pm$0.44 & 48.58$\\pm$0.67 & 74.39$\\pm$0.45 & 71.36$\\pm$1.15 & 86.80$\\pm$1.34 & \\underline{89.53$\\pm$0.84} & 77.87$\\pm$0.41 \\\\%& 78.62$\\pm$0.40 \\\\\n        & JOAO        & 70.21$\\pm$3.08 & 49.20$\\pm$0.77     & \\underline{74.55$\\pm$0.41} & 69.50$\\pm$0.36 & 87.35$\\pm$1.02 & 85.29$\\pm$1.35 & 78.07$\\pm$0.47 \\\\%& 77.32$\\pm$0.54  \\\\\n        & GCC         & 72.0           & 49.4           & -    & 78.9    &  - & \\bf 89.8 & - \\\\%& - \\\\ \n        & MVGRL       & 74.2$\\pm$0.7   & 51.2$\\pm$0.5   & -              & -              & \\underline{89.7$\\pm$1.1}   & 84.5$\\pm$0.6   & -               \\\\%& -              \\\\\n        & InfoGCL     & \\underline{75.1$\\pm$0.9}   & \\underline{51.4$\\pm$0.8}   &  -             & \\underline{80.0$\\pm$1.3}   & \\bf 91.2$\\pm$1.3   & -              &  \\underline{80.2$\\pm$0.6}   \\\\%& -  \\\\\n        \\cmidrule{2-9}\n        & \\model      & \\bf 75.52$\\pm$0.66 & \\bf 51.63$\\pm$0.52 & \\bf 75.30$\\pm$0.39 & \\bf 80.32$\\pm$0.46 & 88.19$\\pm$1.26 & 88.01$\\pm$0.19 & \\bf 80.40$\\pm$0.30  \\\\%& 78.17$\\pm$0.72 \\\\\n        \\bottomrule[1.2pt]\n    \\end{tabular}\n        \\begin{tablenotes}\n            \\footnotesize\n            \\item[]   The reported results of baselines are from previous papers if available.\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{tab:graph_clf}\n\\end{table*}\n\n\n\\begin{table*}[htp]\n\\caption{\\textmd{Experiment results in \\textbf{\\textit{transfer learning}} on molecular property prediction benchmarks. The model is first pre-trained on ZINC15 and then finetuned on the following datasets. We report ROC-AUC(\\%) scores.}}\n\\label{tab:mol_clf}\n\\renewcommand\\tabcolsep{6pt}\n\\begin{tabular}{c|cccccccc}\n\\toprule[1.2pt]\n                        & BBBP       & Tox21      & ToxCast    & SIDER      & ClinTox    & MUV        & HIV        & BACE       \\\\\n\\midrule\n    No-pretrain         & 65.5$\\pm$1.8   & 74.3$\\pm$0.5   & 63.3$\\pm$1.5   & 57.2$\\pm$0.7   & 58.2$\\pm$2.8   & 71.7$\\pm$2.3   & 75.4$\\pm$1.5   & 70.0$\\pm$2.5   \\\\\n    ContextPred         & 64.3$\\pm$2.8   & 75.7$\\pm$0.7   & 63.9$\\pm$0.6   & 60.9$\\pm$0.6   & 65.9$\\pm$3.8   & 75.8$\\pm$1.7   & 77.3$\\pm$1.0   & 79.6$\\pm$1.2   \\\\\n    AttrMasking         & 64.3$\\pm$2.8   & 76.7$\\pm$0.4   & 64.2$\\pm$0.5   & 61.0$\\pm$0.7   & 71.8$\\pm$4.1   & 74.7$\\pm$1.4   & 77.2$\\pm$1.1   & 79.3$\\pm$1.6   \\\\\n    Infomax             & 68.8$\\pm$0.8  & 75.3$\\pm$0.5  & 62.7$\\pm$0.4  & 58.4 $\\pm$0.8  & 69.9$\\pm$3.0  & 75.3$\\pm$2.5  & 76.0$\\pm$0.7  & 75.9$\\pm$1.6  \\\\\n    % GPT-GNN             & 67.5\u00b11.3   & 76.1\u00b10.4   & 63.1\u00b10.5   & 59.3\u00b10.8   & 74.9\u00b12.7   & 75.0\u00b12.5   & 77.0\u00b11.7   & 78.5\u00b10.9   \\\\\n    GraphCL             & 69.68$\\pm$0.67 & 73.87$\\pm$0.66 & 62.40$\\pm$0.57 & 60.53$\\pm$0.88 & 75.99$\\pm$2.65 & 69.80$\\pm$2.66 & 78.47$\\pm$1.22 & 75.38$\\pm$1.44 \\\\\n    JOAO                & 70.22$\\pm$0.98 & 74.98$\\pm$0.29 & 62.94$\\pm$0.48 & 59.97$\\pm$0.79 & 81.32$\\pm$2.49 & 71.66$\\pm$1.43 & 76.73$\\pm$1.23 & 77.34$\\pm$0.48 \\\\\n    % GCC                 & 66.9\u00b10.7   & 76.6\u00b10.5   & 63.5\u00b10.4   & 58.0\u00b10.9   & 73.2\u00b12.6   & 74.1\u00b11.4   & 75.5\u00b10.8   & 75.0\u00b11.5   \\\\\n    % Grover              & 68.0\u00b11.5   & 76.3\u00b10.6   & 63.4\u00b10.6   & 60.7\u00b10.5   & 76.9\u00b11.9   & 75.8\u00b11.7   & 77.8\u00b11.4   & 79.5\u00b11.1   \\\\\n    % MGSSL               & 69.7\u00b10.9   & 76.5\u00b10.3   & 64.1\u00b10.7   & 61.8\u00b10.8   & 80.7\u00b12.1   & 78.7\u00b11.5   & 78.8\u00b11.2   & 79.1\u00b10.9   \\\\\n    GraphLog \\\\ \n\\midrule\n    % GraphMAE (ac)   & 71.43\u00b10.68 & 75.48\u00b10.52 & 63.82\u00b10.33 & 58.99\u00b10.66 & 80.86\u00b12.55 & 72.23\u00b11.82 & 77.49\u00b10.78 & 80.73\u00b10.77 \\\\\n    % GraphMAE (a)    & 70.79\u00b10.94 & 75.11\u00b10.46 & 64.35\u00b10.39 & 59.66\u00b10.62 & 82.88\u00b11.49 & 73.83\u00b11.70 & 77.41\u00b11.08 & 80.44\u00b10.77 \\\\\n    GraphMAE        & 72.04$\\pm$0.66 & 75.51$\\pm$0.61 & 64.06$\\pm$0.29 & 60.25$\\pm$1.13 & 82.32$\\pm$1.15 & 76.26$\\pm$2.39 & 77.19$\\pm$0.95 & 83.13$\\pm$0.86 \\\\\n\\bottomrule[1.2pt]\n\\end{tabular}\n\\end{table*}\n\n\n\n\n\n\\subsubsection{Transfer Learning}\\hfill\n\n\n\n\\vpara{Setup.} To evaluate the transferability of our proposed method, we test the performance on transfer learning on molecular property prediction in chemistry, following the setting of ~\\cite{hu2020strategies,you2020graph,you2021graph}. The model is first pre-trained in  2 million unlabeled molecules sampled from the ZINC15 database~\\cite{sterling2015zinc}, and then finetuned in 8 binary classification benchmark datasets contained in MoleculeNet~\\cite{wu2018moleculenet}. The downstream datasets are splitted by scaffold-split to mimic real-world use case.  Atom number and chirality tag are input as node features and bond type and direction are regarded as edge features. In our experiments, we only consider reconstructing node features.\n\nFor evaluation protocal,  we run experiments for 10 times and report the mean and standard deviation of ROC-AUC scores(\\%). Following the default setting in ~\\cite{hu2020strategies},  we adopt a 5-layer GIN as backbone of the encoder, and a single-layer GIN as the decoder.\n\n\n\\vpara{Experiment results.}\nWe evaluate the performance of \\model against SOTA self-supervised pre-training methods for GNNs, AttrMasking~\\cite{hu2020strategies} and ContextPred~\\cite{hu2020strategies} , and SOTA contrastive learning methods, GraphCL~\\cite{you2020graph}, JOAO~\\cite{you2021graph}, and Grover~\\cite{rong2020self}. We also report the result of another generating pre-training method, GPT-GNN~\\cite{hu2020gpt}. Table \\ref{tab:graph_clf} shows that the performance on downstream molecular property prediction tasks is comparable to SOTA self-supervised methods, in which our model has a small edge over previous best results in 4 datasets. \n\n\n% it is noteworthy \nTo summarize, our \\model achieves the competitive performance on node, graph classification, and transfer learning benchmarks using a unified learning approach. and we do not devise a specialized self-supervised learning method for each task. The results in three tasks demonstrate that \\model is effective and universal approach for various applications. \n\n\n\n\n\n\n\n\n\n\n% \\begin{table}\n%     \\centering\n%     \\caption{Performance on PPI using GAT with 4 attention heads, compared to other unsupervised and supervised baselines. \\model could ourperform supervised }\n%     \\begin{tabular}{c|c|c|c|c}\n%     \\toprule\n%                 &  hd=256*4    &   hd=512*4    &   hd=1024*4   &   hd=2048*4   \\\\\n%     \\midrule\n%         BGRL    & 73.63$\\pm$0.16 & 81.94$\\pm$0.24 & 89.75$\\pm$0.69 & 93.51$\\pm$1.05 \\\\\n%         CCA-SSG & 73.34$\\pm$0.17 & 83.75$\\pm$0.64 & 92.16$\\pm$0.63 & 96.99$\\pm$0.16 \\\\\n%         \\model  &  74.50\u00b10.24  & 85.81\u00b10.26    & 94.14\u00b10.30    &   97.79\u00b10.06 \\\\\n%     \\bottomrule\n%     \\end{tabular}\n%     \\label{tab:ppi_hd}\n% \\end{table}\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{imgs/ppi_hidden.pdf}\n    \\vspace{-4mm}\n    \\caption{Performance on PPI using GAT with 4 attention heads, compared to other baselines. Self-supervised methods benefit much from larger model size, and \\model could outperform supervised model. }\n    \\label{fig:ppi_hidden}\n\\end{figure}\n\n\n\n\\subsection{Ablation Studies}\nTo verify the effects of main components of our model, we further conduct a series of ablation studies. Without loss of generalization, we choose 3 datasets from node classification and 2 datasets from graph classification as the study objects.\n\n\n\n\\vpara{Effect of reconstruction criterion.} We study the influence of reconstruction criterion, and Table \\ref{tab:ablation} shows the results of MSE and our proposed SCE loss function. Node features are normalized in preprocessing. This helps reconstruction and improves performance in downstream evaluation. The conclusion from Table \\ref{tab:ablation} varies between node classification and graph classification.\nGenerally, input features in node classification lie in continuous space, containing more discriminative information.\n% The performance using MSE degrades after training, which indicates that using MSE as criterion may mislead learning and harm performance. \nThe results manifest that SCE has significant advantage over MSE as replacing MSE with SCE brings 4.1\\% $\\sim$ 15.6\\% absolute gain.\n%SCE forces the reconstructed embedding and node feature to be aligned in the unit sphere, while MSE measures the reconstruction quality in Euclidean space. \nIn graph classification, pre-training with either MSE or SCE as criterion improves accuracy. The input features typically lie in discrete value space, and are one-hot encoding in these benchmarks, representing degree or node-label. Reconstructing one-hot feature is, to some extent, similar to a classification task, and MSE works in this setting of discrete node feature.  Nevertheless, SCE has a small edge over MSE, as indicated by the results.\nThe same may also apply to transfer learning of molecule graphs. \nThis suggests that a proper criterion is the cornerstone of the success of generative graph self-supervised learning.\n\n\nFigure \\ref{fig:ablation} shows the influence of scaling factor $\\gamma$.  We observe that $\\gamma > 1$ benefits in most cases, especially in node classification. However, in IMDB-B, it seems that a larger $\\gamma$ value harms the performance. In our experiments, we noticed that the training error in node classification is much higher than that in graph classification. This further demonstrates that aligning continuous vectors in unit sphere is more challenging thus scaling $\\gamma$ brings improvement. \n\n\n\\vpara{Effect of decoder type.} Previous GAEs typically follow BERT and use an MLP as the decoder. In Table \\ref{tab:ablation}, we compare different decoder types, including MLP/GCN/GAT for node classification, and MLP/GIN/GAT for graph classification. The re-mask strategy is not used for MLP decoder.  The type of decoder matters in the training, and using GNN decoder typically achieves better performance in downstream tasks. Compared to MLP, which reconstructs original node feature from latent representation, GNN aims at forcing the masked nodes to extract information about its original feature from neighbors' representations. One reasonable assumption is that GNN avoids the output of encoder being aligned with original features. The learned representations is expected to be discriminative in downstream classification tasks. Therefore, the encoder could learn meaningful latent representations. MLP also works in our framework, and this may attribute to the design of our criterion.\n\nAmong different GNNs, GIN performs better in graph-level tasks, and GAT is a more reasonable option for node classification. It is observed that replacing GAT with GCN causes a significant drop, especially in Cora ($\\sim$2.9\\%) and PubMed ($\\sim$2.0\\%). We speculate that attention mechanism plays an important role when the latent representations vary in neighbors with re-masked ones.\n\n\n\n\\begin{table}[htbp]\n\\caption{Ablation study of decoder type, re-mask and reconstruction criterion on node- and graph-level benchmarks.}\n\\label{tab:ablation}\n\\renewcommand\\tabcolsep{4pt}\n% \\begin{tabular}{cccccccc}\n% \\toprule[1.2pt]\n% \\multicolumn{2}{c}{\\multirow{2}{*}{Dataset}}\n%                      & \\multicolumn{3}{c}{Node-Level} & \\phantom{} & \\multicolumn{2}{c}{Graph-Level} \\\\\n% \\cmidrule{3-5} \\cmidrule{7-8}\n%                     % & & \\small{Cora}        & \\small{PubMed}      & \\small{Arxiv}  & \\small{MUTAG}              & \\small{IMDB-B}    \\\\\n%                     & & Cora        & PubMed      & Arxiv  && MUTAG              & IMDB-B              \\\\\n\n% % \\cmidrule{2-7}\n% \\midrule\n% \\multirow{3}{*}{\\rotatebox[origin=c]{90}{\\small{COMP.}}}\n% & \\small{\\model}        & 84.16  & 81.10  & 71.75  && 88.19         & 75.52         \\\\\n% & \\small{w/o re-mask}   & 81.84  & 80.02  & 71.61  && 86.29         & 74.42          \\\\\n% & w/ MSE                & 68.49  & 73.07  & 67.44  && 86.30         & 74.04          \\\\\n% \\midrule\n% \\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\small{Decoder}}}\n% & MLP                   & 81.47  & 80.39  & 71.54  && 87.01         & 73.94          \\\\\n% & GCN                   & 81.25  & 79.06  & 71.59  &&   -           &  -             \\\\\n% & GIN                   &    -   &  -     &  -     && 88.19         & 75.52          \\\\\n% & GAT                   & 84.16  & 81.10  & 71.73  && 85.75         & 74.04           \\\\\n% \\bottomrule[1.2pt]\n% \\end{tabular}\n\\end{table}\n\n\n\n\\vpara{Effect of mask and re-mask.} \\textit{Mask} plays an important role in our method. \\model employ two mask strategies special --- masking input feature before encoder, and \\textit{re-mask} latent representations before decoder. Table \\ref{tab:ablation} studies the designs. We observe a significant drop in performance if without masking input features, indicating that input masking is vital to avoid trivial solution. For the re-mask strategy, the accuracy drops by 0.1\\%$\\sim$2.9\\% if removing the operation. In such case, it behaves like training a $K+1$ layer GNN and adopting the output of the $K$-th layer as the encoding representations. Re-mask is designed for GNN decoder and could be regarded as regularization, making the self-supervisory task more challenging.\n\n\n\n\\vpara{Effect of mask ratio.} Figure \\ref{fig:ablation} shows the influence of mask ratio. Similar to the observation in ~\\cite{he2021masked}, in most cases, the reconstruction task is not challenging enough to learn useful features with low mask ratio (0.1), and would be too difficult if the mask ratio is too high (0.9). Different from the behavior in BERT and MAE, the optimal ratio varies in different graphs. In Cora and IMDB-B, increasing the mask ratio degrades the performance after the optimal value ($\\sim$0.5), while \\model still works with a surprisingly high ratio (0.7$\\sim$0.9) in PubMed, Ogbn-arxiv and MUTAG. This motivates us to think about information redundancy in graphs. Large node degree and high homogeneity may lead to heavy information redundancy, in which a missing node feature can be recovered from very few neighboring nodes with little high-level understanding of local structure and features. Masking a very high portion of node features is necessary to learn useful representations. In contrast, lower redundancy means excessively high mask ratio would make it impossible to recover features and degrade the performance. As for how to measure information redundancy, we leave it for future work. \n\n\n% \\vpara{Effect of scaling factor $\\gamma$.} \n\n\n\n\n\n\n% \\begin{table}\n%     \\centering\n%     \\caption{Re-mask ablation.}\n%     \\label{tab:abl_remask}\n%     \\begin{tabular}{c|c|c|c|c|c}\n%     \\toprule\n%                     & Cora       & PubMed     & \\textcolor{gray}{Ogbn-arxiv} & MUTAG & IMDB-B \\\\\n%     \\midrule\n%         w/ re-mask  & 84.16\u00b10.44 & 81.10\u00b10.41 & 71.75\u00b10.17 & 88.19$\\pm$1.26 & 75.52$\\pm$ 0.35 \\\\\n%         w/o re-mask & 81.84\u00b10.58 & 80.02\u00b10.40 & 71.61\u00b10.13 & 86.29$\\pm$2.32 & 74.42$\\pm$0.32 \\\\\n%     \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}\n%     \\centering\n%     \\caption{Decoder ablation.}\n%     \\begin{tabular}{c|c|c|c|c|c}\n%         \\toprule\n%         Decoder     & Cora       & PubMed     & \\textcolor{gray}{Ogbn-arxiv} & MUTAG   & IMDB-B \\\\\n%         \\midrule\n%         MLP         & 81.47\u00b10.65 & 80.39\u00b10.68 & 71.54\u00b10.17 & 87.01\u00b11.28 & 73.94\u00b10.45 \\\\\n%         GCN(GIN)    & 81.25\u00b10.93 & 79.06\u00b10.77 & 71.59\u00b10.27 & 88.19\u00b11.26 & 75.52\u00b10.35 \\\\\n%         GAT         & 84.16\u00b10.44 & 81.10\u00b10.41 & 71.73\u00b10.24 & 85.75\u00b11.74 & 74.04\u00b10.38 \\\\\n%         \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}[]\n%     \\centering\n%     \\caption{Reconstruction ablation. }\n%     \\label{tab:loss}\n%     \\begin{tabular}{c|c|c|c|c|c}\n%         \\toprule\n%                   & Cora       & PubMed     & \\textcolor{gray}{Ogbn-arxiv} & MUTAG      & IMDB-B     \\\\\n%         \\midrule\n%         SCE(ours) & 84.16\u00b10.44 & 81.10\u00b10.41 & 71.75\u00b10.17 & 88.19\u00b11.26 & 75.52\u00b10.35 \\\\\n%         MSE & 68.49\u00b13.65 & 73.07\u00b11.41 & 67.44\u00b10.38 & 86.30\u00b11.54 & 74.04\u00b10.32 \\\\\n%         \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}[htbp]\n% \\begin{tabular}{cccc|cc}\n% \\toprule\n%                       & \\multicolumn{3}{c}{Node-classification} & \\multicolumn{2}{l}{Graph-classification} \\\\\n% \\cmidrule{2-6}\n%                       & Cora        & PubMed      & Ogbn-arxiv  & MUTAG              & IMDB-B              \\\\\n% \\cmidrule{2-6}\n% \\model & 84.16\u00b10.44  & 81.10\u00b10.41  & 71.75\u00b10.17  & 88.19\u00b11.26         & 75.52\u00b1 0.35         \\\\\n% w/o re-mask           & 81.84\u00b10.58  & 80.02\u00b10.40  & 71.61\u00b10.13  & 86.29\u00b12.32         & 74.42\u00b10.32          \\\\\n% w/ MSE                & 68.49\u00b13.65  & 73.07\u00b11.41  & 67.44\u00b10.38  & 86.30\u00b11.54         & 74.04\u00b10.32          \\\\\n% \\midrule\n% MLP                 & 81.47\u00b10.65  & 80.39\u00b10.68  & 71.54\u00b10.17  & 87.01\u00b11.28         & 73.94\u00b10.45          \\\\\n% GCN                 & 81.25\u00b10.93  & 79.06\u00b10.77  & 71.59\u00b10.27  &                    &                     \\\\\n% GIN                 &             &             &             & 88.19\u00b11.26         & 75.52\u00b10.35          \\\\\n% GAT                 & 84.16\u00b10.44  & 81.10\u00b10.41  & 71.73\u00b10.24  & 85.75\u00b11.74         & 74.04\u00b10.38           \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\end{table}\n\n\n% \\begin{table}[htbp]\n% \\small\n% \\begin{tabular}{cccc|cc}\n% \\toprule\n%                       & \\multicolumn{3}{c}{Node-classification} & \\multicolumn{2}{l}{Graph-classification} \\\\\n% \\cmidrule{2-6}\n%                       & Cora        & PubMed      & Ogbn-arxiv  & MUTAG              & IMDB-B              \\\\\n% \\cmidrule{2-6}\n% \\model                & 84.2\u00b10.4  & 81.1\u00b10.4  & 71.8\u00b10.2    & 88.2\u00b11.3         & 75.5\u00b1 0.4         \\\\\n% w/o re-mask           & 81.8\u00b10.6  & 80.0\u00b10.4  & 71.6\u00b10.1    & 86.3\u00b12.3         & 74.4\u00b10.3          \\\\\n% w/ MSE                & 68.5\u00b13.6  & 73.1\u00b11.4  & 67.4\u00b10.2    & 86.3\u00b11.5         & 74.0\u00b10.3          \\\\\n% \\midrule\n% MLP                   & 81.5\u00b10.7  & 80.4\u00b10.7  & 71.5\u00b10.2    & 87.0\u00b11.3         & 73.9\u00b10.5          \\\\\n% GCN                   & 81.2\u00b10.9  & 79.1\u00b10.8  & 71.6\u00b10.27   &                  &                     \\\\\n% GIN                   &           &           &             & 88.2\u00b11.3         & 75.5\u00b10.4          \\\\\n% GAT                   & 84.2\u00b10.4  & 81.1\u00b10.4  & 71.75\u00b10.2   & 85.8\u00b11.7         & 74.0\u00b10.4           \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\end{table}\n\n\n\n\n\\begin{figure}\n    \\centering\n    \\begin{minipage}[t]{0.235\\textwidth}\n        \\includegraphics[width=\\textwidth]{imgs/gmae_mask_ratio.pdf}\n        % \\caption{Mask ratio ablation.}\n    \\end{minipage}\n    \\begin{minipage}[t]{0.235\\textwidth}\n        \\includegraphics[width=\\textwidth]{imgs/gamma_value.pdf}\n        % \\caption{Gamma ablation.}\n    \\end{minipage}\n    \\caption{Ablation study of mask ratio and scaling factor $\\gamma$.}\n    \\label{fig:ablation}\n\\end{figure}\n\n\n% \\begin{figure}\n%     \\subfigure[name of the subfigure]{   %\u7b2c\u4e00\u5f20\u5b50\u56fe\n%     % \\begin{minipage}[t]\n%     % \\centering    %\u5b50\u56fe\u5c45\u4e2d\n%     \\includegraphics[width=0.22\\textwidth]{imgs/gmae_mask_ratio.pdf}  \n%     % \\end{minipage}\n%     }\n%     \\subfigure[name of the subfigure]{   %\u7b2c\u4e00\u5f20\u5b50\u56fe\n%     % \\begin{minipage}[t]\n%     % \\centering    %\u5b50\u56fe\u5c45\u4e2d\n%     \\includegraphics[width=0.22\\textwidth]{imgs/gmae_mask_ratio.pdf}  \n%     % \\end{minipage}\n%     }\n%     \\caption{Ablation study of mask ratio and scale coefficient $\\gamma$.}\n%     \\label{fig:my_label}\n% \\end{figure}\n\n\n\n}%=-=---------------------------------------------------------------%\\clearpage\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "%\n\\label{sec:conclusion}\nIn this work, we explore generative self-supervised learning in graphs and identify \nthe common issues that are faced by graph autoencoders. \nWe present \\model---a simple masked graph autoencoder---to address them from the  perspective of reconstruction objective, learning, loss function, and model architecture.\n%the challenges of current graph self-supervised learning. \nIn \\model, we design the masked feature reconstruction strategy with a scaled cosine error as the reconstruction criterion.  \n%from the perspective of objective, learning, loss function, and model architecture.\nWe conduct extensive experiments on a wide range of node and graph classification benchmarks, and the results demonstrate the  effectiveness and generalizability of \\model. \nOur work shows that generative SSL can offer great potential to graph representation learning and pre-training, requiring  \n%and deserve \nmore in-depth explorations for future work.\n% \\newpage\n\n\n\n\n\n\\hide{\n\n\n\\section{Conclusion}%\n\\label{sec:conclusion}\nIn this work, we explore generative learning in graphs and present the challenges of current graph self-supervised learning. Then we propose \\model a masked graph autoencoder, to address the problems from the perspective of objective, learning, loss function, and model architecture.\nWe conduct extensive experiments on a wide range of node and graph classification benchmarks, and the results demonstrate the generalizability and effectiveness of \\model. \nOur work shows that generative SSL can offer great potential to graph representation learning and deserve more in-depth exploration in the future.\n% \\newpage\n\n\n\n}%\\clearpage\n\n\\vpara{Acknowledgements.} This work is supported by Technology and Innovation Major Project of the Ministry of Science and Technology of China under Grant 2020AAA0108400 and 2020AAA0108402, Natural Science\nFoundation of China (Key Program, No. 61836013), and National Science\nFoundation for Distinguished Young Scholars (No. 61825602). \n\n\n\\balance\n\n% \\printbibliography\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{reference}\n\n\\clearpage\n\\appendix\n"
            },
            "section 6": {
                "name": "Appendix",
                "content": "\n\\label{app:appendix}\n% \\subsection{Comparison to Other Attribute-Masking strategies}\n% \\label{app:comparison}\n\n\n\n\n\n",
                "subsection 6.1": {
                    "name": "Results in the PPI Dataset",
                    "content": "\n\\label{app:ppi}\nFigure \\ref{fig:ppi_hidden} shows the results of self-supervised learning methods in PPI dataset. Previous studies often report a large gap between supervised and self-supervised results on PPI. We find that increasing the number of model parameters helps little in supervised setting, but could highly boost the performance in self-supervised setting. As the hidden size reaches 2048$\\times$4, \\model even outperforms supervised counter-part, although the model would be too much larger. At least, this indicates that the gap could be filled.\n% and \\model benefits more from the larger model scale. \n\nThe results of BGRL and GRACE in PPI are a bit different from those reported in ~\\cite{thakoor2021bootstrapped}. This is because we train the linear classifier until convergence during evaluation, using the authors\u2019 official code. In ~\\cite{thakoor2021bootstrapped}, the classifier is trained only for a small fixed number of epochs, and the training does not converge.\n% This might indicate the potential of large-scale GNNs in pre-training.\n\n\n\n\n\n"
                },
                "subsection 6.2": {
                    "name": "Ablation on the Encoder Architecture",
                    "content": "\n% \\vpara{Effect of encoder architecture.}  \nIn node classification, \\model uses GAT as the encoder. To have a fair comparison and investigate the influence of different GNN backbones, we compare the best baselines, BGRL and CCA-SSG, in node classification datasets using GCN and GAT as the encoder. The results are shown in Table \\ref{tab:backbone}. We observe that \\model still outperforms the baselines with the same GAT backbone. \nIn addition, the results manifest that attention mechanism would not always benefit graph self-supervised learning, as GCN is inferior to GAT, for instance, in (Cora, CCA-SSG) and (Ogbn-arxiv, \\model). Under the training setting of \\model, GAT could be a better option in most cases.\n\n\n\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Implementation Details",
                    "content": "\n",
                    "subsubsection 6.3.1": {
                        "name": "Environment",
                        "content": "\nMost experiments are conducted on Linux servers equipped with an Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, 256GB RAM and NVIDIA 2080Ti GPUs. Experiments of Ogbn-arxiv and Reddit for node classificatin are conducted on Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz and NVIDIA 3090 GPUs, as they require large memory. Models of node and graph classification are implemented in PyTorch version 1.9.0, DGL version 0.7.2 (\\textit{\\url{https://www.dgl.ai/}})  with CUDA version 10.2, scikit-learn version 0.24.1 and Python 3.7. \n% Our code and datasets will be available. \nFor molecular property prediction, we implement our model based on the code in \\textit{\\url{https://github.com/snap-stanford/pretrain-gnns}} with Pytorch Geometric 2.0.4 (\\textit{\\url{https://www.pyg.org/}}).\n\n\n\n\n\n\n\n\\hide{\n\\subsubsection{Model Configuration}\nFor node classification, we train the model using Adam Optimizer with $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 1 \\times 10^{-8}$. The initial learning rate is set to 0.001, using cosine learning rate decay without warmup. We use PReLU as the non-linear activation. More details about hyper-parameters and datasets are in Table \\ref{tab:hp_node} and \\url{https://github.com/THUDM/GraphMAE}.\n\nFor graph classification, we set the initial learning rate to 0.00015 with cosine learning rate decay for most cases. For the evaluation,  the parameter C of SVM is searched in the sets $\\{10^{-3},...,10\\}$. The hyper-parameters and statistics of datasets are in Table ~\\ref{tab:hp_graph}.\n\nFor transfer learning of molecule property prediction, we adopt a single-layer GIN as decoder, the mask rate is set to 0.25, and we pretrain the whole model for 100 epochs.\n% we implement the pre-training and finetune based on the code in \\textit{\\url{https://github.com/snap-stanford/pretrain-gnns}}.\n% For the pre-training, \nFor the finetuning, an Adam optimizer (learning rate: 0.001, batch size: 32) is employed to train the model for 100 epochs. We utilize a learning rate scheduler with fix step size, which multiplies the learning rate by 0.3 every 30 epochs for BACE only. Table ~\\ref{tab:stat_mol} shows the statistics of datasets.\n% for pre-training and finetune.\n\n\n\n\\subsection{Baselines}\n% For GAT, GATE, DGI, and InfoGCL, we use the results reported in the previous papers. MVGRL's results are from DGL' reproducing. For GPT-GNN, GRACE, BRGL, and CCA-SSG, as they didn't report \nFor node classification, the results of supervised baselines of GCN in Reddit, and GAT in Ogbn-arxiv and Reddit are from CogDL(\\textit{\\url{https://cogdl.ai/}}) if not reported before. For unsupervised baselines, GRACE~\\cite{zhu2020deep}, BGRL~\\cite{thakoor2021bootstrapped}, CCA-SSG~\\cite{zhang2021canonical} are state-of-the-art contrastive learning methods in graph. \n% They did not report results in several benchmarks in their paper. \nGRACE and BGRL did not report the results in Cora, Citeseer, and PubMed of the public split. To have a fair comparison, we download the public source code and use the same GNN backbone as \\model. We conduct hyper-parameter search for them and select the best results on the validation set.\n% if the hyper-parameters are not provided in their repository.\nThe results of CCA-SSG are the output of the official code after the bugs in the code are fixed. For MVGRL~\\cite{hassani2020contrastive}, we adopt DGL's reproducing results. We implement GPT-GNN~\\cite{hu2020gpt} based on the official code for homogeneous networks, as the code is for heterogeneous networks, and report the results in Cora, Citeseer, and PubMed. \n% The sources of the codes used are as follows:\n% \\begin{itemize}\n%     \\item BRGL: \\url{https://github.com/Namkyeong/BGRL_Pytorch}\n%     \\item GRACE: \\url{https://github.com/CRIPAC-DIG/GRACE}\n%     \\item CCA-SSG: \\url{https://github.com/hengruizhang98/CCA-SSG/}\n%     \\item MVGRL: \\url{https://github.com/hengruizhang98/mvgrl}\n%     \\item GPT-GNN: \\url{https://github.com/acbull/GPT-GNN}\n% \\end{itemize}\n\nFor graph classification and transfer learning of molecular property prediction, we adopt the results reported in previous papers if available. For the results of GraphCL~\\cite{you2020graph} and JOAO~\\cite{you2021graph} in IMDB-MULTI, we download the authors' official codes and keep hyper-parameters the same to get the output. }\n\n\n\n\n\n\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 6.3.2": {
                        "name": "Model Configuration",
                        "content": "\nFor node classification, we train the model using Adam Optimizer with $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 1 \\times 10^{-8}$. The initial learning rate is set to 0.001, using cosine learning rate decay without warmup. We use PReLU as the non-linear activation. More details about hyper-parameters and datasets are in Table \\ref{tab:hp_node} and \\url{https://github.com/THUDM/GraphMAE}.\n\nFor graph classification, we set the initial learning rate to 0.00015 with cosine learning rate decay for most cases. For the evaluation,  the parameter C of SVM is searched in the sets $\\{10^{-3},...,10\\}$. The hyper-parameters and statistics of datasets are in Table ~\\ref{tab:hp_graph}.\n\nFor transfer learning of molecule property prediction, we adopt a single-layer GIN as decoder, the mask rate is set to 0.25, and we pretrain the whole model for 100 epochs.\n% we implement the pre-training and finetune based on the code in \\textit{\\url{https://github.com/snap-stanford/pretrain-gnns}}.\n% For the pre-training, \nFor the finetuning, an Adam optimizer (learning rate: 0.001, batch size: 32) is employed to train the model for 100 epochs. We utilize a learning rate scheduler with fix step size, which multiplies the learning rate by 0.3 every 30 epochs for BACE only. Table ~\\ref{tab:stat_mol} shows the statistics of datasets.\n% for pre-training and finetune.\n\n\n"
                    }
                },
                "subsection 6.4": {
                    "name": "Baselines",
                    "content": "\n% For GAT, GATE, DGI, and InfoGCL, we use the results reported in the previous papers. MVGRL's results are from DGL' reproducing. For GPT-GNN, GRACE, BRGL, and CCA-SSG, as they didn't report \nFor node classification, the results of supervised baselines of GCN in Reddit, and GAT in Ogbn-arxiv and Reddit are from CogDL(\\textit{\\url{https://cogdl.ai/}}) if not reported before. For unsupervised baselines, GRACE~\\cite{zhu2020deep}, BGRL~\\cite{thakoor2021bootstrapped}, CCA-SSG~\\cite{zhang2021canonical} are state-of-the-art contrastive learning methods in graph. \n% They did not report results in several benchmarks in their paper. \nGRACE and BGRL did not report the results in Cora, Citeseer, and PubMed of the public split. To have a fair comparison, we download the public source code and use the same GNN backbone as \\model. We conduct hyper-parameter search for them and select the best results on the validation set.\n% if the hyper-parameters are not provided in their repository.\nThe results of CCA-SSG are the output of the official code after the bugs in the code are fixed. For MVGRL~\\cite{hassani2020contrastive}, we adopt DGL's reproducing results. We implement GPT-GNN~\\cite{hu2020gpt} based on the official code for homogeneous networks, as the code is for heterogeneous networks, and report the results in Cora, Citeseer, and PubMed. \n% The sources of the codes used are as follows:\n% \\begin{itemize}\n%     \\item BRGL: \\url{https://github.com/Namkyeong/BGRL_Pytorch}\n%     \\item GRACE: \\url{https://github.com/CRIPAC-DIG/GRACE}\n%     \\item CCA-SSG: \\url{https://github.com/hengruizhang98/CCA-SSG/}\n%     \\item MVGRL: \\url{https://github.com/hengruizhang98/mvgrl}\n%     \\item GPT-GNN: \\url{https://github.com/acbull/GPT-GNN}\n% \\end{itemize}\n\nFor graph classification and transfer learning of molecular property prediction, we adopt the results reported in previous papers if available. For the results of GraphCL~\\cite{you2020graph} and JOAO~\\cite{you2021graph} in IMDB-MULTI, we download the authors' official codes and keep hyper-parameters the same to get the output. \n% Codes:\n% \\begin{itemize}\n%     \\item GraphCL : \\url{https://github.com/Shen-Lab/GraphCL}\n%     \\item JOAO: \\url{https://github.com/Shen-Lab/GraphCL_Automated}\n% \\end{itemize}\n\n\n"
                }
            }
        },
        "tables": {
            "tab:node_clf": "\\begin{table*}[htbp]\n    \\centering\n    \\caption{Experiment results in unsupervised representation learning for \\underline{node classification}. \\textmd{We report the Micro-F1 (\\%) score for PPI and accuracy (\\%) for the other datasets. }\n    }\n    \\begin{threeparttable}\n    \\renewcommand\\tabcolsep{10pt}\n    \\renewcommand\\arraystretch{1.05}\n    \\begin{tabular}{c|c|cccccc}\n        \\toprule[1.2pt]\n            & Dataset &   Cora      & CiteSeer      & PubMed                & Ogbn-arxiv        & PPI               & Reddit        \\\\\n        % \\midrule\n        % \\multirow{3}{*}{Statistics}\n\n         \\midrule\n        \\multirow{2}{*}{Supervised} \n        & GCN     &  81.5          & 70.3          & 79.0                   & 71.74$\\pm$0.29    & 75.7$\\pm$0.1    & 95.3$\\pm$0.1           \\\\\n        & GAT     &  83.0$\\pm$0.7  & 72.5$\\pm$0.7  & 79.0$\\pm$0.3           & 72.10$\\pm$0.13     & 97.30$\\pm$0.20    & 96.0$\\pm$0.1           \\\\\n        \\midrule\n        \\multirow{10}{*}{Self-supervised} \n        & GAE     &  71.5$\\pm$0.4  & 65.8$\\pm$0.4  & 72.1$\\pm$0.5           & -               & -           & - \\\\\n        & GPT-GNN &  80.1$\\pm$1.0  & 68.4$\\pm$1.6  & 76.3$\\pm$0.8 & - & - & -\\\\\n        & GATE    &  83.2$\\pm$0.6  & 71.8$\\pm$0.8  & 80.9$\\pm$0.3           & -                 & -             & -   \\\\ \n        & DGI     &  82.3$\\pm$0.6  & 71.8$\\pm$0.7  & 76.8$\\pm$0.6           & 70.34$\\pm$0.16 & 63.80$\\pm$0.20     & 94.0$\\pm$0.10 \\\\\n        & MVGRL   & 83.5$\\pm$0.4   & 73.3$\\pm$0.5  & 80.1$\\pm$0.7           & -               & - & - \\\\\n        & GRACE$^{1}$   & 81.9$\\pm$0.4   & 71.2$\\pm$0.5  & 80.6$\\pm$0.4           & 71.51$\\pm$0.11  & 69.71$\\pm$0.17  &    94.72$\\pm$0.04\\\\  \n        & BGRL$^{1}$    & 82.7$\\pm$0.6   & 71.1$\\pm$0.8  & 79.6$\\pm$0.5           & \\underline{71.64$\\pm$0.12}   & \\underline{73.63$\\pm$0.16}  & 94.22$\\pm$0.03         \\\\\n        & InfoGCL  & 83.5$\\pm$0.3   & \\bf 73.5$\\pm$0.4  & 79.1$\\pm$0.2  & - & - & - \\\\\n        & CCA-SSG$^{1}$ & \\underline{84.0$\\pm$0.4}   & 73.1$\\pm$0.3  & \\underline{81.0$\\pm$0.4}  & 71.24$\\pm$0.20  & 73.34$\\pm$0.17  & \\underline{95.07$\\pm$0.02}   \\\\\n        \\cmidrule{2-8}\n        %  & \\model  & \\bf 84.16\u00b10.44  & \\underline{73.35\u00b10.42}  & \\bf 81.10\u00b10.41  & \\bf 71.75$\\pm$0.17 & \\bf 74.50$\\pm$0.29    & \\bf 96.01$\\pm$0.08    \\\\\n         & \\model  & \\bf 84.2\u00b10.4  & \\underline{73.4\u00b10.4}  & \\bf 81.1\u00b10.4  & \\bf 71.75$\\pm$0.17 & \\bf 74.50$\\pm$0.29    & \\bf 96.01$\\pm$0.08    \\\\\n\n        \\bottomrule[1.2pt]\n    \\end{tabular}\n     \\begin{tablenotes}\n        \\footnotesize\n        \\item[] The results not reported are due to unavailable code or out-of-memory.\n        \\item[1] Results are from reproducing using authors' official code, as they did not report the results in part of datasets. The result of PPI is a bit different from what the authors' reported. This is because we train the linear classifier until convergence, rather than for a small fixed number of epochs during evaluation, using the official code.\n    \\end{tablenotes}\n\n    \\end{threeparttable}\n    \\label{tab:node_clf}\n\\end{table*}",
            "tab:graph_clf": "\\begin{table*}[htbp]\n    \\centering\n    \\caption{Experiment results in unsupervised representation learning for \\underline{graph classification}. \\textmd{We report accuracy (\\%) for all datasets.}}\n    \\begin{threeparttable}\n    \\renewcommand\\arraystretch{1.05}\n    \\begin{tabular}{c|c|ccccccc}\n        \\toprule[1.2pt]\n              & Dataset  & IMDB-B     & IMDB-M     & PROTEINS   & COLLAB     & MUTAG      & REDDIT-B   & NCI1     \\\\ % & DD         \\\\ \n        % \\midrule\n        % \\multirow{3}{*}{Statistics}\n\n        \\midrule\n        \\multirow{2}{*}{Supervised}\n        & GIN         & 75.1$\\pm$5.1   & 52.3$\\pm$2.8   & 76.2$\\pm$2.8   & 80.2$\\pm$1.9   & 89.4$\\pm$5.6   & 92.4$\\pm$2.5   & 82.7$\\pm$1.7 \\\\ % & -   \\\\\n        & DiffPool    & 72.6$\\pm$3.9 &  -           &  75.1$\\pm$3.5   & 78.9$\\pm$2.3 & 85.0$\\pm$10.3 & 92.1$\\pm$2.6 & - \\\\\n        \\midrule\n        \\multirow{2}{*}{Graph Kernels}\n        & WL          & 72.30$\\pm$3.44 & 46.95$\\pm$0.46 & 72.92$\\pm$0.56 & - & 80.72$\\pm$3.00 & 68.82$\\pm$0.41 & 80.31$\\pm$0.46 \\\\ % & - \\\\\n        & DGK         & 66.96$\\pm$0.56 & 44.55$\\pm$0.52 & 73.30$\\pm$0.82 & - & 87.44$\\pm$2.72 & 78.04$\\pm$0.39 & 80.31$\\pm$0.46 \\\\ % & -  \\\\\n        \\midrule\n        \\multirow{8}{*}{Self-supervised}\n        & graph2vec   & 71.10$\\pm$0.54 & 50.44$\\pm$0.87 & 73.30$\\pm$2.05 & -              & 83.15$\\pm$9.25 & 75.78$\\pm$1.03 & 73.22$\\pm$1.81 \\\\ % & -        \\\\\n        & Infograph   & 73.03$\\pm$0.87 & 49.69$\\pm$0.53 & 74.44$\\pm$0.31 & 70.65$\\pm$1.13 & 89.01$\\pm$1.13 & 82.50$\\pm$1.42 & 76.20$\\pm$1.06 \\\\%& 72.85$\\pm$1.78 \\\\\n        & GraphCL     & 71.14$\\pm$0.44 & 48.58$\\pm$0.67 & 74.39$\\pm$0.45 & 71.36$\\pm$1.15 & 86.80$\\pm$1.34 & \\underline{89.53$\\pm$0.84} & 77.87$\\pm$0.41 \\\\%& 78.62$\\pm$0.40 \\\\\n        & JOAO        & 70.21$\\pm$3.08 & 49.20$\\pm$0.77     & \\underline{74.55$\\pm$0.41} & 69.50$\\pm$0.36 & 87.35$\\pm$1.02 & 85.29$\\pm$1.35 & 78.07$\\pm$0.47 \\\\%& 77.32$\\pm$0.54  \\\\\n        & GCC         & 72.0           & 49.4           & -    & 78.9    &  - & \\bf 89.8 & - \\\\%& - \\\\ \n        & MVGRL       & 74.20$\\pm$0.70   & 51.20$\\pm$0.50   & -              & -              & \\underline{89.70$\\pm$1.10}   & 84.50$\\pm$0.60   & -               \\\\%& -              \\\\\n        & InfoGCL     & \\underline{75.10$\\pm$0.90}   & \\underline{51.40$\\pm$0.80}   &  -             & \\underline{80.00$\\pm$1.30}   & \\bf 91.20$\\pm$1.30   & -              &  \\underline{80.20$\\pm$0.60}   \\\\%& -  \\\\\n        \\cmidrule{2-9}\n        & \\model      & \\bf 75.52$\\pm$0.66 & \\bf 51.63$\\pm$0.52 & \\bf 75.30$\\pm$0.39 & \\bf 80.32$\\pm$0.46 & 88.19$\\pm$1.26 & 88.01$\\pm$0.19 & \\bf 80.40$\\pm$0.30  \\\\%& 78.17$\\pm$0.72 \\\\\n        \\bottomrule[1.2pt]\n    \\end{tabular}\n        \\begin{tablenotes}\n            \\footnotesize\n            \\item[]   The reported results of baselines are from previous papers if available.\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{tab:graph_clf}\n\\end{table*}",
            "tab:mol_clf": "\\begin{table*}[htp]\n\\caption{Experiment results in \\underline{transfer learning} on molecular property prediction benchmarks. \\textmd{The model is first pre-trained on ZINC15 and then finetuned on the following datasets. We report ROC-AUC scores (\\%).}}\n\\label{tab:mol_clf}\n\\renewcommand\\tabcolsep{8pt}\n\\renewcommand\\arraystretch{1.05}\n\\begin{tabular}{c|cccccccc|c}\n\\toprule[1.2pt]\n                        & BBBP       & Tox21      & ToxCast    & SIDER      & ClinTox    & MUV        & HIV        & BACE & Avg.      \\\\\n\\midrule\n    No-pretrain         & 65.5\u00b11.8   & 74.3\u00b10.5   & 63.3\u00b11.5   & 57.2\u00b10.7   & 58.2\u00b12.8   & 71.7\u00b12.3   & 75.4\u00b11.5   & 70.0\u00b12.5 & 67.0  \\\\\n\\midrule    \n    ContextPred         & 64.3\u00b12.8   & \\underline{75.7\u00b10.7}   & 63.9\u00b10.6   & 60.9\u00b10.6   & 65.9\u00b13.8   & 75.8\u00b11.7   & 77.3\u00b11.0   & 79.6\u00b11.2 & 70.4 \\\\\n    AttrMasking         & 64.3\u00b12.8   & \\bf 76.7\u00b10.4   & \\bf 64.2\u00b10.5   &  \\underline{61.0\u00b10.7}   & 71.8\u00b14.1   & 74.7\u00b11.4   & 77.2\u00b11.1   & 79.3\u00b11.6 & 71.1 \\\\\n    Infomax             & 68.8 \u00b10.8  & 75.3 \u00b10.5  & 62.7 \u00b10.4  & 58.4 \u00b10.8  & 69.9\u00b13.0  & 75.3 \u00b12.5  & 76.0 \u00b10.7  & 75.9 \u00b11.6 & 70.3 \\\\\n    % GraphCL             & 69.68\u00b10.67 & 73.87\u00b10.66 & 62.40\u00b10.57 & 60.53\u00b10.88 & 75.99\u00b12.65 & 69.80\u00b12.66 & \\bf 78.47\u00b11.22 & 75.38\u00b11.44 & \\\\\n    % JOAO                & 70.22\u00b10.98 & 74.98\u00b10.29 & 62.94\u00b10.48 & 59.97\u00b10.79 & \\underline{81.32\u00b12.49} & 71.66\u00b11.43 & 76.73\u00b11.23 & 77.34\u00b10.48 & \\\\\n    GraphCL             & 69.7\u00b10.7 & 73.9\u00b10.7 & 62.4\u00b10.6 & 60.5\u00b10.9 & 76.0\u00b12.7 & 69.8\u00b12.7 & \\bf 78.5\u00b11.2 & 75.4\u00b11.4 & 70.8 \\\\\n    JOAO                & 70.2\u00b11.0 & 75.0\u00b10.3 & 62.9\u00b10.5 & 60.0\u00b10.8 & \\underline{81.3\u00b12.5} & 71.7\u00b11.4 & 76.7\u00b11.2 & 77.3\u00b10.5 & 71.9 \\\\\n    GraphLoG            & \\bf 72.5\u00b10.8 &  \\underline{75.7\u00b10.5}  &  63.5\u00b10.7     & \\bf 61.2\u00b11.1  & 76.7\u00b13.3   & \\underline{76.0\u00b11.1}    & \\underline{77.8\u00b10.8}  & \\bf 83.5\u00b11.2 & \\underline{73.4} \\\\ \n\\midrule\n    % GraphMAE (ac)   & 71.43\u00b10.68 & 75.48\u00b10.52 & 63.82\u00b10.33 & 58.99\u00b10.66 & 80.86\u00b12.55 & 72.23\u00b11.82 & 77.49\u00b10.78 & 80.73\u00b10.77 \\\\\n    % GraphMAE (a)    & 70.79\u00b10.94 & 75.11\u00b10.46 & 64.35\u00b10.39 & 59.66\u00b10.62 & 82.88\u00b11.49 & 73.83\u00b11.70 & 77.41\u00b11.08 & 80.44\u00b10.77 \\\\\n    % GraphMAE            & \\underline{72.04\u00b10.66} & 75.51\u00b10.61 & \\underline{64.06\u00b10.29} & 60.25\u00b11.13 & \\bf 82.32\u00b11.15 & \\bf 76.26\u00b12.39 & 77.19\u00b10.95 & \\underline{83.13\u00b10.86} & \\bf 73.81 \\\\\n    GraphMAE            & \\underline{72.0\u00b10.6} & 75.5\u00b10.6 & \\underline{64.1\u00b10.3} & 60.3\u00b11.1 & \\bf 82.3\u00b11.2 & \\bf 76.3\u00b12.4 & 77.2\u00b11.0 & \\underline{83.1\u00b10.9} & \\bf 73.8 \\\\\n\\bottomrule[1.2pt]\n\\end{tabular}\n\\end{table*}",
            "tab:ablation": "\\begin{table}[t]\n\\caption{Ablation studies of the decoder type, re-mask and reconstruction criterion on node- and graph-level datasets.}\n\\label{tab:ablation}\n\\renewcommand\\tabcolsep{4pt}\n% \\begin{tabular}{cccccccc}\n% \\toprule[1.2pt]\n% \\multicolumn{2}{c}{\\multirow{2}{*}{Dataset}}\n%                      & \\multicolumn{3}{c}{Node-Level} & \\phantom{} & \\multicolumn{2}{c}{Graph-Level} \\\\\n% \\cmidrule{3-5} \\cmidrule{7-8}\n%                     % & & \\small{Cora}        & \\small{PubMed}      & \\small{Arxiv}  & \\small{MUTAG}              & \\small{IMDB-B}    \\\\\n%                     & & Cora        & PubMed      & Arxiv  && MUTAG              & IMDB-B              \\\\\n\n% % \\cmidrule{2-7}\n% \\midrule\n% \\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\small{COMP.}}}\n% & \\model        & 84.16  & 81.10  & 71.75  && 88.19         & 75.52         \\\\\n% & \\small{w/o mask} &  79.72 & 77.92 & 70.97 & & 82.58 & 74.42 \\\\\n% & \\small{w/o re-mask}   & 82.70  & 80.02  & 71.61  && 86.29         & 74.42          \\\\\n% & \\small{w/ MSE}                & 79.12  & 73.07  & 67.44  && 86.30         & 74.04          \\\\\n% \\midrule\n% \\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\small{Decoder}}}\n% & MLP                   & 82.20  & 80.39  & 71.54  && 87.16         & 73.94          \\\\\n% & GCN                   & 81.25  & 79.06  & 71.59  && 87.78         & 74.54             \\\\\n% & GIN                   & 81.81  & 80.15  & 71.41  && 88.19         & 75.52          \\\\\n% & GAT                   & 84.16  & 81.10  & 71.75  && 86.27         & 74.04           \\\\\n% \\bottomrule[1.2pt]\n% \\end{tabular}\n\n\n\n\\begin{tabular}{cccccccc}\n\\toprule[1.2pt]\n\\multicolumn{2}{c}{\\multirow{2}{*}{Dataset}}\n                     & \\multicolumn{3}{c}{Node-Level} & \\phantom{} & \\multicolumn{2}{c}{Graph-Level} \\\\\n\\cmidrule{3-5} \\cmidrule{7-8}\n                    & & Cora        & PubMed      & Arxiv  && MUTAG              & IMDB-B              \\\\\n\n\\midrule\n\\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\small{COMP.}}}\n& \\model        & 84.2  & 81.1  & 71.75  && 88.19         & 75.52         \\\\\n& \\small{w/o mask} &  79.7 & 77.9 & 70.97 & & 82.58 & 74.42 \\\\\n& \\small{w/o re-mask}   & 82.7  & 80.0  & 71.61  && 86.29         & 74.42          \\\\\n& \\small{w/ MSE}                & 79.1  & 73.1  & 67.44  && 86.30         & 74.04          \\\\\n\\midrule\n\\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\small{Decoder}}}\n& MLP                   & 82.2  & 80.4  & 71.54  && 87.16         & 73.94          \\\\\n& GCN                   & 81.3  & 79.1  & 71.59  && 87.78         & 74.54             \\\\\n& GIN                   & 81.8  & 80.2  & 71.41  && 88.19         & 75.52          \\\\\n& GAT                   & 84.2  & 81.1  & 71.75  && 86.27         & 74.04           \\\\\n\\bottomrule[1.2pt]\n\\end{tabular}\n\n\n\\vspace{-2mm}\n\\end{table}",
            "tab:com_attm": "\\begin{table}[htbp]\n    \\centering\n    \\caption{Comparison with other attributed-masking methods in node classification. ``FT\" means finetuning the model in downstream tasks, while ``LP\" represents training a linear classifier for classification.}\n    \\begin{threeparttable}\n    \\begin{tabular}{c|ccc|c}\n        \\toprule[1.2pt]\n                     & Cora     & Citeseer   & PubMed  & Type   \\\\\n        \\midrule\n        GAE          & 71.5 &   65.8    & 72.1 & LP \\\\\n        GPT-GNN      &  80.1    & 68.4 &     76.3       & LP \\\\\n        NodeProp     &  81.94   &  71.60     &  79.44   & FT \\\\\n        RASSL-GCN$^{1}$    &  83.80   &  72.95     & *81.23   & FT \\\\\n        GATE         &  83.2    &  71.8      &  80.9    &  LP    \\\\\n        \\midrule\n        \\model       & 84.16    & 73.35      & 81.10    & LP  \\\\\n        \\bottomrule[1.2pt]\n    \\end{tabular}\n            \\begin{tablenotes}\n            \\footnotesize\n            \\item[1] PubMed dataset used is different from that in other baselines.\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{tab:com_attm}\n\\end{table}",
            "tab:backbone": "\\begin{table}[htbp]\n\\centering\n\\caption{Experiment results using different encoder backbones in node classification.}\n\\renewcommand\\tabcolsep{3pt}\n\\begin{tabular}{c|cccc}\n\\toprule[1.2pt]\n          & Cora & Citeseer & Pubmed & Ogbn-arxiv \\\\\n\\midrule\nBGRL (GCN)    &   82.7$\\pm$0.6    &  71.1$\\pm$0.7        & 79.4$\\pm$0.6       &  71.64$\\pm$0.12      \\\\\nBGRL (GAT)    &   82.8$\\pm$0.5    &   71.1$\\pm$0.8       &  79.6$\\pm$0.5      &   70.07$\\pm$0.02 \\\\\nCCA-SSG (GCN) &   84.0$\\pm$0.4      &  73.1$\\pm$0.3         &  81.0$\\pm$0.4        &  70.81$\\pm$0.13  \\\\\nCCA-SSG (GAT) &   83.8$\\pm$0.5    &  72.6$\\pm$0.7        &   79.9$\\pm$1.1     &  71.24$\\pm$0.20  \\\\\n\\model (GCN)  &   82.9$\\pm$0.6             &  72.5$\\pm$0.5                 &  81.0$\\pm$0.5               &  \\bf 71.87$\\pm$0.21\\\\   \n\\model (GAT)  &   \\bf 84.2$\\pm$0.4        &  \\bf 73.4$\\pm$0.4       & \\bf 81.1$\\pm$0.4      &  71.75$\\pm$0.17 \\\\\n\\bottomrule[1.2pt]\n\\end{tabular}\n\\vspace{-2mm}\n\\label{tab:backbone}\n\\end{table}",
            "tab:graph_sta_hyper": "\\begin{table*}[htbp]\n\\centering\n\\label{tab:graph_sta_hyper}\n\\caption{Statistics and hyper-parameters for node classification datasets. ``s'' represents multi-class classification, and ``m'' means multi-label classification. \n% ``LN'' means layer-normalization.\n}\n\\renewcommand\\tabcolsep{6.5pt}\n\\renewcommand\\arraystretch{1.05}\n\\begin{tabular}{c|c|cccccc}\n\\toprule[1.2pt]\n& Dataset               & Cora & Citeseer & PubMed & Ogbn-arxiv & PPI       & Reddit    \\\\\n\\midrule\n\\multirow{3}{*}{Statistics}\n&         \\# nodes   & 2,708 & 3,327 & 19,717 & 169,343      & 56,944  & 232,965      \\\\\n&         \\# edges   & 5,429 & 4,732 & 44,338 & 1,166,243    & 818,736 & 11,606,919   \\\\\n&         \\# classes & 7(s)  & 6(s)  & 3(s)   & 40(s)        & 121(m)  & 41(s)         \\\\\n\\midrule\n\\multirow{6}{*}{Hyper-parameters} \n& scaling factor $\\gamma$ & 3    & 1        & 3      & 3          & 3         & 3         \\\\\n& masking rate            & 0.5  & 0.5      & 0.75   & 0.5        & 0.5       & 0.75      \\\\\n& replacing rate          & 0.05 & 0.10     & 0      & 0          & 0         & 0.15      \\\\\n& hidden\\_size            & 512  & 512      & 1024   & 1024       & 1024      & 512       \\\\\n% & num\\_layer              & 2    & 2        & 2      & 3          & 3         & 4         \\\\\n& weight\\_decay           & 2e-4 & 2e-5     & 1e-5   & 0          & 0         & 2e-4      \\\\\n& max\\_epoch              & 1500 & 300      & 1000   & 1000       & 1000      & 500       \\\\\n% & Norm                    & -    & -        & -      & LN  & LN & LN \\\\\n\\bottomrule[1.2pt]\n\\end{tabular}\n\\label{tab:hp_node}\n\\end{table*}",
            "tab:hp_node": "\\begin{table*}[htbp]\n\\centering\n\\label{tab:graph_sta_hyper}\n\\caption{Statistics and hyper-parameters for node classification datasets. ``s'' represents multi-class classification, and ``m'' means multi-label classification. \n% ``LN'' means layer-normalization.\n}\n\\renewcommand\\tabcolsep{6.5pt}\n\\renewcommand\\arraystretch{1.05}\n\\begin{tabular}{c|c|cccccc}\n\\toprule[1.2pt]\n& Dataset               & Cora & Citeseer & PubMed & Ogbn-arxiv & PPI       & Reddit    \\\\\n\\midrule\n\\multirow{3}{*}{Statistics}\n&         \\# nodes   & 2,708 & 3,327 & 19,717 & 169,343      & 56,944  & 232,965      \\\\\n&         \\# edges   & 5,429 & 4,732 & 44,338 & 1,166,243    & 818,736 & 11,606,919   \\\\\n&         \\# classes & 7(s)  & 6(s)  & 3(s)   & 40(s)        & 121(m)  & 41(s)         \\\\\n\\midrule\n\\multirow{6}{*}{Hyper-parameters} \n& scaling factor $\\gamma$ & 3    & 1        & 3      & 3          & 3         & 3         \\\\\n& masking rate            & 0.5  & 0.5      & 0.75   & 0.5        & 0.5       & 0.75      \\\\\n& replacing rate          & 0.05 & 0.10     & 0      & 0          & 0         & 0.15      \\\\\n& hidden\\_size            & 512  & 512      & 1024   & 1024       & 1024      & 512       \\\\\n% & num\\_layer              & 2    & 2        & 2      & 3          & 3         & 4         \\\\\n& weight\\_decay           & 2e-4 & 2e-5     & 1e-5   & 0          & 0         & 2e-4      \\\\\n& max\\_epoch              & 1500 & 300      & 1000   & 1000       & 1000      & 500       \\\\\n% & Norm                    & -    & -        & -      & LN  & LN & LN \\\\\n\\bottomrule[1.2pt]\n\\end{tabular}\n\\label{tab:hp_node}\n\\end{table*}",
            "tab:node_sta_hyper": "\\begin{table*}[htbp]\n\\centering\n\\label{tab:node_sta_hyper}\n\\caption{Statistics and hyper-parameters for graph classification datasets. \n% ``LN'' and ``BN'' represents layer-normalization and batch normalization, respectively.\n}\n\\renewcommand\\arraystretch{1.05}\n\\begin{tabular}{c|c|ccccccc}\n        \\toprule[1.2pt]\n         &  Dataset             & IMDB-B    & IMDB-M    & PROTEINS  & COLLAB    & MUTAG     & REDDIT-B  & NCI1      \\\\\n        \\midrule\n\\multirow{3}{*}{Statistics}             \n\n        & \\# graphs      & 1,000 & 1,500 & 1,113 & 5,000 & 188  & 2,000 & 4,110 \\\\     \n        & \\# classes     &  2    & 3     & 2     & 3     & 2    & 2     & 2    \\\\            \n        & Avg. \\# nodes  & 19.8  & 13.0  & 39.1  & 74.5  & 17.9 & 429.7 & 29.8  \\\\\n        \\midrule\n\\multirow{7}{*}{\\begin{tabular}[c]{@{}c@{}}Hyper-\\\\ parameters\\end{tabular}} \n         & Scaling factor $\\gamma$ & 1         & 1         & 1         & 1         & 2         & 1         & 2         \\\\\n         & masking rate            & 0.5       & 0.5       & 0.5       & 0.75      & 0.75      & 0.75      & 0.25      \\\\\n         & replacing rate          & 0.0       & 0.0       & 0.0       & 0.0       & 0.1       & 0.1       & 0.1       \\\\\n         & hidden\\_size            & 512       & 512       & 512       & 512       & 32        & 512       & 512       \\\\\n        %  & num\\_layer              & 2         & 3         & 3         & 2         & 5         & 2         & 2         \\\\\n         & weight\\_decay           & 0         & 0         & 0         & 0         & 0.0       & 0.0       & 0         \\\\\n         & max\\_epoch              & 60        & 50        & 100       & 20        & 20        & 100       & 300       \\\\\n         & batch\\_size             & 32        & 32        & 32        & 32        & 64        & 8         & 16         \\\\\n        %  & Norm                    & BN  & BN & BN & BN & BN & LN & BN \\\\\n         & Pooling                 & mean      & mean      & max       & max       & sum       & max       & sum        \\\\\n        \\bottomrule[1.2pt]\n\\end{tabular}\n\\label{tab:hp_graph}\n\\end{table*}",
            "tab:hp_graph": "\\begin{table*}[htbp]\n\\centering\n\\label{tab:node_sta_hyper}\n\\caption{Statistics and hyper-parameters for graph classification datasets. \n% ``LN'' and ``BN'' represents layer-normalization and batch normalization, respectively.\n}\n\\renewcommand\\arraystretch{1.05}\n\\begin{tabular}{c|c|ccccccc}\n        \\toprule[1.2pt]\n         &  Dataset             & IMDB-B    & IMDB-M    & PROTEINS  & COLLAB    & MUTAG     & REDDIT-B  & NCI1      \\\\\n        \\midrule\n\\multirow{3}{*}{Statistics}             \n\n        & \\# graphs      & 1,000 & 1,500 & 1,113 & 5,000 & 188  & 2,000 & 4,110 \\\\     \n        & \\# classes     &  2    & 3     & 2     & 3     & 2    & 2     & 2    \\\\            \n        & Avg. \\# nodes  & 19.8  & 13.0  & 39.1  & 74.5  & 17.9 & 429.7 & 29.8  \\\\\n        \\midrule\n\\multirow{7}{*}{\\begin{tabular}[c]{@{}c@{}}Hyper-\\\\ parameters\\end{tabular}} \n         & Scaling factor $\\gamma$ & 1         & 1         & 1         & 1         & 2         & 1         & 2         \\\\\n         & masking rate            & 0.5       & 0.5       & 0.5       & 0.75      & 0.75      & 0.75      & 0.25      \\\\\n         & replacing rate          & 0.0       & 0.0       & 0.0       & 0.0       & 0.1       & 0.1       & 0.1       \\\\\n         & hidden\\_size            & 512       & 512       & 512       & 512       & 32        & 512       & 512       \\\\\n        %  & num\\_layer              & 2         & 3         & 3         & 2         & 5         & 2         & 2         \\\\\n         & weight\\_decay           & 0         & 0         & 0         & 0         & 0.0       & 0.0       & 0         \\\\\n         & max\\_epoch              & 60        & 50        & 100       & 20        & 20        & 100       & 300       \\\\\n         & batch\\_size             & 32        & 32        & 32        & 32        & 64        & 8         & 16         \\\\\n        %  & Norm                    & BN  & BN & BN & BN & BN & LN & BN \\\\\n         & Pooling                 & mean      & mean      & max       & max       & sum       & max       & sum        \\\\\n        \\bottomrule[1.2pt]\n\\end{tabular}\n\\label{tab:hp_graph}\n\\end{table*}",
            "tab:stat_mol": "\\begin{table*}[htbp]\n    \\centering\n    % \\renewcommand\\tabcolsep{3.pt}\n    \\renewcommand\\arraystretch{1.05}\n    \\caption{Statistics of datasets for molecular property prediction. ``ZINC'' is used for pre-training.}\n    \\begin{tabular}{@{}c|ccccccccc}  \n        \\toprule[1.2pt]\n                      & ZINC & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE \\\\\n        \\midrule\n        \\# graphs      & 2,000,000 & 2,039 & 7,831 & 8,576 & 1,427 & 1,477 & 93,087 & 41,127 & 1,513\\\\     \n        \\# binary prediction tasks     & - & 1 & 12 & 617 & 27 & 2 & 17 & 1 & 1   \\\\            \n        Avg. \\# nodes  & 26.6 & 24.1 & 18.6 & 18.8 &  33.6 & 26.2 & 24.2 & 24.5 & 34.1 \\\\ \n        \\bottomrule[1.2pt]\n    \\end{tabular}\n    \\label{tab:stat_mol}\n\\end{table*}"
        },
        "figures": {
            "fig:cmp_deg": "\\begin{figure*}\n    \\centering\n    \\subfloat[\\textmd{Technical comparison between generative SSL methods. }]{\n        \\adjustbox{valign=b}{\n        \\small\n        \\renewcommand\\tabcolsep{3pt}\n        \\input{tables/comparsion}\n        \\label{fig:cmp_tab}}}\n    \\ \n    \\\n    \\subfloat[\\textmd{The effect of \\model designs on the performance on Cora dataset. %\\model significantly improves the accuracy with starred operations.\n    }]{           \n        \\includegraphics[width=0.48\\linewidth,valign=b]{imgs/GraphMAEXT.pdf} \n        \\label{fig:design_eff}          \n        }\n    \\caption{Comparison between generative SSL methods and the effect of \\model design. \\textmd{\n    %(a). The comparison between generative SSL methods. \n    %(b). The effect of \\model design choices on .\n    \\textit{AE}: autoencoder methods;\n    \\textit{No Struct.}: no structure reconstruction objective;\n    \\textit{Mask Feat.}: use masking to corrupt input features;\n    \\textit{GNN Decoder}: use GNN as the decoder;\n    \\textit{Re-mask Dec.}: re-mask encoder output before fed into decoder;\n    \\textit{Space}:  run-time memory consumption;\n\\textit{MSE}: Mean Squared Error; \n    \\textit{SCE}: Scaled Cosine Error; \n    \\textit{CE}: Cross-Entropy Error;\n    \\textit{SCE} represents our proposed Scaled Cosine Error.}\n    }\n    \\label{fig:cmp_deg}\n    % \\vspace{-3mm}\n\\end{figure*}",
            "fig:overview": "\\begin{figure*}[htbp]\n    \\centering\n    \\includegraphics[width=\\textwidth]{imgs/overview_sin.pdf}\n    \\caption{\n    Illustration of \\model and the comparison with GAE. \n    \\textmd{We \\underline{underline} the key operations in \\model. \n    During pre-training,  \\model first masks input node features with a mask token [MASK].  The corrupted graph is encoded into code by a GNN encoder. \n    In the decoding, \\model re-masks the code of selected nodes with another token [DMASK], and then employs a GNN, e.g., GAT, GIN, as the decoder. \n    The output of the decoder is used to reconstruct input node features of masked nodes, with the {scaled cosine error} as the criterion.\n    Previous GAEs usually use a single-layer MLP or Laplacian matrix in the decoding and focus more on restoring graph structure.}}\n    \\label{fig:overview}\n\\end{figure*}",
            "fig:ablation": "\\begin{figure}\n    \\centering\n    \\begin{minipage}[t]{0.235\\textwidth}\n        \\includegraphics[width=\\textwidth]{imgs/gmae_mask_ratio.pdf}\n        % \\caption{Mask ratio ablation.}\n    \\end{minipage}\n    \\begin{minipage}[t]{0.235\\textwidth}\n        \\includegraphics[width=\\textwidth]{imgs/gamma_value.pdf}\n        % \\caption{Gamma ablation.}\n    \\end{minipage}\n    \\vspace{-6mm}\n    \\caption{Ablation studies of mask ratio and scaling factor.}\n    \\label{fig:ablation}\n    \\vspace{-4mm}\n\\end{figure}",
            "fig:ppi_hidden": "\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=0.37\\textwidth]{imgs/ppi_hidden.pdf}\n    \\caption{Performance on PPI using GAT with 4 attention heads, compared to other baselines. Self-supervised methods benefit much from larger model size, and \\model could outperform supervised model. }\n    \\label{fig:ppi_hidden}\n\\end{figure}",
            "fig:abl_append": "\\begin{figure}[htbp]\n    \\centering\n    \\begin{minipage}{0.23\\textwidth}\n        \\includegraphics[width=\\textwidth]{imgs/gmae_mask_ratio_append.pdf}\n    \\end{minipage}\n    \\begin{minipage}{0.23\\textwidth}\n        \\includegraphics[width=\\textwidth]{imgs/gamma_value_append.pdf}\n    \\end{minipage}\n    \\caption{Ablation study of mask ratio and scaling factor $\\gamma$ in Ogbn-arxiv and IMDB-B.}\n    \\label{fig:abl_append}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\begin{split}\n        \\mH = f_E(\\mA, \\mX), \\ \\mathcal{G}^{\\prime} = f_D(\\mA, \\mH),\n    \\end{split}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\mathcal{L}_{\\textrm{SCE}} = \\frac{1}{|\\widetilde{\\mathcal{V}}|} \\sum_{v_i \\in \\widetilde{\\mathcal{V}}} (1 - \\frac{\\vx^T_i \\vz_i}{\\| \\vx_i\\| \\cdot \\| \\vz_i\\|})^\\gamma,~ \\gamma \\ge 1,\n    \\label{eq:loss}\n\\end{equation}"
        },
        "git_link": "https://github.com/THUDM/GraphMAE"
    }
}