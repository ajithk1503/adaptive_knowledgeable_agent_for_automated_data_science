{
    "meta_info": {
        "title": "Line Graph Neural Networks for Link Prediction",
        "abstract": "We consider the graph link prediction task, which is a classic graph\nanalytical problem with many real-world applications. With the advances of deep\nlearning, current link prediction methods commonly compute features from\nsubgraphs centered at two neighboring nodes and use the features to predict the\nlabel of the link between these two nodes. In this formalism, a link prediction\nproblem is converted to a graph classification task. In order to extract\nfixed-size features for classification, graph pooling layers are necessary in\nthe deep learning model, thereby incurring information loss. To overcome this\nkey limitation, we propose to seek a radically different and novel path by\nmaking use of the line graphs in graph theory. In particular, each node in a\nline graph corresponds to a unique edge in the original graph. Therefore, link\nprediction problems in the original graph can be equivalently solved as a node\nclassification problem in its corresponding line graph, instead of a graph\nclassification task. Experimental results on fourteen datasets from different\napplications demonstrate that our proposed method consistently outperforms the\nstate-of-the-art methods, while it has fewer parameters and high training\nefficiency.",
        "author": "Lei Cai, Jundong Li, Jie Wang, Shuiwang Ji",
        "link": "http://arxiv.org/abs/2010.10046v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Related Work",
                "content": "\n\nLink prediction models can be grouped into three categories -- heuristic methods, embedding methods, and deep learning methods. \n\n\\textbf{Heuristic Methods:} The key idea of heuristic methods is to compute the similarity score from the neighborhood of two target nodes. Based on the maximum hop of neighbors used in the computation procedure, heuristic methods can be categorized into three groups, including first-order, second-order, and high-order heuristics. Common neighbors and preferential attachment \\cite{barabasi1999emergence} are typical first-order heuristics since only one-hop neighbors are employed to compute the similarity.  Second-order heuristic methods that involve two-hop neighbors include Adamic-Adar \\cite{adamic2003friends} and resource allocation \\cite{barabasi1999emergence, zhou2009predicting}. In addition, high-order heuristics, including Katz \\cite{katz1953new}, rooted PageRank \\cite{brin2012reprint}, and SimRank \\cite{jeh2002simrank} were proposed to compute the similarity score between a pair of nodes using the whole graph. High-order heuristic methods can often achieve better performance than low-order heuristics but require more computation cost. Since many heuristic methods were proposed to handle different graphs, selecting a favorable heuristic method becomes a challenging problem. \n\n\\textbf{Embedding Methods:} The similarity between two target nodes can also be calculated based on node embeddings \\cite{ng2002spectral}. Therefore, embedding methods that can learn the features of nodes from graph topology were also employed to solve the link prediction task, and typical methods along this line include matrix factorization \\cite{koren2009matrix} and stochastic block \\cite{airoldi2008mixed} etc. Recently, inspired by world embedding methods in natural language processing tasks, recent advances such as deepwalk \\cite{perozzi2014deepwalk}, LINE \\cite{tang2015line}, and node2vec \\cite{qiu2018network} were proposed to learn node embedding via the skip-gram method. Deepwalk generates random walks for each vertex with a given length and picks the next visited node uniformly from the neighbors of the current node. Later on, the skip-gram method is employed to learn node embeddings from the generated node sequence. The node embedding methods can learn informative features from the graph and thus achieve satisfactory performance for the link prediction task. However, the performance of link node embedding methods can be affected if the graph becomes very sparse.\n\n\\textbf{Deep Learning:} To overcome the limitations of heuristic methods, deep learning based methods were proposed to learn the distribution of links from the graph automatically \\cite{zhang2017weisfeiler, zhang2018link, cai2020link}. Weisfeiler-Lehman Neural Machine was proposed to predict the existence of a link using a fully-connected neural network based on a fixed-size enclosing subgraph centered on the two target nodes \\cite{zhang2017weisfeiler}. To predict the existence of a link from a general enclosing subgraph, SEAL \\cite{zhang2018link} converts the link prediction task to a graph classification problem and solve it using graph neural networks. Due to the promising learning ability of graph neural networks, the SEAL model achieves the state-of-the-art performance for the link prediction problem. Later on, a multi-scale link model was also proposed to extend SEAL to achieve better performance on plain graphs \\cite{cai2020link}.\n\n"
            },
            "section 2": {
                "name": "The Proposed Methods",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Problem Formulation",
                    "content": "\n\nIn the link prediction task, we are often given a network represented as an undirected graph $G=(V,E)$ which consists of a set of vertices $V=\\{v_1, v_2, ..., v_n\\}$ and a set of links $E \\subseteq V \\times V$. The graph can also be represented by the adjacency matrix $A$. If there exists a link between vertex $i$ and $j$, then $A_{i,j}=1$ and $A_{i,j}=0$ otherwise. The goal of link prediction is to predict potential or missing links that may appear in a foreseeable future. \n\n"
                },
                "subsection 2.2": {
                    "name": "Overall Framework",
                    "content": "\n\nDeep learning based link prediction models were proposed to learn the link distribution from the existing links and determine whether a link exists between two target nodes in the graph. For example, when we predict if there exists a link between two users in a social network, the number of mutual friends is commonly considered as a main criterion. If two users share many mutual friends, they are more likely to be connected. In this sense, if the $1$-hop subgraph induced from two target nodes are densely connected, we will have higher chances to observe a link between them. Considering the variation of networks from different areas, deep learning based methods were proposed to learn the topology feature of subgraphs automatically and predict the existence of links\\cite{zhang2018link}. In particular, the deep learning based link prediction models generally consist of the following three components:\t\n\\begin{enumerate}\n\t\\item Enclosing subgraph extraction: The existence of a potential link can be determined by the topology of a local enclosing subgraph centered on two target nodes. To seek a balance between computation cost and prediction performance, an $h-$hop enclosing subgraph is extracted for learning features and predicting the existence of potential links. \n\t\\item Node labeling: Given an enclosing subgraph, we are required to identify the role of each node in the graph before learning features and predict the existence of the link. That is, we need to identify the target nodes and mark the structural importance of other nodes. A favorable labeling function is of great importance for the further feature learning procedure.\n\t\\item  Feature learning and link prediction: The output of node labeling function can be used as the attribute of each node in the graph. The attribute can indicate the structural importance of the link to be predicted. Graph neural networks are commonly employed to learn features from the given enclosing subgraph, which can be further used to predict the existence of a link.\n\\end{enumerate}\n\nIn this work, we propose line graph neural networks for the link prediction task. Our proposed model can be illustrated in Figure \\ref{fig:pipeline}. Following the general framework of deep learning based link prediction models, we extract an $h$-hop enclosing subgraph centered on two target nodes and assign each node with a label that can represent the structural importance to the target link. The key contribution of our proposed method is the feature learning component. In the previous state-of-the-art model, graph convolution and graph pooling layers are employed to obtain a fixed-size feature vector to predict the existence of the link considering the scale variation of different graphs. Since this graph pooling layer is employed in the state-of-the-art model, only part of graph information can be preserved for further prediction. To overcome the limitations of the SEAL method, we propose to convert the enclosing subgraph to a line graph where each node corresponds to a unique link in the original graph. The feature of the link can be learned directly using the entire input from line graph representation. Thus the proposed method can greatly improve the performance of the link prediction.\n\n"
                },
                "subsection 2.3": {
                    "name": "Line Graph Neural Networks",
                    "content": "\n\n\\textbf{Line Graph Space Transformation} In order to predict the existence of a link, graph neural networks are employed to learn features from a given enclosing subgraph $G_{v_1,v_2}^h$, where $G_{v_1,v_2}^h$ is an $h$-hop enclosing subgraph centered on two target nodes $v_1$ and $v_2$, and each node in the enclosing subgraph is associated with a label that can indicate the structural importance to the target link.  Different enclosing subgraphs commonly contain a different number of nodes. To extract a fixed-size feature vector for the further prediction, we will lose some information during the procedure. To overcome this challenge, we propose to convert the enclosing subgraph to the line graph, which represents the adjacencies between edges of the original graph. Thus, the feature of the link to be predicted can be learned directly in the line graph representation using graph convolution neural networks. \n\nThe line graph $L(G)$ of a given undirected graph $G$ is proposed to represent the adjacencies between edges of $G$ \\cite{harary1960some, chen2018supervised}. The definition of line graph $L(G)$ can be defined as follows. \n\\begin{definition}\n\tThe edges in the original graph $G$ are considered as nodes in the line graph $L(G)$. Two nodes in $L(G)$ are connected if and only if the two corresponding links share the same node. \n\\end{definition}\nAn example of the line graph transformation procedure is illustrated in Figure \\ref{fig:line}. The original undirected graph $G$ contains four nodes and five edges. Therefore, the line graph $L(G)$ contains five nodes. The node $(a-b)$ and $(a-c)$ in the line graph are connected since the corresponding edges in the original graph $G$ share a common node $a$ based on the definition of the line graph. \n\n\n\nBased on the definition of the line graph, we can obtain the following property of $L(G)$: \\emph{Given a graph $G$ with $m$ nodes and $n$ edges, the number of nodes of the line graph $L(G)$ equals to $n$. The number of edges in $L(G)$ is $\\frac{1}{2}\\sum_{i=1}^{m}d_i^2-n$, where $d_i$ is the degree of node $i$ in graph $G$.}\n\nThis property guarantees that learning features in the line graph space will not increase the computation complexity significantly. In additional, converting a graph $G$ to line graph $L(G)$ only costs linear time complexity \\cite{roussopoulos1973max,lehot1974optimal}.\n\n\\textbf{Node Label Transformation} An enclosing subgraph can be converted into the corresponding line graph through transformation. However, this procedure can only transform the topology of a given graph. Each node in the enclosing subgraph also contains a label $l\\in \\mathbb{R}$ generated by labeling function as node attributes that can represent the structural importance. During the transformation procedure, edges in the original graph are represented as nodes in the line graph. The label $l$ is only assigned for nodes in the original graph. To transfer the node label from the original graph directly, a transformation function is required to convert the node label to edge attribute. Thus the edge attribute can be assigned directly as the node attribute in the line graph. In this work, we propose to generate the edge attribute from the node label through the following function:\n\\begin{equation} \\label{eq: concate} \n\tl_{(v_1, v_2)} = \\textrm{concate}(\\min(f_l(v_1),f_l(v_2)), \\max(f_l(v_1),f_l(v_2))),\n\\end{equation}\nwhere $f_l(\\cdot)$ is the node labeling function, $v_1$ and $v_2$ are the two end nodes of the edge, and $\\textrm{concate}(\\cdot)$ represents the concatenation operation for the two inputs. %$l_{(v_1, v_2)}$ is represented as concatenation of two one hot vector for the training of neural network. \nSince we only consider undirected graph link prediction in this work, the attribute of edge $(v1, v2)$ and $(v2, v1)$ should be the same. It is easy to prove that the edge attribute generated by equation (\\ref{eq: concate}) is consistent when switching the end nodes. In addition, the structural importance information of the node can be well preserved in the function.\n\nThe proposed method in equation (\\ref{eq: concate}) can well address the edge attribute transformation in plain graphs. In some cases, graphs are commonly provided with node attributes. For example, in citation networks, the node attribute describing a summary of the paper can be provided in the graph. For attributed graphs, node attributes also play an important role in the link prediction task. Therefore, the edge attribute transformation function should be generalized to deal with attributed graph. Following the edge attribute transformation function in equation (\\ref{eq: concate}), we can concatenate the original node attribute with the node label as the edge attribute. But the edge attribute will not be consistent when we switch the order of two end nodes in the undirected graph. To overcome this limitation, we propose to deal with the original node attribute and node label in different ways by:\n\\begin{equation} \\label{eq: concate_att} \n\tl_{(v_1, v_2)} = \\textrm{concate}(\\min(f_l(v_1),f_l(v_2)), \\max(f_l(v_1),f_l(v_2)), X_{v_1}+X_{v_2}),\n\\end{equation}\nwhere $X_{v_1}$ and $X_{v_2}$ are the original attribute of node $v_1$ and $v_2$.  We propose to combine the node attribute using summation operation, which can guarantee the invariance of edge attributes when switching the end nodes. The generated edge attribute $l_{(v_1, v2)}$ can be used as the node attribute directly in the line graph. Therefore, the link prediction task is converted to a node classification problem which can be solved by graph convolution neural networks.\n\n\\textbf{Feature Learning by Graph Neural Networks} With recent progress in graph neural networks, learning the graph feature becomes a favorable solution that has been explored in various graph analytical tasks \\cite{Yuan2020StructPool:, kipf2016semi, ying2018hierarchical}. In this work, we employ graph convolution neural networks to learn the node embedding in the line graph, which can represent an edge in the original graph. Thus, the node embedding in the line graph can be used to predict whether a potential link is likely to exist in the network.\n\nGiven a line graph representation of the enclosing subgraph $L(G_{v1,v2}^h)$, the node embedding of $(v_i, v_j)$ in the $k$-th layer of the graph convolution neural network is indicated as $Z_{(v_i,  v_j)}^{(k)}$. Then the embedding of $(v_i, v_j)$ in the $(k+1)$-th layer is given by:\n\\begin{equation}\\label{eq:gcn}\n\tZ_{(v_i, v_j)}^{(k+1)} = (Z_{(v_i, v_j)}^{(k)} + \\beta \\sum_{d \\in \\mathcal{N}_{(v_i, v_j)}} Z_d^{(k)})W^{(k)}, \n\\end{equation}\nwhere $\\mathcal{N}_{(v_i, v_j)}$ is the set of neighbors of node $(v_i, v_j)$ in the line graph, $W^{(k)}$ is the weight matrix for the $k$-th layer, $\\beta$ is a normalization coefficient. The input for the first layer of graph convolution neural network is set to node attribute in the line graph as $Z_{(v_i,  v_j)}^{0} = l_{(v1, v2)}$. We then consider the link prediction task as a binary classification problem and train the neural network by minimizing the cross-entropy loss for all potential links as:\n\\begin{equation}\n\t\\mathcal{L}_{CE} = -\\sum_{l \\in L_t} (y_l\\log(p_l) + (1-y_l)\\log(1-p_l)),\n\\end{equation}\nwhere $L_t$ is the set of target links to be predicted, $p_l$ is the probability that the link $l$ exists in the graph, and $y_l \\in \\{0,1\\}$ is the label of a target link that indicating whether the link exists or not.\n\n\\textbf{Connection with Learning on Original Graphs} The key idea of our proposed method is to learn edge features from the enclosing subgraph and predict the existence of edge using the features. In this work, the feature of edge $e=(v_1, v_2)$ is learned based on attributes of two end nodes as:\n\\begin{equation}\n\tf_e = g(f_l(v1), f_l(v2)),\n\\end{equation}\nwhere $f_e$ is the edge feature, $g(\\cdot)$ is the graph neural network function. Although graph convolutional layers are performed on the line graph, it still has connections with the same operation on the original graph. We use the first layer graph convolution layer as an example to illustrate this relationship. We reformulate the equation (\\ref{eq:gcn}) as:\n\\begin{eqnarray} \\label{eq:lgcn}\n\tZ_{(v_i, v_j)}^1 = (l_{(v1, v2)} &+& \\beta \\sum_{d_1 \\in \\mathcal{N}_{v1}} \\sum_{d_2 \\in \\mathcal{N}_{d1}} l_{(d1, d2)} \\nonumber \\\\ \n\t&+& \\beta \\sum_{d_3 \\in \\mathcal{N}_{v2}} \\sum_{d_4 \\in \\mathcal{N}_{d3}} l_{(d3, d4)}) W^{(0)}.\n\\end{eqnarray}\nThe graph convolution operation learns embedding for each node by aggregating node embedding from its $1-$hop neighbors. It can be seen from equation (\\ref{eq:lgcn}) that the graph convolution on the line graph can aggregate the node embedding from $2-$hop neighbors. In the line graph transformation procedure, each node attribute is derived from two corresponding node attributes. That is, the attribute of each node in the line graph contains attributes from two nodes in the original graph. Therefore, aggregating information from $1-hop$ neighbors is equivalent to performing the same operation on $2-$hop neighbors. It also shows that learning node embedding through graph convolution in the line graph is more efficient than that in the original graph in terms of neighbor embedding aggregation. \n\n"
                },
                "subsection 2.4": {
                    "name": "The Proposed Algorithm",
                    "content": "\nIn this section, we provide a detailed description of the three components for our proposed framework. There is no strict restriction for the three steps. The given enclosing subgraph extraction and graph topology labeling function in this section can work well for most networks.\n\n\\textbf{Enclosing Subgraph Extraction} The existence of the link between two nodes can be determined by the graph topology centered on them. In general, we can achieve better performance when more topology information is involved. However, it will incur more computation cost. To seek a balance between performance and computation cost, we predict the existence of the link between node $v_i$ and $v_j$ using  $2-$hop enclosing subgraph as:\n\\begin{equation}\n\tG_{(v_i, v_j)}^2 = \\{v|\\min(d(v, v_i), d(v, v_j) \\leq 2)\\},\n\\end{equation}\nwhere $d(v, v_i)$ is the shortest path$/$geodesic distance between $v$ and $v_i$.\n\n\\textbf{Node Labeling} Given an enclosing subgraph, we only know the topology of the graph. Before we learn features of the target link, we need to identify the role of each node in the graph through a labeling function. The node labeling function must satisfy the following criteria: (1) Identifying the two target nodes. (2) Provide the structural importance of each node to the target nodes. In this work, we employ an effective node labeling function proposed by \\cite{zhang2018link} as:\n\\begin{equation}\\label{eq:label}\n\tf_l(v) = 1+\\min(d(v,v_1), d(v, v_2))+(d_{s}/2)[(d_{s}/2)+(d_{s})\\%2-1],\n\\end{equation}\nwhere $d_{s} = d(v, v_1) + d(v, v_2)$, $(d_s/2)$ and $(d_s\\%2)$ are the integer quotient and remainder of $d$ divided by 2, respectively. In addition, the two target nodes $v_1$ and $v_2$ are assigned with label 1 as $f_l(v_1) = 1$ and $f_l(v_2) = 1$. For any node $v$ satisfying $d(v,v_1)=\\infty$ or $ d(v, v_2)=\\infty$, it will be assigned with label 0 as $f_l(v)=0$. The node labeling function provides a label $f_l(\\cdot) \\in \\mathbb{R}$. In practice, the node label is represented as a one-hot vector. As discussed above, the edge is represented as an order invariant pair. The edge feature pair is represented as a concatenation of two one-hot vectors. Algorithm \\ref{alg: lglp} shows the link prediction procedure using our proposed framework.\n\n\\begin{algorithm}\n\t\\caption{Link Prediction Model Based on Line Graph Neural Networks}  \\label{alg: lglp}\n\t\\begin{algorithmic}[1]\n\t\t\\State \\textbf{Input}: Target link $(v_1, v_2)$, Graph $G$\n\t\t\\State \\textbf{Output}: Prediction result\n\t\t\\State Extract $h-$hop enclosing subgraph $G_{(v_1, v_2)}^h$\n\t\t\\State Apply node labeling function (\\ref{eq:label}) to $G_{(v_1, v_2)}^h$\n\t\t\\State Generate edge attribute using equation (\\ref{eq: concate}) or (\\ref{eq: concate_att}) \n\t\t\\State Transform $G_{(v_1, v_2)}^h$ to line graph $L(G_{(v_1, v_2)}^h)$\n\t\t\\State Apply graph neural networks to extract node embeddings on $L(G_{(v_1, v_2)}^h)$ to predict the existence of link\n\t\\end{algorithmic}\n\\end{algorithm}\n\n"
                }
            },
            "section 3": {
                "name": "Experiments",
                "content": "\n\nIn this section, we evaluate our proposed method on 14 different datasets for the link prediction task. Two evaluation metrics, including area under the curve (AUC) and average precision (AP) are employed in this work to measure the performance of different models. The code and dataset used in this work will be available online after the paper is published.\n\n\n\n",
                "subsection 3.1": {
                    "name": "Datatsets and Baseline Models",
                    "content": "\nIn this work, we perform our proposed line graph link prediction (LGLP) model on 14 different datasets, including BUP, C.ele, HPD, YST, SMG, NSC, KHN, GRQ, LDG, ZWL, USAir, EML, Power, and ADV \\cite{watts1998collective,newman2001structure}. To demonstrate that our proposed method can work well in different areas, 14 datasets are collected from 6 areas. In addition, graphs in different scales, including the number of nodes and links, are used in the experiments. The details of the datasets are shown in Table \\ref{summary}. \n\n\n\n\n\nIn this work, we compare our proposed method with three high-order heuristic methods including Katz \\cite{katz1953new}, PageRank (PR) \\cite{brin2012reprint}, SimRank (SR) \\cite{jeh2002simrank}. In addition, graph embedding method node2vec (N2V) \\cite{grover2016node2vec} and the state-of-the-art method SEAL \\cite{zhang2018link} are selected as baseline methods. \n\n"
                },
                "subsection 3.2": {
                    "name": "Experimental Setup ",
                    "content": "\nTo assess the performance of our proposed method, we randomly select 50\\% of existing links as positive training samples, and the rest are used as positive test samples. In addition, the same number of non-existed links are randomly selected from the graph as negative samples for training and testing. To demonstrate the effectiveness of our proposed method with a different number of training samples, we also select 80\\% training links for the experiments. \n\nThe parameters of baseline methods are tuned to achieve the best performance on datasets. The damping factor in Katz method is set to 0.001. The damping factor in PageRank is set to 0.85. The constant factor in the SimRank is set to 0.8. The dimension of node embedding for node2vec is set to 128. \n\nFor the SEAL framework, we employ the same setting as the original paper \\cite{zhang2018link}. The $2-hop$ enclosing subgraph is extracted for the SEAL framework,  and the labeling function is the same as equation (\\ref{eq:label}) in this work. Three graph convolution layers are employed to compute node embeddings, and the sort pooling \\cite{zhang2018end} is used to generate a fixed-size feature vector for the enclosing subgraph. The output feature map for three graph convolution layers is set to 32. The ratio of the sort pooling layer is set to 0.6. Two 1-D convolution layers with the number of output channels as 16 and 32, and two fully connected layers are employed as a classifier to predict the existence of a link. The SEAL model is trained for 50 epochs on each dataset.\n\nTo guarantee the comparison between our proposed method and SEAL model is fair, we employ the same graph neural network architecture to compute node embeddings in the line graph. It is worth noting that our proposed method does not employ graph pooling and 1-D convolution layers. Therefore, the number of parameters in our proposed method is much fewer than that in the SEAL model. Our proposed method is trained for 15 epochs on each dataset.\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Results and Analysis",
                    "content": "\n\\textbf{Plain Graph Link Prediction} We perform our proposed method and baseline methods on 14 datasets to compare the performance of each model. We randomly split each dataset into training and testing dataset for ten times. The averaged AUC and standard deviations using 80\\% training links are shown in Table \\ref{result801}. The results in terms of AP are shown in Table \\ref{result802}. It can be seen from results that heuristic methods cannot achieve satisfactory performance on all datasets since the heuristic function is manually designed thus cannot handle different cases. We find that the state-of-the-art model SEAL always outperforms all heuristic methods and embedding methods since it can learn the distribution of links automatically from datasets. Our proposed LGLP model can consistently achieve better performance than all baseline methods, including SEAL in terms of two evaluation metrics. It shows that our proposed method can learn better features to represent the target link for prediction in line graph space. In addition, our proposed method is more stable than other baseline methods.\n\nTo demonstrate our proposed method can still achieve satisfactory performance with limited training samples, we conduct experiments on all datasets using 50\\% training links. The averaged AUC and AP are shown in Table \\ref{result501} and Table \\ref{result502}, respectively. It can be seen from the results that our proposed method outperforms all baseline methods significantly on most datasets. We find that our proposed method can still perform well, even using 50\\% training links. The AUC and AP are close to that of using 80\\% training links. \n\n\n\nIn the experiments, we dynamically take 30\\%, 40\\%, 50\\%, 60\\%, 70\\%, and 80\\% of all the links in $G$ as the training set and the rest as the test set, respectively. We conduct experiments with different training percentages and describe the AUC results in Figure \\ref{fig:percent}. The AUC value of our proposed method is marked with a sold line, and other baseline methods are marked with dashed lines in different colors. It can be seen from the results that our proposed method can outperform all baseline methods with different percentages of the training data. In addition, the performance of our proposed method is not sensitive to the number of training samples.\n\n\\textbf{Attributed Graph Link Prediction} We also conduct experiments on attributed graphs. Since the heuristic method can only be applied to plain graphs, we mainly focus on the comparison between our proposed method and SEAL. In the SEAL framework, the attribute is concatenated with the node label as the input for graph neural networks. In this work, we propose a new function to combine the node label and node attributes. We perform the experiment on Cora dataset \\cite{vsubelj2013model} that contains 2,708 nodes and 5,429 links. Each node in the Cora dataset is associated with an attribute vector in 1433 dimensions. We conduct the experiments without node attribute first, and then involve the node attributes to analyze the performance. The results are shown in Table \\ref{tbl: att}. We can find both AUC and AP decrease after using node attributes as input in the SEAL model. In our proposed method, the performance does not change significantly. It shows that our proposed method can work well for both plain and attributed graphs.\n\n\\textbf{Convergence Speed Analysis} Our proposed method can learn features for the target link directly in the line graph. Therefore, only graph convolution layers are required to extract features. In the SEAL model, the procedure is completed in the original graph and thus requires graph convolution and pooling layers to achieve this goal. Compared with the SEAL model, our proposed method contains fewer parameters and converges faster. To analyze the converging speed of two models, we run the models on different datasets and collect the loss and test AUC value for each epoch. The result is shown in Figure \\ref{fig:converage}. The loss and AUC of our proposed method are marked with solid lines. Those of the SEAL model are marked with dashed lines. It can be seen from the results that our proposed model can converge faster than the SEAL. Only 10 to 15 epochs are required to achieve the best performance for our proposed method. It takes 50 epochs for SEAL to converge. Therefore, our proposed method saves training time and requires fewer model parameters.\n\n"
                }
            },
            "section 4": {
                "name": "Conclusion",
                "content": "\n\nIn this work, we propose a novel link prediction model based on line graph neural networks. Graph neural networks have achieved promising performance for the link prediction task. To deal with graphs in different scales, graph pooling layers are employed to extract a fixed-size feature vector in predicting the existence of a link. However, valuable information can be ignored in the pooling operation. In addition, graph neural networks with pooling layers commonly require more training time to converge. To overcome these limitations, we propose to transform the original input graph into line graph and thus the feature of the target link can be learned directly in the line graph without pooling operation. Experimental results on 14 datasets from different areas demonstrate that our proposed method can outperform all baseline methods, including the state-of-the-art models. In addition, our proposed method can converge faster than the state-of-the-art model significantly. \n\n% use section* for acknowledgment\n"
            },
            "section 5": {
                "name": "Acknowledgment",
                "content": "\n\n\nThis work was supported in part by National Science Foundation grant DBI-2028361.\n\n\\bibliography{deep}\n\\bibliographystyle{IEEEtran}\n\n"
            }
        },
        "tables": {
            "summary": "\\begin{table}[t]\n\t\\caption{Summary of datasets used in our experiments. The number of node, link, average node degree, and graph type are provided for each dataset.}\n\t\\label{summary}\n\t\\centering\n\t\\resizebox{0.49\\textwidth}{!}{\n\t\t\\begin{tabular}{lccccccccc}\n\t\t\t\\hline\n\t\t\tName & \\#Nodes & \\#Links & Degree  & Type\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tBUP & 105 & 441 & 8.4 & Political Blogs\\\\\n\t\t\tC.ele & 297 & 2148 & 14.46 & Biology\\\\\n\t\t\tUSAir & 332 & 2126 & 12.81 & Transportation\\\\\n\t\t\tSMG & 1024 & 4916 & 9.6 & Co-authorship\\\\\n\t\t\tEML & 1133 & 5451 & 9.62 & Shared Emails\\\\\n\t\t\tNSC & 1461 & 2742 & 3.75 & Co-authorship\\\\\n\t\t\tYST & 2284 & 6646 & 5.82 & Biology\\\\\n\t\t\tPower & 4941 & 6594 & 2.669& Power Network\\\\\n\t\t\tKHN & 3772 & 12718 & 6.74 & Co-authorship\\\\\n\t\t\tADV & 5155 & 39285 & 15.24 & Social Network\\\\\n\t\t\tGRQ & 5241 & 14484 & 5.53 & Co-authorship\\\\\n\t\t\tLDG & 8324 & 41532 & 9.98 & Co-authorship\\\\\n\t\t\tHPD & 8756 & 32331 & 7.38 & Biology\\\\\n\t\t\tZWL & 6651 & 54182 & 16.29 & Co-authorship\\\\\n\t\t\t\\hline  \n\t\t\\end{tabular}\n\t}\n\\end{table}",
            "result801": "\\begin{table*}[t]\n\t\\caption{AUC comparison with baseline methods (80\\% training links).}\n\t\\label{result801}\n\t\\centering\n\t%\\resizebox{1\\textwidth}{!}{\n\t\\begin{tabular}{lccccccc}\n\t\t\\hline\n\t\tModel & BUP & C.ele & USAir & SMG & EML & NSC & YST\\\\\n\t\t\\hline\n\t\t%CN & $84.46(\\pm3.12)$ & $81.96(\\pm2.08)$ & $92.96(\\pm0.80)$ & $81.30(\\pm0.94)$ & $82.01(\\pm0.68)$ & $97.20(\\pm0.46)$ & $68.50(\\pm0.68)$ \\\\\n\t\t%Jaccard & $83.21(\\pm3.13)$ & $76.43(\\pm2.14)$ & $ 89.43(\\pm0.93)$ & $77.65(\\pm1.14)$ & $81.79(\\pm0.66)$ & $96.98(\\pm0.48)$ & $68.36(\\pm0.68)$\\\\\n\t\t%PA & $65.18(\\pm3.21)$ & $75.92(\\pm2.07)$ & $88.79(\\pm1.38)$ & $83.00(\\pm1.23)$ & $77.89(\\pm0.87)$ & $97.99(\\pm0.24)$ & $77.37(\\pm0.78)$ \\\\\n\t\t%AA & $85.22(\\pm3.09)$ & $83.63(\\pm1.91)$ & $94.08(\\pm0.76)$ & $82.08(\\pm0.92)$ & $82.19(\\pm0.66)$ & $97.27(\\pm0.46)$ & $68.53(\\pm0.68)$ \\\\\n\t\t%RA & $85.39(\\pm2.95)$ & $83.96(\\pm1.75)$ & $94.61(\\pm0.69)$ & $82.05(\\pm0.91)$ & $82.16(\\pm0.65)$ & $97.28(\\pm0.45)$ & $68.52(\\pm0.69)$ \\\\\n\t\tKatz & $87.10(\\pm2.73)$ & $84.84(\\pm2.05)$ & $92.01(\\pm0.88)$ & $86.09(\\pm1.06)$ & $88.45(\\pm0.68)$ & $98.00(\\pm0.31)$ & $80.56(\\pm0.78)$ \\\\\n\t\tPR & $90.13(\\pm2.45)$ & 89.14$(\\pm1.35)$ & $93.74(\\pm1.01)$ & $89.13(\\pm0.90)$ & $89.46(\\pm0.63)$ & $98.05(\\pm0.29)$ & $81.40(\\pm0.75)$ \\\\\n\t\tSR & $85.47(\\pm2.75)$ & $75.65(\\pm2.24)$ & $79.21(\\pm1.50)$ & $78.39(\\pm1.14)$ & $86.90(\\pm0.71)$ & $97.19(\\pm0.48)$ & $73.93(\\pm0.95)$ \\\\\n\t\t%ENS & $85.79(\\pm3.30)$ & $76.09(\\pm2.07)$ & $88.90(\\pm1.37)$ & $83.14(\\pm1.17)$ & $78.09(\\pm0.86)$ & $98.00(\\pm0.24)$ & $77.53(\\pm0.78)$ \\\\\n\t\tN2V & $80.25(\\pm5.55)$ & $80.08(\\pm1.52)$ & $85.40(\\pm0.96)$ & $78.30(\\pm1.22)$ & $83.06(\\pm1.42)$ & $96.23(\\pm0.95)$ & $77.07(\\pm0.36)$ \\\\\n\t\tSEAL & $93.32(\\pm0.84)$ & $87.44(\\pm1.21)$ & $95.21(\\pm0.77)$ & $91.53(\\pm0.46)$ & $92.01(\\pm0.38)$ & $99.55(\\pm0.01)$ &$90.72(\\pm0.25)$ \\\\ \n\t\tLGLP & \\textbf{95.24$(\\pm0.53)$} & \\textbf{90.16$(\\pm0.76$)} & \\textbf{97.44$(\\pm0.32)$} & \\textbf{92.53$(\\pm0.29$)} & \\textbf{92.03$(\\pm0.28)$} & \\textbf{99.82$(\\pm0.01)$} & \\textbf{91.97$(\\pm0.12)$}\\\\\n\t\t\\hline  \n\t\t\\hline\n\t\tModel & Power & KHN & ADV & LDG & HPD & GRQ & ZWL\\\\\n\t\t\\hline\n\t\t%CN & $57.32(\\pm0.39)$ & $77.83(\\pm0.46)$ & $88.39(\\pm0.21)$ & $86.40(\\pm0.19)$ & $71.49(\\pm0.41)$ & $89.56(\\pm0.44)$ & $91.93(\\pm0.16)$ \\\\\n\t\t%Jaccard & $57.32(\\pm0.39)$ & $76.19(\\pm0.51)$ & $ 86.87(\\pm0.17)$ & $85.33(\\pm0.20)$ & $71.35(\\pm0.41)$ & $89.57(\\pm0.44)$ & $91.66(\\pm0.18)$\\\\\n\t\t%PA & $45.09(\\pm1.03)$ & $81.18(\\pm0.83)$ & $89.44(\\pm0.23)$ & $84.27(\\pm0.16)$ & $81.98(\\pm0.31)$ & $73.95(\\pm0.38)$ & $82.11(\\pm0.25)$ \\\\\n\t\t%AA & $57.32(\\pm0.39)$ & $78.38(\\pm0.44)$ & $88.71(\\pm0.22)$ & $86.69(\\pm0.18)$ & $71.55(\\pm0.41)$ & $89.59(\\pm0.43)$ & $92.08(\\pm0.17)$ \\\\\n\t\t%RA & $57.32(\\pm0.39)$ & $78.38(\\pm0.44)$ & $88.71(\\pm0.21)$ & $86.70(\\pm0.18)$ & $71.54(\\pm0.41)$ & $89.59(\\pm0.43)$ & $92.08(\\pm0.16)$ \\\\\n\t\tKatz & $59.59(\\pm1.51)$ & $84.60(\\pm0.79)$ & $92.13(\\pm0.21)$ & $92.96(\\pm0.19)$ & $85.47(\\pm0.35)$ & $89.81(\\pm0.59)$ & $96.42(\\pm0.12)$ \\\\\n\t\tPR & $59.88(\\pm1.51)$ & $88.43(\\pm0.80)$ & $92.78(\\pm0.18)$ & $94.46(\\pm0.19)$ & $87.19(\\pm0.34)$ & $89.98(\\pm0.57)$ & $97.20(\\pm0.12)$ \\\\\n\t\tSR & $70.18(\\pm0.75)$ & $79.55(\\pm0.90)$ & $86.18(\\pm0.22)$ & $90.95(\\pm0.14)$ & $81.73(\\pm0.37)$ & $89.81(\\pm0.58)$ & $95.97(\\pm0.16)$ \\\\\n\t\t%ENS & $77.14(\\pm1.36)$ & $81.44(\\pm0.83)$ & $89.47(\\pm0.23)$ & $84.43(\\pm0.17)$ & $82.14(\\pm0.31)$ & $74.65(\\pm0.39)$ & $82.22(\\pm0.25)$ \\\\\n\t\tN2V & $70.37(\\pm1.15)$ & $82.21(\\pm1.19)$ & $77.70(\\pm0.83)$ & $91.88(\\pm0.56)$ & $79.61(\\pm1.14)$ & $91.33(\\pm0.53)$ & $94.38(\\pm0.51)$ \\\\\n\t\tSEAL & $81.37(\\pm0.93)$ & $92.69(\\pm0.14)$ & $95.07(\\pm0.13)$ & $96.44(\\pm0.13)$ & $92.26(\\pm0.09)$ & $97.10(\\pm0.12)$ & $97.46(\\pm0.02)$ \\\\ \n\t\tLGLP & \\textbf{82.17$(\\pm0.57)$} & \\textbf{93.30$(\\pm0.09)$} & \\textbf{95.40$(\\pm0.10)$} & \\textbf{96.70$(\\pm0.07)$} & \\textbf{92.58$(\\pm0.08)$} & \\textbf{97.68$(\\pm0.10)$} & \\textbf{97.76$(\\pm0.01)$}\\\\\n\t\t\\hline  \n\t\\end{tabular}\n\t%}\n\\end{table*}",
            "result802": "\\begin{table*}[t]\n\t\\caption{AP comparison with baseline methods (80\\% training links).}\n\t\\label{result802}\n\t\\centering\n\t%\\resizebox{1\\textwidth}{!}{\n\t\\begin{tabular}{lccccccc}\n\t\t\\hline\n\t\tModel & BUP & C.ele & USAir & SMG & EML & NSC & YST\\\\\n\t\t\\hline\n\t\t%CN & $81.68(\\pm3.44)$ & $79.20(\\pm2.51)$ & $92.56(\\pm0.82)$ & $79.62(\\pm0.84)$ & $81.27(\\pm0.51)$ & $97.12(\\pm0.62)$ & $68.42(\\pm0.82)$ \\\\\n\t\t%Jaccard & $80.55(\\pm3.65)$ & $72.10(\\pm2.11)$ & $ 86.96(\\pm1.50)$ & $69.57(\\pm1.67)$ & $81.16(\\pm0.69)$ & $96.87(\\pm0.56)$ & $67.78(\\pm1.01)$\\\\\n\t\t%PA & $66.88(\\pm4.27)$ & $76.27(\\pm1.93)$ & $91.16(\\pm1.12)$ & $84.52(\\pm1.06)$ & $78.28(\\pm0.76)$ & $97.89(\\pm0.45)$ & $80.79(\\pm0.53)$ \\\\\n\t\t%AA & $84.33(\\pm3.36)$ & $82.96(\\pm2.52)$ & $94.47(\\pm0.73)$ & $82.47(\\pm0.68)$ & $82.26(\\pm0.50)$ & $97.24(\\pm0.60)$ & $68.83(\\pm0.78)$ \\\\\n\t\t%RA & $84.55(\\pm3.02)$ & $83.32(\\pm2.28)$ & $95.16(\\pm0.57)$ & $82.42(\\pm0.69)$ & $82.20(\\pm0.51)$ & $97.27(\\pm0.58)$ & $68.77(\\pm0.77)$ \\\\\n\t\tKatz & $85.94(\\pm3.46)$ & $85.94(\\pm3.46)$ & $93.51(\\pm0.79)$ & $87.68(\\pm0.90)$ & $90.54(\\pm0.53)$ & $98.02(\\pm0.43)$ & $85.76(\\pm0.64)$ \\\\\n\t\tPR & $89.53(\\pm3.11)$ & 87.96$(\\pm1.69)$ & $94.30(\\pm1.27)$ & $91.07(\\pm0.59)$ & $91.01(\\pm0.67)$ & $98.08(\\pm0.34)$ & $86.34(\\pm0.72)$ \\\\\n\t\tSR & $81.10(\\pm3.31)$ & $66.43(\\pm2.39)$ & $69.80(\\pm1.99)$ & $70.39(\\pm1.67)$ & $87.24(\\pm0.84)$ & $96.55(\\pm1.14)$ & $77.56(\\pm1.09)$ \\\\\n\t\t%ENS & $85.78(\\pm3.35)$ & $76.48(\\pm1.94)$ & $91.27(\\pm1.12)$ & $84.72(\\pm1.05)$ & $78.55(\\pm0.76)$ & $97.90(\\pm0.45)$ & $81.09(\\pm0.53)$ \\\\\n\t\tN2V & $81.47(\\pm4.48)$ & $77.98(\\pm1.54)$ & $82.53(\\pm1.12)$ & $77.01(\\pm1.79)$ & $83.08(\\pm1.36)$ & $96.81(\\pm0.86)$ & $78.48(\\pm1.03)$ \\\\\n\t\tSEAL & $93.58(\\pm0.68)$ & $86.49(\\pm1.08)$ & $95.46(\\pm0.59)$ & $91.90(\\pm0.31)$ & $91.93(\\pm0.31)$ & $99.51(\\pm0.01)$ &$91.85(\\pm0.20)$ \\\\ \n\t\tLGLP & \\textbf{95.46$(\\pm0.43)$} & \\textbf{89.70$(\\pm0.53)$} & \\textbf{97.37$(\\pm0.25)$} & \\textbf{92.92$(\\pm0.21)$} & \\textbf{92.61$(\\pm0.23)$} & \\textbf{99.82$(\\pm0.01)$} & \\textbf{92.98$(\\pm0.10)$}\\\\\n\t\t\\hline  \n\t\t\\hline\n\t\tModel & Power & KHN & ADV & LDG & HPD & GRQ & ZWL\\\\\n\t\t\\hline\n\t\t%CN & $57.42(\\pm0.44)$ & $77.13(\\pm0.51)$ & $88.07(\\pm0.19)$ & $86.08(\\pm0.39)$ & $71.32(\\pm0.42)$ & $89.43(\\pm0.30)$ & $91.65(\\pm0.11)$ \\\\\n\t\t%Jaccard & $57.36(\\pm0.41)$ & $71.48(\\pm0.74)$ & $ 85.28(\\pm0.28)$ & $82.71(\\pm0.73)$ & $70.55(\\pm0.45)$ & $89.46(\\pm0.30)$ & $91.33(\\pm0.15)$\\\\\n\t\t%PA & $52.05(\\pm0.85)$ & $84.51(\\pm0.34)$ & $90.53(\\pm0.26)$ & $86.11(\\pm0.27)$ & $85.26(\\pm0.43)$ & $79.46(\\pm0.38)$ & $82.37(\\pm0.15)$ \\\\\n\t\t%AA & $57.43(\\pm0.42)$ & $79.23(\\pm0.43)$ & $89.11(\\pm0.18)$ & $86.99(\\pm0.33)$ & $71.69(\\pm0.41)$ & $89.51(\\pm0.32)$ & $92.12(\\pm0.11)$ \\\\\n\t\t%RA & $57.42(\\pm0.42)$ & $79.24(\\pm0.41)$ & $89.10(\\pm0.19)$ & $86.99(\\pm0.32)$ & $71.65(\\pm0.41)$ & $89.51(\\pm0.32)$ & $92.15(\\pm0.11)$ \\\\\n\t\tKatz & $74.29(\\pm0.83)$ & $88.27(\\pm0.32)$ & $93.72(\\pm0.16)$ & $94.91(\\pm0.27)$ & $89.52(\\pm0.32)$ & $93.08(\\pm0.29)$ & $97.08(\\pm0.09)$ \\\\\n\t\tPR & $74.74(\\pm0.81)$ & $92.17(\\pm0.24)$ & $94.03(\\pm0.24)$ & $96.26(\\pm0.22)$ & $91.01(\\pm0.23)$ & $93.18(\\pm0.34)$ & $97.69(\\pm0.08)$ \\\\\n\t\tSR & $70.69(\\pm0.67)$ & $77.16(\\pm0.81)$ & $83.31(\\pm0.35)$ & $88.71(\\pm0.79)$ & $84.16(\\pm0.42)$ & $92.97(\\pm0.31)$ & $95.44(\\pm0.15)$ \\\\\n\t\t%ENS & $80.09(\\pm1.17)$ & $84.83(\\pm0.33)$ & $90.57(\\pm0.26)$ & $86.29(\\pm0.27)$ & $85.48(\\pm0.42)$ & $80.12(\\pm0.38)$ & $82.49(\\pm0.15)$ \\\\\n\t\tN2V & $76.55(\\pm0.75)$ & $83.26(\\pm0.79)$ & $79.02(\\pm0.65)$ & $92.12(\\pm0.50)$ & $80.57(\\pm0.81)$ & $93.92(\\pm0.31)$ & $93.82(\\pm0.39)$ \\\\\n\t\tSEAL & $83.91(\\pm0.83)$ & $93.40(\\pm0.13)$ & $95.18(\\pm0.12)$ & $96.55(\\pm0.11)$ & $93.41(\\pm0.09)$ & $97.86(\\pm0.11)$ & $97.54(\\pm0.02)$ \\\\ \n\t\tLGLP & \\textbf{84.78$(\\pm0.53)$} & \\textbf{94.14$(\\pm0.09)$} & \\textbf{95.72$(\\pm0.08)$} & \\textbf{96.86$(\\pm0.06)$} & \\textbf{93.65$(\\pm0.08)$} & \\textbf{98.14$(\\pm0.10)$} & \\textbf{97.91$(\\pm0.01)$}\\\\\n\t\t\\hline  \n\t\\end{tabular}\n\t%}\n\\end{table*}",
            "result501": "\\begin{table*}[t]\n\t\\caption{AUC comparison with baseline methods (50\\% training links).}\n\t\\label{result501}\n\t\\centering\n\t%\\resizebox{1\\textwidth}{!}{\n\t\\begin{tabular}{lccccccc}\n\t\t\\hline\n\t\tModel & BUP & C.ele & USAir & SMG & EML & NSC & YST\\\\\n\t\t\\hline\n\t\t%CN & $73.62(\\pm2.06)$ & $72.29(\\pm0.82)$ & $87.93(\\pm0.43)$ & $70.26(\\pm0.54)$ & $70.80(\\pm0.44)$ & $91.87(\\pm0.68)$ & $61.37(\\pm0.29)$ \\\\\n\t\t%Jaccard & $72.93(\\pm2.06)$ & $69.75(\\pm0.86)$ & $ 84.82(\\pm0.52)$ & $69.06(\\pm0.66)$ & $70.74(\\pm0.45)$ & $91.73(\\pm0.72)$ & $61.33(\\pm0.30)$\\\\\n\t\t%PA & $64.21(\\pm2.37)$ & $73.81(\\pm0.97)$ & $87.59(\\pm0.50)$ & $80.87(\\pm0.47)$ & $76.53(\\pm0.27)$ & $96.31(\\pm0.58)$ & $75.92(\\pm0.40)$ \\\\\n\t\t%AA & $74.08(\\pm2.00)$ & $73.37(\\pm0.80)$ & $88.61(\\pm0.40)$ & $70.68(\\pm0.49)$ & $70.87(\\pm0.43)$ & $91.91(\\pm0.68)$ & $61.38(\\pm0.29)$ \\\\\n\t\t%RA & $74.12(\\pm1.97)$ & $73.42(\\pm0.82)$ & $88.73(\\pm0.39)$ & $70.67(\\pm0.49)$ & $70.87(\\pm0.43)$ & $91.92(\\pm0.68)$ & $61.38(\\pm0.29)$ \\\\\n\t\tKatz & $81.61(\\pm3.40)$ & $79.99(\\pm0.59)$ & $88.91(\\pm0.39)$ & $80.65(\\pm0.58)$ & $84.16(\\pm0.64)$ & $95.99(\\pm0.62)$ & $77.28(\\pm0.37)$ \\\\\n\t\tPR & $84.07(\\pm3.39)$ & \\textbf{84.95$(\\pm0.58)$} & $90.57(\\pm0.39)$ & $84.59(\\pm0.45)$ & $85.43(\\pm0.63)$ & $96.06(\\pm0.60)$ & $77.90(\\pm3.69)$ \\\\\n\t\tSR & $80.98(\\pm3.03)$ & $76.05(\\pm0.80)$ & $81.09(\\pm0.59)$ & $75.28(\\pm0.74)$ & $83.05(\\pm0.64)$ & $95.59(\\pm0.68)$ & $73.71(\\pm0.41)$ \\\\\n\t\t%ENS & $76.52(\\pm7.51)$ & $74.11(\\pm0.96)$ & $87.81(\\pm0.50)$ & $81.06(\\pm0.47)$ & $76.84(\\pm0.28)$ & $96.32(\\pm0.58)$ & $76.08(\\pm0.39)$ \\\\\n\t\tN2V & $80.94(\\pm2.65)$ & $75.53(\\pm1.23)$ & $84.63(\\pm1.58)$ & $73.50(\\pm1.22)$ & $80.15(\\pm1.26)$ & $94.20(\\pm1.25)$ & $73.62(\\pm0.74)$ \\\\\n\t\tSEAL & $85.10(\\pm0.82)$ & $81.23(\\pm1.52)$ & $93.23(\\pm1.46)$ & $86.56(\\pm0.53)$ & $85.83(\\pm0.46)$ & $99.07(\\pm0.02)$ &$85.56(\\pm0.28)$ \\\\ \n\t\tLGLP & \\textbf{88.57$(\\pm0.52)$} & 84.60$(\\pm0.82)$ & \\textbf{95.18$(\\pm0.33)$} & \\textbf{89.54$(\\pm0.36)$} & \\textbf{86.77$(\\pm0.26)$} & \\textbf{99.33$(\\pm0.01)$} & \\textbf{87.63$(\\pm0.15)$}\\\\\n\t\t\\hline  \n\t\t\\hline\n\t\tModel & Power & KHN & ADV & LDG & HPD & GRQ & ZWL\\\\\n\t\t\\hline\n\t\t%CN & $53.58(\\pm0.22)$ & $66.40(\\pm0.25)$ & $79.70(\\pm0.20)$ & $74.23(\\pm0.19)$ & $62.76(\\pm0.11)$ & $78.52(\\pm0.19)$ & $81.80(\\pm0.01)$ \\\\\n\t\t%Jaccard & $53.38(\\pm0.22)$ & $66.02(\\pm0.25)$ & $ 79.02(\\pm0.20)$ & $73.92(\\pm0.20)$ & $62.71(\\pm0.11)$ & $78.52(\\pm0.18)$ & $81.66(\\pm0.01)$\\\\\n\t\t%PA & $46.79(\\pm0.69)$ & $78.04(\\pm0.46)$ & $88.61(\\pm0.15)$ & $82.24(\\pm0.16)$ & $80.42(\\pm0.10)$ & $73.00(\\pm0.28)$ & $81.17(\\pm0.14)$ \\\\\n\t\t%AA & $53.38(\\pm0.22)$ & $66.60(\\pm0.25)$ & $79.90(\\pm0.19)$ & $74.37(\\pm0.18)$ & $62.77(\\pm0.11)$ & $78.53(\\pm0.18)$ & $81.89(\\pm0.08)$ \\\\\n\t\t%RA & $53.38(\\pm0.22)$ & $66.60(\\pm0.25)$ & $79.89(\\pm0.19)$ & $74.37(\\pm0.18)$ & $62.77(\\pm0.11)$ & $78.53(\\pm0.18)$ & $81.89(\\pm0.08)$ \\\\\n\t\tKatz & $57.34(\\pm0.51)$ & $78.99(\\pm0.20)$ & $90.04(\\pm0.17)$ & $88.61(\\pm0.19)$ & $81.60(\\pm0.12)$ & $82.50(\\pm0.21)$ & $93.72(\\pm0.06)$ \\\\\n\t\tPR & $57.34(\\pm0.52)$ & $82.34(\\pm0.21)$ & $90.97(\\pm0.15)$ & $90.50(\\pm0.19)$ & $83.15(\\pm0.17)$ & $82.64(\\pm0.22)$ & 95.11$(\\pm0.09)$ \\\\\n\t\tSR & $56.16(\\pm0.45)$ & $75.87(\\pm0.19)$ & $84.87(\\pm0.14)$ & $87.95(\\pm0.14)$ & $78.88(\\pm0.22)$ & $82.68(\\pm0.24)$ & $94.00(\\pm0.10)$ \\\\\n\t\t%ENS & $62.70(\\pm0.95)$ & $78.16(\\pm0.46)$ & $88.66(\\pm0.16)$ & $82.50(\\pm0.17)$ & $80.58(\\pm0.10)$ & $73.60(\\pm0.27)$ & $81.37(\\pm0.13)$ \\\\\n\t\tN2V & $55.40(\\pm0.84)$ & $78.53(\\pm0.72)$ & $74.67(\\pm0.98)$ & $88.82(\\pm0.44)$ & $75.84(\\pm1.03)$ & $84.24(\\pm0.35)$ & $92.06(\\pm0.61)$ \\\\\n\t\tSEAL & $65.80(\\pm1.10)$ & $87.43(\\pm0.17)$ & $92.75(\\pm0.14)$ & $92.98(\\pm0.16)$ & $88.05(\\pm0.10)$ & $90.07(\\pm0.15)$ & $94.94(\\pm0.02)$ \\\\ \n\t\tLGLP & \\textbf{66.94$(\\pm0.60)$} & \\textbf{88.88$(\\pm0.13)$} & \\textbf{93.28$(\\pm0.10)$} & \\textbf{93.43$(\\pm0.11)$} & \\textbf{88.65$(\\pm0.09)$} & \\textbf{91.31$(\\pm0.11)$} & \\textbf{95.51$(\\pm0.01)$} \\\\\n\t\t\\hline  \n\t\\end{tabular}\n\t%}\n\\end{table*}",
            "result502": "\\begin{table*}[t]\n\t\\caption{AP comparison with baseline methods (50\\% training links).}\n\t\\label{result502}\n\t\\centering\n\t%\\resizebox{1\\textwidth}{!}{\n\t\\begin{tabular}{lccccccc}\n\t\t\\hline\n\t\tModel & BUP & C.ele & USAir & SMG & EML & NSC & YST\\\\\n\t\t\\hline\n\t\t%CN & $81.68(\\pm2.27)$ & $79.20(\\pm0.74)$ & $92.56(\\pm0.44)$ & $79.62(\\pm0.62)$ & $81.27(\\pm0.54)$ & $91.12(\\pm0.41)$ & $61.37(\\pm0.30)$ \\\\\n\t\t%Jaccard & $80.55(\\pm2.95)$ & $72.10(\\pm1.02)$ & $ 86.96(\\pm1.25)$ & $69.56(\\pm0.79)$ & $81.15(\\pm0.46)$ & $96.87(\\pm0.61)$ & $60.91(\\pm0.31)$\\\\\n\t\t%PA & $66.88(\\pm2.66)$ & $76.26(\\pm0.64)$ & $91.15(\\pm0.45)$ & $84.52(\\pm0.59)$ & $78.28(\\pm0.48)$ & $97.89(\\pm0.46)$ & $78.41(\\pm0.78)$ \\\\\n\t\t%AA & $84.33(\\pm2.06)$ & $82.96(\\pm0.76)$ & $94.47(\\pm0.39)$ & $82.47(\\pm0.59)$ & $82.26(\\pm0.55)$ & $97.24(\\pm0.40)$ & $61.60(\\pm0.29)$ \\\\\n\t\t%RA & $84.55(\\pm1.97)$ & $83.32(\\pm0.96)$ & $95.16(\\pm0.35)$ & $82.42(\\pm0.60)$ & $82.20(\\pm0.54)$ & $97.27(\\pm0.40)$ & $61.55(\\pm0.30)$ \\\\\n\t\tKatz & $85.94(\\pm2.03)$ & $83.99(\\pm0.79)$ & $93.51(\\pm0.35)$ & $87.68(\\pm0.79)$ & $80.54(\\pm0.31)$ & $98.02(\\pm0.53)$ & $81.63(\\pm0.41)$ \\\\\n\t\tPR & $89.53(\\pm2.58)$ & \\textbf{87.96$(\\pm0.86)$} & $94.30(\\pm0.49)$ & $91.07(\\pm0.69)$ & $91.01(\\pm0.52)$ & $98.08(\\pm0.59)$ & $82.08(\\pm0.46)$ \\\\\n\t\tSR & $81.09(\\pm2.57)$ & $66.43(\\pm1.17)$ & $69.78(\\pm0.84)$ & $70.39(\\pm0.96)$ & $87.24(\\pm0.52)$ & $96.55(\\pm0.75)$ & $76.02(\\pm0.49)$ \\\\\n\t\t%ENS & $85.78(\\pm7.35)$ & $76.48(\\pm0.64)$ & $91.27(\\pm0.45)$ & $84.72(\\pm0.61)$ & $78.55(\\pm0.48)$ & $97.90(\\pm0.46)$ & $78.82(\\pm0.78)$ \\\\\n\t\tN2V & $76.05(\\pm3.20)$ & $73.37(\\pm1.23)$ & $81.03(\\pm1.18)$ & $73.32(\\pm1.34)$ & $81.12(\\pm0.92)$ & $95.32(\\pm1.08)$ & $76.61(\\pm0.94)$ \\\\\n\t\tSEAL & $84.17(\\pm0.62)$ & $83.94(\\pm1.31)$ & $94.31(\\pm1.13)$ & $86.76(\\pm0.41)$ & $87.45(\\pm0.41)$ & $99.09(\\pm0.02)$ &$86.45(\\pm0.25)$ \\\\ \n\t\tLGLP & \\textbf{89.03$(\\pm0.41)$} & 84.80$(\\pm0.63)$ & \\textbf{94.89$(\\pm0.33)$} & \\textbf{90.23$(\\pm0.26)$} & \\textbf{88.49$(\\pm0.23)$} & \\textbf{99.38$(\\pm0.01)$} & \\textbf{89.22$(\\pm0.13)$}\\\\\n\t\t\\hline  \n\t\t\\hline\n\t\tModel & Power & KHN & ADV & LDG & HPD & GRQ & ZWL\\\\\n\t\t\\hline\n\t\t%CN & $53.36(\\pm0.22)$ & $65.88(\\pm0.30)$ & $79.39(\\pm0.15)$ & $73.95(\\pm0.21)$ & $62.64(\\pm0.11)$ & $78.51(\\pm0.18)$ & $81.54(\\pm0.01)$ \\\\\n\t\t%Jaccard & $53.34(\\pm0.23)$ & $63.17(\\pm0.47)$ & $ 77.19(\\pm0.16)$ & $72.04(\\pm0.30)$ & $62.17(\\pm0.12)$ & $78.50(\\pm0.18)$ & $81.15(\\pm0.10)$\\\\\n\t\t%PA & $51.44(\\pm0.58)$ & $81.96(\\pm0.21)$ & $89.92(\\pm0.16)$ & $84.56(\\pm0.13)$ & $83.63(\\pm0.12)$ & $77.63(\\pm0.25)$ & $81.67(\\pm0.13)$ \\\\\n\t\t%AA & $53.37(\\pm0.22)$ & $67.17(\\pm0.22)$ & $80.30(\\pm0.12)$ & $74.59(\\pm0.17)$ & $62.82(\\pm0.11)$ & $78.55(\\pm0.18)$ & $81.96(\\pm0.07)$ \\\\\n\t\t%RA & $53.37(\\pm0.22)$ & $67.17(\\pm0.22)$ & $80.26(\\pm0.11)$ & $74.59(\\pm0.16)$ & $62.80(\\pm0.11)$ & $78.55(\\pm0.18)$ & $81.94(\\pm0.07)$ \\\\\n\t\tKatz & $57.63(\\pm0.51)$ & $83.04(\\pm0.38)$ & $91.76(\\pm0.15)$ & $91.57(\\pm0.17)$ & $85.73(\\pm0.89)$ & $86.59(\\pm0.20)$ & $95.12(\\pm0.05)$ \\\\\n\t\tPR & $57.61(\\pm0.56)$ & $87.18(\\pm0.26)$ & $92.43(\\pm0.17)$ & $93.53(\\pm0.14)$ & $87.20(\\pm0.15)$ & $86.73(\\pm0.20)$ & 96.24$(\\pm0.05)$ \\\\\n\t\tSR & $56.19(\\pm0.49)$ & $75.87(\\pm0.66)$ & $83.22(\\pm0.20)$ & $88.11(\\pm0.25)$ & $81.07(\\pm0.18)$ & $86.27(\\pm0.20)$ & $94.26(\\pm0.11)$ \\\\\n\t\t%ENS & $61.80(\\pm0.71)$ & $82.49(\\pm0.23)$ & $89.98(\\pm0.16)$ & $84.91(\\pm0.13)$ & $83.96(\\pm0.12)$ & $78.44(\\pm0.24)$ & $81.90(\\pm0.13)$ \\\\\n\t\tN2V & $60.46(\\pm0.86)$ & $80.60(\\pm0.74)$ & $76.70(\\pm0.82)$ & $89.57(\\pm0.64)$ & $77.66(\\pm0.54)$ & $88.70(\\pm0.26)$ & $91.61(\\pm0.49)$ \\\\\n\t\tSEAL & $68.67(\\pm0.98)$ & $90.37(\\pm0.16)$ & $93.52(\\pm0.13)$ & $94.33(\\pm0.15)$ & $90.25(\\pm0.10)$ & $92.80(\\pm0.12)$ & $95.88(\\pm0.02)$ \\\\ \n\t\tLGLP & \\textbf{69.41$(\\pm0.50)$} & \\textbf{90.83$(\\pm0.11)$} & \\textbf{93.82$(\\pm0.10)$} & \\textbf{94.63$(\\pm0.10)$} & \\textbf{90.34$(\\pm0.09)$} & \\textbf{93.01$(\\pm0.10)$} & \\textbf{96.19$(\\pm0.01)$} \\\\\n\t\t\\hline  \n\t\\end{tabular}\n\t%}\n\\end{table*}",
            "tbl: att": "\\begin{table}[t]\n\t\\caption{Comparison on Cora dataset using plain graph and attributed graph (50\\% training links).}\n\t\\centering\n\t\\label{tbl: att}\n\t\\resizebox{0.37\\textwidth}{!}{\n\t\t\\begin{tabular}{lcccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{} & \\multicolumn{2}{c}{Attribute} & \\multicolumn{2}{c}{Plain} \\\\ \n\t\t\t& AUC            & AP            & AUC              & AP              \\\\ \\hline \\hline\n\t\t\tSEAL              & 75.33          & 77.69          & 79.95            & 82.91           \\\\ \n\t\t\tLGLP              & \\textbf{81.45}           & \\textbf{81.99}          & \\textbf{79.96}             & \\textbf{83.30}            \\\\ \\hline\n\t\t\\end{tabular}\n\t}\n\\end{table}"
        },
        "figures": {
            "fig:pipeline": "\\begin{figure*}[t]\n\t\\center\n\t\\includegraphics[width=\\textwidth]{figure/pipeline.pdf}\n\t\\caption{Illustration of our proposed model based on line graph\n\t\tneural networks. The two target nodes in the graph are marked with\n\t\tdouble circles. To predict the existence of the link, an $h$-hop\n\t\tenclosing subgraph centered on two target nodes is extracted. A node\n\t\tlabeling function is employed to assign the label for each node to\n\t\trepresent the structural importance to the target link. To learn the\n\t\tfeature of the target link, we transform the enclosing subgraph into\n\t\ta corresponding line graph. The graph convolution networks are used\n\t\tto learn the feature that is employed to predict the existence of\n\t\tlink.} \\label{fig:pipeline}\n\\end{figure*}",
            "fig:line": "\\begin{figure}[t]\n\t\\center\n\t\\includegraphics[width=\\columnwidth]{figure/line.pdf}\n\t\\caption{Illustration of the line graph transformation procedure. Each node in the line graph corresponds to a unique edge in the original graph and is marked with the name of two end nodes.\n\t} \\label{fig:line}\n\\end{figure}",
            "fig:converage": "\\begin{figure*}[t]\n\t\\center\n\t\\includegraphics[width=\\textwidth]{figure/all_train.pdf}\n\t\\caption{Training loss and testing AUC comparison between our proposed LGLP and SEAL method. The training loss and testing AUC on BUP, C.ele, EML, and SMG dataset. The training loss and testing AUC of LGLP are marked with blue and orange solid lines. Those of SEAL are marked with blue, orange dashed lines.\n\t} \\label{fig:converage}\n\\end{figure*}",
            "fig:percent": "\\begin{figure*}[t]\n\t\\center\n\t\\includegraphics[width=\\textwidth]{figure/percent2.pdf}\n\t\\caption{AUC comparison on all datasets for Katz, PR, SR, SEAL, and LGLP using different percent of training links. On each dataset, we take 30\\%, 40\\%, 50\\%, 60\\%, 70\\%, and 80\\% of all the links in $G$ as the training set. Our proposed method LGLP is marked with solid line and all baseline methods are marked with dashed lines in different colors.\n\t} \\label{fig:percent}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation} \\label{eq: concate} \n\tl_{(v_1, v_2)} = \\textrm{concate}(\\min(f_l(v_1),f_l(v_2)), \\max(f_l(v_1),f_l(v_2))),\n\\end{equation}",
            "eq:2": "\\begin{equation} \\label{eq: concate_att} \n\tl_{(v_1, v_2)} = \\textrm{concate}(\\min(f_l(v_1),f_l(v_2)), \\max(f_l(v_1),f_l(v_2)), X_{v_1}+X_{v_2}),\n\\end{equation}",
            "eq:eq:gcn": "\\begin{equation}\\label{eq:gcn}\n\tZ_{(v_i, v_j)}^{(k+1)} = (Z_{(v_i, v_j)}^{(k)} + \\beta \\sum_{d \\in \\mathcal{N}_{(v_i, v_j)}} Z_d^{(k)})W^{(k)}, \n\\end{equation}",
            "eq:3": "\\begin{equation}\n\t\\mathcal{L}_{CE} = -\\sum_{l \\in L_t} (y_l\\log(p_l) + (1-y_l)\\log(1-p_l)),\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\tf_e = g(f_l(v1), f_l(v2)),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\tG_{(v_i, v_j)}^2 = \\{v|\\min(d(v, v_i), d(v, v_j) \\leq 2)\\},\n\\end{equation}",
            "eq:eq:label": "\\begin{equation}\\label{eq:label}\n\tf_l(v) = 1+\\min(d(v,v_1), d(v, v_2))+(d_{s}/2)[(d_{s}/2)+(d_{s})\\%2-1],\n\\end{equation}"
        }
    }
}