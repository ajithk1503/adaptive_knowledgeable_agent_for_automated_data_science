{
    "meta_info": {
        "title": "Deep Representations for Time-varying Brain Datasets",
        "abstract": "Finding an appropriate representation of dynamic activities in the brain is\ncrucial for many downstream applications. Due to its highly dynamic nature,\ntemporally averaged fMRI (functional magnetic resonance imaging) can only\nprovide a narrow view of underlying brain activities. Previous works lack the\nability to learn and interpret the latent dynamics in brain architectures. This\npaper builds an efficient graph neural network model that incorporates both\nregion-mapped fMRI sequences and structural connectivities obtained from DWI\n(diffusion-weighted imaging) as inputs. We find good representations of the\nlatent brain dynamics through learning sample-level adaptive adjacency matrices\nand performing a novel multi-resolution inner cluster smoothing. We also\nattribute inputs with integrated gradients, which enables us to infer (1)\nhighly involved brain connections and subnetworks for each task, (2) temporal\nkeyframes of imaging sequences that characterize tasks, and (3) subnetworks\nthat discriminate between individual subjects. This ability to identify\ncritical subnetworks that characterize signal states across heterogeneous tasks\nand individuals is of great importance to neuroscience and other scientific\ndomains. Extensive experiments and ablation studies demonstrate our proposed\nmethod's superiority and efficiency in spatial-temporal graph signal modeling\nwith insightful interpretations of brain dynamics.",
        "author": "Sikun Lin, Shuyun Tang, Scott Grafton, Ambuj Singh",
        "link": "http://arxiv.org/abs/2205.11648v3",
        "category": [
            "cs.LG",
            "q-bio.NC"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nNeuroimaging techniques such as fMRI (functional magnetic resonance imaging) and DWI (diffusion-weighted imaging) provide a window into complex brain processes. Yet, modeling and understanding these signals has always been a challenge. Network neuroscience \\citep{bassett2017network} views the brain as a multiscale networked system and models these signals in their graph representations: nodes represent brain ROIs (regions of interest), and edges represent either structural or functional connections between pairs of regions.\n\nWith larger imaging datasets and developments in graph neural networks,\n% \\citep{4700287}\nrecent works leverage variants of graph deep learning, modeling brain signals with data-driven models and getting rid of Gaussian assumptions that typically existed in linear models \\citep{zhang2019estimating,li2019modeling}.\nThese methods are making progress on identifying physiological characteristics and brain disorders:\nIn \\cite{10.3389/fnins.2020.00630}, authors combine grad-CAM \\citep{selvaraju2017grad} and GIN \\citep{xu2018powerful} to highlight brain regions that are responsible for gender classification with resting-state fMRI data. Others \\cite{Li2020PoolingRG} propose to use regularized pooling with GNN to identify fMRI biomarkers. However, these works use time-averaged fMRI, losing rich dynamics in the temporal domain. They also do not incorporate structural modality that can provide additional connectivity information missing in the functional modality. \nAnother work \\cite{noman2021graph} embeds both topological structures and node signals of fMRI networks into a low-dimensional latent representations for better identification of depression, but it combines nodes' temporal and feature dimensions instead of handling them separately, leading to a suboptimal representation\n% which can not capture as good a spatial-temporal representation of latent dynamics as separating them do\n(as discussed in \\cref{ssc:model_comparison}).\n% Most current works ignore the underlying dynamic structures and cannot distinguish the important signals over the spatial-temporal dimension.\nTo overcome these issues, we propose ReBraiD (Deep \\textbf{Re}presentations for Time-varying \\textbf{Brai}n \\textbf{D}atasets), a graph neural network model that jointly models dynamic functional signals and structural connectivities, leading to a more comprehensive deep representation of brain dynamics.\n% We use fMRI and DWI data for these two modalities.\n\nTo simultaneously encode signals along spatial and temporal dimensions, some works in traffic prediction and activity recognition domains such as Graph WaveNet \\citep{wu2019graph} alternate between TCN (temporal convolution network) \\citep{10.1007/978-3-319-49409-8_7} and GCN (graph convolutional network) \\citep{kipf2017semisupervised}.\nOthers \\citep{song2020spatial,liu2020disentangling} use localized spatial-temporal graph to embed both domains' information in this extended graph. Some proposed methods also incorporate gated recurrent networks for the temporal domain such as \\citep{seo2018structured,ruiz2020gated}.\nWe choose to alternate TCN with GCN layers for ReBraiD, as it is more memory and time-efficient and can support much longer inputs.\nOn top of this design, we propose novel ``sample-level adaptive adjacency matrix learning'' and ``multi-resolution inner cluster smoothing,'' both of which learn and refine latent dynamic structures. With the choice of the temporal layer, our model is more efficient than other baselines while having the highest performance.\n%  We also explore the best option when alternating spatial and temporal layers for encoding brain activities with extensive ablation studies.\n% as it without constructing multiple $(\\tau N, \\tau N)$ adjacency matrices ($N$ is graph node number and $\\tau$ is localized sliding window size) along temporal axis, or having memory issues as in recurrent networks.\n% One major obstacle in applying deep learning to neuroscience is interpretability issues. Understanding why models make certain decision\n% gradient based attribution methods are  more accurate interpretation than pooling based saliency detection \\cite{Li2020PoolingRG} \n\nWe perform extensive ablation studies to examine individual components of the model. We also explore the best option when alternating spatial and temporal layers for encoding brain activities.\n% Our model shows its superior representation ability through these quantitative measures.\n% Equally important as finding a good representation of brain dynamics is interpreting them.\nAfter quantitatively showing the representation ability of our model, we utilize IG (integrated gradients)  \\citep{sundararajan2017axiomatic} to identify how brain ROIs participate in various processes. This can lead to better behavioral understanding, discovery of biomarkers, and characterization of individuals or groups. We also make the novel contribution of identifying temporally important frames with graph attribution techniques; this can enable more fine-grained temporal analysis around keyframes when combined with other imaging modalities such as EEG (electroencephalogram). In addition, our subject-level and group-level attribution studies unveil heterogeneities among ROIs, tasks, and individuals.\n\nIn summary, the main contributions of our work are as follows:\n\\begin{itemize}[leftmargin=*]\n    \\item We present ReBraid, an efficient graph neural network model that jointly models both structural and dynamic functional brain signals, providing a more comprehensive representation of brain activities when compared to the current fMRI literature.\n    % We also explore the best option when alternating spatial and temporal layers for encoding brain activities with extensive ablation studies.\n    \\item Unlike typical spatial-temporal GCNs that learn a universal latent structure, we propose sample-level latent adaptive adjacency matrix learning based on input snippets. This captures the evolving dynamics of a task better.\n    \\item We propose multi-resolution inner cluster smoothing, which effectively encodes long-range node relationships while keeping the graph structure, enabling the model to leverage structural and latent adjacency matrices throughout the process. Together with subject SC and sample-level adjacency matrix learning, the inner cluster smoothing learns and refines latent dynamic structures on limited signal data.\n    \\item We carry out extensive ablation studies and model comparisons to show ReBraid's superiority in representing brain dynamics. We also leverage integrated gradients to attribute and interpret the importance of both spatial brain ROIs and temporal keyframes, as well as heterogeneities among brain ROIs, tasks, and subjects. These can open up new opportunities for identifying biomarkers for different tasks or diseases and markers for other complex scientific phenomena.\n\\end{itemize}\n"
            },
            "section 2": {
                "name": "Method",
                "content": "\n\\label{sc:method}\n\n",
                "subsection 2.1": {
                    "name": "Preliminaries",
                    "content": "\n\\label{sc:preliminaries}\nWe utilize two brain imaging modalities mapped onto a same coordinate: SC (structural connectivity) from DWI scans, and time-varying fMRI scans. We represent them as a set of $L$ graphs \n\\begin{small}\n$\\mathcal{G}_i = (A_i, X_i)\\text{ with } i\\in[1,L]$.\n\\end{small}\n\\begin{small}\n$A_i \\in \\mathbb{R}^{N \\times N}$ \n\\end{small} represents normalized adjacency matrix with an added self-loop:\n\\begin{small}\n\\(A_i = \\tilde{D}_{\\text{SC}_i}^{-\\frac{1}{2}}\\tilde{\\text{SC}_i}\\tilde{D}_{\\text{SC}_i}^{-\\frac{1}{2}}\\), \\(\\tilde{\\text{SC}_i}=\\text{SC}_i+I_N\\)\n\\end{small}\nand\n\\begin{small}\n\\(\\tilde{D}_{\\text{SC}_i}=\\sum_w (\\tilde{\\text{SC}_i})_{vw}\\)\n\\end{small}\nis the diagonal node degree matrix.\nGraph signal matrix obtained from fMRI scans of the \\(i^{th}\\) sample is represented as \\begin{small}\n$X_i \\in \\mathbb{R}^{N\\times T}$.\n\\end{small}\nHere $N$ is the number of nodes, and each node represents a brain region; $T$ is the input signal length on each node. We refine our representation using the task of classifying brain signals $\\mathcal{G}_i$ into one of $C$ task classes through learning latent graph structures.\n\n"
                },
                "subsection 2.2": {
                    "name": "Model",
                    "content": "\n\\label{sc:model}\n\n\\footnotetext{Axis order follows PyTorch conventions. Dimension at the second index is the expanded feature dimension.}\nReBraiD takes $(A, X)$ as inputs and outputs task class predictions. The overall model structure is shown in \\cref{fig:model_structure}.\nFor the $i^{th}$\n% input graph signal\nsample\n\\begin{small}\n$X_i\\in \\mathbb{R}^{N\\times 1\\times T}$,\n\\end{small}\nthe initial $1\\times 1$ convolution layer increases its hidden feature dimension to $d_{h1}$, outputting\n\\begin{small}\n$(N, d_{h1}, T)$.\n\\end{small}\nThe encoder then encodes temporal and spatial information alternately, and generates a hidden representation of size\n\\begin{small}\n$(N, d_{h2}, 1)$.\n\\end{small}\nThe encoder is followed by two linear layers to perform pooling on node embeddings and two MLP layers for classification. Cross entropy is used as the loss function:\n\\begin{small}\n$L_{CE} = -\\sum_i y_i \\operatorname{log}\\hat{y}_i$,\n\\end{small}\nwhere $y_i\\in \\mathbb{R}^C$ is the one-hot vector of ground truth task labels and $\\hat{y}_i\\in \\mathbb{R}^C$ is the model's predicted distribution.\nWe now explain the different components of the model.\n\n\\textbf{(I) Learning sample-level latent graph structures.}\nStructural scans serve as our graph adjacency matrices. However, they remain fixed across temporal frames and across tasks. In contrast, FC (functional connectivities) are highly dynamic, resulting in different connection patterns across both time and tasks.\n% as shown in \\cref{sec:dynamic_fc}.\nTo better capture dynamic graph structures, we learn an adaptive adjacency matrix from each input graph signal. Unlike other works such as \\cite{wu2019graph} that use a universal latent graph structure, our model does not assume that all samples share the same latent graph. Instead, our goal is to give each sample a unique latent structure that can reflect its own signaling pattern.\n% Instead, in ReBraiD, each sample has a unique latent structure reflecting its signal status.\nThis implies that the latent adjacency matrix cannot be directly treated as a learnable parameter as a part of the model. To solve this, we minimize the assumption down to a shared projection $\\Theta_{\\text {adp}}$ that projects each input sequence into an embedding space and use this embedding to generate the latent graph structure. Projection $\\Theta_{\\text {adp}}$ can be learned in an end-to-end manner. The generated adaptive adjacency matrix for the $i^{th}$ sample can be written as follows ($\\operatorname{Softmax}$ is applied column-wise):\n\\begin{small}\n\\begin{equation}\n    A_{i\\_\\text{adp}}=\\operatorname{Softmax}\\left(\\operatorname{ReLU}\\left(\\left(X_{i} \\Theta_{\\text {adp }}\\right)\\left(X_{i} \\Theta_{\\text {adp }}\\right)^{\\top}\\right)\\right), \\Theta_{\\text {adp }} \\in \\mathbb{R}^{T \\times h_{\\text {adp }}}\n    \\label{eq:adp}\n\\end{equation}\n\\end{small}\n\n\\textbf{(II) Gated TCN (Temporal Convolutional Network).}\nTo encode signal dynamics, we use the gating mechanism as in \\cite{oord2016conditional} in our temporal layers:\n\\begin{small}\n\\begin{equation}\n    H^{(l+1)} = \\tanh\\left(\\operatorname{TCN}_{\\text{emb}}(H^{(l)})\\right) \\odot \\sigma\\left(\\operatorname{TCN}_{\\text{gate}}(H^{(l)})\\right),\n\\end{equation}\n\\end{small}\nwhere $H^{(l)}\\in\\mathbb{R}^{N\\times d\\times t}$ is one sample's activation matrix of the $l^{th}$ layer, $\\odot$ denotes the Hadamard product, and $\\sigma$ is the Sigmoid function.\nIn contrast to TCNs that are generally used in sequence to sequence models that consist of dilated $\\operatorname{Conv1d}$ and causal padding along the temporal dimension (\\cite{oord2016wavenet}), we simply apply $\\operatorname{Conv1d}$ with kernel = 2 and stride = 2 as our $\\operatorname{TCN}_{\\text{emb}}$ and $\\operatorname{TCN}_{\\text{gate}}$ to embed temporal information.\nThe reason is twofold: \nfirst, for a sequence to sequence model with a length-$T$ output, $y_\\tau$ should only depend on $x_{t\\leq\\tau}$ to avoid information leakage and causal convolution can ensure this. In contrast, our model's task is classification, and the goal of our encoder along the temporal dimension is to embed signal information into the feature axis while reducing the temporal dimension to 1.\n% which allows the output head to only deal with combining spatial information and working along the feature axis.\nThe receptive field of this single temporal point (with multiple feature channels) is meant to be the entire input sequence. Essentially, our TCN is the same as the last output node of a \\textit{kernel-two causal TCN} whose dilation increases by two at each layer (\\cref{fig:tcn}).\nSecond, from a practical perspective, directly using strided non-causal TCN works the same as taking the last node of dilated causal TCNs, as discussed above,\nwhile simplifying the model structure and reducing training time to less than a quarter.\n\n\\textbf{(III) Graph Network layer.}\nIn our model, every set of $l$ temporal layers\n(\\cref{sssc:ab_study}\nstudies the best $l$ to choose) is followed by a spatial layer to encode signals with the graph structure. Building temporal and spatial layers alternately helps spatial modules to learn embeddings at different temporal scales,\n% and vice versa,\nand this generates better results than placing spatial layers after all the temporal ones.\n\nTo encode spatial information, \\cite{kipf2017semisupervised} uses first-order approximation of spectral filters to form the layer-wise propagation rule of a GCN layer:\n\\begin{small}\n$H^{(l+1)} = \\operatorname{GCN}(H^{(l)}) = f(AH^{(l)}W^{(l)})$.\n\\end{small} It can be understood as spatially aggregating information among neighboring nodes to form new node embeddings. In the original setting without temporal signals, \\begin{small}\n$H^{(l)} \\in \\mathbb{R}^{N\\times d}$\n\\end{small} is the activation matrix of $l^{th}$ layer, \\begin{small}\n$A\\in \\mathbb{R}^{N\\times N}$\n\\end{small} denotes the normalized adjacency matrix with self-connections as discussed in \\cref{sc:preliminaries}, \\begin{small}\n$W^{(l)}\\in\\mathbb{R}^{d\\times d'}$\n\\end{small} is learnable model parameters, and $f$ is a nonlinear activation function of choice. Parameters $d$ and $d'$ are the number of feature channels.\n\nWe view a GCN layer as a local smoothing operation followed by an MLP, and simplify stacking K layers to \\begin{small}$A^KH$\\end{small} as in \\cite{wu2019simplifying}.\n% and removing nonlinear activation function $f$ to simplify the stacking of multiple layers.\nIn ReBraiD, every graph network layer aggregates information from each node's K-hop neighborhoods based on both brain structural connectivity and the latent adaptive adjacency matrix: thus, we have both \\begin{small}$A_{i}{}^{K} H^{(l)}W_{K}$\\end{small} and \\begin{small}$A_{i\\_\\text{adp}}{}^{K} H^{(l)} W_{K\\_\\text{adp}}$\\end{small} for input $H^{(l)}$. We also gather different levels (from $0$ to $K$) of neighbor information with concatenation.\nIn other words, one graph convolution layer here corresponds to a small module that is equivalent to K simple GCN layers with residual connections.\nWe can write our layer as:\n\\begin{small}\n\\begin{equation}\n\\label{eq:gnn}\n    \\begin{gathered}[b]\n    H^{(l+1)}=\\operatorname{GNN}^{(l)}\\left(H^{(l)}\\right)\\\\\n    =\\operatorname{MLP}\\left[\\operatorname{Concat}_{k=1}^{K}\\left(H^{(l)}, \\operatorname{ReLU}(A_{i}{}^{k} H^{(l)}), \\operatorname{ReLU}(A_{i\\_\\text{adp}}{}^k H^{(l)})\\right)\\right] \n    \\end{gathered}\n\\end{equation}\n% \\begin{multline*}\n% % \\begin{split}\n% % \\end{split}\n% \\end{multline*}\n\\end{small}\nNote that in \\cref{eq:gnn}, \\begin{small}$A_i\\in \\mathbb{R}^{N\\times N}$\\end{small} and \\begin{small}$H^{(l)}\\in \\mathbb{R}^{N\\times d\\times t}$\\end{small}, and as a result their product \\begin{small}$ \\in \\mathbb{R}^{N\\times d \\times t}$\\end{small}.\nOutputs of different $\\operatorname{GNN}^{(l)}$ layers are parameterized and then skip connected with a summation. Since the temporal lengths of these outputs are different because of $\\operatorname{TCN}$s, max-pooling is used before each summation to make the lengths identical.\n\n\\textbf{(IV) Multi-resolution inner cluster smoothing.} While $\\operatorname{GNN}$ layers can effectively pass information between neighboring nodes, long-range relationships among brain regions that neither appear in SC nor learned by latent $A_{\\text{adp}}$ can be better captured using soft assignments, similar to \\textsc{DiffPool}\\cite{ying2018hierarchical}.\n% This is especially true for the first two $\\operatorname{GNN}$ layers when node features haven't propagated to far-away neighbors.\nTo generate the soft assignment tensor $S^{(l)}$ that assigns $N$ nodes into $c$ clusters ($c$ chosen manually), we use \\begin{small}\n$\\operatorname{GNN}_{pool}^{(l)}$\n\\end{small} that obeys the same propagation rule as in \\cref{eq:gnn}, followed by $\\operatorname{Softmax}$ along $c$. This assignment is applied to $Z^{(l)}$, the output of \\begin{small}\n$\\operatorname{GNN}_{emb}^{(l)}$\n\\end{small} which carries out the spatial embedding for the $l^{th}$ layer input $H^{(l)}$, producing clustered representation $\\tilde{H}^{(l)}$:\n\\begin{small}\n\\begin{equation}\n    \\begin{aligned}[c]\n    &S^{(l)}=\\operatorname{Softmax} \\left(\\operatorname{GNN}_{pool}^{(l)}\\left(H^{(\\ell)}\\right), 1\\right) \\in \\mathbb{R}^{N \\times c \\times t}\\\\\n    &Z^{(l)}=\\operatorname{GNN}_{emb}^{(l)}\\left(H^{(l)}\\right) \\in \\mathbb{R}^{N \\times d \\times t}\\\\\n    &\\tilde{H}^{(l)}= S^{(l) \\top}Z^{(l)} \\in \\mathbb{R}^{c \\times d \\times t}\\\\\n    % &\\tilde{H}^{(l)}= S^{(l) \\top}\\operatorname{GNN}_{emb}^{(l)}\\left(H^{(l)}\\right), \\quad \\tilde{H} \\in \\mathbb{R}^{c \\times d \\times t}\\\\\n    \\end{aligned}\n\\label{eq:diffpool}    \n\\end{equation}\n\\end{small}\nThe additional temporal dimension allows nodes to be assigned to heterogeneous clusters at different frames. \n% In practice, \\cref{eq:diffpool} is done with $\\operatorname{einsum}$ because of this extra $t$ axis. \nWe find that using coarsened \\begin{small}$A_i^{(l+1)}=S^{(l)\\top} A_i^{(l)} S^{(l)} \\in \\mathbb{R}^{c\\times c}$\\end{small} as the graph adjacency matrix leads to worse performance compared to using SC-generated $A_i$ and learned $A_{i\\_\\text{adp}}$\n(comparison in \\cref{ssc:model_components}).\nIn addition, if the number of nodes is changed, residual connections coming from the beginning of temporal-spatial blocks can not be used, impacting the overall performance. To continue using $A_i$ and $A_{i\\_\\text{adp}}$ as graph adjacency matrices and to allow residual connections, we reverse-assign $\\tilde{H}^{(l)}$ with assignment tensor obtained from applying $\\operatorname{Softmax}$ on $S^{(l) \\top}$ along $N$, so that the number of nodes is kept unchanged:\n\\begin{small}\n\\begin{equation}\n    \\begin{aligned}[c]\n    &\\tilde{S}^{(l)}=\\operatorname{Softmax} \\left(S^{(l) \\top}, 1\\right) \\in \\mathbb{R}^{c \\times N \\times t}\\\\\n    &H^{(\\ell+1)}= \\tilde{S}^{(l)^{\\top}}\\tilde{H}^{(l)} \\in \\mathbb{R}^{N \\times d \\times t}\n    \\end{aligned}\n\\label{eq:smoothing}\n\\end{equation}\n\\end{small}\nIn fact, \\cref{eq:diffpool,eq:smoothing} perform signal smoothing on nodes within each soft-assigned cluster.\n% (\\cref{ssc:toy_example} shows a toy example).\nWith the bottleneck $c < N$, the model is forced to pick up latent community structures.\nThis inner cluster smoothing is carried out at multiple spatial resolutions: as the spatial receptive field increases with more graph layers, we decrease cluster number $c$ for the assignment operation.\nAs these $\\operatorname{GNN}$ layers alternate with $\\operatorname{TCN}$ layers, the inner cluster smoothing also learns the community information across multiple temporal scales.\n% We find using a parallel $\\operatorname{GNN}_{pool}$ for extracting soft assignments alongside the embedding $\\operatorname{GNN}_{emb}$ outperforms using attention based graph layers like $\\operatorname{GAT}$ \\cite{velivckovic2017graph} in representing brain dynamics.\n\n"
                },
                "subsection 2.3": {
                    "name": "Attribution with IG (Integrated Gradients)",
                    "content": " \nAs one approach to model interpretability, \\textit{attribution} assigns credits to each part of the input, assessing how important they are to the final predictions.\n% Understanding how signals on different brain regions contribute to the final classification outputs has many important applications in neuroscience as we have mentioned.\n\\cite{wiltschko2020evaluating} gives an extensive comparison between different graph attribution approaches, in which IG \\cite{sundararajan2017axiomatic} is top-performing and can be applied to trained models without any alterations of the model structure. IG also has other desirable properties, such as implementation invariance that other gradient methods lack.\nIt is also more rigorous and accurate than obtaining explanations from attention weights or pooling matrices that span multiple feature channels. Intuitively, IG calculates how real inputs contribute differently compared to a selected baseline; it does so by aggregating model gradients at linearly interpolated inputs between the real and baseline inputs. \n\nIn order to apply IG, we calculate attributions at each point of both input \\begin{small}$A\\in\\mathbb{R}^{N\\times N}$\\end{small} and \\begin{small}$X\\in\\mathbb{R}^{N\\times T}$\\end{small} for each sample:\n\\begin{small}\n% \\begin{equation}\n%     \\begin{aligned}\n%     & \\operatorname{\\textsc{Attr}}_{A_{vw}} =\\left(A_{vw}-A_{vw}^{\\prime}\\right) \\times \n%     % \\int_{\\alpha=0}^{1} \n%     \\sum_{m=1}^{M} \\frac{\\partial F\\left(A_{\\text{Intrpl}}, X\\right)}{\\partial A_{\\text{Intrpl}_{vw}}} % d \\alpha\n%     \\times\\frac{1}{M},\\quad A_{\\text{Intrpl}} = A^{\\prime}+%\\alpha\n%     \\frac{m}{M}\\times\\left(A-A^{\\prime}\\right)\\\\\n%     &\\operatorname{\\textsc{Attr}}_{X_{vt}}\n%     =\\left(X_{vt}-X_{vt}^{\\prime}\\right) \\times %\\int_{\\alpha=0}^{1}\n%     \\sum_{m=1}^{M} \\frac{\\partial F\\left(A, X_{\\text{Intrpl}}\\right)}{\\partial X_{\\text{Intrpl}_{vt}}} %d \\alpha\n%     \\times\\frac{1}{M},\\quad X_{\\text{Intrpl}} = X^{\\prime}+%\\alpha\n%     \\frac{m}{M}\\times\\left(X-X^{\\prime}\\right)\n%     \\end{aligned}\n% \\label{eq:ig}    \n% \\end{equation}\n\\begin{equation}\n    \\begin{aligned}\n    & \\operatorname{\\textsc{Attr}}_{\\mathcal{G}_{vw}} =\\left(\\mathcal{G}_{vw}-\\mathcal{G}_{vw}^{\\prime}\\right) \\times \n    % \\int_{\\alpha=0}^{1} \n    \\sum_{m=1}^{M} \\frac{\\partial F\\left(\\mathcal{G}_{\\text{Intrpl}}\\right)}{\\partial \\mathcal{G}_{\\text{Intrpl}_{vw}}} % d \\alpha\n    \\times\\frac{1}{M},\\\\\n    & \\mathcal{G} = (A, X), \\quad \\mathcal{G}_{\\text{Intrpl}} = \\mathcal{G}^{\\prime}+%\\alpha\n    \\frac{m}{M}\\times\\left(\\mathcal{G}-\\mathcal{G}^{\\prime}\\right)\n    \\end{aligned}\n\\label{eq:ig}    \n\\end{equation}\n\\end{small}\n\\begin{small}\n$F(\\mathcal{G})$\n\\end{small} here represents our signal classification model, $M$ is the step number when making Riemann approximation of the path integral, and $\\mathcal{G}^{\\prime}$ is the baselines of $\\mathcal{G}$ (see \\cref{ssc:interpretations}\nfor more details). Note that \\cref{eq:ig} calculates the attribution of one edge or one node on one sample. \nThe process is repeated for every input point, so attributions $\\operatorname{\\textsc{Attr}}_A, \\operatorname{\\textsc{Attr}}_X$ have identical dimensions as inputs $A, X$.\nTo obtain the brain region importance of a task, we aggregate attributions across multiple samples of that task.\n\n"
                }
            },
            "section 3": {
                "name": "Experiments",
                "content": "\nWe use fMRI signals from the CRASH dataset \\cite{lauharatanahirun2020flexibility} for our experiments. The model classifies input fMRI into six tasks: resting state, VWM (visual working memory task), DYN (dynamic attention task), MOD (math task), DOT (dot-probe task), and PVT (psychomotor vigilance task). We preprocess 4D voxel-level fMRI images into graph signals $\\mathcal{G} = (A, X)$ by averaging voxel activities into regional signals with the 200-ROI cortical parcellation (voxel to region mapping) specified by \\cite{schaefer2018local}. We also standardize signals for each region and discard scan sessions with obvious abnormal spikes that may be caused by head movement, etc. DWI scans are mapped into the same MNI152 coordinate and processed into adjacency matrices with the same parcellation as fMRI. Our processed data contains 1940 scan sessions from 56 subjects. Session length varies from 265 frames\n% (each frame is one scan)\nto 828 frames (see \\cref{tab:data_detail} for details).\nTR (Repetition Time) is 0.91s.\n\nThe 1940 scan sessions from CRASH are separated into training, validation, and test sets with a ratio of 0.7-0.15-0.15 (subject-wise split does not lead to any noticeable difference). Each split receives a proportional number of samples for each class. Hyperparameters including dropout rate, learning rate, and weight decay are selected using grid search based on validation loss. All results reported in this section are obtained from the test set. For each scan session, we use a stride-10 sliding window to generate input sequences (in the following experiments $T \\in \\{8, 16, 32, 64, 128, 256\\}$) and feed them to the model. To encode temporal and spatial information alternately, we find stacking two $\\operatorname{TCN}$ layers per one $\\operatorname{GNN}$ layer leads to better performance most times (see \\cref{sssc:ab_study} (I)). \nWe tested $h_\\text{adp} = 2, 5, 10$ in \\cref{eq:adp} for our experiments, and 5 appears to be the best; so we use this value for all the following experiments. $K = 1,2,3$ in \\cref{eq:gnn}  were tested on a few settings, and K = 2, 3 have a similar performance, both outperforming K = 1. Since smaller values of K have smaller computation needs, we use $K = 2$ for all experiment settings, meaning each $\\operatorname{GNN}$ layer aggregates information from 2-hop neighbors based on the provided adjacency matrices. We evaluate our model with weighted F1 as the metric in order to account for the imbalance in the number of samples in each task.\nOur models are written in PyTorch, trained with Google Colab GPU runtimes, and 30 epochs are run for each experiment setting.\nCode is publicly available \\footnote{\\href{https://github.com/sklin93/ReBraiD}{https://github.com/sklin93/ReBraiD}}.\n\n\n\n",
                "subsection 3.1": {
                    "name": "Model components",
                    "content": "\n\\label{ssc:model_components}\n\n\n\\textbf{Ablation studies on graph adjacency matrices.}\nFor each input sample $\\mathcal{G}_i$, we test different options to provide graph adjacency matrices to the $\\operatorname{GNN}$ layer. They include (i) our proposed method: using both adaptive adjacency matrix $A_{i\\_\\text{adp}}$ and SC-induced $A_i$, (ii) only using $A_{i}$, (iii) only using $A_{i\\_\\text{adp}}$, (iv) replacing $A_{i\\_\\text{adp}}$ in setting i with $A_{i\\_\\text{FC}}$ derived from functional connectivity, and (v) only using random graph adjacency matrices with the same level of sparsity as real $A$'s.\nThe results under different settings are reported in \\cref{fig:graph_supports} (and \\cref{tab:ab_study} in appendix for numerical values).\n\nFrom the results of setting (ii) plotted in \\cref{fig:graph_supports}, we see that removing the adaptive adjacency matrix impacts the performance differently at different input lengths: the gap peaks for signals of length 64--128, and becomes smaller for either shorter or longer sequences. This could suggest the existence of more distinct latent states of brain signals of this length that structural connectivities cannot capture.\nOn the other hand, removing SC (setting (iii)) seems to have a more constant impact on the model performance, with shorter inputs more likely to see a slightly larger drop. In general, only using $A_{\\text{adp}}$ leads to a smaller performance drop than only using SC,\n% the gaps between setting (iii) and (i) are either close to or smaller than those between setting (ii) and (i),\nindicating the effectiveness of $A_{\\text{adp}}$ in capturing useful latent graph structures. More detailed studies below show that $A_{\\text{adp}}$ learns distinct representations not captured by $A$.\n% For all settings, weighted F1 generally increases as the input length gets larger, with some exceptions occurring at length 64.\n% \\todo{why?}\n\nAs mentioned in \\cref{sc:method}, our motivation behind creating sample-level adaptive adjacency matrices is FC's highly dynamic nature. Therefore, for setting (iv), we test directly using adjacency matrices $A_{i\\_\\text{FC}}$ obtained from FC instead of the learned $A_{i\\_\\text{adp}}$. In particular,\n\\begin{small}\n$A_{i\\_\\text{FC}} =  \\tilde{D}_{\\text{FC}_i}^{-\\frac{1}{2}}\\tilde{\\text{FC}_i}\\tilde{D}_{\\text{FC}_i}^{-\\frac{1}{2}}\\in \\mathbb{R}^{200\\times 200}$,\n\\end{small} where\n\\begin{small}\n$(\\text{FC}_i)_{vw} = \\operatorname{corr}((X_i)_v, (X_i)_w)$,\n$\\tilde{\\text{FC}_i}=\\text{FC}_i+I_N$\n\\end{small} and\n\\begin{small}\n$\\tilde{D}_{\\text{FC}_i}=\\sum_w (\\tilde{\\text{FC}_i})_{vw}$.\n\\end{small}\nFig. \\ref{fig:graph_supports} shows $A_{i\\_\\text{FC}}$ constantly underperforms $A_{i\\_\\text{adp}}$, except for being really close for length-8 inputs. Larger performance gaps are observed for longer inputs, where \\begin{small}$\\operatorname{Corr}((X_i)_v, (X_i)_w)$ \\end{small} struggles to capture the changing dynamics in the inputs. This demonstrates that our input-based latent $A_{i\\_\\text{adp}}$ has better representation power than input-based FC. We also notice batch correlation coefficients calculation for $A_{i\\_\\text{FC}}$ results in a slower training speed than computing $A_{i\\_\\text{adp}}$.\n\nAn interesting result comes from setting (v), where we use randomly generated Erdős-Rényi graphs with the edge creation probability the same as averaged edge existence probability of $A$'s. Its performance is similar to or even better than settings (ii) and (iii).\nWe examine this further in \\cref{ssc:interpretations}.\n% We hypothesize that the model can learn the latent graph structure out of randomness, and we will verify this hypothesis in \\cref{ssc:interpretations}.\n\n% Overall, $X$ plays a more important role than $A$ for predicting the input signal class, but adding well suited graph supports can help boost the performance a little further \\todo{especially for certain (mid-range) lengths}.\n\n\\textbf{Latent adaptive adjacency matrix $A_{\\text{adp}}$.} The above results demonstrate latent $A_{\\text{adp}}$ can complement the task- and temporal-fixed $A$. We now show that the learned $A_{i\\_\\text{adp}}$ is sparse for each sample, has evident task-based patterns, and provides new information beyond $A_i$.\nThe sparsity of $A_{i\\_\\text{adp}}$ can be seen from  \\cref{fig:adp_ind_task3} in appendix: each input only gets a few important columns (information-providing nodes in $\\operatorname{GNN}$). These columns vary from one sample to another, indicating $A_{\\text{adp}}$'s ability to adapt to changing inputs within the same task. However, when we look into inputs generated by consecutive sliding windows (not shuffled) from the same scan session as in \\cref{fig:adp_ind_task3_same_ses}, we can see the latent structures change smoothly.\nIn addition, when we aggregate samples inside each task, noticeable task-based patterns emerge (\\cref{fig:adp_task_avg_roi}). These patterns are different from $\\operatorname{Attr}_A$ in \\cref{fig:attr_A_with_rand_colsum}, suggesting that $A_{\\text{adp}}$ embeds dynamics not captured by $A$.\n% On the subnetwork level, one interesting phenomenon is LimbicB\\_OFC, Default\\_B, and LimbicA\\_TempPole are consistently among the most attributed $A_{\\text{adp}}$ subnetworks across tasks. Further exploration is needed to explain the case.\n\nQuantitatively, $A_{i\\_\\text{adp}}$ entry values range between (0, 1) because of the $\\operatorname{Softmax}$, and only around 2\\% of entries in $A_{i\\_\\text{adp}}$ have values larger than 0.05. As a reference, the largest entry value is larger than 0.99. A similar sparsity pattern is found when using synthetic data on the same model, indicating that the sparsity is more due to the model than the underlying biology. Given how $A_{i\\_\\text{adp}}$ is used in GNN layers, each column of it represents a signal-originating node during message passing. We hypothesize that the model learns the most effective \\textit{hubs} that pass information to their neighbors. A related idea is information bottleneck \\cite{tishby2015deep}: deep learning essentially compresses the inputs as much as possible while retaining the mutual information between inputs and outputs. In a sense, $A_{i\\_\\text{adp}}$ represents the compressed hubs for a given input signal.\nWe also note that this sparsity emerges even without any additional constraints. In fact, adding $L_1$ constraints on $A_{\\text{adp}}$ does not change the model performance or the $A_{i\\_\\text{adp}}$ sparsity level. We hypothesize that the naturally trained $A_{i\\_\\text{adp}}$ is sparse enough, and further sparsification is unnecessary.\n% would move it away from the local optima thus cannot be reached.\n% Shorter inputs has different (from visual aspective) task-averaged $A_{\\text{adp}}$ and ROI column sum distributions, but when mapped into subnetworks, it's still very LimbicB\\_OFC- and Default\\_B-dominant!\n% \\todo{$A_{\\text{adp}}$ obtained from models trained without SC has similar per-task patterns as \\cref{fig:adp}, not denser, not changing pattern more like $\\operatorname{Attr}_A$...why?}\n\nWe visualize the projected inputs $X_i \\Theta_{\\text {adp }}$ in \\cref{fig:nodevec}, which clearly shows the task, node and subject heterogeneities. Different tasks have varied representations in the latent space for the same node, but DOT, MOD, PVT has similar embedding patterns across individuals and most nodes. Indeed, when looking at the confusion matrix across models (\\cref{fig:cm} in appendix), the misclassifications mostly cluster between these three tasks, indicating their natural similarity. We want to note here that adding a learnable bias to $X \\Theta_{\\text {adp }}$ does not separate the task embeddings further, nor does it improve overall performance.\nSubjects also exhibit heterogeneity: the same pair of nodes during the same task can have different embedding distances, thus graph edge weights, for each individual.\n\n\\textbf{Multi-resolution inner cluster smoothing.}\nTo verify the capability of inner cluster smoothing operation in capturing latent graph dynamics, we test the following settings: (vi) using our proposed model and inputs, except removing paralleled $\\operatorname{GNN}_{pool}$ and inner cluster smoothing module; (vii) previous setting (v) but remove $\\operatorname{GNN}_{pool}$ and inner cluster smoothing module; (viii) keep $\\operatorname{GNN}_{pool}$, but using coarsened graph instead of smoothing (essentially performing \\textsc{DiffPool} with an added temporal dimension). In this last setting, we hierarchically pool and reduce the graph to a single node, and we keep the total number of $\\operatorname{GNN}$ layers the same as our other settings.\n% In our experiments, soft-assigned cluster numbers $c$ are chosen manually, although we don't find much difference caused by its choices. \nValues of soft-assigned cluster number $c$ are chosen to be halved per smoothing module (e.g., $N/2, N/4, \\cdots$) for our experiments. Different choices of $c$ affect the model convergence rate but only have a minor impact on the final performance (see \\cref{sssc:ab_study} (II)).\nResults are reported in \\cref{fig:graph_supports} (and \\cref{tab:ab_study} in appendix). Apart from these three settings, we also test adding pooling regularization terms (described in \\cref{ssc:pool_reg}) into the loss function, but they do not lead to much of a difference.\n\nThe above results demonstrate that both setting (vi) and (vii) outperforms (viii) by a large margin, indicating the importance of keeping the original node number when representing brain signals. In addition, all three settings underperform our proposed method. They are also mostly worse than changing graph adjacency matrices as in settings (ii)--(v): this shows the inner cluster smoothing module has a more significant impact in learning latent graph dynamics.\nWe also find using adaptive adjacency matrices and inner cluster smoothing can stabilize training, making the model less prone to over-fitting and achieving close-to-best performance over a larger range of hyperparameters (see \\cref{fig:c_effect}).\n\n% \\todo{effectiveness of pooling layers? first 2 most effective as gradients tells. Also, no pool with setting (iv) outperforms with no pool with setting (i) for shorter inputs..why? probably random one is more flexible to choose from, once the model loses ability to get latent structure with pooling, it needs more flexibility?}\n% \\vspace{-1cm}\n"
                },
                "subsection 3.2": {
                    "name": "Model Comparisons",
                    "content": "\n\\label{ssc:model_comparison}\n\n \nSince we adopt a network view to studying the brain, where brain regions are treated as graph nodes, we source our baselines from graph models. To do so, we examined all models in PyTorch Geometric (PyG) \\footnote{\\href{https://pytorch-geometric.readthedocs.io/}{https://pytorch-geometric.readthedocs.io/}} and its temporal extension (PyG-T) \\footnote{\\href{https://pytorch-geometric-temporal.readthedocs.io/}{https://pytorch-geometric-temporal.readthedocs.io/}} as they contain the most up-to-date and well-organized open-source graph neural network model implementations. In particular, we compare our model with the vanilla GCN from \\cite{kipf2017semisupervised},\n% Graph Attention Network (GAT) from \\cite{velickovic2018graph},\nChebyshev Graph Convolutional Gated Recurrent Unit (GConvGRU) from \\cite{seo2018structured}, GraphSAGE from \\cite{hamilton2017inductive}, GAT V2 from \\cite{brody2021attentive}\nand Graph Transformer as in \\cite{shi2021masked}.\n% Their original networks are built for different tasks such as traffic prediction and epidemiological forecasting, which work on a single shared graph adjacency matrix for all samples yet we have different $A_i$ for each sample.\nBaseline models are constructed similar to ours: each has four graph encoding layers taking in both signals and adjacency matrices, followed by two linear layers along the node axis and two linear layers for the final classification.\nWe train baseline models with the same input, loss, optimizer, and epoch settings (all models are well-converged). Grid search is used to optimize the rest of the hyperparameters. We compare weighted F1 and training time per epoch in \\cref{tab:comparison}; we also plot our model and Graph Transformer's confusion matrices in \\cref{fig:cm}.\n\nOur model shows significant performance gains and requires less training time than graph baselines. We believe the most critical reason is that the models in PyG treat temporal signals as feature vectors instead of placing them into a separate temporal dimension. Without sequence modeling on the temporal dimension, even the state-of-the-art graph attention models (GAT-v2 and graph Transformer) cannot perform well. In addition, almost all models in PyG-T assume one common graph for the inputs (application scenarios are traffic network forecasting, link predictions, etc.), whereas we need to feed different SC for every sample. Out of them, we were able to choose one model (GConvGRU) that supports different adjacency matrices, but it didn't give a satisfactory result. Our proposed ideas of sample-level adaptive adjacency matrix learning and multi-resolution inner cluster smoothing help capture latent brain dynamics and improve the performance. The higher model performance here reflects a better encoding ability of brain signals, which can benefit different downstream tasks such as disease and trait prediction.\n\nIn addition to graph baselines, we also tested the state-of-the-art model for multivariate time series classification (MVTS Transformer \\cite{zerveas2021transformer}), which has comparable performance to ours. This stresses the critical role of temporal modeling when dealing with dynamic signals, so we tested our model without GNN layers. We experiment both removing GNN layers altogether and replacing them with $1\\times 1$ CNN layers: both outperform graph models that focus on the spatial modeling aspect. \nAlthough these results demonstrate that temporal modeling is crucial, adding graph modeling that includes signals' spatial relationships as proposed can further improve the performance. Since the MVTS Transformer model has projections to generate queries, keys, and values from the input sequence, it can also implicitly learn spatial relationships between variables (\\textit{nodes}). On the other hand, explicitly adding graph components allows the model to utilize prior structures (e.g., SC). The attribution of graph models can also provide better interpretability of brain networks, such as identifying critical region connections.\n% Besides, the training time of our model is significantly less. For one epoch under the same setting, our model takes roughly 298 seconds while the rest of the baselines take at least 713 seconds.\n\n"
                },
                "subsection 3.3": {
                    "name": "Interpretation with IG",
                    "content": "\n\\label{ssc:interpretations}\nThis section studies the contributions of different brain ROIs and subnetworks defined by their functionalities. For the subnetwork definition, we choose to use the 17 networks specified in \\cite{thomas2011organization}, which has a mapping from our previous 200-ROI parcellation\\footnote{\\href{https://github.com/ThomasYeoLab/CBIG/blob/master/stable_projects/brain_parcellation}{https://github.com/ThomasYeoLab/CBIG/blob/master/stable\\_projects/brain\\_parcellation}}.\nTo select baseline inputs, we follow the general principle for attribution methods: when the model takes in a baseline input, it should produce a near-zero prediction, and $\\operatorname{Softmax}(\\text{outputs})$ should give each class about the same probability in a classification model. \nAll-zero baselines $A^{\\prime}$ and $X^{\\prime}$ can roughly achieve this for our model, so we choose them as our baseline inputs.\nStep number $M$ is set to 30.\nThe IG computation is done on 900 inputs for each task to get an overall distribution.\n\nThe extracted high-attribution regions and connections should be reproducible across different initializations to be used for downstream tasks. Since the overall problem is non-convex, we empirically test and confirm the attribution reproducibility with two randomly initialized models before proceeding to the following analyses. In addition, \\cite{wiltschko2020evaluating} demonstrates IG's consistency (reproducibility among a range of hyperparameters) and faithfulness (more accurate attribution can be obtained with better performing models). Since our model has higher performance with longer inputs, we compute IG attributions of a model trained on length-256 input signals in this section.\n% \\todo{attributions / top contributing ROIs/ subnetworks are fairly consistent across different length}\n\n\\textbf{Temporal importance.}\nOn the single input level, we can attribute which parts of the inputs in $\\mathcal{G}_i$ are more critical in predicting the target class by looking into $(\\operatorname{\\textsc{Attr}}_{X})_i$. This attribution map not only shows which brain regions contribute more but also reveals the important signal frames. One critical drawback of fMRI imaging is its low temporal resolution, but if we know which part is more important, we can turn to more temporally fine-grained signals such as EEG to see if there are any special activities during that time.\nTo confirm that the attributions we get are valid and consistent, we perform a sanity check of IG results on two overlapped inputs with an offset $\\tau$: the first input is obtained from window $[t_0, t_0+T]$ and the second is obtained from window $[t_0+\\tau, t_0+\\tau+T]$. Offset aligned results are shown in \\cref{fig:ig_sanity_check}, in which the attributions agree with each other quite well.\n\n\n\\textbf{Spatial importance.}\nWe examine the connection importance between brain ROIs by looking at $\\operatorname{\\textsc{Attr}}_{A}$.\n% (task-averaged $\\operatorname{\\textsc{Attr}}_{A}$ are plotted in \\cref{fig:attr_A_with_rand} in appendix).\nIn particular, columns in $\\operatorname{\\textsc{Attr}}_{A}$ with higher average values are sender ROIs of high-contributing connections, which is what matters in the $\\operatorname{GNN}$ operation.\n% while row sums reflect receiver ROIs.\nWe also explore why using random graph adjacency matrices (setting (v) in \\cref{ssc:model_components}) can produce a similar result for length-256 inputs compared to using both SC-induced $A_i$ and $A_{i\\_\\text{adp}}$ (setting (i)). By examining $\\operatorname{\\textsc{Attr}}_{A}$ under both settings (\\cref{fig:attr_A_with_rand_colsum}), we see that the column averages of $\\operatorname{\\textsc{Attr}}_{A}$ under these two settings are similar for almost all tasks, meaning the model can learn the important signal sending regions relatively well even without explicit structures.\nWe credit this ability primarily to multi-resolution inner cluster smoothing, as the performance drops notably without it (setting (vii)).\nHowever, using ground truth SC not only gives us higher performance for shorter inputs but also provides the opportunity to interpret brain region connections better. We can directly use task-averaged $\\operatorname{\\textsc{Attr}}_{A}$ as the weighted adjacency matrix to plot edges between brain ROIs, just as in \\cref{fig:bnv_XAcombined}. Important brain regions obtained from $\\operatorname{\\textsc{Attr}}_{A}$ mostly comply with the previous literature (see \\cref{sssc:interp} for details).\n\n\n\n\nIn addition to $\\operatorname{\\textsc{Attr}}_{A}$, $\\operatorname{\\textsc{Attr}}_{X}$ can also provide insights on spatial importance when the attribution maps are aggregated along the temporal dimension. But it does so from another perspective: based on how the model takes in the inputs, larger $\\operatorname{\\textsc{Attr}}_{A}$ implies critical \\textit{structural connections} between brain regions, meaning that information passing between those regions is deemed essential in classifying task states.\nIn contrast, larger $\\operatorname{\\textsc{Attr}}_{X}$ reveals regions or subnetworks that are sources of the important \\textit{signals}: it does not matter if the signal activities propagate from one region to another. Instead, the signals themselves are crucial for differentiating between task states. \nWe notice that signal-important ROIs are not necessarily the same as connection-important ROIs: top-ranked subnetworks for resting state are DefaultA and DefaultB by $\\operatorname{\\textsc{Attr}}_{A}$, and VisCent and DorsAttnA by $\\operatorname{\\textsc{Attr}}_{X}$; although they do coincide with each other for tasks like VMN. This disparity is reflected in \\cref{fig:bnv_XAcombined} as edge and node differences. \nAnother observation is that DYN and PVT have similar $\\operatorname{\\textsc{Attr}}_{A}$ patterns; both have a high attribution on connections originating from visual, control, and somatomotor systems. But when looking at $\\operatorname{\\textsc{Attr}}_{X}$, DYN and PVT are extreme opposites. For example, PVT has a very high $\\operatorname{\\textsc{Attr}}_{X}$ for a few ROIs in LH\\_SomMotA, DorsAttnA\\_TempOcc, and RH\\_VisCent\\_ExStr, while DYN has very low $\\operatorname{\\textsc{Attr}}_{X}$ for them. This suggests that the model uses these ROIs' activities to distinguish between the two tasks. Therefore, the attributions are not absolute but relative to what they are compared against. As a result, when identifying biomarkers with attribution, it is crucial to have \\textit{contrasts}---for example, different tasks, different disease states, etc.\n\n% It can be even more accurate than $\\operatorname{\\textsc{Attr}}_{A}$ since $X$ plays a more important role for model predictions.\nIn \\cref{fig:ig_task}, we plot the distribution of time-averaged and subnetwork-averaged (mapping 200 ROIs into 17 subnetworks) $\\operatorname{\\textsc{Attr}}_{X}$\n% computed from 900 input signals\nduring the VWM task. We can see the clear dominance of VisCent, DorsAttnA, and ContA subnetworks (numbered as 1, 5, 11), indicating signals from these regions are useful for the model to decide if the input is from the VWM task.\n% For the boxplots of other tasks and subnetwork rankings, please see \\cref{fig:ig_6tasks}, \\cref{tab:region_rank_A} and \\cref{tab:region_rank_X} in appendix.\nMore informative than the rankings is the distribution itself: even though VisCent, DorsAttnA, and ContA ranked top 3 for both resting state and VWM for signal attributions, their relative importance and attribution distribution variances are drastically different. In a sense, the distribution can act as a task fingerprint based on brain signal states.\n\n\\textbf{Group, session, and region heterogeneity.} \nAverage variances of attributions are very different across tasks, especially those of $\\operatorname{\\textsc{Attr}}_{X}$: VWM and DYN have much smaller attribution variances compared to other tasks. This can be caused by either task dynamics when certain tasks have more phase transitions and brain status changes, or/and group heterogeneity when individuals carry out specific tasks more differently than the others. We investigate this by examining three subjects that have multiple scan sessions for every task.\n\n\nWe report the following findings: (1) Even only aggregating attributions over a single subject's sessions, attribution variances of the other four tasks are still larger than VWM and DYN. And these variance values are comparable to that of aggregating over many subjects. This means the large variances are not mainly due to group heterogeneity; rather, some tasks have more states than others. (2) There is still group heterogeneity apart from different task dynamics, and the group heterogeneity is also more evident for tasks with more dynamics (high attribution variances). We can see from \\cref{fig:hetero} that attributions for VMM are much more concentrated and universal across subjects than that of MOD. (3) Flexibility of different subnetworks varies: subnetworks with small distribution IQR (interquartile range) of the same subject's different sessions are also more consistent across subjects. One example is that subnetwork 18 during the MOD task has both higher within-subject IQR and more significant across-subject differences than subnetwork 19. This indicates that for a particular task, some subnetworks are more individual and flexible (may activate differently across time), while others are more collective and fixed. In summary, we can find both critical regions that a particular task must rely on and regions that can characterize individual differences during tasks.\n\n"
                },
                "subsection 3.4": {
                    "name": "Simulation study",
                    "content": "\nTo validate the results of our interpretations, we perform simulation studies with known ground truth.\n% For generating graph signals, we first define graph structures.\nAll graphs are generated with SBM (stochastic block model) using the same community structure (200 nodes, 10 communities), but each graph has its own adjacency matrix. This generation process mimics brain structures in that samples share similar community structures but have distinct structural connectivities. Fig \\ref{fig:syn_A} shows a typical adjacency matrix of a synthetic graph. All adjacency matrices are binary. Time-series on each node are then generated with code adapted from pytorch-gnn repository \\footnote{\\href{https://github.com/alelab-upenn/graph-neural-networks}{https://github.com/alelab-upenn/graph-neural-networks}}. In particular, the value at each time step of each node is a small temporal Gaussian random noise plus signals from neighbors' (a small spatial Gaussian noise is added to the adjacency matrix) previous step.\n\n\\textbf{Simulation (I)} We create two classes for this simulation. In class one, only the first three communities (nodes 1--60) generate small temporal noises, and other nodes are only affected by neighbors. In class two, only the last three communities (nodes 141--200) generate small temporal noises, and other nodes are only affected by neighbors. We visualize the task aggregated  $\\operatorname{Attr}_X$ and $A_{\\text{adp}}$ and in \\cref{fig:syn_attr_X,fig:syn12_adp}. The signals are characterized well in $\\operatorname{Attr}_X$. For the generated series, signals are more important in node 1--60 for class 1 and 141--200 for class 2: $A_{\\text{adp}}$ finds this pattern and helps propagate signals in these regions better.\nWe notice that $\\operatorname{Attr}_A$ is mostly random, with no apparent patterns. This is consistent with the graph signal generation: when aggregating information from neighbors, all connected edges are weighted the same (binary); thus, the connections do not affect generated signals. We perform the following study to understand the opposite effect.\n\n\\textbf{Simulation (II)} We again create two classes for the simulation: in class one, connections from nodes 61--100 are strengthened; in class two, connections from nodes 101--140 are strengthened. The weights of strengthened edges are increased from 1 to 5 during signal generation. However, the model still takes in binary adjacency matrices as inputs (processed as mentioned in \\cref{sc:preliminaries} before feeding to the model). We visualize the task aggregated $A_{\\text{adp}}$ and $\\operatorname{Attr}_A$ in \\cref{fig:syn34_A}. This time the connection differences are reflected in $\\operatorname{Attr}_A$.\nSignals in node 61--100 for class 1 or 101--140 for class 2 are less important because stronger connections can send these signals out: this results in smaller values for corresponding columns in $A_{\\text{adp}}$. Combined with the previous simulation results, this suggests that strong signal sending regions \n% that have weak connections (or whose connections are not reflected in the graph adjacency matrix)\nor regions with weak connections that are over-reflected in the graph adjacency matrix \n% (true connections are weak)\ntend to have higher $A_{\\text{adp}}$ values. In other words, $A_{\\text{adp}}$ complements both signals and connections to encode latent dynamics, while attributions obtained from IG are better at interpreting the modalities separately.\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Conclusions",
                "content": "\nThis paper proposes ReBraiD, a high-performing and efficient graph neural network model that embeds both structural and dynamic functional signals for a more comprehensive representation of brain dynamics. To better capture latent structures, we propose sample-level adjacency matrix learning and multi-resolution inner cluster smoothing. Apart from quantitative results showing ReBraiD's superiority in representing brain activities, we also leverage integrated gradients to attribute and interpret the importance of both spatial brain regions and temporal keyframes. The attribution also reveals heterogeneities among brain regions (or subnetworks), tasks, and individuals. These findings can potentially reveal new neural basis, biomarkers of tasks or brain disorders when combined with behavioral metrics. They can also enable more fine-grained temporal analysis around keyframes when combined with other imaging techniques and extend to different scientific domains with sample (subject) heterogeneity.\n% \\vspace{-0.1cm}\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\\begin{acks}\nThis project was  partially supported by funding from the National Science Foundation under grant IIS-1817046.\n\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{sample-base}\n\n% \\newpage\n\\clearpage\n%%\n%% If your work has an appendix, this is the place to put it.\n\\appendix\n\n"
            },
            "section 5": {
                "name": "Models",
                "content": "\n% \\subsection{Dynamic functional connectivities.}\n% \\label{sec:dynamic_fc}\n% Fig. \\ref{fig:dynamic_fc} shows functional connectivities (FCs) among $N$ brain regions, where each $\\text{FC} \\in \\mathbb{R}^{N\\times N}$. The value at $\\text{FC}_{ij}$ is calculated as the Pearson correlation coefficient between signals of brain region $i$ and region $j$. The figure shows 6 FCs calculated from 6 consecutive sliding windows within a same fMRI session, with signal window length being 30 and sliding stride being 30. From the figure, we can clearly tell that FCs are highly dynamic.\n% \\begin{figure}[h]\n%     \\centering\n%     \\includegraphics[width=0.45\\textwidth]{fig/dynamic_fc.png}\n%     \\caption{Dynamic functional connectivities.}\n%     \\label{fig:dynamic_fc}\n%     \\vspace{-10pt}\n% \\end{figure}\n",
                "subsection 5.1": {
                    "name": "Choice of temporal layers",
                    "content": "\nFig. \\ref{fig:tcn} explains the choice of TCN layers.\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Regularization terms for soft-assignment",
                    "content": "\n\\label{ssc:pool_reg}\nFor each soft assignment matrix $S \\in \\mathbb{R}^{N \\times c \\times t}$ in \\cref{eq:diffpool}, we test three regularization terms:\n\\begin{itemize}[leftmargin=*]\n    \\item Similar to \\textsc{DiffPool}, to ensure a more clearly defined node assignment, namely each node is only assigned to few clusters (the closer to one the better), we minimize the entropy of single node assignments: $L_{E_1} = \\frac{1}{c} \\sum_{i=1}^{c}H(S_{i})$.\n    \\item To ensure a representation separation among nodes, meaning the assignment should not assign all the nodes a same way, we maximize the entropy of node assignment \\textit{patterns} across all nodes: $L_{E_2} = - \\frac{1}{c} \\sum_{i=1}^{c}H(\\sum_{j=1}^{n}S_{ij})$.\n    \\item To make the assignment along temporal axis smoother, we penalize assignment variances within a small time window $[\\hat{t}, \\hat{t}+\\tau]$: $L_T = \\frac{1}{t-\\tau} \\sum_{\\hat{t} = 0}^{t-\\tau}\\sigma(S_{[\\hat{t}, \\hat{t}+\\tau]})$, where $\\sigma$ represents standard deviation.\n\\end{itemize}\nTogether with cross entropy classification loss $L_{CE}$, the final loss function of the model becomes:\n\\begin{equation}\n    L_{reg} = \\alpha_1 L_{CE} + \\alpha_2 L_{E_1} + \\alpha_3 L_{E_2} + \\alpha_4 L_T,\\quad \\sum_i \\alpha_i = 1\n\\end{equation}\n\n"
                }
            },
            "section 6": {
                "name": "Experiments",
                "content": "\n\n",
                "subsection 6.1": {
                    "name": "Ablation studies",
                    "content": "\n\\label{sssc:ab_study}\n\nNumerical values of \\cref{fig:graph_supports} are reported in \\cref{tab:ab_study}. Training time ranges from 51 seconds / epoch for length-8 inputs to 298 seconds / epoch for length-256 inputs. Models converges to a relatively stable loss level within 20 epochs.\n\n\n\n\\textbf{(I) Number of GNN layers.}\nThe total number of temporal layers depends on the input signal length since each strided $\\operatorname{TCN}$ layer reduces the temporal length by a factor of two: if the input length is $2^i$, there need to be $i$ temporal layers. \\textit{But is alternating every TCN with GNN the best strategy, or do we only need to follow one GNN after a few TCNs}? We study this question with different input lengths. \n\nModel weighted F1 are plotted in \\cref{fig:number_gnn} for all possible $\\operatorname{GNN}$ to total $\\operatorname{TCN}$ ratios (e.g. length-256 inputs requires 8 $\\operatorname{TCN}$ layers. The possible ratios are $\\frac{1}{8}, \\frac{1}{4}, \\frac{1}{2}, 1$ since we can insert one $\\operatorname{GNN}$ per 8, 4, 2, 1 $\\operatorname{TCN}$ layers). The figure shows alternating every layer rarely yields the highest performance and the best ratio lies around one $\\operatorname{GNN}$ per two $\\operatorname{TCN}$ layers for our dataset. We repeat the experiment for $K=1,3$ (in \\cref{eq:gnn}) to rule out the possibility that this result is related to how many neighbors one $\\operatorname{GNN}$ layer can reach; we find they have roughly the same pattern as the $K=2$ case. We hypothesize that a lower $\\operatorname{GNN}$ to $\\operatorname{TCN}$ ratio does not capture enough spatial context, while higher ones might be overfitting. We leave exploring the relationship between this ratio and the number of nodes $N$ to a future study.\n\nThe best $\\operatorname{GNN}$ to $\\operatorname{TCN}$ ratio also depends on whether model incorporates latent adjacency matrices or not: without $A_{\\text{adp}}$, length-128 signals achieves its relative best (among all ratios) when having one $\\operatorname{GNN}$ per two $\\operatorname{TCN}$s, but it only needs one $\\operatorname{GNN}$ per three $\\operatorname{TCN}$s if using $A_{\\text{adp}}$. This shows learning latent structures $A_{\\text{adp}}$ not only improves overall model accuracy but can also reduce model parameters, thus complexity, in achieving better results.\n\n\n\n\\textbf{(II) Effects of soft-assignment cluster numbers.} During our experiments, we find that as long as the smoothing module is used, the final performance will be close to each other, only the convergence rates are different. Fig. \\ref{fig:c_num} shows how validation loss converges with different $c$ (cluster number) or when there is no smoothing module. From it, we can observe that halving the numbers (100-50-25-12) is the most helpful setting, and we use it for our other experiments; decreasing the numbers (160-120-80-40) or all larger numbers (all 100) works better than increasing the numbers (12-25-50-100) or all smaller numbers (all 12). With the inner cluster smoothing module, all cluster number settings converge to around 0.23 at their smallest when trained for 30 epochs; their test weighted F1 range from 89.47 (model with 12-25-50-100) to 90.85 (model with 100-50-25-12).\n\nOn the contrary, if no smoothing module is used, the model overfits easily, and the validation loss can only reach about 0.4 before going up (with the best set of learning rate and weight decay parameters found with grid search). Understandably, the model is prone to overfitting given the complexity of $\\operatorname{GNN}$ and the relatively small dataset size. However, our added inner cluster smoothing module effectively counters the effect and further brings the loss down in a stable manner. \n\n\n\n\n\n"
                },
                "subsection 6.2": {
                    "name": "Model comparisons",
                    "content": "\nWe plot confusion matrices of ReBraiD, the model from ablation study setting (viii), and the best performing graph baseline in \\cref{fig:cm}. Misclassification pairs clustered at the first three tasks (resting, VWM, DYN) and the latter three (DOT, MOD, PVT). Shown confusion matrices are from models trained on length-256 inputs. We note that these misclassification pairs may differ for models trained on other input lengths (like 128-frame, etc.).\n\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Attributions",
                    "content": "\n% \\subsection{IG $\\operatorname{Attr}_A$ and $\\operatorname{Attr}_X$}\n\\label{sssc:interp}\n% \\begin{figure}[h!]\n%     \\centering\n%     \\includegraphics[width=0.16\\textwidth]{fig/BNV/A0_s.png}\n%     \\includegraphics[width=0.16\\textwidth]{fig/BNV/A1_s.png}\n%     \\includegraphics[width=0.16\\textwidth]{fig/BNV/A2_s.png}\n%     \\includegraphics[width=0.16\\textwidth]{fig/BNV/A3_s.png}\n%     \\includegraphics[width=0.16\\textwidth]{fig/BNV/A4_s.png}\n%     \\includegraphics[width=0.16\\textwidth]{fig/BNV/A5_s.png}\n%     \\\\\n%     \\includegraphics[width=0.12\\textwidth]{fig/BNV/A0_a.png}\\hspace{+14pt}\n%     \\includegraphics[width=0.12\\textwidth]{fig/BNV/A1_a.png}\\hspace{+14pt}\n%     \\includegraphics[width=0.12\\textwidth]{fig/BNV/A2_a.png}\\hspace{+14pt}\n%     \\includegraphics[width=0.12\\textwidth]{fig/BNV/A3_a.png}\\hspace{+14pt}\n%     \\includegraphics[width=0.12\\textwidth]{fig/BNV/A4_a.png}\\hspace{+14pt}\n%     \\includegraphics[width=0.12\\textwidth]{fig/BNV/A5_a.png}\n%     \\caption{Important ROIs based on $\\operatorname{Attr}_A$. Tasks are: Resting, VWM, DYN, DOT, MOD, PVT from left to right. Node sizes are based on column sums of $\\operatorname{Attr}_A \\in \\mathbb{R}^{200\\times 200}$ and edge width are direcly based on $\\operatorname{Attr}_A$. For the visualization purpose, only edges with highest attributions are kept to ensure sparsity being 0.009 (down from around 0.196).}\n%     \\label{fig:bnv_A}\n% \\end{figure}\n\n% \\begin{figure}[h!]\n%     \\centering\n%     \\includegraphics[width=0.9\\textwidth]{fig/ig_attr_A/attr_A_tasks.png}\\\\\n%     \\includegraphics[width=0.9\\textwidth]{fig/ig_attr_A/attr_A_tasks_rand_SC.png}\n%     \\caption{Task-averaged $\\operatorname{\\textsc{Attr}}_{A} \\in \\mathbb{R}^{200\\times 200}$. The top row is obtained from real SC induced $A$, and the bottom row is obtained from random SC induced $A_{\\operatorname{rand}}$.}\n%     \\label{fig:attr_A_with_rand}\n% \\end{figure}\n\n% See \\cref{fig:bnv_A} for the visualization of important ROIs based on $\\operatorname{Attr}_A$, and \\cref{fig:attr_A_with_rand} for task-averaged $\\operatorname{Attr}_A$ under real and random $\\operatorname{SC}$ settings.\nMany discriminatory regions obtained from $\\operatorname{Attr}_A$ are consistent with existing literature:\\\\\n\\textbf{Resting state}: The top attributed ROIs belong to the default mode network, which is regarded salient during the resting state \\citep{raichle2015brain}.\\\\\n\\textbf{VWM}: The dominant attributions are from visual regions and posterior parietal regions, which complies with \\cite{todd2004capacity}.\\\\\n\\textbf{DYN}: Attributions from our model suggest regions along cingulate gyrus (defaultA-SalValAttnB-ContA-ContC-defaultC), as well as peripheral visual and somatomotor regions. Literature suggests anterior cingulate cortex (ACC) to be active \\citep{kim2016anterior} and posterior cingulate cortex (PCC) to be inactive \\citep{leech2014role} during visual attention tasks. This means both regions provide discriminative information about the DYN states, which is what our attribution method votes for.\\\\\n\\textbf{DOT}: Important ROIs from our analysis are located in control networks, in particular both ACC and PCC, as well as in the peripheral visual system. In the literature, dorsal and rostral regions of the ACC are proved to be involved with dot-probe performance \\citep{carlson2012nonconscious, carlson2013functional}.\\\\\n% The visual system should be related to the nature of the task.\n\\textbf{MOD}: Our important ROIs are mostly in temporal-parietal regions and default mode network (anatomically frontoparietal), and literature suggests similar regions: parietal \\citep{grabner2011brain} and prefrontal \\citep{friedrich2013mathematical}.\\\\\n\\textbf{PVT}: Our top attributed ROIs belong to control networks, attention networks, and somatomotor regions. This is similar to \\cite{drummond2005neural}, where both attention and motor systems are considered important.\n\n% To view brain regions in the 17-networks setting instead of 200-ROI parcellations, \\cref{tab:region_rank_A} has the subnetwork rankings based on column-average $\\operatorname{Attr}_A$ and \\cref{tab:region_rank_X} has the subnetwork rankings based on temporal-averaged $\\operatorname{Attr}_X$. For a visualization of the 17-network parcellation, please refer to \\cref{fig:17network}.\n\n% See \\cref{fig:ig_6tasks} for the complete attribution distributions for every task based on temporal-averaged $\\operatorname{Attr}_X$. Corresponding brain regions of a certain number are listed in \\cref{tab:number_region_map}. \n% For boxplots showing 17 regions, we combine LH and RH for their common network.\n\n% \\begin{table}[h]\n% \\centering\n% \\caption{Brain subnetworks in the 17-network definition.}\n% \\label{tab:number_region_map}\n% \\hspace{20pt}\n% \\begin{small}\n% \\begin{tabular}{cl|cl}\n% 1  & LH\\_VisCent           & 18 & RH\\_VisCent           \\\\\n% 2  & LH\\_VisPeri           & 19 & RH\\_VisPeri           \\\\\n% 3  & LH\\_SomMotA           & 20 & RH\\_SomMotA           \\\\\n% 4  & LH\\_SomMotB           & 21 & RH\\_SomMotB           \\\\\n% 5  & LH\\_DorsAttnA         & 22 & RH\\_DorsAttnA         \\\\\n% 6  & LH\\_DorsAttnB         & 23 & RH\\_DorsAttnB         \\\\\n% 7  & LH\\_SalVentAttnA      & 24 & RH\\_SalVentAttnA      \\\\\n% 8  & LH\\_SalVentAttnB      & 25 & RH\\_SalVentAttnB      \\\\\n% 9  & LH\\_LimbicB\\_OFC      & 26 & RH\\_LimbicB\\_OFC      \\\\\n% 10 & LH\\_LimbicA\\_TempPole & 27 & RH\\_LimbicA\\_TempPole \\\\\n% 11 & LH\\_ContA             & 28 & RH\\_ContA             \\\\\n% 12 & LH\\_ContB             & 29 & RH\\_ContB             \\\\\n% 13 & LH\\_ContC             & 30 & RH\\_ContC             \\\\\n% 14 & LH\\_DefaultA          & 31 & RH\\_DefaultA          \\\\\n% 15 & LH\\_DefaultB          & 32 & RH\\_DefaultB          \\\\\n% 16 & LH\\_DefaultC          & 33 & RH\\_DefaultC          \\\\\n% 17 & LH\\_TempPar           & 34 & RH\\_TempPar          \n% \\end{tabular}\n% \\end{small}\n% \\end{table}\n\n% \\begin{figure}[h]\n%     \\centering\n%     \\includegraphics[width=0.7\\textwidth]{fig/Network-parcellation-of-Yeos-17-networks-The-17-networks-include-the-following-regions.png}\n%     \\caption{Network parcellation of Yeo's 17-networks. Figure is from \\cite{kahali2021role}}\n%     \\label{fig:17network}\n% \\end{figure}\n\n% \\begin{figure}[h]\n% \\vspace{-1.3cm}\n%     \\centering\n%     \\begin{subfigure}[b]{0.45\\textwidth}\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task0-34.png}\\\\\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task0-17.png}\n%     \\caption{Resting state.}\n%     \\label{fig:ig_task0}\n%     \\end{subfigure}    \n%     \\begin{subfigure}[b]{0.45\\textwidth}\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task1-34.png}\\\\\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task1-17.png}\n%     \\caption{Visual working memory task.}\n%     \\label{fig:ig_task1}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.45\\textwidth}\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task2-34.png}\\\\\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task2-17.png}\n%     \\caption{Dynamic Attention Task.}\n%     \\label{fig:ig_task2}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.45\\textwidth}\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task3-34.png}\\\\\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task3-17.png}\n%     \\caption{Dot Probe Task.}\n%     \\label{fig:ig_task3}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.45\\textwidth}\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task4-34.png}\\\\\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task4-17.png}\n%     \\caption{Math Task.}\n%     \\label{fig:ig_task4}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.45\\textwidth}\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task5-34.png}\\\\\n%     \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task5-17.png}\n%     \\caption{Psychomotor vigilance task.}\n%     \\label{fig:ig_task5}\n%     \\end{subfigure}    \n%     \\caption{Brain subnetwork attribution distributions from $\\operatorname{Attr}_X$.}\n%     \\label{fig:ig_6tasks}\n% \\end{figure}\n\n"
                },
                "subsection 6.4": {
                    "name": "Inner cluster smoothing toy example.",
                    "content": "\n\\label{ssc:toy_example}\n\nHere we show a toy example demonstrating the inner cluster smoothing module described in \\cref{eq:diffpool,eq:smoothing}. Note that we will only show one time slice, and the same operation is done along every $t$: on a particular $t$, we have $Z\\in\\mathbb{R}^{N\\times d}, S\\in\\mathbb{R}^{N\\times c}$. We will use $N=3, c=2$ and node values $a,b,c\\in \\mathbb{R}^d$ for this toy example.\nIn addition, this example is just to illustrate the concept behind the smoothing operation, and $\\operatorname{Softmax}$ along the axis 1 is simplified as row normalization for a more straightforward presentation.\n\n\n\nIn this example, $1^{st}$ and $2^{nd}$ nodes are assigned to the first cluster, and $2^{nd}$ and $3^{rd}$ node are assigned to the second cluster. The final $H_{new}$ after our smoothing module will mingle the first two nodes' values, and the last two nodes' values (based on assignment weights) while keeping their original node number unchanged.\n\n"
                },
                "subsection 6.5": {
                    "name": "Task descriptions.",
                    "content": "\n\\label{ssc:task_des}\n% \\todo{it seems crash paper didn't have task descriptions, but should have these somewhere?? can directly cite it if find}\nThe following are task descriptions of CRASH (Cognitive Resilience and Sleep History) dataset:\n\n\\textbf{Resting state}: The subject simply lays in the scanner awake, with eyes open for 5 minutes.\n\n\\textbf{Visual working memory task (VWM)}: The subject is presented with a pattern of colored squares on a computer screen for a very brief period (100ms). After ~1000ms, they are presented with a single square and must determine if it is the same or different color as the previously presented square at that location. Responses are made with a button press (\\cite{luck1997capacity}).\n\n\\textbf{Dynamic Attention Task (DYN)}: Two streams of orientation gratings are presented to the left and right of fixation.  Subjects monitor specified stream for a target (about 2 degree shift in orientation, clockwise or counter clockwise) that indicates whether the subject should continue to monitor the current stream (hold) or monitor the other stream (shift) and respond with a button press (\\cite{yantis2002transient}).\n\n\\textbf{Dot Probe Task (Faces) (DOT)}: On each trial, two faces are presented, one neutral and the other happy or angry for 500ms. Then, either of two simple symbols is presented at the position of either of the faces. The subject must make a forced choice discrimination of the symbol. Reaction time differences as a function of the valance for the preceding facial expression are calculated.  There is increased variability of the bias with PTSD and fatigue (\\cite{sipos2014postdeployment}).\n\n\\textbf{Math task (MOD)}: Subjects perform a modular math computation every 8 seconds and respond with a yes or no button press. The object of modular arithmetic is to judge the validity of problems such as 51=19(mod 4). One way to solve it is to subtract the middle number from the first number (i.e., 51–19) and then divide this difference is by the last number (32/4). If the dividend is a whole number, the answer is “true.” Otherwise the answer is false (\\cite{mattarella2011choke}).\n\n\\textbf{Psychomotor vigilance task (PVT)}: The subject monitors the outline of a red circle on a computer screen for 10 minutes, and whenever a counter clockwise red sweep begins, they press a button as fast as possible.  Subjects are provided with response time feedback.  The experimenter records response latencies (\\cite{loh2004validity}).\n% Perceptual learning task: Subjects are shown synthetic computer generated mammograms with and without artificial \"lesions\". They make a yes/no response as to the presence or absence of a lesion.\n\n"
                }
            }
        },
        "tables": {
            "tab:data_detail": "\\begin{table}\n\\caption{fMRI scan details for six tasks.}\n\\vspace{-0.1cm}\n\\label{tab:data_detail}\n\\resizebox{0.48\\textwidth}{!}{%\n\\begin{tabular}{c|cccccc|c}\n\\toprule\nTasks               & Rest & VWM & DYN & DOT & MOD & PVT & (Total) \\\\\\hline\nValid sessions & 209     & 514 & 767 & 155 & 138 & 157 & 1940    \\\\\nFrames / Session       & 321     & 300 & 265 & 798 & 828 & 680 & ---\n\\\\\\bottomrule\n\\end{tabular}}\n\\end{table}",
            "tab:comparison": "\\begin{table}[t]\n    \\centering\n    % \\captionsetup{font=small}\n    % \\vspace{-1cm}\n    \\caption{Model comparisons with length-256 inputs.}\n    \\label{tab:comparison}\n    % \\begin{tabular}{c|c|c|c}\n    %     \\hline\n    %     Model & \\multicolumn{1}{c|}{\\begin{tabular}[c]{@{}c@{}}Accuracy\\\\ (\\%)\\end{tabular}} & Weighted F1 & \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Training time\\\\ (s / epoch)\\end{tabular}} \\\\\n    %     \\hline\n    %     GCN (\\cite{kipf2017semisupervised}) & 41.53 & 42.84 & 713\\\\\n    %     % GAT (\\cite{velickovic2018graph}) & 47.93 & 45.73 \\\\\n    %     GAT V2 (\\cite{brody2021attentive}) & 50.44 & 50.36 & 1142 \\\\\n    %     GConvGRU (\\cite{seo2018structured}) & 52.26 & 56.05 & 9886 \\\\\n    %     GraphSAGE (\\cite{hamilton2017inductive}) & 61.84 & 61.87 & 1048 \\\\\n    %     Graph Transformer (\\cite{shi2021masked}) & 66.51 & 66.11 & 1890 \\\\\\hline\n    %     \\textbf{ReBraiD} (proposed: TCN + GNN) & \\textbf{85.56} & \\textbf{90.85} & 298 \\\\\n    %     ReBraiD (TCN only) & 72.44 & 71.98 & 119 \\\\\n    %     ReBraiD (TCN + CNN) & 75.89 & 75.79 & 124\n    % \\end{tabular}\n    \\vspace{-0.1cm}\n    \\resizebox{0.47\\textwidth}{!}{%\n    \\begin{tabular}{c|c|c}\n        % \\hline\n        \\toprule\n        Model & Weighted F1 & \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Training time\\\\ (s / epoch)\\end{tabular}} \\\\\n        \\hline\n        GCN \\cite{kipf2017semisupervised} & 42.84 & 713\\\\\n        % GAT (\\cite{velickovic2018graph}) & 47.93 & 45.73 \\\\\n        GAT V2 \\cite{brody2021attentive} & 50.36 & 1142 \\\\\n        GConvGRU \\cite{seo2018structured} & 56.05 & 9886 \\\\\n        GraphSAGE \\cite{hamilton2017inductive} & 61.87 & 1048 \\\\\n        Graph Transformer \\cite{shi2021masked} & 66.11 & 1890 \\\\\\hline\n        MVTS Transformer \\cite{zerveas2021transformer} & 88.16 & \\textbf{39} \\\\\\hline\n        \\textbf{ReBraiD} (proposed: TCN + GNN) & \\textbf{90.85} & 298 \\\\\n        ReBraiD (TCN only) & 71.98 & 119 \\\\\n        ReBraiD (TCN + CNN) & 75.79 & 124\n        \\\\\\bottomrule\n    \\end{tabular}}\n    % \\vspace{-0.5cm}\n\\end{table}",
            "tab:ab_study": "\\begin{table}[h]\n    \\centering\n    \\captionsetup{font=small}\n    \\caption{Weighted F1 of ablation study settings.}\n    \\vspace{-0.2cm}\n    \\label{tab:ab_study}\n    \\resizebox{0.5\\textwidth}{!}{%\n    \\begin{tabular}{c|cccccc}\n    \\toprule\n    Input length (frames)  & 8      & 16     & 32     & 64     & 128    & 256    \\\\ \\hline\n    (i): SC + adp          & \\textbf{66.19} & \\textbf{70.18} & \\textbf{75.87} & \\textbf{76.14} & \\textbf{82.91} & \\textbf{90.85} \\\\\n    (ii): SC only          & 64.54 & 65.58 & 71.79 & 70.31 & 73.63 & 89.79 \\\\\n    (iii): adp only        & 64.32 & 65.20 & 74.01 & 71.42 & 80.63 & 89.46 \\\\\n    (iv): SC + FC          & 66.10 & 67.58 & 70.26 & 75.02 & 76.91 & 84.68 \\\\\n    (v): random adj   & 62.17 & 66.25 & 72.30 & 73.72 & 76.58 & 89.22 \\\\\\hline\n    (vi): (i) without smoothing & 63.57 & 62.82 & 70.19 & 65.82 & 72.91 & 79.65 \\\\\n    (vii): (v) without smoothing & 56.88 & 64.08 & 72.27 & 62.72 & 75.16 & 83.75 \\\\\n    (viii): coarsened graph & 37.92 & 42.23 & 46.18 & 52.12 & 57.17 & 64.25\n    \\\\\\bottomrule\n    \\end{tabular}}\n\\end{table}"
        },
        "figures": {
            "fig:model_structure": "\\begin{figure}[t]\n    \\centering\n    \\captionsetup{font=small}\n    % \\vspace{-0.5cm}\n    \\includegraphics[width=0.5\\textwidth]{fig/model/model_with_sc.jpg}\n    \\caption[Caption for LOF]{The proposed ReBraiD model for integrating brain structure and dynamics (the architecture shown is for classification). For each batch with batch size $B$, input $X$ has a dimension of \\begin{small}$(B, 1, N, T)$\\end{small}\\footnotemark, and $A, A_{\\text{adp}}$ both have the dimension \\begin{small}$(B, N, N)$\\end{small}. The encoder (green part) encodes temporal and spatial information alternately, producing a latent representation in \\begin{small}$(B, d_{\\text{latent}}, N, 1)$\\end{small}. These embeddings are followed by linear layers for pooling and classification. The final output has a dimension of \\begin{small}$(B, C)$\\end{small}.}\n    \\label{fig:model_structure}\n\\end{figure}",
            "fig:graph_supports": "\\begin{figure}[t]\n    \\centering\n    \\captionsetup{font=small}\n    % \\begin{subfigure}[b]{0.45\\textwidth}\n    \\centering\n    \\vspace{-0.2cm}\n    \\includegraphics[ width=0.47\\textwidth]{fig/F1.png}\n    % \\caption{}\n    \n    % \\end{subfigure}\n    % \\begin{subfigure}[b]{0.32\\textwidth}\n    % \\centering\n    % % \\vspace{-2cm}\n    % \\includegraphics[trim={0.65cm 0 1cm 0.5cm}, clip, width=\\textwidth]{fig/number_gnn.png}\n    % \\caption{}\n    % \\label{fig:number_gnn}\n    % \\end{subfigure}\n    \\vspace{-0.1cm}\n    \\caption{\n    % \\ref{fig:graph_supports}:\n    Ablation studies on different input length (please see \\cref{tab:ab_study} in appendix for numerical values of weighted F1 under each setting).\n    % ; \\ref{fig:number_gnn}: Choosing number of GNN to TCN layer ratio.\n    }\n    % \\label{fig:model_components}\n    \\label{fig:graph_supports}\n    % \\vspace{-0.2cm}\n\\end{figure}",
            "fig:ig": "\\begin{figure}[t]\n    \\centering\n    \\captionsetup{font=small}\n    % \\vspace{-1cm}\n    \\begin{subfigure}[b]{0.14\\textwidth}\n    \\includegraphics[width=\\textwidth]{fig/t_sanity_check.png}\n    \\caption{}\n    \\label{fig:ig_sanity_check}\n    \\end{subfigure}    \n    \\begin{subfigure}[b]{0.32\\textwidth}\n    \\centering\n    % \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task1-34.png}\\\\\n    \\includegraphics[width=\\textwidth]{fig/ig_boxplots/ig-task1-17.png}\n    \\caption{}\n    \\label{fig:ig_task}\n    \\end{subfigure}\n    \\caption{\n    % (\\ref{fig:ig_sanity_check})\n    (a) Temporal importance sanity check of IG results on two pieces of inputs with a large overlap period. Attribution maps are offset aligned. \n    % (\\ref{fig:ig_task})\n    (b) $\\operatorname{\\textsc{Attr}}_{X}$ distributions across 17 brain subnetworks (defined as in \\cite{thomas2011organization}) for VWM.\n    % where the upper one separates left and right hemispheres and the lower one combines them \n    % left and right hemispheres regions are combined (e.g. LH\\_VisCent and RH\\_VisCent are combined to region VisCent).\n    % Refer to \\cref{tab:number_region_map} in appendix to see brain region names that the x-axis numbers represent.\n    }\n    \\label{fig:ig}\n    \\vspace{-0.5cm}\n\\end{figure}",
            "fig:attr_A_with_rand_colsum": "\\begin{figure*}[t]\n    \\centering\n    \\captionsetup{font=small}\n    % \\vspace{-0.8cm}\n    \\includegraphics[trim={10pt 0 6pt 0}, clip, width=0.9\\textwidth]{fig/ig_attr_A/attr_A_nets_col.png}\\\\\n    \\includegraphics[trim={10pt 0 6pt 0}, clip, width=0.9\\textwidth]{fig/ig_attr_A/attr_A_nets_rand_SC.png}\n    \\caption{Column averages of task-averaged $\\operatorname{\\textsc{Attr}}_{A}$ (mapped into 34 subnetworks defined by the 17-network parcellation with left, right hemispheres). Top row is obtained from real SC induced $A$ and bottom rows is obtained from random SC induced $A_{\\operatorname{rand}}$. Attributions are normalized to $[0, 1]$. Tasks are: Rest, VWM, DYN, DOT, MOD, PVT from left to right.}\n    % \\vspace{-0.2cm}\n    \\label{fig:attr_A_with_rand_colsum}\n\\end{figure*}",
            "fig:bnv_XAcombined": "\\begin{figure*}[t]\n    \\centering\n    \\captionsetup{font=small}\n    % \\vspace{-1cm}\n    \\includegraphics[width=0.15\\textwidth]{fig/BNV/X0_s.png}\n    \\includegraphics[width=0.15\\textwidth]{fig/BNV/X1_s.png}\n    \\includegraphics[width=0.15\\textwidth]{fig/BNV/X2_s.png}\n    \\includegraphics[width=0.15\\textwidth]{fig/BNV/X3_s.png}\n    \\includegraphics[width=0.15\\textwidth]{fig/BNV/X4_s.png}\n    \\includegraphics[width=0.15\\textwidth]{fig/BNV/X5_s.png}\n    \\includegraphics[width=0.015\\textwidth]{fig/BNV/edge_cmap2.png}\n    \\\\\n    \\includegraphics[width=0.115\\textwidth]{fig/BNV/X0_a.png}\\hspace{+13pt}\n    \\includegraphics[width=0.115\\textwidth]{fig/BNV/X1_a.png}\\hspace{+13pt}\n    \\includegraphics[width=0.115\\textwidth]{fig/BNV/X2_a.png}\\hspace{+13pt}\n    \\includegraphics[width=0.115\\textwidth]{fig/BNV/X3_a.png}\\hspace{+13pt}\n    \\includegraphics[width=0.115\\textwidth]{fig/BNV/X4_a.png}\\hspace{+13pt}\n    \\includegraphics[width=0.115\\textwidth]{fig/BNV/X5_a.png}\\hspace{+13pt}\n    \\includegraphics[width=0.017\\textwidth]{fig/BNV/node_cmap2.png}\n    % \\includegraphics[width=0.15\\textwidth]{fig/BNV/x5_a_cl.png}\n    % \\\\\n    % \\includegraphics[width=0.14\\textwidth]{fig/BNV/X0_c.png}\\hspace{+10pt}\n    % \\includegraphics[width=0.14\\textwidth]{fig/BNV/X1_c.png}\\hspace{+10pt}\n    % \\includegraphics[width=0.14\\textwidth]{fig/BNV/X2_c.png}\\hspace{+10pt}\n    % \\includegraphics[width=0.14\\textwidth]{fig/BNV/X3_c.png}\\hspace{+10pt}\n    % \\includegraphics[width=0.14\\textwidth]{fig/BNV/X4_c.png}\\hspace{+10pt}\n    % \\includegraphics[width=0.14\\textwidth]{fig/BNV/X5_c.png}\\hspace{+6pt}\n    % \\includegraphics[width=0.02\\textwidth]{fig/BNV/node_cmap2.png}\n    \\caption{ROI attributions from $\\operatorname{\\textsc{Attr}}_{A}$ and $\\operatorname{\\textsc{Attr}}_{X}$. (Task order is the same as \\cref{fig:attr_A_with_rand_colsum}). Edge color and width are based on task-averaged $\\operatorname{\\textsc{Attr}}_{A}\\in \\mathbb{R}^{200\\times200}$, and node color and size are based on task and temporal-averaged $\\operatorname{\\textsc{Attr}}_{X}\\in\\mathbb{R}^{200}$. For visualization, only edges with highest attributions are shown (the resulting sparsity reduces to 0.009 from 0.196).\n    % For ROI attributions based only on $\\operatorname{\\textsc{Attr}}_{A}$ where important sender ROIs are reflected by node sizes, please refer to \\cref{fig:bnv_A} in appendix.\n    }\n    \\label{fig:bnv_XAcombined}\n\\end{figure*}",
            "fig:hetero": "\\begin{figure}[h]\n    \\centering\n    \\captionsetup{font=small}\n    % \\vspace{-1cm}\n    \\includegraphics[width=0.235\\textwidth]{fig/ig_boxplots/three_subj_task1.png}\n    % \\hfill\n    % \\quad\n    \\includegraphics[width=0.235\\textwidth]{fig/ig_boxplots/three_subj_task4.png}\n    \\caption{34 subnetworks' $\\operatorname{\\textsc{Attr}}_{X}$ distributions of 3 subjects performing the VWM task (left) and the MOD task (right). Outliers that go beyond \\begin{small} $[Q1 - 1.5 \\operatorname{IQR}, Q3 + 1.5 \\operatorname{IQR}]$\n    \\end{small} are omitted. VWM has a much smaller average attribution variance than MOD.}\n    \\label{fig:hetero}\n    \\vspace{-0.3cm}\n\\end{figure}",
            "fig:syn": "\\begin{figure}\n    \\centering\n    \\captionsetup{font=small}\n    \\begin{subfigure}[b]{0.115\\textwidth}\n    \\includegraphics[width=\\textwidth]{fig/syn/syn_A.png}\n    \\caption{A}\n    \\label{fig:syn_A}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.355\\textwidth}\n    \\includegraphics[width=0.48\\textwidth]{fig/syn/syn_attr1.png}\n    \\includegraphics[width=0.48\\textwidth]{fig/syn/syn_attr2.png} \n    \\caption{Simulation (I) $\\operatorname{Attr}_X$ of 200 nodes}\n    \\label{fig:syn_attr_X}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.505\\textwidth}\n    \\includegraphics[width=0.35\\textwidth]{fig/syn/syn_adp.png}\n    \\includegraphics[width=0.62\\textwidth]{fig/syn/syn_adp_mean.png}\n    \\caption{Simulation (I) $A_{\\text{adp}}$ of 200 nodes}\n    \\label{fig:syn12_adp}\n    \\end{subfigure}\n    \\centering\n    \\begin{subfigure}[b]{0.5\\textwidth}\n    \\includegraphics[width=0.37\\textwidth]{fig/syn/syn34_adp.png}\n    \\includegraphics[trim={0 8pt 0 0}, clip,width=0.29\\textwidth]{fig/syn/syn34_A1.png}\n    \\includegraphics[trim={0 8pt 0 0}, clip, width=0.29\\textwidth]{fig/syn/syn34_A2.png}\n    \\caption{Simulation (II) $A_{\\text{adp}}$ and $\\operatorname{Attr}_A$ of 200 nodes}\n    \\label{fig:syn34_A}\n    \\end{subfigure}\n    \\caption{\n    % (\\ref{fig:syn_A})\n    (a) A typical adjacency matrix for simulated graph signals.\n    % (\\ref{fig:syn_attr_X})\n    (b) Task averaged $\\operatorname{Attr}_X$ of simulation (I). Attribution values are normalized.\n    % (\\ref{fig:syn12_adp})\n    (c) Task averaged $A_{\\text{adp}}$ of simulation (I) and its entry averages per column.\n    % (\\ref{fig:syn34_A})\n    (d) Task averaged $A_{\\text{adp}}$ and task averaged $\\operatorname{Attr}_A$ of simulation (II). Attribution values are normalized.}\n    \\label{fig:syn}\n    \\vspace{-0.3cm}\n\\end{figure}",
            "fig:tcn": "\\begin{figure}[h!]\n    \\centering\n    \\captionsetup{font=small}\n    % \\vspace{-1.1cm}\n    \\includegraphics[width=0.5\\textwidth]{fig/tcn.pdf}\n    \\vspace{-0.5cm}\n    \\caption{Comparison of strided non-causal TCN (left) and dilated causal TCN (right). For a causal TCN, the causal aspect is achieved through padding $(\\text{kernel\\_size} - 1) \\times \\text{dilation}$ number of zeros to the layer's input. The resulting $\\mathbf{y}$ always has the same length as input $\\mathbf{x}$, in which $\\mathbf{y}_\\tau$ only depends on inputs $\\mathbf{x}_{t\\leq\\tau}$. We can view strided non-causal TCN as the rightmost node of a dilated causal TCN.}\n    \\label{fig:tcn}\n    \\vspace{-0.5cm}\n\\end{figure}",
            "fig:number_gnn": "\\begin{figure}\n    \\centering\n    \\captionsetup{font=small}\n        \\includegraphics[trim={0.65cm 0 1cm 0.5cm}, clip, width=0.25\\textwidth]{fig/number_gnn.png}\n    \\vspace{-0.2cm}\n    \\caption{Choosing number of GNN to TCN layer ratio for different input lengths. In most cases, two TCN layers per GNN layer results in the best model performance in terms of F1.}\n    \\label{fig:number_gnn}\n    \\vspace{-0.5cm}\n\\end{figure}",
            "fig:c_effect": "\\begin{figure}\n    % \\vspace{-20pt}\n    \\centering\n    \\captionsetup{font=small}\n    \\begin{subfigure}[b]{0.236\\textwidth}\n    \\includegraphics[trim={20pt 0 20pt 10pt}, clip, width=\\textwidth]{fig/grid_search.png}\n    \\caption{}\n    \\label{fig:grid_search}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.236\\textwidth}\n    \\includegraphics[trim={20pt 10pt 20pt 10pt}, clip, width=\\textwidth]{fig/c_num.png}  \n    \\caption{}\n    \\label{fig:c_num}\n    \\end{subfigure}\n    \\vspace{-0.5cm}\n    \\caption{\n    % (\\ref{fig:grid_search})\n    (a) adding inner cluster smoothing or input-dependent adaptive adjacency matrix makes the model more stable across various learning rates (results shown are from length-16 inputs).\n    % (\\ref{fig:c_num})\n    (b) Validation loss v.s. training epochs. Input length is 256, and four smoothing modules are used. Legends are the soft-assignment cluster numbers of the four smoothing modules. Our other experiments use decreasing cluster numbers that halved at each module, corresponding to the 100-50-25-12 choice here.}\n    \\vspace{-0.3cm}\n    \\label{fig:c_effect}\n\\end{figure}",
            "fig:adp": "\\begin{figure}\n    \\centering\n    \\captionsetup{font=small}\n    \\begin{subfigure}{0.38\\textwidth}\n    \\includegraphics[width=\\textwidth]{fig/adp/adp_ind_task3_1.jpg}\n    \\caption{}\n    \\label{fig:adp_ind_task3}\n    \\end{subfigure}\n    \\begin{subfigure}{0.38\\textwidth}\n    \\includegraphics[width=\\textwidth]{fig/adp/adp_ind_task3_same_ses_1.jpg}\n    \\caption{}\n    \\label{fig:adp_ind_task3_same_ses}\n    \\end{subfigure}    \n    % \\begin{subfigure}{\\textwidth}\n    % \\includegraphics[width=\\textwidth]{fig/adp/adp_task_avg.png}\n    % \\caption{Task-averaged $A_{\\text{adp}} \\in \\mathbb{R}^{200\\times 200}$ for resting state, VWM, DYN, DOT, MOD, PVT.}\n    % \\label{fig:adp_task_avg}\n    % \\end{subfigure}\n    \\begin{subfigure}{0.44\\textwidth}\n    \\includegraphics[width=\\textwidth]{fig/adp/adp_task_avg_roi1.png}\n    \\includegraphics[width=\\textwidth]{fig/adp/adp_task_avg_roi2.png}\n    \\caption{}\n    \\label{fig:adp_task_avg_roi}\n    \\end{subfigure}\n    \\begin{subfigure}{0.47\\textwidth}\n    \\includegraphics[width=0.235\\textwidth]{fig/nodevec/tsne_node2_5dvec_s1_1.png}    \n    \\includegraphics[width=0.235\\textwidth]{fig/nodevec/tsne_node156_5dvec_s1_1.png}\n    \\includegraphics[width=0.235\\textwidth]{fig/nodevec/tsne_resting_2nodes_s1_1.png}\n    \\includegraphics[trim={0 5pt 0 20pt}, clip, width=0.235\\textwidth]{fig/nodevec/tsne_resting_2nodes_s2_1.png}\n    \\caption{}\n    \\label{fig:nodevec}\n    \\end{subfigure}    \n    % \\begin{subfigure}{\\textwidth}\n    % \\includegraphics[width=\\textwidth]{fig/adp/adp_task_avg_subnet.png}\n    % \\caption{34 (17 with LR) subnetworks' column averages of task-averaged $A_{\\text{adp}}$. Task order is the same as (c).}\n    % \\label{fig:adp_task_avg_subnet}\n    % \\end{subfigure}\n    \\vspace{-0.3cm}\n    \\caption{Learned latent adaptive adjacency matrices.\n    % (\\ref{fig:adp_ind_task3})\n    (a) $A_{i\\_\\text{adp}}$ of 3 randomly sampled inputs during the DOT task.\n    % (\\ref{fig:adp_ind_task3_same_ses})\n    (b) $A_{i\\_\\text{adp}}$ of 3 consecutive inputs from a same session during the DOT task. \n    % (\\ref{fig:adp_task_avg_roi})\n    (c) column averages of task-averaged $A_{\\text{adp}}$ for resting state, VWM, DYN, DOT, MOD, PVT. \n    % (\\ref{fig:nodevec})\n    (d) left two: t-SNE of $X^{(\\text{node-2, 156})} \\Theta_{\\text {adp }}$ in six tasks of one subject; right two: t-SNE of $X^{(\\text{node-155, 156})} \\Theta_{\\text {adp }}$  during the resting state of two subjects (multiple sessions are aggregated).}\n    \\label{fig:adp}\n    % \\vspace{-0.2cm}\n\\end{figure}",
            "fig:cm": "\\begin{figure}[t]\n    \\centering\n    \\captionsetup{font=small}\n    \\begin{subfigure}[b]{0.156\\textwidth}\n    \\includegraphics[width=\\textwidth]{fig/cm_256.png}\n    \\caption{}\n    \\label{fig:cm_256}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.156\\textwidth}\n    \\includegraphics[width=\\textwidth]{fig/cm_coarsened.png}\n    \\caption{}\n    \\label{fig:cm_coarsened}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.156\\textwidth}\n    \\includegraphics[width=\\textwidth]{fig/cm_TG.png}\n    \\caption{}\n    \\label{fig:cm_TG}\n    \\end{subfigure}\n    % \\vspace{-0.2cm}\n    \\caption{Confusion matrices of:\n    % (\\ref{fig:cm_256})\n    (a) ReBraiD (our proposed model),\n    % (\\ref{fig:cm_coarsened})\n    (b) model with coarsened graph (setting (viii)),\n    % (\\ref{fig:cm_TG})\n    (c) Graph Transformer (best graph baseline). Tasks are 1-Rest, 2-VWM, 3-DYN, 4-DOT, 5-MOD, 6-PVT.}\n    \\vspace{-0.2cm}\n    \\label{fig:cm}\n\\end{figure}",
            "fig:toy_example": "\\begin{figure} [h]\n    % \\vspace{-5pt}\n    \\centering\n    \\begin{minipage}[t]{0.4\\textwidth}\n    \\includegraphics[trim={0 0 0 3.3cm}, clip, width=0.4\\textwidth]{fig/toy_example.png}\n    \\end{minipage}\n    % \\quad\n    \\begin{minipage}[t]{0.1\\textwidth}\n      \\vspace{-2.4cm}\\(\\text{\\quad cluster assignment}\\)\n    \\end{minipage}\n    \\begin{minipage}[t]{0.38\\textwidth}\n    \\vspace{-0.7cm}\n    \\[\n    \\begin{aligned}\n    &Z=\\left(\\begin{array}{l}\n    a \\\\\n    b \\\\\n    c\n    \\end{array}\\right), S=\\left(\\begin{array}{ll}\n    1 & 0 \\\\\n    \\frac{1}{2} & \\frac{1}{2} \\\\\n    0 & 1\n    \\end{array}\\right) \\Rightarrow \\tilde{H}=S^{\\top} Z=\\left(\\begin{array}{l}\n    a+\\frac{1}{2} b \\\\\n    \\frac{1}{2} b+c\n    \\end{array}\\right) \\\\\n    &\\tilde{S}=\\text { row-normalized }\\left(S^{T}\\right)=\\left(\\begin{array}{ccc}\n    \\frac{2}{3} & \\frac{1}{3} & 0 \\\\\n    0 & \\frac{1}{3} & \\frac{2}{3}\n    \\end{array}\\right) \\\\\n    & \\Rightarrow H_{new} = \\tilde{S}^{\\top} H=\\left(\\begin{array}{l}\n    \\frac{2}{3} a+\\frac{1}{3} b \\\\\n    \\frac{1}{3} a+\\frac{1}{3} b+\\frac{1}{3} c \\\\\n    \\frac{1}{3} b+\\frac{2}{3} c\n    \\end{array}\\right)\n    \\end{aligned}\n    \\]\n    \\end{minipage}\n    \\caption{Inner cluster smoothing toy example.}\n    \\label{fig:toy_example}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    L_{reg} = \\alpha_1 L_{CE} + \\alpha_2 L_{E_1} + \\alpha_3 L_{E_2} + \\alpha_4 L_T,\\quad \\sum_i \\alpha_i = 1\n\\end{equation}"
        },
        "git_link": "https://github.com/ThomasYeoLab/CBIG/"
    }
}