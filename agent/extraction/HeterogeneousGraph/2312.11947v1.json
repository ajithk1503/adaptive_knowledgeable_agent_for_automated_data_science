{
    "meta_info": {
        "title": "Emotion Rendering for Conversational Speech Synthesis with Heterogeneous\n  Graph-Based Context Modeling",
        "abstract": "Conversational Speech Synthesis (CSS) aims to accurately express an utterance\nwith the appropriate prosody and emotional inflection within a conversational\nsetting. While recognising the significance of CSS task, the prior studies have\nnot thoroughly investigated the emotional expressiveness problems due to the\nscarcity of emotional conversational datasets and the difficulty of stateful\nemotion modeling. In this paper, we propose a novel emotional CSS model, termed\nECSS, that includes two main components: 1) to enhance emotion understanding,\nwe introduce a heterogeneous graph-based emotional context modeling mechanism,\nwhich takes the multi-source dialogue history as input to model the dialogue\ncontext and learn the emotion cues from the context; 2) to achieve emotion\nrendering, we employ a contrastive learning-based emotion renderer module to\ninfer the accurate emotion style for the target utterance. To address the issue\nof data scarcity, we meticulously create emotional labels in terms of category\nand intensity, and annotate additional emotional information on the existing\nconversational dataset (DailyTalk). Both objective and subjective evaluations\nsuggest that our model outperforms the baseline models in understanding and\nrendering emotions. These evaluations also underscore the importance of\ncomprehensive emotional annotations. Code and audio samples can be found at:\nhttps://github.com/walker-hyf/ECSS.",
        "author": "Rui Liu, Yifan Hu, Yi Ren, Xiang Yin, Haizhou Li",
        "link": "http://arxiv.org/abs/2312.11947v1",
        "category": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "additional_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n% \\hl{[Para 1]} \\textcolor{red}{Introduce what is \"Conversational Speech Synthesis\"?. highlight its importance.}  \\textcolor{blue}{what is the conversational TTS?}\n\n\n\n\n\n\n\n\n\nConversational speech synthesis (CSS) aims to express a target utterance with the proper linguistic and affective prosody in a conversational context \\cite{DBLP:journals/corr/abs-2005-10438}.\nWith the development of human-machine conversations, CSS has become an integral part of intelligent interactive systems \\cite{tulshan2019survey,zhou2020design,seaborn2021voice,mctear2022conversational} and plays an important role in areas such as virtual assistants and voice agents, etc.\n\n\n% speech synthesis technology is rapidly changing. Interaction systems in various conversational scenarios, such as conversational agents and intelligent customer service, have flooded people's daily lives. \n% However, these systems utilize generic speech synthesis techniques that no longer meet users' needs. To provide users with a better and more immersive interactive experience, the speech synthesized by the interactive system needs to contain richer prosody and fuller emotions.\n\n\n\n% \\hl{[para 2]} \\textcolor{red}{Traditional Conversational TTS works can be summarized as following categories:1) 2) 3) .../}\n\nUnlike the speech synthesis technology for single utterance that just predict the speaking style according to its linguistic content \\cite{wang2017tacotron,li2019neural,ren2021fastspeech,kim2021conditional,9271923,9420276,liu2021fasttalker,9767637} or attempt to transfer the style information from an additional reference speech \\cite{wang2018style,9747987,huang2022generspeech}, CSS methods usually infer the speaking style of the target utterance according to the dialogue interaction history between two interlocutors. \nTraditional CSS works attempt to acquire the speaking style information from various aspects, such as inter and intra-speaker \\cite{li2022enhancing}, and multi-modal context \\cite{li2022enhancing,li2022inferring,xue2023m} dependencies modeling, etc.\nFor example, \\citet{DBLP:journals/corr/abs-2005-10438} construct a coarse-grained context encoder at the sentence level and use the recurrent neural network (RNN) for dialogue history encoding. \n% \\cite{xue2023m} used fine-grained (word-level or phoneme-level) and coarse-grained (sentence-level) information for dialog context modeling.\n\\citet{li2022enhancing} model inter-speaker and intra-speaker dependencies in a conversation by a dialog graph convolutional neural (GCN) network and summarize the output of the graph neural network using the attention mechanism.\nIn more recent studies, \\citet{li2022inferring} propose a multi-scale relational graph convolutional network (MRGCN) to learn the dependencies in conversations at both global and local scales among the multi-modal information. \nThe above researches contribute to understanding the conversational context and determining appropriate speaking styles in synthesized speeches.\n\n% Traditional CSS works falls into four categories: 1) coarse and fine-grained information context modeling. [1] constructed a coarse-grained context encoder at the sentence level and used an RNN recurrent neural network for sequential encoding. [2] used fine-grained (word-level or phoneme-level) and coarse-grained (sentence-level) information for dialog context modeling. 2) Paralinguistic modeling for predicting fills and pauses. [Guo] constructed a paralinguistic encoder to predict spontaneous behaviors such as repetitions and fillers by counting information about the semantics and syntax of the text. \n\n\n\nHowever, \\textbf{emotion understanding} and \\textbf{emotion rendering} are largely missing in prior CSS research due to the scarcity of emotional conversational datasets and the difficulty of stateful emotion modeling.\nIn human-computer conversations, modeling the emotional expression in the conversational context is crucial for the speech synthesis system to generate speech with the appropriate emotional states and improve the user experience in speech-based interactions.\n\\textcolor{black}{As illustrated by the example in Fig.\\ref{fig1}, the current utterance in Fig. \\ref{fig1}(a) performs the ``Surprise'' emotion when the emotion flow of dialogue history is ``Happy $\\rightarrow$ Happy $\\rightarrow$ Surprise $\\rightarrow$ Happy'', while in Fig. \\ref{fig1}(b) performs ``Disgust'' emotion when emotion flow is ``Angry $\\rightarrow$ Happy $\\rightarrow$ Angry $\\rightarrow$ Angry''.} We can conclude that the emotional expression in context can directly affect the speaking style for the target utterance.\nIn addition, humans tend to have multiple emotions with varying intensities, such as weak, medium and strong, while expressing their thoughts and feelings \\cite{firdaus-etal-2020-meisd,9747098,liu2022accurate,9778970}. Therefore, emotion intensity also has an essential impact on speech expressiveness.\nIn a nutshell, how to fully understand the emotional cues of contextual information, and based on this, adequate emotional rendering in synthesizing conversational speech will be the focus of this paper.\nLast but not least, the existing emotion-aware multimodal data \\cite{busso2008iemocap,DBLP:journals/corr/abs-1810-02508,saganowski2022emognition,dias2022cross,10095836} are mostly targeted at emotion recognition scenarios, in which the speech fidelity of the audio modality is not high enough to meet the data requirements of conversational speech synthesis, resulting in the problem of data scarcity.\n\n\n\n\n% while previous works just focus on enhancing the prosody of synthesized speech and ignore the emotion rendering.\n% \\hl{[Para 3]} \\textcolor{red}{However, the above works suffer from two issues: 1) ignoring the dependency among the multi-modal signal of dialogue history.  2) ignoring the emotion rendering.} However, these approaches face two main problems: 1) Ignoring the dependency among the multi-modal signal of dialogue history. The human-to-human dialog contains the exchange of emotions, ideas, thoughts, experiences, and other aspects and is expressed through sounds, semantics, and emotions. A sentence in a dialog often corresponds to information of multiple modalities, and thus this information also has direct correlations and correspondences. 2) Ignoring the emotion rendering.  The interactive system's ability to accurately indicate emotion will make the synthesized speech more relevant to the current conversational context, enhancing the user experience.\n\n\n \n\n\n\n\n% \\hl{[para 4]} \\textcolor{red}{We introduce our method. xxxxx.} \n\nTo address the above challenges, we propose a novel emotional CSS model, termed \\textbf{ECSS}, that includes two novel mechanisms: 1) to enhance emotion understanding for the context utterances, \\textit{heterogeneous graph-based emotional context modeling} module is proposed to learn the emotion cues of emotional conversational context. Specifically, given the multi-modal context in a conversation, the text, audio, speaker, emotion, and emotion intensity information are treated as the multi-source knowledge and used to build the nodes of the heterogeneous Emotional Conversational Graph called \\textbf{ECG}. Then, the graph encoding module adopts the Heterogeneous Graph Transformer (HGT) \\cite{hu2020heterogeneous} as the backbone to learn complex emotional dependencies in the context, and to learn the impact of such complex dependencies on the emotional expression of the current utterance;\n2) to achieve the emotion rendering for the current utterance, we employ a contrastive learning-based emotion renderer module to infer the accurate emotion style for the target utterance.\nSpecifically, the emotion renderer takes the emotion-aware graph-enhanced node features from ECG as input and predicts the appropriate emotion, emotion intensity, and prosody features for the current utterance. This information is later aggregated with the content and speaker representations of the current utterance into the acoustic decoder to synthesize the final emotional conversational speech.\nNote that the new contrastive learning losses are used to enhance the differentiation of emotion and emotional intensity expressions by drawing the same categories of emotion (or intensity) closer together and pushing different categories farther apart.\nIt's worth noting that, to guarantee the successful development of the ECSS model, we designed seven emotion labels (happy, sad, angry, disgust, fear, surprise, neutral), and three emotion intensity labels (weak, medium, strong) for a recent expressive conversational speech synthesis dataset, DailyTalk \\cite{lee2023dailytalk}, and invited professional practitioners to annotate the labels. All annotated data will be open-sourced.\nThe main contributions of this paper include:\n\\begin{itemize}\n\\vspace{-1mm}\n\\item We propose a novel emotional conversational speech synthesis model, termed ECSS. To our best knowledge, this is the first in-depth conversational speech synthesis study that models emotional expressiveness.\n\\vspace{-1mm}\n\\item The proposed heterogeneous graph-based emotional context modeling and emotion rendering mechanisms ensure the accurate generation of emotional conversational speech in terms of emotion understanding and expression, respectively.\n\\vspace{-1mm}\n\\item Objective and subjective experiments show that the proposed model outperforms all state-of-the-art baselines in terms of emotional expressiveness.\n\\vspace{-1mm}\n% , thus demonstrating the effectiveness of the proposed approach. The ablation studies further reflect the contributions of heterogeneous graph-based multi-modal emotional context modeling and emotion rendering mechanisms. \n\\end{itemize}\n\n% The rest of this paper is organized as follows. In Section II, we discuss the related works. In Section III, we propose the ERCSS model. We report the experimental results in Section IV. Finally, Section V concludes the study.\n\n"
            },
            "section 2": {
                "name": "Related works",
                "content": "\n\n% \\textcolor{red}{(Some related works about Conversational TTS.\n% Highlight the difference between ours and those works.)}\n\n% \\textcolor{red}{Para1: NLP}.\n\nThe emotion modeling for conversation has been studied in both natural language processing (NLP) and speech processing fields.\nIn NLP field, \n% \\citet{zhou2018emotional} uses three kinds of knowledge, including emotion category embedding, internal emotion memory, and external memory, to model the emotional impact in large-scale conversation generation.\n\\citet{DBLP:journals/corr/abs-1909-10681} use context-aware emotion graph attention mechanisms to utilize external commonsense knowledge for emotion recognition dynamically. \\citet{goel2021emotion} proposed a novel transformer encoder that adds and normalizes the word embedding with emotion embedding, thereby integrating the semantic and affective aspects of the input utterance. \nHowever, multi-modal information other than textual information is rarely considered in NLP.\n\n\n% \\textcolor{red}{Para2: Conversational emotion recognition filed}.\n\n\nIn speech processing field, the advanced conversational speech synthesis methods use Graph Neural Networks (GNNs) models \\cite{li2022enhancing,li2022inferring} to understand the multi-modal conversational context and infer the appropriate speaking style for the target utterance. For example, multi-modal features of all past utterances in the conversation, including textual information and speaking style information, are modeled by Dialogue Graph Convolutional Network (DialogueGCN) \\cite{ghosal-etal-2019-dialoguegcn} to produce new representations holding richer context knowledge. However, the GNNs for CSS are designed for homogeneous graphs, in which all nodes and edges belong to the same types (such as utterance nodes from different speakers), making them infeasible to represent the natural heterogeneous structures in conversation. Especially for emotional expressions in conversation, information such as text, audio, speaker, emotion, and emotion intensity can be seen as nodes of the heterogeneous graph.\n\nWe note that there are some multi-modal conversational emotion recognition (MMCER) works that adopt heterogeneous graph networks to model the complex dependencies of contexts adequately \\cite{li2022developing,song2023sunet}.\n% ------For example, \\citet{li2022developing} propose a heterogeneous dialog graph-based MMCER model that calculates the attention weights between utterance nodes and between nodes and edges separately and then learns contextual utterance representations through these learnable edge representations. \\citet{song2023sunet} construct a speaker-utterance interactive heterogeneous network that effectively models context while taking into account the global characteristics of speakers.--------\n% studies tried to understand the emotion cues in multi-modal dialogue history, thus predicting the emotion state for target utterance. \n% For example, \\cite{ghosal2019dialoguegcn} proposed Dialogue Graph Convolutional Network (DialogueGCN), which extracts the speaker's self-dependence and interlocutor's dependence from text and speech to simulate dialogue context for emotion recognition. \\cite{shen2020wise} built a word-level interaction-based multimodal fusion framework for speech emotion recognition, explicitly modeling dynamic interactions between audio and text at the word level via interaction units between two long short-term memory networks representing audio and text. \\cite{zhang2023haan} used hierarchical dialogue context information to model intra-speaker, inter-speaker, intra-modal, and intermodal influences to infer the emotional state of speakers.\nUnlike previous studies, our heterogeneous graph module has some clear differences from these works: 1) we add emotion and emotion intensity nodes into the graph structure to model the dynamic emotion cues in conversation context; 2) we adopt \\textit{Heterogeneous Graph Transformer} as the backbone to encode the relations between heterogeneous nodes to learn the high-level feature representation for the constructed graph.\nNote that our work is the first attempt to model the emotion understanding and rendering in conversations for CSS with heterogeneous graph networks.\n\n\n% Existing conversational speech synthesis methods also use methods such as multi-scale contextual information modeling \\cite{xx}, speaker dependency modeling \\cite{xx} to understand the conversational context and infer the appropriate speaking style for the target utterance. However, emotion modeling is missing.\n\n% \\textcolor{red}{Para3: Conversational speech synthesis filed}.\n\n% \\textcolor{red}{Para4: difference}.\n\n% Compared to the above, our work differs from theirs in many ways. 1) this is the first attempt to model the emotion rendering in conversations for CSS, where we model the complex dependencies of different elements in a conversation history to reason about the appropriate emotion state information for the current utterance; 2) we present an emotional CSS framework based on a novel heterogeneous graph-based multi-modal emotional context modeling to fully understand the emotional conversation context; 3) we propose a novel emotion renderer module to improve the synthesis of the emotion expression, which outperforms other state-of-the-art CSS systems.\n\n\n% Compared to general-purpose speech synthesis techniques, conversational speech synthesis can utilize various modal information in the conversation. So modeling conversational context dependencies becomes an important research direction. Early conversational speech synthesis used complex conversation-related labels [3, 4, 5 in Guo's paper] to compensate for the lack of statistical parametric speech synthesis in context modeling. However, the labeling cost is expensive. In recent years, TTS based on the sequence-to-sequence (seq2seq) paradigm [fastspeech series, tacotron series] has shown strong modeling capabilities. [Guo] proposed a conversation context encoder to extract prosody-related features directly from sentence embeddings containing rich information. The individual utterances of the chat logs are passed through a pre-trained BERT model to obtain sentence embeddings, after which the past utterances are encoded in a unidirectional GRU layer and concatenated with the current utterance. This method achieves the purpose of contextual modeling compared to generic speech synthesis methods, but acoustic information is also crucial for improving speech prosody.\n\n% [Tsinghua ICASSP2022, Tsinghua ACM MM, Beipiao, Japan] Tsinghua utilized textual and acoustic information for contextual modeling. [Tsinghua ICASSP2022] takes the features of both modalities of each utterance together as a node of a directed graph neural network, after which the attention mechanism is utilized to find out the most relevant feature to the text of the current discourse from the original multimodal features and the new representation encoded by the graph neural network, and finally the feature is used to predict the GST weight of the current utterance. [Tsinghua ACM M] combines two modal features at global and local scales as nodes of a directed graph neural network and uses the attention mechanism to reason about the speaking style of the current utterance after the two scales of information are encoded by a graph convolutional neural network. [bei you] encodes the features of the two modalities through coarse- and fine-grained encoders containing the attention mechanism, respectively, and feeds the acquired features of the two scales of the two modalities most relevant to the current utterance directly into the fastspeech2 backbone. [Japan] extracts the features corresponding to the two modalities in the dialog separately and fuses them as contextual representations for input to the acoustic decoder using the cross-modal attention mechanism. Several of the above approaches considered textual and audio modal information in the dialog history, spliced or processed separately, but did not model the dependency between the two modal information for the same utterance. In addition, the above work neglects the modeling of emotions in a conversation.\n\n% Leveraging multimodal information to model conversational context is the focus of research in conversational TTS. We focus on the fact that heterogeneous graph neural networks can handle various types of nodes and edges compared to homogeneous graph neural networks. Moreover, it has demonstrated excellent capabilities in various tasks in dialog scenarios. In Emotional Dialogue Generation, [Emotional Dialogue Generation] constructs a heterogeneous graph of the dialog content using the dialog history information of different modalities (text, emotion tags, facial expressions, speaker, and audio) and learns the representation of the constructed graph using an encoder to understand the dialog content and perceive the emotions fully. In dialog emotion classification, [dialog sentiment recognition] proposes a learnable edge messaging model based on heterogeneous dialog graphs, which first calculates the attentional weights between discourse nodes and between nodes and edges in the heterogeneous graphs, respectively, and then learns the contextual utterance representations through these learnable edge representations. In discourse parsing for multiparty dialogue, [*] uses a heterogeneous graph neural network to encode the dialog graph, aggregating speaker and discourse nodes through iterative updating, and then predicts discourse-dependent links and relationships. In dialogue relation extraction, [*] introduces a heterogeneous graphical attention network to model cross-sentence relations in dialogues, which models multiple types of features of dialogues, such as information about discourse, words, speakers, arguments, and entity types.\n\n\\nocite{r:80, hcr:83}\n\n\n\n\n\n\n"
            },
            "section 3": {
                "name": "Task Definition",
                "content": "\nA conversation can be defined as a sequence of utterances ($utt_{1}, utt_{2}, ..., utt_{\\mathcal{J}}, utt_{\\mathcal{C}}$), where \\{$utt_{1}, utt_{2}, ..., utt_{\\mathcal{J}}$\\} is the dialogue history till round $\\mathcal{J}$ while $utt_{\\mathcal{C}}$ means the current utterance to be synthesized. \nThe task of emotional conversational speech synthesis aims to synthesize the audio $a_{\\mathcal{C}}$ given the $utt_{\\mathcal{C}}$ and the dialogue history \\{$utt_{1}, utt_{2}, ..., utt_{\\mathcal{J}}$\\}. \nFor the multi-modal context, each utterance $utt_{j}$ ($j \\in [1, \\mathcal{J}]$) in the dialogue history can be represented by five-tuples like $<$text$_{j}$, speaker$_{j}$, audio$_{j}$, emotion$_{j}$, emotion intensity$_{j}$$>$, in short for $<u_{j}$, $s_{j}$, $a_{j}$, $e_{j}$, $i_{j}$$>$. Note that the $utt_{\\mathcal{C}}$ can be represented by only two-tuples like $<$text$_{\\mathcal{C}}$, speaker$_{\\mathcal{C}}>$, in short for $<u_{\\mathcal{C}}$, $s_{\\mathcal{C}}>$, since emotion and intensity information need to be generated by ECSS.\nParticularly, the emotional expression of the synthesized speech $a_{\\mathcal{C}}$ should confirm to the emotional conversational context characterized by the multi-modal dialogue history. To this end, the emotional CSS methods need to consider: 1) How to mine the multi-source emotional information in conversation history that is important for emotional expression; 2) How to model dynamic emotional cues in dialogue context while modeling intra-speaker, multi-modal context dependencies, etc.; 3) How to infer the appropriate emotional expression information of the current discourse based on understanding the emotional cues of the conversation.\n\n\n%\n% All involved information in a conversation needs to be encoded to participate in model training.\n% $f_{u_{j}}$ means the feature representation of text modality. $f_{s_{j}}$is the speaker representation. $f_{a_{j}}$ represents the acoustic representation of the audio signal. $f_{e_{j}}$ is the high-level feature representation of the emotion state. $f_{i_{j}}$ is the feature representation of the emotion intensity.\n\n% To model the inter and intra-speaker, multi-modal context dependencies, etc., among these above five heterogeneous elements in conversation, a heterogeneous graph-based emotional context encoder needs to be built to learn the effect of emotional context on the current utterance and obtain the high-level feature representation of all nodes from the graph structure. \n% The learned graph-enhanced representations contain rich information which considers the complex emotional dependencies. After that, the conversational speech synthesis process should utilize such information to enhance the emotional expressiveness of the synthesized speech.\n\n\n\n"
            },
            "section 4": {
                "name": "Methodology",
                "content": "\n% \\Such conversational representations could be used in many conversation-related researches, such as conversational emotion analysis and conversational speech synthesis.\n% \\subsection{Overall Architecture}\nAs shown in the pipeline of Fig. \\ref{fig:model}, the proposed ECSS consists of three components, that are 1) \\textit{Multi-source knowledge}; 2) \\textit{Heterogeneous Graph-based Emotional Context Encoder} and 3) \\textit{Emotional Conversational Speech Synthesizer}.\nAs mentioned before, the multi-modal context, including text, speaker, audio, emotion and intensity, contains natural multi-source information and can therefore be viewed as multi-source knowledge.\nTo enhance emotion understating, the \\textit{Heterogeneous Graph-based Emotional Context Encoder} constructs a heterogeneous Emotional Conversational Graph (ECG) by considering each kind of information in the multi-source knowledge as a node, and obtains a graph-enhanced emotional contextual representation for each node after modeling the dependencies among all heterogeneous nodes. To achieve emotion rendering, the \\textit{Emotional Conversational Speech Synthesizer} utilizes the graph-enhanced contextual representation in the ECG to make a reasonable prediction of the emotion expression information of the current sentence and further generates the emotional conversational speech. \n% Note that the \\textit{Dynamic Feature Aggregator} effectively blends the predicted emotion information and the content and speaker information of the current utterance together. \nWe elaborate our ECSS from the following three aspects: \\textit{Heterogeneous Graph-based Emotional Context Encoder}, \\textit{Emotional Conversational Speech Synthesizer} and \\textit{Contrastive Learning Training Criterion}.\n\n\n% consists of a context encoder based on heterogeneous graphs and an acoustic decoder based on emotion rendering. The context encoder learns various dependencies from the multimodal information of the conversation history to understand the content of the conversation and the flow of emotions. At the same time, the acoustic decoder predicts the rhymes and emotions that match the current conversation context and decodes them to generate the mel-spectrum.\n\n\n",
                "subsection 4.1": {
                    "name": "Heterogeneous Graph-based Emotional Context Encoder",
                    "content": "\n\nAs shown in the middle panel of Fig. \\ref{fig:model}, the heterogeneous graph-based emotional context encoder\n% aims to take the multi-source context as input to extract the graph-enhanced high-level encoding for the current utterance. \n% It \nconsists of three parts: 1) ECG Construction, constructing the heterogeneous graph with multi-source context; 2) ECG Initialization, initializing different heterogeneous nodes via their own feature representations; 3) ECG Encoding, perceiving emotion cues and generating the emotion-aware feature representation for the heterogeneous nodes. \n\n% The new representation of the conversation, encoded by the heterogeneous graph, contains various complex contextual dependencies such as inter-speaker and intra-speaker dependencies, dependencies between different modal information of the same utterance, and so on.\n\n",
                    "subsubsection 4.1.1": {
                        "name": "ECG Construction",
                        "content": " \nUnlike previous GNNs based CSS methods, we aim to introduce the multi-source knowledge, which are 5 kinds of nodes including text $f_u$, audio $f_a$, speaker $f_s$, emotion $f_e$, and intensity $f_i$ and build an emotional conversational graph or ECG $\\mathcal{G = (N, E)}$, where $\\mathcal{N}$ denotes the set of nodes, and $\\mathcal{E}$ denotes the set of edges representing the relations between two nodes. Note that the speaker, audio and text nodes seek to introduce the basic dialogue attributes, while emotion and intensity nodes can introduce the dynamic emotion traits and bridge the emotion interaction between remote utterances.\nAs shown in the middle part of Fig. \\ref{fig:model}, different shapes of diagrams mean different kinds of nodes. \n% the feature representations of multi-source knowledge in the conversation are organized into a heterogeneous graph consisting of heterogeneous nodes, edges, and edge weights. \n\n% We bridge any two nodes with edges $\\mathcal{E}$ to build the natural connection between them.\n\nConsidering the multi-source knowledge, We created 14 different types of edges, as shown by the different colored connecting lines shown in the middle part of Fig. \\ref{fig:model}. However, due to space limits, not all the edges of the nodes are depicted. \nIn a nutshell, these 14 edges connect  1) the text and each of the other nodes, 2) the audio and speaker nodes, 3) the emotion and speaker, emotion intensity, and audio nodes, and 4) emotion intensity and speaker, audio nodes. Note that all edges include past-to-future and future-to-past connections to model the bidirectional relation.\n\n\n% The following is a detailed description of the 14 types of edges:\n\n% \\begin{itemize}\n%     \\item $\\mathcal{E}_{1}$ connect two text nodes that belong to adjacent utterances or the same speaker.\n%     \\item $\\mathcal{E}_{2}$ connect the text node and its emotion node.\n%     \\item $\\mathcal{E}_{3}$ connect the text node and its audio node.\n%     \\item $\\mathcal{E}_{4}$ connect the text node and its intensity node.\n%     \\item $\\mathcal{E}_{5}$ connect the text node and its speaker node.\n\n%     \\item $\\mathcal{E}_{6}$ connect two audio nodes that belong to adjacent utterances or the same speaker.\n%     \\item $\\mathcal{E}_{7}$ connect the audio node and its speaker node.\n\n%     \\item $\\mathcal{E}_{8}$ connect two speaker nodes that belong to the same or different speakers.\n         \n%     \\item $\\mathcal{E}_{9}$ connect two emotion nodes that belong to adjacent utterances or the same speaker.\n%     \\item $\\mathcal{E}_{10}$ connect the emotion node and its intensity node.\n%     \\item $\\mathcal{E}_{11}$ connect the emotion node and its audio node.\n\n        \n%     \\item $\\mathcal{E}_{12}$ connect two intensity nodes that belong to adjacent utterances or the same speaker.\n%     \\item $\\mathcal{E}_{13}$ connect the intensity node and its speaker node.\n%     \\item $\\mathcal{E}_{14}$ connect the intensity node and its audio node.\n% \\end{itemize}\n\n% construct corresponding edges on top of these nodes, including edges consisting of every two nodes between the same modality and edges consisting of every two nodes between different modalities. Different edges represent different dependencies, e.g., text nodes represent semantic dependencies, speaker nodes represent inter-speaker dependencies, emotion nodes and audio nodes corresponding to the same utterance represent emotional states saturated in the audio, etc.\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "ECG Initialization",
                        "content": " \nTo achieve meaningful heterogeneous graph encoding, we need to initialize all nodes with their feature representations. As shown in the middle and bottom panels of Fig. \\ref{fig:model}, to take the multi-turn dialogue as an example, we employ various encoders to obtain $f_{u_j}$, $f_{s_j}$, $f_{a_j}$, $f_{e_j}$, $f_{i_j}$ ($j \\in [1,4]]$) for text, speaker, audio, emotion, and intensity nodes. \n\n\\begin{itemize}\n    \\item \\textbf{Text Nodes}. We adopt a pre-trained BERT\\footnote{\\label{bert}https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1} model to extract the linguistic feature: $f_{u_{j}} = {\\rm BERT}(u_{j})$.\n    % The initialized representation $X_t$ for each text node is defined as follows:\n    % \\begin{equation}\n    %     f_{u_{j}} = {\\rm BERT}(u_{j}) \n    % \\end{equation}\n\n\n    \\item \\textbf{Audio Nodes}. We employ the global style token (GST) \\cite{wang2018style} module, which includes a reference encoder and style token layer, as the audio encoder to extract the acoustic features contained in each audio $a_j$: $f_{a_{j}} = {\\rm GST}(a_j)$.\n    % The initialization representation $f_{a_j}$ for each audio node is defined as follows: $f_{a_{j}} = {\\rm GST}(a_j)$.\n    % \\begin{equation}\n    %     f_{a_{j}} = {\\rm GST}(a_j)\n    %     % X_a\\in \\mathbb{R}^{N\\times d_a}\n    % \\end{equation}\n    % Where $d_a$ represents the dimension of the audio representation $X_a$.\n\n \n    \\item \\textbf{Speaker}, \\textbf{Emotion}, and \\textbf{Emotion Intensity Nodes}. The speaker, emotion and intensity encoders are used to define three randomly initialized trainable parameter matrices \n    % $\\mathcal{\\hat S}$, $\\hat E$, and $\\mathcal{\\hat I}$\n    $f_{s_{j}}$, $f_{e_{j}}$, and $f_{i_j}$\n    to learn two speaker identity features, seven emotion label (happiness, sadness, anger, disgust, fear, surprise, neutral) features, and three emotion intensity label (weak, medium, strong) features respectively. \n    % Where $d_s$, $d_e$, $d_i$ is the dimension of the speaker node representations, the dimension of the emotion node representations, and the emotion intensity dimension of the node representations.\n    % Thus, the initialized representations of speaker, emotion, and emotion intensity nodes are extracted from $S$, $E$, and $D$ and defined as $f_\\mathcal{S}$, $f_{E}$, and $f_\\mathcal{I}$, respectively.\n \\vspace{-2mm}\n\\end{itemize}\n\nNote that the text and speaker nodes $f_{u_\\mathcal{C}}$, $f_{i_\\mathcal{C}}$ of current utterance are initialized in the same way as the nodes $f_{u_j}$ and $f_{s_j}$ in the dialogue history.\n \n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "ECG Encoding",
                        "content": "\n%Encouraged by the excellent performance of the HGT network, our heterogeneous graph neural network adopts the same structure as it. The goal of the heterogeneous graph is to aggregate the information of the source node $s$ to obtain a contextual representation of the target node $t$. Specifically, this process is divided into three parts: 1) Heterogeneous Mutual Attention, which estimates the importance of each source node 2) Heterogeneous Message Passing, which extracts the messages of the source nodes 3) Target-Specific Aggregation, which aggregates the domain messages by attention weights to aggregate domain messages. The heterogeneous graph neural network is defined as follows:%\nAfter initializing the constructed Graph, the heterogeneous graph encoding module is used to encode the emotion cues in dialogue context to obtain the graph-enhanced representations for each node.\nInspired by Heterogeneous Graph Transformer (HGT) \\cite{hu2020heterogeneous}, we also adopt a three-stage emotional HGT network, that includes \\textit{Heterogeneous Mutual Attention} (HMA), \\textit{Heterogeneous Message Passing} (HMP), and \\textit{Emotional knowledge Aggregation} (EKA) operations, to model the dependencies in emotional conversations.\nAssuming that one node in the heterogeneous ECG is the target node, then any node of any type can be viewed as the source node. The goal of HGT is to aggregate information from source nodes to get a contextualized representation for the target node. \n\nFirstly, given a target node $\\mathcal{N}_{\\hat {tgt}}$ and all its neighbor source nodes $\\mathcal{N}_{\\hat {src}}$, the HMA mechanism maps $\\mathcal{N}_{\\hat {tgt}}$ into a Query vector, and $\\mathcal{N}_{\\hat {src}}$ into a Key vector, and calculate their dot product as attention, that indicates the importance of each source node for the target node. Then we concatenate $h$ attention heads together to get the attention vector for each node pair $\\mathcal{E}_{\\hat {src} \\rightarrow \\hat {tgt}}= \\{\\mathcal{N}_{\\hat {src}}, \\mathcal{N}_{\\hat {tgt}}\\}$ ($\\mathcal{E}_{\\hat {src} \\rightarrow \\hat {tgt}}$ means the edge between $\\mathcal{N}_{\\hat {src}}$ and $\\mathcal{N}_{\\hat {tgt}}$). For each target node $\\mathcal{N}_{\\hat {tgt}}$, we gather all attention vectors from its neighbors $\\mathcal{N}_{\\hat {src}}$ and conduct softmax to get the final attention score.\n\nSecondly, HMP is computed parallel to pass the dependency information from $\\mathcal{N}_{\\hat {src}}$ to $\\mathcal{N}_{\\hat {tgt}}$. Specifically, for a pair of nodes $\\mathcal{E}_{\\hat {src} \\rightarrow \\hat {tgt}}= \\{\\mathcal{N}_{\\hat {src}}, \\mathcal{N}_{\\hat {tgt}}\\}$, HMP uses a linear projection to project the feature vector of $\\mathcal{N}_{\\hat {src}}$ into a message vector, that then followed by a matrix $W_{\\hat {src} \\rightarrow \\hat {tgt}}$ for incorporating the edge dependency. The final step is to concatenate all $h$ message heads to get the final message vector for each node pair.\n\nAt last, the EKA module aims to aggregate the computed attention score and message vector.\nWe use the attention score as the weight to average the corresponding messages from all neighbor source nodes $\\mathcal{N}_{\\hat {src}}$ and get the emotion-augmented vector $f_{\\mathcal{N}_{\\hat {tgt}}}$ for the target node $\\mathcal{N}_{\\hat {tgt}}$. \nIt's worth noting that all ECG nodes, including emotion and intensity nodes, can be treated as the source or target nodes to learn its high-level contextual information. Therefore, the EKA operation ultimately incorporates the emotional information from the dialog history into all ECG nodes. The graph-enhanced feature representation of each node incorporates the emotional cues in context.\n\n% . The heterogeneous graph encoding is defined as follows:\n% \\begin{equation}\n%     H^l_{[{\\hat t}]} = \\underset{\\forall {\\hat s}\\in \\mathcal{N}({\\hat t})   \\forall {\\hat e}\\in \\mathcal{E}({\\hat s},{\\hat t})}{Aggregate}\\left ( Attention({\\hat s},{\\hat t})\\cdot Message({\\hat s})\\right )\n% \\end{equation}\n\n% We denote the output of the $l$-th HGT layer of the heterogeneous graph as $H^l$. \nIn this way, the ECG encoding module can obtain the final emotion-aware graph-enhanced feature representation $f_{u}^{'}$, $f_{s}^{'}$, $f_{a}^{'}$, $f_{e}^{'}$ and $f_{i}^{'}$ for all text, speaker, audio, emotion and intensity nodes respectively, which can be fed into subsequent models to provide emotional information.\n \n \n\n%  {\n% \\color{red}\n% \\textbf{Heterogeneous Mutual Attention} Given a target node $t$ and all its neighbor nodes $s\\in N(t)$, calculate their mutual attention grounded on the edges between them, i.e., $<\\tau(s), \\phi(e), \\tau(t)>$ triples. We map the target node $t$ to a Query vector and the source node $s$ to a Key vector, using the dot product between them as the attention. Specifically, we obtain the value of the multiple attention of the edge $e = (s, t)$ by the following method.\n\n% \\begin{equation}\n%     Attention(s,e,t) = \\underset{\\forall s\\in N(t)}{Softmax}\\left (\\underset{i\\in [1,h]}{\\parallel} ATT-head^i(s,e,t) \\right) \n% \\end{equation}\n\n% \\begin{equation}\n%     ATT-head^i(s,e,t)=\\left ( K^i(s)W_{\\phi(e)}^{ATT}Q^i(t)^T \\right ) \\cdot \\frac{\\mu_{\\left \\langle \\tau_{(s)},\\phi_{(e)},\\tau_{(t)} \\right \\rangle}}{\\sqrt{d}}\\nonumber\n% \\end{equation}\n\n% \\begin{equation}\n%     K^i(s)=K-Linear_{\\tau(s)}^{i} \\left ( H^{(l-1)}[s] \\right )\\nonumber\n% \\end{equation}\n\n% \\begin{equation}\n%     Q^i(t)=Q-Linear_{\\tau(t)}^{i} \\left ( H^{(l-1)}[t] \\right )\\nonumber\n% \\end{equation}\n\n% For the $i-th$ attention head $ATT-head^i(s, e, t)$, we first linear project $K-Linear_{\\tau (s)}^{i}$ the source node $s$ of type $\\tau(s)$ to the $i-th$ Key vector, where $h$ is the number of attention heads, and $\\frac{d}{h}$ is the dimension of each head. Each type of source node $\\tau(s)$ corresponds to a linear projection, which is guaranteed to maximize the simulation of distributional differences. Similarly, we map the target node $t$ to the $i-th$ Query vector via a linear projection. Since heterogeneous graphs have different edge relationships, a different edge-based matrix is maintained for each edge type when computing the dot product of Query vectors and Key vectors, which also captures the different semantic relationships between nodes of the same type. The a prior tensor $\\mu\\in \\mathbb{R}^{|A|\\times|R|\\times|A|}$ represents the general meaning of each edge triad as an adaptive scaling of attention.\n\n% \\textbf{Heterogeneous Message Passing}\n% This step and mutual attention are computed parallel to passing the message from the source node to the target node. In order to alleviate the differences in the distribution of different types of nodes and edges, we incorporate the relationship of edges into the message-passing process. For a pair of nodes $e = (s, t)$, we compute their polytope information by:\n\n% \\begin{equation}\n%     Message(s,e,t)=\\underset{i\\in[1,h]}{\\parallel } MSG-head^i(s,e,t)\n% \\end{equation}\n\n% \\begin{equation}\n%     MSG-head^i(s,e,t)=M-Linear_{\\tau(s)}^{i}\\left ( H^{(l-1)}[s] \\right ) W_{\\phi(e)}^{MSG} \\nonumber\n% \\end{equation}\n\n% In order to get the $i-th$ message header, we first map the source node of type $\\tau(s)$ with a linear projection $M-Linear_{\\tau (s)}^{i}$ and then multiply it with a matrix $W_{\\phi(e)}^{MSG}\\in  \\mathbb{R}^{\\frac{d}{h}\\times \\frac{d}{h}}$ for merging the edge dependencies. Finally, to get the $Message(s,e,t)$ for each node pair, all $h$ message headers are connected.\n\n\n% \\textbf{Target-Specific Aggregation}\n% This method aggregates the computed multi-head attention and messages from the source node to the target node. Since $softmax$ method is used in Eq. (4) to make the sum of the attention vectors of each target node $t$ to be one, we directly use the attention vectors as weights to aggregate messages from all neighbors (source nodes) with different feature distributions to the target node and get the updated target node vectors $\\tilde{H}^{(l)}[t]$ as:\n% \\begin{equation}\n%     \\tilde{H}^{(l)}[t]= \\underset{\\forall s\\in N(t)}{\\bigoplus}\\left ( Attention(s,e,t)\\cdot Message(s,e,t) \\right )\n% \\end{equation}\n\n% The final step is to map the vector of the target node $t$ back to its type-special distribution, indexed by its node type $\\tau(t)$. A linear projection $A-Linear_{\\tau(t)}$ updates the vector and follows a residual connection.\n\n% \\begin{equation}\n%     H^{(l)}[t]=A-Linear_{\\tau(t)}\\left ( \\sigma(\\tilde{H}^{(l)}[t]) \\right )+H^{(l-1)}[t]\n% \\end{equation}\n\n% $H^{(l)}[t]$ is the output of the target node $t$ in the $l-th$ layer of the heterogeneous graph-based context encoder.\n\n% }\n \n\n% \\subsubsection{Run-time Inference}\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Emotional Conversational Speech Synthesizer",
                    "content": "\n% Emotional conversational speech synthesizer aims to predict accurate emotional information for the current utterance to be synthesized and incorporate it into the speech generation process with a full understanding of the emotional cues of the conversation.\nAs shown in the right panel of Fig. \\ref{fig:model}, the emotional conversational speech synthesizer consists of the following four components: \\textit{Text encoder, Speaker Encoder, Emotion Renderer and Acoustic Decoder}.\nThe text and speaker encoders seek to encode the content and speaker identity features $\\mathcal{H}^c_\\mathcal{C}$ and $\\mathcal{H}^s_\\mathcal{C}$ for the current utterance and speaker. \nNote that the emotion renderer attempts to predict the current utterance's emotion, intensity, and prosody features $\\mathcal{H}^e_\\mathcal{C}$, $\\mathcal{H}^i_\\mathcal{C}$, and $\\mathcal{H}^p_\\mathcal{C}$ using the graph-enhanced node features. \nTo obtain a robust feature representation  of the current utterance, the feature aggregator module set a set of trainable weight parameters\n% $\\mathcal{W}_s$, $\\mathcal{W}_p$, $\\mathcal{W}_e$, and $\\mathcal{W}_i$ \nfor the above five features and then output the final mixup feature $\\mathcal{H}_\\mathcal{C}$.\nFor the acoustic decoder, we use \\textit{FastSpeech2} \\cite{ren2021fastspeech} as the backbone, which includes the variance adaptor, mel decoder and vocoder. The variance adapter takes the $\\mathcal{H}_\\mathcal{C}$ as inputs to predict the duration, energy, and pitch. The Mel Decoder aims to predict the mel-spectrum features. \nFinally, a well-trained HiFi-GAN \\cite{kong2020hifi} is used as the vocoder to generate speech waveform $a_\\mathcal{C}$ with desired emotion style.\n\nNote that to achieve emotion rendering, our emotion renderer extracts the encoded node features from ECG and predicts the emotion and emotion intensity feature representations of the current utterance while performing the prosody prediction.  More importantly, to achieve accurate emotion category and emotion intensity feature representation prediction, we propose contrastive learning-based emotion and emotion intensity loss functions $\\mathcal{L}^{cl}$. \n\n\n\n\n\n% \\begin{equation}\n%     h_c= {\\rm FFN}(u_{\\mathcal{J}})\n%     % ,H_t\\in \\mathbb{R}^{N\\times d_t}\n% \\end{equation}\n\n% Where $d_t$ is the dimension of the textual representation $H_t$ and $n$ is the number of layers of the $FFN$.\n\n\n\n\n% \\begin{figure}[t]\n% \\centering\n% % \\setlength{\\abovecaptionskip}{-0mm}   %\u8c03\u6574\u56fe\u7247\u6807\u9898\u4e0e\u56fe\u8ddd\u79bb\n% \\centerline{\\includegraphics[width=1\\linewidth]{CameraReady/LaTeX/fig/loss_v1.pdf}}\n% \\caption{xxx.}\n% % \\vspace{-4mm}\n% \\label{fig:loss}\n% \\end{figure}\n\\vspace{-1mm}\n",
                    "subsubsection 4.2.1": {
                        "name": "Emotion Renderer",
                        "content": "\n% The emotion renderer module first extracts the encoded node information from the heterogeneous graph neural network, including text, prosody, emotion, and emotion intensity. Then we use the extracted multimodal emotion context information to input the prosody predictor, emotion predictor, and pand emotion intensity predictor, respectively, to predict the suitable prosody features and emotion states for the current utterance(including the prosody representation Hp, the emotion representation He, and the emotion intensity representation Hd). The following are the three predictors included in the emotion renderer respectively:\n% The emotion renderer infers the emotion and emotion intensity information while predicting the speaking prosody for the current utterance.\nAs shown in the blue part of Fig. \\ref{fig:model}, the emotion renderer consists of emotion, intensity and prosody predictors. The prosody predictor adopts a multi-head attention layer to infer the speaking prosody information of the current utterance from the feature representations of text nodes in the dialogue history. The features of audio nodes are not used because we believe that audio and text modalities have already interacted during the ECG encoding, thus the text node already contains audio information. We use the MSE loss for the prosody predictor to constrain its training, where the target is obtained from a GST-based prosody extractor. Next, we will introduce emotion and intensity predictors.\n% The above three predictors take the $f^{'}_u$, $f^{'}_e$ and $f^{'}_i$ as input to predict the prosody representation $\\mathcal{H}_{P}$, the emotion representation $\\mathcal{H}_{E}$, and the emotion intensity representation $\\mathcal{H}_{I}$ respectively in a supervised way. \n\n% \u97f5\u5f8b\u9884\u6d4b\u5668\n\\begin{itemize}\n   \\vspace{-1mm}\n    % \u60c5\u611f\u9884\u6d4b\u5668\n    \\item \\textbf{Emotion Predictor} uses the encoded features of the emotion nodes in dialog history to infer the emotion representation $\\mathcal{H}^e_\\mathcal{C}$ of the current sentence.\n    It includes two convolutional layers, a bidirectional LSTM layer, and two fully connected layers. \n    % The predicted emotional representation $H_e$ of the current utterance is as follows:\n    \\vspace{-1mm}\n    \\begin{equation}\n        \\mathcal{H}^e_\\mathcal{C}= {\\rm FC(BiLSTM(CNN(}f_e^{'})))\n        % ,H_e\\in \\mathbb{R}^{7\\times d_e}\n    \\end{equation}\n    where $f_e^{'}$ represents all emotion-type nodes in the dialogue history after ECG encoding.\n    % $d_e$ represents the dimension of the emotion-type representation $H_e$.\n\n \\vspace{-0.3mm}\n    %\u60c5\u611f\u5f3a\u5ea6\u9884\u6d4b\u5668\n    \\item \\textbf{Intensity Predictor} uses the encoded features of the intensity nodes in dialog history to infer the emotion intensity representation $\\mathcal{H}^i_\\mathcal{C}$ of the current sentence. It consists of two convolutional layers, a bidirectional LSTM layer, two fully connected layers, and a mean pooling layer. \n    % The predicted emotion intensity representation $\\mathcal{H}_i$ of the current utterance is as follows:\n    \\begin{equation}\n    \\begin{split}\n        % & \\small \n        \\mathcal{H}^i_\\mathcal{C}= {\\rm AvgPooling(FC_2(BiLSTM(CNN_2(}f_i^{'}))))\n        % \\\\ \n        % & \\small{H_i\\in \\mathbb{R}^{3\\times d_i}}\n    \\end{split}\n    \\end{equation}\n    where $f_i^{'}$ is a universal representation of all emotion intensity nodes in the dialog history after ECG encoding.\n    % $d_i$ represents the dimension of emotion intensity that characterizes $H_i$.\n \n\\end{itemize}\n\n\n\n \n% The dynamic information fusion module aims to fuse information from various modalities in the dialog history to assist the vocoder in synthesizing expressive speech. In addition to the rich prosody $H_a$ , emotion $H_e$ and intensity $H_i$ predicted by the emotion renderer, the semantic content of the current utterance $H_t$ and the speaker's identity information $H_s$ are also input data to the dynamic information fusion module.\n% Different sources of information play different roles in the final emotional expression of the current utterance,\n% % The contribution of information from various sources to synthesize the current speech tends to be different, \n% so we employ a Dynamic Feature Aggregator approach in aggregating the information from the text, speaker encoders and the emotion renderer. In this way, we can \n% Specifically, we set a set of trainable dynamic weight parameters $\\mathcal{W}_s$, $\\mathcal{W}_p$, $\\mathcal{W}_e$, and $\\mathcal{W}_i$ for the speaker representation $\\mathcal{H}^s_\\mathcal{C}$, the prosodic representation $\\mathcal{H}^p_\\mathcal{C}$, the emotional representation $\\mathcal{H}^e_\\mathcal{C}$, and the emotional intensity representation $\\mathcal{H}^i_\\mathcal{C}$, respectively. The final input to the acoustic decoder is $\\mathcal{H}_\\mathcal{C}$:\n\n% \\begin{equation}\n% \\mathcal{H}_\\mathcal{C}=\\mathcal{H}^c_\\mathcal{C}+\\mathcal{W}_s\\ast \\mathcal{H}^s_\\mathcal{C} +\\mathcal{W}_a\\ast \\mathcal{H}^a_\\mathcal{C}+\\mathcal{W}_e\\ast \\mathcal{H}^e_\\mathcal{C}+\\mathcal{W}_i\\ast \n%    \\mathcal{H}^i_\\mathcal{C}\n% \\end{equation}\n\n \n\n\n% \\begin{equation}\n% waveform= {\\rm HiFi\\!\\!-\\!\\!GAN(AcousticDecoder(}V_{in}))   \n% \\end{equation}\n\n\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "Contrastive Learning Training Criterion",
                        "content": "\n\n \nFor emotion and intensity predictors of the emotion renderer, inspired by \\cite{DBLP:journals/corr/abs-2004-11362}, we design the emotion-supervised contrastive learning losses\n$\\mathcal{L}^{cl}$ to motivate the emotion renderer to better distinguish different emotions categories and intensity degrees. \nSpecifically, contrastive learning loss $\\mathcal{L}^{cl}_{emo}$ for emotion category and  $\\mathcal{L}^{cl}_{int}$ for emotion intensity share the same spirits, that is treating all examples with the same emotion category or intensity label in the batch as positive examples while different labels as negative.\n\nFor the emotion feature $\\mathcal{H}^{e}_\\mathcal{C}$, a batch of $K$ emotion representations is denoted as $\\mathcal{H}^{K} = [\\mathcal{H}^{e}_{\\mathcal{C}1}, \\mathcal{H}^{e}_{\\mathcal{C}2}, ..., \\mathcal{H}^{e}_{\\mathcal{C}K}]$,\n % Contrastive learning loss $\\mathcal{L}^{cl}_{emo}$ for the emotion category is used to bring the positive samples closer together and push the negative samples farther apart.\n $\\mathcal{L}^{cl}_{emo}$ for $\\mathcal{H}^{e}_{\\mathcal{C}k}$ as follows,\n% The contrastive learning losses for emotion categories $\\mathcal{L}^{cl}_{emo}$ and emotion intensities  $\\mathcal{L}^{cl}_{int}$  are calculated as follows:\n% \\vspace{-2mm}\n\\vspace{-1.5mm}\n\\begin{equation}\n% \\scriptsize\n    \\mathcal{L}^{cl}_{emo}= log \\frac{-1}{|\\mathcal{P}(k)|} \\frac{\\underset{\\mathcal{H}^{e}_{\\mathcal{C}q}\\in \\mathcal{P}(k)}{\\sum } exp(sim(\\mathcal{H}^{e}_{\\mathcal{C}k},\\mathcal{H}^{e}_{\\mathcal{C}q} )/\\tau)}{\\underset{\\mathcal{H}^{e}_{\\mathcal{C}d}\\in B(k)}{\\sum} exp(sim(\\mathcal{H}^{e}_{\\mathcal{C}k},\\mathcal{H}^{e}_{\\mathcal{C}d})/\\tau)}\n\\end{equation}\nwhere $sim(\\cdot,\\cdot)$ is a cosine similarity function. $\\tau$ is a scalar temperature parameter. $B(k) \\equiv \\mathcal{H}^{K} \\backslash \\{\\mathcal{H}^{e}_{\\mathcal{C}k} \\}$ contains all representations in $\\mathcal{H}^{K}$ except $\\mathcal{H}^{e}_{\\mathcal{C}k}$. $\\mathcal{P}(k)$ is the set of positive samples that have the same emotion label with $\\mathcal{H}^{e}_{\\mathcal{C}k}$ in a batch.\nSimilarity,  $\\mathcal{L}^{cl}_{int}$ for $\\mathcal{H}^{i}_{\\mathcal{C}k}$ as follows,\n\n\\vspace{-1.5mm}\n\\begin{equation}\n% \\scriptsize\n    \\mathcal{L}^{cl}_{int}= log \\frac{-1}{|\\mathcal{P}(k)|} \\frac{\\underset{\\mathcal{H}^{i}_{\\mathcal{C}q}\\in \\mathcal{P}(k)}{\\sum } exp(sim(\\mathcal{H}^{i}_{\\mathcal{C}k},\\mathcal{H}^{i}_{\\mathcal{C}q} )/\\tau)}{\\underset{\\mathcal{H}^{i}_{\\mathcal{C}d}\\in B(k)}{\\sum} exp(sim(\\mathcal{H}^{i}_{\\mathcal{C}k},\\mathcal{H}^{i}_{\\mathcal{C}d})/\\tau)}\n\\end{equation}\n\\vspace{-1.5mm}\n\nAt last, the total loss function $\\mathcal{L}$ is: $\n\\mathcal{L} = \\mathcal{L}^{cl}_{emo} + \\mathcal{L}^{cl}_{int} + \\mathcal{L}^{mse}_{pro} + \\mathcal{L}_{fs2}$, where $\\mathcal{L}_{fs2}$ refers to the acoustic feature loss of traditional FastSpeech2, including pitch, energy, and duration, as well as the mel spectrum, $\\mathcal{L}^{mse}_{pro}$ indicates the MSE loss for prosody predictor. \n\n% \\begin{equation}\\scriptsize\n%     \\mathcal{L}^{cl}_{emo}=\\underset{m\\in M}{\\sum }\\frac{-1}{|\\mathcal{P}_{emo}(m)|} \\underset{p\\in \\mathcal{P}_{emo}(m)}{\\sum }log\\frac{exp(z_m\\cdot z_p/\\tau)}{\\underset{a\\in A(m)}{\\sum }exp(z_m\\cdot z_a/\\tau)}\n% \\end{equation}\n% where $m$ is the index of the currently predicted emotion representation $\\mathcal{H}^{e}_\\mathcal{C}$ in the sample set $M$, $P_{emo}(m)$ is the set of positive samples, $|\\cdot|$ is the number of positive samples, $p$ is a positive sample, and $A(m)$ is the set of samples in the data set other than $m$. $\\tau \\in R^+$ is a scalar temperature parameter, and $z$ is the emotion representation $\\mathcal{H}^e$. \n\n% where, for each emotion data $\\mathcal{H}^{e}_\\mathcal{C}$, $\\mathcal{P}_{emo}$ means the positive sample set, $|\\cdot|$ means the set number. $p$ and $q$ are positive samples, respectively. $A(m)$ and $B(n)$ are the other samples in the data set besides the current sample.\n\n\n% Contrastive learning loss $\\mathcal{L}^{cl}_{int}$ for the emotion intensity share the same spirit with $\\mathcal{L}^{cl}_{emo}$. In a training batch, samples belonging to the same emotion intensity are treated as positive samples, and samples belonging to different emotion intensities are treated as negative samples. $\\mathcal{L}^{cl}_{int}$ is used to enhance the differentiation of emotion intensity.\n\n% \\begin{equation}\\scriptsize\n%     \\mathcal{L}^{cl}_{int}=\\underset{d\\in D}{\\sum }\\frac{-1}{|\\mathcal{P}_{int}(d)|} \\underset{q\\in \\mathcal{P}_{int}(d)}{\\sum }log\\frac{exp(z_d\\cdot z_q/\\tau)}{\\underset{b\\in B(d)}{\\sum }exp(z_d\\cdot z_b/\\tau)}\n% \\end{equation}\n \n%  where, $d$ is the index of the currently predicted emotion intensity representation $\\mathcal{H}^{i}_\\mathcal{C}$ in the sample set $D$, $P_{int}(d)$ is the set of positive samples, $|\\cdot|$ is the number of positive samples, $q$ is a positive sample, and $B(d)$ is the set of samples in the data set other than $d$. $z$ is the emotion intensity representation $\\mathcal{H}^i$.\n \n%  }\n\n\n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage[table,xcdraw]{xcolor}\n% If you use beamer only pass \"xcolor=table\" option, i.e. \\documentclass[xcolor=table]{beamer}\n\n\n\n\n\n"
                    }
                }
            },
            "section 5": {
                "name": "Experiments and Results",
                "content": "\n",
                "subsection 5.0": {
                    "subsubsection 5.0.1": {
                        "name": "Dataset",
                        "content": "\nWe validate the ECSS on a recently public dataset for conversational speech synthesis called DailyTalk \\cite{lee2023dailytalk}, \n% which is a subset of the open-domain conversation dataset DailyDialog \\cite{li2017dailydialog}. DailyTalk\nthat consists of 23,773 audio clips representing 20 hours in total, in which 2,541 conversations were sampled, modified, and recorded. All dialogues are long enough to represent the context of each conversation. The dataset was recorded by a male and a female simultaneously. All speech samples are sampled at 44.10 kHz and coded in 16 bits. We partition the data into training, validation, and test sets at a ratio of 8:1:1.\n\nTo obtain the multi-source knowledge,\nwe invited a professional practitioner to perform fine-grained labeling of DailyTalk data for emotion category and intensity labels while listening to speech and understanding the semantics of utterance. The final distribution of the data is as follows, the number of each of the 7 emotion category labels (happy, sad, angry, disgust, fear, surprise, neutral) are 3871, 722, 226, 186, 74, 497 and 18,197, and the number of each of the 3 emotion intensity labels (weak, medium, strong) are 19,973, 3,646 and 154.\n\n\n\n% \\hl{how to obtain the emotion and emotion intensity labels?} \\textcolor{red}{\n% we added the annotation of emotion intensity (three levels: weak, medium, and strong) to the DailyTalk dataset, with a total of 19,973 weak labels, 3,646 medium labels, and 154 strong labels.\n% Moreover, we corrected \\hl{how much?} of the original neutral emotion labels based on dialogue semantics and speech to express sentence emotions more accurately.\n% }\n\n\n\n\n\\vspace{-1.5mm}\n"
                    },
                    "subsubsection 5.0.2": {
                        "name": "Experimental Setup",
                        "content": "\nIn the heterogeneous graph-based emotion context encoder, the dimension of the text node representation $f_{u_j}$ is set to 512, and the dimensions of the remaining type node representations $f_{e_j}$,$f_{i_j}$,$f_{s_j}$, and $f_{a_j}$ are all set to 256. For multi-head attention-based methods, we set the head number as 8.\nFor the emotion predictor of emotion renderer, the convolutional layer has a convolutional kernel of 3, the LSTM input dimension is 384, the hidden state size is 256, and the forward and backward outputs from the last time steps of the LSTM are spliced using concat before going into the linear layer. For the intensity predictor of emotion renderer, the mean pooling layer convolution kernel size is 2. For the prosody predictor of emotion renderer, the dimensions of the $Query$, $Key$ and $Value$ and output feature $\\mathcal{H}^{p}_\\mathcal{C}$ are 512, 384, 384 and 256.\nThe acoustic decoder is configured with reference to FastSpeech2 \\cite{ren2021fastspeech}. we use Adam optimizer with $\\beta_1$ = 0.9, $\\beta_2$ = 0.98. \nFor text input, we adopt Grapheme-to-Phoneme (G2P) toolkit\\footnote{https://www.github.com/kyubyong/g2p} to convert all text into its phoneme sequence. All speech samples are re-sampled to 22.05 kHz. The mel-spectrum features are extracted with a window length of 25ms and a shift of 10ms. The model is trained on a Tesla V100 GPU with a batch size of 16 and 600k steps.\nDuring ECSS training, the context or dialogue history length is set to 10.\nMore detailed experimental settings are accessed in the \\textit{Appendix} section.\n\n\\vspace{-1.5mm}\n"
                    },
                    "subsubsection 5.0.3": {
                        "name": "Evaluation Metrics",
                        "content": "\n% Evaluation metrics include subjective and objective metrics.\nFor the subjective evaluation metrics, we organized a dialogue-level Mean Opinion Score (DMOS) \\cite{streijl2016mean} listening test with 30 master students whose second language is English and provided specialized training on the rules to all listeners. Given the dialogue history, they were asked to rate the naturalness DMOS (N-DMOS) and emotional DMOS (E-DMOS) of the synthesized speech of the current utterance, ranging from 1 to 5. Each listener was access to 50 audio samples.\n% , with a 1-point interval.\nNote that the N-DMOS focuses on the speaking prosody, while the E-DMOS focuses on the richness of the emotional expression of the current utterance and whether it matches the emotional expression of the context.\n% They received professional training on the rules of subjective assessment and listening to the appraisal samples before the evaluation.\n% The subjective scores ranged from 1 to 5, with a 1-point interval, from which the Naturalness Score (N-MOS) and the Emotional Expression Score (EE-MOS) were calculated. \n% The naturalness index includes whether the audio-synthesized speech contains repetitions, omissions, or mispronunciations and whether the intonation and accents are natural. Emotional performance refers to whether the emotion contained in the audio is sufficiently complete and how appropriate it is in the current dialogue context.\n\n\nFor the objective evaluation metrics, we calculate the Mean Absolute Error (MAE) between the predicted and real acoustic features to assess the emotional expressiveness of the synthesized audio. Specifically, we assess the acoustic feature in terms of mel-spectrum, pitch, energy, and duration with MAE-M, MAE-P, MAE-E and MAE-D.\nIn addition, we conduct a visualization study with the help of a third-party Speech Emotion Recognition (SER) model, that was used to identify the emotion categories of the synthesized emotional conversational speech. We plot the confusion matrix to validate the ECSS.\n\n\n\\vspace{-1.6mm}\n"
                    },
                    "subsubsection 5.0.4": {
                        "name": "Comparative Study",
                        "content": "\nTo demonstrate the effectiveness of our ECSS, we employ three advanced approaches which also employ FastSpeech2 as the TTS backbone as the baseline systems. \n\n\\begin{itemize}\n\\vspace{-1mm}\n    \\item \\textbf{No emotional context modeling}. The first baseline approach is a vanilla FastSpeech2 \\cite{ren2021fastspeech} with no context modeling, which is also representative of state-of-the-art non-conversational TTS systems.\n\\vspace{-1mm}\n    \\item \\textbf{GRU-based context modeling}. This method involves only text modality and uses an RNN-based uni-directional GRU network to model contextual dependencies in a dialogue sequentially \\cite{DBLP:journals/corr/abs-2005-10438}.\n    % ,lee2023dailytalk}. \n    % To achieve emotion rendering using this method, we can extract the encoded feature of all utterances in dialogue history to predict the emotion and intensity information.\n    \\vspace{-1mm}\n    \\item  \\textbf{Homogeneous Graph-based emotion context modeling}. In the homogeneous graph-based approach \\cite{li2022inferring}, each past utterance in the conversation is represented as a node in the graph. Each node is initialized with the corresponding multi-modal features. To achieve emotion rendering using this method, we can extract the graph-enhanced feature of all nodes in dialogue history to predict the emotion and intensity information.\n\\end{itemize}\n\\vspace{-2mm}\n"
                    },
                    "subsubsection 5.0.5": {
                        "name": "Main Results",
                        "content": "\n\nAs shown in the first five rows of Table \\ref{tab:methods}, the ECSS achieves state-of-the-art performance on average, which obtains the optimum results in MAE-M (0.654) and MAE-P (0.455), and suboptimal results in MAE-E (0.215) and MAE-D (0.152).\nHowever, objective experiments may not fully reflect human feelings. By observing the subjective results,\nthe proposed ECSS model outperforms all baselines with an N-DMOS score of 3.506 and an E-DMOS score of 3.619, which reflects the superiority of our ECSS. ECSS contributes to adequate emotion understanding based on heterogeneous graph context modeling, in which the emotion renderer fully mines the emotion cues to infer the emotion expression state of the current sentence, thus achieving satisfactory emotion rendering effects for conversational speech synthesis.\n\n\n\n\n\n% for modeling dialogue context.\n% the context modeling approach based on a homomorphic graph neural network improves the N-MOS score by 0.07 and the EE-MOS score by 0.205 compared to the GRU-based methods. Our proposed heterogeneous graph-based ECSS method obtains the best results with an N-MOS score of 3.506 and an E-MOS score of 3.619, which reflects the superiority of heterogeneous graphs for modeling dialogue context.\n\n\n\n% \\subsubsection{Ablation Study}\n\n% \\begin{table}[t]\n% \\caption{\\label{tab:modal}\\textcolor{black}{Ablation studies on modalities, DFA, and $\\mathcal{L}^{cl}$}.} \n% \\resizebox{\\linewidth}{!}{\n% \\begin{tabular}{ccccc}\n% \\hline\n% {\\color[HTML]{393939} \\textbf{Models}}                & {\\color[HTML]{393939} \\textbf{MAE-M} ($\\downarrow$)}          & {\\color[HTML]{393939} \\textbf{MAE-P} ($\\downarrow$)}           & {\\color[HTML]{393939} \\textbf{MAE-E} ($\\downarrow$)}          & {\\color[HTML]{393939} \\textbf{MAE-D} ($\\downarrow$)}           \\\\ \\hline\n% {\\color[HTML]{393939} \\textbf{ECSS}} & {\\color[HTML]{393939} \\textbf{0.654}} & {\\color[HTML]{393939} \\textbf{0.455*}} & {\\color[HTML]{393939} \\textbf{0.215*}}          & {\\color[HTML]{393939} \\textbf{0.152*}} \\\\\n% {\\color[HTML]{393939} w/o intensity}           & {\\color[HTML]{393939} 0.660}          & {\\color[HTML]{393939} \\textbf{0.453}}  & {\\color[HTML]{393939} 0.210}          & {\\color[HTML]{393939} 0.157}           \\\\\n% {\\color[HTML]{393939} w/o emotion}             & {\\color[HTML]{393939} 0.658}          & {\\color[HTML]{393939} 0.467}           & {\\color[HTML]{393939} 0.224}          & {\\color[HTML]{393939} \\textbf{0.150}}  \\\\\n% {\\color[HTML]{393939} w/o speaker}             & {\\color[HTML]{393939} 0.666}          & {\\color[HTML]{393939} 0.456}           & {\\color[HTML]{393939} 0.231}          & {\\color[HTML]{393939} 0.152}           \\\\\n% {\\color[HTML]{393939} w/o audio}             & {\\color[HTML]{393939} 0.656}          & {\\color[HTML]{393939} 0.457}           & {\\color[HTML]{393939} \\textbf{0.198}} & {\\color[HTML]{393939} 0.154}           \\\\ \\hline\n% {\\color[HTML]{393939} w/o DFA}                 & {\\color[HTML]{393939} 0.660}          & {\\color[HTML]{393939} 0.470}          & {\\color[HTML]{393939} 0.215} & {\\color[HTML]{393939} 0.155}          \\\\\n% {\\color[HTML]{393939} w/o $\\mathcal{L}^{cl}$}                 & {\\color[HTML]{393939} 0.665}          & {\\color[HTML]{393939} 0.459}          & {\\color[HTML]{393939} 0.222}          & {\\color[HTML]{393939} 0.156}          \\\\\n% {\\color[HTML]{393939} w/o DFA \\& $\\mathcal{L}^{cl}$}         & {\\color[HTML]{393939} 0.661}          & {\\color[HTML]{393939} 0.469}          & {\\color[HTML]{393939} 0.218}          & {\\color[HTML]{393939} 0.155}          \\\\ \\hline\n% \\end{tabular}}\n% \\end{table}\n\n"
                    },
                    "subsubsection 5.0.6": {
                        "name": "Ablation Results",
                        "content": "\nTo evaluate the individual effects of various heterogeneous nodes, including emotion, intensity, speaker and audio, in ECG and the contrastive learning loss $\\mathcal{L}^{cl}$ for emotion renderer, we remove these components to build various systems and conduct a series of ablation experiments, and the subjective and objective results are shown in the rows 6 through 10 of Table \\ref{tab:methods}.\n\nWe can find that removing different types of nodes in the heterogeneous ECG brought about a decrease in objective metrics performance in the vast majority of metrics, and the subjective DMOS scores also showed a drop. For example, after removing the emotion node, the MAE-M, MAE-P and MAE-E decreased by 0.004, 0.012 and 0.009 respectively, while N-DMOS and E-DMOS decreased by 0.082 and 0.123. This suggests that our heterogeneous graph nodes can learn the complex emotional dependencies in the dialog history and achieve full emotional understanding. \nIn addition, to validate the $\\mathcal{L}^{cl}$, we replace it with the cross-entropy loss. As shown in the last row of Table \\ref{tab:methods}, all subjective and objective values are reduced after removing the $\\mathcal{L}^{cl}$. This demonstrates that the contrastive learning strategy allows the emotion renderer to distinguish between different emotion categories and intensities better. \n\n\n\n\n\n\n\n\n% removing the nodes of any of the modalities reduces the effectiveness of the context modeling of the heterogeneous graph. It is important to note that this part of the experiment only verifies the removal of information of a specific modality in the heterogeneous graph and the emotion renderer. For example, only the speaker nodes in the heterogeneous graph are removed when removing the speaker modality, and the speaker encoder part remains.\n\n% Please add the following required packages to your document preamble:\n% \\usepackage[table,xcdraw]{xcolor}\n% If you use beamer only pass \"xcolor=table\" option, i.e. \\documentclass[xcolor=table]{beamer}\n\n\n% \\begin{table}[]\n% \\caption{\\label{tab:dandc}\\textcolor{black}{Validation of dynamic weighting parameters and contrast learning loss.}} \n% \\resizebox{\\linewidth}{!}{\n% \\begin{tabular}{ccccc}\n% \\hline\n% {\\color[HTML]{393939} \\textbf{Approach}}       & {\\color[HTML]{393939} \\textbf{MAE-M}} & {\\color[HTML]{393939} \\textbf{MAE-P}} & {\\color[HTML]{393939} \\textbf{MAE-E}} & {\\color[HTML]{393939} \\textbf{MAE-D}} \\\\ \\hline\n% {\\color[HTML]{393939} \\textbf{ECSS}} & {\\color[HTML]{393939} \\textbf{0.654}} & {\\color[HTML]{393939} \\textbf{0.455}} & {\\color[HTML]{393939} \\textbf{0.215}} & {\\color[HTML]{393939} \\textbf{0.152}} \\\\\n% {\\color[HTML]{393939} w/o DWP}                 & {\\color[HTML]{393939} 0.660}          & {\\color[HTML]{393939} 0.470}          & {\\color[HTML]{393939} \\textbf{0.215}} & {\\color[HTML]{393939} 0.155}          \\\\\n% {\\color[HTML]{393939} w/o CLL}                 & {\\color[HTML]{393939} 0.665}          & {\\color[HTML]{393939} 0.459}          & {\\color[HTML]{393939} 0.222}          & {\\color[HTML]{393939} 0.156}          \\\\\n% {\\color[HTML]{393939} w/o DWP \\& CLL}         & {\\color[HTML]{393939} 0.661}          & {\\color[HTML]{393939} 0.469}          & {\\color[HTML]{393939} 0.218}          & {\\color[HTML]{393939} 0.155}          \\\\ \\hline\n% \\end{tabular}}\n% \\end{table}\n\n\n\n\n\\vspace{-2mm}\n"
                    },
                    "subsubsection 5.0.7": {
                        "name": "Context Length Analysis",
                        "content": "\nWe also explore the effectiveness of emotional context modeling with different context lengths.\nSpecifically, considering the average number 9.3 of dialogue turns in the DailyTalk, we set the utterance length of dialogue history ranging from 2 to 14 to compare the objective performance. As shown in Table \\ref{tab:length}, from a general view, all values decrease when enlarging the context length from 2 to 10, and increase from 10 to 14. This shows that either insufficient or redundant context information will interfere the understanding of emotion cues in context.\n \n% \\vspace{-2mm}\n"
                    },
                    "subsubsection 5.0.8": {
                        "name": "Visualization Study",
                        "content": "\n\nTo demonstrate the emotional expressiveness of synthesized speech more visually, we employ a pre-trained SER model\\footnote{https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition} to identify the emotion category of 400 audio samples synthesized from the ECSS and all baselines, respectively. As shown in Fig. \\ref{fig3}, we plot the confusion matrices to show the gap between the different systems. It can be seen that our ECSS outperforms all baselines by presenting a clear diagonal line in the confusion matrix. It proves that the emotional conversational speech synthesized by ECSS performs a clear emotional expression.\n% \\hl{add description about emotion intensity.} \nIn addition, we further conduct a visualization study to validate the emotion intensity rendering. Specifically, five listeners were invited and asked to rate the emotion intensity labels of 400 audios. As shown in Fig. \\ref{fig4}, the confusion matrix suggests that the emotional conversational speech synthesized by ECSS performs a clear emotional intensity expression.\nThe above results also provide further evidence that our ECSS draws on the contextual modeling capabilities of heterogeneous graph and the effective constraints of contrastive learning, thus leading to remarkable performance in both emotion and emotion intensity rendering.\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-1mm}\n"
                    }
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nTo improve the emotion understanding and rendering in CSS systems, we present a novel ECSS model whereby a heterogeneous Emotional Conversational Graph (ECG) armed with multi-source knowledge in context is used for emotional context modeling, and the emotion renderer with contrastive learning constraint to achieve accurate emotional style inference. Experimental results demonstrate the superiority of ECSS over state-of-the-art CSS systems. The contribution of heterogeneous nodes in ECG and emotion renderer are further demonstrated in ablation studies. To the best of our knowledge, ECSS is the first in-depth conversational speech synthesis study that models emotional expressiveness.  We hope that our work will serve as a basis for future emotional CSS studies.\n\n \n\n\n% \\section{Reference Examples}\n% \\label{sec:reference_examples}\n\n% \\nobibliography*\n% Formatted bibliographies should look like the following examples. You should use BibTeX to generate the references. Missing fields are unacceptable when compiling references, and usually indicate that you are using the wrong type of entry (BibTeX class).\n\n% \\paragraph{Book with multiple authors~\\nocite{em:86}} Use the \\texttt{@book} class.\\\\[.2em]\n% \\bibentry{em:86}.\n\n% \\paragraph{Journal and magazine articles~\\nocite{r:80, hcr:83}} Use the \\texttt{@article} class.\\\\[.2em]\n% \\bibentry{r:80}.\\\\[.2em]\n% \\bibentry{hcr:83}.\n\n% \\paragraph{Proceedings paper published by a society, press or publisher~\\nocite{c:83, c:84}} Use the \\texttt{@inproceedings} class. You may abbreviate the \\emph{booktitle} field, but make sure that the conference edition is clear.\\\\[.2em]\n% \\bibentry{c:84}.\\\\[.2em]\n% \\bibentry{c:83}.\n\n% \\paragraph{University technical report~\\nocite{r:86}} Use the \\texttt{@techreport} class.\\\\[.2em]\n% \\bibentry{r:86}.\n\n% \\paragraph{Dissertation or thesis~\\nocite{c:79}} Use the \\texttt{@phdthesis} class.\\\\[.2em]\n% \\bibentry{c:79}.\n\n% \\paragraph{Forthcoming publication~\\nocite{c:21}} Use the \\texttt{@misc} class with a \\texttt{note=\"Forthcoming\"} annotation.\n% \\begin{quote}\n% \\begin{footnotesize}\n% \\begin{verbatim}\n% @misc(key,\n%   [...]\n%   note=\"Forthcoming\",\n% )\n% \\end{verbatim}\n% \\end{footnotesize}\n% \\end{quote}\n% \\bibentry{c:21}.\n\n% \\paragraph{ArXiv paper~\\nocite{c:22}} Fetch the BibTeX entry from the \"Export Bibtex Citation\" link in the arXiv website. Notice it uses the \\texttt{@misc} class instead of the \\texttt{@article} one, and that it includes the \\texttt{eprint} and \\texttt{archivePrefix} keys.\n% \\begin{quote}\n% \\begin{footnotesize}\n% \\begin{verbatim}\n% @misc(key,\n%   [...]\n%   eprint=\"xxxx.yyyy\",\n%   archivePrefix=\"arXiv\",\n% )\n% \\end{verbatim}\n% \\end{footnotesize}\n% \\end{quote}\n% \\bibentry{c:22}.\n\n% \\paragraph{Website or online resource~\\nocite{c:23}} Use the \\texttt{@misc} class. Add the url in the \\texttt{howpublished} field and the date of access in the \\texttt{note} field:\n% \\begin{quote}\n% \\begin{footnotesize}\n% \\begin{verbatim}\n% @misc(key,\n%   [...]\n%   howpublished=\"\\url{http://...}\",\n%   note=\"Accessed: YYYY-mm-dd\",\n% )\n% \\end{verbatim}\n% \\end{footnotesize}\n% \\end{quote}\n% \\bibentry{c:23}.\n\n% For the most up to date version of the AAAI reference style, please consult the \\textit{AI Magazine} Author Guidelines at \\url{https://aaai.org/ojs/index.php/aimagazine/about/submissions#authorGuidelines}\n\n"
            },
            "section 7": {
                "name": "Acknowledgments",
                "content": "\nThe research by Rui Liu was funded by \n% the High-level Talents Introduction Project of Inner Mongolia University (No. 10000-22311201)\n% \\textcolor{black}{(with Rui Liu as the Principal Investigator)}\n% , \nthe Young Scientists Fund of the National Natural Science Foundation of China (No. 62206136), and Guangdong Provincial Key Laboratory of Human Digital Twin (No. 2022B1212010004).\nThe research by Yifan Hu was funded by the Research and Innovation Projects for Graduate Students in Inner Mongolia Autonomous Region (No. 11200-121028).\nThe work by Haizhou Li was supported by the National Natural Science Foundation of China (Grant No. 62271432), and Shenzhen Science and Technology Research Fund (Fundamental Research Key Project Grant No. JCYJ20220818103001002).\n\n\\bigskip\n% \\noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!\n\n\\bibliography{aaai24}\n\n\n\\appendix\n\n% \\newpage\n"
            },
            "section 8": {
                "name": "Appendix. Detailed Experimental Settings",
                "content": "\n\\label{app:config}\n\n\n% \\subsection{ More Details of Model Configurations}\n% \\label{app:config1}\n\n\nWe list the detailed model setup of ECSS. The detailed experimental settings of the \\textit{Heterogeneous Graph-based Emotional Context Encoder} and \\textit{Emotional Conversational Speech Synthesizer} are shown in Table \\ref{tab:setup}.\n\n\n\n\n\n \n \n% Specifically, the text encoder contains a pre-trained BERT and a multi-layer position-wise feed-forward network $\\rm FFN(\\cdot)$, that reads $u_{\\mathcal{C}}$ as input to obtain the content representation $\\mathcal{H}^c_\\mathcal{C}$ for the current utterance: $\\mathcal{H}^c_\\mathcal{C}= {\\rm FFN(BERT}(u_{\\mathcal{C}}))$. The speaker encoder shares the same concept as that in the ECG initialization section, which uses the same trainable parameter matrix $f_{s_\\mathcal{C}}$ to obtain the speaker representation $\\mathcal{H}^s_\\mathcal{C}$ for the current utterance $u_{\\mathcal{C}}$. \n% Note that the emotion renderer attempts to predict the current utterance's emotion, intensity, and prosody features $\\mathcal{H}^e_\\mathcal{C}$, $\\mathcal{H}^i_\\mathcal{C}$, and $\\mathcal{H}^p_\\mathcal{C}$ using the graph-enhanced node features. \n% To obtain a robust feature representation of the current utterance. we set a set of trainable weight parameters $\\mathcal{W}_s$, $\\mathcal{W}_p$, $\\mathcal{W}_e$, and $\\mathcal{W}_i$ for the above four features. The final input to the acoustic decoder is $\\mathcal{H}_\\mathcal{C}$: $\\mathcal{H}_\\mathcal{C}=\\mathcal{H}^c_\\mathcal{C}+\\mathcal{W}_s\\ast \\mathcal{H}^s_\\mathcal{C} +\\mathcal{W}_a\\ast \\mathcal{H}^a_\\mathcal{C}+\\mathcal{W}_e\\ast \\mathcal{H}^e_\\mathcal{C}+\\mathcal{W}_i\\ast \\mathcal{H}^i_\\mathcal{C}$.\n\n \n \n% \\textbf{Prosody Predictor} adopts a multi-head attention layer to infer the speaking prosody information of the current utterance from the feature representations of text nodes in the dialogue history. The features of audio nodes are not used because we believe that audio and text modalities have already interacted during the ECG encoding, thus the text node already contains audio information. \n    \n%     \\begin{equation}\n%         \\mathcal{H}^p_\\mathcal{C}= {\\rm MultiHead(}f_{u_{\\mathcal{C}}}, f_u^{'}, f_u^{'})\n%         % , H_p\\in \\mathbb{R}^{N\\times d_p}\n%     \\end{equation}\n%     where $f_{u_{\\mathcal{C}}}$ indicates the $Query$. $Key$ and $Value$ are the $f_u^{'}$ that represent all text-type nodes in the dialogue history after ECG encoding.\n%     % \\textcolor{red}{A two-dimensional matrix of all textual node representations $f_u^{'}$ (excluding $f_{u_{J}}^{'}$) of the heterogeneous graph-encoded dialogue history utterances is formed as key and value.}\n%     % $d_a$ is the dimension of the rhyme information. \n    \n% We use the MSE loss for the prosody predictor of emotion renderer to constrain its predicted rhymes. Specifically, we introduce a GST-based prosody extractor that extracts the accurate prosody information $\\mathcal{Y}^{p}_{C}$ of the current audio. The extractor comprises a reference encoder and a style token layer with 7 GSTs. Prosodic constraint loss $\\mathcal{L}^{mse}_{pro}$ is calculated as follows:\n% \\begin{equation}\n%     \\mathcal{L}^{mse}_{pro} = MSE(\\mathcal{H}^{p}_{C},\\mathcal{Y}^{p}_{C})\n% \\end{equation}\n \n% }\n\n \n\n"
            }
        },
        "figures": {
            "fig1": "\\begin{figure}[ht!]\n\\centering\n% \\setlength{\\abovecaptionskip}{0mm}   %\u8c03\u6574\u56fe\u7247\u6807\u9898\u4e0e\u56fe\u8ddd\u79bb\n\\centerline{\\includegraphics[width=1\\linewidth]{fig/fig1.pdf}}\n\\vspace{-2mm}\n\\caption{Graphical depiction of spoken conversation, where different emotion cues in the dialogue history perform a direct impact on the emotion expression for the current utterance.\n% \\textcolor{red}{to Rui: I cannot understand the scenario of the dialogue from the scripts - and the relation to the emotion. i suggest that we use a different example. Current Utterance $=>$ current utterance; which is the current utterance and current speaker in the diagram?}\n}\n\\vspace{-8mm}\n\\label{fig1}\n\\end{figure}",
            "fig:model": "\\begin{figure*}[t!]\n\\centering\n% \\setlength{\\abovecaptionskip}{-0mm}   %\u8c03\u6574\u56fe\u7247\u6807\u9898\u4e0e\u56fe\u8ddd\u79bb\n\\centerline{\n\\includegraphics[width=1.06\\linewidth]{fig/fig2-new.pdf}\n}\n\\vspace{-1mm}\n\\caption{The overall architecture of ECSS.\n1) Multi-source knowledge includes text, speaker, audio, emotion, and emotion intensity information of the multi-turn conversation; 2) Heterogeneous graph-based emotional context encoder aims to model the complex dependencies among the multi-source knowledge with the Emotional Conversational Graph (ECG), thus understanding the emotion cues in context; 3) Emotion rendering for CSS seeks to render the accurate emotion state for synthesized speech, in which \\textit{Emotion Renderer} aims to infer the emotion feature according to the heterogeneous ECG encoding.\n% (j $\\in$ [1, $\\mathcal{J}$]).\n}\n\\vspace{-5mm}\n\\label{fig:model}\n\\end{figure*}",
            "fig3": "\\begin{figure}[t!]\n\\centering\n% \\setlength{\\abovecaptionskip}{0mm}   %\u8c03\u6574\u56fe\u7247\u6807\u9898\u4e0e\u56fe\u8ddd\u79bb\n\\centerline{\\includegraphics[width=\\linewidth]{fig/vis_emo.png}}\n\\vspace{-2mm}\n\\caption{The confusion matrix results about the emotion category rendering with the help of speech emotion recognition. The X-axis and Y-axis of subfigures represent perceived and true emotion categories.}\n% \\textcolor{red}{to Rui: I cannot understand the scenario of the dialogue from the scripts - and the relation to the emotion. i suggest that we use a different example. Current Utterance $=>$ current utterance; which is the current utterance and current speaker in the diagram?}\n\\vspace{-3mm}\n\\label{fig3}\n\\end{figure}",
            "fig4": "\\begin{figure}[t!]\n\\centering\n% \\setlength{\\abovecaptionskip}{0mm}   %\u8c03\u6574\u56fe\u7247\u6807\u9898\u4e0e\u56fe\u8ddd\u79bb\n\\centerline{\\includegraphics[width=0.75\\linewidth]{fig/vis_int.png}}\n\\vspace{-3mm}\n\\caption{The confusion matrix results about the emotion {intensity} rendering. The X-axis and Y-axis of subfigures represent perceived and true {intensity} categories.}\n% \\textcolor{red}{to Rui: I cannot understand the scenario of the dialogue from the scripts - and the relation to the emotion. i suggest that we use a different example. Current Utterance $=>$ current utterance; which is the current utterance and current speaker in the diagram?}\n\\vspace{-3.5mm}\n\\label{fig4}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n% \\scriptsize\n    \\mathcal{L}^{cl}_{emo}= log \\frac{-1}{|\\mathcal{P}(k)|} \\frac{\\underset{\\mathcal{H}^{e}_{\\mathcal{C}q}\\in \\mathcal{P}(k)}{\\sum } exp(sim(\\mathcal{H}^{e}_{\\mathcal{C}k},\\mathcal{H}^{e}_{\\mathcal{C}q} )/\\tau)}{\\underset{\\mathcal{H}^{e}_{\\mathcal{C}d}\\in B(k)}{\\sum} exp(sim(\\mathcal{H}^{e}_{\\mathcal{C}k},\\mathcal{H}^{e}_{\\mathcal{C}d})/\\tau)}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n% \\scriptsize\n    \\mathcal{L}^{cl}_{int}= log \\frac{-1}{|\\mathcal{P}(k)|} \\frac{\\underset{\\mathcal{H}^{i}_{\\mathcal{C}q}\\in \\mathcal{P}(k)}{\\sum } exp(sim(\\mathcal{H}^{i}_{\\mathcal{C}k},\\mathcal{H}^{i}_{\\mathcal{C}q} )/\\tau)}{\\underset{\\mathcal{H}^{i}_{\\mathcal{C}d}\\in B(k)}{\\sum} exp(sim(\\mathcal{H}^{i}_{\\mathcal{C}k},\\mathcal{H}^{i}_{\\mathcal{C}d})/\\tau)}\n\\end{equation}"
        },
        "git_link": "https://github.com/walker-hyf/ECSS"
    }
}