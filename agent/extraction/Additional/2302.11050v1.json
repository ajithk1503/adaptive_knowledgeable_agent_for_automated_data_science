{
    "meta_info": {
        "title": "Edgeformers: Graph-Empowered Transformers for Representation Learning on  Textual-Edge Networks",
        "abstract": "Edges in many real-world social/information networks are associated with rich\ntext information (e.g., user-user communications or user-product reviews).\nHowever, mainstream network representation learning models focus on propagating\nand aggregating node attributes, lacking specific designs to utilize text\nsemantics on edges. While there exist edge-aware graph neural networks, they\ndirectly initialize edge attributes as a feature vector, which cannot fully\ncapture the contextualized text semantics of edges. In this paper, we propose\nEdgeformers, a framework built upon graph-enhanced Transformers, to perform\nedge and node representation learning by modeling texts on edges in a\ncontextualized way. Specifically, in edge representation learning, we inject\nnetwork information into each Transformer layer when encoding edge texts; in\nnode representation learning, we aggregate edge representations through an\nattention mechanism within each node's ego-graph. On five public datasets from\nthree different domains, Edgeformers consistently outperform state-of-the-art\nbaselines in edge classification and link prediction, demonstrating the\nefficacy in learning edge and node representations, respectively.",
        "author": "Bowen Jin, Yu Zhang, Yu Meng, Jiawei Han",
        "link": "http://arxiv.org/abs/2302.11050v1",
        "category": [
            "cs.LG",
            "cs.CL",
            "cs.SI"
        ],
        "additionl_info": "ICLR 2023. (Code: https://github.com/PeterGriffinJin/Edgeformers)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\vspace{-0.3cm}\n\nNetworks are ubiquitous and are widely used to model interrelated data in the real world, such as user-user and user-item interactions on social media \\citep{kwak2010twitter,leskovec2010predicting} and recommender systems \\citep{wang2019neural,jin2020multi}. \nIn recent years, graph neural networks (GNNs) \\citep{kipf2017semi,hamilton2017inductive,velivckovic2017graph,xu2018powerful} have demonstrated their power in network representation learning. However, a vast majority of GNN models leverage node attributes only and lack specific designs to capture information on edges. (We refer to these models as \\textit{node-centric} GNNs.)\nYet, in many scenarios, there is rich information associated with edges in a network. For example, when a person replies to another on social media, there will be a directed edge between them accompanied by the response texts; when a user comments on an item, the user's review will be naturally associated with the user-item edge.\n\nTo utilize edge information during network representation learning, some edge-aware GNNs \\citep{gong2019exploiting,jiang2019censnet,yang2020nenn,jo2021edge} have been proposed. Nevertheless, these studies assume the information carried by edges can be directly described as an attribute vector. This assumption holds well when edge features are categorical (\\textit{e.g.}, bond features in molecular graphs \\citep{hu2020open} and relation features in knowledge graphs \\citep{schlichtkrull2018modeling}). \nHowever, effectively modeling free-text edge information in edge-aware GNNs has remained elusive, mainly because bag-of-words and context-free embeddings \\citep{mikolov2013distributed} used in previous edge-aware GNNs cannot fully capture contextualized text semantics.\nFor example, ``\\textit{Byzantine}'' in history book reviews and ``\\textit{Byzantine}'' in distributed system papers should have different meanings given their context, but they correspond to the same entry in a bag-of-words vector and have the same context-free embedding.\n\nTo accurately capture contextualized semantics, a straightforward idea is to integrate pretrained language models (PLMs) \\citep{devlin2018bert,liu2019roberta,clark2020electra} with GNNs. \nIn node-centric GNN studies, this idea has been instantiated by a PLM-GNN cascaded architecture \\citep{fang2020hierarchical,li2021adsgnn,zhu2021textgnn}, where text information is first encoded by a PLM and then aggregated by a GNN. \nHowever, such architectures process text and graph signals one after the other, and fail to simultaneously model the deep interactions between both types of information.\nThis could be a loss to the text encoder because network signals are often strong indicators to text semantics.\nFor example, a brief political tweet may become more comprehensible if the stands of the two communicators are known.\nTo deeply couple PLMs and GNNs, the recent GraphFormers model \\citep{yang2021graphformers} proposes a GNN-nested PLM architecture to inject network information into the text encoding process. They introduce GNNs nested in between Transformer layers so that the center node encoding not only leverages its own textual information, but also aggregates the signals from its neighbors. Nevertheless, they assume that only nodes are associated with textual information and cannot be easily adapted to handle text-rich edges.\n\nTo effectively model the textual and network structure information via a unified encoder architecture, in this paper, we propose a novel network representation learning framework, \\Ours, that leverage graph-enhanced Transformers to model edge texts in a contextualized way. \\Ours include two architectures, \\OursE and \\OursN, for edge and node representation learning, respectively. \nIn \\OursE, we add virtual node tokens to each Transformer layer inside the PLM when encoding edge texts. Such an architecture goes beyond the PLM-GNN cascaded architecture and enables deep, layer-wise interactions between network and text signals to produce edge representations. \nIn \\OursN, we aggregate the network-and-text-aware edge representations to obtain node representations through an attention mechanism within each node's ego-graph.\nThe two architectures can be trained via edge classification (which relies on good edge representations) and link prediction (which relies on good node representations) tasks, respectively.\nTo summarize, our main contributions are as follows:\n\\vspace{-0.2cm}\n\\begin{itemize}[leftmargin=*]\n  \\item Conceptually, we identify the importance of modeling text information on network edges and formulate the problem of representation learning on textual-edge networks.\n  \\item Methodologically, we propose \\Ours (\\textit{i.e.}, \\OursE and \\OursN), two graph-enhanced Transformer architectures, to deeply couple network and text information in a contextualized way for edge and node representation learning.\n  \\item Empirically, we conduct experiments on five public datasets from different domains and demonstrate the superiority of \\Ours over various baselines, including node-centric GNNs, edge-aware GNNs, and PLM-GNN cascaded architectures.\n\\end{itemize}\n\n\\vspace{-0.5cm}\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\vspace{-0.3cm}\n\n",
                "subsection 2.1": {
                    "name": "Textual-Edge Networks",
                    "content": "\n\\vspace{-0.2cm}\n\nIn a textual-edge network, each edge is associated with texts. We view the texts on each edge as a document, and all such documents constitute a corpus $\\mathcal{D}$. Since the major goal of this work is to explore the effect of textual information on edges, we assume there is no auxiliary information (\\textit{e.g.}, categorical or textual attributes) associated with nodes in the network. \n\\vspace{-0.2cm}\n\\newtheorem{definition}{Definition}\n\\begin{definition}{(Textual-Edge Networks)}\nA textual-edge network is defined as $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E}, \\mathcal{D})$, where $\\mathcal{V}$, $\\mathcal{E}$, $\\mathcal{D}$ represent the sets of nodes, edges, and documents, respectively. Each edge $e_{ij} \\in \\mathcal{E}$ is associated with a document $d_{ij} \\in \\mathcal{D}$.\n\\end{definition}\n\\vspace{-0.2cm}\nTo give an example of textual-edge networks, consider a review network (\\textit{e.g.}, Amazon \\citep{he2016ups}) where nodes are users and items. If a user $v_i$ writes a review about an item $v_j$, there will be an edge $e_{ij}$ connecting them, and the review text will be the associated document $d_{ij}$.\n\n\\vspace{-0.3cm}\n"
                },
                "subsection 2.2": {
                    "name": "Transformer",
                    "content": "\n\\vspace{-0.2cm}\nMany PLMs (\\textit{e.g.}, BERT \\citep{devlin2018bert}) adopt a multi-layer Transformer architecture \\citep{vaswani2017attention} to encode texts.\nEach Transformer layer utilizes a multi-head self-attention mechanism to obtain a contextualized representation of each text token.\nSpecifically, let $\\bmH^{(l)}=[\\bm{h}^{(l)}_1, \\bm{h}^{(l)}_2, ..., \\bm{h}^{(l)}_n]$ denote the output sequence of the $l$-th Transformer layer, where $\\bm{h}^{(l)}_i\\in\\mathcal{R}^d$ is the hidden representation of the text token at position $i$.\nThen, in the $(l+1)$-th Transformer layer, the multi-head self-attention (MHA) is calculated as\n\\vspace{-0.2cm}\n\\begin{gather}\n    {\\rm MHA}({\\bmH}^{(l)}) = \\mathop{\\Vert}_{t=1}^k {\\rm head}^t({\\bmH}^{(l)}_{t}) \\\\\n    {\\rm head}^t({\\bmH}^{(l)}_{t}) = \\bmV^{(l)}_{t}\\cdot{\\rm softmax}(\\frac{\\bmK^{(l)\\top}_{t}\\bmQ^{(l)}_{t}}{\\sqrt{d/k}}) \\\\\n    \\bmQ^{(l)}_{t} = \\bmW^{(l)}_{Q,t}{\\bmH}^{(l)}_{t},\\ \\ \\ \\  \\bmK^{(l)}_{t} = \\bmW^{(l)}_{K,t}{\\bmH}^{(l)}_{t},\\ \\ \\ \\ \\bmV^{(l)}_{t} = \\bmW^{(l)}_{V,t}{\\bmH}^{(l)}_{t},\n\\end{gather}\nwhere $\\bmW_{Q,t}, \\bmW_{K,t}, \\bmW_{V,t}$ are query, key, and value matrices to be learned by the model, $k$ is the number of attention head and $\\Vert$ is the concatenate operation.\n\n\\vspace{-0.2cm}\n"
                },
                "subsection 2.3": {
                    "name": "Problem Definitions",
                    "content": "\\label{sec:problem}\n\\vspace{-0.2cm}\nOur general goal is to learn meaningful edge and node embeddings in textual-edge networks so as to benefit downstream tasks. To be specific, we consider the following two tasks focusing on edge representation learning and node representation learning, respectively.\n\nThe first task is \\textit{edge classification}, which relies on learning a good representation $\\bmh_{e}$ of an edge $e \\in \\mathcal{E}$. We assume each edge $e_{ij}$ belongs to a category $y \\in \\mathcal{Y}$. The category can be indicated by its associated text $d_{ij}$ and/or the nodes $v_i$ and $v_j$. For example, in the Amazon review network, $\\mathcal{Y}=\\{$1-star, 2-star, ..., 5-star$\\}$. The category of $e_{ij}$ reflects how the user $v_i$ is satisfied with the item $v_j$, which may be expressed by the sentiment of $d_{ij}$ and/or implied by $v_i$'s preference and $v_j$'s quality. Given a review, the task is to predict its category based on review text and user/item information.\n\\vspace{-0.2cm}\n\\begin{definition}{(Edge Classification)}\nIn a textual-edge network $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E}, \\mathcal{D})$, we can observe the category of some edges $\\mathcal{E}_{train} \\subseteq \\mathcal{E}$. Given an edge $e_{ij} \\in \\mathcal{E}\\backslash \\mathcal{E}_{train}$, predict its category $y \\in \\mathcal{Y}$ based on $d_{ij} \\in \\mathcal{D}$ and $v_i,v_j\\in \\mathcal{V}$.\n\\end{definition}\n\\vspace{-0.2cm}\nThe second task is \\textit{link prediction}, which relies on learning an accurate representation $\\bmh_{v_i}$ of a node $v_i \\in \\mathcal{V}$. Given two nodes $v_i$ and $v_j$, the task is to predict whether there is an edge between them. Note that, unlike edge classification, we no longer have the text information $d_{ij}$ (because we even do not know whether $e_{ij}$ exists). Instead, we need to exploit other edges (local network structure) involving $v_i$ or $v_j$ as well as their text to learn node representations $\\bmh_{v_i}$ and $\\bmh_{v_j}$. \nFor example, in the Amazon review network, we aim to predict whether a user will be satisfied with a product according to the user's reviews towards other products and the item's reviews from other users.\n\\vspace{-0.2cm}\n\\begin{definition}{(Link Prediction)}\nIn a textual-edge network $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E}, \\mathcal{D})$, we can observe some edges $\\mathcal{E}_{train} \\subseteq \\mathcal{E}$ and their associated text. Given $v_i,v_j \\in \\mathcal{V}$ where $e_{ij} \\notin \\mathcal{E}_{train}$, predict \nwhether $e_{ij} \\in \\mathcal{E}$.\n\\end{definition}\n\\vspace{-0.5cm}\n"
                }
            },
            "section 3": {
                "name": "Proposed Method",
                "content": "\n\\vspace{-0.4cm}\n\nIn this section, we present our \\Ours framework. Based on the two tasks mentioned in Section \\ref{sec:problem}, we first introduce how we conduct \\textit{edge} representation learning by jointly considering text and network information via a Transformer-based architecture (\\OursE). Then, we illustrate how to perform \\textit{node} representation learning using the edge representation learning module as building blocks (\\OursN). The overview of \\Ours is shown in Figure \\ref{overview}.\n\n\n\n\\vspace{-0.2cm}\n",
                "subsection 3.1": {
                    "name": "Edge Representation Learning (\\OursE)",
                    "content": "\\label{edge-representation}\n\n\\vspace{-0.2cm}\n\\textbf{Network-aware Edge Text Encoding with Virtual Node Tokens.}\nEncoding $d_{ij}$ in a textual-edge network is different from encoding plain text, mainly because edge texts are naturally accompanied by network structure information, which can provide auxiliary signals. \nGiven that text semantics can be well captured by a multi-layer Transformer architecture \\citep{devlin2018bert}, we propose a simple and effective way to inject network signals into the Transformer encoding process. The key idea is to introduce \\textit{virtual node tokens}. \nGiven an edge $e_{ij} = (v_i, v_j)$ and its associated texts $d_{ij}$, let $\\bmH^{(l)}_{e_{ij}}\\in\\mathcal{R}^{d\\times n}$ denote the output representations of all text tokens in $d_{ij}$ after the $l$-th model layer ($l\\geq 1$).\nIn each layer, we introduce two \\textit{virtual node tokens} to represent $v_i$ and $v_j$, respectively. Their embeddings are denoted as $\\bm{z}^{(l)}_{v_i}$ and $\\bm{z}^{(l)}_{v_j} \\in \\mathcal{R}^d$, which are concatenated to the text token sequence hidden states as follows:\n\\vspace{-0.2cm}\n\\begin{gather}\\label{edge-prop}\n    \\widetilde{\\bmH}^{(l)}_{e_{ij}} = \\bm{\\bm{z}}^{(l)}_{v_i} \\mathop{\\Vert} \\bm{z}^{(l)}_{v_j} \\mathop{\\Vert} \\bmH^{(l)}_{e_{ij}}.\n\\end{gather}\nAfter the concatenation, $\\widetilde{\\bmH}^{(l)}_{e_{ij}}$ contains information from both $e_{ij}$'s associated text $d_{ij}$ and its involving nodes $v_i$ and $v_j$. To let text token representations carry node signals, we adopt a multi-head attention mechanism:\n\\vspace{-0.2cm}\n\\begin{gather}\n    {\\rm MHA}(\\bmH^{(l)}_{e_{ij}},\\widetilde{\\bmH}^{(l)}_{e_{ij}}) = \\mathop{\\Vert}_{t=1}^k {\\rm head}^t(\\bmH^{(l)}_{e_{ij},t},\\widetilde{\\bmH}^{(l)}_{e_{ij},t}), \\label{mha_asy} \\\\\n    \\bmQ^{(l)}_{t} = \\bmW^{(l)}_{Q,t}\\bmH^{(l)}_{e_{ij},t},\\ \\ \\ \\  \\bmK^{(l)}_{t} = \\bmW^{(l)}_{K,t}\\widetilde{\\bmH}^{(l)}_{e_{ij},t},\\ \\ \\ \\ \\bmV^{(l)}_{t} = \\bmW^{(l)}_{V,t}\\widetilde{\\bmH}^{(l)}_{e_{ij},t}.\n\\end{gather}\nIn Eq. (\\ref{mha_asy}), the multi-head attention is asymmetric (\\textit{i.e.}, the keys $\\bmK$ and values $\\bmV$ are augmented with virtual node embeddings but queries $\\bmQ$ are not) to avoid network information being overwritten by text signals. This design has been used in existing studies \\cite{yang2021graphformers}, and offers better effectiveness than the original self-attention mechanism according to our experiments in Section \\ref{edge-setting}. The output of MHA includes updated node-aware representations of text tokens. Then, following the Transformer architecture \\citep{vaswani2017attention}, the updated representations will go through a feed-forward network (FFN) to finish our $(l+1)$-th model layer encoding. Formally,\n\\begin{gather}\\label{MHA}\n    \\bmH^{(l)'}_{e_{ij}} = {\\rm Normalize}(\\bmH^{(l)}_{e_{ij}} + {\\rm MHA}(\\bmH^{(l)}_{e_{ij}}, \\widetilde{\\bmH}^{(l)}_{e_{ij}})), \\\\\n    \\bmH^{(l+1)}_{e_{ij}} = {\\rm Normalize}(\\bmH^{(l)'}_{e_{ij}} + {\\rm FFN}(\\bmH^{(l)'}_{e_{ij}})),  \\label{FFN}\n\\end{gather}\nwhere ${\\rm Normalize}(\\cdot)$ is the layer normalization function.\nAfter $L$ model layers, the final representation of the [CLS] token will be used as the edge representation of $e_{ij}$, \\textit{i.e.}, ${\\bmh}_{e_{ij}}=\\bmH^{(L)}_{e_{ij}}[{\\rm CLS}]$.\n\n\\vspace{-0.25cm}\n\\textbf{Representation of Virtual Node Tokens.} \nThe virtual node representation $\\bmz^{(l)}_{v_i}$ used in Eq.(\\ref{edge-prop}) is obtained by a layer-specific mapping of the initial node embedding $\\bm{z}^{(0)}_{v_i}$. Formally,\n\\begin{gather}\\label{node-initial}\n    \\bmz^{(l)}_{v_i} = \\bmW^{(l)}_n \\bmz^{(0)}_{v_i},\n\\end{gather}\nwhere $\\bmW^{(l)}_n\\in\\mathcal{R}^{d\\times d'}$ is the mapping matrix for the $l$-th layer. \nThe large population of nodes will introduce a large number of parameters to our framework, which may finally lead to model underfitting. As a result, in \\Ours, we set the initial node embedding to be low-dimensional (\\textit{e.g.}, $\\bmz^{(0)}_{v_i} \\in \\mathbb{R}^{64}$) and project it to the high-dimensional token representation space (\\textit{e.g.}, $\\bmz^{(l)}_{v_i} \\in \\mathbb{R}^{768}$).\nNote that it is possible to go beyond the linear mapping in Eq. (\\ref{node-initial}) and use structure-aware encoders such as GNNs to obtain $\\bmz^{(l)}_{v_i}$, and we leave such extensions for future studies.\n\\vspace{-0.2cm}\n\n"
                },
                "subsection 3.2": {
                    "name": "Text-Aware Node Representation Learning (\\OursN)",
                    "content": "\n\\vspace{-0.3cm}\nIn this section, we first discuss how to perform text-aware node representation learning by taking the aforementioned edge representation learning module (\\textit{i.e.}, \\OursE) as building blocks. Then, we propose to enhance the edge representation learning module with the target node's additional local network structure.\n\n\\textbf{Aggregating Edge Representations.}\nSince the edge representations learned by \\OursE capture both text semantics and network structure information, a straightforward way to obtain a node representation is to aggregate the representations of all edges involving the node. Given a node $v_i$, its representation ${\\bmh}_{v_i}$ is given by\n\\begin{gather}\\label{final-agg}\n    {\\bmh}_{v_i} = {\\rm AGG}(\\{{\\bmh}_{e_{ij}}|e_{ij}\\in \\mathcal{N}_e(v_i)\\}),\n\\end{gather}\nwhere $\\mathcal{N}_e(v_i)$ is the set of edges containing $v_i$. ${\\rm AGG}(\\cdot)$ can be any permutation invariant function such as $\\rm mean(\\cdot)$ or $\\rm max(\\cdot)$. Here, we instantiate ${\\rm AGG}(\\cdot)$ with an attention-based aggregation:\n\\begin{gather}\n    \\alpha_{e_{ij}, v_i}={\\rm softmax}({\\bmh}^\\top_{e_{ij}}\\bmW_s \\bmz^{(0)}_{v_i}), \\ \\ \\ \\ \n    {\\bmh}_{v_i} = \\sum_{e_{ij}\\in \\mathcal{N}_e(v_i)}\\alpha_{e_{ij}, v_i} {\\bmh}_{e_{ij}},\n\\end{gather}\n\\vspace{-0.2cm}\nwhere $\\bmW_s\\in\\mathcal{R}^{d\\times d'}$ is a learnable scoring matrix.\n\n\\textbf{Enhancing Edge Representations with the Node's Local Network Structure.}\nSince we are aggregating information from multiple edges, it is intuitive that they can mutually improve each other's representation by providing auxiliary semantic signals. For example, given a conversation about ``Transformers'' and their participants' other conversations centered around ``machine learning'', it is more likely that the term ``Transformers'' refers to a deep learning architecture rather than a character in the movie. \nTo implement this intuition in the edge representation learning module, we introduce the third virtual token hidden state $\\bar{\\bmh}^{(l)}_{e_{ij}|v_i}$ during edge encoding:\n\\vspace{-0.1cm}\n\\begin{gather}\\label{node-agg}\n    \\widetilde{\\bmH}^{(l)}_{e_{ij}|v_i} = \\bm{z}^{(l)}_{v_i} \\mathop{\\Vert} \\bm{z}^{(l)}_{v_j} \\mathop{\\Vert} \\bar{\\bmh}^{(l)}_{e_{ij}|v_i} \\mathop{\\Vert} \\bmH^{(l)}_{e_{ij}},\n\\end{gather}\n\\vspace{-0.2cm}\nwhere $\\bar{\\bmh}^{(l)}_{e_{ij}|v_i}$ is the contextualized representation of $e_{ij}$ given target node $v_i$'s local network structure. Now we introduce how to calculate $\\bar{\\bmh}^{(l)}_{e_{ij}|v_i}$ by aggregating information from $\\mathcal{N}_e(v_i)$.\n\n\\vspace{-0.2cm}\n\\textbf{Representation of $\\bar{\\bmh}^{(l)}_{e_{ij}|v_i}$.} \nFor each edge $e_{is}\\in \\mathcal{N}_e(v_i)$ (including $e_{ij}$), we treat the hidden state of its [CLS] token after the $l$-th layer as its representation (\\textit{i.e.}, $\\bmh^{(l)}_{e_{is}}=\\bmH^{(l)}_{e_{is}}[{\\rm CLS}]$). \nTo obtain $\\bar{\\bmh}^{(l)}_{e_{is}|v_i}$, we adopt MHA to let all edges in $\\mathcal{N}_e(v_i)$ interact with each other. \n\\begin{gather}\\label{node-prop}\n    \\left [...,\\bar{\\bmh}^{(l)}_{e_{ij}|v_i},...,\\bar{\\bmh}^{(l)}_{e_{is}|v_i},... \\right] = {\\rm MHA}\\left(\\left [...,{\\bmh}^{(l)}_{e_{ij}},...,{\\bmh}^{(l)}_{e_{is},...} \\right]\\right).\n\\end{gather}\nIn the equation above, $\\left [...,{\\bmh}^{(l)}_{e_{ij}},...,{\\bmh}^{(l)}_{e_{is},...} \\right]$ contains the $l$-th layer representations of all edges involving $v_i$. Therefore, after MHA, the edge representation $\\bar{\\bmh}^{(l)}_{e_{ij}|v_i}$ essentially aggregates information from $v_i$'s local network structure $\\mathcal{N}_e(v_i)$.\n\n\\textbf{Connection between \\OursN and GNNs.} \nAccording to Figure \\ref{overview}, \\OursN adopts a Transformer-based architecture. Meanwhile, it can also be viewed as a GNN model. Indeed, GNN models \\citep{wu2020comprehensive,yang2020heterogeneous} mainly adopt a propagation-aggregation paradigm to obtain node representations:\n\\begin{small}\n\\begin{gather}\n    \\bm{a}^{(l-1)}_{ij} = {\\rm PROP}^{(l)}\\left(\\bmh^{(l-1)}_i,\\bmh^{(l-1)}_j\\right), \\big(\\forall j\\in \\mathcal{N}(i)\\big); \\ \\ \n    \\bmh^{(l)}_i = {\\rm AGG}^{(l)}\\left(\\bmh^{(l-1)}_i,\\{\\bm{a}^{(l-1)}_{ij}|j\\in \\mathcal{N}(i)\\}\\right).\n\\end{gather}\n\\end{small}\nAnalogously, in \\OursN, Eq. (\\ref{node-prop}) can be treated as the propagation function ${\\rm PROP}^{(l)}$, and the aggregation step ${\\rm AGG}^{(l)}$ is the combination of Eqs. (\\ref{node-agg}), (\\ref{MHA}), (\\ref{FFN}), and (\\ref{final-agg}).\n\n\\vspace{-0.2cm}\n\n"
                },
                "subsection 3.3": {
                    "name": "Training",
                    "content": "\n\\label{sec:training}\n\n\\vspace{-0.2cm}\nAs mentioned in Section \\ref{sec:problem}, we consider edge classification and link prediction as two tasks to train \\OursE and \\OursN, respectively.\n\n\\textbf{Edge Classification.}\nFor \\OursE (\\textit{i.e.}, edge representation learning), we adopt supervised training, the objective function of which is as follows.\n\\begin{gather}\n    \\mathcal{L}_e = -\\sum_{e_{ij}}\\bm{y}_{e_{ij}}^\\top \\text{log}\\hat{\\bm{y}}_{e_{ij}} + (1-\\bm{y}_{e_{ij}})^\\top \\text{log}(1-\\hat{\\bm{y}}_{e_{ij}}),\n\\end{gather}\n\\vspace{-0.2cm}\nwhere $\\hat{\\bm{y}}_{e_{ij}}=f({\\bmh}_{e_{ij}})$ is the predicted category distribution of $e_{ij}$ and $f(\\cdot)$ is a learnable classifier.\n\n\\vspace{1mm}\n\n\\textbf{Link Prediction.}\nFor \\OursN (\\textit{i.e.}, node representation learning), we conduct unsupervised training, where the objective function is as follows.\n\\begin{gather}\n    \\mathcal{L}_n = \\sum_{v\\in\\mathcal{V}}\\sum_{u\\in \\mathcal{N}_n(v)} -\\text{log}\\frac{\\text{exp}(\\bmh^\\top_v \\bmh_u)}{\\text{exp}(\\bmh^\\top_v \\bmh_u)+\\sum_{u'}\\text{exp}(\\bmh^\\top_v \\bmh_{u'})}.\n\\end{gather}\nHere, $\\mathcal{N}_n(v)$ is the set of $v$'s node neighbors and $u'$ denotes a random negative sample. In our implementation, we utilize ``in-batch negative samples'' \\citep{karpukhin2020dense} to reduce encoding and training costs.\n\n\\textbf{Overall Algorithm.}\nThe workflow of our edge representation learning (\\OursE) and node representation learning (\\OursN) algorithms can be found in Alg. \\ref{apx:alg-edge} and Alg. \\ref{apx:alg-node}, respectively.\n\n\\textbf{Complexity Analysis.}\nGiven a node involved in $N$ edges, and each edge has $P$ text tokens, the time complexity of \\textit{edge} encoding for each \\OursE layer is $\\mathcal{O}(P^2)$ (the same as one vanilla Transformer layer). The time complexity of \\textit{node} encoding for each \\OursN layer is $\\mathcal{O}(NP^2+N^2)$. For most nodes in the network, we can assume $N^2 \\ll NP^2$, so the complexity is roughly $\\mathcal{O}(NP^2)$ (the same as one PLM-GNN cascaded layer). For more discussions about time complexity, please refer to Section \\ref{efficiency}.\n\\vspace{-0.4cm}\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\vspace{-0.3cm}\nIn this section, we first introduce five datasets. Then, we demonstrate the effectiveness of \\Ours on both edge-level (\\textit{e.g.}, edge classification) and node-level (\\textit{e.g.}, link prediction) tasks. Finally, we conduct visualization and efficiency analysis to further understand \\Ours.\n\n\\vspace{-0.4cm}\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\n\\vspace{-0.3cm}\nWe run experiments on three real-world networks: Amazon \\citep{he2016ups}, Goodreads \\citep{wan2019fine}, and StackOverflow\\footnote{\\url{https://www.kaggle.com/datasets/stackoverflow/stackoverflow}}. Amazon is a user-item interaction network, where reviews are treated as text on edges; Goodreads is a reader-book network, where readers' comments are used as edge text information; StackOverflow is an expert-question network, and there will an edge when an expert posts an answer to a question. Since Amazon and Goodreads both have multiple domains, we select two domains for each of them. In total, there are five datasets used in evaluation (\\textit{i.e.}, Amazon-Movie, Amazon-Apps, Goodreads-Crime, Goodreads-Children, StackOverflow). Dataset statistics can be found in Appendix \\ref{app-dataset}.\n\\vspace{-0.3cm}\n"
                },
                "subsection 4.2": {
                    "name": "Task for Edge Representation Learning",
                    "content": "\n\\vspace{-0.3cm}\n\\textbf{Baselines.}\nWe compare our \\OursE model with a bag-of-words method (TF-IDF \\citep{robertson1994some}) and a pretrained language model (BERT \\citep{devlin2018bert}). Both baselines are further enhanced with network information by concatenating the node embedding $\\bmz_i$ with the bag-of-words vector (TF-IDF+nodes) or appending it to the input token sequence (BERT+nodes). \n\n\\textbf{Edge Classification.}\\label{edge-setting}\nThe model is asked to predict the category of each edge based on its associated text and local network structure. There are 5 categories for edges in Amazon (\\textit{i.e.}, 1-star, ..., 5-star) and 6 categories for edges in Goodreads (\\textit{i.e.}, 0-star, ..., 5-star). \n\n\n\nFor TF-IDF methods, the dimension of the bag-of-words vector is 2000.\nBERT-involved models and \\OursE have the same model size ($L=12,d=768$) and are initialized by the same checkpoint\\footnote{\\url{https://huggingface.co/bert-base-uncased}}. The dimension of initial node embeddings $d'$ is set to be 64. We use AdamW as the optimizer with $(\\epsilon, \\beta_1, \\beta_2)=(\\text{1e-8},0.9,0.999)$. The learning rate is 1e-5. The early stopping patience is 3 epochs. The batch size is 25.\nMacro-F1 and Micro-F1 are used as evaluation metrics. For BERT-involved models, parameters in BERT are trainable.\n\nTable \\ref{edge-classification} summarizes the performance comparison on the five datasets. \nFrom the table, we can observe that: (a) our \\OursE consistently outperforms all the baseline methods; (b) PLM-based methods (\\textit{i.e.}, BERT, BERT+nodes, and \\OursE) can have more promising results than bag-of-words methods (\\textit{i.e.}, TF-IDF and TF-IDF+nodes); (c) injecting node information can significantly improve performance if we compare TF-IDF with TF-IDF+nodes or compare BERT with BERT+nodes; (d) the performance of \\OursE is better than that of directly appending node embeddings to the input token sequence (\\textit{i.e.}, BERT+nodes), possibly because\nnetwork information is overwritten by text signals in BERT+nodes' deeper layers.\n\\vspace{-0.4cm}\n\n"
                },
                "subsection 4.3": {
                    "name": "Tasks for Node Representation Learning",
                    "content": "\n\\vspace{-0.2cm}\n\\paragraph{Baselines.}\nWe compare \\OursN with several \\textbf{vanilla GNN} models and \\textbf{PLM-integrated GNN} models. \\textbf{Vanilla GNN} models include \\textit{node-centric GNNs} such as MeanSAGE \\citep{hamilton2017inductive}, MaxSAGE \\citep{hamilton2017inductive} and GIN \\citep{xu2018powerful}, and \\textit{edge-aware GNNs} such as CensNet \\citep{jiang2019censnet} and NENN \\citep{yang2020nenn}. All \\textbf{vanilla} \\textit{edge-aware GNNs} models use bag-of-words as initial edge feature representations. \\textbf{PLM-integrated GNN} models utilize a PLM \\citep{devlin2018bert} to obtain text representations on edges and adopt a GNN to obtain node representations by aggregating edge representations. Baselines include BERT+MeanSAGE \\citep{hamilton2017inductive}, BERT+MaxSAGE \\citep{hamilton2017inductive}, BERT+GIN \\citep{xu2018powerful}, BERT+CensNet \\citep{jiang2019censnet}, BERT+NENN \\citep{yang2020nenn}, and GraphFormers \\citep{yang2021graphformers}. To verify the importance of both text and network information in text-rich networks, we also include matrix factorization (MF) \\citep{qiu2018network} and vanilla BERT \\citep{devlin2018bert} in the comparison.\n\n\n\n\\vspace{-0.4cm}\n\\paragraph{Link Prediction.}\nThe task is to predict whether there will be an edge between two target nodes, given their local network structures. Specifically, in the Amazon and Goodreads datasets, given the target user's reviews to other items/books and the target item/book's reviews from other users, we aim to predict whether there will be a 5-star link between the target user and the target item/book. In the StackOverflow dataset, we aim to predict whether the target expert can give an answer to the target question. We use MRR and NDCG as evaluation metrics. \n\nFor vanilla GNN models, we find that adopting MF node embeddings as initial node embeddings can help them obtain better performance \\citep{lv2021we}. For edge-aware GNNs, bag-of-words vectors are used as edge features, the size of which is set as 2000. For BERT-involved models, the training parameters are the same as \\ref{edge-setting}. During the testing stage, all methods are evaluated with samples in the batch for efficiency, \\textit{i.e.}, each query node is provided with one positive key node and 99 randomly sampled negative key nodes. More details can be found in Appendix \\ref{apx:rep}.\n\nTable \\ref{link-prediction2} shows the performance comparison. From the table, we can find that: (a) \\OursN outperforms all the baseline methods consistently; (b) BERT-based methods can have significantly better performance than bag-of-words GNN methods, which demonstrates the importance of contextualized text semantics encoding; (c) edge-aware methods can have better performance, but it depends on how the edge information contributes to node representation learning; (d) our \\OursN is the best since it takes edge text into consideration and deeply integrates text encoding and local network structure encoding.\n\n\\vspace{-0.2cm}\n\\paragraph{Ablation Study.}\nWe further conduct an ablation study to validate the effectiveness of all the three virtual tokens on node representation learning. The three virtual token hidden states are deleted respectively from the whole model and the results are shown in Table \\ref{lp-ablation}. From the table, we can find that \\OursN generally outperforms all the model variants on all the datasets, except for that without neighbor edge information virtual token in Amazon-Apps, which indicates the importance of all three virtual token hidden states.\n\n\n\n\n\\vspace{-0.3cm}\n\\paragraph{Node Classification with unsupervised node embedding.}\\label{sec::node-classification}\nTo further evaluate the quality of the unsupervised learned node embeddings, we fix the node embeddings obtained from link prediction and train a logistic regression classifier to predict nodes' categories. This is a multi-class multi-label classification task, where there are 2 classes for Amazon-Movie and 26 classes for Amazon-Apps. \nTable \\ref{node-classification} summarizes the performance comparison between several edge-aware methods. We can find that: (a) \\OursN can outperform all the baselines significantly and consistently, which indicates that \\OursN can learn more effective node representations; (b) edge-aware models can have better performance, but it depends on how the edge text information is employed.\n\n\n\n\\vspace{-0.3cm}\n"
                },
                "subsection 4.4": {
                    "name": "Embedding Visualization",
                    "content": "\n\\vspace{-0.3cm}\n\n\nTo reveal the relation between edge embeddings and node embeddings learned by our model, we apply t-SNE \\citep{van2008visualizing} to visualize them in Figure \\ref{visualization}. Node embeddings (\\textit{i.e.}, $\\{h_v|v\\in\\mathcal{V}\\}$) are denoted as stars, while edge embeddings (\\textit{i.e.}, $\\{h_e|e\\in\\mathcal{E}\\}$) are denoted as points with the same color as the node they link to. From the figure, we observe that: (1) node embeddings tend to be closer to each other in the embedding space compared with edge embeddings; (2) the embeddings of edges linked to the same node are in the vicinity of each other.\n\\vspace{-0.3cm}\n\n"
                },
                "subsection 4.5": {
                    "name": "Efficiency Analysis",
                    "content": "\\label{efficiency}\n\\vspace{-0.3cm}\n\n\n\nWe now compare the efficiency of BERT+GIN (a node-centric GNN), BERT+NENN (an edge-aware GNN), GraphFormers (a PLM-GNN nested architecture), and our \\OursN. All models are run on one NVIDIA A6000. The mini-batch size is 25; each sample contains one center node and $|\\mathcal{N}_e(v)|$ neighbor edges; the maximum text length is 64 tokens. The running time (per mini-batch) of compared models is reported in Table \\ref{tab::efficiency}, where we have the following findings: (a) the time cost of training \\OursN is quite close to that of BERT+GIN, BERT+NENN, and GraphFormers; (b) PLM-GNN nested architectures (i.e., GraphFormers and \\OursN) require slightly longer time during training than PLM-GNN cascaded architectures (i.e., BERT+GIN and BERT+NENN); (c) the time cost of \\OursN increases linearly with the neighbor size $|\\mathcal{N}_e(v)|$, which is consistent with our analysis in Section \\ref{sec:training} that the time complexity of \\OursN is $\\mathcal{O}(NP^2+N^2)\\sim\\mathcal{O}(NP^2)$ when $N\\ll P$.\n\\vspace{-0.4cm}\n"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n\\vspace{-0.3cm}\n",
                "subsection 5.1": {
                    "name": "Pretrained Language Models",
                    "content": "\n\\vspace{-0.3cm}\nPLMs are proposed to learn universal language representations from large-scale text corpora. Early studies such as word2vec \\citep{mikolov2013distributed}, fastText \\citep{bojanowski2017enriching}, and GloVe \\citep{pennington2014glove} aim to learn a set of context-independent word embeddings to capture word semantics. However, many NLP tasks are beyond word-level, so it is beneficial to derive word representations based on specific contexts. Contextualized language models are extensively studied recently to achieve this goal. For example, GPT \\citep{peters2018deep,radford2019language} adopts auto-regressive language modeling to predict a token given all previous tokens; BERT \\citep{devlin2018bert} and RoBERTa \\citep{liu2019roberta} are trained via masked language modeling to recover randomly masked tokens; XLNet \\citep{yang2019xlnet} proposes permutation language modeling; ELECTRA \\citep{clark2020electra} uses an auxiliary Transformer to replace some tokens and pretrains the main Transformer to detect the replaced tokens. For more related studies, one can refer to a recent survey \\citep{qiu2020pre}. \nTo jointly leverage text and graph information, previous studies \\citep{zhang2019shne,fang2020hierarchical,li2021adsgnn,zhu2021textgnn} propose a PLM-GNN cascaded architecture, where the text information of each node is first encoded via PLMs, then the node representations are aggregated via GNNs. \\citep{bi2022relphormer} proposes a triple2seq operation to linearize subgraphs and a ``mask prediction'' paradigm to conduct inference. Recently, GraphFormers \\citep{yang2021graphformers} introduces a GNN-nested Transformer to stack GNN layers and Transformer layers alternately. However, these works mainly consider \\textit{textual-node} networks, thus their focus is orthogonal to ours on \\textit{textual-edge} networks.\n\n\\vspace{-0.4cm}\n"
                },
                "subsection 5.2": {
                    "name": "Edge-Aware Graph Neural Networks",
                    "content": "\n\\vspace{-0.3cm}\nA vast majority of GNN models \\citep{kipf2017semi,hamilton2017inductive,velivckovic2017graph,xu2018powerful} leverage node attributes only and lack specific designs to utilize edge features. Heterogeneous GNNs \\citep{schlichtkrull2018modeling,yang2020heterogeneous} assume each edge has a pre-defined type and take such types into consideration during aggregation. However, they still cannot deal with more complicated features (\\textit{e.g.}, text) associated with the edges. EGNN \\citep{gong2019exploiting} introduces an attention mechanism to inject edge features into node representations; CensNet \\citep{jiang2019censnet} alternately updates node embeddings and edge embeddings in convolution layers; NENN \\citep{yang2020nenn} aggregates the representation of each node/edge from both its node and edge neighbors via a GAT-like attention mechanism. EHGNN \\citep{jo2021edge} proposes the dual hypergraph transformation and conducts graph convolutions for edges. Nevertheless, these models do not collaborate PLMs and GNNs to specifically deal with text features on edges, thus they underperform our \\Ours model, even stacked with a BERT encoder. \n\n\\vspace{-0.5cm}\n"
                }
            },
            "section 6": {
                "name": "Conclusions",
                "content": "\n\\vspace{-0.4cm}\nWe tackle the problem of representation learning on textual-edge networks. \nTo this end, we propose a novel graph-empowered Transformer framework, which integrates local network structure information into each Transformer layer text encoding for edge representation learning and aggregates edge representation fused by network and text signals for node representation.\nComprehensive experiments on five real-world datasets from different domains demonstrate the effectiveness of \\Ours on both edge-level and node-level tasks. \nInteresting future directions include (1) exploring other variants of introducing network signals into Transformer text encoding and (2) applying the framework to more network-related tasks such as recommendation and text-rich social network analysis.\n\n",
                "subsection 6.0": {
                    "subsubsection 6.0.1": {
                        "name": "Acknowledgments",
                        "content": "\nWe thank anonymous reviewers for their valuable and insightful feedback.\nResearch was supported in part by US DARPA KAIROS Program No. FA8750-19-2-1004 and INCAS Program No. HR001121C0165, National Science Foundation IIS-19-56151, IIS-17-41317, and IIS 17-04532, and the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA or the U.S. Government.\n\n\\bibliography{iclr2023_conference}\n\\bibliographystyle{iclr2023_conference}\n\n\\appendix\n% \\section{Appendix}\n% You may include other additional sections here.\n\\newpage\n\n\\appendix\n\n\n"
                    }
                }
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n",
                "subsection 7.1": {
                    "name": "Datasets",
                    "content": "\\label{app-dataset}\n\nThe statistics of the five datasets can be found in Table \\ref{app-dataset-table}.\n\n\n\n"
                },
                "subsection 7.2": {
                    "name": "Summary of \\Ours' Encoding Procedure",
                    "content": "\n\n\\RestyleAlgo{ruled}\n\\SetKwComment{Comment}{/* }{ */}\n\\begin{algorithm}\n\\small\n\\setstretch{0.6}\n\\SetKwInOut{Input}{Input}\\SetKwInOut{Output}{Output}\n\\caption{Edge Representation Learning Procedure of \\OursE}\\label{apx:alg-edge}\n\\Input{The edge $e_{ij}$, its associated text $d_{ij}$, and its involved nodes $v_i$ and $v_j$. \\\\ \nThe initial token embeddings ${\\bmH}^{(0)}_{e_{ij}}$ of the document $d_{ij}$.}\n\\Output{The embedding $\\bmh_{e_{ij}}$ of the edge $e_{ij}$.}\n\\Begin{\n    \\tcp{\\textcolor{myblue}{obtain layer-1 representation of text tokens}}\n    ${\\bmH}^{(0)'}_{e_{ij}} \\gets {\\rm Normalize}(\\bmH^{(0)}_{e_{ij}} + {\\rm MHA}^{(0)}(\\bmH^{(0)}_{e_{ij}}))$ \\;\n    $\\bmH^{(1)}_{e_{ij}} \\gets {\\rm Normalize}({\\bmH}^{(0)'}_{e_{ij}} + {\\rm FFN}^{(0)}({\\bmH}^{(0)'}_{e_{ij}}))$ \\;\n    \\tcp{\\textcolor{myblue}{obtain the base embedding of $v_i$ and $v_j$}}\n    $\\bmz^{(0)}_{v_i} \\gets {\\rm Embedding}(v_i)$ \\;\n    $\\bmz^{(0)}_{v_j} \\gets {\\rm Embedding}(v_j)$ \\;\n    \\For{$l=1,...,L$}{\n    \n        \\tcp{\\textcolor{myblue}{obtain layer-$l$ representation of virtual node tokens}}\n        ${\\bm z}^{(l)}_{v_i} \\gets \\bmW^{(l)}_n {\\bm z}^{(0)}_{v_i}$ \\;\n        ${\\bm z}^{(l)}_{v_j} \\gets \\bmW^{(l)}_n {\\bm z}^{(0)}_{v_j}$ \\;\n\n        \\tcp{\\textcolor{myblue}{obtain layer-$(l+1)$ representation of text tokens}}\n        $\\widetilde{\\bmH}^{(l)}_{e_{ij}} = \\bm{\\bm{z}}^{(l)}_{v_i} \\mathop{\\Vert} \\bm{z}^{(l)}_{v_j} \\mathop{\\Vert} \\bmH^{(l)}_{e_{ij}}$ \\;\n        $\\bmH^{(l)'}_{e_{ij}} \\gets {\\rm Normalize}(\\bmH^{(l)}_{e_{ij}} + {\\rm MHA}_{asy}^{(l)}(\\bmH^{(l)}_{e_{ij}}, \\widetilde{\\bmH}^{(l)}_{e_{ij}}))$ \\;\n        $\\bmH^{(l+1)}_{e_{ij}} \\gets {\\rm Normalize}(\\bmH^{(l)'}_{e_{ij}} + {\\rm FFN}^{(l)}(\\bmH^{(l)'}_{e_{ij}}))$  \\;\n        }\n    \\KwRet{$\\bmh_{e_{ij}} \\gets \\bmH^{(L+1)}_{e_{ij}}[{\\rm CLS}]$} \\;\n}\n\\end{algorithm}\n\n\\RestyleAlgo{ruled}\n\\SetKwComment{Comment}{/* }{ */}\n\\begin{algorithm}\n\\small\n\\setstretch{0.6}\n\\SetKwInOut{Input}{Input}\\SetKwInOut{Output}{Output}\n\\caption{Node Representation Learning Procedure of \\OursN.}\\label{apx:alg-node}\n\\Input{The center node $v_i$, its edge neighbors $\\mathcal{N}_e(v_i)$, and its node neighbors $\\mathcal{N}_n(v_i)$. \\\\ \nThe initial token embedding ${\\bmH}^{(0)}_{e_{ij}}$ of each document $d_{ij}$ associated with $e_{ij}\\in \\mathcal{N}_e(v_i)$.}\n\\Output{The embedding $\\bmh_{v_i}$ of the center node $v_i$.}\n\\Begin{\n    \\tcp{\\textcolor{myblue}{obtain layer-1 representation of text tokens}}\n    \\For{$e_{ij}\\in{N}_e(v_i)$}{\n        ${\\bmH}^{(0)'}_{e_{ij}} \\gets {\\rm Normalize}(\\bmH^{(0)}_{e_{ij}} + {\\rm MHA}^{(0)}(\\bmH^{(0)}_{e_{ij}}))$ \\;\n        $\\bmH^{(1)}_{e_{ij}} \\gets {\\rm Normalize}({\\bmH}^{(0)'}_{e_{ij}} + {\\rm FFN}^{(0)}({\\bmH}^{(0)'}_{e_{ij}}))$ \\;\n        }\n    \n    \\tcp{\\textcolor{myblue}{obtain the base embedding of each node}}\n    \\For{$u\\in{N}_n(v_i)\\cup\\{v_i\\}$}{\n        $\\bmz^{(0)}_u \\gets {\\rm Embedding}(u)$ \\;\n        }\n\n    \\For{$l=1,...,L$}{\n        \\tcp{\\textcolor{myblue}{obtain layer-$l$ representation of virtual node tokens}}\n        \\For{$u\\in{N}_n(v_i)\\cup\\{v_i\\}$}{\n        ${\\bm z}^{(l)}_u \\gets \\bmW^{(l)}_n {\\bm z}^{(0)}_u$ \\;\n        }\n\n        \\tcp{\\textcolor{myblue}{obtain layer-$l$ representation of virtual neighbor aggregation tokens}} \n        \\For{$e_{ij}\\in N_e(v_i)$}{\n        ${\\bm h}^{(l)}_{e_{ij}} \\gets \\bmH^{(l)}_{e_{ij}}[{\\rm CLS}]$ \\;\n        }\n        $\\left [\\bar{\\bmh}^{(l)}_{e_{ij}|v_i},...,\\bar{\\bmh}^{(l)}_{e_{is}|v_i} \\right] \\gets \\text{MHA}^{(l)}_g\\left(\\left [{\\bmh}^{(l)}_{e_{ij}},...,{\\bmh}^{(l)}_{e_{is}} \\right]\\right)$ \\;\n\n        \\tcp{\\textcolor{myblue}{obtain layer-$(l+1)$ representation of text tokens}}\n        \\For{$e_{ij}\\in N_e(v_i)$}{\n        $\\widetilde{\\bmH}^{(l)}_{e_{ij}} \\gets \\bm{\\bm{z}}^{(l)}_i \\mathop{\\Vert} \\bm{z}^{(l)}_j \\mathop{\\Vert} \\bar{\\bmh}^{(l)}_{e_{ij}|v_i} \\mathop{\\Vert} \\bmH^{(l)}_{e_{ij}}$ \\;\n        $\\bmH^{(l)'}_{e_{ij}} \\gets {\\rm Normalize}(\\bmH^{(l)}_{e_{ij}} + {\\rm MHA}_{asy}^{(l)}(\\bmH^{(l)}_{e_{ij}}, \\widetilde{\\bmH}^{(l)}_{e_{ij}}))$ \\;\n        $\\bmH^{(l+1)}_{e_{ij}} \\gets {\\rm Normalize}(\\bmH^{(l)'}_{e_{ij}} + {\\rm FFN}^{(l)}(\\bmH^{(l)'}_{e_{ij}}))$  \\;\n        }\n    }\n    \\tcp{\\textcolor{myblue}{obtain the edge representation}} \n    \\For{$e_{ij}\\in N_e(v_i)$}{\n    ${\\bm h}_{e_{ij}|v_i} \\gets \\bmH^{(L+1)}_{e_{ij}}[{\\rm v_j}]$ \\;\n    }\n    \\tcp{\\textcolor{myblue}{obtain the node representation}} \n    ${\\bmh}_{v_i} = \\text{AGG}(\\{{\\bmh}_{e_{ij}|v_i}|e_{ij}\\in N_e(v_i)\\})$ \\;\n    \n    \\KwRet{$\\bmh_{v_i}$}\n}\n\\end{algorithm}\n\n\n"
                },
                "subsection 7.3": {
                    "name": "Edge Classification",
                    "content": "\n\nWe also compare our method with the state-of-the-art edge representation learning  method EHGNN \\citep{jo2021edge} and two node-centric PLM-GNN methods BERT+MaxSAGE \\citep{hamilton2017inductive} and GraphFormers \\citep{yang2021graphformers}. The experimental results can be found in Table \\ref{edge-classification-appendix}. From the results, we can find that Edgeformer-E consistently outperforms all the baseline methods, including EHGNN, BERT+EHGNN, BERT+MaxSAGE and GraphFormers.\n\nEHGNN cannot obtain promising results because of two reasons: 1) edge-edge propagation: EHGNN proposes to transform edge to node and node to edge in the original network, followed by graph convolutions on the new hypernetwork. This results in edge-edge information propagation when conducting edge representation learning. However, edge-edge information propagation has the underlying edge-edge homophily assumption which is not always true in textual-edge networks. For example, when predicting the rate of a review text $e_ij$ given by $u$ to $i$, it is not straightforward to make the judgment based on reviews for $i$ written by other users (neighbor edges); 2) The integration of text and network signals are loose for BERT+EHGNN, since such architectures process text and graph signals one after the other, and fail to simultaneously model the deep interactions between both types of information. However, our Edgeformer-E is designed following the more reasonable node-edge homophily hypothesis and deeply integrating text \\& network signals by introducing virtual node tokens in Transformer encoding.\n\nNote that both PLM+GNN and Edgeformer-E require textual information on ALL nodes in the network. However, this assumption does not hold in many textual-edge networks. Therefore, we propose a way around to concatenate the text on the edges linked to the given node together to make up node text. However, such a strategy does not lead to competitive performance of PLM+MaxSAGE and GraphFormers according to our experimental results. Therefore, to make our model generalizable to the case of missing node text, the proposed Edgeformers can be a better solution.\n\n\n\n\n\n"
                },
                "subsection 7.4": {
                    "name": "Link Prediction",
                    "content": "\nWe further report the link prediction performance of compared models on the validation set in Table \\ref{link-prediction1}.\n\n\n\n"
                },
                "subsection 7.5": {
                    "name": "Node Classification",
                    "content": "\nThe 26 classes of Amazon-Apps nodes are: ``Books \\& Comics'', ``Communication'', ``Cooking'', ``Education'', ``Entertainment'', ``Finance'', ``Games'', ``Health \\& Fitness'', ``Kids'', ``Lifestyle'', ``Music'', ``Navigation'', ``News \\& Magazines'', ``Novelty'', ``Photography'', ``Podcasts'', ``Productivity'', ``Reference'', ``Ringtones'', ``Shopping'', ``Social Networking'', ``Sports'', ``Themes'', ``Travel'', ``Utilities'', and ``Weather''.\n\nThe 2 classes of Amazon-Movie nodes are: ``Movies'' and ``TV''.\n\n\n"
                },
                "subsection 7.6": {
                    "name": "Hyper-parameter Study",
                    "content": "\n% hyperparameter figure\n\n\n\n\n\\vspace{-0.2cm}\n\\paragraph{Node Dimension.} We conduct experiments on Amazon-Apps, Goodreads-Children, and StackOverflow to understand the effect of the initial node embedding dimension in Eq.(\\ref{node-initial}). We test the performance of \\OursN on the link prediction task with the initial node embedding dimension varying in 4, 8, 16, 32, and 64. The results are shown in Figure \\ref{fig::embed}, where we can find that the performance of \\OursN generally increases as the initial node embedding dimension increases. This finding is straightforward since the more parameters an initial node embedding has before overfitting, the more information it can represent.\n\n\\vspace{-0.3cm}\n\\paragraph{Sampled Neighbor Size.}\nWe further analyze the impact of sampled neighbor size for node representation learning on Amazon-Apps, Goodreads-Children, and StackOverflow, with a fraction of edges randomly sampled for the center node. The result can be found in Figure \\ref{neighbor-size}. We can find that the performance increases progressively as sampled neighbor size $|N_e(v)|$ increases. It is intuitive since the more neighbors we have, the more information can contribute to center node learning. Meantime, the increase rate decreases as $|N_e(v)|$ increases linearly because the information between neighbors can have information overlap.\n\\vspace{-0.2cm}\n\n\n"
                },
                "subsection 7.7": {
                    "name": "Self-attention Map Study",
                    "content": "\n\nIn order to study how the virtual node token will benifit the encoding of Edgeformer-E, we plot the self-attention probability map for a random sample in Figure \\ref{attention-map}. We random pick up a token from this sample and plot the self-attention probability of how different tokens (x-axis), including virtual node tokens and the first twenty original text tokens, will contribute to the encoding of this random token in different layers (y-axis). From the figure, we can find that: In higher layers (e.g., Layers 10-11), the attention weights of virtual node tokens are significantly larger than those of original node tokens. Since virtual node token hidden states are of $R^{d\\times 2}$ and the original text token hidden states are of $R^{d\\times l}$ ($l$ is text sequence length), the ratio of network tokens to text tokens is $2:l$ in $\\widetilde{\\bmH}^{(l)}_{e_{ij}}$ (Eq.\\ref{edge-prop}), where $l$ is the text sequence length. However, the self-attention mechanism can automatically learn to balance the two types of information by assigning higher weights to the corresponding virtual node tokens, so a larger number of tokens representing textual information will not cause network information to be overwhelmed.\n\n\n\n\n\n\n"
                },
                "subsection 7.8": {
                    "name": "Reproducibility Settings",
                    "content": "\\label{apx:rep}\n\nFor a fair comparison, the training objectives of \\OursN and all PLM-involved baselines are the same. The hyper-parameter configuration for obtaining the results in Tables \\ref{edge-classification} and \\ref{link-prediction2} can be found in Table \\ref{apx:hyper}, where ``sampled neighbor size'' stands for the number of neighbors sampled for each type of the center node during node representation learning. This hyper-parameter is determined according to the average node degree of the corresponding node type. The edge classification and link prediction experiments are conducted on one NVIDIA V100 and one NVIDIA A6000, respectively.\n\nIn Section \\ref{sec::node-classification}, we adopt logistic regression as our classifier. We employ the Adam optimizer \\citep{kingma2014adam} with the early-stopping patience as 10 to train our classifier. The learning rate is set as 0.001.\n\n\n\n\n"
                },
                "subsection 7.9": {
                    "name": "Limitations",
                    "content": "\nIn this work, we mainly focus on modeling homogeneous textual-edge networks and solving fundamental tasks in graph learning such as node/edge classification and link prediction. Interesting future studies include designing models to characterize network heterogeneity and applying our proposed model to real-world applications such as recommendation.\n\n\n"
                },
                "subsection 7.10": {
                    "name": "Ethical Considerations",
                    "content": "\nWhile it has been demonstrated that PLMs are powerful in language understanding \\citep{devlin2018bert,liu2019roberta,clark2020electra}, there are studies pointing out their drawbacks such as containing social bias \\citep{liang2021towards} and misinformation \\citep{abid2021persistent}. In our work, we focus on enriching PLMs' text encoding process with the associated network structure information, which could be a way to mitigate the bias and wipe out the contained misinformation.\n\n% \\end{spacing}\n\n"
                }
            }
        },
        "tables": {
            "edge-classification": "\\begin{table}\n  \\caption{Edge classification performance on Amazon-Movie, Amazon-App, Goodreads-Crime, and Goodreads-Children.}\n  \\label{edge-classification}\n  \\centering\n  \\scalebox{0.8}{\n  \\begin{tabular}{lcccccccc}\n    \\toprule\n    \\multicolumn{1}{c}{} & \\multicolumn{2}{c}{Amazon-Movie} & \\multicolumn{2}{c}{Amazon-Apps} & \\multicolumn{2}{c}{Goodreads-Crime} & \\multicolumn{2}{c}{Goodreads-Children}                \\\\\n    \\cmidrule(r){2-3} \\cmidrule(r){4-5} \\cmidrule(r){6-7} \\cmidrule(r){8-9}\n    Model     & Macro-F1    &  Micro-F1   & Macro-F1    &  Micro-F1   & Macro-F1    &  Micro-F1   & Macro-F1    &  Micro-F1  \\\\\n    \\midrule\n    TF-IDF &  50.01  & 64.22  &  48.30  & 62.88  & 43.07  & 51.72 &  39.42  & 49.90  \\\\\n    TF-IDF+nodes   &  53.59  & 66.34  &  50.56 & 65.08  &  49.35  & 57.50 & 47.32  & 56.78    \\\\\n    \\midrule\n    BERT     &  61.38  & 71.36  &  59.11  & 69.27  &  56.41  & 61.29  &  51.57  & 57.72  \\\\\n    BERT+nodes     &  \\textit{63.00}  & \\textit{72.45}  &  \\textit{59.72}  & \\textit{70.82} &  \\textit{58.64}  & \\textit{65.02}  &  \\textit{54.42}  & \\textit{60.46}  \\\\\n    \\midrule\n    \\OursE   &  \\textbf{64.18}  & \\textbf{73.59}  &  \\textbf{60.67}  & \\textbf{71.28}  &  \\textbf{61.03}  & \\textbf{65.86}  &  \\textbf{57.45}  & \\textbf{61.71}  \\\\\n    \\bottomrule\n  \\end{tabular}}\n\\vspace{-1.5em}\n\\end{table}",
            "link-prediction2": "\\begin{table}[t]\n  \\caption{Link prediction performance (on the testing set) on Amazon-Movie, Amazon-Apps, Goodreads-Crime, Goodreads-Children, and StackOverflow. $\\Delta$ denotes the relative improvement of our model comparing with the best baseline.}\n  \\label{link-prediction2}\n  \\centering\n  \\scalebox{0.75}{\n  \\begin{tabular}{lcccccccccc}\n    \\toprule\n    \\multicolumn{1}{c}{} & \\multicolumn{2}{c}{Amazon-Movie} & \\multicolumn{2}{c}{Amazon-Apps} & \\multicolumn{2}{c}{Goodreads-Crime} & \\multicolumn{2}{c}{Goodreads-Children} & \\multicolumn{2}{c}{StackOverflow} \\\\\n    \\cmidrule(r){2-3} \\cmidrule(r){4-5} \\cmidrule(r){6-7} \\cmidrule(r){8-9} \\cmidrule(r){10-11}\n    Model     & MRR    & NDCG  & MRR    & NDCG  & MRR  & NDCG   & MRR  & NDCG  & MRR  & NDCG    \\\\\n    \\midrule\n    MF &  0.2032  & 0.3546  &  0.1482  & 0.3052  &  0.1923  & 0.3443  &  0.1137  & 0.2716 &  0.1040  & 0.2642  \\\\\n    MeanSAGE   &  0.2138  & 0.3657  &  0.1766  & 0.3343  &  0.1832  & 0.3368  &  0.1066  & 0.2647 &  0.1174  & 0.2768     \\\\\n    MaxSAGE     &  0.2178  & 0.3694 &  0.1674  & 0.3258  &  0.1846  & 0.3387  &  0.1066  & 0.2647 &  0.1173  & 0.2769  \\\\\n    GIN     &  0.2140  & 0.3648 &  0.1797  & 0.3362  &  0.1846  & 0.3374  &  0.1128  &  0.2700 &  0.1189  & 0.2778   \\\\\n    CensNet   &  0.2048  & 0.3568  &  0.1894  & 0.3457  &  0.1880  & 0.3398  &  0.1157  & 0.2726  &  0.1235  & 0.2806   \\\\\n    NENN     &  0.2565  & 0.4032  &  0.1996  & 0.3552  &  0.2173  & 0.3670  &  0.1297  & 0.2854 &  0.1257  & 0.2854    \\\\\n    \\midrule\n    BERT     &  0.2391  & 0.3864  &  0.1790  & 0.3350  &  0.1986  & 0.3498  &  0.1274  & 0.2836 &  0.1666  & 0.3252   \\\\\n    BERT+MaxSAGE     &  0.2780  & 0.4224  &  0.2055  & 0.3602  &  0.2193  & 0.3694  &  0.1312  & 0.2872  &  0.1681  & 0.3264   \\\\\n    BERT+MeanSAGE     &  0.2491  & 0.3972  &  0.1983  & 0.3540  &  0.1952  & 0.3477  &  0.1223  & 0.2791 &  0.1678  & 0.3264   \\\\\n    BERT+GIN     &  0.2573  & 0.4037  &  0.2000  & 0.3552  &  0.2007  & 0.3522  &  0.1238  & 0.2801 &  \\textit{0.1708}  & \\textit{0.3279}   \\\\\n    GraphFormers     &  0.2756  & 0.4198  &  0.2066  & 0.3607  &  0.2176  & 0.3684  &  0.1323  & 0.2887 &  0.1693  & 0.3278   \\\\\n    BERT+CensNet     &  0.1919  & 0.3462  &  0.1544  & 0.3132  &  0.1437  & 0.3000  &  0.0847  & 0.2436 &  0.1173  & 0.2789   \\\\\n    BERT+NENN     &  \\textit{0.2821}  & \\textit{0.4256}  &  \\textit{0.2127}  & \\textit{0.3666}  &  \\textit{0.2262}  & \\textit{0.3756}  &  \\textit{0.1365}  & \\textit{0.2925} &  0.1619  & 0.3215   \\\\\n    \\midrule\n    \\OursN     &  \\textbf{0.2919}  & \\textbf{0.4344}  &  \\textbf{0.2239}  & \\textbf{0.3771}  &  \\textbf{0.2395}  & \\textbf{0.3875}  &  \\textbf{0.1446}  & \\textbf{0.3000} &  \\textbf{0.1754}  & \\textbf{0.3339}  \\\\\n    \\hdashline\n    + $\\Delta$ \\%  &  \\textbf{3.5\\%}  & \\textbf{2.1\\%}  &  \\textbf{5.3\\%}  & \\textbf{2.9\\%}  &  \\textbf{5.9\\%}  & \\textbf{3.2\\%}  &  \\textbf{5.9\\%}  & \\textbf{2.6\\%} &  \\textbf{2.7\\%}  & \\textbf{1.8\\%} \\\\\n    \\bottomrule\n  \\end{tabular}}\n  \\vspace{-0.6cm}\n\\end{table}",
            "lp-ablation": "\\begin{table}[t]\n  \\caption{Ablation study of link prediction performance (on the testing set) on Amazon-Movie, Amazon-Apps, Goodreads-Crime, Goodreads-Children, and StackOverflow. (-) means removing the corresponding virtual tokens.}\n  \\label{lp-ablation}\n  \\centering\n  \\scalebox{0.75}{\n  \\begin{tabular}{lcccccccccc}\n    \\toprule\n    \\multicolumn{1}{c}{} & \\multicolumn{2}{c}{Amazon-Movie} & \\multicolumn{2}{c}{Amazon-Apps} & \\multicolumn{2}{c}{Goodreads-Crime} & \\multicolumn{2}{c}{Goodreads-Children} & \\multicolumn{2}{c}{StackOverflow} \\\\\n    \\cmidrule(r){2-3} \\cmidrule(r){4-5} \\cmidrule(r){6-7} \\cmidrule(r){8-9} \\cmidrule(r){10-11}\n    Model     & MRR    & NDCG  & MRR    & NDCG  & MRR  & NDCG   & MRR  & NDCG  & MRR  & NDCG    \\\\\n    \\midrule\n    \\OursN     &  \\textbf{0.2919}  & \\textbf{0.4344}  &  0.2239  & 0.3771  &  \\textbf{0.2395}  & \\textbf{0.3875}  &  \\textbf{0.1446}  & \\textbf{0.3000} &  \\textbf{0.1754}  & \\textbf{0.3339}  \\\\\n    \\midrule\n    - center node token     &  0.2899  & 0.4325  &  0.2178  & 0.3717  &  0.2361  & 0.3847  &  0.1407  & 0.2964 &  0.1702  & 0.3291   \\\\\n    - neighbor node token     &  0.2880  & 0.4306  &  0.2115  & 0.3656  &  0.2322  & 0.3807  &  0.1411  & 0.2963  &  0.1730  & 0.3310   \\\\\n    - neighbor text token     &  0.2895  & 0.4321  &  \\textbf{0.2260}  & \\textbf{0.3789}  &  0.2386  & 0.3867  &  0.1442  & 0.2998 &  0.1734  & 0.3314   \\\\\n    \\bottomrule\n  \\end{tabular}}\n\\end{table}",
            "node-classification": "\\begin{table}[t]\n\\vspace{-0.6cm}\n  \\caption{Node classification performance on Amazon-Movie and Amazon-App.}\n  \\label{node-classification}\n  \\centering\n  \\scalebox{0.75}{\n  \\begin{tabular}{lcccccc}\n    \\toprule\n    \\multicolumn{1}{c}{} & \\multicolumn{3}{c}{Amazon-Movie} & \\multicolumn{3}{c}{Amazon-Apps}  \\\\\n    \\cmidrule(r){2-4} \\cmidrule(r){5-7} \n    Model     & Macro-F1    &  Micro-F1   & PREC  & Macro-F1   &  Micro-F1   &  PREC   \\\\\n    \\midrule\n    MF &  0.75660.0017  & 0.82340.0013  &  0.82410.0013  & 0.46470.0151  &  0.83930.0012  & 0.84620.0006   \\\\\n    CensNet   &  0.85280.0010  & 0.88390.0008  &  0.88450.0007  & 0.27820.0168  &  0.82790.0006  & 0.83310.0005   \\\\\n    NENN     &  0.91860.0008  & 0.93410.0008  &  0.93470.0007  & 0.34080.0082  &  0.87890.0019  & 0.88190.0017  \\\\\n    \\midrule\n    BERT     &  0.92090.0005  & 0.93610.0003  &  0.93670.0003  & 0.76080.0175  &  0.92830.0015  & 0.93370.0015   \\\\\n    BERT+CensNet     &  0.90320.0006  & 0.92210.0004  &  0.92270.0004  & 0.57500.0277  &  0.86920.0034  & 0.87310.0028  \\\\\n    BERT+NENN     &  0.92470.0005  & 0.93870.0004  &  0.93930.0005  & 0.75560.0092  &  0.93060.0008  & 0.93820.0006  \\\\\n    \\midrule\n    \\OursN     &  \\textbf{0.92760.0007}  & \\textbf{0.94110.0006}  &  \\textbf{0.94170.0005}  & \\textbf{0.77580.0100}  &  \\textbf{0.93390.0007}  & \\textbf{0.94310.0005}   \\\\\n    \\bottomrule\n  \\end{tabular}}\n\\vspace{-0.5cm}\n\\end{table}",
            "tab::efficiency": "\\begin{table}[t]\n  \\caption{Time cost (\\textit{ms}) per mini-batch for BERT+GIN, BERT+NENN, GraphFormers, and \\OursN, with neighbor size $|\\mathcal{N}_e(v)|$ increasing from 2 to 5 on Amazon-Apps, Goodreads-Children, and StackOverflow. \\OursN achieves similar efficiency with the baselines.}\n  \\label{tab::efficiency}\n  \\centering\n  \\scalebox{0.75}{\n  \\begin{tabular}{lcccccccccccc}\n    \\toprule\n    \\multicolumn{1}{c}{} & \\multicolumn{4}{c}{Amazon-Apps} & \\multicolumn{4}{c}{Goodreads-Children}  & \\multicolumn{4}{c}{StackOverflow} \\\\\n    \\cmidrule(r){2-5} \\cmidrule(r){6-9} \\cmidrule(r){10-13} \n    Model      &  2   & 3  & 4   &  5     &  2   & 3  & 4   &  5   &  2   & 3  & 4   &  5 \\\\\n    \\midrule\n    BERT+GIN  &  15.53 &   21.44  &  25.69  &  31.19   &  15.44 &   21.38  &  25.88  &  31.02  & 15.29  &   21.41 & 25.58  &  31.05\\\\\n    BERT+NENN   &   15.70 & 21.71  &  26.03  &  31.50   &  15.78 &   21.86  &  26.15  &  31.46   & 15.74  & 21.97   & 26.09  &  31.46   \\\\\n    GraphFormers   &  17.21  &   23.56   &  28.29  &  34.43   & 17.08  & 23.76 &  28.43  & 34.38    &  17.13 & 23.65  &  28.60  &  34.45  \\\\\n    \\OursN   &  18.68  &   25.39   &  30.45  &  36.57   &  18.74 &   25.17   &  30.42  &  36.42   & 18.57  &  25.32  & 30.46  &  36.32  \\\\\n    \\bottomrule\n  \\end{tabular}}\n  \\vspace{-0.6cm}\n\\end{table}",
            "app-dataset-table": "\\begin{table}[ht]\n  \\caption{Dataset Statistics}\\label{app-dataset-table}\n  \\vspace{1mm}\n  \\centering\n  \\scalebox{1}{\n  \\begin{tabular}{lcc}\n    \\toprule\n    Dataset     & \\# Node     & \\# Edge \\\\\n    \\midrule\n    Amazon-Movie &  173,986 &  1,697,533  \\\\\n    Amazon-Apps & 100,468 & 752,937 \\\\\n    Goodreads-Crime  & 385,203 & 1,849,236 \\\\\n    Goodreads-Children  & 192,036  &  734,640 \\\\\n    StackOverflow  & 129,322 &  281,657 \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n\\end{table}",
            "edge-classification-appendix": "\\begin{table}[ht]\n  \\caption{Edge classification performance on Amazon-Movie, Amazon-App, Goodreads-Crime, and Goodreads-Children.}\n  \\label{edge-classification-appendix}\n  \\centering\n  \\scalebox{0.8}{\n  \\begin{tabular}{lcccccccc}\n    \\toprule\n    \\multicolumn{1}{c}{} & \\multicolumn{2}{c}{Amazon-Movie} & \\multicolumn{2}{c}{Amazon-Apps} & \\multicolumn{2}{c}{Goodreads-Crime} & \\multicolumn{2}{c}{Goodreads-Children}                \\\\\n    \\cmidrule(r){2-3} \\cmidrule(r){4-5} \\cmidrule(r){6-7} \\cmidrule(r){8-9}\n    Model     & Macro-F1    &  Micro-F1   & Macro-F1    &  Micro-F1   & Macro-F1    &  Micro-F1   & Macro-F1    &  Micro-F1  \\\\\n    \\midrule\n    TF-IDF &  50.01  & 64.22  &  48.30  & 62.88  & 43.07  & 51.72 &  39.42  & 49.90  \\\\\n    TF-IDF+nodes   &  53.59  & 66.34  &  50.56 & 65.08  &  49.35  & 57.50 & 47.32  & 56.78    \\\\\n    EHGNN   &  49.90  & 64.04  &  48.20 & 63.63  &  44.49  & 52.30 & 40.01  & 50.23    \\\\\n    \\midrule\n    BERT     &  61.38  & 71.36  &  59.11  & 69.27  &  56.41  & 61.29  &  51.57  & 57.72  \\\\\n    BERT+nodes     &  \\textit{63.00}  & \\textit{72.45}  &  \\textit{59.72}  & \\textit{70.82} &  \\textit{58.64}  & \\textit{65.02}  &  \\textit{54.42}  & \\textit{60.46}  \\\\\n    BERT+EHGNN   &  61.45  & 70.73  &  58.86 & 70.79  &  56.92 & 61.66 & 52.46  & 57.97    \\\\\n    BERT+MaxSAGE   &  61.57  & 70.79  &  58.95 & 70.45  &  57.20  & 61.98 & 52.75  & 58.53    \\\\\n    GraphFormers   &  61.73  & 71.52  &  59.67 & 70.19  &  57.49  & 62.37 & 52.93  & 58.34    \\\\\n    \\midrule\n    \\OursE   &  \\textbf{64.18}  & \\textbf{73.59}  &  \\textbf{60.67}  & \\textbf{71.28}  &  \\textbf{61.03}  & \\textbf{65.86}  &  \\textbf{57.45}  & \\textbf{61.71}  \\\\\n    \\bottomrule\n  \\end{tabular}}\n\\vspace{-1.5em}\n\\end{table}",
            "link-prediction1": "\\begin{table}[ht]\n  \\caption{Link prediction performance (on the validation set) on Amazon-Movie, Amazon-Apps, Goodreads-Crime, Goodreads-Children, and StackOverflow. $\\Delta$ denotes the relative improvement of our model comparing with the best baseline.}\n  \\label{link-prediction1}\n  \\centering\n  \\scalebox{0.75}{\n  \\begin{tabular}{lcccccccccc}\n    \\toprule\n    \\multicolumn{1}{c}{} & \\multicolumn{2}{c}{Amazon-Movie} & \\multicolumn{2}{c}{Amazon-Apps} & \\multicolumn{2}{c}{Goodreads-Crime} & \\multicolumn{2}{c}{Goodreads-Children} & \\multicolumn{2}{c}{StackOverflow} \\\\\n    \\cmidrule(r){2-3} \\cmidrule(r){4-5} \\cmidrule(r){6-7} \\cmidrule(r){8-9} \\cmidrule(r){10-11}\n    Model     & MRR    & NDCG  & MRR    & NDCG  & MRR  & NDCG   & MRR  & NDCG  & MRR  & NDCG    \\\\\n    \\midrule\n    MF &  0.2178  & 0.3666  &  0.1523  & 0.3086  &  0.2492  & 0.3966  &  0.1470  & 0.3042 &  0.1104  & 0.2702  \\\\\n    MeanSAGE   &  0.2280  & 0.3775  &  0.1804  & 0.3375  &  0.2286  & 0.3792  &  0.1348  & 0.2927 &  0.1258  & 0.2846     \\\\\n    MaxSAGE     &  0.2321  & 0.3812  &  0.1708  & 0.3288  &  0.2299  & 0.3812  &  0.1339  & 0.2919 &  0.1257  & 0.2848  \\\\\n    GIN     &  0.2287  & 0.3769  &  0.1846  & 0.3402  &  0.2306  & 0.3802  &  0.1420  & 0.2989  &  0.1275  & 0.2860   \\\\\n    CensNet   &  0.2186  & 0.3682  &  0.1953  & 0.3504  &  0.2399  & 0.3875  &  0.1501  & 0.3059  &  0.1338  & 0.2900   \\\\\n    NENN     &  0.2776  & 0.4204  &  0.2068  & 0.3610  &  0.2777  & 0.4224  &  0.1658  & 0.3207 &  0.1361  & 0.2948    \\\\\n    \\midrule\n    BERT    &  0.2582  & 0.4017  &  0.1863  & 0.3407  &  0.2540  & 0.4001  &  0.1608  & 0.3156 &  0.1798  & 0.3371   \\\\\n    BERT+MaxSAGE     &  0.3028  & 0.4424  &  0.2128  & 0.3661  &  0.2859  & 0.4299  &  0.1687  & 0.3236  &  0.1828  & 0.3399   \\\\\n    BERT+MeanSAGE     &  0.2705  & 0.4145  &  0.2024  & 0.3572  &  0.2527  & 0.4004  &  0.1572  & 0.3129 &  0.1849  & \\textit{0.3418}   \\\\\n    BERT+GIN     &  0.2790  & 0.4212  &  0.2040  & 0.3583  &  0.2613  & 0.4073  &  0.1611  & 0.3158 &  \\textit{0.1858}  & 0.3413   \\\\\n    GraphFormers     &  0.2998  & 0.4393 &  0.2111  & 0.3642  &  0.2852  & 0.4294  &  0.1671  & 0.3220 &  0.1833  & 0.3405   \\\\\n    BERT+CensNet     &  0.2025  & 0.3552  &  0.1577  & 0.3163  &  0.1822  & 0.3361  &  0.1007  & 0.2603 &  0.1232  & 0.2845   \\\\\n    BERT+NENN     &  \\textit{0.3087}  & \\textit{0.4470}  &  \\textit{0.2193}  & \\textit{0.3719}  &  \\textit{0.2956}  & \\textit{0.4382}  &  \\textit{0.1737}  & \\textit{0.3280} &  0.1759  & 0.3341   \\\\\n    \\midrule\n    \\OursN     &  \\textbf{0.3206}  & \\textbf{0.4574}  &  \\textbf{0.2320}  & \\textbf{0.3838}  &  \\textbf{0.3106}  & \\textbf{0.4514}  &  \\textbf{0.1849}  & \\textbf{0.3385} &  \\textbf{0.1944}  & \\textbf{0.3508}  \\\\\n    \\hdashline\n    + $\\Delta$ \\%  &  \\textbf{3.9\\%}  & \\textbf{2.3\\%}  &  \\textbf{5.8\\%}  & \\textbf{3.2\\%}  &  \\textbf{5.1\\%}  & \\textbf{3.0\\%}  &  \\textbf{6.4\\%}  & \\textbf{3.2\\%} &  \\textbf{4.6\\%}  & \\textbf{2.6\\%} \\\\\n    \\bottomrule\n  \\end{tabular}}\n\\end{table}",
            "apx:hyper": "\\begin{table}[ht]\n\\caption{Hyper-parameter configuration.}\\label{apx:hyper}\n\\centering\n\\scalebox{0.78}{\n\\begin{tabular}{cccccc}\n\\toprule\nParameter & Amazon-Movie & Amazon-Apps & Goodreads-Crime & Goodreads-Children & StackOverflow \\\\\n\\midrule\noptimizer & \\multicolumn{5}{c}{Adam} \\\\\n% \\midrule\nlearning rate & \\multicolumn{5}{c}{1e-5} \\\\\n% \\midrule\nweight decay & \\multicolumn{5}{c}{1e-3} \\\\\n% \\midrule\nAdam $\\epsilon$ & \\multicolumn{5}{c}{1e-8} \\\\\n% \\midrule\nearly-stopping patience & \\multicolumn{5}{c}{3} \\\\\n\\midrule\ntraining batch size & \\multicolumn{5}{c}{30} \\\\\n% \\midrule\ntesting batch size & \\multicolumn{5}{c}{100} \\\\\n\\midrule\nnode embedding dim & \\multicolumn{5}{c}{64} \\\\\n% \\midrule\nchunk $k$ & \\multicolumn{5}{c}{12} \\\\\n% \\midrule\nmax sequence length & \\multicolumn{5}{c}{64} \\\\\n% \\midrule\nbackbone PLM & \\multicolumn{5}{c}{BERT-base-uncased} \\\\\n\\midrule\n\\multirow{2}{*}{sampled neighbor size} & user:8 & user:3 & reader:8 & reader:6 & expert:2 \\\\\n                                & item:10 & item:5 & book:10 & book:4 & question:5  \\\\\n\\bottomrule            \n\\end{tabular}\n}\n\\end{table}"
        },
        "figures": {
            "overview": "\\begin{figure}\n\\centering\n\\includegraphics[scale=0.38]{figure/whole-model3.eps}\n\\vspace{-0.5cm}\n\\caption{Model Framework Overview. (a) An illustration of \\OursE for edge representation learning, where virtual node token hidden states are concatenated to the edge text original token hidden states to inject network signal into edge text encoding. (b) An illustration of \\OursN for node representation learning, where \\OursE is enhanced by local network structure virtual token hidden state and edge representations are aggregated to obtain node representation.}\\label{overview}\n\\vspace{-0.5cm}\n\\end{figure}",
            "visualization": "\\begin{figure}[t]\n\\centering\n\\subfigure[Apps]{\n\\includegraphics[width=5.4cm]{figure/Apps_node_edge.eps}}\n\\hspace{0.0in}\n\\subfigure[Children]{               \n\\includegraphics[width=5.4cm]{figure/Children_node_edge.eps}}\n\\vspace{-0.4cm}\n\\caption{Embedding visualization. Node embeddings are denoted as stars, and the embeddings of edges are denoted as points with the same color if they are linked to the same node.}\\label{visualization}\n\\vspace{-0.6cm}\n\\end{figure}",
            "fig::embed": "\\begin{figure}[ht]\n\\centering\n\\subfigure[Amazon-Apps]{               \n\\includegraphics[width=4.4cm]{figure/apps-embed.eps}}\n\\hspace{0in}\n\\subfigure[Goodreads-Children]{\n\\includegraphics[width=4.4cm]{figure/children-embed.eps}}\n\\hspace{0in}\n\\subfigure[StackOverflow]{\n\\includegraphics[width=4.4cm]{figure/stackoverflow-embed.eps}}\n\\vspace{-0.3cm}\n\\caption{Effect of the dimension of initial node embeddings.}\\label{fig::embed}\n\\vspace{-0.3cm}\n\\end{figure}",
            "neighbor-size": "\\begin{figure}[ht]\n\\centering\n\\subfigure[Amazon-Apps]{               \n\\includegraphics[width=4.4cm]{figure/apps-neighbor.eps}}\n\\hspace{0in}\n\\subfigure[Goodreads-Children]{\n\\includegraphics[width=4.4cm]{figure/children-neighbor.eps}}\n\\hspace{0in}\n\\subfigure[StackOverflow]{\n\\includegraphics[width=4.4cm]{figure/stackoverflow-neighbor.eps}}\n\\vspace{-0.3cm}\n\\caption{Effect of the sampled neighbor size (\\textit{i.e.}, $|N_e(v)|$).}\\label{neighbor-size} %$z_v, \\phi(v)\\in\\mathcal{A}_p$\n\\vspace{-0.3cm}\n\\end{figure}",
            "attention-map": "\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.5]{figure/attention_map.eps}\n\\vspace{-3.5cm}\n\\caption{Self-attention probability map of Edgeformer-E for a random sample. The x-axis corresponds to different key/value tokens and the y-axis corresponds to different Edgeformer-E layers. In higher layers (e.g., layers 10-11), the attention weights of virtual node tokens are significantly larger than those of original node tokens. The ratio of network tokens to text tokens is $2:l$ in $\\widetilde{\\bmH}^{(l)}_{e_{ij}}$ (Eq.4), where $l$ is the text sequence length. However, the self-attention mechanism can automatically learn to balance the two types of information by assigning higher weights to the corresponding virtual node tokens, so a larger number of tokens representing textual information will not cause network information to be overwhelmed.}\\label{attention-map}\n\\end{figure}"
        },
        "git_link": "https://github.com/PeterGriffinJin/Edgeformers"
    }
}