{
    "meta_info": {
        "title": "An Embedded Feature Selection Framework for Control",
        "abstract": "Reducing sensor requirements while keeping optimal control performance is\ncrucial to many industrial control applications to achieve robust, low-cost,\nand computation-efficient controllers. However, existing feature selection\nsolutions for the typical machine learning domain can hardly be applied in the\ndomain of control with changing dynamics. In this paper, a novel framework,\nnamely the Dual-world embedded Attentive Feature Selection (D-AFS), can\nefficiently select the most relevant sensors for the system under dynamic\ncontrol. Rather than the one world used in most Deep Reinforcement Learning\n(DRL) algorithms, D-AFS has both the real world and its virtual peer with\ntwisted features. By analyzing the DRL's response in two worlds, D-AFS can\nquantitatively identify respective features' importance towards control. A\nwell-known active flow control problem, cylinder drag reduction, is used for\nevaluation. Results show that D-AFS successfully finds an optimized five-probes\nlayout with 18.7\\% drag reduction than the state-of-the-art solution with 151\nprobes and 49.2\\% reduction than five-probes layout by human experts. We also\napply this solution to four OpenAI classical control cases. In all cases, D-AFS\nachieves the same or better sensor configurations than originally provided\nsolutions. Results highlight, we argued, a new way to achieve efficient and\noptimal sensor designs for experimental or industrial systems. Our source codes\nare made publicly available at https://github.com/G-AILab/DAFSFluid.",
        "author": "Jiawen Wei, Fangyuan Wang, Wanxin Zeng, Wenwei Lin, Ning Gui",
        "link": "http://arxiv.org/abs/2206.11064v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "9 pages, 9 figures, accepted by SIGKDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nFor experimental and industrial systems, reducing sensor requirements while maintaining optimal control performance is crucial for human experts to understand target systems and potentially transpose these techniques to industrial cases. It is one of human beings' most critical intellectual activities, i.e., extracting knowledge from unknown environments to recognize and exploit the world\\cite{sutton2018reinforcement}. To better understand the target system, scientists/engineers use intuition and domain knowledge to identify critical information based on continuous observations of system performance\\cite{khurana2018feature}. \nBut for a complex system with massive sensory inputs, this handcrafted solution through trial-and-error is normally time-consuming, costly, and heavily reliant on human expertise.\nThus, it is vital to develop an effective and systematic solution to identify relevant sensors without or with minimal human intervention.\n\nNowadays, DRL has shown great success in complex system control problems\\cite{lillicrap2016continuous} by combining reinforcement learning and deep learning\\cite{mnih2015human}. The introduction of Deep Neural Networks (DNN) can extract important features from a huge set of raw inputs. Despite its success, the opaque nature of DNN hides the critical information about which sensors are most relevant for control\\cite{samek2017explainable,du2018towards}. Existing solutions for sensor selection under DRL have been limited to the recursive sensor set selection and evaluation with certain heuristic algorithms\\cite{paris2021robust}. \nAs their complexity increases rapidly with the number of candidate sensors, these methods are inapplicable for systems with massive possible inputs.\n\n\n\nThis requirement is similar to the Feature Selection (FS) problem in Machine Learning(ML) \\cite{li2017feature}, also known as variable selection or attribute selection. This process aims to select a subset of relevant features with respect to the modeling labels. \n% A subset of relevant features is selected for model construction according to the performance of a learning algorithm or model, and labels are demanded to guide the feature selection process.\nHowever, due to the distinctiveness of control process, choosing a suitable variable as a label is very troublesome. \nFor example, Fig.~\\ref{fig:intro} shows a classic flow control problem to reduce the drag and lift of a cylinder by adjusting flow injection/suction with two jets(represented by two red dots) and 151 probes. \nFirstly, the immediate reward corresponding to the time series state set is often meaningless as a label. The instance injection/suction might not change the immediate reward significantly. Secondly, the goal for control typically uses the discounted cumulative return as a loss function that often has long and uncertain latency and has little relevance to the single-step state. \nThus, neither immediate reward nor cumulative return is appropriate as the mandatory training label in most feature selection methods. In later experiments, we also experimentally prove that existing feature selection methods are not suitable in systems with control. \n\n\n% Actually, this requirement is similar to the feature selection problem in machine learning(ML)\\cite{li2017feature}, also known as variable selection or attribute selection. \n% A subset of relevant features is selected for model construction according to the performance of a learning algorithm or model, and labels are demanded to guide the feature selection process.\n% However, due to the distinctiveness of the control, choosing a suitable variable as a label is very troublesome. Neither immediate reward nor cumulative return is appropriate. Fig.~\\ref{fig:intro} shows a classic flow control problem to reduce the drag and lift of a cylinder by adjusting flow injection/suction with two jets(represented by the two red dots) and 151 probes. \n% Firstly, the immediate reward corresponding to the time series state set is often meaningless as a label. The instance injection/suction might not have significant changes of immediate reward. Secondly, the goal for control normally uses the discounted cumulative return as a loss function that often has long and uncertain latency and has little relevance to the single-step state.\n\n% In control, DRL has shown great success in complex system control problems\\cite{lillicrap2016continuous} by combining Reinforcement Learning (RL) and deep learning\\cite{mnih2015human}. The introduction of Deep Neural Networks (DNN) can abstract importance features from a huge set of raw inputs. However, although its success, the opaque nature of DNN hides the critical information about which sensory data are most related to the target system and control goals. Thus, existing limited solutions for sensor selection with DRL have been limited to the recursive sensor set selection and evaluation with certain heuristic algorithms\\cite{paris2021robust} whose complexity increases exponentially with the number of candidate sensors and not applicable for the system with 151 probes. As DRL already contains all relevant sensor inputs, can we make the hidden knowledge on the relevance of respective sensors explicit and provide qualitative evaluation during DRL trainning process? \n\n% It allows agents to make decisions from unconstructed features and interact with unknown systems to explore and exploit. \n% However, many existing works have pointed out that a good representation of the target systems with most related features helps DRL achieve a more effective and robust controller than straightforward representations\\cite{suarez2019feature}. \n% Thus, many recent DRL, e.g.,\\cite{agnewunsupervised,vinyals2017starcraft,topin2019generation} resort back to handcraft constructed features to improve DRL's performance. \n% It is argued, the opaque nature of DNN hides the critical information about which sensory data are most related to the target system and control goals. \n% Some researchers try to understand the operation of DNN by visualizing the internal structure of DRL's perception\\cite{iyer2018transparency} and actions\\cite{topin2019generation}. \n% However, those works are still limited to the operation of DNN themselves rather than understanding the environments. \n% Can we make the hidden knowledge on the relevance of respective inputs and their representation explicit and provide qualitative evaluation?  That information is often the key factor in understanding the underlying physical models.\n\n% However, the distinctiveness of the DRL system makes the feature selection methods used in the machine learning (ML) domain can not be transferred directly. For a control system, it is difficult to generate a feature-label pair dataset. Although the control goal are normally well represented by the so called immediate reward and accumulated rewards. Neither of them is not suitable to be label data. The immediate reward can be significantly influenced by the action, and the accumulated reward is time delayed and highly influenced by the whole system control trajectory. \n\nFor effective control, a DRL controller has already learned the relevance of different inputs during its exploration and exploitation. Can we make the hidden knowledge on the relevance of respective sensors explicit and provide qualitative evaluation during DRL training? To address this challenge, we propose a new solution, the Dual-world based Attentive embedded Feature Selection(D-AFS), a general mechanism that enables the DRL controller to interact with the real world and its virtual peer with twisted features. \nOur method combines general DRL framework and feature selection module, which can identify the contribution of inputs simultaneously with strategy learning process. In summary, our contributions are:\n\n\n%  Currently, those tasks are limited mainly to human expertise and depend on hand-crafted experiments. \n\\begin{itemize}\n    \\item \\textbf{A new direction for embedded feature selection in control}: We combine the exploration and exploitation capabilities of DRL with an attention-based feature evaluation module, that qualitatively evaluates the impact of different features by translating feature weight calculation to feature selection probability problem. The method can identify the feature importance of unknown systems for control during DRL's strategy learning process.\n    \\item \\textbf{A dual-world architecture design}: We propose a dual-world architecture that includes real world and virtual world, of which the virtual world is mapped from its real peer by the feature evaluation module. Dual-world enables D-AFS complete feature importance ranking without jeopardizing DRL's strategy exploration. \n    % A dual-world architecture with the traditional real world and the virtual world twisted from its real peers is proposed to complete the feature selection task without jeopardizing the DRL control task. An attention-based feature evaluation mechanism is proposed to qualitatively evaluate the impact of different features by translating the feature weight calculation to the feature selection probability problem.\n    \\item \\textbf{Excellent results on active flow control}: We apply D-AFS with the Proximal Policy Optimization (PPO) algorithm\\cite{schulman2017proximal} in the case mentioned above. D-AFS/PPO successfully reduces a five-sensor layout from 151 possible probes, with a drag coefficient of 2.938, very close to the theoretical limit of 2.93 without vortex shedding, and achieves 18.7\\% drag reduction than the state-of-the-art solutions using 151 probes.\n\\end{itemize}\n\nWe also successfully adapt Deep Q-Network(DQN)\\cite{mnih2015human} and Deep Deterministic Policy Gradient (DDPG)\\cite{lillicrap2016continuous} into D-AFS, apply them on four OpenAI Gym\\cite{brockman2016openai} classical control cases. In all cases, D-AFS selects the same or better sensor subsets than the original expert-based solutions (Gym provided). Results clearly show that our solution can effectively identify system key sensors, improve the interpretability of the target system and reduce the reliance on human expertise. \n\n% The detailed simulation results and source codes are attached in the supplementary materials.  \n\n% Experiment results show that our proposed method achieves . Compared to the five-sensor layout by human experts, D-AFS achieves 252.9\\% increase in the recirculation area and 37.5\\% drag reduction. \n\n% The rest of this paper is organized as follows: Section II discusses related work. Then the proposed dual system and the major modules in D-AFS are introduced in Section III. Section IV integrates an attention-based module into two existing DRL algorithms to form the concrete algorithms of D-AFS. Then experiments are performed, and the results are analyzed in Section V. The final section concludes our work and points out the possible directions for future work.\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\nThis section discusses the related work from two aspects: 1) The feature selection methods in machine learning; 2) The abbreviated research for sensor selection in active flow control.\n\n",
                "subsection 2.1": {
                    "name": "Feature Selection in ML",
                    "content": "\nFeature selection is a technology that contains automatically discovering, evaluating, and selecting relevant features\\cite{bengio2013representation}. \nGenerally, it can be divided into three categories\\cite{li2017feature}:\n1) Filter methods separate feature selection from the learning process and only rely on the measures of the general characteristics of the training data to evaluate the feature weights. Different feature selection algorithms exploit various types of criteria to define the relevance of features, e.g., Fisher score\\cite{gu2011generalized} and ReliefF\\cite{robnik2003theoretical}. 2) Wrapper methods\\cite{tang2014feature}  predict accuracy of a predefined learning algorithm to evaluate the quality of recursive selected features. They normally suffer high computation costs and low scalability. 3) Embedded methods depend on the interactions with the learning algorithm and evaluate feature sets according to the interactions. The most recent approaches are in this class e.g., XGBoost\\shortcite{chen2016xgboost}, LightGBM\\shortcite{ke2017lightgbm} and Random Forest(RF)\\shortcite{breiman2001random}. Recently, DNN-based solutions are proposed,e.g., the attention-based, AFS\\shortcite{gui2019afs} or casualty-based \\cite{Yu2021Unified}. However, trajectories generated by the DRL controller do not follow the independent and identically distributed assumption, making it impossible to directly transfer and use the feature selection methods in the ML domain.\n% \\textcolor{red}{the uniqueness of the DRL mechanism makes it impossible to directly transfer and use the feature selection methods in the ML domain.} \\textcolor{blue}{There's independent and identically distributed hypothesis in machine learning.}\n\\cite{hu2019design} creates virtual sensors to do fault detection by using existing sensors and applying domain knowledge to the data. However, this approach does not actually reduce the number of sensors and cannot be applied to control tasks.\nAs pointed out in the introduction, for model-free systems, it is challenging to provide appropriate labels for each system state, as demanded in most ML feature selection approaches. Thus, it is difficult for existing ML feature selection methods to be applied in the DRL control environment.\n\n% \\noindent\\textbf{Policy Gradient Method}\n% \\vspace{-0.2in}\n% Policy gradient methods update the policy $\\pi$ to the direction of the expected total reward gradient $g = \\nabla_{\\theta}\\mathbb{E}_{\\pi}[R_{\\tau}]$, \n% where $\\tau$ is a trajectory generated by sampling actions according to the policy $\\pi$\\cite{sutton2018reinforcement}.\n% It is also reasonable to learn a value function estimation in addition to the policy $\\pi$. And that is exactly what the Actor-Critic does. \n% PPO\\cite{schulman2017proximal} is popular Actor-Critic methods among them. It proposes a novel method that forms a lower bound estimate of the policy performance and use a clipped surrogate objective to constrain the size of policy update.\n% This guarantee that the policy will not update too large to stuck in the local optimal while still proceeding towards the direction of gradient descent.\n"
                },
                "subsection 2.2": {
                    "name": "Optimal Sensor Placement in Flow Control",
                    "content": "\n% \\vspace{-0.1in}\nThe issue of optimal sensor placement has been investigated with certain domain-specific knowledge. Most existing solutions,e.g., \\cite{bright2013compressive,cohen2006heuristic} are based on compressed sensing theory that selects a limited number of sensors that can effectively perform flow reconstruction.  These approaches do not depend on the learning algorithm and can be seen as a special filter feature selection algorithm. This stream of methods also has very high computation complexity, as we need to reconstruct the whole flow. Furthermore, as stressed by Stephan \\& Simon \\shortcite{oehler2018sensor}, control does not systematically require faithful flow reconstruction. The partial knowledge of relevant `hidden' variables may be sufficient. \n\nDespite its success in many domains, only in the recent work of Rabault et al. \\shortcite{rabault2019artificial},  DRL has been used to control the flow past a 2D cylinder. They use 151 probes to learn control strategy by PPO in the 2D Kármán Vortex Street simulation. Tang et al. \\shortcite{tang2020robust} further improve their work through extending the Reynolds numbers range and sensor observations. These researches have not yet involved the optimization of sensor selection. Paris et al. \\shortcite{paris2021robust} recently proposes a sensor selection algorithm based on a well-trained PPO controller. However, this approach adopts the so-called wrapper method by evaluating different combinations of features. Their work has very high computation complexity, and their discussion is limited to a system with 15 probes.\n\nTo the best of our knowledge, there have been no systematic solutions yet to obtain interpretable environmental representations through the learning and exploration processes of DRL.\n\n\n"
                }
            },
            "section 3": {
                "name": "Definitions and Notations",
                "content": "\nThis section gives a general definition of the problem studied in this paper and briefly demonstrates the related notations. \n\n\\noindent\\textbf{Raw Features.} Given a controlled object, the basic observations are a set of states periodically collected by sensors deployed in the system. We use $\\bm{\\mathcal{S}}_r =\\{s^k|k=1,2,...,m\\}$ to describe the set of raw states, where $k$ represents the number of sensors. \n\n\\noindent\\textbf{Feature Selection for Control.}\nThe goal of feature selection for control under the DRL framework is stated as follows. Given a set of features $\\bm{\\mathcal{S}}_r$ from the real world, find a subset of features, $\\bm{\\mathcal{S}}^* \\subseteq \\bm{\\mathcal{S}}_r$, to maximize the expected cumulative return $\\mathcal{R}_t$ for a given DRL algorithm.\nAs shown in Eq. (\\ref{eq:optimal_s}):\n\\begin{equation}\n% \\vspace{-0.1in}\n\\label{eq:optimal_s}\n    \\bm{\\mathcal{S}}^* = \\arg\\max_{\\bm{\\mathcal{S}}_r} \\mathcal{R}_t = \\arg\\max_{\\bm{\\mathcal{S}}_r} \\sum_{t}^{\\infty}\\gamma^{t}r_{t}\n\\end{equation}\nwhere $\\gamma$ is a discount rate.\n\n\\noindent\\textbf{Discussion.} For a target system, if a well-designed controller is capable of achieving effective control, the raw features must contain all the necessary information for control.\n% it is logical to assume that raw features contain all the necessary information for control if a well-designed controller can achieve effective control.\nHowever, it has been wildly agreed that reducing sensor requirements while keeping optimal control performance is crucial to many industrial control applications to achieve robust, low cost, and computation efficient controllers\\cite{khurana2018feature}. \nPrevious works on the industrial control with DRL\\cite{paris2021robust} demonstrate that when the selected feature representation matches the form required by the physical model, the DRL controller can usually even achieve a better control effect. \nThis characteristic is of vital importance in understanding the underlying physical model of the target systems.\n\n\n\n\n\n"
            },
            "section 4": {
                "name": "Proposed Method",
                "content": "\n\nIn this section, the overall structure of D-AFS and the mechanism of embedded feature selection are illustrated and analyzed.\n\n% ThroughWe provide experiential studies on the impacts of feature representations towards the control results. We observe that when engineered features match the variate used in the physical models, we can effectively improve the performance of DRL controllers. This characteristic is of vital importance in understanding the underlying model of the target systems. \n\n\n% many DRL solutions often ignore this transformation process and rely on the feature extraction ability of DNN, which is one of the major reason of its success in many fields. However, due to the limited sensor data, it has been observed that many carefully engineered features from raw inputs can effectively improve the performance of DRL controllers, especially  when engineered features match the variate used in the physical models. \n\n\n\n% It means the size of  all possible translated features, $\\mathcal{T}(\\rm{S})$, is normally much bigger than required $F^p\\ and\\ F^r$. In order to achieve good control effects, our empirical study found that if F* covers the set of  $F^r\\bigcup F^p$. Normally, a DRL controller can achieve good control effects.  Thus, ${\\bm{O}^\\mathcal{P} \\bigcup\\ \\bm{O}^\\mathcal{R}}\\subseteq\\ \\bm{O}^*$, Please note, the size of ${|F}^p|\\ and F^r$  is much smaller than the whole possible set of features $|F|$.Then, we can achieve certain interpretable information about the system’s basic requirements. \n% \\subsection{Dual-world based Feature Selection Architecture}\n\n% To perform feature selection operations in the control environment, we need to select the critical features from the whole sets of possible features based on the exploration \\& exploitation process of DRL.  However, the feature selection process results in a ``virtual'' world(the system represented by partial features) rather than the ``real'' world in which full features represent the system. \n% The system information represented by the above two is inconsistent, and the selected features constantly change during the strategy update process.\n% By assessing the selected features in the strategy update process of DRL, it is possible to identify valuable features.\n\n",
                "subsection 4.1": {
                    "name": "Design Principles",
                    "content": "\n\nThe basic design principle for two different worlds is rather straightforward. Firstly, if a feature is irrelevant or weakly related to the system, it has little effect on a ``good'' control strategy. Conversely, a feature strongly related to the physical model of the system or the control goal should have a pronounced impact on the control strategy.  Generally, the one that is more benign for the controller might have more impacts. \n\nHowever, the dual-world design brought a series of problems to the existing DRL solutions with only one world. Three main issues need to be tackled: 1) How to effectively combine the real and virtual world without affecting the normal operation of DRL? 2) How to dynamically adjust the combination of features in the virtual world space without jeopardizing the DRL learning process? 3) How to modify existing DRL algorithms to allows online feature evaluation?\n\nThus, we introduce the dual-world architecture, which contains three main modules, the environment module, the evaluation module, and the DRL module, as shown in Fig. \\ref{fig:architecture}. The environment and the DRL module are inherited directly from the original DRL framework, but detailed designs are slightly different to support the two worlds. The evaluation module calculates the importance of each feature through the attention mechanism and then helps generate the virtual world.\nThe DRL module, evaluation module, and the unique dual-world system are loosely coupled in the architecture we designed. \nWe minimize the modification of the general structure of traditional DRL algorithms and maintain its integrity to the greatest extent, which significantly promotes the reuse of the existing state-of-the-art DRL algorithms. Therefore, the DRL module can be determined according to the specific problem. \n\n\n% \\noindent\\textbf{Dual-world, DRL and Evaluation Module }\n\n% To address these three questions, we first need to clarify the different roles between the real world and the virtual world of the dual-world space. On the one hand, we need the real world to provide system information to explore the correct control strategy. On the other hand, the virtual world searches for the optimal features and representations by continuously tuning the mapping from the real world to the virtual world. \n\n% The DRL module, evaluation module, and the unique dual-world system are loosely coupled in the architecture we designed. \n% We minimize the modification of the general structure of the traditional DRL algorithm and maintain its integrity to the greatest extent, which significantly promotes the reuse of the existing state-of-art DRL algorithm. Therefore, DRL modules can be determined according to the specific problem. \n\n% \\textcolor{red}{\n% The dual-world environment is proposed to tackle the first issue and is illustrated in Sec.~\\ref{sec:dual-world}. \n% The evaluation and virtual world generation are performed in the Evaluation module (described in Sec.~\\ref{sec:AE}) to address the second issue. \n% The final issue is alleviated by design proposed in Sec.~\\ref{sec:ppo}. \n% }\n\n% \\vspace{-0.2cm}\n"
                },
                "subsection 4.2": {
                    "name": "Dual-world",
                    "content": "\n\\label{sec:dual-world}\nWe first need to clarify the different roles between the real world and the virtual world of the dual-world space. On the one hand, we need the real world to provide system information to explore the correct control strategy. On the other hand, the virtual world searches for the optimal features and representations by continuously tuning the mapping from the real world to the virtual world. \nThe real world features are the system's original features $\\bm{\\mathcal{S}}_r$, which is used to interact with the policy of the DRL module.\nAnd the virtual world is the real world mapping through the evaluation module, which can be used to evaluate the validity of the selected features. The mapping function is generated with the attention-based evaluation module illustrated in the following section.  In D-AFS, the environment is the combination of the real and virtual world $\\bm{\\mathcal{S}}_r \\cup \\bm{\\mathcal{S}}_v $. \n\n\n"
                },
                "subsection 4.3": {
                    "name": "Attention-based Evaluation Module",
                    "content": "\n\\label{sec:AE}\n \n\n% In our paper, an Attention-based Evaluation (short as AE) module is designed to evaluate the importance of different features by assigning different weights (represented by $\\bm{P}$). The principal architecture of this module is shown in Fig. \\ref{fig:feature_evaluation}.\n\n% The AE module specifically assigns Attention Nets(short as AN) to each feature. It uses a revised attention mechanism to generate weights, which reflect the importance of a particular feature for the subsequent DRL module. These weights are calculated according to the selection pattern of respective features. The higher the weight of the feature might, the higher importance towards the agent.\n\n% \\begin{figure}[t]\n%     \\centering\n%     \\includegraphics[width=0.48\\textwidth]{pic/feature_evaluation.eps}\n%     \\caption{The method of calculating the importance of each features. An attention network is assigned to each feature to get the possibility of each feature being selected or not selected.}\n%     % \\vspace{-0.1in}\n%     \\label{fig:feature_evaluation}\n% \\end{figure}\n\n% Before AN, there is a dense network $\\bm{E}$ with $N_E$ neuron units. This dense network is used to abstract the internal relationship among input $\\bm{\\mathcal{S}}_r$. The nonlinear function $\\tanh(\\cdot)$ operation keeps both positive and negative values.\n% Eq. (\\ref{eq:Enet}) shows the function of this layer, where $\\bm{W}_E\\in\\mathbb{R}^{m\\times N_E}$ and $\\bm{b}_E\\in\\mathbb{R}^{N_E}$ are the parameters to be learned. The units of the dense neural network $N_E$ could be adjusted according to respective tasks. \n% \\begin{equation}\n%     \\label{eq:Enet}\n%     \\bm{E}=\\tanh{({\\bm{\\mathcal{S}}_r}^{\\rm T} \\bm{W}_E+\\bm{b}_E)}\n% \\end{equation}\n\n% After extraction of $\\bm{E}$, each attention net is used for one feature to determine whether this feature should be selected. It will generate two values: $\\bm{X}$, $\\bm{Y}$ represents selected/unselected values for one feature respectively, calculated as follow: \n% \\begin{equation}\n%     \\label{eq:select}\n%     \\bm{X}=\\bm{E}\\bm{W}_x+\\bm{b}_x\n% \\end{equation}\n% \\begin{equation}\n%     \\label{eq:unselect}\n%     \\bm{Y}=\\bm{E}\\bm{W}_y+\\bm{b}_y\n% \\end{equation}\n%  with Eq. (\\ref{eq:select}) and Eq. (\\ref{eq:unselect}), where $\\bm{W_x}$,$\\bm{W_y}$,$\\bm{b_x}$ and $\\bm{b_y}$ are parameters to be learned. \n\nThe attention-based evaluation (AE for short) module aims to evaluate the importance of different features by assigning different weights (denoted as $\\bm{P}$). The structure of AE is shown in the green area in Fig. \\ref{fig:architecture}.\n\nThe AE module is a revised attention mechanism extended from our previous work\\cite{gui2019afs}. This module consists of two key components, a dense network $\\bm{E}$ with $N_E$ neuronal units and several Attention Networks (AN for short) for calculating the importance weights of each input.\n% $\\bm{E}=\\tanh{({\\bm{\\mathcal{S}}}_r}^{\\rm T}\\bm{W}_E+\\bm{b}_E)}$\n\n\nThe dense network \\(\\bm{E} = \\tanh({\\bm{\\mathcal{S}}_r}^{\\rm T} \\bm{W}_E + \\bm{b}_E)\\), \\(\\bm{W}_E\\in\\mathbb {R}^{m\\times N_E}\\), \\(\\bm{b}_E\\in\\mathbb{R}^{N_E}\\), is used to abstract the internal relationship between the inputs $\\bm{\\mathcal{S}}_r$. $\\bm{W}_E$ and $\\bm{b}_E$ are parameters, nonlinear function $\\tanh(\\cdot)$ keeps positive and negative values. To some extend, the dense network can reduce the redundancy of all sensors, of which the units $N_E$ can be adjusted according to respective inputs.\n\nOnce we have extracted the relationship of features by $\\bm{E}$, we configure an attention network for each feature to generate weight separately. We do not adopt the typical soft attention mechanism to generate a weighted arithmetic mean of all features. This operation might result in small weights for most features and large weights for a tiny number of features and suffer the loss of details on the whole feature sets.\nThese weights can determine whether one feature should be selected or not. More detailed, it generates two values, $\\bm{X}=\\bm{E}\\bm{W}_x+\\bm{b}_x$ and $\\bm{Y}=\\bm{E}\\bm{ W}_y+\\bm{b}_y$, to represent the selected/unselected probability of a feature, respectively. \nIn general, the higher the weight of the feature being selected ($\\bm{X}$), the more important the feature is for control. \nThen we use the softmax function to magnify the distance between selected and unselected.\n\\begin{equation}\n    \\label{eq:softmax}\n    p^k=\\frac{\\exp(x^k)}{\\exp{\\left(x^k\\right)}+\\exp(y^k)},x^k\\in\\bm{X},y^k\\in\\bm{Y}\n\\end{equation}\nFinally, these attention nets will generate a matrix of attention weights $\\bm{P} =\\left \\{p^k|k=1,2,...,m\\right\\}$, which has the same dimension as features. \nWith Eq. \\ref{eq:att}, we generate a feature representation of the virtual world with $\\bm{\\mathcal{S}}_r$ multiplied by attention weights.\n\\begin{equation}\n    \\bm{\\mathcal{S}}_v=\\bm{\\mathcal{S}}_r \\odot \\bm{P}\n\\label{eq:att}\n\\end{equation}\nThe AE module generates a dynamic mapping function that maps representations from the real world $\\bm{\\mathcal{S}}_r$ to the virtual world $\\bm{\\mathcal{S}}_v$, which will be used for DRL to update the strategy. Additionally, with the attention weights matrix $\\bm{P}$ generated by AE, we can evaluate respective features when DRL has explored an efficient strategy.\n% {\\color{red}\n% The virtual world $\\bm{\\mathcal{S}}_v$ will be used for the DRL model to update strategy and the AE module actually generate a dynamic mapping function that maps the real world representation to the virtual world. How to update the parameters of the AE module will be explained later. When the system reaches a good control state, the importance of respective features can be analyzed according to $\\bm{P}$ generated by AE.\n% }\n\n% \\vspace{-0.2cm}\n"
                },
                "subsection 4.4": {
                    "name": "PPO in Dual-world",
                    "content": "\n\\label{sec:ppo}\nAs far as we know, value functions are used in one way or another among the current model-free DRL algorithms\\cite{SpinningUp2018}.\nIn our algorithm, in addition to the role of the original DRL algorithm, the value function also needs to guide the AE module to select valuable features during the training process. This subsection shows how to integrate the D-AFS with the popular Proximal Policy Optimization(PPO) algorithm, denoted as D-AFS/PPO. PPO is a popular policy-based online algorithm with clipped probability ratios that forms a lower bound of the performance of the policy.\nIn order to allow the agent to interact with the environment correctly, we use the real world as the input of the PPO actor network, which is the same as the original PPO algorithm.\nThus, the actor network with parameters $\\theta$ can be updated by:\n\n\\begin{equation}\n    \\begin{aligned}\n        \\theta^* = \\arg \\max_{\\theta} L^{CLIP}(\\theta)\n    \\end{aligned}\n    % L_t(\\theta)=min(r_t(\\theta_k) A^{\\pi_{\\theta_k}}(s_t, a_t), clip(r_t(\\theta)), 1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_k}}(s_t, a_t)\n    % \\theta_{k+1} = \\arg \\max_{\\theta} \\frac{1}{|\\mathcal{D}_k|T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T min(r_t(\\theta))A^{\\pi_{\\theta_k}}(s_r,a)\n    \\label{eq:actor}\n\\end{equation}\nwhere,\n\\begin{equation*}\n    L^{CLIP}(\\theta)=\\mathbb{E}_t[\\min(r_t(\\theta), {\\rm clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon))A^{\\pi_{\\theta_t}}(s_r,a)],\n\\end{equation*}\n$r_t(\\theta)$ is the probability ratio of the old and the new policy, the subscript $t$ is the number of iteration.\n% and $r_t(\\theta)=\\frac{\\pi_{\\theta}(a_t | s_{r}_{t})}{\\pi_{\\theta}_{old}(a_t | s_{r}_{t})}$.\n\nThe difference is that in our algorithm, we use the virtual world as the input of the critic network, allowing the critic network to evaluate the goodness of the selected features of the evaluation module while scoring the performance of the actor network. That is:\n\\begin{equation}\n    \\phi^*, \\psi^*=\\arg \\min_{\\phi,\\psi} \\mathbb{E}_t (V_{\\phi,\\psi}(s_v)-\\mathcal{R}_t)^2\n    \\label{eq:critic}\n\\end{equation}\n$\\phi$ and $\\psi$ are parameters of critic network and attention module, respectively. $V_{\\phi, \\psi}(s_v)$ is value function and $\\mathcal{R}_t$ is accumulative return.\n\nThe detailed structure of this D-AFS/PPO is shown in Algorithm \\ref{algorithm}. The bold pseudo-codes are the revised parts to support the dual-world DRL learning. \n\\vspace{-0.1in}\n\\begin{algorithm}\n    \\caption{D-AFS/PPO, Actor-Critic based}\n    \\label{algorithm}\n    \\begin{algorithmic}\n        \\STATE Initial parameters of actor network $\\theta_0$, critic network $\\phi_0$ and attention network $\\psi_0$.\n        \\FOR{$t=1,2,...$}\n        \\STATE Run policy $\\pi(\\theta_{t})$ to get trajectories $\\mathcal{D}_t(s_r, a, r, s^\\prime_r)$\n        \\STATE Compute accumulative return $\\mathcal{R}_t$\n        \\STATE \\textbf{Compute attention weights $\\bm{p}$ based on Eq. (\\ref{eq:softmax})}\n        \\STATE Compute $\\bm{s_v \\gets s_r \\odot p}$\n        \\STATE Compute advantage estimates $A_t$ based on current function $\\bm{V_{\\phi_t, \\psi_t}(s_v)}$\n        \\STATE Update the actor network by maximizing the PPO-Clip based on Eq. (\\ref{eq:actor}) with Adam $\\theta_{old} \\gets \\theta$\n        \\STATE Update the critic network by regression on MSE based on Eq. (\\ref{eq:critic}) with Adam $\\{ \\phi_{old} \\gets \\phi, \\bm{\\psi_{old} \\gets \\psi} \\}$\n        \\ENDFOR\n    \\end{algorithmic}\n\\end{algorithm}\n\n% Since the virtual world as the input of the critic network in our architecture, the parameter of attention networks $\\theta^A$ is updated while optimizing the critic network by minimizing the loss function as follows:\n\n% $r_t$ is the reward returned by the current step, and $\\gamma$ is a discounting factor, $y_i$ is the output of the target network of the critic.\n\n% It can be known from the Fig. \\ref{fig:ddpg} that the input of all policy functions in the actor network is the real world environment, so when the network parameters of the actor network are updated, the parameters of attention nets ${\\theta}^A$ remain unchanged. Besides the parameters updating mentioned above, the target network parameters' updating methods are the same as those in the conventional DDPG, using the soft update method.\n\n\n% It is worth noting that our modifications are mainly about the computation of the value function, which uses the virtual world as input. These modifications are designed as non-intrusive as possible so that it can be easily extended to various DRL algorithms. Therefore, our structure can make full use of the advancement of the DRL algorithm and identify the key features of the system at the same time. \n% \\vspace{-0.1in}\n"
                }
            },
            "section 5": {
                "name": "Experiments and Analysis",
                "content": "\n\nIn this section, the experimental system and settings are firstly introduced. Then the results for both FS solutions and DRL solutions are provided and analyzed.\nWe also briefly provide further experiments in OpenAI Gym to demonstrate D-AFS's adaptability. \n\n",
                "subsection 5.1": {
                    "name": "System Description and Settings",
                    "content": "\n\n Drag reduction and active flow control are critical technologies in the fluid industry. Here, we briefly introduce the target system and experiment settings. \n \n \\noindent\\textbf{Target System: } This environment is from a well-known benchmark \\cite{schafer1996benchmark} which consists of a cylinder of diameter $D=0.1$ immersed in a constant flow of a box of total length $L=2.2$ and height $H=0.41$. The cylinder is slightly downward from the center of the Y-Axis by 0.005. The mean velocity magnitude is $\\Bar{U}=1$.\nAnd the Reynolds number based on the mean velocity magnitude and cylinder diameter ($Re=\\Bar{U}D/v$, with $v$ the kinematic viscosity) is set to $Re=100$.\nWhen the target system is in an unstable state, the flow velocity and pressure of the wake behind the cylinder show periodic changes due to the transmission of the vortex street. As the learning of the control strategy becomes effective, the velocity and the pressure of the wake gradually decrease.\n\n\n\nTwo synthetic jets are located at the top/bottom extremities of the cylinder, which we can use DRL to control, as indicated in Fig. \\ref{fig:cfd_env} with solid red dots. There are 151 velocity probes placed around the cylinder and the near weak, which provide the network with a relatively detailed flow description, as indicated in Fig.~\\ref{fig:cfd_env} with black dots. Circle and Square are Top5 and Top11 probes selected by human experts. Our experimental environment is inherited from \\cite{rabault2019artificial}. Detailed settings and calculations can refer to the paper.\\footnote{https://github.com/jerabaul29/Cylinder2DFlowControlDRL}.\n\n% \\begin{figure}[ht]\n%     \\centering\n%     \\includegraphics[width=0.4\\textwidth]{pic/paperprobes.eps}\n%     \\caption{The location of all 151 probes.  The 5/11 probes selected by \\cite{rabault2019artificial}.}\n%     \\label{fig:my_label}\n% \\end{figure}\n\n% Figure \\ref{fig:cfd_p_v}, top a. and b. show the unsteady pressure and velocity fluctuations induced by the vortexes without active control.\n\n\\noindent\\textbf{D-AFS/PPO Settings: } Due to the limited continuous action space of the 2D flow control system, we use Beta policy\\cite{Chou-2017-26161}, which has finite support and no probability density that falls outside the boundary, to sample actions in the training process and use deterministic policy in the testing process. It is also the default policy of Tensorforce\\cite{tensorforce} for limited continuous action space, which is used by \\cite{rabault2019accelerating}.\nThe critic network consists of 4 layers: an input layer, two hidden layers with 512 cells, and an output layer.\nIn the AE module, the dense network \\(N_E\\) has 20 cells.\n\n\n\n\n\n\nThe original paper\\cite{rabault2019artificial} takes 24 hours to train a stable controller on a single CPU.\nAnd about 99.7\\% of the computation time is spent on the environment itself using FEniCS\\cite{alnaes2015fenics} rather than the DRL algorithm \\cite{rabault2019accelerating}.\nThus, we designed the asynchronous training architecture to accelerate Algorithm \\ref{algorithm} by syncing parameters and averaging gradients on each worker. The architecture reduced the training time from 40 hours to 3.6 hours by using 20 threads.\n\n\\noindent\\textbf{Evaluation Metrics and Reward Function:} We use drag coefficient $C_D$, lift coefficient $C_L$, recirculation area size, and velocity fluctuations and pressure fluctuations to evaluate the performance of a trained controller. The drag coefficient and lift coefficient are the normalized total instantaneous drag (opposite the constant flow direction) and lift (perpendicular to the constant flow direction) on the cylinder. And the recirculation area is the region in the downstream neighborhood of the cylinder where the horizontal component of the velocity is negative. To compare fairly, we use the standard PPO with the absolute same configuration for learning as the arbiter to evaluate different probe configurations. \n\nIn short, an excellent active flow control strategy means multiple evaluations, small $C_D$ and $C_L$, low $C_L$ fluctuations, pressure drop and velocity magnitude, and a large recirculation area. In this way, efficient active control can reduce vortex shedding caused by the bluff cylinder and gain a smoother flow. Here, the reward function is defined as follows:\n\\begin{equation}\n    r_t = -C_D - 0.2 |C_L|,\n\\end{equation}\nwhich is to minimize the drag and lift to stabilize the vortex alley and the same as other papers to facilitate comparison. \n\nIn the 2D simulation of the target system, our method is applied to learn the active flow control strategy and obtain attention weights of all 151 probes sensors simultaneously. We use the attention weight from the AE module when the D-AFS/PPO controller achieves stable rewards. Then, the probes with Top K weights are used for control with the same reward function and the original PPO controller. The $C_D$ and $C_L$ achieved with different methods are used for evaluations. \n\n\n"
                },
                "subsection 5.2": {
                    "name": "Comparison with Other FS Methods",
                    "content": "\n\n\nIn this section, we test the performance of state-of-the-art baseline feature selection algorithms in their feature identification capabilities in control. We choose eight strong feature selection baselines, including classic methods, gradient boosting methods and recent population-wise methods. We also provide one instance-wise feature selection method. Implementation of these algorithms are either by the scikit-learn package\\cite{pedregosa2011scikit} or from codes with their original papers. \\textbf{F\\&S} \\cite{gu2011generalized}, short for Fisher Score, a similarity-based method, which selects each feature independently according to their scores under the fisher criterion. \\textbf{XGB}\\cite{chen2016xgboost}, short for XGBoost,  an optimized distributed gradient boosting library that can calculate the feature importance across many trees. \\textbf{LGB}\\cite{ke2017lightgbm}, short for LightGBM, gradient boosting framework that uses tree based learning algorithms. \\textbf{RF}\\cite{ho1995random}, short for Random Forest, operates by constructing a multitude of decision trees at training time. \\textbf{FIR}\\cite{wojtas2020feature}, a novel dual-net architecture consisting of operator and selector for discovery of an optimal feature subset. \\textbf{CCM}\\cite{chen2017kernel}, employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response. \\textbf{SAN}\\cite{vskrlj2020feature}, explores the use of attention-based neural networks mechanism for estimating feature importance. We use both the local/global variants: SAN-local and SAN-global. \\textbf{L2X}\\cite{chen2018learning}, an instance-wise feature selection method that extract a subset of features that are most informative for each given example.\n\n\n% \\vspace{0.1in}\nTime series sensory data and immediate rewards during the training and exploration of the PPO controller are collected to provide training data set for supervised feature selection. The data set includes 50 epochs series, with 80 control steps in each epoch.\n\nFig. \\ref{fig:fs_comparsions} shows the Top 5 probes selected by different feature selection methods and corresponding control effects. The layouts selected by these supervised feature selection methods are not suitable for control, whether from numerical or physical perspectives. Most of them select probes whose characteristics do not change significantly during the periodic vortex shedding. Some of them, e.g. F\\&S, SAN, and L2X,select the probes located on the left side of the cylinder, generally seen as an ineffective area. The value of these probes generally provides little help for the drag reduction task. In order to verify their effectiveness in control, we use PPO to control systems with only Top 5 selected probe, respectively.  The drag and lift coefficient in the table (bottom part of Fig. \\ref{fig:fs_comparsions}) do clarify that these probes layouts cannot achieve effective active flow control behind the cylinder. In contrast, D-AFS achieve the best control effect for both $C_D$ and $C_L$ with the same number of probes. \n\n% \\subsection{Results \\& Analysis}\n"
                },
                "subsection 5.3": {
                    "name": "Comparison with Existing DRL solutions",
                    "content": "\n\n\n\n% \\noindent\\textbf{Comparision with existing DRL solutions}\n\nWe verify the control performance obtained using the probe configurations selected by different DRL solutions with 151 probes and observe their results.\nThe performance of the active flow control strategy under different configurations is shown in the upper part of Fig.~\\ref{tab:sensor_placements}. The numerical results for drag coefficient $C_D$, lift coefficient $C_L$ and recirculation area size are shown in the bottom part of Fig.~\\ref{tab:sensor_placements}. In addition to TopK probe configurations, we also show the performance of \\citeauthor{rabault2019artificial}'s result with selected five-probe, marked as Expert(5) and a randomly selected five-probe configuration denoted as Random(5). In Random(5), the best performance in five rounds is reported. Here, other reference use similar environment but different settings, e.g. Tang \\citep{tang2020robust} use four actuators instead of two, thus the result can not be directly compared. Reference \\cite{paris2021robust} requires heuristic enumeration of sensor layouts with only 15 probes and has not yet been open-sourced.\n\n\n\n% \\begin{figure}[ht]\n%     \\centering\n%     \\includegraphics[width=0.45\\textwidth]{pic/alldrag.eps}\n%     \\caption{Time-evolutionary value of the drag coefficient $C_D$ in the case without control(uncontrolled) and with active flow control(using different probes configurations), and the ideal case(the optimal value of the target environment).}\n%     \\label{fig:drag}\n% \\end{figure}\n\n% \\begin{table}[t]\n%     \\centering\n%     \\begin{tabular}{|c|c|c|c|c|}\n%     \\hline\n%         \\makecell[c]{Methods} & Probes & $C_D$ & $C_L$ & \\makecell[c]{Rec\\\\ Area} \\\\\n%         % \\makecell[c]{Rec \\\\ Area}\\\\\n%         \\hline\n%         % Uncontrolled & 151 & 3.2060 & 0.6764 & 0.0116 \\\\\n%         Uncontrolled & 151 & 3.205 & 0.676 & 0.012 \\\\\n%         % \\hline\n%         % Expert & 151 & 2.980 & 0.2100 & 0.0216 \\\\\n%         Rabault et al.\\shortcite{rabault2019artificial} & 151 & 2.980 & 0.210 & 0.022 \\\\\n%         % \\hline\n%         % Total & 151 & 2.946 & 0.0838 & 0.027 \\\\\n%         % Expert  & 5 & 3.026 & 0.2594 & 0.0167 \\\\\n%         Rabault et al.\\shortcite{rabault2019artificial} & 5 & 3.026 & 0.259 & 0.017 \\\\\n%         % \\hline\n%         % \\multirow{2}*{Expert} & 11 & 0.1554 & 0.0168 & 0.0131\\\\ \n%         % ~ & 5 & 0.1515 & 0.0150 & 0.0167\\\\ \n%         % Random & 5 & 2.9870 & 0.4240 & 0.0194 \\\\\n%         Random Selection & 5 & 2.987 & 0.424 & 0.019 \\\\\n%         \\hline\n%         % \\hhline{|=|=|=|=|=|}\n%         % \\multirow{4}*{\\makecell[c]{Random\\\\ Selection}} & \\multirow{4}*{5} & 0.1526 & 0.0399 & 0.0148\\\\\n%         % ~ & ~ & 0.1500 & 0.0359 & 0.0166\\\\\n%         % ~ & ~ & 0.1532 & 0.0277 & 0.0161\\\\\n%         % ~ & ~ & 0.1493 & 0.0212 & 0.0194\\\\\n%         % D-AFS & 5 & \\textbf{2.9380} & \\textbf{0.1260} & \\textbf{0.0296} \\\\\n%         D-AFS(15) & 15 & \\textbf{2.938} & \\textbf{0.112} & 0.029 \\\\\n%         D-AFS(10) & 10 & 2.964 & 0.129 & 0.025 \\\\\n%         D-AFS(5) & 5 & \\textbf{2.938} & 0.126 & \\textbf{0.030} \\\\\n%         % D-AFS(5) \\% & 5 & 8.36% & 81.36% & 150%\n%         Ideal Case & - & 2.93 & - &  - \\\\\n%         % \\multirow{3}*{D-AFS} & 15 & 0.1486 & 0.0100 & \\textbf{0.0298}\\\\\n%         % ~ & 10 & 0.1488 & 0.0103 & 0.0241\\\\\n%         % ~ & 5 & \\textbf{0.1482} & \\textbf{0.0090} & 0.0296\\\\\n%         % \\midrule\n%         % Improvement & \\\\ & 8.4\\% &  \n        \n%         % \\hline\n%         % The ideal drag coefficient on the full cylinder in a steady state without vortex shedding $C_D=2.93$\n%     \\hline\n%     \\end{tabular}\n%     \\caption{The mean values of $C_D$, $C_L$ and Rec Area when they get steady in the different sensor configurations.}\n%     \\label{tab:sensor_placements}\n% \\end{table}\nThe top part of the figure shows that the drag coefficient $C_D$ varies periodically as the vortex shedding when the system is uncontrolled. The mean value of  $C_D$ is around 3.205, and the fluctuation amplitude is around 0.034. In comparison, taking the Top5/10/15 probes selected by D-AFS as observations, the PPO controller can achieve better active flow control. \nThe $C_D$ of D-AFS with the five-sensor is very close to, more exactly 99.73\\%, the ideal 2.93(denoted by dashed gray line) where no vortex shedding exists\\cite{rabault2019artificial}. \nCompared with the five-sensor configuration by human experts\\cite{rabault2019artificial}, D-AFS achieves a 260\\% increase in the recirculation area and 49.2\\% drag reduction. Even compared to the state-of-the-art solution with 151 probes, our five-sensor configuration can achieve a further 18.7\\% drag reduction. \nThe drag coefficient $C_D$ of the best random selection is better than Expert(5), but the lift coefficient $C_L$ is worse than the latter. That means the control strategy using randomly selected probes is unstable.\n\n\n% In comparison, the human arbitrary selected by suffer the worst performance in all three 5 configurations, even much worse than Random(5). \n\n\n%  The strategies in 5 and 15 probes cases are relatively better.\n \n \n\n% Furthermore, our results are very stable with little fluctuation. This fact shows that our solution can effectively weak the vortex.\nInterestingly, adding more sensors to the five-sensor layout selected by D-AFS does not bring further drag reduction, which makes this five-sensor configuration the optimized trade-off between DRL performance and sensor selection complexity. It shows that too many possibly redundant inputs may cause interference and hindrance to the controller with limited modeling capabilities, albeit they might contain more information. This finding matches precisely the finding stated in \\cite{paris2021robust}.\n\n% some similar good control laws compared to the uncontrolled case. The drag is obviously reduced with the active flow control, almost the same as the full-featured training strategy.\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Further Analysis",
                    "content": "\n  In this section, we provide further analysis on the probe weight distribution, control effect analysis,  impacts of different probe configurations and the complexity analysis.\n  \n\\noindent\\textbf{Feature weight distribution:} Fig.~\\ref{fig:selection} illustrates the attention weight of 151 probes with different gradient colors. The darker the color, the bigger the weight and the higher the importance of the probe to the controller. We also marked the Top5, Top10, and Top15 probes with the cross, circle, and diamond. The Top5 probes selected by D-AFS are scattered behind the cylinder, and they provide an excellent `coverage' of the instantaneous vortex variation, as shown in Fig.~\\ref{fig:equ_sensors}. These probes are normally critical for the control task, which can explain the selection preference of D-AFS to a certain extent. That is, probes that can deliver critical information conducive to learning will get a higher weight.\n% This weight refers to the test result of the model saved under the training convergence.\n% We have repeated experiments many times to achieve stable results. Although the probes selected by each experiment are different, the control strategies are very similar.\n\n\n\n\n\n\n\\noindent\\textbf{Control effect analysis:} We take a look at macroscopic flow characteristics and how the active control affects them. Fig.~\\ref{fig:cfd_p_v} shows the snapshots of the pressure (subgraph a.) and velocity distribution(subgraph b.) under three configurations when the system enters steady states. The three configurations are Uncontrolled, Expert(5), and D-AFS(5). \n\n% Based on the results of the previous subsection, it is found that D-AFS can automatically generate the quantitative importance of different observations through the exploration and exploitation process of DRL in the dual world.\n% In this section, we will further pay attention to the optimal probe placement selection problem and expect to use partial system information provided by selected probes to get a better strategy. This experiment aims to inspect the control strategy using the partial observation selected by our method, which is mainly compared with the selection guided by human experts and random selection.\n\n% We reproduce the paper's \\cite{rabault2019artificial} result, both the complete observation case(151 probes, a relatively detailed flow description) and the partial observation case(5/11 probes manually selected by experts, which is marked in Figure \\ref{fig:cfd_env}). \n\n\n\n\n\nThe pressure distribution shows that the vortex in the wake caused by the cylinder is manifested as a significant pressure drop without active control. Similarly, the velocity magnitude fluctuates a lot, visible from the top sub-figure of the velocity distribution. The control effect from Expert(5) is shown in the middle of the two subgraphs, and we can see those probes are not in the key positions for vortex shedding. In comparison, the selected Top5 probes have good ``coverage'' of the instantaneous recirculation bubble. Thus, the control effect of D-AFS is much better. PPO with D-AFS(5) attenuates velocity fluctuations caused by the vortexes, achieves that vortexes are globally very weak and least active close to the upper and lower walls. More strikingly, the extent of the recirculation area is dramatically increased. \n\n\\noindent\\textbf{Equivalent probe layouts:}\nAs introduced previously, optimizing sensors' number and location are widely demanded in various industrial domains to find cost-effective sensor solutions in redundant probes. For D-AFS, it is possible to select probes with totally different layouts because of the random exploration and exploitation nature. Fig. \\ref{fig:equ_sensors} shows two identified Top5 probes with two separated experiments, illustrated with black circles and yellow squares. They provide almost identical control performance. The drag coefficient $C_D$ is around 2.940, the lift coefficient $C_L$ is around 0.130.\n\n\n\nThe left and the right part of Fig. \\ref{fig:equ_sensors} are two different snapshots, which show the forward propagation of the vortex to better illustrate selected probes' positions on the route of vortex. We can find that, although the two configurations are highly different(only one overlap), each configuration covers the key positions which contain richer information of periodic vortex shedding or located around the edge of the recirculation area. For instance, observations detected by probes located at (0.15, -0.1), (0.25, -0.15), (0.3, 0.05), (0.4,0.15) and (0.45, -0.05) enable the DRL controller obtain rich periodic information. This also means that the control task can be converted from drag reduction to vibration reduction for these observations.\n\n\n% No probe in the centerline (Y=0) is selected. A possible explanation could be that these sensors are unable to provide information related to the instantaneous asymmetry of flow and are therefore less useful in choosing control actions, which also matches the finding from \\cite{paris2021robust}.\n\n% Three major popular algorithms are implemented for comparison: XGB(short for XGBoost\\cite{chen2016xgboost}), LGB(short for LightGBM\\cite{ke2017lightgbm}) and RF(short for Random Forest). These algorithms are all provided by the scikit-learn package\\cite{pedregosa2011scikit}. Other feature selection algorithms, like ReliefF\\cite{robnik2003theoretical} and Fisher Score\\cite{gu2011generalized}, are failed to generate the weights after 20 hours of computation in the Dell T640 20 core server. Thus, these results have been discarded. \n\n% Moreover, we also compare with the features selected via human experts provided in the OpenAI Gym environment.\n\n\n\n% instancewise: L2X\\cite{chen2018learning}\n% populationwise: FIR\\cite{wojtas2020feature}, ccm\\cite{chen2017kernel}, SANs\\cite{vskrlj2020feature}\n\n\n% \\begin{figure*}\n%     \\centering\n%     \\includegraphics[width=\\textwidth]{pic/otherfs.png}\n%     \\caption{Caption}\n%     \\label{fig:my_label}\n% \\end{figure*}\n\n% \\begin{table}[]\n%     \\centering\n%     \\begin{tabular}{cccccc}\n%     \\hline\n%         Methods & FIR & ccm & SAN-global & L2X & SAN-local \\\\\n%         \\hline\n%         $C_D$ & 3.038 & 3.116 & 3.012 & 3.068 & 3.076 \\\\\n%         $C_L$ & 2.226 & 4.204 & 7.300 & 2.872 & 3.116 \\\\\n%         \\hline\n%         % FIR & 3.038 & 2.226 \\\\\n%         % ccm & 3.116 & 4.204 \\\\\n%         % SAN-global & 3.012 & 7.300 \\\\\n%         % L2X & 3.068 & 2.872 \\\\\n%         % SAN-local & 3.076 & 3.116 \\\\\n%         % \\hline\n%     \\end{tabular}\n%     \\caption{Caption}\n%     \\label{tab:coneff_fs}\n% \\end{table}\n\n\n\n% Though the experts' case can reduce the effect of vortex shedding to a certain, our scheme achieves a lower mean pressure drop and more smooth velocity magnitude obviously.\n% Besides, in the case of using our sensor placement, the recirculation area of the separated wake behind the cylinder is larger than the experts' scheme.\n\n\n\n% the controller trained by partial observation schemes selected by our method eclipse the one trained by human experts' selection under the same size and structure of PPO.\n% The phenomenon is particularly prominent in the case of 5 probes selected by D-AFS, of which the pressure and velocity field snapshots are presented in Fig. \\ref{fig:cfd_p_v}. \n\n\n\n% Compared with the expert selected probes(5/11) and the total probes(151), we can see from Figure \\ref{fig:cfd_p_v} that the velocity fluctuations and pressure fluctuations induced by the vortexes are globally not too strong and less active close to the upper and lower walls. \n% This illustrates that despite DRL can perform control when only partial information is available, the effectiveness of the control strategy still has a great difference. \n% To better verify this point, we conducted repeated experiments with 5 sensors randomly set, and the results also conformed the effectiveness of our method in sensor placement. \n% The value of $C_D$ under various sensor placements is plotted in Figure \\ref{fig:comparison}, where the random curve is the best result in all random placement situations.\n\n\n% The value of $C_D$ under various sensor placements is plotted in Fig. \\ref{fig:drag}, where the random curve is the best result in all random placement situations.\n% This illustrates that despite DRL can perform control when only partial information is available, the effectiveness of the control strategy still has a great difference. \n% As visible in here, the control strategy trained using the sensor placement selected by D-AFS has the lowest mean $C_D$ and the smallest fluctuation after stabilization.\n% This illustrates that despite DRL can perform control when only partial information is available, the effectiveness of the control strategy still has a great difference. \n\n% \\begin{figure}[ht]\n%     \\centering\n%     \\includegraphics[width=0.4\\textwidth]{pic/5probescon.eps}\n%     \\caption{The drag coefficient convergence in different probe placements(5 probes). The mean value of $C_D$ reaches 2.938 for D-AFS, and 3.026 for human experts.}\n%     \\label{fig:comparison}\n% \\end{figure}\n\n% As visible in Figure \\ref{fig:comparison}, the control strategy trained using the sensor placement selected by D-AFS has the lowest mean $C_D$ and the smallest fluctuation after stabilization. The \n% the asymptotic drag reduction in the case of a reduced input information size is a bit less good than with full flow information except D-AFS. \n% In a word, D-AFS gets the most steady and lowest drag coefficient among four random placements and human expert placement.\n\n% More obviously, the extent of the recirculation area has increased significantly. \n% In Table \\ref{tab:sensor_placements}, the performance of the controller trained by 5 probes selected by D-AFS has a 252.9\\% increase and a 16.9\\% increase in the recirculation area compared to the expert selected 5 probes and a total of 151 probes, respectively.\n% Cause the input space we use is only 1/15 compared with the original paper(other settings are the same), so D-AFS can significantly reduce the computation loads.\n\n\n% which use 151 probes to get a stable and excellent active control strategy, as well as an unsteady and relatively poor strategy by using 5 or 11 probes selected from 151 probes by expert experience.\n\n% \\noindent\\textbf{Observation Noise Robustness}\n\n% This experiment aims to study the effectiveness of our method when some probes are completely Gaussian noise. \n% We randomly selected 15 probes from 151 probes and set their values to Gaussian noise. \n% The attention weights of some of them during the training process are presented in Fig.~\\ref{fig:noise}. \n% The solid line shows the smooth probe weights with Savitzky-Golay filter to better observe the trend of weights. Though the weights calculated by attention networks during initialization are different, a clear downward trend can be seen in Fig.~\\ref{fig:noise}. This shows that our method can effectively eliminate noise or faulty sensors. \n\n% \\vspace{-0.2cm}\n% \\begin{figure}[ht]\n%     \\centering\n%     \\includegraphics[width=0.4\\textwidth]{pic/noiseweight.eps}\n%     \\caption{Robustness to faulty probes with Gaussian noises.}\n%     \\label{fig:noise}\n%     \\vspace{-0.2cm}\n% \\end{figure}\n\n\\noindent\\textbf{Complexity analysis:} Table~\\ref{tab:complexity} shows the time complexity in generating feature weights for systems with different numbers of probes, ranging from 5 to 151. We also show the time when pure PPO is used for training. The settings for epochs and steps are the same as the experiment settings mentioned earlier. From this table, we can see that D-AFS generates feature weights in the process of DRL exploration and exploitation, and the overall time consumed is almost the same as that of pure PPO. The computation and resource complexity is relatively low. \n\nIn comparison, if the DRL is only used as an arbiter with a certain heuristic-guided probe configuration, e.g., \\cite{paris2021robust}, the computation complexity is about $O(C_n^r)$ with n are possible probes, and $r$ are probes to select. It will increase rather fast when n and r increase. The author pointed out that each training costs around 40 CPU hours when the system only has 12 probes. Although they provide no discussion on computation complexity, we deduce that this approach can hardly be applied to the system with 151 probes. \n\n\n% \\vspace{-0.2cm}\n\n"
                },
                "subsection 5.5": {
                    "name": "Further Experiments in OpenAI Gym",
                    "content": "\n\nTo better explore the generalization of D-AFS, we also tune two popular DRL algorithms, DQN and DDPG into our framework. We choose four classical control cases provided by OpenAI Gym\\cite{brockman2016openai}, including Pendulum, MountainCar, CartPole, and Acrobot, to observe control performance with feature subsets selected by D-AFS. \n\n\n% We also tune two popular DRL algorithms, DQN and DDPG\\cite{lillicrap2016continuous} into our D-AFS framework. A set of experiments has been performed on four classical control cases provided by OpenAI Gym\\cite{brockman2016openai}, including Pendulum, MountainCar, CartPole, and Acrobot. We use the original inputs and certain transformation functions to generate a set of new constructed features and one random noise as possible inputs. We set the same network and training configuration for DQN as the ones defined in Gym. The data constitutes 200000 steps of interaction data between the controller and the system which cover all the interaction process information between the controller and the system.\n\n\n\nThe lower part of Fig.~\\ref{fig:weight_diff} is a table, in which values are rewards of four systems with feature subsets selected by different methods. The larger the rewards, the better the performance of DRL's controller. \nTo verify that the control performance is influenced only by features, experimental settings are all the same except for the feature subsets.\nResults in the table show that D-AFS can select relevant features effectively and achieve the same or even better control performance than the original expert-based subsets. In Pendulum, MountainCar and Acrobot, D-AFS achieves the same rewards as Gym. Actually, it deduces the same sensor sets used with Gym in all three cases. In Cartpole, D-AFS achieves the maximum return under the limit of 1000 maximal steps in an episode, which is about 54.70\\% higher than the 646.4 achieved by the features provided by Gym with a different set of features. Due to the page limits, the upper part of Fig.~\\ref{fig:weight_diff} only shows the feature weights calculated by different methods for CartPole. Since the feature weight values generated by different methods are not uniform, we normalize all weights for better comparison. Regarding the features provided by Gym based on human expertise, we set their weights to 1, while other features that are not included are set to 0. More comprehensive weights for other cases are provided in Appendix A. \n\n\n\n%  show the feature weights generated by different methods. Since the feature weight values generated by different methods are not uniform, we normalize all weights for better comparison. Regarding the features provided by Gym based on human expertise, we set their weights to 1, while other features that are not included are set to 0. The table in the lower part shows the maximal rewards achieved with the set of features selected via different methods.\n\n% Results in the table further verify our findings of the limitations of traditional FS for control. Even for those four comparable simple systems, XGB and LGB can not effectively find most relevant features. In addition, their performance in different systems is also volatile. \n% % XGB and RF can effectively eliminate noise signals, but they cannot find the necessary features for control. Moreover, the feature weights generated by these ML methods sometimes are very indistinguishable from each other, such as the performance of XGB in CartPole. \n% The poor cumulative return in the table can also illustrate their limitations in control.\n% These facts show that D-AFS can effectively select the most relevant features for control, which is crucial for understanding the environments without a model. Due to page limits, comprehensive experiment results are provided in Appendix A. \n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nUnlike humans who are good at learning unknown environments through interaction, the most existing DRL algorithms cannot gain explicit environmental knowledge from the learning and exploration process. In this paper, a dual-world feature selection method D-AFS is proposed to quantitatively evaluate the impacts of different inputs for control during the DRL training process. A set of experiments is performed in one classic flow control. Results show that D-AFS can effectively identify the system-mechanism-related inputs. With the deduced five-probe layout, we can achieve the best drag reduction than all other strong baselines. Results highlight, we argue, a new way to achieve efficient and optimal sensor designs for experimental or industrial systems. In this paper, our work limits to the active flow problem with limited complexity due to the high computation complexity with FEniCS for solving partial differential equations. We are working on implementing D-AFS on a deep neural network based simulator for more fast and complex tasks. The other possible direction is to find a solution that can automatically decide the optimal set of sensors selected for control. \n% \\textcolor{blue}{add limitation about cannot decide the optimal number of sensors.}\n\n\n% The proposed method provides an automatic and feasible way to understand unknown systems with the DRL's learning capabilities.\n\n% There remain some questions to be answered in future work. Currently, we only focus on the average weight distributions of different features. It is possible, in the specific scenarios, different inputs might have scenario-specific weights. Those temporal effects sometimes have profound means for the system dynamics interpretations. Another direction for the D-AFS is the better integration of attention-weighted mechanisms with more DRL algorithms.\n\n"
            },
            "section 7": {
                "name": "Acknowledgement",
                "content": "\nThis work was supported by the National Science Foundation of China (61772473, 62073345 and 62011530148). F. Wang was supported by the Equipment Pre-research Key Laboratory Funds No. 61425010102.\n\n% \\begin{verbatim}\n  \\bibliographystyle{ACM-Reference-Format}\n  \\bibliography{kdd22}\n% \\end{verbatim}\n\n\\onecolumn\n"
            },
            "section 8": {
                "name": "Appendix A:  The experimental results of OpenAI Gym",
                "content": "\nSince our method has good scalability and can minimize the changes to the existing DRL algorithm, we apply the architecture to two mainstream algorithms, DQN and DDPG.\nWe apply D-AFS/DQN and D-AFS/DDPG to four classical control environments provided by OpenAI Gym, the Pendulum, MountainCar, CartPole, and Acrobot.\nThe four target systems are shown in Fig.~\\ref{fig:system}, and the weights generated by D-AFS are shown in Table \\ref{D-AFS_selection}. The boldface in the table indicates the features selected by D-AFS. ``P\\_Ran\" and ``Ran\" represent partial noise and complete noise, respectively. The results show that our method can effectively select the most relevant sensors and reduce human expertise reliance.\n\n\n\\label{sec:appendixA}\n% \\FloatBarrier\n\n% \\begin{table*}[hbp]\n% \\renewcommand{\\arraystretch}{1.3}\n% \\caption{The four classical control systems provided by OpenAI Gym}\n% \\label{table_system}\n% \\resizebox{\\textwidth}{!}{\n% \\centering\n% \\begin{tabular}{ccccc}\n% \\hline\n% \\bfseries Systems & \\bfseries Raw inputs & \\bfseries Gym inputs & \\bfseries Constructed inputs & \\bfseries Reward functions\\\\\n% \\hline\n% Pendulum & $\\theta$ & $\\cos\\theta$, $\\sin\\theta$, $\\omega$ & $\\theta$, $\\cos\\theta$, $\\sin\\theta$, $\\omega$, P\\_Ran, Ran & reward$=\\theta^{2} + 0.1\\omega^{2} + 0.001u^{2}$\\\\\n% MountainCar & $x$, $v$ & $x$, $v$ & $x$, $v$, P\\_Ran, Ran & \\makecell[c]{Discrete: if not $x>0.5$, reward=$-1.0$\\\\Continuous: reward$-=0.1u^2$,if $x>0.5$, reward=100.0}\\\\\n% CartPole & $x$, $\\theta$ & $x$, $v$, $\\theta$, $\\omega$ & $x$, $v$, $\\theta$, $\\omega$, $\\sin\\theta$, $\\cos\\theta$, P\\_Ran, Ran & $\\forall(abs(x)<2.4\\land abs(\\theta)<12^{\\circ})$, reward=$1.0-x^2$\\\\\n% % \\tabincell{c}{Discrete:$\\forall(abs(x)<2.4\\land abs(\\theta)<12^{\\circ})$, reward+=$1.0$ \\\\ Continuous:$\\forall(abs(x)<2.4\\land abs(\\theta)<12^{\\circ})$, reward=$1.0-x^2$} \\\\\n% Acrobot & $\\theta_1$, $\\theta_2$ & \\makecell[c]{$\\sin\\theta_1$, $\\cos\\theta_1$, \\\\ $\\sin\\theta_2$, $\\cos\\theta_2$, \\\\ $\\omega_1$, $\\omega_2$} & \\makecell[c]{$\\theta_1$, $\\theta_2$, $\\sin\\theta_1$, $\\cos\\theta_1$, $\\sin\\theta_2$, \\\\$\\cos\\theta_2$, $\\omega_1$, $\\omega_2$, P\\_Ran, Ran} & $\\forall(-\\cos\\theta_1-\\cos(\\theta_1+\\theta_2))<=1.0$, reward=$-1.0$\\\\\n% \\hline\n% \\end{tabular}}\n% \\end{table*}\n\\setcounter{figure}{0}\n\\renewcommand{\\thefigure}{A\\arabic{figure}}\n\n\n\\setcounter{table}{0}\n\\renewcommand{\\thetable}{A\\arabic{table}}\n% \\FloatBarrier\n\n\n"
            }
        },
        "tables": {
            "tab:complexity": "\\begin{table}[ht]\n\\caption{The computational complexity of D-AFS and pure PPO without feature selection.(Time unit: hour)}\n    \\centering\n    \n    \\setlength{\\belowdisplayskip}{0.5cm}\n    \\begin{tabular}{c|ccccc}\n    \\hline\n        Probes & 5 & 10 & 50 & 100 & 151  \\\\\n    \\hline\n        PPO & 3.17 & 3.24 & 3.30 & 3.39 & 3.46 \\\\\n        % \\hline\n        D-AFS/PPO & 3.19 & 3.28 & 3.37 & 3.49 & 3.60 \\\\\n        \\hline\n    \\end{tabular}\n    \\label{tab:complexity}\n    % \\vspace{-0.2in}\n\\end{table}",
            "D-AFS_selection": "\\begin{table*}[hb]\n\\renewcommand{\\arraystretch}{1.3}\n\\caption{Results of D-AFS in four classical control systems provided OpenAI Gym}\n\\label{D-AFS_selection}\n\\resizebox{\\textwidth}{4cm}{\n\\centering\n\\begin{tabular}{cccccccccccc}\n\\hline\n\\bfseries Algorithm & \\bfseries Systems & \\multicolumn{9}{c}{\\textbf{All inputs and weights}} \\\\\n\\hline\n\\multirow{9}*{D-AFS/DQN} & \\multirow{2}*{Pendulum} & $\\theta$ & $\\omega$ & $\\sin\\theta$ & $\\cos\\theta$ & P\\_Ran & Ran & ~ & ~ & ~ & ~ \\\\\n\t\t~ & ~ & 0.516 & \\textbf{0.968} & \\textbf{0.960} & \\textbf{0.879} & 1.27e-4 & 2.32e-5& ~ & ~ & ~ & ~ \\\\\n~ & \\multirow{2}*{MountainCar} & $x$ & $v$ & P\\_Ran & Ran & ~ & ~ & ~ & ~ & ~ & ~ \\\\\n~ & ~ & \\textbf{0.862} & \\textbf{0.828} & 7.99e-5 & 1.75e-4 & ~ & ~ & ~ & ~ & ~ & ~ \\\\\n~ & \\multirow{2}*{CartPole} & $x$ & $v$ & $\\theta$ & $\\omega$ & $\\sin\\theta$ & $\\cos\\theta$ & P\\_Ran & Ran & ~ & ~ \\\\\n~ & ~ & \\textbf{0.618} & \\textbf{0.601} & 0.047 & \\textbf{0.952} & \\textbf{0.997} & \\textbf{0.955} & 2.28e-4 & 9.57e-4 & ~ & ~ \\\\\n~ & \\multirow{3}*{Acrobot} & $\\theta_1$ & $\\theta_2$ & $\\omega_1$ & $\\omega_2$ & $\\sin\\theta_1$ & $\\cos\\theta_1$ & $\\sin\\theta_2$ & $\\cos\\theta_2$ & P\\_Ran & Ran \\\\\n~ & ~ & \\textbf{0.907} & 0.407 & \\textbf{0.921} & \\textbf{0.470} & 0.356 & \\textbf{0.965} & \\textbf{0.809} & \\textbf{0.848} & 1.04e-3 & 1.95e-3 \\\\\n~ & ~ & 0.408 & 0.427 & \\textbf{0.934} & \\textbf{0.537} & \\textbf{0.674} & \\textbf{0.938} & \\textbf{0.658} & \\textbf{0.924} & 1.75e-3 & 2.79e-3 \\\\\n\\hline\n\\multirow{8}*{D-AFS/DDPG} & \\multirow{2}*{Pendulum} & $\\theta$ & $\\omega$ & $\\sin\\theta$ & $\\cos\\theta$ & P\\_Ran & Ran & ~ & ~ & ~ & ~ \\\\\n\t\t~ & ~ & 0.243 & \\textbf{0.561} & \\textbf{0.956} & \\textbf{0.739} & 1.20e-3 & 2.96e-3& ~ & ~ & ~ & ~ \\\\\n~ & \\multirow{2}*{MountainCar} & $x$ & $v$ & P\\_Ran & Ran & ~ & ~ & ~ & ~ & ~ & ~ \\\\\n~ & ~ & \\textbf{0.178} & \\textbf{0.894} & 3.48e-4 & 3.27e-4 & ~ & ~ & ~ & ~ & ~ & ~ \\\\\n~ & \\multirow{2}*{CartPole} & $x$ & $v$ & $\\theta$ & $\\omega$ & $\\sin\\theta$ & $\\cos\\theta$ & P\\_Ran & Ran & ~ & ~ \\\\\n~ & ~ & \\textbf{0.822} & 0.386 & \\textbf{0.955} & 0.356 & \\textbf{0.868} & \\textbf{0.607} & 0.009 & 0.002 & ~ & ~ \\\\\n~ & \\multirow{2}*{Acrobot} & $\\theta_1$ & $\\theta_2$ & $\\omega_1$ & $\\omega_2$ & $\\sin\\theta_1$ & $\\cos\\theta_1$ & $\\sin\\theta_2$ & $\\cos\\theta_2$ & P\\_Ran & Ran \\\\\n~ & ~ & \\textbf{0.651} & \\textbf{0.459} & 0.273 & 0.302 & \\textbf{0.609} & \\textbf{0.754} & \\textbf{0.785} & \\textbf{0.821} & 2.67e-3 & 2.16e-3 \\\\\n\\hline\n\\end{tabular}}\n\\end{table*}"
        },
        "figures": {
            "fig:intro": "\\begin{figure}\n    \\centering\n    \\includegraphics[width= 0.48\\textwidth]{pdflatex-pic/intro2.pdf}\n    \\caption{Instantaneous velocity flow field with 151 probes. Black dots represent sensor locations and colors represent velocity(fields).}\n    \\label{fig:intro}\n    \\vspace{-0.2in}\n\\end{figure}",
            "fig:architecture": "\\begin{figure*}[tb]\n    \\centering\n    \\includegraphics[width=0.85\\textwidth]{pdflatex-pic/structure.pdf}\n    \\caption{The overall architecture of D-AFS with the ``real'' and ``virtual'' world environment, the evaluation module and the DRL module. The virtual world is mapped from the real world. DRL learns in two mixed worlds, explores the correct control strategy in the real world, and provides evaluation information for feature selection in the virtual world.}\n% \\vspace{-0.15in}\n    \\label{fig:architecture}\n\\end{figure*}",
            "fig:cfd_env": "\\begin{figure}[th]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{pdflatex-pic/cfd_env.pdf}\n    \\caption{The 2D active flow control simulation environment without control. The location of the jets is indicated by the red dots. }\n    \\label{fig:cfd_env}\n\\end{figure}",
            "fig:fs_comparsions": "\\begin{figure*}\n    % \\centering\n      \\begin{minipage}[b]{1.0\\textwidth}\n       \\includegraphics[width=1.0\\textwidth]{pdflatex-pic/otherfs.png}\n       \\end{minipage}%\n \\vspace{0.3cm}      \n\\begin{minipage}[b]{0.9\\textwidth}\n\\centering\n    \\begin{tabular}{cccccccccccc}\n    \\hline\n        Methods & Uncontrolled & F\\&S & XGB & LGB & RF & FIR & CCM & SAN-global & L2X & SAN-local & D-AFS\\\\\n        \\hline\n        $C_D$ & 3.205 & 3.118 & 3.062 & 3.044 & 3.082 & 3.038 & 3.116 & 3.012 & 3.068 & 3.076 & \\textbf{2.938} \\\\\n        $C_L$ & 0.676 & 0.784 & 0.452 & 0.291 & 1.036 & 0.338 & 0.655 & 1.099 & 0.441 & 0.479  & \\textbf{0.126} \\\\\n        \\hline\n    \\end{tabular}\n% \\end{table}\n\\end{minipage}%\n\\caption{Comparison of the Top 5 probes selected with different feature selection solutions. Table shows the average values of the drag coefficient $C_D$  and lift coefficient $C_L$  when the control strategy converges with only the selected five probes with the PPO controller}\n\\label{fig:fs_comparsions}\n\\end{figure*}",
            "tab:sensor_placements": "\\begin{figure}[t]\n    \\centering\n      \\begin{minipage}[b]{0.48\\textwidth}\n       \\includegraphics[width=0.99\\textwidth]{pdflatex-pic/alldrag.pdf}\n    %   \\caption{Image}\n       \\end{minipage}%\n       \n\\begin{minipage}[b]{0.48\\textwidth}\n    \\centering\n    \\begin{tabular}{ccccc}\n    % \\hline\n    \\toprule\n        Methods & Probes & $C_D$ & $C_L$ & \\makecell[c]{Rec Area} \\\\\n        % \\makecell[c]{Rec \\\\ Area}\\\\\n        \\midrule\n        % \\hline\n        Uncontrolled & 151 & 3.205 & 0.676 & 0.012 \\\\\n        Rabault et al.\\shortcite{rabault2019artificial} & 151 & 2.980 & 0.210 & 0.022 \\\\\n        % Tang et al. .\\shortcite{tang2020robust} & 151 & 3.026 & 0.259 & 0.017 \\\\\n        Rabault et al.\\shortcite{rabault2019artificial} & 5 & 3.026 & 0.259 & 0.017 \\\\\n        Random Selection & 5 & 2.987 & 0.424 & 0.019 \\\\\n        \\hline\n        D-AFS(5) & 5 & \\textbf{2.938} & 0.126 & \\textbf{0.030} \\\\\n        D-AFS(10) & 10 & 2.964 & 0.129 & 0.025 \\\\\n        D-AFS(15) & 15 & \\textbf{2.938} & \\textbf{0.112} & 0.029 \\\\\n        Ideal Case & - & 2.93 & - &  - \\\\\n    % \\hline\n    \\bottomrule\n    \\end{tabular}\n   \n    % \\label{tab:sensor_placements}\n% \\end{table}\n\\end{minipage}%\n% \\vspace{-0.1in}\n \\caption{Time-evolutionary value of the drag coefficient $C_D$  with different solution. Table shows the mean values of $C_D$, $C_L$ and Rec Area when system becomes steady.}\n%  \\vspace{-0.2in}\n \\label{tab:sensor_placements}\n\\end{figure}",
            "fig:selection": "\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{pdflatex-pic/ourprobes.pdf}\n    \\caption{The attention weight distribution of 151 probes. The Top5/10/15 probes in weight ranking have been marked.}\n    \\label{fig:selection}\n\\end{figure}",
            "fig:cfd_p_v": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]{pdflatex-pic/cfd_p_v_2.pdf}\n    \\caption{Comparison of active control performance(velocity and pressure) of uncontrolled and two five-probe configurations.}\n    \\label{fig:cfd_p_v}\n    % \\vspace{-0.1in}\n\\end{figure}",
            "fig:equ_sensors": "\\begin{figure}\n\\includegraphics[width=0.47\\textwidth]{pdflatex-pic/group3.png}\n% \\vspace{-0.1in}\n \\caption{Identified two equivalent probe layouts with similar DRL performance, black circles and yellow squares indicate the probes selected in two separated experiments.}\n \\label{fig:equ_sensors}\n \\vspace{-0.1in}\n\\end{figure}",
            "fig:weight_diff": "\\begin{figure}[hbp]\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]{pdflatex-pic/gymresult.pdf}\n    \\caption{Comparison of controller performance with the feature subsets selected by different methods.}\n        % \\vspace{-0.2in}\n    \\label{fig:weight_diff}\n\\end{figure}",
            "fig:system": "\\begin{figure*}[hbp]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{pdflatex-pic/system.png}\n    \\caption{The physical meaning of the major features in the four systems}\n    \\label{fig:system}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n% \\vspace{-0.1in}\n\\label{eq:optimal_s}\n    \\bm{\\mathcal{S}}^* = \\arg\\max_{\\bm{\\mathcal{S}}_r} \\mathcal{R}_t = \\arg\\max_{\\bm{\\mathcal{S}}_r} \\sum_{t}^{\\infty}\\gamma^{t}r_{t}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\label{eq:softmax}\n    p^k=\\frac{\\exp(x^k)}{\\exp{\\left(x^k\\right)}+\\exp(y^k)},x^k\\in\\bm{X},y^k\\in\\bm{Y}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\bm{\\mathcal{S}}_v=\\bm{\\mathcal{S}}_r \\odot \\bm{P}\n\\label{eq:att}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\begin{aligned}\n        \\theta^* = \\arg \\max_{\\theta} L^{CLIP}(\\theta)\n    \\end{aligned}\n    % L_t(\\theta)=min(r_t(\\theta_k) A^{\\pi_{\\theta_k}}(s_t, a_t), clip(r_t(\\theta)), 1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_k}}(s_t, a_t)\n    % \\theta_{k+1} = \\arg \\max_{\\theta} \\frac{1}{|\\mathcal{D}_k|T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T min(r_t(\\theta))A^{\\pi_{\\theta_k}}(s_r,a)\n    \\label{eq:actor}\n\\end{equation}",
            "eq:5": "\\begin{equation*}\n    L^{CLIP}(\\theta)=\\mathbb{E}_t[\\min(r_t(\\theta), {\\rm clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon))A^{\\pi_{\\theta_t}}(s_r,a)],\n\\end{equation*}",
            "eq:6": "\\begin{equation}\n    \\phi^*, \\psi^*=\\arg \\min_{\\phi,\\psi} \\mathbb{E}_t (V_{\\phi,\\psi}(s_v)-\\mathcal{R}_t)^2\n    \\label{eq:critic}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    r_t = -C_D - 0.2 |C_L|,\n\\end{equation}"
        },
        "git_link": "https://github.com/G-AILab/DAFSFluid."
    }
}