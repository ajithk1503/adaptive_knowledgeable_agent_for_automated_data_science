{
    "meta_info": {
        "title": "Focal Sparse Convolutional Networks for 3D Object Detection",
        "abstract": "Non-uniformed 3D sparse data, e.g., point clouds or voxels in different\nspatial positions, make contribution to the task of 3D object detection in\ndifferent ways. Existing basic components in sparse convolutional networks\n(Sparse CNNs) process all sparse data, regardless of regular or submanifold\nsparse convolution. In this paper, we introduce two new modules to enhance the\ncapability of Sparse CNNs, both are based on making feature sparsity learnable\nwith position-wise importance prediction. They are focal sparse convolution\n(Focals Conv) and its multi-modal variant of focal sparse convolution with\nfusion, or Focals Conv-F for short. The new modules can readily substitute\ntheir plain counterparts in existing Sparse CNNs and be jointly trained in an\nend-to-end fashion. For the first time, we show that spatially learnable\nsparsity in sparse convolution is essential for sophisticated 3D object\ndetection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks\nvalidate the effectiveness of our approach. Without bells and whistles, our\nresults outperform all existing single-model entries on the nuScenes test\nbenchmark at the paper submission time. Code and models are at\nhttps://github.com/dvlab-research/FocalsConv.",
        "author": "Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, Jiaya Jia",
        "link": "http://arxiv.org/abs/2204.12463v1",
        "category": [
            "cs.CV",
            "cs.LG"
        ],
        "additionl_info": "CVPR 2022 Oral. Code is at  http://github.com/dvlab-research/FocalsConv"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n% first paragraph\n% Key challenge for 3D object detection -- unstructured data\n% Voxel-based methods (Sparse, outdoor, autonomous driving), Point based methods.\nA key challenge in 3D object detection is to learn effective representations from the unstructured and sparse 3D geometric data such as point clouds. In general, there are two ways for this job. The first is to process point clouds~\\cite{point-rcnn, 3dssd} directly, based on PointNet++~\\cite{pointnet++} networks. However, the neighbour sampling and grouping operations are time-consuming. This makes it improper for large-scale autonomous driving scenes that require real-time efficiency. The The second is to convert point clouds into voxelizations and apply 3D sparse convolutional neural networks~(Sparse CNNs) for feature extraction~\\cite{pvrcnn,voxel-rcnn}. 3D Sparse CNNs resemble 2D CNNs in structures, including several feature stages and down-sampling operations. They typically consist of {\\em regular} and {\\em submanifold} sparse convolutions~\\cite{submanifold-sparse-conv-v2}.\n\n% second paragraph\n% Drawbacks in the current sparse convolution\nAlthough regular and submanifold sparse convolutions have been widely used, they have respective limitations. \nRegular sparse convolution dilates all sparse features. It inevitably burdens models with considerable computations. That is why backbone networks commonly limit its usage only in down-sampling layers~\\cite{second,pvrcnn}. In addition, detectors aim to distinguish target objects from massive background features. But regular sparse convolution reduces the sparsity sharply and blurs feature distinctions. \n\nOn the other hand, submanifold sparse convolutions avoid the computation issue by restricting the output feature positions to the input. But it misses necessary information flow, especially for the spatially disconnected features. The above issues on regular and submanifold sparse convolutions limit Sparse CNNs to achieve high representation capability and efficiency. We illustrate the submanifold and regular sparse convolutional operations in Fig.~\\ref{fig:illustrate23d}.\n\n\n\n\n% third paragraph\n% Analyze the reasons behind these drawbacks. the design of the operations influenced by 2D CNN, whose inputs are structured.\nThese limitations originate from the conventional convolution pattern: all input features are treated equally in the convolution process. It is natural for 2D CNNs, and yet is improper for 3D sparse features. 2D convolution is designed for structured data. All pixels in the same layer typically share receptive field sizes. But 3D sparse data is with varying sparsity and importance in space. It is not optimal to handle non-uniform data with uniform treatment. In terms of {\\em sparsity}, upon the distance to LIDAR sensors, objects present large sparsity variance. \nIn terms of {\\em importance}, the contribution of features varies with different locations for 3D object detection, {\\em e.g.}, foreground or background. \nAlthough 3D object detection is achieved~\\cite{point-rcnn, pvrcnn, voxel-rcnn, centerpoint}, state-of-the-art methods still rely on RoI (region-of-interest) feature extraction. It corresponds to the idea that we should shoot arrows at the target in the feature extraction of 3D detectors.\n\n% fourth paragraph\n% In this paper, we propose our Focal Sparse Convolution\n% Multi-modal focal sparse conv, why it is feasible in image-involved methods.\nIn this paper, we propose a general format of sparse convolution by relaxing the conceptual difference between regular and submanifold ones. We introduce two new modules that improve the representation capacity of Sparse CNNs for 3D object detection. \nThe first is focal sparse convolution~({\\em \\Ours}). It predicts {\\em cubic importance} maps for the output pattern of convolutions. Features predicted as {\\em important} ones are dilated into a {\\em deformable} output shape, as shown in Fig~\\ref{fig:illustrate23d}. The importance is learned via an additional convolutional layer, dynamically conditioned on the input features. This module increases the ratio of valuable information among all features. \nThe second is its multi-modal improved version of Focal sparse Convolution with Fusion~(named as {\\em \\OursF}). Upon the LIDAR-only {\\em \\Ours}, we enhance importance prediction with RGB features fused, as image features typically contain rich appearance information and large receptive fields.\n\n% fifth paragraph\n% Attributes of them proposed modules\nThe proposed modules are novel in two aspects. \nFirst, {\\em \\Ours} presents a dynamic mechanism for learning spatial sparsity of features. It makes the learning process concentrated on the more valuable foreground data. With the down-sampling operations, valuable information increases in stages. Meanwhile, the large amount of background voxels are removed. Fig.~\\ref{fig:sparsity_comparison_3pairs} illustrates the learnable feature sparsity, including the common, crowded, and remote objects, where {\\em \\Ours} enriches the learned voxel features on the foreground without redundant voxels added in other areas.\nSecond, both modules are lightweight. The importance prediction involves small overhead parameters and computation, as measured in Tab.~\\ref{tab:improvements-pvrcnn}. \nThe RGB feature extraction of {\\em \\OursF} involves only {\\em several layers}, instead of heady 2D detection or segmentation models~\\cite{pointpainting}. \n\n% sixth paragraph\n% Introduce the experimental results.\n\nThe proposed modules of {\\em \\Ours} and {\\em \\OursF} can readily replace their original counterparts in sparse CNNs. To demonstrate the effectiveness, we build the backbone networks on existing 3D object detection frameworks~\\cite{pvrcnn, voxel-rcnn, centerpoint}. Our method enables non-trivial enhancement with small model complexity overhead on both the KITTI~\\cite{kitti} and nuScenes~\\cite{nuscenes} benchmarks. These results manifest that learnable sparsity with focal points is essential. \nWithout bells and whistles, our approach outperforms state-of-the-art ones on the nuScenes {\\em test} split~\\cite{nuscenes}. \n\n% sixth paragraph\n% sumarize the contributions.\nConvolutional dynamic mechanism adapts the operations conditioned on input data, {\\em e.g.}, deformable convolutions~\\cite{deformableconv, deformableconvv2} and dynamic convolutions~\\cite{condconv, dynamicconv}. %They all adapt the convolution operation to be conditioned on input data. \nThe key difference is that our approach makes use of the {\\em intrinsic sparsity} of data. It promotes feature learning to be concentrated on more valuable information.\nWe deem the non-uniform property as a great benefit.  We discuss the relations and differences to previous literature in Sec.~\\ref{sec:related_work}.\n\n\n%------------------------------------------------------------------------\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec:related_work}\n",
                "subsection 2.1": {
                    "name": "Convolutional Dynamic Mechanism",
                    "content": "\nDynamic mechanisms have been widely studied in CNNs, due to their advantages of high accuracy and easy adaption in scenarios. We discuss two kinds of related methods, i.e., %kernel weight attention~\\cite{condconv, dynamicconv, pixel-adaptive-conv},\nkernel shape adaption~\\cite{deformableconv, deformableconvv2, kpconv}, and input attention mask~\\cite{dynamicconv-fasterinference, spatialsampling, sbnet}.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Kernel shape adaption.}\nKernel shape adaption methods~\\cite{deformableconv, deformableconvv2, deformablekernels,minkowskinet} adapt the effective receptive fields of networks. Deformable convolution~\\cite{deformableconv} predicts offsets for feature sampling. Its variant~\\cite{deformableconvv2} introduces an additional attention mask to modulate features. %Deformable kernel~\\cite{deformablekernels} shows better robustness to geometric transformation. \nFor 3D feature learning, KPConv~\\cite{kpconv} learns local offsets for kernel points. MinkowskiNet~\\cite{minkowskinet} generalizes sparse convolution to arbitrary kernel shape. Overall, these methods modify the input feature sampling process. \n\nDeformable PV-RCNN~\\cite{deformable-pvrcnn} applies offset prediction for feature sampling in 3D object detection. In contrast, focal sparse convolution improves the {output} feature spatial sparsity and makes it learned, helpful for 3D object detection. \n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Attention mask on input.}\nMethods of~\\cite{spatialsampling, dynamicconv-fasterinference, pac, eq-paradigm} seek spatial-wise sparsity for efficient inference. These methods receive dense images and prune unimportant pixels based on attention masks. \nThese methods aim to sparsify dense data while we make use of {intrinsic data sparsity}. %PAC~\\cite{pac} targets at adaptive kernel weights instead of pruning.\nAlthough SBNet~\\cite{sbnet} also utilizes the sparse property, it limits application to 2D BEV (bird-eye-views) images, and shares the static masks over all layers in the network. In contrast, our improved convolution is more adaptive and is applicable to related tasks, e.g., 3D instance segmentation~\\cite{icm-3d}.\n\n\n\n\n% 3D Object Detection\n\n"
                },
                "subsection 2.2": {
                    "name": "3D Object Detection",
                    "content": "\n% Point Cloud\n\\noindent\n\\textbf{LIDAR-only detectors.}\n%3D object detection~\\cite{second, pointpillars, votenet, pvrcnn} aims to predict 3D rotated bounding boxes and corresponding categories. \n3D object detection frameworks usually resemble 2D detectors, {\\em e.g.}, the R-CNN family~\\cite{point-rcnn, voxel-rcnn, pvrcnn, pyramid-rcnn} and the SSD family~\\cite{3dssd, sessd, sassd, cia-ssd}. \nThe main difference on 2D detectors lies in input encoders.\nVoxelNet~\\cite{voxelnet} encodes voxel features using PointNet~\\cite{pointnet} and applies a RPN~(region proposal network)~\\cite{fasterrcnn}. SECOND~\\cite{second} uses accelerated sparse convolutions and improves efficiency from VoxelNet~\\cite{voxelnet}. VoTr~\\cite{voxeltransformer} applies transformer architectures to voxels. Various detectors~\\cite{pvrcnn, voxel-rcnn, centerpoint} have been presented based on feature encoders. We validate the proposed approach on backbones of frameworks of~\\cite{pvrcnn,voxel-rcnn,centerpoint} on multiple datasets~\\cite{kitti,nuscenes,waymo}.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Completion-based detectors.}\n% Point Completion\nCompletion-based methods~\\cite{pcrgnn, sienet, spg, gsdn} form another line of efforts in enriching foreground information. We focus on feature learning instead of point completion.\nPC-RGNN~\\cite{pcrgnn} has a point completion module by a graph neural network. SIENet~\\cite{sienet} builds upon PCN~\\cite{pcn} for point completion in a two-stage framework. The completion process relies on the prior generated proposals. GSDN~\\cite{gsdn} expands all features first through transposed convolutions and then by pruning. SPG~\\cite{spg} designs a semantic point generation module for domain adaption 3D object detection. It is applied during data preprocessing, complicating the detection pipelines. \n%In contrast, our end-to-end approach makes effects in the input encoding process and is more economical. \n\n\n% Cross Modal\n\\vspace{0.5em}\n\\noindent\n\\textbf{Multi modal fusion.}\n%Representative multi-modal fusion methods~\\cite{3dcvf, deepfusion, pointaugmenting, pointpainting, mutliview3ddet} in 3D object detection can roughly be categorized in to region-level~\\cite{mutliview3ddet, joint3dproposal} and pixel-level fusion methods~\\cite{mvxnet, epnet}.\nMulti-modal fusion methods~\\cite{3dcvf, deepfusion, epnet} use more information than LIDAR-only ones.\nThe KITTI~\\cite{kitti} benchmark had been dominated by LIDAR-only methods until PointPainting~\\cite{pointpainting} was proposed. It decorates raw point clouds with the corresponding image segmentation scores. PointAugmenting~\\cite{pointaugmenting} further replaces the segmentation model with an 2D object detection one~\\cite{centernet}. They are both decoration-based methods, which require image feature extraction on off-the-shelf 2D networks, before feeding into 3D detectors. Although promising results are achieved by these methods, the overall inference pipelines are complicated.\nOur multi-modal focal sparse convolution differs from the above methods in two aspects. First, we only require several jointly trained layers for image feature extraction, rather than the heavy segmentation or detection models. Second, we only strengthen the predicted {\\em important} features, instead of the uniform decoration~\\cite{pointpainting,pointaugmenting} for all LIDAR features. \n\n\n%------------------------------------------------------------------------\n"
                }
            },
            "section 3": {
                "name": "Focal Sparse Convolutional Networks",
                "content": "\n\\label{sec:focal-sparse-convnet}\n\nIn this section, we first review the formulation of sparse convolution in Sec.~\\ref{sec:review-sparse-conv}. Then, the proposed focal sparse convolution and its multi-modal extension will be elaborated in Sec.~\\ref{sec:focal-sparse-conv} and Sec.~\\ref{sec:multi-modal-extension}. We finally introduce the resulting focal sparse convolutional networks in Sec.~\\ref{sec:convnetwork}.\n\n",
                "subsection 3.1": {
                    "name": "Review of Sparse Convolution",
                    "content": "\n\\label{sec:review-sparse-conv}\nGiven an input feature $\\mathrm{x}_p$ with number of $c_{\\mathrm{in}}$ channels at position $p$ in the $d$ dimensional spatial space, we process this feature by a convolution with kernel weights $\\mathrm{w}\\in \\mathbb{R}^{\\mathnormal{K}^d\\times c_{\\mathrm{in}} \\times c_{\\mathrm{out}}}$. For example, in the 3D coordinate space, $\\mathrm{w}$ contains $c_{\\mathrm{in}} \\times c_{\\mathrm{out}}$ spatial kernels with size 3 and $|\\mathnormal{K}^d|=3^3$. The convolution process is represented as\n\\begin{equation}\n    \\mathrm{y}_p=\\sum_{k\\in \\mathnormal{K}^d} \\mathrm{w}_k \\cdot \\mathrm{x}_{\\bar{p}_k},\n\\end{equation}\nwhere $k$ enumerates all discrete locations in the kernel space $\\mathnormal{K}^d$. $\\bar{p}_k=p+k$ is the corresponding location around center $p$, where $k$ is an offset distance from $p$.\n\nThis formulation accommodates most types of convolutions with simple modifications. When $p\\in \\mathbb{Z}$, the common convolution for dense input data is yielded. When $\\bar{p}_k$ is added with a learned offset $\\Delta \\bar{p}_k$, it includes the kernel shape adaption methods, {\\em e.g.}, deformable convolutions~\\cite{deformableconv, deformableconvv2}. \nFurther, if $\\mathrm{W}$ equals to a weighted sum $\\sum\\limits\\alpha_i \\mathrm{W}^i$, it generalizes to weight attention, {\\em e.g.}, dynamic convolution~\\cite{condconv,dynamicconv}. Finally, when attention masks are multiplied to the input feature map $\\mathrm{x}$, this formulation makes input attention mask methods~\\cite{sbnet, spatialsampling}.\n\nFor sparse input data, the feature position $p$ does not belong to the dense discrete space $\\mathbb{Z}$. The input and output feature spatial space is relaxed to $P_{\\mathrm{in}}$ and $P_{\\mathrm{out}}$, respectively. The formulation is converted to\n\\begin{equation}\n    \\mathrm{y}_{p\\in P_{\\mathrm{out}}}=\\sum_{k\\in \\mathnormal{K}^d(p, P_{\\mathrm{in}})} \\mathrm{w}_k \\cdot \\mathrm{x}_{\\bar{p}_k},\n    \\label{eq:sparse-conv}\n\\end{equation}\nwhere $\\mathnormal{K}^d(p, P_{\\mathrm{in}})$ is a subset of $\\mathnormal{K}^d$, leaving out the empty position. It is conditioned on the position $p$ and input feature space $P_{\\mathrm{in}}$ as\n\\begin{equation}\n    \\mathnormal{K}^d(p, P_{\\mathrm{in}})=\\{k\\,|\\, p+k\\in P_{\\mathrm{in}},k\\in\\mathnormal{K}^d\\}.\n\\end{equation}\nIf $P_{\\mathrm{out}}$ includes a union of all dilated positions around $P_{\\mathrm{in}}$ within $\\mathnormal{K}^d$ neighbours, this process is formulated as \n\\begin{equation}\n    P_{\\mathrm{out}}=\\bigcup_{p\\in P_{\\mathrm{in}}} P(p, \\mathnormal{K}^d),\n\\end{equation}\nwhere\n\\begin{equation}\n    P(p,{\\mathnormal{K}^d})=\\{p+k\\,|\\,k\\in\\mathnormal{K}^d\\}.\n    \\label{eq:output-positions}\n\\end{equation}\nOn this condition, the formulation becomes regular sparse convolution. It acts at all positions where any voxels exist in its kernel space. It does not skip any information gathering in the total spatial space. \n\nThis strategy involves two drawbacks. (\\romannumeral1) It introduces considerable computation cost. The number of sparse features is doubled or even tripled, increasing burden for following layers. (\\romannumeral2) We empirically find that continuously increasing the number of sparse features may harm 3D object detection~(Tab.~\\ref{tab:ablation-dynamicoutput}). Crowded and unpromising candidate features may blur the valuable information. It degrades foreground features and further declines the feature discrimination capacity of 3D object detectors. \n\nWhen $P_{\\mathrm{in}}=P_{\\mathrm{out}}$, submanifold sparse convolution~\\cite{submanifold-sparse-conv-v2} is yielded. It happens only when the kernel centers locate at the input, restricting the active positions to input sets. This setting avoids the computation burden, but abandons necessary information flow between disconnected features. Note that the flow is common in the irregular point cloud data. Thus, effective receptive field sizes are constrained by the feature disconnection, which degrade the model capability.\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Focal Sparse Convolution",
                    "content": "\n\\label{sec:focal-sparse-conv}\n% formulation\nRegardless of regular or submanifold sparse convolution, output positions $P_{\\mathrm{out}}$ are static across all $p\\in P_{\\mathrm{in}}$, which is undesirable.\nIn contrast, we perform adaptive determination of sparsity or receptive field sizes in a fine-grained manner. \nWe relax output positions $P_{\\mathrm{out}}$ to be dynamically determined by the sparse features. We illustrate this proposed process in Fig.~\\ref{fig:pipeline} (via solid lines).\n\nIn our formulation, output positions $P_{\\mathrm{out}}$ generalize to a union of all important positions with their dilated area and other unimportant positions. The dilated areas are deformable and dynamic to input positions. Eq.~\\eqref{eq:output-positions} becomes\n\\begin{equation}\n    P_{\\mathrm{out}} = \\left(\\bigcup_{p\\in P_{\\mathrm{im}}} P(p,{\\mathnormal{K}_{\\mathrm{im}}^d(p)})\\right)\\cup P_{\\mathrm{in}/\\mathrm{im}}.\n\\end{equation}\nWe factorize this process into three steps: (\\romannumeral1) {\\em cubic importance prediction}, (\\romannumeral2) {\\em important input selection}, and (\\romannumeral3) {\\em dynamic output shape generation}. \n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Cubic importance prediction.}\nA cubic importance map $I^p$ involves importance for candidate output features around the input feature at position $p$. Each cubic importance map shares the same shape $\\mathnormal{K}^d$ with the main processing convolution kernel weight, $e.g.$, $k^3=3\\times3\\times3$ with the kernel size 3. It is predicted by an additional submanifold sparse convolution with a sigmoid function. The latter steps depend on the predicted cubic importance maps.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Important input selection.}\nIn Eq.~\\eqref{eq:output-positions}, $P_{\\mathrm{im}}$ is a subset of $P_{\\mathrm{in}}$. It contains the positions of relatively important input features. We select $P_{\\mathrm{im}}$ as\n\\begin{equation}\n    P_{\\mathrm{im}} = \\{p\\,|\\, I^p_{0}\\geq\\tau,p\\in P_{\\mathrm{in}}\\},\n    \\label{eq:p_im}\n\\end{equation}\nwhere $I^p_{0}$ is the {\\em center} of the cubic importance map at position $p$. And $\\tau$ is a pre-defined threshold (Tab.~\\ref{tab:ablation-importancesampling} and \\ref{tab:importance-threshold}). Our formulation becomes the regular or submanifold sparse convolution when $\\tau$ is 0 or 1 respectively. We also find that using top-k ratio to select is an alternative of threshold.\n\n% importance maps\n\\vspace{0.5em}\n\\noindent\n\\textbf{Dynamic output shape generation.}\nFeatures in $P_{\\mathrm{im}}$ is dilated to a dynamic shape. The output around $p$ is determined by the dynamic output shape $\\mathnormal{K}_{\\mathrm{im}}^d(p)$. Note that our deformable output shapes are pruned inside the original dilation without offsets. It is computed similarly to Eq.~\\eqref{eq:p_im} as\n\\begin{equation}\n    \\mathnormal{K}_{\\mathrm{im}}^d(p) = \\{k\\,|\\, p+k\\in P_{\\mathrm{in}}, I^p_{k}\\geq\\tau, k\\in\\mathnormal{K}^d\\}.\n\\end{equation}\nWe analyze the dynamic output shape in Tab.~\\ref{tab:ablation-dynamicoutput}.\nFor the remaining unimportant features, their output positions are fixed as input, {\\em i.e.}, submanifold. We found that directly removing them or using a fully dynamic manner without preserving them makes the training process unstable.\n\n% supervision\n\\vspace{0.5em}\n\\noindent\n\\textbf{Supervision manners.}\n\\label{par:supervision-manner}\nIn 3D object detection, we have a prior knowledge that foreground objects are more valuable information. Based on this prior, we apply focal loss~\\cite{focalloss} as an objective loss function to supervise the importance prediction. We construct the objective targets for the centers of feature voxels inside 3D ground-truth boxes. We keep its loss weight as 1 for the generality of our modules.\n\nAdditional supervision comes from multiplying the predicted cubic importance maps to output features as attention. It makes the importance prediction branch differentiable naturally. It shares motivation with the kernel weight sparsification methods~\\cite{sparsecnn} in the area of model compression.\nWe empirically show that this attention manner benefits the performance for minor classes, {\\em e.g.}, Pedestrian and Cyclist on KITTI (investigated in Tab.~\\ref{tab:ablation-objectiveloss}).\n\n"
                },
                "subsection 3.3": {
                    "name": "Fusion Focal Sparse Convolution",
                    "content": "\n\\label{sec:multi-modal-extension}\nWe provide a multi-modal version of focal sparse convolution, as illustrated in Fig.~\\ref{fig:pipeline} (via dashed lines). This extension is conceptually simple but effective. We extract RGB features from images and align LIDAR features to them. The extracted features are fused to input and {\\em important} output sparse features in focal sparse convolution.\n\n% RGB feature extraction\n\\vspace{0.5em}\n\\noindent\n\\textbf{Feature extraction.}\nThe fusion module is lightweight. It contains a conv-bn-relu layer and a max-pooling layer. It down-samples the input image to 1/4 resolutions. It is followed by {\\em 3 conv-bn-relu layers} with residual connection~\\cite{resnet}. The channel number is then reduced to be consistent with that of sparse features, with an MLP layer. This facilitates a simple summation of multi-modal features.\n\n\n% Sparse feature alignment\n\\vspace{0.5em}\n\\noindent\n\\textbf{Feature alignment.}\nA common issue during fusion is misalignment in the 3D-to-2D projection. Point cloud data is commonly processed by transformation and augmentation. Transformations include flip, re-scale, rotation, translation. The typical augmentation is ground-truth sampling~\\cite{second}, copying paste objects from other scenes. %These introduce a misalignment along 3D to 2D projection. \nFor these invertible transformations, we reverse the coordinates of sparse features with the recorded transformation parameters~\\cite{pointaugmenting,moca}. For ground-truth sampling, we copy the corresponding 2D objects onto images. Rather than using an additional segmentation model or mask annotations~\\cite{moca}, we directly crop objects in bounding boxes for simplification.\n\n% Multi modal feature fusion\n\\vspace{0.5em}\n\\noindent\n\\textbf{Fusion manners.}\nThe aligned RGB features are directly fused to sparse features in {\\em summation}, as they share the same channel numbers. Although other fusion methods, $e.g.$, concatenation or cross-attention, can be used, we choose the most concise summation for efficiency. The aligned RGB features are fused with sparse features twice in this module. It is first fused to input features for cubic importance prediction. Then we fuse RGB features only to {\\em important} output sparse features, {\\em i.e.}, the first part in Eq.~\\eqref{eq:output-positions}, instead of all of them~(investigated in Tab.~\\ref{tab:improvements-multimodal-stage}).\n\nOverall, the multi-modal layers are lightweight in terms of parameters and fusion strtegies. They are jointly trained with detectors. It provides an efficient and economical solution for the fusion module in 3D object detection.\n\n"
                },
                "subsection 3.4": {
                    "name": "Focal Sparse Convolutional Networks",
                    "content": "\n\\label{sec:convnetwork}\nBoth focal sparse convolution and its multi-modal extension can readily replace their counterparts in the backbone networks of 3D detectors. During training, we do not use any special initialization or learning rate settings for the introduced modules. The importance prediction branch is trained via back-propagation through the attention multiplication and objective loss function as introduced in Sec.~\\ref{par:supervision-manner}.\n\nThe backbone networks in 3D object detectors~\\cite{pvrcnn,voxel-rcnn,centerpoint} typically consist of one stem layer and 4 stages. Each stage, except the first one, includes a regular sparse convolution with down-sampling and two submanifold blocks. In the first stage, there are one~\\cite{pvrcnn,voxel-rcnn} or two~\\cite{centerpoint} sparse convolutional layers. By default, each sparse convolution is followed by batch normalization~\\cite{batchnorm} and ReLU activation. \n\nWe validate focal sparse convolution on the backbone networks of existing 3D detectors~\\cite{pvrcnn, voxel-rcnn, centerpoint}. \nWe directly apply focal sparse convolution at the last layer of certain stages. We analyze the stages for using our focal sparse convolution in experiments~(ablated in Tab.~\\ref{tab:ablation-usingstages} and \\ref{tab:improvements-multimodal-stage}).\n\n%------------------------------------------------------------------------\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:experiments}\nWe conduct ablations and comparisons for {\\em \\Ours} and its multi-modal variant. More experiments, such as results on Waymo~\\cite{waymo}, are in the {supplementary material}.\n",
                "subsection 4.1": {
                    "name": "Setup and Implementation",
                    "content": "\n\\noindent\n\\textbf{KITTI.}\nThe KITTI dataset~\\cite{kitti} consists of 7,481 samples and 7,518 testing samples. The training samples are split into a {\\em train} set with 3,717 samples and a {\\em val} set with 3,769 samples. Models are commonly evaluated in terms of the mean Average Precision~(mAP) metric.\n%with the 0.7 IoU threshold for the car class. \nmAP is calculated with recall 40 positions~(R40). \nWe perform ablation studies with AP$_{\\textrm{3D}}$~(R40) on the {\\em val} split.\nWe conduct main comparisons with AP$_{\\textrm{3D}}$~(R40) on {\\em test} split and AP$_{\\textrm{3D}}$~(R11) on the {\\em val} split. For the optional multi-modal settings, RGB features are extracted from single front-view for fusion.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{nuScenes.}\nThe nuScenes~\\cite{nuscenes} is a large-scale dataset, which contains 1,000 driving sequences in total. It is split into 700 scenes for training, 150 scenes for validation, and 150 scenes for testing. %nuScenes provides a rich data modality. \nIt is collected using a 32-beam synced LIDAR and 6 cameras with the complete 360$^{\\mathrm{o}}$ environment coverage. \n%Following the settings in our baseline method, CenterPoint~\\cite{centerpoint}, we set the detection range to [-54m,~54m] for $X$ and $Y$ axes, and [-5m,~3m] for $Z$ axis, where the voxel size is set as (0.075m,~0.075m,~0.2m) for the corresponding axes. \nIn evaluation, the main metrics are mAP and nuScenes detection score~(NDS). \n%We report the metrics mAP, NDS, and AP for all 10 classes. \nIn terms of multi-modal experiments, we use images of 6 views for fusion. For ablation study, models are trained on $\\frac{1}{4}$ training data and evaluated on the entire validation set, {\\em i.e.}, nuScenes $\\frac{1}{4}$ split.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Implementation details.}\nIn experiments, we validate our modules on state-of-the-art frameworks of PV-RCNN~\\cite{pvrcnn}, Voxel R-CNN~\\cite{voxel-rcnn} on KITTI~\\cite{kitti}, and CenterPoint~\\cite{centerpoint} on nuScenes~\\cite{nuscenes}. In LIDAR-only experiments, we apply \\Ours in the first three stages of backbone networks. In multi-modal cases, we apply \\OursF only in the first stage of the backbone network, for affordable memory and inference cost. We set the importance threshold $\\tau$ to 0.5.\nWe keep other settings intact. More experimental details are provided in the {supplementary material}.\n%({\\em e.g.}, learning rate, optimizer, and others hyperparamters). %These details can be found in the {\\em supplementary material}.\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Ablation Studies",
                    "content": "\n\\noindent\n\\textbf{Improvements on KITTI.}\nWe first evaluate our methods over PV-RCNN~\\cite{pvrcnn} in Tab.~\\ref{tab:improvements-pvrcnn}, as it is a high-performance, multi-class, and open-sourced framework. In Tab.~\\ref{tab:improvements-pvrcnn}, \nthe 1st and 2nd lines show the reported results~\\cite{pvrcnn} and results tested from the released model. We take the latter as the baseline. Focal~S-Conv and \\OursF achieve non-trivial improvement over this strong baseline. %Our multi-modal approach further improves this strong baseline.\n\n\n\n\n\n\n\n\n%\\vspace{0.5em}\n\\noindent\n\\textbf{Dynamic output shape.}\nIn \\Ours, the output shape from every single voxel is dynamically determined by the predicted importance maps. We ablate this by fixing output shapes as regular dilation, without any other change. Tab.~\\ref{tab:ablation-dynamicoutput} shows that dilating all sparse features is harmful. It dramatically increases the number of unpromising voxel features.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Importance sampling.}\n% gains importance sampling, replace it with random sampling (KITTI) [Done]\n\\Ours selects sparse features that need dilation with predicted importance. To ablate this module, we replace the importance selection (the {important input selection} step) with a random sample in Tab.~\\ref{tab:ablation-importancesampling} without other changes. It shows that large performance drop occurs without the guidance of importance. This validates that the importance prediction is necessary.\n\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Supervision setting.}\n% gains from loss function (KITTI) [Done]\nThe additional branch in \\Ours is supervised by both attention multiplication and the objective loss. We ablate them in Tab.~\\ref{tab:ablation-objectiveloss}. Only using objective loss supervision is enough to ensure performance on {\\em Car}. However, its performance on minor classes, {\\em Ped.} and {\\em Cyc.}, is not optimal. Attention multiplication is beneficial to {\\em Ped.} and {\\em Cyc}. We assume that minor classes cannot get balanced supervision from the objective loss like the long-tailed distribution. In contrast, attention multiplication is object-agnostic, relaxing the imbalance to some degree.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Stages for using focal sparse convolution.}\n% number of focal sparse convolution to use (KITTI) [Done]\nTab.~\\ref{tab:ablation-usingstages} shows results of using \\Ours in different numbers of stages. (1) Applying \\Ours in the first stage, which already obtains clear improvement. The performance enhances as the used stage increases until all stages are involved. Since \\Ours adjusts output sparsity, it is reasonable to be used in early stages that make effects on subsequent feature learning. The spatial feature space in the last stage is down-sampled to a very limited size, which might not be large enough for sparsity adaptation. Empirically, usage in the last layer of the first three stages is the best choice. It is thus used as the default setting in our experiments. \n\n\n\n\n\n\n\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Importance threshold.} We ablate the importance threshold $\\tau$ used in \\Ours in Tab.~\\ref{tab:importance-threshold}. We run experiments with this value ranging from 0.1 to 0.9 and interval 0.2, without other change of settings. The accuracy AP$_{3D}$ (R40) on {\\em Car} serves as the metric in this ablation.\nThe performance is stable as the threshold value $\\tau$ varies. \n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Improvements over multi-modal baseline on nuScenes.}\n% further gains from multi-modal focal sparse convolution (KITTI + 1/4 nuScenes)\nWe evaluate our multi-modal \\Ours on the nuScenes~\\cite{nuscenes} 1/4 dataset. More improvement is presented in Tab.~\\ref{tab:improvements-multimodal-1/4}. We build a multi-modal CenterPoint baseline by fusing image features to the same fusion layer used in our methods, with the same fusion and feature extraction layers. This multi-modal CenterPoint enhances the LIDAR-only baseline from 56.1\\% to 59.0\\% mAP. \\OursF improves to 61.7\\% mAP on this strong baseline.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Use stages and fusion scope for \\OursF.}\n% positions to use multi-modal focal sparse convolution (1/4 nuScenes) [Done]\nWe ablate the usage stages and fusion scope for \\OursF in Tab.~\\ref{tab:improvements-multimodal-stage}. Fusion scope is the scope of sparse features to fuse with RGB features at the output of \\OursF. It shows that fusion in the early stages is beneficial, and becomes adverse in the last two stages. {\\em Imp.} means only fusing onto important output features (judged by importance maps). When fusing in the first stage, it is better to fuse on {\\em important} features, instead of all of them, making representation discriminative.\n\n\n% visualization on both images and bev (nuScenes) [TODO]\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Model complexity and runtime.}\nWe report the model complexity and runtime comparisons in Tab.~\\ref{tab:improvements-pvrcnn} and \\ref{tab:improvements-multimodal-1/4}. The runtimes are evaluated on the same GPU machine. \\Ours and its multi-modal variant only add a small overhead to model parameters and computation, on KITTI~\\cite{kitti}. This indicates that the performance improvement comes from the model capacity of sparsity learning, instead of increasing model sizes. On nuScenes~\\cite{nuscenes}, the overall runtime rises from 93 ms to 159 ms. But parameters are still limited. It is a common limitation in multi-view fusion methods. The multi-modal baseline also requires 145 ms. The reason is that there are 6-view images to process per frame. \n\n"
                },
                "subsection 4.3": {
                    "name": "Main Results",
                    "content": "\n% KITTI and nuScenes, LIDAR-only and multi-modal\n\n%\\vspace{0.5em}\n\\noindent\n\\textbf{KITTI.}\n% KITTI test AP R40, val AP R11\nWe compare our \\Ours modules upon Voxel R-CNN~\\cite{voxel-rcnn} with previous state-of-the-art methods on both the KITTI {\\em test} and {\\em val} split. In Tab.~\\ref{tab:kitti-test}, we compare with both LIDAR-only and multi-modal methods. The original Voxel R-CNN~\\cite{voxel-rcnn} is comparable to PV-RCNN~\\cite{pvrcnn} and is inferior to Pyramid-PV~\\cite{pyramid-rcnn} and VoTr-TSD~\\cite{voxeltransformer}. \\Ours improves it to surpass these two new methods. Using \\OursF, the multi-modal Voxel R-CNN achieves 82.28\\% AP$_{3D}$ on the KITTI {\\em test} split. Tab.~\\ref{tab:kitti-val} shows comparisons on KITTI {\\em val} split in AP$_{3D}$ in recall 11 positions. \n%Voxel R-CNN~\\cite{voxel-rcnn} already achieves state-of-the-art performance.\n\\Ours and \\OursF enhance this leading result to 84.93\\% and 85.22\\% respectively in {\\em Car} class. \n\n\\vspace{0.5em}\n\\noindent\n\\textbf{nuScenes.}\n% nuScenes test [TODO]\nOn the nuScenes dataset, we evaluate our models on the test server and compare them with both LIDAR-only and multi-modal methods, as in Tab.~\\ref{tab:nuscenes-test}. \\Ours improves CenterPoint~\\cite{centerpoint} by a large margin to 63.8\\% mAP. Multi-modal methods present much better performance than LIDAR-only methods on the nuScenes dataset. \n%PointAugmenting~\\cite{pointaugmenting}, also based on CenterPoint~\\cite{centerpoint}, achieves previous state-of-the-art performance, while it requires a 2D detection model to enrich raw LIDAR data.\nCenterPoint v2$^{\\star}$ includes PointPainting~\\cite{pointpainting}, Cascade R-CNN~\\cite{cascade-rcnn} instance segmentation models pre-trained on nuImages, and five-model ensembling.\nAs the testing augmentations are not unified or stated in previous methods, we provide two results of our final model.\n\\OursF achieves 67.8\\% mAP and 71.8\\% mAP without any ensembling or testing augmentation. \\OursF$^{\\ddagger}$ further achieves 70.1\\% mAP and 73.6\\% NDS with test-time augmentations~\\cite{centerpoint}. Both results outperform previous methods.\n\n%------------------------------------------------------------------------\n"
                }
            },
            "section 5": {
                "name": "Conclusion and Discussion",
                "content": "\n\\label{sec:conclusion}\nThis paper presents a focal sparse convolution and a multi-modal extension, which are simple and effective. They are end-to-end solutions for LIDAR-only and multi-modal 3D object detection. For the first time, we show that the learned sparsity with focal points is essential for 3D object detectors. Notably, focal and fusion sparse CNNs achieve leading performance on the large-scale nuScenes.\n\n\\vspace{0.35em}\n\\noindent\n\\textbf{Limitations.}\nIn the multi-modal 3D detection that requires multiple views, {\\em e.g.}, 6 high-resolution images per frame in nuScenes~\\cite{nuscenes}, computation cost increases, although the image branch is already largely simplified.\n\n\\noindent\n\\textbf{Boarder Impacts.}\nThe proposed method replies on the sparsity of data distribution. It might reflect biases in data collection, including the ones of negative societal impacts.\n\n%\\vspace{0.35em}\n\\noindent\n\\textbf{Acknowledgements.} This work is in part supported by The National Key Research and Development Program of China (No. 2017YFA0700800) and Beijing Academy of Artificial Intelligence (BAAI).\n\n%%%%%%%%% REFERENCES\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egbib}\n}\n\n\n\\appendix\n\\captionsetup[table]{labelformat={default},labelsep=period,name={Table S -}}\n\\captionsetup[figure]{labelformat={default},labelsep=period,name={Figure S -}}\n\n%%%%%%%%% TITLE - PLEASE UPDATE\n%\\title{Focal Sparse Convolutional Networks for 3D Object Detection \\\\\n%{\\em Supplementary Material}}\n"
            },
            "section 6": {
                "name": "\\Large Appendix",
                "content": "\n\n%%%%%%%%% BODY TEXT\n"
            },
            "section 7": {
                "name": "More Implementation Details",
                "content": "\n\\label{sec:training-details}\nOur implementation is based on the open-sourced OpenPCDet~\\cite{pvrcnn, voxel-rcnn}, and the released code of CenterPoint~\\cite{centerpoint}.\n\n",
                "subsection 7.1": {
                    "name": "Voxelization",
                    "content": "\n\\noindent\n\\textbf{KITTI.}\nThe 3D object detectors in this work convert point clouds into voxels as input data. \nOn the KITTI~\\cite{kitti} dataset, the range of point clouds is clipped into [0, 70.4m] for {\\em X} axis, [-40m,40m] for {\\em Y} axis, and [-3, 1]m for {\\em Z} axis. The voxelization size for input is (0.05m, 0.05m, 0.1m). \n\n\\vspace{0.5em}\n\\noindent\n\\textbf{nuScenes.}\nOn the nuScenes~\\cite{nuscenes}, the detection range is set to [-54m, 54m] for both {\\em X} and {\\em Y} axes, and [-5m, 3m] for the {\\em Z} axis. The voxel size is set as (0.075m, 0.075m, 0.2m).\n\n"
                },
                "subsection 7.2": {
                    "name": "Data Augmentations",
                    "content": "\n\\noindent\n\\textbf{KITTI.}\nOn the KITTI~\\cite{kitti} dataset, data transformation and augmentations include random flipping, global scaling, global rotation, and ground-truth (GT) sampling~\\cite{second}. The random flipping is conducted along the {\\em X} axis. The global scaling factor is sampled from 0.95 to 1.05. The global rotation is conducted around the {\\em Z} axis. The rotation angle is sampled from -45$^{\\mathrm{o}}$ and 45$^{\\mathrm{o}}$. The ground-truth sampling is to copy-paste some new objects from other scenes to the current training data, which enriches objects in the environments. For the multi-modal setting, we do not transform images with the corresponding operations, except ground-truth sampling. We copy-paste the corresponding image crops from other scenes onto the current training images.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{nuScenes.}\nOn the nuScenes~\\cite{nuscenes} dataset, data augmentations includes random flipping, global scaling, global rotation, GT sampling~\\cite{second}, and an additional translation. The random flipping is conducted along both {\\em X} and {\\em Y} axes. The rotation angle is also randomly sampled in [-45$^{\\mathrm{o}}$, 45$^{\\mathrm{o}}$]. The global scaling factor is sampled in [0.9, 1.1]. The translation noise is conducted on all three axes, {\\em X}, {\\em Y}, and {\\em Z}, with a factor independently sampled from 0 to 0.5. We also conduct the corresponding point-image GT sampling on the nuScenes. GT sampling is disabled in the last 4 epochs~\\cite{pointaugmenting} for performance enhancement.\n\n\n\n\n"
                },
                "subsection 7.3": {
                    "name": "Training Settings",
                    "content": "\n\\noindent\n\\textbf{KITTI.}\nFor model training on the KITTI dataset, {\\em i.e.}, PV-RCNN~\\cite{pvrcnn} and Voxel R-CNN~\\cite{voxel-rcnn}, we train the network for 80 epochs with the batch size 16. We adopt the Adam optimizer. The learning rate is set as 0.01 and decreases in the cosine annealing strategy. The weight decay is set as 0.01. The momentum is set as 0.9. The gradient norms of training parameters are clipped by 10.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{nuScenes.}\nFor models trained on the nuScenes datasets, {\\em i.e.}, CenterPoint~\\cite{centerpoint}, we also train the network for 20 epochs with batch size 32. They are also trained with the Adam optimizer. The learning rate is initialized as 1e-3 and decreases in the cosine annealing strategy to 1e-4. The weight decay is set as 0.01. The gradient norms of training parameters are clipped by 35.\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 8": {
                "name": "Backbone Networks",
                "content": "\n\\label{sec:backbone-networks}\nWe illustrate the structure of the backbone networks in Fig.~S~-~\\ref{fig:backbone-network}. In this illustration, {\\em Reg block} and {\\em Subm block} mean the regular sparse convolutional block and the submanifold sparse convolutional block, respectively. The backbone networks are based on VoxelNet~\\cite{voxelnet}.  It contains a stem layer and 4 stages. In the last three stages, Stage 1, 2, and 3, there a regular sparse convolutional block with stride as 2 for down-sampling. There are some detailed differences among different frameworks, as the following. \n",
                "subsection 8.1": {
                    "name": "Architecture settings",
                    "content": "\n\\noindent\n\\textbf{PV-RCNN and Voxel R-CNN.}\nIn the backbones of PV-RCNN~\\cite{pvrcnn} and Voxel R-CNN~\\cite{voxel-rcnn}, the channels for the stem and stages, \\{$c_0$, $c_1$, $c_2$, $c_3$, $c_4$\\}, are \\{16, 16, 32, 64, 64\\}. The numbers of {\\em Subm blocks} in these stages, \\{$n_1$, $n_2$, $n_3$, $n_4$\\}, are \\{1, 2, 2, 2\\}. A {\\em Reg} or {\\em Subm block} is a conv-bn-relu layer, which includes a regular or submanifold convolution, a batch normalization layer~\\cite{batchnorm}, and a ReLU activation.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{CenterPoint.}\nIn the backbone network of the CenterPoint~\\cite{centerpoint} detector, the backbone network is larger. The channels for the stem and stages, \\{$c_0$, $c_1$, $c_2$, $c_3$, $c_4$\\}, equal to \\{16, 16, 32, 64, 128\\}. The numbers of repeated {\\em Subm blocks} in these stages, \\{$n_1$, $n_2$, $n_3$, $n_4$\\}, are \\{2, 2, 2, 2\\}. Compared to that in the PV-RCNN~\\cite{pvrcnn} and Voxel R-CNN~\\cite{voxel-rcnn} detectors, the {\\em Subm block} is more complicated in this backbone network. Except the stem, it contains two sequential conv-bn-relu layers, with a residual connection.\n\n\n\n\n"
                },
                "subsection 8.2": {
                    "name": "Focal Sparse Convolution Usage",
                    "content": "\nIn our approach, the above architecture-level settings are directly inherited from the original PV-RCNN~\\cite{pvrcnn}, Voxel R-CNN~\\cite{voxel-rcnn}, and CenterPoint~\\cite{centernet} frameworks, without any adjustment, for a fair comparison. \nIn the LIDAR-only task, we insert the {\\em Focals Conv} in the last layer of Stage 1, 2, and 3. In the multi-modal task, we insert the {\\em Focals Conv - F} only at the last layer of Stage 1. This relieves the efficiency and memory issues caused by the RGB feature extraction. Note that, in the CenterPoint~\\cite{centerpoint} detectors, it is also used in the last {\\em layer}, not the total {\\em block}. In other words, although there are two conv-bn-relu layers in each Subm block in CenterPoint~\\cite{centerpoint}, we only apply it as the last layer. For simplification, we do not double it as a block. \n\n"
                }
            },
            "section 9": {
                "name": "Additional Experiments",
                "content": "\n",
                "subsection 9.1": {
                    "name": "Results on Bird's Eye View on KITTI",
                    "content": "\nWe report the accuracy for 3D object detection and Bird's Eye View (BEV) of \\OursF upon Voxel R-CNN~\\cite{voxel-rcnn} on the KITTI~\\cite{kitti} dataset in Tab.~S~-~\\ref{tab:kitti-val-r40}. The results are calculated by recall 40 positions with the IoU threshold of 0.7. It performs better than the strong Voxel R-CNN~\\cite{voxel-rcnn} baseline on both AP$_{\\textrm{BEV}}$ and AP$_{\\textrm{3D}}$ in moderate and hard cases. \nWe also provide the Prevision-Recall (PR) curves of \\OursF on KITTI {\\em test} split in Fig.~S~-~\\ref{fig:kitti-pr-curves}.\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Objective Loss Weight",
                    "content": "\n\\label{sec:ablations}\nThe training of the focal sparse convolutional networks involves the objective loss function. We implement it as a focal loss~\\cite{focalloss} as in Eq~\\eqref{eq:objloss}.\n\\begin{equation}\n    L_{obj}=\\frac{1}{|N|}\\sum_{i\\in \\mathnormal{N}} -(1-\\bar{p_{i}})^{\\gamma}\\mathrm{log}(\\bar{p_{i}}).\n    \\label{eq:objloss}\n\\end{equation}\nwhere $i\\in N$ enumerates all sparse features in the current feature space. Following the original focal loss~\\cite{focalloss}, we directly set $\\gamma=2$ and it works well.\nFor notational convenience, we define $\\bar{p_{i}}$ as follow\n\\begin{equation}\n\\bar{p_{i}}=\\left\\{\n\\begin{aligned}\n& p_i, & \\mathrm{if}\\;\\;\\, y_i=1, \\\\\n& 1 - p_i, & \\mathrm{otherwise},\n\\end{aligned}\n\\right.\n\\label{eq:pi}\n\\end{equation}\nwhere $p_i\\in[0,1]$ is the estimated probability for the class with label $y_i=1$. It is the estimation that whether the feature $i$ contributes any foreground objects.\n\nWe analyze the loss weight for this objective loss in Tab.~S~-~\\ref{tab:ablation-lossweight}. This ablation study is conducted upon the PV-RCNN~\\cite{pvrcnn} detector on the KITTI~\\cite{kitti} datasets. The results on AP$_{\\textrm{3D}}$ with 40 recall positions are reported as the metric. We change the loss weight values from \\{0.1, 0.5, 1.0, 2.0\\}. It shows that too large or too small loss weight values degrade the results. Loss weights 0.5 and 1.0 present\ncompetitive performance. We remain the 1.0 loss weight as a default setting for simplification.\n\n"
                },
                "subsection 9.3": {
                    "name": "Improvements on the Waymo Open Dataset.",
                    "content": "\nTo show our generalization capacity, we conduct further experiments on Waymo dataset. We use $\\frac{1}{5}$ training data, following the default setting in the OpenPCdet codebase~\\footnote{https://github.com/open-mmlab/OpenPCDet}. As shown in Tab.~S~-~\\ref{tab:waymo}, \\Ours also brings non-trivial improvements on the Waymo~\\cite{waymo} dataset.\n\n"
                },
                "subsection 9.4": {
                    "name": "Improvements on the nuScenes val split.",
                    "content": "\nTab.~S~-~\\ref{tab:improvements-over-centerpoint} presents the improvements over CenterPoint~\\cite{centerpoint} on the nuScenes {\\em val} split. The CenterPoint baseline in Tab.~S~-~\\ref{tab:improvements-over-centerpoint} is re-implemented in the same settings to \\Ours and \\OursF. It shows that both \\Ours and \\OursF bring non-trivial improvements. Notably, \\OursF improves the plain CenterPoint~\\cite{centerpoint} by 4.9\\% mAP on the nuScenes {\\em val} split. We further apply some tricks for performance enhancement, {\\em e.g.}, disabling ground-truth sampling in the last 4 epochs~\\cite{pointaugmenting} and double-flip testing~\\cite{centerpoint}.\n\n\n"
                },
                "subsection 9.5": {
                    "name": "Accuracy loss on some categories after fusion.",
                    "content": "\nA surprising case is that the multi-modal fusion make the performance stay the same or worse on some popular categories, {\\em e.g.}, Car, Ped, Bar (from {\\em Focals Conv} to {\\em Focals Conv - F} in Tab.~\\textcolor{red}{11}). The improvements over the baseline are consistent on all categories. \nTo analyze this special case, we conduct ablations on augmentations on CenterPoint and the nuScenes $\\frac{1}{4}$ training set. We find {\\em ground-truth sampling}~(GT Sampl.) is the keypoint. \nAs in Tab.~A~-~\\ref{tab:nuscenes-1/4-augs}, when GT Sampl. is used, the performance on some popular categories ({\\em e.g., Car, Bus, Ped, Bar}) stays the same or worse. In contrast, when we disable GT Sampl. and apply all other transformations (flip, rotation, re-scaling, and translation), all categories are benefited from the fusion.\nWe suppose that this is from the image-level copy-paste in GT Sampl. When other objects are pasted onto images, popular objects inevitably have more chance to be covered by the pasted, which degrades the performance on these categories.\n\n"
                },
                "subsection 9.6": {
                    "name": "Ablations on Voxel Size.",
                    "content": "\nWe ablate the effects of different voxel sizes upon \\OursF on the nuScenes {\\em val} split in Tab.~S~-~\\ref{tab:ablations-voxelsize}. We change the voxel sizes in {\\em X} and {\\em Y} axes from 0.05m to 0.15m, with the interval 0.025m. The overall mAP achieves the best performance at the voxel size (0.075, 0.075, 0.2)m.\nHowever, the proper voxel sizes vary across different classes. This phenomenon deserves further analysis or a dynamic mechanism design in the future.\n\n"
                }
            },
            "section 10": {
                "name": "Visualizations",
                "content": "\n\\label{sec:Visualizations}\nWe provide additional visual comparisons between the plain network and the focal sparse convolutional networks in Fig.~S~-~\\ref{fig:illustration}. It shares the same settings to the Fig.~\\textcolor{red}{2} in the paper. These visualizations are based on the PV-RCNN~\\cite{pvrcnn} detectors and on the KITTI~\\cite{kitti} dataset. In each visualization group, the top figure is the distribution of input voxels. The middle and the bottom figures are from the plain and the focal sparse convolutional networks, respectively. We project the coordinate centers of the output voxel features from the backbone networks onto the 2D image plane. The projection is based on the calibration matrices of KITTI~\\cite{kitti}. \n\n"
            }
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\mathrm{y}_p=\\sum_{k\\in \\mathnormal{K}^d} \\mathrm{w}_k \\cdot \\mathrm{x}_{\\bar{p}_k},\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\mathrm{y}_{p\\in P_{\\mathrm{out}}}=\\sum_{k\\in \\mathnormal{K}^d(p, P_{\\mathrm{in}})} \\mathrm{w}_k \\cdot \\mathrm{x}_{\\bar{p}_k},\n    \\label{eq:sparse-conv}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\mathnormal{K}^d(p, P_{\\mathrm{in}})=\\{k\\,|\\, p+k\\in P_{\\mathrm{in}},k\\in\\mathnormal{K}^d\\}.\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    P_{\\mathrm{out}}=\\bigcup_{p\\in P_{\\mathrm{in}}} P(p, \\mathnormal{K}^d),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    P(p,{\\mathnormal{K}^d})=\\{p+k\\,|\\,k\\in\\mathnormal{K}^d\\}.\n    \\label{eq:output-positions}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    P_{\\mathrm{out}} = \\left(\\bigcup_{p\\in P_{\\mathrm{im}}} P(p,{\\mathnormal{K}_{\\mathrm{im}}^d(p)})\\right)\\cup P_{\\mathrm{in}/\\mathrm{im}}.\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    P_{\\mathrm{im}} = \\{p\\,|\\, I^p_{0}\\geq\\tau,p\\in P_{\\mathrm{in}}\\},\n    \\label{eq:p_im}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\mathnormal{K}_{\\mathrm{im}}^d(p) = \\{k\\,|\\, p+k\\in P_{\\mathrm{in}}, I^p_{k}\\geq\\tau, k\\in\\mathnormal{K}^d\\}.\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    L_{obj}=\\frac{1}{|N|}\\sum_{i\\in \\mathnormal{N}} -(1-\\bar{p_{i}})^{\\gamma}\\mathrm{log}(\\bar{p_{i}}).\n    \\label{eq:objloss}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\bar{p_{i}}=\\left\\{\n\\begin{aligned}\n& p_i, & \\mathrm{if}\\;\\;\\, y_i=1, \\\\\n& 1 - p_i, & \\mathrm{otherwise},\n\\end{aligned}\n\\right.\n\\label{eq:pi}\n\\end{equation}"
        },
        "git_link": "https://github.com/dvlab-research/FocalsConv"
    }
}