{
    "meta_info": {
        "title": "Deconfounding Actor-Critic Network with Policy Adaptation for Dynamic  Treatment Regimes",
        "abstract": "Despite intense efforts in basic and clinical research, an individualized\nventilation strategy for critically ill patients remains a major challenge.\nRecently, dynamic treatment regime (DTR) with reinforcement learning (RL) on\nelectronic health records (EHR) has attracted interest from both the healthcare\nindustry and machine learning research community. However, most learned DTR\npolicies might be biased due to the existence of confounders. Although some\ntreatment actions non-survivors received may be helpful, if confounders cause\nthe mortality, the training of RL models guided by long-term outcomes (e.g.,\n90-day mortality) would punish those treatment actions causing the learned DTR\npolicies to be suboptimal. In this study, we develop a new deconfounding\nactor-critic network (DAC) to learn optimal DTR policies for patients. To\nalleviate confounding issues, we incorporate a patient resampling module and a\nconfounding balance module into our actor-critic framework. To avoid punishing\nthe effective treatment actions non-survivors received, we design a short-term\nreward to capture patients' immediate health state changes. Combining\nshort-term with long-term rewards could further improve the model performance.\nMoreover, we introduce a policy adaptation method to successfully transfer the\nlearned model to new-source small-scale datasets. The experimental results on\none semi-synthetic and two different real-world datasets show the proposed\nmodel outperforms the state-of-the-art models. The proposed model provides\nindividualized treatment decisions for mechanical ventilation that could\nimprove patient outcomes.",
        "author": "Changchang Yin, Ruoqi Liu, Jeffrey Caterino, Ping Zhang",
        "link": "http://arxiv.org/abs/2205.09852v2",
        "category": [
            "cs.LG",
            "cs.AI"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n \nMechanical ventilation is one of the most widely used interventions in admissions to the intensive care unit (ICU). Around 40\\% of patients in the ICU are supported on invasive mechanical ventilation at any given time, accounting for 12\\% of total hospital costs in the United States \\cite{ambrosino2010difficult,wunsch2013icu}. \nDespite intense efforts in basic and clinical research, an individualized ventilation strategy for critically ill patients remains a major challenge \\cite{mechnical_ventilation,mechnical_ventilation_prasad}.\nIf not applied adequately, suboptimal ventilator settings can result in ventilator-induced lung injury, hemodynamic instability, and toxic effects of oxygen. Dynamic treatment regime (DTR) learning on electronic health records (EHR) with reinforcement learning (RL) might be helpful for learning optimal treatments by analyzing a myriad of (mostly suboptimal) treatment decisions. \n\nRecently, DTR learning with RL has attracted the interest of healthcare researchers \\cite{aiclinician,raghu2017deep,raghu2019reinforcement,ciq,cirl,amia2018,mechnical_ventilation}. However, most existing studies suffer from three limitations. First, most existing RL-based methods \\cite{aiclinician,kdd2018,amia2018,mechnical_ventilation} punish the treatment actions for patients who ultimately suffer from mortality. However, for some patients with worse health states, the mortality rates remain high even if they received optimal treatment. Actions that did not contribute to mortality should not be punished in the treatment of non-survivors. Second, RL strategies learned from initial EHR datasets may be biased due to the existence of confounders (patients' health states are confounders for treatment actions and clinical outcomes) and data unbalance (mortality rates in different datasets vary widely and might be less than 25\\%). Third, external validation on different-source data is lacking (e.g., how a model trained on data extracted from the United States performs on European datasets). Especially when the treatment action distributions are different, efficient adaptation to new datasets has not been considered.\n% \\textcolor{blue}{It is very good to talk on the introduction. But NOT in the abstract.}\n\n\nIn this study, we propose a new deconfounding actor-critic model (\\textbf{DAC}) to address these issues.\nFirst, we resample paired survivor and non-survivor patients with similar estimated mortality risks to build balanced mini-batches.\nThen we adopt an actor-critic model to learn the optimal DTR policies. The longitudinal patients' data are sent to a long short-term memory network (LSTM) \\cite{lstm} to generate the health state sequences.\nThe actor network produces the probabilities of different treatment actions at next time step and is trained by maximizing the rewards generated by the critic network. To avoid punishing some effective treatment actions in EHR history of non-survivors, the critic network produces both short-term and long-term rewards. Short-term rewards can encourage the treatment actions that improve patients' health states at coming time steps, even if the patients ultimately suffer from mortality.\nTo further remove the confounding bias, we introduce a dynamic inverse probability of treatment weighting method to assign weights to the rewards at each time step for each patient and train the actor network with the weighted rewards.\nFinally, we introduce a policy adaptation method to transfer well-learned models to new-source small-scale datasets. The policy adaption method chooses actions so that the resulting next-state distribution on the target environment is similar to the next-state distribution resulting from the recommended action on the source environment.\n\n\nWe conduct DTR learning experiments on a semi-synthetic dataset and two real-world datasets (i.e., MIMIC-III \\cite{mimic} and AmsterdamUMCdb \\cite{amsterdamumcdb}). The experimental results show that the proposed model outperforms the baselines and can reduce the estimated mortality rates. Moreover, we find the mortality rates are lowest in patients for whom cliniciansâ€™ actual treatment actions matched the model's decisions.\n The proposed model can provide individualized treatment decisions that could improve patients' clinical outcomes.\n% Moreover, to evaluate the effectiveness of the proposed deconfounding method, we conducted treatment effect estimation experiments on semi-synthetic datasets and compared the proposed methods with state-of-the-art models. Due to limited space, we put the treatment effect estimation results in the supplementary material.\n\nIn sum, our contributions are as follows: (i) We develop a new DTR learning framework with RL and experiments on MIMIC-III and AmsterdamUMCdb datasets demonstrate the effectiveness of the proposed model; (ii) We present a patient resampling operation and a confounding balance module to alleviate the confounding bias; (iii) We propose combining long-term and short-term rewards to train the RL models; (iv) We propose a policy adaptation model that can effectively adapt pre-trained models to new small-scale datasets.  The code of our proposed DAC model can be found at GitHub\\footnote{\\label{github}\\url{https://github.com/yinchangchang/DAC}}. \n\n\n% The rest of the paper is organized as follows.  In Section 2, we describe technical details of the proposed dynamic treatment regime learning framework. In Section 3, we conduct experiments on two real-world EHR datasets. We review the related studies in Section 4. Section 5 concludes our work.\n\n\n\n\n\n"
            },
            "section 2": {
                "name": "Problem Formulation",
                "content": "\n\\textbf{Setup.}\nDTR is modeled as a Markov decision process (MDP) with finite time steps and a deterministic policy consisting of an action space $\\mathcal{A}$, a hidden state space $\\mathcal{S}$, a observational state space $\\mathcal{O}$, and a reward function: $\\mathcal{A} \\times \\mathcal{S} \\rightarrow R$. \nA patient's EHR data consists of a sequence of observational variables (including demographics, vital signs and lab values), denoted by $O=\\{o_1, o_2, ..., o_T\\}$, $o_t \\in \\mathcal{O}$, the treatment actions represented as $A=\\{a_1, a_2, ..., a_T\\}$, $a_t \\in \\mathcal{A}$ and mortality outcome $y \\in \\{0, 1\\}$, where $T$ denotes the length of the patient's EHR history.\nWe assume some hidden variables $S=\\{s_1, s_2, ..., s_T\\}$, $s_t \\in \\mathcal{S}$ can represent the health states of a patient and include the key information of previous observational data of the patients. \nGiven the previous hidden state sequence $S_t = \\{s_1, s_2, ..., s_t\\}$, action sequence $A_{t-1} = \\{a_1, a_2, ..., a_{t-1}\\}$ and observation sequence $O_t = \\{o_1, o_2, ..., o_t\\}$ up to time step $t$, our goal is to learn a policy $\\pi_\\theta(\\cdot|S_t, O_t, A_{t-1})$ to select the optimal action $\\hat{a}_t$ by maximizing the the sum of discounted rewards (return) from time step $t$. We use LSTM to model patient health states and  LSTM can remember the key information of patients' EHR history. We assume state $s_t$ contains the key information of the previous data, and learn a policy $\\pi_\\theta(\\cdot|s_t)$ instead of $\\pi_\\theta(\\cdot|S_t, O_t, A_{t-1})$.\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Time-varying confounders.}\nFigure \\ref{fig:model} (a) shows the causal relationship of various variables.  $o_t$ denotes the time-dependent covariates of the observational data at time step $t$, which is only affected by hidden state $s_t$. The treatment actions $a_t$ are affected by both observed variable $o_t$ and hidden state $s_t$. The potential outcomes $y$ are affected by last observational variable $o_T$, treatment assignments $a_T$ and hidden state $s_T$. Patients' health states $S$ are time-varying confounders for both treatment actions $A$ and clinical outcomes $y$. Without the consideration of the causal relationship among the variables, it is possible that RL models may focus on the strong correlation between positive outcomes and \"safe\" actions (e.g., without mechanical ventilator) and prefer to recommend the \"safe\" actions, which will cause much higher mortality rates for high-risk patients. It is very important to remove the confounding when training DTR policies on real-world datasets. \nDTR policies learned from initial clinical data could be biased due to the existence of time-varying confounders. \n\n\nWe summarize the important notations in this paper in Table \\ref{tab:notations}.\n\n\n\n\n\n\n"
            },
            "section 3": {
                "name": "Method",
                "content": "\n\nIn this section, we propose a new causal reinforcement learning framework to learn optimal treatment strategies. We first introduce the deconfounding module that resamples patients according to their mortality risks and computes the weights for rewards in RL model. Then we develop an actor-critic network to learn DTR policies with the weighted rewards. Finally, we present a policy adaptation method that can transfer well-trained models to new-source environments.\n\n\n\n\n",
                "subsection 3.1": {
                    "name": "Deconfounding Module",
                    "content": "\n\n\nDTR policies learned from initial clinical data could be biased for two-fold reasons. First, the training of RL models is usually guided by designed rewards, which are highly related to patients' long-term outcomes. Existing DTR models \\cite{aiclinician,kdd2018,raghu2017deep} encourage the treatment actions that survivors received and punish the treatment actions that non-survivors received. The mortality rates of the collected datasets have important effects on the learned policies and might cause policy bias. The mortality rates of different-source datasets vary widely and the bias could further limit the model performance when adapting learned DTR policies to new-source datasets. The second reason for policy bias is the existence of confounders. Patients' clinical outcomes $y$ (e.g., mortality or discharged) are affected by both patient health states $s_t$ and treatment actions $o_t$, as shown in Fig. \\ref{fig:model} (a). The treatment actions are also affected by patient health states $s_t$. The patient hidden states $s_{t}$ are confounders for both actions $a_{t}$ and final clinical outcome $y$. \nIn this subsection, we introduce patient resampling module and confounding balance module to address the policy bias problems.\n\n% Action distributions between the case pool (patients suffered from mortality) and the control pool (patients discharged) are different. As shown in Fig. \\ref{fig:distribution}, the percentages of high-dose vasopressors in the EHRs of patients with mortality are more than two times of the percentages of high-dose vasopressors in the EHRs of discharged patients.\n% Existing models learn treatment strategies from initial-distribution data, which makes it possible to learn a biased treatment policy (e.g., a policy that always recommends low-dose vasopressors might significantly increase the mortality risks of patients with higher severity of illness).\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Patient resampling module.} \nWe resample the patients according to their mortality risks when training our treatment learning models. First, we train a mortality risk prediction model, which takes the patients' observational data as inputs and produces the 90-day mortality probability at each time step $t$.\nThen, patients are divided into a survivor pool and a non-survivor pool. When training treatment learning models, we always sample paired patients from the two pools respectively with similar maximal mortality risks in their EHR sequence. With the resampling operation, we build balanced mini-batch where survivors and non-survivors have similar mortality risk distributions, as shown in Figure \\ref{fig:model} (b).\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Confounding balance module.}\nTo adjust the confounder, we train the actor-critic network with weighted rewards and the weights are computed based on the probabilities that the corresponding treatment actions are assigned. Given a patient health state $s_t$ at time step $t$, the probability that an action $a$ would be assigned is represented as $\\pi_\\theta(a|s_t)$. We compute the weights using inverse probability of treatment weighting (IPTW) \\cite{iptw,iptwm} and extend to dynamic multi-action setting as follows,  \n\\begin{equation} \n\\label{eq:weight}\nw_t = \\Pi^t_{\\tau=1} \\frac{f(a_\\tau|A_{\\tau - 1})}{f(a_\\tau|A_{\\tau-1},O_{\\tau -1})}\n=\\Pi^t_{\\tau=1} \\frac{f(a_\\tau|A_{\\tau - 1})}{\\pi^c(a_\\tau|s_\\tau)}\n%\\sum_i \\frac{I(a_t = a_i)*P(a_t|a_{t-1})}{\\pi^c(a_i|a_{t-1},s_t)},\n% p(a_0, a_1, ..., a_t) / p(a_0, ..., a_t, s_t) =\n\\end{equation}\nwhere $f(a_\\tau|A_{\\tau - 1})$ is the posterior probability of action $a_\\tau$ given last action sequence $A_{\\tau - 1}$, which could be modelled with LSTM. $f(a_\\tau|A_{\\tau-1},O_{\\tau})$ denotes predicted probability of receiving treatment $a_\\tau$\ngiven the observed data and historical information, and is computed with clinician policy $\\pi^c$. $\\pi^c(a_\\tau|s_\\tau)$ is the probability for action $a_\\tau$ given patient's health state $s_\\tau$. $\\pi^c$ shares the same actor network as the proposed DAC model and is trained by mimicking clinicians' policy.\nThe computed weights are used in the training of the actor network.\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Actor-Critic Framework",
                    "content": "\n\nIn this subsection, we present the details of our RL model based on actor-critic network, including how to model patients' health states and update the actor and critic networks.\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Observational data embedding and health state representation.} \nThe observational data contain different vital signs and lab tests, which have lots of missing values. Existing models usually impute the missing values based on previous observational data. However, for some patients with some high missing-rate variables, the imputation results might be inaccurate and thus introduce more imputation bias, which is harmful for modeling the patient health states. Following \\cite{tame}, we embed the observed variables with corresponding values, and only input the embeddings of observed variables to the model. Given the variable $i$  and the observed values in the whole dataset, we sort the values and discretize the values into $V$ sub-ranges with equal number of observed values in each sub-range. The variable $i$ is embedded into a vector $e^i \\in R^k$ with an embedding layer. As for the sub-range $v (1 \\leq v \\leq V)$, we embed it into a vector $e'^v \\in R^{2k}$: \n\\vspace{-1mm}\n\\begin{equation} \n\\label{eq:value-embedding-1} \ne'^v_j = sin(\\frac{v*j}{V*k}), \\qquad\ne'^v_{k+j} = cos(\\frac{v*j}{V*k}), \n\\end{equation}\nwhere $0 \\leq j < k$. By concatenating $e^i$ and $e'^v$, we obtain vector containing both the variable's and its value's information. A fully connected layer is followed to map the concatenation vector into a new value embedding vector $e^{iv} \\in R^k$. \n \nGiven the value embeddings of observational variables in the same collection, a max-pooling layer is followed to generate the collection representation vector $e_t$.\nThey are sent to a LSTM to generate a sequence of health state vectors  $S=\\{s_1, s_2, ..., s_{|S|}\\}$, $s_t \\in R^k$.\n\n\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Actor network update.}\nGiven a patient's health states, a fully connected layer and a softmax layer are followed to generate the probabilities for next actions. \nThe actor network generates the probabilities for next actions $\\pi_\\theta$. \nThe critic network produce the rewards for action $a$, denoted as $Q(s, a)$. We update the actor network by maximizing the expected reward:\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:expected-reward}\nJ(\\pi_\\theta) = \\int_{s \\in \\mathcal{S}} \\rho(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s) Q(s,a) ds,\n\\end{equation}\nwhere $\\rho(s)$ denotes the state distribution. We use policy gradient to learn\nthe parameter $\\theta$ by the gradient $\\triangledown_\\theta J(\\pi_\\theta)$ which is calculated using\nthe policy gradient theorem \\cite{policy_gradient}:\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:policy-gradient}\n\\begin{aligned}\n\\triangledown_\\theta J(\\pi_\\theta) = \\int_{s \\in \\mathcal{S}} \\rho(s) \\sum_{a \\in \\mathcal{A}} \\triangledown_\\theta \\pi_\\theta(a|s) Q(s,a) ds \\\\\n= E_{s \\sim \\rho, a \\in \\pi_\\theta} [\\triangledown_\\theta \\log \\pi_\\theta(a|s) Q(s,a)]\n\\end{aligned}\n\\end{equation}\n\\vspace{-2mm}\n\n\n\\begin{algorithm}[t]\n\\caption{Deconfounding Actor-Critic}\n\\label{alg:RL}\n\n\\textbf{Input}: Observations $O$, treatment actions $A$, outcome $y$;\n\n\\noindent\n\\textbf{Output}: Policy $\\pi_\\theta$;\n\\begin{algorithmic}[1] %[1] enables line numbers\n\\STATE Train a mortality risk prediction model and compute the risks for patients in training set;\n\\REPEAT\n\\STATE Sample paired patients from survivor and non-survivor pools with similar mortality risks;\n\\STATE \\textit{ \\# Inference}\n\\FOR{$t = 1, ..., T$} \n\\STATE Input the observations $o_t$ to LSTM to generate health states $s_t$;\n\\STATE Produce probability distribution for next actions $\\pi_\\theta(\\cdot|s_t)$;\n\\STATE Compute reward weight $w_t$ according to Eq. (\\ref{eq:weight});\n\\STATE Compute long-term reward $R^l(s_t, a_t)$ according to Eq. (\\ref{eq:long-term-reward});\n\\STATE Compute short-term reward $R^s(s_t, a_t)$ according to Eq. (\\ref{eq:short-reward});\n\\STATE Compute the weighted reward $Q(s_t, a_t)$ according to Eq. (\\ref{eq:weighted-reward});\n\\ENDFOR\n\\STATE \\textit{ \\# Actor network update}\n\\STATE Update policy  $\\pi_\\theta$ according to Eq. (\\ref{eq:policy-gradient});\n\\STATE \\textit{ \\# Critic network update}\n\\STATE Update long-term reward function $R^l(s,a)$ by minimizing $J(w_l, b_l)$ in Eq. (\\ref{eq:long-term-reward-loss});\n\\STATE Update mortality risk prediction function $p_m(s,a)$ by minimizing $J(w_m, b_m)$ in Eq. (\\ref{eq:long-term-reward-loss});\n\\UNTIL{Convergence.}\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Critic network update.} The critic network takes patients' health states and treatment actions as inputs, and output the rewards. \nWe use fully connected layers to learn the long-term reward function:\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:long-term-reward} \nR^l(s_t) = s_t w_l + b_l,\n\\end{equation}\nwhere $w_l \\in R^{k \\times |\\mathcal{A}|}$, $b_l \\in R^{|\\mathcal{A}|}$ are learnable parameters.\nGiven the state-action pairs at time $t$, the long-term reward function is trained by minimizing $J(w_l, w_l)$:\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:long-term-reward-loss}\n\\begin{aligned}\nJ(w_l, b_l) = E_{s_t \\sim \\rho} [ R^l(s_t, a_t) - z_t)^2] \\\\\nz_t = R^m(s_t,a_t) + \\gamma R^l(s_{t+1}, \\hat{a}_{t+1}),\n\\end{aligned}\n\\end{equation}\nwhere $\\hat{a}_{t+1}$ is the action with the maximum reward in the next step, $R^l(s_t, a_t) \\in R$ is the corresponding dimension reward of $R^l(s_t)$ for action $a_t$  and $R^m(s_t, a_t)$ denotes the reward at the last time step. Given a patient with EHR length equal to $T$, $R^m(s_t, a_t) = 0, t<T$. Following \\cite{aiclinician,raghu2019reinforcement}, the reward for the last action is set as $\\pm 15$. Specially, if the patient suffers from mortality, $R^m(s_T, a_T) = -15$. Otherwise, $R^m(s_T, a_T) = 15$.\n\nMost existing RL-based models are trained with long-term rewards and punish the actions non-survivors received. However, for some patients with worse health states, the probability of mortality is still high even if they receive optimal treatment. Some actions should not be punished in the treatment of patients with mortality. We propose a short-term reward based on estimated mortality risk to improve the training of RL models. The estimated mortality risks $p_m(s_t)$ are generated with fully connected layers and a Sigmoid layer:\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:mortality-risk-prediction}\np_m(s_t) = Sigmoid (s_t w_m + b_m),\n\\end{equation}\nwhere $w_m \\in R^{k \\times |\\mathcal{A}|}$, $b_m \\in R^{|\\mathcal{A}|}$ are learnable parameters.\nThe mortality probability with an action $a$ at time $t$ is the action's corresponding dimension of $p_m(s_t)$, denoted as $p_m(s_t, a)$. The mortality risk prediction function $p_m$ is trained by minimizing $J(w_m, b_m)$:\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:mortality-risk-prediction-loss}\nJ(w_m, b_m) = E_{s_t \\sim \\rho} [-y\\log (p_m(s_t, a_t)) - (1-y) \\log (1 - p_m(s_t, a_t))]\n\\end{equation}\nThe short-term reward is computed as the mortality probability decrease given the action as follows, \n\\begin{equation}\n\\label{eq:short-reward}\nR^s(s_t, a_t) = \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s_t) p_m(s_t, a) - p_m(s_t, a_t)\n\\end{equation}\n\nThe overall reward $Q$ is computed by combining short-term and long-term reward:\n\\begin{equation}\n\\label{eq:weighted-reward}\nQ(s_t,a_t) = w_t ( \\alpha R^l(s_t,a_t) +  (1-\\alpha)R^s(s_t,a_t)),\n\\end{equation}\nwhere $\\alpha$ is a hyper-parameter to adjust the weights of the two rewards and $w_t$ denotes the inverse weight computed by the confounding balance module. The details of $\\alpha$ selection can be found in supplementary material. Algorithm \\ref{alg:RL} describes the training process of the proposed DAC.\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Policy adaptation",
                    "content": "\nIn real-world clinical settings, a pre-trained model might suffer from performance decline in new environments when the patient distribution is different. It is possible that we cannot collect enough data to train a new model in the new environment. To address the problem, we propose a policy adaptation method to transfer the pre-trained model to new environments.\n\nWe first train a policy $\\pi_\\theta^S$ on a source dataset (i.e., MIMIC-III), and then adapt the model to a target dataset (i.e., AmsterdamUMCdb). We learn two dynamic function $f^S$ and $f^T$ on the source dataset and the target dataset respectively to predict next state $s_{t+1}$ given the state $s_t$ and action $a_t$ at time step $t$.\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:dynamic-function}\nf^T(s_t, a_t) = s_t w_d + b_d,\n\\end{equation}\nwhere $w_d \\in R^{k \\times |\\mathcal{O}|}$, $b_d \\in R^{|\\mathcal{O}|}$ are learnable parameters. The dynamic functions are trained by minimizing $J(w_d, b_d)$:\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:MLE}\n\\begin{aligned}\nJ(w_d, b_d) = E_{s_t \\sim \\rho} [ f^T(s_t, a_t) - s_{t+1})^2]\n\\end{aligned}\n\\end{equation} \nNote that $f^S$ and $f^T$ share the same structure and objective function, but are trained on different datasets. The target dynamics $f^T$ is initialized as source dynamics $f^S$ and fine-tuned on the small-scale target dataset.\n\nGiven $\\pi_\\theta^S$, $f^S$ and $f^T$, we define the policy $\\pi_\\theta^T$ on target dataset as:\n\\begin{equation}\n\\pi_\\theta^T(s) = \\arg \\min _{a \\in A} ( f^T(s, a)  - f^S(s, \\pi_\\theta^S(s)))^2\n\\label{eq:transfer-learning} \n\\end{equation}\nAssuming $f^S$ and $f^T$ are accurate in terms of modeling patient state transition on source and target environments, $\\pi_\\theta^T(s)$ can pick the action such that the resulting next state distribution under $f^T$ on target environment is similar to the next state distribution resulting from $\\pi_\\theta^S(s)$ under the source dynamics. Algorithm \\ref{alg:policy-adaptation} describes the details of policy adaptation.\n\n\n\n\\begin{algorithm}[!tb]\n\\caption{Policy Adaptation}\n\\label{alg:policy-adaptation}\n\n\\textbf{Input}: Source domain policy $\\pi^S_\\theta$, source dynamics $f^S$, patient state $s$;\n\n\\noindent\n\\textbf{Output}: Next action on target domain $\\pi^T(s)$,  target dynamics $f^T$;\n\\begin{algorithmic}[1] %[1] enables line numbers\n\\STATE Initialize $f^T$ = $f^S$;\n\\STATE \\# Learn the target dynamics $f^T$;\n\\REPEAT\n\\STATE Sample a bath patients; \n\\FOR{$t = 1, ..., T$} \n\\STATE Compute $f^T(s_t,a_t)$;\n\\ENDFOR\n\\STATE Update $f^T$ by minimizing $J(w_d, b_d)$;\n\\UNTIL{Convergence.}\n\\STATE \\# Adapt $\\pi^S_\\theta$ to target domain; \n\\FOR{patient $p$ in $P$}\n\\FOR{$t = 1, ..., T$} \n\\STATE Compute the optimal action $a_t^S=\\pi_\\theta^S(s_t)$ on source domain;\n\\STATE Compute the predicted next state $f^S(s_t, a_t^S)$ on source domain;\n% \\STATE Compute the state distance $ dis_a = ||f^T(\\cdot|s_t, a) - f^S(\\cdot|s_t, a_t^S)||$; \n\\FOR{action $a$ $\\leftarrow$ $A$} \n\\STATE Compute the state distance $||f^T(s_t, a) - f^S(s_t, a_t^S)||$; \n\\ENDFOR\n\\STATE Recommend the action with minimal state distance;\n\\ENDFOR \n\\ENDFOR \n\\end{algorithmic} \n\\end{algorithm}\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\nTo evaluate the performance of the proposed model, we conduct comprehensive comparison experiments on three datasets, including two real-world EHR datasets and a semi-synthetic dataset.\n\n% a semi-synthetic dataset and two real-world EHR datasets, MIMIC-III \\cite{mimic} and AmsterdamUMCdb \\cite{amsterdamumcdb}. \n\n\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\n\\textbf{Real-world datasets.}\nBoth MIMIC-III\\footnote{\\url{https://mimic.physionet.org/}} and AmsterdamUMCdb\\footnote{\\url{https://amsterdammedicaldatascience.nl}} are publicly available real-world EHR datasets. \nFollowing \\cite{mechnical_ventilation},  we extract all adult patients undergoing invasive ventilation more than 24 hours and extract a set of 48 variables, including demographics, vital signs and laboratory values. Following \\cite{mechnical_ventilation}, \nWe learn the DTR policies for positive end-expiratory pressure (PEEP), fraction of inspired oxygen (FiO2) and ideal body weight-adjusted tidal volume (Vt). We discretize the action space into $7\\times 7 \\times 7$ actions, as Table \\ref{tab:dose_range} shown. \nThe statistics of extracted data from MIMIC-III and AmsterdamUMCdb are displayed in Table \\ref{tab:stati}.  More details of data preprocessing (e.g., the list of extracted variables) can be found in GitHub\\textsuperscript{\\ref{github}}.\n\n\n\n\n\n\n\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Semi-synthetic dataset based on MIMIC-III.} As the MIMIC-III dataset is real-world observational data, it is impossible to obtain the potential outcomes for underlying counterfactual treatment actions. To evaluate the proposed model's ability to learn optimal DTR policies, we further validate the method in a simulated environment. \nWe simulate hidden state $s_t$ and observational data $o_t$ for each patient at time $t$ following $p$-order autoregressive process \\cite{mills1991time}. The details of the simulation can be found in supplementary material and GitHub\\textsuperscript{\\ref{github}}.   \n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Methods for comparison",
                    "content": " \nWe compare the proposed model with following baselines.\n\n\\noindent\n\\textbf{Supervised models:}\n\\begin{itemize}\n    \\item \\textbf{Markov Decision Process (MDP)}: The observations of variables are clustered into 750 discrete\n    mutually exclusive patient states with k-means. Markov decision process is used to learn the state transition matrix with different actions.  Only the discharged patients are used during the training phase. \n    \n    \\item \\textbf{Imitation Learning}: Imitation learning models the patient states with LSTM, and mimics the human clinician policy. Different from MDP, the hidden states of LSTM can represent continuous states of patients. We implemented three versions of imitation learning by training the same model on different datasets. Imitation Learning~$^S$ is trained on the discharged patients. Imitation Learning~$^M$ is trained on the patients with 90-day mortality. Imitation Learning~$^A$ is trained on all the patients in the training set.\n\\end{itemize}\n\n\\noindent\n\\textbf{RL-based DTR learning models:}\n\\begin{itemize}\n    \\item \\textbf{AI Clinician} \\cite{aiclinician}: AI clinician clustered patient states into 750 groups and adopts MDP to model the patient state transition. The difference between AI clinician and MDP is that AI clinician model is trained based on Q-learning while MDP only mimics the human clinician strategy. \n\n    \\item \\textbf{VentAI} \\cite{mechnical_ventilation}: VentAI also adopts MDP to model the patient state transition and uses Q-learning to learn optimal policies for mechanical ventilation.\n    \n    \n    \\item \\textbf{DQN} \\cite{dqn}: DQN leverages LSTM to model patient health states, and Q-learning is used to train the dynamic treatment regime learning model. \n    \n    \\item \\textbf{Mixture-of-Experts (MoE)} \\cite{amia2018}: MoE is a mixture model of a neighbor-based policy learning expert (kernel) and a model-free policy learning expert (DQN). The mixture model switches between kernel and DQN experts depending on patient's current history.\n    \n    \\item \\textbf{SRL-RNN} \\cite{kdd2018}: SRL-RNN is based on actor-critic framework. LSTM is used to map patients' temporal EHRs into vector sequences. The model combines the indicator signal and evaluation signal through joint supervised and reinforcement learning. \n\\end{itemize}\n\n\\noindent\n\\textbf{RL-based models with causal inference:}\n\\begin{itemize}\n    \\item \\textbf{Causal inference Q-network (CIQ)} \\cite{ciq}: CIQ trains Q-network with interfered states and labels.\n    Gassian noise and adversarial observations are considered in the training of CIQ.\n    \n    \\item \\textbf{Counterfactual inverse reinforcement learning (CIRL)} \\cite{cirl}: \n    CIRL learns to estimate counterfactuals and integrates counterfactual reasoning into batch inverse reinforcement learning.  \n    \n\\end{itemize}\n\n\\noindent\n\\textbf{Variants of DAC:}\nWe implement the proposed model with five versions.\nDAC is the main version. By removing the patient resampling module, confounding balance module, long-term rewards or short-term rewards, we train another four versions DAC$^{-rsp}$, DAC$^{-dcf}$, DAC$^{-long}$, DAC$^{-short}$ to conduct the ablation study.\n\n\n\n\n\n\n% \\subsubsection{}\n% We compare the propose model with two supervised methods (i.e., Markov Decision Process (MDP) and Imitation Learning), five existing DTR learning models with RL (i.e., AI Clinician \\cite{aiclinician}, VentAI \\cite{mechnical_ventilation}, DQN \\cite{dqn}, Mixture-of-Experts (MoE) \\cite{amia2018}, SRL-RNN \\cite{kdd2018}) and two RL models with causal inference (i.e., CIQ \\cite{ciq}, CIRL \\cite{cirl}). Note that we implement three versions of imitation learning by training the same model on different datasets.  Imitation Learning~$^S$ is trained on the discharged patients. \n%  Imitation Learning~$^M$ is trained on the patients with 90-day mortality. \n%  Imitation Learning~$^A$ is trained on all the patients in the training set. The four Imitation Learning versions share the same framework with LSTM to model patient states and two fully connected layers to produce the probability distributions of next actions.\nNote that the extracted variables contain lots of vital signs and lab values, which have lots of missing values. The baselines can only take fixed-sized observed variables as inputs. Following \\cite{aiclinician,raghu2019reinforcement}, we impute the missing values with multi-variable nearest-neighbor imputation~\\cite{imputation} before training the baseline models.\n\n\n\n\n\\noindent\n\\textbf{Implementation details.} \nWe implement our proposed model with Python 2.7.15 and PyTorch 1.3.0. For training models, we use Adam optimizer with a mini-batch of 256 patients. The observed variables and corresponding values are projected into a $512$-d space. The models are trained on 1 GPU (TITAN RTX 6000), with a learning rate of 0.0001. We randomly divide the datasets into 10 sets. All the experiment results are averaged from 10-fold cross validation, in which 7 sets were used for training every time, 1 set for validation and 2 sets for test. The validation sets are used to determine the best values of parameters in the training iterations. \nMore details and implementation code are available in GitHub\\textsuperscript{\\ref{github}}.\n\nNote that there are $7 \\times 7 \\times 7=343$ kinds of actions three parameters (i.e., PEEP, FiO2 and tidal volume). At the beginning of training phase, it might be inaccurate to compute the probabilities of 343 kinds of actions, which would cause the computed weight in Eq.~(\\ref{eq:weight}) to be unstable. \nMoreover, clinical guidelines \\cite{guideline_fan2017official,guideline_lahouti} also recommend clinicians to increase or decrease the parameters according to patients' health states. When computing the inverse probabilities, we use the probabilities for 3 action changes (i.e., increase, decrease or keep the same for each parameter) instead of the probabilities of 7 actions. \n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Evaluation Metrics",
                    "content": "\n\\noindent\n\\textbf{Evaluation metrics.}\nThe evaluation metrics for treatment recommendation in real-world datasets is still a challenge \\cite{kdd2018,rl_evaluation}. Following  \\cite{kdd2018,raghu2017deep,weng2017representation,aiclinician,zhang2017leap}, we try two evaluation metrics estimated mortality (\\textbf{EM}), weighted importance sampling (\\textbf{WIS}) to compare the proposed model with the state-of-art methods for real-world datasets. In the simulated environment, we have access to the ground truth of optimal actions and compute the optimal action accuracy rate following \\cite{cirl,ciq}. Mechanical ventilator has three important parameters: PEEP, Vt and FiO2. We compute two kinds of accuracy rates: \\textbf{ACC-3} (whether the three parameters are set the same as the optimal action simultaneously) and \\textbf{ACC-1} (whether each parameter is set correctly).  The details of the metric calculation can be found in supplementary material.\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Result Analysis",
                    "content": "\n\n \n \n\nTable \\ref{tab:res} displays the estimated mortality, WIS and action accuracy rates on the three datasets. The results show that the proposed model outperforms the baselines. \nThe RL-based models (e.g., AI Clinician, MoE, SRL-RNN) achieve lower estimated mortality rates and higher WIS and action accuracy rates than supervised models (i.e., Imitation Learning and MDP), which demonstrates the effectiveness of RL in DTR learning tasks.\n\n\nAmong the three versions of imitation learning,  Imitation Learning $^M$ is trained on the non-survivor patients and thus performs worse than the other two versions. However, Imitation Learning $^M$ still achieves comparable performance to MDP trained on discharged patients, which demonstrates the clinicians' treatment strategies for survivors and non-survivors are similar. Thus it is not appropriate to directly punish the treatment actions prescribed to patients with mortality. We speculate that for some non-survivors, the treatment actions might be helpful but the confounder (e.g., the poor health states before treatments) caused the 90-day mortality. Thus we propose deconfounding modules to alleviate the patient state distribution bias.\nTaking into account the confounders, CIQ, CIRL and the proposed models outperform the RL baselines, which demonstrates the effectiveness of incorporation of counterfactual reasoning in DTR learning tasks. Among the models with the consideration of confounders, the proposed DAC performs better than CIQ and CIRL. We speculate the reasons are two-fold: (i) we train DAC on balanced mini-batch by resampling the patients, which makes critic network's counterfactual action reward estimation more accurate; (ii) the proposed short-term rewards are more efficient at capturing short-term patients' health state changes than discounted long-term rewards during the training of RL models. \n\n\n\n\n\nAmong the five versions of the proposed model, the main version (i.e., DAC) outperforms DAC$^{-rsp}$ and DAC$^{-dcf}$, which demonstrates the effectiveness of proposed patient resampling and confounding balance modules. Combining short-term and long-term rewards, DAC outperforms DAC$^{-short}$ and DAC$^{-long}$, which demonstrates the effectiveness of the two designed rewards. \n\n\n\n \n\n\n\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Distribution of Actions}: Visualization of the action distribution in the 3-D action space on MIMIC-III are shown in Figure \\ref{fig:distribution_mimic}. The results show that our model learned similar policies to clinicians on MIMIC-III dataset. DAC suggests more actions with the higher PEEP and FiO2. Besides, the learning policies recommend more frequent lower tidal volume compared to clinician policy.\n\n\n\n\n% \\vspace{5pt}\n\\noindent\n\\textbf{Comparison of Clinician and DAC policies}: \nWe find that the mortality rates are lowest in patients for whom cliniciansâ€™ actual treatments matched the actions the learned policies recommend. Figure~\\ref{fig:dose_diff_mimic} shows the relations between mortality rate and mechanical ventilation setting difference on MIMIC-III. The results show when patients received lower values of FiO2, PEEP and Tidal Volume, the mortality rates increase much faster. We speculate the reasons are two-fold: (i) DAC only recommends high values of FiO2, PEEP and Tidal volume to the high-risk patients, who still have relatively higher mortality rates even with optimal treatments; (ii) the high-risk patients received low-value settings, which further increased their mortality rates.\n\n\n\n\\vspace{5pt}\n\\noindent\n\\textbf{Policy adaptation}: We adapt the model trained on MIMIC-III to AmsterdamUMCdb, and Fig. \\ref{fig:transfer} shows the estimated mortality and WIS with different training sizes on AmsterdamUMCdb. DAC-M is trained on MIMIC-III and directly validated on AmsterdamUMCdb. DAC-A is trained on AmsterdamUMCdb and DACPA is pretrained on MIMIC-III and then adapted to AmsterdamUMCdb. The results show that with transfer learning on AmsterdamUMCdb, DACPA outperforms DAC-M, which demonstrates that the policy adaption is very helpful and improved model performance. When training size becomes smaller, the performance gaps between DACPA and DAC-A are larger, which demonstrates that the introduced policy adaption method is useful when adapting trained models to new-source small-scale datasets. \n \n\n\n \n \n\n"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\nIn this section, we briefly review the existing works related to DTR and causal inference.  \n\n\\vspace{5pt}\n\\noindent\n\\textbf{DTR learning.}\nDuring recent years, there have been some studies that focus on applying RL to the optimal treatment learning from existing (sub)optimal clinical datasets. \nKomorowski et al. \\cite{aiclinician} proposed AI Clinician model which uses a Markov decision process (MDP) to model patients' health states and learns the treatment strategy to prescribe vasopressors and IV fluids with Q-learning. \nRaghu et al. \\cite{mechnical_ventilation} uses a similar model to AI Clinician to learn the optimal DTR policies for mechanical ventilation and achieves lower estimated mortality rates than human clinicians.\n\\cite{raghu2017deep} expands on Komorowskiâ€™s initial work by proposing a Dueling Double Deep Q network Q-learning model with a continuous state space and introduces a continuous reward function to train the model. They show that for patients with higher severity of illness, due to a lack of data, the model did not outperform the human clinicians.\n\\cite{amia2018} presents mixture-of-experts (MoE) to combine the restricted DRL approach with a kernel RL approach selectively based on the context and find that the combination of the two methods achieves a lower estimated mortality rate. \\cite{kdd2018} proposes a new Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN) model for dynamic treatment regime, which combines the indicator signal and evaluation signal through the joint supervised learning and RL. The experiments demonstrate that the introduced supervised learning is helpful for stably learning the optimal policy. \nAlthough the DTR learning algorithms can achieve high performance on treatment recommendation tasks, the learned policies could be biased without the consideration of confounding issues. \n\n\\vspace{5pt}\n\\noindent\n\\textbf{DTR learning with causal inference.}\nCausal inference \\cite{greenland1999causal,pearl2009causality,robins1995analysis} has been used to empower the learning process under noisy observation and can provide interpretability for decision-making models \\cite{schulam2017reliable,bica2020estimating,atan2018deep,bica2020time,johansson2016learning}. In this paper, we focus on the related work of DTR learning with causal inference. Zhang and Schaar \\cite{zhang2020gradient} propose a gradient regularized V-learning method to learn the value function of DTR with the consideration of time-varying confounders. Bica et al. \\cite{bica2020estimating} present a  Counterfactual Recurrent Network (CRN) to estimate treatment effects over time and recommend optimal treatments to patients.\nYang et al. \\cite{ciq} investigates the resilience ability of an RL agent to withstand adversarial and potentially catastrophic interferences and proposed a causal inference Q-network (CIQ) by training RL with additional inference labels to achieve high performance in the presence of interference. Bica et al. \\cite{cirl} propose a counterfactual inverse reinforcement learning (CIRL) by integrating counterfactual reasoning into batch inverse reinforcement learning. From a conceptual point of view, the studies most closely related to ours are \\cite{ciq,cirl} and we compare the proposed DAC with them. Both two studies incorporate causal inference into RL models. The key difference between ours and theirs are: (i) we resample the patients and the training DAC with balanced mini-batch can improve the model performance; (ii) we design a short-term reward that can further remove the confounding;\n(iii) our model is based on actor-critic framework and the critic network can provide more accurate rewards with the help of the patient resampling module and short-term reward;\n(iv) we introduce a policy adaptation method to the proposed DAC, which can efficiently adapt trained models to new-source environments.\n\n\n\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n \n\nIn this paper, we investigate the confounding issues and data imbalance problem in clinical settings, which could limit optimal DTR learning performance of RL models. The training of most existing DTR learning methods is guided by the long-term clinical outcomes (e.g., 90 day mortality), so some optimal treatment actions in the history of non-survivors might be punished. \nTo address the issues, we propose a new deconfounding actor-critic network (DAC) for mechanical ventilation dynamic treatment regime learning.\nWe propose a patient resampling module and a confounding balance module to alleviate the confounding issues.  \nMoreover, we introduce a policy adaptation method to the proposed DAC to transfer the learned DTR policies to new-source datasets. Experiments on a semi-synthetic   dataset and two publicly available real-world datasets (i.e., MIMIC-III and AmsterdamUMCdb) show that the proposed model outperforms state-of-the-art methods, demonstrating the effectiveness of the proposed framework.  \nThe proposed model can provide individualized treatment decisions that could improve patient outcomes. \n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{main}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\appendix\n\n\n\\clearpage\n\n% \\section{Description of Datasets}\n\n% We conduct comprehensive experiments on a semi-synthetic dataset and two real-world EHR datasets.\n\n% \\subsection{Description of Real-world Datasets}\n% \\textbf{MIMIC-III}: \n% Medical Information Mart for Intensive Care version III (MIMIC-III) \\cite{mimic} is a large, freely-available database comprising de-identified health-related data associated with over 40,000 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. It contains patientsâ€™ demographics and temporal information, including vital signs, lab tests, and treatment decisions. \n\n\n% \\textbf{AmsterdamUMCdb}: AmsterdamUMCdb is the first freely accessible European intensive care database. It is endorsed by the European Society of Intensive Care Medicine (ESICM) and its Data Science Section. It contains de-identified health data related to tens of thousands of intensive care unit admissions, including demographics, vital signs, laboratory tests and medications.\n\n\n% Both MIMIC-III\\footnote{\\url{https://mimic.physionet.org/}} and AmsterdamUMCdb\\footnote{\\url{https://amsterdammedicaldatascience.nl}} datasets are publicly available EHRs and can be accessed online. \n% Following \\cite{mechnical_ventilation},  we extract all adult patients undergoing invasive ventilation more than 24 hours and extract a set of 48 variables, including demographics, vital signs, laboratory values. Following \\cite{mechnical_ventilation}, \n% We learn the DTR policies for positive end-expiratory pressure (PEEP), fraction of inspired oxygen (FiO2) and ideal body weight-adjusted tidal volume (Vt). We discretize the action space into $7\\times 7 \\times 7$ actions. \n% % The extracted variables are listed in Table \\ref{tab:variables}. The construction of the action space is shown in Table \\ref{tab:dose_range}.\n% The extracted variables and the construction of the action space can be found in the github\\textcolor{red}{github link}.\n\n\n% \\begin{table*} [!t]\n% \\centering\n% \\caption{List of variables} \n% \\label{tab:variables} \n% \\begin{tabular}{c|l}\n% \\toprule\n% Category & Items\\\\\n% \\midrule\n% Static variables & \n% Age,Gender,Weight,Readmission to intensive, care, Elixhauser score (premorbid status) \\\\\n% \\hline\n% & Modified SOFA, SIRS, Glasgow coma scale, Heart rate, systolic, mean and diastolic, \\\\\n% & blood pressure, shock index, Respiratory rate, SpO2, Temperature \\\\\n\n% & Potassium, sodium, chloride, Glucose, BUN, creatinine, Magnesium, calcium, \\\\ \n% Time-varying & ionized calcium, carbon dioxide, SGOT, SGPT, total bilirubin, albumin, Hemoglobin, \\\\ \n% variables\n% & White blood cells count, platelets, count, PTT, PT, INR, pH, PaO2, PaCO2, base excess, \\\\\n% & bicarbonate, lactate, PaO2/FiO2 ratio, Mechanical ventilation, FiO2, \\\\\n% & IV fluid intake over 4h, vasopressor over 4h, Urine output over 4h, \\\\\n% & Cumulated fluid balance since admission (includes preadmission data when available)\\\\\n% \\hline\n% Treatment & positive end-expiratory pressure (PEEP), fraction of inspired oxygen (FiO2), \\\\\n% actions\n% & ideal body weight-adjusted tidal volume (Vt) \\\\\n% \\hline\n% Outcome & Hospital mortality 90-day mortality \\\\\n% \\bottomrule\n% \\end{tabular} \n% \\end{table*}\n\n\n% \\begin{table*} [!t]\n% \\centering\n% \\caption{Construction of the action space.} \n% \\label{tab:dose_range} \n% \\begin{tabular}{cccccccc}\n% \\toprule\n%   & 1 & 2 & 3 & 4 & 5 & 6 & 7\\\\\n%  \\midrule\n%  Vt (mL/Kg) & 0â€“2.5 & 2.5â€“5 & 5â€“7.5 & 7.5â€“10 & 10â€“12.5 & 12.5â€“15 & >15 \\\\\n%  PEEP (cmH2O) &  0â€“5  & 5â€“7 &  7â€“9 &  9â€“11 &  11â€“13 &  13â€“15 &  >15 \\\\\n%  FiO2 (\\%)  &  25â€“30  & 30â€“35  & 35â€“40 &  40â€“45 &  45â€“50 &  50â€“55 &  >55\\\\\n% \\bottomrule\n% \\end{tabular} \n% \\vspace{-4pt}\n% \\end{table*}\n\n \n\n\n% \\subsection{Semi-synthetic dataset based on MIMIC-III}\n\n% As the MIMIC-III dataset is real-world observational data, it is impossible to obtain the potential outcomes for underlying counterfactual treatment actions. To evaluate the proposed model's ability to learn optimal DTR policies, we further validate the method in a simulated environment. \n% The treatment assignments $a_t$ at each time stamp are influenced by the confounders $q_t$, which are consist of hidden confounders $s_{t}$ and time-varying covariates $o_t$. We first simulate $o_t$ and $s_{t}$ for each patient at time $t$ following $p$-order autoregressive process \\cite{mills1991time} as,\n% \\begin{equation}\n% \\begin{aligned}\n%     o_{t,j} = \\frac{1}{p}\\sum_{r=1}^{p}(\\alpha_{r,j}o_{t-r,j}+\\beta_{r}a_{t-r}) + \\eta_{t}\\\\\n%     s_{t,j} = \\frac{1}{p}\\sum_{r=1}^{p}(\\mu_{r,j}s_{t-r,j}+\\upsilon_{r}a_{t-r}) + \\epsilon_{t}\n% \\end{aligned}\n% \\end{equation}\n% where $o_{t,j}$ and $s_{t,j}$ denote the $j$-th column of $o_t$ and $s_{t}$, respectively. For each $j$, $\\alpha_{r,j},\\mu_{r,j}\\sim \\mathcal{N}(1-(r/p),(1/p)^{2})$ control the amount of historical information of last p time stamps incorporated to the current representations. $\\beta_{r},\\upsilon_{r}\\sim \\mathcal{N}(0, 0.02^{2})$ controls the influence of previous treatment assignments. $\\eta_{t},\\epsilon_{t}\\sim \\mathcal{N}(0,0.01^{2})$ are randomly sampled noises. \n\n% To simulate the treatment assignments, we generate $10,000$ survivor patients and $30,000$ non-survivor patients. \n% The confounders $q_t$ at time stamp $t$ and outcome $y$ can be simulated using the hidden confounders and current covariates as follows,\n% \\begin{equation}\n% \\label{eq:synthetic-outcome}\n% \\begin{aligned}\n% & q_t =  \\frac{1}{t}\\sum_{r=1}^{t}s_{r} +  g(o_t)\\\\\n% & y = w^{\\top}q_{T}+b\n% \\end{aligned}\n% \\end{equation} \n% where   $w\\sim\\mathcal{U}(-1,1)$ and $b\\sim\\mathcal{N}(0,0.1)$. The function $g(\\cdot)$ maps  $o_t$ into the hidden space. \n\n\n"
            },
            "section 7": {
                "name": "Semi-synthetic dataset based on MIMIC-III",
                "content": "\n\nAs the MIMIC-III dataset is real-world observational data, it is impossible to obtain the potential outcomes for underlying counterfactual treatment actions. To evaluate the proposed model's ability to learn optimal DTR policies, we further validate the method in a simulated environment. \nThe treatment assignments $a_t$ at each time stamp are influenced by the confounders $q_t$, which are consist of hidden confounders $s_{t}$ and time-varying covariates $o_t$. We first simulate $o_t$ and $s_{t}$ for each patient at time $t$ following $p$-order autoregressive process \\cite{mills1991time} as,\n\\begin{equation}\n\\begin{aligned}\n    o_{t,j} = \\frac{1}{p}\\sum_{r=1}^{p}(\\alpha_{r,j}o_{t-r,j}+\\beta_{r}a_{t-r}) + \\eta_{t}\\\\\n    s_{t,j} = \\frac{1}{p}\\sum_{r=1}^{p}(\\mu_{r,j}s_{t-r,j}+\\upsilon_{r}a_{t-r}) + \\epsilon_{t}\n\\end{aligned}\n\\end{equation}\nwhere $o_{t,j}$ and $s_{t,j}$ denote the $j$-th column of $o_t$ and $s_{t}$, respectively. For each $j$, $\\alpha_{r,j},\\mu_{r,j}\\sim \\mathcal{N}(1-(r/p),(1/p)^{2})$ control the amount of historical information of last p time stamps incorporated to the current representations. $\\beta_{r},\\upsilon_{r}\\sim \\mathcal{N}(0, 0.02^{2})$ controls the influence of previous treatment assignments. $\\eta_{t},\\epsilon_{t}\\sim \\mathcal{N}(0,0.01^{2})$ are randomly sampled noises. \n\nTo simulate the treatment assignments, we generate $10,000$ survivor patients and $30,000$ non-survivor patients. \nThe confounders $q_t$ at time stamp $t$ and outcome $y$ can be simulated using the hidden confounders and current covariates as follows,\n\\begin{equation}\n\\label{eq:synthetic-outcome}\n\\begin{aligned}\n& q_t =  \\frac{1}{t}\\sum_{r=1}^{t}s_{r} +  g(o_t)\\\\\n& y = w^{\\top}q_{T}+b\n\\end{aligned}\n\\end{equation} \nwhere   $w\\sim\\mathcal{U}(-1,1)$ and $b\\sim\\mathcal{N}(0,0.1)$. The function $g(\\cdot)$ maps  $o_t$ into the hidden space. \n\n"
            },
            "section 8": {
                "name": "Evaluation Metrics",
                "content": "\nThe evaluation metrics for treatment recommendation is still a challenge \\cite{kdd2018,rl_evaluation}. Thus we try different evaluation metrics to compare the proposed model with the state-of-art methods. \n\n\\noindent\n\\textbf{Estimated mortality}: Following \\cite{kdd2018,raghu2017deep,weng2017representation}, we use the estimated in-hospital mortality rates to measure whether policies would eventually reduce the patient mortality or not. \nSpecifically, we train a mortality risk prediction model, which takes the patient states and next actions as inputs, and output mortality risks. The predicted mortality risks are discretized into different units with small intervals shown in the x-axis of Figure \\ref{fig:predicted_mortality}. Discharged patients dominate both datasets, so the predicted mortality rates are smaller than the actual mortality rate in the real-world clinical setting. We adjusted the predicted mortality rate to calculate a new estimated mortality rate. \nGiven an example denoting an admission of a patient, if the patient died in hospital, all the predicted mortality rates belonging to this admission are associated with a value of mortality and the corresponding units add up these values. After scanning all test examples, the average estimated mortality rates for each unit are calculated, shown in y-axis of Figure \\ref{fig:predicted_mortality}. Based on these\nresults, the estimated mortality rates corresponding to the predicted mortality rate of different policies are used as the measurements to denote the estimated in-hospital mortality. Although the estimated mortality does not equal the mortality in the real-world clinical setting, it is a universal metric currently for computational testing. The relations between estimated mortality rate and predicted mortality probability are shown in Figure \\ref{fig:predicted_mortality}. \n\n\n \n\n\n\n\\noindent\n\\textbf{Weighted importance sampling(WIS)}: Following \\cite{aiclinician,raghu2017deep}, we also implement  a high-confidence off-policy evaluation (HCOPE) method (WIS). The human clinician policy is defined as $\\pi_0$, and $\\pi_1$ denotes the learned AI policy. We defined $\\rho_t = \\pi_1(a_t, s_t)/\\pi_0(a_t,s_t)$ as the per-step importance ratio, where $(a_t, s_t)$ represent the $t^{th}$ actual (action, state) pair for a patient. $\\rho_{1:t} = \\pi_{t'=1}^t \\rho_{t'}$ is the cumulative importance ratio up to step $t$ and $w_t = \\sum_{i=1}^{|D|} \\rho_{1:t}^{(i)}/ |D|$ denotes the average cumulative importance ratio at horizon $t$ in dataset $D$ and $|D|$ as the number of trajectories in $D$. The trajectory-wise WIS estimator is given by:\n\\vspace{-4mm} \n\\begin{equation}\n\\label{eq:vwis} \nV_{WIS} = \\frac{\\rho_{1:H}}{w_H}(\\sum_{t=1}^H \\gamma^{t-1}R_t),\n\\vspace{-1mm}\n\\end{equation}\nwhere $H$ denotes the length of steps for the patient and $R_t$ denote the long-term reward. \n% Following \\cite{aiclinician,raghu2017deep}, if a patient suffers from mortality in 90 days, the reward for the last action is set as -15. Otherwise, the reward for the last action is set as 15. \nThen, the WIS estimator is the average estimate over all trajectories, namely:\n\\vspace{-2mm}\n\\begin{equation}\n\\label{eq:wis} \nWIS = \\frac{1}{|D|}\\sum_{i=1}^{|D|} V_{WIS}^{(i)},\n\\vspace{-2mm}\n\\end{equation} \nwhere $V_{WIS}^{(i)}$ is WIS applied to the trajectory for $i^{th}$ patient.\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\n\\textbf{Action accuracy rate}: \nFollowing \\cite{cirl,ciq}, we compute the optimal action accuracy rate to evaluate the models' performance to learn optimal DTR policies in simulated environments.\nMechanical ventilator has three important parameters: PEEP, Vt and FiO2. We compute two kinds of accuracy rates: \\textbf{ACC-3} (whether the three parameters are set the same as the optimal action simultaneously) and \\textbf{ACC-1} (whether each parameter is set correctly).  The metrics are computed as follows:\n\\begin{equation*}\n\\label{eq:acc-3} \nACC-3 = \\frac{1}{|D|}\\sum_{i=1}^{|D|} \\frac{1}{T}\\sum_{t=1}^T f(a_t^p, \\hat{a}_t^p) * f(a_t^v, \\hat{a}_t^v) * f(a_t^f, \\hat{a}_t^f),\n\\end{equation*} \n\\begin{equation}\n\\label{eq:acc-1} \nACC-1 = \\frac{1}{|D|}\\sum_{i=1}^{|D|} \\frac{1}{T*3}\\sum_{t=1}^T f(a_t^p, \\hat{a}_t^p) + f(a_t^v, \\hat{a}_t^v) + f(a_t^f, \\hat{a}_t^f),\n\\end{equation} \n$$f(a,b)=\n\\begin{cases}\n1& \\text{if }a = b\\\\\n0& \\text{else }\n\\end{cases},$$ \nwhere $a_t^p$, $a_t^v$, $a_t^f$ are recommened actions for PEEP, Vt and FiO2, $\\hat{a}_t^p$, $\\hat{a}_t^v$, $\\hat{a}_t^f$ are optimal actions. \n\n"
            },
            "section 9": {
                "name": "Additional Experimental Results",
                "content": "\n\nThe relations between expected returns and mortality rates are shown in Figure \\ref{fig:q_value}. The results show that our model has a more clear negative correlation between expected returns and mortality rates than DQN in both MIMIC-III and AmsterdamUMCdb datasets. The reason might be two-fold: (i) DQN is trained on the initial EHR data with confounder bias; (ii) DQN punishes the actions used for patients who suffer from mortality, while some actions might be optimal. \n\n\n\n\n\n\\noindent\n\\textbf{Distribution of Actions}: \nVisualization of the action distribution in the 3-dimensional action space on AmsterdamUMCdb are shown in Figure \\ref{fig:distribution_ast}. The results show that the proposed model learned similar policies to clinicians. DAC suggests more actions with the lowest and highest PEEP and FiO2. Besides, the learning policies recommend more frequent lower tidal volume compared to clinician policy.\n\n\\noindent\n\\textbf{Comparison of Clinician and DAC policies}:\nWe find that the mortality rates are lowest in patients for whom cliniciansâ€™ actual treatments matched the actions the learned policies recommend both on MIMIC-III and AmsterdamUMCdb datasets. \nFigure~\\ref{fig:dose_diff_ast} shows the relations between mortality rate and mechanical ventilation setting difference on AmsterdamUMCdb.\n\n\n\\noindent\n\\textbf{Hyper-parameter optimization}: \nFigure \\ref{fig:para} shows the optimization of parameter $\\alpha$ on MIMIC-III dataset. We find the model performance is not sensitive when $0.05 \\leq \\alpha \\le 0.2$. We set $\\alpha=0.1$ when training the DAC model. Because the long-term rewards' value range (i.e., from -15 to +15)  is wider than short-term rewards' value range (i.e., from -1 to +1), the weight of long-term reward is smaller than the weight of short-term reward.\n\n\n\n\n"
            }
        },
        "tables": {
            "tab:notations": "\\begin{table}[!h]\n\\caption{Important Notations}\n\\label{tab:notations}\n    % \\centering\n\\Scale[0.9]{\n{\\renewcommand{\\arraystretch}{1.3}%\n\\begin{tabular}{ll}\\hline\nNotation    &   Definition  \\\\\\hline\n$\\mathcal{O}$     &   The space of time-varying covariates \\\\ \n$\\mathcal{A}$   &   The set of treatment options of interest   \\\\  \n$\\mathcal{S}$   &   The space of hidden confounders \\\\\n$o_{t}$   &   The time-varying covariates at time $t$  \\\\ \n$a_{t}$   &   The treatment assigned at time $t$  \\\\\n$s_{t}$   &   The hidden state at time $t$  \\\\\n$w_t$ & The reward weight at time $t$ \\\\\n$y$    &   The outcome \\\\   \n$\\pi_\\theta$ & The learned DTR policy \\\\\n$\\rho$ & The state distribution \\\\\n$R^l$ & The long-term reward \\\\\n$R^s$ & The short-term reward \\\\\n$Q$ & The reward for treatment actions \\\\\n$p^m$ & The patient mortality probability \\\\\n$\\alpha$ &  The hyper-parameter to adjust the weights of two rewards \\\\\n$w_*, b_*$ & The learnable parameters \\\\\n\\hline\n\\end{tabular}\n}\\quad }\n\\vspace{-10pt}\n\\end{table}",
            "tab:stati": "\\begin{table} [!t]\n\\centering\n\\caption{Statistics of MIMIC-III and AmsterdamUMCdb} \n\\label{tab:stati} \n\\begin{tabular}{ccc}\n\\toprule\n & MIMIC & AmsterdamUMCdb \\\\\n \\midrule\n \\#. of patients & 10,843 & 6,560 \\\\\n \\#. of male & 5,931 & 3,412\\\\\n \\#. of female & 4,912 & 3,148\\\\\n Age (mean $\\pm$ std) & 60.7 $\\pm$ 11.6 & 62.1 $\\pm$ 12.3 \\\\\n Mortality rate & 24\\% & 35\\% \\\\\n \\bottomrule\n\\end{tabular} \n\\end{table}",
            "tab:dose_range": "\\begin{table*} [!t]\n\\centering\n\\caption{Construction of the action space.} \n\\label{tab:dose_range} \n\\begin{tabular}{cccccccc}\n\\toprule\n  & 1 & 2 & 3 & 4 & 5 & 6 & 7\\\\\n \\midrule\n Vt (mL/Kg) & 0â€“2.5 & 2.5â€“5 & 5â€“7.5 & 7.5â€“10 & 10â€“12.5 & 12.5â€“15 & >15 \\\\\n PEEP (cmH2O) &  0â€“5  & 5â€“7 &  7â€“9 &  9â€“11 &  11â€“13 &  13â€“15 &  >15 \\\\\n FiO2 (\\%)  &  25â€“30  & 30â€“35  & 35â€“40 &  40â€“45 &  45â€“50 &  50â€“55 &  >55\\\\\n\\bottomrule\n\\end{tabular} \n\\vspace{-4pt}\n\\end{table*}",
            "tab:res": "\\begin{table*} [h]\n\\centering\n\\caption{Performance comparison for policy evaluation on test sets. \n% Performance comparison on test sets for prescription prediction. \nNote that RL and CI denote reinforcement learning and causal inference respectively.} \n\\label{tab:res}  \n\\begin{tabular}{llcc|cc|cc}\n\\toprule  \n &  & \\multicolumn{2}{c}{MIMIC} \n & \\multicolumn{2}{c}{AmsterdamUMCdb} \n & \\multicolumn{2}{c}{Semi-synthetic} \\\\ \n \\cline{3-8}\n &  & EM $\\downarrow$ & WIS $\\uparrow$ & ~~EM~~ $\\downarrow$ &  ~~WIS~~ $\\uparrow$ & ACC-3$\\uparrow$ & ACC-1$\\uparrow$ \\\\ \n \\hline  \n  & Imitation Learning $^S$            & 0.21 & 1.85  & 0.26 & 1.21  & 0.31 & 0.63\\\\\nSupervised\n  & Imitation Learning $^M$            & 0.23 & 1.84  & 0.28 & 0.95  &  0.27 & 0.61\\\\\n learning\n  & Imitation Learning $^A$            & 0.21 & 1.98  & 0.25 & 1.26  & 0.32 & 0.65\\\\\n \n  & MDP                                & 0.22 & 2.04  & 0.26 & 1.03  & 0.28 & 0.62\\\\\n \\hline\n \n \\multirow{4}{*}{RL} \n  & AI Clinician \\cite{aiclinician}    & 0.19 & 2.15 & 0.24 & 1.45  & 0.34 & 0.68\\\\\n  & VentAI \\cite{mechnical_ventilation}    & 0.19 & 2.21 & 0.24 & 1.46  & 0.34 & 0.69\\\\\n  & DQN \\cite{dqn}                     & 0.20 & 2.33 & 0.25 & 1.43  & 0.36 & 0.70\\\\\n  & MoE \\cite{amia2018}                & 0.19 & 2.29 & 0.24 & 1.40  & 0.36 & 0.69\\\\\n  & SRL-RNN \\cite{kdd2018}             & 0.19 & 2.47 & 0.25 & 1.58  & 0.37 & 0.70\\\\\n \\hline\n% Reweiting                    & 0.18 & 2.68 & 0.42 & 0.23 & 1.66 & 0.42\\\\\n  \n \\multirow{2}{*}{RL with CI} \n  & CIQ \\cite{ciq}                     & 0.18 & 2.68 & 0.24 & 1.68 & 0.41 & 0.72\\\\\n  \n  & CIRL \\cite{cirl}                   & 0.18 & 2.70 & 0.23 & 1.65 & 0.42 & 0.73\\\\\n \\hline\n \n \\multirow{5}{*}{Ours} \n  & DAC$^{-rsp}$           & 0.18 & 2.75 & 0.23 & 1.78 & 0.42 & 0.74\\\\\n  & DAC$^{-dcf}$      & 0.17 & 2.78 & 0.23 & 1.82 & 0.41 & 0.72\\\\\n  \n  & DAC$^{-short}$                     & 0.17 & 2.93 & \\textbf{0.22} & 1.89 & 0.44 & 0.74\\\\\n  & DAC$^{-long}$                      & 0.18 & 2.80 & 0.24 &  1.79 & 0.42 & 0.72 \\\\\n  & DAC                               & \\textbf{0.16} & \\textbf{3.13} & \\textbf{0.22} & \\textbf{2.03} & \\textbf{0.45} & \\textbf{0.76}\\\\ \n\n\\bottomrule \n\\end{tabular} \n\\end{table*}"
        },
        "figures": {
            "fig:model": "\\begin{figure*}[!t]\n\\centering  \n\\includegraphics[width=0.85\\textwidth]{pics/Mechnical.pdf} \n\\caption{\\textbf{Framework of proposed DTR learning model}. \\textbf{(a)} The causal graph of variables. $a_t$ denotes the assigned actions. The observational variables $o_t$ are covariates. $y$ denotes the final clinical outcomes. The patient health states $s_t$ are hidden confounders for both $a_t$ and $y$. \n\\textbf{(b)} Patient resampling operation. Non-survivors have more high-risk health states than survivors. The unbalanced data might introduce bias to learned DTR policies. We resample the patients according to their mortality risks such that both survivor and non-survivor groups follow similar mortality risk distributions.  \\textbf{(c)} Framework of the proposed model.\nGiven the resampled datasets, the embeddings of observed variable $o_t$ are sent to LSTM to model the patients' health state sequences. Actor network generates the probabilities for next actions based on the health states and critic network produces the short-term reward $R^s_t$ and long-term reward $R^l_t$ for the ($s_t, a_t$) pairs. Considering the causal relationship among states $s_t$, observations $o_t$, actions $a_t$ and outcome $y$, we compute an inverse weight $w_t$ at each time step $t$ for the rewards. \nThe actor network is trained by maximizing the expected weighted reward.} \n\\label{fig:model} \n\\end{figure*}",
            "fig:distribution_mimic": "\\begin{figure*}\n\\centering \n\\subfigure[]{\n\\includegraphics[width=0.28\\textwidth]{pics/MIMIC-III_FiO2_action_distribution.pdf}} \n\\subfigure[]{\n\\includegraphics[width=0.28\\textwidth]{pics/MIMIC-III_PEEP_action_distribution.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.28\\textwidth]{pics/MIMIC-III_TidalVolume_action_distribution.pdf}} \n\\caption{Visualization of the action distribution in the 3-dimensional action space on MIMIC-III dataset.  The horizontal axis denotes the discritized actions and the vertical axis denotes the distribution of corresponding actions.} \n\\label{fig:distribution_mimic} \n\\end{figure*}",
            "fig:dose_diff_mimic": "\\begin{figure*}\n\\centering \n\\subfigure[]{\n\\includegraphics[width=0.28\\textwidth]{pics/mimic.FiO2.diff.pdf}} \n\\subfigure[]{\n\\includegraphics[width=0.28\\textwidth]{pics/mimic.PEEP.diff.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.28\\textwidth]{pics/mimic.TidalVolume.diff.pdf}} \n\\caption{The relations between mortality rates and mechanical ventilation setting difference (recommended setting - actual setting) on MIMIC-III dataset. } \n\\label{fig:dose_diff_mimic} \n\\end{figure*}",
            "fig:transfer": "\\begin{figure}\n \\centering \n\\subfigure[]{\n\\includegraphics[width=0.23\\textwidth]{pics/transfer_em.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.23\\textwidth]{pics/transfer_wis.pdf}} \n\\caption{Performance of policy adaptation to AmsterdamUMCdb dataset over different training size. } \n\\vspace{-4pt}\n\\label{fig:transfer} \n\\end{figure}",
            "fig:predicted_mortality": "\\begin{figure}[h]\n\\centering\n\\subfigure[]{\n\\includegraphics[width=0.23\\textwidth]{pics/mimic.predicted_mortality.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.23\\textwidth]{pics/ast.predicted_mortality.pdf}}\n\\vspace{-4mm}\n\\caption{The positive correlations between estimated mortality rate and predicted mortality probability on MIMIC-III and AmsterdamUMCdb datasets. } \n\\label{fig:predicted_mortality}\n\\end{figure}",
            "fig:distribution_ast": "\\begin{figure*}[tbp]\n\\centering\n\\subfigure[]{\n\\includegraphics[width=0.25\\textwidth]{pics/AmsterdamUMCdb_FiO2_action_distribution.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.25\\textwidth]{pics/AmsterdamUMCdb_PEEP_action_distribution.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.25\\textwidth]{pics/AmsterdamUMCdb_TidalVolume_action_distribution.pdf}} \n\\vspace{-10pt}\n\\caption{Visualization of the action distribution in the 3-dimensional action space on AmsterdamUMCdb.} \n\\label{fig:distribution_ast}\n\\end{figure*}",
            "fig:dose_diff_ast": "\\begin{figure*}[!ht]\n\\centering\n\\subfigure[]{\n\\includegraphics[width=0.25\\textwidth]{pics/ast.FiO2.diff.pdf}} \n\\subfigure[]{\n\\includegraphics[width=0.25\\textwidth]{pics/ast.PEEP.diff.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.25\\textwidth]{pics/ast.TidalVolume.diff.pdf}}\n\\vspace{-10pt}\n\\caption{The relations between mortality rate and medicine dose gaps between human clinician and DAC policies on AmsterdamUMCdb. } \n\\label{fig:dose_diff_ast}\n\\end{figure*}",
            "fig:q_value": "\\begin{figure}[ht]\n\\centering\n\\subfigure[]{\n\\includegraphics[width=0.23\\textwidth]{pics/mimic.q_value.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.23\\textwidth]{pics/ast.q_value.pdf}}\n\\vspace{-4mm}\n\\caption{ Mortality-expected-return curve computed by the learned policies} \n\\label{fig:q_value}\n\\end{figure}",
            "fig:para": "\\begin{figure}[!h]\n\\centering\n\\subfigure[]{\n\\includegraphics[width=0.23\\textwidth]{pics/alpha_em.pdf}}\n\\subfigure[]{\n\\includegraphics[width=0.23\\textwidth]{pics/alpha_wis.pdf}}\n\\vspace{-4mm}\n\\caption{Hyper-parameter optimization } \n\\label{fig:para}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation} \n\\label{eq:weight}\nw_t = \\Pi^t_{\\tau=1} \\frac{f(a_\\tau|A_{\\tau - 1})}{f(a_\\tau|A_{\\tau-1},O_{\\tau -1})}\n=\\Pi^t_{\\tau=1} \\frac{f(a_\\tau|A_{\\tau - 1})}{\\pi^c(a_\\tau|s_\\tau)}\n%\\sum_i \\frac{I(a_t = a_i)*P(a_t|a_{t-1})}{\\pi^c(a_i|a_{t-1},s_t)},\n% p(a_0, a_1, ..., a_t) / p(a_0, ..., a_t, s_t) =\n\\end{equation}",
            "eq:2": "\\begin{equation} \n\\label{eq:value-embedding-1} \ne'^v_j = sin(\\frac{v*j}{V*k}), \\qquad\ne'^v_{k+j} = cos(\\frac{v*j}{V*k}), \n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:expected-reward}\nJ(\\pi_\\theta) = \\int_{s \\in \\mathcal{S}} \\rho(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s) Q(s,a) ds,\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:policy-gradient}\n\\begin{aligned}\n\\triangledown_\\theta J(\\pi_\\theta) = \\int_{s \\in \\mathcal{S}} \\rho(s) \\sum_{a \\in \\mathcal{A}} \\triangledown_\\theta \\pi_\\theta(a|s) Q(s,a) ds \\\\\n= E_{s \\sim \\rho, a \\in \\pi_\\theta} [\\triangledown_\\theta \\log \\pi_\\theta(a|s) Q(s,a)]\n\\end{aligned}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\label{eq:long-term-reward} \nR^l(s_t) = s_t w_l + b_l,\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\label{eq:long-term-reward-loss}\n\\begin{aligned}\nJ(w_l, b_l) = E_{s_t \\sim \\rho} [ R^l(s_t, a_t) - z_t)^2] \\\\\nz_t = R^m(s_t,a_t) + \\gamma R^l(s_{t+1}, \\hat{a}_{t+1}),\n\\end{aligned}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\label{eq:mortality-risk-prediction}\np_m(s_t) = Sigmoid (s_t w_m + b_m),\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\label{eq:mortality-risk-prediction-loss}\nJ(w_m, b_m) = E_{s_t \\sim \\rho} [-y\\log (p_m(s_t, a_t)) - (1-y) \\log (1 - p_m(s_t, a_t))]\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\label{eq:short-reward}\nR^s(s_t, a_t) = \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s_t) p_m(s_t, a) - p_m(s_t, a_t)\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\label{eq:weighted-reward}\nQ(s_t,a_t) = w_t ( \\alpha R^l(s_t,a_t) +  (1-\\alpha)R^s(s_t,a_t)),\n\\end{equation}",
            "eq:11": "\\begin{equation}\n\\label{eq:dynamic-function}\nf^T(s_t, a_t) = s_t w_d + b_d,\n\\end{equation}",
            "eq:12": "\\begin{equation}\n\\label{eq:MLE}\n\\begin{aligned}\nJ(w_d, b_d) = E_{s_t \\sim \\rho} [ f^T(s_t, a_t) - s_{t+1})^2]\n\\end{aligned}\n\\end{equation}",
            "eq:13": "\\begin{equation}\n\\pi_\\theta^T(s) = \\arg \\min _{a \\in A} ( f^T(s, a)  - f^S(s, \\pi_\\theta^S(s)))^2\n\\label{eq:transfer-learning} \n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\begin{aligned}\n    o_{t,j} = \\frac{1}{p}\\sum_{r=1}^{p}(\\alpha_{r,j}o_{t-r,j}+\\beta_{r}a_{t-r}) + \\eta_{t}\\\\\n    s_{t,j} = \\frac{1}{p}\\sum_{r=1}^{p}(\\mu_{r,j}s_{t-r,j}+\\upsilon_{r}a_{t-r}) + \\epsilon_{t}\n\\end{aligned}\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\label{eq:synthetic-outcome}\n\\begin{aligned}\n& q_t =  \\frac{1}{t}\\sum_{r=1}^{t}s_{r} +  g(o_t)\\\\\n& y = w^{\\top}q_{T}+b\n\\end{aligned}\n\\end{equation}",
            "eq:16": "\\begin{equation}\n\\label{eq:vwis} \nV_{WIS} = \\frac{\\rho_{1:H}}{w_H}(\\sum_{t=1}^H \\gamma^{t-1}R_t),\n\\vspace{-1mm}\n\\end{equation}",
            "eq:17": "\\begin{equation}\n\\label{eq:wis} \nWIS = \\frac{1}{|D|}\\sum_{i=1}^{|D|} V_{WIS}^{(i)},\n\\vspace{-2mm}\n\\end{equation}",
            "eq:18": "\\begin{equation*}\n\\label{eq:acc-3} \nACC-3 = \\frac{1}{|D|}\\sum_{i=1}^{|D|} \\frac{1}{T}\\sum_{t=1}^T f(a_t^p, \\hat{a}_t^p) * f(a_t^v, \\hat{a}_t^v) * f(a_t^f, \\hat{a}_t^f),\n\\end{equation*}",
            "eq:19": "\\begin{equation}\n\\label{eq:acc-1} \nACC-1 = \\frac{1}{|D|}\\sum_{i=1}^{|D|} \\frac{1}{T*3}\\sum_{t=1}^T f(a_t^p, \\hat{a}_t^p) + f(a_t^v, \\hat{a}_t^v) + f(a_t^f, \\hat{a}_t^f),\n\\end{equation}"
        },
        "git_link": "https://github.com/yinchangchang/DAC"
    }
}