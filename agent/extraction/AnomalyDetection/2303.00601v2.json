{
    "meta_info": {
        "title": "Multimodal Industrial Anomaly Detection via Hybrid Fusion",
        "abstract": "2D-based Industrial Anomaly Detection has been widely discussed, however,\nmultimodal industrial anomaly detection based on 3D point clouds and RGB images\nstill has many untouched fields. Existing multimodal industrial anomaly\ndetection methods directly concatenate the multimodal features, which leads to\na strong disturbance between features and harms the detection performance. In\nthis paper, we propose Multi-3D-Memory (M3DM), a novel multimodal anomaly\ndetection method with hybrid fusion scheme: firstly, we design an unsupervised\nfeature fusion with patch-wise contrastive learning to encourage the\ninteraction of different modal features; secondly, we use a decision layer\nfusion with multiple memory banks to avoid loss of information and additional\nnovelty classifiers to make the final decision. We further propose a point\nfeature alignment operation to better align the point cloud and RGB features.\nExtensive experiments show that our multimodal industrial anomaly detection\nmodel outperforms the state-of-the-art (SOTA) methods on both detection and\nsegmentation precision on MVTec-3D AD dataset. Code is available at\nhttps://github.com/nomewang/M3DM.",
        "author": "Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, Chengjie Wang",
        "link": "http://arxiv.org/abs/2303.00601v2",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "Accepted by CVPR 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n\n\n\n\nIndustrial anomaly detection aims to find the abnormal region of products and plays an important role in industrial quality inspection.\nIn industrial scenarios, it's easy to acquire a large number of normal examples, but defect examples are rare.\nCurrent industrial anomaly detection methods are mostly unsupervised methods, i.e., only training on normal examples, and testing on detect examples only during inference.\nMoreover, most existing industrial anomaly detection methods \\cite{mvtec, draem, ReverseDistillation, patchcore} are based on 2D images.\nHowever, in the quality inspection of industrial products, human inspectors utilize both the 3D shape and color characteristics to determine whether it is a defective product, where 3D shape information is important and essential for correct detection. \nAs shown in \\cref{fig:teaser}, for cookie and potato, it is hard to identify defects from the RGB image alone.\nWith the development of 3D sensors, recently MVTec-3D AD dataset \\cite{mvtec3dad} (\\cref{fig:teaser}) with both 2D images and 3D point cloud data has been released and facilitates the research on multimodal industrial anomaly detection.\n\nThe core idea for unsupervised anomaly detection is to find out the difference between normal representations and anomalies.%anomaly and normal representation.\nCurrent 2D industrial anomaly detection methods can be categorized into two categories:\n(1) Reconstruction-based methods.\nImage reconstruction tasks are widely used in anomaly detection methods \\cite{mvtec, memorizing-ae, reconstruction, draem, ReverseDistillation, ocgan} to learn normal representation. \nReconstruction-based methods are easy to implement for a single modal input (2D image or 3D point cloud).\nBut for multimodal inputs, it is hard to find a reconstruction target.\n(2)  Pretrained feature extractor-based methods.\nAn intuitive way to utilize the feature extractor is to map the extracted feature to a normal distribution and find the out-of-distribution one as an anomaly.\nNormalizing flow-based methods \\cite{cflow,fastflow,ast} use an invertible transformation to directly construct normal distribution, and memory bank-based methods\\cite{padim, patchcore} store some representative features to implicitly construct the feature distribution.\nCompared with reconstruction-based methods, directly using a pretrained feature extractor does not involve the design of a multimodal reconstruction target and is a better choice for the multimodal task.\nBesides that, current multimodal industrial anomaly detection methods \\cite{3d-ads, ast} directly concatenate the features of the two modalities together.\nHowever, when the feature dimension is high, the disturbance between multimodal features will be violent and cause performance reduction.\n\n\nTo address the above issues, we propose a novel multimodal anomaly detection scheme based on RGB images and 3D point cloud, named \\textit{Multi-3D-Memory (M3DM)}.\nDifferent from the existing methods that directly concatenate the features of the two modalities, we propose a hybrid fusion scheme to reduce the disturbance between multimodal features and encourage feature interaction.\nWe propose \\textit{Unsupervised Feature Fusion (UFF)} to fuse multimodal features, which is trained using a patch-wise contrastive loss to learn the inherent relation between multimodal feature patches at the same position. \nTo encourage the anomaly detection model to keep the single domain inference ability, we construct three memory banks separately for RGB, 3D and fused features.\nFor the final decision, we construct \\textit{Decision Layer Fusion (DLF)} to consider all of the memory banks for anomaly detection and segmentation.\n\n\nAnomaly detection needs features that contain both global and local information, \nwhere the local information helps detect small defects, \nand global information focuses on the relationship among all parts. \nBased on this observation, we utilize a Point Transformer\\cite{point_transformer, pointmae} for the 3D feature and Vision Transformer\\cite{vit, DINO} for the RGB feature. \nWe further propose a \\textit{Point Feature Alignment (PFA)} operation to better align the 3D and 2D features.\n\n\nOur contributions are summarized as follows:\n\\begin{itemize}\n  \\item We propose M3DM, a novel multimodal industrial anomaly detection method with hybrid feature fusion,\n  which outperforms the state-of-the-art detection and segmentation precision on MVTec-3D AD. \n  \\item We propose Unsupervised Feature Fusion (UFF) with patch-wise contrastive loss to encourage interaction between multimodal features.\n  \\item We design Decision Layer Fusion (DLF) utilizing multiple memory banks for robust decision-making.\n  \\item We explore the feasibility of the Point Transformer in multimodal anomaly detection and propose Point Feature Alignment (PFA) operation to align the Point Transformer feature to a 2D plane for high-performance 3D anomaly detection.\n\\end{itemize}\n\n\n"
            },
            "section 2": {
                "name": "Related Works",
                "content": "\n\\label{sec:related}\n\n\n\n\n\nFor most anomaly detection methods, the core idea is to find out good representations of the normal data.\nTraditional anomaly detection has developed several different roads.\nProbabilistic-based methods use empirical cumulative distribution functions\\cite{crosby1994detect, ecod} of normal samples to make decisions.\nThe position of representation space neighbors can also be used, and it can be done with several cluster methods, for example, k-NN\\cite{knn, fastknn}, correlation integral\\cite{loci} and histogram\\cite{histogram}.\nOutlier ensembles use a series of decision models to detect anomaly data, the most famous outlier ensembles method is Isolation Forest\\cite{isolationforest}.\nThe linear model can also be used in anomaly detection, for example simply using the properties of principal component analysis\\cite{PCA} or one-class support vector machine (OCSVM)\\cite{OCSVM}.\nThe traditional machine learning method relies on less training data than deep learning, so we capture this advantage and design a decision layer fusion module based on OCSVM and stochastic gradient descent.\n\n{\\bf 2D Industrial Anomaly Detection}\nIndustrial anomaly detection is usually under an unsupervised setting.\nThe MVTec AD dataset is widely used for industrial anomaly detection\\cite{mvtec} research, and it only contains good cases in the training dataset but contains both good and bad cases in the testing dataset.\nIndustrial anomaly detection needs to extract image features for decision, and the features can be used either implicitly or explicitly.\nImplicit feature methods utilize some image reconstruction model, for example, auto-encoder\\cite{mvtec, memorizing-ae, reconstruction} and generative adversarial network\\cite{ocgan};\nReconstruction methods could not recover the anomaly region, and comparing the generated image and the original image could locate the anomaly and make decisions.\nSome data augmentation methods\\cite{draem} were proposed to improve the anomaly detection performance, in which researchers manually add some pseudo anomaly to normal samples and the training goal is to locate pseudo anomaly.\nExplicit feature methods rely on the pretrained feature extractor, and additional detection modules learn to locate the abnormal area with the learned feature or representation.\nKnowledge distillation methods\\cite{ReverseDistillation} aim to learn a student network to reconstruct images or extract the feature, the difference between the teacher network and student network can represent the anomaly.\nNormalizing flow\\cite{cflow, fastflow} utilizes an invertible transformation to convert the image feature to Normal distribution, and the anomaly feature would fall on the edge of the distribution.\nActually, all of the above methods try to store feature information in the parameters of deep networks, recent work shows that simply using a memory bank\\cite{patchcore} can get a total recall on anomaly detection.\nThere are many similarities between 2D and 3D anomaly detection, we extend the memory bank method to 3D and multimodal settings and get an impressive result.\n\n\n\n\n{\\bf 3D Industrial Anomaly Detection}\nThe first public 3D industrial anomaly detection dataset is MVTec-3D\\cite{mvtec3dad} AD dataset, which contains both RGB information and point position information for the same instance. \nInspired by medical anomaly detection voxel auto-encoder and generative adversarial network\\cite{mvtec3dad} were first explored in 3D industrial anomaly detection, but those methods lost much spacial structure information and get a poor results. \nAfter that, a 3D student-teacher network\\cite{3d-st} was proposed to focus on local point clouds geometry descriptor with extra data for pretraining.\nMemory bank method\\cite{3d-ads} has also been explored in 3D anomaly detection with geometry point feature and a simple feature concatenation.\nKnowledge distillation method\\cite{ast} further improved the pure RGB and multimodal anomaly detection results with Depth information.\nOur method is based on memory banks, and in contrast to previous methods, we propose a novel pipeline to utilize a pretrained point transformer and a hybrid feature fusion scheme for more precise detection. \n\n\n"
            },
            "section 3": {
                "name": "Method",
                "content": "\n\\label{sec:method}\n\n\n\n",
                "subsection 3.1": {
                    "name": "Overview",
                    "content": "\n\\label{subsec:Over}\nOur Multi-3D-Memory (M3DM) method takes a 3D point cloud and an RGB image as inputs and conducts 3D anomaly detection and segmentation.\nWe propose a hybrid fusion scheme to promote cross-domain information interaction and maintain the original information of every single domain at the same time.\nWe utilize two pretrained feature extractors, DINO\\cite{DINO} for RGB and PointMAE\\cite{pointmae} for point clouds, to extract color and 3D representations respectively. \n\nAs shown in \\cref{fig:pipeline}, M3DM consists of three important parts: \n(1) Point Feature Alignment (PFA in \\cref{subsec:PFA}): to solve the position information mismatch problem of the color feature and 3D feature, we propose Point Feature Alignment to align the 3D feature to 2D space, which helps simplify multimodal interaction and promotes detection performance.\n(2) Unsupervised Feature Fusion (UFF in \\cref{subsec:UFF}): since the interaction between multimodal features can generate new representations helpful to anomaly detection \\cite{3d-ads, ast}, we propose an Unsupervised Feature Fusion module to help unify the distribution of multimodal features and learn the inherent connection between them.\n(3) Decision Layer Fusion (DLF in \\cref{subsec:DLF}): although UFF helps improve the detection performance, we found that information loss is unavoidable and propose Decision Layer Fusion to utilize multiple memory banks for the final decision.\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Point Feature Alignment",
                    "content": "\n\\label{subsec:PFA}\n\n{\\bf Point Feature Extraction.}\nWe utilize a Point Transformer ($\\mathcal F_{pt}$)~\\cite{point_transformer} to extract the point clouds feature.\nThe input point cloud $p$ is a point position sequence with $N$ points. After the farthest point sampling (FPS)~\\cite{pointnet++}, the point cloud is divided into $M$ groups, each with $S$ points.\nThen the points in each group are encoded into a feature vector, and the $M$ vectors are input into the Point Transformer.\nThe output $g$ from the Point Transformer are $M$ point features, \nwhich are then organized as point feature groups: each group has a single point feature, which can be seen as the feature of the center point.\n\n\n{\\bf Point Feature Interpolation.}\nSince after the farthest point sampling (FPS), the point center points are not evenly distributed in space, which leads to a unbalance density of point features.\nWe propose to interpolate the feature back to the original point cloud.\nGiven $M$ point features ${g_i}$ associated with $M$ group center points $c_i$, we use inverse distance weight to interpolate the feature to each point $p_j$ ($j \\in \\{ 1,2,...,N\\}$) in the input point clouds.\nThe process can be described as:\n\\begin{equation}\n\\begin{aligned}\n    p'_j = \\sum_{i=1}^{M} \\alpha_i g_i, \\quad\n    \\alpha_i = \\frac{ \\frac{1}{\\Vert c_i - p_j \\Vert_2+\\epsilon}} {\\sum_{k=1}^M \\sum_{t=1}^{N} \\frac{1}{\\Vert c_k - p_t \\Vert_2+\\epsilon}},\n    \\label{eq:interpolation}\n\\end{aligned}\n\\end{equation}\nwhere $\\epsilon$ is a fairly small constant to avoid $0$ denominator.\n\n\n{\\bf Point Feature Projection.}\nAfter interpolation, we project $p'_j$ onto the 2D plane using the point coordinate and camera parameters, and we denote the projected points as $\\hat{p}$.\nWe noticed that the point clouds could be sparse, if a 2D plane position doesn't match any point, we simply set the position as 0. \nWe denote the projected feature map as $\\{\\hat{p}_{x,y}|(x,y)\\in \\mathbb{D}\\}$ ($\\mathbb{D}$ is the 2D plane region of the RGB image), which has the same size as the input RGB image.\nFinally, we use an average pooling operation to get the patch feature on the 2D plane feature map.\n\n"
                },
                "subsection 3.3": {
                    "name": "Unsupervised Feature Fusion",
                    "content": "\n\\label{subsec:UFF}\n\n\n\n\nThe interaction between multimodal features can create new information that is helpful for industrial anomaly detection.\nFor example, in \\cref{fig:teaser}, we need to combine both the black color and the shape depression to detect the hole on the cookie.\nTo learn the inherent relation between the two modalities that exists in training data, we design the Unsupervised Feature Fusion (UFF) module. \nWe propose a patch-wise contrastive loss to train the feature fusion module:\ngiven RGB features $f_{rgb}$ and point clouds feature $f_{pt}$, we aim to encourage the features from different modalities at the same position to have more corresponding information, while the features at different positions have less corresponding information.\n\nWe denote the features of a patch as $\\{f^{(i,j)}_{rgb},f^{(i,j)}_{pt}\\}$, where $i$ is the index of the training sample and $j$ is the index of the patch.\nWe conduct multilayer perceptron (MLP) layers $\\{\\chi_{rgb}, \\chi_{pt} \\}$ to extract interaction information between two modals and use fully connected layers $\\{\\sigma_{rgb}, \\sigma_{pt} \\}$ to map processed feature to query or key vectors. We denote the mapped features as $\\{h^{(i,j)}_{rgb},h^{(i,j)}_{pt}\\}$.\nThen we adopt InfoNCE~\\cite{infonce} loss for the contrastive learning:\n\\begin{equation}\n\\mathcal{L}_{con} = \\frac{h_{rgb}^{(i,j)} \\cdot h_{pt}^{(i,j)}{}^T}{\\sum_{t=1}^{N_b} \\sum_{k=1}^{N_p} h_{rgb}^{(t,k)} \\cdot h_{pt}^{(t,k)}{}^T},\n\\end{equation}\nwhere  $N_b$ is the batch size and $N_p$ is the nonzero patch number.\nUFF is a unified module trained with all categories' training data of the MVTec-3D AD, and the architecture of UFF is shown in \\cref{fig:UFF}.\n\nDuring the inference stage, we concatenate the MLP layers outputs as a fused patch feature denoted as $f^{(i, j)}_{fs}$ :\n\\begin{equation}\n    f^{(i,j)}_{fs} = \\chi_{rgb}(f^{(i,j)}_{rgb}) \\oplus \\chi_{pt}(f^{(i,j)}_{pt}).\n\\end{equation}\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Decision Layer Fusion",
                    "content": "\n\\label{subsec:DLF}\n\n\nAs shown in \\cref{fig:teaser}, a part of industrial anomaly only appears in a single domain (e.g., the protruding part of potato), and the correspondence between multimodal features may not be extremely obvious.\nMoreover, although Feature Fusion promotes the interaction between multimodal features, we still found that some information has been lost during the fusion process.\n\n\nTo solve the above problem, we propose to utilize multiple memory banks to store the original color feature, position feature and fusion feature.\nWe denote the three kind of memory banks as $\\mathcal{M}_{rgb}, \\mathcal{M}_{pt}, \\mathcal{M}_{fs}$ respectively.\nWe refer PatchCore\\cite{patchcore} to build these three memory banks, and during inference, each memory bank is used to predict an anomaly score and a segmentation map.\nThen we use two learnable One-Class Support Vector Machines (OCSVM)\\cite{OCSVM} $\\mathcal D_a$ and $\\mathcal D_s$ to make the final decision for both anomaly score $a$ and segmentation map ${\\bf \\textit{S}}$. \nWe call the above process Decision Layer Fusion (DLF), which can be described as:\n\\begin{equation}\n    a = \\mathcal D_a (\\phi(\\mathcal{M}_{rgb},f_{rgb}), \\phi(\\mathcal{M}_{pt},f_{pt}), \\phi(\\mathcal{M}_{fs},f_{fs})),\n    \\label{eq:dlf_a}\n\\end{equation}\n\\begin{equation}\n\\small\n    \\textbf{\\it S} = \\mathcal D_s (\\psi (\\mathcal{M}_{rgb},f_{rgb}), \\psi(\\mathcal{M}_{pt},f_{pt}), \\psi(\\mathcal{M}_{fs},f_{fs})),\n    \\label{eq:dlf_s}\n\\end{equation}\nwhere $\\phi, \\psi$ are the score functions introduced by \\cite{patchcore}, which can be formulated as:\n\\begin{equation}\n     \\phi(\\mathcal{M}, f) = \\eta\\Vert f^{(i,j), *} - m^*\\Vert_2,\n\\end{equation}\n\\begin{equation}\n     \\psi(\\mathcal{M}, f) = \\{\\min _{m\\in \\mathcal{M}} \\Vert f^{(i,j)} - m\\Vert_2 \\Big{|} f^{(i,j)}\\in f\\},\n\\end{equation}\n\\begin{equation}\n     f^{(i,j), *}, m^* = \\arg \\max _{f^{(i,j)}\\in f} \\arg \\min _{m\\in \\mathcal{M}}\\Vert  f^{(i,j)} - m\\Vert_2,\n\\end{equation}\nwhere $\\mathcal{M} \\in \\{ \\mathcal{M}_{rgb}, \\mathcal{M}_{pt}, \\mathcal{M}_{fs} \\}$, $f \\in \\{f_{rgb}, f_{pt}, f_{fs}\\}$ and $\\eta$ is a re-weight parameter. \n\nWe propose a two-stage training procedure: in the first stage we construct memory banks, and in the second stage we train the decision layer. \nThe pseudo-code of DLF is shown as \\cref{alg:mmb}.\n\n\\begin{algorithm}\n\\footnotesize\n\\caption{Decision Layer Fusion Training}\\label{alg:mmb}\n\\KwIn {Memory bank building algorithm $\\mathcal P$\\cite{patchcore}, %anomaly score functions $\\{\\phi, \\psi\\}$, \ndecision layer $\\{\\mathcal D_a,\\mathcal D_s\\}$, OCSVM loss function $\\mathcal{L}_{oc}$\\cite{OCSVM}}\n\\KwData {Training set features $\\{\\mathbb F_{rgb},\\mathbb F_{pt},\\mathbb F_{fs}\\}$.}\n\\KwOut {Multimodal memory banks $\\{ \\mathcal{M}_{rgb}, \\mathcal{M}_{pt}, \\mathcal{M}_{fs}\\}$, decision layer parameters $\\{\\Theta_{\\mathcal D_a}, \\Theta_{\\mathcal D_s}\\}$.}\n\n\\For{$ modal \\in \\{rgb, pt ,fs\\}$}{\n    \\For{$f_{modal}\\in \\mathbb F_{modal}$}{\n        $\\mathcal{M}_{modal} \\leftarrow f_{modal}$\n    }\n    $\\mathcal{M}_{modal} \\leftarrow \\mathcal P(\\mathcal{M}_{modal})$\n}\n\\For{$f_{rgb}\\in \\mathbb F_{rgb}, f_{pt} \\in  \\mathbb F_{pt}, f_{fs} \\in \\mathbb F_{fs}$}{\n{   \\small\n    % Optimizing:\n    $ \\Theta_{\\mathcal D_a} \\stackrel{optim}{\\longleftarrow} \\mathcal{L}_{oc}(\\mathcal D_a; \\Theta_{\\mathcal D_a})$ \\\\\n    $ \\Theta_{\\mathcal D_s} \\stackrel{optim}{\\longleftarrow} \\mathcal{L}_{oc}(\\mathcal D_s; \\Theta_{\\mathcal D_s})$\n    % $\\mathcal D_a (\\phi(\\mathcal{M}_{rgb},f_{rgb}), \\phi(\\mathcal{M}_{pt},f_{pt}),  \\phi(\\mathcal{M}_{fs},f_{fs}); \\Theta_{D_a})$\n    % $\\mathcal D_s (\\psi(\\mathcal{M}_{rgb},f_{rgb}), \\psi(\\mathcal{M}_{pt},f_{pt}),  \\psi(\\mathcal{M}_{fs},f_{fs}); \\Theta_{D_s})$\n    }\n}\n\\end{algorithm}\n\\vspace{-15pt}\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:experiments}\n\n",
                "subsection 4.1": {
                    "name": "Experimental Details",
                    "content": "\n\\label{sec:experiments_detail}\n\n{\\bf Dataset.}\n3D industrial anomaly detection is in the beginning stage. \nThe MVTec-3D AD dataset is the first 3D industrial anomaly detection dataset.\nOur experiments were performed on the MVTec-3D dataset.\n\nMVTec-3D AD\\cite{mvtec3dad} dataset consists of 10 categories, a total of 2656 training samples, and 1137 testing samples.\nThe 3D scans were acquired by an industrial sensor using structured light, and position information was stored in 3 channel tensors representing $x$, $y$ and $z$ coordinates.\nThose 3 channel tensors can be single-mapped to the corresponding point clouds.\nAdditionally, the RGB information is recorded for each point.\nBecause all samples in the dataset are viewed from the same angle, the RGB information of each sample can be stored in a single image.\nTotally, each sample of the MVTec-3D AD dataset contains a colored point cloud.\n\n\n{\\bf Data Preprocess.}\nDifferent from 2D data, 3D ones are easier to remove the background information.\nFollowing \\cite{3d-ads}, we estimate the background plane with RANSAC\\cite{RANSAC} and any point within 0.005 distance is removed.\nAt the same time, we set the corresponding pixel of removed points in the RGB image as 0.\nThis operation not only accelerates the 3D feature processing during training and inference but also reduces the background disturbance for anomaly detection.\nFinally, we resize both the position tensor and the RGB image to $224 \\times 224$ size, which is matched with the feature extractor input size.\n\n{\\bf Feature Extractors.}\nWe use 2 Transformer-based feature extractors to separately extract the RGB feature and point clouds feature:\n1) For the RGB feature, we use a Vision Transformer (ViT)\\cite{vit} to directly extract each patch feature, and in order to adapt to the anomaly detection task, we use a ViT-B/8 architecture for both efficiency and detection grain size;\nFor higher performance, we use the ViT-B/8 pretrained on ImageNet\\cite{imagenet} with DINO\\cite{DINO}, and this pretrained model recieves a $224 \\times 224$ image and outputs totally $784$ patches feature for each image;\nSince previous research shows that ViT concentrated on both global and local information on each layer, we use the output of the final layer with $768$ dimensions for anomaly detection. \n2) For the point cloud feature, we use a Point Transformer\\cite{point_transformer, pointmae}, which is pretrained on ShapeNet\\cite{shapenet} dataset, as our 3D feature extractor, and use the $\\{3, 7, 11\\}$ layer output as our 3D feature; Point Transformer firstly encodes point cloud to point groups which are similar with patches of ViT and each group has a center point for position and neighbor numbers for group size.\nAs described in \\cref{subsec:PFA}, we separately test the setting $M=784, S=64$ and $M=1024, S=128$ for our experiments.\nIn the PFA operation, we separately pool the point feature to $28 \\times 28$ and $56 \\times 56$ for testing.\n\n{\\bf Learnable Module Details. }\nM3DM has 2 learnable modules: the Unsupervised Feature Fusion module and the Decision Layer Fusion module. 1) For UFF, the $\\chi_{rgb}, \\chi_{pc}$ are 2 two-layer MLPs with $4\\times$ hidden dimension as input feature;  We use AdamW optimizer, set learning rate as 0.003 with cosine warm-up in 250 steps and batch size as 16, we report the best anomaly detection results under 750 UFF training steps. \n2) For DLF, we use two linear OCSVMs with SGD optimizers, the learning rate is set as $1\\times10^{-4}$ and train 1000 steps for each class.\n\n\n\n{\\bf Evaluation Metrics.}\nAll evaluation metrics are exactly the same as in \\cite{mvtec3dad}.\nWe evaluate the image-level anomaly detection performance with the area under the receiver operator curve (I-AUROC), and higher I-AUROC means better image-level anomaly detection performance. \nFor segmentation evaluation, we use the per-region overlap (AUPRO) metric, %\\cameraready{also} following \\cite{mvtec3dad}, \nwhich is defined as the average relative overlap of the binary prediction with each connected component of the ground truth.\nSimilar to I-AUROC, the receiver operator curve of pixel level predictions can be used to calculate P-AUROC for evaluating the segmentation performance.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Anomaly Detection on MVTec-3D AD",
                    "content": "\n\\label{subsec:ad_results}\n\n\n\nWe compare our method with several 3D, RGB and RGB + 3D multimodal methods on MVTec-3D, \\cref{tab:iaucroc} shows the anomaly detection results record with I-AUROC, \\cref{tab:aupro} shows the segmentation results report with AUPRO and we report the P-AUROC in supplementary materials.\n1) On pure 3D anomaly detection we get the highest I-AUROC and outperform AST\\cite{ast} 4.1\\%, which shows our method has much better detection performance than the previous method, and with our PFA, the Point Transformer is the better 3D feature extractor for this task; for segmentation, we get the second best result with AUPRO as 0.906, since our 3D domain segmentation is based on the point cloud, we find there is a bias between point clouds and ground truth label and discuss this problem in \\cref{subsec:discuss}.\n2) On RGB anomaly detection, the difference between our method and Patchcore\\cite{patchcore} is that we use a Transformer based feature extractor instead of a Wide-ResNet one and remove the pooling operation before building the memory bank; Our I-AUROC in RGB domain is 8.0\\% higher than the original PatchCore results and get the highest AUPRO score for segmentation, which is 7.6\\% higher than the second best one.\n3) On RGB + 3D multimodel anomaly detection, our method gets the best results on both I-AUROC and AUPRO scores, we get 0.8\\% better I-AUROC than the AST and 0.5\\% better AUPRO than the PatchCore + FPFH \\cite{3d-ads}; These results are contributed by our fusion strategy and the high-performance 3D anomaly detection results. The previous method couldn't have great detection and segmentation performance at the same time, as shown in \\cref{tab:p-aucroc}. Since the AST\\cite{ast} didn't report the AUPRO results, we compare the segmentation performance with P-AUROC here.\nAlthough PatchCore + FPFH method gets a high P-AUROC score, its I-AUROC is much lower than the other two.\nBesides, AST gets a worse P-AUROC score than the other two methods, which means the AST is weak in locating anomalies.\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Study",
                    "content": "\n\\label{subsec:ablation}\n\nWe conduct an ablation study on a multimodal setting, and to demonstrate our contributions to multimodal fusion, we analyze our method in \\cref{tab:ablation} with the following settings:\n1) Only Point Clouds ($\\mathcal{M}_{pt}$) information; \n2) Only RGB ($\\mathcal{M}_{rgb}$) information; \n3) Single memory bank ($\\mathcal{M}_{fs}$) directly concatenating Point Transformer feature and RGB feature together;\n4) Single memory bank ($\\mathcal{M}_{fs}$) using UFF to fuse multimodal features;\n5) Building two memory banks ($\\mathcal{M}_{rgb},\\mathcal{M}_{pt}$) separately and directly adding the scores together; \n6) Building two memory banks separately ($\\mathcal{M}_{rgb},\\mathcal{M}_{pt}$) and using DLF for the final result;\n7) Building three memory banks ($\\mathcal{M}_{rgb},\\mathcal{M}_{pt},\\mathcal{M}_{fs}$) (Ours).\nComparing row 3 and 4 in \\cref{tab:ablation}, we can find that adding UFF greatly improves the results on all three metrics (I-AUROC 4.1\\% $\\uparrow $, AURPO 1.2\\% $\\uparrow $ and P-AUROC 0.3\\% $\\uparrow $), which shows UFF plays an important role for multimodal interaction and helps unify the feature distribution;\nCompare row 5 and 6, we demonstrate that DLF model helps improve both anomaly detection and segmentation performance (I-AUROC 0.3\\% $\\uparrow $, AURPO 0.6\\% $\\uparrow $ and P-AUROC 0.3\\% $\\uparrow $). \nOur full set is shown in row 7 in \\cref{tab:ablation}, and compared with row 6 we have 1.3\\% I-AUROC and 0.5\\% AUPRO improvement, which further demonstrates that UFF activates the interaction between two modals and creates a new feature for anomaly detection.   \n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Analysis of PFA Hyper-parameter",
                    "content": "\n\\label{subsec:PointTransformerSetting}\n\nSince we are the first to use Point Transformer for 3D anomaly detection, we conduct a series of exploring experiments on the Point Transformer setting.\n1) We first explore two important hyper-parameters of Point Transformer: the number of groups and the groups' size during farthest point sampling.\nThe number of groups decides how many features will be extracted by the Point Transformer and the groups' size is equal to the concept of the receptive field.\nAs shown in \\cref{tab:pt-setting}, the model with 1,024 point groups and 128 points per group performs better in this task,\nwhich we think is because more feature vectors help the model find refined representation and a suitable neighbor number would give more local position information.\n2) To verify the PFA operation, we conduct another 3D anomaly detection experiment with the original point groups feature: a point group can be seen as a {\\it patch}, and the memory bank store point groups feature here; \nThe detection method is as same as the patch-based one, and to get the segmentation predictions, we first project point group feature to a 2D plane and use an inverse distance interpolation to get every pixel value; \nAs shown in \\cref{tab:pt-setting} row 2, the group-based method has better performance than its peer in row 4, however, when the patch-size gets smaller, the PFA-based method gets the best result on three metrics.% in row 5.\n\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Analysis of Multimodal Feature distribution",
                    "content": "\n\\label{subsec:distribution}\n\nWe visualize the feature distribution with histogram and t-SNE\\cite{t-SNE}. \nThe original point cloud features have two disconnected regions in the t-SNE map (\\cref{fig:tsne}), and it is caused by the pooling operation on the edge between the non-point region and the point cloud region.\nThe two fused features have similar distribution (in the \\cref{fig:hist}), and these properties make the concatenated feature more suitable for memory bank building and feature distance calculation.\nThe original features have a more complex distribution, which is helpful for single-domain anomaly detection.\nOur hybrid fusion scheme integrates the advantage of both original features and fused features and thus has a better performance than the single memory bank method.\n\n\n\n\n"
                },
                "subsection 4.6": {
                    "name": "Few-shot Anomaly detection",
                    "content": "\n\\label{subsec:few-shot}\n\nWe evaluate our method on Few-shot settings, and the results are illustrated in \\cref{tab:few-shot}.\nWe randomly select 10 and 5 images from each category as training data and test the few-shot model on the full testing dataset.\nWe find that our method in a 10-shot or 5-shot setting still has a better segmentation performance than some non-few-shot methods.\n\n\n\n"
                },
                "subsection 4.7": {
                    "name": "Discussion about the MVTec-3D AD",
                    "content": "\n\\label{subsec:discuss}\n\n\nIn this section, we discuss some properties of the MVTec-3D AD dataset.\n1) The 3D information helps detect more kinds of anomalies.\nIn the first row of \\cref{fig:discuss}, we can find that the model fails to detect the anomaly with RGB information, but with the point cloud, the anomaly is accurately predicted.\nThis indicates that 3D information indeed plays an important role in this dataset. \n2) The label bias will cause inaccurate segmentation.\nAs shown in \\cref{fig:discuss}, the cookie of row 2 has some cuts, and the anomaly label is annotated on the missing area.\nHowever, for the pure point clouds method, the non-point region will not be reported, instead, the cut edge will be reported as an anomaly region, the phenomenon can be seen in the PC prediction of cookie in the \\cref{fig:discuss}.\nBecause of this kind of bias between 3D point clouds and the 2D ground truth, the 3D version has a lower AUPRO score than the RGB one in \\cref{tab:aupro}.\nwith the RGB information, the missing region will be more correctly reported as an anomaly.\nAlthough we successfully predict more anomaly areas with multimodal data, there is still a gap between the prediction map and the ground truth.\nWe will focus on resolving this problem in future research. \n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\n\nIn this paper, we propose a multimodal industrial anomaly detection method with point clouds and RGB images.\nOur method is based on multiple memory banks and we propose a hybrid feature fusion scheme to process the multimodal data.\nIn detail, we propose a patch-wise contrastive loss-based Unsupervised Feature Fusion to promote multimodal interaction and unify the distribution,\nand then we propose Decision Layer Fusion to fuse multiple memory bank outputs.\nMoreover, we utilize pretrained Point Transformer and Vision Transformer as our feature extractors, and to align the above two feature extractors to the same spatial position, we propose Point Feature Alignment to convert 3D features to a 2D plane.\nOur method outperforms the SOTA results on MVTec-3D AD datasets and we hope our work be helpful for further research.\n\n\\noindent{\\bf Acknowledgements.}\nThis work was supported by  National Natural Science Foundation of China (72192821, 61972157, 62272447), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), Shanghai Science and Technology Commission (21511101200), Shanghai Sailing Program (22YF1420300, 23YF1410500), CCF-Tencent Open Research Fund (RAGR20220121) and Young Elite Scientists Sponsorship Program by CAST (2022QNRC001).\n\n%%%%%%%%% REFERENCES\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egbib}\n}\n\n\\clearpage\n\\appendix\n\\begin{subappendices}\n\\setcounter{table}{0}\n\\setcounter{figure}{0}\n\\renewcommand\\thesection{\\Alph{section}}\n\\renewcommand\\thetable{\\Roman{table}}\n\\renewcommand\\thefigure{\\Roman{figure}}\n\n\\section*{Overview}\n\\label{sec:overview}\nThis supplementary material includes:\n\\begin{itemize}\n\\item The implementation details about the hardware and software packages (\\cref{sec:implementation});\n\\item P-AUROC score for anomaly segmentation (\\cref{sec:pauroc});\n\\item Detailed score for ablation study of all categories of MVTec-3D AD (\\cref{sec:ablation});\n\\item Detailed score for Point Transformer setting of all categories of MVTec-3D AD (\\cref{sec:transformer});\n\\item Detailed score for few-shot setting of all categories of MVTec-3D AD (\\cref{sec:few-shot});\n\\item Discussion about Backbone Choices (\\cref{sec:backbone});\n\\item The results of all categories of Eyecandies (\\cref{sec:eyecandies});\n\\item The visualization results of all categories of MVTec-3D AD (\\cref{sec:visual}).\n\\end{itemize}\n\n\\section{Implementation Details}\n\\label{sec:implementation}\n\nWe implement M3DM with Pytorch\\footnote{https://pytorch.org/} and Scikit-Learn package\\footnote{https://scikit-learn.org/}. \nThe feature extractors and memory banks algorithm are based on Pytorch and we use the Scikit-Learn package for OCSVM\\cite{OCSVM}.\nThe AUROC calculation also relies on Scikit-Learn package.\nAll experiments are run on a single Nvidia Tesla V100 and cost at most 50 GB of memories for the full setting.\n\n\\section{P-AUROC for Segmentation}\n\\label{sec:pauroc}\n%The P-AUROC is not widely reported in the previous work, and it can't reflect the segmentation result as well as the AUPRO metric.\nIn the main paper, we report the AUPRO score for anomaly segmentation.\nIn this section, we report the P-AUROC score to further verify the segmentation performance of our method, as shown in \\cref{tab:pauroc}. \nWe mainly compare our results with FPFH \\cite{3d-ads}, PatchCore \\cite{patchcore} and AST \\cite{ast}\\footnote{Since AST \\cite{ast} only provided the mean score in its paper, we simply illustrate the mean score of AST.}.\nFor the multimodal input, we get the same score as PatchCore + FPFH method and is 1.6\\% higher than the AST.\nFor single RGB input, we still have a 2\\% improvement over PatchCore.\nFor 3D segmentation, similar to the AUPRO results reported in the main paper, our 3D segmentation results are a little bit lower than the FPFH-based method, and we believe this is also caused by the bias between the label and the point clouds we discuss in Section 4.7 in the main paper.\nThe P-AUROC is a saturated metric for anomaly segmentation, and the difference between methods is smaller than the difference in AUPRO.\n\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}cl|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n    & Method & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n    \\midrule\n     \\multirow{2}*{\\rotatebox{90}{3D}} &FPFH\\cite{3d-ads} &    0.994&    0.966&    0.999&    0.946&    0.966& 0.927&    0.996&    0.999&    0.996&    0.990&    {\\bf 0.978}\\\\\n     ~&Ours &  0.981&  0.949&  0.997&  0.932&  0.959&  0.925&  0.989&  0.995&  0.994&  0.981&  0.970\\\\\n    \\midrule\n     \\multirow{2}*{\\rotatebox{90}{RGB}} &PatchCore\\cite{patchcore}&  0.983&  0.984&  0.980&  0.974&  0.972&  0.849&  0.976&  0.983&  0.987&  0.977&  0.967\\\\\n     & Ours & 0.992 & 0.990 & 0.994 &  0.977 & 0.983 &  0.955 & 0.994 & 0.990 &  0.995 &   0.994& {\\bf 0.987} \\\\\n    % & & & & & & & & & & &\\\\\n    \\midrule\n     \\multirow{3}*{\\rotatebox{90}{RGB+3D}} &AST\\cite{ast} & -& -& -& -& -& -& -& -& -& -& 0.976  \\\\\n     &PatchCore + FPFH\\cite{3d-ads}& 0.996 & 0.992 & 0.997 & 0.994 & 0.981 & 0.974 & 0.996 & 0.998 & 0.994 & 0.995 & {\\bf 0.992} \\\\\n     &Ours & 0.995 &  0.993 & 0.997 & 0.985 & 0.985 & 0.984 & 0.996 & 0.994  & 0.997 & 0.996 & {\\bf 0.992}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{P-AUROC score for anomaly segmentation of all categories of MVTec-3D AD\\cite{mvtec3dad} dataset. The P-AUROC is a saturated metric for anomaly segmentation, and the difference between methods is smaller than the AUPRO.}\n  \\label{tab:pauroc}\n\\end{table*}\n\n\\section{Detailed Results of Ablation Study}\n\\label{sec:ablation}\n\n\nIn the main paper Section 4.3, we conduct ablation studies on UFF, DLF, and multiple memory banks.\nIn this section, we report the detailed ablation study results of all categories of MVTec-3D AD.\n\\cref{tab:ablation_iauroc} and \\cref{tab:ablation_aupro} separately  illustrate the I-AUPRO and AUPRO scores with the following settings:\n1) Only Point Clouds ($\\mathcal{M}_{pt}$) information; \n2) Only RGB ($\\mathcal{M}_{rgb}$) information; \n3) Single memory bank ($\\mathcal{M}_{fs}$) directly concatenating Point Transformer feature and RGB feature together;\n4) Single memory bank ($\\mathcal{M}_{fs}$) using UFF to fuse multimodal features;\n5) Building two memory banks ($\\mathcal{M}_{rgb},\\mathcal{M}_{pt}$) separately and directly adding the scores together; \n6) Building two memory banks separately ($\\mathcal{M}_{rgb},\\mathcal{M}_{pt}$) and using DFL for the final result;\n7) Building three memory banks ($\\mathcal{M}_{rgb},\\mathcal{M}_{pt},\\mathcal{M}_{fs}$) (Ours).\nWith the UFF, the Foam, Cookie, and Peach have a great improvement to the single domain input and the w/o UFF version, which means the UFF encourages the interaction between multimodal features and creates useful information for anomaly detection and segmentation.\n% \u4e00\u8d77\u6548\u679c\u66f4\u597d\nWith double memory banks, the Carrot, Cookie and Potato score have an improvement, and the DLF help improve the hard categories such as Cable Gland and Tire.\nWith three memory banks, most advantages of DLF and UFF have been maintained, and our full setting gets the best results, which indicates DLF and UFF complements each other and jointly achieves the best performance. \n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}l|c|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n     Method & Memory Banks & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n    \\midrule\n     Only PC & $\\mathcal{M}_{pt}$ & 0.941 & 0.651 & 0.965 & 0.969 & 0.905 & 0.760 & 0.880 & 0.974 & 0.926& 0.765 & 0.874\\\\\n     Only RGB & $\\mathcal{M}_{rgb}$ & 0.944 & 0.918 & 0.896 & 0.749 & 0.959 & 0.767 & 0.919 & 0.648 & 0.938 & 0.767 & 0.850 \\\\\n     w/o  UFF & $\\mathcal{M}_{fs}$ & 0.920 & 0.900 & 0.914  & 0.727 & 0.963 & 0.795 & 0.946 & 0.656 & 0.954 & 0.792 & 0.857 \\\\\n     w/ UFF & $\\mathcal{M}_{fs}$ & 0.976 & 0.895 & 0.922 & 0.912 & 0.949 & 0.868 & 0.978 & 0.723 & 0.960 & 0.798 & {\\bf 0.898}\\\\\n    \\midrule\n     w/o DLF & $\\mathcal{M}_{pt}, \\mathcal{M}_{rgb}$ & 0.981 & 0.831 & 0.980 & 0.985 & 0.960 & 0.905 & 0.936 & 0.964 & 0.967 & 0.780 & 0.929\\\\\n     w/ DLF & $\\mathcal{M}_{pt}, \\mathcal{M}_{rgb}$ & 0.980 & 0.880 & 0.975 & 0.965 & 0.947 &0.910  & 0.943  & 0.927 & 0.958 & 0.840 & {\\bf 0.932}\\\\\n    \\midrule\n     Ours & $\\mathcal{M}_{pt}, \\mathcal{M}_{rgb}, \\mathcal{M}_{fs}$ &  0.994 & 0.909 & 0.972 &  0.976 &  0.960 &  0.942 & 0.973 & 0.899& 0.972 & 0.850 & {\\bf 0.945}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Detailed I-AUROC score for ablation on anomaly detection of all categories of MVTec-3D AD.}\n  \\label{tab:ablation_iauroc}\n\\end{table*}\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}l|c|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n     Method & Memory Banks & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n    \\midrule\n     Only PC & $\\mathcal{M}_{pt}$ & 0.943 & 0.818 & 0.977 & 0.882 & 0.881 & 0.743 & 0.958 & 0.974 & 0.950 & 0.929 & 0.906\\\\\n     Only RGB & $\\mathcal{M}_{rgb}$ & 0.952 & 0.972& 0.973 & 0.891 & 0.932& 0.843 & 0.970 & 0.956& 0.968 & 0.966& 0.942\\\\\n     w/o  UFF & $\\mathcal{M}_{fs}$ & 0.951 & 0.971 & 0.974 & 0.893 & 0.935 & 0.855 & 0.972 & 0.958 & 0.969 & 0.967 & 0.944\\\\\n     w/ UFF & $\\mathcal{M}_{fs}$ &  0.963 & 0.964 & 0.978  &0.930  & 0.946 & 0.896 & 0.974 & 0.966 & 0.972 & 0.972 & {\\bf 0.956}\\\\\n    \\midrule\n     w/o DLF & $\\mathcal{M}_{pt}, \\mathcal{M}_{rgb}$ & 0.968 & 0.925 & 0.979 & 0.914 & 0.909 & 0.948 & 0.975 & 0.976 & 0.967 & 0.965 & 0.953\\\\\n     w/ DLF & $\\mathcal{M}_{pt}, \\mathcal{M}_{rgb}$ & 0.965 & 0.968 & 0.978 & 0.933 & 0.933 & 0.927 & 0.976 & 0.967 & 0.971 & 0.973 & {\\bf 0.959}\\\\\n    \\midrule\n     Ours & $\\mathcal{M}_{pt}, \\mathcal{M}_{rgb}, \\mathcal{M}_{fs}$ & 0.970 & 0.971 & 0.979 & 0.950 & 0.941 & 0.932 & 0.977 & 0.971  & 0.971 & 0.975 & {\\bf 0.964}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Detailed AUPRO score for ablation anomaly segmentation of all categories of MVTec-3D AD.}\n  \\label{tab:ablation_aupro}\n\\end{table*}\n\n\n\\section{Detailed Results of PFA Analysis}\n\\label{sec:transformer}\nIn the main paper Section 4.4, we conduct experiments on PFA settings.\nHere we report the detailed results of the main paper Table 5 with scores on each category in\n\\cref{tab:detail-pt-settiing} and \\cref{tab:detail-pt-settiing-aupro}.\nThe PFA settings are:\n\\begin{itemize}\n    \\item Two important hyper-parameters of Point Transformer: the number of groups and the groups' size during farthest point sampling;\n    The number of groups decides how many features will be extracted by the Point Transformer and the groups' size is equal to the concept of the receptive field;\n    \\item 3D anomaly detection experiment with the original point groups feature: a point group can be seen as a {\\it patch}, and the memory bank store point groups feature here; \n    The detection method is as same as the patch-based one, and to get the segmentation predictions, we first project point group feature to a 2D plane and use an inverse distance interpolation to get every pixel value.\n\\end{itemize}\n\nWe found that directly calculating the anomaly on the point groups has some advantage in certain categories (e.g. Bagel, Cable Gland, Foam and Rope), and the reason is that after furthest point sampling (FPS) the original point group feature contains more small defects information.\nAs the patch gets smaller, our 2D plane point feature gets a better performance in detecting the small defects.\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}lcc|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n     S.G & N.G & Sampling & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n     \\midrule\n     64 & 784 & point groups & 0.933 & 0.579 & 0.854 & 0.843 & 0.874 & 0.748 & 0.761 & 0.863 & 0.989 & 0.483 & 0.793\\\\\n     128 & 1024 & point groups & 0.945 & 0.690 & 0.905 & 0.925 & 0.897 & 0.809 & 0.854 & 0.888 & 0.991 & 0.509 & 0.841\\\\\n     64 & 784 & $8\\times8$ patch & 0.905 & 0.508 & 0.939 & 0.923 & 0.817 & 0.725 & 0.857 & 0.916 & 0.897 & 0.561 & 0.805 \\\\\n     128 & 1024 & $8\\times8$ patch & 0.886 & 0.560 & 0.925 & 0.971 & 0.832 & 0.711& 0.873 & 0.909 & 0.897 & 0.624 & 0.819\\\\\n     128 & 1024 & $4\\times4$ patch &  0.941 & 0.651 & 0.965 & 0.969 & 0.905 & 0.760 & 0.880 & 0.974 & 0.926& 0.765 & {\\bf 0.874}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Detailed I-AUROC results of exploring Point Transformer setting on the pure 3D setting. S.G means the point number per group, and N.G means the total number of point groups.  We achieve the best performance with 1,024 point groups per sample and each point group contains 128 points; Compared with directly calculating anomaly scores on point groups, the method based on a 2D plane patch needs a small patch size towards high performance.}\n  \\label{tab:detail-pt-settiing}\n\\end{table*}\n\n\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}lcc|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n     S.G & N.G & Sampling & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n     \\midrule\n     64 & 784 & point groups & 0.906 & 0.709 & 0.942 & 0.854 & 0.869 & 0.681 & 0.871 & 0.906 & 0.943 & 0.447 & 0.813\\\\\n     128 & 1024 & point groups & 0.956 & 0.812 & 0.964 & 0.895 & 0.892 & 0.644 & 0.965 & 0.974 & 0.966 & 0.891 & 0.896 \\\\\n     64 & 784 & $8\\times8$ patch & 0.899 & 0.789 &0.970 & 0.848 & 0.871 & 0.718 & 0.931 & 0.951 & 0.939 & 0.874 & 0.879 \\\\\n     128 & 1024 & $8\\times8$ patch & 0.934 & 0.808 & 0.977 & 0.856 & 0.877 & 0.745 & 0.949 & 0.970 & 0.948 & 0.894 & 0.896\\\\\n     128 & 1024 & $4\\times4$ patch & 0.943 & 0.818 & 0.977 & 0.882 & 0.881 & 0.743 & 0.958 & 0.974 & 0.950 & 0.929 & {\\bf 0.906}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Detailed AUPRO results of exploring Point Transformer setting on the pure 3D setting. S.G means the point number per group, and N.G means the total number of point groups.  We get the best performance with 1024 point groups per sample and each point group contains 128 points; Compared with directly calculating segmentation scores on point groups, the method based on a 2D plane patch needs a small patch size towards high performance.}\n  \\label{tab:detail-pt-settiing-aupro}\n\\end{table*}\n\n\n\n\\section{Detailed Results of Few-shot Setting}\n\\label{sec:few-shot}\n\nIn the main paper Section 4.6, we evaluate our method on Few-shot settings, and the detailed results on all of the categories are illustrated in \\cref{tab:detail-fs-au} and \\cref{tab:detail-fs-iauroc}.\nWe randomly select 10 and 5 images from each category as training data and test the few-shot model on the full testing dataset.\nWe find that our method in a 10-shot or 5-shot setting still has a better segmentation performance than some non-few-shot methods.\nIn the 50-shot setting, We found that some categories get better performance than the full dataset version (e.g. Bagel and Potato), which means the memory bank building algorithm still has some improvement space, and we will discuss the problem in future research.\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}l|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n     Method & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n     \\midrule\n     5-shot  &0.974 &\t0.645&\t0.833&\t0.942&\t0.636&\t0.798&\t0.820&\t0.781&\t0.914&\t0.615&\t0.796\\\\\n     10-shot &0.987&\t0.662&\t0.854&\t0.969&\t0.643&\t0.799&\t0.908&\t0.771&\t0.931&\t0.682&\t0.821\\\\\n     50-shot &0.997&\t0.745&\t0.957&\t0.966&\t0.910&\t0.915&\t0.937&\t0.910&\t0.946&\t0.744&\t0.903\\\\\n     Full dataset  &  0.994 & 0.909 & 0.972 &  0.976 &  0.960 &  0.942 & 0.973 & 0.899& 0.972 & 0.850 &  {\\bf 0.945}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Few-shot I-AUROC of all categories of MVTec-3D AD. Our method still has good anomaly detection performance on few-shot settings.}\n  \\label{tab:detail-fs-iauroc}\n\\end{table*}\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}l|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n     Method & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n     \\midrule\n     5-shot &0.959&\t0.879&\t0.974&\t0.906&\t0.879&\t0.848&\t0.968&\t0.957&\t0.963&\t0.935&\t0.927\\\\\n     10-shot &0.972& 0.910&\t0.976&\t0.923&\t0.905&\t0.870&\t0.972&\t0.956&\t0.967&\t0.939&\t0.939\\\\\n     50-shot &0.969&\t0.955&\t0.977&\t0.940&\t0.906&\t0.912&\t0.971&\t0.965&\t0.968&\t0.959&\t0.952\\\\\n     Full dataset & 0.970 & 0.971 & 0.979 & 0.950 & 0.941 & 0.932 & 0.977 & 0.971  & 0.971 & 0.975 & {\\bf 0.964}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Few-shot AUPRO of all categories of MVTec-3D AD. Our method on few-shot still has a better anomaly segmentation performance than most non-few-shot methods.}\n  \\label{tab:detail-fs-au}\n\\end{table*}\n\n\\section{Backbone Choices}\n\\label{sec:backbone}\n\nFeature extractors play an important role in anomaly detection. In this section, we explore different backbone settings on both point cloud and RGB images. For RGB we compare four extractor settings:\n1) A ViT-B/8 supervised backbone pretrained with ImageNet\\cite{imagenet} 1K;\n2) A ViT-B/8 supervised backbone pretrained with ImageNet 21K;\n3) A ViT-S/8 self-supervised backbone pertrained via DINO\\cite{DINO};\n4) A ViT-B/8 self-supervised backbone pertrained via DINO.\nAnd for Point Clouds transformer, we compare two self-supervised pretrained backbones:\n1) Point-Bert\\cite{point-bert}; 2) Point-MAE\\cite{pointmae}.\nThe detection and segmentation results are separately illustrated in \\cref{tab:backbone_iauroc} and \\cref{tab:backbone_aupro}.\nThe results show that  \nthe self-supervised pretrained methods have better results than the supervised ones, and \n%we can find with self-supervised pretrain has a small backbone has a better performance than the bigger one with a supervised pretrain.\nsmall backbones pretrained with self-supervised methods perform better than the bigger ones pretrained with supervised methods.\nThe performance of Point-MAE is better than that of Point-Bert, we think the reason is that Point-MAE needs to reconstruct more point cloud details than Point-Bert, thus can catch small defects in anomaly detection.\n\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}cl|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n    & Method & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n    \\midrule\n     \\multirow{4}*{\\rotatebox{90}{RGB}} & Supervised ImageNet 1K&0.793\t&0.729&\t0.774&\t0.709&\t0.723&\t0.601&\t0.607&\t0.606&\t0.605&\t0.556&\t0.670\\\\\n     &Supervised ImageNet 21K  & 0.814& \t0.658&\t0.788&\t0.630&\t0.784&\t0.582&\t0.615&\t0.459&\t0.674&\t0.621&\t0.662\\\\\n     &DINO ViT-S/8  & 0.933&\t0.865&\t0.898&\t0.786&\t0.878&\t0.759&\t0.902&\t0.520 &\t0.898&\t0.748&\t0.819\\\\\n     & DINO ViT-B/8 & 0.944 & 0.918 & 0.896 & 0.749 & 0.959 & 0.767 & 0.919 & 0.648 & 0.938 & 0.767 & {\\bf 0.850} \\\\\n     \\midrule\n     \\multirow{2}*{\\rotatebox{90}{3D}} & Point-Bert & 0.900&\t0.632&\t0.932&\t0.915&\t0.851&\t0.659&\t0.826&\t0.899&\t0.894&\t0.530&\t0.803\\\\\n     &Point-MAE  & 0.941 & 0.651 & 0.965 & 0.969 & 0.905 & 0.760 & 0.880 & 0.974 & 0.926& 0.765 & {\\bf 0.874}\\\\\n    % & & & & & & & & & & &\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{I-AUROC score for anomaly detection of MVTec-3D AD\\cite{mvtec3dad} dataset with different backbone. For RGB feature extractor, The self-supervised backbone is better than the supervised ones.}\n  \\label{tab:backbone_iauroc}\n\\end{table*}\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}cl|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n    & Method & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n    \\midrule\n     \\multirow{4}*{\\rotatebox{90}{RGB}} & Supervised ImageNet 1K& 0.844&\t0.842&\t0.892&\t0.681&\t0.842&\t0.568&\t0.765&\t0.865&\t0.915&\t0.871&\t0.808\\\\\n     & Supervised ImageNet 21K &0.805&\t0.878&\t0.927&\t0.712&\t0.888&\t0.62&\t0.785&\t0.909&\t0.919&\t0.930&\t0.837\\\\\n     & DINO ViT-S/8 & 0.948\t&0.973&\t0.971&\t0.906&\t0.947&\t0.788&\t0.972&\t0.954&\t0.964&\t0.949&\t0.937\\\\\n     & DINO ViT-B/8 &0.952&\t0.972&\t0.973&\t0.891&\t0.932&\t0.843&\t0.970&\t0.956&\t0.968&\t0.966&\t{\\bf 0.942}\\\\\n     \\midrule\n     \\multirow{2}*{\\rotatebox{90}{3D}} & Point-Bert & 0.895 &\t0.775&\t0.972&\t0.841&\t0.871&\t0.680&\t0.918&\t0.964&\t0.938&\t0.877&\t0.873\\\\\n     & Point-MAE & 0.943&\t0.818&\t0.977&\t0.882&\t0.881&\t0.743&\t0.958&\t0.974&\t0.950&\t0.929&\t{\\bf 0.906}\\\\\n    % & & & & & & & & & & &\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{AUPRO score for anomaly segmentation of MVTec-3D AD\\cite{mvtec3dad} dataset with different backbones. For RGB feature extractor, the self-supervised backbone is better than the supervised ones.}\n  \\label{tab:backbone_aupro}\n\\end{table*}\n\n\n\n\n\\section{Eyecandies Results}\n\\label{sec:eyecandies}\n\nWe have noticed that recently a new dataset Eyecandies~\\cite{eyecandies} provides multimodel information of 10 categories of candies, and each category contains 1000 images for training and 50 images for public testing. \nThe source dataset provides 6 RGB images, which are in different light conditions, a depth map, and a normal map of each sample.\nIn this section, we convert the Eyecandies dataset to the format supported by  M3DM.\nIn detail, we use the environment light image as our input RGB data, and for 3D data, we first convert the depth image to point clouds with internal parameters, then we remove the background points with point coordinates.\nFor computation efficiency, we use only less than 400 samples from each category for training.\nWe try to build memory banks of different sizes (ranging from 10 to 400 samples) to find the best one under this dataset.\nAs illustrated in \\cref{tab:eyecandies} and \\cref{tab:eyecandies_p_aucroc}, we report the best I-AUCROC and P-AUCROC scores.\nCompared with baseline methods, we have significant improvement in both the RGB setting and RGB+3D setting.\nPrevious work did not report the AUPRO score on the Eyecandies dataset, and for reference in further study, we provide the this segmentation performance metric score of M3DM in  \\cref{tab:eyecandies_aupro}.\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}cl|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n    & Method & Candy Cane & Chocolate Cookie & Chocolate Praline & Confetto & Gummy Bear & Hazelnut Truffle & Licorice Sandwich & Lollipop & Marshm -allow & Pepper-mint Candy & Mean\\\\\n    \\midrule\n     \\multirow{1}*{\\rotatebox{90}{3D}} &Ours  &   0.482  & 0.589 & 0.805 & 0.845 & 0.780 & 0.538 & 0.766 & 0.827 & 0.800 & 0.822 & 0.725\\\\% 0.411 & 0.55 & 0.797 & 0.816 & 0.763 & 0.518 & 0.726 & 0.858 & 0.819 & 0.822 & 0.708\\\\\n    \\midrule\n     \\multirow{4}*{\\rotatebox{90}{RGB}} & RGB\\cite{eyecandies}& 0.527 & 0.848 & 0.772 & 0.734 & 0.590 & 0.508 & 0.693 & 0.760 & 0.851 & 0.730 & 0.701 \\\\\n     & STFPM\\cite{STFPM} &  0.551 & 0.654 & 0.576 & 0.784 & 0.737 & \\bf 0.790 & 0.778 & 0.620 & 0.840 & 0.749 & 0.708\\\\\n     & PaDiM\\cite{padim} & 0.531 & 0.816 & 0.821 & 0.856 & 0.826 & 0.727 & 0.784 & 0.665 & 0.987 & 0.924 & 0.794 \\\\\n     & Ours & \\bf 0.648  & \\bf\t0.949  & \\bf\t0.941 & \\bf\t1.000\t & \\bf 0.878 & \t0.632 & \\bf\t0.933 & \\bf\t0.811 & \\bf\t0.998 & \\bf\t1.000 & \\bf\t0.879 \\\\%& \\bf 0.622 &  \\bf 0.950 &  \\bf 0.934 &  \\bf 1.000 &  0.793 & 0.666 & \\bf 0.928 & \\bf 0.814 & \\bf 0.998 & \\bf 1.000 & \\bf 0.871 \\\\\n    % & & & & & & & & & & &\\\\\n    \\midrule\n     \\multirow{3}*{\\rotatebox{90}{RGB+3D}} & RGB-D\\cite{eyecandies} & 0.529 & 0.861 & 0.739 & 0.752 & 0.594 & 0.498 & 0.679 & 0.651 & 0.838 & 0.75 & 0.689 \\\\\n     &RGB-cD-N\\cite{eyecandies} & 0.596 & 0.843 & 0.819 & 0.846 & 0.833 & 0.550 & 0.750 & \\bf 0.846 & 0.940 & 0.848 & 0.787  \\\\\n     &Ours  & \\bf 0.624 & \\bf 0.958 & \\bf 0.958 & \\bf 1.000 & \\bf 0.886 & \\bf 0.758 & \\bf 0.949 &  0.836 &  \\bf 1.000 & \\bf  1.000 & \\bf 0.897\\\\ %& \\bf 0.602 & \\bf 0.968 & \\bf 0.962 & \\bf 0.998 & \\bf 0.886 & \\bf 0.666 & \\bf 0.923 &  0.816 &  \\bf 0.998 & \\bf  1.000 & \\bf 0.882\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{I-AUROC score for anomaly detection of all categories of Eyecandies~\\cite{eyecandies} dataset. The results of baselines are from the ~\\cite{eyecandies}.}\n  \\label{tab:eyecandies}\n\\end{table*}\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}cl|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n    & Method & Candy Cane & Chocolate Cookie & Chocolate Praline & Confetto & Gummy Bear & Hazelnut Truffle & Licorice Sandwich & Lollipop & Marshm -allow & Pepper-mint Candy & Mean\\\\\n    \\midrule\n     \\multirow{1}*{\\rotatebox{90}{3D}} & Ours & 0.977 & 0.903 & 0.902 & 0.93 & 0.875 & 0.832 & 0.909 & 0.968 & 0.868 & 0.918 & 0.908 \\\\\n    \\midrule\n     \\multirow{2}*{\\rotatebox{90}{RGB}} & RGB\\cite{eyecandies}& \\bf 0.972 & 0.933 & \\textbf{0.960} & 0.945 & 0.929 & 0.815 & 0.855 & 0.977 & 0.931 & 0.928 & 0.925 \\\\\n     & Ours & \\textbf{0.956} & \\textbf{0.979} & 0.958 & \\textbf{0.998} & \\textbf{0.976} & \\textbf{0.941} & \\textbf{0.977} & \\textbf{0.986} & \\textbf{0.997} & \\textbf{0.988} & \\textbf{0.976} \\\\\n    % & & & & & & & & & & &\\\\\n    \\midrule\n     \\multirow{3}*{\\rotatebox{90}{RGB+3D}} & RGB-D\\cite{eyecandies} & 0.973          & 0.927          & 0.958          & 0.945          & 0.929          & 0.806          & 0.827          & 0.977          & 0.931          & 0.928          & 0.920          \\\\\n      %0.529 & 0.861 & 0.739 & 0.752 & 0.594 & 0.498 & 0.679 & 0.651 & 0.838 & 0.75 & 0.689 \\\\\n     &RGB-cD-N\\cite{eyecandies} & \\bf 0.980          & 0.979          & \\bf 0.982          & 0.978          & 0.951          & 0.853          & 0.971          & 0.978          & 0.985          & 0.967          & 0.962   \\\\%0.596 & 0.843 & 0.819 & 0.846 & 0.833 & 0.550 & 0.750 & \\bf 0.846 & 0.940 & 0.848 & 0.787  \\\\\n     &Ours  & 0.974          & \\textbf{0.987} & 0.962          & \\textbf{0.998} & \\textbf{0.966} & \\textbf{0.941} & \\textbf{0.973} & \\textbf{0.984} & \\textbf{0.996} & \\textbf{0.985} & \\textbf{0.977} \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{P-AUROC score for anomaly detection of all categories of Eyecandies~\\cite{eyecandies} dataset. The results of baselines are from the ~\\cite{eyecandies}.}\n  \\label{tab:eyecandies_p_aucroc}\n\\end{table*}\n\n\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{l|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n     Method & Candy Cane & Chocolate Cookie & Chocolate Praline & Confetto & Gummy Bear & Hazelnut Truffle & Licorice Sandwich & Lollipop & Marshm -allow & Pepper-mint Candy & Mean\\\\\n     \\midrule\n     Point Clouds &0.911          & 0.645          & 0.581          & 0.748          & 0.748          & 0.484          & 0.608          & 0.904          & 0.646          & 0.750           & 0.702  \\\\\n     RGB & 0.867          & 0.904          & 0.805          & 0.982          & 0.871          & 0.662          & 0.882          & 0.895          & 0.970           & 0.962          & 0.880 \\\\\n     Point Clouds + RGB & 0.906          & 0.923          & 0.803          & 0.983          & 0.855          & 0.688          & 0.880          & 0.906          & 0.966          & 0.955          & 0.882 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{AUPRO score for anomaly detection of all categories of Eyecandies~\\cite{eyecandies} dataset.}\n  \\label{tab:eyecandies_aupro}\n\\end{table*}\n\n\\section{Visualization Results}\n\\label{sec:visual}\n\nIn this section, we visualize more anomaly segmentation results for all categories of MVTec-3D AD datasets.\nAs shown in \\cref{fig:compare}, we visualize the heatmap results of our method and PatchCore + FPFH, both with multimodal inputs. Compared with PatchCore + FPFH results, our method gets better segmentation maps. % with multimodal feature. \n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{fig/compare.pdf}\n\n   \\caption{Heatmap of our anomaly segmentation results (multimodal inputs). Compared with PatchCore + FPFH, our method outputs a more accurate segmentation region.}\n   \\label{fig:compare}\n\\end{figure*}\n\\end{subappendices}\n"
            }
        },
        "tables": {
            "tab:iaucroc": "\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}cl|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n    & Method & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n    \\midrule\n     \\multirow{10}*{\\rotatebox{90}{3D}}&Depth GAN\\cite{mvtec3dad} & 0.530 & 0.376 & 0.607 & 0.603 & 0.497 & 0.484 & 0.595 & 0.489 & 0.536 & 0.521 & 0.523\\\\\n     ~&Depth AE\\cite{mvtec3dad} & 0.468 & \\underline{0.731} & 0.497 & 0.673 & 0.534 & 0.417 & 0.485 & 0.549 & 0.564 & 0.546 & 0.546\\\\\n     ~&Depth VM\\cite{mvtec3dad} & 0.510 & 0.542 & 0.469 & 0.576 & 0.609 & 0.699 & 0.450 & 0.419 & 0.668 & 0.520 & 0.546\\\\\n     ~&Voxel GAN\\cite{mvtec3dad} & 0.383 & 0.623 & 0.474 & 0.639 & 0.564 & 0.409 & 0.617 & 0.427 & 0.663 & 0.577 & 0.537\\\\\n     ~&Voxel AE\\cite{mvtec3dad} & 0.693 & 0.425 & 0.515&  0.790 & 0.494 & 0.558 & 0.537 & 0.484 & 0.639 & 0.583 & 0.571\\\\\n     ~&Voxel VM\\cite{mvtec3dad} & 0.750  & \\bf{0.747} & 0.613 & 0.738 & 0.823 & 0.693 & 0.679 & 0.652 & 0.609 &  \\underline{0.690} & 0.699\\\\\n     ~&3D-ST\\cite{3d-st} & 0.862& 0.484 & 0.832 & 0.894 & 0.848 & 0.663 & 0.763 & 0.687 & \\bf{0.958} & 0.486 & 0.748 \\\\\n     ~&FPFH\\cite{3d-ads} & 0.825 & 0.551 & \\underline{0.952} & 0.797 & \\underline{0.883} & 0.582 & 0.758 & 0.889 & 0.929 & 0.653 & 0.782\\\\\n     ~&AST\\cite{ast} & \\underline{0.881} & 0.576 & {\\bf 0.965} & \\underline{0.957} & 0.679 & {\\bf 0.797} & {\\bf 0.990} & \\underline{0.915} & \\underline{0.956} & 0.611 & \\underline{0.833}\\\\\n     ~&Ours & {\\bf 0.941} & 0.651 & {\\bf 0.965} & {\\bf 0.969} & {\\bf 0.905} & \\underline{0.760} & \\underline{0.880} & {\\bf 0.974}& 0.926 & {\\bf 0.765} & {\\bf 0.874}\\\\\n    \\midrule\n      \\multirow{7}*{\\rotatebox{90}{RGB}}&DifferNet\\cite{differnet} & 0.859 & 0.703 & 0.643 & 0.435 & 0.797 & 0.790 & 0.787 & \\underline{0.643} & 0.715 & 0.590 & 0.696\\\\\n     &PADiM\\cite{padim} & {\\bf 0.975} & 0.775 & 0.698 & 0.582 & 0.959 & 0.663 & 0.858 & 0.535 & 0.832 & 0.760 & 0.764\\\\\n     &PatchCore\\cite{patchcore} & 0.876 & 0.880 & 0.791 & 0.682 & 0.912 & 0.701 & 0.695 & 0.618 & 0.841 & 0.702 & 0.770\\\\\n     &STFPM\\cite{STFPM} & 0.930 & 0.847 & \\underline{0.890} & 0.575 & 0.947 & 0.766 & 0.710 & 0.598 & 0.965 & 0.701 & 0.793\\\\\n     &CS-Flow\\cite{cflow} & 0.941 & {\\bf 0.930} & 0.827 & \\underline{0.795} & {\\bf 0.990} & \\underline{0.886} & 0.731 & 0.471 &  \\underline{0.986} & 0.745 & 0.830\\\\\n     &AST\\cite{ast} & \\underline{ 0.947} & \\underline{0.928} & 0.851 & {\\bf 0.825} & \\underline{0.981} & {\\bf 0.951} & \\underline{0.895} & 0.613 & {\\bf 0.992}& {\\bf 0.821} & {\\bf 0.880}\\\\\n     &Ours & 0.944 & 0.918 & {\\bf 0.896} & 0.749 & 0.959 & 0.767 & {\\bf 0.919} & {\\bf 0.648} & 0.938 & \\underline{0.767} & \\underline{0.850}\\\\\n    \\midrule\n     \\multirow{10}*{\\rotatebox{90}{RGB + 3D}}&Depth GAN\\cite{mvtec3dad} & 0.538 & 0.372& 0.580& 0.603& 0.430& 0.534& 0.642& 0.601& 0.443& 0.577& 0.532\\\\\n     &Depth AE\\cite{mvtec3dad} & 0.648 & 0.502 & 0.650 & 0.488& 0.805 & 0.522& 0.712 & 0.529 & 0.540 & 0.552 & 0.595\\\\\n     &Depth VM\\cite{mvtec3dad} & 0.513& 0.551& 0.477 & 0.581 & 0.617 & 0.716 & 0.450 & 0.421& 0.598& 0.623& 0.555\\\\\n     &Voxel GAN\\cite{mvtec3dad} & 0.680& 0.324& 0.565 & 0.399& 0.497& 0.482& 0.566& 0.579& 0.601& 0.482& 0.517\\\\\n     &Voxel AE\\cite{mvtec3dad} & 0.510& 0.540 & 0.384& 0.693& 0.446& 0.632& 0.550& 0.494& 0.721& 0.413& 0.538\\\\\n     &Voxel VM\\cite{mvtec3dad} & 0.553 & 0.772& 0.484& 0.701& 0.751& 0.578& 0.480& 0.466& 0.689& 0.611& 0.609\\\\\n     % &3D-ST\\cite{3d-st} & 0.950 & 0.483 & {\\bf 0.986} & 0.921 & 0.905 & 0.632 & 0.945 & {\\bf 0.988} & \\underline{0.976} & 0.542 & 0.833\\\\\n     &PatchCore + FPFH\\cite{3d-ads} & 0.918 & 0.748 & 0.967 & 0.883 & \\underline{0.932} & 0.582 & 0.896 & \\underline{0.912} & 0.921 & {\\bf 0.886} & 0.865\\\\\n     &AST\\cite{ast}  & \\underline {0.983} & \\underline{0.873} & \\bf{0.976} & \\underline{0.971} & \\underline{0.932} & \\underline{0.885} & {\\bf 0.974} & \\bf{0.981} & {\\bf 1.000} & 0.797 & \\underline{0.937} \\\\\n     &Ours & {\\bf 0.994} & {\\bf 0.909} & \\underline{0.972} & {\\bf 0.976} & {\\bf 0.960} & {\\bf 0.942} & \\underline{0.973} & 0.899& \\underline{0.972} & \\underline{0.850} & {\\bf 0.945}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{-5pt}\n  \\caption{I-AUROC score for anomaly detection of all categories of MVTec-3D AD. Our method clearly outperforms other methods in 3D and 3D + RGB setting; For pure 3D setting, our method reaches 0.874 mean I-AUROC score, and for 3D + RGB setting, we get 0.945 mean I-AUROC score. The results of baselines are from the ~\\cite{mvtec3dad, 3d-ads, ast, benchmarking}.}\n  \\vspace{-10pt}\n  \\label{tab:iaucroc}\n\\end{table*}",
            "tab:aupro": "\\begin{table*}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}cl|m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}m{0.8cm}<{\\centering}|m{0.8cm}<{\\centering}}\n    \\toprule\n    & Method & Bagel & Cable Gland & Carrot & Cookie & Dowel & Foam & Peach & Potato & Rope & Tire & Mean\\\\\n    \\midrule\n     \\multirow{8}*{\\rotatebox{90}{3D}}&Depth GAN\\cite{mvtec3dad} & 0.111& 0.072& 0.212& 0.174 & 0.160 & 0.128 & 0.003 & 0.042 & 0.446 & 0.075 & 0.143\\\\\n     ~&Depth AE\\cite{mvtec3dad} & 0.147 & 0.069 & 0.293 & 0.217 & 0.207 & 0.181 & 0.164 & 0.066 & 0.545 & 0.142 & 0.203 \\\\\n     ~&Depth VM\\cite{mvtec3dad} & 0.280 & 0.374 & 0.243 & 0.526 & 0.485 & 0.314 & 0.199 & 0.388 & 0.543 & 0.385 & 0.374\\\\\n     ~& Voxel GAN\\cite{mvtec3dad} & 0.440 & 0.453 & 0.875 & 0.755 & 0.782 & 0.378 & 0.392 & 0.639 & 0.775 & 0.389 & 0.583 \\\\\n     ~&Voxel AE\\cite{mvtec3dad} & 0.260 & 0.341 & 0.581 & 0.351 & 0.502 & 0.234 & 0.351 & 0.658 & 0.015 & 0.185 & 0.348 \\\\\n     ~&Voxel VM\\cite{mvtec3dad} & 0.453 & 0.343 & 0.521 & 0.697 & 0.680 & 0.284 & 0.349 & 0.634 & 0.616 & 0.346 & 0.492\\\\\n     ~&3D-ST\\cite{3d-st} & \\underline{0.950} & 0.483 & {\\bf 0.986} & {\\bf 0.921} & {\\bf 0.905} & 0.632 & 0.945 & {\\bf 0.988} & {\\bf 0.976} & 0.542 & 0.833\\\\\n     ~&FPFH\\cite{3d-ads} & {\\bf 0.973} & {\\bf 0.879} & {\\underline{0.982}} & {\\underline{ 0.906}} & {\\underline{0.892}} & \\underline{0.735} & {\\bf 0.977} & {\\underline{0.982}} & {\\underline{0.956}} & {\\bf 0.961} & {\\bf 0.924}\\\\\n     ~&Ours & 0.943 & \\underline{0.818} & 0.977 & 0.882 & 0.881 & {\\bf 0.743} & \\underline{0.958} & 0.974 & 0.95 & \\underline{0.929} & \\underline{0.906} \\\\\n    \\midrule\n     \\multirow{4}*{\\rotatebox{90}{RGB}}& CFlow\\cite{cflow} & 0.855 & 0.919 & \\underline{0.958} & 0.867 & \\bf 0.969 & 0.500 & 0.889 & 0.935 & 0.904 & 0.919 & 0.871 \\\\\n     & PatchCore\\cite{patchcore} & 0.901 & \\underline{0.949} & 0.928 & 0.877 & 0.892 & 0.563 & 0.904 & 0.932 & 0.908 & 0.906 & 0.876\\\\\n     &  PADiM\\cite{padim} & \\bf 0.980 &  0.944 & 0.945 & \\underline{0.925} & \\underline{0.961} & \\underline{0.792} & \\underline{0.966} & \\underline{0.940} & \\underline{0.937} & \\underline{0.912} & \\underline{0.930}  \\\\\n     & Ours & \\underline{0.952}& {\\bf 0.972} & {\\bf 0.973} & {\\bf 0.891} & 0.932 & {\\bf 0.843} & {\\bf 0.97} & {\\bf 0.956} & {\\bf 0.968} & {\\bf 0.966} & {\\bf 0.942} \\\\\n    \\midrule\n     \\multirow{9}*{\\rotatebox{90}{RGB + 3D}}&Depth GAN\\cite{mvtec3dad} & 0.421 & 0.422& 0.778 & 0.696 & 0.494 & 0.252 & 0.285 & 0.362 & 0.402 & 0.631 & 0.474 \\\\\n     &Depth AE\\cite{mvtec3dad} & 0.432 & 0.158 & 0.808 & 0.491 & 0.841 & 0.406 & 0.262 & 0.216 & 0.716 & 0.478 & 0.481 \\\\\n     &Depth VM\\cite{mvtec3dad} & 0.388 & 0.321 & 0.194 & 0.570 & 0.408 & 0.282 & 0.244 & 0.349 & 0.268 & 0.331 & 0.335\\\\\n     &Voxel GAN\\cite{mvtec3dad} & 0.664 & 0.620 & 0.766 & 0.740 & 0.783 & 0.332 & 0.582 & 0.790 & 0.633 & 0.483 & 0.639 \\\\\n     &Voxel AE\\cite{mvtec3dad} & 0.467 & 0.750 & 0.808 & 0.550 & 0.765 & 0.473 & 0.721 & 0.918 & 0.019 & 0.170 & 0.564\\\\\n     &Voxel VM\\cite{mvtec3dad} & 0.510 & 0.331 & 0.413 & 0.715 & 0.680 & 0.279 & 0.300 & 0.507 & 0.611 & 0.366 & 0.471\\\\\n     &PatchCore + FPFH\\cite{3d-ads} & {\\bf 0.976} & \\underline{0.969} & \\bf{0.979} & {\\bf 0.973} & \\underline{0.933} & \\underline{0.888} & \\underline{0.975} & \\bf{0.981} & \\underline{0.950} & \\underline{0.971} & \\underline{0.959} \\\\\n     &Ours & \\underline{0.970} & {\\bf 0.971} & \\bf{0.979} & \\underline{0.950} & {\\bf 0.941} & {\\bf 0.932} & {\\bf 0.977} & \\underline{0.971}  & \\bf{0.971} & {\\bf 0.975} & {\\bf 0.964}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{-5pt}\n  \\caption{AUPRO score for anomaly segmentation of all categories of MVTec-3D. Our method outperforms other methods on RGB and RGB + 3D settings. for the RGB setting, our method reaches 0.942 mean AUPRO score, and for the RGB + 3D setting, our method reaches 0.964 mean AUPRO score. The results of baselines are from the ~\\cite{mvtec3dad, 3d-ads, benchmarking}.}\n  \\vspace{-10pt}\n  \\label{tab:aupro}\n\\end{table*}",
            "tab:p-aucroc": "\\begin{table}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}l|cc}\n    \\toprule\n    Method  & I-AUROC & P-AUROC\\\\\n    \\midrule\n     PatchCore + FPFH \\cite{3d-ads} & 0.865 & 0.992 \\\\\n     AST\\cite{ast} & 0.937 & 0.976\\\\\n     Ours & {\\bf 0.945}& {\\bf 0.992}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{-5pt}\n  \\caption{Mean I-AUROC and P-AUROC score for anomaly detection of all categories of MVTec-3D. Our method performance well on both anomaly detection and segmentation.}\n  \\vspace{-5pt}\n  \\label{tab:p-aucroc}\n\\end{table}",
            "tab:ablation": "\\begin{table}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}l|cccc}\n    \\toprule\n   Method  & Memory bank &  I-AUROC & AUPRO & P-AUROC\\\\\n    \\midrule\n    Only PC &$\\mathcal{M}_{pt}$&0.874 & 0.906 &  0.970 \\\\\n    Only RGB  &$\\mathcal{M}_{rgb}$& 0.850 & 0.942 & 0.987 \\\\\n    \\midrule\n    w/o UFF &$\\mathcal{M}_{fs}$& 0.857  & 0.944& 0.987 \\\\\n    w/ UFF &$\\mathcal{M}_{fs}$ & 0.898  & 0.956 & 0.990 \\\\\n    \\midrule\n     w/o DLF  & $\\mathcal{M}_{rgb},\\mathcal{M}_{pt}$ & 0.929 & 0.953 & 0.987 \\\\\n     w/ DLF   & $\\mathcal{M}_{rgb},\\mathcal{M}_{pt}$ & 0.932 & 0.959 & 0.990 \\\\\n     \\midrule\n     Ours  & $\\mathcal{M}_{rgb},\\mathcal{M}_{pt},\\mathcal{M}_{fs}$ & {\\bf 0.945} & {\\bf 0.964}& {\\bf 0.992}\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{-5pt}\n  \\caption{Ablation study on fusion block. M is the number of memory banks used. Compared with directly concatenating feature, with UFF, the single memory bank method get better performance. With DLF, the anomaly detection and segmentation performance gets great improvement.}\n  \\vspace{-10pt}\n  \\label{tab:ablation}\n\\end{table}",
            "tab:pt-setting": "\\begin{table}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}ccc|ccc}\n    \\toprule\n     S.G & N.G & Sampling &I-AUROC & AUPRO & P-AUROC\\\\\n    \\midrule\n    64 & 784  & point group & 0.793 & 0.813 & 0.922 \\\\\n    % 128 + 784 (group) & 0.835 & &  \\\\\n    128 & 1024 & point group & 0.841 & 0.896 & 0.960 \\\\\n    64 & 784 &$28\\times28$ patches& 0.805 & 0.879  & 0.963 \\\\\n    128 & 1024 &$28\\times28$ patches  & 0.819 & 0.896 &  0.967\\\\\n    128 & 1024 &$56\\times56$ patches  & {\\bf 0.874} & {\\bf 0.906} &  {\\bf 0.970}  \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{-5pt}\n  \\caption{Exploring Point Transformer setting on the pure 3D setting. S.G means the point number per group, and N.G means the total number of point groups.  We get the best performance with 1024 point groups per sample and each point group contains 128 points; Compared with directly calculating anomaly and segmentation scores on point groups, the method based on a 2D plane patch needs a small patch size towards high performance.}\n  \\label{tab:pt-setting}\n   \\vspace{-5pt}\n\\end{table}",
            "tab:few-shot": "\\begin{table}\n  \\centering\n  \\scriptsize\n  \\begin{tabular}{@{}l|ccc}\n    \\toprule\n    Method  & I-AUROC & AUPRO & P-AUROC\\\\\n    \\midrule\n    5-shot & 0.796 & 0.927 & 0.981 \\\\\n    10-shot & 0.821 & 0.939 & 0.985 \\\\\n    50-shot & 0.903 & 0.953 & 0.988 \\\\\n    Full dataset  & 0.945 & 0.964 &  0.992  \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{-5pt}\n  \\caption{Few-shot setting results. On 10-shot or 5-shot setting, our method still has good segmentation performance and outperforms most Non-few-shot methods.}\n  \\vspace{-10pt}\n  \\label{tab:few-shot}\n\\end{table}"
        },
        "figures": {
            "fig:teaser": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.86\\linewidth]{fig/teaser.pdf}\n  \\vspace{-5pt}\n   \\caption{Illustrations of MVTec-3D AD dataset\\cite{mvtec3dad}. The second and third rows are the input point cloud data and the RGB data. The fourth and fifth rows are prediction results, and according to the ground truth, our prediction has more accurate prediction results than the previous method.}\n   \\vspace{-10pt}\n   \\label{fig:teaser}\n\\end{figure}",
            "fig:pipeline": "\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{fig/pipeline.pdf}\n    % \\vspace{-5pt}\n   \\caption{\\textbf{The pipeline of  Multi-3D-Memory (M3DM).} Our M3DM contains three important parts: (1) {\\bf \\textit{ \\color{OliveGreen}{Point Feature Alignment}}} (PFA) converts Point Group features to plane features with interpolation and project operation, $\\text{FPS}$ is the farthest point sampling and $\\mathcal F_{pt}$ is a pretrained Point Transformer; (2)\n    {\\bf \\textit{ \\color{RoyalBlue}{Unsupervised Feature Fusion}}} (UFF) fuses point feature and image feature together with a patch-wise contrastive loss $\\mathcal L_{con}$, where $\\mathcal F_{rgb}$ is a Vision Transformer, $\\chi_{rgb},\\chi_{pt}$ are MLP layers and $\\sigma_r, \\sigma_p$ are single fully connected layers; (3) {\\bf \\textit{Decision Layer Fusion}} (DLF) combines multimodal information with multiple memory banks and makes the final decision with 2 learnable modules $\\mathcal D_a, \\mathcal D_s$ for anomaly detection and segmentation, where $\\mathcal{M}_{rgb}, \\mathcal{M}_{fs}, \\mathcal{M}_{pt}$ are memory banks, $\\phi, \\psi$ are score function for single memory bank detection and segmentation, and  $\\mathcal{P}$ is the memory bank building algorithm.}\n   \\label{fig:pipeline}\n\\end{figure*}",
            "fig:UFF": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{fig/contrastive.pdf}\n    % \\vspace{-5pt}\n   \\caption{UFF architecture. UFF is a unified module trained with all training data of MVTec-3D AD. The patch-wise contrastive loss $\\mathcal{L}_{con}$ encourages the multimodal patch features in the same position to have the most mutual information, i.e., the diagonal elements of the contrastive matrix have the biggest values.}\n   \\vspace{-5pt}\n   \\label{fig:UFF}\n\\end{figure}",
            "fig:distri": "\\begin{figure}[t]\n    \\subfloat[Statistic distribution.]{\n        \\begin{minipage}[t]{0.46\\linewidth}\n            \\centering\n            \\includegraphics[width=0.9\\linewidth]{fig/hist_fuse.jpg}\n            \\label{fig:hist}\n        \\end{minipage}\n    }\n    \\hfill\n    \\subfloat[T-SNE distribution.]{\n        \\begin{minipage}[t]{0.46\\linewidth}\n            \\centering\n            \\includegraphics[width=0.9\\linewidth]{fig/all_emb.png}\n            \\label{fig:tsne}\n        \\end{minipage}\n    }\n    \\vspace{-5pt}\n     \\caption{Distribution of bagel multimodal features. The fused point feature has a smaller variance in distribution and has a closer distribution with the fused RGB feature.}\n      \\vspace{-5pt}\n     \\label{fig:distri}\n\\end{figure}",
            "fig:discuss": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{fig/discuss.pdf}\n  \\vspace{-5pt}\n   \\caption{\n      Properties of the MVTec-3D AD. PC is short for point cloud prediction, and M is short for multimodal prediction. With 3D information on the point cloud, the anomaly is accurately located in row 1. The label bias causes inaccurate segmentation, for the model hard to focus on the missing area.\n   }\n   \\vspace{-10pt}\n   \\label{fig:discuss}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\begin{aligned}\n    p'_j = \\sum_{i=1}^{M} \\alpha_i g_i, \\quad\n    \\alpha_i = \\frac{ \\frac{1}{\\Vert c_i - p_j \\Vert_2+\\epsilon}} {\\sum_{k=1}^M \\sum_{t=1}^{N} \\frac{1}{\\Vert c_k - p_t \\Vert_2+\\epsilon}},\n    \\label{eq:interpolation}\n\\end{aligned}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\mathcal{L}_{con} = \\frac{h_{rgb}^{(i,j)} \\cdot h_{pt}^{(i,j)}{}^T}{\\sum_{t=1}^{N_b} \\sum_{k=1}^{N_p} h_{rgb}^{(t,k)} \\cdot h_{pt}^{(t,k)}{}^T},\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    f^{(i,j)}_{fs} = \\chi_{rgb}(f^{(i,j)}_{rgb}) \\oplus \\chi_{pt}(f^{(i,j)}_{pt}).\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    a = \\mathcal D_a (\\phi(\\mathcal{M}_{rgb},f_{rgb}), \\phi(\\mathcal{M}_{pt},f_{pt}), \\phi(\\mathcal{M}_{fs},f_{fs})),\n    \\label{eq:dlf_a}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\small\n    \\textbf{\\it S} = \\mathcal D_s (\\psi (\\mathcal{M}_{rgb},f_{rgb}), \\psi(\\mathcal{M}_{pt},f_{pt}), \\psi(\\mathcal{M}_{fs},f_{fs})),\n    \\label{eq:dlf_s}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n     \\phi(\\mathcal{M}, f) = \\eta\\Vert f^{(i,j), *} - m^*\\Vert_2,\n\\end{equation}",
            "eq:7": "\\begin{equation}\n     \\psi(\\mathcal{M}, f) = \\{\\min _{m\\in \\mathcal{M}} \\Vert f^{(i,j)} - m\\Vert_2 \\Big{|} f^{(i,j)}\\in f\\},\n\\end{equation}",
            "eq:8": "\\begin{equation}\n     f^{(i,j), *}, m^* = \\arg \\max _{f^{(i,j)}\\in f} \\arg \\min _{m\\in \\mathcal{M}}\\Vert  f^{(i,j)} - m\\Vert_2,\n\\end{equation}"
        },
        "git_link": "https://github.com/nomewang/M3DM"
    }
}