{
    "meta_info": {
        "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",
        "author": "Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun",
        "link": "http://arxiv.org/abs/2006.15437",
        "category": [
            "cs.LG",
            "cs.SI",
            "stat.ML"
        ],
        "additional_info": "Published on KDD 2020"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\\label{sec:introduction}\n\n%\\yd{to add refs later}\n\nThe breakthroughs in graph neural networks (GNNs) have revolutionized graph mining from structural feature engineering to representation learning~\\cite{bruna2013spectral, gilmer2017neural,gcn}. \nRecent GNN developments have been demonstrated to benefit various graph applications and network tasks, such as semi-supervised node classification~\\cite{gcn}, recommendation systems~\\cite{DBLP:conf/kdd/YingHCEHL18}, and knowledge graph inference~\\cite{DBLP:conf/esws/SchlichtkrullKB18}.\n\n\nCommonly, GNNs take a graph with attributes as input and apply convolutional filters to generate node-level representations layer by layer. \nOften, a GNN model is trained with supervised information in an end-to-end manner for one task on the input graph. \nThat said, for different tasks on the same graph, it is required to have enough and different sets of labeled data to train dedicated GNNs corresponding to each task.  \nUsually, it is arduously expensive and sometimes infeasible to access sufficient labeled data for those tasks, particularly for large-scale graphs. \nTake, for example, the author disambiguation task in academic graphs~\\cite{tang2008arnetminer}, it has still faced the challenge of the lack of ground-truth to date.  \n\n\n\nSimilar issues had also been experienced in natural language processing (NLP). \nRecent advances in NLP address them by training a model from a large unlabeled corpus and transferring the learned model to downstream tasks with only a few labels---the idea of pre-training. \nFor example, the pre-trained BERT language model~\\cite{DBLP:journals/corr/abs-1810-04805} is able to learn expressive contextualized word representations by reconstructing the input text---next sentence and masked language predictions, and thus it can significantly improve the performance of various downstream tasks. \nAdditionally, similar observations have also been demonstrated in computer vision~\\cite{DBLP:journals/corr/abs-1807-03748, he2019momentum, DBLP:journals/corr/abs-2002-05709}.\n\n\nInspired by these developments, we propose to pre-train graph neural networks for graph mining. The goal of the pre-training is to empower GNNs to capture the structural and semantic properties of a input graph, so that it can easily generalize to any downstream tasks with a few fine-tuning steps on the graphs within the same domain. To achieve this goal, we propose to model the graph distribution by learning to reconstruct the input attributed graph.\n\nTo pre-train GNNs based on graph reconstruction, one straightforward option could be to directly adopt the neural graph generation techniques~\\cite{ DBLP:journals/corr/KipfW16a, DBLP:conf/icml/YouYRHL18, DBLP:conf/nips/LiaoLSWHDUZ19}. \nHowever, they are not suitable for pre-training GNNs by design. \nFirst, most of them focus only on generating graph structure without attributes, which does not capture the underlying patterns between node attributes and graph structure---the core of convolutional aggregation in GNNs. Second, they are designed to handle small graphs to date, limiting their potential to pre-train on large-scale graphs. \n\n\\vpara{Contributions.}\nIn this work, we design a self-supervised attributed graph generation task for GNN pre-training, with which both the structure and attributes of the graph are modeled. Based on this task, we present the \\method\\ framework for generative pre-training of graph neural networks (Cf. Figure \\ref{fig:arch}). \nThe pre-trained GNN on the input graph can be then used as the initialization of models for \n%and can well served \ndifferent downstream tasks on the same type of graphs. \n%with just a few fine-tuning steps. \nSpecifically, our contributions are illustrated below. \n\nFirst, we design an attributed graph generation task to model both node attributes and graph structure. We decompose the graph generation objective into two components: Attribute Generation and Edge Generation, whose joint optimization is equivalent to maximizing the probability likelihood of the whole attributed graph. \nIn doing this, the pre-trained model can capture  the inherent dependency between node attributes and graph structure. \n\n\nSecond, we propose an efficient framework \\method to conduct generative pre-training with the aforementioned task. \\method can calculate the attribute and edge generation losses of each node simultaneously, and thus only need to run the GNN once for the graph. \nAdditionally, \\method can handle large-scale graphs with sub-graph sampling and mitigate the inaccurate loss brought by negative sampling with an adaptive embedding queue. \n\n\nFinally, we pre-train GNNs on two large-scale graphs---the Open Academic Graph (OAG) of 179 million nodes \\& 2 billion edges and Amazon recommendation data of 113 million nodes. \nExtensive experiments show that the \\method\\ pre-training framework can significantly benefit various downstream tasks. % under different domain transfer settings. \nFor example, by applying the pre-trained model on OAG, the node classification and link prediction performance is on average lifted by 9.1\\% over the state-of-the-art GNN models without pre-training. \nIn addition, we show that \\method\\ %is general and \ncan consistently improve the performance of different base GNNs under various settings. \n\n\n\n\n\n\n\n%the same type of graphs\n%graphs of the same domain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hide{\nGraph neural networks (GNNs)~\\cite{gcn} \\YS{more citations} are a powerful tool for modeling graph-structured data. \\YS{Graph neural networks have been demonstrated to be a powerful tool in handling graph data. }\nGNNs take a graph with or without attributes as input and apply convolution filters to generate node-level representations layer by layer. The GNN framework can be trained in an end-to-end manner and it has been shown to achieve competitive performance in various graph-based applications, such as semi-supervised node classification~\\cite{gcn}, recommendation systems~\\cite{DBLP:conf/kdd/YingHCEHL18} and knowledge graphs~\\cite{DBLP:conf/esws/SchlichtkrullKB18}.\n\n\n\n\\begin{figure}[ht!]\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{picture/gpt-intro.pdf}\n    \\caption{Overall pipeline of \\method.}\n    \\label{fig:arch}\n\\end{figure}\n\n\n\nDespite the success, training an accurate GNN requires a large number of high-quality labeled data, which are often expensive or even inaccessible in practice. For example, in an academic graph, an important task is author name disambiguation, which aims to match a paper to its correct author against those authors with the same name. Despite the large size of academic graph, the correct ground-truth for this disambiguation task requires manual label, which cannot be scaled up. One effective solution for this issue is to train a model from a large unlabelled corpus and transfer the learned knowledge to downstream tasks with only a few labels, an idea known as pre-training. Recently, pre-trained language models (e.g., BERT~\\cite{DBLP:journals/corr/abs-1810-04805}, XLNet~\\cite{xlnet}) have been shown to be able to learn expressive word representations and to significantly improve the performance of various downstream tasks by reconstructing the text. Similar to language data, the graph data also contain rich information. For example, in an academic graph, by referring the citation of a single paper, we can estimate its main research topic, which can be used to solve various inference tasks. Therefore, a model learned to reconstruct the input graph data is able to encode important semantic knowledge and rule of this graph domain, which can be transferred to other downstream tasks. Motivated by this, in this paper, we try to explore whether graph neural networks can be pre-trained via graph generation, and to what extent can it benefit downstream tasks.\n\n\nAlthough recently there are several studies on graph generation~\\cite{DBLP:conf/icml/YouYRHL18, DBLP:conf/icann/SimonovskyK18,  DBLP:conf/nips/LiaoLSWHDUZ19}, they are not suitable for pre-training graph neural networks. On one hand, most of them only focus on generating graph structure without attributes, which does not apply to most real-world setting. On the other hand, they can only handle small graphs, which limits their potential to pre-training on large-scale graphs. In light of these limitations, we propose a novel generative pre-training framework. \n\nFirstly, we propose a novel pre-training task, which factorize the attributed graph probability into \\YS{this level of introduction is essential, before going to technical details. What is the pre-training task? Graph generation or graph reconstruction. In other words, the designed GNN model is able to generate an attributed graph that is close to real observation. Then challenges: (1) it is impossible to generate a whole graph (sampled subgraphs as training instances); (2) ...}\n\n\nThe major contribution of this framework is a novel attributed graph generation factorization which incorporates observed links as an intermediate random variable, and then decompose the objective into two components, i.e., Attribute Generation and Edge Generation. In this way, optimizing the two components is equivalent to maximizing the probability likelihood of the whole graph, and we can inherently capture both the dependency between each node to its context neighborhood, and also its attribute with structure. To avoid information leakage for Attribute Generation, we also propose to separate the nodes in the graph into two sets, one without input attribute, and is used for Attribute Generation; the other is the same as standard node, which is used for Edge Generation. Based on the factorization formula, we parameterize the Attribute Generation and Edge Generation process by graph neural networks with corresponding decoders.\n\n\nIn addition, directly applying the generative pre-training objective to the whole large-scale graph is impossible, as we cannot get the representations of each node in the graph with limited resources. A naive solution is to sample a subgraph and only pre-train over it. However, as it only considers the sampled nodes instead of the whole graph for calculating the contrastive loss of Edge Generation. To alleviate such limitation, we propose an adaptive node representation queue, which stores the most recently calculated node representations, and use them as contrastive negative examples. By updating the queue each batch, the representation is correct and the ultimate objective is closer to the one over the whole graph. \n\n\\YS{will come back after reading the technical sections}\n\n\n\nOur pre-training framework can also be applied to heterogeneous graphs with a simple modification. We thus pretrain a state-of-the-art heterogeneous graph transformer on two large-scale graphs. The first one is Microsoft Open Academic Graph (OAG), which is currently the largest heterogeneous graph dataset, comprised of 179 million nodes and 2 billion edges. The second one is Amazon Review Recommendation Dataset (Amazon), consisting of 113 million nodes. We test our generative pre-training framework under different domain transfer setting, i.e., temporal transfer, field transfer and a combined transfer setting. Empirically, our proposed framework can significantly enhance the node classification and link prediction performance on both Microsoft Academic Graphs and Amazon recommendation datasets, for 9.1\\% and 5.7\\% on average, and the pre-training framework is robust to different domain transfer setting and different sizes of fine-tuning datasets. \\YS{the last sentence needs revision. ``different sizes of fine-tuning datasets''? }\n\n\n}\n"
            },
            "section 2": {
                "name": "Preliminaries and Related Work",
                "content": "\\label{sec:related}\n\n\n\n\n\n\n\nThe goal of pre-training is to allow a model (usually neural networks) to initialize its parameters with pre-trained weights. % learned during pre-training. \nIn this way, the model can leverage the commonality between the pre-training and  downstream tasks. Recently pre-training has shown superiority in boosting the performance of many downstream applications in computer vision and natural language processing. In the following, we first introduce the preliminaries about GNNs and then review pre-training approaches in graphs and other domains.\n\n",
                "subsection 2.1": {
                    "name": "Preliminaries of Graph Neural Networks",
                    "content": "\n\nRecent years have witnessed the success of GNNs for modeling graph data~\\cite{gcn,gat,graphsage,hgt}. \nA GNN can be regarded as using the input graph structure as the computation graph for message passing~\\cite{gilmer2017neural}, during which the local neighborhood information is aggregated to get a more contextual representation. Formally, suppose $H^{(l)}_t$ is the node representation of node $t$ at the $(l)$-th GNN layer, the update procedure from the $(l$-$1)$-th layer to the $(l)$-th layer is:\n\\begin{align}\nH^{(l)}_t \\gets \\underset{\\forall s \\in N(t), \\forall e \\in E(s,t)}{\\textbf{Aggregate}}\\bigg(  \\Big\\{ \\textbf{Extract}\\big(H^{(l-1)}_s; H^{(l-1)}_t, e\\big)\\Big\\} \\bigg),\n\\end{align}\nwhere $N(t)$ denotes all the source nodes of node $t$ and $E(s,t)$  denotes all the edges from node $s$ to $t$. \n% {\\color{red}{YS: what's the difference between ; and , in Extract()?}}\\znh{before ; is the source node information, which serves as key, while after it is target and edge, which serves as query}\n\nThere are two basic operators for GNNs, which are \\textbf{Extract($\\cdot$)} and \\textbf{Aggregate($\\cdot$)}. Among them,\n\\textbf{Extract($\\cdot$)} represents the neighbor information extractor. It uses the target node's representation $H^{(l-1)}_t$ and the edge $e$ between the two nodes as query, and extract useful information from source node $H^{(l-1)}_s$. \\textbf{Aggregate($\\cdot$)} serves as the aggregation function of the neighborhood information. The \\textit{mean, sum,} and \\textit{max} functions are often considered as the basic aggregation operators, while sophisticated pooling and normalization functions can also be designed. Under this framework, various GNN architectures have been proposed.  \nFor example, the graph convolutional network (GCN) proposed by Kipf \\textit{et al.}~\\cite{gcn} averages the one-hop neighbor of each node in the graph, followed by a linear projection and then a non-linear activation. \nHamilton \\textit{et al.}~\\cite{graphsage} propose GraphSAGE that generalizes GCN's aggregation operation from \\textit{average} to \\textit{sum, max} and a \\textit{RNN unit}. \n\n\n\nAlso, there are a bunch of works incorporating the attention mechanism into GNNs. In general, the attention-based models implement the \\textbf{Extract($\\cdot$)} operation by estimating the importance of each source node, based on which a weighted aggregation is applied. For example, Velickovi \\textit{et al.}~\\cite{gat} propose the graph attention network (GAT), which adopts an additive mechanism to calculate attention and uses the same weight for calculating messages. %Wang \\textit{et al.} extends GAT to heterogeneous graph setting and proposed heterogeneous graph attention networks (HAN)~\\cite{DBLP:conf/www/WangJSWYCY19}. \nRecently, Hu \\textit{et al.} propose the heterogeneous graph transformer (HGT)~\\cite{hgt} that leverages multi-head attentions for different relation types to get type-dependent attentions. \nThe proposed pre-training framework \\method\\ can apply to all of these GNN models. \n%The proposed pre-training framework \\method\\ can apply to all of these GNN models, among which HGT generates the best performance, and thus is used as the main base model for pre-training.\n\n\n\\hide{\nAlso, there are a bunch of works incorporate attention mechanism into GNNs. In general, all the attention-based models can fall into the following framework: \n\\begin{align}\nH^{l}_t \\gets \\underset{\\forall s \\in N(t), \\forall e \\in E(s,t)}{\\textbf{Aggregate}}\\Big(  \\textbf{Attention}(s, t) \\cdot \\textbf{Message}(s)\\Big), \\label{eq:att}\n\\end{align}\nwhich implements the \\textbf{Extract($\\cdot$)} operation by two major components: \\textbf{Attention}, which estimates the importance of each source node, and \\textbf{Message}, which extracts the message by using only the source node $s$. For example, Velickovi \\textit{et al.}~\\cite{gat} proposed graph attention network (GAT), which adopts an additive mechanism as \\textbf{Attention}, and use the same weight for calculating \\textbf{Message}. Wang \\textit{et al.} extends GAT to heterogeneous graph setting and proposed heterogeneous graph attention networks (HAN)~\\cite{DBLP:conf/www/WangJSWYCY19}. Recently, Hu \\textit{et al.} proposed Heterogeneous Graph Transformer (HGT)~\\cite{hgt} that leverage multi-head attention for different relation type to get type-dependent \\textbf{Attention} and \\textbf{Message}. Our pre-training framework \\method\\ can apply to all of these GNN models. Experiments show that HGT perform the best, thus we use it as our main base model.\n}\n\n\n\n% \\YS{Add one sentence: our pre-training framework can apply to all of these base GNN models. Experiments shown that HGT perform the best, thus we use it as our main base model. ---In this case, we do not have to emphasize HGT anymore.}\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Pre-Training for Graphs",
                    "content": " \n\nPrevious studies have proposed to utilize pre-training to learn node representations, which largely belong to two categories. The first category is usually termed as network/graph embedding, which directly parameterizes the node embedding vectors and optimizes them by preserving some similarity measures, such as the network proximity~\\cite{tang2015line} or statistics derived from random walks~\\cite{grover2016node2vec,dong2017metapath2vec,qiu2018network}. \nHowever, the embeddings learned in this way cannot be used to initialize other models for fine-tuning over other tasks. % on the same graph. \n%\\YS{unseen nodes or unseen graphs. Note our paper focus on one graph setting, so we probably do not want to emphasize unseen graph setting.} \n%However, the embeddings learned in this way cannot generalize to another unseen graph as the information they capture are graph-specified. \nIn contrast, we consider a transfer learning setting, where the goal is to pre-train a generic GNN that can deal with different tasks. \n\nWith the increasing focus on GNNs, researchers have explored the direction of pre-training GNNs on unannotated data. Kipf \\textit{et al.} propose Variational Graph Auto-Encoders~\\cite{DBLP:journals/corr/KipfW16a} to reconstruct the graph structure. Hamilton \\textit{et al.} propose GraphSAGE~\\cite{graphsage}, which can optimize via an unsupervised loss by using random walk based similarity metric. \nVelickovic \\textit{et al.} introduce Graph Infomax~\\cite{DBLP:journals/corr/abs-1809-10341}, which maximizes the mutual information between node representations obtained from GNNs and a pooled graph representation. \nAlthough these methods show enhancements over purely-supervised learning settings, the learning tasks can be  achieved by forcing nearby nodes to have similar embeddings, ignoring the rich semantics and higher-order structure of the graph. Our work proposes to pre-train GNNs by the permutated generative objective, which is a harder graph task and thus can guide the model to learn more complex semantics and structure of the input graph.\n\n\n\nIn addition, there are attempts to pre-train GNNs to extract graph-level representations. Sun \\textit{et al.} present InfoGraph~\\cite{infograph}, which maximizes the mutual information between graph-level representations obtained from GNNs and the representations of sub-structures. \nHu \\textit{et al.}~\\cite{pretrain} introduce different strategies to pre-train GNNs at both node and graph levels and show that combining them together can improve the performance on graph classification tasks. \nOur work is different with them as our goal is to pre-train GNNs over a single (large-scale) graph and conduct the node-level transfer. %, so the problem setting is different with them.\n\n\n"
                },
                "subsection 2.3": {
                    "name": "Pre-Training for Vision and Language",
                    "content": " \n\n%Pre-training has shown a great success in computer vision. \n\nPre-training has  been widely used in computer vision (CV) and natural language processing (NLP). \nIn CV, early pre-training techniques~\\cite{girshick2014rich,donahue2014decaf,DBLP:conf/cvpr/PathakKDDE16}  mostly follow the paradigm of first pre-training a model on large-scale supervised datasets (such as ImageNet~\\cite{deng2009imagenet}) and then fine-tuning the pre-trained model on downstream tasks~\\cite{girshick2014rich} or directly extracting the representations as features~\\cite{donahue2014decaf}. Recently, some self-supervised tasks~\\cite{DBLP:journals/corr/abs-1807-03748, he2019momentum, DBLP:journals/corr/abs-2002-05709} have also been utilized to pre-train vision models.\nIn NLP, Early works have been focused on learning (shallow) word embeddings~\\cite{mikolov2013distributed,pennington2014glove} by leveraging the co-occurrence statistics on the text corpus. \nMore recently, significant progresses have been made on contextualized word embeddings, such as BERT~\\cite{DBLP:journals/corr/abs-1810-04805}, XLNET~\\cite{xlnet} and GPT~\\cite{radford2019language}. \nTake BERT as an example, it pre-trains a text encoder with two self-supervised tasks in order to better encode words and their contexts. \nThese pre-training approaches have been shown to yield state-of-the-art performance in a wide range of NLP tasks and thus used as a fundamental component in many NLP systems. \n\n% , including multiple language understanding tasks on the GLUE benchmark~\\cite{DBLP:conf/emnlp/WangSMHLB18} and on question answering dataset~\\cite{DBLP:conf/emnlp/RajpurkarZLL16}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hide{\n\n\\begin{algorithm}[h!] \n\\caption{Overall \\method\\ Implementation} \n\\label{alg:pipeline} \n\\begin{algorithmic}[1] \n\\REQUIRE\nInput Attributed Graph $G$, Graph Sampler $S(\\cdot)$.\\\\\n\\ENSURE\n\\STATE Initialize a GNN Model as $f_{\\theta}$, attribute generation decoder as ${Dec}^{Attr}$, edge generation decoder as ${Dec}^{Edge}$\n\\STATE initialize adaptive node embedding queue $Q=\\{\\}$, initialize attribute vector $h^{init}$.\n \\FOR {each sample graph $\\hat{G} \\in S(G)$}\n    \\STATE For each node, sample observed edge index $o$ and masked edges $\\neg o$. Delete masked edges $E^{\\pi}_{i, \\neg o}$ accordingly. \\label{alg:step2}\n    \\STATE Separate each node into attribute generation and edge generation node. replace the input to attribute generation node as $h_{init}$. Apply GNN $f_{\\theta}$ to get two sets of node embeddings $h^{Attr}$ and $h^{Edge}$ for each node in the graph. \\label{alg:step3}\n    \\FOR {node $i$ with attribute $X^{\\pi}_{i}$ and masked edges $A^{\\pi,\\tau}_{i, \\geq t}$}\n        \\STATE Calculate Attribute Generation Loss $\\mathcal{L}^{Attr}$ by Eq. \\ref{eq:gen} \\label{alg:step4}\n        \\STATE Prepare negative samples $S^{-}_i$ for edge generation by concatenating the unconnected nodes and adaptive queue $Q$. \\label{alg:step9}\n        \\STATE Calculate Edge Generation Loss $\\mathcal{L}^{Edge}$ by Eq. \\ref{eq:contrastive} \\label{alg:step5}\n    \\ENDFOR\n    \\STATE Optimize $\\theta$ by minimizing $\\mathcal{L}^{Attr}$ and $\\mathcal{L}^{Edge}$. \\label{alg:step6}\n    \\STATE update $Q$ by adding in $h^{Edge}$ and popping out most out-dated embeddings. \\label{alg:step7}\n\\ENDFOR\n\\RETURN Pre-trained model parameter $\\theta^*$ for downstream tasks\n\\end{algorithmic} \n\\end{algorithm}\n\n\n\n\n\n\nThe goal of pre-training is to allow a neural network model to initialize its parameters with weights learned from the pre-training tasks. \\YS{very good summary. Need to mention it in abs/introduction adapted to GNN.} In this way, the model can leverage the commonality between the pre-training and the downstream tasks. Pre-training has shown superior in boosting the performance of many downstream applications in computer vision, natural language processing and graph mining. In the following, we first review some preliminaries about graph neural networks, and then review approaches of pre-training graph neural networks, as well as those in other domains.\n\n\\subsection{Preliminaries of graph neural networks}\n\nRecent years have witnessed the success of graph neural networks for relational data~\\cite{gcn,gat,graphsage}. \nGenerally, a GNN can be regarded as using the input graph structure as the computation graph for message passing~\\cite{gilmer2017neural}, during which the local neighborhood information is aggregated to get a more contextual representation. Formally, Suppose $H^{l}_t$ is the node representation of node $t$ at the $(l)$-th GNN layer, the update procedure from the $(l$-$1)$-th layer to the $(l)$-th layer is:\n\\begin{align}\nH^{l}_t \\gets \\underset{\\forall s \\in N(t), \\forall e \\in E(s,t)}{\\textbf{Aggregate}}\\bigg(  \\textbf{Extract}\\Big(H^{l-1}_s; H^{l-1}_t, e\\Big)\\bigg),\n\\end{align}\nwhere $N(t)$ denotes all the source nodes of node $t$ and $E(s,t)$  denotes all the edges from node $s$ to $t$. \n\nThere are two basic operators used in GNNs: \\textbf{Extract($\\cdot$)} and \\textbf{Aggregate($\\cdot$)}. \n\\textbf{Extract($\\cdot$)} represents the neighbor information extractor. It uses the target node's representation $H^{l-1}_t$ and the edge $e$ between the two nodes as query, and extract useful information from source node $H^{l-1}_s$. \\textbf{Aggregate($\\cdot$)} serves as the aggregation function of the neighborhood information. The \\textit{mean, sum,} and \\textit{max} functions are often considered as the basic aggregation operators, and more sophisticated pooling and normalization functions can be also designed. Under this framework, various GNN architectures have been proposed due to its power for modeling relational data.  \nFor example, the graph convolutional network (GCN) proposed by Kipf \\textit{et al.}~\\cite{gcn} averages the one-hop neighbor of each node in the graph, followed by a linear projection and non-linear activation operations. \nHamilton \\textit{et al.}~\\cite{graphsage} proposed GraphSAGE that generalizes GCN's aggregation operation from \\textit{average} to \\textit{sum, max} and a \\textit{RNN unit}. \n\nAlso, there are a bunch of works incorporate attention mechanism into GNNs. In general, all the attention-based models can fall into the following framework: \n\\begin{align}\nH^{l}_t \\gets \\underset{\\forall s \\in N(t), \\forall e \\in E(s,t)}{\\textbf{Aggregate}}\\Big(  \\textbf{Attention}(s, t) \\cdot \\textbf{Message}(s)\\Big), \\label{eq:att}\n\\end{align}\nwhich implements the \\textbf{Extract($\\cdot$)} operation by two major components: \\textbf{Attention}, which estimates the importance of each source node, and \\textbf{Message}, which extracts the message by using only the source node $s$. For example, Velickovi \\textit{et al.}~\\cite{gat} proposed graph attention network (GAT), which adopts an additive mechanism as \\textbf{Attention}, and use the same weight for calculating \\textbf{Message}. Wang \\textit{et al.} extends GAT to heterogeneous graph setting and proposed heterogeneous graph attention networks (HAN)~\\cite{DBLP:conf/www/WangJSWYCY19}. Recently, Hu \\textit{et al.} proposed Heterogeneous Graph Transformer (HGT)~\\cite{hgt} that leverage multi-head attention for different relation type to get type-dependent \\textbf{Attention} and \\textbf{Message}. Our pre-training framework \\method can apply to all of these GNN models. Experiments show that HGT perform the best, thus we use it as our main base model.\n\n% \\YS{Add one sentence: our pre-training framework can apply to all of these base GNN models. Experiments shown that HGT perform the best, thus we use it as our main base model. ---In this case, we do not have to emphasize HGT anymore.}\n\n\n\n\\subsection{Pre-training for graph applications} Previous studies have proposed to utilize pre-training to learn node representations, which largely belong to two categories. The first category directly parameterizes the node embedding vectors and optimize them by preserving some deterministic measures, such as the network proximity~\\cite{tang2015line} or statistics derived from random walks~\\cite{grover2016node2vec}. However, the embedding learned in this way cannot generalize to another unseen graph \\YS{unseen nodes or unseen graphs. Note our paper focus on one graph setting, so we probably do not want to emphasize unseen graph setting.} as the information they capture are graph-specified. In contrast, we consider a transfer learning setting, where our goal is to pre-train a generic graph neural networks that can deal with every unseen nodes or graphs. \n\nWith the increasing focus on graph neural networks (GNNs), researchers have explored the direction of pre-training GNNs on unannotated data. Hamilton \\textit{et al.} proposed GraphSAGE~\\cite{graphsage}, which adds an unsupervised loss by using random walk based similarity metric. Velickovic proposed Graph Infomax~\\cite{DBLP:journals/corr/abs-1809-10341}, which maximize mutual information between node representations obtained from GNNs and a pooled graph representation. Although these methods can show enhancement over purely supervised learning, the learning task can be simply achieved by forcing nearby nodes to have similar embeddings, which cannot capture rich semantic or higher-order structure of the graph. We propose to pre-train GNNs by permutated generative objective, which is a harder task and can guide the model to learn more complex semantic and structure of the input graph.\n\n\n\nAlso there are a bunch of works trying to learn GNNs to extract graph-level representation. Sun \\textit{et al.} prposed InfoGraph~\\cite{infograph}, which maximizes mutual information between graph-level representation obtained from GNNs and the representations of substructures of different scales. Hu \\textit{et al.}~\\cite{pretrain} proposed different strategies to pretrain graph neural networks at both node and graph levels, and show that combining them together can achieve the best performance on various graph classification tasks. Our work is different with them as our goal is to pretrain GNNs over a single large graph, and conduct node-level transfer, so the problem setting is different with them.\n\n\n\\subsection{Pre-training for other applications} Pre-training in computer vision~\\cite{girshick2014rich, zeiler2014visualizing,donahue2014decaf, DBLP:conf/cvpr/PathakKDDE16}  mostly follows the following paradigm: first pre-train a model on large-scale supervised datasets (such as ImageNet~\\cite{deng2009imagenet}), then fine-tune the pre-trained model on downstream tasks~\\cite{girshick2014rich} or directly extract the representation as features~\\cite{donahue2014decaf}. \n% This approach has been shown to make significant impact on various downstream tasks including object detection~\\cite{girshick2014rich, he2017mask}, image segmentation~\\cite{chen2018encoder, long2015fully}, etc.\n\nPre-training has also been used in various natural language processing (NLP). \nOn the word level, word embedding models~\\cite{mikolov2013distributed,pennington2014glove, bojanowski2017enriching} capture semantics of words by leveraging occurrence statistics on text corpus and have been used as a fundamental component in many NLP systems. On the sentence level, pre-training approaches have been applied to derive sentence representations~\\cite{kiros2015skip, le2014distributed}.  Recently, contextualized word embeddings~\\cite{DBLP:conf/naacl/PetersNIGCLZ18,DBLP:journals/corr/abs-1810-04805}\nare proposed to pre-train a text encoder on large corpus with a language model objective to better encode words and their context. Also, Yang \\textit{et al.}~\\cite{xlnet} showed that generative pre-training can guide the model to learn more complex dependency between words. These approach has shown to reach state-of-the-art performance in a wide range of NLP tasks.\n\n\n% , including multiple language understanding tasks on the GLUE benchmark~\\cite{DBLP:conf/emnlp/WangSMHLB18} and on question answering dataset~\\cite{DBLP:conf/emnlp/RajpurkarZLL16}.\n\n}\n\n"
                }
            },
            "section 3": {
                "name": "Generative Pre-Training of GNNs",
                "content": "\\label{sec:approach}\n\n\n\nIn this section, we formalize the attributed graph generation task and introduce the generative pre-training framework (\\method). \n\n\n",
                "subsection 3.1": {
                    "name": "The GNN Pre-Training Problem",
                    "content": "\n\n\n\nThe input to GNNs is usually an attributed graph $G = (\\cV, \\cE, \\cX)$, where $\\cV$ and $\\cE$ denote its node and edge sets, and $\\cX$ represents the node feature matrix. \nA GNN model learns to output node representations under the supervision of a specific downstream task, such as node classification. Sometimes there exist multiple tasks on a single graph, and most GNNs require sufficient dedicated labeled data for each task. However, it is often challenging to obtain sufficient annotations, in particular for large-scale graphs, hindering the training of a well-generalized GNN. \nTherefore it is desirable to have a pre-trained GNN model that can generalize with few labels. \nConceptually, this model should 1) capture the intrinsic structure and attribute patterns underlying the graph and 2) thus benefit various downstream tasks on this graph. \n\n\\vpara{GNN Pre-Training.}Formally, our goal of GNN pre-training concerns the learning of a general GNN model $f_{\\theta}$ purely based on single (large-scale) graph $G = (\\cV, \\cE, \\cX)$ without labeled data such that $f_{\\theta}$ is a good initialization for various (unseen) downstream tasks on the same graph or graphs of the same domain. \n%This differs our work from a recent GNN pre-training attempt~\\cite{pretrain}, which pre-trains over a collection of small graphs in the chemical or biology domain with applications on unseen graphs.  \nTo learn such a general GNN model without labeled data on the graph, a natural question arises here is: \\textit{how to design an unsupervised learning task over the graph for pre-training the GNN model?}\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "The Generative Pre-Training Framework",
                    "content": "\n\n\\yd{if space allowed, to move the algo box from appendix here. So much text in this section, an algo flow will help a lot. Will come back after going through intro}\n\n\\hide{\nRecent advances in self-supervised learning for NLP~\\cite{DBLP:journals/corr/abs-1810-04805}, computer vision~\\cite{DBLP:conf/cvpr/PathakKDDE16}, and speech~\\cite{DBLP:journals/corr/abs-1807-03748}\\kw{need a few referneces here}, we propose to design the pre-training task as self-supervised learning. \nIts promise is that we can learn an expressive model by predicting between parts of the input graph or its different views~\\cite{}.\n\nIn this work, we propose to mask parts of the input graph (both its structure and node features) and predict/generate the masked structure and features based on the remaining part. \nTo address this self-supervised learning task, we present the generative graph neural network pre-training model (\\method). \n}\n\nRecent advances in self-supervised learning for NLP~\\cite{DBLP:journals/corr/abs-1810-04805,xlnet} and CV~\\cite{DBLP:journals/corr/abs-1807-03748, he2019momentum, DBLP:journals/corr/abs-2002-05709} have shown that unlabeled data itself contains rich semantic knowledge, and thus a model that can capture the data distribution is able to transfer onto various downstream tasks. Inspired by this, we propose \\method, which pre-trains a GNN by reconstructing/generating the input graph's structure and attributes. \n% The overall flow of \\method\\ is illustrated in  Algorithm \\ref{alg:pipeline}. \n\n\nFormally, given an input graph $G = (\\cV, \\cE, \\cX)$ and a GNN model $f_{\\theta}$, we model the likelihood over this graph by this GNN as $p(G; \\theta)$---representing how the nodes in $G$ are attributed and connected. \\method\\ aims to pre-train the GNN model by maximizing the graph likelihood, i.e., $\\theta^* = \\max_{\\theta} p(G; \\theta)$. \n\nThen, the first question becomes how to properly model $p(G; \\theta)$. \n%To leverage the auto-regressive factorization technique, which is used for discrete generation, \nNote that most existing graph generation methods~\\cite{DBLP:conf/icml/YouYRHL18, DBLP:conf/nips/LiaoLSWHDUZ19} follow the auto-regressive manner to factorize the probability objective, i.e., the nodes in the graph come in an order, and the edges are generated by connecting each new arriving node to existing nodes. \nSimilarly, we denote a permutation vector $\\pi$ to determine the node ordering, where $i^{\\pi}$ denotes the node id of $i$-th position in permutation $\\pi$. \n% \\YS{where $\\pi_i$ denotes the node id of $i_$th position in permutation $\\pi$} \nConsequently, the graph distribution $p(G; \\theta)$ is equivalent to the expected likelihood over all possible permutations, i.e.,  \n$$p(G; \\theta) = \\mathbb{E}_{\\pi}\\big[p_{\\theta}(X^{\\pi}, E^{\\pi})\\big],$$  \nwhere $X^{\\pi} \\in \\RR^{|\\cV| \\times d}$ denotes permutated node attributes and $E$ is a set of edges, while $E^{\\pi}_i$ denotes  all edges connected with node $i^{\\pi}$. For simplicity, we assume that observing any node ordering $\\pi$ has an equal probability and also omit the subscript $\\pi$ when illustrating the generative process for one permutation in the following sections. \n%    \\dong{$E^{\\pi}_i$ are edges, so $j \\in E^{\\pi}_i$ may not be accurate}\\znh{change the definition of $E^{\\pi}_i$ to all the neighbors of node i}\nGiven a permutated order, we can factorize the log likelihood autoregressively---generating one node per iteration---as:\n\\begin{align}\\label{eq:gpt}\n    \\log p_{\\theta}(X, E) = \\sum_{i=1}^{|\\cV|} \\log p_{\\theta}(X_{i}, E_{i} \\mid X_{<i}, E_{<i}).\n\\end{align} \n% \\YS{(1) T to $V$?} \n\\YS{(2) A small figure/table to illustrate these notations?}\nAt each step $i$, we use all nodes that are generated before $i$, their attributes $X_{<i}$, and %interconnected\nthe structure (edges) between these nodes $E_{<i}$ to generate a new node $i$, \n%$\\pi_i$, \nincluding both its attribute $X_{i}$ and its connections with existing nodes $E_{i}$. \n\nEssentially, the objective in Eq. \\ref{eq:gpt} describes the autoregressive generative process of an attributed graph. \nThe question becomes: \\textit{how to model the conditional probability $p_{\\theta}(X_{i}, E_{i} | X_{<i}, E_{<i})$?}\n \n\n\\hide{%copied from online. don't use it directly!!!\n\nBy trying to predict the self-output from the self-input, you end up learning about the intrinsic properties / semantics of the object, which otherwise would have taken a ton of examples to learn from.\n\nThe principle is pretty simple: to encode an object, you try to setup learning tasks between parts of it or different views of it (the self).\n\ngenerate output labels \u2018intrinsically\u2019 from data objects by exposing a relation between parts of the object, or different views of the object.\n}%end of hide\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Factorizing Attributed Graph Generation",
                    "content": "\n\nTo compute\n$p_{\\theta}(X_{i}, E_{i} | X_{<i}, E_{<i})$, one naive solution could be to simply assume that $X_{i}$ and $E_{i}$ are independent, \nthat is, \n$$p_{\\theta}(X_{i}, E_{i} | X_{<i}, E_{<i}) = p_{\\theta}(X_{i} | X_{<i}, E_{<i}) \\cdot p_{\\theta}(E_{i} | X_{<i}, E_{<i})$$\n%having it equivalent to the product of their generation probabilities---$p_{\\theta}(X^{\\pi}_{i} | X^{\\pi}_{<i}, E^{\\pi}_{<i}) \\cdot p_{\\theta}(E^{\\pi}_{i} | X^{\\pi}_{<i}, E^{\\pi}_{<i})$. \nWith such decomposition, for each node, the dependency between its attributes and connections are completely neglected. \nHowever, the ignored dependency is the core property of attributed graphs and also the foundation of convolutional aggregation in GNNs. Therefore, such a naive decomposition cannot provide informative guidance for pre-training GNNs. \n% \\yd{example TBD}\n\n\n%\\vpara{Factorizing Attributed Graph Generation as Attribute \\& Edge Generation.}\nTo address this issue, we present the dependency-aware factorization mechanism for the attributed graph generation process.  \nSpecifically, when estimating a new node's attributes, we are given its structure information, and vice versa. \nDuring the process, a part of the edges has already been observed (or generated). \nThen the generation can be decomposed into two coupled parts: \n\\begin{itemize}\n\\item given the observed edges, generate node attributes; \n\\item given the observed edges and generated node attributes, generate the remaining edges. \n\\end{itemize}\nIn this way, the model can capture the dependency between the attributes and structure for each node. \n\nFormally, we define a variable $o$ to denote the index vector of all the observed edges within $E_i$. Thus, $E_{i, o}$ denotes the observed edges. \nSimilarly, $\\neg o$ denotes the index of all the masked edges, which are to be generated. \nWith this, we can rewrite the conditional probability as an expected likelihood over all observed edges: \n% \\begin{align}\n%   &  p_{\\theta}(X^{\\pi}_{i}, E^{\\pi}_{i} \\mid X^{\\pi}_{<i}, E^{\\pi}_{<i}) \\nonumber\\\\\n% = &\\sum_{\\tau, t} p_{\\theta}(X^{\\pi}_{i}, E^{\\pi,\\tau}_{i, \\geq t} \\mid E^{\\pi,\\tau}_{i, <t}, X^{\\pi}_{<i}, E^{\\pi}_{<i}) \\cdot p_{\\theta}(E^{\\pi,\\tau}_{i, <t} \\mid X^{\\pi}_{<i}, E^{\\pi}_{<i}) \\nonumber\\\\\n% = &\\mathbb{E}_{\\tau, t}\\Big[p_{\\theta}(X^{\\pi}_{i}, E^{\\pi,\\tau}_{i, \\geq t} \\mid E^{\\pi,\\tau}_{i, <t}, X^{\\pi}_{<i}, E^{\\pi}_{<i})\\Big] \\nonumber \\\\ \n% =&\\mathbb{E}_{\\tau, t}\\Big[\\underbrace{\n%  p_{\\theta}(X^{\\pi}_{i} \\mid E^{\\pi,\\tau}_{i, <t}, X^{\\pi}_{<i}, E^{\\pi}_{<i})\n%  }_\\text{1) generate attribute} \\cdot \n%  \\underbrace{\n%  p_{\\theta}(E^{\\pi,\\tau}_{i, \\geq t} \\mid E^{\\pi,\\tau}_{i, <t}, X^{\\pi}_{\\leq i}, E^{\\pi}_{<i})\n%  }_\\text{2) generate edges}\\Big] \\label{eq:overall}\n% \\end{align}\n% \\zn{Another version that use $o$ to denote the observed edges' index, and masked(unobserved) ones as $\\neg o$:}\n\\begin{align}\n   &  p_{\\theta}(X_{i}, E_{i} \\mid X_{<i}, E_{<i}) \\nonumber\\\\\n= &\\sum\\nolimits_{o} p_{\\theta}(X_{i}, E_{i, \\neg o} \\mid E_{i, o}, X_{<i}, E_{<i}) \\cdot p_{\\theta}(E_{i, o} \\mid X_{<i}, E_{<i}) \\nonumber\\\\\n= &\\mathbb{E}_{o}\\Big[p_{\\theta}(X_{i}, E_{i, \\neg o} \\mid E_{i, o}, X_{<i}, E_{<i})\\Big] \\nonumber \\\\ \n=&\\mathbb{E}_{o}\\Big[\\underbrace{\n p_{\\theta}(X_{i} \\mid E_{i, o}, X_{<i}, E_{<i})\n }_\\text{1) generate attributes} \\cdot \n \\underbrace{\n p_{\\theta}(E_{i, \\neg o} \\mid E_{i, o}, X_{\\leq i}, E_{<i})\n }_\\text{2) generate edges}\\Big]. \\label{eq:overall}\n\\end{align}\n\n\n\\YS{The above factorization is not exact. We also need to multiply $p(A_{i,<t}|...)$. In order to get rid of this item, i.e., how to generate the first t links, we need to use sampling to approximate it, which has to be mentioned. But how to sample first t links given a permutation? It should already fixed. So I still have question on how to compute $p(A_{i,<t}|...)$. Of course, we can always downgrade our claim saying we are optimizing the conditional distribution of X and remaining edges given observed edges. }\n\\zn{Determine $\\tau$ and $t$ itself is a biased sample. We want the sampling to reflect the probability of whether a node should be observed link. I write it in the expectation (already rewrite it). We can easily implement it by sorting it by degree or treat it as uniform.}\n\\kw{This notation is better, but I still feel we should prevent presenting the framework in this way as it is too general. We are not actually modeling $p_{\\theta}(E_{i, o} \\mid X_{<i}, E_{<i})$ and just approximate it by sampling. Decomposing the probability in this way, we hide the essential component to the expectation symbol and the readers will get confused. My main concern is that generating a whole graph from scratch sounds unrealistic and we do not have a good answer if it is possible. It may confuse the reviewers that you're trying to use some unrealistic objective to train a model. In contrast, the setting in your experiments makes much more sense.}\\YS{For the second line, what is the meaning of sum over all the possible observations? How we come from first line to second line?}\\zn{Since o is a intermediate variable, to marginalize the probability, we need to sum over all possible observation with its probability, following \\url{https://en.wikipedia.org/wiki/Law_of_total_expectation}}\n\n\nThis factorization design is able to model the dependency between node $i$'s attributes $X_{i}$ and its associated connections $E_{i}$. \nThe first term $p_{\\theta}(X_{i} \\mid E_{i, o}, X_{<i}, E_{<i})$ denotes the generation of attributes for node $i$. \nBased on the observed edges $E_{i, o}$, we gather the target node $i$'s neighborhood information to generate its attributes $X_{i}$. \nThe second term $p_{\\theta}(E_{i, \\neg o} \\mid E_{i, o}, X_{\\leq i}, E_{<i})$ denotes the generation of masked edges. \nBased on both the observed edges $E_{i, o}$ and the generated attributes $X_{i}$, we generate the representation of the target node $i$ and predict whether each edge within $E_{i, \\neg o}$ exists. \n\n\\vpara{A graph generation example.}\nWe intuitively show how the proposed factorization-based graph generation process works. Take, for example, an academic graph, if we would like to generate one paper node, whose title is considered as its attribute, while this paper node is connected to its authors, published venue, and cited papers. \nBased on some observed edges between this paper and some of its authors, our generation process first generates its title. Then,\nbased on both the observed edges and generated title, we predict its remaining authors, published venue, and references. \nIn this way, this process models the interaction between the paper's attribute (title) and structure (observed and remaining edges) to complete the generation task, bringing in informative signals for pre-training GNNs over the academic graph.\n\n\nSo far, we factorize the attributed graph generation process into a node attribute generation step and an edge generation step. \nThe question we need to answer here is: \\textit{How to efficiently pre-train GNNs by optimizing both attribute and edge generation tasks?}\n%\\yd{this Q tbd}\n%Next, we show how to design the attribute and edge generation \n\n"
                },
                "subsection 3.4": {
                    "name": "Efficient Attribute and Edge Generation",
                    "content": "\n\\label{subsec:attr-edge-nodes}\n\n%run the GNN for a single round to get the node representations, with which to calculate all the above loss. The first challenge is to support auto-regressive objective. We thus delete all edges from the high-order node to the low-order node according to $\\pi$, so that each node can only receive information from lower-order node (nodes that are already generated). The second challenge is to conduct attribute generation and edge generation simultaneously, as we require to use attribute for edge generation, but don't want to take it as input for Attribute generation (otherwise there's information leakage). We thus propose to separate each node into two sets......\n\n\n%\\vpara{Attribute and Edge Generation.} \n%\\yd{tbd from here}\n\n \n%Edges $E^{\\pi, \\tau}_{i,<t}$ with index smaller than $t$ have already been observed, while the others $E^{\\pi, \\tau}_{i,\\geq t}$ are masked out and to be predicted. \n\n%In the following, we discuss the implementation details for attribute and edge generations. \n%To efficently train \n%At each step, we load a sub-graph and split nodes into two sets: \n%Given the existing graph and the target node's observed edges, the goal is to predict its attributes and masked edges by using a GNN model, which at the first place requires the (initial) attributes of the target node as the input. \n%To solve this contradictory, we propose to split nodes into two sets:\n\n\nFor the sake of efficiency, it is desired to compute the loss of attribute and edge generations by running the GNN only once for the input graph. In addition, we expect to conduct attribute generation and edge generation simultaneously. However, edge generation requires node attributes as input, which can be leaked to attribute generation. To avoid information leakage, we design to separate each node into two types:\n\\begin{itemize}\n    \\item Attribute Generation Nodes. We mask out the attributes of these nodes by replacing their attributes with a dummy token and learn a shared vector $X^{init}$ to represent it\\footnote{$X^{init}$ has the same dimension as $X_{i}$ and can be learned during pre-training.}. This is equivalent to the trick of using the [Mask] token in the masked language model~\\cite{DBLP:journals/corr/abs-1810-04805}. \n    \n    \n    \\item Edge Generation Nodes. For these nodes, we keep their attributes and put them as input to the GNN. \n    \n    \n%    Thus, when propagating . W a previously generated node to new nodes, its Edge Generation Node's representation is used as input. \n    % \\YS{what is $f_{\\theta}$? The GNN-based function?}\n\\end{itemize}\nWe then input the modified graph to the GNN model and generate the output representations. We use $h^{Attr}$ and $h^{Edge}$  to represent the output embeddings of Attribute Generation and Edge Generation Nodes, respectively. As the attributes of Attribute Generation Nodes are masked out,  $h^{Attr}$ in general contains less information than  $h^{Edge}$. Therefore, when conduct the GNN message passing, we only use Edge Generation Nodes' output $h^{Edge}$ as outward messages. The representations of the two sets of nodes are then used to generate attributes and edges with different decoders. \n\n\n%The model then aggregate their neighbor information and generate representation $h^{Attr}_i$ for node $\\pi_i$.\n% with embedding $h^{Edge}_i$ for node $\\pi_i$. It takes the attribute $X^{\\pi}_{i}$ as input. % and is similar to standard node representation in GNNs. \n%    Obviously, its representation is more accurate than the Attribute Generation Node's. \\kw{I don't understand this sentence}\n\n\nFor Attribute Generation, we denote its decoder as ${Dec}^{Attr}(\\cdot)$, which takes $h^{Attr}$ as input and generates the masked attributes. The modeling choice depends on the type of attributes. For example, if the input attribute of a node is text, we can use the text generator model (e.g., LSTM) to generate it. If the input attribute is a standard vector, we can apply a multi-layer Perceptron to generate it. Also, we define a distance function as a metric between the generated attributes and the real ones, such as perplexity for text or L2-distance for vectors. Thus, we calculate the attribute generation loss via:\n\\begin{align}\n\\mathcal{L}^{Attr}_i = Distance\\big({Dec}^{Attr}(h^{Attr}_i), X_{i}\\big). \\label{eq:gen}\n\\end{align}\n\nBy minimizing the distance between the generated and masked attributes, it is equivalent to maximize the likelihood to observe each node attribute, i.e., $ p_{\\theta}(X_{i} \\mid E_{i, o}, X_{<i}, E_{<i})$, and thus the pre-trained model can capture the semantic of this graph.\n\nFor Edge Generation, we assume that the generation of each edge is independent with others, so that we can factorize the likelihood:\n\\begin{align}\n    p_{\\theta}(E_{i, \\neg o} \\mid E_{i, o}, X_{\\leq i}, E_{<i}) = \\prod_{j^{+} \\in E_{i, \\neg o}} \\! p_{\\theta}(j^{+} \\mid E_{i, o}, X_{\\leq i}, E_{<i}). \\label{eq:obj}\n\\end{align}\n\nNext, after getting the Edge Generation node representation $h^{Edge}$, we model the likelihood that node $i$ is connected with node ${j}$ by $Dec^{Edge}(h^{Edge}_i, h^{Edge}_{j})$, where $Dec^{Edge}$ is a pairwise score function. Finally, we adopt the negative contrastive estimation to calculate the likelihood for each linked node $j^+$. We prepare all the unconnected nodes as $S^{-}_i$ and calculate the contrastive loss via\n% the pairwise score between the target node $\\cV[\\pi_t]$ and each support node $\\cV[\\pi_j]$ by using the DistMult product function $Dec^{Edge}(x,y) = x W y^T$. \n% Second, we pick all the connected edges as positive samples and unconnected ones as negative samples. \n% Third, for each positive sample, the contrastive loss is calculated by comparing it with the negative samples via \n\\begin{align}\n\\label{eq:contrastive}\n\\mathcal{L}^{Edge}_i \\!= -\\!\\!\\sum_{j^{+} \\in E_{i, \\neg o}}\\log \\frac{\\exp\\big(Dec^{Edge}(h^{Edge}_i,\\  h^{Edge}_{j^{+}})\\big)}{\\sum_{j^{} \\in S^{-}_i \\cup \\{ j^{+}\\} }\\exp\\big(Dec^{Edge}(h^{Edge}_i,\\ h^{Edge}_{j^{}})\\big)}\n\\end{align}\n%\\YS{(1) Do you use $f_{\\theta}(i)$ to denote the representation for ith node? f is a function, and it takes other things as arguments! (2) what is i? $\\pi_i$?}\\zn{I change the embeddings to h. (2) Yes, i denotes pi\\_i}\n% Such a loss will force the scores between the target node and positive nodes to be higher than those between target and negative nodes, and thus can incorporate the dependency between them. As proved by Grover \\textit{et al.}~\\cite{grover2016node2vec}, optimizing over $\\mathcal{L}_{Edge}$ is equivalent to maximizing the edge generation probability.\n\nBy optimizing $\\mathcal{L}^{Edge}$, it is equivalent to maximizing the likelihood of generating all the edges, and thus the pre-trained model is able to capture the intrinsic structure of the graph.\n\n\\YS{What I expected is to see how to compute $p(remaining edges|observed edges, features)$. What is provided here is a loss. How to connect this loss to probability? As a generative model, we have to provide probabilities.}\\zn{(1) I change the previous notation from A -> E, denoting only linked edges. In this way, the problem becomes a standard node embedding learning problem, and the NCE Loss we use can well capture the probability (2) I've changed the writing for these two tasks, does it make sense now?}\n\n\n\n\nFigure~\\ref{fig:framework} illustrates the attributed graph generation process. Specifically:\n(a) We determine the node permutation order $\\pi$ for the input graph. (b) We randomly select a portion of the target node's edges as observed edges $E_{i, o}$ and the remaining as masked edges $E_{i, \\neg o}$ (grey dashed lines with cross). We delete masked edges in the graph.\n%We denote the masked edges by grey dashed with cross in the Figure~\\ref{fig:framework}(b). \n(c) We separate each node into the Attribute Generation and Edge Generation nodes to avoid information leakage.\n(d) After the pre-processing, we use the modified adjacency matrix to calculate the representations of node 3,4 and 5, including both their Attribute and Edge Generation Nodes. \nFinally, as illustrated in (d)--(e), we train the GNN model via the attribute prediction and masked edge prediction task for each node in parallel. The overall pipeline of \\method\\ is illustrated in Algo.~\\ref{alg:pipeline} (See Appendix B for details).\n\n\\begin{algorithm}[ht!] \n\\caption{The \\method\\ Pre-Training Framework} \n\\label{alg:pipeline} \n\\begin{algorithmic}[1] \n\\REQUIRE\nInput Attributed Graph $G$, Graph Sampler $Sampler(\\cdot)$.\\\\\n\\ENSURE\n\\STATE Initialize the GNN model as $f_{\\theta}$, the attribute generation decoder as ${Dec}^{Attr}$, and the edge generation decoder as ${Dec}^{Edge}$.\n\\STATE Initialize the adaptive node embedding queue $Q=\\{\\}$ and the attribute vector $h^{init}$.\n \\FOR {each sampled graph $\\hat{G} \\in Sampler(G)$}\n    \\STATE For each node, sample the observed edge index $o$ and masked edges $\\neg o$, and delete masked edges $E_{i, \\neg o}$ accordingly. \\label{alg:step2}\n    \\STATE Separate each node into the Attribute Generation and Edge Generation nodes. Replace the input to Attribute Generation node as $h_{init}$. Apply GNN $f_{\\theta}$ to get two sets of node embeddings $h^{Attr}$ and $h^{Edge}$ for each node in the graph. \\label{alg:step3}\n    \\FOR {node $i$ with attributes $X_{i}$ and masked edges $E_{i, \\neg o}$}\n        \\STATE Calculate the attribute generation loss $\\mathcal{L}^{Attr}$ by Eq. \\ref{eq:gen} \\label{alg:step4}\n        \\STATE Prepare negative samples $S^{-}_i$ for edge generation by concatenating unconnected nodes and adaptive queue $Q$.\n        \\label{alg:step9}\\STATE Calculate the edge generation loss $\\mathcal{L}^{Edge}$ by Eq. \\ref{eq:contrastive} \\label{alg:step5}\n    \\ENDFOR\n    \\STATE Optimize $\\theta$ by minimizing $\\mathcal{L}^{Attr}$ and $\\mathcal{L}^{Edge}$. \\label{alg:step6}\n    \\STATE Update $Q$ by adding in $h^{Edge}$ and popping out most outdated embeddings. \\label{alg:step7}\n\\ENDFOR\n\\RETURN Pre-trained model parameters $\\theta^*$ for downstream tasks\n\\end{algorithmic} \n\\end{algorithm}\n\n\n\n\n\n\n"
                },
                "subsection 3.5": {
                    "name": "-0.3cm",
                    "content": "\n\n%Previously, we introduce the generative pre-training  of graph neural networks under the assumption that the input graph is homogeneous and can be fed into the GPU memory. \nIn this section, we discuss how to apply \\method\\ to pre-train for large-scale and heterogeneous graphs, which can be of practical use for modeling real-world complex systems~\\cite{Sun:BOOK2012,dong2020HeterNRL}, such as academic graphs, product graphs, IoT networks, and knowledge graphs.  \n\n\n\\vpara{Heterogeneous graphs.}\n%Conceptually, a heterogeneous graph is one that has multiple types of nodes and edges. \nMany real-world graphs are heterogeneous, meaning that they contain different types of nodes and edges. %, such as academic graphs, product graphs, IoT networks, and knowledge graphs.  \nFor heterogeneous graphs, the proposed \\method\\ framework can be straightforwardly applied to pre-train heterogeneous GNNs. \nThe only difference is that each type of nodes and edges may have its own decoder, which is specified by the heterogeneous GNNs rather than  the pre-training framework. \nAll the other components remain exactly the same. \n\n\n\n\\vpara{Large-scale graphs.}\nTo pre-train GNNs on graphs that are too large to fit into the hardware, we sample subgraphs for training. \n%based on each node and its neighbors. The pre-train the model on all subgraphs, in which two issues arise. \n%The first one is how to sample the subgraph for effective GNN training. \nIn particular, we propose to sample a dense subgraph from homogeneous and heterogeneous graphs by using the LADIES algorithm~\\cite{ladies} and its heterogeneous version HGSampling~\\cite{hgt}, respectively. %  \\dong{xxxxxxxxxxxxxxxxx} \nBoth methods theoretically guarantee that the sampled nodes are highly interconnected with each other and maximally preserve the structural information. \n\n\n%The second issue lies in edge generation. \nTo estimate the contrastive loss in Eq. \\ref{eq:contrastive}, it is required to go over all nodes of the input graph. \nHowever, we only have access to the sampled nodes in a subgraph for estimating this loss, making the (self-)supervision only focus on local signals.\n%One drawback of this approach is the task becomes easier, and as a result, the guidance for pre-training is weaker. \n\\yd{to kw and zn, plz check whether this would work}\n\\kw{The motivation here is a bit not clear. You may want to elaborate more on why the task becomes easier and why the guidance is weaker and why the adaptive queue can resolve this issue.} \n%Thus, the task becomes easier, and may provide weaker guidance for pre-training. \nTo alleviate this issue, we propose the \\textit{Adaptive Queue}, which stores node representations in previously-sampled subgraphs as negative samples. \nEach time we process a new subgraph, we progressively update this queue by adding the latest node representations and remove the oldest ones. As the model parameters will not be updated rigorously, the negative samples stored in the queue are consistent and accurate. \nThe \\textit{Adaptive Queue} enables us to use much larger negative sample pools $S^{-}_i$. Moreover, the nodes across different sampled sub-graphs can bring in the global structural guidance for contrastive learning.\n\n%Based on this sampling algorithm, the nodes sampled in the upper layer have more structural connectivity and should get more accurate node representation. \n%Therefore, when we determine the node permutation order $\\pi$, we follow the reversed order of sampling, which means that the nodes sampled first, which have richer structural information will be ranked at the bottom in $\\pi$.\n%In this way, we are able to obtain more informative signal to pre-train GNNs based on these nodes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hide{\n\nIn this section, we first describe the problem definition of generative pre-training, and then introduce our proposed framework GARNET framework, which pre-train a graph neural network via autoregressively generate a given graph's attribute and structure.\n\n\\subsection{Notations and Problem Definition}\n\nIn this paper, we focus on conducting pre-training GNN over a given large attributed graph $G = (\\cV, \\cE, \\cX)$, where $\\cV$ and $\\cE$ denotes the node and edge sets, and $\\cX$ is defined as the node attributes (features) for each node. If the graph has different node and edge type, we call it a heterogeneous graph $G = (\\cV, \\cE, \\cX, \\cA, \\cR)$, where each node $v \\in \\cV$ and each edge $e \\in \\cE$ are associated with their type mapping functions $\\tau(v): V \\rightarrow \\cA$ and $\\phi(e): \\cE \\rightarrow \\cR$, respectively.\n\nA GNN model $f_{\\theta}$ takes the attributed graph $G$ as input, and output node representation for each node, which will be fed into task-specific decoder to conduct downstream tasks, such as node or edge prediction. However, many tasks only contain a portion of labelled nodes, which hinders training a well-generalized GNN. Therefore, the goal of GNN pre-training is to design some self-supervised task, which only relies on the graph itself instead of manual label to pre-train an expressive GNN. Afterwards, we can use the learned parameter $\\thetE^*$ as initialization to downstream task. Here we assume many downstream tasks is related to the graph distribution $p(G)$, i.e., how the nodes in the graph is generated and connected. Therefore, a model that can well captures the graph distribution only requires few fine-tuning labels to achieve good results on the related tasks. Therefore, we propose to study \\underline{G}ener\\underline{A}tive g\\underline{R}aph \\underline{NE}ural network pre-\\underline{T}raining (GARNET), which tries to model the graph distribution via GNN, and then optimize it over the input graph, i.e., $\\max_{\\theta} p(G; \\theta)$. \n\nThe first question is how to properly model the graph distribution. Most discrete generation follows auto-regressive manner to factorize the probability objective. However, graphs naturally don't have a sequential order. Therefore, we need a permutation vector $\\pi$ to determine the node ordering, and the distribution is equivalent to the expected likelihood over all possible permutation $p(G; \\theta) = \\mathbb{E}_{\\pi}\\big[p_{\\theta}(X, E)\\big]$, where $X \\in \\RR^{|\\cV| \\times d}$ denotes node attributes and $E \\in \\RR^{|\\cV| \\times |\\cV|}$ denotes the adjacency matrix, where $E_{i,j} = 1 \\iff \\big(\\cV[\\pi_i], \\cV[\\pi_j]\\big) \\in \\cE$. For simplicity, we assume that observing any node ordering $\\pi$ has equal probability. With the help of $\\pi$, we can factorize the likelihood autoregressively, meaning that we generates one node per iteration, as:\n\\begin{align}\n    \\log p_{\\theta}(X, E) = \\sum_{i=1}^{T} \\log p_{\\theta}(X_{i}, E_{i} \\mid X_{<i}, E_{<i})\n\\end{align}\nAt each step $i$, we uses all the previously generated nodes, their attributes $X_{<i}$ and interconnected structure $E_{<i}$, to generate a new node $\\cV[\\pi_i]$. Specifically, we aim to generate its attribute $X_{i}$ and its connection with existing nodes $E_{i}$. \n\n\\subsection{Generative GNN Pre-training Framework}\nModelling the condition probability $p_{\\theta}(X_{i}, E_{i} | E_{<i}, X_{<t})$ is not trivial. One naive solution is to assume $X_{i}$ and $E_{i}$ are independent, so that the condition probability is equivalent to the product of two seperate probability, i.e., $p_{\\theta}(X_{i} | E_{<i}, X_{<i}) \\cdot p_{\\theta}(E_{i} | E_{<i}, X_{<i})$. With this decomposition, we generate the attribute and structure seperately, only use the information from $E_{<i}$, $X_{<i}$. The problem of such decomposition is that it neglects the dependency between each node's structure and attribute, which is not reasonable in reality. For example, if we'd like to predict a newly published paper's title, if we don't know any of its structure information, as its author, its published venue, etc, we will not know any of its topic, and the objective degenerate to a simple language model. Also, if we don't have this paper's attribute while all the other nodes in the graph have attributes, we cannot use a unified GNNs to get this paper's representation. Due to these limitations, simply neglecting the dependency between attribute and structure cannot provide informative guidance for pre-training. Therefore, we require a proper factorization design to consider the dependency between attribute and structure, i.e., when we estimate a new node's attribute, we should know this node's structure information, and vice versa. \n\n\n\n\\vpara{Attributed Graph Generation Factorization.} To model such dependency, we assume that a part of the edges has already been observed. In this way, the generation can be decomposed into two parts: 1) given the observed edges, generate node attribute; 2) given observed edges and node attribute, generate the remaining edges. In this way, for each part the model can capture the dependency between attribute and structure. Formally, we define a variable $\\tau$ to permutation order of edges, with which we can redefine the permutated adjacency matrix as:\n\\begin{align}\nE^{\\pi, \\tau} \\in \\RR^{|\\cV| \\times |\\cV|}\\text{ , where   }E^{\\pi, \\tau}_{i,j} = 1 \\iff E_{i,\\tau_j} = 1 \n\\end{align} \nWe then define a threshold variable $t$, denoting that all the edges $E^{\\pi, \\tau}_{i,<t}$ with index smaller than $t$ has already been observed, while the others $E^{\\pi, \\tau}_{i,\\geq t}$ are masked out and to be predicted. Therefore, we can rewrite the conditional likelihood as a expected likelihood over all edge permutation and threshold: \n\\begin{align}\n     p_{\\theta}(X_{i}, E_{i} \\mid X_{<i}, E_{<i}) = \\mathbb{E}_{\\tau, t}\\Big[p_{\\theta}(X_{i}, E^{\\pi,\\tau}_{i, \\geq t} \\mid E^{\\pi,\\tau}_{i, <t}, X_{<i}, E_{<i})\\Big] \\nonumber \\\\ \n     =\\mathbb{E}_{\\tau, t}\\Big[\\underbrace{\n     p_{\\theta}(X_{i} \\mid E^{\\pi,\\tau}_{i, <t}, X_{<i}, E_{<i})\n     }_\\text{1) generate attribute} \\cdot \n     \\underbrace{\n     p_{\\theta}(E^{\\pi,\\tau}_{i, \\geq t} \\mid E^{\\pi,\\tau}_{i, <t}, X_{\\leq i}, E_{<i})\n     }_\\text{2) generate edges}\\Big] \\label{eq:overall}\n\\end{align}\n\nThough this factorization, we can model the dependency between attribute $X_{i}$ and structure $E_{i}$. the first term $p_{\\theta}(X_{i} \\mid E^{\\pi,\\tau}_{i, <t}, X_{<i}, E_{<i})$ denotes the generation of attribute for node $\\cV[\\pi_i]$. Based on the observed edges $E^{\\pi,\\tau}_{i, <t}$ for each node, we can gather target node $\\pi_i$'s neighborhoods' information to generate its attribute $X_{i}$. The second term $p_{\\theta}(E^{\\pi,\\tau}_{i, \\geq t} \\mid E^{\\pi,\\tau}_{i, <t}, X_{\\leq i}, E_{<i})$ denotes the generation of masked edges. Based on both the observed edges $E^{\\pi,\\tau}_{i, <t}$ and the generated attribute $X_{i}$, we can get accurate node representation for the target node $\\pi_i$, and predict whether mask edges $E^{\\pi,\\tau}_{i, \\geq t}$ exist. \n\nIntuitively, this factorization is reasonable in the real graph generation setting. For example, in an academic graph, if we want to determine which paper a specific author will write and publish next year, the author is our observed edge. Based on his previous records, we can determine the paper title, and then predict its co-authorship, citations and published venue, etc. In this way, we are considering the interaction between the paper's content with its structure to complete the generation task, which can bring in informative signal for GNN pre-training.\n\n\n\n\n\\begin{figure*}[ht!]\n    \\centering\n    \\includegraphics[width=1\\textwidth]{picture/illustrate.png}\n    \\caption{A motivating example of the proposed graph generation process.}\n    \\label{fig:framework}\n\\end{figure*}\n\n\n\\vpara{Attribute and Edge Generation.} To conduct attribute and edge generation, we require to get the node representations of each target node, with only observed edges and existing graphs. However, directly applying the original GNNs setting will lead to the problem. All existing GNNs will use each node's self-information as part of the input to calculate its node representation. However, when we generate a new node's attribute $X_{i}$, we obviously don't want the $X_{i}$ be utilized for calculating $f_{\\theta}(\\pi[t])$, otherwise, the objective become trivial. On the contrary, when we generate a new node's masked edges, we require to incorporate its attribute $X_{i}$ as input to get accurate node representation. To solve this discrepancy, for every single node, we propose to separate it into two different types of nodes:\n\\begin{itemize}\n    \\item Attribute Generation Node, $f^{Attr}_{\\theta}(X^{init}, E^{\\pi,\\tau}_{i, <t})$, which doesn't take its own attribute $X_{i}$ as input. Instead, all the new node will take input a shared learnable vector $X^{init}$ as initialized attribute. \n    \\item Edge Generation Node, $f^{Edge}_{\\theta}(X_{i}, E^{\\pi,\\tau}_{i, <t})$, which takes the attribute $X^{\\pi}_{i}$ as input and is similar to standard node representation in GNNs. Obviously, Edge Generation Node's representation is more accurate than that of Attribute Generation Node. Thus, when we propagate information from previously generated nodes to new nodes, we will use Edge Generation Node's representation as source input.\n\\end{itemize}\nAfter getting the representations for the two sets of nodes, we use them to generate attribute and edges with different decoders. For attribute generation, if the input attribute is text, we can use the text generator model, i.e., RNN or Transformer, to generate the text attribute, and use the encoded node representation as to the initial hidden state. If the input attribute is an ordinary attribute, we can simply use an MLP to reconstruct it. \n\nFor edge generation, we'd like to model the probability for each masked edge. A naive solution is to simply regarding each edge independently and regard each pair as binary classification. However, the edgeage behavior is obviously correlated. To maintain the dependency of different output edge, we propose to use a contrastive loss~\\cite{DBLP:journals/corr/abs-1807-03748} for edge prediction. After getting the Edge Generation Node representations, we calculate the pairwise score of the new node $\\cV[\\pi_t]$ with each support node $\\cV[\\pi_j]$, using a DistMult product function $D(x,y) = x W y^T$. Then, we pick all the connected edge as positive samples, and unconnected edge as negative samples. For each positive sample, we calculate the loss by comparing it with the negative samples via $\\mathcal{L}^{edge}(i) = - \\sum_{j^{+}}\\log \\frac{\\exp\\big(D\\big(h^{Edge}(i),\\  h^{Edge}(j^{+})\\big)\\big)}{\\sum_{j^{-}}\\exp\\big(D\\big(h^{Edge}(i),\\ h^{Edge}(j^{-}\\big)\\big)}$. Such loss will force the score between target node with positive nodes to be higher than those with negative nodes, and thus can incorporate the dependency between them.\n\n\nAlso, to support parallel training, we want to conduct the forward pass for a single run and get the representation of the whole graph, so that we can simultaneously calculate the loss for each node, instead of do it recursively. To implement this, we remove all the edges from nodes with higher index to those with lower index, which means each node can only receive information from already generated nodes. In this way, we can get the all of the node representation that are suitable for generative pre-training in a single run, and thus can conduct efficient parallel training.\n\n\nWe show the detailed generation procedure in Figure~\\ref{fig:framework}. In the beginning, we should determine the node permutation order $\\pi$ for the graph generation. Next, for each node $\\cV[\\pi_i]$, we utilize all the previous generated nodes $\\cV[\\pi_{<t}]$ to generate its attribute and edges. For example, when we generate node $4$, we utilize nodes $1, 2$ and $3$ as support nodes to provide information. Next, we randomly select a portion of target node's edges as observed edges $E^{\\pi,\\tau}_{i, <t}$, and the remaining as masked edges $E^{\\pi,\\tau}_{i, \\geq t}$. We denote the masked edges by grey dashed with cross in the Figure~\\ref{fig:framework}(b). After such pre-processing, we use the modified adjacency matrix to calculate all of the node representation for both Attribute and Edge Generation Nodes. Finally,  as illustrated in Figure~\\ref{fig:framework}(c-e), we train the GNN via the attribute prediction and masked edge prediction task for each node in parallel.\n\n\n\\vpara{Difference with Edge Prediction.} Our edge generation is different from the traditional edge prediction task. The traditional way is to randomly remove part of the edge of the whole graph. In this way, most of the nodes only have one edge removed and to be predicted. Our task, on the contrary, enumerate the portion of masked edge by $t$. Thus, we may mask out a span of edges instead of few of them. For example, a node with ten edges may have nine of them masked out. In this way, the task becomes harder, and will force the pre-trained model to see longer dependency for filling out the whole span. For example, assume we have a paper published on KDD, and belongs to data mining field. If we only mask out its edge to KDD, we can have a easy estimation that it should be submitted to some data mining conferences based on its field, making the prediction too easy for the model to learn longer dependency. On the other hand, if we remove both the venue and field information, the model requires to see its citation paper's topics, its authors' publication records, etc, to determine its own topic, field, and venue. Such span masking can improve the performance against uniformly masking, which will be shown in the evaluation section.\n\n\n\\subsection{Pre-training for Large-Scale and Heterogeneous Graph}\nAll the previous discussion is based on a whole graph $G$. However, for those large-scale graphs, we cannot load the whole graph into GPU and conduct generative pre-training. A naive solution is to sample a subgraph and only conduct pre-training on it. In this way, we'll only use the sampled nodes to estimate the contrastive loss for edge prediction, instead of learning over the whole graph. Thus, the task becomes easier, and may provide weaker guidance for pre-training. To alleviate this issue, we propose \\textit{adaptive queue}, which stores previously calculated node representation as negative samples. Each time we process a new graph, we'll progressively update the queue by all the newly calculated node representations, and removing the oldest ones. As the queue size can be much larger than a typical sampled graph size, we can use much larger negative sample pools with this queue. Also, as the model parameter will not be updated rigorously, the negative samples stored in the queue are consistent and accurate.\n\n\nFor sampling, we propose to first sample a dense subgraph using the algorithm discussed in Hu \\textit{et al.}~\\cite{hgt, ladies}, where the nodes are highly interconnected with each other and maximally preserve the structural information. Based on this sampling algorithm, the nodes sampled in the upper layer have more structural connectivity and should get more accurate node representation. Therefore, when we determine the node permutation order $\\pi$, we follow the reversed order of sampling, which means that the nodes sampled first, which have richer structural information will be ranked at the bottom in $\\pi$. In this way, we can get more informative signal to pre-train GNNs based on these nodes.\n\n\nIn addition, many real-world graphs are heterogeneous, meaning that the node and edge types can be different, such as chemical molecules, knowledge graphs, academic graphs, etc. For those graphs, we can also use the same framework discussed above to pre-train a heterogeneous GNNs. The main difference is that each type of node and edge should have their own decoder. All the other components remain the same. \n\n% \\subsection{Robust Fine-tuning by Adversarial Training}\n% After pre-training, we can apply the learned GNNs as encoder to get node representations, apply them to any downstream tasks with task-specific decoder, and train them end-to-end. With a good initialization by pre-training, the model can normally converge faster and get a well-generalized result, even with few training samples.\n\n% However, in many graph-related tasks, such as semi-supervised node classification, the training data amount could be extremely small. Thus, conducting aggressive fine-tuning will make the adapted model overfit the few training labels, and forget the knowledge learned from pre-training. To address this issue and conduct robust fine-tuning, we propose to modify a widely adopted adversarial training algorithm TRADES~\\cite{DBLP:conf/icml/ZhangYJXGJ19} to stabilize the fine-tuning. The basic idea of TRADES is to make the decision boundary of the adapted model to be smooth regarding the few training samples, which can help the model preserve the pre-trained knowledge and thus generalize well.\n\n% Specifically, given a pre-trained GNN model $f_{\\theta}$, a graph with adjacency matrix $A$ and attribute $X$, with n labelled nodes $\\{y_i\\}_{i=1}^n$ for the downstream tasks, the regularized fine-tuning objective for each data point is:\n% \\begin{align}\n%     \\min_{\\theta} \\Big\\{ \\mathbb{L}\\big(f_{\\theta}(A, X)_i, y_i\\big) + \\max_{||\\hat{X}-X||_{\\infty} \\leq \\epsilon} KL\\big(f_{\\theta}(A, X)_i, f_{\\theta}(\\hat{A}, \\hat{X})_i\\big) \\Big\\}\n% \\end{align}\n\n% The first term is the training loss, where $\\mathbb{L}$ is the task-specifically loss, and the second term is the TRADES's regularization term for model smoothness, where KL denotes KL divergence between two probability, $\\hat{X}$ and $\\hat{A}$ is the perturbed attribute and adjacency matrix that within a small region of original $X$ and $A$. The basic workflow is: firstly, we randomly shuffle the input adjacency matrix within a limited budget (by changing k edges). Then, we optimize the second term to find a perturbed attribute $\\hat{X}$ which can maximize the decision gap from the original one. This perturbed $\\hat{X}$ and $\\hat{G}$ can be regarded as an adversarial example. Finally, we optimize the regularized loss, so as to get an accurate model with a smooth decision boundary.\n\n\n\n\n\n\n}%end of hide\n\n    \n"
                }
            },
            "section 4": {
                "name": "Evaluation",
                "content": "\\label{sec:evaluation}\n\nTo evaluate the performance of \\method, we conduct experiments on the Open Academic Graph (OAG) and Amazon Recommendation datasets.\nTo evaluate the generalizability of \\method, we consider different transfer settings---time transfer and field transfer---which are of practical importance. \n%To further consider a realistic transfer environment, we consider both domain transfer and Time transfer setting. \n\n\n\n% \\begin{table*}[th]\n% \\centering\n% \\footnotesize\n% \\renewcommand\\arraystretch{1.2}\n% \\begin{tabular}{c|r|r|rrrrr|rrrrr} \n% \\toprule\n% Dataset & $\\#$nodes & $\\#$edges & $\\#$papers & $\\#$authors & $\\#$fields & $\\#$venues & $\\#$institutes & $\\#$P-A & $\\#$P-F & $\\#$P-V & $\\#$A-I & $\\#$P-P \\\\\n% \\midrule\n% CS & 11,732,027 & 107,263,811  & 5,597,605 & 5,985,759 &  119,537&  27,433\u202c&  16,931   & 15,571,614 & 47,462,559 & 5,597,606 & 7,190,480 & 31,441,552\\\\\n% \\midrule\n% Med &51,044,324 & 451,468,375\u202c& 21,931,587 & 28,779,507&  289,930&  25,044&  18,256 &85,620,479 & 149,728,483&21,931,588 & 28,779,507& 165,408,318\\\\\n% \\midrule\n% OAG &178,663,927 & 2,236,196,802  & 89,606,257 & 88,364,081 &  615,228&  53,073&  25,288 & 300,853,688&  657,049,405&  89,606,258&  167,449,933 & 1,021,237,518\\\\\n% \\bottomrule\n% \\end{tabular}\n% \\caption{Open Academic Graph (OAG) Statistics.} \n% \\label{tab:stat} \n% \\end{table*}\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "Experimental Setup",
                    "content": "\n\n\\vpara{Datasets and Tasks.}We conduct experiments on both heterogeneous and homogeneous graphs. \nFor heterogeneous graphs, we use \n%We conduct experiments on two publicly-available heterogeneous graphs: \nthe Open Academic Graph and Amazon Review Recommendation data. \nFor homogeneous graphs, we use the Reddit dataset~\\cite{graphsage} and the paper citation network extracted from OAG. \nAll datasets are publicly available and the details can be found in Appendix A.\n\n\\textit{Open Academic Graph (OAG)}~\\cite{wang2020mag,DBLP:conf/kdd/ZhangLTDYZGWSLW19,tang2008arnetminer} contains more than 178 million nodes and 2.236 billion edges. It is the largest publicly available heterogeneous academic dataset to date. Each paper is labeled with a set of research topics/fields (e.g., Physics and Medicine) and the publication date ranges from 1900 to 2019. \n%In addition, all papers in OAG are associated with their publication dates from 1900 to 2019, and also associated with different topics, such as CS, Medical, etc. \nWe consider the prediction of Paper--Field, Paper--Venue, and Author Name Disambiguation (Author ND) as three downstream tasks~\\cite{hgt,dong2020HeterNRL}. The performance is evaluated by MRR---a widely adopted ranking metric~\\cite{DBLP:books/daglib/0027504}.\n%, as the evaluation measures for these tasks. \n\n\n\\textit{Amazon Review Recommendation Dataset (Amazon)}~\\cite{DBLP:conf/emnlp/NiLM19}  contains 82.8 million reviews, 20.9 million users, and 9.3 million products. \nThe reviews are published from 1996 to 2018. \nEach review consists of a discrete rating score from 1 to 5 and a specific field, including book, fashion, etc. For downstream tasks, we predict the rating score as a five-class classification task within the Fashion, Beauty, and Luxury fields. We use micro F1-score as the evaluation metric.\n% \\dong{don't understand the problem setting....and also fig 4} \\znh{just a node classification (classify the rating for each review). Fig 4 shows the schema of such graph, I treat some discrete features of reviews as nodes.}\n\n%\\textit{}\n\n\n\n%\\subsection{Experimental Setup}\n\n\\vpara{The base GNN model.} On the OAG and Amazon datasets, we use the state-of-the-art heterogeneous GNN---Heterogeneous Graph Transformer (HGT)~\\cite{hgt}---as the base model for \\method. \nFurthermore, we also use other (heterogeneous) GNNs as the base model to test our generative pre-training framework. \n\n%In addition, we  conduct ablation studies to analyze the importance of each sub-component of the \\method framework, including the node separation (Attribute Node \\& Edge Node in Section \\ref{subsec:attr-edge-nodes}) and adaptive queue. %, and GNN architectures. \n\n\\vpara{Implementation details.}For all base models, we set the hidden dimension \n% \\kw{\"throughout the networks\" sounds weird and I'm not sure what you want to say. Perhpas you should briefly describe what is the architecture of HGT.}\nas 400, the head number as 8, and the number of GNN layers as 3. \nAll of them are implemented using the PyTorch Geometric (PyG) package~\\cite{pyG}. \n\n\nWe optimize the model via the AdamW optimizer~\\cite{DBLP:conf/iclr/LoshchilovH19} with the Cosine Annealing Learning Rate Scheduler~\\cite{DBLP:conf/iclr/LoshchilovH17} with 500 epochs and select the one with the lowest validation loss as the pre-trained model. \nWe set the adaptive queue size to be 256. \n\nDuring downstream evaluation, we fine-tune the model using the same optimization setting for 200 epochs as that in pre-training. We train the model on the downstream tasks for five times and report the mean and standard deviation of test performance.\n\n%\\kw{I think it is unclear what does \"unsupervised pre-training baseline\" mean here. You need a few sentences to describe there are several alternative objectives can be used to pre-train a model. We compare the proposed pre-training objective with other objectives that used for initializing GNNs...}\\zn{Try to explain it}\n\\vpara{Pre-training baselines.}There exist several works that propose unsupervised objectives over graphs, which can potentially be used to pre-train GNNs. We thus compare the proposed \\method\\ framework with these baselines:\\begin{itemize}\n    \\item \\textit{GAE}~\\cite{DBLP:journals/corr/KipfW16a}, which denotes graph auto-encoders, focuses on a traditional link prediction task. It randomly masks out a fixed proportion of the edges and asks the model to reconstruct these masked edges.\n    \\item \\textit{GraphSAGE (unsp.)}~\\cite{graphsage} forces connected nodes to have similar output node embeddings. Its main difference with GAE lies in that it does not mask out the edges during  pre-training.\n    \\item \\textit{Graph Infomax}~\\cite{DBLP:journals/corr/abs-1809-10341} tries to maximize the local node embeddings with global graph summary embeddings. Following its setting for a large-scale graph, for each sampled subgraph, we shuffle the graph to construct negative samples. \n\\end{itemize}\nIn addition, we also evaluate the two pre-training tasks in \\method by using each one of them alone, that is, attribute generation---\\textit{\\method (Attr)}---and edge generation---\\textit{\\method (Edge)}.\n\n\n%Attribute Generation and Edge Generation. Finally, we report the results of the Graph Generation, which use these two pre-training tasks together. We also add experiments that remove the two major components of our framework, the node separation and adaptive queue, under the combined transfer setting. KW: I remove the above two setnences as they are too lengthy.\n\n% As it adds mask on the input graph, the problem is harder than the previous baseline, and asks the model to utilize longer-range context to make decision instead of just its local neighborhood.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%\\vpara{Domain Transfer Setting.}\n"
                },
                "subsection 4.2": {
                    "name": "Pre-Training and Fine-Tuning Setup",
                    "content": "\n\nThe goal of pre-training is to transfer knowledge learned from numerous unlabeled nodes of a large graph to facilitate the downstream tasks with a few labels. Specifically, we first pre-train a GNN and use the pre-trained model weights to initialize models for downstream tasks. We then fine-tune the models with the downstream task specific decoder on the training (fine-tuning) set and evaluate the performance on the test set. \n%For different task, we add different task-specific decoders, and then use the training dataset to fine-tune the model. \n%Finally, we use the tuned model to evaluate on the testing dataset.\n\nBroadly, there are two different setups. \nThe first one is to pre-train and fine-tune on exactly the same graph. \nThe second one is relatively more practical, which is to pre-train on one graph and fine-tune on unseen graphs of the same type as the pre-training one. % as the pre-training graph. \n%In practice, the unlabeled data might have a distribution gap with the target labeled data, making the transfer of knowledge challenging. \n%For example, online shopping sites usually have new domains (fields) emerging over time, but we still expect a pre-trained model trained from existing fields can generalize well on the new ones. \n%Therefore, it is practically crucial that the pre-trained models can also support the transfer between different fields. \n%To serve the practical and challenging scenario, we use one field of data for pre-training and conduct downstream evaluation to different fields. \n%\\kw{I think you don't need the explanation that you have to use data in the same domains for pretraining. This sounds that your approach cannot be generalized. You probably just need to say, we pre-train the model on a domain similar to the downstream tasks. Then introduce the data splits. Also this sounds contradict to the field transfer setting.}\\zn{This example tries to explain that pre-training should be able to transfer cross domain, instead of saying we need pre-train in the same domain. I've changed the writing to clarify it.}\n%For OAG, we use the graph in the field of medicine for pre-training and those in computer science (CS) for downstream evaluation. \n%For Amazon, we pre-train on \\dong{xxx} products and fine-tune on products from the Fashion, Beauty, and Luxury fields. \nSpecifically, we consider the following three graph transfer settings between the pre-training and fine-tuning stages: \n%The remaining question is which data we should use for pre-training. We consider three different domain transfer settings:\n\\begin{itemize}\n    \\item \\textit{Time Transfer}, where we use data from different time spans for pre-training and fine-tuning.\n    For both OAG and Amazon, we use data before 2014 for pre-training and data since 2014 for fine-tuning. \n    %Since we have already chosen nodes after 2014 for downstream evaluation, we choose all nodes before 2014 for pre-training in this setting.\n    \n    \\item \\textit{Field Transfer}, where we use data from different fields for pre-training and evaluating. \n    %consider each field as a different domain, and the data for pre-training and evaluating are from different fields. \n    In OAG, we use papers in the field of computer science (CS) for downstream fine-tuning and use all  papers in the remaining fields (e.g., Medicine) for pre-training. \n    In Amazon, we pre-train on products in Arts, Crafts, and Sewing, and fine-tune on products in Fashion, Beauty, and Luxury. \n    \n%    we choose products from Arts, Crafts and Sewing field for pre-training. \n    \\item \\textit{Time + Field Transfer}, where we use the graph of particular fields before 2014 to pre-train the model and use the data from other fields since 2014 for fine-tuning. \n    Intuitively, this combined transfer setting is more challenging than the transfer of time or field alone. \n    For example, we pre-train on the OAG graph except CS field before 2014 and fine-tune on the CS graph since 2014.  \n%    . For downstream evaluation, we use the data from other fields after 2014, which makes this setting more challenging.  \n%    T, and this field is different from downstream evaluation's fields. \n     %The most challenging setting is to combine both field transfer and time transfer. \n    %\\kw{what do you mean by \"restricted fields\"?}\\zn{same to field transfer, which is: fields except CS for OAG, and fields from Arts, Crafts and Sweing for Amazon}\n\\end{itemize}\n\n\nDuring fine-tuning, for both datasets, we choose nodes from 2014 to 2016 for training, 2017 for validation, and since 2018 for testing. \nTo meet the assumption that training data is usually scarce, we only provide 10\\% of the labels for training (fine-tuning) by default, while the ablation study over different data percentages is also conducted.\nDuring pre-training, we randomly select a subset of the data as the validation set. %\\kw{need to say what is the size of the validation set} \n\n%In the following, we test our pre-training methods against other baselines on both OAG and Amazon datasets, under these three different transfer settings.\n%In the following, we show that the proposed \\method\\ improves the performance of downstream models for both OAG and Amazon in the transfer settings. \n\n%against other baselines on both OAG and Amazon datasets, under these three different transfer settings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Experimental Results",
                    "content": "\n\nWe summarize the performance of downstream tasks with different pre-training methods on OAG and Amazon in Table ~\\ref{tab:result}. \nAs discussed above, we setup three different transfer settings between pre-training and fine-tuning stages: Field Transfer, Time Transfer, and Field + Time Combined Transfer, as organized in three different blocks in the Table. % ~\\ref{tab:result}), fine-tuning on the training set over different tasks and datasets. \n\nOverall, the proposed \\method\\ framework significantly enhances the performance for all downstream tasks on both datasets. \nOn average, \\method achieves relative performance gains of  13.3\\% and 5.7\\% over the base model without pre-training on OAG and Amazon, respectively. \nMoreover, it consistently outperforms other pre-training frameworks, such as Graph Infomax, across different downstream tasks for all three transfer settings on both datasets. \n\n\n%It consistently outperforms other pre-training baselines across all different settings and achieves performance gain of 9.1\\% and 5.7\\% (i.e., the performance difference after pre-training divided by the baseline performance without pre-training) on OAG and Amazon respectively, averaging over different task and transfer setting. \n% \\kw{it is unclear how these averages are computed also I think you can give example and refer reader to see a specific row of results. Otherwise, it is hard to make sense the table.} \n\n\\vpara{Different transfer settings.}Observed from Table \\ref{tab:result}, the performance gain lifted by pre-training under the field transfer is higher than that under the time transfer, and the time + field combined transfer is the most challenging setting as evident in the least performance gain brought by pre-training. \n% \\kw{I don't quite understand this sentence what does distribution gap mean here and it is unclear why the larger in gain implies the gap is smaller.} \nNonetheless, under the combined transfer, \\method\\ still achieves 11.7\\% and 4.6\\% performance gains on both datasets, respectively. \nAltogether, the results suggest that \\textit{the proposed generative pre-training strategy enables the GNN model to capture the generic structural and semantic knowledge of the input graph, which can be used to fine-tune on the unseen part of the graph data. }\n\n\n\n\\vpara{Ablation studies on pre-training tasks.}We analyze the effectiveness of the two pre-training tasks in \\method---attribute generation and edge generation---by examining which of them is more beneficial for the pre-training framework and, by extension, downstream tasks. \n%We further conduct ablation studies to analyze the proposed framework. \n%We analyze which pre-training task is most beneficial for a given downstream task. \n%We report the downstream performance by only utilizing Attribute Generation or Edge Generation. \nIn Table \\ref{tab:result}, we report the performance of \\method by using attribute generation and edge generation alone, that is, \\method (Attr) and \\method (Edge). \nOn OAG, the average performance gains by \\method (Attr) and \\method (Edge) are 7.4\\% and 10.3\\%, respectively, suggesting that Edge Generation is a more informative pre-training task than Attribute Generation in \\method. \nHowever, we have an opposite observation for Amazon, on which the performance improved by Attribute Generation is 5.2\\% in contrast to the 4.1\\% improvement lifted by Edge Generation.  \n%While for Amazon, the average pre-training gains are 5.2\\% and 4.1\\%, meaning that Attribute Generation is more beneficial for Amazon rate prediction task. \n\\textit{This suggests that the \\method framework benefits differently from attribute and edge generations on different datasets. \n%for different datasets, the most beneficial pre-training task is different. \nHowever, combining the two pre-training tasks together produces the best performance on both cases. \n%Overall, combining these two tasks together achieves the best results.\n}\n\n\nWe further compare the Edge Generation task against other edge-based pre-training methods---GAE and GraphSage (unsp.)---in Table \\ref{tab:result}. \nOn OAG, the performance improvements brought by \\method's edge generation, GAE, and GraphSage over no pre-training are 10.3\\%, 7.4\\%, and 4.0\\%, respectively. \nOn Amazon, the gains are 5.2\\%, 3.1\\%, and 1.3\\%, respectively. \n%We then analyze the effectiveness of the proposed Edge Generation task against existing edge-based pre-training methods. \n%Comparing Edge Generation with GAE, GraphSAGE (which can be regarded as GAE without mask) on OAG, the average performance gains are  7.9\\%, 5.6\\%, and 1.4\\% respectively. \n%For Amazon, the gains are 5.2\\%, 3.1\\%, and 1.3\\%. \nFrom the comparisons, we have the following observations. \n%These results show that: 1) \nFirst, both GAE and \\method's edge generation offer better results than GraphSage on both datasets, demonstrating that \n%and  are better than that of GraphSAGE, showing that \nmasking on edges is an effective strategy for self-supervised graph representation learning. \nWithout edge masking, the model simply retains a similar embedding for connected nodes, as the label we would like to predict (whether two nodes are linked) has already been encoded in the input graph structure. Such information leakage will downgrade the edge prediction task to a trivial problem. \n%2) \nSecond, the proposed Edge Generation task consistently outperforms GAE. \nThe main advantage of \\method's edge generation comes from that \n%The main difference is that our method captures \n%it captures the dependencies between the masked edges and \nit learns to generate missing edges autoregressively and thus can capture the dependencies between the masked edges, which are discarded by GAE. \n%, wh GAE simply discards such dependency. \n%The experimental \n\\textit{In summary, the results suggest that the proposed graph generation tasks can give informative self-supervision for GNN pre-training.}\n\n\n\n\n\n\n\n\n\\vpara{Ablation studies on the base GNN.}\nWe investigate whether the other GNN architectures can benefit from the proposed pre-training framework. \n%We investigate whether HGT is the most powerful base GNN model for the \\method pre-training framework. \n%which GNN architectures are the most suitable for our generative pre-training framework. \nTherefore, in addition to HGT~\\cite{hgt}, we try\n%We choose \nGCN~\\cite{gcn}, GAT~\\cite{gat}, RGCN~\\cite{DBLP:conf/esws/SchlichtkrullKB18}, and HAN~\\cite{DBLP:conf/www/WangJSWYCY19} as the base model. \n%as comparison, \nSpecifically, we pre-train them on OAG and then use the paper-field prediction task under the combined transfer setting with 10\\% of training data for fine-tuning and testing. \nModel-independent hyper-parameters, such as the hidden dimension size and optimization, are set the same. \nThe results are reported in in Table \\ref{tab:gnn}. \n%As is illustrated in Table\\ref{tab:gnn}, \nWe can observe that 1) HGT achieves the best performance among all non pre-trained GNN models; \n2) \\method with HGT generates the most promising results for the concerned downstream task; \nand 3) \\textit{more importantly, the proposed \\method pre-training framework can enhance the downstream performance for all the GNN architectures.}\n%and 3) \n%Overall, \n%Besides, our adopted heterogeneous graph transformer (HGT) achieves the largest downstream performance. This shows that an expressive GNN architecture can get more improvement from pre-training.\n\n\n\\vpara{Ablation studies on the node separation and adaptive queue.} Finally, we examine the effectiveness of the two design choices of \\method, i.e., node separation and adaptive queue. \n%components of our framework, the node separation, adaptive queue, and GNN architectures. \nThe node separation is designed for alleviating the information leakage problem for the Attribute Generation task. \nWithout this component, the attributes would appear in the input and thus the pre-training method would only need to maintain the input features for output. \nIn other words, it cannot learn any knowledge of the input graph that could be transferred to downstream tasks and thus affect the results negatively. \n%This can be reflected in the experimental results. \nFrom Table \\ref{tab:result}, we can see that \n%after removing the node separation component, \nthe attribute generation based pre-training model suffers from the removal of the node separation component (w/o node separation), \n%cannot bring benefits to the downstream tasks, \nand in many cases, its performance is even worse than the ones without pre-training. \n\\textit{This demonstrates the significance of this node separation design in avoiding\n%separating two sets of nodes and avoid \nattribute information leakage. }\n\n\nThe adaptive queue is designed for alleviating the gap between the sampled graphs and the full graph. \nSimilarly, we conduct the ablation study by removing it from the Edge Generation based pre-training model and from Table \\ref{tab:result}, we witness the consistent performance drops for all tasks---\\method (Edge) vs. (w/o adaptive queue). \n%After removing this component, the average performance gain is 2.7\\%, which is 2.5\\% lower than Edge Generation. \n\\textit{This indicates that adding more negative samples by using the adaptive queue is indeed helpful to the pre-training framework.}\n\n\n\n\n\n\n\n\n\n\n\n\\vpara{Training data size.}In Figure~\\ref{fig:percentage}, we examine whether the proposed \\method\\ method can generalize well with different training data size during fine-tuning, i.e., from 10\\% to 100\\%. \n%As is illustrated in Figure~\\ref{fig:percentage}, \nFirst, we can observe that \\method\\ and other pre-training frameworks consistently improve the downstream task performance with more labeled training data. %, and performs the best among all the pre-training tasks. \nSecond, it is clear that \\method performs the best among all pre-training tasks/frameworks.\nFinally, we can see that \\textit{with the pre-trained model, fine-tuning with only 10--20\\% of data (the two leftmost blue circles) generates comparative performance to the supervised learning with all 100\\% of training data (the rightmost purple diamond), demonstrating the superiority of GNN pre-training, especially when the label is scarce.}\n\n\n\n\n\n\\vpara{Results for homogeneous graphs.}In addition to heterogeneous graphs, we also test whether the \\method pre-training framework can benefit downstream tasks on homogeneous graphs. \nSpecifically, we pre-train and fine-tune on two homogeneous graphs: 1) the paper citation network extracted from the field of CS in OAG, on which the topic of each paper is predicted; 2) the Reddit network consisting of Reddit posts, on which the community of each post is inferred.\nAs there is only one type of nodes and edges in homogeneous graphs, we  require one single set of edge and attribute decoders for pre-training. \nHGT is used as the base pre-training model by ignoring its heterogeneous components.  \nThe fine-tuned results with 10\\% labeled data are presented in Table~\\ref{tab:homo}. \nWe can observe that the downstream tasks on both homogeneous graphs can benefit from all pre-training frameworks, among which the proposed \\method offers the largest performance gains. \n\n%Similarly, we apply different pre-training methods with HGT as initialization, and fine-tune the models with 10\\% labelled data. As is illustrated in table~\\ref{tab:homo}, even for simple-structured graph, our proposed \\method still enjoys significant performance gain, and outperforms all the baselines.\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\\label{sec:conclusion}\n\n\n\n\n\n\n\nIn this work, we study the problem of graph neural network pre-training. \nWe present \\method---a  \n%we propose the \ngenerative GNN pre-training framework. \n%Given a large-scale attributed graph,\nIn \\method, we design the graph generation factorization to guide the base GNN model to autoregressively reconstruct both the attributes and structure of the input graph. \nFurthermore, we propose to separate the attribute and edge generation nodes to avoid information leakage. \nIn addition, we introduce the adaptive node representation queue to mitigate the gap between the likelihoods over the sampled graph and the full graph. \nThe pre-trained GNNs with fine-tuning over few labeled data can achieve significant performance gains on various downstream tasks across different datasets. %Comprehensive experiments demonstrate that our proposed framework can significantly enhance the node classification and link prediction performance on both Microsoft Academic Graphs (OAG) and Amazon recommendation datasets, for 9.1\\% and 5.7\\% on average. \n%KW: I think the above sentence is not needed as it's already highlighted in the abstarct\n%Finally, we demonstrae that \nAlso,  \\method  is robust to different transfer settings between pre-training and fine-tuning. \n% and fine-tuning data percentage.\nFinally, we find that fine-tuning the generative pre-trained GNN model with 10--20\\% of labeled data offers comparative performance for downstream tasks to the supervised GNN model with 100\\% of training data. \n\n\n\n\\vpara{Acknowledgements.} This work is partially supported by NSF III-1705169, NSF CAREER Award 1741634, NSF 1937599, DARPA HR00112090027, DARPA N660011924032, Okawa Foundation Grant, and Amazon Research Award.\n% \\clearpage\n\\bibliographystyle{ACM-Reference-Format}\\balance\n\\bibliography{kdd}\n\n\\newpage\n\\appendix\n\n\n\n\n\n\n\n\n\n\n\n\n"
            },
            "section 6": {
                "name": "Dataset Details",
                "content": "\n\nWe mainly use \\textit{Open Academic Graph (OAG)} and \\textit{Amazon Review Recommendation Dataset (Amazon)} for evaluation. Both are widely used heterogeneous graph~\\cite{Sun:VLDB11, DBLP:conf/kdd/SunNHYYY12, Sun:BOOK2012}. Here we introduce their statistics, schema and how we prepare the attributes and tasks in detail.\n\n% We mainly use Heterogeneous Graph Transformer (HGT) as our base model through this paper. It takes input a heterogeneous graph $G = (\\cV, \\cE, \\cX, \\cA, \\cR)$, where where each node $v \\in \\cV$ and each edge $e \\in \\cE$ are associated with their type mapping functions $\\tau(v): V \\rightarrow \\cA$ and $\\phi(e): E \\rightarrow \\cR$, respectively. For an edge $e=(s, t)$ linked from source node $s$ to target node $t$, we define its \\textit{meta relation} as $\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$.\n\n% The basic idea of HGT is to implement an attention-based GNN framework described in equation~\\ref{eq:att}, while each weight matrices are parameterized by the link's meta relations, so that we can maximize parameter sharing while still maintaining specific characteristics of different relations. The detailed model design can be found in Hu \\textit{et al.}~\\cite{hgt} and their released code\\url{https://github.com/acbull/pyHGT}.\n% 1/3 page\n\n\\textit{Open Academic Graph (OAG)}~\\cite{DBLP:conf/kdd/ZhangLTDYZGWSLW19,wang2020mag,tang2008arnetminer} consists of five types of nodes: `Paper', `Author', `Field', `Venue', and `Institute', and 14 types of edges between these nodes. The schema and meta relations are illustrated in Figure~\\ref{fig:schema}(a). For example, the `Field' nodes in the OAG are categorized into six levels from $L_0$ to $L_5$, which are organized with a hierarchical tree (We use `is\\_organized\\_in' to represent this hierarchy). \nTherefore, we differentiate the `Paper--Field' edges in the corresponding field levels. \nBesides, we differentiate the different author orders (i.e., the first author, the last one, and others) and venue types (i.e., journal, conference, and preprint) as well. \nFinally, the `Self' type corresponds to the self-loop connection, which is widely added in GNN architectures. Despite `Self' and `CoAuthor' edge relationships, which are symmetric, all other edge types $X$ have a reverse edge type $X^{-1}$. \n\nFor downstream tasks, we choose the following three tasks: the prediction of Paper--Field (L2), Paper--Venue, and Author Disambiguation. In the first two tasks, we give a model a paper and want it to predict the correct fields it belongs to or the venue it is published at. We model these three tasks as node classification problem, where we use GNNs to get the contextual node representation of the paper and use a softmax output layer to get its classification results. \nFor author disambiguation, we pick all the authors with the same name, and the papers that link to one of these same-name authors. The task is to conduct link prediction between papers and candidate authors. \nAfter getting the paper and author node representations from GNNs, we use a Neural Tensor Network to get the probability of each author-paper pair to be linked. \n\n\nFor input attributes of heterogeneous graph, as we don't assume the attribute of each data type belongs to the same distribution, and we are free to use the most appropriate attributes to represent each type of node. For paper and author nodes, the node numbers are extremely large. Therefore, traditional node embedding algorithms are not suitable for extracting attributes for them. \nWe, therefore, resort to the paper titles for extracting attributes. For each paper, we get its title text and use a pre-trained XLNet~\\cite{xlnet, wolf2019transformers} to get the representation of each word in the title. We then average them weighted by each word's attention to get the title representation for each paper. The initial attribute of each author is simply an average of his/her published papers' embeddings. For field, venue and institute nodes, the node numbers are small and we use the metapath2vec model~\\cite{dong2017metapath2vec} to train their node embeddings by reflecting the heterogeneous network structures. \n\n\n\\textit{Amazon Review Recommendation Dataset (Amazon)}~\\cite{DBLP:conf/emnlp/NiLM19} consists of three types of nodes, including reviews (ratings and text), users and products, and some other meta-types of the product, including color, size, style and quantity. The schema and meta relations are illustrated in Figure~\\ref{fig:schema}(b). Compared to a general user-item bipartite graph, this dataset have review in between, each is associated with text information and a rating from 1 to 5. The reviews are also associated with those product meta-type descriptions. For simplicity, we only consider these review-type link as `categorize\\_in' type. Thus, there are totally three types of relations in this graph.\n\nFor downstream task, we choose the rating classification for each new review. Since the problem is a node-level multi-class classification, we use GNNs to get the contextual node representation of the review and use a softmax output layer to get 5-class prediction.\n\n\nFor input attributes of Amazon, we also use a pre-trained XLNet to get each review embedding, and the attributes for all the other nodes are simple an average of its associated review's embeddings.\n\n\n% 2/3 page\n\\hide{% the algo box was moved to Section Model\n\\section{Overall \\method\\ Implementation}\n\n\\begin{algorithm}[h!] \n\\caption{Overall \\method\\ Implementation} \n\\label{alg:pipeline} \n\\begin{algorithmic}[1] \n\\REQUIRE\nInput Attributed Graph $G$, Graph Sampler $S(\\cdot)$.\\\\\n\\ENSURE\n\\STATE Initialize a GNN Model as $f_{\\theta}$, attribute generation decoder as ${Dec}^{Attr}$, edge generation decoder as ${Dec}^{Edge}$\n\\STATE initialize adaptive node embedding queue $Q=\\{\\}$, initialize attribute vector $h^{init}$.\n \\FOR {each sample graph $\\hat{G} \\in S(G)$}\n    \\STATE  Determine node permutation order $\\pi$ for the sampled graph $\\hat{G}$. Delete all the all edges from high-order node to low-order node according to $\\pi$. \\label{alg:step1}\n    \\STATE For each node, sample observed edge index $o$ and masked edges $\\neg o$. Delete masked edges $E^{\\pi}_{i, \\neg o}$ accordingly. \\label{alg:step2}\n    \\STATE Separate each node into attribute generation and edge generation node. replace the input to attribute generation node as $h_{init}$. Apply GNN $f_{\\theta}$ to get two sets of node embeddings $h^{Attr}$ and $h^{Edge}$ for each node in the graph. \\label{alg:step3}\n    \\FOR {node $i$ with attribute $X^{\\pi}_{i}$ and masked edges $A^{\\pi,\\tau}_{i, \\geq t}$}\n        \\STATE Calculate Attribute Generation Loss $\\mathcal{L}^{Attr}$ by Eq. \\ref{eq:gen} \\label{alg:step4}\n        \\STATE Prepare negative samples $S^{-}_i$ for edge generation by concatenating the unconnected nodes and adaptive queue $Q$. \\label{alg:step9}\n        \\STATE Calculate Edge Generation Loss $\\mathcal{L}^{Edge}$ by Eq. \\ref{eq:contrastive} \\label{alg:step5}\n    \\ENDFOR\n    \\STATE Optimize $\\theta$ by minimizing $\\mathcal{L}^{Attr}$ and $\\mathcal{L}^{Edge}$. \\label{alg:step6}\n    \\STATE update $Q$ by adding in $h^{Edge}$ and popping out most out-dated embeddings. \\label{alg:step7}\n\\ENDFOR\n\\RETURN Pre-trained model parameter $\\theta^*$ for downstream tasks\n\\end{algorithmic} \n\\end{algorithm}\n}%end of hide\n\n\n"
            },
            "section 7": {
                "name": "Overall Pipeline of GPT-GNN",
                "content": "\nThe overall pipeline of GPT-GNN is illustrated by Algorithm~\\ref{alg:pipeline}. Given an attributed graph $G$, we each time sample a subgraph $\\hat{G}$ as training instance for generative pre-training. The first step is to determine the node permutation order $\\pi$. To support parallel training, we want to conduct the forward pass for a single run and get the representation of the whole graph, so that we can simultaneously calculate the loss for each node, instead of processing each node recursively. Therefore, we remove all the edges from nodes with higher order to those with lower order according to $\\pi$, which means each node can only receive information from nodes with lower order. In this way, they won't leak the information to the autoregressive generative objective, and thus we only need one single round to get node embeddings of the whole graph, which can directly be utilized to conduct generative pre-training.\n\nAfterwards, we need to determine the edges to be masked out. For each node, we get all its outward edges, randomly select a set of edges to be masked out. This corresponds to line~\\ref{alg:step2}. Next, we conduct node separation and get contextualized node embeddings for the whole graph in line~\\ref{alg:step3}, which will be utilized to calculate generative loss. line~\\ref{alg:step4}-\\ref{alg:step5}. For both OAG and Amazon, the main nodes that contain meaningful attributes are paper and review nodes, which both have text feature as input. Thus, we only consider them for Attribute Generation, with a 2-layer LSTM as decoder. Note that in line~\\ref{alg:step9}, we prepare the negative samples by aggregating both the unconnected nodes within this sampled graph and the previously calculated embeddings stored in the adaptive queue $Q$. This can mitigate the gap between optimizing over sampled graph with over the whole graph. Finally, we optimize the model and update the adaptaive queue in line ~\\ref{alg:step6}-\\ref{alg:step7}. Afterwards, we can use the pre-trained model as initialization, to fine-tune on other downstream tasks.\n\n\n\n\n\n\n\n\n\n\n\n% psudo code here\n\n% 1/3 page\n\n\n"
            },
            "section 8": {
                "name": "Implementation Details and Convergence Curves",
                "content": "\nWe use a Tesla K80 to run both pre-training and downstream tasks. For graph sampling, we follow the HGSampling~\\cite{hgt, ladies} to sample subgraph over large-scale heterogeneous graph. For each type of node, we sample 128 nodes per layer. We repeat sampling for 6 times for OAG and average sampled nodes in the sub-graph is 3561 nodes. We repeat for 8 times for Amazon, and average sampled nodes is 1478. For each batch, we sample 32 graphs to conduct generative pre-training. During GPU training, we conduct multi-process sampling to prepare the pre-training data. Such CPU-GPU cooperation can help us save the sampling time.\n\n\n\nWe here illustrate the convergence curves for pre-training and fine-tuning. For pre-training, as illustrated in Figure~\\ref{fig:cv1}, we show the pre-training validation error curve with respect to the  epoch and time. Results show that the model's validation loss keep dropping instead of just finding a trivial solution very fast. This to some extent shows that the generative pre-training task is hard enough and can thus can guide the model to really capture the intrinsic structure of the graph data. It took about  12 hours for \\method\\ to converge. For downstream tasks, we show the convergence curve utilizing our \\method\\ with no-pretrain, with different data percentage. As is illustrated in Figure~\\ref{fig:cv2}, \\method\\ can always get a more generalized model than no-pretrain, and is more robust to over-fitting since a good initialization from pre-training.\n\n"
            },
            "section 9": {
                "name": "Paper Title Generation Examples",
                "content": "\nFor OAG, since our attribute generation task is oriented on the paper title, we'd like to see how well our \\method \\ can learn to generate the title. The results are shown in table~\\ref{tab:gen}. We can see that the model can capture the main meaning of each paper to be predicted, only by looking at partial neighborhoods (note that we use Attribute Generation Node for this task, which replace the input attribute as a share vector). For example, for the first sentence, our model successfully predict the key words for this paper, including `person recognition', `probabilistic', etc. This shows that the graph itself contains rich semantic information, and explains why a pre-trained model can generalize well to downstream tasks.\n\n\n\n\n\n% that's all folks\n"
            }
        },
        "tables": {
            "tab:result": "\\begin{table*}[!th]\n\\centering\n% \\hspace{20cm}\n\\begin{tabular}{llcccccccccc}\n\\toprule\n%~ & \\multirow{2}{*}{\\tabincell{l}{Downstream Dataset \\\\ Evaluation Task}}  & & \\multicolumn{3}{c}{OAG} & & & \\multicolumn{3}{c}{Amazon}  & \\\\ \\cmidrule(lr){4-6} \\cmidrule(lr){9-11}\n & {Downstream Dataset} &&  \\multicolumn{3}{c}{OAG} & & & \\multicolumn{3}{c}{Amazon}  & \\\\ \\cmidrule(lr){1-2}  \\cmidrule(lr){4-6} \\cmidrule(lr){9-11}\n& {Evaluation Task}    && Paper--Field    & Paper--Venue    & Author ND & & & Fashion & Beauty & Luxury \\\\ \\toprule\n~ & No Pre-train && .336$\\pm$.149 & .365$\\pm$.122    &  .794$\\pm$.105 &&& .586$\\pm$.074 & .546$\\pm$.071 & .494$\\pm$.067 \\\\ \\toprule\n\\multirow{6}{*}{\\rotatebox[origin=c]{90}{{Field Transfer}}}  & GAE && .403$\\pm$.114  &  .418$\\pm$.093   &  .816$\\pm$.084  &&&  .610$\\pm$.070  & .568$\\pm$.066 & .516$\\pm$.071     \\\\ \n~ & GraphSAGE (unsp.)&& .368$\\pm$.125  &  .401$\\pm$.096   &  .803$\\pm$.092 &&&  .597$\\pm$.065  & .554$\\pm$.061 & .509$\\pm$.052      \\\\ \n~ & Graph Infomax && .387$\\pm$.112  &  .404$\\pm$.097   &  .810$\\pm$.084  &&& .604$\\pm$.063  & .561$\\pm$.063 & .506$\\pm$.074    \\\\ \\cmidrule{2-11}\n~ & \\method\\ (Attr) & &   .396$\\pm$.118   &  .423$\\pm$.105 &  .818$\\pm$.086   && &  .621$\\pm$.053  &   .576$\\pm$.056   &  .528$\\pm$.061      \\\\\n~ & \\method\\ (Edge) &&   .401$\\pm$.109  &   .428$\\pm$.096  &  .826$\\pm$.093 && &   .616$\\pm$.060  &   .570$\\pm$.059  & .520$\\pm$.047             \\\\\n~ & \\method\\   &&   \\textbf{.407$\\pm$.107}    &  \\textbf{.432$\\pm$.098}    &   \\textbf{.831$\\pm$.102}    &  & &\\textbf{.625$\\pm$.055}   &   \\textbf{.577$\\pm$.054}    &  \\textbf{.531$\\pm$.043} \\\\ \\toprule\n\\multirow{6}{*}{\\rotatebox[origin=c]{90}{{Time Transfer}}}  & GAE && .384$\\pm$.117  &  .412$\\pm$.101   &  .812$\\pm$.095 & &&  .603$\\pm$.065  & .562$\\pm$.063 & .510$\\pm$.071     \\\\ \n~ & GraphSAGE (unsp.)&& .352$\\pm$.121  &  .394$\\pm$.105   &  .799$\\pm$.093 &&&  .594$\\pm$.067  & .553$\\pm$.069 & .501$\\pm$.064      \\\\ \n~ & Graph Infomax && .369$\\pm$.116  &  .398$\\pm$.102   &  .805$\\pm$.089  & && .599$\\pm$.063  & .558$\\pm$.060 & .503$\\pm$.063    \\\\ \\cmidrule{2-11}\n~ & \\method\\ (Attr) & &   .382$\\pm$.114   &  .414$\\pm$.098 &  .811$\\pm$.089   && &  .614$\\pm$.057  &   \\textbf{.573$\\pm$.053}   &  .522$\\pm$.051      \\\\\n~ & \\method\\ (Edge) &&   .392$\\pm$.105  &   .421$\\pm$.102  &  .821$\\pm$.088 && &   .608$\\pm$.055  &   .567$\\pm$.038  & .513$\\pm$.058             \\\\\n~ & \\method\\   &&   \\textbf{.400$\\pm$.108}    &  \\textbf{.429$\\pm$.101}    &   \\textbf{.825$\\pm$.093}    &  & &\\textbf{.617$\\pm$.059}   &   .572$\\pm$.059    &  \\textbf{.525$\\pm$.057} \\\\ \\toprule\n\n\\multirow{8}{*}{\\rotatebox[origin=c]{90}{{Time + Field Transfer}}} & GAE && .371$\\pm$.124  &  .403$\\pm$.108   &  .806$\\pm$.102 & &&  .596$\\pm$.065  & .554$\\pm$.063 & .505$\\pm$.061     \\\\ \n~ & GraphSAGE (unsp.) && .349$\\pm$.130  &  .393$\\pm$.118   &  .797$\\pm$.097&  &&  .589$\\pm$.071  & .545$\\pm$.068 & .498$\\pm$.064      \\\\ \n~ & Graph Infomax && .360$\\pm$.121  &  .391$\\pm$.102   &  .800$\\pm$.093  & && .591$\\pm$.068  & .550$\\pm$.058 & .501$\\pm$.063    \\\\ \\cmidrule{2-11}\n~ & \\method\\ (Attr) & &   .364$\\pm$.115   &  .409$\\pm$.103 &  .809$\\pm$.094   && &  .608$\\pm$.062  &   .569$\\pm$.057   &  .517$\\pm$.057      \\\\\n~ & --- (w/o node separation) & &   .347$\\pm$.128   &  .391$\\pm$.102 &  .791$\\pm$.108   && &  .585$\\pm$.068  &   .546$\\pm$.062   &  .497$\\pm$.062      \\\\\n~ & \\method\\ (Edge) &&   .386$\\pm$.116  &   .414$\\pm$.104  &  .815$\\pm$.105 && &   .604$\\pm$.058  &   .565$\\pm$.057  & .514$\\pm$.047             \\\\\n~ & --- (w/o adaptive queue) &&   .376$\\pm$.121  &   .410$\\pm$.115  &  .808$\\pm$.104 && &   .599$\\pm$.068  &   .562$\\pm$.065  & .509$\\pm$.062             \\\\\n~ & \\method\\   &&   \\textbf{.393$\\pm$.112}    &  \\textbf{.420$\\pm$.108}    &   \\textbf{.818$\\pm$.102}    &  & &\\textbf{.610$\\pm$.054}   &   \\textbf{.572$\\pm$.063}    &  \\textbf{.521$\\pm$.049} \\\\ \\bottomrule\n\\end{tabular} \n\\caption{Performance of different downstream tasks on OAG and Amazon by using different pre-training frameworks with the heterogeneous graph transformer (HGT)~\\cite{hgt} as the base model. \n10\\% of labeled data is used for fine-tuning. \n%We report the results under different transfer settings with 10\\% fine-tuning data.\n% Our proposed Generative Pre-training framework can enhance the downstream evaluation performance for 9.1\\% and 5.7\\% to OAG and Amazon respectively, and it can consistently outperform all the other baselines under different settings.\n} \n\\label{tab:result}\n\\end{table*}",
            "tab:gnn": "\\begin{table}[!t]\n% \\small\n\\centering\n\\begin{tabular}{lccccc}\n\\toprule\nModel  & HGT & GCN & GAT & RGCN  & HAN \\\\ \\midrule\nNo Pre-train & \\textbf{.336} & .317   &  .308  &    .296 &    .322\\\\ \n\\method  & \\textbf{.407} & .349   &  .362  &    .351 &    .384\\\\ \nRelative Gain &   21.1\\% & 10.1\\%   &  17.5\\%  &    18.6\\% &    19.3\\%\\\\ \n\\bottomrule\n\\end{tabular} \n\\caption{Compare the pre-training Gain with different GNN architectures. Evaluate on OAG, Paper-Field Task, under Combined Transfer setting with 10\\% training data.} \n\\label{tab:gnn}\n\\vspace{-0.25cm}\n\\end{table}",
            "tab:homo": "\\begin{table}[!t]\n% \\small\n\\centering\n\\begin{tabular}{lcc}\n\\toprule\nDownstream Dataset  & OAG (citation) & Reddit \\\\ \\midrule\nNo Pre-train & .281$\\pm$.087 & .873$\\pm$.036   \\\\  \\midrule\nGAE & .296$\\pm$.095 &.885$\\pm$.039   \\\\ \nGraphSAGE (unsp.)  & .287$\\pm$.093 & .880$\\pm$.042  \\\\ \nGraph Infomax  & .291$\\pm$.086 & .877$\\pm$.034   \\\\ \n\\midrule\n\\method  & \\textbf{.309$\\pm$.081} & \\textbf{.896$\\pm$.028}  \\\\ \n\\bottomrule\n\\end{tabular} \n\\caption{Downstream performance on homogeneous graphs, including the paper citation network in OAG and Reddit. } \n\\label{tab:homo}\n\\vspace{-0.25cm}\n\\end{table}",
            "tab:gen": "\\begin{table*}[!t]\n\\centering\n\\small\n\\begin{tabular}{ll}\n\\toprule\nPredict Paper Title & Groundtruth Paper Title \\\\ \\midrule\nperson recognition system using automatic probabilistic classification & person re-identification by probabilistic relative distance comparison\n\\\\ \na novel framework using spectrum sensing in wireless systems   & a secure collaborative spectrum sensing strategy in cyber physical systems\\\\ \na efficient evaluation of a distributed data storage service storage  &   an empirical analysis of a large scale mobile cloud storage service\\\\ \nparameter control in wireless sensor networks networks networks  & optimal parameter estimation under controlled communication over sensor networks\\\\\na experimental system for for to the analysis of graphics &  an interactive computer graphics approach to surface representation\\\\\n\\bottomrule\n\\end{tabular} \n\\caption{Generated paper title samples. The left column is generated by \\method, and the right column is the groundtruth.} \n\\label{tab:gen}\n\\end{table*}"
        },
        "figures": {
            "fig:arch": "\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.475\\textwidth]{picture/gpt-intro.pdf}\n     %\\vspace{-0.3cm}\n    \\caption{The pre-training and fine-tuning flow of \\method: First, a GNN is pre-trained with the self-supervised learning task---attribute and structure generations. Second, the pre-trained model and its parameters are then used to initialize models for downstream tasks on the input graph or graphs of the same domain.}\n    \\label{fig:arch}\n    %\\vspace{-0.3cm}\n\\end{figure}",
            "fig:framework": "\\begin{figure*}[ht!]\n    \\centering\n    \\includegraphics[width=1\\textwidth]{picture/illustrate.pdf}\n    \\caption{An illustrative example of the proposed attributed graph generation procedure.}\n    \\label{fig:framework}\n\\end{figure*}",
            "fig:percentage": "\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.475\\textwidth, trim = 0 0 10 10, clip]{picture/percentage-pdf.pdf}\n    \\vspace{-0.65cm}\n    \\caption{Compare pre-training tasks with different training data size. \n    %\\YS{training data size at fine-tuning stage? avoid fine-tuning data size.} \n    Evaluated by the paper--field prediction task on OAG under the field transfer setting.}\n    \\label{fig:percentage}\n    \\vspace{0.3cm}\n\\end{figure}",
            "fig:schema": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=1.0\\textwidth, trim = 0 0 10 0, clip]{picture/schema.pdf}\n    \\caption{The schema and meta relations of Open Academic Graph and Amazon Review Recommendation Dataset.}\n    \\label{fig:schema}\n\\end{figure*}",
            "fig:cv2": "\\begin{figure*}[t!]\n    \\subfigure[Data Percentage: 10\\%]{\n        \\label{fig:d_s}\n        \\includegraphics[width=0.3\\textwidth]{picture/10.pdf}\n    }\n    \\subfigure[Data Percentage: 20\\%]{\n        \\label{fig:d_u}\n        \\includegraphics[width=0.3\\textwidth]{picture/20.pdf}\n    }\n    \\subfigure[Data Percentage: 50\\%]{\n        \\label{fig:s_s}\n        \\includegraphics[width=0.3\\textwidth]{picture/50.pdf}\n    }\n    \\caption{Fine-tuning convergence comparison of GPT-GNN with no-pretrain, under different training data percentage.}\n    \\label{fig:cv2}\n\\end{figure*}",
            "fig:cv1": "\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.44\\textwidth]{picture/pretrain_curve.pdf}\n    \\caption{The pre-training convergence curve of GPT-GNN on OAG. It took about 10 hours (400 epochs) to converge.}\n    \\label{fig:cv1}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\nH^{(l)}_t \\gets \\underset{\\forall s \\in N(t), \\forall e \\in E(s,t)}{\\textbf{Aggregate}}\\bigg(  \\Big\\{ \\textbf{Extract}\\big(H^{(l-1)}_s; H^{(l-1)}_t, e\\big)\\Big\\} \\bigg),\n\\end{align}",
            "eq:eq:gpt": "\\begin{align}\\label{eq:gpt}\n    \\log p_{\\theta}(X, E) = \\sum_{i=1}^{|\\cV|} \\log p_{\\theta}(X_{i}, E_{i} \\mid X_{<i}, E_{<i}).\n\\end{align}",
            "eq:2": "\\begin{align}\n   &  p_{\\theta}(X_{i}, E_{i} \\mid X_{<i}, E_{<i}) \\nonumber\\\\\n= &\\sum\\nolimits_{o} p_{\\theta}(X_{i}, E_{i, \\neg o} \\mid E_{i, o}, X_{<i}, E_{<i}) \\cdot p_{\\theta}(E_{i, o} \\mid X_{<i}, E_{<i}) \\nonumber\\\\\n= &\\mathbb{E}_{o}\\Big[p_{\\theta}(X_{i}, E_{i, \\neg o} \\mid E_{i, o}, X_{<i}, E_{<i})\\Big] \\nonumber \\\\ \n=&\\mathbb{E}_{o}\\Big[\\underbrace{\n p_{\\theta}(X_{i} \\mid E_{i, o}, X_{<i}, E_{<i})\n }_\\text{1) generate attributes} \\cdot \n \\underbrace{\n p_{\\theta}(E_{i, \\neg o} \\mid E_{i, o}, X_{\\leq i}, E_{<i})\n }_\\text{2) generate edges}\\Big]. \\label{eq:overall}\n\\end{align}",
            "eq:3": "\\begin{align}\n\\mathcal{L}^{Attr}_i = Distance\\big({Dec}^{Attr}(h^{Attr}_i), X_{i}\\big). \\label{eq:gen}\n\\end{align}",
            "eq:4": "\\begin{align}\n    p_{\\theta}(E_{i, \\neg o} \\mid E_{i, o}, X_{\\leq i}, E_{<i}) = \\prod_{j^{+} \\in E_{i, \\neg o}} \\! p_{\\theta}(j^{+} \\mid E_{i, o}, X_{\\leq i}, E_{<i}). \\label{eq:obj}\n\\end{align}",
            "eq:5": "\\begin{align}\n\\label{eq:contrastive}\n\\mathcal{L}^{Edge}_i \\!= -\\!\\!\\sum_{j^{+} \\in E_{i, \\neg o}}\\log \\frac{\\exp\\big(Dec^{Edge}(h^{Edge}_i,\\  h^{Edge}_{j^{+}})\\big)}{\\sum_{j^{} \\in S^{-}_i \\cup \\{ j^{+}\\} }\\exp\\big(Dec^{Edge}(h^{Edge}_i,\\ h^{Edge}_{j^{}})\\big)}\n\\end{align}"
        },
        "git_link": "https://github.com/acbull/GPT-GNN"
    }
}