{
    "meta_info": {
        "title": "Semi-supervised Drifted Stream Learning with Short Lookback",
        "abstract": "In many scenarios, 1) data streams are generated in real time; 2) labeled\ndata are expensive and only limited labels are available in the beginning; 3)\nreal-world data is not always i.i.d. and data drift over time gradually; 4) the\nstorage of historical streams is limited and model updating can only be\nachieved based on a very short lookback window. This learning setting limits\nthe applicability and availability of many Machine Learning (ML) algorithms. We\ngeneralize the learning task under such setting as a semi-supervised drifted\nstream learning with short lookback problem (SDSL). SDSL imposes two\nunder-addressed challenges on existing methods in semi-supervised learning,\ncontinuous learning, and domain adaptation: 1) robust pseudo-labeling under\ngradual shifts and 2) anti-forgetting adaptation with short lookback. To tackle\nthese challenges, we propose a principled and generic generation-replay\nframework to solve SDSL. The framework is able to accomplish: 1) robust\npseudo-labeling in the generation step; 2) anti-forgetting adaption in the\nreplay step. To achieve robust pseudo-labeling, we develop a novel pseudo-label\nclassification model to leverage supervised knowledge of previously labeled\ndata, unsupervised knowledge of new data, and, structure knowledge of invariant\nlabel semantics. To achieve adaptive anti-forgetting model replay, we propose\nto view the anti-forgetting adaptation task as a flat region search problem. We\npropose a novel minimax game-based replay objective function to solve the flat\nregion search problem and develop an effective optimization solver. Finally, we\npresent extensive experiments to demonstrate our framework can effectively\naddress the task of anti-forgetting learning in drifted streams with short\nlookback.",
        "author": "Weijieying Ren, Pengyang Wang, Xiaolin Li, Charles E. Hughes, Yanjie Fu",
        "link": "http://arxiv.org/abs/2205.13066v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "To appear in KDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nConsidering a motivating application of in-App activity analysis. Many mobile Apps, such as Snapchat, generate unlabeled internet traffic streams in real time. In-App activities (e.g., share photos, videos, text, and drawings) could drift over time, resulting in distribution shifts. \nDue to mobile privacy concerns, many companies implement a very short data retention duration policy. \nWe only have access to the most recent data (e.g., a lookback window). \nTherefore, learning with short lookback in unlabeled drifted streams is critical for in-App activity classification. \nThis scenario can be generalized as a new learning problem: \\textit{\\textbf{S}emi-supervised \\textbf{D}rifted \\textbf{S}tream learning with short \\textbf{L}ookback (SDSL)}. which is depicted in Figure \\ref{fig:train_setting}.\nSDSL can enable a model to adaptively learn from an unlabeled stream of evolving distribution shifts, with limited initial labels and small lookback windows for training.\nSolving SDSL can address multiple critical issues to increase the availability and applicability of ML algorithms.\nFor example, in many scenarios, 1) data are generated in real time; 2) labeled data are expensive and only limited labels are available in the beginning of time; 3) real-world data is not always i.i.d. and data drift over time gradually; 4) the storage of historical streams is limited and model updating can only be achieved based on a very short lookback window. \n\n\nThere are two major challenges in solving SDSL: 1) robust pseudo-labeling under distribution shifts, 2) anti-forgetting adaptation with short lookback. \nFirstly, except for the initially given labels, all the incoming streams are unlabeled. \nThe evolving distribution shifts further introduce bias into the task of labeling new data: training a classifier on the previously labeled data, but utilizing the classifier to predict labels of forthcoming drifted data. \nRobust pseudo-labeling is to answer: how can we generate robust and quality pseudo-labels of unlabeled streams to augment old data?\nSecondly, SDSL can suffer from forgetting. This is because:\n1) due to stream storage limitations and privacy-driven short data retention policies, less old data are stored for retraining;\n2) when a model adapts to new drifted data, the model parameters change to fit new data and forget old knowledge.  \nA model that forgets old knowledge will perform poorly when new data with old distribution re-appear in the future stream. \nTherefore, anti-forgetting adaptation under short lookback is to answer: how can we learn new knowledge, while prevent forgetting old knowledge with limited historical data for replay?\n\nRelevant works can only partially solve SDSL.\nFirstly, SDSL is related to Semi-Supervised Learning (SSL) algorithms~\\cite{guo2020record,berthelot2019mixmatch}, which combines a small amount of labeled data with a large amount of unlabeled data during training. \nHowever, in classic SSL, 1) the labeled and unlabeled data are assumed to sample from an i.i.d distribution; 2) unlabeled data is static without evolving shift over streams~\\cite{berthelot2019mixmatch}. \nSecondly, SDSL is related to continual learning that learns knowledge of new data without forgetting learned knowledge of old data. \nHowever, classic continual learning assumes newly generated data in streams are all labeled~\\cite{pan2010domain,rolnick2019experience}.\nThirdly, SDSL is related to domain adaptation ~\\cite{tzeng2017adversarial,zhao2020semi} that aims to transfer knowledge learned from one or multiple labeled source domains to an unlabeled target domain.\nHowever, in classic domain adaption, both source domains and target domains are static~\\cite{pan2010domain,rolnick2019experience}. \nExisting studies demonstrate the inability to jointly address both robust pseudo-labeling and anti-forgetting adaptation with short lookback in SDSL. \nAs a result, it highly necessitates a novel perspective to derive the novel formulation and solver of SDSL. \n\n\\textbf{\\textbf{Our Contribution: an integrated robust and antiforgetting perspective.}}\nWe formulate a generic learning problem of SDSL for semi-supervised, stream, limited lookback memory, evolving distribution shift environments. \nWe show that semi-supervised learning in streams can be solved by iterating the label generation and the model replay process, where the label generation is to generate pseudo-labels for newly coming unlabeled data stream, and the model replay is to retrain learning model with old data of short lookback and pseudo-labeled new data. \nRobust pseudo-labels are important for effective SDSL. \nWe find that leveraging invariant structure knowledge in streaming data can fight against the bias introduced by evolving distribution drifts for the robust pseudo-label generation. \nWe demonstrate that to better learn patterns at the transition between old and new data, the model replay needs to be adaptive while anti-forgetting, even with a short lookback window.\nThis requirement can be reformulated into a task of automated identification of flat regions.\nWe highlight that the automated flat gradient region identification problem is indeed a minimax game.\nSolving the minimax game can effectively help the model to achieve both the anti-forgetting replay with limited old lookback data and adaption to new data.\n\n\\textbf{Summary of Proposed Approach}. Inspired by these findings, this paper presents the first attempt to develop a principled and generic generation-replay framework for the SDSL problem by iterating the robust pseudo-label generation and the adaptive anti-forgetting model replay.\nThe framework has two goals: 1) robust pseudo-label generation against evolving distribution shifts in the generation step; 2) balancing anti-forgetting replay and effective adaptation in the replay step. \n%\n%Specifically, to achieve the Goal 1, we argue that the unrobustness of existing pseudo label generation methods is caused by training a classifier on previous labeled data, but utilizing the classifier to predict labels of forthcoming drifted data.  The temporal drift leads to the inability of  the trained classifier to predict robust labels for unseen drifted data. \nEspecially, to achieve Goal 1, we develop a three-step robust label generation method to leverage multi-level knowledge.\nIn particular, we find that robustness of pseudo-labels can be improved by modeling supervised knowledge from previously labeled data, unsupervised knowledge from new unlabeled drifted data, and structure knowledge from invariant label class semantics. \nStep 1 develops a supervised neural encoder-based classifier trained on previously labeled data. Then, We adopt a center-based clustering method to adjust labels on new drifted data, and leverage the invariance of label class semantics to regularize the neural encoder-based classifier. \nTo achieve Goal 2, we develop an adaptive anti-forgetting model replay technique. \nIn particular, we reformulate the adaptive anti-forgetting model replay into a flat region search problem. We propose a novel minimax game-based replay objective to automatically find the flat region to  minimize the predictive loss on both previous data and new drifted data. \nAnd we develop an effective optimization method to solve the minimax game.\nFinally, we present extensive empirical results to demonstrate the effectiveness of our method for learning in the semi-supervised, streaming, gradually drifted, and short lookback setting.  \n\n\n\n\n\n\n\n\n\\begin{comment}\n% para1: deep learning <- large labeled data.\n% exists unlabeled data due to annotation labor\n% semi-supervised learning provide one way to solve\n\n% para 2: most of focus: offline and static\n% realistic example\n\n% para 3: two difficulty: \n% 1. stabilize on a local optimal, the weight may shift a lot when adapting on evolving data\n% 2. infeasible to infer accurate pseudol label due to distribution shift.\n\n% para 4: our technique \n\n% para 5: contribution\nDeep learning techniques had achieved remarkable success recent due to the \nthe availability of large amounts of labeled data \\cite{kornblith2019better,young2018recent,fawaz2019deep}.\n%in various domains, like image classification \\cite{kornblith2019better}, natural language translation \\cite{young2018recent} and time series forecasting \\cite{fawaz2019deep}, etc. \n%One of the significant factors attributing to the recent success of deep learning is  \\cite{krizhevsky2012imagenet}. \nHowever, creating large labeled datasets is often time-consuming and expensive in terms of domain expert annotation cost.\nSemi-supervised learning (SSL) algorithm is an effective tool to leverage unlabeled data. \nRecent researches focus on leveraging unlabeled data structure \\cite{berthelot2019mixmatch,xie2020self} and generate high-quality pseudo-labels \\cite{guo2020record}, and observe that SSL achieves comparable performance to standard supervised learning using significantly fewer labeled data instances \\cite{grill2020bootstrap,sohn2020fixmatch}. \n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=\\linewidth]{samples/figure/data_stream.JPG}\n  \\caption{Problem setting of semi-supervised learning via evolving distribution shift. Data in solid line box is available at current time.}\n  \\Description{Protopyre}\n\\end{figure}\n\nHowever, current SSL algorithms mainly consider an offline manner, i.e., 1) the labeled and unlabeled data are assumed to sample from the i.i.d distribution; 2) unlabeled data is static without a steaming shift.\nSuch constraint is not plausible in the realistic scenario. \nMany machine learning applications require consistent performance across different underlying data distributions.\nImagine a tumor image scanning agent with a disease recognition system trained on scenes from a\nstationary condition, i.e., patients biomedical image in the early condition.\nWhen deployed in real world, the tumor structure may vary in a continually evolving way. \nAnother restriction we are confronted with is the limited computational resources of the agent when deployed. Thus, we may learn representations in the factory, and deploy only light-weight model when adapting to new shifted data in the real world.\nTherefore, we hope the agent gradually adapts to the shift environment and make decisions well on scenes from all \\textcolor{red}{environments} with limited memory budget.\n\nIn this paper, we consider a more realistic setting of semi-supervised classification problem — adapting to evolving unlabeled data with restricted memory buffer.\nwe formulate this setting as: \n(1) we have access to a small number of labeled data \\textbf{at} the beginning of time; \n(2) Unlabeled data arrives sequentially with a shifted distribution over time \\textcolor{red}{and can only be stored at once};\n(3) \\textcolor{red}{after adapting to new coming data sequentially, we test the model on current test data.} \nGenerally speaking, we assume a closed label set and a gradual shift across time. \nThough extensive approaches have been proposed to deal with distribution shift, such as domain adaptation \\cite{ben2007analysis,wang2018deep,pan2010domain},\n%such as re-weighting methods \\cite{} and learning with an invariant feature embedding \\cite{}. \nthey mainly consider a static shift and are not directly applied to SSL on steaming data. \n\nRecently, \\cite{guo2020record} proposes a pioneer SSL work, named record, operated on shifting environment. \n\\textbf{At each round}, record detects a fixed set of influential samples to satisfy the resource requirement and further augment such pseudo-labeled data to improve adaptation ability. \nThis framework is compatible to arbitrary SSL techniques while skip three challenges confronting with shifted stream data in SSL: \n\\rmnum{1}) pseudo-label generation with shifted data. Record generates pseudo-labels via classifiers trained on previous data. \n\\textcolor{red}{The classifier is biased toward previous data distribution and ignore data lying far from previous distribution.}\n\\rmnum{2}) memory restriction. Record maintains a memory buffer to store learned data sequentially. The replay strategy still incurs a resource  workload due to accumulated data. \n\\rmnum{3}) forgetting problem. Since neural networks are typically trained with stationary training batches by stochastic gradient descent methods \\cite{} and stabilize on a local optimal. \nAs new shifted data are added, the trained parameters will deviate from the previous local optimal, \\textcolor{red}{leading to `catastrophic forgetting' problem}, i.e., an abrupt performance decrease on previous data \\cite{}. If we further release the memory resource restriction and store less previous data, The forgetting issue will be exacerbated.  \n%Several crucial questions remains to be addressed: Can we improve the pseudo labeling techniques concerning distribution shift? Can we further release memory buffer restriction as well as mitigate forgetting issues? \n%In this paper, we provide to answers mentioned above. To consistently generate high-quality pseudo labels, our key insight is \n%Besides, current research has demonstrated that the invariant feature representation may lose discriminative information for source domain so as to decrease its performance. \n%Another line of works, e.g., continual learning aims to solve the `catastrophic forgetting' problem in a supervised way. \n%A typical Regularization method prevent parameters with important knowledge from deviating too much on the new tasks, but calculating `importance\" relies the availability of supervision of incoming data. Moreover, it is not the optimal ....\n\n% goal is to learn adaptively and autonomously when the underlying input distribution drifts over extended time periods\n%(1) we have access to adequate labeled examples from the source domain, and\n%part of the target unlabeled data from a target domain evolving over time in the meta-training phase,\n%(2) new target data of the meta-testing phase arrive sequentially online from the same evolving target distribution and cannot be stored, and (3) after adapting to these new target data sequentially, we test the model on all target data of meta-testing.\n%Adapting to evolving shift unlabeled data poses a challenge to remember previous learned knowledge and makes it non-trivial to generate high-quality pseudo labels. \n%First,  \n%Second, Moreover, the accumulated errors will propagate in the network training in the evolving process.\n% \\textcolor{red}{We borrow the insight of adversarial perturbations from\n%[10, 1, 37] to simply describe the principle of the proposed method. }\n%In this paper, we investigate a novel problem of semi-supervised learning via evolving data shift. In essence, we face with two challenging problems: \\rmnum{1}) given the streaming shifted data, how to consistently generate high-quality pseudo labels? \\rmnum{2}) while generalizing well on incoming data, how to preserve previous knowledge? \nIn an attempt to solve the three challenges, we propose a new SSL framework named \\textcolor{red}{model name}.\nthat can adapt to continually evolving data without forgetting.\nIt consists of two steps, \\textcolor{red}{1) pseudo-label generation, 2) replay model optimization}. Our framework has no heuristic and is generalized to arbitrary SSL algorithms. \nTo better infer pseudo-labels over shifted data, we introduce a target-oriented classifier to mitigate labeling bias. We assume this classifier should follow two properties: 1)\nlearn a compact target data structure; 2) preserve the invariant label relations across streaming data.\nSpecifically, We resort to prototypes, i.e., the class-wise feature centroids.\nWe rectify the pseudo-labels by estimating the class-wise likelihoods according to its relative feature distances to all class prototypes.  \n%This depends on a practical assumption that the prototype lies closer to the true centroid of the underlying cluster, implying that false pseudo labels are in the minority. \nBesides, we force the label relations are invariant across the prototypes.  \n%To robustly distill the structural knowledge of the teacher\n%space, we define a new cross-space relation between two samples and supervise this new relation by its correspond- ing relation in the teacher representation space.\n%design an auxiliary classifier \\textcolor{red}{for the incoming data.} Specifically, \nTo mitigate forgetting problems, we modify the replay objective function by finding a trust flat region. The flat region can avoid model parameters deviating much from previous one. \n\nOur contribution are as follows:\n\\begin{itemize}\n\\item We study a novel problem of SSL via shifted streaming data.\n\\item We propose a novel framework which could leverage unlabeled data structure and the relations\nbetween classes to generate high-quality pseudo-labeled data.\nBesides, we modify the objective function by finding a trust flat region, which help model adapt well as well we alleviate forgetting.\n\\item Extensive empirical results demonstrate the effectiveness of our proposed SSL framework for adapting on shifted streaming data with forgetting. \n\\end{itemize}\n\n\n\n\n\n% In particular, we design two types of non-parametric classi- fiers, i.e., nearest centroid classifier (NC) and neighborhood aggregation (NA), to avoid additional network parameters. Both class centroids and local neighborhood structures are capable of representing the target domain, thus the generated target-oriented pseudo labels are fairly unbiased and reliable.\n\\end{comment}\n\n\n\n\n\n\n\\vspace{-0.3cm}\n"
            },
            "section 2": {
                "name": "Problem Statement",
                "content": "\n\\textbf{The SDSL Problem.} \nLet use $\\mathcal{D}$ to denote gold standard label data at $t$=0  and use $\\hat{\\mathcal{D}}$ to denote pseudo-labeled data at $t$>0.\nConsidering the existence of the initial labeled data that includes a feature matrix $\\mathbf{X}_0$ and a gold standard label vector $\\mathbf{y}_0$ in the very beginning (denoted by  $\\mathcal{D}=\\{\\mathbf{X}_0, \\mathbf{y}_0\\}$ ), and a drifted unlabeled data stream (denoted by a feature matrix list of stream segmentations $ [\\mathbf{X}_t]_{t=1}^{T}$). \nWe aim to train an adaptive model to classify all the data points of the unlabeled data stream into a fixed number of classes. \nAt the time $t$, the model generates a list of pseudo-labeled sets $[\\mathbf{y}_1,..., \\mathbf{y}_{t-1}]$ for the list of unlabeled stream segmentations $[\\mathbf{X}_1,..., \\mathbf{X}_{t-1}]$.\n\n\nDuring the training phase (Figure \\ref{fig:train}), we iteratively learn the adaptive model $h$ that takes only the initial gold standard labeled data $\\mathcal{D}$ and the short lookback of the ($t$-1)-th pseudo-labeled stream segmentation (denoted by $\\hat{\\mathcal{D}}_{t-1} = \\{X_{t-1}, \\hat{\\mathbf{y}}_{t-1}\\}$) as inputs, and predicts the pseudo-labels (denoted by $\\hat{\\mathbf{y}}_t$) of the $t$-th stream segmentation (denoted by $\\mathbf{X}_{t}$) at the time $t$.\nFormally, the approximation function $h$ with learning parameters $\\theta$ is given by:\n\\begin{equation}\n    h_{\\theta}(\\mathcal{D}, \\hat{\\mathcal{D}}_{t-1}, \\mathbf{X}_{t}) \\rightarrow \\mathbf{\\hat{\\mathbf{y}}}_t.\n\\end{equation}\nThe optimization objective is to learn the model that can \n1) generate robust pseudo-labels,\n%to \\textbf{augment unlabeled stream data} \nand 2) prevent forgetting old data while adapt well to new data. \n%in model retraining stage.\n\n\\begin{comment}\n\\begin{equation}\n\\begin{aligned}\n    \\max_{\\mathcal{D}_l^t}  &\\max_{\\mathcal{\\hat{D}}_u^t} &performance (f^t) \\\\\n    & s.t. &f^t = \\Phi^t (\\mathcal{D}_l \\cup \\mathcal{\\hat{D}}_u^{t}) \\\\\n    && \\mathcal{\\hat{D}}_u^t = \\mathbf{\\hat{D}}_u^{t-1} \\cup  \\mathbf{\\hat{D}}_u^{t}.\n\\end{aligned}  \n\\end{equation}\n\n\\textbf{To release the memory restriction, at each time step $t$, we are not allowed to access  pseudo-labeled pairs before $t-1$. Besides, we aim to learn a target-specific classifier to generate high-quality labels, which can leverage the unlabeled data structure and preserve label relation. After infer the pseudo labels, we aim to find a local flat region which can improve the generalization ability}. \n%The existence of evolving shift data makes existing SSL method performs poorly to generate high-quality pseudo labels. Besides, adapting to the shifted data makes it non-trivial to \\textcolor{red}{preserve previous knowledge.}\n%Given a set of training tasks {T1, T2, · · · , T|U| }, \n%strike a balance between the performance of adapting to new shifted data as well as preserve previous knowledge.\n\\end{comment}\n\n\\begin{comment}\n\\subsection{Preliminary on SSL}\nSSL consists of labeled examples and unlabeled examples. Generally speaking, SSL performs pseudo-label estimation and joint network learning under a unified loss minimization problem. The network $h = f \\circ g$ can be regarded as the composition of a feature extractor $f(x) \\in \\mathbb{R}^d$ and a classifier $g(f(x)) \\in \\mathbb{R}^K$, where $d$ means the feature dimension and $K$ means the class numbers. \nThe data is sequentially feeds into the feature extractor and classifer to obtain the probabilistic predictions as follows:\n    \\begin{equation}\n    \\begin{aligned}\n    p(\\mathbf{x}) = \\sigma(g(f(\\mathbf{x})))\n    \\end{aligned}\n    \\end{equation}\nwhere $\\sigma()$ is the softmax function with $\\sigma_k(\\mathbf{a}) = \\frac{exp(a_k)}{\\sum_i(exp(a_i))}$ denoting the $K$-th element in the soft-max output of a $K$ dimensional vector $\\mathbf{a}$. \nGiven unlabeled training examples $\\mathcal{D}_u = \\{x_{u_j}\\}_{j=1}^{N_u}$, one can generate pseudo\nlabels $\\mathbf{Y}_u$ based on the predictions of a supervised model on labeled data. \n\n\\textbf{a) Pseudo label generation.} Fix $\\theta$ and solve:\n    \\begin{equation}\n    \\begin{aligned}\n  \\min_{\\mathbf{\\hat{Y}}_u} -\\sum_{k=1}^{K} \\hat{\\mathbf{Y}}^c_u \\log p(c|\\mathbf{X}_u;\\theta)\n    \\end{aligned}\n    \\end{equation}\n\n\n\\textbf{b) Network retraining.} Fix $\\mathbf{Y}_l$ and $\\mathbf{\\hat{Y}}_u$  to solve:\n    \\begin{equation}\n    \\begin{aligned}\n  \\min_{\\theta} -\\sum_{k=1}^{K} \\mathbf{Y}_l \\log p(k|\\mathbf{X}_l;\\theta) -\\sum_{k=1}^{K} \\hat{\\mathbf{Y}}^k_u \\log p(k|\\mathcal{X}_u;\\theta)\n    \\end{aligned}\n    \\end{equation}\n\nwe call $\\hat{\\mathbf{D}}_u = \\{\\mathbf{X}_u,\\mathbf{\\hat{Y}}_u\\}$ as pseudo labeling data in our analysis.\n\\end{comment}\\vspace{-0.1cm}\n"
            },
            "section 3": {
                "name": "The Generation-Replay Framework",
                "content": "\n\n",
                "subsection 3.1": {
                    "name": "Overview",
                    "content": "\nFigure~\\ref{fig:overview} shows our proposed generation-replay framework including two components: \n1) pseudo label generation; \n2) adaptive anti-forgetting model replay. \nTo achieve robust pseudo-label generation, \nwe propose a three-step approach to improve the robustness of generated pseudo-labels by leveraging the knowledge from previously labeled data, new unlabeled data, and invariant label classes semantics, i.e., class correlations.\nIn particular, in Step 1, we first train an integrated encoder and a classifier by minimizing the predictive loss on previous gold-labeled data and pseudo-labeled data at $t$-1.  \nWe then use the trained classifier to predict and obtain the initial pseudo-labels of forthcoming unlabeled drifted data at $t$. \nSince the trained neural classifier learns previous knowledge that is partially overlapped with the knowledge of forthcoming drifted data, the labels of new drifted data that are non-overlapped with previous knowledge could be biased and inaccurate. \nIn Step 2, we then propose to use an unsupervised centroid-based clustering method to adjust the labels of new drifted data by repeatably assigning the new drifted data to the nearest class centroid and updating class centroids until converged. \nThe adjusted pseudo-labels of the forthcoming unlabeled drifted data are compared with model predicted labels to create loss signals as feedback to improve the neural integrated encoder-classifier method. \nBesides, we find that the semantic meanings of label classes remain invariant during the SDSL. \nWe propose to leverage the invariance property of label class embeddings to further refine the class centroids. Specifically, we optimize the label class centroid via fixing label latent embedding learned from initial gold-labeled data.  %optimizing the label class centroid matrix factorization task while \nAfter obtaining the refined class centroids, we reassign forthcoming drifted data to the nearest class cluster centroid to generate debiased and robust labels.  \nTo achieve adaptive anti-forgetting model replay, we aim to retrain the integrated encoder-classifier to prevent forgetting previous knowledge while adapting well to new drifted data. \nWe formulate this joint objective into a problem of searching the flat region.\nIt is challenging to not just search the flat region but also identify the width of the flat region. \nWe found that the challenge can be converted into a formulation of solving a minimax game. \nWe develop an effective optimization method to solve the minimax game to find the flat region and identify its width. \n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Pseudo Label",
                    "content": "\n\\textbf{Why Robust \\textbf{Pseudo Label} Generation Matters.} Conventional semi-supervised learning generates pseudo-labels based on the predictions of a supervised model on labeled data (i.e., pseudo-label generation stage) and then integrates the pseudo-labeled new data to retrain an updated model (i.e., replay stage). In this way, the network gradually generalizes to unlabeled data in a self-paced curriculum learning manner \\cite{lee2013pseudo}. \nHowever, such a strategy is not applicable to the SDSL setting because the unlabeled stream data drift over time. \nBe sure to notice that, the supervised model is trained based on previously labeled data. \nWhen the supervised model predicts on forthcoming drifted unlabeled data, the generated pseudo-labels are likely to be biased and inaccurate, which will propagate errors to the replay stage.\n\n\\noindent\\textbf{Leveraging Multi-level Knowledge to Robustify Pseudo-Label Generation.} \nWe find that label generation of unlabeled drifted data under SDSL can be robustized by integrating supervised knowledge from previously labeled data, unsupervised knowledge from new unlabeled drifted data, and structure knowledge from invariant label class semantic meanings and relationships.\nBased on our unique insight, we propose a step-by-step testable method that includes three steps. Figure \\ref{fig:label_relation} illustrates the framework of the robust pseudo-label generation process.\n\n\\noindent\\textbf{ Step 1: Leveraging Supervised Knowledge}\nIn a stream, data distributions drift gradually. In other words, the distribution of forthcoming unlabeled drifted data partially overlaps with the distribution of previous data.  \nAs shown in Figure \\ref{fig:label_relation}, historical gold-labeled data and pseudo-labeled data still contain useful knowledge that can be used to predict the overlapped part of forthcoming drifted unlabeled data.\n\n\n\nTo this end, we develop an encoder-based neural classification model. This model jointly includes a multi-layer neural encoder $f$ and a neural classifier $g$ as an approximation function of the  classification model: $h = g(f(\\cdot))$. \nThe neural encoder takes a data point as input and outputs the embedding of the data point. The neural classifier takes the embedding of the data point as features and outputs the predicted labels. \nAside from the approximation function, we use the \\textbf{C}ross-\\textbf{E}ntropy (CE) loss $\\ell(\\cdot, \\cdot)$ to measure classification errors. Formally, the optimization objective function can be formulated as:\n\\begin{equation}\n\\label{classifier}\n  \\min_{\\theta}\\mathcal{L}_{CE}(\\mathcal{D}, \\hat{\\mathcal{D}}_{t-1};\\theta) = \\sum_{(\\mathbf{x}_i, y_i) \\in \\{\\mathcal{D} \\cup \\mathbf{\\hat{\\mathcal{D}}}_{t-1}\\}} \\ell(h_\\theta(\\mathbf{x}_i),y_i).\n\\end{equation}\n%Finally, we minimize the cross-entropy loss to learn the parameters of the encoder-based neural classifier. \nIn this way, we leverage the gold labels and previous pseudo-labels of historical data as supervision signals to learn the encoder-based neural classifier to further generate initial pseudo-labels for current unlabeled data $\\mathbf{X}_t$, which will be introduced next.\n\n\\noindent\\textbf{ Step 2: Leveraging Unsupervised Knowledge.}\nSince the trained encoder-based neural classifier learns previous knowledge that is partially overlapped with the knowledge of forthcoming drifted data, the labels of new drifted data that are non-overlapped with previous data distributions could be biased and inaccurate. How can we improve the label quality of new drifted data whose patterns are not seen in the knowledge of previous data?\n\nWe find that unsupervised knowledge in new drifted unlabeled data is helpful for improving and refining the labels of such data themselves. \nCentroid-based clustering is an unsupervised learning method to exploit unsupervised information to discover data grouping patterns.\nDifferent from using the centroid-based clustering for data grouping, we propose to use such a method for data label adjustment. \nThe high-level idea is to exploit the centroid-based clustering to adjust the labels of new drifted data by repeatably assigning the new drifted data to the nearest class centroid and updating class centroids until converged. \nThe underlying insight is that label class centroid-based clustering reassigns labels based on the global pattern structure of new unlabeled drifted data. \n%\\textbf{Such label adjustment is less sensitive to biased or wrong labels generated by Step 1. }\n\nSpecifically, in Step 2, we firstly use the trained encoder-based neural classifier to classify the labels of new drifted unlabeled data. We then use these classified labels to compute the centroids of all the label classes. \nIn the initialization of the centroid-based clustering, we exploit all the class centroids as the initial cluster centroids. Formally, \nthe centroid embedding in class $c$ are initialized as via:\n\\begin{equation}\n\\label{eq:initialization}\n\\mathbf{u}_c^{(0)} = \\frac{\\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t} \\delta (h(\\mathbf{x}_i))f(\\mathbf{x}_i)}{\\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t}\\delta (h(\\mathbf{x}_i))},\n\\end{equation}\nwhere $\\delta(\\cdot)$ is the softmax function \\cite{liu2016large}.\n%with $\\delta(\\mathbf{a}_k) = \\frac{exp(a_k)}{\\sum_i^K(exp(a_i))}$ denoting the $k$-th element in the softmax output of a $K$ dimensional vector $\\mathbf{a}$.  \nWe then repeat two tasks:  assigning data points to the nearest class and updating class centroids, until converged (e.g., maximum number of iterations). \nParticularly, in the data reassignment task, given the class centroid $\\mathbf{u}_k$, we construct the nearest centroid classifier to assign each unlabeled data point $x_i \\in \\mathbf{X}_t$ to a class cluster as:\n\\begin{equation}\n\\label{eq:label}\n    \\hat{y}_i = \\arg \\min_{c \\in C} d(f(\\mathbf{x}_i), \\mathbf{u}_{c}),\n\\end{equation}\nwhere $d(\\cdot, \\cdot)$ measures the cosine distance between data $\\mathbf{x}_i$ and the class centroid $\\mathbf{u}_c$. And $C$ denotes class numbers.\n%In the centroid updating task, \nGiven the new pseudo-labels, we update the class centroids at each iteration $k$ by:\n\\begin{equation}\n\\label{eq:centroid}\n    \\mathbf{u}_c^{(k)} = \\frac{\\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t}f(\\mathbf{x}_i) * \\mathbbm{1} (y_i ==1)}{\\sum_{\\mathbf{x}_t \\in \\mathbf{X}_i} \\mathbbm{1} (y_{i} ==c)},\n\\end{equation}\nwhere $\\mathbbm{1}(\\cdot)$ is the indicator function. \nThe centroid-based clustering can reduce the error caused by supervised prediction under drifted data, and generate adjusted pseudo-labels. \n\nFinally, the adjusted pseudo-labels of the forthcoming unlabeled drifted data are compared with model predictions to create loss signals as feedback to improve the training of the encoder-based neural classifier. \nThe objective function can be formulated as:\n\\begin{equation}\n\\label{classifier}\n  \\min_{\\theta}\\mathcal{L}_{PL}(\\mathbf{X_t;\\theta})= \\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t} \\ell(h_\\theta(x_i),\n  \\hat{y}_i).\n\\end{equation}\n\\noindent\\textbf{ Step 3: Leveraging Structure Knowledge of Invariant Label semantics}\n%Label Class Embedding Invariance.}\nIntuitively, the semantic meanings of label classes remain invariant during the SDSL. \nWe show that leveraging the invariance property of label class semantics can further refine the quality of the generated pseudo-labels of new drifted data.  \nOur insight is based on the invariance property of label classes. The embeddings of label classes should remain invariant over timelines.\n%from $t$=0 to $T$.\n%To exploit such regularization to improve the quality of pseudo-labels, we propose to use the matrix factorization method \\cite{} of label class centroid matrix to \n%bridge the gap between label reassignment and label class embedding. \n\nThe underlying idea is that label quality can be improved by reassigning data points based on improved label class centroids.\nTo improve the accuracy of label class centroids, we can treat label class centroids as a  matrix and exploit factorization-based matrix reconstruction to reconstruct improved label class centroids. \n%Be sure to notice that, \nNotablely, factorizing the class centroid matrix can factorized into the embeddings of features and the embeddings of label classes, which links to the invariance regularization of label class embedding.\n\nSpecifically, in Step 3, we first obtain the gold standard label class embedding $\\hat{\\mathbf{V}}$ by factorizing the class centroid matrix $\\mathbf{U}$ of the gold standard labeled data ($\\mathcal{D}$) at $t$=0. We then perform factorization-based class centroid matrix reconstruction, by fixing the embedding of label classes $\\hat{\\mathbf{V}}$ to that learned from initial gold standard label data at $t$=0. \nFormally, the regularization term is defined as:\n\\begin{equation}\n\\mathcal{R}(\\mathbf{U}_t) = \\min_{\\mathbf{U}_t,\\mathbf{H}_t} \n\\|\\mathbf{U}_{t} - \\mathbf{H}_{t}^{T} {\\hat{\\mathbf{V}}}\\|_2.\\\\\n \\label{Eq:semantics}\n\\end{equation}\nWith the given label class embedding $\\hat{\\mathbf{V}}$, we update class centroids $\\mathbf{U}_t$ and feature latent embeddings $\\mathbf{H}_{t}$ iteratively.\nBy solving the optimization problem, we obtain the refined class centroids. Finally, we reassign forthcoming drifted data to the nearest class cluster centroid to generate debiased and robust pseudo-labels. \n\n\\textbf{Final loss function} The final objective function of robust pseudo-label generation can be represented as :\n\\begin{equation}\n\\label{classifier}\n\\mathcal{L}_{total} =\\mathcal{L}_{CE}(\\mathcal{D}, \\mathcal{\\hat{D}}_{t-1};\\theta) + \\mathcal{L}_{PL}(\\mathbf{X}_t;\\theta) + \\mathcal{R}(\\mathbf{U}_t),\n\\end{equation}\nwhere $\\mathcal{L}_{CE}(\\mathcal{D}, \\mathcal{\\hat{D}}_{t-1};\\theta)$ represents cross-entropy loss on gold-labeled data $\\mathcal{D}$ and pseudo-labeled data $\\mathcal{\\hat{D}}_{t-1}$. Besides, $\\mathcal{R}(\\mathbf{U}_t)$ stabilize the updating of centroids which contains invariant label class semantics. %regularizes the centroids updating.\nA detailed optimization process can be referred to Algorithm 1 in Appendix A.1.\n\n\\iffalse\nthat data cluster corresponding to the same class across time should be geometrical similar \\cite{}.\n%\\cite{} also theoretically prove that testing error on the target domain is bound by the \\cite{hypothesis distance} between source and target classifiers. \n%To eliminate hypothesis difference, existing works force a class-wise alignment, i.e., $p^{t-1}(x|y) = p^{t}(x|y)$ to make an accurate and stable adaptation \\cite{}.\nExisting works mainly force a class-wise feature alignment \\cite{} to supervised the structure learning on unlabeled data \\cite{}.\nUnfortunately, recent works observe that this implementation is somewhat strict and may deteriorate data structure in realistic dataset.\n%data structure across domains are not geometrically similar in realistic dataset \\cite{}.\n%Such phenomenon could be even worse confronting continuous shift scenario. \n%\\textcolor{red}{There is a big challenge on revealing data structure as well as eliminate hypothesis difference.} \nTo move it further, we propose a new regularization with the goal of preserving label relation. \nThe insight is, cluster centroids, i.e., prototypes in Eq.\\ref{eq:centroid}, could evolve across streaming data while class relations should be invariant. The learning of evolving prototypes should follow the class-relation closeness property. \nThe prototype serves as classifier paprameters in Eq.\\ref{eq:centroid}, motivating the learning of stable classifier across domains.\n\nTo do so, we factorize the class parameter $U$ into two latent factors: latent feature embedding $H \\in \\mathcal{R}^{d \\times r}$and latent label embedding $V \\in \\mathcal{R}^{r \\times K}$, where $r$ is the rank of $U$ and $K$ is the class numbers.\nWe make a low-rank assumption here that the latent label embedding could preserve label relations which has theoretically support \\cite{} and empirical support, i.e., multi-label classification.\nObviously, different NC classifier $F_t$ is evolving with the shifted feature space while being consistent with the shared label space. \n\n\nthe class centroid via optimizing the label class centroid matrix factorization task while fixing the embedding of label classes learned from initial golden standard labeled data. \nAfter obtaining the refined class centroids, we reassign forthcoming drifted data to the nearest class cluster centroid generate debiased and robust labels.  \n\n\n\n\nThe insight can be summarized into two notions:\n\\begin{itemize}\n    \\item Shifted data structure discrimination assumes there exists intrinsic structures for each unlabeled data $\\mathbf{X}_t$, i.e., data distribution in each time slot $[\\mathbf{X}_t]$ are discriminative clustered whilst sharing the same label space. \n    \\item Class-wise relation closeness assumes class relations should be invariant across shifted data distribution.   \n\\end{itemize}\nTo implement the two ideas, we introduce an auxiliary nearest centroid classifier $F_t$ to generate pseudo-labels for each $\\mathbf{X}_t$. Meanwhile, we restrict the evolving prototypes for each $\\mathbf{X}_t$ should follow the invariant class relations across time. \n%To deal with this challenge, we mining the unlabeled data structure to generate high-quality pseudo labels. \n%an intuitive idea is to  iteratively generate a set of pseudo labels based on the most confident predictions on the unlabeled data \\cite{}.\n%, and relying on the pseudo labels to retrain the model. \n%However, the performance still lags behind\n%\\textcolor{red}{(supervised and semi-supervised domain adaptation where a few labeled data is given in the target domain.)}\n%The network is inevitably biased toward the labeled source data during training, giving low-quality pseudo labels and propagating errors \\textcolor{red}{in the target domain.}\n\n%To alleviate such drawback, typical attempts on domain adaptation \\cite{} suggests a learning of invariant representation, then the learned classifier can be applied to target domain directly. However, the invariant representation is often lack of discriminative information for both source and target task, especifically for the evolving setting. To eliminate such research gap, we introduce an auxiliary classifier $F_t$ for the current task at each round $t$, with the goal to exploit target data structure and preserve label relations. \n\\begin{comment}\n\\begin{figure}[h]\n  \\centroiding\n  \\includegraphics[width=\\linewidth]{samples/figure/prototype.JPG}\n  \\label{sketch}\n  \\caption{Comparison between the classical SSL method and our method. (a) Classical pseudo labeling method easily generates false pseudo labels for shifted unlabeled data, specifically for data lying far away from the previous distribution (b) prototypes which mining unlabeled data structure, can underutilize false pseudo labels and are robust to outliers.  Besides, it is obvious that class relations are invariant across data distribution.  }\n  \\Description{Protopyre}\n\\end{figure}\n\\end{comment}\n",
                    "subsubsection 3.2.1": {
                        "name": "Structure learning via Nearest Centroid (NC) classifier",
                        "content": "\n%The condensed nearest neighbor rule is the most simple yet practical method in machine learning \\cite{}.\n%The goal here is to dynamic update a set of prototype $\\mathbf{U} = \\{u_1,u_2,...,u_K\\}$ in such a way that if $x_i$ is the nearest neighbor of $u$, then $l(u) = y_i$, where $l(u)$ is the label of $u$. Motivated by prior works \\cite{} which utilize class centroids to bridge the domain shift problem  for DA problems, we describe the data structure of the \\textcolor{red}{target domain with its class centroids.}\n%\\textbf{Prototype Computation}\nActually, it is non-trivial to train the auxiliary classifier without any supervision on unlabeled data $\\mathbf{X}_t$. Motivated by previous work that there exists some overlap between gradual shifted data \\cite{}, which can\nbe used to help build the classifier proposed here and teach\nthe remaining samples sequentially.\nAt first, prototypes are initialized according to the previously trained model via:\n\\begin{equation}\n\\label{eq:initialization}\nu_k^{(0)} = \\frac{\\sum_{x_i \\in \\mathbf{X}_t} \\delta (h(x_i))f(x_i)}{\\sum_{x_i \\in \\mathbf{X}_t}\\delta (h(x_i))}\n\\end{equation}\nwhere $\\delta()$ is the softmax function with $\\delta_k(\\mathbf{a}) = \\frac{exp(a_k)}{\\sum_i(exp(a_i))}$ denoting the $K$-th element in the soft-max output of a $K$ dimensional vector $\\mathbf{a}$. \nWith the class prototypes $u_k$, we readily construct the Nearest centroid classifier and generate the pseudo labels for each unlabeled data $x_t$ as below:\n\\begin{equation}\n\\label{eq:label}\n    \\hat{y}_i = \\arg \\min_{k} d(f(x_i), u_{k}),\n\\end{equation}\nwhere $d(\\cdot, \\cdot)$ measures the distance between features and prototypes, here we adopt the cosine distance as default. Given the new pseudo labels, we further update the cluster centroids as:\n\\begin{equation}\n\\label{eq:centroid}\n    u_k^{(1)} = \\frac{\\sum_{x_i \\in \\mathbf{X}_t}f(x_i) * \\mathbbm{1} (y_i ==1)}{\\sum_{x_t \\in \\mathbf{X}_i} \\mathbbm{1} (y_{i} ==k)}.\n\\end{equation}\n%an unstable optimization process as the representative prototypes depend on stochastic sampling of the class distributions. Therefore, we design the prototypes to evolve continually with a high momentum based update for each observed batch, aiming to stabilize the impetuous changes in\n%However, such prototype calculation is computational-intensive during training. To address this, we estimate the prototypes as the moving average of the cluster centroids in mini-batches, so that we can track the prototypes that moves slowly:\n\\begin{comment}\n\\begin{equation}\n    u_k  = \\beta u_k + (1-\\beta) u_k^{'}\n\\end{equation}\n\\end{comment}\nwhere $\\mathbbm{1}$ is the indicator function. \nIn practice, we need update labels Eq.\\ref{eq:label} and centroids Eq.\\ref{eq:centroid} for multiple rounds. In our experimentsss, we found that a few iteration can guarantee a good pseudo label set. \n%And $\\beta$ is the smoothing parameter fixed to 0.3 as default in this paper.\n\n% how to align classifier\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Label-relation aware constraints",
                        "content": "\nThe adaptive NC classifier $F_t$ we build so far leverage current data structure and also could reflect the evolving nature of prototype since $F_t$ is changed across time slot. \nIn addition to incorporating the data structure into the construction of NC classifier, class relation should also be considered.\nIntuitively, data cluster corresponding to the same class across time should be geometrical similar \\cite{}.\n%\\cite{} also theoretically prove that testing error on the target domain is bound by the \\cite{hypothesis distance} between source and target classifiers. \n%To eliminate hypothesis difference, existing works force a class-wise alignment, i.e., $p^{t-1}(x|y) = p^{t}(x|y)$ to make an accurate and stable adaptation \\cite{}.\nExisting works mainly force a class-wise feature alignment \\cite{} to supervised the structure learning on unlabeled data \\cite{}.\nUnfortunately, recent works observe that this implementation is somewhat strict and may deteriorate data structure in realistic dataset.\n%data structure across domains are not geometrically similar in realistic dataset \\cite{}.\n%Such phenomenon could be even worse confronting continuous shift scenario. \n%\\textcolor{red}{There is a big challenge on revealing data structure as well as eliminate hypothesis difference.} \nTo move it further, we propose a new regularization with the goal of preserving label relation. \nThe insight is, cluster centroids, i.e., prototypes in Eq.\\ref{eq:centroid}, could evolve across streaming data while class relations should be invariant. The learning of evolving prototypes should follow the class-relation closeness property. \nThe prototype serves as classifier paprameters in Eq.\\ref{eq:centroid}, motivating the learning of stable classifier across domains.\n\nTo do so, we factorize the class parameter $U$ into two latent factors: latent feature embedding $H \\in \\mathcal{R}^{d \\times r}$and latent label embedding $V \\in \\mathcal{R}^{r \\times K}$, where $r$ is the rank of $U$ and $K$ is the class numbers.\nWe make a low-rank assumption here that the latent label embedding could preserve label relations which has theoretically support \\cite{} and empirical support, i.e., multi-label classification.\nObviously, different NC classifier $F_t$ is evolving with the shifted feature space while being consistent with the shared label space. \n\nIt is non-trivial to obtain the embedding of $\\hat{V}$ since we only have gold labeled data in the beginning stage. To simplify the implementation, we factorize the train classifier parameter for golden data $\\mathbf{D}$ with SVD decomposition and regard the right eigen vector as golden label factor $\\hat{V}$. For the streaming data $[X_t]$,   \nWe optimize the NC classifier parameters (prototypes) with $L_2$ loss as follows:\n\\begin{equation}\n\\label{eq:relation}\n   \\min_{U_t,H_t} ||U^t - (H^t)^T \\dot V ||_2,\n\\end{equation}\nSpecifically, we iteratively optimize $\\mathbf{U}_t$ and $\\mathbf{H}_t$. A detailed optimization process can be refer to Alg.?.\n%How to distill the label relation into the forming of class centroid, i.e., classifier papramter in Eq.\\ref{classifer}\n%Motivated by such finding, recent works force the class-wise alignment across domains in the feature level \\cite{}, distribution level \\cite{} and gradient level \\cite{}, etc. \n\n\\textbf{Pseudo labeling loss}\nWith the generated pseudo labels in Eq.\\ref{eq:label}, we are able to incorporate the pseudo labeling loss  \n Finally, a standard cross-entropy loss is developed as follows:\n\\begin{equation}\n\\label{classifier}\n    \\mathcal{L}_{PL} = \\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t} \\ell_\\theta(h(x),\\hat{y}_i).\n\\end{equation}\n\\fi\n\n"
                    }
                },
                "subsection 3.3": {
                    "name": "Adaptive Anti-forgetting Model Replay",
                    "content": "\nAfter generating robust pseudo-labels for the newly coming unlabeled drifted data at time $t$, we treat such pseudo-labeled data as part of training data, and retrain the encoder-based neural classifier using the gold label data $\\mathcal{D}$ and the newly generated pseudo-labeled data $\\hat{\\mathcal{D}_t}$ at time $t$. To simplify our description, we ignore $\\mathcal{D}$ in the following sections, since $\\mathcal{D}$ is available across timelines.\n\n\\noindent\\textbf{Why Anti-forgetting Adaptation Matters?} \n\nThe key challenge of retraining the model is that, due to the privacy concerned short data retention policies in mobile and social applications and limited memory storage capacity of big stream data, a continuous learning model adapts to new data, new patterns, and new knowledge, while forgetting old data and knowledge at the same time.\nFigure \\ref{fig:flat_region} (a) shows after adapting to new data, the new model ($\\theta_2$) shifts to the right side of the old model ($\\theta_1$). There are two key observations of adapting to drifted data: \n1) the overlapped area between the new model and the old model becomes smaller and smaller.\n2) the red inflection point of loss minimal in the new model results in a higher loss in the old model. \nBoth observations indicate the new adapted model loses previous knowledge. \nHow can we strive for a balance between anti-forgetting and adaptation?\nMany studies develop various technical solutions \\cite{liu2020learning} to impose a regularization: the parameters of the new model should not deviate too much from previous parameters, i.e.,\n$\\displaystyle min_{\\theta_2} \\|\\theta_1 -\\theta_2\\|_2$.\nSuch a regularization term, however, can still cause a significant performance drop on previous data. Figure \\ref{fig:flat_region} (b) shows the new model parameters are forced to be similar to the old model parameters, so that the overlapped area of the two models grows larger. \nHowever, the red-color loss minimal point of the new model still obtains a high loss in the old model, i.e., $\\mathcal{L}(\\theta_2) \\gg \\mathcal{L}(\\theta_1)$. \n\n\\noindent\\textbf{Anti-forgetting Adaptation as A Minimax Game.} Inspired by \\cite{schulman2015trust}, we leverage the concept of the flat region to strive a balance between anti-forgetting and adaptation in semi-supervised stream learning. \nFigure \\ref{fig:flat_region} (c) shows why the flat region (denoted by $\\theta^{\\star} - a \\leqslant \\theta \\leqslant \\theta^{\\star} + a$, where $a$ is the width of the flat region) concept works. \nFundamentally, the flat region represents a set of optimal or near optimal candidate model instantiations in the model space on old data. \nFinding such a flat region can effectively increase the overlapped area of the new model and the old model and maintain a low loss on previous data, while at the same time allowing the new model to adapt and shift to new data. \nThis flat region searching and optimization relaxation process to strive for a balance between anti-forgetting and adaptation can be described by finding a model parameter ($\\theta$) that satisfies: \n\\vspace{-0.2cm}\n\\begin{equation}\n\\begin{aligned}\n\\label{minmax}\n \\min_{\\theta} \\quad &   \\sum_{\\mathbf{x}_i \\in \\textbf{D}_t} \\mathcal{L}(\\mathbf{x}_i;\\theta) \\\\\n    s.t. \\quad &\\theta^{\\star} - a \\leqslant \\theta \\leqslant \\theta^{\\star} + a.\\\\\n\\end{aligned}\n\\end{equation}\nwhere the width $a$ of the flat region is manually specified based on empirical and domain experiences \\cite{schulman2015trust,shi2021overcoming}. \n\nHowever, in a dynamic learning environment of SDSL, the best flat region width will dynamically vary when both old data and new data change at different times. \nTherefore, it is impractical to directly integrate the above formulation of the flat region. \nThe key research question is: can we find the flat region while automatically identify the best width of the flat region?\nWe find that searching the flat region while identifying the best width can be reformulated into a computationally tangible minimax game. \nAssuming the parameters of a model can vary in a certain flat region defined by $\\theta^{\\star}-\\xi \\leqslant \\theta  \\leqslant \\theta^{\\star}  + \\xi $.\nWe identify the upper bound of the model loss in the region of $\\xi$ and find the worst case by maximizing the training loss of the network on the new data $\\mathcal{\\hat{D}}_t$, which is described by $max_{\\xi} \\sum_{\\mathbf{x}_i \\in \\mathcal{D}_t } \\mathcal{L}_{\\mathcal{D}_{t}} (\\mathbf{x}_i | \\theta + \\xi )$.\nAfter the loss upper bound (worst case) over the region is measured, we minimize and lower the loss upper bound over the region to \nfind the feasible parameters that can minimize the current loss.\nBesides, as depicted in Figure \\ref{fig:flat_region}.(c), $\\xi$ lies in the neighborhoods around parameters train on previous data $\\mathcal{D}_{t-1}$, the optimization of $\\xi$ should follow $\\xi \\in \\mathcal{M}$ where $\\mathcal{M}$ represents the space span by previous parameters trained on $\\mathcal{D}_{t-1}$. Formally, the objective function can be formulated as:\n\\begin{equation}\n\\begin{aligned}\n\\label{eq:minmax}\n \\min_{\\theta} \\max_{\\xi}\\quad &  \\sum_{\\mathbf{x}_i \\in \\mathcal{D}_t } \\mathcal{L}_{\\mathcal{D}_{t}} (\\mathbf{x}_i ; \\theta + \\xi )  \\\\\n    s.t. \\quad & \\xi \\in \\mathcal{M}.\\\\\n\\end{aligned}\n\\end{equation}\n\n\\noindent\\textbf{Solving the Optimization Problem.} Based on the gradient projection method \\cite{rosen1960gradient}, the adversarial weight perturbation $\\xi$ can be updated with the projection on space $M$ via the step size $\\eta_{1}$,\n\\begin{equation}\n    \\xi \\leftarrow  \\xi + \\eta_{1}\n    \\vectorproj[M](\\nabla_{(\\theta_t)} \\mathcal{L}_{\\mathcal{D}_t}(\\theta_t +\\xi)).\n\\label{eq:flat}\n\\end{equation}\nNotably, $\\theta$ at time $t-1$ preserves the previously acquired knowledge which is spanned by $M$. When coming to $\\mathcal{D}_t$, We only update $\\theta$ along the orthogonal direction of $M$, leading to the least change (or locally no change) to the learned $\\theta$. Especially, the parameter $\\theta$ can be adaptively updated with each $\\mathcal{D}_t$ as:\n\\begin{equation}\n    \\theta \\leftarrow  \\theta - \\eta_{2}(I-\\vectorproj[\\mathbf{M}])\n     (\\nabla_{(\\theta_t)} L_{\\mathcal{D}_t}(\\theta_t +\\xi)).\n     \\label{eq:theta}\n\\end{equation}\nwhere $I$ is the identity matrix and $\\eta_2$ is the step size. Concretely, We present the Algorithm overview of the model replay stage in Appendix A.2. Besides, we show a theoretical analysis on why the flat region can help mitigate forgetting when adapting on shifted streaming data with short lookback and why our method works in Appendix A.3.\n\n\\iffalse\n%By Xiaoying\nAfter finishing the optimization in time $t$ and obtaining accurate pseudo labels $\\mathbf{\\hat{D}}_t$. We omit the $\\mathbf{D}$ in the following description since $\\mathbf{D}$ is available across time. The model need to retrain on the golden label set $\\mathbf{D}$ and pseudo label set $\\mathbf{D}_t$ first when accomodating to $\\mathbf{X}_{t+1}$. \nThe inaccessibility of previous data before $\\mathbf{X}_t$ \nmakes the model easily forget previous knowledge.\nAs we illustrated in Figure 2.(a), the current network adjusts the learned parameters from $\\theta_1$ to $\\theta_2$ and suffer from catastrophic forgetting. \nA direct solution is to enforce current parameter $\\theta_2$ approximating to previous parameters $\\theta_1$, i.e., $ min ||\\theta_1 -\\theta_2||_2$.\nHowever, the strong constraints may still lead to a significant performance drop on previous data, i.e., $\\mathcal{L}(\\theta_2) \\gg  \\mathcal{L}(\\theta_1)$ in Figure 2.(b). \n\nInspired by \\cite{schulman2015trust}, we introduce a novel notion of ‘trust flat region’ that search for a flat region $a$ of the base training objective.\nAs we depicted in Figure \\ref{fig:flat_region} (c), for any parameter vector $\\theta$ in the flat region, i.e., $\\theta^{\\star} - a \\leqslant \\theta \\leqslant \\theta^{\\star} + a$, the loss on previous trained data $\\mathbf{D}_{t-1}$ is minimized. When adapting on new data $\\mathbf{D}_t$, we fine-tune the model parameter within the region to learn the new shifted data:\n\\begin{equation}\n\\begin{aligned}\n\\label{minmax}\n \\min_{\\theta} \\quad &   \\sum_{x_i \\in \\textbf{D}_t} \\mathcal{L}(x_i;\\theta) \\\\\n    s.t. \\quad &\\theta^{\\star} - a \\leqslant \\theta \\leqslant \\theta^{\\star} + a\\\\\n\\end{aligned}\n\\end{equation}\n\nIt is obvious the incorporation of flat region $a$ makes the model is not sensitive the changeable parameter within flat region $a$ so as to alleviate forgetting. \n%Here, We borrow the insight of adversarial perturbations from \\cite{} to \n%Inspired by \\cite{}, \n%Different from \\cite{wu2020adversarial} with the goal to improve model generalization for adversarial examples, \nHowever, it is non-trivial to identify the width of the flat region without any prior. To solve such difficulty, we release the restriction on $a$ to a variable $\\xi$ and reformulate this problem as a min-max problem in a heuristic way.\nTo seek for a possible width of flat\nregion, we can first find the worst case by maximizing the training loss of the network on the $\\mathbf{D}_t$,\ni.e., $\\max_\\xi \\mathcal{L}_{\\mathbf{D}_t}(\\theta + \\xi)$. Intuitively, this objective function can be rewritten as $[\\max_\\xi \\mathcal{L}_{\\mathbf{D}_t}(\\theta + \\xi) - \\mathcal{L}_{\\mathbf{D}_{t-1}}(\\theta) ] +\\mathcal{L}_{\\mathbf{D}_{t-1}}(\\theta)$. \nThe first term evaluates how quickly the training loss can be increased by moving from $\\theta$ to a nearby parameter value $\\theta + \\xi$ when adapting on $\\mathbf{D}_t$. The flat region guarantees the model adapt well on new data $\\mathbf{D}_t$.\nAnd then minimize this loss function to guarantee a low loss on current data. Besides, since $\\xi$ lie in the neighborhoods around parameters train on previous data $\\mathbf{D}_{t-1}$, the optimization of $\\xi$ should follow $\\xi \\in M$ where $M$ represents the space span by previous parameter trained on $\\mathbf{D}_{t-1}$. Formally, the optimization function can be represented as:\n%our idea seeks out parameter values whose entire neighborhoods (flat region) have low training loss value. Thus, a change $\\xi$ to optimal parameter $\\theta_t$ may not decrease the training loss on $D_{t+1}$. Any parameter vector in the flat region $\\xi$ around the minima $\\theta_t$, can guarantee a small loss on the new task $\\mathcal{D}_{t+1}$.\n%Functionally speaking, we formalize the objective function as :\n\\begin{equation}\n\\begin{aligned}\n\\label{minmax}\n \\min_{\\theta} \\max_{\\xi}\\quad &  \\sum \\mathcal{L}_{D_{t}} (\\theta + \\xi )  \\\\\n    s.t. \\quad & \\xi \\in M\n\\end{aligned}\n\\end{equation}\n%where $M = \\{m_1,m_2,...,m_r\\}$ denotes the bases matrix that spans the parameter subspace of the previous task $\\mathbf{D}_t$, and $r$ is rank of $V$.\n%the subspace spanned by parameters $\\theta_t$ in round $t$.\n%The first term in Eq.\\ref{minmax} minimizes the training loss on task $\\mathcal{D}_t$.\n%The second term explore a possible flat region to maximize the probability of perturbing parameter $\\theta$ with $\\xi$, while simultaneously minimizing the training loss on task $D_{t+1}$. \n%\\textcolor{red}{Concretely, the second term depicts the generalization ability of current model parameters $\\theta + \\xi$ at time $t+1$. Specifically, can be further represented as $(\\mathcal{L}_{D_{t+1}} (\\theta + \\xi ) - \\mathcal{L}_{D_{t+1}} (\\theta)) + \\mathcal{L}_{D_{t+1}} (\\theta )$}. \n\nBased on the gradient projection method, the adversarial weight perturbation $\\xi$ can be updated with the projection on space $M$ via step size $\\eta_{1}$,\n\\begin{equation}\n\\label{flat}\n    \\xi \\leftarrow  \\xi + \\eta_{1}\n    \\vectorproj[M](\\nabla_{(\\theta_t)} L_{\\mathcal{D}_t}(\\theta_t +\\xi))\n\\end{equation}\n\n%To effectively store the optimal parameter $\\theta_t$ at each round $t$, we perform SVD decomposition on $V$ with a low-rank assumption:  $V_t = U_t \\Sigma_t V_t^T$, where $\\Sigma = [\\sigma_1, \\sigma_2,...,\\sigma_r]$ depict the importance of each singular vector with rank $r$. Consequently, Eq.\\ref{flat} can be further denoted as :\n\\begin{comment}\n\\begin{equation}\n    \\xi \\leftarrow  \\xi + \\eta_{1}\n    U_t \\Sigma_t V_t^T(\\nabla_{(\\theta_t +\\xi)} L_{\\mathcal{D}_t}(\\theta_t +\\xi))\n\\end{equation}\n\\end{comment}\nNotably, current data $D_t$ is inaccessible when coming to the round $t+1$, despite the reuse of $D_t$ when updating $\\theta$ to task $\\mathcal{D}_{t+1}$ in Eq.\\ref{minmax}. Recall that $\\theta_t$ preserves the previously acquired knowledge by maintaining a space $V$ consisting of the optimal model parameters learned at round $t$. We only update $\\theta$ along the orthogonal direction of $V$ after round $t$, leading to the least change (or locally no change) to $\\theta_t$ \\cite{}. Specially, the parameter $\\theta$ can be updated with step size $\\eta_2$:\n\\begin{equation}\n \\label{theta}\n    \\theta \\leftarrow  \\theta - \\eta_{2}(I-\\vectorproj[v])\n     (\\nabla_{(\\theta_t)} L_{\\mathcal{D}_t}(\\theta_t +\\xi))\n\\end{equation}\nwhere $I$ is the identity matrix.\n\\fi\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\nWe conduct extensive experiments on various datasets to evaluate the performance of our method. \nSpecifically, our experiments aim to answer the following questions:\n\\noindent\\textbf{Q1:} Can our method outperform baselines on the semi-supervised drifted stream learning problem?\n\\noindent\\textbf{Q2:} Can our  method generate robust pseudo-labels?\n\\noindent\\textbf{Q3:} Can our method effectively alleviate the forgetting problem? \n\\noindent\\textbf{Q4:} Can the flat region theory be supported by empirical investigations?\n\n\\vspace{-0.2cm}\n",
                "subsection 4.1": {
                    "name": "Experimental Setup",
                    "content": "\n",
                    "subsubsection 4.1.1": {
                        "name": "Data Description",
                        "content": "\nWe conducted experiments on eight datasets, including four widely-used synthetic benchmark datasets of stream learning research (i.e., UG\\_2C\\_2D, UG\\_2C\\_3D, UG\\_2C\\_5D and MG\\_2C\\_2D ) and four real-world stationary classification datasets (i.e., Optdigits, Spam, Satimage and Twonorm). \nTable \\ref{Tab:statistics} shows the statistics of datasets. \nSpecifically, we exploited the same setting in~\\cite{souza2015data,guo2020record} to simulate the distribution shift manually by regrouping the instances for the four classification datasets, \nAt each time, 1,000 instances arrived for UG\\_2C\\_2D and 2,000 instances arrived for UG\\_2C\\_3D, UG\\_2C\\_5D, MG\\_2C\\_2D, which were split into test and unlabeled datasets in a 30\\% and 70\\% ratio.  \nFor the Optdigits, Twonorm and Satimage datasets, 200 instances arrived each time and were split into 160 as unlabeled data and 40 as test data. \nFor the Spam dataset, 400 instances arrived at every time step and were split into  280 as unlabeled data and 120 as test data. \n%Labeled data is available in the beginning and keep the same number with unlabeled data.\n\n\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Baseline Algorithms",
                        "content": "\nSince there are limited studies working on the SDSL setting, we compared our method with the semi-supervised learning methods~\\cite{guo2020record}, domain adaptation methods via evolving shifted data~\\cite{liu2020learning,tzeng2017adversarial} and other competitive baseline methods: \n\\noindent(1) \\textbf{Supervised Training (ST)}: simply trains the model on the gold standard labeled data once and test the model on stream data without adaptation. This setting can be viewed as the lower bound of the performance.\n\\noindent(2) \\textbf{Joint Training (JT)}: assumes that the gold standard labels are available for all the streaming data at each time. The model is jointly trained on all the labeled data ever seen. JT is a strong baseline and can be viewed as an upper bound of the performance, since JT leverages all labeled data. \n\\noindent(3) \\textbf{Pseudo-labeling with high confidence (PL-Conf)} ~\\cite{lee2013pseudo}:  stores examples with high softmax probability at each time.\n\\noindent(4) \\textbf{Evolution Adaptive Meta-Learning (EAML)}~\\cite{liu2020learning}: is a strong baseline that adapts to gradually shifted data without forgetting. EAML penalizes model parameters with a $L_2$ Regularizer to alleviate forgetting. Without access to incoming data labels, EAML minimizes feature discrepancy at different times as an alternative to cross-entropy loss.\n\\noindent(5) \\textbf{Resource Constrained SSL under Distribution Shift (Record)} ~\\cite{guo2020record}: exploits a generation-detection-restoring pipeline. Differently, Record needs to generate the pseudo- labeled set based on the previously trained model and restore influential samples ever seen with a memory buffer. \n\\noindent(6) \\textbf{Domain-adversarial training of neural networks (DANN)} ~\\cite{tzeng2017adversarial}: is a representative domain adaptation method. We applied DANN to the evolving distribution shift setting by training the model with labeled data, and learning an invariant embedding on the evolving shifted data sequentially.\n\nWe reported the average results with standard deviations of 5 runs for all experiments.\nFollowing the setting in~\\cite{guo2020record}, we chose the mean teacher (MT) \\cite{tarvainen2017mean} as the base model of SSL classifier in our framework. \nWe used a two-layer multi-layer perceptron as a feature extractor. \nThe memory reply buffer is set as 100 for our framework (the look back size) and baselines, i.e., only 100 unlabeled examples can be stored in memory. For PL\\_Conf, we chose the 100 most confident samples to retrain the model.\nThe parameters of all the baseline models are defined in accordance with their respective publications. The step size $\\eta_{1}$\nand $\\eta_{2}$ are set as 0.01.\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Evaluation Metric",
                        "content": "\nFollowing the setting in \\cite{guo2020record, liu2020learning}, we evaluated the classification performance by averaging classification accuracy through each time as $Acc_t$:\n\\vspace{-0.3cm}\n\\begin{equation}\nAcc_t = \\frac{1}{T}\\sum_{i=1}^{T}R_{i,i}.\n\\vspace{-0.25cm}\n\\end{equation}\nAnd we evaluate the memorization ability by averaging classification accuracy on the final model as $Acc_T$:\n\\begin{equation}\nAcc_T = \\frac{1}{T}\\sum_{i=1}^{T}R_{T,i}.\n\\vspace{-0.25cm}\n\\end{equation}\nwhere $T$ is the total number of data sequences. $R_{i,j}$ is the test classification accuracy of the model at time $j$ after learning the last sample from $i$-th data. \n\\vspace{-0.3cm}\n%\\subsection{Experimental Results}\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Q1: Overall Comparison",
                    "content": "\n%\\subsubsection{Q1: Overall Comparison with Baselines}\nTo answer Q1, \\textbf{we compared our method with baselines that leverage unlabeled data through different strategies.} \nFigure \\ref{figure:1} shows the mean classification accuracy and standard deviation on the test data for five runs. The Y-axis represents accuracy ($Acc_t$ here), while the shaded regions show standard error computed using various random seeds. Figure~\\ref{figure:1} shows that our method achieves significant improvements over the baselines and is even comparable with the JT method (upper bound of the setting). \n%This is because our framework leverages labeled data $\\mathbf{D}$ and a short lookback pseudo label set $\\mathbf{D}_{t-1}$. The lookback data and adaptive Nearest Neighbor classifier both help to generate high-quality pseudo labels.\nThe observation validates the effectiveness of leveraging multi-level knowledge (supervised, unsupervised, and structure) for robust labeling and minimax-based flat region solver for anti-forgetting adaptation. \n\nBesides, one interesting observation arises from the results that unlabeled data matters for streaming adaption in the semi-supervised setting. \nSpecifically, both DANN and EAML perform poorly in streaming shifted data.  Our method and Record that utilize pseudo labels show significantly better performance. This implies pseudo labels can introduce auxiliary information about shifted data and improve generalization ability for streaming data. \nMoreover, while Record benefits from the influential shifted data detection mechanism, the pseudo labels are generated by the classifier trained on previous data. \nThe performance gap between our method and Record validates the necessity to mitigate classifier bias and verified our motivation to leverage the structure knowledge of invariant label class semantics.\n% Note that the outstanding performance of our method only depends on limited labeled data, i.e. golden data and a short look back and current unlabeled data, indicates that our model can quickly learn a usable model rather than waiting for more data to be collected. \n\\vspace{-0.3cm}\n"
                },
                "subsection 4.3": {
                    "name": "Q2: Study of Robust Pseudo Labeling",
                    "content": "\n\\iffalse\n",
                    "subsubsection 4.3.1": {
                        "name": "Effectiveness in Robust Pseudo Labeling ",
                        "content": "\n\n\\fi\n\n"
                    },
                    "subsubsection 4.3.2": {
                        "name": "Effectiveness of Robust Pseudo Labeling ",
                        "content": "\n% To understand the impact of robust pseudo labeling strategy, we test our method with two settings for which pseudo labels are generated. Ours-PL, ours-Record that replaces the generation strategy with PL\\_conf and Record method, respectively. \nTo validate the effectiveness of the proposed robust pseudo labeling method, we compare our method with two variants. \nSince our proposed solution is a generation-replay pipeline, we replace the generation part (pseudo label generation) with two widely used methods to construct the two variants: (1) the variant that takes PL\\_conf as the generation part, denoted by ``Ours\\_PL''; \n(2) the variant that takes the Record's generation method as the generation part, denoted by ``Ours\\_Record''. \nThe difference is the variant ``Ours\\_PL'' selects the most confident samples based on the softmax-based predictive confidence, while the variant ``Ours\\_Record'' selects the influential samples for the pseudo-labels generation.\n\n% To understand the impact of robust pseudo labeling strategy, we test our method with two settings for which pseudo labels are generated. Ours-PL, Ours-Record that replaces the generation strategy with PL\\_conf and Record method, respectively.\n% %Figure \\ref{Fig:robust_labeling} summarizes the results on UG\\_2C\\_2D and Satimage dataset.\n% \\textbf{Both record and PL\\_conf generates pseudo labels based on previously trained model. The difference lies in the selection strategies.\n% Ours\\_record replaces the generation strategy with record method, which they track the distribution changes and locate the distribution shifted samples. Then the most influential sample for the distribution change is selected based on a influence- based approach.\n% PL\\_conf selects the most confidence samples based on softmax value.}\n\nFigure~\\ref{Fig:431_robust_pseudo_labeling} shows our method uses the robust pseudo labeling method and outperforms the two variants. Specifically, the variant ``Ours\\_Record'' performs better than the variant ``Ours\\_PL'' since the Record method selects the most influential samples for the distribution change. \nThis observation verified our motivation that mining unlabeled shifted data could boost adaptation performance. \nHowever, the selected pseudo labels still lie in the overlap region with previous data. And it is insufficient to incorporate the non-overlapped information.\nIn contrast, our method achieves consistently promising performance. \nThe boost in performance verifies our motivation that class centroid-based clustering can exploit the global pattern structure and assign accurate labels for the new unlabeled drifted data that are non-overlapped with previous knowledge.\n  \\vspace{-0.4cm}\n\n\\vspace{-0.4cm}\n\n"
                    },
                    "subsubsection 4.3.3": {
                        "name": "Ablation study of Robust Pseudo labeling",
                        "content": " \nOur proposed robust pseudo labeling method generates promising pseudo labels for unlabeled data by preserving the invariant label semantics.\nTo validate the contribution of the invariant label semantics, we conduct the ablation study, in which we compare our method with the variant that omits the Invariant Label Semantics (ILS) constraint in Equation~\\ref{Eq:semantics}. \nWe denote the variant as ``Ours w/o ILS''.\n\nFigure~\\ref{Fig:robust_labeling_ablation} suggests that the ILS constraint contributes a stable classifier learning and can generate high-quality pseudo-labeled pairs. so, the model is easy to adapt well and mitigate forgetting to some extent due to the denoise ability. \nThe observation verifies our motivation that the utilization of unsupervised knowledge of new data could provide extra information than previous labeled data and benefits the pseudo label generation of shifted data.\n\n \n"
                    }
                },
                "subsection 4.4": {
                    "name": "Q3: Study of Alleviating Forgetting",
                    "content": "\n\n\n\n\n",
                    "subsubsection 4.4.1": {
                        "name": "Effectiveness of alleviating forgetting",
                        "content": "\nTo answer Q3, we validated the competence of our method on remembering previous knowledge.\nNoted that JT is the upper bound of the performance since JT assumes the availability of ground-truth labels across streaming data and restores all these data for training the model.\nIn our experiments, we considered a sequential setting where shifted data come one after another. To validate the effectiveness of the proposed algorithm, we trained the model until the final task, and then tested the model performance on all previously seen tasks. \nAll methods stop at $t= T$ after seeing all tasks.\n\nTable~\\ref{Tab:forget_competence} shows the average accuracy across all the tasks ($Acc_T$) on all the datasets. Specifically, we show the detailed results for each task along the timeline on $UG\\_2C\\_5D$ and Satimage datasets, which is illustrated in Figure~\\ref{Fig:forget}.\nThe results indicate that our method achieves better performance than other baseline methods on alleviating forgetting. \nSpecifically, Record takes a reply method and stores influential samples with a memory buffer on each sequential task. \nHowever, the restricted storing buffer determines the generalization ability and is incapable of recovering all the learned information. \nOn the contrary, EAML, DANN and our method relax the storage requirements. \nBut EAML uses the meta-learning strategies and is inferior to other techniques due to the lack of high-quality validation set on shifted data. \nMoreover, DANN mitigates forgetting via learning an invariant representation, which is insufficient to store distinctive knowledge about previous data. \n% Noted that our method achieves promising improvement over baselines despite of Joint training (JT) methods. \nCompared to these baseline algorithms, the performance of our method is closest to the upper bound (JT). \nThe reason is that the generated high-quality pseudo labels provide exact supervisions to adjust model decision boundary, meanwhile, seek a flat minimal region to further enhance generalization ability. \n"
                    },
                    "subsubsection 4.4.2": {
                        "name": "Ablation study of alleviating forgetting",
                        "content": "\n\nWe introduce a \\textbf{F}lat \\textbf{R}egion (FR) constraint to better alleviate the forgetting issue. \nIn the experiment, we also conduct an ablation study to investigate the contribution of the flat region constraint. \nSpecifically, we compare our method with a variant that omits the FR constraint in Equation~\\ref{eq:minmax}, denoted as ``Ours w/o FR''.\n\nFigure~\\ref{Fig:flat_region_1} shows our method takes merits from finding flat minimal region. The flatness of the optimal minima makes the model insensitive to parameter change. Besides, the model updates the parameters in the orthogonal direction of previous parameters space, which preserves previous knowledge when adapting to new data.\n\n\\vspace{-0.1cm}\n"
                    }
                },
                "subsection 4.5": {
                    "name": "Empirical Validation for Flat Region Theory",
                    "content": "\n\n\\vspace{-0.2cm}\n\n\n\\iffalse\n\n\\fi\n\nIn the Appendix A.2, we introduced the theoretical analysis of the flat region concept for boosting the model generalization ability. \nThe flat region enables the model insensitive to the drift, resulting in promising generalization ability.\nOur question is: whether the flat region exists? If yes, can our method find a wider flat region?\n% In the experiment, we also aim to provide empirical support for validating the flat region theory. \nTherefore, we conducted an empirical validation to answer the question.\nSuppose the flat region exists, the deviation of the model parameters within the region will not cause significant fluctuation of the model performance. \nTherefore, in the experiment, we validated the flat region theory based on a noise-sampling method. \nSpecifically, to find the local optimal $\\theta^{\\star}$, we measure its flatness as follows. \nWe firstly sampled noise from a pre-defined region $[0,b]$, then injected the noise to the trained model parameters only in the testing phase, and reported $Acc_T$.\n% We set the region upper bound b approximating to $1\\%$ of current optimal parameter range. \nWe changed the value of $b$, and averaged the model classification accuracy to measure its sensitiveness to noise. \nIntuitively, if the flat region exists, the model will be more robust against (less sensitive to) the injected noise. \nWe compare our method with one variant that does not take flat region search (denoted as Record) in the same setting.\nDue to the page limitation, we only present the result on UG\\_2C\\_5D and Satimage. \n%For the results on other datasets, please refer to the Appendix.\n \nFigure \\ref{Fig:Flat_region_2} shows that all the models have a respective flat region, but the range of the region is different. \nFor example, for the UG\\_2C\\_5D dataset, when the upper bound $b$ changes from 0 to 0.1, the performance of our method barely changes; when $b$ is larger than 0.1, the performance begins to drop significantly. \nThe observation reveals the fact that there exists a flat region $[0, 0.1]$, in which the model is insensitive to the drift. \nSimilarly, we can also observe a flat region $[0, 0.15]$ for our method on the Satimage dataset.\nAmong the comparison, our method has the widest flat region, which means our method has the best generalization ability. \nTherefore, the empirical results validate the effectiveness of the seeking for a flat region method.\n\\vspace{-0.4cm}\n\n\\iffalse\nthat the introduced flat region can boost model generalization ability, i.e., making the model not be sensitive to parameter changes. \nHere we present an empirical support on Satimage dataset to validate our proposed method can find a more flat region than baselines.\nFor a found local optimal $\\theta^{\\star}$, we measure its flatness as follows. \nWe sample a noise from a pre-defined region [0,b], then inject the noise to the model parameters only in test time, and report Acc\\_ave.\nWe set the region upper bound b approximating to $1\\%$ of current optimal parameter range. \nWe average the model classification accuracy to measure its sensitiveness to noise. Intuitively, if our method can find a flat region, the model will be more robust to the injected noise. We compare two baselines: Record and \nOur variant that ignores flat region search (Ours w/o flat).\nFigure \\ref{Fig:weight_analysis} represents the model performance on Satigate dataset. \nWe obtain the following observations and findings:\n1) A larger b (>0.15) leads to a significant performance drop, indicating that there may not exist a large flat region around a good local minima. \n2) When noise increase from 0.15 to 0.25, performance of Record and Our variant (Ours w/o flat region research) drops dramatically, i.e., Record drops from 0.8754 to 0.6439, while our method drops from 0.9108 to 0.7983.\nThe minima flatness makes the model not be sensitive to paramter changes, so as to alleviate forgetting. \n\n\\fi\n\n\\iffalse\n"
                },
                "subsection 4.6": {
                    "name": "Ablation studies",
                    "content": "\nWe conducted additional experiments to demonstrate the contributions of the three key technical components of our method: \\textbf{t}arget aware \\textbf{c}lassifier constraint (tc) in Eq.\\ref{classifier}, label relation aware (lr) constraint in Eq.\\ref{Eq:relation}, and the continual learning constraint in Eq.\\ref{minmax}. \nParticularly, target aware constraint and label relation aware constraint are used to regularize pseudo label generation, and continual learning constraint are used to alleviate forgetting problem. \nDue to space limitation, We test data on Satimage dataset.\n\nThe key findings in Figure \\ref{Fig:weight_analysis} are (1) lr constraint contributes a stable classifier learning and can generate high-quality pseudo-labeling pairs. Consequently, the model is easily to adapt well and mitigate forgetting to some extent due to the denoise ability.\n(2) Nearest Neighbor classifier plays more important role in our method. This is due to shifting property of data, the classifier trained on previous batch data is biased toward current shifted data. \n(3) Our method takes merits from finding flat minima region. Flatness of optimal minima makes the model not be sensitive to parameter change. Besides, model updates parameters which is orthogonal to previous optimization direction, this procedure mitigates forgetting when adapting on new data.\n\\fi\n\\begin{comment}\n\n\\begin{figure}[t]\n  \\centering\n  \\subfigure[Satimage on classification ability through time.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/ablation_satimage_arff_quiz_weight.pdf}}\n  % second\n  \\subfigure[Satimage on forgetting ability through time.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/ablation_satimage_weight_on_forget.pdf}}\n \\vspace{-0.5cm}\n  \\caption{Ablation study of our proposed models. (1) w/o tc: our method without target-oriented classifier constraints; (2) w/o fm:\nour method without finding flat minima to mitigate forgetting in Eq.(7); (3) w/o lr: our method without the label relation aware constraints.}\n\\vspace{-0.5cm}\n\\label{Fig:weight_analysis}\n\\end{figure}\n\n\\end{comment}\\vspace{-0.2cm}\n"
                }
            },
            "section 5": {
                "name": "Related Works",
                "content": "\nOur work lies at the intersection of semi-supervised learning, continual learning (life-long learning) , and domain adaptation. Next, we provide an overview of the related research efforts and briefly discuss the connections with our work.\n\n\\noindent \\textbf{Semi-Supervised Learning.} Our work is related to semi-supervised learning (SSL) ~\\cite{lee2013pseudo}. \nSSL is a special case of machine learning that leverages a large amount of unlabeled data with a small portion of labeled data to enhance the learning performance.\nSSL methods can be categorized into consistency-based~\\cite{berthelot2019mixmatch}, temporal ensembling~\\cite{laine2016temporal}, virtual adversarial training~\\cite{miyato2018virtual}, pseudo labeling~\\cite{lee2013pseudo}. \nMost of SSL studies are designed both for offline and ``identical and independent distribution'' (i.i.d.) data but ignore the evolving nature of unlabeled samples. \nThere are some emerging works designed for streaming data in the SSL setting~ by integrating local consistency propagation on graph~\\cite{zhao2022exploring,zhao2021graphsmote}. \nHowever, these methods assume the streaming data are i.i.d. with the labeled data which is not ideal for a realistic scenario. \nRecently, \\cite{guo2020record} considers learning from streaming data with a distribution shift in a semi-supervised way. However, they still generate pseudo labels via classifiers trained on previous data and maintain a memory buffer to store pseudo labeled data sequentially. \nDifferently, We improve the pseudo label generation process considering shifted data, and only replay data with short lookback.\n%\\textbf{Moreover, \\cite{XXX} , but assuming that the labeled data is available at every time step, which is difficult to satisfy in real-world applications. }\n%Comparing to the literature, our proposed method can work for real streaming shift in the SSL setting by \\textcolor{red}{[one sentence to summarize]}.\n\n\\noindent \\textbf{Continual Learning.} \nOur method also connects to continual learning (CL). \nCL studies the problem of learning new information throughout their lifespan, without forgetting previously learned data or tasks.\nGenerally speaking, there are three typical scenarios for CL: (1) class-incremental~\\cite{liu2020mnemonics}, (2) task-incremental~\\cite{maltoni2019continuous}, and (3) data-incremental scenario (with class label set fixed)~\\cite{chrysakis2020online,ren2018tracking}. \nOur work is related to the third category, but the incoming data is shifted without any supervision. \nIn contrast, we release the requirement of the availability of labeled incoming data.\n%\\textbf{We refer to this problem as semi supervised continual domain adaptation.}\n%See~\\cite{delange2021continual} for a closer look at the different problem formulations.\nDifferent from traditional regularization-based methods that penalize any changes to previous important parameters,\n%we alleviate catastrophic forgetting without necessarily replaying old data ~\\cite{rolnick2019experience} nor increasing the model branch over time ~\\cite{hadsell2020embracing}. \nwe modify the objective function by introducing a flat region \\cite{schulman2015trust,shi2021overcoming} into SDSL setting. \nDifferent from previous works in CL that learn the flat region with prior knowledge \\cite{,schulman2015trust, shi2021overcoming}, we formulate the automated flat region identification problem as a minimax game into SDSL, which can ease the forgetting issue and adapt well to the new timeline.\n\n\\noindent \\textbf{Unsupervised Domain Adaptation.} \nOur work is relevant to unsupervised domain adaptation~\\cite{tzeng2017adversarial}. Unsupervised domain adaptation aims to transfer the knowledge learned from labeled source domains to the unlabeled target domain. \nExisting works mainly focus on minimizing the discrepancy between the source and target distributions for learning domain-invariant features~\\cite{tzeng2017adversarial,fernando2013unsupervised}. \n% Generally, the weights of the deep architecture containing a feature encoder and a classifier are shared for both domains \\cite{}.\n% With the covariate shift assumption \\cite{}, such invariant representations, along with the source predictor can generalize to the target domain \\cite{}.\nYet, recent theoretical analysis and empirical findings suggest that distinct marginal label distributions across domains provably undermine the target generalization. \nTherefore, to minimize the distance between labeling functions, \\cite{li2021learning,ren2021cross} accesses a small amount of labeled data in the target domain as extra supervision.\n\\cite{wang2021self} designs a contrastive re-weighting method to dynamically modify the generated labels on the target domain. Our work is enlightened by this line of works, differently, we consider a label class semantic constraints. Besides, we aim to alleviate the catastrophic forgetting problem, which is ignored by this pipeline.\n \n\n\n\n\n\n\n\n\n\\vspace{-0.3cm}\n"
            },
            "section 6": {
                "name": "Conclusion Remarks",
                "content": "\nIn this work, we provide a systematic analysis of semi-supervised drifted streaming learning with short lookback (SDSL), which is a realistic yet challenging setting without extensive study.\nThen we propose a novel method that follows the 'generation-replay' pipiline.  \nTo generate accurate pseudo labels for incoming shifted data, we leverage supervised knowledge of previously labeled data to label overlapped data, unsupervised knowledge of new data to refine non-overlapped data, as well as structure knowledge of invariant label semantic embedding to regularize the classifier.\nTo achieve adaptive anti-forgetting model replay, we introduce the flat region notion and search the feasible region with a minimax game. \nComprehensive experimental results verified our motivations and demonstrate the effectiveness of our method.  \nIn the future, we aim to explore\nmore properties of unlabeled data to further improve the robustness of SDSL setting.\n\\vspace{-0.4cm}\n\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\\iffalse\n\\begin{acks}\nTo Robert, for the bagels and explaining CMYK and color spaces.\n\\end{acks}\n\\fi\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\normalem\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{main}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\newpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n",
                "subsection 7.1": {
                    "name": "Algorithm of Robust Pseudo-Label Generation",
                    "content": "\n\\begin{algorithm}[h]\n  \\caption{Robust Pseudo-Label Generation}\n  \\label{alg:Framwork}\n  %\\scalebox{0.75}{\n  \\begin{algorithmic}[1] \n  \\REQUIRE %??????????Input\n    gold-label data $\\mathcal{D}$,\n    Pseudo-labeled set $\\mathcal{\\hat{D}}_{t-1}$,\n    Randomly initialization on model parameter $\\theta$, \n    Randomly initialization on latent feature matrix $\\mathbf{H}_t$.\n    \\STATE Pretraining the model on golden label set $\\mathcal{D}$ with feature extractor and classifier.\n    \\STATE Generating latent label vector $\\mathbf{V}$ with classifier parameters via SVD decomposition.\n    \\FOR{$t$ in $T$}\n    \\STATE Training the model with $\\mathcal{D}$ and $\\mathcal{D}_{t-1}$ with $\\theta$.\n    \\STATE Applying the model with $\\theta$ on $\\mathbf{X}_t$ to initialize prototypes $\\mathbf{U}_t$ in Eq.\\ref{eq:initialization}.\n    \\WHILE {Not Converged}\n    \\STATE Calculating $\\hat{\\mathbf{H}}_t$ with current $\\mathbf{U}_t$ and $\\mathbf{\\hat{V}}$ in Eq.\\ref{Eq:semantics}.\n    \\STATE Generating $\\hat{y}_t$ in Eq.\\ref{eq:label}.\n    \\STATE Updating prototypes $\\mathbf{U}_t$ with Eq.\\ref{eq:centroid} and Eq.\\ref{Eq:semantics}. \n    \\STATE Update $\\theta$ with gradients to minimize Loss $\\mathcal{L}_{full}$.\n    \\ENDWHILE\n    \\RETURN Pseudo-labeled set $\\mathcal{D}_t$ and model parameter $\\theta$.\n    \\ENDFOR\n  \\end{algorithmic}\n  %}%% resizebox\n\\end{algorithm}\n\n"
                },
                "subsection 7.2": {
                    "name": "Algorithm of Adaptive Anti-forgetting Model Replay",
                    "content": "\nAfter the robust pseudo-label generation stage, we obtain pseduo-labeled pairs $\\mathcal{D}_{t}$ (also collected it with memory buffer as shown in the paper.). Then we replay the data $\\mathcal{D}$ and $\\mathcal{D}_{t}$, and then move to the next robust pseudo-label generation stage (t+1). Here, we represent the adaptive anti-forgetting model replay stage. \n\\begin{algorithm}[h]\n  \\caption{daptive Anti-forgetting Model Replay}\n  \\label{alg:Framwork}\n  %\\scalebox{0.75}{\n  \\begin{algorithmic}[1] \n  \\REQUIRE %??????????Input\n    gold-label data $\\mathcal{D}$,\n    Pseudo-labeled set $\\mathcal{\\hat{D}}_t$,\n    trained model parameters after time $t-1$, step size $\\eta_{1}$ and $\\eta_{2}$, the model parameters $\\theta$ train on $t-1$\n    \\WHILE {Not Converged}\n    \\STATE update $\\theta$ on $\\mathcal{D}$ and $\\mathcal{\\hat{D}}_t$ via Eq.\\ref{eq:theta}\n    \\STATE update $\\xi $ via Eq.\\ref{eq:flat} \n    \\ENDWHILE\n    \\RETURN model parameter $\\theta$.\n  \\end{algorithmic}\n  %}%% resizebox\n\\end{algorithm}\n\n\n"
                },
                "subsection 7.3": {
                    "name": "Theoretical Analysis",
                    "content": "\nWe present a theoretical analysis on why the flat region can characterize the continual learning property on streaming data and why our method works. \nWithout loss of generalization, we simplify the drifted stream data with $\\mathcal{D}_{t-1}$ and $\\mathcal{D}_{t}$, which are sampled from data distribution $Q_{t-1}$ and $Q_t$, respectively. \nBased on previous works on PAC-Bayes bound \\cite{neyshabur2017exploring,deng2021flattening}, given a `prior' distribution $\\mathbf{P}$ (a common assumption is zero mean, $\\sigma^2$ variance Gaussian distribution) over the weights, the expected error can be bounded with probability at least 1 - $\\delta$:\n\\begin{equation}\n\\begin{matrix}\n%first line\n&\\displaystyle \\min_{\\Delta\\theta}\\mathbb{E}_{\\xi}[\\mathcal{L}_{\\mathbf{Q}_{t-1} \\cup \\mathbf{Q}_t }(\\theta +\\Delta\\theta+ \\xi)]  \\\\\n\\leq \n%second line\n&\\displaystyle \\min_{\\Delta\\theta \\in M^c}\\mathbb{E}_{\\xi}[\\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta + \\Delta\\theta+\\xi)]\n+4\\sqrt{\\frac{1}{n} KL(\\theta+\\xi||P) +ln\\frac{2N}{\\delta }{}}\n\n\\\\\n% third line\n&+ \\underbrace{\\max_{\\xi \\in M}[\\mathcal{L}_{\\mathcal{D}_t }(\\theta + \\Delta\\theta+ \\xi)] \n-  \\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta + \\Delta \\theta) }_\\text{Generalization Gap} \n\n+\\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta + \\Delta\\theta)\n\\end{matrix}\n\\end{equation}\nwhere $\\xi \\in M$ and $\\Delta \\theta$ is updated along the orthogonal direction of previous optimal solution $\\theta$ learned on previous data $\\mathcal{D}_{t-1}$, i.e., $\\theta \\in M^c$. \nSo that $\\displaystyle \\min_{\\Delta\\theta \\in M^c}\\mathbb{E}_{\\xi}[\\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta + \\Delta\\theta+\\xi)]$ does not change too much compared with the previously minimized $\\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta)$.\nSimilarly, the updated parameters will not increase the training loss on $\\mathcal{D}_{t-1}$.\nThe second term depicts the Kullback Leibler (KL) divergence to the ``prior'' P \\cite{neyshabur2017exploring}.\nOur method exactly optimizes the worst-case of the\nflatness of weight loss landscape $\\displaystyle \\max_{\\xi \\in M}\\mathcal{L}_{\\mathcal{D}_t }(\\theta + \\Delta\\theta+ \\xi)\n-  \\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta + \\Delta \\theta)$ to control the above PAC-Bayes bound, which theoretically justifies why our method works.\n\n"
                },
                "subsection 7.4": {
                    "name": "Additional Implementation Details",
                    "content": "\nAll experiments were conducted on the Ubuntu 18.04.5 LTS operating system, Intel(R) Core(TM) i9-10900X CPU@ 3.70GHz, and 1 way SLI RTX 3090 and 128GB of RAM, with the framework of Python 3.8.5 and PyTorch 1.8.1.\n\n\n\\iffalse\n"
                }
            },
            "section 8": {
                "name": "Online Resources",
                "content": "\nThe code and data for the experiments could be downloaded at \\url{https://www.dropbox.com/s/7edvhaeiypk6sl7/dataset_and_code.zip?dl=0}\n\\fi\n\n"
            }
        },
        "tables": {
            "Tab:statistics": "\\begin{table}[h]\n\\vspace{-0.2cm}\n\\centering\n\\caption{Statistic Analysis of Datasets.}\n\\vspace{-0.2cm}\n\\begin{tabular}{cccc}\n\\hline\n\\textbf{Dataset} & Instances & Features &Classes\\\\\n\\hline\nUG\\_2C\\_2D &  100,000&2 &2\\\\\nUG\\_2C\\_3D & 200,000 &3 &2\\\\\nUG\\_2C\\_5D & 200,000 &5&2 \\\\\nMG\\_2C\\_2D&200,000&2&2\\\\\nOptdigits&5620&64&10\\\\\nSatimage &6435&36&7 \\\\\nSpambase&9324&500&2\\\\\nTwonorm & 7400 &2&2\\\\\n\\hline\n\\end{tabular}\n\\label{Tab:statistics}\n\\vspace{-0.3cm}\n\\end{table}",
            "Tab:forget_competence": "\\begin{table*}[t!]\n\\centering\n% \\caption{End task performance on previously seen data over all baseline methods. High value indicates the model keeps high competence over previous knowledge. Joint Training (JT) shows upper bound performance.}\n\\caption{Comparison of effectiveness on alleviating forgetting. Noted that Joint Training (JT) shows the ideal performance of the SDSL setting. The closer to JT, the better the performance.}\n\\begin{tabular}{cllllllll}\n\\hline\n\\textbf{Method} & MG\\_2C\\_2D & UG\\_2C\\_2D&UG\\_2C\\_3D& UG\\_2C\\_5D&Optdigits\n&Satimage&Spam&Twonorm\\\\\n\\hline\nST \n& 0.479$\\pm$0.020\n&0.367$\\pm$0.020 \n& 0.438 $\\pm$0.019\n&0.570$\\pm$0.015\n& 0.439$\\pm$0.032\n&0.318$\\pm$0.019\n&0.965$\\pm$0.018\n&0.881$\\pm$0.009\\\\\n\nJT & 0.593$\\pm$0.012 \n&0.907$\\pm$0.0252 \n&0.837$\\pm$0.0103\n&0.919$\\pm$0.007\n&0.921$\\pm$0.019\n&0.816 $\\pm$0.018\n&0.971$\\pm$0.016 \n&0.966$\\pm$0.012\\\\\n\nPL\\_conf \n&0.513$\\pm$0.020\n&0.531$\\pm$0.021\n&0.567$\\pm$0.219\n&0.614$\\pm$0.019\n&0.406$\\pm$0.025\n&0.385$\\pm$0.026\n&0.966$\\pm$0.021\n& 0.956$\\pm$0.011\\\\\n\nDANN \n&0.532$\\pm$0.039\n&0.349$\\pm$0.011\n&0.578$\\pm$0.018\n&0.701$\\pm$0.014\n&0.415$\\pm$0.031\n&0.607$\\pm$0.019\n&0.969$\\pm$0.013\n&0.948$\\pm$0.018\\\\\n\nEAML \n&0.508$\\pm$0.014\n&0.499$\\pm$0.016\n&0.499$\\pm$0.012\n&0.569$\\pm$0.016\n&0.527$\\pm$0.018\n&0.397$\\pm$0.015\n&0.725$\\pm$0.014\n&0.729$\\pm$0.021\\\\\n\nRecord \n&0.499$\\pm$0.021\n&0.883$\\pm$0.010\n&0.599$\\pm$0.035\n&0.725$\\pm$0.020\n&0.613$\\pm$0.021\n&0.536$\\pm$0.026 \n&0.965$\\pm$0.018\n&0.961$\\pm$0.010 \\\\\n\nOurs\n&0.549$\\pm$0.021\n&0.894$\\pm$0.017\n&0.717$\\pm$0.015\n&0.768$\\pm$0.057\n&0.657$\\pm$0.024\n&0.635$\\pm$0.034\n&0.973$\\pm$0.012 \n&0.964$\\pm$0.015 \\\\\n\\hline\n\\end{tabular}\n\\label{Tab:forget_competence}\n\\end{table*}"
        },
        "figures": {
            "fig:train_setting": "\\begin{figure}[t]\n\\vspace{-0.0cm}\n  \\centering\n  \\includegraphics[width=\\linewidth]{samples/figure/SDSL_setting.pdf}\n  \\vspace{-0.3cm}\n\\caption{Learning in the semi-supervised, stream, gradually drifted data environment with short lookback.}\n\\vspace{-0.5cm}\n\\label{fig:train_setting}\n\\end{figure}",
            "fig:train": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{samples/figure/training_stage.pdf}\n\\vspace{-0.5cm}\n\\caption{The training and testing stage of SDSL.}\n\\label{fig:train}\n \\vspace{-0.5cm}\n\\end{figure}",
            "fig:overview": "\\begin{figure}[t]\n\\vspace{-0.3cm}\n  \\centering\n  \\includegraphics[width=\\linewidth]{samples/figure/overview.pdf}\n \\vspace{-0.7cm}\n\\caption{Overview of the proposed framework. }\n\\label{fig:overview}\n\\vspace{-0.6cm}\n\\end{figure}",
            "fig:label_relation": "\\begin{figure}[t]\n\\vspace{-0.3cm}\n  \\centering\n  \\includegraphics[width=\\linewidth]{samples/figure/sketch_figure.pdf}\n \\vspace{-0.7cm}\n\\caption{The Robust Pseudo-Label Generation Process. }\n\\label{fig:label_relation}\n\\vspace{-0.5cm}\n\\end{figure}",
            "fig:flat_region": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth,height = 4cm]{samples/figure/flat_region.pdf}\n \\vspace{-0.8cm}\n  \\caption{Motivation of the flat region.}\n  \\label{fig:flat_region}\n  \\vspace{-0.4cm}\n\\end{figure}",
            "fig:case": "\\begin{figure*}[h]\n \\vspace{-0.2cm}\n  \\centering\n  \\subfigure[MG\\_2C\\_2D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_MG_2C_2D_100.pdf}}\n  % second\n  \\subfigure[UG\\_2C\\_2D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_UG_2C_2D_100.pdf}}\n  % third\n      \\subfigure[UG\\_2C\\_3D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_UG_2C_3D_30.pdf}}\n  %fourth\n    \\subfigure[UG\\_2C\\_5D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_UG_2C_5D_30.pdf}}\n  %fifth\n    \\subfigure[Optdigits]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_optdigits_arff_100.pdf}}\n %six \n    \\subfigure[Satimage]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_satimage_arff_quiz.pdf}} \n  %seven\n  \\subfigure[Spambase]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_spambase_arff_30.pdf}}\n    \\subfigure[Twonorm]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_twonorm_30.pdf}}\n \\vspace{-0.5cm}\n    \\caption{Overall performance comparisons between our method and baseline methods.} \\label{fig:case}\n  \\setlength{\\abovecaptionskip}{0cm}\n  \\label{figure:1}\n \\vspace{-0.2cm}\n\\end{figure*}",
            "figure:1": "\\begin{figure*}[h]\n \\vspace{-0.2cm}\n  \\centering\n  \\subfigure[MG\\_2C\\_2D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_MG_2C_2D_100.pdf}}\n  % second\n  \\subfigure[UG\\_2C\\_2D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_UG_2C_2D_100.pdf}}\n  % third\n      \\subfigure[UG\\_2C\\_3D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_UG_2C_3D_30.pdf}}\n  %fourth\n    \\subfigure[UG\\_2C\\_5D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_UG_2C_5D_30.pdf}}\n  %fifth\n    \\subfigure[Optdigits]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_optdigits_arff_100.pdf}}\n %six \n    \\subfigure[Satimage]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_satimage_arff_quiz.pdf}} \n  %seven\n  \\subfigure[Spambase]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_spambase_arff_30.pdf}}\n    \\subfigure[Twonorm]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_twonorm_30.pdf}}\n \\vspace{-0.5cm}\n    \\caption{Overall performance comparisons between our method and baseline methods.} \\label{fig:case}\n  \\setlength{\\abovecaptionskip}{0cm}\n  \\label{figure:1}\n \\vspace{-0.2cm}\n\\end{figure*}",
            "Fig:robust_labeling": "\\begin{figure}[h]\n  \\vspace{-0.5cm}\n  \\centering\n  \\subfigure[UG\\_2C\\_5D]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/UG_2C_5D_diff_prob_30.pdf}}\n  % second\n  \\subfigure[Satimage]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/satimage_arff_diff_prob_100.pdf}}\n%   \\caption{classification results w/o robust labeling component on UG\\_2C\\_5D and Satimage dataset through each round.}\n\\caption{Validation of robust pseudo labeling.}\n    % \\vspace{-0.5cm}\n    \\label{Fig:robust_labeling}\n \\vspace{-0.5cm}\n\\end{figure}",
            "Fig:431_robust_pseudo_labeling": "\\begin{figure}[h]\n  \\vspace{-0.5cm}\n  \\centering\n  \\subfigure[Classification performance through timelines on the UG\\_2C\\_5D dataset. ]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_UG_2C_5D_ablation_30.pdf}}\n  % second\n  \\subfigure[Classification performance through timelines on the Satimage dataset.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/new_satimage_arff_diff_100.pdf}}\n%   \\caption{classification results w/o robust labeling component on UG\\_2C\\_5D and Satimage dataset through each round.}\n\\vspace{-0.4cm}\n\\caption{Validation of robust pseudo labeling.}\n\\vspace{-0.4cm}\n\\label{Fig:431_robust_pseudo_labeling}\n\\end{figure}",
            "Fig:robust_labeling_ablation": "\\begin{figure}[h]\n  \\vspace{-0.3cm}\n  \\centering\n  \\subfigure[Classification performance ($Acc_t$) through timelines on Satimage data.]{\n  \\includegraphics[width=0.22\\textwidth,height = 0.13\\textheight]{samples/figure/new_weight_satimage_arff_quiz.pdf}}\n  \\subfigure[Memorization ability ($Acc_T$) through timelines on Satimage data.]{\n  \\includegraphics[width=0.22\\textwidth,height = 0.13\\textheight]{samples/figure/relation_satimage_arff_forget.pdf}}\n\\vspace{-0.3cm}\n\\caption{Ablation study in Pseudo labeling.}\n\\vspace{-0.5cm}\n\\label{Fig:robust_labeling_ablation}\n\\end{figure}",
            "Fig:forget": "\\begin{figure}[t]\n  \\centering\n  \\subfigure[Memorization ability through timelines on the UG\\_2C\\_5D dataset.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/ablation_UG_2C_5D_forget.pdf}}\n  % second\n  \\subfigure[Memorization ability through timelines on the Satimage dataset.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/ablation_satimage_arff_forget.pdf}}\n \\vspace{-0.3cm}\n  \\caption{Memorization ability through timelines. Noted that Joint Training (JT) shows the ideal performance of the SDSL setting. The closer to JT (in green curve), the better the performance.}\n    \\label{Fig:forget}\n \\vspace{-0.5cm}\n\\end{figure}",
            "Fig:flat_region_1": "\\begin{figure}[t]\n  \\vspace{-0.3cm}\n  \\centering\n    \\subfigure[Classification performance through timelines on the Satimage data.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/FR__satimage_arff_quiz.pdf}} \n  \\subfigure[Memorization ability through timelines on the Satimage data.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/FR__satimage_arff_forget.pdf}}\n  % second\n\\vspace{-0.4cm}\n\n\\caption{Ablation study on flat region searching term.}\n    % \\vspace{-0.5cm}\n\\vspace{-0.4cm}\n\\label{Fig:flat_region_1}\n\\end{figure}",
            "Fig:Flat_region_2": "\\begin{figure}[h]\n\\vspace{-0.5cm}\n  \\centering\n  \\subfigure[Memorization ability through timelines on the UG\\_2C\\_5D dataset.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/now_2_UG_2C_5D.pdf}}\n  % second\n  \\subfigure[Memorization ability through timelines on the Satimage dataset.]{\n  \\includegraphics[width=0.23\\textwidth,height = 0.13\\textheight]{samples/figure/now_2_Satimage.pdf}}\n%   \\caption{classification results w/o robust labeling component on UG\\_2C\\_5D and Satimage dataset through each round.}\n\\vspace{-0.4cm}\n\\caption{Flat region validation.}\n\\vspace{-0.2cm}\n\\label{Fig:Flat_region_2}\n\\end{figure}",
            "Fig:weight_analysis": "\\begin{figure}[h]\n  \\centering\n  \\subfigure[Satimage on classification ability through time.]{\n  \\includegraphics[width=0.35\\textwidth,height = 0.2\\textheight]{samples/figure/noise_ref.pdf}}\n\n%   \\caption{Comparison of the flatness of the local optimal minima found by the Our method and baselines.}\n\\caption{Flat region validation.}\n\\label{Fig:weight_analysis}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    h_{\\theta}(\\mathcal{D}, \\hat{\\mathcal{D}}_{t-1}, \\mathbf{X}_{t}) \\rightarrow \\mathbf{\\hat{\\mathbf{y}}}_t.\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{classifier}\n  \\min_{\\theta}\\mathcal{L}_{CE}(\\mathcal{D}, \\hat{\\mathcal{D}}_{t-1};\\theta) = \\sum_{(\\mathbf{x}_i, y_i) \\in \\{\\mathcal{D} \\cup \\mathbf{\\hat{\\mathcal{D}}}_{t-1}\\}} \\ell(h_\\theta(\\mathbf{x}_i),y_i).\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:initialization}\n\\mathbf{u}_c^{(0)} = \\frac{\\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t} \\delta (h(\\mathbf{x}_i))f(\\mathbf{x}_i)}{\\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t}\\delta (h(\\mathbf{x}_i))},\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:label}\n    \\hat{y}_i = \\arg \\min_{c \\in C} d(f(\\mathbf{x}_i), \\mathbf{u}_{c}),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\label{eq:centroid}\n    \\mathbf{u}_c^{(k)} = \\frac{\\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t}f(\\mathbf{x}_i) * \\mathbbm{1} (y_i ==1)}{\\sum_{\\mathbf{x}_t \\in \\mathbf{X}_i} \\mathbbm{1} (y_{i} ==c)},\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\label{classifier}\n  \\min_{\\theta}\\mathcal{L}_{PL}(\\mathbf{X_t;\\theta})= \\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t} \\ell(h_\\theta(x_i),\n  \\hat{y}_i).\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\mathcal{R}(\\mathbf{U}_t) = \\min_{\\mathbf{U}_t,\\mathbf{H}_t} \n\\|\\mathbf{U}_{t} - \\mathbf{H}_{t}^{T} {\\hat{\\mathbf{V}}}\\|_2.\\\\\n \\label{Eq:semantics}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\label{classifier}\n\\mathcal{L}_{total} =\\mathcal{L}_{CE}(\\mathcal{D}, \\mathcal{\\hat{D}}_{t-1};\\theta) + \\mathcal{L}_{PL}(\\mathbf{X}_t;\\theta) + \\mathcal{R}(\\mathbf{U}_t),\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\label{eq:initialization}\nu_k^{(0)} = \\frac{\\sum_{x_i \\in \\mathbf{X}_t} \\delta (h(x_i))f(x_i)}{\\sum_{x_i \\in \\mathbf{X}_t}\\delta (h(x_i))}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\label{eq:label}\n    \\hat{y}_i = \\arg \\min_{k} d(f(x_i), u_{k}),\n\\end{equation}",
            "eq:11": "\\begin{equation}\n\\label{eq:centroid}\n    u_k^{(1)} = \\frac{\\sum_{x_i \\in \\mathbf{X}_t}f(x_i) * \\mathbbm{1} (y_i ==1)}{\\sum_{x_t \\in \\mathbf{X}_i} \\mathbbm{1} (y_{i} ==k)}.\n\\end{equation}",
            "eq:12": "\\begin{equation}\n\\label{eq:relation}\n   \\min_{U_t,H_t} ||U^t - (H^t)^T \\dot V ||_2,\n\\end{equation}",
            "eq:13": "\\begin{equation}\n\\label{classifier}\n    \\mathcal{L}_{PL} = \\sum_{\\mathbf{x}_i \\in \\mathbf{X}_t} \\ell_\\theta(h(x),\\hat{y}_i).\n\\end{equation}",
            "eq:14": "\\begin{equation}\n\\begin{aligned}\n\\label{minmax}\n \\min_{\\theta} \\quad &   \\sum_{\\mathbf{x}_i \\in \\textbf{D}_t} \\mathcal{L}(\\mathbf{x}_i;\\theta) \\\\\n    s.t. \\quad &\\theta^{\\star} - a \\leqslant \\theta \\leqslant \\theta^{\\star} + a.\\\\\n\\end{aligned}\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\begin{aligned}\n\\label{eq:minmax}\n \\min_{\\theta} \\max_{\\xi}\\quad &  \\sum_{\\mathbf{x}_i \\in \\mathcal{D}_t } \\mathcal{L}_{\\mathcal{D}_{t}} (\\mathbf{x}_i ; \\theta + \\xi )  \\\\\n    s.t. \\quad & \\xi \\in \\mathcal{M}.\\\\\n\\end{aligned}\n\\end{equation}",
            "eq:16": "\\begin{equation}\n    \\xi \\leftarrow  \\xi + \\eta_{1}\n    \\vectorproj[M](\\nabla_{(\\theta_t)} \\mathcal{L}_{\\mathcal{D}_t}(\\theta_t +\\xi)).\n\\label{eq:flat}\n\\end{equation}",
            "eq:17": "\\begin{equation}\n    \\theta \\leftarrow  \\theta - \\eta_{2}(I-\\vectorproj[\\mathbf{M}])\n     (\\nabla_{(\\theta_t)} L_{\\mathcal{D}_t}(\\theta_t +\\xi)).\n     \\label{eq:theta}\n\\end{equation}",
            "eq:18": "\\begin{equation}\n\\begin{aligned}\n\\label{minmax}\n \\min_{\\theta} \\quad &   \\sum_{x_i \\in \\textbf{D}_t} \\mathcal{L}(x_i;\\theta) \\\\\n    s.t. \\quad &\\theta^{\\star} - a \\leqslant \\theta \\leqslant \\theta^{\\star} + a\\\\\n\\end{aligned}\n\\end{equation}",
            "eq:19": "\\begin{equation}\n\\begin{aligned}\n\\label{minmax}\n \\min_{\\theta} \\max_{\\xi}\\quad &  \\sum \\mathcal{L}_{D_{t}} (\\theta + \\xi )  \\\\\n    s.t. \\quad & \\xi \\in M\n\\end{aligned}\n\\end{equation}",
            "eq:20": "\\begin{equation}\n\\label{flat}\n    \\xi \\leftarrow  \\xi + \\eta_{1}\n    \\vectorproj[M](\\nabla_{(\\theta_t)} L_{\\mathcal{D}_t}(\\theta_t +\\xi))\n\\end{equation}",
            "eq:21": "\\begin{equation}\n \\label{theta}\n    \\theta \\leftarrow  \\theta - \\eta_{2}(I-\\vectorproj[v])\n     (\\nabla_{(\\theta_t)} L_{\\mathcal{D}_t}(\\theta_t +\\xi))\n\\end{equation}",
            "eq:22": "\\begin{equation}\nAcc_t = \\frac{1}{T}\\sum_{i=1}^{T}R_{i,i}.\n\\vspace{-0.25cm}\n\\end{equation}",
            "eq:23": "\\begin{equation}\nAcc_T = \\frac{1}{T}\\sum_{i=1}^{T}R_{T,i}.\n\\vspace{-0.25cm}\n\\end{equation}",
            "eq:24": "\\begin{equation}\n\\begin{matrix}\n%first line\n&\\displaystyle \\min_{\\Delta\\theta}\\mathbb{E}_{\\xi}[\\mathcal{L}_{\\mathbf{Q}_{t-1} \\cup \\mathbf{Q}_t }(\\theta +\\Delta\\theta+ \\xi)]  \\\\\n\\leq \n%second line\n&\\displaystyle \\min_{\\Delta\\theta \\in M^c}\\mathbb{E}_{\\xi}[\\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta + \\Delta\\theta+\\xi)]\n+4\\sqrt{\\frac{1}{n} KL(\\theta+\\xi||P) +ln\\frac{2N}{\\delta }{}}\n\n\\\\\n% third line\n&+ \\underbrace{\\max_{\\xi \\in M}[\\mathcal{L}_{\\mathcal{D}_t }(\\theta + \\Delta\\theta+ \\xi)] \n-  \\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta + \\Delta \\theta) }_\\text{Generalization Gap} \n\n+\\mathcal{L}_{\\mathcal{D}_{t-1} }(\\theta + \\Delta\\theta)\n\\end{matrix}\n\\end{equation}"
        }
    }
}