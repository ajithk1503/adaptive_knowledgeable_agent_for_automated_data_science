{
    "meta_info": {
        "title": "Learning Task-relevant Representations for Generalization via  Characteristic Functions of Reward Sequence Distributions",
        "abstract": "Generalization across different environments with the same tasks is critical\nfor successful applications of visual reinforcement learning (RL) in real\nscenarios. However, visual distractions -- which are common in real scenes --\nfrom high-dimensional observations can be hurtful to the learned\nrepresentations in visual RL, thus degrading the performance of generalization.\nTo tackle this problem, we propose a novel approach, namely Characteristic\nReward Sequence Prediction (CRESP), to extract the task-relevant information by\nlearning reward sequence distributions (RSDs), as the reward signals are\ntask-relevant in RL and invariant to visual distractions. Specifically, to\neffectively capture the task-relevant information via RSDs, CRESP introduces an\nauxiliary task -- that is, predicting the characteristic functions of RSDs --\nto learn task-relevant representations, because we can well approximate the\nhigh-dimensional distributions by leveraging the corresponding characteristic\nfunctions. Experiments demonstrate that CRESP significantly improves the\nperformance of generalization on unseen environments, outperforming several\nstate-of-the-arts on DeepMind Control tasks with different visual distractions.",
        "author": "Rui Yang, Jie Wang, Zijie Geng, Mingxuan Ye, Shuiwang Ji, Bin Li, Feng Wu",
        "link": "http://arxiv.org/abs/2205.10218v3",
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "additionl_info": "Accepted to KDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Proofs",
                "content": "\n\\begin{theorem}\n\t\\label{app-thm:bd}\n\tLet $\\Phi:\\mathcal{O}\\to\\mathcal{Z}$ be a $T$-level representation, $V^e_*:\\mathcal{O}\\to\\mathbb{R}$ be the optimal value function in the environment $e\\in\\mathcal{E}$, and $\\Bar{V}^e_*:\\mathcal{Z}\\to\\mathbb{R}$ be the optimal value function on the latent representation space, built on top of the representation $\\Phi$. Let $\\bar{r}$ be a bound of the reward space, i.e., $|r|<\\bar{r}$ for any $r\\in\\mathcal{R}$.\n\tThen we have\n\t\\begin{align*}\n\t\t0\\le V^e_*(o)-\\Bar{V}^e_*\\circ\\Phi(o)\\le\\frac{2\\gamma^T}{1-\\gamma}\\bar{r}\n\t\\end{align*}\n\tfor any $o\\in\\mathcal{O}$ and $e\\in\\mathcal{E}$.\n\\end{theorem}\n\\begin{proof}\n\tBy the definition of optimal value function, obviously we have $\\Bar{V}^e_*\\circ\\Phi(o)\\le V^e_*(o)$.\n\tIt suffices to show that $V^e_*(o)-\\Bar{V}^e_*\\circ\\Phi(o)\\le\\frac{\\gamma^T}{1-\\gamma}\\bar{r}$.\n\tWithout loss of generality, let $\\pi_*$ be an optimal policy such that $\\pi_*(\\cdot|o)=\\pi_*(\\cdot|o')$ if $[o]_s=[o']_s$. \n\tLet $\\hat{\\pi}$ be any policy built on top of $\\Phi$.\n\tThen we have\n\t\\begin{align*}\n\t\t&V^e_*(o)-\\Bar{V}^e_*\\circ\\Phi(o)\\le \\mathbb{E}_{\\pi_*}^e\\left[\\sum_{t=0}^\\infty\\gamma^tR_{t+1}\\right]-\\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^\\infty\\gamma^tR_{t+1}\\right]\\\\\n\t\t\\le& \\mathbb{E}_{\\pi_*}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right] - \\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right]\\\\\n\t\t&+ \\sum_{t=T}^\\infty \\gamma^t \\left|\\mathbb{E}_{\\pi_*}^e\\left[R_{t+1}\\right]-\\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[R_{t+1}\\right]\\right|\\\\\n\t\t\\le& \\mathbb{E}_{\\pi_*}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right] - \\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right]+ 2\\sum_{t=T}^\\infty \\gamma^t \\bar{r}\\\\\n\t\t\\le& \\mathbb{E}_{\\pi_*}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right] - \\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right] +\\frac{2\\gamma^T}{1-\\gamma}\\bar{r}.\n\t\\end{align*}\n\tWe are now to show that there exists a $\\hat{\\pi}$ such that \\begin{align*}\n\t\t\\mathbb{E}_{\\pi_*}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right] = \\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right].\n\t\\end{align*}\n\tFor any two observations $o,o'\\in\\mathcal{O}$ such that $\\Phi(o)=\\Phi(o')$, since $\\Phi$ is a $T$-level reward sequence representation, the RSDs $p(\\mathbf{r}|o,\\mathbf{a})=p(\\mathbf{r}|o',\\mathbf{a})$.\n\tThus we have \n\t\\begin{align*}\n\t\t\\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}|O_0=o\\right]=\\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}|O_0=o'\\right]\n\t\\end{align*} for any $\\hat{\\pi}$.\n\tDefine $\\hat{\\pi}:\\mathcal{Z}\\to\\Delta(\\mathcal{R}^T)$ as $\\hat{\\pi}(z)=\\pi_*(\\bar{o})$, where $\\bar{o}$ is an representative observation such that $\\Phi(\\bar{o})=z$. \n\tThen we have\n\t\\begin{align*}\n\t\t\\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}|O_0=o\\right]=&\\mathbb{E}_{\\hat{\\pi}\\circ\\Phi}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}|O_0=\\bar{o}\\right]\\\\\n\t\t=&\\mathbb{E}_{\\pi_*}^e\\left[\\sum_{t=0}^{T-1}\\gamma^tR_{t+1}\\right],\n\t\\end{align*}\n\twhich completes the proof.\n\\end{proof}\n\n\\begin{theorem}\n\t\\label{app-thm:cf}\n\tA representation $\\Phi:\\mathcal{O}\\to\\mathcal{Z}$ is a $T$-level reward sequence representation if and only if there exits a predictor $\\Psi$ such that\n\t\\begin{align*}\n\t\t\\Psi(\\bm{\\omega};\\Phi(o),\\mathbf{a})=\\varphi_{\\mathbf{R}|o,\\mathbf{a}}(\\bm{\\omega})=\\mathbb{E}_{\\mathbf{R}\\sim p(\\cdot|o,\\mathbf{a})}\\left[e^{i\\langle\\bm{\\omega},\\mathbf{R}\\rangle}\\right],\n\t\\end{align*}\n\tfor all $\\bm{w}\\in\\mathbb{R}^{T},o\\in\\mathcal{O}$ and $\\mathbf{a}\\in\\mathcal{A}^T$.\n\\end{theorem}\n\\begin{proof}\n\tBy definition, $\\Phi$ is a $T$-level reward sequence representation if and only if there exists $f$ such that\n\t\\begin{align*}\n\t\tf(\\mathbf{r};\\Phi(o),\\mathbf{a})=p(\\mathbf{r}|o,\\mathbf{a}).\n\t\\end{align*}\n\tLet $\\mathcal{M}$ be the set of mappings from $\\mathcal{A}^T$ to $\\Delta(\\mathcal{R}^T)$. Then $\\Phi$ is a $T$-level reward sequence representation if and only if there exists $\\hat{f}:\\mathcal{Z}\\to\\mathcal{M}$ such that $\\hat{f}\\circ\\Phi (o)=M_o$ for any $o\\in\\mathcal{O}$, where $M_o(\\mathbf{a})=p(\\cdot|o,\\mathbf{a})$ for any $\\mathbf{a}\\in\\mathcal{A}^T$.\n\tSince a characteristic function uniquely determines a distribution, and vice versa, there exists a bijection between the distributions $p(\\cdot|o,\\mathbf{a})$ and their corresponding characteristic functions $\\varphi_{\\mathbf{R}|o,\\mathbf{a}}(\\bm{\\omega})=\\mathbb{E}_{\\mathbf{R}\\sim p(\\cdot|o,\\mathbf{a})}\\left[e^{i\\langle\\bm{\\omega},\\mathbf{R}\\rangle}\\right]$.\n\tAs a result, $\\Phi$ is a $T$-level reward sequence representation if and only if there exists $\\Tilde{f}:\\mathcal{Z}\\to\\mathcal{M}$ such that $\\Tilde{f}\\circ\\Phi (o)=\\Tilde{M}_o$ for any $o\\in\\mathcal{O}$, where $M_o(\\mathbf{a})=\\varphi_{\\mathbf{R}|o,\\mathbf{a}}(\\cdot)$ for any $\\mathbf{a}\\in\\mathcal{A}^T$, which is equivalent to what we want.\n\\end{proof}\n\n"
            },
            "section 2": {
                "name": "Research Methods",
                "content": "\n\\label{app-rm}\n\n",
                "subsection 2.1": {
                    "name": "Implementation Details",
                    "content": "\n\n\\paragraph{Dynamic Background Distractions}\nWe follow the dynamic background settings of Distracting Control Suite (DCS)~\\citep{corr/abs-2101-02722}.\nWe take the 2 first videos from the DAVIS 2017 training set and randomly sample a scene and a frame from those at the start of every episode. In the RL training process, we alternate the 2 videos. Moreover, we set $\\beta_{\\text{bg}}=1.0$, which means we use the distracting background instead of original skybox.\nWe then apply the 30 videos from the DAVIS 2017 validation dataset for evaluation.\n\n\\paragraph{Dynamic Color Distractions}\nIn dynamic color settings of DCS~\\citep{corr/abs-2101-02722}, the color is sampled uniformly per channel $x_{0} \\sim \\mathcal{U}(x-\\beta, x+\\beta)$ at the start of each episode, where $x$ is the original color in DCS, and $\\beta$ is a hyperparameter. We enable the dynamic setting that the color $x_t$ changes to $x_{t+1} = \\text{clip} (\\hat{x}_{t+1}, x_t - \\beta, x_t + \\beta)$, where $\\hat{x}_{t+1} \\sim \\mathcal{N}(x_t, 0.03\\cdot\\beta)$. During the training process, we use $\\beta_1=0.1$, $\\beta_2=0.2$ and evaluate agents with $\\beta_{\\text{test}}=0.5$.\n\n\\paragraph{Network Details}\n\\label{app-nd}\nA shared pixel encoder utilizes four convolutional layers using 3 Ã— 3 kernels and 32 filters with an stride of 2 for the first layer and 1 for others. Rectified linear unit (ReLU) activations are applied after each convolution. Thereafter, a 50 dimensional output dense layer normalized by LayerNorm is applied with a $\\tanh$ activation. Both critic and actor networks are parametrized with 3 fully connected layers using ReLU activations up until the last layer. The output dimension of these hidden layers is 1024.  The pixel encoder weights are shared for the critic and the actor, and gradients of the encoder are not computed through the actor optimizer.\nMoreover, we use the random cropping for image pre-processing proposed by DrQ and RAD~\\citep{laskin2020reinforcement} as a weak augmentation without prior knowledge of test environments.\nFor the predictor of characteristic functions, we use 3 additional layers and ReLU after the pixel encoder.\n\n\n\n\\paragraph{Auxiliary Loss in CRESP}\n\\label{app-pred-loss}\nFor all our experiments, we estimate the auxiliary loss $\\mathcal{L}^{\\mathcal{N}}_{\\mathcal{D}}(\\Phi,\\Psi)$ to learn representations. For the computation of $\\mathbb{E}_{\\Omega\\sim\\mathcal{N}}\\left[\\cdot\\right]$ in $\\mathcal{L}^{\\mathcal{N}}_{\\mathcal{D}}(\\Phi,\\Psi)$, we take an ablation study of the sample number $\\kappa$ of $\\Omega$ in the left of Figure~\\ref{fig-k}. The results show that $\\kappa=256$ performs better than others. Then, for the choice of the distribution $\\mathcal{N}$, we also parameterize this distribution to maximize the auxiliary loss (N-max) or minimize (N-min). The results with batch size 128 in the right of Figure~\\ref{fig-k} indicate that PN-max is not stable and PN-min is similar to CRESP with the standard Gaussian distribution. For computational efficiency, we choose the standard Gaussian distribution in all experiments.\n\nFurthermore, we list the hyperparameters in Table~\\ref{table-hypp}. \n\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Additional Results",
                    "content": "\n\\label{app-ar}\n\n\n\n\n\n\n\\paragraph{Additional Curves}\nIn Figure~\\ref{fig-result-dc} we show learning curves under the default settings on 6 different environments from DCS with dynamic color distractions.\n\n\\paragraph{Additional Visualizations}\nIn addition to Figure~\\ref{fig-tsne-cs}, we also visualize the representations of CRESP and DrQ in cartpole-swingup task via t-SNE. We leverage 500 observations from 2 environments with different dynamic backgrounds. All labels are generated by KMeans with the original states as inputs.\n\n\n\n",
                    "subsubsection 2.2.1": {
                        "name": "Reward Sequence Distributions vs Distributions of Sum of Reward Sequences",
                        "content": "\n\\label{app-as}\nIn Section~\\ref{subsec-4.1}, we introduce the notion of reward sequence distributions (RSDs) to determine task relevance. We can also identify the task relevance---learning $T$-level representations---via the distributions of sum of reward sequences. Therefore, we compare the performance of learning reward sequence distributions (CRESP) with that of learning distributions of sum of reward sequences (CRESP-Sum). \nMoreover, we evaluate the method to estimate the expected sums of reward sequences (RP-Sum). We adopt the same hyperparameters and ablate the reward length $T$. \n\nIn Table~\\ref{table-sum}, we boldface the results that have highest means. The average performances of different reward lengths of RP-Sum are lower than that of RP. This result shows that the high-dimensional targets can provide more helpful information than one-dimensional targets, which is similar to the effectiveness of knowledge distillation by a soft target distribution.\nMoreover, CRESP has higher average performance than CRESP-Sum, outperforming RP and RP-Sum, which empirically demonstrates that learning distributions provides benefits for representation learning in the deep RL setting.\n\n\n"
                    }
                },
                "subsection 2.3": {
                    "name": "Code",
                    "content": "\nWe implement all of our codes in Python version 3.8 and make the code available online~\\footnote{\\url{https://github.com/MIRALab-USTC/RL-CRESP}}.\n% We also present the detailed configurations.\nWe used NVIDIA GeForce RTX 2080 Ti GPUs for all experiments. Each trials of our method was trained for 20 hours on average.\n\n\n"
                }
            }
        },
        "tables": {
            "table-hypp": "\\begin{table}[h]\n\t\\caption{Hyperparameters in the Distracting Control Suite.}\n\t\\vskip -0.1in\n\t\\label{table-hypp}\n\t\\centering\n\t\\begin{tabular}{l|l}\n\t\t\\toprule\n\t\t\\textbf{Hyperparameter}             & \\textbf{Setting} \\\\\n\t\t\\midrule\n\t\tOptimizer                           & Adam \\\\\n\t\tDiscount $\\gamma$                   & 0.99 \\\\\n\t\tLearning rate                       & 0.0005 \\\\\n\t\tNumber of batch size                & 256 \\\\\n\t\tNumber of hidden layers             & 2 \\\\\n\t\tNumber of hidden units per layer    & 1024 \\\\\n\t\tReplay buffer size                  & 100,000 \\\\\n\t\tInitial steps                       & 1000 \\\\\n\t\tTarget smoothing coefficient $\\tau$ & 0.01 \\\\\n\t\tCritic target update frequency      & 2 \\\\\n\t\tActor update frequency              & 1 \\\\\n\t\tActor log stddev bounds             & [-5, 2] \\\\\n\t\tInitial temperature $\\alpha$        & 0.1 \\\\\n\t\t\\midrule\n\t\t\\emph{Hyperparameters of CRESP}     & \\\\\n\t\t\\quad Gaussian distribution $\\mathcal{N}$ & $\\mathcal{N}(\\textbf{0}, \\textbf{1})$\\\\\n\t\t\\quad Sample number $\\kappa$ from $\\mathcal{N}$ & 256 \\\\\n\t\t\\quad Discount of reward sequences & 0.8 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}",
            "table-sum": "\\begin{table}\n\t\\caption{Performance comparison with 3 seeds on cartpole-swingup with dynamic backgrounds at 500K steps. $T$ is the reward length for the ablation study.}\n\t\\vskip -0.1in\n\t\\label{table-sum}\n\t\\centering\n\t\\begin{tabular}{c|cccc}\n\t\t\\toprule\n\t\tR Length & RP & RP-Sum & CRESP & CRESP-Sum \\\\\n\t\t\\midrule\n\t\t$T=1$  & $\\textbf{625} \\pm \\textbf{85}$ & $\\textbf{625} \\pm \\textbf{85}$ & $616 \\pm 155$ & $616 \\pm 155$\\\\\n\t\t$T=3$  & $645 \\pm 55$ & $631 \\pm 35$ & $\\textbf{666} \\pm \\textbf{42}$ & $610 \\pm 88$\\\\\n\t\t$T=5$  & $575 \\pm 111$ & $610 \\pm 85$ & $\\textbf{687} \\pm \\textbf{50}$ & $629 \\pm 61$\\\\\n\t\t$T=7$  & $654 \\pm 97$ & $599 \\pm 136$ & $\\textbf{667} \\pm \\textbf{43}$ & $639 \\pm 61$\\\\\n\t\t\\midrule\n\t\tAverage  & $625 \\pm 94$ & $613 \\pm 96$ & $\\textbf{658} \\pm \\textbf{98}$ & $623 \\pm 100$\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\vskip -0.1in\n\\end{table}"
        },
        "figures": {
            "fig-k": "\\begin{figure}[h]\n\t\\centering\n\t\\hspace{-0.1in}\n\t\\includegraphics[width=2.4cm]{b-cs-as-K.pdf}\n\t\\hskip 0.1in\n\t\\includegraphics[width=2.4cm]{b-cs-as-N.pdf}\n\t\\vskip -0.1in\n\t\\caption{Ablation studies of CRESP with batch size 128 and 2 random seeds on cartpole-swingup with dynamic backgrounds. In the left, $\\kappa$ is sample number of $\\Omega$ from $\\mathcal{N}$. In the right, N is CRESP with the standard Gaussian distribution. N-max and N-min are CRESP with Gaussian distributions that maximize and minimize the auxiliary loss, respectively.}\n\t\\Description{The generalization performance of CRESP on unseen background dynamics.}\n\t\\label{fig-k}\n\t% \\vskip -0.2in\n\\end{figure}",
            "fig-tsne-cs": "\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=8.6cm]{new-cs-tsne.pdf}\n\t\\vskip -0.1in\n\t\\caption{t-SNE visualization of latent representations learned by CRESP (left), DrQ (center), and CURL (right) in cartpole-swingup with dynamic backgrounds.}\n\t\\Description{t-SNE of latent spaces learned by CRESP and Drq.}\n\t\\label{fig-tsne-cs}\n\t\\vskip -0.1in\n\\end{figure}",
            "fig-result-dc": "\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[width=2.4cm]{co-bc.pdf}\n\t\\hskip 0.15in\n\t\\includegraphics[width=2.4cm]{co-cs.pdf}\n\t\\hskip 0.15in\n\t\\includegraphics[width=2.4cm]{co-cr.pdf}\n\t\n\t\\includegraphics[width=2.4cm]{co-fs.pdf}\n\t\\hskip 0.15in\n\t\\includegraphics[width=2.4cm]{co-re.pdf}\n\t\\hskip 0.15in\n\t\\includegraphics[width=2.4cm]{co-ww.pdf}\n\t\n\t\\includegraphics[width=7.4cm]{b-legend.pdf}\n\t\\vskip -0.1in\n\t\\caption{Learning curves of six methods on six tasks with dynamic color distractions for 500K environment steps.}\n\t\\Description{The performances of different algorithms on unseen environments with dynamic color distractions.}\n\t\\vskip -0.1in\n\t\\label{fig-result-dc}\n\\end{figure}"
        },
        "git_link": "https://github.com/MIRALab-USTC/RL-CRESP"
    }
}