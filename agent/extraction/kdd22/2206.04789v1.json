{
    "meta_info": {
        "title": "Comprehensive Fair Meta-learned Recommender System",
        "abstract": "In recommender systems, one common challenge is the cold-start problem, where\ninteractions are very limited for fresh users in the systems. To address this\nchallenge, recently, many works introduce the meta-optimization idea into the\nrecommendation scenarios, i.e. learning to learn the user preference by only a\nfew past interaction items. The core idea is to learn global shared\nmeta-initialization parameters for all users and rapidly adapt them into local\nparameters for each user respectively. They aim at deriving general knowledge\nacross preference learning of various users, so as to rapidly adapt to the\nfuture new user with the learned prior and a small amount of training data.\nHowever, previous works have shown that recommender systems are generally\nvulnerable to bias and unfairness. Despite the success of meta-learning at\nimproving the recommendation performance with cold-start, the fairness issues\nare largely overlooked. In this paper, we propose a comprehensive fair\nmeta-learning framework, named CLOVER, for ensuring the fairness of\nmeta-learned recommendation models. We systematically study three kinds of\nfairness - individual fairness, counterfactual fairness, and group fairness in\nthe recommender systems, and propose to satisfy all three kinds via a\nmulti-task adversarial learning scheme. Our framework offers a generic training\nparadigm that is applicable to different meta-learned recommender systems. We\ndemonstrate the effectiveness of CLOVER on the representative meta-learned user\npreference estimator on three real-world data sets. Empirical results show that\nCLOVER achieves comprehensive fairness without deteriorating the overall\ncold-start recommendation performance.",
        "author": "Tianxin Wei, Jingrui He",
        "link": "http://arxiv.org/abs/2206.04789v1",
        "category": [
            "cs.IR",
            "cs.LG"
        ],
        "additionl_info": "Accepted to SIGKDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nPersonalized recommender systems have been widely used for mining user preference in various web services, such as e-commerce \\cite{ying2018graph}, search engines \\cite{shen2005implicit}, and social media \\cite{fan2019graph}, which largely relieve the information overload issues. In practice, the common challenge that most recommender systems face is the cold-start problem. When data is scarce or fresh users appear frequently, the recommender systems must adapt rapidly. Over the past years, meta-learning methods \\cite{lu2020meta,wei2020fast,dong2020mamo,lee2019melu} have been widely used to create recommender systems that learn quickly from the limited data with computationally affordable fine-tuning. The core idea of meta-learning is learning-to-learn, i.e., learning to solve the training tasks well with the generalization ability on future tasks. Meta-learned recommender systems are trained with an interleaving training procedure, comprised of inner loop update that fine-tunes on each user and outer loop update that produces the initialization of all users. However, recommender systems are generally vulnerable to bias and unfairness \\cite{li2021towards,wu2021learning}. Although meta-learning could better help fresh users to find potentially interesting items, the understanding and mitigation of fairness in this context is largely under-explored.\n% figure\n\n\n\nRecently, there has been growing attention on fairness considerations in the recommender systems \\cite{beutel2019fairness, beigi2020privacy, wu2021learning, ge2021towards, li2021user}. The underlying unfairness issue in recommendation will hurt users’ or providers’ satisfaction as well as their interests in the platforms. Therefore, it is crucial to address the potential unfairness problems. The fairness measures in the recommender systems can be classified as follows. (1) \\textbf{Individual fairness} refers to the protection of the sensitive attributes in the user modeling process against adversary attackers. There have been works \\cite{beigi2020privacy, wu2021learning} relying on adversarial learning techniques \\cite{goodfellow2014generative} that aim to match the conditional distribution of outputs given each sensitive attribute value. (2) \\textbf{Counterfactual fairness} requires the recommendation results for a user to be unchanged in the counterfactual world where the user’s features remain the same except for certain sensitive features specified by the user. \\citet{li2021towards} first explored this concept by generating feature-independent user embeddings to satisfy the counterfactual fairness requirements in the recommendation. (3) \\textbf{Group fairness} aims to make the recommender systems not favor a particular demographic group over others. This fairness requirement is usually achieved by specific fairness regularization terms \\cite{yao2017beyond, zhu2018fairness}. However, these methods are not designed to address the cold-start problem, and may render sub-optimal performance if used in this scenario. Furthermore, these methods only consider one specific kind of fairness. In other words, the effects and connections among different kinds of fairness in the recommender systems have not been studied.\n\nTo address these limitations, in this paper, we propose a novel comprehensive fair meta-learned recommender framework, {\\it \\model}, for ensuring the fairness of cold-start recommendation models. It offers a general training paradigm that is applicable to any meta-learned recommender system. We formulate various fairness constraints as the adversarial learning problem with two main components: the unfairness discriminator that seeks to infer users’ sensitive-attribute information, and the fair results generator that prevents the discriminator from inferring the sensitive information. We systematically illustrate the three kinds of fairness - individual fairness, counterfactual fairness, and group fairness, in the recommender system, and improve them in a unified manner via the multi-task adversarial learning scheme. Figure \\ref{fig:diff} shows the difference between the existing fair recommender systems and the meta-learned fair recommender systems with cold-start, where we need to consider imposing fairness in both the inner loop and outer loop. To summarize, the major contributions of this paper are outlined as follows:\n\\begin{itemize}[leftmargin=*]\n    \\item We systematically illustrate the comprehensive fairness issues in the recommender systems and formulate enhancing comprehensive fairness as the multi-task adversarial learning problem.\n    \\item We propose \\model, which is carefully designed to impose fairness in the framework of meta-learning with the interleaving training procedure. To the best of our knowledge, we are the first to explore the fair meta-learned recommender system.\n    \\item We demonstrate \\model~ on the representative meta-learned user preference estimator on three real-world data sets. Empirical results show that \\model~ achieves comprehensive fairness without deteriorating the overall cold-start recommendation performance.\n\\end{itemize}\nThe rest of the paper is organized as follows. We show the preliminary definition in Section \\ref{section:preliminary} and introduce the proposed \\model~ in Section \\ref{section:method}. Then we present the experimental results in Section \\ref{section:experiments}. Section \\ref{section:related work} briefly discusses the existing work. In the end, we conclude the paper in Section \\ref{section:conclusion}.\n\n\n\n% These problems are even more severe in the cold-start scenario where interactions are very limited for new users in the system, as revealed by \\cite{yao2017beyond}. In the previous few years, meta-learning techniques \\citep{lu2020meta,wei2020fast,dong2020mamo,lee2019melu} have been proved successful in cold-start recsys. The goal of these approaches is to learn proper model initialization so that the recsys can be updated with only a few observed interactions and quickly applied to fresh users. Despite their popularity, learning fairly with unbiased outcomes and representation is yet under-explored in the meta-learning recommendation. In this work, we propose an adversarial fairness-aware meta-learning recommendation debiasing method. The key idea is to formulate this problem as an adversarial learning problem with two main components: the sensitive attribute inference attacker, and the personalized meta-learning recommender. The attacker seeks to infer users’ sensitive-attribute information according to the user embedding. The recommender aims to extract users’ interest while preventing the attacker from inferring the sensitive information. We will conduct experiments on public real-world datasets including MovieLens-1M to verify the capability of our method of preserving the quality of recommendation service and protecting users from discrimination.\n\n% There has been very limited work on the unfairness issue in the recommendation task, and the fairness definitions and metrics the authors have used are quite different from each other. In this project, we focus on the notion of individual fairness introduced by \\cite{dwork2012fairness}, which refers to not utilizing sensitive features in the user modeling process. Besides, as shown in the typical framework of recsys in Fig \\ref{fig:framework}, unfairness issue usually first happens at the beginning stage of the recsys, where the training data we have on hand may encode social bias. With the development of the whole system and the aggregation of user interaction data, such as user click history, the unfairness issue may echo or even reinforce the bias caused at the beginning.\n% Then, a natural question is: How can we achieve individual fairness at the beginning stage where the available data is sparse (i.e. the cold-start scenario)?\n% In this project, we are trying to answer this question.\\section{Preliminary}\n\\label{section:preliminary}\nIn this section, we introduce the cold-start problem and the fairness considerations in the recommender systems. \n",
                "subsection 1.1": {
                    "name": "Problem Statement",
                    "content": "\n\\label{subsection:state}\n% Let $U_e=\\{u^e_1,u^e_2,...u^e_n\\}$ and $I_e=\\{i^e_1,i^e_2,...i^e_m\\}$\\he{It seems that $U_e$ and $I_e$ are never used in the rest of the paper.} denote the sets of existing users and items in the system respectively, where $n$ is the number of users, and $m$ is the number of items. \n\n\nGiven a user $u$ with the profile $x_{u}$ and limited \\textit{existing} rated items $I_{u}^e$, each item $i\\in I_{u}^e$ is associated with a description profile $p_i$ and the corresponding rating $y_{ui}$. Our goal is to predict the rating $y_{ui^q}$ by user $u$ for the new \\textit{query} item $i^q \\in I_u^q$. Here, $I_u^q$ stands for the items needed to predict. There are two data sets for each user, one for fine-tuning, the other for testing. We define the existing fine-tuning data for each user $u$ as $D_u^e= \\{x_u,p_i,y_{ui}\\}_{i \\in I_u^e}$, and query data as $D_u^q=\\{x_u,p_i,y_{ui}\\}_{i \\in I_u^q}$. Following the previous works \\cite{lee2019melu,dong2020mamo,wei2020fast}, we treat the fast adaption on each user $u$ as a task $t_{u}$:\n\\begin{equation}\n    t_{u}: (\\theta, D_{u}^e) \\xrightarrow{} \\theta_{u},\n\\end{equation}\nwhere $\\theta$ is the meta-model learned from the meta-learning recommender system, and $\\theta_{u}$ is the personalized model fine-tuned on $D_u^e$ for each user. Cold-start recommendation focuses on new users arriving after the training stage. Let $U^f$ represent the sets of \\textit{fresh} users that will arrive in the system. We aim to learn a proper $\\theta$ on existing users that generalizes well on new users $U^f$, i.e., solving the fast adaption tasks.\n\n\n% Cold-start recommendation focuses on new users arriving after the training stage. Let $U^f$ represent the sets of \\textit{fresh} users that will arrive in the system. Given a new user $u^f$ with his/her profile $x_{u^f}$ and limited \\textit{existing} rated items $I_{u^f}^e$. Each item $i\\in I_{u^f}^e$ is associated with a description profile $p_i$ and the corresponding rating $y_{u^fi}$.\\he{This is not a sentence.} Our goal is to predict the rating $y_{ui^q}$ by user $u$ for a new \\textit{query} item $i^q \\in I_u^q$. There are two data sets for each user, one for fine-tuning, the other for testing. We define the existing fine-tuning data for each user $u$ as $D_u^e= \\{x_u,p_i,y_{ui}\\}_{i \\in I_u^e}$, and testing query data as $D_u^q=\\{x_u,p_i,y_{ui}\\}_{i \\in I_u^q}$. Here, $I_u^q$ stands for the items needed to predict. Following the previous works \\cite{lee2019melu,dong2020mamo,wei2020fast}, we treat the fast adaption on each new user $u^f$ as a task $t_{u^f}$:\n% \\begin{equation}\n%     t_{u^f}: (\\theta, D_{u^f}^e) \\xrightarrow{} \\theta_{u^f},\n% \\end{equation}\n% where $\\theta$ is the meta-model learned from the meta-learning recommender system, and $\\theta_{u}$\\he{$\\theta_{u^f}$?} is the personalized model fine-tuned on $D_u^e$ for each user.\n\n\n"
                },
                "subsection 1.2": {
                    "name": "User-oriented Fairness",
                    "content": "\n\\label{subsection:fairness}\nNext, we give the comprehensive fairness definition of a recommender system. A fair recommender system should reflect the personalized preferences of users while being fair with respect to sensitive information. Specifically, the learned representation and recommender results should not expose the sensitive information that correlates with the users or show discrimination towards any individual or group of users. The fairness in the recommender system can be reflected from several different perspectives, which are summarized below.\n%, which means that the results of the fair recommender should hardly be affected by such information.\n\n\\textbf{Individual Fairness} \\cite{beigi2020privacy,wu2021learning}. Here, the fairness requirements refer to not exposing sensitive feature in the user modeling process against attackers. We define such fairness formally as:\n\\begin{equation}\n    IF=\\frac{1}{|U^f|}\\max_g\\sum_{u\\in U^f} M(\\hat{a}_u=g(e_u), a_u)\n\\end{equation}\nwhere $e_u$ is the representation of user $u$, $a_u$ is the sensitive information of $u$, $g$ is the user representation attacker, aiming to predict the sensitive information from the user representation, and $M$ is the evaluation metric of the prediction performance. This definition requires the user modeling network to defend against any possible attacker that tries to hack the sensitive information. A lower $IF$ score indicates a fairer recommender.\n\n\\textbf{Counterfactual Fairness} \\cite{li2021towards}. This is a causal-based fairness notion \\cite{kusner2017counterfactual}. It explicitly requires that for any individual, the predicted result of the learning system should not discriminate towards the users' sensitive information. We will imagine a counterfactual world here in which we only make an intervention on the user’s sensitive information while making other independent features unchanged, and we expect the prediction to be the same as in the real world. We first refer to the definition of counterfactually fair recommendation in \\cite{li2021towards}. \n\\begin{definition}[Counterfactually fair recommendation \\cite{li2021towards}]\\label{counterfactual fairness}\nA recommender model is counterfactually fair if for any possible user $u$ with features $X=x$ and $A=a$:\n$$\nP\\left(L_{a} \\mid X=x, A=a\\right)=P\\left(L_{a^{\\prime}} \\mid X=x\n, A=a\\right)\n$$\nfor all $A$ and for any value $a^{\\prime}$ attainable by $A$, where $L$ denotes the recommendation results for user $u$, $A$ is the user's sensitive feature and $X$ are the features that are independent on $A$.\n\\end{definition}\nThis definition requires that for any possible user, sensitive information $A$ should not be a cause of the recommendation results. In our setting of explicit rating prediction, we can formally define the counterfactual fairness as:\n\\begin{equation}\n    CF=\\frac{1}{|U^f|}\\sum_{u\\in U^f} |R(L_{a_u}^u \\mid X=x, A=a_u)-R(L_{a^{\\prime}_u}^u \\mid X=x, A=a_u)|\n\\end{equation}\nwhere $R$ is the performance of the recommendation, in our case the MAE (mean absolute error) of the rating prediction. $L_{a_u}^u$ is the recommendation results of user $u$ with sensitive information $a_u$. For simplicity, we abbreviate $R(L_{a_u}^u \\mid X=x, A=a_u)$ into $R(u)$ in the following to represent the recommendation performance of user $u$. A lower $CF$ score indicates a fairer system.\n\n\n\\textbf{Group Fairness} \\cite{liu2022dual,fu2020fairness,li2021user}. Group fairness requires the protected groups to be treated similarly as the advantaged group in the recommender system. The recommender systems will achieve fairness when users with two different attribute values maintain the same recommendation performance. We consider grouping testing users as $A_1$ and $A_2$ based on their sensitive information. More formally, the group recommendation unfairness \\cite{fu2020fairness} is defined as follows:\n\\begin{equation}\n    GF=|\\frac{1}{|A_1|}\\sum_{u_1\\in A_1} R(u_1) - \\frac{1}{|A_2|}\\sum_{u_2\\in A_2} R(u_2)|\n\\end{equation}\nwhere notation $R$ is a metric that evaluates the recommendation performance, $|A_1|$ is the number of users in group $A_1$. Lower GF also represents better fairness performance.\n\n% \\he{Following the discussion of all the fairness measures suitable for the recommender system, you should introduce the notation of comprehensive fairness.}\n\nThese are all the desired properties of a fair recommender system. We regard the combination of these three requirements as comprehensive fairness and aim to design algorithms to improve comprehensive fairness simultaneously. In this work, we propose to design the comprehensive framework \\model~ to incorporate all these fairness requirements. For counterfactual fairness and group fairness, our definition here mainly focuses on the case of binary sensitive attributes. For sensitive attributes with multiple values, we can measure the fairness with the sum of the differences between all possible other values of the sensitive attribute. In addition, exploring the fairness issues within the combination of multiple sensitive attributes is left as the future work.\n\n% In the rest of this paper, we use gender as an example for the sensitive information, but our model can be easily extended to multiple kinds of sensitive information.\n\n"
                }
            },
            "section 2": {
                "name": "Proposed \\model\\ Framework",
                "content": "\n\\label{section:method}\nIn this section, we first briefly describe the training process of the meta-learned recommender system. Next, we introduce how to mitigate the comprehensive fairness issues in the recommender system by designing a novel multi-task adversarial learning component. Then, \\model~ is proposed to build a fair recommender system in the cold-start meta-learning scenario. Finally, we demonstrate how to instantiate our generic method on the representative meta-learned user preference estimator. Note that our framework can also be applied to other methods, which we'll show in the experiments.\n% , as shown in Figure \\ref{fig:frame}.\n\n\n",
                "subsection 2.1": {
                    "name": "Meta-learned Recommender System",
                    "content": "\n\\label{subsection:meta}\nThe core idea of the meta-learned recommender system is learning to solve the fast adaption task $t_u$ on new users. We follow the most representative meta-learning recommender framework \\cite{lee2019melu,dong2020mamo,wei2020fast} to show the workflow. To learn the parameters of a new task, the meta-model aims to provide a promising initialization by learning from various similar tasks. Then from the learned parameters, the meta-model can be further fine-tuned on the new task (user) with limited interactions to achieve personalized recommendation. The workflow of the meta-learned recommender system is shown in Figure \\ref{fig:meta}. The framework iteratively updates the meta-model according to the following procedure:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Inner loop.} For each task $t_u$, the framework initializes $\\theta_u$ with the latest parameters of the meta-model $\\theta$, and updates $\\theta_u$ according to the user's training data.\n    \\item \\textbf{Outer loop.} The framework updates the meta-model $\\theta$ by minimizing the recommendation loss of $\\theta_u$ regarding each user $u$ to provide a promising initialization for every user.\n    \\end{itemize}\n\nIn the inner loop, the model parameters regarding a user $u$ will be updated iteratively as follows:\n\\begin{equation}\n\\begin{split}\n    \\theta_u=\\theta_u-\\alpha\\nabla_{\\theta_u} L(f_{\\theta_u}, D_u^e),\\\\\n\\end{split}\n\\vspace{-0.2cm}\n\\end{equation}\nwhere $\\alpha$ is the learning rate of user parameter update, $L(f_{\\theta_u}, D_u^e)$ denotes the recommendation loss (e.g., the cross-entropy log loss \\cite{berg2017graph}) on data $D_u^e$. $f_{\\theta_u}$ suggests the loss is parametrized by parameter $\\theta_u$. The model parameter $\\theta_u$ of user $u$ is initialized by the  meta-model parameter $\\theta$. $D_u^e$ is the user's existing training data, as introduced in Section~\\ref{subsection:state}.\n\n\nIn the outer loop, the meta-model parameters will be updated by summing up all user $u$'s specific loss $L(\\theta_u, D_u^q)$ together to provide a promising initialization. $D^q_{u}$ is the query data set according to Section~\\ref{subsection:state}. Specifically, in each step of training, the parameters are updated as follows:\n\\begin{equation}\n    \\theta= \\theta-\\beta \\nabla_{\\theta} \\sum_{u\\in B}L(f_{\\theta_{u}},D_{u}^q)\n\\end{equation}\nwhere $B$ is a set of users involved in the batch, $\\beta$ is the learning rate of the meta-model parameters. In the following section, we abbreviate the notation $D$ in the above loss function for simplicity.\n\nAfter introducing the training process of the meta-model. We will then discuss the testing process. For evaluation of new user $u^f\\in U^f$, the framework will initialize the user model parameters with the meta-model $\\theta$, and the user model is then fine-tuned with the user's observed interaction data $D_{u^f}^e$. Finally, the fine-tuned model will be applied to make recommendations.\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Comprehensive Unfairness Mitigation",
                    "content": "\nNext, we will discuss how to comprehensively mitigate the fairness issues in the recommender systems. The naive solution is to add fairness regularization into the training process of the meta-learned recommender system. However, this approach faces the following flaws. \n\\begin{itemize}[leftmargin=*]\n    \\item In the inner loop update, the meta-model will be fine-tuned only according to the users' data. In this case, we are unable to calculate the fair regularization loss as it requires information from different groups of users. Thus, this kind of method is not appropriate for the meta-learned recommender system. \n    \\item A strong fair recommender system should not expose the sensitive information that correlates with the users in any situation. The unknown attackers are not explicitly considered in the regularization-based approaches and therefore the performance may vary; whereas adversarial learning methods train the recommender to defend against any possible model that tries to hack the sensitive information.\n\\end{itemize}\nIn this paper, we formulate comprehensive fairness as the multi-task adversarial learning problem, where different fairness requirements are associated with different adversarial learning objectives. Our basic idea is to train a recommender and the adversarial attacker jointly. The attacker seeks to optimize its model to predict the sensitive information. The recommender aims to extract users' actual preferences and fool the adversary in the meantime. In this way, the sensitive information will be removed and the recommender is encouraged to provide fair results. We model this as a min-max game between the recommender and the attack discriminator. We argue the different fairness requirements in \\ref{subsection:fairness} can be represented as yielding identical distribution concerning the sensitive attribute at either representation or prediction level. As shown in the Proposition 2 of \\cite{goodfellow2014generative}, if the generator $G$ and the discriminator $D$ have enough capacity, and at each iteration, the discriminator $D$ is allowed to reach its optimum and the generator $G$ is updated to fool the discriminator, then the generated data distribution will converge to the distribution of the real data. Therefore, in our model with the sensitive information discriminator, upon convergence, the distribution of generated recommender representation and prediction with distinct sensitive attributes will be the same. Next, we will detail how to implement it in the recommender system.\n\nSpecifically, the model parameters consist of two parts $\\theta=\\{\\theta^r, \\theta^d\\}$, where $\\theta^r$ is the parameter of the recommender model and $\\theta^d$ represents the sensitive information discriminator parameter. The objective function can be written as follows:\n\\begin{equation}\n    \\min_R \\max_D L=L(f_{\\theta^r, \\theta^d})=l_R(f_{\\theta^r})-l_D(f_{\\theta^r, \\theta^d})\n    \\vspace{-0.2cm}\n\\end{equation}\n\\begin{equation}\n    l_D(f_{\\theta^r, \\theta^d}))=l_D(a_{u},\\hat{a}_{u})=-\\frac{1}{N}\\sum_{u=1}^{N}\\sum_{c=1}^Ca_{u}^c log(\\hat{a}_{u}^c)\n\\end{equation}\nwhere $L$ consists of the recommender loss $l_R$ and the sensitive attribute discriminator loss $l_D$. $L(f_{\\theta^r, \\theta^d})$ implies the loss is parameterized by the $\\theta^r, \\theta^d$. Note that the recommender loss $l_R$ is unrelated with the discriminator parameter $\\theta^d$. $C$ is the number of classes of the sensitive attribute, $\\hat{a}_{u}^c$ is the probability that the sensitive attribute of user $u$ is predicted by the discriminator to be class $c$, $\\hat{a}_{u}$ is the predicted probability vector for user $u$, and $a_u$ is the one-hot vector of the sensitive information in which 1 denotes the ground-truth class. The recommender is asked to minimize the recommendation error while fooling the discriminator to generate fair results. Meanwhile, the discriminator is optimized to predict the sensitive information from the user as accurately as possible. In this work, we choose to optimize the recommender and discriminator simultaneously as it performs the best. The next problem is how to relate the adversarial learning problem with the comprehensive fairness mitigation. We argue that we can achieve comprehensive fairness by conducting both the representation and prediction level adversarial learning.\n\n\n\nFor the \\textbf{individual fairness}, from the definition in Subsection \\ref{subsection:fairness}, we can find that it requires the user modeling process of the recommender system to protect against the most powerful hacker from inferring the sensitive information. Therefore, we conduct the representation level adversarial learning which aims to generate the user embedding irrelevant to the sensitive information. More specifically, for a user $u$, the discriminator will predict the sensitive information according to the corresponding user representation:\n% \\begin{equation}\n%     \\hat{a}_u^g = g(e_u, E_g)\n% \\end{equation}\n\\begin{equation}\n    l_D^g = l_D(a_u, \\hat{a}_u=g(e_u, E_g))\n    \\vspace{-0.1cm}\n\\end{equation}\nwhere $g$ is the representation discriminator, $e_u$ is the user embedding of $u$. Different from the traditional adversarial game, we also input the external information $E_g$ for better performance. Here we concatenate the corresponding historical rating $y_{ui}$ with the user embedding as the input information. The benefits are two-fold: (1) Due to the external information $y_{ui}$ that the discriminator has access to, the recommender cannot hope to fully fool the discriminator, since the external rating information can give some insights of the users' sensitive information. In this way, the recommendation generator will focus more on removing the sensitive information of users. (2) Adding such external information requires the learned representation to be independent of the sensitive attributes conditioned on any values of $y$. It sets higher requirements for model learning and will be beneficial for comprehensive fairness, which we will show in the experiments section.\n\n\n\nAs for the \\textbf{counterfactual fairness}, we prove it can be achieved with the above representation level adversarial learning for individual fairness. The proof is in the following.\n\\begin{proposition}\nIf the adversarial game for individual fairness converges to the optimal solution, then the rating prediction recommender which leverages these representations will also satisfy the counterfactual fairness.\n\\end{proposition}\n\\textit{Proof.} The downstream recommender uses the representation $e_u$ for predicting the rating $\\hat{y}_{ui}$ of user $u$, thus forming a Markov chain $a_u \\rightarrow e_u \\rightarrow \\hat{y}_{ui}$ (user sensitive attribute $a_u$ won't affect the recommender prediction via item embedding $e_i$). As we discussed before in this section, if the adversarial learning converges to the optimal, the generated recommender results will be independent of the sensitive attribute, i.e., the mutual information between the sensitive attribute $a_u$ and the representation $e_u$ for any given user $u$ is zero: $I(a_u;e_u) = 0$. Using the properties of inequality and non-negativity of mutual information:\n\\begin{equation}\n    0 \\leq\t I(a_u; \\hat{y}_{ui}) \\leq I(a_u; e_u) \\And I(a_u; e_u)=0 \\Longrightarrow I(a_u; \\hat{y}_{ui})=0\n\\end{equation}\nTherefore, the prediction $\\hat{y}_{ui}$ for any given user $u$ is independent of the sensitive attribute $a_u$, and consequently, the downstream recommender satisfies the counterfactual fairness.$\\blacksquare$\n\n\n\n\nConcerning the \\textbf{group fairness}, it requires the recommendation performance of users to be identical between different groups. Here we consider the mean absolute error $MAE=|y_{ui}-\\hat{y}_{ui}|$. The goal can then be interpreted as to achieve the same $\\hat{y}_{ui}$ across groups given true rating value $y_{ui}$, which will be satisfied when the discriminator cannot predict the sensitive attribute with a high accuracy given $y_{ui}$ and $\\hat{y}_{ui}$. Thus we concatenate the predicted logits $\\hat{y}_{ui}$ and $y_{ui}$ as the input to carry out the prediction-level adversary game:\n% \\begin{equation}\n%     \\hat{a}_u^h = h(y_{ui}, \\hat{y}_{ui}, E_h)\n% \\end{equation}\n\\begin{equation}\n    l_D^h = l_D(a_u, \\hat{a}_u=h(y_{ui}, \\hat{y}_{ui}, E_h))\n    \\vspace{-0.1cm}\n\\end{equation}\nwhere $h$ is the prediction discriminator, $E_h$ is the external information for $h$. Here we regard the item embedding $e_i$ as the additional information. Except for the reasons mentioned above, the item data usually contain some sensitive information. For example, males tend to borrow some science fictions, while females may prefer the romance novels. In this way, we can remove the encoded bias in the item data.\n\nTo comprehensively mitigate the fairness issues, we perform multi-task learning with both representation and prediction level adversarial learning as follows:\n\\begin{equation}\n    l_D = \\lambda*l_D^g+\\gamma*l_D^h\n\\end{equation}\nwhere the hyper-parameters $\\lambda$ and $\\gamma$ are used to balance the contribution of these two losses. As $l_D$ will be further combined with recommendation loss $l_R$ for training, we use two hyper-parameters here to control the loss function more flexibly. The whole network structure is shown in the figure \\ref{fig:structure}.\n\n% Although MeLU can perform well on new users, this model has serious fairness problems. On the one hand, the attacker can easily infer the sensitive information of the user from the current user embedding. On the other hand, the model will rely on the sensitive data of the user for prediction, resulting in completely different prediction results for the users that are the same except for the sensitive information. Therefore, it is crucial to address the potential unfairness problems in the meta-learning recommender system.\n\n\n\n"
                },
                "subsection 2.3": {
                    "name": "Fair Meta-Learned Recommender System",
                    "content": "\nIn the following, we will discuss how to achieve fairness in the context of the meta-learned recommender system. As shown in Subsection \\ref{subsection:meta}, tackling the fairness problem in such a setting is challenging, since it is trained with a bi-level interleaving learning procedure, comprised of the inner loop update that fine-tunes on each user and outer loop update that produces the initialization of all users. Therefore, it remains elusive when and how fairness should be enhanced to strike a graceful balance between fairness, recommendation performance, and computational efficiency. About when to update, we consider the fairness enhancement from both the inner and outer loops perspectives. As for how to update, we denote the recommender task as $T_r$, and disentangle the adversarial game into two objectives $T_1$ and $T_2$. The three tasks can be shown as follows:\n\\begin{itemize}[leftmargin=*]\n    \\item $T_r$: the task of recommender loss minimization for $\\theta^r$, which is required for all models.\n    \\item $T_1$: the task of optimizing the discriminator $\\theta^d$ to predict the sensitive information.\n    \\item $T_2$: the task of updating the recommender $\\theta^r$ to generate fair results by fooling the discriminator.\n\\end{itemize}\nThen we will show when and how to update the meta-learned recommender system toward comprehensive fairness.\n\n\n\n\nIn the \\textbf{outer loop}, optimizing the loss on query data represents better performance on testing data after fine-tuning. We hope the meta-model can provide a general initialization such that the fine-tuned model of various users will yield fair results. Therefore, we need to consider both the $T_1$ and $T_2$ tasks. The optimization process of the objective can be formulated as:\n\\begin{equation}\n    \\theta^r= \\theta^r-\\beta \\nabla_{\\theta^r} \\sum_{u\\in B}L(f_{\\theta_{u}})\n\\vspace{-0.1cm}\n\\end{equation}\n\\begin{equation}\n    \\theta^d= \\theta^d+\\beta \\nabla_{\\theta^d} \\sum_{u\\in B}L(f_{\\theta_{u}})\n\\end{equation}\nwhere $\\theta_u=\\{\\theta_u^r,\\theta_u^d\\}$ is initialized by $\\theta=\\{\\theta^r,\\theta^d\\}$.\n% and it consists of the recommender $\\theta_u^r$, and the discriminator $\\theta_u^d$.\n\n\\begin{algorithm}[t]\n    % \\SetAlgoLined\n    \\caption{\\model: Comprehensive Fair Meta-learned Recommender System}\n    \\label{alg:fairmeta}\n    \\begin{flushleft}\n    \\textbf{Input:} Traing user distribution:$d(u)$; $\\alpha, \\beta$: learning rate hyper-parameters; $\\lambda$, $\\gamma$: adversarial learning trade-off. \\\\\n    \\end{flushleft}\n    \\begin{algorithmic}[1]\n    \n    \\STATE /* Training on the existing users */\n    \\STATE Randomly initialize $\\theta$, where $\\theta=\\{\\theta^r,\\theta^d\\}$;\n    \\STATE Denote $L=l_R-l_D=l_R-\\lambda*l^g_D-\\gamma*l^h_D$\n    \\WHILE{not converge}\n        \\STATE Sample batch of users $B\\sim d(u)$;\n        \\FOR{user $u$ in $B$}\n            \\STATE Set $\\theta^r_u,\\theta^d_u = \\theta^r,\\theta^d$; $\\theta_u=\\{\\theta^r_u,\\theta^d_u\\}$;\n            % \\STATE evaluate $\\nabla_{\\theta^r_u} L(f_{\\theta_u})$, $\\nabla_{\\theta^r_u} L(f_{\\theta_u})$;\n            \\STATE \n                Inner loop update:\n                \\begin{equation*}\n                \\begin{split}\n                &\\theta^r_u \\gets \\theta^r_u - \\alpha \\nabla_{\\theta^r_u} l_R(f_{\\theta_u^r}) \\\\\n                &\\theta^d_u \\gets \\theta^d_u + \\alpha \\nabla_{\\theta^d_u} L(f_{\\theta_u})\\\\\n                \\end{split}\n                \\end{equation*}\n            \n            % \\multiline{Local update: \n            % $\\theta_2^i \\gets \\theta_2^i - \\alpha \\nabla_{\\theta_2^i} \\mathcal{L}_{i}(f_{\\theta_1, \\theta_2^i, \\theta_3^i, \\theta_4^i})$ \\\\\n            % $\\mspace{163mu}\\theta_3^i \\gets \\theta_3^i + \\alpha \\nabla_{\\theta_3^i} \\mathcal{L}_{i}(f_{\\theta_1, \\theta_2^i, \\theta_3^i, \\theta_4^i})$ \\\\\n            % $\\mspace{163mu}\\theta_4^i \\gets \\theta_4^i - \\alpha \\nabla_{\\theta_4^i} \\mathcal{L}_{i}(f_{\\theta_1, \\theta_2^i, \\theta_3^i, \\theta_4^i})$\n            % }\n            \n        \\ENDFOR\n        \\STATE \n        Outer loop update:\n        \\begin{equation*}\n            \\begin{split}\n                & \\theta^r \\gets \\theta^r - \\beta \\sum_{u \\in B} \\nabla_{\\theta^r} L(f_{\\theta_u}) \\\\\n                & \\theta^d \\gets \\theta^d + \\beta \\sum_{u \\in B} \\nabla_{\\theta^d} L(f_{\\theta_u}) \\\\\n            \\end{split}\n        \\end{equation*}\n        %     \\multiline{Global update:\n        %  $\\theta_1 \\gets \\theta_1 - \\beta \\sum_{i \\in B} \\nabla_{\\theta_1} \\mathcal{L}_{i}(f_{\\theta_1, \\theta_2^i,\\theta_3^i, \\theta_4^i})$ \\\\\n        % $\\mspace{140mu} \\theta_2 \\gets \\theta_2 - \\beta \\sum_{i \\in B} \\nabla_{\\theta_2} \\mathcal{L}_{i}(f_{\\theta_1, \\theta_2^i,\\theta_3^i, \\theta_4^i})$ \\\\\n        % $\\mspace{140mu} \\theta_3 \\gets \\theta_3 + \\beta \\sum_{i \\in B} \\nabla_{\\theta_3} \\mathcal{L}_{i}(f_{\\theta_1, \\theta_2^i,\\theta_3^i, \\theta_4^i})$ \\\\\n        % $\\mspace{140mu} \\theta_4 \\gets \\theta_4 - \\beta \\sum_{i \\in B} \\nabla_{\\theta_4} \\mathcal{L}_{i}(f_{\\theta_1, \\theta_2^i,\\theta_3^i, \\theta_4^i})$\n        % }\n    \\ENDWHILE\n    % \\STATE \n    \\STATE /* Testing on the new users */\n    \\FOR{user ${u^f}$ in $U^f$}\n    \\STATE \n                Finetune:\n                \\begin{equation*}\n                \\begin{split}\n                &\\theta^r_{u^f} \\gets \\theta^r_{u^f} - \\alpha \\nabla_{\\theta^r_{u^f}} l_R(f_{\\theta_{u^f}^r}) \\\\\n                \\end{split}\n                \\end{equation*}\n    \\STATE Perform recommendation for user $u^f$ base on $\\theta^r_{u^f}$\n    \\ENDFOR\n    \\end{algorithmic}\n\\end{algorithm}\n\n\\setlength{\\textfloatsep}{0pt}% Remove \\textfloatsep\n\nFor the \\textbf{inner loop}, a natural question is whether we still need to consider both of the tasks $T_1$ and $T_2$. First of all, we argue task $T_1$ is still essential. If the discriminator is not updated, it will be easy for the recommender to fool the discriminator. Then it will fail to achieve the purpose of guiding the meta-model to provide initialization that can produce fair results. Then for task $T_2$, we propose to remove it from the inner loop. There are three main reasons for this:\n\\vspace{-0.7cm}\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Model stability.} For fast adaptation, the user model will be fine-tuned by only a few steps of gradient descent, whereas it usually takes a longer time for the adversarial game to reach the desired equilibrium \\cite{karras2019style}. Moreover, generative adversarial network is notoriously difficult to tune \\cite{goodfellow2014generative}. If we include the task $T_2$ in the inner loop, we will need to maintain two sets of adversarial networks, which will greatly increase the training instability of the model.\n    \n    \\item \\textbf{Training efficiency.} The bulk of computation is largely spent performing the adversarial game in the inner loop as it requires multiple times of gradient descent. Optimizing the task $T_2$ will additionally increases the cost of fine-tuning during both training and deployment. Thus, eliminating such an operation significantly accelerates the training time and improves efficiency. \n    \\item \\textbf{Privacy.} Task $T_2$ requires each user's sensitive information during fine-tuning. If we do not execute this task, we can perform fair recommendations without leveraging the sensitive information of new users during deployment, and thus protect the privacy of new users. In other words, we can train on risk-free, user-approved privacy data and then perform privacy-preserving fair recommendations for new users.\n\\end{itemize}\nDue to the above reasons, we choose to perform only the optimization of task $T_1$ as follows:\n\\begin{equation}\n    \\theta^d_u= \\theta^d_u+\\beta \\nabla_{\\theta^d_u} L(f_{\\theta_{u}})\n\\end{equation}\nAs task $T_2$ is not required, we just need to optimize the task $T_r$ for $\\theta^r_u$:\n\\begin{equation}\n    \\theta^r_u= \\theta^r_u-\\beta \\nabla_{\\theta^r_u} l_R(f_{\\theta_{u}})\n\\end{equation}\n\nDuring testing, the model will be fine-tuned on the user's existing data with the initialization parameters of the meta-model. Since we no longer need to update the meta-model during deployment, we only need to perform task $T_r$. The complete algorithm is shown in Algorithm \\ref{alg:fairmeta}.\n\n% \\setlength{\\textfloatsep}{5pt}\n% \\setlength{\\floatsep}{0.1cm}\n\\vspace{-0.3cm}\n"
                },
                "subsection 2.4": {
                    "name": "Instantiation to MELU",
                    "content": "\nTo demonstrate how our proposed framework works, we provide an implementation based on Meta-Learned User Preference Estimator (MELU) \\cite{lee2019melu}, a representative embedding model for cold-start recommendation. It introduces the idea of meta-optimization mentioned above into the cold-start scenario and builds a powerful network for the recommendation. Although it has shown great capacity at the cold-start recommendation, it does not consider fairness issues. Our framework \\model~ is demonstrated based on the recommender network and meta-optimization process of MELU. Note that our framework is model-agnostic and can be applied to other meta-learned cold-start models, which we'll show in the experiments.\n\n% First, it performs embedding processes based on the input and concatenates the embedded vectors. Second, from the embedding, MELU models the decision-making process through a multi-layered neural network.\n%The recommender takes user content (e.g., age and occupation) and item content (e.g., genre and publication year) as the input, and output the predicted ratings of the user.\n% \\begin{equation}\n%     \\begin{split}\n%         x_0 & = [e_u; e_i], \\\\\n%         x_1 & = a(W_1^\\intercal x_0+b_1), \\\\\n%             & \\vdots\\\\\n%         x_N & = a(W_N^\\intercal x_{N-1}+b_N),\\\\\n%         \\hat{y}_{ij} & = \\sigma (W_{o}^\\intercal x_N +b_{o}),\n%     \\end{split}\n%     \\label{eq:decisionlayer}\n% \\end{equation}\nIn the following, we will briefly introduce the recommender network and the associated loss. For the user embedding process, MELU generates each content embedding and uses the concatenated embeddings. When the number of user contents is $P$, it defines the embedding vector $e_u$ for user $u$ as follows:\n\\begin{equation}\n    e_u = W^T_U\\left[ E_{U}^1x_{u}^1;\\, \\cdots; E_{U}^Px_{u}^P \\right]^\\intercal+b_U\n    \\label{eq:user_emb}\n\\end{equation}\nwhere $x_{u}^p$ is a $d_p$-dimensional one-hot vector for categorical content $p$ of user profile $x_u$, and $E_{U}^p$ represents the $d_e\\times d_p$ embedding matrix for the corresponding categorical content of the users. $d_e$ and $d_p$ are the embedding dimension and the number of categories for content $p$, respectively. In order to increase the representation capability of user embedding to meet the fairness requirements, a one-layer fully-connected neural network is added in the modeling process. The item embedding $e_i$ of item $i$ is obtained in the same way. Following \\cite{lee2019melu}, the initial embedding tables $E$ are only updated in the outer loop to guarantee the stability of model training. Then a N-layer fully-connected neural network decision-making module $F_N$ is constructed to estimate the user preferences, e.g., ratings. The module can be formulated as:\n\\begin{equation}\n    \\hat{y}_{ui} = F_N([e_u; e_i])\n    \\label{eq:decisionlayer}\n\\end{equation}\nwhere the input is the concatenation of user and item embeddings, and $\\hat{y}_{ui}$ is user $u$’s estimated preference for item $i$. The above together form the recommendation model $\\theta_r$. As for the recommendation loss, we adopt one of the most widely used loss functions, cross-entropy classification loss, as it can be applied to both the explicit rating prediction \\cite{berg2017graph} and implicit click forecasting \\cite{he2018nais}:\n\\begin{equation}\n    l_R^{ui} = -\\sum_{c=1}^{|Y|}y_{ui}^c log(\\hat{y}_{ui}^c)\n    %\\frac{1}{|B|}\\sum_{u=1}^{|B|}\\sum_{i=1}^{|B|}\n\\vspace{-0.2cm}\n\\end{equation}\nwhere $l_R^{ui}$ here is the loss for each user-item rating pair, the loss $l_R$ is calculated on all interaction pairs, $|Y|$ is the number of ratings, and $\\hat{y}_{ui}^c$ is the probability that the predicted rating of user-item interaction pair $ui$ is $c$."
                }
            },
            "section 3": {
                "name": "Experiments",
                "content": "\n\\label{section:experiments}\n\n\n\nIn this section, we conduct experiments to evaluate the performance of our proposed \\model. Our experiments intend to answer the following research questions:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{RQ1: }Does \\model~ enable the cold-start recommender system to learn fair and accurate recommendations?\n    \\item \\textbf{RQ2: }How do different components in our framework contribute to the performance?\n    \\item \\textbf{RQ3: }How do different hyper-parameter settings (e.g. $\\lambda$, $\\gamma$) affect the recommendation and fairness performance?\n    \\item \\textbf{RQ4: }How is the generalization ability of our proposed \\model~ on other cold-start meta-learning models?\n\\end{itemize}\n",
                "subsection 3.1": {
                    "name": "Experimental Settings",
                    "content": "\n\n\n\n\n\n\n\\paragraph{Evaluation}\nTo verify the effectiveness of our proposed method, we conduct experiments on three publicly accessible recommender system datasets: ML-1M, BookCrossing, and ML-100K. The statistics of the datasets are shown in Table~\\ref{tab:data}. We provide the details of the datasets in Appendix~\\ref{appendix:datasets}. To evaluate the performance of the user cold-start recommendation, we split the users into training, validation, testing datasets with a ratio of 7:1:2. Following exactly the settings in \\cite{lee2019melu}, for each user $u$, to mimic the real-world situation, we use a limited, unfixed number of interactions of the user as the existing fine-tuned data $D_u^e$, and leave the last 10 interactions of the user as the query data $D_u^q$ for evaluation. For fair recommendation, we choose the gender attribute as the sensitive attribute for ML-100K and ML-1M datasets and regard the age attribute as the sensitive attribute for the BookCrossing dataset. For individual fairness, we aim to evaluate whether the sensitive information is exposed by the learned user representation. Similar to many works for fairness recommenders \\cite{beigi2020privacy, wu2021learning}, we take the training users’ attributes as ground truth and train a linear classification model by taking the learned fair representations as input. We report the classification accuracy on the users in the test set for the individual fairness evaluation. We use the AUC metric to measure individual fairness performance. Concerning the counterfactual fairness, we perturb the sensitive attribute to measure whether it will affect the recommendation results. The results of all metrics in our experiments are averaged over all users. For metrics, we report MAE and NDCG to evaluate the rating prediction and ranking performance. We adopt AUC, CF (counterfactual), and GF (group) to show the performance of the three kinds of fairness. The function details of these metrics can be found in Appendix \\ref{appendix:metrics}. For the baselines, we first compare with the content-based filtering methods PPR, Wide\\&Deep and the traditional cold-start recommender system baselines DropoutNet and NLBA. For fairness consideration, due to the lack of relevant baselines on meta-learned recommender system, we test the following fairness approaches instead based on MELU: BS, Reg, IPW and MACR. The details of these baselines are shown in Appendix \\ref{appendix: baselines}.\n%Note that we process the data ourselves due to the unavailability of previous processed data.\n\n\n\n\n\n% \\begin{table*}[htbp]\n% \\caption{Experimental results on the three datasets.}\n% \\label{table:main}\n% \\scalebox{0.66}{\n% \\begin{tabular}{|c|ccccc|ccccc|ccccc|}\n% \\hline\n% \\multirow{2}{*}{} & \\multicolumn{5}{c|}{ML-1M}                                                                                     & \\multicolumn{5}{c|}{BookCrossing}                                                                              & \\multicolumn{5}{c|}{ML-100K}                                                                                   \\\\ \\cline{2-16} \n%                   & \\multicolumn{1}{c|}{MAE} & \\multicolumn{1}{c|}{NDCG} & \\multicolumn{1}{c|}{AUC} & \\multicolumn{1}{c|}{CF} & GF & \\multicolumn{1}{c|}{MAE} & \\multicolumn{1}{c|}{NDCG} & \\multicolumn{1}{c|}{AUC} & \\multicolumn{1}{c|}{CF} & GF & \\multicolumn{1}{c|}{MAE} & \\multicolumn{1}{c|}{NDCG} & \\multicolumn{1}{c|}{AUC} & \\multicolumn{1}{c|}{CF} & GF \\\\ \\hline\n% PPR               & \\multicolumn{1}{c|}{0.754\\std{0.121}}    & \\multicolumn{1}{c|}{0.754\\std{0.121}}     & \\multicolumn{1}{c|}{0.754\\std{0.121}}    & \\multicolumn{1}{c|}{0.754\\std{0.121}}   &  0.754\\std{0.121}  & \\multicolumn{1}{c|}{0.754\\std{0.121}}    & \\multicolumn{1}{c|}{0.754\\std{0.121}}     & \\multicolumn{1}{c|}{0.754\\std{0.121}}    & \\multicolumn{1}{c|}{0.754\\std{0.121}}   &  0.754\\std{0.121}  & \\multicolumn{1}{c|}{0.754\\std{0.121}}    & \\multicolumn{1}{c|}{0.754\\std{0.121}}     & \\multicolumn{1}{c|}{0.754\\std{0.121}}    & \\multicolumn{1}{c|}{0.754\\std{0.121}}   &  0.754\\std{0.121}  \\\\ \\hline\n% Wide\\&Deep        & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    \\\\ \\hline\n% DropoutNet        & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    \\\\ \\hline\n% NLBA              & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    \\\\ \\hline\n% MELU              & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    \\\\ \\hline\n% BS                & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    \\\\ \\hline\n% Reg               & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    \\\\ \\hline\n% IPW               & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    \\\\ \\hline\n% CLOVER            & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{}    & \\multicolumn{1}{c|}{}   &    \\\\ \\hline\n% \\end{tabular}}\n% \\end{table*}\n\n% \\begin{table*}[htbp]\n% \\caption{Experimental results on the three datasets.}\n% \\label{table:main}\n% \\scalebox{0.66}{\n% \\begin{tabular}{cccccccccccccccc}\n% \\hline\n% \\multicolumn{1}{c|}{\\multirow{2}{*}{}} & \\multicolumn{5}{c|}{ML-1M}                                                            & \\multicolumn{5}{c|}{BookCrossing}                                                     & \\multicolumn{5}{c}{ML-100K}                                                           \\\\\n% \\multicolumn{1}{c|}{}                  & MAE        & NDCG       & AUC        & CF         & \\multicolumn{1}{c|}{GF}           & MAE        & NDCG       & AUC        & CF         & \\multicolumn{1}{c|}{GF}           & MAE        & NDCG       & AUC        & CF         & GF                                \\\\ \\hline\n% \\hline\n% PPR                                    & 0.7540.121 & 0.7540.121 & 0.7540.121 & 0.7540.121 & 0.754\\textbackslash{}std\\{0.121\\} & 0.7540.121 & 0.7540.121 & 0.7540.121 & 0.7540.121 & 0.754\\textbackslash{}std\\{0.121\\} & 0.7540.121 & 0.7540.121 & 0.7540.121 & 0.7540.121 & 0.754\\textbackslash{}std\\{0.121\\} \\\\\n% Wide\\&Deep                             &            &            &            &            &                                   &            &            &            &            &                                   &            &            &            &            &                                   \\\\\n% DropoutNet                             &            &            &            &            &                                   &            &            &            &            &                                   &            &            &            &            &                                   \\\\\n% NLBA                                   &            &            &            &            &                                   &            &            &            &            &                                   &            &            &            &            &                                   \\\\\n% MELU                                   &            &            &            &            &                                   &            &            &            &            &                                   &            &            &            &            &                                   \\\\\n% BS                                     &            &            &            &            &                                   &            &            &            &            &                                   &            &            &            &            &                                   \\\\\n% Reg                                    &            &            &            &            &                                   &            &            &            &            &                                   &            &            &            &            &                                   \\\\\n% IPW                                    &            &            &            &            &                                   &            &            &            &            &                                   &            &            &            &            &                                   \\\\\n% CLOVER                                 &            &            &            &            &                                   &            &            &            &            &                                   &            &            &            &            &                                  \n% \\end{tabular}}\n% \\end{table*}\n\n\n\n\n% \\paragraph{Baselines}\n% We compare our method with the following baselines:\n% \\begin{itemize}[leftmargin=*]\n% \\item \\textbf{PPR \\cite{park2009pairwise}} (Pairwise Preference Regression): It estimates user preferences via bilinear regression.\n% user and item content vectors.\n% \\item \\textbf{Wide \\& Deep \\cite{cheng2016wide}}: It predicts whether the\n% user likes an item via a deep neural network. Here the neural network architecture is the same as MELU.\n% \\item \\textbf{DropoutNet \\cite{volkovs2017dropoutnet}}: It combines the dropout technique with a deep neural network to learn effective features of the input to solve the cold-start problem.\n% \\item \\textbf{NLBA \\cite{vartak2017meta}}: It learns a cold-start neural network recommender, where all weights (output and hidden) in it are constant across users, while the biases of output and hidden units are adapted per user. Note that though the algorithm is called meta, it only adapts the bias for each user and does not include the meta update.\n% \\item \\textbf{MELU \\cite{lee2019melu}}: A personalized user preference estimation model based on meta-learning that can rapidly adapt to new users. We adopt this method as our basic model for our framework.\n% \\end{itemize}\n% The first two baselines are about content-based filtering. The next two baselines are about the traditional cold-start recommender system. For fairness consideration, due to the lack of relevant baselines, we test the following fairness approaches instead based on MELU:\n% \\begin{itemize}[leftmargin=*]\n%     \\item \\textbf{BS \\cite{koren2009matrix}: } BS learns a biased score from the training stage and then removes the bias in the prediction in the testing stage.\n%     \\item \\textbf{Reg \\cite{yao2017beyond}: } Reg is a regularization-based approach that directly adds the fairness objective on the loss function. However, this approach can't be directly applied to our problem. Here we try to optimize the fairness objective in the outer loop meta update process. We add both the group fairness and counterfactual fairness objectives in the outer loop loss function.\n%     \\item \\textbf{IPW \\cite{liang2016causal} }: IPW adds the standard inverse propensity weight to reweight samples to alleviate the fairness issues.\n%     \\item \\textbf{MACR}~\\cite{wei2021model}: It provides a general debias framework for recommender system. In our context, it captures the bias related to the sensitive information at the training step and removes the bias at the inference stage.\n% \\end{itemize}\n% Implementation details and detailed parameter settings of the models can be found in Appendix~\\ref{appendix:implemented details}.\n\n\n\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Results (RQ1)",
                    "content": "\nTable \\ref{table:main} shows the recommendation and fairness performance of all compared methods w.r.t. MAE, NDCG, AUC, CF, and GF on the ML-1M, BookCrossing, and ML-100K datasets under the new user cold-start settings. From the table, we have the following findings:\n% \\setlength{\\textfloatsep}{1pt}\n\\begin{itemize}[leftmargin=*]\n    % \\item Compared with the content filtering-based baselines, cold-start baseline methods including DropoutNet and NLBA perform better in the cold-start recommendation. However, we find they perform worse with regard to comprehensive fairness.\n    \\item We observe that MELU, where the meta-learning strategy is used for new users, has a much better performance than the DropoutNet, NLBA, and content filtering baselines. This demonstrates the advantage of leveraging meta-learned prior in the cold-start problem of recommender systems. \n    \\item We notice the fairness performance of the MELU method is relatively worse than other cold-start and content-based baselines. This shows that while the meta-learned cold-start algorithm makes better use of user data and experience for the recommendation, it also increases the unfairness and bias in recommendation results. This also indicates the importance of designing a fair learning method in this setting.\n    \\item We find that \\model~ substantially outperforms all baseline methods concerning the fairness performance while not sacrificing the recommendation performance. This indicates the success of the comprehensive adversarial learning framework. Meanwhile, in many cases, our proposed method can slightly improve the recommendation performance, which indicates that incorporating adversarial learning may regularize the recommender model and help model better convergence to optimal.\n    \\item As to the fairness baselines of MELU, we observe that they are mostly designed for group fairness, and cannot perform well on imposing individual and counterfactual fairness. Among the baselines, Reg and MACR perform the best. Reg explicitly adds the fairness regularization to the objective, and MACR incorporates the module to detect and remove the biased information in the learning procedure. However, they do not consider the characteristics and convergence of the meta-learned recommender system and fail to achieve the desired performance.\n    \\end{itemize}\n\n% \\begin{table}[H]\n% \\centering\n% \\setlength{\\tabcolsep}{4pt}{\n% \\centering\n% \\caption{Experiment Results on the MoiveLens-1M dataset}\n% \\begin{tabular}{ccccc}\n% \\toprule\n% \\textbf{Method} &\\textbf{MAE$\\downarrow$}&\\textbf{NDCG@3$\\uparrow$} &\\textbf{AUC$\\downarrow$} &\\textbf{CF$\\downarrow$} \\\\\n% \\midrule\n% \\textit{PPR}   & 1.085 & 0.732 & 0.842 & 0.246   \\\\\n% \\textit{Wide \\& Deep}   & 1.072 & 0.736 & 0.795 & 0.218   \\\\\n% \\textit{MeLU}   & {0.750} & 0.799 & 1.000 & 0.295   \\\\\n% \\cline{1-5}\n% \\textit{BiasedMF}  & 1.259 & 0.698 & 0.786 & 0.178   \\\\\n% \\textit{ATCM} & 1.104 & 0.717 & 0.663 & 0.126   \\\\\n% \\cline{1-5} \n% \\textit{\\fm}     & \\textbf{0.747} & \\textbf{0.804} & \\textbf{0.502} & \\textbf{0.015} \\\\\n% \\bottomrule\n% \\label{tab:result}\n% \\end{tabular}}\n% \\end{table}\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Case Study",
                    "content": "\n\\paragraph{Impact of different components (RQ2)}\nIn this part, we investigate the effectiveness of the proposed \\model~by evaluating the impact of different components. First, we show how do the defined tasks $T_1$ and $T_2$ affect the performance of the model. Here we conduct ablation studies on \\model~ on the ML-1M dataset to show the performance of different combinations of the tasks. Note that these experiments are all about the optimization of inner-loop. For outer-loop, we'll perform both $T_1$ and $T_2$ tasks. Specifically, we compare \\model~ with its three variants: \\model$_{w/o}$, \\model$_{T_2}$, \\model$_{T_1\\&T_2}$. \\model$_{T_2}$ ($_{T_1\\&T_2}$) denotes that we optimize the task $T_2$ (${T_1\\&T_2}$) in the inner loop and \\model$_{w/o}$ means that we do not optimize either of the two tasks. In our framework~\\model, we only incorporate the optimization of task $T_1$ in the inner loop. The results are shown in Table \\ref{table:task}. From the table, we can find that removing task $T_1$ will greatly hurt the performance of recommendation, which validates the significance of optimizing the discriminator in the inner loop so as to generate fair recommendation results. Moreover, compared with the results that incorporate task $T_2$, we can observe its disadvantage for the model stability and will hurt the model performance.\n\nNext, we explore the impact of the designed loss function. We compare \\model~ with its four special cases: \\model$_{w/o~E^h}$ ($_{w/o~E^g}$), where the external information $E^h$ ($E^g$) is removed from the input to the loss; \\model$_{w/o~l_D^h}$ ($_{w/o~l_D^g}$), where we remove the adversarial loss function designed for the individual and counterfactual fairness. From Table \\ref{table:loss}, we can have the following findings: each designed loss function can improve the corresponding fairness metrics, and combining the two loss functions to perform multi-task learning can further enhance the results. Furthermore, compared with removing the external information, \\model~ performs better concerning all fairness metrics. We can also find adding external information $E^h$ not only improve the individual and counterfactual fairness but also substantially increases the group fairness metric, which verifies our assumption that it plays an important function in imposing comprehensive fairness.\n% This shows that incorporating the specified external information is beneficial for fairness.\n\\paragraph{Effect of Hyper-parameters (RQ3)} We find that our framework can improve the recommendation and fairness performance in a wide range of hyper-parameters. Due to the space limit, we provide the detailed results and analysis of parameter sensitivity of $\\lambda$, $\\gamma$ in Appendix~\\ref{appendix: hyper-parameters}.\n\n\n\n\n\\paragraph{Ability of Generalization (RQ4)}\nTo verify the generalization ability of our model, we test our framework on two other representative meta-learned cold-start recommender systems. MetaCS \\cite{bharadhwaj2019meta} follows a similar idea of MeLU when constructing the recommender model while using a more flexible meta-update strategy to learn the model parameters. MAMO \\cite{dong2020mamo} designed two memory matrices that can store task-specific memories and feature-specific memories to achieve personalized initialization and adaptation on the new users. We employ our \\model~ on these two methods to show the effectiveness, where \\model$_{MetaCS}$ (\\model$_{MAMO}$) represents the method with \\model~ framework applied on MetaCS (MAMO). The results are presented in Table \\ref{table:general}. It can be seen that our \\model~ brings consistent improvements that are model-agnostic, which clearly show the generalization ability of our framework.\n\n\n\n"
                }
            },
            "section 4": {
                "name": "related work",
                "content": "\n\\label{section:related work}\nIn this section, we briefly review the related work on cold-start in recommendation, fairness in recommendation, and fairness in meta-learning.\n\n\n% \\citet{chen2021improving} designs a novel accuracy-based task clustering scheme with the double gradient to learn multiple priors for the recommendation. \n\\vspace{-0.1cm}\n",
                "subsection 4.1": {
                    "name": "Cold-start in Recommendation",
                    "content": "\nCold-start is a common problem that recommender systems will face when there is insufficient history information in recommending items to users. Over the past years, meta-learning, also known as learning to learn, has been massively adopted to deal with the cold-start problem in the recommender system with great success. It enables models to quickly learn a new task with scarce labeled data by utilizing prior knowledge learned from previous tasks. \\citet{lee2019melu} proposed MELU to learn the initial weights of the neural networks for cold-start users based on MAML \\cite{finn2017model}. \\citet{bharadhwaj2019meta} follows a similar idea of MELU when constructing the recommender model while using a more flexible meta-update strategy to learn the model parameters. \\citet{dong2020mamo} designs two memory matrices that store task-specific memories and feature-specific memories to achieve personalized parameter initialization and adaptation for each user. \\citet{lu2020meta} took the advantage of the heterogeneous information network and proposed semantic-specific meta-learning to address the cold-start problem. \\citet{wei2020fast} equips meta-learning with collaborative filtering and dynamic subgraph sampling, which can leverage other users' history information to predict the user preference more accurately.  There're also many works that leverage meta-learning in different settings, such as online recommender \\cite{du2019sequential, sun2021form}, session recommender \\cite{song2021cbml}, bandit \\cite{DBLP:journals/corr/abs-2201-13395} and sequential recommender \\cite{wang2021sequential}, etc. However, the understanding and mitigation of fairness under the meta-learning framework is largely under-explored. In this work, we explore mitigating the fairness issues of the cold start meta-learned recommender system.\n\\vspace{-0.3cm}\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Fairness in Recommendation",
                    "content": "\nFairness has increasingly become one of the most important topics in machine learning. Recently, there has been a small amount of work focused on fairness in recommendation tasks. The fairness problem in the recommender system is sophisticated because there are multi-stakeholders and various kinds of fairness measures. As such, most of the previous works take a pretty different perspective. In our work, we are concerned about fairness on the user side due to the personalized requirements of the recommendation system \\cite{li2021user}. \\citet{yao2017beyond} proposed four new metrics for the group fairness in the recommendation and added corresponding constraints to the learning objective to explore the fairness in collaborative filtering recommender systems. \\citet{beutel2019fairness} leveraged pairwise comparisons to measure the fairness and designed a pairwise regularization training approach to improve recommender system fairness. \\citet{li2021user} studied fairness in recommendation algorithms from the user perspective and applied the fairness constrained re-ranking method to mitigate the group unfairness. \\citet{wu2021learning} leveraged the higher-order information in the interaction graph to mitigate the individual fairness problem. \\citet{li2021towards} first explored the concept of counterfactual fairness in the recommendation by generating feature-independent user embeddings to satisfy the counterfactual fairness requirements. There are also related works \\cite{zhang2021causal, wei2021model} explored the bias of users in the recommender system. \\citet{zhu2021fairness} explored the activity fairness problem of new items and designed a learnable post-processing framework to enhance fairness. However, their method is not designed for meta-learning and is not applicable for user-oriented fairness. To the best of our knowledge, our work is the first to explicitly consider different kinds of fairness and the fairness in the meta-learned recommender systems. In our work, we are concerned about the unfair issues on the user side and design a comprehensive framework to mitigate the unfairness issue with adversarial training.  \n% However, these methods only consider one specific kind of fairness, the effect and connection between other kinds of fairness are ignored. Moreover, these approaches didn't take the scenarios and models of meta-learning into consideration and thus are not appropriate for our problem.\n% \\vspace{-0.4cm}\n\n"
                },
                "subsection 4.3": {
                    "name": "Fairness in Meta Learning",
                    "content": "\nWith the development of meta-learning, a few works have started to explore the fairness problem of it. \\citet{slack2020fairness,zhao2020fair} proposed to add group regularization terms in the optimization process of meta-learning. \\citet{zhao2020primal} also designed a novel fair meta-learning framework with a task-specific group soft fairness constraint. \n% They considered the Lagrange multipliers as dual variables which are optimized to minimize the duality gap between the primal and dual functions to achieve fairness.\n\\citet{zhao2021fairness} then extended their previous work to study the problem of online fairness-aware meta-learning to deal with non-stationary task distributions using the long-term fairness constraints. However, these methods are not aimed at recommender systems and didn't consider the properties of recommender systems, thus not applicable for user-oriented fairness. Moreover, these methods are built based on adding group distribution regularized constraints on the inner and outer loop of the meta-learning algorithms. Therefore they can't be applied in the field of recommender systems, since in each adaptation, we can only access the data from every single user. As far as we know, we are the first to explore the fairness issue in the meta-learned recommender system."
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\\label{section:conclusion}\nIn this paper, we present the first fairness view for the meta-learned recommender systems with cold-start. We systematically illustrate the fairness issues in the recommender systems. Then we propose the concept of comprehensive fairness, and formulate it as the adversarial learning problem. A carefully designed framework, \\model, is proposed to enable fair adversarial learning in the meta-learned recommender system. Extensive experiments on three real-world data sets demonstrate the effectiveness of \\model, which outperforms a set of strong baselines. In the future, we would like to design fairness metrics for multi-class sensitive attributes and explore the fairness issues within the combination of multiple sensitive attributes. We are also interested in considering fairness from a user-item interaction graph perspective.\\begin{acks}\nThis work is supported by National Science Foundation under Award No. IIS-1947203, IIS-2117902, IIS-2137468, and Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agencies or the government.\n\\end{acks}\n% \\end{spacing}\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n% \\begin{spacing}{0.96}\n% \\vspace{-0.2cm}\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{ref}\n\\flushend\n% \\end{spacing}\n%%\n%% If your work has an appendix, this is the place to put it.\n% \\begin{spacing}{0.96}\n\\appendix\n% \\newpage\n"
            },
            "section 6": {
                "name": "Datasets",
                "content": "\n\\label{appendix:datasets}\nTo verify the effectiveness of our proposed method, we conducted experiments on three publicly accessible recommender system datasets. We follow \\cite{lee2019melu, beigi2020privacy} and use ML-1M , BookCrossing and  ML-100K as our experimental dataset. ML-100K. ML-1M and ML-100K datasets are both from the Movielens Collection\\footnote{The MovieLens dataset: \\url{https://grouplens.org/datasets/movielens/}}, which consists of the movie rating data from the users who joined the MovieLens website. BookCrossing\\footnote{The BookCrossing dataset: \\url{http://www2.informatik.uni-freiburg.de/~cziegler/BX/}} is another widely used benchmark for recommender system. It includes the book rating data released on the web. These three datasets all provide the basic information of users and items, as well as the sensitive information of users which can be used for fairness evaluation. We use all the available information for performing recommendations. For attributes with single value, we encode them into one-hot vector for following usage. For attributes with multiple values, we first encode each value into one-hot vector and get embedding of them. Then we average the embedding to obtain the embedding of this attribute.\n\n"
            },
            "section 7": {
                "name": "Metrics",
                "content": "\n\\label{appendix:metrics}\nFor evaluating both the recommendation and fairness performance, we adopt the following metrics:\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{MAE} (Mean Absolute Error): To measure the errors between prediction and ground-truth:\n$$MAE=\\frac{1}{|U^f|} \\sum_{u \\in U^f} \\frac{1}{|D^q_u|} \\sum_{i \\in D^e_u}\\left|y_{ui}-\\hat{y}_{ui}\\right|$$\n\\item \\textbf{NDCG}: The normalized DCG to measure the average performance of a search engine's top-K ranking algorithm.\n$$\n\\begin{aligned}\nn D C G_{k} &=\\frac{1}{|U^f|} \\sum_{u \\in U^f} \\frac{D C G_{k}^{u}}{I D C G_{k}^{u}} \\\\\nD C G_{k}^{u} &=\\sum_{r=1}^{k} \\frac{2^{y_{u}^r}-1}{\\log _{2}(1+r)}\n\\end{aligned}\n$$\nwhere $y_{u}^r$ is the real rating of user $u$ for the $r$-th ranked item. $DCG_k$ calculates the top-k actual rating values sorted by predicted rating values; and $IDCG_k$ calculates the top-k sorted actual rating values, which is the best possible value. Following \\cite{dong2020mamo}, we set $k$ to 3 to measure the performance.\n\n\\end{itemize} \nThe above metrics measure the performance of the recommender, next, we will introduce the metrics for quantifying the performance of comprehensive fairness in the recommender system.\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{AUC} (Area Under the Curve): The curve is created by plotting the true positive rate against the false-positive rate at various threshold settings.\n\\item \\textbf{CF} (Counterfactual Fairness): To measure the difference of predictions by perturbing the sensitive attribute. We denote the sensitive attribute of user $u$ as $a_u$ (e.g., female), any other attributes as $x_u$. $a_u'$ is the other value attainable of the sensitive attribute (e.g., male).\n\\begin{equation*}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n  CF=\\frac{1}{|U^f|} \\sum_{u \\in U^f} \\frac{1}{|D^q_u|} \\sum_{i \\in D^q_u}\\left|\\hat{y}_{ui}(x_u,a_u)-\\hat{y}_{ui}(x_u,a_u')\\right|\n\\end{equation*}\n\\item \\textbf{GF} (Group Fairness): Same as the definition in \\ref{subsection:fairness}, we consider GF as the difference of recommender performance (MAE) between users of diverse sensitive attribute.\n$$\n    GF=|\\frac{1}{|A_1|}\\sum_{u_1\\in A_1} R(u_1) - \\frac{1}{|A_2|}\\sum_{u_2\\in A_2} R(u_2)|\n$$\nwhere $A_1$ and $A_2$ are two user groups splited by the sensitive attribute $A$.\n\\end{itemize}\n\n\n\n"
            },
            "section 8": {
                "name": "Baselines",
                "content": "\n\\label{appendix: baselines}\nWe compare our method with the following baselines:\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{PPR \\cite{park2009pairwise}} (Pairwise Preference Regression): It estimates user preferences via bilinear regression.\nuser and item content vectors.\n\\item \\textbf{Wide \\& Deep \\cite{cheng2016wide}}: It predicts whether the\nuser likes an item via a deep neural network. Here the neural network architecture is the same as MELU.\n\\item \\textbf{DropoutNet \\cite{volkovs2017dropoutnet}}: It combines the dropout technique with a deep neural network to learn effective features of the input to solve the cold-start problem.\n\\item \\textbf{NLBA \\cite{vartak2017meta}}: It learns a cold-start neural network recommender, where all weights (output and hidden) in it are constant across users, while the biases of output and hidden units are adapted per user. Note that though the algorithm is called meta, it only adapts the bias for each user and does not include the meta update.\n\\item \\textbf{MELU \\cite{lee2019melu}}: A personalized user preference estimation model based on meta-learning that can rapidly adapt to new users. We adopt this method as our basic model for our framework.\n\\end{itemize}\nThe first two baselines are about content-based filtering. The next two baselines are about the traditional cold-start recommender system. For fairness consideration, due to the lack of relevant baselines, we test the following fairness approaches instead based on MELU:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{BS \\cite{koren2009matrix}: } BS learns a biased score from the training stage and then removes the bias in the prediction in the testing stage.\n    \\item \\textbf{Reg \\cite{yao2017beyond}: } Reg is a regularization-based approach that directly adds the fairness objective on the loss function. However, this approach can't be directly applied to our problem. Here we try to optimize the fairness objective in the outer loop meta update process. We add both the group fairness and counterfactual fairness objectives in the outer loop loss function.\n    \\item \\textbf{IPW \\cite{liang2016causal} }: IPW adopt the reweight-based method \\cite{wang2022training} and leverage the standard inverse propensity weight to reweight samples to alleviate the fairness issues.\n    \\item \\textbf{MACR}~\\cite{wei2021model}: It provides a general debias framework for recommender system. In our context, it captures the bias related to the sensitive information at the training step and removes the bias at the inference stage.\n\\end{itemize}\nImplementation details and detailed parameter settings of the models can be found in the next section.\n\n"
            },
            "section 9": {
                "name": "Implemented Details",
                "content": "\n\\label{appendix:implemented details}\nWe use Pytorch to build our model. We set the dimensions of embedding vectors of all the above models to 64 by default, and use the same recommender loss function for all models for a fair comparison. For Reg method, we test the fairness objective coefficient from 0 to 1 with a step size of 0.1, and is set to 0.5 because it works best. For MACR model, we search the bias elimination parameter $c$ from 20 to 40 with the step size of 2, which is suggested by the authors. The trainable parameters are initialized with the Xavier method. Adam algorithm is used to optimize the model. We set the number of training epochs to 50 and trains 32 user tasks in a batch. During the sampling within each batch, we uniformly sample from all users. The inner learning rate $\\alpha$ is set to 1e-2 and the outer loop learning rate $\\beta$ is set to 1e-3. For all meta-learned models, we set the number of finetuning steps in the inner loop to 5. The loss function trade-off hyper-parameter $\\lambda, \\gamma$ are both searched in the range of $\\{1e-2,1e-1,1,5\\}$ $\\lambda$ and $\\gamma$ are set to 1 and 0.1 by default.\n\n\n\n\n\n"
            },
            "section 10": {
                "name": "Effect of Hyper-parameters (RQ3)",
                "content": "\n\\label{appendix: hyper-parameters}\nAs formulated in the loss function, $\\lambda$ is the trade-off hyper-parameter that balances the contribution of the recommendation model loss and the individual and counterfactual fairness loss while $\\gamma$ is to balance the recommender model loss and the group fairness loss. To investigate the benefits of them, we conduct experiments of \\model~  with varying $\\lambda$ and $\\gamma$ respectively. In particular, we search their values in the range of \\{0.01, 0.1, 1, 5\\}. When varying one parameter, the other is set as constant. From Table~\\ref{table:lambda} and Table~\\ref{table:gamma} we have the following findings. We can find our method can improve the recommender and fairness performance in a wide range. As $\\lambda$ increases from 1e-2 to 1, the performance of \\model~ will become better. A similar trend can be observed by varying $\\gamma$ from 1e-2 to 1e-1. However, when $\\lambda$ or $\\gamma$ surpasses a threshold (1 for $\\lambda$), the performance becomes worse. As the hyper-parameters become larger, the training of the recommendation model will be less important and the bi-level meta-learned model will be difficult to converge, which brings the worse results.\n\n"
            }
        },
        "tables": {
            "tab:data": "\\begin{table}[t]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\centering\n\\caption{Statistics of Datasets.}\n\\label{tab:data}\n\\scalebox{0.78}{\n\\begin{tabular}{c|c|c|c}\n\\toprule\n    \\textbf{Dataset}           & \\textbf{ML-1M} & \\textbf{BookCrossing} & \\textbf{ML-100K} \\\\ \n    \\hline\n    No. of users     & 6,040              & 278,858 &  943 \\\\ \n    No. of items     & 3,706              & 271,379 &  1,682 \\\\ \n    No. of ratings   & 1,000,209          & 1,149,780 &  100,000 \\\\ \n    Sparsity            & 95.5316\\%          & 99.9985\\% &  93.6953\\%\\\\ \n    \\hline \n    User contents       & \\makecell{Gender, Age, \\\\ Occupation,\\\\ Zip code}  & \\makecell{Age, Location}  & \\makecell{Gender, Age, \\\\ Occupation,\\\\ Zip code}   \\\\ \n    \\hline \n    Item contents       & \\makecell{Publication year, \\\\ Rate, Genre, \\\\ Director, Actor}  & \\makecell{Publication year, \\\\ Author, Publisher } &  \\makecell{Publication year, Genre} \\\\ \n    \\hline \n    Range of ratings    & 1 $\\sim$ 5         & 1 $\\sim$ 10 & 1 $\\sim$ 5 \\\\ \n\\bottomrule\n\\end{tabular}}\n% \\vspace{-0.4cm}\n\\end{table}",
            "table:main": "\\begin{table*}[htbp]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\caption{Experimental results on the three datasets averaged over five independent runs. Arrows ($\\uparrow$, $\\downarrow$) indicate the direction of better performance. \\model~ keeps the predictive power of the original recommender model while improving their fairness. Bold values indicate the best performance with regard to the meta-learned recommender model.}\n\\label{table:main}\n\\scalebox{0.69}{\n% \\resizebox{\\textwidth}{18mm}{\n\\begin{tabular}{c|ccccc|ccccc|ccccc}\n\\hline\n\\multirow{2}{*}{} & \\multicolumn{5}{c|}{ML-1M}                                                                                                                & \\multicolumn{5}{c|}{BookCrossing}                                                                                                         & \\multicolumn{5}{c}{ML-100K}                                                                                                               \\\\ \\cline{2-16} \n                  & \\multicolumn{1}{c|}{MAE $\\downarrow$}  & \\multicolumn{1}{c|}{NDCG $\\uparrow$} & \\multicolumn{1}{c|}{AUC $\\downarrow$}  & \\multicolumn{1}{c|}{CF $\\downarrow$}   & GF $\\downarrow$                        & \\multicolumn{1}{c|}{MAE $\\downarrow$ }  & \\multicolumn{1}{c|}{NDCG $\\uparrow$} & \\multicolumn{1}{c|}{AUC $\\downarrow$ }  & \\multicolumn{1}{c|}{CF $\\downarrow$}   & GF $\\downarrow$                         & \\multicolumn{1}{c|}{MAE $\\downarrow$ }  & \\multicolumn{1}{c|}{NDCG $\\uparrow$} & \\multicolumn{1}{c|}{AUC $\\downarrow$ }  & \\multicolumn{1}{c|}{CF $\\downarrow$ }   & GF $\\downarrow$                         \\\\ \\hline \\hline\nPPR               & 0.943\\std{0.012}          & 0.672\\std{0.004}          & 0.932\\std{0.013}          & 0.187\\std{0.058}          & 0.052\\std{0.012}          & 2.374\\std{0.015}          & 0.598\\std{0.005}          & 0.854\\std{0.051}          & 0.194\\std{0.035}          & 0.101\\std{0.013}          & 1.398\\std{0.009}          & 0.486\\std{0.005}          & 0.772\\std{0.036}          & 0.095\\std{0.032}          & 0.051\\std{0.008}          \\\\ \nWide\\&Deep        & 0.815\\std{0.007}          & 0.691\\std{0.003}          & 0.945\\std{0.024}          & 0.214\\std{0.034}          & 0.055\\std{0.014}          & 1.945\\std{0.028}          & 0.627\\std{0.008}          & 0.833\\std{0.034}          & 0.156\\std{0.029}          & 0.098\\std{0.005}          & 1.256\\std{0.002}          & 0.519\\std{0.006}          & 0.863\\std{0.044}          & 0.112\\std{0.021}          & 0.046\\std{0.012}          \\\\  \\hline\nDropoutNet        & 0.813\\std{0.005}          & 0.702\\std{0.004}          & 0.965\\std{0.013}          & 0.204\\std{0.021}          & 0.063\\std{0.003}          & 1.855\\std{0.002}          & 0.634\\std{0.005}          & 0.889\\std{0.051}          & 0.194\\std{0.035}          & 0.103\\std{0.011}          & 1.172\\std{0.007}          & 0.544\\std{0.005}          & 0.842\\std{0.036}          & 0.131\\std{0.018}          & 0.044\\std{0.013}          \\\\ \nNLBA              & 0.795\\std{0.006}          & 0.701\\std{0.002}          & 0.971\\std{0.024}          & 0.254\\std{0.047}          & 0.056\\std{0.007}          & 1.718\\std{0.007}          & 0.651\\std{0.008}          & 0.943\\std{0.034}          & 0.156\\std{0.029}          & 0.112\\std{0.005}          & 1.213\\std{0.003}          & 0.553\\std{0.006}          & 0.891\\std{0.044}          & 0.129\\std{0.023}          & 0.047\\std{0.004}          \\\\  \\hline\nMELU              & 0.743\\std{0.008}          & 0.755\\std{0.002}          & 1.000\\std{0.000}          & 0.264\\std{0.058}          & 0.054\\std{0.011}          & \\textbf{1.332\\std{0.008}} & \\textbf{0.723\\std{0.006}} & 1.000\\std{0.000}          & 0.259\\std{0.094}          & 0.103\\std{0.011}          & 0.892\\std{0.009}          & 0.652\\std{0.017}          & 0.974\\std{0.016}          & 0.157\\std{0.032}          & 0.048\\std{0.014}          \\\\ \nMELU+BS                & 0.745\\std{0.003}          & 0.744\\std{0.003}          & 1.000\\std{0.000}          & 0.277\\std{0.047}          & 0.049\\std{0.012}          & 1.343\\std{0.005}          & 0.712\\std{0.005}          & 1.000\\std{0.000}          & 0.291\\std{0.058}          & 0.101\\std{0.010}          & 0.891\\std{0.002}          & 0.651\\std{0.011}          & 1.000\\std{0.000}          & 0.174\\std{0.019}          & 0.046\\std{0.008}          \\\\ \nMELU+Reg               & 0.762\\std{0.002}          & 0.748\\std{0.002}          & 0.894\\std{0.011}          & 0.145\\std{0.033}          & 0.048\\std{0.009}          & 1.357\\std{0.008}          & 0.713\\std{0.004}          & 0.912\\std{0.023}          & 0.232\\std{0.024}          & 0.097\\std{0.011}          & 0.914\\std{0.007}          & 0.648\\std{0.013}          & 0.873\\std{0.036}          & 0.132\\std{0.012}          & 0.045\\std{0.005}          \\\\ \nMELU+IPW               & 0.751\\std{0.012}          & 0.751\\std{0.002}          & 0.975\\std{0.008}          & 0.187\\std{0.013}          & 0.047\\std{0.011}          & 1.365\\std{0.004}          & 0.707\\std{0.006}          & 1.000\\std{0.000}          & 0.245\\std{0.044}          & 0.096\\std{0.007}          & 0.903\\std{0.008}          & 0.643\\std{0.005}          & 0.946\\std{0.024}          & 0.117\\std{0.038}          & 0.047\\std{0.009}          \\\\ \nMELU+MACR              & 0.744\\std{0.005}          & 0.750\\std{0.004}          & 0.878\\std{0.031}          & 0.116\\std{0.027}          & 0.051\\std{0.011}          & 1.352\\std{0.006}          & 0.715\\std{0.002}          & 0.867\\std{0.016}          & 0.179\\std{0.094}          & 0.097\\std{0.013}          & 0.887\\std{0.011}          & 0.656\\std{0.002}          & 0.815\\std{0.019}          & 0.108\\std{0.016}          & 0.044\\std{0.007}          \\\\ \\hline\n\\textbf{CLOVER}            & \\textbf{0.731\\std{0.005}} & \\textbf{0.756\\std{0.005}} & \\textbf{0.632\\std{0.057}} & \\textbf{0.044\\std{0.014}} & \\textbf{0.040\\std{0.006}} & 1.352\\std{0.006}          & 0.722\\std{0.006}          & \\textbf{0.546\\std{0.032}} & \\textbf{0.027\\std{0.011}} & \\textbf{0.089\\std{0.009}} & \\textbf{0.880\\std{0.009}} & \\textbf{0.666\\std{0.003}} & \\textbf{0.562\\std{0.021}} & \\textbf{0.032\\std{0.015}} & \\textbf{0.036\\std{0.006}} \\\\ \\hline\n\\end{tabular}}\n\\end{table*}",
            "table:task": "\\begin{table}[t]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\caption{Effect of different optimization objectives.}\n\\label{table:task}\n\\scalebox{0.85}{\n\\begin{tabular}{c|ccccc}\n\\hline \n              & MAE $\\downarrow$           & NDCG $\\uparrow$            & AUC $\\downarrow$           & CF $\\downarrow$            & GF $\\downarrow$            \\\\ \\hline\nMELU          & 0.743\\std{0.008}          & 0.755\\std{0.002}          & 1.000\\std{0.000}          & 0.264\\std{0.058}          & 0.054\\std{0.011}          \\\\\nCLOVER w/o    & 0.739\\std{0.004}          & 0.754\\std{0.003}          & 1.000\\std{0.000}          & 0.332\\std{0.069}          & 0.056\\std{0.013}          \\\\ \nCLOVER$_{T2}$     & 0.761\\std{0.007}          & 0.741\\std{0.004}          & 1.000\\std{0.000}          & 0.247\\std{0.042}          & 0.062\\std{0.007}          \\\\ \nCLOVER$_{T1\\&T2}$ & 0.749\\std{0.005}          & 0.751\\std{0.008}          & 0.792\\std{0.035}          & 0.105\\std{0.015}          & 0.051\\std{0.008}          \\\\ \\hline\nCLOVER        & \\textbf{0.731\\std{0.005}} & \\textbf{0.756\\std{0.005}} & \\textbf{0.632\\std{0.057}} & \\textbf{0.044\\std{0.014}} & \\textbf{0.040\\std{0.006}} \\\\ \\hline\n\\end{tabular}}\n% \\vspace{-0.5cm}\n\\end{table}",
            "table:loss": "\\begin{table}[t]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\caption{Effect of different adversarial loss functions.}\n\\label{table:loss}\n\\scalebox{0.8}{\n\\begin{tabular}{c|ccccc}\n\\hline\n                   & MAE $\\downarrow$          & NDCG $\\uparrow$           & AUC $\\downarrow$          & CF $\\downarrow$           & GF $\\downarrow$           \\\\ \\hline\nMELU               & 0.743\\std{0.008}          & 0.755\\std{0.002}          & 1.000\\std{0.000}          & 0.264\\std{0.058}          & 0.054\\std{0.011}          \\\\\nCLOVER w/o $E^h$   & 0.735\\std{0.005}          & \\textbf{0.758\\std{0.002}}          & 0.667\\std{0.041}          & 0.051\\std{0.009}          & 0.044\\std{0.006}          \\\\\nCLOVER w/o $l_D^h$ & \\textbf{0.730\\std{0.004}}          & 0.754\\std{0.006}          & 0.687\\std{0.062}          & 0.067\\std{0.013}          & 0.055\\std{0.010}          \\\\\nCLOVER w/o $E^g$   & 0.736\\std{0.006}          & 0.752\\std{0.003}          & 0.725\\std{0.037}          & 0.056\\std{0.015}          & 0.049\\std{0.012}          \\\\\nCLOVER w/o $l_D^g$ & 0.738\\std{0.005}          & 0.754\\std{0.008}          & 0.914\\std{0.018}          & 0.198\\std{0.021}          & 0.043\\std{0.008}          \\\\ \\hline\nCLOVER             & 0.731\\std{0.005} & 0.756\\std{0.005} & \\textbf{0.632\\std{0.057}} & \\textbf{0.044\\std{0.014}} & \\textbf{0.040\\std{0.006}} \\\\ \\hline\n\\end{tabular}}\n% \\vspace{-0.2cm}\n\\end{table}",
            "table:general": "\\begin{table}[t]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\caption{Generalization ability of \\model.}\n\\label{table:general}\n\\scalebox{0.8}{\n\\begin{tabular}{c|ccccc}\n\\hline\n              & MAE $\\downarrow$          & NDCG $\\uparrow$           & AUC $\\downarrow$          & CF $\\downarrow$           & GF $\\downarrow$           \\\\ \\hline\nMELU          & 0.743\\std{0.008}          & 0.755\\std{0.002}          & 1.000\\std{0.000}          & 0.264\\std{0.058}          & 0.054\\std{0.011}          \\\\\nCLOVER$_{MELU}$   & \\textbf{0.731\\std{0.005}} & \\textbf{0.756\\std{0.005}} & \\textbf{0.632\\std{0.057}} & \\textbf{0.044\\std{0.014}} & \\textbf{0.040\\std{0.006}} \\\\ \\hline\nMetaCS        & 0.721\\std{0.005}          & \\textbf{0.776\\std{0.003}} & 1.000\\std{0.000}          & 0.245\\std{0.058}          & 0.061\\std{0.015}          \\\\\nCLOVER$_{MetaCS}$ & \\textbf{0.720\\std{0.002}} & 0.775\\std{0.005}          & \\textbf{0.578\\std{0.037}} & \\textbf{0.029\\std{0.009}} & \\textbf{0.044\\std{0.008}} \\\\ \\hline\nMAMO          & 0.717\\std{0.003}          & 0.781\\std{0.004}          & 1.000\\std{0.000}          & 0.331\\std{0.058}          & 0.057\\std{0.013}          \\\\\nCLOVER$_{MAMO}$   & \\textbf{0.711\\std{0.004}} & \\textbf{0.786\\std{0.002}} & \\textbf{0.612\\std{0.044}} & \\textbf{0.036\\std{0.017}} & \\textbf{0.039\\std{0.011}} \\\\ \\hline\n\\end{tabular}}\n% \\vspace{-0.2cm}\n\\end{table}",
            "table:lambda": "\\begin{table}[t]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\caption{Effect of hyper-parameter $\\lambda$.}\n\\label{table:lambda}\n\\scalebox{0.9}{\n\\begin{tabular}{c|ccccc}\n\\hline\n     & MAE $\\downarrow$          & NDCG $\\uparrow$           & AUC $\\downarrow$          & CF $\\downarrow$           & GF $\\downarrow$           \\\\ \\hline\n1e-2 & 0.735\\std{0.002}          & 0.755\\std{0.003}          & 0.872\\std{0.056}          & 0.092\\std{0.019}          & 0.050\\std{0.011}          \\\\\n1e-1 & 0.733\\std{0.004}          & 0.754\\std{0.004}          & 0.679\\std{0.052}          & 0.067\\std{0.009}          & 0.045\\std{0.007}          \\\\\n1    & \\textbf{0.731\\std{0.005}} & \\textbf{0.756\\std{0.005}} & \\textbf{0.632\\std{0.057}} & \\textbf{0.044\\std{0.014}} & \\textbf{0.040\\std{0.006}} \\\\\n5    & 0.737\\std{0.007}          & 0.754\\std{0.004}          & 0.722\\std{0.043}          & 0.079\\std{0.021}          & 0.048\\std{0.006}          \\\\ \\hline\n\\end{tabular}}\n\\end{table}",
            "table:gamma": "\\begin{table}[t]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\caption{Effect of hyper-parameter $\\gamma$.}\n\\label{table:gamma}\n\\scalebox{0.9}{\n\\begin{tabular}{c|ccccc}\n\\hline\n     & MAE $\\downarrow$          & NDCG $\\uparrow$           & AUC $\\downarrow$          & CF $\\downarrow$           & GF $\\downarrow$           \\\\ \\hline\n1e-2 & 0.733\\std{0.003}          & 0.754\\std{0.004}          & 0.647\\std{0.046}          & 0.055\\std{0.013}          & 0.044\\std{0.011}          \\\\\n1e-1 & \\textbf{0.731\\std{0.005}} & \\textbf{0.756\\std{0.005}} & \\textbf{0.632\\std{0.057}} & \\textbf{0.044\\std{0.014}} & \\textbf{0.040\\std{0.006}} \\\\\n1    & 0.734\\std{0.005}          & 0.754\\std{0.003}          & 0.651\\std{0.055}          & 0.060\\std{0.020}          & 0.047\\std{0.006}          \\\\\n5    & 0.741\\std{0.003}          & 0.754\\std{0.004}          & 0.941\\std{0.017}          & 0.176\\std{0.023}          & 0.061\\std{0.008}          \\\\ \\hline\n\\end{tabular}}\n\\end{table}"
        },
        "figures": {
            "fig:diff": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{image/diff.png}\n  \\caption{Difference between cold-start fair recommender system and existing fair recommender system.}\n%   \\caption{A typical framework of recommender systems.}\n  \\label{fig:diff}\n  \\vspace{-0.6cm}\n\\end{figure}",
            "fig:meta": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{image/META.pdf}\n  \\caption{The training process of the meta-learned recommender system.}\n%   \\caption{A typical framework of recommender systems.}\n  \\label{fig:meta}\n  \\vspace{-0.4cm}\n\\end{figure}",
            "fig:structure": "\\begin{figure}[t] \n% \\vspace{-20 pt}\n\\centering\n% \\subfloat{\\includegraphics[width = 2.4in]{image/melu1.png}}\n% % \\subcaption[a]{The original \\textit{MeLU}}\n% \\hspace{3 pt}\n% \\subfloat{\\includegraphics[width = 2.6in]{image/adv_melu2.png}}\n% \\subcaption[b]{ \\fm }\n\\includegraphics[width=0.95\\linewidth]{image/frame.pdf}\n\\caption{The network structure of proposed \\model.}\n% \\vspace{-0.3cm}\n\\label{fig:structure}\n\\vspace{-0.4cm}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    t_{u}: (\\theta, D_{u}^e) \\xrightarrow{} \\theta_{u},\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    IF=\\frac{1}{|U^f|}\\max_g\\sum_{u\\in U^f} M(\\hat{a}_u=g(e_u), a_u)\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    CF=\\frac{1}{|U^f|}\\sum_{u\\in U^f} |R(L_{a_u}^u \\mid X=x, A=a_u)-R(L_{a^{\\prime}_u}^u \\mid X=x, A=a_u)|\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    GF=|\\frac{1}{|A_1|}\\sum_{u_1\\in A_1} R(u_1) - \\frac{1}{|A_2|}\\sum_{u_2\\in A_2} R(u_2)|\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\begin{split}\n    \\theta_u=\\theta_u-\\alpha\\nabla_{\\theta_u} L(f_{\\theta_u}, D_u^e),\\\\\n\\end{split}\n\\vspace{-0.2cm}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\theta= \\theta-\\beta \\nabla_{\\theta} \\sum_{u\\in B}L(f_{\\theta_{u}},D_{u}^q)\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\min_R \\max_D L=L(f_{\\theta^r, \\theta^d})=l_R(f_{\\theta^r})-l_D(f_{\\theta^r, \\theta^d})\n    \\vspace{-0.2cm}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    l_D(f_{\\theta^r, \\theta^d}))=l_D(a_{u},\\hat{a}_{u})=-\\frac{1}{N}\\sum_{u=1}^{N}\\sum_{c=1}^Ca_{u}^c log(\\hat{a}_{u}^c)\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    l_D^g = l_D(a_u, \\hat{a}_u=g(e_u, E_g))\n    \\vspace{-0.1cm}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    0 \\leq\t I(a_u; \\hat{y}_{ui}) \\leq I(a_u; e_u) \\And I(a_u; e_u)=0 \\Longrightarrow I(a_u; \\hat{y}_{ui})=0\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    l_D^h = l_D(a_u, \\hat{a}_u=h(y_{ui}, \\hat{y}_{ui}, E_h))\n    \\vspace{-0.1cm}\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    l_D = \\lambda*l_D^g+\\gamma*l_D^h\n\\end{equation}",
            "eq:13": "\\begin{equation}\n    \\theta^r= \\theta^r-\\beta \\nabla_{\\theta^r} \\sum_{u\\in B}L(f_{\\theta_{u}})\n\\vspace{-0.1cm}\n\\end{equation}",
            "eq:14": "\\begin{equation}\n    \\theta^d= \\theta^d+\\beta \\nabla_{\\theta^d} \\sum_{u\\in B}L(f_{\\theta_{u}})\n\\end{equation}",
            "eq:15": "\\begin{equation}\n    \\theta^d_u= \\theta^d_u+\\beta \\nabla_{\\theta^d_u} L(f_{\\theta_{u}})\n\\end{equation}",
            "eq:16": "\\begin{equation}\n    \\theta^r_u= \\theta^r_u-\\beta \\nabla_{\\theta^r_u} l_R(f_{\\theta_{u}})\n\\end{equation}",
            "eq:17": "\\begin{equation}\n    e_u = W^T_U\\left[ E_{U}^1x_{u}^1;\\, \\cdots; E_{U}^Px_{u}^P \\right]^\\intercal+b_U\n    \\label{eq:user_emb}\n\\end{equation}",
            "eq:18": "\\begin{equation}\n    \\hat{y}_{ui} = F_N([e_u; e_i])\n    \\label{eq:decisionlayer}\n\\end{equation}",
            "eq:19": "\\begin{equation}\n    l_R^{ui} = -\\sum_{c=1}^{|Y|}y_{ui}^c log(\\hat{y}_{ui}^c)\n    %\\frac{1}{|B|}\\sum_{u=1}^{|B|}\\sum_{i=1}^{|B|}\n\\vspace{-0.2cm}\n\\end{equation}"
        }
    }
}