{
    "meta_info": {
        "title": "FedDefender: Client-Side Attack-Tolerant Federated Learning",
        "abstract": "Federated learning enables learning from decentralized data sources without\ncompromising privacy, which makes it a crucial technique. However, it is\nvulnerable to model poisoning attacks, where malicious clients interfere with\nthe training process. Previous defense mechanisms have focused on the\nserver-side by using careful model aggregation, but this may not be effective\nwhen the data is not identically distributed or when attackers can access the\ninformation of benign clients. In this paper, we propose a new defense\nmechanism that focuses on the client-side, called FedDefender, to help benign\nclients train robust local models and avoid the adverse impact of malicious\nmodel updates from attackers, even when a server-side defense cannot identify\nor remove adversaries. Our method consists of two main components: (1)\nattack-tolerant local meta update and (2) attack-tolerant global knowledge\ndistillation. These components are used to find noise-resilient model\nparameters while accurately extracting knowledge from a potentially corrupted\nglobal model. Our client-side defense strategy has a flexible structure and can\nwork in conjunction with any existing server-side strategies. Evaluations of\nreal-world scenarios across multiple datasets show that the proposed method\nenhances the robustness of federated learning against model poisoning attacks.",
        "author": "Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha",
        "link": "http://arxiv.org/abs/2307.09048v1",
        "category": [
            "cs.CR",
            "cs.AI"
        ],
        "additionl_info": "KDD'23 research track accepted"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "\\cutsectionup\n"
            },
            "section 2": {
                "name": "Introduction",
                "content": "\n\nFederated learning has become a popular model training method to guarantee the minimum level of data privacy~\\cite{bonawitz2019towards}. \nIn each training round of federated learning, clients optimize their local models and send updates to the central server to aggregate them to produce a global model of the entire data distribution. Thus federated learning enables clients to jointly train a global model without directly sharing their private training data, making it a privacy-friendly solution~\\cite{wang2021field}. \nFederated learning has been rapidly adopted by various applications that require data privacy~\\cite{luo2021cost,rieke2020future,wu2022fedctr,park2022knowledge}.\n\\looseness=-1\n\n\nDespite its advantages, federated learning is vulnerable to attacks due to its decentralized nature. Any client can easily participate in the training process, introducing the possibility of maliciousness~\\cite{baruch2019little,lyu2020threats}. For example, federated learning systems assume that all participants are benign and that their data can help improve the performance of resulting models. This leaves room for \\emph{model poisoning attacks}, wherein malicious users deceive the system by pretending to be benign and sending ``poisoned'' updates to the central server. \nThis type of attack can disrupt parameter optimization~\\cite{shafahi2018poison} and adversely impact the model's performance~\\cite{gu2017badnets}, undermining the integrity of the federated learning system. \n\nExisting studies focus on using robust aggregation on the server side as a defense against model poisoning attacks.\nA central server can be trained to preserve updates from likely benign clients while discarding updates from likely corrupted clients or adversaries.\nStatistics like the trimmed mean or median can be used for outlier-resistant aggregation instead of simply averaging all updates.\nFu \\textit{et al.,} extended this method by introducing a concept of confidence computed from residuals of repeated median estimator~\\cite{fu2019attack}.\nOther detection algorithms like Norm Bound, Multi-Krum, and FoolsGold assign lower weights to outlier updates and reduce their adverse effect~\\cite{blanchard2017machine,fung2020limitations,sun2019can}.\nHowever, these methods are limited in their ability to detect adversaries in the real world, where local datasets are no longer independent and identically distributed (i.e., non-IID). Non-IID makes benign local updates diverse and indistinguishable from corrupted ones~\\cite{fung2020limitations}.\nFurthermore, extant defenses have been breached by newer attacks that introduce elaborate local updates~\\cite {baruch2019little,fang2020local,shejwalkar2021manipulating}, leading server-side strategies alone to be obsolete.\n\n\nThis paper takes a step further by introducing a client-side defense strategy named \\model{}, which can run alongside existing server-side defense strategies to enhance resilience against poisoning attacks in federated learning.\nThis strategy modifies the local training process of benign clients by obtaining robust local models even when the server-side defense fails to filter out corrupt updates from malicious clients.\n\n\\model{} consists of two unique training components: (1) \\textit{attack-tolerant local meta update} that finds local parameters that are less susceptible to noisy training by malicious clients and (2) \\textit{attack-tolerant global knowledge distillation} that extracts the correct information from a noisy global model.\n\n\n\nIn our attack-tolerant local meta update component, \\model{} generates a synthetic batch of corrupted data from the local data, including randomly flipped labels. \nThe local model is then perturbed with this noisy batch of samples through one iteration update before the main objective is optimized using the clean batch of samples. \nThis meta update is analogous to a vaccination process, where synthetic noise is introduced to prevent the model from collapsing in the face of attacks, and to find the model parameters that are more tolerant to noise. \nTo generate more realistic noise during training, \\model{} discovers $k$-nearest neighbors from the local model's embeddings to replace the original label. \\looseness=-1\n\nMeanwhile, the main objective of the attack-tolerant global knowledge distillation is to convey only useful knowledge from the possibly corrupted global model on the central server.\nConventional knowledge distillation cannot be used in federated learning attack scenarios because attackers' malicious influence on the corrupted global model can disrupt local training.\nInstead, we leverage the fact that the last layer of a model is more prone to overfitting from noisy updates than the intermediate layer~\\cite{kundu2021analyzing,stephenson2021geometry}. \n\\model{} can learn more reliable information from the global model without overfitting by distilling knowledge only to the intermediate layer via an auxiliary network.\nWe also add a self-knowledge distillation objective to calibrate the prediction so that it works better for distillation.  \\looseness=-1\n\n\n\nExperiments show that \\model{} is highly effective in various scenarios when applied in conjunction with existing server-side defense strategies.\nComparison with several recent baselines such as Norm bound, Multi-Krum, and ResidualBase shows that combining our method with these server-side strategies consistently results in better performance.\nFor instance, in the label poisoning attack scenario, the classification accuracy increased by 18\\%, 20\\%, 17\\%, and 17\\% for the CIFAR-10, CIFAR-100, TinyImageNet, and FEMNIST datasets, respectively.\nWe further show that \\model{} is able to effectively defend against advanced poisoning attacks like LIE~\\cite{baruch2019little}, STAT-OPT~\\cite{fang2020local}, and DYN-OPT~\\cite{shejwalkar2021manipulating}.\n\nThe major contributions and findings of our work include: \n\\vspace{1mm}\n\n\\begin{itemize} [nosep,leftmargin=1em,labelwidth=*,align=left]\n\n\\item Proposing a unique client-side defense strategy, \\model{}, that trains robust local models to thwart model poisoning attacks in federated learning. \\looseness=-1\n\n\\item Designing an attack-tolerant local meta update that helps discover noise-tolerant parameters for local models by utilizing a synthetically corrupted training set.\n\n\\item Introducing an attack-tolerant global knowledge distillation technique that efficiently aligns the local model's knowledge to the global data distribution while reducing the adverse effects of false information in the possibly-corrupted global model.\n\n\\item Showcasing that \\model{} can easily be applied in combination with any server-side defense strategies to enhance accuracy by 17-20\\% under poisoning attacks across various datasets. \n\\end{itemize}\n\n\n\\noindent \nThe code and implementation details are available at the following URL: \\url{https://github.com/deu30303/FedDefender/}.\\looseness=-1\n\n\n\n\n\n\n"
            },
            "section 3": {
                "name": "\\cutsectionup\n"
            },
            "section 4": {
                "name": "Related Work",
                "content": "\n",
                "subsection 4.0": {
                    "subsubsection 4.0.1": {
                        "name": "\\cutsubsectionup\n"
                    }
                },
                "subsection 4.1": {
                    "name": "Poisoning Attacks in Federated Learning",
                    "content": " \nIn recent years, there is a growing concern about security issues in machine learning, leading to the emergence of various model poisoning attacks~\\cite{chen2022federated,shejwalkar2022back}. \nFederated learning is particularly susceptible to these attacks because malicious users can easily access intermediate processes of the training and send poisoning updates to the central server~\\cite{lyu2020threats, wu2022fedattack}.\nAttackers can attack the training without access to the entire data distribution, as the data is decentralized and cannot be shared among clients~\\cite{fang2020local,sikandar2023detailed}.\n\nModel poisoning attacks can be divided into two categories: targeted attacks and untargeted attacks~\\cite{gu2017badnets,shafahi2018poison}.\nIn targeted attacks, malicious clients inject a backdoor into the central server so that any instance with the backdoor trigger will be classified as ``targeted'' without degrading the overall performance of the model. \nIn untargeted attacks, on the other hand, attackers aim to degrade model performance indiscriminately across all classes. \nA simple and widely used untargeted attack is the label-flipping attack~\\cite{xiao2012adversarial}, in which malicious clients corrupt local models by randomly flipping labels of training samples from original classes to other classes. \\looseness=-1\n\nRecently, some studies have used benign clients' partial information to improve the stealthiness of untargeted attack performance. \nFor example, Baruch \\textit{et al.} infer the standard deviation and the intensity factor based on benign client's updates and inject perturbed model updates accordingly~\\cite{baruch2019little}.\nFang \\textit{et al.} inject malicious updates by adding constant opposite direction noise estimated from benign clients' updates mean~\\cite{fang2020local}. \nShejwalkar \\textit{et al.} propose a model poisoning attack by calculating a dynamic malicious update opposite to the direction of benign model updates~\\cite{shejwalkar2021manipulating}. \n\nThis paper focuses on defending against \\textit{untargeted attacks} in federated learning using the client-side defense.  \\looseness=-1\n\n",
                    "subsubsection 4.1.1": {
                        "name": "\\cutsubsectionup\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Server-Side Defenses Against Poisoning Attacks in Federated Learning",
                    "content": " \nIn federated learning, dimension-wise averaging is a commonly used and effective method for aggregating local updates~\\cite{chen2016revisiting,mcmahan2017communication}. \nHowever, this naive averaging method is vulnerable to model poisoning attacks, which may succeed even with just a single malicious model update~\\cite{awan2021contra,blanchard2017machine}.\nTo address this threat, two server-side defense strategies have been proposed: coordinate-wise aggregation and detection-wise aggregation. Both aim to filter out malicious updates from adversary clients during the aggregation and ensure the integrity of the federated learning process.\n\n\\cutparagraphup\n\\noindent\\textbf{Coordinate-wise Aggregation.} \nCoordinate-wise aggregation introduces outlier-resistant operations instead of averaging. \nFor example, \\textit{Trimmed Mean} eliminates the largest and smallest values in each dimension, before computing the mean~\\cite{yin2018byzantine}.\n\\textit{Median} is another dimension-wise aggregation algorithm that computes the median of the updates in place of averaging~\\cite{xie2018generalized}. \nHowever, both approaches are ineffective when the data distribution is non-IID as they may overlook underrepresented updates. \nTo tackle this limitation, \\textit{ResidualBase} proposes a residual-based aggregation method~\\cite{fu2019attack}, which calculates residuals of each parameter in the local model using a repeated median estimator. \nThese residuals are then used to determine parameter confidence and detect false updates.\n \n\\cutparagraphup\n\\noindent\\textbf{Detection-wise Aggregation.} \nTo mitigate the adverse effect of malicious clients' updates, detection-wise aggregation adjusts learning rates based on the abnormality score of each local update.\n\\textit{Norm Bound} disregards clients with the norm of local updates being above a certain threshold by exploiting the observation that malicious clients often produce updates with a larger variance and norm than benign clients~\\cite{sun2019can}. \n\\textit{Krum} introduces a Byzantine-resilient algorithm assuming that malicious updates would be placed far from benign updates in the Euclidean space~\\cite{blanchard2017machine}.\nMore specifically, it selects a single local client update that is the most similar to its $n-m-2$ neighboring updates to produce the global model, where $m$ is the expected number of malicious local clients. \nKrum can be extended to Multi-Krum by iteratively running the algorithm to select multiple local updates, which shows better robustness than the original Krum. \n\\textit{FoolsGold} identifies grouped actions of targeted attacks based on the similarity score among local updates~\\cite{fung2020limitations}. \nUnlike benign clients, malicious clients in a targeted attack scenario share the common loss objective and tend to have a more similar update pattern than benign clients. \nIn this context, FoolsGold adjusts the learning rates of local models in proportion to the degree of diversity in each local update.\n\nWhile the client-side defense has been relatively under-investigated, FL-WBC proposed a client-side defense strategy specifically designed to mitigate backdoor attacks~\\cite{sun2021fl}. In contrast, our proposed method, \\model{}, stands as a pioneering client-side defense strategy against untargeted attacks in federated learning. By complementing existing server-side defense strategies, it significantly enhances the overall resilience against model poisoning attacks when used in conjunction with them. \\looseness=-1\\label{sec:method}\n"
                }
            },
            "section 5": {
                "name": "\\cutsectionup\n"
            },
            "section 6": {
                "name": "Problem Formulation",
                "content": "\n \nWe will outline model poisoning attacks in federated learning and defenses against them in this section. \nFigure~\\ref{fig:fedavg} highlights the basic flow of the training process in federated learning when malicious attackers are present. \\looseness=-1\n",
                "subsection 6.0": {
                    "subsubsection 6.0.1": {
                        "name": "\\cutsubsectionup\n"
                    }
                },
                "subsection 6.1": {
                    "name": "Federated Learning",
                    "content": "\n\\cutparagraphup \\noindent\n\\textbf{Overview.~}  Federated learning aims to optimize a global model, denoted as $F_{\\theta}$, by utilizing data distributed across $N$ local clients. \nWe denote the loss objective and local dataset in the $k$-th client by $\\mathcal{L}_{k}$ and $\\mathcal{D}_{k}$, respectively. The primary objective for training $\\theta$ can be expressed as: \\looseness=-1\n\\begin{align}\n    \\min_{\\theta} \\mathcal{L}(\\theta) = \\min_{\\theta} {\\sum_{k=1}^{N} |\\mathcal{D}_k| \\cdot \\mathcal{L}_k(\\theta, \\mathcal{D}_{k}) \\over \\sum_{k=1}^{N} |\\mathcal{D}_k|}.\n    \\label{eq:fed_objective}\n\\end{align}\n\nIn this paper, we use FedAvg~\\cite{mcmahan2017communication}, a widely used aggregation method as the default setting. FedAvg comprises four stages in each communication round:\n\\begin{enumerate}[leftmargin=*,label=\\large\\protect\\textcircled{\\footnotesize\\arabic*}]\n\\item Local model synchronization: At the beginning of each round $t$, a random subset of local clients is selected, and these clients synchronize their model parameters by downloading the current global model parameter $\\theta^t$.\n\n\\item Local model train: Each selected client $k$ trains the downloaded model with her private data $\\mathcal{D}_{k}$ for a few epochs and \ncomputes the local model update $\\Delta \\theta_k^t = \\theta_k^{t} - \\theta^{t}$.\n\n\\item Local model upload: The local model update $\\Delta \\theta_k^t$ is sent to the central server.\n\n\\item Global model aggregation: The global model is updated by averaging all local updates received from the clients at the central server, as shown in \\eqref{eq:global_aggregation}:\n\\end{enumerate}\n\\begin{align}\n    \\theta^{t+1} = \\theta^t + \\sum_{k=1}^{N} {|\\mathcal{D}_k| \\over |\\mathcal{D}|} \\Delta \\theta_k^t, \n    \\label{eq:global_aggregation}\n\\end{align}\nwhere $|\\mathcal{D}|=\\sum_{k=1}^N |\\mathcal{D}_k|$. This process is repeated for each communication round. \\looseness=-1\n\n\\cutparagraphup \\noindent\n\\textbf{Threat model.~} We hypothesize that all benign clients are honest and follow the proposed protocol, whereas malicious clients do not. Adversaries aim to disrupt the convergence and deteriorate the performance of the global model (i.e., an untargeted attack) by poisoning the local models of malicious clients. We also assume that the central server is trustworthy and performs the required task faithfully.\nRegarding the attacker's capability, we assume that adversaries can bypass the verification process and infiltrate the federated system as participating clients (e.g., during optimization). We consider two scenarios for the threat model: (1) attackers have no access to benign clients' information, and (2) attackers have partial access to benign client information, such as the standard deviation or average updates from benign clients.\n\n",
                    "subsubsection 6.1.1": {
                        "name": "\\cutsubsectionup\n"
                    }
                },
                "subsection 6.2": {
                    "name": "Federated Learning When Facing Poisoning Attacks",
                    "content": "\n\nPoisoning attacks rely on sending corrupted local updates to the central server, ultimately contaminating the trained global model.\nSeveral malicious objectives can be used for such attacks.\nFor instance, attackers may adopt the same loss objective as legitimate clients for optimization, but use deliberately corrupted training set $\\Tilde{\\mathcal{D}}_k$ to convey false information (i.e., $\\mathcal{L}_k (\\theta, \\Tilde{\\mathcal{D}}_k)$).\nAdditionally, attackers can also introduce an adversarial objective $\\Tilde{\\mathcal{L}}_k$ that disrupts the overall convergence (i.e., $\\Tilde{\\mathcal{L}}_k (\\theta, \\mathcal{D}_k)$). \\looseness=-1\n\n\nWe denote the subset of malicious clients as $\\mathcal{S}_m$ and the subset of benign clients as $\\mathcal{S}_b$ from the entire set of $N$ clients $\\mathcal{S}$ (i.e., $\\mathcal{S}$ = $\\mathcal{S}_m$ $\\cup$ $\\mathcal{S}_b$).\nTo defend against poisoning attacks, the optimal objective for training the global model weight $\\theta$ can be defined as:\n\\begin{align}\n   \\min_{\\theta} \\mathcal{L}^*(\\theta) &=  \\min_{\\theta} { \\sum_{k \\in \\mathcal{S}_b} |\\mathcal{D}_k| \\cdot \\mathcal{L}_k(\\theta, \\mathcal{D}_{k}) \\over \\sum_{k \\in \\mathcal{S}_b} |\\mathcal{D}_k|}.\n    \\label{eq:fed_optimal_objective}\n\\end{align}\n\nOne approach to achieving this objective is to implement a server-side robust aggregation strategy for the global model aggregation (i.e., stage \\textcircled{\\footnotesize{4}}), which aims to retain the updates from benign clients while discarding all false updates from malicious clients during the global aggregation phase.\nThe objective of robust aggregation can be expressed as follows: \\looseness=-1\n\\begin{align}\n\\theta^{t+1} &= \\theta^t + {{\\sum_{k=1}^N \\mathbbm{1}_{\\{k \\in S_{b}\\}}} \\cdot \\Delta \\theta_{k}^{t} \\over |S_b|},\n\\label{Eq:optimal_aggregation}\n\\end{align}\n\\noindent where $\\mathbbm{1}$ is an indicator function. $|\\mathcal{D}_k|$ is removed in order to mitigate the attack scenario where attackers try to manipulate the size of the local training data.\n\nIn contrast to existing defenses that focus on robust aggregation strategies, \\model{} aims to directly solve \\eqref{eq:fed_optimal_objective} by redesigning the local model train process (stage \\textcircled{\\footnotesize{2}}) and modifying the local updates of legitimate clients $\\Delta \\theta_{k}^{t}$. The detail is described in the next section. \\looseness=-1\n\n"
                }
            },
            "section 7": {
                "name": "Description of Our Method",
                "content": "\n%\\textbf{Overview. } \n\\model{} comprises the following two steps,% attack-tolerant training update for local models at each mini-batch $\\mathcal{X}$.\n\\begin{itemize}[leftmargin=*]\n    \\item \\noindent \\textbf{Step 1. Attack-Tolerant Local Meta Update (Section~\\ref{Sec:local_update})}:   \n    We train the benign local model $f_{\\theta_k}$ in a robust manner via meta-learning. The goal is to discover the model's parameters that produce accurate predictions even after it has been perturbed by noisy information.\n    To achieve this, we first generate a noisy synthetic label, then apply one gradient update to perturb the local network parameters. Afterward, the gradient for the perturbed network to predict the correct outputs is computed and utilized to optimize the original local model. \n\n    \\cutparagraphup\n    \\item \\noindent \\textbf{Step 2. Attack-Tolerant Global Knowledge Distillation (Section~\\ref{Sec:global_update})}: \n    If the global aggregation defense cannot block updates from malicious clients, the global model $F_{\\theta}$ is no longer trustworthy. \n    Given the meta-updated local model from the previous step, we apply global knowledge distillation to an auxiliary classifier using intermediate feature maps to neutralize the adverse impact on the contaminated global model $F_\\theta$. \n    Self-knowledge distillation is applied between the auxiliary classifier and the original classifier to further incorporate the global knowledge into the deeper layers of the local model. \\looseness=-1\n    %to improve the performance of the local model and incorporate the calibrated global knowledge. \\looseness=-1\n\n    \n\\end{itemize}\n\nFigure~\\ref{fig:main_model} present the overall pipeline of our proposed defense. Our technique can mitigate model poisoning attacks in federated learning. We will now provide a detailed description of each component of \\model{} next.\n\n",
                "subsection 7.0": {
                    "subsubsection 7.0.1": {
                        "name": "\\cutsubsectionup\n"
                    }
                },
                "subsection 7.1": {
                    "name": "Attack-Tolerant Local Meta Update",
                    "content": " \\label{Sec:local_update}\n\nIf the local model parameters $\\theta_{k}$ are only optimized with a traditional supervised loss term (e.g., cross-entropy), it is susceptible to overfitting and being influenced by noise generated by malicious clients. \nOur key idea is to learn noise-tolerant parameters $\\theta_{k}$ in a way that \"vaccinates\" the local model $f_{\\theta_{k}}$ against model poisoning with synthetic noise, drawing inspiration from recent meta-learning works~\\cite{li2019learning, finn2017model, ravi2017optimization}.\nThe proposed local meta-update replicates the training context with a model poisoning attack and makes the network less sensitive to noisy perturbations. \\looseness=-1\n\n\n\\cutparagraphup \\noindent\\textbf{Local model poisoning with synthetic noise.} \nLet us denote a mini-batch from local dataset $\\mathcal{D}_{k}$ as $\\mathcal{X} = \\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^B$, where $\\mathbf{x}$ is an input instance and $\\mathbf{y}$ is the corresponding one-hot label. \nWe want to generate synthetic batches $\\mathcal{\\Tilde{X}} = \\{(\\mathbf{x}_i, \\mathbf{\\Tilde{y}}_i)\\}_{i=1}^B$ with label noise to simulate the poisoning attack and perturb the local model.\n\nExcessive perturbation can overly deform the model's decision boundary, leading to degraded performance. \nTo mitigate this risk of severe deformation, we create more realistic noisy labels that resemble the distribution of $\\mathbf{y}$ by transferring labels from similar samples.\nFor each $(\\mathbf{x},\\mathbf{y})\\in\\mathcal{X}$, we calculate its feature representation $h_\\mathbf{x}$ from the model's backbone network.\nThen, a random instance from top-$k$ nearest neighbors in the representation space is randomly selected to replace the original label: \\looseness=-1\n\\begin{align}\n\\mathcal{\\Tilde{X}} &= \\{(\\mathbf{x}, \\mathbf{\\Tilde{y}}) | (\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{X} \\text{ and }\\mathbf{\\Tilde{y}} = \\text{Sample}_{\\mathbf{y}}(\\mathcal{N}_{k}(\\mathbf{x}, \\theta_k))  \\},\n%\\mathcal{\\Tilde{X}} &= \\text{Perturb}(\\mathcal{X})\n\\end{align}\nwhere $\\mathcal{N}_{k}(\\mathbf{x}, \\theta)$ indicates the set of $k$-nearest neighbors of $\\mathbf{x}$ from the representation space made by $f_\\theta$. \n$\\text{Sample}_{\\mathbf{y}}(\\cdot)$ is the random selector function to extract the synthetic label.\nSince the nearest neighbors $\\mathcal{N}_{k}(\\mathbf{x}, \\theta)$ are likely to share the same label as $\\mathbf{x}$, this can generate noise within an acceptable range (i.e., 5-20\\% error rate).\n\nGiven the synthetic batch samples $\\mathcal{\\Tilde{X}}$, we perturb the local model parameter $\\theta_{k}$ using one gradient descent step:\n\\begin{align}\n\\mathcal{L}_{Perturb} &= {1 \\over |\\mathcal{\\Tilde{X}}|} \\sum_{{\\mathbf{x}}, {\\mathbf{\\Tilde{y}}} \\in \\mathcal{\\Tilde{X}}} H(\\mathbf{\\Tilde{y}}, f_{\\theta_k}({\\mathbf{x}})) \\\\\n\\Tilde{\\theta}_k &\\leftarrow \\theta_k - \\eta \\nabla_{\\theta_k}  \\mathcal{L}_{Perturb},\n\\end{align}\nwhere $H(p, q)$ is the conventional cross entropy between $p$ and $q$, and $\\eta$ is a learning rate. \\looseness=-1\n\n\\cutparagraphup \\noindent\\textbf{Local model correction with meta update.}\nTo discover the model parameter that is less susceptible to noisy training by malicious clients, we propose a meta update to guide the model training.\nGiven the perturbed model $\\Tilde{\\theta}_k$, we optimize a classification loss $\\mathcal{L}_{Meta}$ (\\eqref{eq:meta_loss}) for one gradient descent step to encourage the perturbed local model $f_{\\Tilde{\\theta}_k}$ to give\ncorrect predictions from $\\mathcal{X}$.\nNote that the optimization process is applied to the parameters of the original local model $\\theta_{k}$ (\\eqref{eq:meta_update}), although the loss calculation is based on the perturbed model's parameters $\\Tilde{\\theta}_k$ (\\eqref{eq:meta_loss}).\nThis prevents the local model from being contaminated by synthetic noise during the training.\n\\begin{align}\n\\mathcal{L}_{Meta} &= {1 \\over |\\mathcal{X}|} \\sum_{{\\mathbf{x}}, {\\mathbf{y}} \\in \\mathcal{X}} H(\\mathbf{y}, f_{\\Tilde{\\theta}_k}({\\mathbf{x}})) \\label{eq:meta_loss} \\\\\n\\theta_k &\\leftarrow \\theta_k - \\eta \\nabla_{\\theta_k}  \\mathcal{L}_{Meta} \\label{eq:meta_update}\n\\end{align} \\looseness=-1\n\n\n"
                },
                "subsection 7.2": {
                    "name": "Attack-Tolerant Global \nKnowledge Distillation",
                    "content": "\\label{Sec:global_update}\nGiven the meta-updated network $f_{\\theta_{k}}$, our next step is to enhance the performance of the refined local model $\\theta_k$ through global knowledge distillation. \nIn the case of non-IID settings, learning relying on local data leads to representation bias, as local data distribution differs from the global distribution. \nRegularization techniques, including global knowledge distillation, can be employed to control the local updates, thus preventing the local drift fallacy~\\cite{han2022fedx, zhuang2020performance,wu2022communication}.\n\nIn the scenario of a model poisoning attack, however, the credibility of the global model $F_{\\theta}$ can be compromised if the global defense fails to block malicious updates from hostile clients. \nThis can lead to suboptimal results when the $F_{\\theta}$ is perturbed.\nWe propose attack-tolerant global knowledge distillation to mitigate the adverse effects of a potentially corrupted global model $F_{\\theta}$. \\looseness=-1\n\n\n\\cutparagraphup \\noindent\\textbf{Refined knowledge distillation with an auxiliary network.} \nWe start with the fact that a deeper layer in deep neural networks is much easier to overfit to noise (i.e., memorization) due to the inherent nature of gradient descent-based optimization~\\cite{kundu2021analyzing, baldock2021deep, stephenson2021geometry}.\nIn this context, \\model{} transfers global knowledge to a shallow intermediate part of the local model to reduce the adverse effect of false information.\nWe attach an auxiliary classifier on top of intermediate layers to produce a new model parameter $\\phi_k$, and perform knowledge distillation on it (i.e., $\\theta_{k}$ and $\\phi_k$ share the shallow section of the entire network). Given an input data point $\\mathbf{x}$ from mini-batch $\\mathcal{X}$, we regard the output probability from the global model $F_\\theta$ as a target to train the local model parameter with auxiliary classifier $\\phi_k$. The global knowledge distillation loss is defined as follows:\n \\begin{align}\n &F_\\theta (\\mathbf{x}, \\tau) = \\text{Sharpen}(F_\\theta(\\mathbf{x}), \\tau) \\\\\n    &\\mathcal{L}_{Global} = {1 \\over |\\mathcal{X}|} \\sum_{{\\mathbf{x}}, {\\mathbf{y}} \\in \\mathcal{X}} H(F_{\\theta}(\\mathbf{x}, \\tau), f_{\\phi_{k}}({\\mathbf{x}})),\n\\end{align} \nwhere Sharpen($\\cdot$) is the function that adjusts the confidence with sharpening temperature $\\tau$~\\cite{hinton2015distilling}.\n\n\nSince the global model's prediction is still not reliable in the attack scenario, we further introduce a simple strategy to refine target probabilities for robust knowledge distillation.\nGiven a data point ($\\mathbf{x}, \\mathbf{y}$) from $\\mathcal{X}$, we calculate the scale coefficient $\\alpha$, which represents the cosine similarity between original label $\\mathbf{y}$ and the global model prediction $F_{\\theta}(\\mathbf{x})$.\n\\model{} then generates a refined label $\\mathbf{\\hat{y}}$ by applying a linear sum to calibrate the global information as follows:\n\\begin{align}\n\\mathbf{\\hat{y}} = (1 - \\alpha) \\cdot \\mathbf{y} + \\alpha\\cdot F_{\\theta}(\\mathbf{x}, \\tau). \\label{eq:refine_label}\n\\end{align}\nIf the global model's prediction is well aligned with the ground-truth labels, the scale coefficient becomes large and the final label weighs more on the global model's knowledge.\nMeanwhile, if the prediction is far different from the truth, we neglect the global model's information.\nWe construct a modified batch $\\mathcal{\\hat{X}}$ with the refined labels $\\mathbf{\\hat{y}}$, and transfer the global knowledge to $\\phi_k$ with this batch (\\eqref{eq:global_kd_real}).\n\\begin{align}\n    \\mathcal{L}_{Global} = {1 \\over |\\mathcal{\\hat{X}}|} \\sum_{{\\mathbf{x}}, {\\mathbf{\\hat{y}}} \\in \\mathcal{\\hat{X}}} H(\\mathbf{\\hat{y}}, f_{\\phi_{k}}({\\mathbf{x}})) \\label{eq:global_kd_real}\n\\end{align} \n\\looseness=-1\n\n\n\\cutparagraphup \\noindent\\textbf{Auxiliary self-knowledge distillation.} \nWe propose an additional design to improve the deeper layers of the local model, whose weights are excluded from global knowledge distillation, in learning calibrated global knowledge.\nMotivated by the notion of self-knowledge distillation~\\cite{zhang2019your, park2021improving,han2020mitigating}, we extract knowledge from the local model using an auxiliary classifier $\\phi_k$ and transfer it back to the original local model $\\theta_k$. \nBy doing so, we can distill the calibrated global knowledge learned from the refined label (\\eqref{eq:refine_label}) to the latter part of the model, effectively regularizing the entire model.\nMoreover, the output from the auxiliary head can be considered as a different view (i.e., augmentation) of the same instance.\nMaximizing the agreement between these two views can further enhance the model training.\nThe self-knowledge distillation loss between the auxiliary classifier $f_{\\phi_k}$ and the original model $f_{\\theta_k}$ is defined as follows.\n\\begin{align}\n\\mathcal{L}_{Self} &= {1 \\over |\\mathcal{X}|} \\sum_{{\\mathbf{x}}, {\\mathbf{y}} \\in \\mathcal{X}} KL( f_{\\theta_k}({\\mathbf{x}, \\tau}) || f_{\\phi_k}({\\mathbf{x}, \\tau})), \n\\end{align}\n\\noindent where $KL(p||q)$ is a Kullback–Leibler (KL) divergence between two probabilities $p$ and $q$.\nFinally, the final loss for our global knowledge distillation is given in \\eqref{eq:our_global_kd}.\n\\begin{align}\n \\mathcal{L}_{KD} &= \\mathcal{L}_{Global} + \\mathcal{L}_{Self} \\label{eq:our_global_kd}\n\\end{align}\n\nThis global knowledge distillation loss is optimized in conjunction with original cross-entropy loss to train the \\model{} (\\eqref{eq:total_training}).\n\\begin{align}\n&\\mathcal{L}_{CE} = {1 \\over |\\mathcal{X}|} \\sum_{{\\mathbf{x}}, {\\mathbf{y}} \\in \\mathcal{X}} H(\\mathbf{y}, f_{\\theta_k}({\\mathbf{x}})) \\\\\n&\\mathcal{L}_{total} = \\mathcal{L}_{CE} + \\mathcal{L}_{KD} \\\\\n&\\theta_k \\leftarrow \\theta_k - \\eta \\nabla_{\\theta_k}  \\mathcal{L}_{total} \\label{eq:total_training}\n\\end{align}\n\\looseness=-1\n\n\n"
                }
            },
            "section 8": {
                "name": "\\cutsectionup\n"
            },
            "section 9": {
                "name": "Experiments",
                "content": "\nWe evaluate the robustness of \\model{} under various attacks on multiple datasets and compare it with server-side global aggregation baselines. We also examine how the model components and hyperparameters affect the overall performance. \\looseness=-1\n\n",
                "subsection 9.0": {
                    "subsubsection 9.0.1": {
                        "name": "\\cutsubsectionup\n"
                    }
                },
                "subsection 9.1": {
                    "name": "Experimental Setup",
                    "content": "\n\\cutparagraphup\n\\noindent \\textbf{Data settings.} We use four image classification benchmark datasets in our experiments. The first two are CIFAR-10 and CIFAR-100, each containing 60,000 images of 32x32 pixels~\\cite{krizhevsky2009learning}. \nCIFAR-10 comprises 10 classes, such as airplanes, cats, and dogs, while CIFAR-100 comprises 100 classes. The third dataset is TinyImageNet, which comprises 100,000 images of 64x64 pixels and 200 classes. \nThe fourth is the Federated Extended MNIST (FEMNIST) dataset, which contains 805,263 images of handwriting digits/characters of the size of 28x28 pixels~\\cite{caldas2018leaf}. \n\n\nThe Dirichlet distribution is used to simulate the non-IID characteristics of CIFAR-10, CIFAR-100, and TinyImageNet datasets on federated learning. \nWe denote the Dirichlet distribution by $Dir(N,\\beta)$, where $N$ is the total number of local clients and $\\beta$ represents the concentration parameter controlling the degree of non-IIDness of decentralized local data distributions. \nThe probability $p_{k,j}$ is sampled from $Dir(N,\\beta)$ and assigns the proportion of class $j$ in $k$-th client's dataset. \nWith this non-IID distribution strategy with a low $\\beta$ value, local clients will have a disparate class distribution from one another. The default values for $N$ and $\\beta$ are set to 20 and 0.5, respectively.\nIn the case of FEMNIST, on the other hand, the writers of digits/characters in the data are randomly distributed to $N$ clients. $N$ is set to 20 for FEMNIST as well.\n\\looseness=-1\n\n\n\n\\cutparagraphup\n\\noindent \\textbf{Implementation details.} \nThe number of communication rounds is set to 100, with 1 epoch per round for all federated learning experiments. Half of the clients (i.e., $10=N/2$ clients) are randomly selected in each round for communication to make the federated setting more realistic. ResNet18~\\cite{he2016deep} is used as the default backbone network following the literature in federated learning~\\cite{li2021model,han2022fedx}. \nThe learning rate ($\\eta$), weight decay, and momentum for the SGD optimizer are set to 0.01, 1e-5, and 0.9, respectively. \nThe batch size is set to 64. \nFor knowledge distillation objectives, the temperature $\\tau$ is set to 2.\nRandom crop, color jitter, and random horizontal flip are used for data augmentations in local model training. \\looseness=-1\n\n\n\\cutparagraphup\n\\noindent\\textbf{Model poisoning attack scenario.}\nWe divide a total of $N$ clients into benign and malicious clients with a ratio of 8:2 (i.e., attacker ratio = 20\\%) as the default setting. \nFor example, given $N$ to be 20, we assign four clients to play the adversarial role from a pool of 20 clients. Because we randomly select 10 out of 20 clients in every communication round, the number of malicious clients in each round may vary.\nWe consider two representative scenarios of an untargeted poisoning attack on the federated learning framework as follows: \n\\looseness=-1\n\n\n\n\\cutparagraphup\n\\noindent\\textbf{Scenario-1) No access to benign clients' information:} \nThis scenario assumes that malicious clients cannot obtain any information about benign clients. \nWe consider the label flipping attack as the model poisoning attack in this case, as it does not require prior knowledge of the training data or the benign clients' update information. For instance, in CIFAR-10, a class `dog' image may have a random label of `ship' or `horse'.\nThis false model update is sent to the central server after training the local network with randomly flipped noisy labels.  \\looseness=-1\n\n\n\\cutparagraphup\n\\noindent\\textbf{Scenario-2) Partial access to benign client information:} \nThis scenario represents a more advanced attack in which malicious clients can access local updates from benign clients. \nUpdates from malicious clients are computed using the statistics of benign updates.\nWe consider three variations:\n\\begin{enumerate}\n\\item \\textsf{LIE} algorithm infers both the standard deviation and intensify factors from the updates of benign clients. Then, the perturbed noise is generated by multiplying the two to deceive the server-side defense~\\cite{baruch2019little}.\n\\item \\textsf{STAT-OPT} injects a static inverse unit vector into the central server, computed as the opposite direction of the noise from the mean of the benign clients' updates~\\cite{fang2020local}.\n\\item \\textsf{DYN-OPT} injects     dynamically perturbed noise, where the maximum distance from any other updates is bounded by the maximum distance between any two benign updates~\\cite{shejwalkar2021manipulating}.\n\\end{enumerate} \\looseness=-1\n",
                    "subsubsection 9.1.1": {
                        "name": "\\cutsubsectionup\n"
                    }
                },
                "subsection 9.2": {
                    "name": "Defense Performance Evaluation",
                    "content": "\n\\cutparagraphup\n\\noindent\\textbf{Baselines.} \nWe have implemented six server-side defense strategies as baselines for untargeted attacks: (1) \\textsf{No Defense} refers to the typical Federated Averaging (FedAvg) algorithm without any robust aggregation strategy. \n(2) \\textsf{Median} is an aggregation algorithm that computes the median of each dimension of the updates rather than the average.\n(3) \\textsf{Trimmed Mean} is an aggregation algorithm that computes the mean of local updates by removing the largest and smallest values.\n(4) \\textsf{Norm Bound} is an algorithm that removes updates if the norm of the local update is above a certain threshold.\n(5) \\textsf{Multi-Krum} is an algorithm that iteratively selects local updates using the Krum method, which selects a single honest client by calculating the Euclidean distance between the client's update and the updates of its neighbors.\n(6) \\textsf{ResidualBase} is an algorithm that computes parameter confidence using the residuals of each parameter from the repeated mean. \n\n\\cutparagraphup\n\\noindent\\textbf{Evaluation.} \nAll models are evaluated using the same attacks and experimental settings (e.g., client pool, ratio of attacker numbers, attack strategy, number of local epochs, optimizer, and communication rounds) to ensure a fair comparison.  \nWe report the top-1 classification accuracy on the test set, including the last and best accuracy. \nThe reported accuracy (labeled \\textsf{Last} and \\textsf{Best}) is calculated by averaging the accuracy of the last and best five rounds, respectively. \n\n\n\n\n\n\n\\cutparagraphup\n\\noindent\\textbf{Result.} \nTable~\\ref{tab:main_result} compares the performance of defense algorithms against the label flipping attack described in Scenario 1.\nWe can see that \\model{} consistently improves the performance when applied to existing server-side defense algorithms. \nDespite unfiltered malicious updates from failures in server-side defense or the absence of a defense strategy (i.e., \\textsf{No Defense}), incorporating \\model{} can lower the risk of performance degradation.\n\nTable~\\ref{tab:main_result2} reports the results against advanced attacks in Scenario 2, where malicious clients have access to information from benign clients. \n\\model{} consistently outperforms the baselines across all cases. \nServer-side defenses alone can lead to removing updates of benign users during training, which can cause the performance to drop. \nFor example, existing server-side defense strategies perform poorly in LIE cases, even compared to \\textsf{No Defense}.\nHowever, adding \\model{} can alleviate this issue and improve performance, highlighting the effectiveness of our approach.\n\n\n\n",
                    "subsubsection 9.2.1": {
                        "name": "\\cutsubsectionup\n"
                    }
                },
                "subsection 9.3": {
                    "name": "Component Analyses",
                    "content": "\\cutparagraphup\n\\noindent \\textbf{Ablation study.~} \nWe have conducted an ablation study by removing and altering the key components.\nWe compare the following variations:  \\looseness=-1\n\\begin{itemize}\n\\item Full Components: Include all components. \n\\item Step 1 Only: Only using the local meta update component (Section~\\ref{Sec:local_update}), with Step 2 removed.\n\\item Altering Step 2: Altering knowledge distillation (Section~\\ref{Sec:global_update}) with a conventional alternative, which naively distills global knowledge without calibration.\n\\item Sever-Side Only: A model that uses only the server-side defense method without any attack-tolerant local updates. \n\\end{itemize}\n\\looseness=-1\n \n\n\n\nFigure~\\ref{fig:ablation} confirms that the full model provides the best performance among the ablations. Due to space limitations, we report results when our method is used along with \\textsf{Multi-Krum}.  \nAblations using Step 1 only shows a more robust performance than using the server-side defense strategy alone.\nAltering Step 2 to a conventional knowledge distillation, on the other hand, degrades performance compared with entirely removing Step 2.\nThis is likely due to corrupted information in the global model, $F_\\theta$, and demonstrates the need for a carefully designed attack-tolerant knowledge distillation.   \n\n\n\n\n\n\n\\cutparagraphup\n\\noindent\n\\textbf{Comparison with alternative global regularization approaches.}\nThe proposed global knowledge distillation (Section~\\ref{Sec:global_update}) may be replaced with existing global regularization techniques that don't consider malicious attacks. We have tested the three alternatives that replace the global knowledge distillation with existing methods FedProx~\\cite{li2020fedprox}, Scaffold~\\cite{karimireddy2020scaffold}, or Moon~\\cite{li2021model} on top of the local meta update module. \nThese existing methods also regularize the local model and address the local drift fallacy caused by the disagreement between local and global data distribution. \n\nTable~\\ref{tab:global_ablation} compares our method with these alternatives over two different global aggregation strategies: simple averaging (i.e., No Defense) and Multi-Krum.\n\\model{} outperforms in both the last and best accuracy across both defense cases. \nWhen the global model is heavily perturbed (i.e., No Defense), the existing global knowledge distillation method experiences a significant drop in accuracy compared with when the global model is less perturbed (i.e., Multi-Krum). \nWe also find that our proposed global knowledge distillation leads to smaller accuracy fluctuation across training rounds compared with other methods (see Figure~\\ref{fig:global_ablation} in the Appendix).\nThese results demonstrate the critical role that our attack-tolerant global knowledge distillation plays.\n\n\n\n\n\\cutparagraphup\n\\noindent\\textbf{Qualitative analyses.} \nWe investigate how well \\model{} discovers the attack-tolerant parameters for benign local models. \nWe are inspired by the experiment to visualize the loss landscape in~\\cite{visualloss} and follow it as follows. \nWe add random direction perturbations to the model parameter (for example, for two random directions X and Y in Fig.~\\ref{fig:acc_surface}) and examine how the accuracy changes.\nResults in Fig.~\\ref{fig:acc_surface} indicate that our local meta update (step 1) produces a higher and smoother accuracy surface compared with when this step is missing. \nResistance to random parameter perturbations suggests that, when utilizing FedDefender, each benign client can find a solution with flat minima in the loss curve within the parameter space. \n\n\n\n\n\nNext, we examine how well the correct global knowledge is conveyed to the local model by \\model{} under poisoning attacks.\nBased on the experiment, we may exactly compute a hypothetical global model that is immune to attacks (i.e., a clean global model) by removing all adversarial clients in the global aggregation phase. \nMathematically, the corrupted and clean global models in round $t$ are defined as follows:\n\\begin{align}\n\\theta_{\\text{corrupted}}^{t+1} &= \\theta^t + {{\\sum_{k=1}^N \\mathbbm{1}_{\\{k \\in S_{b} \\cup S_{m}\\}}} \\cdot \\Delta \\theta_{k}^{t} \\over {|S_b| + |S_m|}} \\label{eq:corrupted_model}\n\\end{align}\n\\vspace{-3mm}\n\\begin{align}\n\\theta_{\\text{clean}}^{t+1} &= \\theta^t + {{\\sum_{k=1}^N \\mathbbm{1}_{\\{k \\in S_{b}\\}}} \\cdot \\Delta \\theta_{k}^{t} \\over |S_b|}, \\label{eq:clean_model} \n\\end{align}\nwhere $\\mathcal{S}_b$ and $\\mathcal{S}_m$ represent the sets of benign and malicious clients, respectively.\nWe compute the similarity between the local models with two global models: one is corrupted and the other is clean. \nAverage cosine similarity between model predictions over the same dataset is used to evaluate the model similarity.\n\n\nFigures~\\ref{fig:pertur_class} and~\\ref{fig:clean_class} show the relationship between local and global models for each class in CIFAR-10.\nThe results show that local models in \\model{} have a substantially higher similarity to the clean global model (0.64) compared with the corrupted global model (0.53). \nIn contrast, a vanilla knowledge distillation does not guarantee the same ability to filter out contaminated knowledge from the global model (clean: 0.55 vs corrupted: 0.61). \nThis suggests that our method can effectively distill more correct and calibrated knowledge from the corrupted global model.\n\n\n\\cutparagraphup \\noindent\n\\textbf{Robustness test.~} \nTo test robustness, we consider different experimental settings and vary the key hyperparameters.\nThese include (a) the number of participating clients $N$, (b) the percentage of attackers that infiltrate the system $p_a$, and (c) the level of non-IIDness in the distributed local dataset, which is controlled by the beta parameter $\\beta$ in Dirichlet distribution. A lower beta parameter leads to higher non-IIDness. Table~\\ref{tab:robustness} shows the results for using Multi-Krum as a baseline aggregation strategy on CIFAR-10. \n\nThe complexity of data training increases as we increase the number of clients, the percentage of attackers, and the level of non-IIDness. Irrespective of these changes, \\model{} consistently demonstrates significant performance improvements.\n\n"
                }
            },
            "section 10": {
                "name": "\\cutsectionup",
                "content": "\n\n"
            },
            "section 11": {
                "name": "Conclusion",
                "content": "\nThis paper proposed \\model{}, a client-side approach to improve existing server-side defense strategies against model poisoning attacks. \nWe proposed two attack-tolerant training components for benign clients: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation.\nWith these components, our method helps mitigate model poisoning attacks and produces more trustworthy results, even when server-side defenses fail to filter out malicious updates. \nAs a result, our method has achieved a meaningful robustness improvement against various model poisoning attacks when used in conjunction with existing server-side defense strategies.\nWe hope that our technique can serve as a foundation for further research in client-side robust federated learning. \\looseness=-1\n\n"
            },
            "section 12": {
                "name": "Acknowledgement",
                "content": "\nThis research was supported by the Institute for Basic Science (IBS-R029-C2), Microsoft Research Asia, the Potential Individuals Global Training Program (2022-00155958), and the IITP grant (RS-2023-00216011) by the Ministry of Science and ICT in Korea.\n\n\n\n\\newpage\n\\balance\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{main}\n\n\\newpage\n\n%\\newpage\n\\appendix\n"
            },
            "section 13": {
                "name": "APPENDIX",
                "content": "\n",
                "subsection 13.1": {
                    "name": "Training Details",
                    "content": "\nThe overall training process is presented in Algorithm~\\ref{algo:overall}, and the \\model{} is open-sourced at \\url{https://github.com/deu30303/FedDefender}.\n\n\n\nThe default backbone network for our experiments is ResNet18~\\cite{he2016deep}, following prior work in federated learning~\\cite{li2021model,han2022fedx}.\nFor the attack-tolerant local meta update (Step 1), we use a first-order approximation to speed up computation. The number of nearest neighbors, k, used to perturb the label is set to 10.\nIn attack-tolerant global knowledge distillation (Step 2), we attach an auxiliary classifier to the output of the second residual block layer's feature map. \\looseness=-1\n\n\n\nWe followed the details from original works for implementing server-side defense baselines. \nWhen deciding hyperparameters of Multi-Krum and Norm Bounding algorithms, we assumed that the central server already knows the upper bound of attacker numbers.\nThe two hyperparameters in ResidualBase algorithm, confidence interval and clipping threshold, are set to 2.0 and 0.05 respectively.\n\n%In Scenario-1, it is assumed that the attacker has no prior knowledge of the benign clients. \nIn our experiment, Scenario-2 assumes a central-server aggregation agnostic attack, where malicious users have access to benign clients' update information. We evaluate using three attack algorithms: LIE, STAT-OPT, and DYN-OPT.\nFor LIE, we set the intensity factor to 0.3.\nFor DYN-OPT, we set the initial intensity factor and threshold to 10 and $1e-5$, respectively. The optimal intensity factor value is then determined by repeatedly finding the optimal value  between the smallest and largest values, until the intensity factor change  falls under the threshold.\n\n\n\n\n\n\n"
                },
                "subsection 13.2": {
                    "name": "Further Results on Defense Performance",
                    "content": "\n% \\cutparagraphup\n\n% \\cutparagraphup\n\\noindent\n\\textbf{Comparison with other possible defense strategy.} We have conducted additional comparison experiments with more recent baselines, including \\cite{sun2021fl, karimireddybyzantine, pillutla2022robust} under a label flipping attack scenario (Scenario 1). FL-WBC is a client-side defense, while the others are server-side defenses. Based on the experimental results in Table~\\ref{tab:recent_result}, we can confirm that our proposed model still offers a performance improvement for the local model when integrated with other server-side defenses. It is important to note that FL-WBC was originally designed exclusively for backdoor attacks, resulting in smaller performance gains compared with our method in untargeted attack scenarios. \n\n\\noindent\n\\textbf{Detection recall of Multi-Krum.} Figure~\\ref{fig:krum_recall} shows the detection recall kernel density plot of the Multi-Krum algorithm, plotted against the level of non-IID data distribution. \nAs non-IID data distribution becomes more extreme (i.e., lower values of $\\beta$), updates from benign clients become increasingly diverse.\nThis makes it harder for server-side defenses to identify malicious client updates.\nIn this light, we propose a client-side defense strategy, \\model{}, to achieve additional robustness and account for inherent model poisoning attacks during training. \\smallskip\n\n\\noindent\n\\textbf{Comparison with other global regularization approaches across communication rounds.} \nOur proposed architecture introduces an attack-tolerant global knowledge distillation technique to mitigate the negative effects of a malicious global model during training. As an alternative to this method, one may also use existing approaches, such as FedProx~\\cite{li2020fedprox}, Scaffold~\\cite{karimireddy2020scaffold}, or Moon~\\cite{li2021model}, in conjunction with our local meta update module.\nFigure~\\ref{fig:global_ablation} compares the classification accuracy of our method and alternative global regularization methods across communication rounds, without the use of any server-side defense. The models with other global regularization methods exhibit significant fluctuations in accuracy when a large number of attackers participate in each round. In contrast, our method's accuracy remains stable and consistent throughout the rounds, even when the number of participating attackers is high. \n\n\n\n\n\\noindent \\textbf{Model comparison across communication rounds.}\nThe addition of \\model{} results in significant improvement in robustness against model poisoning attacks, compared to relying on server-side defense only.\nFigure~\\ref{fig:training_round} plots the accuracy changes throughout training rounds, including six baselines: No Defense, Median, Trimmed Mean, Norm Bound, Multi-Krum, and ResidualBase. \n\n\n\n\n% \\cutparagraphup\n\n\n\\noindent\n% \\textbf{Model comparison via confusion matrix.}\n% Figure~\\ref{fig:confusion_matrix} contrasts the confusion matrices of Multi-Krum and Multi-Krum+\\model{} for the CIFAR-10 dataset.\n% Multi-Krum alone results in a degradation of performance across all classes.\n% However, the confusion matrix for Multi-Krum+\\model{} reveals a greater concentration of correct predictions along the diagonal, indicating the improved robustness brought by \\model{}. \\smallskip \n\n\n\n\n\n\n\n\n\n\\smallskip\n\n\n\n\n\n\n"
                }
            }
        },
        "figures": {
            "fig:fedavg": "\\begin{figure*}[t!]\n\\centerline{\n   \\includegraphics[width=0.85\\linewidth]{figures/problem_statement_v3.png}}\n   \\caption{Illustration of federated learning under poisoning attacks. Attackers attempt to send malicious updates to the central server to corrupt the aggregated model. Unlike existing works that primarily focus on robust aggregation on the server side (stage \\textcircled{\\footnotesize{4}}), \\model{} focuses on training robust local models by benign clients (stage \\textcircled{\\footnotesize{2}}) to protect against malicious updates from adversaries. \\looseness=-1\n   } \n\\label{fig:fedavg}\n\\end{figure*}",
            "fig:main_model": "\\begin{figure*}[t!]\n\\centerline{\n      \\includegraphics[width=1\\linewidth]{figures/model_v2_horizontal.pdf}}\n      \\caption{The overall design of \\model{} framework. \\model{} comprises two steps: (1) attack-tolerant local meta-update, which finds local model parameters that are less prone to overfitting to noise, and (2) attack-tolerant global knowledge distillation, which aims to convey correct global knowledge from a potentially contaminated global model, regularizing the local model to mitigate data bias. \\looseness=-1\n      } \n\\label{fig:main_model}\n\\end{figure*}",
            "fig:ablation": "\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figures/ablation_study_new_name.pdf}\n\\cutcationup\n\\caption{Performance comparison of ablations across communication rounds on CIFAR-10. Removing or altering any component results in a performance drop.}\n\\vspace{-3mm}\n    \\label{fig:ablation}\n\\end{figure}",
            "fig:krum_recall": "\\begin{figure}[b!]\n\\centering\n\\includegraphics[width=0.7\\columnwidth]{figures/krum_recall.pdf}\n    \\vspace*{-0.12in}\n    \\caption{Detection recall plot of Multi-Krum with different levels of non-IID. The higher level of non-IID in data leads to the lower detection performance of the server-side defense strategy.\\looseness=-1}\n    \\label{fig:krum_recall}\n\\end{figure}",
            "fig:global_ablation": "\\begin{figure}[b!]\n\\centering\n\\includegraphics[width=0.7\\columnwidth]{figures/ablation_study_global_v2.pdf}\n\\vspace*{-0.15in}\n\\caption{Performance comparison with other possible global\nregularization methods across communication rounds. The proposed distillation method worked the best for all rounds.} \n    \\label{fig:global_ablation}\n\\end{figure}",
            "fig:training_round": "\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=1.95\\columnwidth]{figures/training_round.pdf}\n    \\cutcationup\n    \\caption{Performance comparison between server-side defense baselines and \\model{}-enhanced versions across rounds.}\n    \\label{fig:training_round}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n    \\min_{\\theta} \\mathcal{L}(\\theta) = \\min_{\\theta} {\\sum_{k=1}^{N} |\\mathcal{D}_k| \\cdot \\mathcal{L}_k(\\theta, \\mathcal{D}_{k}) \\over \\sum_{k=1}^{N} |\\mathcal{D}_k|}.\n    \\label{eq:fed_objective}\n\\end{align}",
            "eq:2": "\\begin{align}\n    \\theta^{t+1} = \\theta^t + \\sum_{k=1}^{N} {|\\mathcal{D}_k| \\over |\\mathcal{D}|} \\Delta \\theta_k^t, \n    \\label{eq:global_aggregation}\n\\end{align}",
            "eq:3": "\\begin{align}\n   \\min_{\\theta} \\mathcal{L}^*(\\theta) &=  \\min_{\\theta} { \\sum_{k \\in \\mathcal{S}_b} |\\mathcal{D}_k| \\cdot \\mathcal{L}_k(\\theta, \\mathcal{D}_{k}) \\over \\sum_{k \\in \\mathcal{S}_b} |\\mathcal{D}_k|}.\n    \\label{eq:fed_optimal_objective}\n\\end{align}",
            "eq:4": "\\begin{align}\n\\theta^{t+1} &= \\theta^t + {{\\sum_{k=1}^N \\mathbbm{1}_{\\{k \\in S_{b}\\}}} \\cdot \\Delta \\theta_{k}^{t} \\over |S_b|},\n\\label{Eq:optimal_aggregation}\n\\end{align}",
            "eq:5": "\\begin{align}\n\\mathcal{\\Tilde{X}} &= \\{(\\mathbf{x}, \\mathbf{\\Tilde{y}}) | (\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{X} \\text{ and }\\mathbf{\\Tilde{y}} = \\text{Sample}_{\\mathbf{y}}(\\mathcal{N}_{k}(\\mathbf{x}, \\theta_k))  \\},\n%\\mathcal{\\Tilde{X}} &= \\text{Perturb}(\\mathcal{X})\n\\end{align}",
            "eq:6": "\\begin{align}\n\\mathcal{L}_{Perturb} &= {1 \\over |\\mathcal{\\Tilde{X}}|} \\sum_{{\\mathbf{x}}, {\\mathbf{\\Tilde{y}}} \\in \\mathcal{\\Tilde{X}}} H(\\mathbf{\\Tilde{y}}, f_{\\theta_k}({\\mathbf{x}})) \\\\\n\\Tilde{\\theta}_k &\\leftarrow \\theta_k - \\eta \\nabla_{\\theta_k}  \\mathcal{L}_{Perturb},\n\\end{align}",
            "eq:7": "\\begin{align}\n\\mathcal{L}_{Meta} &= {1 \\over |\\mathcal{X}|} \\sum_{{\\mathbf{x}}, {\\mathbf{y}} \\in \\mathcal{X}} H(\\mathbf{y}, f_{\\Tilde{\\theta}_k}({\\mathbf{x}})) \\label{eq:meta_loss} \\\\\n\\theta_k &\\leftarrow \\theta_k - \\eta \\nabla_{\\theta_k}  \\mathcal{L}_{Meta} \\label{eq:meta_update}\n\\end{align}",
            "eq:8": "\\begin{align}\n &F_\\theta (\\mathbf{x}, \\tau) = \\text{Sharpen}(F_\\theta(\\mathbf{x}), \\tau) \\\\\n    &\\mathcal{L}_{Global} = {1 \\over |\\mathcal{X}|} \\sum_{{\\mathbf{x}}, {\\mathbf{y}} \\in \\mathcal{X}} H(F_{\\theta}(\\mathbf{x}, \\tau), f_{\\phi_{k}}({\\mathbf{x}})),\n\\end{align}",
            "eq:9": "\\begin{align}\n\\mathbf{\\hat{y}} = (1 - \\alpha) \\cdot \\mathbf{y} + \\alpha\\cdot F_{\\theta}(\\mathbf{x}, \\tau). \\label{eq:refine_label}\n\\end{align}",
            "eq:10": "\\begin{align}\n    \\mathcal{L}_{Global} = {1 \\over |\\mathcal{\\hat{X}}|} \\sum_{{\\mathbf{x}}, {\\mathbf{\\hat{y}}} \\in \\mathcal{\\hat{X}}} H(\\mathbf{\\hat{y}}, f_{\\phi_{k}}({\\mathbf{x}})) \\label{eq:global_kd_real}\n\\end{align}",
            "eq:11": "\\begin{align}\n\\mathcal{L}_{Self} &= {1 \\over |\\mathcal{X}|} \\sum_{{\\mathbf{x}}, {\\mathbf{y}} \\in \\mathcal{X}} KL( f_{\\theta_k}({\\mathbf{x}, \\tau}) || f_{\\phi_k}({\\mathbf{x}, \\tau})), \n\\end{align}",
            "eq:12": "\\begin{align}\n \\mathcal{L}_{KD} &= \\mathcal{L}_{Global} + \\mathcal{L}_{Self} \\label{eq:our_global_kd}\n\\end{align}",
            "eq:13": "\\begin{align}\n&\\mathcal{L}_{CE} = {1 \\over |\\mathcal{X}|} \\sum_{{\\mathbf{x}}, {\\mathbf{y}} \\in \\mathcal{X}} H(\\mathbf{y}, f_{\\theta_k}({\\mathbf{x}})) \\\\\n&\\mathcal{L}_{total} = \\mathcal{L}_{CE} + \\mathcal{L}_{KD} \\\\\n&\\theta_k \\leftarrow \\theta_k - \\eta \\nabla_{\\theta_k}  \\mathcal{L}_{total} \\label{eq:total_training}\n\\end{align}",
            "eq:14": "\\begin{align}\n\\theta_{\\text{corrupted}}^{t+1} &= \\theta^t + {{\\sum_{k=1}^N \\mathbbm{1}_{\\{k \\in S_{b} \\cup S_{m}\\}}} \\cdot \\Delta \\theta_{k}^{t} \\over {|S_b| + |S_m|}} \\label{eq:corrupted_model}\n\\end{align}",
            "eq:15": "\\begin{align}\n\\theta_{\\text{clean}}^{t+1} &= \\theta^t + {{\\sum_{k=1}^N \\mathbbm{1}_{\\{k \\in S_{b}\\}}} \\cdot \\Delta \\theta_{k}^{t} \\over |S_b|}, \\label{eq:clean_model} \n\\end{align}"
        },
        "git_link": "https://github.com/deu30303/FedDefender/"
    }
}