{
    "meta_info": {
        "title": "Accelerating Dynamic Network Embedding with Billions of Parameter  Updates to Milliseconds",
        "abstract": "Network embedding, a graph representation learning method illustrating\nnetwork topology by mapping nodes into lower-dimension vectors, is challenging\nto accommodate the ever-changing dynamic graphs in practice. Existing research\nis mainly based on node-by-node embedding modifications, which falls into the\ndilemma of efficient calculation and accuracy. Observing that the embedding\ndimensions are usually much smaller than the number of nodes, we break this\ndilemma with a novel dynamic network embedding paradigm that rotates and scales\nthe axes of embedding space instead of a node-by-node update. Specifically, we\npropose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, which\nachieves an efficient and accurate dynamic network embedding by rotating and\nscaling the coordinate system where the network embedding resides with no more\nthan the number of edge modifications changes of node embeddings. Moreover, a\ndynamic Personalized PageRank is applied to the obtained network embeddings to\nenhance node embeddings and capture higher-order neighbor information\ndynamically. Experiments of node classification, link prediction, and graph\nreconstruction on different-sized dynamic graphs suggest that DAMF advances\ndynamic network embedding. Further, we unprecedentedly expand dynamic network\nembedding experiments to billion-edge graphs, where DAMF updates billion-level\nparameters in less than 10ms.",
        "author": "Haoran Deng, Yang Yang, Jiahe Li, Haoyang Cai, Shiliang Pu, Weihao Jiang",
        "link": "http://arxiv.org/abs/2306.08967v1",
        "category": [
            "cs.SI",
            "cs.LG"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n% Network Embedding\n\nNetwork embedding is an advanced graph representation learning method that maps each node in a graph to a vector in a low-dimensional space while preserving the graph's topological information~\\cite{Arsov2019NetworkEA}. \nIt is a versatile approach with successful practical applications in a wide range of fields, such as recommendation systems and bioinformatics~\\cite{cui2018survey, shi2018heterogeneous, wen2018network,Choudhary2021ASO, nelson2019embed, su2020network}.\n%Compared to another popular graph learning method, Graph Neural Networks (GNNs), network embedding is unsupervised and does not require any features (or attributes), reflecting the topological information of the nodes in the graph.\n\n% Dynamic Network Embedding\n    \nIn practice, however, changes in graph structure are frequent and inevitable. \n% Existing network embedding methods often neglect the \nFor instance, in a social network, the nodes representing the new members are added in connection to the original network over time, forming new edges. \nOn such dynamic graphs, the network embeddings should be updated with the transformation of the graphs to empower the model to capture crucial insights, such as which groups are more active or which members are more likely to be influencers. Moreover, the temporal evolution contributes to the portrait of the new members.\nUnfortunately, the frequent evolving patterns and the extensive network scale require enormous time and space to retrain the network embedding to model the dynamic network effectively.\n% Existing dynamic network embedding methods is an optimized network embedding method to accommodate the dynamic characteristics of graphs.\n\n% However, for large-scale networks, retraining the network embedding model can be a time-consuming process.\n% Therefore, the research to maintain a significant efficiency of dynamic network embedding is of practical significance and has received broad interest.\n\n%However, in reality, the vast majority of graphs are in a constant state of change.\n%For instance, in a social network, new people and new relationships will be added over time, i.e. there will be new nodes and new edges in the graph.\n%And it is time-consuming to recalculate the network embedding for any of these changes, especially for a graph with billions of edges.\n%Thus, dynamic network embedding that can be applied to changes in graphs is attracting much attention from the community.\n\n% \\begin{figure}[t]\n% \\centering \n% \\includegraphics[height=5cm,width=8.5cm]{fig/it.png}\n% \\caption\n%   {An example of network embedding (distance of connected node pairs $= 6$, otherwise $> 6$) updated by rotating and scaling the axes for an edge change.\n% }\n% \\label{fig:it}\n% \\vspace{-1em}\n% \\end{figure}\n\n% Try to think of how much computational resources are wasted by recalculating network embedding for a graph with billion of parameters just for a single newly registered user.\n% dilemma\n\nRecent research has encountered a dilemma in efficiency and effectiveness. \nThat is, precisely modifying the network embeddings leads to excessive inefficiencies.\nSome methods~\\cite{chen2015fast, zhang2018timers, zhu2018high} choose to adjust the embedding of nearly every node (also known as global updates), resulting in high time complexity.\nOn the other hand, \nsome works~\\cite{liu2019real, du2018dynamic, hou2020glodyne, mahdavi2018dynnode2vec} make trade-offs by updating only the more affected nodes in the graph (also known as local updates) to ensure good efficiency, but constantly bringing errors and, consequently, leading to performance degradation.\n% Others   make trade-offs at both ends of this scale.\nMoreover, existing applications like short video instant recommendations require high instantaneity, which indicates\nit is inappropriate to use delayed retraining methods, like gathering multiple modifications for a single update.\n% to alleviate the lack of efficiency, they tend to gather multiple modifications for a single update, which causes lagging and discontinuous graphical changes.\n%Some works~\\cite{liu2019real, du2018dynamic, hou2020glodyne} choose to be efficient, by selecting some of the most affected nodes in the graph to update (a.k.a. local updates), thus making each update produce errors leading to performance degradation.\n%Other works choose to modify the embedding of almost every node (a.k.a. global updates), resulting in long run times for a single operation, and they often combine multiple modifications into one making the graph modifications discrete.\n\n\n% solution\n\nWe break the dilemma by rotating and scaling the coordinate axes of the embedding space instead of updating individually from the nodes' perspective.\n% As shown in Figure~\\ref{fig:it}, the square in the graph is represented in a skewed coordinate. \nBy calculating the space projection matrices, the newly added edge is captured while retaining the semantics of the embedding from the previous step. \nBy means of an embedding space projection matrix and a small number of modifications of the node embedding, each node vector embedding in the graph will be relocated at the updated position in the embedding space.\nThis approach is efficient since the number of coordinate axes (i.e., the dimension of the embedding) is significantly fewer than the number of nodes while retaining a high accuracy level compared to local updates.\n\n% DAMF\n\nIn light of the preceding ideas, we propose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm.\nThe corresponding space projection matrix is solved based on \\emph{Zha-Simon's t-SVD update formula}~\\cite{zha1999updating}, with additional modification at most $\\Delta m$ nodes' embedding, where $\\Delta m$ is the number of edge changes in the graph. \n% Besides, based on the sparsity of the adjacency matrix, we propose a numerical method for solving the above formula with time complexity $O(\\Delta m)$ considering the embedding dimension as a constant.\nFurther, inspired by the great success of the application of Personalized PageRank (PPR) to static network embeddings~\\cite{yang2020nrp, tsitsulin2018verse, yin2019strap} and graph neural networks~\\cite{gasteiger2018predict}, we use a dynamic PPR to enhance the network embedding in order to capture high-order neighbors' information.\n\nWith the above design and ideas, DAMF also provides theoretical guarantees of effectiveness and efficiency. For effectiveness specifically, let $\\widetilde{\\mathbf{A}}$ be the low-rank approximation of the adjacency matrix represented by current network embedding, the unenhanced DAMF achieves the matrix factorization of the updated $\\widetilde{\\mathbf{A}}$ with node or edge change (Theorem \\ref{theorem:node} \\& \\ref{theorem:edge} ). Moreover, the dynamic embedding enhancement converges into accurate PPR propagation. For efficiency, the time complexity of the DAMF algorithm is proved to be $O(\\Delta m)$ when hyperparameters are considered constants.\n\n% DPRMF\n% % Further, we improve the quality of dynamic network embeddings using a special case of DAMF with $\\alpha=1$.\n% Inspired by the great success of the application of Personalized PageRank (PPR) to static network ebmeddings~\\cite{yang2020nrp, tsitsulin2018verse, yin2019strap} and graph neural networks~\\cite{gasteiger2018predict}, DAMF($\\alpha=1$) uses a dynamic PPR to augment the network embeddings so that the matrix decomposition results in an approximate factorization of the PPR matrix. \n\n% Theory\n% The DAMF algorithm produces no extra eigenvalue truncation error after multiple node additions, i.e., it yields the best low-rank approximation, thus ensuring effectiveness.\n\n\n% Massive Graphs\nIn addition, we are the first to extend dynamic network embedding experiments to billion-edge graphs, which is a breakthrough in the scale of massive graphs.\nWe conduct our experiment on Twitter, a real-world graph dataset with 41 million nodes and \\textbf{1.4 billion} edges, and map each node to a $128$-dimensional vector with the number of learnable parameters more than 10 times that the BERT-Large's~\\cite{kenton2019bert}.\nWe add nodes to the graph and updated the network embeddings individually, starting from 1000 nodes. The total updating time of DAMF is 110 hours, illustrating that DAMF achieves billion-level parameter updates in less than 10ms.\nIn addition, we have conducted experiments on node classification, link prediction, and graph reconstruction on graphs of different sizes. The experimental results show that DAMF reaches a new state of the art in dynamic network embedding.\n\nTo summarize, we make the following contributions:\n\\begin{itemize}[leftmargin=*, topsep=2pt, itemsep=1pt]\n\\item We present the DAMF algorithm, a novel dynamic network embedding method based on embedding space projections, and augment the embedding with dynamic PPR to capture higher-order neighbourhood information.\n\n\\item We theoretically illustrate the efficiency and effectiveness of DAMF.\n\n\\item To the best of our knowledge, we are the first to extend dynamic network embedding to a dataset with billions of edges. Experimental results show that our proposed methods use only an average of $10ms$ to complete global parameter updates. We also conduct experiments on five other actual network data, and the results show that the proposed methods reach a new state of the art in dynamic network embedding.\n\\end{itemize}\n\n\n\n% \\begin{itemize}\n%     \\item \\textbf{Efficiency: Update massive parameters in a short time}.\n%     % A network embedding that maps $n$ nodes of a graph to a $d$-dimensional space contains $n\\times d$ learnable parameters. \n%     Although there are many different methods of node embedding, they all require updating almost every node embedding to achieve an accurate update. \n%     But for a graph with billions of nodes, we have billions of learnable parameters.\n%     So, how to update all parameters (some call it global update) in a very short time (low time complexity) is the challenge of dynamic network ebmedding in terms of efficiency.\n    \n%     \\item \\textbf{Quality: Capture multi-hop topological information}.\n%     Some static network embedding methods attempt to capture higher-order neighborhood information through some methods such as propagation or random walk on a graph, which significantly improves the quality of network embedding but also presents new challenges to dynamic network embedding where a simple change can have a greater impact on a larger number of nodes.\n\n% \\end{itemize}\n\n% Existing dynamic network ebmedding methods fail on the above challenges. \n% In terms of efficiency, some of the works for subtle changes are still so complex that they can only handle discrete-time dynamic graphs (i.e. graphs that are updated less frequently), while the remaining works for continuous-time dynamic graphs are mainly made by extrapolation or local fine-tuning methods instead of more precise global updates. \n% In terms of quality, some work ignores higher-order neighborhood information, or the higher-order neighborhood information obtained is difficult to update over time.\n\n% In this paper, we revisit the low-rank update algorithm for truncated singular value decomposition and then formulate the update of the graph as a low-rank update of the adjacency matrix, proposing an efficient and scalable dynamic network ebmedding method.\n\n% To achieve efficiency, we improve the algorithm for the sparsity of the adjacency matrix of the graph to substantially improve the performance of the algorithm, so that the time complexity of the update is only comparable to the size of the updated part.\n% In particular, an innovative attempt is made to rotate the embedding space instead of updating the embedding vector to achieve a global update. By introducing a numerically stable space rotation matrix, two consecutive updates can be merged quickly by matrix multiplication making this step very efficient.\n\n% To achieve high quality, inspired by work using \\textit{Personalized PageRank (PPR)}~\\cite{page1999pagerank} on static network embedding~\\cite{tsitsulin2018verse, yin2019strap, yang2020nrp} and work applying PPR on dynamic graphs~\\cite{wang2021approximate, zheng2022instant}, we use a dynamic PPR to enhance the embedding in order to capture topological information from higher-order neighbours, so that the enhanced embedding is a matrix decomposition of the PPR matrix.\n% Moreover, this dynamic PPR can update the enhanced network embedding based on the update of the previous adjacency matrix factorization.\n\n"
            },
            "section 2": {
                "name": "preliminaries",
                "content": "\n\n\nConsider a graph $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})$, where $\\mathcal{V}$ denotes the node set of $n$ nodes and $\\mathcal{E}$ denotes the edge set of $m$ edges. \n\\emph{Dynamic graph scenario} is a sequential update events $\\{Event_1, ..., Event_T \\}$ with an initial graph $\\mathcal{G}_0$. Each event is one of either node change or edge change.\nLet $\\mathbf{A} \\in \\mathbb{R}^{n\\times n}$ be the adjacency matrix of $\\mathcal{G}$, and $\\mathbf{D}=diag\\{deg[1],\ndeg[2], ... , deg[n]\\}$ be the diagonal degree matrix where $\\deg[i]=\\sum_j \\mathbf{A}[i, j]$ is the out-degree of $i$-th node. \nNetwork embedding aims at mapping each node in graph $\\mathcal{G}$ to one or two low-dimensional vectors, which capture each node's structural information in the graph. In this paper, for a given dimension $d\\ll n$, the $i$-th node in graph $\\mathcal{G}$ is mapped to two vectors $\\mathbf{X}[i], \\mathbf{Y}[i] \\in \\mathbb{R}^{\\frac{d}{2}}$ with equal dimension, which capture the structural information. Dynamic Network Embedding updates the result of the embedding for the $t$-th event, and the snapshot of the updated embedding is noted as $\\mathbf{X}_t$, $\\mathbf{Y}_t$.\n\nIn this paper, matrices are denoted in bold uppercase letters. Let $\\mathbf{M}$ be arbitrary matrix, $\\mathbf{M}[i]$ is the $i$-th row vector of $\\mathbf{M}$, $\\mathbf{M}[:,j]$ is the $j$-th column vector of $\\mathbf{M}$, and $\\mathbf{M}[i,j]$ is the element on the $i$-th row $j$-th column of $\\mathbf{M}$. In addition, we use $\\mathbf{M}[:l]$ to denote the submatrix consisting of the first $l$ rows of $\\mathbf{M}$, and use $\\mathbf{M}[l:]$ to denote the submatrix consisting of the remaining part of $\\mathbf{M}$.\n"
            },
            "section 3": {
                "name": "Related Work",
                "content": "\n",
                "subsection 3.1": {
                    "name": "Network Embedding",
                    "content": "\nNetwork embedding aims to reflect the structural information of nodes in a graph by mapping each node into a low-dimensional vector~\\cite{hamilton2017representation}.\nCompared to another popular graph learning method, Graph Neural Networks (GNNs), it has no requirement for any features or attributes of nodes.\n\nStudies on network embedding can simply be classified into gradient-based methods and matrix factorization-based methods.\nGradient-based methods like DeepWalk~\\cite{perozzi2014deepwalk}, node2vec~\\cite{grover2016node2vec}, and LINE~\\cite{tang2015line} learn a skip-gram with negative sampling from random-walk on the graph, while GraphGAN ~\\cite{wang2018graphgan}, GA~\\cite{abu2018watch}, DNGR~\\cite{cao2016deep} use deep learning method to learn network embedding.\nFor matrix factorization-based methods, AROPE~\\cite{zhang2018arbitrary}, NetMF~\\cite{qiu2018netmf}, NetSMF~\\cite{qiu2019netsmf},  and LightNE~\\cite{qiu2021lightne} construct a matrix that reflects the properties of the graph and factorizes it to obtain the network embedding, while ProNE~\\cite{zhang2019prone} and NRP ~\\cite{yang2020nrp} obtain network embedding by propagates the embedding on the graph after the factorization.\n\nAmong the network embedding methods, those who use PageRank~\\cite{page1999pagerank}, such as VERSE~\\cite{tsitsulin2018verse}, STRAP~\\cite{yin2019scalable}, and NRP~\\cite{yang2020nrp}, have achieved an ideal result since they better capture the information of high-order neighbors.\n\n"
                },
                "subsection 3.2": {
                    "name": "Dynamic Network Embedding",
                    "content": "\nNumerous graphs in the industry are dynamic with frequent node or edge modifications, which leads to the emergence of network embedding methods on dynamic graphs. Methods for dynamic graph embedding can be categorized as node-selection-based, matrix-factorization-based, and others.\n\n\\textbf{Node-selection-based methods.}\nNode-selection-based methods choose to update only a limited number of embeddings node-by-node, resulting in poor performance.\nDNE~\\cite{du2018dynamic} updates the embedding of nodes by adapting the skip-gram to the dynamic graph;\nDynnode2vec~\\cite{mahdavi2018dynnode2vec} fine-tunes the previous result with the newly sampled data;\nGloDyNE~\\cite{hou2020glodyne} improves the node selection strategy to guarantee a global topological relationship;\nLocalAction~\\cite{liu2019real} achieves an efficient and dynamic network but with poor performance by randomly selecting a small number of neighbors around the updated nodes and modifying their embedding.\n\n\n\\textbf{Matrix-Factorization-based methods.}\nMatrix-factorization-based methods prefer global updates that adjust the embedding of almost every node, which leads to high time complexity.\nTRIP~\\cite{chen2015fast} efficiently updates the top eigen-pairs of the graph, but leads to a significant accumulation of errors; TIMERS~\\cite{zhang2018timers} uses recalculation to mitigate the errors caused by TRIP;\nRandNE~\\cite{zhang2018billion} uses a random projection method to update the factorization of the adjacency matrix; DHEP ~\\cite{zhu2018high} modifies the most affected eigenvectors using the matrix perturbation theory. Unfortunately, regardless of the amount of changes, the time complexity of these methods except RandNE for a graph with $n$ nodes is at least $O(n)$.\n\n\n\\textbf{Other methods.} DynGEM ~\\cite{goyal2018dyngem} and NetWalk~\\cite{yu2018netwalk} use an auto-encoder to continuously train the model with parameters inherited from the last time step with a regularization term. \nDepthLGP~\\cite{ma2018depthlgp} considers adding nodes to the dynamic graph as if they were out-of-sample and interpolating their embedding. However, the above methods are difficult to adapt to frequently changed graphs or cold start scenarios.\n\nAs a distinction, the dynamic network embedding in this paper tries to adapt embedding updates to dynamically changing graphs instead of mining the graph for temporal information(e.g., DynamicTraid~\\cite{zhou2018dynamictrad}, CTDNE~\\cite{nguyen2018ctdne}, DTINE~\\cite{gong2020ctine}, tNE\\cite{singer2019tne}), and the nodes have no attributes or features(e.g., EvolveGCN~\\cite{pareja2020evolvegcn}, DANE~\\cite{li2017dane}).\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Methodology",
                "content": "\n\n% Observing that the embedding dimension of a network embedding is always much smaller than the number of nodes, we propose a dynamic network embedding based on rotating and scaling the axes of the embedding space (i.e. space projection) and modifying a small number of node embeddings. Then we enhance the network embedding with a personalized pagerank to get the higher-order neighbors' information.\nIn this section, we develop the Dynamic Adjacency Matrix Factorization (DAMF) algorithm.\nFigure \\ref{fig:DAMF} shows an overview of the proposed DAMF algorithm.\nIn what follows, Section \\ref{sec:project} introduces the concept of dynamic network embeddings based on space projections.\nSection \\ref{sec:NodeChange} and Section \\ref{sec:EdgeChange} demonstrate how to modify the space projection matrix and the node embedding for node and edge changes, respectively. Section \\ref{sec:pagerank} presents an enhanced approach for dynamic network embedding using dynamic Personalized PageRank. Section \\ref{sec:damf} gives the detailed steps of the DAMF. Section \\ref{sec:complexity} analyzes the time and space complexity of DAMF. \n\n\n\n",
                "subsection 4.1": {
                    "name": "Dynamic Embedding via Space Projection",
                    "content": "\n\\label{sec:project}\nThe truncated singular value decomposition(t-SVD) of the adjacency matrix of a graph provides an ideal network embedding. For a graph $\\mathcal{G}$ with adjacency matrix $\\mathbf{A}\\in \\mathbb{R}^{n \\times n}$, this method provides a network embedding $\\mathbf{X} ,\\mathbf{Y} \\in \\mathbb{R}^{n \\times d}$ with dimension $d$ by\n\\begin{equation}\n\\label{eq:ne}\n    \\mathbf{U},\\mathbf{\\Sigma}, \\mathbf{V}\\gets \\texttt{\\textbf{t-SVD}}(\\mathbf{A},d), \\quad\n    \\mathbf{X}\\gets \\mathbf{U}\\sqrt{\\mathbf{\\Sigma}}, \\quad\n    \\mathbf{Y}\\gets \\mathbf{V}\\sqrt{\\mathbf{\\Sigma}}, \\quad\n\\end{equation}\nIn the scenario of a dynamic graph, the adjacency matrix changes over time, causing its t-SVD to change as well. \nHowever, the instantaneous recalculation of t-SVD is time-consuming, especially for large-scale adjacency matrices.\n\nTo cope with this problem, we propose a novel paradigm for updating network embedding by rotating and scaling (i.e., space projection) the space coordinate axes of embedding, with only a small number of nodes to update additionally.\nSpecifically, we take the network embedding $\\mathbf{X}, \\mathbf{Y}$ at time $t-1$, rotate and scale its coordinate axis by the space projection matrices $\\mathbf{F}, \\mathbf{G} \\in \\mathbb{R}^{d\\times d}$, then add $\\Delta X$ and $\\Delta Y$ respectively to get the updated network embeddings $\\mathbf{X}_t$ and $\\mathbf{Y}_t$ at time $t$ in the new coordinate system by\n\\begin{equation}\n\\label{eq:raw}\n    \\mathbf{X}_t \\gets  \\mathbf{X}_{t-1} \\cdot \\mathbf{F} + \\Delta \\mathbf{X}_{t}, \\quad\n    \\mathbf{Y}_t \\gets  \\mathbf{Y}_{t-1} \\cdot \\mathbf{G} + \\Delta \\mathbf{Y}_t\n\\end{equation}\nwhere the \\textbf{number of non-zero rows} in $\\Delta \\mathbf{X}_{t}$ and $\\Delta \\mathbf{Y}_{t}$ is the number of nodes that need to be modified for embedding.\n\nNevertheless, the complexity of computing $\\mathbf{X}_{t-1} \\cdot \\mathbf{F}$ and $\\mathbf{Y}_{t-1} \\cdot \\mathbf{G}$ is very high. \nTo address this issue, we map all graph modifications to a base space that stores network embeddings at any given timestamp. By matrix multiplication, successive space projections can be merged. To improve efficiency, we apply a “lazy” query optimization that keeps the accumulated modifications until a  query of new embedding appears.\n\nSpecifically, $\\mathbf{X}_0$ and $\\mathbf{Y}_0$ are the initialized network embeddings of $\\mathbf{X_b}, \\mathbf{Y_b} \\in \\mathbb{R}^{n \\times d}$ at timestamp $0$, while the space projection matrix $\\mathbf{P_X}, \\mathbf{P_Y} \\in \\mathbb{R}^{d \\times d}$ are initialized with the identity matrix. The operation in Eq. (\\ref{eq:raw}) can be transformed into\n\\begin{equation}\n\\label{eq:update}\n\\begin{aligned}\n    \\mathbf{P}_{\\mathbf{X},t} \\gets \\mathbf{P}_{\\mathbf{X}, t-1} \\cdot \\mathbf{F}, \\quad\n    \\mathbf{X}_{b,t} \\gets \\mathbf{X}_{b, t-1} + \\Delta \\mathbf{X}_{t} \\cdot \\mathbf{P}_{\\mathbf{X}, t}^{-1} \\\\\n    \\mathbf{P}_{\\mathbf{Y},t} \\gets \\mathbf{P}_{\\mathbf{Y}, t-1} \\cdot \\mathbf{G}, \\quad\n    \\mathbf{Y}_{b,t} \\gets \\mathbf{Y}_{b, t-1} + \\Delta \\mathbf{Y}_{t} \\cdot \\mathbf{P}_{\\mathbf{Y}, t}^{-1}\n\\end{aligned}\n\\end{equation}\n\nThen, the space projection matrix and node embedding will be modified for each change to the graph."
                },
                "subsection 4.2": {
                    "name": "Node Change",
                    "content": "\n\\label{sec:NodeChange}\n\nAdding nodes to a graph is equivalent to adding an equal number of rows and columns to its adjacency matrix.\nWithout losing generality, we consider the newly added node is in the last column and row of adjacency matrix, and we treat the column and row addition as two individual steps, that is, \n\n\\begin{equation}\n\\label{eq:node_B}\n    \\mathbf{A}^\\prime \\gets [\\mathbf{A}_{t-1}^{}, \\mathbf{B}_1^{}], \\quad\n    \\mathbf{A}_t^{} \\gets [\\mathbf{A}^{\\prime \\top}, \\mathbf{B}_2^{}]^\\top\n\\end{equation}\n\nBecause adding a row and adding a column are symmetrical, we describe our method in terms of adding a column to reduce redundant statements, and we use $\\mathbf{B}$ to denote $\\mathbf{B}_1$ and $\\mathbf{B}_2$ above. \nThe procedure for adding a row is the same, with all matrices are simply transposed.\n\nIn DAMF, without loss of generality, we consider the single node or edge update, that is, $\\mathbf{B}$ (and $\\mathbf{B}$, $\\mathbf{C}$ in the Section \\ref{sec:EdgeChange}) is $n$-by-$1$ matrix (or vector)\n\nDue to the connectivity of graphs, adding nodes is often accompanied by adding edges. \nLet $\\Delta m$ be the number of edges added to the graph. Since $\\mathbf{B}$ is a part of the adjacency matrix, we give the following property without proof.\n\n\\begin{property}\n$\\mathbf{B}$ has at most $\\Delta m$ non-zero elements.\n\\end{property}\n\nFor the matrix's expanded part $\\mathbf{B}$, we use a space projection idea on the embedding space with a small number of modifications for node embeddings to fit the updated graph.\n\n\\begin{lemma}\n\\label{lemma:nzr}\nFor arbitrary matrices $\\mathbf{B} \\in \\mathbb{R}^{n \\times q}, \\mathbf{C} \\in \\mathbb{R}^{q \\times p}$, if $\\mathbf{B}$ has $t$ non-zero rows, then $\\mathbf{B}\\mathbf{C}$ has at most $t$ non-zero rows.\n\\end{lemma}\n\n\\begin{theorem}[Space Projection for Node Change]\n\\label{theorem:node}\nAssuming $\\mathbf{B}$ is a matrix $\\in \\mathbb{R}^{n_1 \\times 1}$ with at most $\\Delta m$ non-zero elements. \nLet $\\mathbf{X}_1 \\in \\mathbb{R}^{n_1 \\times d}, \\mathbf{Y}_1 \\in \\mathbb{R}^{n_2 \\times d}$ be arbitrary network embedding  with \n\\begin{equation}\n\\label{eq:svd1}\n    \\mathbf{X}_1^{}   \\mathbf{Y}_1^\\top = \\mathbf{U}_1 \\boldsymbol{\\Sigma}_1 \\mathbf{V}_1^\\top = \\widetilde{\\mathbf{A}},\n\\end{equation}\nwhere $\\widetilde{\\mathbf{A}} \\in \\mathbb{R}^{n_1 \\times n_2}$, \n\nthen there exists space projection matrices $\\mathbf{F} \\in \\mathbb{R}^{d\\times d}, \\mathbf{G} \\in \\mathbb{R}^{d \\times d}$, and embedding modification matrices $\\Delta \\mathbf{X}\\in \\mathbb{R}^{n_1 \\times d}$, $\\Delta \\mathbf{Y} \\in \\mathbb{R}^{(n_2+1) \\times d}$ with at most $\\Delta m$ non-zero rows , such that\n\\begin{equation}\n    \\mathbf{X}_2 = \\mathbf{X}_{1}   \\mathbf{F} + \\Delta \\mathbf{X}, \\quad\n    \\mathbf{Y}_2 = \\mathbf{Y}_{1}   \\mathbf{G} + \\Delta \\mathbf{Y},\n\\end{equation} where $\\mathbf{X}_2 \\in \\mathbb{R}^{n_1 \\times d}, \\mathbf{Y}_2 \\in \\mathbb{R}^{(n_2+1) \\times d}$ is a network embedding from a rank-$d$ t-SVD of $[ \\widetilde{\\mathbf{A}}, \\mathbf{B} ]$, i.e.,\n\\begin{equation}\n\\label{eq:node_target}\n    (\\mathbf{U}_2, \\boldsymbol{\\Sigma}_2, \\mathbf{V}_2) \\gets \\texttt{\\textbf{t-SVD}}([ \\widetilde{\\mathbf{A}}, \\mathbf{B} ], d)\n\\end{equation}\n\\begin{equation}\n\\label{eq:svd2}\n     \\mathbf{X}_2 = \\mathbf{U}_2   \\sqrt{\\boldsymbol{\\Sigma}_2}, \\quad \\mathbf{Y}_2 = \\mathbf{V}_2   \\sqrt{\\boldsymbol{\\Sigma}_2}\n\\end{equation}\n\\end{theorem}\n\n\n\\begin{proof}\n\nLet $\\mathbf{W}_\\mathbf{B}=\\mathbf{U}_1^\\top \\mathbf{B}$, and the normalized $(\\mathbf{I}-\\mathbf{U}_1\\mathbf{U}_1^\\top)\\mathbf{B}$ be\n\\begin{equation}\n\\label{eq:qrnode}\nR_\\mathbf{B} = \\Vert (\\mathbf{I}-\\mathbf{U}_1\\mathbf{U}_1^\\top) \\mathbf{B} \\Vert_2,\\quad \\mathbf{Q_B} = (\\mathbf{I}-\\mathbf{U}_1\\mathbf{U}_1^\\top) \\mathbf{B} / R_\\mathbf{B}\n\\end{equation} \nIt can be proved that such a $\\mathbf{Q}_\\mathbf{B}$ is orthogonal to all column vectors of $\\mathbf{U}_1$.\n\nAccording to Zha-Simon's formula~\\cite{zha1999updating}, we have\n\\begin{equation}\n\\label{eq:zhasimonnode1}\n\\begin{aligned}\n\\begin{bmatrix}\n\\mathbf{X}_1^{} \\mathbf{Y}_1^\\top & \\mathbf{B}\n\\end{bmatrix}&=\n\\begin{bmatrix}\n\\mathbf{U}_1 \\boldsymbol{\\Sigma}_1 \\mathbf{V}_1^\\top & \\mathbf{B}    \n\\end{bmatrix} \\\\ \n&=\n\\begin{bmatrix}\n    \\mathbf{U}_1 & \\mathbf{Q_B}    \n\\end{bmatrix} \n\\begin{bmatrix}\n    \\Sigma_1 & \\mathbf{W_B} \\\\\n      & R_\\mathbf{B}\n\\end{bmatrix} \n\\begin{bmatrix}\n\\mathbf{V}_1^\\top & \\\\\n& \\mathbf{I}\n\\end{bmatrix} \\\\\n& \\approx ( \n\\begin{bmatrix}\n    \\mathbf{U}_1 & \\mathbf{Q_B}\n\\end{bmatrix}\n  \\mathbf{E} )   \\boldsymbol{\\Theta}   \n(\\begin{bmatrix}\n    \\mathbf{V}_1 & \\\\\n    & \\mathbf{I}\n\\end{bmatrix}   \\mathbf{H} )^\\top \\\\\n& = \\mathbf{U}_2  \\boldsymbol{\\Sigma}_2   \\mathbf{V}_2^\\top\n\\end{aligned}\n\\end{equation} \nwith \n\\begin{equation}\n\\label{eq:appd1}\n\\mathbf{U}_2=[\\mathbf{U}_1\\quad \\mathbf{Q}_B]\\mathbf{E}, \\quad \\Sigma_2=\\boldsymbol{\\Theta}, \\quad \\mathbf{V}_2=(\\begin{bmatrix}\n    \\mathbf{V}_1 & \\\\\n    & \\mathbf{I}\n\\end{bmatrix}   \\mathbf{H} )\n\\end{equation}\nwhere the matrix product $\\mathbf{E} \\boldsymbol{\\Theta} \\mathbf{H}$ denotes a compact rank-$d$ t-SVD with\n\\begin{equation}\n\\label{eq:svd_small_node}\n    \\mathbf{E}, \\boldsymbol{\\Theta}, \\mathbf{H} \\gets \\textbf{\\texttt{t-SVD}}(\\begin{bmatrix}\n    \\boldsymbol{\\Sigma}_1 & \\mathbf{W}_\\mathbf{B}\\\\\n      & R_\\mathbf{B}\n\\end{bmatrix}, d)\n\\end{equation}\n\nWhat is mentioned above is Zha-Simon's t-SVD update scheme.\nThe following will show how to obtain the space projection matrix and embedding modification vectors.\n\n\\emph{\\textbf{The central idea of the proof is to express the update of \n$\\ \\mathbf{U}, \\mathbf{V}$ in Eq.(\\ref{eq:appd1}) as a space projection onto $\\mathbf{U}, \\mathbf{V}$ plus a matrix with at most $\\Delta m$ non-zero rows.}} This is because in the update of $\\mathbf{U}$ in Eq.(\\ref{eq:appd1}), $\\mathbf{Q}_\\mathbf{B}$ can be written as \n\\begin{equation}\n\\mathbf{Q}_\\mathbf{B}=R_\\mathbf{B}^{-1}(\\mathbf{I}-\\mathbf{U}_1\\mathbf{U}_1^\\top)\\mathbf{B}\n=R_\\mathbf{B}^{-1}\\mathbf{B}-\\mathbf{U}_1(R_\\mathbf{B}^{-1}\\mathbf{U}_1^\\top \\mathbf{B})\n\\end{equation}\nNotice that $R_\\mathbf{B}$ is a scalar, so $R_\\mathbf{B}^{-1} \\mathbf{B}$ is a sparse matrix with at most $\\Delta m$ non-zero rows. Moreover, $\\mathbf{U}_1(R_\\mathbf{B}^{-1} \\mathbf{U}_1^\\top \\mathbf{B})$ is a space projection onto $\\mathbf{U}_1$ (which can be further merged).\n\n% Similarly, the update of V has a similar property.\nSpecifically, we split the matrix multiplication in Eq.(\\ref{eq:appd1}) for $U$ with\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{U}_2 \n&=[\\mathbf{U}_1\\quad \\mathbf{Q}_B]\\mathbf{E} \\\\\n&=\\mathbf{U}_1\\mathbf{E}[:d] + \\mathbf{Q}_\\mathbf{B} \\mathbf{E}[d:] \\\\\n&=\\mathbf{U}_1\\mathbf{E}[:d] + (R_\\mathbf{B}^{-1}\\mathbf{B}-\\mathbf{U}_1(R_\\mathbf{B}^{-1}\\mathbf{U}_1^\\top \\mathbf{B})\\mathbf{E}[d:] \\\\\n&=\\mathbf{U}_1(\\mathbf{E}[:d] - (R_\\mathbf{B}^{-1}\\mathbf{U}_1^\\top \\mathbf{B})\\mathbf{E}[d:]) + R_\\mathbf{B}^{-1} \\mathbf{B} \\mathbf{E}[d:]\n\\end{aligned}\n\\end{equation}\nwhich is clearly a space projection on the columns of $\\mathbf{U}_1$ plus a sparse matrix with at most $\\Delta m$ non-zero rows. And, similarly, for the update on $\\mathbf{V}$ in Eq.(\\ref{eq:appd1}) we have\n\\begin{equation}\n\\mathbf{V}_2=(\\begin{bmatrix}\n    \\mathbf{V}_1 & \\\\\n    & \\mathbf{I}\n\\end{bmatrix}   \\mathbf{H} )\n=\\begin{bmatrix}\n    \\mathbf{V}_1\\\\\n    \\\\\n\\end{bmatrix} \\mathbf{H[:d]} + \n\\begin{bmatrix}\n    \\\\\n    \\mathbf{I}_{1\\times 1}\n\\end{bmatrix} \\mathbf{H[d:]}\n\\end{equation}\n\n\nSince $\\mathbf{X}_1=\\mathbf{U}_1\\sqrt{\\mathbf{\\Sigma}_1}$, $\\mathbf{Y}_1=\\mathbf{V}_1 \\sqrt {\\mathbf{\\Sigma}_1}$ and $\\mathbf{X}_2=\\mathbf{U}_2\\sqrt{\\mathbf{\\Sigma}_2}$, $\\mathbf{Y}_2=\\mathbf{V}_2 \\sqrt {\\mathbf{\\Sigma}_2}$, we have\n\n\\begin{equation}\n\\begin{aligned}\n\\label{eq:fgxynode}\n    \\mathbf{X}_2 =& \\mathbf{X}_1   \\boldsymbol{\\Sigma}_1^{-1/2}   (\\mathbf{E}[:d]-R_\\mathbf{B}^{-1} \\mathbf{W}_\\mathbf{B} \\mathbf{E}[d:])  \\boldsymbol{\\Sigma}_2^{1/2} \\\\ \n    & + R_{\\mathbf{B}}^{-1} \\mathbf{B} \\mathbf{E}[d:]   \\boldsymbol{\\Sigma}_2^{1/2}  \\\\\n    \\mathbf{Y}_2 =& \\mathbf{Y}_1   \\boldsymbol{\\Sigma}_1^{-1/2}   \\mathbf{H}[:d]  \\boldsymbol{\\Sigma}_2^{1/2}  \\\\\n    & + \\begin{bmatrix}\n    \\\\\n    \\mathbf{I}_{1 \\times 1} \\\\\n    \\end{bmatrix}   \\mathbf{H}[d:]   \\boldsymbol{\\Sigma}_2^{1/2} \n\\end{aligned}\n\\end{equation} \nand we take\n\\begin{equation}\n    \\label{eq:f_node}\n    \\mathbf{F} = \\boldsymbol{\\Sigma}_1^{-1/2}   (\\mathbf{E}[:d]-R_\\mathbf{B}^{-1} \\mathbf{W}_\\mathbf{B}\\mathbf{E}[d:])  \\boldsymbol{\\Sigma}^{1/2}_2\n\\end{equation}\n\\begin{equation}\n    \\label{eq:g_node}\n    \\mathbf{G} = \\boldsymbol{\\Sigma}_1^{-1/2}   \\mathbf{H}[:d]   \\boldsymbol{\\Sigma}^{1/2}_2\n\\end{equation}\n\\begin{equation}\n    \\label{eq:x_node}\n    \\Delta \\mathbf{X} = R_{\\mathbf{B}}^{-1} \\mathbf{B} \\mathbf{E}[d:]   \\boldsymbol{\\Sigma}_2^{1/2}\n\\end{equation}\n\\begin{equation}\n    \\label{eq:y_node}\n    \\Delta \\mathbf{Y} = \\begin{bmatrix}\n         \\\\\n        \\mathbf{I}_{1 \\times 1} \\\\\n    \\end{bmatrix}   \\mathbf{H}[d:]   \\boldsymbol{\\Sigma}^{1/2}_2\n\\end{equation} \n\nSince $\\mathbf{B}$ has at most $\\Delta m$ non-zero rows (as stated in Lemma \\ref{lemma:nzr}), $\\Delta \\mathbf{X}$ has at most $\\Delta m$ non-zero rows, and it is clear that $\\Delta \\mathbf{Y}$ has only one non-zero row.\n\\end{proof}\nThe above proof provides the space projection matrices $\\mathbf{F}, \\mathbf{G}$ to use in the embedding space, and shows that $\\Delta \\mathbf{X}$ and $\\Delta \\mathbf{Y}$ require at most $\\Delta m$ nodes' embedding to be modified additionally in a node change process. \n\n"
                },
                "subsection 4.3": {
                    "name": "Edge Change",
                    "content": "\n\\label{sec:EdgeChange}\nConsidering adding a directed edge $(u,v)$ to the graph, the change in the adjacency matrix can be expressed as a low-rank update by\n\\begin{equation}\n\\mathbf{A}_t \\gets \\mathbf{A}_{t-1} + \\Delta \\mathbf{A}_t, \\quad \n\\Delta \\mathbf{A}_{t-1} = \\mathbf{B}\\mathbf{C}^\\top\n\\end{equation}\n with \n\\begin{equation}\n\\label{eq:edge_B}\n \\mathbf{B}=e_u,\\quad \\mathbf{C}=e_v\n\\end{equation} where $e_i$ denotes the standard basis (i.e., a vector whose components are all zero, except the $i$-th element is 1).\n\n\\begin{theorem}[Space Projection for Edge Change]\n\\label{theorem:edge}\nAssuming $\\mathbf{B}$ and $\\mathbf{C}$ are matrices $\\in \\mathbb{R}^{n \\times 1}$ with at most $\\Delta m$ non-zero elements. \nLet $\\mathbf{X}_1 \\in \\mathbb{R}^{n \\times d}, \\mathbf{Y}_1 \\in \\mathbb{R}^{n \\times d}$ be arbitrary network embedding with \n\\begin{equation}\n    \\mathbf{X}_1^{}   \\mathbf{Y}_1^\\top = \\mathbf{U}_1 \\boldsymbol{\\Sigma}_1 \\mathbf{V}_1^\\top = \\widetilde{\\mathbf{A}},\n\\end{equation}\nwhere $\\widetilde{\\mathbf{A}} \\in \\mathbb{R}^{n \\times n}$, \n\nthen there exists space projection matrices $\\mathbf{F} \\in \\mathbb{R}^{d\\times d}, \\mathbf{G} \\in \\mathbb{R}^{d \\times d}$, and embedding modification vectors $\\Delta \\mathbf{X}\\in \\mathbb{R}^{n \\times d}$ and $\\Delta \\mathbf{Y} \\in \\mathbb{R}^{n \\times d}$ with at most $\\Delta m$ non-zero rows , such that\n\\begin{equation}\n    \\mathbf{X}_2 = \\mathbf{X}_{1}   \\mathbf{F} + \\Delta \\mathbf{X}, \\quad\n    \\mathbf{Y}_2 = \\mathbf{Y}_{1}   \\mathbf{G} + \\Delta \\mathbf{Y},\n\\end{equation} where $\\mathbf{X}_2 \\in \\mathbb{R}^{n \\times d}, \\mathbf{Y}_2 \\in \\mathbb{R}^{n \\times d}$ is a network embedding from a rank-$d$ t-SVD of $\\widetilde{\\mathbf{A}} + \\mathbf{B} \\mathbf{C} ^ \\top $, i.e.,\n\\begin{equation}\n\\label{eq:node_target1}\n    (\\mathbf{U}_2, \\boldsymbol{\\Sigma}_2, \\mathbf{V}_2) \\gets \\texttt{\\textbf{t-SVD}}(\\widetilde{\\mathbf{A}} + \\mathbf{B} \\mathbf{C}^\\top, d)\n\\end{equation}\n\\begin{equation}\n\\label{eq:svd_edge_2}\n     \\mathbf{X}_2 = \\mathbf{U}_2   \\sqrt{\\boldsymbol{\\Sigma}_2}, \\quad \\mathbf{Y}_2 = \\mathbf{V}_2   \\sqrt{\\boldsymbol{\\Sigma}_2}\n\\end{equation}\n\\end{theorem}\n\\begin{proof}\n\nSimilar to the proof of Theorem \\ref{theorem:node}, firstly, let $\\mathbf{W}_\\mathbf{B}=\\mathbf{U}_1^\\top B$ and $\\mathbf{W}_\\mathbf{C}=\\mathbf{V}_1^\\top \\mathbf{C}$. \nAnd the normalized $(\\mathbf{I}-\\mathbf{U}_1\\mathbf{U}_1^\\top) \\mathbf{B}$, $(\\mathbf{I}-\\mathbf{V}_1\\mathbf{V}_1^\\top) \\mathbf{C}$ can be calculated by\n\\begin{equation}\n\\label{eq:qredge1}\nR_\\mathbf{B} = \\Vert (\\mathbf{I}-\\mathbf{U}_1\\mathbf{U}_1^\\top) \\mathbf{B} \\Vert_2,\n\\quad\n\\mathbf{Q_B} = (\\mathbf{I}-\\mathbf{U}_1\\mathbf{U}_1^\\top) \\mathbf{B} / R_\\mathbf{B}\n\\end{equation}\n\\begin{equation}\n\\label{eq:qredge2}\nR_\\mathbf{C} = \\Vert (\\mathbf{I}-\\mathbf{V}_1\\mathbf{V}_1^\\top) \\mathbf{C} \\Vert_2,\n\\quad\n\\mathbf{Q_C} = (\\mathbf{I}-\\mathbf{V}_1\\mathbf{V}_1^\\top) \\mathbf{C} / R_\\mathbf{C}\n\\end{equation}\n\nThen let \n\\begin{equation}\n\\label{eq:svd_small_edge}\n    \\mathbf{E},\\mathbf{\\Sigma}_2, \\mathbf{H} \\gets \\texttt{\\textbf{t-SVD}}(\n    \\begin{bmatrix}\n        \\Sigma_1 & \\mathbf{0} \\\\\n        \\mathbf{0} & \\mathbf{0}\n    \\end{bmatrix}\n    +\n    \\begin{bmatrix}\n        \\mathbf{W}_\\mathbf{B} \\\\\n        R_\\mathbf{B}\n    \\end{bmatrix}\n    \\begin{bmatrix}\n        \\mathbf{W}_\\mathbf{C} \\\\\n        R_\\mathbf{C}\n    \\end{bmatrix}^\\top, d)\n\\end{equation}\nWe can get the space projection matrices $\\mathbf{F}, \\mathbf{G}$ and embedding modification vectors $\\Delta \\mathbf{X}$, $\\Delta \\mathbf{Y}$ by\n\\begin{equation}\n    \\label{eq:f_edge}\n    \\mathbf{F} = \\boldsymbol{\\Sigma}_1^{-1/2}   (\\mathbf{E}[:d]- R_\\mathbf{B}^{-1} \\mathbf{W}_\\mathbf{B} \\mathbf{E}[d:])  \\boldsymbol{\\Sigma}^{1/2}_2\n\\end{equation}\n\\begin{equation}\n    \\label{eq:g_edge}\n    \\mathbf{G} = \\boldsymbol{\\Sigma}_1^{-1/2}   (\\mathbf{H}[:d] - R_\\mathbf{C}^{-1} \\mathbf{W}_\\mathbf{C} \\mathbf{H}[d:])   \\boldsymbol{\\Sigma}^{1/2}_2\n\\end{equation}\n\\begin{equation}\n\\label{eq:x_edge}\n    \\Delta \\mathbf{X} = R_{\\mathbf{B}}^{-1} \\mathbf{B} \\mathbf{E}[d:] \\boldsymbol{\\Sigma}_2^{1/2}\n\\end{equation}\n\\begin{equation}\n\\label{eq:y_edge}\n    \\Delta \\mathbf{Y} = R_{\\mathbf{C}}^{-1} \\mathbf{C} \\mathbf{H}[d:] \\boldsymbol{\\Sigma}_2^{1/2}\n\\end{equation} \n\nSince $\\mathbf{B}, \\mathbf{C}$ have at most $\\Delta m$ non-zero rows, by Lemma \\ref{lemma:nzr}, $\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}$ have at most $\\Delta m$ non-zero rows.\n\\end{proof}\nThe above proof provides the space projection matrices $\\mathbf{F}, \\mathbf{G}$ to use in the embedding space, and shows that $\\Delta \\mathbf{X}$ and $\\Delta \\mathbf{Y}$ require at most $\\Delta m$ nodes' embedding to be modified additionally in an edge change process."
                },
                "subsection 4.4": {
                    "name": "Dynamic Embedding Enhancement via PPR",
                    "content": "\n\\label{sec:pagerank}\n% The method proposed in Section \\ref{sec:NodeChange} and Section \\ref{sec:EdgeChange} provides an adjacency matrix factorization method that is efficiently applicable to dynamic graphs.\n% However, the obtained results have many limitations since the adjacency matrix is sparse and needs more information on higher-order neighbors other than 1-hop ones.\nIn order to capture higher-order neighbors's information, an enhancement is applied to the dynamic network embedding.\nSpecifically, we apply a dynamic \\emph{Personalized PageRank} (PPR)\\cite{page1999pagerank} to the updated context embedding $\\mathbf{X}$ to get the enhanced context embedding $\\mathbf{Z}$. \n\n\\textbf{PPR Enhancement in Static Network Embedding.} To better capture higher-order neighborhood information, a mainstream approach on static network embeddings is to use \\emph{Personalized PageRank} (PPR)~\\cite{page1999pagerank} to enhance the network embeddings (e.g., APP, Lemane, STRAP, VERSE, and NRP). \n\nSpecifically, for a graph with adjacency matrix $\\mathbf{A}$ and graph signal $\\mathbf{X}\\in \\mathbb{R}^{n\\times d}$, the principles of PPR can be formulated as\n\\begin{equation}\n\\label{eq:ppr}\n    \\texttt{PPR}(\\mathbf{X})=\\sum_{i=0}^\\infty \\alpha(1-\\alpha)^i (\\mathbf{D}^{-1} \\mathbf{A})^i \\mathbf{X}\n\\end{equation}\nwhere $\\mathbf{D} \\in \\mathbb{R}^{n\\times n}$ is the diagonal out-degree matrix and $\\alpha$ is the damping factor for PageRank. \n\nSince solving the PPR is an infinite process, the truncated version of PPR is commonly used in practice. Moreover, to better control errors and balance efficiency, existing PageRank-based static network embedding methods (e.g., all methods mentioned above) typically introduce an error tolerance $\\epsilon$ and require PPR to converge to that error.\n\n\\textbf{Dynamic Embedding Enhancement via Dynamic PPR.}\nUnfortunately, despite the success of PageRank enhancements in static network embeddings, their methods cannot be directly migrated to dynamic scenarios. In this work, following \\cite{bojchevski2020scaling, wang2021approximate, zheng2022instant, zhang2016approximate}, we approximate the PPR based on graph propagation under a given error limit $\\epsilon$. \n\nSpecifically, we use an InstantGNN~\\cite{zheng2022instant} to enhance the embedding obtained in Section \\ref{sec:NodeChange} and Section \\ref{sec:EdgeChange}. InstantGNN is an efficient dynamic PPR method suitable for graph structure changes and node signals (attributes).\nIn Dynamic Embedding Enhancement, we use the embedding obtained from dynamic truncated singular value decomposition as the graph signal input to InstantGNN. Moreover, as the network changes, according to Theorem \\ref{theorem:node} and Theorem \\ref{theorem:edge}, at most $\\Delta m$ nodes' signals need to be changed. The \nInstantGNN updates the result, which converges to an error tolerance $\\epsilon$ according to the graph structure and node signal change to achieve the Dynamic Embedding Enhancement.\n\n% \\textbf{Connection to PageRank Matrix Factorization.} Following the static network embedding method with PageRank\\cite{yang2020nrp}, we show the connection between the PageRank matrix and network embedding enhanced by a PPR as follows.\n\n% For a graph with adjacency matrix $\\mathbf{A}$, the principles of PPR can be formulated as\n% \\begin{equation}\n% \\label{eq:ppr}\n%     \\texttt{PPR}(\\mathbf{X}, \\alpha)=\\sum_{i=0}^\\infty \\alpha(1-\\alpha)^i (\\mathbf{A} \\mathbf{D}^{-1})^i \\mathbf{X}\n% \\end{equation}\n% where $\\mathbf{D} \\in \\mathbb{R}^{n\\times n}$ is the diagonal out-degree matrix and $\\alpha$ is the dump factor for PageRank. \n% By using a power series expansion, the PPR process described in Eq.(\\ref{eq:ppr}) satisfies:\n% \\begin{equation}\n% \\label{eq:ppr2}\n%     \\texttt{PPR}(\\mathbf{X}) = \\alpha \\mathbf{X} + (1-\\alpha) \\mathbf{AD}^{-1} \\texttt{PPR}(\\mathbf{X})\n% \\end{equation}\n\n% Moreover, the PageRank matrix $\\mathbf{\\Pi} \\in \\mathbb{R}^{n \\times n}$ is defined as\n% \\begin{equation}\n%  \\mathbf{\\Pi}=\\sum_{i=0}^\\infty \\alpha(1-\\alpha)^i (\\mathbf{A} \\mathbf{D}^{-1})^i\n% \\end{equation}\n\n% % \\mathbf{Z} \\mathbf{Y}^\\top \\approx\n% Now let $\\texttt{PPR}(\\mathbf{X})$ be the enhanced context embedding. Since the network embedding $\\mathbf{X}, \\mathbf{Y}$ is a matrix factorization of the adjacency matrix with $\\mathbf{X}\\mathbf{Y}^\\top \\approx \\mathbf{A}$, the connection between the enhanced network embedding $\\texttt{PPR}(\\mathbf{X}),\\mathbf{Y}$ and an factorization of PageRank matrix $\\mathbf{\\Pi}$ can be established by\n% \\begin{equation}\n% \\label{eq:ppr_connection}\n% \\texttt{PPR}(\\mathbf{X}) \\cdot \\mathbf{Y} ^\\top \\approx \\sum_{i=0}^\\infty \\alpha(1-\\alpha)^i (\\mathbf{A} \\mathbf{D}^{-1})^i  \\mathbf{X} \\mathbf{Y}^\\top = \\mathbf{\\Pi} \\cdot \\frac{\\mathbf{D}}{(1-\\alpha)} \n% \\end{equation}\n% which illustrates that $(1-\\alpha)\\texttt{PPR}(\\mathbf{X})$ and $\\mathbf{YD}^{-1}$ is the matrix factorization of the PageRank matrix .\n\n% \\textbf{Embedding Enhancement via PPR.}\n% Following \\cite{bojchevski2020scaling, wang2021approximate, zheng2022instant, zhang2016approximate}, we approximate the PPR based on graph propagation under a given error limit $\\epsilon$. \n% We refer to the context embedding $\\mathbf{X}$ as the graph signal input to the PPR.\n% Let $\\mathbf{r} \\in \\mathbb{R}^{n \\times d}$ be the residual, which indicates the mass of each node that has not yet propagated out and let $\\mathbf{Z} \\in \\mathbb{R}^{n \\times d}$ be the estimated approximation of $\\texttt{PPR}(\\mathbf{X})$. \n% The relationship described by Lemma \\ref{lemma:ppr} concerning $\\mathbf{Z}$, $\\mathbf{r}$, and graph signal $\\mathbf{X}$ should always be observed throughout the propagation process.\n\n% \\begin{algorithm}[t]\n%     \\LinesNumbered %要求显示行号\n%     \\caption{Dynamic embedding enhancement via PPR}%算法名字\n%     \\SetKwProg{myproc}{Procedure}{}{}\n%     \\label{algo:ppr}\n%     \\SetKwFunction{DynamicEnhancement}{DynamicEnhancement}\n%     \\SetKwFunction{Propagation}{Propagation}\n%      \\myproc{\\Propagation{$\\mathbf{Z}$, $\\mathbf{r}$, $Event$, $\\mathcal{G}$, $\\alpha$, $\\epsilon$}}{\n%         \\While{exist node $s$ in $\\mathcal{G}$ with $|\\mathbf{r}(s)| > \\epsilon$}{\n%             $\\mathbf{Z}(s) \\gets \\mathbf{Z}(s) + \\alpha \\mathbf{r}(s)$\\; \n%             \\For{each node $t\\in \\mathcal{N}(s)$}{\n%                 $\\mathbf{r}(t) \\gets \\mathbf{r}(t)+\\frac{(1-\\alpha)}{deg(s)}\\mathbf{r}(s)$\\;\n%             }\n%             $\\mathbf{r}(s) \\gets \\mathbf{0}$\\;\n%         }\n%         \\KwRet{$\\mathbf{Z}, \\mathbf{r}$}\\;\n%     }\n%     \\myproc{\\DynamicEnhancement{$\\mathbf{Z}$, $\\mathbf{r}$, $Event$, $\\mathcal{G}$, $\\Delta \\mathbf{X}$, $\\alpha$, $\\epsilon$}}{\n%         \\ForEach{edge change $u, v \\in Event$}{\n%             Let $d_0, d_1$ be out-degree of $u$ before and after adding this edge respectively \\;\n%             $\\mathbf{r}(v) \\gets \\mathbf{r}(v) + \\frac{(1-\\alpha) \\mathbf{Z}(u)}{\\alpha d_0(u)}$\\;\n%             $\\mathbf{r}(u) \\gets \\mathbf{r}(u) + (\\frac{1}{d_1(u)} - \\frac{1}{d_0(u)}) \\cdot \\frac{\\mathbf{Z}(u)}{\\alpha}$\\;\n%             $\\mathbf{Z}(u) \\gets \\mathbf{Z}(u) - (\\frac{1}{d_1(u)} - \\frac{1}{d_0(u)})\\cdot \\mathbf{Z}(u)$\\;\n%         }\n%         $\\mathbf{r} \\gets \\mathbf{r} + \\Delta \\mathbf{X}$\\;\n%         $\\mathbf{Z}, \\mathbf{r} \\gets $\\Propagation{$\\mathbf{Z}, \\mathbf{r}, Event, \\mathcal{G}, \\Delta \\mathbf{X}, \\alpha, \\epsilon$}\\;\n%         \\KwRet{$\\mathbf{Z}, \\mathbf{r}$}\\;\n%     }\n% \\end{algorithm} \n\n% \\begin{lemma} \n% \\label{lemma:ppr}{(Lemma 1 in \\cite{zheng2022instant})}\n% For each graph signal vector $\\mathbf{X}$, the estimated vector $\\mathbf{Z}$ and the residual $\\mathbf{r}$ satisfy the following invariant property during the propagation process:\n% \\begin{equation}\n%     \\label{eq:pr}\n%     \\mathbf{Z} + \\alpha \\mathbf{r}=\\alpha \\mathbf{X} + (1-\\alpha) \\mathbf{A} \\mathbf{D}^{-1} \\mathbf{Z}\n% \\end{equation}\n% \\end{lemma} \n\n% Based on Lemma \\ref{lemma:ppr}, Algorithm \\ref{algo:ppr} describes the process of solving the approximate $\\texttt{PPR}(\\mathbf{X})$.\n% As $|\\boldsymbol{r}|$ tends to $\\mathbf{0}$, the estimated approximation $\\mathbf{Z}$ also converges to $\\texttt{PPR}(\\mathbf{X})$ described in Eq. (\\ref{eq:ppr2}).\n% Specifically, for each node $u$ with out-degree $deg(u)$, the propagation process adds $\\alpha \\cdot \\mathbf{r}(u)$ to $\\mathbf{Z}(u)$ and adds $\\frac{(1-\\alpha)}{deg(u)} \\cdot \\mathbf{r}(u)$ to the residual $\\mathbf{r}(v)$ of each neighbor $v \\in \\mathcal{N}(u)$.\n\n% For dynamic graphs, the graph structure changes over time. In addition, $\\Delta \\mathbf{X}$ directly modifies some embeddings in the base space, resulting in at most $\\Delta m$ context embedding changes in the PPR's input $\\mathbf{X}$.\n% In the following, we describe how the dynamic embedding enhancement method handles changes of the graph structure and context embedding respectively.\n\n% \\begin{itemize}[leftmargin=*, topsep=2pt, itemsep=1pt]\n% \\item \\textbf{Change of Graph Structure}.\n% When adding (or deleting) an directed edge from $(u,v)$, it no longer satisfies Eq.(\\ref{eq:pr}) in Lemma \\ref{lemma:ppr} for two reasons:\n% \\begin{enumerate}\n%     \\item All previous propagations starting from node $u$ should be added to (or decreased from) node $v$ with a magnification of $\\frac{1-\\alpha}{deg_t(u)}$ due to the new link.\n%     \\item All previous propagations starting from node $u$ should be reduced (or increased) with $\\frac{1}{deg_t(u)}-\\frac{1}{deg_{t-1}(u)}$ times due to the change of the out-degree of $u$.\n% \\end{enumerate}\n\n% Since the sum of the propagation flow from $u$ equals to $\\frac{\\mathbf{Z}(u)}{\\alpha}$ (which can be deduced from Line 3 of Algorithm \\ref{algo:ppr}), we can update $\\mathbf{Z}$ and $\\mathbf{r}$ to satisfy the original equation relation for dynamic PPR by\n% \\begin{equation}\n% \\label{eq:ppr_update_1}\n% \\mathbf{r}(v) \\gets \\mathbf{r}(v) + \\frac{(1-\\alpha) \\mathbf{Z}(u)}{\\alpha deg_{t-1}(u)}\n% \\end{equation}\n% \\begin{equation}\n% \\label{eq:ppr_update_2}\n% \\mathbf{r}(u) \\gets \\mathbf{r}(u) + (\\frac{1}{deg_{t}(u)} - \\frac{1}{deg_{t-1}(u)}) \\cdot \\frac{\\mathbf{Z}(u)}{\\alpha}    \n% \\end{equation}\n% \\begin{equation}\n% \\label{eq:ppr_update_3}\n% \\mathbf{Z}(u) \\gets \\mathbf{Z}(u) - (\\frac{1}{deg_{t}(u)} - \\frac{1}{deg_{t-1}(u)})\\cdot \\mathbf{Z}(u)\n% \\end{equation}\n\n% \\item \\textbf{Change of Context Embedding}.\n% As the residual $\\mathbf{r}$ is defined as the propagation mass that received but not yet propagated by a node, an update process can directly add the amount of change in the graph signal $\\Delta \\mathbf{X}$ to it.\n% \\end{itemize}\n\n% After updating these residuals, we can use the propagation algorithm to eliminate the residual and reach to an approximation under a certain error $\\epsilon$. \n\\begin{algorithm}[tp]\n  \\LinesNumbered %要求显示行号\n  \\SetKwFunction{UpdateEmbeddingN}{UpdateEmbeddingN}\n  \\SetKwFunction{UpdateEmbeddingE}{UpdateEmbeddingE}\n  \\SetKwProg{myproc}{Procedure}{}{}\n  \n  \\myproc{\\UpdateEmbeddingN{$\\mathbf{X}, \\mathbf{Y}, \\mathbf{P_X}, \\mathbf{P_Y}, \\mathbf{\\Sigma}_1, \\mathbf{B}, d$}}{\n    \\tcc{Step 1: Orthogonalization}\n    $\\mathbf{W}_\\mathbf{B} \\gets (\\mathbf{B}^\\top \\mathbf{X} \\mathbf{P_X} \\mathbf{\\Sigma}_1^{1/2})^\\top $ \\;\n    $R_\\mathbf{B} \\gets \\sqrt{ \\Vert \\mathbf{B} \\Vert_2^2 - \\Vert \\mathbf{W}_\\mathbf{B} \\Vert_2^2 }$\\;\n    \\tcc{Step 2: Rediagonalization}\n    $\\mathbf{M} \\gets \\begin{bmatrix}\n    \\Sigma_1 & \\mathbf{W}_\\mathbf{B} \\\\\n      & R_\\mathbf{B} \\end{bmatrix}$\n      \\tcp*{$\\mathbf{M} \\in \\mathbb{R}^{d+1, d+1}$}\n    $\\mathbf{E}, \\Sigma_2, \\mathbf{H} \\gets $ \\texttt{t-SVD}($\\mathbf{M}, d$) \\;\n    \\tcc{Step 3: Space Projection}\n    $\\mathbf{F} \\gets \\boldsymbol{\\Sigma}_1^{-1/2}  (\\mathbf{E}[:d]-R_\\mathbf{B}^{-1} \\mathbf{W}_\\mathbf{B}   \\mathbf{E}[d:]) \\boldsymbol{\\Sigma}^{1/2}_2$ \\;\n    $\\mathbf{G} \\gets \\boldsymbol{\\Sigma}_1^{-1/2}    \\mathbf{H}[:d]    \\boldsymbol{\\Sigma}^{1/2}_2 $ \\;\n    $\\Delta \\mathbf{X} = R_\\mathbf{B}^{-1} \\mathbf{B}   \\mathbf{E}[d:]   \\boldsymbol{\\Sigma}_2^{1/2}$ \\;\n    $\\Delta \\mathbf{Y} =   \\begin{bmatrix}\n        \\\\\n        \\mathbf{I}_{1 \\times 1} \\\\\n    \\end{bmatrix}  \\mathbf{H}[d:]    \\boldsymbol{\\Sigma}^{1/2}_2$ \\;\n    \n   \\KwRet{$\\mathbf{F}, \\mathbf{G}, \\Delta \\mathbf{ X}, \\Delta \\mathbf{Y}, \\Sigma_2$ }\\;}\n  \\SetKwProg{myproc}{Procedure}{}{}\n  \\myproc{\\UpdateEmbeddingE{$\\mathbf{X}, \\mathbf{Y}, \\mathbf{P_X}, \\mathbf{P_Y}, \\mathbf{\\Sigma}_1, \\mathbf{B}, \\mathbf{C}, d$}}{\n    \\tcc{Step 1: Orthogonalization}\n    $\\mathbf{W}_\\mathbf{B} \\gets (\\mathbf{B}^\\top \\mathbf{X} \\mathbf{P_X} \\mathbf{\\Sigma}_1^{1/2})^\\top $ \\;\n    $\\mathbf{W}_\\mathbf{C} \\gets (\\mathbf{C}^\\top \\mathbf{Y} \\mathbf{P_Y} \\mathbf{\\Sigma}_1^{1/2})^\\top $ \\;\n    $R_\\mathbf{B} \\gets \\sqrt{ \\Vert \\mathbf{B} \\Vert_2^2 - \\Vert \\mathbf{W}_\\mathbf{B} \\Vert_2^2 }$\\;\n    $R_\\mathbf{C} \\gets \\sqrt{ \\Vert \\mathbf{C} \\Vert_2^2 - \\Vert \\mathbf{W}_\\mathbf{C} \\Vert_2^2 }$\\;\n\n    \\tcc{Step 2: Rediagonalization}\n    $\\mathbf{M} \\gets\\begin{bmatrix}\n            \\Sigma_1 & \\mathbf{0} \\\\\n            \\mathbf{0} & \\mathbf{0}\n        \\end{bmatrix}\n        +\n        \\begin{bmatrix}\n            \\mathbf{W}_\\mathbf{B} \\\\\n            \\mathbf{R}_\\mathbf{B}\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\mathbf{W}_\\mathbf{C} \\\\\n            \\mathbf{R}_\\mathbf{C}\n        \\end{bmatrix}^\\top  $ \\;\n    $\\mathbf{E}, \\Sigma_2, \\mathbf{H} \\gets $ \\texttt{t-SVD}($\\mathbf{M}, d$) \\;\n    \n    \\tcc{Step 3: Space Projection}\n    $\\mathbf{F} \\gets \\boldsymbol{\\Sigma}_1^{-1/2}   (\\mathbf{E}[:d]-R_\\mathbf{B}^{-1}\\mathbf{W}_\\mathbf{B}   \\mathbf{E}[d:])  \\boldsymbol{\\Sigma}^{\\frac{1}{2}}_2$ \\; \n    $\\mathbf{G} \\gets \\boldsymbol{\\Sigma}_1^{-1/2}   (\\mathbf{H}[:d]-R_\\mathbf{C}^{-1}\\mathbf{W}_\\mathbf{C}   \\mathbf{H}[d:])   \\boldsymbol{\\Sigma}^{1/2}_2$ \\;\n    $\\Delta \\mathbf{X} \\gets R_\\mathbf{B}^{-1} \\mathbf{B}   \\mathbf{E}[d:]   \\boldsymbol{\\Sigma}_2^{1/2}$ \\;\n    $\\Delta \\mathbf{Y} \\gets R_\\mathbf{C}^{-1} \\mathbf{C}   \\mathbf{H}[d:]   \\boldsymbol{\\Sigma}_2^{1/2}$ \\;\n    \\KwRet{$\\mathbf{F}, \\mathbf{G}, \\Delta \\mathbf{X}, \\Delta \\mathbf{Y}, \\Sigma_2$ }\\;}\n  \\caption{Update embedding via space projection}\n  \\label{algo:sp}\n\n\\end{algorithm} \n\n\n"
                },
                "subsection 4.5": {
                    "name": "the Proposed DAMF Algorithm",
                    "content": "\n\\label{sec:damf}\n\\begin{algorithm}[t] \n    \\caption{DAMF}%算法名字\n    \\label{algo:damf}\n    \\LinesNumbered %要求显示行号\n    \\SetKwFunction{UpdateEmbeddingN}{UpdateEmebddingN}\n    \\SetKwFunction{UpdateEmbeddingM}{UpdateEmbeddingM}\n    \\SetKwFunction{DynamicEnhancement}{DynamicEnhancement}\n    \\KwIn{Network embedding $\\mathbf{X}_{b}, \\mathbf{Y}_{b}$ in the base space;\n    enhanced embedding $\\mathbf{Z}_{b}$; space projection matrix $\\mathbf{P}_{\\mathbf{X}}, \\mathbf{P}_{\\mathbf{Y}}$; singular values $\\boldsymbol{{\\Sigma}}$; the Changes occurring in the network $Event_t$ residual vector $\\mathbf{r}$; PageRank damping factor $\\alpha$; error tolerance $\\epsilon$; Graph $\\mathcal{G}$; }%输入参数\n    \\KwOut{Updated $\\mathbf{X}_{b}, \\mathbf{Y}_{b}, \\mathbf{Z}_{b}, \\mathbf{P}_{\\mathbf{X}},  \\mathbf{P}_{\\mathbf{Y}}, \\boldsymbol{{\\Sigma}}$}\n\n    \\eIf{$Event_t$ is node change}{\n        Convert $Event_t$ to $\\mathbf{B}_1, \\mathbf{B}_2$ by Eq. (\\ref{eq:node_B}) \\;\n        $\\mathbf{F}, \\mathbf{G}, \\Delta \\mathbf{X}, \\Delta \\mathbf{Y} , \\Sigma \\gets$ \\UpdateEmbeddingN{$\\mathbf{X}, \\mathbf{Y}, \\mathbf{\\Sigma}, \\mathbf{B}_1$} \\;\n        Update $\\mathbf{X}_{b}, \\mathbf{Y}_{b}, \\mathbf{P}_{\\mathbf{X}}, \\mathbf{P}_{\\mathbf{Y}}$ by Eq. (\\ref{eq:update}) \\;\n        $\\mathbf{F}, \\mathbf{G}, \\Delta \\mathbf{X}, \\Delta \\mathbf{Y} , \\Sigma \\gets$ \\UpdateEmbeddingN{$\\mathbf{Y}, \\mathbf{X}, \\mathbf{\\Sigma}, \\mathbf{B}_2$}  \\;\n        Update $\\mathbf{X}_{b}, \\mathbf{Y}_{b}, \\mathbf{P}_{\\mathbf{X}}, \\mathbf{P}_{\\mathbf{Y}}$ by Eq. (\\ref{eq:update}) \\;\n    }{\n        Convert $Event_t$ to $\\mathbf{B}, \\mathbf{C}$ by Eq. (\\ref{eq:edge_B}) \\;\n        $\\mathbf{F}, \\mathbf{G}, \\Delta \\mathbf{X}, \\Delta \\mathbf{Y} , \\Sigma \\gets$ \\UpdateEmbeddingM{$\\mathbf{X}, \\mathbf{Y}, \\mathbf{\\Sigma}, \\mathbf{B}, \\mathbf{C}$}\\;\n        Update $\\mathbf{X}_{b}, \\mathbf{Y}_{b}, \\mathbf{P}_{\\mathbf{X}}, \\mathbf{P}_{\\mathbf{Y}}$ by Eq. (\\ref{eq:update}) \\;\n    }\n    $\\mathbf{Z}_{b}, \\mathbf{r} \\gets $ \\DynamicEnhancement{$\\mathbf{Z}_{b}, \\mathbf{r}, Event_t, \\mathcal{G}, \\Delta \\mathbf{X}, \\alpha, \\epsilon$)}\\;\n    \\KwRet{$\\mathbf{X}_{b}, \\mathbf{Y}_{b}, \\mathbf{Z}_{b}, \\mathbf{P}_{\\mathbf{X}},  \\mathbf{P}_{\\mathbf{Y}}, \\boldsymbol{{\\Sigma}}$}\n\\end{algorithm}\n\nIn this section, we propose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, consisting of four steps: Orthogonalization, Rediagonalization, Space Projection, and Dynamic Embedding Enhancement.\nAlgorithm \\ref{algo:sp} and Algorithm \\ref{algo:damf} is the pseudo-code for the DAMF algorithm.\n\n\\textbf{Step 1: Orthogonalization.}\nAccording to the equations $\\mathbf{X} = \\mathbf{U} \\sqrt{\\Sigma}$ and $\\mathbf{X}_t=\\mathbf{X}_b\\mathbf{P_X}$, we know that $\\mathbf{U}=\\mathbf{X}_b \\mathbf{P_X} \\Sigma^{-1/2}$. \nThen, with $\\mathbf{W_B} = \\mathbf{U}^\\top\\mathbf{B}$ (and $\\mathbf{W_C} = \\mathbf{V}^\\top\\mathbf{C}$ for edge change), we can get that $R_B=\\sqrt{ \\Vert \\mathbf{B}\\Vert_2^2 - \\Vert \\mathbf{W}_\\mathbf{B} \\Vert_2^2 }$, (and $R_C=\\sqrt{ \\Vert \\mathbf{C}\\Vert_2^2 - \\Vert \\mathbf{W}_\\mathbf{C} \\Vert_2^2 }$ for edge change) by Eq. (\\ref{eq:qrnode}) and Eq. (\\ref{eq:qredge2}) and Proposition \\ref{proposition:fast}.\n\n\\begin{proposition}\n    \\label{proposition:fast}\n    Let $\\mathbf{U}\\in \\mathbb{R}^{n\\times d}$ be an arbitrary orthonormal matrix with $\\mathbf{U}^\\top\\mathbf{U}=\\mathbf{I}$. and $\\vec{x} \\in \\mathbb{R}^{n}$ be an arbitrary vector, then \n    $$\\Vert (\\mathbf{I}-\\mathbf{U} \\mathbf{U}^\\top)\\vec{x} \\Vert_2 = \\sqrt{ \\Vert \\vec{x}\\Vert_2^2 - \\Vert \\mathbf{U}^\\top \\vec{x}\\Vert_2^2 }$$\n\\end{proposition}\n\n\\textbf{Step 2: Rediagonalization.}\nIn this step, follow Eq. (\\ref{eq:svd_small_node}) for node change, and Eq. (\\ref{eq:svd_small_edge}) for edge change to get the t-SVD.\n\n\\textbf{Step 3: Space Rotation.}\nFor node change, get $\\mathbf{F}, \\mathbf{G}, \\Delta \\mathbf{X}, \\Delta \\mathbf{Y}$ by Eq. (\\ref{eq:f_node}), Eq. (\\ref{eq:g_node}), Eq. (\\ref{eq:x_node}), Eq. (\\ref{eq:y_node}), respectively. And for edge change, get $\\mathbf{F}, \\mathbf{G}, \\Delta \\mathbf{X}, \\Delta \\mathbf{Y}$ by Eq. (\\ref{eq:f_edge}), Eq. (\\ref{eq:g_edge}), Eq. (\\ref{eq:x_edge}), Eq. (\\ref{eq:y_edge}), respectively. Then update the network embedding with space projection matrix $\\mathbf{F}, \\mathbf{G}, \\Delta \\mathbf{X}, \\Delta \\mathbf{Y}$ by Eq. (\\ref{eq:update}).\n\n\\textbf{Step 4: Dynamic Embedding Enhancement.} \nIn this step, the PPR-enhanced embedding is updated by InstantGNN~\\cite{zheng2022instant}.\nSpecifically, InstantGNN can estimate the PPR values for dynamic graph structures and dynamic node attributes.\nHere, we treat the changes in the graph as the dynamic graph structure, and the updates of few nodes' embedding as the dynamic node attributes.\nInstantGNN will update the embedding so that the results converge to the given error tolerance.\n\n\\textbf{Initialization of DAMF.}\nInitially, $\\mathbf{X}_b, \\mathbf{Y}_b$ is set by a t-SVD of the adjacency matrix $\\mathbf{A}$ of the initial graph, and $\\mathbf{P_X}, \\mathbf{P_y}$ is set to the identity matrix. \nThen, use the basic propagation of PPR in InstantGNN~\\cite{zheng2022instant} to enhance the initial network embedding."
                },
                "subsection 4.6": {
                    "name": "Complexity Analysis",
                    "content": "\n\\label{sec:complexity}\n\nIn this section, we analyze the DAMF algorithm in terms of time and space complexity.\nNote that the time complexity we analysis here is only for a single graph change.\n\nDAMF is very efficient due to the fact that the large-scale matrix multiplications involved in DAMF are mainly of two kinds showed in Figure \\ref{fig:matrixmultiply}. And Lemma \\ref{lemma:complexity_A} (Figure \\ref{fig:matrixmultiply}(a)) and Lemma \\ref{lemma:complexity_B} ((Figure \\ref{fig:matrixmultiply}(b))) demonstrate how to use sparsity of these two special matrix multiplications and how they can be efficiently computed.\nThe proofs of these two lemmas are in the Appendix \\ref{appendix:complexity_A} and Appendix \\ref{appendix:complexity_B}.\n\n\n\n\n\\begin{lemma}\n    \\label{lemma:complexity_A}\n    Let $\\mathbf{A} \\in \\mathbb{R}^{n\\times p}, \\mathbf{B}\\in \\mathbb{R}^{n\\times q}$ be arbitrary matrices with $\\mathbf{B}$ has $t$ non-zero rows, the time complexity to calculate $\\mathbf{B}^\\top \\mathbf{A}$ is $O(tpq)$.\n    % (Figure \\ref{fig:matrixmultiply}(a))\n\\end{lemma}\n\\begin{lemma}\n    \\label{lemma:complexity_B}\n    Let $\\mathbf{B}\\in \\mathbb{R}^{n\\times q}, \\mathbf{C} \\in \\mathbb{R}^{q \\times p}$ be arbitrary matrices with $\\mathbf{B}$ has $t$ non-zero rows, the time complexity to calculate $\\mathbf{BC}$ is $O(tpq)$. \n\\end{lemma}\n\n\\textbf{Time Complexity of Orthogonalization (Step 1). }\nAccording to Lemma \\ref{lemma:complexity_A}, the time complexity of calculating $\\mathbf{B}^\\top \\mathbf{X}$ (or $\\mathbf{C}^\\top \\mathbf{Y}$) is $O((\\Delta m) d^2)$ since $\\mathbf{B}$ (and $\\mathbf{C}$) has at most $\\Delta m$ non-zero rows.\nNext, since $\\mathbf{P_X}$ and $\\Sigma_1^{1/2}$ are $d$-by-$d$ matrices, the complexity of calculating $\\mathbf{W_B}$ (or $\\mathbf{W_C}$, Line 2, 12, 13 in Algorithm \\ref{algo:sp}) is $O((\\Delta m)d^2)$. \nAt last, the time complexity of calculating $R_\\mathbf{B}$(or $R_\\mathbf{C}$, Line 3, 14, 15 in Algorithm \\ref{algo:sp}) is $O(\\Delta m + d)$. \nThus, the overall time complexity of the Orthogonalization step is $O((\\Delta m)d^2)$.\n\n\\textbf{Time Complexity of Rediagonalization (Step 2). }\nThe time complexity of the Rediagonalization step is $O(d^3)$ by directly applying a t-SVD on a $(d+1)$-by-$(d+1)$ matrix (Line 4, 5, 16, 17 in Algorithm \\ref{algo:sp}).\n\n\\textbf{Time Complexity of Space Rotation (Step 3). }\nThe time complexity of calculating $\\mathbf{F}$ and $\\mathbf{G}$ (Line 6, 7, 18, 19 in Algorithm \\ref{algo:sp}) is $O(d^3)$.\nAccording to Lemma \\ref{lemma:complexity_B}, the time complexity of calculating $\\Delta \\mathbf{X}$ and $\\Delta \\mathbf{Y}$ (Line 8, 9, 20, 21 in Algorithm \\ref{algo:sp}) is $O((\\Delta m) d^2)$.\n\n\n\nMoreover, the time complexity of space projection (Line 4, 6, 10 in Algorithm \\ref{algo:damf}) is $O((\\Delta m)d^2+d^3)$ by the following steps: (1) computing the update of $\\mathbf{P_X}$ by a $d$-by-$d$ matrix multiplication; (2) computing the inverse of $\\mathbf{P_X}$; (3) computing $\\mathbf{BP}^{-1}_\\mathbf{X}$. The time complexity of both the matrix multiplication and the inverse is $O(d^3)$, while the time complexity of $\\mathbf{BP}^{-1}_\\mathbf{X}$ is $O((\\Delta m)d^2)$ according to Lemma \\ref{lemma:complexity_B}.\n\n\\textbf{Time Complexity of Dynamic Embedding Enhancement (Step 4).}\n% Let $C$ be the maximum of the $l_1$-norm of the node embeddings, then the time complexity of the propagation is $O(\\frac{(\\Delta m) C}{\\alpha \\epsilon})$. Since we let the $l_1$-norm of a node's residual be reduced by at least alpha epsilon, the time complexity is the sum of the $l_1$-norm of each node's residual for each propagation. Each time the edges of the signal change so that the $l_1$-norm of the node's residual increases by at most $O(\\Delta m C)$. Therefore the time complexity for each node or edge update is $O(\\frac{(\\Delta m) C}{\\alpha \\epsilon})$.\n% A direct way to calculate the complexity of this step is to substitute the change of $\\Delta m$ edges and the change of embedding of $\\Delta m$ nodes into Theorem 3 and Theorem 4 of ~\\cite{zheng2022instant}.\nLet $c=\\max_{u\\in \\mathcal{V}}\\Vert deg(u) \\mathbf{X}(u) \\Vert_\\infty$ where $\\mathbf{X}[u]$ is the node $u$'s context embedding. Since the number of edges changed by the graph structure is $\\Delta m$, and at most $\\Delta m$ nodes' context embedding that need to be updated in the previous step, according to Theorem 3 and Theorem 4 in \\cite{zheng2022instant}, the time complexity of this step is $O(\\frac{(\\Delta m)cd}{\\alpha^2 \\epsilon})$.\n\n% The detailed proof is shown in the Append ix.\n\nOverall, if we consider embedding dimension $d$, Personalized PageRank damping factor $\\alpha$, error tolerance $\\epsilon$ and $c$ as constant, the purposed DAMF algorithm achieves a dynamic network embedding for a single graph change with time complexity $O(\\Delta m)$.\n\n\\textbf{Time Complexity of Query Node's Embedding}\nThe node embedding query at any moment can be obtained by multiplying the embedding $\\mathbf{X}_b, \\mathbf{Y}_b$ of that node in the initial space by the space projection matrix $\\mathbf{P}_\\mathbf{X}, \\mathbf{P}_\\mathbf{y}$ with time complexity of $O(d^2)$, while the direct query in the static embedding method has a time complexity of $O(d)$. When we consider dimensionality a constant, the query complexity is $O(1)$.\n\n\n\n\n\\textbf{Space Complexity.} \nThe largest matrices in the DAMF method is an $n$-by-$d$ matrix, so the space complexity is $O(nd)$.\nDue to the additional need to store the structure of the graph, the total space complexity is $O(nd + m)$.\n\n"
                }
            },
            "section 5": {
                "name": "Experiment",
                "content": "\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{booktabs}\n\nIn this section, we experimentally compare DAMF to six existing methods on six datasets of varying sizes for three popular analytic tasks: node classification, link prediction, and graph reconstruction. Section \\ref{sec:settings} and Section \\ref{sec:tasks} introduce the experimental settings and tasks.\nIn Section \\ref{sec:smallg}, Section \\ref{sec:largeg}, and Section \\ref{sec:massive}, we present our experimental results on small, large, and massive graphs respectively. Finally, we compare the runtime of DAMF with baselines in Section \\ref{sec:runtime} and analyze the results of the ablation study on dynamic embedding enhancement in Section \\ref{sec:ablation}.\n\n\n\n",
                "subsection 5.1": {
                    "name": "Experimental Settings",
                    "content": "\n\n\\label{sec:settings}\n\\textbf{Baseline}.\nWe evaluate DAMF against six existing methods, including LocalAction~\\cite{liu2019real}, GloDyNE~\\cite{hou2020glodyne}, Dynnode2vec~\\cite{mahdavi2018dynnode2vec}, RandNE~\\cite{zhang2018billion}, DynGEM~\\cite{goyal2018dyngem} and DNE~\\cite{du2018dynamic}.\n\n\\noindent\\textbf{Datasets}. \nWe conduct experiments on 6 publicly available graph datasets and divide the datasets into small, large, and massive scales.\nAs shown in Table \\ref{tab:dataset}, small ones include \\emph{Wiki}~\\cite{wiki}, \\emph{Cora}~\\cite{hou2020glodyne} and \\emph{Flickr}~\\cite{flickr}, large ones include \\emph{YouTube}~\\cite{youtube} and \\emph{Orkut}~\\cite{orkut}, and \\emph{Twitter}~\\cite{kwak2010twitter} is considered as a massive dataset.\nA short description of each dataset is given in Appendix \\ref{appendix:dataset}.\n\n\\noindent \\textbf{Setting of Dynamicity}.\nWe follow GloDyNE's setup for the real dataset \\emph{Cora}~\\cite{hou2020glodyne}, a widely accepted dataset whose dynamic graph has 11 snapshots.\nFor other datasets, following the setting in LocalAction~\\cite{liu2019real} and DNE~\\cite{du2018dynamic}, we start with a small number of nodes and gradually add the remaining ones to the graph individually (streaming scenario); for the discrete methods GloDyNE~\\cite{hou2020glodyne}, dynGEM~\\cite{goyal2018dyngem}, Dynnode2vec~\\cite{mahdavi2018dynnode2vec}, the continuous streaming modifications are split into 100 discrete modifications.\nTo reflect the reality of the recent exponential rise in social network users, we utilize a considerably lower initial setting of 1000 nodes compared with the methods above.\n\n \n\\noindent \\textbf{Time Limits}.\nFor small and large datasets, methods that have not produced results for more than $3$ days ($72$ hours) will not be included in the results. For massive datasets, methods that have not produced results for more than $7$ days ($168$ hours) will not be included in the results. \n \n\\noindent \\textbf{Paramater Settings}. \nThe embedding dimension $d$ of all methods is set to $128$ for a fair comparison. \nFor DAMF, the damping factor $\\alpha$ for PPR is set to $0.3$ (except for the node classification task on \\emph{Cora} where it is set to $0.03$) and the error tolerance $\\epsilon$ is set to $10^{-5}$.\n\n% \\noindent \\textbf{Numerial Tricks}. \n% To avoid the matrix in the space projection-based approach being ill-conditioned after a large number of multiplications, we reset the projection matrix to identity with $O(nd^2)$ extra time whenever its condition number exceeds 10.\n% The activation of such an operation is uncommon and needs minimal time.\n% The experimental result shows the number of times required to reset the projection matrix to the identity matrix for each data less than 19 for small and large datasets.\n\n"
                },
                "subsection 5.2": {
                    "name": "Experimental Tasks",
                    "content": "\n\\label{sec:tasks}\n% three tasks\nDynamic network embedding is tested on three tasks: node classification, link prediction and graph reconstruction.\n \n%Node Classification\n\\textbf{Node Classification} is a common model training task to obtain the labels based on the embedding of each node by training a simple classifier.\nFollowing previous work~\\cite{tsitsulin2018verse, yang2020nrp}, we randomly select a subset of nodes to train a one-vs-all logistic regression classifier, and then use the classifier to predict the labels of the remaining nodes.\nFor each node $v$, we first concatenate the normalized context and content vectors as the feature representation of $v$ and then feed it to the classifier.\n\n%Link Prediction\n\\textbf{Link Prediction} is the method of predicting the possibility of a link between two graph nodes.\nBased on earlier research, we first generate the modified graph $\\mathcal{G'}$ by removing 30\\% of randomly selected edges from the input graph $\\mathcal{G}$, and then construct embeddings on $\\mathcal{G'}$.\nThen, we build the testing set $\\mathcal{E}_{test}$ by selecting the node pairs connected by the removed edges and an equal number of unconnected node pairs in $\\mathcal{G}$. \n\nThe inner product of node's embedding are used to make predictions.\nResults are evaluated by \\emph{Area Under Curve (AUC)} and \\emph{Average Precision(AP)}.  \n\n%Graph Reconstruction\n\\textbf{Graph Reconstruction} is a basic objective of network embedding.\nAccording to earlier work, we begin by selecting a set $\\mathcal{S}$ of node pairs from the input graph $\\mathcal{G}$. Afterward, we apply the same method used in link prediction to generate a score for each pair. Then, we calculate the \\emph{precision@K}, the proportion of the top-K node pairings that match edges in $\\mathcal{G}$.\n \n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Experiment",
                    "content": "\n\\textbf{Experiment on Small Graphs.}\n\\label{sec:smallg}\nWe conduct experiments on small graphs \\emph{Wiki}, \\emph{Cora} and \\emph{Flickr} with all three tasks of Node Classification, Link Prediction, and Graph Reconstruction.\n\n\n\nThe results of node classification, link prediction and graph reconstruction are displayed in Figure \\ref{fig:nc}, Table \\ref{tab:lp} and Figure \\ref{fig:gr} respectively. \nOn the node classification task, DAMF outperforms all other competitors significantly at \\emph{Wiki}, is comparable to other methods on other datasets, and had better performance when the ratio of training data was smaller.\n%lp\nFor link prediction, we can see that DAMF outperforms the baselines on almost all datasets based on both \\emph{AUC} and \\emph{AP} values, except on the \\emph{Cora} dataset where it is slightly inferior compared to GloDyNE. In terms of the result of graph reconstruction, DAMF is significantly better than the other baselines. \nOverall, the DAMF method performs well and is stable across different data sets. \n\n\n \n\n%gr\n\n\n\n \n\\textbf{Experiment on Large Graphs.}\n\\label{sec:largeg}\nOur extensive graph includes \\emph{YouTube} and \\emph{Orkut} datasets. Unlike small graphs, we only conduct Node Classification and Link Prediction. Due to the high number of potential $\\binom{n}{2}$ pairs, we discard the Graph Reconstruction task. It is worth noting that only LocalAction reaches the set time limit among all competitors.\n\n%nc\nThe results of node classification presented in Figure \\ref{fig:nc} indicate that DAMF performs significantly better in comparison to LocalAction on both large graphs.\nThe \\emph{AUC} and \\emph{AP} results for the link prediction task are shown in Table \\ref{tab:llp}. In both indicators, DAMF has a significant advantage over the only competitor that can complete the work within the allotted time on large graphs.\n\n\\textbf{Experiment on the Massive Graphs.}\n\\label{sec:massive}\n%par1 - setting\n%Motivation\nMassive graphs with billion-level edges such as large-scale social networks are widely available in the industry.\nHowever, to the best of our knowledge, dynamic network embedding studies on massive graphs with billion-level edges are unprecedented.\nWe conduct the first dynamic network embedding experiments on a billion-level edge dataset \\emph{Twitter} with 41 million nodes and \\textbf{1.4 billion} edges, and mapping each node as a $128$-dimensional vector which is more than 10 times as many learnable parameters as BERT-Large~\\cite{kenton2019bert}.\n\n%link prediction\nWe conduct a link prediction task to determine the nodes' connection probability. Notably, we reduce the ratio of edges deleted in the first stage from 30\\% to 0.5\\% to ensure graph connectivity when there are only a few nodes.\n%\nWe discard node classification and graph reconstruction tasks since the Twitter dataset lacks labels to build a classifier and the excessive number of potential pairings makes reconstruction unattainable.\n\n%par2 - result\nTable \\ref{tab:llp} shows the \\emph{AUC} and \\emph{AP} values. Overall, DAMF performs well on both evaluation indicators, obtaining an \\emph{AUC} value of $0.9055$ and an \\emph{AP} of $0.9353$. Moreover, DAMF's overall updating time of $\\textbf{110}$ hours demonstrates that DAMF is capable of changing parameters at the billion level in under $10$ milliseconds.\n \n%par3 - comparison \nAll baseline methods except LocalAction~\\cite{liu2019real} exceed the set time limit (7 days) and are therefore excluded. However, LocalAction seems to fail to converge on this dataset since its \\emph{AUC} on link prediction is only $0.5006$.\n \n"
                },
                "subsection 5.4": {
                    "name": "Efficency Study",
                    "content": "\n\\label{sec:runtime}\n\n\n\nTable \\ref{tab:run} shows the running time of each method on small and large datasets. The experimental results show that the speed of DAMF increases more consistently as the size of the dataset expands. According to Table \\ref{tab:lp} and Table \\ref{tab:llp}, although LocalAction is faster, its \\emph{AUC} is only slightly greater than 0.5, indicating that LocalAction makes unbearable tread-offs to obtain speed, whereas DAMF maintains stable effectiveness.\n\n"
                },
                "subsection 5.5": {
                    "name": "Ablation Study",
                    "content": "\n\\label{sec:ablation}\n\nTo investigate whether the dynamic embedding enhancement better captures the information of higher-order neighbors and thus improves the quality of dynamic network embedding, we obtain unenhanced embeddings by setting the PageRank damping factor $\\alpha$ to 1.\nThe experimental results in Figure \\ref{fig:nc} ,Table \\ref{tab:lp}, Table \\ref{tab:llp} and Figure \\ref{fig:gr} show that the enhanced embeddings are significantly better in node classification, link prediction and graph reconstruction on \\emph{Wiki}, demonstrating the effectiveness of the dynamic embedding enhancement.\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nIn this work, we propose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, which utilizes a projection onto the embedding space and modifies a few node embeddings to achieve an efficient adjacency matrix decomposition method.\nIn addition, we use dynamic Personalized PageRank to enhance the embedding to capture high-order neighbors' information dynamically. \nExperimental results show that our proposed method performs well in node classification, link prediction, and graph reconstruction, achieving an average dynamic network embedding update time of 10ms on billion-edge graphs.\n\n\\noindent \\textbf{Acknowledgement.} This work is supported by NSFC (No.62176233), \nNational Key Research and Development Project of China \n\n\\noindent (No.2018AAA0101900) and Fundamental Research Funds for the Central Universities.\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{sample-base}\n\n\\appendix\n% \\newpage\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n"
            },
            "section 8": {
                "name": "Initialization of DAMF",
                "content": "\n\n\\begin{algorithm}[h] \n    \\caption{Initialization of DAMF}\n    \\label{algo:init}\n    \\LinesNumbered %要求显示行号\n    \\KwIn{A graph $\\mathcal{G}$ with adjacency matrix $\\mathbf{A}$, embedding dimension $d$, PageRank damping factor $\\alpha$, error tolerance $\\epsilon$.}%输入参数\n    \\KwOut{$\\mathbf{X}_b, \\mathbf{Y}_b, \\mathbf{Z}_b, \\mathbf{P_X}, \\mathbf{P_Y}, \\mathbf{r}$}\n    $\\mathbf{U}, \\mathbf{\\Sigma}, \\mathbf{V} \\gets \\textbf{\\texttt{t-SVD}}(\\mathbf{A}, d)$\\;\n    $\\mathbf{X}_b \\gets \\mathbf{U} \\mathbf{\\Sigma}^{1/2}, \\quad \\mathbf{Y}_b \\gets \\mathbf{V} \\mathbf{\\Sigma}^{1/2}$\\;\n    $\\mathbf{P_X}\\gets \\mathbf{I},\\quad \\mathbf{P_Y} \\gets \\mathbf{I}$\\;\n    $\\mathbf{r} \\gets \\mathbf{X}_b, \\quad \\mathbf{Z}_b \\gets \\mathbf{O}$\\;\n    $\\mathbf{Z}_b, \\mathbf{r} \\gets \\texttt{Propagation}(\\mathbf{Z}_b, \\mathbf{r}, \\mathcal{G}, \\alpha, \\epsilon)$\\;\n    \\KwRet{$\\mathbf{X}_b, \\mathbf{Y}_b, \\mathbf{Z}_b, \\mathbf{P_X}, \\mathbf{P_Y}, \\mathbf{r}$}\\;\n\\end{algorithm}\n\nAlgorithm \\ref{algo:init} gives a detailed pseudo-code for the initialization of the DAMF. The random projection-based truncated SVD algorithm \\cite{halko2011randomsvd} is able to complete the t-SVD in $O(nd^2+md)$ time, while the time complexity of the PPR enhancement that follows the initialization is O($\\frac{ncd}{\\alpha \\epsilon}$)~\\cite{zheng2022instant}. Overall, the time complexity of the initialization step is $O(nd^2 + md + \\frac{ncd}{\\alpha \\epsilon})$.\n\n"
            },
            "section 9": {
                "name": "Proof",
                "content": "\n",
                "subsection 9.1": {
                    "name": "lemma:nzr",
                    "content": "\n\\begin{proof}[Proof of Lemma \\ref{lemma:nzr}]\nThe $i$-th row of the result matrix of the matrix multiplication of $\\mathbf{BC}$ can be considered as the $i$-th row of $\\mathbf{B}$ multiplied by the matrix $\\mathbf{C}$. \nTherefore, if the $i$-th row of $\\mathbf{B}$ is all-zero, the $i$-th row of the result matrix will also be all-zero. \nSince $\\mathbf{B}$ has only t non-zero rows, $\\mathbf{BC}$ has at most $t$ non-zero rows.\n\\end{proof}\n\n\n"
                },
                "subsection 9.2": {
                    "name": "proposition:fast",
                    "content": "\n\\begin{proof}[Proof of Proposition \\ref{proposition:fast}]\n\\begin{equation}\n\\begin{aligned}\n\\left \\|  (\\mathbf{I}-\\mathbf{UU}^\\top)\\vec{x} \\right \\| _{2}\n&=\\sqrt{((\\mathbf{I}-\\mathbf{UU}^\\top)\\vec{x})^\\top (\\mathbf{I}-\\mathbf{UU}^\\top)\\vec{x}} \\\\\n&=\\sqrt{\\vec{x}^\\top (\\mathbf{I}-\\mathbf{UU}^\\top)^\\top (\\mathbf{I}-\\mathbf{UU}^\\top) \\vec{x}} \\\\\n&=\\sqrt{\\vec{x}^\\top (\\mathbf{I}-2\\mathbf{UU}^\\top + \\mathbf{UU}^\\top\\mathbf{UU}^\\top) \\vec{x}} \\\\\n% ?\n&=\\sqrt{\\vec{x}^\\top (\\mathbf{I}-\\mathbf{UU}^\\top) \\vec{x}} \\\\\n&=\\sqrt{\\vec{x}^\\top \\vec{x} - \\vec{x}^\\top \\mathbf{UU}^\\top \\vec{x}} \\\\\n&=\\sqrt{\\Vert \\vec{x} \\Vert_2^2 - \\Vert \\mathbf{U}^\\top\\vec{x} \\Vert_2^2} \\\\\n\\end{aligned}\n\\end{equation}\n\\end{proof}\n\n\n"
                },
                "subsection 9.3": {
                    "name": "lemma:complexity_A",
                    "content": "\n\\label{appendix:complexity_A}\n\\begin{proof}\n\nWhen performing the matrix multiplication of $\\mathbf{B}^\\top \\mathbf{A}$, the element on the $i$-th row and $j$-th column of the result matrix can be obtained by the product of the $i$-th row of $\\mathbf{B}^\\top$ and the $j$-th row of $\\mathbf{A}$ with\n\\begin{equation}\n    (\\mathbf{B}^\\top \\mathbf{A})[i, j] = \\mathbf{B}^\\top [i] \\cdot \\mathbf{A}[:,j]\n\\end{equation}\n\nBecause $\\mathbf{B}$ has at most $t$ non-zero rows, $\\mathbf{B}^\\top[i]$ has at most $t$ non-zero elements. \nBy skipping zeros in calculating the product, the above equation can be calculated with the time complexity of $O(t)$.\nAnd since $\\mathbf{B}^\\top \\mathbf{A}$ is a $q$-by-$p$ matrix, the time complexity of calculating $\\mathbf{B}^\\top \\mathbf{A}$ is $O(tpq)$.\n\nAlgorithm \\ref{algo:complexity_A} is a pseudo-code for the above scheme.\n\n\\end{proof}\n\n\\begin{algorithm}[hb] \n    \\caption{Algorithm for Lemma \\ref{lemma:complexity_A}}\n    \\label{algo:complexity_A}\n    \\LinesNumbered %要求显示行号\n    \\KwIn{$\\mathbf{A}\\in \\mathbb{R}^{n\\times p}, \\mathbf{B} \\in \\mathbb{R}^{n \\times q}$ and $\\mathbf{B}$ has $t$ non-zero rows.}%输入参数\n    \\KwOut{$\\mathbf{B}^\\top \\mathbf{A}$}\n    $\\mathbf{C} \\gets \\mathbf{O}_{q\\times p}$\\;\n    \\ForEach{l with $\\mathbf{B}[l]$ is non-zero}{\n        \\For{$i\\gets 1$ to $q$}{\n            \\For{$j\\gets 1$ to $p$}{\n                $\\mathbf{C}[i,j] \\gets \\mathbf{C}[i,j] + \\mathbf{A}[l,j] \\times \\mathbf{B}[l, i]$\\;\n            }\n        }\n    }\n    \\KwRet{$\\mathbf{C}$}\n\\end{algorithm}\n\n\n"
                },
                "subsection 9.4": {
                    "name": "lemma:complexity_B",
                    "content": "\n\\label{appendix:complexity_B}\n\\begin{proof}\nFrom Lemma \\ref{lemma:nzr}, there are only at most $t$ non-zero rows in the result of $\\mathbf{BC}$. \nSo, by skipping the calculation of all-zero rows, the time complexity of of calculating $\\mathbf{BC}$ is $O(tpq)$.\n\nAlgorithm \\ref{algo:complexity_B} is a pseudo-code for the above scheme.\n\\end{proof}\n\\begin{algorithm}[hb] \n    \\caption{Algorithm for Lemma \\ref{lemma:complexity_B}}\n    \\label{algo:complexity_B}\n    \\LinesNumbered %要求显示行号\n    \\KwIn{$\\mathbf{B} \\in \\mathbb{R}^{n \\times q}$ and $\\mathbf{B}$ has $t$ non-zero rows, $\\mathbf{C} \\in \\mathbb{R}^{q \\times p}$}%输入参数\n    \\KwOut{$\\mathbf{BC}$}\n    $\\mathbf{D} \\gets \\mathbf{O}_{n\\times p}$\\;\n    \\ForEach{l with $\\mathbf{B}[l]$ is non-zero}{\n        \\For{$i\\gets 1$ to $q$}{\n            \\For{$j\\gets 1$ to $p$}{\n                $\\mathbf{D}[l,j] \\gets \\mathbf{D}[l,j] + \\mathbf{B}[l,i] \\times \\mathbf{C}[i, j]$\\;\n            }\n        }\n    }\n    \\KwRet{$\\mathbf{D}$}\n\\end{algorithm}\n\n\n\n\n"
                }
            },
            "section 10": {
                "name": "Detailed Experimental Settings",
                "content": "\n\nAll experiments are conducted using 8 threads on a Linux machine powered by an AMD Epyc 7H12@3.2GHz and 768GB RAM.\nFor baselines that require a GPU, we used an additional NVIDIA GeForce RTX 3090 with 24G memory.\nThe experimental results for each task are the average of the results of five experiments.\n\n",
                "subsection 10.1": {
                    "name": "Additional Details of Link Prediction",
                    "content": "\n\nThe objective of link prediction is, for the directed node pair $(u, v)$, to predict whether there is a directed edge from $u$ to $v$.\nFor each pair of nodes $(u, v)$ in the test set, we determine a score by taking the inner product of $u$'s context vector and $v$'s content vector.\nFor methods that do not distinguish between context embedding and content embedding, we set both their context embedding and content embedding to be node embeddings.\nFor undirected graphs, we calculate link prediction scores for both directions separately, and then select the larger value as the score.\n\n"
                },
                "subsection 10.2": {
                    "name": "Additional Details of Graph Reconstruction",
                    "content": "\nFor \\emph{Wiki} and \\emph{Cora} datasets, we define set $\\mathcal{S}$ as the collection of all conceivable node pairs. On \\emph{Flickr}, we construct $\\mathcal{S}$ by taking a 1\\% sample of all possible pairs of nodes. Figure \\ref{fig:gr} depicts the performance of all methods in the graph reconstruction task with $K$ values ranging from $10^{1}$ to $10^{6}$. DAMF performs better than its competitors on all the datasets and for nearly every $K$ value. The remarkable accuracy of DAMF is especially noticeable on the \\emph{Wiki} and \\emph{Cora} datasets as $K$ increases.\n\n"
                },
                "subsection 10.3": {
                    "name": "Dataset",
                    "content": "\n\n\\label{appendix:dataset}\n\\noindent \n\n\\textbf{Wiki~\\cite{wiki}} is a hyperlinked network of Wikipedia. Each node in the directed graph represents a page, and the edges represent hyperlinks.\n\n\\textbf{Cora~\\cite{hou2020glodyne}} is a dynamic citation undirected network where each node represents a paper, the edges represent citations, and each article is labeled with the domain to which the article belongs.\n\n\\textbf{Flickr~\\cite{flickr}} is an undirected network of user links on Flickr, where nodes represent users and labels are groups of interest to the user.\n\n\\textbf{YouTube~\\cite{youtube}} is a video-sharing website where users can upload and watch videos, and share, comment and rate them. Each user is labeled with their favorite video genre.\n\n\\textbf{Orkut~\\cite{orkut}} is an undirected social network where nodes represent users and edges represent user interactions. Users are labeled with the community they are in.\n\n\\textbf{Twitter~\\cite{kwak2010twitter}} is a massive-scale directed social network where nodes represent users and edges represent the following relationship. \n\n"
                },
                "subsection 10.4": {
                    "name": "Code",
                    "content": "\nWe use the following code as our baseline:\n\nDynnode2vec~\\cite{mahdavi2018dynnode2vec}: https://github.com/pedugnat/dynnode2vec\n\nGloDyNE~\\cite{hou2020glodyne}: https://github.com/houchengbin/GloDyNE\n\nDynGEM~\\cite{goyal2018dyngem}: https://github.com/palash1992/DynamicGEM\n\nDNE~\\cite{du2018dynamic}: https://github.com/lundu28/DynamicNetworkEmbedding\n\nRandNE~\\cite{zhang2018billion}: https://github.com/ZW-ZHANG/RandNE\n\nAs we could not find the code for LocalAction~\\cite{liu2019real}, we re-implemented the pseudo-code from the paper in Python and made our implementation available along with our experimental code.\n\nOur code for experiments is available on:\n\nhttps://github.com/zjunet/DAMF\n\n\n% \\newpage\n\n\n"
                }
            }
        },
        "tables": {
            "tab:notaions": "\\begin{table}[htbp]\n% \\vspace{-6em} \n\\newcommand{\\tabincell}[2]{\\begin{tabular}{@{}#1@{}}#2\\end{tabular}}\n    \\caption{Notations}\n    \\label{tab:notaions}\n    \\begin{tabular}{p{1.1cm} | p{6.5cm}}\n        % \\toprule\n        \\toprule\n        Notation & Description\\\\\n        \\midrule\n        $\\mathcal{G}(\\mathcal{V},\\mathcal{E})$ & the graph with node set $\\mathcal{V}$ and edge set $\\mathcal{E}$ \\\\\n        $n, m$ & the number of nodes and the number of edges \\\\\n        $\\Delta m$ & the number of edges change in graph \\\\\n        $deg(u)$ & the degree (in-degree or out-degree) of node $u$ \\\\\n        $\\mathbf{A}$, $\\mathbf{D}$  & the adjacency matrix and degree matrix \\\\\n        $\\mathbf{B}$, $\\mathbf{C}$ & the low-rank representation of the updated matrix. \\\\\n        $\\mathbf{X}$, $\\mathbf{Y}$ & the context embedding and content embedding \\\\\n        $\\mathbf{Z}$ & the enhanced context embedding \\\\\n        $\\mathbf{F}, \\mathbf{G}$ & the space projection matrix for $\\mathbf{X}$ and $\\mathbf{Y}$ \\\\\n        $\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}$ & the node vectors that are directly modified \\\\\n        $d$ & the dimension of the embedding space \\\\\n        % $\\mathbf{\\Pi}$ & the \\textit{Personalized PageRank} matrix \\\\\n        $\\alpha$ & the damping factor in \\textit{Personalized PageRank} \\\\\n        $\\epsilon$ & the error tolerance in \\textit{Personalized PageRank} \\\\\n        % \\hline \n        % $\\mathbf{M}_t$ & the snapshot of $\\mathbf{M}$ just after the $t$-th event \\\\\n        % $\\mathbf{M}^\\top, \\mathbf{M}^{-1}$ & the transpose of $\\mathbf{M}$ and the inverse of $\\mathbf{M}$ \\\\\n        % $\\mathbf{M}[:l]$ & the submatrix consisting of the first $l$ rows of $\\mathbf{M}$ \\\\\n        % $\\mathbf{M}[l:]$ & the submatrix consisting rows from the $(l+1)$-th rows to the last row of $\\mathbf{M}$ \\\\\n        \\bottomrule\n\\end{tabular}\n% \\vspace{-1em}\n\\end{table}",
            "tab:dataset": "\\begin{table}[h]\n\\caption{Statistics of Datasets}\n\\label{tab:dataset}\n\\begin{tabular}{cc|c|c|c}\n\\toprule\n\\multicolumn{2}{c|}{\\textbf{Dataset}}                 & $\\mathbf{|\\mathcal{V}|}$ & $\\mathbf{|\\mathcal{E}|}$ & \\textbf{\\#labels} \\\\ \\hline\n\\multicolumn{1}{c|}{\\multirow{3}{*}{small}} & Wiki    & 4,777                     & 184,812                  & 40                \\\\ \\cline{2-5} \n\\multicolumn{1}{c|}{}                       & Cora    & 12,022                   & 45,421                   & 10                \\\\ \\cline{2-5} \n\\multicolumn{1}{c|}{}                       & Flickr  & 80,513                   & 11,799,764               & 195               \\\\ \\hline\n\\multicolumn{1}{c|}{\\multirow{2}{*}{large}} & YouTube & 1,138,499                & 2,990,443                & 47                \\\\ \\cline{2-5} \n\\multicolumn{1}{c|}{}                       & Orkut   & 3,072,441                & 117,185,083              & 100               \\\\ \\hline\n\\multicolumn{1}{c|}{massive}                & Twitter & 41,652,230               & \\textbf{1,468,365,182}   & -                 \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:lp": "\\begin{table}[h]\n\\captionsetup{justification=centering}\n\\setlength{\\abovecaptionskip}{0.2cm}\n\\caption{Link prediction results on small graphs\\\\\nTLE: Time Limit Exceeded, $\\times $: No Legal Output\n}\\\n\\setlength{\\tabcolsep}{0.7mm}\n\\label{tab:lp}\n\\begin{tabular}{@{}c|cccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\diagbox{\\textbf{Method}}{\\textbf{Dataset}}} & \\multicolumn{3}{c|}{\\textbf{AUC}}                    & \\multicolumn{3}{c}{\\textbf{AP}}                              \\\\ \\cmidrule(l){2-7} \n                           & Wiki            & Cora            &\\multicolumn{1}{c|}{Flickr}          & Wiki            & Cora                     & Flickr          \\\\ \\midrule\nLocalAction                & 0.5083          & 0.5562          & \\multicolumn{1}{c|}{0.4995}         & 0.5812          & 0.5923                   & 0.5833          \\\\\nGloDyNE                    & 0.6386          & \\textbf{0.9296}          & \\multicolumn{1}{c|}{0.8511}               & 0.6844          & \\textbf{0.9383}          & 0.8639               \\\\\nDynnode2vec                & 0.5904          & 0.8242          & \\multicolumn{1}{c|}{0.8187}          & 0.6639          & 0.8975                   & 0.8476          \\\\\nRandNE                     & 0.1092          & 0.8324          & \\multicolumn{1}{c|}{TLE}               & 0.3208          & 0.8435                   & TLE               \\\\\nDynGEM                 & $\\times$               & 0.8340          & \\multicolumn{1}{c|}{TLE}               & $\\times$               & 0.8568                   & TLE               \\\\\nDNE                        & 0.1845          & 0.8407          & \\multicolumn{1}{c|}{0.5361}          & 0.3391          & 0.8662                   & 0.5287          \\\\ \\midrule\nDAMF($\\alpha = 1$)                       & 0.6618          & 0.8555          & \\multicolumn{1}{c|}{0.9269}          & 0.7541          & 0.8853                   & 0.9474          \\\\\nDAMF         & \\textbf{0.7543} & 0.9010 & \\multicolumn{1}{c|}{\\textbf{0.9550}} & \\textbf{0.7840} & 0.9168 & \\textbf{0.9615} \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:llp": "\\begin{table}[h]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\caption{Link prediction results on large and massive graphs}\n\\label{tab:llp}\n\\setlength{\\tabcolsep}{0.35mm}\n\\begin{tabular}{@{}c|cccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\diagbox{\\textbf{Method}}{\\textbf{Dataset}}} & \\multicolumn{3}{c|}{\\textbf{AUC}}                                                  & \\multicolumn{3}{c}{\\textbf{AP}}                              \\\\ \\cmidrule(l){2-7} \n                        & Youtube         & Orkut           & \\multicolumn{1}{c|}{Twitter}         & Youtube         & Orkut           & Twitter         \\\\ \\midrule\nLocalAction             & 0.4849          & 0.4978          & \\multicolumn{1}{c|}{0.5006}          & 0.5548          & 0.5355          & 0.5923          \\\\ \\midrule\nDAMF($\\alpha=1$)        & 0.7648          & 0.8662          & \\multicolumn{1}{c|}{0.8732}                & 0.8290          & 0.8828          &    0.9018             \\\\\nDAMF                    & \\textbf{0.7946} & \\textbf{0.8724} & \\multicolumn{1}{c|}{\\textbf{0.9055}} & \\textbf{0.8510} & \\textbf{0.8882} & \\textbf{0.9353} \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:run": "\\begin{table}[t]\n\\setlength{\\abovecaptionskip}{0.cm}\n\\caption{Running time}\n\\setlength{\\abovecaptionskip}{0.cm}\n\\setlength{\\tabcolsep}{0.8mm}\n\\label{tab:run}\n\\begin{tabular}{@{}c|ccc|cc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{\\diagbox{Method}{Size}}} & \\multicolumn{3}{c|}{\\textbf{Small}}                                       & \\multicolumn{2}{c}{\\textbf{Large}}                 \\\\ \\cmidrule(l){2-6} \n                               & Wiki & Cora           & Flickr              & Youtube             & Orkut               \\\\ \\midrule\nLocalAction                    & \\textbf{7s}              &  \\textbf{6s}      &   \\textbf{5m28s}    &   \\textbf{5m10s} & \\textbf{1h43m}       \\\\ \nGloDyNE                        & \\textbf{10m45s}           & \\textbf{1m5s}  & \\textbf{18h38m}     & \\textgreater{}3days & \\textgreater{}3days \\\\ \nDynnode2vec                    & \\textbf{1m58s}            & \\textbf{1m40s} & \\textbf{5h10m}      & \\textgreater{}3days & \\textgreater{}3days \\\\ \nRandNE                         & \\textbf{3m}               &  \\textbf{5s} & \\textgreater{}3days & \\textgreater{}3days & \\textgreater{}3days \\\\ \nDynGEM                     & \\textbf{17h4m}            & \\textbf{6h20m} & \\textgreater{}3days & \\textgreater{}3days & \\textgreater{}3days \\\\ \nDNE                            & \\textbf{22m38s}           & \\textbf{8m57s}  & \\textbf{8h11m}      & \\textgreater{}3days & \\textgreater{}3days \\\\ \\midrule\nDAMF($\\alpha=1$)               & \\textbf{36s}              & \\textbf{1m19s} & \\textbf{33m13s}     & \\textbf{3h10m}      & \\textbf{8h15m}      \\\\ \nDAMF                           & \\textbf{2m47s}            & \\textbf{2m6s}  & \\textbf{1h33m}      & \\textbf{3h39m}      & \\textbf{14h7m}      \\\\ \n\\bottomrule\n\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:DAMF": "\\begin{figure*}\n    \\centering\n    \\includegraphics[width=6in]{fig/framework.png}\n    \\caption{Overview of DAMF}\n    \\label{fig:DAMF}\n\\end{figure*}",
            "fig:a": "\\begin{figure}[htbp]\n\t\\centering\n        \\setlength{\\abovecaptionskip}{0.cm}\n        \\setlength{\\belowcaptionskip}{-0.cm}\n\t\\subfigure[\\label{fig:a}]{\n\t\t\\includegraphics[scale=2]{fig/lemma1.png}}\n        \\quad \\quad\n\t\\subfigure[\\label{fig:c}]{\n\t\t\\includegraphics[scale=2]{fig/lemma2.png}}\n\t\\caption{Two special matrix multiplications that can be efficiently computed as proved by Lemma 2 and Lemma 3 (the white color indicates zero elements in matrices)}\n        \\label{fig:matrixmultiply}\n\\end{figure}",
            "fig:c": "\\begin{figure}[htbp]\n\t\\centering\n        \\setlength{\\abovecaptionskip}{0.cm}\n        \\setlength{\\belowcaptionskip}{-0.cm}\n\t\\subfigure[\\label{fig:a}]{\n\t\t\\includegraphics[scale=2]{fig/lemma1.png}}\n        \\quad \\quad\n\t\\subfigure[\\label{fig:c}]{\n\t\t\\includegraphics[scale=2]{fig/lemma2.png}}\n\t\\caption{Two special matrix multiplications that can be efficiently computed as proved by Lemma 2 and Lemma 3 (the white color indicates zero elements in matrices)}\n        \\label{fig:matrixmultiply}\n\\end{figure}",
            "fig:matrixmultiply": "\\begin{figure}[htbp]\n\t\\centering\n        \\setlength{\\abovecaptionskip}{0.cm}\n        \\setlength{\\belowcaptionskip}{-0.cm}\n\t\\subfigure[\\label{fig:a}]{\n\t\t\\includegraphics[scale=2]{fig/lemma1.png}}\n        \\quad \\quad\n\t\\subfigure[\\label{fig:c}]{\n\t\t\\includegraphics[scale=2]{fig/lemma2.png}}\n\t\\caption{Two special matrix multiplications that can be efficiently computed as proved by Lemma 2 and Lemma 3 (the white color indicates zero elements in matrices)}\n        \\label{fig:matrixmultiply}\n\\end{figure}",
            "fig:nc": "\\begin{figure*}[t]\n\\setlength{\\abovecaptionskip}{0.cm}\n% \\setlength{\\abovecaptionskip}{0.cm}\n\\centering \n\\includegraphics[height=4.2cm,width=18cm]{fig/nc.png}\n\\caption{Node classification's predictive performance w.r.t. the ratio of training data}\n\\label{fig:nc}\n\\vspace{-1em}\n\\end{figure*}",
            "fig:gr": "\\begin{figure}[b]\n\\centering \n\\setlength{\\abovecaptionskip}{0.cm}\n\\includegraphics[height=3.1cm,width=8.5cm]{fig/gr.png}\n\\caption{Graph Reconstruction (\\emph{precision@K})}\n\\label{fig:gr}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{eq:ne}\n    \\mathbf{U},\\mathbf{\\Sigma}, \\mathbf{V}\\gets \\texttt{\\textbf{t-SVD}}(\\mathbf{A},d), \\quad\n    \\mathbf{X}\\gets \\mathbf{U}\\sqrt{\\mathbf{\\Sigma}}, \\quad\n    \\mathbf{Y}\\gets \\mathbf{V}\\sqrt{\\mathbf{\\Sigma}}, \\quad\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{eq:raw}\n    \\mathbf{X}_t \\gets  \\mathbf{X}_{t-1} \\cdot \\mathbf{F} + \\Delta \\mathbf{X}_{t}, \\quad\n    \\mathbf{Y}_t \\gets  \\mathbf{Y}_{t-1} \\cdot \\mathbf{G} + \\Delta \\mathbf{Y}_t\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:update}\n\\begin{aligned}\n    \\mathbf{P}_{\\mathbf{X},t} \\gets \\mathbf{P}_{\\mathbf{X}, t-1} \\cdot \\mathbf{F}, \\quad\n    \\mathbf{X}_{b,t} \\gets \\mathbf{X}_{b, t-1} + \\Delta \\mathbf{X}_{t} \\cdot \\mathbf{P}_{\\mathbf{X}, t}^{-1} \\\\\n    \\mathbf{P}_{\\mathbf{Y},t} \\gets \\mathbf{P}_{\\mathbf{Y}, t-1} \\cdot \\mathbf{G}, \\quad\n    \\mathbf{Y}_{b,t} \\gets \\mathbf{Y}_{b, t-1} + \\Delta \\mathbf{Y}_{t} \\cdot \\mathbf{P}_{\\mathbf{Y}, t}^{-1}\n\\end{aligned}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:node_B}\n    \\mathbf{A}^\\prime \\gets [\\mathbf{A}_{t-1}^{}, \\mathbf{B}_1^{}], \\quad\n    \\mathbf{A}_t^{} \\gets [\\mathbf{A}^{\\prime \\top}, \\mathbf{B}_2^{}]^\\top\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\mathbf{A}_t \\gets \\mathbf{A}_{t-1} + \\Delta \\mathbf{A}_t, \\quad \n\\Delta \\mathbf{A}_{t-1} = \\mathbf{B}\\mathbf{C}^\\top\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\label{eq:edge_B}\n \\mathbf{B}=e_u,\\quad \\mathbf{C}=e_v\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\label{eq:ppr}\n    \\texttt{PPR}(\\mathbf{X})=\\sum_{i=0}^\\infty \\alpha(1-\\alpha)^i (\\mathbf{D}^{-1} \\mathbf{A})^i \\mathbf{X}\n\\end{equation}"
        },
        "git_link": "https://github.com/zjunet/DAMF"
    }
}