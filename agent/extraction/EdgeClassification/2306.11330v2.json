{
    "meta_info": {
        "title": "Low Latency Edge Classification GNN for Particle Trajectory Tracking on  FPGAs",
        "abstract": "In-time particle trajectory reconstruction in the Large Hadron Collider is\nchallenging due to the high collision rate and numerous particle hits. Using\nGNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible\ntrajectory classification. However, existing GNN architectures have inefficient\nresource usage and insufficient parallelism for edge classification. This paper\nintroduces a resource-efficient GNN architecture on FPGAs for low latency\nparticle tracking. The modular architecture facilitates design scalability to\nsupport large graphs. Leveraging the geometric properties of hit detectors\nfurther reduces graph complexity and resource usage. Our results on Xilinx\nUltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU\nand GPU respectively.",
        "author": "Shi-Yu Huang, Yun-Chen Yang, Yu-Ru Su, Bo-Cheng Lai, Javier Duarte, Scott Hauck, Shih-Chieh Hsu, Jin-Xuan Hu, Mark S. Neubauer",
        "link": "http://arxiv.org/abs/2306.11330v2",
        "category": [
            "cs.AR",
            "cs.LG",
            "hep-ex"
        ],
        "additionl_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{chapter:intro}\n\n%\\par\n%Particle trajectory reconstruction is an important task for the collision analysis for the Large Hadron Collider (LHC) \\cite{ref27}. In the LHC, proton-proton collisions occur at a frequency of 40 MHz, and produce data at a rate of roughly 40 TB/s \\cite{ref27}. This requires accurate and in-time trajectory reconstruction from complex hits within the detector to decide which collision events to read out  \\cite{ref27}. The existing reconstruction algorithms are typically based on the combinatorial Kalman filter \\cite{ref15,ref16, ref17, ref18}, but their quadratically increasing complexity makes it very difficult to meet the strict latency requirement. The High-Luminosity LHC project \\cite{ref28, ref29}, which aims to increase the instantaneous luminosity by 5x to 7x in 2027, will pose even greater design challenges for trajectory reconstruction.  \n\n%\\par\n%Recent research shows that edge classifying GNNs (Graph Neural Networks) achieve high accuracy in trajectory reconstruction and are more scalable with the increased luminosity compared to previous approaches \\cite{ref7,ref8,ref9,ref10,ref12,ref20}. Scientists at CERN (European Organization for Nuclear Research) plan to replace the existing track reconstruction with GNN-based algorithms to improve the efficiency of particle trajectory reconstruction and meet the substantial computing requirements.\n\n\\par\nParticle trajectory reconstruction in Large Hadron Collider (LHC) is a vital task for collision analysis, which requires accurate and in-time reconstruction to decide which collision events to read out \\cite{ref27}. The existing reconstruction algorithms are based on Kalman filter \\cite{ref15,ref16, ref17, ref18},  which are difficult to meet strict latency requirements due to the quadratically increasing complexity. The High-Luminosity LHC project \\cite{ref28, ref29} aims to boost the instantaneous luminosity by 5x to 7x in 2027, even exacerbating the design challenges for trajectory reconstruction. Recent research shows that edge-classifying GNNs (Graph Neural Networks) achieve high accuracy in trajectory reconstruction and are scalable with the increased luminosity \\cite{ref7,ref8,ref9,ref10,ref12,ref20}, making them a preferred solution for future collision analysis in LHC.\n\n\n%\\par\n%One of the main concerns for GNNs to be implemented in the LHC system is the long processing latency with irregular data accesses. Recently proposed GNN accelerators could shorten the latency, but most of these accelerators \\cite{ref22,ref23,ref35,ref36,ref37,ref38} are designed to focus on node data in a graph, such as Graph Convolutional Networks (GCNs) \\cite{gcn} and GraphSAGE \\cite{graphsage}. These cannot be applied directly to the edge classifying GNNs in trajectory reconstruction, which uses edge embedding and predicts the results on edges. Moreover, preprocessing on graphs is not fit for trajectory reconstruction which has relatively smaller but dynamic graph properties. State-of-the-art accelerators often utilize techniques to optimize their performance by reducing irregularity, such as rearranging processing patterns and order. Some accelerators even preprocess graphs to better fit with their architectures \\cite{ref23,ref38}, or monitor the utilization of processing elements (PEs) to average the workload \\cite{ref36}. These techniques are beneficial for static and large graphs, which have stable graph relationships that can be reused. However, it is not efficient to spend several minutes preprocessing a small dynamic graph for one-time use.\n\n\\par\nOne of the main concerns for GNNs to be implemented in the LHC system is the long processing latency with irregular data accesses. Recently proposed GNN accelerators \\cite{ref22,ref23,ref35,ref36,ref37,ref38} are designed to focus on node data, such as Graph Convolutional Networks (GCNs) \\cite{gcn} and GraphSAGE \\cite{graphsage}. These cannot be applied directly to the edge classifying GNNs in trajectory reconstruction, which uses edge embedding and predicts the results on edges. Moreover, preprocessing on graphs is not fit for trajectory reconstruction which has relatively smaller but dynamic graph properties. State-of-the-art accelerators optimize their performance by reducing irregularity, such as rearranging processing patterns and order to better fit with their architectures \\cite{ref23,ref38}, or monitor the utilization of processing elements (PEs) to balance the workload \\cite{ref36}. These techniques are beneficial for static and large graphs, where stable graph characteristics can be reused. However, it is not efficient to spend long preprocessing time for one-time use on small graphs with dynamic features. \n\n\n%\\par\n%In this paper, we propose an efficient architecture on FPGAs to support edge classifying GNNs, and meet the timing requirement of LHC trajectory reconstruction. The design is integrated with hls4ml \\cite{hls4ml_1}, a compilation flow that can automatically translate machine learning models via high-level-synthesis tools into designs on FPGAs. There are three novel contributions in the proposed architecture: First, we introduce a modular parallel architecture to facilitate the design and scaling of the architecture with the size of the graphs. Second, data allocation and buffer arrays in the proposed architecture considerably reduces memory conflicts during parallel accesses. Third, by properly exploiting the geometry of collision events, we significantly lower the graph irregularity by constraining the node connections, and thus increase processing parallelism. \n% The flexibility of high-level-synthesis design flow also enables the proposed architecture to be applied to other applications and structures of edge classifying GNNs. \n\n\\par\nIn this paper, we propose an efficient architecture on FPGAs to support edge-classifying GNNs, meeting the timing requirement of LHC tasks. There are three novel contributions in the proposed architecture. First, a modular parallel architecture facilitates the design and scaling of the architecture with the size of the graphs. Second, efficient data allocation and buffer arrays considerably reduce memory conflicts during parallel data accesses. Third, exploitation of the geometry of collision events significantly lowers the graph irregularity by constraining the node connections, and thus increases processing parallelism. \n\n\n \n%\\par\n%This work is implemented with the high-level-synthesis tool of Xilinx (Vivado HLS) via the hls4ml framework \\cite{hls4ml_1,hls4ml_2}. The experiments were performed on a real collision graph with 739 nodes and 1252 edges. The results on a Xilinx Virtex UltraScale+ VU9P FPGA have shown that the proposed GNN architecture achieves 1,625x and 1,574x speedup respectively compared with a Intel Xeon W-2125 CPU and an NVIDIA RTX2080 GPU. \n\n\\par\nThis work is implemented with the high-level-synthesis framework, hls4ml \\cite{hls4ml_1,hls4ml_2}, which enables an automatic translation of machine learning models to FPGA designs. The experiments were performed on real collision graphs. The results on a Xilinx Virtex UltraScale+ VU9P FPGA show that the proposed GNN architecture achieves 1,625x and 1,574x speedup respectively compared with a Intel Xeon W-2125 CPU and an NVIDIA RTX2080 GPU. Section II of this paper discusses the background of particle tracking. Section III introduces the proposed GNN architecture. Section IV evaluates the performance and Section V concludes this work.\n\n\n\n% \\par\n% The rest of this paper is organized as follows: Section II discusses the background of particle tracking and related works of GNN processing. Section III introduces our proposed GNN architecture and the optimization techniques. Section IV evaluates the implementation and performance results. Finally, in Section V we conclude this work.\n"
            },
            "section 2": {
                "name": "Background",
                "content": "\n",
                "subsection 2.1": {
                    "name": "Large Hadron Collider System",
                    "content": "\nThe Large Hadron Collider (LHC) is the largest and most powerful particle accelerator in the world \\cite{ref28}. For high-energy particle physics collider experiments in the LHC, proton-proton collisions occur at a frequency of 40MHz and produce data at a rate of roughly 40 TB/s \\cite{ref27}. After the collision, the trackers record the locations of particle detections (``hits\") and transfer this information to the trigger systems. The trigger system will perform trajectory reconstruction to recognize which hits belong to the same particle, as shown in Fig.~\\ref{fig:LHC_Collision_Detector}. The collision events are processed by 18 FPGAs in a multiplexed manner, where each FPGA needs to handle 2.22 million graphs per second (MGPS) \\cite{cms2020}.\n\n\\par\nTrackers are composed of cylindrical detecting layers \\cite{ref28,ref20}. These layers are immersed in an axis-aligned magnetic field, and their geometry is naturally described by cylindrical coordinates. We focus on the innermost layers, a highly granular set of 4 barrel and 14 endcap layers \\cite{ref39}.  \n\n\n\n%\\begin{figure}[h] \n%\t\\centering \n%        \\includegraphics[width=0.45\\textwidth]{figures/graph_construction.pdf} \t\n%        %\\includegraphics[width=0.48\\textwidth]{figures/graph_construction_1.png} \n%\t\\caption{The flow of GNN-based algorithms} \n%\t\\label{fig:graph_construction} \n%\\end{figure}\n%\\par\n%Fig.~\\ref{fig:graph_construction} illustrates the whole reconstruction flow. The algorithm takes collision hits as input. First, it constructs input graphs by taking hits as nodes and potential connecting hit pairs on the same trajectories as edges. Second, it uses edge classifying GNNs to estimate the truth of each edge in the input graphs. After being classified by GNNs, the track-building algorithm uses the edge weights to cluster and  reconstruct particle trajectories.\n\n"
                },
                "subsection 2.2": {
                    "name": "GNN-based Algorithms for Track Reconstruction",
                    "content": " \\label{{GNN-based Algorithms for Track Reconstruction}}\nThe edge classifying GNN algorithm is based on interaction networks (IN) \\cite{ref21}. IN is a physics-motivated GNN capable of analyzing objects and their relations. Hit information is embedded in the node feature, and trajectory segment information is embedded in the edge feature. The index set stores the sender and receiver node indexes of each edge. There are three types of functions in IN: Edgeblock, Aggregate, and Nodeblock. Functions in Edgeblock and Nodeblock are multi-layer perceptrons (MLPs) that re-embed edge and node features according to their input.  Aggregate accumulates edge features to their receiver nodes.\n\n%\\begin{figure}[h] \n%\t\\centering \n%\t\\includegraphics[width=0.45\\textwidth]{figures/input_format.png} \n%\t\\caption{Input Format of Particle-tracking GNNs} \n%\t\\label{fig:input}\n%\\end{figure}\n\n%The flow of IN algorithm is shown in Fig.~\\ref{fig:GNN}. \n%There are three types of functions in IN: Edgeblock, Aggregate, and Nodeblock. First, the Edgeblock re-embeds edges and corresponding connected nodes to get updated edges, as in Eq.(\\ref{eq:eq1}). Second, the Aggregate accumulates edge features to their receiver nodes, as in Eq.(\\ref{eq:eq2}).  Next, the Nodeblock re-embeds node features with the aggregated result, as in Eq.(\\ref{eq:eq3}). Finally, the algorithm will go through the second Edgeblock to re-embed updated features again, as in Eq.(\\ref{eq:eq4}). This represents the truth of each edge segment.  \n%Table~\\ref{table:gnn notation} lists the notation of the IN network. \n%In particular, functions $\\phi_{R,1}$, $\\phi_{R,2}$, and $\\phi_{O,1}$ are multi-layer perceptrons (MLPs) that need to be trained.\n\n\n\n\n%\\par\n%There are three types of functions in IN: Edgeblock, Aggregate, and Nodeblock. First, the Edgeblock re-embeds edges and corresponding connected nodes to get updated edges. Second, the Aggregate accumulates edge features to their receiver nodes.  Next, the Nodeblock re-embeds node features with the aggregated result. Finally, the algorithm will go through the second Edgeblock to re-embed updated features again. This represents the truth of each edge segment. In particular, functions in Edgeblock and Nodeblock are multi-layer perceptrons (MLPs) that need to be trained.\n\n%\\begin{figure}[h] \n%\t\\centering \n%\t\\includegraphics[width=.5\\textwidth]{figures/IN_flow.pdf} \n%\t\\caption{Computation flow of the Interaction Network (IN)} \n%\t\\label{fig:GNN}\n%\\end{figure}\n\n\n%\\begin{table}[h] \n%\t\\centering \n%        \\renewcommand\\arraystretch{1.3}\n%\t\\caption{Notation of Interaction Network (IN)} \n%\t\\resizebox{.5\\textwidth}{!}{\n%\t\t\\begin{tabular}{ c c  c c }\n\t\t\t\n%\t\t\t\\hline\n%\t\t\tNotation & Meaning & Notation & Meaning \\\\\n%\t\t\t\\hline\n%\t\t\tV & set of nodes  & C($v$) & connected edges set of node $v$\\\\\n%\t\t\t%\\hline\n%\t\t\tE & set of edges  & $a_v$ & aggregated features of node $v$ \\\\\n%\t\t\t%\\hline\n%\t\t\t$e_{i,j}$ & edge between node $i$, $j$ & $e^{'}_{i,j}$ & updated edge \\\\\n%\t\t\t%\\hline\n%\t\t\t$x_v$ & node $v$ features  & $e^{''}_{i,j}$ & output edge  \\\\\n%\t\t\t%\\hline\n%\t\t\t$x_i$,$x_j$ & in, out node features of $e_{i,j}$ & $x^{'}_v$ & updated node features \\\\\n%\t\t\t\\hline\n%\t\t\\end{tabular}\n%\t}\n%   \\label{table:gnn notation}\n%\\end{table}\n\n\n\n%\\begin{equation}\n%    \\label{eq:eq1}\n%\t\\forall e_{i,j} \\in \\mathbf{E}:\n%\t\\qquad e^{'}_{i,j}=\\phi_{R,1}(x_i,x_j,e_{i,j} )\n%\\end{equation}\n%\\begin{equation}\n%    \\label{eq:eq2}\n%\t\\forall v \\in \\mathbf{V}:\n%\t\\qquad a_v=\\sum_{e^{'}_{i,v} \\in C(v)} e^{'}_{i,v}\n%\\end{equation}\n%\\begin{equation}\n%    \\label{eq:eq3}\n%\t\\forall v \\in \\mathbf{V}:\n%\t\\qquad x^{'}_v=\\phi_O(x_v,a_v )\n%\\end{equation}\n%\\begin{equation}\n%    \\label{eq:eq4}\n%\t\\forall e^{'}_{i,j} \\in \\mathbf{E}:\n%\t\\qquad e^{''}_{i,j}=\\phi_{R,2}(x^{'}_i,x^{'}_j,e^{'}_{i,j} )\n%\\end{equation}\n\n\n\n% need to add more discussion and comparison for GNN accelerators on FPGA\n"
                },
                "subsection 2.3": {
                    "name": "Designs of GNN Accelerators on FPGAs",
                    "content": "\n\\par\n Several studies have implemented GNNs on FPGAs for particle physics \\cite{ref7,ref39,ref11}. \\cite{ref7} focuses on jet tagging, which targets fully connected graphs and aims to predict features of the entire graph instead of each individual edge. While \\cite{ref7} addresses the issue of irregular access with fully connected graph properties, it is not suitable for the LHC application. The graphs generated from the LHC consist of hundreds of nodes per graph. Utilizing the methods from \\cite{ref7} may generate excessive unnecessary connections which may be tens to hundreds of times greater than the original graph, leading to a significant impact on processing time.\n \n %\\par\n %There are other works aiming to implement the interaction network on FPGAs \\cite{ref39,ref11}. In \\cite{ref39}, the authors proposed two designs for different requirements and implemented them on FPGAs via hls4ml. However, the throughput-optimized version can not fit the required graph size of LHC, while the resource-optimized version cannot provide a high enough throughput to meet the requirement of LHC. \n\n\n\n\n\n\n%\\begin{figure}[h] \n%\t\\centering \n%\t\\includegraphics[width=.45\\textwidth]{figures/hls4ml.png} \n%\t\\caption{The Flow of HLS4ML}\n% \t\\label{fig:HLS4ML}\n%\\end{figure}\n"
                }
            },
            "section 3": {
                "name": "A Low Latency GNN Architecture for Trajectory Reconstruction",
                "content": "\\label{subsection:PF}\n\n \n\n\n\n\n%\\begin{figure*}[hb]\n %   \\centering \n %   \\includegraphics[width=\\textwidth]{figures/top_design.pdf}\n %   \\caption{Overview architecture of the system processing block. PP represents the Ping-Pong buffers between functional units.} \n %   \\label{fig:overview_arc} \n%\\end{figure*}\n\n%To support the graph generated from LHC trajectory reconstruction and meet the latency requirement, we propose a low latency edge classification GNN accelerator. The implementation is synthesized using hls4ml framework and deployed on Xilinx UltraScale+ FPGA. \n\n",
                "subsection 3.1": {
                    "name": "Overview Architecture",
                    "content": "\nBased on the GNN computation flow of IN in Section \\ref{{GNN-based Algorithms for Track Reconstruction}}, the computation can be split into pipeline stages at the function level. We use the Vivado HLS dataflow architecture to implement the pipeline.  Fig.~\\ref{fig:overview_arc} shows the proposed pipeline. We design a modularized parallel architecture for each function, including Edgeblock, Aggregate and Nodeblock. Each function is composed of several processing elements (PEs) as basic compute units. Between these functions, we insert FIFO buffers with different depths to ensure that data would not be stuck in the dataflow paths. With this architecture, users can configure the pipeline and scale the system throughput with the available resources on FPGAs.\n\n% delete the following\n\n% Because of the pipeline structure, the system can achieve high throughput by using multiple PEs where each PE can process one element per cycle. The same dataflow pipeline can be easily replicated to attain higher throughput. With this architecture, users can configure the pipeline and scale the system throughput with the available resources on FPGAs. \n\n% In addition to the modular parallel architecture, our design takes advantage of the geometric constraint property to partition input graphs. This reduces the complexity of data patterns and resource consumption, resulting in a more efficient implementation. Section \\ref{Geometry-constrained} will describe the specific properties of particle trajectories and how we efficiently implement them using this approach.\n\n"
                },
                "subsection 3.2": {
                    "name": "Modular Parallel Architecture",
                    "content": "\n\\label{Modular Parallel Architecture}\nThe modular parallel architecture enables parallel processing of each function. The following introduces the design and optimizations of these functions.\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Edgeblock",
                        "content": "\nThe Edgeblock computation involves accessing edge features, edge indexes, and connected node features. The access of node features changes dynamically according to the edge indexes. \n% The memory conflict could happen when multiple features stored in the same memory module are requested simultaneously. \nTo address this irregularity, we added node arrays, which contain the features of all the nodes in the graph, into each PE to support concurrent accesses to node features. As shown in Fig.~\\ref{fig:new_edgeblock}, during the computation, the edge features ($e_{i,j}$) and edge indexes (\\textit{i}, \\textit{j}) of each edge will be sent to PEs. In each PE, node features ($X_i$ and $X_j$) are accessed based on the edge indexes. After the above steps, multiplier engines in PEs will process the MLP computation and output the updated edge features. \n% An Edgeblock architecture with four PEs is shown in the upper part in Fig.~\\ref{fig:new_edgeblock}. By referring the sender and receiver indexes of edges, this architecture supports reading out two nodes per cycle.\n\n%\\begin{figure}[h]\n%    \\centering \n%    \\includegraphics[width=.9\\linewidth]{figures/edgeblock.pdf}\n%    \\caption{An architecture of Edgeblock with four PEs} \n%    \\label{fig:new_edgeblock} \n%\\end{figure}\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Aggregate",
                        "content": "\n\\par\nThe purpose of Aggregate is to send the updated edge features to their receiver nodes. During this process, the aggregate function first reads updated edge features and edge indexes. Based on the edge indexes, the aggregated edge features are accessed and added to the updated edge features, and then stored back to internal registers. The architecture of Aggregate PE is shown in Fig.~\\ref{fig:new_aggregate}. \n% Similar to the computation in Edgeblock, the accesses to aggregated edge features of the receiver node are irregular. To deal with irregularity, we place aggregated edge feature arrays in each PE. \nAfter aggregating all updated edge features, the parallel adder tree accumulates the values of the same node indexes. With this architecture, multiple Aggregate PEs can process multiple edges simultaneously.\n\n% However, when two successive edges of one PE have the same receiver node, it requires two cycles in order to prevent read-after-write (RAW) conflicts. To resolve this issue, we add registers to store the current node index and the aggregated result. If the receiver node index is the same as the previous one, the PE will read the aggregated result in the registers. This design will effectively eliminate the potential RAW conflicts and attain the throughput of processing one edge per cycle.\n\n%\\begin{figure}[h]\n%    \\centering \n%    \\includegraphics[width=.9\\linewidth]{figures/agg.pdf}\n%    \\caption{The architecture of Aggregate with four PEs} \n%    \\label{fig:new_aggregate} \n%\\end{figure}\n\n"
                    },
                    "subsubsection 3.2.3": {
                        "name": "Nodeblock",
                        "content": "\nThe Nodeblock is used for re-embedding node features by the original and aggregated node features. The data access of the Nodeblock computation is more regular when compared with Edgeblock and Aggregate. For each node, Nodeblock collects node features and aggregated node features, and then uses MLPs to obtain updated node features.\n\n"
                    }
                },
                "subsection 3.3": {
                    "name": "Exploiting Geometric Property of LHC Trackers",
                    "content": " \\label{Geometry-constrained}\nWhile the architecture in the previous section successfully enables significant parallelism between processing elements (PEs), the individual memory in PEs costs considerable amount of BlockRAMs (BRAM) in an FPGA. Therefore, we propose a method that can reduce memory utilization and enable a more parallel architecture by taking advantage of the data properties of LHC detectors. \n\n\\par\nIn Section II-A, we introduced the architecture of the LHC particle trackers and how hit data is applied to the input graphs of GNNs. In the original graph constructed from LHC trackers, an edge from a node could connect to any other node in a graph. This assumption could cause excessive number of edges in the graph. Since the LHC trackers are composed of cylindrical layers surrounding the colliding beams, the particles have to pass through the inner tracker layer first and then move out, as shown in Fig.~\\ref{fig:type}. These trajectories exhibit similar connection behaviors. For example, hits on the B1 layer only connect to the B2 layer or E1 layer. The relationship between hits and legal edges can be applied to a geometry-constrained graph. We reorganize the input graph structure by grouping hits based on their layer locations. We partition the graphs into 13 parallel sub-graphs, each containing only two node groups, as shown in Fig.~\\ref{fig:5_15} and \\ref{fig:connects}.\n\n \n\n\n\n\n\\par\nWith the geometry-constrained approach, an edge can only exist between specific node groups. This reduces the number of candidate nodes in node arrays used in Edgeblock PEs and Aggregate PEs, resulting in significant reduction of memory usage. Furthermore, since these 13 sub-graphs are independent of each other, they can be assigned to different PEs and computed in parallel. By relieving memory usage pressure and improving parallelism, the performance is greatly enhanced compared to the original design.\n%By relieving memory usage pressure and improving parallelism, the performance of the proposed architecture is greatly enhanced compared to the original design.\\section{Evaluation}\n\n"
                },
                "subsection 3.4": {
                    "name": "Experiment Setup",
                    "content": "\nWe implemented our design with Vivado HLS 2019.2 and loaded it onto a Xilinx Virtex UltraScale+ VU9P FPGA. The clock frequency of the design runs at 200 MHz. The resource utilization is from the v-synthesis report. The performance is based on  the simulation result of the generated HDL code. We use the metric of Million Graphs Per Second (MGPS) to measure the system throughput. \n\n%However, for comparison with the preliminary study \\cite{ref39}, we use the c-synthesis result, as was done in that paper. \n\n\n\\par\nWe evaluate our architecture with the TrackML dataset \\cite{ref40} generated by CERN. \n%enough to store on-chip; therefore we do not focus on alleviating DRAM access latency. \nAll the data points are based on a fixed point format of 7 integer bits and 7 fractional bits. This is the same format used in \\cite{ref39} to ensure acceptable accuracy. A system PE in the experiment contains an Edgeblock PE, an Aggregate PE, and a Nodeblock PE. The graph size of the dataset will be elaborated in Section \\ref{Graph Size Analysis}. The performance of our proposed architecture will be evaluated in Sections \\ref{Scalability Analysis of Modularized Parallel Architecture} to \\ref{Advanced Analysis of Geometry-constrained Graphs}. In Section \\ref{comparison}, we compare the performance of our architecture with the CPU, GPU, and prior FPGA designs. \n\n\n\n "
                },
                "subsection 3.5": {
                    "name": "Supporting In-time Graph Processing of Collision Events",
                    "content": " \\label{Graph Size Analysis}\nThe ultimate goal of this work is to perform in-time trajectory reconstruction based on the graphs generated from LHC collision events. Input graphs are prepared based on the same flow as the prior work \\cite{ref39}. Each graph is divided into two sectors based on the position $z$ of hits. We use the graph size that can cover 95 percentile of collision events as the nominal size, which contains 739 nodes with 1252 edges. According to \\cite{cms2020}, these graphs  should be computed at the throughput higher than 2.22 MGPS. \n\n\\par\nTable~\\ref{table:5_5_re_pe} compares the three proposed architectures. The architecture MPA represents the Modular Parallel Architecture introduced in Section \\ref{Modular Parallel Architecture}. MPA\\textsubscript{geo} and MPA\\textsubscript{geo\\_rsrc} are the extended designs of MPA with the proposed techniques of geometry-constrained optimization and data-aware resource allocation respectively. The designs of MPA\\textsubscript{geo} and MPA\\textsubscript{geo\\_rsrc} will be elaborated in Section \\ref{Influence of Geometry-constrained Implementation} and \\ref{Advanced Analysis of Geometry-constrained Graphs}. Latency measures the time from input graph to the output result. The design can take a new input in every Interval time and attain throughput in MGPS. As shown in Table~\\ref{table:5_5_re_pe}, the proposed MPA\\textsubscript{geo\\_rsrc} meets the LHC requirement by supporting graphs of 739 nodes with 1252 edges at throughput of 3.225 MGPS.  \n\n\n\n%\\par\n%Note that the previous design \\cite{ref39} on the same FPGA VU9P that meets the LHC throughput requirement can only accommodate graphs of 28 nodes with 56 edges. To process the graph of 739 nodes with 1252 edges, the previous design needs to further partition the target graph on the $\\phi$ axis to generate sub-graphs that can be fitted into the FPGA. Partitioning a graph could cause undesirable cuts on some true edges connecting across sub-graphs, and would increase the overhead of graph manipulation and data management. \n\n"
                },
                "subsection 3.6": {
                    "name": "Scalability of MPA (Modular Parallel Architecture)",
                    "content": " \\label{Scalability Analysis of Modularized Parallel Architecture}\nIn Section III-B, we introduced the MPA architecture. The processing throughput of the architecture can scale by deploying more PEs. To evaluate the scalability of MPA, Fig.~\\ref{fig:scale_fuse} illustrates the performance and resource utilization of MPA from one PE to eight PEs. The results show the latency and interval can be reduced by deploying more PEs. However, when scaling up the number of PEs, BRAMs will become the limiting factor of FPGA resources.  \n\n%\\begin{figure}[htb]\n%     \\centering\n%  \\includesvg[inkscapelatex = false,width=.4\\textwidth]{figures/scale_fuse_4.svg}  \n%   \\caption{Scalability of the modular architecture}\n%   \\label{fig:scale_fuse}\n%\\end{figure}\n\n\n\n"
                },
                "subsection 3.7": {
                    "name": "MPA with Geometry-constrained Optimization",
                    "content": "\n\\label{Influence of Geometry-constrained Implementation}\nBy taking advantage of the geometry-constrained property described in Section III-C, MPA\\textsubscript{geo} not only relieves the constraints of the range of node in each PE, but also makes node groups independent to others. MPA\\textsubscript{geo} alleviates the resource demand of BRAM and allows the deployment of more PEs for greater processing parallelism. There are 11 node groups and 13 edge groups. We allocate one PE for each of these groups and result in a total of 11 Nodeblock PEs, 13 Edgeblock PEs, and 13 Aggregate PEs. As shown in Table~\\ref{table:5_5_re_pe}, MPA\\textsubscript{geo} achieves 13\\% improvement in throughput compared to the original MPA architecture.\n\n"
                },
                "subsection 3.8": {
                    "name": "Data-aware Resource Allocation",
                    "content": "\n\\label{Advanced Analysis of Geometry-constrained Graphs}\nWe further analyze the distribution of graph sizes in the dataset and propose the design MPA\\textsubscript{geo\\_rsrc} which applies data-aware resource allocation. \nAfter applying the geometry-constrained property, the number of nodes are not evenly distributed across different layers. The barrel layers (B1 to B4) contain more nodes and connections than endcap layers (E1 to E7). To address this issue, we propose the design MPA\\textsubscript{geo\\_rsrc} which classifies the node groups into two types. As shown in Fig.~\\ref{fig:5_15}, the layers B1 to B4 contain relatively more nodes and belong to type A, while layers E1 to E7 with fewer nodes are assigned to type B. We will assign two PEs to process each node group of type A, and one PE to handle each node group in type B. For the edge groups, we apply the same allocation principle as for node groups.\n\n% Allocating the same amount of resource for all the nodes could easily result in an over-design when B1 contains 138 nodes while there are only 34 nodes in E7. \n\n% \\begin{table}[htb]\n%     \\centering\n%     \\renewcommand\\arraystretch{1.3}\n%     \\caption{Number of nodes in different layers of the geometry-constrained graph}\n%     \\label{table:dis_node}    \n%     \\resizebox{.49\\textwidth}{!}{\n%         \\begin{tabular}{ c|c c c c|c c c c c c c }\n%             \\hline\n%             Layer & B1  & B2  & B3  & B4 & E1 & E2 & E3 & E4 & E5 & E6 & E7 \\\\\n%             \\hline\n%             \\# node & 138 & 127 & 108 & 96 & 62 & 54 & 54 & 49 & 42 & 40 & 34 \\\\\n%             \\hline\n%         \\end{tabular}}\n% \\end{table}\n\n%\\par\n%To address this issue, we propose the design MPA\\textsubscript{geo\\_rsrc} which classifies the node groups into two types. As shown in Fig.~\\ref{fig:5_15}, the layers B1 to B4 contain relatively more nodes and belong to type A, while layers E1 to E7 with fewer nodes are assigned to type B. We will assign two PEs to process each node group of type A, and one PE to handle each node group in type B. For the edge groups, we apply the same allocation principle as for node groups.\n\n\n\n\\par\n%During implementation, we encountered issues with the DSP utilization exceeding the resource limit of the FPGA board. To overcome this, we migrated some computation from DSP to LUT by forcing multiplications that are less than 8 bits into LUTs. Table~\\ref{table:5_5_re_pe} presents the results of the three different architectures. The optimized geometry-constrained implementation achieved a 1.37x increase in throughput and a 0.76x decrease in latency compared to the origin geometry-constrained design. \n\n"
                },
                "subsection 3.9": {
                    "name": "Comparison with Previous Designs",
                    "content": "\n\\label{comparison}\n",
                    "subsubsection 3.9.1": {
                        "name": "Comparison with Previous GNN Trajectory Reconstruction on FPGA",
                        "content": "\n There are two architectures in the previous work \\cite{ref39} of GNN for trajectory reconstruction on FPGA. The throughput-optimized design (ThrpOpt) focuses on attaining high throughput, but would reduce the graph size it can handle. The resource-optimized design (RsrcOpt) aims to accommodate large graphs, but would suffer from low throughput. Table~\\ref{table:baseline compared} compares the performance between these two architectures and our proposed architecture. The platform of all the three architectures is XCVU9P, and the frequency is 200 MHz. ThrpOpt design can achieve a higher throughput of 200 MGPS, but can only handle small graphs of 28 nodes with 56 edges. RsrcOpt architecture can accommodate large graphs of 448 nodes with 896 edges, but with lower throughput than the ThrpOpt design. Our proposed MPA\\textsubscript{geo\\_resrc} can handle the largest graph (739 nodes with 1252 edges) among all the designs, and attains higher throughput than the RsrcOpt design. \n\n\n\n"
                    },
                    "subsubsection 3.9.2": {
                        "name": "Comparison with CPU and GPU",
                        "content": "\n\n% We execute the same particle-tracking GNN algorithm on an Intel(R)Xeon(R) W-2125 CPU and an NVIDIA GeForce RTX 2080 Ti (CUDA 10.2) based on PyTorch (1.11.0) and the PyTorch Geometric 2.0.4 framework. To ensure a fair and accurate comparison, \n\nWe execute the same particle-tracking GNN algorithm on an Intel(R)Xeon(R) W-2125 CPU and an NVIDIA GeForce RTX 2080 Ti (CUDA 10.2) based on PyTorch (1.11.0) and the PyTorch Geometric 2.0.4 framework.\nWe ran 1000 graphs on each platform. Each graph contains 739 nodes and 1252 edges. Table~\\ref{cpugpu} shows the details of experiment and normalized throughput. Our proposed design on FPGA achieved significantly higher throughput of 1,625x and 1,574x when compared with CPU and GPU respectively.\n\n%Table~\\ref{cpugpu} shows the details of experiment and normalized throughput.\n%We ran 1000 graphs on each platform, each graph contains 739 nodes and 1252edges. Our proposed design on FPGA achieved significantly higher throughput of 1,625x and 1,574x when compared with CPU and GPU respectively.\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Conclusion",
                "content": "\nWe propose a novel architecture for particle-tracking GNNs on FPGAs.  By utilizing LHC detector geometry, our design reduces graph complexity and FPGA resource requirements. The modular architecture of processing units and buffers also efficiently handle the irregular data access patterns and facilitate design scalability to support large graphs while attaining high parallelism and computation throughput. Experiment results show that our design achieves 1,625x speedup compared to the CPU, and 1,574x speedup compared to the GPU.\n\n%\\newpage\n\n\n"
            },
            "section 5": {
                "name": "Acknowledgments",
                "content": "\nHuang, Yang, Su, Lai and Hu are supported by National Science and Technology Council grant 111-2221-E-A49-092-MY3. Duarte, Hauck, Hsu and Neubauer are supported by National Science Foundation (NSF) grants No. 2117997.\n\\par\n\n\\balance\n\\bibliographystyle{IEEEtran}\n\\bibliography{ref}\n\n"
            }
        },
        "tables": {
            "table:5_5_re_pe": "\\begin{table}[htb]\n    \\centering \n    \\renewcommand\\arraystretch{1.3}\n\t\\caption{Performance of the proposed architectures} \n        \\label{table:5_5_re_pe}\n        \\resizebox{.48\\textwidth}{!}{\n        \\begin{tabular}{ c | c c c c c}\n        \\hline\n           Architectures & Latency($\\mu$s) & Interval($\\mu$s) & Throughput(MGPS) \\\\ \n        \\hline\n            MPA & 3.165 & 0.48 & 2.083 \\\\ \n        \\hline\n            MPA$_\\text{geo}$ & 2.69 & 0.425 & 2.352 \\\\ \n        \\hline\n            MPA$_\\text{geo\\_rsrc}$ & 2.07 & 0.31 & 3.225 \\\\ \n        \\hline\n        \\end{tabular}\n}\n\\end{table}",
            "table:dis": "\\begin{table}[htb]\n    \\centering \n    \\renewcommand\\arraystretch{1.3}\n\t\\caption{Allocate PEs based on different sizes of data} \n        \\label{table:dis}\n            \\begin{tabular}{c|cc|ccc}\n            \\hline\n            & \\multicolumn{2}{c|}{Node} & \\multicolumn{3}{c}{Edge} \\\\ \\hline\n            & A            & B          & A-A    & A-B    & B-B    \\\\ \\hline\n            \\#data& 138          & 62         & 277    & 77     & 87     \\\\ \\hline\n            \\#PE  & 2            & 1          & 4      & 1      & 1      \\\\ \\hline\n        \\end{tabular}\t\n        %\\begin{tabular}{c|c|c|c}\n        %\\hline\n        %              & group & \\#data & \\#PE \\\\ \\hline\n        %\\multirow{2}{*}{Node} & A     & 138    & 2    \\\\\n        %              & B     & 62     & 1    \\\\ \\hline\n        %\\multirow{3}{*}{Edge} & A-A   & 277    & 4    \\\\\n        %              & A-B   & 77     & 1    \\\\\n        %              & B-B   & 87     & 1    \\\\ \\hline\n        %\\end{tabular}\n\\end{table}",
            "table:baseline compared": "\\begin{table}[h]\n\\centering\n\\renewcommand\\arraystretch{1.3}\n\\caption{Comparison with previous FPGA designs}\n\\resizebox{.48\\textwidth}{!}{\n\\begin{tabular}[htb]{ c|c c c }\n    \\hline\n     & ThrpOpt \\cite{ref39} & RsrcOpt \\cite{ref39} & MPA\\textsubscript{geo\\_rsrc} (proposed) \\\\\n     \\hline\n    %Platform & \\multicolumn{3}{c}{XCVU9P }\\\\\n    % Platform & XCVU9P & XCVU9P & XCVU9P\\\\\n    % \\hline\n    %Frequency & \\multicolumn{3}{c}{200 MHz} \\\\\n    % Frequency & 200 MHz & 200 MHz & 200 MHz \\\\\n    % \\hline\n    Graph Size & 28 nodes/56 edges & 448 nodes/896 edges& 739 nodes/1252 edges \\\\\n    % \\hline\n    %  Latency & 0.29 $\\mu$s & 2.35 $\\mu$s & 1.15 $\\mu$s \\\\\n    \\hline \n    Throughput & 200 MGPS & 1.14 MGPS & 3.17 MGPS\\\\\n    \\hline\n\\end{tabular}}\n\\label{table:baseline compared}\n\\end{table}",
            "cpugpu": "\\begin{table}[htb]\n\\centering\n\\renewcommand\\arraystretch{1.3}\n\\caption{Comparison with CPU and GPU}\n\\resizebox{.48\\textwidth}{!}{\n\\begin{tabular}[htb]{ c| c c c }\n    \\hline\n     & CPU & GPU & FPGA \\\\\n     \\hline\n    Platform & Intel(R) Xeon(R) & NVIDIA GeForce & XCVU9P \\\\\n    & W-2125  & RTX 2080 Ti & \\\\\n    \\hline\n    Compute Unit & 4.00 GHz@8 cores & 1.63 GHz@4352 cores & 200 MHz \\\\\n    \\hline\n    Technology & 14 nm & 12 nm & 14 nm \\\\\n    \\hline\n    Normalized Thrp. & 1 & 1.03 & 1625 \\\\\n    \\hline\n\\end{tabular}}\n\\label{cpugpu}\n\\end{table}"
        },
        "figures": {
            "fig:LHC_Collision_Detector": "\\begin{figure}[htb]  \n\t\\centering \n\t\\includegraphics[width=0.5\\textwidth]{figures/reconstruct.pdf} \n\t%\\caption{Illustration of trajectory reconstruction from particle hits} \n        \\caption{Illustration of trajectory reconstruction} \n\t\\label{fig:LHC_Collision_Detector} \n\\end{figure}",
            "fig:test_sep": "\\begin{figure*}[ht]\n      \\begin{subfigure}{0.53\\textwidth}\n      \\centering\n      \\includegraphics[width=\\textwidth]{figures/top_design_v4.pdf}\n      \\caption{}\n      \\label{fig:overview_arc}\n    \\end{subfigure}\n    \\begin{subfigure}{.23\\textwidth}\n      \\centering\n      \\includegraphics[width=\\linewidth]{figures/edgeblock_v2.pdf}  \n      \\caption{}\n      \\label{fig:new_edgeblock}\n     \\end{subfigure}\n    \\begin{subfigure}{.23\\textwidth}\n      \\centering\n      \\includegraphics[width=\\linewidth]{figures/agg_v2.pdf}\n      \\caption{}\n      \\label{fig:new_aggregate}\n    \\end{subfigure}\n    \\caption{(a) Overview architecture of the system processing block. (b) An architecture of Edgeblock with two PEs. (c) The architecture of Aggregate with two FEs}\n    \\label{fig:test_sep}\n\\end{figure*}",
            "fig:test_sep2": "\\begin{figure}[ht]\n      \\begin{subfigure}{.19\\textwidth}\n      \\centering\n      \\includegraphics[width=\\linewidth]{figures/full_a.pdf} \n      \\caption{}\n      \\label{fig:type}\n    \\end{subfigure}\n    \\begin{subfigure}{.17\\textwidth}\n      \\centering\n      \\includegraphics[width=\\linewidth]{figures/full_b.pdf}  \n      \\caption{}\n      \\label{fig:5_15}\n     \\end{subfigure}\n    \\begin{subfigure}{.1\\textwidth}\n      \\centering\n      \\includegraphics[width=\\linewidth]{figures/full_c.pdf}\n      \\caption{}\n      \\label{fig:connects}\n    \\end{subfigure}\n    \\caption{(a) Possible trajectories of particles, (b) Partition layers into two types of node group (c) Partition a graph into sub-graphs}\n    \\label{fig:test_sep2}\n\\end{figure}",
            "fig:scale_fuse": "\\begin{figure}[h]\n    \\centering \n    \\includegraphics[width=.4\\textwidth]{figures/scale_fuse_4.pdf}\n    \\caption{Scalability of the modular architecture} \n    \\label{fig:scale_fuse} \n\\end{figure}"
        }
    }
}