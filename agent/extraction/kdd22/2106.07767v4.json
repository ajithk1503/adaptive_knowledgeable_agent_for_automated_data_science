{
    "meta_info": {
        "title": "How does Heterophily Impact the Robustness of Graph Neural Networks?  Theoretical Connections and Practical Implications",
        "abstract": "We bridge two research directions on graph neural networks (GNNs), by\nformalizing the relation between heterophily of node labels (i.e., connected\nnodes tend to have dissimilar labels) and the robustness of GNNs to adversarial\nattacks. Our theoretical and empirical analyses show that for homophilous graph\ndata, impactful structural attacks always lead to reduced homophily, while for\nheterophilous graph data the change in the homophily level depends on the node\ndegrees. These insights have practical implications for defending against\nattacks on real-world graphs: we deduce that separate aggregators for ego- and\nneighbor-embeddings, a design principle which has been identified to\nsignificantly improve prediction for heterophilous graph data, can also offer\nincreased robustness to GNNs. Our comprehensive experiments show that GNNs\nmerely adopting this design achieve improved empirical and certifiable\nrobustness compared to the best-performing unvaccinated model. Additionally,\ncombining this design with explicit defense mechanisms against adversarial\nattacks leads to an improved robustness with up to 18.33% performance increase\nunder attacks compared to the best-performing vaccinated model.",
        "author": "Jiong Zhu, Junchen Jin, Donald Loveland, Michael T. Schaub, Danai Koutra",
        "link": "http://arxiv.org/abs/2106.07767v4",
        "category": [
            "cs.LG",
            "stat.ML"
        ],
        "additionl_info": "KDD 2022 camera ready version + full appendix; 20 pages, 2 figures"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nGraph neural networks (GNNs) aim to translate the enormous empirical success of deep learning to data defined on non-Euclidean domains, such as manifolds or graphs~\\citep{bronstein2017geometric}, \nand have become important tools to solve a variety of learning problems for graph structured and geometrically embedded data.\nHowever, recent works\nshow that GNNs---much like their ``standard'' deep learning counterparts---have a high sensitivity to adversarial attacks: intentionally introduced minor changes in the graph structure can lead to significant changes in performance.\nThis finding, first articulated by \\citet{zugner2018adversarial} and \\citet{dai2018adversarial}, has triggered studies that investigated different attack scenarios~\\citep{xu2019topology,wu2019adversarial,li2020adversarial,ma2020towards}.\n\nA different aspect of GNNs that has been scrutinized recently is that most GNNs do not perform well with many heterophilous datasets. %\nGNNs generally perform well under homophily (or assortativity), i.e., the tendency of nodes with similar features or class labels to connect~\\citep{Pei2020Geom-GCN,zhu2020beyond}. \nSuch datasets are thus called \\emph{homophilous} (or \\emph{assortative}). \nWhile homophilous datasets dominate the study of networks, homophily is not a universal principle; certain networks, such as romantic relationship networks or predator-prey networks in ecology, are mostly \\emph{heterophilous} (or \\emph{disassortative}).\nEmploying a GNN which does not account for heterophily can lead to significant performance loss in heterophilous settings~\\citep{MixHop,zhu2020beyond,bo2021beyond}.\nPrevious works have thus proposed architectures for heterophilous data.\n\n\nWhile previous work has focused on naturally-occurring heterophily, heterophilous interactions may also be introduced as adversarial noise: \nas many GNNs exploit homophilous correlation, they can be sensitive to changes that render the data more heterophilous. A natural follow-up question is if and how this observation manifests itself in previously proposed attacking strategies on GNNs.\nIn this work, we thus investigate the relation between heterophily and robustness of GNNs against adversarial attacks of graph structure, focusing on semi-supervised node classification.\nMore specifically, our main contributions are:\n\\vspace{-0.3cm}\n\\setlist{leftmargin=*}\n\\begin{itemize*}\n    \\item \\textbf{Formalization:}\n        We formalize the relation between adversarial structural attacks and the change of homophily level in the underlying graphs with theoretical (\\S\\ref{sec:theories-connections}) and empirical (\\S\\ref{sec:exp-perturb-observations}) analysis.\n        Specifically, we show that on homophilous graphs, effective structural attacks lead to increased heterophily, \n        while, on heterophilous graphs, they alter the homophily level contingent on node degrees. To our knowledge, this is the first formal analysis of such kind. %\n    \\item \\textbf{Heterophily-inspired Design:}\n        We show how the relation between attacks and heterophily can inspire more robust GNNs by demonstrating %\n        that a key architectural feature in handling heterophily, separate aggregators \n        for ego- and neighbor-embeddings, also improves the robustness of GNNs against attacks (\\S\\ref{sec:robustness-design}).\n    \\item \\textbf{Extensive Empirical Analysis:}\n        We show the effectiveness of the heterophilous design in improving empirical (\\S\\ref{sec:exp-benchmark-study}) and certifiable (\\S\\ref{sec:exp-cert-robustness}) robustness of GNNs\n        with extensive experiments on real-world homophilous and heterophilous datasets. \n        Specifically, we compare GNNs with this design, which we refer to as \\emph{heterophily-adjusted} GNNs, to non-adjusted models, including state-of-the-art models designed with robustness in mind. \n        We find that heterophily-adjusted GNNs are up to 11.1 times more certifiably robust and have stronger performance under attacks by up to 40.00\\% compared to non-adjusted, standard models.\n        Moreover, this design can be combined with existing vaccination mechanisms, yielding up to 18.33\\% higher accuracy under attacks than the best non-adjusted vaccinated model.\n        Our code is available at \\githubrepourl.\n\\end{itemize*}\n\n"
            },
            "section 2": {
                "name": "Notation and Preliminaries",
                "content": "\n\\label{sec:notation}\n\nLet $\\graph=(\\vertexSet,\\edgeSet,\\matX)$ be a simple graph with node set $\\vertexSet$, edge set $\\edgeSet$ and node attributes $\\matX$.\nThe one-hop neighborhood $N(v) = \\{u: (u,v) \\in \\edgeSet \\}$ of a node $v\\in \\vertexSet$ is the set of all nodes directly adjacent to $v$;\nthe $k$-hop neighborhood of $v\\in \\vertexSet$ is the set of nodes reachable by a shortest path of length $k$.\nWe represent the graph $\\graph$ algebraically by an adjacency matrix $\\matA \\in \\{0,1\\}^{|\\vertexSet|\\times |\\vertexSet|}$ and node feature matrix $\\matX \\in \\mathbb{R}^{|\\vertexSet| \\times F}$. \nWe use $\\matAs = \\matA + \\matI$ to denote the adjacency matrix with self-loops added, and denote the corresponding row-stochastic matrices as $\\bar{\\matA} = \\matD^{-1}\\matA$ and $\\bar{\\matA}_{\\mathrm{s}} = \\matD_{\\mathrm{s}}^{-1}\\matAs$, respectively, where $\\matD$ is a diagonal matrix with $\\matD_{ii} = \\sum_{j} \\matA_{ij}$ ($\\matD_{\\mathrm{s}}$ is defined analogously).\nWe further assume that there exists a vector $\\vecy$, which contains a unique class label $y_v$ for each node $v$.\nGiven a training set $\\setT = \\{(v_1,y_1), (v_2, y_2), ...\\}$ of labeled nodes, the goal of semi-supervised node classification is to learn a mapping $\\ell: \\vertexSet \\rightarrow \\setY$ from the nodes to the set $\\setY$ of class labels.\n\n\\paragraph{Graph neural networks (GNNs)} \nMost current GNNs operate according to a message passing paradigm where a representation vector $\\V{r}_v$ is assigned to each node $v \\in \\vertexSet$ and continually updated by $K$ layers of learnable transformations.\nThese layers first aggregate representations over neighboring nodes $N(v)$ and then update the current representation via an encoder \\texttt{ENC}. For prevailing GNN models like GCN~\\citep{kipf2016semi} and GAT~\\citep{velickovic2018graph}, each layer can be formalized as\n    {\\small $\\V{r}^{(k)}_v = \\texttt{ENC}\\left(\\texttt{AGGR}\\left(\\left\\{\\V{r}^{(k-1)}_u: u \\in \\neighNoSelfLoop(v) \\cup \\{v\\}\\right\\}\\right)\\right)$},\nwhere $\\texttt{AGGR}$ is the mean function weighted by node degrees (GCN) or an attention mechanism (GAT), and $\\texttt{ENC}$ is a learnable (nonlinear) mapping.\n\n\\paragraph{Adversarial attacks on graphs} \nGiven a  graph $\\graph=(\\vertexSet,\\edgeSet,\\matX)$ and a GNN $f$ that processes $\\graph$, an adversarial attacker tries to create a perturbed graph $\\graph'=(\\vertexSet,\\edgeSet',\\matX)$ with a modified edge-set $\\edgeSet'$ such that the performance of the GNN $f$ is maximally degraded.\nThe information available to the attacker can vary under different scenarios~\\citep{jin2020adversarial,sun2020adversarial}. \nHere, we follow the gray-box formalization by \\cite{zugner2018adversarial}, where the attacker knows the training set $\\setT$, but not the trained GNN $f$. \nThe attacker thus considers a surrogate GNN and picks perturbations that maximize an attack loss $\\attackLoss$~\\citep{zugner2018adversarial,zugner2019adversarial}, assuming that attacks to the surrogate model are transferable to the attacked GNN.\nFor node classification, the attack loss $\\attackLoss$ quantifies how the predictions $\\mathbf{z}_v \\in [0,1]^{|\\setY|}$ made by the GNN $f$ differ from the true labels $\\vecy$. \nFor a targeted attack of node $v$ with class label $y_v \\in \\setY$, we adopt the negative classification margin (\\textbf{CM-type})~\\citep{zugner2018adversarial,xu2019topology}:\n    $\\attackLoss = - \\Delta_c = - (\\mathbf{z}_{v, y_v} - \\max_{y \\neq y_v} \\mathbf{z}_{v, y}).$\nThe attacker usually has additional constraints, such as a limit on the size of the perturbations allowed~\\citep{zugner2018adversarial,zugner2019adversarial}.\n\n\n\\paragraph{Taxonomy of attacks}\nWe follow the taxonomy of attacks introduced in~\\citep{jin2020adversarial,sun2020adversarial}. \nFor node classification, the attacker may aim to change the classification of a specific node $v\\in \\vertexSet$ (\\textbf{targeted attack}), or to decrease the overall classification accuracy (\\textbf{untargeted attack}). \nAttacks can also happen at different stages of the training process: we refer to attacks introduced before training as (pre-training) \\textbf{poison attacks}, and attacks introduced after the training process (and before potential retraining on perturbed data) as (post-training) \\textbf{evasion attacks}. While our theoretical analysis (\\S\\ref{sec:theories}) mainly considers targeted evasion attacks, we consider other attacks in our empirical evaluation (\\S\\ref{sec:exp}). \n\n\\paragraph{Characterizing homophily and heterophily in graphs} \n{Using class labels}, we characterize the types of connections in a graph contributing to its overall level of homophily/heterophily as follows:\n\\begin{definition}[Homo/Heterophilous path and edge]\n\\vspace{-0.2cm}\n\\label{dfn:homophilous-heterophilous-connections}\nA \\emph{$k$-hop homophilous path} from node $w$ to $u$ is a length-$k$ path  \nbetween endpoint nodes with the same class label $y_w = y_u$. \nOtherwise, the %\npath is called \\emph{heterophilous}. \nA homophilous or heterophilous edge is a special case with $k=1$.\n\\end{definition} \n\nFollowing \\citep{zhu2020beyond,lim2021new}, we define the homophily ratio $h$ as:\n\\begin{definition}[Homophily ratio]\n\\vspace{-0.2cm}\n\\label{dfn:homophily-ratio}\nThe \\emph{homophily ratio} \nis the fraction of homophilous edges among all the edges in a graph: $h = |\\{(u,v) \\in \\edgeSet | y_u = y_v\\}|/|\\edgeSet|$. \n\\end{definition}\n\n\\vspace{-0.1cm}\nWhen the edges in a graph are wired randomly, independent to the node labels, the expectation for $h$ is $h_r =  {1}/{|\\setY|}$ for balanced classes~\\citep{lim2021new}. \nFor simplicity, we informally refer to graphs with homophily ratio $h \\gg {1}/{|\\setY|}$ as \\textbf{homophilous graphs} (which have been the focus in most prior works), graphs with homophily ratio $h \\ll {1}/{|\\setY|}$ as \\textbf{heterophilous graphs}, and graphs with homophily ratio $h \\approx {1}/{|\\setY|}$ as \\textbf{weakly heterophilous graphs}. \n\n"
            },
            "section 3": {
                "name": "Relation between Graph Heterophily \\&  Model Robustness",
                "content": "\n\\label{sec:theories}\nIn this section, we first show theoretical results on the relation between adversarial structural attacks and the change in the homophily level of the underlying graphs. \nThough empirical analyses from previous works have suggested this relation on homophilous graphs~\\citep{wu2019adversarial,jin2020adversarial}, to our knowledge, we are the first to formalize it with theoretical analysis and address the case of heterophilous graphs. \nAs an implication of the relation, we then discuss how a key design that improves predictive performance of GNNs under heterophily can also help boost their robustness. \n\\vspace{-0.2cm}\n",
                "subsection 3.1": {
                    "name": "How Do Structural Attacks Change Homophily in Graphs?",
                    "content": " \n\\label{sec:theories-connections}\n\n\\paragraph{Homophilous graphs: structural attacks are mostly hetero\\-philous attacks} \nOur first result shows that, for homophilous data, effective structural attacks on GNNs (as measured by loss $\\attackLoss$) always result in a reduced level of homophily where either new heterophilous connections are added or existing homophilous connections are removed. \nIt also states that direct perturbations on 1-hop neighbors of the target nodes are more effective than indirect perturbations (influencer attacks~\\citep{zugner2018adversarial}) on multi-hop neighbors. \nFor simplicity, akin to previous works \\citep{zugner2018adversarial, zugner2019adversarial} we establish our results for targeted evasion (post-training) attacks in a stylized learning setup with a linear GNN. \nHowever, our findings generalize to more general setups on real-world datasets as we show in our experiments (\\S\\ref{sec:exp-perturb-observations}).\n{In the theorems below, we use the notion of \\emph{\\gambit node}: node $u$ is called a gambit if  a perturbation that targets node $v \\in \\vertexSet$  adjusts the connectivity of node $u \\in \\vertexSet$.}\n\n\\vspace{-0.1cm}\n\\begin{theorem}\n    \\label{thm:1} Let $\\graph=(\\vertexSet,\\edgeSet,\\matX)$ be a \n    self-loop-free graph with adjacency matrix $\\matA$ and node features {\\small $\\V{x}_v = p\\cdot \\mathrm{onehot}(y_v) + \\tfrac{1-p}{|\\setY|}\\cdot\\mathbf{1}$} for each node $v$, where $\\mathbf{1}$ is an all-1 vector, and $p$ is a parameter that regulates the signal to noise ratio. \n    Assume that a fraction $h$ of each node's neighbors belong to the same class, while a fraction {\\small $\\tfrac{1-h}{|\\setY|-1}$} belongs uniformly to any other class.\n    Consider a 2-layer linear GNN \n    {\\small $f_s^{(2)}(\\matA,\\matX) = \\bar{\\matA}^2_\\mathrm{s}\\matX\\matW$} trained on a training set $\\setT \\subseteq \\mathcal{D}_{\\vertexSet}$, %\n    {with at least one node from each class $y\\in\\setY$, and degree $d$ for all nodes with a distance less than 2 to any $v\\in \\mathcal{D}_{\\vertexSet}$}. %\n    For a unit structural perturbation that %\n    {involves a target node $v \\in  \\mathcal{D}_{\\vertexSet}$, and a %\n    {correctly classified} \\gambit node with degree $d_a$}, the following statements hold if {\\small $h \\geq \\tfrac{1}{|\\setY|}$:}\n    \\begin{enumerate*}%\n        \\item the attack loss $\\attackLoss$ (\\S\\ref{sec:notation}) of the target $v$ increases only for actions \\emph{increasing heterophily}, i.e., when removing a homophilous edge or path, or adding a heterophilous edge or path to node~$v$;\n        \\item direct perturbations on edges (or 1-hop paths) incident to the target node $v$ lead to greater increase in $\\attackLoss$ than indirect perturbations on multi-hop paths to target node~$v$.\n    \\end{enumerate*}\n\\end{theorem}\n\n\\vspace{-0.2cm}\nWe give the proof in App.~\\ref{app:proof-thm1}. \nIntuitively, the relative inability of existing GNNs to make full use of heterophilous data~\\citep{Pei2020Geom-GCN,zhu2020beyond} can be exploited by inserting heterophilous connections in graphs where homophilous ones are expected. Though the theorem shows that effective attacks on homophilous graphs \\emph{necessarily} reduce the homophily level, the converse is not true: not all perturbations which reduce the homophily level are effective attacks~\\citep{ma2021homophily}.\n\n\n\n        \n\n\\paragraph{Heterophilous graphs: structural attacks can be homo\\-phil\\-ous or hetero\\-philous, depending on node degrees} \nWhen a graph displays heterophily, our analysis shows a more complicated picture on how the level of homophily in the graph is changed\nby effective structural attacks: in heterophilous case, the direction of change is dependent on the degrees of %\n{both the target node $v$ and the gambit node $u$ of the attack}. \nSpecifically, if %\n{the degree of \\emph{either node}} is low, attacks increasing the heterophily are still effective; however, %\n{if the degrees $d$ and $d_a$ of \\emph{both nodes} are high}, attacks \\emph{decreasing} the heterophily will be effective. Similar to the homophilous case, we formalize our results below for targeted evasion attacks in a stylized learning setup. \n\n\n\n\\vspace{-0.2cm}\n\\begin{theorem}\n    \\label{thm:heterophily}\n    Under the setup of Thm.~\\ref{thm:1}, \n    for a unit perturbation that %\n    {involves a target node $v$ with degree $d$, and a %\n    {correctly classified} \\gambit node with degree $d_a$}, the following statements hold:\n    \\begin{enumerate*}%\n            \\item \\emph{\\textbf{(Low-degree target node)}} if $0 < d \\leq |\\setY| - 2$, for any $d_a \\geq 0$ and $h\\in [0, 1]$, \n            the attack loss $\\attackLoss$ (\\S\\ref{sec:notation}) of $v$ increases only under actions {\\emph{increasing heterophily}} in the graph;\n            \\item \\emph{\\textbf{(High-degree target node)}} if $d> |\\setY| - 2$, conditioning on the degree $d_a$ of the \\gambit node:\n            \\begin{enumerate}\n                \\item \\emph{\\textbf{(Low-degree \\gambit node)}} if $d_a < \\frac{(d+2) (|\\setY|-1)}{d-|\\setY|+2}$, for any $h\\in [0, 1]$, the attack loss $\\attackLoss$ (\\S\\ref{sec:notation}) of $v$ increases only under actions \\emph{increasing heterophily} in the graph;\n                \\item \\emph{\\textbf{(High-degree \\gambit node)}} if $d_a \\geq \\tfrac{(d+2) (|\\setY|-1)}{d-|\\setY|+2}$, for $0 \\leq h < \\tfrac{d_a (d-|\\setY|+2)-(d+2) (|\\setY|-1)}{(d+1) |\\setY| d_a} < \\frac{1}{|\\setY|}$, $\\attackLoss$ (\\S\\ref{sec:notation}) of $v$ increases only under actions \\emph{reducing heterophily}.\n            \\end{enumerate}\n    \\end{enumerate*}\n    In the statements above, the actions \\emph{increasing heterophily} include removing a homophilous edge or adding a heterophilous edge to node $v$, and the actions \\emph{reducing heterophily} include adding a homophilous edge or removing a heterophilous edge to node $v$.\n\\end{theorem}\n\n\n\\vspace{-0.2cm}\n{The above theorems cover the situation when the gambit nodes are initially classified correctly (where attacks introducing heterophily can be unambiguously defined using the ground-truth class labels of the nodes involved).\nHowever, in \\S\\ref{sec:exp-perturb-observations}, we show on real-world datasets that a relaxed interpretation of the theorems, where heterophily is instead defined by the \\emph{predicted} class labels of GNNs, \ncan explain the behavior of the attacks regardless of the initial correctness of the gambits.}\n\n\\vspace{-0.2cm}\n"
                },
                "subsection 3.2": {
                    "name": "Boosting Robustness with A Simple Heterophilous Design",
                    "content": " \n\\label{sec:robustness-design}\nA natural follow-up question is whether GNNs with better performance under heterophily are also more robust against structural attacks. \nWe deduce that a key design for improving GNN performance for heterophilous data---separate aggregators for ego- and neighbor-embeddings---can also boost the robustness of GNNs by enabling them to better cope with {adversarial changes in heterophily}.\n\n\\paragraph{Separate aggregators for ego- and neighbor-embeddings} \nThis design uses separate GNN aggregators for ego-embedding $\\V{r}_v$ and neighbor-embeddings $\\{\\V{r}_u: u \\in \\neighNoSelfLoop(v)\\}$. \nFormally, the representation learned for node $v$ in the $k$-th layer is:\n{\\small\n\\begin{equation}\n    \\V{r}^{(k)}_v = \\texttt{ENC}\n    \\left( {\\texttt{AGGR1}}(\\V{r}^{(k-1)}_v, \\V{r}^{(k-2)}_v, ..., \\V{r}^{(0)}_v), \\; \n    {\\texttt{AGGR2}}(\\{\\V{r}^{(k-1)}_u: u \\in {\\neighNoSelfLoop(v)}\\})\n    \\right),\n    \\label{eq:design1}\n\\end{equation}\n}%\nwhere \\texttt{AGGR1} and \\texttt{AGGR2} are \\emph{separate} aggregators, such as averaging functions (GCN), attention mechanisms (GAT), or other pooling mechanisms~\\citep{hamilton2017inductive}.\nThis design has been utilized in existing GNN models \n{(we show examples later in this section)}, \nand has been shown to significantly boost the representation power of GNNs under natural heterophily~\\citep{zhu2020beyond}. \nThe ego-aggregator \\texttt{AGGR1} may also introduce skip connections~\\citep{XuLTSKJ18-jkn} to the ego-embeddings aggregated in previous layers as shown in Eq.~\\eqref{eq:design1}, {which is another design that further improves the representation power under heterophily~\\citep{zhu2020beyond}.}\n\n\n\\paragraph{Intuition}\nThe key design changes, as compared to the GCN formulation in \\S\\ref{sec:notation}, allow for the ego-embedding $\\V{r}_v$ to be aggregated and weighted \\emph{separately} from the neighbor-embeddings $\\{\\V{r}_u: u \\in \\neighNoSelfLoop(v)\\}$, as well as for the use of skip connections to ego-embeddings of previous layers.\nIntuitively, ego-embeddings of feature vectors at the first layer are independent of the graph structure and thus unaffected by adversarial structural perturbations. \nHence, a separate aggregator and skip connections can provide better access to unperturbed information and mitigate the effects of the attacks.\n\n\\paragraph{Theoretical analysis} \nWe formalize the above intuition \nthat shows how separate aggregators for ego- and neighbor-embeddings enable GNN layers to reduce the attack loss.\n\n\\begin{theorem}\n    \\label{thm:2}\n    Under the setup of Thm.~\\ref{thm:1}, consider two alternative layers from which a two-layer linear GNN is built: \\textbf{(1)} a layer defined as $f_s(\\matA,\\matX)=\\bar{\\matA}_\\mathrm{s}\\matX\\matW$; and %\n    \\textbf{(2)} a layer formulated as $f(\\matA,\\matX;\\alpha) = \\left((1-\\alpha) \\bar{\\matA} + \\alpha \\matI \\right) \\matX \\matW$, which mixes the ego- and neighbor-embedding linearly under a predefined weight $\\alpha \\in [0, 1]$. \n    Then, for $h > {1}/{|\\setY|}$, $\\alpha > {1}/{(1+d_a)}$, and a unit perturbation increasing $\\attackLoss$ as in Thm.~\\ref{thm:1}, outputs of layer $f$ lead to a strictly smaller increase in $\\attackLoss$ than $f_s$.\n    \\vspace{-0.1cm}\n\\end{theorem}\n\nWe provide the proof in App.~\\ref{app:proof-thm2};  note that for $\\alpha = {1}/{(1+d_a)}$, the two layers are the same: $f(\\matA,\\matX;\\alpha) = f_s(\\matA,\\matX)$. Theorem~\\ref{thm:2} shows that an increase to the weights of ego-embedding \n{(manually or through training)}\nimproves the robustness of the GNN $f$ for a homophily ratio {$h > {1}/{|\\setY|}$}. \nThough aggregators and encoders are stylized \n{in this \\emph{simple} instantiation of the design}\nin the theorem, the empirical analysis in \\S\\ref{sec:exp-benchmark-study} confirms that GNNs with more advanced aggregators and encoders,\n{which we will discuss next},\nalso benefit from separate aggregators. Specifically, we find that such GNNs outperform methods without this design by up to 40.00\\% and 48.88\\% on homophilous and heterophilous graphs, respectively, while performing comparably on clean datasets.\n\n\\paragraph{Instantiations of the design on GNNs} %\n\\label{sec:robustness-design-instantiations}\nWe demonstrate how the heterophilous design outlined in Eq.~\\eqref{eq:design1} \nis instantiated in various GNN models, which are used in our empirical evaluation in \\S\\ref{sec:exp}. \nIn particular, we highlight how these GNN architectures allow separate aggregations of the ego- and neighbor-embeddings. \n\\begin{itemize}\n    \\item In \\textbf{H$_2$GCN}~\\cite{zhu2020beyond}, a final representation is computed for each node $v\\in \\vertexSet$ through $\\mathbf{r}_{v}^{(\\mathrm{final})} =  \\texttt{CONCAT}(\\mathbf{r}_{v}^{(0)}, \\mathbf{r}_{v}^{(1)}, ..., \\mathbf{r}_{v}^{(K)})$, where $\\mathbf{r}_{v}^{(0)}$ is the non-linear ego-embedding of node features and $\\mathbf{r}_{v}^{(k)}$ are the intermediate representations aggregated in the $k$-th layer, where $k \\in (1, ..., K)$. By interpreting the update rule's $\\texttt{CONCAT}$ as the $\\texttt{ENC}$ operation, $\\texttt{AGGR1}$ as the skip connection to the ego-embedding of node features, and the concatenation of the intermediate representations as $\\texttt{AGGR2}$, the ego- and neighbor-embeddings are separately aggregated as stated in the design.\n    \n    \\item \\textbf{GraphSAGE} (with mean aggregator)~\\citep{hamilton2017inductive} utilizes a concatenation-based encoding scheme through their update of \n    \\vspace{-0.2cm}\n    \\begin{equation*}\n        \\mathbf{r}_{v}^{(k)}= \\sigma\\left(\n            \\texttt{CONCAT}\\left(\n                \\mathbf{r}_{v}^{(k-1)}, \\;\n                \\texttt{MEAN}\\left(\n                    \\{\\mathbf{r}_{u}^{(k-1)}, \\forall u\\in N(i)\\}\\right)\n            \\right) \\cdot \\mathbf{W}\n        \\right),\n    \\vspace{-0.2cm}\n    \\end{equation*}\n    where $\\texttt{ENC}(\\mathbf{x}_1, \\mathbf{x}_2) = \\sigma(\\texttt{CONCAT}(\\mathbf{x}_1, \\mathbf{x}_2)\\cdot\\mathbf{W})$, $\\texttt{AGGR1}(\\cdot) = \\mathbf{r}_{u}^{(k-1)}$, and $\\texttt{AGGR2}$ is the mean function. \n\n    \\item \\textbf{GPR-GNN}~\\citep{chien2021adaptive} embeds each node feature vector separately with a fully connected layer to compute $\\mathbf{R}_{v:}^{(0)}$ (or $\\mathbf{H}_{v:}^{(0)}$ as in the original paper), similar to \\method, and then updates each node's hidden representations through a weighted sum of all $k$-th hop layers around the ego-node, where $k \\in (0, 1, ... , K)$. By interpreting the summation as the $\\texttt{ENC}$ operation, $\\texttt{AGGR1}(\\cdot) = \\bm{\\gamma}_{0}\\mathbf{R}^{(0)}$, and $\\texttt{AGGR2}(\\cdot) = \\sum_{k=1}^{K} \\bm{\\gamma}_{k}{\\tilde{\\mathbf{A}}^k}_{\\mathrm{sym}}\\mathbf{R}^{(k-1)}$, where $\\bm{\\gamma}$ denotes the weights associated with each $k$-hop ego network, the aggregation of the ego- and neighbor-embeddings is decoupled.\n    \n    \\item \\textbf{FAGCN}~\\citep{bo2021beyond} follows a similar update function to GPR-GNN with \n    \\vspace{-0.2cm}\n    \\begin{equation*}\n        \\mathbf{r}_{i}^{(l)} = \\varepsilon \\mathbf{r}_{i}^{(0)} + \\sum_{j\\in N(i)}  \\frac{\\alpha_{ij}^{G}}{\\sqrt{d_{i}d_{j}}} \\mathbf{r}_{j}^{(l-1)}\n        \\vspace{-0.3cm}\n    \\end{equation*}\n    where $\\mathbf{r}_{i}^{(0)}$ (or $\\mathbf{h}_{i}^{(0)}$ in the original paper) represents the non-linear ego-embedding and $\\alpha_{ij}^{G}$ is a constant measuring the ratio of low and high frequency components. The heterophilous design can similarly be recovered by interpreting the sum as the $\\texttt{ENC}$ operation, $\\texttt{AGGR1}(\\cdot) = \\varepsilon \\mathbf{r}_{i}^{(0)}$ as a weighted skip connection to the ego-embedding of features, and the weighted sum of embeddings within the neighborhood $N(i)$ of node $i \\in \\vertexSet$ as $\\texttt{AGGR2}(\\cdot)$.\n    \n    \\item \\textbf{CPGNN}~\\citep{zhu2020graph} formulates the update function of belief vectors $\\mathbf{R}^{(k)}$ after the $k$-th propagation layer as\n    $\n        \\mathbf{R}^{(k)} = \\mathbf{R}^{(0)} + \\mathbf{A} \\mathbf{R}^{(k-1)} \\bar{\\mathbf{H}}\n    $, where $\\mathbf{R}^{(0)}$ ($\\bar{\\mathbf{B}}^{(0)}$ in the original paper) consists of prior belief vectors for each node (as the ego-embeddings $\\mathbf{r}_{i}^{(0)}$ in Eq.~\\eqref{eq:design1}), and $\\bar{\\mathbf{H}}$ is the learnable compatibility matrix. The heterophilous design is recovered by letting $\\texttt{AGGR1}(\\cdot) = {\\mathbf{R}}^{(0)}$ as a skip connection, $\\texttt{AGGR2}(\\cdot) = \\mathbf{A} {\\mathbf{R}}^{(k-1)} \\bar{\\mathbf{H}}$, and the $\\texttt{ENC}$ operation as the summation.\n\n    \\item \\textbf{APPNP}~\\cite{klicpera2018predict} first generates predictions $\\mathbf{R}_{v:}^{(0)}$ (or $\\mathbf{H}_{v:}^{(0)}$ as in the original paper) of each node $v$ based on its own feature, then updates the predictions through power iterations of Personalized PageRank. More specifically, the $k$-th iteration step is formulated as $\\mathbf{R}^{(k)} = (1 - \\alpha) {\\tilde{\\mathbf{A}}}_{\\mathrm{sym}}\\mathbf{R}^{(k-1)} + \\alpha \\mathbf{R}^{(0)}$. The heterophilous design can be recovered by letting $\\texttt{AGGR1}(\\cdot) = \\mathbf{R}^{(0)}$ as a skip connection to the initial prediction, $\\texttt{AGGR2}(\\cdot) = {\\tilde{\\mathbf{A}}}_{\\mathrm{sym}}\\mathbf{R}^{(k-1)}$, and the summation weighted by $\\alpha$ as the $\\texttt{ENC}$ operation.\n\\end{itemize}\n\n\n"
                }
            },
            "section 4": {
                "name": "Related Work",
                "content": "\n\n\n\\paragraph{Adversarial attacks and defense strategies for graphs} \nSince \\nettack~\\citep{zugner2018adversarial} and RL-S2V~\\citep{dai2018adversarial} first demonstrated the vulnerabilities of GNNs against adversarial perturbations, a variety of attack strategies under different scenarios have been proposed,\nincluding adversarial attacks on the graph structure~\\citep{dai2018adversarial,xu2019topology,bojchevski2019adversarial,li2020adversarial,chang2020restricted}, node features \\citep{takahashi2019indirect,ma2020towards}, or combinations of both~\\citep{zugner2018adversarial,zugner2019adversarial,wu2019adversarial}. \nOn the defense side, \nvarious techniques for \nimproving the GNN robustness against adversarial attacks \nhave been proposed, including: \nadversarial training~\\citep{xu2019topology,zugner2019adversarial,bojchevski2019certifiable};  \nRGCN~\\citep{zhu2019robust}, which adopts Gaussian-based embeddings and a variance-based attention mechanism;\nlow-rank approximation of graph adjacency~\\citep{entezari2020all} against Nettack~\\citep{zugner2018adversarial};\nPro-GNN~\\citep{jin2020graph}, which estimates the unperturbed graph structure in training with the assumptions of low-rank, sparsity, and homophily of node features; GCN-Jaccard~\\citep{wu2019adversarial} and GNNGuard~\\citep{zhang2020gnnguard}, which assume homophily of features (or structural embeddings) and train GNN models on a pruned graph with only strong homophilous links;\nand Soft Medoid~\\citep{geisler2020reliable}, an aggregation function with improved robustness. Other recent works have looked into the certification of nodes that are guaranteed to be robust against certain structural and feature perturbations~\\citep{zugner2019certifiable,bojchevski2019certifiable,zugner2020certifiable}, including approaches based on model-agnostic randomized smoothing~\\citep{cohen2019certified,lee2019tight,bojchevski2020efficient}. \nInterested readers can refer to the recent surveys~\\citep{jin2020adversarial,sun2020adversarial} for a comprehensive review. \n\n\\paragraph{GNNs \\& Heterophily}\nRecent works~\\citep{Pei2020Geom-GCN,liu2020non,zhu2020beyond,ma2021homophily} have shown that heterophilous datasets can lead to significant performance loss for popular GNN architectures \n(e.g., GCN~\\citep{kipf2016semi},  GAT~\\citep{velickovic2018graph}).\nThis issue is also known in classical semi-supervised learning~\\citep{peel2017graph}.\nTo address this issue, several GNN designs for handling heterophilous connections have been proposed~\\citep{MixHop,Pei2020Geom-GCN,zhu2020beyond,dong2021graph,li2021beyond,zhu2020graph,bo2021beyond}.\n\\citet{yan2021two} recently discussed the connection between heterophily and oversmoothing for GNNs, and  designs to address both issues;\n{\\cite{donald2022fairness} studied how locally-occuring heterophily affects fairness of GNNs.}\nHowever, the formal connection between heterophily and robustness of GNNs\nhas received little attention.\nHere we focus on a simple yet powerful design that significantly improves performance under heterophily~\\citep{zhu2020beyond}, and can be readily incorporated into GNNs. \n"
            },
            "section 5": {
                "name": "Empirical Evaluation",
                "content": "\n\\label{sec:exp}\n\nOur analysis seeks to answer the following questions:\n(Q1) Does our theoretical analysis on the relations between adversarial attacks and changes in heterophily level generalize to real-world datasets? \n(Q2) Do heterophily-adjusted GNNs, i.e., models with separate aggregators for ego- and neighbor-embeddings,\nshow improved robustness against state-of-the-art attacks? \n(Q3) Does the identified design improve the \\emph{certifiable} robustness of GNNs? \n\nFirst, we describe the experimental setup and datasets that we use to answer the above questions. \n\n\\label{sec:exp-setups}\n\n\\vspace{0.1cm}\n\\paragraph{Attack Setup} We consider both targeted and untargeted attacks (\\S\\ref{sec:notation}), generated by \\nettack~\\citep{zugner2018adversarial} and Metattack~\\citep{zugner2019adversarial}, respectively.\nFor each attack method, we consider poison (pre-training) and evasion (post-training) attacks, yielding 4 attack scenarios in total.\nWe focus on robustness against structural perturbations and keep the node features unchanged. \nWe randomly generate 3 sets of perturbations per attack method and dataset, and consistently evaluate each GNN model on them.\nFor \\nettack, we randomly select 60 nodes from the graph as the target nodes for each set of perturbations, instead of the GCN-based target selection approach as in \\citep{zugner2018adversarial}:\nthe approach in \\citep{zugner2018adversarial} only selects nodes that are correctly classified by GCN~\\citep{kipf2016semi} on clean data;\nthis introduces unfair advantage towards GCN, especially on heterophilous datasets where GCN can exhibit significantly inferior accuracy to models like GraphSAGE~\\citep{zhu2020beyond}. \nFor the experiments in \\S\\ref{sec:exp-perturb-observations}, we use a budget of 1 perturbation per target node to match the setup of our theorems; for the benchmark study in \\S\\ref{sec:exp-benchmark-study}, we use an attack budget equal to a node's degree and allow direct attacks on target nodes. \nFor Metattack, we budget the attack as 20\\% of the number of edges in each dataset, and we use the Meta-Self variant as it shows the most destructiveness~\\citep{zugner2019adversarial}.\n\n\\paragraph{GNN Models} \nTo show the effectiveness of our identified design, we evaluate four groups of models against adversarial attacks: \n\\textbf{(1)}~Baseline models without any vaccination, including some of the most popular methods: GCN~\\citep{kipf2016semi},  GAT~\\citep{velickovic2018graph},\nand the graph-agnostic multilayer perceptron (MLP) which relies only on node features; \n\\textbf{(2)}~State-of-the-art ``vaccinated'' baselines designed with robustness in mind: \nProGNN~\\citep{jin2020graph}, GNNGuard~\\citep{zhang2020gnnguard}, GCN-SVD~\\citep{entezari2020all} and GCN-SMGDC, which adopts the Soft Medoid aggregator~\\cite{geisler2020reliable} and GDC~\\cite{klicpera_diffusion_2019} on GCN~\\cite{kipf2016semi} architecture;  \n\\textbf{(3)}~Models with the heterophilous design only: GraphSAGE~\\citep{hamilton2017inductive}, \\method~\\citep{zhu2020beyond}, CPGNN~\\citep{zhu2020graph}, GPR-GNN~\\citep{chien2021adaptive} FAGCN~\\citep{bo2021beyond} and APPNP~\\cite{klicpera2018predict}; \nwe discussed how these models instantiate this design in \\S\\ref{sec:robustness-design-instantiations}; \n\\textbf{(4)}~{\nModels with both the %\nheterophilous design and explicit robustness-enhancing mechanisms, where we adopt two existing mechanisms: (i)~SVD-based low-rank approxmiation~\\citep{entezari2020all} (H$_2$GCN-SVD and GraphSAGE-SVD), \nand (ii)~Soft Medoid aggregator~\\cite{geisler2020reliable} with GDC~\\cite{klicpera_diffusion_2019} (\\method-SMGDC and GraphSAGE-SMGDC).\nWe combine both these mechanisms with heterophily-adjusted GNNs instead of non-adjusted models (e.g., GCN)---detailed formulations are given on our \\repo{}. \n}\nWe set the number of layers as 2 and the size of hidden units per layer as 64 for all models to ensure a fair comparison between different architectures and designs.\nWe provide more implementation details and hyperparameter settings on our \\repo{} (App. \\S\\ref{app:exp-details}).\n\n\n\n\\paragraph{Datasets \\& Evaluation Setup} We consider {three widely-used citation networks~\\citep{citeseer_dataset,namata2012query}} with strong homophily---Cora~\\citep{cora_dataset}, {Pubmed, and} Citeseer---along with one weakly and one strongly heterophilous graph, introduced by \\citet{lim2021new}: FB100~\\citep{FB100Source} and Snap Patents~\\citep{SnapSource1, snapnets}. \nWe report summary statistics in Table~\\ref{table:dataset-stats}, and provide more details %\non our \\repo.\nFor computational tractability, we subsample the Snap Patents data via snowball sampling~\\citep{Leo1961Snowball}, where we keep 20\\% of the neighbors for each traversed node; we give detailed algorithm on our \\repo.\n{The sizes of the datasets that we used in our experiments are similar to those in previous works on GNN robustness~\\cite{geisler2020reliable, jin2020graph}.}\nWe follow the evaluation procedure of \\cite{zugner2018adversarial, jin2020graph} to split the nodes of each dataset into training (10\\%), validation (10\\%) and test (80\\%) data, and determine the model parameters on training and validation splits.\nWe report the average performance and standard deviation on the 3 sets of generated perturbations. \nFor targeted attacks with \\nettack, we report the classification accuracy on the target nodes; for untargeted attacks with Metattack, we report it over the whole test data.\n\n\n\\paragraph{Robustness Certificates} We adopt randomized smoothing for GNNs~\\citep{bojchevski2020efficient} to evaluate the certifiable robustness, with parameter choices detailed in our \\githubrepo.\nWe only consider structural perturbations in the randomization scheme. \nFollowing \\citet{geisler2020reliable}, we measure the certifiable robustness of GNN models with the accumulated certifications (AC) and the average maximum certifiable radii for edge additions ($\\bar{r}_a$) and deletions ($\\bar{r}_d$) over all correctly predicted nodes. \nMore specifically, AC is defined as\n$-R(0, 0) + \\sum_{r_a, r_d \\geq 0} R(r_a, r_d)$,\nwhere $R(r_a, r_d)$ is the \\emph{certifiably correct ratio}, i.e., the ratio of the nodes in the test splits that are \\emph{both} predicted correctly by the smoothed classifier \\emph{and} certifiably robust at radius $(r_a, r_d)$.\nIn addition, we report the accuracy of each model with randomized smoothing enabled on the test splits of the clean datasets, which is equal to $R(0, 0)$.\nWe report the average and standard deviation of each statistic over the 3 different training, validation and test splits. %\n\n\\paragraph{Hardware Specifications} \nWe use a workstation with a 12-core AMD Ryzen 9 3900X CPU, 64GB RAM, and a Quadro P6000 GPU with 24 GB GPU Memory. \n\n\\paragraph{Code and Additional Details} Code and additional details on the setups and results are available on GitHub repository: \\githubrepourl.\n\n\n\n\n\\vspace{-0.2cm}\n",
                "subsection 5.1": {
                    "name": "(Q1) Structural Attacks are Mostly Heterophilous: Empirical Validation",
                    "content": "\n\\label{sec:exp-perturb-observations}\n\nTo show that our theoretical analysis in \\S\\ref{sec:theories-connections} generalizes to more complex settings beyond the assumptions we made in the theorems, we look into effective targeted attacks made by \\nettack{} on real-world homophilous and heterophilous datasets, and present statistics of the attacks in Table~\\ref{tab:perturbation-stats}, with a focus on the ratios of heterophilous attacks.\nWe use a budget of 1 perturbation per target node in this experiment, and the statistics are reported among all effective perturbations\ntargeting nodes that are correctly classified on clean datasets by the surrogate GNN of \\nettack{} (i.e., GCN) as described in \\S\\ref{sec:exp-setups}.\nTo validate the dependency between the degrees of the target/gambit nodes and the changes of heterophily predicted by Thm.~\\ref{thm:heterophily}, we also show the scatter plots of node degrees in Fig.~\\ref{fig:nettack-degree-scatter}.\n\n\n\n\n\n\\paragraph{Homophilous Networks} For the strongly homophilous {Cora, Pubmed and Citeseer graphs, all changes introduced by effective attacks in the graph structure} follow the conclusion of Thm.~\\ref{thm:1}: they reduce homophily (increase heterophily) by adding heterophilous edges or removing homophilous edges. \nThese results show that despite the simplified analysis, the takeaway of Thm.~\\ref{thm:1} can be generalized to real-world datasets. \nIn addition, the attacks mostly introduce, rather than prune, edges, suggesting that attacks adding outlier edges to the graph are more powerful than attacks removing informative existing edges. \nThese observations in our experiments are consistent with the observations from previous works~\\citep{jin2020adversarial,geisler2020reliable}.\n\n\n\\paragraph{Heterophilous Networks} For heterophilous graphs FB100 ($h \\approx {1}/{|\\setY|}$) and Snap ($h < {1}/{|\\setY|}$), \nFig.~\\ref{fig:nettack-degree-scatter} %\nshows that almost all attacks leverage gambit nodes with low degrees (1 or 2); no node with degree higher than 5 is leveraged.\nAll attacks leveraging correctly classified gambit nodes are connecting node $u \\in \\vertexSet$ with a different ground-truth class label $y_u \\neq y_v$ to the target nodes $v \\in \\vertexSet$; attacks leveraging incorrectly classified gambit nodes are always connecting node $u$ with a different \\emph{predicted} class label $\\hat{y}_u \\neq \\hat{y}_v = y_v $ to the target node $v$, even though some gambit nodes have the same \\emph{ground-truth} class label $y_u = y_v \\neq \\hat{y}_u$ as the target nodes. \nThese results validate the conclusion of Thm.~\\ref{thm:heterophily} on correctly classified gambit nodes, and demonstrate its generalizability under the heterophily definition based on predicted class labels. Note that the predicted class labels $\\hat{y}_u$ for each node $u \\in \\vertexSet$ are based on GCN, which is the surrogate GNN used by \\nettack{}.\n\n\n\n\n\n\n\n\n        \n        \n\n        \n        \n        \n\n\n\n\n\\vspace{-0.4cm}\n"
                },
                "subsection 5.2": {
                    "name": "(Q2) Benchmark Study of GNN Models: Heterophilous Design Leads to Improved Empirical Robustness",
                    "content": "\n\\label{sec:exp-benchmark-study}\n\n\n\n\n\n\nTo answer (Q2) on whether heterophily-adjusted GNN models show improved performance against state-of-the-art attacks, we conduct a comprehensive benchmark study. \nWe consider all four categories of GNN models mentioned in \\S\\ref{sec:exp-setups}, and evaluate their robustness against both targeted and untargeted attacks. \nWe report the hyperparameters for each method \non our \\repo{} (App. \\S\\ref{app:exp-details-params}). \nTable~\\ref{table:real-results-poison-only} shows the performance of each method under poison (pre-training) attacks and on clean (unperturbed) data, and\nFig.~\\ref{fig:nettack_cora_FB100} visualizes the corresponding performance changes relative to the clean datasets. \nFor conciseness, we report {additional results on Pubmed in Table~\\ref{table:results-pubmed}}, and under evasion (post-training) attacks on our \\githubrepo~(Table~\\ref{table:real-results-detailed-netattack} and \\ref{table:real-results-detailed-metattack-20p}), where\nwe also discuss how our simple heterophilous design leads to only minor computational overhead compared to existing vaccination mechanisms (App. \\S\\ref{app:real-runtime-complexity}). \n\n\n\n\n\\paragraph{Targeted attacks by \\nettack}\n\\textcircled{\\raisebox{-0.9pt}{1}}~\\textit{Poison attacks}. \nUnder targeted poison attacks, Table~\\ref{table:real-results-poison-only} (left) shows that GraphSAGE-SVD and H$_2$GCN-SVD, which combine our identified design with a low-rank vaccination approach adopted in GCN-SVD~\\citep{entezari2020all},\noutperform state-of-the-art vaccinated methods across all datasets by up to \n13.34\\% in homophilous settings %\nand 18.33\\% in heterophilous settings. %\nFurthermore, GraphSAGE-SMGDC and H$_2$GCN-SMGDC, which combine our design with existing vaccinations based on Soft Medoid~\\cite{geisler2020reliable} and GDC~\\cite{klicpera_diffusion_2019}, show better performance against attacks in all datasets compared to GCN-SMGDC, the corresponding baseline without our design, with \nup to 19.44\\% improvement on homophilous settings %\nand 30.55\\% improvement on heterophilous settings.\nIn summary, these observations show that the heterophilous design improves the robustness of GNNs alongside existing vaccination mechanisms.\n\n\n\nMethods merely employing the identified design also show significantly improved robustness, though there are differences in the amount of robustness improvement due to architectural differences. \nSpecifically, these methods outperform the best unvaccinated method (GAT) on all datasets by up to 33.75\\% in average, \ndespite having mostly comparable performance on clean datasets; \nmethods like APPNP and CPGNN also show comparable or even better robustness than state-of-the-art vaccinated GNNs. {These observations also apply to the larger Pubmed dataset in Table~\\ref{table:results-pubmed}}.\n{We also note that the graph-agnostic MLP, which is immune to structural attacks, outperforms all GNNs against attacks on Citeseer and Snap; this\nshows the challenges in defending against targeted attacks and \ncalls for more effective defense strategies upon our discoveries.}\n\n\\textcircled{\\raisebox{-0.9pt}{2}}~\\textit{Evasion attacks}. \n{Under evasion attacks \n(detailed results are reported in App. Table~\\ref{table:real-results-detailed-netattack} on our \\repo{}),\nwe observe \\textit{similar trends as in poison attacks}: \nGraphSAGE-SVD and H$_2$GCN-SVD are up to 20.55\\% more accurate than the GCN-SVD, the corresponding baseline without the heterophilous design,\nand GraphSAGE-SMGDC and H$_2$GCN-SMGDC outperform GCN-SMGDC by up to 19.44\\%. }\nMethods featuring the identified design alone achieve up to 38.89\\% gain in average performance against the best unvaccinated baseline, {which we also observe on Pubmed.}\nWe note that two baselines, GNNGuard and ProGNN, are designed specifically to defend against poison attacks, and are not capable of addressing evasion attacks.\n\n\n\n\n\n\\paragraph{Untargeted attacks by Metattack}\n\\textcircled{\\raisebox{-0.9pt}{1}}~\\textit{Poison attacks}.\nWe also test the robustness of each method against untargeted attacks. \nTable~\\ref{table:real-results-poison-only} (right) shows the performance under poison attacks.  \nThough our theoretical analysis in \\S\\ref{sec:theories} focuses on the effect of the heterophilous design under targeted attacks, we observe similar improvements in robustness against untargeted attacks in the poison setup. GNNs with the identified design show mostly improved robustness compared to unvaccinated models, while having similar performance on the clean datasets. \nSpecifically, CPGNN shows exceptional robustness, outperforming the best unvaccinated model by up to 32.85\\%. %\n{\nMoreover, models combining the identified design with low-rank approximation show up to 21.04\\% improvement in accuracy compared to GCN-SVD, which uses only low-rank approximation. \nModels combining the design with Soft Medoid and GDC show up to 37.29\\% improvement in accuracy compared to GCN-SMGDC. \nWe also note that the most robust method for each dataset is among the ones with the identified design. \nThese results again support the effectiveness of the heterophilous design in boosting the robustness of GNNs in addition to existing vaccination mechanisms.\n}\n\n\n\n\\textcircled{\\raisebox{-0.9pt}{2}}~\\textit{Evasion attacks}.\nWe present the performance under evasion attacks \non our \\githubrepo. \n\\textit{Unlike the poison attacks}, the evasion setup only leads to a slight decrease in average accuracy of less than 2\\% for most models. \nMoreover, there appears to be no clearly increased robustness for vaccinated models (with the identified design or other vaccination machanisms) compared to unvaccinated models. \nThis can be attributed to the reduced effectiveness of evasion vs.\\ poison attacks (as in \\nettack), and the increased challenges of untargeted attacks.\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "(Q3) Heterophily-adjusted GNNs are Certifiably More Robust",
                    "content": "\n\\label{sec:exp-cert-robustness}\n\n\nIt is worth noting that robustness against specific attacks such as \\nettack{} and Metattack does not guarantee robustness towards other possible attacks. \nTo overcome this limitation, \\emph{robustness certificates} provide guarantees (in some cases probabilistically) that attacks within a certain radius cannot change a model's predictions. \nComplementary to our evaluation on empirical robustness, we further demonstrate that heterophily-adjusted GNNs featuring our identified design are certifiably more robust than methods without it, thus answering (Q3).\nFor GNN models and datasets,\nwe exclude {the larger Pubmed dataset} and models that learn to rewrite the graph structure through the training process, or require recalculation of the low-rank approximation or inverse of matrices (as used by GDC~\\cite{klicpera_diffusion_2019}) for every randomized perturbation, as we find that sampling on {these setups} is computationally challenging. \nWe use the same hyperparameters as the benchmark study in \\S\\ref{sec:exp-benchmark-study}. \n\n\n\n\nTable~\\ref{table:real-results-cert-both} shows multiple metrics of certifiable robustness of each GNN model under edge randomization schemes allowing both addition and deletion, and allowing addition only; we additionally report results under a scheme allowing only deletion on our \\githubrepo. \nFor the scheme allowing both addition and deletion, we observe that all heterophily-adjusted methods have better certifiable robustness compared to methods without the design. \nSpecifically, on homophilous datasets (Cora and Citeseer), methods with the identified design achieve an up to 5.3 times relative improvement in accumulated certification.\nOn heterophilous datasets (FB100 and Snap), they outperform the baselines by a factor of 11.1. \nIn the more challenging case with the addition only scheme, methods with the design also show up to 2.9 times relative increase in AC on the homophilous datasets \nand 11.0 times relative increase in AC on the heterophilous datasets \ncompared to the baselines. \nFor the deletion only scheme, we find that unvaccinated models like GCN already have decent certifiable robustness in this scenario, commensurating with our discussions in \\S\\ref{sec:exp-perturb-observations}\nthat deletions create less severe perturbations.\nOverall, our results show that models featuring our identified design achieve significantly improved \\emph{certifiable robustness} compared to models lacking this design.\nHowever, like in our empirical robustness evaluation, architectural differences lead to some variability of robustness; the results also show tradeoffs between accuracy and robustness.\nWe also observe that the rankings under certifiable and empirical robustness are different, as in the previous results from \\cite{geisler2020reliable}; {we discuss more in our \\repo}.\n\n\n\\vspace{-0.4cm}\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\n\\label{sec:conclusion}\nWe formalized the relation between heterophily and adversarial structural attacks, and showed theoretically and empirically %\nthat effective attacks gravitate towards increasing heterophily in %\n{both homophilous and heterophilous graphs by leveraging low-degree (gambit) nodes}. %\nUsing these insights, we showed that a key design addressing heterophily, namely separate aggregators for ego- and neighbor-embeddings, can lead to competitive improvement on  empirical and certifiable robustness, with only small influence on clean performance.\nFinally, we compared the design with state-of-the-art vaccination mechanisms under different attack scenarios for various datasets, and illustrated \nthat they are complementary and that their combination can lead to more robust GNN models.\nWe note that while we focus on the structural attacks, GNNs are also vulnerable to other types of attacks such as feature perturbations.\n{We hope our analysis can inspire more effective defense strategies against adversarial attacks, especially designs that improve robustness by better addressing heterophily, such as heterophily in node features, or locally-occuring heterophily in homophilous graphs.}\n\n\n\n\n\n\n\n\n\n\\vspace{-0.2cm}\n"
            },
            "section 7": {
                "name": "Acknowledgments",
                "content": "\nThis material is based upon work supported by the National Science Foundation under CAREER Grant No.~IIS 1845491 and Medium grant, Army Young Investigator Award No.~W911NF1810397, an Adobe Digital Experience research faculty award, an Amazon faculty award, a Google faculty award, and AWS Cloud Credits for Research. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Quadro P6000 GPU used for this research. \nMTS received funding from the Ministry of Culture and Science (MKW) of the German State of North Rhine-Westphalia (``NRW R\\\"uckkehrprogramm'') and the Excellence Strategy of the Federal Government and the Länder.\nAny opinions, findings, and conclusions \nexpressed in this material are those of the authors and do not necessarily reflect the views of the funding parties.\n\n\\vspace{-0.3cm}\n\n\n\\bibliographystyle{iclr2022_conference}\n\\bibliography{BIB/abbreviations,BIB/ACM-abbreviations,BIB/main,BIB/all}\n\n\n\n\n\n\n\\appendix\n\\newpage\n\\twocolumn["
            },
            "section 8": {
                "name": "3cm",
                "content": "\n\\vspace{0.4cm}\n]\n\n\n    \n    \n    \n    \n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\\clearpage\n\\newpage\n\\onecolumn"
            },
            "section 9": {
                "name": "3.5cm",
                "content": "\n\\vspace{0.4cm}\n\n\n\n\n\n\n\n\n\n"
            }
        },
        "tables": {
            "table:dataset-stats": "\\begin{table}[t]\n    \\caption{Dataset statistics.}\n    \\label{table:dataset-stats}\n    \\vspace{-0.4cm}\n    \\resizebox{\\columnwidth}{!}\n    {%\n    \\begin{tabular}{Hl c  c  c  c  c  c}\n        \\toprule\n        & & \\multicolumn{3}{c}{\\bf Homophilous} && \\multicolumn{2}{c}{\\bf Heterophilous} \\\\\n        \\cmidrule{3-5} \\cmidrule{7-8}  \n        & & \\multicolumn{1}{c}{\\texttt{\\bf Cora}} & \\multicolumn{1}{c}{\\texttt{\\bf Pubmed}} & \\multicolumn{1}{c}{\\texttt{\\bf Citeseer}}  && \\multicolumn{1}{c}{\\texttt{\\bf FB100}} & \\multicolumn{1}{c}{\\texttt{\\bf Snap}} \\\\\n        \\midrule\n        & \\textbf{\\#Nodes} $|\\vertexSet|$ & \\multicolumn{1}{c}{2,485} & \\multicolumn{1}{c}{19,717} & \\multicolumn{1}{c}{2,110} && \\multicolumn{1}{c}{2,032} & \\multicolumn{1}{c}{4,562}\\\\\n        & \\textbf{\\#Edges}  $|\\edgeSet|$ & \\multicolumn{1}{c}{5,069} & \\multicolumn{1}{c}{44,324} & \\multicolumn{1}{c}{3,668} && \\multicolumn{1}{c}{78,733} & \\multicolumn{1}{c}{12,103}\\\\\n        & \\textbf{\\#Classes} $|\\setY|$ & \\multicolumn{1}{c}{7} & \\multicolumn{1}{c}{3} & \\multicolumn{1}{c}{6} && \\multicolumn{1}{c}{2} & \\multicolumn{1}{c}{5}\\\\\n        & \\textbf{\\#Features}  $F$ & \\multicolumn{1}{c}{1,433} & \\multicolumn{1}{c}{500} & \\multicolumn{1}{c}{3,703} && \\multicolumn{1}{c}{1,193} & \\multicolumn{1}{c}{269}\\\\\n        & \\textbf{Homophily} $h$ & \\multicolumn{1}{c}{0.804} & \\multicolumn{1}{c}{0.802} & \\multicolumn{1}{c}{0.736} && \\multicolumn{1}{c}{0.531} & \\multicolumn{1}{c}{0.134}  \\\\\n    \\bottomrule \\end{tabular}}\n    \\vspace{-0.6cm}\n\\end{table}",
            "tab:perturbation-stats": "\\begin{table}[t]\n    \\centering\n    \\caption{Effective targeted attacks by \\nettack~(\\S\\ref{sec:exp-perturb-observations}): ratios of edge additions, deletions and heterophilous attacks (i.e., attacks increasing heterophily). \n    We consider two heterophily definitions, one based on ground-truth class labels (Label), and the other on predicted class labels by GCN on clean datasets (Pred.).\n    All attacks are direct perturbations on edges incident to the targets.\n    Degrees of target and gambit nodes in the attacks are shown in Fig.~\\ref{fig:nettack-degree-scatter}. \n    All attacks introduce heterophilous edges that connect nodes with different \\emph{predicted} labels, following the takeaways of Thm.~\\ref{thm:1} and \\ref{thm:heterophily}. \n    }\n    \\label{tab:perturbation-stats}\n    \\ifdefmacro{\\ispreprint}{}{\\vspace{-0.15cm}}\n    \\resizebox{\\columnwidth}{!}{\n        \\begin{tabular}{l l l rr c rr}\n        \\toprule\n        \n        & \\multirow{2.5}{*}{\\textbf{Dataset}} \n        & \\multirow{2.5}{*}{\\textbf{\\shortstack[l]{Sample\\\\Sizes}}}\n        & \\multicolumn{2}{c}{\\textbf{Attack Type}} & \n        & \\multicolumn{2}{c}{\\textbf{Hete. Attacks}} \n        \\\\\n        \\cmidrule{4-5} \\cmidrule{7-8}\n        & & & Add. & Del. && Label & Pred. \\\\\n        \\midrule\n        \\multirow{4}{*}{\\rotatebox[origin=r]{90}{\\nettack}} \n        & Cora & 150 & 99.33\\% & 0.67\\% &  & 100.00\\% & 100.00\\%\\\\\n        & Pubmed & 153 & 100.00\\% & 0.00\\% &  & 100.00\\% & 100.00\\%\\\\\n        & Citeseer & 121 & 100.00\\% & 0.00\\% &  & 100.00\\% & 100.00\\%\\\\\n        \n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{2-8}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        & FB100 & 112 & 100.00\\% & 0.00\\% &  & 50.00\\% & 100.00\\%\\\\\n        & Snap & 51 & 100.00\\% & 0.00\\% &  & 64.71\\% & 100.00\\%\\\\\n        \n\n        \\bottomrule\n        \\end{tabular}\n    }\n    \\vspace{-0.01cm}\n\\end{table}",
            "table:real-results-poison-only": "\\begin{table*}[t]\n\n    \\centering\n    \\caption{Benchmark study: %\n    {mean accuracy $\\pm$ stdev against poison attacks, with accuracy on clean datasets in gray for reference}. Accuracy is reported on target nodes for \\nettack, and on full test splits for Metattack. Best GNN performance against attacks is highlighted in blue per dataset, and in gray per model group. MLP is immune to structural attacks and not considered as a GNN model. \n    Accuracy against evasion attacks are listed on our \\githubrepo{} (App. \\S\\ref{app:real-benchmark-results}), \n    and the setups in \\S\\ref{sec:exp-setups}. \n    Additional results on Pubmed are listed in App. Table~\\ref{table:results-pubmed}.\n    GNNs merely adopting this design achieve up to 40.00\\% improvement in accuracy against \\nettack{} compared to the best-performing unvaccinated model (GCN).\n    Additionally, methods combining this design \n    alongside explicit defense mechanisms (e.g., GraphSAGE-SVD) achieve further robustness improvement to the corresponding base mechanism without the design (e.g., GCN-SVD), and outperform the best vaccinated baseline by up to 18.33\\%.\n    }\n    \\label{table:real-results-poison-only}\n    \\ifdefmacro{\\ispreprint}{}{\\vspace{-0.15cm}}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{ll cc c>{\\color{dark-gray}}c c c>{\\color{dark-gray}}c c c>{\\color{dark-gray}}c c c>{\\color{dark-gray}}c c c>{\\color{dark-gray}}c c c>{\\color{dark-gray}}c c c>{\\color{dark-gray}}c c c>{\\color{dark-gray}}c}\n        \\toprule\n        &  &  &\n        \n        & \\multicolumn{11}{c}{\\textsc{ Nettack}}  \n        && \\multicolumn{11}{c}{\\texttt{\\bf Metattack}} \\\\ \\cmidrule{5-15} \\cmidrule{17-27}\n        \n        & \\multirow{3}{*}{\\rotatebox[origin=r]{90}{\\textbf{Hetero.}}} & \\multirow{3}{*}{\\rotatebox[origin=r]{90}{\\textbf{Vaccin.}}} &\n        \n        & \\multicolumn{2}{c}{\\texttt{\\bf Cora}} && \\multicolumn{2}{c}{\\texttt{\\bf Citeseer}} \n        && \\multicolumn{2}{c}{\\texttt{\\bf FB100}} && \\multicolumn{2}{c}{\\texttt{\\bf Snap}} \n        && \\multicolumn{2}{c}{\\texttt{\\bf Cora}} && \\multicolumn{2}{c}{\\texttt{\\bf Citeseer}} \n        && \\multicolumn{2}{c}{\\texttt{\\bf FB100}} && \\multicolumn{2}{c}{\\texttt{\\bf Snap}}\\\\\n        \n        & & & & \\multicolumn{2}{c}{$h$=0.804} && \\multicolumn{2}{c}{$h$=0.736} && \\multicolumn{2}{c}{$h$=0.531} && \\multicolumn{2}{c}{$h$=0.134} && \\multicolumn{2}{c}{$h$=0.804} && \\multicolumn{2}{c}{$h$=0.736} && \\multicolumn{2}{c}{$h$=0.531} && \\multicolumn{2}{c}{$h$=0.134}  \\\\\n        \n        \\cmidrule{5-6}  \\cmidrule{8-9}  \\cmidrule{11-12} \\cmidrule{14-15} \\cmidrule{17-18} \\cmidrule{20-21} \\cmidrule{23-24} \\cmidrule{26-27}\n        \n        & & & & \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Clean} && \n                \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Clean} &&\n                \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Clean} &&\n                \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Clean} &&\n                \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Clean} && \n                \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Clean} &&\n                \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Clean} &&\n                \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Clean} \\\\\n        \n        \n        \\cmidrule{1-3} \\cmidrule{5-6}  \\cmidrule{8-9}  \\cmidrule{11-12} \\cmidrule{14-15} \\cmidrule{17-18} \\cmidrule{20-21} \\cmidrule{23-24} \\cmidrule{26-27}\n        \\textbf{H$_2$GCN-SVD} & \\checkmark & \\checkmark  &    &   $\\underset{\\scriptscriptstyle{\\pm 2.72}}{70.00}$ &  $\\underset{\\scriptscriptstyle{\\pm 3.42}}{74.44}$ & &  $\\underset{\\scriptscriptstyle{\\pm 3.60}}{65.00}$ &            $\\underset{\\scriptscriptstyle{\\pm 2.72}}{70.00}$ &\n        \n        &  $\\underset{\\scriptscriptstyle{\\pm 3.42}}{59.44}$ & $\\underset{\\scriptscriptstyle{\\pm 2.36}}{61.67}$& & \\cellcolor{blue!20} $\\underset{\\scriptscriptstyle{\\pm 3.42}}{28.89}$ &                $\\underset{\\scriptscriptstyle{\\pm 2.08}}{30.56}$ \n        \n        && $\\underset{\\scriptscriptstyle{\\pm 0.47}}{67.87}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.37}}{76.89}$& &        \\cellcolor{blue!20}$\\underset{\\scriptscriptstyle{\\pm 0.46}}{70.42}$ &      $\\underset{\\scriptscriptstyle{\\pm 1.03}}{73.42}$& &     \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.08}}{56.72}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.77}}{56.81}$& &                        $\\underset{\\scriptscriptstyle{\\pm 0.14}}{25.60}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.26}}{27.63}$\n        \n        \\\\\n\n        \\textbf{GraphSAGE-SVD} & \\checkmark & \\checkmark & & \\cellcolor{blue!20}$\\underset{\\scriptscriptstyle{\\pm 2.36}}{71.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.78}}{77.22}$& &    \\cellcolor{blue!20}$\\underset{\\scriptscriptstyle{\\pm 3.42}}{67.78}$ &  $\\underset{\\scriptscriptstyle{\\pm 1.36}}{70.00}$& &   \\cellcolor{blue!20}$\\underset{\\scriptscriptstyle{\\pm 1.36}}{60.00}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.08}}{60.00}$& &                    $\\underset{\\scriptscriptstyle{\\pm 6.80}}{26.67}$ &                   $\\underset{\\scriptscriptstyle{\\pm 5.50}}{27.22}$ \n        \n        && \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 1.32}}{68.86}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.29}}{77.52}$& &        $\\underset{\\scriptscriptstyle{\\pm 0.52}}{69.10}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.17}}{72.16}$& &     $\\underset{\\scriptscriptstyle{\\pm 0.33}}{55.76}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.86}}{57.38}$& &                        \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.30}}{26.58}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.70}}{26.72}$\n        \n        \\\\\n        \n        \\textbf{H$_2$GCN-SMGDC} & \\checkmark & \\checkmark & & $\\underset{\\scriptscriptstyle{\\pm 4.37}}{59.44}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.78}}{77.22}$& &    $\\underset{\\scriptscriptstyle{\\pm 3.60}}{43.33}$ &  $\\underset{\\scriptscriptstyle{\\pm 1.57}}{67.22}$& &   $\\underset{\\scriptscriptstyle{\\pm 1.57}}{47.22}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.00}}{61.67}$& &                    $\\underset{\\scriptscriptstyle{\\pm 1.57}}{22.22}$ &                   $\\underset{\\scriptscriptstyle{\\pm 0.79}}{30.56}$ \n        \n        && $\\underset{\\scriptscriptstyle{\\pm 1.65}}{66.50}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.33}}{80.60}$& &        $\\underset{\\scriptscriptstyle{\\pm 1.24}}{69.04}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.92}}{74.31}$& &     $\\underset{\\scriptscriptstyle{\\pm 1.51}}{54.63}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.10}}{56.52}$& &                        $\\underset{\\scriptscriptstyle{\\pm 1.09}}{24.41}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.62}}{27.50}$\n        \n        \\\\\n        \n        \\textbf{GraphSAGE-SMGDC} & \\checkmark & \\checkmark & & $\\underset{\\scriptscriptstyle{\\pm 8.28}}{56.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 5.44}}{78.33}$& &    $\\underset{\\scriptscriptstyle{\\pm 3.60}}{46.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 2.83}}{67.78}$& &   $\\underset{\\scriptscriptstyle{\\pm 4.16}}{47.22}$ &  $\\underset{\\scriptscriptstyle{\\pm 1.57}}{59.44}$& &                    $\\underset{\\scriptscriptstyle{\\pm 3.14}}{20.56}$ &                   $\\underset{\\scriptscriptstyle{\\pm 4.16}}{29.44}$ \n        \n        && $\\underset{\\scriptscriptstyle{\\pm 2.07}}{66.95}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.26}}{79.39}$& &        $\\underset{\\scriptscriptstyle{\\pm 0.97}}{68.68}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.38}}{74.31}$& &     $\\underset{\\scriptscriptstyle{\\pm 0.29}}{55.39}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.19}}{55.19}$& &                        $\\underset{\\scriptscriptstyle{\\pm 0.76}}{25.21}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.29}}{26.38}$\n        \n        \\\\\n\n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\n        \\cdashline{5-15}[0.8pt/2pt]\n        \\cdashline{17-27}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{H$_2$GCN} & \\checkmark &         & &  $\\underset{\\scriptscriptstyle{\\pm 5.50}}{38.89}$ &  $\\underset{\\scriptscriptstyle{\\pm 8.31}}{82.78}$& &    $\\underset{\\scriptscriptstyle{\\pm 1.57}}{27.22}$ &  $\\underset{\\scriptscriptstyle{\\pm 6.98}}{69.44}$& &   $\\underset{\\scriptscriptstyle{\\pm 3.42}}{27.78}$ &  $\\underset{\\scriptscriptstyle{\\pm 1.57}}{60.56}$& &                    $\\underset{\\scriptscriptstyle{\\pm 2.83}}{12.78}$ &                   $\\underset{\\scriptscriptstyle{\\pm 2.72}}{30.00}$ && \n        \n        $\\underset{\\scriptscriptstyle{\\pm 6.61}}{57.75}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.97}}{83.94}$& &        $\\underset{\\scriptscriptstyle{\\pm 0.82}}{54.34}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.90}}{75.34}$& &     $\\underset{\\scriptscriptstyle{\\pm 0.76}}{54.84}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.13}}{56.95}$& &                        $\\underset{\\scriptscriptstyle{\\pm 0.59}}{25.34}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.05}}{27.49}$\n        \n        \\\\\n        \n        \\textbf{GraphSAGE} & \\checkmark &     & & $\\underset{\\scriptscriptstyle{\\pm 2.72}}{36.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 9.56}}{82.22}$& &   $\\underset{\\scriptscriptstyle{\\pm 10.89}}{31.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 6.85}}{70.56}$& &   $\\underset{\\scriptscriptstyle{\\pm 3.42}}{33.89}$ &  $\\underset{\\scriptscriptstyle{\\pm 2.72}}{60.00}$& &                    $\\underset{\\scriptscriptstyle{\\pm 7.07}}{16.67}$ &                   $\\underset{\\scriptscriptstyle{\\pm 4.16}}{24.44}$ \n        \n        && $\\underset{\\scriptscriptstyle{\\pm 2.56}}{54.68}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.63}}{82.21}$& &        $\\underset{\\scriptscriptstyle{\\pm 1.74}}{59.74}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.93}}{74.64}$& &     $\\underset{\\scriptscriptstyle{\\pm 0.83}}{54.72}$ &   $\\underset{\\scriptscriptstyle{\\pm 1.40}}{56.60}$& &                        $\\underset{\\scriptscriptstyle{\\pm 0.76}}{24.14}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.84}}{27.18}$\n        \\\\\n        \n        \\textbf{CPGNN} & \\checkmark &         && $\\underset{\\scriptscriptstyle{\\pm 6.14}}{47.22}$ &  $\\underset{\\scriptscriptstyle{\\pm 8.28}}{81.67}$& &    $\\underset{\\scriptscriptstyle{\\pm 9.65}}{40.56}$ &  $\\underset{\\scriptscriptstyle{\\pm 1.36}}{73.33}$& &  \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 10.30}}{49.44}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.16}}{66.11}$& &                    $\\underset{\\scriptscriptstyle{\\pm 2.72}}{21.67}$ &                   $\\underset{\\scriptscriptstyle{\\pm 5.50}}{28.89}$ \n        \n        && \\cellcolor{blue!20}$\\underset{\\scriptscriptstyle{\\pm 1.23}}{74.55}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.51}}{80.67}$& &        \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 1.93}}{68.07}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.62}}{74.92}$& &     \\cellcolor{blue!20}$\\underset{\\scriptscriptstyle{\\pm 1.50}}{61.58}$ &   $\\underset{\\scriptscriptstyle{\\pm 7.09}}{60.17}$& &                        $\\underset{\\scriptscriptstyle{\\pm 0.41}}{26.76}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.63}}{27.13}$\n        \n        \\\\\n        \n        \\textbf{GPR-GNN} & \\checkmark & & & $\\underset{\\scriptscriptstyle{\\pm 2.72}}{21.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 7.49}}{82.22}$& &    $\\underset{\\scriptscriptstyle{\\pm 2.08}}{24.44}$ &  $\\underset{\\scriptscriptstyle{\\pm 2.08}}{67.78}$& &    $\\underset{\\scriptscriptstyle{\\pm 0.79}}{2.78}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.91}}{56.67}$& &                     $\\underset{\\scriptscriptstyle{\\pm 2.08}}{4.44}$ &                   $\\underset{\\scriptscriptstyle{\\pm 3.42}}{27.78}$ \n        \n        && $\\underset{\\scriptscriptstyle{\\pm 5.23}}{48.29}$ &  $\\underset{\\scriptscriptstyle{\\pm 1.75}}{81.84}$& &        $\\underset{\\scriptscriptstyle{\\pm 2.77}}{35.25}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.46}}{70.71}$& &     $\\underset{\\scriptscriptstyle{\\pm 0.60}}{59.94}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.83}}{62.40}$& &                        $\\underset{\\scriptscriptstyle{\\pm 1.29}}{21.06}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.31}}{26.08}$\n        \n        \\\\\n        \n        \\textbf{FAGCN} & \\checkmark & & &\n        $\\underset{\\scriptscriptstyle{\\pm 6.14}}{26.11}$ &  $\\underset{\\scriptscriptstyle{\\pm 8.16}}{83.33}$& &    $\\underset{\\scriptscriptstyle{\\pm 6.43}}{25.56}$ &  $\\underset{\\scriptscriptstyle{\\pm 5.15}}{70.56}$& &    $\\underset{\\scriptscriptstyle{\\pm 2.83}}{6.11}$ &  $\\underset{\\scriptscriptstyle{\\pm 5.93}}{58.33}$& &                     $\\underset{\\scriptscriptstyle{\\pm 3.60}}{8.33}$ &                   $\\underset{\\scriptscriptstyle{\\pm 0.79}}{29.44}$ \n        \n        && $\\underset{\\scriptscriptstyle{\\pm 4.82}}{60.11}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.82}}{81.59}$& &        $\\underset{\\scriptscriptstyle{\\pm 6.00}}{53.18}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.63}}{73.99}$& &     $\\underset{\\scriptscriptstyle{\\pm 1.81}}{55.97}$ &   $\\underset{\\scriptscriptstyle{\\pm 1.38}}{59.64}$& &                        $\\underset{\\scriptscriptstyle{\\pm 0.62}}{24.04}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.23}}{27.15}$\n        \n        \\\\\n        \n        \\textbf{APPNP} & \\checkmark & & & \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 3.60}}{58.33}$ &  $\\underset{\\scriptscriptstyle{\\pm 5.50}}{72.22}$& &    \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 3.14}}{56.11}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.71}}{68.33}$& &   $\\underset{\\scriptscriptstyle{\\pm 2.36}}{36.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 3.93}}{58.89}$& &                    \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 1.36}}{25.00}$ &                   $\\underset{\\scriptscriptstyle{\\pm 2.36}}{28.33}$ \n        \n        && $\\underset{\\scriptscriptstyle{\\pm 0.91}}{62.56}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.38}}{72.87}$& &        $\\underset{\\scriptscriptstyle{\\pm 1.73}}{49.70}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.23}}{69.59}$& &     $\\underset{\\scriptscriptstyle{\\pm 0.35}}{57.81}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.59}}{57.89}$& &                        \\cellcolor{blue!20}$\\underset{\\scriptscriptstyle{\\pm 0.29}}{27.76}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.11}}{27.41}$\n        \n        \\\\\n        \n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\n        \\cdashline{5-15}[0.8pt/2pt]\n        \\cdashline{17-27}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{GNNGuard} & &\\checkmark        & & \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 1.36}}{58.33}$ &  $\\underset{\\scriptscriptstyle{\\pm 6.29}}{77.22}$& &    \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 3.14}}{59.44}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.78}}{67.78}$& &    $\\underset{\\scriptscriptstyle{\\pm 0.79}}{0.56}$ &  $\\underset{\\scriptscriptstyle{\\pm 2.08}}{67.22}$& &                     $\\underset{\\scriptscriptstyle{\\pm 1.57}}{9.44}$ &                   $\\underset{\\scriptscriptstyle{\\pm 3.60}}{28.33}$ \n        \n        &&\\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.55}}{74.20}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.55}}{80.15}$& &        \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.74}}{68.13}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.28}}{72.61}$& &     \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.48}}{60.89}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.60}}{65.66}$& &                        $\\underset{\\scriptscriptstyle{\\pm 0.67}}{23.78}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.98}}{26.51}$ \n        \\\\\n        \n        \\textbf{ProGNN} & &\\checkmark          &&  $\\underset{\\scriptscriptstyle{\\pm 7.97}}{48.89}$ &  $\\underset{\\scriptscriptstyle{\\pm 3.42}}{79.44}$& &    $\\underset{\\scriptscriptstyle{\\pm 7.49}}{32.78}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.78}}{67.22}$& &   $\\underset{\\scriptscriptstyle{\\pm 4.78}}{33.89}$ &  $\\underset{\\scriptscriptstyle{\\pm 3.93}}{51.11}$& &                    $\\underset{\\scriptscriptstyle{\\pm 9.26}}{17.78}$ &                   $\\underset{\\scriptscriptstyle{\\pm 5.50}}{27.22}$ \n        &&\n        \n        $\\underset{\\scriptscriptstyle{\\pm 6.20}}{45.10}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.43}}{81.32}$& &        $\\underset{\\scriptscriptstyle{\\pm 1.02}}{46.58}$ &      $\\underset{\\scriptscriptstyle{\\pm 1.12}}{71.82}$& &     $\\underset{\\scriptscriptstyle{\\pm 1.19}}{53.40}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.03}}{49.84}$& &                        $\\underset{\\scriptscriptstyle{\\pm 1.09}}{24.80}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.66}}{27.49}$\n        \n        \\\\\n        \n        \\textbf{GCN-SVD} & &\\checkmark         & & $\\underset{\\scriptscriptstyle{\\pm 4.91}}{53.33}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.16}}{75.56}$& &    $\\underset{\\scriptscriptstyle{\\pm 2.08}}{28.89}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.79}}{59.44}$& &   \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 2.36}}{41.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 4.37}}{50.56}$& &                    \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 5.44}}{25.00}$ &                   $\\underset{\\scriptscriptstyle{\\pm 6.71}}{27.78}$ &&\n        \n        $\\underset{\\scriptscriptstyle{\\pm 7.59}}{47.82}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.31}}{76.61}$& &        $\\underset{\\scriptscriptstyle{\\pm 1.78}}{51.20}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.16}}{66.90}$& &     $\\underset{\\scriptscriptstyle{\\pm 2.06}}{55.00}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.23}}{55.47}$& &                        \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.91}}{25.25}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.25}}{26.63}$\n        \\\\\n        \n        \\textbf{GCN-SMGDC} & & \\checkmark     && $\\underset{\\scriptscriptstyle{\\pm 4.91}}{40.00}$ &  $\\underset{\\scriptscriptstyle{\\pm 3.93}}{77.78}$& &    $\\underset{\\scriptscriptstyle{\\pm 2.83}}{33.89}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.79}}{62.22}$& &   $\\underset{\\scriptscriptstyle{\\pm 4.08}}{16.67}$ &  $\\underset{\\scriptscriptstyle{\\pm 5.67}}{51.11}$& &                    $\\underset{\\scriptscriptstyle{\\pm 5.15}}{20.56}$ &                   $\\underset{\\scriptscriptstyle{\\pm 2.36}}{28.33}$      &&\n        \n        $\\underset{\\scriptscriptstyle{\\pm 1.18}}{29.66}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.52}}{77.26}$& &        $\\underset{\\scriptscriptstyle{\\pm 2.36}}{55.04}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.59}}{72.33}$& &     $\\underset{\\scriptscriptstyle{\\pm 1.19}}{50.76}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.30}}{51.99}$& &                        $\\underset{\\scriptscriptstyle{\\pm 1.21}}{24.71}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.60}}{26.06}$\n        \n        \\\\\n\n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\n        \\cdashline{5-15}[0.8pt/2pt]\n        \\cdashline{17-27}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{GAT} & &             & & $\\underset{\\scriptscriptstyle{\\pm 0.79}}{13.89}$ &  $\\underset{\\scriptscriptstyle{\\pm 3.42}}{84.44}$& &     $\\underset{\\scriptscriptstyle{\\pm 3.42}}{8.89}$ &  $\\underset{\\scriptscriptstyle{\\pm 7.20}}{70.00}$& &    \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.79}}{0.56}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.79}}{60.56}$& &                     \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 4.37}}{3.89}$ &                   $\\underset{\\scriptscriptstyle{\\pm 2.83}}{30.56}$ \n        &&\n        \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 3.60}}{41.70}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.24}}{83.72}$& &        $\\underset{\\scriptscriptstyle{\\pm 2.17}}{48.40}$ &      $\\underset{\\scriptscriptstyle{\\pm 1.00}}{73.40}$& &     $\\underset{\\scriptscriptstyle{\\pm 0.66}}{50.37}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.92}}{61.69}$& &                        \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.73}}{25.00}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.03}}{27.30}$\n        \n        \\\\\n        \n        \\textbf{GCN} & &           &  & \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 3.60}}{18.33}$ &  $\\underset{\\scriptscriptstyle{\\pm 5.50}}{82.78}$& &    \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 5.50}}{20.56}$ &  $\\underset{\\scriptscriptstyle{\\pm 8.20}}{72.78}$& &    $\\underset{\\scriptscriptstyle{\\pm 0.00}}{0.00}$ &  $\\underset{\\scriptscriptstyle{\\pm 7.97}}{56.11}$& &                     $\\underset{\\scriptscriptstyle{\\pm 3.14}}{2.22}$ &                   $\\underset{\\scriptscriptstyle{\\pm 2.08}}{30.56}$ \n        &&\n        $\\underset{\\scriptscriptstyle{\\pm 4.83}}{31.98}$ &  $\\underset{\\scriptscriptstyle{\\pm 0.96}}{83.12}$& &        \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 2.52}}{49.43}$ &      $\\underset{\\scriptscriptstyle{\\pm 1.05}}{75.30}$& &     \\cellcolor{gray!20}$\\underset{\\scriptscriptstyle{\\pm 0.25}}{52.62}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.13}}{54.20}$& &                        $\\underset{\\scriptscriptstyle{\\pm 0.63}}{24.36}$ &                       $\\underset{\\scriptscriptstyle{\\pm 0.13}}{26.68}$\n        \n        \\\\\n    \n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\n        \\cdashline{5-15}[0.8pt/2pt]\n        \\cdashline{17-27}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{MLP}* & &             &\n       &  $\\underset{\\scriptscriptstyle{\\pm 3.42}}{64.44}$ &  $\\underset{\\scriptscriptstyle{\\pm 3.42}}{64.44}$& &    $\\underset{\\scriptscriptstyle{\\pm 3.42}}{\\textbf{70.56}}$ &  $\\underset{\\scriptscriptstyle{\\pm 3.42}}{70.56}$& &   $\\underset{\\scriptscriptstyle{\\pm 2.83}}{57.78}$ &  $\\underset{\\scriptscriptstyle{\\pm 2.83}}{57.78}$& &                    $\\underset{\\scriptscriptstyle{\\pm 2.72}}{\\textbf{30.00}}$ &                   $\\underset{\\scriptscriptstyle{\\pm 2.72}}{30.00}$ \n        \n        & %\n        \n        &$\\underset{\\scriptscriptstyle{\\pm 1.58}}{64.55}$ &  $\\underset{\\scriptscriptstyle{\\pm 1.58}}{64.55}$& &        $\\underset{\\scriptscriptstyle{\\pm 0.11}}{67.67}$ &      $\\underset{\\scriptscriptstyle{\\pm 0.11}}{67.67}$& &     $\\underset{\\scriptscriptstyle{\\pm 0.58}}{56.56}$ &   $\\underset{\\scriptscriptstyle{\\pm 0.58}}{56.56}$& &                        $\\underset{\\scriptscriptstyle{\\pm 1.05}}{26.25}$ &                       $\\underset{\\scriptscriptstyle{\\pm 1.05}}{26.25}$\\\\\n        \\bottomrule \n        \\end{tabular}}\n    \\vspace{-0.45cm}\n\\end{table*}",
            "table:real-results-cert-both": "\\begin{table*}[t]\n    \\centering\n    \\caption{Accumulated certifications (AC), average certifiable radii ($\\bar{r}_a$ and $\\bar{r}_d$) and accuracy of GNNs with randomized smoothing enabled (i.e., $f(\\phi(\\mathbf{s}))$) on the test splits of the clean datasets, with ramdomization schemes $\\phi$ allowing both addition and deletion (i.e., $p_+ = 0.001, p_- = 0.4$), and additional only (i.e., $p_+ = 0.001, p_- = 0$). For each statistic, we report the mean and stdev across 3 runs. Best results highlighted in blue per dataset, and in gray per model group. \n    We provide results with the deletion only scheme on our \\repo{} (App. \\S\\ref{app:real-cert-results}).\n    APPNP, with the identified design, improves the accumulated certification (AC) by up to 5.3x on homophilous datasets and 10.1x on heterophilous ones compared to the best performing baseline without the design.\n    }\n    \\vspace{-0.3cm}\n    \\label{table:real-results-cert-both}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{l lH c rrr>{\\color{dark-gray}}r c rrH>{\\color{dark-gray}}r c rrr>{\\color{dark-gray}}r c rrH>{\\color{dark-gray}}r}\n        \\toprule\n        & \\multirow{2}{*}{\\rotatebox[origin=r]{90}{\\textbf{Hete.}}} & \\multirow{3}{*}{\\rotatebox[origin=r]{90}{\\textbf{Vaccin.}}} &\n        \n        & \\multicolumn{4}{c}{\\bf Addition \\& Deletion} & \n        & \\multicolumn{4}{c}{\\bf Addition Only } &\n        & \\multicolumn{4}{c}{\\bf Addition \\& Deletion} &\n        & \\multicolumn{4}{c}{\\bf Addition Only} \n        \\\\\n        \\cmidrule{5-8} \\cmidrule{10-13} \\cmidrule{15-18} \\cmidrule{20-23}\n        \n        & & & & \n        \\multicolumn{1}{c}{AC}  & \\multicolumn{1}{c}{$\\bar{r}_a$} & \\multicolumn{1}{c}{$\\bar{r}_d$} & \\multicolumn{1}{c}{Acc. \\%} & & \n        \\multicolumn{1}{c}{AC}  & \\multicolumn{1}{c}{$\\bar{r}_a$} &  & \\multicolumn{1}{c}{Acc. \\%} & & \n        \\multicolumn{1}{c}{AC}  & \\multicolumn{1}{c}{$\\bar{r}_a$} & \\multicolumn{1}{c}{$\\bar{r}_d$} & \\multicolumn{1}{c}{Acc. \\%} & & \n        \\multicolumn{1}{c}{AC}  & \\multicolumn{1}{c}{$\\bar{r}_a$} &  & \\multicolumn{1}{c}{Acc. \\%} \n        \\\\\n        \n        \n        \\cmidrule{1-3} \\cmidrule{5-8} \\cmidrule{10-13} \\cmidrule{15-18} \\cmidrule{20-23}\n        \n        \\textbf{H$_2$GCN} & \\checkmark & &\n        \\multirow{9}{*}{\\rotatebox[origin=c]{90}{\\textbf{Cora}}}\n        & 3.96\\tiny{$\\pm$0.33} & 0.46\\tiny{$\\pm$0.08} & 3.90\\tiny{$\\pm$0.30} & 79.34\\tiny{$\\pm$1.93} & \n        & 0.42\\tiny{$\\pm$0.02} & 0.53\\tiny{$\\pm$0.03} & - & 80.97\\tiny{$\\pm$1.95} & \n        \\multirow{9}{*}{\\rotatebox[origin=c]{90}{\\textbf{Citeseer}}}\n        & 2.96\\tiny{$\\pm$0.88} & 0.33\\tiny{$\\pm$0.13} & 3.27\\tiny{$\\pm$0.67} & 71.76\\tiny{$\\pm$4.05} & \n        & 0.29\\tiny{$\\pm$0.05} & 0.40\\tiny{$\\pm$0.06} & - & 72.99\\tiny{$\\pm$2.22} \\\\\n\n        \\textbf{GraphSAGE} & \\checkmark & &\n        & 2.16\\tiny{$\\pm$0.06} & 0.13\\tiny{$\\pm$0.00} & 2.43\\tiny{$\\pm$0.03} & 79.61\\tiny{$\\pm$1.48} & \n        & 0.28\\tiny{$\\pm$0.03} & 0.34\\tiny{$\\pm$0.04} & - & 81.07\\tiny{$\\pm$1.11} & \n        & 2.21\\tiny{$\\pm$0.15} & 0.19\\tiny{$\\pm$0.01} & 2.56\\tiny{$\\pm$0.09} & 73.48\\tiny{$\\pm$2.90} & \n        & 0.33\\tiny{$\\pm$0.01} & 0.44\\tiny{$\\pm$0.01} & - & 74.70\\tiny{$\\pm$1.37} \\\\\n\n        \\textbf{CPGNN} & \\checkmark & &\n        & 1.87\\tiny{$\\pm$0.27} & 0.14\\tiny{$\\pm$0.05} & 2.24\\tiny{$\\pm$0.30} & 75.37\\tiny{$\\pm$1.65} & \n        & 0.17\\tiny{$\\pm$0.02} & 0.21\\tiny{$\\pm$0.03} & - & 78.34\\tiny{$\\pm$1.26} & \n        & 2.03\\tiny{$\\pm$0.17} & 0.11\\tiny{$\\pm$0.01} & 2.52\\tiny{$\\pm$0.20} & 73.48\\tiny{$\\pm$0.61} & \n        & 0.15\\tiny{$\\pm$0.02} & 0.20\\tiny{$\\pm$0.02} & - & 74.62\\tiny{$\\pm$0.30} \\\\\n\n        \\textbf{GPR-GNN} & \\checkmark & &\n        & 4.42\\tiny{$\\pm$0.43} & 0.63\\tiny{$\\pm$0.06} & 4.35\\tiny{$\\pm$0.22} & 74.90\\tiny{$\\pm$2.34} & \n        & 0.43\\tiny{$\\pm$0.03} & 0.55\\tiny{$\\pm$0.03} & - & 76.96\\tiny{$\\pm$2.18} & \n        & 4.63\\tiny{$\\pm$0.27} & 0.81\\tiny{$\\pm$0.07} & 4.92\\tiny{$\\pm$0.24} & 66.33\\tiny{$\\pm$0.20} & \n        & 0.40\\tiny{$\\pm$0.01} & 0.59\\tiny{$\\pm$0.02} & - & 67.52\\tiny{$\\pm$0.49} \\\\\n\n        \\textbf{FAGCN} & \\checkmark & &\n        & 4.30\\tiny{$\\pm$0.07} & 0.57\\tiny{$\\pm$0.02} & 4.25\\tiny{$\\pm$0.04} & 76.49\\tiny{$\\pm$1.73} & \n        & 0.43\\tiny{$\\pm$0.01} & 0.54\\tiny{$\\pm$0.01} & - & 79.04\\tiny{$\\pm$0.68} & \n        & 4.07\\tiny{$\\pm$0.15} & 0.58\\tiny{$\\pm$0.02} & 4.23\\tiny{$\\pm$0.09} & 71.82\\tiny{$\\pm$0.73} & \n        & 0.38\\tiny{$\\pm$0.02} & 0.53\\tiny{$\\pm$0.02} & - & 72.41\\tiny{$\\pm$1.03} \\\\\n\n        \\textbf{APPNP} & \\checkmark & &\n        & 10.11\\tiny{$\\pm$0.04}\\cellcolor{blue!20} & 1.86\\tiny{$\\pm$0.01}\\cellcolor{blue!20} & 8.52\\tiny{$\\pm$0.06}\\cellcolor{blue!20} & 71.97\\tiny{$\\pm$0.25} & \n        & 0.69\\tiny{$\\pm$0.00}\\cellcolor{blue!20} & 0.95\\tiny{$\\pm$0.00}\\cellcolor{blue!20} & - & 72.27\\tiny{$\\pm$0.31} & \n        & 9.87\\tiny{$\\pm$0.02}\\cellcolor{blue!20} & 1.88\\tiny{$\\pm$0.00}\\cellcolor{blue!20} & 8.61\\tiny{$\\pm$0.01}\\cellcolor{blue!20} & 69.39\\tiny{$\\pm$0.23} & \n        & 0.66\\tiny{$\\pm$0.00}\\cellcolor{blue!20} & 0.95\\tiny{$\\pm$0.00}\\cellcolor{blue!20} & - & 69.41\\tiny{$\\pm$0.22} \\\\\n\n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\n        \\cdashline{5-8}[0.8pt/2pt]\n        \\cdashline{10-13}[0.8pt/2pt]\n        \\cdashline{15-18}[0.8pt/2pt]\n        \\cdashline{20-23}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n\n        \\textbf{GAT} & & &\n        & \\cellcolor{gray!20}1.61\\tiny{$\\pm$0.10} & \\cellcolor{gray!20}0.08\\tiny{$\\pm$0.01} & \\cellcolor{gray!20}1.85\\tiny{$\\pm$0.06} & 79.83\\tiny{$\\pm$2.36} & \n        & \\cellcolor{gray!20}0.19\\tiny{$\\pm$0.04} & \\cellcolor{gray!20}0.23\\tiny{$\\pm$0.04} & - & 81.99\\tiny{$\\pm$1.94} & \n        & 1.29\\tiny{$\\pm$0.07} & 0.07\\tiny{$\\pm$0.02} & 1.60\\tiny{$\\pm$0.06} & 73.62\\tiny{$\\pm$1.06} & \n        & 0.09\\tiny{$\\pm$0.01} & 0.12\\tiny{$\\pm$0.02} & - & 74.47\\tiny{$\\pm$0.26} \\\\\n\n        \\textbf{GCN} & & &\n        & 1.40\\tiny{$\\pm$0.02} & 0.06\\tiny{$\\pm$0.01} & 1.75\\tiny{$\\pm$0.08} & 74.36\\tiny{$\\pm$3.46} & \n        & 0.13\\tiny{$\\pm$0.00} & 0.17\\tiny{$\\pm$0.01} & - & 78.17\\tiny{$\\pm$2.89} & \n        & \\cellcolor{gray!20}1.79\\tiny{$\\pm$0.04} & \\cellcolor{gray!20}0.17\\tiny{$\\pm$0.02} & \\cellcolor{gray!20}2.15\\tiny{$\\pm$0.11} & 70.38\\tiny{$\\pm$4.17} & \n        & \\cellcolor{gray!20}0.17\\tiny{$\\pm$0.01} & \\cellcolor{gray!20}0.24\\tiny{$\\pm$0.02} & - & 72.04\\tiny{$\\pm$3.64} \\\\\n        \n        \n        \\cmidrule{1-3} \\cmidrule{5-8} \\cmidrule{10-13} \\cmidrule{15-18} \\cmidrule{20-23}\n        \n        \\textbf{H$_2$GCN} & \\checkmark & &\n        \\multirow{9}{*}{\\rotatebox[origin=c]{90}{\\textbf{FB100}}}\n         & 8.12\\tiny{$\\pm$0.10} & 1.76\\tiny{$\\pm$0.02} & 8.14\\tiny{$\\pm$0.06} & 57.38\\tiny{$\\pm$0.17} & \n         & 0.54\\tiny{$\\pm$0.00} & 0.94\\tiny{$\\pm$0.00} & - & 57.11\\tiny{$\\pm$0.10} & \n        \\multirow{9}{*}{\\rotatebox[origin=c]{90}{\\textbf{Snap}}}\n         & 1.44\\tiny{$\\pm$0.18} & 0.59\\tiny{$\\pm$0.10} & 3.79\\tiny{$\\pm$0.40} & 26.97\\tiny{$\\pm$0.10} & \n         & 0.11\\tiny{$\\pm$0.01} & 0.42\\tiny{$\\pm$0.05} & - & 26.74\\tiny{$\\pm$0.18} \\\\\n        \n        \\textbf{GraphSAGE} & \\checkmark & &\n         & 6.98\\tiny{$\\pm$0.06} & 1.50\\tiny{$\\pm$0.04} & 7.32\\tiny{$\\pm$0.13} & 56.72\\tiny{$\\pm$1.56} & \n         & 0.52\\tiny{$\\pm$0.01} & 0.92\\tiny{$\\pm$0.01} & - & 56.70\\tiny{$\\pm$1.41} & \n         & 0.70\\tiny{$\\pm$0.21} & 0.19\\tiny{$\\pm$0.11} & 2.16\\tiny{$\\pm$0.54} & 26.84\\tiny{$\\pm$0.47} & \n         & 0.06\\tiny{$\\pm$0.02} & 0.24\\tiny{$\\pm$0.08} & - & 27.00\\tiny{$\\pm$0.63} \\\\\n        \n        \\textbf{CPGNN} & \\checkmark & &\n         & 6.80\\tiny{$\\pm$0.19} & 1.41\\tiny{$\\pm$0.21} & 7.05\\tiny{$\\pm$0.70} & 59.00\\tiny{$\\pm$5.71} & \n         & 0.54\\tiny{$\\pm$0.04} & 0.90\\tiny{$\\pm$0.04} & - & 60.39\\tiny{$\\pm$7.26} & \n         & 1.45\\tiny{$\\pm$0.23} & 0.61\\tiny{$\\pm$0.14} & 3.89\\tiny{$\\pm$0.51} & 26.71\\tiny{$\\pm$0.25} & \n         & 0.12\\tiny{$\\pm$0.02} & 0.43\\tiny{$\\pm$0.08} & - & 27.00\\tiny{$\\pm$0.41} \\\\\n        \n        \\textbf{GPR-GNN} & \\checkmark & &\n         & 5.81\\tiny{$\\pm$0.16} & 1.11\\tiny{$\\pm$0.02} & 5.95\\tiny{$\\pm$0.10} & 61.99\\tiny{$\\pm$0.44} & \n         & 0.46\\tiny{$\\pm$0.01} & 0.73\\tiny{$\\pm$0.02} & - & 62.26\\tiny{$\\pm$0.26} & \n         & 0.52\\tiny{$\\pm$0.06} & 0.11\\tiny{$\\pm$0.01} & 1.70\\tiny{$\\pm$0.14} & 26.31\\tiny{$\\pm$1.03} & \n         & 0.03\\tiny{$\\pm$0.01} & 0.11\\tiny{$\\pm$0.02} & - & 26.14\\tiny{$\\pm$0.73} \\\\\n        \n        \\textbf{FAGCN} & \\checkmark & &\n         & 7.45\\tiny{$\\pm$0.21} & 1.53\\tiny{$\\pm$0.02} & 7.40\\tiny{$\\pm$0.06} & 59.76\\tiny{$\\pm$1.47} & \n         & 0.55\\tiny{$\\pm$0.00} & 0.90\\tiny{$\\pm$0.01} & - & 60.60\\tiny{$\\pm$0.36} & \n         & 1.41\\tiny{$\\pm$0.10} & 0.56\\tiny{$\\pm$0.06} & 3.81\\tiny{$\\pm$0.22} & 27.07\\tiny{$\\pm$0.16} & \n         & 0.10\\tiny{$\\pm$0.01} & 0.36\\tiny{$\\pm$0.03} & - & 27.13\\tiny{$\\pm$0.16} \\\\\n        \n        \\textbf{APPNP} & \\checkmark & &\n         & 8.90\\tiny{$\\pm$0.03}\\cellcolor{blue!20} & 1.92\\tiny{$\\pm$0.02}\\cellcolor{blue!20} & 8.73\\tiny{$\\pm$0.05}\\cellcolor{blue!20} & 57.87\\tiny{$\\pm$0.57} & \n         & 0.57\\tiny{$\\pm$0.00}\\cellcolor{blue!20} & 0.98\\tiny{$\\pm$0.01}\\cellcolor{blue!20} & - & 57.89\\tiny{$\\pm$0.59} & \n         & 3.54\\tiny{$\\pm$0.03}\\cellcolor{blue!20} & 1.68\\tiny{$\\pm$0.01}\\cellcolor{blue!20} & 7.95\\tiny{$\\pm$0.04}\\cellcolor{blue!20} & 27.45\\tiny{$\\pm$0.14} & \n         & 0.24\\tiny{$\\pm$0.00}\\cellcolor{blue!20} & 0.86\\tiny{$\\pm$0.00}\\cellcolor{blue!20} & - & 27.46\\tiny{$\\pm$0.17} \\\\\n        \n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\n        \\cdashline{5-8}[0.8pt/2pt]\n        \\cdashline{10-13}[0.8pt/2pt]\n        \\cdashline{15-18}[0.8pt/2pt]\n        \\cdashline{20-23}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{GAT} & & &\n         & 4.30\\tiny{$\\pm$0.26} & 0.77\\tiny{$\\pm$0.04} & 4.72\\tiny{$\\pm$0.19} & 61.56\\tiny{$\\pm$0.78} & \n         & \\cellcolor{gray!20}0.46\\tiny{$\\pm$0.03} & 0.74\\tiny{$\\pm$0.04} & - & 61.97\\tiny{$\\pm$1.41} & \n         & 0.28\\tiny{$\\pm$0.09} & 0.04\\tiny{$\\pm$0.01} & 0.95\\tiny{$\\pm$0.33} & 27.12\\tiny{$\\pm$0.52} & \n         & \\cellcolor{gray!20}0.02\\tiny{$\\pm$0.00} & \\cellcolor{gray!20}0.08\\tiny{$\\pm$0.02} & - & 27.00\\tiny{$\\pm$0.59} \\\\\n        \n        \\textbf{GCN} & & &\n         & \\cellcolor{gray!20}5.19\\tiny{$\\pm$0.03} & \\cellcolor{gray!20}1.14\\tiny{$\\pm$0.00} & \\cellcolor{gray!20}6.05\\tiny{$\\pm$0.01} & 54.16\\tiny{$\\pm$0.08} & \n         & 0.43\\tiny{$\\pm$0.00} & \\cellcolor{gray!20}0.79\\tiny{$\\pm$0.01} & - & 54.39\\tiny{$\\pm$0.14} & \n         & \\cellcolor{gray!20}0.32\\tiny{$\\pm$0.08} & \\cellcolor{gray!20}0.06\\tiny{$\\pm$0.03} & \\cellcolor{gray!20}1.08\\tiny{$\\pm$0.24} & 26.17\\tiny{$\\pm$0.34} & \n         & \\cellcolor{gray!20}0.02\\tiny{$\\pm$0.01} & \\cellcolor{gray!20}0.08\\tiny{$\\pm$0.03} & - & 26.38\\tiny{$\\pm$0.49} \\\\        \n        \n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-0.4cm}\n\\end{table*}",
            "table:results-pubmed": "\\begin{table}[t]\n    \\centering\n    \\caption{\n        Additional results on Pubmed (details in App. \\S\\ref{app:real-benchmark-results}).\n    }\n    \\vspace{-0.35cm}\n    \\label{table:results-pubmed}\n    \\resizebox*{0.89\\linewidth}{!}{%\n    \\begin{tabular}{l ccc rr>{\\color{dark-gray}}r}\n        \\toprule\n        \n        & \\multirow{-0.9}{*}{\\rotatebox[origin=r]{90}{\\small{\\textbf{Hete.}}}} & \\multirow{-1.5}{*}{\\rotatebox[origin=r]{90}{\\small{\\textbf{Vaccin.}}}} &\n        \n        & \\multicolumn{3}{c}{\\texttt{\\bf Pubmed}} \\\\\n        \n        \\cmidrule{5-7}\n        \n        &&& & \\multicolumn{1}{c}{Poison} & \\multicolumn{1}{c}{Evasion} & \\multicolumn{1}{c}{Clean} \\\\\n        \n        \\cmidrule{1-3} \\cmidrule{5-7}\n        \n        \n        \\textbf{H$_2$GCN-SVD}       & \\checkmark & \\checkmark && \\cellcolor{blue!20}  86.11\\tiny{$\\pm 3.93$}\n        &\\cellcolor{blue!20} 86.11\\tiny{$\\pm 3.93$}&\n        87.22\\tiny{$\\pm 4.37$}\\\\\n        \n        \\textbf{GraphSAGE-SVD}   & \\checkmark &\\checkmark & & 81.11\\tiny{$\\pm 4.16$}& 81.11\\tiny{$\\pm 3.42$}\n        &84.44\\tiny{$\\pm 2.08$}\\\\\n  \n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\\cdashline{5-7}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{H$_2$GCN}           & \\checkmark & & & 44.44\\tiny{$\\pm 5.67$} &\n        46.67\\tiny{$\\pm 8.16$}& 87.78\\tiny{$\\pm 3.14$}\\\\\n        \n        \\textbf{GraphSAGE}      & \\checkmark & & & 33.33\\tiny{$\\pm 8.92$} &34.44\\tiny{$\\pm 9.06$} &84.44\\tiny{$\\pm 3.93$}\\\\\n        \n        \\textbf{CPGNN}       & \\checkmark & & &  60.00\\tiny{$\\pm 7.20$}&  60.00\\tiny{$\\pm 5.93$} & 82.78\\tiny{$\\pm 5.67$}\\\\\n        \n        \n        \\textbf{GPR-GNN}         & \\checkmark & & & 13.89\\tiny{$\\pm 4.78$} & 15.56\\tiny{$\\pm 6.14$}&85.56\\tiny{$\\pm 1.57$}\\\\\n        \n        \\textbf{FAGCN}           & \\checkmark & & &  27.78\\tiny{$\\pm 11.00$} &31.67\\tiny{$\\pm 13.40$}&86.67\\tiny{$\\pm 2.72$}\\\\\n        \n        \\textbf{APPNP}           & \\checkmark & & & \\cellcolor{gray!20} 79.44\\tiny{$\\pm 2.83$} &\\cellcolor{gray!20}81.67\\tiny{$\\pm 2.72$} & 86.67\\tiny{$\\pm 2.36$}\\\\\n        \n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\\cdashline{5-7}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{GNNGuard}        & & \\checkmark & & \\cellcolor{gray!20} 73.89\\tiny{$\\pm 6.71$}& \\multicolumn{1}{c}{-} &82.78\\tiny{$\\pm 2.83$}\\\\\n        \n        \n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\\cdashline{5-7}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{GAT}           &&&  &  \\cellcolor{gray!20}7.22\\tiny{$\\pm 4.16$}&\\cellcolor{gray!20} 6.67\\tiny{$\\pm 4.08$}&83.33\\tiny{$\\pm 1.36$}\\\\\n        \n        \\textbf{GCN}          &&&   &5.56\\tiny{$\\pm 0.79$} & 5.56\\tiny{$\\pm 0.79$} & 85.00\\tiny{$\\pm 2.72$}\\\\\n        \n        \\noalign{\\vskip 0.25ex}\n        \\cdashline{1-3}[0.8pt/2pt]\\cdashline{5-7}[0.8pt/2pt]\n        \\noalign{\\vskip 0.25ex}\n        \n        \\textbf{MLP*}        &&&  \\multirow{-14}{*}{\\rotatebox[origin=c]{90}{\\nettack}}   &  86.11\\tiny{$\\pm 4.37$}& 86.11\\tiny{$\\pm 4.37$}&86.11\\tiny{$\\pm 4.37$} \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-0.5cm}\n  \\end{table}"
        },
        "figures": {
            "fig:nettack-degree-scatter": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\columnwidth, keepaspectratio, trim={0 0 0 0.6cm}]{FIG/nettack-single-perturb-degree-scatter-homo.pdf}\\\\\n    \\vspace{0.2cm}\n    \\includegraphics[width=0.6\\columnwidth, keepaspectratio, trim={0 0 0 0.6cm}]{FIG/nettack-single-perturb-degree-scatter-hete.pdf}\n    \\vspace{-0.3cm}\n    \\caption{Scatter plots of the degrees of the target nodes (x-axis) and gambit nodes (y-axis) involved in the targeted attacks (\\S\\ref{sec:exp-perturb-observations}). Attacks tend to leverage gambit nodes with low degrees, \n    which makes attacks increasing heterophily effective for heterophilous graphs following Thm.~\\ref{thm:heterophily}. }\n    \\label{fig:nettack-degree-scatter}\n    \\vspace{-0.5cm}\n\\end{figure}",
            "fig:nettack_cora_FB100": "\\begin{figure}[t]\n    \\vspace{0.05cm}\n    \\begin{center}\n    \\includegraphics[width=\\linewidth,keepaspectratio]{FIG/KDD_nettack_citeseer_FB100_ver2.pdf}\n    \\end{center}\n    \\vspace{-0.5cm}\n    \\caption{(Best viewed in color.) Classification accuracy on clean data and against poison attacks for target nodes attacked by \\textsc{Nettack}. Error bars show standard deviation across different sets of experiments. Detailed results are listed in \n    Table \\ref{table:real-results-poison-only}. \n    As expected, MLP is not influenced by the adversarial structural attacks.}\n    \n    \\label{fig:nettack_cora_FB100}\n    \\ifdefmacro{\\ispreprint}{}{\\vspace{-0.5cm}}\n\\end{figure}"
        }
    }
}