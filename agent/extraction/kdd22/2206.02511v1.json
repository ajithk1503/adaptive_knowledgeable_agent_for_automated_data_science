{
    "meta_info": {
        "title": "Transfer Learning based Search Space Design for Hyperparameter Tuning",
        "abstract": "The tuning of hyperparameters becomes increasingly important as machine\nlearning (ML) models have been extensively applied in data mining applications.\nAmong various approaches, Bayesian optimization (BO) is a successful\nmethodology to tune hyper-parameters automatically. While traditional methods\noptimize each tuning task in isolation, there has been recent interest in\nspeeding up BO by transferring knowledge across previous tasks. In this work,\nwe introduce an automatic method to design the BO search space with the aid of\ntuning history from past tasks. This simple yet effective approach can be used\nto endow many existing BO methods with transfer learning capabilities. In\naddition, it enjoys the three advantages: universality, generality, and\nsafeness. The extensive experiments show that our approach considerably boosts\nBO by designing a promising and compact search space instead of using the\nentire space, and outperforms the state-of-the-arts on a wide range of\nbenchmarks, including machine learning and deep learning tuning tasks, and\nneural architecture search.",
        "author": "Yang Li, Yu Shen, Huaijun Jiang, Tianyi Bai, Wentao Zhang, Ce Zhang, Bin Cui",
        "link": "http://arxiv.org/abs/2206.02511v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "9 pages and 2 extra pages for appendix"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n\nThe performance of modern machine learning (ML) and data mining\nmethods highly depends on their hyperparameter configurations~\\cite{he2016deep, hinton2012deep,goodfellow2016deep,devlin2018bert}, e.g., learning rate, the number of hidden layers in a deep neural network, etc.\nAs a result, automatically tuning the hyperparameters has attracted lots of interest from both academia and industry~\\cite{quanming2018taking,bischl2021hyperparameter}.\nA large number of approaches have been proposed to automate this process, including random search~\\cite{bergstra2012random}, Bayesian optimization~\\cite{hutter2011sequential,bergstra2011algorithms,snoek2012practical}, evolutionary optimization~\\cite{hansen2016cma,real2019regularized} and bandit-based methods~\\cite{jamieson2016non,li2018hyperband,falkner2018bohb,li2021mfes,li2022hyper}.\n\nAmong various alternatives, Bayesian optimization (BO) is one of the most prevailing frameworks for automatic hyperparameter optimization (HPO)~\\cite{hutter2011sequential, bergstra2011algorithms, snoek2012practical,boreview}.\nThe main idea of BO is to use a surrogate model, typically a Gaussian Process (GP)~\\cite{rasmussen2004gaussian}, to model the relationship between a hyperparameter configuration and its performance (e.g., validation error), and then utilize this surrogate to guide the configuration search over the given hyperparameter space in an iterative manner. \n% Concretely, in each iteration, BO methods fit a surrogate on the observations, use this surrogate to select a promising configuration to evaluate, and augment observations after evaluating this configuration.\nHowever, with the rise of big data and deep learning (DL) techniques, the following two factors greatly hampers the efficiency of BO: (a) large search space and (b) computationally-expensive evaluation for each configuration.\n% Given a limited budget, BO methods with scarce observations often fail to converge to the optimal configuration, which we refer to as the ``low efficiency'' issue.\nGiven a limited budget, BO methods can obtain only a few observations over the large space.\nIn this case, BO fails to converge to the optimal configuration quickly, which we refer to as the low-efficiency issue~\\cite{falkner2018bohb,li2021volcanoml}.\n% Given a limited budget, few observations can be obtained, and it is insufficient for BO methods to fit an accurate surrogate that approximates the objective function well.\n% In this case, BO methods suffer from the ``low efficiency'' issue.\n\n\\iffalse\nHyperparameter search is one of the most cumbersome tasks in machine learning projects. \nThe complexity of deep learning\nmethod is growing with its popularity, and the framework of efficient automatic hyperparameter tuning is in higher demand\nthan ever.\n\\fi\n\n\n\n\n\n{\\bf (Opportunities) }\nTo address this issue, researchers in the HPO community propose to incorporate the spirit of transfer learning to accelerate hyperparameter tuning, which could borrow strength from the past tasks (source tasks) to accelerate the current task (target task).\nA line of work~\\cite{feurer2018scalable,perrone2018scalable,wistuba2016two} aims to learn surrogates with the aid of past tuning history, and the surrogates are utilized to guide the search of configurations.\nOrthogonal to these approaches, we concentrate on how to design a more promising and compact search space with the merit of transfer learning, and further speed up the HPO process.\nThe basic idea is that although the optimal configuration may be different for each HPO task, the region of well-performing hyperparameter configurations for the current task may share some similarity with previous HPO tasks due to the relevancy among tasks (See the left and middle heatmaps in Figure~\\ref{fig:intro_heatmap}).\nMotivated by the observation, in this paper, {\\em we focus on developing an automatic search space design method for BO, instead of using the entire and large search space.}\n\n{\\bf (Challenges) } \nHowever, designing the compact search space automatically is non-trivial. To fully unleash the potential of transfer learning for space design, we need to consider the following problems: (a) {\\em Representation of good/promising region}: The region of good configurations (promising region) in source tasks has uncertain shapes (See the deep red regions in Figure~\\ref{fig:intro_heatmap}); in this case, simple geometrical representations of the search space (e.g., bounding box or ellipsoid used in ~\\cite{pNIPS2019_9438}) cannot capture the shape of promising regions well. (b) {\\em Relevancy between tasks}: While sharing some correlation between tasks, the underlying objective surface may be quite different (See the middle and right heatmaps in Figure~\\ref{fig:intro_heatmap}). \nIf one ignores this diversity, the performance of transfer learning may be greatly hampered, where a loose search space is obtained, or the optimal configuration is dismissed from the generated search space, which further leads to the ``negative transfer'' issue~\\cite{pan2010survey}.\nSeveral HPO methods~\\cite{feurer2015initializing,wistuba2016two} measure the similarity scores between HPO tasks by computing the distance of meta-features for data set, while the meta-features in practice are often hard to obtain and need careful manual design~\\cite{feurer2018scalable,pNIPS2019_9438}.\nTherefore, we need to learn this similarity during the HPO process automatically.\n\nIn addition, (c) {\\em utilization of promising regions} is also a challenging problem. \nTo prevent negative transfer, the dissimilar tasks should hold large enough promising regions, so that the optimal configuration of the target task is not excluded.\nOn the contrary, given similar source tasks, a small promising region could greatly speed up the search of configuration in the target task.\nAs a result, the size of promising regions should be carefully crafted based on task relevancy.\nFinally, we need to leverage these promising regions to build a compact search space for the target task.\nRather than simple intuitions that use a single promising region, a mature design should utilize multiple source tasks.\nHowever, how to select and leverage those promising regions is still an open question.\n\n\nIn this paper, we propose a novel transfer learning-based search space design method for hyperparameter optimization.\nInstead of restricting the promising region to geometrical shapes, we propose to use a machine learning algorithm to learn the good region automatically. Concretely, we turn it into a supervised classification problem. \nIn addition, we develop a ranking-based method to measure the task correlation between the source and target task on the fly, and the size of promising regions could be adjusted adaptively based on this correlation.\nWith these basic ingredients, we first extract the promising region from each source task based on the task similarity, respectively, and then generate the final search space for the target task -- a sub-region of the complete search space -- via sampling-based framework along with a voting mechanism.\nIn this way, the proposed method could leverage the promising regions obtained from similar previous HPO tasks to craft a promising and compact search space for the current task automatically. \n\n\\iffalse\nTo evaluate our method, we create new {\\em large-scale} benchmarks that conduct hyperparameter optimization of ML algorithms on a wide range of HPO tasks. \nThe creation process takes more than 50k CPU and 2k GPU hours and involves more than 1.8 million model evaluations.\nThis benchmark is publicly available now (more details can be found in Section~\\ref{exp_sec}).\n\\fi\n\n{\\bf (Contributions) } \nWe summarize our main contributions as follows:\n(a) We present a novel transfer learning-based search space design method for hyperparameter optimization. Our method learns a suitable search space in an adaptive manner: it can endow BO algorithms with transfer learning capabilities.\n(b) On an extensive set of benchmarks, including tuning ML algorithm on OpenML problems, optimizing ResNet on three vision tasks, and conducting neural architecture search (NAS) on NASBench201, the empirical results demonstrate that our approach outperforms the existing method and significantly speeds up the HPO process.\n(c) Despite the empirical performance, our method enjoys the following three advantages: universality, practicality, and safeness. With these properties, our approach can be seamlessly combined with a wide range of existing HPO techniques.\n(d) We create and publish large-scale benchmarks for transfer learning of HPO, which are significantly larger than the existing ones. We hope this benchmark would help facilitate the research on search space design for HPO.\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n\\iffalse\n\\begin{itemize}\n    \\item HPO methods and systems.\n    \\item Transfer learning methods for HPO: focused on search algorithm.\n    \\item the closest baseline and search space related methods.\n    \\item our contribution: the cloest methods and novelty of our methods.\n\\end{itemize}\n\\fi\n\nHyperparameter optimization (HPO) is one of the most fundamental tasks when developing machine learning (ML) and data mining (DM) applications~\\cite{10.1145/3447548.3470827,bischl2021hyperparameter}. \nMany approaches have been proposed to conduct automatic HPO, such as random search~\\cite{bergstra2012random}, Bayesian optimization (BO)~\\cite{hutter2011sequential,bergstra2011algorithms}, bandit based method~\\cite{jamieson2016non,li2018hyperband,li2020efficient}, etc.\nIn addition, many tuning systems~\\cite{openbox,Amazon_SageMaker,oboe,autotune,li2022hyper} in the ML/DM community have successfully integrate the HPO algorithms.\n\nRecently, many researchers have proposed to utilize transfer learning within the BO framework to accelerate HPO.\nThe target is to leverage auxiliary knowledge acquired from the previous source tasks to achieve faster optimization on the target task, that is, borrow strength from past HPO tasks.\nOrthogonal to our contribution, one common way is to learn surrogate models from past tuning history and use them to guide the search of hyperparameters.\nFor instance, several methods learn all available information from both source and target tasks in a single surrogate, and make the data comparable through multi-task GPs~\\cite{swersky2013multi}, a ranking algorithm~\\cite{bardenet2013collaborative}, a mixed kernel GP~\\cite{yogatama2014efficient}, the GP noisy model~\\cite{joy2016flexible}, a multi-layer perceptron with Bayesian linear regression heads~\\cite{snoek2015scalable,perrone2018scalable}, a Gaussian copula process~\\cite{salinas2020quantile} or replace GP with Bayesian neural networks~\\cite{springenberg2016bayesian}.\nIn addition, several approaches train multiple base surrogates, and then combine all base surrogates into a single surrogate with dataset similarities~\\cite{wistuba2016two}, weights adjusted via GP uncertainties~\\cite{schilling2016scalable} or the weights estimated by rankings~\\cite{feurer2018scalable}. \nSimilarly, ~\\citet{golovin2017google} build a stack of GPs by iteratively regressing the residuals with the most recent source task.\nFinally, instead of fitting surrogates on the past observations, several approaches~\\cite{DBLP:journals/corr/abs-1709-04636,kim2017learning} achieve transfer learning in a different way. \nThey warm-start BO by selecting several initial hyperparameter configurations as the start points of search procedures to accelerate the searching process.\n\n%Bayesian optimization (BO) has been successfully applied to hyperparameter optimization (HPO).\n%For ML models that are computationally expensive to train (e.g., deep learning models or models trained on large datasets), BO methods~\\cite{hutter2011sequential,bergstra2011algorithms,snoek2012practical} suffer from the low-efficiency issue due to insufficient configuration evaluations within a limited budget.\n%To speed up HPO of ML algorithms with limited trials, recent BO methods extend the traditional black-box assumption by exploiting cheaper fidelities from the current task \\cite{klein2017fast, swersky2014freeze, kandasamy2017multi, klein2016learning, poloczek2017multi, falkner2018bohb}. Orthogonal to these methods, we focus on borrowing strength from previously finished (source) tasks to accelerate the HPO process of the current (target) task.\n\n%Transfer Learning (TL) methods for HPO aim to leverage auxiliary knowledge acquired from the source tasks to achieve faster optimization on the target task. \n%Previously many works have applied many different transfer learning methods for HPO. For example, \n%SGPR \\cite{golovin2017google} uses the predictions of the previous tasks as prior to the target task and iteratively fits the residuals of the current task relative to the predictions of the prior surrogate. \n%SMFO \\cite{wistuba2015sequential} uses a static ranking of hyperparameter combinations to choose the next promising configurations in a model-free way.\n%These two methods utilize the knowledge from all source tasks equally and suffer from performance deterioration when the knowledge of source tasks is not applicable to the target task.\n%FMLP \\cite{schilling2015hyperparameter} uses multi-layer perceptrons (MLP) as the surrogate model and improves them by using a factorization approach in the first layer to directly model all interactions between hyperparameters and datasets. \n\n%SCoT \\cite{bardenet2013collaborative} puts all observations from both source tasks and target task on the same scale by using a ranker, and then fits a GP-based surrogate on them.\n%MKL-GP \\cite{yogatama2014efficient} uses deviations from the per-task mean as the performance values, and still trains a single GP-based surrogate on all adjusted observations.\n%To distinguish the varied performance of the same configuration on different tasks, the two methods use the meta-features of datasets to represent the tasks;\n%while the meta-features are often unavailable for broad classes of HPO problems \\cite{feurer2018scalable, wistuba2016two}.\n%In addition, due to the high computational complexity of GP ($\\mathcal{O}(n^3)$ where $n$ is the number of observations), these methods suffer from \\emph{the scalability issue} --- it is difficult for them to scale to a large number of source tasks. \n\n% POGPE \\cite{schilling2016scalable} uses a product of GP experts as the transfer learning surrogate, where each GP-based expert is trained on the corresponding single task.\n% All above approaches ignore the fact that the knowledge in the target task is dynamically increasing rather than static, and thus transfer the same amount of knowledge from source tasks throughout the HPO process.\n% Two recent methods consider the two above issues by regrading the target task as a ``source'' task, and then give each source task with a weight.\n%To obtain better scalability, several methods adopt the one-phase framework to conduct TL for HPO, where they train a GP-based surrogate on each source task and the target task respectively, and then combine all base surrogates into an ensemble surrogate with different weighting strategies. \n%POGPE \\cite{schilling2016scalable} sets the weights of base surrogates to some constants.\n%TST \\cite{wistuba2016two} linearly combines the base surrogates with a Nadaraya-Watson kernel weighting by defining a distance metric across tasks; the weights are calculated by using either meta-features (TST-M) or pairwise hyperparameter configuration rankings (TST-R).\n%RGPE \\cite{feurer2018scalable} uses the probability that the base surrogate has the lowest ranking loss on the target task to estimate the weight for each base surrogate.\n%Since the ranking loss in TST and RGPE is not differentiable, they have to resort to heuristics to obtain sub-optimal weights.\n%Therefore, \\sys is the first method to learn the weights in a principled way.\n\n%Warm-starting methods~\\cite{DBLP:journals/corr/abs-1709-04636,kim2017learning} select several initial hyperparameter configurations as the start points of search procedures. \n%\\cite{NIPS2019_9438} designs the BO search space automatically by relying on observations from previous tasks.\n%While sharing some common spirits, these methods are orthogonal and complementary to TL methods for the re-optimization of hyperparameters.\n\nRecently, transferring search space has become another way for applying transfer learning in HPO. ~\\citet{wistuba2015hyperparameter} prune the bad regions of search space according to the results from previous tasks. \nThis method suffers from the complexity of obtaining meta-features and relies on some other parameters to construct a GP model. \nOn that basis, ~\\citet{pNIPS2019_9438} propose to utilize previous tasks to design a sub-region of the entire search space for the new task. \nHowever, this method ignores the similarity between HPO tasks, and applies a simple low-volume geometrical shape (bounding box or ellipsoid) to obtain the sub-region that contains the optimal configurations from all past tasks. \nTherefore, it may design a loose search space or exclude the best configurations and further lead to negative transfer~\\cite{pan2010survey}.\nThis work is the most closely related to our method. \nWe want to highlight that the other forms of transfer (e.g., surrogate-based transfer or warm-starting BO) are orthogonal to the search space transfer methods, and thus these approaches can be seamlessly combined to pursue better performance of HPO.\n"
            },
            "section 3": {
                "name": "Preliminary",
                "content": "\nIn this section, we first introduce the problem definition and then describe the basic framework of Bayesian optimization.\n\n",
                "subsection 3.1": {
                    "name": "HPO over a Reduced Search Space",
                    "content": "\n\\label{sec:reduced}\nThe HPO of ML algorithms can be modeled as a black-box optimization problem. \n% Consider a common hyperparameter space $\\mathcal{X}$ and $K$ previous (source) HPO tasks with underlying objective functions and a target task with objective function $f^T(\\cdot)$.\n% The former K tasks are finished source tasks, while the target task currently needs to be optimized.\nGiven a common hyperparameter space $\\mathcal{X}$ and tuning history from previous HPO tasks, we need to optimize the current tuning task.\nThe goal of this HPO problem is to find the best hyperparameter configuration that minimizes the objective function $f^T$ on the target task, which is formulated as follows,\n\\begin{equation}\n    \\mathop{\\arg\\min}_{\\bm{x} \\in \\mathcal{X}}f^T(\\bm{x}),\n    \\label{eq:original}\n\\end{equation}\nwhere $f^T(\\bm{x})$ is the ML model's performance metric (e.g., validation error) corresponding to the configuration $\\bm{x}$. \nDue to the intrinsic randomness of most ML algorithms, we evaluate a configuration $\\bm{x}$ and can only get its noisy observation $y = f(\\bm{x}) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. \n% We denote the observed data points for the $i$-th task as observations $D^i=\\{(\\bm{x}^i_j, y^i_j)\\}_{j=1}^{n^i}$, where $n^i$ is the size of $D^i$.\nIn this work, we consider methods that output a compact search space $\\hat{\\mathcal{X}}\\subseteq\\mathcal{X}$ with the aid of tuning history from past tasks. \nInstead of Equation~\\ref{eq:original}, we solve the following problem:\n\\begin{equation}\n    \\mathop{\\arg\\min}_{\\bm{x} \\in \\hat{\\mathcal{X}}}f^T(\\bm{x}).\n    \\label{eq:new}\n\\end{equation}\n\nWhile $\\hat{\\mathcal{X}}$ is much smaller than $\\mathcal{X}$, optimization methods may find those optimal configurations faster. \nTherefore, we aim to design $\\hat{\\mathcal{X}}$ such that it contains a proper set of promising configurations, which is close to the good region in the original space $\\mathcal{X}$.\n \n"
                },
                "subsection 3.2": {
                    "name": "Bayesian Optimization",
                    "content": "\n\\label{sec:bo}\nBayesian optimization (BO) works as follows. \nSince evaluating the objective function $f$ for a\ngiven configuration $\\bm{x}$ is expensive, it approximates $f$ using a surrogate model $M:p(f|D)$ fitted on observations $D$, and this surrogate is much cheaper to evaluate.\nGiven a configuration $\\bm{x}$, the surrogate model $M$ outputs the posterior predictive distribution at $\\bm{x}$, that is,\n$f(\\bm{x}) \\sim \\mathcal{N}(\\mu_{M}(\\bm{x}), \\sigma^2_{M}(\\bm{x}))$.\n% Bayesian optimization is a sequential model-based framework for solving expensive black-box functions. \n% In the $n$-th iteration, BO methods first fit a probabilistic surrogate model $M:p(f|D)$ on the already observed data points $D=\\{\\bm{x}_j,y_j\\}_{j=1}^{n-1}$, which is much cheaper to evaluate compared with the expensive objective function $f$.\nBO methods iterate the following three steps: 1) use surrogate $M$ to select a promising configuration $\\bm{x}_n$ that maximizes the acquisition function $\\bm{x}_{n}=\\arg\\max_{\\bm{x} \\in \\mathcal{X}}a(\\bm{x}; M)$, where the acquisition function is to balance the exploration and exploitation trade-off; 2) evaluate this point to get its performance $y_n$, and add the new observation $(\\bm{x}_{n}, y_{n})$ to $D=\\{(\\bm{x}_j,y_j))\\}_{j=1}^{n-1}$; 3) refit $M$ on the augmented $D$. \n\nExpected Improvement (EI) \\cite{jones1998efficient} is a common acquisition function, and it is widely used in the HPO community for its excellent empirical performance. \nEI is defined as follows:\n\\begin{equation}\n\\label{eq_ei}\na(\\bm{x}; M)=\\int_{-\\infty}^{\\infty} \\max(y^{\\ast}-y, 0)p_{M}(y|\\bm{x})dy,\n\\end{equation}\nwhere $M$ is the surrogate model and $y^{\\ast}$ is the best performance observed in $D$, i.e., $y^{\\ast}=\\min\\{y_1, ..., y_n\\}$. \nBy maximizing this EI function $a(\\bm{x}; M)$ over the hyperparameter space $\\mathcal{X}$, BO methods can find a configuration with the largest EI value to evaluate for each iteration.\nAlgorithm~\\ref{algo:bo} displays the BO framework. \nWhile the \\textit{design} function does nothing in vanilla BO and returns the original space (Line 8), we aim to design a compact and promising search space to accelerate HPO as introduced in Section~\\ref{sec:reduced}.\n\n\n\\begin{algorithm}[tb]\n  \\small\n  \\caption{Pseudo code for Bayesian Optimization}\n  \\label{algo:bo}\n  \\begin{algorithmic}[1]\n  \\REQUIRE the number of trials $T$, the hyper-parameter space $X$,  surrogate model $M$, acquisition function $\\alpha$, and initial hyper-parameter configurations $X_{init}$.\n  \\FOR{\\{$\\bm{x} \\in X_{init}\\}$}\n    \\STATE evaluate the configuration $\\bm{x}$ and obtain its performance $y$.\n    \\STATE augment $D = D \\cup (\\bm{x}, y)$.\n  \\ENDFOR\n  \\STATE initialize observations $D$ with initial design.\n  \\FOR{\\{ $i = |X_{init}| + 1, ..., T\\}$}\n  \\STATE fit surrogate $M$ based on observations $D$.\n  \\STATE design the search space: $\\hat{\\mathcal{X}}=\\operatorname{design}(\\mathcal{X})$.\n  \\STATE select the configuration to evaluate: $\\bm{x}_i=\\operatorname{argmax}_{\\bm{x}\\in \\hat{\\mathcal{X}}}\\alpha(\\bm{x}, M)$.\n  \\STATE evaluate the configuration $\\bm{x}_i$ and obtain its performance $y_i$.\n  \\STATE augment $D=D\\cup(\\bm{x}_i,y_i)$.\n  \\ENDFOR\n  \\STATE \\textbf{return} the configuration with the best observed performance.\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\iffalse\n"
                },
                "subsection 3.3": {
                    "name": "Gaussian Process",
                    "content": "\nSince the Gaussian process (GP) \\cite{rasmussen2004gaussian} is basically hyperparameter-free and provides closed under-sampling, it becomes the most common model choice for the surrogate in BO.\nIt is a convenient and powerful way of putting prior distributions over functions. \nGP is specified by a mean function $m: \\mathcal{X} \\to \\mathbb{R}$ and a positive definite covariance, or kernel function $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$. The convenient properties of the GP allow us to compute marginal and conditional means and variances in closed form. Given the observations $D=\\{(\\bm{x}_i, y_i)\\}_{i=1}^{n}$, the posterior predictive mean and variance under a GP at a new point $\\bm{x}$ can be respectively expressed as:\n\\begin{equation}\n\\begin{aligned}\n    & \\mu(\\textbf{x})=m(\\textbf{x}) + \\textbf{K}_{\\star}\\textbf{K}^{-1}(\\textbf{y} - \\textbf{m}), \\\\\n    & var(\\textbf{x})=\\textbf{K}_{\\star\\star} - \\textbf{K}_{\\star}\\textbf{K}^{-1}\\textbf{K}_{\\star}^T, \\\\\n\\end{aligned}\n\\end{equation}\nwhere $\\textbf{K}$ is the Gram matrix formed by applying the kernel function to all points in $D$, $\\textbf{K}_{\\star}$ is formed by computing $k(\\textbf{x},\\textbf{x}_n)$ for all $\\textbf{x}_n$, $\\textbf{K}_{\\star\\star}=k(\\textbf{x},\\textbf{x})$, and $\\textbf{m}$ is the vector formed by applying the mean function to $D$. Inverting the Gram matrix causes a computational complexity $O(N^3)$ in GP; this operation makes GP infeasible for $D$ with many points and leads to the scalability issue.\n\n\n\\fi"
                }
            },
            "section 4": {
                "name": "The Proposed Method",
                "content": "\n\\label{sec4}\n\nIn this section, we introduce a novel search space design method for hyperparameter tuning. We first present the notations and overview of our method, then introduce the two critical steps: promising region extraction and target search space generation. Finally, we end this section with an algorithm summary and discussion.\n\n",
                "subsection 4.1": {
                    "name": "Notations and Overview",
                    "content": "\nOur method takes the observations from $K+1$ tasks as input, in which $D^1$, ..., $D^K$ are tuning history from $K$ source tasks and $D^T$ is the observations in the target task. \nThe $i$-th source task contains $n^i$ evaluated configurations $D^i=\\{(\\bm{x}_j^i, y_j^i)\\}_{j=1}^{n^i}$.\nUnlike $\\{D^i\\}_{i=1}^K$ that are obtained in previous tuning procedures, the number of observations in $D^T$ grows along with the current tuning process.\nAfter finishing $t$ trials, the target observations are $D^T=\\{(\\bm{x}_j^T, y_j^T)\\}_{j=1}^{t}$.\n% We train a surrogate model on observations from each source tasks, which is denoted by $M^i$.\n% These surrogate models can be fitted in advance before optimization on the target task.\n% Since the configuration performance in $D^i$ and $D^T$ may be of different scales, we standardize the $y$s in each task by removing the mean and scaling to unit variance.\n% Given a configuration $\\bm{x}_j$, the surrogate $M^i$ outputs a posterior predictive distribution $\\mathcal{N}(\\mu_{M^i}(\\bm{x}_j), \\sigma^2_{M^i}(\\bm{x}_j))$. For brevity, we denote the predictive mean at $\\bm{x}_j$ as $M^i(\\bm{x}_j)=\\mu_{M^i}(\\bm{x}_j)$.\n\nIn this work, we consider methods that take the previous observations $\\{D^i\\}_{i=1}^K$, and the current observations $D^T$ as inputs, and output a compact search space $\\hat{\\mathcal{X}}\\subseteq\\mathcal{X}$. \nOur proposed method designs the compact search space based on the promising regions obtained from source tasks. \nThe \\textbf{promising region} refers to a sub-region of the original search space $\\mathcal{X}^i \\in \\mathcal{X}$ where the optimal configurations of the target task $\\bm{x}^*$ are located with a high probability. \nBefore optimizing the target task, our approach trains a surrogate model $M^i$ on observations $D^i$ from each source task $i$. \nThese surrogate models can be fitted in advance.\nTo utilize the promising regions and address the challenges in Section~\\ref{sec:intro}, our search space design method includes two critical steps: (1) promising region extraction and (2) target search space generation. \nIn short, our approach applies the machine learning algorithm to represent complex promising regions and adopt a ranking-based method to measure the task correlation.\nThen, it generates the target search space via a sampling framework along with the voting mechanism. \n% In short, our approach first extracts a promising region from each source task based on its similarity with the target task and then generates the target search space via sampling framework along with the voting mechanism. \nWe will introduce the details in Sections~\\ref{sec:prg} and~\\ref{sec:prc}, respectively.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Promising Region Extraction",
                    "content": "\n\\label{sec:prg}\nTo transfer the search spaces, our method first extracts a promising region $\\mathcal{X}^i$ from each source task $i$. \nA simple intuition is that when the target task is quite similar to a source task, the region of the best configurations are similar in both two tasks, \nso we have high confidence to extract a relatively small region that the optimal configuration of the target task locates in that region.\nOn the contrary, when the target task is quite different from a source task, we are not sure whether the best configurations of the source task work well on the target task,\nso we need a conservatively large enough space to contain a sufficient number of candidate configurations.\nTherefore, the size of the promising region should be carefully crafted.\nIn the following, we will answer two questions: 1) how to define the similarity between the source tasks and the target task, and 2) how to extract the promising region with a proper size according to the similarity.\n\nWe measure the task similarity between source tasks and the target task on the target observations $D^T$ via a ranking-based method.\nIn HPO, the ranking is more reasonable than the actual performance value of each configuration, and we care about the partial orderings over configurations, i.e., the region of promising configurations.\nTherefore, we apply the ratio of order-preserving pairs to measure this similarity. The definition of the similarity $S(M^i;D^T)$ between the $i$-th source task and the target task is given by,\n\\begin{equation}\n\\small\n\\begin{aligned}\n    F(M^i;D^T)& =\\sum_{j=1}^{\\left|D^T\\right|} \\sum_{k=j+1}^{\\left|D^T\\right|} \\mathds{1}\\left(\\left(M^i(\\bm{x}_j)<M^i(\\bm{x}_k)\\right)\\otimes \\left(y_j<y_k\\right) \\right) \\\\\n    S(M^i;D^T)& =2F(M^i;D^T)/(|D^T|*(|D^T|-1)),\n\\end{aligned}\n\\label{eq:similarity}\n\\end{equation}\nwhere $F(M^i;D^T)$ is the number of order-preserving pairs, $|D^T|$ is the number of target observations, $M^i(x_j)$ is the predictive mean of the surrogate $M^i$ of the $i$-th source task given the configuration $x_j$, and $\\otimes$ is the exclusive-nor operation, in which the statement value is true only if the two sub-statements return the same value.\n% Therefore, the value of $S(M^i; D^T)$ is proportional to the similarity between the source task $i$ and the target task.\n\nWhile previous work~\\cite{pNIPS2019_9438} represents the promising region of a source task as a single area, we argue that the promising regions in many HPO tasks are non-convex and discontinuous, i.e., there may be several local optima, and the final promising region often consists of several good areas.\nIn addition, this method restricts the region with some geometrical shape, while the shape of promising regions in most real-world HPO tasks is uncertain and cannot be recognized easily.\nTo represent multiple promising areas along with uncertain shapes, We turn this problem into a binary classification task and employ the Gaussian Process Classifier (GPC) to learn the promising region, where the classifier could predict whether a configuration from the search space belongs to the promising region or not. \nFor each source task $i$, our method prepares the training data $\\{\\bm{x^i_j},b^i_j\\}^{|D^i|}_{j=1}$ as follows,\n\\begin{equation}\n    b^i_j=\\left\\{\n    \\begin{array}{lcr}\n     1    &    &  \\text{if} \\ y^i_j < y^i_+\\\\\n     0    &    &  \\text{if} \\ y^i_j \\geq y^i_+\n    \\end{array}\\right.,\n\\label{eq:training_data}\n\\end{equation}\nwhere $y^i_+$ is determined by some quantile $\\alpha^i$ of performance values in $D^i$, so that the cumulative distribution function $P(y<y^i_+)=\\alpha^i$. \nInspired by the aforementioned intuition, we further propose to control the size of promising regions by adjusting $\\alpha^i$ based on task similarity, and the adjustment rule is given by,\n\\begin{equation}\n\\small\n    \\alpha^i=\\alpha_{min}+(1-2*\\mathop{\\max}(S(M^i;D^T)-0.5,0))*(\\alpha_{max}-\\alpha_{min}),\n\\label{eq:percentile}\n\\end{equation}\nwhere $\\alpha_{max}$ and $\\alpha_{min}$ are two parameters set close to 1 and 0 respectively, which control the aggressiveness of the quantile $\\alpha^i$. \nThe two parameters ensure that in the worst case, when the source task is adverse to the target task, the size of the promising region will be determined by $\\alpha_{max}$, i.e., the original complete search space.\nWhen a source task is quite similar to the target task, the value of $\\alpha^i$ approaches $\\alpha_{min}$ so that a compact promising region can be extracted.\nFinally, our method fits a GPC model $G^i$ on the training data in Eq.~\\ref{eq:training_data} for each source task and extracts the promising region as $\\mathcal{X}^i=\\{\\bm{x}_j|\\bm{x}_j \\in \\mathcal{X},G^i(\\bm{x}_j)=1\\}$.\nNote that, due to the growing observation $D^T$ during the HPO process, the promising region from each source task adaptively adjusts based on the task similarity.\n\n"
                },
                "subsection 4.3": {
                    "name": "Target Search Space Generation",
                    "content": "\n\\label{sec:prc}\nWhile each source task holds a promising region, the next step is to combine those regions and generate the search space for the current iteration. \nAn intuitive way is to select the most similar task and directly apply its promising region as the target space, that is:\n\\begin{equation}\n    \\hat{\\mathcal{X}}=\\mathcal{X}^m,\\quad m=\\mathop{\\arg\\max}_{i=1,...,k}S(M^i;D^T).\n\\end{equation}\n\nHowever, since the search process could easily be trapped in a sub-optimal local region provided by the most similar source task, this design may lead to the over-exploitation issue over the whole search space. \nAnother alternative is to sample a task according to the similarity and then apply its promising region, that is:\n\\begin{equation}\n    \\hat{\\mathcal{X}}=\\mathcal{X}^m, \\quad m \\sim \\mathbb{P}(.),\n\\label{eq:sample}\n\\end{equation}\nwhere $\\mathbb{P}(.)$ is the distribution computed based on the similarity, in which $p(i)=S(M^i;D^T)/ \\sum_{i=1}^K S(M^i;D^T)$.\nThough sampling enables exploration on different promising regions, the source information is not fully utilized in optimization, i.e., the information from only one source task is used in this design of space. \n\nTo encourage exploration and utilize more source tasks, our method adopts a sampling framework along with the voting mechanism.\nIt first samples $k$ source tasks out of $K$ tasks without replacement. \nConcretely, we scale the sum of similarity to 1 and sample source tasks based on this distribution.\nWe denote the sampled tasks as $s_1,...,s_k$.\nThen, for each configuration $\\bm{x}\\in\\mathcal{X}$, our method builds a voting ensemble $\\hat{G}$ of $k$ GPC models as,\n\\begin{equation}\n    \\hat{G}(\\bm{x}_j)=\\left\\{\n    \\begin{array}{lcl}\n     1    &    &  \\text{if} \\ \\sum_{i=1}^k G^{s_i}(\\bm{x}_j) \\geq \\lfloor \\frac{k}{2} \\rfloor\\\\\n     0    &    &  \\text{else}\n    \\end{array}\\right..\n\\label{eq:voting}\n\\end{equation}\n\nWhen a configuration is in the majority of promising regions of the sampled tasks, it is regarded as a feasible configuration in the compact search space. \nThe final target space is formulated as $\\hat{\\mathcal{X}}=\\{\\bm{x}_j|\\bm{x}_j \\in \\mathcal{X},\\hat{G}(\\bm{x}_j)=1\\}$.\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Algorithm Summary",
                    "content": "\nAlgorithm \\ref{algo:framework} gives the pseudo code of the \\textit{design} function. \nThe function is called during each iteration as shown in Algorithm~\\ref{algo:bo}.\nWe first extract a promising region for each source task by computing the similarity based on Equation~\\ref{eq:similarity} (Line 1) and training the GPC model based on the generated training data given by Equation~\\ref{eq:training_data} (Line 2).\nThen, we sample source tasks based on the similarity distribution (Line 3) and generate the target search space by combining those promising regions via the voting results of GPC models (Line 4).\n\n\n\\begin{algorithm}[tb]\n  \\small\n  \\caption{Pseudo code for \\textit{design} function.}\n  \\label{algo:framework}\n  \\begin{algorithmic}[1]\n  \\REQUIRE the surrogates in $K$ source tasks: $M^{i}$ with $i\\in [1, K]$, the target observation $D^T$, the configuration space $\\mathcal{X}$.\n  \\STATE compute the similarity $S(M^i;D^T)$ between each source task $i$ and \\newline the target task based on Eq.~\\ref{eq:similarity}.\n  \\STATE Train a Gaussian Process Classifier $G^i$ for each source task $i$ based \\newline on training data from Eq.~\\ref{eq:training_data}.\n  \\STATE Sample $k$ source tasks $s_1,...,s_k$ based on similarity distribution \\newline from Eq.~\\ref{eq:sample}.\n  \\STATE Generate the target search space $\\hat{\\mathcal{X}}$ based on the voting results of \\newline $G^{s_i}$ with $i\\in [1,k]$.\n  \\STATE \\textbf{return} the final search space $\\hat{\\mathcal{X}}$.\n\\end{algorithmic}\n\\end{algorithm}\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Discussion",
                    "content": "\n% Table \\ref{cmp-table} summarizes the related methods. \nTo our knowledge, our method is the first method that owns the following desirable properties simultaneously in the field of search space design.\n\\textbf{1. Universality.}\nOur method focuses on accelerating HPO by designing promising and compact search space on the target task, so it is orthogonal to and compatible with other forms of transfer methods and a wide range of BO methods. In other words, it can be easily combined into those methods by integrating the \\textit{design} function as in Algorithm~\\ref{algo:bo}.\n\\textbf{2. Practicality.} \nA practical search space design method should support different types of hyperparameters, including both numerical and categorical ones. \nWhile previous work~\\cite{pNIPS2019_9438} only works on numerical hyperparameters due to the use of geometrical shapes, our method employs an ML model to learn promising regions with uncertain shapes, so that it supports both two types of hyperparameters.\nIn addition, the computational complexity of \\textit{design} function is $O(Kn^3)$, where $K$ is the number of source tasks and $n$ is the number of observations in each source task; due to the linear growth on $K$, the proposed method could scale to a large number of source tasks $K$ (good scalability).\n\\textbf{3. Safeness.}\nThe design of our method ensures that dissimilar source tasks may influence little to the final search space. \nThose tasks have little chance to be sampled due to their low similarity.\nAnd once they are sampled by coincidence, Eq.~\\ref{eq:percentile} returns a large percentile, so that their promising regions will be relatively large, and the optimal configuration of the target task is less likely to be excluded.\n"
                }
            },
            "section 5": {
                "name": "Experiments and Results",
                "content": "\n\\label{exp_sec}\n\nIn this section, we evaluate the superiority of our approach from two perspectives: 1) the empirical performance compared with the existing search space design method on a wide range of benchmarks, 2) its advantages in terms of universality, practicality and safeness.\n\n\n\n\n\n\n\n",
                "subsection 5.1": {
                    "name": "Experimental Settings",
                    "content": "\n\\para{\\textbf{Benchmarks.}}\nWe evaluate the performance of our proposed method on three benchmarks:\n% \\begin{itemize}\n%     \\item Random Forest Tuning Benchmark: The benchmark tunes 5 Random Forest hyperparameters on 20 OpenML datasets~\\cite{vanschoren2014openml}. Each task contains the evaluation results of 50k randomly chosen configurations.\n%     \\item ResNet Tuning Benchmark: The benchmark tunes 5 ResNet hyperparameters on CIFAR-10, SVHN and Tiny-ImageNet. Each task contains the evaluation results of 1500 randomly chosen configurations and 500 configurations selected by running a GP-based BO.\n%     \\item NAS-Bench-201~\\cite{dong2019bench}: This is a light-weight benchmark with 6 hyperparameters for neural architecture search, which includes the statistics of 15,625 CNN models on three datasets -- CIFAR-10-valid, CIFAR-100, and ImageNet16-120.\n% \\end{itemize}\n(1) Random Forest Tuning Benchmark: The benchmark tunes 5 Random Forest hyperparameters on 20 OpenML datasets~\\cite{vanschoren2014openml}. Each task contains the evaluation results of 50k randomly chosen configurations.\n(2) ResNet Tuning Benchmark: The benchmark tunes 5 ResNet hyperparameters on CIFAR-10, SVHN and Tiny-ImageNet. Each task contains the evaluation results of 1500 randomly chosen configurations and 500 configurations selected by running a GP-based BO.\n(3) NAS-Bench-201~\\cite{dong2019bench}: This is a light-weight benchmark with 6 hyperparameters for neural architecture search, which includes the statistics of 15,625 CNN models on three datasets -- CIFAR-10-valid, CIFAR-100, and ImageNet16-120.\n\nThe hyperparameter space in the Random Forest Tuning Benchmark follows the implementation in Auto-Sklearn~\\cite{feurer2015efficient}, while the space in ResNet Tuning Benchmark follows the previous work~\\cite{li2021mfes}.\nIt takes more than 50k CPU hours and 5k GPU hours to collect the first two benchmarks.\nMore details about the search space and datasets of the benchmarks are provided in Appendix \\ref{a.1}.\n\n\\para{\\textbf{Search Space Baselines.}} \n% \\blue{only one baseline in detail, and in addition, we also use BO, SMAC, Random search to evaluate the performance of generated search space.}\nAs a search space design method, we compare our proposed method with other three search space design: \n(1) No design: using the original search space without any space extraction;\n(2) Box~\\cite{pNIPS2019_9438}: search space design by representing the promising region as a low-volume bounding box;\n(3) Ellipsoid~\\cite{pNIPS2019_9438}: search space design by representing the promising region as a low-volume ellipsoid.\nBased on the search space design methods, we further evaluate random search and GP-based BO~\\cite{snoek2012practical} on the compact (or original) search space. \nIn addition, for neural architecture search, we also evaluate the regularized evolutionary algorithm (REA)~\\cite{real2019regularized} and SMAC~\\cite{hutter2011sequential}, which are state-of-the-art methods on neural architecture search.\nNote that, since our method enjoys high universality, it can be easily implemented into those methods.\nWe will further illustrate the performance of the combination of space transfer and surrogate transfer methods in Section~\\ref{sec:property_study}.\n\n% (1) Random search~\\cite{bergstra2012random} without using source data;\n% (2) GP~\\cite{snoek2012practical}: BO with Gaussian process-based surrogate without using source data;\n% (3) SMAC~\\cite{hutter2011sequential}: BO with probabilistic random forest without using source data;\n\n% (2) GP~\\cite{snoek2012practical}: BO with Gaussian process-based surrogate without using any source data, \n% (3) SMAC~\\cite{hutter2011sequential}: BO with probabilistic random forest without using any source data,\n% (4) \n% (4) SCoT~\\cite{bardenet2013collaborative}: it models the relationship between datasets and hyperparamter performance by training a single surrogate on the scaled and merged observations from both source tasks and the target task, (4) SGPR: the core TL algorithm used in the well-known service --- Google Vizier~\\cite{golovin2017google}, and four one-phase TL methods: (5) POGPE~\\cite{schilling2016scalable}, (6) TST~\\cite{wistuba2016two}, (7) TST-M: a variant of TST using dataset meta-features~\\cite{wistuba2016two}, and (8) RGPE~\\cite{feurer2018scalable}.\n\n\\para{\\textbf{Basic Settings.}}\nTo evaluate the performance of the considered transfer learning method, the experiments are performed in a leave-one-out fashion, i.e., each method optimizes the hyperparameters of a specific task while treating the remaining tasks as the source tasks. \nWe follow the experiment settings as in previous work~~\\cite{wistuba2016two, feurer2018scalable}.\nConcretely, only $N_S$ observations in the source tasks (here $N_S=100$) are used to extract promising region.\nIn addition, following~\\cite{feurer2018scalable}, all the compared methods are initialized with three randomly selected configurations, and then\na total of $N_{T}=50$ configuration evaluations are conducted sequentially to evaluate the effectiveness of the transfer learning based space design given limited trials. \nTo avoid the effect of randomness, each method is repeated $20$ times and the mean performance metrics are reported along with their variance.\n\n\n\\para{Evaluation Metrics.}\nFor each task, the objective of HPO is to find the configuration with the lowest validation error.\nHowever, since the classification error is not commensurable across the benchmark datasets, we follow the previous work~\\cite{bardenet2013collaborative,wistuba2016two,feurer2018scalable} and report the Normalized Classification Error (NCE) of all compared baselines on each dataset. The NCE after $t$ trials is defined as:\n\\begin{equation}\nNCE(\\mathcal{X}^i_t) = \\frac{min_{\\bm{x} \\in \\mathcal{X}_t} y^{i}_{\\bm{x}} - y^{i}_{min}}{y^i_{max} - y^{i}_{min}},\n\\end{equation}\nwhere $y^i_{min}$ and $y^i_{max}$ are the best and worst ground-truth performance value (i.e., classification error) on the $i$-th task, $y_{\\bm{x}}^i$ corresponds to the performance of configuration $\\bm{x}$ in the $i$-th task, and $\\mathcal{X}^i_t$ is the set of hyperparameter configurations on the $i$-th task that have been evaluated in the previous $t$ trials.\nTo measure a method on the entire benchmark, we average the NCE over all considered tasks, that is, $\\frac{1}{K}\\sum_{i=1}^KNCE(\\mathcal{X}^i_t)$, where $K$ is the number of tasks.\n\n% \\frac{1}{K}\\sum_{i \\in [1:K]}\n% Comparing each method in terms of classification error is questionable because the classification error is not commensurable across datasets. \n% Following the previous works~\\cite{bardenet2013collaborative,wistuba2016two,feurer2018scalable}, we adopt the metrics as follows:\n\n% \\emph{Average Rank.} For each target task, we rank all compared methods based on the performance of the best configuration they have found so far. \n% Furthermore, ties are being solved by giving the average rank. \n% For example, if one method observes the lowest validation error of 0.2, another two methods find 0.3, and the last method finds only 0.45, we would rank the methods with $1$, $\\frac{2+3}{2}$, $\\frac{2+3}{2}$, $4$.\n\n% \\emph{Average Distance to Minimum.} The average distance to the global minimum after $t$ trials is defined as:\n% \\begin{equation}\n% ADTM(\\mathcal{X}_t) = \\frac{1}{K}\\sum_{i \\in [1:K]}\\frac{min_{\\bm{x} \\in \\mathcal{X}_t} y^{i}_{\\bm{x}} - y^{i}_{min}}{y^i_{max} - y^{i}_{min}},\n% \\end{equation}\n% where $y^i_{min}$ and $y^i_{max}$ are the best and worst performance value on the $i$-th task, $K$ is the number of tasks, i.e., $K=30$, $y_{\\bm{x}}^i$ corresponds to the performance of configuration $\\bm{x}$ in the $i$-th task, and $\\mathcal{X}_t$ is the set of hyperparameter configurations that have been evaluated in the previous $t$ trials. \n% The relative distances over all considered tasks are averaged to obtain the final ADTM value.\n\n\\para{Implementations \\& Parameters.}\nPlease refer to Appendix \\ref{reproduction} for more details about implementations and reproductions.\n\n\n\n\n\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Tuning Random Forest on OpenML Tasks",
                    "content": "\nWe first compare our method with other space design methods on the Random Forest Tuning Benchmark. \nConcretely, each task is selected as the target task in turn, and the remaining 19 tasks are the source tasks.\nThe search space from each space method is further evaluated by random search and GP-based BO.\nFigure~\\ref{fig:exp_rf} demonstrates the normalized classification error (NCE) on 8 datasets and Figure~\\ref{fig:exp_agg} shows the aggregated NCE on the entire benchmark.\nWe observe that the Ellipsoid variants perform better than the Box and non-transfer counterparts on most tasks in terms of convergence speed. \nHowever, on musk and satimage, GP works slightly better than Ellipsoid GP and Box GP.\nAs we have explained in Section~\\ref{sec:intro}, the reason is that the two space design methods ignore the relevancy between tasks, which hampers the performance of transfer learning when using dissimilar source tasks.\nGenerally, the variants of our method consistently outperform their counterparts.\nThe NCE of Ours + GP rapidly drops in the first 25 iterations, and achieves the best final performance on 7 out of 8 tasks.\nIn addition, we find that random search equipped with our method outperforms the Box and Ellipsoid variants on sick and musk, which further indicates the superiority of our space design method.\n\n% \\para{Online Scenario.}\n% To simulate the real-world transfer learning scenario, we perform the online experiment on different benchmarks. \n% In this experiment, 30 tasks arrive sequentially; when the $i$-th task arrives, the previous $i$-1 tasks are used as the source tasks. \n% The maximum number of trials for each task is 50, and we compare \\sys with TST, RGPE, and POGPE based on the best-observed performance on each task. Table~\\ref{online-table} reports the number of tasks on which each TL method gets the highest and second-highest performance. \n% Note that the sum of each column may be more than 30 since some of the TL methods are tied for first or second place.\n\n% As shown in Table~\\ref{online-table}, \\sys achieves the largest number of top1 and top2 online performance among the compared methods. \n% Take Adaboost as an example, \\sys gets 25 top2 results among 30 tasks, while this number is 13 for RGPE.\n% RGPE gets a similar performance with TST on LightGBM and Extra Trees, but its performance decreases on Adaboost. Thus, RGPE is not stable in this scenario.\n% Compared with the baselines, \\sys is more powerful in getting stable and satisfactory performance in the online scenario. \n\n"
                },
                "subsection 5.3": {
                    "name": "Tuning ResNet on Vision Problems",
                    "content": "\nIn this part, we evaluate our method on a deep learning algorithm tuning problem.\nFigure~\\ref{fig:exp_resnet} shows the compared results on three vision problems. \nOn CIFAR-10 and SVHN, we observe that GP and Ellipsoid + GP show almost the same performance.\nThe reason is that, the optimal configuration of Tiny-ImageNet is quite far away from the other two tasks. \nAnd then, it will take a large ellipsoid to cover the optimal configurations of all source tasks, which is almost the same as the original space.\nIn this case, they achieve almost the same performance due to the same underlying search space.\nSimilar to the Random Forest Tuning Benchmark, the variants of our method consistently outperforms the compared counterparts.\nRemarkably, both two variants of our method reduces the NCE of the second-best baseline Box + GP by 23.7\\% on Tiny-ImageNet. \n\n"
                },
                "subsection 5.4": {
                    "name": "Architecture Search on NASBench201",
                    "content": "\nDifferent from the above two benchmarks, the search space for neural architecture search (NAS) contains only categorical hyperparameters.\nIn this case, the Box and Ellipsoid variants fail to find a compact space and revert to standard search space since there is no partial order between different values of categorical hyperparameters.\nTherefore, we compare our method with non-transfer counterparts.\nTo further demonstrate we extend the optimization algorithms with state-of-the-art SMAC and REA and the results are shown in Figure~\\ref{fig:exp_nas}. \nOn all the three datasets, we observe that the variants of our method consistently performs better than non-transfer counterparts.\nWhen we equip the state-of-the-art method SMAC with our method, we observe a significant reduction on NCE, which indicates that our method enjoys practicality on different types of HPO tasks.\n\n\n% \\begin{figure}[t!]\n% \t\\centering\n% \t\\scalebox{.65}[.65]{\n% \t  \\includegraphics[width=1\\linewidth]{images/exp_seq10_rank.pdf}\n% \t}\n% \t\\vspace{-1.em}\n% \t\\caption{Average rank results when tuning random forest on 10 OpenML tasks sequentially (OR ONLINE?)todo! update plot.}\n%   \\label{fig:exp_rank_online}\n% \\end{figure}\n\n\n% \\subsection{Online Experiments}\n% To simulate the real-world HPO scenarios, in which tasks arrive sequentially. \n% When the $i$-th task arrives, the former $i-1$ tasks are regarded as the source tasks.\n% We choose a sequence of 10 tasks from 20 tasks in the Random Forest Tuning Benchmark, and plot the results in Figure~\\red{xx}.\n% \\begin{itemize}\n%     \\item Online setting and results.\n%     \\item a plot of average rank.\n% \\end{itemize}\n\n\n\n"
                },
                "subsection 5.5": {
                    "name": "Case Study on Universality and Safeness",
                    "content": "\n\\label{sec:property_study}\nIn this subsection, we investigate the advantages of our proposed method.\nWe first study how our method works with existing surrogate transfer methods (Universality) on Random Forest Tuning Benchmark.\nWe choose the state-of-the-art RGPE~\\cite{feurer2018scalable} and TST~\\cite{wistuba2016two} as the optimization algorithms.\nFigure~\\ref{fig:exp_rgpetst} demonstrates the optimization results with and without the surrogate transfer methods on Random Forest Tuning Benchmark.\nWe observe that the surrogate transfer methods alone (RGPE and TST) indeed accelerate hyperparameter optimization.\nWhen our method is implemented, the performance is further improved. \nConcretely, our method reduces the aggregated NCE of\nRGPE and TST by 10.1\\% and 22.6\\% on the entire benchmark, respectively.\n\nIn addition, we set the number of trials to 200 showcase the convergence results of compared methods given a larger budget (Safeness). \nFigure~\\ref{fig:exp_long} shows that our method still outperforms the compared GP variants.\nConcretely, our method with GP reduces the aggregated NCE by 36.0\\% compared with the second-best baseline Ellipsoid + GP on the Random Forest Tuning Benchmark.\n\n\n\n\n"
                },
                "subsection 5.6": {
                    "name": "Case Study on Search Space Design",
                    "content": "\nIn this subsection, we visualize the compact search space given by our method on two datasets in Random Forest Tuning Benchmark.\nFigure~\\ref{fig:exp_visual_real} plots the candidate points in our compact search space during the 50-th iteration on two datasets. \nRather than optimizing over the entire cube, our method generates a significantly smaller search space, which still contains the global optimum (the red point in Figure~\\ref{fig:exp_visual_real}).\nRemarkably, the size of search space designed by our method is 375 and 1904 on musk and cpu\\_act, respectively. \nCompared with the original search space with 50000 candidates, the size of space shrinks to 0.75\\% and 3.81\\% of the original search space on the two tasks.\n\n\n\n\n\n\nIn addition, we also plot the promising regions (source tasks) and the compact search space (target task) in Figures~\\ref{fig:ablation_vis_segment} and~\\ref{fig:ablation_vis_pageblocks} using three datasets from Figure~\\ref{fig:intro_heatmap}.\nRecall that satimage is similar to segment, but different from page-blocks(2).\nThe red bounding box refers to the search space from the baseline Box.\nAs it only depends on the best configurations found in the source tasks, we observe that neither of the two search spaces from Box covers the global optimum (the red point in Figures~\\ref{fig:ablation_vis_segment} and~\\ref{fig:ablation_vis_pageblocks}) on the target task.\nIn Figure~\\ref{fig:ablation_vis_segment} where the first source task is similar to target task, our compact search space covers the global optimum since the 5-th iteration.\nHowever, as shown in Figure~\\ref{fig:ablation_vis_pageblocks} where the target task is quite dissimilar to both source tasks, the compact search space does not include the global optimum in the very beginning. \nIn this case, the promising region of source tasks gradually expand due to the dynamic quantile introduced in Section~\\ref{sec:prg}, and it eventually covers the global optimum in the 40-th iteration while still excluding the bad regions as shown in Figure~\\ref{fig:intro_heatmap}.\nThis demonstrates the effectiveness and safeness of our approach. \n\n\n\n\n\n"
                },
                "subsection 5.7": {
                    "name": "Ablation Study",
                    "content": "\n\nFinally, we provide ablation study on the machine learning classifiers used in promising region extraction and the target search space generation strategy on Random Forset Tuning Benchmark.\nFor classifiers, we compare the influence of different machine learning algorithms used in promising region extraction.\nFigure~\\ref{fig:ab_c} shows the results of four algorithm choices: KNN, LibSVM, Random Forest (RF), and Gaussian Process Classifier (GPC) used in our method.\nAmong the choices, the GPC shows the most stable performance across all tasks.\nMoreover, GPC is a the only choice without algorithm hyperparameters, and thus we employ it as the classifier for promising region extraction.\n\nIn addition, we denote 1) OURS-v1 as using the promising region of the most similar source task; 2) OURS-v2 as using the promising region of a sampled source task; 3) OURS-v3 as our method.\nFigure~\\ref{fig:ab_e} demonstrates the results of those three variants. \nAmong the three methods, OURS-v3 performs the best while OURS-v1 and OURS-v2 perform worse that it.\nAs we have explained in Section~\\ref{sec:prc}, the reason is that OURS-v1 may be trapped in a sub-optimal region provided by the most similar task, and OURS-v2 can not leverage the information from various source tasks.\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\nWe presented a novel approach to incorporate transfer learning in BO. Rather than designing\na specialized multi-task surrogate model, our method automatically crafts promising search spaces based on previously tuning tasks. \nThe extensive experiments on a wide range of tuning tasks demonstrate that our\napproach could significantly speed up the HPO process and enjoy desirable properties.\n\n\\begin{acks}\nThis work was supported by the National Natural Science Foundation of China (No.61832001), Beijing Academy of Artificial Intelligence (BAAI), PKU-Tencent Joint Research Lab. Bin Cui is the corresponding author.\n\\end{acks}\n\n%\\clearpage\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{reference}\n\n\\clearpage\n\n\\appendix\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n",
                "subsection 7.1": {
                    "name": "The Details of Benchmark",
                    "content": "\n\\label{a.1}\nAs described in Section 5, we create two benchmarks to evaluate the performance of search space design methods, namely the Random Forest Tuning Benchmark and ResNet Tuning Benchmark. In addition, we apply NASBench-201~\\cite{dong2019bench} to test the practicality of our method on different types of hyperparameters.\n\n\\textbf{Random Forest Tuning Benchmark.}\nThe random forest classifier is a widely used tree-based ensemble model in data analysis.\nThe implementation of random forest and the design of their hyperparameter space in the benchmark follows the widely-used AutoML system --- Auto-sklearn~\\cite{feurer2015efficient}. \nThe range and the default value of each hyperparameter are illustrated in Tables \\ref{hp_trees}. \nTo collect sufficient source HPO data for transfer learning, we select 20 real-world datasets from OpenML repository~\\cite{vanschoren2014openml}, and evaluate the validation and test performance (i.e., the balanced accuracy) of 50k configurations for each dataset, which are randomly sampled from the hyperparameter space. \nThe datasets used in our benchmarks are of medium size, whose number of rows ranges from 2000 to 8192. \nFor more details, see Table~\\ref{table_dataset}. \nThe total number of model evaluations (observations) in our benchmarks reaches 10 million, and it takes more than 50k CPU hours to obtain the evaluation results. \nFor reproduction purposes, we also upload the benchmark data (e.g., evaluation results and the corresponding scripts) along with this submission.\nThe benchmark data (with size  355.4Mb); due to the space limit (maximum 20Mb) on CMT3, we only upload a small subset of this benchmark. \nAfter the review process, we will make the complete benchmark publicly available (e.g., on Google Drive).\n\n\n\n\n\n\n\\textbf{ResNet Tuning Benchmark.}\nWhile deep learning has attracted great attention in recent years, we also create a benchmark for DL algorithm tuning.\nConcretely, we tune the ResNet with five hyperparameters as shown in Table~\\ref{hp_resnet}.\nThe benchmark contains three vision tasks, including CIFAR-10, SVHN, and Tiny-ImageNet. \nEach task contains the evaluation results of 1500 randomly chosen configurations and 500 configurations selected by running a GP-based BO.\nIt takes more than 5k GPU hours to obtain the evaluation results.\n% For reproduction purposes, we publish the evaluation results at anonymous Github repository\n% \\red{xx}.\n\n\n\n\\textbf{NASBench-201}\nNASBench-201~\\cite{dong2019bench} is a benchmark for neural architecture search (NAS). \nThe architecture space includes six operations, and the choices for each operation are provided in Table~\\ref{hp_nas}.\nTo help compare different NAS algorithms, NASBench-201 exhaustively evaluates all the configurations from its search space on three datasets (CIFAR-10, CIFAR-100, ImageNet16-120).\nEach task includes the results of 15625 evaluations, including the validation accuracy, test accuracy, and training time.\n\n\n\n\n\n"
                },
                "subsection 7.2": {
                    "name": "Implementation and Reproduction Details",
                    "content": "\n\\label{reproduction}\n\nWe use the implementation of Bayesian optimization in SMAC~\\cite{hutter2011sequential,Lindauer2021SMAC3AV}, a toolkit for black-box optimization that support a complex hyperparameter space, including numerical, categorical, and conditional hyperparameters. \nThe kernel hyperparameters in Gaussian Process are inferred by maximizing the marginal likelihood. \nIn the BO module, the popular EI acquisition function is used.\nThe other baselines are implemented following their original papers. \nIn all experiments, the $\\alpha_{max}$ and $\\alpha_{min}$ in our approach are set to 0.95 and 0.05, respectively.\nThe sampling framework samples $k=5$ tasks during each iteration on the benchmarks.\n\nThe source code and benchmark data are uploaded along with this submission on CMT3, and the source code is also available in the anonymous repository~\\footnote{https://anonymous.4open.science/r/2022-5263-XXXXX} now.\nTo reproduce the experimental results in this paper, an environment of Python 3.7+ is required. We introduce the experiment scripts and installation of required tools in \\emph{README.md} and list the required Python packages in \\emph{requirements.txt} under the root directory. \nTake one experiment as an example, to evaluate the offline performance of our method with random search and other baselines on Random Forest Tuning Benchmark with 50 trials, just execute the following script: \\newline\n{\\em\npython tools/offline\\_benchmark.py --algo\\_id random\\_forest --trial\\_num 50 --methods rs,box-rs,ellipsoid-rs,ours-rs --rep 20\n}\n\nPlease check the document \\emph{README.md} in this repository for more details, e.g., how to use the benchmark data in this benchmark, and how to run the experiments.\n"
                }
            }
        },
        "tables": {
            "hp_trees": "\\begin{table}[h]\n\\centering\n\\small\n\\begin{tabular}{lccc}\n    \\toprule\n    Hyperparameter & Range  & Default \\\\\n    \\midrule\n    criterion &  \\{gini, entropy\\} & gini \\\\\n    max\\_features & [0, 1] & 0.5 \\\\\n    min\\_sample\\_split & [2, 20] & 2 \\\\\n    min\\_sample\\_leaf & [1, 20] & 1 \\\\\n    bootstrap & \\{True, False\\} & True \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption {Hyperparameters of Random Forest.}\n\\label{hp_trees}\n\\vskip -0.1in\n\\end{table}",
            "table_dataset": "\\begin{table}[htb]\n\\centering\n\\resizebox{1\\columnwidth}{!}{\n\\begin{tabular}{lccccc}\n    \\toprule\n    Datasets & OpenML ID & Classes & Samples & Continuous & Nominal \\\\ \n    \\midrule\n    kc1 & 1067 & 2 & 2109 & 21 & 0 \\\\\n    quake & 772 & 2 & 2178 & 3 & 0 \\\\\n    segment & 36 & 7 & 2310 & 19 & 0 \\\\\n    madelon & 1485 & 2 & 2600 & 500 & 0 \\\\\n    space\\_ga & 737 & 2 & 3107 & 6 & 0 \\\\\n    sick & 38 & 2 & 3772 & 7 & 22 \\\\\n    pollen & 871 & 2 & 3848 & 5 & 0 \\\\\n    abalone & 183 & 26 & 4177 & 7 & 1 \\\\\n    winequality\\_white & - & 7 & 4898 & 11 & 0 \\\\\n    waveform(1) & 979 & 3 & 5000 & 40 & 0 \\\\\n    waveform(2) & 979 & 2 & 5000 & 40 & 0 \\\\\n    page-blocks(2) & 1021 & 2 & 5473 & 10 & 0 \\\\\n    optdigits & 28 & 10 & 5610 & 64 & 0 \\\\\n    satimage & 182 & 6 & 6430 & 36 & 0 \\\\\n    wind & 847 & 2 & 6574 & 14 & 0 \\\\\n    musk & 1116 & 2 & 6598 & 167 & 0 \\\\\n    delta\\_ailerons & 803 & 2 & 7129 & 5 & 0 \\\\\n    puma8NH & 816 & 2 & 8192 & 8 & 0 \\\\\n    cpu\\_act & 761 & 2 & 8192 & 21 & 0 \\\\\n    cpu\\_small & 735 & 2 & 8192 & 12 & 0 \\\\\n    \\bottomrule\n\\end{tabular}\n}\n\\caption{Datasets used in Random Forest Tuning Benchmark.}\n\\vspace{-1em}\n\\label{table_dataset}\n\\end{table}",
            "hp_resnet": "\\begin{table}[h]\n\\centering\n\\small\n\\begin{tabular}{lccc}\n    \\toprule\n    Hyperparameter & Range  & Default \\\\\n    \\midrule\n    batch size &  [32, 256] & 64 \\\\\n    learning rate & [1e-3, 0.3] & 0.1 \\\\\n    weight decay & [1e-5, 1e-2] & 2e-4 \\\\\n    momentum & [0.5, 0.99] & 0.9 \\\\\n    nesterov & \\{True, False\\} & True \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption {Hyperparameters of ResNet.}\n\\label{hp_resnet}\n\\vskip -0.1in\n\\end{table}",
            "hp_nas": "\\begin{table}[h]\n\\centering\n\\small\n\\resizebox{1\\columnwidth}{!}{\n\\begin{tabular}{lccc}\n    \\toprule\n    Hyperparameter & Choice & Default \\\\\n    \\midrule\n    Operation 1 &  \\{None, Skip, 1*1 Conv, 3*3 Conv, 3*3 Pooling\\} & Skip \\\\\n    Operation 2 &  \\{None, Skip, 1*1 Conv, 3*3 Conv, 3*3 Pooling\\} & Skip \\\\\n    Operation 3 &  \\{None, Skip, 1*1 Conv, 3*3 Conv, 3*3 Pooling\\} & Skip \\\\\n    Operation 4 &  \\{None, Skip, 1*1 Conv, 3*3 Conv, 3*3 Pooling\\} & Skip \\\\\n    Operation 5 &  \\{None, Skip, 1*1 Conv, 3*3 Conv, 3*3 Pooling\\} & Skip \\\\\n    Operation 6 &  \\{None, Skip, 1*1 Conv, 3*3 Conv, 3*3 Pooling\\} & Skip \\\\\n    \\bottomrule\n\\end{tabular}\n}\n\\caption {Hyperparameters of NASBench-201.}\n\\label{hp_nas}\n\\vskip -0.1in\n\\end{table}"
        },
        "figures": {
            "fig:intro_heatmap": "\\begin{figure*}[htb]\n\t\\centering\n\t\t\\scalebox{.9}[.9] {\n\t\t\\includegraphics[width=1\\linewidth]{images/intro_heatmap.pdf}\n         }\n    \\vspace{-1.em}\n\t\\caption{Validation error of 2500 configurations (50 settings of max features and 50 settings of min samples split) when tuning two hyperparameters of random forest on three OpenML datasets. The red region indicates the promising configurations.}\n    \\vspace{-1.1em}\n    \\label{fig:intro_heatmap}\n\\end{figure*}",
            "fig:exp_rf": "\\begin{figure*}[htb]\n\t\\centering\n\t\\subfigure[winequality\\_white]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_winequality_white.pdf}\n\t}}\n\t\\subfigure[page-blocks(2)]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_page-blocks2.pdf}\n\t}}\n\t\\subfigure[sick]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_sick.pdf}\n\t}}\n    \\subfigure[musk]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_musk.pdf}\n\t}}\n\t\\quad\n\t\\subfigure[puma8NH]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_puma8NH.pdf}\n\t}}\n\t\\subfigure[satimage]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_satimage.pdf}\n\t}}\n\t\\subfigure[segment]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_segment.pdf}\n\t}}\n    \\subfigure[cpu\\_act]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.23}[0.23]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_cpu_act.pdf}\n\t}}\n\t\\vspace{-.5em}\n\t\\caption{Normalized classification error (NCE) when tuning Random Forest on eight OpenML datasets.}\n\t\\vspace{-.5em}\n  \\label{fig:exp_rf}\n\\end{figure*}",
            "fig:exp_agg": "\\begin{figure}[t!]\n\t\\centering\n\t\\scalebox{.65}[.65]{\n\t  \\includegraphics[width=1\\linewidth]{images/exp_data20_average.pdf}\n\t}\n\t\\vspace{-.5em}\n\t\\caption{Average NCE results when tuning random forest across 20 OpenML tasks.}\n  \\label{fig:exp_agg}\n\\end{figure}",
            "fig:exp_resnet": "\\begin{figure*}[htb]\n\t\\centering\n\t\\subfigure[CIFAR-10]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.27}[0.27]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_resnet_cifar-10.pdf}\n\t}}\n\t\\subfigure[SVHN]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.27}[0.27]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_resnet_svhn.pdf}\n\t}}\n\t\\subfigure[Tiny-ImageNet]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.27}[0.27]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_resnet_tiny-imagenet.pdf}\n\t}}\n\t\\vspace{-1.em}\n\t\\caption{Normalized classification error (NCE) when tuning ResNet on three vision problems.}\n\t\\vspace{-1.em}\n  \\label{fig:exp_resnet}\n\\end{figure*}",
            "fig:exp_nas": "\\begin{figure*}[htb]\n\t\\centering\n\t\\subfigure[CIFAR-10]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.27}[0.27]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_nas_cifar-10.pdf}\n\t}}\n\t\\subfigure[CIFAR-100]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.27}[0.27]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_nas_cifar-100.pdf}\n\t}}\n\t\\subfigure[ImageNet16-120]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.27}[0.27]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_nas_imagenet.pdf}\n\t}}\n\t\\vspace{-1.em}\n\t\\caption{Normalized classification error (NCE) when conducting architecture search on NASBench201.}\n\t\\vspace{-.5em}\n  \\label{fig:exp_nas}\n\\end{figure*}",
            "fig:exp_universal": "\\begin{figure}[t!]\n\\centering\n\t\\subfigure[Universality]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.47}[0.47]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_rgpetst_average.pdf}\n\t\t\t\\label{fig:exp_rgpetst}\n\t}}\n\t\\subfigure[Safeness]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.47}[0.47]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_data20_200_trials.pdf}\n\t\t\\label{fig:exp_long}\n\t}}\n\t\\vspace{-1.em}\n\t\\caption{Case study on universality and safeness.}\n    \\label{fig:exp_universal}\n\\end{figure}",
            "fig:exp_visual_real": "\\begin{figure}[t!]\n\t\\centering\n\t\\subfigure[musk]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.4}[0.4]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/ablation_vis_real_musk.pdf}\n\t}}\n\t\\subfigure[cpu\\_act]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.4}[0.4]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/ablation_vis_real_cpu_act.pdf}\n\t}}\n\t\\caption{Candidate points in our compact search space. The blue points refer to candidates and the red points refer to the global optimum.}\n  \\label{fig:exp_visual_real}\n\\end{figure}",
            "fig:ablation_vis_segment": "\\begin{figure}[t!]\n\t\\centering\n\t\t\\scalebox{1.0}[1.0] {\n\t\t\\includegraphics[width=1\\linewidth]{images/ablation_vis_segment.pdf}\n         }\n\t\\caption{Promising regions for source tasks and target search space when tuning on segment. The blue area refers to the promising regions of the source tasks and the search space of the target task from our method. The red point is the ground-truth optimum on that task, and the red box is the search space generated by the baseline Box.}\n    \\label{fig:ablation_vis_segment}\n\\end{figure}",
            "fig:ablation_vis_pageblocks": "\\begin{figure}[t!]\n\t\\centering\n\t\t\\scalebox{1.0}[1.0] {\n\t\t\\includegraphics[width=1\\linewidth]{images/ablation_vis_page-blocks2.pdf}\n         }\n\t\\caption{Promising regions for source tasks and target search space when tuning on page-blocks(2).}\n    \\label{fig:ablation_vis_pageblocks}\n\\end{figure}",
            "fig:ablation_method": "\\begin{figure}[t!]\n\\centering\n\t\\subfigure[Promising Region Extraction]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.47}[0.47]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_ablation_clf.pdf}\n\t\t\t\\label{fig:ab_c}\n\t}}\n\t\\subfigure[Target Search Space Generation]{\n\t\t% Requires \\usepackage{graphicx}\n\t\t\\scalebox{0.47}[0.47]{\n\t\t\t\\includegraphics[width=1\\linewidth]{images/exp_ablation_on_data20.pdf}\n\t\t\t\\label{fig:ab_e}\n\t}}\n\t\\caption{Ablation study on promising region extraction and target search space generation.}\n    \\label{fig:ablation_method}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\mathop{\\arg\\min}_{\\bm{x} \\in \\mathcal{X}}f^T(\\bm{x}),\n    \\label{eq:original}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\mathop{\\arg\\min}_{\\bm{x} \\in \\hat{\\mathcal{X}}}f^T(\\bm{x}).\n    \\label{eq:new}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq_ei}\na(\\bm{x}; M)=\\int_{-\\infty}^{\\infty} \\max(y^{\\ast}-y, 0)p_{M}(y|\\bm{x})dy,\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\begin{aligned}\n    & \\mu(\\textbf{x})=m(\\textbf{x}) + \\textbf{K}_{\\star}\\textbf{K}^{-1}(\\textbf{y} - \\textbf{m}), \\\\\n    & var(\\textbf{x})=\\textbf{K}_{\\star\\star} - \\textbf{K}_{\\star}\\textbf{K}^{-1}\\textbf{K}_{\\star}^T, \\\\\n\\end{aligned}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\small\n\\begin{aligned}\n    F(M^i;D^T)& =\\sum_{j=1}^{\\left|D^T\\right|} \\sum_{k=j+1}^{\\left|D^T\\right|} \\mathds{1}\\left(\\left(M^i(\\bm{x}_j)<M^i(\\bm{x}_k)\\right)\\otimes \\left(y_j<y_k\\right) \\right) \\\\\n    S(M^i;D^T)& =2F(M^i;D^T)/(|D^T|*(|D^T|-1)),\n\\end{aligned}\n\\label{eq:similarity}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    b^i_j=\\left\\{\n    \\begin{array}{lcr}\n     1    &    &  \\text{if} \\ y^i_j < y^i_+\\\\\n     0    &    &  \\text{if} \\ y^i_j \\geq y^i_+\n    \\end{array}\\right.,\n\\label{eq:training_data}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\small\n    \\alpha^i=\\alpha_{min}+(1-2*\\mathop{\\max}(S(M^i;D^T)-0.5,0))*(\\alpha_{max}-\\alpha_{min}),\n\\label{eq:percentile}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\hat{\\mathcal{X}}=\\mathcal{X}^m,\\quad m=\\mathop{\\arg\\max}_{i=1,...,k}S(M^i;D^T).\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    \\hat{\\mathcal{X}}=\\mathcal{X}^m, \\quad m \\sim \\mathbb{P}(.),\n\\label{eq:sample}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\hat{G}(\\bm{x}_j)=\\left\\{\n    \\begin{array}{lcl}\n     1    &    &  \\text{if} \\ \\sum_{i=1}^k G^{s_i}(\\bm{x}_j) \\geq \\lfloor \\frac{k}{2} \\rfloor\\\\\n     0    &    &  \\text{else}\n    \\end{array}\\right..\n\\label{eq:voting}\n\\end{equation}",
            "eq:11": "\\begin{equation}\nNCE(\\mathcal{X}^i_t) = \\frac{min_{\\bm{x} \\in \\mathcal{X}_t} y^{i}_{\\bm{x}} - y^{i}_{min}}{y^i_{max} - y^{i}_{min}},\n\\end{equation}"
        }
    }
}