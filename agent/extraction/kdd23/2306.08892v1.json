{
    "meta_info": {
        "title": "MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text  Classification",
        "abstract": "Prompting methods have shown impressive performance in a variety of text\nmining tasks and applications, especially few-shot ones. Despite the promising\nprospects, the performance of prompting model largely depends on the design of\nprompt template and verbalizer. In this work, we propose MetricPrompt, which\neases verbalizer design difficulty by reformulating few-shot text\nclassification task into text pair relevance estimation task. MetricPrompt\nadopts prompting model as the relevance metric, further bridging the gap\nbetween Pre-trained Language Model's (PLM) pre-training objective and text\nclassification task, making possible PLM's smooth adaption. Taking a training\nsample and a query one simultaneously, MetricPrompt captures cross-sample\nrelevance information for accurate relevance estimation. We conduct experiments\non three widely used text classification datasets across four few-shot\nsettings. Results show that MetricPrompt outperforms manual verbalizer and\nother automatic verbalizer design methods across all few-shot settings,\nachieving new state-of-the-art (SOTA) performance.",
        "author": "Hongyuan Dong, Weinan Zhang, Wanxiang Che",
        "link": "http://arxiv.org/abs/2306.08892v1",
        "category": [
            "cs.CL"
        ],
        "additionl_info": "Accepted at KDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nSince unstructured text data takes up over 80\\% information in our society, text mining is believed to have significant commercial value~\\cite{korde2012text}.\n% ~\\cite{korde2012text}. \nText classification is regarded as a fundamental and essential task in text mining, and the related techniques are used in various kinds of text mining applications, such as information retrieval~\\cite{hoogeveen2018web, dwivedi2016automatic}, sentiment analysis~\\cite{liu2012survey, pang2008opinion}, recommendation system~\\cite{aggarwal2016content}, knowledge management~\\cite{sumathy2013text}, document summarization~\\cite{cao2017improving}, etc.\n% As revealed in neuroscience and psychological research, human intelligence has impressive ability to extract task-related information and adapt to a new task with only few labeled samples. \n% The capability to learn from few labeled samples is critical for general intelligence.\nRecently proposed pre-trained language models (PLMs) achieve satisfactory text classification performance under data-rich setting~\\cite{radford2018improving, radford2019language, devlin2019bert, raffel2019exploring, brown2020language}, but these models' Few-Shot Learning (FSL) ability still lags far behind human intelligence~\\cite{brown2020language}. \n% Recently proposed Natural Language Processing (NLP) models, especially those pre-trained on large scale corpus~\\cite{radford2018improving, radford2019language, devlin2019bert, raffel2019exploring, brown2020language}, achieve satisfactory text classification performance under data-rich setting, but these models' Few-Shot Learning (FSL) ability still lags far behind human intelligence. \n% For example, GPT-3 shows human-level language abilities on SuperGLUE dataset, but suffers a 20\\% performance drop when it comes to few-shot setting~\\cite{brown2020language}. \n\n\n% \\begin{figure}[t]\n% \\centering\n% \\begin{tikzpicture}\n% \\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=\\columnwidth, trim={0cm 5.5cm 13.5cm 0cm}, clip]{./figs/verbalizer_design.pdf}};\n% \\end{tikzpicture}\n% \\caption{\n% A comparison across verbalizer design methods. \n% CE stands for Cross-Entropy loss and PCL is Prototypical Contrastive Learning loss.\n% }\\label{fig:intro}\n% \\end{figure}\n\n\n\n% \\begin{figure*}[t]\n% \\centering\n% \\begin{tikzpicture}\n% \\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=2\\columnwidth, trim={0cm 0cm 7.5cm 0cm}, clip]{./figs/verbalizer_design_long_wide.pdf}};\n% \\end{tikzpicture}\n% \\caption{\n% A comparison across verbalizer design methods. \n% CE stands for Cross-Entropy loss and PCL is Prototypical Contrastive Learning loss.\n% }\\label{fig:intro}\n% \\end{figure*}\n\nPrompting methods are proposed to better utilize PLM's general knowledge by aligning downstream tasks to its pre-training objective. \nPrompting method inserts sample text into prompt template to form prompted text. \n% Taking news topic classification task as an example, if want to classify news \"Messi won the World Cup\" with a Masked Language Model (MLM), we can insert it into a prompt template \"\\texttt{<text>} The topic of the news is \\texttt{[MASK]}\". \n% In this way, news topic classification task is reformulated as an MLM problem.\n% We can obtain the classification result by mapping the output word at \\texttt{[MASK]} position to topic labels. \nPrompting model takes as input the prompted text, and the result is obtained by projecting the model's output words to corresponding labels. \n% Label mapping is conducted with a \\textit{verbalizer}, which computes each label's probability by aggregating its corresponding answer words output probability. \n% Verbalizer directly maps PLMs' output words to corresponding labels, and therefore influence prompting models' performance greatly. \nLabel mapping is conducted by \\textit{verbalizer}, which serves as a critical part of the prompting model and determines its performance. \n\n\n\nAlthough achieving promising results in a wide variety of text mining tasks, prompting methods are very susceptible to sub-optimal verbalizer design~\\cite{gao2020making}.\nHowever, designing a proper verbalizer requires deep understanding of both downstream task and PLM's inner mechanism. \nIn practice, this procedure can be extremely time and resource-consuming. \n% As a result, many researchers seek to automate the verbalizer design process and achieve better results. \n% Automatic verbalizer design methods can be classified in 2 genres: discrete verbalizer design and soft verbalizer design~\\cite{liu2021pre}.\nTo this end, automatic verbalizer design methods are proposed to ease the difficulty.\nThese algorithms can be classified into discrete verbalizer design and soft verbalizer design~\\cite{liu2021pre} methods. \nDiscrete verbalizer design methods, such as AVS~\\cite{schick2021eacl}, LM-BFF~\\cite{gao2020making} and AutoPrompt~\\cite{shin2020autoprompt}, search each label's corresponding answer words in PLM's vocabulary to build the verbalizer. \nSoft verbalizer design methods like WARP~\\cite{hambardzumyan2021warp} and ProtoVerb~\\cite{cui2022prototypical} search for proper verbalizer parameters in an infinite continuous space and thus achieve better performance.\n% with large PLMs, gradient-based algorithms, etc. \n% These methods reduce human labor required by verbalizer design significantly, but their performance lags far behind carefully hand-crafted ones~\\cite{cui2022prototypical}. \n% Soft verbalizer design methods are proposed to search for proper verbalizer parameters in an infinite continuous space.\n% Expanding the possibilities of verbalizer design space, these methods achieves better results than discrete verbalizer design algorithms~\\cite{hambardzumyan2021warp, cui2022prototypical}. \n\nDespite the promising prospects of soft verbalizer, current methods' performance is still far from satisfactory. \nThe main reason is that these methods' optimization and inference formulations are distinct from PLM's pre-training objective. \n% WARP~\\cite{hambardzumyan2021warp} trains each label's soft embedding from scratch on few labeled samples of the downstream task, while ProtoVerb~\\cite{cui2022prototypical} takes prompting model as a feature extractor and aggregates each training sample's representation at \\texttt{[MASK]} position to form corresponding label's prototypical embedding. \nAs illustrated in Fig~\\ref{fig:intro}, WARP~\\cite{hambardzumyan2021warp} and ProtoVerb~\\cite{cui2022prototypical} introduce task-specific label embeddings and use PLM's inner representation to make predictions. \n% WARP~\\cite{hambardzumyan2021warp} trains each label's soft embedding from scratch, while ProtoVerb~\\cite{cui2022prototypical} extracts training samples' representation at \\texttt{[MASK]} position to form each label's prototypical embedding. \nAlthough adopting prompting method, both methods force PLM to adapt to a distinct task formulation from its pre-training objective which only operates on output word probabilities.\nWhat's worse, these label embeddings have to be trained from scratch in downstream tasks, leading to severe over-fitting problem.\nAs a result, PLM cannot adapt to downstream tasks smoothly. \n% WARP~\\cite{hambardzumyan2021warp} introduce additional parameter as each label's embedding. These parameters have to be trained from scratch on few labeled samples of the downstream task, and insufficient joint training therefore leads to sub-optimal performance. \n% Another representative soft verbalizer method ProtoVerb~\\cite{cui2022prototypical} takes prompting model as a feature extractor, and aggregates each training sample's representation at \\texttt{[MASK]} position to form corresponding label's prototypical embedding. \n% ProtoVerb conduct optimization on PLM's inner representations to obtain each label's prototype, which is distinct from PLM's pre-training objectives.\n% Thus, PLM cannot function as it did in the pre-training phase and consequently cannot adapt to downstream tasks smoothly. \n\n% What's worse, both WARP and ProtoVerb compute each label and query sample's representation respectively. \n% As a result, the model cannot analyze cross-sample relevance information between training and query samples to aid classification.\n\nTo tackle the above issues, we propose MetricPrompt, which frees human labor from task-specific verbalizer design by reformulating few-shot text classification task into text pair relevance estimation task. \nWe re-organize training data into text pairs, and adopt prompting model to learn the relevance metric. \nThe learned metric is then used to evaluate each query sample's relevance with training samples, and the classification result is obtained by pooling the estimated relevance scores.\n% are classified according to their relevance with each training sample.\nAs shown in Fig~\\ref{fig:intro}, an explicit task-specific verbalizer is no longer required in our method.\nFollowing PLM's pre-training objective, MetricPrompt only operates with PLM's output word probabilities, and thus enabling smooth adaption to downstream tasks. \n% which learns a relevance metric fulfilled by prompting model instead of learning a verbalizer explicitly. \n% which takes prompting model as a relevance metric to estimate the relevance between query and training samples directly, and therefore requires no manual verbalizer design. \nTo produce accurate relevance estimations, MetricPrompt takes text pairs as input and aid estimation accuracy with cross-sample relevance information. \n% MetricPrompt adopts PLM's pre-training objective to conduct optimization and inference on few-shot text classification tasks, and thus realizing more smooth adaption to downstream tasks.\n% Moreover, training and query samples are fed to prompting model simultaneously, aiding classification tasks with cross-sample relevance information to make more accurate predictions. \nExperiments on three widely used few-shot text classification datasets across four few-shot settings indicate that MetricPrompt achieves the highest few-shot text classification accuracy over previous SOTA verbalizer design baselines.\n\nWe summarize the contribution of this paper as below:\n\n\\textbf{(1)} We propose a novel prompting method MetricPrompt, which eases task-specific verbalizer design difficulty by reformulating few-shot classification task into relevance estimation problem and learning the relevance metric with prompting model.\n\n\\textbf{(2)} We conduct experiments on three widely used few-shot text classification datasets with four few-shot settings, and results show that MetricPrompt outperforms all automatic verbalizer baselines and even manual verbalizer which requires heavy human labor in task-specific verbalizer design.\n\n\\textbf{(3)} We provide analysis to demonstrate the extensibility and robustness of MetricPrompt, and explain its performance variance when equipped with different pooling methods.\n\n% \\begin{itemize}\n%     \\item We propose a novel prompting method MetricPrompt, which eases task-specific verbalizer design difficulty by reformulating few-shot classification task into relevance estimation problem and learning the relevance metric with prompting model.\n%     % MetricPrompt further bridges the gap between PLM's pre-training objective and few-shot text classification task's formulation, and introduces cross-sample relevance information to improve classification accuracy. \n%     \\item We conduct experiments on three widely used few-shot text classification datasets with four few-shot settings, and results show that MetricPrompt outperforms all automatic verbalizer baselines and even manual verbalizer which requires heavy human labor in task-specific verbalizer design.\n%     % achieving new SOTA performance. \n%     \\item We provide analysis to demonstrate the extensibility and robustness of MetricPrompt, and explain its performance variance when equipped with different pooling methods.\n%     % demonstrate the extensibility and robustness of MetricPrompt, and analyze its behaviors when equipped \n% \\end{itemize}\n\nAll code and data will be publicly available at \\href{https://github.com/Dousia/MetricPrompt}{https://github.com/\\\\Dousia/MetricPrompt}.\n\n\n\n% \\section{Preliminaries and Related Work}\n% In this section, we review recent works related to the proposed method, and introduce preliminaries revolving prompting models and verbalizer design techniques. \n\n\n"
            },
            "section 2": {
                "name": "Preliminaries and Related Work",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Prompting methods",
                    "content": "\nTraditionally, PLM is adapted to downstream tasks via vanilla fine-tuning. \nAs shown in Fig~\\ref{fig:background}, fine-tuning pools PLM's last layer hidden states to form a sentence representation, and makes predictions with a task-specific classification head. \nFine-tuning is effective given sufficient training data, but its performance degrades significantly under few-shot scenario. \nThe newly initialized task-specific head is prone to over-fitting problem, and the gap between downstream task formulation and the pre-training objective hinders PLM's smooth adaption. \n\nPrompting methods are proposed to reformulate downstream tasks to enable PLM's smooth adaption to new tasks. \nA prompting model's pipeline is illustrated in Fig~\\ref{fig:background}.\nDenoting $p(\\cdot)$ as the prompting function which fills the input text $\\mathbf{x}$ into a prompt template, prompting model takes the prompted text and produces output word probability distribution over the vocabulary:\n\\begin{equation}\n\\label{eq: background_1}\nf_{vocab}(p(\\mathbf{x}); \\theta) = P(p(\\mathbf{x}); \\theta), \n\\end{equation}\nwhere $P(\\cdot)$ is a PLM parameterized by $\\theta$. \nThe output word probability is then mapped to final predicted probability distribution over classes with a verbalizer $v(\\cdot)$:\n\\begin{equation}\n\\label{eq: background_2}\nf_{cls}(p(\\mathbf{x}); \\theta) = v(f_{vocab}(p(\\mathbf{x}); \\theta)).  \n\\end{equation}\n% Taking news topic classification task as an example, if we want to classify news ``Messi won the World Cup\" with a Masked Language Model (MLM), we can prompt it with a template ``\\texttt{<text>} The topic of the news is \\texttt{[MASK]}\". \n% A verbalizer maps the output word at \\texttt{[MASK]} position to topic labels.\n% For instance, `sports' and `athlete' can be mapped to sports topic, while `diplomacy' and `government' can be aligned to politics topic. \n\nFirst proposed by~\\citeauthor{radford2019language}, prompting methods have been used in a variety of text mining tasks, such as text classification~\\cite{puri2019zero, schick2021eacl, PET, P-tuning}, text generation~\\cite{schick2020few, li2021prefix, chen2021evaluating}, named entity recognition~\\cite{cui2021template, hou2022inverse}, knowledge probing~\\cite{petroni2019language, talmor2020olmpics}, etc. \nPrompting model alleviates the over-fitting problem under few-shot scenario significantly~\\cite{brown2020language, PET, tam2021improving}. \nHowever, prompting model's performance relies largely on the selection of prompt template and verbalizer~\\cite{liu2021pre}. \nAlthough a number of works focus on the design of prompt~\\cite{gao2020making, shin2020autoprompt, li2021prefix, P-tuning}, less are proposed to ease verbalizer design difficulty. \nVerbalizer maps prompting model's output words to classification results directly, and therefore influences prompting model's performance significantly~\\cite{gao2020making}.\nIn this work, we seek to free human labor from verbalizer design with less performance loss.\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Verbalizer Design Methods",
                    "content": "\n% \\begin{figure*}[t]\n% \\centering\n% \\begin{tikzpicture}\n% \\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=2\\columnwidth, trim={0cm 1cm 2cm 0cm}, clip]{./figs/MetricPrompt.pdf}};\n% \\end{tikzpicture}\n% \\caption{\n% A demonstration of Metricprompt's data construction and functioning procedure. \n% }\\label{fig:method}\n% \\end{figure*}\n\nCurrent verbalizer design methods can be classified into manual verbalizer design, discrete verbalizer design and soft verbalizer design. \nA carefully hand-crafted verbalizer can achieve highly competitive performance in various text mining tasks~\\cite{PET, schick2021eacl}. \nHowever, designing such a well-performing verbalizer relies heavily on human's accurate understanding of the target task and requires heavy trial-and-error work. \nDiscrete verbalizer design methods frees human labor from tedious and time-consuming verbalizer design process. \nLM-BFF~\\cite{gao2020making} generates proper answer words with large PLMs such as T5~\\cite{raffel2019exploring}.\nAutoPrompt~\\cite{shin2020autoprompt} and AVS~\\cite{schick2021eacl} initialize a verbalizer and optimize it to meet a predefined criteria iteratively. \nPETAL~\\cite{schick2020coling} searches for answer words that maximize the likelihood of the training data.\nThese methods reduce human labor required by verbalizer design significantly, but their performance lags far behind carefully hand-crafted ones~\\cite{cui2022prototypical}. \nTo achieve better performance, soft verbalizer design methods render answer words from a fixed vocabulary as differentiable label embeddings represented in an infinite continuous space. \nExpanding the possibilities of verbalizer design space, these methods achieve better results than discrete verbalizer design algorithms~\\cite{hambardzumyan2021warp, zhang2021differentiable, cui2022prototypical}. \n% However, task-specific label embeddings are prone to over-fitting problem, and these methods functions in a way distinct from PLM' pre-training objective. \n\nIn this work, we ease task-specific verbalizer design difficulty by reformulating text classification task into text pair relevance estimation task.\nIn this way, no additional task-specific parameter is introduced, and the over-fitting problem is alleviated. \nAdopting prompting model as the relevance metric, PLM can adapt to new tasks more smoothly.\nA recent work also views text classification task as natural language inference problem~\\cite{plaza2022natural}, but it focuses on zero-shot scenario and hand-craft each label's description.\nIn comparison, our method tackle few-shot text classification tasks instead of zero-shot ones, and does not require human labor in task-specific verbalizer design. \n\n\n\n% Prompting methods are proposed to better utilize Pre-trained Language Models' (PLM) general knowledge in downstream NLP tasks by aligning them to PLMs'  pre-training objectives. \n% Prompting method insert sample news text into prompt template to form prompted text. \n% Taking news topic classification task as an example, if want to classify news ``Messi won the World Cup\" with a Masked Language Model (MLM), we can insert it into a prompt template ``\\texttt{<text>} The topic of the news is \\texttt{[MASK]}\". \n% In this way, news topic classification task is reformulated as an MLM problem.\n% We can obtain the classification result by mapping the output word at \\texttt{[MASK]} position to topic labels. \n% Label mapping is conducted with a \\textit{verbalizer}, which computes each label's probability by aggregating its corresponding answer words output probability. \n% % Verbalizer directly maps PLMs' output words to corresponding labels, and therefore influence prompting models' performance greatly. \n\n% Although achieving promising results a a wide variety of NLP tasks, prompt methods are very susceptible to sub-optimal verbalizer design~\\cite{gao2020making}.\n% However, designing a proper verbalizer requires deep understanding of both downstream NLP task and PLM's inner mechanism. \n% In practice, this procedure can be extremely resource-consuming. \n% As a result, many researchers seek to automate the verbalizer design process and achieve better results. \n% Automatic verbalizer design methods can be classified in 2 genres: discrete verbalizer design and soft verbalizer design~\\cite{liu2021pre}. \n% Discrete verbalizer design methods, such as LM-BFF~\\cite{gao2020making} and AutoPrompt~\\cite{shin2020autoprompt}, search each label's corresponding answer words with large PLMs, gradient-based algorithms, etc. \n% These methods reduce human labor required by verbalizer design significantly, but their performance lags far behind carefully hand-crafted ones~\\cite{cui2022prototypical}. \n% Soft verbalizer design methods are proposed to search for proper verbalizer parameters in an infinite continuous space. Expanding the possibilities of verbalizer design space, these methods achieves better results than discrete verbalizer design algorithms~\\cite{hambardzumyan2021warp, cui2022prototypical}. \n\n% Despite the promising prospects of soft verbalizer, current methods' performance are still far from satisfactory. \n% The first reason is that current soft verbalizer methods' optimization and inference formulation are distinct from PLM's pre-training objective. \n% WARP~\\cite{hambardzumyan2021warp} trains each label's soft embedding from scratch on few labeled samples of the downstream task, while ProtoVerb~\\cite{cui2022prototypical} takes prompting model as a feature extractor and aggregates each training sample's representation at \\texttt{[MASK]} position to form corresponding label's prototypical embedding. \n% Both methods force PLM to adapt to a distinct task formulation from its pre-training objective, and therefore hinder smooth adaption to downstream tasks. \n% What's worse, both WARP and ProtoVerb compute each label and query sample's representation respectively. \n% As a result, the model cannot analyze cross-sample relevance information between training and query samples to aid classification.\n\n\n\n\n\n\n\n\n\n\n\n\n% \\subsection{Data construction}\n% Given a few-shot text classification dataset $\\mathcal{D}$, we denote training data with $\\mathcal{D}_{t}$ and query samples with $\\mathcal{D}_{q}$. \n% A sample is formulated as $d=(\\mathbf{x}_d, \\mathbf{y}_d)$, where $\\mathbf{x}_d$ stands for sample text and $\\mathbf{y}_d$ represents its label. \n% % Since MetricPrompt takes as input a pair of sample text, we construct training data as follows:\n% We construct training data as follows:\n% \\begin{equation}\n% \\label{eq: data_construction}\n% \\begin{split}\n% &\\mathcal{D}_{t}^{M} = \\bigcup_{(d_i, d_j) \\in \\mathcal{D}_{t} \\times \\mathcal{D}_{t}} \\{ (p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}), \\mathbf{y}_{ij}) \\}, \\\\\n% &\\mathcal{D}_{q}^{M} = \\bigcup_{(d_i, d_j) \\in \\mathcal{D}_{q} \\times \\mathcal{D}_{t}} \\{ (p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}), \\mathbf{y}_{ij}) \\},\n% \\end{split}\n% \\end{equation}\n% where $p(\\cdot, \\cdot)$ is MetricPrompt's prompting function. \n% It takes two pieces of sample text and produces a filled prompt template. \n% % We use ``$\\mathbf{x}_{d_i}$ \\texttt{[SEP]} A news of \\texttt{[MASK]} topic: $\\mathbf{x}_{d_j}$\" as the prompt template. \n% $\\mathbf{y}_{ij}=\\mathbb{I}(\\mathbf{y}_{d_i}=\\mathbf{y}_{d_j})$ indicates whether the pair of text are of the same class.\n\n% % Similarly, we build query data for MetricPrompt as follows:\n% % \\begin{equation}\n% % \\label{eq: data_construction_query}\n% % \\mathcal{D}_{q}^{M} = \\bigcup_{(d_i, d_j) \\in \\mathcal{D}_{q} \\times \\mathcal{D}_{t}} \\{ (p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}), \\mathbf{y}_{ij}) \\}.\n% % \\end{equation}\n\n% The overall data construction process is illustrated in Fig~\\ref{fig:method_1}.\n% We reorganize the original classification problem to be a relevance estimation task, requiring no label's semantic representation or human labor in task-specific verbalizer design.\n% % We reorganize single samples of a given few-shot text classification task into paired samples, and the original multi-class classification problem is rendered as a relevance estimation task. \n% % No label's semantic representation is involved in this data construction procedure, and therefore MetricPrompt requires no human labor in task-specific verbalizer design.\n\n\n% \\subsection{Optimization}\n\n% % \\begin{figure*}[t]\n% % \\centering\n% % \\begin{tikzpicture}\n% % \\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=2\\columnwidth, trim={0cm 1cm 2cm 9cm}, clip]{./figs/MetricPrompt.pdf}};\n% % \\end{tikzpicture}\n% % \\caption{\n% % The inference procedure of MetricPrompt. \n% % A query sample is paired with each training sample, and the relevance scores are pooled to produce final classification probabilities.\n% % }\\label{fig:method_2}\n% % \\end{figure*}\n\n% MetricPrompt adopts a generalized (M)LM task for optimization, which is consistent with PLM's pre-training objective. \n\n% % Let $P(\\cdot;\\theta)$ be an MLM model parameterized by $\\theta$ and $f_{vocab}(\\cdot;\\theta)$ be its output word probability over the vocabulary.\n% We define the optimization objective of MetricPrompt as follows:\n% \\begin{equation}\n% \\label{eq: optimization}\n% \\small\n% \\begin{split}\n% \\hat{\\theta} \n% &=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{d^M \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}(f_{vocab}(\\mathbf{x}_{d^M};\\theta), \\mathbf{y}_{d^M}) \\\\\n% &=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{d^M \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}_{CE}(f_{cls}(\\mathbf{x}_{d^M};\\theta)), \\phi(\\mathbf{y}_{d^M})), \n% \\end{split}\n% \\end{equation}\n% where $\\phi(\\cdot)$ is the probability distribution over label categories. \n% The corresponding position of the sample label is set as 1 while others are set to be 0. \n% Output word probability $f_{vocab}(\\mathbf{x}_{d^M}; \\theta)$ is mapped to a binomial distribution $f_{cls}(\\mathbf{x}_{d^M}; \\theta)$ with a predefined task-general meta verbalizer $v(\\cdot)$. \n% We task this meta verbalizer to aggregate logits at $\\texttt{\\{relevant}, \\texttt{similar}, \\texttt{consistent\\}}$ to be the predicted logit of label 1, while logits at $\\texttt{\\{irrelevant}, \\texttt{inconsistent}, \\texttt{different\\}}$ are aggregated to be the logit of label 0.\\footnote{Note that this meta verbalizer can be used in all few-shot text classification tasks, so MetricPrompt requires no extra human labor in task-specific verbalizer design.}\n% $\\mathcal{L}$ is the loss function of MetricPrompt, which is defined as the cross-entropy loss between the probability distributions produced by the meta verbalizer $v(\\cdot)$ and the ground truth distribution $\\phi(\\mathbf{y}_{d^M})$. \n\n% MetricPrompt formulates few-shot text classification task's optimization objective to be a generalized (M)LM task which only involves output word probabilities. \n% % which is the minimization of cross-entropy loss between predicted word probability at \\texttt{[MASK]} position and the ground truth one-hot distribution. \n% % The optimized prompting model can be used as a metric to estimate the relevance between two sample text. \n\n% % adapt epochs?\n\n% \\subsection{Inference}\n% After optimization, the prompting model serves as a relevance metric during inference. \n% As illustrated in Fig~\\ref{fig:method_1}, we take an original query sample $d_q$ colored in black and pair it with all training samples colored differently to form inference samples.\n% % its paired training samples in other colors to illustrate MetricPrompt's inference process. \n% Given an original training sample $d_i$, MetricPrompt computes its relevance score $s_{d_i}$ with $d_q$ as follows:\n% \\begin{equation}\n% \\label{eq: inference_1}\n% s_{d_i} = \\Delta(f_{cls}(p(\\mathbf{x}_{d_q}, \\mathbf{x}_{d_i}); \\hat{\\theta})), \n% \\end{equation}\n% where $\\Delta(\\cdot)$ computes the difference between the binomial distribution's probability at 1 and 0. \n% We denote $l$ as a label of the few-shot text classification task. \n% Let $\\mathcal{D}_{l}=\\{d_i | d_i \\in \\mathcal{D}_{t}, \\mathbf{y}_{d_i}=l_i\\}$, MetricPrompt calculates $l$'s classification score $s_{l}$ by pooling its corresponding samples' relevance with $d_q$:\n% \\begin{equation}\n% \\label{eq: inference_2}\n% s_{l} \n% = \\sum_{d_i \\in \\mathcal{D}_{l}} s_{d_i} / |\\mathcal{D}_{l}|. \n% \\end{equation}\n% Finally, MetricPrompt selects label $\\hat{l}$ with the highest relevance score as the classification result:\n% \\begin{equation}\n% \\label{eq: inference_3}\n% \\hat{l} \n% = \\mathop{\\arg\\max}\\limits_{l} s_l. \n% \\end{equation}\n\n% We present MetricPrompt with sum pooling in Equation~\\ref{eq: inference_2}.\n% This pooling function can also be replaced by max pooling and K-Nearest-Neighborhood (KNN)~\\cite{cover1967nearest} pooling. \n% Max pooling classifies query sample $d_q$ into its most relevant training sample's class by replacing Equation~\\ref{eq: inference_2} with:\n% \\begin{equation}\n% \\label{eq: inference_4}\n% s_{l} \n% = \\max_{d_i \\in \\mathcal{D}_{l}} s_{d_i}. \n% \\end{equation}\n% For KNN pooling, we denote $\\mathcal{D}_{topk}$ as $k$ training samples most relevant to $d_q$ and reformulate Equation~\\ref{eq: inference_2} as:\n% \\begin{equation}\n% \\label{eq: inference_5}\n% s_{l} \n% = |\\{d_i | d_i \\in \\mathcal{D}_{topk}, \\mathbf{y}_{d_i}=l_i\\}|. \n% \\end{equation}\n% We set $k$ as half the size of training set. \n% When several labels appear in $\\mathcal{D}_{topk}$ for the same number of times, the query sample is classified to the label of its most relevant training sample. \n% % we select the sample obtaining the highest relevance score from these labels' corresponding samples, and classify $d_q$ to its class.\n% % we select the training sample of the highest relevance score with $d_q$ and classify $d_q$ to its class.\n\n% MetricPrompt renders prompting model as a relevance metric, and classifies query samples according to their relevance with training samples of the few-shot task. \n% Taking paired text as input, MetricPrompt uses cross-sample information to aid relevance estimation accuracy. \n% % Prompting model takes two pieces of sample text at once. \n% % Thus, MetricPrompt is able to use cross-sample information to estimate relevance and make predictions accurately. \n\n\n% \\subsection{More Efficient Inference}\n% % MetricPrompt pairs each query sample with all training samples, leading to relatively high inference cost. \n% To improve the efficiency of MetricPrompt, we propose to use pivot samples to reduce inference time complexity. \n% We use the optimized prompting model to select representative training samples. \n% For a training sample $d_i$ labeled with $l$, its representativeness $r_{d_i}$ as follows:\n% \\begin{equation}\n% \\small\n%     r_{d_i} \n%     = \\frac{\\sum_{d_j \\in \\mathcal{D}_l}{s_{d_j}}}{|\\{d_j|d_j \\in \\mathcal{D}_l\\}|} - \\frac{\\sum_{d_k \\in \\mathcal{D}_t-\\mathcal{D}_l}{s_{d_k}}}{|\\{d_k|d_k \\in \\mathcal{D}_t-\\mathcal{D}_l\\}|}. \n% \\end{equation}\n% $s_{d_j}$ represents the relevance score between samples $d_j$ and $d_i$. \n% Based on this representativeness metric, we select top $p$ representative samples for each label.\n% These samples are marked as pivot samples.\n% % and participate in the inference process. \n% During inference, query samples are only paired with these pivot samples to make classification prediction.\n\n% Pivot samples reduce the time complexity of MetricPrompt's inference process significantly. \n% Assume a few-shot text classification task with $n$ labels and $k$ samples per label.\n% The introduction of pivot samples reduce the number of each query sample's text pairs for relevance estimation from $n*k$ to $n*p$.\n% Accordingly, $O(n*k)$ inference time complexity is reduced to $O(n*p)$.\n% Since $p$ is a pre-defined constant, the improved inference time complexity is $O(n)$, which is equivalent with vanilla prompting method where the input text representation is dot multiplied with each label word's embedding. \n% We set the number of pivot samples per label $p$ to 2 in the experiments. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Methods",
                "content": "\n% In this section, we introduce MetricPrompt, which frees human labor from task-specific verbalizer design by rendering prompting model as a metric to estimate the relevance between training and query samples, and making classification predictions with the estimations.\n% Below we will describe the data construction, optimization and inference procedure of MetricPrompt in detail. \n\nIn this section, we introduce the data construction, optimization and inference procedure of MetricPrompt in detail. \n\n\n",
                "subsection 3.1": {
                    "name": "Data construction",
                    "content": "\nGiven a few-shot text classification dataset $\\mathcal{D}$, we denote training data with $\\mathcal{D}_{t}$ and query samples with $\\mathcal{D}_{q}$. \nA sample is formulated as $d=(\\mathbf{x}_d, \\mathbf{y}_d)$, where $\\mathbf{x}_d$ stands for sample text and $\\mathbf{y}_d$ represents its label. \nSince MetricPrompt takes as input a pair of sample text, we construct training data as follows:\n\\begin{equation}\n\\label{eq: data_construction_train}\n\\mathcal{D}_{t}^{M} = \\bigcup_{(d_i, d_j) \\in \\mathcal{D}_{t} \\times \\mathcal{D}_{t}} \\{ (p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}), \\mathbf{y}_{ij}) \\},\n\\end{equation}\nwhere $p(\\cdot, \\cdot)$ is MetricPromt's prompting function. It takes two pieces of sample text and produces a filled prompt template with the given text. \nWe use ``$\\mathbf{x}_{d_i}$ \\texttt{[SEP]} A news of \\texttt{[MASK]} topic: $\\mathbf{x}_{d_j}$\" as the prompt template. \n$\\mathbf{y}_{ij}=\\mathbb{I}(\\mathbf{y}_{d_i}=\\mathbf{y}_{d_j})$ indicates whether the pair of text are of the same class.\n\nSimilarly, we build query data for MetricPrompt as follows:\n\\begin{equation}\n\\label{eq: data_construction_query}\n\\mathcal{D}_{q}^{M} = \\bigcup_{(d_i, d_j) \\in \\mathcal{D}_{q} \\times \\mathcal{D}_{t}} \\{ (p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}), \\mathbf{y}_{ij}) \\}.\n\\end{equation}\n\n\n\nThe overall data construction process is illustrated in Fig~\\ref{fig:method_1} and Fig~\\ref{fig:method_2}.\nWe reorganize single samples of a given few-shot text classification task into paired samples, and the original multi-class classification problem is rendered as a relevance estimation task. \nNo label's semantic representation is involved in this data construction procedure, and therefore MetricPrompt requires no human labor in task-specific verbalizer design.\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Optimization",
                    "content": "\nMetricPrompt differs from previous soft verbalizer design methods for we do not introduce task specific label embeddings, but instead reformulate few-shot text classification task as a text pair relevance estimation task to let loose the need of task-specific verbalizer.\nBy reformulating the task, MetricPrompt's optimization coincides with PLM's pre-training objectives.\n% We design a prompt template and a verbalizer for MetricPrompt \n% Given reorganized training set $\\mathcal{D}_{t}^{Metric}$, we define the optimization \n\nLet $P(\\cdot;\\theta)$ be an MLM model parameterized by $\\theta$ and $f_{vocab}(\\cdot;\\theta)$ be its output word probability over the vocabulary at \\texttt{[MASK]} position. \n% It takes as input a piece of prompted text and predict the probability distribution over the vocabulary of the output word at \\texttt{[MASK]} position. \nWe define the optimization objective of MetricPrompt as follows:\n\\begin{equation}\n\\label{eq: optimization}\n\\begin{split}\n\\hat{\\theta} \n&=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{d^M \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}(f_{vocab}(\\mathbf{x}_{d^M};\\theta), \\mathbf{y}_{d^M}) \\\\\n&=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{d^M \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}_{CE}(v(f_{vocab}(\\mathbf{x}_{d^M};\\theta)), \\phi(\\mathbf{y}_{d^M})), \\\\\n&=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{d^M \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}_{CE}(f_{cls}(\\mathbf{x}_{d^M};\\theta)), \\phi(\\mathbf{y}_{d^M})), \n\\end{split}\n\\end{equation}\n% \\begin{equation}\n% \\label{eq: optimization}\n% \\begin{split}\n% \\hat{\\theta} \n% &=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}, \\mathbf{y}_{ij}) \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}(f_{vocab}(p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j};\\theta)), \\mathbf{y}_{ij}) \\\\\n% &=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}, \\mathbf{y}_{ij}) \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}_{CE}(v(f_{vocab}(p(\\mathbf{x}_{d_j}, \\mathbf{y}_{ij};\\theta))), \\phi(\\mathbf{y}_{ij})), \\\\\n% &=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}, \\mathbf{y}_{ij}) \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}_{CE}(f_{cls}(p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j};\\theta))), \\phi(\\mathbf{y}_{ij})), \n% \\end{split}\n% \\end{equation}\nwhere $\\phi(\\cdot)$ stands for a probability distribution over label categories. \nThe corresponding position of the input sample's label is set as 1 while others are set to be 0. \n$v(\\cdot)$ represents a predefined task-general meta verbalizer, which projects output word probability $f_{vocab}(\\cdot;\\theta)$ to a binomial distribution $f_{cls}(\\cdot;\\theta)$. \nWe task this meta verbalizer to aggregate logits at $\\texttt{\\{relevant}, \\texttt{similar}, \\texttt{consistent\\}}$ to be the predicted logit of label 1, while logits at $\\texttt{\\{irrelevant}, \\texttt{in-}$ $\\texttt{consistent}, \\texttt{different\\}}$ are aggregated to be the logit of label 0.\\footnote{Note that this meta verbalizer can be used in all few-shot text classification tasks, so MetricPrompt requires no extra human labor in task-specific verbalizer design.}\n$\\mathcal{L}$ is the loss function of MetricPrompt, which is defined as the cross-entropy loss between the probability distributions produced by the meta verbalizer $v(\\cdot)$ and the ground truth distribution. \n\nMetricPrompt formulates few-shot text classification task's optimization objective to be a generalized MLM task, which is the minimization of cross-entropy loss between predicted word probability at \\texttt{[MASK]} position and the ground truth one-hot distribution. The optimized prompting model can be used as a metric to estimate the relevance between two sample text. \n\n% adapt epochs?\n\n"
                },
                "subsection 3.3": {
                    "name": "Inference",
                    "content": "\nAfter optimization, the prompting model serves as a relevance metric during inference. \nAs illustrated in Fig~\\ref{fig:method_2}, we take an original query sample $d_q$ colored in black and pair it with all training samples colored differently to form inference samples.\n% its paired training samples in other colors to illustrate MetricPrompt's inference process. \nGiven an original training sample $d_i$, MetricPrompt computes its relevance score $s_{d_i}$ with $d_q$ as follows:\n\\begin{equation}\n\\label{eq: inference_1}\ns_{d_i} = \\Delta(f_{cls}(p(\\mathbf{x}_{d_q}, \\mathbf{x}_{d_i}); \\hat{\\theta})), \n\\end{equation}\nwhere $\\Delta(\\cdot)$ computes the difference between the binomial distribution's probability at 1 and 0. \nWe denote $l$ as a label of the few-shot text classification task. \nLet $\\mathcal{D}_{l}=\\{d_i | d_i \\in \\mathcal{D}_{t}, \\mathbf{y}_{d_i}=l_i\\}$, MetricPrompt calculates $l$'s classification score $s_{l}$ by pooling its corresponding samples' relevance with $d_q$:\n\\begin{equation}\n\\label{eq: inference_2}\ns_{l} \n= \\sum_{d_i \\in \\mathcal{D}_{l}} s_{d_i} / |\\mathcal{D}_{l}|. \n\\end{equation}\nFinally, MetricPrompt selects the label $\\hat{l}$ with the highest the relevance score as the classification result:\n\\begin{equation}\n\\label{eq: inference_3}\n\\hat{l} \n= \\mathop{\\arg\\max}\\limits_{l} s_l. \n\\end{equation}\n\nWe present MetricPrompt with sum pooling in Equation~\\ref{eq: inference_2}.\nThis pooling function can also be replaced by max pooling and K-Nearest-Neighborhood (KNN)~\\cite{cover1967nearest} pooling. \nMax pooling classifies query sample $d_q$ into its most relevant training sample's class. \nMetricPrompt works with max pooling by replacing Equation~\\ref{eq: inference_2} with:\n\\begin{equation}\n\\label{eq: inference_4}\ns_{l} \n= \\max_{d_i \\in \\mathcal{D}_{l}} s_{d_i}. \n\\end{equation}\nFor KNN pooling, we denote $\\mathcal{D}_{topk}$ as $k$ training samples most relevant to $d_q$ in $\\mathcal{D}_{t}$ and reformulate Equation~\\ref{eq: inference_2} as:\n\\begin{equation}\n\\label{eq: inference_5}\ns_{l} \n= |\\{d_i | d_i \\in \\mathcal{D}_{topk}, \\mathbf{y}_{d_i}=l_i\\}|. \n\\end{equation}\nWe set $k$ as half the size of training set. \nWhen several labels appear in $\\mathcal{D}_{topk}$ for the same number of times, we select the sample obtaining the highest relevance score from these labels' corresponding samples, and classify $d_q$ to its class.\n% we select the training sample of the highest relevance score with $d_q$ and classify $d_q$ to its class.\n\nMetricPrompt renders prompting model as a relevance metric, and classify query samples according to its relevance with training samples of the few-shot task. \nPrompting model takes two pieces of sample text at once. \nThus, MetricPrompt is able to use cross-sample information to estimate relevance and make predictions accurately. \n\n\n"
                },
                "subsection 3.4": {
                    "name": "More Efficient Inference",
                    "content": "\n% MetricPrompt pairs a test sample with each training sample and adopts the optimized prompting model as a relevance metric for relevance estimation. \nThe cost of the inference procedure is relatively high because of the pairing strategy. \n% This inference method results in a significant increase in runtime overhead. \nTo further improve the efficiency of MetricPrompt, we propose to use pivot samples to reduce the time complexity of the inference stage of MetricPrompt.\n\nWe propose to use the optimized prompting model to calculate the representativeness of each training sample. \nFor a training sample $d_i$ labeled with $l$, we use $r_{d_i}$ to denote its representativeness, which is calculated as follows:\n\\begin{equation}\n% \\small\n    r_{d_i} \n    = \\frac{\\sum_{d_j \\in \\mathcal{D}_l}{s_{d_j}}}{|\\{d_j|d_j \\in \\mathcal{D}_l\\}|} - \\frac{\\sum_{d_k \\in \\mathcal{D}_t-\\mathcal{D}_l}{s_{d_k}}}{|\\{d_k|d_k \\in \\mathcal{D}_t-\\mathcal{D}_l\\}|}. \n\\end{equation}\n$s_{d_j}$ represents the relevance score between samples $d_j$ and $d_i$. \nBased on this representativeness metric, we select the top $p$ samples with the highest representativeness scores from the training samples corresponding to each label.\nThese samples are marked as pivot samples.\n% and participate in the inference process. \nDuring inference, we only pair each test sample with each label's pivot samples and compute their relevance scores to make classification prediction.\n\nPivot samples reduce the time complexity of MetricPrompt's inference process significantly.\nAssume a few-shot text classification task with $n$ labels and $k$ samples per label.\nWithout introducing pivot samples, each test sample needs to be paired with $n*k$ training samples and compute relevance scores, resulting into a time complexity of $O(n*k)$. \n% For a few-shot text classification task with $n$ labels and $k$ samples per label, , each test sample needs to be paired with $n*k$ training samples and compute relevance scores, resulting in a time complexity of $O(n*k)$. \nIn contrast, prompting methods with human-designed or automatically designed verbalizer calculate the dot product similarity between the feature representations of test samples extracted by the pre-trained model and the feature representations of each label.\n% In contrast, traditional prompting methods and other prompt learning methods that do not require manual label mapping design only need to calculate the dot product similarity between the feature representations of test samples extracted by the pre-trained model and the feature representations of each label. \nSince the total number of labels is $n$, the time complexity of these methods is only $O(n)$. \nBy introducing pivot samples to improve the inference process, MetricPrompt only needs to estimate the relevance between each test sample and the pivot samples of each label, reducing the time complexity to $O(p*n)$. \nBecause $p$ is a pre-defined constant, the time complexity of MetricPrompt's inference process accelerated with pivot samples is $O(n)$, which is consistent with other commonly used prompting methods. \nWe set the number of pivot samples per label $p$ to 2 in the experiments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n% We conduct extensive experiments with MetricPrompt and previous SOTA verbalizer design methods to show MetricPrompt's effectiveness. \nIn this section, we first introduce the datasets used in our experiments. \nThen we describe our experiment settings and implementation details. \nFinally, we present experiment results and analyze MetricPrompt's performance.\n\n\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\nWe conduct experiments with three widely used text classification tasks, which contain numerous classes and hence require extensive resources to build a proper manual verbalizer. \nWe adopt AG’s News, Yahoo Answers topics~\\cite{Zhang2015CharacterlevelCN} and DBPedia~\\cite{lehmann2015dbpedia} as our text classification tasks.\nThe statistics of the datasets are given in Table ~\\ref{tbl: datasets_statistics}.\n\n\n\nSince the average text length of the three datasets is short, we truncate all sample text to 120 tokens for better efficiency with little semantic meaning loss.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Few-shot experiment settings",
                    "content": "\nWe conduct experiments under 2, 4, 8 and 16-shot settings, where corresponding number of training samples are sampled from each dataset's training set randomly.\nModels' performance are observed to fluctuate largely when the training set is sampled with different random seeds.  \nHence, we sample 10 training sets for each dataset and each few-shot setting to alleviate the influence of randomness in training set selection. \nAll experiment results are reported as the average value of the model's performance on the 10 training sets. \n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Implementation Details",
                    "content": "\nWe implement MetricPrompt with PyTorch~\\cite{paszke2019pytorch} and Huggingface~\\cite{wolf2020transformers} framework, and baselines are implemented with OpenPrompt toolkit~\\cite{ding2021openprompt}.\nWe introduce Kernl to accelerate the inference procedure.\\footnote{\\href{https://github.com/ELS-RD/kernl}{https://github.com/ELS-RD/kernl}}\n\nWe adopt BERT-base-uncased~\\cite{devlin2019bert} as the backbone model for both MetricPrompt and all baseline models for fair comparison. \nModel parameters are optimized with AdamW optimizer~\\cite{AdamW}, and the learning rate is set as 1e-5. \n% We train MetricPrompt for 120 epochs with batch size set as 16 for AG's News dataset 2-shot setting. \nWe set total training steps proportionally to the size of training set, and the number of training epochs is adjusted accordingly. \n% To realize such a training strategy, the epoch number is decreased proportionally to shot number, because the quantity of trainning samples increases proportionally to its square. \nThe size of training set varies across datasets and shot numbers, and the specific number of training epochs is given in Table~\\ref{tbl: epochs}.\n% Since the number of class varies across datasets, we also modify the number of training epochs based on the size of their training data, and the specific epoch number is given in Table~\\ref{tbl: epochs}.\n\n\n\n\n\n% \\subsection{Baselines}\n% We select several representative verbalizer design methods for comparison.\n% Among the listed baselines, only manual verbalizer requires human effort in task-specific verbalizer design. \n\n% \\textbf{Manual Verbalizer} (ManualVerb) uses hand-crafted verbalizer to map PLM's output word to classification labels. We use the verbalizer provided by OpenPrompt~\\cite{ding2021openprompt} in our experiments. \n\n% \\textbf{Automatic Verbalize Search} (AVS)~\\cite{schick2021eacl} initializes the verbalizer with random words, and improves the verbalizer by replacing each label's corresponding answer words iteratively. \n% Words which the PLM considers to represent the given samples more suitably are selected as replacing words in each iteration. \n\n% \\textbf{Soft Verbalizer} (SoftVerb)~\\cite{hambardzumyan2021warp} represents each label with a trainable embedding and optimizes it during training. \n% In the originally proposed work WARP, the prompt template is also represented as trainable embeddings.\n% In this work, however, we follow \\citeauthor{cui2022prototypical} to use manual template instead for fair comparison.\n\n% \\textbf{Prototypical Verbalizer} (ProtoVerb)~\\cite{cui2022prototypical} also represents classification labels as soft embeddings and samples as the last layer's hidden states at \\texttt{[MASK]} position. \n% ProtoVerb differs from SoftVerb in its loss function, where prototypical contrastive learning loss~\\cite{liprototypical} is adopted instead of vanilla cross-entropy loss.  \n\n% We hand-craft a task-general prompt template and verbalizer for MetricPrompt and use them in all experiments.  \n% For other baselines, we make minor modifications to the default template and verbalizer used in OpenPrompt to unify the input format. \n% An overview of prompt template and verbalizer used for our experiments is given in Table~\\ref{tbl: 4_4}.\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Baselines",
                    "content": "\nWe select several representative verbalizer design methods for comparison.\nAmong the listed baselines, only manual verbalizer requires human effort in task-specific verbalizer design. \n\n\\textbf{Manual Verbalizer} (ManualVerb) uses hand-crafted verbalizer to map PLM's output word to classification labels. \n% We use the verbalizer provided by OpenPrompt~\\cite{ding2021openprompt} in our experiments. \n\n\\textbf{Automatic Verbalize Search} (AVS)~\\cite{schick2021eacl} is a search-based verbalizer design method. It initializes the verbalizer with random words, and improves answer words iteratively. \n% Words which the PLM considers to represent the given samples more suitably are selected as replacing words in each iteration. \n\n\\textbf{Soft Verbalizer} (SoftVerb)~\\cite{hambardzumyan2021warp} represents each label with a trainable embedding. \nIn the original work WARP, the prompt template is also represented as trainable embeddings.\nIn this work, however, we follow \\citeauthor{cui2022prototypical} to use manual template for fair comparison.\n\n\\textbf{Prototypical Verbalizer} (ProtoVerb)~\\cite{cui2022prototypical} also represents classification labels as soft embeddings and samples as features encoded by PLM.  \nProtoVerb adopts prototypical contrastive learning loss~\\cite{liprototypical} instead of vanilla cross-entropy loss to optimize model parameters. \n\nWe hand-craft a task-general prompt template and verbalizer for MetricPrompt.  \nFor other baselines, we make minor modifications to the default template and verbalizer used in OpenPrompt to unify the input format. \nAn overview of prompt template and verbalizer used for our experiments is given in Table~\\ref{tbl: 4_4}.\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Main Results",
                    "content": "\n\\label{sec: 4_5}\n\n\n\n\n\n% \\begin{table*}[t]\n% % \\small\n% \\resizebox{\\textwidth}{15mm}{\n% \\begin{tabular}{l cccc cccc cccc cccc}\n% \\toprule\n% \\multicolumn{1}{c}{Method} &\n% \\multicolumn{4}{c}{AG's News} &\n% \\multicolumn{4}{c}{DBPedia} &\n% \\multicolumn{4}{c}{Yahoo} &\n% \\multicolumn{4}{c}{Average}\n% \\\\\n% \\cmidrule(lr{0.5em}){2-5}\\cmidrule(lr{0.5em}){6-9}\\cmidrule(lr{0.5em}){10-13}\\cmidrule(lr{0.5em}){14-17} \n% & 2-shot & 4-shot & 8-shot & 16-shot & 2-shot & 4-shot & 8-shot & 16-shot & 2-shot & 4-shot & 8-shot & 16-shot & 2-shot & 4-shot & 8-shot & 16-shot \n% \\\\\n% \\midrule\n% \\textsc{ManualVerb}       & $45.87$ &  $76.22$ & $78.94$ & $83.66$ & $69.81$ &  $84.15$ & $94.24$ &  $97.27$ & $34.82$ &  $55.56$ & $58.30$ &  $62.42$ &  $-$ &  $-$ & $-$ &  $-$\\\\\n% \\textsc{AVS}       & $44.93$ & $57.49$ &  $71.37$ & $77.81$ & $-$ &  $-$ & $-$ & $-$ & $21.88$ &  $28.44$ & $46.53$ &  $-$ &  $-$ &  $-$ & $-$ &  $-$\\\\\n% \\textsc{SoftVerb}       & $48.86$ & $61.15$ &  $73.28$ & $80.61$ & $53.98$ &  $76.19$ & $90.34$ &  $96.93$ & $22.63$ &  $33.43$ & $45.01$ &  $59.09$ &  $-$ &  $-$ & $-$ &  $-$\\\\\n% \\textsc{ProtoVerb}       & $58.38$ & $65.04$ &  $75.57$ & $80.31$ & $60.89$ &  $74.49$ & $87.45$ &  $97.16$ & $28.80$ &  $43.01$ & $52.87$ &  $61.57$ &  $-$ &  $-$ & $-$ &  $-$\\\\\n% \\midrule\n% \\textsc{MetricPrompt}       & $65.77$ & $76.33$ &  $82.04$ & $84.69$ & $71.20$ &  $88.44$ & $94.57$ &  $96.59$ & $28.76$ &  $53.54$ & $59.68$ &  $62.45$ &  $-$ &  $-$ & $-$ &  $-$\\\\\n% \\bottomrule\n% \\end{tabular}\n% }\n% \\centering\n% \\caption{}\n% \\label{tbl:main}\n% % \\vspace{-5mm}\n% \\vspace*{-3mm}\n% \\end{table*}\n\n\nWe conduct experiments on three text classification datasets with different text styles under four few-shot settings. \nExperiment results for 2 and 4-shot settings are listed in Table~\\ref{tbl: main_24}, while 8 and 16-shot's experiment results are shown in Table~\\ref{tbl: main_816}. \nMetricPrompt outperforms previous SOTA automatic verbalizer design method ProtoVerb by a large margin, improving 2-shot accuracy by $5.88$ ($11.91\\%\\uparrow$), 4-shot accuracy by $11.92$ ($19.59\\%\\uparrow$), 8-shot accuracy by $6.80$ ($9.45\\%\\uparrow$) and 16-shot accuracy by $1.56$ ($1.96\\%\\uparrow$). \nMetricPrompt's performance even surpasses that of ManualVerb under all few-shot settings without any human labor involved in task-specific verbalizer design. \n\nMeanwhile, we have following observations:\n % based on Table~\\ref{tbl: main_24} and Table~\\ref{tbl: main_816}\n\n\\textbf{(1)} MetricPrompt is the only prompting method outperforming ManualVerb without any human labor involved in task-specific verbalizer design. \nMetricPrompt benefits from paired input text, which enables the model to use cross-sample information to make up the lack of extra human knowledge and trial-and-error work.\n\n\\textbf{(2)} MetricPrompt achieves the highest score over automatic verbalizer design methods. \nCompared with AVS, MetricPrompt does not restrict each class's representation to several sub-optimal words from the vocabulary, but instead represent it with corresponding training samples, leading to more accurate semantic representation. \nFor the comparison with SoftVerb and ProtoVerb, we attribute MetricPrompt's leading performance to the smaller gap between its task formulation and PLM's pre-training objective. \nUnlike SoftVerb and ProtoVerb, MetricPrompt does not operate on PLM's inner representations, but functions with only the output word probability distribution at \\texttt{[MASK]} position, enabling PLM to adapt to few-shot text classification task more smoothly.\nMoreover, MetricPrompt does not introduce task-specific parameters to be trained from scratch, avoiding over-fitting problem under few-shot scenarios. \n\n\\textbf{(3)} MetricPrompt achieves comparable results with mean pooling and max pooling, but a performance drop is witnessed with KNN pooling.  \nWe ascribe the performance gap to the incompatibility between KNN pooling and the non-uniform distribution of MetricPrompt's predicted relevance scores.\nWe further discuss this phenomenon in Section~\\ref{sec: 5_3}. \n\n% \\begin{itemize}\n%     \\item MetricPrompt is the only and first prompting method outperforming ManualVerb without any human labor involved in task-specific verbalizer design. \n%     MetricPrompt benefits from paired input text, which enables the model to introduce cross-sample information to make up the lack of the extra human knowledge and trial-and-error work.\n%     % We attribute MetricPrompt makes up the lack of the extra human knowledge by introducing cross-sample information to aid classification task. \n%     % It marks the first time when automatically designed verbalizer gains better performance than that of carefully hand-crafted ones. \n    \n%     \\item MetricPrompt achieves the highest score over automatic verbalizer design methods. \n%     Compared with AVS, MetricPrompt does not restrict each class's representation to several sub-optimal words from the vocabulary, but instead represent it with corresponding training samples, leading to more accurate semantic representation. \n%     For the comparison with SoftVerb and ProtoVerb, we attribute MetricPrompt's leading performance to the smaller gap between its task formulation and PLM's pre-training objective. \n%     Unlike SoftVerb and ProtoVerb, MetricPrompt does not operate on PLM's inner representations, but functions with only the output word probability distribution at \\texttt{[MASK]} position, enabling PLM to adapt to few-shot text classification task more smoothly.\n%     Moreover, MetricPrompt does not introduce task-specific parameters to be trained from scratch, avoiding over-fitting problem under few-shot scenarios. \n%     % Therefore, MetricPrompt allows the PLM to adapt to few-shot text classification task more smoothly.\n%     % \\item MetricPrompt performs better than ManualVerb even though no human labor is consumed in task-specific verbalizer design. \n%     % We conjecture MetricPrompt makes up the lack of the extra human knowledge by introducing cross-sample information to aid classification task. \n    \n%     % \\item MetricPrompt outperforms previous SOTA baselines by a smaller fraction as shot number increases.\n%     % It indicates the marginal diminishing effect in few-shot learning: the larger the training set is, the less the semantic information introduced by extra samples. \n%     % As a result, models which cannot utilize few-shot samples as effectively as MetricPrompt still achieves comparable results as shot number increases. \n\n%     \\item MetricPrompt achieves comparable results with mean pooling and max pooling, but a performance drop is witnessed with KNN pooling.  \n%     We ascribe the performance gap to the incompatibility between KNN pooling and the non-uniform distribution of MetricPrompt's predicted relevance scores.\n%     % across training samples. \n%     We further discuss this phenomenon in Section~\\ref{sec: 5_3}. \n    \n% \\end{itemize}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% \\input{5_analysis}\n% \\input{2_background_short}\n% \\input{3_methods_short}\n% \\input{4_experiments_short}\n\n"
                }
            },
            "section 5": {
                "name": "Analysis",
                "content": "\nIn this section, we further evaluate MetricPrompt from different aspects to illustrate its effectiveness. \n\n\n\n\n",
                "subsection 5.1": {
                    "name": "Extensibility with Out-Of-Domain Data",
                    "content": "\n\\label{sec: 5_1}\nSince it is impractical to always obtain sufficient training data for downstream few-shot text classification tasks, we further evaluate MetricPrompt's extensibility with Out-Of-Domain (OOD) data.\nWe use 16-shot training sets of other datasets to aid each task's 1, 2 and 4-shot classification, where models' performance remains to be improved. \nWe adopt mean pooling and simply mix up training samples of the original dataset and OOD ones to train the model. \nFor ProtoVerb baseline, we first train the model on OOD training data, and then re-initialize prototype parameters for the training on the original dataset. \n\nAs shown in Table~\\ref{tbl: extensibility}, MetricPrompt achieves much higher accuracy when boosted by OOD training data. \n% The performance gap between fewer-shot and 16-shot settings are reduced by a large fraction. \nCompared with previous SOTA baseline ProtoVerb, MetricPrompt obtains better prediction accuracy in 17 out of 18 few-shot and OOD data settings (underlined numbers in the table), showing remarkable extensibility with OOD samples. \n% We can also observe that the fewer original training samples are, the more improvement OOD data brings about.\n% It can be explained by the marginal diminishing effect of extra training samples mentioned in Section~\\ref{sec: 4_5}. \nAlthough OOD data improves the model's performance under most settings, AG's News training set fail to aid DBPedia 4-shot and Yahoo 4-shot performance.\nWe ascribe this failure to the abundance of training samples under these settings. \nDBPedia and Yahoo contains 4096 and 1600 training samples under 4-shot setting, which is sufficient for the model to adapt to the relevance estimation task. \nOOD data serves more as noises and therefore harms the model's performance. \n\n\n\nIt is worth noticing that MetricPrompt's performance improvement under 1-shot setting is abnormally high. \nMetricPrompt underperforms ProtoVerb on the three datasets without OOD data, because MetricPrompt only takes two identical pieces of text as positive sample under 1-shot setting, leading to severe over-fitting problem. \nThe model is optimized to only produce high relevance score when given two identical pieces of text. \nDiversified OOD data alleviate the over-fitting problem effectively, and therefore improve MetricPrompt's 1-shot performance by a large fraction. \n% MetricPrompt outperforms ProtoVerb in 5 out of 6 1-shot OOD data experiments, showing remarkable extensibility with OOD samples.\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Robustness against Noisy Samples",
                    "content": "\nNoisy samples harm few-shot text classification model's performance severely due to the lack of supervision signal. \nIn this section, we evaluate MetricPrompt's robustness against noisy samples on AG's News dataset. \n% In this part, we evaluate MetricPrompt's robustness against noisy samples, which often pose a severe problem for few-shot tasks.\nFollowing~\\citeauthor{cui2022prototypical}, we conduct experiments under 8 and 16-shot settings for training stability.\nWe replace 1, 2 and 4 training samples' labels randomly to introduce noises, and evaluate MetricPrompt's performance when equipped with mean, max and KNN pooling. \nWe compare our method with previous SOTA baseline ProtoVerb, which shows the best robustness against noisy samples among automatic verbalizer design methods~\\cite{cui2022prototypical}. \nThe performance drop caused by noisy samples is displayed in Table~\\ref{tbl: robustness}. \n\nCompared with ProtoVerb, MetricPrompt suffers from less performance drop and achieves higher classification accuracy with mean and max pooling, while KNN pooling leads to worse performance. \nWe attribute the large performance drop of KNN pooling to its susceptibility to the variance of each class's training sample number, which is introduced by noisy samples.\nWe provide a detailed analysis in Section~\\ref{sec: 5_3}.\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Comparison across Pooling Methods",
                    "content": "\n\\label{sec: 5_3}\n% The choice of pooling method influences MetricPrompt's performance significantly. \n% and behaviors. \nIn this part, we analyze different pooling methods with clean and noised training data to explain their performance gap. \n\nFirstly, we focus on clean data scenario without noisy samples.\n% We collect statistics of the distribution of relevance scores computed by MetricPrompt. \nWe choose AG's News 2-shot setting and compute the average relevance score of each query sample's most relevant training sample, second most relevant training sample, etc. \nAs shown in Fig~\\ref{fig: 5_3_1}, the distribution of relevance scores is highly non-uniform. \nThe highest relevance score is much larger than others, so the max relevance score serves as a decisive factor for MetricPrompt with mean pooling.\nTherefore, mean pooling shows similar behavior with max pooling. \n% Compared with the top two relevance scores, other scores distribute more evenly. \n% For KNN pooling, subtle difference in tail scores \n% Training samples of the correct label can slip into in the second half easily.\n% Even if a correct sample gains the highest score, it cannot make up for wrongly distributed samples of the same class due to the voting strategy of KNN pooling.\n% KNN pooling, however, adopts voting strategy, where the largest relevance score cannot determine the classification result, losing information \nKNN pooling, however, adopts voting strategy which ignores score value information, leading to deteriorated performance.\n% KNN pooling, however, adopts voting strategy, where score value information is ignored and even the largest relevance score cannot determine the classification result.\n% Once other training samples of the correct label cannot achieve top $k$ relevance score, the classification result will be unpredictable. \n% As a result, KNN pooling does not fit MetricPrompt's relevance score distribution and therefore makes more mistakes than other pooling methods do.\n\n\n\n\n\n% As we can observe in Fig~\\ref{fig: 5_3_2}, KNN pooling shows significantly stronger preference to classes with more training samples than mean pooling and max pooling do.\n% When one noisy sample is introduced to the training set, mean pooling classifies 533 more query samples to classes with 9 training samples than those with 7 training samples averagely. \n% On the contrary, KNN pooling classifies 1337 more query samples to 9-sample classes than 7-sample classes, which is 1.5 times larger than that of mean pooling. \n\nWe then analyze MetricPrompt's performance when noisy samples are introduced.\nWe first categorize classes of AG's News dataset 8-shot training sets according to the number of their corresponding training samples.\nThen we collect the statistics of the average predicted query sample number for each type of class and show them in Figure~\\ref{fig: 5_3_2}.\nKNN pooling shows significantly stronger preference to classes with more training samples than mean pooling and max pooling do. \nSince the distribution of MetricPrompt's relevance score except the top ones is relatively even, samples from classes with large training set are more likely to become the majority of KNN pooling's top $k$ relevant samples.\n% classes with more training samples are more likely to take up the majority of KNN pooling's top $k$ relevant samples.\nWithout considering relevance score value, a large drop in KNN pooling's performance is witnessed.\n% when noisy samples are introduced.\n% KNN pooling's voting mechanism regards each top $k$ most relevant training sample as one vote without taking relevance score value into consideration, and therefore develops a strong preference to classes with more training samples. \nOn the contrary, mean pooling and max pooling take each training sample's relevance score value into consideration, so the influence of training sample number is mitigated. \nAs a result, they suffer from smaller performance drops than KNN pooling do. \n\n% \\begin{table}[t]\n% \\resizebox{\\columnwidth}{!}{\n% \\begin{tabular}{c c c c}\n% \\toprule\n% \\# Noisy sample & \\textsc{Mean Pooling} & \\textsc{Max Pooling} & \\textsc{KNN Pooling}\n% \\\\\n% \\midrule\n%  1 & $267.05$ & $293.97$ & $\\bm{668.90}$ \\\\\n%  4 & $254.33$ & $275.34$ & $\\bm{668.96}$ \\\\\n% \\bottomrule\n% \\end{tabular}\n% }\n% \\centering\n% \\caption{\n% Standard deviation of predicted query sample number across classes with different training sample number. \n% ``\\# Noisy sample” refers to the number of noisy samples. \n% Bold number indicates the largest deviation.\n% }\\label{tbl: 5_3}\n% \\end{table}\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Influence of Pivot Sample Number",
                    "content": "\n\n\n\n\n\n% Although MetricPrompt achieves promising text classification accuracy, the time-consuming inference process may limit its applicability in real world text classification scenarios with high demands on execution speed. \n% We propose to use pivot samples to reduce the time complexity of the inference stage to be the same with other prompting methods. \nIn this part, we investigate the influence of pivot sample numbers to the performance of MetricPrompt. \nWe conduct experiments on the three datasets across four few-shot settings with pivot sample number $p$ set as 1, 2 and 4, the performance of MetricPrompt under different few-shot settings are displayed Table~\\ref{tbl: pivot_24} and Table~\\ref{tbl: pivot_816}. \n% Complete experiment results are shown in Appendix~\\ref{}. \n\nAs shown in the tables, the performance of MetricPrompt correlates with the number of pivot samples positively. \nAs the number of pivot samples increase, MetricPrompt captures each label's semantic meaning more accurately and therefore achieves better performance. \nIt is worth noticing that even if only one pivot sample is selected for each class, MetricPrompt still outperforms ProtoVerb under the four few-shot settings. \nThe selection of pivot sample number $p$ serves as a trade-off between classification accuracy and efficiency.\nIn real world applications, MetricPrompt can adapt to varying scenarios with different requirements by adjusting the number of pivot samples. \n% hen applied in real world text classification scenarios\n\n\n% \\begin{table}[t]\n% \\resizebox{\\columnwidth}{!}{\n% \\begin{tabular}{l c c c c}\n% \\toprule\n% Method & 2-shot & 4-shot & 8-shot & 16-shot\n% \\\\\n% \\midrule\n%  \\textsc{ProtoVerb} & $49.36$ & $60.85$ & $71.96$ & $79.68$ \\\\\n%  \\textsc{$\\text{MetricPrompt}_{1\\text{pivot}}$}  & $50.90$ & $67.97$ & $76.60$ & $80.45$ \\\\\n%  \\textsc{$\\text{MetricPrompt}_{2\\text{pivot}}$}  & $\\bm{55.24}$ & $70.66$ & $77.98$ & $80.72$ \\\\\n%  \\textsc{$\\text{MetricPrompt}_{4\\text{pivot}}$}  & $\\bm{55.24}$ & $\\bm{72.77}$ & $\\bm{78.25}$ & $\\bm{81.01}$ \\\\\n% \\bottomrule\n% \\end{tabular}\n% }\n% \\centering\n% \\caption{\n% The performance of MetricPrompt with different number of pivot samples.\n% Each result in the table is the average performance across the three datasets. \n% ``\\# Pivot sample” refers to the number of pivot samples per class. \n% Bold number indicates the largest deviation.\n% }\\label{tbl: 5_4}\n% \\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusions and Future Work",
                "content": "\nIn this work, we propose MetricPrompt, which frees human labor from task-specific verbalizer design by reformulating few-shot text classification task into a text pair relevance estimation problem. \n% MetricPrompt fulfill relevance estimation\nMetricPrompt prompts query-training text pair to fulfill relevance estimation, which coincides with PLM's pre-training objective and thus enables smooth adaption to downstream tasks.\nTaking a pair of sample text simultaneously, MetricPrompt introduces cross-sample information for better accuracy.\nExperiments on three widely used text classification datasets under four few-shot settings indicate MetricPrompt's leading performance over previous SOTA baselines. \n% MetricPrompt also surpasses manual verbalizer which consumes heavy human effort, \nOur analysis further demonstrates MetircPrompt's promising extensibility and robustness, and explains its performance variance with different pooling methods and pivot sample numbers. \n\nAlthough MetricPrompt achieves satisfactory few-shot text classification performance in our experiments, its performance with large language models remains to be explored. \nCurrent large language models achieve impressive performance in few-shot text classification tasks with prompting methods, but they still suffer from prompting methods' susceptibility to the design of verbalizers. \nWhen given classes which are difficult to be described with several words, these models' performance deteriorates significantly. \nThe proposed MetricPrompt is not bounded with specific backbone model, and can be easily generalized to large language models.\nWe look forward to using MetricPrompt to ease human effort from verbalizer design and further improve the few-shot text classification accuracy of large language models. \n% We believe the proposed method can achieve competitive performance with \n\n\n\n% Although MetricPrompt achieves satisfactory performance on the given few-shot text classification tasks, the proposed method still remains to be further improved in several aspects. \n% The first one is the inference speed of MetricPrompt. \n% Since MetricPrompt works as a single tower model, the PLM has to process the query sample's paired text with every training sample. \n% % Since MetricPrompt works as a single tower model which only takes paired samples, the PLM have to process the query sample's paired text with each training sample. \n% Tough MetricPrompt works with acceptable efficiency under few-shot setting, its inference speed still remains to be improved when more training data is provided. \n% We believe a two tower model, which filters out representative training samples efficiently, can reduce MetricPrompt's inference cost with little accuracy loss.\n% % A common solution to the exploding inference cost of single tower model is combining it with two tower model to filter out representative training samples.\n% % We believe a proper combination can reduce MetricPrompt's inference cost with little accuracy loss. \n% MetricPrompt is also reluctant to handle one-shot text classification task.\n% Under one-shot setting, MetricPrompt only takes samples composed of 2 identical pieces of text as positive ones, resulting into severe over-fitting problem. \n% % It does not discover input text pair's semantic meaning to estimate the relevance score, but simply produce high score when given two identical pieces of text. \n% % Fortunately, this problem can be alleviated to a great extent with OOD data as described in Section~\\ref{sec: 5_1}.\n% OOD training data alleviates this problem effectively as described in Section~\\ref{sec: 5_1}. \n% % However, MetricPrompt's one-shot performance still remains to be improved with in-domain data. \n% We believe better data construction methods, for example pairing original training samples with their noised copies, will further settle the over-fitting problem and improve MetricPrompt's one-shot performance with only in-domain data.\n% % We believe MetricPrompt's one-shot performance with only in-domain data can be further improved with better data construction strategies, \n\n\n\n\n\n\n\n\n\n% \\section{Limitations}\n% Although MetricPrompt achieves satisfactory performance on the given few-shot text classification tasks, the proposed method still suffers from 2 shortcomings. \n% The first one comes from MetricPrompt's inference speed. \n% Since MetricPrompt works as a single tower model which only takes paired samples, the PLM have to process the query sample's paired text with each training sample. \n% This problem looms larger as the size of training set increases.\n% Fortunately, MetricPrompt works with acceptable efficiency when shot number is relatively small. \n% The second shortcoming is that MetricPrompt is reluctant to handle one-shot text classification task.\n% Under one-shot setting, MetricPrompt only takes samples composed of 2 identical pieces of text as positive ones, resulting into severe over-fitting problem. \n% This problem can be alleviated with OOD data as described in Section~\\ref{sec: 5_1}.\n\n\n\n% \\section*{Ethics Statement}\n% Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\\footnote{\\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).\n\n"
            },
            "section 7": {
                "name": "Acknowledgements",
                "content": "\nThis work was supported by the National Key R\\&D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 62236004 and 61976072.\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{sample-base}\n\n"
            }
        },
        "tables": {
            "tbl: datasets_statistics": "\\begin{table}[t]\n\\small\n\\begin{tabular}{c c c c}\n\\toprule\nDataset & \\# Class & \\# Test & Avg len\n\\\\\n\\midrule\nAG’s News & 4 & 7,600 & 52 \\\\\nDBPedia & 14 & 70,000 & 68 \\\\\nYahoo & 10 & 60,000 & 130 \\\\\n\\bottomrule\n\\end{tabular}\n\\centering\n\\caption{\nStatistics of the three text classification datasets used in our experiments. \n}\n\\label{tbl: datasets_statistics}\n\\vspace*{-3mm}\n\\end{table}",
            "tbl: epochs": "\\begin{table}[t]\n\\small\n\\begin{tabular}{c c c c c}\n\\toprule\nDataset & 2-shot & 4-shot & 8-shot & 16-shot\n\\\\\n\\midrule\nAG’s News & 120 & 60 & 30 & 15 \\\\\nDBPedia & 32 & 16 & 8 & 4 \\\\\nYahoo & 36 & 18 & 9 & 5 \\\\\n\\bottomrule\n\\end{tabular}\n\\centering\n\\caption{\nThe number of training epochs for given datasets and few-shot settings. \n}\n\\label{tbl: epochs}\n\\vspace*{-3mm}\n\\end{table}",
            "tbl: 4_4": "\\begin{table*}[t]\n\\small\n\\begin{tabular}{c c c c}\n\\toprule\nMethod & Dataset & Prompt template & Task-specific verbalizer\n\\\\\n\\midrule\n\\multirow{5}{*}{\\textsc{ManualVerb}} & AG's News & A \\texttt{[MASK]} news: \\texttt{<text>} & sports, politics, business, technology \\\\\n & \\multirow{2}{*}{DBPedia} & \\multirow{2}{*}{\\texttt{<text>} In this sentence, the topic is \\texttt{[MASK]}} & company, school, artist, athlete, politics, transportation, \\\\\n & & & building, river, village, animal, plant, album, film, book \\\\\n & \\multirow{2}{*}{Yahoo} & \\multirow{2}{*}{A \\texttt{[MASK]} question: \\texttt{<text>}} & society, science, health, education, computers,  \\\\\n & & & sports, business, entertainment, relationships, politics \\\\\n\\midrule\n\\multirow{3}{*}{\\textsc{AVS}} & AG's News & A \\texttt{[MASK]} news: \\texttt{<text>} & Automatically searched label words \\\\\n & DBPedia & \\texttt{<text>} In this sentence, the topic is \\texttt{[MASK]} & Automatically searched label words \\\\\n & Yahoo & A \\texttt{[MASK]} question: \\texttt{<text>} & Automatically searched label words \\\\\n \\midrule\n\\multirow{3}{*}{\\textsc{SoftVerb}} & AG's News & \\multirow{3}{*}{\\texttt{<text>} In this sentence, the topic is \\texttt{[MASK]}} & \\multirow{3}{*}{Soft label embeddings} \\\\\n & DBPedia &  &  \\\\\n & Yahoo &  &  \\\\\n\\midrule\n\\multirow{3}{*}{\\textsc{ProtoVerb}} & AG's News & A \\texttt{[MASK]} news: \\texttt{<text>} & Soft label embeddings \\\\\n & DBPedia & \\texttt{<text>} In this sentence, the topic is \\texttt{[MASK]} & Soft label embeddings \\\\\n & Yahoo & A \\texttt{[MASK]} question: \\texttt{<text>} & Soft label embeddings \\\\\n\\midrule\n\\multirow{3}{*}{\\textsc{MetricPrompt}} & AG's News & \\multirow{3}{*}{\\texttt{<text\\_a>} A news of \\texttt{[MASK]} topic: \\texttt{<text\\_b>}} & \\multirow{3}{*}{$-$} \\\\\n & DBPedia &  &  \\\\\n & Yahoo &  &  \\\\\n\\bottomrule\n\\end{tabular}\n\\centering\n\\caption{\nPrompt templates and task-specific verbalizers used by MetricPrompt and other baselines.\n``-” means no task-specific verbalizer is required.\n}\n\\label{tbl: 4_4}\n\\vspace*{-3mm}\n\\end{table*}",
            "tbl: main_24": "\\begin{table*}[t]\n% \\small\n% \\resizebox{\\textwidth}{15mm}{\n\\begin{tabular}{l cc cc cc cc}\n\\toprule\n\\multicolumn{1}{c}{Method} &\n\\multicolumn{2}{c}{AG's News} &\n\\multicolumn{2}{c}{DBPedia} &\n\\multicolumn{2}{c}{Yahoo} &\n\\multicolumn{2}{c}{Average}\n\\\\\n\\cmidrule(lr{0.5em}){2-3}\\cmidrule(lr{0.5em}){4-5}\\cmidrule(lr{0.5em}){6-7}\\cmidrule(lr{0.5em}){8-9} \n& 2-shot & 4-shot & 2-shot & 4-shot & 2-shot & 4-shot & 2-shot & 4-shot \n\\\\\n\\midrule\n\\textsc{ManualVerb}       & $\\textit{45.87}$ & $\\textit{76.22}$ & $\\textit{69.81}$ &  $\\textit{84.15}$ & $\\textit{34.82}$ &  $\\textit{55.56}$ & $\\textit{50.17}$ &  $\\textit{71.98}$\\\\\n\\textsc{AVS}~\\citeyearpar{schick2021eacl}        & $44.93$ & $57.49$ & $32.22$ &  $53.55$ & $21.88$ &  $28.44$ & $33.01$ &  $46.49$\\\\\n\\textsc{SoftVerb}~\\citeyearpar{hambardzumyan2021warp}        & $48.86$ & $61.15$ & $53.98$ & $76.19$ & $22.63$ &  $33.43$ & $41.82$ & $56.92$\\\\\n\\textsc{ProtoVerb}~\\citeyearpar{cui2022prototypical}        & $58.38$ & $65.04$ & $60.89$ & $74.49$ & $28.80$ &  $43.01$ & $49.36$ & $60.85$\\\\\n\\midrule\n\\textsc{$\\text{MetricPrompt}_{\\text{knn}}$}     & $62.69$ & $73.17$ & $66.27$ & $86.06$ & $26.02$ &  $50.90$ & $51.66$ &  $70.04$\\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{max}}$}     & $65.64$ & $76.12$ & $\\bm{71.28}$ & $\\bm{88.44}$ & $\\bm{28.85}$ &  $52.99$ & $\\bm{55.26}$ &  $72.52$\\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{mean}}$}       & $\\bm{65.77}$ & $\\bm{76.33}$ & $71.20$ & $\\bm{88.44}$ & $28.76$ &  $\\bm{53.54}$ & $55.24$ &  $\\bm{72.77}$ \\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{pivot}}$}       & $65.76$ & $74.53$ & $71.20$ & $86.12$ & $28.76$ &  $51.32$ & $55.24$ &  $70.66$ \\\\\n\\bottomrule\n\\end{tabular}\n% }\n\\centering\n\\caption{\nExperiment results in terms of accuracy under 2-shot and 4-shot settings. Italic score means human labor is involved in task-specific verbalizer design, and bold number indicates the best result among methods requiring no human labor. \n}\n\\label{tbl: main_24}\n% \\vspace{-5mm}\n\\vspace*{-3mm}\n\\end{table*}",
            "tbl: main_816": "\\begin{table*}[t]\n% \\small\n% \\resizebox{\\textwidth}{15mm}{\n\\begin{tabular}{l cc cc cc cc}\n\\toprule\n\\multicolumn{1}{c}{Method} &\n\\multicolumn{2}{c}{AG's News} &\n\\multicolumn{2}{c}{DBPedia} &\n\\multicolumn{2}{c}{Yahoo} &\n\\multicolumn{2}{c}{Average}\n\\\\ \n\\cmidrule(lr{0.5em}){2-3}\\cmidrule(lr{0.5em}){4-5}\\cmidrule(lr{0.5em}){6-7}\\cmidrule(lr{0.5em}){8-9} \n& 8-shot & 16-shot & 8-shot & 16-shot & 8-shot & 16-shot & 8-shot & 16-shot \n\\\\\n\\midrule\n\\textsc{ManualVerb}       & $\\textit{78.94}$ & $\\textit{83.66}$ & $\\textit{94.24}$ & $\\textit{97.27}$ & $\\textit{58.30}$ &  $\\textit{62.42}$ & $\\textit{77.16}$ & $\\textit{81.12}$\\\\\n\\textsc{AVS}~\\citeyearpar{schick2021eacl}      & $71.37$ & $77.81$ & $75.91$ & $85.36$ & $46.53$ & $57.68$ & $64.60$ &  $73.62$\\\\\n\\textsc{SoftVerb}~\\citeyearpar{hambardzumyan2021warp}       & $73.28$ & $80.61$ & $90.34$ & $96.93$ & $45.01$ &  $59.09$ & $69.54$ &  $78.88$\\\\\n\\textsc{ProtoVerb}~\\citeyearpar{cui2022prototypical}       & $75.57$ & $80.31$ & $87.45$ &  $\\bm{97.16}$ & $52.87$ &  $61.57$ & $71.96$ &  $79.68$\\\\\n\\midrule\n\\textsc{$\\text{MetricPrompt}_{\\text{knn}}$}     & $80.64$ & $84.43$ & $94.25$ & $96.55$ & $58.09$ &  $62.05$ & $77.66$ & $81.01$\\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{max}}$}     & $81.03$ & $84.27$ & $94.28$ & $96.55$ & $59.68$ &  $62.66$ & $78.33$ & $81.16$\\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{mean}}$}       &  $\\bm{82.04}$ & $\\bm{84.69}$ & $\\bm{94.57}$ & $96.59$ & $\\bm{59.68}$ & $\\bm{62.45}$ & $\\bm{78.76}$ & $\\bm{81.24}$\\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{pivot}}$}       & $81.19$ & $84.15$ & $94.13$ & $96.22$ & $58.63$ &  $61.78$ & $77.98$ &  $80.72$ \\\\\n\\bottomrule\n\\end{tabular}\n% }\n\\centering\n\\caption{\nExperiment results in terms of accuracy under 8-shot and 16-shot settings. Italic score means human labor is involved in task-specific verbalizer design, and bold number indicates the best result among methods requiring no human labor. \n}\n\\label{tbl: main_816}\n% \\vspace{-5mm}\n\\vspace*{-3mm}\n\\end{table*}",
            "tbl: extensibility": "\\begin{table*}[t]\n% \\small\n% \\resizebox{\\textwidth}{15mm}{\n\\begin{tabular}{l ccc l ccc l ccc}\n\\toprule\n\\multicolumn{1}{c}{Method} &\n\\multicolumn{3}{c}{AG's News} &\n\\multicolumn{1}{c}{Method} &\n\\multicolumn{3}{c}{DBPedia} &\n\\multicolumn{1}{c}{Method} &\n\\multicolumn{3}{c}{Yahoo}\n\\\\\n\\cmidrule(lr{0.5em}){2-4}\\cmidrule(lr{0.5em}){6-8}\\cmidrule(lr{0.5em}){10-12}\n& 1-shot & 2-shot & 4-shot & & 1-shot & 2-shot & 4-shot & & 1-shot & 2-shot & 4-shot\n\\\\\n\\midrule\n\\textsc{ProtoVerb}     & $46.79$ & $58.38$ & $65.04$ & \\textsc{ProtoVerb} & $45.86$ & $60.89$ &  $74.49$ & \\textsc{ProtoVerb} & $21.60$ & $28.80$ & $43.01$ \\\\\n\\quad\\textsc{+DBPedia}     & $57.43$ & $65.72$ & $71.27$ & \\quad\\textsc{+AG's News} & $49.00$ & $63.29$ &  $75.56$ & \\quad\\textsc{+AG's News} & $28.93$ & $39.15$ & $49.96$ \\\\\n\\quad\\textsc{+Yahoo}       &  $63.63$ & $71.84$ & $75.34$ & \\quad\\textsc{+Yahoo} & $\\bm{\\underline{54.33}}$ & $66.78$ &  $77.56$ & \\quad\\textsc{+DBPedia} & $30.13$ & $39.57$ & $51.39$ \\\\\n\\midrule\n\\textsc{MetricPrompt}     & $39.16$ & $65.77$ & $76.33$ & \\textsc{MetricPrompt} & $32.31$ & $71.20$ & $88.44$ & \\textsc{MetricPrompt} & $18.80$ & $28.76$ & $53.54$ \\\\\n\\quad\\textsc{+DBPedia}     & $\\underline{66.95}$ & $\\underline{71.40}$ & $\\underline{77.34}$ & \\quad\\textsc{+AG's News} & $\\underline{53.23}$ & $\\underline{74.21}$ & $\\underline{88.34}$ & \\quad\\textsc{+AG's News} & $\\underline{32.10}$ & $\\bm{\\underline{44.27}}$ & $\\underline{52.29}$ \\\\\n\\quad\\textsc{+Yahoo}       &  $\\bm{\\underline{71.00}}$ & $\\bm{\\underline{73.99}}$ & $\\bm{\\underline{79.57}}$ & \\quad\\textsc{+Yahoo} & $53.03$ & $\\bm{\\underline{76.41}}$ &  $\\bm{\\underline{89.47}}$ & \\quad\\textsc{+DBPedia} & $\\bm{\\underline{32.77}}$ & $\\underline{43.63}$ & $\\bm{\\underline{53.78}}$ \\\\\n\\bottomrule\n\\end{tabular}\n% }\n\\centering\n\\caption{\nThe performance of MetricPrompt and ProtoVerb with additional OOD training data. \nUnderlined number indicates the best result under the same few-shot and OOD setting. \nBold number represents the best result on the few-shot task. \n}\n\\label{tbl: extensibility}\n% \\vspace{-5mm}\n\\vspace*{-3mm}\n\\end{table*}",
            "tbl: robustness": "\\begin{table*}[t]\n% \\small\n% \\resizebox{\\textwidth}{15mm}{\n\\begin{tabular}{l cc cc cc cc}\n\\toprule\n\\multicolumn{1}{c}{Method} &\n\\multicolumn{2}{c}{1 wrong} &\n\\multicolumn{2}{c}{2 wrong} &\n\\multicolumn{2}{c}{4 wrong} &\n\\multicolumn{2}{c}{Average}\n\\\\\n\\cmidrule(lr{0.5em}){2-3}\\cmidrule(lr{0.5em}){4-5}\\cmidrule(lr{0.5em}){6-7}\\cmidrule(lr{0.5em}){8-9}\n& 8-shot & 16-shot & 8-shot & 16-shot & 8-shot & 16-shot & 8-shot & 16-shot \\\\\n\\midrule\n\\textsc{ProtoVerb}     & $2.79$ & $0.83$ & $4.95$ & $1.85$ & $11.31$ &  $3.71$ & $6.35$ & $2.13$ \\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{knn}}$}     & $5.74$ & $2.61$ & $5.66$ & $3.08$ & $12.38$ &  $3.38$ & $7.93$ & $3.02$ \\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{max}}$}     & $\\bm{1.59}$ & $0.59$ & $3.12$ & $\\bm{0.89}$ & $\\bm{7.01}$ &  $\\bm{1.21}$ & $3.91$ & $\\bm{0.90}$ \\\\\n\\textsc{$\\text{MetricPrompt}_{\\text{mean}}$}     & $1.81$ & $\\bm{0.55}$ & $\\bm{2.72}$ & $1.04$ & $7.06$ & $1.52$ & $\\bm{3.86}$ & $1.04$ \\\\\n\\bottomrule\n\\end{tabular}\n% }\n\\centering\n\\caption{\nModels' performance drop under AG's News 8 and 16-shot settings with 1, 2 and 4 noisy samples.\nBold number indicates the least drop among all methods.\n}\n\\label{tbl: robustness}\n% \\vspace{-5mm}\n\\vspace*{-3mm}\n\\end{table*}",
            "tbl: pivot_24": "\\begin{table*}[t]\n    % \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l cc cc cc cc}\n    \\toprule\n    \\multicolumn{1}{c}{Method} &\n    \\multicolumn{2}{c}{AG's News} &\n    \\multicolumn{2}{c}{DBPedia} &\n    \\multicolumn{2}{c}{Yahoo} &\n    \\multicolumn{2}{c}{Average}\n    \\\\\n    \\cmidrule(lr{0.5em}){2-3}\\cmidrule(lr{0.5em}){4-5}\\cmidrule(lr{0.5em}){6-7}\\cmidrule(lr{0.5em}){8-9} \n    & 2-shot & 4-shot & 2-shot & 4-shot & 2-shot & 4-shot & 2-shot & 4-shot \n    \\\\\n    \\midrule\n    \\textsc{ProtoVerb}        & $58.38$ & $65.04$ & $60.89$ & $74.49$ & $28.80$ &  $43.01$ & $49.36$ & $60.85$\\\\\n    \\textsc{$\\text{MetricPrompt}_{\\text{1pivot}}$}       & $61.77$ & $71.41$ & $65.01$ & $84.07$ & $25.92$ &  $48.42$ & $50.90$ &  $67.97$ \\\\\n    \\textsc{$\\text{MetricPrompt}_{\\text{2pivot}}$}       & $\\bm{65.76}$ & $74.53$ & $\\bm{71.20}$ & $86.12$ & $\\bm{28.76}$ &  $51.32$ & $\\bm{55.24}$ &  $70.66$ \\\\\n    \\textsc{$\\text{MetricPrompt}_{\\text{4pivot}}$}       & $\\bm{65.76}$ & $\\bm{76.33}$ & $\\bm{71.20}$ & $\\bm{88.44}$ & $\\bm{28.76}$ &  $\\bm{53.54}$ & $\\bm{55.24}$ &  $\\bm{72.77}$ \\\\\n    \\bottomrule\n    \\end{tabular}\n    % }\n    \\centering\n    \\caption{\n    Experiment results with pivot samples under 2-shot and 4-shot settings. \n    Bold number indicates the best result. \n    }\n    \\label{tbl: pivot_24}\n    \\vspace*{-3mm}\n\\end{table*}",
            "tbl: pivot_816": "\\begin{table*}[t]\n    % \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l cc cc cc cc}\n    \\toprule\n    \\multicolumn{1}{c}{Method} &\n    \\multicolumn{2}{c}{AG's News} &\n    \\multicolumn{2}{c}{DBPedia} &\n    \\multicolumn{2}{c}{Yahoo} &\n    \\multicolumn{2}{c}{Average}\n    \\\\ \n    \\cmidrule(lr{0.5em}){2-3}\\cmidrule(lr{0.5em}){4-5}\\cmidrule(lr{0.5em}){6-7}\\cmidrule(lr{0.5em}){8-9} \n    & 8-shot & 16-shot & 8-shot & 16-shot & 8-shot & 16-shot & 8-shot & 16-shot \n    \\\\\n    \\midrule\n    \\textsc{ProtoVerb}       & $75.57$ & $80.31$ & $87.45$ &  $\\bm{97.16}$ & $52.87$ &  $61.57$ & $71.96$ &  $79.68$\\\\\n    \\textsc{$\\text{MetricPrompt}_{\\text{1pivot}}$}       & $79.23$ & $84.17$ & $93.38$ & $96.04$ & $57.20$ &  $61.15$ & $76.60$ &  $80.45$ \\\\\n    \\textsc{$\\text{MetricPrompt}_{\\text{2pivot}}$}       & $\\bm{81.19}$ & $84.15$ & $94.13$ & $96.22$ & $58.63$ &  $61.78$ & $77.98$ &  $80.72$ \\\\\n    \\textsc{$\\text{MetricPrompt}_{\\text{4pivot}}$}       & $81.13$ & $\\bm{84.62}$ & $\\bm{94.42}$ & $96.49$ & $\\bm{59.21}$ &  $\\bm{61.93}$ & $\\bm{78.25}$ &  $\\bm{81.01}$ \\\\\n    \\bottomrule\n    \\end{tabular}\n    % }\n    \\centering\n    \\caption{\n    Experiment results with pivot samples under 8-shot and 16-shot settings. \n    Bold number indicates the best result. \n    }\n    \\label{tbl: pivot_816}\n    \\vspace*{-3mm}\n\\end{table*}"
        },
        "figures": {
            "fig:intro": "\\begin{figure}[t]\n\\centering\n\\begin{tikzpicture}\n\\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=\\columnwidth, trim={0cm 2.5cm 12.7cm -1cm}, clip]{./figs/verbalizer_design_long.pdf}};\n\\end{tikzpicture}\n\\caption{\nA comparison across verbalizer design methods. \nCE stands for Cross-Entropy loss and PCL is Prototypical Contrastive Learning loss~\\cite{liprototypical}.\n}\\label{fig:intro}\n\\end{figure}",
            "fig:background": "\\begin{figure*}[t]\n\\centering\n\\begin{tikzpicture}\n\\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=2\\columnwidth, trim={0cm 9.5cm 6.7cm 0cm}, clip]{./figs/prompt_wide.pdf}};\n\\end{tikzpicture}\n\\caption{\nA comparison between vanilla fine-tuning and prompt tuning with MLM for text classification. \n}\\label{fig:background}\n\\end{figure*}",
            "fig:method_1": "\\begin{figure*}[t]\n\\centering\n\\begin{tikzpicture}\n\\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=2\\columnwidth, trim={0cm 11cm 2cm 0cm}, clip]{./figs/MetricPrompt.pdf}};\n\\end{tikzpicture}\n\\caption{\nA demonstration of Metricprompt's data construction and training procedure. \nOriginal data is paired to form positive and negative samples, and the prompting model is optimized to map these samples to corresponding words. \n}\\label{fig:method_1}\n\\end{figure*}",
            "fig:method_2": "\\begin{figure*}[t]\n\\centering\n\\begin{tikzpicture}\n\\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=2\\columnwidth, trim={0cm 1cm 2cm 9cm}, clip]{./figs/MetricPrompt.pdf}};\n\\end{tikzpicture}\n\\caption{\nThe inference procedure of MetricPrompt. \nA query sample is paired with each training sample, and the relevance scores are pooled to produce final classification probabilities.\n}\\label{fig:method_2}\n\\end{figure*}",
            "fig: 5_3_1": "\\begin{figure}[t]\n\\centering\n\\begin{tikzpicture}\n\\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=\\columnwidth, trim={0cm 0cm 0cm 1cm}, clip]{./figs/distribution_blue.pdf}};\n\\end{tikzpicture}\n\\caption{\nAverage relevance scores between each query sample and all training samples under AG's News 2-shot setting. \nThe scores are sorted and shifted to non-negative region.\n}\\label{fig: 5_3_1}\n\\end{figure}",
            "fig: 5_3_2": "\\begin{figure*}[t]\n\\centering\n\\begin{tikzpicture}\n\\draw (0,0 ) node[inner sep=0] {\\includegraphics[width=2\\columnwidth, trim={0cm 0cm 0cm 0cm}, clip]{./figs/bias_blue_short.pdf}};\n\\end{tikzpicture}\n\\caption{\nAverage query sample number classified to classes with 7, 8 and 9 training samples under AG's News 8-shot setting. \n``\\# Predicted query sample” indicates the average number of query samples predicted to the class. \nKNN pooling shows stronger preference to 9-sample classes than classes with fewer training samples. \n}\\label{fig: 5_3_2}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{eq: background_1}\nf_{vocab}(p(\\mathbf{x}); \\theta) = P(p(\\mathbf{x}); \\theta), \n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{eq: background_2}\nf_{cls}(p(\\mathbf{x}); \\theta) = v(f_{vocab}(p(\\mathbf{x}); \\theta)).  \n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq: data_construction_train}\n\\mathcal{D}_{t}^{M} = \\bigcup_{(d_i, d_j) \\in \\mathcal{D}_{t} \\times \\mathcal{D}_{t}} \\{ (p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}), \\mathbf{y}_{ij}) \\},\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq: data_construction_query}\n\\mathcal{D}_{q}^{M} = \\bigcup_{(d_i, d_j) \\in \\mathcal{D}_{q} \\times \\mathcal{D}_{t}} \\{ (p(\\mathbf{x}_{d_i}, \\mathbf{x}_{d_j}), \\mathbf{y}_{ij}) \\}.\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\label{eq: optimization}\n\\begin{split}\n\\hat{\\theta} \n&=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{d^M \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}(f_{vocab}(\\mathbf{x}_{d^M};\\theta), \\mathbf{y}_{d^M}) \\\\\n&=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{d^M \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}_{CE}(v(f_{vocab}(\\mathbf{x}_{d^M};\\theta)), \\phi(\\mathbf{y}_{d^M})), \\\\\n&=\\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{d^M \\in \\mathcal{D}_{t}^{M}} \\mathcal{L}_{CE}(f_{cls}(\\mathbf{x}_{d^M};\\theta)), \\phi(\\mathbf{y}_{d^M})), \n\\end{split}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\label{eq: inference_1}\ns_{d_i} = \\Delta(f_{cls}(p(\\mathbf{x}_{d_q}, \\mathbf{x}_{d_i}); \\hat{\\theta})), \n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\label{eq: inference_2}\ns_{l} \n= \\sum_{d_i \\in \\mathcal{D}_{l}} s_{d_i} / |\\mathcal{D}_{l}|. \n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\label{eq: inference_3}\n\\hat{l} \n= \\mathop{\\arg\\max}\\limits_{l} s_l. \n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\label{eq: inference_4}\ns_{l} \n= \\max_{d_i \\in \\mathcal{D}_{l}} s_{d_i}. \n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\label{eq: inference_5}\ns_{l} \n= |\\{d_i | d_i \\in \\mathcal{D}_{topk}, \\mathbf{y}_{d_i}=l_i\\}|. \n\\end{equation}",
            "eq:11": "\\begin{equation}\n% \\small\n    r_{d_i} \n    = \\frac{\\sum_{d_j \\in \\mathcal{D}_l}{s_{d_j}}}{|\\{d_j|d_j \\in \\mathcal{D}_l\\}|} - \\frac{\\sum_{d_k \\in \\mathcal{D}_t-\\mathcal{D}_l}{s_{d_k}}}{|\\{d_k|d_k \\in \\mathcal{D}_t-\\mathcal{D}_l\\}|}. \n\\end{equation}"
        },
        "git_link": "https://github.com/Dousia/MetricPrompt"
    }
}