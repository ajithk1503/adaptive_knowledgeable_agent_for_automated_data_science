{
    "meta_info": {
        "title": "CoRGi: Content-Rich Graph Neural Networks with Attention",
        "abstract": "Graph representations of a target domain often project it to a set of\nentities (nodes) and their relations (edges). However, such projections often\nmiss important and rich information. For example, in graph representations used\nin missing value imputation, items - represented as nodes - may contain rich\ntextual information. However, when processing graphs with graph neural networks\n(GNN), such information is either ignored or summarized into a single vector\nrepresentation used to initialize the GNN. Towards addressing this, we present\nCoRGi, a GNN that considers the rich data within nodes in the context of their\nneighbors. This is achieved by endowing CoRGi's message passing with a\npersonalized attention mechanism over the content of each node. This way, CoRGi\nassigns user-item-specific attention scores with respect to the words that\nappear in an item's content. We evaluate CoRGi on two edge-value prediction\ntasks and show that CoRGi is better at making edge-value predictions over\nexisting methods, especially on sparse regions of the graph.",
        "author": "Jooyeon Kim, Angus Lamb, Simon Woodhead, Simon Peyton Jones, Cheng Zheng, Miltiadis Allamanis",
        "link": "http://arxiv.org/abs/2110.04866v1",
        "category": [
            "cs.LG"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Summary of key notations used in the paper",
                "content": "\n%\n\\label{appendix:notations}\n%\n\n\\renewcommand{\\arraystretch}{1.5}\n%\n\n\n%\n\nWe summarize the key notations used throughout the paper.\n%\nWe group notations in three groups: \n%\n(a) notations on graph sets and the corresponding elements.\n%\n(b) variables and parameters used to describe the forward pass of \\modelName.\n%\n(c) notations used to describe the content information associated to content or item nodes.\n%\n%\n\n%\nIn addition to this, we have the trainable weights explained during the message passing of \\modelName:\n%\n$\\mathbf{P}^{(l)}$ and $\\mathbf{Q}^{(l)}$ for updating node embeddings,\n%\n$\\mathbf{W}_U^{(l)}$, $\\mathbf{W}_M^{(l)}$, and $\\mathbf{p}^{(l)}$ for computing attention coefficients,\n%\nand $\\mathbf{w}_{\\text{out}}$ and $b$ for the prediction MLP.\n%\n\n\\clearpage\n\n"
            },
            "section 2": {
                "name": "Additional experiments",
                "content": "\n%\n\\renewcommand{\\arraystretch}{1.2}\n%\n",
                "subsection 2.1": {
                    "name": "Truncation threshold and test performance",
                    "content": "\n%\n\\label{appendix:truncation}\n%\n\n%\n\n%\nWe test the affect of the truncation threshold $T$ on the test performance.\n%\nIn sequential content encoders $\\mathbf{F}$ such as the transformer encodes $\\mathbf{D_m}$, i.e., the contents of an item node $v_m$ of size $n(m)$ into a set of content vector representations $\\mathbf{Z}_m$.\n%\nDuring encoding, if $n(m)$ is greater than the truncation threshold $T$, we only the first $T$ words only, i.e., $\\vert \\mathbf{Z}_m \\vert = \\min (n(m), T)$.\n%\nSetting $T$ to a high value enables \\modelName to fully exploit the content information, at a trade-off that makes the models slow to train with larger memory requirement.\n%\n\nFig.~\\ref{fig:appendix_truncation_eedi} and~\\ref{fig:appendix_truncation_goodreads} show the test performance with respect to varying values of $T$ on the Goodreads and Eedi datasets.\n%\nFor both datasets, increasing $T$ results in higher test performance ($T$ greater than 64 results in the memory error on our computing infrastructure).\n%\nThe average number of words on Eedi questions 20.02 (Tbl.~\\ref{tab:data_statistics}), and the test performance converges at $T=32$.\n%\nOn the other hand, the average number of words on Goodreads book descriptions is 132.32, and we observe that the test performance does not fully converge at $T=64$.\n\n%\n"
                },
                "subsection 2.2": {
                    "name": "Performance comparison on caching trick",
                    "content": "\n%\nIn Sec.~\\ref{subsection:complexity}, we introduce a caching trick that allows the reduction in training time and memory requirement.\n%\nSpecifically, the caching trick is realized by creating a cache for $\\mathbf{e}_{ij, \\text{CA}}^{(l)}$ with zero initializations.\n%\nWe then update at the final layer $L$ only, by computing $\\mathbf{e}_{ij, \\text{CA}}^{(L)}$ using Eq.~\\ref{eqn:edge_emb} and updating the cache for all $\\mathbf{e}_{ij, \\text{CA}}^{(l)}$ to the computed $\\mathbf{e}_{ij, \\text{CA}}^{(L)}$.\n%\nUsing the caching trick along with the neighbor sampling~\\citep{hamilton2017inductive}, the time complexity reduces from\n%\n\\begin{multline*}\n%\n\\mathcal{O} \\Big (\n    %\n    \\vert \\mathcal{V}_U \\vert \\cdot \\mathit{C}^{(l-1, h)} \\cdot \\mathit{C}^{(l, e)}\n    +\n    \\vert \\mathcal{V}_M \\vert \\cdot \\mathit{T} \\cdot \\mathit{D} \\cdot \\mathit{C}^{(l, e)} \\\\\n    +\n    \\vert \\mathcal{E}\\vert \\cdot \\mathit{T} \\cdot \\mathit{C}^{(l, e)}\n    %\n\\Big )\n\\end{multline*}\n%\nto\n%\n\\begin{multline*}\n%\n\\mathcal{O} \\Big(\n    %\n    \\vert \\mathcal{N}(\\mathcal{V}_{M}^\\prime) \\vert \\cdot \\mathit{C}^{(l-1, h)} \\cdot \\mathit{C}^{(l, e)}\n    +\n    \\vert \\mathcal{V}_M^\\prime \\vert \\cdot \\mathit{T} \\cdot \\mathit{D} \\cdot \\mathit{C}^{(l, e)} \\\\\n    +\n    \\vert \\mathcal{E}^\\prime \\vert \\cdot \\mathit{T} \\cdot \\mathit{C}^{(l, e)}\n    %\n\\Big),\n\\end{multline*}\n\nwhere we sample a subset of nodes $\\mathcal{V}^\\prime = \\big\\{ \\mathcal{V}_{U}^\\prime \\cup \\mathcal{V}_{M}^\\prime \\big\\}$ for the neighbor sampling and only update $\\mathbf{e}_{ij, \\text{CA}}$ whose target node $v_j$ is in $\\mathcal{V}_{M}^\\prime$ and source node $v_i$ is in $\\mathcal{N}(\\mathcal{V}_{M}^\\prime)$.\n%\n\\renewcommand{\\arraystretch}{1.5}\n%\n\n\nTbl.~\\ref{tab:appendix_caching} compares the predictive performance of \\modelName with and without the caching trick on Goodreads dataset in terms of RMSE and wall-clock training time.\n%\nThe predictive performance comparison on Eedi dataset was not feasible due to excessive memory requirement in the absence of the caching trick.\n%\nIn the absence of the caching trick, the predictive performance remains the same, but the training time is increased more than $4$ times per iteration.\n%\n\\clearpage\n\n%\n"
                },
                "subsection 2.3": {
                    "name": "Comparison on different combination methods",
                    "content": "\n%\n\\label{appendix:combination_methods}\n%\n\n%\n\n%\n\nIn Sec.~\\ref{sec:method} we introduce two ways to augment the computed content-attention (CA) edge embeddings $\\mathbf{e}_{ij, \\text{CA}}^{(l)}$ to edge embeddings $\\mathbf{e}_{ij}^{(l)}$ between nodes $v_i$ and $v_j$ at $l^{th}$ layer.\n%\nThe first method is to first update the edge embeddings without the content attention ($\\mathbf{e}_{ij}^{(l)\\prime}$) and use the element-wise addition.\n%\nThe second method is to concatenate with the input edge feature $\\mathbf{e}_{ij}^{(0)}$.\n%\nWe compare the predictive performances of these methods for the Goodreads and Eedi datasets.\n%\nIn Tables~\\ref{tab:appendix_combination_goodreads} and \\ref{tab:appendix_combination_eedi}, element-wise addition yields better predictive performance than concatenation for on both datasets with varying methods of attention computation: concat and dot-product.\n%\n\n\n%\n\n\n%\n%\n"
                },
                "subsection 2.4": {
                    "name": "Bi-directional setting for \\modelName",
                    "content": "\n%\nIn the formulation introduced in Eq.~\\ref{eqn:edge_emb}, the computation of the content attention vector is skipped when the target node is an item and the source node is a user. In fact, \\modelName works in more general settings than the recommendation system, where every node can potentially be associated with contents. However, in a recommendation system with bipartite graphs, it is common that only item nodes are associated with such content information. We have made a change in this formulation so that the update of the content attention (CA) vector is now bi-directional, e.g., whenever gets updated, update in the same fashion instead of skipping. In Tables~\\ref{tbl:bidirection1} and~\\ref{tbl:bidirection2}, we report results when the update of CA edge representation is bi-directional by simultaneously updating both from $i$ to $j$ and $j$ to $i$ edges. The results show the significant improvement in the predictive performance.\n%\n\n% \\pagebreak\n%\n\n"
                }
            },
            "section 3": {
                "name": "Additional details on model training",
                "content": "\n%\n",
                "subsection 3.1": {
                    "name": "Datasets",
                    "content": "\n%\n\\label{appendix:datasets}\n%\nHere, we provide additional information about the two real-world datasets used in our experiment.\n%\nWe chose the Goodreads and Eedi datasets because they contains text information in sentences associated with each item. \n\n%\nWe use Goodreads dataset from~\\citet{goodreads2020}.\n%\nWe filtered out books whose descriptions are written in non-English languages,\n%\nand removed duplicate books based on their titles.\n%\nOriginally, the ratings were text-based.\n%\nWe converted the ratings as follows:\n%\n\"Did not like it\" to rating 1,\n%\n\"It was okay\" to rating 2,\n%\n\"Liked it\" to rating 3,\n%\n\"Really liked it\" to rating 4,\n%\nand \"It was amazing\" to rating 5.\n%\n\n%\nWe used Eedi dataset from~\\citet{wang2020diagnostic}.\n%\nThe content of the text information is extracted using optical character recognition (OCR) from the raw question images, as no question text is available.\n\n%\nIn order to train \\modelName and comparison models on a single GPU within our computation infrastructure, \n%\nwe took a subset of the Eedi dataset,\n%\ntaking student responses from between March $4^{th}$ and March $27^{th}$.\n%\n"
                },
                "subsection 3.2": {
                    "name": "Configurations for the baseline methods",
                    "content": "\n%\n\\label{appendix:additional_config}\n%\nWe detail the configurations used specific to each baseline for recording the test performance.\n%\nFor GC-MC, we assign the separate message passing channels and their corresponding parameters for modeling different discrete edge labels.\n%\nThe number of layer is set to 1, and We do not use the weight sharing method.\n%\nFor the accumulation method, we use concatenation.\n%\nFor GraphSAGE, we use the neighbor sampling size of 32 throughout all message passing layers.\n%\nFor GRAPE, we do not use the one-hot node initializations for both Eedi and Goodreads, because of the large number of item nodes leading to GPU memory errors.\n%\nInstead, we use random initialization just like all GNN model configurations in our experiment.\n%\nFor GAT, we use a single self-attention head. Alternatively, we also tested using multi-head attention with 4 heads with smaller $C^{(l,h)} = 16$, but the predictive performance did not increase. We could not test multi-head attention with $C^{(l,h)} = 64$ due to the GPU memory limits.\n%\nFor GIN, we make the epsilon parameter trainable (Noted as GIN-$\\epsilon$ in the original GIN paper). For JK, we choose LSTM during aggregation (Noted as JK-LSTM in the original JK paper). Both settings have shown to perform best amongst all configurations in the respective papers. \n\n%\n"
                },
                "subsection 3.3": {
                    "name": "Computing infrastructure",
                    "content": "\n%\n\\label{appendix:computation}\n%\nEach experiment was run on a single GPU, which was either an NVIDIA Tesla K80 or an NVIDIA Tesla V100.\n%\nAll experiments were scheduled and performed in Azure Machine Learning.\n%\n\n"
                }
            }
        },
        "tables": {
            "tab:notations": "\\begin{table*}[h]\n\\centering\n\\caption{Key notations used in the paper.}\n\\label{tab:notations}\n\\begin{tabular}{@{}lll@{}}\n\\toprule\n &\n  \\multicolumn{1}{c}{Symbols} &\n  \\multicolumn{1}{c}{Description} \\\\ \\midrule\n\\multirow{4}{*}{\\begin{tabular}[c]{@{}l@{}}\\textbf{Graph sets}\\\\ \\& \\textbf{elements}\\end{tabular}} &\n  $\\mathcal{V}$ &\n  The set of all nodes in the graph \\\\\n &\n  $\\mathcal{V}_{C}, \\mathcal{V}_M, \\mathcal{V}_U \\subset \\mathcal{V}$ &\n  The sets of content, item, and user nodes \\\\\n &\n  $\\mathcal{E}$ &\n  The set of all edges in the graph \\\\\n &\n  $\\mathcal{N}(i)$ &\n  Neighborhood function for node $v_i$ \\\\ \\midrule\n\\multirow{8}{*}{\\begin{tabular}[c]{@{}l@{}}\\textbf{\\modelName}\\\\ \\textbf{variables}\\end{tabular}} &\n  $\\mathbf{h}_i^{(0)}$ &\n  Input feature of node $v_i$ of size $C^{(l, h)}$ \\\\\n &\n  $\\mathbf{e}_{ij}^{(0)}$ &\n  Input feature of edge $e_{ij}$ of size $C^{(l, e)}$ \\\\\n &\n  $\\mathbf{h}_i^{(l)}$ &\n  Node embedding of $v_i$ at $l^{th}$ layer \\\\\n &\n  $\\mathbf{e}_{ij}^{(l)}$ &\n  Edge embedding between $v_i$ and $v_j$ at $l^{th}$ layer \\\\\n &\n  $\\mathbf{e}_{ij}^{(l)\\prime}$ &\n  Edge embedding before content update \\\\\n &\n  $\\mathbf{e}_{ij, \\text{CA}}^{(l)}$ &\n  Edge embedding from content-attention \\\\\n &\n  $c_{ik}^{(l)}$ &\n  Attention coefficient between $v_i$ and content $k$ \\\\\n &\n  $\\alpha_{ik}^{(l)}$ &\n  Attention probability from $c_{ik}^{(l)}$ after \\textsc{Softmax} \\\\ \\midrule\n\\multirow{4}{*}{\\begin{tabular}[c]{@{}l@{}}\\textbf{Content-related}\\\\ \\textbf{notations}\\end{tabular}} &\n  $n(i)$ &\n  The number of content vectors associated to $v_i$ \\\\\n &\n  $\\mathbf{Z}_{i} = \\{ \\mathbf{z}_k^{(\\mathit{i})}  \\}_{1}^{n(i)} \\subset \\mathbb{R}^D  $ &\n  A set of content vectors associated to $v_i$ \\\\\n &\n  $\\mathbf{D}_i = [w_1^{(i)}, \\cdots, w_{n(i)}^{(i)}]$ &\n  A sequence of words associated to $v_i$ \\\\\n &\n  $\\mathbf{F}$ &\n  A sequence encoder that projects $\\mathbf{D}_i$ to $\\mathbf{Z}_i$ \\\\ \\bottomrule\n\\end{tabular}\n\\end{table*}",
            "tab:appendix_caching": "\\begin{table}[t!]\n\\centering\n\\caption{Performance comparison with and without the caching trick on Goodreads dataset.}\n\\label{tab:appendix_caching}\n\\begin{tabular}{@{}lrr@{}}\n\\toprule\n                & \\multicolumn{1}{c}{RMSE} & \\multicolumn{1}{r}{\\begin{tabular}[r]{@{}r@{}}Training time \\\\ / iteration (sec.)\\end{tabular}} \\\\ \\midrule\nWith caching    & 0.879${}_{\\pm 0.000}$                   & 10.41                                                                                           \\\\\nWithout caching & 0.879${}_{\\pm 0.000}$                    & 41.86                                                                                           \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:appendix_combination_goodreads": "\\begin{table}[t!]\n\\centering\n\\caption{\n%\n    Comparison of different combination methods for updating the edge embedding $\\mathbf{e}_{ij}^{(l)}$ on the Goodreads dataset. \n    %\n    We report RMSE (lower the better).\n    %\n%\n}\n\\label{tab:appendix_combination_goodreads}\n\\begin{tabular}{@{}lrr@{}}\n\\toprule\nCombination method &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}\\modelName\\\\ :Concat\\end{tabular}} &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}\\modelName\\\\ :Dot-product\\end{tabular}} \\\\ \\midrule\n$\\mathbf{e}_{ij}^{(l)\\prime} + \\mathbf{e}_{ij, \\text{CA}}^{(l)}$ &\n  0.879${}_{\\pm0.000}$ &\n  0.879${}_{\\pm0.000}$ \\\\\n$\\textsc{Concat} (\\mathbf{e}_{ij}^{(0)}, \\mathbf{e}_{ij, \\text{CA}}^{(l)})$ &\n  0.886${}_{\\pm0.000}$ &\n  0.884${}_{\\pm0.001}$ \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:appendix_combination_eedi": "\\begin{table}[t!]\n\\centering\n\\caption{\n%\n    Comparison of different combination methods for updating the edge embedding $\\mathbf{e}_{ij}^{(l)}$ on the Eedi dataset. \n    %\n    We report test accuracy (higher the better).\n    %\n%\n}\n\\label{tab:appendix_combination_eedi}\n\\begin{tabular}{@{}lrr@{}}\n\\toprule\nCombination method &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}\\modelName\\\\ :Concat\\end{tabular}} &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}\\modelName\\\\ :Dot-product\\end{tabular}} \\\\ \\midrule\n$\\mathbf{e}_{ij}^{(l)\\prime} + \\mathbf{e}_{ij, \\text{CA}}^{(l)}$ &\n  0.756${}_{\\pm0.001}$ &\n  0.757${}_{\\pm0.001}$ \\\\\n$\\textsc{Concat} (\\mathbf{e}_{ij}^{(0)}, \\mathbf{e}_{ij, \\text{CA}}^{(l)})$ &\n  0.752${}_{\\pm0.001}$ &\n  0.752${}_{\\pm0.001}$ \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tbl:bidirection1": "\\begin{table*}[t!]\n\\centering\n\\caption{Predictive performance of \\modelName for bidirectional setting when concatenation is used for attention computation.}\n\\begin{tabular}{@{}lccrr@{}}\n\\toprule\n\\multicolumn{1}{c}{\\textbf{Concat}} & Goodreads                         & \\multicolumn{3}{c}{EEDI}                                                                 \\\\ \\midrule\n                                    & RMSE ($\\downarrow$)                              & Accuracy ($\\uparrow$)                          & \\multicolumn{1}{c}{AUROC ($\\uparrow$)} & \\multicolumn{1}{c}{AUPR ($\\uparrow$)} \\\\\nUser-only                           & \\multicolumn{1}{r}{0.879 $\\pm$ 0.000} & \\multicolumn{1}{r}{0.756 $\\pm$ 0.002} & 0.715 $\\pm$ 0.002             & 0.874 $\\pm$ 0.002            \\\\\n\\textbf{Bidirectional}              & \\multicolumn{1}{r}{\\textbf{0.873}$\\pm$0.001} & \\multicolumn{1}{r}{\\textbf{0.761}$\\pm$0.001} & \\textbf{0.720}$\\pm$0.002             & \\textbf{0.891}$\\pm$0.001            \\\\ \\bottomrule\n\\end{tabular}\n\\label{tbl:bidirection1}\n\\end{table*}",
            "tbl:bidirection2": "\\begin{table*}[t!]\n\\centering\n\\caption{Predictive performance of \\modelName for bidirectional setting when dot-product is used for attention computation.}\n\\begin{tabular}{@{}lccrr@{}}\n\\toprule\n\\multicolumn{1}{c}{\\textbf{Dot-product}} & Goodreads                         & \\multicolumn{3}{c}{EEDI}                                                                 \\\\ \\midrule\n                                    & RMSE ($\\downarrow$)                              & Accuracy ($\\uparrow$)                          & \\multicolumn{1}{c}{AUROC ($\\uparrow$)} & \\multicolumn{1}{c}{AUPR ($\\uparrow$)} \\\\\nUser-only                           & \\multicolumn{1}{r}{0.879 $\\pm$ 0.001} & \\multicolumn{1}{r}{0.757 $\\pm$ 0.001} & 0.717 $\\pm$ 0.001             & 0.874 $\\pm$ 0.001            \\\\\n\\textbf{Bidirectional}              & \\multicolumn{1}{r}{\\textbf{0.872}$\\pm$0.001} & \\multicolumn{1}{r}{\\textbf{0.760}$\\pm$0.002} & \\textbf{0.721}$\\pm$0.002             & \\textbf{0.888}$\\pm$0.001            \\\\ \\bottomrule\n\\end{tabular}\n\\label{tbl:bidirection2}\n\\end{table*}"
        },
        "figures": {
            "fig:appendix_truncation_eedi": "\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figs/truncation_goodreads.pdf}\n    \\caption{Truncation size and test accuracy for the Goodreads dataset.  Note semi-log-$x$ and starting point for $y$-axis.}\n    \\label{fig:appendix_truncation_eedi}\n\\end{figure}",
            "fig:appendix_truncation_goodreads": "\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figs/truncation_eedi.pdf}\n    \\caption{Truncation size and RMSE for the Eedi dataset. Note semi-log-$x$ and starting point for $y$-axis.}\n    \\label{fig:appendix_truncation_goodreads}\n\\end{figure}"
        }
    }
}