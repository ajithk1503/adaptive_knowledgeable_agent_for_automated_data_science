{
    "meta_info": {
        "title": "TFAD: A Decomposition Time Series Anomaly Detection Architecture with  Time-Frequency Analysis",
        "abstract": "Time series anomaly detection is a challenging problem due to the complex\ntemporal dependencies and the limited label data. Although some algorithms\nincluding both traditional and deep models have been proposed, most of them\nmainly focus on time-domain modeling, and do not fully utilize the information\nin the frequency domain of the time series data. In this paper, we propose a\nTime-Frequency analysis based time series Anomaly Detection model, or TFAD for\nshort, to exploit both time and frequency domains for performance improvement.\nBesides, we incorporate time series decomposition and data augmentation\nmechanisms in the designed time-frequency architecture to further boost the\nabilities of performance and interpretability. Empirical studies on widely used\nbenchmark datasets show that our approach obtains state-of-the-art performance\nin univariate and multivariate time series anomaly detection tasks. Code is\nprovided at https://github.com/DAMO-DI-ML/CIKM22-TFAD.",
        "author": "Chaoli Zhang, Tian Zhou, Qingsong Wen, Liang Sun",
        "link": "http://arxiv.org/abs/2210.09693v2",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "Accepted by the ACM International Conference on Information and  Knowledge Management (CIKM 2022)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n%With the development of large systems (financial markets, Internet of Things systems, et al.) in reality, it is significant to monitor and detect anomalies in time series data to discover faults and avoid potential risks.\n%Generally, an anomaly is an observation that deviates from normality, and its detection has been widely studied, including statistics, data mining, and machine learning \\cite{ruff2021unifying}. \n\nWith the rapid development of the Internet of Things (IoT) and other monitoring systems, there has been an enormous increase in time series data~\\cite{robustts22,forecastFaloutsos20}. Thus, effectively monitoring and detecting anomalies or outliers on the time series data is crucial to discovering faults and avoiding potential risks in many real-world applications. Generally, an anomaly is an observation that deviates from normality. Anomaly detection has been studied widely in different disciplines, including statistics, data mining, and machine learning \\cite{ruff2021unifying}, but how to perform it effectively on time series data is an active research topic and has received a lot of attentions recently~\\cite{ren2019time,gao2020robusttad,zhang2021cloudrca,lai2021revisiting,tuli2022tranad,li2022learning,AnomalyKiTs_2022} due to the special properties of time series. \n\n% \\cite{anomalyVLDB22,xu2021anomaly, }\n% % \n\nUnlike ordinary tabular data, one distinguishing property of time series is the temporal dependencies. Usually, a point or a subsequence of time series is called an anomaly when compared to its corresponding ``context\". Based on the relationship between the anomaly in time series and its context, we can define different types of anomalies, such as global point anomaly, seasonality anomaly, shapelet anomaly, etc. Thus, the first challenge in time series anomaly is how to model the relationship between a point/subsequence and its temporal context for different types of anomalies. Secondly, like other anomaly detection tasks, anomaly happens rarely, and there is usually limited labeled data for data-driven models. A possible solution is data augmentation, which is widely used in deep learning training~\\cite{shorten2019survey}. Although some data augmentation methods have been proposed for time series data~\\cite{wen2020time}, how to design and apply data augmentation in time series anomaly detection remains an open problem. \n\n\n\n% Due to the complex temporal dynamics in time series, sequential anomaly detection is challenging. Firstly, although time series is composed of points one by one, a single point sample wise detection method usually fails. %it does not work if we detect point anomalies using methods in images directly. \n% The main difference is that an individual point in time series can not be considered as a general \\emph{sample}. Instead, temporal relations among points need to be fully considered. \n%Secondly, with complex temporal dependence among points, there are diverse time series anomalies, global point anomaly, context point anomaly, seasonality anomaly, shapelet anomaly, et al. Different types of anomalies usually have rather different patterns. Therefore, it is hard to design a general method to detect all different anomalies. %Specifically speaking, although both time series data and image data have group anomalies, the situations are rather different. For image data, every anomaly in group is also abnormal if considered individually. But in time series, the group anomaly is usually an abnormal sub-sequence where each point may seem to be normal individually but the pattern of the sub-sequence is rather different from others. \n%Thirdly, similar to other anomaly detection situations, anomaly happens rarely, and there is usually not enough labeled data for data-driven models. Data augmentation is widely used in computer vision \\cite{shorten2019survey}. It is natural to produce more training images by doing RGBshift, ChannelShuffle, filping, et al. However, for time series anomaly detection, producing diverse, reasonable, useful augmented time series data is still an open question.\n\nAs a typical signal, the time series data can be analyzed not only from the time domain but also from the frequency domain~\\cite{ts:textbook:1994}. Most of the existing methods, including conventional and deep methods, focus mainly on time-domain modeling and do not fully utilize the information in the frequency domain. The frequency domain can provide vital information for time series, such as seasonality~\\cite{robustPeriod:2021}. In addition, it is much easier to detect in the frequency domain than in the time domain for some complex group anomalies and seasonality anomalies. Recently, there have been some attempts to model time series from the frequency domain~\\cite{parhizkar2015sequences}, such as the data augmentation in the frequency domain~\\cite{gao2020robusttad}. \n% we notice the difficulty to design a general frame single structure to detect anomalies both in time domain and frequency~\\cite{parhizkar2015sequences}, \nUnfortunately, how to systematically and directly utilize the frequency domain and time domain information simultaneously in modeling time series anomaly detection is still not fully explored in the literature. \n\n% The frequency domain can provide vital information for time series, such as seasonality~\\cite{cPD_vlachos2005Autoperiod,robustPeriod:2021}.\n\n\n\n% Great efforts are taken to utilize the temporal relation information in time series. While traditional methods take advantage of sequential time series characteristics by decomposition (decompose original time series into trend, seasonal and residual components), sequential-wise distance design et al, deep methods gain rich temporal relation information of the complex dynamics by powerful representation learning module of neural network. Though they have achieved great success, the question of how to effectively use the time-frequency property of time series for anomaly detection has not received enough attention. Most of the existing methods, including traditional and deep methods, focus solely on time-domain modeling. However, detection in the frequency domain is far easier and natural for some complex group anomalies. % Traditional methods take advantages of sequential time series characteristics by time series decomposition, sequential-wise distance design and seasonality analysis, et al. \n\n%The uncertainty principle~\\cite{parhizkar2015sequences} shows the difficulty of designing a single structure to detect anomalies both in time domain and frequency domain simultaneously. So without a detailed design, models that focus on time-domain anomaly detection would have limited detection power in the frequency domain. %However, most of the existing methods focus only on direct time domain analysis which limits the detection power in frequency domain, e.g., seasonal anomaly. % Frequency domain analysis widely used in signal processing~\\cite{sandryhaila2014discrete, qian1996joint} to help better identify the characteristics of signal and make it easy to add/subtract frequencies to the original signal, is not fully considered in time series anomaly detection. \n%Frequency domain analysis can help identify particular pattern changes in time series, like seasonality anomaly and group anomaly. Moreover, a different perspective from the frequency domain can make model detection more robust. %For instance, seasonality anomaly, which is usually hard to detect in time domain, can be easily detected in frequency domain. \n\n% In this work, to better detect various kinds of time series anomalies, we proposed a \\textbf{T}ime-\\textbf{F}requency domain analysis time series \\textbf{A}nomaly \\textbf{D}etection model, TFAD. It mainly contains two branches: time domain analysis branch and frequency domain analysis branch. \n%Although deep methods have an advantage over traditional methods in representation, existing deep methods take less consideration of time series characteristics. \n\n\nIn this paper, to better detect various kinds of time series anomalies, we proposed a \\textbf{T}ime-\\textbf{F}requency domain analysis time series \\textbf{A}nomaly \\textbf{D}etection model, TFAD. It mainly contains two branches: the time-domain analysis branch and the frequency domain analysis branch. Specifically, with a well-designed window-based model structure, we implement a time series decomposition module to detect anomalies in different components with interference among different components reduced and a representation learning module with a neural network to gain richer sequence information. \nTo deal with the challenges of insufficient anomaly data, we conduct data augmentation of TFAD in different views: normal data augmentation, abnormal data augmentation (not fully considered in existing works), time-domain data augmentation, and frequency domain data augmentation. \n\n% highlights of contributions:\n% 1. Deep learning model from both time and frequency domains\n% 2. Integrating time series decomposition into the deep learning framework\n% 3. Systematic data augmentation methods for limited anomaly labels\n\n\n\n\nIn summary, our main contributions are listed as follows:\n\\begin{itemize}\n\\item We integrate the frequency domain analysis branch with the time domain analysis branch to identify the temporal information and improve detection performance. \n\\item We combine the time series decomposition module with a concise neural representation network. With the help of time series decomposition, a simple temporal convolution neural network performs well. Moreover, it makes the model easy to be implemented, and the anomaly results of different components give insights into why it is abnormal.\n\\item Various data augmentation methods, besides normal data augmentation and time domain data augmentation, abnormal data augmentation and frequency domain data augmentation are also implemented to overcome the lack of anomaly data.\n\\end{itemize}\n\n\n\n\n\n\n\n% With the development of information technology, more and more data can be collected with rich time information. Time series analysis is used in almost every field, such as the stock market, tracking key performance indicators, medical sensor technologies. \n% Anomaly detection is one of the basic task in time series analysis and has been widely explored in recent years. It is quite important to find the anomalies/outliers in real-world efficiently. \n\n% Due to the fast development in deep learning and the power of representation networks, there are many deep anomaly detection algorithms in images or text domain~\\cite{ruff2021unifying}. \n% Most of them are based on reconstruction model which distinguish anomalies by reconstruction error. \n% It is natural for images or text anomalies be detected through the distance to normal distribution. \n% However, the situation in time series is different. \n\n% Firstly, although time series is composed by points one by one, it does not work if we detect point anomalies using methods in images or text. One point individually is not a \"sample\". Secondly, in time series, context information is rather important. We can not determine whether one point is abnormal without context sequence. Thirdly, although there are also group anomalies in time series, the situation is also different. Generally, for images, every anomaly in group is also abnormal if considered individually. But in time series, the group anomaly is usually an abnormal sub-sequence where every point may seem to be normal but the pattern of the sub-sequence is rather different from others. \n\n% Considering the above characteristics in time series anomaly, one of the kernel issue is to model the time series appropriately. \n% One widely used scheme is to split the time series into windows and consider the window as individual sample. \n% The intuition is that to get context information we need a sequence of point. \n% It is similar with text analysis while a sentence is a sequence with relatively complete/necessary semantics. \n\n% Deep learning method achieves great success in many fields except time series anomaly detection, benefiting from its power in representation. \n% Many deep representation anomaly detection algorithms in time series are for multi-variate time series. \n% Most of them gain contextual sequence information by representation and the design is sophisticated and complex. \n% Multi-variate time series usually has richer information which may be a hinder in classical methods but a favour for deep techniques. Many deep learning methods for multi-variate appears~\\cite{shen2020timeseries, ruff2021unifying, ruff2018deep, xu2021anomaly, zong2018deep}.\n\n% Criticism in time series anomaly detection using deep learning method has appeared recent years. Firstly, it requires vast amounts of data. Unlike computer vision, time series filed usually has limited amount of data. It limits the application of deep methods.  Results in \\cite{lai2021revisiting} show that some classical algorithm can over perform many recent deep learning methods. In this paper, we propose TFAD algorithm which is combined traditional time series analysis technologies with popular representation networks. Although representation network is used, we mainly use it as a powerful auxiliary in distance computing. \n\n% TFAD take the advantage of frequency domain analysis in pattern-wise time series anomaly detection and is composed of frequency domain analysis branch and classical time domain analysis branch. To get rich semantics information, window-based architecture is used. As to representation network, convolutional network for time series, that is, TCN (temporal convolutional network) is employed. \n\n% In many classical time series anomaly detection works, time series decomposition~\\cite{wen2019robuststl,wen2020fast} contributes a lot to protruding anomalies in different components~\\cite{gao2020robusttad}.\n% Generally, complex time series can be decomposed into trend, seasonality and remainder components. It is widely used in anomaly detection and forecasting task.\n% The anomalies in time series can also be classified as trend anomaly, seasonal anomaly and anomaly in residual part. \n% When the components are separated from the original complex time series, the interference among different components is reduced and we can focus on every component respectively. \n \n% Our main contributions are listed as follows:\n% \\begin{itemize}\n% \t\\item combination of different kinds of data augmentation: normal data augmentation and abnormal data augmentation, time domain data augmentation and frequency domain data augmentation. The experiment results show that, although normal data augmentation contributes tiny improvement in performance, when combined with abnormal data augmentation, the improvement is enlarged. \n% \t\\item integrating frequency domain analysis branch with time domain analysis branch to get better detection on pattern-wise anomaly. We show why two different branches are needed and the experiments show not only the performance on accuracy is improved but also the robustness is improved.\n% \t\\item with time series decomposition module implemented in model processing, interpretability of TFAD is gained.\n% \\end{itemize}\n\nThe rest of the paper is organized as follows. In Section~\\ref{RelatedWork}, we review the related work. \nIn Section~\\ref{Preliminaries}, we briefly introduce the definitions of time series anomalies.\nIn Section~\\ref{method}, we introduce our proposed TFAD algorithm, including motivations, architecture, and network design. In Section~\\ref{experiments}, we evaluate our algorithm empirically on both univariate and multivariate time series datasets in comparison with other state-of-the-art algorithms. An ablation study is also performed to analyze different modules in the network. And we conclude our discussion in Section~\\ref{Conclusion}. \n\n \n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\\label{RelatedWork}\n\n% \\subsection{Anomaly Detection}\n% \u4ecb\u7ecd\u5df2\u6709\u7684\u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\u76f8\u5173\u7684\u6587\u7ae0\u548c\u7b97\u6cd5\n\n% \\subsection{Data Augmentation}\n% \\subsection{Time Series Decomposition}\n% \\subsection{Temporal Convolutional Networks}\n% \\subsection{Contrastive Learning}\n\n% This section will briefly review the related architectures in time series anomaly detection. \\cite{laptev2015generic} demonstrates a myth that there is an anomaly detection method that is optimal in all domains due to the complexity of anomalies in reality. \\cite{lai2021revisiting} revisits the anomaly definitions and benchmarks in time series data. \n\nBoth traditional and deep methods have been applied in time series anomaly detection tasks. \nA survey of traditional techniques for time series anomaly detection is in~\\cite{gupta2013outlier}. \nTraditional techniques can be roughly classified as similarity-based~\\cite{lane1997sequence, guan2016mapping}, window-based~\\cite{gao2002hmms}, decomposition-based~\\cite{gao2020robusttad, wen2019robuststl}, deviants detection based methods~\\cite{muthukrishnan2004mining}, et al. \nWith the rapid development of deep learning, a vast of deep anomaly detection methods emerge~\\cite{kiran2018overview,hendrycks2018deep,ruff2021unifying}, including one-class type SVDD-based model~\\cite{ruff2018deep}, reconstruction type GAN/VAE-based model~\\cite{rezende2014stochastic}, et al. \nDeep methods are also applied in time series anomaly detection~\\cite{shen2020timeseries, xu2021anomaly, zong2018deep}. \n% mainly credited to the powerful representation learning of deep neural network. \n\n% In another view, detection techniques vary depending on input data type, that is, three are different methods for uni-variate time series anomaly detection and multi-variate time series anomaly detection. \n% anomaly in time series data can be classified as point-wise and pattern-wise anomalies. \n\n% There is a vast sea of literature on time series anomaly detection. \n\n\nAccording to the input data type, there are methods designed for univariate time series, multivariate time series, or both. According to the access of labels, there are unsupervised, semi-supervised, and supervised time series anomaly detection methods. \nOnline anomaly detection~\\cite{chen2022adaptive, bock2022online} is also quite different from offline settings. \nFor univariate time series anomaly detection, many traditional methods have been designed~\\cite{ren2019time, siffer2017anomaly, xu2018unsupervised}. POT~\\cite{siffer2017anomaly} proposes an approach based on Extreme Value Theory to detect outliers without the assumption of distribution. M-ELBO~\\cite{xu2018unsupervised} designs a VAE-based algorithm on the Yahoo dataset. \nFor multivariate time series anomaly detection, the development of representation learning stimulates blowout growth in the multivariate time series anomaly detection field. \nTHOC~\\cite{shen2020timeseries} proposes a temporal classification model for time series anomaly detection by capturing temporal dynamics in multi scales. \nIt follows the one-class classification framework generally used~\\cite{ruff2021unifying, ruff2018deep}. \nAnomaly Transformer~\\cite{xu2021anomaly} designs unsupervised anomaly detection methods combining the transformer framework with a point-wise prior association by a mini-max strategy. \nGenerative models, such as DAGMM~\\cite{zong2018deep}, AnoGAN\\cite{schlegl2017unsupervised}, and LSTM-VAE~\\cite{park2018multimodal}, also form a mainstream research direction. \nSome surveys compare different kinds of anomaly detection methods in time series~\\cite{freeman2021experimental, blazquez2021review}. \n\nThe most related work of this paper is~\\cite{carmona2021neural}, which introduces a window-based framework for anomaly detection in time series applied in unsupervised/supervised and univariate/multivariate settings. \nHowever, like most existing anomaly detection methods, it only works in the time domain. \nAlthough it works well for point-wise anomalies, it is usually hard to detect complex pattern anomalies, e.g., a sub-sequence time series. Instead, AutoAI-TS~\\cite{AutoAI-TS} also takes the advantage of frequency domain analysis for period/seasonal forecasting.\nIn summary, our designed TFAD model contains two branches: the time-domain analysis branch and the frequency domain analysis branch, which is different from existing works. Besides, the time series decomposition module used in traditional methods is also implemented in our TFAD architecture. Furthermore, several well-designed data augmentation methods are also considered in TFAD to gain rich, reasonable, and reliable dataset. \n\n% In this paper, we mainly focus on anomaly detection in uni-variate time series in offline setting and leave the design for multivariate time series as future work.\n% We combine a concise representation network, TCN (Temporal convolutional network)~\\cite{bai2018empirical}, with the traditional time series analysis method to get a good performance. \n% TCN with few layers is not a strong representation network. Our design does not over-rely on the deep learning method. Instead, we are inspired by the power of traditional methods based on the situations. \n\n\n\n% More specifically, we follow the window-based framework in existing works to get semantics information but containing two branches: time domain analysis branch and frequency domain analysis branch. \n\n% Time series decomposition~\\cite{huang1998empirical} has been widely used in traditional analysis and deep learning forecasting~\\cite{wu2021autoformer}. We excavate its power when combined with the deep method in the anomaly detection field.  \n\n% Correspondingly, to handle the common challenge in anomaly detection tasks that labeled data is usually expensive to get, we do data augmentation~\\cite{wen2020time, oh2020time} not only in the time domain but also in the frequency domain. \n\n\n\n\n"
            },
            "section 3": {
                "name": "Preliminaries",
                "content": "\\label{Preliminaries}\n% Time series anomaly detection has been widely studied. \n% Generally, the anomaly in non-sequential data is defined as an observation that deviates considerably from some concept of normality~\\cite{ruff2021unifying}. \n\nIn this Section, we will first give a brief view of the definitions of general anomalies. Anomalies appear in many situations. Anomalies in time series are a special type as the point without information of neighbor points means nothing. Such characteristics make time series analysis different from others. Thus, we will also show the definitions of time series or more specially, saying, sequential anomaly definitions. \n\n",
                "subsection 3.1": {
                    "name": "General Anomaly Definitions",
                    "content": "\nIn applications with non-sequential data, set $\\mathcal{D}\\in R$ as the data space and the normality follows distribution $\\mathcal{N}^{+}$. Assuming $\\mathcal{N}^{+}$ has a corresponding probability density function $p^{+}$, then the anomalies can be set as \n\\begin{equation}\nA=\\{d\\in \\mathcal{D}\\ | p^{+}(d) \\leq \\tau \\}, \\tau>0,\n\\end{equation}\nwhere $\\tau$ is the threshold of anomaly. \n\nAnomalies in non-sequential data can be mainly classified into three types: point anomaly, contextual anomaly, and group anomaly. \nA point anomaly is an individual data point that deviates from normality and is the most common case in anomaly detection. It can also be called a global anomaly. \nA context anomaly is also called a conditional anomaly. \nIt is anomalous in a specific context. \nFor instance, the temperature of 20 degrees is normal in most areas but abnormal in Antarctica.\nA group or collective anomaly is a group of points that abnormal. \n\n% \\begin{figure}[ht]\n% \\centering  \n% \\subfigure[Global Point Anomaly]{\n% \\label{sub1}\n% \\includegraphics[width=0.22\\textwidth]{globalP.png}}\n% \\subfigure[Context Point Anomaly]{\n% \\label{sub2}\n% \\includegraphics[width=0.22\\textwidth]{contP.png}}\n% \\caption{Examples of Point-wise Anomaly.}\n% \\label{point-anomalyClass}\n% \\end{figure}\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Sequential Anomaly Definitions",
                    "content": "\n\n% A time-series anomaly is a kind of anomaly in sequential data. \n% The definitions in non-sequential context can not sufficiently define the group and context anomalies in sequential data. \n% \\cite{lai2021revisiting} refines sequential anomaly definitions. \n\n% To better model the temporal correlations among observations, behaviors of time series should be well-defined. \nTime series data is a sequence of data points $X=(x_0, x_1, x_2, \\cdots, x_n)$ where $x_i$ is the point at timestamp $i$. \nMore specially, time series can be formally defined by structural modeling~\\cite{shumway2000time,lai2021revisiting} to include trend, seasonality and shapelets, as   \n\\begin{equation}\nX =\\sum_n \\{ A\\sin(2\\pi\\omega_n T) + B\\cos(2\\pi\\omega_n T)\\} + \\tau(T),\n\\end{equation}\nwhere $T=(1,2,3,\\cdots, n)$ is a series of timestamps, $\\omega_n$ is the frequency of wave $n$, $A, B$ are coefficients, the combination of sinusoidal wave represents the shapelets and seasonality, and $\\tau(T)$ is trend component. \n\nThe definitions in a non-sequential context can not sufficiently define the group and context anomalies in sequential data. Instead, sequential anomalies can be classified as point anomaly (global point anomaly and context point anomaly) and pattern anomaly (shapelet anomaly, seasonal anomaly, and trend anomaly)~\\cite{lai2021revisiting}, as shown in Figure~\\ref{anomalyClass}.\n\nFormally, point-wise anomalies can be defined as ${|x_t - \\hat{x_t}|}>{\\sigma}$ where $\\hat{x_t}$ is the expected value and $\\sigma$ is threshold.\nShapelet anomaly can be defined as $s(\\rho(\\cdot), \\hat{\\rho}(\\cdot)) > \\sigma,$ where function $s$ measures the difference between two subsequences, $\\hat{\\rho}$ is the expected shapelet and $\\sigma$ is a threshold.\nSeasonal anomaly refers to subsequence with abnormal seasonality and it is defined as $s(\\omega, \\hat{\\omega}) > \\sigma$ where $\\hat{\\omega}$ is expected seasonality. \nTrend anomaly is a subsequence whose trend alters the trend of $X$. It can be defined as $s(\\tau(\\cdot), \\hat{\\tau}(\\cdot))>\\sigma$ where $\\hat{\\tau}$ is the expected trend of subsequence. \n\n\n\n\n\n% In a spectral view, time-series data combines sinusoidal waves and trends.\n% % Sinusoidal waves present the seasonality and the shapelet of the data. \n% % Formally, $X_s=\\sum_n \\{ A\\sin(2\\pi\\omega_n T) + B\\cos(2\\pi\\omega_n T) \\}$ where $T=(1,2,3,\\cdots, n)$ is series of timestamp, $\\omega_n$ is the frequency of wave $n$ and $A, B$ are coefficients. \n% % Combined with trend, \n% Time series can be defined as $$X = \\sum_n \\{A\\sin(2\\pi\\omega_n T) + B\\cos(2\\pi\\omega_n T)\\} + \\tau(T),$$ where $T=(1,2,3,\\cdots, n)$ is series of timestamp, $\\omega_n$ is the frequency of wave $n$, $A, B$ are coefficients, $\\tau(T)$ is trend part. \n% % For simplicity, $X$ can be noted as $X=\\rho (2\\pi T, \\omega) + \\tau(T).$ \n\n\n\n\n\n% \\subsubsection{Point-wise Anomaly}\n\n% Point-wise anomaly in time series is usually a glitch or a spike, an extreme individual value. \n% Intuitively, it can be defined as ${|x_t - \\hat{x_t}|}>{\\sigma}$ where $\\hat{x_t}$ is the expected value and $\\sigma$ is threshold.\n\n% With different thresholds, \\textbf{global} point anomaly and \\textbf{contextual} point anomaly can be defined using the same format above, illustrated in Figure~\\ref{point-anomalyClass}. \n% For global point anomaly, $\\sigma = \\lambda \\cdot \\delta(X)$ where $\\delta(X)$ is the standard deviation of the whole time series and $\\lambda$ controls the range. \n% For contextual point anomaly, ${\\sigma}={\\lambda\\cdot\\delta(X_{t-k, t+k})}$ where $X_{t-k, t+k} = (x_{t-k}, x_{t-k+1},\\cdots, x_{t+k})$ is a context time series window.\n\n\n% \\subsubsection{Pattern-wise Anomaly}\n\n% Different from point-wise anomaly, pattern wise anomaly is more typical in sequential data. With a time series data $X$ given, $X_{i,j}$ denoted as a time sequence staring from timestamp $i$ to $j$ and $X_{i,j}=\\rho(2\\pi\\omega T_{i,j}) + \\tau(T_{i, j})$.  \n% There are mainly three types of pattern-wise anomaly: \\textbf{shapelet} anomaly, \\textbf{seasonal} anomaly and \\textbf{trend} anomaly. \n\n% Shapelet anomaly usually refers to subsequence with different basic shapelets from normal shapelets. \n% Thus shapelet anomaly can be defined as $$s(\\rho(\\cdot), \\hat{\\rho}(\\cdot)) > \\sigma,$$ where function $s$ measures the difference of two subsequence, $\\hat{\\rho}$ is the expected shapelet and $\\sigma$ is a threshold.\n\n% Seasonal anomaly refers subsequence with abnormal seasonalies and it is defined as $$s(\\omega, \\hat{\\omega}) > \\sigma$$ where $\\hat{\\omega}$ is expected seasonality. \n\n% Trend anomaly is subsequence whose trend alter the trend of $X$. It can be defined as $s(\\tau(\\cdot), \\hat{\\tau}(\\cdot))>\\sigma$ where $\\hat{\\tau}$ is the expected trend of subsequence. \n\n\n% \u753b\u70b9\u56fe\u793a\u610f\u4e00\u4e0b\uff1f\n \n% \\subsection{Anomaly Detection Problem Definition}\n% \u8fd9\u90e8\u5206\u5185\u5bb9\u6709\u70b9\u513f\u5c11\uff0c\u5148\u4e0d\u653e\u4e86\u5427\n\n\n% \\subsection{Anomaly Score Definition}\n% \u5185\u5bb9\u592a\u5c11\u7684\u8bdd\uff0c\u53ef\u4ee5\u4e0d\u653e\n% \\section{Methodology}\\label{method}\n"
                }
            },
            "section 4": {
                "name": "Proposed TFAD Algorithm",
                "content": "\\label{method}\n",
                "subsection 4.1": {
                    "name": "Motivation: Time-Frequency Analysis",
                    "content": "\nIn this part, we will provide the design motivation of TFAD algorithm through the uncertainty principle of time-frequency analysis and demonstrate its effectiveness in time series anomaly detection. \n\n",
                    "subsubsection 4.1.1": {
                        "name": "Uncertainty Principle for Time Series Representation in Time and Frequency Domains",
                        "content": "\n\n\nThe uncertainty principle expresses a fundamental relationship between the standard deviation of a continuous function and the standard deviation of its Fourier transformation~\\cite{cohen1995time}. Let the input signal $s(t)$ have a spectrum $S(\\omega)$ as\n\\begin{equation}\nS(\\omega)=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} s(t) e^{-j \\omega t} d t.\n\\end{equation}\nThe standard deviations of the time and frequency density functions, $\\sigma_{t}$ and $\\sigma_{\\omega}$, are defined as the parameters that describe the broadness of the signal in time and frequency domains, respectively\n\\begin{equation}\n\\sigma_{t}^{2}=\\int(t-<t>)^{2}|s(t)|^{2} d t, ~~\n\\sigma_{\\omega}^{2}=\\int(\\omega-<\\omega>)^{2}|S(\\omega)|^{2} d \\omega.\n\\end{equation}\n% \\begin{equation}\n% \\begin{aligned}\n% \\sigma_{t}^{2}&=\\int(t-<t>)^{2}|s(t)|^{2} d t, \\\\\n% \\sigma_{\\omega}^{2}&=\\int(\\omega-<\\omega>)^{2}|S(\\omega)|^{2} d \\omega.\n% \\end{aligned}\n% \\end{equation}\nThen we have the following result based on Schwarz inequality\n\\begin{equation}\n\\begin{aligned}\n\\sigma_{t}^{2} \\sigma_{\\omega}^{2} &=\\int|t s(t)|^{2} d t \\times \\int\\left|s^{\\prime}(t)\\right|^{2} d t  \\\\\n& \\geq\\left|\\int t s^{\\star}(t) s^{\\prime}(t) d t\\right|^{2}=\\left|-\\frac{1}{2}+j \\operatorname{Cov}_{t \\omega}\\right|^{2}=\\frac{1}{4}+\\operatorname{Cov}_{t \\omega}^{2}.\n\\end{aligned}\n\\end{equation}\n% Hence \n% $$\n% \\sigma_{t}^{2} \\sigma_{\\omega}^{2} \\geq\\left|\\int t s^{\\star}(t) s^{\\prime}(t) d t\\right|^{2}=\\left|-\\frac{1}{2}+j \\operatorname{Cov}_{t \\omega}\\right|^{2}=\\frac{1}{4}+\\operatorname{Cov}_{t \\omega}^{2}\n% $$\nWhen we view the uncertainty principle from time series anomaly detection, it means that when one structure has a good catch of point-wise anomaly in the time domain, it would have a less sensitive detection power in the frequency domain, and vice versa. Designing a model with a single structure that can detect the time and frequency simultaneously is difficult. A better approach could be using different structures to detect them and merge them as a whole model.  \n\n\n% \\subsection{Main Modules}\n\n% \\noindent\\textbf{Normal/Abnormal Data Augmentation in Time Domain.}\n    \n% \\noindent\\textbf{Normal/Abnormal Data Augmentation in Frequency Domain.}\n\n% \\noindent\\textbf{Decomposition.}\n\n% \\noindent\\textbf{Two Branch Neural Networks Representation.}\n\n% \\subsection{\\color{red}{Sequential Anomaly Detection}}\\label{SDexample}\n\n% \\begin{figure*}[ht]\n% \\centering  \n% \\subfigure[Point-wise Anomaly Example]{\n% \\label{PDt}\n% \\includegraphics[width=0.24\\textwidth]{PD-t.png}}\n% \\subfigure[Point-wise Anomaly Diff in Time Domain]{\n% \\label{PDdt}\n% \\includegraphics[width=0.24\\textwidth]{PD-dt.png}}\n% \\subfigure[Seasonality Anomaly Example]{\n% \\label{SDt}\n% \\includegraphics[width=0.24\\textwidth]{SD-t.png}}\n% \\subfigure[Seasonality Anomaly Diff in Time Domain]{\n% \\label{SDdt}\n% \\includegraphics[width=0.24\\textwidth]{SD-dt.png}}\n% \\caption{Examples of Point-wise and Pattern-wise Anomaly in Time Domain.}\n% \\label{exTime}\n% \\end{figure*}\n\n\n% \\begin{figure*}[ht]\n% \\centering  \n% \\subfigure[Point-wise Anomaly.]{\n% \\label{PDsub1}\n% \\includegraphics[width=0.24\\textwidth]{PD-1.png}}\n% % \\subfigure[TS with Point-wise Anomaly in Frequency Domain.]{\n% % \\label{PDsub4}\n% % \\includegraphics[width=0.3\\textwidth]{PD-4.png}}\n% % \\subfigure[Expected Frequency Domain result of Fig~\\ref{PDsub1}.]{\n% % \\label{PDsub5}\n% % \\includegraphics[width=0.3\\textwidth]{PD-5.png}}\n% \\subfigure[Frequency of TS with/without point-wise anomaly.]{\n% \\label{PDsub6}\n% \\includegraphics[width=0.24\\textwidth]{PD-6.png}}\n% \\subfigure[Seasonality Anomaly.]{\n% \\label{SDsub1}\n% \\includegraphics[width=0.24\\textwidth]{SD-1.png}}\n% % \\subfigure[TS with Seasonality Anomaly in Frequency Domain.]{\n% % \\label{SDsub4}\n% % \\includegraphics[width=0.3\\textwidth]{SD-4.png}}\n% % \\subfigure[Expected Frequency Domain result of Fig~\\ref{SDsub1}.]{\n% % \\label{SDsub5}\n% % \\includegraphics[width=0.3\\textwidth]{SD-5.png}}\n% \\subfigure[Frequency of TS with/without seasonality-wise anomaly.]{\n% \\label{SDsub6}\n% \\includegraphics[width=0.24\\textwidth]{SD-6.png}}\n% \\caption{Examples of Point-wise and Pattern-wise Anomaly in Frequency Domain.}\n% \\label{exFreq}\n% \\end{figure*}\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Understanding the Limitations of Single Domain Analysis",
                        "content": "\\label{SDexample}\n\nThis subsection will show the significant difference in point-wise anomaly detection and pattern-wise anomaly detection intuitively by two simple examples and demonstrate the limitations of detecting anomalies only in the time or frequency domain. The illustration examples are plotted in Figure~\\ref{exPoint} and Figure~\\ref{exPattern}, where\n%For simplicity, we assume the expected value of anomalies can be gained ideally, for example, by the forecasting method. \nFigure~\\ref{exPoint} shows the differences between the detection of point-wise anomaly in time domain (fig~\\ref{PDdt}) and frequency domain (fig~\\ref{PDdf}) by a global point anomaly example (fig~\\ref{PDt}), and \nFigure~\\ref{exPattern} shows the differences between the detection of pattern wise anomaly in time domain (fig~\\ref{SDdt}) and frequency domain (fig~\\ref{SDdf}) by a seasonality anomaly example (fig~\\ref{SDt}).\n\n\nGenerally, anomaly detection in time domain is done by comparing the point with its past neighbors. For simplicity, assume the time step is one between two adjacent points.\nWe show the analysis in time domain intuitively by computing the difference (noted as diff) between the original point (noted as Ori) and the point before it (noted as Past-1). \nFigure~\\ref{PDdt} and figure~\\ref{SDdt} show the different results in time domain of the point-wise anomaly example and pattern-wise anomaly example, respectively. \nIt can be seen that point anomaly is easier to be detected than seasonality anomaly in time domain analysis. Detecting seasonality anomalies only through time-domain analysis is not easy.\n\n% Figure~\\ref{exFreq} shows the differences between the detection of point wise anomaly and the detection pattern-wise anomaly in \\textbf{frequency} domain by two typical examples (fig~\\ref{PDsub1}, fig~\\ref{SDsub1}). \n\nFor anomaly detection in the frequency domain, we compare the time series results with/without anomalies after time to frequency transform (by Fourier transform).\nFigure~\\ref{PDdf} shows results of the time series in frequency domain with and without point-wise anomalies. The difference is subtle and disperses in many channels, indicating it is hard to detect point-wise anomaly only with time domain analysis.\nFigure~\\ref{SDdf} shows the results of time series in frequency domain with and without seasonality-wise anomaly. Unlike the situation in point-wise anomalies, the difference is quite evident as different numbers of peaks are shown.\nThus, seasonality anomaly is easier to detect than point anomaly in frequency domain analysis. It is impractical to detect point-wise anomalies only by frequency domain analysis.\n\n\n\n% From Fig~\\ref{PDsub1}\\textasciitilde Fig\\ref{PDsub6}, it can be seen that the global point anomaly is evident and easy to be detected in the time domain but subtle in many frequency channels and hard to be detected in the frequency domain. \n% On the contrary, the seasonality anomaly is easier to be detected in the frequency domain by fewer channels (evident drop in frequency around eight and evident rise in frequency around 30) than in the time domain, which is shown in Fig~\\ref{SDsub1}\\textasciitilde Fig~\\ref{SDsub6}. \n% More analysis, in theory, is in Section~\\ref{TheoryAna}. \n\n% \\subsection{Intuition of Model Design}\n% \\subsection{Motivation: Time-Frequency Analysis}\n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Motivation: Data Augmentation and Decomposition",
                    "content": "\nBesides the time-frequency analysis, we also consider data augmentation and decomposition to further improve the performance of time series anomaly detection. \n\n\n\n\n% The most common way is to split trends from time series. \n\n% There are different kinds of methods to make time-series decomposition, for example, moving averages, classical decomposition methods such as additive decomposition and multiplicative decomposition, X11 decomposition method~\\cite{dagum2016seasonal}, seat decomposition method~\\cite{dagum2016seasonal}, and STL decomposition method~\\cite{robert1990stl}. \n\n% Time series decomposition makes it possible to analyze trend and seasonality separately. \n\n\n\n\n",
                    "subsubsection 4.2.1": {
                        "name": "Time Series Data Augmentation",
                        "content": "\n\nThe performance of machine learning usually relies on many training data. \nHowever, in reality, labeled data is usually limited. Data augmentation~\\cite{wen2020time} contributes a lot to help mitigate these challenges.\nUnlike most existing works where augmented data should follow the original data distribution, we consider two kinds of data augmentation methods for the anomaly detection task:\nData augmentation for normal data and anomaly data.\nIn the anomaly detection task, anomalies are samples different from normal data. There are usually various kinds of anomalies. \nThus, when anomaly data is augmented, there is no need to create anomalies identical to real anomalies in the dataset, which is also impractical. Diverse augmented anomalies generally contribute to the robustness of models. \n\n\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "Time Series Decomposition",
                        "content": "\n\nGenerally speaking, time-series data often exhibit various patterns, and it is usually helpful to split a time series into main components. \nIt is a powerful technology for analyzing complex time series widely adopted in time series anomaly detection~\\cite{hochenbaum2017automatic,gao2020robusttad,zhang2021cloudrca} and forecasting~\\cite{FedFormer,xu2021autoformer,quatformer22}.\nFurthermore, with the help of time series decomposition, simple temporal convolution neural networks can bring desirable performance (this will be discussed later). It makes the model easy to be implemented and provides insights based on anomaly results of different components. \n\n\n\n\n\n"
                    }
                },
                "subsection 4.3": {
                    "name": "High Level Architecture of TFAD",
                    "content": "\nFollowing the aforementioned motivations, the high level architecture of our designed TFAD algorithm is summarized in Fig~\\ref{overview}. Our method consists of two main branches, the time-domain analysis branch and the frequency-domain analysis branch. Besides that, both normal data augmentation module and anomaly data augmentation module are designed to increase the robustness of our method. Furthermore, the time series decomposition module is adopted to better detect anomalies in different components and provide insights into the explanation of anomalies. \n\n% we proposed our TFAD algorithm: a time series decomposition anomaly detection architecture with Time-Frequency analysis. \n% The overview architecture of TFAD is shown in Fig~\\ref{overview}. \n% The following modules are contained: normal data augmentation module, anomaly data augmentation module, time series decomposition module, time domain analysis module, and frequency domain analysis module. \n\n\n\n\n\n\n\n\n\n\n\n% \\subsection{Overview Architecture of TFAD}\n\n\n\n% With frequency domain analysis added, the property of time frequency domain can be effectively combined. The robustness of TFAD is also improved. The intuition is that, with complex anomaly types, only time-domain analysis or only frequency domain analysis results in over-fitting in special anomalies. Time-domain analysis can not work well in seasonal anomalies, which leads to over-fitting in the training set and thus can not get good and reliable detection in seasonality anomalies. Frequency domain analysis can not work well in point anomalies and similar problem happens. \n\n\n \n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Network Design of TFAD",
                    "content": "\nIn this section, we provide the detailed design of the TFAD algorithm. The whole network structure of TFAD is plotted in Fig.~\\ref{network}, where each module will be elaborated in the following parts.\n\n\n\n\n% In this section, we will show the details of the modules in TFAD.\n\n% \\subsubsection{Normal data augmentation}\\label{normAug}\n",
                    "subsubsection 4.4.1": {
                        "name": "Data Augmentation Module",
                        "content": "\\label{normAug}\nAs discussed before, we consider both normal and anomaly data augmentation. \n\n\n% In normal data augmentation module, besides augmenting similar data to normal ones, we also gain \\emph{more normal} data with noise reduction methods (e.g., Robust STL, et al). \n% The anomaly data augmentation module is conducive to generating a diversity of anomalies, that is, including anomalies not entirely consistent with real anomalies in datasets. \n\n\n\n% More details can be found in Section~\\ref{normAug} and Section~\\ref{abnormAug}, respectively.\n\nFor normal data augmentation, firstly, we generate data with low noise, which is \\emph{more normal}. Robust STL~\\cite{wen2019robuststl} is a desirable option to get trend and seasonal information of time series data, and the residual, which is in some way noise, can be ignored. \n% Besides that, we also reduce noises of original data to gain more \\emph{normal} data. \nSuch \\emph{more normal} data keep the innate character of normal patterns and have larger differences with anomalies. It is easier to distinguish \\emph{more normal} data from anomalies and helps our model learn the intrinsic quality of normal data. \nTo create diverse normal data, we transfer time series to the frequency domain by Fourier transform and make small changes in both the imaginary and the real parts to gain new data. In this way, diverse data is augmented. An example is shown in Figure~\\ref{NormAug}.\n\n\n\n% \\subsubsection{Anomaly data augmentation}\\label{abnormAug}\nBesides the typical anomalies in the data set for anomaly data augmentation, other possible anomalies should also be considered for anomaly data augmentation. \nIt helps to detect new anomalies and contributes to the robustness of our method. \nSimilar results also appear in the computer vision field~\\cite{ruff2020rethinking}, which demonstrates that relatively few random outlier exposure images help to yield state-of-the-art detection performance. \nSpecifically, we consider several data augmentation methods. \nPoint scale modification in the time domain is adopted for point change anomaly, which is the most common type.\nFor context anomalies, point/short sequence exchange and a mix-up between two different time series are considered. \nFor anomalies in seasonal and some other complicated anomalies, several different data augmentation methods in the frequency domain are used to generate various sequence anomalies. \n\n"
                    },
                    "subsubsection 4.4.2": {
                        "name": "Decomposition Module",
                        "content": "\nThere are different kinds of methods to make time-series decomposition, for example, moving averages, classical decomposition methods such as additive decomposition and multiplicative decomposition, X11 decomposition method~\\cite{dagum2016seasonal}, seat decomposition method~\\cite{dagum2016seasonal}, and STL decomposition method~\\cite{robert1990stl}. \nIn this paper, Hodrick\u2013Prescott (HP) filter~\\cite{hodrick1997postwar} is adopted for time series decomposition since it is easy to implement and works well in the real world. \n\n \n% Denote time series $y_t, t=1,2, \\dots, T$ contains a trend component ${\\tau}_t$, a residual part ${\\epsilon}_t$ and a possible seasonal component $c_t$. That is, $y_t = \\tau_t + c_t + \\epsilon_t$. Then, in HP filter, the trend component can be obtained by solving the following minimization problem\nDenote time series $y_t$, $t=\\{1,2, \\dots, T\\}$ contains a trend component ${\\tau}_t$, a residual part ${\\epsilon}_t$. That is, $y_t = \\tau_t + \\epsilon_t$. Then, in HP filter, the trend component can be obtained by solving the following minimization problem\n\\begin{equation}\n{\\min}_{\\tau} \\left( \\sum_{t=1}^{T}(y_t - \\tau_t)^2 + \\lambda\\sum_{t=2}^{T-1}{[(\\tau_{t+1} - \\tau_{t}) - (\\tau_t - \\tau_{t-1})]}^2 \\right),\n\\end{equation}\nwhere the multiplier $\\lambda$ is the parameter that adjusts the sensitivity of the trend to fluctuation and can be adjusted according to the frequency of observations~\\cite{ravn2002adjusting}. After decomposition, both trend and residual components are utilized to improve performance since different anomalies may appear in different components.\n\nWe mainly consider the decomposition method for univariate time series in TFAD. \nFor multivariate time series, we decompose each time series sequence separately. \nIt may not be the best method for multivariate time series decomposition. However, significant improvement is gained, and we will leave a special design for multivariate time series as future work. \n\n\n"
                    },
                    "subsubsection 4.4.3": {
                        "name": "Window Splitting Module",
                        "content": "\n\n\n% \\subsection{Network Structure}{\\label{NetworkStruc}}\n \n In the time series anomaly detection task, temporal correlations among observations are significant, and sequence anomalies are usually harder to be detected than point anomalies. \n Furthermore, even point anomaly is hard to be detected without temporal correlations. \n Therefore, we adopt time series window to better gain sequence-wise/temporal correlation information~\\cite{carmona2021neural, tuli2022tranad}. Specifically, a full time series window and a context time series window are set to detect anomalies in a suspect sequence, where the full window consists of a context window and a suspect window, as illustrated in Figure~\\ref{window}. The assumption is that suppose the context window is normal. If the pattern of the full window is consistent with the pattern of the context window, then there is no anomaly in the suspect window. \n If there is an anomaly in the suspect window, the pattern of the full window will not be consistent with the context window. With sliding windows, the label of each time point can be known and more details are in Section~\\ref{AS}.\n\n \n\n\n \n\n"
                    },
                    "subsubsection 4.4.4": {
                        "name": "Time and Frequency Branches",
                        "content": "\\label{FFT}\n In this section, the time and frequency branches will be discussed. \n As shown in Fig~\\ref{network}, the original time series will be first decomposed into trend and residual components. \n For each component, we set the full window sequence and context window sequence with the aforementioned window splitting. After that, time-domain representation learning and frequency-domain representation learning for each window sequence will be done to gain rich information on sequences. After that, the distance between the context window and the full window would be measured to calculate the anomaly score.\n\nFor the representation, most of the classical distances, such as Cosine distance, dynamic time warping (DTW) distance, are too susceptible to the length of time series to be used here. \nInstead, neural networks are widely used to gain the representation of complex samples in many tasks due to the power of representation ability. Thus, we utilize a neural representation network to overcome the above shortcoming of classical methods. \nSpecifically, the temporal convolutional network (TCN)~\\cite{bai2018empirical} is a simple and powerful architecture. Therefore, we adopt the TCN design as our representation network. \n\nTFAD first transforms the time series from time-domain to frequency-domain for the frequency branch by discrete Fourier transform (DFT).\nFor time series $\\{x_n\\}: = x_0, x_1, \\dots, x_{N-1}$, its discrete Fourier transform $\\{X_k\\}:=X_0, X_1, \\dots, X_{N-1}$ is defined as\n\\begin{equation}\n    X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-\\frac{i2\\pi}{N}kn} \\\\\n= \\sum_{n=0}^{N-1}x_n\\cdot\\left[ cos(\\frac{2\\pi}{N}kn) - i\\cdot sin(\\frac{2\\pi}{N}kn)\\right].\n\\end{equation}\nThe results of DFT contain real-part (denoted as $Re$) and imaginary-part (denoted as $Im$). To better get the information of time series in different locations, TFAD intersects $Re$ and $Im$ and gets $\\{X_k\\} = F_{\\{x_n\\}} = \\{Re_1, Im_1, Re_2, Im_2, \\dots, Re_n, Im_n\\}$.\nAfter obtaining DFT results, representation learning is done by TCN similarly to the time branch. \n \nThere are different transform methods for time-frequency translation besides DFT, such as continuous Fourier Transform (CFT), short-time Fourier Transform (STFT), and wavelet transform. DFT is more suitable for our situation:\nthe time-series data is usually discrete.\nAs the length of the anomaly sequence is unknown, the window length of STFT is hard to set.\nWavelet transform is considered to contain time and frequency information simultaneously, but evaluation shows that it does not gain a good performance as DFT.\nThe main reason may be that, with the time-domain branch added already, a DFT with frequency information can gain more marginal utility than a wavelet.\n\n%  Time series anomalies appear in point level or pattern level. \n%  TFAD designs corresponding modules for different anomalies. \n%  More specifically, for complex pattern anomalies, which harder to be detected efficiently compared with point anomalies, time series decomposition is used to protrude anomalies in trend, and frequency domain analysis is applied to detect anomalies in seasonal and some other intractable patterns. \n \n \n \n%  \\subsubsection{Basic NN Model for representation learning}\n\n \n \n  \n\n%  In section~\\ref{FFT}, we show how to prepare data for frequency domain analysis by transforming time series from the time domain to the frequency domain by discrete Fourier transform. \n \n\n \n \n \n \n %%%---> remove this part in the Experiments\n%  Actually, we also tried the popular representation learning model for time series analysis: Transformer~\\cite{vaswani2017attention}, which is widely used in time series forecasting~\\cite{xu2021autoformer, zhou2021informer}. Besides the high memory usage, the F1 score on KPI is much less than TCN; that is, with TCN, we gain an F1 score of 0.75, but with transformer encoder, only 0.5999 is gained. \n%  Main reason is that the transformer encoder takes use of the location information of time series by positional encoding and works well with long time series.\n%  However, with time window splitting, the only difference between the full window and context window is the suspect window, and the most significant need is to gain representation of the whole sequence of full window and context window. \n%  Hierarchical convolution network models the local information between near points better. It is very important in the time series anomaly detection task.\n%  In our setting, TCN is a good choice not only in respect of implementation but in performance. \n \n\n \n"
                    },
                    "subsubsection 4.4.5": {
                        "name": "Anomaly Score Module",
                        "content": "\\label{AS}\n  \n Comparison between full window sequence and context window sequence is used to set anomaly score, which is a widely used metric for the \\emph{degree of anomalousness}~\\cite{ruff2021unifying}. Here, the cosine similarity between full window sequence and context window sequence is set as anomaly score. Specifically, denote anomaly score as $AS$, then \n \\begin{equation}\nAS = \\mathcal{F} \\{dis(RV_{treT}, RV_{resT}, RV_{treF}, RV_{resF})\\},\n \\end{equation}\nwhere $RV_{treT}$, $RV_{resT}$ are representation results of trend component and residual component in \\emph{time} domain respectively, $RV_{treF}, RV_{resF}$ are representation results of trend component and residual component in \\emph{frequency} domain respectively, and the distance function $dis()$ is the cosine similarity. \n The higher the anomaly score is, the higher the dissimilarity is, which means the suspect window is more likely to be abnormal. We can decide whether the suspect window is abnormal with a threshold for anomaly score.\n Thus, suspect windows can be labeled with an anomaly or not. \n However, we also want to know what label should be set for every time point. \n Therefore, a voting strategy is adopted. With sliding full and context windows, the suspect window is also sliding. Every point belongs to several suspect windows, and if more than half of them are labeled as an anomaly, the point will be set as an anomaly.  \n\n \n%  For each window, with time series decomposition, trend and seasonality parts are gained. \n%  Then representation tensors in the time domain and frequency domain are produced, respectively. Representation tensors from different parts but the same domain are combined together. \n%  Finally, the distances of full window and context window in time domain and frequency domain are computed, respectively. Anomaly score is designed upon such distances. \n \n\n\n\n% \\subsection{\\color{red}{Theoretical Analysis}}\\label{TheoryAna}\n\n% In this subsection, we show that usually the longer the anomaly sequence, the easier to be detected in frequency domain. Note that, the point anomaly can be considered as an anomaly sequence of length 1. For simplicity, we set the time series as $$\n% X(t)=\\left\\{\n% \\begin{array}{rcl}\n% s(t), & t_0\\leq t\\leq t_p, \\quad t_q\\leq t\\leq t_N \\\\\n% a(t), & t_p \\leq t \\leq t_q\n% \\end{array}\n% \\right.$$ where $t\\in \\{t_0, t_1, \\dots, t_N\\}$, $s(t)$ is the expected normal function and $a(t)$ is the function of anomaly.\n% Without loss of generality, set $a(t) = s(t) + \\delta(t), t_p\\leq t \\leq t_q$ where $\\delta(t)$ is the deviation from expected value. \n\n% The above formulation is defined in the discrete-time domain. \n% It is trivial to see that, the shorted the anomaly sequence, the shorter $\\delta(t)$ and the anomaly can be detected by deviation on fewer points. \n% Thus, point-wise anomaly are usually easy to handle in time-domain analysis. \n\n% \\subsubsection{\\color{red}{DFT}}\n\n% In frequency domain analysis, however, it is quite different. For time series $X = \\{x(n)\\}_{0 \\leq n \\leq N}$, discrete Fourier transform of point $k$ is $$\\mathcal{F}(x(k)) =  \\sum_{n=0}^{N-1} e^{-i\\frac{2\\pi}{N}nk}x(n), k=0,1,2, \\dots, N-1.$$\n\n% Note point anomaly $x(p)$ with expected value as $\\hat{x(p)}$ where $p\\in \\{t_0, t_1, \\dots, t_N\\}$. The deviation in discrete frequency domain is, $$\\delta(x(p)) = \\{F_k\\} = e^{-i\\frac{2\\pi}{N}pk}(x(p)-\\hat{x(p)}), k=0,1,2, \\dots, N-1.$$ That is, in every $F_k$ there is a small deviation. \n\n% % The intuition is that, \u57fa\u51fd\u6570\u6b63\u4ea4\n\n% {\\color{blue}{Note seasonal anomaly}}\n\n% \\subsubsection{\\color{red}{CFT}}\n\n% Firstly, similar with Section~\\ref{SDexample}, we show two typical examples. \n% \\noindent\\textbf{Point Anomaly.} \n% $$\n% X(t)=\\left\\{\n% \\begin{array}{rcl}\n% s(t), & t_0\\leq t < t_1, \\quad t_1 < t\\leq t_n \\\\\n% a(t), & t = t_1 \n% \\end{array}\\\\\n% \\right.$$\n\n% Set \n% $$\n% \\delta (t)=\\left\\{\n% \\begin{array}{rcl}\n% 1 , & t=0 \\\\\n% 0, & t \\neq 0 \n% \\end{array}\\\\\n% \\right.$$\n\n% Then, \n% \\begin{eqnarray*}    \n% X(t) &=& s(t)[1-\\delta(t-t_1)] + a(t)\\delta(t-t_1)  \\nonumber    \\\\\n% ~ &=& s(t) + [a(t)-s(t)]\\delta(t - t_1). \\nonumber\n% \\end{eqnarray*}\n\n% Note Fourier transform function as $\\mathcal{F}$ and transform result as $F(\\omega)$, then for point anomaly, \n% \\begin{eqnarray}\\label{FFTP}\n% F(\\omega) &=& \\mathcal{F}[(a(t)-s(t))\\delta(t-t_1)]  \\nonumber  \\\\\n% ~ &=& \\int_{t_0}^{t_n} (a(t)-s(t))\\delta(t-t_1) e^{-j\\omega t} dt \\nonumber \\\\\n% \\end{eqnarray}\n\n% % Equation~\\ref{FFTP} \n\n"
                    }
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\\label{experiments}\n\n% In this section, we evaluate the performance of our method on commonly used benchmark data sets. To show the advantage of TFAD more directly, we evaluate our algorithm mainly on univariate time-series datasets. It is intuitive to get the trend and cycle part from univariate TS. We leave the application in multivariates as future work. \n\n\nThis section studies the proposed TFAD model empirically compared to other state-of-the-art time series anomaly detection algorithms on both univariate and multivariate time series benchmark datasets. We also investigate how each component in TFAD contributes to the final accurate detection by ablated studies and discuss the insights. \n\n\n",
                "subsection 5.1": {
                    "name": "Baselines, Datasets, Metrics, and Evaluation",
                    "content": "\n",
                    "subsubsection 5.1.1": {
                        "name": "Baselines.",
                        "content": " For univariate time series anomaly detection, we compare our method with the state-of-the-art algorithms, including SPOT, DSPOT~\\cite{siffer2017anomaly}, DONUT~\\cite{xu2018unsupervised}, SR, SR-DNN, SR-CNN~\\cite{ren2019time}, and NCAD~\\cite{carmona2021neural}. For multivariate time series anomaly detection, we compare recent deep neural network models like\n% Many deep neural network models have recently emerged for multivariate time series anomaly detection. Most of them, such as, \nAnoGAN~\\cite{schlegl2017unsupervised}, DeepSVDD~\\cite{ruff2018deep}, DAGMM~\\cite{zong2018deep}, LSTM-VAE~\\cite{park2018multimodal}, MSCRED~\\cite{zhang2019deep}, OmniAnomaly~\\cite{su2019robust}, MTAD-GAT~\\cite{zhao2020multivariate}, THOC~\\cite{shen2020timeseries}. Note that some baselines above are not designed for temporal data, but they are extended for time series data with fixed lengths by sliding windows.\n\n% but with , the time series inputs with fixed lengths can be considered as general examples. \n\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Datasets.",
                        "content": " We adopt the widely-adopted univariate and multivariate datasets for time series anomaly detection as follows: \n\\begin{itemize}\n\\item KPI~\\cite{kpidata} is a univariate time series dataset released in the AIOPS anomaly detection competition. It contains dozens of KPI curves with labeled anomaly points. The points are collected every 1 minute or 5 minutes from Internet Companies, for instance, Sogou, Tencent, eBay, etc. \n\n\\item Yahoo~\\cite{yahoolink} is a univariate time series dataset for time series anomaly detection released by Yahoo research. Part of the dataset is synthetic, where the anomalies are algorithmically generated. Part of it is real traffic data to Yahoo services, where the anomalies are labeled manually by editors. \n    \n\\item SMAP and MSL~\\cite{hundman2018detecting} are two multivariate time series datasets published by NASA. SMAP and MSL have 55 and 27 unique telemetry channels, respectively, that is, 55 and 27 dimensions time series. More specifically, anomaly sequences in SMAP are composed of $62\\%$ point anomalies and $38\\%$ contextual anomalies, while MSL consists of $53\\%$ point anomalies and $47\\%$ contextual anomalies. \n\n\\end{itemize}\n   \nThe summary of these datasets is shown in Table~\\ref{tab:summary}.\n\n\n\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Metrics.",
                        "content": " Point adjusted F1 score~\\cite{shen2020timeseries, audibert2020usad, su2019robust} is the widely used metric in the time series anomaly detection task. In this metric, if one point is detected as an anomaly in a segment, the whole anomaly segment will be considered as detected. This metric fits well with real-world situations as, in most cases, the anomaly event affects more than one time point. For such an abnormal event, a single anomaly alarm is enough. Note that several other F1-type metrics~\\cite{kim2021towards, garg2021evaluation, jacob2020exathlon, hwang2022you} have been proposed to provide a more precise evaluation of abnormal event detection. Either the first alarm of group anomalies is set with higher importance, or the proportion of alarms is evaluated. However, in this paper, our primary aim is not to discuss which metric is the best as different metrics are applied in different situations. Thus, we adopt the widely used point-adjusted F1 score as our metric and leave evaluations on more different metrics for future work.\n% and precision will also be shown to evaluate our design in special cases. \n\n"
                    },
                    "subsubsection 5.1.4": {
                        "name": "Evaluation Details.",
                        "content": " We follow the common setting as in~\\cite{ren2019time, carmona2021neural} for better comparisons. Specifically, we split each dataset into the train part, validation part, and test part to choose models. For the Yahoo dataset, we split 50\\% as test data, 30\\% as train data, and 20\\% as validation data.\nThe original KPI dataset contains train and test data, and we set 30\\% of the training data as validation data. \nFor the evaluation of the KPI dataset, we apply both supervised setting (sup.) where all labeled data are utilized and unsupervised setting (un.) where the label information is not utilized.\nEvery setting has been run ten times, and the mean and variance are reported. \n\n\n\n\n% \\begin {table}[t]\n% \\caption {F1 score of anomaly detection on \\textit{univariate} time series datasets. The best results are highlighted.} \\label{tab:unires} \n% \\begin{center}\n% \\begin{tabular}{c|c|c|c}\n% \\hline\n% % Model & Yahoo (semi-sup) & KPI (semi-sup)  & KPI (sup) \\\\\n% Model & Yahoo (un.) & KPI (un.)  & KPI (sup.) \\\\\n% \\hline\n% SPOT & 33.8 & 21.7 & -- \\\\\n% DSPOT & 31.6 & 52.1 & -- \\\\\n% DONUT & 2.6 & 34.7 & -- \\\\\n% SR  & 56.3 & 62.2 & -- \\\\\n% SR-CNN & 65.2 & 77.1 & -- \\\\\n% SR-DNN & -- & -- & 81.1 \\\\\n% % NCAD & $80.531\\pm 1.42$ & $76.64\\pm 0.89$ & $79.2\\pm 0.92$ \\\\\n% NCAD & $\\textbf{81.16}\\pm 1.43$ & $76.64\\pm 0.89$ & $79.20\\pm 0.92$ \\\\\n% \\hline\n% % TFAD & $\\textbf{81.127}\\pm \\textbf{0.52}$ & $\\textbf{79.8}\\pm \\textbf{0.74}$ & $\\textbf{82.1}\\pm \\textbf{0.42}$ \\\\\n% \\textbf{TFAD} & ${81.13}\\pm \\textbf{0.52}$ & $\\textbf{79.80}\\pm \\textbf{0.74}$ & $\\textbf{82.10}\\pm \\textbf{0.42}$ \\\\\n% \\hline\n% \\end{tabular}\n% \\end{center}\n% \\end{table}\n\n% \\begin{table}[t]\n% \\caption {F1 score of anomaly detection on \\textit{multivariate} time series datasets. The best results are highlighted. } \\label{tab:mulres} \n% \\begin{center}\n% \\begin{tabular}{c|c|c}\n% \\hline\n% Model & SMAP (un.) & MSL (un.)  \\\\\n% \\hline\n% AnoGAN & 74.59 & 86.39  \\\\\n% DeepSVDD & 71.71 & 88.12  \\\\\n% DAGMM & 82.04 & 86.08  \\\\\n% LSTM-VAE  & 75.73 & 73.79  \\\\\n% MSCRED & 77.45 & 85.97  \\\\\n% OmniAnomaly & 84.34 & 89.89  \\\\\n% MTAD-GAT & 90.13 & 90.84 \\\\\n% THOC & 95.18 & 93.67 \\\\\n% NCAD & $94.45\\pm \\textbf{0.68}$ & $95.60\\pm 0.59$ \\\\\n% \\hline\n% % TFAD & $\\textbf{96.324}\\pm 1.57$ & $\\textbf{96.409}\\pm \\textbf{0.34}$ \\\\\n% \\textbf{TFAD} & $\\textbf{96.32}\\pm 1.57$ & $\\textbf{96.41}\\pm \\textbf{0.34}$ \\\\\n% \\hline\n% \\end{tabular}\n% \\end{center}\n% \\end{table}\n\n\n\n% % FreqBran & ${13.81}\\pm {3.08}$ & ${35.97}\\pm {4.40}$ & ${19.59}\\pm {2.81}$ \\\\\n% \\begin {table*}[t] %\u8de8\u680f\u52a0*\n% % \\caption {Results of ablation study on KPI data.} \\label{tab:ablation} \n% \\caption {Ablation studies of the proposed TFAD on KPI dataset. Denote the time series decomposition module as \\textbf{Dec}, the norm data augmentation module as \\textbf{NorAug}, the time domain anomaly data augmentation module as \\textbf{TimeAnAug}, the frequency domain anomaly data augmentation module as \\textbf{FreqAnAug}, and the frequency domain analysis module as \\textbf{FreqBran}. The proposed TFAD algorithm combines all these modules. The best results are highlighted.} \\label{tab:ablation} \n% \\begin{center}\n% \\begin{tabular}{c|c|c|c|c|c|c|c|c|c}\n% \\hline\n% case & TCN & Dec & NorAug & TimeAnAug & FreqAnAug & FreqBran & Precision & Recall &  F1 score\\\\\n% \\hline\n% Freq Branch &  & & & & & $\\checkmark$ & ${13.81}\\pm {3.08}$ & ${35.97}\\pm {4.40}$ & ${19.59}\\pm {2.81}$\\\\\n% Time Branch & $\\checkmark$ & & & & & & $44.888\\pm0.095$ & $57.227\\pm0.0381$ & $50.312\\pm 0.0569$\\\\\n% (a) & $\\checkmark$ & $\\checkmark$ & & & & &$57.557\\pm5.374$ & $81.111\\pm5.339$& $66.968\\pm2.58$\\\\\n% (b) & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & & & & $57.869\\pm4.65$ & $80.49\\pm 4.85$ & $67.099\\pm 3.075$\\\\\n% (c) & $\\checkmark$ & $\\checkmark$ & & $\\checkmark$ & & & $62.569\\pm7.059$ & $\\textbf{89.45}\\pm7.84$& $72.942\\pm 2.644$\\\\\n% (d) & $\\checkmark$& $\\checkmark$ & $\\checkmark$ &$\\checkmark$ & & & $68.528\\pm 9.412$ & $85.949\\pm 11.3698$ & $74.934\\pm 2.908$\\\\\n% (e) & $\\checkmark$& $\\checkmark$ & $\\checkmark$ &$\\checkmark$ & $\\checkmark$ & & $69.444\\pm 7.133$ & $86.638\\pm 8.044$ & $76.385\\pm 2.387$\\\\\n% \\textbf{TFAD} & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & $\\textbf{79.176}\\pm 1.875$ & $85.231\\pm 1.464$ & $\\textbf{82.058}\\pm 0.4199$\\\\\n% \\hline\n% \\end{tabular}\n% \\end{center}\n% \\end{table*}\n\n\n\n   \n"
                    }
                },
                "subsection 5.2": {
                    "name": "Performance Comparisons",
                    "content": "\nThe performance comparisons of different baseline algorithms and our TFAD are summarized in Table~\\ref{tab:unires} and Table~\\ref{tab:mulres} for univariate and multivariate time series anomaly detection tasks, respectively.\n\nFor the univariate time series anomaly detection in Table~\\ref{tab:unires}, it can be seen that deep learning methods usually bring better performance than the conventional methods like SPOT and DSPOT, due to their strong representation abilities.  \nNote that both NCAD~\\cite{carmona2021neural} and our TFAD introduce data augmentation, and the randomness of augmented data brings in fluctuation in the F1 score. The variance of TFAD is significantly lower than NCAD in most cases, which indicates our TFAD method would be more robust and stable in practical systems. \nIn summary, our TFAD algorithm produces comparable performance to the best NCAD algorithm on the Yahoo dataset, and significantly outperforms all other competing algorithms on the KPI dataset. \n\n\n\n\n\n\n\n\n\n% FreqBran & ${13.81}\\pm {3.08}$ & ${35.97}\\pm {4.40}$ & ${19.59}\\pm {2.81}$ \\\\\n\n\n\nFor the multivariate time series anomaly detection in Table~\\ref{tab:mulres}, we only compare TFAD with recent deep neural network models, since the conventional non-deep methods exhibit worse performance due to the limited ability for modeling the complex interaction and nonliterary of multivariate time series data. \nNote that all algorithms adopt unsupervised setting since none of multivariate datasets provides labels in the training data. \nIt is interesting to find that our method even outperforms the state-of-the-art algorithms THOC~\\cite{shen2020timeseries} and NCAD~\\cite{carmona2021neural} by a reasonable margin. This is mainly due to our novel architecture with both time and frequency branches, while existing works detect anomalies only in the time domain.\nIn summary, our TFAD algorithm achieves the best F1 score among all competing algorithms in both SMAP and MSL datasets for multivariate time series anomaly detection.\n \n\n% Most existing works detect anomalies only in the time domain. \n% Although with elaborate deep neural network design and the fact that most of the anomalies in existing public data sets are point-wise, OmniAnomaly, THOC, and so on gain good performances, few designs of sequence anomaly detection limit the scalability of the models. \n\n% % outperforms other methods in univariate and multivariate time series anomaly detection tasks. \n% As shown in Fig~\\ref{network}, the anomaly score is computed. We can use such a score either in a supervised learning setting or in an unsupervised learning setting. \n% For the KPI data set, we considered both settings with/without giving the true data labels. \n% Note that, for unsupervised settings, although no true labels are used when training, labels are preliminary information for some data augmentation methods. \n% So we call this a semi-supervised method. \n\n% Actually, for multivariate time series, while we still use Hodrick-Prescott decomposition, which is designed for univariate time series, outstanding performances are gained. \n% A unique design for multivariate time series decomposition is future work.\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Ablation Studies",
                    "content": "\n\nTo better understand how each component in TFAD contributes to the final accurate anomaly detection, we conduct ablation studies in the KPI dataset under supervised setting, and the results are summarized in Table~\\ref{tab:ablation}. \n\nFirstly, when only time branch (base TCN model) or frequency branch (DFT followed by TCN) is adopted, it performs not well.\nSecondly, with the decomposition module added in the time branch, the F1 score gains nearly 30\\% improvement compared with the same base TCN model, which demonstrates the benefits of decomposition in TFAD model.\nThirdly, with the extra normal data augmentation module added where the ratio of augmentation data is set as 0.5, additional marginal improvement can be achieved. Similar improvements can be obtained with the time-domain abnormal data augmentation module added where the ratio of augmentation data is set as 0.4.\nAn interesting phenomenon is that, when both NormAug and TimeAnAug modules are added in the previous version, the improvement of the F1 score is more than the sum of improvement when they are added respectively. \nIt can be explained that such two directions of data augmentation make the distance between normality and anomaly larger with a nearly multiplicative effect.\nNote that the ratios of augmentation data may not be the best hyperparameters, but their performance improvements are still obvious. \nLastly, after the frequency branch are added (corresponding to the full TFAD model), not only the F1 score improves, but the variance decreases. The reason is that, only with the time-domain branch without frequency branch, some sequence-wise anomalies are hard to be detected, and overfitting usually appears in the train set, which affects the performance in the test set. These ablation studies demonstrate the effectiveness of our TFAD design with time-frequency branches, data augmentation, and decomposition.\n\n% The reverse (with only the frequency-domain branch) is also true. \n\n% We show the results with only the frequency domain analysis branch for the KPI dataset in table~\\ref{tab:FreBran}.\n% \\begin {table}\n% \\caption {Results for KPI with only frequency domain analysis branch.} \\label{tab:FreBran} \n% \\begin{center}\n% \\begin{tabular}{c|c|c|c}\n% \\hline\n% Branch  & KPI (Precision)  & KPI (Recall) & KPI (F1 score)\\\\\n% \\hline\n% FreqBran & ${13.81}\\pm {3.08}$ & ${35.97}\\pm {4.40}$ & ${19.59}\\pm {2.81}$ \\\\\n% \\hline\n% \\end{tabular}\n% \\end{center}\n% \\end{table}\n\n\n\n% by ablated studies and discuss the insights. \n\n\n% To better understand the design of TFAD, we use an ablation study in KPI supervised setting to show the contribution of each module. The results are shown in Table~\\ref{tab:ablation}. \n% and all the hyperparameters keep the same in different settings. \n%We use an ablation study to show the contribution of each module.\n%all the hyperparameters keep the same except for the ratio of data augmentation. \n\n% We use the same hyperparameters under the following settings. The base model of TFAD is a classical temporal convolutional network (noted as TCN).  \n% Denote the time series decomposition (HP filter) module as \\textbf{Dec}, the norm data augmentation module as \\textbf{NorAug}, the time domain anomaly data augmentation module as \\textbf{TimeAnAug}, and the frequency domain anomaly data augmentation module as \\textbf{FreqAnAug}. Moreover, the frequency branch, i.e., the frequency domain analysis module, is noted as \\textbf{FreqBran}. \n% TFAD combines all the above modules.\n\n\n\n\n\n% with kpi data:\n% only time TCN;\\\\\n% time TCN + decomp;\\\\\n% time TCN (+ decomp) + norm data augment;\\\\\n% time TCN (+ decomp) + abnorm data augment;\\\\\n% time TCN + decomp + time domain data aug;\\\\\n% time TCN + decomp + freq domain data aug;\\\\\n% time TCN + decomp + freq branch; and so on \\\\\n\n% \\subsection{\\color{red}{Hyperparameters Analysis.}} \n% % In data augmentation module, the ratio influences a lot on the performance. \n% window size may be a main hyperparameter. \n\n% % \u5f02\u5e38\u6570\u636e\u6ce8\u5165\u7684\u6536\u76ca\u66f4\u9ad8\uff0c\u6240\u4ee5\u9009\u62e9\u9891\u57df\u4e3b\u8981\u5bf9\u5f02\u5e38\u6570\u636e\u6ce8\u5165-- \u56e0\u4e3a\u5f02\u5e38\u6570\u636e\u8981\u5404\u79cd\u5404\u6837\u7684\uff0c\u4f46\u662f\u6b63\u5e38\u6570\u636e\u9700\u8981\u8ddf\u539f\u5e8f\u5217\u4fdd\u6301\u6bd4\u8f83\u9ad8\u7684\u4e00\u81f4\u6027\u3002\n\n% % ratio of norm data aug?\n% % ratio of abnormal data aug?\n% % window size? -- two large or too small\n\n"
                },
                "subsection 5.4": {
                    "name": "Model Analysis and Discussion",
                    "content": " \n\n% In this section, we analyze the performance of our model in theory and by visualization.\n\n% % \\noindent\\textbf{Theory}\n\nIn this section, we provide visualizations and case studies to explain how the model works intuitively and obtain insights.\n\n",
                    "subsubsection 5.4.1": {
                        "name": "Contribution of time series decomposition module",
                        "content": "\n\n\n\nFigure~\\ref{yahooD} shows an example in the Yahoo data set to help understand how time series decomposition contributes to anomaly detection. Figure~\\ref{yahoowoD} is the original time series without time series decomposition. Figure~\\ref{yahoowD} shows the results of Figure~\\ref{yahoowoD} after time series decomposition. The blue line is the residual component, the yellow line is the trend component, and the anomalies detected are labeled with red circles. Obviously, with the decomposition module, the anomalies are easier to detect. \n\nWhat is more, in our TFAD architecture as shown in Figure~\\ref{network}, representation results can be gained for trend component and residual component independently, which makes it possible to obtain an anomaly score for each component and explain in which component anomaly happens.\n\n\n\n\n"
                    },
                    "subsubsection 5.4.2": {
                        "name": "Effect of special anomaly data augmentation.",
                        "content": "\nResults in Table~\\ref{tab:ablation} show that, with data augmentation added, performance can be improved. \nThe anomaly data augmentation methods above are general and not designed for specific datasets. \nHowever, if prior information of the dataset is given, special anomaly data augmentation methods can be designed to take advantage of the pattern of anomalies for further performance improvements.\n\nTo demonstrate it, one case study is summarized in Table~\\ref{tab:slow-slop} on SMAP dataset. The observation is that the first dimension of SMAP contains slow slops when anomalies appear. By utilizing this prior information, we conduct slow-slop injection on the first dimension of SMAP datasets as special anomaly data augmentation. With this specially designed data augmentation method, it can be seen in Table~\\ref{tab:slow-slop} that significant extra performance gain is achieved in the TFAD model.\n\n\n\n \n\n\n\n% \\begin{figure}[ht] \n% \\centering \n% \\includegraphics[scale=0.2]{KPIex2.png} \n% \\caption{Example: Part of Anomalies Detected.} \n% \\label{KPIEX2} \n% \\end{figure}\n\n%todo\uff1a trend anomaly, cycle anomaly \u7b49\u627e\u4e00\u4e9byahoo\u7684\u7ed3\u679c\uff0c\u4e0d\u8fc7\u600e\u4e48\u8bf4\u660e\u8fd9\u4e2a\u6bd4\u522b\u7684\u597d\u5462\uff1f\n% \u7edf\u8ba1\u4e00\u4e0bKPI\u548cYahoo\u5404\u81ea\u7684\u6bb5\u5f02\u5e38\u7684\u6bd4\u4f8b\uff1f\n\n\n   \n"
                    },
                    "subsubsection 5.4.3": {
                        "name": "Adaptive window size",
                        "content": "\nSeveral typical detected anomalies of the proposed TFAD model are shown in Figure~\\ref{KPIEX}. In Figure~\\ref{KPIEX1}, all the anomalies are detected, including point anomaly, shapelet anomaly, and seasonal anomaly. While in Figure~\\ref{KPIEX2}, one interesting phenomenon is that part of the rightmost sub-sequence anomalies are not detected effectively. \nConsidering that the anomalies in the middle and rightmost parts are near to each other and the anomalies in the middle part are more obvious than those in the rightmost part, thus when the suspect window is tested, the anomalies in the middle part change the representation of the full window and context window significantly, which will conceal the anomalies in the suspect window in the rightmost part. \nThe adaptive window size may be a feasible solution for this problem, and we will leave it for future work. \n% The ratio and method of data augmentation also influence the final results significantly. It is in some way trifling to find the best ratios for different datasets. One promising solution is to choose them through adaptive learning. \n    \n% \u9891\u57df\u5206\u652f\u7684\u65b9\u6cd5\uff1aSTFT\u7b49\u7684\u6548\u679c\u5bf9\u6bd4\n\n%%\n\n"
                    },
                    "subsubsection 5.4.4": {
                        "name": "Representation Network",
                        "content": "\nIn the TFAD model, simple temporal convolution neural (TCN) network is adopted as a representation network. \nWe also tried the popular Transformer~\\cite{vaswani2017attention} based representation learning networks for time series anomaly detection, since they are widely investigated in recent time series analysis~\\cite{FedFormer,xu2021autoformer, zhou2021informer}. Unfortunately, besides the high memory usage of the Transformer, the F1 score is also much less than that of TCN: F1 score of 0.59 is gained with Transformer while F1 score of 0.75 is gained with TCN on KPI dataset. \nThe reason is that the Transformer usually works well with long time series.\nHowever, with window splitting for time series anomaly detection, the window length can not be set too long to distinguish between the full window and the context window. In this case, TCN can better model the local time series information in the window than Transformer networks.\nTherefore, we adopt TCN network in the TFAD model, and it not only performs quite well in anomaly detection performance but also simplifies the implementation with low memory usage.\n\n% since the decomposition in TFAD can simplify the complex time series. \n% In our setting, TCN is a good choice not only in respect of implementation but in performance. \n \n \n \n\n% the only difference between the full window and context window is the suspect window. The most significant need is to gain representation of the whole sequence of the full window and context window. \n% Hierarchical convolution network models the local information between near points better. \n% Therefore, it is essential in the time series anomaly detection task.\n\n\n"
                    }
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\\label{Conclusion}\n\nThis paper proposes a time-frequency analysis-based model (TFAD) for time series anomaly detection. \nAlthough most of the traditional and deep methods in time series anomaly detection have achieved great success, how to take advantage of the time-frequency properties of time series is not well investigated. Our design with time and frequency branches fills in this gap.\nBesides, time series decomposition is implemented to bring insights into the explainability of the proposed model as well as simplify the neural network design. Furthermore, we also adopt data augmentation to overcome the lack of labeled anomaly data. \n\nBased on these considerations, the proposed TFAD with time-frequency architecture can handle the challenges of various anomalies in time series. Although no complex neural network architecture is implemented, we gain better performance than most existing deep models in time series anomaly detection. \nExtensive empirical studies with four benchmark datasets show that our TFAD scheme obtains a state-of-the-art performance. Furthermore, ablation studies show that TFAD gains not only higher accuracy but also lower variance for time series anomaly detection in both univariate and multivariate scenarios. \n\n\n% We also discuss the explanation function of time series decomposition, the effect of specially designed anomaly injection methods, etc.\n% A specially designed decomposition module for multi-variate time series and window size chosen with adaptive learning are left as future work. \n% we design a novel TFAD architecture. As it is hard to detect a deviation in time and frequency domain with single domain analysis, our two-branch design handles the challenges of various anomalies in time series.\n% \\vfill\\pagebreak\n% \\clearpage\n\n\\bibliographystyle{ACM-Reference-Format.bst}\n\\bibliography{7_bibfile}\n\n"
            }
        },
        "tables": {
            "tab:summary": "\\begin {table}\n\\caption {Summary of adopted datasets.} \\label{tab:summary} \n\\begin{center}\n\\begin{tabular}{c|c|c|c}\n\\hline\ndata set & $\\#$Curves/Dims & $\\#$Points & $\\%$Anomaly \\\\\n\\hline\nKPI & 58 & 5922913 & 2.26 \\\\\nYahoo & 367 & 572966 & 0.68 \\\\\nSMAP & 55 & 429735 & 12.8\\\\\nMSL  & 27 & 66709 &  10.5\\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}",
            "tab:unires": "\\begin {table}[t]\n\\caption {F1 score of anomaly detection on \\textit{univariate} time series datasets. The best results are highlighted.} \\label{tab:unires} \n\\begin{center}\n\\begin{tabular}{c|c|c|c}\n\\hline\n% Model & Yahoo (semi-sup) & KPI (semi-sup)  & KPI (sup) \\\\\nModel & Yahoo (un.) & KPI (un.)  & KPI (sup.) \\\\\n\\hline\nSPOT & 33.8 & 21.7 & -- \\\\\nDSPOT & 31.6 & 52.1 & -- \\\\\nDONUT & 2.6 & 34.7 & -- \\\\\nSR  & 56.3 & 62.2 & -- \\\\\nSR-CNN & 65.2 & 77.1 & -- \\\\\nSR-DNN & -- & -- & 81.1 \\\\\n% NCAD & $80.531\\pm 1.42$ & $76.64\\pm 0.89$ & $79.2\\pm 0.92$ \\\\\nNCAD & $\\textbf{81.16}\\pm 1.43$ & $76.64\\pm 0.89$ & $79.20\\pm 0.92$ \\\\\n\\hline\n% TFAD & $\\textbf{81.127}\\pm \\textbf{0.52}$ & $\\textbf{79.8}\\pm \\textbf{0.74}$ & $\\textbf{82.1}\\pm \\textbf{0.42}$ \\\\\n\\textbf{TFAD} & ${81.13}\\pm \\textbf{0.52}$ & $\\textbf{79.80}\\pm \\textbf{0.74}$ & $\\textbf{82.10}\\pm \\textbf{0.42}$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}",
            "tab:mulres": "\\begin{table}[t]\n\\caption {F1 score of anomaly detection on \\textit{multivariate} time series datasets. The best results are highlighted. } \\label{tab:mulres} \n\\begin{center}\n\\begin{tabular}{c|c|c}\n\\hline\nModel & SMAP (un.) & MSL (un.)  \\\\\n\\hline\nAnoGAN & 74.59 & 86.39  \\\\\nDeepSVDD & 71.71 & 88.12  \\\\\nDAGMM & 82.04 & 86.08  \\\\\nLSTM-VAE  & 75.73 & 73.79  \\\\\nMSCRED & 77.45 & 85.97  \\\\\nOmniAnomaly & 84.34 & 89.89  \\\\\nMTAD-GAT & 90.13 & 90.84 \\\\\nTHOC & 95.18 & 93.67 \\\\\nNCAD & $94.45\\pm \\textbf{0.68}$ & $95.60\\pm 0.59$ \\\\\n\\hline\n% TFAD & $\\textbf{96.324}\\pm 1.57$ & $\\textbf{96.409}\\pm \\textbf{0.34}$ \\\\\n\\textbf{TFAD} & $\\textbf{96.32}\\pm 1.57$ & $\\textbf{96.41}\\pm \\textbf{0.34}$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}",
            "tab:ablation": "\\begin {table*}[t] %\u8de8\u680f\u52a0*\n% \\caption {Results of ablation study on KPI data.} \\label{tab:ablation} \n\\caption {Ablation studies of the proposed TFAD on KPI dataset. Denote the time series decomposition module as \\textbf{Dec}, the norm data augmentation module as \\textbf{NorAug}, the time domain anomaly data augmentation module as \\textbf{TimeAnAug}, the frequency domain anomaly data augmentation module as \\textbf{FreqAnAug}, and the frequency domain analysis module as \\textbf{FreqBran}. The proposed TFAD algorithm combines all these modules. The best results are highlighted.} \\label{tab:ablation} \n\\begin{center}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|c}\n\\hline\ncase & TCN & Dec & NorAug & TimeAnAug & FreqAnAug & FreqBran & Precision & Recall &  F1 score\\\\\n\\hline\nFreq Branch &  & & & & & $\\checkmark$ & ${13.81}\\pm {3.08}$ & ${35.97}\\pm {4.40}$ & ${19.59}\\pm {2.81}$\\\\\nTime Branch & $\\checkmark$ & & & & & & $44.888\\pm0.095$ & $57.227\\pm0.0381$ & $50.312\\pm 0.0569$\\\\\n(a) & $\\checkmark$ & $\\checkmark$ & & & & &$57.557\\pm5.374$ & $81.111\\pm5.339$& $66.968\\pm2.58$\\\\\n(b) & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & & & & $57.869\\pm4.65$ & $80.49\\pm 4.85$ & $67.099\\pm 3.075$\\\\\n(c) & $\\checkmark$ & $\\checkmark$ & & $\\checkmark$ & & & $62.569\\pm7.059$ & $\\textbf{89.45}\\pm7.84$& $72.942\\pm 2.644$\\\\\n(d) & $\\checkmark$& $\\checkmark$ & $\\checkmark$ &$\\checkmark$ & & & $68.528\\pm 9.412$ & $85.949\\pm 11.3698$ & $74.934\\pm 2.908$\\\\\n(e) & $\\checkmark$& $\\checkmark$ & $\\checkmark$ &$\\checkmark$ & $\\checkmark$ & & $69.444\\pm 7.133$ & $86.638\\pm 8.044$ & $76.385\\pm 2.387$\\\\\n\\textbf{TFAD} & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & $\\textbf{79.176}\\pm 1.875$ & $85.231\\pm 1.464$ & $\\textbf{82.058}\\pm 0.4199$\\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}",
            "tab:slow-slop": "\\begin {table}[h]\n\\caption {Results with slow-slop injection on first dimension of SMAP datasets as special anomaly data augmentation.} \\label{tab:slow-slop}\n\\begin{center}\n\\begin{tabular}{c|c|c|c}\n\\hline\nModel & Precision & Recall & F1 score \\\\\n\\hline\nTFAD & 91.90 & 89.32 & 90.32\\\\\nTFAD+injection & 94.04 & 98.36 & 96.09\\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}"
        },
        "figures": {
            "anomalyClass": "\\begin{figure}[t]\n\\centering  \n% \\caption{Examples of Point-wise Anomaly.}\n\\subfigure[shapelet anomaly]{\n\\label{sub2}\n\\includegraphics[width=0.15\\textwidth]{shap.png}}\n\\subfigure[seasonal anomaly]{\n\\label{sub2}\n\\includegraphics[width=0.15\\textwidth]{season.png}}\n\\subfigure[trend anomaly]{\n\\label{sub3}\n\\includegraphics[width=0.15\\textwidth]{trend.png}}\n\\subfigure[global point anomaly]{\n\\label{sub01}\n\\includegraphics[width=0.15\\textwidth]{globalP.png}}\n\\subfigure[context point anomaly]{\n\\label{sub02}\n\\includegraphics[width=0.15\\textwidth]{contP.png}}\n\\caption{Anomalies in time series.}\n\\label{anomalyClass}\n\\end{figure}",
            "exPoint": "\\begin{figure}[t]\n\\centering  \n\\subfigure[Point-wise anomaly example]{\n\\label{PDt}\n\\includegraphics[width=0.151\\textwidth]{PD-1.png}}\n\\subfigure[Diff in time domain w/o point-wise anomaly]{\n\\label{PDdt}\n\\includegraphics[width=0.151\\textwidth]{PD-2.png}}\n\\subfigure[Diff in frequency domain w/o point-wise anomaly]{\n\\label{PDdf}\n\\includegraphics[width=0.151\\textwidth]{PD-3.png}}\n\\caption{Examples of point-wise anomaly in time and frequency domain.}\n\\label{exPoint}\n\\end{figure}",
            "exPattern": "\\begin{figure}[t]\n\\centering  \n\\subfigure[Seasonality anomaly example.]{\n\\label{SDt}\n\\includegraphics[width=0.151\\textwidth]{SD-1.png}}\n\\subfigure[Diff in time domain w/o seasonality anomaly]{\n\\label{SDdt}\n\\includegraphics[width=0.151\\textwidth]{SD-2.png}}\n\\subfigure[Diff in frequency domain w/o seasonality anomaly]{\n\\label{SDdf}\n\\includegraphics[width=0.151\\textwidth]{SD-3.png}}\n\\caption{Examples of pattern-wise anomaly in time and frequency domain.}\n\\label{exPattern}\n\\end{figure}",
            "overview": "\\begin{figure}[t] \n\\centering \n\\includegraphics[scale=0.2]{overview.png} \n\\caption{High-level diagram of TFAD architecture.} \n\\label{overview} \n\\end{figure}",
            "network": "\\begin{figure*}[ht] \n\\centering \n\\includegraphics[scale=0.25]{TFAD-model-v2.png} \n\\caption{Network structure of the proposed TFAD algorithm. The shaded blocks indicate the frequency domain branch.} \n\\label{network} \n\\end{figure*}",
            "NormAug": "\\begin{figure}[t] \n\\centering \n\\includegraphics[scale=0.3]{NormAug-1.png} \n\\includegraphics[scale=0.3]{NormAug-2.png} \n\\includegraphics[scale=0.3]{NormAug-3.png} \n\\caption{Norm data augmentation by frequency domain changes.} \n\\label{NormAug} \n\\end{figure}",
            "window": "\\begin{figure}[t] \n\\centering \n\\includegraphics[scale=0.4]{window.png} \n\\caption{Window splitting.} \n\\label{window} \n\\end{figure}",
            "yahooD": "\\begin{figure}[t] \n\\centering \n\\subfigure[Original time series]{\n\\label{yahoowoD}\n\\includegraphics[width=0.23\\textwidth]{yahoo-wo-decomp.png}}\n\\subfigure[Time series with decomposition]{\n\\label{yahoowD}\n\\includegraphics[width=0.23\\textwidth]{yahoo-w-decomp.png}}\n\\caption{Examples on how decomposition helps time series anomaly detection.}\n\\label{yahooD} \n\\end{figure}",
            "KPIEX": "\\begin{figure}[!t]\n\\centering  \n\\subfigure[Typical anomalies detected]{\n\\label{KPIEX1}\n\\includegraphics[width=0.23\\textwidth]{KPIex1.png}}\n\\subfigure[Part of anomalies detected]{\n\\label{KPIEX2}\n\\includegraphics[width=0.23\\textwidth]{KPIex2.png}}\n\\caption{Detected anomaly examples of the proposed TFAD model on KPI datasets.}\n\\label{KPIEX}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\nA=\\{d\\in \\mathcal{D}\\ | p^{+}(d) \\leq \\tau \\}, \\tau>0,\n\\end{equation}",
            "eq:2": "\\begin{equation}\nX =\\sum_n \\{ A\\sin(2\\pi\\omega_n T) + B\\cos(2\\pi\\omega_n T)\\} + \\tau(T),\n\\end{equation}",
            "eq:3": "\\begin{equation}\nS(\\omega)=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} s(t) e^{-j \\omega t} d t.\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\sigma_{t}^{2}=\\int(t-<t>)^{2}|s(t)|^{2} d t, ~~\n\\sigma_{\\omega}^{2}=\\int(\\omega-<\\omega>)^{2}|S(\\omega)|^{2} d \\omega.\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\begin{aligned}\n\\sigma_{t}^{2} \\sigma_{\\omega}^{2} &=\\int|t s(t)|^{2} d t \\times \\int\\left|s^{\\prime}(t)\\right|^{2} d t  \\\\\n& \\geq\\left|\\int t s^{\\star}(t) s^{\\prime}(t) d t\\right|^{2}=\\left|-\\frac{1}{2}+j \\operatorname{Cov}_{t \\omega}\\right|^{2}=\\frac{1}{4}+\\operatorname{Cov}_{t \\omega}^{2}.\n\\end{aligned}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n{\\min}_{\\tau} \\left( \\sum_{t=1}^{T}(y_t - \\tau_t)^2 + \\lambda\\sum_{t=2}^{T-1}{[(\\tau_{t+1} - \\tau_{t}) - (\\tau_t - \\tau_{t-1})]}^2 \\right),\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-\\frac{i2\\pi}{N}kn} \\\\\n= \\sum_{n=0}^{N-1}x_n\\cdot\\left[ cos(\\frac{2\\pi}{N}kn) - i\\cdot sin(\\frac{2\\pi}{N}kn)\\right].\n\\end{equation}",
            "eq:8": "\\begin{equation}\nAS = \\mathcal{F} \\{dis(RV_{treT}, RV_{resT}, RV_{treF}, RV_{resF})\\},\n \\end{equation}"
        },
        "git_link": "https://github.com/DAMO-DI-ML/CIKM22-TFAD."
    }
}