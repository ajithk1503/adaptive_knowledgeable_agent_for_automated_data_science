{
    "meta_info": {
        "title": "Disentangled Dynamic Intrusion Detection",
        "abstract": "Network-based intrusion detection system (NIDS) monitors network traffic for\nmalicious activities, forming the frontline defense against increasing attacks\nover information infrastructures. Although promising, our quantitative analysis\nshows that existing methods perform inconsistently in declaring various\nattacks, and perform poorly in few-shot intrusion detections. We reveal that\nthe underlying cause is entangled distributions of flow features. This\nmotivates us to propose DIDS-MFL, a disentangled intrusion detection method to\nhandle various intrusion detection scenarios. DIDS-MFL involves two key\ncomponents, respectively: a double Disentanglementbased Intrusion Detection\nSystem (DIDS) and a plug-and-play Multi-scale Few-shot Learning-based (MFL)\nintrusion detection module. Specifically, the proposed DIDS first disentangles\ntraffic features by a non-parameterized optimization, automatically\ndifferentiating tens and hundreds of complex features of various attacks. Such\ndifferentiated features will be further disentangled to highlight the\nattack-specific features. Our DIDS additionally uses a novel graph diffusion\nmethod that dynamically fuses the network topology in evolving data streams.\nFurthermore, the proposed MFL involves an alternating optimization framework to\naddress the entangled representations in few-shot traffic threats with rigorous\nderivation. MFL first captures multiscale information in latent space to\ndistinguish attack-specific information and then optimizes the disentanglement\nterm to highlight the attack-specific information. Finally, MFL fuses and\nalternately solves them in an end-to-end way. Experiments show the superiority\nof our proposed DIDS-MFL. Our code is available at\nhttps://github.com/qcydm/DIDS-MFL",
        "author": "Chenyang Qiu, Guoshun Nan, Hongrui Xia, Zheng Weng, Xueting Wang, Meng Shen, Xiaofeng Tao, Jun Liu",
        "link": "http://arxiv.org/abs/2307.11079v2",
        "category": [
            "cs.CR",
            "cs.AI",
            "cs.LG",
            "cs.SI"
        ],
        "additionl_info": "V2(Existing Extension Version):A SUBMISSION TO IEEE TRANSACTION ON  PATTERN ANALYSIS AND MACHINE INTELLIGENCE(TPAMI) (Under Review) |||||  V1:Accepted and appeared in the proceedings of the KDD 2023 Research Track  (DOI:10.1145/3580305.3599238)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n\n%\\IEEEPARstart{N}{ETWORK} attacks \\cite{bhushan2018recent}, such as password cracking \\cite{kelley2012guess}, man-in-the-middle attacks (MITM) \\cite{cekerevac2017internet}, and denial of service (Dos) \\cite{prasad2014and}, refer to unauthorized attempts on various digital assets in an organization's networks, to steal data or perform malicious actions. Network attacks can also be broadly regarded as network anomalies \\cite{li2006detection}, as they may involve different characteristics from most other traffics. It was reported that $31\\%$ of companies worldwide are now being attacked at least one time a day \\cite{wueest2014targeted}, due to the growing trends of mobile online business. This urgently calls for an intelligent system that can help administrators automatically filter out these network anomalies in a huge amount of online traffic. A network-based intrusion detection system (NIDS) \\cite{kumar2016machine}, which monitors network traffic and identifies malicious activities, facilitates administrators to form the frontline defense against increasing attacks over information infrastructures (e.g., sensors and servers). Hence, NIDS is widely applied in many information systems of governments and e-commercial business sectors \\cite{zhao2009unknown}. Figure \\ref{fig:intro-illustrate} illustrates that two network attacks threaten information systems, and an NIDS builds a frontline defense against these threats.\n\n\\IEEEPARstart{U}{nauthorized} attempts like password cracking~\\cite{kelley2012guess}, man-in-the-middle attacks (MITM)~\\cite{cekerevac2017internet}, and denial-of-service (DoS)~\\cite{prasad2014and} are known as the network attacks\\cite{bhushan2018recent}, targeting an organization's digital assets with the intention of data leakage or the execution of harmful deeds. These attacks are frequently perceived as network anomalies~\\cite{li2006detection} due to their distinct characteristics that differ from standard traffic patterns. An alarming statistic~\\cite{wueest2014targeted} notes that $31\\%$ companies across the globe experience a daily average of at least one cyber attack, a frequency amplified by the proliferation of mobile online business endeavors. The situations necessitate an intelligent system deployment to assist network administrators in the automated segregation of these anomalies from the vast sea of internet traffic. A network-based intrusion detection system (NIDS)~\\cite{kumar2016machine}, which monitors network traffic and identifies malicious activities, facilitates administrators to form the frontline defense against increasing attacks over information infrastructures (e.g., sensors and servers). Hence, NIDS is widely applied in many information systems of governments and e-commercial business sectors \\cite{zhao2009unknown}. Figure \\ref{fig:intro-illustrate} demonstrates how NIDS builds a frontline defense against two network attacks, protecting systems from potential cyber threats.\n\n\n\nExisting NIDS can be categorized into two types, i.e., signature-based ones \\cite{ioulianou2018signature,erlacher2018fixids,masdari2020survey} and anomaly-based ones \\cite{van2017anomaly,aljawarneh2018anomaly,eskandari2020passban}. The former detects network attacks based on pre-defined patterns or known malicious sequences stored in a database, such as the number of bytes in traffic. These patterns in the NIDS are referred to as signatures. The latter anomaly-based NIDS learns to track attacks with machine learning techniques. Early statistical approaches~\\cite{cekerevac2017internet,prasad2014and}, such as Support Vector Machine (SVM), Logistic Regression (LR), and Decision Tree (DT), rely on carefully designed handcrafted features to learn classification boundaries. Recent deep learning-based methods \\cite{ahmad2021network,Bhatia_2021} use millions of neural parameters to mine the knowledge underlying the training samples, and have achieved great success in automatically modeling complex correlations for tens and thousands of features. The state-of-the-art E-GraphSAGE \\cite{lo2022graphsage} employs graph convolution networks (GCNs) to learn the feature representations for better prediction.\n\n% \\begin{figure*}\n%     \\centering\n%     \\subfigbottomskip=0pt    %两行子图之间的行间距\n%     \\subfigcapskip=-3pt %设置子图与子标题之间的距离\n%     \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n%     \\setlength{\\belowcaptionskip}{0pt}\n%     %\\setcounter{subfigure}{0}\n%     \\subfigure[F1-Score]{\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/bar1.pdf}\n%     }\n%     %\\setcounter{subfigure}{2}\n%      \\subfigure[Normalized feature distribution of MITM attacks]{\n%      \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/injection_distribution.pdf}\n%     }\n%     %\\setcounter{subfigure}{3}\n%     \\subfigure[Normalized feature distributions of DDoS attacks]{\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/ddos_distrubution.pdf}\n%     }\n%     %\\setcounter{subfigure}{1}\n%     \\subfigure[F1-Score]{\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/bar22.pdf}\n%     }\n%     %\\setcounter{subfigure}{4}\n%     \\subfigure[Representation correlation map of MITM attacks]{\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/injectionn.pdf}\n%     }\n%     %\\setcounter{subfigure}{5}\n%     \\subfigure[Representation correlation map of DDoS attacks] {\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/ddoss.pdf}\n%     }\n%     % \\vspace{-20pt}\n%     \\\\\n%     \\caption{Quantitative analysis on CTC-TON-IOT.\n%     (a) Comparisons of detecting various attacks, which are regarded as an unknown type in evaluation. Specifically, we train an SVM model without using the data points of these attacks, and evaluate the instances of these attacks on the test set. (b) and (c) show the feature distributions of two attacks, MITM and DDoS, respectively. (d) Comparisons of detecting various known attacks on the previous state-of-the-art deep learning model E-GraphSAGE, (e) and (f) are correlation maps of representations of the two attacks, where the representations are generated by E-GraphSAGE.     \n%     %The correlation between each dimension of injection attack representation is significant, (d) The distribution of  DDoS attack features are not overlapped, (e) The distribution of injection attack features are overlapped, (f) The correlation between each dimension of DDoS attack representation is not significant. \n%     }\n%    % \\vspace{-4mm}\n%     \\label{fig:intro-analysis}\n% \\end{figure*}\n\n% \\begin{figure*}[ht]\n%     \\centering\n\n%     % \\subfigbottomskip=0pt    %两行子图之间的行间距\n%     % \\subfigcapskip=-3pt      %设置子图与子标题之间的距离\n%     % \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n%     % \\setlength{\\belowcaptionskip}{0pt}\n%     \\hspace*{-0.8cm} % 负值可以使图像向左移动\n%     \\subfigure[F1-Score]{\n%     % \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/bar1.pdf}\n%     % }\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/P2.pdf}\n%     }\n%     \\hspace*{0.2cm} % 调整子图之间的距离\n%     \\subfigure[Normalized feature distribution of MITM attacks]{\n%     % \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/injection_distribution.pdf}\n%     % }\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/Z1.pdf}\n%     }\n%     \\hspace*{0.2cm}\n%     \\subfigure[Normalized feature distributions of DDoS attacks]{\n%     % \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/ddos_distrubution.pdf}\n%     % }\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/Z2.pdf}\n%     }\n%     \\\\\n%     \\hspace*{-0.8cm}\n%     \\subfigure[F1-Score]{\n%     % \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/bar22.pdf}\n%     % }\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/P21.pdf}\n%     }\n%     \\hspace*{0.2cm}\n%     \\subfigure[Representation correlation map of MITM attacks]{\n%     \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/introe.pdf}\n%     }\n%     \\hspace*{0.2cm}\n%     \\subfigure[Representation correlation map of DDoS attacks] {\n%     % \\includegraphics[height=2.0cm,width=0.33\\linewidth]{picture/ddoss.pdf}\n%         \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/new_f_co_heatmap2.pdf}\n%     }\n%     \\caption{Quantitative analysis on CIC-TON-IOT.\n%     (a) Comparisons of detecting various attacks, which are regarded as an unknown type in evaluation. Specifically, we train an SVM model without using the data points of these attacks, and evaluate the instances of these attacks on the test set. (b) and (c) show the feature distributions of two attacks, MITM and DDoS, respectively. (d) Comparisons of detecting various known attacks on the previous state-of-the-art deep learning model E-GraphSAGE, (e) and (f) are correlation maps of representations of the two attacks, where the representations are generated by E-GraphSAGE.     \n%     }\n%     \\label{fig:intro-analysis}\n% \\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n% \\begin{figure}\n%     \\centering\n%     \\subfigbottomskip=0pt    %两行子图之间的行间距\n%     \\subfigcapskip=0pt %设置子图与子标题之间的距离\n%     \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n%     \\setlength{\\belowcaptionskip}{0pt}\n%     %\\setcounter{subfigure}{0}\n%     \\subfigure[]{\n%     \\includegraphics[width=0.45\\linewidth,height=2.05cm]{picture/page2/pic2/summary.pdf}\n%     }\n%     \\subfigure[] {\n%     \\includegraphics[width=0.45\\linewidth]{picture/page2/pic2/3model_f1_scores_and_average(mbase bsnet).pdf}\n%     }\n%     \\caption{(a) Performance comparisons among different intrusion detection methods with normal-size traffic and few-shot traffic. (b) F1-score comparisons between DIDS few-shot learning and two SOTA few-shot learning baselines. We repeat the above comparisons 10 times.}\n%     \\vspace{-10pt}\n%     \\label{fig:few-shot performance}\n% \\end{figure}\n\n\n\nAlthough promising, existing NIDS approaches face two challenges in detecting traffic threats accurately:\n\\begin{itemize}\n    \\item \\textbf{Challenge $1$.} \\textbf{The existing NIDS methods yield {extremely} inconsistent results in identifying distinct attacks.} For statistical methods, Figure \\ref{fig:intro-analysis} (a) demonstrates that the\ndetection performance of an SVM-based method \\cite{ioannou2021network} for an unknown attack~\\footnote{{The unknown attacks referred to in this paper are never appeared new type of attacks in training set}} can be as low as $9\\%$ in terms of F1 on the CIC-ToN-IOT  \\cite{moustafa2021new} dataset. While the model achieves $40\\%$ F1 in declaring another unknown threat (DDoS) on the same benchmark. Regarding the deep learning-based methods, Figure \\ref{fig:intro-analysis} (d) demonstrates that E-GraphSAGE achieves a lower than $20$\\% F1 score for MITM attacks and a higher than $90$\\% F1 score for DDoS on the CTC-ToN-IOT dataset.\n    \\item \\textbf{Challenge $2$.} \\textbf{The existing NIDS approaches struggle to detect few-shot traffic threats accurately.} In practical scenarios, the strengthening of defensive measures and the consequent escalating trend of cyber threats have inevitably caused the rise of new-type attacks with a limited instance number, referred to as few-shot traffic threats. However, our empirical observations reveal that the NIDS methods have dramatic performance deterioration in few-shot scenarios, as shown in Fig.~\\ref{fig:few-shot performance} (a), e.g., our previous work DIDS~\\cite{qiu20233d} dramatically drops from $91.57$\\% {under supervised learning setting} to $36.12$\\% {under the few-shot learning setting} over the CIC-ToN-IoT dataset, {and similarly}, EURLER and E-GraphSAGE have an average F1-scores lower to $36.26$\\% {under the few-shot learning setting}.\n\\end{itemize}\n\nFor the first challenge, we depict feature distributions and visualize the representations to investigate the underlying cause of why existing methods perform inconsistently for various attacks, including unknown ones and known ones\\iffalse(see more details in Appendix \\ref{sec:distribution-of-all-features})\\fi. Figure \\ref{fig:intro-analysis} (b) and (c) depict statistical distributions of the two unknown attacks for the SVM-based model during testing. We observe that feature distributions of MITM attacks are entangled, while the ones of DDoS are more separated. It can be inferred that statistical distributions of traffic features are one of the main underlying causes of performance variations. Separated distributions benefit the unknown attack identification, while entangled ones are indistinguishable and unable to help the NIDS to make accurate decisions\\iffalse \\footnote{More detailed illustrations are available in Figure \\ref{fig:attack-distribution} of Appendix \\ref{sec:distribution-of-all-features}.}\\fi. We refer to such a phenomenon as \\textbf{the entangled distribution of statistical features}. To analyze the reason for performance variations of acknowledged attacks during testing, we use Pearson correlation heat map \\cite{cohen2009pearson} to visualize the representations of MITM and DDoS respectively, where the representations are generated by the encoder of E-GraphSAGE. Figure \\ref{fig:intro-analysis} (e) and Figure \\ref{fig:intro-analysis} (f) demonstrate the two correlation maps. Interestingly, we observe that the coefficients of MITM representations are much larger than those of DDoS. We further compare MITM with other attacks, including Backdoor and Dos, and find those high coefficients in the representation will lead to lower intrusion detection scores\\iffalse \\footnote{More detailed illustrations are available in Figure \\ref{cormap1} of Appendix \\ref{sec:representation}.}\\fi. We refer to such a phenomenon as \\textbf{the entangled distribution of representational features}, which can be considered as another main cause for the degradation of attack classification.\n\n% \\begin{figure}\n%     \\centering\n%     \\subfigbottomskip=0pt    %两行子图之间的行间距\n%     \\subfigcapskip=0pt %设置子图与子标题之间的距离\n%     \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n%     \\setlength{\\belowcaptionskip}{0pt}\n%     %\\setcounter{subfigure}{0}\n%     \\subfigure[The t-SNE of DIDS]{\n%     \\includegraphics[width=0.45\\linewidth]{picture/page2//pic2/3D-IDS_tsne3.pdf}\n%     }\n%     \\subfigure[The t-SNE of BSNet] {\n%     \\includegraphics[width=0.45\\linewidth]{picture/page2/pic2/similarity_tsne1.pdf}\n%     }\n%     \\subfigure[The correlation map of DIDS]{\n%     \\includegraphics[width=0.45\\linewidth]{picture/page2/pic2/3D-IDS_heatmap9.pdf}\n%     }\n%     %\\setcounter{subfigure}{2}\n%     \\subfigure[The correlation map of BSNet]{\n%     \\includegraphics[width=0.45\\linewidth]{picture/page2/pic2/similarity_heatmap8.pdf}\n%     }\n%     % \\vspace{-20pt}\n%     \\\\\n%     \\caption{The correlation map and the t-SNE visualization of representations generated by DIDS and BSNet on CIC-ToN-IoT dataset.}\n%    % \\vspace{-4mm}\n%    \\vspace{-10pt}\n%     \\label{fig:few-shot empirical}\n% \\end{figure}\n\n\n\nFor the second challenge, we observe that the NIDS approaches still perform poorly when combining the state-of-the-art few-shot learning methods, as shown in Fig.~\\ref{fig:few-shot performance} (b). For the meta-learning-based MBase~\\cite{chen2021metabaseline}, we attribute the poor performance to the limited anomaly types in network traffic. Effective meta-learning approaches typically necessitate vast types of few-shot anomalies, which are more aligned with the field of computer vision and natural language processing\\cite{li2024promptad,Belton_2023_CVPR,liao2024coftad,ma2023fewshot}. Therefore, we focus on the similarity-based method BSNet~\\cite{Li_2021} and uncover why BSNet outperforms DIDS. We depict the t-SNE visualizations of their respective learned representations. As shown in Fig.~\\ref{fig:few-shot empirical}(a) (b), we observe that the representations of BSNet in different attack types are more separated in the latent space. It can be inferred as one of the main causes of higher F1 scores in BSNet. Following this thread, we further depict their representation correlations via correlation heatmaps. We have found that the representations of BSNet are more entangled than DIDS as shown in Fig.~\\ref{fig:few-shot empirical}(c) (d). Aligning with the disentanglement studies in Challenge $1$, we attribute the main obstacle to the performance improvement of BSNet to the entangled representations.\n \n\nIn light of the discussions, we raise two critical questions:\n\\begin{itemize}\n    \\item \\textit{$Q1$: How can an intrusion detection model automatically address the first challenges, i.e., two entangled distributions, to benefit the detection of both unknown and known attacks}\n    \\item \\textit{$Q2$: Can a few-shot intrusion detection method automatically learn the representations as separated in latent space, simultaneously as disentangled among representation elements?}\n\\end{itemize}\n\nAchieving $Q1$ is challenging. To mitigate the issue of the first entangled distribution, we need to differentiate tens and thousands of features involved in real-time network traffic, without prior knowledge of the statistical distributions. \nSuch a problem is largely under-explored in the field of NIDS. For the second entangled distribution, there are some remotely related methods \\cite{liu2019bidirectional,brodbeck2020continuous,sharma2020comprehensive} in other fields, including computer vision and natural language processing. However, these approaches mainly focus on object-level representation learning, and hence they are hardly directly applied to intrusion detection to tackle this challenge. \n\nSecond, the difficulties of $Q2$ lie in two aspects: 1. tackling $Q2$ involves two optimization terms, a disentanglement term, and a latent space term. How to optimize them into an end-to-end framework is under-explored in NIDS and few-shot learning. 2. As for latent space optimization, existing similarity-based methods~\\cite{Li_2021,Xi2022FewShotLW,hu2023understanding} mainly focused on utilizing the original-scale information of representations, while ignoring their multi-scale information to help few-shot traffic separated in latent space. \n\n%existing similarity-based methods~\\cite{Li_2021,Xi2022FewShotLW,hu2023understanding} mainly focused on utilizing the original-scale information of representations, while ignoring their multi-scale information to help few-shot traffic separated in latent space. Second, end-to-end achieving the above goal involves double optimization terms, which are under-explored in the field of few-shot learning. A specialized joint optimization framework is needed to solve them.\n\nTo address the above two questions, we propose a novel model called \\textbf{D}isentangled dynamic \\textbf{I}ntrusion \\textbf{D}etection \\textbf{S}ystem with \\textbf{M}ulti-scale \\textbf{F}ew-shot \\textbf{L}earning (DIDS-MFL). Specifically, DIDS-MFL has two critical components: DIDS and MFL. The former DIDS disentangles the statistical flow features with a non-parametric optimization, aiming to automatically separate entangled distributions for representation learning. We refer to this step as statistical disentanglement. Then DIDS further learns to differentiate the representations by a regularization function, aiming to highlight the salient features for specific attacks with smaller coefficients. We refer to this step as representational disentanglement. DIDS finally introduces a novel graph diffusion module that dynamically fuses the graph topology in evolving traffic. The latter MFL is a flexible plug-and-play module for few-shot detection matching multiple NIDS baselines. We first optimize the multi-scale traffic representations in latent space to distinguish attack-specific information. Then we propose an element-wise disentanglement term to highlight the attack-specific information. Finally, we alternately solve them in an end-to-end framework.\n\nExtensive experiments on five benchmarks show the superiority of our DIDS-MFL. The main contributions are\\iffalse \\footnote{We will release the code to the research community.}\\fi:\n\\begin{itemize}\n    \\item We propose DIDS-MFL, aiming to mitigate the entangled distributions of flow features for NIDS in various practical scenarios, e.g., known, few-shot, and unknown attacks~\\footnote{{More details on the unknown attack detection ability are available in Appendix 14.1.}}. To the best of our knowledge, we are the first to quantitatively analyze such an interesting problem and empirically reveal the underlying cause in the intrusion detection field.  \n    \\item As for DIDS, we present a double disentanglement scheme for differentiating the general features of various attacks and highlighting the attack-specific features, respectively. We additionally introduce a novel graph diffusion method for dynamic feature aggregation.\n    \\item As for MFL, we propose a fusion-based framework by capturing multi-scale information and disentangling representations. We alternately solve them with rigorous derivation for few-shot intrusion detection. As a plug-and-play module, MFL also shows versatility in multiple baselines and downstream tasks.  \n    \\item Extensive quantitative and qualitative experiments on various benchmarks demonstrate the effectiveness of DIDS-MFL and provide some helpful insights for effective NIDS in various practical scenarios.\n\\end{itemize}\n%To the best of our knowledge, MFL is the first few-shot method designed for intrusion detection with representation disentanglement.\n\\iffalse\nWe answer the above question and address the two challenging problems with a novel method called DIDS (\\textbf{D}oubly \\textbf{D}isentangle \\textbf{D}ynamic IDS). Specifically, we first disentangle the statistical flow features with a non-parametric optimization based on mutual information, so that entangled distributions can be automatically separated for modeling the general features of various attacks. We refer to this step as statistical disentanglement. Then we further learn to differentiate the representations by a loss function, aiming to highlight the salient features for specific attacks with smaller coefficients. We refer to this step as representational disentanglement. We finally introduce a novel graph diffusion method that fuses the graph topology for spatial-temporal aggregation in evolving traffic. \n\\fi\n%However, due to the upward spiral trend of intrusion attacks and defense, newly emerged anomaly types commonly exist with a small number of their samples, denoted as few-shot samples. These to-be-learned new-type attacks are crucial for improving the performance of 3D-IDS in practical scenarios and also benefit the defending ability against unknown attacks, e.g. the variants of the new-type attack. Unfortunately, we empirically observed that our 3D-IDS model is incompetent in few-shot learning. To push 3D-IDS towards more practical scenarios, we further study the few-shot learning scheme on 3D-IDS.\n\n\\iffalse\nSo far we have proposed a doubly disentangled method to tackle the performance inconsistency problem in NIDS. Nevertheless, the strengthening of defensive measures and the consequent escalating trend of cyber threats have inevitably caused the rise of new-type attacks with a limited instance number, referred to as few-shot traffic. The effective identification of few-shot traffic is critical to the practical application of 3D-IDS. However, our empirical observations show that the 3D-IDS model struggles to detect few-shot anomalies accurately, with an average F1-score as low as $36.12$\\% over the CIC-ToN-IoT dataset. The poor few-shot performances are also observed on existing intrusion detection methods, e.g., EURLER, E-GraphSAGE, etc., with low F1 scores from $20.14$\\% to $52.38$\\%, as shown in Fig.~\\ref{fig:few-shot performance} (a). Given the widespread few-shot traffic in real-world scenarios, proposing a few-shot learning scheme to benefit the intrusion detection methods is necessary.\n\nThe existing few-shot learning methods can be divided into two categories: the meta-learning-based ones, and the similarity-based ones. Our empirical investigations have revealed that the state-of-the-art methods from both categories perform poorly on intrusion detection, as shown in Fig.~\\ref{fig:few-shot performance} (b). For the meta-learning-based MBase~\\cite{chen2021metabaseline}, we attribute the poor performance to the limited anomaly types in network traffic. Effective meta-learning approaches typically necessitate vast types of few-shot anomalies, which are more aligned with the field of computer vision and natural language processing\\cite{NEURIPS2022_1fe6f635,li2024promptad,Belton_2023_CVPR,liao2024coftad,ma2023fewshot}. Then we turned our attention to the similarity-based method BSNet~\\cite{Li_2021}. It outperforms the 3D-IDS and MBase and inspires us to delve into the learned representations of BSNet and 3D-IDS.\n\n\\fi\n\n%A competitive meta-learning method requires massive types of few-shot samples, more suitable for CV, NLP, etc. Employing meta-learning in traffic intrusion detection is also facing an efficiency problem, as it involves a large number of parameters to be updated. Therefore, we turned our attention to the similarity-based ones. It outperforms the 3D-IDS and meta-learning methods and inspires us to delve into the learned representations of BSNet and 3D-IDS, thus designing a few-shot learning model with high detection scores.\n\n%Achieving this goal is challenging. The mainstream two kinds of popular few-shot learning methods, the meta-learning-based ones, and the similarity-based ones are inconsistent with our 3D-IDS. The former employs a model-agnostic meta-learning framework to handle few-shot learning problems. However, a competitive meta-learning algorithm requires massive types of few-shot samples, more suitable for CV, NLP, etc. We can only collect limited types of anomaly traffic, and can hardly improve intrusion detection performance via meta-learning. The meta-learning's efficiency is another problem. It usually involves a large number of parameters to update, especially being incorporated into 3D-IDS. To this end, we turned our attention to the similarity-based ones. As shown in Fig.~x, we depict the performance comparison of the above two methods and 3D-IDS. The performance improves when simply incorporating an advanced similarity-based method into 3D-IDS, but is still unsatisfactory and unstable. To design a few-shot learning method to overcome the above drawbacks, we delve into the learned embedding between xxx and our 3D-IDS.\n\n%discover the underlying cause of why the similarity-based methods perform poorly, we delve into the learned embedding between xxx and our 3D-IDS.\n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\linewidth]{picture//page2/pic2/3model_f1_scores_and_average(mbase bsnet).pdf}\n%     \\caption{The F1 score comparison between 3D-IDS few-shot learning and two advanced baselines. We repeat 10 times.}\n%     \\label{fig:few-shot performance}\n% \\end{figure}\n\n\n% \\begin{figure}[t]\n%     \\centering\n%     % 两行子图之间的行间距\n%     \\setlength{\\subfigbottomskip}{0pt}\n%     % 图像在caption上方的距离\n%     \\setlength{\\abovecaptionskip}{8pt}\n%     % 第一张图\n%     \\begin{minipage}{0.46\\linewidth}\n%         \\centering\n%         \\raisebox{0cm}{\\includegraphics[width=\\linewidth,height=2.8cm]{picture/page2/pic2/summary.pdf}}\n%     \\end{minipage}\\hfill\n%     % 第二张图\n%     \\begin{minipage}{0.53\\linewidth}\n%         \\centering\n%         \\raisebox{0.4cm}{\\includegraphics[width=\\linewidth,height=3cm]{picture/page2/pic2/3model_f1_scores_and_average(mbase bsnet)new.pdf}}\n%     \\end{minipage}\n \n%     \\caption{The F1 score comparison between 3D-IDS few-shot learning and two advanced baselines. We repeat 10 times.}\n%     \\label{fig:few-shot performance}\n% \\end{figure}\n\n\\iffalse\nTo uncover the underlying reason of why BSNet outperforms 3D-IDS, we depict the t-SNE visualizations of their respective learned representations. As shown in Fig.~\\ref{fig:few-shot empirical}(a) (b), we observe that the representations of BSNet in different attack types are more separated in the latent space. It can be inferred as one of the main causes of higher F1 scores in BSNet. Following this thread, we further depict their representation correlations via correlation heatmaps. We have found that the representations of BSNet are more entangled than 3D-IDS as shown in Fig.~\\ref{fig:few-shot empirical}(c) (d). Aligning with the above disentanglement studies, we attribute the main obstacle to the performance improvement of BSNet to the entangled representations.\n\\fi\n\n%According to the disentanglement studies, we need to design a targeted few-shot disentanglement learning scheme.\n\n\\iffalse\nIn light of the above empirical study, we raise a crucial question: `\\textit{Can a few-shot intrusion detection method learn the representations as separated in the latent space as possible, simultaneously as disentangled among representation elements?}' Achieving this goal is challenging. First, existing similarity-based methods~\\cite{Li_2021,Xi2022FewShotLW,hu2023understanding} mainly focused on utilizing the original-scale information of representations, while ignoring their multi-scale information to help few-shot traffic separated in latent space. Second, end-to-end achieving the above goal involves double optimization terms, which are under-explored in the field of few-shot learning. A specialized joint optimization framework is needed to solve them.\n\\fi\n\n%multiple metrics information, i.e. BSNet, self-expressive technology, etc., while ignoring the learned representation quality of few-shot traffic. One feasible way is to utilize the multi-scale information of representations to separate them in latent space. Second, jointly optimizing the separated and disentangled representations in an end-to-end way is difficult, involving two different optimization terms. It challenges the existing single\n\n %To further separate the representations in latent space, enhance the connectivity of same-type attacks, and suppress the different-type ones in the latent space. \n\n%disentangled representations, simultaneously as separated in the latent space as possible?'. aim to design a few-shot learner with disentangled representations. The generated representations of our learner should be also well-separated in latent space, thus enhancing intrusion detection in few-shot scenarios. Achieving both of the above conditions is challenging. For the first point, self-expressive technology enhances the similarity learning of representations in a latent space called a coefficient matrix. However, the existing self-expressive technologies are not specially designed for few-shot samples. Second, how to incorporate the double optimizations of similarity and disentanglement in an end-to-end way is difficult.\n\n\\iffalse\nWe propose a fusion-based \\textbf{M}ulti-scale \\textbf{F}ew-shot \\textbf{L}earning (MFL) method to address the above issues in few-shot intrusion detection. To the best of our knowledge, MFL is the first few-shot method designed for intrusion detection with representation disentanglement. We first optimize the multi-scale traffic representations in latent space to distinguish attack-specific information. Then we propose an element-wise disentanglement term to highlight the attack-specific information. Finally, we alternatively solve them in an end-to-end framework.\n\\fi\n%enforce multiple transformations of representations to be similar in the latent space and introduce an element-wise disentanglement term. Then we alternatively solve it in an end-to-end way. Finally, we fuse multiple metric matrices to help distinguish few-shot traffic samples.\n\n\n%project representations into a latent space and an alternating few-shot optimization framework. We project representations into a latent space and then enforce the same-type representations after multiple transformations to be similar in the latent space. We also introduce a disentanglement regularization term in the framework. Finally, we alternatively optimize them end-to-end by calculating the numerical convergence bound.\n\n%For the first principle, we introduce self-expressiveness into our few-shot learning, aiming to fully learn the embedding information in a latent space constructed by the linear combination of the few-shot. The self-expressiveness technology can also benefit the following similarity learning of the same-class attack.\n\n%Although promising, classical self-expressive technology is also facing the few-shot learning problem. Inspired by XXX, our goal is to incorporate the multiple transformations of embedding into the self-expressive framework, thus enhancing the inherent connectivity of the traffic embedding. Different from the instance-level augmentation, our designed algorithm can be considered augmentation in the latent-space level, which can capture more subtle and significant abnormal patterns between different types of anomaly attacks, thus mitigating the instability and false positive problems.\n\n%The few-shot learning module MFL can also serve as a plug-and-play module for intrusion detection methods. \n\n\n"
            },
            "section 2": {
                "name": "RELATED WORK",
                "content": "\n",
                "subsection 2.1": {
                    "name": "Network Intrusion Detection System",
                    "content": "\n\\justifying\n\nExisting NIDS can be classified into two groups, i.e., signature-based ones \\cite{ioulianou2018signature,erlacher2018fixids,masdari2020survey} and anomaly-based ones \\cite{samrin2017review,van2017anomaly,aljawarneh2018anomaly,eskandari2020passban}. Signature-based intrusion detection systems (IDS) rely on identifying known attack patterns or signatures. These systems are adept at recognizing established threats, where the attack signatures are predefined, employing rule sets to match against network traffic for indicative malicious activities. However, their dependency on signature databases poses a limitation, as they are typically ineffective against novel or zero-day attacks that do not yet have a defined signature. In contrast, the anomaly-based ones do not rely on prior knowledge of attack signatures. They leverage machine learning or statistical techniques to model normal network behavior and flag deviations as potential threats. Anomaly detection holds the advantage of identifying unknown attacks and adapting to new threat patterns. The anomaly-based ones involve statistical methods \\cite{cekerevac2017internet,prasad2014and} and deep learning-based ones\\cite{ahmad2021network,Bhatia_2021}. Early deep learning studies model the traffic as independent sequences \\cite{ma2020deep,hu2021multiple,li2022graphDDoS,doriguzzi2020lucid}. Recent popular studies rely on GCN to aggregate traffic information \\cite{kalander2020spatio,cong2021dynamic,song2020spatial}. Most related to our work is Euler \\cite{kingeuler}, which builds a series of static graphs based on traffic flow and then performs information aggregation. Our DIDS differs from the above methods in two aspects: 1) We build dynamic graphs rather than static ones, and such a dynamic aggregation can capture fine-grained traffic features for attack detection. Furthermore, we fuse the network layer information into our graph aggregation. 2) We introduce a double disentanglement scheme, including statistical disentanglement and representational one, to benefit the detection of both known and unknown attacks.\n\n"
                },
                "subsection 2.2": {
                    "name": "Disentangled Representation Learning",
                    "content": "\nDisentanglement aims to learn representations that separate the underlying explanatory factors responsible for variation in the data. Previous studies \\cite{gilpin2018explaining,gondal2020transfer,locatello2020weakly,lipton2018mythos} focus on the generative models by employing constraints on the loss functions, such as $\\beta$-VAE \\cite{higgins2017beta} modifying the VAE framework to emphasize the independence of latent variables. Beta-VAE achieves disentanglement by introducing a trade-off term between reconstruction and KL divergence, while FactorVAE\\cite{kim2019disentangling} introduces a total correlation penalty. CausalVAE\\cite{yang2023causalvae} takes a significant step by integrating causal reasoning, allowing for interventions in the generative process. Disentangled Graph Neural Networks are another active area, where methods aim to learn independent node representations decoupled from the graph structure, e.g., GD-GAN employs a GAN framework to disentangle the latent factors of variation in graph data, allowing for controllable generation of new graph structures and node features. Some recent approaches \\cite{suter2019robustly,ridgeway2018learning,trauble2021disentangled,zhou2022link} capture the intrinsic factors in graph-structured data. Most related to our work is DisenLink \n~\\cite{zhou2022link}, which disentangled the original features into a fixed number of factors, with selective factor-wise message passing for better node representations. While our DIDS uses a double disentanglement method, which first disentangles the statistical features via non-parametric optimization, and then learns to highlight the attack-specific features with a regularization. \n\n"
                },
                "subsection 2.3": {
                    "name": "Dynamic Graph Convolution Networks",
                    "content": "\nDynamic graph convolution networks (GCNs) focus on evolving graph streams. There is a line of early studies in GCNs on dynamic graphs, which incorporate temporal information into graphs. These methods can be categorized into the spatio-temporal decoupled ones~\\cite{pareja2020evolvegcn,kalander2020spatial,cong2021dynamic} and the spatio-temporal coupled ones~\\cite{song2020spatial,gu2020implicit,chen2022optimization,chamberlain2021grand}. The former employs two separate modules to capture temporal and spatial information, e.g., DynGCN\\cite{10.1007/978-3-030-62005-9_7} performs spatial and temporal convolutions interleaved, updating model parameters to adapt to new graph snapshots, DIDA\\cite{yue2020interventional} Handles spatio-temporal distribution shifts by discovering invariant patterns and using intervention mechanisms to eliminate spurious impacts. The latter incorporates spatial-temporal dependencies by proposing a synchronous modeling mechanism, e.g., AST-GCN\\cite{wang2022progressive} adapts the graph structure and convolutional filters based on the temporal context to capture the evolving relationships in dynamic graphs, TD-GCN\\cite{8809901} uses temporal difference learning to update spatial-temporal graph convolutional filters, capturing the changes in graph structure and node states over time. Our DIDS is mainly inspired by the GIND \\cite{chen2022optimization}, which adaptively aggregates information via a non-linear diffusion method. The key difference between our GCN approach and GIND is: we introduce the non-linear graph diffusion method into a multi-layer graph that considers the network topology for dynamic intrusion detection.\n\n\n"
                },
                "subsection 2.4": {
                    "name": "Few-shot Learning",
                    "content": "\nFew-shot learning is a machine learning paradigm designed to enable a model to learn from a small number of training instances and generalize effectively to unseen data. The existing few-shot learning methods can be divided into two categories: meta-learning-based ones \\cite{sun2019metatransfer,10.1063/5.0199881,9009075}, {including meta-learning-based NIDS approaches~\\cite{9083983,9500667,10118898,Yan2024MetaLF}}, and similarity-based ones. Meta-learning-based methods focus on learning a model that can adapt to new tasks with limited data. These methods often involve a meta-training phase where the model learns to learn from a variety of tasks, followed by a meta-testing phase where the model rapidly tunes to a new task. In contrast, the similarity-based ones rely on measuring the similarity or distance between the query instances and the support set to make predictions without necessitating vast types of few-shot samples. The similarity-based ones can also be grouped into augmentation-based ones \\cite{9712302,9446583,wang2022contrastive,rong2023espt,xu2021kshot}, metric learning-based ones \\cite{Rizve_2021_CVPR,9841445,zhou2023automatic,10224275,hu2023understanding}, and other similarity-based few-shot learning methods \\cite{10.5555/3294996.3295163,Sung2017LearningTC,10.5555/3157382.3157504,oh2022understanding}. Recent popular few-shot learning methods mainly focused on utilizing multiple learning metrics, or prototype completion. The most related to us is BSNet~\\cite{Li_2021}. Ours differs from it in two aspects: first, we emphasize the importance of preserving multi-scale intrinsic representation information, rather than optimizing the original-scale information; second, we propose a joint optimization scheme to generate separated and disentangled representations to make few-shot threats distinguishable.\n\nFinally, this work is an extension version form~\\cite{qiu20233d}. In~\\cite{qiu20233d}, we address the performance inconsistency issue caused by entangled features on sufficient training data. More practical scenarios is collecting attack traffic with limited instances. It is more challenging compared to~\\cite{qiu20233d} with involving double optimization goals. To this end, this paper proposes an alternating optimization framework MFL to address the entangled representations in few-shot traffic threats with rigorous derivation. The proposed MFL is also a plug-and-play few-shot intrusion detection module that is compatible with other NIDS methods.\n % Data. This method expands and enhances the labeled small samples in our hands through prior knowledge\\cite{chen2019multi},\\cite{chen2021few}; Models, we can narrow down our unknown hypothesis range through prior knowledge\\cite{yao2020graph},\\cite{gauch2022few}; Algorithms, this method utilizes prior knowledge to help us better search for optimal results in an unknown solution space of a problem\\cite{yao2020graph},\\cite{jiang2024few}. \n \n\n\\iffalse\n"
                },
                "subsection 2.5": {
                    "name": "Self-Expressiveness Technologies",
                    "content": "\nThe concept of Self- Expressiveness was proposed to cluster data drawn from multiple low dimensional linear or affine subspaces embedded\nin a high dimensional space. Traditionally, methods have been proposed where a graph representation learning algorithm is trained on a generic unsupervised\nloss and then a clustering algorithm is applied as a postprocessing step to discover communities. Such approaches are\nsuboptimal as the node representation learning\nmodule and the clustering algorithm work independently. we use the principle of self-expressiveness on randomly\nsampled batches of nodes to generate a set of soft must-link\nand no-link constraints\\cite{bandyopadhyay2021unsupervised,10.5555/3295222.3295349,busch2020learning}.\n\\fi\n"
                }
            },
            "section 3": {
                "name": "PRELIMINARIES",
                "content": "\n",
                "subsection 3.1": {
                    "name": "Multi-Layer Graphs",
                    "content": "\n\n{To model the sophisticated traffic network topology, we formally define the multi-layer graphs.} First, we consider a single-layer network modeled by a graph  $\\mathrm{G}=(\\mathrm{V}, \\mathrm{E}, \\omega)$, where  $V$  is the set of nodes and  $E \\subset \\mathrm{V} \\times \\mathrm{V}$  is the set of edges. Here $\\omega: \\mathrm{V} \\times \\mathrm{V} \\mapsto \\mathrm{R}$  is an edge weight function such that each edge  $e_{uv} \\in \\mathrm{E}$ has a weight $ \\omega_{uv}$. \n% The adjacency matrix  $\\mathrm{A}$  is symmetric, i.e.,  $\\mathrm{A}_{(i, j)}=\\mathrm{A}_{(j, i)}$.\nThen the multi-layer graph can be defined as follows:\n% $\\mathrm{G}$, consisting of  $\\mathrm{m}$  non-overlapping layers, where each layer is modeled with a weighted graph $ \\mathrm{G}_{i}$, with an associated adjacency matrix $\\mathrm{A}_{(i, i)}, i=1, \\ldots, m$. The elements of the set  $\\mathrm{A}=\\left\\{\\mathrm{A}_{(1,1)}, \\mathrm{A}_{(2,2)}, \\ldots, \\mathrm{A}_{(m, m)}\\right\\}$ are referred to as the within-layer matrices, representing c, connections along a single layer, known as intra-layer links. To model dependencies between two graphs, $ \\mathrm{G}_{k}$ and $\\mathrm{G}_{l}$  with their adjacency matrices, $ \\mathrm{A}_{(k, k)} $ and  $\\mathrm{A}_{(l,k)}$, respectively  $(k, l=1,2, \\ldots, m ; k \\neq l)$, we consider one-to-one symmetric inter-connectivity of nodes in the corresponding graphs. As a result, we obtain a set of cross-layer adjacency matrices  $\\mathrm{D}_{p}=\\left\\{A_{(l, k)}, k \\neq l\\right\\}$  that specifies the edges between nodes in different layers, where  $p$  is the number of dependencies. That is, a multi-layer network,  $\\mathrm{G}$, has a set  $\\mathrm{E}_{I}(\\mathrm{G})$  of inter-layer links that connect nodes across layers, i.e., for each edge\n%  $(u, v) \\in \\mathrm{E}_{I}(\\mathrm{G})$, we have  $u \\in V\\left(\\mathrm{G}_{k}\\right)$  and  $v \\in \\mathrm{V}\\left(\\mathrm{G}_{l}\\right)$  for  $k \\neq l$. The supra-adjacency matrix of the multi-layer network  G  is defined as a block-matrix structure:\n\n\\begin{equation}\n    \\mathbb{A}=\\left(\\begin{array}{ccccc}\n    A_{(1,1)} & \\cdots & A_{(1, k)} & \\cdots & A_{(1, m)} \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    A_{(l, 1)} & \\cdots & A_{(k, k)} & \\cdots & A_{(l, m)} \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    A_{(m, 1)} & \\cdots & A_{(m, k)} & \\cdots & A_{(m, m)}\n    \\end{array}\\right).\n\\end{equation}\nwhere $A_{i,i}$ refers to the intra-layer adjacency matrix and  $A_{i,j} (i\\neq j)$ refers to cross-layer adjacency matrix.  \n% The diagonal elements corresponding to set A are within layer matrices. Off-diagonal matrices  $\\mathrm{A}_{(l, k)}$ \n%  for $ k, l=1,2, \\ldots, m ; k \\neq l$ represent inter-layer links that connect nodes in layer  $\\mathrm{G}_{k}$  to nodes in layer  $\\mathrm{G}_{l} $.\n\n "
                },
                "subsection 3.2": {
                    "name": "Edge Construction",
                    "content": "\n% This module will generate NetFlow packets in network traffic and then transform NetFlow packets to edges of dynamic graphs with the definition:\n% The traffic data in the network can be sampl\n%Network intrusion detection is typically performed on flow-based network traffic data such as NetFlow \\cite{claise2004cisco}, where traffic is identified by the communication endpoints (IP address, IP port number, protocol), and annotated via a set of traffic fields that provide traffic profiles, such as the number of packets, flow duration, etc. \n\nWe have constructed a multi-layer dynamic graph by defining the devices as nodes and the communications between two devices as edges. {To construct the edge relationships in the multi-layer graph,} we first transform the Netflow data to edges by the following definition:  \n\\begin{equation}\n\\mathbf{E}_{i j}(t)=(v_{i}, l_{i}, v_{j}, l_{j}, t, \\Delta t, \\mathbf{F}_{ij}(t)).\n\\label{eqa}\n\\end{equation}\nFirst, we concatenate the source IP and source port in the original traffic flows as the source identity for the device $i$. Similarly, we can obtain the destination identity for the device $j$. We denote $v_i$ and $v_j$ as the source and destination nodes respectively. Secondly, $l_i$ denotes the layer of device $i$, and $l_i=0$ indicates that $i$ is a terminal device such as a PC, a server, or an IoT device. Here $l_i=1$ indicates that $i$ is an intermediate device such as a router in the communication link. Specifically, we assign devices with the router address $192.168.0.1$ or with many stable connections in layer $1$\\iffalse\\footnote{More descriptions of the multi-layer construction are provided in Appendix \\ref{Multi-Layer Construstion}.}\\fi. Then $t$ refers to the timestamp of traffic and $\\Delta t$ indicates the traffic duration time. Finally, $\\mathbf{F}_{ij}(t)$ is the traffic features.\n\n"
                },
                "subsection 3.3": {
                    "name": "Problem Formulation",
                    "content": "\n",
                    "subsubsection 3.3.1": {
                        "name": "Dynamic Traffic Intrusion Detection",
                        "content": "\nWe first define the devices as nodes and the communications with timestamps between any pair of devices as edges. We use $\\mathrm{T}$ to represent the maximum timestamp. An Edge sequence $\\mathbb{E}$  is denoted by  $\\left\\{\\mathcal{E}^{t}\\right\\}_{t=1}^{T}$, where each  $\\mathcal{E}^{t}$ represents a network traffic. \nAlso, after each edge, there is a corresponding multi-layer graph, then the corresponding multi-layer graph stream  $\\mathbb{G}$ takes the form of  $\\left\\{\\mathcal{G}^{t}\\right\\}_{t=1}^{T} $, where each  $\\mathcal{G}^{t}=\\left(\\mathcal{V}^{t}, \\mathcal{E}^{t}\\right)$  represents the multi-layer graph at timestamp $t$. \n% and  $\\mathcal{V}^{t}$  and  $\\mathcal{E}^{t}$  are the set of nodes and edges respectively. An edge  $e=(i, j, w) \\in \\mathcal{E}^{t} $ means that the  $i$-th node and the  $j$-th node have a connection in the dynamic graph at the timestamp $t$ with its weight  $w$. For unweighted graphs,  $w$  is always 1; \nA multi-layer adjacency matrix  $\\mathbb{A}^{t} \\in \\mathbb{R}^{m \\times n} $ represents the edges in  $\\mathcal{E}^{t}$, where  $\\forall(i, j, w) \\in \\mathcal{E}^{t}, \\mathbb{A}^{t}[i][j]=w_{ij}$ and $w_{ij}$ is the weight of the matrix. The goal of intrusion detection is to learn to predict the edge  $\\mathcal{E}^{t}$ as a benign traffic or an attack in binary classification, and a specific type under the multi-classification.  \n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Few-shot Intrusion Detection",
                        "content": " Given a few-shot intrusion detection task $T = \\{C_{\\text{tr}}, C_{\\text{te}}\\}$, where $C_{\\text{tr}}$ is a training set with few-shot traffic, and $C_{\\text{te}}$ is a test set. Our goal is to design a few-shot module to accurately classify the samples in $C_{\\text{te}}$ by few-shot learning on $C_{\\text{tr}}$.  $C_{\\text{tr}}$ includes two key components, support set and query set. The former provides the few-shot traffic information to the model, and the latter contains new instances for model evaluation and optimization. The query set can be sampled from the support set for the traffic-limited scenarios. Specifically, we conduct $\\mathcal N$-way $\\mathcal K$-shot intrusion detection, $\\mathcal N$-class traffic with $\\mathcal K$ samples, i.e., $5$, in the support set. The few-shot intrusion detection model aims to conduct accurate detection in the query set, while the only available reference is the few-shot traffic in the support set.\n\n%, our goal is to develop a learning model such that after meta-training on labeled nodes in $C_{\\text{tr}}$, the model can accurately predict labels for the nodes in the query set $Q$, where the only available reference is the limited labeled nodes in the support set $S$.\n\n%Moreover, under the $\\mathcal N$-way $\\mathcal K$-shot setting, the support set $S$ consists of exactly $\\mathcal K$ labeled nodes for each of the $\\mathcal N$ classes from $C_{\\text{te}}$, and the query set $Q$ is also sampled from the same $\\mathcal N$ classes. In this scenario, the problem is called an $\\mathcal N$-way $\\mathcal K$-shot node classification problem. Essentially, the objective of few-shot node classification is to learn a model that can well generalize to meta-test classes in $C_{\\text{te}}$ with only limited labeled nodes as the reference.\n\n"
                    }
                }
            },
            "section 4": {
                "name": "MODEL",
                "content": "\nIn this section, we present the DIDS-MFL, which consists of five main modules. Figure \\ref{model} shows the architecture of our proposed model. Next, we dive into the details of these modules.\n \n\n ",
                "subsection 4.1": {
                    "name": "Statistical Disentanglement",
                    "content": "\n\\label{Edge Representation Disentangle module}\n\nAs we discussed in Section \\ref{sec:intro}, statistical distributions of traffic features are one of the main underlying causes of performance variations. i.e., separated distributions benefit the unknown attack identification, while entangled ones are indistinguishable and thus unable to help the NIDS to make accurate decisions. Therefore, our aim is there to disentangle the traffic features and make them distinguishable.\n\n\n%As we have analyzed in the introduction section, given tens and hundreds of complex entangled traffic features, deep learning-based methods cannot disentangle them on the feature-element level, thus leading to poor detection performance. \n%to better capture the difference between normal traffic and different attacks\n\nTo separate the features of traffic without any prior knowledge, we formulate the differentiation as a constrained non-parametric optimization problem and approximate the optimal results by solving the Satisfiability Modulo Theory (SMT) \\cite{de2008z3}. We perform a min-max normalization on the edge feature $\\mathbf{F}_{ij}(t)$. For convenience, we denote the normalized edge feature as $\\mathcal{F}$, and $\\mathcal{F}_i$ is the $i$-th normalized element.\n\n% The solution is to treat this problem of disentangling coupled features as a constrained optimization problem and transform the original problem into a Satisfiability Modulo Theory (SMT) problem. We approximate the optimal solution to the actual situation by solving the SMT problem.\n\n% We obtain the transformed edges $\\mathbf{E}_{ij}(t)$ from the output of the previous module, and we consider the edge feature $\\mathbf{F}_{ij}(t)$ in the edge. Firstly, we perform min-max normalization on $\\mathbf{F}_{ij}(t)$ and denote the normalized vector by $\\mathbf{n}$.\n% % , the dimension of $n$ is same as $F_{ij}(t)$. \n\n% , the dimension of $n$ is same as $F_{ij}(t)$. \n\nWe need a weight matrix $\\mathbf{w}$ to generate the disentangled representation of $\\mathcal{F}$. Our key optimization objectives are to minimize the mutual information between the elements of traffic features and also bound the range of $\\mathbf{w}$ when we perform aggregation. We start by constraining the weight matrix $\\mathbf{w}$ with the range of the superposition function as follows:\n% First, we assume that we can find a set of independent variables $V$ to\n% describe the values of each network traffic data feature. \n% We consider all the obtained network traffic features to be a sample of independent variables that can be written as $V_i$. We need to find a vector $\\mathbf{w}$ to represent these random variables.\n% Our key optimization objectives are to minimize the mutual interference\n% between the variables and to limit the overall range when the variables\n% are superimposed. We can start by constraining the range of the encoding\n% vector \\(w\\) with the range of the superposition function as follows:\n\\begin{gather}\n  % \\begin{gather}\n    W_{\\min } \\leq \\mathbf{w}_{i} \\leq W_{\\max } \\quad(1 \\leq i \\leq N),\\quad\n    \\sum_{i=1}^{N} \\mathbf{w}_{i} \\mathcal{F}_{i} \\leq B,\n  % \\end{gather}\n\\end{gather}\nwhere $W_{\\min }$, $W_{\\max }$ and $B$ are constants, $N$ is the edge feature dimension.\nWe then constrain the order-preserving properties of generated representations:\n\\begin{equation}\n  \\mathbf{w}_{i} \\mathcal{F}_{i} \\leq \\mathbf{w}_{i+1} \\mathcal{F}_{i+1}\\quad(1 \\leq {i} \\leq {N-1}).\n\\end{equation}\n\nFinally, we maximize the distance between components in the vector $\\mathbf{w}$, consequently minimizing the mutual information between each two feature elements. In this way, we can disentangle the distribution of element-wised features. The optimization objective can be expressed as follows:\n\n\\begin{equation}\n\\begin{split}\n    \\widetilde{\\mathbf{w}}=\\arg \\max (\\mathbf{w}_{N} \\mathcal{F}_{N}-\\mathbf{w}_{1} \\mathcal{F}_{1}-\n    \\\\\\sum_{i=2}^{N-1} \\left|2 \\mathbf{w}_{i} \\mathcal{F}_{i}-\\mathbf{w}_{i+1} \\mathcal{F}_{i+1}-\\mathbf{w}_{i-1} \\mathcal{F}_{i-1}\\right|).\n  \\end{split}\n  \\label{target}\n\\end{equation}\nWe are unable to determine the convexity of the optimization\nobject due to its closed form. Therefore, we transform the above problem into an SMT problem with an optimization objective (\\ref{eqb}) and a subjection (\\ref{constrain}) to approximate the optimal results.\n%For each edge of feature $\\mathbf{F}_{ij}(t)$, we perform min-max normalization on $\\mathbf{F}_{ij}(t)$ and denote the normalization vector by $\\mathbf{n}$. \n% We list the constraints (\\ref{constrain}). Boolean Satisfiability Problem (SAT) solutions to the SMT problem are obtained that maximize the following objectives:\n\n\\begin{equation}\n    \\begin{split}\n        \\widetilde{\\mathbf w}=\\arg \\max (\\mathbf{w}_{N} \\mathcal{F}_{N}-\\mathbf{w}_{1} \\mathcal{F}_{1}+\n        \\\\ \\sum_{i=2}^{N-1} 2 \\mathbf{w}_{i} \\mathcal{F}_{i }-\\mathbf{w}_{i-1} \\mathcal{F}_{i-1}-\\mathbf{w}_{i+1} \\mathcal{F}_{i+1}),\n    \\end{split}\n    \\label{eqb}\n\\end{equation}\nsubjects to:\n\\begin{equation}\n  \\left\\{\\begin{array}{lll}\n    \\mathbf{w}_{i} & \\in & {\\left[W_{\\min }, W_{\\max }\\right]} \\\\\n    \\sum_{i=1}^{N} \\mathbf{w}_{i} \\mathbf{n}_{i} & \\leq & B \\\\\n    \\mathbf{w}_{i} \\mathcal{F}_{i} & \\leq & \\mathbf{w}_{i+1} \\mathcal{F}_{(i+1)} \\\\\n    2 \\mathbf{w}_{i} \\mathcal{F}_{i} & \\leq & \\mathbf{w}_{i-1} \\mathcal{F}_{i-1}+\\mathbf{w}_{i+1} \\mathcal{F}_{i+1}.\n  \\end{array}\\right.\n  \\label{constrain}\n\\end{equation}\n{The subjection in Eq.~\\ref{constrain} ensures variants' ranges and the order-preserving property of generated representation. It also ensures symbols when removing the absolute value sign. The detailed explanation of why Eq.~\\ref{eqb} can generate the disentangled representation is available in Apeendix~11.}\n\n{Specifically, we employ Eq.~\\ref{eqb} as an optimization objective, i.e., loss function, and utilize Adam optimizer to solve it.} Then we can generate the disentangled edge representation $\\mathbf h_{i,j}$, which can be expressed as $\\mathbf h_{i,j}=\\mathbf{w}\\odot\\mathcal{F}$, where the symbol $\\odot$ represents the Hadamard product. \n\nEquipped with the above non-parametric optimization, we can differentiate tens and hundreds of complex features of various attacks, mitigating the entangled distribution of statistical features. Such statistically disentangled features facilitate our model to be more sensitive to various attacks.\n\n"
                },
                "subsection 4.2": {
                    "name": "Representational Disentanglement",
                    "content": "\n% To represent the embedding update of the two devices involved in the network interaction, we set up the module that can: 1) generate messages from the disentangled edge embedding, which represent the jump of the involved nodes embedding; 2) continuously update the memory to store historical information of the nodes; 3) incorporate the message with past nodes embedding to generate the new nodes embedding, while maintaining the disentangled property. Our Doubly Disentangled Memory\nSo far we have constructed edges and statistically differentiated the features of traffic flows {$\\mathbf h_{i,j}=\\mathbf{w}\\odot\\mathcal{F}$}. This module generates contextualized \\textbf{node representations} $\\mathbf X$ from edge representations {$\\mathbf h_{i,j}$ by using the temporal information}. This involves three steps:\n\n\\textbf{1) Generating updating messages:}\n% There is an abrupt change in node representation caused by the interaction between nodes, and we use message $c(t)$ to represent it. We generate messages by incorporating historical memory and disentangled edge representations.\n% memory is a compressed format of the node’s history information. \n% We pass the disentangled embedding $\\mathbf{h}_{i,j}$, edge duration time $\\Delta t$ and the past memory of the two interacting nodes \\(s_i(t^-) \\), \\(s_j(t^-) \\) to the message function $\\operatorname{Msg}$.\n% Specifically, we generate the messages of node $i$ and $j$, using the message function noted as\n% can be computed for the node involved in the event:\nFor an incoming traffic flow, we will build an edge or update the corresponding edge, which may lead to a dramatic change in the node representations involved in this interaction. We can update the node representations by utilizing this change. Therefore, we name the abrupt change an updating message and denote it as {$\\mathbf c(t)$}. Specifically, we generate {$\\mathbf c(t)$} by incorporating historical memory and disentangled edge representation $\\mathbf h_{i,j}$. The messages of node $i$ and $j$ are as follows:        \n% we use message c(t) and we generate messages by incorporating historical memory and disentangled edge representations.\n%Specifically, the messages of node $i$ and $j$ are noted as:\n% Msg,c,,,Mem,m,x,h\n% $\\operatorname{Msg}$:\n       \\begin{align}\n              \\mathbf{c}_{i}(t)&=\\operatorname{Msg}\\left(\\mathbf{m}_{i}\\left(t^-\\right),\\mathbf{m}_{j}\\left(t^-\\right),t,\\Delta t,l_i, l_j, \\mathbf{h_{i,j}}\\right),\\\\\n              \\mathbf{c}_{j}(t)&=\\operatorname{Msg}\\left(\\mathbf{m}_{j}\\left(t^-\\right),\\mathbf{m}_{i}\\left(t^-\\right),t,\\Delta t,l_i, l_j,\\mathbf{h_{i,j}}\\right),\n              \\label{eqc}\n        \\end{align}\n        where $\\mathbf{h}_{i,j}$ is the disentangled edge representation. $\\Delta t$ is the edge duration time, $l_i,l_j$ is the layer marks of edge, $\\mathbf {m}_i(t^-)$ is the historical memory of the two interacting nodes, {where $t^-$ is a historical time point, compared to the existing time point $t$}, $\\operatorname{Msg}$ is a learnable function, and we use RNN. {The initial memory $\\mathbf m_i(0) $ is $\\mathbf 0$, $ \\forall i\\in V$}. {Specifically, RNN aims to generate the new memory message $\\mathbf c (t)$ combining the historical memory $\\mathbf m(t^-)$ and disentangled representation $\\mathbf h_{ij}$ by learnable weight matrices and gating mechanism.}\n \n        % When processing the same batch of edges, a node may have multiple edges information, we only use the latest edge information to calculate the message. \n        \n\\textbf{2) Updating node memory:}\n        % Memory is at time t consists of a vector si(t) for each node i the model has seen so far\n        %C，D\n        We update the node memory by merging the latest message with the historical memory and then encode the merging information, which can be expressed as follows:\n               \\begin{equation}\n          \\mathbf{m}_{i}(t)=\\operatorname{Mem}\\left(\\mathbf{c}_{i}(t),  \\mathbf{m}_{i}\\left(t^{-}\\right)\\right),\n          \\label{eqd}\n        \\end{equation}\n        where $\\operatorname{Mem}$ is an encoder, and here we use GRU. We similarly update the memory of node $j$ via (\\ref{eqd}). {Specifically, GRU introduces Reset Gate and Update Gate to better generate the updated node memory $\\mathbf m_i(t)$ by fusing historical memory $\\mathbf{m}_{i}(t^-)$ and new memory message $\\mathbf c_i(t)$. GRU considers both long-term and short-term dependencies, thus mitigating the vanishing gradient issue in temporal models.}\n%t0时刻\n          \n\\textbf{3) Generating second disentangled node representations:}\n%初始化结点话特征是0\nWe can generate the node representation by utilizing the updated memory in (\\ref{eqd}), and the representation of node $i$ in time $t$ can be expressed as $\\mathbf{x}_i(t)=\\mathbf{x}_i(t^-)+\\mathbf{m}_i(t)$, where\n$\\mathbf{{x}}_i(t^-)$ is the historical representation of node $i$. {In this way, we obtain the dynamic node representations of traffic in an evolving time flow.} The representation of node $j$ can also be generated in a similar way. The initial value of node representations {$\\mathbf X(0)$ is $\\mathbf 0$.}\n\nWe aim to preserve the disentangled property in node representations. However, the update operations above may entangle them at the element level again. \nTherefore, we propose the second representational disentanglement, which aims to highlight the attack-specific features in node representations in the following end-to-end manner. It also ensures that the element-wised representations are close to orthogonal.\n\n%\\vspace{-8pt}\n        \\begin{equation} \n        %-的符号\n             \\mathcal{L}_{\\text {Dis }}=\\frac{1}{2}\\left\\|\\mathbf{X} (t){\\mathbf{X}({t^-})}^{\\top}-\\mathbf{I}\\right\\|_{F}^{2}.\n             \\label{eqe}\n        \\end{equation}\n%we aim to highlight the attack-specific features for the following end-to-end training, which forms our second neural-based disentanglement. This constraint can be expressed as follows. It also makes sure that element-wised representations are close to orthogonal.\n\nThe above regularization encourages the model to learn smaller coefficients between every two elements of node representations. Such representations can be more differentiated. As the module is supervised by the signal of a specific attack, it can learn to highlight the attack-specific features, so as to improve the detection accuracy, as discussed in Section \\ref{sec:intro}. By doing so, we are able to mitigate the entangled distribution of representational features as mentioned at the beginning.   \n\n"
                },
                "subsection 4.3": {
                    "name": "Multi-Layer Graph Diffusion",
                    "content": "\n\\label{Multi-Layer Graph Diffusion module}\n% On the one hand, the network just like the real-world scenario, \nSo far, we have generated the {dynamic} disentangled node representations with temporal information. For further fusing the multi-layer topological structure information, we propose a multi-layer graph diffusion module. Please note that we still preserve the disentangled property by the following customized designs. \n\nWe utilize the following graph diffusion method to fuse the topological information in evolving graph streams, which can capture the fine-grained spatial-temporal {coupled} information. The previous dynamic intrusion detection methods may lose this information in separated time gaps, due to the employed time-window or snapshots-based methods. {More discussions on the `dynamic' and superiority of the proposed graph diffusion approach compared to the previous methods, are available in Appendix 14.2.}\n\\begin{equation}\n    \\left\\{\\begin{array}{l}\n\\partial_t\\mathbf{X}=\\mathbf{F}\\left( \\mathbf{X}, \\mathbf{\\Theta}_{}\\right) \\\\\n\\mathbf{X}(0)=\\mathbf{0},\n\\end{array} \\right.\n\\label{dynamic_ini}\n\\end{equation}\nwhere $\\mathbf{F}$ is a matrix–valued nonlinear function conditioned on graph $\\mathcal{G}$, and $\\mathbf{\\Theta}$ is the tensor of trainable parameters. {The above Eq.~\\ref{dynamic_ini} establishes the foundation for spatial-temporal coupled information modeling.}\n%$\\mathbf {C}$ is the dynamic node messages calculated in Eq.~\\ref{eqc}.\n\n%Furthermore, the multiple interactions (e.g., intra- and inter-layer interactions) within the multi-layer traffic networks brings more challenges to traditional GCN aggregated across different types of edges. The common single-layer based graph diffusion method treats all embedding dimensions similarly and thus obtains a relatively coarse embedding of node features. \n\n%Our node embedding preserves the disentangled ability, we can assume that the connection can be represented by some dimension of the disentangled embedding rather than the whole representation.\n% The above insights can be depict via the pearson correlations between element-wised representation, .\nSpecifically, we aim to amplify the important dimensions of disentangled representations and depress the influence of trivial feature elements in the diffusion process. To formulate this process, we consider PM (PeronaMalik) diffusion \\cite{perona1990scale}, a type of nonlinear filtering. It can be expressed as:\n \\begin{equation}\n  \\left\\{\\begin{array}{ll}\n    \\frac{\\partial x(u,t)}{\\partial t} & = \\operatorname{div}[g(|\\nabla x(u, t)|) \\nabla x(u, t)] \\\\  \n    x(u, 0)& = 0,\n    \\end{array}\\right.\n    \\label{pm}\n\\end{equation}\n%u的解释\nwhere $div$ is the divergence operator, $\\nabla$ is the gradient operator, and $g$ is a function inversely proportional to the absolute value of the gradient. \n\n{Before formally formulating the multi-layer graph diffusion, we propose the following spatial-temporal influence coefficient $s_{ij} \\in \\mathbf S$ between nodes i,j in $t_{ij}$ time. The coefficient matrix considers the information changes over spatial-temporal coupled traffic data. Specifically, different layers and topology structures over traffic networks and the traffic feature interactions at different times will influence the node representations dynamically:}\n\n%In addition, the edges in dynamic multi-layer graphs always have different timestamps and are in different layers. There are also different interactions at different layers and times. These factors will dynamically affect the information exchange results over graphs. Therefore, we aim to incorporate the above factors into the diffusion process. We propose the following layer-temporal coefficient $s_{ij}$ between nodes i,j in $t_{ij}$ time: \n%%as the layer-temporal coefficient  $s_k$:\n\n% Moreover, given edges are located inter- or intra-layer and with  different timestamps,  which will control the information exchange through these edges.  To represent the influence, we  compute  the  layer-temporal coefficient $s_k$ in edge $e_k $ between nodes $v_i$ and $v_j$ as:\n\\begin{align}\n\\label{co}\n    s_{ij}&=f(l_i||l_j||\\phi(t-t_{ij})),\\\\\n    f(\\mathbf{x})&= \\mathbf{W}^{(2)} \\cdot \\operatorname{ReLU}\\left(\\mathbf{W}^{(1)} \\mathbf{x}\\right),\n\\end{align}\nwhere {$||$ is a concatenate operator}, $\\phi(\\cdot)$ is a generic time encoder \\cite{xu2020inductive} to generate temporal representations, $\\mathbf{W}^{(1)}$  and  $\\mathbf{W}^{(2)}$  are the parameters of the first and second layer MLP.\n%$f(\\cdot)$ is defined in (15)\n\n{Now we formally propose the multi-layer graph diffusion module.} We first define a differential operator on the multi-layer graph, aiming to transfer the above continuous PM diffusion {in Eq.~\\ref{pm}} to multi-layer graphs. As known from previous literature \\cite{chung1997spectral}, the gradient operator corresponds to the instance matrix $\\mathbf{M}$, while the divergence operator corresponds to the matrix $\\mathbf{M}^{\\top}$, and we can compute the matrix $\\mathbf M$ by the equation $\\mathbf{M}^T\\mathbf{M}=\\mathbf{D}-\\mathbf{A}$, where $\\mathbf{D}$ is the diagonal matrix.\n% To incorporate the above consideration, as known in previous literature \\cite{chung1997spectral}, the differential operators could be instantiated on a discrete graph. The gradient operator corresponding to the matrix $\\mathbf{M}$ , while the divergence operator corresponding to the matrix $\\mathbf{M}^{\\top}$, and $\\mathbf{M}^T\\mathbf{M}=\\mathbf{D}-\\mathbb{A}$, where the $\\mathbf{D}$ is the diagonal matrix.\nThen our novel multi-layer diffusion can be expressed as:\n\n% \\begin{equation}\n%      {\\partial\\mathbf{X}_t}=-\\mathbf{M}^{\\top} \\sigma(\\mathbf{M} \\mathbf{X}\\mathbf{K}^{\\top})\\mathbf{S}\\left(\\mathbf{M} \\mathbf{X} \\mathbf{K}^{\\top}\\right) \\mathbf{K},\n%      \\label{eqf}\n% \\end{equation}\n\n\\begin{equation}\n    \\resizebox{.7\\linewidth}{!}{$\n        {\\partial\\mathbf{X}_t}=-\\mathbf{M}^{\\top} \\sigma(\\mathbf{M} \\mathbf{X}\\mathbf{K}^{\\top})\\mathbf{S}\\left(\\mathbf{M} \\mathbf{X} \\mathbf{K}^{\\top}\\right) \\mathbf{K},\n    $}\n    \\label{eqf}\n\\end{equation}\nwhere $\\mathbf{K}$ is a transformation matrix, $\\mathbf{S}$ is the structure-temporal influence coefficients {calculated in Eq.~\\ref{co}} and $\\sigma(\\cdot)$ represents the function $\\exp(-|\\cdot|)$. %这里需要一个缩写 \nThe solution to equation \\ref{eqf} can be expressed as:\n\\begin{equation}\n  \\mathbf{X}_{t+\\Delta t}=\\mathbf{X}_t+\\int_t^{t+\\Delta t}{\\partial_t \\mathbf{X}_t}d \\tau,\n  \\label{eqk}\n\\end{equation}\nwhere $t$ is the last edge occurrence time and $\\Delta t $ is the edge\nduration and we use the Runge-Kutta method to solve this equation.\n\n"
                },
                "subsection 4.4": {
                    "name": "Classifier and Loss Function",
                    "content": "\nFor DIDS, we make the two-step predictions for intrusion detection. We utilize the first MLP to classify whether the traffic is benign or anomalous and utilize the second MLP to detect the specific type of attack. When an unknown attack invades, the direct multi-classifications will be easy to fail to assign the anomalous label thus leading to poor performance. In contrast, through the first-step binary classification, DIDS will focus more on inconsistencies with normal behavior to improve the performance of detecting unknown attacks. The following second multi-classification will further alert the administrators that what kind of attack it is more similar to so that similar mitigation measures can be taken. Specifically, the intrusion loss can be expressed as:\n\n\n%The first MLP is to detect if the traffic is normal or an attack, and the second MLP is to detect the specific type of attack. This is because when faced with an unknown attack, it is difficult to assign a classification label through direct multi-classification, which significantly reduces its ability to detect unknown attacks.In contrast, we first perform binary classification on the targeted traffic and then perform multi-classification. Therefore, the detection can first focus more on the inconsistencies with normal behavior and can detect the unknown attack. If they are judged to be attacks, the result of the multi-classification can tell us which attack it is more similar to, so that the same mitigation measures can be taken.Then, the intrusion loss can be expressed as:\n% \\begin{equation}\n%     \\mathcal{L}_{\\text {Int}}=-\\sum_{i=1}^{m}\\log \\left(1-p\\left(e_{\\mathrm{nor}, i}\\right)\\right)+\\log \\left(p\\left(e_{\\mathrm{att}, i}\\right)\\right).\n%     \\label{eqg}\n% \\end{equation}\n% \\begin{equation}\n%     \\mathcal{L}_{\\text {Int}}=-\\sum_{i=1}^{m}(\\log(1-p_{\\mathrm{nor},i})+\\log(p_{\\mathrm{att}, i})+\\sum_{i=1}^{m}\\sum_{j=1}^{K}y_{i,k}{\\log(p_{i,k})}).\n%     \\label{eqg}\n% \\end{equation}\n\\begin{equation}\n    \\resizebox{.9\\linewidth}{!}{$\n        \\mathcal{L}_{\\text{Int}} = -\\sum_{i=1}^{m} (\\log(1-p_{\\mathrm{nor},i}) + \\log(p_{\\mathrm{att}, i}) + \\sum_{j=1}^{K} y_{i,k} \\log(p_{i,k}))\n    $}\n    \\label{eqg}\n\\end{equation}\nwhere $m$ is the batch size, $K$ is the number of attack classes, $p_{\\mathrm{nor},i}$ is the probability of normal, $p_{\\mathrm{att}, i}$ is the probability of attack.\nAdditionally, the adjacent time intervals may cause adjacent times embedding to be farther apart in embedded space, due to the learning process independency. To address this problem, we constrained the variation between adjacent timestamps embedding by minimizing the Euclidean Distance: \n\\begin{equation}\n    \\mathcal{L}_{\\text {Smooth}}=\\sum_{t=0}^{T}\\left\\|\\mathbf{X}_{t+\\Delta {t}}-\\mathbf{X}_{t}\\right\\|_{2}.\n    \\label{eqh}\n\\end{equation}\nFinally, the overall loss of DIDS can be expressed as follows:\n\\begin{equation}\n \\mathcal{L}= \\mathcal{L}_{\\text {Int}}+\\delta  \\mathcal{L}_{\\text {Smooth }}+\\zeta  \\mathcal{L}_{\\text{Dis}},\n \\label{eqj}\n\\end{equation}\nwhere $\\delta$, and $\\zeta$ are trade-off parameters.\n\n"
                },
                "subsection 4.5": {
                    "name": "Multi-scale Few-shot Learning",
                    "content": "\n{So far we have proposed DIDS for supervised intrusion detection. Furthermore, we propose the following MFL to detect few-shot threats accurately, e.g., $5$ samples of each attack type.}\nWe first denote the learned representations $\\mathbf{X}_{t + \\Delta t}$ in Eq.~\\ref{eqk} as $\\mathbf Z \\in \\mathcal R ^{L*N}$, where $L$ is the length of representations, and $N$ is the few-shot sample number. The few-shot learner aims to generate a coefficient matrix $\\mathbf S$ as a learned similarity matrix between each sample pair, thus benefiting the distinction of few-shot traffic threats.\n%spanned by the multi-scale transformed few-shot representation $\\mathbf{Z}$.\n\n%To obtain $\\mathbf S$, \n\nWe first focus on the original representations $\\mathbf Z$ to generate a coefficient matrix in the following formulation:\n\\begin{equation}\n\\label{eq: Q_1}\n\\min _{\\mathbf H} =\\|\\mathbf Z- \\mathbf Z\\mathbf H \\|_{F}^{2}+\\lambda_{1}\\|\\mathbf \nH\\|_{F}^{2},  s.t. \\rm{diag}(\\mathbf H)=0,\n\\end{equation}\nwhere $|\\cdot|_{F}^{2}$ is the Frobenius matrix norm (F-norm) and $\\rm{diag}(\\mathbf H)$ denotes the diagonal entries of $\\mathbf H$. $\\mathbf H$ can be directly derived by solving Eq.~\\ref{eq: Q_1}. {Specifically, we directly employ Eq.~\\ref{eq: Q_1} as a loss function and utilize backpropagation and optimizer, e.g., Adam, to solve it.}\n\nThe above self-expressive optimization only focuses on preserving the original-scale information of $\\mathbf Z$. The multi-scale information of $\\mathbf Z$ can also provide necessary information to make few-shot traffic samples distinguishable. To this end, we proposed a transform-based algorithm to generate coefficient matrix $\\mathbf Q$ by using $\\mathbf Z$'s multi-scale information. This process can be considered as an `augmentation' of the original representations $\\mathbf Z$ in the latent space, aiming to capture the invariant attack-specific features across different scales. Finally, we fuse the obtained coefficient matrices $\\mathbf H$ and $\\mathbf Q$. We refer to the above model as Multi-scale Few-shot learning (MFL). Equipped with the fusion of generated matrices, MFL can effectively utilize few-shot representations across different scales, i.e., under the original and transformed scale, thus benefiting the distinction of few-shot traffic threats. We have the following principles to achieve MFL and {clarify the motivations behind these designs}:\n\\begin{itemize}\n    \\item 1. MFL should encourage the representations under different scales to be close in latent space, thus capturing the attack-specific features and benefiting the distinction of traffic samples in different types, {which is motivated by the empirical studies in Fig.~\\ref{fig:few-shot empirical} (a)(b)}\n    \\item 2. MFL should ensure the disentanglement among the elements of the learned representation, thus highlighting the attack-specific information, {which is motivated by the empirical studies in Fig.~\\ref{fig:few-shot empirical} (c)(d)}\n\\end{itemize}\n%consider a fine-grained latent space to generate coefficient matrix $\\mathbf{Q}$. Then the metric matrix $\\mathbf S$ can be obtained by fusing $\\mathbf{Q}_1$ and $\\mathbf Q_2$. We refer to the above model as a Multi-scale Few-shot Metric (MFL) learning algorithm. \n\n\\justifying\n% \\raggedright\nTo achieve the above goals, we first propose a transformation-based learning scheme by projecting the representations $\\mathbf Z$ to a latent space, aiming to discover the attack-specific invariant features across different scales. Then we introduce a regularization term to disentangle the few-shot representations. Finally, we propose an alternating optimization algorithm to derive $\\mathbf Q$ in an end-to-end way. The implementation involves the following three steps:\n\n\n\\iffalse\nAs elaborated in the Introduction section, we design the following Multi-scale Stable Few-shot (MSF) algorithm. MSF enhances the stable similarity learning of the few-shot samples by spanning a latent space and disentangling the learned representations. MSF is a highly efficient and plug-and-play module to benefits few-shot intrusion detection.\n\n\n%As elaborated in the Introduction section, the above 3D-IDS model performs worse when the few-shot intrusion samples are incorporated into the training. Therefore, we design the following algorithm to enhance the stability of the few-shot samples in the latent learning space, thus improving few-shot learning performances in 3D-IDS. It is a highly efficient and plug-and-play module to benefits the detection.\n\n%The empirical studies in the Experiments section show a stable learner can also improve the disentanglement ability of few-shot sample learning.\n\nFor the learned representations $\\mathbf{X}_{t+\\Delta t}$ in Eq.~\\ref{eqk}, we denote it as $\\mathbf Z \\in \\mathcal R ^{N*L}$, where $N$ is the few-shot sample number, and $L$ is the length of embeddings. We aim to derive a coefficient matrix $\\mathbf S$, spanned by the multi-scale transformed few-shot representation $\\mathbf{Z}$. The stable learning algorithm provides a latent subspace to support the few-shot sample learning. To this end, we propose the following two design principles:\n\\begin{itemize}\n    \\item 1. MSF should prompt the similarity learning of the representations in latent space, thus improving the learning stability of few-shot learning.\n    \\item 2. MSF should ensure the disentanglement of the learned representations in spanned latent space, thus highlighting the attack-specific information.\n\\end{itemize}\n\nFor the first principle, we propose a multi-scale transformation-based self-expressive framework. \n\nFor the second principle, we introduce a regularization term in the self-expressive framework to disentangle the few-shot representations. Finally, we propose a highly efficient alternating optimization algorithm MSF to optimize the above self-expressive and disentanglement in an end-to-end way. \n\\fi\n\\textbf{1) Generating Representation Transformation.}\n\\noindent\n\\\n\nWe denote $\\mathbf Z _ o = \\mathbf Z$  as the original representations, and $\\mathbf Z _ t = \\mathcal G (\\mathbf Z)$ as the transformed representations, where $\\mathcal G$ is a transformation operator. Specifically, we introduce a scaling operator $\\mathcal S$ as follows:\n\\begin{equation}\n\\mathcal{S}=\\operatorname {diag}(s_1, s_2, ..., s_L)=\\left[\\begin{array}{ccc}\ns_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & s_L\n\\end{array}\\right],\n\\end{equation}\nwhere $s_1, s_2, ..., s_D$ is the scale factor, and different values determine different scalars. Then the scaling transformation can be written as $\\mathbf Z _ t = \\mathcal G (\\mathbf Z) = \\mathcal S \\mathbf Z$. Especially, when $s_1=s_2=...=s_L=\\gamma$, $\\mathcal{S}$ is an equal-rate scaling operator with the rate of $\\gamma$. Hence, the transformation can be rewritten as $\\mathbf Z_t = \\mathcal G (\\mathbf Z) = \\mathcal {S} \\mathbf Z = \\gamma \\mathbf I \\mathbf Z = \\gamma \\mathbf Z$. By the equal-rate transformation, we generate transformed representations while preserving their content, e.g., scaling up an image by the rate of 2, will not change the content we see. \n\n%It encourages the coefficient matrix $\\mathbf S$ to learn stable similarity metrics under multi-scale representations. This learning paradigm also benefits \n\n\\textbf{2) Generating Multi-scale Coefficient Matrix.}\n\\noindent\n\\\n\nSo far, we have generated the transformations of $\\mathbf Z$. In step $2$, we propose a constraint term to encourage the representation to be close between the original and transformed scale. Furthermore, we introduce a learnable projection operator $\\mathbf P$ to uncover the attack-specific invariant information, thus benefiting the distinction of few-shot traffic threats.\n\n%introduce a learnable projection operator $\\mathbf P$  as a metric matrix containing multiple information of $\\mathbf Z$ by spanning a latent space. To this end, we propose a constraint term to enhance the similarity learning of multiple representations in latent space as follows:\n\\begin{equation}\n f_1(\\mathbf{P})=\\left\\|\\mathbf{P}^T \\mathbf{Z}_o-\\mathbf{P}^T \\mathbf{Z}_t\\right\\|_F^2,\n\\end{equation}\n\nThe projection operator $\\mathbf P$ can be considered as a metric matrix across different-scale representation, also benefiting the connectivity of same-attack traffic samples under the transformation. Then we formally propose an optimization problem as follows:\n\n%encourages the connectivity of same-attack traffic samples under the transformation, thus enhancing the learning stability.\n\n%provides a learnable latent space for few-shot learning and\n\n\\begin{equation}\n\\begin{aligned}\\label{eq: Q_2}\n&\\min f_1(\\mathbf{P})+\\eta \\|\\mathbf{Q}\\|_F,\\\\\n&\\text { s.t. } \\mathbf{P} \\mathbf{Z}=\\mathbf{P} \\mathcal G (\\mathbf{Z}) \\mathbf{Q},\n\\end{aligned}\n\\end{equation}\n%where $f_1(\\mathbf{P})$ is the self-supervised term.\n{The above formulation Eq.~\\ref{eq: Q_2} corresponds to design principle 1 and is motivated by the empirical studies in Fig.~\\ref{fig:few-shot empirical} (a)(b), aiming to distinguish the attack-specific information in the latent space.}\n\n\\textbf{3) Disentangling the Learned Representations.}\n\\noindent\n\\\n\n%So far, we can derive $\\mathbf S$ by solving Eq.~\\ref{MSF_1}. However, \n%Considering the representation entangled problem in traffic features, \nFinally, we introduce a disentangle regularization term to highlight the attack-specific information, thus mitigating the representation entangled problem in few-shot traffic samples. \n\\begin{equation}\n f_2(\\mathbf{P})=- \\sum_j\\left\\|\\mathbf{P}^T \\mathbf{Z}_o^{+j}-\\mathbf{P}^T \\mathbf{Z}_t^{-j}\\right\\|_F^2,\n\\label{reg}\n\\end{equation}\nwhere $\\mathbf{Z}_o^{+j} \\in \\mathbb{R}^{(L-1) \\times N}$ repeats the $j$-th row of $\\mathbf{Z}_o$ and $\\mathbf{Z}_t^{-j} \\in \\mathbb{R}^{(L-1) \\times N}$ denotes the transformation matrix by removing the $j$-th row. {The above disentanglement regularization term Eq.~\\ref{reg} corresponds to design principle 2 and is motivated by the empirical studies in Fig.~\\ref{fig:few-shot empirical} (c)(d), aiming to highlight the attack-specific information.}\n\n{Combining the above key formulations Eq.~\\ref{eq: Q_2} and Eq.~\\ref{reg}}, we can rewrite the optimization problem as:\n\\begin{equation}\n\\begin{aligned}\n\\label{MSF_2}\n&\\min f(\\mathbf{P})+\\eta\\|\\tilde{\\mathbf{Q}}\\|_F, \\\\\n&\\text { s.t. } \\mathbf{P} \\mathbf{Z}=\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}, \\tilde{\\mathbf{Q}}=\\mathbf{Q},\n\\end{aligned}\n\\end{equation}\nwhere $f(\\mathbf{P})=\\frac{\\alpha}{2}(f_1(\\mathbf{P})+f_2(\\mathbf{P}))$, $\\alpha$ and $\\eta$ are trade-off hyperparameters, $\\tilde{Q}$ is a auxiliary variable, and $\\gamma$ is scaling rate hyperparameter.\n\n\\iffalse\n\\begin{equation}\n f(\\mathbf{P})=\\frac{\\alpha}{2}\\left\\|\\mathbf{P}^T \\mathbf{Z}_o-\\mathbf{P}^T \\mathbf{Z}_t\\right\\|_F^2-\\frac{\\alpha}{2} \\sum_j\\left\\|\\mathbf{P}^T \\mathbf{Z}_o^{+j}-\\mathbf{P}^T \\mathbf{Z}_t^{-j}\\right\\|_F^2,\n\\end{equation}\n\\fi\n%where $\\mathbf{Z}_o^{+j} \\in \\mathbb{R}^{D \\times(n-1)}$ repeats the $j$-th column of $\\mathbf{Z}_o$ and $\\mathbf{Z}_t^{-j} \\in \\mathbb{R}^{D \\times(n-1)}$ denotes the transformation matrix by removing the $j$-th column. The introduction of projection operator $\\mathbf P$ can not only better learn the information-condensed node embedding, but more importantly, it can encourage the connectivity of same-community nodes under the multi-scale transformation ($\\mathbf Z_o$ and $\\mathbf Z_t$), so that the MSSC module can mitigate the embedding scales sensitivity problem, thus enhancing the subspace clustering stability.\n\n\n%The scale of embedding vectors has been widely concerned, not only in graph clustering. As for single-view clustering, the normalizations are used to force vectors to scale to a uniform scale, thus improve the clustering effect. In multiple views, scale is even more important, because the multiple views are often at different scales, so a scaling factor is needed to unify them. In our proposed MSSC module, on the contrary, we utilize the scaling operator to derive multi-scale node embedding, and designed a self-supervised learning to encourage the connectivity of same-community node embedding under multi-scale.\n\n%From the objective function defined in Eq.~\\ref{MSF_1}, the proposed MSSC module expects to search for a low-dimensional embedding space (projected by $\\mathbf P$), where the invariant and common features under multi-scale can be learned for more stable subspace clustering.\n\n%when $s_1=s_2=...=s_D=\\gamma$, $\\operatorname{scale}$ is a scaling operator with equal rate $\\gamma$. And $\\mathcal G$ is a transformation function with above operator, then $\\mathbf Z_t = \\mathcal G (\\mathbf Z) = \\operatorname {scale} (\\gamma, \\gamma, ..., \\gamma) \\mathbf Z = \\gamma \\mathbf I \\mathbf Z = \\gamma \\mathbf Z$\n\nSince the optimization problem in Eq.~\\ref{MSF_2} is not convex with the unknowns $\\{\\mathbf{P}, \\mathbf{Q}\\}$, we solve $\\mathbf Q$ by iteratively updating variables while fixing another. We propose the following alternating solution to derive the coefficient matrix $\\mathbf Q$.\n\n%\\noindent\\quad \\textbf{\\large{Optimization}}\n{\\textbf{Alternating Solution for MFL}}\n\\noindent\n\\\n\\iffalse\nBy introducing the equal-rate scaling operator $\\mathcal G (\\mathbf{Z})=\\gamma \\mathbf Z$ and the auxiliary variable $\\tilde{Q}$, the problem can be converted into\n\\begin{equation}\n\\begin{aligned}\n&\\min f(\\mathbf{P})+\\eta\\|\\tilde{\\mathbf{Q}}\\|_F, \\\\\n&\\text { s.t. } \\mathbf{P} \\mathbf{Z}=\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}, \\tilde{\\mathbf{Q}}=\\mathbf{Q},\n\\end{aligned}\n\\end{equation}\nwhere $\\alpha$ and $\\eta$ are trade-off parameters.\n\\fi\n\nWe solve Eq.~\\ref{MSF_2} by converting the original problem to the augmented Lagrange minimizing problem, as Eq.~\\ref{MSF_2} involving a multi-objective optimization:\n\\begin{equation}\n\\begin{aligned}\n&\\mathcal{L}=\\frac{\\alpha}{2}\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}\\right\\|_F^2\\\\\n&-\\frac{\\alpha}{2} \\sum_j\\left\\|\\mathbf{P} \\mathbf{Z}^{+j}-\\gamma \\mathbf{P} \\mathbf{Z}^{-j}\\right\\|_F^2+\\eta\\|\\tilde{\\mathbf{Q}}\\|_F \\\\\n&+<\\Phi_1, \\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}+<\\Phi_2, \\mathbf{Q}-\\tilde{\\mathbf{Q}}> \\\\\n&+\\frac{\\mu}{2}\\left(\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}\\right\\|_F^2+\\|\\mathbf{Q}-\\tilde{\\mathbf{Q}}\\|_F^2\\right),\n\\end{aligned}\n\\end{equation}\nwhere $\\Phi_1$ and $\\Phi_2$ are the Lagrange multipliers and the penalty parameter $\\mu>0$.\n\nThen, the alternating solving and updating of each variable can be involved in the following four steps:\n\n\\textbf{1) Update $\\mathbf P$ by fixing $\\mathbf{Q}$ and $\\Phi_1$.}\n\\noindent\n\\\n\n% \\begin{equation}\n% \\begin{aligned}\n% \\mathbf{P}^* &=\\arg \\min _{\\mathbf{P}} \\frac{\\alpha}{2}\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}\\right\\|_F^2-\\frac{\\alpha}{2} \\sum_j\\left\\|\\mathbf{P} \\mathbf{Z}^{+j}-\\gamma \\mathbf{P} \\mathbf{Z}^{-j}\\right\\|_F^2 \\\\\n% &+\\frac{\\mu}{2}\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}+\\frac{\\Phi_1}{\\mu}\\right\\|_F^2 .\n% \\end{aligned}\n% \\end{equation}\n\n\\begin{equation}\n    \\scalebox{0.9}{$\n        \\begin{aligned}\n            \\mathbf{P}^* &= \\arg \\min _{\\mathbf{P}} \\frac{\\alpha}{2}\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}\\right\\|_F^2\n            - \\frac{\\alpha}{2} \\sum_j\\left\\|\\mathbf{P} \\mathbf{Z}^{+j}-\\gamma \\mathbf{P} \\mathbf{Z}^{-j}\\right\\|_F^2 \\\\\n            & + \\frac{\\mu}{2}\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}+\\frac{\\Phi_1}{\\mu}\\right\\|_F^2\n        \\end{aligned}\n    $}\n\\end{equation}\n\n\n\nThe closed-form solution to the problem in Eq.\n(27) can be given as follows:\n\\begin{equation}\\label{P}\n\\mathbf{P}^*= -\\Phi_1 \\Delta_3^T (\\alpha \\Delta_1 \\Delta_1^T-\\lambda \\sum_j \\Delta_2 \\Delta_2^T+\\mu \\Delta_3 \\Delta_3^T+\\mathbf{I})^{-1}  {, }\n\\end{equation}\nwhere $\\Delta_1=(1-\\gamma)\\mathbf{Z}, \\Delta_2=\\mathbf{Z}^{+j}-\\mathbf{Z}^{-j}, \\Delta_3=\\mathbf{Z}-\\gamma \\mathbf{Z} \\mathbf{Q}$, and $\\mathbf{I}$ is an identity matrix with proper size.\n\n\\textbf{2) Update $\\mathbf{Q}$ by fixing $\\mathbf{P}, \\tilde{\\mathbf{Q}}$ and $\\Phi_1, \\Phi_2$.}\n\\noindent\n\\\n\nThe optimization problem is shown as:\n\\begin{equation}\n\\begin{aligned}\\label{Q}\n\\mathbf{Q}^*&=\\arg \\min _{\\mathbf{Q}} \\frac{\\mu}{2}\\left\\|\\mathbf{P}^T \\mathbf {Z}-\\gamma \\mathbf{P}^T \\mathbf{Z} \\mathbf{Q}+\\frac{\\Phi_1}{\\mu}\\right\\|_F^2\\\\\n&+\\frac{\\mu}{2}\\left\\|\\mathbf{Q}-\\tilde{\\mathbf{Q}}+\\frac{\\Phi_2}{\\mu}\\right\\|_F^2 .\n\\end{aligned}\n\\end{equation}\nThe closed-form solution to the problem above is:\n\\begin{equation}\n\\mathbf{Q}^*=\\left(\\mu \\gamma ^2 \\mathbf{P}^T \\mathbf{Z Z}^T \\mathbf{P}+\\mu \\mathbf{I}\\right)^{-1}\\left(\\mu \\gamma \\mathbf{Z}^T \\mathbf{P} \\Delta_3+\\mu \\tilde{\\mathbf{Q}}-\\Phi_2\\right),\n\\end{equation}\nwhere $\\Delta_3=\\mathrm{P}^T \\mathbf{Z}+\\frac{\\Phi_1}{\\mu}$.\n\n\\textbf{3) Update the auxiliary variable $\\tilde{\\mathbf Q}$.}\n\\noindent\n\\\n\nIt can be solved via the singular value thresholding method,\n\\begin{equation}\n\\tilde{\\mathbf{Q}}^*=\\arg \\min _{\\tilde{\\mathbf{Q}}}\\|\\tilde{\\mathbf{Q}}\\|_*+\\frac{\\mu}{2}\\left\\|\\mathbf{Q}-\\tilde{\\mathbf{Q}}+\\frac{\\Phi_2}{\\mu}\\right\\|_F^2,\n\\end{equation}\nand the closed-form solution of $\\tilde{\\mathbf{Q}}^*$ is\n\\begin{equation}\n\\begin{aligned}\\label{Qhat}\n\\tilde{\\mathbf{Q}}^* &=\\mathcal{S}_{1 / \\mu}\\left(\\mathbf{Q}+\\Phi_2 / \\mu\\right) \\\\\n&=\\mathbf{U}\\left\\{\\operatorname{sign}\\left(\\Sigma_{i i}\\right) \\max \\left(\\left|\\Sigma_{i i}-1 / \\mu\\right|, 0\\right)\\right\\} \\mathbf{V}^T,\n\\end{aligned}\n\\end{equation}\nwhere $\\mathcal{S}$ is the singular value thresholding operator, $\\operatorname{sign}(\\cdot)$ is the indicator function, and $\\tilde{\\mathbf Q}+\\Phi_2 / \\mu=\\mathbf{U \\Sigma } \\mathbf{V}^T$ is the singular value decomposition of $\\tilde{\\mathbf Q}+\\Phi_2 / \\mu$.\n\n\\textbf{4) Update the multipliers $\\Phi_1, \\Phi_2, \\mu$.}\n\\noindent\n\\\n\n\\begin{equation}\n\\begin{aligned}\\label{other}\n\\Phi_1 &=\\Phi_1+\\mu\\left(\\mathbf{P}^T \\mathbf Z-\\gamma \\mathbf{P}^T \\mathbf Z \\mathbf{Q}\\right), \\\\\n\\Phi_2 &=\\Phi_2+\\mu(\\mathbf{Q}-\\tilde{\\mathbf{Q}}), \\\\\n\\mu &=\\min \\left(1.01 \\mu, \\mu_{\\max }\\right).\n\\end{aligned}\n\\end{equation}\n\nWe can obtain the coefficient matrix $\\mathbf Q$ following the above updating rules. The pseudo-code of MFL is proposed in Algorithm~\\ref{algMFL}.\n\\vspace{-10pt}\n\\begin{algorithm}\n  \\footnotesize\n  %\\textsl{}\\setstretch{1.8}\n  \\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n  \\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n  \\caption{Alternating Algorithm for MFL}\n  \\label{algMFL}\n  \\begin{algorithmic}[1]\n \\REQUIRE  $\\mathbf{Z}_o= \\mathbf Z$, scaling rate $\\gamma$, $\\alpha$ and $\\eta$.\n \\ENSURE  Affinity matrix $\\mathbf S$ for later few-shot intrusion detection.\n\\STATE \\textbf{Initialization}: $\\mathbf Z_t = \\gamma \\mathbf Z$ by $\\gamma$-rate scaling transform on $\\mathbf Z_o$; $\\mathbf Q=\\tilde{\\mathbf Q}=\\mathbf 0$; and $\\Phi_1=\\Phi_2=0 ; \\mu=0.1, \\mu_{\\max }=10^7$.\n\\WHILE {Convergence conditions are not satisfied}\n\\STATE \\textbf{Update}: $\\mathbf{P}$ via Eq.~\\ref{P}.\n\\STATE \\textbf{Update}: $\\mathbf{Q}$ via Eq.~\\ref{Q}.\n\\STATE \\textbf{Update}: $\\tilde{\\mathbf{Q}}$ via Eq.~\\ref{Qhat}.\n\\STATE \\textbf{Update}: $\\Phi_1, \\Phi_2$ and $\\mu$ via Eq.~\\ref{other}.\n\\STATE \\textbf{Check} the convergence conditions:\n$$\n\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}\\right\\|_{\\infty}<10^{-6} \\text {. }\n$$\n\\ENDWHILE\n%\\STATE \\textbf{Obtain} the affinity matrix $\\mathbf{S}=\\left(|\\mathbf{Q}|+|\\mathbf{Q}|^T\\right) / 2$.\n\\STATE \\textbf{Obtain} the coefficient matrix $\\mathbf Q$ \n\\end{algorithmic}\n\\end{algorithm}\n\\vspace{-10pt}\nSo far we have obtained coefficient matrices $\\mathbf H$ and $\\mathbf Q$ by parallel computing of Eq.~\\ref{eq: Q_1} and Algorithm~\\ref{algMFL}. We can fuse two coefficient matrices by introducing a trade-off hyperparameter $\\epsilon$:\n\\begin{equation}\n    \\mathbf{Q^*} = \\mathbf{H} + \\epsilon * \\mathbf{Q}\n\\end{equation}\nwhere $\\mathbf Q^*$ contains fine-grained multi-scale information to help few-shot traffic threats distinguishable.\n\nFinally, we can generate the coefficient matrix of MFL by $\\mathbf{S^*}=\\left(|\\mathbf{Q^*}|+|\\mathbf{Q^*}|^T\\right) / 2$. Given the noise information and outliers in the obtained $\\mathbf S^*$, in practical implementations, we further conduct SVD decomposition of $\\mathbf S^*$ to filter noisy information and generate a normalized matrix $\\mathbf S$. The detailed implementation pseudo-code of generating $\\mathbf S$ is available in the Appendix~$15$. \n\n%In practical implementations, we can conduct SVD decomposition of $\\mathbf S^*$ to filter noisy information and generate a normalized matrix $\\mathbf S$, due to the obtained matrix $\\mathbf S$ still having noise and outliers. The detailed implementation pseudo-code of generating $\\mathbf S$ is available in the Appendix. \n\n\\textbf{DIDS-MFL Loss}\n\\noindent\n\\\n\nThe DIDS-MFL loss consists of two components, a multi-scale few-shot learning term and a regularization term. The former uses a cross-entropy loss, aiming to match the prediction to the query set via the generated multi-scale coefficient matrix $\\mathbf S$. {In the few-shot intrusion detection task, $\\mathbf S$ is derived from the representations of support and query samples.} The latter enforces the representations $\\mathbf Z$ to be close to the original-scale representations in latent space. Finally, we introduce a trade-off hyperparameter $\\beta$ to control the regularization intensity. The specific DIDS-MFL loss is as follows:\n\\begin{equation}\n\\begin{aligned} \n\\label{MFL LOSS}\n\\mathcal{L}=\\sum_{i=1}^{\\mathcal N}\\sum_{ j=1}^{\\mathcal Q}Y_{i,j}\\log(\\mathrm P_{i, j})+ \\beta \\|\\mathbf Z \\mathbf H- \\mathbf Z\\|\\\n\\end{aligned}\n\\end{equation}\nwhere $\\mathcal N$ is the category number of attacks, $\\mathcal Q$ is sample number of query set in each category, $Y_{i,j}$ and $\\operatorname{P}_{i,j}$ are ground-truth and prediction, respectively, {$\\mathrm P_{ij}$ is query sample's mean similarity score derived from the coefficient matrix $\\mathbf S$}, $\\beta$ is a trade-off hyperparameter. In Eq.~\\ref{MFL LOSS}, the first term discovers the multi-scale information of $\\mathbf Z$, while \nthe second term preserves the information under the original scale via the coefficient matrix $\\mathbf H$. We can fine-tune $\\beta$ to control these two terms.\n\n\\iffalse\nWe follow the existing $N$-way $K$-shot few-shot learning setting\\cite{lin2022revisiting} {\\color{red}[cite]} . In few-shot learning, a training set with diverse categories is used. For each category, a small set of K samples is chosen. From these, N categories are picked randomly to create a support set with N*K samples. The model learns from this set to classify a new query set of samples from the same N categories.\n\\begin{equation}\n\\begin{aligned}   \\mathcal{L}=\\sum_{i=1}^{J}(L_{\\mathrm{att},i})\\log(P_{\\mathrm{query}, i})+ \\|\\mathbf{T}_{se} - \\mathbf{T}\\|\\\n\\end{aligned}\n\\end{equation}\nWhere J denotes the total number of samples in the query set. $\\mathcal(L_{\\mathrm{att},i})$ is a set consisting of the true query labels. $\\mathcal(P_{\\mathrm{query}, i})$ is also a set that includes the probabilities of the query sets for N categories. $\\mathbf{T}$ and $\\mathbf{T}_{se}$ represent the concatenated vectors output by MGD from the support set and query set, respectively, and the vectors output by the SE model after processing them.\n\n\\fi\n%\\fi\n\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n",
                "subsection 5.1": {
                    "name": "Experimental Settings",
                    "content": "\n\\textbf{Datasets\\iffalse\\footnote{More  details are available in Table \\ref{DATASETTABLE} of Appendix \\ref{sec-datasets}.}\\fi:} We conduct experiments on five popular datasets that involve massive network traffic over the internet of things (IoT). We give detailed descriptions as follows:\n\\begin{itemize}\n  \\item CIC-ToN-IoT : This dataset is generated from the existing ToN-IoT dataset by a network traffic tool CICFlowMeter, where TON-IoT is a well-known database for intrusion detection collected from Telemetry datasets of IoT services.  This dataset consists of $5,351,760$ flows, with $53.00\\%$ attack samples and $47.00\\%$ benign samples.\n\n  % \\footnote{https://github.com/ahlashkari/CICFlowMeter} \n    % These datasets have been labeled with binary- and multi-class categories which include different types of IoT data, such as operating system logs, telemetry data of IoT/IIoT services, as well as IoT network traffic collected from a medium scale network at the Cyber Range and IoT Labs at the UNSW Canberra (Australia). In our experiment, only the network traffic component of the dataset is used. This will accommodate a reliable evaluation of our models. The dataset consists of 5,351,760 data where 2,836,524 (53.00\\%) are attacks and 2,515,236 (47.00\\%) are benign samples.\n  \\item CIC-BoT-IoT: It is generated from the existing BoT-IoT dataset by CICFlowMeter. This dataset consists of $6,714,300$ traffic flows, with $98.82\\%$ attack samples and $1.18\\%$ benign samples.\n  % The authors simulated IoT services such as water stations, by using the Node-red tool\\footnote{https://nodered.org/} and generated the corresponding IoT traffic. The dataset contains 13,428,602 records in total, containing 13,339,356 (99.34\\%) attack samples and 89,246 (0.66\\%) benign samples. The attack samples are made up of four attack scenarios inherited from the parent dataset, i.e., DDoS, DoS, reconnaissance, and theft.\n  \\item EdgeIIoT: It is collected from an IoT/IIoT system that contains mobile devices and sensors. This dataset includes $1,692,555$ flows, with $21.15\\%$ attack samples and $78.85\\%$ benign samples.\n\\item NF-UNSW-NB15-v2: It is NetFlow-based and generated from the UNSW-NB15 dataset, which has been expanded with additional NetFlow features and labeled with respective attack categories. This dataset includes $2,390,275$ flows, with $3.98\\%$ attack samples and $96.02\\%$ benign samples.\n\\item NF-CSE-CIC-IDS2018-v2: It is a  NetFlow-based dataset generated from the original pcap files of CSE-CIC-IDS2018 dataset. This dataset includes $18,893,708$ flows, with $11.95\\%$ attack samples and $88.05\\%$ benign samples.\n  % The IoT data are generated from various IoT devices (more than 10 types) such as Low-cost digital sensors for sensing temperature and humidity, Ultrasonic sensors, Water level detection sensors, pH Sensor Meters, etc. Furthermore, this dataset contains fourteen attacks related to IoT and IIoT connectivity protocols, which are categorized into five threats, including, DoS/DDoS attacks, Information gathering, Man in middle attacks, Injection attacks, and Malware attacks. In addition, this dataset extract features obtained from different sources, including alerts, system resources, logs, and network traffic, and proposes new 61 features with high correlations from 1176 found features, but in this paper, only network traffic is used.\n\\end{itemize}\n%\\par As the large scale of datasets, we divide each dataset into ten smaller datasets based on the time span for the convenience of the experiment. All the results obtained are averaged over 5 runs. Every result for each dataset in the table is the average of ten sections of the whole dataset. \n% The results for the total dataset are from the mean of the ten datasets.\n%\\par For a fair comparison, we divide each dataset into 3 sections we use about 75\\% of the data for training, 15\\% of data for testing, and the remaining 10\\% for validation. For a multi-layer network, we mark each device with the layer information referred to in the original paper of the dataset.\\\\\n\\textbf{Configurations\\iffalse\\footnote{More details are available in Appendix \\ref{sec:Implementation Details}.}\\fi:} \n% For a fair comparison, we divide each dataset into 3 sections we use about 75\\% of the data for training, 15\\% of data for testing, and the remaining 10\\% for validation. For a multi-layer network, we label each device with the layer information given in the original dataset paper.\\\\\nAll experiments and timings are conducted on a machine with Intel Xeon Gold 6330@ $2.00$GHz, RTX$3090$ GPU, and $24$G memory. We use the Adam optimizer with a learning rate of $0.01$, the learning rate scheduler reducing rate as $0.9$, with weight decay being $1e^{-5}$. We train all the models with 500 epochs. \\\\\n% \\textbf{Hyper-parameter.} The detail of the hyper-parameters is shown in Appendix \\ref{A-B}.\\\\\n%As the large scale of datasets, we divide each dataset into ten smaller datasets based on the time span for the convenience of the experiment. All the results obtained are averaged over 5 runs. Each result for each dataset in the table is the average of ten sections of the whole dataset.\n\\textbf{Baselines\\iffalse\\footnote{More  details are available in Appendix \\ref{A:Baselines}.}\\fi:} \nTo evaluate the performance of the proposed DIDS, we select $10$ deep learning based-models as baselines, including $3$ sequence models (i.e., MLP \\cite{roopak2019deep}, MStream \\cite{Bhatia_2021}, LUCID \\cite{doriguzzi2020lucid}), $4$ static GCN models (i.e., GAT \\cite{velivckovic2017graph} and E-GraphSAGE \\cite{hamilton2017representation}, SSDCM \\cite{mitra2021semi}, DMGI \\cite{park2020unsupervised}), where SSDCM and DMGI are designed for static multi-layer graphs,  $4$ dynamic GCN models (i.e., TGN \\cite{rossi2020temporal}, EULER \\cite{kingeuler}, AnomRank \\cite{2019Fast}, DynAnom \\cite{2019Fast}). Additionally, we choose $3$ rule-based baselines to compare with the proposed DIDS, (i.e., ML \\cite{sarhan2021netflow}, AdaBoost \\cite{lalouani2021robust}, and Logistic Regression).\n\n\n\\noindent\n\\textbf{Metrics:} We follow the previous works \\cite{sarhan2022evaluating} to evaluate the performances of all baselines by two commonly used metrics in intrusion detection including F1-score (F1) and ROC-AUC score (AUC).\\\\\n\\vspace{-20pt}\n\n\n\n% \\begin{figure*}[htb]\n%     \\centering\n%     \\subfigbottomskip=-3pt %两行子图之间的行间距\n%     \\subfigcapskip=-7pt %设置子图与子标题之间的距离\n%     \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n%     \\includegraphics[width=0.90\\linewidth,height=0.4cm]{picture/legend1.png}\n    \n%     \\subfigure[EdgeIIoT]{\n%     % \\includegraphics[width=0.53\\linewidth]{picture/EdgeIIoT.png}\n%     % }\n%     \\includegraphics[width=0.5\\linewidth]{picture/EdgeIIoT.pdf}\n%     }\n%     % \\subfigure[CIC-ToN-IoT]{\n%     % \\includegraphics[width=0.44\\linewidth]{picture/CIC-ToN-IoT.png}\n%     % }\n%     \\subfigure[CIC-ToN-IoT]{\n%     \\includegraphics[width=0.42\\linewidth]{picture/CIC-ToN-IoT.pdf}\n%     }\n%     % \\subfigure[CIC-BoT-IoT]{\n%     % \\includegraphics[width=0.24\\linewidth]{picture/CIC-BoT-IoT.png}\n%     % }\n    \n%     \\subfigure[CIC-BoT-IoT]{\n%     \\includegraphics[width=0.23\\linewidth]{picture/CIC-BoT-IoT.pdf}\n%     }\n%     % \\subfigure[NF-UNSW-NB15-v2]{\n%     % \\includegraphics[width=0.315\\linewidth]{picture/NF-UNSW-NB15-v2.png}\n%     % }\n%     \\subfigure[NF-UNSW-NB15-v2]{\n%     \\includegraphics[width=0.30\\linewidth]{picture/NF-UNSW-NB15-v2.pdf}\n%     }\n%     % \\subfigure[NF-CSE-CIC-IDS2018-v2]{\n%     % \\includegraphics[width=0.40\\linewidth]{picture/NF-CSE-CIC-IDS20181.png}\n%     % }\n%     \\subfigure[NF-CSE-CIC-IDS2018-v2]{\n%     \\includegraphics[width=0.38\\linewidth]{picture/NF-CSE-CIC-IDS20181.pdf}\n%     }\n%     \\caption{Comparisons of multi-classification. Here $\\dagger$ indicates that the results are directly copied from the previous works.}\n%     \\label{classfication}\n% \\end{figure*}\n\n% \\begin{figure*}[htb]\n%     \\centering\n%     \\subfigbottomskip=3pt %两行子图之间的行间距\n%     \\subfigcapskip=7pt %设置子图与子标题之间的距离\n%     \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n    \n    \n%     \\subfigure[EdgeIIoT]{\n%     % \\includegraphics[width=0.53\\linewidth]{picture/}\n%     % }\n%     \\includegraphics[width=0.47\\linewidth,height=3.5cm]{picture/page2/pic2/Q1.pdf}\n%     }\n%     % \\subfigure[CIC-ToN-IoT]{\n%     % \\includegraphics[width=0.44\\linewidth]{picture/CIC-ToN-IoT.png}\n%     % }\n%     \\subfigure[CIC-ToN-IoT]{\n%     \\includegraphics[width=0.47\\linewidth,height=3.5cm]{picture/page2/pic2/Q2.pdf}\n%     }\n%     % \\subfigure[CIC-BoT-IoT]{\n%     % \\includegraphics[width=0.24\\linewidth]{picture/CIC-BoT-IoT.png}\n%     % }\n    \n%     \\subfigure[CIC-BoT-IoT]{\n%     \\includegraphics[width=0.3\\linewidth,height=3cm]{picture/page2/pic2/Q3.pdf}\n%     }\n%     % \\subfigure[NF-UNSW-NB15-v2]{\n%     % \\includegraphics[width=0.315\\linewidth]{picture/NF-UNSW-NB15-v2.png}\n%     % }\n%     \\subfigure[NF-UNSW-NB15-v2]{\n%     \\includegraphics[width=0.30\\linewidth,height=3cm]{picture/page2/pic2/Q4.pdf}\n%     }\n%     % \\subfigure[NF-CSE-CIC-IDS2018-v2]{\n%     % \\includegraphics[width=0.40\\linewidth]{picture/NF-CSE-CIC-IDS20181.png}\n%     % }\n%     \\subfigure[NF-CSE-CIC-IDS2018-v2]{\n%     \\includegraphics[width=0.3\\linewidth,height=3cm]{picture/page2/pic2/Q5.pdf}\n%     }\n%     \\includegraphics[width=0.90\\linewidth,height=0.5cm]{picture/page2/pic2/comment.pdf}\n%     \\caption{Comparisons of multi-classification. Here $\\dagger$ indicates that the results are directly copied from the previous works.}\n%     \\label{classfication}\n% \\end{figure*}\n\n\n\n%\\hspace{-10pt}\n"
                },
                "subsection 5.2": {
                    "name": "Main Results",
                    "content": "\n\n",
                    "subsubsection 5.2.1": {
                        "name": "Comparisons of binary classification",
                        "content": "\n% In this section, we use the full dataset, except for the CIC-BoT-IoT dataset, where we considered 50\\% of the original size, due to the large size of the original dataset. In regards to training and evaluation data split, most of the baselines use 75\\% of the flow records of each dataset were selected for training and 15\\% were reserved for testing, and the remaining 10\\% for validation, others use 80\\% of the dataset for training and 20\\% for testing.\nUnder this setting, we classify a traffic flow as an attack or a benign one. We categorize the baselines into three groups, including dynamic GCNs at the top of Table \\ref{E-1}, static GCNs at the middle of the table, and another three baselines at the bottom. It should be noted that AnomRank and DynAnom are two popular baselines for anomaly detection. We run our experiment $5$ times and report the mean and variance values. The comparison results in Table \\ref{E-1} show that our DIDS consistently performs the best among all baselines over the five benchmarks, which shows the superiority of our method for intrusion detection. Specifically, compared to the E-GraphSAGE, the previous state-of-art GCN-based approach, our method achieves a $4.80\\%$ higher F1-score over the CIC-BoT-IoT dataset. Our DIDS outperforms AnomRank, the previous state-of-the-art method for anomaly detection on F1, by $15.27$ points over the EdgeIIoT dataset. We attribute the above results to the gains of our statistical disentanglement, representational disentanglement and dynamic graph diffusion method.\n\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "Comparisons of multi-classification",
                        "content": "\nWe compare the performance of our method to four baselines in declaring the specific attack type. These baselines include E-GraphSAGE \\cite{lo2022graphsage}, TGN \\cite{rossi2020temporal}, ML \\cite{wang2016machine}, and AdaBoost \\cite{schapire2013explaining}, as they are representative of different types of intrusion detection models and have been widely used in previous studies. Figure \\ref{classfication} shows that the proposed DIDS consistently achieves the best results among all others over five datasets. For example, ours yields higher classification accuracy of up to $25$ points compared to the baseline TGN for detecting  Injection attacks. The existing graph-based methods, including E-GraphSAGE, ML, and AdaBoost, perform inconsistently in the identification of complex attacks (e.g., MITM, Uploading, and XSS). We also observe that some attacks that are not easily detected by the baseline approaches, can be identified by the proposed DIDS with high F1 scores. For example, E-GraphSAGE only achieves $18.34\\%$ and $30.7\\%$ F1 scores on CIC-ToN-IoT for MITM and Backdoor attacks, respectively, while our DIDS is able to obtain higher than $23\\%$ F1 score for each attack. These results further show the superiority of the proposed DIDS. We also find that the average scores of our method on CIC-BoT-loT and CIC-ToN-loT are lower than the ones on the other three datasets. The underlying reason is the unbalanced attack distributions in the training set, where the dominant type may mislead the classifications. Such a finding aligns with previous work in the field of computer vision \\cite{fisher2017operational}. Nevertheless, the proposed DIDS is still the best under such distributions. We leave this interesting observation as our future work.\n\n\n\n% \\begin{table*}[t]\n% \\centering\n%   \\setlength{\\belowcaptionskip}{2pt}\n%   \\caption{Comparisons of few-shot learning classification on five datasets. The results are the average scores of ten-time repetitions.}\n% %\\caption{Comparisons of traffic predictions on the three benchmarks, including CIC-ToN-IoT, CIC-BoT-IoT, and EdgeIIoT. We repeat our experiments five times and report both the mean and variance values of the F1-Score (\\%) and ROC-AUC score (\\%).}\n% \\label{tab:few-shot}\n% \\resizebox{\\linewidth}{!}\n% {\n% \\begin{tabular}{c|cc|cc|cc|cc|cc} \n% \\toprule\n% \\multirow{2}{*}{Methods} & \\multicolumn{2}{c|}{CIC-TON-IoT}   & \\multicolumn{2}{c|}{CIC-BoT-IoT}    & \\multicolumn{2}{c|}{EdgeIIoT}  &\\multicolumn{2}{c|}{NF-UNSW-NB15-v2} &\\multicolumn{2}{c}{NF-CSE-CIC-IDS2018-v2}                                   \\\\\n%      & F1          & NMI          & F1           & NMI               & F1            & NMI               & F1              & NMI               & F1                 & NMI     \\\\  \n% \\midrule \n% MBase \\cite{chen2021metabaseline}   & \n% {\\textcolor{red}$\\textbf{41.74}$$\\pm$\\scriptsize$\\textbf{3.22}$} & {\\textcolor{red}$\\textbf{35.81}$$\\pm$\\scriptsize$\\textbf{2.84}$} &\n% {\\textcolor{red}$\\textbf{38.52}$$\\pm$\\scriptsize$\\textbf{3.58}$} &\n% {\\textcolor{red}$\\textbf{27.89}$$\\pm$\\scriptsize$\\textbf{3.84}$} &\n% {\\textcolor{red}{$\\textbf{51.13}$$ \\pm $\\scriptsize$ \\textbf{2.31}$}}   & \n% {\\textcolor{red}{$\\textbf{55.61}$$ \\pm $\\scriptsize$ \\textbf{2.05}$}}   &\n% {\\textcolor{red}$\\textbf{41.74}$$\\pm$\\scriptsize$\\textbf{3.22}$} & {\\textcolor{red}$\\textbf{35.81}$$\\pm$\\scriptsize$\\textbf{2.84}$} &\n% {\\textcolor{red}$\\textbf{63.74}$$\\pm$\\scriptsize$\\textbf{4.14}$} &\n% {\\textcolor{red}$\\textbf{68.76}$$\\pm$\\scriptsize$\\textbf{3.16}$} & \n% MTL \\cite{sun2019metatransfer}         &\n% {\\textcolor{red}$\\textbf{30.79}$$\\pm$\\scriptsize$\\textbf{3.13}$}    &\n% {\\textcolor{red}$\\textbf{29.81}$$\\pm$\\scriptsize$\\textbf{3.58}$}  &\n% {\\textcolor{red}$\\textbf{37.80}$$\\pm$\\scriptsize$\\textbf{5.66}$}  &\n% {\\textcolor{red}$\\textbf{25.10}$$\\pm$\\scriptsize$\\textbf{2.69}$} &\n% {\\textcolor{red}{$\\textbf{52.64}$$ \\pm $\\scriptsize$ \\textbf{5.69}$}}   & \n% {\\textcolor{red}{$\\textbf{55.41}$$ \\pm $\\scriptsize$ \\textbf{4.16}$}}   &\n% {\\textcolor{red}{$\\textbf{44.18}$$ \\pm $\\scriptsize$ \\textbf{4.22}$}}   & \n% {\\textcolor{red}{$\\textbf{35.85}$$ \\pm $\\scriptsize$ \\textbf{2.79}$}}   &\n% {\\textcolor{red}{$\\textbf{45.38}$$ \\pm $\\scriptsize$ \\textbf{5.88}$}}   & \n% {\\textcolor{red}{$\\textbf{47.04}$$ \\pm $\\scriptsize$ \\textbf{4.82}$}}   \\\\ \n% TEG \\cite{kim2023task}             & \n% {\\textcolor{red}{$\\textbf{29.92}$$ \\pm $\\scriptsize$ \\textbf{2.17}$}}   & \n% {\\textcolor{red}{$\\textbf{22.64}$$ \\pm $\\scriptsize$ \\textbf{2.22}$}}   & \n% {\\textcolor{red}{$\\textbf{29.62}$$ \\pm $\\scriptsize$ \\textbf{3.74}$}}   & \n% {\\textcolor{red}{$\\textbf{25.71}$$ \\pm $\\scriptsize$ \\textbf{2.82}$}}   & \n% {\\textcolor{red}{$\\textbf{26.34}$$ \\pm $\\scriptsize$ \\textbf{2.45}$}}   & \n% {\\textcolor{red}{$\\textbf{27.58}$$ \\pm $\\scriptsize$ \\textbf{2.64}$}}   & \n% {\\textcolor{red}{$\\textbf{27.80}$$ \\pm $\\scriptsize$ \\textbf{2.14}$}}   & \n% {\\textcolor{red}{$\\textbf{21.44}$$ \\pm $\\scriptsize$ \\textbf{1.74}$}}   & \n% {\\textcolor{red}{$\\textbf{25.20}$$ \\pm $\\scriptsize$ \\textbf{3.17}$}}   & \n% {\\textcolor{red}{$\\textbf{23.48}$$ \\pm $\\scriptsize$ \\textbf{3.97}$}}   \\\\\n% % DynAnom \\cite{guo2022dynanom}        & $79.23$$\\pm$\\scriptsize$1.81$   & $75.22$$\\pm$\\scriptsize$0.92$  & $83.25$$\\pm$\\scriptsize$0.62$   & $79.04$$\\pm$\\scriptsize$0.84$        & $81.56$$\\pm$\\scriptsize$0.94$    & $83.94$$\\pm$\\scriptsize$0.36$        & $89.11$$\\pm$\\scriptsize$1.48$     & $85.25$$\\pm$\\scriptsize$0.64$        & $91.21$$\\pm$\\scriptsize$0.95$          & $88.79$$\\pm$\\scriptsize$0.54$ \\\\\n% \\midrule\n% % Anomal \\cite{Anomal-E}&-  & -  &  - &     -  & -   &    -    & $91.89\\ddagger$     &-       & $94.51\\ddagger$        &- \\\\\n% CLSA \\cite{wang2022contrastive}     & \n% {\\textcolor{red}{$\\textbf{47.28}$$ \\pm $\\scriptsize$ \\textbf{4.98}$}}   & \n% {\\textcolor{red}{$\\textbf{44.37}$$ \\pm $\\scriptsize$ \\textbf{5.86}$}}   & \n% {\\textcolor{red}{$\\textbf{52.19}$$ \\pm $\\scriptsize$ \\textbf{6.17}$}}   & \n% {\\textcolor{red}{$\\textbf{41.26}$$ \\pm $\\scriptsize$ \\textbf{4.97}$}}   & \n% {\\textcolor{red}{$\\textbf{57.48}$$ \\pm $\\scriptsize$ \\textbf{7.63}$}}   & \n% {\\textcolor{red}{$\\textbf{47.46}$$ \\pm $\\scriptsize$ \\textbf{6.49}$}}   & \n% $92.20$$\\pm$\\scriptsize$1.60$     &\n% $89.91$$\\pm$\\scriptsize$0.62$  &\n% \\underline{$96.08$$\\pm$\\scriptsize$0.24$}   \n% & $90.56$$\\pm$\\scriptsize$0.34$    \\\\\n% ESPT \\cite{rong2023espt}  & \n% {\\textcolor{red}{$\\textbf{46.76}$$ \\pm $\\scriptsize$ \\textbf{5.37}$}}   & \n% {\\textcolor{red}{$\\textbf{38.51}$$ \\pm $\\scriptsize$ \\textbf{3.29}$}}   &\n% $93.74$$\\pm$\\scriptsize$0.76$   & \n% $90.53$$\\pm$\\scriptsize$1.90$       &\n% {\\textcolor{red}{$\\textbf{61.28}$$ \\pm $\\scriptsize$ \\textbf{8.34}$}}   & \n% {\\textcolor{red}{$\\textbf{53.77}$$ \\pm $\\scriptsize$ \\textbf{6.91}$}}   &\n% \\underline{$94.10$$\\pm$\\scriptsize$0.33$}&\n% \\underline{$90.39$$\\pm$\\scriptsize$0.26$} & \n% $95.71$$\\pm$\\scriptsize$0.35$        & \n% $90.22$$\\pm$\\scriptsize$0.48$ \\\\\n \n% ICI \\cite{9446583}    & \n% {\\textcolor{red}{$\\textbf{55.67}$$ \\pm $\\scriptsize$ \\textbf{5.14}$}}   & \n% {\\textcolor{red}{$\\textbf{47.80}$$ \\pm $\\scriptsize$ \\textbf{3.77}$}}   & \n% {\\textcolor{red}{$\\textbf{71.32}$$ \\pm $\\scriptsize$ \\textbf{3.35}$}}   & \n% {\\textcolor{red}{$\\textbf{57.51}$$ \\pm $\\scriptsize$ \\textbf{4.50}$}}   & \n% {\\textcolor{red}{$\\textbf{49.74}$$ \\pm $\\scriptsize$ \\textbf{4.65}$}}   & \n% {\\textcolor{red}{$\\textbf{51.30}$$ \\pm $\\scriptsize$ \\mathbf{5.27}$}}   & \n% {\\textcolor{red}{$\\textbf{39.17}$$ \\pm $\\scriptsize$ \\mathbf{3.66}$}}   & \n% {\\textcolor{red}{$\\textbf{31.81}$$ \\pm $\\scriptsize$ \\textbf{2.21}$}}   & \n% {\\textcolor{red}{$\\textbf{76.53}$$ \\pm $\\scriptsize$ \\textbf{9.43}$}}   &    \n% {\\textcolor{red}{$\\textbf{79.00}$$ \\pm $\\scriptsize$ \\textbf{6.20}$}}  \\\\\n% KSCL \\cite{xu2021kshot}  & \n% {\\textcolor{red}{$\\textbf{36.19}$$ \\pm $\\scriptsize$ \\mathbf{2.66}$}}   & \n% {\\textcolor{red}{$\\textbf{38.17}$$ \\pm $\\scriptsize$ \\textbf{3.90}$}}   & \n% {\\textcolor{red}{$\\textbf{28.30}$$ \\pm $\\scriptsize$ \\mathbf{4.57}$}}   & \n% {\\textcolor{red}{$\\textbf{29.40}$$ \\pm $\\scriptsize$ \\textbf{4.80}$}}   & \n% {\\textcolor{red}{$\\textbf{34.87}$$ \\pm $\\scriptsize$ \\mathbf{2.14}$}}   & \n% {\\textcolor{red}{$\\textbf{30.63}$$ \\pm $\\scriptsize$ \\textbf{2.56}$}}   &\n% {\\textcolor{red}{$\\textbf{26.98}$$ \\pm $\\scriptsize$ \\mathbf{3.13}$}}   & \n% {\\textcolor{red}{$\\textbf{21.90}$$ \\pm $\\scriptsize$ \\textbf{1.98}$}}   &\n% $94.96$$\\pm$\\scriptsize$0.52$         &\n% $88.61$$\\pm$\\scriptsize$0.38$  \\\\ \n% \\midrule\n% BSNet \\cite{Li_2021}      & {\\textcolor{red}$\\textbf{41.50}$$\\pm$\\scriptsize$\\textbf{1.96}$}   & {\\textcolor{red}$\\textbf{38.13}$$\\pm$\\scriptsize$\\textbf{1.84}$}   & {\\textcolor{red}$\\textbf{42.00}$$\\pm$\\scriptsize$\\textbf{5.19}$}   & \n% {\\textcolor{red}$\\textbf{31.41}$$\\pm$\\scriptsize$\\textbf{4.71}$}   & \n% {\\textcolor{red}{$\\textbf{43.74}$$ \\pm $\\scriptsize$ \\textbf{4.20}$}}   & \n% {\\textcolor{red}{$\\textbf{49.68}$$ \\pm $\\scriptsize$ \\textbf{5.09}$}}   &    \n% {\\textcolor{red}$\\textbf{65.73}$$\\pm$\\scriptsize$\\textbf{2.65}$}         & \n% {\\textcolor{red}$\\textbf{58.96}$$\\pm$\\scriptsize$\\textbf{2.08}$}         & \n% {\\textcolor{red}$\\textbf{56.39}$$\\pm$\\scriptsize$\\textbf{5.75}$}      &\n% {\\textcolor{red}$\\textbf{58.56}$$\\pm$\\scriptsize$\\textbf{3.76}$}      \\\\\n% CMFSL \\cite{Xi2022FewShotLW} & \n% {\\textcolor{red}$\\textbf{53.48}$$\\pm$\\scriptsize$\\textbf{4.32}$}   & \n% {\\textcolor{red}$\\textbf{51.28}$$\\pm$\\scriptsize$\\textbf{3.70}$}  &\n% {\\textcolor{red}{$\\textbf{58.62}$$ \\pm $\\scriptsize$ \\textbf{3.58}$}}   & \n% {\\textcolor{red}{$\\textbf{51.47}$$ \\pm $\\scriptsize$ \\textbf{6.28}$}}   & \n% {\\textcolor{red}{$\\textbf{39.43}$$ \\pm $\\scriptsize$ \\textbf{3.88}$}}       & \n% {\\textcolor{red}{$\\textbf{43.67}$$ \\pm $\\scriptsize$ \\textbf{5.75}$}}    & \n% {\\textcolor{red}{$\\textbf{51.30}$$ \\pm $\\scriptsize$ \\mathbf{4.29}$}}        & \n% {\\textcolor{red}{$\\textbf{47.62}$$ \\pm $\\scriptsize$ \\mathbf{6.47}$}}     & \n% {\\textcolor{red}{$\\textbf{61.37}$$ \\pm $\\scriptsize$ \\textbf{4.95}$}}         & \n% {\\textcolor{red}{$\\textbf{58.42}$$ \\pm $\\scriptsize$ \\textbf{6.17}$}}  \\\\\n% TAD \\cite{hu2023understanding} & \n% {\\textcolor{red}$\\textbf{49.80}$$\\pm$\\scriptsize$\\textbf{2.13}$}         & \n% {\\textcolor{red}$\\textbf{42.23}$$\\pm$\\scriptsize$\\textbf{1.35}$}         & \n% {\\textcolor{red}$\\textbf{62.75}$$\\pm$\\scriptsize$\\textbf{6.40}$}         & \n% {\\textcolor{red}$\\textbf{56.53}$$\\pm$\\scriptsize$\\textbf{7.70}$}         & \n% {\\textcolor{red}{$\\textbf{50.82}$$ \\pm $\\scriptsize$ \\textbf{3.93}$}}    & \n% {\\textcolor{red}{$\\textbf{53.40}$$ \\pm $\\scriptsize$ \\textbf{3.86}$}}    & \n% {\\textcolor{red}$\\textbf{58.97}$$\\pm$\\scriptsize$\\textbf{3.76}$}         & \n% {\\textcolor{red}$\\textbf{58.72}$$\\pm$\\scriptsize$\\textbf{2.60}$}         & \n% {\\textcolor{red}$\\textbf{54.88}$$\\pm$\\scriptsize$\\textbf{3.34}$}         & \n% {\\textcolor{red}$\\textbf{52.15}$$\\pm$\\scriptsize$\\textbf{1.57}$}         \\\\\n% PCWPK\\cite{zhang2021prototype} & \n% {\\textcolor{red}$\\textbf{42.92}$$\\pm$\\scriptsize$\\textbf{3.80}$}  & \n% {\\textcolor{red}$\\textbf{44.17}$$\\pm$\\scriptsize$\\textbf{3.12}$}  & \n% {\\textcolor{red}$\\textbf{56.08}$$\\pm$\\scriptsize$\\textbf{2.91}$}  & \n% {\\textcolor{red}$\\textbf{53.01}$$\\pm$\\scriptsize$\\textbf{5.17}$}  & \n% {\\textcolor{red}$\\textbf{34.74}$$\\pm$\\scriptsize$\\textbf{2.54}$}  & \n% {\\textcolor{red}$\\textbf{34.29}$$\\pm$\\scriptsize$\\textbf{3.36}$}  & \n% {\\textcolor{red}$\\textbf{51.97}$$\\pm$\\scriptsize$\\textbf{1.76}$}  & \n% {\\textcolor{red}$\\textbf{43.62}$$\\pm$\\scriptsize$\\textbf{2.27}$}     & \n% {\\textcolor{red}$52.39$$\\pm$\\scriptsize$\\textbf{8.60}$} & \n% {\\textcolor{red}{$\\textbf{64.56}$$ \\pm $\\scriptsize$ \\textbf{5.57}$}} \\\\\n\n% \\midrule\n% \\textbf{Ours(MFL)} &{\\textcolor{red}$\\textbf{97.47}$$\\pm$\\scriptsize$\\textbf{1.17}$}&{\\textcolor{red}$\\textbf{94.27}$$\\pm$\\scriptsize$\\textbf{2.62}$}&{\\textcolor{red}$\\textbf{96.64}$$\\pm$\\scriptsize$\\textbf{1.88}$}&{\\textcolor{red}$\\textbf{91.27}$$\\pm$\\scriptsize$\\textbf{3.94}$}&{\\textcolor{red}$\\textbf{92.73}$$\\pm$\\scriptsize$\\textbf{2.57}$}&{\\textcolor{red}$\\textbf{88.21}$$\\pm$\\scriptsize$\\textbf{3.25}$}&{\\textcolor{red}$\\textbf{97.39}$$\\pm$\\scriptsize$\\textbf{1.41}$}&{\\textcolor{red}$\\textbf{94.32}$$\\pm$\\scriptsize$\\textbf{2.84}$}&{\\textcolor{red}$\\textbf{93.93}$$\\pm$\\scriptsize$\\textbf{3.27}$}&{\\textcolor{red}$\\textbf{90.98}$$\\pm$\\scriptsize$\\textbf{3.69}$}  \\\\\n% \\bottomrule\n% \\end{tabular}}\n% \\end{table*}\n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 5.2.3": {
                        "name": "Comparisons of unknown attacks",
                        "content": "\nTo further investigate the performance of detecting unknown attacks, we conduct experiments on the four attack types by discarding the corresponding instances in the train set and detecting them in the test set. {From this perspective, the unknown attacks are the new-type attacks that do not belong to the existing types of attacks in the training set.} We run our experiments $5$ times and report the mean and variance values with different random seeds. Table \\ref{UAC} reports the classification results on the CIC-ToN-IOT dataset. It shows that the statistical rule-based method Logistic Regression can only achieve as low as a $1.68$ F1 score for DDoS attacks, this confirms our analysis at the very beginning that rule-based methods can hardly detect unknown attacks. The score of graph-based E-GraphSAGE is much smaller than DIDS, e.g., $6.05\\%$ for MITM, indicating the limitations of the static graph in detecting unknown attacks. We also observe that TGN performs better than E-GraphSAGE, although both of them are graph-based methods. We attribute the improvement to the dynamic module for TGN. Nevertheless, our DIDS outperforms all these methods by a large margin, with an average score of $33.65\\%$ on the four attacks. The results also suggest that our method is more consistent in detecting various unknown attacks, showing the effectiveness of the two disentanglements. {We further provide empirical studies on more datasets in Appendix 16 to showcase the unknown detection ability of DIDS.}\n\n\n"
                    },
                    "subsubsection 5.2.4": {
                        "name": " Comparisons under few-shot settings",
                        "content": "\n\\noindent\n\\\n\n%training set & validation set (support and query) test set\n\n\\textbf{Task Setting:} We first conduct the DIDS pretraining over the known benign and attack traffic. Then we conduct a $\\mathcal N$-way-$\\mathcal K$-shot learning for the few-shot attack traffic. Specifically, $\\mathcal N$ is the category number of few-shot traffic, and $\\mathcal K$ is the training number of each category, referred to support set. In our few-shot experiments, we set $\\mathcal K$ as $5$. We also construct a query set for our few-shot task, with $15$ sample numbers for each category. The query set can be sampled from the category samples, or augmented from the support set. We split a $5$-fold cross-validation set from the training set, with $20$\\% of support and query set. Taking CIC-TON-IoT as an example, we conduct a five-way-five-shot learning and five-classification test with $100$ training samples. We repeat the above training, validate, test ten times, and report the average performance. Each time's training and validation samples are randomly selected from our existing traffic data, thus simulating the few-shot learning setting in real-world scenarios.\n\n%randomly select some of the anomaly traffic types for the 3D-IDS pretraining. Then we use the left anomaly traffic types for the few-shot learning and testing, i.e. for five-class anomaly traffic, we conduct a five-way-five-shot learning. The well-trained few-shot model conducts a five-classification test. We repeat the above training, validate, test ten times, and report the average performance. The training and validation samples for each time are randomly selected from a larger sample library, thus simulating the few-shot learning setting in real-world scenarios.\\\\\n\n%We split the data set into an 80\\% training set and a remaining 20\\% test set. During training, we designate 20\\% of the training set as the verification set. Each training iteration randomly extracts $N \\times 5$ samples from the corresponding data set to form the support set of few-shot learning, and $N \\times 20$ samples to form the query set of few-shot learning. Then we conduct training and verify the model state after this training.\n\n\n\\textbf{Baselines:} \nTo evaluate the performance of the proposed few-shot learner, we select $11$ few-shot learning models to incorporate into DIDS as baselines, including $3$ meta-learning based models (i.e., MBase~\\cite{chen2021metabaseline}, MTL~\\cite{sun2019metatransfer}, TEG~\\cite{kim2023task}), where TEG is designed for graph-structure-based few-shot learning, $4$ augmentation-based models (i.e., CLSA~\\cite{wang2022contrastive}, ESPT~\\cite{rong2023espt}, ICI~\\cite{9446583}, KSCL~\\cite{xu2021kshot}, where CLAS and ESPT are based on contrastive augmentation, ICI and KSCL are based on instance augmentation), $4$ metric learning-based models (i.e., BSNet ~\\cite{Li_2021}, CMFSL ~\\cite{Xi2022FewShotLW}, TAD~\\cite{hu2023understanding}, PCWPK~\\cite{zhang2021prototype}).\n\n\\textbf{Performance metrics:} We follow the previous work \\cite{bandyopadhyay2021unsupervised} using F1 scores and NMI\\cite{grandini2020metrics} metrics for multi-classification comparison.\n\nAs shown in Table~\\ref{tab:few-shot}, our MFL consistently performs better among $11$ baselines under the five benchmarks, showing the superiority of our proposed MFL module in few-shot intrusion detection. Specifically, our MFL achieves reproducible average results of $95.63$\\% and $91.81$\\% in F1-score and NMI value over five public datasets. The results gain $97.87$\\% - $126.99$\\% and $73.59$\\% - $139.53$\\% improvements in average F1-score and NMI value of $11$ baselines over five public datasets, respectively. We attribute the superior and impressive reproducible results of MFL to our multi-scale representation information using and disentanglement among few-shot traffic representations. The $RQ 5$ and $RQ 6$ in the discussion section further verified our attributions. {Furthermore, we also discuss the practical time cost of DIDS-MFL and the superiority of our model designs in time cost in Appendix 20.}\n\n\n%compared to the xxx, the previous state-of-the-art xxx-based approach, MFL respectively achieves a $4.80\\%$ higher F1-score over the CIC-BoT-IoT dataset in F1 and NMI scores. Our multi-scale few-shot learner also outperforms xxx and xxx, the previous state-of-the-art method for anomaly detection on F1, by $x$-$x$ points over the five datasets. We attribute the superior reproducible results of MFL to our multi-scale representation information using and disentanglement among few-shot traffic representations. The $RQ 5$ and $RQ 6$ in the discussion section further verified our attributions.\n\n%Specifically, compared to the E-GraphSAGE, the previous state-of-art GCN-based approach, our method achieves a $4.80\\%$ higher F1-score over the CIC-BoT-IoT dataset. Our 3D-IDS outperforms AnomRank, the previous state-of-the-art method for anomaly detection on F1, by $15.27$ points over the EdgeIIoT dataset. We attribute the above results to the gains of our statistical disentanglement, representational disentanglement, and dynamic graph diffusion method.\n\n\n\n\n"
                    }
                },
                "subsection 5.3": {
                    "name": "Ablation Study",
                    "content": "\n",
                    "subsubsection 5.3.1": {
                        "name": "DIDS",
                        "content": "\nIn this section, we conduct an ablation study on the CIC-ToN-IoT dataset to evaluate the effectiveness of each component. We remove our statistical disentanglement and denote it as \"w/o SD\". We use \"w/o RD\" and \"w/o MLGRAND\"  to refer to the model that removes representational disentanglement and the multi-layer graph diffusion module, respectively. Table \\ref{Tab:abl} reports the comparison results. It shows that removing the multi-layer graph diffusion module leads to the most significant performance degradation, e.g., an $18.33$ points decrease in AUC, indicating that it is the key component for the accuracy of the proposed DIDS. \nOur second disentangled memory is also non-trivial to the overall detection accuracy, as removing this component can decrease the performance by $12.47$ points in AUC. We observe that the {SD} module also benefits the model performance. The above ablation study further confirms the effectiveness of the three key components.\n\n\n\n\n"
                    },
                    "subsubsection 5.3.2": {
                        "name": " MFL",
                        "content": "\nFurthermore, we conduct an ablation study on the CIC-TON-IOT dataset to evaluate the effectiveness of our designed multi-scale few-shot learning (MFL) module. We denote our model without {multi-scale} latent optimization space as \"w/o LOS\" and \"w/o DR\" as our model without the disentanglement regularization term. For the model without both of the above components, we denote it as \"SE\", which degenerated into a naive self-expressiveness model. Table~\\ref{Tab:abl_2} shows that removing both LOS and DR leads to dramatic performance degradation, i.e., $9.92$\\% and $9.75$\\% drop in F1-score and NMI, respectively, indicating the effectiveness of our designed MFL. Furthermore, we observed that each component of MFL, e,g. LOS or DR is necessary for performance improvement, cause removing one of them will lead to performance degradation, i.e., $22.21$\\%-$23.16$\\% and $8.63$\\%-$9.24$\\% drop in F1-score and NMI, respectively. {Without any one of these two components, the performance will be poorer than the base model SE. Results showcase the complementary relationships of the proposed two modules. The LOS provides a latent optimization space across multi-scale representations, which is the foundation of the DR term. Then the DR term generates effective disentangled representations across multiple scales based on LOS.}\n\n\n\n\n  "
                    }
                },
                "subsection 5.4": {
                    "name": "Discussion",
                    "content": "\n%In this section, we further analyze the influence of components in the proposed module. Specifically, we want to answer the following questions:\n%We aim to emphasize the role of each module of 3D-IDS by answering the following questions:\n% \\begin{itemize}\n%     \\item \\textbf{RQ1} \n%     \\item \\textbf{RQ2}\n%     \\item \\textbf{RQ3} \n%     \\item \\textbf{RQ4} \n% \\end{itemize}\n\\textbf{DIDS:}\n\\noindent\n\\\n\n\\textbf{RQ$1$: How does the statistical disentanglement help the detection of various attacks?}\nTo answer this question, we visualize the distributions of features before and after the statistical disentanglement.\nFigure \\ref{dis} shows the visualizations of the two distributions respectively.\nWe can observe that there is less overlap between distributions of features after the disentanglement compared with the original data, which demonstrates this module could decrease the mutual reference between features and enable them to be distinguishable.\nWe also observe that the distributions gradually shift to the right side, representing the order-preserved constraints within our disentangling method.\n% \\begin{figure}[ht]\n% \t\\centering  %图片全局居中\n%         \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n%     % \\vspace{6pt}\n% \t%\\subfigbottomskip=0pt %两行子图之间的行间距\n% \t% \\subfigcapskip=-5pt %设置子图与子标题之间的距离\n% \t% \\subfigure[Origin]{\n% \t% \t\\includegraphics[width=0.48\\linewidth]{picture/origin.pdf}}\n% \t% \\subfigure[First Distangled]{\n% \t% \t\\includegraphics[width=0.48\\linewidth]{picture/dis.pdf}}\n%  \t\\subfigure[Origin]{\n% \t\t\\includegraphics[width=0.48\\linewidth]{picture/page2/pic2/ZP12.pdf}}\n% \t\\subfigure[First Distangled]{\n% \t\t\\includegraphics[width=0.48\\linewidth]{picture/page2/pic2/ZP121.pdf}}\n%           \\\\\n%   %  \\subfigure[Second Distangled]{\n% \t\t% \\includegraphics[width=0.7\\linewidth]{picture/attmap.pdf}}\n%    \\caption{Statistical disentanglement of traffic features.}\n% \\label{dis}\n% \\end{figure}\n\n\n\n\n\\noindent\n\\textbf{RQ$2$: How does the representational disentanglement benefit {\"highlighting the attack-specific features\"?}}\nTo answer this question, we track several Injection attack data in the CIC-ToN-IoT dataset and obtain the representation of these data in DIDS and E-GraphSAGE. Meanwhile, we calculate the above two methods’ average values of embeddings for the benign traffic. As shown in Figure \\ref{casestudy}, the representation values of E-GraphSAGE are much closer to the normal. It illustrates that as nodes aggregate, the discrepancies in features become blurred, leading to inaccurate classification. While benefiting from the representational disentanglement, each dimension of features in DIDS can effectively preserve its own properties, deviating from the averages. Especially for the attack-specific features Fwd Pkt Len Max and Fwd Pkt Std, etc., these attack-specific features are significantly highlighted in Fig. 1(a), thus improving the accuracy of detection. The result proves the effectiveness of the proposed DIDS in maintaining a disentangled representation during the aggregation process, ensuring the presence of discrepancies, thus highlighting the attack-specific features and leading to more accurate classification for attacks.\n%increasing the probability of identifying the injection attack as normal\n \n%  \\begin{figure}[ht]\n%      \\centering \n%       \\subfigcapskip=-3pt %设置子图与子标题之间的距离\n%      \\setlength{\\abovecaptionskip}{2pt}  %图像在caption上方距离\n%  %\\subfigure[]{\n%  %\\includegraphics[width=0.30\\linewidth]{picture/casestudy1.pdf}\n%  %}\n%  % \\subfigure[3D-IDS]{\n%  % \\includegraphics[width=0.47\\linewidth]{picture/ca1.pdf}\n%  % }\n% \\subfigure[DIDS]{\n%  \\includegraphics[width=0.45\\linewidth]{picture/ca1.png}\n%  }\n%   % \\includegraphics[width=0.47\\linewidth]{picture/page2/pic2/casestudy2.pdf}}\n%  % \\subfigure[E-GraphSAGE]{\n%  % \\includegraphics[width=0.47\\linewidth]{picture/ca2.pdf}\n%  \\subfigure[E-GraphSAGE]{\n%  \\includegraphics[width=0.45\\linewidth]{picture/ca2.png}\n%  %  \\includegraphics[width=0.47\\linewidth]{picture/page2/pic2/casestudy3.pdf}\n% }\n% \\caption{The comparison of node representation  of the Injection attack after graph aggregation of our DIDS and E-GraphSAGE. The grey line presents benign data.}\n% \\label{casestudy}\n% \\end{figure}\n\n\n\n\\noindent\n\\textbf{RQ$3$: How does the multi-layer diffusion module perform effectively for intrusion detection?}\nWe have illustrated the principle of multi-layer diffusion in Section \\ref{Multi-Layer Graph Diffusion module}. In this part, we take the MITM attack as an example to illustrate the effectiveness of spatial-temporal in intrusion detection.\n\n%MITM attack happens when attackers place themselves between users and applications to snoop or masquerade as one of the parties. establishing the deceptive appearance as if an ordinary exchange of information is afoot.\nFigure \\ref{attact_scenira} (a) shows a deep learning-based NIDS. When a MITM attack occurs, it is difficult to detect the intrusion\nsince the spatial and temporal information of those packets is not considered. There are also some methods that only consider a single aspect of spatial and temporal information, such as E-GraphSAGE and MStream. In this case, for example, E-GraphSAGE mainly focuses on the spatial relationship of the set nodes and extracts features from them. However, we observe that different streams have their own timestamps from Table \\ref{IPTABLE}, so the lack of temporal information makes it impossible to analyze the dynamic structural changes of the edge. Similarly,  taking the temporal information as the only effect factor will also get incomplete characteristics that do not contain spatial information (IP address). Moreover, some methods that take both the spatial and temporal information into account, such as Euler, take the snapshot method to capture the feature of the flow which does not achieve the synchronous update for spatial and  temporal information.\n%the ability to collect information across a period of time and identify the different layers of devices%we would ignore the characteristics of the flow itself when considering the timing relationship alone. The lack of characteristics is not conducive to the detection of complex attacks. Also, it is difficult for us to achieve multi-classification\n%Figure \\ref{attact_scenira}.B indicates that our 3D-IDS model can detect the MITM with high accuracy. 3D-IDS has outstanding perform on those attacks which involve different layers because of its novel multi-layer structure.\nAs shown in Figure \\ref{attact_scenira} (b), intuitively, we can quickly detect that UE6 is an intrusion device of layer 1 when the flow changes from $SW2-SW3$ to $SW2-UE6$ and $UE6-SW3$ considering $SW2$, $SW3$ are layer $2$ devices. Also, we have noticed the changes in dynamic graph structure with a multi-Layer graph diffusion module to realize spatio-temporal coupling and synchronous updating. Overall, DIDS performs best among these baselines in detecting various attacks.\n\n\n\n%we model the network equipment as a multi-layer structure, so that the flow originally transmitted from User Equipment $2$ (UE$2$) to User Equipment $3$ (UE$3$) becomes UE$2$ through UE$6$ and then to UE$3$. The transmission path of the original flow should be a layer $2$ device. When the attack occurs, a layer $1$ device will suddenly appear, which indicates that there is an intrusion. Secondly, we also consider the time sequence of the packet. The normal transmission happens at t$2$, but it takes two periods t$3$ and t$4$ to accomplish in this case which indicates that there is an intrusion.\n\n\\noindent\n\\textbf{RQ$4$: How does the disentanglement facilitate the explainability of DIDS?}\nFor this question, we rely on Figure \\ref{casestudy} (a) as an example to recover the possible traffic features of a password attack. Since the original features are retained after the disentanglement, we can find some feature values that deviate significantly from the normal values in the node embedding. In the password attack, we observe that the deviated features after disentanglement are \"Fwd Pkt Std\" and \"Fwd Pkt Len Max\". It aligns with our common sense for the main causes of password attacks and further benefits the explainability of DIDS.\n\n\\iffalse\nAs shown in Figure \\ref{seconddis}, due to the disentangled edge representation, there exists a linear relationship between the disentangled edge vector and the disentangled node representation. Therefore, based on this relationship, we observe that the deviated features \"Fwd Pkt Std\" and \"Fwd Pkt Len Max\" present a strong correlation with the password attack. \n\n\\fi\n\\textbf{MFL:}\n\\noindent\n\\\n\n\\textbf{RQ$5$: How does multi-scale transform-based MFL benefit the distinction of few-shot traffic threats?}\nTo answer this question, we visualize the learned representations of DIDS and the few-shot learning module MFL on the CIC-TON-IoT dataset via t-SNE technology. As shown in Fig.~\\ref{fig: RQ_5}(a) (b), the MFL's representations are highly separated and distinguishable for different few-shot attacks, i.e., the red square box, compared to the representation generated by DIDS, i.e., the red round box. It verifies the effectiveness of our proposed multi-scale few-shot learning framework, i.e., multiple coefficient matrices fusion and multi-scale transformation. They discover the attack-specific invariant features among few-shot traffic in latent space, thus improving the distinction of attack representations.\n% \\begin{figure}[ht]\n%     \\centering\n%     \\subfigbottomskip=0pt    %两行子图之间的行间距\n%     \\subfigcapskip=0pt %设置子图与子标题之间的距离\n%     \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n%     \\setlength{\\belowcaptionskip}{0pt}\n%     %\\setcounter{subfigure}{0}\n%     \\subfigure[The t-SNE of DIDS]{\n%     \\includegraphics[width=0.46\\linewidth]{picture/page2/pic2/3D-IDS_tsne3.pdf}\n%     }\n%     %\\setcounter{subfigure}{2}\n%     \\subfigure[The t-SNE of MFL]{\n%     \\includegraphics[width=0.47\\linewidth]{picture/page2/pic2/v2_tsne0.pdf}\n%     }\n%     % \\vspace{-20pt}\n%     \\\\\n%     \\caption{The t-SNE visualization of representations on the CIC-TON-IoT dataset generated by DIDS and MFL.}\n%    % \\vspace{-4mm}\n%     \\label{fig: RQ_5}\n% \\end{figure}\n\n\n\n\\noindent\n\\textbf{RQ$6$: Can MFL disentangle the representations of few-shot samples?} \nTo answer this question, we visualize the learned representations of DIDS and MFL on the CIC-TON-IoT dataset via correlation heatmaps. As shown in Fig.~\\ref{fig: RQ_6}(a) (b), the visualization results significantly reveal that MFL can generate highly disentangled representations via our designed regularization term. Specifically, MFL generates a block diagonal heatmap with high correlations and non-diagonal areas with very low correlations. It verifies that MFL disentangles the few-shot traffic representations and highlights the attack-specific ones, making them more distinguishable. \n\n% \\begin{figure}[ht]\n%     \\centering\n%     \\subfigbottomskip=0pt    %两行子图之间的行间距\n%     \\subfigcapskip=0pt %设置子图与子标题之间的距离\n%     \\setlength{\\abovecaptionskip}{8pt}  %图像在caption上方距离\n%     \\setlength{\\belowcaptionskip}{0pt}\n%     %\\setcounter{subfigure}{0}\n%     \\subfigure[The correlation map of DIDS]{\n%     \\includegraphics[width=0.46\\linewidth]{picture/page2/pic2/3D-IDS_heatmap9.pdf}\n%     }\n%     %\\setcounter{subfigure}{2}\n%     \\subfigure[The correlation map of MFL]{\n%     \\includegraphics[width=0.47\\linewidth]{picture/page2/pic2/v2_heatmap8.pdf}\n%     }\n%     % \\vspace{-20pt}\n%     \\\\\n%     \\caption{The correlation map and the t-SNE visualization of representations generated by DIDS and DIDS-MFL with MFL.}\n%    % \\vspace{-4mm}\n%     \\label{fig: RQ_6}\n% \\end{figure}\n\n\n\n\n\\iffalse\n\n\\fi\n\\noindent\n\\textbf{RQ$7$: Can MFL be served for other intrusion detection methods to strengthen their few-shot learning ability?}\nTo answer this question, we use MFL as a plug-and-play module to incorporate into other intrusion detection methods, i.e., E-GraphSAGE, EULER, and TGN. We follow our few-shot setting to conduct experiments on these methods with and without MFL. The methods with MFL significantly outperform the ones without MFL by $17.45$\\%-$544.77$\\%, as shown in Table~\\ref{Tab: RQ7}. It demonstrates the effectiveness of our proposed MFL as a plug-and-play module to strengthen the few-shot learning ability of other NIDS. Additionally, the underlying cause of the slight improvement of E-GraphSAGE and EULER with MFL is interesting for future few-shot NIDS designs. {The performance differences between E-GraphSAGE and TGN lie in the generated representation qualities of respective approaches, with more details available in Appendix 18. We also provide the visualizations of different baselines to showcase the disentangle representation ability of MFL in Appendix 19.}\n\n%can be easily incorporated into existing intrusion detection methods to improve their few-shot learning ability. The \n\n\n\n\\noindent\n\\textbf{RQ$8$: Can MFL improve the performance of DIDS?}\nSo far, we have verified the effectiveness of MFL in few-shot traffic intrusion detection, including serving as a plug-and-play module for other methods. To further study the applicability of MFL in large-scale traffic, we conduct DIDS training with MFL and report the multi-classification results, as shown in Table~\\ref{Tab:appli_large}. DIDS with MFL achieves a $2.31$\\% to $15.82$\\% F1-score improvements among three datasets. The results reveal the effectiveness of MFL when serving as a multi-classification module for normal-size traffic training. Our designed multi-scale transform-based framework also sheds light on the future intrusion detection model design. \n \n\n\n\n\\noindent\n\\textbf{RQ$9$: Network intrusion detection and Large language model (LLM).}\nRecently, there has been a significant surge in the development and application of Large Language Models (LLMs) \\cite{bahrini2023chatgpt}cross various domains, including Natural Language Processing (NLP)\\cite{lauriola2022introduction}, Computer Vision (CV)\\cite{Liu_2022}, and multimodal tasks. However, the existing LLMs, e.g. GPT-$3.5$, GPT-$4$ struggle to detect traffic threats accurately and efficiently. As shown in Table~\\ref{Tab: LLM}, the F1-score of the existing LLM is significantly lower than our SOTA method DIDS. It may lie in the hallucinations\\cite{huang2023survey} of existing neural language-based LLM, which struggle to comprehend the intrinsic attack features in traffic data. Second, the LLM necessitates more time to process the input traffic data and may fail as the size of the input traffic increases. It may be attributed to the numerical values of the traffic data involved. To sum up, the future directions to empower NIDS via LLM can be: 1. Aligning encrypted traffic data with the input requirements of LLMs, thus speeding the intrusion detection via LLM; 2. Determining the necessary volume of traffic data and corresponding traffic mining technology to unlock the powerful inference capabilities of LLM.\n\n \n\n\n%to the scarcity of adapting LLMs for intrusion detection, as the traffic data usually involves numerical values. Second, text-understanding-driven LLM tends to have lower accuracy and poor scalability. In our empirical studies, the LLM breaks down when inputting large-scale traffic data, and the F1-score is significantly lower than the existing SOTA methods, e.g. 3D-IDS. It may lie that existing LLM can hardly understand the attack features intrinsic in traffic. To sum up, the future directions to empower NIDS via LLM can be the following aspects: 1. Aligning encrypted traffic data with the input requirements of LLMs, thus speeding the intrusion detection via LLM; 2. Determining the necessary volume of traffic data and corresponding traffic mining technology to unlock the powerful inference capabilities of LLM.\n\n%adapting LLMs for intrusion detection remains scarce, as the traffic data usually involves numerical values.\n\n\n%Recently we have witnessed prosperity in large language model (LLM) and their applications on downstream NLP, CV, and multimodal tasks. However, the available large language models for intrusion detection are rare as the traffic data usually involves numerical-based value. The existing deep-learning-based NIDS still shows the state-of-the-art performances. The future directions to empower NIDS via LLM can be the following aspects: 1. How to align encrypted traffic data with LLM inputs? 2. What volume of traffic data is needed to unlock the powerful inference capabilities of LLM? 3. Can LLM replace existing traffic intrusion detection models? Or as a tool to benefit the existing NIDS models?\n%We can empower NIDS by LLM in the following aspects: 1. How to align encrypted traffic with LLM inputs? 2. What amount of traffic data is needed to unlock the powerful inference capabilities of LLM? 3. Can LLM replace existing traffic intrusion detection techniques? Or as a tool to benefit the existing NIDS models?\n\n\\iffalse\n"
                }
            },
            "section 6": {
                "name": "Hyperparameter Analysis",
                "content": "\nWe conduct the following experiments to analyze the impacts of eight hyperparameters in DIDS and MFL, including $W_{min}$, $W_{max}$, $B$, Memory Dimension, trade-off hyperparameter of MFL regularization term $\\alpha$, trade-off hyperparameter of MFL loss $\\beta$, fusion trade-off hyperparameter $\\epsilon$, and scaling rate $\\gamma$. Specifically, we depict the F1 score of different hyperparameters in different steps. For $W_{min}$,  $W_{max}$, and B, the numbers $0$, $1$, and $1$ are the optimal values, respectively, showing the range of $[0,1] $ is sufficient to capture the property of each dimension and disentangle the representations. Furthermore, setting the Memory Dimension to $150$ ensures that the representation does not result in statistical redundancies. For hyperparameters $\\alpha$, a smaller value $0.1$ is sufficient to regularize the optimization. For hyperparameter $\\beta$, $1$ is the optimal value, revealing that the cross-entropy and the regularization term have the same impact on the few-shot learning. Finally, as for $\\epsilon$ and $\\gamma$, smaller values, i.e., $0.3$ and $0.1$ are sufficient to capture the multi-scale information to help distinguish attack traffic, as shown in Fig.~\\ref{hyperparameter2}(g) (h).\n\\fi\n\\iffalse\n\nMessage Dimension, Embedding Dimension, Hidden Units, and Hidden Channels. \n\n\nWe use the F1-score as the main metric to evaluate the effect of these hyper-parameters. The best-performed hyper-parameters are shown in Table \\ref{para}.\n\nFor $W_{min}$,  $W_{max}$, and B, we can find that $0$, $1$, and $1$ are the optimal values which show the range of $[ 0,1] $  is enough to represent the property of each dimension and disentangle the embedding. For Memory Dimension, Message Dimension, Embedding Dimension, Hidden Units, and Hidden Channels, we can find that $150$, $100$, $100$, $64$, and $256$ are the optimal values. A common phenomenon found is that the performance of our model first increases and then decreases as these hyper-parameters increase. The reason is that DIDS needs a suitable dimension to encode the informative representation, and a larger dimension may introduce additional redundancies.\n\n\n\\fi\n\n"
            },
            "section 7": {
                "name": "CONCLUSION",
                "content": "\nThis paper quantitatively studies the inconsistent performances of existing NIDS under various attacks and reveals that the underlying cause is entangled feature distributions. Furthermore, we delve into the deeper reasons for the poor few-shot intrusion detection performance of existing NIDS. These interesting observations motivate us to propose DIDS-MFL. The former is a novel method that aims to benefit known and unknown attacks with a double disentanglement scheme and graph diffusion mechanism, and the latter is a transform-based multi-scale few-shot learner to help few-shot traffic threats distinguishable. The proposed DIDS first employs statistical disentanglement on the traffic features to automatically differentiate tens and hundreds of complex features and then employs representational disentanglement on the embeddings to highlight attack-specific features. DIDS also fuses the network topology via multi-layer graph diffusion methods for dynamic intrusion detection. Finally, the proposed MFL uses an alternating optimization framework to separate and disentangle the few-shot traffic representations. Extensive experiments on five benchmarks show the effectiveness and the practical employment potential of our DIDS-MFL, including binary classifications, multi-classifications, unknown attack identification, and few-shot intrusion detection. Future work could focus on the deployment of DIDS-MFL on future wireless networks.\n\n"
            },
            "section 8": {
                "name": "ACKNOWLEDGEMENT",
                "content": "\nThis work was supported by the National Key R\\&D Program of China (Grant No.2022YFB2902200).\n\n%\\clearpage\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{main} \n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Chenyang Qiu} received his B.Sc. degree in science from Beijing Jiaotong University, Beijing, China, in 2020. He is currently pursuing his Ph.D degree at the National Engineering Research Center for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing. His research interests include Wireless Network Intrusion Detection, Semantic Communication, Graph Neural Networks, and AI Robustness. He has published papers in top-tier conferences and journals including KDD, AAAI, and IEEE TCYB.\n\\end{IEEEbiographynophoto}\n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Guoshun Nan} (Member, IEEE) is a tenure-track professor at the National Engineering Research Center for Mobile Network Technologies, Beijing University of Posts and Telecommunications. He has broad interest in natural language processing, computer vision, machine learning, and wireless communications, such as information extraction, model robustness, multimodal retrieval, and next-generation wireless networks. He is a member of the National Engineering Research Center for Mobile Network Technologies. He has published papers in top-tier conferences and journals including ACL, CVPR, EMNLP, SIGIR, IJCAI, CKIM, SIGCOMM, IEEE JSAC, IEEE Communications Magazine, IEEE Network, Computer Networks. He served as a Reviewer for ACL, EMNLP, AAAI, IJCAI, Neurocomputing, and IEEE Transactions on Image Processing. \n\\end{IEEEbiographynophoto}\n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Hongrui Xia} is an undergraduate student at the National Engineering Research Center for Mobile Network Technologies, Beijing University of Posts and Telecommunications. He specializes in network security and intrusion detection, aiming to advance his expertise in safeguarding digital systems and detecting cyber threats. Dedicated to contributing to the field through research and innovation.\n\\end{IEEEbiographynophoto}\n\\iffalse\n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Mingwei Wang} is an undergraduate student at the National Engineering Research Center for Mobile Network Technologies, Beijing University of Posts and Telecommunications. His major is Artificial Intelligence, with a particular focus on the field of computer vision. \n\\end{IEEEbiographynophoto}\n\\fi\n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Zheng Weng} is an undergraduate student at the National Engineering Research Center for Mobile Network Technologies, Beijing University of Posts and Telecommunications. He focuses on Graph Neural Networks (GNNs) and machine learning, exploring innovative methods to enhance data security and network integrity through advanced computational models.\n\\end{IEEEbiographynophoto}\n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Xueting Wang} is an undergraduate student at the National Engineering Research Center for Mobile Network Technologies, Beijing University of Posts and Telecommunications. Her current research interests include artificial intelligence security, the analysis and detection of network traffic attacks, and the use of AI automation to detect and analyze viruses such as Trojans.\n\\end{IEEEbiographynophoto}\n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Meng Shen} (Member, IEEE) received the B.Eng degree from Shandong University, Jinan, China in 2009, and the Ph.D degree from Tsinghua University, Beijing, China in 2014, both in computer science. Currently he serves in Beijing Institute of Technology, Beijing, China, as a professor. His research interests include privacy protection for cloud and IoT, blockchain applications, and encrypted traffic classification. He received the Best Paper Runner-Up Award at IEEE IPCCC 2014. He is a member of the IEEE.\n\\end{IEEEbiographynophoto}\n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Xiaofeng Tao} (Senior Member, IEEE) received the bachelor’s degree in electrical engineering from Xi’an Jiaotong University, Xi’an, China, in 1993, and the master’s and Ph.D. degrees in telecommunication engineering from the Beijing University of Posts and Telecommunications (BUPT), Beijing, China, in 1999 and 2002, respectively. He is currently a Professor at the National Engineering Research Center for Mobile Network Technologies, Beijing University of Posts and Telecommunications, a fellow of the Institution of Engineering and Technology, and the Chair of the IEEE ComSoc Beijing Chapter. He has authored or coauthored over 200 articles and three books in wireless communication areas. He focuses on B5G/6G research.\n\\end{IEEEbiographynophoto}\n\\vspace{-40pt}\n\\begin{IEEEbiographynophoto}\n{Jun Liu} (Member, IEEE) received the master’s degree from Fudan University and the Ph.D. degree from the School of Electrical and Electronic Engineering, Nanyang Technological University. He is currently an Assistant Professor with the Singapore University of Technology and Design. His research interests include computer vision, machine learning, and artificial intelligence. His research works are published in many conferences and journals, including IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Image Processing, NeurIPS, CVPR, ICCV, and ECCV. He is also an organizer of international workshops. He was the Area Chair of ICML/NeurIPS (2022–2023), ICLR (2022–2024), IJCAI 2023, and CVPR 2024.\n\n\n\\end{IEEEbiographynophoto}\n\\clearpage\n\n\\iffalse\n\n\n% \\iffalse\n"
            },
            "section 9": {
                "name": "Mathematical Notations",
                "content": "\n\n\n"
            },
            "section 10": {
                "name": "Datasets",
                "content": "\n\\label{sec-datasets}\nThe details of all datasets are shown in Table \\ref{DATASETTABLE}.\n\n",
                "subsection 10.1": {
                    "name": "CIC-ToN-IoT",
                    "content": "\n  This dataset has been labeled with binary- and multi-class categories which include different types of IoT data, such as operating system logs, telemetry data of IoT/IIoT services, as well as IoT network traffic collected from a medium-scale network at the Cyber Range and IoT Labs at the UNSW Canberra (Australia). In our experiment, only the network traffic component of the dataset is used. This will accommodate a reliable evaluation of our models. This dataset contains a variety of attack types especially XSS, backdoor, and injection, thus it is used to test the model's ability in stealth attack detection.\n  % The dataset consists of $5,351,760$ data where $2,836,524$ ($53.00\\%$) are attacks and $2,515,236$ ($47.00\\%$) are benign samples.\n"
                },
                "subsection 10.2": {
                    "name": "CIC-BoT-IoT",
                    "content": "\nThis dataset is generated from the corresponding IoT traffic based on the Node-RED platform, which can simulate the IoT services by wiring together hardware devices, APIs, and online services in new and interesting ways. The attack samples are made up of four attack scenarios inherited from the parent dataset, i.e., DDoS ($36.84\\%$), DoS ($36.80\\%$), reconnaissance ($26.35\\%$) and theft ($0.01\\%$). As the first three attack takes $99.99\\%$ of the dataset, it is mainly used to test the model's ability in long-term and large attack detection.\n"
                },
                "subsection 10.3": {
                    "name": "EdgeIIoT",
                    "content": "\n  The IoT data in this dataset are generated from various IoT devices (more than 10 types) such as Low-cost digital sensors for sensing temperature and humidity, Ultrasonic sensors, Water level detection sensors, pH Sensor Meters, etc. This dataset contains fourteen attacks related to IoT and IIoT connectivity protocols, which are categorized into five threats, including, DoS/DDoS attacks, Information gathering, Man in middle attacks, Injection attacks, and Malware attacks. In addition, this dataset extracts features obtained from different sources, including alerts, system resources, logs, and network traffic, and proposes new 61 features with high correlations from $1176$ found features. Because of the complex and multiplex scenarios, this dataset is used to test the model ability of multi-layer attack detection.\n"
                },
                "subsection 10.4": {
                    "name": "NF-UNSW-NB15-v2",
                    "content": "\nA NetFlow-based dataset released in 2021 contains nine attack scenarios; Exploits, Fuzzers, Generic, Reconnaissance, DoS, Analysis, Backdoor, Shellcode, and Worms. This dataset was released in 2021 and widely used NIDS dataset in the research community. Since there is no timestamp column in this dataset, we generate the column by using a random number between 0 and $10\\%$ duration time. \n"
                },
                "subsection 10.5": {
                    "name": "NF-CSE-CIC-IDS2018-v2",
                    "content": "\nThis dataset is generated in the same research of NF-UNSW-NB15-v2, including 6 attack types, this dataset also has no timestamp, we generated this column in the same way as NF-UNSW-NB15-v2.\n% \\iffalse\n\n% \\fi\n"
                }
            },
            "section 11": {
                "name": "Baselines",
                "content": "\n\\label{A:Baselines}\nWe include more details of the baselines below and the public source codes of baselines can be available at the following URLs.\n",
                "subsection 11.1": {
                    "name": "Rule-based approaches",
                    "content": "\n        \\begin{itemize}\n            \\item \\textbf{ML}: A machine learning-based network intrusion detection system.\n            \\item \\textbf{AdaBoost}: A robust distributed intrusion detection system for the edge of things.\n            \\item \\textbf{Logistic Regression}: A linear regression analysis model.\n        \\end{itemize}\n"
                },
                "subsection 11.2": {
                    "name": "Static graph approaches",
                    "content": "\nIn static graph approaches models or embedding vectors are trained on the static graph formed before the testing time, regardless of the temporal information.\n        \\begin{itemize}\n\t    \\item \\textbf{GAT}: An extension of the standard attention mechanism, allowing the model to focus on specific edges and nodes in the graph, depending on their importance for a given task. \n            \\item \\textbf{E-GraphSAGE}: A GNN model on static graphs, which supports inductive representation learning on graphs. \n            \\item \\textbf{DMGI}: A multi-layer graph based on matrix factorization techniques, it learns embedding to reconstruct the adjacency matrix and fill in the missing entries. \n            \\item \\textbf{SSDCM}: A novel semi-supervised approach for structure-aware representation learning on multiplex networks.\n        \\end{itemize}\n\n"
                },
                "subsection 11.3": {
                    "name": "Continuous time dynamic graph approaches",
                    "content": "\nContinuous time dynamic graph approaches use a continuous method to extract temporal information during the evolution of the graph, rather than a discrete method like snapshots.\n        \\begin{itemize}\n          \\item \\textbf{TGN}: An efficient framework for deep learning on dynamic graphs represented as sequences of timed edges. \n\t   \\item \\textbf{EULER}: A framework consists of a model-agnostic graph neural network stacked upon a model-agnostic sequence encoding layer such as a recurrent neural network. \n\t   \\item \\textbf{AnomRank}: An online algorithm for anomaly detection in dynamic graphs. \n\t   \\item \\textbf{DynAnom}: An efficient framework to quantify the changes and localize per-node anomalies over large dynamic weighted graphs.\n        \\end{itemize}\n   \n"
                },
                "subsection 11.4": {
                    "name": "Non-graph-based approaches",
                    "content": "\nNon-graph-based approaches are the basic approaches applied to structured data, without considering topological information in the graph.\n        \\begin{itemize}\n        \\item \\textbf{MStream}: A streaming multi-aspect data anomaly detection framework that can detect unusual group anomalies as they occur, in a dynamic manner. \n\t    \\item \\textbf{MLP}: A type of neural network model that is composed of multiple layers of interconnected artificial neurons. \n        \\end{itemize}\n%The effect of the hyper-parameter is shown in Figure x respectively. We find when we adjust the dimension nearby 170 and the depth nearby 7, the model performed well, and the ROC-AUC increases to 0.9 much higher than other parameter combinations.\n"
                }
            },
            "section 12": {
                "name": "Implementation Details",
                "content": "\n\\label{sec:Implementation Details}\nFor all datasets, we use the Adam optimizer with a learning rate of $0.01$, the learning rate scheduler reducing rate as $0.9$,  weight decay as $1e^{-5}$, and early stopping with the patience of $5$. The batch size is $200$  for training, validation, and testing. For ODE-specific hyperparameters, we adopt the Runge-Kutta method of order 5 with adaptive steps. Additional hyperparameters are reported in Table \\ref{A-B}. All experiments and timings are conducted on a machine with Intel Xeon Gold $6330$@ $2.00$GHz, RTX$3090$ GPU, and $24$G memory. The results are averaged over five runs. The code will be made available for all our experiments to be reproduced.\n\\par  For all non-graph-based methods, the node embedding dimension is set to $128$, which tends to perform empirically. For all GNN-based methods, the number of layers, the node embedding dimensions, and the learning rates are set to the same as our model 3D-IDS. For multi-layer methods, the $l2$ coefficient is searched from the range of \\{$0.0001$,$0.001$,$0.1$,$1$\\}. Other hyper-parameters are chosen empirically, following guidance from the literature. Another notable setting is that we set the node embedding as the average of the edge embedding if node embedding is not used in the method.\n"
            },
            "section 13": {
                "name": "Data preparation",
                "content": "\n",
                "subsection 13.1": {
                    "name": "Feature Selection",
                    "content": "\n\\label{Feature Selection}\nFirst, we drop the unnecessary column flow-id, then We select all other features in datasets CIC-BoT-IoT and CIC-ToN-IoT. For dataset EdgeIIoT, we copy the UDP port and address and paste them in place of the IP port and address, ARP source and destination are seen as two features in each flow between node $0$ and $0$, in order to simplify the training process. Besides, some of the data in column \"frame. time\" may be wrongly occupied by data in \"ip.src\\_host\", thus, all the features are misplaced. We delete all the rows like this.\n"
                },
                "subsection 13.2": {
                    "name": "Layer Number Generation",
                    "content": "\n\\label{Multi-Layer Construstion}\nFirst, we select the addresses of conventional router addresses or other intermediate devices such as $192.168.0.1$ in the datasets. Second, since the intermediate devices are usually stable devices on two sides of the connection, we select IPs that  have a large number of connections and connect the IP and some stable IPs. Then, we pick out the IPs involved in the man-in-the-middle attack, some of these IPs are intermediate devices themselves, and the other devices become the middleman by spoofing other devices, they exchange between two devices, these kinds of devices also satisfy the conditions of intermediate devices. Finally, we add two new columns of \"source/destination layer number\" to the dataset and mark the layer number of the above IP as $2$, and the other IPs as $1$.\n"
                },
                "subsection 13.3": {
                    "name": "Node Conversion",
                    "content": "\n\\label{Node Conversion}\nSince our model needs a unique identification to represent each node in the graph, we create this function to convert each address like \"IP: port\" to a number in the range of $0$ to the number of unique nodes. First, we use Algorithm \\ref{Address Conversion} to convert addresses into a number $r$. After converting all the addresses, we get a list of $r$. Next, we sort the list and delete the repetitive elements. Then we use the index of each element to replace the element itself. Finally, we get the node identification list. \n\\begin{algorithm}\n\\caption{Address Conversion}\n\\label{Address Conversion}\n\\KwIn{IP address (string), port (integer)}\n\\KwOut{node identification (integer)}\n// $bin(x)$ function returns the binary version of a specified integer $x$.\\;\n//$int(x,y)$ function turn string $x$ into decimal number with base-$y$ numeral system and return.\\;\n//$zfill(x,y$) function adds zeros at the beginning of the string $x$, until it reaches the specified length $y$.\\;\n$r = $ EMPTY STRING\\;\n $l =$ Split the IP address by \".\"\\;\n\\For{$i \\in l$}{\n$i = zfill(bin(i),8)$\\;\n$r \\leftarrow i$\\;\n}\n$port = zfill(bin(port),16)$ \\;\n$r \\leftarrow port$\\;\n$r = int(r,2)$\\;\nreturn $r$\\;\n\\end{algorithm}\n"
                },
                "subsection 13.4": {
                    "name": "Other Data Processing",
                    "content": "\nAfter \\ref{Feature Selection}, \\ref{Multi-Layer Construstion} and \\ref{Node Conversion}, we adjust the order of the columns to \"source, destination, timestamp, label, source layer, destination layer, duration time, and other features\" so that the training process can be easier. Next, we transform the timestamp into a unified format (integer time sequence). Finally, we sort the dataset by column timestamp, since we need the flow data to be read in by time.\n\t% \\begin{algorithm} \t\n\t% \t\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n\t% \t\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n\t% \t\\caption{3D-IDS Learning and Prediction}\\label{3D-IDSalgorithm}\\label{algorithm}\n\t% \t\\begin{algorithmic}\n %        \\Require\\\\ %换行\n\t% \tRaw network traffic sequences $\\mathbf{X}_L$, traffic labels $\\mathbf{Y}_L$, loss trade-off, and other hyper-parameters.\n\t% \t\\Ensure\\\\\n\t% \tLabel predictions of target traffic $\\mathbf{\\hat{Y}}_L$, optimized model parameters $\\Theta$.\t\n\t% \t\\end{algorithmic} \n  \n\t% \t\\begin{algorithmic}[1] %显示行号\n\t% \t\\State {\\textbf{Initialization:}}\n\t% \t\\State{\\indent $best\\_norm =  + \\inf $};\n\t% \t\\State{\\textbf{for} $m \\leftarrow 0$ \\textbf{to} $M$ \\textbf{do}}\n\t% \t\\State{\\indent playplaydancedance};\n\t% \t\\State{\\indent Focus on};\n\t% \t\\State {\\textbf{end}}\n\t% \t\\end{algorithmic}\n\t% \\end{algorithm}\n % \\iffalse\n"
                }
            },
            "section 14": {
                "name": "Hyper-parameter Range",
                "content": "\n\\label{A-B}\n\n% \\fi\n\n \\iffalse\n\n% \\fi\n\n"
            },
            "section 15": {
                "name": "General Hyper-parameter Analyze",
                "content": "\n\\label{sec:general-hyper-parmeter-analyze}\n% \\par We use the F1-Score as the main metric to evaluate the effect of the hyper-parameter and present the results on the three datasets in Figure \\ref{hyperparameter}. The best-performed hyper-parameters are shown in Table \\ref{para}.\n\nWe conduct experiments to analyze the impacts of eight hyper-parameters in 3D-IDS, including $W_{min}$, $W_{max}$, $B$, Memory Dimension, Message Dimension, Embedding Dimension, Hidden Units, and Hidden Channels. We use the F1-score as the main metric to evaluate the effect of these hyper-parameters. The best-performed hyper-parameters are shown in Table \\ref{para}.\n\nFor $W_{min}$,  $W_{max}$, and B, we can find that $0$, $1$, and $1$ are the optimal values which show the range of $[ 0,1] $  is enough to represent the property of each dimension and disentangle the embedding. For Memory Dimension, Message Dimension, Embedding Dimension, Hidden Units, and Hidden Channels, we can find that $150$, $100$, $100$, $64$, and $256$ are the optimal values. And a common phenomenon found is that the performance of our model first increases and then decreases as these hyper-parameters increases. The reason is that 3D-IDS needs a suitable dimension to encode the informative representation, and a larger dimension may introduce additional redundancies.\n\n% We conduct experiments to analyze the impacts of ten hyperparameters in 3D-IDS, for the dynamic part including Memory Dimension, Node Embedding Dimension, Time Embedding Dimension, Attention Heads, Dropout, $\\alpha$, and $\\beta$; for the multi-layer part including Hidden Units, Patience, Attention Heads, $\\epsilon$, GCN layers. In Table \\ref{E-8}, we give the details of the hyper-parameter range search for two parts of 3D-IDS.\n\n\n\\fi\n\n"
            },
            "section 16": {
                "name": "Algorithm Pseudo-Code",
                "content": "\n\\begin{algorithm}\n% \\begin{algorithmic}[1]\n\\caption{3D-IDS Learning and Prediction}\\label{3D-IDSalgorithm}\n\\KwIn{Raw network traffic sequences $\\mathbf{X}_L$, traffic labels $\\mathbf{Y}_L$, \\quad loss trade-off, and other hyper-parameters.}\n\\KwOut{Label predictions of target traffic $\\mathbf{\\hat{Y}}_L$, optimized model parameters $\\Theta$}\n\\For{a batch $\\in$ training set}{\n%怎么对齐呢\nTransform traffic sequences to edges via Eq.(\\ref{eqa}).\n\nDisentangle transformed edge features via solve Eq.(\\ref{eqb}).\n\nGenerate message via Eq.(\\ref{eqc}).\n\nUpdate the node embedding\n\n% and  Eq.(\\ref{eqd}).\nUpdate the historical memory via Eq.(\\ref{eqd}).\n\nCalculate the Disentangle loss $\\mathcal{L}_{\\text{Dis}}$ via Eq.(\\ref{eqe}).\n\nAggregate the spatial-temporal dependencies in multi-layer networks via Eq.(\\ref{eqf}) and Eq.(\\ref{eqk}).\n\nCalculate the Time Smooth loss $\\mathcal{L}_{\\text{Smooth}}$ via Eq.(\\ref{eqg}).\n\nPredict Label of target traffic $\\mathbf{\\hat{Y}}_L$ via a two layers MLP.\n\nCalculate the Intrusion loss $\\mathcal{L}_{\\text{Int}}$ via Eq.(\\ref{eqh}).\n\nAdd $\\mathcal{L}_{\\text{Int}}$, $\\mathcal{L}_{\\text{Smooth}}$ and $\\mathcal{L}_{\\text{Dis}}$ via Eq.(\\ref{eqj}).\n% \\dontprintsemicolon\n\nBackpropagate the gradients and update the learned parameters $\\Theta$ with Adam optimizer.\n}\n\\end{algorithm}\n\\iffalse\n"
            },
            "section 17": {
                "name": "Multi-layer Diffusion",
                "content": "\nThe result of multi-layer diffusion on one dimension of an edge is Figure \\ref{diffresult}, where the input is $\\nabla{h_i}$, and the output is $\\partial_t h$.\nIt shows that when $\\nabla{h_i}$ is small, we preserve the value and expect to exchange information, while when $\\nabla{h_i}$ is significant, we shrink the value and limit them to sharing information. And the trends of $\\sigma(x)$ in each dimension could be controlled by the transformation matrix. When there are multiple dimensions and neighbors, our proposed method could selectively aggregate more from similar neighbors and less from dissimilar neighbors in each dimension of disentangled embedding between nodes. As a result, our proposed method can capture the spatial-temporal dependencies in the multi-layer network and refine the traffic embedding for better intrusion detection. \n\n\\fi\n\n"
            },
            "section 18": {
                "name": "Different Intrusion Detection Methods",
                "content": "\nFigure \\ref{methodcom} shows some basic intrusion detection methods. We mark the anomaly network traffic as red. Figure \\ref{methodcom} (a) ignores the network's spatial-temporal correlations. Figure \\ref{methodcom} (b), Figure \\ref{methodcom} (c) only focused on one temporal and spatial information aspect. Figure \\ref{methodcom} (d) neglects the continuity of network traffic and processes the spatial-temporal correlations decoupled. \n\\label{A-A}\n\n"
            },
            "section 19": {
                "name": "Distributions of all Features",
                "content": "\n\\label{sec:distribution-of-all-features}\nIn this section, we draw the features distribution of injection (row 1, 2) and DDoS attack (row 3, 4). The results are shown in Figure \\ref{fig:attack-distribution}. The figures are arranged such that the injection and DDos pair appear in the corresponding position in their groups. The distribution of \\ref{fig:attack-distribution} (a) and \\ref{fig:attack-distribution} (g) are nearly identical, which means these two attacks are similar in these $5$ features. Other pairs have different quantities of peaks and different places, which means these features may be detected in the model.\n\n\n\n"
            },
            "section 20": {
                "name": "REPRESENTATION",
                "content": "\n\\label{sec:representation}\nFigure \\ref{cormap2} gives the correlation maps of representations of six types of attacks, including DDoS, DoS, scanning, injection, backdoor, and MITM. Under comparison, we observe that easily recognized attacks such as DDoS and Dos tend to be light-colored in the correlation map. These types of attacks with a more uneven color distribution in the maps are often difficult to identify. This illustrates that the coupled traffic features affect the results of the detection accuracy.\n\n\n\n\n\n% \\fi\n\n\n% \\section{Input data and output results}\n"
            },
            "section 21": {
                "name": "The Input and output ",
                "content": "\n\\label{sec:Input-data-and-ouput-results}\nThe input instances include statistical features of network traffic, including protocol, flow duration, etc. The output results include confidence scores of the binary labels (benign or abnormal) and multi-class labels (attack labels) for each input. Here we provide an example as follows:\n\n\n% \\section{Obtainment of Eq.5}\n"
            },
            "section 22": {
                "name": "More Details about Eq.5",
                "content": "\n\\label{sec:Obtainment-of-Eq.5}\n% \\iffalse\nWe start by disentangled features' required properties and obtain Eq.5, and  Eq.5 is \n\n% \\begin{equation}\n% \\tilde{\\mathbf{w}} = \\arg \\max \\biggl(\\mathbf{w_\\textit{N}} \\mathcal{F_\\textit{N}}-\\mathbf{w_\\textit{1}} \\mathcal{F_\\textit{1}}-\\sum_{i=2}^{N-1} \\biggl|2 \\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}}-\\mathbf{w_\\textit{i+1}} \\mathcal{F_\\textit{i+1}}-\\mathbf{w_\\textit{i-1}} \\mathcal{F_\\textit{i-1}}\\biggr|\\biggr)\n% \\end{equation}\n\n\\begin{equation}\n\\resizebox{0.45\\textwidth}{!}{$\n    \\tilde{\\mathbf{w}} = \\arg \\max \\biggl( \\mathbf{w_\\textit{N}} \\mathcal{F_\\textit{N}} - \\mathbf{w_\\textit{1}} \\mathcal{F_\\textit{1}} - \\sum_{i=2}^{N-1} \\biggl| 2 \\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}} - \\mathbf{w_\\textit{i+1}} \\mathcal{F_\\textit{i+1}} - \\mathbf{w_\\textit{i-1}} \\mathcal{F_\\textit{i-1}} \\biggr| \\biggr)\n$}\n\\end{equation}\n\nSpecifically, the first term $\\mathbf{w_\\textit{N}} \\mathcal{F_\\textit{N}}-\\mathbf{w_\\textit{1}} \\mathcal{F_\\textit{1}}$ represents the full range of the feature distributions, and we aim to maximize this range to get a larger area to separate the features in distributions.\n\nThe second term can be rewritten as \n\n$\\sum_{i=2}^{N-1} \\biggl| \\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}}-\\mathbf{w_\\textit{i+1}} \\mathcal{F_\\textit{i+1}}-(\\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}}-\\mathbf{w_\\textit{i-1}} \\mathcal{F_\\textit{i-1}})\\biggr|$. The $\\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}}-\\mathbf{w_\\textit{i+1}} \\mathcal{F_\\textit{i+1}}-(\\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}}-\\mathbf{w_\\textit{i-1}} \\mathcal{F_\\textit{i-1}})$ \n\nrepresents the distance between i-neighboring features. We expect this term to be small so that these features can be spread evenly over the range. In this way, according to the second term, the distance between two features can be represented as $L/(N-1)$, where the first term ensures the total range $L$ in dimension is maximum, and $N$ is the number of features. When $L$ is maxed by Eq.5, the distance between i-neighboring features will be the largest, then we can obtain the well-separated disentangled features.\n% \\fi\n%\\cor{Adding 3B8n A2}\n\nEq.5 in the main paper can be expressed as follows:\n% $$\n% \t\\begin{array}{c}\\tilde{\\mathbf{w}}=\\arg \\max \\left(\\mathbf{w_\\textit{N}} \\mathcal{F_\\textit{N}}-\\mathbf{w_\\textit{1}} \\mathcal{F_\\textit{1}}-\\sum_{i=2}^{N-1}\\left|2 \\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}}-\\mathbf{w_\\textit{i+1}} \\mathcal{F_\\textit{i+1}}-\\mathbf{w_\\textit{i-1}} \\mathcal{F_\\textit{i-1}}\\right|\\right) \\\\\\\\\n%     =\\arg \\min \\left(-L+\\sum_{i=2}^{N-1}\\left|d_{i+1}-d_{i}\\right|\\right) \\\\\\\\\n%     =\\arg \\min \\left(\\frac{\\sum_{i=2}^{N-1}\\left|d_{i+1}-d_{i}\\right|}{N-1}-\\frac{L}{N-1}\\right),\\end{array}\n% $$\n\n\\begin{equation}\n\\resizebox{0.45\\textwidth}{!}{$\n    \\begin{aligned}\n        \\tilde{\\mathbf{w}} &= \\arg \\max \\left( \\mathbf{w_\\textit{N}} \\mathcal{F_\\textit{N}} - \\mathbf{w_\\textit{1}} \\mathcal{F_\\textit{1}} - \\sum_{i=2}^{N-1} \\left| 2 \\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}} - \\mathbf{w_\\textit{i+1}} \\mathcal{F_\\textit{i+1}} - \\mathbf{w_\\textit{i-1}} \\mathcal{F_\\textit{i-1}} \\right| \\right) \\\\\n        &= \\arg \\min \\left( -L + \\sum_{i=2}^{N-1} \\left| d_{i+1} - d_{i} \\right| \\right) \\\\\n        &= \\arg \\min \\left( \\frac{\\sum_{i=2}^{N-1} \\left| d_{i+1} - d_{i} \\right|}{N-1} - \\frac{L}{N-1} \\right)\n    \\end{aligned}\n$}\n\\end{equation}\nwhere $d_i$ refers to the distribution distance of the neighboring features \n$\\mathcal{F_\\textit{i}}$ and $\\mathcal{F_\\textit{i+1}}$. Intuitively, Eq.5 aims to maximize the distribution distance between every two features and encourages these features to be evenly distributed in the range for better disentanglement. Therefore, we use Eq.5 as our optimization objective.\nAnother form of the objective can also be expressed as:\n$$\\operatorname{argmin}\\left(\\frac{\\sum_{i=1}^{N-1}\\left|\\mathrm{~d}_{i}\\right|}{\\mathrm{N}-1}-\\frac{\\mathrm{L}}{\\mathrm{N}-1}\\right)\\left(5^{*}\\right).$$\n\nThe difference between the above two expressions lies in the first terms $\\frac{\\sum_{i=2}^{N-1}\\left|d_{i+1}-d_{i}\\right|}{N-1}$ and $\\frac{\\sum_{i=1}^{N-1}\\left|d_{i}\\right|}{N-1}$. Our Eq.5 in the main paper learns to shape each distance $d_i$ to the optimal distance $\\frac{L}{N-1}$, while the latter Eq.5* learns to shape the average distance $\\frac{\\sum_{i=1}^{N-1}\\left|d_{i}\\right|}{N-1}$ to the optimal $\\frac{L}{N-1}$.\n\nWe conduct the following experiment to show the slight performance differences between the above two expressions as Table \\ref{two eq1} and Table \\ref{two eq2}.\n\n\n% \\iffalse\n\n% \\fi\n%如果空间足够放实验的话,这个EQ5的顺序应该放在C之后\n% Table \\ref{twometric} shows the overlap ratio on the dataset CIC-ToN-IoT after shifting using the above two constraints.\n% \\begin{table}[b]\n%     \\caption{Average overlap ratio of the feature distributions before and after two kinds of \"shifting\".}\n%     \\label{twometric}\n%     \\centering\n%     \\begin{tabular}{@{}cccc@{}}\n%     \\toprule\n%      Dataset &Before  &After(1)  &After(2) \\\\ \\midrule\n%      CIC-ToN-IoT  &62.37\\%  &24.16\\%  &33.32\\%\\\\ \\bottomrule\n%     \\end{tabular}\n% \\end{table}\n"
            },
            "section 23": {
                "name": "Disentanglement Studies",
                "content": "\n",
                "subsection 23.1": {
                    "name": "Shifting the entangled distribution",
                    "content": "\nTo show that similar feature distributions contribute to lower detection performance, we involved all traffic features for \"manual shifting\" in the controlled experiments and reported the results in Table \\ref{shift}.\n\nTo quantify the overlap among features that exist at the beginning and after shifting, we define an average overlap ratio among features. The ratio can be expressed as follows:\n$$\n\\frac{1}{N^{2}} \\sum_{i, j}\\left(\\frac{\\text { overlap}_{i,j}}{\\operatorname{man}(\\text {range}_i, \\text {range}_j)}\\right), \\forall i, j \\in[1, N],\n$$\nwhere $overlap_{i,j}$ indicates the overlap distance between the min-max normalized feature $i$ and feature $j$. Here the symbol $range_i$ refers to the distribution range of feature $i$, and it can be computed by $[\\min (0, \\mu-3 \\sigma), \\max (\\mu+3 \\sigma, 1)]$, where $\\mu$ is the mean and $\\sigma$ is the standard deviation. $N$ is the number of features.\n\nFinally, we show that the above ratio on the dataset CIC-ToN-IoT are 62.37\\% and 24.16\\% respectively before and after \"manual shifting\", which has a significant reduction after the shifting, i.e., 38.21 points decrease.\n\n\n\n% \\iffalse\n\n% \\fi\n\n% \\iffalse\nUnder the same attack during the evaluation, the results show that the model trained on a dataset with “manual shifting” performs better than the one with a more entangled dataset distribution. The empirical results in Table \\ref{shift} show that the entangled similar distributions led to poor performance, while the shifting operation can alleviate such an issue. For example, our \"manually' shifting scheme yields 3.85 points and 3.57 points improvement for Logistic Regression and E-GraphSAGE, respectively in terms of classification accuracy under Backdoor attacks.\n% \\fi\n\n\n"
                },
                "subsection 23.2": {
                    "name": "Experiments with disentangled features",
                    "content": "\n%\\cor{Adding L6SU A1}\n\nThe proposed statistical disentanglement can be considered as an explicit automatic feature processing module, aiming to reduce redundant correlations between raw traffic features with more specific guidance.\n\nWe also applied such a module to statistical and deep learning models (E-GraphSAGE). The results in Table~\\ref{F1 score comparisons} show that our statistical disentanglement also significantly improves attack detection.\n\n%For example, the proposed disentanglement yields 9.44 points and 4.3 points improvement for Logistic Regression and E-GraphSAGE, respectively in terms of classification accuracy under Backdoor attack\n\n\n\n\n\n\n\n"
                },
                "subsection 23.3": {
                    "name": "The impact of statistical disentanglement",
                    "content": "\nThe proposed feature disentanglement aims to mitigate the redundant correlations between features. It can be considered as \"denoising\" between features or \"shifting\" between feature distribution, making the features more distinguishable for better prediction. We show that our 3D-IDS can still capture important patterns jointly contributed by multiple features during the disentanglement.\nHere is a more detailed analysis to verify our hypothesis. We conduct experiments on \"NF-UNSW-NB15-v2\" with an existing pattern that consists of three features including AVG\\_THROUGHPUT (traffic throughput), PROTOCOL (protocol), and MAX\\_TTL (packet survival time), which we denoted the id of them as A, B, and C, respectively, and the results in Table \\ref{importance} show that our 3D-IDS model has a very slight impact on the contribution of such a pattern to attack detection. In experiments, we define an importance score to quantify the impact of the pattern on the detection, and it can be computed by the data entropy variation after using a set of joint features for classification. Importance score can be expressed as:\n$$\nImportance=Entropy_{sum}-(Entropy_l+Entropy_r),\n$$\nwhere the $Entropy_{sum}$ indicates the entropy without joint features, and $Entropy_l+Entropy_r$ refers to the entropy with the joint features.\n\nTable \\ref{importance} shows the importance score without and with disentangled features, and we can observe the importance scores of the multiple joined features almost keep the same after disentanglement.\n\n\n\n\n\n"
                }
            },
            "section 24": {
                "name": "Technical Details",
                "content": "\n",
                "subsection 24.1": {
                    "name": "Technical contributions",
                    "content": "\nThis paper has first observed the inconsistent detection performance in existing NIDS and customized a doubly disentangled module to address this issue. Specifically, our key ingredients include a double-feature disentanglement scheme for modeling the general features of various attacks and highlighting the attack-specific features, respectively, and a novel graph diffusion method for better feature aggregation.\n\n"
                },
                "subsection 24.2": {
                    "name": "Ability of unknown attack detection",
                    "content": "\n%\\cor{Adding i4ga A2}\n\nThe reason 3D-IDS can generalize to unknown attacks is the \\textbf{better outlier network traffic (anomaly) detection ability}, which benefits from the proposed double disentanglement and multi-layer graph diffusion modules. The unknown attack detection can be cast as an outlier detection task. A method that can better augment and highlight the deviations in features will better detect the outlier from benign. From this perspective, 3D-IDS first automatically distinguishes hundreds of dimensional features via the proposed statistical entanglement and captures subtle spatiotemporal information via the multi-layer graph diffusion module. Our second disentanglement module guarantees each dimension of features maintains the disentangled properties during the training. By doing so, the attacks with unknown labels and each-dimension-distinguishable features can be properly detected as an outlier, thus improving the ability to generalize to unknown attacks.\n\n"
                },
                "subsection 24.3": {
                    "name": "Incorporating the new nodes",
                    "content": "\n3D-IDS first appends the new node and the corresponding new edges to the list of nodes and edges. And then we initialize the memory and the representation vector associated with the new node as a $\\mathbf{0}$ vector. After the above operations, the node will be incorporated into our methods.\n\n"
                },
                "subsection 24.4": {
                    "name": "Strategies for Data Imbalance",
                    "content": "\n%\\cor{Adding L6SU A4}\n\nData imbalance is an interesting problem to be explored, and we observe such a phenomenon slightly impacts the model performance from experiments. Here are the detailed explanations.\n\\begin{itemize}\n\\item Datasets. The public datasets we selected have balanced and imbalanced distributions, such as CIC-ToN-IoT with 47\\% Benign samples and EdgeIIoT with 92.72\\% Benign samples.\n%这里的注释，引用注意处理一下\n\\item Implementation. We use the re-weight trick for the unbalanced-class loss \\cite{lin2017focal} in our implementation, which will help improve those unbalanced sample learning. We can also utilize data sampling and data augmentation techniques to alleviate the data imbalance.\n\\end{itemize}\n%We will clarify this point in our revised version.\n\n\n%[1] Lin T Y, et al. Focal loss for dense object detection[C] CVPR 2017.\n\n% We conducted an experiment to test the hypothesis that similar feature distributions lower detection performance. We compared two kind of train methods under the same attack during the evaluation: one trained on a dataset with \"manual shifting\" and another trained on a dataset with a more entangled distribution. Table 9 shows that the entangled distribution resulted in poor performance, while the shifting operation alleviated this issue. For instance, our \"manual\" shifting scheme improved the classification accuracy under Backdoor attacks by 3.85 points for Logistic Regression and 3.57 points for E-GraphSAGE, respectively.\n\n\n\n\n"
                }
            },
            "section 25": {
                "name": "Handling False Positive Problem",
                "content": "\n\\label{sec:false-positive-comparsion}\nWe conduct experiments on the UNSW-NB15 dataset to show the false positives rate. We observe that the false positives rate of our ID-3DS can be as low as 3.27, while the values of the previous methods MLP and E-GraphSAGE are 16.82 and 8.29, respectively. We attribute such a gain to the double disentanglement that can highlight attack-specific features, and thus these more differentiable features help to model the to reduce the false positives.\n\n\nIn addition to model design, the high false positive problem can be seen as a trade-off between false positives and false negatives. We can introduce a learnable parameter to control the tolerance for suspected benign samples according to real-world scenarios.\n\n"
            },
            "section 26": {
                "name": "Real time detection",
                "content": "\n\\label{sec:real-time-detection}\nOur 3D-IDS can be applied to real-time attack detection due to the following reasons.\n\\begin{itemize}\n    \\item Training\\&Testing. 3D-IDS supports offline model training and online model detection. When the model has been well-trained with training datasets (e.g., the training frequency can be once a week), the trained model can conduct real-time online detection for new observations.\n    \\item Datasets. As an automatic deep model, 3D-IDS supports different data with features. For real-time anomaly detection in a specific scene, we can use the scene's historical (domain-specific) data as the training set. If the application scenario is similar to our network traffic anomaly detection, the datasets in our paper can be trained for real-time detection.\n    \\item Efficiency. We conduct a real-time detection on RTX 3090 GPU and report the detection number per minute and accuracy, as shown in Table \\ref{realtime}, indicating the potential of 3D-IDS to extend to real-world detection.\n    \n\\end{itemize}\n\n\n\n"
            },
            "section 27": {
                "name": "Denoising algorithm of MFL",
                "content": "\n\n\\label{denoising}\n\\begin{algorithm}\n\\footnotesize\n  %\\textsl{}\\setstretch{1.8}\n  \\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n  \\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n  \\caption{The generation algorithm of $S^*$}\n  \\label{alg1}\n  \\begin{algorithmic}[1]\n \\REQUIRE Coefficient matrix $Q$, subspaces dimension $K=4$, rank $r=Kd+1$, where $d$ is the number of node classes.\n \\ENSURE  Latent structure $\\mathbf S^*$.\n\\STATE \\textbf{Initialization}: $Q^{\\prime}=\\frac{1}{2}\\left(Q+Q^{T}\\right)$.\n\\STATE \\textbf{Compute}: the $r$ rank \\textbf{SVD} of $Q^{\\prime}$ via $Q^{\\prime}=U \\Sigma V^{T}$.\n\\STATE \\textbf{Compute}: $L=U \\Sigma^{\\frac{1}{2}}$ and  normalize each row of $L$.\n\\STATE \\textbf{Update}: $L^{\\prime}$ $\\leftarrow$ set the negative values in $L$ to zero.\n\\STATE \\textbf{Obtain}: $S^*=\\left(L^{\\prime}+\\right.$ $\\left.L^{\\prime T}\\right) /\\|L\\|_{\\infty}$, where $s_{i j} \\in[0,1]$.\n\\end{algorithmic}\n\\label{algo}\n\\end{algorithm}\n\n\n\n% \\begin{figure}[ht]\n% \t\\centering  %图片全局居中\n%         \\setlength{\\abovecaptionskip}{5pt}  %图像在caption上方距离\n% \t\\includegraphics[width=0.95\\linewidth]{picture/page2/pic2/bot.pdf}\n%    \\caption{Linear relationship between the disentangled edge vector and the disentangled node representation.}\n%  \\label{seconddis}\n% \\end{figure}\n\n% \\begin{figure}[ht]\n% \t\\centering  %图片全局居中\n%         \\setlength{\\abovecaptionskip}{5pt}  %图像在caption上方距离\n% \t\\includegraphics[width=0.95\\linewidth]{picture/page2/pic2/cse.pdf}\n%    \\caption{Linear relationship between the disentangled edge vector and the disentangled node representation.}\n%  \\label{seconddis}\n% \\end{figure}\n\n% \\begin{figure}[ht]\n% \t\\centering  %图片全局居中\n%         \\setlength{\\abovecaptionskip}{5pt}  %图像在caption上方距离\n% \t\\includegraphics[width=0.95\\linewidth]{picture/page2/pic2/ton.pdf}\n%    \\caption{Linear relationship between the disentangled edge vector and the disentangled node representation.}\n%  \\label{seconddis}\n% \\end{figure}\n\n% \\begin{figure}[ht]\n% \t\\centering  %图片全局居中\n%         \\setlength{\\abovecaptionskip}{5pt}  %图像在caption上方距离\n% \t\\includegraphics[width=0.95\\linewidth]{picture/page2/pic2/unsw.pdf}\n%    \\caption{Linear relationship between the disentangled edge vector and the disentangled node representation.}\n%  \\label{seconddis}\n% \\end{figure}\n\n\n\n\n\n\n\\fi\n\n\n\n\\iffalse\n\\clearpage\n"
            },
            "section 28": {
                "name": "few-shot part",
                "content": "\n% since our 3D-IDS model has a good performance on several occasions, it has a long way to go to be applied to practical work. One obvious drawback is that our model doesn't perform in few-shot tasks as well as it does in normal tasks. Conspicuously, few-shot cases are more common in practical works. Though certain types of attack samples can be captured, the amounts are dramatically small. Thus, to enhance our 3D-IDS module, we propose a few-shot method to our original model. The method we use is MTL (Meta-Transfer-Learning), an implementation of the MAML model proposed in 2017. MTL model uses transfer learning in meta-learning, being able to apply a comparatively huge model to the meta-learning phase, it is the most applicable model in our task.\n",
                "subsection 28.1": {
                    "name": "related works",
                    "content": "\n",
                    "subsubsection 28.1.1": {
                        "name": "few-shot learning",
                        "content": "\n% Great diversity could be seen on the few-shot learning field.Methods we focus on and compare with ours in the experiments are the ones following supervised meta-learning paradigm.We can categorize the methods into three respective type.1)Metric learning methods do a mapping on the data onto another similarity space in which few-shot learning can be done with more efficiency(to be continued).2)memory network methods learns to reform the 'memories', or in other word ,'experiences' obtained in 'visible' tasks to appeal them in the 'invisible' target tasks. 3)Gradient descent based methods \nFew-shot learning is a task where a classifier needs to categorize samples from some new classes not seen during training and give only a few labelled examples. Previous studies focused on Metric learning methods that do a mapping of the data onto another similarity space in which few-shot learning can be done with more efficiency. Some recent approaches have applied memory network methods to reform the experiences obtained in previous tasks to apply them to the target tasks. \n(1)Most related to our work is MTL, which leverages the advantages of both transfer-learning and meta-learning and then utilizes meta-batch as the training strategy. Our new model uses a persistent homology method attempting to mine topological features in time and graph structure to support MTL.\n\n(2)Most related to our work is COSMIC, which improves the meta-learning strategy for few-shot node classification from the perspective of intra-class and inter-class generalizability of the learned node embeddings. Our 3D-IDS model leverages the advantages of both transfer-learning and meta-learning and uses a persistent homology method attempting to mine topological features in time and graph structure to support meta-learning\n\n\n\n"
                    }
                }
            },
            "section 29": {
                "name": "related work - fractal/Self-Similarity part",
                "content": "\n\n% Therefore, the network attacks can be detected accordingly based on the analysis results of self-similarity of the network traffic.\n% The network anomaly detection focuses to implement the detection algorithm and build the corresponding network anomaly detection model which is also an important factor that affects the effect of\n% abnormal behavior detection.\n\nThe self-similarity of network traffic exists everywhere and anytime, and has nothing to do with the scale structure, network type, and network service of the network.\nPrevious studies focused on the analysis and experimental verification of fractal characteristics of network traffic.\nSome recent approaches have applied the fractal characteristics of network traffic to the network traffic detection to identify abnormal traffic or network attacks.\nMost related to our work is the self-similarity model, which uses wavelet transform to perform the network traffic and then the segment traffic into sub-traffic, in which the local Hurst exponent (LHE) is estimated with the wavelet analysis and a self-similarity model is selected.\nWhile our new model takes advantage of the self-similar characteristics of network traffic, performs discrete wavelet transform on the traffic, and calculates the Hurst index to detect abnormal traffic.\n\n"
            },
            "section 30": {
                "name": "related work - topological data analysis",
                "content": "\nThe analysis of higher-order topology of complex networks can reveal the intrinsic properties and behaviors of the network. Previous network traffic data analysis considers complex higher-order data for dimensionality reduction. In contrast, recent topological data analysis carves the data through the use of persistent homology, which focuses on the study of k-dimensional holes and thus reflects important information about the topological space.Our work is influenced by a persistence graph-based clustering method for topological data. The method introduces the multiple-resolution lenses of topological data analysis into the clustering approach, thus systematically quantifying the shape dynamics of multi-layer networks on a changing similarity scale.The innovation of our approach to topological data analysis is that we improve the modeling algorithm for generating persistence graphs, making the model much faster to compute on large dense graphs.\n\n\\newpage\n\n"
            },
            "section 31": {
                "name": "francis-introduction",
                "content": "\nNIDS, known as Network Intrusion Detection System, keeps an eye on the network traffic data to alert the administrator in case of any anomaly. Recently, deep learning-based methods such as statistical methods have shown great potential in anomaly detection, which results in high-accuracy classification and outperforms the traditional anomaly detection methods. Generally, deep learning-based methods require a massive amount of historical data to train a high-accuracy model, and it results in poor performance in cases of being given a new sample. However, it costs a fortune to obtain real-time traffic data with labels to feed the model in realistic situations. This led to the emergence of meta-learning as a method to deal with the problems associated with an absence of labelled data. A meta-learning-based intrusion detection system is targeted to deal with more realistic tasks such as tackling a specific attack with only a few lines of network traffic data, also named few-shot learning.\n\nFew-shot learning methods can be approximately categorized into two classes: data augmentation and task-based ones. The former is a classical method that enlarges the proportion of available data and is thus useful for few-shot learning, other methods proposed to appeal a data generator on Gaussian noise to reform the data structure to cater to few-shot learning tasks. However, the generative models always consume quite a large amount of time, which makes them not applicable to real-time detection tasks. The latter meta-learning method learns to use formerly accumulated experiences. A SOTA method of this, namely Model-Agnostic Meta-Learning, searches for a primitive stage to fast-adapt the pre-trained base learner to a new task. While the base learner focuses on learning a single task provided with abundant labelled data.\n\nHowever, a main limitation of existing methods is they are not able to generalize a new anomaly type with few labelled samples. It’s common to witness a sharp decline in testing accuracy when it comes to a new anomaly class which has few labelled data on existing approaches. Another cause of bad performances in few-shot tasks is over-fitting, making it hard to generalize the new anomaly class. We have also done many experiments on this. Concretely, there is a main challenge in applying the experiences that our model learned from our labelled data to learn a new anomaly class. To transfer the learned knowledge to a new task, we need to rescale the weights of our model using particular mathematical methods. The parameters then could be viewed as hyper-parameters to be trained in the few-shot tasks.\n\nTo tackle the low efficiency in few-shot anomaly detection, we introduce an intrusion detection method with Meta-Transfer Learning (MTL). Specifically, we made some adjustments in the MTL procedure, so the transfer phase could perform with higher efficiency.\n\n\n\\begin{itemize}\n    \\item We apply the MTL in our model and we apply a new method to rescale the weight in the base learner in order to cater to the meta learner.\n    \n\\end{itemize}\n\n\n"
            },
            "section 32": {
                "name": "introduction--",
                "content": "\n\nComputer network is a very large and complex system, which contains a large number of nonlinear dynamic behaviors, configuration application metabolism, and some social cooperation and competitions. The interactions among them have made network traffic more and more complicated. In recent years, the analysis of network traffic has become a research hotspot in the computer field. Analyzing the characteristics of network traffic can better understand the behaviors of the network, thereby improving the performance of network operation and ensuring network security. However, there is currently no mathematical model that can accurately describe the network traffic, and some network attacks have greatly threatened the network security, which are urgently needed to be solved in current years.\n\n\nAlthough the commendable performance of our 3D-IDS model on various occasions, there remains significant room for improvement before its practical deployment. A glaring limitation is its suboptimal performance in few-shot tasks when contrasted with its performance in standard tasks. Notably, practical scenarios often entail a higher prevalence of few-shot cases. To bolster the efficacy of our 3D-IDS model, we propose incorporating a few-shot learning method into our original model. We use the few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. \n\nAt the same time, we noticed the good role of network traffic self-similarity and topological persistent homology and topological data analysis in abnormal traffic detection, and applied them to our model. We analyzed the characteristics of traffic from multiple scales and performed analytical clustering to form a multi-layer network. At the same time, we calculated the similarity index based on the self-similarity of network traffic time scales to better identify abnormal traffic.\n\n\n\nThe main contributions of this paper are as follows.\n\\begin{itemize}\n    \\item In order to process unlabeled sample data that is closer to the actual situation, we use few-shot method to our original model to improve the model and better handle different situations. It has a good performance on several occasions. \n    \n    \\item In order to better analyze and process different traffic data, we use persistent homology of topology and topological data analysis methods to analyze the characteristics of network traffic at multiple scales and identify anomalies based on the self-similarity of network traffic. \n    \n    \\item We conduct extensive quantitative and qualitative experiments on various benchmarks and provide some helpful insights for effective NIDS.\n\\end{itemize}\n\n"
            },
            "section 33": {
                "name": "introduction-tsy",
                "content": "\nIn recent years, more and more achievements and articles on topological machine learning have been published in the intersection of fields of topological data analysis (TDA) and machine learning, with the theoretical system has become more and more mature. This makes the calculation of topological features (e.g. via persistent coherence) increasingly flexible and scalable to more complex and larger data sets. This provides us with new methods in the application of Intrusion Detection System(IDS) based on unsupervised machine learning.\n\nTopology is known as encoding the overall data and its topology structure. Further more, topological features are suitable for capturing the multi-scale, global and intrinsic properties of a dataset. With the rise of TDA, this usefulness has been recognized, and topological information is now generally considered relevant to data analysis. Many works aim to exploit this information to gain fundamentally different perspectives on their datasets. We would like to focus on TDA’s recent “outgrowth” of integrated topological methods to enhance or augment classic machine learning methods and deep learning models.\n\nTo this end, we combine the network topology formed by complex computer networks with theories in the field of topological data analysis in IDS. With the usage of persistent homology, a method that can accurately describe the whole features of the data without dimensionality reduction for data processing. It can avoid data loss caused by dimensionality reduction, and can still perform efficient calculations while encoding large amounts of information.\n\n\\newpage\n"
            },
            "section 34": {
                "name": "introduction-final",
                "content": "\nToday's network environment is complex and constantly evolving, with new methods of attacks emerging frequently, including Sniffing Attack, RAT, and Trickbot. The threats we face are becoming increasingly significant. High-accuracy classification as our NIDS presents, it results in poor performance in cases of being given one or several new samples due to its’ requiring a massive amount of historical data to train. Furthermore, the high consumption of obtaining real-time traffic data with labels to feed the model in realistic situations has made matters worse. Thus, a NIDS capable of identifying new methods of attacks with only a few labels as precedent knowledge is what is needed urgently now. A main challenge for our NIDS to overcome is that our NIDS is not able to generalize a new anomaly type with few labelled samples. It’s common to witness a sharp decline in testing accuracy when it comes to a new anomaly class which has few labelled data on existing approaches. Another cause of bad performances in few-shot tasks is over-fitting, making it hard to generalize the new anomaly class. Concretely, the main problem is how can we make our NIDS capable of using the experiences it learned from labelled data to identify the new class by obtaining the features of a new anomaly class with a few labels, which is commonly seen in practical scenarios.\n\nTo tackle this problem, we first proposed some few-shot learning methods for our NIDS. A few-shot learning-based intrusion detection system is targeted to deal with more realistic tasks such as tackling a specific attack with only a few lines of network traffic data, also named few-shot learning. Few-shot learning methods can be approximately categorized into two classes: data augmentation and task-based ones. The former is a classical method that enlarges the proportion of available data and is thus useful for few-shot learning, other methods proposed to apply a data generator on Gaussian noise to reform the data structure to cater to few-shot learning tasks. However, the generative models always consume quite a large amount of time, which makes them not applicable to real-time detection tasks. The latter meta-learning method learns to use formerly accumulated experiences. A SOTA method of this, namely Model-Agnostic Meta-Learning, searches for a primitive stage to fast-adapt the pre-trained base learner to a new task. While the base learner focuses on learning a single task provided with abundant labelled data. We had done several experiments, using MAML and MTL structure as well. To fit our tasks, some adjustments have been made to the MTL and MAML structure, to transfer the learned knowledge to a new task, we need to rescale the weights of our model using particular mathematical methods and the parameters then could be viewed as hyper-parameters to be trained in the few-shot tasks. However, the results were not as good as we expected them to be (some pictures). The results show that NIDS still has a problem of over-fitting even with meta-learning modules. Apparently, there are some features, contributing to the few-shot classification tasks, that our meta-NIDS failed to make use of.\n\n% [Part4] TDA \\& Persistent Homology\n% Furthermore, we can see a computer network has its own topological structure. Adding and removing nodes (computers in our work) will take a mild change of the original topological structure, which is easy to detect and analyze through methods of topological analysis machine learning and persistent homology.\n\n% Since then, we have introduced TDA methods are promising as a means of enhancing the performance of machine learning methods. Instead of feeding a massive amount of data into the machine learning algorithm, TDA can identify a smaller set of relevant topological features, which can be used as our traffic data input into a simpler, faster, and more accurate machine learning model. Especially, TDA methods seem naturally suited to studying phenomena such as fractals, which is very helpful to our work on identifying network traffic on small-scale node sets. \n\nThus, we have applied the persistent homology method to our existing model, a typical TDA method. TDA (Topological Data Analysis) methods are invented to study features hidden behind the graph’s topological structures, such as certain patterns on a small scale or some high-dimensional data features. In our model, we first extract the topological features of each class in the network flow data through the persistent homology and use a specific encoder to transfer the features into an embedding in our pre-training phase. We have done an all-class classification in our pre-training phase, which we’ll demonstrate in our model part. The embeddings of each class, carrying the features extracted from a comparatively larger amount of data, will then be an extra input along with the data itself to be fed into our model in the meta-training phase to update the parameters in our model. \n\n\n\n\n\n\n\n"
            },
            "section 35": {
                "name": "premilinaries",
                "content": "\n"
            },
            "section 36": {
                "name": "Formal Definition of Few-shot Node Classification",
                "content": "\n\nIn this section, we provide the formal definition for the problem of few-shot node classification. We first denote an input attributed graph as $G = (V, E, X)$, where $V$ is the set of nodes, and $E$ is the set of edges. $X \\in \\mathbb{R}^{|V| \\times d}$ denotes the node feature matrix, where $d$ is the feature dimension. Furthermore, the entire set of node classes is denoted as $C$, which can be further divided into two disjoint sets: $C_{\\text{tr}}$ and $C_{\\text{te}}$, i.e., the sets of meta-training classes and meta-test classes, respectively. Specifically, $C = C_{\\text{tr}} \\cup C_{\\text{te}}$ and $C_{\\text{tr}} \\cap C_{\\text{te}} = \\emptyset$. It is noteworthy that the number of labeled nodes in $C_{\\text{tr}}$ is sufficient for meta-training, while it is generally small in $C_{\\text{te}}$ [7, 15, 24, 57].\n\nIn this way, the studied problem of few-shot node classification is formulated as follows:\n\n\\textbf{Definition 1. Few-shot Node Classification:} Given an attributed graph $G = (V, E, X)$ and a meta-task $T = \\{S, Q\\}$ sampled from $C_{\\text{te}}$, our goal is to develop a learning model such that after meta-training on labeled nodes in $C_{\\text{tr}}$, the model can accurately predict labels for the nodes in the query set $Q$, where the only available reference is the limited labeled nodes in the support set $S$.\n\nMoreover, under the $N$-way $K$-shot setting, the support set $S$ consists of exactly $K$ labeled nodes for each of the $N$ classes from $C_{\\text{te}}$, and the query set $Q$ is also sampled from the same $N$ classes. In this scenario, the problem is called an $N$-way $K$-shot node classification problem. Essentially, the objective of few-shot node classification is to learn a model that can well generalize to meta-test classes in $C_{\\text{te}}$ with only limited labeled nodes as the reference.\n\n\\newpage\n\n\n"
            },
            "section 37": {
                "name": "models",
                "content": "\nThe MTL structure consists of two phases, which is similar to the classic two stages in other meta-learning structures. \n\n\\textbf{1)the pre-learning phase:}\n In this phase, we first merge all classes of samples into one as an all-class classifier. It is important to note that in the pre-learning phase, we first utilize our Multi-Layer Graph Diffusion module to generate node representations. The node embedding is then transferred into our two-step prediction module and then the whole model is optimized by SGD(Stochastic Gradient Descent) as follows, \n \\vspace{-8pt}\n\\begin{equation}\n[\\Theta; \\theta] =: [\\Theta; \\theta] - \\alpha \\nabla \\mathcal{L}_D\n\\end{equation}\nwhere $\\mathbf{L}$ denotes the following loss.\n\n\\vspace{-8pt}\n        \\begin{equation} \n        %-的符号\n             \\mathcal{L}_{\\mathcal{D}}([\\Theta; \\theta])=\\frac{1}{|\\mathcal{D}|}\\sum_{(x,y)\\subset\\mathcal{D}} l(f_{[\\Theta; \\theta]}(x),y)        \n        \\end{equation}\nIn this section, Theta is what we learned and will remain until the meta-learning phase. Meanwhile, the classifier theta will become useless for the following task, which is a few-shot one containing different classification objectives, e.g., 5-class classification.\n\n\\textbf{2)the meta-learning phase:}\nAt first, we do the SS(Scaling and Shifting) operations, which is to optimize the current classifier $\\mathcal{\\theta'}$ by gradient descent using the loss of training task $\\mathcal{T}^{tr}$:\n\\begin{equation}\n    \\theta' \\leftarrow \\theta - \\beta \\nabla_{\\theta} \\mathcal{L}_{T^{(\\text{tr})}}([\\Theta; \\theta],\\Phi_{S_{\\{1,2\\}}}),\n    \\label{meta1}\n\\end{equation}\n\nAs we don’t update $\\mathbf{\\Theta}$. It is noted that the  $\\mathcal{\\theta}$ here is not the same one in the previous phase.  $\\mathcal{\\theta'}$ refers to a temporal one only available in this task, initialized by the previously optimized  $\\mathcal{\\theta}$:\n\\begin{equation}\n    \\theta =: \\theta - \\beta \\nabla_{\\theta} \\mathcal{L}_{T^{(\\text{te})}}([\\Theta; \\theta'],\\Phi_{S_{\\{1,2\\}}}),\n    \\label{meta2}\n\\end{equation}\n\nThe scalars we used in equation\\eqref{meta1} and equation\\eqref{meta2} $\\mathbf{\\Phi_{S_{1}}}$ and  $\\mathbf{\\Phi_{S_{2}}}$ are initialized respectively by ones and zeros, they are then optimized by the loss of $\\mathcal{T}^{te}$:\n\n\\begin{equation}\n    \\Phi_{Si} =: \\Phi_{Si} - \\gamma \\nabla \\Phi_{Si} \\mathcal{L}_{T^{(\\text{te})}}([\\Theta; \\theta'],\\Phi_{S_{\\{1,2\\}}}),\n\\end{equation}\n\nNext, we apply $ \\mathbf{\\Phi_{S_{\\{1,2\\}}}}$ to the neurons that remained in the previous phase, which are those in the former frozen feature extractor $\\mathbf{\\Theta}$. We learned $K$ pairs of scalars $ \\{\\mathbf{\\Phi_{S_{\\{1,2\\}}}}\\}$and applied them to the weight $W$ and the bias $b$:\n\\begin{equation}\n    SS(X; W, b; \\Phi_{S_{\\{1,2\\}}}) = (W \\odot \\Phi_{S_{1}})X + (b + \\Phi_{S_{2}})\n\\end{equation}\nwhere $\\odot$ denotes the element-wise multiplications.\n\nTo sum up, SS operations do benefit our model for its’ fast convergence for MTL, which should be credited to the robust initialization with the large-scale pre-trained model. Simultaneously, its’ doing nothing to the pre-trained model weights also prevents the occurrence of the ‘forgetting’ phenomenon when learning specific tasks in MTL.\n\\vfill\n\\fi\n\n"
            }
        },
        "tables": {
            "E-1": "\\begin{table*}[t]\n\\centering\n  \\setlength{\\belowcaptionskip}{2pt}\n  \\caption{Comparisons of binary classification on five datasets. The results with $\\ddagger$ are directly copied from \\cite{Anomal-E}.}\n%\\caption{Comparisons of traffic predictions on the three benchmarks, including CIC-ToN-IoT, CIC-BoT-IoT, and EdgeIIoT. We repeat our experiments five times and report both the mean and variance values of the F1-Score (\\%) and ROC-AUC score (\\%).}\n\\label{E-1}\n\\resizebox{\\linewidth}{!}\n{\n\\begin{tabular}{c|cc|cc|cc|cc|cc} \n\\toprule\n\\multirow{2}{*}{Methods} & \\multicolumn{2}{c|}{CIC-TON-IoT}   & \\multicolumn{2}{c|}{CIC-BoT-IoT}    & \\multicolumn{2}{c|}{EdgeIIoT}  &\\multicolumn{2}{c|}{NF-UNSW-NB15-v2} &\\multicolumn{2}{c}{NF-CSE-CIC-IDS2018-v2}                                   \\\\\n     & F1          & AUC          & F1           & AUC               & F1            & AUC               & F1              & AUC               & F1                 & AUC     \\\\  \n\\midrule \nTGN \\cite{rossi2020temporal}   & \\underline{$89.90$$\\pm$\\scriptsize$1.66$} & \\underline{$82.09$$\\pm$\\scriptsize$1.36$}  & $96.84$$\\pm$\\scriptsize$0.44$  & $94.41$$\\pm$\\scriptsize$0.81$ & \\underline{$94.99$$\\pm$\\scriptsize$0.61$} & $89.50$$\\pm$\\scriptsize$2.04$  & $93.55$$\\pm$\\scriptsize$0.23$ & $88.01$$\\pm$\\scriptsize$1.97$  & $95.11$$\\pm$\\scriptsize$0.46$         & \\underline{$91.30$$\\pm$\\scriptsize$0.76$}      \\\\\nEULER \\cite{kingeuler}         &$89.73$$\\pm$\\scriptsize$1.13$    & $80.48$$\\pm$\\scriptsize$2.46$  & $96.00$$\\pm$\\scriptsize$0.29$   &  $91.47$$\\pm$\\scriptsize$1.36$  & $92.89$$\\pm$\\scriptsize$0.32$  & \\underline{$90.64$$\\pm$\\scriptsize$1.80$}   & $92.76$$\\pm$\\scriptsize$0.86$     & $86.97$$\\pm$\\scriptsize$1.11$         & $95.87$$\\pm$\\scriptsize$0.51$         & $90.76$$\\pm$\\scriptsize$0.64$    \\\\ \nAnomRank \\cite{2019Fast}             & $76.51$$\\pm$\\scriptsize$0.98$   & $77.41$$\\pm$\\scriptsize$1.64$  & $84.84$$\\pm$\\scriptsize$0.49$   & $82.50$$\\pm$\\scriptsize$0.59$        & $78.37$$\\pm$\\scriptsize$0.43$    & $81.36$$\\pm$\\scriptsize$0.41$        & $90.54$$\\pm$\\scriptsize$2.40$     & $79.63$$\\pm$\\scriptsize$0.12$        & $90.08$$\\pm$\\scriptsize$0.82$          & $83.76$$\\pm$\\scriptsize$0.35$\\\\\nDynAnom \\cite{guo2022dynanom}        & $79.23$$\\pm$\\scriptsize$1.81$   & $75.22$$\\pm$\\scriptsize$0.92$  & $83.25$$\\pm$\\scriptsize$0.62$   & $79.04$$\\pm$\\scriptsize$0.84$        & $81.56$$\\pm$\\scriptsize$0.94$    & $83.94$$\\pm$\\scriptsize$0.36$        & $89.11$$\\pm$\\scriptsize$1.48$     & $85.25$$\\pm$\\scriptsize$0.64$        & $91.21$$\\pm$\\scriptsize$0.95$          & $88.79$$\\pm$\\scriptsize$0.54$ \\\\\n\\midrule\nAnomal-E \\cite{Anomal-E}&-  & -  &  - &     -  & -   &    -    & $91.89\\ddagger$     &-       & $94.51\\ddagger$        &- \\\\\nGAT \\cite{velivckovic2017graph}     & $86.30$$\\pm$\\scriptsize$1.16$   & $74.66$$\\pm$\\scriptsize$1.37$  & $94.56$$\\pm$\\scriptsize$0.75$   & $93.09$$\\pm$\\scriptsize$2.83$        & $93.30$$\\pm$\\scriptsize$0.14$    &$88.30$$\\pm$\\scriptsize$1.56$         & $92.20$$\\pm$\\scriptsize$1.60$     & $89.91$$\\pm$\\scriptsize$0.62$  & \\underline{$96.08$$\\pm$\\scriptsize$0.24$}    & $90.56$$\\pm$\\scriptsize$0.34$    \\\\\nE-GraphSAGE \\cite{lo2022graphsage}  & $89.46$$\\pm$\\scriptsize$1.25$   & $79.56$$\\pm$\\scriptsize$1.63$  & $93.74$$\\pm$\\scriptsize$0.76$   & $90.53$$\\pm$\\scriptsize$1.90$       & $92.10$$\\pm$\\scriptsize$1.46$    & $89.10$$\\pm$\\scriptsize$0.64$ &\\underline{$94.10$$\\pm$\\scriptsize$0.33$} &\\underline{$90.39$$\\pm$\\scriptsize$0.26$} & $95.71$$\\pm$\\scriptsize$0.35$        & $90.22$$\\pm$\\scriptsize$0.48$ \\\\\n \nDMGI \\cite{park2020unsupervised}    & $88.83$$\\pm$\\scriptsize$0.48$   & $79.13$$\\pm$\\scriptsize$2.11$  & $96.07$$\\pm$\\scriptsize$1.89$   &  $92.65$$\\pm$\\scriptsize$1.57$       & $93.83$$\\pm$\\scriptsize$1.67$    & $86.03$$\\pm$\\scriptsize$2.45$        & $93.11$$\\pm$\\scriptsize$0.98$     & $88.51$$\\pm$\\scriptsize$1.00$         & $93.87$$\\pm$\\scriptsize$0.84$         & $87.56$$\\pm$\\scriptsize$0.55$  \\\\\nSSDCM \\cite{mitra2021semi}  & $89.23$$\\pm$\\scriptsize$0.87$ & $80.84$$\\pm$\\scriptsize$2.32$ & \\underline{$97.11$$\\pm$\\scriptsize$0.63$} &\\underline{$94.82$$\\pm$\\scriptsize$0.96$} & $94.72$$\\pm$\\scriptsize$1.59$ & $86.69$$\\pm$\\scriptsize$0.76$        & $93.30$$\\pm$\\scriptsize$0.25$     & $89.22$$\\pm$\\scriptsize$1.94$         & $94.96$$\\pm$\\scriptsize$0.52$         & $88.61$$\\pm$\\scriptsize$0.38$  \\\\ \n\\midrule\nMLP \\cite{roopak2019deep}      & $80.74$$\\pm$\\scriptsize$0.43$   & $61.80$$\\pm$\\scriptsize$1.48$  & $93.01$$\\pm$\\scriptsize$0.60$   & $87.90$$\\pm$\\scriptsize$0.54$        & $88.78$$\\pm$\\scriptsize$0.44$    & $86.00$$\\pm$\\scriptsize$1.49$        & $93.12$$\\pm$\\scriptsize$0.64$     & $89.92$$\\pm$\\scriptsize$0.55$         & $94.59$$\\pm$\\scriptsize$0.94$         & $90.42$$\\pm$\\scriptsize$0.89$      \\\\\n\nMStream \\cite{Bhatia_2021} & $73.90$$\\pm$\\scriptsize$1.13$   & $70.22$$\\pm$\\scriptsize$1.61$  & $78.48$$\\pm$\\scriptsize$0.19$   & $74.04$$\\pm$\\scriptsize$1.66$     & $82.47$$\\pm$\\scriptsize$1.67$    & $77.89$$\\pm$\\scriptsize$0.58$        & $89.47$$\\pm$\\scriptsize$1.13$   & $84.38$$\\pm$\\scriptsize$1.01$       & $88.34$$\\pm$\\scriptsize$0.45$ & $83.66$$\\pm$\\scriptsize$1.79$\\\\\n\nLUCID \\cite{doriguzzi2020lucid} & $83.62$$\\pm$\\scriptsize$1.69$   & $72.31$$\\pm$\\scriptsize$1.14$  & $94.36$$\\pm$\\scriptsize$0.41$   & $89.46$$\\pm$\\scriptsize$0.72$     & $88.94$$\\pm$\\scriptsize$1.73$    & $85.23$$\\pm$\\scriptsize$0.94$        & $92.77$$\\pm$\\scriptsize$1.39$   & $88.32$$\\pm$\\scriptsize$0.91$       & $95.84$$\\pm$\\scriptsize$1.46$ & $90.75$$\\pm$\\scriptsize$0.79$\\\\\n\n\\midrule\n\\textbf{Ours (DIDS)} &{$\\textbf{91.57}$$\\pm$\\scriptsize$\\textbf{0.40}$}&{$\\textbf{84.06}$$\\pm$\\scriptsize$\\textbf{1.01}$}&{$\\textbf{98.24}$$\\pm$\\scriptsize$\\textbf{0.32}$}&{$\\textbf{96.32}$$\\pm$\\scriptsize$\\textbf{0.25}$}&{$\\textbf{96.83}$$\\pm$\\scriptsize$\\textbf{0.36}$}&{$\\textbf{92.34}$$\\pm$\\scriptsize$\\textbf{1.10}$}&{$\\textbf{95.45}$$\\pm$\\scriptsize$\\textbf{0.67}$}&{$\\textbf{91.55}$$\\pm$\\scriptsize$\\textbf{1.03}$}&{$\\textbf{96.34}$$\\pm$\\scriptsize$\\textbf{0.21}$}&{$\\textbf{93.23}$$\\pm$\\scriptsize$\\textbf{1.50}$}  \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table*}",
            "tab:few-shot": "\\begin{table*}[t]\n\\centering\n  \\setlength{\\belowcaptionskip}{2pt}\n  \\caption{Comparisons of few-shot learning classification on five datasets. The results are the average scores of ten-time repetitions.}\n\\label{tab:few-shot}\n\\resizebox{\\linewidth}{!}\n{\\begin{tabular}{c|cc|cc|cc|cc|cc} \n\\toprule\n\\multirow{2}{*}{Methods} & \\multicolumn{2}{c|}{CIC-TON-IoT}   & \\multicolumn{2}{c|}{CIC-BoT-IoT}    & \\multicolumn{2}{c|}{EdgeIIoT}  &\\multicolumn{2}{c|}{NF-UNSW-NB15-v2} &\\multicolumn{2}{c}{NF-CSE-CIC-IDS2018-v2}                                   \\\\\n     & F1          & NMI          & F1           & NMI               & F1            & NMI               & F1              & NMI               & F1                 & NMI     \\\\  \n\\midrule \nMBase \\cite{chen2021metabaseline}   & \n{$41.74$$\\pm$\\scriptsize$3.22$} & {$35.81$$\\pm$\\scriptsize$2.84$} &\n{$38.52$$\\pm$\\scriptsize$3.58$} &\n{$27.89$$\\pm$\\scriptsize$3.84$} &\n{{$51.13$$ \\pm $ \\scriptsize$ 2.31$}}   & \n{{\\underline{$55.61$$ \\pm $ \\scriptsize$ 2.05$}}}   &\n{$41.74$$\\pm$\\scriptsize$3.22$} & {$35.81$$\\pm$\\scriptsize$2.84$} &\n{$63.74$$\\pm$\\scriptsize$4.14$} &\n{$68.76$$\\pm$\\scriptsize$3.16$} \\\\\nMTL \\cite{sun2019metatransfer}         &\n{$30.79$$\\pm$\\scriptsize$3.13$}    &\n{$29.81$$\\pm$\\scriptsize$3.58$}  &\n{$37.80$$\\pm$\\scriptsize$5.66$}  &\n{$25.10$$\\pm$\\scriptsize$2.69$} &\n{{$52.64$$ \\pm $ \\scriptsize$ 5.69$}}   & \n{{$55.41$$ \\pm $ \\scriptsize$ 4.16$}}   &\n{{$44.18$$ \\pm $ \\scriptsize$ 4.22$}}   & \n{{$35.85$$ \\pm $ \\scriptsize$ 2.79$}}   &\n{{$45.38$$ \\pm $ \\scriptsize$ 5.88$}}   & \n{{$47.04$$ \\pm $ \\scriptsize$ 4.82$}}   \\\\ \nTEG \\cite{kim2023task}             & \n{{$29.92$$ \\pm $ \\scriptsize$ 2.17$}}   & \n{{$22.64$$ \\pm $ \\scriptsize$ 2.22$}}   & \n{{$29.62$$ \\pm $ \\scriptsize$ 3.74$}}   & \n{{$25.71$$ \\pm $ \\scriptsize$ 2.82$}}   & \n{{$26.34$$ \\pm $ \\scriptsize$ 2.45$}}   & \n{{$27.58$$ \\pm $ \\scriptsize$ 2.64$}}   & \n{{$27.80$$ \\pm $ \\scriptsize$ 2.14$}}   & \n{{$21.44$$ \\pm $ \\scriptsize$ 1.74$}}   & \n{{$25.20$$ \\pm $ \\scriptsize$ 3.17$}}   & \n{{$23.48$$ \\pm $ \\scriptsize$ 3.97$}}   \\\\\n% DynAnom \\cite{guo2022dynanom}        & $79.23$$\\pm$\\scriptsize$1.81$   & $75.22$$\\pm$\\scriptsize$0.92$  & $83.25$$\\pm$\\scriptsize$0.62$   & $79.04$$\\pm$\\scriptsize$0.84$        & $81.56$$\\pm$\\scriptsize$0.94$    & $83.94$$\\pm$\\scriptsize$0.36$        & $89.11$$\\pm$\\scriptsize$1.48$     & $85.25$$\\pm$\\scriptsize$0.64$        & $91.21$$\\pm$\\scriptsize$0.95$          & $88.79$$\\pm$\\scriptsize$0.54$ \\\\\n\\midrule\n% Anomal \\cite{Anomal-E}&-  & -  &  - &     -  & -   &    -    & $91.89\\ddagger$     &-       & $94.51\\ddagger$        &- \\\\\nCLSA \\cite{wang2022contrastive}     & \n{{$17.21$$ \\pm $ \\scriptsize$ 3.63$}}   & \n{{$45.01$$ \\pm $ \\scriptsize$ 2.83$}}   & \n{{$38.54$$ \\pm $ \\scriptsize$ 3.49$}}   & \n{{$51.70$$ \\pm $ \\scriptsize$ 4.90$}}   & \n{{$18.84$$ \\pm $ \\scriptsize$ 3.10$}}   & \n{{$44.25$$ \\pm $ \\scriptsize$ 3.27$}}   & \n{{$17.53$$ \\pm $ \\scriptsize$ 1.95$}}   & \n{{$45.67$$ \\pm $ \\scriptsize$ 4.13$}}   &\n{{$19.19$$ \\pm $ \\scriptsize$ 2.90$}}   & \n{{$49.76$$ \\pm $ \\scriptsize$ 3.46$}}   \\\\\nESPT \\cite{rong2023espt}  & \n{{$36.76$$ \\pm $ \\scriptsize$ 4.37$}}   & \n{{$38.51$$ \\pm $ \\scriptsize$ 3.29$}}   &\n{{$39.41$$ \\pm $ \\scriptsize$ 5.37$}}   & \n{{$34.66$$ \\pm $ \\scriptsize$ 3.49$}}   &\n{{\\underline{$61.28$$ \\pm $ \\scriptsize$ 8.34$}}}   & \n{{$53.77$$ \\pm $ \\scriptsize$ 6.91$}}   &\n{{$46.58$$ \\pm $ \\scriptsize$ 5.94$}}   & \n{{$49.37$$ \\pm $ \\scriptsize$ 6.58$}}   &\n{{$41.37$$ \\pm $ \\scriptsize$ 4.98$}}   & \n{{$46.52$$ \\pm $ \\scriptsize$ 6.71$}}   \\\\\n \nICI \\cite{9446583}    & \n{{$55.67$$ \\pm $ \\scriptsize$ 5.14$}} & \n{{$47.80$$ \\pm $ \\scriptsize$ 3.77$}}   & \n{{\\underline{$71.32$$ \\pm $ \\scriptsize$ 3.35$}}}   & \n{{\\underline{$57.51$$ \\pm $ \\scriptsize$ 4.50$}}}   & \n{{$49.74$$ \\pm $ \\scriptsize$ 4.65$}}   & \n{{$51.30$$ \\pm $ \\scriptsize$ 5.27$}}   & \n{{$39.17$$ \\pm $ \\scriptsize$ 3.66$}}   & \n{{$31.81$$ \\pm $ \\scriptsize$ 2.21$}}   & \n{{\\underline{$76.53$$ \\pm $ \\scriptsize$ 9.43$}}}   &    \n{{\\underline{$79.00$$ \\pm $ \\scriptsize$ 6.20$}}}  \\\\\nKSCL \\cite{xu2021kshot}  & \n{{$36.19$$ \\pm $ \\scriptsize$ 2.66$}}   & \n{{$38.17$$ \\pm $ \\scriptsize$ 3.90$}}   & \n{{$33.50$$ \\pm $ \\scriptsize$ 4.23$}}   & \n{{$29.40$$ \\pm $ \\scriptsize$ 4.80$}}   & \n{{$34.87$$ \\pm $ \\scriptsize$ 2.14$}}   & \n{{$30.63$$ \\pm $ \\scriptsize$ 2.56$}}   &\n{{$17.03$$ \\pm $ \\scriptsize$ 1.97$}}   & \n{{$21.90$$ \\pm $ \\scriptsize$ 1.98$}}   &\n{{$25.74$$ \\pm $ \\scriptsize$ 3.01$}}   & \n{{$28.26$$ \\pm $ \\scriptsize$ 3.15$}}   \\\\ \n\\midrule\nBSNet \\cite{Li_2021}      & {{\\underline{$58.41$$\\pm$\\scriptsize$2.36$}}}   & {{\\underline{$57.76$$\\pm$\\scriptsize$2.96$}}}   & {$42.00$$\\pm$\\scriptsize$5.19$}   & \n{$31.41$$\\pm$\\scriptsize$4.71$}   & \n{{$43.74$$ \\pm $ \\scriptsize$ 4.20$}}   & \n{{$49.68$$ \\pm $ \\scriptsize$ 5.09$}}   &    \n{{\\underline{$65.73$$\\pm$\\scriptsize$2.65$}}}         & \n{{\\underline{$58.96$$\\pm$\\scriptsize$2.08$}}}         & \n{$56.39$$\\pm$\\scriptsize$5.75$}      &\n{$58.56$$\\pm$\\scriptsize$3.76$}      \\\\\nCMFSL \\cite{Xi2022FewShotLW} & \n{$37.46$$\\pm$\\scriptsize$5.37$}   & \n{{$47.51$$\\pm$\\scriptsize$4.29$}}  &\n{{$58.62$$ \\pm $ \\scriptsize$ 3.58$}}   & \n{{$51.47$$ \\pm $ \\scriptsize$ 6.28$}}   & \n{{$39.43$$ \\pm $ \\scriptsize$ 3.88$}}       & \n{{$43.67$$ \\pm $ \\scriptsize$ 5.75$}}    & \n{{$51.30$$ \\pm $ \\scriptsize$ 4.29$}}        & \n{{$47.62$$ \\pm $ \\scriptsize$ 6.47$}}     & \n{{$61.37$$ \\pm $ \\scriptsize$ 4.95$}}         & \n{{$58.42$$ \\pm $ \\scriptsize$ 6.17$}}  \\\\\nTAD \\cite{hu2023understanding} & \n{$49.80$$\\pm$\\scriptsize$2.13$}         & \n{$42.23$$\\pm$\\scriptsize$1.35$}         & \n{{$62.75$$\\pm$\\scriptsize$6.40$}}     & \n{$56.53$$\\pm$\\scriptsize$7.70$}         & \n{{$50.82$$ \\pm $ \\scriptsize$ 3.93$}}    & \n{{$53.40$$ \\pm $ \\scriptsize$ 3.86$}}    & \n{$58.97$$\\pm$\\scriptsize$3.76$}         & \n{$58.72$$\\pm$\\scriptsize$2.60$}         & \n{$54.88$$\\pm$\\scriptsize$3.34$}         & \n{$52.15$$\\pm$\\scriptsize$1.57$}         \\\\\nPCWPK\\cite{zhang2021prototype} & \n{$42.92$$\\pm$\\scriptsize$3.80$}  & \n{$44.17$$\\pm$\\scriptsize$3.12$}  & \n{$56.08$$\\pm$\\scriptsize$2.91$}  & \n{$53.01$$\\pm$\\scriptsize$5.17$}  & \n{$34.74$$\\pm$\\scriptsize$2.54$}  & \n{$34.29$$\\pm$\\scriptsize$3.36$}  & \n{$51.97$$\\pm$\\scriptsize$1.76$}  & \n{$43.62$$\\pm$\\scriptsize$2.27$}     & \n{$52.39$$\\pm$\\scriptsize$8.60$} & \n{{$64.56$$ \\pm $ \\scriptsize$ 5.57$}} \\\\\n\n\\midrule\n\\textbf{Ours(DIDS-MFL)} &{$\\textbf{97.47}$$\\pm$\\scriptsize$\\textbf{1.17}$}&{$\\textbf{94.27}$$\\pm$\\scriptsize$\\textbf{2.62}$}&{$\\textbf{96.64}$$\\pm$\\scriptsize$\\textbf{1.88}$}&{$\\textbf{91.27}$$\\pm$\\scriptsize$\\textbf{3.94}$}&{$\\textbf{92.73}$$\\pm$\\scriptsize$\\textbf{2.57}$}&{$\\textbf{88.21}$$\\pm$\\scriptsize$\\textbf{3.25}$}&{$\\textbf{97.39}$$\\pm$\\scriptsize$\\textbf{1.41}$}&{$\\textbf{94.32}$$\\pm$\\scriptsize$\\textbf{2.84}$}&{$\\textbf{93.93}$$\\pm$\\scriptsize$\\textbf{3.27}$}&{$\\textbf{90.98}$$\\pm$\\scriptsize$\\textbf{3.69}$}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table*}",
            "UAC": "\\begin{table}[h]\n\n\\centering\n\\caption{Unknown classification on the CIC-ToN-IoT dataset with metric F1-score (\\%)}\n\\label{UAC}\n\\resizebox{\\linewidth}{!}{\n\n\\begin{tabular}{cccccc} \n\\toprule\nMethod                             &Logistic Regression         & MSteam      & E-GraphSAGE                   & TGN                     & \\textbf{Ours (DIDS)}     \\\\ \n\\midrule \\midrule\nDDoS                          & $1.68$$\\pm$\\scriptsize$0.67$  & $26.73$$\\pm$\\scriptsize$1.21$  & $10.20$$\\pm$\\scriptsize$1.46$                    & $32.84$$\\pm$\\scriptsize$1.03$              & {$\\textbf{41.78}$$\\pm$\\scriptsize$\\textbf{0.26}$}  \\\\\nMITM                          & $2.17$$\\pm$\\scriptsize$0.84$  & $12.82$$\\pm$\\scriptsize$1.90$  & $6.05$$\\pm$\\scriptsize$0.35$                     & $15.32$$\\pm$\\scriptsize$0.54$              & {$\\textbf{34.91}$$\\pm$\\scriptsize$\\textbf{0.91}$}  \\\\\nInjection                     & $0.00$$\\pm$\\scriptsize$0.34$  & $15.73$$\\pm$\\scriptsize$1.73$  & $12.37$$\\pm$\\scriptsize$0.88$                    & $22.83$$\\pm$\\scriptsize$0.35$              & {$\\textbf{25.63}$$\\pm$\\scriptsize$\\textbf{0.93}$}  \\\\\nBackdoor                      & $3.13$$\\pm$\\scriptsize$1.33$  & $20.85$$\\pm$\\scriptsize$0.63$  & $9.51$$\\pm$\\scriptsize$0.46$                     & $23.10$$\\pm$\\scriptsize$1.03$              & {$\\textbf{32.29}$$\\pm$\\scriptsize$\\textbf{0.81}$}  \\\\\n\\bottomrule\n\\end{tabular}}\n%\\vspace{-5pt}\n\\end{table}",
            "Tab:abl": "\\begin{table}[!htb]\n  \\centering\n  \\caption{Ablation study of DIDS. F1-score (\\%), Precision (\\%), ROC-AUC (\\%), Recall (\\%), all metrics above are the average of five repeated experiments on the CIC-ToN-IoT dataset.}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{c|cccc} \n  \\toprule\n  Variants              & P & R & F1 & AUC   \\\\ \n  \\midrule \\midrule\n  w/o SD  & $92.70$$\\pm$\\scriptsize$0.46$  & $90.71$$\\pm$\\scriptsize$0.89$ & $91.69$$\\pm$\\scriptsize$0.33$   & $86.87$$\\pm$\\scriptsize$0.54$   \\\\\n  w/o RD   & $91.06$$\\pm$\\scriptsize$0.57$  & $87.32$$\\pm$\\scriptsize$0.67$ & $89.15$$\\pm$\\scriptsize$0.42$   & $83.57$$\\pm$\\scriptsize$1.33$   \\\\\n  %3D-IDS-linear        & 91.84$$\\pm$\\scriptsize$0.29  & 87.91$$\\pm$\\scriptsize$0.73 & 96.14$$\\pm$\\scriptsize$0.76   & 84.91$$\\pm$\\scriptsize$0.63   \\\\\n  w/o MLGRAND        & $88.76$$\\pm$\\scriptsize$0.54$  & $84.43$$\\pm$\\scriptsize$0.26$ & $86.54$$\\pm$\\scriptsize$0.71$   & $79.32$$\\pm$\\scriptsize$0.30$   \\\\\n  \\hline\n  \\textbf{DIDS(ours)} &$\\textbf{97.78}$$\\pm$\\scriptsize$\\textbf{0.32}$  & $\\textbf{98.06}$$\\pm$\\scriptsize$\\textbf{0.43}$ & $\\textbf{97.92}$$\\pm$\\scriptsize$\\textbf{0.26}$   & $\\textbf{96.04}$$\\pm$\\scriptsize$\\textbf{0.25}$   \\\\\n  \\bottomrule\n  \\end{tabular}}\n   \\label{Tab:abl}\n  \\end{table}",
            "Tab:abl_2": "\\begin{table}[!htb]\n  \\centering\n  \\caption{Ablation study of MFL. F1-score (\\%), Precision (\\%), ROC-AUC (\\%), Recall (\\%), all metrics above are the average of ten repeated experiments on the CIC-ToN-IoT dataset.}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{c|cccc} \n  \\toprule\n  Variants              & P & R & F1 & NMI   \\\\ \n  \\midrule \\midrule\n  w/o LOS  & $77.80$$\\pm$\\scriptsize$3.83$  & $78.80$$\\pm$\\scriptsize$1.99$ & $75.82$$\\pm$\\scriptsize$2.89$   & $86.13$$\\pm$\\scriptsize$2.15$   \\\\\n  w/o DR   &  $75.51$$\\pm$\\scriptsize$3.01$  & $77.30$$\\pm$\\scriptsize$1.77$ & $74.90$$\\pm$\\scriptsize$1.66$   & $85.56$$\\pm$\\scriptsize$1.64$   \\\\\n  %3D-IDS-linear        & 91.84$$\\pm$\\scriptsize$0.29  & 87.91$$\\pm$\\scriptsize$0.73 & 96.14$$\\pm$\\scriptsize$0.76   & 84.91$$\\pm$\\scriptsize$0.63   \\\\\n  SE        & $88.62$$\\pm$\\scriptsize$3.56$  & $88.80$$\\pm$\\scriptsize$3.27$ & $87.80$$\\pm$\\scriptsize$3.17$   & $85.08$$\\pm$\\scriptsize$2.31$   \\\\\n  \\hline\n  \\textbf{MFL(ours)} &$\\textbf{97.68}$$\\pm$\\scriptsize$\\textbf{1.09}$  & $\\textbf{97.50}$$\\pm$\\scriptsize$\\textbf{1.18}$ & $\\textbf{97.47}$$\\pm$\\scriptsize$\\textbf{1.17}$   & $\\textbf{94.27}$$\\pm$\\scriptsize$\\textbf{2.62}$   \\\\\n  \\bottomrule\n  \\end{tabular}}\n   \\label{Tab:abl_2}\n  \\end{table}",
            "IPTABLE": "\\begin{table}[ht]\n    \\centering\n    \\caption{TimeStamp and IP}\n    % \\label{DATASETTABLE}\n    \\label{IPTABLE}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{cccc}\n    \\hline\n        Time & TimeStamp & Src IP & Dst IP \\\\ \\hline\n        $t1$ & $25/04/2019$ $05:18:37$ pm & $183.68.192.168$ & $1.169.216.58$ \\\\ \n        $t2$ & $25/04/2019$ $05:18:42$ pm & $1.169.216.58$ & $25.162.192.168$ \\\\ \n        $t3$ & $25/04/2019$ $05:18:42$ pm & $1.169.216.58$ & $230.158.52.59$ \\\\ \n        $t4$ & $25/04/2019$ $05:18:49$ pm & $230.158.52.59$ & $25.162.192.168$ \\\\ \n        $t5$ & $25/04/2019$ $05:18:52$ pm & $25.162.192.168$ & $69.151.192.168$ \\\\ \n        $t6$ & $25/04/2019$ $05:19:00$ pm & $177.21.192.168$ & $230.158.52.59$ \\\\ \\hline\n    \\end{tabular}}\n\\end{table}",
            "Tab: RQ7": "\\begin{table}[!htb]\n  \\centering\n  \\caption{The few-shot learning F1-score (\\%) comparison between different methods with and without MFL on the CIC-TON-IoT dataset.}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{c|ccc} \n  \\toprule\n  Methods             & E-GraphSAGE & EULER & TGN    \\\\ \n  \\midrule \\midrule\n  Original  & $20.14$$\\pm$\\scriptsize$0.15$  & $52.38$$\\pm$\\scriptsize$0.29$ & $14.25$$\\pm$\\scriptsize$0.33$   \\\\\n  MFL  &   \\textbf{$\\textbf{26.70}$$\\pm$\\scriptsize$\\textbf{0.21}$}  & \\textbf{$\\textbf{61.52}$$\\pm$\\scriptsize$\\textbf{0.37}$} & \\textbf{$\\textbf{91.88}$$\\pm$\\scriptsize$\\textbf{0.14}$}     \\\\\n  \\bottomrule\n  \\end{tabular}}\n   \\label{Tab: RQ7}\n  \\end{table}",
            "Tab:appli_large": "\\begin{table}[!htb]\n  \\centering\n  \\caption{The multi-classification F1-score (\\%) comparison between different datasets on DIDS with MFL module.}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{c|ccc} \n  \\toprule\n  Datasets             & CIC-TON-IoT & CIC-BoT-Iot & NF-UNSW-NB15-v2   \\\\ \n  \\midrule \\midrule\n  DIDS  & $83.56$$\\pm$\\scriptsize$1.18$  & $88.34$$\\pm$\\scriptsize$1.75$ & $93.55$$\\pm$\\scriptsize$0.97$   \\\\\n  DIDS-MFL  & \\textbf{$\\textbf{96.78}$$\\pm$\\scriptsize$\\textbf{1.72}$}  & \\textbf{$\\textbf{91.22}$$\\pm$\\scriptsize$\\textbf{3.45}$} & \\textbf{$\\textbf{95.71}$$\\pm$\\scriptsize$\\textbf{5.89}$}     \\\\\n  \\bottomrule\n  \\end{tabular}}\n   \\label{Tab:appli_large}\n  \\end{table}",
            "Tab: LLM": "\\begin{table}[h]\n  \\centering\n  \\caption{The F1-score (\\%) and time cost (s) comparisons between the LLM and DIDS-MFL on 100 traffic samples from the CIC-TON-IoT dataset.}\n  \\begin{tabular}{c|ccc} \n  \\toprule\n               & F1-Score & Time Cost    \\\\ \n  \\midrule \\midrule\n  LLM  & $66.67$\\% & $13.88$s \\\\\n  DIDS-MFL & $\\textbf{99.52}$\\%&  $\\textbf{6.41}$s  \\\\\n  \\bottomrule\n  \\end{tabular}\n   \\label{Tab: LLM}\n  \\end{table}",
            "para": "\\begin{table}[!htb]\n  \\caption{Hyper-parameters list.}\n  \\label{para}\n  \\centering\n  \\begin{tabular}{c|c}\n    \\toprule\n Hyper-parameter             & Value  \\\\\n  \\midrule \\midrule\n $W_{\\min }$                 & $0$      \\\\\n $W_{\\max }$                 & $1$     \\\\\n $B$                         & $1$     \\\\\n $\\alpha$                    & $0.3$    \\\\\n $\\beta$                     & $0.7$    \\\\\n Memory Dimension            & $150$    \\\\\n Message Dimension           & $100$    \\\\\n Embedding Dimension         & $100$    \\\\\n Hidden Units                & $64$    \\\\\n Hidden Channels             & $256$    \\\\\n                             \\bottomrule\n\\end{tabular}\n\\end{table}",
            "DATASETTABLE": "\\begin{table*}[htb]\n    \\centering\n    \\caption{ATTRIBUTES OF THE FIVE DATASETS}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{|c||cc|cc|cc|cc|cc|}\n        \\toprule\n        \\textbf{Dataset} & \\multicolumn{2}{c|}{CIC-Bot-IoT} & \\multicolumn{2}{c|}{CIC-ToN-IoT} & \\multicolumn{2}{c|}{EdgeIIoT} & \\multicolumn{2}{c|}{NF-UNSW-NB15-v2} &\\multicolumn{2}{c|}{NF-CSE-CIC-IDS2018-v2}\\\\\n        \\hline\n        \\textbf{Number of Features} & \\multicolumn{2}{c|}{$83$} & \\multicolumn{2}{c|}{$83$} &  \\multicolumn{2}{c|}{$61$} & \\multicolumn{2}{c|}{$43$} & \\multicolumn{2}{c|}{$43$} \\\\\n        \\hline\n        \\textbf{Number of Flows} & \\multicolumn{2}{c|}{$13428602$} & \\multicolumn{2}{c|}{$5351760$} &  \\multicolumn{2}{c|}{$2219201$} & \\multicolumn{2}{c|}{$2390275$} & \\multicolumn{2}{c|}{$8392401$} \\\\\n        \\hline\n        \\textbf{Benign Percentage} & \\multicolumn{2}{c|}{$0.66$} & \\multicolumn{2}{c|}{$47.00$} &  \\multicolumn{2}{c|}{$92.72$} & \\multicolumn{2}{c|}{$96.02$} & \\multicolumn{2}{c|}{$87.86$} \\\\\n        \\hline\n        \\textbf{IP addresses Number} & \\multicolumn{2}{c|}{$400$} & \\multicolumn{2}{c|}{$353604$} &  \\multicolumn{2}{c|}{$4653$} & \\multicolumn{2}{c|}{$293$} & \\multicolumn{2}{c|}{$92093$} \\\\\n        \\hline\n        \\textbf{Ports Number} & \\multicolumn{2}{c|}{$130685$} & \\multicolumn{2}{c|}{$63090$} &  \\multicolumn{2}{c|}{$16034$} & \\multicolumn{2}{c|}{$126334$} & \\multicolumn{2}{c|}{$65267$} \\\\\n        \\hline\n        \\textbf{Mean flow duration(ms)} & \\multicolumn{2}{c|}{$1.74\\times {{10}^{7}}$} & \\multicolumn{2}{c|}{$5.89\\times {{10}^{6}}$} &  \\multicolumn{2}{c|}{{\\texttimes}} & \\multicolumn{2}{c|}{$2.59\\times {{10}^{5}}$} & \\multicolumn{2}{c|}{$2.83\\times {{10}^{6}}$} \\\\\n        \\hline\n        \\textbf{Mean incoming bytes} & \\multicolumn{2}{c|}{{\\texttimes}} & \\multicolumn{2}{c|}{{\\texttimes}} &  \\multicolumn{2}{c|}{{\\texttimes}} & \\multicolumn{2}{c|}{$4622$} & \\multicolumn{2}{c|}{$3761$} \\\\\n        \\hline\n        \\textbf{Mean outgoing bytes} & \\multicolumn{2}{c|}{{\\texttimes}} & \\multicolumn{2}{c|}{{\\texttimes}} &  \\multicolumn{2}{c|}{{\\texttimes}} & \\multicolumn{2}{c|}{$35266$} & \\multicolumn{2}{c|}{$4346$} \\\\\n        \\hline\n        \\textbf{Cumulative number of TCP flags} & \\multicolumn{2}{c|}{$54$} & \\multicolumn{2}{c|}{$42$} &  \\multicolumn{2}{c|}{$26$} & \\multicolumn{2}{c|}{$28$} & \\multicolumn{2}{c|}{$48$} \\\\\n        \\hline\n        \\multirow{15}{*}{\\textbf{\\textbf{Attack Categories}}} \n                           & Categories & Number & Categories & Number & Categories & Number & Categories & Number & Categories & Number \\\\ \\cline{2-11} \n                           & \\multirow{14}{*}{\\shortstack{DDoS\\\\DoS\\\\Reconnaissance\\\\Theft}} & \\multirow{14}{*}{\\shortstack{$4913920$\\\\$4909405$\\\\$3514330$\\\\$1701$}} \n                    & \\multirow{14}{*}{\\shortstack{Backdoor\\\\DoS\\\\DDoS\\\\Injection\\\\MITM\\\\Password\\\\Ransomware\\\\Scanning\\\\XSS}} & \\multirow{14}{*}{\\shortstack{$27145$\\\\$145$\\\\$202$\\\\$277696$\\\\$517$\\\\$340208$\\\\$5098$\\\\$36205$\\\\$2149308$}} \n                           & \\multirow{14}{*}{\\shortstack{Backdoor\\\\DDoS\\_HTTP\\\\DDoS\\_ICM\\\\DDoS\\_TCP\\\\DDoS\\_UDF\\\\Fingerprinting\\\\MITM\\\\Password\\\\Port\\_Scanning\\\\Ransomware\\\\SQL\\_injection\\\\Uploading\\\\Vulnerability\\_scanner\\\\XSS}} & \\multirow{14}{*}{\\shortstack{$24862$\\\\$49911$\\\\$116436$\\\\$50062$\\\\$121568$\\\\$1001$\\\\$1214$\\\\$50153$\\\\$22564$\\\\$10925$\\\\$51203$\\\\$37634$\\\\$50110$\\\\$15915$}} \n                           & \\multirow{14}{*}{\\shortstack{Benign\\\\Fuzzers\\\\Analysis\\\\Backdoor\\\\DoS\\\\Exploits\\\\Generic\\\\Reconnaissance\\\\Shellcode\\\\Worms}} & \\multirow{14}{*}{\\shortstack{$2295222$\\\\$22310$\\\\$2299$\\\\$2169$\\\\$5794$\\\\$31551$\\\\$16560$\\\\$12779$\\\\$1427$\\\\$164$}} \n                           & \\multirow{14}{*}{\\shortstack{Benign\\\\BruteForce\\\\Bot\\\\DoS\\\\DDoS\\\\Infiltration\\\\Web Attacks}} & \\multirow{14}{*}{\\shortstack{$16635567$\\\\$120912$\\\\$143097$\\\\$483999$\\\\$1390270$\\\\$116361$\\\\$3502$}} \\\\\n                           &                &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                           &              &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                           &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                           &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                           &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                           &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\ \n                           &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                           &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\ \n                            &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                             &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                              &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                               &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\\n                                &                  &                   &                   &                   &                   &                   &                   &                   &                   &                   \\\\                        \n        \\bottomrule\n    \\end{tabular}}\n\n    \\label{DATASETTABLE}\n\n\\end{table*}",
            "E-8": "\\begin{table}[!htb]\n  \\caption{Hyper-parameter range.}\n  \\label{E-8}\n  \\centering\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{c|c}\n    \\toprule\n  Hyper-parameter             & Range  \\\\\n   \\midrule\\midrule\n  $W_{\\min }$                 & \\{$0$,$0.25$,$0.5$,$0.75$,$1$\\}      \\\\\n  $W_{\\max }$                 & \\{$0$,$0.25$,$0.5$,$0.75$,$1$\\}     \\\\\n  $B$                         & \\{$0$,$0.25$,$0.5$,$0.75$,$1$\\}     \\\\\n  $\\alpha$                    & \\{$0.1$,$0.3$,$0.5$,$0.7$,$0.9$\\}    \\\\\n  $\\beta$                     & \\{$0.1$,$0.3$,$0.5$,$0.7$,$0.9$\\}    \\\\\n  Memory Dimension            & \\{$100$,$110$,$120$,$130$,$140$,$150$,$160$,$170$,$180$,$190$,$200$\\}    \\\\\n  Message Dimension           & \\{$50$,$60$,$70$,$80$,$90$,$100$,$110$,$120$,$130$,$140$,$150$\\}    \\\\\n  Embedding Dimension         & \\{$50$,,$60$,$70$,$80$,$90$,$100$,$110$,$120$,$130$,$140$,$150$\\}    \\\\\n  Hidden Units                & \\{$1$,$2$,$4$,$8$,$32$,$64$,$128$,$256$,$512$,$1024$\\}     \\\\\n  Hidden Channels             & \\{$1$,$2$,$4$,$8$,$32$,$64$,$128$,$256$,$512$,$1024$\\}    \\\\\n                             \\bottomrule\n  \\end{tabular}\n  }\n\n  \\end{table}",
            "Input data and output results": "\\begin{table}[!htb]\n    \\centering\n    \\setlength{\\abovecaptionskip}{0cm}\n    \\caption{An example of the traffic data.}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{ccccccccc}\n    \\hline\n        Src IP & Src Port & Dst IP & Dst Port & Protocol & Flow Duration & ... & Label & Attack \\\\ \\hline\n        192.168.1.195 & 65025 & 40.90.190.179 & 443 & 6 & 60155602 & ... & 0 & Benign \\\\ \\hline\n    \\end{tabular}}\n    \\label{Input data and output results}\n\\end{table}",
            "two eq1": "\\begin{table}[ht]\n    \\caption{F1-score of the two expressions}\n    \\label{two eq1}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}cccc@{}}\n    \\toprule\n     Loss  &EdgeIIoT  &NF-UNSW-NB15-v2 &NF-CSE-CIC-IDS2018-v2 \\\\ \\midrule\n     Eq.5  &$96.83$$\\pm$\\scriptsize$0.36$ &$95.45$$\\pm$\\scriptsize$0.67$ &$96.34$$\\pm$\\scriptsize$0.21$ \\\\\n     Eq.5*  &$96.55$$\\pm$\\scriptsize$0.69$ &$94.22$$\\pm$\\scriptsize$0.89$ &$95.17$$\\pm$\\scriptsize$0.68$ \\\\ \\bottomrule\n    \\end{tabular}}\n\\end{table}",
            "two eq2": "\\begin{table}[ht]\n    \\caption{The speed and accuracy of real-time detection in different scenarios.}\n    \\label{two eq2}\n    \\begin{tabular}{ccc}\n    \\toprule\n     Datasets  &Speed  &Accuracy  \\\\ \\midrule\n     Binary Classification\t&11334 traffic/min\t&0.99  \\\\\n     Multi Classification    &10603 traffic/min\t&0.75  \\\\ \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "shift": "\\begin{table}[ht]\n    \\caption{F1 score comparisons between methods with raw features and the ones with shift-distribution features, where $*_R$ represents the former and $*_S$ represents the latter. Here LR represents the Logistic Regression method.}\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{ccccccc}\n    \\hline\n     Attack  &LR\\_R  &LR\\_S  &E-GraphSage\\_R  &E-GraphSage\\_S  &3D-IDS\\_R &3D-IDS\\_S \\\\ \\hline\n     Backdoor  &4.21  &8.05  &30.89  &34.46  &52.92\t  &54.79  \\\\\n     DoS &53.32  &55.25  &80.76  &81.80  &81.87  &85.53  \\\\\n     Injection  &39.76  &41.76  &86.49  &87.42  &87.39  &87.50  \\\\\n     MITM  &3.82  &6.33  &13.29  &16.08  &49.34  &51.64  \\\\ \\hline\n    \\end{tabular}}\n    \\label{shift}\n\\end{table}",
            "overlap": "\\begin{table}[b]\n    \\caption{Average overlap ratio of the feature distributions before and after \"manual shifting\".}\n    \\label{overlap}\n    \\centering\n    \\begin{tabular}{@{}ccc@{}}\n    \\toprule\n     Dataset &Before  &After  \\\\ \\midrule\n     CIC-ToN-IoT  &62.37\\%  &24.16\\%  \\\\ \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "F1 score comparisons": "\\begin{table}[!htb]\n    \\centering\n    \\caption{F1 score comparisons between methods with disentangled features (with \"+\") and with raw features (without \"+\"), respectively. Here LR represents the Logistic Regression.}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{ccccccc}\n    \\hline\n        Attack & LR & LR+ & E-GraphSage & E-GraphSage+ & 3D-IDS & 3D-IDS+(Ours) \\\\ \\hline\n        Backdoor & 4.21 & 13.65 & 30.89 & 35.19 & 52.92 & 64.17 \\\\ \n        Injection & 39.76 & 50.16 & 86.49 & 87.37 & 87.39 & 87.52 \\\\ \\hline\n    \\end{tabular}}\n    \n    \\label{F1 score comparisons}\n\\end{table}",
            "importance": "\\begin{table}[!htb]\n   \n    \\caption{Features importance score comparisons.}\n    \\label{importance}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}cccc@{}}\n    \\toprule\n     Feature id  &Features  &Importance score with raw features  &Importance score with disentangled features  \\\\ \\midrule\n     1&A+B\t  &0.14  &0.13  \\\\\n     2&A+C\t  &0.11  &0.11  \\\\\n     3&B+C  &0.09  &0.10  \\\\\n     4&A+B+C  &0.18  &0.18  \\\\ \\bottomrule\n    \\end{tabular}}\n\\end{table}",
            "realtime": "\\begin{table}[ht]\n    \\caption{The speed and accuracy of real-time detection in different scenarios.}\n    \\label{realtime}\n    \\begin{tabular}{ccc}\n    \\toprule\n     Scenario  &Speed  &Accuracy  \\\\ \\midrule\n     Binary Classification\t&11334 traffic/min\t&0.99  \\\\\n     Multi Classification    &10603 traffic/min\t&0.75  \\\\ \\bottomrule\n    \\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:intro-illustrate": "\\begin{figure}\n  \\centering\n  \\setlength{\\abovecaptionskip}{5pt}  %图像在caption上方距离\n  \\setlength{\\belowcaptionskip}{0pt}\n  % Requires \\usepackage{graphicx}\n  % \\includegraphics[scale=0.41]{picture/page2/pic2/Intro7.pdf}\\\\\n    \\includegraphics[scale=0.41]{picture/page2/pic2/Intro.pdf}\\\\\n  \\caption{Illustration of two network attacks DoS and MITM. DoS floods the target with massive traffic to overwhelm an online service, and MITM eavesdrops on the communication between two targets and steals private information. An NIDS can be easily deployed in a single location to collect statistical\nfeatures and alert administrators for potential threats.}\\label{fig:intro-illustrate}%The network architecture in real-world scenarios and three kinds of intrusions. For the first challenge, the existing single-layer methods can not well identified cross-layer attacks, such as MITM. For the second challenge, the directly utilizing of the node-level features will lead to detection failed, because the node-level feature processing such as aggregation has blurred the important information for detection in specific dimensions. }\n%  \\vspace{-10pt}\n\\end{figure}",
            "fig:intro-analysis": "\\begin{figure*}[ht]\n    \\centering\n\n    % 设置图像间距和子标题间距\n    \\setlength{\\abovecaptionskip}{8pt}  % 图像在 caption 上方的距离\n    \\setlength{\\belowcaptionskip}{0pt} % 图像在 caption 下方的距离\n    \\hspace*{-0.8cm} % 负值可以使图像向左移动\n\n    % 使用 subfloat 代替 subfigure\n    \\subfloat[F1-Score]{%\n        \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/P2.pdf}%\n    }\n    \\hspace*{0.2cm} % 调整子图之间的距离\n    \\subfloat[Normalized feature distribution of MITM attacks]{%\n        \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/Z1.pdf}%\n    }\n    \\hspace*{0.2cm}\n    \\subfloat[Normalized feature distributions of DDoS attacks]{%\n        \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/Z2.pdf}%\n    }\n    \\\\\n    \\hspace*{-0.8cm}\n    \\subfloat[F1-Score]{%\n        \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/P21.pdf}%\n    }\n    \\hspace*{0.2cm}\n    \\subfloat[Representation correlation map of MITM attacks]{%\n        \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/introe.pdf}%\n    }\n    \\hspace*{0.2cm}\n    \\subfloat[Representation correlation map of DDoS attacks]{%\n        \\includegraphics[height=1.8cm,width=0.3\\linewidth]{picture/page2/pic2/new_f_co_heatmap2.pdf}%\n    }\n\n    % 设置总的标题和标签\n    \\caption{Quantitative analysis on CIC-TON-IOT.\n    (a) Comparisons of detecting various attacks, which are regarded as an unknown type in evaluation. Specifically, we train an SVM model without using the data points of these attacks, and evaluate the instances of these attacks on the test set. (b) and (c) show the feature distributions of two attacks, MITM and DDoS, respectively. (d) Comparisons of detecting various known attacks on the previous state-of-the-art deep learning model E-GraphSAGE, (e) and (f) are correlation maps of representations of the two attacks, where the representations are generated by E-GraphSAGE.}\n    \\label{fig:intro-analysis}\n\\end{figure*}",
            "fig:few-shot performance": "\\begin{figure}\n    \\centering\n    % 调整图像间距\n    \\setlength{\\abovecaptionskip}{8pt}  % 图像与标题之间的间距\n    \\setlength{\\belowcaptionskip}{0pt}  % 图像与正文之间的间距\n\n    % 设置子图\n    \\subfloat[]{%\n        \\includegraphics[width=0.45\\linewidth,height=2.05cm]{picture/page2/pic2/summary.pdf}%\n    }\n    \\hspace{0.1cm} % 子图之间的水平间距\n    \\subfloat[]{%\n        \\includegraphics[width=0.45\\linewidth]{picture/page2/pic2/3model_f1_scores_and_average.pdf}%\n    }\n\n    % 标题和说明\n    \\caption{(a) Performance comparisons among different intrusion detection methods with normal-size traffic and few-shot traffic. (b) F1-score comparisons between DIDS few-shot learning and two SOTA few-shot learning baselines. We repeat the above comparisons 10 times.}\n    \\vspace{-10pt} % 调整整个图与正文之间的垂直间距\n    \\label{fig:few-shot performance}\n\\end{figure}",
            "fig:few-shot empirical": "\\begin{figure}\n    \\centering\n    % 调整图像间距\n    \\setlength{\\abovecaptionskip}{8pt}  % 图像与标题之间的间距\n    \\setlength{\\belowcaptionskip}{0pt}  % 图像与正文之间的间距\n\n    % 设置子图\n    \\subfloat[The t-SNE of DIDS]{%\n        \\includegraphics[width=0.45\\linewidth]{picture/page2//pic2/3D-IDS_tsne3.pdf}%\n    }\n    \\hspace{0.1cm} % 子图之间的水平间距\n    \\subfloat[The t-SNE of BSNet]{%\n        \\includegraphics[width=0.45\\linewidth]{picture/page2/pic2/similarity_tsne1.pdf}%\n    }\n    \\hspace{0.1cm}\n    \\subfloat[The correlation map of DIDS]{%\n        \\includegraphics[width=0.475\\linewidth]{picture/page2/pic2/3D-IDS_heatmap9.pdf}%\n    }\n    \\hspace{0.1cm}\n    \\subfloat[The correlation map of BSNet]{%\n        \\includegraphics[width=0.475\\linewidth]{picture/page2/pic2/similarity_heatmap8.pdf}%\n    }\n\n    % 标题和说明\n    \\caption{The correlation map and the t-SNE visualization of representations generated by DIDS and BSNet on CIC-ToN-IoT dataset.}\n    \\vspace{-10pt} % 调整整个图与正文之间的垂直间距\n    \\label{fig:few-shot empirical}\n\\end{figure}",
            "model": "\\begin{figure*}[!htb]\n% \\vspace{-5pt}\n \\centering\n     % \\includegraphics[width=\\linewidth,height=0.40\\textwidth]{picture/MODELa.png}\n     \\includegraphics[width=\\linewidth,height=0.40\\textwidth]{picture/page2/pic2/model3.png}\n     % \\vspace{-10pt}\n     \\caption{Overview of the proposed DIDS-MFL, which consists of five modules. 1) Edge construction module builds edges based on traffic flow. 2) Statistical disentanglement module differentiates values in vectors to facilitate the identification of various attacks. 3) Representational disentanglement module learns to highlight attack-specific features. 4) Multi-Layer graph diffusion module fuses the network topology for better aggregation over evolving dynamic traffic. 5) Multi-scale few-shot learning module aims to few-shot traffic threats detection. Finally, traffic classifier takes the traffic representation as an input to yield the detection results. DIDS takes the first four steps and flows through the yellow arrows to the classifier, while DIDS-MFL takes the five steps by flowing through the blue arrows to the classifier. }\n     % \\caption{An overview of the 3D-IDS framework, \\cor{which consists of xxx modules.} }\n     \\label{model}\n \\end{figure*}",
            "classfication": "\\begin{figure*}[htb]\n    \\centering\n    % 调整图像间距\n    \\setlength{\\abovecaptionskip}{8pt}  % 图像与标题之间的间距\n\n    % 设置子图\n    \\subfloat[EdgeIIoT]{%\n        \\includegraphics[width=0.47\\linewidth,height=3.5cm]{picture/page2/pic2/Q1.pdf}%\n    }\n    \\hspace{0.5cm} % 调整子图之间的水平间距\n    \\subfloat[CIC-ToN-IoT]{%\n        \\includegraphics[width=0.47\\linewidth,height=3.5cm]{picture/page2/pic2/Q2.pdf}%\n    }\n    \\\\\n    \\subfloat[CIC-BoT-IoT]{%\n        \\includegraphics[width=0.30\\linewidth,height=3cm]{picture/page2/pic2/Q3.pdf}%\n    }\n    \\hspace{0.5cm}\n    \\subfloat[NF-UNSW-NB15-v2]{%\n        \\includegraphics[width=0.30\\linewidth,height=3cm]{picture/page2/pic2/Q4.pdf}%\n    }\n    \\hspace{0.5cm}\n    \\subfloat[NF-CSE-CIC-IDS2018-v2]{%\n        \\includegraphics[width=0.30\\linewidth,height=3cm]{picture/page2/pic2/Q5.pdf}%\n    }\n    \\\\\n    \\includegraphics[width=0.90\\linewidth,height=0.5cm]{picture/page2/pic2/comment.pdf}\n\n    % 图标题和说明\n    \\caption{Comparisons of multi-classification. Here $\\dagger$ indicates that the results are directly copied from the previous works.}\n    \\label{classfication}\n\\end{figure*}",
            "dis": "\\begin{figure}[ht]\n\t\\centering  % 图片全局居中\n    \\setlength{\\abovecaptionskip}{8pt}  % 图像在 caption 上方距离\n\n    % 子图设置\n \t\\subfloat[Origin]{%\n\t\t\\includegraphics[width=0.48\\linewidth]{picture/page2/pic2/ZP12.pdf}%\n\t}\n\t\\hspace{0.1cm} % 子图之间的间距\n\t\\subfloat[First Distangled]{%\n\t\t\\includegraphics[width=0.48\\linewidth]{picture/page2/pic2/ZP121.pdf}%\n\t}\n\t\n    % 图标题和标签\n    \\caption{Statistical disentanglement of traffic features.}\n    \\label{dis}\n\\end{figure}",
            "casestudy": "\\begin{figure}[ht]\n    \\centering \n    \\setlength{\\abovecaptionskip}{2pt}  % 图像在 caption 上方距离\n\n    % 子图设置\n    \\subfloat[DIDS]{%\n        \\includegraphics[width=0.45\\linewidth]{picture/ca1.png}%\n    }\n    \\hspace{0.1cm} % 子图之间的间距\n    \\subfloat[E-GraphSAGE]{%\n        \\includegraphics[width=0.45\\linewidth]{picture/ca2.png}%\n    }\n\n    % 图标题和标签\n    \\caption{The comparison of node representation of the Injection attack after graph aggregation of our DIDS and E-GraphSAGE. The grey line presents benign data.}\n    \\label{casestudy}\n\\end{figure}",
            "attact_scenira": "\\begin{figure}\n  \\centering\n  \\setlength{\\abovecaptionskip}{5pt}  %图像在caption上方距离\n  \\setlength{\\belowcaptionskip}{-0.3cm}\n  % \\includegraphics[width=\\linewidth]{picture/MITM-2.pdf}\n    \\includegraphics[width=\\linewidth]{picture/page2/pic2/dis.pdf}\n  % \\caption{A possible real-world attack scenario}\n  \\caption{Spatial-temporal coupling in intrusion detection.}\n  % \\Description{wating...}\n  \\label{attact_scenira} \n % \\setlength{\\belowcaptionskip}{-8.5cm}\n\\end{figure}",
            "seconddis": "\\begin{figure}[ht]\n\t\\centering  %图片全局居中\n        \\setlength{\\abovecaptionskip}{5pt}  %图像在caption上方距离\n\t\\includegraphics[width=0.95\\linewidth]{picture/atmap.pdf}\n   \\caption{Linear relationship between the disentangled edge vector and the disentangled node representation.}\n \\label{seconddis}\n\\end{figure}",
            "fig: RQ_5": "\\begin{figure}[ht]\n    \\centering\n    % 调整图像间距\n    \\setlength{\\abovecaptionskip}{8pt}  % 图像与标题之间的间距\n    \\setlength{\\belowcaptionskip}{0pt}  % 图像与正文之间的间距\n\n    % 设置子图\n    \\subfloat[The t-SNE of DIDS]{%\n        \\includegraphics[width=0.46\\linewidth]{picture/page2/pic2/3D-IDS_tsne3.pdf}%\n    }\n    \\hspace{0.1cm} % 子图之间的水平间距\n    \\subfloat[The t-SNE of MFL]{%\n        \\includegraphics[width=0.47\\linewidth]{picture/page2/pic2/v2_tsne0.pdf}%\n    }\n\n    % 图标题和说明\n    \\caption{The t-SNE visualization of representations on the CIC-TON-IoT dataset generated by DIDS and MFL.}\n    \\label{fig: RQ_5}\n\\end{figure}",
            "fig: RQ_6": "\\begin{figure}[ht]\n    \\centering\n    % 调整图像间距\n    \\setlength{\\abovecaptionskip}{8pt}  % 图像与标题之间的间距\n    \\setlength{\\belowcaptionskip}{0pt}  % 图像与正文之间的间距\n\n    % 设置子图\n    \\subfloat[The correlation map of DIDS]{%\n        \\includegraphics[width=0.46\\linewidth]{picture/page2/pic2/3D-IDS_heatmap9.pdf}%\n    }\n    \\hspace{0.1cm} % 子图之间的水平间距\n    \\subfloat[The correlation map of MFL]{%\n        \\includegraphics[width=0.47\\linewidth]{picture/page2/pic2/v2_heatmap8.pdf}%\n    }\n\n    % 图标题和说明\n    \\caption{The correlation map and the t-SNE visualization of representations generated by DIDS and DIDS-MFL with MFL.}\n    \\label{fig: RQ_6}\n\\end{figure}",
            "hyperparameter1": "\\begin{figure*}[t]\n  \\centering\n   \\subfigure[$W_{\\min }$]{\\includegraphics[width=0.23\\textwidth]{picture/hy_wmin.pdf}}\n  \\subfigure[$W_{\\max }$]{\\includegraphics[width=0.23\\textwidth]{picture/hy_wmax.pdf}}\n  \\subfigure[B]{\\includegraphics[width=0.23\\textwidth]{picture/hy_B.pdf}}\n  \\subfigure[Memory dimension]{\\includegraphics[width=0.23\\textwidth]{picture/hy_memory.pdf}}\n  \\label{hyperparameter1}\n   \\subfigure[$\\alpha$ ]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/alpha.pdf}}\n  \\subfigure[$\\beta$]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/beta.pdf}}\n  \\subfigure[$\\epsilon$]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/epsilon.pdf}}\n  \\subfigure[$\\gamma$]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/gamma.pdf}}\n  \n  %\\subfigure[$\\eta$]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/eta.pdf}}\n  \\label{hyperparameter2}\n\\caption{The hyperparameter sensitivity analysis of eight hyperparameters.}\n\\end{figure*}",
            "hyperparameter2": "\\begin{figure*}[t]\n  \\centering\n   \\subfigure[$W_{\\min }$]{\\includegraphics[width=0.23\\textwidth]{picture/hy_wmin.pdf}}\n  \\subfigure[$W_{\\max }$]{\\includegraphics[width=0.23\\textwidth]{picture/hy_wmax.pdf}}\n  \\subfigure[B]{\\includegraphics[width=0.23\\textwidth]{picture/hy_B.pdf}}\n  \\subfigure[Memory dimension]{\\includegraphics[width=0.23\\textwidth]{picture/hy_memory.pdf}}\n  \\label{hyperparameter1}\n   \\subfigure[$\\alpha$ ]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/alpha.pdf}}\n  \\subfigure[$\\beta$]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/beta.pdf}}\n  \\subfigure[$\\epsilon$]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/epsilon.pdf}}\n  \\subfigure[$\\gamma$]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/gamma.pdf}}\n  \n  %\\subfigure[$\\eta$]{\\includegraphics[width=0.23\\textwidth]{picture/page2/pic2/eta.pdf}}\n  \\label{hyperparameter2}\n\\caption{The hyperparameter sensitivity analysis of eight hyperparameters.}\n\\end{figure*}",
            "hyperparameter3": "\\begin{figure*}[b]\n\n  \\centering\n   \\subfigure[$W_{\\min }$]{\\includegraphics[width=0.23\\textwidth]{picture/hy_wmin.pdf}}\n  \\subfigure[$W_{\\max }$]{\\includegraphics[width=0.23\\textwidth]{picture/hy_wmax.pdf}}\n  \\subfigure[B]{\\includegraphics[width=0.23\\textwidth]{picture/hy_B.pdf}}\n  \\subfigure[Memory dimension]{\\includegraphics[width=0.23\\textwidth]{picture/hy_memory.pdf}}\n  \\subfigure[Message dimension]{\\includegraphics[width=0.23\\textwidth]{picture/hy_message.pdf}}\n  \\subfigure[Embedding dimension]{\\includegraphics[width=0.23\\textwidth]{picture/hy_emb.pdf}}\n  \\subfigure[Hidden Units]{\\includegraphics[width=0.23\\textwidth]{picture/hy_hu.pdf}}\n  \\subfigure[Hidden Channels]{\\includegraphics[width=0.23\\textwidth]{picture/hy_hc.pdf}}\n  \\caption{Hyper-parameter analysis}\n  \\label{hyperparameter3}\n\\end{figure*}",
            "hyperparameter4": "\\begin{figure*}[!htb]\n\n  \\centering\n   \\subfigure[$W_{\\min }$]{\\includegraphics[width=0.23\\textwidth]{picture/hy_wmin.pdf}}\n  \\subfigure[$W_{\\max }$]{\\includegraphics[width=0.23\\textwidth]{picture/hy_wmax.pdf}}\n  \\subfigure[B]{\\includegraphics[width=0.23\\textwidth]{picture/hy_B.pdf}}\n  \\subfigure[Memory dimension]{\\includegraphics[width=0.23\\textwidth]{picture/hy_memory.pdf}}\n  \\subfigure[Message dimension]{\\includegraphics[width=0.23\\textwidth]{picture/hy_message.pdf}}\n  \\subfigure[Embedding dimension]{\\includegraphics[width=0.23\\textwidth]{picture/hy_emb.pdf}}\n  \\subfigure[Hidden Units]{\\includegraphics[width=0.23\\textwidth]{picture/hy_hu.pdf}}\n  \\subfigure[Hidden Channels]{\\includegraphics[width=0.23\\textwidth]{picture/hy_hc.pdf}}\n  \\caption{Hyper-parameter analysis}\n  \\label{hyperparameter4}\n\\end{figure*}",
            "diffresult": "\\begin{figure}[htb]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{picture/res.pdf}\\\\\n\\caption{Result of the Multi-Layer Graph Diffusion on one dimension of disentangled embedding.}\n\\label{diffresult}\n\\end{figure}",
            "methodcom": "\\begin{figure}[htb]\n  \\centering\n    {\n    \\subfigure[Feature-based: Find this packet is anomaly]{\\includegraphics[width=0.4\\textwidth]{picture/single.pdf}} \n    \\subfigure[Sequence-based: Find Traffic \\# $N+1$ is anomaly]{\\includegraphics[width=0.4\\textwidth]{picture/sequence.pdf}} \\\\\n    \\subfigure[Static Graph-based: Find Link $(3,6)$ is anomaly ]{\\includegraphics[width=0.4\\textwidth]{picture/static.pdf}} \n    \\subfigure[Graph snapshot-based: Find Link at $t\\in{[0,1]}$ is anomaly]{\\includegraphics[width=0.4\\textwidth]{picture/snapshot.pdf}} \\\\ \n    }\n  \\caption{A comparison between different intrusion detection methods. }\n    \\label{methodcom}\n\\end{figure}",
            "fig:attack-distribution": "\\begin{figure*}\n\n    \\centering\n    \\subfigbottomskip=0pt\n    \\subfigcapskip=-3pt\n    \\setlength{\\abovecaptionskip}{8pt} \n    \\setlength{\\belowcaptionskip}{0pt}\n    %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of Injection]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/DDoS_1.pdf}\n    }\n    %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of Injection]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/DDoS_2.pdf}\n    }\n    %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of Injection]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/DDoS_3.pdf}\n    }\n    %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of Injection]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/DDoS_4.pdf}\n    }\n        %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of Injection]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/DDoS_7.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of Injection]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/DDoS_6.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of DDoS]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/Injection4.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of DDoS]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/Injection5.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of DDoS]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/Injection6.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of DDoS]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/Injection7.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of DDoS]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/Injection_10.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[Features distribution of DDoS]{\n    \\includegraphics[height=1.8cm,width=0.30\\linewidth]{picture/Injection9.pdf}\n    }\n    % \\vspace{-20pt}\n    \\\\\n    \\caption{Details of all features distribution of Injection and DDoS attack generated by each feature's mean and standard deviation.}\n    \\label{fig:attack-distribution}\n\\end{figure*}",
            "cormap1": "\\begin{figure*}\n    \\centering\n    \\subfigbottomskip=0pt\n    \\subfigcapskip=-3pt\n    \\setlength{\\abovecaptionskip}{8pt} \n    \\setlength{\\belowcaptionskip}{0pt}\n    %\\setcounter{subfigure}{0}\n    \\subfigure[DDoS]{\n    \\includegraphics[width=0.30\\linewidth]{picture/0.2.pdf}\n    }\n    %\\setcounter{subfigure}{0}\n    \\subfigure[DoS]{\n    \\includegraphics[width=0.30\\linewidth]{picture/0.3.pdf}\n    }\n    %\\setcounter{subfigure}{0}\n    \\subfigure[Scanning]{\n    \\includegraphics[width=0.30\\linewidth]{picture/0.4.pdf}\n    }\n        %\\setcounter{subfigure}{0}\n    \\subfigure[Injection]{\n    \\includegraphics[width=0.30\\linewidth]{picture/0.5.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[Backdoor]{\n    \\includegraphics[width=0.30\\linewidth]{picture/0.6.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[MITM]{\n    \\includegraphics[width=0.30\\linewidth]{picture/0.7.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\\\\n    \\caption{Correlation maps of different attack types.}\n    \\label{cormap1}\n\\end{figure*}",
            "cormap2": "\\begin{figure*}\n    \\centering\n    \\subfigbottomskip=0pt\n    \\subfigcapskip=-3pt\n    \\setlength{\\abovecaptionskip}{8pt} \n    \\setlength{\\belowcaptionskip}{0pt}\n    %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of 3D-IDS]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/3D-IDS_heatmap3.pdf}\n    }\n    %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of 3D-IDS]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/3D-IDS_heatmap4.pdf}\n    }\n    %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of 3D-IDS]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/3D-IDS_heatmap5.pdf}\n    }\n        %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of BSNet]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/similarity_heatmap3.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of BSNet]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/similarity_heatmap4.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of BSNet]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/similarity_heatmap5.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of MFL]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/v2_heatmap3.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of MFL]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/v2_heatmap4.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\subfigure[The correlation map of MFL]{\n    \\includegraphics[width=0.30\\linewidth]{picture/page2/pic2/v2_heatmap5.pdf}\n    }\n            %\\setcounter{subfigure}{0}\n    \\\\\n    \\caption{Correlation maps of different models.}\n    \\label{cormap2}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\mathbb{A}=\\left(\\begin{array}{ccccc}\n    A_{(1,1)} & \\cdots & A_{(1, k)} & \\cdots & A_{(1, m)} \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    A_{(l, 1)} & \\cdots & A_{(k, k)} & \\cdots & A_{(l, m)} \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    A_{(m, 1)} & \\cdots & A_{(m, k)} & \\cdots & A_{(m, m)}\n    \\end{array}\\right).\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\mathbf{E}_{i j}(t)=(v_{i}, l_{i}, v_{j}, l_{j}, t, \\Delta t, \\mathbf{F}_{ij}(t)).\n\\label{eqa}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n  \\mathbf{w}_{i} \\mathcal{F}_{i} \\leq \\mathbf{w}_{i+1} \\mathcal{F}_{i+1}\\quad(1 \\leq {i} \\leq {N-1}).\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\begin{split}\n    \\widetilde{\\mathbf{w}}=\\arg \\max (\\mathbf{w}_{N} \\mathcal{F}_{N}-\\mathbf{w}_{1} \\mathcal{F}_{1}-\n    \\\\\\sum_{i=2}^{N-1} \\left|2 \\mathbf{w}_{i} \\mathcal{F}_{i}-\\mathbf{w}_{i+1} \\mathcal{F}_{i+1}-\\mathbf{w}_{i-1} \\mathcal{F}_{i-1}\\right|).\n  \\end{split}\n  \\label{target}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\begin{split}\n        \\widetilde{\\mathbf w}=\\arg \\max (\\mathbf{w}_{N} \\mathcal{F}_{N}-\\mathbf{w}_{1} \\mathcal{F}_{1}+\n        \\\\ \\sum_{i=2}^{N-1} 2 \\mathbf{w}_{i} \\mathcal{F}_{i }-\\mathbf{w}_{i-1} \\mathcal{F}_{i-1}-\\mathbf{w}_{i+1} \\mathcal{F}_{i+1}),\n    \\end{split}\n    \\label{eqb}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n  \\left\\{\\begin{array}{lll}\n    \\mathbf{w}_{i} & \\in & {\\left[W_{\\min }, W_{\\max }\\right]} \\\\\n    \\sum_{i=1}^{N} \\mathbf{w}_{i} \\mathbf{n}_{i} & \\leq & B \\\\\n    \\mathbf{w}_{i} \\mathcal{F}_{i} & \\leq & \\mathbf{w}_{i+1} \\mathcal{F}_{(i+1)} \\\\\n    2 \\mathbf{w}_{i} \\mathcal{F}_{i} & \\leq & \\mathbf{w}_{i-1} \\mathcal{F}_{i-1}+\\mathbf{w}_{i+1} \\mathcal{F}_{i+1}.\n  \\end{array}\\right.\n  \\label{constrain}\n\\end{equation}",
            "eq:7": "\\begin{align}\n              \\mathbf{c}_{i}(t)&=\\operatorname{Msg}\\left(\\mathbf{m}_{i}\\left(t^-\\right),\\mathbf{m}_{j}\\left(t^-\\right),t,\\Delta t,l_i, l_j, \\mathbf{h_{i,j}}\\right),\\\\\n              \\mathbf{c}_{j}(t)&=\\operatorname{Msg}\\left(\\mathbf{m}_{j}\\left(t^-\\right),\\mathbf{m}_{i}\\left(t^-\\right),t,\\Delta t,l_i, l_j,\\mathbf{h_{i,j}}\\right),\n              \\label{eqc}\n        \\end{align}",
            "eq:8": "\\begin{equation}\n          \\mathbf{m}_{i}(t)=\\operatorname{Mem}\\left(\\mathbf{c}_{i}(t),  \\mathbf{m}_{i}\\left(t^{-}\\right)\\right),\n          \\label{eqd}\n        \\end{equation}",
            "eq:9": "\\begin{equation} \n        %-的符号\n             \\mathcal{L}_{\\text {Dis }}=\\frac{1}{2}\\left\\|\\mathbf{X} (t){\\mathbf{X}({t^-})}^{\\top}-\\mathbf{I}\\right\\|_{F}^{2}.\n             \\label{eqe}\n        \\end{equation}",
            "eq:10": "\\begin{equation}\n    \\left\\{\\begin{array}{l}\n\\partial_t\\mathbf{X}=\\mathbf{F}\\left( \\mathbf{X}, \\mathbf{\\Theta}_{}\\right) \\\\\n\\mathbf{X}(0)=\\mathbf{0},\n\\end{array} \\right.\n\\label{dynamic_ini}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n  \\left\\{\\begin{array}{ll}\n    \\frac{\\partial x(u,t)}{\\partial t} & = \\operatorname{div}[g(|\\nabla x(u, t)|) \\nabla x(u, t)] \\\\  \n    x(u, 0)& = 0,\n    \\end{array}\\right.\n    \\label{pm}\n\\end{equation}",
            "eq:12": "\\begin{align}\n\\label{co}\n    s_{ij}&=f(l_i||l_j||\\phi(t-t_{ij})),\\\\\n    f(\\mathbf{x})&= \\mathbf{W}^{(2)} \\cdot \\operatorname{ReLU}\\left(\\mathbf{W}^{(1)} \\mathbf{x}\\right),\n\\end{align}",
            "eq:13": "\\begin{equation}\n    \\resizebox{.7\\linewidth}{!}{$\n        {\\partial\\mathbf{X}_t}=-\\mathbf{M}^{\\top} \\sigma(\\mathbf{M} \\mathbf{X}\\mathbf{K}^{\\top})\\mathbf{S}\\left(\\mathbf{M} \\mathbf{X} \\mathbf{K}^{\\top}\\right) \\mathbf{K},\n    $}\n    \\label{eqf}\n\\end{equation}",
            "eq:14": "\\begin{equation}\n  \\mathbf{X}_{t+\\Delta t}=\\mathbf{X}_t+\\int_t^{t+\\Delta t}{\\partial_t \\mathbf{X}_t}d \\tau,\n  \\label{eqk}\n\\end{equation}",
            "eq:15": "\\begin{equation}\n    \\resizebox{.9\\linewidth}{!}{$\n        \\mathcal{L}_{\\text{Int}} = -\\sum_{i=1}^{m} (\\log(1-p_{\\mathrm{nor},i}) + \\log(p_{\\mathrm{att}, i}) + \\sum_{j=1}^{K} y_{i,k} \\log(p_{i,k}))\n    $}\n    \\label{eqg}\n\\end{equation}",
            "eq:16": "\\begin{equation}\n    \\mathcal{L}_{\\text {Smooth}}=\\sum_{t=0}^{T}\\left\\|\\mathbf{X}_{t+\\Delta {t}}-\\mathbf{X}_{t}\\right\\|_{2}.\n    \\label{eqh}\n\\end{equation}",
            "eq:17": "\\begin{equation}\n \\mathcal{L}= \\mathcal{L}_{\\text {Int}}+\\delta  \\mathcal{L}_{\\text {Smooth }}+\\zeta  \\mathcal{L}_{\\text{Dis}},\n \\label{eqj}\n\\end{equation}",
            "eq:18": "\\begin{equation}\n\\label{eq: Q_1}\n\\min _{\\mathbf H} =\\|\\mathbf Z- \\mathbf Z\\mathbf H \\|_{F}^{2}+\\lambda_{1}\\|\\mathbf \nH\\|_{F}^{2},  s.t. \\rm{diag}(\\mathbf H)=0,\n\\end{equation}",
            "eq:19": "\\begin{equation}\n\\mathcal{S}=\\operatorname {diag}(s_1, s_2, ..., s_L)=\\left[\\begin{array}{ccc}\ns_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & s_L\n\\end{array}\\right],\n\\end{equation}",
            "eq:20": "\\begin{equation}\n f_1(\\mathbf{P})=\\left\\|\\mathbf{P}^T \\mathbf{Z}_o-\\mathbf{P}^T \\mathbf{Z}_t\\right\\|_F^2,\n\\end{equation}",
            "eq:21": "\\begin{equation}\n\\begin{aligned}\\label{eq: Q_2}\n&\\min f_1(\\mathbf{P})+\\eta \\|\\mathbf{Q}\\|_F,\\\\\n&\\text { s.t. } \\mathbf{P} \\mathbf{Z}=\\mathbf{P} \\mathcal G (\\mathbf{Z}) \\mathbf{Q},\n\\end{aligned}\n\\end{equation}",
            "eq:22": "\\begin{equation}\n f_2(\\mathbf{P})=- \\sum_j\\left\\|\\mathbf{P}^T \\mathbf{Z}_o^{+j}-\\mathbf{P}^T \\mathbf{Z}_t^{-j}\\right\\|_F^2,\n\\label{reg}\n\\end{equation}",
            "eq:23": "\\begin{equation}\n\\begin{aligned}\n\\label{MSF_2}\n&\\min f(\\mathbf{P})+\\eta\\|\\tilde{\\mathbf{Q}}\\|_F, \\\\\n&\\text { s.t. } \\mathbf{P} \\mathbf{Z}=\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}, \\tilde{\\mathbf{Q}}=\\mathbf{Q},\n\\end{aligned}\n\\end{equation}",
            "eq:24": "\\begin{equation}\n f(\\mathbf{P})=\\frac{\\alpha}{2}\\left\\|\\mathbf{P}^T \\mathbf{Z}_o-\\mathbf{P}^T \\mathbf{Z}_t\\right\\|_F^2-\\frac{\\alpha}{2} \\sum_j\\left\\|\\mathbf{P}^T \\mathbf{Z}_o^{+j}-\\mathbf{P}^T \\mathbf{Z}_t^{-j}\\right\\|_F^2,\n\\end{equation}",
            "eq:25": "\\begin{equation}\n\\begin{aligned}\n&\\min f(\\mathbf{P})+\\eta\\|\\tilde{\\mathbf{Q}}\\|_F, \\\\\n&\\text { s.t. } \\mathbf{P} \\mathbf{Z}=\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}, \\tilde{\\mathbf{Q}}=\\mathbf{Q},\n\\end{aligned}\n\\end{equation}",
            "eq:26": "\\begin{equation}\n\\begin{aligned}\n&\\mathcal{L}=\\frac{\\alpha}{2}\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}\\right\\|_F^2\\\\\n&-\\frac{\\alpha}{2} \\sum_j\\left\\|\\mathbf{P} \\mathbf{Z}^{+j}-\\gamma \\mathbf{P} \\mathbf{Z}^{-j}\\right\\|_F^2+\\eta\\|\\tilde{\\mathbf{Q}}\\|_F \\\\\n&+<\\Phi_1, \\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}+<\\Phi_2, \\mathbf{Q}-\\tilde{\\mathbf{Q}}> \\\\\n&+\\frac{\\mu}{2}\\left(\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}\\right\\|_F^2+\\|\\mathbf{Q}-\\tilde{\\mathbf{Q}}\\|_F^2\\right),\n\\end{aligned}\n\\end{equation}",
            "eq:27": "\\begin{equation}\n    \\scalebox{0.9}{$\n        \\begin{aligned}\n            \\mathbf{P}^* &= \\arg \\min _{\\mathbf{P}} \\frac{\\alpha}{2}\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}\\right\\|_F^2\n            - \\frac{\\alpha}{2} \\sum_j\\left\\|\\mathbf{P} \\mathbf{Z}^{+j}-\\gamma \\mathbf{P} \\mathbf{Z}^{-j}\\right\\|_F^2 \\\\\n            & + \\frac{\\mu}{2}\\left\\|\\mathbf{P} \\mathbf{Z}-\\gamma \\mathbf{P} \\mathbf{Z} \\mathbf{Q}+\\frac{\\Phi_1}{\\mu}\\right\\|_F^2\n        \\end{aligned}\n    $}\n\\end{equation}",
            "eq:P": "\\begin{equation}\\label{P}\n\\mathbf{P}^*= -\\Phi_1 \\Delta_3^T (\\alpha \\Delta_1 \\Delta_1^T-\\lambda \\sum_j \\Delta_2 \\Delta_2^T+\\mu \\Delta_3 \\Delta_3^T+\\mathbf{I})^{-1}  {, }\n\\end{equation}",
            "eq:28": "\\begin{equation}\n\\begin{aligned}\\label{Q}\n\\mathbf{Q}^*&=\\arg \\min _{\\mathbf{Q}} \\frac{\\mu}{2}\\left\\|\\mathbf{P}^T \\mathbf {Z}-\\gamma \\mathbf{P}^T \\mathbf{Z} \\mathbf{Q}+\\frac{\\Phi_1}{\\mu}\\right\\|_F^2\\\\\n&+\\frac{\\mu}{2}\\left\\|\\mathbf{Q}-\\tilde{\\mathbf{Q}}+\\frac{\\Phi_2}{\\mu}\\right\\|_F^2 .\n\\end{aligned}\n\\end{equation}",
            "eq:29": "\\begin{equation}\n\\mathbf{Q}^*=\\left(\\mu \\gamma ^2 \\mathbf{P}^T \\mathbf{Z Z}^T \\mathbf{P}+\\mu \\mathbf{I}\\right)^{-1}\\left(\\mu \\gamma \\mathbf{Z}^T \\mathbf{P} \\Delta_3+\\mu \\tilde{\\mathbf{Q}}-\\Phi_2\\right),\n\\end{equation}",
            "eq:30": "\\begin{equation}\n\\tilde{\\mathbf{Q}}^*=\\arg \\min _{\\tilde{\\mathbf{Q}}}\\|\\tilde{\\mathbf{Q}}\\|_*+\\frac{\\mu}{2}\\left\\|\\mathbf{Q}-\\tilde{\\mathbf{Q}}+\\frac{\\Phi_2}{\\mu}\\right\\|_F^2,\n\\end{equation}",
            "eq:31": "\\begin{equation}\n\\begin{aligned}\\label{Qhat}\n\\tilde{\\mathbf{Q}}^* &=\\mathcal{S}_{1 / \\mu}\\left(\\mathbf{Q}+\\Phi_2 / \\mu\\right) \\\\\n&=\\mathbf{U}\\left\\{\\operatorname{sign}\\left(\\Sigma_{i i}\\right) \\max \\left(\\left|\\Sigma_{i i}-1 / \\mu\\right|, 0\\right)\\right\\} \\mathbf{V}^T,\n\\end{aligned}\n\\end{equation}",
            "eq:32": "\\begin{equation}\n\\begin{aligned}\\label{other}\n\\Phi_1 &=\\Phi_1+\\mu\\left(\\mathbf{P}^T \\mathbf Z-\\gamma \\mathbf{P}^T \\mathbf Z \\mathbf{Q}\\right), \\\\\n\\Phi_2 &=\\Phi_2+\\mu(\\mathbf{Q}-\\tilde{\\mathbf{Q}}), \\\\\n\\mu &=\\min \\left(1.01 \\mu, \\mu_{\\max }\\right).\n\\end{aligned}\n\\end{equation}",
            "eq:33": "\\begin{equation}\n    \\mathbf{Q^*} = \\mathbf{H} + \\epsilon * \\mathbf{Q}\n\\end{equation}",
            "eq:34": "\\begin{equation}\n\\begin{aligned} \n\\label{MFL LOSS}\n\\mathcal{L}=\\sum_{i=1}^{\\mathcal N}\\sum_{ j=1}^{\\mathcal Q}Y_{i,j}\\log(\\mathrm P_{i, j})+ \\beta \\|\\mathbf Z \\mathbf H- \\mathbf Z\\|\\\n\\end{aligned}\n\\end{equation}",
            "eq:35": "\\begin{equation}\n\\begin{aligned}   \\mathcal{L}=\\sum_{i=1}^{J}(L_{\\mathrm{att},i})\\log(P_{\\mathrm{query}, i})+ \\|\\mathbf{T}_{se} - \\mathbf{T}\\|\\\n\\end{aligned}\n\\end{equation}",
            "eq:36": "\\begin{equation}\n\\resizebox{0.45\\textwidth}{!}{$\n    \\tilde{\\mathbf{w}} = \\arg \\max \\biggl( \\mathbf{w_\\textit{N}} \\mathcal{F_\\textit{N}} - \\mathbf{w_\\textit{1}} \\mathcal{F_\\textit{1}} - \\sum_{i=2}^{N-1} \\biggl| 2 \\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}} - \\mathbf{w_\\textit{i+1}} \\mathcal{F_\\textit{i+1}} - \\mathbf{w_\\textit{i-1}} \\mathcal{F_\\textit{i-1}} \\biggr| \\biggr)\n$}\n\\end{equation}",
            "eq:37": "\\begin{equation}\n\\resizebox{0.45\\textwidth}{!}{$\n    \\begin{aligned}\n        \\tilde{\\mathbf{w}} &= \\arg \\max \\left( \\mathbf{w_\\textit{N}} \\mathcal{F_\\textit{N}} - \\mathbf{w_\\textit{1}} \\mathcal{F_\\textit{1}} - \\sum_{i=2}^{N-1} \\left| 2 \\mathbf{w_\\textit{i}} \\mathcal{F_\\textit{i}} - \\mathbf{w_\\textit{i+1}} \\mathcal{F_\\textit{i+1}} - \\mathbf{w_\\textit{i-1}} \\mathcal{F_\\textit{i-1}} \\right| \\right) \\\\\n        &= \\arg \\min \\left( -L + \\sum_{i=2}^{N-1} \\left| d_{i+1} - d_{i} \\right| \\right) \\\\\n        &= \\arg \\min \\left( \\frac{\\sum_{i=2}^{N-1} \\left| d_{i+1} - d_{i} \\right|}{N-1} - \\frac{L}{N-1} \\right)\n    \\end{aligned}\n$}\n\\end{equation}",
            "eq:38": "\\begin{equation}\n[\\Theta; \\theta] =: [\\Theta; \\theta] - \\alpha \\nabla \\mathcal{L}_D\n\\end{equation}",
            "eq:39": "\\begin{equation} \n        %-的符号\n             \\mathcal{L}_{\\mathcal{D}}([\\Theta; \\theta])=\\frac{1}{|\\mathcal{D}|}\\sum_{(x,y)\\subset\\mathcal{D}} l(f_{[\\Theta; \\theta]}(x),y)        \n        \\end{equation}",
            "eq:40": "\\begin{equation}\n    \\theta' \\leftarrow \\theta - \\beta \\nabla_{\\theta} \\mathcal{L}_{T^{(\\text{tr})}}([\\Theta; \\theta],\\Phi_{S_{\\{1,2\\}}}),\n    \\label{meta1}\n\\end{equation}",
            "eq:41": "\\begin{equation}\n    \\theta =: \\theta - \\beta \\nabla_{\\theta} \\mathcal{L}_{T^{(\\text{te})}}([\\Theta; \\theta'],\\Phi_{S_{\\{1,2\\}}}),\n    \\label{meta2}\n\\end{equation}",
            "eq:42": "\\begin{equation}\n    \\Phi_{Si} =: \\Phi_{Si} - \\gamma \\nabla \\Phi_{Si} \\mathcal{L}_{T^{(\\text{te})}}([\\Theta; \\theta'],\\Phi_{S_{\\{1,2\\}}}),\n\\end{equation}",
            "eq:43": "\\begin{equation}\n    SS(X; W, b; \\Phi_{S_{\\{1,2\\}}}) = (W \\odot \\Phi_{S_{1}})X + (b + \\Phi_{S_{2}})\n\\end{equation}"
        },
        "git_link": "https://github.com/qcydm/DIDS-MFL"
    }
}