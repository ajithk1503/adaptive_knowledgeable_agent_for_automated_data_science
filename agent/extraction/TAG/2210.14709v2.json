{
    "meta_info": {
        "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
        "abstract": "This paper studies learning on text-attributed graphs (TAGs), where each node\nis associated with a text description. An ideal solution for such a problem\nwould be integrating both the text and graph structure information with large\nlanguage models and graph neural networks (GNNs). However, the problem becomes\nvery challenging when graphs are large due to the high computational complexity\nbrought by training large language models and GNNs together. In this paper, we\npropose an efficient and effective solution to learning on large\ntext-attributed graphs by fusing graph structure and language learning with a\nvariational Expectation-Maximization (EM) framework, called GLEM. Instead of\nsimultaneously training large language models and GNNs on big graphs, GLEM\nproposes to alternatively update the two modules in the E-step and M-step. Such\na procedure allows training the two modules separately while simultaneously\nallowing the two modules to interact and mutually enhance each other. Extensive\nexperiments on multiple data sets demonstrate the efficiency and effectiveness\nof the proposed approach.",
        "author": "Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang",
        "link": "http://arxiv.org/abs/2210.14709v2",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "ICLR 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n% \\vspace{-5pt}\nGraphs are ubiquitous in the real world. In many graphs, nodes are often associated with text attributes, resulting in text-attributed graphs (TAGs)~\\citep{GraphFormers}. For example, in social graphs, each user might have a text description; in paper citation graphs, each paper is associated with its textual content. Learning on TAG has become an important research topic in multiple areas including graph learning, information retrieval, and natural language processing.\n\n% TAG 两个领域————GNN 和 LM\n%A TAG is rich of both textual and structural information, both of which could be beneficial for successful classification. Take paper classification as an example, the category of a paper could be classified either by its text or by its citations. For text learning, one can use a transformer-based language model (LM)~\\citep{Transformer,BERT} as a text encoder for modeling rich semantics within the text; For graph learning, one can leverage a graph neural network (GNN) as a graph aggregator to aggregate and transform the learned textual feature. Therefore the structural relationships could be well-modeled.\n\nIn this paper, we focus on a fundamental problem, learning effective node representations, which could be used for a variety of applications such as node classification and link prediction. Intuitively, a TAG is rich in textual and structural information, both of which could be beneficial for learning good node representations. The textual information presents rich semantics to characterize the property of each node, and one could use a pre-trained language model (LM) (e.g., BERT~\\citep{BERT}) as a text encoder. Meanwhile, the structural information preserves the proximity between nodes, and connected nodes are more likely to have similar representations. Such structural relationships could be effectively modeled by a graph neural network (GNN) via the message-passing mechanism. In summary, LMs leverage the local textual information of individual nodes, while GNNs use the global structural relationship among nodes.\n\n%The superiority of LMs and GNNs in modeling text and graph drives researchers to fuse them together. Conventionally, a static text representation, e.g. pre-trained LMs (PLMs)~\\citep{BERT} embedding, is taken as numerical feature for GNN training on a TAG~\\citep{PinSAGE}. However, since the semantic embedding is obtained without knowing graph structure and downstream tasks, this static learned feature could be far from optimal~\\citep{GraphFormers}.\n%For example, if we want to classify the field of papers with title and abstract in a citation network paper, the term ``Bert\" would strongly indicate a NLP paper. However, the static PLM embedding on other general corpus like books and news~\\citep{\n%BERT,Roberta} where ``Bert\" is recognized as a normal human name. In sum, with the fixed text encoding, the semantic information cannot be effectively updated for graph and downstream task.\n\n% Meng\n% LMs and GNNs solve node classification from complementary perspectives, and ideally one would wish to combine both models for more effective node classification in TAGs. Towards this goal, one straightforward idea is to cascade a LM-based text encoder and GNN-based message-passing module and train both modules together. However, this method suffers from severe scalability issues. This is because the memory complexity is proportional to the size of the observed graph and the amount of parameters in the LM. Therefore, on real-world TAGs where various nodes are densely connected, the memory cost of this method would become unaffordable.\n\n% 唐老师写的，感觉不太准确。因为分 batch 训练，Memory size 和图的大小无关，所以和小图大图无关。\n% An ideal approach for learning effective node representations is therefore to combine both the textual information and graph structure. One straightforward solution is to cascade an LM-based text encoder and GNN-based message-passing module and train both modules together. Such an approach is effective when graphs are small. However, it becomes problematic once the graphs become very large. This is because the size of the computational graph is proportional to the size of the graph structure between nodes as well as the language model capacity, and the memory complexity is proportional to the size of the observed graph and the number of parameters in the LM. Therefore, on real-world TAGs where various nodes are densely connected, the memory cost of this method would become unaffordable.\n\n\nAn ideal approach for learning effective node representations is therefore to combine both the textual information and graph structure. One straightforward solution is to cascade an LM-based text encoder and GNN-based message-passing module and train both modules together. However, this method suffers from severe scalability issues. This is because the memory complexity is proportional to the graph size as neighborhood texts are also encoded. Therefore, on real-world TAGs where nodes are densely connected, the memory cost of this method would become unaffordable.\n\n% 交互存在 Scalability 的问题\n%This intrinsic drawback of static LMs drives researchers to dynamically fuse LMs and GNNs together. However, directly joint training of both modules~\\citep{TextGNN,AdsGNN,GraphFormers} suffers from severe scalability. Specifically, LMs models the text in a fine-grained token-level manner, which takes large memory for each sentence. What's worse, as the neighborhood size grows up exponentially, extensive  sentences are involved for a single node. In practise, these models are trained on very few, e.g. 3~\\citep{AdsGNN,TextGNN}, sampled first-hop neighbors which leads to severe information loss. \n\nTo address such a problem, multiple solutions have been proposed. These methods reduce either the capacity of LMs or the size of graph structures for GNNs. More specifically, some studies choose to fix the parameters of LMs without fine-tuning them \\citep{ACL_LiuXSL20}. Some other studies reduce graph structures via edge sampling and perform message-passing only on the sampled edges \\citep{TextGNN,AdsGNN,GraphFormers}. Despite the improved scalability, reducing the LM capacity or graph size sacrifices the model effectiveness, leading to degraded performance of learning effective node representation. Therefore, we are wondering whether there exists a scalable and effective approach to integrating large LMs and GNNs on large text-attributed graphs.\n\n\n% Scalability and effectiveness\n%Therefore, a natural question raises: \\textit{Can we fuse GNN and LMs in a scalable and effective way?}\nIn this paper, we propose such an approach, named \\textbf{G}raph and \\textbf{L}anguage Learning by \\textbf{E}xpectation \\textbf{M}aximization GLEM. In the \\ours, instead of simultaneously training both the LMs and GNNs, we leverage a variational EM framework~\\citep{VariationalEM} to alternatively update the two modules. Take the node classification task as an example. The LM uses local textual information of each node to learn a good representation for label prediction, which thus models label distributions conditioned on text attributes. By contrast, for a node, the GNN leverages the labels and textual encodings of surrounding nodes for label prediction, and it essentially defines a global conditional label distribution. The two components are optimized to maximize a variational lower bound of the log-likelihood function, which can be achieved by alternating between an E-step and an M-step, where at each step we fix one component to update the other one. This separate training framework significantly improves the efficiency of \\ours, allowing it to scale up to real-world TAGs. In each step, one component presents pseudo-labels of nodes for the other component to mimic. By doing this, \\ours can effectively distill the local textual information and global structural information into both components, and thus \\ours enjoys better effectiveness in node classification. We conduct extensive experiments on three benchmark datasets to demonstrate the superior performance of \\ours. Notably, by leveraging the merits of both graph learning and language learning, \\ours-LM achieves on par or even better performance than existing GNN models, \\ours-GNN achieves new state-of-the-art results on ogbn-arxiv, ogbn-product, and ogbn-papers100M. \n\n%In \\ours, GNNs and LMs are alternatively trained by a variational EM ~\\citep{VariationalEM} framework. The text representation is modeled as the hidden variable, in LM phase (E-step), a LM is trained by the gold labels and soft labels predicted by the GNN to update the text representation; in GNN phase (M-step), a GNN is trained by the gold labels and soft labels predicted by LM. \\ours enjoys the merits of scalable training as GNN and LM are separately trained without restricting message-passing. \\ours is also effective: Since both GNN and LM modules are trained towards the mimicking the pseudo predictions of other modules, both modules are mutually enhanced by both text and graph information. We conduct extensive experiments on 3 benchmark datasets to demonstrate the superior performance of \\ours. Specifically, \\ours boosts both GNNs and LMs by 2.?\\% and 2.?\\% on average. \\ours achieve new state-of-the-art results on ogbn-arxiv. Moreover, we show that \\ours-LM achieves on par performance with GNNs which enables cheap and accurate inductive learning even for unseen nodes without graph structure.\n% We also show that with our framework LMs could have on par performance compared with GNNs.\n\n%In \\ours, GNNs and LMs are alternatively trained by a variational EM ~\\citep{VariationalEM} framework. The text representation is modeled as the hidden variable, in LM phase (E-step), a LM is trained by the gold labels and soft labels predicted by the GNN to update the text representation; in GNN phase (M-step), a GNN is trained by the gold labels and soft labels predicted by LM. \\ours enjoys the merits of scalable training as GNN and LM are separately trained without restricting message-passing. \\ours is also effective: Since both GNN and LM modules are trained towards the mimicking the pseudo predictions of other modules, both modules are mutually enhanced by both text and graph information. We conduct extensive experiments on 3 benchmark datasets to demonstrate the superior performance of \\ours. Specifically, \\ours boosts both GNNs and LMs by 2.?\\% and 2.?\\% on average. \\ours achieve new state-of-the-art results on ogbn-arxiv. Moreover, we show that \\ours-LM achieves on par performance with GNNs which enables cheap and accurate inductive learning even for unseen nodes without graph structure.\n% We also show that with our framework LMs could have on par performance compared with GNNs.\n\n\n% \\begin{figure}[th]\n% \\begin{center}\n% \\includegraphics[width=0.95\\linewidth]{figures/SeqG Intro.pdf}\n% \\end{center}\n% \\caption{(a) An example TAG. (b) Message passing GNN models. (c) language models.}\n% \\label{fig: Intro}\n% \\end{figure}\n\n\\vspace{-5pt}\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec: related_work}\n\\vspace{-5pt}\nRepresentation learning on text-attributed graphs (TAGs)~\\citep{GraphFormers} has been attracting growing attention in graph machine learning, and one of the most important problems is node classification.\n%The text-encoding is basically a natural language understanding task where the goal is to understand raw text, and has been rapidly evolving over the last years. Early works encode text data into latent distributed vectors~\\citep{DSSM,DSSPN}. Recently, with the superior performance of transformers~\\citep{Transformer} and pre-trained language models~\\citep{BERT,XLNet}, LMs have become the go-to model for encoding contextual semantics in the sentences and generate high-quality text representations. At the same time, the graph learning area has been vastly developed by graph neural network (GNNs)~\\citep{GCN,GAT,GIN}. A GNN takes numerical features as input and learns node representation by transforming representations and aggregating them according to the graph structure. With the capability of considering both node attributes and graph structure, GNNs have shown great performances in various tasks such as node classification~\\citep{GCN,GCNII}, link prediction~\\citep{SEAL,Grail} and graph classification~\\citep{GIN}.\nThe problem can be directly formalized as a text representation learning task, where the goal is to use the text feature of each node for learning. Early works resort to convolutional neural networks~\\citep{kim-2014-convolutional,DSSM} or recurrent neural networks~\\cite{tai2015improved}. Recently, with the superior performance of transformers~\\citep{Transformer} and pre-trained language models~\\citep{BERT,XLNet}, LMs have become the go-to model for encoding contextual semantics in sentences for text representation learning. At the same time, the problem can also be viewed as a graph learning task that has been vastly developed by graph neural networks (GNNs)~\\citep{GCN,GAT,GIN,SEAL,Grail}. A GNN takes numerical features as input and learns node representations by transforming representations and aggregating them according to the graph structure. With the capability of considering both node attributes and graph structures, GNNs have shown great performance in various applications, including node classification and link prediction. Nevertheless, LMs and GNNs only focus on parts of observed information (i.e., textual or structural) for representation learning, and the results remain to be improved.\n\n% Freeze\n%To enjoy the merits from both models, the combination of GNNs and LMs is a natural thought. One widely adopted way is to use a static~\\citep{GCN,GraphSAGE,ACL_LiuXSL20} text encoder to provide unlearnable node feature and only train GNN. Recently, a few methods proposed to utilize domain-adaptive pretraining~\\citep{gururangan-etal-2020-dont} on TAGs and predicts the graph structure using LMs~\\citep{GIANT,LinkBERT} to provide better LM embeddings for GNN. Nevertheless, although achieving better results, these LM embeddings still remain unlearnable in the GNN training phase.\n% \\czl{One widely adopted way is to embed node textual attributes into low-dimensional embeddings via (continuous) pre-trained language models, which will be viewed as unlearnable vectors in the subsequent GNN training.} \n%Such separated training paradigm ensures the model scalability as the number of trainable parameters in GNNs is relatively small. However, its performance is hindered by task and topology-irrelevant semantic modeling process.\n% Cascade\n%To overcome these limitations, endeavors have been made ~\\citep{TextGNN,AdsGNN,GraphFormers,bi2021leveraging,pang2022improving} co-train GNNs and LM under a joint learning framework. However, such co-training approaches suffer from severe scalability issues as all the neighbors need to be encoded by language models from scratch, incurring huge extra computation cost.\n%In practise, these models restrict the message-passing to very few (e.g. 3~\\citep{AdsGNN,TextGNN}) sampled neighbors, resulting in severe information loss. \n% \\czl{In practise, only very few neighbors (e.g. 3~\\citep{AdsGNN,TextGNN}) are incorporated, sacrificing massive topological information. }\n\nThere are some recent efforts focusing on the combination of GNNs and LMs, which allows one to enjoy the merits of both models. One widely adopted way is to encode the texts of nodes with a fixed LM, and further treat the LM embeddings as features to train a GNN for message passing. Recently, a few methods propose to utilize domain-adaptive pretraining~\\citep{gururangan-etal-2020-dont} on TAGs and predict the graph structure using LMs~\\citep{GIANT,LinkBERT} to provide better LM embeddings for GNN. Despite better results, these LM embeddings still remain unlearnable in the GNN training phase. Such a separated training paradigm ensures the model scalability as the number of trainable parameters in GNNs is relatively small. However, its performance is hindered by the task and topology-irrelevant semantic modeling process. To overcome these limitations, endeavors have been made ~\\citep{TextGNN,AdsGNN,GraphFormers,bi2021leveraging,pang2022improving} to co-train GNNs and LM under a joint learning framework. However, such co-training approaches suffer from severe scalability issues as all the neighbors need to be encoded by language models from scratch, incurring huge extra computation costs. In practice, these models restrict the message-passing to very few, e.g. 3~\\citep{AdsGNN,TextGNN}, sampled first-hop neighbors, resulting in severe information loss. \n\nTo summarize, existing methods of fusing LMs and GNNs suffer from either unsatisfactory results or poor scalability. In contrast to these methods, \\ours uses a pseudo-likelihood variational framework to integrate an LM and a GNN, which allows the two components to be trained separately, leading to good scalability. Also, \\ours encourages the collaboration of both components, so that it is able to use both textual semantics and structural semantics for representation learning, and thus enjoys better effectiveness.\n\nBesides, there are also some efforts using GNNs for text classification. Different from our approach, which uses GNNs to model the observed structural relationship between nodes, these methods assume graph structures are unobserved. They apply GNNs on synthetic graphs generated by words in a text~\\citep{huang2019text,linmei2019heterogeneous,zhang2020every} or co-occurrence patterns between texts and words~\\citep{huang2019text,liu2020tensor}. As structures are observed in the problem of node classification in text-attributed graphs, these methods cannot well address the studied problem.\n\nLastly, our work is related to GMNN \\citet{GMNN}, which also uses a pseudo-likelihood variational framework for node representation learning. However, GMNN aims to combine two GNNs for general graphs and it does not consider modeling textual features. Different from GMNN, \\ours focuses on TAGs, which are more challenging to deal with. Also, \\ours fuses a GNN and an LM, which can better leverage both structural features and textual features in a TAG, and thus achieves state-of-the-art results on a few benchmarks.\n\n%In sum, existing methods of fusing LMs and GNNs together suffer sacrifice effectiveness to alleviate scalability issues. In contrast, \\ours alternatively trains LM and GNN, which enables fine-grained graph-aware semantic encoding and train GNN on all semantics, achieving scalability and effectiveness at the same time.\n%In sum, to our best knowledge, current ways of fusing LMs and GNNs suffer from either scalability or effectiveness issues, which motivates us to develop a new framework to overcome these issues.\n\n\\vspace{-5pt}\n"
            },
            "section 3": {
                "name": "Background",
                "content": "\n\\label{sec: background}\nIn this paper, we focus on learning representations for nodes in TAGs, where we take node classification as an example for illustration. Before diving into the details of our proposed \\ours, we start with presenting a few basic concepts, including the definition of TAGs and how LMs and GNNs can be used for node classification in TAGs.\n\n\\vspace{-5pt}\n",
                "subsection 3.1": {
                    "name": "Text-attributed Graph",
                    "content": "\n\\label{Sec. SeqGraphLearning}\nFormally, a TAG $\\mathcal{G}_{S}=(V, A, \\mathbf{s}_{V})$ is composed of nodes $V$ and their adjacency matrix $A \\in \\mathbb{R}^{|V|\\times |V|}$, where each node $n\\in V$ is associated with a sequential text feature (sentence) $\\mathbf{s}_n$. %Specifically, each sequence is composed of discrete tokens: $\\mathbf{s}_n = \\left(s_{1}, \\ldots, s_{|\\mathbf{s}_n|}\\right)$. \nIn this paper, we study the problem of node classification on TAGs. Given a few labeled nodes $\\mathbf{y}_L$ of $L \\subset V$, the goal is to predict the labels $\\mathbf{y}_U$ for the remaining unlabeled objects $U = V \\: \\backslash \\: L$. \n\n\n%Specifically, for node classification on TAG, the goal is to model the joint distribution of node labels conditioned on TAG, i.e. $p\\left(\\mathbf{y}_V \\mid \\mathbf{s}_V \\right)$ and maximize the likelihood of observed data, i.e. $p\\left(\\mathbf{y}_L \\mid \\mathbf{s}_V\\right)$%However, to our best knowledge, existing models can not directly handle $A$ and $\\mathbf{s}_V$ together. \n\nIntuitively, node labels can be predicted by using either the textual information or the structural information, and representative methods are language models (LMs) and graph neural networks (GNNs) respectively. Next, we introduce the high-level ideas of both methods.\n\n%In fact, the node classification on TAG could be viewed as either a text leaning task, i.e. sequence/sentence classification, or a graph learning task, i.e. node classification. As our proposed methods are related to both views, we will first introduce how to solve the problems in each view. \n\n%GNNs well handle graph structure by message-passing numerical features according to $A$ yet fails in dealing with sequential text data; language models achieve remarkable performance in understanding texts, yet cannot leverage the neighborhood information directly.\n\n\n% Perhaps we could use \\ours in other text datasets, e.g. DNA, if they have structural relationships as well.\n% Learning on TAGs aim to learn a function $f_{\\Theta}(\\mathbf{s}_{V}): G_{T} \\rightarrow \\mathbf{y}, \\mathbf{y}$ could be either supervised information or self-supervised information!\n\\vspace{-5pt}\n"
                },
                "subsection 3.2": {
                    "name": "Language Models for Node Classification",
                    "content": "\n\\label{Sec: Textlearning}\nLanguage models aim to use the sentence $\\mathbf{s}_n$ of each node $n$ for label prediction, resulting in a text classification task~\\citep{socher-etal-2013-recursive,williams-etal-2018-broad}. The workflow of LMs can be characterized as below:\n%Through the lens of text learning, The aforementioned node classification on TAGs can be viewed as a traditional sentence classification problem. The learning could be achieved by maximizing the log-likelihood objective:\n%\\begin{equation}\n%     \\mathcal{O}_{\\text{MLE}}=\\sum_{n \\in L} \\log P_{\\theta}(\\mathbf{y}_n | \\mathbf{s}_n) \n%\\label{Eq. MLE loss}\n%\\end{equation}\n%where the posterior probability $P_{\\theta}(\\mathbf{y}_n | \\mathbf{s}_n)$ can be represented as a categorical distribution:\n\\begin{equation}\n    p_{\\theta}(\\mathbf{y}_n | \\mathbf{s}_n)=\\operatorname{Cat}(\\mathbf{y}_n\\mid \\operatorname{softmax}(\\operatorname{MLP}_{\\theta_2}(\\mathbf{h}_{n})));\\quad\\quad \\mathbf{h}_{n}=\\operatorname{SeqEnc}_{\\theta_1}(\\mathbf{s}_n),\n\\end{equation}\nwhere $\\operatorname{SeqEnc}_{\\theta_1}$ is a text encoder such as a transformer-based model~\\citep{Transformer,XLNet}, which projects the sentence $\\mathbf{s}_n$ into a vector representation $\\mathbf{h}_{n}$. Afterwards, the node label distribution $\\mathbf{y}_n$ can be simply predicted by applying an $\\operatorname{MLP}_{\\theta_2}$ with a softmax function to $\\mathbf{h}_{n}$.\n\nLeveraging deep architectures and pre-training on large-scale corpora, LMs achieve impressive results on many text classification tasks~\\citep{BERT,Roberta}.\nNevertheless, the memory cost is often high due to large model sizes. Also, for each node, LMs solely use its own sentence for classification, and the interactions between nodes are ignored, leading to sub-optimal results especially on nodes with insufficient text features.\n\n%where the $\\operatorname{SeqEnc}_{\\theta}$ is often parameterized by a sequence encoder such as transformer-based LM~\\citep{Transformer,XLNet}. Specifically, a transformer firstly parameterizes the token representations and models their interactions by the attention mechanism. Then, the token representations are then pooled into a sentence representation $\\mathbf{z}_n\\in \\mathbb{R}^{1\\times d}$, by looking up the embedding of the [CLS] token~\\citep{BERT}, and further mapped to sentence labels via a MLP. By optimizing Eq.~\\ref{Eq. MLE loss}, a LM learns to classify each sentence by encoding the fine-grained semantics of node text-attributes. Yet, it cannot directly leverage the interactions between the nodes.\n\\vspace{-5pt}\n"
                },
                "subsection 3.3": {
                    "name": "Graph Neural Networks for Node Classification",
                    "content": "\nGraph neural networks approach node classification by using the structural interactions between nodes. Specifically, GNNs leverage a message-passing mechanism, which can be described as:\n%Through the lens of graph learning, the node classification problem could be addressed by a message passing GNN. A $L$-layer GNN with parameters $\\phi$ models the posterior probability $P_{\\phi}(\\mathbf{y}_n | \\mathbf{s}_n)$ as:\n\\begin{equation}\np_{\\phi}(\\mathbf{y}_n | A)=\\operatorname{Cat}(\\mathbf{y}_n\\mid \\operatorname{softmax}(\\mathbf{h}_{n}^{(L)}));\\quad\\quad\n\\mathbf{h}_{n}^{(l)} = \\sigma(\\operatorname{AGG}_\\phi(\\operatorname{MSG}_\\phi(\\mathbf{h}_{\\operatorname{NB}(n)}^{(l-1)}),A)),\n\\end{equation}\nwhere $\\phi$ denotes the parameters of GNN, $\\sigma$ is an activation function, $\\operatorname{MSG}_\\phi(\\cdot)$ and $\\operatorname{AGG}_\\phi(\\cdot)$ stand for the message and aggregation functions respectively, $\\operatorname{NB}(n)$ denotes the neighbor nodes of $n$. Given the initial text encodings $\\mathbf{h}_{n}^{(0)}$, e.g. pre-trained LM embeddings, a GNN iteratively updates them by applying the message function and the aggregation function, so that the learned node representations and predictions can well capture the structural interactions between nodes.\n\nWith the message-passing mechanism, GNNs are able to effectively leverage the structural information for node classification. Despite the good performance on many graphs, GNNs are not able to well utilize the textual information, and thus GNNs often suffer on nodes with few neighbors.\n\n\n\n\n\\vspace{-4pt}\n"
                }
            },
            "section 4": {
                "name": "Methodology",
                "content": "\n%In this section, we introduce our approach called the Circular Training (\\ours) for semi-supervised node classification on TAG. The goal of \\ours is to combine the advantages of both the language models and MP-GNNs in a scalable manner, such that we can learn fine-grained sequence representations aiming at predicting object labels, as well as model the structural dependency between nodes. Specifically, \\ours models the object labels  $\\mathbf{h}_V$, i.e. $p\\left(\\mathbf{y}_{L} \\mid \\mathbf{s}_{V}\\right)$ (\\textcolor{blue} by a variational EM~\\citep{VariationalEM} framework which models the sequence representation as a latent distribution $\\mathbf{h}_V$. In the E-step, a language model is used to learn the latent distribution for label prediction. In the M-step, a graph neural network is employed to model the structural interactions of the sequence representations. Next, we introduce the details of the \\ours approach.\n%In this section, we introduce our proposed approach which combines GNN and LM for node classification in TAGs. One straightforward way is to apply an LM to each node for node representation learning, and then feed the representations to a GNN, which further performs message passing for node classification. Although the method can be trained in an end-to-end fashion, it suffers from severe scalability issues. Towards improving scalability, recent works either fix the LM encoder or choosing only a few neighbors via sampling algorithms for message passing. Despite the better efficiency, they sacrifice the model capacity, leading to degraded classification results.\n\nIn this section, we introduce our proposed approach which combines GNN and LM for node representation learning in TAGs. Existing methods either suffer from scalability issues or have poor results in downstream applications such as node classification. Therefore, we are looking for an approach that enjoys both good scalability and capacity.\n\nToward this goal, we take node classification as an example and propose \\ours. \\ours leverages a variational EM framework, where the LM uses the text information of each sole node to predict its label, which essentially models the label distribution conditioned on local text attribute; whereas the GNN leverages the text and label information of surrounding nodes for label prediction, which characterizes the global conditional label distribution. The two modules are optimized by alternating between an E-step and an M-step. In the E-step, we fix the GNN and let the LM mimic the labels inferred by the GNN, allowing the global knowledge learned by the GNN to be distilled into the LM. In the M-step, we fix the LM, and the GNN is optimized by using the node representations learned by the LM as features and the node labels inferred by the LM as targets. By doing this, the GNN can effectively capture the global correlations of nodes for precise label prediction. With such a framework, the LM and GNN can be trained separately, leading to better scalability. Meanwhile, the LM and GNN are encouraged to benefit each other, without sacrificing model performance.\n\n%models the joint distribution of object labels conditioned on object attributes, i.e.  $p\\left(\\mathbf{y}_{V} \\mid A, \\mathbf{s}_{V}\\right)$, by message-passing and language modeling modules, which is optimized with a variational EM framework. \n\n%\\subsection{The \\ours Framework}\n",
                "subsection 4.1": {
                    "name": "The Pseudo-likelihood Variational Framework",
                    "content": "\n%As introduced in $\\S$\\ref{Sec. SeqGraphLearning}, while directly optimizing Eq. \\ref{Eq. Likelihood} is intractable, existing methods jointly optimize Eq. \\ref{Eq. JointTraining} while restricting the message passing or language modeling module to achieve scalable learning on TAG. Alternatively, instead of optimizing Eq. \\ref{Eq. Likelihood} with restricted modules, we optimize the evidence lower bound (ELBO) of the log-likelihood function:\n\nOur approach is based on a pseudo-likelihood variational framework, which offers a principled and flexible formalization for model design. To be more specific, the framework tries to maximize the log-likelihood function of the observed node labels, i.e., $p(\\mathbf{y}_L | \\mathbf{s}_V, A)$. Directly optimizing the function is often hard due to the unobserved node labels $\\mathbf{y}_U$, and thus the framework instead optimizes the evidence lower bound as below:\n\\begin{equation}\n    \\log p(\\mathbf{y}_L | \\mathbf{s}_V, A) %= \\log \\sum_{\\mathbf{y}_U} p(\\mathbf{y}_L, \\mathbf{y}_U | \\mathbf{s}_V, A) \n    \\geq \\mathbb{E}_{q(\\mathbf{y}_U|\\mathbf{s}_U)}[\\log p(\\mathbf{y}_L, \\mathbf{y}_U | \\mathbf{s}_V, A) - \\log q(\\mathbf{y}_U|\\mathbf{s}_U)],\n\\end{equation}\nwhere $q(\\mathbf{y}_U|\\mathbf{s}_U)$ is a variational distribution and the above inequality holds for any $q$. The ELBO can be optimized by alternating between optimizing the distribution $q$ (i.e., E-step) and the distribution $p$ (i.e., M-step). In the E-step, we aim at updating $q$ to minimize the KL divergence between $q(\\mathbf{y}_U|\\mathbf{s}_U)$ and $p(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)$, so that the above lower bound can be tightened. In the M-step, we then update $p$ towards maximizing the following pseudo-likelihood~\\citep{pseudolikelihood} function:\n\\begin{equation}\n\\label{eq:pseudo-likelihood}\n    \\mathbb{E}_{q(\\mathbf{y}_U|\\mathbf{s}_U)}[\\log p(\\mathbf{y}_L, \\mathbf{y}_U | \\mathbf{s}_V, A)] \\approx \\mathbb{E}_{q(\\mathbf{y}_U|\\mathbf{s}_U)}[\\sum_{n \\in V} \\log p(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_{V \\setminus n})].\n\\end{equation}\nThe pseudo-likelihood variational framework yields a formalization with two distributions to maximize data likelihood. The two distributions are trained via a separate E-step and M-step, and thus we no longer need the end-to-end training paradigm, leading to better scalability which naturally fits our scenario. Next, we introduce how we apply the framework to node classification in TAGs by instantiating the $p$ and $q$ distributions with GNNs and LMs respectively.\n\n"
                },
                "subsection 4.2": {
                    "name": "Parameterization",
                    "content": "\n%The pseudolikelihood variational framework consists of a distribution $p$ and a distribution $q$. In this section, we explain how we instantiate in our case.\n\nThe distribution $q$ aims to use the text information $\\mathbf{s}_U$ to define node label distribution. In \\ours, we use a mean-field form, assuming the labels of different nodes are independent and the label of each node only depends on its own text information, yielding the following form of factorization:\n\\begin{equation}\n    q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U) = \\prod_{n \\in U} q_\\theta(\\mathbf{y}_n | \\mathbf{s}_n).\n\\end{equation}\nAs introduced in Section 3.2, each term $q_\\theta(\\mathbf{y}_n | \\mathbf{s}_n)$ can be modeled by a transformer-based LM $q_\\theta$ parameterized by $\\theta$, which effectively models the fine-grained token interactions by the attention mechanism~\\citep{Transformer}.\n\nOn the other hand, the distribution $p$ defines a conditional distribution $p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_{V \\setminus n})$, aiming to leverage the node features $\\mathbf{s}_V$, graph structure $A$, and other node labels $\\mathbf{y}_{V \\setminus n}$ to characterize the label distribution of each node $n$. Such a formalization can be naturally captured by a GNN through the message-passing mechanism. Thus, we model $p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_{V \\setminus n})$ as a GNN $p_\\phi$ parameterized by $\\phi$ to effectively model the structural interactions between nodes. Note that the GNN $p_\\phi$ takes the node texts $\\mathbf{s}_V$ as input to output the node label distribution. However, the node texts are discrete variables, which cannot be directly used by the GNN. Thus, in practice we first encode the node texts with the LM $q_\\theta$, and then use the obtained embeddings as a surrogate of node texts for the GNN $p_\\phi$.\n\nIn the following sections, we further explain how we optimize the LM $q_\\theta$ and the GNN $p_\\phi$ to let them collaborate with each other.\n\n"
                },
                "subsection 4.3": {
                    "name": "E-step: LM Optimization",
                    "content": "\n\nIn the E-step, we fix the GNN and aim to update the LM to maximize the evidence lower bound. By doing this, the global semantic correlations between different nodes can be distilled into the LM.\n\nFormally, maximizing the evidence lower bound with respect to the LM is equivalent to minimizing the KL divergence between the posterior distribution and the variational distribution, i.e., $\\text{KL}(q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)||p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L))$. However, directly optimizing the KL divergence is nontrivial, as the KL divergence relies on the entropy of $q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)$, which is hard to deal with. To overcome the challenge, we follow the wake-sleep algorithm~\\citep{hinton1995wake} to minimize the reverse KL divergence, yielding the following objective function to maximize with respect to the LM $q_\\theta$:\n\\begin{equation}\n\\begin{aligned}\n    -\\text{KL}(p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L) || q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)) &= \\mathbb{E}_{p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)}[\\log q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)] + \\text{const} \\\\\n    &= \\sum_{n \\in U} \\mathbb{E}_{p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L)}[\\log q_\\theta(\\mathbf{y}_n | \\mathbf{s}_n)]+ \\text{const},\n\\end{aligned}\n\\end{equation}\nwhich is more tractable as we no longer need to consider the entropy of $q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)$. Now, the sole difficulty lies in computing the distribution $p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L)$. Remember that in the original GNN which defines the distribution $p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_{V \\setminus n})$, we aim to predict the label distribution of a node $n$ based on the surrounding node labels $\\mathbf{y}_{V \\setminus n}$. However, in the above distribution $p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L)$, we only condition on the observed node labels $\\mathbf{y}_L$, and the labels of other nodes are unspecified, so we cannot compute the distribution directly with the GNN. In order to solve the problem, we propose to annotate all the unlabeled nodes in the graph with the pseudo-labels predicted by the LM, so that we can approximate the distribution as follows:\n\\begin{equation}\n    p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L) \\approx p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L, \\mathbf{\\hat{y}}_{U \\setminus n}),\n\\end{equation}\nwhere $\\mathbf{\\hat{y}}_{U \\setminus n} = \\{ \\mathbf{\\hat{y}}_{n^{\\prime}} \\}_{n^{\\prime} \\in U \\setminus n}$ with each $\\mathbf{\\hat{y}}_{n^{\\prime}} \\sim q_\\theta(\\mathbf{y}_{n^{\\prime}} | \\mathbf{s}_{n^{\\prime}})$.\n\nBesides, the labeled nodes can also be used for training the LM. Combining it with the above objective function, we obtain the final objective function for training the LM:\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{O}(q) %&= \\alpha \\mathcal{O}_{\\text{\\text{Inf}}}+ (1-\\alpha)\\mathcal{O}_{\\text{MLE}} \\\\\n&= \\alpha \\sum_{n \\in U} \\mathbb{E}_{p(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L, \\mathbf{\\hat{y}}_{U \\setminus n})}[\\log q(\\mathbf{y}_n | \\mathbf{s}_n)] + (1 - \\alpha) \\sum_{n \\in L} \\log q(\\mathbf{y}_n | \\mathbf{s}_n),\n\\end{aligned}\n\\label{Eq. Inf-Objective}\n\\end{equation}\nwhere $\\alpha$ is a hyperparameter. Intuitively, the second term $\\sum_{n \\in L} \\log q(\\mathbf{y}_n | \\mathbf{s}_n)$ is a supervised objective which uses the given labeled nodes for training. Meanwhile, the first term could be viewed as a knowledge distilling process which teaches the LM by forcing it to predict the label distribution based on neighborhood text-information. \n\n%Intuitively, as disccused in $\\S$\\ref{Sec: Textlearning}, the $\\mathcal{O}_{\\text{\\text{MLE}}}$ performs text learning by mining the text information in each node without considering structure; the $\\mathcal{O}_{\\text{\\text{Inf}}}$ could be viewed as a knowledge distilling process which teaches the LM by forcing it to predict the label distribution based on neighborhood text-information. \n\n\n\n\n%where $p_\\phi\\left(\\mathbf{z}_V | \\mathbf{s}_V \\right)$ defines a language model, aiming to generate node representations $\\mathbf{z}_V$ from node texts $\\mathbf{s}_V$, and $p_\\theta\\left( \\mathbf{y}_V | \\mathbf{z}_V, A \\right)$ defines a graph neural network which predicts node labels $\\mathbf{y}_V$ according to node representations $\\mathbf{z}_V$ and graph structures $A$. $\\theta$ and $\\phi$ are parameters of the graph neural network and the language model respectively.\n\n%To optimize the LM and GNN, one straightforward way is to maximize the log-likelihood function of the observed node labels. Formally, the objective function is given as below:\n%\\begin{equation}\n%    \\max_{\\theta,\\phi} \\mathcal{O}(\\theta,\\phi) = \\log p_{\\theta,\\phi}\\left(\\mathbf{y}_L \\mid \\mathbf{s}_{V},A\\right) = \\log \\int_{\\mathbf{z}_V} p_\\theta\\left( \\mathbf{y}_L, \\mathbf{z}_V | \\mathbf{s}_V, A \\right) = \\log \\int_{\\mathbf{z}_V} p_\\theta\\left( \\mathbf{y}_L | \\mathbf{z}_V, A \\right) p_\\phi\\left(\\mathbf{z}_V | \\mathbf{s}_V \\right).\n%\\end{equation}\n%However, directly optimizing the above objective is nontrivial, as $\\mathbf{y}_U$ and $\\mathbf{z}_V$ serve as latent variables, leading to a summation over all possible values of $\\mathbf{y}_U$ and $\\mathbf{z}_V$, which is intractable. Although one can use the reparameterization trick~\\citep{VAE} as a solution, the resulted method suffers from severe scalability issues as introduced above.\n\n%In order to address the challenge and develop an efficient algorithm for fusing GNNs and LMs, we propose a two-phase optimization approach, which alternates between optimizing the GNN $p_\\theta$ (i.e., the GNN phase) and optimizing the LM $p_\\phi$ (i.e., the LM phase). Next, we introduce the details of both phases.\n\n"
                },
                "subsection 4.4": {
                    "name": "M-step: GNN Optimization",
                    "content": "\n\nDuring the GNN phase, we aim at fixing the language model $q_\\theta$ and optimizing the graph neural network $p_\\phi$ to maximize the pseudo-likelihood as introduced in \\eqref{eq:pseudo-likelihood}.\n\nTo be more specific, we use the language model to generate node representations $\\mathbf{h}_V$ for all nodes and feed them into the graph neural network as text features for message passing. Besides, note that \\eqref{eq:pseudo-likelihood} relies on the expectation with respect to $q_\\theta$, which can be approximated by drawing a sample $\\mathbf{\\hat{y}}_U$ from $q_\\theta(\\mathbf{y}_U|\\mathbf{s}_U)$. In other words, we use the language model $q_\\theta$ to predict a pseudo-label $\\mathbf{\\hat{y}}_n$ for each unlabeled node $n \\in U$, and combine all the labels $\\{ \\mathbf{\\hat{y}}_n \\}_{n \\in U}$ into $\\mathbf{\\hat{y}}_U$. With both the node representations and pseudo-labels from the LM $q_\\theta$, the pseudo-likelihood can be rewritten as follows:\n\\begin{equation}\n    \\mathcal{O}(\\phi) = \\beta \\sum_{n \\in U} \\log p_\\phi(\\mathbf{\\hat{y}}_n | \\mathbf{s}_V, A, \\mathbf{y}_{L}, \\mathbf{\\hat{y}}_{U \\setminus n}) + (1 - \\beta) \\sum_{n \\in L} \\log p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_{L \\setminus n}, \\mathbf{\\hat{y}}_{U}),\n\\label{Eq. GNN-Objective}\n\\end{equation}\nwhere $\\beta$ is a hyperparameter which is added to balance the weight of the two terms. Again, the first term can be viewed as a knowledge distillation process which injects the knowledge captured by the LM into the GNN via all the pseudo-labels. The second term is simply a supervised loss, where we use observed node labels for model training. \n\nFinally, the workflow of the EM algorithm is summarized in Fig.~\\ref{fig: Model Framework}. The optimization process iteratively does the E-step and the M-step. In the E-step, the pseudo-labels predicted by the GNN together with the observed labels are utilized for LM training. In the M-step, the LM provides both text embeddings and pseudo-labels for the GNN, which are treated as input and target respectively for label prediction. Once trained, both the LM in E-step (denoted as \\ours-LM) and the GNN (denoted as \\ours-GNN) in M-step can be used for node label prediction. %In our experiments, we empirically compare their performance.\n\n\\vspace{-5pt}\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n%In this section, we conduct extensive experiments to evaluate the proposed \\ours framework comprehensively.\n%We first demonstrate the effectiveness of \\ours of transductive node classification on TAG in $\\S$\\ref{Sec: Exp_NodeCla}. We also show that \\ours enables both GNNs and LMs to generalize to structure-free inductive scenarios, where new nodes are given only text attributes without connected neighbors. Then, we discuss the scalability, efficiency, and effectiveness of different ways of fusing LMs and GNNs.  \nIn this section, we conduct experiments to evaluate the proposed \\ours framework, where two settings are considered. The first setting is transductive node classification, where given a few labeled nodes in a TAG, we aim to classify the rest of the nodes. Besides that, we also consider a structure-free inductive setting, and the goal is to transfer models trained on labeled nodes to unseen nodes, for which we only observe the text attributes without knowing their connected neighbors. \n",
                "subsection 5.1": {
                    "name": "Experimental Setup",
                    "content": "\n\\label{sec. exp_setup}\n\n%We consider node classification on TAG as our downstream task and evaluate \\ours on three large-scale TAG datasets~\\citep{OGB}: ogbn-arxiv, ogbn-products, and ogbn-papers100M. \n%In ogbn-arxiv and ogbn-papers100M~\\citep{MAGdata,OGB}, the goal is to classify the fields of the papers in the citation network. We concatenate the raw text feature, i.e. title and abstract, of each paper for LM input. In ogbn-products~\\cite{ClusterGCN,OGB}, the goal is to predict the category of products in Amazon co-purchase network. The text-features are the product descriptions. \n%The statistics of these dataset are shown in Table ~\\ref. Following the OGB benchmarking protocol, the mean and standard deviation of validation and test accuracy on 3 runs are reported in the experiments.\n\\noindent\\textbf{Datasets.}\nThree TAG node classification benchmarks are used in our experiment, including ogbn-arxiv, ogbn-products, and ogbn-papers100M~\\citep{OGB}. The statistics of these datasets are shown in Table ~\\ref{tab: data statistic}. %Following the OGB benchmarking protocol, the mean and standard deviation of validation and test accuracy over three runs are reported.\n\n\\noindent\\textbf{Compared Methods.}\n%As introduced in $\\S$\\ref{sec: background}, the node classification on TAG could be handled by either GNN or LM. For LM, we use DeBERTa~\\cite{DeBERTa} as the backbone for our model and report the fine-tuning results denoted as \\textbf{LM-Ft}. For GNNs, we use widely adopted baselines, e.g. \\textbf{GCN}~\\citep{GCN}, and \\textbf{GraphSAGE}~\\citep{GraphSAGE}, and top-ranked baselines such as \\textbf{RevGAT}~\\citep{RevGAT}, \\textbf{GAMLP}~\\citep{GAMLP}, and \\textbf{SAGN}~\\citep{SAGN}. As mentioned in the $\\S$\\ref{sec: related_work} one way to combine GNNs and LMs is to use a pre-trained LM to generate node embeddings, which are fixed and further fed into a GNN for message passing and node classification. Here we choose three representatives: the raw feature of OGB, denoted as $\\mathbf{X}_{\\text {OGB}}$; the LM embedding inferenced by pre-trained LM, i.e. the DeBERTa-base checkpoint~\\footnote{\\url{https://huggingface.co/microsoft/DeBERTa-base}}, denoted as $\\mathbf{X}_{\\text {PLM}}$; and the GIANT~\\citep{GIANT} feature, denoted as $\\mathbf{X}_{\\text {GIANT}}$. %Notably, the GIANT feature is obtained by aXRTransformer~\\citep{XRTransformer} pre-trained to predict the graph structure and achieved top performances on the OGB leaderboard. \n%We also compare our method against other joint learning method such as GraphFormers~, \nWe compare \\ours-LM and \\ours-GNN against LMs, GNNs, and methods combining both of worlds. For language models, we apply DeBERTa~\\cite{DeBERTa} to our setting by fine-tuning it on labeled nodes, and we denote it as LM-Ft.\n% \\emph{Language Models}. We apply DeBERTa~\\cite{DeBERTa} to our setting by fine-tuning it on labeled nodes, and we denote it as  \\textbf{LM-Ft}.\nFor GNNs, a few well-known GNNs are selected, i.e., GCN~\\citep{GCN} and GraphSAGE~\\citep{GraphSAGE}. Three top-ranked baselines on leaderboards are included, i.e., RevGAT~\\citep{RevGAT}, GAMLP~\\citep{GAMLP},  SAGN~\\citep{SAGN}. For each GNN, we try different kinds of node features, including (1) the raw feature of OGB, denoted as $\\mathbf{X}_{\\text {OGB}}$; (2) the LM embedding inferenced by pre-trained LM, i.e. the DeBERTa-base checkpoint~\\footnote{\\url{https://huggingface.co/microsoft/DeBERTa-base}}, denoted as \\textbf{$\\mathbf{X}_{\\text {PLM}}$}; (3) the GIANT~\\citep{GIANT} feature, denoted as \\textbf{$\\mathbf{X}_{\\text {GIANT}}$}.\n\n%\\emph{Hybrid Methods}. We also compare against hybrid methods which combine LMs and GNNs. Specifically, we explore various ways to generate node embeddigns, which are fixed and fed into a GNN for message passing and node classification. Here we choose three methods, i.e., the raw feature of OGB, denoted as \\textbf{$\\mathbf{X}_{\\text {OGB}}$}; the LM embedding inferenced by pre-trained LM, i.e. the DeBERTa-base checkpoint~\\footnote{\\url{https://huggingface.co/microsoft/DeBERTa-base}}, denoted as \\textbf{$\\mathbf{X}_{\\text {PLM}}$}; and the GIANT~\\citep{GIANT} feature, denoted as \\textbf{$\\mathbf{X}_{\\text {GIANT}}$}.\n\\noindent\\textbf{Implementation Details. } \n\nWe adopt the DeBERTa~\\citep{DeBERTa} as the LM model and fine-tune it for node classification to provide initial checkpoints for LMs and further infer text embeddings and predictions for the first GNN M-step. To provide predictions for the first-LM E-step, we use pre-trained GNN predictions, e.g. the original GNN predictions, for the initial target labels. The best EM-iteration is chosen based on the validation accuracy of \\ours-GNN. During optimization, \\ours can start with either the E-step or the M-step. For better performance, we let the better module generates pseudo-labels and train the other module first. For example, if pre-trained GNN outperforms pre-trained LM, we start with the E-step (LM training).\nFor fair comparison against other feature learning methods such as GIANT, the hyper-parameters of GNNs are set to the best settings described in the paper or in the official repository, other parameters are tuned by grid search.\n\n"
                },
                "subsection 5.2": {
                    "name": "Transductive Node Classification",
                    "content": "\n\\label{Sec: Exp_NodeCla}\n\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Node classification accuracy for the obgn-Arxiv and ogbn-Products datasets. (mean±std\\%, the best results are bolded and the runner-ups are underlined). G ↑ denotes the improvements of \\ours-GNN over the same GNN trained with OGB feature; L ↑ denotes the improvements of \\ours-LM over directly fine-tune LM. ``+'' denotes that tricks are integrated by the GNN model.}\n% \\resizebox{\\textwidth}{35mm}{\n%     \\begin{tabular}{c|cc|cccccccc}\n%     \\hline\n%     \\multicolumn{2}{c}{Methods} &       & \\multicolumn{5}{c|}{GNN}              & \\multicolumn{3}{c}{LM} \\bigstrut\\\\\n%     \\hline\n%     \\multicolumn{2}{c}{} & \\multicolumn{1}{c}{} & X-OGB & X-GIANT & X-FLM & GLEM-G & G ↑   & LM-Ft & GLEM-L & L↑ \\bigstrut\\\\\n%     \\hline\n%     \\hline\n%     \\multirow{8}[2]{*}{Arxiv} & \\multicolumn{2}{c|}{\\multirow{2}[1]{*}{GCN}} & 73.00 ± 0.17 & 74.89 ± 0.17 & 47.56 ± 1.91 & 76.86 ± 0.19 & 3.86  & 75.27 & 76.17 ± 0.47 & 0.9\\\\\n%           & \\multicolumn{2}{c|}{} & 71.74 ± 0.29 & 73.29 ± 0.1 & 48.19 ± 1.47 & 75.93 ± 0.19 & 4.19  & 74.13 & 75.71 ± 0.24 & 1.58 \\\\\n%           & \\multicolumn{2}{c|}{\\multirow{2}[0]{*}{SAGE}} & 72.77 ± 0.16 & 75.95 ± 0.11 & 56.16 ± 0.46 & 76.45 ± 0.05 & 3.68  & 75.27 & 75.32 ± 0.04 & 0.6 \\\\\n%           & \\multicolumn{2}{c|}{} & 71.49 ± 0.27 & 74.35 ± 0.14 & 56.39 ± 0.82 & 75.5 ± 0.24 & 4.01  & 74.13 & 74.53 ± 0.12 & 1.44 \\\\\n%           & \\multicolumn{2}{c|}{\\multirow{2}[0]{*}{GAMLP}} &  62.20 ± 0.11 & 75.01 ± 0.02 & 71.14 ± 0.19 & 76.95 ± 0.14 & 14.75 & 75.27 & 75.64 ± 0.30 & 0.44 \\\\\n%           & \\multicolumn{2}{c|}{} &  56.53 ± 0.02 &  73.35 ± 0.14 & 70.15 ± 0.22 & 75.62 ± 0.23 & 19.09 & 74.13 & 74.48 ± 0.41 & 2.04 \\\\\n%           & \\multicolumn{2}{c|}{\\multirow{2}[1]{*}{RevGAT}} & 75.01 ± 0.10 & 77.01 ± 0.09 & 71.40 ± 0.23 & 77.07 ± 0.04 & 2.06  & 75.27 & 75.75 ± 0.07 & 0.48 \\\\\n%           & \\multicolumn{2}{c|}{} & 74.02 ± 0.18 & 75.90 ± 0.19\t & 70.21 ± 0.3 & 76.74 ± 0.08 & 2.72  & 74.13 & 75.45 ± 0.12 & 1.32\\\\\n% \\cline{4-11}    \\multirow{6}[2]{*}{Products} & \\multicolumn{2}{c|}{\\multirow{2}[1]{*}{SAGE}} & 91.99 ± 0.07     & 93.47 ± 0.14 & 86.74 & 93.84 & -     & 92    & 92.71 & 0.71 \\\\\n%           & \\multicolumn{2}{c|}{} & 78.29 ± 0.16 & 82.33 ± 0.37 & 71.09 ± 0.65 & 83.16 & 4.87  & 79.84 & 81.45 & 1.61 \\\\\n%           & \\multicolumn{2}{c|}{\\multirow{2}[0]{*}{GAMLP}} & 93.12 ± 0.03 & 93.99 ± 0.04 & 91.65 ± 0.17 & 94.19 ± 0.01 & 1.07  & 91.82 & 90.56 ± 0.04 & -1.26 \\\\\n%           & \\multicolumn{2}{c|}{} & 83.54 ± 0.09 & 83.16 ± 0.07 & 80.49 ± 0.19 & 85.09 ± 0.21 & 1.55  & 79.63 & 82.23 ± 0.27 & 2.6 \\\\\n%           & \\multicolumn{2}{c|}{\\multirow{2}[1]{*}{SAGN+}} & 93.25 ± 0.04 & 93.89 ± 0.02 & 92.78 ± 0.04 & 94.03 ± 0.04 & 0.78  & 91.82 & 92.03 & 0.21 \\\\\n%           & \\multicolumn{2}{c|}{} & 84.41 ± 0.05 & 86.51± 0.09 & 84.20 ± 0.39 & 87.00 ± 0.03 & 2.59  & 79.63 & 84.8  & 5.17 \\\\\n% \\cline{4-11}    \\multirow{4}[2]{*}{Papers} & \\multicolumn{2}{c|}{\\multirow{2}[1]{*}{GAMLP}} & 71.17 ± 0.14 & 72.7 ± 0.02 & 69.75 & 71.71 & 0.54  & 68.05 & 69.94 & 1.89 \\\\\n%           & \\multicolumn{2}{c|}{} & 67.71 ± 0.20 & 69.33 ± 0.06 & 65.99 & 68.25 & 0.54  & 63.52 & 65.3  & 1.78 \\\\\n%           & \\multicolumn{2}{c|}{\\multirow{2}[1]{*}{GAMLP+}} & 71.59 ± 0.05 & 73.05 ± 0.04 & 69.87 & 72.04 & 0.45  & 68.05 & 70    & 1.95 \\\\\n%           & \\multicolumn{2}{c|}{} & 68.25 ± 0.11 & 69.67 ± 0.05 & 66.36 & 68.66 & 0.41  & 63.52 & 65.42 & 1.9 \\\\\n%     \\hline\n% \\end{tabular}}%\n% \\label{tab: main results}\n% \\end{table}%\n\n%\n\n% % Table generated by Excel2LaTeX from sheet 'Sheet4'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Node classification accuracy for the obgn-Arxiv and ogbn-Products datasets. (mean±std\\%, the best results are bolded and the runner-ups are underlined). G ↑ denotes the improvements of \\ours-GNN over the same GNN trained with OGB feature; L ↑ denotes the improvements of \\ours-LM over directly fine-tune LM. ``+'' denotes that tricks are integrated by the GNN model.}\n%     \\label{tab: main_table}\n\n%   \\resizebox{\\textwidth}{37mm}{\n%       \\begin{tabular}{c|cl|cccccccc}\n%     \\hline\n%     \\multicolumn{1}{l|}{Datasets} & \\multicolumn{2}{c|}{ Methods} & \\multicolumn{5}{c|}{GNN}              & \\multicolumn{3}{c}{LM} \\bigstrut\\\\\n%     \\hline\n%     \\multicolumn{2}{c}{} & \\multicolumn{1}{c}{} & X-OGB & X-GIANT & X-FLM & GLEM-G & G ↑   & LM-Ft & GLEM-L & L↑ \\bigstrut\\\\\n%     \\hline\n%     \\multirow{8}[2]{*}{Arxiv} & \\multirow{2}[1]{*}{GCN} & \\textit{val} & 73.00 ± 0.17 & 74.89 ± 0.17 & 47.56 ± 1.91 & 76.86 ± 0.19 & 3.86  & 75.27 ± 0.09  & 76.17 ± 0.47 & 0.9 \\\\\n%           &       & \\textit{test} & 71.74 ± 0.29 & 73.29 ± 0.1 & 48.19 ± 1.47 & \\underline{75.93 ± 0.19 }& 4.19  & 74.13 ± 0.04 & 75.71 ± 0.24 & 1.58 \\\\\n%           & \\multirow{2}[0]{*}{SAGE} & \\textit{val} & 72.77 ± 0.16 & 75.95 ± 0.11 & 56.16 ± 0.46 & 76.45 ± 0.05 & 3.68  & 75.27 ± 0.09  & 75.32 ± 0.04 & 0.6 \\\\\n%           &       & \\textit{test} & 71.49 ± 0.27 & 74.35 ± 0.14 & 56.39 ± 0.82 & 75.5 ± 0.24 & 4.01  & 74.13 ± 0.04 & 74.53 ± 0.12 & 1.44 \\\\\n%           & \\multirow{2}[0]{*}{GAMLP} & \\textit{val} &  62.20 ± 0.11 & 75.01 ± 0.02 & 71.14 ± 0.19 & 76.95 ± 0.14 & 14.75 & 75.27 ± 0.09  & 75.64 ± 0.30 & 0.44 \\\\\n%           &       & \\textit{test} &  56.53 ± 0.02 &  73.35 ± 0.14 & 70.15 ± 0.22 & 75.62 ± 0.23 & 19.09 & 74.13 ± 0.04 & 74.48 ± 0.41 & 2.04 \\\\\n%           & \\multirow{2}[1]{*}{RevGAT} & \\textit{val} & 75.01 ± 0.10 & 77.01 ± 0.09 & 71.40 ± 0.23 & 77.07 ± 0.04 & 2.06  & 75.27 ± 0.09  & 75.75 ± 0.07 & 0.48 \\\\\n%           &       & \\textit{test} & 74.02 ± 0.18 & 75.90 ± 0.19\t & 70.21 ± 0.3 & \\textbf{76.74 ± 0.08} & 2.72  & 74.13 ± 0.04 & 75.45 ± 0.12 & 1.32\\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Products} & \\multirow{2}[1]{*}{SAGE} & \\textit{val} & 91.99 ± 0.07 & 93.47 ± 0.14 & 86.74 ± 0.31 & 93.84 ± 0.12 & 1.85  & 91.82 ± 0.11 & 92.71 ± 0.15 & 0.71\\\\\n%           &       & \\textit{test} & 79.21 ± 0.15 & 82.33 ± 0.37 & 71.09 ± 0.65 & 83.16 ± 0.19 & 3.95  & 79.63 ± 0.12 & 81.25 ± 0.15 & 1.61 \\\\\n%           & \\multirow{2}[0]{*}{GAMLP} & \\textit{val} & 93.12 ± 0.03 & 93.99 ± 0.04 & 91.65 ± 0.17 & 94.19 ± 0.01 & 1.07  & 91.82 ± 0.11 & 90.56 ± 0.04 & -1.26 \\\\\n%           &       & \\textit{test} & 83.54 ± 0.09 & 83.16 ± 0.07 & 80.49 ± 0.19 & 85.09 ± 0.21 & 1.55  & 79.63 ± 0.12 & 82.23 ± 0.27 & 2.6 \\\\\n%           & \\multirow{2}[1]{*}{SAGN+} & \\textit{val} & 93.25 ± 0.04 & 93.89 ± 0.02 & 92.78 ± 0.04 & 94.03 ± 0.04 & 0.78  & 91.82 ± 0.11 & 92.01 ± 0.05 & 0.21 \\\\\n%           &       & \\textit{test} & 84.41 ± 0.05 & \\underline{86.51± 0.09} & 84.20 ± 0.39 & \\textbf{87.00 ± 0.03}& 2.59  & 79.63 ± 0.12 & 84.83 ± 0.04 & 5.17 \\\\\n%     \\hline\n%     \\multirow{4}[2]{*}{Papers} & \\multirow{2}[1]{*}{GAMLP} & \\textit{val} & 71.17 ± 0.14 & 72.7 ± 0.07 & 69.78 ± 0.07 & 71.71 ± 0.09 & 0.54  & 68.05 ± 0.03 & 69.94 ± 0.16 & 1.89 \\\\\n%           &       & \\textit{test} & 67.71 ± 0.20 & 69.33 ± 0.06 & 65.94 ± 0.10 & 68.25 ± 0.14 & 0.54  & 63.52 ± 0.06 & 64.80 ± 0.06 & 1.78 \\\\\n%           & \\multirow{2}[1]{*}{GAMLP+} & \\textit{val} & 71.59 ± 0.05 & 73.05 ± 0.04 & 69.87 ± 0.06 & 72.11 ± 0.06 & 0.45  & 68.05 ± 0.03 & 70.00 ± 0.08 & 1.95 \\\\\n%           &       & \\textit{test} & 68.25 ± 0.11 & \\textbf{69.67 ± 0.05} & 66.36 ± 0.09 & \\underline{68.91 ± 0.12} & 0.41  & 63.52 ± 0.06 & 65.42 ± 0.04 & 1.9 \\\\\n%     \\hline\n%     \\end{tabular}}%\n% \\end{table}%\n\n% Table generated by Excel2LaTeX from sheet 'Sheet4'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\setlength{\\abovecaptionskip}{0pt}%\n%   \\setlength{\\belowcaptionskip}{10pt}%\n%   \\caption{Node classification accuracy for the obgn-Arxiv and ogbn-Products datasets. (mean±std\\%, the best results are bolded and the runner-ups are underlined). G ↑ denotes the improvements of \\ours-GNN over the same GNN trained with OGB feature; L ↑ denotes the improvements of \\ours-LM over directly fine-tune LM. ``+'' denotes that tricks are integrated by the GNN model.}\n%   \\label{tab: main_table}\n%   \\resizebox{\\textwidth}{37mm}{\n%     \\begin{tabular}{c|cl|cccccccc}\n%     \\hline\n%     \\multicolumn{1}{l|}{Datasets} & \\multicolumn{2}{c|}{ Methods} & \\multicolumn{5}{c|}{GNN}              & \\multicolumn{3}{c}{LM} \\bigstrut\\\\\n%     \\hline\n%     \\multicolumn{2}{c}{} & \\multicolumn{1}{c}{} & X-OGB & X-GIANT & X-FLM & GLEM-G & G ↑   & LM-Ft & GLEM-L & L↑ \\bigstrut\\\\\n%     \\hline\n%     \\multirow{8}[2]{*}{Arxiv} & \\multirow{2}[1]{*}{GCN} & \\textit{val} & 73.00 ± 0.17 & 74.89 ± 0.17 & 47.56 ± 1.91 & 76.86 ± 0.19 & 3.86  & 75.27 ± 0.09  & 76.17 ± 0.47 & 0.9 \\\\\n%           &       & \\textit{test} & 71.74 ± 0.29 & 73.29 ± 0.1 & 48.19 ± 1.47 & \\underline{75.93 ± 0.19 }& 4.19  & 74.13 ± 0.04 & 75.71 ± 0.24 & 1.58 \\\\\n%           & \\multirow{2}[0]{*}{SAGE} & \\textit{val} & 72.77 ± 0.16 & 75.95 ± 0.11 & 56.16 ± 0.46 & 76.45 ± 0.05 & 3.68  & 75.27 ± 0.09  & 75.32 ± 0.04 & 0.6 \\\\\n%           &       & \\textit{test} & 71.49 ± 0.27 & 74.35 ± 0.14 & 56.39 ± 0.82 & 75.5 ± 0.24 & 4.01  & 74.13 ± 0.04 & 74.53 ± 0.12 & 1.44 \\\\\n%           & \\multirow{2}[0]{*}{GAMLP} & \\textit{val} &  62.20 ± 0.11 & 75.01 ± 0.02 & 71.14 ± 0.19 & 76.95 ± 0.14 & 14.75 & 75.27 ± 0.09  & 75.64 ± 0.30 & 0.44 \\\\\n%           &       & \\textit{test} &  56.53 ± 0.02 &  73.35 ± 0.14 & 70.15 ± 0.22 & 75.62 ± 0.23 & 19.09 & 74.13 ± 0.04 & 74.48 ± 0.41 & 2.04 \\\\\n%           & \\multirow{2}[1]{*}{RevGAT} & \\textit{val} & 75.01 ± 0.10 & 77.01 ± 0.09 & 71.40 ± 0.23 & 77.07 ± 0.04 & 2.06  & 75.27 ± 0.09  & 75.75 ± 0.07 & 0.48 \\\\\n%           &       & \\textit{test} & 74.02 ± 0.18 & 75.90 ± 0.19\t & 70.21 ± 0.3 & \\textbf{76.74 ± 0.08} & 2.72  & 74.13 ± 0.04 & 75.45 ± 0.12 & 1.32\\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Products} & \\multirow{2}[1]{*}{SAGE} & \\textit{val} & 91.99 ± 0.07 & 93.47 ± 0.14 & 86.74 ± 0.31 & 93.84 ± 0.12 & 1.85  & 91.82 ± 0.11 & 92.71 ± 0.15 & 0.71\\\\\n%           &       & \\textit{test} & 79.21 ± 0.15 & 82.33 ± 0.37 & 71.09 ± 0.65 & 83.16 ± 0.19 & 3.95  & 79.63 ± 0.12 & 81.25 ± 0.15 & 1.61 \\\\\n%           & \\multirow{2}[0]{*}{GAMLP} & \\textit{val} & 93.12 ± 0.03 & 93.99 ± 0.04 & 91.65 ± 0.17 & 94.19 ± 0.01 & 1.07  & 91.82 ± 0.11 & 90.56 ± 0.04 & -1.26 \\\\\n%           &       & \\textit{test} & 83.54 ± 0.09 & 83.16 ± 0.07 & 80.49 ± 0.19 & 85.09 ± 0.21 & 1.55  & 79.63 ± 0.12 & 82.23 ± 0.27 & 2.6 \\\\\n%           & \\multirow{2}[1]{*}{SAGN+} & \\textit{val} & 93.25 ± 0.04 & 93.89 ± 0.02 & 92.78 ± 0.04 & 94.03 ± 0.04 & 0.78  & 91.82 ± 0.11 & 92.01 ± 0.05 & 0.21 \\\\\n%           &       & \\textit{test} & 84.41 ± 0.05 & \\underline{86.51± 0.09} & 84.20 ± 0.39 & \\textbf{87.00 ± 0.03}& 2.59  & 79.63 ± 0.12 & 84.83 ± 0.04 & 5.17 \\\\\n%     \\hline\n%     \\multirow{4}[2]{*}{Papers} & \\multirow{2}[1]{*}{GAMLP} & \\textit{val} & 71.17 ± 0.14 & 72.7 ± 0.07 & 69.78 ± 0.07 & 71.71 ± 0.09 & 0.54  & 68.05 ± 0.03 & 69.94 ± 0.16 & 1.89 \\\\\n%           &       & \\textit{test} & 67.71 ± 0.20 & 69.33 ± 0.06 & 65.94 ± 0.10 & 68.25 ± 0.14 & 0.54  & 63.52 ± 0.06 & 64.80 ± 0.06 & 1.78 \\\\\n%           & \\multirow{2}[1]{*}{GAMLP+} & \\textit{val} & 71.59 ± 0.05 & 73.05 ± 0.04 & 69.87 ± 0.06 & 72.11 ± 0.06 & 0.45  & 68.05 ± 0.03 & 70.00 ± 0.08 & 1.95 \\\\\n%           &       & \\textit{test} & 68.25 ± 0.11 & \\textbf{69.67 ± 0.05} & 66.36 ± 0.09 & \\underline{68.91 ± 0.12} & 0.41  & 63.52 ± 0.06 & 65.42 ± 0.04 & 1.9 \\\\\n%     \\hline\n%     \\end{tabular}}%\n%   \\label{tab:addlabel}%\n% \\end{table}%\n\\noindent\\textbf{Main Results.}\nNext, we evaluate \\ours in the transductive setting. The results of three OGB datasets are presented in Table \\ref{tab: main_table}. For LMs, we see that fine-tuned LMs (LM-Ft) have competitive results, showing the importance of text attributes in a TAG. By further leveraging the structural information for message passing, our approach (\\ours-LM) achieves significant improvement over LMs, which demonstrates its advantage over LMs.\n\nFor GNN-based methods, we see that for each GNN model, using OGB and GIANT node embeddings as GNN inputs ($\\mathbf{X}_{\\text {OGB}}$ and $\\mathbf{X}_{\\text {GIANT}}$) yields strong results. However, these embeddings remain unchanged during training. By dynamically updating the LM to generate more useful node embeddings and pseudo-labels for the GNN, \\ours-GNN significantly outperforms all the other methods with fixed node embeddings in most cases. Notably, \\ours-GNN achieves new state-of-the-art results on all three TAG datasets on the OGB benchmark.\n\n\n%we observe that the static  $\\mathbf{X}_{\\text {PLM}}$ models generally achieve the worst results compared with $\\mathbf{X}_{\\text {OGB}}$ and $\\mathbf{X}_{\\text {GIANT}}$ due to the unawareness of graph structure and label information. $\\mathbf{X}_{\\text {GIANT}}$ outperforms $\\mathbf{X}_{\\text {OGB}}$ and $\\mathbf{X}_{\\text {PLM}}$ in most cases by leveraging the graph information. Meanwhile, with the help of both graph and label information, \\ours-LM in turn boosts \\ours-GNN by contributing updated embedding and pseudo-labels, achieving an average improvement of 19.00\\%, ?\\%, and ?\\% on Arxiv, Products, and Papers100M respectively. Notably, \\ours-GNN surpasses existing top-ranked GNN methods, achieving new state-of-the-art results on ogbn-arxiv and ogbn-products. The significant improvement of both \\ours-LM and \\ours-GNN clearly shows the benefits of fusing both graph learning and text learning together.\n\n\n%In this section, we compare our proposed \\ours framework on the OGB benchmarks. The LM model used is the DeBERTa-base~\\citep{DeBERTa}. The node classification performance is shown in Table \\ref{tab: main_table}. %The proposed \\ours framework consistently boosts both GNN and LM by a significant margin. \n% GNN both emb and kd\n%For language learning, we can observe that LM-Ft itself could be a competitive baseline against GNNs, indicating the importance of mining fine-grained text information on TAGs. Moreover, with the ability to leverage structural correlations between text semantics by learning from pseudo-label of GNNs, \\ours-LM further boost the LM performance with an average improvement of ?\\%, ?\\%, and ?\\% on three datasets respectively. \n\n%For graph learning, we observe that the static $\\mathbf{X}_{\\text {PLM}}$ models generally achieve the worst results compared with $\\mathbf{X}_{\\text {OGB}}$ and $\\mathbf{X}_{\\text {GIANT}}$ due to the unawareness of graph structure and label information. $\\mathbf{X}_{\\text {GIANT}}$ outperforms $\\mathbf{X}_{\\text {OGB}}$ and $\\mathbf{X}_{\\text {PLM}}$ in most cases by leveraging the graph information. Meanwhile, with the help of both graph and label information, \\ours-LM in turn boosts \\ours-GNN by contributing updated embedding and pseudo-labels, achieving an average improvement of 19.00\\%, ?\\%, and ?\\% on Arxiv, Products, and Papers100M respectively. Notably, \\ours-GNN surpasses existing top-ranked GNN methods, achieving new state-of-the-art results on ogbn-arxiv and ogbn-products. The significant improvement of both \\ours-LM and \\ours-GNN clearly shows the benefits of fusing both graph learning and text learning together.\n\n\n% \\subsubsection{Scalability}\n\\noindent\\textbf{Scalability.}\nOne key challenge of fusing LMs and GNNs lies in scalability. When using large LMs with numerous parameters, the combined method will suffer from severe scalability problems. GLEM eases this challenge through the EM-based optimization paradigm, allowing it to be adapted to large LMs. To verify this claim, we train \\ours with DeBERTa-large~\\citep{DeBERTa} on ogbn-arxiv. The results are reported in Table~\\ref{tab:largelm}. We observe that GLEM is able to generalize to DeBERTa-large with about 0.4B parameters, showing the appealing scalability. Besides, for every LM, applying \\ours yields consistent improvement, which proves the effectiveness of \\ours.\n\\label{sec: exp_largeLMs}\n% % Table generated by Excel2LaTeX from sheet 'Sheet6'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Add caption}\n%     \\begin{tabular}{lrrl}\n%     \\hline\n%     \\multicolumn{4}{c}{Arxiv} \\bigstrut\\\\\n%     \\hline\n%           & \\multicolumn{1}{l}{val-acc} & \\multicolumn{1}{l}{test-acc} & \\# Parameter\\\\\n%           &       &       &  \\\\\n%     GNN-OGB &       & \\multicolumn{1}{l}{74.02 ± 0.18} &  \\\\\n%     GNN-GIANT &       & \\multicolumn{1}{l}{75.90 ± 0.19} &  \\\\\n%     GNN-ours-base &       & 76.7  &  \\\\\n%     GNN-ours-large &       & 77.6  &  \\\\\n%     LM-base-finetune &       & 74.13 & 100M \\\\\n%     LM-base-ours &       &       & 100M \\\\\n%     LM-large-finetune &       & 73.8  & 350M \\\\\n%     LM-large-ours &       & 76.8  & 350M \\\\\n%     \\hline\n%     \\end{tabular}%\n%   \\label{tab:addlabel}%\n% \\end{table}%\n\n\n% \\begin{table}[htbp]\n%     \\caption{Experiments with DeBERTa-large as LM backbone, RevGAT as GNN backbone.}\n% \\label{tab: largelm}\n%   \\centering\n%     \\begin{tabular}{lccc}\n%     \\hline\n%     \\multicolumn{4}{c}{ arxiv} \\bigstrut\\\\\n%     \\hline\n%     Methods & \\multicolumn{1}{l}{Val accuracy} & \\multicolumn{1}{l}{Test accuracy} & \\multicolumn{1}{l}{\\# Parameters}\\\\\n%     GNN-OGB & 75.01 ± 0.10 & 74.02 ± 0.18 &{2,098,256} \\\\\n%     GNN-GIANT & 77.01 ± 0.09 & 75.90 ± 0.19 & {1,304,912} \\\\\n%     GNN-ours-base & 77.07 ± 0.04 & 76.74 ± 0.08 & 1835600 \\\\\n%     GNN-ours-large & 77.92 ± 0.06 & 77.62 ± 0.16 & 2228816 \\\\\n%     LM-base-finetune & 75.27 ± 0.09  & 74.13 ± 0.04 & 138,632,488 \\\\\n%     LM-base-ours & 75.75 ± 0.07 & 75.45 ± 0.12 & 138,632,488 \\\\\n%     LM-large-finetune & 75.08 ± 0.06 & 73.81 ± 0.08 & 405,204,008 \\\\\n% \\cline{4-4}    LM-large-ours & 77.16 ± 0.04 & 76.80 ± 0.05 & 405,204,008 \\\n    \n%     \\hline\n%     \\end{tabular}%\n%   \\label{tab:largeLM}%\n% \\end{table}%\n% Table generated by Excel2LaTeX from sheet 'LARGE'\n%\n\n\n% Scale: No DeBERTa-small! DeBERTa-base, -large, Bert-base, Bert-tiny, Bert-Large\n\\vspace{-5pt}\n"
                },
                "subsection 5.3": {
                    "name": "Structure-free Inductive Node Classification",
                    "content": "\n\\label{sec: exp_struct_free}\n\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Structure Free Inductive Setting \\jianan{Todo: we need to add (1) the difference column comparing the difference of validation and test datasets (2) The Improvements column, showing the relative improvements of our feature.}}\n%     \\resizebox{\\textwidth}{22mm}{\n%     \\begin{tabular}{lrrrr}\n%     \\hline\n%     \\multicolumn{1}{r|}{} & \\multicolumn{2}{c}{Arxiv} & \\multicolumn{2}{c}{Products} \\bigstrut\\\\\n% \\cline{2-5}    \\multicolumn{1}{r|}{} & \\multicolumn{1}{l}{val-acc} & \\multicolumn{1}{l}{test-acc} & \\multicolumn{1}{l}{val-acc} & \\multicolumn{1}{l}{test-acc} \\bigstrut\\\\\n%     \\hline\n%     MLP-OGB & \\multicolumn{1}{l}{57.65 ± 0.12} & \\multicolumn{1}{l}{55.50 ± 0.23} & \\multicolumn{1}{l}{75.54 ± 0.14} & \\multicolumn{1}{l}{61.06 ± 0.08}\\\\\n%     MLP-oursLight & \\multicolumn{1}{l}{57.65 ± 0.12} & \\multicolumn{1}{l}{55.50 ± 0.23} & \\multicolumn{1}{l}{75.54 ± 0.14} & \\multicolumn{1}{l}{61.06 ± 0.08} \\\\\n%     MLP-oursDeep & \\multicolumn{1}{l}{57.65 ± 0.12} & \\multicolumn{1}{l}{55.50 ± 0.23} & \\multicolumn{1}{l}{75.54 ± 0.14} & \\multicolumn{1}{l}{61.06 ± 0.08} \\\\\n%     GNN-OGB & \\multicolumn{1}{l}{63.06} & \\multicolumn{1}{l}{44.46} &       &  \\\\\n%     GNN-Light & \\multicolumn{1}{l}{76.73 ± 0.02} & \\multicolumn{1}{l}{73.94 ± 0.03} &       &  \\\\\n%     GNN-Deep & \\multicolumn{1}{l}{76.79 ± 0.06} & \\multicolumn{1}{l}{74.29 ± 0.11} & \\multicolumn{1}{l}{93.22 ± 0.03} & \\multicolumn{1}{l}{ 79.81 ± 0.01} \\\\\n%     LM-base-ft & 75.27 ± 0.09 & 74.13 ± 0.04 & 91.82 ± 0.11 & 79.63 \\\\\n%     LM-base-oursLight & 75.48 & 74.57 & 91.82 & 79.75 \\\\\n%     LM-base-oursDeep & 75.68 & 74.69 & 91.96 & 79.25 \\bigstrut[b]\\\\\n%     \\hline\n%     \\end{tabular}}\n%   \\label{tab: inductive}\n% \\end{table}%\n\n\n% % Table generated by Excel2LaTeX from sheet 'Sheet6'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Structure Free Inductive Setting \\jianan{Todo: we need to add (1) the difference column comparing the difference of validation and test datasets (2) The Improvements column, showing the relative improvements of our feature.}}\n%     \\setlength{\\tabcolsep}{7mm}{\n%     \\begin{tabular}{lrrr}\n%     \\hline\n%     \\multicolumn{1}{c}{Method} & \\multicolumn{3}{c}{Arxiv} \\bigstrut\\\\\n%     \\hline\n%           & \\multicolumn{1}{l}{Val-Acc} & \\multicolumn{1}{l}{Test-Acc} & \\multicolumn{1}{l}{Test-Val} \\bigstrut\\\\\n%     \\hline\n%     MLP-OGB & \\multicolumn{1}{l}{57.65 ± 0.12} & \\multicolumn{1}{l}{55.50 ± 0.23} & -2.15 \\\\\n%     MLP-X-LM-ft & \\multicolumn{1}{l}{74.56 ± 0.01} & \\multicolumn{1}{l}{72.98 ± 0.06} & -1.58 \\\\\n%     MLP-X-GLEM-light & \\multicolumn{1}{l}{75.20 ± 0.03} & \\multicolumn{1}{l}{73.32 ± 0.31} & -1.88 \\\\\n%     MLP-X-GLEM-deep & \\multicolumn{1}{l}{75.57 ± 0.03} & \\multicolumn{1}{l}{73.90 ± 0.08} & -1.67 \\\\\n%     GNN-OGB-light & \\multicolumn{1}{l}{70.73 ± 0.02} & \\multicolumn{1}{l}{48.59 ± 0.19} & -22.14 \\\\\n%     GNN-OGB-Deep & \\multicolumn{1}{l}{72.67 ± 0.03} & \\multicolumn{1}{l}{50.92 ± 0.19} & -21.75 \\\\\n%     GLEM-GNN-light & \\multicolumn{1}{l}{76.73 ± 0.02} & \\multicolumn{1}{l}{73.94 ± 0.03} & -2.79 \\\\\n%     GLEM-GNN-deep & \\multicolumn{1}{l}{76.79 ± 0.06} & \\multicolumn{1}{l}{74.29 ± 0.11} & -2.5 \\\\\n%     LM-base-ft & 75.27 & 74.13 & -1.14 \\\\\n%     LM-base-oursLight & 75.48 & 74.57 & -0.91 \\\\\n%     LM-base-oursDeep & 75.68 & 74.69 & -0.99 \\\\\n%     \\hline\n%     \\end{tabular}}%\n%   \\label{tab:addlabel}%\n% \\end{table}%\n\n% Table generated by Excel2LaTeX from sheet 'Sheet6'\n% \\begin{table}[htbp]\n% \\caption{Structure free inductive experiments on ogbn-arxiv and ogbn-products. The validation and test accuracies, denoted as w/ struct and wo struct, and their relative differences are reported.}\n% \\label{tab: inductive}\n%   \\centering\n%     \\resizebox{\\textwidth}{22mm}{\n%     \\begin{tabular}{lcccccc}\n%     \\hline\n%     \\multicolumn{1}{c}{Method} & \\multicolumn{3}{c}{Arxiv} & \\multicolumn{3}{c}{Products} \\bigstrut\\\\\n%     \\hline\n%           & \\multicolumn{1}{l}{w/ struct} & \\multicolumn{1}{l}{wo struct} & \\multicolumn{1}{l}{diff} & \\multicolumn{1}{l}{w/ struct} & \\multicolumn{1}{l}{wo struct} & \\multicolumn{1}{l}{diff} \\bigstrut\\\\\n%     \\hline\n%     MLP-X-OGB & 57.65 ± 0.12 & 55.50 ± 0.23 & -2.15 & 75.54 ± 0.14 & 61.06 ± 0.08 & -14.48 \\\\\n%     MLP-X-LM-ft & 74.56 ± 0.01 & 72.98 ± 0.06 & -1.58 & 91.79± 0.01 & 79.93 ± 0.22 & -11.86 \\\\\n%     MLP-X-GLEM-light & 75.20 ± 0.03 & 73.32 ± 0.31 & -1.88 & 91.96 ± 0.01 & 79.38 ± 0.14 & -12.58 \\\\\n%     MLP-X-GLEM-deep & 75.57 ± 0.03 & 73.90 ± 0.08 & -1.67 &  91.85 ± 0.02     & 80.04 ± 0.15      & -11.81 \\\\\n%     GNN-light-X-OGB & 70.73 ± 0.02 & 48.59 ± 0.19 & -22.14 & 90.54 ± 0.04 & 51.23 ± 0.17 & -39.31 \\\\\n%     GNN-deep-X-OGB & 72.67 ± 0.03 & 50.92 ± 0.19 & -21.75 & 91.85 ± 0.11 & 32.71 ± 2.23 & -59.14 \\\\\n%     GLEM-GNN-light & 76.73 ± 0.02 & 73.94 ± 0.03 & -2.79 & 92.95 ± 0.03 & 78.75 ± 0.39 & -14.2 \\\\\n%     GLEM-GNN-deep & 76.79 ± 0.06 & 74.29 ± 0.11 & -2.5  & 93.22 ± 0.03 &  79.81 ± 0.01 & -13.41 \\\\\n%     LM-Ft & 75.27 ± 0.09  & 74.13 ± 0.04  & -1.14 & 91.82 ± 0.11 & 79.63 ± 0.12 & -12.19 \\\\\n%     \\ours-LM-light & 75.48 & 74.57 & -0.91 & 91.82 & 79.75 & -12.07 \\\\\n%     \\ours-LM-deep & 75.68 & 74.69 & -0.99 & 91.96 & 79.25 & -12.71\\\\\n%     \\hline\n%     \\end{tabular}}%\n% \\end{table}%\n\t\n% % Table generated by Excel2LaTeX from sheet 'sTRUCTURE'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\setlength{\\belowcaptionskip}{5pt}\n%   \\caption{Structure free inductive experiments on ogbn-arxiv and ogbn-products. The validation and test accuracies, denoted as w/ struct and wo struct, and their relative differences (the lower the better) are reported. Boldfaced numbers indicate the best performances in each group.}\n%   \\scalebox{0.82}{\n%     \\begin{tabular}{c|lcccccc}\n%     \\hline\n%     \\multicolumn{2}{c}{\\multirow{2}[2]{*}{Method}} & \\multicolumn{3}{c}{Arxiv} & \\multicolumn{3}{c}{Products} \\\\\n%     \\multicolumn{2}{c}{} & w/ struct & wo struct & diff  & w/ struct & wo struct & diff \\\\\n%     \\hline\n%     \\multirow{4}[2]{*}{\\textbf{MLP}} & MLP-OGB & 57.65 ± 0.12 & 55.50 ± 0.23 & -2.15 & 75.54 ± 0.14 & 61.06 ± 0.08 & -14.48 \\\\\n%           & MLP-X-LM-ft & 74.56 ± 0.01 & 72.98 ± 0.06 & -1.58 & 91.79± 0.01 & 79.93 ± 0.22 & -11.86 \\\\\n%           & MLP-X-GLEM-light & 75.20 ± 0.03 & 73.32 ± 0.31 & -1.88 & 91.96 ± 0.01 & 79.38 ± 0.14 & -12.58 \\\\\n%           & MLP-X-GLEM-deep & 75.57 ± 0.03 & 73.90 ± 0.08 & -1.67 & 91.85 ± 0.02 & 80.04 ± 0.15 & -11.81 \\\\\n%     \\hline\n%     \\multirow{2}[2]{*}{\\textbf{light-GNN}} & GNN-OGB-light & 70.73 ± 0.02 & 48.59 ± 0.19 & -22.14 & 90.54 ± 0.04 & 51.23 ± 0.17 & -39.31 \\\\\n%           & GLEM-GNN-light & 76.73 ± 0.02 & 73.94 ± 0.03 & -2.79 & 92.95 ± 0.03 & 78.75 ± 0.39 & -14.2 \\\\\n%     \\hline\n%     \\multirow{2}[2]{*}{\\textbf{deep-GNN}} & GNN-OGB-Deep & 72.67 ± 0.03 & 50.92 ± 0.19 & -21.75 & 91.85 ± 0.11 & 32.71 ± 2.23 & -59.14 \\\\\n%           & GLEM-GNN-deep & 76.79 ± 0.06 & 74.29 ± 0.11 & -2.5  & 93.22 ± 0.03 &  79.81 ± 0.01 & -13.41\\\\\n%     \\hline\n%     \\multirow{3}[2]{*}{\\textbf{LM}} & LM-base-ft & 75.27 ± 0.09  & 74.13 ± 0.04 & -1.14 & 91.82 ± 0.11 & 79.63 ± 0.12 & -12.19 \\\\\n%           & LM-base-oursLight & 75.49 ± 0.11 & 74.50 ± 0.16 & -0.99 & 91.90 ± 0.06 & 79.53 ± 0.13 & -12.37 \\\\\n%           & LM-base-oursDeep & 75.59 ± 0.08 & 74.60 ± 0.05 & -0.99 & 91.81 ± 0.04 & 79.69 ± 0.51 & -12.12\\\\\n%     \\hline\n%     \\end{tabular}}%\n%   \\label{tab:Table3}%\n% \\end{table}%\n\n% % Table generated by Excel2LaTeX from sheet 'sTRUCTURE'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Structure free inductive experiments on ogbn-arxiv and ogbn-products. The validation and test accuracies, denoted as w/ struct and wo struct, and their relative differences (the lower the better) are reported. Boldfaced numbers indicate the best performances in each group.}\n%   \\scalebox{0.82}{\n%     \\begin{tabular}{c|l|ccc|ccc}\n%     \\hline\n%     \\multicolumn{2}{c|}{\\multirow{2}[2]{*}{Method}} & \\multicolumn{3}{c|}{\\textbf{Arxiv}} & \\multicolumn{3}{c}{\\textbf{Products}}\\\\\n%     \\multicolumn{2}{c|}{} & w/ struct & wo struct & diff  & w/ struct & wo struct & diff \\\\\n%     \\hline\n%     \\multirow{4}[2]{*}{\\textbf{MLP}} & $\\mathbf{X}_{\\text {OGB}}$  & 57.65 ± 0.12 & 55.50 ± 0.23 & -2.15 & 75.54 ± 0.14 & 61.06 ± 0.08 & -14.48\\\\\n%           & X-LM-ft  & 74.56 ± 0.01 & 72.98 ± 0.06 & -1.58 & 91.79± 0.01 & 79.93 ± 0.22 & -11.86 \\\\\n%           & X-GLEM-light & 75.20 ± 0.03 & 73.32 ± 0.31 & -1.88 & 91.96 ± 0.01 & 79.38 ± 0.14 & -12.58 \\\\\n%           & X-GLEM-deep & 75.57 ± 0.03 & \\textbf{73.90 ± 0.08} & \\textbf{-1.67} & 91.85 ± 0.02 & \\textbf{80.04 ± 0.15} & \\textbf{-11.81} \\\\\n%     \\hline\n%     \\multirow{4}[2]{*}{ \\textbf{GNN}} & OGB-light & 70.73 ± 0.02 & 48.59 ± 0.19 & -22.14 & 90.54 ± 0.04 & 51.23 ± 0.17 & -39.31 \\\\\n%           & GLEM-GNN-light & 76.73 ± 0.02 & 73.94 ± 0.03 & -2.79 & 92.95 ± 0.03 & 78.75 ± 0.39 & -14.2 \\\\\n%           & OGB-Deep & 72.67 ± 0.03 & 50.92 ± 0.19 & -21.75 & 91.85 ± 0.11 & 32.71 ± 2.23 & -59.14 \\\\\n%           & GLEM-GNN-deep & 76.79 ± 0.06 & \\textbf{74.29 ± 0.11} & \\textbf{-2.5} & 93.22 ± 0.03 & \\textbf{ 79.81 ± 0.01} & \\textbf{-13.41}\\\\\n%     \\hline\n%     \\multirow{3}[2]{*}{\\textbf{LM}} & Fine-tune    & 75.27 ± 0.09  & 74.13 ± 0.04 & -1.14 & 91.82 ± 0.11 & 79.63 ± 0.12 & -12.19 \\\\\n%           & \\ours-light  & 75.49 ± 0.11 & 74.50 ± 0.16 & -0.99 & 91.90 ± 0.06 & 79.53 ± 0.13 & -12.37 \\\\\n%           & \\ours-deep  & 75.59 ± 0.08 & \\textbf{74.60 ± 0.05} & \\textbf{-0.99} & 91.81 ± 0.04 & \\textbf{79.69 ± 0.51} & \\textbf{-12.12}\\\\\n%     \\hline\n%     \\end{tabular}}%\n%   \\label{tab:Table3}%\n% \\end{table}%\n\n% Table generated by Excel2LaTeX from sheet 'sTRUCTURE'\n%\n\n% % Table generated by Excel2LaTeX from sheet 'sTRUCTURE'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Add caption}\n%     \\begin{tabular}{clcccccc}\n%     \\hline\n%     \\multicolumn{1}{l}{\\textbf{Type}} & \\textbf{Methods} & \\multicolumn{3}{c}{\\textbf{Arxiv}} & \\multicolumn{3}{c}{\\textbf{Products}} \\bigstrut[t]\\\\\n%           &       & w/ struct & wo struct & diff  & w/ struct & wo struct & diff \\bigstrut[b]\\\\\n%     \\hline\n%     \\multirow{4}[2]{*}{MLP} & X-OGB & 57.65 ± 0.12 & 55.50 ± 0.23 & -2.15 & 75.54 ± 0.14 & 61.06 ± 0.08 & -14.48 \\bigstrut[t]\\\\\n%           & X-LM-ft & 74.56 ± 0.01 & 72.98 ± 0.06 & -1.58 & 91.79± 0.01 & 79.93 ± 0.22 & -11.86 \\\\\n%           & X-GLEM-light & 75.20 ± 0.03 & 73.32 ± 0.31 & -1.88 & 91.96 ± 0.01 & 79.38 ± 0.14 & -12.58 \\\\\n%           & X-GLEM-deep & 75.57 ± 0.03 & \\textbf{73.90 ± 0.08} & \\textbf{-1.67} & 91.85 ± 0.02 & \\textbf{80.04 ± 0.15} & \\textbf{-11.81} \\bigstrut[b]\\\\\n%     \\hline\n%     \\multirow{4}[2]{*}{ GNN} & OGB-light & 70.73 ± 0.02 & 48.59 ± 0.19 & -22.14 & 90.54 ± 0.04 & 51.23 ± 0.17 & -39.31 \\bigstrut[t]\\\\\n%           & GLEM-GNN-light & 76.73 ± 0.02 & 73.94 ± 0.03 & -2.79 & 92.95 ± 0.03 & 78.75 ± 0.39 & -14.2 \\\\\n%           & OGB-Deep & 72.67 ± 0.03 & 50.92 ± 0.19 & -21.75 & 91.85 ± 0.11 & 32.71 ± 2.23 & -59.14 \\\\\n%           & GLEM-GNN-deep & 76.79 ± 0.06 & \\textbf{74.29 ± 0.11} & \\textbf{-2.5} & 93.22 ± 0.03 & \\textbf{ 79.81 ± 0.01} & \\textbf{-13.41} \\bigstrut[b]\\\\\n%     \\hline\n%     \\multirow{3}[2]{*}{LM} & ft    & 75.27 ± 0.09  & 74.13 ± 0.04 & -1.14 & 91.82 ± 0.11 & 79.63 ± 0.12 & -12.19 \\bigstrut[t]\\\\\n%           & oursLight & 75.49 ± 0.11 & 74.50 ± 0.16 & -0.99 & 91.90 ± 0.06 & 79.53 ± 0.13 & -12.37 \\\\\n%           & oursDeep & 75.59 ± 0.08 & \\textbf{74.60 ± 0.05} & \\textbf{-0.99} & 91.81 ± 0.04 & \\textbf{79.69 ± 0.51} & \\textbf{-12.12} \\bigstrut[b]\\\\\n%     \\hline\n%     \\end{tabular}%\n%   \\label{tab:addlabel}%\n% \\end{table}%\n%Although the transductive settings are mostly studied in the GNN community, it might not match the case in real-world scenarios where unseen nodes with text-attributes are sometimes linked with no or very few nodes in the existing graph~\\citep{logeswaran-etal-2019-zero,zhang2022graphless}. For example, in a product graph, when a new product is registered, their text description is available but their co-purchase history is missing. In light of this realistic scenario, we propose a structure-free inductive node classification task to evaluate the generalization ability of TAG models without structures. We conduct the structure-free inductive settings on ogbn-arxiv and ogbn-products datasets by eliminating the edges connecting to the test nodes. In this way when performing predictions on test set, GNNs aggregate information from each node itself by the self-loop edge.\n\nBesides the transductive setting, inductive settings are also important, where we aim to train models on nodes of a training graph, and then generalize models to unobserved nodes. In many real cases, these new nodes are often low-degree nodes or even isolated nodes, meaning that we can hardly use the structural information for node classification. Therefore, we consider a challenging setting named structure-free inductive setting, where we assume for each test node, we only observe its text attributes without any connected neighbors. For this setting, we consider different types of methods for label prediction, including GNN models (GNN), neural networks without using structural information (MLP), and \\ours. The results are shown in Table \\ref{tab:Structure_free}.\n\nWe can see that the structure-free inductive setting is a more challenging task, especially for GNNs where a sheer performance drop is observed. Meanwhile, by effectively fusing with graph learning, \\ours-LM is able to consider local semantics as well as neighboring structural information, leading to more accurate structure-free inductive predictions. Besides, the generated embeddings are able to boost other models (e.g., see $\\mathbf{X}_{\\text {\\ours}}$-deep in the MLP and LM sections), enabling both MLP and GNNs with better structure-free inference ability.\n\n\n% With the ability of leveraging observed graph information, \\ours-LM achieves excellent performance on both two datasets. Moreover, as the local text information is well-preserved in the embedding of \\ours-LM, X-GLEM successfully boosts other framework such as MLP and \\ours-GNN to perform well on structure-free inference.\n% We evaluate the structure-free inductive performance on both light and deep GNNs. For ogbn-arxiv, we select GCN and RevGAT for light and deep GNNs. For ogbn-products, we use GraphSAGE and GAMLP as light and deep GNNs. The results are shown in Table \\ref{tab: inductive}, from which we have the following observations: (1) The differences of validation and test set of LM-Fts and MLPs are comparable, as both modules don't leverage graph information and therefore the structure-free setting doesn't harm. (2) The test set performances of GNNs with OGB feature are much lower than that on the validation set since GNNs downgrade to traditional MLPs when graph structure is not available. Interestingly, for both GNN-light and GNN-deep, the performance of GNN-OGB is even lower than MLP-OGB which indicates the GNN relies too much on aggregating neighborhood feature and fails in generalization to nodes without structure. (3) With the ability of leveraging observed graph information, \\ours-LM achieves excellent performance on both two datasets. Moreover, as the local text information is well-preserved in the embedding of \\ours-LM, X-GLEM successfully boosts other framework such as MLP and \\ours-GNN to perform well on structure-free inference.\n\n% difference, demonstrating the effectiveness of leveraging local semantics from \\ours-LM.\n% In sum, \\ours fuses both graph and language learning and performs effective structure-free inference. \n\\vspace{-5pt}\n"
                },
                "subsection 5.4": {
                    "name": "Comparison of Different Training Paradigms",
                    "content": "\n\n% % Table generated by Excel2LaTeX from sheet 'Sheet7'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Add caption}\n%     \\resizebox{\\textwidth}{21mm}{\n%     \\begin{tabular}{clllrlrrl}\n%     \\hline\n%     \\multicolumn{1}{l}{ } &       & \\multicolumn{2}{c}{Static} & \\multicolumn{2}{c}{Joint} & \\multicolumn{2}{c}{Alternate} &  \\bigstrut[t]\\\\\n%     \\multicolumn{1}{l}{ } &       & SAGE-OGB & SAGE-X-GIANT & \\multicolumn{1}{l}{SAGE+TinyBert (joint)} & GraphFormers & \\multicolumn{1}{l}{GLEM-G-Step} & \\multicolumn{1}{l}{GLEM-L-Step} & Deberta-base-Ft \\bigstrut[b]\\\\\n% \\cline{3-9}    \\multirow{6}[2]{*}{Arxiv} & val acc. & \\multicolumn{1}{c}{72.77 ± 0.16} & \\multicolumn{1}{c}{75.95 ± 0.11} &       &       & \\multicolumn{1}{c}{76.45 ± 0.05} & \\multicolumn{1}{c}{75.32 ± 0.04} & \\multicolumn{1}{c}{75.27} \\bigstrut[t]\\\\\n%           & test acc.. & \\multicolumn{1}{c}{71.49 ± 0.27} & \\multicolumn{1}{c}{74.35 ± 0.14} &       & \\multicolumn{1}{c}{72.81} & \\multicolumn{1}{c}{75.50 ± 0.24} & \\multicolumn{1}{c}{74.53 ± 0.12} & \\multicolumn{1}{c}{74.13} \\\\\n%           & parameters & \\multicolumn{1}{r}{218664} & \\multicolumn{1}{r}{546344} &       & \\multicolumn{1}{r}{110694592} & 548911 & 138632488 & \\multicolumn{1}{r}{138632488} \\\\\n%           & max bsz. & all nodes & all nodes &       & \\multicolumn{1}{r}{500} & \\multicolumn{1}{l}{all nodes} & 18    & \\multicolumn{1}{r}{18} \\\\\n%           & neighbors & All   & All   &       & \\multicolumn{1}{r}{10} & \\multicolumn{1}{l}{All} & 0     & \\multicolumn{1}{r}{0} \\\\\n%           & time/epoch & 0.09s & 0.13s &       & 1724.8s & \\multicolumn{1}{l}{0.13s} & \\multicolumn{1}{l}{7980S} & 5608S \\bigstrut[b]\\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Products} & val acc. & \\multicolumn{1}{c}{91.99 ± 0.07} & \\multicolumn{1}{c}{93.47 ± 0.14} &       &       & \\multicolumn{1}{c}{93.84 ± 0.12} & \\multicolumn{1}{c}{92.71 ± 0.15} & \\multicolumn{1}{c}{91.82 ± 0.11} \\bigstrut[t]\\\\\n%           & test acc.. & \\multicolumn{1}{c}{79.21 ± 0.15} & \\multicolumn{1}{c}{82.33 ± 0.37} &       & \\multicolumn{1}{r}{74.72} & \\multicolumn{1}{c}{83.16 ± 0.19} & \\multicolumn{1}{c}{81.25 ± 0.15} & \\multicolumn{1}{c}{79.63 ± 0.12} \\\\\n%           & parameters & \\multicolumn{1}{r}{206895} & \\multicolumn{1}{r}{548911} &       & \\multicolumn{1}{r}{110694592} & 548911 & 138637871 & \\multicolumn{1}{r}{138637871} \\\\\n%           & max bsz. & all nodes & \\multicolumn{1}{r}{2000} &       & \\multicolumn{1}{r}{200} &       &       & \\multicolumn{1}{r}{18} \\\\\n%           & neighbors & Full sample & Full sample &       & \\multicolumn{1}{r}{10} &       &       & \\multicolumn{1}{r}{0} \\\\\n%           & time/epoch & 8.1s  & 20.9s &       & 4631.3s &       &       & 6060s \\bigstrut[b]\\\\\n%     \\hline\n%     \\end{tabular}}%\n%   \\label{tab:addlabel}%\n% \\end{table}%\n\n\n\n% % Table generated by Excel2LaTeX from sheet 'Sheet7'\n% %We use a NVIDIA Tesla V100 32GB to conduct the experiments.\n% \\begin{table}[htbp]\n%   \\centering\n%   \\caption{Comparison of different ways of fusing LM and GNNs. The max bsz.. denotes the maximum batch size }\n%     \\resizebox{\\textwidth}{21mm}{\n%     \\begin{tabular}{c|cccccccc}\n%     \\hline\n%     \\multicolumn{1}{c}{ } &       & \\multicolumn{2}{c}{StaticLM} & \\multicolumn{2}{c}{Joint} & \\multicolumn{2}{c}{Alternate} & \\\\\n%     \\multicolumn{1}{c}{ } &       & SAGE-OGB & SAGE-X-GIANT & SAGE+TinyBert (joint) & GraphFormers & GLEM-G-Step & GLEM-L-Step & Deberta-base-Ft\\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Arxiv} & val acc. & 72.77 ± 0.16 & 75.95 ± 0.11 &       &       & 76.45 ± 0.05 & 75.32 ± 0.04 & 75.27 \\\\\n%           & test acc.. & 71.49 ± 0.27 & 74.35 ± 0.14 &       & 72.81 & 75.50 ± 0.24 & 74.53 ± 0.12 & 74.13 \\\\\n%           & parameters & 218664 & 546344 &       & 110694592 & 548911 & 138632488 & 138632488 \\\\\n%           & max bsz. & all nodes & all nodes &       & 500   & all nodes & 18    & 18 \\\\\n%           & neighbors & All   & All   &       & 10    & All   & 0     & 0 \\\\\n%           & time/epoch & 0.09s & 0.13s &       & 1724.8s & 0.13s & 4380s & 2760s \\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Products} & val acc. & 91.99 ± 0.07 & 93.47 ± 0.14 &       &       & 93.84 ± 0.12 & 92.71 ± 0.15 & 91.82 ± 0.11\\\\\n%           & test acc.. & 79.21 ± 0.15 & 82.33 ± 0.37 &       & 74.72 & 83.16 ± 0.19 & 81.25 ± 0.15 & 79.63 ± 0.12 \\\\\n%           & parameters & 206895 & 548911 &       & 110694592 & 548911 & 138637871 & 138637871 \\\\\n%           & max bsz. & all nodes & 2000  &       & 200   & 50000 & 18    & 18 \\\\\n%           & neighbors & Full sample & Full sample &       & 10    & Full sample & 0     & 0 \\\\\n%           & time/epoch & 8.1s  & 20.9s &       & 4631.3s & 153s  & 9360s & 5940s \\\\\n%     \\hline\n%     \\end{tabular}}%\n%   \\label{tab:addlabel}%\n% \\end{table}%\n\n% % Table generated by Excel2LaTeX from sheet 'Sheet7'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\setlength{\\belowcaptionskip}{5pt}\n%   \\caption{Comparison of different ways of fusing LM and GNNs. The max batch size (max bsz..) and time/epoch are tested on a single 32GB GPU.}\n%  \\scalebox{0.72}{\n%     \\begin{tabular}{c|ccccccc}\n%     \\hline\n%     \\multicolumn{2}{c}{\\multirow{2}[2]{*}{Datasets }} & \\multicolumn{2}{c}{Vanilla} & \\multicolumn{2}{c}{Joint} & \\multicolumn{2}{c}{Alternate} \\\\\n%     \\multicolumn{2}{c}{} & SAGE-OGB & Deberta-base-Ft &  joint-BERT-tiny & GraphFormers & GLEM-G-Step & GLEM-L-Step \\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Arxiv} & val acc. & 72.77 ± 0.16 & 75.27 ± 0.09  & \\multicolumn{1}{l}{71.58 ± 0.18} & 73.33 ± 0.06 & 76.45 ± 0.05 & 75.32 ± 0.04 \\\\\n%           & test acc.. & 71.49 ± 0.27 & 74.13 ± 0.04 & \\multicolumn{1}{l}{70.87 ± 0.12} & 72.81 ± 0.20 & 75.50 ± 0.24 & 74.53 ± 0.12 \\\\\n%           & parameters & 218,664 & 138,632,488 & 110,694,592 & 110,694,592 & 548,911 & 138,632,488 \\\\\n%           & max bsz. & all nodes & 30    & 200   & 180   & all nodes & 30 \\\\\n%           & neighbors & All   & 0     & 10    & 10 first-order    & All   & 0 \\\\\n%           & time/epoch & 0.09s & 2760s &  1827s & 4824s & 0.13s & 3801s\\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Products} & val acc. & 91.99 ± 0.07 & 91.82 ± 0.11 & 90.85 & 91.77 & 93.84 ± 0.12 & 92.71 ± 0.15\\\\\n%           & test acc.. & 79.21 ± 0.15 & 79.63 ± 0.12 & 73.13 & 74.72 & 83.16 ± 0.19 & 81.25 ± 0.15 \\\\\n%           & parameters & 206,895 & 138,637,871 & 110,699,975 & 110,699,975 & 548,911 & 138,637,871 \\\\\n%           & max bsz. & all nodes & 30    & 100   & 100   & 50000 & 30 \\\\\n%           & neighbors & Full sample & 0     &  10 first-order &  10,0,0 & Full sample & 0 \\\\\n%           & time/epoch & 8.1s  & 5460s & 8456s & 12574s & 153s  & 7740s \\\\\n%     \\hline\n%     \\end{tabular}}%\n%   \\label{tab: comparison}%\n% \\end{table}%\n\n% % Table generated by Excel2LaTeX from sheet 'Sheet7'\n% \\begin{table}[htbp]\n%   \\centering\n%   \\setlength{\\belowcaptionskip}{5pt}\n%   \\caption{Comparison of different ways of fusing LM and GNNs. The max batch size (max bsz.) and time/epoch are tested on a single 32GB GPU.}\n%   \\scalebox{0.75}{\n%     \\begin{tabular}{cccccccc}\n%     \\hline\n%     \\multirow{2}[2]{*}{Datasets } & \\multirow{2}[2]{*}{Metric} & \\multicolumn{2}{c}{Static} & \\multicolumn{2}{c}{Joint} & \\multicolumn{2}{c}{Alternate}\\\\\n%           &       & SAGE-OGB & Deberta-base-Ft &  joint-BERT-tiny & GraphFormers & GLEM-G-Step & GLEM-L-Step \\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Arxiv} & val acc. & 72.77 ± 0.16 & 75.27 ± 0.09  & 71.58 ± 0.18 & 73.33 ± 0.06 & 76.45 ± 0.05 & 75.32 ± 0.04\\\\\n%           & test acc. & 71.49 ± 0.27 & 74.13 ± 0.04 & 70.87 ± 0.12 & 72.81 ± 0.20 & 75.50 ± 0.24 & 74.53 ± 0.12 \\\\\n%           & parameters & 218,664 & 138,632,488 & 110,694,592 & 110,694,592 & 545,320 & 138,632,488 \\\\\n%           & max bsz & all nodes & 18    & 200   & 180   & all nodes & 18 \\\\\n%         %   & neighbors & All   & 0     & 10    & 10    & All   & 0 \\\\\n%           & time/epoch & 0.09s & 2760s &  1827s & 4824s & 0.13s & 3801s\\\\\n%     \\hline\n%     \\multirow{6}[2]{*}{Products} & val acc. & 91.99 ± 0.07 & 91.82 ± 0.11 & 90.85 ± 0.12 & 91.77 ± 0.09 & 93.84 ± 0.12 & 92.71 ± 0.15\\\\\n%           & test acc. & 79.21 ± 0.15 & 79.63 ± 0.12 & 73.13 ± 0.11 & 74.72 ± 0.16 & 83.16 ± 0.19 & 81.25 ± 0.15 \\\\\n%           & parameters & 206,895 & 138,637,871 & 110,699,975 & 110,699,975 & 548,911 & 138,637,871 \\\\\n%           & max bsz & all nodes & 30    & 100   & 100   & all nodes  & 30 \\\\\n%         %   & neighbors & 5,10,15 & 0     &  10,0,0 &  10,0,0 & 5,10,15 & 0 \\\\\n%           & time/epoch & 8.1s  & 5460s & 8456s & 12574s & 153s  & 7740s\\\\\n%     \\hline\n%     \\end{tabular}}%\n%   \\label{tab: comparison}%\n% \\end{table}%\n\n\n% Table generated by Excel2LaTeX from sheet 'Sheet7'\n%\n\n\n% % Table generated by Excel2LaTeX from sheet 'Sheet7'\n% \\begin{table}[bt]\n%   \\centering\n%   \\setlength{\\belowcaptionskip}{5pt}\n%   \\caption{Comparison of different training paradigms of fusing LM and GNNs. The max batch size (bsz.) and time/epoch are tested on a single 32GB GPU.}\n%   \\scalebox{0.76}{\n%     \\begin{tabular}{cccccccc}\n%     \\toprule\n%      &  & {\\textbf{LM}} & \\textbf{Static} & \\multicolumn{2}{c}{\\textbf{Joint}} & \\multicolumn{2}{c}{\\textbf{GLEM}} \\\\\n%      Datasets     &    Metric   & Deberta-base-Ft & SAGE-{$\\mathbf{X}_{\\text {OGB}}$}  &  joint-BERT-tiny & GraphFormers & GLEM-G-step & GLEM-L-step \\\\\n%     \\midrule\n%     \\multirow{6}[2]{*}{Arxiv} & val.  &  75.27 ± 0.09 &72.77 ± 0.16  & 71.58 ± 0.18 & 73.33 ± 0.06 & 76.45 ± 0.05 & 75.32 ± 0.04\\\\\n%           & test acc..  & 74.13 ± 0.04 & 71.49 ± 0.27 & 70.87 ± 0.12 & 72.81 ± 0.20 & 75.50 ± 0.24 & 74.53 ± 0.12 \\\\\n%           & parameters & 138,632,488 & 218,664 & 110,694,592 & 110,694,592 & 545,320 & 138,632,488 \\\\\n%           & max bsz. & all nodes & 30    & 200   & 180   & all nodes & 30 \\\\\n%         %   & neighbors & All   & 0     & 10    & 10    & All   & 0 \\\\\n%           & time/epoch & 0.09s & 2760s &  1827s & 4824s & 0.13s & 3801s\\\\\n%     \\midrule\n%     \\multirow{6}[2]{*}{Products} & val$.$  & 91.99 ± 0.07 & 91.82 ± 0.11 & 90.85 ± 0.12 & 91.77 ± 0.09 & 93.84 ± 0.12 & 92.71 ± 0.15 \\\\\n%           & test acc.. & 79.21 ± 0.15 & 79.63 ± 0.12 & 73.13 ± 0.11 & 74.72 ± 0.16 & 83.16 ± 0.19 & 81.25 ± 0.15 \\\\\n%           & parameters & 206,895 & 138,637,871 & 110,699,975 & 110,699,975 & 548,911 & 138,637,871 \\\\\n%           & max bsz. & all nodes & 30    & 100   & 100   & 80000 & 30 \\\\\n%         %   & neighbors & 5,10,15 & 0     &  10,0,0 &  10,0,0 & 5,10,15 & 0 \\\\\n%           & time/epoch & 8.1s  & 5460s & 8456s & 12574s & 153s  & 7740s \\\\\n%     \\bottomrule\n%     \\end{tabular}}%\n%   \\label{tab: comparison}%\n% \\end{table}%\\label{sec: exp_comparison}\nAs discussed before, besides directly fine-tuning LM (denoted as \\textbf{LM-Ft}), a few training paradigms have been proposed for fusing GNNs and LMs. One paradigm is to use fixed/static LMs to generate node embeddings for GNN to do label prediction (denoted as \\textbf{Static}). Besides that, another paradigm is to restrict message passing to a few sampled neighbors, so that the memory cost can be reduced (denoted as \\textbf{Joint}). In this section, we compare our proposed paradigm (\\textbf{GLEM}) against the others. For each paradigm, we choose two models trained with it. The results are presented in Table~\\ref{tab: comparison}. \nwe see that although static training has the best efficiency, its classification accuracy is not very high due to the restricted model capacity caused by the fixed LM. On the other hand, the joint training paradigm has the worst efficiency and effectiveness due to the reduced graph structure. Finally, our proposed paradigm achieves the optimal classification results, thanks to its ability to encourage the collaboration of LMs and GNNs. Meanwhile, our paradigm remains close to static training in terms of efficiency (time/epoch). To summarize, our proposed approach achieves much better results than other paradigms without sacrificing efficiency.\n\n%: The effectiveness is hampered by existing joint learning frameworks: \n%The joint-BERT-tiny model restricts LM capacity and leads to worst results, indicating the necessity of well-modeled text encoding. As for GraphFormers, the performance are also inferior as the neighborhood information is restricted to first-hop sampled neighbors, losing a great deal of information.\n% (2) For the static embedding family models, they are much more scalable compared with joint models yet they are not capable of learning both GNN and LM parameters for graph application, leading to downgraded performance.\n%The proposed alternate training framework of \\ours enables both GNN and LM to be trained separately and therefore enables large LM training and efficient GNN training on all nodes. With these merits, the potential of both modules could be unleashed, achieving strong results without much additional time compared to the single module training process.\n\n\n%As introduced in $\\S$\\ref{sec: related_work}, directly jointly training GNN and LM together suffer from scalability issues. In this section, we compare the scalability and effectiveness of different ways of fusing LM and GNN together. Specifically, we use GraphSAGE~\\citep{GraphSAGE} as the base GNN model and integrate multiple framework based on it. To perform joint training, one way is to utilize a LM with small capacity to achieve scalability, we choose BERT-tiny~\\citep{berttiny} as the LM text encoder and jointly-train it with the GraphSAGE model, denoted as joint-BERT-tiny. Another way is to restrict message passing to few sampled neighbors, we choose GraphFormers~\\citep{GraphFormers} as the representative for this category. The results of ogbn-arxiv and ogbn-products are listed in Table~\\ref{tab: comparison}, from which we \n%can observe that: The effectiveness is hampered by existing joint learning frameworks: \n%The joint-BERT-tiny model restricts LM capacity and leads to worst results, indicating the necessity of well-modeled text encoding. As for GraphFormers, the performance are also inferior as the neighborhood information is restricted to first-hop sampled neighbors, losing a great deal of information.\n% (2) For the static embedding family models, they are much more scalable compared with joint models yet they are not capable of learning both GNN and LM parameters for graph application, leading to downgraded performance.\n%The proposed alternate training framework of \\ours enables both GNN and LM to be trained separately and therefore enables large LM training and efficient GNN training on all nodes. With these merits, the potential of both modules could be unleashed, achieving strong results without much additional time compared to the single module training process. % to be added\n\n\n% \\subsection{Case Study of Attention}\n% \\label{sec: exp_case_study}\n\n% \\subsection{Parameter Analysis}\n% \\label{sec: exp_para_analysis}\n\n% \\subsubsection{Pl-weight}\n% Done\n% \\subsubsection{Efficient Inference: PL-ratio}\n% Done\n\\vspace{-5pt}\n"
                },
                "subsection 5.5": {
                    "name": "Convergence Analysis",
                    "content": "\n\n\\label{sec: exp_convergence_analysis}\nIn \\ours, we utilize the variational EM algorithm for optimization, which consists of an E-step training \\ours-LM and an M-step training \\ours-GNN in each iteration. Here, we analyze the convergence of \\ours by looking into the training curves of validation accuracy on ogbn-arxiv and OGB-Products. From the results in Fig.~\\ref{fig: converge}. We can clearly see that with each E-step and M-step, both the performance of \\ours-GNN and the \\ours-LM consistently increase to a maximum point and converge in a few iterations. Notably, \\ours takes only one iteration to converge on the ogbn-arxiv dataset, which is very efficient.\n\\vspace{-5pt}\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nThis paper studies how to fuse LMs and GNNs together for node representation learning in TAGs. We propose an approach \\ours based on a pseudo-likelihood variational framework. GLEM alternatively updates the LM and GNN via an E-step and an M-step, allowing for better scalability. In each step, both GNN and LM are mutually enhanced by learning from pseudo-labels predicted by the other module, fusing graph and language learning together. Extensive experiments on multiple datasets in two settings demonstrate the effectiveness and efficiency of GLEM. %Specifically, GLEM-LM yields structure-free yet effective predictions (comparable with GNNs). GLEM-GNN achieves new state-of-the-art results on ogbn-arxiv and ogbn-products.\n% \\input{figures/att_ana_para}\n\n"
            },
            "section 7": {
                "name": "Acknowledgement",
                "content": "\nThis project is supported by Twitter, Intel, the Microsoft Research Asia internship program, the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund, a NRC Collaborative R\\&D Project (AI4D-CORE-06) as well as the IVADO Fundamental Research Project grant PRF-2019-3583139727.\n\\bibliography{references.bib}\n\\bibliographystyle{iclr2023_conference}\n\n\\newpage\n\\appendix\n\n\\onecolumn\n\n\n\n"
            },
            "section 8": {
                "name": "Sensitivity Analysis",
                "content": "\n\n% Todo GCN experiments\n\nRecall that GLEM fuses LMs and GNNs by training them separately with an E-step and an M-step, where at each step both the observed labels and pseudo-labels are used for model training as shown in Eq.\\ref{Eq. Inf-Objective} and Eq.\\ref{Eq. GNN-Objective}. For both objectives, we introduce a coefficient, i.e., $\\alpha$ and $\\beta$) denoted as LM-PL-weight and GNN-PL-weight respectlively, to control the relative weight of pseudo-labels. Next, we systematically analyze these two hyperparameters by investigating how the performance varies when changing the hyperparameters. We treat GCN~\\citep{GCN} and RevGAT~\\citep{RevGAT} as the backbone GNNs. The results on the OGBN-Arxiv dataset are shown in Figure \\ref{fig: para_anal_gcn} and Figure \\ref{fig: para_anal_revgat} respectively. \n\n%Recall that the GLEM framework fuses LMs and GNNs by training them separately with pseudo-label-enhanced objectives in Eq.\\ref{Eq. Inf-Objective} and Eq.\\ref{Eq. GNN-Objective}. The effect of the psuedo-label loss weight in GLEM-LM and GLEM-GNN are balanced by $\\alpha$ and $\\beta$, denoted as LM-PL-weight and GNN-PL-weight respectlively.The importance of pseudo labels in training LM and GNN, are balanced by $\\alpha$ and $\\beta$ respectively. Here, we analyse these important parameters for GCN~\\citep{GCN} and RevGAT~\\citep{RevGAT} on the OGBN-Arxiv dataset, shown in Figure \\ref{fig: para_anal_gcn} and Figure \\ref{fig: para_anal_revgat} respectively. \n\nOn one hand, we can clearly see that the LM-PL-weight $\\alpha$ is an important parameter, as GLEM-LM provides both node feature and pseudo-labels for GLEM-GNN. We also observe that guiding LM with pseudo labels of GNN consistently boosts the performance of both LM and GNN compared with optimizing LM with gold label only, i.e. $\\alpha=0$, demonstrating the effectiveness of fusing the knowledge of GNN into LM. \nOn the other hand, the GNN-PL-weight $\\alpha$ that balance the important of LM pseudo-labels in training GLEM-GNN is not very sensitive. Lastly, we also observe that for different GNN and LM, the optimal $\\alpha$ and $\\beta$ varies, indicating these parameters should be carefully selected.\n% Findings: LM Performance 和 Fine tuning 相比提升很多\n\n% LM影响比较明显，GNN 影响不明显；\n\n% GNN and \n\n% 两种方案\n% RevGAT LM-first\n% GCN GNN-first\n% LM-First 三条线三个颜色 LM-Iter1, GNN-Iter-1, GNN-Iter1\n\n% Pros: 可以讨论EM-order 对 GNN 和 LM 效果的影响。\n\n\n% RevGAT LM-first\n% GCN GNN-first\n% 4条线2个颜色 LM-First LM-Iter1, GNN-Iter-1, LM-Iter2, [GNN-Iter1\n\n\n% Cons: 不知道\n\n\n\n% 还差一点GCN上lm pl weight的影响\n\n% % \\section{Optimality Condition for $q_\\theta$}\n% \\section{Derivation of the E-step's Objective}\n\n% Due to the limited space, Sec. 4.3 does not provide the detailed derivation of the E-step. In this section, we present a more detailed analysis of the E-step.\n\n% Recall that the goal of the E-step is to adjust $q_\\theta$ to minimize the KL divergence between $q_\\theta$ and $p_\\phi$ as follows:\n% \\begin{equation}\n% \\begin{aligned}\n%     &\\text{KL}(q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)||p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L))\\\\\n%     & = - \\mathbb{E}_{q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)}[\\log p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)] + \\mathbb{E}_{q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)}[\\log q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)].\n% \\end{aligned}\n% \\end{equation}\n% The optimal $q_\\theta$ is achieved if and only if $q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U) = p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)$.\n\n% However, the KL divergence is hard to be optimized as it involves the entropy of $q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)$, and dealing with the entropy requires some more advanced techniques such as REINFORCE~\\citep{williams1992simple}. To simplify the optimization, we instead resort to the wake-sleep~\\citep{hinton1995wake} algorithm, which considers the reverse KL divergence as below:\n% \\begin{equation}\n% \\begin{aligned}\n%     &\\text{KL}(p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)||q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U))\\\\\n%     & = - \\mathbb{E}_{p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)}[\\log q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)] + \\mathbb{E}_{p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)}[\\log p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)] \\\\\n%     & = - \\mathbb{E}_{p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)}[\\log q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)] + \\text{const}.\n% \\end{aligned}\n% \\end{equation}\n% Here, the entropy term $\\mathbb{E}_{p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)}[\\log p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)]$ is constant as $p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)$ is fixed in the E-step.\n\n% \\label{sec:appendix-maxmin}\n\n\n"
            },
            "section 9": {
                "name": "Reproducibility Statement",
                "content": "\n\nWe provide our code in a public repository along with detailed instructions on conducting experiments. Our experimental settings and implementation details are stated in Section \\ref{sec. exp_setup}, the important hyper-parameters are discussed in the Appendix.\n"
            }
        },
        "tables": {
            "tab: data statistic": "\\begin{table}[tb]\n\\caption{Statistics of the OGB datasets~\\citep{OGB}.}\n\\label{tab: data statistic}\n\\small\n\\begin{tabular}{lrrccc}\n\\toprule\n& \\#\\textbf{Nodes}    & \\#\\textbf{Edges}       & \\textbf{Avg. Node Degree} & \\textbf{Train}\\;/\\;\\textbf{Val}\\;/\\;\\textbf{Test} (\\%) \\\\ \\midrule\nogbn-arxiv (Arxiv)     & 169,343     & 1,166,243     & 13.7             & 54 / 18 / 28             \\\\\nogbn-products (Products)  & 2,449,029   & 61,859,140    & 50.5             & 8 / 2 / 90              \\\\\nogbn-papers100M (Papers) & 111,059,956 & 1,615,685,872 & 29.1             & 78 / 8 / 14              \\\\ \\bottomrule\n\\end{tabular}\n\\vspace{-3mm}\n\\end{table}",
            "tab: main_table": "\\begin{table*}[bt]\n  \\centering\n  \\setlength{\\belowcaptionskip}{5pt}\n  \\caption{Node classification accuracy for the Arxiv and Products datasets. (mean\\,±\\,std\\%, the best results are bolded and the runner-ups are underlined). G ↑ denotes the improvements of \\ours-GNN over the same GNN trained on $\\mathbf{X}_{\\text {OGB}}$; L ↑ denotes the improvements of \\ours-LM over LM-Ft. ``+'' denotes additional tricks are implemented in the original GNN models.}\n  \\label{tab: main_table}\n\n  \\scalebox{0.70}{\n       \\begin{tabular}{c|cl|ccccc|ccc}\n    \\toprule\n    \\multicolumn{1}{c}{\\textbf{{Datasets}}} & \\multicolumn{2}{c}{\\textbf{{Methods}}} & \\multicolumn{5}{c}{\\textbf{GNN}}               & \\multicolumn{3}{c}{\\textbf{LM}} \\\\\n    \\multicolumn{1}{c}{} & \\multicolumn{2}{c}{} & $\\mathbf{X}_{\\text {OGB}}$  & $\\mathbf{X}_{\\text {GIANT}}$ & $\\mathbf{X}_{\\text {PLM}}$ & GLEM-GNN & \\multicolumn{1}{c}{G ↑} & LM-Ft & GLEM-LM & L↑\\\\\n    \\midrule\n    \\multirow{8}[2]{*}{Arxiv} & \\multirow{2}[1]{*}{GCN} & \\textit{val}   & 73.00 ± 0.17 & 74.89 ± 0.17 & 47.56 ± 1.91 & 76.86 ± 0.19 & 3.86  & 75.27 ± 0.09  & 76.17 ± 0.47 & 0.90 \\\\\n          &       & \\textit{test}  & 71.74 ± 0.29 & 73.29 ± 0.10 & 48.19 ± 1.47 & \\underline{75.93 ± 0.19} & 4.19  & 74.13 ± 0.04 & 75.71 ± 0.24 & 1.58 \\\\\n          & \\multirow{2}[0]{*}{SAGE} & \\textit{val}   & 72.77 ± 0.16 & 75.95 ± 0.11 & 56.16 ± 0.46 & 76.45 ± 0.05 & 3.68  & 75.27 ± 0.09  & 75.32 ± 0.04 & 0.6 \\\\\n          &       & \\textit{test}  & 71.49 ± 0.27 & 74.35 ± 0.14 & 56.39 ± 0.82 & 75.50 ± 0.24 & 4.01  & 74.13 ± 0.04 & 74.53 ± 0.12 & 1.44 \\\\\n          & \\multirow{2}[0]{*}{GAMLP} & \\textit{val}   &  62.20 ± 0.11 & 75.01 ± 0.02 & 71.14 ± 0.19 & 76.95 ± 0.14 & 14.75 & 75.27 ± 0.09  & 75.64 ± 0.30 & 0.44 \\\\\n          &       & \\textit{test}  &  56.53 ± 0.02 &  73.35 ± 0.14 & 70.15 ± 0.22 & 75.62 ± 0.23 & 19.09 & 74.13 ± 0.04 & 74.48 ± 0.41 & 2.04 \\\\\n          & \\multirow{2}[1]{*}{RevGAT} & \\textit{val}   & 75.01 ± 0.10 & 77.01 ± 0.09 & 71.40 ± 0.23 & 77.49 ± 0.17 & 2.48  & 75.27 ± 0.09  & 75.75 ± 0.07 & 0.48 \\\\\n          &       & \\textit{test}  & 74.02 ± 0.18 & 75.90 ± 0.19\t & 70.21 ± 0.30 & \\textbf{76.97 ± 0.19} & 2.95  & 74.13 ± 0.04 & 75.45 ± 0.12 & 1.32 \\\\\n    \\midrule\n    \\multirow{6}[2]{*}{Products} & \\multirow{2}[1]{*}{SAGE} & \\textit{val}   & 91.99 ± 0.07 & 93.47 ± 0.14 & 86.74 ± 0.31 & 93.84 ± 0.12 & 1.85  & 91.82 ± 0.11 & 92.71 ± 0.15 & 0.71 \\\\\n          &       & \\textit{test}  & 79.21 ± 0.15 & 82.33 ± 0.37 & 71.09 ± 0.65 & 83.16 ± 0.19 & 3.95  & 79.63 ± 0.12 & 81.25 ± 0.15 & 1.61 \\\\\n          & \\multirow{2}[0]{*}{GAMLP} & \\textit{val}   & 93.12 ± 0.03 & 93.99 ± 0.04 & 91.65 ± 0.17 & 94.19 ± 0.01 & 1.07  & 91.82 ± 0.11 & 90.56 ± 0.04 & -1.26 \\\\\n          &       & \\textit{test}  & 83.54 ± 0.09 & 83.16 ± 0.07 & 80.49 ± 0.19 & 85.09 ± 0.21 & 1.55  & 79.63 ± 0.12 & 82.23 ± 0.27 & 2.60 \\\\\n          & \\multirow{2}[1]{*}{SAGN+} & \\textit{val}   & 93.02 ± 0.04 & 93.64 ± 0.05 & 92.78 ± 0.04 & 94.00 ± 0.03 & 0.98  & 91.82 ± 0.11 & 92.01 ± 0.05 & 0.21 \\\\\n          &       & \\textit{test}  & 84.35 ± 0.09 & \\underline{86.67 ± 0.09} & 84.20 ± 0.39 & \\textbf{87.36 ± 0.07} & 3.01  & 79.63 ± 0.12 & 84.83 ± 0.04 & 5.17 \\\\\n    \\midrule\n    \\multirow{4}[2]{*}{Papers} & \\multirow{2}[1]{*}{GAMLP} & \\textit{val}   & 71.17 ± 0.14 & 72.70 ± 0.07 & 69.78 ± 0.07 & 71.71 ± 0.09 & 0.54  & 68.05 ± 0.03 & 69.94 ± 0.16 & 1.89 \\\\\n          &       & \\textit{test}  & 67.71 ± 0.20 & 69.33 ± 0.06 & 65.94 ± 0.10 & 68.25 ± 0.14 & 0.54  & 63.52 ± 0.06 & 64.80 ± 0.06 & 1.78 \\\\\n          & \\multirow{2}[1]{*}{GAMLP+} & \\textit{val}   & 71.59 ± 0.05 & 73.05 ± 0.04 & 69.87 ± 0.06 & 73.54 ± 0.01 & 1.95  & 68.05 ± 0.03 & 71.16 ± 0.45 & 3.11 \\\\\n          &       & \\textit{test}  & 68.25 ± 0.11 & \\underline{69.67 ± 0.05} & 66.36 ± 0.09 & \\textbf{70.36 ± 0.02} & 2.11  & 63.52 ± 0.06 & 66.71 ± 0.25 & 3.19 \\\\\n    \\bottomrule\n    \\end{tabular}}%\n    \\vspace{-4mm}\n\\end{table*}",
            "tab:largelm": "\\begin{table}[bt]\n  \\centering\n  \\small\n  \\setlength{\\belowcaptionskip}{3pt}\n  \\caption{Experiments on Arxiv (RevGAT as GNN backbone) with different scale of LM.}\n  \\label{tab:largelm}\n    \\begin{tabular}{lccc}\n    \\toprule\n    \\textbf{Methods} & \\multicolumn{1}{l}{\\textbf{Val. accuracy}} & \\multicolumn{1}{l}{\\textbf{Test accuracy}} & \\multicolumn{1}{l}{\\# \\textbf{Parameters}}\\\\\n        \\midrule\n    GNN-$\\mathbf{X}_{\\text {OGB}}$ & 75.01 ± 0.10 & 74.02 ± 0.18 & 2,098,256 \\\\\n    GNN-$\\mathbf{X}_{\\text {GIANT}}$ & 77.01 ± 0.09 & 75.90 ± 0.19 & {1,304,912} \\\\\n    \\ours-GNN-base & 77.49 ± 0.17 & 76.97 ± 0.19 & 1,835,600 \\\\\n    \\ours-GNN-large & 77.92 ± 0.06 & 77.62 ± 0.16 & 2,228,816 \\\\\n    \\midrule\n    LM-base-Ft & 75.27 ± 0.09  & 74.13 ± 0.04 & 138,632,488 \\\\\n    LM-large-Ft & 75.08 ± 0.06 & 73.81 ± 0.08 & 405,204,008 \\\\\n    \\ours-LM-base & 75.75 ± 0.07 & 75.45 ± 0.12 & 138,632,488 \\\\\n    \\ours-LM-large & 77.16 ± 0.04 & 76.80 ± 0.05 & 405,204,008\\\\\n    \\bottomrule\n    \\end{tabular}%\n    \\vspace{-4mm}\n\\end{table}",
            "tab:Structure_free": "\\begin{table}[t]\n  \\centering\n  \\small\n  \\setlength{\\belowcaptionskip}{5pt}\n  \\caption{Structure free inductive experiments on ogbn-arxiv and ogbn-products. The validation and test accuracies, denoted as w/ struct and wo struct, and their relative differences (the lower the better) are reported. Boldfaced numbers indicate the best performances in each group.}\n  \\scalebox{0.95}{\n    \\begin{tabular}{clcccccc}\n    \\toprule\n    \\multicolumn{1}{l}{\\textbf{Type}} & \\textbf{Methods} & \\multicolumn{3}{c}{\\textbf{Arxiv}} & \\multicolumn{3}{c}{\\textbf{Products}}\\\\\n          &       & w/ struct & wo struct & diff  & w/ struct & wo struct & diff \\\\\n    \\midrule\n    \\multirow{4}[2]{*}{MLP} & $\\mathbf{X}_{\\text {OGB}}$& 57.65 ± 0.12 & 55.50 ± 0.23 & -2.15 & 75.54 ± 0.14 & 61.06 ± 0.08 & -14.48 \\\\\n          & $\\mathbf{X}_{\\text {LM-Ft}}$ & 74.56 ± 0.01 & 72.98 ± 0.06 & \\textbf{-1.58} & 91.79 ± 0.01 & 79.93 ± 0.22 & -11.86 \\\\\n          & $\\mathbf{X}_{\\text {\\ours}}$-light & 75.20 ± 0.03 & 73.32 ± 0.31 & -1.88 & 91.96 ± 0.01 & 79.38 ± 0.14 & -12.58 \\\\\n          & $\\mathbf{X}_{\\text {\\ours}}$-deep & 75.57 ± 0.03 & \\textbf{73.90 ± 0.08} & -1.67 & 91.85 ± 0.02 & \\textbf{80.04 ± 0.15} & \\textbf{-11.81}\\\\\n    \\midrule\n    \\multirow{4}[2]{*}{ GNN} & $\\mathbf{X}_{\\text {OGB}}$-light & 70.73 ± 0.02 & 48.59 ± 0.19 & -22.14 & 90.54 ± 0.04 & 51.23 ± 0.17 & -39.31 \\\\\n          & $\\mathbf{X}_{\\text {\\ours}}$-light & 76.73 ± 0.02 & 73.94 ± 0.03 & -2.79 & 92.95 ± 0.03 & 78.75 ± 0.39 & -14.20 \\\\\n          & $\\mathbf{X}_{\\text {OGB}}$-deep & 72.67 ± 0.03 & 50.92 ± 0.19 & -21.75 & 91.85 ± 0.11 & 32.71 ± 2.23 & -59.14 \\\\\n          & $\\mathbf{X}_{\\text {\\ours}}$-deep & 76.79 ± 0.06 & \\textbf{74.29 ± 0.11} & \\textbf{-2.50} & 93.22 ± 0.03 & \\textbf{ 79.81 ± 0.01} & \\textbf{-13.41}\\\\\n    \\midrule\n    \\multirow{3}[2]{*}{LM} & Fine-tune    & 75.27 ± 0.09  & 74.13 ± 0.04 & -1.14 & 91.82 ± 0.11 & 79.63 ± 0.12 & -12.19 \\\\\n          & \\ours-light & 75.49 ± 0.11 & 74.50 ± 0.16 & -0.99 & 91.90 ± 0.06 & 79.53 ± 0.13 & -12.37 \\\\\n          & \\ours-deep & 75.59 ± 0.08 & \\textbf{74.60 ± 0.05} & \\textbf{-0.99} & 91.81 ± 0.04 & \\textbf{79.69 ± 0.51} & \\textbf{-12.12} \\\\\n    \\bottomrule\n    \\end{tabular}}%\n  \\label{tab:Structure_free}%\n  \\vspace{-4mm}\n\\end{table}",
            "tab: comparison": "\\begin{table}[bt]\n  \\centering\n  \\setlength{\\belowcaptionskip}{2pt}\n  \\caption{Comparison of different training paradigms of fusing LM and GNNs. The maximum batch size (max bsz.) and time/epoch are tested on a single NVIDIA Tesla V100 32GB GPU.}\n   \\scalebox{0.76}{\n    \\begin{tabular}{cccccccc}\n    \\toprule\n     &  & {\\textbf{LM-Ft}} & \\textbf{Static} & \\multicolumn{2}{c}{\\textbf{Joint}} & \\multicolumn{2}{c}{\\textbf{GLEM}} \\\\\n     Datasets     &    Metric   & DeBERTa-base & SAGE-{$\\mathbf{X}_{\\text {OGB}}$}  &  joint-BERT-tiny & GraphFormers & GLEM-GNN & GLEM-LM \\\\\n    \\midrule\n    \\multirow{6}[2]{*}{Arxiv} & val. acc. &  75.27 ± 0.09 &72.77 ± 0.16  & 71.58 ± 0.18 & 73.33 ± 0.06 & 76.45 ± 0.05 & 75.32 ± 0.04\\\\\n          & test acc.  & 74.13 ± 0.04 & 71.49 ± 0.27 & 70.87 ± 0.12 & 72.81 ± 0.20 & 75.50 ± 0.24 & 74.53 ± 0.12 \\\\\n          & parameters & 138,632,488 & 218,664 & 110,694,592 & 110,694,592 & 545,320 & 138,632,488 \\\\\n          & max bsz.  & 30 & all nodes   & 200   & 180   & all nodes & 30 \\\\\n        %   & neighbors & All   & 0     & 10    & 10    & All   & 0 \\\\\n          & time/epoch  & 2760s & 0.09s &  1827s & 4824s & 0.13s & 3801s\\\\\n    \\midrule\n    \\multirow{6}[2]{*}{Products} & val. acc.   & 91.82 ± 0.11 & 91.99 ± 0.07 & 90.85 ± 0.12 & 91.77 ± 0.09 & 93.84 ± 0.12 & 92.71 ± 0.15 \\\\\n          & test acc..  & 79.63 ± 0.12 & 79.21 ± 0.15 & 73.13 ± 0.11 & 74.72 ± 0.16 & 83.16 ± 0.19 & 81.25 ± 0.15 \\\\\n          & parameters  & 138,637,871 & 206,895 & 110,699,975 & 110,699,975 & 548,911 & 138,637,871 \\\\\n          & max bsz. & 30 & all nodes    & 100   & 100   & 80000 & 30 \\\\\n        %   & neighbors & 5,10,15 & 0     &  10,0,0 &  10,0,0 & 5,10,15 & 0 \\\\\n          & time/epoch   & 5460s & 8.1s & 8456s & 12574s & 153s  & 7740s \\\\\n    \\bottomrule\n    \\end{tabular}}%\n  \\label{tab: comparison}%\n\\end{table}"
        },
        "figures": {
            "fig: Model Framework": "\\begin{figure*}[h]\n\\centering\n\\includegraphics[width=0.65\\linewidth]{figures/Model.pdf}\n\\caption{The proposed \\ours framework trains GNN and LM separately in a variational EM framework: In E-step, an LM is trained towards predicting both the gold labels and GNN-predicted pseudo-labels; In M-step, a GNN is trained by predicting both gold labels and LM-inferred pseudo-labels using the embeddings and pseudo-labels predicted by LM.}\n\\label{fig: Model Framework}\n\\end{figure*}",
            "fig: converge": "\\begin{figure}[bt]\n% \\small\n\\centering\n\\begin{tikzpicture}\n\\small{\n\\begin{axis}[\nat={(0,0)},\nwidth=.28\\textwidth, height=.2\\textwidth ,\nxtick={1,2,3},\nxticklabels={$0$,$1$,$2$},\nytick={73,74,...,78},\ngrid style=dashed,\nylabel={Test Set (\\%)},\nxlabel={ogbn-arxiv},\nxlabel style={align=center,font=\\small,yshift=0.2em},\nylabel style={font=\\small,yshift=-1.2em},\ny tick style={opacity=0},\n% ymode=log,\ny tick label style={font=\\scriptsize},\nymajorgrids=true,\nxmajorgrids=true,\ntick align=inside,\nyticklabel style={/pgf/number format/precision=1,/pgf/number format/fixed zerofill},\nxmin=0.5,\nxmax=3.5,\nymin=73,\nymax=78]\n    \\addplot[\n        red!60,mark=*,mark size=2pt,thick,mark options={fill=white,draw=red,line width=1pt}\n        ]\n        coordinates {\n        (1, 76.79)\n        (2, 76.82)\n        (3, 76.95)\n        };\n    \\addplot[\n        blue!60,mark=square*,mark size=2pt,thick,mark options={fill=white,draw=blue,line width=1pt}\n        ]\n        coordinates {\n        (1, 74.13)\n        (2, 76.86)\n        (3, 76.62)\n        };\n\\end{axis}\n}\n\\vspace{1cm}\n\\small{\n\\begin{axis}[\nat={(12.0em,0)},\nwidth=.28\\textwidth, height=.2\\textwidth ,\nxtick={1,2,3},\nxticklabels={$0$,$1$,$2$},\nytick={62,64,...,72},\ngrid style=dashed,\nylabel={Test Set (\\%)},\nxlabel={ogbn-papers100M},\nxlabel style={align=center,font=\\small,yshift=0.2em},\nylabel style={font=\\scriptsize,yshift=-1.2em},\ny tick style={opacity=0},\n% ymode=log,\ny tick label style={font=\\scriptsize},\nymajorgrids=true,\nxmajorgrids=true,\ntick align=inside,\nyticklabel style={/pgf/number format/precision=1,/pgf/number format/fixed zerofill},\nxmin=0.5,\nxmax=3.5,\nymin=62,\nymax=72]\n    \\addplot[\n        red!60,mark=*,mark size=2pt,thick,mark options={fill=white,draw=red,line width=1pt}\n        ]\n        coordinates {\n        (1, 68.07)\n        (2, 68.66)\n        (3, 68.91)\n        };\n    \\addplot[\n        blue!60,mark=square*,mark size=2pt,thick,mark options={fill=white,draw=blue,line width=1pt}\n        ]\n        coordinates {\n        (1, 63.52)\n        (2, 64.92)\n        (3, 65.42)\n        };\n\\end{axis}\n}\n\\vspace{1cm}\n\\small{\n\\begin{axis}[\nat={(24.0em,0)},\nwidth=.28\\textwidth, height=.2\\textwidth ,\nxtick={1,2,3,4,5,6},\nxticklabels={$0$,$1$,$2$,$3$,$4$,$5$},\nytick={78,80,...,88},\ngrid style=dashed,\nxlabel={ogbn-products},\n% ylabel={Dev\\ \\ Perplexity},\nxlabel style={align=center,font=\\small,yshift=0.2em},\nyticklabel style={/pgf/number format/precision=1,/pgf/number format/fixed zerofill},\nylabel style={font=\\scriptsize,yshift=-1em},\ny tick style={opacity=0},\n%x tick label style={font=\\small},\ny tick label style={font=\\scriptsize},\nymajorgrids=true,\nylabel={Test Set (\\%)},\nxmajorgrids=true,\ntick align=inside,\nlegend style={at={(-1.25,1.18)},anchor=south},\nlegend columns=2,\nlegend cell align={left},\nxmin=0.5,\nxmax=6.5,\nymin=78,\nymax=88]\n\n    \\addplot[\n        red!60,mark=*,mark size=2pt,thick,mark options={fill=white,draw=red,line width=1pt}\n        ]\n        coordinates {\n        (1, 82.3)\n        (2, 84.3)\n        (3, 84.8)\n        (4, 85.2)\n        (5, 85.4)\n        (6, 85.6)\n        };\n        \\addlegendentry{\\scriptsize{\\ours-GNN}}\n\n    \\addplot[\n        blue,mark=square*,mark size=2pt,thick,mark options={fill=white,draw=blue,line width=1pt}\n        ]\n        coordinates {\n        (1, 79.6)\n        (2, 81.6)\n        (3, 82.1)\n        (4, 82.4)\n        (5, 82.6)\n        (6, 82.8)\n        };\n        \\addlegendentry{\\scriptsize{\\ours-LM}}\n\n\\end{axis}\n}\n\\end{tikzpicture}\n\\vspace{-3mm}\n\\caption{The convergence curves of GLEM on OGB datasets.}\n\\label{fig: converge}\n\\vspace{-3mm}\n\\end{figure}",
            "fig: para_anal_gcn": "\\begin{figure}[bt]\n% \\small\n\\centering\n\\begin{tikzpicture}\n\\small{\n\\begin{axis}[\nat={(0,0)},\nwidth=.38\\textwidth, height=.3\\textwidth ,\nxtick={1,2,3,4,5,6,7,8,9,10,11},\nxticklabels={$0$,$.1$,$.2$,$.3$,$.4$,$.5$,$.6$,$.7$,$.8$,$.9$,$1$},\nytick={69,71,...,77},\ngrid style=dashed,\nylabel={Test Acc (\\%)},\nxlabel={(a) LM-PL-weight $\\alpha$},\nxlabel style={align=center,font=\\small,yshift=0.2em},\nylabel style={font=\\small,yshift=-1.2em},\ny tick style={opacity=0},\n% ymode=log,\ny tick label style={font=\\scriptsize},\nymajorgrids=true,\nxmajorgrids=true,\ntick align=inside,\nyticklabel style={/pgf/number format/precision=1,/pgf/number format/fixed zerofill},\nxmin=0.5,\nxmax=11.5,\nymin=68,\nymax=77.5]\n    \\addplot[\n        blue!60,mark=square*,mark size=2pt,thick,mark options={fill=white,draw=blue,line width=1pt}\n        ]\n        coordinates {\n        (1, 70.80)\n        (2, 72.02)\n        (3, 72.54)\n        (4, 72.28)\n        (5, 73.68)\n        (6, 73.48)\n        (7, 74.05)\n        (8, 73.61)\n        (9, 73.95)\n        (10, 73.14)\n        (11, 72.53)\n        };\n    \\addplot[\n        red!60,mark=*,mark size=2pt,thick,mark options={fill=white,draw=red,line width=1pt}\n        ]\n        coordinates {\n        (1, 74.39)\n        (2, 74.52)\n        (3, 74.77)\n        (4, 75.16)\n        (5, 75.32)\n        (6, 75.17)\n        (7, 75.25)\n        (8, 74.87)\n        (9, 74.76)\n        (10, 74.65)\n        (11, 74.73)\n        };\n\\end{axis}\n}\n\\vspace{1cm}\n\\small{\n\\begin{axis}[\nat={(15.5em,0)},\nwidth=.38\\textwidth, height=.3\\textwidth ,\nxtick={1,2,3,4,5,6,7,8,9,10,11},\nxticklabels={$0$,$.1$,$.2$,$.3$,$.4$,$.5$,$.6$,$.7$,$.8$,$.9$,$1$},\nytick={74,75,...,76},\ngrid style=dashed,\nylabel={Test Acc (\\%)},\nxlabel={(b) GNN-PL-Weight $\\beta$},\nxlabel style={align=center,font=\\small,yshift=0.2em},\nylabel style={font=\\small,yshift=-1.2em},\ny tick style={opacity=0},\n% ymode=log,\ny tick label style={font=\\scriptsize},\nymajorgrids=true,\nxmajorgrids=true,\ntick align=inside,\nyticklabel style={/pgf/number format/precision=1,/pgf/number format/fixed zerofill},\nxmin=0.5,\nxmax=11.5,\nymin=73.00,\nymax=76.5]\n    \\addplot[\n        blue!60,mark=square*,mark size=2pt,thick,mark options={fill=white,draw=blue,line width=1pt}\n        ]\n        coordinates {\n        (1, 73.58)\n        (2, 73.74)\n        (3, 73.80)\n        (4, 73.79)\n        (5, 73.28)\n        (6, 73.52)\n        (7, 73.53)\n        (8, 73.58)\n        (9, 73.85)\n        (10, 73.92)\n        (11, 73.58)\n        };\n    \\addplot[\n        red!60,mark=*,mark size=2pt,thick,mark options={fill=white,draw=red,line width=1pt}\n        ]\n        coordinates {\n        (1, 75.35)\n        (2, 75.4)\n        (3, 75.66)\n        (4, 75.71)\n        (5, 75.65)\n        (6, 75.95)\n        (7, 76.09)\n        (8, 76.12)\n        (9, 76.07)\n        (10, 75.95)\n        (11, 75.83)\n        };\n\\end{axis}\n}\n\\end{tikzpicture}\n\\vspace{-3mm}\n\\caption{The effect of the $\\alpha$ and $\\beta$ for GLEM-GCN.}\n\\label{fig: para_anal_gcn}\n\\vspace{-3mm}\n\\end{figure}",
            "fig: para_anal_revgat": "\\begin{figure}[bt]\n% \\small\n\\centering\n\\begin{tikzpicture}\n\\small{\n\\begin{axis}[\nat={(0,0)},\nwidth=.38\\textwidth, height=.3\\textwidth ,\nxtick={1,2,3,4,5,6,7,8,9,10,11},\nxticklabels={$0$,$.1$,$.2$,$.3$,$.4$,$.5$,$.6$,$.7$,$.8$,$.9$,$1$},\nytick={73,74,...,77},\ngrid style=dashed,\nylabel={Test Acc (\\%)},\nxlabel={(a) LM-PL-Weight $\\alpha$},\nxlabel style={align=center,font=\\small,yshift=0.2em},\nylabel style={font=\\small,yshift=-1.2em},\ny tick style={opacity=0},\n% ymode=log,\ny tick label style={font=\\scriptsize},\nymajorgrids=true,\nxmajorgrids=true,\ntick align=inside,\nyticklabel style={/pgf/number format/precision=1,/pgf/number format/fixed zerofill},\nxmin=0.5,\nxmax=11.5,\nymin=73.5,\nymax=77.5]\n    \\addplot[\n        blue!60,mark=square*,mark size=2pt,thick,mark options={fill=white,draw=blue,line width=1pt}\n        ]\n        coordinates {\n        (1, 74.13)\n        (2, 75.03)\n        (3, 75.24)\n        (4, 75.44)\n        (5, 75.79)\n        (6, 76.00)\n        (7, 75.93)\n        (8, 75.86)\n        (9, 75.97)\n        (10, 75.68)\n        (11, 74.87)\n        };\n        \n    \\addplot[\n        red!60,mark=*,mark size=2pt,thick,mark options={fill=white,draw=red,line width=1pt}\n        ]\n        coordinates {\n        (1, 75.38)\n        (2, 76.06)\n        (3, 76.27)\n        (4, 76.34)\n        (5, 76.83)\n        (6, 76.82)\n        (7, 77.03)\n        (8, 76.71)\n        (9, 77.17)\n        (10, 77.00)\n        (11, 76.93)\n        };\n        \n\\end{axis}\n}\n\\vspace{1cm}\n\\small{\n\\begin{axis}[\nat={(15.5em,0)},\nwidth=.38\\textwidth, height=.3\\textwidth ,\nxtick={1,2,3,4,5,6,7},\nxticklabels={$0$,$.05$,$.1$,$.25$,$.5$,$.75$,$1$},\nytick={73,74,75,...,77},\ngrid style=dashed,\nylabel={Test Acc (\\%)},\n% xlabel={Pseudo label imp. at for GLEM-GNN ($\\beta$}),\nxlabel={(b) GNN-PL-Weight $\\beta$},\nxlabel style={align=center,font=\\small,yshift=0.2em},\nylabel style={font=\\small,yshift=-1.2em},\ny tick style={opacity=0},\n% ymode=log,\ny tick label style={font=\\scriptsize},\nymajorgrids=true,\nxmajorgrids=true,\ntick align=inside,\nlegend style={at={(-0.2,1.18)},anchor=south},\nlegend columns=2,\nlegend cell align={left},\nyticklabel style={/pgf/number format/precision=1,/pgf/number format/fixed zerofill},\nxmin=0.5,\nxmax=7.5,\nymin=72,\nymax=77.5]\n\n    \\addplot[\n        red!60,mark=*,mark size=2pt,thick,mark options={fill=white,draw=red,line width=1pt}\n        ]\n        coordinates {\n        (1, 76.80)\n        (2, 77.07)\n        (3, 76.78)\n        (4, 75.93)\n        (5, 75.62)\n        (6, 75.79)\n        (7, 76.34)\n        };\n        \\addlegendentry{\\scriptsize{\\ours-GNN}}\n        \n    \\addplot[\n        blue!60,mark=square*,mark size=2pt,thick,mark options={fill=white,draw=blue,line width=1pt}\n        ]\n        coordinates {\n        (1, 73.29)\n        (2, 73.95)\n        (3, 73.07)\n        (4, 73.48)\n        (5, 73.73)\n        (6, 72.96)\n        (7, 73.85)\n        };\n        \\addlegendentry{\\scriptsize{\\ours-LM}}\n        \n        \n\\end{axis}\n}\n\\end{tikzpicture}\n\\vspace{-3mm}\n\\caption{The effect of the $\\alpha$ and $\\beta$ for GLEM-RevGAT.}\n\\label{fig: para_anal_revgat}\n\\vspace{-3mm}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    p_{\\theta}(\\mathbf{y}_n | \\mathbf{s}_n)=\\operatorname{Cat}(\\mathbf{y}_n\\mid \\operatorname{softmax}(\\operatorname{MLP}_{\\theta_2}(\\mathbf{h}_{n})));\\quad\\quad \\mathbf{h}_{n}=\\operatorname{SeqEnc}_{\\theta_1}(\\mathbf{s}_n),\n\\end{equation}",
            "eq:2": "\\begin{equation}\np_{\\phi}(\\mathbf{y}_n | A)=\\operatorname{Cat}(\\mathbf{y}_n\\mid \\operatorname{softmax}(\\mathbf{h}_{n}^{(L)}));\\quad\\quad\n\\mathbf{h}_{n}^{(l)} = \\sigma(\\operatorname{AGG}_\\phi(\\operatorname{MSG}_\\phi(\\mathbf{h}_{\\operatorname{NB}(n)}^{(l-1)}),A)),\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\log p(\\mathbf{y}_L | \\mathbf{s}_V, A) %= \\log \\sum_{\\mathbf{y}_U} p(\\mathbf{y}_L, \\mathbf{y}_U | \\mathbf{s}_V, A) \n    \\geq \\mathbb{E}_{q(\\mathbf{y}_U|\\mathbf{s}_U)}[\\log p(\\mathbf{y}_L, \\mathbf{y}_U | \\mathbf{s}_V, A) - \\log q(\\mathbf{y}_U|\\mathbf{s}_U)],\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:pseudo-likelihood}\n    \\mathbb{E}_{q(\\mathbf{y}_U|\\mathbf{s}_U)}[\\log p(\\mathbf{y}_L, \\mathbf{y}_U | \\mathbf{s}_V, A)] \\approx \\mathbb{E}_{q(\\mathbf{y}_U|\\mathbf{s}_U)}[\\sum_{n \\in V} \\log p(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_{V \\setminus n})].\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U) = \\prod_{n \\in U} q_\\theta(\\mathbf{y}_n | \\mathbf{s}_n).\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\begin{aligned}\n    -\\text{KL}(p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L) || q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)) &= \\mathbb{E}_{p_\\phi(\\mathbf{y}_U | \\mathbf{s}_V, A, \\mathbf{y}_L)}[\\log q_\\theta(\\mathbf{y}_U | \\mathbf{s}_U)] + \\text{const} \\\\\n    &= \\sum_{n \\in U} \\mathbb{E}_{p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L)}[\\log q_\\theta(\\mathbf{y}_n | \\mathbf{s}_n)]+ \\text{const},\n\\end{aligned}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L) \\approx p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L, \\mathbf{\\hat{y}}_{U \\setminus n}),\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\begin{aligned}\n\\mathcal{O}(q) %&= \\alpha \\mathcal{O}_{\\text{\\text{Inf}}}+ (1-\\alpha)\\mathcal{O}_{\\text{MLE}} \\\\\n&= \\alpha \\sum_{n \\in U} \\mathbb{E}_{p(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_L, \\mathbf{\\hat{y}}_{U \\setminus n})}[\\log q(\\mathbf{y}_n | \\mathbf{s}_n)] + (1 - \\alpha) \\sum_{n \\in L} \\log q(\\mathbf{y}_n | \\mathbf{s}_n),\n\\end{aligned}\n\\label{Eq. Inf-Objective}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    \\mathcal{O}(\\phi) = \\beta \\sum_{n \\in U} \\log p_\\phi(\\mathbf{\\hat{y}}_n | \\mathbf{s}_V, A, \\mathbf{y}_{L}, \\mathbf{\\hat{y}}_{U \\setminus n}) + (1 - \\beta) \\sum_{n \\in L} \\log p_\\phi(\\mathbf{y}_n | \\mathbf{s}_V, A, \\mathbf{y}_{L \\setminus n}, \\mathbf{\\hat{y}}_{U}),\n\\label{Eq. GNN-Objective}\n\\end{equation}"
        },
        "git_link": "https://github.com/AndyJZhao/GLEM"
    }
}