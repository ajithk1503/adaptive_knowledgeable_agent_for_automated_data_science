{
    "meta_info": {
        "title": "Rethinking Graph Transformers with Spectral Attention",
        "abstract": "In recent years, the Transformer architecture has proven to be very\nsuccessful in sequence processing, but its application to other data\nstructures, such as graphs, has remained limited due to the difficulty of\nproperly defining positions. Here, we present the $\\textit{Spectral Attention\nNetwork}$ (SAN), which uses a learned positional encoding (LPE) that can take\nadvantage of the full Laplacian spectrum to learn the position of each node in\na given graph. This LPE is then added to the node features of the graph and\npassed to a fully-connected Transformer. By leveraging the full spectrum of the\nLaplacian, our model is theoretically powerful in distinguishing graphs, and\ncan better detect similar sub-structures from their resonance. Further, by\nfully connecting the graph, the Transformer does not suffer from\nover-squashing, an information bottleneck of most GNNs, and enables better\nmodeling of physical phenomenons such as heat transfer and electric\ninteraction. When tested empirically on a set of 4 standard datasets, our model\nperforms on par or better than state-of-the-art GNNs, and outperforms any\nattention-based model by a wide margin, becoming the first fully-connected\narchitecture to perform well on graph benchmarks.",
        "author": "Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent L\u00e9tourneau, Prudencio Tossou",
        "link": "http://arxiv.org/abs/2106.03893v3",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "Accepted in Proceedings of NeurIPS 2021"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nThe prevailing strategy for graph neural networks (GNNs) has been to directly encode graph structure structure through a sparse message-passing process \\cite{gilmer2017mpnn,hamilton_2020}.\nIn this approach, vector messages are iteratively passed between nodes that are connected in the graph.\nMultiple instantiations of this message-passing paradigm have been proposed, differing in the architectural details of the message-passing apparatus (see \\cite{hamilton_2020} for a review). \n\nHowever, there is a growing recognition that the message-passing paradigm has inherent limitations.\nThe expressive power of message passing appears inexorably bounded by the Weisfeiler-Lehman isomorphism hierarchy  \\cite{maron2019provably,morris2019weisfeiler,xu2018gin}. \nMessage-passing GNNs are known to suffer from pathologies, such as {\\em oversmoothing}, due to their repeated aggregation of local information \\cite{hamilton_2020}, and {\\em over-squashing}, due to the exponential blow-up in computation paths as the model depth increases \\cite{alon_bottleneck_2020}.\n\nAs a result, there is a growing interest in deep learning techniques that encode graph structure as a {\\em soft inductive bias}, rather than as a hard-coded aspect of message passing \\cite{dwivedi2020generalization,kazi2020differentiable}.\nA central issue with message-passing paradigm is that input graph structure is encoded by restricting the structure of the model's computation graph, inherently limiting its flexibility. \nThis reminds us of how early recurrent neural networks (RNNs) encoded sequential structure via their computation graph---a strategy that leads to well-known pathologies such as the inability to model long-range dependencies \\cite{hochreiter1997long}.\n\nThere is a growing trend across deep learning towards more flexible architectures, which avoid strict and structural inductive biases. \nMost notably, the exceptionally successful Transformer architecture removes any structural inductive bias by encoding the structure via soft inductive biases, such as positional encodings \\cite{vaswani_2017_attention}. \nIn the context of GNNs, the self-attention mechanism of a Transformer can be viewed as passing messages between all nodes, regardless of the input graph connectivity. \n\nPrior work has proposed to use attention in GNNs in different ways. First, the GAT model \\cite{velikovic2017gat} proposed local attention on pairs of nodes that allows a learnable convolutional kernel. The GTN work \\cite{yun_graph_2020} has improved on the GAT for node and link predictions while keeping a similar architecture, while other message-passing approaches have used enhancing spectral features \\cite{changeSGAN, dongadaptive} . More recently, the GT model \\cite{dwivedi2020generalization} was proposed as a generalization of Transformers to graphs, where they experimented with sparse and full graph attention while providing low-frequency eigenvectors of the Laplacian as positional encodings. \n\nIn this work, we offer a principled investigation of how Transformer architectures can be applied in graph representation learning.\n\\textbf{Our primary contribution} is the development of novel and powerful learnable positional encoding methods, which are rooted in spectral graph theory. \nOur positional encoding technique --- and the resulting {\\em spectral attention network (SAN)} architecture --- addresses key theoretical limitations in prior graph Transformer work \\cite{dwivedi2020generalization} and provably exceeds the expressive power of standard message-passing GNNs. \nWe show that full Transformer-style attention provides consistent empirical gains compared to an equivalent sparse message-passing model, and we demonstrate that our SAN architecture is competitive with or exceeding the state-of-the-art on several well-known graph benchmarks. An overview of the entire method is presented in Figure \\ref{fig:full-method}, with a link to the code here: \\url{https://github.com/DevinKreuzer/SAN}.\n\n\n\n\n\n\n",
                "category": [
                    "introduction"
                ]
            },
            "section 2": {
                "name": "Theoretical Motivations",
                "content": "\n\nThere can be a significant loss in structural information if naively generalizing Transformers to graphs. To preserve this information as well as local connectivity, previous studies \\cite{velikovic2017gat, dwivedi2020generalization} have proposed to use the eigenfunctions of their Laplacian as positional encodings. Taking this idea further by using the full expressivity of eigenfunctions as positional encodings, we can propose a principled way of understanding graph structures using their spectra. \nThe advantages of our methods compared to previous studies \\cite{velikovic2017gat, dwivedi2020generalization} are shown in Table \\ref{tab:comparison-theory}.\n\n\n\n\n\n\\footnotetext[1]{Presented results add full connectivity before computing the eigenvectors, thus losing the structural information of the graph.}\n\n\n",
                "subsection 2.1": {
                    "name": "Absolute and relative positional encoding with eigenfunctions",
                    "content": "\n\nThe notion of positional encodings (PEs) in graphs is not a trivial concept, as there exists no canonical way of ordering nodes or defining axes. In this section, we investigate how eigenfunctions of the Laplacian can be used to define absolute and relative PEs in graphs, to measure physical interactions between nodes, and to enable ''hearing'' of specific sub-structures - similar to how the sound of a drum can reveal its structure.\n\n",
                    "subsubsection 2.1.1": {
                        "name": "Eigenvectors equate to sine functions over graphs",
                        "content": "\n\nIn the Transformer architecture, a fundamental aspect is the use of sine and cosine functions as PEs for sequences \\cite{vaswani_2017_attention}.\nHowever, sinusoids cannot be clearly defined for arbitrary graphs, since there is no clear notion of position along an axis. Instead, their equivalent is given by the eigenvectors $\\eigvec$ of the graph Laplacian $\\mL$.\nIndeed, in a Euclidean space, the Laplacian (or Laplace) operator corresponds to the divergence of the gradient and its eigenfunctions are sine/cosine functions, with the squared frequencies corresponding to the eigenvalues (we sometimes interchange the two notions from here on). \nHence, in the graph domain, the eigenvectors of the graph Laplacian are the natural equivalent of sine functions, and this intuition was employed in multiple recent works which use the eigenvectors as PEs for GNNs \\cite{dwivedi2020benchmarking}, for directional flows \\cite{beaini_directional_2021} and for Transformers \\cite{dwivedi2020generalization}. \n\n\n\n\nBeing equivalent to sine functions, we naturally find that the Fourier Transform of a function $\\mathcal{F}[f]$ applied to a graph gives $\\mathcal{F}[f](\\lambda_i) = \\langle f, \\eigvec_i \\rangle$, where the eigenvalue is considered as a position in the Fourier domain of that graph \\cite{bronstein_geometric_2017}.\nThus, the eigenvectors are best viewed as vectors positioned on the axis of eigenvalues rather than components of a matrix as illustrated in Figure \\ref{fig:eigvec-matrix}.\n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 2.1.2": {
                        "name": "What do eigenfunctions tell us about relative positions?",
                        "content": "\n\\label{sec:distances}\n\nIn addition to being the analog of sine functions, the eigenvectors of the Laplacian also hold important information about the physics of a system and can reveal distance metrics. This is not surprising as the Laplacian is a fundamental operator in physics and is notably used in Maxwell's equations \\cite{Feynman:1494701} and the heat diffusion \\cite{bronstein_geometric_2017}.\n\nIn electromagnetic theory, the (pseudo)inverse of the Laplacian, known in mathematics as the Green's function of the Laplacian \\cite{chung_discrete_2000}, represents the electrostatic potential of a given charge. \nIn a graph, the same concept uses the pseudo-inverse of the Laplacian $\\mG$ and can be computed by its eigenfunctions. \nSee \\eqref{eq:greens_function} , where $\\mG(j_1,j_2)$ is the electric potential between nodes $j_1$ and $j_2$, $\\hat{\\eigvec_i}$ and $\\hat{\\lambda_i}$ are the $i$-th eigenvectors and eigenvalues of the symmetric Laplacian $\\mD^\\frac{-1}{2} \\mL \\mD^\\frac{-1}{2}$, and $\\mD$ is the degree matrix, and $\\hat{\\eigvec}_{i,j}$ the $j$-th row of the vector.\n\\begin{equation}\n\\label{eq:greens_function}\n    \\mG(j_1,j_2) = \n    d_{j_1}^\\frac{1}{2}d_{j_2}^\n    \\frac{-1}{2}\\sum_{i>0}\\frac{(\\hat{\\eigvec}_{i,j_1} \\hat{\\eigvec}_{i,j_2})^2}\n    {\\hat{\\lambda}_i}\n\\end{equation}\n\nFurther, the original solution of the heat equation given by Fourier relied on a sum of sines/cosines known as a Fourier series \\cite{cajori1999history}. As eigenvectors of the Laplacian are the analogue of these functions in graphs, we find similar solutions. Knowing that heat kernels are correlated to random walks \\cite{bronstein_geometric_2017, beaini_directional_2021}, we use the interaction between two heat kernels to define in \\eqref{eq:diffusion_biharmonic_dist} the diffusion distance $d_D$ between nodes $j_1, j_2$ \\cite{bronstein_geometric_2017, COIFMAN20065}. Similarly, the biharmonic distance $d_B$ was proposed as a better measure of distances \\cite{lipman_2010_biharmonic}. Here we use the eigenfunctions of the regular Laplacian $\\mL$. \n\\begin{equation}\n\\label{eq:diffusion_biharmonic_dist}\nd_D^2(j_1,j_2) = \n    \\sum_{k>0}{e^{-2 t \\lambda_i}\n    (\\eigvec_{i,j_1} - \\eigvec_{i,j_2})^2} \n\\quad,\\quad\nd_B^2(j_1, j_2) = \n    \\sum_{i>0}\n    \\frac{(\\eigvec_{i,j_1} - \\eigvec_{i,j_2})^2}\n    {\\lambda_i^2}\n\\end{equation}\n\nThere are a few things to note from these equations. Firstly, they highlight the importance of pairing \\textit{eigenvectors and their corresponding eigenvalues} when supplying information about relative positions in a graph. Secondly, we notice that the product of eigenvectors is proportional to the electrostatic interaction, while the subtraction is proportional to the diffusion and biharmonic distances. Lastly, there is a consistent pattern across all 3 equations: smaller frequencies/eigenvalues are more heavily weighted when determining distances between nodes.\n\n"
                    },
                    "subsubsection 2.1.3": {
                        "name": "Hearing the shape of a graph and its sub-structures",
                        "content": "\n\nAnother well-known property of eigenvalues is how they can be used to discriminate between different graph structures and sub-structures, as they can be interpreted as the frequencies of resonance of the graph. This led to the famous question about whether we can hear the shape of a drum from its eigenvalues \\cite{kac_can_1966}, with the same questions also applying to geometric objects \\cite{cosmo_isospectralization_2019} and 3D molecules \\cite{schrier_can_2020}. Various success was found with the eigenfunctions being used for partial functional correspondence \\cite{rodola_partial_2015}, algorithmic understanding geometries \\cite{levy_laplace_beltrami_2006}, and style correspondence \\cite{cosmo_isospectralization_2019}. Examples of eigenvectors for molecular graphs are presented in Figure \\ref{fig:eigvec-examples}.\n\n\n\n\n"
                    }
                },
                "subsection 2.2": {
                    "name": "etiquette",
                    "content": "\n\\label{sec:eigfunc-etiquette}\n\nIn Euclidean space and sequences, using sinusoids as PEs is trivial: we can simply select a set of frequencies, compute the sinusoids, and add or concatenate them to the input embeddings, as is done in the original Transformer \\cite{vaswani_2017_attention}.\nHowever, in arbitrary graphs, reproducing these steps is not as simple since each graph has a unique set of eigenfunctions.\nIn the following section, we present key principles from spectral graph theory to consider when constructing PEs for graphs, most of which have been overlooked by prior methods. They include normalization, the importance of the eigenvalues and their multiplicities, the number of eigenvectors being variable, and sign ambiguities. Our LPE architectures, presented in section \\ref{sec:architecture}, aim to address them.\n\n\\xhdr{Normalization}\nGiven an eigenvalue of the Laplacian, there is an associated eigenspace of dimension greater than 1. To make use of this information in our model, a single eigenvector has to be chosen.\nIn our work, we use the $L_2$ normalization since it is compatible with the definition of the Green's function (\\ref{eq:greens_function}). Thus, we will always chose eigenvectors $\\eigvec$ such that $\\langle \\eigvec, \\eigvec \\rangle = 1$. \n\n\\xhdr{Eigenvalues}\nAnother fundamental aspect is that the eigenvalue associated with each eigenvector supplies valuable information. An ordering of the eigenvectors based on their eigenvalue works in sequences since the frequencies are pre-determined. However, this assumption does not work in graphs since the eigenvalues in their spectrum can vary. \nFor example, in Figure \\ref{fig:eigvec-examples}, we observe how an ordering would miss the fact that both molecules resonate at $\\lambda=1$ in different ways.\n\n\n\\xhdr{Multiplicities}\nAnother important problem with choosing eigenfunctions is the possibility of a high multiplicity of the eigenvalues, i.e. when an eigenvalue appears as a root of the characteristic polynomial more than once. \nIn this case, the associated eigenspace may have dimension 2 or more as we can generate a valid eigenvector from any linear combination of eigenvectors with the same eigenvalue. This further complicates the problem of choosing eigenvectors for algorithmic computations and highlights the importance of having a model that can handle this ambiguity.\n\n\n\\xhdr{Variable number of eigenvectors}\nA graph $G_i$ can have at most $N_i$ linearly independent eigenvectors with $N_i$ being its number of nodes. Most importantly, $N_i$ can vary across all $G_i$ in the dataset. Prior work \\cite{dwivedi2020generalization} elected to select a fixed number $k$ eigenvectors for each graph, where $k\\leq N_i,  \\forall i$. This produces a major bottleneck when the smallest graphs have significantly fewer nodes than the largest graphs in the dataset since a very small proportion of eigenvectors will be used for large graphs. This inevitably causes loss of information and motivates the need for a model which constructs fixed PEs of dimension $k$, where $k$ does not depend on the number of eigenvectors in the graph.\n\n\n\n\\xhdr{Sign invariance}\nAs noted earlier, there is a sign ambiguity with the eigenvectors. With the sign of $\\eigvec$ being independent of its normalization, we are left with a total of $2^k$ possible combination of signs when choosing $k$ eigenvectors of a graph. Previous work has proposed to do data augmentation by randomly flipping the sign of the eigenvectors \\cite{beaini_directional_2021, dwivedi2020benchmarking, dwivedi2020generalization}, and\nalthough it can work when $k$ is small, it becomes intractable for large $k$.\n\n"
                },
                "subsection 2.3": {
                    "name": "Learning with Eigenfunctions",
                    "content": "\n\nLearning generalizable information from eigenfunctions is fundamental to their succesful usage. Here we detail important points that support it is possible to do so if done correctly.\n\n\\xhdr{Similar graphs have similar spectra} Thus, we can expect the network to transfer patterns across graphs through the similarity of their spectra. In fact, spectral graph theory tells us that the lowest and largest non-zero eigenvalues are both linked to the geometry of the graph (algebraic connectivity and spectral radius).\n\n\\xhdr{Eigenspaces contain geometric information} Spectral graph theory has studied the geometric and physical properties of graphs from their Laplacian eigenfunctions in depth. Developing a method that can use the full spectrum of a graph makes it theoretically possible capture this information. It us thus important to capture differences between the full eigenspaces instead of minor differences between specific eigenvalues or eigenvectors from graph to graph. \n\n\\xhdr{Learned positions are relative within graphs} Eigenspaces are used to understand the relationship between nodes within graphs, not across them. Proposed models should therefore only compare the eigenfunctions of nodes within graphs.\n\n\n\n\n"
                },
                "category": [
                    "method"
                ]
            },
            "section 3": {
                "name": "Model Architecture",
                "content": "\n\\label{sec:architecture}\n\nIn this section, we propose an elegant architecture that can use the eigenfunctions as PEs while addressing the concerns raised in section \\ref{sec:eigfunc-etiquette}. Our \\textit{Spectral Attention Network} (SAN) model inputs eigenfunctions of a graph and projects them into a learned positional encoding (LPE) of fixed size. The LPE allows the network to use up to the entire Laplace spectrum of each graph, learn how the frequencies interact, and decide which are most important for the given task.\n\nWe propose a two-step learning process summarized earlier in Figure \\ref{fig:full-method}. The first step, depicted by blocks (c-d-e) in the figure, applies a Transformer over the eigenfunctions of each node to generate an LPE matrix for each graph. The LPE is then concatenated to the node embeddings (blocks g-h), before being passed to the Graph Transformer (block i). If the task involves graph classification or regression, the final node embeddings are subsequently passed to a final pooling layer.\n\n\n\n\n",
                "subsection 3.1": {
                    "name": "LPE Transformer Over Nodes",
                    "content": "\n\n\nUsing Laplace encodings as node features is ubiquitous in the literature concerning the topic. Here, we propose a method for learning node PEs motivated by the principles from section \\ref{sec:eigfunc-etiquette}. \nThe idea of our LPE is inspired by Figure \\ref{fig:eigvec-matrix}, where the eigenvectors $\\eigvec$ are represented as a non-uniform sequence with the eigenvalue $\\lambda$ being the position on the frequency axis. With this representation, Transformers are a natural choice for processing them and generating a fixed-size PE.\n\n\nThe proposed LPE architecture is presented in Figure \\ref{fig:lpe-transformer-nodes}. First, we create an embedding matrix of size $2 \\times m$ for each node $j$ by concatenating the $m$-lowest eigenvalues with their associated eigenvectors. Here, $m$ is a hyper-parameter for the maximum number of eigenvectors to compute and is analog to the variable-length sequence for a standard Transformer. For graphs where $m > N$, a masked-padding is simply added. Note that to capture the entire spectrum of all graphs, one can simply select $m$ such that it is equal to the maximum number of nodes a graph has in the dataset. A linear layer is then applied on the dimension of size $2$ to generate new embeddings of size $k$. A Transformer Encoder then computes self-attention on the sequence of length $m$ and hidden dimension $k$. Finally, a sum pooling reduces the sequence into a fixed $k$-dimensional node embedding.\n\nThe LPE model addresses key limitations of previous graph Transformers and is aligned with the first four \\textit{etiquettes} presented in section \\ref{sec:eigfunc-etiquette}. \nBy concatenating the eigenvalues with the normalized eigenvector, this model directly addresses the first three \\textit{etiquettes}. Namely, it \\textbf{normalizes} the eigenvectors, pairs eigenvectors with their \\textbf{eigenvalues} and treats  \\textbf{the number of eigenvectors as a variable}. Furthermore, the model is aware of \\textbf{multiplicities} and has the potential to linearly combine or ignore some of the repeated eigenvalues.\n\nHowever, this method still does not address the limitation that the sign of the pre-computed eigenvectors is arbitrary. To combat this issue, we randomly flip the sign of the pre-computed eigenvectors during training as employed by previous work \\cite{dwivedi2020benchmarking, dwivedi2020generalization}, to promote invariance to the sign ambiguity.\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "LPE Transformer Over Edges",
                    "content": "\n\\label{sec:lpe-edges}\n\nHere we present an alternative formulation for Laplace encodings. This method addresses the same issues as the LPE over nodes, but also resolves the eigenvector sign ambiguity. Instead of encoding \\textit{absolute} positions as node features, the idea is to consider \\textit{relative} positions encoded as edge features.\n\nInspired by the physical interactions introduced in \\ref{eq:greens_function} and \\ref{eq:diffusion_biharmonic_dist}, we can take a pair of nodes $(j_1, j_2)$ and obtain \\textbf{sign-invariant} operators \nusing the absolute subtraction $|\\eigvec_{i,j_1} - \\eigvec_{i,j_2}|$ and the product $\\eigvec_{i,j_1} \\eigvec_{i,j_2}$. \nThese operators acknowledge that the sign of $\\eigvec_{i, j_1}$ at a given node $j_1$ is not important, but that the relative sign between nodes $j_1$ and $j_2$ is important. One might argue that we could directly compute the deterministic values from equations (\\ref{eq:greens_function}, \\ref{eq:diffusion_biharmonic_dist}) as edge features instead. However, our goal is to construct models that can learn which frequencies to emphasize and are not biased towards the lower frequencies --- despite lower frequencies being useful in many tasks.\n\n\nThis approach is only presented thoroughly in appendix \\ref{app:lpe-edges}, since it suffers from a major computational bottleneck compared to the LPE over nodes. In fact, for a fully-connected graph, there are $N$ times more edges than nodes, thus the computation complexity is $O(m^2 N^2)$, or $O(N^4)$ considering all eigenfunctions. The same limitation also affects memory and prevents the use of large batch sizes.\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Main Graph Transformer",
                    "content": "\n\n\nOur attention mechanism in the main Transformer is based on previous work \\cite{dwivedi2020generalization}, which attempts to repurpose the original Transformer to graphs by considering the graph structure and improving attention estimates with edge feature embeddings.\n\nIn the following, note that $\\vh^l_i$ is the $i$-th node's features at the $l$-th layer, and $\\ve_{ij}$ is the edge feature embedding between nodes $i$ and $j$. Our model employs multi-head attention over all nodes:\n\\begin{equation}\n\\label{eq:multi-head}\n    \\hat{\\vh}_i^{l+1}= \\mO_h^l \\bigparallel_{k=1}^H (\\sum_{j \\in V} w_{ij}^{k,l}\\mV^{k,l}\\vh_{j}^l)\n\\end{equation} \n\nwhere $\\mO^l_h \\in \\mathbb{R}^{d \\times d}$, $\\mV^{k,l} \\in \\mathbb{R}^{d_k \\times d}$, $H$ denotes the number of heads, $L$ the number of layers, and $\\bigparallel$ concatenation. Note that $d$ is the hidden dimension, while $d_k$ is the dimension of a head ($\\frac{d}{H}=d_k$).\n\nA key addition from our work is the design of an architecture that performs full-graph attention while preserving local connectivity with edge features via two sets of attention mechanisms: one for nodes connected by real edges in the sparse graph and one for nodes connected by added edges in the fully-connected graph. The attention weights $w_{ij}^{k,l}$ in \\eqref{eq:multi-head} at layer $l$ and head $k$ are given by:\n\\begin{equation}\n        \\label{eq:attention}\n        \\hat{\\vw}_{ij}^{k,l} = \\left\\{\\begin{array}{lr}\n        \\frac{\\mQ^{1,k,l}\\vh_i^l \\circ \\mK^{1,k,l} \\vh_j^l\\circ \\mE^{1,k,l}\\ve_{ij}}{\\sqrt{d_k}} & \\text{if $i$ and $j$ are connected in sparse graph}\\\\\n        \\\\\n        \\frac{\\mQ^{2,k,l}\\vh_i^l \\circ \\mK^{2,k,l}\\vh_j^l\\circ \\mE^{2,k,l}\\ve_{ij}}{\\sqrt{d_k}} & \\text{otherwise }\\\\\n        \\end{array}\\right\\}\n\\end{equation}\n%\n%\n\\begin{equation}\n    \\label{eq:gamma}\n    {w}_{ij}^{k,l} = \\left\\{\\begin{array}{lr}\n        \\frac{1}{1+\\gamma}\\cdot\\text{softmax}(\\sum_{d_k}\\hat{\\vw}_{ij}^{k,l}) & \\text{if $i$ and $j$ are connected in sparse graph}\\\\\n        \\\\\n        \\frac{\\gamma}{1+\\gamma}\\cdot\\text{softmax}(\\sum_{d_k}\\hat{\\vw}_{ij}^{k,l})& \\text{otherwise }\\\\\n        \\end{array}\\right\\}\n\\end{equation}\n\n\nwhere $\\circ$ denotes element-wise multiplication and $\\mQ^{1,k,l}$, $\\mQ^{2,k,l}$, $\\mK^{1,k,l}$, $\\mK^{2,k,l}$, $\\mE^{1,k,l}$, $\\mE^{2,k,l}\\in \\mathbb{R}^{d_k \\times d}$. $\\gamma \\in \\mathbb{R^+}$ is a hyperparameter which tunes the amount of bias towards full-graph attention, allowing flexibility of the model to different datasets and tasks where the necessity to capture long-range dependencies may vary. Note that softmax outputs are clamped between $-5$ and $5$ for numerical stability and that the keys, queries and edge projections are different for pairs of connected nodes ($\\mQ^1, \\mK^1, \\mE^1$) and disconnected nodes ($\\mQ^2, \\mK^2, \\mE^2$).\n\nA multi-layer perceptron (MLP) with residual connections and normalization layers are then applied to update representations, in the same fashion as the GT method \\cite{dwivedi2020generalization}.\n\\vspace{-4pt}\n\\begin{equation}\n    \\hat{\\hat{\\vh}}^{l+1} = \\textnormal{Norm}(\\vh_i^l + \\hat{\\vh}_i^{l+1} ),\n    \\quad\n    \\hat{\\hat{\\hat{\\vh}}}_i^{l+1}=\\mW_2^l\\textnormal{ReLU}(\\mW_1^l\\hat{\\hat{\\vh}}_i^{l+1}),\n    \\quad\n    \\vh_i^{l+1} = \\textnormal{Norm}(   \\hat{\\hat{\\vh}}^{l+1} +  \\hat{\\hat{\\hat{\\vh}}}_i^{l+1})\n\\end{equation}\n\nwith the weight matrices $\\mW_1^l \\in \\mathbb{R}^{2d \\times d}$, $\\mW_2^l \\in \\mathbb{R}^{d \\times 2d}$. Edge representations are not updated as it adds complexity with little to no performance gain. Bias terms are omitted for presentation.\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Limitations",
                    "content": "\n\\label{sec:limitations}\n\nThe first limitation of the node-wise LPE, and noted in Table \\ref{tab:comparison-theory} is the lack of sign invariance of the model. A random sign-flip of an eigenvector can produce different outputs for the LPE, meaning that the model needs to learn a representation invariant to these flips. We resolve this issue with the edge-wise LPE proposed in \\ref{sec:lpe-edges}, but it comes at a computational cost.\n\nAnother limitation of the approach is the computational complexity of the LPE being $O(m^2 N)$, or $O(N^3)$ if considering all eigenfunctions. Further, as nodes are batched in the LPE, the total memory on the GPU will be \\textit{num\\_params * num\\_nodes\\_in\\_batch} instead of \\textit{num\\_params * batch\\_size}. Although this is limiting, the LPE is not parameter hungry, with $k$ usually kept around 16. Most of the model's parameters are in the \\textit{Main Graph Transformer} of complexity $O(N^2)$.\n\nDespite Transformers having increased complexity, they managed to revolutionalize the NLP community. We argue that to shift away from the message-passing paradigm and generalize Transformers to graphs, it is natural to expect higher computational complexities. This is exacerbated by sequences being much simpler to understand than graphs due to their linear structure. Future work could overcome this by using variations of Transformers that scale linearly or logarithmically \\cite{tay_efficient_2020}.\n\n\n"
                },
                "subsection 3.5": {
                    "name": "Theoretical properties of the architecture",
                    "content": "\n\nDue to the full connectivity, it is trivial that our model does not suffer from the same limitations in expressivity as its convolutional/message-passing counterpart. \n\n\\xhdr{WL test and universality} The DGN paper \\cite{beaini_directional_2021} showed that using the eigenvector $\\eigvec_1$ is enough to distinguish some non-isomorphic graphs indistinguishable by the 1-WL test. \n\nGiven that our model uses the full set of eigenfunctions, and given enough parameters, our model can distinguish any pair of non-isomorphic graphs and is more powerful than any WL test in that regard.\nHowever, this does not solve the graph isomorphism problem in polynomial time; it only approximates a solution, and the number of parameters required is unknown and possibly non-polynomial.\nIn appendix \\ref{app:full-expressivity-analysis}, we present a proof of our statement, and discuss why the WL test is not well suited to study the expressivity of graph Transformers due to their universality.\n\n\\xhdr{Reduced over-squashing} Over-squashing represents the difficulty of a graph neural network to pass information to distant neighbours due to the exponential blow-up in computational paths \\cite{alon_bottleneck_2020}. \n\nFor the fully-connected network, it is trivial to see that over-squashing is non-existent since there are direct paths between distant nodes.\n\n\\xhdr{Physical interactions} Another point to consider is the ability of the network to learn physical interactions between nodes. This is especially important when the graph models physical, chemical, or biological structures, but can also help understanding pixel interaction in images \\cite{beaini_deep_2020,\nbeaini_improving_2020}. Here, we argue that our SAN model, which uses the Laplace spectrum more effectively, can learn to mimic the physical interactions presented in section \\ref{sec:distances}.\n% the laws of electrostatic potential from (\\ref{eq:greens_function}), the heat diffusion interacton, and the biharmonic interaction from (\\ref{eq:diffusion_biharmonic_dist}) since it has access to the full set of eigenfunctions. \nThis contrasts with the convolutional approach that requires deep layers for the receptive field to capture long-distance interactions. It also contrasts with the GT model \\cite{dwivedi2020generalization}, which does not use eigenvalues or enough eigenfunctions to properly model physical interactions in early layers. \nHowever, due to the lack of sign-invariance in the proposed node-wise LPE, it is difficult to learn these interactions accurately. The edge-wise LPE (section \\ref{sec:lpe-edges}) could be better suited for the problem, but it suffers from higher computational complexity.\n\n\n\n"
                },
                "category": [
                    "method"
                ]
            },
            "section 4": {
                "name": "Experimental Results",
                "content": "\n\\label{sec:experimental-results}\n\n\nThe model is implemented in PyTorch \\cite{paszke2017pytorch} and DGL \\cite{wang2019dgl} and tested on established benchmarks from \\cite{dwivedi2020benchmarking} and \\cite{hu2020open} provided under MIT license. Specifically, we applied our method on ZINC, PATTERN, CLUSTER, MolHIV and MolPCBA, while following their respective training protocols with minor changes, as detailed in the appendix \\ref{app:benchmarks}. The computation time and hardware is provided in appendix \\ref{app:computation-details}.\n\nWe first conducted an ablation study to fairly compare the benefits of using full attention and/or the node LPE. We then took the best-performing model, tuned some of its hyperparameters, and matched it up against the current state-of-the-art methods. Since we use a similar attention mechanism, our code was developed on top of the code from the GT paper \\cite{dwivedi2020generalization}, provided under the MIT license.\n\n\n\n",
                "subsection 4.1": {
                    "name": "Sparse vs. Full Attention",
                    "content": "\n\n\n\n\n\n\n\n\n\n\nTo study the effect of incorporating full attention, we present an ablation study of the $\\gamma$ parameter in Figure \\ref{fig:boxplots}. We remind readers that $\\gamma$ is used in equation \\ref{eq:gamma} to balance between \\textit{sparse} and \\textit{full} attention. Setting $\\gamma=0$ strictly enables sparse attention, while $\\gamma=1$ does not bias the model in any direction.\n\nIt is apparent that molecular datasets, namely ZINC and MOLHIV, benefit less from full attention, with the best parameter being $\\log\\gamma \\in (-7, -5)$. On the other hand, the larger SBM datasets (PATTERN and CLUSTER) benefit from a higher $\\gamma$ value. This can be explained by the fact that molecular graphs rely more on understanding local structures such as the presence of rings and specific bonds, especially in the artificial task from ZINC which relies on counting these specific patterns \\cite{dwivedi2020benchmarking}. Furthermore, molecules are generally smaller than SBMs. As a result, we would expect less need for full attention, as information between distant nodes can be propagated with few iterations of even sparse attention. We also expect molecules to have fewer multiplicities, thus reducing the space of eigenvectors. Lastly, the performance gains in using full attention on the CLUSTER dataset can be attributed to it being a semi-supervised task, where some nodes within each graph are assigned their true labels. With full attention, every node receives information from the labeled nodes at each iteration, reinforcing confidence about the community they belong to. \n\nIn Figure \\ref{fig:ablation}, we present another ablation study to measure the impact of the node LPE in both the \\textit{sparse} and \\textit{full} architectures. We observe that the proposed node-wise LPE contributes significantly to the performance for molecular tasks (ZINC and MOLHIV), and believe that it can be attributed to the detection of substructures (see Figure \\ref{fig:eigvec-examples}). For PATTERN and CLUSTER, the improvement is modest as the tasks are simple clustering \\cite{dwivedi2020benchmarking}. \nPrevious work even found that the optimal number of eigenvectors to construct PE for PATTERN is only 2 \\cite{dwivedi2020generalization}.\n\n%To compare the LPE to the simple concatenation of eigenvectors to node features, one can refer to results from the sparse GT model \\cite{dwivedi2020generalization} presented in Figure \\ref{fig:results}.\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Comparison to the state-of-the-art",
                    "content": "\n\nWhen comparing to the state-of-the-art (SOTA) models in the literature in Figure \\ref{fig:results}, we observe that our SAN model consistently performs better on all synthetic datasets from \\cite{dwivedi2020benchmarking}, highlighting the strong expressive power of the model. On the MolHIV dataset, the performance on the test set is slightly lower than the SOTA. However, the model performs better on the validation set (85.30\\%) in comparison to PNA (84.25\\%) and DGN (84.70\\%). This can be attributed to a well-known issue with this dataset: the validation and test metrics have low correlation. In our experiments, we found higher test results with lower validation scores when restricting the number of epochs. Here, we also included results on the MolPCBA dataset, where we witnessed competitive results as well.\n\nOther top-performing models, namely PNA \\cite{corso2020principal} and DGN \\cite{beaini_directional_2021}, use a message-passing approach \\cite{gilmer2017mpnn} with multiple aggregators. When compared to attention-based models, SAN consistently outperforms the SOTA by a wide margin. To the best of our knowledge, SAN is the first fully-connected model to perform well on graph tasks, as is evident by the poor performance of the \\textit{GT (full)} model.\n\n\n\n\n\n\n\n\n"
                },
                "category": [
                    "experiments",
                    "results"
                ]
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\nIn summary, we presented the SAN model for graph neural networks, a new Transformer-based architecture that is aware of the Laplace spectrum of a given graph from the learned positional encodings.\nThe model was shown to perform on par or better than the SOTA on multiple benchmarks and outperforms other Attention-based models by a large margin.\nAs is often the case with Transformers, the current model suffers from a computational bottleneck, and we leave it for future work to implement variations of Transformers that scale linearly or logarithmically. This will enable the edge-wise LPE presented in appendix \\ref{app:lpe-edges}, a theoretically more powerful version of the SAN model.\n\n\\xhdr{Societal Impact} The presented work is focused on theoretical and methodological improvements to graph neural networks, so there are limited direct societal impacts. However, indirect negative impacts could be caused by malicious applications developed using the algorithm. One such example is the tracking of people on social media by representing their interaction as graphs, thus predicting and influencing their behavior towards an external goal. It also has an environmental impact due to the greater energy use that arises from the computational cost $O(m^2 N + N^2)$ being larger than standard message passing or convolutional approaches of $O(E)$.\n\n\\xhdr{Funding Disclosure}\n\nDevin Kreuzer is supported by an NSERC grant.\n\n\\bibliography{citations}\n\\bibliographystyle{plain}\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%\\section*{Checklist}\n\n%\\definecolor{blue(pigment)}{rgb}{0.2, 0.2, 0.6}\n\n%\\begin{enumerate}\n\n%\\item For all authors...\n%\\begin{enumerate}\n%  \\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n%    \\textcolor{blue(pigment)}{Yes}\n%  \\item Did you describe the limitations of your work?\n%    \\textcolor{blue(pigment)}{Yes, mainly in section \\ref{sec:limitations}.}\n%  \\item Did you discuss any potential negative societal impacts of your work?\n%    \\textcolor{blue(pigment)}{Yes, in the conclusion.}\n%  \\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\n%    \\textcolor{blue(pigment)}{Yes.}\n%\\end{enumerate}\n\n%\\item If you are including theoretical results...\n%\\begin{enumerate}\n%  \\item Did you state the full set of assumptions of all theoretical results?\n%    \\textcolor{blue(pigment)}{Yes, assumptions are provided.}\n%\t\\item Did you include complete proofs of all %theoretical results?\n%    \\textcolor{blue(pigment)}{Yes, proofs are provided %in Appendix \\ref{app:full-expressivity-analysis}.}\n%\\end{enumerate}\n%\n%\\item If you ran experiments...\n%\\begin{enumerate}\n%  \\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\n%    \\textcolor{blue(pigment)}{Yes. The URL is included in the final sentence of the Introduction, with the link repeated here \\url{https://anonymous.4open.science/r/SAN-5C8C}.}\n%  \\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\n%    \\textcolor{blue(pigment)}{Yes. All hyper-parameters are specified Appendix \\ref{app:ablation} and \\ref{app:sota-comparison}. The splits are provided by the public benchmarks \\cite{dwivedi2020benchmarking, hu2020open}.}\n%\t\\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\n%    \\textcolor{blue(pigment)}{Yes. Each experiment was run 4 or 10 times, with reported mean and standard deviation.}\n%\t\\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\n%    \\textcolor{blue(pigment)}{Yes, provided in appendix \\ref{app:computation-details}.}\n%\\end{enumerate}\n\n%\\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n%\\begin{enumerate}\n%  \\item If your work uses existing assets, did you cite the creators?\n%    \\textcolor{blue(pigment)}{Yes. See section \\ref{sec:experimental-results}. The reference for the datasets are \\cite{dwivedi2020benchmarking, hu2020open}, for the main frameworks are \\cite{paszke2017pytorch, wang2019dgl}, and for the MIT-licensed code we used \\cite{dwivedi2020generalization}.}\n%  \\item Did you mention the license of the assets?\n%    \\textcolor{blue(pigment)}{Yes. All under MIT license.}\n%  \\item Did you include any new assets either in the supplemental material or as a URL?\n%    \\textcolor{blue(pigment)}{Not applicable.}\n%  \\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\n%    \\textcolor{blue(pigment)}{Yes. The datasets are provided publicly, and cited appropriately \\cite{dwivedi2020benchmarking, hu2020open}.}\n%  \\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\n%    \\textcolor{blue(pigment)}{Not applicable. As presented in the appendix \\ref{app:benchmarks}, the data is either artificially generated, or on a molecular dataset for HIV inhibition.}\n%\\end{enumerate}\n\n%\\item If you used crowdsourcing or conducted research with human subjects...\n%\\begin{enumerate}\n%  \\item Did you include the full text of instructions given to participants and screenshots, if applicable?\n%    \\textcolor{blue(pigment)}{Not applicable.}\n % \\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\n  %  \\textcolor{blue(pigment)}{Not applicable.}\n  %\\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\n  %  \\textcolor{blue(pigment)}{Not applicable.}\n%\\end{enumerate}\n\n%\\end{enumerate}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\n\\clearpage\n\\newpage\n\n\\appendix\n\n",
                "category": [
                    "conclusion"
                ]
            },
            "section 6": {
                "name": "LPE Transformer Over Edges",
                "content": "\n\\label{app:lpe-edges}\n\nConsider one of the most fundamental notions in physics; \\textit{Potential energy}. Interestingly, potential energy is always measured as a potential difference; it is not an inherent individual property, such as mass. Strikingly, it is also the \\textit{relative} Laplace embeddings of two nodes that paint the picture, as a node's Laplace embedding on its own reveals no information at all. With this in mind, we argue that Laplace positional encodings are more naturally represented as edge features, which encode a notion of \\textit{relative} position of the two endpoints in the graph. This can be viewed as a distance encoding, which was shown to improve the performance of node and link prediction in GNNs \\cite{li_distance_2020}.\n\n\nThe formulation is very similar to the method for learning positional node embeddings. Here, a Transformer Encoder is applied on each graph by treating edges as a batch of variable size and eigenvectors as a variable sequence length. We again compute up to the $m$-lowest eigenvectors with their eigenvalues but, instead of directly using the eigenvector elements, we compute the following vectors:\n\n\\noindent\\begin{minipage}{.5\\linewidth}\n\\begin{equation}\n  %\\gamma_{:,j_1, j_2} =\n  |\\boldsymbol{\\eigvec_{:, j_1}} - \\boldsymbol{\\eigvec_{:, j_2}}|\n\\end{equation}\n\\end{minipage}%\n\\begin{minipage}{.5\\linewidth}\n\\begin{equation}\n  %\\sigma_{:,j_1, j_2} =\n  \\boldsymbol{\\eigvec_{:, j_1}} \\circ \\boldsymbol{\\eigvec_{:, j_2}}\n\\end{equation}\n\\end{minipage}\n\n%Here, $\\boldsymbol{\\gamma_{:, j_1, j_2}}$ and $\\boldsymbol{\\sigma_{:, j_1, j_2}}$ represent the embedding vectors of the edge between nodes $j_1$ and $j_2$.\nwhere ``$:$'' denotes along all up to $m$ eigenvectors, and $\\circ$ denotes element-wise multiplication. Note that these new vectors are completely invariant to sign permutations of the precomputed eigenvectors.\n\nAs per the LPE over nodes,\nthe 3-length vectors are expanded with a linear layer to generate embeddings of size $k$ before being input to the Transformer Encoder.\nThe final embeddings are then passed to a sum pooling layer to generate fixed-size edge positional encodings, which are then used to compute attention weights in equation \\ref{eq:attention}.\n\nThis method addresses \\textbf{all etiquettes} raised in section \\ref{sec:eigfunc-etiquette}. However, it suffers from a major computational bottleneck compared to the LPE over nodes. Indeed, for a fully-connected graph, there are $N$ times more edges than nodes, thus the computation complexity is $O(m^2 N^2)$, or $O(N^4)$ considering all eigenfunctions. This same limitation also affects the memory, as efficiently batching the $N^2$ edges will increase the memory consumption of the LPE by a drastic amount, preventing the model from using large batch sizes and making it difficult to train.\n\n\n\n\n\n\n",
                "category": [
                    "method"
                ]
            },
            "section 7": {
                "name": "Appendix - Implementation details",
                "content": "\n\n",
                "subsection 7.1": {
                    "name": "Benchmarks and datasets",
                    "content": "\n\\label{app:benchmarks}\n\nTo test our models' performance, we rely on standard benchmarks proposed by \\cite{dwivedi2020benchmarking} and \\cite{hu2020open} and provided under the MIT license. In particular, we chose ZINC, PATTERN, CLUSTER, and MolHIV.\n\n\n\\xhdr{ZINC \\cite{dwivedi2020benchmarking}} \nA synthetic molecular graph regression dataset, where the predicted score is given by the subtraction of computationally estimated properties $logP-SA$. Here, $logP$ is the computed octanol-water partition coefficient, and $SA$ is the synthetic accessibility score \\cite{jin_junction_2018}.\n    \n\\xhdr{CLUSTER \\cite{dwivedi2020benchmarking}}\nA synthetic benchmark for node classification. The graphs are generated with Stochastic Block Models, a type of graph used to model communities in social networks. In total, 6 communities are generated and each community has a single node with its true label assigned. The task is to classify which nodes belong to the same community.\n\n\\xhdr{PATTERN \\cite{dwivedi2020benchmarking}} \nA synthetic benchmark for node classification. The graphs are generated with Stochastic Block Models, a type of graph used to model communities in social networks. The task is to classify the nodes into 2 communities, testing the GNNs ability to recognize predetermined subgraphs.\n    \n\\xhdr{MolHIV \\cite{hu2020open}} \nA real-world molecular graph classification benchmark. The task is to predict whether a molecule inhibits HIV replication or not. The molecules in the training, validation, and test sets are divided using a scaffold splitting procedure that splits the molecules based on their two-dimensional structural frameworks. The dataset is heavily imbalanced towards negative samples. It is also known that this dataset suffers from a strong de-correlation between validation and test set performance, meaning that more hyperparameter fine-tuning on the validation set often leads to lower test set results.\n\n\\xhdr{MolPCBA \\cite{hu2020open}} \nAnother real-world molecular graph classification benchmark. The dataset is larger than MolHIV and applies a similar scaffold spliting procedure. It consists of multiple, extremely skewed (only 1.4\\% positivity) molecular classification tasks, and employs Average Precision (AP) over them as a metric.\n    \n"
                },
                "subsection 7.2": {
                    "name": "Ablation studies",
                    "content": "\n\\label{app:ablation}\n\nThe results in Figures \\ref{fig:boxplots}-\\ref{fig:ablation} are done as an ablation study with a minimal tuning of the hyperparameters of the network to measure the impact of the node LPE and full attention. A majority of the hyperparameters used were tuned in previous work \\cite{dwivedi2020benchmarking}. However, we altered some of the existing parameters to accommodate the parameter-heavy LPE, and modified the Main Graph Transformer hidden dimension such that all models have approximately $\\thicksim 500k$ parameters for a fair comparison. We present results for the full attention with the optimal $\\gamma$ value optimized on the Node LPE model. We did this to isolate the impact that the Node LPE has on improving full attention. Details concerning the model architecture parameters are visible in Figure \\ref{ablation_params}.\n\n\nFor the training parameters, we employed an Adam optimizer with a learning rate decay strategy initialized in $\\{10^{-3}, 10^{-4}\\}$as per \\cite{dwivedi2020benchmarking}, with some minor modifications:\n\n\\xhdr{ZINC \\cite{dwivedi2020benchmarking}} We selected an \\textit{initial learning rate} of $7\\times 10^{-4}$ and increased the \\textit{patience} from 10 to 25 to ensure convergence.\n\\xhdr{PATTERN \\cite{dwivedi2020benchmarking}} We selected an \\textit{initial learning rate} of $5\\times 10^{-4}$.\n\\xhdr{CLUSTER \\cite{dwivedi2020benchmarking}} We selected an \\textit{initial learning rate} of $5\\times 10^{-4}$ and reduced the \\textit{minimum learning rate} from $10^{-6}$ to $10^{-5}$ to speed up training time.\n\\xhdr{MolHIV \\cite{hu2020open}} We elected to use similar training procedures for consistency. We selected an \\textit{initial learning rate} of $10^{-4}$, a \\textit{reduce factor} of $0.5$, a \\textit{patience} of $20$, a \\textit{minimum learning rate} of $10^{-5}$, a \\textit{weight decay} of $0$ and a \\textit{dropout} of $0.03$.\n\n"
                },
                "subsection 7.3": {
                    "name": "SOTA Comparison study",
                    "content": "\n\\label{app:sota-comparison}\n\nFor the results in Figure \\ref{fig:results}, we tuned some of the hyperparameters, using the following strategies. The optimal parameters are in \\textbf{bold}.\n\n\\xhdr{ZINC}\nDue to the $500k$ parameter budget, we tuned the pairing \\{\\textit{GT layers}, \\textit{GT hidden dimension}\\} $\\in \\{\\{6,72\\}, \\{8,64\\}, \\textbf{\\{10,56\\}}\\}$ and \\textit{readout} $\\in \\{\\textnormal{\"mean\"}, \\textbf{\"sum\"}\\}$\n\\xhdr{PATTERN}\nDue to the $500k$ parameter budget and long training times, we only tuned the pairing \\{\\textit{GT layers}, \\textit{GT hidden dimension}\\} $\\in \\{\\textbf{\\{4,80\\}}, \\{6,64\\}\\}$\n\\xhdr{CLUSTER}\nDue to the $500k$ parameter budget and long training times, we only tuned the pairing \\{\\textit{GT layers}, \\textit{GT hidden dimension}\\} $\\in \\{\\{12,64\\}, \\textbf{\\{16,48\\}}\\}$\n\\xhdr{MolHIV}\nWith no parameter budget, we elected to do a more extensive parameter tuning in a two-step process while measuring validation metrics on 3 runs with identical seeds.\n\n\\begin{enumerate}\n    \\item We tuned \\textit{LPE dimension} $\\in\\{8,\\textbf{16}\\}$,  \\textit{GT layers} $\\in\\{4,6,8,\\textbf{10}\\}$,  \\textit{GT hidden dimension} $\\in\\{48, \\textbf{64}, 72, 80, 96\\}$\n    \\item With the highest performing validation model from step 1, we then tuned \\textit{dropout} $\\in\\{0, \\textbf{0.01}, 0.025\\}$ and \\textit{weight decay} $\\in\\{\\textbf{0}, 10^{-6}, 10^{-5}\\}$\n\\end{enumerate}\n\nWith the final optimized parameters, we reran 10 experiments with identical seeds.\n\n\\xhdr{MolPCBA}\nWith no parameter budget, we elected to do a more extensive parameter tuning as well. We tuned $\\textit{learning rate} \\in \\{0.0001, \\textbf{0.0003}, 0.0005\\}$, $\\textit{dropout} \\in \\{0, 0.1, 0.2, 0.3, 0.4, \\textbf{0.5}\\}$, \\textit{GT layers} $\\in \\{2,4,\\textbf{5},6,8,10,12\\}$, \\textit{GT layers} $\\in \\{128, 256, \\textbf{304}, 512\\}$, \\textit{LPE layers} $\\in \\{8, \\textbf{10}, 12\\}$ amd \\textit{LPE dimension} $\\in \\{8, \\textbf{16}\\}$\n\n"
                },
                "subsection 7.4": {
                    "name": "Computation details",
                    "content": "\n\\label{app:computation-details}\n\n\n\n\n\n"
                },
                "category": [
                    "experiments"
                ]
            },
            "section 8": {
                "name": "Expressivity and complexity analysis of graph Transformers",
                "content": "\n\\label{app:full-expressivity-analysis}\n\nIn this section, we discuss how the universality of Transformers translates to graphs when using different node identifiers. Theoretically, this means that by simply labeling each node, Transformers can learn to distinguish any graph, and the WL test is no longer suited to study their expressivity.\n\nThus, we introduce the notion of learning complexity to better compare each architecture's ability to understand the space of isomorphic graphs. We apply the complexity analysis to the LPE and show that it can more easily capture the structure of graphs than a naive Transformer.\n\n\n",
                "subsection 8.1": {
                    "name": "Universality of Transformers for sequence-to-sequence approximations",
                    "content": "\n\\label{app:universality-transformers-sequences}\n\nIn recent work \\cite{yun2020transformers} \\cite{yun2020sparsetransformers}, it was proven that Transformers are universal sequence-to-sequence approximators, meaning that they can encode any function that approximately maps any first sequence into a second sequence when given enough parameters. More formally, they proved the following theorems for the universality of Transformers:\n\n\\begin{theorem}\\label{thm:equivariant universality}\nFor any $1\\leq p < \\infty$, $\\varepsilon>0$ and any function $f:\\mR^{d\\times n} \\to \\mR^{d\\times n} $ that is equivariant to permutations of the columns, there is a Transformer $g$ such that the $L^p$ distance between $f$ and $g$ is smaller than $\\varepsilon$. \n\\end{theorem}\n\n\nLet $\\mB^n$ be the $n$-dimensional closed ball and denote by $C^0(\\mB^{d\\times n}, \\mR^{d\\times n})$ the set of all continuous functions of the ball to $\\mR^{d\\times n}$. A Transformer with positional encoding $g_p$ is a Transformer $g$ such that to each input $\\mX$, a fixed learned positional encoding $\\mE$ is added such that $g_p(\\mX) = g(\\mX + \\mE)$.\n\n\n\\begin{theorem}\\label{thm:universality}\nFor any $1\\leq p < \\infty$, $\\varepsilon>0$ and any function $f\\in C^0(\\mB^{d\\times n}, \\mR^{d\\times n}) $, there is a Transformer with positional encoding $g$ such that the $L^p$ distance between $f$ and $g$ is smaller than $\\varepsilon$.\n\\end{theorem}\n\n\n"
                },
                "subsection 8.2": {
                    "name": "Graph Transformers approximate solutions to the graph isomorphism problem",
                    "content": "\n\\label{app:universality-transformers-graphs}\n\nWe now explore the consequences of the previous 2 theorems on the use of Transformers for graph representation learning. We first describe 2 types of Transformers on graphs; one for node and one for edge inputs. They will be used to deduce corollaries of theorems \\ref{thm:equivariant universality} and \\ref{thm:universality} for graph learning and later comparison with our proposed architecture. Assume now that all nodes of the graphs we consider are given an integer label in $\\{1,...,N\\}$.\n\\newline\\newline\nThe naive edge transformer takes as input a graph represented as a sequence of ordered pairs $((i,j),\\sigma_{i,j})$ with $i\\leq j$ the indices of 2 vertices and $\\sigma_{i,j}$ equal to 1 or 0 if the vertices $i,j$ are connected or not. Recall there are $N(N-1)/2$ pairs of integers $i,j$ in $\\{1,...,N\\}$ with $i < j$ the indices of 2 vertices and $\\sigma_{i,j}$ equal to 1 or 0 if the vertices $i,j$ are connected or not. It is obvious that any ordering of these edge vectors describe the same graph. Recall there are $N(N-1)/2$ pairs of integers $i,j$ in $\\{1,...,N\\}$ with $i\\leq j$. Consider the set of functions $f: \\mR^{N (N-1)/2 \\times 2} \\to \\mR^{N (N-1)/2 \\times 2}$ that are equivariant to the permutations of columns then theorem \\ref{thm:equivariant universality} says the function $f$ can be approximated with arbitrary accuracy by Transformers on edge input.\n%taking as input the sequences $((i,j),\\sigma_{i,j})_{i\\leq j \\in \\{1,...,N\\}}$. (the vectors $((i,j),\\sigma_{i,j})$), invariant under permutations of the labelling of the nodes of the graph and distinguish non-isomorphic graphs. Pick one such $f$,\n\\newline\\newline\nThe naive node Transformer can be defined as a Transformer with positional encodings. This graph Transformer will take as input the identity matrix and as positional encodings the padded adjacency matrix. This can be viewed as a one-hot encoding of each node's neighbors. Consider the set of continuous functions $f:\\mR^{N\\times N} \\to \\mR^{N\\times N}$, then theorem \\ref{thm:universality} says the function $f$ can be approximated with arbitrary accuracy by Transformers on node inputs.\n\\newline\\newline\nFrom these two observations on the universality of graph Transformers, we get as a corollary that these 2 types of Transformers can approximate solutions of the graph isomorphism problem. In each case, pick a function that is invariant under node index permutations and maps non-isomorphic graphs to different values and apply theorem \\ref{thm:equivariant universality} or \\ref{thm:universality} that shows there is a Transformer approximating that function to an arbitrarily small error in the $L^p$ distance. This is an interesting fact since it is known that the discrimination power of most message passing graph networks is upper bounded by the Weisfeiler-Lehman test which is unable to distinguish some graphs.\n\n\n\n\\begin{comment}\nHere, we want to prove that given a unique node label, node connectivity, and the right architecture, Transformers can approximate a solution to the graph isomorphism problem. \n\n\nAssume now that all nodes of the graphs we consider are given an integer label in $\\{1,...,N\\}$.\nFrom theorem \\ref{thm:equivariant universality}, we can deduce the following about Transformers on graphs. First, we consider the consequence of these theorems for classification of graph isomorphism classes using a Transformer on the edges. A graph will be represented as a sequence of ordered pairs $((i,j),\\sigma_{i,j})$ with $i\\leq j$ the indices of 2 vertices and $\\sigma_{i,j}$ equal to 1 or 0 if the vertices $i,j$ are connected or not. Recall there are $N(N-1)/2$ pairs of integers $i,j$ in $\\{1,...,N\\}$ with $i\\leq j$. Consider the set of functions $f: \\mR^{N (N-1)/2 \\times 2} \\to \\mR^{N (N-1)/2 \\times 2}$ that are equivariant to the permutations of columns (the vectors $((i,j),\\sigma_{i,j})$), invariant under permutations of the labelling of the nodes of the graph and distinguish non-isomorphic graphs. Pick one such $f$, then theorem \\ref{thm:equivariant universality} says the function $f$ can be approximated with arbitrary accuracy by Transformers taking as input the sequences $((i,j),\\sigma_{i,j})_{i\\leq j \\in \\{1,...,N\\}}$.\n\n\n\nNow we consider the problem of classification of graph isomorphism classes using Transformers on the nodes. Choosing instead as input the sequence of graph nodes with positional encoding the columns of the adjacency matrix and $f: \\mR^{N\\times N} \\to \\mR^{N\\times N}$ a map that is invariant under permutation of node order and has distinct values for different isomorphism class of graphs, theorem \\ref{thm:universality} says that the map $f$ can be approximated by Transformers in the $L^p$ distance with arbitrary accuracy.\n\\end{comment}\n\n\nThis may seem strange since it is unlikely there is an algorithm solving the graph isomorphism problem in polynomial time to the number of nodes $N$, and we address this issue in the notes below.\n\n\\xhdr{Note 1: Only an approximate solution}\nThe universality theorems do not state that Transformers solve the isomorphism problem, but that they can approximate a solution. They only learn the invariant functions only up to some error so they still can mislabel graphs.\n\n%\\xhdr{Note 2: Unknown computation complexity}Also, for the approximation to be precise enough, datasets with larger graphs will require more parameters. In \\cite{yun2020transformers}, it is stated that the universal class of function is obtained by composing Transformer blocks with 2 heads of size 1 followed by a feed-forward layer with 4 hidden nodes, but no specification of the number of stacked Transformer blocks is made. Since the number of parameters necessary to get the desired precision is unknown the proposed algorithm is not necessarily polynomial in the number of graph nodes.\n\n\\xhdr{Note 2: Estimate of number of Transformer blocks} For the approximation of the function $f$ by a Transformer to be precise, a large number of Transformer blocks will be needed. In \\cite{yun2020transformers}, it is stated that the universal class of function is obtained by composing Transformer blocks with 2 heads of size 1 followed by a feed-forward layer with 4 hidden nodes. In  \\cite{yun2020sparsetransformers} section 4.1, an estimate of the number of blocks is given. If $f:\\mR^{d\\times n} \\to \\mR^{d\\times n}$ is $L$-Lipschitz, then $||f(\\mX) - f(\\mY)||< \\varepsilon/2$ when  $||\\mX - \\mY|| < \\varepsilon/2L = \\delta $ . In the notation of \\cite{yun2020sparsetransformers}, the LPE has constants $p=2$ and $s=1$. If $g$ is a composition of Transformer blocks then an error $||f-g||_{L^p}< \\varepsilon $  can be achieved with a number of Transformer blocks larger than\n\\[\n\\left(\\frac{dn}{\\delta} \\right) + \n\\left( \\frac{p(n-1)}{\\delta^d} + s\\right) + \n\\left( \\frac{n}{\\delta^{dn}} \\right) = \\frac{dn2L}{\\varepsilon}+ \\frac{2(n-1)(2L)^d}{\\varepsilon^d} + 1 + \\frac{n(2L)^{dn}}{\\varepsilon^{dn}}\n\\]\nIn the case of the node encoder described above, $n =d= N$ (the number of nodes) and the last term in the sum above becomes $N(2L/\\varepsilon)^{N^2}$, so the number of parameters and therefore the computational time is exponential in the number of nodes for a fixed error $\\varepsilon$. Note that this bound on the number of Transformer blocks might not be tight and might be much lower for a specific problem.\n%\\[ \\left( 2LN^2\\varepsilon^{N^2-1} + 2(2L)^N(N-1)\\varepsilon^{N(N-1)} + N(2L)^{N^2}  +\\varepsilon^{N^2}\\right)/\\varepsilon^{N^2}\\]\n\n\n\\xhdr{Note 3: Learning invariance to label permutations}\nIn the above proof, the Transformer is assumed to be able to label all isomorphic graphs into the same class within a small error. However, given a graph of $N$ nodes, there are $N!$ different node labeling permutations, and they all need to be mapped to the same output class. It seems unlikely that such function can be learned with polynomial complexity to $N$.\n\nFollowing these observations, it does not seem appropriate to compare Transformers to the WL test as is the custom for graph neural networks and we think at this point we should seek a new measure of expressiveness of graph Transformers.\n\n\n"
                },
                "subsection 8.3": {
                    "name": "Expressivity of the node-LPE",
                    "content": "\n\nHere, we want to show that the proposed node-LPE can generate a unique node identifier that allows our Transformer model to be a universal approximator on graphs, thus allowing us to approximate a solution to graph isomorphism.\n\n%\\textcolor{blue}{Need to mention a unique identifier, which is guaranteed by choosing $m \\geq N$ for all $N$ in the dataset.}\n\nRecall the node LPE takes as input an $N\\times m \\times 2$ tensor with $m$ the number of eigenvalues and eigenvectors that are used to represent the nodes. The output is a $N  \\times k$ tensor. Notice that 2 non-isomorphic graphs on $N$ nodes can have the same $m < N$ eigenvalues and eigenspaces and disagree on the last $N-m$ eigenvalues and eigenspaces. Any learning algorithm missing the last $N-m$ pieces of information won't be able to distinguish these graphs. Here we will fix some $m$ and show that the resulting Transformer can approximately classify all graphs with $N \\leq m$.\n\nFix some linear injection $\\mM:\\mR^{N\\times 2\\times m} \\to \\mR^{N\\times k\\times m}$. Let $G$ be a graph and $U \\subset \\mR^{N\\times 2\\times m}$ be bounded set containing all the tensor representations of graphs $\\mT_G$ and let $R$ be the radius of a ball containing $\\mM(U)$. Consider the set $C^0(\\mB_R^{N\\times k\\times m}, \\mR^{N\\times k\\times m}) $ of continuous functions of the closed radius $R$ ball in $\\mR^{N\\times k\\times m}$. \nFinally, denote by $\\mS: \\mR^{N\\times k \\times m} \\to \\mR^{N\\times k }$ the linear function taking the sum of all values in the $m$ dimension.\n%The reason for this specific choice of radius is that it needs to contain the representation of every $N$ vertices graphs as a tensor $\\mT_G \\in \\mR^{N\\times m\\times 2}$ and since we have chosen the vectors $\\eigvec_j$ such that $\\sum_i=1,...,N\\eigvec_j(i)^2 =1 $ and all graph  eigenvalues are smaller than 2 (see \\cite{chung1997spectral} for example), then $|| \\mT_G ||^2 = (\\sum_{i=1,...,N}\\lambda_i + \\sum_{i=1,...,N})\\sum_{j=1,...,N}\\eigvec_i(j)) \\leq 2N + N$.\n\\newtheorem{proposition}{Proposition}\nThe following universality result for LPE Transformers is a direct consequence of theorem \\ref{thm:universality}.\n\n\\begin{proposition}\n\\label{prop:lpe-expressivity}\nFor any $1\\leq p < \\infty$, $\\varepsilon>0$ and any continuous function $F: \\mB_R^{N\\times k\\times m} \\to \\mR^{N\\times k} $, there is an LPE  Transformer $g$ such that the $L^p$ distance between $M\\circ f \\circ S$ and $g$ is smaller than $\\varepsilon$.\n\\end{proposition}\nAs a corollary, we get the same kind of approximation to solutions of the graph isomorphism problem as with the naive Transformers.\nLet $f$ be a function of $C^0(\\mB_R^{N\\times k\\times m}, \\mR^{N\\times k\\times m})$ that maps $M(\\mT_G)$ to a value that is only dependent of the isomorphism class of the graph and assigns different values to different isomorphism classes. We can further assume that $f$ takes values that are 0 for all but one coordinate in the $k$ dimension. The same type of argument is possible for the edge-LPE from figure \\ref{fig:lpe-transformer-edges}.\n\n\n%\\textcolor{blue}{This simply means that the node-LPE can generate a unique node identifier from the set of eigenfunctions and that the edge-LPE can generate a unique edge identifier from the set of eigenfunctions. Then, it naturally follows from appendix \\ref{app:universality-transformers-graphs} that a Transformer appended to either the node-LPE or edge-LPE can use these unique identifiers to approximate a solution to graph isomorphism. }\n\n"
                },
                "subsection 8.4": {
                    "name": "Comparison of the learning complexity of naive graph Transformers and LPE",
                    "content": "\n\nWe now argue that while the LPE Transformer and the naive graph Transformers of section \\ref{app:universality-transformers-graphs} can all approximate a function $f$ solution of the graph isomorphism problem, the complexity of the learning problem of the LPE is much lower since the spaces it has to learn are simpler.\n\n\\xhdr{Naive Node Transformer}\n    First recall that the naive node Transformer learns a map $f:\\mR^{N^2} \\to \\mR^{N^2}$. In this situation, each graph is represented by $N!$ different matrices which all have to be identified by the Transformer. This encoding also does not provide any high-level structural information about the graph.\n\n\\xhdr{Naive Edge Transformer}\n    The naive edge Transformer has the same difficulty since the function its learning is $\\mR^{N(N-1)} \\to \\mR^{N(N-1)}$ and the representation of each edge depend on a choice of labeling of the vertices and the $N!$ possible labelings need to be identified again. \n\n\\xhdr{Node-LPE Transformer}\n    In the absence of eigenvalues with multiplicity $>1$, the node LPE that learns a function $\\mR^{N\\times 2\\times m} \\to \\mR^{N\\times k}$ does not take as input a representation of the graph that depends on the ordering of the nodes. It does, however, depend on the choice of the sign of each of the eigenvectors so there are still $2^N$ possible choices of graph representations that need to be identified by the Transformer but this is a big drop in complexity compared to the previous $N!$. The eigenfunctions also provide high-level structural information about the graph that can simplify the learning task of the graph.\n\n\\xhdr{Edge-LPE Transformer}\n    Finally, the edge LPE of appendix \\ref{app:lpe-edges} uses a graph representation as input that is also independent of the sign choice of the eigenvectors so each graph has a unique representation (considering the absence of eigenvalues with multiplicity $>1$). Again, the eigenfunctions provide high-level structural information that is not available to the naive Transformer.\n\n\\xhdr{LPE Transformers for non-isospectral graphs}\n    Isospectral graphs are graphs that have the same set of eigenvalues despite having different eigenvectors. Here, we argue that the proposed node LPE can approximate a solution to the graph isomorphism problem for all pairs of non-isospectral graphs, without having to learn invariance to the sign of their eigenvectors nor their multiplicities.\n    By considering only the eigenvalues in the initial linear layer (assigning a weight of $0$ to all $\\phi$), and knowing that the eigenvalues are provided as inputs, the model can effectively learn to replicate the input eigenvalues at its output, thus discriminating between all pairs of non-isospectral graphs.\n    Hence, the problem of learning an invariant mapping to the sign of eigenvectors and multiplicities is limited only to non-isospectral graphs. \n    Knowing that the ratio of isospectral graphs decreases as the number of nodes increases (and is believed to tend to 0) \\cite{van_dam_which_2003}, this is especially important for large graphs and mitigates the problem of having to learn to identify $2^N$ with eigenvectors with different signs.\n    In Figure \\ref{fig:isomorphic-eigvals}, we present an example of non-isomorphic graphs that can be distinguished by their eigenvalues but not by the 1-WL test.\n\n\n\n\n%\\textcolor{blue}{Also, for the approximation to be precise enough, datasets with larger graphs will likely require more parameters. With the required increase in parameters being unknown, the proposed algorithm is not necessarily polynomial, especially considering the $2^N$ sign-permutations of the eigenvector that the model needs to learn invariance. The model can distinguish any pair of non-isospectral graphs using only the eigenvalues without learning the permutations, but requires the eigenvectors to distinguish iso-spectral graphs.}\n\n\n\n% \\subsection{Weisfeiler-Lehman (WL) test}\n% \\label{app:proof:wl}\n\n% Below, we want to prove that, given enough parameters, the node-wise learned positional encoding (LPE) can learn a  unique embedding that can distinguish any pair of graphs, thus making it more powerful than any WL-test. It does not however solve the graph isomorphism problem as there is no cannonical choice of eigenvectors (see section \\ref{sec:eigfunc-etiquette}) and isomorphic graphs might have different embeddings.\n% \\begin{comment}\n% as it suffers from the presence of multiplicities and sign invariance.\n% \\end{comment}\n\n% To follow the steps of the proof below, you can refer to the node-wise LPE depicted in Figure \\ref{fig:lpe-transformer-nodes}.\n\n% \\begin{proof}\n\n\n% Given a graph of $N$ nodes, adjacency matrix $\\mA$ and Laplacian matrix $\\mL$. It's full set of Laplacian eigenvalues is the $N\\times N$ diagonal matrix $\\mathbf{\\Lambda}$, and its full set of eigenvectors is the $N \\times N$ matrix $\\mathbf{\\Phi}$. The following equation associates the eigenfunctions to the Laplacian matrix $\\mL \\mathbf{\\Phi} = \\mathbf{\\Phi}  \\mathbf{\\Lambda}$. The Laplacian matrix of the graph can be fully deterrmined from its Laplacian spectrum $\\mL = \\mathbf{\\Phi} \\mathbf{\\Lambda} \\mathbf{\\Phi}^{-1}$, and consequently it's adjacency matrix.\n\n% Given 2 non-isomorphic graphs $G$ and $G'$, their set of eigenvalues and eigenvectors will necessarily be different since they uniquely determine the Laplacian. Recall that the learned positional encoding (LPE) presented in Figure \\ref{fig:lpe-transformer-nodes} takes all eigenvalues and eigenvectors of a given node as inputs when chosing the number $m$ of eigenfunctions equal to the number of nodes $m = N$. For a given node $i$, the input matrix of the LPE is:\n\n% \\[\n% \\text{LPE input: } A = \n% \\begin{bmatrix}\n% \\lambda_0 & \\eigvec_{i,0}\\\\ \n% \\lambda_1 & \\eigvec_{i,1}\\\\ \n% \\vdots & \\vdots\\\\ \n% \\lambda_{m-1} & \\eigvec_{i, m-1}\n% \\end{bmatrix}_{(m \\times 2)}\n% \\]\n\n\n% Now we want to prove that, given a unique input $A$, and given enough layers and parameters, then the LPE Transformer can produce a unique output. The proof will require to refer to the architecture of the LPE given in Figure \\ref{fig:lpe-transformer-nodes}.\n\n% One example of unique output for the LPE is the simple concatenation (with zero padding if $m > N$) of all eigenvalues and eigenvectors of a specific node, with size $1 \\times 2m$:\n\n% \\[\n% \\text{Possible unique output}: D = \n% \\left[\n% \\lambda_0, \\lambda_1, ..., \\lambda_{m-1}, \\eigvec_{i,0}, \\eigvec_{i,1}, ..., \\eigvec_{i,m-1}\n% \\right]_{(1 \\times 2m)} \n% \\]\n\n% Recall the sum pooling at the last layer of the LPE Transformer. To generate the previous output from a sum pooling, a simple solution is to generate the concatenation of two diagonal matrices with size $m \\times 2m$:\n\n% \\[\n% \\text{LPE output}: B =\n% \\left[\n% \\text{diag}([\n% \\lambda_0, \\lambda_1, ..., \\lambda_{m-1}]),\n% \\text{diag}([\n% \\eigvec_{i,0}, \\eigvec_{i,1}, ..., \\eigvec_{i,m-1}])\n% \\right]_{(m \\times 2m)} \n% \\]\n\n% \\[\n% \\text{Transformer output}: C =\n% \\begin{bmatrix}\n% \\lambda_0 & 0 & ... & 0 & \\eigvec_{i,0} & 0 & ... & 0 \\\\ \n% 0  & \\lambda_1 &  & \\vdots & 0  & \\eigvec_{i,1} &  & \\vdots \\\\ \n% \\vdots &  & \\ddots & 0 & \\vdots &  & \\ddots & 0 \\\\ \n% 0 & ... & 0 & \\lambda_{m-1} & 0 & ... & 0 & \\eigvec_{i,m-1} \n% \\end{bmatrix}_{(m \\times 2m)}\n% \\]\n\n% We can view this last matrix as a sequence of $m$ elements, with hidden dimension of $2N$. We know that, given enough layers, heads and parameters, the Transformers is a Universal sequence-to-sequence approximator \\cite{yun2020transformers}. Then it is possible to learn a transformation from the sequence $A$ to approximate sequence $C$, since all the information required to build $C$ is in $A$.\n\n% For this proof to be valid on a set of graphs, we need to chose the hyperparameters of the LPE according to the maximum node size of the given set $N_\\text{max}$. We need the hidden dimension $k \\geq 2N_\\text{max}$ and the sequence length $m \\geq N_\\text{max}$ (with masked padding if $m > N$).\n\n% Thus, the LPE can generate approximate a vector $B$ that uniquely identify any graphs, and given enough layers and parameters, can be more powerful than any WL-test in distinguishing isomorphic graphs. However, it suffers from the drawbacks noted below.\n\n% \\xhdr{Note 1}\n% \\textit{In the case of multiplicities of the eigenvalues, the eigenvectors are not unique. Thus, the output of two isomorphic graphs can also be different.}\n\n% \\xhdr{Note 2}\n% \\textit{Due to the lack of sign invariance, flipping the sign of the eigenvectors will also result in a different output. The augmentation by random-flipping of the eigenvectors helps alleviate this problem, although not solving it completely.\n% %The edge-wise LPE proposed in Appendix \\ref{app:lpe-edges} answers this problem.\n% }\n\n% \\xhdr{Note 3}\n% \\textit{Since all isomorphic graphs are co-spectral with regards to the eigenvalues, then it is the eigenvectors that allow to distinguish isomorphic graphs. And eigenvectors are limited by the above notes.}\n\n\n% \\end{proof}\n\n\n\n"
                },
                "category": [
                    "analysis"
                ]
            }
        },
        "tables": {
            "tab:comparison-theory": "\\begin{table}[h]\n\\centering\n\\caption{Comparison of the properties of different graph Transformer models.}\n\\label{tab:comparison-theory}\n\n\\resizebox{12cm}{!}{\n\\begin{tabular}{l|cccc}\n\n\nMODELS\n\n& GAT \\cite{velikovic2017gat}\n\n& GT sparse \\cite{dwivedi2020generalization}\n\n& GT full \\cite{dwivedi2020generalization}\n\n& SAN (Node LPE)\n\n% & \\begin{tabular}[c]{@{}c@{}}\n% GAT\\\\ \\cite{velikovic2017gat}\\end{tabular}\n\n% & \\begin{tabular}[c]{@{}c@{}}\n% GT sparse\\\\ \\cite{dwivedi2020generalization}\\end{tabular}\n\n% & \\begin{tabular}[c]{@{}c@{}}\n% GT full\\\\ \\cite{dwivedi2020generalization}\\end{tabular}\n\n% & \\begin{tabular}[c]{@{}c@{}}\n% ours\\\\ (full)\\end{tabular}\n\n\\\\\n\n\\hline\n\nPreserves local structure in attention\n& \\cmark & \\cmark & \\xmark & \\cmark \\\\\n\nUses edge features\n& \\xmark & \\cmark & \\xmark & \\cmark \\\\\n\nConnects non-neighbouring nodes\n& \\xmark & \\xmark & \\cmark & \\cmark \\\\\n\nUses eigenvector-based PE for attention\n& \\xmark & \\cmark & \\cmark & \\cmark \\\\\n\nUse a PE with structural information\n& \\xmark & \\cmark & \\xmark \\footnotemark & \\cmark \\\\\n\nConsiders the ordering of the eigenvalues\n& \\xmark & \\cmark & \\cmark & \\cmark \\\\\n\nInvariant to the norm of the eigenvector\n& - & \\cmark & \\cmark & \\cmark \\\\\n\nConsiders the spectrum of eigenvalues\n& \\xmark & \\xmark & \\xmark & \\cmark \\\\\n\nConsiders variable \\# of eigenvectors\n& - & \\xmark & \\xmark & \\cmark \\\\\n\nAware of eigenvalue multiplicities\n& - & \\xmark & \\xmark & \\cmark \\\\\n\nInvariant to the sign of the eigenvectors\n& - & \\xmark & \\xmark & \\xmark\n\n\\end{tabular}}\n\\vspace{-5pt}\n\\end{table}"
        },
        "figures": {
            "fig:full-method": "\\begin{figure}[h]\n\\vspace{-5pt}\n  \\centering\n    \\includegraphics[width=1 \\linewidth]{full_method.pdf}\n    \\vspace{-16pt}\n    \\caption{The proposed SAN model with the node LPE, a generalization of Transformers to graphs.}\n    \\label{fig:full-method}\n\\end{figure}",
            "fig:eigvec-matrix": "\\begin{figure}[h]\n  \\centering\n    \\includegraphics[height=1.8cm]{eigenvector_matrix.pdf}\n    \\vspace{-5pt}\n    \\caption{a) Standard view of the eigenvectors as a matrix. b) Eigenvectors $\\eigvec_i$ viewed as vectors positionned on the axis of frequencies (eigenvalues).}\n    \\label{fig:eigvec-matrix}\n    \\vspace{-5pt}\n\\end{figure}",
            "fig:eigvec-examples": "\\begin{figure}[h]\n\\vspace{-3pt}\n  \\centering\n    \\includegraphics[height=2.4cm]{eigvec-examples.pdf}\n    \\vspace{-3pt}\n    \\caption{Examples of eigenvalues $\\lambda_i$ and eigenvectors $\\eigvec_i$ for molecular graphs. The low-frequency eigenvectors $\\eigvec_{1}, \\eigvec_{2}$ are spread accross the graph, while higher frequencies, such as $\\eigvec_{14}, \\eigvec_{15}$ for the left molecule or $\\eigvec_{10}, \\eigvec_{11}$ for the right molecule, often resonate in local structures. \n    }\n    \\label{fig:eigvec-examples}\n    \\vspace{-5pt}\n\\end{figure}",
            "fig:lpe-transformer-nodes": "\\begin{figure}[h]\n  \\centering\n    \\includegraphics[width=1 \\linewidth]{LPE_nodes.pdf}\n    \\vspace{-12pt}\n    \\caption{Learned positional encoding (LPE) architectures, with the model being aware of the graph's Laplace spectrum by considering $m$ eigenvalues and eigenvectors, where we permit $m \\leq N$, with $N$ denoting the number of nodes. Since the Transformer loops over the nodes, each node can be viewed as an element of a batch to parallelize the computation.\n    Here $\\eigvec_{i,j}$ is the $j$-th element of the eigenvector paired to the $i$-th lowest eigenvalue $\\lambda_i$.\n    }\n    \\label{fig:lpe-transformer-nodes}\n    \\vspace{-5pt}\n\\end{figure}",
            "fig:boxplots": "\\begin{figure}[h]\n    \\vspace{-11pt}\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{GammaPlot.pdf}\n    \\vspace{-10pt}\n    \\caption{Effect of the $\\gamma$ parameter on the performance across datasets from \\cite{dwivedi2020benchmarking, hu2020open}, using the Node LPE. Dotted black lines indicate sparse attention, which is equivalent to setting $\\gamma=0$. Each box plot consists of 4 runs, with different seeds (except MolHIV).}\n    \\label{fig:boxplots}\n    \\vspace{-8pt}\n\\end{figure}",
            "fig:ablation": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{ablation.pdf}\n    \\vspace{-5pt}\n    \\caption{\n    Ablation study on datasets from \\cite{dwivedi2020benchmarking, hu2020open} for the node LPE and full graph attention, with no hyperparameter tuning other than $\\gamma$ taken from Figure \\ref{fig:boxplots}. For a given dataset, all models use the same hyperparameters, but the hidden dimensions are adjusted to have $\\sim 500k$ learnable parameters. Means and uncertainties are derived from four runs, with different seeds (except MolHIV).}\n    \\label{fig:ablation}\n    \\vspace{-4pt}\n\\end{figure}",
            "fig:results": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{final_SOTA.pdf}\n    \\vspace{-10pt}\n    \\caption{Comparing our tuned model on datasets from \\cite{dwivedi2020benchmarking, hu2020open}, against GCN \\cite{kipf2016gcn}, GraphSage \\cite{hamilton2017inductive}, GIN \\cite{xu2018gin}, GAT \\cite{velikovic2017gat}, GatedGCN \\cite{bresson2017gatedGCN}, PNA \\cite{corso2020principal}, and DGN \\cite{beaini_directional_2021}. Means and uncertainties are derived from four runs with different seeds, except MolHIV which uses 10 runs with identical seed. The number of parameters is fixed to $\\sim 500k$ for ZINC, PATTERN and CLUSTER.}\n    \\label{fig:results}\n    \\vspace{-5pt}\n    \n\\end{figure}",
            "fig:lpe-transformer-edges": "\\begin{figure}[h]\n  \\centering\n    \\includegraphics[width=1 \\linewidth]{LPE_edges.pdf}\n    \\caption{Edge-wise Learned positional encoding (LPE) architectures, where the relative position is considered instead of the absolute position. The model is aware of the graph's Laplace spectrum by considering $m$ eigenvalues and eigenvectors, where we permit $m \\leq N$, with $N$ denoting the number of nodes. Since the Transformer loops over the edges, each edge can be viewed as an element of a batch to parallelize the computation. The computational complexity is $O(m^2 E)$ or $O(m^2 N^2)$ for a fully-connected graph.\n    }\n    \\label{fig:lpe-transformer-edges}\n\\end{figure}",
            "ablation_params": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{ablation_params.pdf}\n    \\caption{Model architecture parameters for the ablation study. We modify the hidden dimensions of the Main Graph Transformer (GT) such that all models have $\\thicksim 500k$ parameters for a fair comparison.\\newline\n    \\dag The batch size was doubled to ensure convergence of the model. All other parameters outside the GT hidden dimension are consistent within a dataset experiment.}\n    \\label{ablation_params}\n\\end{figure}",
            "fig:computation-details": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{sota_resources.pdf}\n    \\caption{Computational details for SOTA Comparison study.}\n    \\label{fig:computation-details}\n\\end{figure}",
            "fig:isomorphic-eigvals": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[height=5cm]{isomorphic_graphs_eigvals.pdf}\n    \\caption{Example of non-isomorphic non-isospectral graphs that can be distinguished by the eigenvalues of their Laplacian matrix, but not by the 1-WL test.}\n    \\label{fig:isomorphic-eigvals}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{eq:greens_function}\n    \\mG(j_1,j_2) = \n    d_{j_1}^\\frac{1}{2}d_{j_2}^\n    \\frac{-1}{2}\\sum_{i>0}\\frac{(\\hat{\\eigvec}_{i,j_1} \\hat{\\eigvec}_{i,j_2})^2}\n    {\\hat{\\lambda}_i}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{eq:diffusion_biharmonic_dist}\nd_D^2(j_1,j_2) = \n    \\sum_{k>0}{e^{-2 t \\lambda_i}\n    (\\eigvec_{i,j_1} - \\eigvec_{i,j_2})^2} \n\\quad,\\quad\nd_B^2(j_1, j_2) = \n    \\sum_{i>0}\n    \\frac{(\\eigvec_{i,j_1} - \\eigvec_{i,j_2})^2}\n    {\\lambda_i^2}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq:multi-head}\n    \\hat{\\vh}_i^{l+1}= \\mO_h^l \\bigparallel_{k=1}^H (\\sum_{j \\in V} w_{ij}^{k,l}\\mV^{k,l}\\vh_{j}^l)\n\\end{equation}",
            "eq:4": "\\begin{equation}\n        \\label{eq:attention}\n        \\hat{\\vw}_{ij}^{k,l} = \\left\\{\\begin{array}{lr}\n        \\frac{\\mQ^{1,k,l}\\vh_i^l \\circ \\mK^{1,k,l} \\vh_j^l\\circ \\mE^{1,k,l}\\ve_{ij}}{\\sqrt{d_k}} & \\text{if $i$ and $j$ are connected in sparse graph}\\\\\n        \\\\\n        \\frac{\\mQ^{2,k,l}\\vh_i^l \\circ \\mK^{2,k,l}\\vh_j^l\\circ \\mE^{2,k,l}\\ve_{ij}}{\\sqrt{d_k}} & \\text{otherwise }\\\\\n        \\end{array}\\right\\}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\label{eq:gamma}\n    {w}_{ij}^{k,l} = \\left\\{\\begin{array}{lr}\n        \\frac{1}{1+\\gamma}\\cdot\\text{softmax}(\\sum_{d_k}\\hat{\\vw}_{ij}^{k,l}) & \\text{if $i$ and $j$ are connected in sparse graph}\\\\\n        \\\\\n        \\frac{\\gamma}{1+\\gamma}\\cdot\\text{softmax}(\\sum_{d_k}\\hat{\\vw}_{ij}^{k,l})& \\text{otherwise }\\\\\n        \\end{array}\\right\\}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\hat{\\hat{\\vh}}^{l+1} = \\textnormal{Norm}(\\vh_i^l + \\hat{\\vh}_i^{l+1} ),\n    \\quad\n    \\hat{\\hat{\\hat{\\vh}}}_i^{l+1}=\\mW_2^l\\textnormal{ReLU}(\\mW_1^l\\hat{\\hat{\\vh}}_i^{l+1}),\n    \\quad\n    \\vh_i^{l+1} = \\textnormal{Norm}(   \\hat{\\hat{\\vh}}^{l+1} +  \\hat{\\hat{\\hat{\\vh}}}_i^{l+1})\n\\end{equation}"
        },
        "git_link": "https://github.com/DevinKreuzer/SAN"
    },
    "llm_extraction": {
        "dataset": [
            {
                "name": "ZINC",
                "description": "A synthetic molecular graph regression dataset, where the predicted score is determined by $logP - SA$.",
                "proposed_in_paper": false,
                "url": null,
                "size": "Not specified, but used for regression tasks.",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Used in a regression task with unspecified splits into training and test sets according to standard experimental protocols.",
                "sampling": null,
                "preprocessing": null
            },
            {
                "name": "PATTERN",
                "description": "A synthetic benchmark for node classification; graphs are generated with Stochastic Block Models to classify nodes into 2 communities, testing the ability to recognize predetermined subgraphs.",
                "proposed_in_paper": false,
                "url": null,
                "size": "Not specified, but serves as a benchmark for classification tasks.",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Classified into specified communities based on Stochastic Block Model generation.",
                "sampling": null,
                "preprocessing": null
            },
            {
                "name": "CLUSTER",
                "description": "A synthetic node classification benchmark with graphs from Stochastic Block Models, having 6 communities; the task is to classify nodes with true labels assigned.",
                "proposed_in_paper": false,
                "url": null,
                "size": "Not specified but used for community detection.",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Each community has a single node with a true label assigned, focusing the classification tasks.",
                "sampling": null,
                "preprocessing": null
            },
            {
                "name": "MolHIV",
                "description": "A real-world molecular graph classification benchmark for predicting HIV inhibition; it suffers from imbalanced classes and de-correlation in performance metrics between validation and test sets, divided using scaffold splitting.",
                "proposed_in_paper": false,
                "url": null,
                "size": "Not explicitly stated, but a well-known benchmark in molecular graph classification.",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Divided using scaffold-based splitting based on two-dimensional molecular structures, creating imbalanced datasets.",
                "sampling": null,
                "preprocessing": "High imbalance towards negative samples, requires careful fine-tuning due to low correlation in metrics between validation and test sets."
            },
            {
                "name": "MolPCBA",
                "description": "A larger real-world molecular graph classification benchmark involving multiple tasks; applies a similar scaffold splitting procedure to MolHIV and uses Average Precision (AP) as a metric.",
                "proposed_in_paper": false,
                "url": null,
                "size": "Larger than MolHIV, with numerous tasks and low positivity rates (only 1.4%).",
                "usage": [
                    "training",
                    "validation",
                    "testing"
                ],
                "splitting": "Similar to MolHIV with a scaffold splitting strategy.",
                "sampling": null,
                "preprocessing": null
            }
        ]
    }
}