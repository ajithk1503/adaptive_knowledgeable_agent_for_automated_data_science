{
    "meta_info": {
        "title": "Group-wise Reinforcement Feature Generation for Optimal and Explainable  Representation Space Reconstruction",
        "abstract": "Representation (feature) space is an environment where data points are\nvectorized, distances are computed, patterns are characterized, and geometric\nstructures are embedded. Extracting a good representation space is critical to\naddress the curse of dimensionality, improve model generalization, overcome\ndata sparsity, and increase the availability of classic models. Existing\nliterature, such as feature engineering and representation learning, is limited\nin achieving full automation (e.g., over heavy reliance on intensive labor and\nempirical experiences), explainable explicitness (e.g., traceable\nreconstruction process and explainable new features), and flexible optimal\n(e.g., optimal feature space reconstruction is not embedded into downstream\ntasks). Can we simultaneously address the automation, explicitness, and optimal\nchallenges in representation space reconstruction for a machine learning task?\nTo answer this question, we propose a group-wise reinforcement generation\nperspective. We reformulate representation space reconstruction into an\ninteractive process of nested feature generation and selection, where feature\ngeneration is to generate new meaningful and explicit features, and feature\nselection is to eliminate redundant features to control feature sizes. We\ndevelop a cascading reinforcement learning method that leverages three\ncascading Markov Decision Processes to learn optimal generation policies to\nautomate the selection of features and operations and the feature crossing. We\ndesign a group-wise generation strategy to cross a feature group, an operation,\nand another feature group to generate new features and find the strategy that\ncan enhance exploration efficiency and augment reward signals of cascading\nagents. Finally, we present extensive experiments to demonstrate the\neffectiveness, efficiency, traceability, and explicitness of our system.",
        "author": "Dongjie Wang, Yanjie Fu, Kunpeng Liu, Xiaolin Li, Yan Solihin",
        "link": "http://arxiv.org/abs/2205.14526v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "KDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nClassic Machine Learning (ML) mainly includes data prepossessing, feature extraction, feature engineering, predictive modeling, and evaluation. \n%Much efforts have been devoted to developing sophisticated learning models, including, approximation function, error measurement, training algorithms, performance bounds. \n%The parameter numbers of deep models have increased from million level to trillion level in the past years~\\cite{fedus2021switch}. \nThe evolution of deep AI, however, has resulted in a new principled and widely used paradigm: i) collecting data, ii) computing data representations, and iii) applying ML models. \nIndeed, the success of ML algorithms highly depends on\ndata representation~\\cite{bengio2013representation}. \n%This is because different representations (or features) can entangle and hide more or less the different explanatory factors of variation behind the data.\nBuilding a good representation space is critical and fundamental because it can help to 1) identify and disentangle the underlying explanatory factors hidden in observed data, 2)  easy the extraction of useful information in predictive modeling,  3) reconstruct distance measures to form discriminative and machine-learnable patterns, 4) embed structure knowledge and priors into representation space and thus make classic ML algorithms available to complex graph, spatiotemporal, sequence, multi-media, or even hybrid data. \n\n\n\n\n%representation learning, i.e., learning transformations of the data that make it easier to extract useful information when building classifiers or other predictors. \nIn this paper, we study the problem of  learning to reconstruct an optimal and explainable feature representation space to advance a downstream ML task (\\textbf{Figure \\ref{fig:intro_frame}}).\nFormally, given a set of original features, a prediction target, and a downstream ML task (e.g., classification, regression, ranking, detection), the objective is to automatically reconstruct an optimal and explainable set of features for the ML task. \n\nPrior literature has partially addressed the problem. \nThe first relevant work is feature engineering, which designs preprocessing, feature extraction, selection~\\cite{li2017feature,guyon2003introduction}, and generation~\\cite{khurana2018feature} to extract a transformed representation of the data. These techniques are essential but labor-intensive, showing the low applicability of current ML practice in the automation of extracting a discriminative feature representation space.\n\\textbf{Issue 1 (full automation): } \\emph{how can we make ML less dependent on feature engineering, construct ML systems faster, and expand the scope and applicability of ML?}\nThe second relevant work is representation learning, such as factorization~\\cite{fusi2018probabilistic},  embedding~\\cite{goyal2018graph}, and deep representation learning~\\cite{wang2021reinforced,wang2021automated}. \nThese studies are devoted to learning effective latent features. However, the learned features are implicit and non-explainable.\nSuch traits limit the deployment of these approaches in many application scenarios (e.g., patient and biomedical domains) that require not just high predictive accuracy but also trusted understanding and interpretation of underlying drivers. \n\\textbf{Issue 2 (explainable explicitness): }\n\\emph{how can we assure that the reconstructing representation space is traceable and explainable?}\nThe third relevant work is learning based feature transformation, such as  principle component analysis~\\cite{candes2011robust}, traversal transformation graph based feature generation~\\cite{khurana2018feature}, sparsity regularization based feature selection~\\cite{friedman2012fast,hastie2019statistical}.\nThese methods are either deeply embedded into or totally irrelevant to a specific ML model. \nFor example, LASSO regression extracts an optimal feature subset for regression, but not for any given ML model. \n\\textbf{Issue 3 (flexible optimal): } \\emph{how can we create a framework to reconstruct a new representation space for any given predictor?} \nThe three issues are well-known challenges. Our goal is to develop a new perspective to address these issues. \n\n\\textbf{Our Contributions: A Traceable Group-wise Reinforcement Generation Perspective}.\nWe propose a novel principled framework to address the automation, explicitness, optimal issues in representation space reconstruction. \nWe view feature generation and selection from the lens of Reinforcement Learning (RL).\nWe show that learning to reconstruct representation space can be accomplished by an interactive process of nested feature generation and selection, where feature generation is to generate new meaningful and explicit features, and feature selection is to remove redundant features to control feature sizes. \nWe highlight that the human intuition and domain expertise in feature generation and selection can be formulated as machine-learnable policies. \nRL is an emerging technique to automatically generate experiences data and learn globally optimized policies.\nSuch traits have sparked considerable interest in recent years. \nWe demonstrate that the iterative sequential feature generation and selection can be generalized as a RL task. \nWe find that crossing features of high information distinctness is more likely to generate meaningful variables in a new representation space, and, thus, leveraging group-group crossing can accelerate the learning efficiency. \n\n\\textbf{Summary of Proposed Approach}.\nBased on our findings, we develop a generic and principled framework: group-wise reinforcement feature generation, for optimal and explainable representation space reconstruction.\nThis framework learns a representation space reconstructor that can\n1)  \\textbf{Goal 1: explainable explicitness:} provide traceable generation process and understand the meanings of each generated feature. \n2) \\textbf{Goal 2: self optimization:} automatically generate an optimal feature set for a downstream ML task without much professional experience and human intervention; \n3)  \\textbf{Goal 3: enhanced efficiency and reward augmentation:} \nenhance the generation and exploration speed in a large feature space and augment reward incentive signal to learn clear policies. \n\nTo achieve Goal 1, we propose an iterative feature generation and selection  strategy, where the generation step is to apply a mathematical operation to two features to create a new feature and the selection step is to control the feature set size. \nThis strategy allows us to explicitly trace the generation process and extract the semantic labels of generated features. \nTo achieve Goal 2, we decompose feature generation into three Markov Decision Processes (MDPs): one is to select the first meta feature, one is to select an operation, and one is to select the second meta feature. We develop a new cascading agent structure to coordinate agents to share states and learn better selection policies for feature generation.\nTo achieve Goal 3, we design a group-operation-group based generation approach, instead of intuitive feature-operation-feature based generation, in order to accelerate representation space reconstruction. \nIn particular, we first cluster the original features into different feature groups by maximizing intra-group feature cohesion and  inter-group feature distinctness, where we propose a novel feature-feature information distance. \nWe then let agents select and cross two feature groups to generate multiple features each time. \nThe benefits of this strategy are two folds: i) it explores feature space faster; ii) if we use feature-operation-feature based generation to add a single feature each time, the state of a feature set cannot be sufficiently changed, thus, restricting the agents from gaining enough reward to learn effective policies.\nInstead, the group-operation-group based generation can alleviate this issue by augmenting the reward signal. \n\n%High dimensionality has been a well-known challenge for data intensive Scientific Machine Learning (SciML). It can cause the issue of pairwise distances between samples converging to the same value, increase data sparsity, reduce model generalization. One of the key thrusts in Data Intensive Scientific Machine Learning [1] is to answer: How should a low-dimensional geometry structure be extracted from high- dimensional data? To solve this issue, researchers proposed feature selection, PCA, sparsity regularization, factorization, embedding, and deep learning. However, existing techniques are limited in achieving full automation, global optimal, and explainable explicitness. For example, classic methods cannot fully free scientists from repeatedly applying, testing and adjusting algorithms and hyperparamters; classic methods need a more globally-optimized non-greedy strategy for maximum information preservation, while tailored for downstream SciML tasks; scientists cannot interpret the explicit semantic meaning of the latent dimensionalities learned by factorization, embedding, and deep learning. Can we address the automation, optimal, and explicitness challenges in structure extraction? A low-dimensional geometry structure is crucial for SciML methods (e.g., GP models), and the accuracy of these methods depends on how well one can learn the structure from data or physics-based models. In response, this project will target the problem of automated identification of an optimal and explicit low-dimensional geometry from high dimensional data. \n\\vspace{-0.1cm}\n"
            },
            "section 2": {
                "name": "Definitions and Problem Statement",
                "content": "\n%We present several important definitions and then outline the problem statement.\n\n",
                "subsection 2.1": {
                    "name": "Important Definitions",
                    "content": "\n\\begin{definition}\n\\textbf{Feature Group.} \nWe aim to reconstruct the feature space of such datasets $\\mathcal{D}<\\mathcal{F},y>$. \nHere, $\\mathcal{F}$ is a feature set, in which each column denotes a feature and each row denotes a data sample;\n$y$ is the target label set corresponding to samples.\nTo effectively and efficiently produce new features, we divide the feature set $\\mathcal{F}$ into different feature groups via clustering, denoted by $\\mathcal{C}$.\nEach feature group  is a feature subset of  $\\mathcal{F}$.\n% The intra-cluster features are similar in terms of information distribution, however the inter-cluster features are distinct.\n\n\n\n\\end{definition}\n\n% \\begin{definition}\n% \\textbf{Operation Set.}\n% To effectively and efficiently produce new features, we divide the feature set $\\mathcal{F}$ into different feature clusters denoted by $\\mathcal{C}$.\n% Each $\\mathcal{C}$ is the feature subset of $\\mathcal{F}$.\n% The intra-cluster features are similar in terms of information distribution, however the inter-cluster features are distinct.\n% \\end{definition}\n\n\n\\begin{definition}\n\\textbf{Operation Set.}\nWe perform a mathematical operation on existing features in order to generate new ones.\nThe collection of all operations is an operation set, denoted by $\\mathcal{O}$.\nThere are two types of operations: unary and binary.\nThe unary operations include ``square'', ``exp'', ``log'', and etc.\nThe binary operations are ``plus'', ``multiply'', ``divide'', and etc.\n\\end{definition}\n\n\n\\begin{definition}\n\\textbf{Cascading Agent.}\nTo address the feature generation challenge, we develop a new cascading agent structure. \nThis structure is made up of three agents: two feature group agents and one operation agent. Such cascading agents share state information and sequentially select feature groups and operations.\n\\end{definition}\n\n% \\begin{definition}\n% \\textbf{Feature Interaction.}\n% We generate new features through group-group feature interaction.\n% To be precise, agents can select one operator and two feature clusters.\n% If the operator is unary, we will apply it to all features of the feature cluster that is more relevant to the target label.\n% If the operator is binary, we will apply it to the topK dissimilar feature pairs between the two clusters.\n\n% \\end{definition}\n\n% \\begin{definition}\n% \\textbf{Downstream Task.}\n% We assess the feature set's quality by performing a downstream task such as regression or classification.\n% The quantitative  indicator (\\textit{e.g.} 1-relative absolute error or f1) of the task is the reflection of the quality.\n% The higher of the value of such an indicator, the better the feature set's quality.\n% \\end{definition}\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Problem Statement",
                    "content": "\nThe research problem is learning to reconstruct an optimal and explainable feature representation space to advance a downstream ML task.\nFormally, given a dataset $D<\\mathcal{F},y>$ that includes an original feature set $\\mathcal{F}$ and a target label set $y$, an operator set $\\mathcal{O}$,   and a downstream ML task $A$ (e.g., classification, regression, ranking, detection), our objective is to automatically reconstruct an optimal and explainable feature set $\\mathcal{F}^{*}$   that optimizes the performance indicator $V$ of the task $A$. \nThe optimization objective  is to find a reconstructed feature set $\\mathcal{\\hat{F}}$ that maximizes:\n\\begin{equation}\n\\label{objective}\n    \\mathcal{F}^{*} = argmax_{\\mathcal{\\hat{F}}}( V_A(\\mathcal{\\hat{F}},y)),\n\\end{equation}\nwhere $\\mathcal{\\hat{F}}$ can be viewed as a subset of a combination of  the original feature set $\\mathcal{F}$ and the generated new features $\\mathcal{F}^g$, and $\\mathcal{F}^g$ is produced by applying the operations $\\mathcal{O}$ to  the original feature set $\\mathcal{F}$ via a certain algorithmic structure.\n%so $\\mathcal{\\hat{F}} \\subseteq  \\mathcal{F} \\bigcup \\mathcal{F}^g$, where $\\mathcal{F}^g$ is produced by applying $\\mathcal{O}$ to $\\mathcal{F}$.\n\n\n\\iffalse\nThe purpose of this research is to present a framework for automated feature generation.\nThis framework is capable of autonomously generating relevant features by performing mathematical operators on existing feature sets until the optimal feature set is discovered.\nFormally, given an operator set $\\mathcal{O}$ and a dataset $D<\\mathcal{F},y>$ constituted of a feature set $\\mathcal{F}$ and a target label set $y$,  we aim to seek the optimal feature set $\\mathcal{F}^{*}$ that optimizes the performance indicator $V$ of the downstream task $A$. \nThe objective can be formulated as:\n\\begin{equation}\n\\label{objective}\n    \\mathcal{F}^{*} = argmax_{\\mathcal{\\hat{F}}}( V_A(\\mathcal{\\hat{F}},y)),\n\\end{equation}\nwhere $\\mathcal{\\hat{F}}$ is the subset of the original feature set $\\mathcal{F}$ and the generated new features $\\mathcal{F}^g$ , so $\\mathcal{\\hat{F}} \\subseteq  \\mathcal{F} \\bigcup \\mathcal{F}^g$, where $\\mathcal{F}^g$ is produced by applying $\\mathcal{O}$ to $\\mathcal{F}$.\n\\fi\n\n\n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Optimal and Explainable  Feature Space Reconstruction",
                "content": "\n%In this section, we will introduce each technical component of our framework and its associated pseudo algorithm.\nWe present an overview, and then detail each technical component of our framework.\n\n\n\n\n",
                "subsection 3.1": {
                    "name": "Framework Overview",
                    "content": "\nFigure \\ref{fig:framework} shows our proposed framework, \\textbf{G}roup-wise \\textbf{R}einforcement \\textbf{F}eature \\textbf{G}eneration (\\textbf{GRFG}).\nIn the first step, we cluster the original features into different feature groups by maximizing intra-group feature cohesion and inter-group feature distinctness. \nIn the second step, we leverage a group-operation-group strategy to cross two feature groups to generate multiple features each time. For this purpose, we develop a novel cascading reinforcement learning method to learn three agents to select the two most informative feature groups and the most appropriate operation from the operation set.\nAs a key enabling technique, the cascading reinforcement method will coordinate the three agents to share their perceived states in a cascading fashion, i.e., (agent1, state of the first feature group), (agent2, fused state of the operation and agent1's state), and (agent3, fused state of the second feature group and agent2's state), in order to learn better choice policies. \nAfter the two feature groups and operation are selected, we generate new features via a group-group crossing strategy. \nIn particular, if the operation is unary, e.g., sqrt, we choose the feature group of higher relevance to target labels from the two feature groups, and apply the operation to the more relevant feature group  to generate new features. \nif the operation is binary,  we will choose the K most distinct feature-pairs from the two feature groups, and apply the binary operation to the chosen feature pairs to generate new features.\nIn the third step, we add the newly generated features to the original features to  create a generated feature set.\nWe feed the generated feature set into a downstream task to collect predictive accuracy as reward feedback to update  policy parameters.\nFinally, we employ feature selection to eliminate redundant features and control the dimensonality of the newly generated feature set, which will be used as the original feature set to restart the iterations to regenerate new features until the maximum number of iterations is reached.\n\n\\noindent{\\bf Comparison with prior literature.}\nAutomated feature engineering has recently attracted substantial research attention and has achieved great success. The transformation graph~\\cite{khurana2018feature} and neural feature search~\\cite{chen2019neural} are two typical methods that are closest to our task in existing literature. \nInstead of conducting personalized feature-feature crossing, the work in  ~\\cite{khurana2018feature} generated new features by applying the selected operation to the entire feature set.\nThis generation strategy ignores features heterogeneity in a feature set, resulting in low-quality and sub-optimal features.\nThe study in ~\\cite{chen2019neural} trained  a single recurrent neural network (RNN) for each feature to learn its feature transformation sequence for feature generation.\nThis strategy overlooks the feature distinctness in a feature set, restricting the method from producing  meaningful combined features.\nMeanwhile, as the size of the feature set grows, the number of RNNs grows accordingly, which makes this work inefficient when dealing with large datasets.\nTo fill these gaps, our framework iteratively generates meaningful features via group-wise feature-feature interactions, which takes the feature heterogeneity into account. Moreover, we decompose the feature generation process into three MDPs and propose a simple cascading agent structure for it. Additionally,  our approach is a self-learning end-to-end  framework, which can be easily and flexibly applied to many scenarios.\n\n\n\n% that takes feature-feature differences into account. Moreover, we implement a straightforward cascading agent structure to select crossing features and operations. Additionally, we propose a group-wise feature generation strategy to accelerate the generation process. To obtain good feature groups in each iteration, we propose a new feature clustering algorithm based on mutual information.\n\n\n\n% This technique leads to inability to conduct personalized feature crossing, which causes the generated features are low-quality.\n\n\n\n\n\n%The two approaches and our method have in common that they all employ reinforcement learning to optimize the feature generation process. \n\n%commonlity: operation + old features\n\n%the work in ~\\cite{khurana2018feature}: feture set hegeroneity -> personalized feature crossing  --> apply same selected operation to the entire feature set ->  xxx method cannot conduct personalized generation with specific operation -> cannot generate high-quality & optimal features.\n\n% ~\\cite{chen2019neural}: can only generate a new feature from a specific single features, inability to conduct feature crossing --> low-quality; requires to train a single RNN to learn the feature transformation operation sequence for each feature -> when feature size is large, train large amount of RNNs, low efficiency -> when confronted with different feature sets\n\n\n% overlooks feature-feature distinctions, resulting in a reduction in the diversity of generated features.\n\n% ~\\cite{chen2019neural} has to adapt its model structure to varied feature sets, restricting its practicality. \n\n\n\n\n\n\n\n% {\\color{red}: please write write this paragraph including commons, distinctions, advantages/strengths\n% \\smallskip\n% \\noindent{\\bf Comparison with prior literature.}\n% Although existing works have achieved great success, they suffer from several restrictions that impair their performance and use.\n% For instance, transformation graph based approaches overlook feature differences, resulting in a reduction in the diversity of generated features ~\\cite{khurana2018feature}. \n% Methods based on \n% recurrent neural network have variable model architectures in response to different feature sets, limiting their practicality ~\\cite{chen2019neural}.\n% In comparison to these prior literature, our framework's primary model structure is straightforward, only consisting of three agents. \n% Meanwhile, we utilize feature clustering and group-group feature interaction to generate meaningful features considering feature information differences.\n% }\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Generation-oriented Feature Clustering",
                    "content": "\nOne of our key findings is that group-wise feature generation can accelerate the generation and exploration, and, moreover, augment reward feedback of agents to learn clear policies. \nInspired by this finding, our system starts with generation oriented feature clustering, which aims to create feature groups that are meaningful for group-group crossing.  \nOur another insight is that crossing features of high (low) information distinctness is more (less) likely to generate meaningful variables in a new representation space. \nAs a result, unlike classic clustering, we aim to  cluster features into different feature groups, with the optimization objective of maximizing  inter-group feature information distinctness while minimizing intra-group feature information distinctness. \nTo achieve this goal, we propose the \\textbf{M-Clustering} for feature clustering, which starts with each feature as a feature group and then merges the closest feature group pair at each iteration. \n\n\\textbf{Distance Between Feature Groups: A Group-level Relevance-Redundancy Ratio Perspective.} To achieve the goal of minimizing intra-group feature distinctness and maximizing inter-group feature distinctness,  we develop a new distance measure to quantify the distance between two feature groups. We highlight two interesting findings: 1) relevance to predictive target: if the relevance between one feature group and predictive target is similar to the relevance of another feature group and predictive target,  the two feature groups are similar; 2) mutual information: if the mutual information between the features of the two groups are large, the two feature groups are similar. \nBased on the two insights, we devise a feature group-group distance. \nThe distance can be used to evaluate the distinctness of two feature groups, and, further, understand how likely crossing the two feature groups will generate more meaningful features. Formally, the distance is given by:\n\\begin{equation}\n\\vspace{-0.18cm}\n    \\label{fea_dis}\n    dis(\\mathcal{C}_i, \\mathcal{C}_j) =\n    \\frac{1}{|\\mathcal{C}_i|\\cdot|\\mathcal{C}_j|}\n    \\sum_{f_i\\in \\mathcal{C}_i}\\sum_{f_j\\in \\mathcal{C}_j}\\frac{|MI(f_i,y)-MI(f_j,y)|}{MI(f_i,f_j)+\\epsilon},\n\\end{equation}\nwhere $\\mathcal{C}_i$ and $\\mathcal{C}_j$ denote two feature groups, $|\\mathcal{C}_i|$ and $|\\mathcal{C}_j|$  respectively are the numbers of features in $\\mathcal{C}_i$ and $\\mathcal{C}_j$, $f_i$ and $f_j$ are two features in $\\mathcal{C}_i$ and $\\mathcal{C}_j$ respectively, $y$ is the target label vector.\n%\nIn particular, $|MI(f_i,y)-MI(f_j,y)|$ quantifies the  difference in relevance  between $y$ and $f_i$, $f_j$.\nIf $|MI(f_i,y)-MI(f_j,y)|$ is small,   $f_i$ and $f_j$ have a more similar influence on classifying $y$;\n$MI(f_i,f_j)+\\epsilon$ quantifies the redundancy between $f_i$ and $f_j$.\n$\\epsilon$ is a small value that is used to prevent the denominator from being zero.\nIf $MI(f_i,f_j)+\\epsilon$ is big, $f_i$ and $f_j$ share more information.\n \n\\textbf{Feature Group Distance based M-Clustering Algorithm:}\nWe develop a group-group distance instead of point-point distance,  and under such a group-level distance, the shape of the feature cluster could be non-spherical. Therefore, it is not appropriate to use K-means or density based methods. Inspired by agglomerative clustering,  given a feature set $\\mathcal{F}$, we propose a three step method: 1) INITIALIZATION:  we regard each feature in $\\mathcal{F}$ as a small feature cluster. \n2) REPEAT:  we calculate the information overlap between any two feature clusters and determine which cluster pair is the most closely overlapped.  We then merge two clusters into one and remove the two original clusters.\n3) STOP CRITERIA:  we iterate the REPEAT step until the distance between the closest feature group pair reaches a certain threshold. Although classic stop criteria is to stop when there is only one cluster, using the distance between the closest feature groups as stop criteria can better help us to semantically understand, gauge, and identify the information distinctness among feature groups. It eases the implementation in practical deployment. \n\n%Before introducing our measurement, we firstly provide the definition of MI ~\\cite{cover1999elements}. The MI between two discrete variable vectors is: $%\\begin{equation}\\label{mutual_info} MI(X_1,X_2) = \\sum_{x_1\\in X_1}\\sum_{x_2\\in X_2} p(x_1,x_2)log(\\frac{p(x_1,x_2)}{p(x_1)p(x_2)}), %\\end{equation} $where $X_1$ and $X_2$ are two vectors, $x_1$ and $x_2$ are elements of $X_1$ and $X_2$ respectively, $p(x_1,x_2)$ is the joint distribution of $X_1$ and $X_2$, $p(x_1)$ and $p(x_2)$ are the marginal distribution of $X_1$ and $X_2$ respectively. The greater the value of MI, the more information the two vectors share. By using this distance metric, we can group the features that share similar information and have similar impact on identifying target label into a single feature cluster. Our framework begins by clustering the feature set into  feature clusters. This step groups similar features into a single cluster and dissimilar features into different clusters. This is because we presume that the generated features are meaningful when the interacted features come from different information distributions; otherwise, the generated features should be redundant. Considering that the boundary and information distribution of feature clusters vary over the feature generation process, we develop a new  M-Clustering algorithm rather than using a conventional clustering technique.\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Cascading Reinforcement Feature Groups and Operation Selection",
                    "content": "\nTo achieve group-wise feature generation, we need to  select a feature group, an operation, and another feature group to perform group-operation-group based crossing. Two key findings inspire us to leverage cascading reinforcement. \n\\textbf{Firstly}, we highlight that although it is hard to define and program the optimal selection criteria of feature groups and operation, we can view selection criteria as a form of machine-learnable policies. \nWe propose three agents to learn the policies by trials and errors. \n\\textbf{Secondly}, we find that the three selection agents are cascading in a sequential auto-correlated decision structure, not independent and separated. \nHere, ``cascading'' refers to the fact that within each iteration agents  make decision sequentially, and downstream agents await for the completion of an upstream agent.  The decision of an upstream agent will change the environment states of downstream agents.\nAs shown in Figure ~\\ref{fig:agents}, the first agent picks the first feature group based on the state of the entire feature space, the second agent picks the operation based on the entire feature space and the selection of the first agent, and the third agent chooses the second feature group based on the entire feature space and the selections of the first and second agents. \n\nWe next propose two generic metrics to quantify the usefulness (reward) of  a feature set, and then form three MDPs to learn three selection policies. \n\n\n\n%MDP is a discrete-time stochastic control process, which can be defined as a 4-tuple $\\mathcal{M}=\\{\\mathcal{S},\\mathcal{A},\\mathcal{P}, \\mathcal{R}\\}$. In this tuple, $\\mathcal{S}$ denotes the state space, $\\mathcal{A}$ denotes the action space, $\\mathcal{P}$ indicates the transition probability that the current action in the current state will lead the next state. $\\mathcal{R}$ indicates the immediate reward gained as a result of the state transition through taking action.\n\n\\smallskip\n\\noindent\\textbf{Two Utility Metrics for Reward Quantification.} The two utility metrics are from  the supervised and unsupervised perspectives. \n\n\\noindent\\ul{Metric 1: Integrated Feature Set Redundancy and Relevance.}  \nWe propose a metric to quantify feature set utility from an information theory perspective: a higher-utility feature set has less redundant information and is more relevant to prediction targets.\nFormally, given a feature set $\\mathcal{F}$ and a predictive target label $y$, such utility metric can be calculated by \n\\begin{equation}\n    U(\\mathcal{F}|y) = -\\frac{1}{|\\mathcal{F}|^2}\\sum_{f_i, f_j \\in \\mathcal{F}} MI(f_i, f_j) + \\frac{1}{|\\mathcal{F}|}\\sum_{f\\in \\mathcal{F}}MI(f,y),\n\\end{equation}\nwhere $MI$ is the mutual information, $f_i, f_j, f$ are features in $\\mathcal{F}$ and $|\\mathcal{F}|$ is the size of the feature set $\\mathcal{F}$.\n\n\\noindent\\ul{Metric 2: Downstream Task Performance.} \nAnother utility metric is whether this feature set can improve a downstream task (e.g., regression, classification). We use a downstream predictive task performance indicator (e.g., 1-RAE, Precision, Recall, F1) as a utility metric of a feature set. \n\n%We assess the utility of the feature set $\\mathcal{F}$ by performing a downstream task (\\textit{e.g.} regression or classification). The value of the quantitative indicator  (\\textit{e.g.} 1-relative absolute error or f1) of the task (denoted by $V_A$) is the reflection of the utility of $\\mathcal{F}$ from another perspective. Intuitively, a high $V_A$ value suggests that the $\\mathcal{F}$ has an excellent distinguishable ability to the target label $y$.\n\n\\smallskip\n\\noindent\\textbf{Learning Selection Agents of Feature Group 1, Operation, and Feature Group 2.} \nLeveraging the two metrics, we next develop three MDPs to learn three agent policies to select the best feature group 1, operation, feature group 2. \n\n\\noindent\\ul{\\textit{Learning the Selection Agent of Feature Group 1.}} \nThe feature group agent 1 iteratively select the best meta feature group 1.\nIts learning system includes: \\textbf{i) Action:} its action \nat the $t$-th iteration is the meta feature group 1  selected from the feature groups of the previous iteration, denoted group $a_t^1 = \\mathcal{C}^1_{t-1}$.\n\\textbf{ii) State:} its state at the $t$-th iteration is a vectorized representation of the generated feature set of the previous iteration.\nLet $Rep$ be a state representation method, the state can be denoted by $s_t^1 = Rep(\\mathcal{F}_{t-1})$.\nWe will discuss the state representation method in the next section.\n\\textbf{iii) Reward:} its reward at the $t$-th iteration is the utility score the selected feature group 1, denoted by  $\\mathcal{R}(s_t^1,a_t^1)=U(\\mathcal{F}_{t-1}|y)$.\n% Formally, let $Rep$ be a state representation method, in the $t$-th iteration, after the feature group agent 1 takes an action $a_t^{1}$ to select the meta feature group $\\mathcal{C}^1_t$ under the state of the previous iteration's feature set $s_t^1 = Rep(\\mathcal{F}_{t-1})$. We provide the reward $U(\\mathcal{F}_{t-1}|y)$ to the agent.\n\n\n%can be defined by $\\mathcal{M}^1 =\\{\\mathcal{S}^1,\\mathcal{A}^1,\\mathcal{P}^1, \\mathcal{R}^1\\}.$ The key components are as follows: (1) \\textbf{State.}  The $t$-th state in $\\mathcal{S}^1$ is the representation of the feature set $\\mathcal{F}_t$ at the $t$-th iteration, denoted by $s^1_t = \\phi(\\mathcal{F}_t)$. The representing process $\\phi$ will be illustrated in the following section. (2) \\textbf{Action.} The $t$-th action is the selected feature cluster1, denoted by  $a^1_t = \\mathcal{C}^1_t$. (3) \\textbf{Transition.} Owing to $\\mathcal{M}^1$ is deterministic, the transition probability from $s^1_t$ to $s^1_{t+1}$ via action $a^1_t$ is 1, denoted by $\\mathcal{P}^1(s^1_t,a^1_t,s^1_{t+1})=1$.\n\n\\noindent\\ul{\\textit{Learning the Selection Agent of Operation.}} \nThe operation agent will iteratively select the best operation (\\textit{e.g.} +, -) from an operation set as a feature crossing tool for feature generation.\nIts learning system includes: \\textbf{i) Action:} its action at the $t$-th iteration is the selected operation, denoted by $a_t^o = o_t$.\n\\textbf{ii) State:} its state at the $t$-th iteration is the combination of $Rep(\\mathcal{F}_{t-1})$ and the representation of the  feature group selected by the previous agent, denoted by $s^o_t = Rep(\\mathcal{F}_{t-1}) \\oplus Rep(\\mathcal{C}_{t-1}^1)$, where $\\oplus$ indicates the concatenation operation. \n\\textbf{iii) Reward:} The selected operation will be used to generate new features by feature crossing. We combine such new features with the original feature set to form a new feature set.\nThus, the feature set at the $t$-th iteration is $\\mathcal{F}_t = \\mathcal{F}_{t-1} \\cup g_t$, where $g_t$ is the generated new features.\nThe reward of this iteration is the improvement in the utility score of the feature set compared with the previous iteration, denoted by $\\mathcal{R}(s_t^o,a_t^o) = U(\\mathcal{F}_t|y) - U(\\mathcal{F}_{t-1}|y)$.\n\n\n%\\textbf{MDP for Operator Selection} can be formulated by $\\mathcal{M}^o =\\{\\mathcal{S}^o,\\mathcal{A}^o,\\mathcal{P}^o, \\mathcal{R}^o\\}.$  The key components are as follows: (1) \\textbf{State.} The $t$-th state in $\\mathcal{S}^{o}$ is the combination of $\\phi(\\mathcal{F}_t)$ and the representation of  the selected feature cluster $\\mathcal{C}_t^1$ in $\\mathcal{M}^1$ at the $t$-th iteration, denoted by $s^o_t =  concat(\\phi(\\mathcal{F}_t),  \\phi(\\mathcal{C}^1_t)$). (2) \\textbf{Action.} The $t$-th action is the selected operator from the operator set, denoted by $a^o_t=o_t$. (3) \\textbf{Transition.} The same to $\\mathcal{M}_1$, the transition probability from $s^o_t$ to $s^o_{t+1}$ via action $a^o_t$ is 1, denoted by $\\mathcal{P}^o(s^o_t,a^o_t,s^o_{t+1})=1$.\n\n\n\\noindent\\ul{\\textit{Learning the Selection Agent of Feature Group 2.}} \nThe feature group agent 2 will iteratively select the best meta feature group 2 for feature generation.\nIts learning system includes: \\textbf{i) Action:} its action at the $t$-th iteration is the meta feature group 2 selected from the clustered feature groups of the previous iteration, denoted by $a_t^2 = \\mathcal{C}^2_t$.\n\\textbf{ii) State:} its state at the $t$-th iteration is combination of $Rep(\\mathcal{F}_{t-1})$, $Rep(\\mathcal{C}_{t-1}^1)$ and the vectorized representation of the  operation selected by the operation agent, denoted by $s_t^2 = Rep(\\mathcal{F}_{t-1})\\oplus Rep(\\mathcal{C}_{t-1}) \\oplus Rep(o_t)$.\n\\textbf{iii) Reward:} its reward at the $t$-th iteration is improvement of the feature set utility and the feedback of the downstream task, denoted by $\\mathcal{R}(s_t^2, a_t^2) = U(\\mathcal{F}_t|y)-U(\\mathcal{F}_{t-1}|y)+V_{A_t}$, where $V_A$ is the performance (e.g., F1) of a downstream predictive task. \n\n\n\n\n\\smallskip\n\\noindent\\textbf{State Representation of a Feature Group and an Operation.}\n%As indicated previously, vectorized representations of the feature set, feature group, or operation are required for agent training. These representations should describe the unique properties of the feature set, feature group, or operation. First, considering the fact that the feature group is a subset of the feature set, they are identical in nature, so we adopt the same representation method for them. \nWe propose to map a feature group to a vector that characterizes the \\textbf{State} of the given feature group. \nIn detail, given a feature group $\\mathcal{F}$, we first calculate the descriptive statistics (\\textit{i.e.} count, standard deviation, minimum, maximum, first , second , and third quartile) column by column.\nThen, row by row, we calculate the descriptive statistics of the outcome of the previous step to obtain the descriptive matrix that shape is $\\mathbb{R}^{7\\times 7}$.\nAfter that, we obtain the feature feature's representation $Rep(\\mathcal{F})\\in \\mathbb{R}^{1\\times 49}$ by flatting the descriptive matrix.\nA fixed-size state vector is produced by the representation method, which accommodates the varying size of the feature set at each  generation iteration.\nSecond, for the operation, we use the one-hot encoding as its representation $Rep(o)$.\n\n\n\n\\smallskip\n\\noindent\\textbf{Solving the Optimization Problem.} \nWe train the three agents by maximizing the discounted and cumulative reward during the iterative feature generation process.\nIn other words, we encourage the cascading agents to collaborate to generate a feature set that is independent, informative, and performs well in the downstream task.\nTo accomplish this goal, we minimize the temporal difference error $\\mathcal{L}$ converted from the Bellman equation, given by:\n\\begin{equation}\n\\label{Q_update}\n    \\mathcal{L} = Q(s_{t},a_t) - (\\mathcal{R}(s_t, a_t) + \\gamma * \\text{max}_{a_{t+1}}Q(s_{t+1},a_{t+1})),\n\\end{equation}\nwhere $\\gamma \\in [0\\sim 1]$ is the discounted factor; $Q$ denotes the $Q$ function estimated by deep neural networks. After agents converge, we expect to discover the optimal policy $\\pi^*$ that can choose the most appropriate action (\\textit{i.e.} feature group or operation) based on the state via the Q-value, which can be formulated as follows:\n\\begin{equation}\n    \\label{rl_policy}\n    \\pi^*(a_t|s_t) = \\text{argmax}_a Q(s_t,a).\n\\end{equation}\n\n\n\n% Owing to the feature set and feature group \n\n% We adopt the same approach to learn the representation of the feature set and feature gr\n\n% For the representation method of \n\n% % As mentioned above, we need to convert the feature set, feature group, or operation to vectorized representations for agent training.\n\n% state representations suitable for different MDPs.\n% We adopt the same transformation process for the feature set and feature cluster.\n% To be simple, we denote a feature matrix (\\textit{i.e.} feature set, feature cluster) as $\\mathcal{X}$.\n% We first calculate the descriptive statistics (\\textit{i.e.} count, standard deviation, minimum, maximum, first , second , and third quartile) of  $\\mathcal{X}$ row by row.\n% Then, column by column, we calculate the descriptive statistics of the outcome of the preceding step to obtain $\\mathcal{\\check{X}} \\in \\mathbb{R}^{7\\times 7}$.\n% Finally, we flat $\\mathcal{\\check{X}}$ to obtain $\\phi(\\mathcal{X}) \\in \\mathbb{R}^{1\\times 49}$.\n% This transformation procedure not only preserves the characteristics of the feature matrix, but also accommodates the variable size of the feature set during the feature generation process.\n% For the transformation of operator, we use the one-hot encoding as its representation $\\phi(o)$.\n\n\n\n\n%\\textbf{MDP for Feature Cluster2 Selection} can be defined by $\\mathcal{M}^2 =\\{\\mathcal{S}^2,\\mathcal{A}^2,\\mathcal{P}^2, \\mathcal{R}^2\\}.$ The key components are as follows: (1) \\textbf{State.} The $t$-th state in $\\mathcal{S}^2$ is the combination of $\\phi(\\mathcal{F}_t)$, $\\phi(\\mathcal{C}^1_t)$, and the representation of the selected operator $o_t$ in $\\mathcal{M}^o$ at the $t$-th iteration, denoted by $s_t^2 = concat(\\phi(\\mathcal{F}_t),\\phi(\\mathcal{C}^1_t), \\phi(o_t))$.  (2) \\textbf{Action.} The $t$-th action is the selected feature cluster2, denoted by $a_t^2=\\mathcal{C}_t^2$.  (3) \\textbf{Transition.} The same to $\\mathcal{M}_1$ and $\\mathcal{M}_2$, the transition probability from $s_t^2$ to $s_{t+1}^2$ via action $a_t^2$ is 1, denoted by $\\mathcal{P}^2(s_t^2,a_t^2,s_{t+1}^2)=1$.\n\n% \\textbf{Sharing Reward.} Since the goal of feature generation is to improve the performance of downstream tasks, we consider this improvement to be the reward and allocate it to $\\mathcal{M}_1, \\mathcal{M}_2, \\mathcal{M}_3$.\n% The reward definition at the $t$-th iteration is as follows:\n\n% \\begin{equation}\n% \\left\\{\n%              \\begin{array}{lr}\n%              \\widetilde{r}_t=V_{A_t} - V_{A_{t-1}}, &  \\\\\n%              r_t^1 = \\lambda_1 \\cdot \\widetilde{r}_t,\\ r_t^o = \\lambda_o \\cdot \\widetilde{r}_t,\\ r_t^2 = \\lambda_2 \\cdot \\widetilde{r}_t, &\n%              \\\\\n%              \\lambda_1 + \\lambda_o + \\lambda_2 = 1, &  \n%              \\end{array}\n% \\right.\n% \\label{equ:reward}\n% \\end{equation}\n% where $A$ indicates the downstream task; $V$ denotes the quantitative performance indicator of $A$; \n% $\\widetilde{r}_t$ is improvement of $A$ at current iteration.\n% $r_t^1, r_t^o, r_t^2$ are the reward signals of $\\mathcal{M}_1, \\mathcal{M}_o, \\mathcal{M}_2$ respectively.\n% $\\lambda_1, \\lambda_o, \\lambda_2$ are the allocation coefficients of $r_t^1, r_t^o, r_t^2$ respectively.\n\n\n\n\n\n% To address the three MDPs, we develop a cascading agents structure based on Deep Q Network (DQN) ~\\cite{mnih2015human}.\n% As illustrated in Figure \\ref{fig:agents}, feature cluster agent1, operator agent, and feature cluster agent2 are used to resolve $\\mathcal{M}^1, \\mathcal{M}^o, \\mathcal{M}^2$ respectively. \n% The cascading relationship between the three agents is as described previously.\n% The policy network is the decision-making module in the three agents. It produces possible selections based on the current state via exploring and exploiting.\n% At the $t$-th iteration, the cascading agents finally output two feature clusters $\\mathcal{C}_t^1$, $\\mathcal{C}_t^2$  and one operator $o_t$.\n\n% \\textbf{Optimization for Reinforcement Learning.}\n% We want to train a policy for cascading agents that optimizes the discounted and cumulative reward $\\sum_{t=0}^{\\infty} \\gamma^t \\cdot  r_t$, where $\\gamma$ is the discount constant between 0 and 1 that ensures the sum converges.\n% In other words, the optimization goal is to maximize downstream task performance improvement during the iterative feature generation process.\n% To accomplish this, we minimize the temporal difference error $\\mathcal{L}$ converted from the bellman equation.\n% The formula of $\\mathcal{L}$ is as follows:\n% \\begin{equation}\n% \\label{Q_update}\n%     \\mathcal{L} = Q(s_t,a_t) - (\\mathcal{R}(s_t, a_t) + \\gamma * \\text{max}_{a_{t+1}}Q(s_{t+1},a_{t+1})),\n% \\end{equation}\n% where $Q$ denotes the $Q$ function estimated by deep neural networks. After agents converge, we expect to discover the optimal policy $\\pi^*$ that can choose the most appropriate action (\\textit{i.e.} feature cluster or operator) based on the state via the Q-value. The process can be  formulated as follows:\n% \\begin{equation}\n%     \\label{rl_policy}\n%     \\pi^*(a_t|s_t) = \\text{argmax}_a Q(s_t,a).\n% \\end{equation}\n\n"
                },
                "subsection 3.4": {
                    "name": "Group-wise Feature Generation",
                    "content": "\n\\label{group_generation}\nWe found that using group-level crossing can generate more features each time, and, thus, accelerate exploration speed, augment reward feedback by adding significant amount of features, and effectively learn policies.  \nThe selection results of our reinforcement learning system include \\textbf{two generation scenarios}: (1) selected are a binary operation and two feature groups;  (2) selected are a unary operation and two feature groups. \nHowever, a challenge arises: what are the most effective generation strategy for the two scenarios?\nWe next propose two  strategies for the two scenarios. \n\n\n\\textbf{Scenario 1: Cosine Similarity Prioritized Feature-Feature Crossing.} \nWe highlight that it is more likely to generate an informative features by crossing two features that are less overlapped in terms of information. \nWe propose a simple yet efficient strategy, that is, to select the top K dissimilar feature pairs between two feature groups. \nSpecifically, we first cross two selected feature groups to prepare feature pairs. We then compute the cosine similarities of all feature pairs. Later, we rank and select the top K dissimilar feature pairs. Finally, we apply the operation to the top K selected feature pairs to generate K new features. \n\n%Given two feature groups and a binary operator, that the generated features are most significant when the interacted features include the least redundant information. Additionally, due to the fact that the number of features in the selected features clusters is usually different, we use cosine similarity to select topK dissimilar feature pairs to align such clusters. Specifically, for two feature clusters $C_t^1, C_t^2$, we first traverse all features in the both clusters and calculate the cosine similarity between feature pairs from different feature clusters. Then, we rank the feature pairs in descending order by their cosine similarity value and select the topK pairs. After that, the features from $\\mathcal{C}_t^1$ in these topK pairs construct a new cluster $\\mathcal{\\check{C}}_t^1$ and the features from $\\mathcal{C}_t^2$ in these topK pairs build a new cluster $\\mathcal{\\check{C}}_t^2$. Finally, we apply the operator $o_t$ to these new clusters to generate new features, which can be defined as $\\mathcal{F}^g_t = o_t(\\mathcal{\\check{C}}_t^1, \\mathcal{\\check{C}}_t^2)$.\n\n\n%When the operator is binary, we use this interaction manner. We presume that the generated features are most significant when the interacted features include the least redundant information. Additionally, due to the fact that the number of features in the selected features clusters is usually different, we use cosine similarity to select topK dissimilar feature pairs to align such clusters. Specifically, for two feature clusters $C_t^1, C_t^2$, we first traverse all features in the both clusters and calculate the cosine similarity between feature pairs from different feature clusters. Then, we rank the feature pairs in descending order by their cosine similarity value and select the topK pairs. After that, the features from $\\mathcal{C}_t^1$ in these topK pairs construct a new cluster $\\mathcal{\\check{C}}_t^1$ and the features from $\\mathcal{C}_t^2$ in these topK pairs build a new cluster $\\mathcal{\\check{C}}_t^2$. Finally, we apply the operator $o_t$ to these new clusters to generate new features, which can be defined as $\\mathcal{F}^g_t = o_t(\\mathcal{\\check{C}}_t^1, \\mathcal{\\check{C}}_t^2)$.\n\n%By interacting with two selected feature clusters via the selected operator, we generate meaningful features to refine the feature space. The manner in which group-group feature interaction can be categorized into two categories depending on the type of the selected operator.  Without sacrificing generality, we will illustrate the two  manners of interaction using the $t$-th iteration.\n\n\n\\textbf{Scenario 2: Relevance Prioritized  Unary Feature Generation. }\nWhen selected  are an unary operation and two feature groups, we directly apply the operation to the feature group that is more relevant to target labels. Given a feature group $C$, we use the average mutual information between all the features in $C$ and the prediction target $y$ to quantify the relevance between the feature group and the prediction targets, which is given by: \n$\n   rel =  \\frac{1}{|\\mathcal{C}|}\\sum_{f_i\\in \\mathcal{C}} MI(f_i,y) \n$, \nwhere $MI$ is a function of mutual information. \nAfter the more relevant feature group is identified, we apply the unary operation to each feature of the feature group to generate new features. \n\n\\iffalse\n\\textbf{Mutual Information based Feature Cluster Selection.} When the operator is unary, we employ this interaction manner. To generate more significant features, we attempt to chose the feature cluster that is more closely related to the target label. Specifically, for two feature clusters $\\mathcal{C}^1_t, \\mathcal{C}^2_t$, we  uses the average relevance between $\\mathcal{C}^1_t$ and $y$ to subtract the average relevance between $\\mathcal{C}^2_t$ and $y$ to calculate the difference $\\delta$. The process can be formulated as:\n\\begin{equation}\n    \\label{differ}\n    \\delta = ( \\frac{1}{|\\mathcal{C}^1_t|}\\sum_{f^1_i\\in \\mathcal{C}^1_t} MI(f^1_i,y) -  \\frac{1}{|\\mathcal{C}^2_t|}\\sum_{f^2_j\\in \\mathcal{C}^2_t} MI(f^2_j,y)),\n\\end{equation}\nwhere $|\\mathcal{C}^1_t|, |\\mathcal{C}^2_t|$ are the number of features in $\\mathcal{C}^1_t, \\mathcal{C}^2_t$ respectively; \n$f_i^1, f_j^2$ are the single feature in $\\mathcal{C}^1_t, \\mathcal{C}^2_t$ respectively. if $\\delta \\geq 0$, which means $\\mathcal{C}_t^1$ is more relevant to $y$ than $\\mathcal{C}_t^2$.  Thus, we apply the selected operator $o_t$ to $\\mathcal{C}^1_t$ to generate new features, which is defined as $\\mathcal{F}_t^g = o_t(\\mathcal{C}^1_t)$.\nOtherwise, $\\mathcal{C}_2^t$ is our selection, we generate new features by applying $o_t$ to $\\mathcal{C}^2_t$, which is formulated as $\\mathcal{F}_t^g = o_t(\\mathcal{C}^2_t)$.\n\\fi\n\n\\textbf{Post-generation Processing.} \nAfter feature generation, we combine the newly generated features with  the original feature set to form an updated feature set, which will be fed into a downstream task to evaluate predictive performance.\nSuch performance is exploited as reward feedback to update the policies of the three cascading agents in order to optimize the next round of feature generation.\nTo prevent feature number explosion during the iterative generation process, \nwe use a feature selection step to control feature size. \nWhen the size of the new feature set exceeds a feature size tolerance threshold, we leverage the K-best feature selection method to reduce the feature size. Otherwise, we don't perform feature selection. \nWe use the tailored new feature set as the original feature set of the next iteration. \n\nFinally, when the maximum number of iterations is reached, the algorithm  returns the optimal feature set $\\mathcal{F^{*}}$ that has the best downstream performance over the entire exploration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.1cm}\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n%We detail experimental setup in detail, and then present experimental results.\n \n% \\iffalse\n% We design experiments to answer the following research questions: \\textbf{Q1.} Is the performance of our feature generation framework (GRFG) superior to existing baseline models? \n% \\textbf{Q2.} {\\color{red} How is the learning process of GRFG interpreted?}\n% \\textbf{Q3.} How does each component in GRRG impact its performance?\n% \\textbf{Q4.} Is M-clustering more effective than traditional clustering algorithms in improving feature generation?\n% \\textbf{Q5.} Is GRFG robust with different machine learning models as downstream tasks?\n% \\textbf{Q6.} Is the feature generation process of GRFG traceable and explainable?\n% \\fi\n\n",
                "subsection 4.1": {
                    "name": "Experimental Setup",
                    "content": "\n",
                    "subsubsection 4.1.1": {
                        "name": "Data Description",
                        "content": "\nWe used 24 publicly available datasets from UCI~\\cite{uci}, \n LibSVM ~\\cite{libsvm},\n Kaggle ~\\cite{kaggle}, and OpenML ~\\cite{openml} to conduct experiments.\nThe 24 datasets involves 14 classification tasks and 10 regression tasks.\nTable \\ref{table_overall_perf} shows the statistics of the data. \n\n% \\vspace{-0.1cm}\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Evaluation Metrics",
                        "content": "\nWe used the F1-score to evaluate the recall and precision of classification tasks. \nWe used  1-relative absolute error (RAE) to evaluate the accuracy of regression tasks. \nSpecifically, $\\text{1-RAE} = 1 - \\frac{\\sum_{i=1}^{n} |y_i-\\check{y}_i|}{\\sum_{i=1}^{n}|y_i-\\bar{y}_i|}$, where $n$ is the number of data points, $y_i, \\check{y}_i, \\bar{y}_i$ respectively denote golden standard  target values, predicted target values, and the mean of golden standard targets. \n%Whether in classification or regression, the higher the value of the evaluation metric, the more accurate the model.\n\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Baseline Algorithms",
                        "content": "\n\\label{baseline}\nWe compared our method with five widely-used feature generation methods: \n(1) \\textbf{RDG} randomly selects feature-operation-feature pairs for feature generation; \n(2) \\textbf{ERT} is a expansion-reduction method, that applies operations to each feature to expand the feature space and selects significant features from the larger space as new features. \n(3) \\textbf{LDA}~\\cite{blei2003latent} extracts latent features via matrix factorization.\n(4) \\textbf{AFT}~\\cite{horn2019autofeat}  is an enhanced ERT implementation that iteratively explores feature space and adopts a multi-step feature selection relying on L1-regularized linear models.\n(5) \\textbf{NFS}~\\cite{chen2019neural} mimics feature transformation trajectory for each feature and optimizes the entire feature generation process through reinforcement learning.\n(6) \\textbf{TTG} ~\\cite{khurana2018feature} records the feature generation process using a transformation graph, then uses reinforcement learning to explore the graph to determine the best feature set.\n\nBesides, we developed four variants of GRFG to validate the impact of each technical component: \n(i) $\\textbf{GRFG}^{-c}$ removes the clustering step of GRFG and generate features by feature-operation-feature based crossing, instead of group-operation-group based crossing. \n(ii) $\\textbf{GRFG}^{-d}$ utilizes the euclidean distance as the measurement of M-Clustering.\n(iii) $\\textbf{GRFG}^{-u}$ selects a feature group at random from the feature group set, when the  operation is unary.\n(iv) $\\textbf{GRFG}^{-b}$ randomly selects features from the larger feature group to align two feature groups when the operation is binary. \nWe adopted random forest, a robust ensemble predictor, as the downstream ML model, in order to ensure the changes of results are mainly caused by the feature space reconstruction, not randomness or variance of the predictor.\nWe performed 5-fold stratified cross-validation in all  experiments, instead of a simple 70\\%-30\\% split. \n\n\n\n"
                    },
                    "subsubsection 4.1.4": {
                        "name": "Hyperparameters, Source Code and Reproducibility",
                        "content": "\nThe operation set consists of \\textit{square root, square, cosine, sine, tangent, exp, cube, log, reciprocal, sigmoid, plus, subtract, multiply, divide}.\nWe limited iterations (epochs) to 30, with each iteration consisting of 15 exploration steps.\nWhen the number of generated features is twice of the original feature set size, we performed feature selection to control feature size.\nIn GRFG, all agents were constructed using a DQN network with two linear layers activated by the RELU function.\nWe optimized DQN using the Adam optimizer with a 0.01 learning rate, and set the limit of the experience replay memory as 32 and the batch size as 8.\nThe parameters of all the baseline models are defined based on the recommendations of corresponding papers.\nFor other detailed experimental settings, please check the code released in the Abstract section.\n\n"
                    },
                    "subsubsection 4.1.5": {
                        "name": "Environmental Settings",
                        "content": "\nAll experiments were conducted on the Ubuntu 18.04.5 LTS operating system, Intel(R) Core(TM) i9-10900X CPU@ 3.70GHz, and 1 way SLI RTX 3090 and 128GB of RAM, with the framework of Python 3.8.5 and PyTorch 1.8.1.\n\n\\vspace{-0.1cm}\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Experimental Results",
                    "content": "\n\n\n\n\n",
                    "subsubsection 4.2.1": {
                        "name": "Overall Comparison",
                        "content": "\nThis experiment aims to answer:\n\\textit{Can our method effectively construct quality feature space and improve a downstream task?} \nTable \\ref{table_overall_perf} shows the comparison of our method with six baseline models on the 24 datasets in terms of F1 score or 1-RAE.\nWe observed that GRFG ranks first on most datasets and ranks second on ``Credit Default'' and ``SpamBase''.\nThe underlying driver is that the personalized feature crossing strategy in GRFG considers feature-feature distinctions when generating new features.\nBesides, the observation that GRFG outperforms random-based  (RDG) and expansion-reduction-based (ERG, AFT) methods \nshows that the agents can share states and rewards in a cascading fashion, and, thus learn an effective policy to select optimal crossing features and operations.\nMoreover, because our method is a self-learning end-to-end framework, users can treat it as a tool and easily apply it to different datasets regardless of implementation details.\nThus, compared with state-of-the-art baselines (NFS, TTG), our method is more practical and automated in real application  scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "Study of the impact of each technical component",
                        "content": "\n\\label{study_lp}\nThis experiment aims to answer:\n\\textit{How does each component in GRFG impact its performance?}\nWe developed four variants of GRFG (Section \\ref{baseline}). \nFigure ~\\ref{ab_study} shows the comparison results in terms of F1 score or 1-RAE on two classification datasests (\\textit{i.e.} PimaIndian, German Credit) and two regression datasets (\\textit{i.e.} Housing Boston, Openml\\_589). \nFirst, we developed GRFG$^{-c}$ by removing the feature clustering step of GRFG. But, GRFG$^{-c}$ performs poorer than GRFG on all datasets. This shows the idea of group-level generation can augment reward feedback to help cascading agents learn better policies. \n%This generation way augments the reward signals of cascading agents, enabling them to learn a more effective policy for feature group and operation selection.\nSecond, we developed  GRFG$^{-d}$ by using euclidean distance as feature distance metric in the M-clustering of GRFG.\nThe superiority of GRFG over GRFG$^{-d}$ suggests that our distance describes group-level information  relevance and redundancy ratio in order to maximize information distinctness across feature groups and minimize it within a feature group. Such a distance can help GRFG generate more useful dimensions.\n%our proposed feature distance metric takes the information distinctness into account to produce more robust feature groups for feature generation.\nThird, we developed GRFG$^{-u}$ and GRFG$^{-b}$ by using random in the two feature generation scenarios (Section \\ref{group_generation}) of  GRFG.\nWe observed that GRFG$^{-u}$ and GRFG$^{-b}$ perform poorer than GRFG. This validates that crossing two distinct features and relevance prioritized generation can generate better features. \n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 4.2.3": {
                        "name": "Study of the impact of M-Clustering",
                        "content": "\nThis experiment aims to answer: \\textit{Is M-Clustering more effective in improving feature generation than classical clustering algorithms?}\nWe replaced the feature clustering algorithm in GRFG with KMeans, Hierarchical Clustering, DBSCAN, and Spectral Clustering respectively.\nWe reported the comparison results in terms of F1 score or 1-RAE on the datasets used in Section \\ref{study_lp}.\nFigure \\ref{differ_cluster} shows M-Clustering beats classical clustering algorithms on all datasets.\nThe underlying driver is that when feature sets change during generation, M-Clustering is more effective in minimizing information overlap of intra-group features and maximizing information distinctness of  inter-group features. So, crossing  the feature groups with distinct information is easier to generate  meaningful dimensions.\n\n"
                    },
                    "subsubsection 4.2.4": {
                        "name": "Robustness check of GRFG under different machine learning (ML) models.",
                        "content": "\nThis experiment is to answer:\n\\textit{Is GRFG robust when different ML models are used as a downstream task?}\nWe examined the robustness of GRFG by changing the ML model of a downstream task to Random Forest (RF), Xgboost (XGB), SVM, KNN, and Ridge Regression, respectively.\nFigure ~\\ref{differ_ml} shows the comparison results in terms of F1 score or 1-RAE on the datasets used in the Section \\ref{study_lp}.\nWe observed that GRFG robustly improves model performances regardless of the ML model used.\nThis observation indicates that GRFG can generalize well to various benchmark applications and ML models.\nWe found that  RF and XGB are the two most powerful and robust predictors over the four datasets, which is consistent with the finding in Kaggle.COM competition community. \nIntuitively,  the accuracy of RF and XGB usually represent the performance ceiling on modeling a dataset. It is hard to break the performance ceiling.\nBut, after using our method to reconstruct the data, we continue to significantly improve the accuracy of RF and XGB and break through the performance ceiling.   \nThis finding clearly validates the strong robustness of our method. \n\n\\iffalse\n\\textbf{\nWe observed that  SVM, KNN, and Ridge perform worse than RF and XGB, regardless of whether the feature space is refined by GRFG. But the amplitude of performance gain in the three models is larger than RF and XGB. A possible reason is that the predictive power of SVM, KNN, and Ridge is lower than RF and XGB.\nThus, the three models cannot perform effectively in the original feature space, but they have greater potential for performance enhancement.\nSo, with GRFG generates more meaningful features, the three models improve the model performance accordingly.\nHowever, their best performance is still lower than RF and XGB.\n}\n\\fi\n\n% \\begin{table}[!t]\n%   \\caption{Comparison of different feature size tolerance  thresholds in terms of F1 score or 1-RAE.}\n%   \\vspace{-0.4cm}\n%   \\begin{center}\n%   \\scalebox{0.85}{\n%     \\begin{tabular}{c|c|c|c|c|c|c}\n%       \\hline\n%       Dataset & $2|\\mathcal{F}|$ & $3|\\mathcal{F}|$ & $4|\\mathcal{F}|$ & $5|\\mathcal{F}|$ & $6|\\mathcal{F}|$ & $7|\\mathcal{F}|$ \\\\\n%       \\hline\n%       PimaIndian & 0.76049 & 0.76351 & 0.76452 & 0.76254& 0.76049 & 0.76576  \\\\ \\hline\n%       German Credit & 0.76151 & 0.76055 & 0.76713 & 0.76841 & 0.75997 & 0.75760  \\\\ \\hline\n%       Housing Boston & 0.65338 & 0.65365 & 0.65042 & 0.65245 & 0.65072 & 0.65108  \\\\ \\hline\n%      Openml\\_589 & 0.67921 & 0.67445 & 0.67396 & 0.67393 & 0.67320 & 0.67663 \\\\ \\hline\n%     \\end{tabular}\n%     }\n%   \\end{center}\n%   \\label{tab:enlarge_per}\n%   \\vspace{-0.4cm}\n% \\end{table}\n\n% \\begin{table}[!t]\n%   \\caption{Comparison of different feature size tolerance thresholds in terms of time cost (s).}\n%   \\vspace{-0.4cm}\n%   \\begin{center}\n%   \\scalebox{0.85}{\n%     \\begin{tabular}{c|c|c|c|c|c|c}\n%       \\hline\n%       Dataset & $2|\\mathcal{F}|$ & $3|\\mathcal{F}|$ & $4|\\mathcal{F}|$ & $5|\\mathcal{F}|$ & $6|\\mathcal{F}|$ & $7|\\mathcal{F}|$ \\\\\n%       \\hline\n%       PimaIndian & 249.449 & 307.915& 452.228 & 520.209 & 565.166 & 532.884  \\\\ \\hline\n%       German Credit & 623.878 & 641.886 & 684.092 &  690.927 &  696.491 & 712.028  \\\\ \\hline\n%       Housing Boston & 521.788 & 655.598 & 750.080 & 795.326 & 881.826 & 894.314  \\\\ \\hline\n%      Openml\\_589 & 1268.98 & 1545.87 & 1570.83 & 1662.85 & 1689.68 & 1796.93 \\\\ \\hline\n%     \\end{tabular}\n%     }\n%   \\end{center}\n%   \\label{tab:enlarge_time}\n%   \\vspace{-0.35cm}\n% \\end{table}\n\n\n\n% \\subsubsection{Study of the impact of the feature size tolerance threshold.}\n% This experiment aims to answer: \\textit{How does the feature size tolerance threshold influence feature generation? }\n% GRFG utilizes a feature size tolerance threshold  to avoid having an excessively large  feature space.\n% Let the size of the original feature space be $|\\mathcal{F}|$, we increased the threshold value from $2|\\mathcal{F}|$,  to $3|\\mathcal{F}|$, to $4|\\mathcal{F}|$, to $5|\\mathcal{F}|$, to $6|\\mathcal{F}|$, to $7|\\mathcal{F}|$, respectively.\n% Table ~\\ref{tab:enlarge_per} and Table ~\\ref{tab:enlarge_time} show the comparison results in terms of model performance (F1 score, 1-RAE) and time costs for feature generation.\n% We found that the model performance is quite good when the threshold is $2|\\mathcal{F}|$ and does not greatly improve with an increase in threshold value.\n% The underlying drivers are: i) the relevance prioritized feature generation strategy can generate meaningful features via interacting distinct features. ii) the group-wise feature generation manner can provide clear reward feedback to cascading agents for learning smart policies.\n% Moreover, we noticed that as the threshold is increased, the time costs relatively grow.\n% A possible reason is that as the threshold is increased, the feature space expands, thus our method has to explore more feature interactions, incurring additional time costs.\n\n\n% \\vspace{-0.5cm}\n"
                    },
                    "subsubsection 4.2.5": {
                        "name": "Study of the traceability and explainability of GRFG",
                        "content": "\nThis experiment aims to answer: \\textit{Can GRFG generate an explainable feature space? Is this generation process traceable?}\nWe identified the top 10 essential features for prediction in both the original and reconstructed feature space using the Housing Boston dataset to predict housing prices with random forest regression.\nFigure ~\\ref{explain} shows the model performances in the central parts of each sub-figure. \nThe texts associated with each pie chart describe the feature name.\nIf the feature name does not include an operation, the corresponding feature is original; otherwise, it is a generated feature. \nThe larger the pie area is, the more essential the corresponding feature is.\nWe observed that the GRFG-reconstructed feature space greatly enhances the model performance by 20.9$\\%$ and the generated features cover 60$\\%$ of the top 10 features.\nThis indicates that GRFG generates informative features to refine the feature space.\nMoreover, we can explicitly trace and explain the source and effect of a feature by checking its name.\nFor instance,  the feature ``lstat'' measures the percentage of the lower status populations in a house, which is negatively related to housing prices.\nThe most essential feature in the reconstructed feature space is \n``lstat*lstat'' that is generated by applying a ``multiply'' operation to ``lstat''.\nThis shows the generation process is traceable and the relationship between ``lstat'' and  housing prices is non-linear.\n%Domain experts can undertake more in-depth analysis using these explicit and traceable feature names.\n\n\n\n\n\n\n\n\n\n\n\n% \\vspace{-0.1cm}\n"
                    }
                }
            },
            "section 5": {
                "name": "Related Works",
                "content": "\n\\noindent\\textbf{Reinforcement Learning (RL)} is the study of how intelligent agents should act in a given environment in order to maximize the expectation of cumulative rewards ~\\cite{sutton2018reinforcement}.\nAccording to the learned policy, we may classify reinforcement learning algorithms into two categories: value-based and policy-based.\nValue-based algorithms (\\textit{e.g.} DQN ~\\cite{mnih2013playing}, Double DQN ~\\cite{van2016deep}) estimate the value of the state or state-action pair for action selection.\nPolicy-based algorithms (\\textit{e.g.} PG ~\\cite{sutton2000policy}) learn a probability distribution to map state to action for action selection.\nAdditionally, an actor-critic reinforcement learning framework is proposed to  incorporate the advantages of value-based and policy-based algorithms ~\\cite{schulman2017proximal}.\nIn recent years, RL has been applied to many domains (e.g. spatial-temporal data mining, recommended systems) and achieves great achievements ~\\cite{wang2022reinforced,wang2022multi}.\nIn this paper, we formulate the selection of feature groups and operation as MDPs and propose a new cascading agent structure to resolve these MDPs.\n% Our framework's simple model structure enables it to be applied in many scenarios easily and flexibly.\n\n\n\n\\noindent\\textbf{Automated Feature Engineering} aims to enhance the feature space through feature generation and feature selection in order to improve the performance of machine learning models ~\\cite{chen2021techniques}.\nFeature selection is to remove redundant features and retain important ones, whereas feature generation is to create and add meaningful variables. \n% Automated feature engineering  can be achieved by combining feature generation and feature selection with a certain algorithmic structure.\n% yang1997comparative,hall1999feature, yang1998feature,narendra1977branch,\n\\ul{\\textit{Feature Selection}} approaches include:\n(i) filter methods (\\textit{e.g}., univariate selection \\cite{forman2003extensive}, correlation based selection \\cite{yu2003feature}), in which features are ranked by a specific score like redundancy, relevance;  (ii) wrapper methods (\\textit{e.g.}, Reinforcement Learning ~\\cite{ liu2021efficient}, Branch and Bound~\\cite{ kohavi1997wrappers}), in which the optimized feature subset is identified by a search strategy under a predictive task;  (iii) embedded methods (\\textit{e.g.}, LASSO \\cite{tibshirani1996regression}, decision tree \\cite{sugumaran2007feature}), in which selection is part of the optimization objective of a predictive task. \n\\ul{\\textit{Feature Generation}} methods include: (i) latent representation learning based methods, e.g. latent dirichlet allocation ~\\cite{blei2003latent}, deep factorization machine ~\\cite{guo2017deepfm}, deep representation learning ~\\cite{bengio2013representation}. \nDue to the latent feature space generated by these methods, it is hard to trace and explain the feature extraction process.\n(ii) feature transformation based methods, which use arithmetic or aggregate operations to generate new features ~\\cite{khurana2018feature,chen2019neural}.\nThese approaches have two weaknesses: (a) ignore feature-feature heterogeneity  among different feature pairs; (b) grow exponentially when the number of exploration steps increases.\nCompared with prior literature, our personalized feature crossing strategy captures the feature distinctness,  cascading agents learn effective feature interaction policies, and  group-wise generation manner accelerates feature generation.\n\n\n\n\n\n\n% \\vspace{-0.2cm}\n"
            },
            "section 6": {
                "name": "Conclusion Remarks",
                "content": "\nWe present a group-wise reinforcement feature generation (GRFG) framework for optimal and explainable representation space reconstruction to  improve the performances of predictive models. \nThis framework nests feature generation and selection in order to iteratively reconstruct a recognizable and size-controllable feature space via feature-crossing.\nSpecifically, first, we decompose the process of selecting crossing features and operations into three MDPs and develop a new cascading agent structure for it.\nSecond, we provide two feature generation strategies based on cosine similarity and mutual information  to deal with two generation scenarios following cascading selection.\nThird, we suggest a group-wise feature generation manner to \nefficiently generate features and augment the rewards of cascading agents.\nTo accomplish this, we propose a new feature clustering algorithm (M-Clustering) to produce robust feature groups from an information theory perspective.\nThrough extensive experiments,\nwe can find that GRFG is effective at refining the feature space and shows competitive results compared to other baselines.\n% Additionally, due to the end-to-end generation manner, GRFG is applicable to multiple applications.\nMoreover, GRFG can provide traceable  routes for feature generation, which improves the explainability of the refined feature space.\nIn the future, we aim to include the pre-training technique into GRFG  to further enhance feature generation.\n\n\\vspace{-0.12cm}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n% \\noindent \\textbf{Summary.}\n% %summarize your contributions: problem formulation, proposed algorithm, outcome benefits\n% In this paper, we study the problem of improving the training efficiency of reinforced feature selection (RFS). We propose a traverse strategy to simplify the multi-agent formulation of the RFS to a single-agent framework, an implementation of Monte Carlo method under the framework, and two strategies to improve the efficiency of the framework. \n\n% \\noindent\\textbf{Theoretical Implications.}\n% %modeling algorithm perspective what are the takeaways messages that can be generalized to other ML tasks\n% The single-agent formulation reduces the requirement of computational resources, the early stopping strategy improves the training efficiency, the decision history based traversing strategy diversify the training process, and the interactive reinforcement learning accelerates the training process without changing the optimal policy.\n\n% \\noindent \\textbf{Practical Implications.}\n% %real world  feature selection practice perspective what are the takeaway messages\n% Experiments show that the Monte Carlo method with the traverse strategy can significantly reduce the hardware occupation in practice, the decision history based traverse strategy can improve performance of the traverse strategy, the interactive reinforcement learning can improve the training of the framework.\n\n% \\noindent \\textbf{Limitations and Future Work.}\n% %our method is not perfect, there are limitations here are something we can do better\n% Our method can be further improved from the following aspects: 1) The framework can be adapted into a parallel framework, where more than one (but much smaller than the feature number) agents work together to finish the traverse; 2) Besides reward level, the interactive reinforcement learning can obtain advice from action level and sampling level.\n% 3) The framework can be implemented on any other reinforcement learning frameworks, e.g., deep Q-network, actor critic and proximal policy optimization (PPO).\n\n\n\\bibliographystyle{ACM-Reference-Format}\n%\\vspace{-0.3cm}\n\\bibliography{acm,Yanjie}\n\n\n% \\clearpage\n% \\input{appendix}\n\n"
            }
        },
        "tables": {
            "table_overall_perf": "\\begin{table*}[!htbp]\n\\centering\n\\vspace{-0.2cm}\n\\caption{Overall performance comparison. `C' for classification and `R' for regression.}\n\\vspace{-0.3cm}\n\\label{table_overall_perf}\n\\setlength{\\tabcolsep}{2.5mm}{\n% {|l|l|l|l|l|l|l|l|l|l|l|l|}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}\n\\hline\nDataset            & Source   & C/R & Samples & Features & RDG  & ERG & LDA & AFT   & NFS   & TTG  & GRFG           \\\\ \\hline\nHiggs Boson        & UCIrvine & C  & 50000   & 28       & 0.683 & 0.674 & 0.509 & 0.711 & 0.715 & 0.705 & \\textbf{0.719} \\\\ \\hline\nAmazon Employee    & Kaggle   & C   & 32769   & 9        & 0.744 & 0.740 & 0.920  & 0.943 & 0.935 & 0.806 & \\textbf{0.946} \\\\ \\hline\nPimaIndian         & UCIrvine & C   & 768     & 8        & 0.693 & 0.703 & 0.676  & 0.736 & 0.762 & 0.747 & \\textbf{0.776} \\\\ \\hline\nSpectF             & UCIrvine & C   & 267     & 44       & 0.790 & 0.748 & 0.774  & 0.775 & 0.876 & 0.788 & \\textbf{0.878} \\\\ \\hline\nSVMGuide3  & LibSVM & C & 1243 & 21 & 0.703 & 0.747 & 0.683 & 0.829 & 0.831 & 0.766 & \\textbf{0.850} \\\\ \\hline\nGerman Credit      & UCIrvine & C   & 1001    & 24       & 0.695 & 0.661 & 0.627  & 0.751 & 0.765 & 0.731 & \\textbf{0.772} \\\\ \\hline\nCredit Default     & UCIrvine & C   & 30000   & 25       & 0.798 & 0.752 & 0.744  & 0.799 & 0.799 & \\textbf{0.809} & 0.800 \\\\ \\hline\nMessidor\\_features & UCIrvine & C   & 1150    & 19       & 0.673 & 0.635 & 0.580  & 0.678 & 0.746 & 0.726 & \\textbf{0.757} \\\\ \\hline\nWine Quality Red   & UCIrvine & C   & 999     & 12       & 0.599 & 0.611 & 0.600  & 0.658 & 0.666 & 0.647 & \\textbf{0.686} \\\\ \\hline\nWine Quality White & UCIrvine & C   & 4900    & 12       & 0.552 & 0.587 & 0.571  & 0.673 & 0.679 & 0.638 & \\textbf{0.685} \\\\ \\hline\nSpamBase           & UCIrvine & C   & 4601    & 57       & 0.951 & 0.931 & 0.908  & 0.951 & 0.955 & \\textbf{0.961} & 0.958 \\\\ \\hline\nAP-omentum-ovary            & OpenML & C   & 275    & 10936        & 0.711 & 0.705 & 0.117  & 0.783 & 0.804 & 0.795 & \\textbf{0.818} \\\\ \\hline\nLymphography       & UCIrvine & C   & 148     & 18       & 0.654 & 0.638 & 0.737 & 0.833 & 0.859 & 0.846 & \\textbf{0.866} \\\\ \\hline\nIonosphere         & UCIrvine & C   & 351     & 34       & 0.919 & 0.926 & 0.730  & 0.827 & 0.949 & 0.938 & \\textbf{0.960} \\\\ \\hline\nBikeshare DC       & Kaggle   & R   & 10886   & 11       & 0.483 & 0.571 & 0.494  & 0.670 & 0.675 & 0.659 & \\textbf{0.681} \\\\ \\hline\nHousing Boston     & UCIrvine & R   & 506     & 13       & 0.605 & 0.617 & 0.174 & 0.641 & 0.665 & 0.658 & \\textbf{0.684} \\\\ \\hline\nAirfoil            & UCIrvine & R   & 1503    & 5        & 0.737 & 0.732 & 0.463  & 0.774 & 0.771 & 0.783 & \\textbf{0.797} \\\\ \\hline\nOpenml\\_618        & OpenML   & R   & 1000    & 50       & 0.415 & 0.427  & 0.372 & 0.665 & 0.640 & 0.587 & \\textbf{0.672} \\\\ \\hline\nOpenml\\_589        & OpenML   & R   & 1000    & 25       & 0.638 & 0.560 & 0.331  & 0.672 & 0.711 & 0.682 & \\textbf{0.753} \\\\ \\hline\nOpenml\\_616        & OpenML   & R   & 500     & 50       & 0.448 & 0.372 & 0.385  & 0.585 & 0.593 & 0.559 & \\textbf{0.603} \\\\ \\hline\nOpenml\\_607        & OpenML   & R   & 1000    & 50       & 0.579 & 0.406 & 0.376  & 0.658 & 0.675 & 0.639 & \\textbf{0.680} \\\\ \\hline\nOpenml\\_620        & OpenML   & R   & 1000    & 25       & 0.575 & 0.584 & 0.425 & 0.663 & 0.698 & 0.656 & \\textbf{0.714} \\\\ \\hline\nOpenml\\_637        & OpenML   & R   & 500     & 50       & 0.561 & 0.497 & 0.494  & 0.564 & 0.581 & 0.575 & \\textbf{0.589} \\\\ \\hline\nOpenml\\_586        & OpenML   & R   & 1000    & 25       & 0.595 & 0.546 & 0.472  & 0.687 & 0.748 & 0.704 & \\textbf{0.783} \\\\ \\hline\n\\end{tabular}}\n\\vspace{-0.2cm}\n\\end{table*}"
        },
        "figures": {
            "fig:intro_frame": "\\begin{figure}[!htbp]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{img/intro_frame.pdf}\n    \\vspace{-0.2cm}\n    \\captionsetup{justification=centering}\n    \\vspace{-0.5cm}\n    \\caption{We want to uncover the optimal feature space that is explainable and performs optimally in a downstream ML task by iteratively reconstructing the feature space.\n    }\n    \\vspace{-0.25cm}\n    \\label{fig:intro_frame}\n\\end{figure}",
            "fig:framework": "\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{img/Framework.pdf}\n    \\vspace{-0.2cm}\n    \\captionsetup{justification=centering}\n    \\vspace{-0.33cm}\n    \\caption{An overview of GRFG. First, we cluster the feature set into feature groups. \n    Second, we employ cascading agents to select two feature groups and one operation.\n    Next, we conduct group-group feature interaction to generate new features and combine them with original features to create a new feature set.\n    Then, the updated feature set is fed into a downstream task to assess the selection process of cascading agents for parameter update.\n    Meanwhile, we adopt feature selection to control the size of feature set and iterate the process until the best feature set is discovered or the maximum number of iterations reaches.\n    }\n    \\vspace{-0.25cm}\n    \\label{fig:framework}\n\\end{figure*}",
            "fig:agents": "\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{img/agents.pdf}\n    \\vspace{-0.2cm}\n    \\captionsetup{justification=centering}\n    \\vspace{-0.2cm}\n    \\caption{The cascading agents are comprised of the feature group agent1, the operation agent, and the feature group agent2. They collaborate to choose two candidate feature groups and a single operation.}\n    \\vspace{-0.2cm}\n    \\label{fig:agents}\n\\end{figure}",
            "fig:state_repr": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{img/state_repr.pdf}\n    \\vspace{-0.3cm}\n    \\captionsetup{justification=centering}\n    \\vspace{-0.2cm}\n    \\caption{State Representation. Given a feature group $\\mathcal{F}$ consisting of several features, we calculate the descriptive statistics of $\\mathcal{F}$ column-by-column, then row-by-row to get a meta descriptive statistics matrix.\n    Then, we flat the matrix to obtain the state representation vector $Rep(\\mathcal{F})$.}\n    \\vspace{-0.2cm}\n    \\label{fig:state_repr}\n\\end{figure}",
            "ab_study": "\\begin{figure*}[htbp]\n% \\vspace{-0.1cm}\n\\centering\n\\subfigure[PimaIndian]{\n\\includegraphics[width=4.4cm]{img/ab_pima_indian.pdf}\n}\n\\hspace{-3mm}\n\\subfigure[German Credit]{ \n\\includegraphics[width=4.4cm]{img/ab_german_credit.pdf}\n}\n\\hspace{-3mm}\n% \\vspace{-4mm}\n\\subfigure[Housing Boston]{\n\\includegraphics[width=4.4cm]{img/ab_housing_boston.pdf}\n}\n\\hspace{-3mm}\n\\subfigure[Openml\\_589]{ \n\\includegraphics[width=4.4cm]{img/ab_openml_589.pdf}\n}\n\\vspace{-0.35cm}\n\\caption{Comparison of different GRFG variants in terms of F1 or 1-RAE.}\n\\label{ab_study}\n\\vspace{-0.45cm}\n\\end{figure*}",
            "differ_cluster": "\\begin{figure*}[htbp]\n\\centering\n\\subfigure[PimaIndian]{\n\\includegraphics[width=4.4cm]{img/c_pima_indian.pdf}\n}\n\\hspace{-3mm}\n\\subfigure[German Credit]{ \n\\includegraphics[width=4.4cm]{img/c_german_credit.pdf}\n}\n\\hspace{-3mm}\n% \\vspace{-4mm}\n\\subfigure[Housing Boston]{\n\\includegraphics[width=4.4cm]{img/c_housing_boston.pdf}\n}\n\\hspace{-3mm}\n\\subfigure[Openml\\_589]{ \n\\includegraphics[width=4.4cm]{img/c_openml_589.pdf}\n}\n\\vspace{-0.35cm}\n\\caption{Comparison of different clustering algorithms in terms of F1 or 1-RAE.}\n\\label{differ_cluster}\n\\vspace{-0.cm}\n\\end{figure*}",
            "differ_ml": "\\begin{figure*}[htbp]\n\\vspace{-0.15cm}\n\\centering\n\\subfigure[PimaIndian]{\n\\includegraphics[width=4.4cm]{img/differ_pima_indian.pdf}\n}\n\\hspace{-3mm}\n\\subfigure[German Credit]{ \n\\includegraphics[width=4.4cm]{img/differ_german_credit.pdf}\n}\n\\hspace{-3mm}\n% \\vspace{-4mm}\n\\subfigure[Housing Boston]{\n\\includegraphics[width=4.4cm]{img/differ_housing_boston.pdf}\n}\n\\hspace{-3mm}\n\\subfigure[Openml 589]{ \n\\includegraphics[width=4.4cm]{img/differ_openml_589.pdf}\n}\n\\vspace{-0.35cm}\n\\caption{Comparison of different machine learning models in terms of F1 or 1-RAE.}\n\\label{differ_ml}\n\\vspace{-0.4cm}\n\\end{figure*}",
            "explain": "\\begin{figure}[!t]\n\\centering\n\\subfigure[Original Feature Space]{\n\\includegraphics[width=4.0cm]{img/explain_hb_ori.pdf}\n}\n\\hspace{0mm}\n\\subfigure[GRFG-reconstructed Feature Space]{ \n\\includegraphics[width=4.0cm]{img/explain_hb_afg.pdf}\n}\n\\vspace{-0.4cm}\n\\caption{Top10 features for prediction in the original and GRFG-reconstructed feature space.}\n\\label{explain}\n\\vspace{-0.5cm}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{objective}\n    \\mathcal{F}^{*} = argmax_{\\mathcal{\\hat{F}}}( V_A(\\mathcal{\\hat{F}},y)),\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\vspace{-0.18cm}\n    \\label{fea_dis}\n    dis(\\mathcal{C}_i, \\mathcal{C}_j) =\n    \\frac{1}{|\\mathcal{C}_i|\\cdot|\\mathcal{C}_j|}\n    \\sum_{f_i\\in \\mathcal{C}_i}\\sum_{f_j\\in \\mathcal{C}_j}\\frac{|MI(f_i,y)-MI(f_j,y)|}{MI(f_i,f_j)+\\epsilon},\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    U(\\mathcal{F}|y) = -\\frac{1}{|\\mathcal{F}|^2}\\sum_{f_i, f_j \\in \\mathcal{F}} MI(f_i, f_j) + \\frac{1}{|\\mathcal{F}|}\\sum_{f\\in \\mathcal{F}}MI(f,y),\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{Q_update}\n    \\mathcal{L} = Q(s_{t},a_t) - (\\mathcal{R}(s_t, a_t) + \\gamma * \\text{max}_{a_{t+1}}Q(s_{t+1},a_{t+1})),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\label{rl_policy}\n    \\pi^*(a_t|s_t) = \\text{argmax}_a Q(s_t,a).\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\label{differ}\n    \\delta = ( \\frac{1}{|\\mathcal{C}^1_t|}\\sum_{f^1_i\\in \\mathcal{C}^1_t} MI(f^1_i,y) -  \\frac{1}{|\\mathcal{C}^2_t|}\\sum_{f^2_j\\in \\mathcal{C}^2_t} MI(f^2_j,y)),\n\\end{equation}"
        }
    }
}