{
    "meta_info": {
        "title": "PETR: Position Embedding Transformation for Multi-View 3D Object  Detection",
        "abstract": "In this paper, we develop position embedding transformation (PETR) for\nmulti-view 3D object detection. PETR encodes the position information of 3D\ncoordinates into image features, producing the 3D position-aware features.\nObject query can perceive the 3D position-aware features and perform end-to-end\nobject detection. PETR achieves state-of-the-art performance (50.4% NDS and\n44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.\nIt can serve as a simple yet strong baseline for future research. Code is\navailable at \\url{https://github.com/megvii-research/PETR}.",
        "author": "Yingfei Liu, Tiancai Wang, Xiangyu Zhang, Jian Sun",
        "link": "http://arxiv.org/abs/2203.05625v3",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "Accepted by ECCV 2022. Code is available at  \\url{https://github.com/megvii-research/PETR}"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n3D object detection from multi-view images is appealing due to its low cost in autonomous driving system. Previous works~\\cite{chen2016monocular,mousavian20173d,wang2021fcos3d,park2021dd3d,wang2022pgd} mainly solved this problem from the perspective of monocular object detection. Recently, DETR~\\cite{carion2020detr} has gained remarkable attention due to its contribution on end-to-end object detection. In DETR~\\cite{carion2020detr}, each object query represents an object and interacts with the 2D features in transformer decoder to produce the predictions (see Fig.~\\ref{arch_comparison}(a)).\n% Recently, DETR3D~\\cite{wang2022detr3d} is introduced to perform end-to-end 3D object detection.\n% Similar to DETR ~\\cite{carion2020detr} (see Fig.~\\ref{arch_comparison}(a)), each object query represents a 3D object in DETR3D. \nSimply extended from DETR~\\cite{carion2020detr} framework, DETR3D~\\cite{wang2022detr3d} provides an intuitive solution for end-to-end 3D object detection. The 3D reference point, predicted by object query, is projected back into the image spaces by the camera parameters and used to sample the 2D features from all camera views (see Fig.~\\ref{arch_comparison}(b)). The decoder will take the sampled features and the queries as input and update the representations of object queries.\n\nHowever, such 2D-to-3D transformation in DETR3D~\\cite{wang2022detr3d} may introduce several problems. First, the predicted coordinates of reference point may not that accurate, making the sampled features out of the object region. Second, only the image feature at the projected point will be collected, which fails to perform the representation learning from global view. Also, the complex feature sampling procedure will hinder the detector from practical application.\n% Such complex 2D-to-3D transformation will hinder the representation learning from global view and practical application.\n% So we wonder if it is possible that we directly query the 3D predictions without the online 2D-to-3D transformation and feature sampling.\nThus, building an end-to-end 3D object detection framework without the online 2D-to-3D transformation and feature sampling is still a remaining problem.\n\n\n\n% Simple methods often scale much better. \nIn this paper, we aim to develop a simple and elegant framework based on DETR~\\cite{carion2020detr} for 3D object detection. We wonder if it is possible that we transform the 2D features from multi-view into 3D-aware features. In this way, the object query can be directly updated under the 3D environment. Our work is inspired by these advances in implicit neural representation~\\cite{hu2019metasr,chen2021liff,mildenhall2020nerf}. \n% One direct problem is that how to transform the 2D features from multi-views into the 3D-aware features. \nIn MetaSR~\\cite{hu2019metasr} and LIFF~\\cite{chen2021liff}, the high-resolution (HR) RGB values are generated from low-resolution (LR) input by encoding HR coordinates information into the LR features. In this paper, we try to transform the 2D features from multi-view images into the 3D representation by encoding 3D position embedding (see Fig.~\\ref{arch_comparison}(c)).\n\nTo achieve this goal, the camera frustum space, shared by different views, is first discretized into meshgrid coordinates. The coordinates are then transformed by different camera parameters to obtain the coordinates of 3D world space. \n% Then 2D image features extracted from backbone and 3D coordinates are input to a simple 3D position encoder to encode the 3D position information into 2D features, producing the 3D position-aware features. \nThen 2D image features extracted from backbone and 3D coordinates are input to a simple 3D position encoder to produce the 3D position-aware features.\nThe 3D position-aware features will interact with the object queries in transformer decoder and the updated object queries are further used to predict the object class and the 3D bounding boxes.  \n\nThe proposed PETR architecture brings many advantages compared to the DETR3D~\\cite{wang2022detr3d}. It keeps the end-to-end spirit of original DETR~\\cite{carion2020detr} while avoiding the complex 2D-to-3D projection and feature sampling. During inference time, the 3D position coordinates can be generated in an offline manner and served as an extra input position embedding. It is relatively easier for practical application.\n\nTo summarize, our contributions are:\n\\begin{itemize}\n\\item We propose a simple and elegant framework, termed PETR, for multi-view 3D object detection. The multi-view features are transformed into 3D domain by encoding the 3D coordinates. Object queries can be updated by interacting with the 3D position-aware features and generate 3D predictions.\n\\item A new 3D position-aware representation is introduced for multi-view 3D object detection. A simple implicit function is introduced to encode the 3D position information into 2D multi-view features.\n\\item Experiments show that PETR achieves state-of-the-art performance (\\textbf{50.4\\%} NDS and \\textbf{44.1\\%} mAP) on standard nuScenes dataset and ranks 1$st$ place on 3D object detection leaderboard. \n\\end{itemize}\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Transformer-based Object Detection",
                    "content": "\nTransformer~\\cite{vaswani2017attention} is an attention block that widely applied to model the long-range dependency. In transformer, the features are usually added with position embedding, which provides the position information of the image~\\cite{dosovitskiy2020image,wu2021rethinking,liu2021swin}, sequence~\\cite{gehring2017convolutional,vaswani2017attention,devlin2018bert,dai2019transformer,yang2019xlnet}, and video~\\cite{bertasius2021space,li2021improved,wu2022memvit}. Transformer-XL~\\cite{dai2019transformer} uses the relative position embedding to encode the relative distance of the pairwise tokens. ViT~\\cite{dosovitskiy2020image} adds the learned position embedding to the patch representations that encode distance of different patches. MViT~\\cite{li2021improved} decomposes the distance computation of the relative position embedding and model the space-time structure.\n\nRecently, DETR~\\cite{carion2020detr} involves the transformer into 2D object detection task for end-to-end detection.\nIn DETR~\\cite{carion2020detr}, each object is represented as an object query which interacts with 2D images features through transformer decoder. However, DETR~\\cite{carion2020detr} suffers from the slow convergence. \\cite{sun2021rethinking} attributes the slow convergence to the cross attention mechanism and designs a encoder-only DETR. Furthermore, many works accelerate the convergence by adding position priors. SMAC~\\cite{gao2021fast} predicts 2D Gaussian-like weight map as spatial prior for each query.\nDeformable DETR~\\cite{zhu2020deformable} associates the object queries with 2D reference points and proposes deformable cross-attention to perform sparse interaction. \\cite{wang2021anchor,meng2021conditional,liu2022dab} generate the object queries from anchor points or anchors that use position prior for fast convergence. Extended from DETR~\\cite{zhu2020deformable}, SOLQ~\\cite{dong2021solq} uses object queries to perform classification, box regression and instance segmentation simultaneously. \n\n"
                },
                "subsection 2.2": {
                    "name": "Vision-based 3D Object Detection",
                    "content": "\nVision-based 3D object detection is to detect 3D bounding boxes from camera images. \n% Thus, how to solve the inconsistency between the input 2D data and the output 3D predictions is the key. \nMany previous works~\\cite{chen2016monocular,mousavian20173d,kehl2017ssd,ku2019monocular,simonelli2019disentangling,jorgensen2019monocular,brazil2019m3d,wang2021fcos3d,wang2022pgd} perform 3D object detection in the image view. \nM3D-RPN~\\cite{brazil2019m3d} introduces the depth-aware convolution, which learns position-aware features for 3D object detection.\nFCOS3D~\\cite{wang2021fcos3d} transforms the 3D ground-truths to image view and extends FCOS~\\cite{tian2019fcos} to predict 3D cuboid parameters. PGD~\\cite{wang2022pgd} follows the FCOS3D~\\cite{wang2021fcos3d} and uses a probabilistic representation to capture the uncertainty of depth. It greatly alleviates the depth estimation problem while introducing more computation budget and larger inference latency. DD3D~\\cite{park2021dd3d} shows that depth pre-training on large-scale depth dataset can significantly improve the performance of 3D object detection. \n\n% Recently, several works attempt to conduct the 3D object detection in 3D world space. \n% OFT~\\cite{roddick2018orthographic} and CaDDN~\\cite{reading2021categorical} map the monocular image features into the bird’s eye view (BEV) and detect 3D objects in BEV space. ImVoxelNet~\\cite{rukhovich2022imvoxelnet} builds a 3D volume in 3D world space and samples multi-view features to obtain the voxel representation. Then 3D convolutions and domain-specific heads are used to detect objects in both indoor and outdoor scenes. Similar to CaDDN~\\cite{reading2021categorical}, BEVDet~\\cite{huang2021bevdet} employs the Lift-Splat-Shoot~\\cite{philion2020lift} to transform 2D multi-view features into BEV representation. With the BEV representation, a CenterPoint~\\cite{yin2021center} head is used to detect 3D objects in an intuitive way. Following DETR~\\cite{carion2020detr}, DETR3D~\\cite{wang2022detr3d} represents 3D objects as object queries. The 3D reference points, generated from object queries, are repeatedly projected back to all camera views and sample the 2D features. Our method also detects 3D objects in 3D world space, but in a simple and effective manner. Without complicated feature transformation, we encode the 3D position information into 2D features, producing the 3D position-aware features. The object queries can directly interact with such 3D position-aware representation. \n\nRecently, several works attempt to conduct the 3D object detection in 3D world space. \nOFT~\\cite{roddick2018orthographic} and CaDDN~\\cite{reading2021categorical} map the monocular image features into the bird’s eye view (BEV) and detect 3D objects in BEV space. ImVoxelNet~\\cite{rukhovich2022imvoxelnet} builds a 3D volume in 3D world space and samples multi-view features to obtain the voxel representation. Then 3D convolutions and domain-specific heads are used to detect objects in both indoor and outdoor scenes. Similar to CaDDN~\\cite{reading2021categorical}, BEVDet~\\cite{huang2021bevdet} employs the Lift-Splat-Shoot~\\cite{philion2020lift} to transform 2D multi-view features into BEV representation. With the BEV representation, a CenterPoint~\\cite{yin2021center} head is used to detect 3D objects in an intuitive way.\nFollowing DETR~\\cite{carion2020detr}, DETR3D~\\cite{wang2022detr3d} represents 3D objects as object queries. The 3D reference points, generated from object queries, are repeatedly projected back to all camera views and sample the 2D features.\n\nBEV-based methods tend to introduce the Z-axis error, resulting in poor performance for other 3D-aware tasks (e.g., 3D lane detection). DETR-based methods can enjoy more benefits from end-to-end modeling with more training augmentations. Our  method is DETR-based that detects 3D objects in a simple and effective manner. We encode the 3D position information into 2D features, producing the 3D position-aware features. The object queries can directly interact with such 3D position-aware representation without projection error. \n\n% Our method also detects 3D objects in 3D world space, but in a simple and effective manner. Without complicated feature transformation, we encode the 3D position information into 2D features, producing the 3D position-aware features. The object queries can directly interact with such 3D position-aware representation. \n\n% In our personal opinion, DETR-based methods will be better in the long term since it can enjoy more benefits from end-to-end modeling with more training augmentations. Recently, 2D DETR-based detector DINO achieves SoAT detection performance on the COCO dataset, outperforming previous two-stage frameworks. 3D object detection may follow the success of 2D detection. Moreover, in our practice, BEV-based methods tend to introduce the Z-axis error, resulting in poor performance for other 3D-aware tasks (e.g., 3D lane detection).\n\n\n\n% We thank R3 for the constructive feedback. BEV-based methods (\\eg, BEVDet~[\\href{https://arxiv.org/abs/2112.11790}{3}]) explicitly transform the multi-view features into BEV representation by ``lift'', ``splat'' and ``shoot''~[\\href{https://arxiv.org/abs/2008.05711}{4}]. \n% % image plane to BEV space by depth-estimation and camera intrinsic/extrinsic. ``none''\n% DETR-based approaches (\\eg, DETR3D~[\\href{https://arxiv.org/abs/2110.06922}{5}]) represent each object as query and achieve end-to-end modeling with Hungarian matching. In our personal opinion, DETR-based methods will be better in the long term since it can enjoy more benefits from end-to-end modeling with more training augmentations. Recently, 2D DETR-based detector DINO~[\\href{https://arxiv.org/abs/2203.03605}{6}] achieves SoAT detection performance on the COCO dataset~[\\href{https://paperswithcode.com/sota/object-detection-on-coco}{7}], outperforming previous two-stage frameworks. 3D object detection may follow the success of 2D detection. Moreover, in our practice, BEV-based methods tend to introduce the Z-axis error, resulting in poor performance for other 3D-aware tasks (\\eg, 3D lane detection~[\\href{https://arxiv.org/abs/2203.11089}{8}]). \n\n\n"
                },
                "subsection 2.3": {
                    "name": "Implicit Neural Representation",
                    "content": "\nImplicit neural representation (INR) usually maps the coordinates to visual signal by a multi-layer perceptron (MLP). It is a high efficiency way for modeling 3D objects~\\cite{park2019deepsdf,chen2019learning,mescheder2019occupancy}, 3D scenes~\\cite{mildenhall2020nerf,sitzmann2019scene,chabra2020deep,peng2020convolutional} and 2D images~\\cite{hu2019metasr,chen2021liff,tancik2020fourier,sitzmann2020implicit}. NeRF~\\cite{mildenhall2020nerf} employs a fully-connected network to denote a specific scene. To synthesize a novel view, the 5D coordinates along camera rays are input to the network as queries and output the volume density and view-dependent emitted radiance. In MetaSR~\\cite{hu2019metasr} and LIFF~\\cite{chen2021liff}, the HR coordinates are encoded into the LR features and HR images of arbitrary size can be generated. Our method can be regarded as an extension of INR in 3D object detection. The 2D images are encoded with 3D coordinates to obtain 3D position-aware features. The anchor points in 3D space are transformed to object queries by a MLP and further interact with 3D position-aware features to predict the corresponding 3D objects.\n\n%\\subsection{Line numbering}\n\n"
                }
            },
            "section 3": {
                "name": "Method",
                "content": "\n\n\n\n% \\subsection{Revisiting DETR}\n\n",
                "subsection 3.1": {
                    "name": "Overall Architecture",
                    "content": "\nFig.~\\ref{architecture} shows the overall architecture of the proposed PETR. Given the images $I=\\{ I_i \\in R^{3 \\times H_I \\times W_I}, i=1,2,\\dots, N \\}$ from $N$ views, the images are input to the backbone network (e.g. ResNet-50~\\cite{he2016resnet}) to extract the 2D multi-view features  $F^{2d}=\\{F^{2d}_i\\in  R^{C \\times H_F \\times W_F}, i=1,2,\\dots, N\\}$. In 3D coordinates generator, the camera frustum space is first discretized into a 3D meshgrid. Then the coordinates of meshgrid are transformed by camera parameters and generate the coordinates in 3D world space. The 3D coordinates together with the 2D multi-view features are input to the 3D position encoder, producing the 3D position-aware features $F^{3d}=\\{F^{3d}_i\\in  R^{C \\times H_F \\times W_F}, i=1,2,\\dots, N\\}$. The 3D features are further input to the transformer decoder and interact with the object queries, generated from query generator. The updated object queries are used to predict the object class as well as the 3D bounding boxes.\n\n% \\subsection{Position Embedding Transformation}\n\n"
                },
                "subsection 3.2": {
                    "name": "3D Coordinates Generator",
                    "content": "\n% The 3D coordinates can be generated in an offline manner and served as an extra input for the network. \nTo build the relation between the 2D images and 3D space, we project the points in camera frustum space to 3D space since the points between these two spaces are one-to-one assignment. Similar to DGSN~\\cite{chen2020dsgn}, we first discretize the camera frustum space to generate a meshgrid of size $(W_F,H_F,D )$. Each point in the meshgrid can be represented as $p^{m}_j = (u_j\\times d_j, v_j\\times d_j, d_j, 1)^T$, where $ (u_j, v_j) $ is a pixel coordinate in the image, $d_j$ is the depth value along the axis orthogonal to the image plane. Since the meshgrid is shared by different views, the corresponding 3D coordinate $p^{3d}_{i,j} = (x_{i,j}, y_{i,j}, z_{i,j}, 1)^T$ in 3D world space can be calculated by reversing 3D projection:\n% \\begin{equation}\\label{eq1}\n% \\left[\n% \\begin{aligned}\n% x_j\\\\y_j\\\\z_j\\\\1\n% \\end{aligned}\n% \\right] = T^{-1} \\left[\n% \\begin{aligned}\n% u_j &\\times d_j\\\\\n% v_j &\\times d_j\\\\\n% &\\,d_j\\\\\n% &\\,\\,1\n% \\end{aligned}\n% \\right]\n% \\end{equation}\n\\begin{equation}\\label{eq1}\np^{3d}_{i,j} = K^{-1}_{i} p^{m}_{j}\n\\end{equation}\nwhere $K_i\\in R^{4 \\times 4}$ is the transformation matrix of $i$-th view that establish the transformation from 3D world space to camera frustum space. As illustrated in Fig. \\ref{architecture}, the 3D coordinates of all views cover the panorama of the scene after the transformation. We further normalize the 3D coordinates as in Eq. \\ref{eq2}.\n\\begin{equation}\\label{eq2}\n\\left\\{\n\\begin{aligned}\n&x_{i,j} = &(x_{i,j}-x_{min}) / &(x_{max}-x_{min})\\\\\n&y_{i,j} = &(y_{i,j}-y_{min}) / &(y_{max}-y_{min})\\\\\n&z_{i,j} = &(z_{i,j}-z_{min}) / &(z_{max}-z_{min})\n\\end{aligned}\n\\right.\n\\end{equation}\nwhere $[x_{min},y_{min},z_{min},x_{max},y_{max},z_{max}]$ is the region of interest (RoI) of 3D world space. The normalized coordinates of $H_F \\times W_F \\times D$ points are finally transposed as $P^{3d}=\\{ P^{3d}_i \\in R^{(D\\times4) \\times H_F\\times W_F }, i=1,2,\\dots, N \\}$.\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "3D Position Encoder",
                    "content": "\n% Motivated by Meta SR\\cite{hu2019metasr}\nThe purpose of the 3D position encoder is to obtain 3D features $F^{3d}=\\{F^{3d}_i\\in  R^{C \\times H_F \\times W_F}, i=1,2,\\dots, N\\}$ by associating 2D image features $F^{2d}=\\{F^{2d}_i\\in \\\\R^{C \\times H_F \\times W_F}, i=1,2,\\dots, N\\}$ with 3D position information. Analogously to Meta SR~\\cite{hu2019metasr}, the 3D position encoder can be formulated as:\n\\begin{equation}\\label{eq3}\nF^{3d}_i = \\psi(F^{2d}_i,P^{3d}_i), \\quad i=1,2,\\dots, N\n\\end{equation}\nwhere $\\psi(.)$ is the position encoding function that is illustrated in Fig.~\\ref{fig2}. Next, we describe the detailed implementation of $\\psi(.)$.\nGiven the 2D features $F^{2d}$ and 3D coordinates $P^{3d}$, the $P^{3d}$ is first feed into a multi-layer perception (MLP) network and transformed to the 3D position embedding (PE). Then, the 2D features $F^{2d}$ is transformed by a $1\\times1$ convolution layer and added with the 3D PE to formulate the 3D position-aware features. Finally, we flatten the 3D position-aware features as the key component of transformer decoder. \n% This is a simple but useful operator, as complex operators may destroy the mapping between feature and positions. Finally, we further flatten the 3D position-aware features for decoder. \n\n\\noindent \\textbf{Analysis on 3D PE:} To demonstrate the effect of 3D PE, we randomly select the PE at three points in the front view and compute the PE similarity between these three points and all multi-view PEs. As shown in Fig. \\ref{similar}, the regions close to these points tend to have the higher similarity. For example, when we select the left point in the front view, the right region of front-left view will have relatively higher response. It indicates that 3D PE implicitly establishes the position correlation of different views in 3D space.\n\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Query Generator and Decoder",
                    "content": "\n",
                    "subsubsection 3.4.1": {
                        "name": "Query Generator:",
                        "content": "\nOriginal DETR~\\cite{carion2020detr} directly uses a set of learnable parameters as the initial object queries. Following Deformable-DETR~\\cite{zhu2020deformable}, DETR3D~\\cite{wang2022detr3d} predicts the reference points based on the initialized object queries. To ease the convergence difficulty in 3D scene, similar to Anchor-DETR~\\cite{wang2021anchor}, we first initialize a set of learnable anchor points in 3D world space with uniform distribution from 0 to 1. Then the coordinates of 3D anchor points are input to a small MLP network with two linear layers and generate the initial object queries $Q_{0}$. In our practice, employing anchor points in 3D space can guarantee the convergence of PETR while adopting the setting in DETR or generating the anchor points in BEV space fail to achieve satisfying detection performance. For more details, please kindly refer to our experimental section.\n\n\\noindent \\textbf{Decoder:}\nFor the decoder network, we follow the standard transformer decoder in DETR~\\cite{carion2020detr}, which includes $L$ decoder layers. Here, we formulate the interaction process in decoder layer as:\n\\begin{equation}\\label{eq4}\n\\begin{aligned}\nQ_l = \\Omega_l(F^{3d}, Q_{l-1}), \\quad l=1, \\dots, L\n\\end{aligned}\n\\end{equation}\nwhere $\\Omega_l$ is the $l$-th layer of the decoder. $Q_{l} \\in R^{M\\times C}$ is the updated object queries of $l$-th layer. $M$ and $C$ are the number of queries and channels, respectively. In each decoder layer, object queries interact with 3D position-aware features through the multi-head attention and feed-forward network. After iterative interaction, the updated object queries have the high-level representations and can be used to predict corresponding objects.\n\n"
                    }
                },
                "subsection 3.5": {
                    "name": "Head and Loss",
                    "content": " The detection head mainly includes two branches for classification and regression. The updated object queries from the decoder are input to the detection head and predict the probability of object classes as well as the 3D bounding boxes. \n% $\\{ c_1, \\dots, c_M \\}$\n% $\\{b_1, \\dots, b_M | b_i = ({\\Delta x}_i, {\\Delta y}_i, {\\Delta z}_i, w_i, h_i, l_i, {\\theta}_i, {v_x}_i, {v_y}_i)\\}$.\nNote that the regression branch predicts the relative offsets with respect to the coordinates of anchor points. For fair comparison with DETR3D, we also adopt the focal loss~\\cite{lin2017focal} for classification and $L1$ loss for 3D bounding box regression. \nLet $y = (c,b)$ and $\\hat{y} = (\\hat{c},\\hat{b})$ denote the set of ground truths and predictions, respectively. The Hungarian algorithm~\\cite{kuhn1955hungarian} is used for label assignment between ground-truths and predictions. Suppose that $\\sigma$ is the optimal assignment function, then the loss for 3D object detection can be summarized as:\n\\begin{equation}\\label{eq5}\n\\begin{aligned}\nL(y,\\hat{y}) = \\lambda_{cls} * L_{cls}(c,\\sigma(\\hat{c})) + L_{reg}(b,\\sigma(\\hat{b}))\n\\end{aligned}\n\\end{equation}\n\nHere $L_{cls}$ denotes the focal loss for classification, $L_{reg}$ is $L1$ loss for regression. $\\lambda_{cls}$ is a hyper-parameter to balance different losses.\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n",
                "subsection 4.1": {
                    "name": "Datasets and Metrics",
                    "content": "\nWe validate our method on nuScenes benchmark~\\cite{caesar2020nuscenes}. NuScenes is a large-scale multimodal dataset that is composed of data collected from 6 cameras, 1 lidar and 5 radars. The dataset has 1000 scenes and is officially divided into 700/150/150 scenes for training/validation/testing, respectively. Each scene has 20s video frames and is fully annotated with 3D bounding boxes every 0.5s. Consistent with official evaluation metrics, we report nuScenes Detection Score (NDS) and mean Average Precision (mAP), along with mean Average Translation Error (mATE), mean Average Scale Error (mASE), mean Average Orientation Error(mAOE), mean Average\nVelocity Error(mAVE), mean Average Attribute Error(mAAE). \n\n"
                },
                "subsection 4.2": {
                    "name": "Implementation Details",
                    "content": "\nTo extract the 2D features, ResNet~\\cite{he2016resnet}, Swin-Transformer~\\cite{liu2021swin} or VoVNetV2~\\cite{lee2020centermask} are employed as the backbone network. The C5 feature (output of 5th stage) is upsampled and fused with C4 feature (output of 4th stage) to produce the P4 feature. The P4 feature with 1/16 input resolution is used as the 2D feature.\n% Following \\cite{li2017fully}, DETR add a dilation to the last stage of the backbone to obtain the DC5 feature map with larger resolution. However, it is difficult to apply the dilation convolution to a transformer network. To keep the consistency between different backbones, we upsample the C5 feature map and fused it with C4 feature map to obtain the  P4 feature map in \\cite{lin2017feature}. \nFor 3D coordinates generation, we sample 64 points along the depth axis following the linear-increasing discretization (LID) in CaDDN~\\cite{reading2021categorical}. We set the region to $[-61.2m, 61.2m]$ for the $X$ and $Y$ axis, and $[-10m, 10m]$ for $Z$ axis. The 3D coordinates in 3D world space are normalized to [0, 1]. Following DETR3D~\\cite{wang2022detr3d}, we set $\\lambda_{cls} = 2.0$ to balance classification and regression.\n\nPETR is trained using AdamW~\\cite{loshchilov2017decoupled} optimizer with weight decay of 0.01. The learning rate is initialized with $2.0\\times10^{-4}$ and decayed with cosine annealing policy~\\cite{loshchilov2016sgdr}. Multi-scale training strategy is adopted, where the shorter side is randomly chosen within [640, 900] and the longer side is less or equal to 1600. Following CenterPoint~\\cite{yin2021center}, the ground-truths of instances are randomly rotated with a range of [$-22.5^{\\circ}$, $22.5^{\\circ}$] in 3D space.  All experiments are trained for 24 epochs (2x schedule) on 8 Tesla V100 GPUs with a batch size of 8. No test time augmentation methods are used during inference.\n\n\\setlength{\\tabcolsep}{1pt}\n\n\\setlength{\\tabcolsep}{1pt}\n\n"
                },
                "subsection 4.3": {
                    "name": "State-of-the-art Comparison",
                    "content": "\nAs shown in Tab. \\ref{table:1}, we first compare the performance with state-of-the-art methods on nuScenes val set. It shows that PETR achieves the best performance on both NDS and mAP metrics. CenterNet~\\cite{zhou2019objects}, FCOS3D~\\cite{wang2021fcos3d} and PGD~\\cite{wang2022pgd} are typical monocular 3D object detection methods. When compare with FCOS3D~\\cite{wang2021fcos3d} and PGD~\\cite{wang2022pgd}, PETR with ResNet-101~\\cite{he2016resnet} surpasses them on NDS by 2.7\\% and 1.4\\%, respectively. However, PGD~\\cite{wang2022pgd} achieves relatively lower mATE because of the explicit depth supervision. Besides, we also compare PETR with multi-view 3D object detection methods DETR3D~\\cite{wang2022detr3d} and BEVDet~\\cite{huang2021bevdet}, which detect 3D objects in a unified view. Since the DETR3D~\\cite{wang2022detr3d} and BEVDet~\\cite{huang2021bevdet} follow different settings on the image size and backbone initialization, we individually compare the PETR with them for fair comparison. Our method outperforms them 0.8\\% and 1.4\\% in NDS, respectively. \n% It is worth noting that our method has a similar mATE compared to DETR3D, but a higher mAP. We suspect that this is because the attention in global view improves the classification performance.\n\n\n% \\setlength{\\tabcolsep}{1.4pt}\n\nTab.~\\ref{table:2} shows the performance comparison on nuScenes test set. Our method also achieves the best performance on both NDS and mAP.\nFor fair comparison with BEVDet~\\cite{huang2021bevdet}, PETR with Swin-S backbone is also trained with an image size of  $2112\\times768$. It shows that PETR surpasses BEVDet~\\cite{huang2021bevdet} by 3.6\\% in mAP and 1.8\\% in NDS, respectively. It is worth noting that PETR with Swin-B achieves a comparable performance compared to existing methods using external data. \nWhen using the external data, PETR with VOVNetV2~\\cite{lee2020centermask} backbone achieves 50.4\\% NDS and 44.1\\% mAP. As far as we know, PETR is the first vision-based method that surpasses 50.0\\% NDS.\n\nWe also perform the analysis on the convergence and detection speed of PETR. We first compare the convergence of DETR3D~\\cite{wang2022detr3d} and PETR (see Fig.~\\ref{analysis}(a)). PETR converges relatively slower than DETR3D~\\cite{wang2022detr3d} within the first 12 epochs and finally achieves much better detection performance. It indicates that PETR requires a relatively longer training schedule for fully convergence. We guess the reason is that PETR learns the 3D correlation through global attention while DETR3D~\\cite{wang2022detr3d} perceives the 3D scene within local regions. Fig.~\\ref{analysis}(b) further reports the detection performance and speed of PETR with different input sizes. The FPS is measured on a single Tesla V100 GPU. For the same image size (e.g., 1056$\\times$384), our PETR infers with 10.7 FPS compared to the BEVDet~\\cite{huang2021bevdet} with 4.2 FPS. Note that the speed of BEVDet~\\cite{huang2021bevdet} is measured on NVIDIA 3090 GPU, which is stronger than Tesla V100 GPU.\n\n\n\\setlength{\\tabcolsep}{2.5pt}\n\n\\setlength{\\tabcolsep}{1.4pt}\n\n\n% \\setlength{\\tabcolsep}{2pt}\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study",
                    "content": "\nIn this section, we perform the ablation study on some important components of PETR. All the experiments are conducted using single-level C5 feature of ResNet-50 backbone without the CBGS~\\cite{zhu2019class}.\n\n\\noindent \\textbf{Impact of 3D Position Embedding.}\nWe evaluate the impact of different position embedding (PE) (see Tab. \\ref{table:3}). When only the standard 2D PE in DETR is used, the model can only converge to 6.9\\% mAP. Then we add the multi-view prior (convert the view numbers into PE) to distinguish different views and it brings a slight improvement. When only using the 3D PE generated by 3D coordinates, PETR can directly achieve 30.5\\% mAP. It indicates that 3D PE provides a strong position prior to perceive the 3D scene. In addition, the performance can be improved when we combine the 3D PE with both 2D PE and multi-view prior. It should be noted that the main improvements are from the 3D PE and the 2D PE/multi-view prior can be selectively used in practice.\n% Finally, the multi-view position embedding is added, and the best performance is obtained.\n\n\\noindent \\textbf{3D Coordinate Generator.}\nIn 3D coordinates generator, the perspective view in camera frustum space is discretized into 3D meshgrid. The transformed coordinates in 3D world space are further normalized with a region of interest (RoI). Here, we explore the effectiveness of different discretization methods and RoI range (see Tab.~\\ref{table:lid}). The Uniform discretization (UD) shows similar performance compared to the linear-increasing discretization (LID). We also tried several common ROI regions and the RoI range of $(-61.2m, -61.2m, -10.0m, 61.2m, 61.2m,$ $ 10.0m)$ achieves better performance than others.\n\n\\noindent \\textbf{3D Position Encoder.}\nThe 3D position encoder is used to encode the 3D position into the 2D features. Here we first explore the effect of the multi-layer perception (MLP) that converts the 3D coordinates into 3D position embedding. It can be seen in Tab.~\\ref{table:4}(a) that the network with a simple MLP can improve the performance by 4.8\\% and 5.3\\% on NDS and mAP compared to the baseline without MLP (aligning the channel number of 2D features to $D\\times4$). When using two $3\\times3$ convolution layers, the model will not converge as the $3\\times3$ convolution destroys the correspondence between 2D feature and 3D position. Furthermore, we compare different ways to fuse the 2D image features with 3D PE in Tab.~\\ref{table:4}(b). The concatenation operation achieves similar performance compared to addition while surpassing the multiply fusion.\n\n\\noindent \\textbf{Query Generator.}\nTab.~\\ref{table:4}(c) shows the effect of different anchor points to generate queries. Here, we compare four types of anchor points: ``None'', ``Fix-BEV'', ``Fix-3D'' and ``Learned-3D''. Original DETR (``None'') directly employs a set of learnable parameters as object queries without anchor points. The global feature of object query fail to make the model converge. ``Fix-BEV'' is the fixed anchor points are generated with the number of $39 \\times 39$ in BEV space. ``Fix-3D'' means the fixed anchor points are with the number of $16 \\times 16 \\times 6$ in 3D world space. ``Learned-3D'' are the learnable anchor points defined in 3D space. We find the performance of both ``Fix-BEV'' and ``Fix-3D'' are lower than learned anchor points. We also explore the number of anchor points (see Tab.~\\ref{table:4}(d)), which ranges from 600 to 1500. The model achieve the best performance with 1500 anchor points. Considering of the computation cost is increasing with the number of anchor points, we simply use 1500 anchor points to make a trade-off.\n\n\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Visualization",
                    "content": "\nFig.~\\ref{vis_result} shows some qualitative detection results. The 3D bounding boxes are projected and drawn in BEV space as well as image view. As shown in the BEV space, the predicted bounding boxes are close to the ground-truths. This indicates that our method achieves good detection performance. \n% As observed in front-right view, the driver in the car is also detected, which is reasonable.\nWe also visualize the attention maps generated from an object query on multi-view images. As shown in Fig. \\ref{attention}, the object query tends to pay attention to the same object, even in different views. It indicates that 3D position embedding can establish the position correlation between different views. Finally, we provide some failure cases (see Fig.~\\ref{badcase}). The failure cases are marked by red and green circles. The red circles show some small objects that are not detected. \n% The car away from the camera is often a small object, which may be overlooked by the detector. \nThe objects in green circle are wrongly classified. The wrong detection mainly occurs when different vehicles share high similarity on appearance.\n\n"
                }
            },
            "section 5": {
                "name": "Conclusions",
                "content": "\nThe paper provides a simple and elegant solution for multi-view 3D object detection. By the 3D coordinates generation and position encoding, 2D features can be transformed into 3D position-aware feature representation. Such 3D representation can be directly incorporated into query-based DETR architecture and achieves end-to-end detection. It achieves state-of-the-art performance and can serve as a strong baseline for future research.\n\n",
                "subsection 5.0": {
                    "subsubsection 5.0.1": {
                        "name": "Acknowledgements:",
                        "content": " This research was supported by National Key R\\&D Program of China (No. 2017YFA0700800) and Beijing Academy of Artificial Intelligence (BAAI).\n\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n% \\bibliographystyle{unsrt}\n\\bibliographystyle{splncs04}\n\\bibliography{egbib}\n"
                    }
                }
            }
        },
        "figures": {
            "arch_comparison": "\\begin{figure*}[t]\n\t\\centering  \n\t\\begin{subfigure}{.32\\textwidth}\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=\\textwidth]{./figs/intro_1.png}\n\t\t\t\\caption{DETR}\n\t\t\\end{subfigure}\n\t\\begin{subfigure}{.32\\textwidth}\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=\\textwidth]{./figs/intro_2.png}\n\t\t\t\\caption{DETR3D}\n\t\t\\end{subfigure}\n\t\\begin{subfigure}{.32\\textwidth}\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=\\textwidth]{./figs/intro_3.png}\n\t\t\t\\caption{PETR}\n\t\t\\end{subfigure}\n\t\\caption{Comparison of DETR, DETR3D, and our proposed PETR. (a) In DETR, the object queries interact with 2D features to perform 2D detection. (b) DETR3D repeatedly projects the generated 3D reference points into image plane and samples the 2D features to interact with object queries in decoder. (c) PETR generates the 3D position-aware features by encoding the 3D position embedding (3D PE) into 2D image features. The object queries directly interact with 3D position-aware features and output 3D detection results.}  \n\t\\label{arch_comparison}\n\\end{figure*}",
            "architecture": "\\begin{figure*}[t]\n\t\\centering  \n\t\\includegraphics[height=5cm,width=12cm]{./figs/overview.png}  \n\t\\caption{The architecture of the proposed PETR paradigm. \n% \tNote that different views share the same perspective view meshgrid in camera frustum space.\n    The multi-view images are input to the backbone network (e.g. ResNet) to extract the multi-view 2D image features. In 3D coordinates generator, the camera frustum space shared by all views is discretized into a 3D meshgrid. The meshgrid coordinates are transformed by different camera parameters, resulting in the coordinates in 3D world space. Then 2D image features and 3D coordinates are injected to proposed 3D position encoder to generate the 3D position-aware features. Object queries, generated from query generator, are updated through the interaction with 3D position-aware features in transformer decoder. The updated queries are further used to predict the 3D bounding boxes and the object classes.\n    % without complicated feature sampling operations.\n    }  \n\t\\label{architecture}\n\\end{figure*}",
            "fig2": "\\begin{figure*}[t]\n\t\\centering  \n\t\\includegraphics[height=4cm,width=12cm]{./figs/encoder.png}  \n\t\\caption{Illustration of the proposed 3D Position Encoder. The multi-view 2D image features are input to a $1\\times1$ convolution layer for dimension reduction. The 3D coordinates produced by the 3D coordinates generator are transformed into 3D position embedding by a multi-layer perception. The 3D position embeddings are added with the 2D image features of the same view, producing the 3D position-aware features. Finally, the 3D position-aware features are flattened and serve as the input of the transformer decoder.   $\\textcircled{\\scriptsize F}$ is the flatten operation.}  \n\t\\label{fig2}\n\\end{figure*}",
            "similar": "\\begin{figure*}[t]\n\t\\centering  \n\t\\includegraphics[height=3cm,width=12cm]{./figs/pe_1.png}  \n\t\\caption{3D position embedding similarity. The red points are selected positions in the front view. We calculated the similarity between the position embedding of these selected positions and all image views. It shows that the regions close to these selective points tend to have  higher similarity.}\n\t\\label{similar}\n\\end{figure*}",
            "analysis": "\\begin{figure*}[t]\n\t\\centering  \n\t\\begin{subfigure}{.49\\textwidth}\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=\\textwidth]{./figs/converge_1.png}\n\t\t\t\\caption{}\n\t\t\\end{subfigure}\n\t\\begin{subfigure}{.49\\textwidth}\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=\\textwidth]{./figs/fps_2.png}\n\t\t\t\\caption{}\n\t\t\\end{subfigure}\n\t\\caption{Convergence and speed analysis on PETR. (a) The convergence comparison between PETR and DETR3D~\\cite{wang2022detr3d}. PETR converges slower at initial stage and requires a relatively longer training schedule for fully convergence. (b) The performance and speed analysis with different backbones and input sizes. }\n\t\\label{analysis}\n\\end{figure*}",
            "vis_result": "\\begin{figure*}[t]\n\t\\centering  \n\t\\includegraphics[height=6.5cm,width=12cm]{./figs/visual_result_1.png}  \n\t\\caption{Qualitative analysis of detection results in BEV and image views. The score threshold is 0.25, while the  backbone is  ResNet-101.\n\tThe 3D bounding boxes are drawn with different colors to distinguish different classes. \n    }  \n\t\\label{vis_result}\n\\end{figure*}",
            "attention": "\\begin{figure*}[t!]\n\t\\centering  \n\t\\includegraphics[height=3.5cm,width=12cm]{./figs/attention_1.png}  \n\t\\caption{Visualization of attention maps, generated from an object query (corresponding to the truck) on multi-view images. Both front-left and back-left views have a high response on the attention map.}\n\t\\label{attention}\n\\end{figure*}",
            "badcase": "\\begin{figure*}[t!]\n\t\\centering  \n\t\\includegraphics[height=5.75cm,width=12cm]{./figs/badcase_a_1.png}  \n\t\\caption{Failure cases of PETR. We mark the failure cases by red and green circles. The red circles are some small objects that are not detected. The green circles are objects that are wrongly classified.}\n\t\\label{badcase}\n\\end{figure*}"
        },
        "equations": {
            "eq:eq1": "\\begin{equation}\\label{eq1}\np^{3d}_{i,j} = K^{-1}_{i} p^{m}_{j}\n\\end{equation}",
            "eq:eq2": "\\begin{equation}\\label{eq2}\n\\left\\{\n\\begin{aligned}\n&x_{i,j} = &(x_{i,j}-x_{min}) / &(x_{max}-x_{min})\\\\\n&y_{i,j} = &(y_{i,j}-y_{min}) / &(y_{max}-y_{min})\\\\\n&z_{i,j} = &(z_{i,j}-z_{min}) / &(z_{max}-z_{min})\n\\end{aligned}\n\\right.\n\\end{equation}",
            "eq:eq3": "\\begin{equation}\\label{eq3}\nF^{3d}_i = \\psi(F^{2d}_i,P^{3d}_i), \\quad i=1,2,\\dots, N\n\\end{equation}",
            "eq:eq4": "\\begin{equation}\\label{eq4}\n\\begin{aligned}\nQ_l = \\Omega_l(F^{3d}, Q_{l-1}), \\quad l=1, \\dots, L\n\\end{aligned}\n\\end{equation}",
            "eq:eq5": "\\begin{equation}\\label{eq5}\n\\begin{aligned}\nL(y,\\hat{y}) = \\lambda_{cls} * L_{cls}(c,\\sigma(\\hat{c})) + L_{reg}(b,\\sigma(\\hat{b}))\n\\end{aligned}\n\\end{equation}"
        },
        "git_link": "https://github.com/megvii-research/PETR"
    }
}