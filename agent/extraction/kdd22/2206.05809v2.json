{
    "meta_info": {
        "title": "Geometric Policy Iteration for Markov Decision Processes",
        "abstract": "Recently discovered polyhedral structures of the value function for finite\nstate-action discounted Markov decision processes (MDP) shed light on\nunderstanding the success of reinforcement learning. We investigate the value\nfunction polytope in greater detail and characterize the polytope boundary\nusing a hyperplane arrangement. We further show that the value space is a union\nof finitely many cells of the same hyperplane arrangement and relate it to the\npolytope of the classical linear programming formulation for MDPs. Inspired by\nthese geometric properties, we propose a new algorithm, Geometric Policy\nIteration (GPI), to solve discounted MDPs. GPI updates the policy of a single\nstate by switching to an action that is mapped to the boundary of the value\nfunction polytope, followed by an immediate update of the value function. This\nnew update rule aims at a faster value improvement without compromising\ncomputational efficiency. Moreover, our algorithm allows asynchronous updates\nof state values which is more flexible and advantageous compared to traditional\npolicy iteration when the state set is large. We prove that the complexity of\nGPI achieves the best known bound $\\mathcal{O}\\left(\\frac{|\\mathcal{A}|}{1 -\n\\gamma}\\log \\frac{1}{1-\\gamma}\\right)$ of policy iteration and empirically\ndemonstrate the strength of GPI on MDPs of various sizes.",
        "author": "Yue Wu, Jes√∫s A. De Loera",
        "link": "http://arxiv.org/abs/2206.05809v2",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "Camera ready version for SIGKDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:introduction}\n\nThe Markov decision process~(MDP) is the mathematical foundation of reinforcement learning~(RL) which has achieved great empirical success in sequential decision problems. Despite RL's success, new mathematical properties of MDPs are to be discovered to better theoretically understand RL algorithms. In this paper, we study the geometric properties of discounted MDPs with finite states and actions, and propose a new value-based algorithm inspired by their polyhedral structures. \n\nA large family of methods for solving MDPs is based on the notion of (state) values. The strategy of these methods is to maximize the values, then extract the optimal policy from the optimal values. One classic method is value iteration~\\cite{howard60dynamic, Bertsekas1987_VI} in which values are greedily improved to optimum using the Bellman operator. It is also well known that the optimal values can be solved by linear programming~(LP)~\\cite{puterman94markov} which attracts a lot of research interest due to its mathematical formulation. The most efficient algorithms in practice are often variants of policy iteration~\\cite{howard60dynamic} which facilitates the value improvement with policy updates. The \\textbf{value function}, which maps policies to the value space, is central to our analysis throughout, and it plays a key role in understanding how values are related to policies from a geometric perspective.\n\nAlthough policy iteration and its variants are very efficient in practice, their worst-case complexity was long believed exponential~\\cite{mansour1999complexity}. The major breakthrough was made by \\citet{ye2011} where the author proved that both policy iteration and LP with Simplex method~\\cite{danzigsimplex} terminate in $\\bigO{\\frac{|\\states||\\actions|}{1-\\gamma}\\log \\frac{|\\states|}{1-\\gamma}}$. The author first proved that the Simplex method with the most-negative-reduced-cost pivoting rule is strongly polynomial in this situation. Then, a variant of policy iteration called simple policy iteration was shown to be equivalent to the Simplex method. \\citet{hansen2013strategy} later improved the complexity of policy iteration by a factor of $|S|$. The best known complexity of policy iteration is $\\bigO{\\frac{|\\actions|}{1-\\gamma}\\log \\frac{1}{1-\\gamma} }$ proved by \\citet{scherrer2016improved}.\n\nIn the LP formulation, the state values are optimized through the vertices of the LP feasible region which is a convex polytope. Surprisingly, it was recently discovered that the space of the value function is a (possibly non-convex) polytopes~\\cite{Dadashi2019value}. We call such object the \\textbf{value function polytope} denoted by $\\valuespace$. As opposed to LP, the state values are navigated through $\\valuespace$ in policy iteration. Moreover, the \\emph{line theorem}~\\cite{Dadashi2019value} states that the set of policies that only differ in one state is mapped onto the same line segment in the value function polytope. This suggests the potential of new algorithms based on single-state updates. \n\nOur first contribution is on the structure of the value function polytope $\\valuespace$. Specifically, we show that a hyperplane arrangement $H_{MDP}$ is shared by $\\valuespace$ and the polytope of the linear programming formulation for MDPs. We characterize these hyperplanes using the Bellman equation of policies that are deterministic in a single state. We prove that the boundary of the value function polytope $\\partial \\valuespace$ is the union of finitely many (convex polyhedral) cells of $H_{MDP}$. Moreover, each full-dimensional cell of the value function polytope is contained in the union of finitely many full-dimensional cells defined by $H_{MDP}$. We further conjecture that the cells of the arrangement cannot be partial, but they have to be entirely contained in the value function polytope.\n\nThe learning dynamic of policy iteration in the value function polytope shows that every policy update leads to an improvement of state values along one line segment of $\\valuespace$. Based on this, we propose a new algorithm, \\textbf{geometric policy iteration}~(GPI), a variant of the classic policy iteration with several improvements. First, policy iteration may perform multiple updates on the same line segment. GPI avoids this situation by always reaching an endpoint of a line segment in the value function polytope for every policy update. This is achieved by efficiently calculating the true state value of each potential policy update instead of using the Bellman operator which only guarantees a value improvement. Second, GPI updates the values for all states immediately after each policy update for a single state, which makes the value function monotonically increasing with respect to every policy update. Last but not least, GPI can be implemented in an asynchronous fashion. This makes GPI more flexible and advantageous over policy iteration in MDPs with a very large state set. \n\nWe prove that GPI converges in $\\bigO{\\frac{|\\actions|}{1-\\gamma}\\log \\frac{1}{1-\\gamma}}$ iterations, which matches the best known bound for solving finite discounted MDPs. Although using a more complicated strategy for policy improvement, GPI maintains the same $\\bigO{|\\states|^2|\\actions|}$ arithmetic operations in each iteration as policy iteration. We empirically demonstrate that GPI takes fewer iterations and policy updates to attain the optimal value.\n\n",
                "subsection 1.1": {
                    "name": "Related Work",
                    "content": "\n\nOne line of work related to this paper is on the complexity of the policy iteration. For MDPs with a fixed discount factor, the complexity of policy iteration has been improved significantly~\\cite{Littman94, ye2011, Ye2013Post, hansen2013strategy, scherrer2016improved}. There are also positive results reported on stochastic games (SG). \\citet{hansen2013strategy} proved that a two-player turn-based SG can be solved by policy iteration in strongly polynomial time when the discount factor is fixed. \\citet{Akian2013PolicyIF} further proved that policy iteration is strongly polynomial in mean-payoff SG with state-dependent discount factors under some restrictions. In terms of more general settings, the worst-case complexity can still be exponential~\\cite{mansour1999complexity, Fearnley, Hollanders2012, Hollanders2016}. Another line of related work studies the geometric properties of MDPs and RL algorithms. The concept of the value function polytope in this paper was first proposed in \\citet{Dadashi2019value}, which was also the first recent work studying the geometry of the value function. Later, \\citet{Bellemare2019Geometric} explored the direction of using these geometric structures as auxiliary tasks in representation learning in deep RL. \\citet{policyimprovepath} also aimed at improving the representation learning by shaping the policy improvement path within the value function polytope. The geometric perspective of RL also contributes to unsupervised skill learning where no reward function can be accessed~\\cite{unsupervised_skill_learning}. Very recently, \\citet{geometryPOMDP} analyzed the geometry of state-action frequencies in partially observable MDPs, and formulated the problem of finding the optimal memoryless policy as a polynomial program with a linear objective and polynomial constraints. The geometry of the value function in robust MDP is also studied in \\citet{geometryRMDP}.\n\n% The rest of the paper is organized as follows. Section~\\ref{sec:prelim} introduces the fundamentals of the MDP. We show the relation of the value function polytope $\\valuespace$ and LP polytope as well as the analysis of the boundary of $\\valuespace$ in Section~\\ref{sec:boundary}. The detail of GPI is presented in Section~\\ref{sec:gpi}.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                }
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\nAn MDP has five components $\\mathcal{M} = \\langle\\states , \\actions, \\rewards, \\transitions, \\gamma\\rangle$ where \n        $\\states$ and $\\actions$ are finite state set and action set,\n        $\\transitions: \\states \\times \\actions \\to \\Delta(\\states)$ is the transition function with $\\Delta(\\cdot)$ denoting the probability simplex.\n        $\\rewards: \\states \\times \\actions \\to \\realset$ is the reward function and\n        $\\gamma = [0,1)$ is the discount factor that represents the value of time.\n\nA policy $\\pi: \\states \\to \\Delta(\\actions)$ is a mapping from states to distributions over actions. The goal is to find a policy that maximizes the cumulative sum of rewards.\n\nDefine $V^{\\pi} \\in \\realset^{|\\states|}$ as the vector of state values. $V^{\\pi}(s)$ is then the expected cumulative reward starting from a particular state $s$ and acting according to $\\pi$:\n\\begin{equation*}\nV^{\\pi}(s) = \\E\\nolimits_{P^\\pi}\\Big(\\sum^{\\infty}_{i=0} \\gamma^{i}\n\\rewards(s_i, a_i) \\cbar s_0=s \\Big). \\label{eq:v_func_cumulative_reward}\n\\end{equation*}\n\nThe \\textbf{Bellman equation}  \\cite{Bellman:DynamicProgramming} connects the value $V^{\\pi}$ at a state $s$ with the value at the subsequent states when following $\\pi$:\n\\begin{align} V^\\pi(s) = \\E\\nolimits_{P^\\pi}\\Big(\\rewards(s, a) + \\gamma V^\\pi(s')\\Big).\n\\label{eq:bellman}\n\\end{align}\nDefine $\\rpi$ and $\\Ppi$ as follows.\n\\begin{align*}\n    \\rpi(s) &= \\sum_{a \\in \\actions} \\pi(a \\cbar s) \\rewards(s, a), \\\\\n    \\Ppi(s' \\cbar s) &= \\sum_{a \\in \\actions} \\pi(a \\cbar s) \\transitions (s' \\cbar s, a),\n\\end{align*}\n% where $\\actions_s$ denotes the set of actions available in $s$.\nThen, the Bellman equation for a policy $\\pi$ can be expressed in matrix form as follows.\n\\begin{align}\n    V^\\pi &= \\rpi + \\gamma \\Ppi V^\\pi \\nonumber\\\\\n    & = (I - \\gamma \\Ppi)^{-1} \\rpi. \\label{eq:bellman_eq}\n\\end{align}\n% The space of policies $\\policies$ is the Cartesian product of simplices that we can express as a space of $|\\states| \\times |\\actions|$ matrices. \n\nUnder this notation, we can define the Bellman operator $\\bellop^{\\pi}$ and the optimality Bellman operator $\\bellop^*$ for an arbitrary value vector $V$ as follows.\n\\begin{align*}\n    \\bellop^{\\pi} V &= \\rpi + \\gamma \\Ppi V, \\\\\n    \\bellop^* V &= \\max_{\\pi} \\bellop^{\\pi} V.\n\\end{align*}\n$V$ is optimal if and only if $V = \\bellop^* V$. MDPs can be solved by \\textbf{value iteration}~(VI)~\\cite{Bellman:DynamicProgramming} which consists of the repeated application of the optimality Bellman operator $V^{(k+1)} := \\bellop^* V^{(k)}$ until a fixed point has been reached.\n\nLet $\\policyspace$ denote the space of all policies, and $\\valuespace$ denote the space of all state values. We define the \\textbf{value function} $f_v(\\pi): \\policyspace \\to \\valuespace$ as\n\\begin{equation}\n    f_v(\\pi) = (I - \\gamma \\Ppi)^{-1} \\rpi.\n    \\label{eq:value_func_mapping}\n\\end{equation}\nThe value function $\\valuefunction$ is fundamental to many algorithmic solutions of an MDP. \\textbf{Policy iteration}~(PI)~\\cite{howard60dynamic} repeatedly alternates between a policy evaluation step and a policy improvement step until convergence. In the policy evaluation step, the state values $\\Vpi$ of the current policy $\\pi$ is evaluated which involves solving a linear system (Eq.~\\eqref{eq:bellman_eq}). In the policy improvement step, PI iterates over all states and update the policy by taking a greedy step using the optimality Bellman operator as follows.\n\\begin{equation*}\n    \\pi'(s) \\in \\argmax_{a \\in \\actions}\\left\\{ \\rewards(s, a) + \\gamma \\sum_{s'}\\transitions(s' \\cbar s, a) \\Vpi(s') \\right\\},\\, \\forall s \\in \\states.\n\\end{equation*}\n\\textbf{Simple policy iteration}~(SPI) is a variant of policy iteration. It only differs from policy iteration in the policy improvement step where the policy is only updated for the state-action pair with the largest improvement over the following advantage function.\n\\begin{equation*}\n    \\tilde{A}(s, a) = \\rewards(s, a) + \\gamma \\sum_{s'}\\transitions(s' \\cbar s, a) \\Vpi(s') - \\Vpi(s). \n\\end{equation*}\nSPI selects a state-action pair from $\\argmax_{s, a} \\tilde{A}(s, a)$ then updates the policy accordingly. \n\n",
                "subsection 2.1": {
                    "name": "Geometry of the Value Function",
                    "content": "\nWhile the space of policies $\\policies$ is the Cartesian product of $|\\states|$ probability simplices, \\citet{Dadashi2019value} proved that the value function space is a possibly non-convex polytope~\\cite{Ziegler_polytope}. Figure~\\ref{fig:vfp_line} shows a convex and a non-convex $f_v$ polytopes of 2 MDPs in blue regions. The proof is built upon the line theorem which is an equally important geometric property of the value space. The line theorem depends on the following definition of policy determinism.\n\n\\begin{definition}[Policy Determinism]\nA policy $\\pi$ is \n\\begin{itemize}\n    \\item \\emph{$s$-deterministic} for $s\\in \\states$ if it selects one concrete action for sure in state $s$, i.e., $\\pi(a|s)\\in \\{0, 1\\},\\, \\forall a$;\n    \\item \\emph{deterministic} if it is $s$-deterministic for all $s \\in \\states$.\n\\end{itemize}\n\\end{definition}\n\n\n\nThe line theorem captures the geometric property of a set of policies that differ in only one state. Specifically, we say two policies $\\pi_1, \\pi_2$ \\emph{agree} on states $s_1,.., s_k \\in \\states$ if \n$\\pi_1(\\cdot \\cbar s_i) = \\pi_2(\\cdot \\cbar s_i)$ for each $s_i$, $i = 1, \\dots, k$.\nFor a given policy $\\pi$, we denote by $Y^\\pi_{s_1, \\dots, s_k} \\subseteq \\policies$ the set of policies that agree with $\\pi$ on \n$s_1, \\dots, s_k$; we will also write $\\agreeone$ to describe the set of policies that agree with $\\pi$ on all states except $s$. When we keep the probabilities fixed at all but state $s$, the functional $f_v$ draws a line segment which is oriented in the positive orthant (that is, one end dominates the other). Furthermore, the endpoints of this line segment are $s$-deterministic \npolicies. \n\nThe line theorem is stated as follows:\n\\begin{theorem}[Line theorem~\\cite{Dadashi2019value}]\n\\label{thm:line}\nLet $s$ be a state and $\\pi$ a policy.\nThen there are two $s\\text{-deterministic}$ policies in $\\agreeone$, denoted $\\pi_l, \\pi_u$, which\nbracket the value of all other policies $\\pi' \\in \\agreeone$: \\begin{equation*}\n    f_v(\\pi_l) \\preccurlyeq f_v(\\pi') \\preccurlyeq f_v(\\pi_u).\n\\end{equation*}\n\\end{theorem}\n\nFor both Figure~\\ref{fig:vfp_line_left} and~\\ref{fig:vfp_line_right}, we plot policies that agree on one state to illustrate the line theorem. The policy determinism decides if policies are mapped to a vertex, onto the boundary or inside the polytope.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                }
            },
            "section 3": {
                "name": "The Cell Structure of the Value Function Polytope",
                "content": "\n\\label{sec:vfp_boundary}\n\n% \\begin{figure}[t]\n%     \\captionsetup[subfloat]{labelformat=empty}\n%     \\centering\n%     \\subfloat[]{\\includegraphics[width=0.5\\linewidth]{imgs/qe_vfp_boundary1.png}%\n%     }\n%     \\hfil\n%     \\subfloat[]{\\includegraphics[width=0.5\\linewidth]{imgs/qe_vfp_boundary2.png}%\n%     }\n% \\end{figure}\n\nIn this section, we revisit the geometry of the (non-convex) value function polytope presented in \\citet{Dadashi2019value}. We establish a connection to linear programming formulations of the MDP which then can be adapted to show a finer description of cells in the value function polytope as unions of cells of a hyperplane arrangement. For more on hyperplane arrangements and their structure, see \\citet{hyperplanes-intro}.\n\nIt is known since at least the 1990's that finding the optimal value function of an MDP can be formulated as a linear program (see for example \\cite{puterman94markov, bertsekas96neurodynamic}). In the primal form, the feasible constraints are defined by $\\{ V \\in \\reals^{|\\states|} \\; \\big| \\; V \\gvec \\bellop^*  V \\}$, where $\\bellop^*$ is the optimality Bellman operator. Concretely, the following linear program is well-known to be equivalent \nto maximizing the expected total reward in Eq.~\\eqref{eq:v_func_cumulative_reward}. \nWe call this convex polyhedron the \\textbf{MDP-LP polytope} (because it is a linear programming form of the MDP problem).\n    \\begin{align*}\n        \\min_V & \\quad \\sum_{s} \\alpha(s) V(s) \\\\\n        \\text{s.t.}  & \\quad V(s) \\ge \\rewards(s, a) + \\gamma \\sum_{s'}\\transitions(s' \\cbar s, a) V(s'),\\,\\, \\forall s \\in \\states, a \\in \\actions.\n    \\end{align*}\nwhere $\\alpha$ is a probability distribution over $\\states$. \n\nOur main new observation is that the MDP-LP polytope and the value polytope are actually closely related, and one\ncan describe the regions of the (non-convex) value function polytope in terms of the (convex) cells of the arrangement.\n\n\\begin{theorem} Consider the hyperplane arrangement $H_{MDP}$, with $|\\actions| |\\states|$ hyperplanes,\nconsisting of those of the MDP polytope, i.e.,\n\\begin{equation*}\n     H_{MDP}= \\left\\{V(s) = \\rewards(s, a) + \\gamma \\sum_{s'}\\transitions (s' \\cbar s, a)V(s') \\mid \\forall s \\in \\states, a \\in \\actions \\right\\}.\n\\end{equation*}\n\\label{thm: vfp_boundary}\nThen, the boundary of the value function polytope $\\partial \\valuespace$ is the union of finitely (convex polyhedral) cells of the arrangement $H_{MDP}$. Moreover, each full-dimensional cell of the value polytope is contained in the union of finitely many full-dimensional cells defined by $H_{MDP}$.\n\\end{theorem}\n\n%For any policies that \\emph{agree} on states $s_1,.., s_k \\in \\states$, namely\n%$\\pi_1(\\cdot \\cbar s_i) = \\pi_2(\\cdot \\cbar s_i)$ for each $s_i$, $i = 1, \\dots, k$.\n%They are the intersection of $k$ MDP hyperplanes. In particular, any when they agree in all states they\n%define a point where $|\\states |$ many MDP hyperplanes intersect.\n\n%Consider the ensemble of policies $\\agree$ that agree with $\\pi$ on states $\\ssubspace = \\{ s_1,..,s_k \\}$. Suppose %$\\forall s \\notin \\ssubspace$, $\\forall a \\in \\actions$, $\\nexists \\pi' \\in \\agree \\cap D_{a,s}$ s.t. %$f_v(\\pi')=f_v(\\pi)$, then $f_v(\\pi)$ has a relative neighborhood in $\\valuefunctions \\cap \\affinesev$.\n%\\label{th:neighbor}\n\n%Consider a policy $\\pi$,  the states $\\ssubspace = \\{s_1, .., s_k\\}$, and the ensemble  $\\agree$ of all policies that %agree with $\\pi$ on $s_1,..,s_k$. Define $\\valuespace^{y} = f_v(\\agree)$, we have that the relative boundary of %$\\valuefunctions^{y}$ in $\\affinesev$ is included in the value functions spanned by policies in $\\agree$ that are %$s$-deterministic for $s \\notin \\ssubspace$:\n%\\begin{equation*}\n%    \\partial \\valuefunctions^y \\subset \\bigcup_{s \\notin \\ssubspace} \\bigcup_{a \\in \\actions} f_v(\\agree \\cap D_{s, a}),\n%\\%end{equation*}\n%where $\\partial$ refers to $\\partial_{\\affinesev}$.\n%\\label{cr:boundary}\n\n%For a given policy $\\pi$, we denote by $Y^\\pi_{s_1, \\dots, s_k} \\subseteq \\policies$ the set of policies which agree with $\\pi$ on \n% $s_1, \\dots, s_k$; we will also write $\\agreeone$ to describe the set of policies that agree with $\\pi$ on all states except $s$. \n% It was proved in \\cite{Dadashi2019value} that when we keep the probabilities fixed at all but state $s$, the functional $f_v$ draws a line segment which is oriented in the positive orthant (that is, one end dominates the other end). Furthermore, the extremes of this line segment are $s$-deterministic \n% policies (i.e., $\\pi(a|s) \\in \\{0, 1\\},\\, \\forall a\\in \\actions_s$). \n % can be characterized as follows.\n\n\\begin{proof}\n%We can prove this by induction on the dimension of the cells of the value polytope.  \n%Now assume by the induction hypothesis that all cells of dimension $k$ in the value polytope are union of $k$-cells of the arrangement $H_{MDP}$. \n\nLet us first consider a point $V^\\pi$ being on the boundary of the value function polytope. Theorem 2 and Corollary 3 of \\citet{Dadashi2019value}\ndemonstrated that the boundary of the space of value functions is a (possibly proper) subset of the ensemble of value functions of policies, where at least one state has a fixed deterministic choice for all actions.\nNote that from the value function Eq.~\\eqref{eq:value_func_mapping},  then the hyperplane \n\\begin{equation*}\n    V(s) = \\rewards(s, a^s_l) + \\gamma \\sum_{s'}\\transitions (s' \\cbar s, a^s_l)V(s')\n\\end{equation*}\nincludes all policies taking policy $a^s_l = \\pi_l(s)$ in state $s$. Thus the points of the boundary of the value function polytope are contained in the hyperplanes of $H_{MDP}$. Now we can see how the $k$-dimensional cells of the boundary are then in the intersections of the hyperplanes too.\n\n%\\begin{equation*}\n%     \\left\\{V(s) =\\rewards(s, a^s_l) + \\gamma \\sum_{s'}\\transitions (s' \\cbar s, a^s_l)V(s') \\cbar \\forall a \\in \\actions \\right\\}\n%\\end{equation*}\n\nThe zero-dimensional cells (vertices) are clearly a subset of the zero-dimensional cells of the arrangement $H_{MDP}$ because, by above results, the zero-dimensional cells are precisely in the intersection of $| \\states |$ many hyperplanes from $H_{MDP}$, which is equivalent to choosing a fixed set of actions for all states. This corresponds to solving a linear system consisting of the hyperplanes that bound $\\valuespace$ (same as Eq.~\\eqref{eq:bellman_eq}).\nBut more generally, if we fix the policies for only $k$ states, the induced space lies in a $|\\states|-k$ dimensional affine space. Consider a policy $\\pi$ and $k$ states $s_1, \\dots, s_k$, and write $C_{k+1}^\\pi, \\dots, C_{|\\states|}^\\pi$ for the columns of the matrix $(I - \\gamma \\Ppi)^{-1}$ corresponding to states \\emph{other} than $s_1, \\dots, s_k$. Define the affine vector space $\\affinesev$\n\\begin{equation*}\n    H^\\pi_{s_1, \\dots, s_k} = V^\\pi + Span(C_{k+1}^\\pi, \\dots, C_{|\\states|}^\\pi) .\n\\end{equation*}\nNow For a given policy $\\pi$, we denote by $Y^\\pi_{s_1, \\dots, s_k} \\subseteq \\policies$ the set of policies which agree with $\\pi$ on $s_1, \\dots, s_k$;\nThus the value functions generated by $\\agree$ are contained in the affine vector space $H^\\pi_{s_1, \\dots, s_k}$:\n$    f_v(\\agree) = \\valuespace \\cap \\affinesev.$\n\n\nThe points of $H^\\pi_{s_1, \\dots, s_k}$ in one or more of the $H_{MDP}$ planes (each hyperplane is precisely fixing one policy action pair). This is the intersection of $k$ hyperplanes given by the following equations. \n\\begin{equation*}\n     \\left\\{V(s) = \\rewards(s, a) + \\gamma \\sum_{s'}\\transitions (s' \\cbar s, a)V(s') \\mid \\forall s \\in \\{s_1,\\ldots,s_k\\}, a \\in \\actions \\right\\}. \n\\end{equation*}\nThus we can be sure of the stated containment.\n\nFinally, the only remaining case is when $V^\\pi$ is in the interior of the value polytope. If that is the case, because $H_{MDP}$ partitions the entire Euclidean space, it must be contained in at least one of the full-dimensional cell of $H_{MDP}$. \n\\qedhere\n\\end{proof}\n\n\n\nFigure \\ref{fig:vfp_lp_boundary} is an example of the value function polytope in blue, MDP-LP polytope in green and its bounding hyperplanes (the arrangement $H_{MDP}$) as blue and red lines. In Figure~\\ref{fig:vfp_boundary} we exemplify Theorem \\ref{thm: vfp_boundary} by presenting a value function polytope with delimited boundaries where  $H_{MDP}$ hyperplanes are indicated in different colors. The deterministic policies are those for which $\\pi(a|s) \\in \\{0, 1\\} \\, \\forall a\\in \\actions, s\\in \\states$.  In both pictures, the values of deterministic policies in the value space are shown as red dots. The boundaries of the value polytope are indeed included in the set of cells of the arrangement $H_{MDP}$  as stated by Theorem~\\ref{thm: vfp_boundary}.\nThese figures of value function polytopes (blue regions) were obtained by randomly sampling policies and plotting their corresponding state values.   \n\n% \\begin{figure}[ht]\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{imgs/vfp_lp_polytope.png}\n%     \\caption{Value function polytopes~(blue regions) and the corresponding LP feasible region~(green region). Deterministic policies are shown as red dots. Both MDPs have 2 states and 2 actions, 3 actions for each state in (a) and (b), respectively.}\n%     \\label{fig:vfp_lp}\n% \\end{figure}\n\n%  \\begin{figure}[ht]\n%      \\centering\n%     \\includegraphics[width=0.6\\linewidth]{imgs/vpolytope_with_boundary_multicolor.png}\n%      \\caption{Value function polytope overlapped with the hyperplane arrangement $H_{MDP}$ from Theorem~\\ref{thm: vfp_boundary}. }\n%      \\label{fig:vfp_boundary}\n%  \\end{figure}\n\nSome remarks are in order. Note how sometimes the several adjacent cells of the MDP arrangement together form \na connected cell of the value function polytope. We also observe that for any set of states $s_1,.., s_k \\in \\states$ and a policy $\\pi$, $V^{\\pi}$ can be expressed as a convex combination of value functions of $\\{s_1,.., s_k\\}$-deterministic policies. In particular,  $\\valuespace$ is included in the convex hull of the value functions of deterministic policies. It is also demonstrated clearly in Figure~\\ref{fig:vfp_boundary} that the value functions of deterministic policies are not always vertices and the vertices of the value polytope are not always value functions of deterministic policies, but they are always intersections of hyperplanes on $H_{MDP}$. \n%Figure~\\ref{fig:vfp_lp} (b) shows that deterministic policies are not necessarily mapped to vertices of the value function polytope. \nHowever, optimal values will always include a deterministic vertex. This observation suggests that it would suffice to find the optimal policy by only visiting deterministic policies on the boundary. It is worthwhile to note that the optimal value of our MDP would be at the unique intersection vertex of the two polytopes. We note that the blue regions in Figure~\\ref{fig:vfp_lp_boundary} are \\emph{not} related to the polytope of the dual formulation of LP. Unlike the MDP polytope which can be characterized as the intersection of finitely many half-spaces, we do not have such a neat representation for the value function polytope. The pictures presented here and many more experiments we have done suggest the following stronger result is true:\n\n{\\bf Conjecture:} if the value polytope intersects a cell of the arrangement $H_{MDP}$, then it contains the entire cell, thus all full-dimensional cells of the value function polytope are equal to the union of full-dimensional cells of the arrangement.\n\nProving this conjecture requires showing that the map from policies to value functions is surjective over the cells it touches. At the moment we can only guarantee that there are no isolated components because the value polytope is a compact set. More strongly \\citet{Dadashi2019value} shown (using the line theorem) that there is  path connectivity from $V^{\\pi}$, in any cell, to others is guaranteed by a polygonal path. More precisely if we let $V^{\\pi}$ and $V^{\\pi'}$ be two value functions. Then there exists a sequence of $k \\le |\\states|$ policies, $\\pi_1, \\dots, \\pi_k$, such that $V^\\pi = V^{\\pi_1}$, $V^{\\pi'} = V^{\\pi_k}$, and for every $i \\in 1, \\dots, k - 1$, the set $\\{ f_v( \\alpha \\pi_i + (1 - \\alpha) \\pi_{i+1}) \\cbar \\alpha \\in [0, 1] \\}$ forms a line segment.\n\n%From the line theorem, there are two $s\\text{-deterministic}$ policies in $\\agreeone$, denoted $\\pi_l, \\pi_u$, which\n%bracket the value of all other policies $\\pi' \\in \\agreeone$: \n%\\begin{equation*}\n%    f_v(\\pi_l) \\preccurlyeq f_v(\\pi') \\preccurlyeq f_v(\\pi_u).\n%\\end{equation*}\n\n\nIt was observed that algorithms for solving MDPs have different learning behavior when visualized in the value polytope space. \nFor example, policy gradient methods~\\cite{sutton2000policy, kakade2002natural, policygradient_actorcritic, policygradientWilliam, policygradientWilliamsPeng91} have an improvement path inside of the value function polytope; value iteration can go outside of the polytope which means there can be no corresponding policy during the update process; and policy iteration navigates exactly through deterministic policies. \nIn the rest of our paper we use this geometric intuition to design a new algorithm.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n \n"
            },
            "section 4": {
                "name": "The Method of Geometric Policy Iteration",
                "content": "\n\\label{sec:gpi}\n\n\n\nWe now present \\emph{geometric policy iteration} (GPI) that improves over PI based on the geometric properties of the learning dynamics. Define an \\textit{action switch} to be an update of policy $\\pi$ in any state $s\\in \\states$. The Line theorem shows that policies agreeing on all but one state lie on a line segment. So an action switch is a move along a line segment to improve the value function. In PI, we use the optimality Bellman operator $\\bellop^* V(s) = \\max_{\\pi}( r^{\\pi} + \\gamma P^{\\pi} V)(s)$ to decide the action to switch to for state $s$. However, $\\bellop^* V (s)$ does not guarantee the largest value improvement $V^*(s) - V(s)$ for $s$. This phenomenon is illustrated in Figure~\\ref{fig:gpiv_endpoints} where we plot the value sequences of PI and the proposed GPI. \n\n% Algebraically, for any two policies $\\pi'$ and $\\pi$, the difference between these two methods can be characterized as follows.\n% \\begin{equation}\n%     \\Vpiprime - \\Vpi = \\left(I - \\gamma \\Ppiprime\\right)^{-1} \\left( \\bellop^{\\pi'}\\Vpi - \\Vpi \\right).\n% \\end{equation}\n% The proof can be found in Section~\\ref{sec:gpi_theory}.\n\nWe propose an alternative action-switch strategy in GPI that directly calculates the improvement of the value function for one state. By choosing the action with the largest value improvement, we can always reach the endpoint of a line segment which potentially reduces the number of action switches.\n\nThis strategy requires efficient computation of the value function because a naive calculation of the value function by Eq.~\\eqref{eq:bellman_eq} is very expensive due to the matrix inversion. On the other hand, PI only re-evaluates the value function once per iteration. Our next theorem states that the new state-value can be efficiently computed. This is achieved by using the fact that the policy improvement step can be done state-by-state within a sweep over the state set, so adjacent policies in the update sequence only differ in one state.\n\n% Our main contribution here is to use the Sherman-Morrison formula to efficiently calculate the best action $a_i$ for state $s_i$ and then update the value function for all states. \n\n\\begin{theorem}\n\\label{thm:sherman_morrison}\n Given $\\bQ^\\pi = (I - \\gamma \\Ppi)^{-1}$ and $V^\\pi = \\bQ^\\pi r^\\pi$. If a new policy $\\delta$ only differs from $\\pi$ in state $s$ with $\\delta(s) = a \\ne \\pi(s)$, $\\Vdelta(s)$ can be calculated efficiently by\n    \\begin{equation}\n        \\Vdelta(s) = \\parentheses{ \\bone_s + \\frac{\\bQ^\\pi(s, s)}{1 - \\bw^\\top_a \\,\\bq_s} \\, \\bw_a}^{\\top} \\parentheses{\\Vpi + \\Delta r_a \\, \\bq_s },\n        \\label{eq:sherman_morrison_thm}\n    \\end{equation}\nwhere $\\bw_a = \\gamma \\left(\\transitions(s, a) - \\transitions(s, \\pi(s))\\right)$ is a $|\\states|$-d vector, $\\Delta r_a = \\rewards(s, a) - \\rewards(s, \\pi(s))$ is a scalar, $\\bq_s$ is the $s^{\\text{th}}$ column of $\\bQ^\\pi$, and $\\bone_s$ is a vector with entry $s$ being $1$, others being $0$.    \n    \n    \n%  where $\\bq_s$ is the $s$-th column of $\\bQ^\\pi$,\n%  \\begin{align*}\n%      \\bw_a &= \\gamma \\left(\\transitions(s, \\delta(s), :) - \\transitions(s, \\pi(s), :)\\right),\\\\\n%      \\Delta r_a &= \\rewards(s, \\delta(s)) - \\rewards(s, \\pi(s)),\n%  \\end{align*}\n% and $\\bone_s$ is a vector with entry $s$ being $1$, others being $0$.\n\\end{theorem}\n\n\\begin{proof}\nWe here provide a general proof that we can calculate $V^{\\delta}$ given policy $\\pi$, $\\Vpi$, and $\\delta$ differs from $\\pi$ in only one state.\n\\begin{align*}\n    \\Vdelta = \\left( I - \\gamma \\Pdelta \\right)^{-1} \\rdelta = \\parentheses{ I - \\gamma \\Ppi - \\gamma \\Delta P}^{-1} \\rdelta,\n\\end{align*}\nwhere $\\Delta P = \\Pdelta - \\Ppi$. Assume $\\delta$ and $\\pi$ differ in state $s$. $\\Delta P$ is a rank-$1$ matrix with row $j$ being $\\transitions(s, \\pi(s)) - \\transitions(s, a)$, and all other rows being zero vectors.\n\nWe can then express $\\Delta P$ as the outer product of two vectors $\\Delta P = \\bone_{s} \\bw_a^\\top$, where $\\bone_{s}$ is a one-hot vector\n\\begin{align}\n    \\bone_{s}(i) = \n    \\begin{cases}\n        1, & \\text{if $i = s$,} \\\\\n        0, & \\text{otherwise,}\n    \\end{cases}\n\\end{align}\nand $\\bw_a$ is defined above.\n% \\begin{equation}\n%     \\bw_a = \\gamma \\left(\\transitions(s, a) - \\transitions(s, \\pi(s))\\right).\n% \\end{equation}\n\nSimilarly, we have $\\rdelta = \\rpi + \\Delta r = \\rpi + \\Delta r_a \\bone_s$.\nThen, we have\n\\begin{align*}\n        \\Vdelta & = \\parentheses{ I - \\gamma \\Pdelta}^{-1} \\rdelta \\\\ \n        & = \\parentheses{ I - \\gamma \\Ppi - \\gamma \\Delta P}^{-1} \\parentheses{\\rpi + \\Delta r} \\\\\n        & = \\parentheses{ I - \\gamma \\Ppi - \\bone_{s} \\bw_a^\\top}^{-1} \\parentheses{\\rpi + \\Delta r_a \\, \\bone_s}  \\\\\n        & = \\parentheses{ \\bQ^\\pi + \\frac{\\bQ^\\pi \\bone_s \\bw_a^\\top \\bQ^\\pi}{1 - \\bw_a^\\top \\bQ^\\pi \\bone_s} } \\parentheses{\\rpi + \\Delta r_a \\, \\bone_s} \\tag*{$\\parentheses{\\text{Sherman-Morrison, }\\bQ^\\pi = \\parentheses{I - \\gamma \\Ppi}^{-1}}$} \\\\\n        % & = \\parentheses{ I + \\frac{\\bQ \\bone_s \\bw^\\top }{1 - \\bw^\\top \\bQ \\bone_s} } \\parentheses{\\bQ \\rpi + \\Delta r(s) \\, \\bQ \\, \\bone_s} \\\\\n        & = \\parentheses{ I + \\frac{\\bq_s \\bw_a^\\top }{1 - \\bw_a^\\top \\bq_s} } \\parentheses{\\Vpi + \\Delta r_a \\, \\bq_s} \\tag*{$\\parentheses{\\text{here, }\\bQ^\\pi \\bone_s = \\bq_s ,\\,\\bQ^\\pi \\rpi = \\Vpi}$}.\n    \\end{align*}\nThus, for state $s$, we have \n\\begin{equation*}\n    \\Vdelta(s) = \\parentheses{\\bone_s + \\frac{\\bQ^\\pi(s, s)}{1 - \\bw_a^\\top \\bq_s} \\bw_a} \\parentheses{\\Vpi + \\Delta r_a \\, \\bq_s},\n\\end{equation*}\nwhich completes the proof.\n\\end{proof}\n\nTheorem~\\ref{thm:sherman_morrison} suggests that updating the value of a single state using Eq.~\\eqref{eq:sherman_morrison_thm} takes $\\bigO{|\\states||\\actions|}$ arithmetic operations which matches the complexity of the optimality Bellman operator used in policy iteration.\n\n\n\nThe second improvement over policy iteration comes from the fact that the value improvement path in $\\valuespace$ may not be monotonic with respect to action switches. Although it is well-known that the update sequence $\\{\\Vpik\\}$ is non-decreasing in the iteration number $k$, the value function could decrease in the policy improvement step of the policy iteration. An illustration is shown as the red path of PI in Figure~\\ref{fig:gpiv_monotone}. The possible value decrease is because when the Bellman operator $\\bellop$ is used to decide an action switch, $V$ is fixed for the entire sweep of states. This leads us to the motivation for GPI which is to update the value function after each action switch such that the value function action-switch-monotone. This idea can be seamlessly combined with Theorem~\\ref{thm:sherman_morrison} since the values of all states can be updated efficiently in $\\bigO{|\\states|^2|\\actions|}$ arithmetic operations. Thus, the complexity of completing one iteration is the same as policy iteration. \n\\begin{algorithm}\n\\caption{Geometric Policy Iteration}\n\\begin{algorithmic}[1]\n    \\Input $\\transitions$, $\\rewards$, $\\gamma$\n    \\State set iteration number $k = 0$ and randomly initialize $\\piki{k}{0}$\n    \\State Calculate $\\bQki{k}{0} = (I - \\gamma \\Pki{k}{0})^{-1}$ and $\\Vki{k}{0} = \\bQki{k}{0} \\rki{k}{0}$ \\label{alg:gpi_line_initial_eval}\n    % \\item[\\textbf{3.}] For all $s \\in \\states$\n    \\For{$i=1, \\ldots, |\\states|$}  \\label{alg:gpi_line_for}\n        \\State calculate the best action $a_i$ according to Eq.~\\eqref{eq:gpiv_action_selection}\n        \\label{alg:gpi_line_action_select}\n        \\State update $\\bQki{k}{i}$ according to Eq.~\\eqref{eq:gpi_Q_update} \\label{alg:gpi_line_Q_update}\n        \\State $\\piki{k}{i}(i) = a_i$ \\label{alg:gpi_line_pi_update}\n        \\State $\\Vki{k}{i} = \\bQki{k}{i} \\, \\rki{k}{i}$ \\label{alg:gpi_line_V_update}\n    \\EndFor\n    \\If{$\\Vki{k}{|\\states|}$ is optimal}\n    % \\State \\textbf{return} $\\piki{k}{i}$\n    \\Return $\\piki{k}{|\\states|}$\n    \\EndIf\n    \\State $\\bQki{k+1}{0} = \\bQki{k}{|\\states|}$, $\\piki{k+1}{0} = \\piki{k}{|\\states|}$, $\\Vki{k+1}{0} = \\Vki{k}{|\\states|}$, $k = k + 1$. Go to step~\\ref{alg:gpi_line_for}\n\\end{algorithmic}\n\\label{alg:gpi}\n\\end{algorithm}\n\nWe summarize GPI in Algorithm~\\ref{alg:gpi}. GPI looks for action switches for all states in one iteration, and updates the value function after each action switch. Let superscript $k$ denote the iteration index, subscript $i$ denote the state index in one iteration. To avoid clutter, we use $i$ to denote the state $s_i$ being updated and drop superscript $\\pi$ in $\\Ppi$ and $\\rpi$.  Step~\\ref{alg:gpi_line_initial_eval} evaluates the initial policy $\\piki{k}{0}$. The difference here is that we store the intermediate matrix $\\bQki{k}{0}$ for later computation. From step~\\ref{alg:gpi_line_for} to step~\\ref{alg:gpi_line_V_update}, we iterate over all states to search for potential updates. In step~\\ref{alg:gpi_line_action_select}, GPI selects the best action by computing the new state-value of each potential action switch by Eq.~\\eqref{eq:gpiv_action_selection}.\n\\begin{equation}\n    a_i \\in \\argmax_{a \\in \\actions}\\left\\{\\parentheses{ \\bone_i + \\frac{\\bQki{k}{i-1}(i, i)}{1 - \\bw^\\top_a \\,\\bqi} \\, \\bw_a}^{\\top} \\parentheses{\\Vki{k}{i-1} + \\Delta r_a \\, \\bqi } \\right\\},\n    \\label{eq:gpiv_action_selection}\n\\end{equation}\nwhere\n\\begin{align}\n    \\bw_a &= \\gamma \\left(\\transitions(i, a) - \\transitions(i, \\piki{k}{i-1}(i))\\right),\\label{eq:gpiv_w} \\\\ \n    \\Delta r_a &= \\parentheses{\\rewards(i, a) - \\rewards(i, \\piki{k}{i-1}(i))},\\label{eq:gpiv_r}\n\\end{align}\nand $\\bone_i$ is a vector with $i^{\\text{th}}$ entry being $1$ and others being $0$.\n\nDefine $\\bqi$ to be the $i^{\\text{th}}$ column of $\\bQki{k}{i-1}$. $\\bwiT$ is obtained by Eq.~\\eqref{eq:gpiv_w} using the selected action $a_i$. In step~\\ref{alg:gpi_line_Q_update}, we update $\\bQki{k}{i}$ as follows.\n\\begin{equation}\n    \\bQki{k}{i} = \\bQki{k}{i-1} + \\frac{\\bqi \\, \\bwiT \\bQki{k}{i-1}}{1 - \\bwiT \\bqi}. \\label{eq:gpi_Q_update}\n\\end{equation}\nThe policy is updated in step~\\ref{alg:gpi_line_pi_update} and the value vector is updated in step~\\ref{alg:gpi_line_V_update} where $\\rki{k}{i}$ is the reward vector under the new policy. The algorithm is terminated when the optimal values are achieved.\n\n",
                "subsection 4.1": {
                    "name": "Theoretical Guarantees",
                    "content": "\n\\label{sec:gpi_theory}\n\nBefore we present any properties of GPI, let us first prove the following very useful lemma.\n\n\\begin{lemma}\nGiven two policies $\\pi$ and $\\pi'$, we have the following equalities.\n\\begin{align}\n    \\Vpiprime - \\Vpi &= \\left(I - \\gamma \\Ppiprime\\right)^{-1} \\left( \\rpiprime + \\gamma \\Ppiprime \\Vpi - \\Vpi \\right)  \\label{eq:lemma_adv_v},\\\\\n    \\Vpiprime - \\Vpi &= \\left(I - \\gamma \\Ppi \\right)^{-1} \\left( \\Vpiprime - \\rpi - \\gamma \\Ppi \\Vpiprime \\right) \\label{eq:lemma_v_adv}.\n\\end{align}\n\\label{lemma:v_adv}\n\\end{lemma}\n\\begin{proof}\nUsing Bellman equation, we have\n\\begin{equation}\n    \\Vpiprime - \\Vpi = \\rpiprime + \\gamma \\Ppiprime \\Vpiprime - \\rpi - \\gamma \\Ppi \\Vpi. \\label{eq:lemme_1_expand}\n\\end{equation}\nEq.~\\eqref{eq:lemme_1_expand} can be rearranged as\n\\begin{equation*}\n    \\Vpiprime - \\Vpi = \\rpiprime - \\rpi + \\gamma \\Ppiprime \\left( \\Vpiprime - \\Vpi \\right) + \\gamma \\left(\\Ppiprime - \\Ppi \\right) \\Vpi,\n    \\label{eq:lemma_adv_v_proof}\n\\end{equation*}\nand Eq.~\\eqref{eq:lemma_adv_v} follows.\n\nTo get Eq.~\\eqref{eq:lemma_v_adv}, we rearrange Eq.~\\eqref{eq:lemme_1_expand} as\n\\begin{equation*}\n    \\Vpiprime - \\Vpi = \\rpiprime - \\rpi + \\gamma \\Ppi \\left( \\Vpiprime - \\Vpi \\right) + \\gamma \\left(\\Ppiprime - \\Ppi \\right) \\Vpiprime, \\label{eq:lemma_v_adv_proof}\n\\end{equation*}\nand Eq.~\\eqref{eq:lemma_v_adv} follows.\n\\end{proof}\n\n\nOur first result is an immediate consequence of re-evaluating the value function after an action switch.\n\\begin{proposition}\nThe value function is non-decreasing with respect to action switches in GPI, i.e., $\\Vki{k}{i+1} \\ge \\Vki{k}{i}$.\n\\label{proposition:action_switch_monotone}\n\\end{proposition}\n\n\\begin{proof}\nFrom Eq.~\\eqref{eq:lemma_adv_v} in Lemma~\\ref{lemma:v_adv}, we have \n% \\begin{align}\n%     \\Vpiprime - \\Vpi &= \\left(I - \\gamma \\Ppiprime\\right)^{-1} \\left( \\rpiprime + \\gamma \\Ppiprime \\Vpi - \\Vpi \\right)  \\label{eq:lemma_adv_v},\\\\\n%     \\Vpiprime - \\Vpi &= \\left(I - \\gamma \\Ppi \\right)^{-1} \\left( \\Vpiprime - \\rpi - \\gamma \\Ppi \\Vpiprime \\right) \\label{eq:lemma_v_adv}.\n% \\end{align}\n\\begin{equation*}\n    \\left(I - \\gamma \\Ppiprime\\right)\\left(\\Vpiprime - \\Vpi\\right) = \\rpiprime + \\gamma \\Ppiprime \\Vpi - \\Vpi.\n\\end{equation*}\nSince $\\Ppi \\ge 0$, we have \n\\begin{equation*}\n    \\Vpiprime - \\Vpi \\ge \\rpiprime + \\gamma \\Ppiprime \\Vpi - \\Vpi = \\bellop^{\\pi'} \\Vpi - \\Vpi,\n\\end{equation*}\nwhich implies that for any $\\pi'$, $\\pi$,\n\\begin{equation}\n    \\Vpiprime \\ge \\bellop^{\\pi'} \\Vpi. \\label{eq:v_ge_tv}\n\\end{equation}\nNow, consider $\\piki{k}{i+1}$ and $\\piki{k}{i}$. According to the updating rule of GPI, for state $i$ we have $\\Vki{k}{i+1}(i) \\ge \\Vki{k}{i}(i)$. For state $j \\ne i$, we have\n\\begin{equation*}\n    \\Vki{k}{i+1}(j) \\ge \\Tki{k}{i+1}\\Vki{k}{i}(j) = \\Tki{k}{i}\\Vki{k}{i}(j) = \\Vki{k}{i}(j).\n\\end{equation*}\nCombined, we have $\\Vki{k}{i+1} \\ge \\Vki{k}{i}$, which completes the proof.\n\\end{proof}\n\nWe next turn to the complexity of GPI and bound the number of iterations required \nto find the optimal solution. The analysis depends on the lemma described as follows.\n\n\\begin{lemma}\nLet $\\Vstar$ denote the optimal value. At iteration $k$ of GPI, we have the following inequality.\n    \\begin{equation*}\n        \\left(\\Vstar - \\Vki{k}{i}\\right)(i) \\le \\gamma \\Pstar \\left(\\Vstar - \\Vki{k-1}{i} \\right)(i).\n    \\end{equation*}\n    \\label{lemma:v_diff_opt_k}\n\\end{lemma}\n\\begin{proof}\nFrom Bellman equation, we have\n    \\begin{align*}\n        \\Vstar - \\Vki{k}{i} & = \\Tstar \\Vstar - \\Vki{k}{i} \\\\\n        & = \\Tstar \\Vstar - \\Tstar \\Vki{k-1}{i} + \\Tstar \\Vki{k-1}{i} - \\Vki{k}{i}.\n    \\end{align*}\n    For state $i$, we have\n    \\begin{align}\n        & \\left(\\Vstar - \\Vki{k}{i}\\right)(i) \\\\ \n        & = \\gamma \\Pstar \\left(\\Vstar - \\Vki{k-1}{i} \\right)(i) + \\left(\\Tstar \\Vki{k-1}{i} - \\Vki{k}{i}\\right)(i) \\nonumber \\\\\n        & \\le \\gamma \\Pstar \\left(\\Vstar - \\Vki{k-1}{i} \\right)(i) + \\left(\\max_\\pi \\bellop^\\pi \\Vki{k-1}{i} - \\Vki{k}{i} \\right)(i)\\nonumber \\\\\n        & \\le \\gamma \\Pstar \\left(\\Vstar - \\Vki{k-1}{i} \\right)(i) + \\left(\\Vki{k}{i} - \\Vki{k}{i}\\right)(i) \\label{eq:proof_lemma_ineq} \\\\\n        & =  \\gamma \\Pstar \\left(\\Vstar - \\Vki{k-1}{i} \\right)(i).\n        \\nonumber\n    \\end{align}\n    % \\eqref{eq:proof_gpi_update_ineq} is because of the updating rule of GPI,\n    % \\begin{align*}\n    %     \\Tstar \\Vki{k-1}{i} (i) & = \\left(\\rstar + \\gamma \\Pstar \\Vki{k-1}{i} \\right)(i) \\\\\n    %     & \\le \\max_{\\pi} \\left(\\rpi + \\gamma \\Ppi \\Vki{k-1}{i} \\right)(i) \\\\\n    %     & = \\Tki \\Vki{k-1}{i} (i).\n    % \\end{align*}\n    Let $\\pi' = \\argmax_\\pi \\bellop^\\pi \\Vki{k-1}{i}$. The inequality \\eqref{eq:proof_lemma_ineq} is because of the updating rule of GPI and \\eqref{eq:v_ge_tv},\n    % \\begin{align*}\n    %     \\left( \\bellop^{\\pi'} \\Vki{k-1}{i} - \\Vki{k-1}{i} \\right) & = \\left(I - \\gamma P^{\\pi'} \\right) \\left( \\Vki{k}{i} - \\Vki{k-1}{i} \\right) \\\\\n    %     & \\le \\Vki{k}{i} - \\Vki{k-1}{i},\n    % \\end{align*}\n    \\begin{equation*}\n        \\Vki{k}{i}\\ge V^{\\pi'}_i \\ge \\bellop^{\\pi'}\\Vki{k-1}{i},\n    \\end{equation*}\n    which completes the proof.\n\\end{proof}\n\n\\begin{theorem}\n GPI finds the optimal policy in $\\bigO{\\frac{|\\actions|}{1-\\gamma}\\log \\frac{1}{1-\\gamma}}$ iterations.\n\\end{theorem}\n\n\\begin{proof}\nDefine $\\Dk \\in \\realset^{|\\states|}$ with $\\Dk(s) = \\Vstar(s) - \\Vki{k}{i}(s),\\, \\forall s\\in \\states$. Then, by Lemma~\\ref{lemma:v_diff_opt_k}, we have \n\\begin{align*}\n    \\Dk & \\le \\gamma \\Pstar \\Dkmo, \\\\\n    \\norminf{\\Dk} & \\le \\gamma^k  \\norminf{\\Dz}.\n\\end{align*}\nLet $j$ be the state such that $\\Dz(j) = \\norminf{\\Dz}$, the following properties can be obtained by Eq.~\\eqref{eq:lemma_v_adv} in Lemma~\\ref{lemma:v_adv}.\n\\begin{align*}\n    \\norminf{\\Dk} &\\le \\gamma^k  \\norminf{\\Dz} \\\\\n    & \\le \\gamma^k \\left\\lVert\\parentheses{I - \\gamma \\Pki{0}{j}}^{-1}\\right\\rVert_{\\infty} \\left\\lVert\\Vstar - \\Tki{0}{j} \\Vstar \\right\\rVert_{\\infty} \\\\\n    & = \\frac{\\gamma^k}{1 - \\gamma} \\left( \\Vstar - \\Tki{0}{j} \\Vstar \\right)(j).\n\\end{align*}\nAlso from Eq.~\\eqref{eq:lemma_v_adv}, we have \n\\begin{equation}\n    \\parentheses{\\Vstar - \\Tki{k}{j} \\Vstar}(j) \\le \\Dk(j) \\le \\norminf{\\Dk}.\n\\end{equation}\nIt follows that\n\\begin{align*}\n    \\left(\\Vstar - \\Tki{k}{j} \\Vstar \\right) (j) \\le \\frac{\\gamma^k}{1 - \\gamma} \\left( \\Vstar - \\Tki{0}{j} \\Vstar \\right)(j),\n\\end{align*}\nwhich implies when $\\frac{\\gamma^k}{1 - \\gamma} < 1$, the non-optimal action for $j$ in $\\pi^{(0)}$ is switched in $\\pi^{(k)}$ and will never be switched back to in future iterations.\nNow we are ready to bound $k$. By taking the logarithm for both sides of $\\frac{\\gamma^k}{1 - \\gamma} < 1$, we have\n\\begin{align*}\n    k \\log \\gamma & \\ge \\log (1 - \\gamma) \\\\\n    k & > \\frac{\\log(1 - \\gamma)}{\\log \\gamma} = \\frac{\\log \\frac{1}{1 - \\gamma}}{\\log \\frac{1}{\\gamma}} \\\\\n    k & > \\frac{1}{1 - \\gamma}\\log \\frac{1}{1 - \\gamma} \\;\\;\\;\\; \\left(\\log\\frac{1}{\\gamma} \\ge \\frac{\\frac{1}{\\gamma} - 1}{\\frac{1}{\\gamma}} = 1 - \\gamma\\right).\n\\end{align*}\nEach non-optimal action is eliminated after at most $\\bigO{\\frac{1}{1 - \\gamma}\\log \\frac{1}{1 - \\gamma}}$ iterations, and there are $\\bigO{|\\actions|}$ non-optimal actions. Thus, GPI takes at most $\\bigO{\\frac{|\\actions|}{1 - \\gamma}\\log \\frac{1}{1 - \\gamma}}$ iterations to reach the optimal policy.\n\\end{proof}\n\n\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Asynchronous Geometric Policy Iteration",
                    "content": "\nWhen the state set is large it would be beneficial to perform policy updates in an orderless way~\\cite{Sutton1998}. This is because iterating over the entire state set may be prohibitive, and exactly evaluating the value function with Eq.~\\eqref{eq:bellman_eq} may be too expensive. Thus, in practice, the value function is often approximated when the state set is large. One example is modified policy iteration~\\cite{puterman_modified_pi, puterman94markov} where the policy evaluation step is approximated with certain steps of value iteration. \n\nSince GPI avoids the matrix inversion by updating the value function incrementally, it has the potential to update the policy for arbitrary states available to the agent. This property also opens up the possibility of asynchronous (orderless) updates of policies and the value function when the state set is large or the agent has to update the policy for the state it encounters in real-time. The asynchronous update strategy can also help avoid being stuck in states that lead to minimal progress and may reach the optimal policy without reaching a certain set of states. \n\nAsynchronous GPI (Async-GPI) follows the action selection mechanism of GPI, and its general framework is as follows. Assume the transition matrix is available to the agent, we randomly initialize the policy $\\pi^{(0)}$ and calculate the initial $\\bQ^{(0)}$ and $V^{(0)}$ accordingly. In real-time settings, the sequence of states $\\{s_0, s_1,s_2, \\ldots \\}$ are collected by an agent through real-time interaction with the environment. At time step $t$, we search for an action switch for state $s_t$ using Eq.~\\eqref{eq:gpiv_action_selection}. Then, we update the $\\pi^{(t)}$, $\\bQ^{(t)}$ with Eq.~\\eqref{eq:gpi_Q_update}, and $V^{(t)}$. Asynchronous value-based methods converge if each state is visited infinitely often~\\cite{bertsekasAsyncVI}. We later demonstrate in experiments that Async-GPI converges well in practice.\n\n\n\n\n% \\begin{algorithm}\n% \\caption{Asyn-GPI}\n% \\hspace*{\\algorithmicindent} \\textbf{Input:} $\\transitions$, $\\rewards$, $\\gamma$\n% \\begin{algorithmic}[1]\n%     \\State Set counter $k = 0$, randomly generate an initial policy $\\piki{k}{}$.\n%     \\State Calculate $\\bQki{k}{} = (I - \\gamma \\Pki{k}{})^{-1}$, $\\Vki{k}{} = \\bQki{k}{} \\rki{k}{}$. \\label{alg:gpi_line_initial_eval}\n%     % \\item[\\textbf{3.}] For all $s \\in \\states$\n%     \\State Pick an infinite sequence $\\{s_1, s_2, \\ldots\\}$.\n%     \\While{not converged}\n%         \\State calculate the best action $a_i$ according to Eq.~\\eqref{eq:gpiv_action_selection};\n%         \\label{alg:gpi_line_action_select}\n%         \\State update $\\bQki{k}{i}$ according to Eq.~\\eqref{eq:gpi_Q_update};\n%         \\State $\\piki{k}{i}(i) = a_i$;\\label{alg:gpi_line_pi_update}\n%         \\State $\\Vki{k}{i} = \\bQki{k}{i} \\, \\rki{k}{i}$;\n%     \\EndWhile\n%     \\State If $\\Vki{k}{n}$ is optimal, set $\\pi^* = \\piki{k}{n}$ and return. \n%     Otherwise, set $\\bQki{k+1}{0} := \\bQki{k}{n}$, $\\piki{k+1}{0} := \\piki{k}{n}$, $\\Vki{k+1}{0} := \\Vki{k}{n}$, $k := k + 1$ and go to step~\\ref{alg:gpi_line_for}.\n% \\end{algorithmic}\n% \\label{alg:gpi}\n% \\end{algorithm}\n\n\n% \\begin{table}[t]\n% \\begin{tabular}{c c c c c c c}\n%     & \\textbf{\\# iter.} & \\textbf{\\# switches} & \\textbf{wall time} (s) & \\textbf{\\# iter.} & \\textbf{\\# switches} & \\textbf{wall time} (s)\\\\\n%     % \\vspace{2pt}\n%     \\hline\n%     & \\multicolumn{3}{c}{$|\\states|=100$, $|\\actions|=5,000$} & \\multicolumn{3}{c}{$|\\states|=100$, $|\\actions|=10,000$} \\\\\n%     \\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n%     SPI & $-$ & \\textbf{187.7} & $5.08$ & $-$ & \\textbf{187.0} & $12.27$ \\\\\n%     PI & $6.0$ & $316.6$ & $0.60$ & $6.2$ & $389.8$ & $1.12$ \\\\\n%     GPI & \\textbf{4.0} & $216.4$ & \\textbf{0.28} & \\textbf{4.0} & $234.2$ & \\textbf{0.49} \\\\\\\\\n%     % \\bottomrule\n%     & \\multicolumn{3}{c}{$|\\states|=300$, $|\\actions|=90,000$} & \\multicolumn{3}{c}{$|\\states|=300$, $|\\actions|=18,000$} \\\\\n%     \\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n%     SPI & $-$ & \\textbf{653} & $107$ & $-$ & \\textbf{671} & $756.57$ \\\\\n%     PI & $6.4$ & $1,310$ & $5.93$ & $6.2$ & $1,361$ & $10.84$ \\\\\n%     GPI & \\textbf{4.0} & $821$ & \\textbf{3.22} & \\textbf{4.0} & $873$ & \\textbf{6.07} \\\\\\\\\n%     & \\multicolumn{3}{c}{$|\\states|=500$, $|\\actions|=100,000$} & \\multicolumn{3}{c}{$|\\states|=500$, $|\\actions|=250,000$} \\\\\n%     \\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n%     PI & $7.2$ & $2,171$ & $7.38$ & $4.8$ & $1,946$ & $12.68$ \\\\\n%     GPI & \\textbf{4.2} & \\textbf{1,317} & \\textbf{4.68} & \\textbf{3.0} & \\textbf{1,240} & \\textbf{7.60} \\\\\\\\\n%     & \\multicolumn{3}{c}{$|\\states|=1,000$, $|\\actions|=500,000$} & \\multicolumn{3}{c}{$|\\states|=1,000$, $|\\actions|=1,000,000$} \\\\\n%     \\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n%     PI & $7.4$ & $4,676$ & $47.04$ & $7.2$ & $4,610$ & $89.18$ \\\\\n%     GPI & \\textbf{5.0} & \\textbf{2,729} & \\textbf{41.58} & \\textbf{4.6} & \\textbf{2,709} & \\textbf{67.72}\n% \\end{tabular}\n% \\caption{Experiments on various MDPs. The number of actions per state can be obtained by $|\\actions|/|\\states|$.}\n% \\label{table:results}\n% \\end{table}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\\label{sec:gpi_experiments}\n\nWe test GPI on random MDPs of different sizes. The baselines are policy iteration~(PI) and simple policy iteration~(SPI). We compare the number of iterations, actions switches, and wall time. Here we denote the number of iterations as the number of sweeps over the entire state set. Action switches are those policy updates within each iteration. The results are shown in Figure~\\ref{fig:results_1}. We generate MDPs with $|\\states| = \\{100, 200, 300, 500, 1000\\}$ corresponding to Figure~\\ref{fig:results_1} (a)-(e). For each state size, we increase the number of actions (horizontal axes) to observe the difference in performance. The rows from the top to bottom are the number of iterations, action switches and wall time (vertical axes), respectively. Since SPI only performs one action switch per iteration, we only show its number of action switches. The purpose of adding SPI to the baseline is to verify if our GPI can effectively reduce the number of action switches. Since SPI sweeps over the entire state set and updates a single state with the largest improvement, it is supposed to have the least number of action switches. However, SPI's larger complexity of performing one update should lead to higher running time. This is supported by the experiments as Figure~\\ref{fig:results_1} (a) and (b) show that SPI (green curves) takes the least number of switches and longest time. We drop SPI in Figure~\\ref{fig:results_1} (c)-(e) to have a clearer comparison between GPI and PI (especially in wall time). \nThe proposed GPI has a clear advantage over PI in almost all tests. The second row of Figure~\\ref{fig:results_1} (a) and (b) shows that the number of action switches of GPI is significantly fewer than PI and very close to SPI although the complexity of a switch is cheaper by a factor of $|\\states|$. And the reduction in the number of action switches leads to fewer iterations. Another important observation is that the margin increases as the action set becomes larger. This is strong empirical evidence that demonstrates the benefits of GPI's action selection strategy which is to reach the endpoints of line segments in the value function polytope. The larger the action set is, the more policies lying on the line segments and thus the more actions being excluded in one switch. The wall time of GPI is also very competitive compared to PI which further demonstrates that GPI can be a very practical algorithm for solving finite discounted MDPs.\n\nWe also test the performance of the asynchronous GPI (Async-GPI) on MDPs with $|\\states| = \\{300, 500, 1000, 2000\\}$ and $|\\actions| = 100$. For each setting, we randomly generate a sequence of states that is larger than $|\\states|$. We compare Async-GPI with asynchronous value iteration (Async-VI) which is classic asynchronous dynamic programming algorithm. At time step $t$, Async-VI performs one step of the optimality Bellman operator on a single state $s_t$ that is available to the algorithm. The results are shown in Figure~\\ref{fig:async}. The mean of the value function is plotted against the number of updates. We observe that Async-GPI took significantly fewer updates to reach the optimal value function. The gap becomes larger when the state set grows in size. These results are expected because Async-GPI also has a higher complexity to perform an update and Async-VI never really solves the real value function before reaching the optimality. \n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
            },
            "section 6": {
                "name": "Conclusions and Future Work",
                "content": "\nIn this paper, we discussed the geometric properties of finite MDPs. We characterized the hyperplane arrangement that includes the boundary of the value function polytope, and further related it to the MDP-LP polytope by showing that they share the same hyperplane arrangement. Unlike the well-defined MDP-LP polytope, it remains unclear which bounding hyperplanes are active and which halfspaces of them belong to the value space. Besides the conjecture stated earlier, we would like to understand in the future which cells of the hyperplane arrangement form the value function polytope, and may derive a bound on the number of convex cells. It is also plausible that the rest of the hyperplane arrangement will help us devise new algorithms for solving MDPs. \n\nFollowing the fact that policies that differ in only one state are mapped onto a line segment in the value function polytope, and that the only two policies on the polytope boundary are deterministic in that state, we proposed a new algorithm called geometric policy iteration that guarantees to reach an endpoint of the line segment for every action switch. We developed a mechanism that makes the value function monotonically increase with respect to action switches and the whole process can be computed efficiently. Our experiments showed that our algorithm is very competitive compared to the widely-used policy iteration and value iteration. We believe this type of algorithm can be extended to multi-agent settings, e.g., stochastic games~\\cite{shapley1953stochastic}. It will also be interesting to apply similar ideas to model-based reinforcement learning. \n\n\\begin{acks}\nThis work is supported by NSF DMS award 1818969 and a seed award from Center for Data Science and Artificial Intelligence Research at UC Davis. \n\\end{acks}\n\n% \\clearpage\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{sample-base}\n\n"
            }
        },
        "figures": {
            "fig:vfp_line_left": "\\begin{figure}[h]\n    \\centering\n    \\captionsetup[subfloat]{farskip=0pt,captionskip=-1pt}\n    \\subfloat[\\label{fig:vfp_line_left}]{\\includegraphics[width=0.51\\linewidth]{imgs/vfp_and_lines_left.png}} \\hspace{-1.2em}\n    \\subfloat[\\label{fig:vfp_line_right}]{\\includegraphics[width=0.51\\linewidth]{imgs/vfp_and_lines_right.png}}\\\\[-2ex]\n    \\caption{The blue regions are the value spaces of 2 MDPs with $|\\states|=2$ and $|\\actions|=2$. The regions are obtained by plotting $f_v$ of $50,000$ random policies. (a): Both $\\{\\pi_i\\}$ and $\\{\\delta_i\\}$ agree on $s_1$ but differ in $s_2$. $\\pi_1$ and $\\pi_3$ are deterministic. $\\pi_2$ is $s_1$-deterministic. $\\delta_1$ and $\\delta_3$ are $s_2$-deterministic. (b): $\\{\\pi_i\\}$ and $\\{\\delta_i\\}$ agree on $s_1$ and $s_2$, respectively. $\\pi_1$, $\\pi_3$, and $\\delta_1$ are deterministic while $\\pi_2$ and $\\delta_2$ are $s_1$ and $s_2$-deterministic, respectively.}\n    \\label{fig:vfp_line}\n\\end{figure}",
            "fig:vfp_line_right": "\\begin{figure}[h]\n    \\centering\n    \\captionsetup[subfloat]{farskip=0pt,captionskip=-1pt}\n    \\subfloat[\\label{fig:vfp_line_left}]{\\includegraphics[width=0.51\\linewidth]{imgs/vfp_and_lines_left.png}} \\hspace{-1.2em}\n    \\subfloat[\\label{fig:vfp_line_right}]{\\includegraphics[width=0.51\\linewidth]{imgs/vfp_and_lines_right.png}}\\\\[-2ex]\n    \\caption{The blue regions are the value spaces of 2 MDPs with $|\\states|=2$ and $|\\actions|=2$. The regions are obtained by plotting $f_v$ of $50,000$ random policies. (a): Both $\\{\\pi_i\\}$ and $\\{\\delta_i\\}$ agree on $s_1$ but differ in $s_2$. $\\pi_1$ and $\\pi_3$ are deterministic. $\\pi_2$ is $s_1$-deterministic. $\\delta_1$ and $\\delta_3$ are $s_2$-deterministic. (b): $\\{\\pi_i\\}$ and $\\{\\delta_i\\}$ agree on $s_1$ and $s_2$, respectively. $\\pi_1$, $\\pi_3$, and $\\delta_1$ are deterministic while $\\pi_2$ and $\\delta_2$ are $s_1$ and $s_2$-deterministic, respectively.}\n    \\label{fig:vfp_line}\n\\end{figure}",
            "fig:vfp_line": "\\begin{figure}[h]\n    \\centering\n    \\captionsetup[subfloat]{farskip=0pt,captionskip=-1pt}\n    \\subfloat[\\label{fig:vfp_line_left}]{\\includegraphics[width=0.51\\linewidth]{imgs/vfp_and_lines_left.png}} \\hspace{-1.2em}\n    \\subfloat[\\label{fig:vfp_line_right}]{\\includegraphics[width=0.51\\linewidth]{imgs/vfp_and_lines_right.png}}\\\\[-2ex]\n    \\caption{The blue regions are the value spaces of 2 MDPs with $|\\states|=2$ and $|\\actions|=2$. The regions are obtained by plotting $f_v$ of $50,000$ random policies. (a): Both $\\{\\pi_i\\}$ and $\\{\\delta_i\\}$ agree on $s_1$ but differ in $s_2$. $\\pi_1$ and $\\pi_3$ are deterministic. $\\pi_2$ is $s_1$-deterministic. $\\delta_1$ and $\\delta_3$ are $s_2$-deterministic. (b): $\\{\\pi_i\\}$ and $\\{\\delta_i\\}$ agree on $s_1$ and $s_2$, respectively. $\\pi_1$, $\\pi_3$, and $\\delta_1$ are deterministic while $\\pi_2$ and $\\delta_2$ are $s_1$ and $s_2$-deterministic, respectively.}\n    \\label{fig:vfp_line}\n\\end{figure}",
            "fig:vfp_lp_boundary": "\\begin{figure}[h]\n    \\centering\n    \\captionsetup[subfloat]{farskip=0pt,captionskip=-1pt}\n    \\subfloat[\\label{fig:vfp_lp_boundary}]{\\includegraphics[width=0.45\\linewidth, height=3.5cm]{imgs/vfp_lp_polytope_left.png}} \\hspace{-1.2em}\n    \\subfloat[\\label{fig:vfp_boundary}]{\\includegraphics[width=0.53\\linewidth, height=3.5cm]{imgs/vpolytope_w_boundary_multicolor2.png}}\\\\[-2ex]\n    \\caption{(a): $f_v$ polytope (blue) and MDP-LP polytope (green) of an MDP with $|\\states|=2$ and $|\\actions|=2$. (b) $f_v$ polytope overlapped with the hyperplane arrangement $H_{MDP}$ from Theorem~\\ref{thm: vfp_boundary}. This MDP has 3 actions so $|H_{MDP}|=6$.}\n\\end{figure}",
            "fig:vfp_boundary": "\\begin{figure}[h]\n    \\centering\n    \\captionsetup[subfloat]{farskip=0pt,captionskip=-1pt}\n    \\subfloat[\\label{fig:vfp_lp_boundary}]{\\includegraphics[width=0.45\\linewidth, height=3.5cm]{imgs/vfp_lp_polytope_left.png}} \\hspace{-1.2em}\n    \\subfloat[\\label{fig:vfp_boundary}]{\\includegraphics[width=0.53\\linewidth, height=3.5cm]{imgs/vpolytope_w_boundary_multicolor2.png}}\\\\[-2ex]\n    \\caption{(a): $f_v$ polytope (blue) and MDP-LP polytope (green) of an MDP with $|\\states|=2$ and $|\\actions|=2$. (b) $f_v$ polytope overlapped with the hyperplane arrangement $H_{MDP}$ from Theorem~\\ref{thm: vfp_boundary}. This MDP has 3 actions so $|H_{MDP}|=6$.}\n\\end{figure}",
            "fig:gpiv_endpoints": "\\begin{figure}[h]\n    \\centering\n    \\captionsetup[subfloat]{farskip=0pt,captionskip=-1pt}\n    \\subfloat[]{\\includegraphics[width=0.51\\linewidth]{imgs/action_switch_gpiv_left.png}} \\hspace{-1.2em}\n    \\subfloat[]{\\includegraphics[width=0.51\\linewidth]{imgs/action_switch_gpiv_right.png}}\\\\[-2ex]\n    \\caption{The value sequences of one iteration which involves a sweep over all states looking for policy updates. (a): In PI, we may not reach the end of a line segment for an action switch. (b): An endpoint is always reached in GPI.}\n    \\label{fig:gpiv_endpoints}\n\\end{figure}",
            "fig:gpiv_monotone": "\\begin{figure}[h]\n    \\centering\n    \\captionsetup[subfloat]{farskip=0pt,captionskip=-1pt}\n    \\subfloat[]{\\includegraphics[width=0.51\\linewidth]{imgs/action_switch_pi_gpi_left.png}} \\hspace{-1.2em}\n    \\subfloat[]{\\includegraphics[width=0.51\\linewidth]{imgs/action_switch_pi_gpi_right.png}}\\\\[-2ex]\n    \\caption{Two paths are shown for each PI, GPI. The green and red paths denote one iteration with $\\pi(s_1)$ and $\\pi(s_2)$ updated first, respectively. (a): The policy improvement path of PI. The red path is not action-switch-monotone which will lead to an additional iteration. (b): GPI is always action-switch-monotone. The red path achieves the optimal values in one action switch.}\n    \\label{fig:gpiv_monotone}\n\\end{figure}",
            "fig:results_1": "\\begin{figure*}[t]\n    % \\captionsetup[subfloat]\n    \\centering\n    \\subfloat[]{\\includegraphics[width=0.20\\linewidth, height=3.1cm]{imgs/results_s100_time.png}}\n    \\hfil\n    \\subfloat[]{\\includegraphics[width=0.20\\linewidth, height=3.1cm]{imgs/results_s200_time.png}}\n    \\hfil\n    \\subfloat[]{\\includegraphics[width=0.20\\linewidth, height=3.1cm]{imgs/results_s300_time.png}}\n    \\hfil\n    \\subfloat[]{\\includegraphics[width=0.20\\linewidth, height=3.1cm]{imgs/results_s500_time.png}}\n    \\hfil\n    \\subfloat[]{\\includegraphics[width=0.20\\linewidth, height=3.1cm]{imgs/results_s1000_time.png}}\n    \\caption{The results of MDPs with $\\{100, 200, 300, 500, 1000\\}$ states in (a)-(e). The horizontal axes are the number of actions for all graphs. The vertical axes are the number of iterations, number of action switches, and wall time for the first to the third row, respectively. The performance curves of SPI, PI, and GPI are in green, blue, and red, respectively. The SPI curves are only presented in (a) and (b) to provide a ``lower bound\" on the number of action switches, and are dropped for larger MDPs due to its higher running time. The number of switches of GPI remains low compared to PI. The proposed GPI consistently outperforms PI in both iteration count and wall time. The advantages of GPI become more significant as the action set size grows.}\n    \\label{fig:results_1}\n\\end{figure*}",
            "fig:async": "\\begin{figure*}[t]\n    \\centering\n    \\subfloat[]{\\includegraphics[width=0.25\\linewidth, height=3.2cm]{imgs/async_s300.png}}\n    \\hfil\n    \\subfloat[]{\\includegraphics[width=0.25\\linewidth, height=3.2cm]{imgs/async_s500.png}}\n    \\hfil\n    \\subfloat[]{\\includegraphics[width=0.25\\linewidth, height=3.2cm]{imgs/async_s1000.png}}\n    \\hfil\n    \\subfloat[]{\\includegraphics[width=0.25\\linewidth, height=3.2cm]{imgs/async_s2000.png}}\n    \\caption{Comparison between asynchronous geometric policy iteration (red curve) and asynchronous value iteration (blue curve) in 4 MDPs. $|\\actions|=100$ for all MDPs and $|\\states|=\\{300, 500, 1000, 2000\\}$ for (a)-(d), respectively. The horizontal axes are the number of updates. The vertical axes show the mean of the value function.}\n    \\label{fig:async}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation*}\nV^{\\pi}(s) = \\E\\nolimits_{P^\\pi}\\Big(\\sum^{\\infty}_{i=0} \\gamma^{i}\n\\rewards(s_i, a_i) \\cbar s_0=s \\Big). \\label{eq:v_func_cumulative_reward}\n\\end{equation*}",
            "eq:2": "\\begin{align} V^\\pi(s) = \\E\\nolimits_{P^\\pi}\\Big(\\rewards(s, a) + \\gamma V^\\pi(s')\\Big).\n\\label{eq:bellman}\n\\end{align}",
            "eq:3": "\\begin{align*}\n    \\rpi(s) &= \\sum_{a \\in \\actions} \\pi(a \\cbar s) \\rewards(s, a), \\\\\n    \\Ppi(s' \\cbar s) &= \\sum_{a \\in \\actions} \\pi(a \\cbar s) \\transitions (s' \\cbar s, a),\n\\end{align*}",
            "eq:4": "\\begin{align}\n    V^\\pi &= \\rpi + \\gamma \\Ppi V^\\pi \\nonumber\\\\\n    & = (I - \\gamma \\Ppi)^{-1} \\rpi. \\label{eq:bellman_eq}\n\\end{align}",
            "eq:5": "\\begin{align*}\n    \\bellop^{\\pi} V &= \\rpi + \\gamma \\Ppi V, \\\\\n    \\bellop^* V &= \\max_{\\pi} \\bellop^{\\pi} V.\n\\end{align*}",
            "eq:6": "\\begin{equation}\n    f_v(\\pi) = (I - \\gamma \\Ppi)^{-1} \\rpi.\n    \\label{eq:value_func_mapping}\n\\end{equation}",
            "eq:7": "\\begin{equation*}\n    \\pi'(s) \\in \\argmax_{a \\in \\actions}\\left\\{ \\rewards(s, a) + \\gamma \\sum_{s'}\\transitions(s' \\cbar s, a) \\Vpi(s') \\right\\},\\, \\forall s \\in \\states.\n\\end{equation*}",
            "eq:8": "\\begin{equation*}\n    \\tilde{A}(s, a) = \\rewards(s, a) + \\gamma \\sum_{s'}\\transitions(s' \\cbar s, a) \\Vpi(s') - \\Vpi(s). \n\\end{equation*}",
            "eq:9": "\\begin{align*}\n        \\min_V & \\quad \\sum_{s} \\alpha(s) V(s) \\\\\n        \\text{s.t.}  & \\quad V(s) \\ge \\rewards(s, a) + \\gamma \\sum_{s'}\\transitions(s' \\cbar s, a) V(s'),\\,\\, \\forall s \\in \\states, a \\in \\actions.\n    \\end{align*}",
            "eq:10": "\\begin{equation}\n    a_i \\in \\argmax_{a \\in \\actions}\\left\\{\\parentheses{ \\bone_i + \\frac{\\bQki{k}{i-1}(i, i)}{1 - \\bw^\\top_a \\,\\bqi} \\, \\bw_a}^{\\top} \\parentheses{\\Vki{k}{i-1} + \\Delta r_a \\, \\bqi } \\right\\},\n    \\label{eq:gpiv_action_selection}\n\\end{equation}",
            "eq:11": "\\begin{align}\n    \\bw_a &= \\gamma \\left(\\transitions(i, a) - \\transitions(i, \\piki{k}{i-1}(i))\\right),\\label{eq:gpiv_w} \\\\ \n    \\Delta r_a &= \\parentheses{\\rewards(i, a) - \\rewards(i, \\piki{k}{i-1}(i))},\\label{eq:gpiv_r}\n\\end{align}",
            "eq:12": "\\begin{equation}\n    \\bQki{k}{i} = \\bQki{k}{i-1} + \\frac{\\bqi \\, \\bwiT \\bQki{k}{i-1}}{1 - \\bwiT \\bqi}. \\label{eq:gpi_Q_update}\n\\end{equation}"
        }
    }
}