{
    "meta_info": {
        "title": "Finding Favourite Tuples on Data Streams with Provably Few Comparisons",
        "abstract": "One of the most fundamental tasks in data science is to assist a user with\nunknown preferences in finding high-utility tuples within a large database. To\naccurately elicit the unknown user preferences, a widely-adopted way is by\nasking the user to compare pairs of tuples. In this paper, we study the problem\nof identifying one or more high-utility tuples by adaptively receiving user\ninput on a minimum number of pairwise comparisons. We devise a single-pass\nstreaming algorithm, which processes each tuple in the stream at most once,\nwhile ensuring that the memory size and the number of requested comparisons are\nin the worst case logarithmic in $n$, where $n$ is the number of all tuples. An\nimportant variant of the problem, which can help to reduce human error in\ncomparisons, is to allow users to declare ties when confronted with pairs of\ntuples of nearly equal utility. We show that the theoretical guarantees of our\nmethod can be maintained for this important problem variant. In addition, we\nshow how to enhance existing pruning techniques in the literature by leveraging\npowerful tools from mathematical programming. Finally, we systematically\nevaluate all proposed algorithms over both synthetic and real-life datasets,\nexamine their scalability, and demonstrate their superior performance over\nexisting methods.",
        "author": "Guangyi Zhang, Nikolaj Tatti, Aristides Gionis",
        "link": "http://arxiv.org/abs/2307.02946v1",
        "category": [
            "cs.DB",
            "cs.DS"
        ],
        "additionl_info": "To appear in KDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{se:intro}\n\n\n\nOne of the most fundamental tasks in data science is to assist \na user with unknown preferences in finding high-utility tuples within a large database.\nSuch a task can be used, for example, \nfor finding relevant papers in scientific literature, \nor recommending favorite movies to a user.\nHowever, utility of tuples is highly personalized.\n``\\emph{One person's trash is another person's treasure,}'' as the saying goes.\nThus, a prerequisite to accomplishing this task is to efficiently and accurately \nelicit user preferences.\n\nIt has long been known, \nboth from studies in psychology \\citep{stewart2005absolute} \nas well as from personal experience, \nthat humans are better at performing \\emph{relative comparisons} \nthan \\emph{absolute assessments}.\nFor instance, \nit is typically easy for a user to select a favorite movie between two given movies,\nwhile it is difficult to score the exact utility of a given movie.\nThis fact has been used in many applications,\nsuch as classification \\citep{haghiri2018comparison}, \nranking \\citep{liu2009learning},\nand clustering~\\citep{chatziafratis2021maximizing}.\n\nIn this paper we leverage the observation that humans are better at comparing\nrather than scoring information items, \nand use relative comparisons to facilitate preference learning and help users find relevant tuples\nin an interactive fashion, \ni.e., by adaptively asking users to compare pairs of tuples.\nTo cope with the issue of information overload, \nit is usually not necessary to identify all relevant tuples for a user.\nInstead, if there exists a small set of high-utility tuples in the database,\na sensible goal is to identify at least one high-utility tuple \nby making a minimum number of comparisons.\nIn particular, assuming that a user acts as an \\emph{oracle},\nthe number of requested comparisons, \nwhich measures the efficiency of preference learning, \nis known as \\emph{query complexity}.\n\nMore specifically, in this paper we focus on the following setting. \nWe consider a database \\D consisting of \\nD tuples, \neach represented as a point in $\\reals^d$.\nUser preference is modeled by an unknown linear function \non the numerical attributes of the data tuples.\nNamely, we assume that a user is associated with an unknown utility \nvector~$\\vecu \\in \\reals^d$, and\nthe utility of a tuple $\\vecx \\in \\reals^d$ for that user is defined to~be \n\\[\n\t\\util(\\vecx) = \\vecu^T\\vecx.\n\\]\n\nA tuple \\vecx is considered to be of high-utility \nif its utility is close to that of the best tuple,\nor more precisely,\nif compared to the best tuple its utility loss is bounded by \nan \\Creg fraction of the best utility,\n\\[\n\t\\util(\\vecx^*) - \\util(\\vecx) \\le \\Creg \\, \\util(\\vecx^*),\n\\]\nwhere $\\vecx^* = \\arg\\max_{\\vecx \\in \\D} \\util(\\vecx)$ is the best tuple in \\D.\nWe call the user-defined parameter \\Creg the ``regret'' ratio,\na terminology used earlier in database literature \\cite{nanongkai2012interactive}.\nWe demonstrate this setting with a concrete example below.\n\n\\begin{example}\nEvery tuple being a point in $\\reals^3$ represents a computer with three attributes:\nprice, CPU speed, and hard disk capacity.\nIt is reasonable to assume that the utility of a computer grows linearly in, \nfor example, the hard disk capacity.\nThus, a user may put a different weight on each attribute, as one entry in the utility vector $\\vecu \\in \\reals^3$, \nwhich measures its relative importance.\n\\end{example}\n\nFor the setting described above\nwith a linear utility function, \nit is obvious that at most $\\nD-1$ comparisons suffice to find the best tuple, \nby sequentially comparing the best tuple so far with a next tuple.\nSurprisingly, despite the importance of this problem in many applications, \nimprovement over the na\\\"ive sequential strategy, in the worst case, has remained elusive.\nA positive result has only been obtained in a very restricted case of two attributes, \ni.e., a tuple is a point in $\\reals^2$~\\citep{xie2019strongly}.\nOther existing improvements rely on strong assumptions~\\citep{jamieson2011active,xie2019strongly},\nfor example, when every tuple is almost equally probable to be the best.\nTo the best of our knowledge, \nwe are the first to offer an improvement on the query complexity \nthat is logarithmic in \\nD, in the worst case.\nWe refer the reader to \\cref{tbl:cmp} for a detailed comparison with existing work.\n\nThere exist heuristics in the literature that are shown to perform empirically better\nthan the na\\\"ive sequential strategy, in terms of the number of requested comparisons.\nFor example, a popular idea is to compare a carefully-chosen pair in each round of interaction with the user~\\citep{qian2015learning,wang2021interactive}.\nHowever, these methods are computationally expensive, and require multiple passes over the whole set of tuples.\nTo illustrate this point, finding a ``good'' pair with respect to a given measure of interest can easily take $\\bigO(\\nD^2)$ time, as one has to go over ${\\nD \\choose 2}$ candidate pairs.\nFurthermore, while such heuristics may work well in practice, \nthey may require $\\Omega(\\nD)$ pairwise comparisons, in the worst case.\n\nWe also address the problem of finding a high-utility tuple \\emph{reliably}, \nwhere we do not force a user to make a clear-cut decision \nwhen confronted with two tuples that have nearly equal utility for the user. \nIn this way we can avoid error-prone decisions by a user.\nInstead, we allow the user to simply declare a tie between the two tuples.\nTo our knowledge, this is the first paper that \nconsiders a scenario of finding a high-utility tuple with ties and\nprovides theoretical guarantees to such a scenario.\n\n\\iffalse\nWe systematically evaluate all proposed algorithms over synthetic and real-life datasets,\nwe demonstrate their superior performance, and\nwe examine their scalability with respect to the data size \\nD, dimension $d$, \nand parameter~\\Creg.\n\\fi\n\nOur contributions in this paper are summarized as follows:\n\t($i$) We devise a \\emph{single-pass} streaming algorithm \n\tthat processes each tuple only once, \n\tand finds a high-utility tuple by making adaptive pairwise comparisons;\n\t($ii$) The proposed algorithm requires a memory size and has query complexity \n\tthat are both logarithmic in \\nD, in the worst case, \n\twhere \\nD is the number of all tuples;\n\t($iii$) We show how to maintain the theoretical guarantee of our method, \n\teven if ties are allowed when comparing tuples with nearly equal utility;\n\t($iv$) We offer significant improvement to existing pruning techniques in the literature, \n\tby leveraging powerful tools from mathematical programming;\n\t($v$) We systematically evaluate all proposed algorithms over synthetic and \n\treal-life datasets, and demonstrate their superior performance compared to existing methods.\n\nThe rest of the paper is organized as follows.\nWe formally define the problem in \\cref{se:def}.\nWe discuss related work in \\cref{se:related}.\nThen, we describe the proposed algorithm in \\cref{se:single}, and \nits extension in \\cref{se:robust} when ties are allowed in a comparison.\nEnhancement to existing techniques follows in \\cref{se:extensions-short}.\nEmpirical evaluation is conducted in \\cref{se:experiment}, \nand we conclude in \\cref{se:conclusion}.\n\n"
            },
            "section 2": {
                "name": "Problem definition",
                "content": "\n\\label{se:def}\n\nIn this section, we formally define the \\emph{interactive regret minimization} (\\irm) problem.\n\nThe goal of the \\irm problem is to find a good tuple among all given tuples $\\D \\subseteq \\reals^d$ in a database.\nThe goodness, or utility, of a tuple \\vecx is determined by an unknown utility vector $\\vecu \\in \\reals^d$ via the dot-product operation $\\util(\\vecx) = \\vecu^T\\vecx$.\nHowever, we assume that we do not have the means to directly compute $\\util(\\vecx)$, \nfor a given tuple \\vecx.\nInstead, we assume that we have access to \nan oracle that can make comparisons between pairs of tuples:\ngiven two tuples \\vecx and \\vecy\nthe oracle will return the tuple with the higher utility. \nThese assumptions are meant to model users\nwho cannot quantify the utility of a given tuple on an absolute scale, \nbut can perform pairwise comparisons of tuples.\n\nIn practice, it is usually acceptable to find a \\emph{sufficiently good} tuple $\\vecx'$ in \\D, instead of the top one $\\vecx^*$.\nThe notion of ``sufficiently good'' is measured by the ratio in utility loss \n$\\frac{\\util(\\vecx^*) - \\util(\\vecx')}{\\util(\\vecx^*)}$, \nwhich is called ``regret.''\nThis notion leads to the definition of the \\irm~problem.\n\\begin{problem}[Interactive Regret Minimization (\\irm)]\n\\label{problem:IRM}\nGiven a set of \\nD tuples in a database $\\D \\subseteq \\reals^d$,\nan unknown utility vector $\\vecu \\in \\reals^d$, and\na parameter $\\Creg \\in [0,1]$,\nfind an \\Creg-regret tuple $\\vecx'$, such~that\n\\[\n\t\\util(\\vecx^*) - \\util(\\vecx') \\le \\Creg \\, \\util(\\vecx^*),\n\\]\nwhere $\\util(\\vecx) = \\vecu^T\\vecx$ and $\\vecx^* = \\arg\\max_{\\vecx \\in \\D} \\util(\\vecx)$.\nIn addition we aim at performing the minimum number of pairwise comparisons.\n\\end{problem}\n\n\\iffalse\n\n\\fi\n\nProblem~\\ref{problem:IRM} is referred to as ``interactive'' due to the fact \nthat a tuple needs to be found via interactive queries to the oracle.\n\nThe parameter \\Creg measures the regret.\nWhen $\\Creg = 0$, the \\irm problem requires to find the top tuple $\\vecx^*$ with no regret.\nWe refer to this special case as \\emph{interactive top tuple} (\\itt) problem.\nFor example, when tuples are in 1-dimension, \n\\itt reduces to finding the maximum (or minimum) among a list of distinct~numbers.\n\n\\iffalse\nWe illustrate the \\irm and \\itt problems with a concrete example in \\cref{fig:irm}.\nWe elaborate on the example below.\n\\begin{example}\nThe tuples of a given database \\D are represented as points within a unit circle in $\\reals^2$.\nThe unknown utility vector \\vecu, which is drawn in orange, satisfies $\\norm{\\vecu}_2 = 1$.\nThus, as the unique top tuple in \\D, the red point has utility value 1.\nThe green points are feasible \\Creg-regret tuples for the \\irm problem, \nas they are within a distance \\Creg from the red point, \nalong the direction of the utility vector \\vecu.\n\\end{example}\n\\fi\n\nClearly, the definition for the \\irm problem is meaningful only when $\\util(\\vecx^*) \\ge 0$,\nwhich is an assumption made in this paper.\nAnother important aspect of the \\irm problem is whether or not the oracle \nwill return a \\emph{tie} in any pairwise comparison. \nIn this paper, we study both scenarios.\nIn the first scenario, we assume that the oracle never returns a tie,\nwhich implies that no two tuples in \\D have the same utility.\nWe state our assumptions for the first (and, in this paper, default) scenario below.\nWe discuss how to relax this assumption for the second scenario in \\cref{se:robust}.\n\\begin{assumption}\n\\label{asm:irm}\nNo two tuples in \\D have the same utility.\nMoreover, the best tuple $\\vecx^*$ has non-negative utility, i.e., $\\util(\\vecx^*) \\ge 0$.\n\\end{assumption}\n\nWithout loss of generality, we further assume that $\\norm{\\vecu}_2 = 1$ and \n$\\norm{\\vecx}_2 \\le 1$, for all $\\vecx \\in \\D$, which can be easily achieved by scaling.\nAs a consequence of our assumptions, we have $c = \\util(\\vecx^*) \\le 1$.\nThe proposed method in this paper essentially finds an $\\Creg/c$-regret tuple,\nwhich is feasible for the \\irm problem when $c = 1$.\nOur solution still makes sense, i.e., a relatively small regret $\\Creg/c$, if $c$ is not too small or a non-trivial lower bound of $c$ can be estimated in advance.\nOn the other hand, if $c$ is very small, \nthere exists no tuple in \\D that can deliver satisfactory utility in the first place,\nwhich means that searching for the top tuple itself is also less rewarding.\nFor simplicity of discussion, we assume that $c = 1$ throughout the paper.\n\nFor all problems we study in this paper, \nwe focus on efficient algorithms under the following computational model.\n\\begin{definition}[One-pass data stream model]\n\\label{def:streaming}\nAn algorithm is a one-pass streaming algorithm if \nits input is presented as a (random-order) sequence of tuples and is examined by the algorithm in a single pass.\nMoreover, the algorithm has access to limited memory, generally logarithmic in the input size. \n\\end{definition}\nThis model is particularly useful in the face of large datasets.\nIt is strictly more challenging than the traditional offline model, \nwhere one is allowed to store all tuples and examine them with random access\nor over multiple passes.\nA random-order data stream is a natural assumption in many applications, \nand it is required for our theoretical analysis. \nIn particular, this assumption will always be met in an offline model, where one can easily simulate a random stream of tuples.\nExtending our results to streams with an arbitrary order of tuples\nis a major open problem.\n\nOne last remark about the \\irm problem is the \\emph{intrinsic dimension} of the database \\D.\nTuples in \\D are explicitly represented by $d$ variables, one for each dimension, and \n$d$ is called the \\emph{ambient dimension}.\nThe intrinsic dimension of \\D is the number of variables that are needed in a minimal representation of \\D.\nMore formally,\nwe say that \\D has an intrinsic dimension of $d'$ if \nthere exist $d'$ orthonormal vectors $\\myvec{b}_1, \\ldots, \\myvec{b}_{d'} \\in \\reals^d$ such that\n$d'$ is minimal and\nevery tuple $\\vecx \\in \\D$ can be written as a linear combination of them.\nIt is common that the intrinsic dimension of realistic data is much smaller than its ambient dimension.\nFor example, images with thousands of pixels can be compressed into a low-dimensional representation with little loss.\nThe proposed method in this paper is able to adapt to the intrinsic dimension of~\\D \nwithout constructing its minimal representation explicitly.\n\nIn the rest of this section,\nwe review existing hardness results for the \\itt and \\irm problems.\n\n\\smallskip\n\\para{Lower bounds.}\nBy an information-theoretical argument, one can show that $\\Omega(\\log \\nD)$ comparisons are necessary for the \\itt problem~\\citep{kulkarni1993active}.\n\\iffalse\nConsider an instance in $\\reals^2$, where tuples are points on the unit circle, and each tuple has the possibility to be the top tuple.\nEvery pairwise comparison as a binary question can eliminate at most half of the remaining possibilities.\n\\note{\nThe argument is similar to thinking about a binary decision tree, where each leaf represents a possibility.\nIt is similar to how one lowers bound \\#comparisons for sorting a list of numbers.\\\\\nSuppose any algorithm asks a seq of $k = \\smallo(\\log \\nD)$ questions.\nThere always exists a seq of answers such that there remain more than one possibilities after $k$ questions.\nSay the first question has an answer $a \\in \\{0,1\\}$.\nLet\n$\\D_1 = \\{\\vecx \\in \\D: \\vecx \\text{ agrees with } a=1 \\}$ and $\\D_0$ analogously.\nRepeatedly choose an answer so that $\\min \\{ |D_0|,|D_1| \\} \\le |\\D|/2$.\n}\n\\fi\nBy letting $d=\\nD$ and  \n$\\D = \\{\\vece_i\\}$ for $i \\in [d]$, where $\\vece_i$ is a vector in the standard basis,\n$\\Omega(d)$ comparisons are necessary to solve the \\itt problem,\nas a comparison between any two dimensions reveals no information about the rest dimensions.\n\nTherefore, one can expect a general lower bound for the \\irm problem to somewhat depend on both $d$ and $\\log \\nD$.\nThanks to the tolerance of \\Creg regret in utility, \n\\iftrue\na refined lower bound $\\Omega(d \\log (1/\\Creg))$ for the \\irm problem is given by \\citet[Theorem 9]{nanongkai2012interactive}.\n\\else\nin theory, one could consider an \\Creg-net of the database \\D, instead of the entire \\D,\nwhose size is only $\\Omega((1/\\Creg)^d)$ and does not depend on \\nD.\nAny tuple in \\D is within a distance of \\Creg from some tuple in the \\Creg-net.\nIndeed, a refined lower bound \n$\\Omega(\\log(1/\\Creg)^d) = \\Omega(d \\log (1/\\Creg))$ for the \\irm problem is given by \\citet[Theorem 9]{nanongkai2012interactive},\nby lower bounding the cardinality of the \\Creg-net of a $d$-sphere.\n\\fi\n"
            },
            "section 3": {
                "name": "Related work",
                "content": "\n\\label{se:related}\n\n\\para{Interactive regret minimization.}\n% The \\irm problem originates from the database domain.\nA database system provides various operators that return a representative subset of tuples (i.e., points in $\\reals^d$) to a user.\nTraditional top-$k$ operators \\citep{carey1997saying} return the top-$k$ tuples \naccording to an explicitly specified scoring function.\nIn the absence of a user utility vector \\vecu for a linear scoring function,\nthe skyline operators \\citep{borzsony2001skyline} return a tuple if it has the potential to be the top tuple for at least one possible utility vector.\nIn the worst case, a skyline operator can return the entire dataset.\n\\citet{nanongkai2010regret} introduce a novel $k$-regret operator that achieves a balance between the previous two problem settings, \nby returning $k$ tuples such that the maximum regret over all possible utility vectors is~minimized.\n\n\\citet{nanongkai2012interactive} further minimize regret in an interactive fashion by making pairwise comparisons.\nThey prove an upper bound on the number of requested comparisons by using synthesized tuples for some comparisons.\nIn fact, their method learns approximately the underlying utility vector.\nHowever, synthesized tuples are often not suitable for practical use.\n\n\\citet{jamieson2011active} deal with a more general task of finding a full ranking of \\nD tuples.\nBy assuming that every possible ranking is equally probable, \nthey show that $\\bigO(d \\log \\nD)$ comparisons suffice to identify the full ranking in expectation.\nNevertheless, in the worst case, one cannot make such an assumption, and\ntheir algorithm may require $\\Omega(\\nD^2)$ comparisons for identifying a full ranking or $\\Omega(\\nD)$ comparisons for identifying the top tuple.\nAnother similar problem assumes a distribution over the utility vector \\vecu without access to the embedding of the underlying metric space~\\citep{karbasi2012comparison}.\nThe problem of combinatorial nearest neighbor search is also related,\nwhere one is to find the top tuple as the nearest neighbor of a given tuple \\vecu without access to the embedding~\\citep{haghiri2017comparison}.\n\n\\citet{xie2019strongly} observe that the \\itt problem is equivalent to a special linear program, \nwhose pivot step for the simplex method can be simulated by making a number of comparisons.\nThus, an immediate guarantee can be obtained by leveraging the fact that \n$\\bigO(\\nD^{1/d})$ pivot steps are needed in expectation for the simplex method \\citep{borgwardt1982average}.\nHere the expectation is taken over some distribution over \\D.\nAlso in the special case when $d=2$, they develop an optimal binary search algorithm~\\citep{xie2019strongly}. \n\\citet{zheng2020sorting} suggest letting a user sort a set of displayed tuples in each round of interaction, \nbut their approaches are similar to \\citet{xie2019strongly}, and do not use a sorted list the way we do.\n\nThere are other attempts to the \\itt problem that adaptively select a greedy pair of tuples with respect to some measure of interest.\n\\citet{qian2015learning} iteratively select a hyperplane (i.e., pair) whose normal vector is the most orthogonal to the current estimate of \\vecu.\n\\citet{wang2021interactive} maintain disjoint regions of \\vecu over $\\reals^d$, one for each tuple, where a tuple is the best if \\vecu is located within its region.\nThen, they iteratively select a hyperplane that separates the remaining regions as evenly as possible.\nHowever, these greedy strategies are highly computationally expensive, and do not have any theoretical guarantee.\n\nCompared to aforementioned existing work,\nour proposed algorithm makes minimal assumptions, is scalable, \nand enjoys the strongest worst-case guarantee.\nIt is worth mentioning that existing research often assumes that increasing any tuple attribute always improves utility,\nby requiring $\\D \\subseteq \\reals_+^d$ and $\\vecu \\in \\reals_+^d$~\\citep{nanongkai2012interactive,xie2019strongly,zheng2020sorting,wang2021interactive}.\nWe do not make such an assumption in this paper.\n\n\n\\smallskip\n\\para{Active learning.}\nThe \\irm problem can be viewed as a special highly-imbalanced linear classification problem.\nConsider a binary classification instance, where the top tuple is the only one with a positive label and the rest are all negative.\nSuch labeling is always realizable by a (non-homogeneous) linear hyperplane, e.g.,\n$\\h = \\{ \\vecx \\in \\reals^d: \\vecu^T\\vecx = \\vecu^T\\vecx^* - \\Coff \\}$ for any sufficiently small $\\Coff \\ge 0$.\nNote that non-homogeneous \\h can be replaced by a homogeneous one (i.e., without the offset term \\Coff) by lifting the tuples into $\\reals^{d+1}$.\n\nActive learning aims to improve sample complexity that is required for learning a classifier by adaptive labeling.\nActive learning with a traditional labeling oracle has been extensively studied.\nThe above imbalanced problem instance happens to be a difficult case for active learning with a labeling oracle \\cite{dasgupta2005coarse}.\nWe refer the reader to \\citet{hanneke2014theory} for a detailed treatment.\n\nActive learning with additional access to pairwise comparisons has been studied by \\citet{kane2017active,kane2018generalized}.\nThat is, one can use both labeling and comparison oracles.\nImportantly, \\citet{kane2017active} introduce a notion of ``inference dimension,'' with which they design an algorithm to effectively infer unknown labels.\nHowever, due to technical conditions, the inference technique is only useful for classification in low dimension ($d \\le 2$) or special instances.\nAs one of our main contributions, \nwe are the first to show that the inference technique can be adapted for the \\irm problem.\n\n\\iffalse\n\\citet{xu2017noise} also utilize pairwise comparisons in active learning,\nbut their main goal is to reduce labeling queries and labeling noise\nby first sorting all data points into a list and then locating the decision boundary via binary search.\n\\fi\n\n\\smallskip\n\\para{Ranking with existing pairwise comparisons.}\nA different problem setting, \nis to rank collection of tuples by aggregating a set of \n(possibly incomplete and conflicting) pairwise comparisons, \ninstead of adaptively selecting which pair of tuples to compare. \nThis problem has been extensively studied in the literature within different abstractions.\nFrom a combinatorial perspective, it is known as the \\emph{feedback arc-set} problem on tournaments, where the objective is to find a ranking by removing a minimum number of inconsistent comparisons~\\citep{ailon2008aggregating}.\nThere also exist statistical approaches to find a consistent ranking, or the top-$k$ tuples, by estimating underlying preference scores~\\citep{minka2018trueskill,negahban2012iterative,chen2015spectral}.\nIn machine learning, the problem is known as ``learning to rank'' with pairwise preferences~\\citep{liu2009learning},\nwhere the aim is to find practical ways to fit and evaluate a ranking.\n\n\n\\note[$X+Y$ sorting\\footnote{\\href{https://en.wikipedia.org/wiki/X\\_\\%2B\\_Y\\_sorting}{https://en.wikipedia.org/wiki/X\\_\\%2B\\_Y\\_sorting}}]{\nThe sorting approach we propose in this paper is also relevant to the $X+Y$ sorting problem~\\cite{bremner2006necklaces}.\nGiven a sample set $S \\subseteq \\D$,\nlet $X = \\{ \\vecu^T \\vecx : \\vecx \\in X\\}$, $Y = -X$, and \n\\[\n\tX+Y = \\{ x+y : x \\in X, y \\in Y \\}.\n\\]\nSorting $X+Y$ reveals all pairwise comparisons in $S$.\nIt remains open if one can do this faster than $\\bigO(s^2 \\log(s))$ where $s = |S|$.\n}\n\n\n"
            },
            "section 4": {
                "name": "Finding a tuple: oracle with no ties",
                "content": "\n\\label{se:single}\n\n\n\nIn this section, we present our single-pass streaming algorithm for the \\irm problem.\nOur approach, presented in \\cref{alg:framework,alg:single}, \nuses the concept of \\emph{filters} to prune sub-optimal tuples without the need of further comparisons.\n\\cref{alg:framework} is a general framework for managing filters, \nwhile \\cref{alg:single} specifies a specific filter we propose.\nAs we will see in \\cref{se:experiment} the framework can also be used for other filters.\n\nThe filter we propose relies on a remarkable inference technique introduced by \\citet{kane2017active,kane2018generalized}.\nNote that the technique was originally developed for active learning in a classification task, and\nits usage is restricted to low dimension ($d \\le 2$) or special instances under technical conditions.\nWe adapt this technique to devise a provably effective filter for the \\irm problem.\nIn addition, we strengthen their technique with a high-probability guarantee and a generalized symmetrization argument.\n\nThe core idea %, shown in \\cref{alg:single}, \nis to construct a filter from a small random sample of tuples.\nIt can be shown that the filter is able to identify \na large fraction of sub-optimal tuples in \\D without further comparisons.\nFixing a specific type of filter with the above property,\n\\cref{alg:framework} iteratively constructs a new filter in a boosting fashion \nto handle the remaining tuples.\nFinally, one can show that, with high probability, \nat most $\\bigO(\\log \\nD)$ such filters will be needed.\n\n\\begin{algorithm}[t]\n\\DontPrintSemicolon\n%\\KwIn{items \\D and parameters $\\Creg, \\nR$}\n%\\KwOut{Ouput}\n\\SetKwFunction{FNewFilter}{NewFilter}\n\\KwIn{tuples \\D and parameters \\nPl, \\CPl}\n$F \\gets \\FNewFilter, \\, \\seq \\gets (), \\, \\Pl \\gets \\emptyset$ \\;\n\\For{tuple $\\vecx \\in \\D$ over a random-order stream}{\n\t\\lIf{$F'.prune(\\vecx)$ for any $F' \\in \\seq$}{\n\t\t\\Continue \n\t}\n\t\\lIf{$|\\Pl| < \\nPl$}{\n\t\t$\\Pl \\gets \\Pl \\cup \\{\\vecx\\} $;\n\t\t\\Continue\n\t}\n\t$F.add(\\vecx)$ \\;\n\t$\\Pl' \\gets \\{ \\vecy \\in \\Pl: F.prune(\\vecy) \\text{ is true} \\}$ \\;\n\t\\If{$|\\Pl'| \\ge \\CPl |\\Pl|$}{ \\label{step:pool}\n\t\tAppend $F$ to sequence \\seq \\;\n\t\t$F \\gets \\FNewFilter, \\, \\Pl \\gets \\Pl \\setminus \\Pl'$ \\;\n\t}\n}\nAppend $F$ to sequence \\seq and let $\\X = \\{ F.best() : F \\in \\seq \\}$ \\;\nLet \\bestsofar be the best tuple in $\\X \\cup \\Pl$ by pairwise comparisons \\;\n\\Return \\bestsofar \\;\n\\caption{A general framework}\n\\label{alg:framework}\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\\DontPrintSemicolon\n\\KwIn{parameter \\Creg}\n%\\KwOut{Ouput}\n\\lClass{\\FNewFilter}{\n\t$S \\gets \\emptyset$\n}\n\n\\Function{prune$(\\vecx)$}{\n\t\\Return true, if $S \\infer \\vecx$ (see \\cref{eq:implies}), otherwise false \\label{step:filter}\\;\n}\n\n\\Function{add$(\\vecx)$}{\n\t$S \\gets S \\cup \\{ \\vecx \\}$ and sort $S$ by pairwise comparisons \\;\n}\n\n\\lFunction{best$()$}{\n\t\\Return the best tuple $\\vecx_1$ in $S$ \n}\n\n\\caption{Functions that define a filter for the \\irm problem with no ties}\n\\label{alg:single}\n\\end{algorithm}\n\nWe proceed to elaborate on the mechanism of a filter.\nThe idea is to maintain a random sample $S$ of $\\sz$ tuples, and\nsort them in order of their utility. \nThe total order of the tuples in $S$ can be constructed by pairwise comparisons, \ne.g., by insertion sort combined with binary search.\nSuppose that $S = \\{\\vecx_1,\\ldots,\\vecx_\\sz\\}$, where $\\vecx_1$ has the best utility.\nNotice that $\\vecu^T(\\vecx_{j+1} - \\vecx_{j}) \\le 0$ for any $j$.\nThus, a sufficient condition for an arbitrary tuple \\vecx to be worse than $\\vecx_1$ is\n\\begin{align}\n\\label{eq:filter:lp}\n\t\\vecx = \\vecx_1 + \\sum_{j=1}^{\\sz-1} \\alpha_j (\\vecx_{j+1} - \\vecx_j)\n\t\\quad\n\t\\text{such that} \\; \\alpha_j \\ge 0 \\quad \\text{for all } j.\n\\end{align}\n\nThis condition asks to verify whether \\vecx lies within a cone with apex $\\vecx_1$, \nalong direction \\vecu.\nThe parameters $\\alpha_j$ can be efficiently computed by a standard Linear Program (LP) solver.\nIf Condition~(\\ref{eq:filter:lp}) can be satisfied for \\vecx,\nthen \\vecx can be pruned for further consideration.\n\nActually, it is possible to act more aggressively and prune tuples slightly better than $\\vecx_1$, \nas long as it is assured that not all feasible tuples will be pruned.\nSpecifically, we can remove any  \\vecx that deviates from the aforementioned cone within a distance of $\\Creg$, that~is,\n\\begin{align}\n\t\\label{eq:filter}\n\t&\\min_\\alpha \\; \\bigg\\| \\vecx - \\vecx_1 - \\sum_{j=1}^{\\sz-1} \\alpha_j (\\vecx_{j+1} - \\vecx_j) \\bigg\\|_2 \\le \\Creg \n\t\\ \\ \\  \\text{s.t.} \\ \\  \\alpha_j \\ge 0 \\  \\text{for all } j .\n\\end{align}\n\nTo test whether a given tuple \\vecx satisfies the above condition, \none needs to search for parameters $\\alpha_j$ over $[0,\\infty)$ for all $j$.\nThe search can be implemented as an instance of constrained least squares, \nwhich can be efficiently solved via a quadratic program~(QP).\n\nGiven a sorted sample $S$ where $\\vecx_1$ is the top tuple, we write \n\\begin{align}\n\t\\label{eq:implies}\n\tS \\infer \\vecx\n\\end{align}\nif a tuple \\vecx can be approximately represented by vectors in $S$ in a form of \\cref{eq:filter}.\n\n\n\nAn example that illustrates the mechanism of a filter is displayed in \\cref{fig:filter},\non which we elaborate below.\n\\begin{example}\nIn \\cref{fig:filter},\na random sample $S = \\{\\vecx_1,\\vecx_2,\\vecx_3\\}$ of three blue points is collected and sorted,\nwhere $\\vecx_1$ has the highest utility.\nThis means that \n$\\util(\\vecx_{j+1}) - \\util(\\vecx_{j}) = \\vecu^T (\\vecx_{j+1} - \\vecx_j) < 0$, for any $j \\in \\{1,2\\}$.\nCompared to the point $\\vecx_1$, \na new point \\vecx in the form of \n$\\vecx = \\vecx_1 + \\sum_{j \\in \\{1,2\\}} \\alpha_j (\\vecx_{j+1} - \\vecx_j)$ with $\\alpha_j \\ge 0$ \ncan only have a lower utility than $\\util(\\vecx_1)$,\nsince\n\\[\n\t\\util(\\vecx) = \\vecu^T \\bigg[ \\vecx_1 + \\sum_{j \\in \\{1,2\\}} \\alpha_j (\\vecx_{j+1} - \\vecx_j) \\bigg] \\le \\util(\\vecx_1).\n\\]\nThus, such a point \\vecx can be safely pruned.\nGeometrically, all such prunable points \\vecx form a cone with apex $\\vecx_1$,\nas highlighted in the blue region in \\cref{fig:filter}.\nAccording to $\\cref{eq:filter}$, \nany point that is sufficiently close to (within a distance of \\Creg) the blue cone can also be pruned.\n\\end{example}\n\nUpon a random-order stream of tuples, \\cref{alg:framework,alg:single} collect a pool \\Pl of \\nPl initial tuples as a testbed for filter performance.\nThen, subsequent tuples are gradually added into the first sample set $S_1$,\nuntil a filter based on $S_1$ can prune at least a $\\CPl = 5/8$ fraction of \\Pl.\nThen, $S_1$ is ready, and is used to prune tuples in the pool \\Pl and future tuples over the stream.\nFuture tuples that survive the filter formed by $S_1$ will be gradually added into the pool \\Pl and a second sample set $S_2$, and the process is repeated iteratively.\nFinally, the algorithm returns the best tuple among all samples.\nThe following theorem states our main result about \\cref{alg:framework,alg:single}.\n\n\\begin{theorem}\n\\label{thm:num_of_S}\nAssume $\\epsilon > 0$ and let $\\nD = |\\D|$ be the size of data.\nLet $c = \\util(\\vecx^*) \\in [0,1]$ be the utility of the best tuple~$\\vecx^*$.\nUnder \\cref{asm:irm},\nwith a pool size $\\nPl = \\ceil{64 \\ln 2\\nD}$ and $\\CPl = 5/8$,\n\\cref{alg:framework,alg:single} return an $\\Creg/c$-regret tuple for the \\irm problem.\n\nLet $\\infd = \\Cinfd$, where\n$d$ is the intrinsic dimension of~\\D.\nThen, with probability at least $1-1/\\nD$, at most\n\\[\n\t% \\ceil{\\log(n)} (4t \\ceil{\\log (4t)} + 1) + \\nPl\n\t\\bigO(\\log(\\nD) \\, 4t \\log (4t)) + \\nPl\n\\]\ncomparisons\nare made.\n\\iffalse\n\nMoreover, \nwith probability at least $1-1/\\nD$,\nat most $\\log(n)$ sample sets will be needed,\neach of a size of $4\\infd$,\nwhere $\\infd = \\Cinfd$, \n$d$ is the intrinsic dimension of~\\D.\n\nThus, \nwith probability at least $1-1/\\nD$,\nat most $\\bigO(\\infd\\log(n))$ tuples will be kept during the execution, and\nat most $2\\log(n) g(4\\infd)$ comparisons will be made,\nwhere $g(a)=a\\log(a)$.\n\\fi\n\\end{theorem}\nThe memory size, i.e., the number of tuples that will be kept by the algorithm during the execution, \nis $\\bigO(\\log(\\nD) \\, 4t)$, \nwhich is also logarithmic in~\\nD.\n\nIn fact, \\cref{alg:framework,alg:single} are an \\emph{anytime} algorithm,\nin the sense that the data stream can be stopped anytime,\nwhile the algorithm is still able to return a feasible solution among all tuples that have arrived so far.\n\n\\begin{theorem}\n\\label{thm:anytime}\nUnder \\cref{asm:irm},\nthe data stream may terminate at any moment during the execution of \\cref{alg:framework,alg:single}, and\nan $\\Creg/c$-regret tuple will be returned for the \\irm problem among all tuples that have arrived so far.\n\\end{theorem}\n\nProofs of \\cref{thm:num_of_S,thm:anytime} are deferred to \\cref{se:single:proofs}.\n\\iffalse\nIn the rest of this section we prove \\cref{thm:num_of_S,thm:anytime}.\nFor simplicity of discussion, we assume that $c = 1$.\n\n",
                "subsection 4.1": {
                    "name": "Proofs",
                    "content": "\n\n\\citet{kane2018generalized} proved a powerful local lemma, which states that\namong a sufficiently large set of vectors from the unit $d$-ball $\\ball^d$, \nthere must exist some vector that can be approximately represented as a special non-negative linear combination of others.\n\\begin{lemma}[\\cite{kane2018generalized}, Claim 15]\n\\label{lemma:comb}\nGiven $\\vecx_1,\\ldots,\\vecx_\\infd \\in \\ball^d$, for any $\\Creg > 0$, if $\\infd \\ge \\Cinfd$,\nthen there exists $a \\in [\\infd]$ such that\n\\begin{align}\n\t\\vecx_a = \\vecx_1 + \\sum_{j = 1}^{a-2} \\alpha_j (\\vecx_{j+1} - \\vecx_j) + \\vece,\n\t\\label{eq:comb}\n\\end{align}\nwhere $\\norm{\\vece}_2 \\le \\Creg$ and $\\alpha_j \\in \\{0,1,2\\}$.\n\\end{lemma}\n\nLet $S = \\{\\vecx_1, \\ldots, \\vecx_\\infd\\}$.\n\\cref{lemma:comb} can be easily extended to hold for the intrinsic dimension of $S$,\nby first applying \\cref{lemma:comb} to the minimal representation $\\vecy_1,\\ldots,\\vecy_\\infd \\in \\ball^{d'}$ of $S$.\n\\note{\nSuppose\n\\[\n\t\\vecy_a = \\vecy_1 + \\sum_{j = 1}^{a-2} \\alpha_j (\\vecy_{j+1} - \\vecy_j) + \\vece',\n\\]\nwhere $\\norm{\\vece'}_2 \\le \\Creg$.\nBy definition, we know that \n$\\vecx_i = M \\vecy_i$ where $M \\in \\reals^{d,d'}$ and columns of $M$ are orthonomal.\nThen, we have $\\norm{\\vece}_2 \\le \\Creg$,\nwhere $\\vece = M\\vece'$.\n}\n\nIn \\cref{lemma:comb}, we have $S-\\vecx_a \\infer \\vecx_a$,\nwhere $S-\\vecx$ is a shorthand for $S \\setminus \\{\\vecx\\}$.\nNote that this is exactly the condition we use in Step~\\ref{step:filter} in \\cref{alg:single} for pruning.\nDenote by $\\filter(S)$ the set of all such pruned tuples, i.e.,\n\\begin{align}\n\t\\filter(S) = \\{ \\vecx \\in \\reals^d: S \\infer \\vecx \\}.\n\\end{align}\n\nGiven any set $S$ of size $4\\infd$,\nat least 3/4 fraction of $S$ can be pruned by other tuples in $S$, by repeatedly applying \\cref{lemma:comb}.\n\\begin{lemma}\n\\label{lemma:4t}\nGiven a sorted set $S$ of size at least $4\\infd$, where $\\infd = \\Cinfd$, we have\n\\[\n\t|\\{ \\vecx \\in S: S-\\vecx \\infer \\vecx \\}| \\ge \\frac34 |S|.\n\\]\n\\end{lemma}\n\\begin{proof}\nsince $|S| \\ge 4\\infd$,\nwe can apply \\cref{lemma:comb} repeatedly to $S$ until only \\infd entries remain.\n\\end{proof}\n\\note{Importantly, $\\{ \\vecx \\in S: S-\\vecx \\infer \\vecx \\}$ do not depend on the arrival order of tuples in $S$.}\n\nAs a consequence of \\cref{lemma:4t},\nthe same fraction of current tuples \\X can be pruned by a random sample set $S$ of a sufficient size in expectation.\n\\begin{lemma}\n\\label{lemma:filter}\nGiven a set of tuples \\X, and\na random sample set $S \\subseteq \\X$ of size $4\\infd$ where $\\infd = \\ceil{\\Cinfd}$, \nwe have\n\\[\n\t\\expect \\left[ |\\filter(S) \\cap \\X| \\right] \\ge \\frac34 |\\X|,\n\\]\nwhere the expectation is taken over $S$.\n\\end{lemma}\n\\ifapx\n\\begin{proof}\\deferredproof\\end{proof}\n\\begin{appendixproof}[Proof of \\cref{lemma:filter}]\n\\else\n\\begin{appendixproof}\n\\fi\nThe proof is by a symmetrization argument introduced by \\citet{kane2017active}.\nLet $\\vecy$ be the last tuple added into $S$. Write $T = S - \\vecy$.\nGiven $T$, the distribution of $\\vecy$ is a uniform distribution from $\\X \\setminus T$.\n%Let $T$ be a sample set of size $4\\infd - 1$ and let $\\vecy$ be a sample from $\\X \\setminus T$.\nLet $\\vecx$ be a random sample from $X$. \nSince $T \\subseteq \\filter(T)$, we have\n\\[\n\t\\Pr \\left[  T \\infer \\vecx \\mid T \\right] \\geq\n\t\\Pr \\left[  T \\infer \\vecy \\mid T \\right].\n\\]\n%Let $\\vecz$ be a random sample from $S$.\n%Note that the distribution of $T + \\vecy$ matches to the distribution of $S$,\n%and the distribution of $S - \\vecz$ matches to the distribution of $T$.\nThen,\n\\begin{align*}\n    \\expect_S \\left[ \\frac{|\\filter(S) \\cap \\X|}{|\\X|} \\right] \n    &= \\expect_{S} \\left[ \\Pr \\left[ S \\infer \\vecx \\mid S \\right] \\right] \\\\\n    &= \\expect_{S} \\left[ \\Pr \\left[ T + \\vecy \\infer \\vecx \\mid S \\right] \\right] \\\\\n    &\\ge \\expect_{S} \\left[ \\Pr \\left[ T \\infer \\vecx \\mid S \\right] \\right] \\\\\n    &= \\expect_{T} \\left[ \\Pr \\left[ T \\infer \\vecx \\mid T \\right] \\right] \\\\\n    &\\ge \\expect_{T} \\left[ \\Pr \\left[ T \\infer \\vecy \\mid T \\right] \\right] \\\\\n    &= \\expect_{S} \\left[ \\indicator \\left[ S - \\vecy \\infer \\vecy \\mid S \\right] \\right].\n\\end{align*}\n\nIn order to bound the right-hand side, notice that \nwhen conditioned on $S$, every permutation of $S$ is equally probable over a random-order stream, \nwhich implies that every tuple in $S$ is equally probable to be the last tuple \\vecy.\nHence, we have $\\Pr \\left[ S - \\vecy \\infer \\vecy \\mid S \\right] \\geq 3/4$ by \\cref{lemma:4t},\nproving immediately the claim.\n\\end{appendixproof}\n\\note{It is important to note that, the above proof doesn't imply that\nfor \\emph{every} $S$, when conditioning on it, one can achieve \n\\[\n\t\\Pr \\left[ S \\infer \\vecx \\mid S \\right] \\ge 3/4,\n\\]\nthat is, every $S$ can prune at least 3/4 of \\X, which is too strong to be true.\nAfter we replace \\vecx by \\vecy, the 3/4 is obtained by taking average over \\vecy, which is part of outer expectation over $S$.\n}\n\n\nAnother important issue to handle is to ensure that our pruning strategy will not discard all feasible tuples.\nThis is prevented by keeping track of the best tuple in any sample set so far, and guaranteed by \\cref{thm:anytime}.\n\\begin{proof}[Proof of \\cref{thm:anytime}]\nDenote by \\D all tuples that have arrived so far.\nSuppose $\\vecx^*$ is the best tuple among \\D.\nTuple $\\vecx^*$ is either collected into our sample sets, \nor pruned by some sample set $S$.\nIn the former case, our statement is trivially true.\nIn the latter case, suppose $S = \\{\\vecx_1,\\ldots\\}$, where $\\vecx_1$ is the best tuple in $S$.\nIf $\\vecx_1$ is feasible, then $\\bestsofar$ is feasible as well, as it is at least as good as $\\vecx_1$.\nIf $\\vecx_1$ is infeasible, i.e., $\\util(\\vecx^*) - \\util(\\vecx_1) > \\Creg$, then $\\vecx^*$ cannot be pruned by $S$ by design, a contradiction.\nThis completes the proof.\n\\end{proof}\n\nBefore proving \\cref{thm:num_of_S},\nwe briefly summarize the hypergeometric tail inequality below~\\citep{skala2013hypergeometric}.\n\\begin{lemma}[Hypergeometric tail inequality~\\citep{skala2013hypergeometric}]\n\\label{lemma:tail}\nDraw $n$ random balls without replacement from a universe of $N$ red and blue balls, and\nlet $i$ be a random variable of the number of red balls that are drawn.\nThen, for any $t > 0$, we have \n\\[\n\t\\Pr[i \\ge \\expect[i] + t n] \\le e^{-2t^2n},\n\\]\nand\n\\[\n\t\\Pr[i \\le \\expect[i] - t n] \\le e^{-2t^2n}.\n\\]\n\\end{lemma}\n\n\\begin{proof}[Proof of \\cref{thm:num_of_S}]\nThe feasibility of the returned tuple \\bestsofar is due to \\cref{thm:anytime}.\nIn the rest of the proof, we upper bound the size of every sample and the number of samples we keep in the sequence~\\seq.\n\nFor any sample $S$ with at least $4\\infd$ samples and any subset $\\X \\subseteq \\D$, \nlet $\\X' = \\filter(S) \\cap \\X$ and by \\cref{lemma:filter} we have\n$\\expect [ |\\X'| ] \\ge \\frac34 |\\X|$.\nIn particular, let $\\X = \\Pl$ and we have \n$\\expect[|\\Pl'|] \\ge \\frac34 |\\Pl|$ and $|\\Pl| = \\nPl$.\nThen, \n\\begin{align*}\n\t\\Pr\\left[ |\\Pl'| < \\frac58 |\\Pl| \\right] \n\t&= \\Pr\\left[ |\\Pl'| < \\frac34 |\\Pl| - \\frac18 \\nPl\\right]  \\\\\n\t&\\le \\Pr\\left[ |\\Pl'| < \\expect[|\\Pl'| - \\frac18 \\nPl] \\right] \n\t\\le e^{-2 \\nPl /8^2},\n\\end{align*}\nwhere the last step invokes \\cref{lemma:tail}.\nSince there can be at most \\nD samples,\nthe probability that any sample fails to pass the pool test is upper bounded by $\\nD e^{-2 \\nPl /8^2}$.\n\nWe continue to upper bound the number of sample sets.\nAt most $\\ceil{\\log(\\nD)}$ sample sets suffice if every sample can prune at least half of the remaining tuples.\nFix an arbitrary sample $S$, and let \\X to be the set of remaining tuples.\nThe pool \\Pl is a random sample from \\X of size \\nPl. Thus,\n$\\expect[|\\Pl'|] / \\nPl = |\\X'| / |\\X|$. Consequently, if  $|\\X'| < |\\X|/2$, then $\\expect[|\\Pl'|] < \\nPl/2$\nand\n\\begin{align*}\n\t\\Pr\\left[ |\\Pl'| \\ge \\frac58 |\\Pl| \\right] \n\t\\le \\Pr\\left[ |\\Pl'| \\ge \\expect[|\\Pl'|] + \\frac18 \\nPl \\right] \n\t\\le e^{-2 \\nPl /8^2}.\n\\end{align*}\nSimilar to the above, \nthe probability that any bad sample passes the test is upper bounded by $\\nD e^{-2 \\nPl /8^2}$.\n\nCombining the two cases above,\nthe total failure probability is $2\\nD e^{-2 \\nPl /8^2} \\leq 1/\\nD$\nHence, with probability at least $1-1/\\nD$,\nit is sufficient to use $\\ceil{\\log(\\nD)}$ sample sets, each with a size $4\\infd$.\nKeeping one sample set requires $4\\infd \\ceil{\\log(4\\infd)}$ comparisons.\n%Finally, the last set requires additional $p \\ceil{\\log(4\\infd + p)}$ comparisons.\nFinally, finding the best tuple among all filters and the pool requires additional $\\nPl + \\ceil{\\log(\\nD)}$ comparisons.\n\\end{proof}\n\\fi\n\n\\section{Finding a tuple: oracle with ties}\n\\label{se:robust}\n\nIn this section, \nwe first introduce a natural notion of uncomparable pairs to avoid error-prone comparisons, and then\nwe show how this new setting affects our algorithms.\n%A simple adaptation of the techniques in the previous section leads to a filter with a worse guarantee on the query complexity;\n%we defer a description of the simple adaptation to \\cref{se:backup}.\n%Instead, in this section, we introduce a novel filter with a better guarantee.\n\nIt is clearly more difficult for a user to distinguish a pair of tuples with nearly equal utility.\nThus, it is reasonable to not force the user to make a choice in the face of a close pair, and \nallow the user to simply declare the comparison a tie instead.\nWe make this intuition formal below.\n\n\\begin{definition}[\\vecu-similar pairs]\n\\label{def:cmp}\nTwo tuples $\\vecx,\\vecy \\in D$ are \\vecu-similar if \n\\[\n\t|\\util(\\vecx) - \\util(\\vecy)| \\le \\Ccmp,\n\\]\nfor some fixed value \\Ccmp.\nWe write $\\vecx \\cmp \\vecy$ if they are uncomparable.\n\\end{definition}\n\n\\begin{assumption}\n\\label{asm:dirm}\nA query about a \\vecu-similar pair to the oracle will be answered with a tie.\nBesides, as before, we assume that the best tuple~$\\vecx^*$ has non-negative utility, \n$\\util(\\vecx^*) \\ge 0$.\n\\end{assumption}\n\nTypically, the value \\Ccmp is fixed by nature, unknown to us, and cannot be controlled.\nNote that when \\Ccmp is sufficiently small, we recover the previous case in \\cref{se:single} where every pair is comparable under \\cref{asm:irm}.\nBy allowing the user to not make a clear-cut comparison for a \\vecu-similar pair, one can no longer be guaranteed total sorting.\nIndeed, it could be that every pair in \\D is \\vecu-similar.\n\nIn \\cref{alg:robust}, we provide a filter to handle ties under \\cref{asm:dirm}.\nWe maintain a totally sorted subset $R$ of \\emph{representative} tuples in a sample set $S$.\nFor each representative $\\vecy \\in R$, we create a group~$G_\\vecy$.\nUpon the arrival of a new tuple \\vecx, we sort \\vecx into $R$ if no tie is encountered.\nOtherwise, we encounter a tie with a tuple $\\vecy \\in R$ such that $\\vecx \\cmp \\vecy$, and\nwe add \\vecx into a group $G_\\vecy$.\nIn the end, the best tuple in $R$ will be returned.\n\n\\begin{algorithm}[t]\n\\DontPrintSemicolon\n\\KwIn{parameter \\Creg}\n%\\KwOut{Ouput}\n\\lClass{\\FNewFilter}{\n\t$R \\gets \\emptyset, \\, S \\gets \\emptyset$;\n}\n\n\\Function{prune$(\\vecx)$}{\n\t\\Return true, if $S \\infersim \\vecx$ (see \\cref{eq:implies-robust}), otherwise false\\;\n}\n\n\\Function{add$(\\vecx)$}{\n\t$S \\gets S \\cup \\{\\vecx\\}$, and sort \\vecx within $R$ \\; \n\t\\eIf{no tie happens}{\n\t\t$R \\gets R \\cup \\{\\vecx\\}$ and create $G_\\vecx \\gets \\{\\vecx\\}$ \\;\n\t}{\n\t\tEncounter a tie with $\\vecy \\in R$ such that $\\vecx \\cmp \\vecy$ \\;\n\t\t$G_\\vecy \\gets G_\\vecy \\cup \\{\\vecx\\}$\\;\n\t}\n}\n\n\\lFunction{best$()$}{\n\t\\Return the best tuple in $R$ \n}\n\n\\caption{Functions that define a filter for the \\irm problem with ties}\n\\label{alg:robust}\n\\end{algorithm}\n\nTo see whether a filter in \\cref{alg:robust} can prune a given tuple~\\vecx, \nwe test the following condition.\nLet $R = \\vecx_1, \\ldots$ be the sorted list of representive tuples, \nwhere $\\vecx_1$ is the top tuple.\nLet $\\Gs = G_1, \\ldots$ be the corresponding groups.\nA tuple \\vecx can be pruned if \nthere exists $\\vecx'$ such that $\\norm{\\vecx - \\vecx'}_2 \\le \\Creg$, where\n\\begin{align}\n\t\\vecx' =& \n\t\\sum_{\\vecy \\in G_{1} \\cup G_{2}} \\nu_\\vecy \\, \\vecy  \n\t+ \\sum_{j=1} \\sum_{ \\vecz \\in G_{j} } \\sum_{\\vecw \\in  G_{{j+2}}} \\alpha_{\\vecw,\\vecz} (\\vecw - \\vecz) \\label{eq:filter-robust}\\\\\n\t&\\text{such that} \n\t\\sum_{\\vecy \\in G_{1} \\cup G_{2}} \\nu_\\vecy = 1 \\text{ and all } \\nu, \\alpha \\ge 0 \\nonumber .\n\\end{align}\nThe idea is similar to \\cref{eq:filter}, except that \nthe top tuple $\\vecx_1$ in \\cref{eq:filter} is replaced by an aggregated tuple by convex combination, and\nevery pair difference $\\vecx_{j+1} - \\vecx_{j}$ is replaced by pair differences between two groups.\nWe avoid using pair differences between two consecutive groups, as tuples in group $G_{j}$ may not have higher utility than tuples in $G_{{j+1}}$.\nIf the above condition is met, then\nwe write \n\\begin{align}\n\t\\Gs \\infersim \\vecx\n\t\\quad\\text{and, if $\\Gs$ is constructed using $S$,}\\quad\n\tS \\infersim \\vecx \\label{eq:implies-robust}.\n\\end{align}\n\nThe number of comparisons that is needed by \\cref{alg:robust} depends on the actual input,\nspecifically, \\Ccmpmax, the largest size of any pairwise \\vecu-similar subset of~\\D.\nNote that the guarantee below recovers that of \\cref{thm:num_of_S} up to a constant factor, if assuming \\cref{asm:irm} where $\\Ccmpmax=1$.\nHowever, in the worst case, $\\Ccmpmax = \\bigO(\\nD)$ and the guarantee becomes vacuous.\n\\begin{theorem}\n\\label{thm:num_of_S-robust}\nAssume $\\epsilon > 0$ and let $\\nD = |\\D|$ be the size of data.\nLet $c = \\util(\\vecx^*) \\in [0,1]$ be the utility of the best tuple~$\\vecx^*$.\nUnder \\cref{asm:dirm},\nwith a pool size $\\nPl = \\ceil{256\\ln 2\\nD}$ and $\\CPl = 3/16$,\n\\cref{alg:framework,alg:robust} return an $(\\Creg/c + 2\\Ccmp)$-regret tuple for the \\irm problem.\n\nLet $\\infd = \\Cinfd$, where\n$d$ is the intrinsic dimension of~\\D, and\n\\Ccmpmax be the largest size of a pairwise \\vecu-similar subset of \\D.\nThen, with probability at least $1-1/\\nD$, at most\n\\[\n\t\\bigO(\\log(\\nD) \\, 16 \\infd \\Ccmpmax \\log(16 \\infd \\Ccmpmax)) + \\nPl\n\\]\ncomparisons\nare made.\n\\end{theorem}\n\nProofs of \\cref{thm:num_of_S-robust} are deferred to \\cref{se:robust:proofs}.\n\\iffalse\nIn the rest of this section, we prove \\cref{thm:num_of_S-robust}.\n\n\\subsection{Proofs}\n\nThe proof is similar to that of \\cref{thm:num_of_S},\nexcept that we need a new proof for the key \\cref{lemma:filter},\nsince in the presence of ties, we may not be able to totally sort a sample $S$.\nInstead, we show that a partially sorted set $S$ of a sufficient size can also be effective in pruning.\n\nFrom now on, we treat the sample $S$ as a \\emph{sequence} instead of a set, \nas a different arrival order of $S$ may result in a different filter by \\cref{alg:robust}.\n\n\\begin{lemma}\n\\label{lemma:4t-robust}\nLet $S \\subseteq \\X$ be a sequence of length $16\\infd \\Ccmpmax$.\nLet $\\Gs$ be the groups constructed by \\cref{alg:robust}.\nUnder \\cref{asm:dirm}, we have\n\\[\n\t|\\{ \\vecx \\in S: \\Gs - \\vecx \\infersim \\vecx \\}| \\ge \\frac34 |S|,\n\\]\nwhere $\\infd = \\Cinfd$, \n\\Ccmpmax is the largest size of a pairwise \\vecu-similar subset of \\X, \nand $\\Gs - \\vecx$ are the groups with $\\vecx$ removed from its group.\n\\end{lemma}\n\\ifapx\n\\begin{proof}\\deferredproof\\end{proof}\n\\begin{appendixproof}[Proof of \\cref{lemma:4t-robust}]\n\\else\n\\begin{appendixproof}\n\\fi\nNote that by the definition of \\Ccmpmax, \nfor any particular tuple $\\vecx \\in S$,\nthere are at most $2(\\Ccmpmax-1)$ tuples that are \\vecu-similar with tuple \\vecx.\nThus, $\\Gs$ must contain at least $8\\infd$ groups, and\nwe split all groups in $\\Gs$ into two parts, those with an odd index and those with an even index.\n\nIn each part, we can extract a totally sorted list $L$ of size at least \\infd, by picking exactly one tuple from each group.\nWe remove one tuple $\\vecw \\in L$ from $S$ such that $L - \\vecw \\infer \\vecw$, whose existence is guaranteed by \\cref{lemma:comb}.\n\\cref{eq:filter-robust} guarantees that $\\Gs - \\vecx \\infersim \\vecx$.\n\nWe repeatedly do so until less than \\infd groups remain in each part, \nwhich means that the number of remaining tuples is at most $2 \\infd \\Ccmpmax$ in each part.\nAs a result, we are able to remove at least $16\\infd \\Ccmpmax - 4 \\infd \\Ccmpmax$ tuples, \nconcluding the claim.\n\\end{appendixproof}\n\nAlthough the above lemma appears similar to \\cref{lemma:4t}, \na crucial difference is that the set of prunable tuples in $S$ now depends on the arrival order of~$S$,\nwhich causes non-trivial technical challenges in the analysis.\nA critical observation that enables our analysis is the following result.\n\\begin{lemma}\n\\label{lemma:bounded-bad-representatives}\nFix a sequence $S$ of size $16\\infd \\Ccmpmax$, there exist at least $\\frac14 |S|$ tuples \\vecz in $S$ that satisfy\n\\[\n\tS - \\vecz \\infersim \\vecz.\n\\]\n\\end{lemma}\n\\ifapx\n\\begin{proof}\\deferredproof\\end{proof}\n\\begin{appendixproof}[Proof of \\cref{lemma:bounded-bad-representatives}]\n\\else\n\\begin{appendixproof}\n\\fi\nLet $\\Gs$ be the groups constructed by \\cref{alg:robust}.\nWrite\n\\[\n\tS' = \\{ \\vecx \\in S: \\Gs - \\vecx \\infersim \\vecx \\}.\n\\]\nBy \\cref{lemma:4t-robust}, we know that $|S'| \\ge \\frac34 |S|$.\nFor an arbitrary tuple $\\vecz \\in S'$, suppose \\vecz is assigned to a group $G \\in \\Gs$.\nWe call a tuple \\vecz \\emph{good} if $|G| = 1$ or \\vecz is not a representative in $R$ in \\cref{alg:robust}.\nLet $\\Gs'$ be the groups constructed by \\cref{alg:robust} using $S - \\vecz$. If $\\vecz$ is good,\nthen $\\Gs' = \\Gs - \\vecz$.\nTherefore, for a good tuple \\vecz we always have \n\\[\n\tS - \\vecz \\infersim \\vecz.\n\\]\nBy definition, it is easy to see that there are at most $|S|/2$ tuples in $S$ that are not good,\nproving the lemma.\n\\end{appendixproof}\n\n\nDenote by $\\filtersim(S)$ the set of tuples that can be pruned by $S$, that is,\n\\begin{align}\n\t\\filtersim(S) = \\{ \\vecx \\in \\reals^d: S \\infersim \\vecx \\}.\n\\end{align}\nWe now prove a similar lemma to \\cref{lemma:filter} by a generalized symmetrization argument over sequences.\n\\begin{lemma}\n\\label{lemma:filter-robust}\nGiven a set of tuples \\X, and\na random sequence $S$\nof at least $16\\infd \\Ccmpmax$\ntuples from $\\X$,\nwe have\n\\[\n\t\\expect \\left[ |\\filtersim(S) \\cap \\X| \\right] \n\t\\ge \\frac14 |\\X|,\n\\]\nwhere $\\infd = \\Cinfd$, \nand \\Ccmpmax is the largest size of a pairwise \\vecu-similar subset of \\X. \nMoreover, the expectation is taken over $S$.\n\\end{lemma}\n\\ifapx\n\\begin{proof}\\deferredproof\\end{proof}\n\\begin{appendixproof}[Proof of \\cref{lemma:filter-robust}]\n\\else\n\\begin{appendixproof}\n\\fi\nLet $\\vecy$ be the last tuple added into $S$. Write $T = S - \\vecy$.\nGiven $T$, the distribution of $\\vecy$ is a uniform distribution from $\\X \\setminus T$.\nLet $\\vecx$ be a random sample from $X$. \nSince $T \\subseteq \\filtersim(T)$, we have\n\\[\n\t\\Pr [  T \\infersim \\vecx \\mid T ] \\geq\n\t\\Pr [  T \\infersim \\vecy \\mid T ].\n\\]\nThen,\n\\begin{align*}\n    \\expect_S \\left[ \\frac{|\\filtersim(S) \\cap \\X|}{|\\X|} \\right] \n    &= \\expect_{S} [ \\Pr [ S \\infersim \\vecx \\mid S ] ] \\\\\n    &= \\expect_{S} [ \\Pr [ T + \\vecy \\infersim \\vecx \\mid S ] ] \\\\\n    &\\ge \\expect_{S} [ \\Pr [ T \\infersim \\vecx \\mid S ] ] \\\\\n    &= \\expect_{T} [ \\Pr [ T \\infersim \\vecx \\mid T ] ] \\\\\n    &\\ge \\expect_{T} [ \\Pr [ T \\infersim \\vecy \\mid T ] ] \\\\\n    &= \\expect_{S} [ \\indicator [ S - \\vecy \\infersim \\vecy \\mid S ] ].\n\\end{align*}\n\nFix $S$, let $\\vecz \\in S$ be a uniformly random tuple in $S$, and we have\n\\begin{align*}\n\t\\expect_{S} [ \\indicator [ S - \\vecy \\infersim \\vecy \\mid S ] ]\n\t&= \\expect_{S} [ \\Pr [ S - \\vecz \\infersim \\vecz \\mid S ] ] \\\\\n\t&\\ge 1/4,\n\\end{align*}\nwhere the last step is by \\cref{lemma:bounded-bad-representatives}, and the first step is due to double counting,\nas every sequence $S$ appears $|S|$ times in the right-hand side,\ncompleting the proof.\n\\end{appendixproof}\n\n\n\\ifapx\n\\begin{proof}[Proof of \\cref{thm:num_of_S-robust}]\nThe proof is similar to \\cref{thm:num_of_S} on a high level, and is deferred to Appendix.\n\\end{proof}\n\\begin{appendixproof}[Proof of \\cref{thm:num_of_S-robust}]\n\\else\n\\begin{appendixproof}\n\\fi\nThe proof is similar to \\cref{thm:num_of_S} on a high level.\nWe only elaborate on their differences.\n\nWe first prove the guarantee on the regret.\nIf the optimal tuple $\\vecx^*$ is in the pool once the algorithm is done, then the regret is at most $\\Ccmp$.\nIf $\\vecx^*$ is not in the pool, then\nthe proof of \\cref{thm:anytime} shows that there is $\\vecx$ in one of the sample, say $S$,\nthat yields a regret of $\\Creg/c$.\nThe top representative of that sample yields $\\Creg/c + \\Ccmp$ regret. Finally, the final top tuple\nyields $\\Creg/c + 2\\Ccmp$ regret.\n\nNext, we upper bound the size of every sample and the number of samples similarly to\nthe proof of \\cref{thm:num_of_S}.\nWe require every sample to prune at least $1/8$ fraction of the remaining tuples instead of $1/2$,\nwhich leads to a demand for $\\ceil{\\log_{8/7} (\\nD)}$ samples.\nThe total failure probability is bounded by $2\\nD e^{-2 \\nPl /16^2} \\leq 1/\\nD$.\nConsequently, with probability at least $1-1/\\nD$,\nwe will use at most\n$\\ceil{\\log_{8/7} (\\nD)}$ sample sets, each with a size $16\\infd \\Ccmpmax$, at most.\n\\note[more details]{\nFor any sample $S$ with at least $16\\infd \\Ccmpmax$ samples and any subset $\\X \\subseteq \\D$, \nlet $\\X' = \\filtersim(S) \\cap \\X$ and by \\cref{lemma:filter-robust} we have\n$\\expect [ |\\X'| ] \\ge \\frac14 |\\X|$.\nIn particular, let \\Pl is a random subset of \\X and we have \n$\\expect[|\\Pl'|] \\ge \\frac14 |\\Pl|$.\nThen, \n\\begin{align*}\n\t\\Pr\\left[ |\\Pl'| < \\frac3{16} |\\Pl| \\right] \n\t&= \\Pr\\left[ |\\Pl'| < \\frac14 |\\Pl| - \\frac1{16} |\\Pl|\\right]  \\\\\n\t&\\le \\Pr\\left[ |\\Pl'| < \\expect[|\\Pl'| - \\frac1{16} \\nPl] \\right] \\\\\n\t&\\le e^{-2 \\nPl /16^2},\n\\end{align*}\nwhere the last step invokes \\cref{lemma:tail}.\nSince there can be at most \\nD samples,\nthe probability that any sample fails to pass the pool test is upper bounded by $\\nD e^{-2 \\nPl /16^2}$.\n\nWe continue to upper bound the number of sample sets.\nAt most $\\ceil{\\log_{8/7}(\\nD)}$ sample sets suffice if every sample can prune at least 1/8 fraction of the remaining tuples.\nFix an arbitrary sample $S$, and let \\X to be the set of remaining tuples.\nThe pool \\Pl is a random sample from \\X of size \\nPl. Thus,\n$\\expect[|\\Pl'|] / \\nPl = |\\X'| / |\\X|$. Consequently, if  $|\\X'| < |\\X|/8$, then $\\expect[|\\Pl'|] < \\nPl/8$\nand\n\\begin{align*}\n\t\\Pr\\left[ |\\Pl'| \\ge \\frac3{16} |\\Pl| \\right] \n\t\\le \\Pr\\left[ |\\Pl'| \\ge \\expect[|\\Pl'|] + \\frac1{16} \\nPl \\right] \n\t\\le e^{-2 \\nPl /16^2}.\n\\end{align*}\nSimilar to the above, \nthe probability that any bad sample passes the test is upper bounded by $\\nD e^{-2 \\nPl /16^2}$.\n}\n\nBuilding one filter requires at most $\\bigO(16\\infd \\Ccmpmax \\log(16\\infd \\Ccmpmax))$ comparisons, because\nsorting an new tuple \\vecx within $R$ by binary search costs at most $\\bigO(\\log(16\\infd \\Ccmpmax))$ comparisons.\nFinally, finding the best tuple among all filters and the pool requires additional $\\nPl + \\ceil{\\log_{8/7} (\\nD)}$ comparisons.\n\\end{appendixproof}\n\\fi\n\\section{Improving baseline filters}\n\\label{se:extensions-short}\n\nIn this section, \nwe improve existing filters by \\citet{xie2019strongly},\nby using linear and quadratic programs.\nWe will use these baselines in the experiments.\nPreviously, their filters rely on explicit computation of convex hulls,\nwhich is feasible only in very low dimension~\\citep{barber1996quickhull}.\nTechnical details are deferred to \\cref{se:extensions}.\n\nExisting filters iteratively compare a pair of random tuples, all of which are kept in \n$A = \\{a_i\\}$,\nwhere $a_i = (\\vecy,\\vecz)$ such that $\\util(\\vecy) < \\util(\\vecz)$, \nand use them to prune potential tuples.\n\n\\paragraph{Filter by constrained utility space}\n\nGiven a tuple \\vecx, we try to find \na vector $\\vecu$ that, for all $ (\\vecy,\\vecz) \\in A$,\n\\begin{equation}\n\\restatableeq{\\eqlp}{\n\t\\vecu^T(\\vecz-\\vecy) \\ge 1,\\quad\n\t\\vecu^T(\\vecx-\\vecz) \\ge 1,\\quad\n\t\\vecu^T((1-\\Creg)\\vecx - \\vecz) \\ge 1.\n%\\begin{aligned}\n\t%\\vecu^T(\\vecz-\\vecy) &\\ge 1,  &&\\text{for all } (\\vecy,\\vecz) \\in A, \\\\\n\t%\\vecu^T(\\vecx-\\vecz) &\\ge 1, &&\\text{for all } (\\vecy,\\vecz) \\in A, \\\\\n\t%\\vecu^T((1-\\Creg)\\vecx - \\vecz) &\\ge 1, &&\\text{for all } (\\vecy,\\vecz) \\in A.\n%\\end{aligned}\n}{eq:lp}\n\\end{equation}\n\nWe claim that a given tuple \\vecx can be safely pruned if there is no vector~$\\vecu$ satisfying LP~(\\cref{eq:lp}).\n\n\\begin{restatable}{proposition}{proplp}\n\t\\label{prop:lp}\n\tConsider a tuple $\\vecx$ with $\\util(\\vecx) > \\util(\\vecz)$ and\n\t$\\util(\\vecx) - \\util(\\vecz) > \\Creg \\util(\\vecx)$ for every $(\\vecy, \\vecz) \\in A$.\n\tThen there is a solution to LP~(\\cref{eq:lp}).\n\\end{restatable}\t\n\n\n\\paragraph{Filter by conical hull of pairs}\n\nGiven a tuple \\vecx, we propose to solve the following quadratic program (QP),\n\\begin{align}\n\\restatableeq{\\eqqppairs}{\n\t& \\min_{\\nu,\\beta} \\bigg\\| \\vecx - \\sum_{a_i = (\\vecy,\\vecz) \\in A} (\\nu_{i1} \\, \\vecy + \\nu_{i2} \\, \\vecz) - \\sum_{a_i = (\\vecy,\\vecz) \\in A} \\beta_i (\\vecy-\\vecz) \\bigg\\| \\\\\n\t& \\text{such that}\n\t\\sum_{a_i = (\\vecy,\\vecz) \\in A} \\nu_{i1}+\\nu_{i2} = 1 \n\t \\quad\\text{and}\\quad\n\t\\nu_{i1}, \\nu_{i2}, \\beta_i \\ge 0 \\quad\\text{for all } i \\nonumber\n}{eq:qp-pairs}.\n\\end{align}\n\nIf the optimal value of the QP is at most $\\Creg$, we prune \\vecx.\n\n\\begin{restatable}{proposition}{propqppairs}\n\t\\label{prop:qp-pairs}\n\tLet $\\vecu^T \\vecx^* = c$.\n\tA tuple $\\vecx \\in \\D$ can be pruned if \n\tthe objective value of the quadratic program~(\\cref{eq:qp-pairs}) is at most $\\Creg / c$.\n\\end{restatable}\t\n\n\nIf we set $\\epsilon = 0$, then we can use LP solver (similar to \\cref{eq:filter:lp}) instead of QP solver.\nThis results in a weaker but computationally more efficient filter.\n%\\label{se:extensions}\n%\\input{extensions}\n\n\\section{Experimental evaluation}\n\\label{se:experiment}\n\nIn this section, we evaluate key aspects of our method and the proposed filters.\nLess important experiments and additional details are deferred to \\cref{se:experiment:supp}.\nIn particular, we investigate the following questions.\n\t($i$) How accurate is the theoretical bound in \\cref{lemma:filter}? \n    More specifically, we want to quantify the sample size required by \\cref{alg:single} to prune at least half of the tuples, \n    and understand its dependance on the data size \\nD, dimension $d$, \n    and regret parameter \\Creg. \n%     (\\cref{sse:ex:sameplesize})\n    ($ii$) Effect of parameters of \\cref{alg:framework}.\n\t (\\cref{sse:ex:param})\n\t($iii$) How scalable are the proposed filters?\n\t %(\\cref{sse:ex:scalable})\n\t($iv$) How do the proposed filters perform over real-life datasets?\n\t %(\\cref{sse:ex:single})\n\t($v$) How do ties in comparisons affect the performance of the proposed filters?\n\t %(\\cref{sse:ex:ties})\nOur implementation is available at \\code\n\n\\smallskip\n\\noindent\nNext, let us introduce the adopted datasets and baselines.\n\n\\smallskip\n\\para{Datasets.}\nA summary of the real-life datasets we use for our evaluation can be found in \\cref{tbl:datasets}.\nTo have more flexible control over the data parameters,\nwe additionally generate the following two types of synthesized data.\n\t\\emph{sphere}: Points sampled from the unit $d$-sphere \\sphere uniformly at random.\n\t\\emph{clusters}: Normally distributed clustered data, where each cluster is centered at a random point on unit $d$-sphere \\sphere.\nTo simulate an oracle, we generate a random utility vector \\vecu on the unit $d$-sphere for every run.\nMore details about datasets can be found in \\cref{se:experiment:supp}.\n\n\n\\smallskip\n\\para{Baselines.}\nA summary of all algorithms is given in \\cref{tbl:algs}.\nWe mainly compare with (enhanced) pruning techniques (\\pairqp, \\pairlp and \\hplp) by \\citet{xie2019strongly}, \nhalfspace-based pruning (\\hp), and \na random baseline (\\rand).\nDiscussion of other baselines is deferred to \\cref{se:experiment:supp}.\nWe instantiate every filter (except for the \\hp and \\rand) in the framework provided in \\cref{alg:framework},\nthat is, we iteratively create a new filter that can prune about half of the remaining tuples.\nThis is a reasonable strategy, and will be justified in detail in \\cref{sse:ex:scalable}.\nFor pair-based filters, a new pair is made after two consecutive calls of the \\emph{add} function.\nThe pool size \\nPl and threshold \\CPl in \\cref{alg:framework} are set to be 100 and 0.5, respectively.\n%which is sufficient for all data sizes in our experiments, according to \\cref{thm:num_of_S}.\nSince the proposed algorithm \\listqp only guarantees a regret of $\\Creg/\\util(x^*)$, where $x^*$ is the best tuple in the dataset,\nwe pre-compute the value of $\\util(x^*) \\in [0,1]$, and adjust the regret parameter of \\listqp to be $\\Creg \\util(x^*)$.\n\n\n\\begin{table}[t]\n  \\caption{Real-life datasets statistics}\n  \\label{tbl:datasets}\n  \\centering\n  \\vspace{-0.3cm}\n\\begin{tabular}{lrr}\n\\toprule\nDataset & $\\nD = |\\D|$ \n        & $d$ \\\\\n\\midrule\nplayer \\citep{dataplayer}\t&17 386\t&20\t\\\\\nyoutube \\citep{datayoutube}\t&29 406\t&50\t\\\\\ngame \\citep{datagame}\t&60 496\t&100\t\\\\\nhouse \\citep{datahouse}\t&303 032\t&78\t\\\\\ncar \\citep{datacar}\t&1 002 350\t&21\t\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\\begin{table}[t]\n  \\caption{Summary of our methods and the baselines.}\n  \\label{tbl:algs}\n  \\centering\n  \\vspace{-0.3cm}\n\\begin{tabular}{l p{0.35\\textwidth}}\n\\toprule\nName & Brief description\\\\\n\\midrule\nList-[QP|LP]\t&\\emph{Our method}: prune a tuple if it is close to a conical hull formed by a sorted list of random tuples (\\cref{alg:single}), equipped with a QP or LP solver.\t\\smallskip\\\\\nPair-[QP|LP]\t&Prune a tuple if it is close to a conical hull formed by a set of compared random pairs, equipped with QP (\\cref{eq:qp-pairs}) or LP \n%(\\cref{eq:lp-pairs}) \nsolver. \\smallskip\\\\\n\\hplp\t&Prune a tuple if LP (\\cref{eq:lp}) is infeasible, i.e.,\nthe tuple is dominated by a set of compared random pairs over the entire constrained utility space.\t\\smallskip\\\\\n\\hp\t&Prune a tuple \\vecx if $\\vecx^T(\\vecz-\\vecy) < 0$ for any compared pair $\\vecz,\\vecy$ such that $\\util(\\vecy) < \\util(\\vecz)$, that is,\ntuple \\vecx falls outside the constrained utility space for \\vecu. \\smallskip\\\\\n\\rand\t&Return the best tuple among a subset of 50 random tuples.\t\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\\begin{figure}[!ht]\n    \\centering\n    \\subcaptionbox{Memory size required to prune half of tuples\\label{fig:n-sz}}{\n    \t\\pgfinput[trim={0 0 0 0.1cm},clip]{prune-n-sz}\\vspace{-0.3cm}\n    }\\par\\vspace{0.2cm}\n    \\subcaptionbox{Scalability with respect to dimension $d$\\label{fig:d-runtime}}{\n  \t    \\pgfinput[trim={0 0 0 0.8cm},clip]{prune-d-runtime}\\vspace{-0.3cm}\n    }\n    \\vspace{-0.3cm}\n    \\caption{Scalability of filters for synthetic data}\n    \\label{fig:ex}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\centering\n    % trim={<left> <lower> <right> <upper>}\n   \t\\pgfinput[trim={0 0 0 0.1cm},clip]{online-player-tol-ncmp}\\par\\vspace{-0.3cm}\n   \t\\pgfinput[trim={0 0 0 0.8cm},clip]{online-youtube-tol-ncmp}\\par\\vspace{-0.3cm}\n   \t\\pgfinput[trim={0 0 0 0.8cm},clip]{online-game-tol-ncmp}\\par\\vspace{-0.3cm}\n \t\\pgfinput[trim={0 0 0 0.8cm},clip]{online-house-tol-ncmp}\\par\\vspace{-0.3cm}\n \t\\pgfinput[trim={0 0 0 0.8cm},clip]{online-car-tol-ncmp}\n \t\\vspace{-0.6cm}\n    \\caption{Solving the \\irm problem on the real-life datasets}\n    \\label{fig:ex:single}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\vspace{-0.4cm}\n    \\pgfinput{gap-gap-ncmp}\n    \\vspace{-0.6cm}\n    \\caption{The effect of ties and parameter \\Ccmp}\n    \\label{fig:ties}\n\\end{figure*}\n\n\n\n\\subsection{Sample size in practice}\n\\label{sse:ex:sameplesize}\n\n\\cref{lemma:filter} proves a theoretical bound on the size of a random sample \nrequired by \\cref{alg:single} to prune at least half of a given set \\D of tuples in expectation.\nThis bound is $2\\infd$ where $\\infd = \\Cinfd$.\nImportantly, the bound does not depend on the data size $|\\D|$,\nwhich we verify later in~\\cref{sse:ex:scalable}.\n\nIn \\cref{fig:d-sz,fig:tol-sz} (in Appendix), we compute and present the exact required size for synthesized data, and illustrate\nhow the size changes with respect to the dimension $d$ and regret parameter \\Creg.\nAs can be seen, the bound provided in \\cref{lemma:filter} captures a reasonably accurate dependence on $d$ and \\Creg, up to a constant factor.\n\n\n\\subsection{Scalability}\n\\label{sse:ex:scalable}\n\nThe running time required for each filter to prune a given tuple depends heavily on its memory size,\ni.e., the number of tuples it keeps.\nIn \\cref{fig:n-sz}, we compute and show the required memory size for a filter to prune half of a given set \\D of tuples, and\nhow the size changes with respect to the data size $\\nD = |\\D|$.\nImpressively, most competing filters that adopt a randomized approach only require constant memory size, regardless of the data size \\nD.\nThis also confirms the effectiveness of randomized algorithms in pruning.\n\nBased on the above observation,\nit is usually not feasible to maintain a single filter to process a large dataset \\D.\nIf a filter requires~$s$ tuples in memory to prune half of \\D,\nthen at least $s \\log(|\\D|)$ tuples are expected to process the whole dataset \\D.\nHowever, the running time for both LP and QP solvers is superlinear in the memory size of a filter~\\citep{cohen2021solving}, \nwhich means that running a filter with $s \\log(|\\D|)$ tuples is considerably slower than running $\\log(|\\D|)$ filters, each with $s$ tuples.\nThe latter approach enables also parallel computing for faster processing.\n\nTherefore, we instantiate each competing filter (except for \\hp and \\rand) in the framework provided in \\cref{alg:framework}, and\nmeasure the running time it takes to solve the \\irm problem.\nIn the rest of this section, \nwe investigate the effect of the data dimension $d$ and regret parameter \\Creg on the running time.\n\n\\smallskip\n\\para{Effect of data dimension $d$.}\nIn \\cref{fig:d-runtime}, \nwe fix a regret parameter $\\Creg=0.01$, and\nexamine how the running time of a filter varies with respect to the data dimension $d$ on synthesized data.\n\nThe first observation from \\cref{fig:d-runtime} is that LP-based filters are more efficient than their QP counterparts.\nParticularly, \\pairqp is too slow to be used, and we have to settle for its LP counterpart \\pairlp in subsequent experiments.\n\nLet us limit the comparison to those LP-based filters.\n\\pairlp and \\hplp are more computationally expensive than \\listlp.\nFor \\pairlp, the reason is obvious: \nas discussed at the end of \\cref{sse:cone-by-pairs}, \n\\pairlp makes relatively more comparisons and \nevery compared pair of tuples adds two more parameters to the LP.\nFor \\hplp, the number of parameters in its LP depends linearly on both the dimension $d$ and number of compared pairs,\nwhile \\listlp only depends on the latter.\nThus, \\hplp is less scalable by design.\n\\note{When $d$ increases, the impact of \\#parameters needed for compared pairs dominates that of those modeling \\vecu for \\hplp, so its running time converges to \\listlp.}\n\n\\smallskip\n\\para{Effect of regret parameter \\Creg.}\nThe effect of the regret parameter \\Creg can be found in \\cref{fig:ex:single} for all real-life datasets.\nGenerally, a larger value of \\Creg decreases the running time, \nas each filter can be benefited by more aggressive pruning.\n% by design of the basic pruning rule: pruning a given tuple \\vecx if it is within a distance of \\Creg from any tuple kept by the filter.\n\nThe running time of \\listqp deteriorates dramatically for a small value of \\Creg, and\nthe number of comparisons needed also rises considerably.\nThe reason is that,\nmost numerical methods for solving a mathematical program have a user-defined \\emph{precision} parameter.\nSmall precision gives a more accurate solution, and at the same time causes a longer running time.\nWhen \\Creg gets close to the default precision, or to the actual precision after the maximum number of iterations is exceeded, \\listqp fails to prune tuples.\nThus, \\listqp is advised to be used for a relatively large regret value \\Creg.\n\nIn regard to the memory size,\nas we can see in \\cref{fig:ex:single},\n\\listqp and \\listlp consistently use a much smaller memory size than \\pairlp and \\hplp.\nThis also demonstrates the advantage of using a sorted list over a set of compared pairs.\n\n\n\\subsection{The case of oracles with no ties}\n\\label{sse:ex:single}\n\nThe performance of competing filters can be found in \\cref{fig:ex:single} for all real-life datasets.\nThe average and standard error of three random runs are reported.\nWe instantiate each competing filter (except for \\hp and \\rand) in the framework provided in \\cref{alg:framework} to solve the \\irm problem.\nMeanwhile, we vary the regret parameter \\Creg to analyze its effect.\nWe also experimented with a smaller \\Creg value such as 0.005, \nthe observations are similar except that the \\listqp filter is significantly slower for reasons we mentioned in \\cref{sse:ex:scalable}.\n\nExcept \\hp and \\rand,\nevery reasonable filter succeeds in returning a low-regret tuple.\nWe limit our discussion to only these reasonable filters.\nIn terms of the number of comparisons needed,\n\\listqp outperforms the rest on most datasets provided that the regret value \\Creg is not too small.\nWe rate \\listlp as the runner-up, and it becomes the top one when the regret value \\Creg is small.\nBesides, \\listlp is the fastest to run.\nThe number of comparisons needed by \\hplp and \\pairlp is similar, and \nthey sometimes perform better than others, for example, over the youtube dataset.\n\nLet us make a remark about the regret value \\Creg.\nBeing able to exploit a large value of \\Creg in pruning is the key to improving performance.\nNotice that both \\pairlp and \\listlp cannot benefit from a large regret value \\Creg by design.\nThough \\hplp is designed with \\Creg in mind, \nit is more conservative as its pruning power depends on $\\Creg \\vecu^T\\vecx$ instead of $\\Creg \\vecu^T\\vecx^*$,\nwhere \\vecx is the tuple to prune.\n\nIn summary, we can conclude that\nthe \\listqp filter is recommended for a not too small regret parameter \\Creg (i.e., $\\Creg \\ge 0.1$), and\nthe \\listlp filter is recommended otherwise.\nIn practice, since both \\listqp and \\listlp follow an almost identical procedure, \none could always start with \\listqp, and switch to \\listlp if the pruning takes too long time. \n\n\\todo{I cannot explain why in house dataset, \\#comparison goes up for \\listlp and \\hplp by increasing \\Creg. \nThis also happens mildly in game dataset. \nOne common feature of them is having a lot of one-hot attributes.}\n\n\n\\subsection{Effect of ties}\n\\label{sse:ex:ties}\n\nAccording to \\cref{asm:dirm},\nthe oracle returns a tie if the difference in utility between two given tuples is within a parameter \\Ccmp.\nFor filters like \\pairlp and \\hplp, the most natural strategy to handle a tie for a pair of tuples is to simply discard one of them.\nIt is expected that ties worsen the performance of a filter, as they fail to provide\nadditional information required by the method for pruning.\n\nIn \\cref{fig:ties}, we vary the value of parameter \\Ccmp to see \nhow it affects the performance of the proposed filters.\nIt is not surprising that\nas the value of \\Ccmp increases, the number of ties encountered and the number of comparisons made by all algorithms both increase.\n\nNotably, the running time of \\listqp and \\listlp grows significantly as \\Ccmp increases. \nThis is because one parameter is needed in their solvers for every pair of tuples between two consecutive groups $G_i,G_j$, and\nthe total number of parameters can increase significantly if the size of both groups increases.\nThis behavior also reflects the fact that a partially sorted list is less effective for pruning.\nHowever, how to handle a large \\Ccmp remains a major open problem.\nHence, we conclude that the proposed algorithms work well provided that the parameter \\Ccmp is not too large.\n\n\n\\paragraph{Summary}\n\\label{sse:ex:summary}\n\nAfter the systematical evaluation,\nwe conclude with the following results.\n\t($i$) LP-based filters are more efficient than their QP counterparts, but less effective in pruning.\n\t($ii$) \\listlp is the most scalable filter.\n\tThe runner-up is \\listqp, provided that the data dimension is not too large ($d < 128$) and the regret parameter \\Creg is not too small ($\\Creg \\ge 0.1$).\n\t($iii$) To minimize the number of requested comparisons,\n\t\\listqp is recommended for a not too small \\Creg ($\\Creg \\ge 0.1$).\n\tWhen \\Creg is small, we recommend \\listlp.\n\t($iv$) Good performance can be retained if the oracle is sufficiently discerning ($\\Ccmp \\le 0.01$).\n\tOtherwise, a better way to handle ties will be needed.\n\n\n\n\n\\todo[Minor points]{\n\\begin{itemize}\n\t\\item Handle (few) rounding error in comparison\n\t\\item Run for offline settings\n\t\\item Combine different kinds of filters\n\\end{itemize}\n}\n\n\\section{Conclusion}\n\\label{se:conclusion}\n\nWe devise a single-pass streaming algorithm for finding a high-utility tuple by making adaptive pairwise comparisons.\n%To the best of our knowledge, the proposed algorithm offers state-of-the-art query complexity that is logarithmic in \\nD in the worst case, \n%where \\nD is the number of tuples.\nWe also show how to maintain the guarantee when ties are allowed in a comparison between two tuples with nearly equal utility.\n%We systematically evaluate all proposed algorithms with both synthesized and real-life datasets, and \n%demonstrate that the proposed algorithms enjoy superior performance over previous state-of-the-art methods. \n%\nOur work suggests several future directions to be explored.\nThose include\nfinding a high-utility tuple in the presence of noise,\nincorporating more general functions for modeling tuple utility, \ndevising methods with provable quarantees for arbitrary-order data streams,\nand\ndevising more efficient algorithms to handle ties.\n\n\\begin{acks}\nThis research is supported by the Academy of Finland projects MALSOME (343045) and MLDB (325117),\nthe ERC Advanced Grant REBOUND (834862),\nthe EC H2020 RIA project SoBigData++ (871042),\nand the Wallenberg AI, Autonomous Systems and Software Program (WASP)\nfunded by the Knut and Alice Wallenberg Foundation.\n\\end{acks}\n\n\\note[SVM]{Just a remark that SVM is a special instance of QP.\nDifferent from constrained least squares, \nit minimizes the $\\ell_2$ norm the coefficient vector for a linear function \nsubject to a set of linear inequalities (i.e., hard margin constraints), one for each data point.}\n\n\n%\\section{Subfiles}\n%\\subfile{legacy/deprecated}\n%\\subfile{legacy/S}\n\n\\newpage\n\\balance\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{references}\n\n\n\\ifsupp % start of appendix: repeat again in case apx is off\n\\ifapx \\else\n\t\\clearpage\n\t\\appendix\n%\t\\section{Another filter for \\irm with ties}\\label{se:backup}\n\t\\input{supp}\n\\fi\n\\fi % end of appendix\n\n\n"
                }
            }
        },
        "figures": {
            "fig:irm": "\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.35\\textwidth]{pics/irm}\n\t\\caption{An illustrative example for the \\irm problem.\n\tThe unknown utility vector \\vecu is drawn in orange.\n\tEvery tuple is shown as a point within a unit circle, \n\twhere the red point represents the top tuple, and \n\tgreen points are feasible \\Creg-regret tuples for the \\irm problem.\n\t}\\label{fig:irm}\n\\end{figure}",
            "fig:filter": "\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.3\\textwidth]{pics/filter}\\par\\vspace{-0.2cm}\n\t\\caption{An illustrative example for a filter in \\cref{alg:single}.\n\tThe unknown utility vector \\vecu is drawn in orange.\n\tEvery tuple is shown as a point within a unit circle, \n\twhere the red point represents the top tuple, and \n\tgreen points are feasible \\Creg-regret tuples for the \\irm problem.\n\t%An illustrative example for a filter in \\cref{alg:single} with the same setup as \\cref{fig:irm}.\n\tSuppose a filter collects a random sample $S$ of blue points.\n\tA sorted sample $S$ can be used to prune any point that falls into or is sufficiently close to (within a distance of \\Creg) the blue cone.\n\t}\\label{fig:filter}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\\label{eq:filter:lp}\n\t\\vecx = \\vecx_1 + \\sum_{j=1}^{\\sz-1} \\alpha_j (\\vecx_{j+1} - \\vecx_j)\n\t\\quad\n\t\\text{such that} \\; \\alpha_j \\ge 0 \\quad \\text{for all } j.\n\\end{align}",
            "eq:2": "\\begin{align}\n\t\\label{eq:filter}\n\t&\\min_\\alpha \\; \\bigg\\| \\vecx - \\vecx_1 - \\sum_{j=1}^{\\sz-1} \\alpha_j (\\vecx_{j+1} - \\vecx_j) \\bigg\\|_2 \\le \\Creg \n\t\\ \\ \\  \\text{s.t.} \\ \\  \\alpha_j \\ge 0 \\  \\text{for all } j .\n\\end{align}",
            "eq:3": "\\begin{align}\n\t\\label{eq:implies}\n\tS \\infer \\vecx\n\\end{align}",
            "eq:4": "\\begin{align}\n\t\\filter(S) = \\{ \\vecx \\in \\reals^d: S \\infer \\vecx \\}.\n\\end{align}"
        }
    }
}