{
    "meta_info": {
        "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
        "abstract": "We introduce a framework for multi-camera 3D object detection. In contrast to\nexisting works, which estimate 3D bounding boxes directly from monocular images\nor use depth prediction networks to generate input for 3D object detection from\n2D information, our method manipulates predictions directly in 3D space. Our\narchitecture extracts 2D features from multiple camera images and then uses a\nsparse set of 3D object queries to index into these 2D features, linking 3D\npositions to multi-view images using camera transformation matrices. Finally,\nour model makes a bounding box prediction per object query, using a set-to-set\nloss to measure the discrepancy between the ground-truth and the prediction.\nThis top-down approach outperforms its bottom-up counterpart in which object\nbounding box prediction follows per-pixel depth estimation, since it does not\nsuffer from the compounding error introduced by a depth prediction model.\nMoreover, our method does not require post-processing such as non-maximum\nsuppression, dramatically improving inference speed. We achieve\nstate-of-the-art performance on the nuScenes autonomous driving benchmark.",
        "author": "Yue Wang, Vitor Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, Justin Solomon",
        "link": "http://arxiv.org/abs/2110.06922v1",
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "additionl_info": "Accepted to CORL 2021"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n3D object detection from visual information is a long-standing challenge for low-cost autonomous driving systems. While object detection from point clouds collected using modalities like LiDAR benefits from information about the 3D structure of visible objects, the camera-based setting is even more ill-posed, since we must generate 3D bounding box predictions solely from the 2D information contained in RGB images. \n\nExisting methods~\\cite{zhou2019centernet,wang2021fcos3d} typically build their detection pipelines purely from 2D computations. That is, they predict 3D information like object pose and velocity using an object detection pipeline designed for 2D tasks (e.g., CenterNet~\\cite{zhou2019centernet}, FCOS~\\cite{tian2019fcos}), without considering 3D scene structure or sensor configuration. These methods require several post-processing steps to fuse predictions across cameras and to remove redundant boxes, yielding a steep trade-off between efficiency and effectiveness. \n%\nAs an alternative to these 2D-based methods, some methods incorporate more 3D computations into our object detection pipeline by applying a 3D reconstruction method like \\cite{monodepth2,bts,packnet} to create a pseudo-LiDAR or range input of the scene from camera images. Then, they could apply 3D object detection methods to this data as if it were collected directly from a 3D sensor.  This strategy, however, is subject to compounding errors~\\cite{demistifying}: poorly-estimated depth values have a strongly negative effect on the performance of 3D object detection, which also can exhibit errors of its own.\n\nIn this paper, we propose a more graceful transition between 2D observations and 3D predictions for autonomous driving, which does not rely on a module for dense depth prediction. \nOur framework, termed \\name (Multi-View 3D Detection), addresses this problem in a top-down fashion. We link 2D feature extraction and 3D object prediction via geometric back-projection with camera transformation matrices. \nOur method starts from a sparse set of object priors, shared across the dataset and learned end-to-end. To gather scene-specific information, we back-project a set of reference points decoded from these object priors to each camera and fetch the corresponding image features extracted by a ResNet backbone~\\cite{He2015}. \n%\nThe features collected from the image features of the reference points then interact with each other through a multi-head self-attention layer~\\cite{Vaswani2017}. After a series of self-attention layers, we read off bounding box parameters from every layer and use a set-to-set loss inspired by DETR~\\cite{detr} to evaluate performance.\n\nOur architecture does not perform point cloud reconstruction or explicit depth prediction from images, making it robust to errors in depth estimation. Moreover, our method does not require any post-processing, such as non-maximum suppression (NMS), improving efficiency and reducing reliance on hand-designed methods for cleaning its output. On the nuScenes dataset, our method (without NMS) is comparable with prior art (with NMS). In the camera overlap regions, our method significantly outperforms others.\n\n%\\justin{maybe conclude with a sentence or two about your empirical results---you tested it and it does well, especially in setting XYZ}\n\n\\textbf{Contributions.} We summarize our key contributions as follows:\n\\begin{itemize}\n  \\item We present a streamlined 3D object detection model from RGB images.\n  Different from existing works that combine object predictions from the different camera views in a final stage, our method  fuses information from all the camera views in each layer of computation. To the best of our knowledge, this is the first attempt to cast multi-camera detection as 3D set-to-set prediction.  \n  \\item We introduce a module that connects 2D feature extraction and 3D bounding box prediction via backward geometric projection. It does not suffer from inaccurate depth predictions from a secondary network, and seamlessly uses information from multiple cameras by back-projecting 3D information onto all available frames.  \n%  We provide analysis to further explain the advantages of backward projection. \n  \\item Similarly to Object DGCNN~\\cite{objdgcnn}, our method does not require post-processing such as per-image or global NMS, and it is on par with existing NMS-based methods. In the camera overlap regions, our method outperforms others by a substantial margin. \n%   Our model outperforms existing works on the nuScenes~\\cite{nuscenes2019} self-driving car benchmark without requiring post-processing such as per-image or global NMS.\\vitor{Do we cite the neurIPS submission?} \\justin{we need to}\n  \\item We release our code to facilitate reproducibility and future research. \\footnote{\\url{https://github.com/WangYueFt/detr3d}} \n\\end{itemize}\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n%RCNN, Fast RCNN, Faster RCNN, Mask RCNN, RetinaNet, CenterNet and etc. \n\\textbf{2D object detection.} RCNN~\\cite{girshick2014rcnn} pioneered object detection using deep learning. It feeds a set of pre-selected object proposals into a convolutional neural network (CNN) and predicts bounding box parameters accordingly. Although this method exhibits surprising performance, it is an order of magnitude slower than others because it performs a ConvNet forward pass for each object proposal. To fix this issue, Fast RCNN~\\cite{ren2015faster} introduces a shared learnable CNN to process the entire image at a single forward pass. To further improve performance and speed, Faster RCNN~\\cite{ren2015faster} includes a region proposal network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. Mask RCNN~\\cite{he2017maskrcnn} incorporates a mask prediction branch to enable instance segmentation in parallel. These methods typically involve multi-stage refinements and can be slow in practice. Different from these multi-stage methods, SSD~\\cite{liu2016ssd} and YOLO~\\cite{Redmon2015YouOL} perform dense predictions in a single shot. Although they are significantly faster than the alternatives above, they still rely on NMS to remove redundant box predictions. These methods %, no matter how many stages they operate, \npredict bounding boxes w.r.t.\\ pre-defined anchors. CenterNet~\\cite{zhou2019centernet} and FCOS~\\cite{tian2019fcos} change the paradigm by shifting from per-anchor prediction to per-pixel prediction, significantly simplifying the common object detection pipeline.\n\n\\textbf{Set-based object detection.} DETR~\\cite{detr} casts object detection as a set-to-set problem. It employs a Transformer~\\cite{Vaswani2017} to capture feature and object interactions. DETR learns to assign predictions to a set of ground-truth boxes; thus, it does not require post-processing to filter out redundant boxes. One critical drawback of DETR, however, is that it requires a significant amount of training time. \nDeformable DETR~\\cite{zhu2021deformable} analyzes DETR's slow convergence and proposes a deformable self-attention module to localize features and accelerate training. Concurrently, \\cite{Sun2020RethinkingTS} attributes the slow convergence of DETR to  the set-based loss and the Transformer cross attention mechanism. They propose two variants, TSP-FCOS and TSP-RCNN, to overcome these problematic aspects. SparseRCNN~\\cite{peize2020sparse} incorporates set prediction into a RCNN-style pipeline; it outperforms multi-stage object detection without NMS. OneNet~\\cite{peize2020onenet} studies an interesting phenomenon: dense-based object detectors can be made NMS-free after they are equipped with a minimum-cost set loss. For 3D domains, Object DGCNN~\\cite{objdgcnn} studies 3D object detection from point clouds. It models 3D object detection as message passing on a dynamic graph, generalizing the DGCNN framework to predict a set of objects. Similar to DETR, Object DGCNN is also NMS-free. \n\n%DETR, Deformable DETR, SparseRCNN, OneNet, Rethinking Transformer detection.\n\n\n\n\\textbf{Monocular 3D object detection.} \n%Mono. FCOS3D and others. PL.  \n%maybe talk about object detection on point clouds. \nAn early method for 3D detection from RGB images is Mono3D~\\cite{chen2016monocular}, which uses semantic and shape cues to %disambiguate over\nselect from\na collection of 3D proposals, using scene constraints and additional priors at training time. \\cite{roddick2018orthographic} uses the birds-eye-view (BEV) for monocular 3D detection, and~\\cite{mousavian20173d} leverages 2D detections for 3D bounding box regression via the minimization of 2D-3D projection error.  The use of 2D detectors as a starting point for 3D computation recently has become a standard approach~\\cite{kehl2017ssd,ku2019monocular}. Other works also explore advances in differentiable rendering~\\cite{beker2020monocular} or 3D keypoint detection~\\cite{barabanau2019monocular,liu2020smoke,zhou2019centernet} to enable state-of-the-art 3D object detection performance. All these methods operate in a monocular setting, and extensions to multiple cameras are done by independently processing each frame before merging the outputs in a post-processing stage. \n\n%Recently, monocular depth estimation methods have started to leverage multi-camera constraints to improve performance~\\cite{fsm}, however these concepts have so far not been extended to 3D object detection. \n\n% \\textbf{Cross Camera}\n\n% Full Surround Monodepth \\cite{fsm}% \\section{Method}\n"
            },
            "section 3": {
                "name": "Multi-view 3D Object Detection",
                "content": "\n",
                "subsection 3.1": {
                    "name": "Overview",
                    "content": "\n\\label{overview}\n% Talk about the pipeline in high level and why our method is exciting. \n% We consider an application of 3D object detection where the scene geometry is missing and only 2D multi-camera images are available. This challenging problem gives rise to  . \n\nOur architecture inputs RGB images collected from a set of cameras whose projection matrices (the combination of intrinsics and relative extrinsics) %\\justin{extrinsics? not sure the right term} \nare known, and it outputs a set of 3D bounding box parameters for the objects in the scene.  In contrast to past approaches, we build our architecture based on a few high-level desiderata:\n\\begin{itemize}\n    \\item %Our 3D object detection paradigm differs from existing methods that predict 3D bounding boxes directly with a 2D object detection pipeline~\\cite{} with post-processing.\n    We incorporate 3D information into intermediate computations within our architecture, rather than performing purely 2D computations in the image plane.\n    \\item %Moreover, our method does not recover the 3D scene geometry, which avoids introducing additional reconstruction errors.\n    We do not estimate dense 3D scene geometry, avoiding associated reconstruction errors.\n    \\item We avoid post-processing steps such as NMS.\n\\end{itemize}\n\n\n\nWe address these desiderata using a new set prediction module, which links 2D feature extraction and 3D box prediction by alternating between 2D and 3D computations. Our model contains three critical components, illustrated in Figure~\\ref{fig:architecture}. First, following  common practice in 2D vision, it extracts features from the camera images using a shared ResNet~\\cite{He2015} backbone. Optionally, these features are enhanced by a feature pyramid network (FPN)~\\cite{Lin2017} (\\S\\ref{feature-learning}). Second, a detection head (\\S\\ref{detection-head})---our main contribution---links the computed 2D features to a set of 3D bounding box predictions in a geometry-aware manner (\\S\\ref{detection-head}). Each layer of the detection head starts from a sparse set of object queries, which are learned from the data. Each object query encodes a 3D location, which is projected to the camera planes and used to collect image features via bilinear interpolation.  Similarly to DETR~\\cite{detr}, we then use multi-head attention~\\cite{Vaswani2017} to refine the object queries by incorporating object interactions. This layer is repeated multiple times, alternating between feature sampling and object query refinement. Finally, we evaluate a set-to-set loss~\\cite{StewartAN16,detr} to train the network (\\S\\ref{loss}).  \n\n"
                },
                "subsection 3.2": {
                    "name": "Feature Learning",
                    "content": "\n\\label{feature-learning}\nOur model starts with a set of images $\\I=\\{\\mathrm{\\mathbf{im}}_1, \\ldots, \\mathrm{\\mathbf{im}}_K\\} \\subset\\R^{\\mathrm{H_{im}}\\times \\mathrm{W_{im}}\\times3}$  (captured by surrounding cameras)%\\justin{maybe move the constants like the image sizes and number of cameras to the experiment section and give them variable names here}\n% \\vitor{Agree, we can use $N$ instead of 6, and maybe $I$ instead of $im$ to define each image}\n, camera matrices $\\T=\\{T_1, \\ldots, T_K\\} \\subset \\R^{3\\times4}$\n%\\justin{use capital letters for matrices, these look like vectors}\n,  ground-truth bounding boxes $\\B=\\{\\bb_1, \\ldots, \\bb_j, \\ldots, \\bb_M\\}\\subset\\R^9$, and categorical labels $\\C = \\{c_1, \\ldots, c_j, \\ldots, c_M\\}\\subset\\mathbb{Z}$. Each $\\bb_j$ contains position, size, heading angle, and velocity in the birds-eye view (BEV); our model aims to predict these boxes and their labels from the these images. We \\emph{do not} use point clouds, which are usually captured by high-end LiDAR. \n\n% \\justin{fix the 4,  6 constants ``hard coded'' below---move those to experiments and use letters}\nThese images are encoded with a ResNet~\\cite{He2015} and a FPN~\\cite{Lin2017} into four sets of features $\\F_1, \\F_2, \\F_3, \\F_4$. Each set $\\F_k=\\{\\bff_{k1}, \\ldots, \\bff_{k6}\\} \\subset \\R^{H\\times W \\times C}$ corresponds to a level of features of the 6 images. These multi-scale features provide rich information to recognize objects of different sizes. \n% \\justin{why four? what do the four things represent? remember to tell us the \\emph{idea} not just the pieces that hook together} \nNext, we detail our approach to transform these 2D features into 3D using a novel set prediction module. \n\n% \\subsection{Object Queries}\n% \\label{object-query}\n\n"
                },
                "subsection 3.3": {
                    "name": "Detection Head ",
                    "content": "\n\\label{detection-head}\n\nExisting methods for detecting objects from camera input typically employ a bottom-up approach, which predicts a dense set of bounding boxes per image, filters redundant boxes between the images, and aggregates predictions across cameras in a post-processing step. This paradigm has two crucial drawbacks: dense bounding box prediction requires accurate depth perception, which itself is a challenging problem; and NMS-based redundancy removal and aggregation are non-parallelizable operations that introduce significant inference overhead. We address these issues using a top-down object detection head described below. \n\nAnalogously to \\cite{objdgcnn, zhu2021deformable}, \\name is \\emph{iterative}; it uses $L$ layers with set-based computations to produce bounding box estimates from 2D feature maps. Each layer includes the following steps:\n\\begin{enumerate}\n    \\item predict a set of bounding box centers associated with object queries;\n    \\item project these centers into all the feature maps using the camera transformation matrices;\n    \\item sample features via bilinear interpolation and incorporate them into object queries; and\n    \\item describe object interactions using multi-head attention.\n\\end{enumerate}\n\nMotivated by DETR~\\cite{detr}, each layer $\\ell\\in\\{0,\\ldots,L-1\\}$ operates on a set of \\emph{object queries} $\\Q_\\ell=\\{\\bq_{\\ell 1}, \\ldots, \\bq_{\\ell M^*}\\}\\subset\\R^{C}$, producing a new set $\\Q_{\\ell+1}$. A reference point $\\bc_{\\ell i}\\in \\R^3$ is decoded from a object query $\\bq_{\\ell i}$ as follows: \n\\begin{equation}\\label{eq:query-point}\n    \\begin{split}\n      \\bc_{\\ell i}=\\Phi^{\\mathrm{ref}}(\\bq_{\\ell i}),  \n    \\end{split} \n\\end{equation}\nwhere $\\Phi^{\\mathrm{ref}}$ is a neural network. $\\bc_{\\ell i}$ can be thought of a hypothesis for the center of the $i$-th box. Next, we acquire image features corresponding to $\\bc_{\\ell i}$ to refine and predict the final bounding box. \n%To perform 3D to 2D projection, we obtain the homogeneous coordinate $\\bc_{\\ell i}^*$ by appending a 1 to $\\bc_{\\ell i}$.\\justin{previous sentence probably not needed, you're just explaining standard math. incorporate into \\eqref{eq:point-projection}} \nThen, $\\bc_{\\ell i}$ (or more accurately its homogeneous counterpart $\\bc_{\\ell i}^*$) is projected into each one of the images using the camera transformation matrices: \n% \\justin{make sure notation is consistent---T?}\n\\begin{equation}\\label{eq:point-projection}\n    \\begin{split}\n      \\bc_{\\ell i}^* = \\bc_{\\ell i} \\oplus 1 \\qquad\\qquad \\bc_{\\ell m i} = T_m \\bc_{\\ell i}^*, \n    \\end{split} \n\\end{equation}\nwhere $\\oplus$ denotes concatenation, and $\\bc_{\\ell m i}$ is the projection of the reference point onto the $m$-th camera. To remove the effects of the feature map size and gather features across different levels, we normalize $\\bc_{\\ell m i}$ to $[-1, 1]$. \nNext, the images features are collected by\n\\begin{equation}\\label{eq:bilinear}\n    \\begin{split}\n        \\bff_{\\ell k m i}  = f^{\\mathrm{bilinear}}(\\F_{km}, \\bc_{\\ell m i}), \n    \\end{split} \n\\end{equation}\nwhere $\\bff_{\\ell k m i}$ is the feature for $i$-th point from $k$-th level of $m$-th camera at $\\ell$-th layer. \n\n\nA given reference point is not necessarily visible in all the camera images, so we need some heuristics to filter invalid points. To that end, we define a binary value $\\sigma_{\\ell k m i}$, which is determined based on whether a reference point is projected outside an image plane. The final feature $\\bff_{\\ell i}$ and object query in next layer $\\bq_{(\\ell+1) i}$ are given by \n\\begin{equation}\\label{eq:feature-agg}\n    \\begin{split}\n        \\bff_{\\ell i}  = \\frac{1}{\\sum_k\\sum_m \\sigma_{\\ell k m i}+\\epsilon}\\sum_k \\sum_m \\bff_{\\ell k m i} \\sigma_{\\ell k m i} \\qquad \\mathrm{and} \\qquad \\bq_{(\\ell+1)i} = \\bff_{\\ell i}  + \\bq_{\\ell i},\n    \\end{split} \n\\end{equation}\nwhere $\\epsilon$ is a small number to avoid division by zero. \n% \\justin{can you divide by zero if a point isn't visible in any camera image?}\\yue{I add a eps=1e-7 whenever I divide a number by zero in the code}\n% \\justin{I really would prefer if you replaced the sum with a max or an average weighted by 1/ the sum of the sigmas.  This current feature above is weird, since it changes scale depending on the number of cameras that see the point.  Please change.}\\yue{empirically i try all these variants and they don't make much different}\nFinally, for each object query $\\bq_{\\ell i}$,\n% \\justin{currently it's $\\ell$ but do you predict box parameters only in the last layer?}\\yue{during training, i use the loss for every layer to better optimize the reference point; during inference, i use the predictions from the last layer}\nwe predict a bounding box $\\hat{\\bb}_{\\ell i}$ and its categorical label $\\hat{c}_{\\ell i}$ with two neural networks $\\Phi_{\\ell}^{\\mathrm{reg}}$ and $\\Phi_{\\ell}^{\\mathrm{cls}}$:\n\\begin{equation}\\label{eq:prediction}\n    \\begin{split}\n      \\hat{\\bb}_{\\ell i} = \\Phi_{\\ell}^{\\mathrm{reg}}(\\bq_{\\ell i})  \\qquad \\mathrm{and} \\qquad   \\hat{c}_{\\ell i} = \\Phi_{\\ell}^{\\mathrm{cls}}(\\bq_{\\ell i}). \n    \\end{split} \n\\end{equation}\nWe compute the loss for the predictions $\\hat{\\B}_\\ell =\\{\\hat{\\bb}_{\\ell 1}, \\ldots, \\hat{\\bb}_{\\ell j}, \\ldots, \\hat{\\bb}_{\\ell M^\\ast}\\}\\subset\\R^9$ and $\\hat{\\C}_\\ell = \\{\\hat{c}_{\\ell 1}, \\ldots, \\hat{c}_{\\ell j}, \\ldots, \\hat{c}_{\\ell M}\\}\\subset\\mathbb{Z}$ from every layer during training. During inference, we only use the outputs from the last layer.\n% \\justin{Something seems to be missing here.  Do you define how you obtain $\\B,\\C$ used in the next section.  Make sure every step is clear here.  Also, where do you predict the \\emph{number} of boxes in the scene?}\n\n"
                },
                "subsection 3.4": {
                    "name": "Loss",
                    "content": "\n\\label{loss}\n\nFollowing~\\cite{StewartAN16,detr}, we use a set-to-set loss to measure the discrepancy between the prediction set $(\\hat{\\B}_\\ell, \\hat{\\C}_\\ell)$ and the ground-truth set $(\\B, \\C)$. This loss consists of two parts: a focal loss~\\cite{lin2017focal} for the class labels and a $L^1$ loss for the bounding box parameters.  \n% \\justin{describe in words what your loss measures here}\nFor notational convenience, we drop the $\\ell$ subscript in  $\\hat{\\B}_\\ell$ and  $\\hat{\\C}_\\ell$. \n% \\justin{why is this relevant? don't you only evaluate this in the last layer? drop the subscript from what?}.\nThe number of ground-truth boxes $M$ is typically smaller than the number of predictions $M^\\ast$, so we pad the set of ground-truth boxes with $\\varnothing$s (no object) up to $M^{\\ast}$ for ease of computation. \n% Our loss can be understood as the cost of an optimal matching between these two sets\\justin{is this right? if so how come the sums in \\eqref{eq:matching} and \\eqref{eq:sup_loss} aren't the same? not sure where the log came from. if the previous sentence isn't right, repair it to give the right intuition.}. \nWe establish a correspondence between the ground-truth and the prediction via a bipartite matching problem: $\\sigma^{\\ast} = \\argmin_{\\sigma \\in \\mathcal{P}}\\sum_{j=1}^M -1_{\\{c_j\\neq\\varnothing\\}}\\hat{p}_{\\sigma(j)}(c_j) + 1_{\\{c_j=\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(\\bb_j, \\hat{\\bb}_{\\sigma(j)}),$\n\n%An optimal bipartite matching is defined by\n% \\begin{equation}\\label{eq:matching}\n%      \\begin{split}\n%       \\sigma^{\\ast} = \\argmin_{\\sigma \\in \\mathcal{P}}\\sum_{j=1}^M -1_{\\{c_j\\neq\\varnothing\\}}\\hat{p}_{\\sigma(j)}(c_j) + 1_{\\{c_j=\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(\\bb_j, \\hat{\\bb}_{\\sigma(j)}),\n%      \\end{split}\n% \\end{equation}\nwhere $\\mathcal{P}$ denotes the set of permutations, $\\hat{p}_{\\sigma(j)}(c_j)$ is the probability of class $c_j$ for the prediction with index $\\sigma(j)$, and $\\mathcal{L}_{\\mathrm{box}}$ is the $L_1$ loss for bounding box parameters. We use the Hungarian algorithm~\\cite{Kuhn55thehungarian} to solve this assignment problem, as in~\\cite{StewartAN16,detr}, yielding the set-to-set loss $\\mathcal{L_{\\mathrm{sup}}} = \\sum_{j=1}^N -\\log\\hat{p}_{\\sigma^{\\ast}(j)}(c_j) + 1_{\\{c_j=\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(\\bb_j, \\hat{\\bb}_{\\sigma^{\\ast}(j)})$.\n% \\begin{equation}\\label{eq:sup_loss}\n%      \\begin{split}\n%       \\mathcal{L_{\\mathrm{sup}}} = \\sum_{j=1}^N -\\log\\hat{p}_{\\sigma^{\\ast}(j)}(c_j) + 1_{\\{c_j=\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(\\bb_j, \\hat{\\bb}_{\\sigma^{\\ast}(j)}).\n%      \\end{split}\n% \\end{equation}\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\nWe present our results as follows: first, we detail the dataset, metrics, and implementation in \\S\\ref{detail}; then we compare our method to existing works in \\S\\ref{sota}; we benchmark the performance of different models in camera overlap regions in \\S\\ref{overlap}; we compare to a forward prediction model in \\S\\ref{pseudo}; and we provide additional analysis and ablations in \\S\\ref{ablation}.  \n\n",
                "subsection 4.1": {
                    "name": "Implementation Details",
                    "content": "\n\\label{detail}\n\n\\textbf{Dataset.} We test our method on the nuScenes dataset~\\cite{nuscenes2019}. nuScenes consists of 1,000 sequences; each sequence is roughly 20s long, with a sampling rate of 20 frames/second. Each sample contains images from 6 cameras \\texttt{[front\\_left, front, front\\_right, back\\_left, back, back\\_right]}. Camera parameters including intrinsics and extrinsics are available. nuScenes provides annotations every 0.5s; in total there are 28k, 6k, and 6k annotated samples for training, validation, and testing, respectively. 10 from the total 23 classes are available to compute the metrics. \n\n\\textbf{Metrics.} We follow the official evaluation protocol provided by nuScenes. We evaluate average translation error (ATE), average scale error (ASE), average orientation error (AOE), average velocity error (AVE), and average attribute error (AAE). These metrics are true positive metrics (TP metrics) and computed in the physical unit. In addition, we measure mean average precision (mAP). To capture all aspects of the detection task, a consolidated scalar metric--the nuScenes Detection Score (NDS)~\\cite{nuscenes2019}--is defined as $\\mathrm{NDS} = \\frac{1}{10}[5\\mathrm{mAP}+\\sum_{\\mathrm{mTP}\\in \\mathbb{TP}}(1-\\min(1, \\mathrm{mTP}))]$.\n% \\begin{equation}\n%     \\begin{split}\n%     \\mathrm{NDS} = \\frac{1}{10}[5\\mathrm{mAP}+\\sum_{\\mathrm{mTP}\\in \\mathbb{TP}}(1-\\min(1, \\mathrm{mTP}))].\n%     \\end{split}\n% \\end{equation}\n\n\\textbf{Model.} Our model consists of a ResNet~\\cite{He2015} feature extractor, a FPN, and a \\name detection head. We use ResNet101 with deformable convolutions~\\cite{dai17dcn} in the 3rd stage and 4th stage. The FPN~\\cite{Lin2017} takes features output by the ResNet and produces 4 feature maps whose sizes are $\\nicefrac18$, $\\nicefrac1{16}$, $\\nicefrac1{32}$, and $\\nicefrac1{64}$ of the input image sizes. The \\name detection head consists of 6 layers, where each layer is a combination of a feature refinement step and a multi-head attention layer. The hidden dimension of the \\name detection head is 256. Finally, two sub-networks predict bounding box parameters and a class label per object query; each sub-network consists of two fully connected layers with hidden dimensions 256. We use LayerNorm~\\cite{BaKH16} in the detection head. \n\n\\textbf{Training \\& inference.} We use AdamW~\\cite{loshchilov2018decoupled} to train the whole pipeline. The weight decay is $10^{-4}$. We use an initial learning rate $10^{-4}$, which is decreased to $10^{-5}$ and $10^{-6}$ at 8th and 11th epochs. The model is trained for 12 epochs in total on 8 RTX 3090 GPUs and the per-GPU batch size is 1. The training procedure takes roughly 18 hours.  We do not use any post-processing such as NMS during inference. For evaluation, we use the nuScenes evalutation toolkit.\n\n"
                },
                "subsection 4.2": {
                    "name": "Comparison to Existing Works",
                    "content": "\n\\label{sota}\nWe compare to previous state-of-the-art methods CenterNet~\\cite{zhou2019centernet} and FCOS3D~\\cite{wang2021fcos3d}. CenterNet is an anchor-free 2D detection method that makes dense predictions in a high resolution feature map.  FCOS3D employs a FCOS~\\cite{tian2019fcos} pipeline to make per-pixel predictions. These methods both turn 3D object detection into a 2D problem, and in doing so ignore scene geometry and sensor configuration. \n% \\tianyuan{For multi-view images 3D object detection, they inference independently on each image, then fuse all predictions using a global nms}.  \nTo perform multi-view object detection, these methods have to process each image independently, and use both per-image and global NMS to remove redundant boxes in each view and in the overlap regions respectively. \nAs shown in Table~\\ref{table:sota}, our method outperforms these methods even though we do not use any post-processing. Our method performs worse than FCOS3D in terms of mATE. We suspect this is because FCOS3D directly predicts bounding box depth, which leads to strong supervision on object translation. \n% We suspect this is due to different parameterizations \\justin{meaning?} of bounding box centers. \nAlso, FCOS3D uses disentangled heads for different bounding box parameters, which can increase performance. \n\nOn the test set (Table~\\ref{table:sota:test}), our method outperforms all existing methods as of 10/13/2021; our method uses the same backbone as DD3D~\\cite{dd3d} for a fair comparison. \n\n\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Comparison in Overlap Regions",
                    "content": "\n\\label{overlap}\n% \\tianyuan{For multi-camera object detection, a great challenge lie in the overlapping regions, where objects more likely to be out-of-borders for each camera.} \nA great challenge lies in the overlap regions where objects are more likely to be cut off. Our method considers all cameras simultaneously, while FCOS3D predicts bounding boxes per camera individually. To further demonstrate the advantages of fused inference, we calculate the metrics for boxes falling into the camera overlaps. To compute the metrics, we select boxes whose 3D center is visible to multiple cameras. On the validation set, there are 18,147 such boxes,  9.7\\% of the total. Table~\\ref{table:overlap} shows the results; our method outperforms FCOS3D remarkably in terms of NDS scores in this setting. This confirms that our integrated prediction approach is more effective.  \n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Comparison to pseudo-LiDAR Methods",
                    "content": "\n\\label{pseudo}\nAnother way to perform 3D object detection is by generating pseudo-LiDAR point clouds from multi-view images using a depth prediction model. On the nuScenes dataset, there are no publicly available pseudo-LiDAR works for us to make a direct comparison. Hence, we implement a baseline ourselves to verify that our approach is more effective than explicit depth prediction. We use a pre-trained PackNet~\\cite{packnet} network to predict dense depth maps from all six cameras and then convert these depth maps into point clouds using the camera transformations. We also experimented with a self-supervised PackNet model with velocity supervision (as in the original paper), but we found that ground-truth depth supervision yielded more realistic point clouds and therefore used a supervised model as baseline. \nFor 3D detection, we employ the recently-proposed CenterPoint architecture~\\cite{yin2021center}. Conceptually, this pipeline is a variant of pseudo-LiDAR~\\cite{wang2019pseudo}. Table~\\ref{pseudolidar} shows the results; we conclude that this pseudo-LiDAR method underperforms ours significantly even when depth estimates are generated by a state-of-the-art model. One possible explanation is that pseudo-LiDAR object detectors suffer from compounding errors introduced by inaccurate depth prediction, that in turn is known to overfit to training data and generalizes poorly to other distributions~\\cite{demistifying}. %, while our approach does not use depth at all.%<--- debatable  \n% \\vitor{Are there any published Pseudo-LiDAR methods that we can also use as comparison? It would make this part more convincing, right now feels like we just came up with something to compare and outperformed it, there is no weight to that}\n\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Ablation \\& Analysis",
                    "content": "\n\\label{ablation}\n\nWe provide a visualization of object query refinement in Figure~\\ref{fig:iter-refine}. We visualize bounding boxes decoded from the object queries in each layer. The predicted bounding boxes get closer to the ground-truth as we go into deeper layers in the model.Also, the leftmost figure shows the learned object query priors shared by all data.  We also provide quantitative results in Table~\\ref{tab:ablation}, which shows that iterative refinement indeed improves performance significantly. This suggests that iterative refinement is both beneficial and necessary to fully leverage our proposed architecture. \\changed{Furthermore, we provide ablations on the number of object queries in Table~\\ref{tab:queries}; increasing the number queries consistently improves the performance until it gets saturated at 900. Finally, Table~\\ref{tab:backbone} shows the results with different backbones.}\n\n\n\n\n\n\n\n\n\nWe also provide qualitative results in Figure~\\ref{fig:prediction} to facilitate an intuitive understanding of model performance. We project the predicted bounding boxes into 6 cameras as well as a BEV perspective. In general, our method generates reasonable results and even detects relatively small objects. However, our method still exhibits substantial translation error (in line with results in Table~\\ref{sota}):  Although our model avoids explicit depth prediction, depth estimation is still a core challenging in this problem.  \n\n% \\begin{comment}\n\n% \\end{comment}\n\n%  Moreover, Table~\\ref{sota} shows our method exhibits high translation error, which we suspect is due to the reference point parameterization; seeking better reference point parameterization can potentially increase the performance. \n\n% 1. Backbone of different sizes\n% 2. Level, cam embedding\n\n\n% Better Scene Prior: evaluate\n% \\begin{itemize}\n%     \\item Overlapping objects\n%     \\item What if one camera fails (low priority)\n% \\end{itemize}\n\n\n\n\n\n% \\subsection{Visualization}\n% Visualize Ours backward\n% \\begin{itemize}\n%     \\item Reference points in 3d Scenes.  iterative refine process\n%     \\item Visualization feature fuse process (mask)\n% \\end{itemize}\n\n% Visualization of Forward methods\n\n% \\begin{itemize}\n%     \\item Lift all pixels.\n% \\end{itemize}\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\n% \\justin{conclusions that are copy-pasted abstracts aren't useful. shorten the parts that are repeating advantages you've already stated a bunch of times, and instead focus on ``lessons learned'' and speculate on future research.}\n% We present the first multi-camera 3D object detection pipeline, which is a completely new paradigm to predict 3D bounding boxes from 2D observations. Our pipeline proceeds from the sparse set of object queries, eliminating the necessity of accurate depth perception. In addition, our model is post-processing free, improving inference efficiency. \n\n% \\begin{comment}\nWe propose a new paradigm to address the ill-posed inverse problem of recovering 3D information from 2D images. In this setting, the input signal lacks essential information for models to make effective predictions without priors learned from data. While other methods either operate solely on 2D computations or use additional depth networks to reconstruct the scene, ours operates in 3D space and uses backward projection to retrieve image features as needed. The benefits of our approach are two-fold: (1) it eliminates the need for middle-level representations (e.g., predicted depth maps or point clouds), which can be a source of compounding errors; and (2) it uses information from multiple cameras by projecting the same 3D point onto all available frames.\n\nBeyond the direct application of our work to 3D object detection for autonomous driving, there are several venues that warrant future investigation. For example, single point projection creates a limited receptive field in the retrieved image feature maps, and sampling multiple points for each object query would incorporate more information for object refinement. Furthermore, the new detection head is input-agnostic, and including other modalities such as LiDAR/RADAR would enhance performance and robustness. Finally, generalizing our pipeline to other domains\nsuch as indoor navigation and object manipulation would increase its scope of application and reveal additional ways for further improvement. \n% \\end{comment}\n\n% Our method tackles an ill-posed inverse problem in which the input signal lacks essential information for models to make effective predictions without priors learned from data. Our performance gains for this task are facilitated by two key observations: (1) selecting the appropriate space \\justin{what space? maybe rephrase this sentence to be simpler and more clear} is crucially important and greatly affects the model performance; (2) an end-to-end approach without unnecessary middle-level representations (e.g., predicted depth maps and point clouds) can reduce potential to introduce compounding errors. Our work suggests a new paradigm for multi-view object detection and sheds light into other robotics perception applications more broadly.\n\n% Beyond the direct application of our work to autonomous driving, there are several venues for future investigation. For example, the single reference point projection gives rise to a limited receptive field in the image feature map. Sampling multiple points for an object query can incorporate more features into object refinement. Furthermore, the new detection head is input-agnostic. Including other modalities such as LiDAR/RADAR point clouds will enhance performance and robustness.Finally, generalizing our pipeline to other domains \\justin{such as XYZ} will increase its scope of application and reveal additional ways in which it can be refined. %===============================================================================\n\n\\clearpage\n% no \\bibliographystyle is required, since the corl style is automatically used.\n\\bibliography{corl}  % .bib\n\n% \\clearpage\n\n% \\input{section/supplement}\n\n"
            }
        },
        "equations": {
            "eq:eq:query-point": "\\begin{equation}\\label{eq:query-point}\n    \\begin{split}\n      \\bc_{\\ell i}=\\Phi^{\\mathrm{ref}}(\\bq_{\\ell i}),  \n    \\end{split} \n\\end{equation}",
            "eq:eq:point-projection": "\\begin{equation}\\label{eq:point-projection}\n    \\begin{split}\n      \\bc_{\\ell i}^* = \\bc_{\\ell i} \\oplus 1 \\qquad\\qquad \\bc_{\\ell m i} = T_m \\bc_{\\ell i}^*, \n    \\end{split} \n\\end{equation}",
            "eq:eq:bilinear": "\\begin{equation}\\label{eq:bilinear}\n    \\begin{split}\n        \\bff_{\\ell k m i}  = f^{\\mathrm{bilinear}}(\\F_{km}, \\bc_{\\ell m i}), \n    \\end{split} \n\\end{equation}",
            "eq:eq:feature-agg": "\\begin{equation}\\label{eq:feature-agg}\n    \\begin{split}\n        \\bff_{\\ell i}  = \\frac{1}{\\sum_k\\sum_m \\sigma_{\\ell k m i}+\\epsilon}\\sum_k \\sum_m \\bff_{\\ell k m i} \\sigma_{\\ell k m i} \\qquad \\mathrm{and} \\qquad \\bq_{(\\ell+1)i} = \\bff_{\\ell i}  + \\bq_{\\ell i},\n    \\end{split} \n\\end{equation}",
            "eq:eq:prediction": "\\begin{equation}\\label{eq:prediction}\n    \\begin{split}\n      \\hat{\\bb}_{\\ell i} = \\Phi_{\\ell}^{\\mathrm{reg}}(\\bq_{\\ell i})  \\qquad \\mathrm{and} \\qquad   \\hat{c}_{\\ell i} = \\Phi_{\\ell}^{\\mathrm{cls}}(\\bq_{\\ell i}). \n    \\end{split} \n\\end{equation}"
        },
        "git_link": "https://github.com/WangYueFt/detr3d"
    }
}