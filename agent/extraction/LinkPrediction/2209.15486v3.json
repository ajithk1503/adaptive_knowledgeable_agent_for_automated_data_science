{
    "meta_info": {
        "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
        "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple\nheuristics on Link Prediction (LP) tasks. This is due to limitations in\nexpressive power such as the inability to count triangles (the backbone of most\nLP heuristics) and because they can not distinguish automorphic nodes (those\nhaving identical structural roles). Both expressiveness issues can be\nalleviated by learning link (rather than node) representations and\nincorporating structural features such as triangle counts. Since explicit link\nrepresentations are often prohibitively expensive, recent works resorted to\nsubgraph-based methods, which have achieved state-of-the-art performance for\nLP, but suffer from poor efficiency due to high levels of redundancy between\nsubgraphs. We analyze the components of subgraph GNN (SGNN) methods for link\nprediction. Based on our analysis, we propose a novel full-graph GNN called\nELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as\nmessages to approximate the key components of SGNNs without explicit subgraph\nconstruction. ELPH is provably more expressive than Message Passing GNNs\n(MPNNs). It outperforms existing SGNN models on many standard LP benchmarks\nwhile being orders of magnitude faster. However, it shares the common GNN\nlimitation that it is only efficient when the dataset fits in GPU memory.\nAccordingly, we develop a highly scalable model, called BUDDY, which uses\nfeature precomputation to circumvent this limitation without sacrificing\npredictive performance. Our experiments show that BUDDY also outperforms SGNNs\non standard LP benchmarks while being highly scalable and faster than ELPH.",
        "author": "Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca, Thomas Markovich, Nils Hammerla, Michael M. Bronstein, Max Hansmire",
        "link": "http://arxiv.org/abs/2209.15486v3",
        "category": [
            "cs.LG",
            "cs.IR"
        ],
        "additionl_info": "29 pages, 19 figures, 6 appendices"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n% intro to link prediction\nLink Prediction (LP) is an important problem in graph ML with many industrial applications. For example, recommender systems can be formulated as LP; link prediction is also a key process in drug discovery and knowledge graph construction.\n% three methods for link prediction\nThere are three main classes of LP methods: (i) {\\bf heuristics} (See Appendix~\\ref{app:heuristics}) that estimate the distance between two nodes (e.g. personalized page rank (PPR)~\\citep{page1999pagerank} or graph distance~\\citep{zhou2009predicting}) or the similarity of their neighborhoods (e.g Common Neighbors (CN), Adamic-Adar (AA)~\\citep{adamic2003friends}, or Resource Allocation (RA)~\\citep{zhou2009predicting}); (ii) {\\bf unsupervised node embeddings} or {\\bf factorization} methods, which encompass the majority of production recommendation systems~\\citep{koren2009matrix, chamberlain2020tuning}; and, recently, (iii) \\textbf{Graph Neural Networks}, in particular of the Message-Passing type (MPNNs)~\\citep{Gilmer2017,kipf2017, Hamilton2017}.\\footnote{GNNs are a broader category than MPNNs. Since the majority of GNNs used in practice are of the message passing type, we will use the terms synonymously.}\nGNNs excel in graph- and node-level tasks, but often fail to outperform node embeddings or heuristics on common LP benchmarks such as the Open Graph Benchmark (OGB)~\\citep{hu2020open}. \n\n% problems with GNN link prediction\nThere are two related reasons why MPNNs tend to be poor link predictors.\n% triangle counting\nFirstly, due to the equivalence of message passing to the Weisfeiler-Leman (WL) graph isomorphism test~\\citep{xu2018how,morris2019weisfeiler}, standard MPNNs are provably incapable of counting triangles~\\citep{chen2020can} and consequently of counting Common Neighbors or computing one-hop or two-hop LP heuristics such as AA or RA. \n% the isomorphic node problem\nSecondly, GNN-based LP approaches combine permutation-equivariant structural node representations (obtained by message passing on the graph) and a readout function that maps from two node representations to a link probability.\nHowever, generating link representations as a function of equivariant node representations encounters the problem that all nodes $u$ in the same orbit induced by the graph automorphism group have equal representations. Therefore,  the link probability\n%Therefore, the link probability \n$p(u,v)$ is the same for all $u$ in the orbit independent of e.g. the graph distance $d(u,v)$ (Figure \\ref{fig:seal_iso_graph}).\n%suffers from %severe issues with \n%an expressiveness issue first described by\n~\\citep{srinivasan2019equivalence}. \n%\n%GNNs generate node representations by propagating features from neighbor nodes\n%subgraphs \n%\n\n%\nThis is a result of GNN's built-in permutation equivariance, which produces equal %structural\nrepresentations for any nodes whose enclosing subgraphs (corresponding to the receptive field of the GNN) are isomorphic.\\footnote{More precisely, WL-equivalent, which is a necessary but insufficient condition for isomorphism. }   \n%\nWe refer to this phenomenon as the \\textit{automorphic node problem} and define \\textit{automorphic nodes} (denoted $u \\cong v$) to be those nodes that are indistinguishable by means of a given $k$-layer GNN.  \n%\nOn the other hand, \ntransductive node embedding methods such as TransE~\\citep{bordes2013translating} and DeepWalk~\\citep{perozzi2014deepwalk}, or matrix factorization~\\citep{koren2009matrix} do not suffer from this problem as the embeddings are not permutation equivariant. \n\n% fixing GNNs with labeling tricks.\nSeveral methods have been proposed to improve GNN expressivity for LP. Most simply, adding unique node IDs immediately makes all structural node representations distinguishable, but at the expense of generalization~\\citep{abboud2021surprising} and training convergence~\\citep{sato2021random}. Substructure counts may act as permutation-equivariant approximately unique identifiers~\\citep{bouritsas2022improving}, but they require a precomputation step which may be computationally intractable in the worst case. More successfully, a family of structural features, sometimes referred to as \\textit{labeling tricks} have recently been proposed that solve the automorphic node problem while still being equivariant and having good generalization~\\citep{li2020distance, zhang2021labeling, you2021identity}. However, adding structural features amounts to computing structural node representations that are conditioned on an edge and so can no longer be efficiently computed in parallel. \n% subgraph methods\nFor the purpose of tractability, state-of-the-art methods for LP restrict computation to subgraphs enclosing a link, transforming link prediction into {\\em binary subgraph classification}~\\citep{zhang2021labeling, zhang2018link, yin2022algorithm}. Subgraph GNNs (SGNN) are inspired by the strong performance of LP heuristics compared to more sophisticated techniques and are motivated as an attempt to learn data-driven LP heuristics. \n\n% the limitations of subgraph methods\nDespite impressive performance on benchmark datasets, SGNNs suffer from some serious limitations: (i) Constructing the subgraphs is expensive; \n% This problem is so severe that the authors of \\citep{zhang2021labeling} could only approximate some OGB benchmarks. \n(ii) Subgraphs are irregular and so batching them is inefficient on GPUs (iii); Each step of inference is almost as expensive as each training step because subgraphs must be constructed for every test link. These drawbacks preclude many applications, where scalability or efficient inference are required. \n% why SEAL doesn't work for recs\n% Specifically, most large scale recommender systems rely on a node embeddings in a metric space plus an ANN to serve recommendations, but GNN based link predictors generate edge-wise and not node-wise embeddings. For this reason there is no efficient way to look up recommendation candidates using subgraph link prediction methods.\n\n% what we do / contributions\n\\paragraph*{Main contributions.} \n(i) We analyze the relative contributions of SGNN components and reveal which properties of the subgraphs are salient to the LP problem.\n(ii) Based on our analysis, we develop an MPNN (ELPH) that passes subgraph sketches as messages. The sketches allow the most important qualities of the subgraphs to be summarized in the nodes. The resulting model removes the need for explicit subgraph construction and is a full-graph MPNN with the similar complexity to GCN. \n(iii) We prove that ELPH is strictly more expressive than MPNNs for LP and that it solves the automorphic node problem.\n(iv) As full-graph GNNs suffer from scalability issues when the data exceeds GPU memory, we develop BUDDY,  a highly scalable model that precomputes sketches and node features.\n(v) We provide an open source Pytorch library for (sub)graph sketching that generates data sketches via message passing on the GPU. \n% In this paper, we first analyze the computational complexity of the components of SGNNs and their contribution to the predictive performance in LP problems. We find that augmenting the graph with structural features is vital for GNN-based link prediction, but that propagating structural features over subgraphs is often detrimental. We find that a major computational bottleneck is in constructing structural features that are conditioned on edges. To address this, we show a method for constructing structural features based on data sketches that has better computational complexity. Furthermore, while it is necessary to propagate the given node features, this can be done globally as a preprocessing step, without constructing subgraphs. The graph level readout function can also be formulated just as effectively as an edge level readout function. Together, these results allow us to develop a GNN-based link prediction method that is subgraph-free while at the same time benefiting from high generalization and expressiveness. \n%\nExperimental evaluation shows that our methods compares favorably to state-of-the-art both in terms of accuracy and speed. \n\n% hash based labeling scheme\n% One of our major contributions is the development of a new labeling scheme for GNN based link prediction. While previous methods require the construction of subgraphs for each link under consideration (an $O(n^2)$ operation) we encode the labeling scheme at the node level using $O(n)$ space complexity with inference time equivalent to a multilayer perceptron.\n\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": " \\label{sec:preliminaries}\n\n\\paragraph{Notation.} Let $G=(\\mathcal{V},\\mathcal{E})$ be an undirected graph comprising the set of $n$ nodes (vertices) $\\mathcal{V}$ and $e$ links (edges) $\\mathcal{E}$. \n% \nWe denote by $d(u,v)$ the {\\em geodesic distance} (shortest walk length) between nodes $u$ and $v$. \n%\nLet $S = (\\mathcal{V}_S \\subseteq \\mathcal{V} ,\\mathcal{E}_S \\subseteq \\mathcal{E})$ be a node-induced subgraph  of $G$ \nsatisfying $(u,v) \\in \\mathcal{E}_S $ iff $(u,v) \\in \\mathcal{E}$ for any $u,v \\in \\mathcal{V}_S$. \n%\nWe denote by \n$S^k_{uv}=(\\mathcal{V}_{uv},\\mathcal{E}_{uv})$ a $k$-hop subgraph enclosing the link $(u,v)$, where $\\mathcal{V}_{uv}$ is the union of the $k$-hop neighbors of $u$ and $v$ and $\\mathcal{E}_{uv}$ is the union of the links that can be reached by a $k$-hop walk originating at $u$ and $v$ (for simplicity, where possible, we omit $k$). \n%\nSimilarly, $S^k_{u}$ is the $k$-hop subgraph enclosing node $u$. \n%\nThe given features of nodes $\\mathcal{V}_{uv}$ are denoted by $\\mathbf{X}_{uv}$ and the derived structure features by $\\mathbf{Z}_{uv}$. The probability of a link $(u,v)$ is denoted by $p(u,v)$. When nodes $u$ and $v$ have isomorphic enclosing subgraphs (i.e., $S_u \\cong S_v$), we write $u \\cong v$. \n\n\\paragraph{Sketches for Intersection Estimation.} \\label{sec:sketches_main}\nWe use two sketching techniques, \\textit{HyperLogLog}~\\citep{hyperloglog,heule2013hyperloglog} and \\textit{MinHashing}~\\citep{broder1997resemblance}. \nGiven sets $\\mathcal{A}$ and $\\mathcal{B}$, \\textit{HyperLogLog} efficiently estimates the cardinality of the union $|\\mathcal{A} \\cup \\mathcal{B}|$ and \\textit{MinHashing} estimates the Jaccard index $J(\\mathcal{A},\\mathcal{B}) = |\\mathcal{A} \\cap \\mathcal{B}| / |\\mathcal{A} \\cup \\mathcal{B}|$. We combine these approaches to estimate the intersection of node sets produced by graph traversals ~\\citep{hyperloglogminhash}. \n%\nThese techniques represent sets as sketches, where the sketches are much smaller than the sets they represent. Each technique has a parameter $p$ controlling the trade-off between the accuracy and computational cost. Running times for merging two sets, adding an element to a set, and extracting estimates only depend on $p$, but they are constant with respect to the size of the set. Importantly, the sketches of the union of sets are given by permutation-invariant operations (element-wise $\\min$ for minhash and element-wise $\\max$ for hyperloglog).\nMore details are provided in Appendix~\\ref{sec:sketches}.\n% hyperloglog\n\n\\paragraph{Graph Neural Networks for Link Prediction.}\n\nMessage-passing GNNs (MPNNs) are parametric functions of the form $\\mathbf{Y} = \\GNN(\\mathbf{X})$, where $\\mathbf{X}$ and $\\mathbf{Y}$ are matrix representations (of size $n\\times d$ and $n\\times d'$, where $n$ is the number of nodes and $d,d'$ are the input and output  dimensions, respectively) of input and output node features. \n%\n{\\em Permutation equivariance} implies that $\\boldsymbol{\\Pi}\\GNN(\\mathbf{X}) = \\GNN(\\boldsymbol{\\Pi}\\mathbf{X})$ for any $n\\times n$ node permutation matrix $\\boldsymbol{\\Pi}$. \n%\n%learn equivariant structural node representations that satisfy $\\Pi \\left(\\GNN(X)\\right) = \\GNN(\\Pi(X))$ where $\\Pi$ is a permutation operator. \n%\n% In GNNs, this is achieved by message passing, applying a local permutation-invariant aggregation function to the neighbor features. %\nThis is achieved in GNNs by applying a local permutation-invariant aggregation function $\\square$ (typically sum, mean, or max) to the neighbor features of every node (`message passing'), resulting in a node-wise update of the form\n\\begin{equation} \\label{eq:mpnn_link_prediction}\n%\\mathbf{x}_u^{(l)} &=\\gamma^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\square_{v \\in \\mathcal{N}(u)} \\phi^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\mathbf{x}_v^{(l-1)}\\right)\\right) \\\\\n%p(u,v) &= \\psi \\left(\\mathbf{x}^k_u \\odot \\mathbf{x}^k_v \\right)\n\\mathbf{y}_u =\\gamma\\left(\\mathbf{x}_u, \\square_{v \\in \\mathcal{N}(u)} \\phi\\left(\\mathbf{x}_u, \\mathbf{x}_v\\right)\\right), \n%\\\\\n%p(u,v) &= \\psi \\left(\\mathbf{x}^k_u \\odot \\mathbf{x}^k_v \\right)\n\\end{equation}\n%\nwhere $\\phi, \\gamma$ are learnable functions. \n%\nMPNNs are upperbounded in their discriminative power by the Weisfeiler-Leman isomorphism test (WL) \\citep{weisfeiler1968reduction}, a procedure iteratively refining the representation of a node by hashing its star-shaped neighborhood. As a consequence, since WL always identically represents automorphic nodes ($u \\cong v$), any MPNN would do the same: $\\mathbf{y}_u = \\mathbf{y}_v$.\n%\n%\n%Given the node representations $\\mathbf{Y}$ computed by a GNN, \n% link probabilities can then be computed as $p_{uv} = R(\\mathbf{y}_u,\\mathbf{y}_v)$, where $R$ is a learnable readout function.  \n%\n%\n%$\\mathbf{x}_u = \\mathbf{x}_v \\forall \\, u \\cong v$\\footnote{The situation is slightly worse in practice as there are cases where GNNs can not distinguish non-isomorphic subgraphs}, where $\\mathbf{x}'$ are the node representations output by a GNN. \n%\n%For link prediction, \n%A GNN will then produce \n%\n%\n%\n%For this property to hold, one must have $\\mathbf{y}_u = \\mathb  f{y}_v \\, \\forall \\, u\\cong v$. \n%\nGiven the node representations $\\mathbf{Y}$ computed by a GNN, \n link probabilities can then be computed as $p(u, v) = R(\\mathbf{y}_u,\\mathbf{y}_v)$, where $R$ is a learnable readout function with the property that $R(\\mathbf{y}_u,\\mathbf{y}_v) = \n R(\\mathbf{y}_u,\\mathbf{y}_w)\n $ \n for any $v\\cong w$. \n %$R(\\mathbf{x}_u,\\mathbf{x}_i) = R(\\mathbf{x}_u,\\mathbf{x}_j) \\forall i \\cong j$. \n This node automorphism problem is detrimental for LP as $p(u,v)=p(u,w)$ if $v \\cong w$ even when $d(u,v) \\gg d(u,w)$. As an example, in Figure\\ref{fig:seal_iso_graph} $2 \\cong 4$, therefore $p(1,2)=p(1,4)$ while $d(1,2)=2 < d(1,4)=3$. As a result, a GNN may suggest to link totally unrelated nodes ($v$ may even be in a separate connected component to $u$ and $w$, but still have equal probability of connecting to $u$~\\citep{srinivasan2019equivalence}). \n\n% Node embedding models (e.g. Deepwalk~\\citep{perozzi2014deepwalk}) are not equivariant as equivariance is broken (usually) by some random process that causes isomorphic nodes to acquire different representations. However, the expectation over the random process is once again equivariant and so node embeddings can be thought of as samples from structural (equivariant) distributions.\n\n\n% intro to subgraph methods\n\\paragraph*{Subgraph GNNs (SGNN).}~\\citep{zhang2021labeling, zhang2018link, yin2022algorithm} convert LP into binary graph classification. For a pair of nodes $u,v$ and the enclosing subgraph $S_{uv}$, SGNNs produce node representations $\\vec{Y}_{uv}$ and one desires $R(\\vec{Y}_{uv})=1$ if $(u,v) \\in \\mathcal{E}$ and zero otherwise. \n%\nIn order to resolve the automorphic node problem, node features are augmented with structural features \\citep{bouritsas2022improving} that improve the ability of networks to count substructures \\citep{chen2020can}. % and resolve .\n% motivation of subgraph methods\nSGNNs were originally motivated by the strong performance of heuristics on benchmark datasets and attempted to learn generalized heuristics. When the graph is large it is not tractable to learn heuristics over the full graph, but global heuristics can be well approximated from subgraphs that are augmented with structural features with an approximation error that decays exponentially with the number of hops taken to construct the subgraph~\\citep{zhang2018link}.\n\n\n\n"
            },
            "section 3": {
                "name": "Analyzing Subgraph Methods for Link Prediction",
                "content": "\n\\label{sec:analysis}\n\n SGNNs can be decomposed into the following steps: (i) subgraph extraction around every pair of nodes for which one desires to perform LP; (ii) augmentation of the subgraph nodes with structure features; (iii) feature propagation over the subgraphs using a GNN, and (iv) learning a graph-level readout function to predict the link. \n Steps (ii)--(iv) rely on the existence of a set of subgraphs (i), which is either constructed on the fly or as a preprocessing step.\n%\n% preview of this section\nIn the remainder of this section we discuss the inherent complexity of each of these steps and perform ablation studies with the goal of understanding the relative importance of each. % We start by discussing the construction of the subgraphs themselves.\n\n% \\paragraph{Subgraph Generation}\n% \\label{sec:subgraph_generation}\n\n\n\n% For regular graphs with degree $\\degree$, subgraph generation has $\\mathcal{O}(\\degree^k)$ time complexity. However, for complex networks with power law degree distributions this becomes $\\mathcal{O}(|\\mathcal{E}|)$ (See Appendix~\\ref{sec:subgraph_complexity}). \n% % precomputation\n% Subgraphs can be pre-computed, but due to high levels of redundancy the subgraph dataset is much larger ($\\degree^k$ times larger for regular graphs) than the original dataset and for moderate sized datasets may exceed available memory.\n% % subsampling\n% Time and space complexity can be reduced by sampling subgraphs by fixing the number of neighbors in each hop, or restricting the number of hops to one, but both cases reduce expressiveness and invalidate the $\\gamma-$decaying heuristic theory of~\\citep{zhang2018link}.\n\n\\paragraph{Structure Features}\n\\label{sec:structure_features}\n\n\n\n\n% intro to structure features\nStructure features address limitations in GNN expressivity stemming from the inherent inability of message passing to distinguish automorphic nodes. In SGNNs, permutation-equivariant distances $d(u,i)$ and $d(v,i) \\, \\forall i \\in \\mathcal{V}_{uv}$ are used. The three most well known are Zero-One (ZO) encoding~\\citep{you2021identity}, Double Radius Node Labeling (DRNL)~\\citep{zhang2018link} and Distance Encoding (DE)~\\citep{li2020distance}.\n% discussion of labeling schemes\nTo solve the automorphic node problem, all that is required is to distinguish $u$ and $v$ from $\\mathcal{V}_{uv} \\setminus \\{u,v\\}$, which ZO achieves with binary node labels. \n%\nDRNL has $z_{u}=z_{v}=1$ and $z_j=f(d(u,j),d(v,j))>1$, where $f: \\mathbb{N}^2 \\to \\mathbb{N}$ is a bijective map. Distance Encoding (DE) generalizes DRNL; each node is encoded with a tuple $z_j = (d(u,j),d(v, j))$ (See Figure~\\ref{fig:DE}). Both of these schemes therefore include a unique label for triangles / common neighbors (See Appendix~\\ref{app:labeling_schemes} for more details on labeling schemes). The relative performance of ZO, DRNL, DE and no structure features is shown in Figure~\\ref{fig:lab_abl}. The use of structure features greatly improves performance and DRNL and DE slightly outperform ZO, with a more pronounced outperformance for the larger Pubmed dataset. \n% downsides of structure features\nIt is important to note that structure features are conditioned on edges and so can not be easily paralellized in the same way that node features usually are in GNNs and must be calculated for each link at both training and inference time. In particular DRNL requires two full traversals of each subgraph, which for regular graphs is $\\mathcal{O}(\\degree^k)$, but for complex networks becomes $O(|\\mathcal{E}|)$.\n\n% ablation of structure features\n% In Figure~\\ref{fig:feature_importance} we investigate the relative importance of DRNL structure features. Feature importance is measured using the weights in a logistic regression link prediction model that uses only structure feature counts as inputs. The Figure indicates that most of the predictive performance is concentrated in low distance DRNL labels. The importance assigned to higher distances (not shown) was negligible.    \n\nIn Figure~\\ref{fig:feature_importance} we investigate the relative importance of DRNL structure features. Feature importance is measured using the weights in a logistic regression link prediction model with only structure feature counts as inputs. We then sum up feature importances corresponding to different max distance $r$ and normalize the total sum to one. The Figure indicates that most of the predictive performance is concentrated in low distances.\n\n% \\citet{zhang2021labeling} define two labeling trick axioms to be equivariance and discrimination of the two target nodes. Valid labeling tricks include: zero-one, the simplest labeling trick, where  $l_{u}=l_{v}=1 , l_j=0 \\,\\, \\forall j \\notin (u,v)$. \n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=0.7\\textwidth]{labeling_ablation.pdf}\n%     \\caption{Adding structure features to a subgraph GNNs.}\n%     \\label{fig:lab_abl}\n%     % \\vspace{-5mm}\n% \\end{figure}\n\n\n\\paragraph{Propagation / GNN}\n\\label{sec:sg_gnn}\n\n% Node features and structure features are concatenated and taken as the input to a GNN operating on $S_{uv}$, which outputs a node embeddings for $\\mathcal{V}_{uv}$.\n% how structure features are used\nIn SGNNs, structure features are usually embedded into a continuous space, concatenated to any node features\n and propagated over subgraphs. While this procedure is necessary for ZO encodings, it is less clear why labels that precisely encode distances and are directly comparable as distances should be embedded in this way. \n% GNN for structure features\nInstead of passing embedded DE or DRNL structure features through a GNN, we fixed the number of DE features by setting the max distance to three and trained an MLP directly on the counts of these nine features (e.g. (1,1): 3, (1,2): 1, etc.). Figure~\\ref{fig:structure_prop} shows that doing so, while leaving ceteris paribus (node features still propagated over the subgraph) actually improves performance in two out of three datasets.\n% GNN for node features\nWe also investigated if any features require SGNN propagation by passing both raw node features and structure feature counts through an MLP. The results in the right columns of Figure~\\ref{fig:feature_prop} indicate that this reduces performance severely, but by pre-propagating the features as $\\mathbf{x}'_u = 1/|N(u)|\\sum_{i \\in N(u)}\\mathbf{x}_i$ (middle columns) it is possible to almost recover the performance of propagation with the SGNN (left columns). \n\n% and show through an ablation study that they can be replaced by a single linear layer (see Section~\\ref{sec:ablations}). Furthermore, we use this linear architecture to study the importance of the various labels induced by the DRNL node labeling scheme, which is known to be the most performant labeling scheme~\\citep{zhang2021labeling}. We find that only the shortest distances contribute meaningfully to the predictive performance (see Section~\\ref{sec:feature_importance}). \n\n\n\n\n\n\n\n\\paragraph{Readout / Pooling Function}\n\\label{sec:sg_readout}\nGiven SGNN node representations  $\\vec{Y}_ {uv}$ on the subgraph, a readout function $R(S_{uv},\\vec{Y}_{uv})$ maps a representations to link probabilities. For graph classification problems, this is most commonly done by pooling node representations (graph pooling) typically with a mean or sum operation plus an MLP. For LP, an alternative is edge pooling with $R(\\mathbf{y}_u,\\mathbf{y}_v)$, usually with the Hadamard product. A major advantage of edge pooling is that it can be formulated subgraph free. Figure~\\ref{fig:readout_ablation} indicates that edge pooling produces better predictive performance than either mean or sum pooling across all nodes in $\\mathcal{V}_{uv}$.\n\n% \\begin{wrapfigure}{r}{0.5\\textwidth}\n%     \\vspace{-8mm}\n%     \\centering\n%     \\includegraphics[width=0.5\\textwidth]{readout_ablation.pdf}\n%     \\caption{The affect of a GNN readout function over the output of all nodes in the subgraph $S_{uv}$ (sum or mean) or just the nodes $u$ and $v$ (edge).}\n%     \\label{fig:readout_ablation}\n%     \\vspace{-5mm}\n% \\end{wrapfigure}\n\n\\paragraph{Analysis Summary}\nThe main results of Section~\\ref{sec:analysis} are that (i) The inclusion of structure features leads to very large improvements across all datasets (Figure~\\ref{fig:lab_abl}); (ii) The processing of these features, by embedding them and propagating them with an SGNN is sub-optimal both in terms of efficiency and performance (Figure~\\ref{fig:structure_prop}); (iii) Most of the importance of the structure features is located in the lowest distances (Figure~\\ref{fig:feature_importance}); and (iv) edge level readout functions greatly outperform mean or sum pooling over subgraphs (Figure~\\ref{fig:readout_ablation}).\n%\nIf, on one hand, subgraphs are employed as a tractable alternative to the full graph for each training edge, on the other, generating them remains an expensive operation ($\\mathcal{O}(\\degree^k)$ time complexity for regular graphs and $\\mathcal{O}(|\\mathcal{E}|)$ for complex networks with power law degree distributions \\footnote{Subgraphs can be pre-computed, but the subgraphs combined are much larger than the original dataset, exceeding available memory for even moderately-sized datasets.}, see Appendix~\\ref{sec:subgraph_complexity}). Within this context, our analysis shows that if the information necessary to compute structure features for an edge can be encoded in the nodes, then it is possible to recover the predictive performance of SGNNs without the cost of generating a different subgraph for each edge. We build upon this observation to design an efficient yet expressive model in Section~\\ref{sec:link_prediction_with_subgraph_sketching}.\n\n% Subgraphs can be pre-computed, but the subgraphs are much larger than the original dataset, exceeding available memory for even moderately-sized datasets. Time and space complexity can be reduced by fixing the number of neighbors in each hop, or restricting the number of hops to one, but this invalidates the $\\gamma-$decaying heuristic theory~\\citep{zhang2018link}.\n"
            },
            "section 4": {
                "name": "Link Prediction with Subgraph Sketching",
                "content": "\n\\label{sec:link_prediction_with_subgraph_sketching}\nWe now develop a full-graph GNN model that uses node-wise subgraph sketches to approximate structure features such as the counts of DE and DRNL labels, which our analysis indicated are sufficient to encompass the salient patterns governing the existence of a link (Figure~\\ref{fig:structure_prop}).\n\n% \\subsection{Structure Features with Subgraph Sketches} \n\n% relationship between DE labels and neighbourhood intersections\n% Figure~\\ref{fig:DE} depicts DE node labels for an edge $(u,v)$ and the corresponding counts vector. \n\n",
                "subsection 4.1": {
                    "name": "Structure Features Counts",
                    "content": "\n\\label{sec:approximating_structure_features}\nLet $\\mathcal{A}_{uv}[d_u, d_v]$ be the number of ($d_u$, $d_v$) labels for the link ($u$, $v$), which is equivalent to the number of nodes at distances exactly $d_u$ and $d_v$ from $u$ and $v$ respectively (See Figure~\\ref{fig:DE}). \nWe compute $\\mathcal{A}_{uv}[d_u, d_v]$ for all $d_u$, $d_v$ less than the receptive field $k$, which guarantees a number of counts that do not depend on the graph size and mitigates overfitting. To alleviate the loss of information coming from a fixed $k$, we also compute $\\mathcal{B}_{uv}[d] = \\textstyle \\sum_{d_v=k+1}^{\\infty}\\mathcal{A}_{uv}[d, d_v]$, counting the number of nodes at distance $d$ from $u$ and at distance $>k$ from $v$. \nWe compute $\\mathcal{B}_{uv}[d]$ for all $1 \\leq d \\leq k$. Figure \\ref{fig:intersections} shows how $\\mathcal{A}$ and $\\mathcal{B}$ relate to the neighborhoods of the two nodes. \nOverall, this results in $k^2$ counts for $\\mathcal{A}$ and $2k$ counts for $\\mathcal{B}$ ($k$ for the source and $k$ for the destination node), for a total of $k(k+2)$ count features.\nThese counts can be computed efficiently without constructing the whole subgraph for each edge. Defining $N_{d_u,d_v}(u,v) \\triangleq N_{d_u}(u)\\cap N_{d_v}(v)$, we have\n%\n\\begin{align}\n\\mathcal{A}_{uv}[d_u, d_v] &= |N_{d_u,d_v}(u,v)|- \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\sum_{x \\leq d_u, y \\leq d_v, (x,y) \\neq (d_u,d_v)} \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! |N_{x,y}(u,v)|\\\\\n{\\mathcal{B}}_{uv}[d] &= |N_{d}(u)| -  {\\mathcal{B}}_{uv}[d-1] - \\sum_{i=1}^d\\sum_{j=1}^{d} {\\mathcal{A}}_{uv}[i, j]\n\\end{align}\nwhere $N_{d}(u)$ are the $d$-hop neighbors of $u$ (i.e. nodes at distance $\\leq d$ from $u$). \n%\n% It is clear that label counts for a given edge can be computed as the intersection of the neighborhoods of nodes $u$ and $v$. The number of $(1,1)$ labels is the common neighbor count given by the intersection of the 1-hop neighborhood set of nodes $u$ and $v$. Higher distance labels can be arrived at by simple arithmetic operations on neighborhood intersections (See Section~\\ref{sec:estimating_intersections}). The cardinality of the intersection of two sets $A$ and $B$ is given by \n% $J(A,B) \\cdot |A \\cup B|$, which can be efficiently estimated using minhash and hyperloglog sketches.\n\n% explaining how we do this with sketches\n\\paragraph{Estimating Intersections and Cardinalities}\n\\label{sec:estimating_intersections}\n% definitions\n$|N_{d_u,d_v}(u,v)|$ and $|N_d(u)|$ can be efficiently approximated with the sketching techniques introduced in section \\ref{sec:preliminaries}.\nLet $\\mathbf{h}_{u}^{(d)}$ and  $\\mathbf{m}_{u}^{(d)}$ be the \\emph{HyperLogLog} and \\emph{MinHash}  sketches of node $u$'s $d$-hop neighborhood, obtained recursively from the initial node sketches $\\mathbf{h}_{u}^{(0)}$ and $\\mathbf{m}_{u}^{(0)}$ using the relations $\\mathbf{m}_{u}^{(d)}=\\min_{v \\in \\mathcal{N}(u)} \\mathbf{m}_{v}^{(d-1)}$ and $\\mathbf{h}_{u}^{(d)}=\\max_{v \\in \\mathcal{N}(u)} \\mathbf{h}_{v}^{(d-1)}$, where $\\min$ and $\\max$ are elementwise. We can approximate the intersection of neighborhood sets as\n\\begin{align}\n|N_{d_u,d_v}(u,v)| &\\triangleq |N_{d_u}(u)\\cap N_{d_v}(v)|  = \\\\\n             & = J(N_{d_u}(u), N_{d_v}(v)) \\cdot  |N_{d_u}(u)\\cup N_{d_v}(v)| \\\\\n             & \\approx H\\left( \\mathbf{m}_{u}^{(d_u)}, \\mathbf{m}_{v}^{(d_v)}\\right) \\cdot \\card \\left(\\max(\\mathbf{h}_{u}^{(d_u)}, \\mathbf{h}_{v}^{(d_v)})\\right),\n\\end{align}\nwhere $H(x,y)=1/n\\sum_i^n \\delta_{x_i,y_i}$ is the Hamming similarity, $\\card$ is the \\emph{HyperLogLog} cardinality estimation function (see Appendix~\\ref{app:hll}) and $\\max$ is taken elementwise. \\emph{MinHashing} approximates the Jaccard similarity ($J$) between two sets and \\emph{HyperLogLog} approximates set cardinality.\n$|N_d(u)|$ can be approximated as $|N_d(u)| \\approx \\card(\\mathbf{h}_{u}^{(d)})$.\nThe cost of this operation only depends on additional parameters of \\emph{HyperLogLog} and \\emph{MinHashing} (outlined in more detail in \nSection~\\ref{sec:sketches_main}) which allow for a tradeoff between speed and accuracy, but is nevertheless independent of the graph size. Using this approximations we obtain estimates $\\hat{\\mathcal{A}}$ and $\\hat{\\mathcal{B}}$ of the structure features counts.\n\n% \\paragraph{Structure Features}\n% \\label{sec:structure_features}\n\n\n\n% Let $k$ be the receptive field of our model, i.e. our model does not consider further than the $k$-hop for either the source or destination nodes. Let $\\mathcal{A}_{uv}[d_u, d_v]$ be the set of nodes at distance exactly $d_u$ and $d_v$ from $u$ and $v$ respectively, and $\\mathcal{B}_{uv}[d_u]$ be the set of nodes at distance up to $d_u$ from $u$ and more than $k$ from $v$. We are interested in computing $\\mathcal{A}_{uv}[d_u, d_v]$ for all $1 \\leq d_u \\leq k$ and $1 \\leq d_v \\leq k$ and $\\mathcal{B}_{uv}[d_u]$ for all $1 \\leq d_u \\leq k$ for a total of $k^3$ counts.  \n\n% $\\mathcal{A}_{uv}$ contains the counts of DE labels, while $\\mathcal{B}_{uv}[d_u]$ contains additional information that we found experimentally useful. We estimate the cardinality of these sets as follows:\n\n% \\begin{align}\n% &\\hat{\\mathcal{B}}_{uv}[d_u, d_v] = \\card(\\mathbf{z}_u[d_u])-  \\hat{\\mathcal{B}}_{uv}[d_u-1,d_v] - \\sum_{i=1}^d\\sum_{j=1}^d_u \\hat{\\mathcal{A}}_{uv}[i, j] \\\\\n% &\\hat{\\mathcal{A}}_{uv}[d_u, d_v] = |N(d_u,d_v)|-\\sum_{x \\leq d_u, y \\leq d_v, (x,y) \\neq (d_u,d_v)}|N(x,y)|\n% \\end{align}\n% where $\\hat{\\mathcal{B}}_{uv}[d_u, d_v] = 0 \\, \\forall \\, d_u<1$ or $d_v<1$. For $d_u=d_v=k=2$ there are eight structural features per edge and in general there are $k(k+2)$.\n\n\n% Figure~\\ref{fig:intersections} depicts the intersection of the neighborhood sets of nodes $(u,v)$ up to the 2-hop. The cardinality of the regions marked \n% $\\mathcal{A}_{uv}[d_u, d_v]$ are equivalent to the number of $(d_u,d_v)$ DE labels. In addition ${\\mathcal{B}}_{uv}[d]$ is a feature not used in DE that counts the number of nodes in the $d$-hop neighborhood of $u$ that are not in the $k$-hop neighborhood of $v$\\footnote{In DE nodes that exceed a maximum distance are assigned a specified maximum value}. \n% % approximating A and B\n% We use \\emph{HyperLogLog} to approximate set cardinality and \\emph{MinHashing} to approximate the Jaccard index ($J$) between two sets. Using these approximations we formulate estimates for ${\\mathcal{A}}_{uv}$ and ${\\mathcal{B}}_{uv}$ as\n% \\begin{align}\n% &\\hat{\\mathcal{B}}_{uv}[d] = \\card(\\mathbf{z}_u[d])-  \\hat{\\mathcal{B}}_{uv}[d-1] - \\sum_{d_u=1}^d\\sum_{d_v=1}^k \\hat{\\mathcal{A}}_{uv}[d_u, d_v] \\\\\n% &\\hat{\\mathcal{A}}_{uv}[d_u, d_v] = |N(d_u,d_v)|-\\sum_{x \\leq d_u, y \\leq d_v, (x,y) \\neq (d_u,d_v)}|N(x,y)|\n% \\end{align}\n% where $\\hat{\\mathcal{B}}_{uv}[d] = 0 \\, \\forall \\, d<1$. For $k=2$ there are eight structural features per edge and in general there are $k(k+2)$.\n% \\begin{align}\n%     &\\hat{\\mathcal{A}}_{uv}[d_u, d_v] = \\mathit{card}\\left(\\mathit{union}(\\{z_u[d_u], z_v[d_v]\\})\\right) \\times \\mathit{J}\\left(z_u[d_u], z_v[d_v]\\right)  \\\\\n%     % \\hat{\\mathcal{B}}_{uv}[d] &= \\mathit{card}(\\mathit{union}(\\{z_u[d], z_v[d]\\})) - \\mathit{card}(z_v[d])\n%     &\\hat{\\mathcal{B}}_{uv}[d] = \\mathit{card}\\left(\\mathit{union}(\\{z_u[d], \\mathit{union}(\\{z_v[i]\\}_{0 \\dots k}\\right) \\})) - \\mathit{card}\\left(\\mathit{union}(\\{z_v[i]\\}_{0 \\dots k})\\right),  %-\\mathit{card}(z_u[d]) \n%     \\notag\n% \\end{align}\n\n% The precision of this approximation depends on additional parameters of \\emph{HyperLogLog} and \\emph{MinHashing} which are outlined in more detail in \n% Section~\\ref{sec:sketches}. \n% %\n% \\todo[inline]{not sure this next paragraph fits anymore}\n% This framework can be used to efficiently estimate other heuristics by substituting the sketching technique. For instance MinHash and SimHash~\\citep{simhash} estimate the \\emph{Jaccard} and \\emph{cosine} heuristics respectively.\n\n\n%\n\n\n% preview of what's to come\n%  we seek to construct a model for LP that has the same theoretical grounding and performance as subgraph GNNs, but is both more efficient and scalable. \n% % overview of subgraph GNN mathematical model\n% A subgraph GNN generates link probabilities as \n% \\begin{align}\n%     p_{uv} = R \\left (\\GNN(S_{uv},X_{uv},Z_{uv}) \\right),\n% \\end{align}\n% where $R$ is a neural network based readout function that pools node embeddings,   and $S_{uv}$, $X_{uv}$ and $Z_{uv}$ are the subgraph with node and structure features respectively. As the construction of explicit subgraphs themselves is not scalable, an efficient algorithm can only be achieved by decoupling $\\GNN$, $Z$ and $R$ from subgraph generation. \n\n\n% Of the five subgraph GNN components, $X_S$ and $R$ are relatively simple to compute, but $\\GNN$, $Z_S$ and $S_{uv}$ scale with the number of links $\\mathcal{E}_{S_{uv}}$, which for networks with power law degree distributions implies $O(\\mathcal{E}_S)=O(\\mathcal{E})$ time complexity as computation is bottlenecked by high degree nodes\\todo{Ben: check this is true}. \n\n\n% \\subsection{Structure Features}\n% % nils rewrite (in progress)\n% It is clear from Figure~\\ref{fig:lab_abl} that structure features encoding graph substructures (e.g. DRNL, DE) are necessary for good performance in link prediction. Figure~\\ref{fig:feature_importance} further indicates that most of the predictive performance of these structure features is concentrated in the lowest distances.\n\n\n\n% To introduce our method, we first observe that the all $k$-hop neighborhoods can be constructed recursively as\n% \\begin{align}\n%     z_u[0] = \\{u\\}, \\quad z_u[d] = \\cup_{i \\in N(u)} z_i[d - 1], \\,\\, d = 1, 2, ..., k\n% \\end{align}\n% where $N(u)$ refers to the neighbors of node $u$ and $\\cup$ is the set union operator. Intuitively, $z_u[d]$ is the set of all nodes that are exactly $d$-hops from $u$. Based on $z_u$ and $z_v$, for a pair of nodes $(u, v)$ we can explicitly count nodes in each possible neighborhood intersection (see figure \\ref{fig:intersections}). We split between intersections $\\mathcal{A}_{uv}$ and differences $\\mathcal{B}_{uv}$:\n% \\begin{align}\n%     \\mathcal{A}_{uv}[d_u, d_v] &= |z_u[d_u] \\cap z_v[d_v]| \\\\\n%     \\mathcal{B}_{uv}[d] &= |z_u[d] \\setminus \\cup_{j\\in \\{0 \\dots k \\}} z_v[j]|\n% \\end{align}\n% where $\\cap$ and $\\setminus$ refer to set intersection and set difference respectively. Intuitively, $\\mathcal{A}_{uv}[i, j]$ corresponds to the number of nodes that are $i$-hops from $u$ and $j$-hops from $v$. Similarly, $\\mathcal{B}_{uv}[d]$ counts the nodes $d$-hops from $u$ that cannot be reached from $v$ in $k$ hops.\n\n% Note that $\\mathcal{A}_{uv}[1, 1]$ corresponds to the LP heuristic of \\emph{Common Neighbors}. Other heuristics such as \\emph{Adamic Adar} or \\emph{Resource Allocation} can be realized within this formulation by generalizing operators for \\emph{intersection}, \\emph{difference}, and \\emph{union} beyond binary set membership, e.g.\\ where set membership is weighted by $f(|N(i)|)$.\n\n% \\paragraph{Improving efficiency}\n% The running time to compute all $z$ values in the worst case is $\\mathcal{O}(k|\\mathcal{E}|u + |\\mathcal{E}|i)$ and $\\mathcal{O}(nk^2 + d)$ to compute $\\mathcal{A}$ and $\\mathcal{B}$, where $u$, $i$, $n$ and $d$ are the running times of \\emph{union}, \\emph{initialize}, \\emph{intersection}, and \\emph{difference} respectively. In addition set operations require $\\mathcal{O}(|\\mathcal{E}|)$ memory, as the $k$-hop neighborhood of a node may span a significant portion of the whole graph. Instead, we approximate $\\mathcal{A}_{uv}$ and $\\mathcal{B}_{uv}$ with sketches whose running times and sizes are constant with respect to $|\\mathcal{E}|$ and $k$. We use \\emph{HyperLogLog} to approximate set cardinality and union, and \\emph{MinHashing} to approximate the Jaccard index ($J$) between two sets. Using these approximations and the inclusion-exclusion principle, we formulate estimates for ${\\mathcal{A}}_{uv}$ and ${\\mathcal{B}}_{uv}$ as\n% \\begin{align}\n%     &\\hat{\\mathcal{A}}_{uv}[d_u, d_v] = \\mathit{card}\\left(\\mathit{union}(\\{z_u[d_u], z_v[d_v]\\})\\right) \\times \\mathit{J}\\left(z_u[d_u], z_v[d_v]\\right)  \\\\\n%     % \\hat{\\mathcal{B}}_{uv}[d] &= \\mathit{card}(\\mathit{union}(\\{z_u[d], z_v[d]\\})) - \\mathit{card}(z_v[d])\n%     &\\hat{\\mathcal{B}}_{uv}[d] = \\mathit{card}\\left(\\mathit{union}(\\{z_u[d], \\mathit{union}(\\{z_v[i]\\}_{0 \\dots k}\\right) \\})) - \\mathit{card}\\left(\\mathit{union}(\\{z_v[i]\\}_{0 \\dots k})\\right)  %-\\mathit{card}(z_u[d]) \n%     \\notag\n% \\end{align}\n% %\n% The precision of this approximation depends on additional parameters of \\emph{HyperLogLog} and \\emph{MinHashing} which are outlined in more detail in section \\ref{sec:sketches}. \n% %\n% This framework can be used to efficiently estimate other heuristics by substituting the sketching technique. For instance MinHash and SimHash~\\citep{simhash} estimate the \\emph{Jaccard} and \\emph{cosine} heuristics respectively.\n\n\n% \\todo[inline]{Ben: worry about why the hashing doesn't overfit even though we don't remove the link between (u,v) at train time and positive links must have 0 (0,1) or (1,0) neighbors\n\n% thomas: If I had to guess, hashing would overfit in the limit that we have small numbers of links. In a medium-large context, it probably just adds some amount of statistical noise\n% }\n\n\\begin{comment}\nThen region 1 in Figure~\\ref{fig:intersections}, which is the common neighbor count, or equivalently the number of triangles that $u$ and $v$ participate in, is given by \n\nwith similar relationships for other intersections and substructures (See SM for details).\n\\end{comment}\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Efficient Link Prediction with Hashes (ELPH)",
                    "content": "\n\n% {\\bf [MB: need to use terminology consistently. GNN=any graph neural networks including higher-order. MPNN=message passing only]}\n\nWe present ELPH, a novel MPNN for link prediction. In common with other full-graph GNNs, it employs a feature propagation component with a link level readout function. However, by augmenting the messages with subgraph sketches, it achieves higher expressiveness for the same asymptotic complexity. ELPH's feature propagation fits the standard MPNN formalism~\\citep{Gilmer2017}:\n\\vspace{-1mm}\n\\begin{align}\n\\mathbf{m}_{u}^{(l)} &= \\min_{v \\in \\mathcal{N}(u)} \\mathbf{m}_{v}^{(l-1)},\\quad\\quad\n\\mathbf{h}_{u}^{(l)} = \\max_{v \\in \\mathcal{N}(u)} \\mathbf{h}_{v}^{(l-1)} \\label{eq:sketches} \\\\\n\\mathbf{e}_{u, v}^{(l)} &= \\{ \\hat{\\mathcal{B}}_{uv}[l], \\hat{\\mathcal{A}}_{uv}[d_u, l], \\hat{\\mathcal{A}}_{uv}[l, d_v] : \\forall d_u,d_v < l \\} \\label{eq:edge_features} \\\\\n\\mathbf{x}_u^{(l)} &=\\gamma^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\square_{v \\in \\mathcal{N}(u)} \\phi^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\mathbf{x}_v^{(l-1)}, \\mathbf{e}_{u, v}^{(l)}\\right)\\right)\n\\label{eq:aggregation}\n% \\mathbf{f}_u^{(l)} &= \\mathbf{x}_u^{(l)} \\quad || \\quad \\mathbf{z}_{hu}^{(l)} \\quad|| \\quad \\mathbf{z}_{mu}^{(l)}\\\\\n% \\mathbf{f}_u^{(l)} &=\\gamma^{(l)}\\left(\\mathbf{f}_u^{(l-1)}, \\square_{v \\in \\mathcal{N}(u)} \\phi^{(l)}\\left(\\mathbf{f}_u^{(l-1)}, \\mathbf{f}_v^{(l-1)}, \\mathbf{e}_{u, v}^{(l-1)}\\right)\\right),\n\\end{align}\nwhere $\\phi, \\gamma$ are learnable functions, $\\square$ is a local permutation-invariant aggregation function (typically sum, mean, or max), $\\mathbf{x}_u^{(0)}$ are the original node features, and $\\mathbf{m}_u^{(0)}$ and $\\mathbf{h}_u^{(0)}$ are the \\textit{MinHashing} and \\textit{HyperLogLog} single node sketches respectively. Minhash and hyperloglog sketches of $N_l(u)$ are computed by aggregating with $\\min$ and $\\max$ operators respectively the sketches of $N_{l-1}(u)$ (Eq. \\ref{eq:sketches}). These sketches can be used to compute the intersection estimations $\\hat{\\mathcal{B}}_{uv}$ and $\\hat{\\mathcal{A}}_{uv}$ up to the $l$-hop neighborhood, which are then used as edge features (Eq. \\ref{eq:edge_features}). Intuitively, the role of the edge features is to modulate message transmission based on local graph structures, similarly to how attention is used to modulate message transmission based on feature couplings.\n%\n% with the nuance that edge features are layer dependent and given by $\\{ \\hat{\\mathcal{B}}_{uv}[d], \\hat{\\mathcal{A}}_{uv}[d_u, d_v] \\}$ for $d_u,d_v,d < l$. \n% $\\mathbf{f}$ contains three separate messages, the propagated node features, $\\mathbf{x}$, the minhash sketches $\\mathbf{z}_m$ and the hyperloglog sketches $\\mathbf{z}_h$. $\\square$ acts differently on each message performing sum, min and max aggregation respectively. Intuitively the role of the edgewise features is to modify message transmission based on local graph structures in a similar way that attention is used to modify message transmission based on feature couplings.\n%\n% link prediction function\nA link predictor is then applied to the final node representations of the form\n\\begin{align} \np(u,v)= \\psi \\left(\\mathbf{x}^{(k)}_u \\odot \\mathbf{x}^{(k)}_v, \\{ \\hat{\\mathcal{B}}_{uv}[d], \\hat{\\mathcal{A}}_{uv}[d_u, d_v] : \\forall \\, d,d_u,d_v \\in [k]  \\}\\right) \\,, \n\\label{eq:readout}\n\\end{align}\nwhere $[k] = \\{1, \\ldots, k\\}$, $k$ is a receptive field, $\\psi$ is an MLP and $\\odot$ is the element-wise product. $p(u,v)$  decouples the readout function from subgraph generation as suggested in  Figure~\\ref{fig:readout_ablation}.\n%\n% the main idea\nThis approach moves computation from edgewise operations e.g. generating subgraphs, to nodewise operations (generating sketches) that encapsulate the most relevant subgraph information for LP. We have shown that (i) GNN propagation is either not needed (structure features) or can be preprocessed globally (given features) (ii) structure features can be generated from sketches instead of subgraphs (iii) an edge pooling readout function performs as well as a graph pooling readout. In combination these approaches circumvent the need to explicitly construct subgraphs. \n% At a high level this is achieved by (i) moving the $Z_{uv}$ dependence to the readout function 2) allowing $Z_S = f(z_u,z_v)$ 3) constructing $z_u$ recursively by hashing one-hop neighbors and 4) decomposing $\\GNN$ into learning and propagation components.\n% our approach\nThe resulting model efficiently combines node features and the graph structure without explicitly constructing subgraphs and is as efficient as GCN~\\citep{kipf2017}.\n\n% The probability of a link $(u,v)$ is given by\n% \\begin{align}\n%     p_{uv} = R \\left(\\Psi(\\mathbf{x}'_u,\\mathbf{x}'_v), \\Phi(\\hat{\\mathcal{A}}_{uv}, \\hat{\\mathcal{B}}_{uv}, \\hat{\\mathcal{B}}_{vu})\\right)\n% \\end{align}\n% where $\\mathbf{x}'_u$ encodes the propagated node features and $\\Psi$ and $\\Phi$ are neural networks.\n\n% \\todo{(FF) BUDDY vs. ELPH? This is the right place to talk about them and specify the differences?}\n\n\\paragraph{Expressive power of ELPH} \n\nELPH combines the advantageous traits of both GNNs and subgraph-based methods, i.e. tractable computational complexity and the access to pair-wise structural features including triangle counts (Appendix~\\ref{sec:curvature}). Furthermore, as the following results show, it is more expressive than MPNNs: \n%But, what can we say about its representational power? We first prove the following:\n% {\\bf [MB: I suggest leaving only Thrm 4.3 and moving the lemmas to Appendix] to save space.]}\n\\begin{proposition}\\label{prop:elph-does-not-suffer}\n    Let $\\mathcal{M}_\\mathrm{ELPH}$ be the family of ELPH models as per Equations~\\ref{eq:sketches} - \\ref{eq:aggregation} and~\\ref{eq:readout} where estimates are exact ($\\hat{\\mathcal{A}} \\equiv \\mathcal{A}, \\hat{\\mathcal{B}} \\equiv \\mathcal{B}$)\\footnote{For sufficiently large samples, this result can be extended to approximate counts via unbiased estimators.}. $\\mathcal{M}_\\mathrm{ELPH}$ \\emph{does not} suffer from the automorphic node problem.\n\\end{proposition}\n% Let $\\mathcal{M}_\\mathrm{ELPH}$ and $\\mathcal{M}_\\mathrm{MPNN}$ be the family of ELPH models (as described per Equations~\\ref{eq:elph_message_passing} and~\\ref{eq:readout}) and MPNN models (Equations~\\ref{eq:mpnn_link_prediction}) respectively. \n%\n% \\begin{lemma} \\label{lemma:same_mpnn}\n% $\\mathcal{M}_\\mathrm{ELPH}$ is more expressive than $\\mathcal{M}_\\mathrm{MPNN}$.\n% \\end{lemma}\n% We define the concept of expressiveness for families of link prediction models in Section~\\ref{def:more_expressive}. Intuitively, this means that an ELPH model can discriminate any pair of links that can be discriminated by an MPNN model. We can prove this by noticing that ELPH generalizes an MPNN, and can learn to behave as an MPNN by ignoring the edge and structural features. Specifically, with 1) $\\phi^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\mathbf{x}_v^{(l-1)}, \\mathbf{e}_{u, v}^{(l)}\\right)=\\phi^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\mathbf{x}_v^{(l-1)}\\right)$, and 2) $\\psi \\left(\\mathbf{x}^k_u \\odot \\mathbf{x}^k_v, \\{ \\hat{\\mathcal{B}}_{uv}[d], \\hat{\\mathcal{A}}_{uv}[d_u, d_v] : \\forall \\, d,d_u,d_v = 1, \\ldots, k  \\}\\right) = \\psi \\left(\\mathbf{x}^k_u \\odot \\mathbf{x}^k_v\\right)$, ELPH defaults to an MPNN.\n%\n% \\begin{proposition} \\label{lemma:more_mpnn}\n% % $\\mathcal{M}_\\mathrm{ELPH}$ can discriminate pair of links that cannot be discriminated by $\\mathcal{M}_\\mathrm{MPNN}$.\n% $\\mathcal{M}_\\mathrm{ELPH}$ \\emph{does not} suffer from the automorphic node problem.\n% \\end{proposition}\n\nWhile we rigorously define the automorphic node problem and prove the above in Section~\\ref{app:proofs}, this result states that there exists non-automorphic links with automorphic nodes that an ELPH model is able to discriminate (thanks to structural features). Contrary to ELPHs, MPNNs are not able to distinguish any of these links; we build upon this consideration, as well as the observation that ELPH models subsume MPNNs, to obtain this additional result:\n%\n% We prove this by showing an example of a link that cannot be distinguished by an MPNN, but is distinguished by ELPH.\n% Since nodes 2 and 4 in figure \\ref{fig:seal_iso_graph} are isomorphic, an MPNN will assign the same probability to links (1,2) and (1,4). However, these two links are not isomorphic. ELPH is able to distinguish these links thanks to the structural features, For (1,2), we have $\\mathcal{A}_{v_{1}v_{2}}[1,1]=2$, $\\mathcal{A}_{v_{1}v_{2}}[1,2]=0$, $\\mathcal{A}_{v_{1}v_{2}}[2, 1]=0$ and $\\mathcal{A}_{v_{1}v_{2}}[2,2]=1$. On the other hand, for (1,4) we have $\\mathcal{A}_{v_{1}v_{2}}[1,1]=0$, $\\mathcal{A}_{v_{1}v_{2}}[1,2]=1$, $\\mathcal{A}_{v_{1}v_{2}}[2, 1]=1$ and $\\mathcal{A}_{v_{1}v_{2}}[2,2]=0$.\n% Since our model computes different structural features, $\\psi$ has different inputs for the two links and is able to assign them different probabilities.\n%\n\\begin{theorem}\\label{thm:elph-strictly-more-expressive-than-mpnn}\n    Let $\\mathcal{M}_\\mathrm{MPNN}$ be the family of Message Passing Neural Networks (Equation~\\ref{eq:mpnn_link_prediction}). $\\mathcal{M}_\\mathrm{ELPH}$ is strictly more powerful than $\\mathcal{M}_\\mathrm{MPNN}$ ($\\mathcal{M}_\\mathrm{ELPH} \\sqsubset \\mathcal{M}_\\mathrm{MPNN}$).\n\\end{theorem}\nWe prove this in Section~\\ref{app:proofs}. Intuitively, the Theorem states that while all links separated by MPNNs are also separated by ELPHs, there also exist links separated by the latter family but not by the former.\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Scaling ELPH with Preprocessing (BUDDY)",
                "content": " \n% ELPH intersperses learned feature maps with propagation over the graph. The propagation is modulated by sketch based edge features. \n\nSimilarly to other full-graph GNNs (e.g. GCN), ELPH is efficient when the dataset fits into GPU memory. When it does not, the graph must be batched into subgraphs. Batching is a major challenge associated with scalable GNNs and invariably introduces high levels of redundancy across batches. Here we introduce a large scale version of ELPH, called BUDDY, which uses preprocessing to side-step the need to have the full dataset in GPU memory.\n% \\begin{wrapfigure}{r}{0.5\\textwidth}\n%     \\vspace{-5mm}\n%     \\centering\n%     \\includegraphics[width=0.5\\textwidth]{feature_propagation_ablation.pdf}\n%     \\caption{The affect of a GNN readout function over the output of all nodes in the subgraph $S_{uv}$ (sum or mean) or just the nodes $u$ and $v$ (edge).}\n%     \\label{fig:readout_ablation}\n%     \\vspace{-5mm}\n% \\end{wrapfigure}\n\n% solution\n\\paragraph{Preprocessing} Figure~\\ref{fig:feature_prop} indicated that a fixed propagation of the node features almost recovers the performance of learnable SGNN propagation. This propagation can be achieved by efficient sparse scatter operations and done only once in preprocessing. Sketches can also be precomputed in a similar way:\n\\begin{align}\n    \\mathbf{M}^{(l)} = \\textrm{scatter\\_min}(\\mathbf{M}^{(l-1)}, G), \\,\\,\n    \\mathbf{H}^{(l)} = \\textrm{scatter\\_max}(\\mathbf{H}^{(l-1)}, G), \\,\\,\n    \\mathbf{X}^{(l)} = \\textrm{scatter\\_mean}(\\mathbf{X}^{(l-1)}, G)\n    \\notag\n\\end{align}\n%\nwhere $\\textrm{scatter\\_min}(\\mathbf{M}^{(l)}, G)_u=\\min_{v \\in \\mathcal{N}(u)}\\mathbf{m}_v^{(l-1)}$ and $\\textrm{scatter\\_max}$ and $\\textrm{scatter\\_mean}$ are defined similarly, $\\mathbf{X}^{(0)}$ are the original node features, and $\\mathbf{M}^{(0)}$ and $\\mathbf{H}^{(0)}$ are the \\textit{MinHashing} and \\textit{HyperLogLog} single node sketches respectively. Similarly to~\\citep{rossi2020sign}, we concatenate features diffused at different hops to obtain the input node features: $\\mathbf{Z}= \\left[\\mathbf{X}^{(0)} \\mathbin\\Vert \\mathbf{X}^{(1)} \\mathbin\\Vert...\\mathbin\\Vert \\mathbf{X}^{(k)} \\right]$. \n\n\\paragraph{Link Predictor} We now have $p(u,v)= \\psi \\left(\\mathbf{z}^{(k)}_u \\odot \\mathbf{z}^{(k)}_v, \\{ \\hat{\\mathcal{B}}_{uv}[d], \\hat{\\mathcal{A}}_{uv}[d_u, d_v] : \\forall \\, d,d_u,d_v \\in [k] \\}\\right)$, where $\\psi$ is an MLP and $\\hat{\\mathcal{A}}_{uv}$ and $\\hat{\\mathcal{B}}_{uv}$ are computed using $\\mathbf{M}$ and $\\mathbf{H}$ as explained in Section \\ref{sec:approximating_structure_features}.\nDoing so effectively converts a GNN into an MLP, removing the need to sample batches of subgraphs when the dataset overflows GPU memory~\\citep{rossi2020sign}. Further improvements in efficiency are possible when multiple computations on the same edge sets are required. In this case, the edge features $\\{ \\hat{\\mathcal{B}}_{uv}[d], \\hat{\\mathcal{A}}_{uv}[d_u, d_v] \\}$ can be precomputed and cached.\n%\n% In general, the input features $\\mathbf{X}$ are given by \n% \\begin{align}\\label{eq:buddy-prop}\n% \\mathbf{X} = \\left[\\mathbf{X} \\mathbin\\Vert \\Sigma \\mathbf{X} \\mathbin\\Vert...\\mathbin\\Vert \\Sigma^k \\mathbf{X} \\right]\n% % \\mathcal{K}_{i=0}^k \\tilde{A}^i X\n% \\end{align}\n% where $\\mathbin\\Vert$ is the concatenation operator and $\\Sigma$ performs scatter mean, min and max over the graph for node features, minhash sketches and hyperloglog sketches respectively. Thus the only change to Equation~\\eqref{eq:readout} is how $\\mathbf{f}$ is calculated. \n\n% is a graph propagation operator such as the symmetrically normalized adjacency employed in GCN~\\citep{kipf2017}. We apply this idea to LP, effectively performing all subgraph feature propagation steps in a single global propagation operation.\n\n\\vspace{-1em}\n\\paragraph{Time Complexity}\n\n\n\n% this next paragraph was when we used to describe this in terms of set operations\n% The running time to compute all $z$ values in the worst case is $\\mathcal{O}(k|\\mathcal{E}|u + |\\mathcal{E}|i)$ and $\\mathcal{O}(nk^2 + d)$ to compute $\\mathcal{A}$ and $\\mathcal{B}$, where $u$, $i$, $n$ and $d$ are the running times of \\emph{union}, \\emph{initialize}, \\emph{intersection}, and \\emph{difference} respectively. In addition set operations require $\\mathcal{O}(|\\mathcal{E}|)$ memory, as the $k$-hop neighborhood of a node may span a significant portion of the whole graph. Instead, we approximate $\\mathcal{A}_{uv}$ and $\\mathcal{B}_{uv}$ with sketches whose running times and sizes are constant with respect to $|\\mathcal{E}|$ and $k$. \n% preprocessing\n\nDenoting the complexity of hash operations as $h$,  the node representation dimension $d$ and the number of edges $E=|\\mathcal{E}|$, the preprocessing complexity of BUDDY is $\\mathcal{O}(kEd + kEh)$. The first term being node feature propagation and the second sketch propagation. \n% However, usually $h \\ll d$ and thus the complexity reduces to $\\mathcal{O}(kEd)$. \nPreprocessing for SEAL and NBFNet are $\\mathcal{O}(1)$.\n%\n% training\nA link probability is computed by (i) extracting $k(k+2)$ $k$-hop structural features, which costs $\\mathcal{O}(k^2 h)$ and (ii) An MLP on structural and node features, which results in $\\mathcal{O}(k^2h + kd^2)$ operations. Both are\n% Again, since $h \\ll d$ we omit the hashing time and are left with $\\mathcal{O}(d^2)$. Note, that the complexity  \n\\textbf{independent of the size of the graph}.\n% BUDDY encodes $k$-hop neighborhood intersections with complexity $\\mathcal{O}(kh|\\mathcal{E}|)$ to preprocess and $\\mathcal{O}(k^2)$ to predict a link. This greatly improves on node labeling schemes such as those employed in~\\citep{zhang2021labeling, zhang2018link} that must be calculated for each link at both training and inference time and have $O(\\mathcal{E}$) worst-case time complexity per link. Denoting $N=|\\mathcal{V}|$, using $d$-dimensional features, letting $E=|\\mathcal{E}|$, assume a maximum $k$-hop feature propagation and hashing operations of complexity $h$ (a small number independent of the graph size derived from $p$).\n% Since SGNN construct $k$-hop subgraphs for each link,  in the \\textbf{best} case of regular graphs they require $\\degree^k$ to construct a subgraph, $\\degree^k$ to label the graph and then $\\degree^k d^2$ to perform the forward pass, thus resulting in $\\mathcal{O}(\\degree^k d^2)$ complexity. \n% % The $\\degree$ is an average node degree in a graph.\n% However, in practice the regularity assumption does not hold and for power law degree distributions, the $\\degree^k$ term must be replaced with $E$, resulting in $\\mathcal{O}(E d^2)$ complexity (See Appendix~\\ref{sec:subgraph_complexity}).\nSince SGNNs construct subgraphs for each link they are $\\mathcal{O}(E d^2)$ for complex networks (See Section~\\ref{sec:subgraph_complexity}).\n%\nFinally, the total complexity of NBFNet is $\\mathcal{O}(Ed + N d^2)$ with an amortized time for a single link prediction of $\\mathcal{O}(Ed / N + d^2)$~\\citep{zhu2021neural}. However, this is only realized in situations where the link probability $p(u, j)$ is required $\\forall j \\in \\mathcal{V}$.\n%\n% tables\nTime complexities are summarized in Table~\\ref{tab:complexity} and space complexities in Appendix~\n\\ref{app:space_complexity}.\n\n% \\vspace{-1em}\n\\paragraph{Expressiveness of BUDDY} Similarly to ELPH, BUDDY does not suffer from the automorphic node problem (see Proposition and its Proof in Appendix~\\ref{app:proofs}). However, due to its non-parametric feature propagation, this class of models does not subsume MPNNs and we cannot exclude the presence of links separated by MPNNs, but not by BUDDY. Nevertheless, BUDDY empirically outperforms common MPNNs by large margins (see Section~\\ref{sec:experiments}), while also being extremely scalable. \n\n% \\begin{table}[t]\n% \\caption{Wall time of our method in comparison to SEAL and NBFNet on Cora (left) and Pubmed (right) datasets. Training time presented is computed based on one training epoch. Inference time presented is for full \\textbf{test} data pass. A batch size of 256 is used for SEAL and Ours and a batch size of 128 is used for NBFNet. Computation is performed on a single Tesla V100 GPU. $^*$ We do not include results for NBFnet obtained using 5 Tesla V100 GPUs with a batch size of 84 for a fair comparison. {\\bf [MB: merge into one table. Add another bigger dataset?]}}\n%     \\resizebox{0.5\\textwidth}{!}{%\n%     \\begin{tabular}{l cccc}\n%     \\toprule \n    \n%          \\textbf{Wall time (sec)} &  \n%          \\textbf{SEAL}&\n%          \\textbf{NBFNet}&\n%          \\textbf{Ours} \\\\\n%          \\toprule\n         \n%          Preprocessing &0 &0& 9.8 \\\\\n         \n%          Training (1 epoch) &9.2  &9.6& 0.3\\\\\n         \n%          Inference &3.0 &0.9 & 0.01\n         \n% \\end{tabular}\n% \\quad\n% }\n%     \\resizebox{0.5\\textwidth}{!}{%\n%     \\begin{tabular}{l cccc}\n%     \\toprule \n    \n%          \\textbf{Wall time (sec)} &  \n%          \\textbf{SEAL}&\n%          \\textbf{NBFNet}&\n%          \\textbf{Ours} \\\\\n%          \\toprule\n         \n%          Preprocessing &0  &0 & 75 \\\\\n         \n%          Training (1 epoch) & 69.9 & OOM & 1.4 \\\\\n         \n%          Inference &22.6 & OOM & 0.06\n         \n% \\end{tabular}\n% }\n% \\quad\n% \\vspace{2mm}\n\n% \\label{tab:wall_time}\n% \\vspace{-5mm}\n% \\end{table}  \n\n \n\n\n% \\subsection{Interpretability}\n\n% \\subsection{$\\gamma$-decaying theory}\n\n% Is there an equivalent theory such as the one presented in~\\citep{zhang2018link}?\n\n% \\subsection{Equivariance}\n\n% Section to analyse to what extent the model is equivariant\n\n\n"
            },
            "section 6": {
                "name": "Related Work",
                "content": "\n\n% subgraph methods: seal and related papers\nSubgraph methods for link prediction were introduced as the Weisfeiler Leman Neural Machine (WLNM) in~\\citep{zhang2017weisfeiler}. As an MLP is used to learn from subgraphs instead of a GNN their model is not permutation-equivariant and uses a hashing-based WL algorithm~\\citep{kersting2014power} to generate node indices for the MLP. Note that `hashing' here refers to injective neighbor aggregation functions, distinct from the data sketching methods employed in the current work. The MLP in WLNM was replaced by a GNN in the seminal SEAL~\\citep{zhang2018link}, thus removing the need for a hashing scheme. Additional methodological improvements were made in~\\citep{zhang2021labeling, zhang2021nested} and subgraph generation was improved in~\\citep{yin2022algorithm}. Applications to recommender systems were addressed in~\\citep{zhang2019inductive} and  random walks are used to represent subgraph topology in~\\citep{yin2022algorithm, pan2021neural}.\n% other labeling tricks\nThe DRNL labeling scheme was introduced in SEAL and further labeling schemes are developed in~\\citep{you2021identity, li2020distance} and generalized with equivariant positional encodings~\\citep{wang2022equivariant}.\n% NBF and related \nNeural Bellman-Ford Networks~\\citep{zhu2021neural} takes a different approach. It is concerned with single source distances where each layer of the neural networks is equivalent to an iteration of the Bellman-Ford algorithm and can be regarded as using a partial labeling scheme. The same graph and augmentation can be shared among multiple destinations and so it is faster (amortized) than SGNNs. However, it suffers from poor space complexity and thus requires impractically high memory consumption.\nNeo-GNN~\\citep{yun2021neo} directly learns a heuristic function using an edgewise and a nodewise neural network. \\#GNN~\\citep{wu2021hashing} directly hashes node features, instead of graph structure, and is limited to node features that can be interpreted as set memberships.\n% knowledge graph methods and BigGraph?\n% Another traditional approach to LP requires explicitly embedding the graph at hand. In such a framework, the nearest neighbors of a query vertex are assumed to be candidate links. \nAnother paradigm for LP uses self-supervised node embeddings combined with an (often) approximate distance metric.\n% , typically have a margin-loss function, and provide quite a simple model that is easily capable of representing both directed and multirelational aspects of the graph at hand. \nAmongst the best known models are TransE~\\citep{bordes2013translating}, DistMult~\\citep{yangYHGD14a}, or complEx~\\citep{pmlr-v48-trouillon16}, which have been shown to scale to large LP systems in e.g.~\\citep{pbg, el2022twhin}. \n% It is for this reason that we have chosen to compare our presented method against these Knowledge Graph embeddings.\n% handling features.\nDecoupling feature propagation and learning in GNNs was first implemented in SGC~\\citep{wu2019simplifying} and then scalably in SIGN~\\citep{rossi2020sign}. \n\n   \n\n"
            },
            "section 7": {
                "name": "Experiments",
                "content": " \\label{sec:experiments}\n\n% \\vspace{-1em}\n\\paragraph{Datasets, Baselines and Experimental Setup}\nWe report results for the most widely used Planetoid citation networks Cora~\\citep{mccallum2000automating}, Citeseer~\\citep{sen2008collective} and Pubmed~\\citep{namata2012query} and the OGB link prediction datasets~\\citep{hu2020open}. \nWe report results for: Three heuristics that have been successfully used for LP, Adamic-Adar (AA)~\\citep{adamic2003friends}, Resource Allocation (RA)~\\citep{zhou2009predicting} and Common Neighbors (CN); two of the most popular GNN architectures: Graph Convolutional Network (GCN)~\\citep{kipf2017} and GraphSAGE~\\citep{Hamilton2017}; the state-of-the-art link prediction GNNs Neo-GNN~\\citep{yun2021neo},  SEAL~\\citep{zhang2018link} and NBFNet~\\citep{zhu2021neural}. Additional details on datasets and the experimental setup are included in Appendix~\\ref{sec:datasets} and the code is public\\footnote{https://github.com/melifluos/subgraph-sketching}.\n\n% link prediction methodology\n\\vspace{-1em}\n\\paragraph{Results} Results are presented in Table~\\ref{tab:main_results} with metrics given in the first row. Either ELPH or BUDDY achieve the best performance in five of the seven datasets, with SEAL being the closest competitor. Being a full-graph method, ELPH runs out of memory on the two largest datasets, while its scalable counterpart, BUDDY, performs extremely well on both. Despite ELPH being more general than BUDDY, there is no clear winner between the two in terms of performance, with ELPH outperforming BUDDY in three of the five datasets where we have results for both. Ablation studies for number of hops, sketching parameters and the importance of node and structure features are in Appendix~\\ref{app:ablation_studies}.\n% Given its scalability and faster training and inference times, we see BUDDY as the preferred method between the two.\n\n\n\n\\vspace{-1em}\n\\paragraph{Runtimes} Wall times are shown in Table~\\ref{tab:wall_time}. We report numbers for both the static mode of SEAL (all subgraphs are generated and labeled as a preprocessing step), and the dynamic mode (subgraphs are generated on the fly). BUDDY is orders of magnitude faster both in training and inference. In particular, Buddy is 200--1000$\\times$ faster than SEAL in dynamic mode for training and inference on the Citation dataset. We also show GCN as a baseline. Further runtimes and a breakdown of preprocessing costs are in Appendix~\\ref{sec:runtimes} with learning curves in Appendix~\\ref{app:learning_curves}.\n\n\n% \\begin{table*}[t]\n%     \\centering\n%     \\caption{The Top three models are colored by \\textbf{\\textcolor{red}{First}}, \\textbf{\\textcolor{blue}{Second}}, \\textbf{\\textcolor{violet}{Third}}. \\textcolor{olive}{HR is Hit Rate and MRR is Mean Reciprocal Rank}. Confidence intervals are $\\pm$ one sd. Planetoid splits are random and the OGB splits are fixed. Where possible, baseline results are taken directly from the OGB leaderboard. $^*$NBFnet was trained without learned node embeddings and with a batch size of 5.}\n%     \\resizebox{\\textwidth}{!}{%\n%     \\begin{tabular}{l ccccccc}\n%     \\toprule \n%          &\n%          \\textbf{Cora} &  \n%          \\textbf{Citeseer} & \n%          \\textbf{Pubmed} &\n%          \\textbf{Collab} &\n         \n%          \\textbf{PPA} &\n%          \\textbf{Citation2} &\n%          \\textbf{DDI}\n%          \\\\\n         \n%                   \\#Nodes &\n\n%          2,708 & \n%          3,327 &\n%          18,717 &\n%          235,868 &\n%          576,289 &\n%          2,927,963 &\n%          4267\n%           \\\\\n         \n%          \\#Edges &\n%          5,278 & \n%          4,676 &\n%          44,327 &\n%          1,285,465 &\n%          30,326,273 &\n%          30,561,187 &\n%          1,334,889 \n%           \\\\\n          \n%          avg deg &\n%          3.9 &\n%          2.74 & \n%          4.5 &\n%          5.45 &\n%          52.62 &\n%          10.44 &\n%          312.84\n%          \\\\\n          \n%          metric &\n\n%          HR@100 &\n%          HR@100 & \n%          HR@100 &\n%          HR@50 &\n%          HR@100 &\n%          MRR &\n%          HR@20\n%          \\\\\n         \n%          splits &\n\n%          rand &\n%          rand & \n%          rand &\n%          time &\n%          throughput &\n%          time &\n%          protein\n%          \\\\ \\midrule\n          \n%          \\textbf{CN} & \n%          $33.92 {\\scriptstyle \\pm 0.46}$& \n%          $29.79 {\\scriptstyle \\pm 0.90}$& \n%          $23.13 {\\scriptstyle \\pm 0.15}$&\n%          $56.44 {\\scriptstyle \\pm 0.00}$&\n%          $27.65 {\\scriptstyle \\pm 0.00}$&\n%          $51.47 {\\scriptstyle \\pm 0.00}$&\n%          $17.73 {\\scriptstyle \\pm 0.00}$\n%          \\\\\n          \n%         \\textbf{AA} & \n%         $39.85 {\\scriptstyle \\pm 1.34}$&\n%         $35.19 {\\scriptstyle \\pm 1.33}$&\n%         $27.38 {\\scriptstyle \\pm 0.11}$&\n%         \\third{64.35}{0.00}&\n%         $32.45 {\\scriptstyle \\pm 0.00}$&\n%         $51.89 {\\scriptstyle \\pm 0.00}$&\n%         $18.61 {\\scriptstyle \\pm 0.00}$\\\\\n        \n%         \\textbf{RA} &\n%         $41.07 {\\scriptstyle \\pm 0.48}$ &\n%         $33.56 {\\scriptstyle \\pm 0.17}$&\n%         $27.03 {\\scriptstyle \\pm 0.35}$& \n%         $64.00 {\\scriptstyle \\pm 0.00}$&\n%         \\second{49.33}{0.00} & \n%         $51.98 {\\scriptstyle \\pm 0.00}$&\n%         $27.60 {\\scriptstyle \\pm 0.00}$\n%         \\\\ \\midrule\n        \n%         \\textbf{transE} &\n%         $67.40 {\\scriptstyle \\pm 1.60}$& \n%         $60.19 {\\scriptstyle \\pm 1.15}$&\n%         $36.67 {\\scriptstyle \\pm 0.99}$& \n%         $29.40 {\\scriptstyle \\pm 1.15}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/hhl5jwfi?workspace=user-tmarkovich\n%         $22.69 {\\scriptstyle \\pm 0.49}$ &\n%         $76.44 {\\scriptstyle \\pm 0.18}$ & \n%         $6.65 {\\scriptstyle \\pm 0.20}$\n%         \\\\\n          \n%         \\textbf{complEx} & \n%         $37.16 {\\scriptstyle \\pm 2.76}$& \n%         $42.72 {\\scriptstyle \\pm 1.68}$& \n%         $37.80 {\\scriptstyle \\pm 1.39}$&\n%         $53.91 {\\scriptstyle \\pm 0.50}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/0bbr9kt9?workspace=user-tmarkovich\n%         $27.42 {\\scriptstyle \\pm 0.49}$ &\n%         $72.83 {\\scriptstyle \\pm 0.38 }$ &\n%         $8.68 {\\scriptstyle \\pm 0.36 }$\n%         \\\\\n        \n%         \\textbf{DistMult} & \n%         $41.38 {\\scriptstyle \\pm 2.49}$& \n%         $47.65 {\\scriptstyle \\pm 1.68}$& \n%         $40.32 {\\scriptstyle \\pm 0.89}$&\n%         $51.00 {\\scriptstyle \\pm 0.54}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/u0mwexre/table?workspace=user-tmarkovich\n%         $28.61 {\\scriptstyle \\pm 1.47} $&\n%         $66.95 {\\scriptstyle \\pm 0.40} $&\n%         $11.01 {\\scriptstyle \\pm 0.49} $\n%          \\\\ \\midrule\n        \n%         \\textbf{GCN} & \n%         $66.79 {\\scriptstyle \\pm 1.65}$& \n%         $67.08 {\\scriptstyle \\pm 2.94}$&\n%         $53.02 {\\scriptstyle \\pm 1.39}$& \n%         $44.75 {\\scriptstyle \\pm 1.07}$&\n%         $18.67 {\\scriptstyle \\pm 1.32}$&\n%         $84.74 {\\scriptstyle \\pm 0.21}$&\n%         \\third{37.07}{5.07}\n%          \\\\\n          \n%         \\textbf{SAGE} & \n%         $55.02 {\\scriptstyle \\pm 4.03}$& \n%         $57.01 {\\scriptstyle \\pm 3.74}$& \n%         $39.66{\\scriptstyle \\pm 0.72}$& \n%         $48.10 {\\scriptstyle \\pm 0.81}$& \n%         $16.55 {\\scriptstyle \\pm 2.40}$&\n%         $82.60 {\\scriptstyle \\pm 0.36}$&\n%         \\second{53.90}{4.74}\\\\ \\midrule\n        \n%         \\textbf{SEAL} & \n%         \\second{81.71}{1.30}& \n%         \\second{83.89}{2.15}& \n%         \\first{75.54}{1.32}&\n%         \\second{64.74}{0.43}& \n%         \\third{48.80}{3.16}& \n%         \\first{87.67}{0.32}&\n%         $30.56 {\\scriptstyle \\pm 3.86}$\\\\ \n        \n%          \\textbf{NBFnet} & \n%          \\third{71.65}{2.27}&\n%          \\third{74.07}{1.75}&\n%          \\third{58.73}{1.99}&\n%          OOM&\n%          OOM&\n%          OOM&\n%          $4.00 {\\scriptstyle \\pm 0.58}$^*\\\\ \n         \n%          \\textbf{Neo-GNN} & \n%          &\n%          $30.56 {\\scriptstyle \\pm 3.86}$ &\n%          &\n%          \\midrule\n\n%          \\textbf{ELPH} & \n%          &\n%          &\n%          &\n%          &\n%          &\n%          &\n         \n%          \\textbf{BUDDY} & \n%          \\first{88.00}{0.44}&\n%          \\first{92.93}{0.27}&\n%          \\second{74.10}{0.78}&\n%          \\first{65.94}{0.58}&\n%          \\first{49.85}{0.20}& \n%          \\second{87.56}{0.11}&\n%          \\first{78.51}{1.36}\n%          \\midrule\n%                   \\bottomrule\n% \\end{tabular}\n% }\n% \\label{tab:main_results}\n% \\vspace{-3mm}\n% \\end{table*}   \n\n \n\n\n"
            },
            "section 8": {
                "name": "Conclusion",
                "content": "\n\nWe have presented a new model for LP that is based on an analysis of existing state-of-the-art models, but which achieves better time and space complexity and superior predictive performance on a range of standard benchmarks.\n% limitations\nThe current work is limited to undirected graphs or directed graphs that are first preprocessed to make them undirected as is common in GNN research.\n% future work\nWe leave as future work extensions to directed graphs and temporal / dynamically evolving graphs and investigations into the links with graph curvature (See~Appendix~\\ref{sec:curvature}).\n\n"
            },
            "section 9": {
                "name": "Acknowledgments",
                "content": "\n\nWe would like to thank Junhyun Lee, Seongjun Yun and Michael Galkin for useful discussions regarding prior works and are particularly grateful to Junhyun and Seongjun for generating additional Neo-GNN results for Table~\\ref{tab:main_results}. MB is supported in part by ERC Consolidator grant no 724228 (LEMAN).\n\n\\pagebreak\n\n\\bibliography{references}\n\\bibliographystyle{plain}\n% \\bibliographystyle{}\n% \\ExecuteOptions{nohyperref}\n\\newcommand{\\showDOI}[1]{\\unskip} \n\\newcommand{\\showURL}[1]{\\unskip} \n\\renewcommand{\\thepage}{}\n\n\n\\clearpage\n\n\\appendix\n\n"
            },
            "section 10": {
                "name": "Theoretical Analyses",
                "content": "\n\n",
                "subsection 10.1": {
                    "name": "Preliminaries",
                    "content": "\n\nWe introduce some preliminary concepts that will be useful in our analysis. Let us start with the definition of graph \\emph{isomorphism} and \\emph{automorphism}.\n\n\\begin{definition}[Graph isomorphism and automorphism]\\label{def:isomorphism-automorphism}\n    Let $G_1 = (V_1, E_1)$, $G_2 = (V_2, E_2)$ be two simple graphs. An \\emph{isomorphism} between $G_1, G_2$ is a bijective map $\\varphi: V_1 \\rightarrow V_2$ which preserves adjacencies, that is: $\\forall u, v \\in V_1: (u, v) \\in E_1 \\Longleftrightarrow (\\varphi(u), \\varphi(v)) \\in E_2$. If $G_1 = G_2$, $\\varphi$ is called an \\emph{automorphism}.\n\\end{definition}\n\nIn view of the definition above, two graphs are also called \\emph{isomorphic} whenever there exists an isomorphism between the two. Intuitively, if two graphs are isomorphic they encode the same exact relational structure, up to relabeling of their nodes. As we shall see next, automorphisms convey a slightly different meaning: they allow to define graph symmetries by formalizing the concept of node structural roles.\n\nThe above definitions can be extended to attributed graphs, more conveniently represented through third-order tensors $\\mathsf{A} \\in \\mathbb{R}^{n^2 \\times d}$, with $n = |V|, d > 0$ the number of features \\citep{maron2019provably, zhang2021labeling}. Indexes in the first two dimensions of tensor $\\mathsf{A}$ univocally correspond to vertexes in $V$ through a bijection $\\iota : V \\rightarrow \\{1, \\dots, n \\}$. Here, elements $\\big( \\mathsf{A} \\big)_{i,i,:}$ correspond to node features, while elements $\\big( \\mathsf{A} \\big)_{i,j,:}, i \\neq j$ to edge features, which include the connectivity information in $E$. Within this context, a graph is defined as a tuple $G = (V, E, \\iota, \\mathsf{A})$, and isomorphisms and automorphisms are interpreted as permutations acting on $\\mathsf{A}$ as:\n\\begin{equation}\n    \\big ( \\sigma \\cdot \\mathsf{A} \\big )_{ijk} = \\mathsf{A}_{\\sigma^{-1}(i)\\sigma^{-1}(j)k}, \\quad \\forall \\sigma \\in S_n\n\\end{equation}\n\\noindent where $S_n$ is the set of all bijections (permutations) over $n$ symbols. We then define the concepts of isomorphism and automorphism as:\n\\begin{definition}[Graph isomorphism and automorphism on attributed graphs]\\label{def:isomorphism-automorphism-attr}\n    Let $G_1, G_2$ be two attributed graphs represented by tensors $\\mathsf{A}_1, \\mathsf{A}_2 \\in \\mathbb{R}^{n^2 \\times d}$. An \\emph{isomorphism} between $G_1, G_2$ is a bijective map (permutation) $\\sigma \\in S_n$ such that $\\sigma \\cdot \\mathsf{A}_1 = \\mathsf{A}_2$. For a graph $G = \\mathsf{A}$, $\\sigma \\in S_n$ is an \\emph{automorphism} if $\\sigma \\cdot \\mathsf{A} = \\mathsf{A}$.\n\\end{definition}\n\nLet us get back to the information conveyed by automorphisms by introducing the following definition:\n\\begin{definition}[Graph automorphism group]\\label{def:auto-group}\n    Let $G$ be a simple graph and $\\mathbb{A}_G$ be the set of graph automorphisms defined on its vertex set. $\\mathrm{Aut}_G = (\\mathbb{A}_G, \\circ)$ is the group having $\\mathbb{A}_G$ as base set and function composition ($\\circ$) as its operation.\n\\end{definition}\n\nGroup $\\mathrm{Aut}_G$ induces an equivalence relation $\\sim_G \\subseteq V \\times V$: $\\forall u, v \\in V, u \\sim_G v \\Leftrightarrow \\exists \\varphi \\in \\mathbb{A}_G: u = \\varphi(v)$ (it is easily proved that $\\sim_G$ is indeed reflexive, symmetric and transitive, thus qualifying as an equivalence relation). The equivalence classes of $\\sim_G$ partition the vertex set into \\emph{orbits}: $\\mathrm{Orb}_G(v) = \\big \\{ u \\in V\\ \\big |\\ u \\sim_G v \\big \\} = \\big \\{ u \\in V\\ \\big |\\ \\exists \\varphi \\in \\mathbb{A}_G : u = \\varphi(v) \\big \\}$. The set of orbits identifies all structural roles in the graph. Two vertexes belonging to the same orbit have the same structural role in the graph and are called `symmetric' or `automorphic'.\n\nIn fact, it is possible to define automorphisms between node-pairs as well~\\citep{srinivasan2019equivalence,zhang2021labeling}:\n\\begin{definition}[Automorphic node pairs]\n    Let $G = \\mathsf{A}$ be an attributed graph and $\\ell_1 = (u_1, v_1), \\ell_2 = (u_2, v_2) \\in V \\times V$ be two pairs of nodes. We say $\\ell_1, \\ell_2$ are \\emph{automorphic} if there exists an automorphism $\\sigma \\in \\mathbb{A}_G$ such that $\\sigma \\cdot \\ell_1 = (\\sigma(u_1), \\sigma(v_1)) = (u_2, v_2) = \\ell_2$. We write $\\ell_1 \\sim^{(2)}_G \\ell_2$.\n\\end{definition}\n\\noindent With the above it is possible to extend the definition of orbits to node-pairs, by considering those node-pair automorphisms which are naturally induced by node ones, i.e.: $\\forall \\varphi \\in \\mathbb{A}_G, \\varphi^{*}: (u, v) \\mapsto (\\varphi(u), \\varphi(v))$.\n\nNotably, for two node pairs $\\ell_1 = (u_1, v_1), \\ell_2 = (u_2, v_2) \\in V \\times V$, $ \\big \\{ u_1 \\sim_G u_2, v_1 \\sim_G v_2 \\big \\} \\notimplies \\ell_1 \\sim^{(2)}_G \\ell_2$, i.e., automorphism between nodes does not imply automorphism between node pairs. For instance, one counter-example of automorphic nodes involved in non-automorphic pairs is depicted in Figure~\\ref{fig:seal_iso_graph}, while another one is constituted by pair $(v_0, v_2), (v_0, v_3)$ in Figure~\\ref{fig:c6}, whose automorphisms are reported in Tables~\\ref{tab:c6-node-auto} and~\\ref{tab:c6-node-pair-auto}.\n% Intuitively, this is because the same, node automorphism is required for node-pair automorphism, while two node pairs may have their nodes automorphic via distinct automorphisms.\nThis `phenomenon' is the cause of the so-called ``automorphic node problem''.\n\\begin{definition}[Automorphic node problem] \\label{def:automorphic_node_problem}\n    Let $\\mathcal{M}$ be a family of models. We say $\\mathcal{M}$ \\emph{suffers from the automorphic node problem} if for any model $M \\in \\mathcal{M}$, and any simple (attributed) graph $G = (V, E, \\iota, \\mathsf{A})$ we have that $\\forall (u_1, v_1), (u_2, v_2) \\in V \\times V$, $\\big \\{ u_1 \\sim_G u_2, v_1 \\sim_G v_2 \\big \\} \\implies M \\big ( (u_1, v_1) \\big ) = M \\big ( (u_2, v_2) \\big )$.\n\\end{definition}\n\\noindent The above property is identified as a `problem' because there exist examples of non-automorphic node pairs composed by automorphic nodes. A model with this property would inevitably compute the same representations for these non-automorphic pairs, despite they feature significantly different characteristics, e.g. shortest-path distance or number of common neighbors.\n\nImportantly, as proved by~\\cite{srinivasan2019equivalence} and restated by~\\cite{zhang2021labeling}, the model family of Message Passing Neural Networks suffer from the aforementioned problem:\n\\begin{proposition}\\label{prop:mpnn-suffers}\nLet $\\mathcal{M}_{\\mathrm{MPNN}}$ be the family of Message Passing Neural Networks (Equation~\\ref{eq:mpnn_link_prediction}) representing node-pairs as a function of their computed (equivariant) node representations. $\\mathcal{M}_{\\mathrm{MPNN}}$ suffers from the automorphic node problem.\n\\end{proposition}\n\nWe conclude these preliminaries by introducing the concept of \\emph{link discrimination}, which gives a (more) fine-grained measure of the link representational power of model families.\n\\begin{definition}[Link discrimination]\n    Let $G = (V, E, \\iota, \\mathsf{A})$ be any simple (attributed) graph and $M$ a model belonging to some family $\\mathcal{M}$. Let $\\ell_1 = (u_1, v_1), \\ell_2 = (u_2, v_2) \\in V \\times V$ be two node pairs. We say $M$ discriminates pairs $\\ell_1, \\ell_2$ iff $M(\\ell_1) \\neq M(\\ell_2)$. We write $\\ell_1 \\neq_M \\ell_2$. If there exists such a model $M \\in \\mathcal{M}$, then family $\\mathcal{M}$ distinguishes between the two pairs and we write $\\ell_1 \\neq_\\mathcal{M} \\ell_2$. \n\\end{definition}\n\\noindent Accordingly, $\\mathcal{M}$ does not discriminate pairs $\\ell_1, \\ell_2$ when it contains no model instances which assign distinct representations to the pairs. We write $\\ell_1 =_\\mathcal{M} \\ell_2$.\n\nWe can compare model families based on their \\emph{expressiveness}, that is, their ability to discriminate node pairs:\n\\begin{definition}[More expressive]\\label{def:more_expressive}\n    Let $\\mathcal{M}_1, \\mathcal{M}_2$ be two model families. We say $\\mathcal{M}_1$ is more expressive than $\\mathcal{M}_2\\ \\mathrm{iff}\\ \\forall G = (V, E, \\iota, \\mathsf{A}), \\ell_1, \\not\\sim^{(2)}_G \\ell_2 \\in V \\times V, \\ell_1 \\neq_{\\mathcal{M}_2} \\ell_2 \\implies \\ell_1 \\neq_{\\mathcal{M}_1} \\ell_2$. We write $\\mathcal{M}_1 \\sqsubseteq \\mathcal{M}_2$.\n\\end{definition}\nPut differently, $\\mathcal{M}_1$ is more expressive than $\\mathcal{M}_2$ when for any two node pairs, if there exists a model in $\\mathcal{M}_2$ which disambiguates between the two pairs, then there exists a model in $\\mathcal{M}_1$ which does so as well. When the opposite is not verified, we say $\\mathcal{M}_1$ is \\emph{strictly} more expressive than $\\mathcal{M}_2$:\n\\begin{definition}[Strictly more expressive] \\label{def:strictly_more_expressive}\n    Let $\\mathcal{M}_1, \\mathcal{M}_2$ be two model families. We say $\\mathcal{M}_1$ is strictly more expressive than $\\mathcal{M}_2\\ \\mathrm{iff}\\ \\mathcal{M}_1 \\sqsubseteq \\mathcal{M}_2 \\land \\mathcal{M}_2 \\not\\sqsubseteq \\mathcal{M}_1$. Equivalently, $\\mathcal{M}_1 \\sqsubseteq \\mathcal{M}_2 \\land \\exists G = (V, E, \\iota, \\mathsf{A}), \\ell_1, \\not\\sim^{(2)}_G \\ell_2 \\in V \\times V,\n    \\ \\mathrm{s.t.}\\ \\ell_1 \\neq_{\\mathcal{M}_1} \\ell_2 \\land \\ell_1 =_{\\mathcal{M}_2} \\ell_2$.\n\\end{definition}\nIn other words, $\\mathcal{M}_1$ is strictly more expressive than $\\mathcal{M}_2$ when there exists no model in the latter family disambiguating between two non-automorphic pairs while there exist some models in the former which do so.\n\n"
                },
                "subsection 10.2": {
                    "name": "Deferred theoretical results and proofs",
                    "content": "\\label{app:proofs}\n\n\n\nLet us start by reporting the Proof for Proposition~\\ref{prop:elph-does-not-suffer}, stating that ELPH models do not suffer from the automorphic node problem.\n\\begin{proof}[Proof of Proposition~\\ref{prop:elph-does-not-suffer}]\\label{proof:elph-does-not-suffer}\n    In order to prove the Proposition it is sufficient to exhibit an ELPH model which distinguishes between two node-pairs whose nodes are automorphic. Consider $G = C_6$, the chordless cycle graph with $6$ nodes, which we depict in Figure~\\ref{fig:c6}. Due to the symmetry of this graph, all nodes are in the same orbit, and are therefore automorphic: we report the set of all graph automorphisms $\\mathbb{A}_{G}$ in Table~\\ref{tab:c6-node-auto}. Let us then consider pairs $\\ell_1 = (v_0,v_2), \\ell_2 = (v_0,v_3)$. The two pairs satisfy the premise in the definition of the automorphic node problem, as $v_0 \\sim_G v_0, v_2 \\sim_G v_3$. One single ELPH message-passing layer ($k=1$) produces the following (exact) structural features. Pair $\\ell_1$: $\\mathcal{A}_{v_0,v_2}[1,1] = 1, \\mathcal{B}_{v_0,v_2}[1] = 1$; pair $\\ell_2$: $\\mathcal{A}_{v_0,v_3}[1,1] = 0, \\mathcal{B}_{v_0,v_3}[1] = 2$. Thus, a one-layer ELPH model $M$ is such that $\\ell_1 \\neq_M \\ell_2$ if the readout layer $p(u,v)= \\psi \\left(\\vec{x}^1_u \\odot \\vec{x}^1_v, ( \\hat{\\mathcal{B}}_{uv}[1], \\hat{\\mathcal{A}}_{uv}[1,1] ) \\right) = \\hat{\\mathcal{A}}_{uv}[1,1]$. An MLP implementing the described $\\psi$ is only required to nullify parts of its input and apply an identity mapping on the remaining ones. This MLP trivially exists (it can be even manually constructed).\n\\end{proof}\n\nWe now move to the counterpart of the above result for BUDDY models.\n\\begin{proposition}\\label{prop:BUDDY-does-not-suffer}\n    Let $\\mathcal{M}_\\mathrm{BUDDY}$ be the family of BUDDY models as described per Equations~\\ref{eq:readout} and pre-processed features $\\mathbf{Z}= \\left[\\mathbf{X}^{(0)} \\mathbin\\Vert \\mathbf{X}^{(1)} \\mathbin\\Vert...\\mathbin\\Vert \\mathbf{X}^{(k)} \\right]$, where estimates are exact ($\\hat{\\mathcal{A}} \\equiv \\mathcal{A}, \\hat{\\mathcal{B}} \\equiv \\mathcal{B}$). $\\mathcal{M}_\\mathrm{BUDDY}$ \\emph{does not} suffer from the automorphic node problem.\n\\end{proposition}\n\\begin{proof}[Proof of Proposition~\\ref{prop:BUDDY-does-not-suffer}]\\label{proof:BUDDY-does-not-suffer}\n    In order to prove the Proposition it is sufficient to notice that the readout in the above Proof completely neglects node features, while only replicating in output the structural features $\\mathcal{A}$. This is also a valid BUDDY readout function, and allows BUDDY to output the same exact node-pair representations of the ELPH model described above. As we have observed, these are enough to disambiguate $(v_0, v_2), (v_0, v_3)$ from graph $C_6$ depicted in Figure~\\ref{fig:c6}. This concludes the proof.\n\\end{proof}\n\nIn these results we have leveraged the assumption that $\\mathcal{A}$ estimates are exact. We observe that, if the MinHash and HyperLogLog estimators are unbiased, it would be possible to choose a sample size large enough to provide distinct estimates for the two counts of interest, so that the above results continue to hold. Unbiased cardinality estimators are, for instance, described in~\\citep{ertl2017new}.\n\n  \n\n  \n\nA more precise assessment of the representational power of ELPH can be obtained by considering the node-pairs this model family is able to discriminate w.r.t. others.\n\n\\begin{lemma}\\label{lemma:elph-more-expressive-than-mpnn}\n    Let $\\mathcal{M}_\\mathrm{ELPH}$ be the family of ELPH models (Equations~\\ref{eq:sketches},~\\ref{eq:edge_features},~\\ref{eq:aggregation}, and~\\ref{eq:readout}), $\\mathcal{M}_\\mathrm{MPNN}$ that of Message Passing Neural Networks (Equation~\\ref{eq:mpnn_link_prediction}). $\\mathcal{M}_\\mathrm{ELPH}$ is more powerful than $\\mathcal{M}_\\mathrm{MPNN}$ ($\\mathcal{M}_\\mathrm{ELPH} \\sqsubseteq \\mathcal{M}_\\mathrm{MPNN}$). \n\\end{lemma}\n\\begin{proof}[Proof of Lemma~\\ref{lemma:elph-more-expressive-than-mpnn}]\n    We prove this Lemma by noticing that the ELPH architecture generalizes that of an MPNN, so that an ELPH model can learn to simulate a standard MPNN by ignoring the structural features. Specifically, ELPH defaults to an MPNN with (1) \n    \\begin{align}\n        \\phi^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\mathbf{x}_v^{(l-1)}, \\mathbf{e}_{u, v}^{(l)}\\right)=\\phi^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\mathbf{x}_v^{(l-1)}\\right),\n    \\end{align}\n    and (2) \n    \\begin{align}\\psi \\left(\\vec{x}^k_u \\odot \\vec{x}^k_v, \\{ \\hat{\\mathcal{B}}_{uv}[d], \\hat{\\mathcal{A}}_{uv}[d_u, d_v] : \\forall \\, d,d_u,d_v = 1, \\ldots, k  \\}\\right) = \\psi \\left(\\vec{x}^k_u \\odot \\vec{x}^k_v\\right).\n    \\end{align}\n    This entails that, anytime a specific MPNN instance distinguishes between two non-automorphic node-pairs, there exists an ELPH model which does so as well: the one which exactly simulates such MPNN.\n\\end{proof}\n\n\\begin{lemma}\\label{lemma:exists-link-distinguished-by-elph-BUDDY-not-by-mpnn}\n    There exist node-pairs distinguished by an ELPH or BUDDY model (with exact cardinality estimates) which are not distinguished by any MPNN model.\n\\end{lemma}\n\\begin{proof}[Proof of Lemma~\\ref{lemma:exists-link-distinguished-by-elph-BUDDY-not-by-mpnn}]\n    The Lemma is proved simply by considering non-automorphic pairs $(v_0, v_2)$, $(v_0, v_3)$ from graph $C_6$ depicted in Figure~\\ref{fig:c6}. We have already shown how there exists both an ELPH and BUDDY model (computing exact estimates of $\\mathcal{A}$, $\\mathcal{B}$) which separate the two. On the other hand, we have already observed how the nodes in the pairs are automorphic as all belonging to the same orbit. Thus, the two pairs cannot possibly be distinguished by any MPNN, since they suffer from the automorphic node problem~\\citep{srinivasan2019equivalence,zhang2021labeling}, as remarked in Proposition~\\ref{prop:mpnn-suffers}.\n\\end{proof}\n\n\\begin{proof}[Proof of Theorem~\\ref{thm:elph-strictly-more-expressive-than-mpnn}]\n    The Theorem follows directly from Lemmas~\\ref{lemma:elph-more-expressive-than-mpnn} and~\\ref{lemma:exists-link-distinguished-by-elph-BUDDY-not-by-mpnn}.\n\\end{proof}\n\nDoes this expressiveness result also extend to BUDDY? While we have proved BUDDY does not suffer from the automorphic node problem, its non-parametric message-passing scheme is such that this family of models do not generally subsume MPNNs. On one hand, in view of Lemma~\\ref{lemma:exists-link-distinguished-by-elph-BUDDY-not-by-mpnn}, we know there exist node-pairs separated by BUDDY but not by any MPNN; on the other, this does not exclude the presence of node-pairs for which the vice-versa is true.\n"
                }
            },
            "section 11": {
                "name": "Additional Experimental Details",
                "content": "\n\n",
                "subsection 11.1": {
                    "name": "Datasets and Their Properties",
                    "content": "\\label{sec:datasets}\n\nTable~\\ref{tab:subgraph properties} basic properties of the experimental datasets, together with the scaling of subgraph size with hops. Subgraph statistics are generated by expanding $k$-hop subgraphs around 1000 randomly selected links. Regular graphs scale as $\\deg^k$, however as these datasets are all complex networks the size of subgraphs grows far more rapidly than this, which poses serious problems for SGNNs. Furthermore, the size of subgraphs is highly irregular with high standard deviations making efficient parallelization in scalable architectures challenging. The one exception to this pattern is DDI. Due to the very high density of DDI, most two-hop subgraphs include almost every node.\n\n\n\n"
                },
                "subsection 11.2": {
                    "name": "Experimental Setup",
                    "content": "\nIn all cases, we use the largest connected component of the graph. %Appendix~\\ref{ap:datasets}.\nLP tasks require links to play dual roles as both supervision labels and message passing links. For all datasets, at training time the message passing links are equal to the supervision links, while at test and validation time, disjoint sets of links are held out for supervision that are never seen at training time. The test supervision links are also never seen at validation time, but for the Planetoid and ogbl-collab\\footnote{The OGB rules allow validation edges to be used for the ogbl-collab dataset.} datasets, the message passing edges at test time are the union of the training message passing edges and the validation supervision edges. OGB datasets have fixed splits whereas for Planetoid, random 70-10-20 percent train-val-test splits were generated. On DDI, NBFnet was trained without learned node embeddings and with a batch size of 5.     \n\n"
                },
                "subsection 11.3": {
                    "name": "Hyperparameters",
                    "content": "\nThe $p$ parameter used by \\emph{HyperLogLog} was 8 and the number of permutations used by \\emph{MinHashing} was 128.\n% In all experiments the adam optimizer~\\citep{kingma2014adam} was used.\n% hyperparameter tuning\nAll hyperparameters were tuned using Weights and Biases random search. The search space was over hidden dimension (64--512), learning rate (0.0001--0.01) and dropout (0--1), layers (1--3) and weight decay (0--0.001). Hyperparameters with the highest validation accuracy were chosen and results are reported on a test set that is used only once.\n\n"
                },
                "subsection 11.4": {
                    "name": "Space Complexity",
                    "content": "\n\\label{app:space_complexity}\nThe space complexity for ELPH is almost the same as a GCN based link predictor. We define the number of feature as $F$, the sketch size $H$ (sum of hll and minhash sketch sizes), number of nodes as $N$, the number of layers $L$, the batch size $B$ and the number of edges as $E$. We split link prediction into i)  learning node representations and ii) predicting  link probabilities from node representations. Learning node representations with GCN has complexity \n\\begin{align}\nE + LF^2 + LNF, \n\\end{align}\nwhere the second and third terms represent the weight matrices and the node representation respectively. ELPH has complexity, \n\\begin{align}\nE + LF^2 + LN(F + H)\n\\end{align}\nAnd so the only difference is the additional space required to store the node sketches. The GCN link predictor has space complexity \n\\begin{align}\nBF + F^2, \n\\end{align}\nWhere the first term is a batch of edges and the second term are the link predictor weight matrices. ELPH also uses structure features, which assuming the number of GNN layers and the number of hops are the same, take up L(L+2) space per node. The link prediction complexity of ELPH is then\n\\begin{align}\nB(F+L(L+2)) + F^2 + F(L(L+2)). \n\\end{align}\nTypically the number of layers L is 1,2,3 and so the additional overhead is small.\nThe space complexity of the BUDDY link predictor is the same as ELPH and node representations are built as a preprocessing step. The preprocessing has space complexity \n\\begin{align}\nE + LNF + LN + LNH,     \n\\end{align}\nwhich correspond to the adjacency matrix, the propagated node features, the hll cardinality estimates and the minhash and hll sketches respectively. The only difference between this and ELPH is that cardinality estimates are cached (LN) and no weight matrices are used.\n\n"
                },
                "subsection 11.5": {
                    "name": "Implementation Details",
                    "content": "\nOur code is implemented in PyTorch~\\citep{torch}, using PyTorch geometric~\\citep{torch_geo}. Code and instructions to reproduce the experiments are available at \\url{https://github.com/melifluos/subgraph-sketching}. We utilized either AWS p2 or p3 machines with 8 Tesla K80 and 8 Tesla V100 respectively to perform all the experiments in the paper.\n\n"
                }
            },
            "section 12": {
                "name": "More on Structural Features",
                "content": "\n\n",
                "subsection 12.1": {
                    "name": "Heuristic Methods for Link Prediction",
                    "content": "\n\\label{app:heuristics}\nHeuristic methods are classified by the receptive field and whether they measure neighborhood similarity or path length. \n% neighborhood similarity metrics\nThe simplest neighborhood similarity method is the 1-hop Common Neighbor (CN) count. Many other heuristics such as cosine similarity, the Jaccard index and the Probabilistic Mutual Information (PMI) differ from CN only by the choice of normalization. Adamic-Adar and Resource Allocation are two closely related second order heuristics that penalize neighbors by a function of the degree \n\\begin{align}\n    \\Gamma(u,v) = \\sum_{i \\in N(u) \\cap N(v)}\\frac{1}{f(|N(i)|)},\n\\end{align}\nwhere $N(u)$ are the neighbors of $u$.\n% path based metrics\nShortest path based heuristics are generally more expensive to calculate than neighborhood similarity as they require global knowledge of the graph to compute exactly. \nThe Katz index takes into account multiple paths between two nodes. Each path is given a weight of $\\alpha^d$, where $\\alpha$ is a hyperparameter attenuation factor and $d$ is the path length. Similarly Personalized PageRank (PPR) estimates landing probabilities of a random walker from a single source node. For a survey comparing 20 different LP heuristics see e.g.~\\citep{lu2011link}.\n\n"
                },
                "subsection 12.2": {
                    "name": "Subgraph Generation Complexity",
                    "content": "\n\\label{sec:subgraph_complexity}\n\nThe complexity for generating regular $k$-hop subgraph is $\\mathcal{O}(\\degree^k)$, where $\\degree$ is the degree of every node in the subgraph. For graphs that are approximately regular, with a well defined mean degree, the situation is slightly worse. However, in general we are interested in complex networks (such as social networks, recommendation systems or citation graphs) that are formed as a result of preferential attachment. Complex networks are typified by power law degree distributions of the form $p(\\degree) = \\degree^{-\\gamma}$. As a result the mean is only well defined if $\\gamma>2$ and the variance is finite only for $\\gamma>3$. As most real world complex networks fall into the range $2 < \\gamma < 3$, we have that the maximum degree is only bounded by the number of edges in the graph and thus, so is the complexity of subgraph generation. Table~\\ref{tab:subgraph properties} includes the average size of one thousand 1-hop and 2-hop randomly generated graphs for each dataset used in our experiments. In all cases, the size of subgraphs greatly exceeds the average degree baseline with very large variances.\n\n"
                },
                "subsection 12.3": {
                    "name": "Subgraph Sketches",
                    "content": "\n\\label{sec:sketches}\n\nWe make use of both the \\emph{HyperLogLog} and \\emph{MinHash} sketching schemes. The former is used to estimate the size of the union, while the latter estimates the Jaccard similarity between two sets. Together they can be used to estimate the size of set intersections.\n\n",
                    "subsubsection 12.3.1": {
                        "name": "Hyperloglog",
                        "content": "\n\\label{app:hll}\n% hyperloglog\nHyperLogLog efficiently estimates the cardinality of large sets. It accomplishes this by representing sets using a constant size data sketch. These sketches can be combined in time that is constant w.r.t the data size and linear in the sketch size using elementwise maximum to estimate the size of a set union. \n% The size of the sketch implies standard error of the estimate. Increasing the size of the sketch decreases the standard error of the estimate. \n\nThe algorithm takes the precision $p$ as a parameter. From $p$, it determines the number of registers to use. The sketch for a set $S$ is comprised of $m$ registers $M_{1} \\ldots M_{m}$ where $m = 2^{p}$. A hash function $h(s)$ maps elements from $S$ into an array of 64-bits. The algorithm uses the first $p$ bits of the hash to associate elements with a register. From the remaining bits, it computes the number of leading zeros, tracking the maximum number of leading zeros per register. Intuitively a large number of leading zero bits is less likely and indicates a higher cardinality and for a single register the expected cardinality for a set where the maximum number of leading zeros is $n$ is $2^n$. This estimate is highly noisy and there are several methods to combine estimates from different registers. We use hyperloglog++~\\citep{heule2013hyperloglog}, for which the standard error is numerically close to $1.04/sqrt(m)$ for large enough $m$.\n\nHyperloglog can be expressed in three functions Initialize, Union, and Card. Sketches are easily merged by populating a new set of registers with the element-wise max values for each register. To extract the estimate, the algorithm finds the harmonic mean of $2^{M[m]}$ for each of the the $m$ registers. This mean estimates the cardinality of the set divided by $m$. To find the estimated cardinality we multiply by $m$ and $\\alpha_m$. $\\alpha_m$ is used to correct multiplicative bias. Additional information about the computation of $\\alpha_m$ along with techniques to improve the estimate can be found in \\citep{hyperloglog,heule2013hyperloglog}. The full algorithm is presented in Algorithm~\\ref{alg:hll}.\n\n\\paragraph{Complexity}\nThe Initialize operation has a $\\mathcal{O}(m + |S|)$ running time. Union and Card are both $\\mathcal{O}(m)$ operations. The size of the sketch for a 64-bit hash is $6*2^p$ bits. \n \n"
                    },
                    "subsubsection 12.3.2": {
                        "name": "Minhashing",
                        "content": "\n% minhashing\nThe MinHash algorithm estimates the Jaccard index. It can similarly be expressed in three functions Initialize, Union, and $J$. The $p$ parameter is the number of permutations on each element of the set. $P_i(x)$ computes a permutation of input $x$ where each $i$ specifies a different permutation. The algorithm stores the minimum value for each of the $p$ permutations of all hashed elements. The Jaccard estimate of the similarity of two sets is given by the Hamming similarity of their sketches. The full algorithm is presented as Algorithm~\\ref{alg:minhash}.\n\n\\paragraph{Complexity}\nThe Initialize operation has a $\\mathcal{O}(np|S|)$ running time. Union and $J$ are both $\\mathcal{O}(np)$ operations. The size of the sketch is $np$ longs. Minhashing gives an unbiased estimate of the Jaccard with a variance given by the Cramer-Rao lower bound that scales as $\\mathcal{O}(1/np)$~\\citep{chamberlain2018real}.\n\n\\begin{algorithm}\n\\caption{HyperLogLog: Estimate cardinality}\n\\begin{algorithmic}\n\\State Parameter $p$ is used to control precision.\n\\State $m = 2^p$\n\\Procedure{HLLInitialize}{$S$}\n  \\For{$i \\in range(m)$}\n    \\State $M[i] = 0$\n  \\EndFor\n  \\For{$v \\in S$}\n    \\State $x = h(v)$\n    \\State $idx = \\langle x_{31}, \\ldots, x_{32-p} \\rangle_2$\n    \\State $w = \\langle x_{{31-p}}, \\ldots, x_{0} \\rangle_2$\n    \\State $M[idx] = max(M[idx], \\varrho(w)))$\n  \\EndFor\n  \\State \\Return $M$\n\\EndProcedure\n\\State\n\\Procedure{HLLUnion}{$M1$, $M2$}\n  \\For{$i \\in range(m)$}\n    \\State $M[i] = max(M1[i], M2[i]))$\n  \\EndFor\n  \\State \\Return $M$\n\\EndProcedure\n\\State\n\\Procedure{Card}{$M$}\n  \\State \\Return $\\alpha_m m^2 ( \\sum_{i=0}^m 2^{-M[i]} )^{-1}$\n\\EndProcedure\n\\end{algorithmic}\n\\label{alg:hll}\n\\end{algorithm}\n\n\n\\begin{algorithm}\n\\caption{MinHash: Estimate Jaccard Similarity}\n\\begin{algorithmic}\n\\State Parameter $np$ controls the number of permutations.\n\\Procedure{MinHashInitialize}{$S$}\n  \\For{$i \\in range(np)$}\n    \\State $M[i] = max value$\n  \\EndFor\n  \\For{$v \\in S$}\n    \\State $x = h(v)$\n    \\For{$i \\in range(np)$}\n      \\State $M[i] = min(M[i], P_i(x))$\n    \\EndFor\n  \\EndFor\n  \\State \\Return $M$\n\\EndProcedure\n\\State\n\\Procedure{MinHashUnion}{$M1$, $M2$}\n  \\For{$i \\in range(np)$}\n    \\State $M[i] = min(M1[i], M2[i]))$\n  \\EndFor\n  \\State \\Return $M$\n\\EndProcedure\n\\State\n\\Procedure{J}{$M1$, $M2$}\n  \\State $num\\_equal = 0$\n  \\For{$i \\in range(np)$}\n    \\If{$M1[i] = M2[i]$}\n      \\State $num\\_equal++$\n    \\EndIf\n  \\EndFor\n  \\State \\Return $num\\_equal / np$\n\\EndProcedure\n\\end{algorithmic}\n\\label{alg:minhash}\n\\end{algorithm}\n\n"
                    }
                },
                "subsection 12.4": {
                    "name": "Labeling Schemes",
                    "content": "\n\\label{app:labeling_schemes}\n\nEmpirically, we found the best performing labeling scheme to be DRNL, which scores each node in $S_{uv}$ based on it's distance to $u$ and $v$ with the caveat that when scoring the distance to $u$, node $v$ and all of its edges are masked and vice versa. This improves expressiveness as otherwise for positive edges every neighbor of $u$ would always be a 2-hop neighbor of $v$ and vice-versa. The result of masking is that distances can be very large even for 2-hop graphs (for instance one node may have an egonet that is a star graph with a ring around the outside. If the edges of the star are masked then the ring must be traversed). The first few DRNL values are\n\\begin{enumerate}\n    \\item disconnected nodes: $(\\infty,0), (0,\\infty) \\to 0$\n    \\item link nodes: $(0,1), (1,0) \\to 1$\n    \\item common neighbors: $(1,1) \\to 2$\n    \\item 12-common-neighbors $(1,2), (2,1) \\to 3$\n    \\item 2-hop common neighbors: $(2,2) \\to 4$\n\\end{enumerate}\nand the pattern has a hash function given by\n\\begin{align}\n    f_l(i) = 1 + \\min(d_{ui},d_{vi} + (d/2)[(d//2)+d\\%2)-1]\n\\end{align}\nwhere $d=d_{ui} + d_{vi}$. It is slightly suboptimal to assign infinite distances to 0, which is at least part of the reason that they are one-hot encode them as labels. Indeed, DE~\\citep{li2020distance} uses a max distance label, which they claim reduces overfitting.\n\n"
                },
                "subsection 12.5": {
                    "name": "Substructure Counting and Curvature",
                    "content": "\n\\label{sec:curvature}\nThe eight features depicted in Figure~\\ref{fig:intersections} can be used to learn local substructures. $\\mathcal{A}_{uv}[1,1]$ counts the number of triangles that $(u,v)$ participates in assuming that the edge $(u,v)$ exists. $\\mathcal{A}_{uv}[2,1]$ and $\\mathcal{A}_{uv}[1,2]$ will double count a four-cycle and single count a four-cycle with a single diagonal. The model can not distinguish a four-clique from two triangles or a five-clique from three triangles. $\\mathcal{A}_{uv}[2,2]$ counts five cycles. LP has also  recently been shown to be closely related to notions of discrete Ricci curvature on graphs~\\citep{topping2021understanding}, a connection that we plan exploring in future work. \n\n"
                },
                "subsection 12.6": {
                    "name": "Example Structure Features",
                    "content": "\n\n\n\nFigure~\\ref{fig:computedz} provides an example of how structure features are calculated from a subgraph. The figure provides the $z$ values for each node in an eight node subgraph. From these the structure features are calculated. As an example\n% $\\mathcal{A}_{67}[2][1]$ and $\\mathcal{B}_{67}[1]$. \n\n\\begin{align}\n    \\mathcal{A}_{67}[2,1] &= |\\{2,3,4,6,7\\} \\cap \\{2,8\\}| = |\\{2\\}| = 1\\\\\n    \\mathcal{B}_{67}[2] &= |\\{2,3,4,6,7\\} \\setminus \\{1,2,3,5,6,7,8\\}| = |\\{4\\}| = 1\n\\end{align}\n\nThe value of $\\mathcal{A}_{67}[2,1] = 1$ indicates that the is one node in common between the two-hop neighbors of node 6 and the 1-hop neighbors of node 7. The common node is node 2. Similarly $\\mathcal{B}_{67}[2] = 1$ means there is one element in the two-hop neighbors of 6 that is not in any of the k-hop neighbors of 7: node 4. \n\n% \\textcolor{olive}{\n% \\subsection{Full Results and Discussion}\n% }\n% The full results Table is given in Table~\\ref{tab:main_results}. For the OGB datasets (Collab, PPA, Citation2 and DDI) the choice of metric is dicated by OGB while for the Planetoid datasets HR@100 is used. The choice of metric is indicated in fourth row of the table.\n\n% \\begin{table*}[t]\n%     \\centering\n%     \\resizebox{\\textwidth}{!}{%\n%     \\begin{tabular}{l ccccccc}\n%     \\toprule \n%          &\n%          \\textbf{Cora} &  \n%          \\textbf{Citeseer} & \n%          \\textbf{Pubmed} &\n%          \\textbf{Collab} &\n         \n%          \\textcolor{olive}{\\textbf{PPA}} &\n%          \\textcolor{olive}{\\textbf{Citation2}} &\n%          \\textcolor{olive}{\\textbf{DDI}} \n%          \\\\\n         \n%                   \\#Nodes &\n\n%          2,708 & \n%          3,327 &\n%          18,717 &\n%          235,868 &\n%          \\textcolor{olive}{576,289} &\n%          \\textcolor{olive}{2,927,963} &\n%          \\textcolor{olive}{4267}\n%           \\\\\n         \n%          \\#Edges &\n%          5,278 & \n%          4,676 &\n%          44,327 &\n%          1,285,465 &\n%          \\textcolor{olive}{30,326,273} &\n%          \\textcolor{olive}{30,561,187} &\n%          \\textcolor{olive}{1,334,889} \n%           \\\\\n          \n%          avg deg &\n%          3.9 &\n%          2.74 & \n%          4.5 &\n%          5.45 &\n%          \\textcolor{olive}{52.62} &\n%          \\textcolor{olive}{10.44} &\n%          \\textcolor{olive}{312.84}\n%          \\\\\n          \n%          metric &\n\n%          HR@100 &\n%          HR@100 & \n%          HR@100 &\n%          HR@50 &\n%          \\textcolor{olive}{HR@100} &\n%          \\textcolor{olive}{MRR} &\n%          \\textcolor{olive}{HR@20}\n%          \\\\\n         \n%          splits &\n\n%          rand &\n%          rand & \n%          rand &\n%          time &\n%          \\textcolor{olive}{throughput} &\n%          \\textcolor{olive}{time} &\n%          \\textcolor{olive}{protein}\n%          \\\\ \\midrule\n          \n%          \\textbf{CN} & \n%          $33.92 {\\scriptstyle \\pm 0.46}$& \n%          $29.79 {\\scriptstyle \\pm 0.90}$& \n%          $23.13 {\\scriptstyle \\pm 0.15}$&\n%          $56.44 {\\scriptstyle \\pm 0.00}$&\n%          \\textcolor{olive}{$27.65 {\\scriptstyle \\pm 0.00}$}&\n%          \\textcolor{olive}{$51.47 {\\scriptstyle \\pm 0.00}$}&\n%          \\textcolor{olive}{$17.73 {\\scriptstyle \\pm 0.00}$}\n%          \\\\\n          \n%         \\textbf{AA} & \n%         $39.85 {\\scriptstyle \\pm 1.34}$&\n%         $35.19 {\\scriptstyle \\pm 1.33}$&\n%         $27.38 {\\scriptstyle \\pm 0.11}$&\n%         \\third{64.35}{0.00}&\n%         \\textcolor{olive}{$32.45 {\\scriptstyle \\pm 0.00}$}&\n%         \\textcolor{olive}{$51.89 {\\scriptstyle \\pm 0.00}$}&\n%         \\textcolor{olive}{$18.61 {\\scriptstyle \\pm 0.00}$}\\\\\n        \n%         \\textbf{RA} &\n%         $41.07 {\\scriptstyle \\pm 0.48}$ &\n%         $33.56 {\\scriptstyle \\pm 0.17}$&\n%         $27.03 {\\scriptstyle \\pm 0.35}$& \n%         $64.00 {\\scriptstyle \\pm 0.00}$&\n%         \\second{49.33}{0.00} & \n%         \\textcolor{olive}{$51.98 {\\scriptstyle \\pm 0.00}$}&\n%         \\textcolor{olive}{$27.60 {\\scriptstyle \\pm 0.00}$}\n%         \\\\ \\midrule\n        \n%         \\textbf{transE} &\n%         $67.40 {\\scriptstyle \\pm 1.60}$& \n%         $60.19 {\\scriptstyle \\pm 1.15}$&\n%         $36.67 {\\scriptstyle \\pm 0.99}$& \n%         $29.40 {\\scriptstyle \\pm 1.15}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/hhl5jwfi?workspace=user-tmarkovich\n%         \\textcolor{olive}{$22.69 {\\scriptstyle \\pm 0.49}$} &\n%         \\textcolor{olive}{$76.44 {\\scriptstyle \\pm 0.18}$} & \n%         \\textcolor{olive}{$6.65 {\\scriptstyle \\pm 0.20}$}\n%         \\\\\n          \n%         \\textbf{complEx} & \n%         $37.16 {\\scriptstyle \\pm 2.76}$& \n%         $42.72 {\\scriptstyle \\pm 1.68}$& \n%         $37.80 {\\scriptstyle \\pm 1.39}$&\n%         $53.91 {\\scriptstyle \\pm 0.50}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/0bbr9kt9?workspace=user-tmarkovich\n%         \\textcolor{olive}{$27.42 {\\scriptstyle \\pm 0.49}$} &\n%         \\textcolor{olive}{$72.83 {\\scriptstyle \\pm 0.38 }$} &\n%         \\textcolor{olive}{$8.68 {\\scriptstyle \\pm 0.36 }$}\n%         \\\\\n        \n%         \\textbf{DistMult} & \n%         $41.38 {\\scriptstyle \\pm 2.49}$& \n%         $47.65 {\\scriptstyle \\pm 1.68}$& \n%         $40.32 {\\scriptstyle \\pm 0.89}$&\n%         $51.00 {\\scriptstyle \\pm 0.54}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/u0mwexre/table?workspace=user-tmarkovich\n%         \\textcolor{olive}{$28.61 {\\scriptstyle \\pm 1.47} $}&\n%         \\textcolor{olive}{$66.95 {\\scriptstyle \\pm 0.40} $}&\n%         \\textcolor{olive}{$11.01 {\\scriptstyle \\pm 0.49} $}\n%          \\\\ \\midrule\n        \n%         \\textbf{GCN} & \n%         $66.79 {\\scriptstyle \\pm 1.65}$& \n%         $67.08 {\\scriptstyle \\pm 2.94}$&\n%         $53.02 {\\scriptstyle \\pm 1.39}$& \n%         $44.75 {\\scriptstyle \\pm 1.07}$&\n%         \\textcolor{olive}{$18.67 {\\scriptstyle \\pm 1.32}$}&\n%         \\textcolor{olive}{$84.74 {\\scriptstyle \\pm 0.21}$}&\n%         \\third{37.07}{5.07}\n%          \\\\\n          \n%         \\textbf{SAGE} & \n%         $55.02 {\\scriptstyle \\pm 4.03}$& \n%         $57.01 {\\scriptstyle \\pm 3.74}$& \n%         $39.66{\\scriptstyle \\pm 0.72}$& \n%         $48.10 {\\scriptstyle \\pm 0.81}$& \n%         \\textcolor{olive}{$16.55 {\\scriptstyle \\pm 2.40}$}&\n%         \\textcolor{olive}{$82.60 {\\scriptstyle \\pm 0.36}$}&\n%         \\second{53.90}{4.74}\\\\ \\midrule\n        \n%         \\textbf{SEAL} & \n%         \\second{81.71}{1.30}& \n%         \\second{83.89}{2.15}& \n%         \\first{75.54}{1.32}&\n%         \\second{64.74}{0.43}& \n%         \\third{48.80}{3.16}& \n%         \\first{87.67}{0.32}&\n%         \\textcolor{olive}{$30.56 {\\scriptstyle \\pm 3.86}$}\\\\ \n        \n%          \\textbf{NBFnet} & \n%          \\third{71.65}{2.27}&\n%          \\third{74.07}{1.75}&\n%          \\third{58.73}{1.99}&\n%          OOM&\n%          \\textcolor{olive}{OOM}&\n%          \\textcolor{olive}{OOM}&\n%          \\textcolor{olive}{$4.00 {\\scriptstyle \\pm 0.58}$}^*\\\\  \\midrule\n         \n%          \\textbf{hashing-ours} & \n%          \\first{88.00}{0.44}&\n%          \\first{92.93}{0.27}&\n%          \\second{74.10}{0.78}&\n%          \\first{65.94}{0.58}&\n%          \\first{49.85}{0.20}& \n%          \\second{87.56}{0.11}&\n%          \\first{78.51}{1.36}\n%          \\midrule\n%                   \\bottomrule\n% \\end{tabular}\n% }\n% \\caption{Results on link prediction benchmarks. Results added since the initial submission are indicated in \\textcolor{olive}{Green}, unless they are in the top three models, in which case they are colored by \\textbf{\\textcolor{red}{First}}, \\textbf{\\textcolor{blue}{Second}}, \\textbf{\\textcolor{violet}{Third}}. For the avoidance of doubt, all results for PPA, Citation2 and DDI are new and, with the exception of coloring, this results table is identical to the results table in the updated main paper. Confidence intervals are $\\pm$ one sd. Planetoid splits are random and the OGB splits are fixed. Where possible, baseline results are taken directly from the OGB leaderboard. $^*$NBFnet was trained without learned node embeddings and with a batch size of 5.}\n% \\label{tab:main_results}\n% \\vspace{-3mm}\n% \\end{table*}  \n\n"
                }
            },
            "section 13": {
                "name": "Additional Experiments",
                "content": "\n\\label{app:additional_exps}\n",
                "subsection 13.1": {
                    "name": "Runtimes and Discussion",
                    "content": "\n\n\\label{sec:runtimes}\n\nThe results in Tables~\\ref{tab:wall_time} and \\ref{tab:wall_time_extended} are obtained by running methods on a single Tesla K80 GPU on an AWS p2 machine. In all cases SEAL used GCN (fastest GNN) and parameters were taken from the SEAL OGB repo. For the OGB datasets, SEAL runtimes are estimated based on samples due to the high runtimes.\n\n% discussion of preprocessing\nTable~\\ref{tab:preproc} breaks BUDDY preprocessing times down into (i) generating hashes and (ii) propagating features for each node, where the same values are used for  training and inference and (iii) constructing structure features from hashes for each query edge, which has a separate cost for inference. We stress that both forms of preprocessing depend only on the dataset and so must only be done once ever for a fixed dataset and not e.g. after every epoch. This is akin to the static behavior of SEAL~\\citep{zhang2018link}, where subgraphs are constructed for each edge (both training and inference) as a preprocessing step.\n\n  \n\n\n\n"
                },
                "subsection 13.2": {
                    "name": "Ablation Studies",
                    "content": "\n\\label{app:ablation_studies}\nThis section contains ablation studies for (i) the affect of varying the number of \\emph{Minhash} permutations and the \\emph{HyperLogLog} $p$ parameter and (ii) removing either node features or structure features from BUDDY.\n\n",
                    "subsubsection 13.2.1": {
                        "name": "Hashing Parameter Ablation",
                        "content": "\n\n\nFigures~\\ref{fig:collab_hashing_abl} and \\ref{fig:planetoid_hashing_abl} are ablation studies of the hashing parameters that trade-off between accuracy and time/space complexity of the intersection estimates. The method is relatively insensitive to both parameters allowing smaller values to be chosen when space / time complexity is a constraint. Figure~\\ref{fig:minhash_abl} shows that good performance is achieved providing more than 16 minhash permutations are used, while Figure~\\ref{fig:hll_abl} shows that $p$ can be as low as 4 in the hyperloglog procedure. For Figure~\\ref{fig:planetoid_hashing_abl} values were calculated with no node features to emphasize the affect of only the hashing parameters. This was not required for Figure~\\ref{fig:collab_hashing_abl} because relatively speaking the node features are less important for Collab (See Table~\\ref{tab:feature_ablation}).\n\n\n\n\n\n\n\n% runtimes\nData sketching typically introduces a tradeoff between estimation accuracy and time and space complexity. However, in our model, the time complexity for generating structure features from hashes is negligible at both training and inference time compared to the cost of a forward pass of the MLP. The only place where these parameters have an appreciable impact on runtimes is in preprocessing the hashes. When preprocessing the hashes, the cost of generating hyperloglog sketches is also negligible compared to the cost of the minhash sketches. The relationship between the number of minhash permutations and the runtime to generate the hashes is shown in Figure~\\ref{fig:hash_runtimes}.\n\n\n\n\n"
                    },
                    "subsubsection 13.2.2": {
                        "name": "Feature Ablation",
                        "content": "\n\nTable~\\ref{tab:feature_ablation} shows the degradation in performance of BUDDY with either structure features or node features removed with all hyperparameters held fixed. DDI has no node features and BUDDY did not use the node features from PPA, which are one-hot species labels. For datasets Collab and PPA, the structure features dominate performance. For the Citation dataset, the contribution of structure features and node features is almost equal, with a relatively small incremental benefit of adding a second feature class. For the Planetoid datasets adding structure features gives a significant, but relatively small incremental benefit beyond the node features. It should also be noted that combining node and structure features dramatically reduces the variance over runs for the Planetoid and Collab datasets.\n\n         \n\n"
                    }
                }
            },
            "section 14": {
                "name": "Full BUDDY Algorithm",
                "content": "\n\nA sketch of the full algorithm is given in Algorithm~\\ref{alg:full_algo}\n\n\\begin{algorithm}\n\\caption{Complete Procedure}\n\\begin{algorithmic}\n\\State Preprocess structure features and cache propagated node features with Graph $G$ and features $X$\n\\Procedure{Preprocessing}{G, X}\n  \\State $H1 = \\textsc{MinHashInitialize}(G)$\n  \\State $H2 = \\textsc{HLLInitialize}(G)$\n  \\State $X' = Propagate(X)$\n\\EndProcedure\n\n\\State Generate edge probability predictions $y$ using an MLP \n\\Procedure{Predict}{H1,H2,X'}\n  \\For{edge $(u,v) \\in$ epoch}\n    \\State $SF_{u,v} = GetStructureFeatures(u,v, H1, H2)$\n    \\State $x_u = GetNodeFeatures(u, X')$\n    \\State $x_v = GetNodeFeatures(v, X')$\n    \\State $y = \\textrm{MLP}(SF_{u,v}, x_u, x_v)$\n  \\EndFor\n\\EndProcedure\n\n\\end{algorithmic}\n\\label{alg:full_algo}\n\\end{algorithm}\n\n\n"
            },
            "section 15": {
                "name": "Learning curves",
                "content": "\n\\label{app:learning_curves}\n\n We provide learning curves in terms of number of epochs in Figures~\\ref{fig:cora_batch} - \\ref{fig:ddi_batch}. \n%  For the ogbl datasets, we used the results from the leaderboard and thus do not have curves for methods we used for comparison. \n%  The learning curves versus epoch can be seen in Figures~\\ref{fig:cora_batch} - \\ref{fig:ddi_batch} and versus wall clock time in Figures~\\ref{fig:cora_time} - \\ref{fig:ddi_time}.\n \n \\newcommand{\\curveCpation}{Loss and train-val-test learning curves as a function of training epoch, averaged over restarts. Solid line represents mean value and shadowed region shows one standard deviation. }\n \n\n \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n"
            },
            "section 16": {
                "name": "Societal Impact",
                "content": "\n\nWe study LP in graph-structured datasets focusing primarily on methods rather than  applications. Our method, in principle, may be employed in industrial recommendation systems. We have no evidence that our method enhances biases, but were it to be deployed, checks would need to be put in place that existing biases were not amplified.\n% due to distribution shifts in inference data~\\citep{ferrara2022link}.\n\n%   \\newcommand{\\curveCpationTime}{Learning curves of various metrics as a function of wall clock time, averaged over multiple restarts. Solid line represents mean value and shadowed region shows one standard deviation. }\n \n%  \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\textwidth]{learning_curves/cora_time.pdf}\n%     \\caption{\\curveCpationTime Cora dataset.}\n%     \\label{fig:cora_time}\n% \\end{figure}\n\n%  \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\textwidth]{learning_curves/citeseer_time.pdf}\n%     \\caption{\\curveCpationTime Citeseer dataset.}\n%     \\label{fig:citeseer_time}\n% \\end{figure}\n\n%  \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\textwidth]{learning_curves/pubmed_time.pdf}\n%     \\caption{\\curveCpationTime Pubmed dataset.}\n%     \\label{fig:pubmed_time}\n% \\end{figure}\n\n%  \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\textwidth]{learning_curves/collab_time.pdf}\n%     \\caption{\\curveCpationTime ogbl-Collab dataset.}\n%     \\label{fig:collab_time}\n% \\end{figure}\n\n%  \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\textwidth]{learning_curves/ppa_time.pdf}\n%     \\caption{\\curveCpationTime ogbl-PPA dataset.}\n%     \\label{fig:ppa_time}\n% \\end{figure}\n\n%  \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\textwidth]{learning_curves/citation_time.pdf}\n%     \\caption{\\curveCpationTime ogbl-Citation2 dataset.}\n%     \\label{fig:citation_time}\n% \\end{figure}\n\n%  \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\textwidth]{learning_curves/ddi_time.pdf}\n%     \\caption{\\curveCpationTime ogbl-DDI dataset.}\n%     \\label{fig:ddi_time}\n% \\end{figure}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% \\section*{Checklist}\n\n% \\begin{enumerate}\n\n\n% \\item For all authors...\n% \\begin{enumerate}\n%   \\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n%     \\answerYes{}\n%   \\item Did you describe the limitations of your work?\n%     \\answerYes{See Conclusion}\n%   \\item Did you discuss any potential negative societal impacts of your work?\n%     \\answerYes{See Societal Impact}\n%   \\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\n%     \\answerYes{}\n% \\end{enumerate}\n\n\n% \\item If you are including theoretical results...\n% \\begin{enumerate}\n%   \\item Did you state the full set of assumptions of all theoretical results?\n%     \\answerNA{}\n%         \\item Did you include complete proofs of all theoretical results?\n%     \\answerNA{}\n% \\end{enumerate}\n\n\n% \\item If you ran experiments...\n% \\begin{enumerate}\n%   \\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\n%     \\answerYes{Will be included in supplemental material}\n%   \\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\n%     \\answerYes{See Experiments section}\n%         \\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\n%     \\answerYes{In all experiments we report one standard deviation computed from five restarts.}\n%         \\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\n%     \\answerYes{See Experiments}\n% \\end{enumerate}\n\n\n% \\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n% \\begin{enumerate}\n%   \\item If your work uses existing assets, did you cite the creators?\n%     \\answerYes{We have cited all the relevant frameworks as well as datasets.}\n%   \\item Did you mention the license of the assets?\n%     \\answerNo{We have cited all the relevant frameworks as well as datasets.}\n%   \\item Did you include any new assets either in the supplemental material or as a URL?\n%     \\answerYes{We are going to include the code in the the supplemental material.}\n%   \\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\n%     \\answerNA{}\n%   \\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\n%     \\answerNA{}\n% \\end{enumerate}\n\n\n% \\item If you used crowdsourcing or conducted research with human subjects...\n% \\begin{enumerate}\n%   \\item Did you include the full text of instructions given to participants and screenshots, if applicable?\n%     \\answerNA{}\n%   \\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\n%     \\answerNA{}\n%   \\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\n%     \\answerNA{}\n% \\end{enumerate}\n\n\n% \\end{enumerate}\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n% \\pagebreak\n\n% \\appendix\n\n\n% \\section{Appendix}\n\n% \\subsection{Dataset Properties}\n\n% Table~\\ref{tab:subgraph properties} shows the scaling of subgraph size with hops\n\n% \\begin{table*}[t]\n%     \\centering\n%     \\resizebox{\\textwidth}{!}{%\n%     \\begin{tabular}{l cccccc}\n%     \\toprule \n%          &\n%          \\textbf{Cora} &  \n%          \\textbf{Citeseer} & \n%          \\textbf{Pubmed} &\n%          \\textbf{Collab} &\n%          \\textbf{PPA} &\n%          \\textbf{Citation2}\\\\\n         \n%                   \\#Nodes &\n\n%          2,708 & \n%          3,327 &\n%          18,717 &\n%          235,868 &\n%          576,289 &\n%          2,927,963\n%           \\\\\n         \n%          \\#Edges &\n%          5,278 & \n%          4,676 &\n%          44,327 &\n%          1,285,465 &\n%          30,326,273 &\n%          30,561,187\n%           \\\\\n          \n%          avg deg &\n%          3.9 &\n%          2.74 & \n%          4.5 &\n%          5.45 &\n%          52.62 &\n%          10.44 \\\\\n         \n%          1-hop size &\n\n%          $12 {\\scriptstyle \\pm 15}$ &\n%          $8 {\\scriptstyle \\pm 8}$ & \n%          $12 {\\scriptstyle \\pm 17}$ &\n%          time &\n%          throughput &\n%          time \\\\\n         \n%         2-hop size &\n\n%          $127 {\\scriptstyle \\pm 131}$ &\n%          $58 {\\scriptstyle \\pm 92}$ & \n%          $260 {\\scriptstyle \\pm 432}$ &\n%          time &\n%          throughput &\n%          time \\\\ \\midrule\n         \n\n         \n% \\end{tabular}\n% }\n% \\caption{Results on link prediction benchmarks. The Top three models are colored by \\textbf{\\textcolor{red}{First}}, \\textbf{\\textcolor{blue}{Second}}, \\textbf{\\textcolor{violet}{Third}}. Confidence intervals are $\\pm$ one standard deviation. Splits for the Planetoid datasets are random and OGB datasets use the fixed OGB splits. Where possible, baseline results are taken directly from the OGB leaderboard}\n% \\label{tab:subgraph properties}\n% \\end{table*}           \n\n% \\subsection{Heuristic Methods for Link Prediction}\n\n% Heuristic methods are classified by the receptive field and whether they measure neighborhood similarity or path length. \n% % neighborhood similarity metrics\n% The simplest neighborhood similarity method is the 1-hop Common Neighbor (CN) count. Many other heuristics such as cosine similarity, the Jaccard index and the Probabilistic Mutual Information (PMI) differ from CN only by the choice of normalization. Adamic-Adar and Resource Allocation are two second order heuristics in the same family that penalize neighbors by a function of the degree \n% \\begin{align}\n%     \\Gamma(u,v) = \\sum_{i \\in N(u) \\cap N(v)}\\frac{1}{f(|N(i)|)},\n% \\end{align}\n% where $N(u)$ are the neighbors of $u$.\n% % path based metrics\n% Shortest path based heuristics are generally more expensive to calculate than neighborhood similarity as they require global knowledge of the graph to compute exactly. \n% The Katz index takes into account multiple paths between two nodes. Each path is given a weight of $\\alpha^d$, where $\\alpha$ is a hyperparameter attenuation factor and $d$ is the path length. Similarly Personalized PageRank (PPR) estimates landing probabilities of a random walker from a single source node. For a survey comparing 20 different LP heuristics see e.g.~\\citep{lu2011link}.\n\n% \\subsection{Subgraph Generation Complexity}\n\n% \\subsection{Subgraph Sketches}\n\n% \\begin{algorithm}\n% \\caption{HyperLogLog: Estimate cardinality}\\label{alg:hll}\n% \\begin{algorithmic}\n% \\State Parameter $p$ is used to control precision.\n% \\State $m = 2^p$\n% \\Procedure{HLLInitialize}{$S$}\n%   \\For{$i \\in range(m)$}\n%     \\State $M[i] = 0$\n%   \\EndFor\n%   \\For{$v \\in S$}\n%     \\State $x = h(v)$\n%     \\State $idx = \\langle x_{31}, \\ldots, x_{32-p} \\rangle_2$\n%     \\State $w = \\langle x_{{31-p}}, \\ldots, x_{0} \\rangle_2$\n%     \\State $M[idx] = max(M[idx], \\varrho(w)))$\n%   \\EndFor\n%   \\State \\Return $M$\n% \\EndProcedure\n% \\State\n% \\Procedure{HLLUnion}{$M1$, $M2$}\n%   \\For{$i \\in range(m)$}\n%     \\State $M[i] = max(M1[i], M2[i]))$\n%   \\EndFor\n%   \\State \\Return $M$\n% \\EndProcedure\n% \\State\n% \\Procedure{Card}{$M$}\n%   \\State \\Return $\\alpha_m m^2 ( \\sum_{i=0}^m 2^{-M[i]} )^{-1}$\n% \\EndProcedure\n% \\end{algorithmic}\n% \\end{algorithm}\n\n\n% \\begin{algorithm}\n% \\caption{MinHash: Estimate Jaccard Similarity}\\label{alg:hll}\n% \\begin{algorithmic}\n% \\State Parameter $np$ controls the number of permutations.\n% \\Procedure{MinHashInitialize}{$S$}\n%   \\For{$i \\in range(np)$}\n%     \\State $M[i] = max value$\n%   \\EndFor\n%   \\For{$v \\in S$}\n%     \\State $x = h(v)$\n%     \\For{$i \\in range(np)$}\n%       \\State $M[i] = min(M[i], P_i(x))$\n%     \\EndFor\n%   \\EndFor\n%   \\State \\Return $M$\n% \\EndProcedure\n% \\State\n% \\Procedure{MinHashUnion}{$M1$, $M2$}\n%   \\For{$i \\in range(np)$}\n%     \\State $M[i] = min(M1[i], M2[i]))$\n%   \\EndFor\n%   \\State \\Return $M$\n% \\EndProcedure\n% \\State\n% \\Procedure{J}{$M1$, $M2$}\n%   \\State $num\\_equal = 0$\n%   \\For{$i \\in range(np)$}\n%     \\If{$M1[i] = M2[i]$}\n%       \\State $num\\_equal++$\n%     \\EndIf\n%   \\EndFor\n%   \\State \\Return $num\\_equal / np$\n% \\EndProcedure\n% \\end{algorithmic}\n% \\end{algorithm}\n\n% \\subsection{Structure Features}\n\n% Empirically, the best performing labeling trick is DRNL, which scores each node in $SG(u,v)$ based on it's distance to $u$ and $v$ with the important caveat that when scoring the source, all destination edges are masked and vice versa. This improves expressiveness as otherwise every neighbor of $u$ would always be a 2-hop neighbor of $v$ and vice-versa. The result of this masking is that distances can be very large even for 2-hop graphs (imagine a star graph with a ring around the outside, then remove the star edges). The first few DRNL values are\n% \\begin{enumerate}\n%     \\item disconnected nodes: $(\\infty,0), (0,\\infty) \\to 0$\n%     \\item link nodes: $(0,1), (1,0) \\to 1$\n%     \\item common neighbors: $(1,1) \\to 2$\n%     \\item 12-common-neighbors $(1,2), (2,1) \\to 3$\n%     \\item 2-hop common neighbors: $(2,2) \\to 4$\n% \\end{enumerate}\n% and the pattern has a hash function given by\n% \\begin{align}\n%     f_l(i) = 1 + \\min(d_{src},d_{dst} + (d/2)[(d//2)+d\\%2)-1]\n% \\end{align}\n% where $d=d_{src} + d_{dst}$. It is slightly sub-optimal to assign infinite distances to 0, which is at least part of the reason that they have to one-hot encode them as labels. Indeed, DE~\\citep{li2020distance} uses a max distance label, which they claim reduces overfitting.\n\n% \\subsection{Example Structure Features}\n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=1\\textwidth]{graph_with_z_features.png}\n%     \\caption{Computed z features}\n%     \\label{fig:computedz}\n%     % https://docs.google.com/drawings/d/11Fq3gUK0OcwGaN4TRadBYQOhZGyd3n3Z-M62ESzpJx4/edit\n% \\end{figure}\n\n% Figure~\\ref{fig:computedz} provides an example graph with the values of z provided. Using these values we compute the structure features.\n\n% \\begin{align}\n%     \\mathcal{A}_{67}[2][1] = |\\{2,3,4,6,7\\} \\cap \\{2,8\\}| = |\\{2\\}| = 1\\\\\n%     \\mathcal{B}_{67}[1][2] = |\\{2,3,4,6,7\\} \\setminus \\{1,2,3,5,6,7,8\\}| = |\\{4\\}| = 1\n% \\end{align}\n\n"
            }
        },
        "tables": {
            "tab:complexity": "\\begin{wraptable}{r}{0.6\\textwidth}\n\n    \\centering\n    % \\vspace{-5mm}\n    \\caption{Time complexity. $N$ and $E$ are the number of nodes and edges respectively. We use $d$-dimensional node features, $k$ hops for propagation and sketches of size $h$.}\n    \\resizebox{0.55\\textwidth}{!}{%\n    \\begin{tabular}{l cccc}\n    \\toprule \n    \n         \\textbf{Complexity} &  \n         \\textbf{SEAL}&\n         \\textbf{NBFNet}&\n         \\textbf{BUDDY} \\\\\n         \\toprule\n         \n         Preprocessing &$\\mathcal{O}(1)$  &$\\mathcal{O}(1)$ & $\\mathcal{O}(kE(d + h))$ \\\\\n         \n         Training (1 link) &$\\mathcal{O}(Ed^2)$ & $\\mathcal{O}(Ed + N d^2)$ & $\\mathcal{O}(k^2h + kd^2)$ \\\\\n         \n         Inference &$\\mathcal{O}(Ed^2)$ & $\\mathcal{O}(Ed + N d^2)$ & $\\mathcal{O}(k^2h + kd^2)$\n         \n\\end{tabular}\n}\n\\quad\n\\vspace{1mm}\n\\label{tab:complexity}\n% \\vspace{-5mm}\n\\end{wraptable}",
            "tab:main_results": "\\begin{table*}[t]\n    \\centering\n    \\caption{Results on link prediction benchmarks. The top three models are colored by \\textbf{\\textcolor{red}{First}}, \\textbf{\\textcolor{blue}{Second}}, \\textbf{\\textcolor{violet}{Third}}.  Where possible, baseline results are taken directly from the OGB leaderboard.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{l ccccccc}\n    \\toprule \n         &\n         \\textbf{Cora} &  \n         \\textbf{Citeseer} & \n         \\textbf{Pubmed} &\n         \\textbf{Collab} &\n         \\textbf{PPA} &\n         \\textbf{Citation2} &\n         \\textbf{DDI} \\\\\n         \n         %          \\#Nodes &\n\n         % 2,708 & \n         % 3,327 &\n         % 18,717 &\n         % 235,868 &\n         % 576,289 &\n         % 2,927,963 &\n         % 4267\n         %  \\\\\n         \n         % \\#Edges &\n         % 5,278 & \n         % 4,676 &\n         % 44,327 &\n         % 1,285,465 &\n         % 30,326,273 &\n         % 30,561,187 &\n         % 1,334,889 \n         %  \\\\\n          \n         % avg deg &\n         % 3.9 &\n         % 2.74 & \n         % 4.5 &\n         % 5.45 &\n         % 52.62 &\n         % 10.44 &\n         % 312.84\n         % \\\\\n          \n          Metric &\n\n          HR@100 &\n          HR@100 & \n          HR@100 &\n          HR@50 &\n          HR@100 &\n          MRR &\n          HR@20\n          %\\\\\n         \n         % splits &\n\n         % rand &\n         % rand & \n         % rand &\n         % time &\n         % throughput &\n         % time &\n         % protein\n         \\\\ \\midrule\n          \n         \\textbf{CN} & \n         $33.92 {\\scriptstyle \\pm 0.46}$& \n         $29.79 {\\scriptstyle \\pm 0.90}$& \n         $23.13 {\\scriptstyle \\pm 0.15}$&\n         $56.44 {\\scriptstyle \\pm 0.00}$&\n         $27.65 {\\scriptstyle \\pm 0.00}$&\n         $51.47 {\\scriptstyle \\pm 0.00}$&\n         $17.73 {\\scriptstyle \\pm 0.00}$\n         \\\\\n          \n        \\textbf{AA} & \n        $39.85 {\\scriptstyle \\pm 1.34}$&\n        $35.19 {\\scriptstyle \\pm 1.33}$&\n        $27.38 {\\scriptstyle \\pm 0.11}$&\n        $64.35 {\\scriptstyle \\pm 0.00}$&\n        $32.45 {\\scriptstyle \\pm 0.00}$&\n        $51.89 {\\scriptstyle \\pm 0.00}$&\n        $18.61 {\\scriptstyle \\pm 0.00}$\\\\\n        \n        \\textbf{RA} &\n        $41.07 {\\scriptstyle \\pm 0.48}$ &\n        $33.56 {\\scriptstyle \\pm 0.17}$&\n        $27.03 {\\scriptstyle \\pm 0.35}$& \n        $64.00 {\\scriptstyle \\pm 0.00}$&\n        \\second{49.33}{0.00} & \n        $51.98 {\\scriptstyle \\pm 0.00}$&\n        $27.60 {\\scriptstyle \\pm 0.00}$\n        \\\\ \\midrule\n        \n        \\textbf{transE} &\n        $67.40 {\\scriptstyle \\pm 1.60}$& \n        $60.19 {\\scriptstyle \\pm 1.15}$&\n        $36.67 {\\scriptstyle \\pm 0.99}$& \n        $29.40 {\\scriptstyle \\pm 1.15}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/hhl5jwfi?workspace=user-tmarkovich\n        $22.69 {\\scriptstyle \\pm 0.49}$ &\n        $76.44 {\\scriptstyle \\pm 0.18}$ & \n        $6.65  {\\scriptstyle \\pm 0.20}$\n        \\\\\n          \n        \\textbf{complEx} & \n        $37.16 {\\scriptstyle \\pm 2.76}$& \n        $42.72 {\\scriptstyle \\pm 1.68}$& \n        $37.80 {\\scriptstyle \\pm 1.39}$&\n        $53.91 {\\scriptstyle \\pm 0.50}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/0bbr9kt9?workspace=user-tmarkovich\n        $27.42 {\\scriptstyle \\pm 0.49}$ &\n        $72.83 {\\scriptstyle \\pm 0.38}$ &\n        $8.68 {\\scriptstyle \\pm 0.36 }$\n        \\\\\n        \n        \\textbf{DistMult} & \n        $41.38 {\\scriptstyle \\pm 2.49}$& \n        $47.65 {\\scriptstyle \\pm 1.68}$& \n        $40.32 {\\scriptstyle \\pm 0.89}$&\n        $51.00 {\\scriptstyle \\pm 0.54}$& % https://wandb.ai/link-prediction/link-prediction/sweeps/u0mwexre/table?workspace=user-tmarkovich\n        $28.61 {\\scriptstyle \\pm 1.47} $&\n        $66.95 {\\scriptstyle \\pm 0.40} $&\n        $11.01 {\\scriptstyle \\pm 0.49} $\n         \\\\ \\midrule\n        \n        \\textbf{GCN} & \n        $66.79 {\\scriptstyle \\pm 1.65}$& \n        $67.08 {\\scriptstyle \\pm 2.94}$&\n        $53.02 {\\scriptstyle \\pm 1.39}$& \n        $47.14 {\\scriptstyle \\pm 1.45}$&\n        $18.67 {\\scriptstyle \\pm 1.32}$&\n        $84.74 {\\scriptstyle \\pm 0.21}$&\n        $37.07 {\\scriptstyle \\pm 5.07}$\n         \\\\\n          \n        \\textbf{SAGE} & \n        $55.02 {\\scriptstyle \\pm 4.03}$& \n        $57.01 {\\scriptstyle \\pm 3.74}$& \n        $39.66 {\\scriptstyle \\pm 0.72}$& \n        $54.63 {\\scriptstyle \\pm 1.12}$& \n        $16.55 {\\scriptstyle \\pm 2.40}$&\n        $82.60 {\\scriptstyle \\pm 0.36}$&\n        $53.90 {\\scriptstyle \\pm 4.74}$ \\\\ \\midrule\n\n        \\textbf{Neo-GNN} & \n        $80.42 {\\scriptstyle \\pm 1.31} $ &\n        \\third{84.67}{2.16} &\n        \\third{73.93}{1.19} &\n        $62.13 {\\scriptstyle \\pm 0.5}$& \n        \\third{49.13}{0.60}& \n        \\third{87.26}{0.84}&\n        \\third{63.57}{3.52}\\\\ \n        \n        \\textbf{SEAL} & \n        \\third{81.71}{1.30}& \n        $83.89 {\\scriptstyle \\pm 2.15}$ & \n        \\first{75.54}{1.32}&\n        \\third{64.74}{0.43}& \n        $48.80 {\\scriptstyle \\pm 3.16}$& \n        \\first{87.67}{0.32}&\n        $30.56 {\\scriptstyle \\pm 3.86}$\\\\ \n        \n         \\textbf{NBFnet} & \n         $71.65 {\\scriptstyle \\pm 2.27}$&\n         $74.07 {\\scriptstyle \\pm 1.75}$&\n         $58.73 {\\scriptstyle \\pm 1.99}$&\n         OOM&\n         OOM&\n         OOM&\n         $4.00 {\\scriptstyle \\pm 0.58}$\\\\  \\midrule\n\n         \\textbf{ELPH} & \n         \\second{87.72}{2.13}&\n         \\first{93.44}{0.53}&\n         $72.99 {\\scriptstyle \\pm 1.43}$ &\n         \\first{66.32}{0.40}&\n         OOM&\n         OOM&\n         \\first{83.19}{2.12}\n         \\\\ \n         \n         \\textbf{BUDDY} & \n         \\first{88.00}{0.44}&\n         \\second{92.93}{0.27}&\n         \\second{74.10}{0.78}&\n         \\second{65.94}{0.58}&\n         \\first{49.85}{0.20}& %https://wandb.ai/link-prediction/link-prediction/sweeps/hv9hfjzf/table?workspace=user-bchamberlain\n         \\second{87.56}{0.11}&\n        %  $60.691 {\\scriptstyle \\pm 0.00}$& %https://wandb.ai/link-prediction/link-prediction/sweeps/2sjx73zm/table?workspace=user-tmarkovich\n        %  $51.89 {\\scriptstyle \\pm 0.00}$&\n         %https://wandb.ai/link-prediction/link-prediction/sweeps/6qmxk7b5?workspace=user-bchamberlain\n         \\second{78.51}{1.36} \\\\\n         % \\midrule\n         \\bottomrule\n\\end{tabular}\n}\n\\label{tab:main_results}\n\\vspace{-3mm}\n\\end{table*}",
            "tab:wall_time": "\\begin{wraptable}{R}{0.6\\textwidth}\n\\vspace{-4mm}\n\\caption{Wall times. Training time is one epoch. Inference time is the full test set. \n% For reference GCN takes ~4s to train and 0.1s inference on Pubmed and is OOM on Citation\n}\n    \\resizebox{0.6\\textwidth}{!}{%\n    \\begin{tabular}{ll cccccc}\n    \\toprule \n        \\textbf{dataset} &\n         \\textbf{time (s)} &  \n         \\textbf{SEAL dyn}&\n         \\textbf{SEAL stat}&\n         \\textbf{ELPH} &\n         \\textbf{BUDDY} & \n         \\textbf{GCN} \\\\         \n         \\toprule\n         \n         % & preproc &0 & 57 & 0 &0.4 \\\\\n         \n         % Cora & train &9.2  & 5 & 1.6 & 0.7\\\\\n         \n         % & inference &3.0 & 1.5 & 0.05 & 0.04 \\\\\n\n         % \\midrule\n\n         % & Preprocessing &0 & 48 & 0 & 0.7 \\\\\n         \n         % Citeseer & Training (1 epoch) & 13.3 & 4.5 & 0.9 &0.5\\\\\n         \n         % & Inference & 19.6 & 1.5 & 0.03 & 0.07 \\\\\n\n         % \\midrule\n\n         & preproc &0  & 630 & 0 & 5 & 0 \\\\\n         \n         Pubmed & train &70 & 30 & 25 & 1 & 4\\\\\n         \n         & inference &23 & 9 & 0.1 & 0.06 & 0.02 \\\\     \n\n        \\midrule\n\n         % & preproc &0  & & 0& 42.6 \\\\\n         \n         % Collab & train & 1e3-1e4 & OOM & 2060 & 105 \\\\\n         \n         % & inference & 1e3-1e4 &  & 1.5 & 1.1 \\\\\n\n         % \\midrule\n\n         % & Preprocessing &0  &0 &  & 840 \\\\\n         \n         % PPA & Training (1 epoch) & 1e5-2e5 & OOM & OOM & 75 \\\\\n         \n         % & Inference & 2e4 & OOM & & 18.2 \\\\\n\n         %  \\midrule\n\n         & preproc &0  & $\\sim$ 3,000,000 &  & 1,200 \\\\\n         \n         Citation & train  & $\\sim$300,000 & $\\sim$200,000 & OOM & 1,500 & OOM \\\\\n         \n         & inference & $\\sim$300,000 & \n         $\\sim$100,000 & & 300 \\\\\n\n        \\bottomrule\n         \n\\end{tabular}\n}\n% \\quad\n\\vspace{-3mm}\n\n\\label{tab:wall_time}\n\\end{wraptable}",
            "tab:c6-node-auto": "\\begin{table*}[t]\n    \\centering\n    \\caption{Images, for each node, according to the $12$ automorphisms for graph $C_6$. See Figure \\ref{fig:c6}.}\n    %\\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{l|cccccccccccc}\n    \\toprule\n    vertex & $A_1$ & $A_2$ & $A_3$ & $A_4$ & $A_5$ & $A_6$ & $A_7$ & $A_8$ & $A_9$ & $A_{10}$ & $A_{11}$ & $A_{12}$ \\\\\n    \\midrule\n    0 & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 & 5 & 5  \\\\\n    1 & 1 & 5 & 0 & 2 & 1 & 3 & 2 & 4 & 3 & 5 & 0 & 4  \\\\\n    2 & 2 & 4 & 5 & 3 & 0 & 4 & 1 & 5 & 2 & 0 & 1 & 3  \\\\\n    3 & 3 & 3 & 4 & 4 & 5 & 5 & 0 & 0 & 1 & 1 & 2 & 2  \\\\\n    4 & 4 & 2 & 3 & 5 & 4 & 0 & 5 & 1 & 0 & 2 & 3 & 1  \\\\\n    5 & 5 & 1 & 2 & 0 & 3 & 1 & 4 & 2 & 5 & 3 & 4 & 0 \\\\\n    \\bottomrule\n    \\end{tabular}%}\n\\label{tab:c6-node-auto}\n\\end{table*}",
            "tab:c6-node-pair-auto": "\\begin{table*}[t]\n    \\centering\n    \\caption{Images, for each node-pair, according to the $12$ induced node-pair automorphisms for graph $C_6$. Pairs are considered as undirected (sets). See Figure \\ref{fig:c6}. As it is possible to notice, there is no automorphism mapping pair $(v_0, v_2)$ to $(v_0, v_3)$ -- hence the two are in distinct orbits.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{l|cccccccccccc}\n    \\toprule\n    pair & $A_1$ & $A_2$ & $A_3$ & $A_4$ & $A_5$ & $A_6$ & $A_7$ & $A_8$ & $A_9$ & $A_{10}$ & $A_{11}$ & $A_{12}$ \\\\\n    \\midrule\n    (0, 1) &  (0, 1) &  (0, 5) &  (0, 1) &  (1, 2) &  (1, 2) &  (2, 3) &  (2, 3) &  (3, 4) &  (3, 4) &  (4, 5) &  (0, 5) &  (4, 5)  \\\\\n    (0, 2) &  (0, 2) &  (0, 4) &  (1, 5) &  (1, 3) &  (0, 2) &  (2, 4) &  (1, 3) &  (3, 5) &  (2, 4) &  (0, 4) &  (1, 5) &  (3, 5)  \\\\\n    (0, 3) &  (0, 3) &  (0, 3) &  (1, 4) &  (1, 4) &  (2, 5) &  (2, 5) &  (0, 3) &  (0, 3) &  (1, 4) &  (1, 4) &  (2, 5) &  (2, 5)  \\\\\n    (0, 4) &  (0, 4) &  (0, 2) &  (1, 3) &  (1, 5) &  (2, 4) &  (0, 2) &  (3, 5) &  (1, 3) &  (0, 4) &  (2, 4) &  (3, 5) &  (1, 5)  \\\\\n    (0, 5) &  (0, 5) &  (0, 1) &  (1, 2) &  (0, 1) &  (2, 3) &  (1, 2) &  (3, 4) &  (2, 3) &  (4, 5) &  (3, 4) &  (4, 5) &  (0, 5)  \\\\\n    (1, 2) &  (1, 2) &  (4, 5) &  (0, 5) &  (2, 3) &  (0, 1) &  (3, 4) &  (1, 2) &  (4, 5) &  (2, 3) &  (0, 5) &  (0, 1) &  (3, 4)  \\\\\n    (1, 3) &  (1, 3) &  (3, 5) &  (0, 4) &  (2, 4) &  (1, 5) &  (3, 5) &  (0, 2) &  (0, 4) &  (1, 3) &  (1, 5) &  (0, 2) &  (2, 4)  \\\\\n    (1, 4) &  (1, 4) &  (2, 5) &  (0, 3) &  (2, 5) &  (1, 4) &  (0, 3) &  (2, 5) &  (1, 4) &  (0, 3) &  (2, 5) &  (0, 3) &  (1, 4)  \\\\\n    (1, 5) &  (1, 5) &  (1, 5) &  (0, 2) &  (0, 2) &  (1, 3) &  (1, 3) &  (2, 4) &  (2, 4) &  (3, 5) &  (3, 5) &  (0, 4) &  (0, 4)  \\\\\n    (2, 3) &  (2, 3) &  (3, 4) &  (4, 5) &  (3, 4) &  (0, 5) &  (4, 5) &  (0, 1) &  (0, 5) &  (1, 2) &  (0, 1) &  (1, 2) &  (2, 3)  \\\\\n    (2, 4) &  (2, 4) &  (2, 4) &  (3, 5) &  (3, 5) &  (0, 4) &  (0, 4) &  (1, 5) &  (1, 5) &  (0, 2) &  (0, 2) &  (1, 3) &  (1, 3)  \\\\\n    (2, 5) &  (2, 5) &  (1, 4) &  (2, 5) &  (0, 3) &  (0, 3) &  (1, 4) &  (1, 4) &  (2, 5) &  (2, 5) &  (0, 3) &  (1, 4) &  (0, 3)  \\\\\n    (3, 4) &  (3, 4) &  (2, 3) &  (3, 4) &  (4, 5) &  (4, 5) &  (0, 5) &  (0, 5) &  (0, 1) &  (0, 1) &  (1, 2) &  (2, 3) &  (1, 2)  \\\\\n    (3, 5) &  (3, 5) &  (1, 3) &  (2, 4) &  (0, 4) &  (3, 5) &  (1, 5) &  (0, 4) &  (0, 2) &  (1, 5) &  (1, 3) &  (2, 4) &  (0, 2)  \\\\\n    (4, 5) &  (4, 5) &  (1, 2) &  (2, 3) &  (0, 5) &  (3, 4) &  (0, 1) &  (4, 5) &  (1, 2) &  (0, 5) &  (2, 3) &  (3, 4) &  (0, 1)  \\\\\n    \\bottomrule\n    \\end{tabular}}\n\\label{tab:c6-node-pair-auto}\n\\end{table*}",
            "tab:subgraph properties": "\\begin{table*}[t]\n    \\centering\n    \\caption{Properties of link prediction benchmarks. Confidence intervals are $\\pm$ one standard deviation. Splits for the Planetoid datasets are random and Collab uses the fixed OGB splits. Where possible, baseline results for Collab are taken directly from the OGB leaderboard}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{l ccccccc}\n    \\toprule \n         &\n         \\textbf{Cora} &  \n         \\textbf{Citeseer} & \n         \\textbf{Pubmed} &\n         \\textbf{Collab} &\n         \\textbf{PPA} &\n         \\textbf{DDI} &\n         \\textbf{Citation2}\n        \\\\\n         \n                  \\#Nodes &\n\n         2,708 & \n         3,327 &\n         18,717 &\n         235,868 &\n         576,289 &\n         4,267 &\n         2,927,963\n          \\\\\n         \n         \\#Edges &\n         5,278 & \n         4,676 &\n         44,327 &\n         1,285,465 &\n         30,326,273 &\n         1,334,889 &\n         30,561,187\n          \\\\\n\n         splits &\n\n         rand &\n         rand & \n         rand &\n         time &\n         throughput &\n         time &\n         protein \\\\\n          \n         avg $\\degree$ &\n         3.9 &\n         2.74 & \n         4.5 &\n         5.45 &\n         52.62 &\n         312.84 &\n         10.44\n        \\\\\n        \n         avg $\\degree^2$ &\n         15.21 &\n         7.51 & \n         20.25 &\n         29.70 &\n         2769 &\n         97,344 &\n         109 \n        \\\\\n         \n         1-hop size &\n\n         $12 {\\scriptstyle \\pm 15}$ &\n         $8 {\\scriptstyle \\pm 8}$ & \n         $12 {\\scriptstyle \\pm 17}$ &\n         $99 {\\scriptstyle \\pm 251}$ &\n         $152 {\\scriptstyle \\pm 152}$ &\n         $901 {\\scriptstyle \\pm 494}$ &\n         $23 {\\scriptstyle \\pm 28}$ \n         \\\\\n         \n        2-hop size &\n\n         $127 {\\scriptstyle \\pm 131}$ &\n         $58 {\\scriptstyle \\pm 92}$ & \n         $260 {\\scriptstyle \\pm 432}$ &\n         $115 {\\scriptstyle \\pm 571}$ & \n         $7790 {\\scriptstyle \\pm 6176}$ &\n         $3830 {\\scriptstyle \\pm 412}$ &\n         $285 {\\scriptstyle \\pm 432}$ \n         \\\\ \\midrule\n         \n\n         \n\\end{tabular}\n}\n\\label{tab:subgraph properties}\n\\end{table*}",
            "tab:preproc": "\\begin{table}[t]\n    \\centering\n    \\caption{The three types of preprocessing used in BUDDY and associated wall times. Entity gives the entity associated with the preprocessed features. Each node has a hash and propagated features while each edge requires structure features}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{l cccccccc}\n    \\toprule \n    \n         \\textbf{Wall time (sec)} &  \n         \\textbf{Entity} &  \n         \\textbf{Cora}&\n         \\textbf{Citeseer}&\n         \\textbf{Pubmed} &\n         \\textbf{Collab} &\n         \\textbf{PPA} &\n         \\textbf{Citation} &\n         \\textbf{DDI} \\\\\n         \\toprule\n         \n         hashing & node & 0.12 & 0.1 & 1.04 & 26 & 469 & 714 & 20.3 \\\\\n         \n         feature propagation & node & 0.27 & 0.60 & 0.67 & 4.6 & 77 & 126 & NA \\\\\n         \n         structure features & edge & 0.02 & 0.01 & 3.3 & 12.0 & 294 & 393 & 45.76 \\\\\n         \\bottomrule\n         \n\\end{tabular}\n}\n\\label{tab:preproc}\n\\end{table}",
            "tab:feature_ablation": "\\begin{table*}[t]\n    \\centering\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{l ccccccc}\n    \\toprule \n         &\n         \\textbf{Cora} &  \n         \\textbf{Citeseer} & \n         \\textbf{Pubmed} &\n         \\textbf{Collab} &\n         \\textbf{PPA} &\n         \\textbf{Citation2} &\n         \\textbf{DDI} \n         \\\\\n         \n                  \\#Nodes &\n\n         2,708 & \n         3,327 &\n         18,717 &\n         235,868 &\n         576,289 &\n         2,927,963 &\n         4267\n          \\\\\n         \n         \\#Edges &\n         5,278 & \n         4,676 &\n         44,327 &\n         1,285,465 &\n         30,326,273 &\n         30,561,187 &\n         1,334,889 \n          \\\\\n          \n          \n         avg deg &\n         3.9 &\n         2.74 & \n         4.5 &\n         5.45 &\n         52.62 &\n         10.44 &\n         312.84\n         \\\\\n         \n        %  Label Homophily &\n        %  0.85 &\n        %  0.81 &\n        %  0.83 & NA & NA & NA &\n        %  NA \\\\\n         \n        %  Rayleigh Quotient &\n        %  0.85 &\n        %  0.80 &\n        %  0.83 & 0.29 & 0.44 & 0.15 &\n        %  NA \\\\\n         \n         metric &\n\n         HR@100 &\n         HR@100 & \n         HR@100 &\n         HR@50 &\n         HR@100 &\n         MRR &\n         HR@20\n\n         \\\\ \\midrule\n          \n         \\textbf{CN} & \n         $33.92 {\\scriptstyle \\pm 0.46}$& \n         $29.79 {\\scriptstyle \\pm 0.90}$& \n         $23.13 {\\scriptstyle \\pm 0.15}$&\n         $56.44 {\\scriptstyle \\pm 0.00}$&\n         $27.65 {\\scriptstyle \\pm 0.00}$&\n         $51.47 {\\scriptstyle \\pm 0.00}$&\n         $17.73 {\\scriptstyle \\pm 0.00}$\n         \\\\\n          \n        \\textbf{AA} & \n        $39.85 {\\scriptstyle \\pm 1.34}$&\n        $35.19 {\\scriptstyle \\pm 1.33}$&\n        $27.38 {\\scriptstyle \\pm 0.11}$&\n        $64.35 {\\scriptstyle \\pm 0.00}$&\n        $32.45 {\\scriptstyle \\pm 0.00}$&\n        $51.89 {\\scriptstyle \\pm 0.00}$&\n        $18.61 {\\scriptstyle \\pm 0.00}$\n        \\\\\n        \n        \\textbf{RA} &\n        $41.07 {\\scriptstyle \\pm 0.48}$ &\n        $33.56 {\\scriptstyle \\pm 0.17}$&\n        $27.03 {\\scriptstyle \\pm 0.35}$& \n        $64.00 {\\scriptstyle \\pm 0.00}$&\n        $49.33 {\\scriptstyle \\pm 0.00}$&\n        $51.98 {\\scriptstyle \\pm 0.00}$&\n        $27.60 {\\scriptstyle \\pm 0.00}$\n        \\\\ \\midrule\n        \n        \\textbf{BUDDY} & \n         \\first{88.00}{0.44}&\n         \\first{92.93}{0.27}&\n         \\first{74.10}{0.78}&\n         \\first{65.94}{0.58}&\n         \\first{49.85}{0.20}& \n         \\first{87.56}{0.11}&\n         \\first{78.51}{1.36}\n        \\\\\n        \\textbf{w\\textbackslash 0 Features} & \n        $48.45 {\\scriptstyle \\pm 4.83}$ &\n        $36.33 {\\scriptstyle \\pm 5.59}$ &\n        $53.50 {\\scriptstyle \\pm 2.23}$ &\n        $60.46 {\\scriptstyle \\pm 0.33}$ &\n        $49.85 {\\scriptstyle \\pm 0.20}$ &\n        $82.27 {\\scriptstyle \\pm 0.10}$ &\n        NA\n        \\\\\n        \n        \\textbf{w\\textbackslash 0 SF} & \n        $83.90 {\\scriptstyle \\pm 2.28}$ &\n        $91.24 {\\scriptstyle \\pm 1.44}$ &\n        $65.57 {\\scriptstyle \\pm 2.86}$ &\n        $22.83 {\\scriptstyle \\pm 1.26}$ &\n        $1.20 {\\scriptstyle \\pm 0.21}$ &\n        $83.59 {\\scriptstyle \\pm 0.13}$ &\n        $74.01 {\\scriptstyle \\pm 13.18}$\n        \\\\\n        \\bottomrule\n        \n\\end{tabular}\n}\n\\caption{Ablation table showing the affects of removing both structure features and node features from BUDDY with all hyperparameters held fixed. Core heuristics are shown for comparison with the w\\textbackslash o features row. Confidence intervals are $\\pm$ one sd. Planetoid splits are random and the OGB splits are fixed.}\n\\label{tab:feature_ablation}\n\\end{table*}"
        },
        "figures": {
            "fig:seal_iso_graph": "\\begin{wrapfigure}{r}{0.32\\textwidth}\n    \\begin{center}\\vspace{-1mm}\n    \\includegraphics[width=0.3\\textwidth]{automorphic_nodes.pdf}\n    % https://docs.google.com/drawings/d/1PqwY6kPMGflxwbf35NnaTNel5Eb2bfHZp6bL5sKu_Ws/edit\n    \\end{center}\\vspace{-2mm}\n    \\caption{Nodes 2 and 4 are in the same orbit induced by the graph's automorphism group. As a result, a conventional GNN will assign the same  probability to links (1,2) and (1,4).\\vspace{-5mm}}\n    \\label{fig:seal_iso_graph}\n\\end{wrapfigure}",
            "fig:feature_importance": "\\begin{wrapfigure}{r}{0.45\\textwidth}\n    %\\centering\n    \\vspace{-3mm}\n    \\includegraphics[width=0.45\\textwidth]{feature_importance_overall.pdf}\\vspace{-2mm}\n    \\caption{The importance of DRNL structure features. Importance is based on the weights in a logistic regression model using all DRNL features without node features.}\n    \\label{fig:feature_importance}\n    \\vspace{-3mm}\n\\end{wrapfigure}",
            "fig:intersections": "\\begin{figure}\n    \\centering\n    %\\includegraphics[width=0.5\\textwidth]{link_prediction_intersections.drawio.pdf}\n    \\includegraphics[width=0.65\\textwidth]{intersection_annotated.pdf}\n    \\caption{Blue and red concentric circles indicate 1 and 2-hop neighborhoods of $u$ and $v$ respectively. Structure features $\\mathcal{A}$ and $\\mathcal{B}$ measure the cardinalities of intersections of these neighborhoods.}\n    \\vspace{-5mm}\n    \\label{fig:intersections}\n\\end{figure}",
            "fig:c6": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{hexagon.pdf}\n    \\caption{Graph $C_6$ with nodes and node-pairs coloured according to the orbit they belong to. Node-pairs corresponding to actual edges are depicted solid, dashed otherwise. There are $2 \\cdot n = 12$ automorphisms which map any node to any other (see Table~\\ref{tab:c6-node-auto}). Hence all nodes are in the same, single orbit. On the contrary, node-pairs are partitioned into three distinct orbits (see Table~\\ref{tab:c6-node-pair-auto}). As it is possible to notice, pairs $(v_0, v_2)$ and $(v_0, v_3)$ are \\emph{not} automorphic, while their constituent nodes are.}\n    \\label{fig:c6}\n\\end{figure}",
            "fig:computedz": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{graph_with_z_features.png}\n    \\caption{Computed z features}\n    \\label{fig:computedz}\n    % https://docs.google.com/drawings/d/11Fq3gUK0OcwGaN4TRadBYQOhZGyd3n3Z-M62ESzpJx4/edit\n\\end{figure}",
            "fig:collab_hashing_abl": "\\begin{figure}\n    \\centering\n\\begin{subfigure}[t]{0.45 \\textwidth}\n    \\includegraphics[width=\\linewidth]{minhash_collab_ablation.pdf}\n    \\caption{number of minhash permutations.}\n    \\label{fig:minhash_abl}\n\\end{subfigure}%\n\\hspace{5mm}\n\\begin{subfigure}[t]{0.45 \\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{hll_collab_ablation.pdf}\n    \\caption{hyperloglog $p$ parameter}\n    \\label{fig:hll_abl}\n\\end{subfigure}\n\\caption{Ablation study for hashing parameters for Collab dataset}\n\\label{fig:collab_hashing_abl}\n\\end{figure}",
            "fig:planetoid_hashing_abl": "\\begin{figure}\n    \\centering\n\\begin{subfigure}[t]{0.45 \\textwidth}\n    \\includegraphics[width=\\linewidth]{minhash_planetoid_ablation.pdf}\n    \\caption{number of minhash permutations.}\n    \\label{fig:minhash_abl_planet}\n\\end{subfigure}%\n\\hspace{5mm}\n\\begin{subfigure}[t]{0.45 \\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{hll_planetoid_ablation.pdf}\n    \\caption{hyperloglog $p$ parameter}\n    \\label{fig:hll_abl_planet}\n\\end{subfigure}\n\\caption{Ablation study for hashing parameters for Planetoid datasets}\n\\label{fig:planetoid_hashing_abl}\n\\end{figure}",
            "fig:hops_abl": "\\begin{figure}\n    \\centering\n\\begin{subfigure}[t]{0.45 \\textwidth}\n    \\includegraphics[width=\\linewidth]{hops_collab_ablation.pdf}\n    \\caption{Collab dataset}\n    \\label{fig:collab_hops_abl}\n\\end{subfigure}%\n\\hspace{5mm}\n\\begin{subfigure}[t]{0.45 \\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{hops_planetoid_ablation.pdf}\n    \\caption{Planetoid datasets}\n    \\label{fig:planetoid_hops_abl}\n\\end{subfigure}\n\\caption{Ablation study for the number of hops}\n\\label{fig:hops_abl}\n\\end{figure}",
            "fig:hash_runtimes": "\\begin{figure}\n    \\centering\n\\begin{subfigure}[t]{0.45 \\textwidth}\n    \\includegraphics[width=\\linewidth]{hashing_generation_runtimes.pdf}\n    \\caption{Planetoid datasets.}\n    \\label{fig:planetoid_hash_runtimes}\n\\end{subfigure}%\n\\hspace{5mm}\n\\begin{subfigure}[t]{0.45 \\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{hashing_generation_collab_runtimes.pdf}\n    \\caption{Collab}\n    \\label{fig:collab_hash_runtimes}\n\\end{subfigure}\n\\caption{runtime of hash generation against number of minhash permutations}\n\\label{fig:hash_runtimes}\n\\end{figure}",
            "fig:cora_batch": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{learning_curves/cora_batch.pdf}\n    \\caption{\\curveCpation Cora dataset.}\n    \\label{fig:cora_batch}\n\\end{figure}",
            "fig:citeseer_batch": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{learning_curves/citeseer_batch.pdf}\n    \\caption{\\curveCpation Citeseer dataset.}\n    \\label{fig:citeseer_batch}\n\\end{figure}",
            "fig:pubmed_batch": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{learning_curves/pubmed_batch.pdf}\n    \\caption{\\curveCpation Pubmed dataset.}\n    \\label{fig:pubmed_batch}\n\\end{figure}",
            "fig:collab_batch": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{learning_curves/collab_batch_.pdf}\n    \\caption{\\curveCpation ogbl-Collab dataset.}\n    \\label{fig:collab_batch}\n\\end{figure}",
            "fig:ppa_batch": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{learning_curves/ppa_batch_.pdf}\n    \\caption{\\curveCpation ogbl-PPA dataset.}\n    \\label{fig:ppa_batch}\n\\end{figure}",
            "fig:citation_batch": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{learning_curves/citation_batch_.pdf}\n    \\caption{\\curveCpation ogbl-Citation2 dataset.}\n    \\label{fig:citation_batch}\n\\end{figure}",
            "fig:ddi_batch": "\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=1\\textwidth]{learning_curves/ddi_batch.pdf}\n    \\caption{\\curveCpation ogbl-DDI dataset.}\n    \\label{fig:ddi_batch}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation} \\label{eq:mpnn_link_prediction}\n%\\mathbf{x}_u^{(l)} &=\\gamma^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\square_{v \\in \\mathcal{N}(u)} \\phi^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\mathbf{x}_v^{(l-1)}\\right)\\right) \\\\\n%p(u,v) &= \\psi \\left(\\mathbf{x}^k_u \\odot \\mathbf{x}^k_v \\right)\n\\mathbf{y}_u =\\gamma\\left(\\mathbf{x}_u, \\square_{v \\in \\mathcal{N}(u)} \\phi\\left(\\mathbf{x}_u, \\mathbf{x}_v\\right)\\right), \n%\\\\\n%p(u,v) &= \\psi \\left(\\mathbf{x}^k_u \\odot \\mathbf{x}^k_v \\right)\n\\end{equation}",
            "eq:2": "\\begin{align}\n\\mathcal{A}_{uv}[d_u, d_v] &= |N_{d_u,d_v}(u,v)|- \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\sum_{x \\leq d_u, y \\leq d_v, (x,y) \\neq (d_u,d_v)} \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! |N_{x,y}(u,v)|\\\\\n{\\mathcal{B}}_{uv}[d] &= |N_{d}(u)| -  {\\mathcal{B}}_{uv}[d-1] - \\sum_{i=1}^d\\sum_{j=1}^{d} {\\mathcal{A}}_{uv}[i, j]\n\\end{align}",
            "eq:3": "\\begin{align}\n|N_{d_u,d_v}(u,v)| &\\triangleq |N_{d_u}(u)\\cap N_{d_v}(v)|  = \\\\\n             & = J(N_{d_u}(u), N_{d_v}(v)) \\cdot  |N_{d_u}(u)\\cup N_{d_v}(v)| \\\\\n             & \\approx H\\left( \\mathbf{m}_{u}^{(d_u)}, \\mathbf{m}_{v}^{(d_v)}\\right) \\cdot \\card \\left(\\max(\\mathbf{h}_{u}^{(d_u)}, \\mathbf{h}_{v}^{(d_v)})\\right),\n\\end{align}",
            "eq:4": "\\begin{align}\n\\mathbf{m}_{u}^{(l)} &= \\min_{v \\in \\mathcal{N}(u)} \\mathbf{m}_{v}^{(l-1)},\\quad\\quad\n\\mathbf{h}_{u}^{(l)} = \\max_{v \\in \\mathcal{N}(u)} \\mathbf{h}_{v}^{(l-1)} \\label{eq:sketches} \\\\\n\\mathbf{e}_{u, v}^{(l)} &= \\{ \\hat{\\mathcal{B}}_{uv}[l], \\hat{\\mathcal{A}}_{uv}[d_u, l], \\hat{\\mathcal{A}}_{uv}[l, d_v] : \\forall d_u,d_v < l \\} \\label{eq:edge_features} \\\\\n\\mathbf{x}_u^{(l)} &=\\gamma^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\square_{v \\in \\mathcal{N}(u)} \\phi^{(l)}\\left(\\mathbf{x}_u^{(l-1)}, \\mathbf{x}_v^{(l-1)}, \\mathbf{e}_{u, v}^{(l)}\\right)\\right)\n\\label{eq:aggregation}\n% \\mathbf{f}_u^{(l)} &= \\mathbf{x}_u^{(l)} \\quad || \\quad \\mathbf{z}_{hu}^{(l)} \\quad|| \\quad \\mathbf{z}_{mu}^{(l)}\\\\\n% \\mathbf{f}_u^{(l)} &=\\gamma^{(l)}\\left(\\mathbf{f}_u^{(l-1)}, \\square_{v \\in \\mathcal{N}(u)} \\phi^{(l)}\\left(\\mathbf{f}_u^{(l-1)}, \\mathbf{f}_v^{(l-1)}, \\mathbf{e}_{u, v}^{(l-1)}\\right)\\right),\n\\end{align}",
            "eq:5": "\\begin{align} \np(u,v)= \\psi \\left(\\mathbf{x}^{(k)}_u \\odot \\mathbf{x}^{(k)}_v, \\{ \\hat{\\mathcal{B}}_{uv}[d], \\hat{\\mathcal{A}}_{uv}[d_u, d_v] : \\forall \\, d,d_u,d_v \\in [k]  \\}\\right) \\,, \n\\label{eq:readout}\n\\end{align}",
            "eq:6": "\\begin{align}\n    \\mathbf{M}^{(l)} = \\textrm{scatter\\_min}(\\mathbf{M}^{(l-1)}, G), \\,\\,\n    \\mathbf{H}^{(l)} = \\textrm{scatter\\_max}(\\mathbf{H}^{(l-1)}, G), \\,\\,\n    \\mathbf{X}^{(l)} = \\textrm{scatter\\_mean}(\\mathbf{X}^{(l-1)}, G)\n    \\notag\n\\end{align}",
            "eq:7": "\\begin{equation}\n    \\big ( \\sigma \\cdot \\mathsf{A} \\big )_{ijk} = \\mathsf{A}_{\\sigma^{-1}(i)\\sigma^{-1}(j)k}, \\quad \\forall \\sigma \\in S_n\n\\end{equation}",
            "eq:8": "\\begin{align}\nE + LF^2 + LNF, \n\\end{align}",
            "eq:9": "\\begin{align}\nE + LF^2 + LN(F + H)\n\\end{align}",
            "eq:10": "\\begin{align}\nBF + F^2, \n\\end{align}",
            "eq:11": "\\begin{align}\nB(F+L(L+2)) + F^2 + F(L(L+2)). \n\\end{align}",
            "eq:12": "\\begin{align}\nE + LNF + LN + LNH,     \n\\end{align}",
            "eq:13": "\\begin{align}\n    \\Gamma(u,v) = \\sum_{i \\in N(u) \\cap N(v)}\\frac{1}{f(|N(i)|)},\n\\end{align}",
            "eq:14": "\\begin{align}\n    f_l(i) = 1 + \\min(d_{ui},d_{vi} + (d/2)[(d//2)+d\\%2)-1]\n\\end{align}",
            "eq:15": "\\begin{align}\n    \\mathcal{A}_{67}[2,1] &= |\\{2,3,4,6,7\\} \\cap \\{2,8\\}| = |\\{2\\}| = 1\\\\\n    \\mathcal{B}_{67}[2] &= |\\{2,3,4,6,7\\} \\setminus \\{1,2,3,5,6,7,8\\}| = |\\{4\\}| = 1\n\\end{align}"
        },
        "git_link": "https://github.com/melifluos/subgraph-sketching"
    }
}