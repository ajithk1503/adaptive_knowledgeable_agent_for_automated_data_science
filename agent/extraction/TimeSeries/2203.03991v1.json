{
    "meta_info": {
        "title": "Sparsification and Filtering for Spatial-temporal GNN in Multivariate  Time-series",
        "abstract": "We propose an end-to-end architecture for multivariate time-series prediction\nthat integrates a spatial-temporal graph neural network with a matrix filtering\nmodule. This module generates filtered (inverse) correlation graphs from\nmultivariate time series before inputting them into a GNN. In contrast with\nexisting sparsification methods adopted in graph neural network, our model\nexplicitly leverage time-series filtering to overcome the low signal-to-noise\nratio typical of complex systems data. We present a set of experiments, where\nwe predict future sales from a synthetic time-series sales dataset. The\nproposed spatial-temporal graph neural network displays superior performances\nwith respect to baseline approaches, with no graphical information, and with\nfully connected, disconnected graphs and unfiltered graphs.",
        "author": "Yuanrong Wang, Tomaso Aste",
        "link": "http://arxiv.org/abs/2203.03991v1",
        "category": [
            "cs.LG",
            "q-fin.CP"
        ],
        "additionl_info": "7 pages, 1 figure, 3tables"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": " \\label{intro}\n\nMultivariate time-series prediction is an important, general, challenge in data science and machine learning. \nSeveral deep learning approaches have been proposed in the literature to address multivariate time-series forecasting. However, while they often perform well at extracting temporal patterns, most of the proposed approaches are not designed to account for the interdependency between time-series. Graph neural network, on the other hand, can model time-series as nodes in a graph to account for dependency. Recent development in spatial-temporal graph neural network has been shown to enable multivariate time-series learning and inference \\cite{Khodayar2019SpatioTemporalGD,Kapoor2020ExaminingCF,Wan2019MultivariateTC}.\n\nApplications of multivariate time-series prediction ranges from day-to-day business e.g., sales forecasting, traffic prediction to convoluted topics like bio-statistics and action recognition. It is also one of the cornerstones of modern quantitative finance. At the finest granular level, in finance, modeling the levels of limit order book is a multivariate problem for high-frequency trading with the aim of mid-price prediction \\cite{Briola2020DeepLM, Briola2021DeepRL}. For longer-term investment like portfolio management \\cite{Markowitz,Wang2021DynamicPO}, prices of assets in a portfolio are usually multivariate time-series. Multivariate time series forecasting methods assume inter-dependencies among dynamically changing variables, which captures systematic trends. Namely, the prediction for each variable not only depends on its historical temporal information, but also the others. Understanding this inter-dependency helps to reveal the underlying dynamics of a larger picture, e.g., the financial market, urban transportation system or the urban distribution of shopping centers. However, its complexity is a key challenge that has been studied for over six decades. \n\nIntra-series temporal patterns and inter-series correlations are jointly the two cores in multivariate time-series forecasting. Recent advancement in deep learning has enabled strong temporal pattern mining. Recurrent Neural Network (RNN) \\cite{Rumelhart1986LearningIR}, Long Short-Term Memory (LSTM) network \\cite{Sak2014LongSM}, Gated Recurrent Units (GRU) \\cite{Chung2014EmpiricalEO}, Gated Linear Units (GLU) \\cite{Dauphin2017LanguageMW} and Temporal Convolution Networks (TCN) \\cite{Bai2018AnEE} demonstrate promising results in temporal modelling. However, existing methods fail to exploit latent inter-dependencies and correlation among time-series. Historical attempts have been made to input covariance/correlation structure into neural network. Matrix-based neural networks have been discussed \\cite{Gao2017MatrixNN,Cai2006LearningWT,Daniusis2008NeuralNW}, but this approach is not specifically designed for the covariance/correlation matrix, and therefore fails to directly and explicitly address the dependency in the covariance/correlation structure inside the calculation.\n\nGraph is a mathematical structure to model the pairwise relation between objects. The permutation-invariant, local connectivity and compositionality of graphs present a perfect data structure to simulate the correlation/covariance matrix. In fact, network science literature has long been including (sparse) covariance/correlation as a special network for analysis \\cite{Chen2018EstimatingLC,Turiel2020SimplicialPO,Procacci2021ForecastingMS,Kojaku2019ConstructingNB, Shen2010CovarianceCM}, and many network properties of covariance/correlation matrix contribute greatly to analytical and predictive tasks in the financial market \\cite{Millington2017RobustPR,Yuan2020ImprovedLD,Lee2020OptimalPU}. Recently, graph neural network (GNN) has been leveraged to incorporate the topology structure between entities. Hence, modeling inter-series correlation via graph learning is a natural extension to analyzing covariance/correlation matrix from a network perspective. Each variable from a multivariate time-series is a node in the graph, and the edge represents their latent inter-dependency. By propagating information between neighboring nodes, the graph neural network enables each time-series to be aware of correlated context.\n\nSpatial-temporal graph neural network is the most used network structure for multivariate time-series problems in the literature \\cite{Zhao2020TGCNAT,Wu2020ConnectingTD,Cao2020SpectralTG,Sesti2021IntegratingLA}, as the temporal part extracts patterns in each uni-variate series with a LSTM/RNN/GRU, while the spatial part (GNN) models the relationship between series with a pre-defined topology or a graph representation learning algorithm. On one hand, existing GNNs heavily utilizes a pre-defined topology structure which is not explicit in multivariate time-series, and does not reflect the temporal dynamics nature of time-series. On the other hand, many graph representation learning methods focus more on generating node embeddings rather than topological structure, and most of the embeddings depend on a pre-defined topological prior or attention mechanism \\cite{Ying2018HierarchicalGR,Zhao2020TGCNAT,Franceschi2019UnsupervisedSR}.\n\nIn this paper, we propose an end-to-end framework termed Filtered Sparse Spatial-temporal GNN (FSST-GNN) for sales prediction of 50 products in 10 stores. By integrating modern spatial-temporal GNN with traditional matrix filtering/sparsification methods, we demonstrate the direct use of the (inverse) correlation matrix in GNN. Correlation filtering techniques generate a sparse inverse correlation matrix from multivariate time-series, which can be inverted to a filtered correlation matrix. Both the (inverse) correlation can be used as a pre-defined topological structure or prior for further representation learning. With the designed architecture, we further illustrate that filtered graphs generates a positive impact in multivariate time-series learning, and sparse graphs acts as a contributing prior to guide attention mechanism in GNN.\n\n"
            },
            "section 2": {
                "name": "Related Works",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Multivariate Time-series Forecasting",
                    "content": "\nTime-series forecasting is one of the long-standing key problems in statistics, data science and machine learning. Techniques ranges from traditional pattern recognition to modern machine learning. Univariate time-series forecasting focuses on analyzing independent time-series by extracting temporal patterns based on historical behaviours, e.g., the moving average (MA), the auto-regressive (AR), the auto-regressive moving average (ARMA) and the autoregressive integrated moving average (ARIMA) \\cite{Young1972TimeSA}. Modern machine learning model like LSTM has been shown as a good fit to tackle this problem by many literature, e.g., FC-LSTM \\cite{Shi2015ConvolutionalLN} and SMF \\cite{Zhang2017StockPP}. Multivariate forecasting considers a correlated collection of time-series. The vector auto-regressive model (VAR) and the vector auto-regressive moving average model (VARMA) \\cite{Kascha2012ACO,Anderson1978MaximumLE} extend the aforementioned linear models into a multivariate space by capturing the interdependecy between time-series. Early attempts combines convolution neural network (CNN) and recurrent neural network (RNN) to learn local spatial dependencies and temporal patterns \\cite{Lai2018ModelingLA,Shih2019TemporalPA}. Further works include state space model in Deep-State \\cite{Rangapuram2018DeepSS} and matrix factorization approach in DeepGLO \\cite{Sen2019ThinkGA}.\n\n"
                },
                "subsection 2.2": {
                    "name": "Spatial-temporal Graph Neural Network",
                    "content": "\nSpatial-temporal graph neural network has been proposed recently for multivariate time-series problems. To capture the correlation be between time-series in the spatial component, each time-series is modelled as a node in a graph whereas the edge between every two nodes represents their correlation. Early work applies spatial-temporal GNN for traffic forecasting \\cite{Li2018DiffusionCR,Yu2018SpatioTemporalGC,Chen2020MultiRangeAB,Wu2019GraphWF,Zheng2020GMANAG}. Further studies have been extended to other fields, e.g., action recognition \\cite{Shi2019TwoStreamAG,Yan2018SpatialTG} and bio-statistics with many interesting works for COVID-19 \\cite{LaGatta2021AnEN,Fritz2021CombiningGN,Kapoor2020ExaminingCF}. For financial applications, Matsunaga et al. \\cite{Matsunaga2019ExploringGN} is one of the first studies exploring the idea of incorporating company knowledge graphs directly into the predictive model by GNN. Later, Hou et al. \\cite{Hou2021STTraderAS} proposed to use a variational autoencoder (VAE) to process stock fundamental information and cluster it into graph structure. This learned adjacency matrix is then fed into a GCN-LSTM for further forecasting. Similar work has been done by Pillay \\& Moodley \\cite{Pillay2022ExploringGN} with a different model architecture called Graph WaveNet. The most recent advancement is a spatial-temporal GNN for portfolio/asset management proposed by Amudi \\cite{Amudi}. They combine a stock sector graph, a correlation graph and a supply-chain graph into one super graph and use the multi-head attention in GAT as a sparsification method to select the meaningful subgraph for prediction. In line with this work, we focus on filtered/sparsified (inverse) correlation graph generated from matrix filtering/sparsification techniques.\n\n"
                },
                "subsection 2.3": {
                    "name": "Correlation Matrix Filtering",
                    "content": "\nMany computational methods employ sparse approximation techniques to estimate the inverse covariance matrix. \nThe sparsification is effective because the least significant components in a covariance matrix are often largely prone to small changes and can lead to instability. \nSparsified models filters out these insignificant components, and thus improve the model resilience to noise. As correlation is a scaled form of covariance, filtering and sparsification methods are equivalently applicable in both cases.\n",
                    "subsubsection 2.3.1": {
                        "name": "Covariance Shrinkage",
                        "content": "\nA shrinkage algorithm minimizes the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix, which is done by simply shifting every eigenvalue according to a given offset. This approach is equivalent of finding the $L_2$-penalized maximum likelihood estimator of the covariance matrix \\cite{Ledoit2003HoneyIS}, which is expressed as a simple convex transformation:\n\\begin{equation}\\label{eq:covariance srhinkage}\n\\begin{aligned}\n    \\Sigma_{\\text{shrunk}}=(1-\\alpha)\\hat{\\Sigma}+\\alpha\\frac{\\text{Tr}\\hat\\Sigma}{n}\\mathbb{1}\n\\end{aligned}\n\\end{equation} \nwhere $\\Sigma_{\\text{shrunk}}$ is the shrunk covariance, $\\hat{\\Sigma}$ is the empirical covariance, $n$ is the number of features in the covariance, $\\mathbb{1}$ is the identity matrix and $\\alpha$ is the shrinkage coefficient. To optimise the selection of the shrinkage coefficient, Ledoit and Wolf in 2004 \\cite{Ledoit2004AWE} proposed to compute $\\alpha$ that minimizes the mean square error between the estimated and empirical covariance. With further assumption on the normality of data, Chen et al. in 2010 \\cite{Chen2010ShrinkageAF} proposed a better $\\alpha$ computation based on minimum mean square error. Further research focuses on large dimension shrinkage \\cite{Ledoit2012NonlinearSE,Donoho2018OptimalSO,Couillet2014LargeDA}.\n"
                    },
                    "subsubsection 2.3.2": {
                        "name": "Graphical Models",
                        "content": "\nA widely used approach for inverse covariance estimation is based on graph models. Meinshausen and Buhlmann in 2006 \\cite{Meinshausen2006HighdimensionalGA} regards the zero entries in the inverse covariance matrix of a multi-variable normal distribution as conditional independence between variables. These structural zeros can thus be obtained through neighborhood selection with LASSO regression by fitting a LASSO to each variable and using the others as predictors. Similar methods that maximizes $L_1$ penalized log-likelihood have been studies by Yuan and Lin \\cite{Yuan2007ModelSA} and Banerjee et al. \\cite{Banerjee2007ModelST}. In 2008, Friedman et al. \\cite{CompNet9} developed an efficient Graphical LASSO that uses $L_1$ norm regularization to control the sparsity in the precision matrix. The sparse inverse covariance matrix can be obtained through minimizing the regularized negative log-likelihood \\cite{Mazumder2012TheGL}:\n\\begin{equation}\\label{eq:glasso}\n\\begin{aligned}\n    \\Sigma_{\\text{glasso}}^{-1}=\\min_{{\\mathbf \\Sigma^{-1}}}( -\\log\\det{\\Sigma^{-1}} + \\text{Tr}(\\hat\\Sigma^{-1}\\Sigma^{-1})+\\lambda||\\Sigma^{-1}||_{1})\n\\end{aligned}\n\\end{equation} \nwhere $\\hat\\Sigma^{-1}$ is the empirical inverse covariance, $||\\Sigma^{-1}||_{1}$ denotes the sum of the absolute values of $\\Sigma^{-1}$, and $\\lambda$ is the regularization constant, optimised by cross-validation. \n"
                    },
                    "subsubsection 2.3.3": {
                        "name": "Information Filtering Network",
                        "content": " \\label{IFN}\nAn alternative approach that uses information filtering networks has been shown to deliver better results with lower computational burden and larger interpretability \\cite{CompNet7}. In the past few years, information filtering network analysis of complex system data has advanced significantly. It models interactions in a complex system as a network structure of elements (vertices) and interactions (edges). The best-known approach, the Minimum Spanning Tree (MST) was firstly introduced by Boruvka in 1926 \\cite{nevsetvril2001otakar} and it can be solved exactly (see \\cite{CompNet10} and  \\cite{CompNet11} for two common approaches). \nThe MST reduces the structure to a connected tree which retains the larger correlations.\nTo better extract useful information, Tumminello et al. \\cite{CompNet3} and Aste and Di Matteo \\cite{CompNet4} introduced the use of planar graphs in the Planar Maximally Filtered Graph (PMFG) algorithm. Recent studies have extended the approach to chordal graphs of flexible sparsity \\cite{CompNet5, CompNet6}. Research fields ranging from finance \\cite{CompNet7} to neural systems \\cite{CompNet8} have applied this approach as a powerful tool to understand high dimensional dependency and construct a sparse representation. \nIt was shown that, for chordal information filtering networks, such as the Triangulated Maximally Filtered Graph (TMFG) \\cite{CompNet5}, one can obtain a sparse precision matrix that is positively definite and has the structure of the network paving the way for a proper $L_0$-norm topological regularization \\cite{aste2020topological}. Further study in Maximally Filtered Clique Forest (MFCF) \\cite{Massara2019LearningCF} extends the generality of the method by applying it to different sizes of cliques. This approach has proven to be computationally more efficient and stable than Graphical LASSO \\cite{CompNet9} and covariance shrinkage methods \\cite{Ledoit2003HoneyIS,Ledoit2004AWE,Chen2010ShrinkageAF}, especially when few data points are available \\cite{CompNet7, CompNet4}. \n\n"
                    }
                },
                "subsection 2.4": {
                    "name": "Sparse GNN",
                    "content": "\nMany literature has discussed graph sparsification in GNN. Some, by including regularization, reduce unnecessary edges, which can largely improve the efficiency and efficacy of large-scale graph problems \\cite{Calandriello2018ImprovedLG,Chakeri2016SpectralSI}. Some leverage stochastic edge pruning in graphs as a dropout-equivalent regularization to enhance the training process \\cite{Rong2020DropEdgeTD,Hasanzadeh2020BayesianGN}. Others train the GNN to learn sparsification as an integrated part before applying it to downstream tasks. NeuralSparse learns to sample k-neighbor subgraph as input for GNN \\cite{Zheng2020GMANAG}. Luo proposes to prune task-irrelevant edges \\cite{Luo2021LearningTD}. Kim uses the disconnected edges of sparse graphs to guide attention in GAT \\cite{Kim2021HowTF}.\n\n"
                }
            },
            "section 3": {
                "name": "Model Implementation",
                "content": "\n\nWe first elaborate on the general framework of our model. As illustrated in Figure \\ref{fig:Architecture}, the model consists of 5 main building blocks. A correlation graph generator is able to transform the multivariate time-series into a correlation graph where each node represents a single time-series and each edge between two nodes denotes their correlation. A standard transformation generates a full (inverse) correlation graph with (inverse) correlation edges between each node. In addition, correlation-filtering based transformation generates a full correlation graph with filtered correlation edges, or a sparse inverse correlation graph. We employ covariance shrinkage, graphical models and information filtering network as the three main correlation-filtering based graph generators. The feature generator generates initial input features for each node based on the multivariate time-series. The generated graph and the features from the two generators are then fed into a GNN to learn meaningful node embeddings as the spatial information. Similarly, the multivariate time-series is also fed into a LSTM to extract temporal information. Then, the spatial and temporal information are input in a multi-layer perceptron (MLP) as the read-out layer for the final output, the predicted sales number.\n\n\n\n\n",
                "subsection 3.1": {
                    "name": "Correlation Graph Generator",
                    "content": "\n",
                    "subsubsection 3.1.1": {
                        "name": "Covariance Shrinkage",
                        "content": "\nCovariance shrinkage method as described in equation \\ref{eq:covariance srhinkage} is equivalently applicable to the correlation matrix. Shrinkage coefficient $\\alpha$ is optimized by cross-validation. There is a implementation, {\\fontfamily{cmr}\\selectfont \\textbf{sklearn.covariance.ShrunkCovariance}} Pyhon library \\cite{scikit-learn}, which is applied in this experiment. Filtered correlation is then directly transform into a graph. Inverse correlation can be obtained by direct matrix inversion, which is implemented by the {\\fontfamily{cmr}\\selectfont \\textbf{numpy.linalg.inv}} library \\cite{2020NumPy-Array}.\n\n"
                    },
                    "subsubsection 3.1.2": {
                        "name": "Graphical LASSO",
                        "content": "\nGraphical LASSO is a graphical model for inverse covariance sparsification, which is epressed in equation \\ref{eq:glasso}. We leverage Python's {\\fontfamily{cmr}\\selectfont \\textbf{sklearn.covariance.GraphicalLasso}} library for implementation, and {\\fontfamily{cmr}\\selectfont \\textbf{sklearn.covariance.GraphicalLassoCV}} \\cite{scikit-learn} for cross valiation and regularization constant $\\lambda$ selection. Graphical LASSO sparsifies an inverse correlation matrix which can be directly transformed into a sparse inverse correlation graph, while a full but filtered correlation graph can be obtained through the matrix inversion of the inverse correlation.\n\n"
                    },
                    "subsubsection 3.1.3": {
                        "name": "Maximally Filtered Clique Forest",
                        "content": "\nWe implement Maximally Filtered Clique Forest (MFCF), an information filtering network, for sparse precision matrix filtering. By setting the minimum and maximum clique size to 4, we simplify our solution to a TMFG-equivalent model discussed in Section \\ref{IFN}. It generates sparse inverse correlation, which will undergoes similar transformation as Graphical LASSO to obtain inverse correlation and correlation graphs.\n\n"
                    }
                },
                "subsection 3.2": {
                    "name": "GNN",
                    "content": "\n",
                    "subsubsection 3.2.1": {
                        "name": "GCN",
                        "content": "\nGraph convolution network is proposed by Kipf and Welling in 2017 \\cite{Kipf2017SemiSupervisedCW}, which generates embeddings for each node in the graph. It takes original features in each node as the initial embeddings, then aggregates neighboring feature representations and updates the node embeddings through a message-passing like network with the adjacency matrix, which can be expressed as:\n\\begin{equation}\\label{GCN_adjacency}\n\\begin{aligned}\n    h_i=\\sum^{N}_{j}{(\\frac{\\phi(W^\\top A_{ij})}{d_j})\\times h_j}\n\\end{aligned}\n\\end{equation}\nwhere $h_i$ is the node embedding, $h_j$ is the neighboring node embedding, $W$ is a learnable parameter, $\\phi$ is non-linear activation, $A_{ij}$ is the adjacency matrix and $d_j$ is the degree of node $j$ for normalization. \n\nIn the experiments, we have replaced the graph information, adjacency matrix, expressed in equation \\ref{GCN_adjacency} by (inverse) correlation matrix, adjacency matrix and Laplacian matrix obtained by thresholding the (inverse) correlation matrix. The empirical results suggest the superiority by simply employing the (inverse) correlation matrix. It can be seen as weighted adjacency matrix, where correlation coefficients are naturally scaled/normalized. Therefore, the graph convolution can be re-expressed as:\n\\begin{equation}\\label{GCN_corr}\n\\begin{aligned}\n    h_i=\\sum^{N}_{j}{\\phi(W^\\top C_{ij})\\times h_j}\n\\end{aligned}\n\\end{equation}\nwhere $C_{ij}$ is the (inverse) correlation matrix, and all the other parameters are previously defined.\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "GAT",
                        "content": "\nThe implementation of GCN limits the model to be used only with static graphs. The embedding update is static across time, which assumes non-stationarity in time-series. Graph attention network uses masked multi-head attention mechanism to solve this issue by dynamically assigning attention coefficients between nodes. The normalized attention coefficient $a_{ij}$ is computed for nodes $i$ and $j$ based on their features (embeddings):\n\\begin{equation}\\label{eq:GAT attention}\n\\begin{aligned}\n    e_{i,j}= & a(Wh_i,Wh_j)\\\\\n    = & \\textrm{LeakyReLU} (\\hat{a}^\\top[Wh_i||Wh_j]) \\\\\n\\end{aligned}\n\\end{equation}\nwhere $W$ is the learnable linear transformation weight matrix to transform node features, $h$, into lower dimensional representations, $a$ is the attention mechanism to perform self-attention on each node and $||$ represents concatenation operation. We normalize the attention coefficient $e_{ij}$ by a softmax function:\n\\begin{equation}\\label{eq:GAT normalized attention}\n    \\begin{aligned}\n        \\alpha_{i,j}=&\\textrm{softmax}_j (e_{i,j})\\\\\n        =&\\frac{\\exp{e_{i,j}}}{\\sum_{k\\in\\mathcal{N}_i}\\exp{e_{i,k}}} \\\\\n        =& \\frac{\\exp{(\\textrm{LeakyReLU}(\\hat{a}^\\top[Wh_i||Wh_j]))}}{\\sum_{k\\in\\mathcal{N}_i} \\exp{(\\textrm{LeakyReLU}(\\hat{a}^\\top[Wh_i||Wh_k]))}}\n    \\end{aligned}\n\\end{equation}\n\nThe multi-head attention mechanism has been proposed by Vaswani et al. \\cite{Vaswani2017AttentionIA} which demonstrates superior and robust performance in network training. GAT incorporates the masked multi-head attention where attention is only computed between neighboring nodes, and the output feature representation is expressed as:\n\\begin{equation}\\label{eq:GAT multi-head aggregation}\n     h_i = \\phi(\\frac{1}{k}\\sum_{k=1}^K(\\sum_{j\\in\\mathcal{N}_i} \\alpha_{i,j}W^kh_j))\n\\end{equation}\nwhere in each level of attention, the representation embedding is updated by a learnable parameter $W$ and the attention coefficient matrix $a_{i,j}$, then the final representation is averaged between the $K$ multi-head attention layers and applied a non-linearity $\\phi$.\n\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n",
                "subsection 4.1": {
                    "name": "Setup",
                    "content": "\nWe test our model on a Kaggle playground code competition, Store Item Demand Forecasting Challenge \\cite{kaggle}. The dataset consists of 5-year sales time-series data of 50 products in 10 different stores. For simplicity, we re-formulate the problem as 50 mini-problems, each focuses on 1 product in 10 different stores. At each time-stamp, the temporal component regresses each of the 10 time-series individually based on its historical value. The dependency between them is reflected by the final embeddings generated from the spatial component. The outputs from each component are subsequently concatenated and, by a read-out layer, to generate final daily forecasting for the product. We assume stationarity in the time-series, therefore, we separate the training and testing data as the 80\\% and 20\\% of the raw dataset.\n\nThe temporal component of the FSST-GNN is a LSTM, which has an input size of ($t_{lb}$, 10) where $t_{lb}=14$ is the look back window size of historical sales, and 10 is the number of different stores. The feature generator produces node features (initial embeddings). We employ the four moments (mean, standard deviation, skewness and kurtosis) of the sales time-series distribution based on each 14-day look back window. The correlation graph generator generates a graph with edge represents the correlation between any two of the 14-day sample time-series in the 10 stores. Then, the generated node features and edges are input into the GNN. In the experiments, a GCN and a GAT have been used as the spatial component.\n\nTo understand the effect of filtering and sparsification for multivariate time-series graph learning, we perform 4 sets of experiment: 1) FSST-GNN (GCN) on different filtered correlation graphs; 2) FSST-GNN (GCN) on different filtered inverse correlation graphs; 3) FSST-GNN (GCN) on GLASSO-filtered and MFCF-filtered inverse correlation graph with different levels of sparsity; and 4) FSST-GNN (GAT) on GLASSO-filtered and MFCF-filtered inverse correlation graph with different levels of sparsity. Each experiment has been re-computed 10 times with different random seeds, and the final results is averaged for statistical robustness.\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Results",
                    "content": "\n\nWe compute the root mean square error (RMSE), mean average error (MAE) and mean average percentage error (MAPE) of the predicted sales number of all 50 products in 10 stores with the ground truth label in Table \\ref{Tab:comparison table} as the evaluation matrix to analyze the effectiveness of filtering methods over FSST-GNN with GCN on the correlation and inverse correlation graph respectively. Since all filtering methods are parametric, the table reports the optimal results from covariance shrinkage (Shrinkage), graphical LASSO (GLASSO) and MFCF, which are obtained through grid-search. We also include a fully connected graph of a matrix of ones, two fully disconnected graphs of a matrix of zeros and an identity matrix as benchmarks for comparison. In addition, a plain LSTM is also presented as the baseline model where no graphical/spatial information is input. \n\n\nIn Table \\ref{Tab:comparison table}, it is evident that all FSST-GNN (GCN) outperforms the plain LSTM, which confirms the efficacy of considering the spatial information in multivariate time-series problems. Other benchmarks of fully connected/disconnected graphs are also presented, and their results in all three measurements are effectively inferior to any (inverse) correlation based graph methods. These results further assert the information gain from meaningful spatial graphs.\n\nHighlighted in each column of Table \\ref{Tab:comparison table} are the best results in first two experiment: 1) FSST-GNN (GCN) on correlation graph; 2) FSST-GNN (GCN) on inverse correlation graph. In correlation graph cases, a filtered correlation graphs demonstrate superior results than the original Empirical correlation graph. Both MFCF and GLASSO filtering are operated on the inverse correlation for sparsification, and then inverted back to a full correlation graphs, while Shrinkage operates directly on correlation. Therefore, the superior results in MFCF and GLASSO than Shrinkage may suggest a stronger filtering effect behind graph/network-based methods, and inversion does not affect filtering.\n\nTo understand the effect in filtering and sparsification, results from the same setup with inverse correlation graphs are compared, where full and Shrinkage-filtered inverse correlation graphs are full graphs and GLASSO-filtered and MFCF-filtered graphs are sparse graphs. In this case, Shrinkage filters a correlation and inverts it to an inverse correlation. Comparably to the correlation graph case, Shrinkage consistently yields better result than the Empirical, which further validates that the filtering mechanism is hardly impacted by inversion operation. Furthermore, we observe even more significant results from two sparse graphs filtered by GLASSO and MFCF. This advantage could possibly come from both the filtering, the sparsification, as well as their combined effect. To further investigate the sole efficacy of sparsification, we perform the third and fourth sets of experiments: 3) FSST-GNN (GCN) on GLASSO-filtered and MFCF-filtered inverse correlation graph with different levels of sparsity; and 4) FSST-GNN (GAT) on GLASSO-filtered and MFCF-filtered inverse correlation graph with different levels of sparsity.\n\nPresented in Table \\ref{Tab:GCN_sparsity} and Table \\ref{Tab:GAT_sparsity} are the results with different levels of sparsity. We select the parameter to match the sparsity level between GLASSO and MFCF for comparison. It is seen that at around $60\\%$ sparsity, the highlighted best results are achieved for both MFCF and GLASSO in FSST-GNN (GCN) and FSST-GNN (GAT) models. Moreover, as the sparsity deviates away from this local minimum, the three errors start to increase, which may suggest an optimal sparsity structure of the inverse correlation graph in our experimental case. In addition, this optimal structure is independent of the chosen model. Furthermore, as illustrated in equation \\ref{eq:GAT multi-head aggregation}, GAT by default does not account for edge weights in weighted graphs (correlation graphs) as GCN. Hence, the sparse inverse correlation graph serves as a thresholded adjacency matrix, where 0 entries are interpreted as disconnection between nodes. Then, attention, which is only calculated between linked nodes, acts as the edge weights. Namely, the superior performance in Table \\ref{Tab:GCN_sparsity} is a mixture of filtering and sparsity, but the performance in Table \\ref{Tab:GAT_sparsity} is merely determined by the sparsity of the input graph without filtering mechanism.\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\nLiterature has presented many GNN-based graph sparsification methods. However, none of them explicitly addresses the filtering and sparsification from a time-series perspective. In small sample time-series problems, especially in finance, graph structure learning models, e.g., graph representation learning, are highly prone to noise. In this paper, we design an end-to-end filtered sparse spatial-temporal graph neural network for time-series forecasting. Our model leverages and integrates traditional matrix filtering methods with modern graph neural networks to achieve robust results, and show the use of a simple and efficient architecture. We employ three different matrix filtering methods, covariance shrinkage, graphical LASSO and information filtering network-maximally filtered clique forest to show a positive gain in graph filtering to graph learning. The results from the three methods surpass all of the benchmark approaches, including a LSTM with no graphical information, the same FSST-GNN architecture with fully connected, disconnected graphs and unfiltered graphs.\n\nIn the experiments, we found the sparse graph in GAT serves only as an indication of which pairs of node require attention calculation, and the advantages from sparsity are significant. The filtered correlation matrix in GCN is interpreted and used as a weighted adjacency matrix for direct graph convolution, where the efficacy of filtering is also obvious. Furthermore, the optimal combined effect of filtering and sparsification in FSST-GNN (GCN) with inverse correlation implies the two contributing factors are complementary. Therefore, by incorporating weighted graphs in GAT like Grassia \\& Mangioni \\cite{Grassia2022wsGATWA}, we may further improve the performance of attention-based graph neural networks.\n \nCurrent work is based on a synthetic dataset from a Kaggle competition for sales prediction. Further work will be applied with real world financial data for practical problems, e.g., portfolio optimization, risk management and price forecasting. The temporal and spatial component of the current architecture are designed to compute in parallel and combined in the end. Therefore, temporal information does not directly contribute to the spatial filtered graph generation or graph node feature generation. In the next phase of this study, we aim to develop a stacked architecture, where temporal signals contribute to spatial graph filtering/sparsification.\n\n\n\n\\newpage\n\\\n\n\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n%\\begin{acks}\n%\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\n\\newpage\n%\\bibliographystyle{ACM-Reference-Format}\n%\\bibliography{ref}\n\n\\begin{thebibliography}{85}\n\n%%% ====================================================================\n%%% NOTE TO THE USER: you can override these defaults by providing\n%%% customized versions of any of these macros before the \\bibliography\n%%% command.  Each of them MUST provide its own final punctuation,\n%%% except for \\shownote{}, \\showDOI{}, and \\showURL{}.  The latter two\n%%% do not use final punctuation, in order to avoid confusing it with\n%%% the Web address.\n%%%\n%%% To suppress output of a particular field, define its macro to expand\n%%% to an empty string, or better, \\unskip, like this:\n%%%\n%%% \\newcommand{\\showDOI}[1]{\\unskip}   % LaTeX syntax\n%%%\n%%% \\def \\showDOI #1{\\unskip}           % plain TeX syntax\n%%%\n%%% ====================================================================\n\n\\ifx \\showCODEN    \\undefined \\def \\showCODEN     #1{\\unskip}     \\fi\n\\ifx \\showDOI      \\undefined \\def \\showDOI       #1{#1}\\fi\n\\ifx \\showISBNx    \\undefined \\def \\showISBNx     #1{\\unskip}     \\fi\n\\ifx \\showISBNxiii \\undefined \\def \\showISBNxiii  #1{\\unskip}     \\fi\n\\ifx \\showISSN     \\undefined \\def \\showISSN      #1{\\unskip}     \\fi\n\\ifx \\showLCCN     \\undefined \\def \\showLCCN      #1{\\unskip}     \\fi\n\\ifx \\shownote     \\undefined \\def \\shownote      #1{#1}          \\fi\n\\ifx \\showarticletitle \\undefined \\def \\showarticletitle #1{#1}   \\fi\n\\ifx \\showURL      \\undefined \\def \\showURL       {\\relax}        \\fi\n% The following commands are used for tagged output and should be\n% invisible to TeX\n\\providecommand\\bibfield[2]{#2}\n\\providecommand\\bibinfo[2]{#2}\n\\providecommand\\natexlab[1]{#1}\n\\providecommand\\showeprint[2][]{arXiv:#2}\n\n\\bibitem[\\protect\\citeauthoryear{Anderson}{Anderson}{1978}]%\n        {Anderson1978MaximumLE}\n\\bibfield{author}{\\bibinfo{person}{Theodore~W. Anderson}.}\n  \\bibinfo{year}{1978}\\natexlab{}.\n\\newblock \\showarticletitle{Maximum Likelihood Estimation for Vector\n  Autoregressive Moving Average Models}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Aste}{Aste}{2020}]%\n        {aste2020topological}\n\\bibfield{author}{\\bibinfo{person}{Tomaso Aste}.}\n  \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Topological regularization with information\n  filtering networks}.\n\\newblock \\bibinfo{journal}{\\emph{arXiv preprint arXiv:2005.04692}}\n  (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Aste and Matteo}{Aste and Matteo}{2017}]%\n        {CompNet4}\n\\bibfield{author}{\\bibinfo{person}{T. Aste} {and} \\bibinfo{person}{T. Matteo}.}\n  \\bibinfo{year}{2017}\\natexlab{}.\n\\newblock \\showarticletitle{Sparse Causality Network Retrieval from Short Time\n  Series}.\n\\newblock \\bibinfo{journal}{\\emph{Complex.}}  \\bibinfo{volume}{2017}\n  (\\bibinfo{year}{2017}), \\bibinfo{pages}{4518429:1--4518429:13}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Bai, Kolter, and Koltun}{Bai\n  et~al\\mbox{.}}{2018}]%\n        {Bai2018AnEE}\n\\bibfield{author}{\\bibinfo{person}{Shaojie Bai}, \\bibinfo{person}{J.~Zico\n  Kolter}, {and} \\bibinfo{person}{Vladlen Koltun}.}\n  \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{An Empirical Evaluation of Generic Convolutional\n  and Recurrent Networks for Sequence Modeling}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1803.01271}\n  (\\bibinfo{year}{2018}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Banerjee, Ghaoui, and d'Aspremont}{Banerjee\n  et~al\\mbox{.}}{2007}]%\n        {Banerjee2007ModelST}\n\\bibfield{author}{\\bibinfo{person}{Onureena Banerjee},\n  \\bibinfo{person}{Laurent~El Ghaoui}, {and} \\bibinfo{person}{Alexandre\n  d'Aspremont}.} \\bibinfo{year}{2007}\\natexlab{}.\n\\newblock \\showarticletitle{Model Selection Through Sparse Maximum Likelihood\n  Estimation}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/0707.0704}\n  (\\bibinfo{year}{2007}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Barfuss, Massara, Di~Matteo, and Aste}{Barfuss\n  et~al\\mbox{.}}{2016}]%\n        {CompNet7}\n\\bibfield{author}{\\bibinfo{person}{Wolfram Barfuss},\n  \\bibinfo{person}{Guido~Previde Massara}, \\bibinfo{person}{T. Di~Matteo},\n  {and} \\bibinfo{person}{Tomaso Aste}.} \\bibinfo{year}{2016}\\natexlab{}.\n\\newblock \\showarticletitle{Parsimonious modeling with information filtering\n  networks}.\n\\newblock \\bibinfo{journal}{\\emph{Physical Review E}} \\bibinfo{volume}{94},\n  \\bibinfo{number}{6} (\\bibinfo{date}{Dec} \\bibinfo{year}{2016}).\n\\newblock\n\\showISSN{2470-0053}\n\\urldef\\tempurl%\n\\url{https://doi.org/10.1103/physreve.94.062306}\n\\showDOI{\\tempurl}\n\n\n\\bibitem[\\protect\\citeauthoryear{Briola, Turiel, and Aste}{Briola\n  et~al\\mbox{.}}{2020}]%\n        {Briola2020DeepLM}\n\\bibfield{author}{\\bibinfo{person}{Antonio Briola}, \\bibinfo{person}{Jeremy~D.\n  Turiel}, {and} \\bibinfo{person}{Tomaso Aste}.}\n  \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Deep Learning Modeling of the Limit Order Book: A\n  Comparative Perspective}.\n\\newblock \\bibinfo{journal}{\\emph{ERN: Other Econometrics: Econometric \\&\n  Statistical Methods - Special Topics (Topic)}} (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Briola, Turiel, Marcaccioli, and Aste}{Briola\n  et~al\\mbox{.}}{2021}]%\n        {Briola2021DeepRL}\n\\bibfield{author}{\\bibinfo{person}{Antonio Briola}, \\bibinfo{person}{Jeremy~D.\n  Turiel}, \\bibinfo{person}{Riccardo Marcaccioli}, {and}\n  \\bibinfo{person}{Tomaso Aste}.} \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{Deep Reinforcement Learning for Active High\n  Frequency Trading}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/2101.07107}\n  (\\bibinfo{year}{2021}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Cai, He, and Han}{Cai et~al\\mbox{.}}{2006}]%\n        {Cai2006LearningWT}\n\\bibfield{author}{\\bibinfo{person}{Deng Cai}, \\bibinfo{person}{Xiaofei He},\n  {and} \\bibinfo{person}{Jiawei Han}.} \\bibinfo{year}{2006}\\natexlab{}.\n\\newblock \\showarticletitle{Learning with Tensor Representation}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Calandriello, Koutis, Lazaric, and\n  Valko}{Calandriello et~al\\mbox{.}}{2018}]%\n        {Calandriello2018ImprovedLG}\n\\bibfield{author}{\\bibinfo{person}{Daniele Calandriello},\n  \\bibinfo{person}{Ioannis Koutis}, \\bibinfo{person}{Alessandro Lazaric}, {and}\n  \\bibinfo{person}{Michal Valko}.} \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Improved Large-Scale Graph Learning through Ridge\n  Spectral Sparsification}. In \\bibinfo{booktitle}{\\emph{ICML}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Cao, Wang, Duan, Zhang, Zhu, Huang, Tong, Xu,\n  Bai, Tong, and Zhang}{Cao et~al\\mbox{.}}{2020}]%\n        {Cao2020SpectralTG}\n\\bibfield{author}{\\bibinfo{person}{Defu Cao}, \\bibinfo{person}{Yujing Wang},\n  \\bibinfo{person}{Juanyong Duan}, \\bibinfo{person}{Ce Zhang},\n  \\bibinfo{person}{Xia Zhu}, \\bibinfo{person}{Congrui Huang},\n  \\bibinfo{person}{Yunhai Tong}, \\bibinfo{person}{Bixiong Xu},\n  \\bibinfo{person}{Jing Bai}, \\bibinfo{person}{Jie Tong}, {and}\n  \\bibinfo{person}{Qi Zhang}.} \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Spectral Temporal Graph Neural Network for\n  Multivariate Time-series Forecasting}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/2103.07719}\n  (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Chakeri, Farhidzadeh, and Hall}{Chakeri\n  et~al\\mbox{.}}{2016}]%\n        {Chakeri2016SpectralSI}\n\\bibfield{author}{\\bibinfo{person}{Alireza Chakeri}, \\bibinfo{person}{Hamidreza\n  Farhidzadeh}, {and} \\bibinfo{person}{Lawrence~O. Hall}.}\n  \\bibinfo{year}{2016}\\natexlab{}.\n\\newblock \\showarticletitle{Spectral sparsification in spectral clustering}.\n\\newblock \\bibinfo{journal}{\\emph{2016 23rd International Conference on Pattern\n  Recognition (ICPR)}} (\\bibinfo{year}{2016}), \\bibinfo{pages}{2301--2306}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Chen, Kang, Xing, Zhao, and Milton}{Chen\n  et~al\\mbox{.}}{2018}]%\n        {Chen2018EstimatingLC}\n\\bibfield{author}{\\bibinfo{person}{Shuo Chen}, \\bibinfo{person}{Jian Kang},\n  \\bibinfo{person}{Yishi Xing}, \\bibinfo{person}{Yunpeng Zhao}, {and}\n  \\bibinfo{person}{Don Milton}.} \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Estimating large covariance matrix with network\n  topology for high-dimensional biomedical data}.\n\\newblock \\bibinfo{journal}{\\emph{Comput. Stat. Data Anal.}}\n  \\bibinfo{volume}{127} (\\bibinfo{year}{2018}), \\bibinfo{pages}{82--95}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Chen, Chen, Xie, Cao, Gao, and Feng}{Chen\n  et~al\\mbox{.}}{2020}]%\n        {Chen2020MultiRangeAB}\n\\bibfield{author}{\\bibinfo{person}{Weiqiu Chen}, \\bibinfo{person}{Ling Chen},\n  \\bibinfo{person}{Yu Xie}, \\bibinfo{person}{Wei Cao}, \\bibinfo{person}{Yusong\n  Gao}, {and} \\bibinfo{person}{Xiaojie Feng}.} \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Multi-Range Attentive Bicomponent Graph\n  Convolutional Network for Traffic Forecasting}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1911.12093}\n  (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Chen, Wiesel, Eldar, and Hero}{Chen\n  et~al\\mbox{.}}{2010}]%\n        {Chen2010ShrinkageAF}\n\\bibfield{author}{\\bibinfo{person}{Yilun Chen}, \\bibinfo{person}{Ami Wiesel},\n  \\bibinfo{person}{Yonina~C. Eldar}, {and} \\bibinfo{person}{Alfred~O. Hero}.}\n  \\bibinfo{year}{2010}\\natexlab{}.\n\\newblock \\showarticletitle{Shrinkage Algorithms for MMSE Covariance\n  Estimation}.\n\\newblock \\bibinfo{journal}{\\emph{IEEE Transactions on Signal Processing}}\n  \\bibinfo{volume}{58} (\\bibinfo{year}{2010}), \\bibinfo{pages}{5016--5029}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Chung, \u00c7aglar G{\\\"u}l\u00e7ehre, Cho, and\n  Bengio}{Chung et~al\\mbox{.}}{2014}]%\n        {Chung2014EmpiricalEO}\n\\bibfield{author}{\\bibinfo{person}{Junyoung Chung}, \\bibinfo{person}{\u00c7aglar\n  G{\\\"u}l\u00e7ehre}, \\bibinfo{person}{Kyunghyun Cho}, {and}\n  \\bibinfo{person}{Yoshua Bengio}.} \\bibinfo{year}{2014}\\natexlab{}.\n\\newblock \\showarticletitle{Empirical Evaluation of Gated Recurrent Neural\n  Networks on Sequence Modeling}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1412.3555}\n  (\\bibinfo{year}{2014}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Couillet and Mckay}{Couillet and\n  Mckay}{2014}]%\n        {Couillet2014LargeDA}\n\\bibfield{author}{\\bibinfo{person}{Romain Couillet} {and}\n  \\bibinfo{person}{Matthew~R. Mckay}.} \\bibinfo{year}{2014}\\natexlab{}.\n\\newblock \\showarticletitle{Large dimensional analysis and optimization of\n  robust shrinkage covariance matrix estimators}.\n\\newblock \\bibinfo{journal}{\\emph{J. Multivar. Anal.}}  \\bibinfo{volume}{131}\n  (\\bibinfo{year}{2014}), \\bibinfo{pages}{99--120}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Daniusis and Vaitkus}{Daniusis and\n  Vaitkus}{2008}]%\n        {Daniusis2008NeuralNW}\n\\bibfield{author}{\\bibinfo{person}{Povilas Daniusis} {and}\n  \\bibinfo{person}{Pranas Vaitkus}.} \\bibinfo{year}{2008}\\natexlab{}.\n\\newblock \\showarticletitle{Neural Network with Matrix Inputs}.\n\\newblock \\bibinfo{journal}{\\emph{Informatica}}  \\bibinfo{volume}{19}\n  (\\bibinfo{year}{2008}), \\bibinfo{pages}{477--486}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Dauphin, Fan, Auli, and Grangier}{Dauphin\n  et~al\\mbox{.}}{2017}]%\n        {Dauphin2017LanguageMW}\n\\bibfield{author}{\\bibinfo{person}{Yann Dauphin}, \\bibinfo{person}{Angela Fan},\n  \\bibinfo{person}{Michael Auli}, {and} \\bibinfo{person}{David Grangier}.}\n  \\bibinfo{year}{2017}\\natexlab{}.\n\\newblock \\showarticletitle{Language Modeling with Gated Convolutional\n  Networks}. In \\bibinfo{booktitle}{\\emph{ICML}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Donoho, Gavish, and Johnstone}{Donoho\n  et~al\\mbox{.}}{2018}]%\n        {Donoho2018OptimalSO}\n\\bibfield{author}{\\bibinfo{person}{David~L. Donoho}, \\bibinfo{person}{Matan\n  Gavish}, {and} \\bibinfo{person}{Iain~M. Johnstone}.}\n  \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Optimal Shrinkage of Eigenvalues in the Spiked\n  Covariance Model.}\n\\newblock \\bibinfo{journal}{\\emph{Annals of statistics}}  \\bibinfo{volume}{46\n  4} (\\bibinfo{year}{2018}), \\bibinfo{pages}{1742--1778}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Franceschi, Dieuleveut, and Jaggi}{Franceschi\n  et~al\\mbox{.}}{2019}]%\n        {Franceschi2019UnsupervisedSR}\n\\bibfield{author}{\\bibinfo{person}{Jean-Yves Franceschi},\n  \\bibinfo{person}{Aymeric Dieuleveut}, {and} \\bibinfo{person}{Martin Jaggi}.}\n  \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Unsupervised Scalable Representation Learning for\n  Multivariate Time Series}. In \\bibinfo{booktitle}{\\emph{NeurIPS}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Friedman, Hastie, and Tibshirani}{Friedman\n  et~al\\mbox{.}}{2008}]%\n        {CompNet9}\n\\bibfield{author}{\\bibinfo{person}{J. Friedman}, \\bibinfo{person}{T. Hastie},\n  {and} \\bibinfo{person}{R. Tibshirani}.} \\bibinfo{year}{2008}\\natexlab{}.\n\\newblock \\showarticletitle{Sparse inverse covariance estimation with the\n  graphical lasso.}\n\\newblock \\bibinfo{journal}{\\emph{Biostatistics}}  \\bibinfo{volume}{9 3}\n  (\\bibinfo{year}{2008}), \\bibinfo{pages}{432--41}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Fritz, Dorigatti, and R{\\\"u}gamer}{Fritz\n  et~al\\mbox{.}}{2021}]%\n        {Fritz2021CombiningGN}\n\\bibfield{author}{\\bibinfo{person}{Cornelius Fritz}, \\bibinfo{person}{Emilio\n  Dorigatti}, {and} \\bibinfo{person}{D. R{\\\"u}gamer}.}\n  \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{Combining Graph Neural Networks and Spatio-temporal\n  Disease Models to Predict COVID-19 Cases in Germany}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/2101.00661}\n  (\\bibinfo{year}{2021}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Gao, Guo, and Wang}{Gao et~al\\mbox{.}}{2017}]%\n        {Gao2017MatrixNN}\n\\bibfield{author}{\\bibinfo{person}{Junbin Gao}, \\bibinfo{person}{Yi Guo}, {and}\n  \\bibinfo{person}{Zhiyong Wang}.} \\bibinfo{year}{2017}\\natexlab{}.\n\\newblock \\showarticletitle{Matrix Neural Networks}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1601.03805}\n  (\\bibinfo{year}{2017}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Gatta, Moscato, Postiglione, and\n  Sperl{\\'i}}{Gatta et~al\\mbox{.}}{2021}]%\n        {LaGatta2021AnEN}\n\\bibfield{author}{\\bibinfo{person}{Valerio~La Gatta}, \\bibinfo{person}{Vincenzo\n  Moscato}, \\bibinfo{person}{Marco Postiglione}, {and}\n  \\bibinfo{person}{Giancarlo Sperl{\\'i}}.} \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{An Epidemiological Neural Network Exploiting\n  Dynamic Graph Structured Data Applied to the COVID-19 Outbreak}.\n\\newblock \\bibinfo{journal}{\\emph{IEEE Transactions on Big Data}}\n  \\bibinfo{volume}{7} (\\bibinfo{year}{2021}), \\bibinfo{pages}{45--55}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Grassia and Mangioni}{Grassia and\n  Mangioni}{2022}]%\n        {Grassia2022wsGATWA}\n\\bibfield{author}{\\bibinfo{person}{Marco Grassia} {and}\n  \\bibinfo{person}{Giuseppe Mangioni}.} \\bibinfo{year}{2022}\\natexlab{}.\n\\newblock \\showarticletitle{wsGAT: Weighted and Signed Graph Attention Networks\n  for Link Prediction}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/2109.11519}\n  (\\bibinfo{year}{2022}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Harris, Millman, van~der Walt, Gommers,\n  Virtanen, Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van\n  Kerkwijk, Brett, Haldane, Fern\u00e1ndez~del R\u00edo, Wiebe, Peterson,\n  G\u00e9rard-Marchant, Sheppard, Reddy, Weckesser, Abbasi, Gohlke, and\n  Oliphant}{Harris et~al\\mbox{.}}{2020}]%\n        {2020NumPy-Array}\n\\bibfield{author}{\\bibinfo{person}{Charles~R. Harris},\n  \\bibinfo{person}{K.~Jarrod Millman}, \\bibinfo{person}{St\u00e9fan~J van~der\n  Walt}, \\bibinfo{person}{Ralf Gommers}, \\bibinfo{person}{Pauli Virtanen},\n  \\bibinfo{person}{David Cournapeau}, \\bibinfo{person}{Eric Wieser},\n  \\bibinfo{person}{Julian Taylor}, \\bibinfo{person}{Sebastian Berg},\n  \\bibinfo{person}{Nathaniel~J. Smith}, \\bibinfo{person}{Robert Kern},\n  \\bibinfo{person}{Matti Picus}, \\bibinfo{person}{Stephan Hoyer},\n  \\bibinfo{person}{Marten~H. van Kerkwijk}, \\bibinfo{person}{Matthew Brett},\n  \\bibinfo{person}{Allan Haldane}, \\bibinfo{person}{Jaime Fern\u00e1ndez~del R\u00edo},\n  \\bibinfo{person}{Mark Wiebe}, \\bibinfo{person}{Pearu Peterson},\n  \\bibinfo{person}{Pierre G\u00e9rard-Marchant}, \\bibinfo{person}{Kevin Sheppard},\n  \\bibinfo{person}{Tyler Reddy}, \\bibinfo{person}{Warren Weckesser},\n  \\bibinfo{person}{Hameer Abbasi}, \\bibinfo{person}{Christoph Gohlke}, {and}\n  \\bibinfo{person}{Travis~E. Oliphant}.} \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Array programming with {NumPy}}.\n\\newblock \\bibinfo{journal}{\\emph{Nature}}  \\bibinfo{volume}{585}\n  (\\bibinfo{year}{2020}), \\bibinfo{pages}{357\u2013362}.\n\\newblock\n\\urldef\\tempurl%\n\\url{https://doi.org/10.1038/s41586-020-2649-2}\n\\showDOI{\\tempurl}\n\n\n\\bibitem[\\protect\\citeauthoryear{Hasanzadeh, Hajiramezanali, Boluki, Zhou,\n  Duffield, Narayanan, and Qian}{Hasanzadeh et~al\\mbox{.}}{2020}]%\n        {Hasanzadeh2020BayesianGN}\n\\bibfield{author}{\\bibinfo{person}{Arman Hasanzadeh}, \\bibinfo{person}{Ehsan\n  Hajiramezanali}, \\bibinfo{person}{Shahin Boluki}, \\bibinfo{person}{Mingyuan\n  Zhou}, \\bibinfo{person}{Nick~G. Duffield}, \\bibinfo{person}{Krishna~R.\n  Narayanan}, {and} \\bibinfo{person}{Xiaoning Qian}.}\n  \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Bayesian Graph Neural Networks with Adaptive\n  Connection Sampling}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/2006.04064}\n  (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Hou, Wang, Zhong, and Wei}{Hou\n  et~al\\mbox{.}}{2021}]%\n        {Hou2021STTraderAS}\n\\bibfield{author}{\\bibinfo{person}{Xiurui Hou}, \\bibinfo{person}{Kai Wang},\n  \\bibinfo{person}{Cheng Zhong}, {and} \\bibinfo{person}{Zhi Wei}.}\n  \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{ST-Trader: A Spatial-Temporal Deep Neural Network\n  for Modeling Stock Market Movement}.\n\\newblock \\bibinfo{journal}{\\emph{IEEE/CAA Journal of Automatica Sinica}}\n  \\bibinfo{volume}{8} (\\bibinfo{year}{2021}), \\bibinfo{pages}{1015--1024}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Kaggle}{Kaggle}{[n.d.]}]%\n        {kaggle}\n\\bibfield{author}{\\bibinfo{person}{Kaggle}.} \\bibinfo{year}{[n.d.]}\\natexlab{}.\n\\newblock \\bibinfo{title}{Kaggle: Store Item Demand Forecasting Challenge}.\n\\newblock\n  \\bibinfo{howpublished}{\\url{https://www.kaggle.com/c/demand-forecasting-kernels-only/data}}.\n\\newblock\n\\newblock\n\\shownote{Accessed: 2022-02-06.}\n\n\n\\bibitem[\\protect\\citeauthoryear{Kapoor, Ben, Liu, Perozzi, Barnes, Blais, and\n  O\u2019Banion}{Kapoor et~al\\mbox{.}}{2020}]%\n        {Kapoor2020ExaminingCF}\n\\bibfield{author}{\\bibinfo{person}{Amol Kapoor}, \\bibinfo{person}{Xue Ben},\n  \\bibinfo{person}{Luyang Liu}, \\bibinfo{person}{Bryan Perozzi},\n  \\bibinfo{person}{Matt Barnes}, \\bibinfo{person}{Martin~J. Blais}, {and}\n  \\bibinfo{person}{Shawn O\u2019Banion}.} \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Examining COVID-19 Forecasting using\n  Spatio-Temporal Graph Neural Networks}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/2007.03113}\n  (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Kascha}{Kascha}{2012}]%\n        {Kascha2012ACO}\n\\bibfield{author}{\\bibinfo{person}{Christian Kascha}.}\n  \\bibinfo{year}{2012}\\natexlab{}.\n\\newblock \\showarticletitle{A Comparison of Estimation Methods for Vector\n  Autoregressive Moving-Average Models}.\n\\newblock \\bibinfo{journal}{\\emph{Econometric Reviews}}  \\bibinfo{volume}{31}\n  (\\bibinfo{year}{2012}), \\bibinfo{pages}{297 -- 324}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Khodayar and Wang}{Khodayar and Wang}{2019}]%\n        {Khodayar2019SpatioTemporalGD}\n\\bibfield{author}{\\bibinfo{person}{Mahdi Khodayar} {and}\n  \\bibinfo{person}{Jianhui Wang}.} \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Spatio-Temporal Graph Deep Neural Network for\n  Short-Term Wind Speed Forecasting}.\n\\newblock \\bibinfo{journal}{\\emph{IEEE Transactions on Sustainable Energy}}\n  \\bibinfo{volume}{10} (\\bibinfo{year}{2019}), \\bibinfo{pages}{670--681}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Kim and Oh}{Kim and Oh}{2021}]%\n        {Kim2021HowTF}\n\\bibfield{author}{\\bibinfo{person}{Dongkwan Kim} {and}\n  \\bibinfo{person}{Alice~H. Oh}.} \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{How to Find Your Friendly Neighborhood: Graph\n  Attention Design with Self-Supervision}. In \\bibinfo{booktitle}{\\emph{ICLR}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Kipf and Welling}{Kipf and Welling}{2017}]%\n        {Kipf2017SemiSupervisedCW}\n\\bibfield{author}{\\bibinfo{person}{Thomas Kipf} {and} \\bibinfo{person}{Max\n  Welling}.} \\bibinfo{year}{2017}\\natexlab{}.\n\\newblock \\showarticletitle{Semi-Supervised Classification with Graph\n  Convolutional Networks}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1609.02907}\n  (\\bibinfo{year}{2017}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Kojaku and Masuda}{Kojaku and Masuda}{2019}]%\n        {Kojaku2019ConstructingNB}\n\\bibfield{author}{\\bibinfo{person}{Sadamori Kojaku} {and}\n  \\bibinfo{person}{Naoki Masuda}.} \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Constructing networks by filtering correlation\n  matrices: a null model approach}.\n\\newblock \\bibinfo{journal}{\\emph{Proceedings of the Royal Society A}}\n  \\bibinfo{volume}{475} (\\bibinfo{year}{2019}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Kruskal}{Kruskal}{1956}]%\n        {CompNet10}\n\\bibfield{author}{\\bibinfo{person}{Joseph~B. Kruskal}.}\n  \\bibinfo{year}{1956}\\natexlab{}.\n\\newblock \\showarticletitle{On the shortest spanning subtree of a graph and the\n  traveling salesman problem}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Lai, Chang, Yang, and Liu}{Lai\n  et~al\\mbox{.}}{2018}]%\n        {Lai2018ModelingLA}\n\\bibfield{author}{\\bibinfo{person}{Guokun Lai}, \\bibinfo{person}{Wei-Cheng\n  Chang}, \\bibinfo{person}{Yiming Yang}, {and} \\bibinfo{person}{Hanxiao Liu}.}\n  \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Modeling Long- and Short-Term Temporal Patterns\n  with Deep Neural Networks}.\n\\newblock \\bibinfo{journal}{\\emph{The 41st International ACM SIGIR Conference\n  on Research \\& Development in Information Retrieval}} (\\bibinfo{year}{2018}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Ledoit and Wolf}{Ledoit and Wolf}{2003}]%\n        {Ledoit2003HoneyIS}\n\\bibfield{author}{\\bibinfo{person}{Olivier Ledoit} {and}\n  \\bibinfo{person}{Michael Wolf}.} \\bibinfo{year}{2003}\\natexlab{}.\n\\newblock \\showarticletitle{Honey, I Shrunk the Sample Covariance Matrix}.\n\\newblock \\bibinfo{journal}{\\emph{Capital Markets: Asset Pricing \\& Valuation}}\n  (\\bibinfo{year}{2003}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Ledoit and Wolf}{Ledoit and Wolf}{2004}]%\n        {Ledoit2004AWE}\n\\bibfield{author}{\\bibinfo{person}{Olivier Ledoit} {and}\n  \\bibinfo{person}{Michael Wolf}.} \\bibinfo{year}{2004}\\natexlab{}.\n\\newblock \\showarticletitle{A well-conditioned estimator for large-dimensional\n  covariance matrices}.\n\\newblock \\bibinfo{journal}{\\emph{Journal of Multivariate Analysis}}\n  \\bibinfo{volume}{88} (\\bibinfo{year}{2004}), \\bibinfo{pages}{365--411}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Ledoit and Wolf}{Ledoit and Wolf}{2012}]%\n        {Ledoit2012NonlinearSE}\n\\bibfield{author}{\\bibinfo{person}{Olivier Ledoit} {and}\n  \\bibinfo{person}{Michael Wolf}.} \\bibinfo{year}{2012}\\natexlab{}.\n\\newblock \\showarticletitle{Nonlinear shrinkage estimation of large-dimensional\n  covariance matrices}.\n\\newblock \\bibinfo{journal}{\\emph{arXiv: Statistics Theory}}\n  (\\bibinfo{year}{2012}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Lee and Seregina}{Lee and Seregina}{2020}]%\n        {Lee2020OptimalPU}\n\\bibfield{author}{\\bibinfo{person}{Tae-Hwy Lee} {and}\n  \\bibinfo{person}{Ekaterina Seregina}.} \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Optimal Portfolio Using Factor Graphical Lasso}.\n\\newblock \\bibinfo{journal}{\\emph{arXiv: Econometrics}} (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Li, Yu, Shahabi, and Liu}{Li\n  et~al\\mbox{.}}{2018}]%\n        {Li2018DiffusionCR}\n\\bibfield{author}{\\bibinfo{person}{Yaguang Li}, \\bibinfo{person}{Rose Yu},\n  \\bibinfo{person}{Cyrus Shahabi}, {and} \\bibinfo{person}{Yan Liu}.}\n  \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Diffusion Convolutional Recurrent Neural Network:\n  Data-Driven Traffic Forecasting}.\n\\newblock \\bibinfo{journal}{\\emph{arXiv: Learning}} (\\bibinfo{year}{2018}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Luo, Cheng, Yu, Zong, Ni, Chen, and Zhang}{Luo\n  et~al\\mbox{.}}{2021}]%\n        {Luo2021LearningTD}\n\\bibfield{author}{\\bibinfo{person}{Dongsheng Luo}, \\bibinfo{person}{Wei Cheng},\n  \\bibinfo{person}{Wenchao Yu}, \\bibinfo{person}{Bo Zong},\n  \\bibinfo{person}{Jingchao Ni}, \\bibinfo{person}{Haifeng Chen}, {and}\n  \\bibinfo{person}{Xiang Zhang}.} \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{Learning to Drop: Robust Graph Neural Network via\n  Topological Denoising}.\n\\newblock \\bibinfo{journal}{\\emph{Proceedings of the 14th ACM International\n  Conference on Web Search and Data Mining}} (\\bibinfo{year}{2021}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{{Markowitz}}{{Markowitz}}{1952}]%\n        {Markowitz}\n\\bibfield{author}{\\bibinfo{person}{H. {Markowitz}}.}\n  \\bibinfo{year}{1952}\\natexlab{}.\n\\newblock \\showarticletitle{Portfolio Selection}.\n\\newblock \\bibinfo{journal}{\\emph{The Journal of Finance}} \\bibinfo{volume}{7},\n  \\bibinfo{number}{1} (\\bibinfo{year}{1952}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Massara and Aste}{Massara and Aste}{2019a}]%\n        {CompNet6}\n\\bibfield{author}{\\bibinfo{person}{Guido~Previde Massara} {and}\n  \\bibinfo{person}{Tomaso Aste}.} \\bibinfo{year}{2019}\\natexlab{a}.\n\\newblock \\showarticletitle{Learning Clique Forests}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{1905.02266}\n  (\\bibinfo{year}{2019}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Massara and Aste}{Massara and Aste}{2019b}]%\n        {Massara2019LearningCF}\n\\bibfield{author}{\\bibinfo{person}{Guido~Previde Massara} {and}\n  \\bibinfo{person}{Tomaso Aste}.} \\bibinfo{year}{2019}\\natexlab{b}.\n\\newblock \\showarticletitle{Learning Clique Forests}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1905.02266}\n  (\\bibinfo{year}{2019}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Massara, Matteo, and Aste}{Massara\n  et~al\\mbox{.}}{2017}]%\n        {CompNet5}\n\\bibfield{author}{\\bibinfo{person}{Guido~Previde Massara}, \\bibinfo{person}{T.\n  Matteo}, {and} \\bibinfo{person}{T. Aste}.} \\bibinfo{year}{2017}\\natexlab{}.\n\\newblock \\showarticletitle{Network Filtering for Big Data: Triangulated\n  Maximally Filtered Graph}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1505.02445}\n  (\\bibinfo{year}{2017}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Matsunaga, Suzumura, and Takahashi}{Matsunaga\n  et~al\\mbox{.}}{2019}]%\n        {Matsunaga2019ExploringGN}\n\\bibfield{author}{\\bibinfo{person}{Daiki Matsunaga}, \\bibinfo{person}{Toyotaro\n  Suzumura}, {and} \\bibinfo{person}{Toshihiro Takahashi}.}\n  \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Exploring Graph Neural Networks for Stock Market\n  Predictions with Rolling Window Analysis}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1909.10660}\n  (\\bibinfo{year}{2019}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Mazumder and Hastie}{Mazumder and\n  Hastie}{2012}]%\n        {Mazumder2012TheGL}\n\\bibfield{author}{\\bibinfo{person}{Rahul Mazumder} {and}\n  \\bibinfo{person}{Trevor~J. Hastie}.} \\bibinfo{year}{2012}\\natexlab{}.\n\\newblock \\showarticletitle{The Graphical Lasso: New Insights and\n  Alternatives}.\n\\newblock \\bibinfo{journal}{\\emph{Electronic journal of statistics}}\n  \\bibinfo{volume}{6} (\\bibinfo{year}{2012}), \\bibinfo{pages}{2125--2149}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Meinshausen and Buhlmann}{Meinshausen and\n  Buhlmann}{2006}]%\n        {Meinshausen2006HighdimensionalGA}\n\\bibfield{author}{\\bibinfo{person}{Nicolai Meinshausen} {and}\n  \\bibinfo{person}{Peter Buhlmann}.} \\bibinfo{year}{2006}\\natexlab{}.\n\\newblock \\showarticletitle{High-dimensional graphs and variable selection with\n  the Lasso}.\n\\newblock \\bibinfo{journal}{\\emph{Annals of Statistics}}  \\bibinfo{volume}{34}\n  (\\bibinfo{year}{2006}), \\bibinfo{pages}{1436--1462}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Millington and Niranjan}{Millington and\n  Niranjan}{2017}]%\n        {Millington2017RobustPR}\n\\bibfield{author}{\\bibinfo{person}{Tristan Millington} {and}\n  \\bibinfo{person}{Mahesan Niranjan}.} \\bibinfo{year}{2017}\\natexlab{}.\n\\newblock \\showarticletitle{Robust Portfolio Risk Minimization Using the\n  Graphical Lasso}. In \\bibinfo{booktitle}{\\emph{ICONIP}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Ne{\\v{s}}et{\\v{r}}il, Milkov{\\'a}, and\n  Ne{\\v{s}}et{\\v{r}}ilov{\\'a}}{Ne{\\v{s}}et{\\v{r}}il et~al\\mbox{.}}{2001}]%\n        {nevsetvril2001otakar}\n\\bibfield{author}{\\bibinfo{person}{Jaroslav Ne{\\v{s}}et{\\v{r}}il},\n  \\bibinfo{person}{Eva Milkov{\\'a}}, {and} \\bibinfo{person}{Helena\n  Ne{\\v{s}}et{\\v{r}}ilov{\\'a}}.} \\bibinfo{year}{2001}\\natexlab{}.\n\\newblock \\showarticletitle{Otakar Boruvka on minimum spanning tree problem\n  Translation of both the 1926 papers, comments, history}.\n\\newblock \\bibinfo{journal}{\\emph{Discrete mathematics}} \\bibinfo{volume}{233},\n  \\bibinfo{number}{1-3} (\\bibinfo{year}{2001}), \\bibinfo{pages}{3--36}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Pacreau, Lezmi, and Xu}{Pacreau\n  et~al\\mbox{.}}{2021}]%\n        {Amudi}\n\\bibfield{author}{\\bibinfo{person}{Gr\u00e9goire Pacreau}, \\bibinfo{person}{Edmond\n  Lezmi}, {and} \\bibinfo{person}{Jiali Xu}.} \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{Graph Neural Networks for Asset Management}.\n\\newblock \\bibinfo{journal}{\\emph{SSRN}} (\\bibinfo{year}{2021}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Pedregosa, Varoquaux, Gramfort, Michel,\n  Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,\n  Cournapeau, Brucher, Perrot, and Duchesnay}{Pedregosa et~al\\mbox{.}}{2011}]%\n        {scikit-learn}\n\\bibfield{author}{\\bibinfo{person}{F. Pedregosa}, \\bibinfo{person}{G.\n  Varoquaux}, \\bibinfo{person}{A. Gramfort}, \\bibinfo{person}{V. Michel},\n  \\bibinfo{person}{B. Thirion}, \\bibinfo{person}{O. Grisel},\n  \\bibinfo{person}{M. Blondel}, \\bibinfo{person}{P. Prettenhofer},\n  \\bibinfo{person}{R. Weiss}, \\bibinfo{person}{V. Dubourg}, \\bibinfo{person}{J.\n  Vanderplas}, \\bibinfo{person}{A. Passos}, \\bibinfo{person}{D. Cournapeau},\n  \\bibinfo{person}{M. Brucher}, \\bibinfo{person}{M. Perrot}, {and}\n  \\bibinfo{person}{E. Duchesnay}.} \\bibinfo{year}{2011}\\natexlab{}.\n\\newblock \\showarticletitle{Scikit-learn: Machine Learning in {P}ython}.\n\\newblock \\bibinfo{journal}{\\emph{Journal of Machine Learning Research}}\n  \\bibinfo{volume}{12} (\\bibinfo{year}{2011}), \\bibinfo{pages}{2825--2830}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Pillay and Moodley}{Pillay and\n  Moodley}{2022}]%\n        {Pillay2022ExploringGN}\n\\bibfield{author}{\\bibinfo{person}{Kialan Pillay} {and}\n  \\bibinfo{person}{Deshendran Moodley}.} \\bibinfo{year}{2022}\\natexlab{}.\n\\newblock \\showarticletitle{Exploring Graph Neural Networks for Stock Market\n  Prediction on the JSE}.\n\\newblock \\bibinfo{journal}{\\emph{Artificial Intelligence Research}}\n  (\\bibinfo{year}{2022}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Prim}{Prim}{1957}]%\n        {CompNet11}\n\\bibfield{author}{\\bibinfo{person}{Robert~C. Prim}.}\n  \\bibinfo{year}{1957}\\natexlab{}.\n\\newblock \\showarticletitle{Shortest connection networks and some\n  generalizations}.\n\\newblock \\bibinfo{journal}{\\emph{Bell System Technical Journal}}\n  \\bibinfo{volume}{36} (\\bibinfo{year}{1957}), \\bibinfo{pages}{1389--1401}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Procacci and Aste}{Procacci and Aste}{2021}]%\n        {Procacci2021ForecastingMS}\n\\bibfield{author}{\\bibinfo{person}{Pier~Francesco Procacci} {and}\n  \\bibinfo{person}{Tomaso Aste}.} \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{Forecasting market states}.\n\\newblock \\bibinfo{journal}{\\emph{Machine Learning and AI in Finance}}\n  (\\bibinfo{year}{2021}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Rangapuram, Seeger, Gasthaus, Stella, Wang,\n  and Januschowski}{Rangapuram et~al\\mbox{.}}{2018}]%\n        {Rangapuram2018DeepSS}\n\\bibfield{author}{\\bibinfo{person}{Syama~Sundar Rangapuram},\n  \\bibinfo{person}{Matthias~W. Seeger}, \\bibinfo{person}{Jan Gasthaus},\n  \\bibinfo{person}{Lorenzo Stella}, \\bibinfo{person}{Bernie Wang}, {and}\n  \\bibinfo{person}{Tim Januschowski}.} \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Deep State Space Models for Time Series\n  Forecasting}. In \\bibinfo{booktitle}{\\emph{NeurIPS}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Rong, bing Huang, Xu, and Huang}{Rong\n  et~al\\mbox{.}}{2020}]%\n        {Rong2020DropEdgeTD}\n\\bibfield{author}{\\bibinfo{person}{Yu Rong}, \\bibinfo{person}{Wen bing Huang},\n  \\bibinfo{person}{Tingyang Xu}, {and} \\bibinfo{person}{Junzhou Huang}.}\n  \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{DropEdge: Towards Deep Graph Convolutional Networks\n  on Node Classification}. In \\bibinfo{booktitle}{\\emph{ICLR}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Rumelhart, Hinton, and Williams}{Rumelhart\n  et~al\\mbox{.}}{1986}]%\n        {Rumelhart1986LearningIR}\n\\bibfield{author}{\\bibinfo{person}{David~E. Rumelhart},\n  \\bibinfo{person}{Geoffrey~E. Hinton}, {and} \\bibinfo{person}{Ronald~J.\n  Williams}.} \\bibinfo{year}{1986}\\natexlab{}.\n\\newblock \\showarticletitle{Learning internal representations by error\n  propagation}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Sak, Senior, and Beaufays}{Sak\n  et~al\\mbox{.}}{2014}]%\n        {Sak2014LongSM}\n\\bibfield{author}{\\bibinfo{person}{Hasim Sak}, \\bibinfo{person}{Andrew~W.\n  Senior}, {and} \\bibinfo{person}{Fran\u00e7oise Beaufays}.}\n  \\bibinfo{year}{2014}\\natexlab{}.\n\\newblock \\showarticletitle{Long short-term memory recurrent neural network\n  architectures for large scale acoustic modeling}. In\n  \\bibinfo{booktitle}{\\emph{INTERSPEECH}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Sen, Yu, and Dhillon}{Sen\n  et~al\\mbox{.}}{2019}]%\n        {Sen2019ThinkGA}\n\\bibfield{author}{\\bibinfo{person}{Rajat Sen}, \\bibinfo{person}{Hsiang-Fu Yu},\n  {and} \\bibinfo{person}{Inderjit~S. Dhillon}.}\n  \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Think Globally, Act Locally: A Deep Neural Network\n  Approach to High-Dimensional Time Series Forecasting}. In\n  \\bibinfo{booktitle}{\\emph{NeurIPS}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Sesti, Luis, Crawley, and Cameron}{Sesti\n  et~al\\mbox{.}}{2021}]%\n        {Sesti2021IntegratingLA}\n\\bibfield{author}{\\bibinfo{person}{Nathan Sesti}, \\bibinfo{person}{Juan\n  Jose~Garau Luis}, \\bibinfo{person}{Edward~F. Crawley}, {and}\n  \\bibinfo{person}{Bruce~G. Cameron}.} \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{Integrating LSTMs and GNNs for COVID-19\n  Forecasting}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/2108.10052}\n  (\\bibinfo{year}{2021}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Shen, qi~Cheng, and xing Fang}{Shen\n  et~al\\mbox{.}}{2010}]%\n        {Shen2010CovarianceCM}\n\\bibfield{author}{\\bibinfo{person}{Huawei Shen}, \\bibinfo{person}{Xue qi\n  Cheng}, {and} \\bibinfo{person}{Bin xing Fang}.}\n  \\bibinfo{year}{2010}\\natexlab{}.\n\\newblock \\showarticletitle{Covariance, correlation matrix, and the multiscale\n  community structure of networks.}\n\\newblock \\bibinfo{journal}{\\emph{Physical review. E, Statistical, nonlinear,\n  and soft matter physics}}  \\bibinfo{volume}{82 1 Pt 2}\n  (\\bibinfo{year}{2010}), \\bibinfo{pages}{016114}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Shi, Zhang, Cheng, and Lu}{Shi\n  et~al\\mbox{.}}{2019}]%\n        {Shi2019TwoStreamAG}\n\\bibfield{author}{\\bibinfo{person}{Lei Shi}, \\bibinfo{person}{Yifan Zhang},\n  \\bibinfo{person}{Jian Cheng}, {and} \\bibinfo{person}{Hanqing Lu}.}\n  \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Two-Stream Adaptive Graph Convolutional Networks\n  for Skeleton-Based Action Recognition}.\n\\newblock \\bibinfo{journal}{\\emph{2019 IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition (CVPR)}} (\\bibinfo{year}{2019}),\n  \\bibinfo{pages}{12018--12027}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Shi, Chen, Wang, Yeung, Wong, and chun\n  Woo}{Shi et~al\\mbox{.}}{2015}]%\n        {Shi2015ConvolutionalLN}\n\\bibfield{author}{\\bibinfo{person}{Xingjian Shi}, \\bibinfo{person}{Zhourong\n  Chen}, \\bibinfo{person}{Hao Wang}, \\bibinfo{person}{Dit-Yan Yeung},\n  \\bibinfo{person}{Wai-Kin Wong}, {and} \\bibinfo{person}{Wang chun Woo}.}\n  \\bibinfo{year}{2015}\\natexlab{}.\n\\newblock \\showarticletitle{Convolutional LSTM Network: A Machine Learning\n  Approach for Precipitation Nowcasting}. In \\bibinfo{booktitle}{\\emph{NIPS}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Shih, Sun, and yi~Lee}{Shih\n  et~al\\mbox{.}}{2019}]%\n        {Shih2019TemporalPA}\n\\bibfield{author}{\\bibinfo{person}{Shun-Yao Shih}, \\bibinfo{person}{Fan-Keng\n  Sun}, {and} \\bibinfo{person}{Hung yi Lee}.} \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Temporal pattern attention for multivariate time\n  series forecasting}.\n\\newblock \\bibinfo{journal}{\\emph{Machine Learning}} (\\bibinfo{year}{2019}),\n  \\bibinfo{pages}{1--21}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Telesford, Simpson, Burdette, Hayasaka, and\n  Laurienti}{Telesford et~al\\mbox{.}}{2011}]%\n        {CompNet8}\n\\bibfield{author}{\\bibinfo{person}{Qawi~K. Telesford}, \\bibinfo{person}{S.\n  Simpson}, \\bibinfo{person}{J. Burdette}, \\bibinfo{person}{S. Hayasaka}, {and}\n  \\bibinfo{person}{P. Laurienti}.} \\bibinfo{year}{2011}\\natexlab{}.\n\\newblock \\showarticletitle{The Brain as a Complex System: Using Network\n  Science as a Tool for Understanding the Brain}.\n\\newblock \\bibinfo{journal}{\\emph{Brain connectivity}}  \\bibinfo{volume}{1 (4)}\n  (\\bibinfo{year}{2011}), \\bibinfo{pages}{295--308}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Tumminello, Aste, Di~Matteo, and\n  Mantegna}{Tumminello et~al\\mbox{.}}{2005}]%\n        {CompNet3}\n\\bibfield{author}{\\bibinfo{person}{M. Tumminello}, \\bibinfo{person}{T. Aste},\n  \\bibinfo{person}{T. Di~Matteo}, {and} \\bibinfo{person}{R.~N. Mantegna}.}\n  \\bibinfo{year}{2005}\\natexlab{}.\n\\newblock \\showarticletitle{A tool for filtering information in complex\n  systems}.\n\\newblock \\bibinfo{journal}{\\emph{Proceedings of the National Academy of\n  Sciences}} \\bibinfo{volume}{102}, \\bibinfo{number}{30}\n  (\\bibinfo{year}{2005}), \\bibinfo{pages}{10421--10426}.\n\\newblock\n\\showISSN{0027-8424}\n\\urldef\\tempurl%\n\\url{https://doi.org/10.1073/pnas.0500298102}\n\\showDOI{\\tempurl}\n\\showeprint{https://www.pnas.org/content/102/30/10421.full.pdf}\n\n\n\\bibitem[\\protect\\citeauthoryear{Turiel, Barucca, and Aste}{Turiel\n  et~al\\mbox{.}}{2020}]%\n        {Turiel2020SimplicialPO}\n\\bibfield{author}{\\bibinfo{person}{Jeremy~D. Turiel}, \\bibinfo{person}{Paolo\n  Barucca}, {and} \\bibinfo{person}{Tomaso Aste}.}\n  \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Simplicial persistence of financial markets:\n  filtering, generative processes and portfolio risk}.\n\\newblock \\bibinfo{journal}{\\emph{arXiv: Statistical Finance}}\n  (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Vaswani, Shazeer, Parmar, Uszkoreit, Jones,\n  Gomez, Kaiser, and Polosukhin}{Vaswani et~al\\mbox{.}}{2017}]%\n        {Vaswani2017AttentionIA}\n\\bibfield{author}{\\bibinfo{person}{Ashish Vaswani}, \\bibinfo{person}{Noam~M.\n  Shazeer}, \\bibinfo{person}{Niki Parmar}, \\bibinfo{person}{Jakob Uszkoreit},\n  \\bibinfo{person}{Llion Jones}, \\bibinfo{person}{Aidan~N. Gomez},\n  \\bibinfo{person}{Lukasz Kaiser}, {and} \\bibinfo{person}{Illia Polosukhin}.}\n  \\bibinfo{year}{2017}\\natexlab{}.\n\\newblock \\showarticletitle{Attention is All you Need}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1706.03762}\n  (\\bibinfo{year}{2017}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Wan, Mei, Wang, Liu, and Yang}{Wan\n  et~al\\mbox{.}}{2019}]%\n        {Wan2019MultivariateTC}\n\\bibfield{author}{\\bibinfo{person}{Renzhuo Wan}, \\bibinfo{person}{Shuping Mei},\n  \\bibinfo{person}{Jun Wang}, \\bibinfo{person}{Min Liu}, {and}\n  \\bibinfo{person}{F. Yang}.} \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Multivariate Temporal Convolutional Network: A Deep\n  Neural Networks Approach for Multivariate Time Series Forecasting}.\n\\newblock \\bibinfo{journal}{\\emph{Electronics}} (\\bibinfo{year}{2019}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Wang and Aste}{Wang and Aste}{2021}]%\n        {Wang2021DynamicPO}\n\\bibfield{author}{\\bibinfo{person}{Yuanrong Wang} {and} \\bibinfo{person}{Tomaso\n  Aste}.} \\bibinfo{year}{2021}\\natexlab{}.\n\\newblock \\showarticletitle{Dynamic Portfolio Optimization with Inverse\n  Covariance Clustering}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Wu, Pan, Long, Jiang, Chang, and Zhang}{Wu\n  et~al\\mbox{.}}{2020}]%\n        {Wu2020ConnectingTD}\n\\bibfield{author}{\\bibinfo{person}{Zonghan Wu}, \\bibinfo{person}{Shirui Pan},\n  \\bibinfo{person}{Guodong Long}, \\bibinfo{person}{Jing Jiang},\n  \\bibinfo{person}{Xiaojun Chang}, {and} \\bibinfo{person}{Chengqi Zhang}.}\n  \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Connecting the Dots: Multivariate Time Series\n  Forecasting with Graph Neural Networks}.\n\\newblock \\bibinfo{journal}{\\emph{Proceedings of the 26th ACM SIGKDD\n  International Conference on Knowledge Discovery \\& Data Mining}}\n  (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Wu, Pan, Long, Jiang, and Zhang}{Wu\n  et~al\\mbox{.}}{2019}]%\n        {Wu2019GraphWF}\n\\bibfield{author}{\\bibinfo{person}{Zonghan Wu}, \\bibinfo{person}{Shirui Pan},\n  \\bibinfo{person}{Guodong Long}, \\bibinfo{person}{Jing Jiang}, {and}\n  \\bibinfo{person}{Chengqi Zhang}.} \\bibinfo{year}{2019}\\natexlab{}.\n\\newblock \\showarticletitle{Graph WaveNet for Deep Spatial-Temporal Graph\n  Modeling}. In \\bibinfo{booktitle}{\\emph{IJCAI}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Yan, Xiong, and Lin}{Yan\n  et~al\\mbox{.}}{2018}]%\n        {Yan2018SpatialTG}\n\\bibfield{author}{\\bibinfo{person}{Sijie Yan}, \\bibinfo{person}{Yuanjun Xiong},\n  {and} \\bibinfo{person}{Dahua Lin}.} \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Spatial Temporal Graph Convolutional Networks for\n  Skeleton-Based Action Recognition}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1801.07455}\n  (\\bibinfo{year}{2018}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Ying, You, Morris, Ren, Hamilton, and\n  Leskovec}{Ying et~al\\mbox{.}}{2018}]%\n        {Ying2018HierarchicalGR}\n\\bibfield{author}{\\bibinfo{person}{Rex Ying}, \\bibinfo{person}{Jiaxuan You},\n  \\bibinfo{person}{Christopher Morris}, \\bibinfo{person}{Xiang Ren},\n  \\bibinfo{person}{William~L. Hamilton}, {and} \\bibinfo{person}{Jure\n  Leskovec}.} \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Hierarchical Graph Representation Learning with\n  Differentiable Pooling}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1806.08804}\n  (\\bibinfo{year}{2018}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Young and Shellswell}{Young and\n  Shellswell}{1972}]%\n        {Young1972TimeSA}\n\\bibfield{author}{\\bibinfo{person}{P.~J. Young} {and} \\bibinfo{person}{Stephen\n  Shellswell}.} \\bibinfo{year}{1972}\\natexlab{}.\n\\newblock \\showarticletitle{Time series analysis, forecasting and control}.\n\\newblock \\bibinfo{journal}{\\emph{IEEE Trans. Automat. Control}}\n  \\bibinfo{volume}{17} (\\bibinfo{year}{1972}), \\bibinfo{pages}{281--283}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Yu, Yin, and Zhu}{Yu et~al\\mbox{.}}{2018}]%\n        {Yu2018SpatioTemporalGC}\n\\bibfield{author}{\\bibinfo{person}{Ting Yu}, \\bibinfo{person}{Haoteng Yin},\n  {and} \\bibinfo{person}{Zhanxing Zhu}.} \\bibinfo{year}{2018}\\natexlab{}.\n\\newblock \\showarticletitle{Spatio-Temporal Graph Convolutional Networks: A\n  Deep Learning Framework for Traffic Forecasting}. In\n  \\bibinfo{booktitle}{\\emph{IJCAI}}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Yuan and Lin}{Yuan and Lin}{2007}]%\n        {Yuan2007ModelSA}\n\\bibfield{author}{\\bibinfo{person}{Ming Yuan} {and} \\bibinfo{person}{Yi Lin}.}\n  \\bibinfo{year}{2007}\\natexlab{}.\n\\newblock \\showarticletitle{Model selection and estimation in the Gaussian\n  graphical model}.\n\\newblock \\bibinfo{journal}{\\emph{Biometrika}}  \\bibinfo{volume}{94}\n  (\\bibinfo{year}{2007}), \\bibinfo{pages}{19--35}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Yuan, Yu, Yin, and Wang}{Yuan\n  et~al\\mbox{.}}{2020}]%\n        {Yuan2020ImprovedLD}\n\\bibfield{author}{\\bibinfo{person}{Xin Yuan}, \\bibinfo{person}{Weiqin Yu},\n  \\bibinfo{person}{Zhixian Yin}, {and} \\bibinfo{person}{Guoqiang Wang}.}\n  \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{Improved Large Dynamic Covariance Matrix Estimation\n  With Graphical Lasso and Its Application in Portfolio Selection}.\n\\newblock \\bibinfo{journal}{\\emph{IEEE Access}}  \\bibinfo{volume}{8}\n  (\\bibinfo{year}{2020}), \\bibinfo{pages}{189179--189188}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Zhang, Aggarwal, and Qi}{Zhang\n  et~al\\mbox{.}}{2017}]%\n        {Zhang2017StockPP}\n\\bibfield{author}{\\bibinfo{person}{Liheng Zhang}, \\bibinfo{person}{Charu~C.\n  Aggarwal}, {and} \\bibinfo{person}{Guo-Jun Qi}.}\n  \\bibinfo{year}{2017}\\natexlab{}.\n\\newblock \\showarticletitle{Stock Price Prediction via Discovering\n  Multi-Frequency Trading Patterns}.\n\\newblock \\bibinfo{journal}{\\emph{Proceedings of the 23rd ACM SIGKDD\n  International Conference on Knowledge Discovery and Data Mining}}\n  (\\bibinfo{year}{2017}).\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Zhao, Song, Zhang, Liu, Wang, Lin, Deng, and\n  Li}{Zhao et~al\\mbox{.}}{2020}]%\n        {Zhao2020TGCNAT}\n\\bibfield{author}{\\bibinfo{person}{Ling Zhao}, \\bibinfo{person}{Yujiao Song},\n  \\bibinfo{person}{Chao Zhang}, \\bibinfo{person}{Yu Liu}, \\bibinfo{person}{Pu\n  Wang}, \\bibinfo{person}{Tao Lin}, \\bibinfo{person}{Min Deng}, {and}\n  \\bibinfo{person}{Haifeng Li}.} \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{T-GCN: A Temporal Graph Convolutional Network for\n  Traffic Prediction}.\n\\newblock \\bibinfo{journal}{\\emph{IEEE Transactions on Intelligent\n  Transportation Systems}}  \\bibinfo{volume}{21} (\\bibinfo{year}{2020}),\n  \\bibinfo{pages}{3848--3858}.\n\\newblock\n\n\n\\bibitem[\\protect\\citeauthoryear{Zheng, Fan, Wang, and Qi}{Zheng\n  et~al\\mbox{.}}{2020}]%\n        {Zheng2020GMANAG}\n\\bibfield{author}{\\bibinfo{person}{Chuanpan Zheng}, \\bibinfo{person}{Xiaoliang\n  Fan}, \\bibinfo{person}{Cheng Wang}, {and} \\bibinfo{person}{Jianzhong Qi}.}\n  \\bibinfo{year}{2020}\\natexlab{}.\n\\newblock \\showarticletitle{GMAN: A Graph Multi-Attention Network for Traffic\n  Prediction}.\n\\newblock \\bibinfo{journal}{\\emph{ArXiv}}  \\bibinfo{volume}{abs/1911.08415}\n  (\\bibinfo{year}{2020}).\n\\newblock\n\n\n\\end{thebibliography}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n%\\appendix\n\n\n"
            }
        },
        "tables": {
            "Tab:comparison table": "\\begin{table*}[h!]\n    \\centering\n    \\begin{tabular}{|c|c|c c c|}\n    \\toprule\n    Graph      &     Filtering &  RMSE & MAE & MAPE    \\\\\n    \\midrule\n    \\multicolumn{5}{|c|}{FSST-GNN (GCN)}  \\\\\n    \\midrule\n    Cor &    Empirical &  \n    10.12 $\\pm$ 0.53 & 7.77 $\\pm$ 0.39 & 17.28$\\%\\pm$1.18$\\%$ \\\\\n\n    Cor &    Shrinkage &  \n     9.99 $\\pm$ 0.17 & 7.72 $\\pm$ 0.09 & 17.34$\\%\\pm$0.61$\\%$ \\\\\n\n    Cor &    GLASSO &  \n     \\textbf{9.76 $\\pm$ 0.47} & \\textbf{7.52 $\\pm$ 0.39} & \\textbf{16.96$\\%\\pm$1.49$\\%$} \\\\\n\n    Cor &    MFCF &  \n     9.80 $\\pm$ 0.61 & 7.66 $\\pm$ 0.43 & 17.27$\\%\\pm$1.13$\\%$  \\\\\n    \\midrule\n    \n    Inv Cor &  Empirical  &  \n    11.74 $\\pm$ 0.99 & 8.69 $\\pm$ 0.46 & 20.08$\\%\\pm$0.90$\\%$  \\\\\n\n    Inv Cor &    Shrinkage &  \n    10.59 $\\pm$ 1.04 & 8.26 $\\pm$ 0.78 & 19.15$\\%\\pm$2.21$\\%$  \\\\\n\n    Inv Cor &    GLASSO &  \n    \\underline{\\textbf{9.67 $\\pm$ 0.41}} & \\underline{\\textbf{7.55 $\\pm$ 0.34}} & \\underline{\\textbf{17.51$\\%\\pm$1.54$\\%$}} \\\\\n\n    Inv Cor &    MFCF &  \n    10.07 $\\pm$ 0.59 & 7.99 $\\pm$ 0.44 & 17.87$\\%\\pm$1.11$\\%$ \\\\\n    \\midrule\n    \n    Zeros &    / &  \n    13.51 $\\pm$ 0.16 & 10.28 $\\pm$ 0.12 & 22.04$\\%\\pm$1.01$\\%$  \\\\\n\n    Ones &    / &  \n    12.33 $\\pm$ 1.12 & 10.00 $\\pm$ 0.98 & 24.76$\\%\\pm$2.33$\\%$  \\\\\n\n    Identity  &    / &  \n    11.78 $\\pm$ 0.52 & 9.23 $\\pm$ 0.46 & 21.01$\\%\\pm$1.78$\\%$  \\\\\n    \\midrule\n    \\multicolumn{5}{|c|}{LSTM}  \\\\\n    \\midrule\n    / &    / &  16.34 $\\pm$ 0.44 & 12.56 $\\pm$ 0.25 & 26.40$\\%\\pm$0.98$\\%$ \\\\\n    \\bottomrule\n\n    \\end{tabular}\n    \\vspace{0.5pt}\n    \\caption{Summary of forecasting results with different models, graphs and filtering methods. Highlighted in bold are the optimal RMSE, MAE and MAPE in each graph, and underlined is the absolute optimal results in the table. A LSTM results is attached as the baseline. }\n    \\label{Tab:comparison table}\n\\end{table*}",
            "Tab:GCN_sparsity": "\\begin{table*}[h!]\n    \\centering\n    \\begin{tabular}{|c|c c c |c | ccc|}\n    \\toprule\n   Sparsity &  RMSE & MAE & MAPE  & Sparsity &  RMSE & MAE & MAPE\\\\\n   \\midrule\n   \\multicolumn{4}{|c|}{GLASSO} & \\multicolumn{4}{c|}{MFCF} \\\\\n    \\midrule\n     77.2\\% &  10.24 $\\pm$ 0.61 & 8.03$\\pm$ 0.47 & 18.89$\\%\\pm$1.35$\\%$ & \n     76.6\\% & 11.35 $\\pm$ 0.76 & 8.76 $\\pm$ 0.61 & 20.36$\\%\\pm$1.97$\\%$\\\\\n    \\midrule\n     71.0\\% &  10.34 $\\pm$ 0.79 & 8.05 $\\pm$ 0.58 & 18.66$\\%\\pm$1.29$\\%$ & \n     72.3\\% & 10.23 $\\pm$ 0.26 & 8.06 $\\pm$ 0.44 & 18.13$\\%\\pm$1.14$\\%$\\\\\n    \\midrule\n     66.6\\% &  9.80 $\\pm$ 0.41 & 7.66 $\\pm$ 0.33 & 17.75$\\%\\pm$0.87$\\%$ & \n     68.6\\% & 10.68 $\\pm$ 0.80 & 8.32 $\\pm$ 0.52 & 19.56$\\%\\pm$1.30$\\%$\\\\\n    \\midrule\n     60.0\\% &  \\underline{\\textbf{9.67 $\\pm$ 0.41}} & \\underline{\\textbf{7.55 $\\pm$ 0.34}} & \\underline{\\textbf{17.51$\\%\\pm$1.54$\\%$}}  & \n     61.3\\% & \\textbf{10.07 $\\pm$ 0.59} & \\textbf{7.99 $\\pm$ 0.44} & \\textbf{17.87$\\%\\pm$1.11$\\%$}\\\\\n    \\midrule\n     56.5\\% &  9.86 $\\pm$ 0.58 & 7.74 $\\pm$ 0.53 & 18.24$\\%\\pm$1.66$\\%$ & \n     58.0\\% & 10.19 $\\pm$ 0.41 & 8.12 $\\pm$ 0.29 & 18.29$\\%\\pm$0.49$\\%$\\\\\n    \\midrule\n     51.3\\% &  10.06 $\\pm$ 0.57 & 7.86 $\\pm$ 0.45 & 17.68$\\%\\pm$1.20$\\%$ & \n     54.8\\% & 10.31 $\\pm$ 0.52 & 8.09 $\\pm$ 0.39 & 18.22$\\%\\pm$1.26$\\%$\\\\\n    \\midrule\n     43.3\\% &  9.99 $\\pm$ 0.27 & 7.88 $\\pm$ 0.19 & 17.60$\\%\\pm$0.47$\\%$ & \n     43.7\\% & 10.75 $\\pm$ 0.68 & 8.16 $\\pm$ 0.40 & 18.44$\\%\\pm$0.55$\\%$\\\\\n    \n    \\bottomrule\n\n    \\end{tabular}\n    \\vspace{0.5pt}\n    \\caption{Summary of forecasting results of FSST-GNN (GCN) with different filtering methods and sparsity on inverse correlation graph. Highlighted in bold are the optimal RMSE, MAE and MAPE in each model-sparsity combination, and underlined is the absolute optimal results in the table.}\n    \\label{Tab:GCN_sparsity}\n\\end{table*}",
            "Tab:GAT_sparsity": "\\begin{table*}[h!]\n    \\centering\n    \\begin{tabular}{|c|c c c |c | ccc|}\n    \\toprule\n   Sparsity &  RMSE & MAE & MAPE  & Sparsity &  RMSE & MAE & MAPE\\\\\n   \\midrule\n   \\multicolumn{4}{|c|}{GLASSO} & \\multicolumn{4}{c|}{MFCF} \\\\\n    \\midrule\n     77.2\\% &  9.88 $\\pm$ 0.59 & 7.58 $\\pm$ 0.43 & 16.17$\\%\\pm$0.53$\\%$ & \n     76.6\\% & 10.47 $\\pm$ 0.75 & 8.09 $\\pm$ 0.68 & 17.61$\\%\\pm$2.64$\\%$\\\\\n    \\midrule\n     71.0\\% &  10.03 $\\pm$ 0.65 & 7.73 $\\pm$ 0.52 & 16.56$\\%\\pm$1.07$\\%$ & \n     72.3\\% & 10.27 $\\pm$ 0.62 & 7.86 $\\pm$ 0.46 & 17.24$\\%\\pm$0.99$\\%$\\\\\n    \\midrule\n     66.6\\% &  9.63 $\\pm$ 0.36 & 7.42 $\\pm$ 0.25 & 15.82$\\%\\pm$0.25$\\%$ & \n     68.6\\% & 9.90 $\\pm$ 1.17 & 7.60 $\\pm$ 0.95 & 16.01$\\%\\pm$1.78$\\%$\\\\\n    \\midrule\n     60.0\\% & \\textbf{ 9.58 $\\pm$ 0.31} & \\textbf{7.37 $\\pm$ 0.20} & \\textbf{15.62$\\%\\pm$0.26$\\%$}  & \n     61.3\\% & \\underline{\\textbf{9.46 $\\pm$ 0.68}} & \\underline{\\textbf{7.31 $\\pm$ 0.45}} & \\underline{\\textbf{15.44$\\%\\pm$0.19$\\%$}}\\\\\n    \\midrule\n     56.5\\% &  9.64 $\\pm$ 0.34 & 7.41 $\\pm$ 0.23 & 15.80$\\%\\pm$0.47$\\%$ & \n     58.0\\% & 9.65 $\\pm$ 0.46 & 7.53 $\\pm$ 0.32 & 15.63$\\%\\pm$0.54$\\%$\\\\\n    \\midrule\n     51.3\\% &  9.75 $\\pm$ 0.33 & 7.55 $\\pm$ 0.31 & 17.28$\\%\\pm$1.18$\\%$ & \n     54.8\\% & 9.81 $\\pm$ 0.53 & 7.53 $\\pm$ 0.37 & 16.03$\\%\\pm$0.54$\\%$\\\\\n    \\midrule\n     43.3\\% &  10.12 $\\pm$ 0.53 & 7.77 $\\pm$ 0.39 & 17.28$\\%\\pm$1.18$\\%$ & \n     43.7\\% & 9.90 $\\pm$ 0.38 & 7.63 $\\pm$ 0.30 & 16.31$\\%\\pm$0.92$\\%$\\\\\n    \n    \\bottomrule\n\n    \\end{tabular}\n    \\vspace{0.5pt}\n    \\caption{Summary of forecasting results of FSST-GNN (GAT) with different filtering methods and sparsity on inverse correlation graph. Highlighted in bold are the optimal RMSE, MAE and MAPE in each model-sparsity combination, and underlined is the absolute optimal results in the table.}\n    \\label{Tab:GAT_sparsity}\n\\end{table*}"
        },
        "figures": {
            "fig:Architecture": "\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{Architecture.png}\n    \\vspace{0.5pt}\n    \\caption[Architecture]{The overall model architecture.}\n         \\label{fig:Architecture}\n\\end{figure*}"
        },
        "equations": {
            "eq:eq:covariance srhinkage": "\\begin{equation}\\label{eq:covariance srhinkage}\n\\begin{aligned}\n    \\Sigma_{\\text{shrunk}}=(1-\\alpha)\\hat{\\Sigma}+\\alpha\\frac{\\text{Tr}\\hat\\Sigma}{n}\\mathbb{1}\n\\end{aligned}\n\\end{equation}",
            "eq:eq:glasso": "\\begin{equation}\\label{eq:glasso}\n\\begin{aligned}\n    \\Sigma_{\\text{glasso}}^{-1}=\\min_{{\\mathbf \\Sigma^{-1}}}( -\\log\\det{\\Sigma^{-1}} + \\text{Tr}(\\hat\\Sigma^{-1}\\Sigma^{-1})+\\lambda||\\Sigma^{-1}||_{1})\n\\end{aligned}\n\\end{equation}",
            "eq:GCN_adjacency": "\\begin{equation}\\label{GCN_adjacency}\n\\begin{aligned}\n    h_i=\\sum^{N}_{j}{(\\frac{\\phi(W^\\top A_{ij})}{d_j})\\times h_j}\n\\end{aligned}\n\\end{equation}",
            "eq:GCN_corr": "\\begin{equation}\\label{GCN_corr}\n\\begin{aligned}\n    h_i=\\sum^{N}_{j}{\\phi(W^\\top C_{ij})\\times h_j}\n\\end{aligned}\n\\end{equation}",
            "eq:eq:GAT attention": "\\begin{equation}\\label{eq:GAT attention}\n\\begin{aligned}\n    e_{i,j}= & a(Wh_i,Wh_j)\\\\\n    = & \\textrm{LeakyReLU} (\\hat{a}^\\top[Wh_i||Wh_j]) \\\\\n\\end{aligned}\n\\end{equation}",
            "eq:eq:GAT normalized attention": "\\begin{equation}\\label{eq:GAT normalized attention}\n    \\begin{aligned}\n        \\alpha_{i,j}=&\\textrm{softmax}_j (e_{i,j})\\\\\n        =&\\frac{\\exp{e_{i,j}}}{\\sum_{k\\in\\mathcal{N}_i}\\exp{e_{i,k}}} \\\\\n        =& \\frac{\\exp{(\\textrm{LeakyReLU}(\\hat{a}^\\top[Wh_i||Wh_j]))}}{\\sum_{k\\in\\mathcal{N}_i} \\exp{(\\textrm{LeakyReLU}(\\hat{a}^\\top[Wh_i||Wh_k]))}}\n    \\end{aligned}\n\\end{equation}",
            "eq:eq:GAT multi-head aggregation": "\\begin{equation}\\label{eq:GAT multi-head aggregation}\n     h_i = \\phi(\\frac{1}{k}\\sum_{k=1}^K(\\sum_{j\\in\\mathcal{N}_i} \\alpha_{i,j}W^kh_j))\n\\end{equation}"
        }
    }
}