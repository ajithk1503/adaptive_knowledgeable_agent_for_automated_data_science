{
    "meta_info": {
        "title": "CrossCBR: Cross-view Contrastive Learning for Bundle Recommendation",
        "abstract": "Bundle recommendation aims to recommend a bundle of related items to users,\nwhich can satisfy the users' various needs with one-stop convenience. Recent\nmethods usually take advantage of both user-bundle and user-item interactions\ninformation to obtain informative representations for users and bundles,\ncorresponding to bundle view and item view, respectively. However, they either\nuse a unified view without differentiation or loosely combine the predictions\nof two separate views, while the crucial cooperative association between the\ntwo views' representations is overlooked. In this work, we propose to model the\ncooperative association between the two different views through cross-view\ncontrastive learning. By encouraging the alignment of the two separately\nlearned views, each view can distill complementary information from the other\nview, achieving mutual enhancement. Moreover, by enlarging the dispersion of\ndifferent users/bundles, the self-discrimination of representations is\nenhanced. Extensive experiments on three public datasets demonstrate that our\nmethod outperforms SOTA baselines by a large margin. Meanwhile, our method\nrequires minimal parameters of three set of embeddings (user, bundle, and item)\nand the computational costs are largely reduced due to more concise graph\nstructure and graph learning module. In addition, various ablation and model\nstudies demystify the working mechanism and justify our hypothesis. Codes and\ndatasets are available at https://github.com/mysbupt/CrossCBR.",
        "author": "Yunshan Ma, Yingzhi He, An Zhang, Xiang Wang, Tat-Seng Chua",
        "link": "http://arxiv.org/abs/2206.00242v5",
        "category": [
            "cs.IR"
        ],
        "additionl_info": "9 pages, 5 figures, 5 tables; this the 3rd version that fixes a typo  in table 1"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": " \\label{sec:introduction}\n\nBundle recommendation aims to recommend a \\za{set} of \\wx{items related with the same theme to users}. \nIn \\za{a variety of} online applications, such as music platforms and fashion shopping \\za{sites}, serving bundles instead of individual items can \\za{boost the users' experience in a one-stop manner.}\n%can satisfy users' various needs with a one-stop manner. \nMore importantly, \\za{platforms taking bundles as the marketing strategy can increase sales revenue and attract customers fond of bundle discounts.}\n%serving bundles can also increase the sale and revenue of platforms by exhibiting more items to users, where promotion and discount are usually accompanied. \nConsequently, both the users and platforms \\wx{would prefer bundles} (\\eg music \\za{playlist} and fashion outfit) instead of single items (\\eg single \\za{song} and \\za{piece-of-clothing}).\nTherefore, developing effective bundle recommender systems \\wx{is attracting a surge of interest in both academia and industry.}\n\n\n%becomes in demand and attracts more attention in both academic and industrial communities.\n\n%To develop effective bundle recommender systems, several strands of models have been developed recently. At the early stage, bundle is just treated as a special type of item and CF(collaborative filtering)-based methods such as Matrix Factorization are the main solutions. For example, Rendle \\etal propose FPMC~\\cite{rendle2010factorizing}, which is usually emphasized by its contribution to sequential recommendation, but also one of the initial work for basket recommendation. However, the items contained by the bundle provide valuable information but are overlooked by such CF-based methods. To address this limitation, several models~\\cite{cao2017embedding,chen2019matching} are developed to take advantage of the additional information of bundle-item composition and user-item interactions, which further advance the SOTA. For example, Cao \\etal propose an Embedding Factorization Model to incorporate the item-item co-occurrence information with bundles, and Chen \\etal introduce the attention mechanism to aggregating the items within each bundle. Following on these works, GNN-based models quickly dominate the cutting edge solutions for bundle recommendation due to its powerful capacity in modeling such heterogeneous relational data. \n%Li \\etal build a hierarchical propagation model for outfit recommendation~\\cite{li2020hierarchical}. \n%Deng \\etal build a tripartite graph over (user, bundle, item) and employ GCN to learn the representations of user and bundle~\\cite{deng2020personalized}. Chang \\etal decompose the user-bundle-item heterogeneous relationships into two levels of graphs (\\ie item-level and bundle-level) to capture item- and bundle-level preferences~\\cite{chang2021bundle} separately, resulting in SOTA performance.\n\n\\wx{Scrutinizing prior studies on bundle recommendation \\cite{rendle2010factorizing,cao2017embedding,chen2019matching,deng2020personalized,chang2020bundle}, we can systematize the sources of user preferences as two views:\n(1) bundle view, which depicts user preferences through the \\underline{u}ser-\\underline{b}undle interactions and can be reorganized as an U-B graph;\nand (2) item view, which delineates user behaviors and bundle knowledge at the granularity of items --- \\ie \\underline{u}ser-\\underline{i}tem interactions in the form of an U-I graph and \\underline{b}undle-\\underline{i}tem affiliation in the form of a B-I graph, respectively.\nThese two views allow us to understand user interests and construct the recommender models from different perspectives. However, there exist clear discrepancies between these two views which have not been modeled in prior studies.\nConsider the running example on the top part of Figure \\ref{fig:motivation}, where $u_1$ is the target user who has interacted with bundle $b_{1}$ and individual items $i_1$, $i_2$, and $i_5$ before, and the historical bundle $b_{1}$ consists of items $i_1$ and $i_2$.\nBy taking the bundle view solely, a recommender model is highly likely to route $b_4$ to $u_1$, if the behavioral similarity between users $u_1$ and $u_2$ is well captured.\nOn the other hand, by taking the item view, a recommender model is prone to yield $b_2$ and $b_3$ as the interested item of $u_1$, since $b_2$ holds items (\\ie $i_2$) shared with the historical bundle $b_1$ and $b_3$ contains items (\\ie $i_5$) individually preferred by $u_1$.\nClearly, the bundle view emphasizes the behavioral similarity among users, while the item view highlights the content relatedness among bundles and users' item-level preference.\nHence they are complementary but different, and the cooperation of these two views is the key to accurate bundle recommendation.\n}\n\n% As evidenced by the evolution of bundle recommendation models~\\cite{rendle2010factorizing,cao2017embedding,chen2019matching,deng2020personalized,chang2020bundle}, users' preferences over bundles can be \\za{naturally} divided into two views (\\ie bundle view and item view), which cooperatively contribute to the bundle recommendation. \n% \\za{Models learned from bundle view} capture users' preference through the user-bundle interaction (formalized as U-B graph), while the \\za{item-view models predict user's behavior based on} both the user-item interaction (formalized as U-I graph) and bundle-item affiliation (formalized as B-I graph). \n% % The bundle view captures users' preference through the user-bundle interaction (formalized as U-B graph), while the item view captures users' preference through both the user-item interaction (formalized as U-I graph) and bundle-item affiliation (formalized as B-I graph). \n% \\za{To provide more context, as a running example shown on top of Figure ~\\ref{fig:motivation}, we consider the bundle recommendation for target user $u_1$ who has historical interaction with $b_1$.}\n% \\za{Models learned from different views focus on various information, and this discrepancy further results in different bundle recommendations.}\n% % Let's take a running example as shown on top of Figure~\\ref{fig:motivation}, the target user $u_1$ has interacted with $b_1$, and all $\\{b_2, b_3, b_4, b_5\\}$ can be recommended to $u_1$ according to the cooperative information from both views. \n% \\za{Specifically,} only $b_5$ can be recommended to $u_1$ from the bundle view due to the user-bundle collaborative filtering (CF) signals. \n% When taking the item view into account, $b_4$ \\za{is prone be} recommended to $u_1$ since it contains $u_1$ preferred item $i_6$;\n% %When taking the item view into account, $b_4$ can be recommended to $u_1$ since it contains $i_6$, and $i_6$ has been individually interacted with $u_1$;\n% $b_2$ \\za{is susceptible to} be recommended because \\za{of overlapped item $i_1$ with bundle $b_1$};\n% %it shares $i_2$ with $b_1$, and $b_1$ has been interacted with $u_1$; \n% and \\za{there is a higher possibility for $b_3$}\n% %$b_3$ has even more chance to be recommended to\n% to be recommended to $u_1$ since $b_3$ and $b_1$ shares $i_3$, and $i_3$ is individually preferred by $u_1$. \n% % In summary, by considering the item-view information, we can additionally recommend $\\{b_2,b_3,b_4\\}$ to $u_1$ and largely improve the performance. \n% Therefore, we \\za{conjecture that both bundle and item view, providing complementary information for each other,} can cooperate to \\za{achieve} enhanced bundle recommendation.\n% % Therefore, we can hypothesize that both bundle and item view can cooperate to provide complementary information for enhanced bundle recommendation.\n\n%In summary, if a bundle has shared items with a given bundle that is preferred by the user (\\eg $b_2$ shares $i_2$ with $b_1$), or a bundle contain items that are preferred by the user through individual interactions (\\eg $b_4$ contains $i_6$), they are probably preferred by the user. Moreover, if the overlapped items that a candidate bundle shares with a preferred bundle have been interacted with the user individually (\\eg $b_3$ shares with $b_1$ the item $i_3$, which is individually preferred by $u_1$), the probability of the candidate bundle be liked by the user will be further promoted. From these intuitions, consequently, we can hypothesize that both bundle- and item-view can cooperate mutually to provide complementary information for enhanced bundle recommendation.\n\nHowever, the cooperative association \\wx{between these two views} has been loosely modeled or even overlooked in \\za{existing} works \\cite{cao2017embedding,chen2019matching, deng2020personalized,chang2020bundle}.\n%current works.\n\\wx{One research line like BundleNet \\cite{deng2020personalized} blindly merges the two views into a unified tripartite graph and employs graph neural networks (GNNs) to aggregate the neighboring information into representations of users and bundles.\nHowever, such representations fail to differentiate the behavioral similarity among users and content relatedness among bundles from these two views, thus obscuring their cooperation.\nAnother line, such as BGCN \\cite{chang2020bundle}, first performs representation learning and preference prediction upon the views individually, and then fuses these two view-specific predictions.\nWhile this loose modeling of the two views performs better than the first line, it only considers the cooperative signal at the level of predictions, rather than directly plugging such signal into the representations optimized for recommendation.\nHence, no mutual enhancement of the two views is guaranteed to be captured.\nConsidering the limitations of the two lines, we believe that it is crucial to properly model the cooperative association and encourage the mutual enhancement across the bundle and item views.\n}\n\n% As shown in the bottom of Figure~\\ref{fig:motivation}, BundleNet~\\cite{deng2020personalized} \\za{formalizes} a unified view for both user and bundle by propagating information over the tripartite graph, without differentiating the two different views and also incapable of mining the cooperative information of both views. BGCN~\\cite{chang2020bundle} first separates the preference into two views, however, the two views are separate during representation learning and just loosely connected in the prediction function by summing the scores of two views. Even though the two separate views significantly improves the modeling capacity, we argue that the cooperative association between the two views should be deliberately modeled. It should be neither be a mixed unified view nor two loosely connected views, while proper modeling of the cross-view cooperative association is required in order to maximize the mutual enhancement. \n\n\\wx{Towards this end,} we propose a \\textbf{Cross}-view \\textbf{C}ontrastive Learning for \\textbf{B}undle \\textbf{R}ecommendation (CrossCBR) \\wx{which captures the cooperative association by cross-view contrastive learning and mutually enhances the view-aware representations.\nThe basic idea is to treat the bundle and item views as two distinct but correlated viewpoints of user-bundle preferences, and apply contrastive learning on these viewpoints to encapsulate their agreements into representations of users and bundles.\nSpecifically, upon the U-B graph, we build a LightGCN \\cite{he2020lightgcn} as the backbone to obtain the bundle-view representations of users and bundles;\nanalogously, upon the U-I graph, we employ another LightGCN to generate the item-view representations of users and items, and aggregate the representations of compositional items as the bundle representation based on the B-I graph.\nWe jointly employ the BPR \\cite{rendle2012bpr} and contrastive loss \\cite{gutmann2010noise} to optimize these representations.\nBenefiting from the cross-view contrastive learning, CrossCBR outperforms the state-of-the-art (SOTA) baselines by a large margin on three datasets.}\n\n% First, we adopt LightGCN as the backbone model to obtain the bundle-view representations of users and bundles based on the U-B graph. Similarly, we get the item-view representations of users and items based on the U-I graph, thereafter, the item-view bundle representations are obtained by pooling over the item representations based on the B-I graph. Second, we adopt the contrastive loss to user and bundle representations across the two different views.\n% Finally, the contrastive loss is jointly optimized with the BPR loss for model training. Surprisingly, just with this simple framework, CrossCBR outperforms SOTA baselines by a large margin on three benchmark datasets. \n\nTo demystify the working mechanism behind \\wx{CrossCBR}, we further investigate the alignment-dispersion characteristics of the learned representations.\n\\wx{Encouraging the cross-view alignment enables the view-aware representations to learn from each other and achieve mutual enhancement;\nmeanwhile, enlarging the cross-view dispersion between different users/bundles is excel at enhancing the representations' discriminative power.\n%Such a powerful representation learning comes with minimal space complexity (\\ie the model parameters compose only three sets of ID embeddings for users, bundles, and items) and low time complexity (\\ie the self-connections and bundle-bundle connections extensively used in the SOTA baseline are discarded).}\nSuch a powerful representation learning comes with minimal space complexity and low time complexity.} Our main contributions are as follows:\n\n% We find that encouraging the cross-view affinity of the representations enable both views to learn with each other, consequently, both views' representations can be mutually enhanced.\n% Meanwhile, enlarging the cross-view dispersion of different users/bundles can indirectly increase the self-discrimination of representations, which is crucial for the performance improvement.\n% Moreover, CrossCBR is a very simple model with minimal space complexity (only three sets of id embeddings for user, bundle, and item). We also largely reduce the time complexity by removing the self-connections and bundle-bundle similarity connections during the graph construction.\n\n% The main contributions of this work are summarized as follows:\n\\begin{itemize}[leftmargin=*]\n    \\item To the best of our knowledge, we are \\za{among} the first to \\za{formulate} the cross-view cooperative association in bundle recommendation, \\za{providing a new research line worthy of further exploration.}\n    \\item We propose a simple yet effective bundle recommender, CrossCBR, to model the cooperative association between two views via cross-view contrastive learning.\n    \\item Our model outperforms SOTA baselines by a large margin on three public datasets, while requires largely reduced training time. \n    %Various studies demystify the working mechanism of CrossCBR, \n    We also demonstrate how the idea of CrossCBR can be generalized to a broader scope of tasks.\n\\end{itemize}\n"
            },
            "section 2": {
                "name": "Methodology",
                "content": " \\label{sec:methodology}\nIn this section, we first formulate the task of bundle recommendation and the present our CrossCBR, as shown in Figure~\\ref{fig:framework}.\nThe in-depth discussion of the working mechanism and analysis of computational complexity for CrossCBR are followed.\n% The overall framework of CrossCBR is shown in Figure~\\ref{fig:framework}. It consists of two key components: representation learning for two views and joint optimization of BPR and contrastive loss. In this section, we first briefly formulate the task of bundle recommendation, followed by the introduction of the two key components, and finally we discuss the model's working mechanism and analyze the computational complexity.\n\n\n\n",
                "subsection 2.1": {
                    "name": "Problem Formulation",
                    "content": " \\label{subsec:problem_formulation}\nGiven a set of users $\\mathcal{U}=\\{u_1, u_2, \\cdots, u_M\\}$, a set of bundles $\\mathcal{B}=\\{b_1, b_2, \\cdots, b_N\\}$, and a set of items $\\mathcal{I}=\\{i_1, i_2, \\cdots, i_O\\}$, where $M$, $N$, and $O$ are the number of users, bundles, and items, respectively. The user-bundle interactions, user-item interactions, and bundle-item affiliations are denoted as $\\mathbf{X}_{M \\times N}=\\{x_{ub}|u\\in{\\mathcal{U}},b\\in{\\mathcal{B}}\\}$, $\\mathbf{Y}_{M \\times O}=\\{y_{ui}|u\\in{\\mathcal{U}},i\\in{\\mathcal{I}}\\}$, and $\\mathbf{Z}_{N \\times O}=\\{z_{bi}|b\\in{\\mathcal{B}},i\\in{\\mathcal{I}}\\}$, respectively. $x_{ub}, y_{ui}, z_{bi} \\in \\{0, 1\\}$, where $1$ \\za{represents} an interaction between the user-bundle or user-item pair, or the item belongs to a certain bundle. \nNote that since we deduplicate the historical bundle and item interactions for each user, \n%To be noted, the deduplication is a common preprocessing step in general recommendation, which will encourage the recommender system to recommend new items to the users. \neach element of $X$ and $Y$ is a binary value \\za{rather than} an integer. In addition, $X$ and $Y$ are separately generated, where users are allowed to directly interact with both bundles and individual items. Therefore, $X$ and $Y$ contain different information, which heuristically enables the cooperative effect between the two different views. \nThe goal of bundle recommendation task is to learn a model from the historical $\\{X, Y, O\\}$ and predict the unseen user-bundle interactions in $X$.\n\n"
                },
                "subsection 2.2": {
                    "name": "Learning of Two Views' Representations",
                    "content": " \\label{subsec:representation_learning}\nFor the first component of CrossCBR, we aim to learn the representations \\za{from} the two views: bundle and item view. \nDespite the \\za{effectiveness of} two views' representation learning module of BGCN~\\cite{chang2020bundle}, its \\za{partial} designs of graph construction and graph learning are useless or even harmful~\\cite{he2020lightgcn}, especially under the circumstance of utilizing the contrastive learning. \n% Despite the two views' representation learning module of BGCN~\\cite{chang2020bundle} has been demonstrated effective, its several designs of both graph construction and graph learning are useless or even harmful, especially under the circumstance of utilizing the contrastive learning. \n\\za{Here} we \\za{devise} our simpler yet more effective representation learning approach.\n\n",
                    "subsubsection 2.2.1": {
                        "name": "Bundle-view Representation Learning",
                        "content": "\nIn order to learn the user and bundle representations \\za{from} the bundle view, we first construct a user-bundle bipartite graph, \\ie U-B graph, based on the user-bundle interaction matrix $\\mathbf{X}$. We \\za{then} employ the prevailing GNN-based recommendation framework LightGCN~\\cite{he2020lightgcn} to learn the representations of both user and bundle. Specifically, we conduct information propagation over the U-B graph, and the $k$-th layer's information propagation is denoted as:\n\\begin{equation} \\label{eq_2}\n\\left\\{\n\\begin{aligned}\n    \\mathbf{e}_{u}^{B(k)}=\\sum_{b \\in \\mathcal{N}_u}{\\frac{1}{\\sqrt{|\\mathcal{N}_u|}\\sqrt{|\\mathcal{N}_b|}}\\mathbf{e}^{B(k-1)}_{b}}, \\\\\n    \\mathbf{e}_{b}^{B(k)}=\\sum_{u \\in \\mathcal{N}_b}{\\frac{1}{\\sqrt{|\\mathcal{N}_b|}\\sqrt{|\\mathcal{N}_u|}}\\mathbf{e}^{B(k-1)}_{u}},\n\\end{aligned}\n\\right.\n\\end{equation}\nwhere $\\mathbf{e}_{u}^{B(k)}, \\mathbf{e}_{b}^{B(k)} \\in \\mathbb{R}^{d}$ are the $k$-th layer's information propagated to user $u$ and bundle $b$; $d$ is the embedding dimensionality; the superscript $B$ indicates the bundle view; $\\mathbf{e}_{u}^{B(0)}$ and $\\mathbf{e}_{b}^{B(0)}$ are randomly initialized at the beginning of the training; $\\mathcal{N}_u$ and $\\mathcal{N}_b$ are the neighbors of the user $u$ and bundle $b$ in the U-B graph. \n\nWe follow LightGCN to remove the self-connections from the U-B graph and the nonlinear transformation from the information propagation function. We will empirically demonstrate that such simplifications, which BGCN does not take into account, are truly helpful for better performance \\za{(\\cf Section \\ref{subsec:ablation_study}) }. \nMore importantly, we do not incorporate the bundle-bundle connections, which are introduced by BGCN and calculated from the degree of overlapped items between the two bundles. \nThe reason lies in \\za{the fact} that bundle-bundle overlap information can be distilled from the item view through the cross-view contrastive learning (\\za{\\cf Section \\ref{subsec:contrastive_loss}}). Meanwhile, the removal of extra bundle-bundle connections can further reduce the computational costs during the graph learning. \n\nWe \\za{concatenate} all $K$ layers' \\za{embedding to combine the information received from neighbors of different depths}. \nThe final bundle-view representations $\\mathbf{e}^{\\za{B}*}_{u}$ and \\za{$\\mathbf{e}^{B*}_b$} are denoted as:\n\\begin{equation} \\label{eq_3}\n    \\mathbf{e}^{B*}_{u} = \\sum^K_{k=0}{\\mathbf{e}^{B(k)}_{u}}, \\ \\ \\ \\ \\mathbf{e}^{B*}_b = \\sum^K_{k=0}{\\mathbf{e}^{B(k)}_b}.\n\\end{equation}\n\n"
                    },
                    "subsubsection 2.2.2": {
                        "name": "Item-view Representation Learning",
                        "content": "\nIn order to learn the user and bundle representations \\za{from} the item view, we first build two bipartite graphs, \\ie U-I and B-I graph, according to the user-item interactions $\\mathbf{Y}$ and bundle-item affiliations \\za{$\\mathbf{Z}$}, respectively. \nSimilar \\za{to} the U-B graph learning, we learn user and item representations using LightGCN. \nThe obtained user representations are the item-view user representations, and the item-view bundle representations are obtained by performing average pooling over the item-view item representations guided by the B-I graph. Specifically, the information propagation over the U-I graph is \\za{defined} as:\n\\begin{equation} \\label{eq_4}\n\\left\\{\n\\begin{aligned}\n    \\mathbf{e}_{u}^{I(k)}=\\sum_{i \\in \\mathcal{N}_u}{\\frac{1}{\\sqrt{|\\mathcal{N}_u|}\\sqrt{|\\mathcal{N}_i|}}\\mathbf{e}^{I(k-1)}_i}, \\\\\n    \\mathbf{e}_{i}^{I(k)}=\\sum_{u \\in \\mathcal{N}_i}{\\frac{1}{\\sqrt{|\\mathcal{N}_i|}\\sqrt{|\\mathcal{N}_u|}}\\mathbf{e}^{I(k-1)}_{u}},\n\\end{aligned}\n\\right.\n\\end{equation}\nwhere $\\mathbf{e}_{u}^{I(k)}, \\mathbf{e}_{i}^{I(k)} \\in \\mathbb{R}^{d}$ are the $k$-th layer's information propagated to user $u$ and item $i$, respectively; the superscript $I$ \\za{refers to} the item view; $\\mathbf{e}_{i}^{I(0)}$ \\za{is} randomly initialized; $\\mathcal{N}_u$ and $\\mathcal{N}_i$ are the neighbors of the user $u$ and item $i$ in the U-I graph. \nWe follow BGCN and share the parameters of $\\mathbf{e}_{u}^{I(0)}$ with $\\mathbf{e}_{u}^{B(0)}$, which empirically does not affect the performance but largely reduces the \\za{number of} parameters. \nMeanwhile, such initial layer's parameters sharing between two views is too weak even impossible to capture the cross-view cooperative association (\\cf CrossCBR-CL in Section \\ref{subsec:ablation_study}). \nSimilar \\za{to} U-B graph, we also remove the self-connections from the U-I graph and nonlinear feature transformation from the information propagation function. \nAnd a layer aggregation operation is \\za{adopted} after $K$ layers of information propagation, formulated as follows:\n\\begin{equation} \\label{eq_5}\n    \\mathbf{e}^{I*}_{u} = \\sum^K_{k=0}{\\mathbf{e}^{I(k)}_{u}}, \\ \\ \\ \\ \\mathbf{e}^{I*}_{i} = \\sum^K_{k=0}{\\mathbf{e}^{I(k)}_{i}},\n\\end{equation}\nwhere $\\mathbf{e}^{I*}_{u}$ and $\\mathbf{e}^{I*}_{i}$ are the item-view user and item representations, respectively. Based on the item-view item representation and the B-I graph, we can obtain the item-view bundle representations $\\mathbf{e}^{I*}_{b}$ through average pooling, denoted as:\n\\begin{equation} \\label{eq_6}\n    \\mathbf{e}^{I*}_{b} = \\frac{1}{|\\mathcal{N}_b|}\\sum_{i \\in \\mathcal{N}_b}\\mathbf{e}^{I*}_i,\n\\end{equation}\nwhere $\\mathcal{N}_b$ represents the \\za{set of items} a certain bundle $b$ contains. \n% where $\\mathcal{N}_b$ represents the contained items of a certain bundle $b$ and $\\mathbf{e}^{I*}_i$ is the item representation in the item view. \n\nIn summary, we can \\za{learn} the representations of all users and bundles \\za{from} two views, denoted as $\\mathbf{E}_U^{B*}, \\mathbf{E}_U^{I*} \\in \\mathbb{R}^{M \\times d}$ and $\\mathbf{E}_B^{B*}, \\mathbf{E}_B^{I*} \\in \\mathbb{R}^{N \\times d}$, where the superscripts $B$ and $I$ \\za{stand for} the bundle and item view, respectively; and the subscripts $U$ and $B$ \\za{indicate} the whole user and bundle set, respectively ($\\mathbf{E}_I^{I*} \\in \\mathbb{R}^{O \\times d}$ are the representations of all items in the item view). Thereafter, given a user $u$ and a bundle $b$, we can obtain their bundle-view representations, \\ie $\\mathbf{e}^{B*}_u$ and $\\mathbf{e}^{B*}_b$, and their item-view representations, \\ie $\\mathbf{e}^{I*}_u$ and $\\mathbf{e}^{I*}_b$. \n\n"
                    }
                },
                "subsection 2.3": {
                    "name": "Cross-view Contrastive Learning",
                    "content": " \\label{subsec:contrastive_learning}\nWe \\za{devise the critical} component to model the cross-view cooperative association \\za{via contrastive learning}. We first \\za{present} the data augmentation methods, followed by the contrastive loss. \n\n",
                    "subsubsection 2.3.1": {
                        "name": "Data Augmentation",
                        "content": "\nThe \\za{main} idea of \\za{self-supervised} contrastive learning is to \\za{encourage} the representation affinity among various views of the same object, while \\za{at the same time} enlarge the representation dispersion of different objects \\cite{wang2020understanding}. \nIn practice, if multiple views naturally exist for each object, \\eg images taken from different angle\\za{s}, or the bundle and item view in bundle recommendation, the contrastive loss can be directly applied. \nOn the other hand, in many scenarios, multiple views are not available, and data augmentation is leveraged to generate multiple views from the original data~\\cite{chen2020simple,gao2021simcse,wu2021self}. \nProper data augmentation not only release the (multi-view) data constraint for applying contrastive learning, but also may \\za{improve} the robustness to counter potential noise. Therefore, while keeping the original preservation (no augmentation) as the default setting, we also introduce two simple data augmentation methods: graph- and embedding-based augmentations.\n\n\\textbf{Graph-based Augmentation}. The main purpose of graph-based augmentation is to generate augmented data by revising the graph structure~\\cite{wu2021self}. We adopt a simple random augmentation method of edge dropout (ED), which randomly removes a certain proportion (dropout ratio $\\rho$) of edges from the original graph. The rationale behind edge dropout lies in that the core local structure of the graph is preserved. Therefore, the robustness of learned representations may be enhanced to counter certain noise.\n\n%for edge replacement, we first randomly pick a certain proportion (replacement ratio $\\rho$) of edges from the original graph, and keep one end of each edge and randomly pick another node to generate a new edge that is not existing before.  For the model-based graph augmentation method, we input the original graphs (user-item, user-bundle, and bundle-item) the Variational Graph Auto Encoder model and reconstruct them. Through such an encoder-decoder framework, the noise and minor patterns within the original graphs will be removed or weaken, while the key patterns will be preserved. Thanks to the merits of variational inference of VGAE, the reconstructed graphs are born with certain randomness, which will improve the robustness of the model as well.\n\n\\textbf{Embedding-based Augmentation}. Different from the graph-based augmentation, which can be applied only to graph data, embedding-based augmentations are more general and suitable for any deep representation learning based methods~\\cite{gao2021simcse}. The \\za{major} idea is to \\za{vary} the learned representation embeddings regardless of how the embeddings are obtained. We employ message dropout (MD), which randomly masks some elements of the propagated embeddings with a certain dropout ratio $\\rho$ during the graph learning. \n\n%For additive noise, we add a random noise to the learned embeddings. For mixup, which has been demonstrated as a strong data augmentation methods in areas such as CV~\\cite{} and Graph Learning~\\cite{}. The basic idea is to linearly combine a pair of representations with a proper weight, resulting in new representations which are hard to discriminate by the model or with new labels. In our case, the mixup is employed to generate hard negatives by injecting the information of target node to the potential negative nodes.\n\n\\textbf{Original Preservation}. We name the approach without any data augmentation as original preservation (OP), where no randomness is introduced and only the original representations are preserved. Since the two views in bundle recommendation are obtained from different sources of data, their representations are distinctive sufficiently to work well.\n\nTo avoid the abuse of notations, after the data augmentation, we still use the same notations of $\\mathbf{e}^{B*}_u, \\mathbf{e}^{B*}_b, \\mathbf{e}^{I*}_u, \\mathbf{e}^{I*}_b$ to denote the embeddings for the bundle-view user, bundle-view bundle, item-view user, and item-view bundle, respectively.\n\n"
                    },
                    "subsubsection 2.3.2": {
                        "name": "Cross-view Contrastive Loss",
                        "content": " \\label{subsec:contrastive_loss}\nWe \\za{leverage} the cross-view contrastive loss \\za{to optimize} two-view representations. As the motivations \\za{illustrate} in Figure~\\ref{fig:motivation}, each view captures a distinctive aspect of user's preference, and the two views have to work cooperatively to maximize the overall modeling capacity. \nTo model the cross-view cooperative association, we employ the cross-view contrastive loss (we leave other potential modeling solutions for future work). We adopt the popular InfoNCE~\\cite{gutmann2010noise} loss built upon the cross-view representations of users and bundles, respectively. \nMore precisely, the contrastive loss \\za{is able to simultaneously encourage the alignment of the same user/bundle from different views and enforce the separation of different users/bundles.}\n%can increase the cross-view affinity of the same user/bundle and enlarge the representation dispersion of different users/bundles. \nThe equations are as follows:\n\\begin{equation} \\label{eq:user_contrastive_loss}\n    \\mathcal{L}^C_{U} = \\frac{1}{|\\mathcal{U}|} \\sum_{u \\in \\mathcal{U}}{-\\text{log}\\frac\n    {\\text{exp}({s(\\mathbf{e}^{B*}_u, \\mathbf{e}^{I*}_u)/\\tau})}\n    {\\sum_{v \\in \\mathcal{U}}{\\text{exp}({s(\\mathbf{e}^{B*}_u, \\mathbf{e}^{I*}_v)/\\tau})}}},\n\\end{equation}\n\\begin{equation} \\label{eq:bundle_contrastive_loss}\n    \\mathcal{L}^C_{B} = \\frac{1}{|\\mathcal{B}|} \\sum_{b \\in \\mathcal{B}}{-\\text{log}\\frac\n    {\\text{exp}({s(\\mathbf{e}^{B*}_b, \\mathbf{e}^{I*}_b)/\\tau})}\n    {\\sum_{p \\in \\mathcal{B}}{\\text{exp}({s(\\mathbf{e}^{B*}_b, \\mathbf{e}^{I*}_p)/\\tau})}}},\n\\end{equation}\nwhere $\\mathcal{L}^C_U$ and $\\mathcal{L}^C_B$ denote the cross-view contrastive losses for users and bundles, respectively; $s(\\cdot,\\cdot)$ is the cosine similarity function; $\\tau$ is a hyper-parameter known as the \\textit{temperature}. We follow SGL \\cite{wu2021self} to perform in-batch negative sampling to construct the negative pairs. By averaging the two cross-view contrastive losses, we obtain the final contastive loss $\\mathcal{L}^C$:\n\\begin{equation} \\label{eq:final_contrastive_loss}\n    \\mathcal{L}^C = \\frac{1}{2}(\\mathcal{L}^C_U + \\mathcal{L}^C_B).\n\\end{equation}\n\n"
                    }
                },
                "subsection 2.4": {
                    "name": "Prediction and Optimization",
                    "content": "\nTo obtain the final prediction for recommendation, we first utilize the inner-product to calculate the item-view and bundle-view predictions, and additively combine them \\za{for} the final prediction.\n\\begin{equation} \\label{eq:prediction}\n    y^*_{u,b} = {\\mathbf{e}^{B*}_u}^{\\intercal}{\\mathbf{e}^{B*}_b} + {\\mathbf{e}^{I*}_u}^{\\intercal}{\\mathbf{e}^{I*}_b}.\n\\end{equation}\nThe \\za{conventional} Bayesian Personalized Ranking (BPR) loss \\cite{rendle2012bpr} \\za{is used} as the main loss.\n\\begin{equation} \\label{eq:bpr_loss}\n    \\mathcal{L}^{BPR} = \\sum_{(u,b,b^{\\prime}) \\in Q}{-\\text{ln} \\sigma (y^*_{u,b} - y^*_{u,b^{\\prime}})}.\n\\end{equation}\nwhere $Q = \\{(u,b,b^{\\prime})|u \\in \\mathcal{U}, b, b^{\\prime} \\in \\mathcal{B}, x_{ub}=1, x_{ub^{\\prime}}=0 \\}$, $\\sigma(\\cdot)$ is the sigmoid function. We achieve the final loss $\\mathcal{L}$ by weighted combing the BPR loss $\\mathcal{L}^{BPR}$, the contrastive loss $\\mathcal{L}^C$, and the L2 regularization term ${\\Vert \\mathbf{\\Theta} \\rVert}_2^2$:\n\\begin{equation} \\label{eq:total_loss}\n    \\mathcal{L} = \\mathcal{L}^{BPR} + \\lambda_{1}{\\mathcal{L}^C} + \\lambda_{2}{\\Vert \\mathbf{\\Theta} \\rVert}_2^2,\n\\end{equation}\nwhere $\\lambda_1$ and $\\lambda_2$ are the \\za{hyperparameters} to balance the three terms, and $\\mathbf{\\Theta} = \\{\\mathbf{E}_{U}^{B(0)},\\mathbf{E}_{B}^{B(0)},\\mathbf{E}_{I}^{I(0)}\\}$ are all of the model parameters.\n\n"
                },
                "subsection 2.5": {
                    "name": "Model Discussion",
                    "content": " \\label{subsec:model_discussion}\n\\za{Integrating the cross-view contrastive loss into BPR loss can provide an additional regularization for representation learning \\cite{zhang2021unleashing}.}\n\\za{Such an effect encourages the bundle recommender to achieve} the cross-view alignment of the same user/bundle \\za{and impose the} dispersion of \\za{different user/bundle pairs in both ego-view and cross-view.}\n\n\n% The cross-view contrastive loss can be treated as a regularization term applied to the representations of two different views~\\cite{zhang2021unleashing}. \nBy \\za{enforcing} the cross-view alignment of the same user/bundle, the distinctive information contained in each view will be distilled to the other view. \nTherefore, both views' representations can be enhanced.\nConsequently, the combined prediction of the two views \\za{can be} further \\za{boosted}. \n\\za{See more details and results in Section~\\ref{subsubsec:mutual-enhancement}.}\n%We will verify this effect by experiments in Section~\\ref{subsubsec:mutual-enhancement}. \n\n\n\nIn addition to the cross-view alignment, the effect of representation dispersion is also \\za{pivotal}. \nBased on Equations~\\ref{eq:user_contrastive_loss} and \\ref{eq:bundle_contrastive_loss}, it seems only the item/bundle pairs across different views are pushed away. \nHowever, the cross-view \\za{alignment also} acts as a bridge to make the \\za{distinct} user/bundle pairs within the same view be \\za{widely separated}. \nLet's take a pair of users $u_1$ and $u_2$ as \\za{an} example, shown in Figure~\\ref{fig:CL_analysis}. \nThe direct effect of the contrastive loss is to pull close the pairs $(\\mathbf{e}^{B*}_{u_1}, \\mathbf{e}^{I*}_{u_1})$ and $(\\mathbf{e}^{B*}_{u_2}, \\mathbf{e}^{I*}_{u_2})$ while push \\za{apart} the pairs $(\\mathbf{e}^{B*}_{u_1}, \\mathbf{e}^{I*}_{u_2})$ and $(\\mathbf{e}^{I*}_{u_1}, \\mathbf{e}^{B*}_{u_2})$. \nConsequently, as an indirect effect, the \\za{distance between} the representations of $u_1$ and $u_2$ in the same view (\\ie $(\\mathbf{e}^{B*}_{u_1}, \\mathbf{e}^{B*}_{u_2})$ and $(\\mathbf{e}^{I*}_{u_1}, \\mathbf{e}^{I*}_{u_2})$) are also \\za{enlarged}.\nTherefore, our proposed cross-view contrastive loss can enhance the discriminative capability of representations in both ego-view and cross-view, resulting in better \\za{bundle} recommendation \\za{quality}. To be noted, solely enlarging the cross-view dispersion without encouraging the cross-view alignment cannot achieve the effect of ego-view dispersion, thus cannot enhance the self-discrimination of the representations.\nWe will justify this effect by analyzing the alignment-dispersion characteristics of representations in Section~\\ref{subsubsec:alignment-dispersion}.\n\n%\\subsubsection{Why there is no model collapse?}\n%1. The raw information of two views of graphs is distinctive enough, which is the main contribution of preventing model collapse.\n%2. The denominator includes positive components which are identical with the numerator of the contrastive loss, weakening the affinity effects of positive pairs. To be noted that, the denominator of the contrastive loss includes the numerator, which means the representations of the positive pairs will not be pulled as close as possible, instead kept a distance by a certain degree. This degree can be controlled by tuning the weight $\\alpha$. We empirically identify the workable range of $\\alpha$. \n\n"
                },
                "subsection 2.6": {
                    "name": "Complexity Analysis",
                    "content": "\nIn terms of space complexity, the parameters of CrossCBR are minimal and only include three sets of embeddings: $\\mathbf{E}^{B(0)}_U$, $\\mathbf{E}^{B(0)}_{B}$, and $\\mathbf{E}^{I(0)}_I$. Therefore, the space complexity of CrossCBR is $\\mathcal{O}((M+N+O)d)$. Our model is more concise than BGCN due to the removal of the feature transformation matrices.\n\nIn terms of time complexity, the main computational cost lies in the two views' graph learning and the cross-view contrastive loss. Note we just focus on the main setting of original preservation. The time complexity of graph learning is $O(|E|Kds\\frac{|E|}{T})$, where $|E|$ is the number of all edges in U-B and U-I graphs, $K$ is the number of propagation layers, $d$ is the embedding size, $s$ is the number of epochs, $T$ is the batch size. For comparison, the time complexity of BGCN is $O((|E|d+|V|d^2)Ks\\frac{|E|}{T})$, where $|V|$ is the number of nodes in U-B and U-I graphs. And the time complexity of graph learning part in CrossCBR is smaller than that of BGCN due to the removal of the feature transformation layers and smaller $|E|$ (due to the removal of self-connections and bundle-bundle connections). The time complexity of calculating the contrastive loss during training is $O(2d(|E|+T^{2})s\\frac{|E|}{T})$. Even though the matrix multiplication of contrastive loss has cubic complexity ($O(dT^{2})$), the time used in practice is very limited due to the acceleration of both hardware GPU and the optimized libraries, justified by the experiments in Section~\\ref{subsubsec:computational-efficiency}. To be noted, the inference of CrossCBR has the identical time complexity with that of BGCN."
                }
            },
            "section 3": {
                "name": "Experiments",
                "content": " \\label{sec:experiment}\nTo evaluate our proposed approach, we conduct experiments on three public datasets: Youshu, NetEase, and iFashion. In particular, we aim to answer the following research questions:\n\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{RQ1: } Can CrossCBR outperform the SOTA baseline models?\n    \\item \\textbf{RQ2: } Are all the key components effective \\wrt performance?\n    \\item \\textbf{RQ3: } Whether the cross-view contrastive learning works as we expected, \\ie achieving cross-view mutual enhancement and alignment and dispersion in the representation space?\n    \\item \\textbf{RQ4: } What about the hyper-parameter sensitivity and training efficiency of the model?\n\\end{itemize}\n\n",
                "subsection 3.1": {
                    "name": "Experimental Settings",
                    "content": "\nWe follow the previous works~\\cite{deng2020personalized,chang2020bundle} to adopt the two established bundle recommendation datasets: Youshu~\\cite{chen2019matching} for book list recommendation and NetEase~\\cite{cao2017embedding} for music playlist recommendation. In addition, we introduce another online fashion outfit recommendation dataset iFashion~\\cite{chen2019pog}, where the outfit consisted of individual fashion items is treated as bundle. We follow the outfit recommendation setting~\\cite{li2020hierarchical} to preprocess the iFashion dataset by the 20-core rule for users and 10-core rule for outfits. All the three datasets have all the required data, \\ie user-bundle interactions, user-item interactions, and bundle-item affiliation. The statistics of the datasets are listed in Table~\\ref{tab:dataset}. To be noted, the three datasets are diverse \\wrt both application scenarios and the statistical characteristics (various scales of interactions and bundle sizes), ensuring the model's robustness to such variance. The training/validation/testing sets are randomly split with the ratio of 70\\%/10\\%/20\\%. Recall@K and NDCG@K are used as the evaluation metrics, where K $\\in$ \\{20, 40\\}. And NDCG@20 is used to select the best model based on the validation set, and all the items are ranked during testing~\\cite{wang2019neural}.\n\n\n\n\n\n",
                    "subsubsection 3.1.1": {
                        "name": "Compared Methods",
                        "content": "\nIn terms of baselines, we select both general user-item recommendation models and bundle-specific recommendation models to compare with our proposed method.\n\n\\noindent \\textbf{The User-item Recommendation Models} treat the bundle as a special type of \\textit{item}, only using the user-bundle interactions without considering the affiliated items within the bundle. We select the following SOTA methods: (1) \\textbf{MFBPR}~\\cite{rendle2012bpr}: Matrix Factorization optimized with the Bayesian Personalized Ranking (BPR) loss; (2) \\textbf{LightGCN}~\\cite{he2020lightgcn}: a GNN- and CF-based recommendation model, which utilizes a light-version graph learning kernel; and (3) \\textbf{SGL}~\\cite{wu2021self}: it enhances the LightGCN model with contrastive graph learning and achieves SOTA performance.\n\n\\noindent \\textbf{The Bundle-specific Recommendation Models} are designed for bundle recommendation and utilize all the user-bundle interactions, user-item interactions, and bundle-item affiliation data. We consider the following models: (1) \\textbf{DAM}~\\cite{chen2019matching}: it uses an attention mechanism to learn bundle representations over the affiliated items and employs multi-task learning to optimize both user-item and user-bundle interactions; (2) \\textbf{BundleNet}~\\cite{deng2020personalized}: it builds a user-bundle-item tripartite graph, leverages GCN to learn the representations, and applies multi-task learning; and (3) \\textbf{BGCN}~\\cite{chang2020bundle,chang2021bundle}: it decomposes the user-bundle-item relations into two separate views, builds two graphs (\\ie bundle-view graph and item-view graph), uses GCN to learn representations, makes prediction by summing the two views' predictions, and achieves SOTA performance. There are also some earlier works (\\eg \\cite{cao2017embedding}) that have been turned to be inferior to the methods listed above, and we do not consider them.\n\n"
                    },
                    "subsubsection 3.1.2": {
                        "name": "Hyper-parameter Settings",
                        "content": "\nFor all methods, the embedding size is set as 64, Xavier normal initialization~\\cite{glorot2010understanding} is adopted, the models are optimized using Adam optimizer~\\cite{kingma2014adam} with the learning rate 0.001, and the batch size is set as 2048. For our method, we tune the hyper-parameters $K$, $\\lambda_1$, $\\lambda_2$, $\\tau$, and $\\rho$ with the ranges of $\\{1, 2, 3\\}$, $\\{0.01, 0.04, 0.1, 0.5, 1\\}$, $\\{10^{-6}, 10^{-5}, 2 \\times 10^{-5}, 4 \\times 10^{-5}, 10^{-4} \\}$, $\\{0.05, \\\\ 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5 \\}$, and $\\{0, 0.1, 0.2, 0.5\\}$. For graph augmentation, we follow SGL to drop edges by every epoch. For baseline methods, we adopt the results of MFBPR, DAM, and BGCN on Youshu and NetEase datasets based on those reported in \\cite{chang2020bundle}, since their settings are the same with ours. We implement all the other baselines by ourselves. All the models are trained using Pytorch 1.9.0, NVIDIA Titan-XP and Titan-V GPUs.\n\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Performance Comparison (RQ1)",
                    "content": " \\label{subsec:performance_comparison}\nWe first compare the overall recommendation performance of CrossCBR with \\za{both user-item recommendation baselines and bundle-specific recommendation baselines} on three datasets, as shown in Table~\\ref{tab:overall_performance}.\n\\za{The best performing methods are bold, while the strongest baselines are underlined; \\%Improv. measures the relative improvements of CrossCBR over the strongest baselines. We observe that:}\n\nIn terms of the general user-item recommendation models, LightGCN \\za{consistently} outperforms MFBPR, indicating the GNN-based method especially the LightGCN graph learning module is effective in modeling the user-bundle CF signals. \nSGL further improves the performance of LightGCN, demonstrating the great power of \\za{contrastive loss} on the user-bundle bipartite graph. \n\\za{Surprisingly}, SGL is the strongest baseline, which is even better than the strongest bundle-specific method (BGCN), implying the effectiveness of graph contrastive learning in recommendation. \n\\za{Our proposed} CrossCBR performs better than SGL by a large margin, showing that the item view truly provides additional useful information and can enhance the \\za{discirminative power of} the model.\n\n\\za{When considering} the bundle-specific models, BGCN performs best \\za{among bundle-specific baselines, \\ie DAM and BundleNet.}\n\\za{We attribute this success to} decomposing the users' preference into two views.\n%can better capture the users' preference. \n\\za{Unfortunately}, BundleNet performs \\za{poorly} since the user-bundle-item tripartite graph fails to differentiate behavioral similarity among users and content relatedness. \nCrossCBR \\za{achieves significant gains over} all the bundle-specific baselines by a large margin, demonstrating the \\za{effectiveness} of modeling the cross-view cooperative association. Our model performs consistently on all the three datasets that belong to varying application scenarios (including book, music, and fashion) and have different statistical characteristics (scales of interactions and bundle sizes). Therefore, out model turns out to be sufficiently robust.\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Ablation Study (RQ2)",
                    "content": " \\label{subsec:ablation_study}\nTo further evaluate the key innovative components of CrossCBR, we conduct a list of ablation studies as shown in Table~\\ref{tab:ablation_study}, where the \\%Improv. \\za{quantifies} the relative improvement of CrossCBR over the SOTA bundle-specific model BGCN.\n\n",
                    "subsubsection 3.3.1": {
                        "name": "Effectiveness of Cross-view Contrastive Learning",
                        "content": "\nTo evaluate whether the cross-view contrastive learning \\za{contributes} to the performance, we remove the contrastive loss $\\mathcal{L}^C$ \\za{in Equation \\eqref{eq:final_contrastive_loss}} during training, \\za{named CrossCBR-CL}. \nCrossCBR-CL \\za{inevitably shows a severe} performance \\za{decline} compared with CrossCBR, justifying the crucial role of \\za{modelling} cross-view \\za{information}. \n\\za{Surprisingly}, CrossCBR-CL still \\za{significantly} outperforms BGCN.\n\\za{We ascribe its success to utilizing the LightGCN kernel, which has been proved to be more effective than the typical GCN kernel used by BGCN \\cite{he2020lightgcn}.}\n% significantly since CrossCBR utilizes the LightGCN kernel which is more effective than the typical GCN kernel used by BGCN. \nTo further identify the \\za{characteristics} of alignment and dispersion, we implement CrossCBR\\_A that only enlarges the cross-view alignment (using the negative cross-view cosine similarity to replace the contrastive loss), and CrossCBR\\_D that only encourages the cross-view dispersion (setting the cosine similarity in numerator of the contrastive loss as static 1).\nThe results in Table~\\ref{tab:ablation_study} demonstrates that the alignment and dispersion collaboratively contribute to the performance, while only modeling either of them can degrade the performance. Especially when only enlarging the cross-view dispersion (CrossCBR\\_D), the model can hardly benefit from it or even collapse, justifying our discussion in Section~\\ref{subsec:model_discussion}.\n\n%As evidenced by He \\etal~\\cite{he2020lightgcn}, in CF-based recommendation scenario, the node features of user and item are randomly initialized, therefore the nonlinear feature transformation and activation function in the typical GCN model is useless and even harmful for model optimization.\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Effectiveness of Data Augmentations",
                        "content": "\nWe try various settings of data augmentations during the representation learning of the two views. CrossCBR\\_OP corresponds to original preservation (\\ie no augmentation), CrossCBR\\_ED \\za{represents} Edge Dropout of the graph-based augmentation method, and CrossCBR\\_MD \\za{refers} to Message Dropout of the embedding-based augmentation method. \nThe results in Table~\\ref{tab:ablation_study} demonstrate that the differences among the three data augmentation settings for CrossCBR are negligible compared with the performance gain over baselines. This phenomenon indicates that the distinction within the original data of the two views provides sufficient variance for the cross-view contrastive learning, while the variance introduced by random data augmentation is insignificant. More advanced and effective data augmentation methods can be explored in the future.\n\n"
                    },
                    "subsubsection 3.3.3": {
                        "name": "The Impact of Simplification of Graph Structure",
                        "content": "\nCrossCBR simplifies the graph structure of BGCN by removing self-connections (SC) in the U-B and U-I graphs and the bundle-bundle (BB) connections in the U-B graph. To justify that these removals do not affect the performance, we specifically add SC and BB connections to our graph construction and obtain CrossCBR+SC and CrossCBR+BB, respectively. The results of both CrossCBR+SC and CrossCBR+BB indicate that both SC and BB contribute little or even none to CrossCBR. The reasons are two-fold. First, in terms of SC, the LightGCN kernel has no feature transformation and activation layers, resulting in the SC a simple summation of the node itself (layer 0), which adds no additional information. \n%However, as shown in Equation~\\ref{eq_2} and \\ref{eq_4}, the final node representation after layer aggregation already includes the node itself (layer 0); consequently, the SC adds no additional information. \nSecond, for the BB connections, they are obtained by calculating the overlap degree of the two bundles according to the B-I graph. However, the BB overlap information can be distilled from the item view to the bundle view representations through the alignment effect of the cross-view contrastive loss. %Therefore, CrossCBR can still perform well without the BB connections in the U-B graph. \n\n"
                    }
                },
                "subsection 3.4": {
                    "name": "Model Study (RQ3)",
                    "content": " \\label{subsec:model_study}\n%To further demystify the large performance leap and reveal the working mechanism of CrossCBR.\nIn this section, we conduct experiments to study: (1) whether the two views are mutually enhanced by the cross-view contrastive loss? and (2) whether the cross-view alignment and dispersion happen as we expected in the represenation space? \n\n\n\n",
                    "subsubsection 3.4.1": {
                        "name": "Mutual Enhancement Effect",
                        "content": " \\label{subsubsec:mutual-enhancement}\nTo directly justify whether the cross-view contrastive learning can achieve cross-view mutual enhancement, we present the performance which is calculated solely based on ego-view predictions, \\ie the bundle-view prediction uses $y_{u,b}^*={\\mathbf{e}_u^{B*}}^{\\intercal} \\mathbf{e}_b^{B*}$, the item-view prediction uses $y_{u,b}^*={\\mathbf{e}_u^{I*}}^{\\intercal} \\mathbf{e}_b^{I*}$, and the both-view prediction is identical with Equation~\\ref{eq:prediction}. The results in Figure~\\ref{fig:perf_views} indicate that using contrastive loss significantly improves the recommendation performance on all the three types of predictions. Interestingly, the bundle view prediction performs much better than that of the item view (even slightly better than the both-view prediction in NetEase), demonstrating that the bundle view plays a more significant role in bundle recommendation. This also helps explain why SGL, which is solely based on the user-bundle interactions, can outperform a list of bundle-specific methods. \n\n\n\n"
                    },
                    "subsubsection 3.4.2": {
                        "name": "Cross-View Alignment and Dispersion Analysis",
                        "content": " \\label{subsubsec:alignment-dispersion}\nWe analyze the cross-view alignment and dispersion characteristics of the representations regularized by the cross-view contrastive learning. Inspired by the alignment-uniformity analysis~\\cite{wang2020understanding,gao2021simcse}, we adopt a simplified version to portray the cross-view alignment and dispersion of the representations. In particular, we just calculate the average cross-view \\textit{cosine similarity} of the users (bundles) as the indication of alignment. Similarly, the average \\textit{cosine similarity} between different users (bundles) of the same view (either item or bundle view) is the indication of the dispersion. Several interesting findings can be derived from the results shown in Table~\\ref{tab:alignment_dispersion}. First, the cross-view alignment metrics of both user and bundle representations ($\\mathbf{A}_U^C$ and $\\mathbf{A}_B^C$) significantly increase after applying the contrastive loss, justifying that the cross-view contrastive loss can effectively pull the two views close to each other. Thereafter, each view can distill cooperative information from the other and they are mutually enhanced. Second, the dispersion of the user representations of both ego-views ($\\mathbf{D}_U^B$ and $\\mathbf{D}_U^I$) significantly reduces after applying the contrastive loss, verifying that the cross-view contrastive loss can improve the discrimination of the users in the ego-view ( \\cf Section~\\ref{subsec:model_discussion}). \n%Interesting, we do not directly push away the representations in the same view, while we push away the representations of two different views (see the denominators of Equation \\ref{eq:user_contrastive_loss} and \\ref{eq:bundle_contrastive_loss}). However, the cross-view alignment performs like a bridge, \\ie the representations of two views are close enough with each other, and pushing away those in cross-view approximates those in self-view. The merit is that CrossCBR does not need self-view discrimination components in the contrastive loss, reducing the computational complexity. \nThird, the dispersion of the bundle representations ($\\mathbf{D}_B^B$ and $\\mathbf{D}_B^I$) does not consistently reduce like that of user's. It may because that the B-I graph directly determines the dispersion of item-view bundle representations via the pooling (\\cf Equation~\\ref{eq_6}), which is distilled to the bundle view. \n%Anyway, the increasing scale is limited and the exact reason is left to explore in the future. \n\n"
                    }
                },
                "subsection 3.5": {
                    "name": "Hyper-parameter and Computational Efficiency Analysis (RQ4)",
                    "content": " \\label{subsec:hyper-parameters}\n\n\n\n",
                    "subsubsection 3.5.1": {
                        "name": "Hyper-parameter Analysis",
                        "content": "\nAs shown in Figure~\\ref{fig:perf_tau_bs}, CrossCBR is sensitive to the temperature $\\tau$, and deviating from the best setting degrades the performance remarkably. \nTo test how the batch size affects the performance, we gradually increase the batch size from 128 to 8192, and the performance first grows quickly and later reaches a plateau as shown in Figure~\\ref{fig:perf_tau_bs}. \nWe keep our default batch size as 2048, since it is widely adopted by the baselines' original implementation and performs sufficiently well in CrossCBR.\n\n"
                    },
                    "subsubsection 3.5.2": {
                        "name": "Computational Efficiency",
                        "content": " \\label{subsubsec:computational-efficiency}\nTo evaluate the computational efficiency of our model, we compare the one-epoch training time \\za{among the variants of} our model and BGCN.\nWe record ten consecutive training epochs and average them to obtain the one-epoch training time, as shown in Table~\\ref{tab:training_time}~\\footnote{The CPU is Intel(R) Xeon(R) CPU E5-2620 v3 with 12 cores, and the memory is 256GB. We use multiprocess dataloader to assure that the computation on CPU is not the bottleneck. In addition, the GPU utility is almost 100\\% during the training of all the settings, therefore, the capacity of GPU dominates the overall training time.}. First of all, Cr\\_OP is significantly more efficient than BGCN, demonstrating the efficiency of CrossCBR. Second, we compare Cr\\_OP with several variants to further explicitly attribute the efficiency boost. In particular, Cr-CL approximates Cr\\_OP, showing that the contrastive loss brings negligible computational costs.\n%and the representation learning of CrossCBR contributes most to the efficiency. \nCr+SC and Cr+BB cost more training time than Cr\\_OP, demonstrating that both SC and BB connections introduce extra costs during training. Especially on NetEase, the costs brought by the BB connections are about three times of Cr\\_OP.\n%strongly verifying our method's advantage by removing the BB connections.\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Related Work",
                "content": " \\label{sec:related_work}\nIn this section, we briefly review the related works in two areas: (1) graph-based and bundle recommendation and (2) contrastive learning in recommendation.\n\n\\textbf{Graph-based and Bundle Recommendation}. Graph-based model has dominated the CF-based recommendation methods due to its superior capability in modeling the higher order interactions between users and items, especially the recent graph neural network-based methods~\\cite{ying2018graph,wang2019neural,he2020lightgcn,chen2019semi}. Wang \\etal propose NGCF~\\cite{wang2019neural} to build a bipartite graph based on the user-item interaction matrix and employ graph convolutional network as the graph learning module. Following NGCF, He \\etal propose to remove some redundant modules (\\eg nonlinear feature transformation and activation function layers) from the NGCF model and significantly improve the performance, resulting in the LightGCN~\\cite{he2020lightgcn} model. LightGCN has achieved great performance in various recommendation tasks~\\cite{ding2021DGSR}, and our model is also base on this backbone.\n\nBundle recommendation aims to solve a special scenario of recommendation, \\ie the recommended object is a bundle of items that related with a certain theme. Initial works just ignore the affiliated items of the bundle and just use an id to represent a bundle~\\cite{rendle2010factorizing}. Following works recognize the importance of affiliated items and develop various models to capture the additional user-item interaction and bundle-item affiliation relations, such as EFM~\\cite{cao2017embedding} and DAM~\\cite{chen2019matching}. With the proliferation of GNN-based recommendation models, Deng \\etal propose BundleNet~\\cite{deng2020personalized} and Chang \\etal propose BGCN~\\cite{chang2020bundle,chang2021bundle}. However, BundleNet mixup the three types of relations among user, bundle, and item, while BGCN decompose the users' preference into item view and bundle view. The two-view representations effectively capture the two types of preferences, resulting in better performance. Our work is based on this two-view modeling framework, and we further emphasize the significance of the cooperative association modeling between the two views. Some related topics, such as set, basket, or package recommendation~\\cite{hu2020modeling,qin2021world,li2021package} and bundle generation~\\cite{bai2019personalized,chang2021bundle}, are different with our scenario in either the recommended object (a loosely/arbitrary co-occurred set/basket/package \\textit{vs} a pre-defined bundle of items related with a theme) or the task (generation of bundle from items \\textit{vs} recommending pre-defined bundles).\n\n\\textbf{Contrastive Learning in Recommendation}. Recently, contrastive learning regains popularity and achieves great success in computer vision~\\cite{hjelm2018learning,oord2018representation,chen2020simple}, natural language processing~\\cite{logeswaran2018efficient,gao2021simcse}, and graph learning~\\cite{DIR,RGCL}. The community of recommender systems also seizes this trend and adapts contrastive learning into various recommendation tasks, such as general CF-based recommendation~\\cite{wu2021self,zhou2021selfcf,zhou2021contrastive}, sequential and session recommendation~\\cite{zhou2020s3,xie2020contrastive,liu2021contrastive,xia2020self,xia2021self}, multimedia and social recommendation~\\cite{wei2021contrastive,yu2021socially}, \\etc The key of introducing contrastive learning into recommender systems lies in proper construction of contrastive pairs. One branch of current approaches are based on various data augmentations to create more views from the original data. For example, SGL~\\cite{chang2020bundle} adopts various graph augmentation methods (\\eg dege dropout or random walk), and CL4SRec~\\cite{xie2020contrastive} and CoSeRec~\\cite{liu2021contrastive} apply different sequence augmentation methods (\\eg insertion, deletioin, and reordering \\etc). Another branch of methods aim at mining multiple views that exist in the data. For example, COTREC~\\cite{xia2021self} builds two views (\\ie an item view and a session view) to learn the session representations from two sources of data (\\ie item transition graph of a session and session-session similarity graph), and apply contrastive learning based on the two views. CLCRec~\\cite{wei2021contrastive} treats different modalities and the user/item as different views to build contrastive pairs. In this work, we unify both types of methods: build two views from different data sources and apply data augmentations.\n\n"
            },
            "section 5": {
                "name": "Conclusion and Future Work",
                "content": " \\label{conclusion}\n\nIn this work, we applied the cross-view contrastive learning to model the cross-view cooperative association in bundle recommendation. \n%Motivated by the intuition of potential cooperative association between the bundle and item view, we introduced the cross-view contrastive learning to regularize the cross-view representations. \n%We proposed a simple, efficient, yet effective method CrossCBR, which significantly enhanced the SOTA performance of bundle recommendation on three public datasets. Various ablation and model studies demystified the working mechanism behind such huge performance leap.\nWe introduced the cross-view contrastive learning to regularize the cross-view representations and proposed a simple, efficient, yet effective method CrossCBR, which significantly enhanced the SOTA performance of bundle recommendation on three public datasets. Various ablation and model studies demystified the working mechanism behind such huge performance leap.\n\nEven though CrossCBR has achieved great performance, the study of contrastive learning on bundle or even general recommendation is still in its infancy, and several directions are promising in the future. First, model-based data augmentations, which can introduce both hard negative and diverse positive samples, should be helpful for further performance improvements. Second, more potential approaches are to be explored for modeling the cross-view cooperative association. Third, the cross-view contrastive learning paradigm is easy to be generalized to other similar tasks, as long as two distinctive views exit.\n%such as sequential recommendation where user's long-term and short-term preferences are two different views. \n\n%Finally, the way of constructing contrastive pairs, which is mainly based on human heuristics, should be systematically studied and more regular patterns are expected to be found, which will facilitate the application of contrastive learning to various unexplored scenarios.\n"
            },
            "section 6": {
                "name": "acknowledgement",
                "content": "\nThis research/project is supported by the Sea-NExT Joint Lab, and CCCD Key Lab of Ministry of Culture and Tourism.\n%\\newpage\n\\bibliographystyle{ACM-Reference-Format}\n%\\balance\n\\bibliography{0_main}\n%\\input{7_appendix}\n"
            }
        },
        "tables": {
            "tab:overall_performance": "\\begin{table*}[t]\n\\caption{The overall performance comparison, where Rec is short of Recall. \\za{Note that the improvement achieved by CrossCBR is significant ($p$-value $\\ll 0.05$).}}\n\\vspace{-0.1in}\n\\label{tab:overall_performance}\n\\centering\n\\setlength{\\tabcolsep}{1mm}{\n    \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{l | cccc | cccc | cccc}\n        \\hline\n        \\multirow{2}{*}{Model} & \\multicolumn{4}{c|}{Youshu} &\\multicolumn{4}{c|}{NetEase} &\\multicolumn{4}{c}{iFashion} \\\\\n        \\cline{2-13} & Rec@20 & NDCG@20 & Rec@40 & NDCG@40 & Rec@20 & NDCG@20 & Rec@40 & NDCG@40 & Rec@20 & NDCG@20 & Rec@40 & NDCG@40 \\\\\n        \\hline\n        \\hline\n        \\textbf{MFBPR}     & 0.1959\t& 0.1117 & 0.2735 & 0.1320 & 0.0355 & 0.0181 & 0.0600 & 0.0246 & 0.0752 & 0.0542 & 0.1162 & 0.0687 \\\\\n        \\textbf{LightGCN}  & 0.2286 & 0.1344 & 0.3190 & 0.1592 & 0.0496 & 0.0254 & 0.0795 & 0.0334 & 0.0837 & 0.0612 & 0.1284 & 0.0770 \\\\ \n        \\textbf{SGL} & \\underline{0.2568} & \\underline{0.1527} & \\underline{0.3537} & \\underline{0.1790} & \\underline{0.0687} & \\underline{0.0368} & \\underline{0.1058} & \\underline{0.0467} & \\underline{0.0933} & \\underline{0.0690} & \\underline{0.1389} & \\underline{0.0851} \\\\\n        \\hline\n        \\hline\n        \\textbf{DAM} & 0.2082 & 0.1198 & 0.2890 & 0.1418 & 0.0411 & 0.0210 & 0.0690 & 0.0281 & 0.0629 & 0.0450 & 0.0995 & 0.0579 \\\\\n        \\textbf{BundleNet} & 0.1895 & 0.1125 & 0.2675 & 0.1335 & 0.0391 & 0.0201 & 0.0661 & 0.0271 & 0.0626 & 0.0447 & 0.0986 & 0.0574 \\\\\n        \\textbf{BGCN}      & 0.2347 & 0.1345 & 0.3248 & 0.1593 & 0.0491 & 0.0258 & 0.0829 & 0.0346 & 0.0733 & 0.0531 & 0.1128 & 0.0671 \\\\\n        \\hline\n        \\hline \t\t\t \t\t\t\t\t\t\n        \\textbf{CrossCBR} & \\textbf{0.2813} & \\textbf{0.1668} & \\textbf{0.3785} & \\textbf{0.1938} & \\textbf{0.0842} & \\textbf{0.0457} & \\textbf{0.1264} & \\textbf{0.0569} & \\textbf{0.1173} & \\textbf{0.0895} & \\textbf{0.1699} & \\textbf{0.1080} \\\\\t\t\t\t\n        \\textbf{\\%Improv.} & 9.57 & 9.26 & 7.02 & 8.28 & 22.57 & 24.33 & 19.48 & 21.96 & 25.76 & 29.63 & 22.33 & 26.85 \\\\\n        \\hline\n        \\end{tabular}\n    }\n}\n\\vspace{-0.1in}\n\\end{table*}"
        },
        "figures": {
            "fig:motivation": "\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width = 0.8\\linewidth]{figures/motivation.pdf}\n    \\vspace{-6pt}\n    \\caption{Top: The bundle and item views presented in the U-B, U-I and B-I graphs. Bottom: Our work models the cooperative association between views, where the superscripts $B$ and $I$ denote the bundle and item view,  and the subscripts $u$, $b$, and $i$ stand for the user, bundle, and item.}\n    \\label{fig:motivation}\n    \\vspace{-12pt}\n\\end{figure}",
            "fig:framework": "\\begin{figure*}\n    \\centering\n    \\includegraphics[width = 0.9\\linewidth]{figures/framework.pdf}\n    \\vspace{-0.1in}\n    \\caption{The overall framework of CrossCBR consists of two parts: (1) representation learning for the two views of users and bundles and (2) the joint optimization of the BPR loss $\\mathcal{L}^{BPR}$ and contrastive loss $\\mathcal{L}^C$.}\n    \\vspace{-0.15in}\n    \\label{fig:framework}\n\\end{figure*}",
            "fig:CL_analysis": "\\begin{figure}\n    \\centering\n    \\includegraphics[width = 0.8\\linewidth]{figures/CL_analysis.pdf}\n    \\vspace{-0.1in}\n    \\caption{The illustration of the direct (a) and indirect (b) effects of the cross-view contrastive loss.}\n    \\label{fig:CL_analysis}\n    \\vspace{-0.2in}\n\\end{figure}",
            "fig:perf_views": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.98\\linewidth]{figures/perf_views.pdf}\n    \\vspace{-0.1in}\n    \\caption{The performance of CrossCBR and CrossCBR-CL \\wrt predictions based on different views.}\n    \\label{fig:perf_views}\n    \\vspace{-0.15in}\n\\end{figure}",
            "fig:perf_tau_bs": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.98\\linewidth]{figures/perf_tau_bs.pdf}\n    \\vspace{-0.1in}\n    \\caption{The performance (NDCG@20) variance of CrossCBR \\wrt the temperature $\\tau$ and the batch size on both datasets of NetEase and iFashion.}\n    \\label{fig:perf_tau_bs}\n    \\vspace{-0.15in}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation} \\label{eq_2}\n\\left\\{\n\\begin{aligned}\n    \\mathbf{e}_{u}^{B(k)}=\\sum_{b \\in \\mathcal{N}_u}{\\frac{1}{\\sqrt{|\\mathcal{N}_u|}\\sqrt{|\\mathcal{N}_b|}}\\mathbf{e}^{B(k-1)}_{b}}, \\\\\n    \\mathbf{e}_{b}^{B(k)}=\\sum_{u \\in \\mathcal{N}_b}{\\frac{1}{\\sqrt{|\\mathcal{N}_b|}\\sqrt{|\\mathcal{N}_u|}}\\mathbf{e}^{B(k-1)}_{u}},\n\\end{aligned}\n\\right.\n\\end{equation}",
            "eq:2": "\\begin{equation} \\label{eq_3}\n    \\mathbf{e}^{B*}_{u} = \\sum^K_{k=0}{\\mathbf{e}^{B(k)}_{u}}, \\ \\ \\ \\ \\mathbf{e}^{B*}_b = \\sum^K_{k=0}{\\mathbf{e}^{B(k)}_b}.\n\\end{equation}",
            "eq:3": "\\begin{equation} \\label{eq_4}\n\\left\\{\n\\begin{aligned}\n    \\mathbf{e}_{u}^{I(k)}=\\sum_{i \\in \\mathcal{N}_u}{\\frac{1}{\\sqrt{|\\mathcal{N}_u|}\\sqrt{|\\mathcal{N}_i|}}\\mathbf{e}^{I(k-1)}_i}, \\\\\n    \\mathbf{e}_{i}^{I(k)}=\\sum_{u \\in \\mathcal{N}_i}{\\frac{1}{\\sqrt{|\\mathcal{N}_i|}\\sqrt{|\\mathcal{N}_u|}}\\mathbf{e}^{I(k-1)}_{u}},\n\\end{aligned}\n\\right.\n\\end{equation}",
            "eq:4": "\\begin{equation} \\label{eq_5}\n    \\mathbf{e}^{I*}_{u} = \\sum^K_{k=0}{\\mathbf{e}^{I(k)}_{u}}, \\ \\ \\ \\ \\mathbf{e}^{I*}_{i} = \\sum^K_{k=0}{\\mathbf{e}^{I(k)}_{i}},\n\\end{equation}",
            "eq:5": "\\begin{equation} \\label{eq_6}\n    \\mathbf{e}^{I*}_{b} = \\frac{1}{|\\mathcal{N}_b|}\\sum_{i \\in \\mathcal{N}_b}\\mathbf{e}^{I*}_i,\n\\end{equation}",
            "eq:6": "\\begin{equation} \\label{eq:user_contrastive_loss}\n    \\mathcal{L}^C_{U} = \\frac{1}{|\\mathcal{U}|} \\sum_{u \\in \\mathcal{U}}{-\\text{log}\\frac\n    {\\text{exp}({s(\\mathbf{e}^{B*}_u, \\mathbf{e}^{I*}_u)/\\tau})}\n    {\\sum_{v \\in \\mathcal{U}}{\\text{exp}({s(\\mathbf{e}^{B*}_u, \\mathbf{e}^{I*}_v)/\\tau})}}},\n\\end{equation}",
            "eq:7": "\\begin{equation} \\label{eq:bundle_contrastive_loss}\n    \\mathcal{L}^C_{B} = \\frac{1}{|\\mathcal{B}|} \\sum_{b \\in \\mathcal{B}}{-\\text{log}\\frac\n    {\\text{exp}({s(\\mathbf{e}^{B*}_b, \\mathbf{e}^{I*}_b)/\\tau})}\n    {\\sum_{p \\in \\mathcal{B}}{\\text{exp}({s(\\mathbf{e}^{B*}_b, \\mathbf{e}^{I*}_p)/\\tau})}}},\n\\end{equation}",
            "eq:8": "\\begin{equation} \\label{eq:final_contrastive_loss}\n    \\mathcal{L}^C = \\frac{1}{2}(\\mathcal{L}^C_U + \\mathcal{L}^C_B).\n\\end{equation}",
            "eq:9": "\\begin{equation} \\label{eq:prediction}\n    y^*_{u,b} = {\\mathbf{e}^{B*}_u}^{\\intercal}{\\mathbf{e}^{B*}_b} + {\\mathbf{e}^{I*}_u}^{\\intercal}{\\mathbf{e}^{I*}_b}.\n\\end{equation}",
            "eq:10": "\\begin{equation} \\label{eq:bpr_loss}\n    \\mathcal{L}^{BPR} = \\sum_{(u,b,b^{\\prime}) \\in Q}{-\\text{ln} \\sigma (y^*_{u,b} - y^*_{u,b^{\\prime}})}.\n\\end{equation}",
            "eq:11": "\\begin{equation} \\label{eq:total_loss}\n    \\mathcal{L} = \\mathcal{L}^{BPR} + \\lambda_{1}{\\mathcal{L}^C} + \\lambda_{2}{\\Vert \\mathbf{\\Theta} \\rVert}_2^2,\n\\end{equation}"
        },
        "git_link": "https://github.com/mysbupt/CrossCBR"
    }
}