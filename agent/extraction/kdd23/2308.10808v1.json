{
    "meta_info": {
        "title": "Graph Neural Bandits",
        "abstract": "Contextual bandits algorithms aim to choose the optimal arm with the highest\nreward out of a set of candidates based on the contextual information. Various\nbandit algorithms have been applied to real-world applications due to their\nability of tackling the exploitation-exploration dilemma. Motivated by online\nrecommendation scenarios, in this paper, we propose a framework named Graph\nNeural Bandits (GNB) to leverage the collaborative nature among users empowered\nby graph neural networks (GNNs). Instead of estimating rigid user clusters as\nin existing works, we model the \"fine-grained\" collaborative effects through\nestimated user graphs in terms of exploitation and exploration respectively.\nThen, to refine the recommendation strategy, we utilize separate GNN-based\nmodels on estimated user graphs for exploitation and adaptive exploration.\nTheoretical analysis and experimental results on multiple real data sets in\ncomparison with state-of-the-art baselines are provided to demonstrate the\neffectiveness of our proposed framework.",
        "author": "Yunzhe Qi, Yikun Ban, Jingrui He",
        "link": "http://arxiv.org/abs/2308.10808v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "Accepted to SIGKDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "   \\label{sec_introduction}\n\nContextual bandits are a specific type of multi-armed bandit problem where the additional contextual information (contexts) related to arms are available in each round, \nand the learner intends to refine its selection strategy based on the received arm contexts and rewards.\nExemplary applications include online content recommendation, advertising \\citep{linucb-Ind_li2010contextual,colla_environ_2016}, and clinical trials \\citep{MAB_clinical_trial-durand2018contextual,MAB_clinical_trial_2_villar2015multi}.\n%\nMeanwhile, collaborative effects among users provide researchers the opportunity to design better recommendation strategies, \nsince the target user's preference can be inferred based on other similar users. Such effects have been studied by many bandit works \\citep{club_2014,SCLUB_li2019improved,CAB_2017,co_filter_bandits_2016,local_clustering-ban2021local}. \n%\nDifferent from the conventional collaborative filtering methods \\citep{NCF_he2017neural,neural_graph_colla_filtering_wang2019neural}, bandit-based approaches focus on more dynamic environments under the online learning settings without pre-training \\cite{dynamic_ensemble_wu2019dynamic}, especially when the user interactions are insufficient during the early stage of recommendation (such as dealing with new items or users under the news, short-video recommendation settings), which is also referred to as the ``cold-start'' problem \\cite{linucb-Ind_li2010contextual}. In such cases, the exploitation-exploration dilemma \\cite{UCB_auer2002finite} inherently exists in the decisions of recommendation.\n\n\n\n\\iffalse\n\nextensively studied by collaborative filtering methods \\cite{NCF_he2017neural,neural_graph_colla_filtering_wang2019neural}. \n\nsince they enable the sharing of user preference information among similar users.\nFor example, these effects could help the learner adapt to items with limited user feedback. In music recommendation, if two users share similar tastes over a specific genre, then this similarity might generalize to the new music pieces from this genre with a high probability. \n\nWe could therefore leverage the feedback from existing users to infer the preferences of their highly correlated users.\nHowever, compared with static settings where collaborative filtering algorithms have proved their effectiveness, e.g., , it is more difficult to exploit the collaborative effects under the bandit settings due to multiple challenges, such as insufficient user feedback and the exploitation-exploration dilemma. \n\n\\fi\n\n\nExisting works for clustering of bandits \\citep{club_2014,SCLUB_li2019improved,CAB_2017,co_filter_bandits_2016,local_clustering-ban2021local,Meta-Ban} \nmodel the user correlations (collaborative effects) by clustering users into \\textbf{rigid user groups} and then assigning each user group an estimator to learn the underlying reward functions, combined with an Upper Confidence Bound (UCB) strategy for exploration.\nHowever, these works only consider the ``coarse-grained'' user correlations. To be specific, they assume that users from the same group would share identical preferences, i.e., the users from the same group are compelled to make equal contributions to the final decision (arm selection) with regard to the target user. \nSuch formulation of user correlations (``coarse-grained'') fails to comply with many real-world application scenarios, because users within the same group can have similar but subtly different preferences.\n%\nFor instance, under the settings of movie recommendation, although we can allocate two users that ``favor'' the same movie into a user group, their ``favoring levels'' can differ significantly: one user could be a die hard fan of this movie while the other user just considers this movie to be average-to-good. In this case, it would not be the best strategy to vaguely consider they share the same preferences and treat them identically.\nNote that similar concerns also exist even when we switch the binary ranking system to a categorical one (e.g., the rating system out of 5 stars or 10 points), because as long as we model with user categories (i.e., rigid user groups), there will likely be divergence among the users within the same group.\n%\nMeanwhile, with more fine-sorted user groups, there will be less historical user interactions allocated to each single group because of the decreasing number of associated users. This can lead to the bottlenecks for the group-specific estimators due to the insufficient user interactions.\n%\nTherefore, given a target user, it is more practical to assume that the rest of the users would impose different levels of collaborative effects on this user. \n\n\n\n\\iffalse\n\n\\fi\n\n\n\n\nMotivated by the limitations of existing works, in this paper, we propose a novel framework, named \\textbf{Graph Neural Bandits (\\name)}, to formulate ``fine-grained'' user collaborative effects, where the correlation of user pairs is preserved by user graphs. Given a target user, other users are allowed to make different contributions to the final decision based on the strength of their correlation with the target user.\nHowever, the user correlations are usually unknown, and the learner is required to estimate them on the fly. Here, the learner aims to approximate the correlation between two users by exploiting their past interactions; on the other hand, the learner can benefit from exploring the potential correlations between users who do not have sufficient interactions, or the correlations that might have changed. In this case, we formulate this problem as the exploitation-exploration dilemma in terms of the user correlations.  \nTo solve this new challenge, \\name\\ separately constructs two kinds of user graphs, named ``user exploitation graphs'' and ``user exploration graphs''. Then, we apply two individual graph neural networks (GNNs) on the user graphs, to incorporate the collaborative effects in terms of both exploitation and exploration in the decision-making process.\nOur main contributions are:\n\n\n\n\n\\iffalse \n\n\nMotivated by aforementioned limitations of existing works, in this paper, we propose a novel framework called \\name~based,\non graph neural networks (GNNs). Unlike existing works with rigid estimated user groups where the divergence within each group is neglected, \\name~considers \\textbf{fine-grained} collaborative effects, where the pair-wise user correlations are modeled through estimated user graphs.\nIn particular, multiple GNN models are adopted to leverage the collaborative effects among users for better arm recommendation strategies. To be specific, first, based on existing received arm contexts and user feedback, we estimate the collaborative effects on existing records into estimated user graphs named ``user exploitation graphs'' for each candidate arm, and apply the first GNN-based model for arm reward estimation (exploitation).\nMeanwhile, to solve the exploitation-exploration dilemma, we formulate the second kind of user graphs for each arm, namely the ``user exploration graphs'', to encode the estimated user correlations for exploration. Then, we utilize a second GNN-based model to estimate the confidence score for reward estimation (exploration) inspired by recent advances of adaptive exploration in contextual bandits \\cite{EE-Net_ban2021ee}. \nOur main contributions can be summarized as follows:\n\\fi\n\n\n\\begin{itemize}[leftmargin=*]\n    \\vspace{-0.1cm}\n    \\item \\textbf{[Problem Settings]} Different from existing works formulating the ``coarse-grained'' user correlations by neglecting the divergence within user groups, we introduce a new problem setting to model the ``fine-grained'' user collaborative effects via user graphs. Here, pair-wise user correlations are preserved to contribute differently to the decision-making process. (\\textbf{Section} \\ref{sec_problem_def_notation})\n    %\n    % \\vspace{-0.1cm}\n    \\item \\textbf{[Proposed Framework]} We propose a framework named \\name, which has the novel ways to build two kinds of user graphs in terms of exploitation and exploration respectively. Then, \\name~utilizes GNN-based models for a refined arm selection strategy by leveraging the user correlations encoded in these two kinds of user graphs for the arm selection. (\\textbf{Section} \\ref{sec_proposed_framework})\n    %\n    % \\vspace{-0.1cm}\n    \\item \\textbf{[Theoretical Analysis]} With standard assumptions, we provide the theoretical analysis showing that \\name\\ can achieve the regret upper bound of complexity $ \\mathcal{O}(\\sqrt{T\\log(Tn)})$, where $T$ is the number of rounds and $n$ is the number of users. This bound is sharper than the existing related works. (\\textbf{Section} \\ref{sec_theoretical_analysis})\n    %\n    % \\vspace{-0.1cm}\n    \\item \\textbf{[Experiments]} Extensive experiments comparing \\name\\ with nine state-of-the-art algorithms are conducted on various data sets with different specifications, which demonstrate the effectiveness of our proposed \\name\\ framework. (\\textbf{Section} \\ref{sec_experiments})\n\\end{itemize}\n\\vspace{-0.05cm}\nDue to the page limit, interested readers can refer to the paper Appendix for supplementary contents.\n\n\n\n\n\n\n\\vspace{-0.2cm}\n"
            },
            "section 2": {
                "name": "Related Works",
                "content": "   \\label{sec_related_works}\nAssuming the reward mapping function to be linear, the linear upper confidence bound (UCB) algorithms \\citep{Lin_UCB-2011,linucb-Ind_li2010contextual,UCB_auer2002finite,improved_linear_bandits_abbasi2011improved} were first proposed to tackle the exploitation-exploration dilemma. \nAfter kernel-based methods \\citep{kernel_ucb-2013,kmtl-ucb_2017} were used to tackle the kernel-based reward mapping function under the non-linear settings, neural algorithms \\citep{Neural-UCB,neural_thompson-zhang2020neural,neural_multifacet-ban2021multi} have been proposed to utilize neural networks to estimate the reward function and confidence bound. \nMeanwhile, AGG-UCB \\citep{AGG-UCB_qi2022neural} adopts GNN to model the arm group correlations. GCN-UCB \\citep{GCN-UCB_upadhyay2020graph} manages to apply the GNN model to embed arm contexts for the downstream linear regression, and GNN-PE \\citep{GNN-PE_kassraie2022graph} utilizes the UCB based on information gains to achieve exploration for classification tasks on graphs.\nInstead of using UCB, EE-Net \\citep{EE-Net_ban2021ee} applies a neural network to estimate prediction uncertainty.\n%\nNonetheless, all of these works fail to consider the collaboration effects among users under the real-world application scenarios.\n\nTo model user correlations, \\citep{colla_environ_2016,GangOfBandits-cesa2013gang} assume the user social graph is known, and apply an ensemble of linear estimators. Without the prior knowledge of user correlations, CLUB \\citep{club_2014} introduces the user clustering problem with the graph-connected components, and SCLUB \\citep{SCLUB_li2019improved} adopts dynamic user sets and set operations, while DynUCB \\citep{Dyn-UCB_nguyen2014dynamic} assigns users to their nearest estimated clusters. Then, CAB \\citep{CAB_2017} studies the arm-specific user clustering, and LOCB \\citep{local_clustering-ban2021local} estimates soft-margin user groups with local clustering. COFIBA \\citep{co_filter_bandits_2016} utilizes user and arm co-clustering for collaborative filtering.\nMeta-Ban \\citep{Meta-Ban} applies a neural meta-model to adapt to estimated user groups.\nHowever, all these algorithms consider rigid user groups, where users from the same group are treated equally with no internal differentiation. \nAlternatively, we leverage GNNs \\citep{GCN_kipf2016semi,fastGCN-chen2018fastgcn,SGC_wu2019simplifying,APPNP-klicpera2018predict,light_GCN-he2020lightgcn,GNN_Fully_Connected_satorras2018few,GNN_community_detection-you2019position,Graph-Recommender_ying2018graph} to learn from the ``fine-grained'' user correlations and arm contexts simultaneously.\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.2cm}\n% ====================================================\n% \\section{Graph Neural Bandits: Problem Definition and Notation}    \\label{sec_problem_def_notation}\n\n% Suppose there are a total of $n$ users with the user set $\\mathcal{U} = \\{1, \\cdots, n\\}$. At each time step $t\\in [T]$, the learner will receive a user $u_{t} \\in \\mathcal{U}$ to serve. \n% Then, as the arm pool is not fixed, we use $\\mathcal{X}_{t} = \\{\\vect{x}_{i, t}\\}_{i\\in [a]}$ to denote the set of candidate arms for recommendation in round $t$.\n% The volume of this arm set is $\\abs{\\mathcal{X}_{t}} = a$, and each arm is described by a $d$-dimensional context vector $\\vect{x}_{i, t}\\in \\mathbb{R}^{d}$. Meanwhile, each arm $\\vect{x}_{i, t}$ will also be related to a reward defined by\n% \\begin{equation}\n% \\begin{split}\n%     r_{i, t} =  h(\\vect{x}_{i, t}, u_{t}, \\matr{\\Lambda}^{*}_{i, t}) + \\epsilon_{i, t}\n% \\end{split}\n% \\label{eq_weighted_reward_func}\n% \\end{equation}\n\n% where $h(\\cdot)$ is the unknown reward mapping function, and $\\epsilon_{i, t}$ stands for the zero-mean noise drawn from a certain distribution with the bounded value range (e.g., truncated Gaussian distribution). \n% Here, $\\matr{\\Lambda}^{*}_{i, t} \\in \\mathbb{R}^{n\\times n}$ is the unknown user affinity matrix that encodes the user correlations w.r.t. the arm $\\vect{x}_{i, t}$, and $\\matr{\\Lambda}^{*}_{i, t}[u, :]$ is the row of $\\matr{\\Lambda}^{*}_{i, t}$ that relates to the user $u\\in \\mathcal{U}$.\n\n\n\n% % ------------------------------------------------------------\n% Then, given the same arm $\\vect{x}_{i, t}$, we consider the difference of expected rewards between any two users $u, u' \\in \\mathcal{U}$ is bounded by\n% \\begin{equation}\n% \\begin{split}\n%     \\abs{h(\\vect{x}_{i, t}, u, \\matr{\\Lambda}^{*}_{i, t}) - h(\\vect{x}_{i, t}, u', \\matr{\\Lambda}^{*}_{i, t})} \\leq \\Psi (\\matr{\\Lambda}^{*}_{i, t}[u, :], \\matr{\\Lambda}^{*}_{i, t}[u', :])\n% \\end{split}   \n% \\label{eq_assumption_correlation_vector}\n% \\end{equation}\n% where $\\matr{\\Lambda}^{*}_{i, t}[u, :]$ is the user correlation vector (i.e., the corresponding row in $\\matr{\\Lambda}^{*}_{i, t}$) of user $u$, and $\\Psi: \\mathbb{R}^{n}\\times\\mathbb{R}^{n} \\mapsto \\mathbb{R}$ \n% denotes an unknown mapping from two user correlation vectors to the difference of the expected rewards.\n% With \\textbf{Eq.} \\ref{eq_weighted_reward_func} being the universal reward function that formulates the mapping from any user $u\\in \\mathcal{U}$ to the expected reward, \n\n% \\TODO{Discuss the user models}\n\n% Note that compared with previous linear methods (e.g., \\cite{GangOfBandits-cesa2013gang,colla_environ_2016}) which simply aggregate expected rewards, our assumption is more generic and could be fit into non-linear settings. \n% Meanwhile, our definition of the reward function can be easily generalized to existing user clustering algorithms (e.g., \\cite{club_2014,CAB_2017,local_clustering-ban2021local}) by allowing $\\matr{\\Lambda}^{*}_{i, t}$ to be a block matrix where each block corresponds to a single user group.\n\n\n% % -------------\n% Analogous to existing works (e.g., \\cite{club_2014,CAB_2017,local_clustering-ban2021local,Meta-Ban}), we assume bounded rewards $r_{i, t} \\in [0, 1]$.\n% %\n% Then, for the received user $u_{t}$ at each time step $t$, the learner is expected to recommend an arm $x_{t}\\in \\mathcal{X}_{t}$ (with reward $r_{t}$) in order to minimize the cumulative pseudo-regret\n% $\n%     R(T) = \\mathbb{E}[\\sum_{t=1}^{T}(r_{t}^{*} - r_{t})] \n% $\n% where $r_{t}^{*}$ is the reward for the optimal arm $ \\mathbb{E}[r_{t}^{*}|\\mathcal{X}_{t}, u_t] = \\max_{\\vect{x}_{i,t}\\in \\mathcal{X}_{t}} h(\\vect{x}_{i, t}, u_{t}, \\matr{\\Lambda}^{*}_{i, t})$. \n\n% With $\\mathcal{T}_{u, t}$ being the collection of past time steps that user $u\\in \\mathcal{U}$ is served up to round $t$, we use $\\mathcal{P}_{u, t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in \\mathcal{T}_{u, t}}$ to represent the collection of received arm-reward pairs associated with user $u$, and apply $T_{u, t} = \\abs{\\mathcal{T}_{u, t}}$ to denote the number of rounds that user $u$ has been served. \n% Analogously, we use $\\mathcal{P}_{t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in [t]}$ to denote all the past records (i.e., arm-reward pairs), up to round $t$.\n\n\n% % --------------\n% \\subsection{User Exploitation Graphs and User Exploration Graphs}   \\label{subsec_true_user_graphs}\n\n% As the user collaborative effects (encoded by $\\matr{\\Lambda}^{*}_{i, t}$ from \\textbf{Eq.} \\ref{eq_weighted_reward_func}) are unknown, we formulate user graphs to represent and exploit the knowledge encoded in $\\matr{\\Lambda}^{*}_{i, t}$.\n\n% % and $\\mathcal{G}^{(2), *}_{i, t}$ to denote the residual (i.e., the potential gain) between $\\mathcal{G}^{(1), *}_{i, t}$ and the estimator output for exploration. \n\n% In order to model the user correlations and deal with the exploration-exploitation dilemma, for each candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, we construct two user correlation graphs: a user exploitation graph $\\mathcal{G}^{(1), *}_{i, t} = (V, E, W^{(1), *}_{i, t})$ and a user exploration graph $\\mathcal{G}^{(2), *}_{i, t} = (V, E, W^{(2), *}_{i, t})$. \n% Here, the user exploitation graph $\\mathcal{G}^{(1), *}_{i, t}$ encodes the collaborative effects in terms of user preferences towards arm $\\vect{x}_{i, t}$, and the user exploration graph $\\mathcal{G}^{(2), *}_{i, t}$ models the user correlation in terms of the uncertainty of reward estimations.\n% %\n\n% Here, each user from $\\mathcal{U}$ is mapped to a corresponding node in node set $V$. With $E = \\{e(c_{i}, c_{j})\\}_{\\forall c_{i}, c_{j} \\in \\mathcal{X}}$ being the set of edges, we have $W^{(1), *}_{i, t}, W^{(2), *}_{i, t}$ to represent the set of edge weights. Note that by definition, $\\mathcal{G}^{(1), *}_{t}, \\mathcal{G}^{(2), *}_{t}$ will stay as fully-connected graphs, and the estimated user (exploitation / exploration) correlations are modeled by the edge weights of node (user) pairs. \n% % For a given node $v \\in V$, we denote the augmented $k$-hop neighborhood $\\widetilde{\\mathcal{N}}_{k}(v) := \\mathcal{N}_{k}(v) \\cup \\{v\\}$ as the union node set of its $k$-hop neighborhood $\\mathcal{N}_{k}(v)$ and node $v$ itself. \n% Given a graph $\\mathcal{G}$, we denote $\\matr{A} \\in \\mathbb{R}^{n \\times n}$ as its adjacency matrix (with added self-loops), and $\\matr{D} \\in \\mathbb{R}^{n \\times n}$ as its degree matrix.\n% Then, we proceed to give the definition of arm-specific user correlations encoded by $\\mathcal{G}^{(1), *}, \\mathcal{G}^{(2), *}$ respectively.\n\n% % ====================================================\n% \\begin{definition}[Arm-Specific User Correlation for Exploitation]  \\label{def_exploitation_simi}\n% In round $t$, for any two users $u, u'\\in \\mathcal{U}$, their exploitation correlation score $w_{i, t}^{(1), *}(u, u')$ w.r.t. a candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$ is defined as\n% \\begin{displaymath}\n% \\begin{aligned}\n%     w_{i, t}^{(1), *}(u, u') = \\Psi^{(1)} \\big( \\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}], \\mathbb{E}[r_{i, t}| u',~ \\vect{x}_{i, t}]\\big)\n% \\end{aligned}\n% \\end{displaymath}\n% where $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}], i\\in [a]$ is the expected reward regarding the user-arm pair $(u, \\vect{x}_{i, t})$. Given any two users $u, u'\\in \\mathcal{U}$, the function $\\Psi^{(1)}: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ maps from their expected rewards $\\psi^{(1)}(\\cdot)$ to their underlying user exploitation score $w_{i, t}^{(1), *}(u, u')$.\n% \\end{definition}\n\n% % ----------------------------------------\n% Given an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, the user correlation for exploitation measures the user preference (i.e., expected reward) correlation between two users $u, u' \\in \\mathcal{U}$, and the corresponding exploitation score $w_{i, t}^{(1), *}(u, u')$ refer to the edge weight between these two users (nodes) in $\\mathcal{G}_{i, t}^{(1), *}$.\n% Next, before defining the second kind of user correlation, user exploration correlation, we first introduce the definition of expected potential gains for reward estimations inspired by \\cite{EE-Net_ban2021ee}.\n% % ----------------------------------------\n% \\begin{definition}[Expected User Potential Gain]  \\label{def_potential_gain}\n% Given user $u\\in \\mathcal{U}$ at time step $t$, given a candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$ and a reward estimation function $f_{u}(\\cdot)$ corresponded to user $u$, the expected potential gain for the reward estimation $f_{u}(\\vect{x}_{i, t})$ is defined to be $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t})$.\n% \\end{definition}\n\n% With the above definition of the potential gain for reward estimations, we proceed to introduce the second kind of user correlation, i.e., user exploration correlation.\n% % ----------------------------------------\n% \\begin{definition}[Arm-Specific User Correlation for Exploration]  \\label{def_exploration_simi}\n% In round $t$, given any two users $u, u'\\in \\mathcal{U}$ and an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, their underlying exploration correlation score $w_{i, t}^{(2), *}(u, u')$ is\n% \\begin{displaymath}\n% \\begin{aligned}\n%   w_{i, t}^{(2), *}(u, u') = \\Psi^{(2)} \\big( \\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t}),\n%   \\mathbb{E}[r_{i, t}| u',~ \\vect{x}_{i, t}] - f_{u'}(\\vect{x}_{i, t})\\big)\n% \\end{aligned}\n% \\end{displaymath}\n% with $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t}), i\\in [a]$ being the potential gain for the user-arm pair $(u, \\vect{x}_{i, t})$. Here, $ f_{u}(\\cdot )$ is the reward estimation function specified to user $u$, and\n% $\\Psi^{(2)}: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ is the mapping from user potential gains $\\psi^{(2)}(\\cdot)$ to their underlying exploration correlation score.\n% \\end{definition}\n\n% Given the arm $\\vect{x}_{i, t}$ and two users $u, u' \\in \\mathcal{U}$, the user exploration correlation score $w_{i, t}^{(2), *}(u, u')$ refers to the correlation of estimation uncertainty between user-specific reward estimation functions $f_{u}(\\cdot)$ and $f_{u'}(\\cdot)$. The score $w_{i, t}^{(2), *}(u, u')$ is considered as the edge weight between these two nodes (users) $u, u'$ in the true user exploration graph $\\mathcal{G}_{i, t}^{(2), *}$.\n% Then, we will proceed to introduce our proposed solution: \\name\\ framework.\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n"
            },
            "section 3": {
                "name": "GNB: Problem Definition",
                "content": "    \\label{sec_problem_def_notation}\n\n%\nSuppose there are a total of $n$ users with the user set $\\mathcal{U} = \\{1, \\cdots, n\\}$.\n%\nAt each time step $t\\in [T]$, the learner will receive a target user $u_{t} \\in \\mathcal{U}$ to serve, along with candidate arms $\\mathcal{X}_{t} = \\{\\vect{x}_{i, t}\\}_{i\\in [a]}$, $\\abs{\\mathcal{X}_{t}}=a$.\n%The cardinality of this arm set is $\\abs{\\mathcal{X}_{t}} = a$, and \nEach arm is described by a $d$-dimensional context vector $\\vect{x}_{i, t}\\in \\mathbb{R}^{d}$ with $\\norm{\\vect{x}_{i, t}}_{2} = 1$, and $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$ is also associated with a reward $r_{i,t}$. As the user correlation is one important factor in determining the reward, we define the following reward function:  \n\\begin{equation}\n\\begin{split}\n    r_{i, t} =  h(\\vect{x}_{i, t}, u_{t}, \\mathcal{G}^{(1), *}_{i, t}) + \\epsilon_{i, t}\n\\end{split}\n\\label{eq_weighted_reward_func}\n\\end{equation}\nwhere $h(\\cdot)$ is the unknown reward mapping function, and $\\epsilon_{i, t}$ stands for some zero-mean noise such that $\\mathbb{E}[r_{i,t}] =  h(\\vect{x}_{i, t}, u_{t}, \\mathcal{G}^{(1), *}_{i, t})$. \n%\nHere, we have $\\mathcal{G}^{(1), *}_{i, t} = (\\mathcal{U}, E, W^{(1), *}_{i, t})$ being the \\textbf{unknown} user graph induced by arm $\\vect{x}_{i, t}$, which encodes the ``fine-grained'' user correlations in terms of the \\textbf{expected rewards}. In graph $\\mathcal{G}^{(1), *}_{i, t}$, each user $u\\in \\mathcal{U}$ will correspond to a node; meanwhile, $E = \\{e(u, u')\\}_{ u, u' \\in \\mathcal{U}}$ refers to the set of edges, and the set $W^{(1), *}_{i, t} = \\{w^{(1), *}_{i, t}(u, u')\\}_{u, u' \\in \\mathcal{U}}$ stores the weights for each edge from $E$.\n%\nNote that under real-world application scenarios, users sharing the same preference for certain arms (e.g., sports news) may have distinct tastes over other arms (e.g., political news).\nThus, we allow each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$ to induce different user collaborations $\\mathcal{G}^{(1), *}_{i, t}$. \n\n%\nThen, motivated by various real applications (e.g., online recommendation with normalized ratings), we consider $r_{i,t}$ to be bounded $r_{i,t}\\in[0, 1]$, which is standard in existing works (e.g., \\cite{club_2014,CAB_2017,local_clustering-ban2021local,Meta-Ban}). Note that as long as $r_{i,t} \\in [0, 1]$, we do not impose the distribution assumption (e.g., sub-Gaussian distribution) on noise term $\\epsilon_{i,t}$.\n\n\n\n\n%\n\n\\textbf{[Reward Constraint]}\nTo bridge user collaborative effects with user preferences (i.e., rewards), we consider the following constraint for reward function in \\textbf{Eq.} \\ref{eq_weighted_reward_func}. The intuition is that for any two users with comparable user correlations, they will incline to share similar tastes for items.\nFor arm $\\vect{x}_{i, t}$, we consider the difference of expected rewards between any two users $u, u' \\in \\mathcal{U}$ to be governed by\n\\begin{equation}\n\\begin{split}\n    \\big| \\mathbb{E}[r_{i,t} | u, \\vect{x}_{i, t}] - \\mathbb{E}[r_{i,t} | u', \\vect{x}_{i, t}] \\big| \\leq \\Psi \\big(\\mathcal{G}^{(1), *}_{i, t}[u:], \\mathcal{G}^{(1), *}_{i, t}[u':] \\big)\n\\end{split}   \n\\label{eq_assumption_correlation_vector}\n\\end{equation}\nwhere $\\mathcal{G}^{(1), *}_{i, t}[u:]$ represents the normalized adjacency matrix row of $\\mathcal{G}^{(1), *}_{i, t}$ that corresponds to user (node) $u$, and $\\Psi: \\mathbb{R}^{n}\\times\\mathbb{R}^{n} \\mapsto \\mathbb{R}$ \ndenotes an unknown mapping function. \n%\nThe reward function definition (\\textbf{Eq.} \\ref{eq_weighted_reward_func}) and the constraint (\\textbf{Eq.} \\ref{eq_assumption_correlation_vector}) motivate us to design the \\name\\ framework, to be introduced in Section \\ref{sec_proposed_framework}.\n\n%\nThen, we proceed to give the formulation of $\\mathcal{G}^{(1), *}_{i, t} = (\\mathcal{U}, E, W^{(1), *}_{i, t})$ below.\nGiven arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, users with strong correlations tend to have similar expected rewards, which will be reflected by $W^{(1), *}_{i, t}$.\n\n%\n% For both kinds of user graphs, each user from $\\mathcal{U}$ is mapped to a corresponding node in node set $\\mathcal{U}$. With $E = \\{e(u_{i}, u_{j})\\}_{\\forall u_{i}, u_{j} \\in \\mathcal{U}}$ being the set of edges, we have $W^{(1), *}_{i, t}, W^{(2), *}_{i, t}$ to respectively represent the set of edge weights for $\\mathcal{G}^{(1), *}_{i, t}, \\mathcal{G}^{(2), *}_{i, t}$. Here, the estimated user (exploitation / exploration) correlations are modeled by the edge weights of node (user) pairs. \n% Next, we proceed to give the definitions of two arm-specific user correlations, which are encoded by $\\mathcal{G}^{(1), *}_{i, t}, \\mathcal{G}^{(2), *}_{i, t}$ respectively.\n\n% ====================================================\n\\begin{definition}[User Correlation for Exploitation]  \\label{def_exploitation_simi}\nIn round $t$, for any two users $u, u'\\in \\mathcal{U}$, their exploitation correlation score $w_{i, t}^{(1), *}(u, u')$ w.r.t. a candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$ is defined as\n\\begin{displaymath}\n\\begin{aligned}\n    w_{i, t}^{(1), *}(u, u') = \\Psi^{(1)} \\big( \\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}],~ \\mathbb{E}[r_{i, t}| u',~ \\vect{x}_{i, t}] \\big)\n\\end{aligned}\n\\end{displaymath}\nwhere $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}], i\\in [a]$ is the expected reward in terms of the user-arm pair $(u, \\vect{x}_{i, t})$. Given two users $u, u'\\in \\mathcal{U}$, the function $\\Psi^{(1)}: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ maps from their expected rewards $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}]$ to their user exploitation score $w_{i, t}^{(1), *}(u, u')$.\n\\end{definition}\n\n% ----------------------------------------\n%Given an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, the user correlation for exploitation measures the user preference (i.e., expected reward) correlation between two users $u, u' \\in \\mathcal{U}$, and the corresponding exploitation score $w_{i, t}^{(1), *}(u, u')$ refers to the edge weight between these two users (nodes) $u, u'$ in exploitation graph $\\mathcal{G}_{i, t}^{(1), *}$.\n\nThe edge weight $w_{i, t}^{(1), *}(u, u')$ measures the correlation between the two users' preferences. \nWhen $w_{i, t}^{(1), *}(u, u')$ is large, $u$ and $u'$ tend to have the same taste; Otherwise, these two users' preferences will be different in expectation.\nIn this paper, we consider the mapping functions $\\Psi^{(1)}$ as the prior knowledge. For example,   $\\Psi^{(1)}$ can be the radial basis function (RBF) kernel or normalized absolute difference.\n\n\n\n\\textbf{[Modeling with User Exploration Graph $\\mathcal{G}^{(2), *}_{i, t}$]}\nUnfortunately, $\\mathcal{G}^{(1), *}_{i, t}$ is the \\textbf{unknown} prior knowledge in our problem setting. \nThus, the learner has to estimate $\\mathcal{G}^{(1), *}_{i, t}$ by exploiting the current knowledge, denoted by $\\mathcal{G}^{(1)}_{i, t} =  (\\mathcal{U}, E, W^{(1)}_{i, t})$, \nwhere $W^{(1)}_{i, t} = \\{w^{(1)}_{i, t}(u, u')\\}_{u, u' \\in \\mathcal{U}}$  is the estimation of $W^{(1), *}_{i, t}$ based on the function class $\\mathcal{F}= \\{ f_u^{(1)} \\}_{u \\in \\mathcal{U}}$ where $f_u^{(1)}$ is the hypothesis specified to user $u$.\nHowever, greedily exploiting $\\mathcal{G}^{(1)}_{i, t}$ may lead to the sub-optimal solution, or overlook some correlations that may only be revealed in the future rounds.\nThus, we propose to construct another \\textbf{user exploration graph} $\\mathcal{G}^{(2), *}_{i, t}$ for principled exploration to measure the estimation gap $\\mathcal{G}^{(1), *}_{i, t} - \\mathcal{G}^{(1)}_{i, t}$, which refers to the uncertainty of the estimation of graph $\\mathcal{G}^{(1)}_{i, t}$.\n\n%\nFor each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, we formulate the user exploration graph $\\mathcal{G}^{(2), *}_{i, t} = (\\mathcal{U}, E, W^{(2), *}_{i, t})$,\nwith the set of edge weights $W^{(2), *}_{i, t} = \\{w^{(2), *}_{i, t}(u, u')\\}_{\\forall u, u' \\in \\mathcal{U}}$.\nHere, $\\mathcal{G}^{(2), *}_{i, t}$ models the uncertainty of estimation $\\mathcal{G}^{(1)}_{i, t}$ in terms of the true exploitation graph $\\mathcal{G}^{(1), \\ast}_{i, t}$, and $\\mathcal{G}^{(2), *}_{i, t}$ can be thought as the oracle exploration graph, i.e., \"perfect exploration\".\n%\nThen, with the aforementioned hypothesis $f_{u}^{(1)}(\\vect{x}_{i, t})$ for estimating the expected reward of arm $\\vect{x}_{i, t}$ given $u$, we introduce the formulation of $\\mathcal{G}^{(2), *}_{i, t}$ as the user exploration correlation.\n\n\n\n% ----------------------------------------\n\\begin{definition}[User Correlation for Exploration]  \\label{def_exploration_simi}\nIn round $t$, given two users $u, u'\\in \\mathcal{U}$ and an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, their underlying exploration correlation score is defined as \n% $w_{i, t}^{(2), *}(u, u')$ \n\\begin{displaymath}\n\\begin{aligned}\n  w_{i, t}^{(2), *}&(u, u') \\\\\n   & = \\Psi^{(2)} \\bigg( \\mathbb{E}[r_{i, t}| u, \\vect{x}_{i, t}] - f_{u}^{(1)}(\\vect{x}_{i, t}), \\mathbb{E}[r_{i, t}| u', \\vect{x}_{i, t}] - f_{u'}(\\vect{x}_{i, t})\\bigg)\n\\end{aligned}\n\\end{displaymath}\nwith $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}^{(1)}(\\vect{x}_{i, t}), i\\in [a]$ being the potential gain of the estimation $f_{u}^{(1)}(\\vect{x}_{i, t})$ for the user-arm pair $(u, \\vect{x}_{i, t})$. Here, $ f_{u}^{(1)}(\\cdot )$ is the reward estimation function specified to user $u$, and\n$\\Psi^{(2)}: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ is the mapping from user potential gains $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}^{(1)}(\\vect{x}_{i, t})$ to their exploration correlation score.\n\\end{definition}\nHere, $w_{i, t}^{(2), *}(u, u')$ is defined based on the potential gain of $f_{u}^{(1)}(\\cdot)$, i.e., $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}^{(1)}(\\vect{x}_{i, t})$, to measure the estimation uncertainty. Note that our formulation is distinct from the formulation in \\cite{EE-Net_ban2021ee}, where they only focus on the single-bandit setting with no user collaborations, and all the users will be treated identically.\n\n\nAs we have discussed, $w_{i, t}^{(2), *}(u, u')$ measures the uncertainty of estimation $w_{i, t}^{(1)}(u, u')$. When $w_{i, t}^{(2), *}(u, u')$ is large, the uncertainty of estimated exploitation correlation, i.e., $w_{i, t}^{(1)}(u, u')$, will also be large, and we should explore them more. Otherwise, we have enough confidence towards $w_{i, t}^{(1)}(u, u')$, and we can exploit $w_{i, t}^{(1)}(u, u')$ in a secure way.\nAnalogous to $\\Psi^{(1)}$ in \\textbf{Def.} \\ref{def_exploitation_simi}, we consider the mapping function $\\Psi^{(2)}$ as the known prior knowledge.\n\n\n% -------------\n\n% -------------\n\\textbf{[Learning Objective]}\nWith the received user $u_{t}$ in each round $t\\in [T]$, the learner is expected to recommend an arm $x_{t}\\in \\mathcal{X}_{t}$ (with reward $r_{t}$) in order to minimize the cumulative pseudo-regret\n\\begin{equation}\n\\begin{split}\n    R(T) = \\mathbb{E}[\\sum_{t=1}^{T}(r_{t}^{*} - r_{t})] \n\\end{split}\n\\label{eq_pseudo_regret}\n\\end{equation}\nwhere we have $r_{t}^{*}$ being the reward for the optimal arm, such that \n$ \\mathbb{E}[r_{t}^{*}| u_t, \\mathcal{X}_{t}] = \\max_{\\vect{x}_{i,t}\\in \\mathcal{X}_{t}} h(\\vect{x}_{i, t}, u_{t}, \\mathcal{G}^{(1), *}_{i, t})$. \n\n\n\n\n\n\n% ------------------------\n\\textbf{[Comparing with Existing Problem Definitions]}\nThe problem definition of existing user clustering works (e.g., \\cite{club_2014,SCLUB_li2019improved,CAB_2017,local_clustering-ban2021local,Meta-Ban}) only formulates \"coarse-grained\" user correlations. In their settings, for a user group $\\mathcal{N} \\subseteq \\mathcal{U}$ with the mapping function $h_{\\mathcal{N}}$, all the users in $\\mathcal{N}$ are forced to share the same reward mapping given an arm $\\vect{x}_{i, t}$, i.e., $\\mathbb{E}[r_{i,t}\\mid u, \\vect{x}_{i,t}]= h_{\\mathcal{N}}(\\vect{x}_{i,t}), \\forall u \\in \\mathcal{N}$. In contrast, our definition of the reward function enables us to model the pair-wise ``fine-grained'' user correlations by introducing another two important factors $u$ and $\\mathcal{G}^{(1), *}_{i, t}$. With our formulation, each user here is allowed to produce different rewards facing the same arm, i.e.,  $\\mathbb{E}[r_{i,t}\\mid u, \\vect{x}_{i,t}]= h(\\vect{x}_{i,t}, u, \\mathcal{G}^{(1), *}_{i, t}), \\forall u \\in \\mathcal{N}$. Here, with different users $u$, the corresponding expected reward $h(\\vect{x}_{i,t}, u, \\mathcal{G}^{(1), *}_{i, t})$ can be different. \n%ur definition of the reward function enables us to model the pair-wise fine-grained user correlations instead of imposing rigid user groups, where users within the same group are forced to share the identical preference. \n%\n%Note that compared with previous linear methods (e.g., \\cite{GangOfBandits-cesa2013gang,colla_environ_2016}) which simply aggregate the expected rewards and assume user correlations are \\textbf{known} to the learner, our assumption is more generic and can fit into non-linear settings. \nTherefore, our definition of the reward function is more generic, and it can also readily generalize to existing user clustering algorithms (with ``coarse-grained'' user correlations) by allowing each single user group to form an isolated sub-graph in $\\mathcal{G}^{(1), *}_{i, t}$ with no connections across different sub-graphs (i.e., user groups).\n\n\n% ------------------------------------------------------------\n\n\n\n\n\n% ---\n\\textbf{[Notation]}\nUp to round $t$, denoting $\\mathcal{T}_{u, t} \\subseteq [t]$ as the collection of time steps at which user $u\\in \\mathcal{U}$ has been served, we use $\\mathcal{P}_{u, t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in \\mathcal{T}_{u, t}}$ to represent the collection of received arm-reward pairs associated with user $u$, and $T_{u, t} = \\abs{\\mathcal{T}_{u, t}}$ refers to the corresponding number of rounds.\nHere, $\\vect{x}_{\\tau} \\in \\mathcal{X}_{\\tau}, r_{\\tau} \\in [0, 1]$ separately refer to the chosen arm and actual received reward in round $\\tau\\in [t]$.\nSimilarly, we use $\\mathcal{P}_{t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in [t]}$ to denote all the past records (i.e., arm-reward pairs), up to round $t$. \n%\nFor a graph $\\mathcal{G}$, we let $\\matr{A} \\in \\mathbb{R}^{n \\times n}$ be its adjacency matrix (with self-loops), and $\\matr{D} \\in \\mathbb{R}^{n \\times n}$ refers to the corresponding degree matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% =====================================================================================================================\n% =====================================================================================================================\n% =====================================================================================================================\n% =====================================================================================================================\n% BACKUP\n\n\n% % \\section{Graph Neural Bandits: Problem Definition and Notation}    \\label{sec_problem_def_notation}\n\n% % Suppose there are a total of $n$ users with the user set $\\mathcal{U} = \\{1, \\cdots, n\\}$. At each time step $t\\in [T]$, the learner will receive a user $u_{t} \\in \\mathcal{U}$ to serve. \n% % Then, as the arm pool is not fixed, we use $\\mathcal{X}_{t} = \\{\\vect{x}_{i, t}\\}_{i\\in [a]}$ to denote the set of candidate arms for recommendation in round $t$.\n% % The volume of this arm set is $\\abs{\\mathcal{X}_{t}} = a$, and each arm is described by a $d$-dimensional context vector $\\vect{x}_{i, t}\\in \\mathbb{R}^{d}$. Meanwhile, each arm $\\vect{x}_{i, t}$ will also be related to a reward defined by\n% % \\begin{equation}\n% % \\begin{split}\n% %     r_{i, t} =  h(\\vect{x}_{i, t}, u_{t}, \\matr{\\Lambda}^{*}_{i, t}) + \\epsilon_{i, t}\n% % \\end{split}\n% % \\label{eq_weighted_reward_func}\n% % \\end{equation}\n\n% % where $h(\\cdot)$ is the unknown reward mapping function, and $\\epsilon_{i, t}$ stands for the zero-mean noise drawn from a certain distribution with the bounded value range (e.g., truncated Gaussian distribution). \n% % Here, $\\matr{\\Lambda}^{*}_{i, t} \\in \\mathbb{R}^{n\\times n}$ is the unknown user affinity matrix that encodes the user correlations w.r.t. the arm $\\vect{x}_{i, t}$, and $\\matr{\\Lambda}^{*}_{i, t}[u, :]$ is the row of $\\matr{\\Lambda}^{*}_{i, t}$ that relates to the user $u\\in \\mathcal{U}$.\n\n\n\n% % % ------------------------------------------------------------\n% % Then, given the same arm $\\vect{x}_{i, t}$, we consider the difference of expected rewards between any two users $u, u' \\in \\mathcal{U}$ is bounded by\n% % \\begin{equation}\n% % \\begin{split}\n% %     \\abs{h(\\vect{x}_{i, t}, u, \\matr{\\Lambda}^{*}_{i, t}) - h(\\vect{x}_{i, t}, u', \\matr{\\Lambda}^{*}_{i, t})} \\leq \\Psi (\\matr{\\Lambda}^{*}_{i, t}[u, :], \\matr{\\Lambda}^{*}_{i, t}[u', :])\n% % \\end{split}   \n% % \\label{eq_assumption_correlation_vector}\n% % \\end{equation}\n% % where $\\matr{\\Lambda}^{*}_{i, t}[u, :]$ is the user correlation vector (i.e., the corresponding row in $\\matr{\\Lambda}^{*}_{i, t}$) of user $u$, and $\\Psi: \\mathbb{R}^{n}\\times\\mathbb{R}^{n} \\mapsto \\mathbb{R}$ \n% % denotes an unknown mapping from two user correlation vectors to the difference of the expected rewards.\n% % With \\textbf{Eq.} \\ref{eq_weighted_reward_func} being the universal reward function that formulates the mapping from any user $u\\in \\mathcal{U}$ to the expected reward, \n\n% % \\TODO{Discuss the user models}\n\n% % Note that compared with previous linear methods (e.g., \\cite{GangOfBandits-cesa2013gang,colla_environ_2016}) which simply aggregate expected rewards, our assumption is more generic and could be fit into non-linear settings. \n% % Meanwhile, our definition of the reward function can be easily generalized to existing user clustering algorithms (e.g., \\cite{club_2014,CAB_2017,local_clustering-ban2021local}) by allowing $\\matr{\\Lambda}^{*}_{i, t}$ to be a block matrix where each block corresponds to a single user group.\n\n\n% % % -------------\n% % Analogous to existing works (e.g., \\cite{club_2014,CAB_2017,local_clustering-ban2021local,Meta-Ban}), we assume bounded rewards $r_{i, t} \\in [0, 1]$.\n% % %\n% % Then, for the received user $u_{t}$ at each time step $t$, the learner is expected to recommend an arm $x_{t}\\in \\mathcal{X}_{t}$ (with reward $r_{t}$) in order to minimize the cumulative pseudo-regret\n% % $\n% %     R(T) = \\mathbb{E}[\\sum_{t=1}^{T}(r_{t}^{*} - r_{t})] \n% % $\n% % where $r_{t}^{*}$ is the reward for the optimal arm $ \\mathbb{E}[r_{t}^{*}|\\mathcal{X}_{t}, u_t] = \\max_{\\vect{x}_{i,t}\\in \\mathcal{X}_{t}} h(\\vect{x}_{i, t}, u_{t}, \\matr{\\Lambda}^{*}_{i, t})$. \n\n% % With $\\mathcal{T}_{u, t}$ being the collection of past time steps that user $u\\in \\mathcal{U}$ is served up to round $t$, we use $\\mathcal{P}_{u, t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in \\mathcal{T}_{u, t}}$ to represent the collection of received arm-reward pairs associated with user $u$, and apply $T_{u, t} = \\abs{\\mathcal{T}_{u, t}}$ to denote the number of rounds that user $u$ has been served. \n% % Analogously, we use $\\mathcal{P}_{t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in [t]}$ to denote all the past records (i.e., arm-reward pairs), up to round $t$.\n\n\n% % % --------------\n% % \\subsection{User Exploitation Graphs and User Exploration Graphs}   \\label{subsec_true_user_graphs}\n\n% % As the user collaborative effects (encoded by $\\matr{\\Lambda}^{*}_{i, t}$ from \\textbf{Eq.} \\ref{eq_weighted_reward_func}) are unknown, we formulate user graphs to represent and exploit the knowledge encoded in $\\matr{\\Lambda}^{*}_{i, t}$.\n\n% % % and $\\mathcal{G}^{(2), *}_{i, t}$ to denote the residual (i.e., the potential gain) between $\\mathcal{G}^{(1), *}_{i, t}$ and the estimator output for exploration. \n\n% % In order to model the user correlations and deal with the exploration-exploitation dilemma, for each candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, we construct two user correlation graphs: a user exploitation graph $\\mathcal{G}^{(1), *}_{i, t} = (\\mathcal{U}, E, W^{(1), *}_{i, t})$ and a user exploration graph $\\mathcal{G}^{(2), *}_{i, t} = (\\mathcal{U}, E, W^{(2), *}_{i, t})$. \n% % Here, the user exploitation graph $\\mathcal{G}^{(1), *}_{i, t}$ encodes the collaborative effects in terms of user preferences towards arm $\\vect{x}_{i, t}$, and the user exploration graph $\\mathcal{G}^{(2), *}_{i, t}$ models the user correlation in terms of the uncertainty of reward estimations.\n% % %\n\n% % Here, each user from $\\mathcal{U}$ is mapped to a corresponding node in node set $V$. With $E = \\{e(c_{i}, c_{j})\\}_{\\forall c_{i}, c_{j} \\in \\mathcal{X}}$ being the set of edges, we have $W^{(1), *}_{i, t}, W^{(2), *}_{i, t}$ to represent the set of edge weights. Note that by definition, $\\mathcal{G}^{(1), *}_{t}, \\mathcal{G}^{(2), *}_{t}$ will stay as fully-connected graphs, and the estimated user (exploitation / exploration) correlations are modeled by the edge weights of node (user) pairs. \n% % % For a given node $v \\in V$, we denote the augmented $k$-hop neighborhood $\\widetilde{\\mathcal{N}}_{k}(v) := \\mathcal{N}_{k}(v) \\cup \\{v\\}$ as the union node set of its $k$-hop neighborhood $\\mathcal{N}_{k}(v)$ and node $v$ itself. \n% % Given a graph $\\mathcal{G}$, we denote $\\matr{A} \\in \\mathbb{R}^{n \\times n}$ as its adjacency matrix (with added self-loops), and $\\matr{D} \\in \\mathbb{R}^{n \\times n}$ as its degree matrix.\n% % Then, we proceed to give the definition of arm-specific user correlations encoded by $\\mathcal{G}^{(1), *}, \\mathcal{G}^{(2), *}$ respectively.\n\n% % % ====================================================\n% % \\begin{definition}[Arm-Specific User Correlation for Exploitation]  \\label{def_exploitation_simi}\n% % In round $t$, for any two users $u, u'\\in \\mathcal{U}$, their exploitation correlation score $w_{i, t}^{(1), *}(u, u')$ w.r.t. a candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$ is defined as\n% % \\begin{displaymath}\n% % \\begin{aligned}\n% %     w_{i, t}^{(1), *}(u, u') = \\Psi^{(1)} \\big( \\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}], \\mathbb{E}[r_{i, t}| u',~ \\vect{x}_{i, t}]\\big)\n% % \\end{aligned}\n% % \\end{displaymath}\n% % where $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}], i\\in [a]$ is the expected reward regarding the user-arm pair $(u, \\vect{x}_{i, t})$. Given any two users $u, u'\\in \\mathcal{U}$, the function $\\Psi^{(1)}: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ maps from their expected rewards $\\psi^{(1)}(\\cdot)$ to their underlying user exploitation score $w_{i, t}^{(1), *}(u, u')$.\n% % \\end{definition}\n\n% % % ----------------------------------------\n% % Given an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, the user correlation for exploitation measures the user preference (i.e., expected reward) correlation between two users $u, u' \\in \\mathcal{U}$, and the corresponding exploitation score $w_{i, t}^{(1), *}(u, u')$ refer to the edge weight between these two users (nodes) in $\\mathcal{G}_{i, t}^{(1), *}$.\n% % Next, before defining the second kind of user correlation, user exploration correlation, we first introduce the definition of expected potential gains for reward estimations inspired by \\cite{EE-Net_ban2021ee}.\n% % % ----------------------------------------\n% % \\begin{definition}[Expected User Potential Gain]  \\label{def_potential_gain}\n% % Given user $u\\in \\mathcal{U}$ at time step $t$, given a candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$ and a reward estimation function $f_{u}(\\cdot)$ corresponded to user $u$, the expected potential gain for the reward estimation $f_{u}(\\vect{x}_{i, t})$ is defined to be $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t})$.\n% % \\end{definition}\n\n% % With the above definition of the potential gain for reward estimations, we proceed to introduce the second kind of user correlation, i.e., user exploration correlation.\n% % % ----------------------------------------\n% % \\begin{definition}[Arm-Specific User Correlation for Exploration]  \\label{def_exploration_simi}\n% % In round $t$, given any two users $u, u'\\in \\mathcal{U}$ and an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, their underlying exploration correlation score $w_{i, t}^{(2), *}(u, u')$ is\n% % \\begin{displaymath}\n% % \\begin{aligned}\n% %   w_{i, t}^{(2), *}(u, u') = \\Psi^{(2)} \\big( \\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t}),\n% %   \\mathbb{E}[r_{i, t}| u',~ \\vect{x}_{i, t}] - f_{u'}(\\vect{x}_{i, t})\\big)\n% % \\end{aligned}\n% % \\end{displaymath}\n% % with $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t}), i\\in [a]$ being the potential gain for the user-arm pair $(u, \\vect{x}_{i, t})$. Here, $ f_{u}(\\cdot )$ is the reward estimation function specified to user $u$, and\n% % $\\Psi^{(2)}: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ is the mapping from user potential gains $\\psi^{(2)}(\\cdot)$ to their underlying exploration correlation score.\n% % \\end{definition}\n\n% % Given the arm $\\vect{x}_{i, t}$ and two users $u, u' \\in \\mathcal{U}$, the user exploration correlation score $w_{i, t}^{(2), *}(u, u')$ refers to the correlation of estimation uncertainty between user-specific reward estimation functions $f_{u}(\\cdot)$ and $f_{u'}(\\cdot)$. The score $w_{i, t}^{(2), *}(u, u')$ is considered as the edge weight between these two nodes (users) $u, u'$ in the true user exploration graph $\\mathcal{G}_{i, t}^{(2), *}$.\n% % Then, we will proceed to introduce our proposed solution: \\name\\ framework.\n\n\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n% \\vspace{-0.2cm}\n% \\section{Graph Neural Bandits: Problem Definition and Notation}    \\label{sec_problem_def_notation}\n% \\vspace{-0.1cm}\n\n% %\n% Suppose there are a total of $n$ users with the user set $\\mathcal{U} = \\{1, \\cdots, n\\}$. At each time step $t\\in [T]$, the learner will receive a user $u_{t} \\in \\mathcal{U}$ to serve. \n% Then, as the arm pool is not fixed, we use $\\mathcal{X}_{t} = \\{\\vect{x}_{i, t}\\}_{i\\in [a]}$ to denote the set of candidate arms for recommendation in round $t$.\n% The cardinality of this arm set is $\\abs{\\mathcal{X}_{t}} = a$, and each arm is described by a $d$-dimensional context vector $\\vect{x}_{i, t}\\in \\mathbb{R}^{d}$ with $\\norm{\\vect{x}_{i, t}}_{2} = 1$. \n% Meanwhile, each arm $\\vect{x}_{i, t}$ is associated with a reward $r_{i,t}$. As the user correlation is one important factor in determining the reward, we define the following reward function:  \n% %will also be related to a reward defined by\n% \\begin{equation}\n% \\begin{split}\n%     r_{i, t} =  h(\\vect{x}_{i, t}, u_{t}, \\matr{\\Lambda}^{*}_{i, t}) + \\epsilon_{i, t}\n% \\end{split}\n% \\label{eq_weighted_reward_func}\n% \\end{equation}\n% where $h(\\cdot)$ is the unknown reward mapping function, and $\\epsilon_{i, t}$ stands for some zero-mean noise such that $\\mathbb{E}[r_{i,t}] =  h(\\vect{x}_{i, t}, u_{t}, \\matr{\\Lambda}^{*}_{i, t})$. \n% %\n% Motivated by various real applications (e.g., online recommendation with normalized ratings), we consider $r_{i,t}$ to be bounded $r_{i,t}\\in[0, 1]$ in this paper, which is standard in existing works (e.g., \\cite{club_2014,CAB_2017,local_clustering-ban2021local,Meta-Ban}). Note that as long as $r_{i,t} \\in [0, 1]$, we do not need any distribution assumption (e.g., sub-Gaussian) on noise $\\epsilon_{i,t}$.\n\n% %\n% Here, the \\textbf{unknown} user affinity matrix $\\matr{\\Lambda}^{*}_{i, t} \\in \\mathbb{R}^{n\\times n}$ encodes the user correlations w.r.t. the arm $\\vect{x}_{i, t}$. \n% %\n% Under real-world application scenarios, the users sharing the same preference for specific arms (e.g., sports news) may have different tastes over other arms (e.g., political news).\n% Therefore, inspired by this phenomenon, we allow each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$ to induce different user collaborations $\\matr{\\Lambda}^{*}_{i, t}$. \n% %\n% %Intuitively, for a specific user-arm pair $(u, \\vect{x}_{i, t}), u\\in \\mathcal{U}, \\vect{x}_{i, t}\\in \\mathcal{X}_{t}$, we can consider the expected reward (i.e., user feedback) $\\mathbb{E}[r_{i,t} | u, \\vect{x}_{i, t}]$ to be generated by a composite function of (1) the unknown reward mapping function $h(\\cdot)$; (2) the unknown mapping from arm $\\vect{x}_{i, t}$ to its true user affinity matrix $\\matr{\\Lambda}_{i, t}^{*}$. \n% %\n% %Moreover, motivated by \\cite{AGG-UCB_qi2022neural}, we can also readily adapt our formulation with the arm group information by modeling the user correlations for each arm group instead. For generality, we will focus on the arm-specific user correlations in this paper.\n% % ------------------------\n% \\textbf{Comparison with Existing Problem Definitions.}\n% The problem definition of existing user clustering works (e.g., \\cite{club_2014,SCLUB_li2019improved,CAB_2017,local_clustering-ban2021local,Meta-Ban}) only can formulate \"coarse-grained\" user correlations. In their settings, given a user group $\\mathcal{N} \\subseteq \\mathcal{U}$, all the users in $\\mathcal{N}$ are forced to share the same reward function given an arm $\\vect{x}_{i, t}$, i.e., $\\mathbb{E}[r_{i,t}\\mid u, \\vect{x}_{i,t}]= h_{\\mathcal{N}}(\\vect{x}_{i,t}), \\forall u \\in \\mathcal{N}$. In contrast, our definition of the reward function enables us to model the pair-wise fine-grained user correlations by introducing another two important factors $u$ and $\\matr{\\Lambda}^{*}_{i, t}$. With our formulation, each user here is allowed to produce different rewards facing the same arm, i.e.,  $\\mathbb{E}[r_{i,t}\\mid u, \\vect{x}_{i,t}]= h(\\vect{x}_{i,t}, u, \\matr{\\Lambda}^{*}_{i, t}), \\forall u \\in \\mathcal{N}$. Here, with different users $u$, the corresponding expected reward $h(\\vect{x}_{i,t}, u, \\matr{\\Lambda}^{*}_{i, t})$ can be different. \n% %ur definition of the reward function enables us to model the pair-wise fine-grained user correlations instead of imposing rigid user groups, where users within the same group are forced to share the identical preference. \n% %\n% %Note that compared with previous linear methods (e.g., \\cite{GangOfBandits-cesa2013gang,colla_environ_2016}) which simply aggregate the expected rewards and assume user correlations are \\textbf{known} to the learner, our assumption is more generic and can fit into non-linear settings. \n% Therefore, our definition of the reward function is more generic, and it can also readily generalize to above user clustering algorithms (with ``coarse-grained'' user correlations), by allowing the affinity matrix $\\matr{\\Lambda}^{*}_{i, t}$ to be a block matrix where each block corresponds to a single user group.\n\n\n% % ------------------------------------------------------------\n% To bridge user collaborative effects with user preferences (rewards), we consider the following constrain for the reward function in \\textbf{Eq.} \\ref{eq_weighted_reward_func}. The intuition is that for any two users with comparable user correlations, they would share similar tastes over the items with a high probability.\n% For arm $\\vect{x}_{i, t}$, we consider the difference of expected rewards between any two users $u, u' \\in \\mathcal{U}$ to be governed by\n% \\begin{equation}\n% \\begin{split}\n%     \\abs{h(\\vect{x}_{i, t}, u, \\matr{\\Lambda}^{*}_{i, t}) - h(\\vect{x}_{i, t}, u', \\matr{\\Lambda}^{*}_{i, t})} \\leq \\Psi (\\matr{\\Lambda}^{*}_{i, t}[u, :], \\matr{\\Lambda}^{*}_{i, t}[u', :])\n% \\end{split}   \n% \\label{eq_assumption_correlation_vector}\n% \\end{equation}\n% where $\\matr{\\Lambda}^{*}_{i, t}[u, :]$ is the user correlation vector (i.e., the corresponding row in $\\matr{\\Lambda}^{*}_{i, t}$) of user $u$, and $\\Psi: \\mathbb{R}^{n}\\times\\mathbb{R}^{n} \\mapsto \\mathbb{R}$ \n% denotes an unknown mapping function. \n% %\n% The reward function definition and the constraint (\\textbf{Eq.} \\ref{eq_weighted_reward_func}-\\ref{eq_assumption_correlation_vector}) motivate us to design the \\name\\ framework, to be introduced in Section \\ref{sec_proposed_framework}.\n\n\n\n\n\n% % =====================================================\n% \\textbf{Modeling User Correlations with User Graphs.}\n% % As the user collaborative effects (encoded by $\\matr{\\Lambda}^{*}_{i, t}$ from \\textbf{Eq.} \\ref{eq_weighted_reward_func}) are unknown for the learner, we formulate user graphs to represent and exploit the knowledge in $\\matr{\\Lambda}^{*}_{i, t}$.\n% %\n% In order to model the unknown user correlations ($\\matr{\\Lambda}^{*}_{i, t}$ from \\textbf{Eq.} \\ref{eq_weighted_reward_func}) and deal with the exploration-exploitation dilemma, for each candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, we propose to formulate two user correlation graphs: a user exploitation graph $\\mathcal{G}^{(1), *}_{i, t} = (V, E, W^{(1), *}_{i, t})$ and a user exploration graph $\\mathcal{G}^{(2), *}_{i, t} = (V, E, W^{(2), *}_{i, t})$. \n% %\n% The defined arm-specific user graphs correspond to our formulation in \\textbf{Eq.} \\ref{eq_weighted_reward_func} where each arm can induce different user collaboration effects.\n% %\n% Here, the user exploitation graph $\\mathcal{G}^{(1), *}_{i, t}$ encodes the collaborative effects in terms of user preferences towards arm $\\vect{x}_{i, t}$, which makes effective use of the information in $\\matr{\\Lambda}^{*}_{i, t}$ (exploitation). \n% Then, we formulate the user exploration graph $\\mathcal{G}^{(2), *}_{i, t}$ to model the user correlation regarding the uncertainty of reward estimation (exploration) from the reward prediction model.\n\n% %\n% For both kinds of user graphs, each user from $\\mathcal{U}$ is mapped to a corresponding node in node set $V$. With $E = \\{e(u_{i}, u_{j})\\}_{\\forall u_{i}, u_{j} \\in \\mathcal{U}}$ being the set of edges, we have $W^{(1), *}_{i, t}, W^{(2), *}_{i, t}$ to respectively represent the set of edge weights for $\\mathcal{G}^{(1), *}_{i, t}, \\mathcal{G}^{(2), *}_{i, t}$. Here, the estimated user (exploitation / exploration) correlations are modeled by the edge weights of node (user) pairs. \n% Next, we proceed to give the definitions of two arm-specific user correlations, which are encoded by $\\mathcal{G}^{(1), *}_{i, t}, \\mathcal{G}^{(2), *}_{i, t}$ respectively.\n\n% % ====================================================\n% \\begin{definition}[User Correlation for Exploitation]  \\label{def_exploitation_simi}\n% In round $t$, for any two users $u, u'\\in \\mathcal{U}$, their exploitation correlation score $w_{i, t}^{(1), *}(u, u')$ w.r.t. a candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$ is defined as\n% \\begin{displaymath}\n% \\begin{aligned}\n%     w_{i, t}^{(1), *}(u, u') = \\Psi^{(1)} \\big( \\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}],~ \\mathbb{E}[r_{i, t}| u',~ \\vect{x}_{i, t}]\\big)\n% \\end{aligned}\n% \\end{displaymath}\n% where $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}], i\\in [a]$ is the expected reward in terms of the user-arm pair $(u, \\vect{x}_{i, t})$. Given two users $u, u'\\in \\mathcal{U}$, the function $\\Psi^{(1)}: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ maps from their expected rewards $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}]$ to their user exploitation score $w_{i, t}^{(1), *}(u, u')$.\n% \\end{definition}\n\n% % ----------------------------------------\n% Given an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, the user correlation for exploitation measures the user preference (i.e., expected reward) correlation between two users $u, u' \\in \\mathcal{U}$, and the corresponding exploitation score $w_{i, t}^{(1), *}(u, u')$ refers to the edge weight between these two users (nodes) $u, u'$ in exploitation graph $\\mathcal{G}_{i, t}^{(1), *}$.\n% In this paper, we consider the mapping functions $\\Psi^{(1)}$ as the prior knowledge, which can be functions such as the radial basis function (RBF) kernel or normalized absolute difference in practice.\n% Inspired by \\cite{EE-Net_ban2021ee}, before defining the second kind of user correlation (i.e., user exploration correlation), we first introduce the definition of expected potential gain for reward estimation, which measures the prediction uncertainty of reward estimators.\n% % ----------------------------------------\n% \\begin{definition}[Expected Potential Gain]  \\label{def_potential_gain}\n% Given user $u\\in \\mathcal{U}$ at time step $t$, given a candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$ and a reward estimation function $f_{u}(\\cdot)$ corresponding to user $u$, the expected potential gain for the reward estimation $f_{u}(\\vect{x}_{i, t})$ is defined as $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t})$.\n% \\end{definition}\n\n% Here, the potential gain for reward estimation essentially formulates the uncertainty of model $f_{u}(\\cdot)$ by measuring the difference between the expected reward $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}]$ and the prediction $f_{u}(\\vect{x}_{i, t})$.\n% Next, we proceed to introduce the second kind of user correlation, i.e., user exploration correlation.\n% % ----------------------------------------\n% \\begin{definition}[User Correlation for Exploration]  \\label{def_exploration_simi}\n% In round $t$, given two users $u, u'\\in \\mathcal{U}$ and an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, their underlying exploration correlation score $w_{i, t}^{(2), *}(u, u')$ is\n% \\begin{displaymath}\n% \\begin{aligned}\n%   w_{i, t}^{(2), *}(u, u') = \\Psi^{(2)} \\big( \\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t}), ~~\n%   \\mathbb{E}[r_{i, t}| u',~ \\vect{x}_{i, t}] - f_{u'}(\\vect{x}_{i, t})\\big)\n% \\end{aligned}\n% \\end{displaymath}\n% with $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t}), i\\in [a]$ being the potential gain for the user-arm pair $(u, \\vect{x}_{i, t})$. Here, $ f_{u}(\\cdot )$ is the reward estimation function specified to user $u$, and\n% $\\Psi^{(2)}: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ is the mapping from user potential gains $\\mathbb{E}[r_{i, t}| u,~ \\vect{x}_{i, t}] - f_{u}(\\vect{x}_{i, t})$ to their exploration correlation score.\n% \\end{definition}\n\n% For the arm $\\vect{x}_{i, t}$ and two users $u, u' \\in \\mathcal{U}$, the user exploration correlation score $w_{i, t}^{(2), *}(u, u')$ refers to the correlation of prediction uncertainty between two user-specific functions $f_{u}(\\cdot)$ and $f_{u'}(\\cdot)$. Then, the exploration score $w_{i, t}^{(2), *}(u, u')$ will be considered as the edge weight between these two nodes (users) $u, u'$ in the true user exploration graph $\\mathcal{G}_{i, t}^{(2), *}$. \n% Analogous to previous $\\Psi^{(1)}$, we also consider the mapping functions $\\Psi^{(2)}$ is given as the prior knowledge.\n% %\n% Intuitively, when the exploration score $w_{i, t}^{(2), *}(u, u')$ is high, we can apply similar exploration strategies for both users $u, u'$. For example, given arm $\\vect{x}_{i, t}$, if the reward estimation error (i.e., prediction uncertainty) is large for both $u$ and $u'$, we may want to explore these two user-arm pairs $(u, \\vect{x}_{i, t}), (u', \\vect{x}_{i, t})$ more for additional knowledge.\n% %\n% % In this paper, we consider the mapping functions $\\Psi^{(1)}, \\Psi^{(2)}$ as the prior knowledge, which can be functions such as the radial basis function (RBF) kernel or normalized absolute difference in practice.\n\n\n% % -------------\n\n% % -------------\n% \\textbf{Learning Objective.}\n% For the received user $u_{t}$ in round $t$, the learner is expected to recommend an arm $x_{t}\\in \\mathcal{X}_{t}$ (with reward $r_{t}$) in order to minimize the cumulative pseudo-regret\n% $\n%     R(T) = \\mathbb{E}[\\sum_{t=1}^{T}(r_{t}^{*} - r_{t})] \n% $\n% where $r_{t}^{*}$ is the reward for the optimal arm $ \\mathbb{E}[r_{t}^{*}| u_t, \\mathcal{X}_{t}] = \\max_{\\vect{x}_{i,t}\\in \\mathcal{X}_{t}} h(\\vect{x}_{i, t}, u_{t}, \\matr{\\Lambda}^{*}_{i, t})$. \n\n% % ---\n% \\textbf{Notation.}\n% Denoting $\\mathcal{T}_{u, t} \\subseteq [t]$ as the collection of time steps that user $u\\in \\mathcal{U}$ is served up to round $t$, we use $\\mathcal{P}_{u, t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in \\mathcal{T}_{u, t}}$ to represent the collection of received arm-reward pairs associated with user $u$, and $T_{u, t} = \\abs{\\mathcal{T}_{u, t}}$ refers to the number of rounds that user $u$ has been served. \n% Here, $\\vect{x}_{\\tau} \\in \\mathcal{A}_{\\tau}, r_{\\tau} \\in \\mathbb{R}$ separately refer to the chosen arm and actual received reward in round $\\tau\\in \\mathcal{T}_{u, t}$.\n% Similarly, we use $\\mathcal{P}_{t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in [t]}$ to denote all the past records (i.e., arm-reward pairs), up to round $t$. \n% %\n% For any graph $\\mathcal{G}$, we denote $\\matr{A} \\in \\mathbb{R}^{n \\times n}$ as its adjacency matrix (with added self-loops), and $\\matr{D} \\in \\mathbb{R}^{n \\times n}$ as its degree matrix.\n% Then, we will introduce our proposed solution, the \\name\\ framework.\n\n% \\begin{figure}[t]\n%   \\centering\n%   \\vspace{-0.5cm}\n%   \\includegraphics[width=\\linewidth]{Figure/User_group.png}\n%   \\vspace{-0.7cm}\n%   \\caption{ Workflow of the proposed Graph Neural Bandits (\\name) framework. }\n%   \\vspace{-0.4cm}\n%   \\label{fig_model_arch}\n% \\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.1cm}\n% ====================================================\n% ===================================================================================== Algorithm 1\n\n\n\n% \\begin{algorithm}[h!]\n% \\caption{Graph Neural Bandits (\\name)}\n% \\label{algo_main}\n% \\begin{algorithmic}[1]\n\n% \\STATE \\textbf{Input:} Number of rounds $T$, network width $m$, information propagation hops $k$. Functions for edge weight estimation $\\Psi^{(1)}(\\cdot, \\cdot), \\Psi^{(2)}(\\cdot, \\cdot): \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$. \\\\\n% \\STATE \\textbf{Output:} Arm recommendation $\\vect{x}_{t}$ for each time step $t$.\\\\\n% \\STATE \\textbf{Initialization:} Initialize parameter $\\vect{\\Theta}_{0}$ for all models. \\\\\n\n% \\FOR{$t = 1, 2, ..., T$}\n%     \\vspace{0.5em}\n%     \\STATE Receive a user $u_{t}$ and a set of arm contexts $\\mathcal{X}_{t} = \\{ \\vect{x}_{i, t} \\}_{i\\in [a]}$. \\\\\n%     \\STATE  Construct two kinds of user graphs $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$ for arm set $\\mathcal{X}_{t}$ with \\textbf{Algorithm} \\ref{algo_updating_graph}. \\\\\n%     \\FOR{each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$}\n%         \\STATE Compute reward estimation $\\hat{r}_{i, t} = f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})$, and the potential gain\n%         $\\hat{b}_{i, t} = f_{gnn}^{(2)}(\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}),~\\mathcal{G}_{i, t}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})$. \\\\\n%     \\ENDFOR\n%     \\STATE Play arm $\\vect{x}_{t} = \\arg\\max_{\\vect{x}_{i, t} \\in \\mathcal{X}_{t}} \\big( \\hat{r}_{i, t} + \\hat{b}_{i, t} \\big)$, and observe its true reward $r_{t}$. \\\\\n%     \\STATE Train the user networks $f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$, $f_{u}^{(2)}(\\cdot; \\matr{\\Theta}_{u}^{(2)})$ and GNN models \n%     $f_{gnn}^{(1)}(\\cdot; \\matr{\\Theta}_{gnn}^{(1)})$, $f_{gnn}^{(2)}(\\cdot; \\matr{\\Theta}_{gnn}^{(2)})$  with gradient descent, according to \\textbf{Algorithm} \\ref{algo_training}.\n% \\ENDFOR\n% \\end{algorithmic}\n% \\end{algorithm}\n\n\n\n\\begin{algorithm}[t]\n\\caption{Graph Neural Bandits (\\name)}\n\\label{algo_main}\n\\textbf{Input:} Number of rounds $T$, network width $m$, information propagation hops $k$. Functions for edge weight estimation $\\Psi^{(1)}(\\cdot, \\cdot), \\Psi^{(2)}(\\cdot, \\cdot): \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$. \\\\\n% arm group graph threshold $\\beta$.\\\\ \n\\textbf{Output:} Arm recommendation $\\vect{x}_{t}$ for each time step $t$.\\\\\n\\textbf{Initialization:} Initialize trainable parameters for all models. \\\\\n\n\\For{$t \\in \\{1, 2, \\dots, T\\}$}{\n    Receive a user $u_{t}$ and a set of arm contexts $\\mathcal{X}_{t} = \\{ \\vect{x}_{i, t} \\}_{i\\in [a]}$. \\\\\n    %\n   % Construct two kinds of user graphs $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]} = $ \\textbf{Procedure}~~ \\textit{Estimating Arm-Specific User Graphs} $(\\mathcal{X}_{t})$ [line 13-27], given the arm set $\\mathcal{X}_{t}$. \\\\\n    %\n    \\For{each candidate arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$}{\n        %\n        Construct two kinds of user graphs $\\mathcal{G}_{i, t}^{(1)}$, $\\mathcal{G}_{i, t}^{(2)} = $ \\textbf{Procedure}~~ \\textit{Estimating Arm-Specific User Graphs} $(\\vect{x}_{i, t})$. [line 13-20] \\\\\n        \n        %%%%%%%%%%%%%\n        Compute reward estimation [\\textbf{Eq.} \\ref{eq_f_1_user_output}]\n        $\\hat{r}_{i, t} = f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})$, and the potential gain [\\textbf{Eq.} \\ref{eq_f_2_explore_user_output}]\n        $\\hat{b}_{i, t} = f_{gnn}^{(2)}(\\nabla [f_{gnn}^{(1)}]_{i, t} ,~\\mathcal{G}_{i, t}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})$. \\\\\n    }\n    %\n    Play arm  \n    $\\vect{x}_{t} = \\arg\\max_{\\vect{x}_{i, t} \\in \\mathcal{X}_{t}} \\big( \\hat{r}_{i, t} + \\hat{b}_{i, t} \\big)$, and observe its true reward $r_{t}$. \\\\\n    %\n    Train the user networks $f_{u_{t}}^{(1)}(\\cdot; \\matr{\\Theta}_{u_{t}}^{(1)})$, $f_{u_{t}}^{(2)}(\\cdot; \\matr{\\Theta}_{u_{t}}^{(2)})$ and GNN models \n    $f_{gnn}^{(1)}(\\cdot; \\matr{\\Theta}_{gnn}^{(1)})$, $f_{gnn}^{(2)}(\\cdot; \\matr{\\Theta}_{gnn}^{(2)})$  with GD.  \n    % (\\textbf{Algorithm} \\ref{algo_training}, Appendix Section \\ref{sec_appd_pseudo_code}).\n    % \\ChangedForCamera\n}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% \\SetKwProg{myproc}{Procedure}{}{end procedure}\n% \\myproc{Estimating Arm-Specific User Graphs $(\\mathcal{X}_{t})$}{\n%     Initialize $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$. \\\\\n%     \\For{each user $u\\in\\mathcal{U}$}{\n%         %\n%         \\For{each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$}{\n%             Compute $\\hat{r}_{u, i} = f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u}^{(1)}]_{t-1})$, and \n%             $\\hat{b}_{u, i} = f_{u}^{(2)}(\\nabla_{\\matr{\\Theta}_{u}^{(1)}} f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u}^{(1)}]_{t-1}); [\\matr{\\Theta}_{u}^{(2)}]_{t-1})$. \\\\\n%         }\n%         %\n%     }\n%     \\For{each arm $\\vect{x}_{i, t}\\in \\mathcal{X}_{t}$}{\n%         \\For{each user pair $(u, u') \\in\\mathcal{U}\\times \\mathcal{U}$}{\n%             For edge weight $w_{i, t}^{(1)}(u, u') \\in W_{i, t}^{(1)}$, update $w_{i, t}^{(1)}(u, u') = \\Psi^{(1)}(\\hat{r}_{u, i}, \\hat{r}_{u', i})$. \\\\\n%             For edge weight $w_{i, t}^{(2)}(u, u') \\in W_{i, t}^{(2)}$, update $w_{i, t}^{(2)}(u, u') = \\Psi^{(2)}(\\hat{b}_{u, i}, \\hat{b}_{u', i})$. \\\\\n%         }\n%     }\n%     Return user graphs $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$. \\\\\n% }\n\n\\SetKwProg{myproc}{Procedure}{}{end procedure}\n\\myproc{Estimating Arm-Specific User Graphs $(\\vect{x}_{i, t})$}{\n    Initialize arm graphs $\\mathcal{G}_{i, t}^{(1)}$, $\\mathcal{G}_{i, t}^{(2)}$. \\\\\n    \n    \\For{each user pair $(u, u') \\in\\mathcal{U}\\times \\mathcal{U}$}{\n        % Compute reward estimations $\\hat{r}_{u, i} = f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u}^{(1)}]_{t-1})$, $\\hat{r}_{u', i} = f_{u'}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u'}^{(1)}]_{t-1})$ and the potential gain\n        %     $\\hat{b}_{u, i} = f_{u}^{(2)}(\\nabla f_{u}^{(1)}(\\vect{x}_{i, t}); [\\matr{\\Theta}_{u}^{(2)}]_{t-1})$, $\\hat{b}_{u', i} = f_{u'}^{(2)}(\\nabla f_{u'}^{(1)}(\\vect{x}_{i, t}); [\\matr{\\Theta}_{u'}^{(2)}]_{t-1})$. \\\\\n        %\n        For edge weight $w_{i, t}^{(1)}(u, u') \\in W_{i, t}^{(1)}$, update $w_{i, t}^{(1)}(u, u') = \\Psi^{(1)} \\big( f_{u}^{(1)}(\\vect{x}_{i, t}), f_{u'}^{(1)}(\\vect{x}_{i, t}) \\big)$. [\\textbf{Eq.} \\ref{eq_est_exploit_correlation}] \\\\\n        For edge weight $w_{i, t}^{(2)}(u, u') \\in W_{i, t}^{(2)}$, based on the [\\textbf{Eq.} \\ref{eq_est_explore_correlation}], update $w_{i, t}^{(2)}(u, u') = \\Psi^{(2)} \\big( f_{u}^{(2)}\\big( \\nabla f_{u}^{(1)}(\\vect{x}_{i, t}) ), f_{u'}^{(2)}\\big( \\nabla f_{u'}^{(1)}(\\vect{x}_{i, t}) \\big) \\big)$. \\\\\n    }\n    Return user graphs $\\mathcal{G}_{i, t}^{(1)}$, $\\mathcal{G}_{i, t}^{(2)}$. \\\\\n}\n\n\n\n\\end{algorithm}\n\n\n\n% ------------------------------------------------\n% \\begin{algorithm}[h]\n% \\caption{Estimating Arm-Specific User Graphs}\n% \\label{algo_updating_graph}\n% \\textbf{Input:} Model parameters $\\matr{\\Theta}_{t-1}$. Functions for edge weight estimation $\\Psi^{(1)}(\\cdot, \\cdot), \\Psi^{(2)}(\\cdot, \\cdot): \\mathbb{R}\\times \\mathbb{R} \\mapsto \\mathbb{R}$. \\\\\n% \\textbf{Output:} Updated user graphs $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$. \\\\\n\n% Initialize $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$. \\\\\n% \\For{each user $u\\in\\mathcal{U}$}{\n%     %\n%     \\For{each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$}{\n%         Compute $\\hat{r}_{u, i} = f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{t-1})$, and \n%         $\\hat{b}_{u, i} = f_{u}^{(2)}(\\nabla_{\\matr{\\Theta}_{u_{t}}^{(1)}} f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{t-1}); [\\matr{\\Theta}_{u_{t}}^{(2)}]_{t-1})$. \\\\\n%     }\n%     %\n% }\n% \\For{each arm $\\vect{x}_{i, t}\\in \\mathcal{X}_{t}$}{\n%     \\For{each user pair $(u, u') \\in\\mathcal{U}\\times \\mathcal{U}$}{\n%         For edge weight $w_{i, t}^{(1)}(u, u') \\in W_{i, t}^{(1)}$, update $w_{i, t}^{(1)}(u, u') = \\Psi^{(1)}(\\hat{r}_{u, i}, \\hat{r}_{u', i})$. \\\\\n%         For edge weight $w_{i, t}^{(2)}(u, u') \\in W_{i, t}^{(2)}$, update $w_{i, t}^{(2)}(u, u') = \\Psi^{(2)}(\\hat{b}_{u, i}, \\hat{b}_{u', i})$. \\\\\n%     }\n% }\n% Return user graphs $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$. \\\\\n% \\end{algorithm}\n\n\n% ------------------------------------\n% \\begin{algorithm}[t]\n% \\caption{\\name}\n% \\label{algo_main}\n% \\textbf{Input:} Number of rounds $T$, network width $m$, neighborhood size $k$. Kernels for edge weight estimation $\\Psi^{(1)}(\\cdot, \\cdot), \\Psi^{(2)}(\\cdot, \\cdot)$. \\\\\n% % arm group graph threshold $\\beta$.\\\\ \n% \\textbf{Output:} Arm recommendation $\\vect{x}_{t}$ for each time step $t$.\\\\\n% \\textbf{Initialization:} Initialize the user graphs $\\mathcal{G}_{0}^{(1)} = (V, E, W_{0}^{(1)})$, $\\mathcal{G}_{0}^{(2)} = (V, E, W_{0}^{(2)})$. Initialize parameter $\\vect{\\Theta_{0}}$ for all models. \\\\\n\n% \\For{$t = 1, 2, ..., T$}{\n%     Receive a user $u_{t}$ and a set of arm contexts $\\mathcal{X}_{t} = \\{ \\vect{x}_{i, t} \\}_{i\\in [a]}$. \\\\\n%     %\n%     Update the user graphs to $\\mathcal{G}_{t}^{(1)}$, $\\mathcal{G}_{t}^{(2)}$ w.r.t. \\textbf{Algorithm} \\ref{algo_updating_graph}. \\\\\n%     %\n%     \\For{each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$}{\n%         Compute reward estimation $\\hat{r}_{i, t} = f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{t}^{(1)}; [\\Theta_{gnn}^{(1)}]_{t-1})$, and the potential gain\n%         $\\hat{b}_{i, t} = f_{gnn}^{(2)}(\\nabla_{\\Theta_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{t}^{(1)}; [\\Theta_{gnn}^{(1)}]_{t-1}),~\\mathcal{G}_{t}^{(2)}; [\\Theta_{gnn}^{(2)}]_{t-1})$. \\\\\n%     }\n%     %\n%     Play arm  \n%     $\\vect{x}_{t} = \\arg\\max_{i\\in [a]} \\big( \\hat{r}_{i, t} + \\hat{b}_{i, t} \\big)$, and observe its true reward $r_{t}$. \\\\\n%     %\n%     Train the user networks $f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$, $f_{u}^{(2)}(\\cdot; \\matr{\\Theta}_{u}^{(2)})$ and GNN models \n%     $f_{gnn}^{(1)}(\\cdot; \\Theta_{gnn}^{(1)})$, $f_{gnn}^{(2)}(\\cdot; \\Theta_{gnn}^{(2)})$ w.r.t. \\textbf{Algorithm} \\ref{algo_training}.\n% }\n% \\end{algorithm}\n\n\n\n\n\n% % ===================================================================================== Algorithm 2\n% \\begin{algorithm}[t]\n% \\caption{Estimating Arm-Specific User Graphs}\n% \\label{algo_updating_graph}\n% \\textbf{Input:} Model parameters $\\matr{\\Theta}_{t-1}$. Functions for edge weight estimation $\\Psi^{(1)}(\\cdot, \\cdot), \\Psi^{(2)}(\\cdot, \\cdot): \\mathbb{R}\\times \\mathbb{R} \\mapsto \\mathbb{R}$. \\\\\n% \\textbf{Output:} Updated user graphs $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$. \\\\\n\n% Initialize $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$. \\\\\n% \\For{each user $u\\in\\mathcal{U}$}{\n%     %\n%     \\For{each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$}{\n%         Compute $\\hat{r}_{u, i} = f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{t-1})$, and \n%         $\\hat{b}_{u, i} = f_{u}^{(2)}(\\nabla_{\\matr{\\Theta}_{u_{t}}^{(1)}} f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{t-1}); [\\matr{\\Theta}_{u_{t}}^{(2)}]_{t-1})$. \\\\\n%     }\n%     %\n% }\n% \\For{each arm $\\vect{x}_{i, t}\\in \\mathcal{X}_{t}$}{\n%     \\For{each user pair $(u, u') \\in\\mathcal{U}\\times \\mathcal{U}$}{\n%         For edge weight $w_{i, t}^{(1)}(u, u') \\in W_{i, t}^{(1)}$, update $w_{i, t}^{(1)}(u, u') = \\Psi^{(1)}(\\hat{r}_{u, i}, \\hat{r}_{u', i})$. \\\\\n%         For edge weight $w_{i, t}^{(2)}(u, u') \\in W_{i, t}^{(2)}$, update $w_{i, t}^{(2)}(u, u') = \\Psi^{(2)}(\\hat{b}_{u, i}, \\hat{b}_{u', i})$. \\\\\n%     }\n% }\n% Return user graphs $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$. \\\\\n% \\end{algorithm}\n\n\n% --------------------------------------------------\n% \\begin{algorithm}[t]\n% \\caption{Updating User Graphs}\n% \\label{algo_updating_graph}\n% \\textbf{Input:} Model parameters $\\matr{\\Theta}_{t-1}$. Kernels for edge weight estimation $\\Psi^{(1)}(\\cdot, \\cdot), \\Psi^{(2)}(\\cdot, \\cdot)$. \\\\\n% \\textbf{Output:} Updated user graphs $\\mathcal{G}_{t}^{(1)}$, $\\mathcal{G}_{t}^{(2)}$. \\\\\n\n% \\For{each user $u\\in\\mathcal{U}$}{\n%     $list_{\\hat{r}}^{u} \\leftarrow \\emptyset$, \\quad     $list_{b}^{u} \\leftarrow \\emptyset$. \\\\\n%     %\n%     \\For{each arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$}{\n%         Compute $\\hat{r}_{u, i} = f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{t})$, and \n%         $\\hat{b}_{u, i} = f_{u}^{(2)}(\\nabla_{\\matr{\\Theta}_{u_{t}}^{(1)}} f_{u}^{(1)}(\\vect{x}_{i, t}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{t-1}); [\\matr{\\Theta}_{u_{t}}^{(2)}]_{t-1})$. \\\\\n%         %\n%         $list_{\\hat{r}}^{u} = list_{\\hat{r}}^{u} \\cup \\{\\hat{r}_{u, i}\\}$, $list_{b}^{u} = list_{b}^{u} \\cup \\{b_{u, i}\\}$. \\\\\n%     }\n%     %\n    \n%     $\\vect{\\hat{r}}_{u}^{t} \\leftarrow \\text{Vectorize}(list_{\\hat{r}}^{u})$, \n%     $\\vect{b}_{u}^{t} \\leftarrow \\text{Vectorize}(list_{b}^{u})$. \n%     \\quad \\# Turn $list_{\\hat{r}}^{u}$ and $list_{b}^{u}$ into vectors. \\\\\n% }\n\n% %\n% \\For{each user pair $(u_{i}, u_{j}) \\in\\mathcal{U}\\times \\mathcal{U}$}{\n%     For edge weight $w_{t}^{(1)}(u_{i}, u_{j}) \\in W_{t}^{(1)}$, update $w_{t}^{(1)}(u_{i}, u_{j}) = \\Psi^{(1)}(\\vect{\\hat{r}}_{u_{i}}^{t}, \\vect{\\hat{r}}_{u_{j}}^{t})$. \\\\\n%     For edge weight $w_{t}^{(2)}(u_{i}, u_{j}) \\in W_{t}^{(2)}$, update $w_{t}^{(2)}(u_{i}, u_{j}) = \\Psi^{(2)}(\\vect{b}_{u_{i}}^{t}, \\vect{b}_{u_{j}}^{t})$. \\\\\n% }\n\n% Return new user graphs $\\mathcal{G}_{t}^{(1)}$, $\\mathcal{G}_{t}^{(2)}$. \\\\\n% \\end{algorithm}\n\n\n\n\n% % ===================================================================================== Algorithm 3\n% \\begin{algorithm}[t]\n% \\caption{Model Training}\n% \\label{algo_training}\n% \\textbf{Input:} Initial parameter $\\vect{\\Theta_{0}}$, step size $\\eta_{1}, \\eta_{2}$, training steps $J_{1}, J_{2}$, network width $m$. Updated user graphs $\\mathcal{G}_{t}^{(1)}$, $\\mathcal{G}_{t}^{(2)}$. Served user $u_{t}$. \\\\\n% \\textbf{Output:} Updated model parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$, $[\\vect{\\Theta}_{gnn}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t}$. \\\\\n\n% % ---------------------------------\n\n% \\BlankLine\n\n% $[\\matr{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]_{t}$ = User-Model-Training $\\big( u_{t}, [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}, [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0} \\big)$. \\\\\n% \\For{$\\forall u'\\in \\mathcal{U}$, $u' \\neq u_{t}$}{ \n%     $[\\matr{\\Theta}_{u'}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{u'}^{(1)}]_{t-1}$, \\quad \n%     $[\\matr{\\Theta}_{u'}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{u'}^{(2)}]_{t-1}$\\\\\n% } \n% $[\\matr{\\Theta}_{gnn}^{(1)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t}$ = GNN-Model-Training $\\big( [\\matr{\\Theta}_{gnn}^{(1)}]_{0}, [\\matr{\\Theta}_{gnn}^{(2)}]_{0} \\big)$. \\\\\n% Return $[\\matr{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(1)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t}$.\n\n% \\BlankLine\\BlankLine\n\n% % ---------------------------------\n% \\SetKwProg{myproc}{Procedure}{}{end procedure}\n% \\myproc{User-Model-Training $\\big( u_{t}, [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}, [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0} \\big)$}{\n%     $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}$, \\quad\n%     $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0}$. \\\\\n    \n%     %\n%     \\BlankLine\\BlankLine\n%     %\n%     \\# Training of $f_{u}^{(1)}(\\cdot )$ \\\\\n%     Let $\\mathcal{L}(\\Theta_{u_{t}}^{(1)}) := \\sum_{\\tau\\in \\mathcal{X}_{u_{t}, t}} \\abs{f_{u}^{(1)}(\\vect{x}_{\\tau}; \n%     \\matr{\\Theta}_{u_{t}}^{(1)}) - r_{\\tau}}^{2}$ \\\\\n%     %\n%     \\For{$j = 1, 2, \\dots, J_{1}$}{ \n%         $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{j} = [\\matr{\\Theta}_{u_{t}}^{(1)}]^{j-1} - \\eta_{1} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{u_{t}}^{(1)}]^{j-1})$ \\\\\n%     } \n    \n%     %\n%     \\BlankLine\\BlankLine\n%     %\n%     \\# Training of $f_{u}^{(2)}(\\cdot )$ \\\\\n%     %\n%     Let $\\mathcal{L}(\\Theta_{u_{t}}^{(2)}) := \\sum_{\\tau\\in \\mathcal{X}_{u_{t}, t}} \n%     \\abs{f_{u}^{(2)}(\\nabla_{\\matr{\\Theta}_{u_{t}}^{(1)}} f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{\\tau-1}); \\matr{\\Theta}_{u_{t}}^{(2)}) - \n%     \\big(r_{\\tau} - f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{\\tau-1})\\big)\n%     }^{2}$ \\\\\n%     %\n%     \\For{$j = 1, 2, \\dots, J_{1}$}{ \n%         $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{j} = [\\matr{\\Theta}_{u_{t}}^{(2)}]^{j-1} - \\eta_{1}\\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{u_{t}}^{(2)}]^{j-1})$ \\\\\n%     } \n%     %\n%     Let $[\\widehat{\\vect{\\Theta}}_{u_{t}}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{u_{t}}^{(1)}]^{J}$, \n%     $[\\widehat{\\vect{\\Theta}}_{u_{t}}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{u_{t}}^{(2)}]^{J}$ \\\\\n%     %\n%     Sample the parameter $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t} \\sim \\{ [\\widehat{\\vect{\\Theta}}_{u_{t}}^{(1)}]_{\\tau} \\}_{\\tau\\in [t]}$ and $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t} \\sim \\{ [\\widehat{\\vect{\\Theta}}_{u_{t}}^{(2)}]_{\\tau} \\}_{\\tau\\in [t]}$. \\\\\n%     %\n%     Return new parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$.\n%     % Return new parameters $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{J}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{J}$. \\\\\n% }\n% %\n% \\BlankLine\\BlankLine\\BlankLine\n% %\n\n% % ---------------------------------\n% \\myproc{GNN-Model-Training $\\big( [\\vect{\\Theta}_{gnn}^{(1)}]_{0}$, $[\\vect{\\Theta}_{gnn}^{(2)}]_{0} \\big)$}{\n%     $[\\matr{\\Theta}_{gnn}^{(1)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{gnn}^{(1)}]_{0}$, \\quad\n%     $[\\matr{\\Theta}_{gnn}^{(2)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{gnn}^{(2)}]_{0}$. \\\\\n    \n%     %\n%     \\BlankLine\n%     %\n%     \\# Training of $f_{gnn}^{(1)}(\\cdot )$ \\\\\n%     Let $\\mathcal{L}(\\Theta_{u_{t}}^{(1)}) := \\sum_{\\tau\\in \\mathcal{X}_{u_{t}, t}} \\abs{f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; \n%     \\matr{\\Theta}_{gnn}^{(1)}) - r_{\\tau}}^{2}$ \\\\\n%     %\n%     \\For{$j = 1, 2, \\dots, J_{2}$}{ \n%         $[\\matr{\\Theta}_{gnn}^{(1)}]^{j} = [\\matr{\\Theta}_{gnn}^{(1)}]^{j-1} - \\eta_{2} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{gnn}^{(1)}]^{j-1})$ \\\\\n%     } \n    \n%     %\n%     \\BlankLine\n%     %\n%     \\# Training of $f_{gnn}^{(2)}(\\cdot )$ \\\\\n%     $f_{gnn}^{(1)}(\\vect{x}_{\\tau}) \\leftarrow f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})$. \\\\\n%     %\n%     Let $\\mathcal{L}(\\Theta_{u_{t}}^{(2)}) := \\sum_{\\tau\\in \\mathcal{X}_{u_{t}, t}} \n%     \\abs{f_{gnn}^{(2)}(\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{\\tau}), \\mathcal{G}_{\\tau}^{(2)}; \\matr{\\Theta}_{gnn}^{(2)}) - \n%     \\big(r_{\\tau} - f_{gnn}^{(1)}(\\vect{x}_{\\tau})\\big)\n%     }^{2}$ \\\\\n%     %\n%     \\For{$j = 1, 2, \\dots, J_{2}$}{ \n%         $[\\matr{\\Theta}_{gnn}^{(2)}]^{j} = [\\matr{\\Theta}_{gnn}^{(2)}]^{j-1} - \\eta_{2} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{gnn}^{(2)}]^{j-1})$ \\\\\n%     }\n%     %\n%     Let $[\\widehat{\\vect{\\Theta}}_{gnn}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{gnn}^{(1)}]^{J}$, \n%     $[\\widehat{\\vect{\\Theta}}_{gnn}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{gnn}^{(2)}]^{J}$ \\\\\n%     %\n%     Sample the parameter $[\\vect{\\Theta}_{gnn}^{(1)}]_{t} \\sim \\{ [\\widehat{\\vect{\\Theta}}_{gnn}^{(1)}]_{\\tau} \\}_{\\tau\\in [t]}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t} \\sim \\{ [\\widehat{\\vect{\\Theta}}_{gnn}^{(2)}]_{\\tau} \\}_{\\tau\\in [t]}$. \\\\\n%     %\n%     Return new parameters $[\\vect{\\Theta}_{gnn}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t}$.\n% }\n% \\end{algorithm}\n\n\n\n\\vspace{-0.1cm}\n% ====================================================\n\n\n% \\vspace{-0.3cm}\n"
            },
            "section 4": {
                "name": "GNB: Proposed Framework",
                "content": "   \\label{sec_proposed_framework}\n% \\vspace{-0.2cm}\n\nThe workflow of our proposed \\name\\ framework (Figure \\ref{fig_model_arch}) consists of four major components: \n\\textbf{First}, we derive the estimation for user exploitation graphs $\\mathcal{G}_{i, t}^{(1), *}, i\\in [a]$ (denoted by $\\mathcal{G}_{i, t}^{(1)}$), and the exploration graphs $\\mathcal{G}_{i, t}^{(2), *}, i\\in [a]$ (denoted by $\\mathcal{G}_{i, t}^{(2)}$), \nto model user correlations in terms of exploitation and exploration respectively; \n\\textbf{Second}, to estimate the reward and the potential gain by leveraging the \"fine-grained\" correlations, we propose the GNN-based models  $f_{gnn}^{(1)}(\\cdot)$, $f_{gnn}^{(2)}(\\cdot)$  to aggregate the correlations of the target user-arm pair on estimated graphs $\\mathcal{G}_{i, t}^{(1)} $ and  $\\mathcal{G}_{i, t}^{(2)}$, respectively;\n\\textbf{Third}, we select the arm $\\vect{x}_{t}$, based on the estimated arm reward and potential gain calculated by our GNN-based models; \n\\textbf{Finally}, we train the parameters of \\name\\ using gradient descent (GD) on past records.\n\n% since the model training with GD is standard knowledge, \n% Here, we choose to include all the information for the training process (e.g., training samples, labels, and the loss function) in the main body, and leave the pseudo-code summarizing the training procedure to Appendix Section \\ref{sec_appd_pseudo_code} (\\textbf{Alg.} \\ref{algo_training}) due to page limit.\n% \\ChangedForCamera\n\n% --------------\n\\vspace{-0.1cm}\n",
                "subsection 4.1": {
                    "name": "User Graph Estimation with User Networks",
                    "content": " \\label{subsec_user_models} \nBased on the definition of unknown true user graphs $\\mathcal{G}^{(1), *}_{i, t}$, $\\mathcal{G}^{(2), *}_{i, t}$ w.r.t. arm $\\vect{x}_{i, t}\\in \\mathcal{X}_{t}$ (\\textbf{Definition} \\ref{def_exploitation_simi} and \\ref{def_exploration_simi}), we proceed to derive their estimations $\\mathcal{G}^{(1)}_{i, t}$, $\\mathcal{G}^{(2)}_{i, t}, i\\in [a]$ with individual user networks $f_{u}^{(1)}$, $f_{u}^{(2)}, u\\in \\mathcal{U}$. \nAfterwards, with these two kinds of estimated user graphs $\\mathcal{G}_{i, t}^{(1)}$ and $\\mathcal{G}_{i, t}^{(2)}$, \nwe will be able to apply our GNN-based models to leverage the user correlations under the exploitation and the exploration settings. The pseudo-code is presented in \\textbf{Alg.} \\ref{algo_main}.\n\n\n\n\\textbf{[User Exploitation Network $f_{u}^{(1)}$]}\nFor each user $u\\in\\mathcal{U}$, we use a neural network $f_{u}^{(1)}(\\cdot) = f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$ to learn user $u$'s preference for arm $\\vect{x}_{i,t}$, i.e.,  $\\mathbb{E}[r_{i,t} | u, \\vect{x}_{i,t}]$. \n%\nFollowing the \\textbf{Def.} \\ref{def_exploitation_simi}, we construct the exploitation graph $\\mathcal{G}^{(1)}_{i, t}$ by estimating the user exploitation correlation based on user preferences.\nThus, in $\\mathcal{G}^{(1)}_{i, t}$, we consider the edge weight of two user nodes $u, u'$ as\n\\begin{equation} \n    w_{i, t}^{(1)}(u, u') = \\Psi^{(1)} \\big( f_{u}^{(1)}(\\vect{x}_{i, t}), f_{u'}^{(1)}(\\vect{x}_{i, t}) \\big)\n\\label{eq_est_exploit_correlation}\n\\end{equation}\nwhere $\\Psi^{(1)}(\\cdot, \\cdot)$ is the mapping function applied in \\textbf{Def.} \\ref{def_exploitation_simi} (line 16, \\textbf{Alg.} \\ref{algo_main}).\n%\nHere, $f_{u}^{(1)}(\\cdot)$ will be trained by GD with chosen arms $\\{\\vect{x}_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$ as samples,\nand received reward $\\{ r_{\\tau} \\}_{\\tau\\in \\mathcal{T}_{u, t}}$ as the labels, where \n$\n\\mathcal{L}_{u}^{(1)}(\\Theta_{u}^{(1)}) = \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\big| f_{u}^{(1)}(\\vect{x}_{\\tau}; \n    \\matr{\\Theta}_{u}^{(1)}) - r_{\\tau}\\big|^{2}   \n$ will be the corresponding quadratic loss. Recall that $\\vect{x}_{\\tau}$ and $r_{\\tau}$ stand for the chosen arm and the received reward respectively in round $\\tau$.\n\n\n\n\\iffalse\n\\textbf{[User Exploitation Network $f_{u}^{(1)}$]}\nFor each user $u\\in\\mathcal{U}$, we propose to apply an exploitation network $f_{u}^{(1)}(\\cdot)$ to learn user $u$'s preference for arm $\\vect{x}_{i,t}$, i.e.,  $\\mathbb{E}[r_{i,t} | u, \\vect{x}_{i,t}]$. \nHere, $f_{u}^{(1)}(\\cdot)$ will be trained with GD on the past records (arm contexts and rewards) $\\mathcal{P}_{u, t}$ from user $u$, and the loss function will be the quadratic loss between the predicted reward and the actual reward.\n%\nFollowing the \\textbf{Def.} \\ref{def_exploitation_simi}, we will then be able to construct the exploitation graph $\\mathcal{G}^{(1)}_{i, t}$ by estimating the user exploitation correlation based on user preferences.\nIn $\\mathcal{G}^{(1)}_{i, t}$, we consider the edge weight between two user nodes $u, u'$ to be\n$$\n    w_{i, t}^{(1)}(u, u') = \\Psi^{(1)} \\big( f_{u}^{(1)}(\\vect{x}_{i, t}), f_{u'}^{(1)}(\\vect{x}_{i, t}) \\big),\n$$ \nwhere $\\Psi^{(1)}(\\cdot, \\cdot)$ is the mapping function applied in \\textbf{Def.} \\ref{def_exploitation_simi} (line 16, \\textbf{Alg.} \\ref{algo_main}).\n\\fi\n%\n\\textbf{[User Exploration Network $f_{u}^{(2)}$]}\nGiven user $u\\in \\mathcal{U}$, to estimate the potential gain (i.e., the uncertainty for the reward estimation) $\\mathbb{E}[r | u, \\vect{x}_{i,t}] - f_{u}^{(1)}(\\vect{x}_{i,t})$ for arm $\\vect{x}_{i,t}$, we adopt the user exploration network $f_{u}^{(2)}(\\cdot) = f_{u}^{(2)}(\\cdot; \\matr{\\Theta}_{u}^{(2)})$ inspired by \\cite{EE-Net_ban2021ee}. \n% \nAs it has proved that the confidence interval (uncertainty) of reward estimation can be expressed as a function of network gradients \\citep{Neural-UCB,AGG-UCB_qi2022neural}, we apply $f_{u}^{(2)}(\\cdot)$ to directly learn the uncertainty with the gradient of $f_{u}^{(1)}(\\cdot)$. Thus, the input of $f_{u}^{(2)}(\\cdot)$ will be the network gradient of $f_{u}^{(1)}(\\cdot)$ given arm $\\vect{x}_{i,t}$, denoted as $\\nabla f_{u}^{(1)}(\\vect{x}_{i,t}) = \\nabla_{\\matr{\\Theta}} f_{u}^{(1)} (\\vect{x}_{i,t}; [\\matr{\\Theta}_{u}^{(1)}]_{t-1} )$, where $[\\matr{\\Theta}_{u}^{(1)}]_{t-1}$ refer to the parameters of $f_{u}^{(1)}$ in round $t$ (before training [line 11, \\textbf{Alg.} \\ref{algo_main}]).\n%\nAnalogously, given the estimated user exploration graph $\\mathcal{G}^{(2)}_{i, t}$ and two user nodes $u, u'$, we let the edge weight be \n\\begin{equation} \n    w_{i, t}^{(2)}(u, u') = \\Psi^{(2)} \\bigg( f_{u}^{(2)}\\big( \\nabla f_{u}^{(1)}(\\vect{x}_{i, t}) ), f_{u'}^{(2)}\\big( \\nabla f_{u'}^{(1)}(\\vect{x}_{i, t}) \\big) \\bigg)\n\\label{eq_est_explore_correlation}\n\\end{equation}\n% where $\\nabla f_{u}^{(1)}(\\vect{x}_{i, t})$ stands for the gradient of $f_{u}^{(1)}(\\cdot)$ given arm $\\vect{x}_{i, t}$ as the input\nas in line 17, \\textbf{Alg.} \\ref{algo_main}, and $\\Psi^{(2)}(\\cdot, \\cdot)$ is the mapping function that has been applied in \\textbf{Def.} \\ref{def_exploration_simi}. \n%\nWith GD, $f_{u}^{(2)}(\\cdot)$ will be trained with the past gradients of $f_{u}^{(1)}$, i.e., $\\{\\nabla f_{u}^{(1)}(\\vect{x}_{\\tau})  \\}_{\\tau\\in \\mathcal{T}_{u, t}}$ as samples;\nand the potential gain (uncertainty) $\\{ r_{\\tau} - f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau-1}) \\}_{\\tau\\in \\mathcal{T}_{u, t}}$ as labels. The quadratic loss is defined as \n$\\mathcal{L}_{u}^{(2)}(\\Theta_{u}^{(2)}) = \\\\\n    \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \n    \\big|f_{u}^{(2)}( \\nabla f_{u}^{(1)}(\\vect{x}_{\\tau}); \\matr{\\Theta}_{u}^{(2)}) - \\big(r_{\\tau} - f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau-1})\\big)\n    \\big|^{2}.\n$\n\n\n\n\n%\n\\textbf{[Network Architecture]}\n% For the theoretical analysis and experiments, we apply separate $L$-layer ($L \\geq 2$) fully-connected (FC) networks for both kinds of user networks.\n% %\n% Their weight matrix entries for the first $L-1$ layers are drawn from the Gaussian distribution $N(0, 2 / m)$. The entries of the last layer ($L$-th layer) are sampled from $N(0, 1 / m)$. Complementary details are in Appendix Section \\ref{sec_appd_user_network_arch}.\nHere, we can apply various architectures for $f_{u}^{(1)}(\\cdot), f_{u}^{(2)}(\\cdot)$ to deal with different application scenarios (e.g., Convolutional Neural Networks [CNNs] for visual content recommendation tasks). For the theoretical analysis and experiments, with user $u\\in \\mathcal{U}$, we apply separate $L$-layer ($L \\geq 2$) fully-connected (FC) networks as the user exploitation and exploration network  \n\\begin{equation} \nf_{u}(\\vect{\\chi}; \\matr{\\Theta}_{u} ) = \\matr{\\Theta}_{L} \\sigma ( \\matr{\\Theta}_{L-1}  \\sigma (\\matr{\\Theta}_{L-2} \\dots  \\sigma(\\matr{\\Theta}_{1} \\vect{\\chi}) )), ~~ \\sigma := \\text{ReLU}(\\cdot) \n\\label{eq_user_model_structure}\n\\end{equation}\nwith $\\matr{\\Theta}_{u} = [\\text{vec}(\\matr{\\Theta}_{1})^{\\intercal}, \\dots, \\text{vec}(\\matr{\\Theta}_{L})^{\\intercal}]^{\\intercal}$ being the vector of trainable parameters.\nHere, since $f_{u}^{(1)}(\\cdot), f_{u}^{(2)} (\\cdot)$ are both the $L$-layer FC network  (\\textbf{Eq.} \\ref{eq_user_model_structure}), the input $\\vect{\\chi}$ can be substituted with either the arm context $\\vect{x}_{i, t}$ or the network gradient $\\nabla f_{u}^{(1)}(\\vect{x}_{i, t})$ accordingly. \n\n\\textbf{[Parameter Initialization]}\nThe weight matrices of the first layer are slightly different for two kinds of user networks, as $\\matr{\\Theta}_{1}^{(1)} \\in \\mathbb{R}^{m\\times d}$, $\\matr{\\Theta}_{1}^{(2)} \\in \\mathbb{R}^{m\\times p_{u}^{(1)}}$ where $p_{u}^{(1)}$ is the dimensionality of $\\matr{\\Theta}_{u}^{(1)}$. The weight matrix shape for the rest of the $L-1$ layers will be the same for these two kinds of user networks, which are $\\matr{\\Theta}_{l} \\in \\mathbb{R}^{m\\times m}, l\\in [2, \\cdots, L-1]$, and $\\matr{\\Theta}_{L} \\in \\mathbb{R}^{1\\times m}$. \nTo initialize $f_{u}^{(1)}, f_{u}^{(2)}$, the weight matrix entries for their first $L-1$ layers $\\{\\matr{\\Theta}_{1}, \\dots \\matr{\\Theta}_{L-1} \\}$ are drawn from the Gaussian distribution $N(0, 2 / m)$, and the entries of the last layer weight matrix $\\matr{\\Theta}_{L}$ are sampled from $N(0, 1 / m)$.\n\n\n\n% \\textbf{User Networks Architecture.}\n% Here, we can choose different architectures for $f_{u}^{(1)}(\\cdot), f_{u}^{(2)}(\\cdot)$ to deal with various application scenarios (e.g., Convolutional Neural Networks [CNNs] for recommendation tasks of visual contents). In this paper, for the theoretical analysis and experiments, we apply separate $L$-layer fully-connected (FC) networks for user exploitation models and exploration models, as \n% \\begin{equation} \n% f_{u}(\\vect{\\chi}; \\matr{\\Theta}_{u} ) = \\matr{\\Theta}_{L} \\sigma ( \\matr{\\Theta}_{L-1}  \\sigma (\\matr{\\Theta}_{L-2} \\dots  \\sigma(\\matr{\\Theta}_{1} \\vect{\\chi}) ))\n% \\label{eq_user_model_structure}\n% \\end{equation}\n% with $\\matr{\\Theta}_{u} = \\{\\matr{\\Theta}_{1}, \\dots, \\matr{\\Theta}_{L}\\}$ being the trainable parameters, and $\\sigma$ being the ReLU activation. \n% Here, since $f_{u}^{(1)}(\\cdot), f_{u}^{(2)}(\\cdot)$ are both $L$-layer networks shown in \\textbf{Eq.}\\ref{eq_user_model_structure}, the input $\\vect{\\chi}$ can be either the arm $\\vect{x}$ or the network gradient $\\nabla_{\\matr{\\Theta}_{u}^{(1)}}f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$.\n% Then, the weight matrix of the input layer is different for two user networks where $\\matr{\\Theta}_{1}^{(1)} \\in \\mathbb{R}^{m\\times d}$ and $\\matr{\\Theta}_{1}^{(2)} \\in \\mathbb{R}^{m\\times p}$. The rest of the layers will be the same comparing the two kinds of user networks, which are $\\matr{\\Theta}_{l} \\in \\mathbb{R}^{m\\times m}, l\\in [2, \\cdots, L-1]$, and $\\matr{\\Theta}_{L} \\in \\mathbb{R}^{1\\times m}$. \n% For both user networks, the entries of $\\{\\matr{\\Theta}_{1}, \\dots \\matr{\\Theta}_{L-1} \\}$ are drawn from the Gaussian distribution $N(0, 2 / m)$. The entries of the last layer $\\matr{\\Theta}_{L}$ are sampled from $N(0, 1 / m)$.\n\n\n% In round $t$, given the arm set $\\mathcal{X}_{t}$, we construct arm graphs $\\mathcal{G}^{(1)}$, $\\mathcal{G}^{(2)}$ separately based on reward estimations / potential gain estimations from all user networks (lines 3-7, \\textbf{Alg.} \\ref{algo_updating_graph}). \n% Edge weights are calculated with two pre-defined functions $\\Psi^{(1)}(\\cdot, \\cdot), \\Psi^{(2)}(\\cdot, \\cdot): \\mathbb{R} \\times \\mathbb{R} \\mapsto \\mathbb{R}$ (lines 8-13, \\textbf{Alg.} \\ref{algo_updating_graph}).\n\n% --------------------------------------------\n% Remark large number of candidate arms\n% \\begin{remark}\n%     When there exists a large candidate pool of arms, inspired by \\cite{AGG-UCB_qi2022neural}, we can estimate separate user graph pairs for different arm groups, instead of each single arm, to reduce the computational cost. For example, in movie recommendation, we can construct a pair of user exploitation \\& exploration graphs for movies of the same genre.\n% \\label{remark_numerous_arms}\n% \\end{remark}\n\n\n% --------------\n% \\vspace{-0.1cm}\n"
                },
                "subsection 4.2": {
                    "name": "Achieving Exploitation and Exploration with GNN Models on Estimated User Graphs",
                    "content": "\n% \\vspace{-0.1cm}\n\nWith derived user exploitation graphs $\\mathcal{G}_{i, t}^{(1)}$, and exploitation graphs $\\mathcal{G}_{i, t}^{(2)}, i\\in [a]$, we apply two GNN models to separately estimate the arm reward and potential gain for a refined arm selection strategy, by utilizing the past interaction records with all the users. \n\n% --------------\n% \\vspace{-0.1cm}\n",
                    "subsubsection 4.2.1": {
                        "name": "(1)",
                        "content": "     \\label{subsec_GNN_arch}\n% \\vspace{-0.05cm}\nIn round $t$, with the estimated user exploitation graph $\\mathcal{G}_{i, t}^{(1)}$ for arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, we apply the exploitation GNN model $f_{gnn}^{(1)}(\\vect{x}_{i, t}, \\mathcal{G}_{i, t}^{(1)}; \\Theta_{gnn}^{(1)})$ to collaboratively estimate the arm reward $\\widehat{r}_{i, t}$ for the received user $u_{t} \\in \\mathcal{U}$. We start from learning the aggregated representation for $k$ hops, as\n\\begin{equation}\n\\begin{split}\n    \\matr{H}_{agg} = \\sigma\\big((\\matr{S}_{i, t}^{(1)})^{k} \\cdot (\\matr{X}_{i, t} \\matr{\\Theta}_{agg}^{(1)})\\big) \\in \\mathbb{R}^{n \\times m}\n\\end{split}\n\\label{eq_GNN_aggegation}\n\\end{equation}\nwhere \n$\\matr{S}_{i, t}^{(1)} = (\\matr{D}_{i, t}^{(1)})^{-\\frac{1}{2}} \\matr{A}_{i, t}^{(1)} (\\matr{D}_{i, t}^{(1)})^{-\\frac{1}{2}}$ is the symmetrically normalized adjacency matrix of $\\mathcal{G}_{i, t}^{(1)}$, and $\\sigma$ represents the ReLU activation function. \nWith $m$ being the network width, we have $\\matr{\\Theta}_{agg}^{(1)} \\in \\mathbb{R}^{nd\\times m}$ as the trainable weight matrix. After propagating the information for $k$ hops over the user graph, each row of $\\matr{H}_{agg}$ corresponds to the aggregated $m$-dimensional hidden representation for one specific user-arm pair $(u, \\vect{x}_{i, t}), u\\in \\mathcal{U}$. \nHere, the propagation of multi-hop information can provide a global perspective over the users, since it also involves the neighborhood information of users' neighbors \\citep{consistency-2004,SGC_wu2019simplifying}.\nTo achieve this, we have the embedding matrix $\\matr{X}_{i, t}$ (in \\textbf{Eq.} \\ref{eq_GNN_aggegation}) for arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}, i\\in [a]$ being\n\\begin{equation}\n\\matr{X}_{i, t} = \n\\left(\\begin{array}{cccc}\n\\vect{x}_{i, t}^{\\intercal} & \\matr{0} & \\cdots & \\matr{0} \\\\\n\\matr{0} & \\vect{x}_{i, t}^{\\intercal} & \\cdots & \\matr{0} \\\\\n\\vdots  &       & \\ddots   & \\vdots \\\\\n\\matr{0} & \\matr{0} & \\cdots & \\vect{x}_{i, t}^{\\intercal} \\\\\n\\end{array} \\right) \\in \\mathbb{R}^{n\\times nd}\n\\label{eq_new_embedding_matrix}\n\\end{equation}\nwhich partitions the weight matrix $\\matr{\\Theta}_{gnn}^{(1)}$ for different users. In this way, it is designed to generate individual representations w.r.t. each user-arm pair $(u, \\vect{x}_{i, t}), u\\in \\mathcal{U}$ before the $k$-hop propagation (i.e., multiplying with $(\\matr{S}_{i, t}^{(1)})^{k}$), which correspond to the rows of the matrix multiplication $(\\matr{X}_{i, t} \\matr{\\Theta}_{agg}^{(1)}) \\in \\mathbb{R}^{n\\times m}$.\n\n%\nAfterwards, with $\\matr{H}_{0} = \\matr{H}_{agg}$, we feed the aggregated representations into the $L$-layer ($L \\geq 2$) FC network, represented by  \n\\begin{equation}\n\\begin{split}\n    &\\matr{H}_{l} = \\sigma(\\matr{H}_{l-1} \\cdot \\matr{\\Theta}_{l}^{(1)}) \\in \\mathbb{R}^{n\\times m} \n    ,~~ l \\in [L-1], \\\\\n    &\\widehat{\\vect{r}}_{all}(\\vect{x}_{i, t}) = \\matr{H}_{L-1} \\cdot  \\matr{\\Theta}_{L}^{(1)} \\in \\mathbb{R}^{n}\n\\end{split}\n\\label{eq_GNN_estimation}\n\\end{equation}\nwhere $\\widehat{\\vect{r}}_{all}(\\vect{x}_{i, t}) \\in \\mathbb{R}^{n}$ represents the reward estimation for all the users in $\\mathcal{U}$, given the arm $\\vect{x}_{i, t}$. Given the target user $u_{t}$ in round $t$, the reward estimation for the user-arm pair $(u_{t}, \\vect{x}_{i, t})$ would be the corresponding element in $\\widehat{\\vect{r}}_{all}$ (line 8, \\textbf{Alg.} \\ref{algo_main}), represented by: \n\\begin{equation}\n\\begin{split}\n    \\widehat{r}_{i, t} = f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) = [\\widehat{\\vect{r}}_{all}(\\vect{x}_{i, t})]_{u_{t}}\n\\end{split}\n\\label{eq_f_1_user_output}\n\\end{equation}\nwhere $\\matr{\\Theta}_{gnn}^{(1)} = [ \\text{vec}(\\matr{\\Theta}_{agg}^{(1)})^{\\intercal}, \\text{vec}(\\matr{\\Theta}_{1}^{(1)})^{\\intercal}, \\dots, \\text{vec}(\\matr{\\Theta}_{L}^{(1)})^{\\intercal} ]^{\\intercal} \\in \\mathbb{R}^{p}$ represent the trainable parameters of the exploitation GNN model, and we have $[\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}$ being the parameters $\\matr{\\Theta}_{gnn}^{(1)}$ in round $t$ (before training [line 11, \\textbf{Alg.} \\ref{algo_main}]).\nHere, the weight matrix shapes are $\\matr{\\Theta}_{l}^{(1)} \\in \\mathbb{R}^{m\\times m}, l\\in [1, \\cdots, L-1]$, and the $L$-th layer $\\matr{\\Theta}_{L}^{(1)} \\in \\mathbb{R}^{m}$.\n%\n% Meanwhile, we use $\\matr{\\Theta}_{gnn}^{(1)} = \\{\\matr{\\Theta}_{agg}^{(1)}, \\matr{\\Theta}_{1}^{(1)}, \\dots, \\matr{\\Theta}_{L}^{(1)} \\}$\n% to represent the collection of trainable weight matrices for the GNN exploitation model.\n\n\\textbf{[Training $f_{gnn}^{(1)}$ with GD]}\nThe exploitation GNN $f_{gnn}^{(1)}(\\cdot)$ will be trained with GD based on the received records $\\mathcal{P}_{t}$. Then, we apply the quadratic loss function based on reward predictions $\\{f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1)}; \\matr{\\Theta}_{gnn}^{(1)})\\}_{\\tau\\in [t]}$ of chosen arms $\\{\\vect{x}_{\\tau}\\}_{\\tau\\in [t]}$, the actual received rewards $\\{r_{\\tau}\\}_{\\tau\\in [t]}$, and the user exploitation graph $\\mathcal{G}_{\\tau}^{(1)}$ for chosen arms $\\vect{x}_{\\tau}, \\tau\\in [t]$. The  corresponding quadratic loss will be\n$\n    \\mathcal{L}_{gnn}^{(1)}(\\Theta_{gnn}^{(1)}) = \\sum_{\\tau\\in [t]} \\big|f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; \n    \\matr{\\Theta}_{gnn}^{(1)}) - r_{\\tau}\\big|^{2}.\n$\n\n% ==========================\n\\textbf{[Connection with the Reward Function Definition (\\textit{Eq.} \\ref{eq_weighted_reward_func}) and Constraint (\\textit{Eq.} \\ref{eq_assumption_correlation_vector})]} \nThe existing works (e.g.,\\citep{conv_theory-allen2019convergence}) show that the FC network is naturally Lipschitz continuous with respect to the input when width $m$ is sufficiently large.\n%\nThus, for \\name, with aggregated hidden representations $\\matr{H}_{agg}$ being the input to the FC network (\\textbf{Eq.} \\ref{eq_GNN_estimation}), we will have the difference of reward estimations $\\widehat{r}_{i, t}$ bounded by the distance of rows in matrix $\\matr{H}_{agg}$ (i.e., aggregated hidden representations). \n%\nHere, given $\\vect{x}_{i, t}\\in \\mathcal{X}_{t}$ and users $u_{1}, u_{2}\\in \\mathcal{U}$, their estimated reward difference $\\abs{[\\widehat{\\vect{r}}_{all}(\\vect{x}_{i, t})]_{u_{1}} - [\\widehat{\\vect{r}}_{all}(\\vect{x}_{i, t})]_{u_{2}}}$ can be bounded by the distance of the corresponding rows in $\\matr{S}_{i, t}$ (i.e., $\\norm{\\matr{S}_{i, t}^{(1)}[u_{1}:]-\\matr{S}_{i, t}^{(1)}[u_{2}:]}$) given the exploitation GNN model.\n%\nThis design matches our definition and the constraint in \\textbf{Eq.} \\ref{eq_weighted_reward_func}-\\ref{eq_assumption_correlation_vector}. \n\n% ==========================\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "(2)",
                        "content": "\nGiven a candidate arm $\\vect{x}_{i, t}\\in \\mathcal{X}_{t}$, to achieve adaptive exploration with the user exploration collaborations encoded in $\\mathcal{G}^{(2)}_{i, t}$, we apply a second GNN model \n$f_{gnn}^{(2)}(\\cdot)$ to evaluate the potential gain $\\widehat{b}_{i, t}$ for the reward estimation $\\widehat{r}_{i, t} = f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})$ [\\textbf{Eq.} \\ref{eq_f_1_user_output}], denoted by\n\\begin{equation}\n\\begin{split}\n    \\widehat{b}_{i, t} = f_{gnn}^{(2)}( \\nabla [f_{gnn}^{(1)}]_{i, t} , \\mathcal{G}_{i, t}^{(2)};  [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) \n    = [\\widehat{\\vect{b}}_{all}(\\vect{x}_{i, t})]_{u_{t}}.\n\\end{split}\n\\label{eq_f_2_explore_user_output}\n\\end{equation}\nThe architecture of $f_{gnn}^{(2)}(\\cdot)$ can also be represented by \\textbf{Eq.} \\ref{eq_GNN_aggegation}-\\ref{eq_f_1_user_output}. While $f_{gnn}^{(1)}(\\cdot), f_{gnn}^{(2)}(\\cdot)$ have the same network width $m$ and number of layers $L$, the dimensionality of $\\matr{\\Theta}_{agg}^{(1)} \\in \\mathbb{R}^{nd\\times m}, \\matr{\\Theta}_{agg}^{(2)}\\in \\mathbb{R}^{np\\times m}$ is different. Analogously, $\\widehat{\\vect{b}}_{all}(\\vect{x}_{i, t}) \\in \\mathbb{R}^{n}$ is the potential gain estimation for all the users in $\\mathcal{U}$, w.r.t.\narm $\\vect{x}_{i, t}$ and the exploitation GNN $f_{gnn}^{(1)}(\\cdot)$.\n%\nHere, the inputs are user exploration graph $\\mathcal{G}_{i, t}^{(2)}$, and the gradient of the exploitation GNN, represented by $\\nabla [f_{gnn}^{(1)}]_{i, t} = \\nabla_{ \\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{i, t}, \\mathcal{G}_{i, t}^{(1)} ; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1} )$.\n%\nThe exploration GNN $f_{gnn}^{(2)}(\\cdot)$ leverages the user exploration graph $\\mathcal{G}_{i, t}^{(2)}$ and the gradients of $f_{gnn}^{(1)}(\\cdot)$ to estimate the uncertainty of reward estimations, which stands for our adaptive exploration strategy (downward or upward exploration). More discussions are in Appendix \\textbf{Section} \\ref{sec_appd_adaptive_exp}.\n% \\TODO{Update here}\n\n%\n\\textbf{[Training $f_{gnn}^{(2)}$ with GD]}\n% Analogous to previous $f_{gnn}^{(1)}(\\cdot)$, the architecture of $f_{gnn}^{(2)}(\\cdot)$ can also be represented by \\textbf{Eq.} \\ref{eq_GNN_aggegation}-\\textbf{Eq.} \\ref{eq_f_1_user_output}.\n%\n% Note that while $f_{gnn}^{(1)}(\\cdot), f_{gnn}^{(2)}(\\cdot)$ have the same network width $m$ and number of layers $L$, the dimensionality of $\\matr{\\Theta}_{agg}^{(1)} \\in \\mathbb{R}^{nd\\times m}, \\matr{\\Theta}_{agg}^{(2)}\\in \\mathbb{R}^{np\\times m}$ is different. \n%\nSimilar to $f_{gnn}^{(1)}$, we train $f_{gnn}^{(2)}$ with GD by minimizing the quadratic loss, denoted by $\\mathcal{L}_{gnn}^{(2)}(\\Theta_{gnn}^{(2)}) = \\sum_{\\tau\\in [t]} $\n$\n    \\big|f_{gnn}^{(2)}(\\nabla [f_{gnn}^{(1)}]_{\\tau}, \\mathcal{G}_{\\tau}^{(2)}; \\matr{\\Theta}_{gnn}^{(2)}) - \n    \\big(r_{\\tau} - f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})\\big)\n    \\big|^{2}.\n$\nThis is defined to measure the difference between the estimated potential gains $\\{f_{gnn}^{(2)}( \\nabla [f_{gnn}^{(1)}]_{\\tau} , \\mathcal{G}_{\\tau}^{(2)};  \\matr{\\Theta}_{gnn}^{(2)})\\}_{\\tau\\in [t]}$, and the corresponding labels $\\{r_{\\tau} - f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})\\}_{\\tau\\in [t]}$.\n\n\n\n\n% --------------------------------------------\n% Remark gradient dimension reduction\n\\vspace{-0.1cm}\n\\begin{remark}[Reducing Input Complexity]\n    The input of $f_{gnn}^{(2)}(\\cdot)$ is the gradient $\\nabla_{\\matr{\\Theta}} f_{gnn}^{(1)}(\\vect{x})$ given the arm $\\vect{x}$, and its dimensionality is naturally $p = (nd\\times m) + (L-1)\\times m^{2} + m$, which can be a large number. Inspired by CNNs, e.g., \\cite{CNN_avg_pooling_radenovic2018fine}, we apply the \\textbf{average pooling} to approximate the original gradient vector in practice. In this way, we can save the running time and reduce space complexity simultaneously. Note this approach is also compatible with user networks in Subsection \\ref{subsec_user_models}. To prove its effectiveness, we will apply this approach on \\name~for all the experiments in Section \\ref{sec_experiments}.\n\\label{remark_avg_pool}\n\\end{remark}\n\n\n% --------------------------------------------\n% Remark large number of users\n\\vspace{-0.1cm}\n\\begin{remark}[Working with Large Systems]\n    When facing a large number of users, we can apply the ``approximated user neighborhood'' to reduce the running time in practice. \n    Given user graphs $\\mathcal{G}_{i, t}^{(1)}, \\mathcal{G}_{i, t}^{(2)}$ in terms of arm $\\vect{x}_{i, t}$, we derive approximated user neighborhoods $\\widetilde{\\mathcal{N}}^{(1)}(u_{t}),~ \\widetilde{\\mathcal{N}}^{(2)}(u_{t}) \\subset \\mathcal{U}$ for the target user $u_{t}$, with size $\\abs{\\widetilde{\\mathcal{N}}^{(1)}(u_{t})} = \\abs{\\widetilde{\\mathcal{N}}^{(2)}(u_{t})} = \\widetilde{n}$,  where $\\widetilde{n} << n$.\n    For instance, we can choose $\\widetilde{n}$ ``representative users'' (e.g., users posting high-quality reviews on e-commerce platforms) to form $\\widetilde{\\mathcal{N}}^{(1)}(u_{t}), \\widetilde{\\mathcal{N}}^{(2)}(u_{t})$, and apply the corresponding approximated user sub-graphs for downstream GNN models to reduce the computation cost and space cost in practice. Related experiments are provided in Subsection \\ref{subsec_exp_approx_neighborhood}.\n    % Here, we also include complementary discussions for the space and time complexity in Appendix Section \\ref{sec_appx_additional_discussion_on_time_space_complexity}.\n    % \\ChangedForCamera\n\\label{remark_numerous_users}\n\\end{remark}\n\\vspace{-0.1cm}\n\n\n\n% --------------\n\\textbf{[Parameter Initialization]}\nFor the parameters of both GNN models (i.e., $\\matr{\\Theta}_{gnn}^{(1)}$ and $\\matr{\\Theta}_{gnn}^{(2)}$), the matrix entries of the aggregation weight matrix $\\matr{\\Theta}_{agg}$ and the first $L-1$ FC layers $\\{\\matr{\\Theta}_{1}, \\dots, \\matr{\\Theta}_{L-1} \\}$ are drawn from the Gaussian distribution $N(0, 2 / m)$. Then, for the last layer weight matrix $\\matr{\\Theta}_{L}$, we draw its entries from $N(0, 1 / m)$.\n\n\n% --------------------------------------------\n\n"
                    },
                    "subsubsection 4.2.3": {
                        "name": "Arm Selection Mechanism and Model Training",
                        "content": "\nIn round $t$, with the current parameters $[\\Theta_{gnn}^{(1)}]_{t-1}, [\\Theta_{gnn}^{(2)}]_{t-1}$ for GNN models before model training, the selected arm is chosen as \n\\begin{displaymath}\n\\begin{split}\n    \\vect{x}_{t} = & \\arg\\max_{\\vect{x}_{i, t} \\in \\mathcal{X}_{t}} \n    \\bigg[ f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\Theta_{gnn}^{(1)}]_{t-1}) \\\\\n    & + f_{gnn}^{(2)}(\\nabla_{\\Theta_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\Theta_{gnn}^{(1)}]_{t-1}),~\\mathcal{G}_{i, t}^{(2)}; [\\Theta_{gnn}^{(2)}]_{t-1}) \\bigg]\n\\end{split}\n\\end{displaymath}\nbased on the estimated reward and potential gain (line 10, \\textbf{Alg.} \\ref{algo_main}). \nAfter receiving reward $r_{t}$, we update user networks $f_{u_{t}}^{(1)}, f_{u_{t}}^{(2)}$ of user $u_{t}$, and GNN models based on GD (line 11, \\textbf{Alg.} \\ref{algo_main}).\n% and \\textbf{Alg.} \\ref{algo_training} in Appendix Section \\ref{sec_appd_pseudo_code}).\n% \\ChangedForCamera\n\n\n\n\n\n\n\n\n\n\\vspace{-0.1cm}\n% ====================================================\n% \\vspace{-0.2cm}\n\n"
                    }
                }
            },
            "section 5": {
                "name": "Theoretical Analysis",
                "content": "    \\label{sec_theoretical_analysis}\n\n% \\vspace{-0.2cm}\n\n\n% ------------------------------------------------------------\n%  Since the user correlations are unknown to the learner and difficult to analyze without prior assumptions, we present one possible solution to take the collaborative effect into consideration. At round $t$, given the sampled user $u_{t}$ and an arm $\\vect{x}_{i, t}\\in \\mathcal{X}_{t}$, we have the following global reward mapping function $h$ as\n% \\begin{equation}\n% \\begin{split}\n%     r_{i, t}' =  h(\\vect{x}_{i, t}, u_{t}, W_{i, t}) + \\epsilon_{i, t}, \n%     \\quad \\mathbb{E}[ r_{i, t}'| u_{t}, \\vect{x}_{i, t} ] = \\mathbb{E}[ r_{i, t}| u_{t}, \\vect{x}_{i, t} ]\n% \\end{split}\n% \\label{eq_weighted_reward_func}\n% \\end{equation}\n% where $\\matr{W}_{i, t} \\in \\mathbb{R}^{n\\times n}$ is the user affinity matrix that encodes the user preference similarity w.r.t. the arm $\\vect{x}_{i, t}$, and $\\matr{W}_{i, t}[u, :]$ is the row of $\\matr{W}_{i, t}$ that relates to the user $u\\in \\mathcal{U}$. \n% % ------------------------------------------------------------\n% Meantime, given the same arm $\\vect{x}_{i, t}$, we assume expected reward difference between any two users $u, u' \\in \\mathcal{U}$ is bounded as\n% \\begin{displaymath}\n% \\begin{split}\n%     \\abs{h(\\vect{x}_{i, t}, u, W_{i, t}) - h(\\vect{x}_{i, t}, u', W_{i, t})} \\leq \\Psi_{W} (\\matr{W}_{i, t}[u, :], \\matr{W}_{i, t}[u', :])\n% \\end{split}\n% \\end{displaymath}\n% where $\\matr{W}_{i, t}[u, :]$ is the preference similarity vector (i.e., the corresponding row in $W$) of user $u$, and $\\Psi_{W}: \\mathbb{R}^{n}\\times\\mathbb{R}^{n} \\mapsto \\mathbb{R}$ \n% % \\TODO{or add the original arm context + Mentioned this assumption} \n% denotes an unknown mapping from two users' preference similarity vectors to the difference of the rewards given the specific arm.\n% Note that compared with previous linear frameworks (e.g., \\cite{GangOfBandits-cesa2013gang,colla_environ_2016}) which aggregate expected rewards, our assumption is more generic and could be fit into the non-linear settings. And our proposed framework in Section \\ref{sec_proposed_framework} matches this assumption, where we \\TODO{Fill in} \n\n% ------------------------------------------------------------\nIn this section, we present the theoretical analysis for the proposed \\name.\n% To model the user behaviors, we adopt two $L$-layer FC networks with the input $\\vect{\\chi}$ for user networks $f_{u}^{(1)}(\\cdot), f_{u}^{(2)}(\\cdot)$ respectively as \n% \\begin{equation} \n% f_{u}(\\vect{\\chi}; \\matr{\\Theta}_{u} ) = \\matr{\\Theta}_{L} \\sigma ( \\matr{\\Theta}_{L-1}  \\sigma (\\matr{\\Theta}_{L-2} \\dots  \\sigma(\\matr{\\Theta}_{1} \\vect{\\chi}) ))\n% \\label{eq_user_model_structure}\n% \\end{equation}\n% with $\\matr{\\Theta}_{u} = \\{\\matr{\\Theta}_{1}, \\dots, \\matr{\\Theta}_{L}\\}$ and $\\sigma$ being the ReLU activation. \n% Here, since $f_{u}^{(1)}(\\cdot), f_{u}^{(2)}(\\cdot)$ are both $L$-layer networks shown in \\textbf{Eq.}\\ref{eq_user_model_structure}, the input $\\vect{\\chi}$ can be either the arm $\\vect{x}$ or the network gradient $\\nabla_{\\matr{\\Theta}_{u}^{(1)}}f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$.\n% Then, the weight matrix of the input layer is different for two user networks where $\\matr{\\Theta}_{1}^{(1)} \\in \\mathbb{R}^{m\\times d}$ and $\\matr{\\Theta}_{1}^{(2)} \\in \\mathbb{R}^{m\\times p}$. The rest of the layers will be the same comparing the two kinds of user networks, which are $\\matr{\\Theta}_{l} \\in \\mathbb{R}^{m\\times m}, l\\in [2, \\cdots, L-1]$, and $\\matr{\\Theta}_{L} \\in \\mathbb{R}^{1\\times m}$. \n% Comparably, for both user networks, the entries of $\\{\\matr{\\Theta}_{1}, \\dots \\matr{\\Theta}_{L-1} \\}$ are drawn from the Gaussian distribution $N(0, 2 / m)$. The entries of the last layer $\\matr{\\Theta}_{L}$ are sampled from $N(0, 1 / m)$.\n%\nHere, we consider each user $u\\in \\mathcal{U}$ to be evenly served $T / n$ rounds up to time step $T$, i.e., $\\abs{\\mathcal{T}_{u, t}} = T_{u, t} = T / n$, which is standard in closely related works \n(e.g., \\cite{club_2014,local_clustering-ban2021local}). \nTo ensure the neural models are able to efficiently learn the underlying reward mapping, we have the following assumption regarding the arm separateness.\n\n%\n% Then, in each round, given the serving user $u_t$, we assume each arm $\\vect{x}_{i,t}, i \\in [a]$ is independently drawn from the data distribution $\\mathcal{D}$. And many existing user clustering algorithms under the stochastic bandit setting also share our assumption of independent sampled arms, e.g., \\cite{club_2014,SCLUB_li2019improved,CAB_2017,Meta-Ban}. \n% \\TODO{Compared with existing clustering of bandits algorithms (e.g., \\cite{club_2014,SCLUB_li2019improved,CAB_2017,Meta-Ban}), we remove the arm i.i.d. assumption  }\n\n\n\n% ---------------\n% \\begin{assumption} [Distribution of Arms and Rewards]  \\label{assumption_distribution}\n% All arms and rewards are assumed to be drawn i.i.d. from the general distribution $\\mathcal{D}$. In particular, the user $u_t$ and its data distribution $\\mathcal{D}_{u_t}$ with respect to $u_t$ are drawn from $\\mathcal{D}$, i.e., $(u_t, \\mathcal{D}_{u_t} ) \\sim \\mathcal{D}$. Then, at time step $t$, $\\forall i \\in [a]$, the arm and reward $(\\vect{x}_{i,t}, r_{i, t})$ with respect to $u_t$ are drawn from the data distribution $\\mathcal{D}_{u_t}$, namely $(\\vect{x}_{i, t}, r_{i, t}) \\sim \\mathcal{D}_{u_t}$.\n% \\end{assumption}\n\n% Here, at each time step $t$, the sampled arm-reward pair $(\\vect{x}_{i, t}, r_{i, t}), i\\in [a]$ and the user $u_{t}$ could be naturally deemed as one randomly sampled information tuple, denoted by $(\\vect{x}_{i, t}, r_{i, t}, u_{t}) \\sim \\mathcal{D}$. \\TODO{(1) Combine into an overall distribution for generalization results.}\n% And the true arm graphs $\\mathcal{G}^{(1), *}_{i, t}. \\mathcal{G}^{(2), *}_{i, t}$ would be derived. \n\n% ---------------\n\n% \\begin{assumption} [Distribution of Arms and Rewards]  \\label{assumption_distribution}\n% All arms and rewards are assumed to be drawn i.i.d. from the general distribution $\\mathcal{D}$. In particular, at time step $t$, $\\forall i \\in [a]$, the arm and reward $(\\vect{x}_{i,t}, r_{i, t})$ along with the sampled user $u_t$ are drawn from the data distribution $\\mathcal{D}$, namely $(\\vect{x}_{i, t}, r_{i, t}, u_{t}) \\sim \\mathcal{D}$.\n% \\end{assumption}\n\n\n% Here, at each time step $t$, the arm-reward pair $(\\vect{x}_{i, t}, r_{i, t})$ from the sampled tuple $(\\vect{x}_{i, t}, r_{i, t}, u_{t}) \\sim \\mathcal{D}$ could also be considered as $(\\vect{x}_{i, t}, r_{i, t}) \\sim \\mathcal{D}_{u_{t}}$, where $\\mathcal{D}_{u_{t}}$ is the data distribution conditioned on sampled user $u_{t}$. And below is our assumption on arm separateness. \\TODO{check}\n\n\n\n% And the true arm graphs $\\mathcal{G}^{(1), *}_{i, t}. \\mathcal{G}^{(2), *}_{i, t}$ would be derived. \n\n% \\TODO{(1) Combine into an overall distribution for generalization results.}\n\n% ---------------\n\\begin{assumption} [$\\rho$-Separateness of Arms]  \\label{assumption_separateness}\nAfter a total of $~T$ rounds, for every pair $\\vect{x}_{i, t}, \\vect{x}_{i', t'}$ with $t, t' \\in[T]$ and $i, i' \\in [a]$,  if $(t, i) \\neq (t', i')$, we have $\\norm{\\vect{x}_{i, t} - \\vect{x}_{i', t'}}_{2} \\geq \\rho$ where $0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$.\n\\end{assumption}\n% \\begin{assumption} [$\\rho$-Separateness of Arms]  \\label{assumption_separateness}\n% After a total of $~T$ rounds, for every pair of chosen arms $(\\vect{x}_{t}, \\vect{x}_{t'})$ with $t, t' \\in[T]$ ,  if $t \\neq t'$, we have $\\norm{\\vect{x}_{t} - \\vect{x}_{t'}}_{2} \\geq \\rho$ where $0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$.\n% \\end{assumption}\n%\nNote that the above assumption is mild, and it has been commonly applied in existing works on neural bandits \\citep{EE-Net_ban2021ee} and over-parameterized neural networks \\citep{conv_theory-allen2019convergence}. \n%\nSince $L$ can be manually set (e.g., $L = 2$), we can easily satisfy the condition $ 0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$ as long as no two arms are identical. \n%\nMeanwhile, Assumption 4.2 in \\cite{Neural-UCB} and Assumption 3.4 from \\cite{neural_thompson-zhang2020neural} also imply that no two arms are the same, and they measure the arm separateness in terms of the minimum eigenvalue $\\lambda_{0}$ (with $\\lambda_{0} > 0$) of the Neural Tangent Kernel (NTK) \\citep{NTK_jacot2018neural} matrix, which is comparable with our Euclidean separateness $\\rho$.\n%Here, as long as we have a finite number of rounds $T$ and no duplicate arms, there will exist $\\rho > 0$ such that the arms received will satisfy the separateness condition. \n%\nBased on \\textbf{Def.} \\ref{def_exploitation_simi} and \\textbf{Def.} \\ref{def_exploration_simi}, given an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, we denote the adjacency matrices as $\\matr{A}^{(1), *}_{i, t}$ and $\\matr{A}^{(2), *}_{i, t}$ for the true arm graphs $\\mathcal{G}^{(1), *}_{i, t}$, $\\mathcal{G}^{(2), *}_{i, t}$.\nFor the sake of analysis, given any adjacency matrix $\\matr{A}$, \nwe derive the normalized adjacency matrix $\\matr{S}$ by scaling the elements of $\\matr{A}$ with $1 / n$. \nWe also set the propagation parameter $k=1$, and define the mapping functions $\\Psi^{(1)}(a, b), \\Psi^{(2)}(a, b) := \\exp( -\\norm{ a - b } )$ given the inputs $a, b$. Note that our results can be readily generalized to other mapping functions with the Lipschitz-continuity properties.\n\n\n% =======================================================\n\n\n\n\n% ============================================================================================\n\n\n% \\TODO{Target: To prove that when the users are similar ($\\matr{A}_{t}$ is identity matrix / all-one matrix), the regret bound will be improved.}\n\n\n\n\n% Here, based on the arm embedding method illustrated in \\textbf{Eq.} \\ref{eq_new_embedding_matrix}, we consider the input of the exploitation GNN model $f_{gnn}^{(1)}(\\cdot)$ as the aggregated arm context over the user neighborhood w.r.t. one user-arm pair $(u_{\\tau}, \\vect{x}_{\\tau})$, represented as\n% \\begin{displaymath}\n% \\begin{split}\n%     \\widetilde{\\matr{X}}_{\\tau} = \\bigg( (w_{1}\\cdot \\vect{x}_{\\tau}^{\\intercal}), (w_{2}\\cdot \\vect{x}_{\\tau}^{\\intercal}), \\dots, (w_{n}\\cdot \\vect{x}_{\\tau}^{\\intercal}) \\bigg)^{\\intercal} \\in \\mathbb{R}^{d_{\\Tilde{x}}}\n% \\end{split}\n% \\end{displaymath}\n% which is the concatenation of the original arm feature $\\vect{x}_{\\tau}$ under the influence of the user similarities. And $w_{i}, i\\in [n]$ is the similarity score of the user pair $(u_{\\tau}, u_{i})$ . \n% And the network parameters are trained on these embedded contexts and their rewards $(\\widetilde{\\matr{X}}_{\\tau}, r_{\\tau})$.\n% Receiving two users $u_{1} = u_{\\tau_{1}}, u_{2} = u_{\\tau_{2}}$ from previous time steps $\\tau_{1}, \\tau_{2} \\in [t]$, we denote the embedding for pulled arms and rewards from these rounds as $(\\widetilde{\\matr{X}}_{\\tau_{1}}, r_{\\tau_{1}}), (\\widetilde{\\matr{X}}_{\\tau_{2}}, r_{\\tau_{2}})$ individually.   \n\n% At each time step $t$, since the model parameter $\\matr{\\Theta}_{gnn}^{(1)}$ is trained on the $\\{(\\widetilde{\\matr{X}}_{\\tau_{1}}, r_{\\tau_{1}})\\}_{\\tau\\in [t-1]}$, we could have the separateness for the embedded arm contexts as\n% \\begin{displaymath}\n% \\begin{split}\n%     \\widetilde{\\rho} = \\min_{\\tau_{1}, \\tau_{2}\\in [t-1]} \\norm{\\widetilde{\\matr{X}}_{\\tau_{1}} - \\widetilde{\\matr{X}}_{\\tau_{2}}}_{2}\n% \\end{split}\n% \\end{displaymath}\n% which is the minimum euclidean distance among all the embedded contexts. Recall the \\textbf{Assumption} \\ref{assumption_separateness} where we have separateness for the original arms $\\vect{x}_{\\tau}$, with $\\norm{\\vect{x}_{\\tau}}_{2} = 1$, that given any two $\\vect{x}_{\\tau}, \\vect{x}_{\\tau'}, \\forall \\tau, \\tau' \\in [t-1]$, their distance is lower bounded by $\\norm{\\vect{x}_{\\tau} - \\vect{x}_{\\tau}'}_{2} \\geq \\rho$. Therefore, by simple calculation, we consider two extreme cases: (1) When all the users are different (i.e., $\\matr{A} = \\matr{I}$), we have the separateness for the embedded arms $\\widetilde{\\rho} = \\sqrt{2}$; (2) Then, if all users are intrinsically similar (i.e., $\\matr{A}$ is an all-one matrix), we will have  $\\widetilde{\\rho} = \\rho$. Therefore, based on \\blemma \\ref{lemma_user_f_1_est_error}, it shows that when $\\rho \\in (\\sqrt{2}, 2]$, our embedding of arm contexts could make the training samples more ``separate\" and thus improve the generalization ability of the model. Meantime, regarding the conclusion from \\blemma \\ref{lemma_convergence_user_f_1}, it will also require less iterations to train the model when the users are similar to each other.\n\n\n% On the other hand, if we get rid of the restriction that $\\norm{\\widetilde{\\matr{X}}_{\\tau}}_{2}=1$, another form of normalization could be applied where $\\matr{S}_{t}[i, j] = \\matr{A}_{t}[i, j] / \\sqrt{n}$. Then, by simply calculation, we have the separateness for the two extreme cases as: (1) When all the users are different (i.e., $\\matr{A} = \\matr{I}$), we have the separateness for the embedded arms $\\widetilde{\\rho} = \\sqrt{\\frac{2}{n}}$; (2) Then, if all users are intrinsically similar (i.e., $\\matr{A}$ is an all-one matrix), we will have  $\\widetilde{\\rho} = \\rho$.\n\n\n% \\TODO{Map the arms into the hidden space first, through fixed linear transformation}\n\n\n% Recall that by the definition of our GNN layer in \\textbf{Eq.} \\ref{eq_GNN_aggegation}, when given a specific arm $\\vect{x}$, the multiplication $\\matr{X}\\cdot  \\matr{\\Theta}_{agg}^{(1)}$ generates the hidden representations $h_{u}\\in \\mathbb{R}^{m}$ w.r.t. each user-arm pair $(\\vect{x}, u), \\forall u \\in \\mathcal{U}$. The idea is to map the original arm $\\vect{x}$ to the user-specific hidden space with linear transformations to obtain different dense hidden representations for each user-arm pair. \n\n% For the sake of analysis, instead of constructing the embedding matrix for each arm as in \\textbf{Eq.}\\ref{eq_new_embedding_matrix}, we first generate a hidden representation for each user-arm pair $(u, \\vect{x}), \\forall u\\in \\mathcal{U}$ given an arbitrary arm $\\vect{x} \\in \\mathbb{R}^{d}$.\n% For each user $u\\in \\mathcal{U}$, we consider a fixed linear transformation matrix $\\matr{W}_{u}\\in \\mathbb{R}^{d\\times m}$ \\TODO{or $\\mathbb{R}^{d\\times d}$} with the Hermitian property. And when given an arm $\\vect{x}$, for each user-arm pair $(\\vect{x}, u)$, we generate the embedding as $h_{u} = (\\vect{x}^{\\intercal}\\cdot \\matr{W}_{u})^{\\intercal} \\in \\mathbb{R}^{m}$ that satisfies: (1) $\\norm{h_{u}}_{2} = 1$; (2) For a given arm $\\vect{x}$ and any two users $u, u' \\in \\mathcal{U}$, we have $\\norm{h_{u} - h_{u'}}_{2} \\geq \\rho$.\n\n\n% Based on our assumption that each user is served $t / n$ times, the minimum separateness of data would be the transformed selected arms from the same user served at different time steps.\n% Here, up to round $t$, we consider two scenarios:\n% (1) When all the users are different (i.e., $\\matr{A} = \\matr{I}$), the minimum separateness among all the historical selected arms of user $u$ could be represented by \n% \\begin{displaymath}\n% \\begin{split}\n%     \\norm{\\vect{x}_{\\tau}^{\\intercal}\\cdot \\matr{W}_{u} - \\vect{x}_{\\tau'}^{\\intercal}\\cdot \\matr{W}_{u}}_{2} \n%     = \\norm{ (\\vect{x}_{\\tau} - \\vect{x}_{\\tau'})^{\\intercal}\\cdot \\matr{W}_{u} }_{2}\n%     \\leq \\norm{\\vect{x}_{\\tau} - \\vect{x}_{\\tau'}}_{2} \\norm{\\matr{W}_{u}}_{2} \\leq \\norm{\\vect{x}_{\\tau} - \\vect{x}_{\\tau'}}_{2}\n% \\end{split}\n% \\end{displaymath}\n% with $\\tau, \\tau\\in \\mathcal{T}_{u, t}$. Then, as for the lower bound, we have\n% \\begin{displaymath}\n% \\begin{split}\n%     \\norm{\\vect{x}_{\\tau}^{\\intercal}\\cdot \\matr{W}_{u} - \\vect{x}_{\\tau'}^{\\intercal}\\cdot \\matr{W}_{u}}_{2} \n%     = \\norm{ (\\vect{x}_{\\tau} - \\vect{x}_{\\tau'})^{\\intercal}\\cdot \\matr{W}_{u} }_{2}\n%     \\geq \\rho\\cdot \\sigma_{\\min}(\\matr{W}_{u}).\n% \\end{split}\n% \\end{displaymath}\n% where $\\sigma_{\\min}(\\cdot)$ represents the minimum eigenvalue of a given matrix.\n% \\TODO{Add a lemma for this conclusion}\n\n% (2) On the other hand, if all users are intrinsically similar (i.e., $\\matr{A}$ is an all-one matrix), with normalized edge weights equal to $\\frac{1}{\\sqrt{n}}$, we will have \n% \\begin{displaymath}\n% \\begin{split}\n%     \\norm{ (\\vect{x}_{\\tau} - \\vect{x}_{\\tau'})^{\\intercal}\\cdot (\\frac{1}{\\sqrt{n}}\\sum_{u'\\in\\mathcal{U}}\\matr{W}_{u'}) }_{2} \\leq\n%     \\norm{ \\vect{x}_{\\tau} - \\vect{x}_{\\tau'}}_{2}\\cdot \\norm{ \\frac{1}{\\sqrt{n}}\\sum_{u'\\in\\mathcal{U}}\\matr{W}_{u'} }_{2} \n%     \\leq \\sqrt{n} \\norm{ \\vect{x}_{\\tau} - \\vect{x}_{\\tau'}}_{2}\n% \\end{split}\n% \\end{displaymath}\n\n\n\n\n\n\n% with $\\tau, \\tau\\in \\mathcal{T}_{u, t}$. And we also have\n% \\begin{displaymath}\n% \\begin{split}\n%     \\norm{ \\vect{x}_{\\tau} - \\vect{x}_{\\tau'}}_{2}\\cdot \\norm{ \\frac{1}{\\sqrt{n}}\\sum_{u'\\in\\mathcal{U}}\\matr{W}_{u'} }_{2} \n%     & = \\norm{ \\vect{x}_{\\tau} - \\vect{x}_{\\tau'}}_{2}\\cdot \\sigma_{\\max}( \\frac{1}{\\sqrt{n}}\\sum_{u'\\in\\mathcal{U}}\\matr{W}_{u'} ) \\\\\n%     & \\leq \\norm{ \\vect{x}_{\\tau} - \\vect{x}_{\\tau'}}_{2}\\cdot \\frac{1}{\\sqrt{n}} \\cdot \\sum_{u'\\in\\mathcal{U}}\\sigma_{\\max}(\\matr{W}_{u'} )\n% \\end{split}\n% \\end{displaymath}\n% by applying the the Weyl's inequality.\n% Meantime, for the lower bound, it leads to\n% \\begin{displaymath}\n% \\begin{split}\n%     \\norm{ (\\vect{x}_{\\tau} - \\vect{x}_{\\tau'})^{\\intercal}\\cdot (\\frac{1}{\\sqrt{n}}\\sum_{u'\\in\\mathcal{U}}\\matr{W}_{u'}) }_{2} \n%     \\geq \\norm{\\vect{x}_{\\tau} - \\vect{x}_{\\tau'}}\\cdot \\abs{ \\sigma_{\\min}(\\frac{1}{\\sqrt{n}}\\cdot \\sum_{u'\\in\\mathcal{U}}\\matr{W}_{u'}) } \n%     \\geq \\frac{\\rho}{\\sqrt{n}}\\cdot \\sum_{u'\\in\\mathcal{U}}\\sigma_{\\min}(\\matr{W}_{u'}).\n% \\end{split}\n% \\end{displaymath}\n% where the first inequality is due to \\TODO{Lemma}, and the second inequality is by applying the Weyl's inequality again.\n\n% \\TODO{(1)  $\\norm{\\vect{x}_{\\tau}^{\\intercal}\\cdot \\matr{W}_{u} - \\vect{x}_{\\tau'}^{\\intercal}\\cdot \\matr{W}_{u}}_{2}$ vs.\n%             $\\norm{\\vect{x}_{\\tau}^{\\intercal}\\cdot \\matr{W}_{u} - \\vect{x}_{\\tau'}^{\\intercal}\\cdot \\matr{W}_{u'}}_{2}$ }\n\n% \\TODO{(2)  Symmetric + Orthogonal matrix (Householder transformation)}\n\n% % -------------------------------------------------------------\n% \\subsubsection{Error Bounds of User Similarity Estimations}\n\n\n\n\n\n\n\n\n% % -------------------------------------------------------------\n% \\subsubsection{Error Bounds of Parameter Estimations}\n\n\n\n\n\n\n% =======================================================\n%\\subsection{Regret Analysis}\n\nNext, we proceed to show the regret bound $R(T)$ after $T$ time steps [\\textbf{Eq.} \\ref{eq_pseudo_regret}]. \nHere, the following \\btheorem \\ref{theorem_regret_bound} covers two types of error: (1) the estimation error of user graphs; and (2) the approximation error of neural models. \nLet $\\eta_{1}, J_{1}$ be the learning rate and GD training iterations for user networks, and $\\eta_{2}, J_{2}$ denote the learning rate and iterations for GNN models.\nThe proof sketch of \\btheorem \\ref{theorem_regret_bound} is presented in Appendix \\textbf{Section} \\ref{sec_appd_regre_bound_proof}.\n\\begin{theorem}[Regret Bound]\nDefine $\\delta \\in (0, 1)$, $0 < \\xi_{1}, \\xi_{2} \\leq \\mathcal{O}(1 / T)$ and $0 < \\rho \\leq \\mathcal{O}(1 / L)$, $c_{\\xi} > 0$, $\\xi_{L} = (c_{\\xi})^{L}$. \nWith the user networks defined in \\textbf{Eq.} \\ref{eq_user_model_structure} and the GNN models defined in \\textbf{Eq.} \\ref{eq_GNN_aggegation}$-$\\ref{eq_GNN_estimation} with $L$ FC-layers, let $m \\geq \\Omega \\big( \\text{Poly}(T, L, a, \\frac{1}{\\rho}) \\cdot \\xi_{L}\\log(1 / \\delta) \\big) $, $n \\geq \\widetilde{\\Omega} \\big(\\text{Poly}(L)\\big)$.\n% With the GD training process in \\textbf{Algorithm} \\ref{algo_training} (Appendix \\textbf{Section} \\ref{sec_appd_pseudo_code}), set parameters\n% \\ChangedForCamera\nSet the learning rates and GD iterations\n\\vspace{-0.2cm}\n\\begin{displaymath}\n\\begin{split}\n    & \\eta_{1} = \\Theta \\big( \\frac{\\rho}{m\\cdot \\text{Poly}(T, n, a, L)} \\big), \\quad\n    \\eta_{2} = \\Theta \\big( \\frac{\\rho}{m\\cdot \\text{Poly}(T, a, L)} \\big), \\\\\n    & J_{1} = \\Theta \\big( \\frac{\\text{Poly}(T, n, a, L)}{\\rho\\cdot \\delta^{2}} \\cdot \\log(\\frac{1}{\\xi_{1}}) \\big), ~~\n    J_{2} = \\Theta \\big( \\frac{\\text{Poly}(T, a, L)}{\\rho\\cdot \\delta^{2}} \\cdot \\log(\\frac{1}{\\xi_{2}}) \\big).\n\\end{split}\n\\end{displaymath}\nThen, following \\textbf{Algorithm} \\ref{algo_main}, with probability at least $1 - \\delta$, the $T$-round pseudo-regret $R(T)$ of \\name\\ can be bounded by\n\\vspace{-0.0cm}\n\\begin{displaymath}\n\\begin{split}\n    R(T) \\leq  \\sqrt{T} \\cdot  \\big( \n        \\mathcal{O}(L\\xi_{L})  \\cdot \\sqrt{2\\log(\\frac{Tn\\cdot a}{\\delta})}\n         \\big) +  \\sqrt{T}\\cdot \\mathcal{O}(L) + \\mathcal{O}(\\xi_{L}) + \\mathcal{O}(1).\n\\end{split}\n\\end{displaymath}\n% where $n$ is the number of users, and $\\abs{\\mathcal{X}_{t}} = a, \\forall t\\in [T]$ is the number of candidate arms for each round.\n\\label{theorem_regret_bound}\n\\end{theorem}\n%\n\\vspace{-0.3cm}\n% \nRecall that $L$ is generally a small integer (e.g., we set $L=2$ for experiments in \\textbf{Section} \\ref{sec_experiments}), which makes the condition on number of users reasonable as $n$ is usually a gigantic number in real-world recommender systems. We also have $m$ to be sufficiently large under the over-parameterization regime, which makes the regret bound hold. Here, we have the following remarks.\n%\n\\vspace{-0.15cm}\n\\begin{remark} [Dimension terms $d, \\tilde{d}$]\n    Existing neural single-bandit (i.e., with no user collaborations) algorithms \\citep{Neural-UCB,neural_thompson-zhang2020neural,AGG-UCB_qi2022neural} keep the bound of  $\\mathcal{O}(\\Tilde{d} \\sqrt{T}\\log(T))$ based on gradient mappings and ridge regression. $\\Tilde{d}$ is the effective dimension of the NTK matrix, which can grow along with the number of parameters $p$ and rounds $T$. \n    The linear user clustering algorithms (e.g., \\cite{SCLUB_li2019improved,local_clustering-ban2021local,CAB_2017}) have the bound $\\mathcal{O}(d \\sqrt{T}\\log(T))$ with context dimension $d$, which can be large with a high-dimensional context space. Alternatively, the regret bound in \\textbf{Theorem} \\ref{theorem_regret_bound} is free of terms $d$ and $\\Tilde{d}$, as we apply the generalization bounds of over-parameterized networks instead \\citep{conv_theory-allen2019convergence,generalization_bound_cao2019generalization}, which are unrelated to dimension terms $d$ or $\\Tilde{d}$.\n\\label{remark_dimension_term}\n\\end{remark}\n\n\\vspace{-0.2cm}\n\\begin{remark} [From $\\sqrt{n}$ to $\\sqrt{\\log(n)}$]\n    % While our $\\mathcal{O}(\\sqrt{T\\log(T)})$ bound matches theoretical bound of state-of-the-art EE-Net \\citep{EE-Net_ban2021ee}, EE-Net only considers the single-bandit setting with no collaboration among users. \n    % Compared with Meta-Ban \\citep{Meta-Ban}, we provide the theoretical analysis from a new perspective regarding the fine-grained user collaborative effect and GNNs. \n    With $n$ being the number of users, existing user clustering works (e.g., \\cite{Meta-Ban,club_2014,SCLUB_li2019improved,local_clustering-ban2021local}) involve a $\\sqrt{n}$ factor in the regret bound as the cost of leveraging user collaborative effects. Instead of applying separate estimators for each user group, our proposed \\name\\ only ends up with a $\\sqrt{\\log(n)}$ term to incorporate user collaborations by utilizing dual GNN models for estimating the arm rewards and potential gains correspondingly.\n\\label{remark_bound_sqrt_n}\n\\end{remark}\n\n\\vspace{-0.2cm}\n\\begin{remark} [Arm i.i.d. Assumption]\n   Existing clustering of bandits algorithms (e.g., \\cite{club_2014,SCLUB_li2019improved,CAB_2017,Meta-Ban}) and the single-bandit algorithm EE-Net \\citep{EE-Net_ban2021ee} typically require the arm i.i.d. assumption for the theoretical analysis, which can be strong since the candidate arm pool $\\mathcal{X}_{t}, t\\in [T]$ is usually conditioned on the past records. Here, instead of using the regression-based analysis as in existing works, our proof of \\btheorem \\ref{theorem_regret_bound} applies the martingale-based analysis instead to help alleviate this concern.\n\\label{remark_bound_iid}\n\\end{remark}\n\n\n\n\n% Existing neural single-bandit (i.e., with no user collaboration) algorithms \\cite{Neural-UCB,neural_thompson-zhang2020neural} derive the bound $\\mathcal{O}(\\Tilde{d} \\sqrt{T}\\log(T))$ based on neural gradient mappings and ridge regression, and it involves the effective dimension term $\\Tilde{d}$ of the NTK matrix \\cite{NTK_jacot2018neural} which could grow along with the scale of network parameters and number of rounds $T$. \n% Meanwhile, the user clustering algorithms under linear settings (e.g., \\cite{SCLUB_li2019improved,local_clustering-ban2021local,CAB_2017}) have the bound $\\mathcal{O}(d \\sqrt{T}\\log(T))$ and there exists the term of arm context dimension $d$, which might lead to large regret bounds given arm contexts in the high-dimensional space. \n% Compared with them, we improve their bounds by a multiplicative factor of $\\sqrt{\\log(T)}$ as well as remove the dimension term $d$ and $\\Tilde{d}$ \n% We achieve this by applying the generalization bound for over-parameterized neural networks \\cite{conv_theory-allen2019convergence,generalization_bound_cao2019generalization} instead of regression-based analysis to remove the $\\sqrt{\\log(T)}$ term, and the generalization error is also unrelated to $d$ or $\\Tilde{d}$ for over-parameterized neural networks.\n\n% --------------------------------------------\n\n% \\begin{remark} [Removing $d, \\tilde{d}$ Terms]\n%     Existing neural single-bandit (i.e., with no user collaboration) algorithms \\citep{Neural-UCB,neural_thompson-zhang2020neural} derive the bound $\\mathcal{O}(\\Tilde{d} \\sqrt{T}\\log(T))$ based on neural gradient mappings and ridge regression, and they involve the effective dimension term $\\Tilde{d}$ of the NTK matrix, which can grow along with the scale of network parameters and number of rounds $T$. \n%     The linear user clustering algorithms (e.g., \\cite{SCLUB_li2019improved,local_clustering-ban2021local,CAB_2017}) have the bound $\\mathcal{O}(d \\sqrt{T}\\log(T))$ with the term of arm dimension $d$, which can be large given arm contexts in the high-dimensional space. \n%     Here, we improve their bounds by a multiplicative factor of $\\sqrt{\\log(T)}$ and remove the dimension terms $d$, $\\Tilde{d}$.\n%     We apply the generalization bound for over-parameterized neural networks \\citep{conv_theory-allen2019convergence,generalization_bound_cao2019generalization} instead of regression-based analysis to remove the $\\sqrt{\\log(T)}$ term, and the generalization error is also unrelated to $d$ or $\\Tilde{d}$ for over-parameterized neural networks.\n% \\label{remark_bound_dimension}\n% \\end{remark}\n\n\n\n\n% --------------------------------------------\n\n\n\n\n\n% \\textbf{[Time complexity]}\n% Suppose we have a total of $n$ users.\n% In each round $t\\in [T]$, we will need to iterate through user models and construct the user graphs. Originally, we will need $O(n)$ time to calculate the output of user models, and $O(n^{2})$ kernel evaluations on scalar values to calculate the exploitation/exploration scores for user pairs based on the user model output. The kernel evaluation of scalar values is fast, and the whole can be highly paralleled by using multiple machines where each machine is in charge of multiple users. \n% After adopting $\\widetilde{n}$ ($\\widetilde{n} << n$) representative users instead to approximately model the fine-grained user correlations (Remark 3.2), we will then need $O(\\widetilde{n})$ time to calculate the output of user models, and $O(\\widetilde{n}^{2})$ time to calculate the exploitation/exploration scores for user pairs with kernel evaluations. In this way, the time complexity can be significantly reduced.\n\n\n% \\textbf{[Space complexity]}\n% After adopting $\\widetilde{n}$ representative users, our space complexity is no larger than $ 2T\\cdot \\widetilde{n}^{2}  + O(\\widetilde{n}\\cdot p\\cdot T) + O(n\\cdot p)$, and applying the approximation method in Remark 3.1 can further significantly reduce the space complexity. Here, the first term $2T\\cdot \\widetilde{n}^{2}$ is the cost of storing past user graph adjacency matrices of selected arms, the second term is for storing past contexts and network gradients, and the third term is the cost of storing individual user models.\n% %\n% To model user behaviors, each user is assigned with a separate neural single-bandit model with no user collaborations \\cite{Neural-UCB,AGG-UCB_qi2022neural,neural_thompson-zhang2020neural}. For neural algorithms using the UCB \\cite{Neural-UCB,AGG-UCB_qi2022neural} and Thompson sampling \\cite{neural_thompson-zhang2020neural} to achieve exploration, they need the space complexity of $O(n\\cdot p^{2})$ ($p$ is the number of parameters in the neural model, which is a considerably large number) to store their gigantic gradient matrices, which demands large amount of running memory especially when the neural model is large. EE-Net \\cite{EE-Net_ban2021ee} has the space complexity of $O(n\\cdot p\\cdot T)$ while no user collaborations are involved.\n\n\n\n\n\n\n\n\n\\vspace{-0.1cm}\n% ====================================================\n% \\vspace{-0.3cm}\n\\vspace{-0.2cm}\n"
            },
            "section 6": {
                "name": "Experiments",
                "content": "   \\label{sec_experiments}\n\n% \\vspace{-0.1cm}\n\nIn this section, we evaluate the proposed \\name\\ framework on multiple real data sets against nine state-of-the-art algorithms, including\n% \\textbf{CLUB} \\citep{club_2014}, \\textbf{SCLUB} \\citep{SCLUB_li2019improved}, \\textbf{LOCB} \\citep{local_clustering-ban2021local}, \\textbf{DynUCB} \\citep{Dyn-UCB_nguyen2014dynamic}, \\textbf{COFIBA} \\citep{co_filter_bandits_2016}, \\textbf{Neural-UCB-Pool (Neural-Pool)} \\citep{Neural-UCB}, \\textbf{Neural-UCB-Ind (Neural-Ind)} \\citep{Neural-UCB}, \\textbf{EE-Net} \\citep{EE-Net_ban2021ee}, and \\textbf{Meta-Ban} \\citep{Meta-Ban}.\n% We will include the descriptions for the baselines in the Appendix Section \\ref{sec_appd_experiments}.\n% The descriptions for our nine baseline methods are:\nthe linear user clustering algorithms: (1) \\textbf{CLUB} \\citep{club_2014}, (2) \\textbf{SCLUB} \\citep{SCLUB_li2019improved}, (3) \\textbf{LOCB} \\citep{local_clustering-ban2021local}, (4) \\textbf{DynUCB} \\citep{Dyn-UCB_nguyen2014dynamic}, (5) \\textbf{COFIBA} \\citep{co_filter_bandits_2016}; the neural single-bandit algorithms: (6) \\textbf{Neural-Pool} adopts one single Neural-UCB \\citep{Neural-UCB} model for all the users with the UCB-type exploration strategy; (7) \\textbf{Neural-Ind} assigns each user with their own separate Neural-UCB \\citep{Neural-UCB} model; (8) \\textbf{EE-Net} \\citep{EE-Net_ban2021ee}; and, the neural user clustering algorithm: (9) \\textbf{Meta-Ban} \\citep{Meta-Ban}.\nWe leave the implementation details and data set URLs to Appendix \\textbf{Section} \\ref{sec_appd_experiments}.\n% \\begin{itemize}[leftmargin=*]\n%     \\item \\textbf{CLUB} \\citep{club_2014} regards connected components as user groups out of the estimated user graph, and adopts a UCB-type exploration strategy;\n%     \\item \\textbf{SCLUB} \\citep{SCLUB_li2019improved} estimates dynamic user sets as user groups, and allows set operations for group updates;\n%     \\item \\textbf{LOCB} \\citep{local_clustering-ban2021local} applies soft-clustering among users with random seeds and choose the best user group for reward and confidence bound estimations;\n%     \\item \\textbf{DynUCB} \\citep{Dyn-UCB_nguyen2014dynamic} dynamically assigns users to its nearest estimated cluster.\n%     \\item  \\textbf{COFIBA} \\citep{co_filter_bandits_2016} estimates user clustering and arm clustering simultaneously, and ensembles linear estimators for  reward and confidence bound estimations;\n%     \\item  \\textbf{Neural-Pool} adopts one single Neural-UCB \\citep{Neural-UCB} model for all the users with UCB-type exploration strategy;\n%     \\item  \\textbf{Neural-Ind} assigns each user with their own separate Neural-UCB \\citep{Neural-UCB} model;\n%     \\item  \\textbf{EE-Net} \\citep{EE-Net_ban2021ee} achieves adaptive exploration by applying additional neural models for the exploration and decision making;\n%     \\item  \\textbf{Meta-Ban} \\citep{Meta-Ban} utilizes individual neural models for each user's behavior, and applies a meta-model to adapt to estimated user groups.\n% \\end{itemize}\n\n\n% % \\vspace{-0.2cm}\n% \\vspace{-0.2cm}\n",
                "subsection 6.1": {
                    "name": "Real Data Sets",
                    "content": "\nIn this section, we compare the proposed \\name\\ with baselines on six data sets with different specifications.\n% \\vspace{-0.1cm}\n\n% ``MovieLens rating dataset'' (\\url{https://www.grouplens.org/datasets/movielens/20m/}) includes reviews from $1.6 \\times 10^5$ users towards $6 \\times 10^4$ movies. \n% Since the genome-scores of user-specified tags are provided for each movie, we select 10 tags with the highest score variance to generate the movie features $\\vect{v}_{i} \\in \\mathbb{R}^{d}, d=10$. Here, the user features $\\vect{v}_{u}\\in\\mathbb{R}^{d}, u\\in \\mathcal{U}$ are obtained through singular value decomposition (SVD) of the rating matrix. \n% As the data set offers no group information, we use K-means to divide users into 50 groups based on $\\vect{v}_{u}$, and the group information is unknown to models.\n% In each round $t$, a user $u_t$ is drawn from a randomly selected group. \n% For the arm pool $\\mathcal{X}_{t}$ of $10$ arms, we randomly choose one bad movie (with two stars or less, out of five) rated by $u_t$ with reward $1$, and randomly pick the other $9$ good movies with reward $0$. This is due to the imbalance data distribution of the data set, i.e., most of entries have good ratings. \n\n% ``Yelp'' data set (\\url{https://www.yelp.com/dataset}) contains user interviews generated by $1.18$ million users towards $1.57 \\times 10^5$ restaurants.\n% Here, we extract ratings in the reviews and build the rating matrix w.r.t. the top $2,000$ users and top $10,000$ arms with the most reviews.\n% Then, we use SVD to extract a normalized $10$-dimensional feature vector for each user and restaurant.\n% The goal of the learner is to select the restaurants with high ratings. Given the rating for a specific user-item pair, if the user's rating is greater than three stars (out of five stars), the reward is set to $1$; otherwise, the reward is $0$. \n% With no user similarity information available, we apply K-means clustering to divide users into 50 groups based on user features, which are unknown to models.\n% In each round $t$, a target $u_t$, is sampled from a randomly selected group. \n% For the arm pool $\\mathcal{X}_{t}$, we randomly choose one good restaurant rated by $u_t$ with reward $1$ and randomly pick the other $9$ bad restaurants with reward $0$. \n\n\n\\textbf{[Recommendation Data Sets]}\n% First, we conduct the experiments for two recommendation data sets with different specifications, which are the ``MovieLens'' data set and the ``Yelp'' data set. Given one user $u_{t}$ to serve in each round $t$, our goal is to recommend the optimal arm (movie / restaurant) from the candidate pool $\\mathcal{X}_{t}$ to the user.\n``MovieLens rating dataset'' includes reviews from $1.6 \\times 10^5$ users towards $6 \\times 10^4$ movies. \nHere, we select 10 genome-scores with the highest variance across movies to generate the movie features $\\vect{v}_{i} \\in \\mathbb{R}^{d}, d=10$. The user features $\\vect{v}_{u}\\in\\mathbb{R}^{d}, u\\in \\mathcal{U}$ are obtained through singular value decomposition (SVD) on the rating matrix. We use K-means to divide users into $n=50$ groups based on $\\vect{v}_{u}$, and consider each group as a node in user graphs. \nIn each round $t$, a user $u_t$ will be drawn from a randomly sampled group. \nFor the candidate pool $\\mathcal{X}_{t}$ with $\\abs{\\mathcal{X}_{t}}=a=10$ arms, we choose one bad movie ($\\leq$ two stars, out of five) rated by $u_t$ with reward $1$, and randomly pick the other $9$ good movies with reward $0$. The target here is to help users avoid bad movies.\n%\nFor ``Yelp'' data set, we build the rating matrix w.r.t. the top $2,000$ users and top $10,000$ arms with the most reviews.\nThen, we use SVD to extract the $10$-dimensional representation for each user and restaurant.\nFor an arm, if the user's rating $\\geq$ three stars (out of five stars), the reward is set to $1$; otherwise, the reward is $0$. \nSimilarly, we apply K-means to obtain $n=50$ groups based on user features.\nIn round $t$, a target $u_t$, is sampled from a randomly selected group. \nFor $\\mathcal{X}_{t}$, we choose one good restaurant rated by $u_t$ with reward $1$, and randomly pick the other $9$ bad restaurants with reward $0$. \n\n\n\n\n\n\\textbf{[Classification Data Sets]}\nWe also perform experiments on four real classification data sets under the recommendation settings, \nwhich are ``MNIST'' (with the number of classes $\\mathcal{C} = 10$), ``Shuttle'' ($\\mathcal{C} = 7$), the ``Letter'' ($\\mathcal{C} = 26$), and the ``Pendigits'' ($\\mathcal{C} = 10$) data sets. Each class will correspond to one node in user graphs.\nSimilar to previous works \\citep{Neural-UCB,Meta-Ban}, given a sample $\\vect{x}\\in \\mathbb{R}^{d}$, we transform it into $\\mathcal{C}$ different arms, denoted by $\\vect{x}_1 = (\\vect{x}, 0, \\dots, 0), \\vect{x}_2 = (0, \\vect{x}, \\dots, 0), \\dots, \\vect{x}_{\\mathcal{C}} = (0, 0, \\dots, \\vect{x}) \\in \\mathbb{R}^{d+\\mathcal{C}-1}$ where we add $\\mathcal{C}-1$ zero digits as the padding. The received reward $r_{t}=1$ if we select the arm of the correct class, otherwise $r_{t}=0$.\n\n\n\n\\vspace{-0.2cm}\n",
                    "subsubsection 6.1.1": {
                        "name": "Experiment Results",
                        "content": "\n\\label{subsec_exp_results_main}\n% \\vspace{-0.1cm}\n\n\n\n\nFigure \\ref{fig_experiment_regret_results} illustrates the cumulative regret results on the six data sets, and the red shade represents the standard deviation of \\name. Here, our proposed \\name\\ manages to achieve the best performance against all these strong baselines. Since the MovieLens data set involves real arm features (i.e., genome-scores), the performance of different algorithms on the MovieLens data set tends to have larger divergence. \nNote that due to the inherent noise within these two recommendation data sets, we can observe the ``linear-like'' regret curves, which are common as in existing works (e.g., \\cite{Meta-Ban}).\nIn this case, to show the model convergence, we will present the convergence results for the recommendation data sets in Appendix \\textbf{Subsec.} \\ref{subsec_appx_conv_GNN}. \nAmong the baselines, the neural algorithms (Neural-Pool, EE-Net, Meta-Ban) generally perform better than linear algorithms due to the representation power of neural networks. However, as Neural-Ind considers no correlations among users, it tends to perform the worst among all baselines on these two data sets. \n%\nFor classification data sets, Meta-Ban performs better than the other baselines by modeling user (class) correlations with the neural network.\nSince the classification data sets generally involve complex reward mapping functions, it can lead to the poor performances of linear algorithms. Our proposed \\name\\ outperforms the baselines by modeling fine-grained correlations and utilizing the adaptive exploration strategy simultaneously. In addition, \\name\\ only takes at most $75\\%$ of Meta-Ban's running time for experiments, since Meta-Ban needs to train the framework individually for each arm before making predictions. We will discuss more about the running time in \\textbf{Subsec.} \\ref{subsec_running_time}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                    }
                },
                "subsection 6.2": {
                    "name": "Effects of Propagation Hops $k$",
                    "content": "\nWe also include the experiments on the MovieLens data set with 100 users to further investigate the effects of the propagation hyper-parameter $k$. Recall that given two input vectors $w, v$, we apply the RBF kernel as the mapping functions $\\Psi^{(1)}(w, v) = \\Psi^{(2)}(w, v) = \\exp(-\\gamma\\cdot \\|w - v\\|^{2})$ where $\\gamma$ is the kernel bandwidth.\nThe experiment results are shown in the \\textbf{Table} \\ref{table_different_hops_100_users} below, and the value in the brackets \"[]\" is the element standard deviation of the normalized adjacency matrix of user exploitation graphs.\n\n\n\n\nHere, increasing the value of parameter $k$ will generally make the normalized adjacency matrix elements \"smoother\", as we can see from the decreasing standard deviation values. This matches the low-pass nature of graph multi-hop feature propagation \\cite{SGC_wu2019simplifying}. \nWith larger $k$ values, \\name\\ will be able to propagate the information for mores hops.\nIn contrast, with a smaller $k$ value, it is possible that the target user will be \"heavily influenced\" by only several specific users. \n% Therefore, the practitioner may need to choose $k$ value properly under different application scenarios.\n% For a larger user graph, propagating multiple hops (larger $k$ values) seems to be more beneficial. One possible explanation can be that with a larger user graph, the user correlations will become more complex. \n% Thus, instead of only utilizing the information directly from the target user's neighbors, we need to involve the neighborhood information of the target user's neighbors for a global perspective over the users. In this case, properly increasing the $k$ value will indeed enable \\name\\ to obtain a more comprehensive neighborhood information around the target user, which leads to a better performance when working with the large user graph.\nHowever, overly large $k$ values can also lead to the ``over-smoothing'' problem \\citep{JK_Net_xu2018representation,linearized-GNN_xu2021optimization}, which can impair the model performance. Therefore, the practitioner may need to choose $k$ value properly under different application scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Effects of the Approximated Neighborhood",
                    "content": "   \\label{subsec_exp_approx_neighborhood}\n\nIn this subsection, we conduct experiments to support our claim that applying approximated user neighborhoods is a feasible solution to reduce the computational cost, when facing the increasing number of users (\\textbf{Remark} \\ref{remark_numerous_users}).\nWe consider three scenarios where the number of users $n \\in \\{200, 300, 500\\}$. Meanwhile, we let the size of the approximated user neighborhood $\\Tilde{\\mathcal{N}}^{(1)}(u_{t}), \\Tilde{\\mathcal{N}}^{(2)}(u_{t})$ fix to $\\tilde{n} = \\abs{\\Tilde{\\mathcal{N}}^{(1)}(u_{t})} = \\abs{\\Tilde{\\mathcal{N}}^{(2)}(u_{t})} = 50$ for all these three experiment settings, and the neighborhood users are sampled from the user pool $\\mathcal{U}$ in the experiments. \n\n\n\n\n\n\n\n%\nHere, we see that the proposed \\name\\ still outperforms the baselines with increasing number of users. In particular, given a total of 500 users, the approximated neighborhood is only $10\\%$ (50 users) of the overall user pool. These results can show that applying approximated user neighborhoods (Remark \\ref{remark_numerous_users}) is a practical way to scale-up \\name\\ in real-world application scenarios.\n%\n% Meanwhile, setting $n=500$ and with different approximated neighborhood sizes $\\widetilde{n}\\in \\{50, 100, 150\\}$, the experiment results on the MovieLens data set are shown in \\textbf{Table} \\ref{table_different_size_appx_neighborhood}.\nIn addition, in \\textbf{Table} \\ref{table_different_size_appx_neighborhood}, we also include the average regret per round across different time steps.\n%\nWith the number of users $n=500$ on the MovieLens data set, we include the experiments given different numbers of representative users $\\widetilde{n}\\in \\{50, 100, 150\\}$ to better show the model performance when applying the approximated neighborhood.\nHere, increasing the number of representative users $\\widetilde{n}$ can lead to better performances of \\name, while it also shows that a small number of representative users will be enough for \\name\\ to achieve satisfactory performances. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.15cm}\n"
                },
                "subsection 6.4": {
                    "name": "Effects of the Adaptive Exploration",
                    "content": "\n\nTo show the necessity of the adaptive exploration strategy, we consider an alternative arm selection mechanism (different from line 10, \\textbf{Alg.} \\ref{algo_main}) in round $t\\in [T]$, as\n% \\begin{displaymath}\n% \\begin{split}\n%     \\vect{x}_{t} &= \\arg\\max_{\\vect{x}_{i, t} \\in \\mathcal{X}_{t}}  \n%     \\bigg[ f_{gnn}^{(1)} \\big( \\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\Theta_{gnn}^{(1)}]_{t-1} \\big) \\\\\n%     & \n%     + \\alpha\\cdot f_{gnn}^{(2)} \\big( \\nabla_{\\Theta_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\Theta_{gnn}^{(1)}]_{t-1}),~\\mathcal{G}_{i, t}^{(2)}; [\\Theta_{gnn}^{(2)}]_{t-1} \\big) \\bigg]\n% \\end{split}\n% \\end{displaymath}\n$\n    \\vect{x}_{t} = \\arg\\max_{\\vect{x}_{i, t} \\in \\mathcal{X}_{t}} \\big( \\hat{r}_{i, t} + \\alpha\\cdot \\hat{b}_{i, t} \\big),\n$\ngiven the estimated reward and potential gain. Here, we introduce an additional parameter $\\alpha\\in [0, 1]$ as the exploration coefficient to control the exploration levels (i.e., larger $\\alpha$ values will lead to higher levels of exploration). Here, we show the experiment results with $\\alpha \\in \\{0, 0.1, 0.3, 0.7, 1.0\\}$ on the ``MNIST'' and ``Yelp'' data sets.\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{Figure/ablation_exp_coef_figure.pdf}\n%   \\caption{Results with different exploration coefficients $\\alpha$.}\n%   \\label{fig_exp_coef_regret_results}\n% \\end{figure}\n\n\n\n\nIn Table \\ref{table_exp_coef_regret_results}, regarding the results of the ``Yelp'' data set, although the performances of \\name\\ do not differ significantly with different $\\alpha$ values, our adaptive exploration strategy based on user exploration graphs is still helpful to improve \\name's performances, which is validated by the fact that setting $\\alpha\\in (0, 1]$ will lead to better results compared with the situation where no exploration strategies are involved (setting $\\alpha =0$).\nOn the other hand, as for the results of the ``MNIST'' data set, different $\\alpha$ values will lead to relatively divergent results. One reason can be that with larger context dimension $d$ in the ``MNIST'' data set, the underlying reward mapping inclines to be more complicated compared with that of the ``Yelp'' data set. In this case, leveraging the exploration correlations will be more beneficial. \nThus, the adaptive exploration strategy is necessary to improve the performance of \\name\\ by estimating the potential gains based on ``fine-grained'' user (class) correlations.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.2cm}\n"
                },
                "subsection 6.5": {
                    "name": "Running Time vs. Performance",
                    "content": "      \\label{subsec_running_time}\n\nIn Figure \\ref{fig_running_time_main}, we show the results in terms of cumulative regret [y-axis, smaller $ = $ better] and running time [x-axis, smaller $ = $ better]. Additional results are in Appendix \\textbf{Subsec.} \\ref{subsec_appx_time_complex}. Each colored point here refers to one single method.\nThe point labeled as ``\\name\\_Run'' refers to the time consumption of \\name\\ on the arm recommendation process only, and the point ``\\name'' denotes the overall running time of \\name, including the recommendation and model training process.\n\n\n\n\n\n%\nAlthough the linear baselines tend to run faster compared with our proposed \\name, their experiment performances (Subsec. \\ref{subsec_exp_results_main}) are not comparable with \\name, as their linear assumption can be too strong for many application scenarios. \nIn particular, for the data set with high context dimension $d$, the mapping from the arm context to the reward will be much more complicated and more difficult to learn. For instance, as shown by the experiments on the MNIST data set ($d=784$), the neural algorithms manage to achieve a significant improvement over the linear algorithms (and the other baselines) while enjoying the reasonable running time.\n%\nMeanwhile, we also have the following remarks: (1) We see that for the two recommendation tasks, \\name\\ takes approximately $0.4$ second per round to make the arm recommendation with satisfactory performances for the received user;\n(2) In all the experiments, we train the \\name\\ framework per 100 rounds after $t > 1000$ and still manage to achieve the good performance. In this case, the running time of \\name\\ in a long run can be further improved considerably by reducing the training frequency when we already have sufficient user interaction data and a well-trained framework;\n(3) Moreover, since we are actually predicting the rewards and potential gains for all the nodes within the user graph (or the approximated user graph as in Remark \\ref{remark_numerous_users}), \\name\\ is able to serve multiple users in each round simultaneously without running the recommendation procedure for multiple times, which is efficient in real-world cases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.2cm}\n"
                },
                "subsection 6.6": {
                    "name": "Supplementary Experiments",
                    "content": "  \\label{subsec_supplement_exps}\n% \\vspace{-0.1cm}\n\nDue to page limit, we present supplementary experiments in Appendix Section \\ref{sec_appd_experiments}, including: (1) [\\textbf{Subsec.} \\ref{subsec_appx_underlying_groups}] experiments showing the potential impact on \\name\\ when there exist underlying user clusters; (2) [\\textbf{Subsec.} \\ref{subsec_appx_time_complex}] complementary contents for \\textbf{Subsec.} \\ref{subsec_running_time} regarding the ``Letter\" and ``Pendigits'' data sets; (3) [\\textbf{Subsec.} \\ref{subsec_appx_conv_GNN}] the convergence results of \\name\\ on recommendation data sets.\n\n\n\n% ====================================================\n% \\vspace{-0.35cm}\n% \\vspace{-0.1cm}\n\\vspace{-0.10cm}\n"
                }
            },
            "section 7": {
                "name": "Conclusion",
                "content": "    \\label{sec_conclusion}\n% \\vspace{-0.25cm}\nIn this paper, we propose a novel framework named \\name~to model the fine-grained user collaborative effects. Instead of modeling user correlations through the estimation of rigid user groups, we estimate the user graphs to preserve the pair-wise user correlations for exploitation and exploration respectively, and utilize individual GNN-based models to achieve the adaptive exploration with respect to the arm selection. Under standard assumptions, we also demonstrate the improvement of the regret bound over existing methods from new perspectives of ``fine-grained'' user collaborative effects and GNNs.\nExtensive experiments are conducted to show the effectiveness of our proposed framework against strong baselines.\n\n\\vspace{-0.10cm}\n\\begin{acks}\nThis work is supported by National Science Foundation under Award No. IIS-1947203, IIS-2117902, IIS-2137468, and Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agencies or the government.\n\\end{acks}\n\n\n\\clearpage\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{User_GNN_Bandits}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\appendix\n\n\n\\onecolumn\n\n\n\n\n\\clearpage\n"
            },
            "section 8": {
                "name": "Experiments (Cont.)",
                "content": "    \\label{sec_appd_experiments}\n\n\n\n% -----------------------------------------------------\n",
                "subsection 8.1": {
                    "name": "Baselines and Experiment Settings",
                    "content": "\nThe brief introduction for the nine baseline methods:\n\\begin{itemize}\n    \\item \\textbf{CLUB} \\citep{club_2014} regards connected components as user groups out of the estimated user graph, and adopts a UCB-type exploration strategy;\n    \\item \\textbf{SCLUB} \\citep{SCLUB_li2019improved} estimates dynamic user sets as user groups, and allows set operations for group updates;\n    \\item \\textbf{LOCB} \\citep{local_clustering-ban2021local} applies soft-clustering among users with random seeds and choose the best user group for reward and confidence bound estimations;\n    \\item \\textbf{DynUCB} \\citep{Dyn-UCB_nguyen2014dynamic} dynamically assigns users to its nearest estimated cluster.\n    \\item  \\textbf{COFIBA} \\citep{co_filter_bandits_2016} estimates user clustering and arm clustering simultaneously, and ensembles linear estimators for  reward and confidence bound estimations;\n    \\item  \\textbf{Neural-Pool} adopts one single Neural-UCB \\citep{Neural-UCB} model for all the users with UCB-type exploration strategy;\n    \\item  \\textbf{Neural-Ind} assigns each user with their own separate Neural-UCB \\citep{Neural-UCB} model;\n    \\item  \\textbf{EE-Net} \\citep{EE-Net_ban2021ee} achieves adaptive exploration by applying additional neural models for the exploration and decision making;\n    \\item  \\textbf{Meta-Ban} \\citep{Meta-Ban} utilizes individual neural models for each user's behavior, and applies a meta-model to adapt to estimated user groups.\n\\end{itemize}\n\n% \\subsection{Experiment Settings} \\label{subsec_appx_baseline_settings}\n% First, we add more descriptions about our baselines:\n% \\begin{itemize}[leftmargin=*]\n%     \\item \\textbf{CLUB} \\citep{club_2014} regards connected components as user groups out of the estimated user graph, and adopts a UCB-type exploration strategy;\n%     \\item \\textbf{SCLUB} \\citep{SCLUB_li2019improved} estimates dynamic user sets as user groups, and allows set operations for group updates;\n%     \\item \\textbf{LOCB} \\citep{local_clustering-ban2021local} applies soft-clustering among users with random seeds and choose the best user group for reward and confidence bound estimations;\n%     \\item \\textbf{DynUCB} \\citep{Dyn-UCB_nguyen2014dynamic} dynamically assigns users to its nearest estimated cluster.\n%     \\item  \\textbf{COFIBA} \\citep{co_filter_bandits_2016} estimates user clustering and arm clustering simultaneously, and ensembles linear estimators for  reward and confidence bound estimations;\n%     \\item  \\textbf{Neural-Pool} adopts one single Neural-UCB \\citep{Neural-UCB} model for all the users with UCB-type exploration strategy;\n%     \\item  \\textbf{Neural-Ind} assigns each user with their own separate Neural-UCB \\citep{Neural-UCB} model;\n%     \\item  \\textbf{EE-Net} \\citep{EE-Net_ban2021ee} achieves adaptive exploration by applying additional neural models for the exploration and decision making;\n%     \\item  \\textbf{Meta-Ban} \\citep{Meta-Ban} utilizes individual neural models for each user's behavior, and applies a meta-model to adapt to estimated user groups.\n% \\end{itemize}\n\n\nHere, for all the UCB-based baselines, we choose their exploration parameter from the range $\\{0.01, 0.1, 1\\}$ with grid search. We set the $L=2$ for all the deep learning models including our proposed \\name, and set the network width $m=100$. The learning rate of all neural algorithms are selected by grid search from the range $\\{0.0001, 0.001, 0.01\\}$. For EE-Net \\cite{EE-Net_ban2021ee}, we follow the default settings in their paper by using a hybrid decision maker, where the estimation is $f_{1} + f_{2}$ for the first 500 time steps, and then we apply an additional neural network for decision making afterwards. For Meta-Ban, we follow the settings in their paper by tuning the clustering parameter $\\gamma$ through the grid search on $\\{0.1, 0.2, 0.3, 0.4\\}$. For \\name, we choose the $k$-hop user neighborhood $k\\in \\{1, 2, 3\\}$ with grid search.\nReported results are the average of 3 runs.\n%\nThe URLs for the data sets are:\n% \\begin{itemize}\nMovieLens: \\url{https://www.grouplens.org/datasets/movielens/20m/}.\nYelp: \\url{https://www.yelp.com/dataset}.\nMNIST/Shuttle/Letter/Pendigits: \\url{https://archive.ics.uci.edu/ml/datasets}.\n\n% URL to the source code of \\name\\ (uploaded anonymously): \\url{https://drive.google.com/file/d/1-dTCOPSp6NMO7Gwku_8v62DmOcITv9KG/view?usp=sharing} \n\n\n% In addition to the two recommendation data sets above, we perform experiments on two real classification data sets under the recommendation settings, \n% which are ``MNIST'' (\\url{http://yann.lecun.com/exdb/mnist/}), ``Shuttle'' (\\url{https://archive.ics.uci.edu/ml/datasets/Statlog+(Shuttle)}), the ``Letter'' data set with $26$ different classes (\\url{https://archive.ics.uci.edu/ml/datasets/letter+recognition}), and the ``Pendigits'' data set with $10$ classes (\\url{https://archive.ics.uci.edu/ml/datasets/Pen Based+Recognition+of+Handwritten+Digits}).\n% Similar to previous works \\citep{Neural-UCB,Meta-Ban}, given a sample $\\vect{x}\\in \\mathbb{R}^{d}$, we transform it into $\\mathcal{C}$ different arms, denoted by $\\vect{x}_1 = (\\vect{x}, 0, \\dots, 0), \\vect{x}_2 = (0, \\vect{x}, \\dots, 0), \\dots, \\vect{x}_{\\abs{\\mathcal{C}}} = (0, 0, \\dots, \\vect{x}) \\in \\mathbb{R}^{d+\\mathcal{C}-1}$ where we add $\\mathcal{C}-1$ zero digits as the padding. The received reward $r_{t}=1$ if we select the arm of the correct class, otherwise $r_{t}=0$.\n\n\n\n\n\n% \\subsection{Descriptions for the Real Data Sets}   \\label{subsec_appd_description_dataset_settings}\n\n% ``MovieLens rating dataset'' (\\url{https://www.grouplens.org/datasets/movielens/20m/}) includes reviews from $1.6 \\times 10^5$ users towards $6 \\times 10^4$ movies. \n% Since the genome-scores of user-specified tags are provided for each movie, we select 10 tags with the highest score variance to generate the movie features $\\vect{v}_{i} \\in \\mathbb{R}^{d}, d=10$. Here, the user features $\\vect{v}_{u}\\in\\mathbb{R}^{d}, u\\in \\mathcal{U}$ are obtained through singular value decomposition (SVD) of the rating matrix. \n% As the data set offers no group information, we use K-means to divide users into 50 groups based on $\\vect{v}_{u}$, and the group information is unknown to models.\n% In each round $t$, a user $u_t$ is drawn from a randomly selected group. \n% For the arm pool $\\mathcal{X}_{t}$ of $10$ arms, we randomly choose one bad movie (with two stars or less, out of five) rated by $u_t$ with reward $1$, and randomly pick the other $9$ good movies with reward $0$. This is due to the imbalance data distribution of the data set, i.e., most of entries have good ratings. \n\n% ``Yelp'' data set (\\url{https://www.yelp.com/dataset}) contains user interviews generated by $1.18$ million users towards $1.57 \\times 10^5$ restaurants.\n% Here, we extract ratings in the reviews and build the rating matrix w.r.t. the top $2,000$ users and top $10,000$ arms with the most reviews.\n% Then, we use SVD to extract a normalized $10$-dimensional feature vector for each user and restaurant.\n% The goal of the learner is to select the restaurants with high ratings. Given the rating for a specific user-item pair, if the user's rating is greater than three stars (out of five stars), the reward is set to $1$; otherwise, the reward is $0$. \n% With no user similarity information available, we apply K-means clustering to divide users into 50 groups based on user features, which are unknown to models.\n% In each round $t$, a target $u_t$, is sampled from a randomly selected group. \n% For the arm pool $\\mathcal{X}_{t}$, we randomly choose one good restaurant rated by $u_t$ with reward $1$ and randomly pick the other $9$ bad restaurants with reward $0$. \n\n% Then, we convert the two recommendation data sets ``MNIST'' (\\url{http://yann.lecun.com/exdb/mnist/}) and ``Shuttle'' (\\url{https://archive.ics.uci.edu/ml/datasets/Statlog+(Shuttle)}) to the recommendation settings.\n% Analogous to previous works \\citep{Neural-UCB,Meta-Ban}, given a sample $\\vect{x}\\in \\mathbb{R}^{d}$, we transform it into $\\mathcal{C}$ different arms, as $\\vect{x}_1 = (\\vect{x}, 0, \\dots, 0), \\vect{x}_2 = (0, \\vect{x}, \\dots, 0), \\dots, \\vect{x}_{\\abs{\\mathcal{C}}} = (0, 0, \\dots, \\vect{x}) \\in \\mathbb{R}^{d+\\mathcal{C}-1}$ where we add $\\mathcal{C}-1$ zero digits as the padding.\n\n\n\n% -----------------------------------------------------\n% \\subsection{Experiments on Additional Data Sets}\n\n% Due to the page limit in the main body and to better compare our \\name~with the benchmarks, here, we include the experiments on two additional classification data sets in this subsection. \n% They are: (1) the ``Letter'' data set with $\\mathcal{C} = 26$ different classes (\\url{https://archive.ics.uci.edu/ml/datasets/letter+recognition}), and (2) the ``Pendigits'' data set with $\\mathcal{C} = 10$ classes (\\url{https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits}), under the recommendation settings.\n% Analogous to settings of the ``MNIST'' and the ``Shuttle'' data set, we consider each class to be a user. Given a sample $\\vect{x}\\in \\mathbb{R}^{d}$, we transform it into $\\mathcal{C}$ arms for different classes similar to previous works \\citep{Neural-UCB,Meta-Ban}, namely $\\vect{x}_1 = (\\vect{x}, 0, \\dots, 0), \\vect{x}_2 = (0, \\vect{x}, \\dots, 0), \\dots, \\vect{x}_{\\mathcal{C}} = (0, 0, \\dots, \\vect{x}) \\in \\mathbb{R}^{d+\\mathcal{C}-1}$ where additional $\\mathcal{C}-1$ zero digits are added as the padding.\n% The reward will be $r=1$ if we choose the correct arm that represents the sample's true class; otherwise, the reward will be $0$. \n\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{Figure/cumu_regret_additional_datasets_figure.pdf}\n%   \\caption{Cumulative regrets on the two additional classification data sets.}\n%   \\label{fig_additional_classification_results}\n% \\end{figure}\n\n% The experiment results for these two additional data sets are presented in Figure \\ref{fig_additional_classification_results}. It is worthwhile to note that EE-Net continues to outperform the two Neural-UCB baselines, which is also another evidence of the effectiveness of the adaptive exploration strategy. On the other hand, our exploration strategy inspired by EE-Net further incorporates the user exploration graphs to exploit the encoded ``fine-grained'' user collaborative effects. \n% % Compared with the two classification datasets in the main body \n\n\n\n\n\n\n\n\n\n% -----------------------------------------------------\n% \\subsection{Effects of the Adaptive Exploration}\n\n% In order to demonstrate the necessity of the adaptive exploration strategy, we consider an alternative arm selection approach (different from line 10, \\textbf{Alg.} \\ref{algo_main}) at each time step $t$, with the following form:\n% \\begin{displaymath}\n% \\begin{split}\n%     \\vect{x}_{t} = \\arg\\max_{\\vect{x}_{i, t} \\in \\mathcal{X}_{t}}  \n%     \\bigg( f_{gnn}^{(1)} \\big( \\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)};& [\\Theta_{gnn}^{(1)}]_{t-1} \\big) \\\\\n%     & \n%     + \\alpha\\cdot f_{gnn}^{(2)} \\big( \\nabla_{\\Theta_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\Theta_{gnn}^{(1)}]_{t-1}),~\\mathcal{G}_{i, t}^{(2)}; [\\Theta_{gnn}^{(2)}]_{t-1} \\big) \\bigg)\n% \\end{split}\n% \\end{displaymath}\n% given the candidate arm set $\\mathcal{X}_{t} = \\{\\vect{x}_{i, t}\\}_{i\\in [a]}$ and the model parameters $[\\Theta_{gnn}^{(1)}]_{t-1}, [\\Theta_{gnn}^{(2)}]_{t-1}$. Here, we introduce an additional parameter $\\alpha\\in [0, 1]$ as the exploration coefficient to control the levels of exploration (larger the $\\alpha$ values will lead to higher levels of exploration). And we will show the experiment results with $\\alpha \\in \\{0, 0.1, 0.3, 0.7, 1.0\\}$ on the ``MNIST'' and the ``Yelp'' data sets.\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{Figure/ablation_exp_coef_figure.pdf}\n%   \\caption{Cumulative regrets for different exploration coefficients $\\alpha$.}\n%   \\label{fig_exp_coef_regret_results}\n% \\end{figure}\n\n% In Figure \\ref{fig_exp_coef_regret_results}, we illustrate the effects of different exploration coefficients. Regarding the results in the left figure (``Yelp'' data set), the adaptive exploration indeed helps to improve the performance \\name, but the performances of \\name~do not differ dramatically with different $\\alpha$ values.\n\n% On the other hand, based on the figure on the right hand side (``MNIST'' data set), different $\\alpha$ values tend to have relatively divergent results. The reason can be that in the ``MNIST'' data set, the mapping from arm contexts to the rewards is more complicated compared with that of the ``Yelp'' data set. Thus, the adaptive exploration strategy is able to prominently improve the performance of \\name~by flexibly estimating potential gains of different classes with the estimated ``fine-grained'' user (class) correlations.\n\n\n% -----------------------------------------------------\n% \\subsection{Effects of Information Propagation Hops}\n\n% Recall that there exists a parameter $k$ for the \\name~framework in \\textbf{Eq.} \\ref{eq_GNN_aggegation}, which controls the user neighborhood hops that the two GNN models learn from. In this subsection, we will present the experiment results with $k\\in \\{1, 2, 3\\}$ on the ``MNIST'' data set and the ``Yelp'' data set, which are presented in Figure \\ref{fig_neighbor_size_results}.\n\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{Figure/ablation_neighbor_size_figure.pdf}\n%   \\caption{Cumulative regrets for different neighborhood hops $k$.}\n%   \\label{fig_neighbor_size_results}\n% \\end{figure}\n\n\n% Based on the results on the two data sets, we can observe that setting $k=1$, namely making the \\name~learn directly from the 1-hop neighborhood, tends to yield the best result. This might be due to the fact that since our user graphs are staying as connected graphs while the user correlations are encoded by the edge weights, learning directly from the neighbor would be good enough. And the pair-wise user correlations between the target user and every other user have already been taken into consideration. Meantime, with larger $k$ values ($k=2, 3$), raising the matrix to the power of $k$ would lead to more even entry values across the adjacency matrix, which can be related to the over-smoothing problem \\citep{JK_Net_xu2018representation,linearized-GNN_xu2021optimization}. The figure on the right hand side (``MNIST'' data set) may support this claim. Since it has already been shown in the Figure \\ref{fig_experiment_regret_results} that applying one single estimator across all users (classes), i.e., Neural-Pool and EE-Net, will lead to poor performances, the ``MNIST'' data set tend to have complex correlations among different classes. In this case, when we increase $k$, different user pairs tend to have similar correlations because entries of the adjacency matrix become more close to each other, which may lead to extra estimation error.\n\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{Figure/cumu_regret_rebuttal_MNIST_datasets_alpha_k.pdf}\n%   \\caption{Cumulative regrets for different neighborhood hops $k$ and exploration parameter $\\alpha$ for the MNIST data set.}\n%   \\label{fig_alpha_k_MNIST}\n% \\end{figure}\n\n\n% Moreover, we also conduct the experiments on the MNIST data set with different sets of $\\alpha$ and $k$ parameters jointly, as shown in Figure \\ref{fig_alpha_k_MNIST}. Following our conclusion above, setting $k=1$ generally leads to better results and the adaptive exploration strategy offers considerable help to improve the GNB's performance. One phenomenon to note is that when we increase the value of parameter $k$, the performance difference of GNB with different $\\alpha$ values will shrink. One reason for this situation is that when we increase the $k$ value, the propagated adjacency matrix of the user graph will become more ``smooth'', which makes the users closer to each other in terms of similarity. In this case, the effect of the adaptive exploration strategy can be affected as the user correlations estimated are less divergent.\n\n\n\n% ---------------------------\n% \\subsection{Effects of Information Propagation Hops}\n% Here, we conduct the experiments on the MovieLens data set with larger user graphs (100 users) to further investigate the influence of the parameter $k$. Meanwhile, given two input vectors $w, v$, we apply the RBF kernel as the mapping functions $\\Psi^{(1)}(w, v) = \\Psi^{(2)}(w, v) = \\exp(-\\gamma\\cdot \\|w - v\\|^{2})$ where $\\gamma$ is the kernel bandwidth parameter.\n% The experiment results are shown in the \\textbf{Table} \\ref{table_different_hops_100_users} below, and the value in the brackets \"[]\" is the element standard deviation of the normalized adjacency matrix of user exploitation graphs.\n\n\n% \\begin{table}[h]\n%     \\centering\n%     \\begin{tabular}{ |p{2cm}|p{2cm}p{2cm}p{2cm}p{2cm}|  }\n%      \\hline\n%      &\\multicolumn{4}{|c|}{ Bandwidth $\\gamma$ } \\\\\n%      \\hline\n%      \\textbf{Hops $k$} & 0.1& 1& 2& 5 \\\\\n%      \\hline\n%      1    & 7276 [$1.6\\times 10^{-4}$]   &  7073 [$1.4\\times 10^{-3}$] &  7151 [$2.2\\times 10^{-3}$] &  7490 [$3.9\\times 10^{-3}$] \\\\\n%      2  &   6968 [$1.0\\times 10^{-4}$] & 6966 [$7.7\\times 10^{-4}$] & 7074 [$1.3\\times 10^{-3}$] &  7087 [$2.5\\times 10^{-3}$] \\\\\n%      3  &  7006 [$7.1\\times 10^{-5}$] & 7018 [$7.0\\times 10^{-4}$] &  6940 [$1.2\\times 10^{-3}$] &  7167 [$1.9\\times 10^{-3}$] \\\\\n%      \\hline\n%     \\end{tabular}\n%     \\vspace{+0.1in}\n%     \\caption{ Cumulative regrets on MovieLens dataset with 100 users (different k / kernel bandwidth). The value in the brackets \"[]\" is the element standard deviation of the normalized adjacency matrix. }\n%     \\label{table_different_hops_100_users}\n% \\end{table}\n\n% Here, we have the following remarks:\n% (1) Increasing the value of parameter $k$ will make the normalized adjacency matrix elements more \"smooth\", as we can see from the decreasing standard deviation values. This matches the low-pass nature of multi-hop feature propagation (Wu et al., 2019). \n% With a larger $k$ value for information propagation, GNB will be able to incorporate the multi-hop user neighborhood. But overly large $k$ values can lead to the \"over-smoothing\" problem. \n% In contrast, with a smaller $k$ value, it is possible that the target user is \"heavily influenced\" by only several specific users. \n% Therefore, the practitioner may need to choose $k$ value properly under different application scenarios.\n\n% (2) For a larger user graph, propagating multiple hops (larger $k$ values) seems to be more beneficial. One possible explanation can be that with a larger user graph, the user correlations will become more complex. \n% Thus, instead of only utilizing the information directly from the target user's neighbors, we need to involve the neighborhood information of the target user's neighbors for a global perspective over the users. In this case, properly increasing the $k$ value will indeed enable GNB to obtain a more comprehensive neighborhood information around the target user, which leads to a better performance when working with the large user graph.\n% However, overly large $k$ values can also lead to additional computational costs and the ``over-smoothing'' problem \\citep{JK_Net_xu2018representation,linearized-GNN_xu2021optimization}, which can impair the model performance based on the experiments above. Therefore, we need to tune the neighborhood hyper-parameter $k$ wisely, such that the performance of \\name is optimized.\n\n\n\n% -----------------------------------------------------\n\\vspace{-0.2cm}\n"
                },
                "subsection 8.2": {
                    "name": "Experiments with Underlying User Groups",
                    "content": "    \\label{subsec_appx_underlying_groups}\n\nTo understand the influence of potential underlying user clusters, we conduct the experiments on the MovieLens and the Yelp data sets, with controlled number of underlying user groups. The underlying user groups are derived by using hierarchical clustering on the user features, with a total of 50 users approximately. Here, we compare \\name\\ with four representative baselines with relatively good performances, including DynUCB \\citep{Dyn-UCB_nguyen2014dynamic} [fixed number of user clusters], LOCB \\citep{local_clustering-ban2021local} [fixed number of user clusters with local clustering], CLUB \\citep{club_2014} [distance-based user clustering], Neural-UCB-Pool \\citep{Neural-UCB} [neural single-bandit algorithm], and Meta-Ban \\citep{Meta-Ban} [neural user clustering bandits]. DynUCB and LOCB are given the \\textbf{true cluster number} as the prior knowledge to determine the quantity of user clusters or random seeds. Results are shown in Fig. \\ref{fig_rebuttal_group_size}. \n\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{Figure/cumu_regret_rebuttal_movielens_datasets_figure.pdf}\n%   \\caption{Cumulative regrets for different number of underlying user groups (MovieLens data set).}\n%   \\label{fig_rebuttal_group_size_MovieLens}\n% \\end{figure}\n\n\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{Figure/cumu_regret_rebuttal_yelp_datasets_figure.pdf}\n%   \\caption{Cumulative regrets for different number of underlying user groups (Yelp data set).}\n%   \\label{fig_rebuttal_group_size_Yelp}\n% \\end{figure}\n\n\n\n\n\n\n\n\n\nAs we can see from the results, our proposed \\name\\ outperforms other baselines across different data sets and number of user groups. In particular, with more underlying user groups, the performance improvement of \\name\\ over the baselines will slightly increase, which can be the result of the increasingly complicated user correlations. \nThe modeling of fine-grained user correlations, the adaptive exploration strategy with the user exploration graph, and the representation power of our GNN-based architecture can be the reasons for \\name's good performances. \n\n\n% -----------------------------------------------------\n% \\subsection{Experiments with Approximated User Neighborhood}\n\n% In this subsection, we conduct experiments to support our claim that applying approximated user neighborhoods is a feasible solution for increasing number of users (Remark \\ref{remark_numerous_users}).\n% Then, we consider three scenarios where the number of users $n \\in \\{200, 300, 500\\}$. Meanwhile, we let the size of the approximated user neighborhood $\\Tilde{\\mathcal{N}}^{(1)}(u_{t}), \\Tilde{\\mathcal{N}}^{(2)}(u_{t})$ fix to $\\tilde{n} = \\abs{\\Tilde{\\mathcal{N}}^{(1)}(u_{t})} = \\abs{\\Tilde{\\mathcal{N}}^{(2)}(u_{t})} = 50$ for all these three experiment settings, and the neighborhood users are sampled from the user pool $\\mathcal{U}$ for experiments. \n\n% \\begin{figure}[!h]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{Figure/cumu_regret_MORE_USERS_MovieLens.pdf}\n%   \\caption{Cumulative regrets for different number of users with approximated user neighborhood (MovieLens data set).}\n%   \\label{fig_MORE_USERS_MovieLens}\n% \\end{figure}\n\n\n\n% \\begin{table}[h]\n%     \\centering\n%     \\begin{tabular}{ |p{4cm}|p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}|  }\n%      \\hline\n%      &\\multicolumn{5}{|c|}{ Time steps } \\\\\n     \n%      \\hline\n%      \\textbf{Data set (Algorithm)} & 2000  & 4000 & 6000 & 8000 & 10000 \\\\\n%      \\hline\n%       CLUB   & 0.7691 & 0.7513 & 0.7464 & 0.7468 & 0.7496\\\\\n%      Neural-UCB-Ind  &   0.8901 &  0.8808 &  0.8790  &  0.8754 & 0.8741\\\\\n%      Neural-UCB-Pool   & 0.7681 & 0.7526 & 0.7405 & 0.7362 & 0.7334\\\\\n%      EE-Net  &   0.7886 & 0.7723 & 0.7642 & 0.7618 & 0.7582\\\\\n%      Meta-Ban  &  0.7811 & 0.7761 & 0.7754 & 0.7729 & 0.7708\\\\\n%      \\hline\n%      GNB ($\\widetilde{n}=50$)  &  0.7760 & 0.7245 & 0.7190 & 0.7265 & 0.7140\\\\\n%      GNB ($\\widetilde{n}=100$)   & 0.7406 & 0.7178 & 0.7172 & 0.7110 & 0.7104\\\\\n%      GNB ($\\widetilde{n}=150$)  &  0.7291 & 0.7228 & 0.7129 & 0.7105 & 0.7085\\\\\n%      \\hline\n%     \\end{tabular}\n%     \\vspace{+0.1in}\n%     \\caption{ With the number of users $n=500$ for the MovieLens data set, the comparison between GNB and baselines on average regret per round. }\n%     \\label{table_different_size_appx_neighborhood}\n% \\end{table}\n\n% %\n% The experiment results are shown in Figure \\ref{fig_MORE_USERS_MovieLens} and \\textbf{Table} \\ref{table_different_size_appx_neighborhood}. Here, we see that the proposed \\name\\ still outperforms the baselines with increasing number of users. In particular, given a total of 500 users, the approximated neighborhood is only $1 / 10$ (50 users) of the overall user pool. These results can serve as a clear support that applying approximated user neighborhoods (Remark \\ref{remark_numerous_users}) is a practical way to scale-up \\name\\ in real-world application scenarios.\n\n% In addition, in \\textbf{Table} \\ref{table_different_size_appx_neighborhood}, with the number of users $n=500$ on the MovieLens data set, we include the experiments given different numbers of representative users $\\widetilde{n}$ to better show the performance of approximated neighborhoods.\n% Here, increasing the number of representative users $n$ can lead to a better performance of GNB, and it also shows that a small number of representative users can indeed enable GNB to achieve good performances. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%%\n"
                },
                "subsection 8.3": {
                    "name": "Running Time vs. Performance (Cont.)",
                    "content": "  \n\\label{subsec_appx_time_complex}\n\nIn Figure \\ref{fig_running_time_appx}, we include the comparison with baselines in terms of the running performance and running time on the last two classification data sets (``Letter'' and ``Pendigits'' data sets).\nAnalogously, the results match our conclusions that by applying the GNN models and the ``fine-grained\" user correlations, \\name\\ can find a good balance between the computational cost and recommendation performance.\n\n% From \\textbf{Table} \\ref{table_running_time}, we see that compared with the most closely related work, Meta-Ban, our proposed GNB is generally faster, since GNB does not required to re-train the model for each candidate arm. \n% \\begin{table}[h]\n%     \\centering\n%     \\begin{tabular}{ |p{1.7cm}|p{1.5cm}p{1cm}p{1cm}p{1cm}|  }\n%      \\hline\n%      &\\multicolumn{4}{|c|}{ \\textbf{Data sets} ($10^{4}$ rounds) } \\\\\n     \n%      \\hline\n%      \\textbf{Methods} & MovieLens  & Yelp & MNIST & Shuttle \\\\\n%      \\hline\n%      CLUB    & 230    &  36 &  359 & 4     \\\\\n%      SCLUB    & 274    &  82 &  363 & 4     \\\\\n%      LOCB    & 215    &  31 &  223 & 3    \\\\\n%      DynUCB    & 214    &  29 &  357 & 3     \\\\\n%      Neural-Pool    & 509    &  321 &  289 & 226     \\\\\n%      Neural-Ind    & 472    &  281 &  265 & 169   \\\\\n%      EE-Net    & 2435    &  2149 &  2903 & 2052    \\\\\n%      COFIBA    & 321    &  135 &  11874 & 13     \\\\\n%      Meta-Ban    & 20170    &  19825 &  18172 & 18101    \\\\\n%      \\hline\n%      \\textbf{GNB} (Ours)    &  14121 [4295]    &  12506 [4082] &  4299 [1072] &  1606 [185]    \\\\\n%      \\hline\n%     \\end{tabular}\n%     \\vspace{+0.1in}\n%     \\caption{Average running time results (seconds) on real data sets. The running time \\textbf{in the brackets ``[]''} is the actual time consumption for recommendation w/o the time consumption for training.}\n%     \\label{table_running_time}\n% \\end{table}\n\n\n\n\n\n% The results match our conclusions that by applying the neural models and the ``fine-grained\" user correlations, \\name\\ can find a good balance between the computational cost and performance.\n% Although the other baselines, especially the linear baselines tend to run faster compared with our proposed GNB, their experiment performances (Section \\ref{sec_experiments}) are also not comparable with our proposed GNB as their linear assumption is too strong for most application scenarios. \n% In particular, for the data set with large arm context dimension $d$, the mapping from the arm context to the reward will be much more complicated. In this case, as shown by the experiments on the MNIST data set ($d=784$) in Figure \\ref{fig_experiment_regret_results}, the neural algorithms manage to achieve an undoubtedly huge improvement over the linear algorithms, and have the reasonable running time.\n% %\n% Here, the numbers in the brackets ``[]'' are the time consumption for the actual recommendation process. We have the following remarks: (1) Based on the running time in the brackets, we see that for the two recommendation tasks, GNB takes approximately $\\sim 0.4$ second / per round to make the arm recommendation for the received user, which is reasonable in real-world cases;\n% (2) In all the experiments, we train the GNB framework per 100 rounds after $T > 1000$ and still manage to achieve good performance. Thus, the running time of GNB in a long run could be further significantly improved by reducing the training frequency since we have already have enough data and an accurate framework;\n% (3) Moreover, since we are actually predicting the rewards and potential gain for all the nodes within the user graph (or the ``approximated'' user graph), GNB is able to handle multiple users in each round simultaneously without running the recommendation procedure multiple times, which is efficient in real-world cases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%%\n"
                },
                "subsection 8.4": {
                    "name": "Convergence of GNN Models",
                    "content": "  \\label{subsec_appx_conv_GNN}\n\nHere, for each separate time step interval of 2000 rounds, we show the average cumulative regrets results on MovieLens and Yelp data sets within this interval. We also include the baselines (EE-Net \\cite{EE-Net_ban2021ee}, Neural-UCB-Pool \\cite{Neural-UCB} and Meta-Ban \\cite{Meta-Ban}) for comparison.\n\n\n\nAs in Table \\ref{table_convergence_GNN}, the average regret per round of GNB is decreasing, along with more time steps. Compared with baselines, GNB manages to achieve the best prediction accuracy across different time step intervals by modeling the fine-grained user correlations and applying the adaptive exploration strategy.\nHere, one possible reason for the ``linear-like'' curves of the cumulative regrets is that these two recommendation data sets contain considerable inherent noise, which makes it hard for algorithms to learn the underlying reward mapping function. In this case, achieving experimental improvements on these two data sets is non-trivial. \n% As in the existing works, the baselines as well as their proposed algorithms will behave similarly, and the performance difference is small. \n% For instance, in existing works \\cite{Neural-UCB,neural_thompson-zhang2020neural,EE-Net_ban2021ee,Meta-Ban,neural_multifacet-ban2021multi}, we can all observe this kind of linear-like cumulative regret curves for the recommendation data sets.\n\n\n\n\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% \\clearpage\n% \\section{Additional Discussion on the Time Complexity and Space Complexity} \\label{sec_appx_additional_discussion_on_time_space_complexity}\n\n% In addition to the main body, in this section, we would like to add complementary discussion regarding the time complexity and space complexity of \\name\\ comparing with existing works.\n\n\n% \\textbf{[Time complexity]}\n% Suppose we have a total of $n$ users.\n% In each round $t\\in [T]$, we will need to iterate through user models and construct the user graphs. Originally, we will need $O(n)$ time to calculate the output of user models, and $O(n^{2})$ kernel evaluations on scalar values to calculate the exploitation/exploration scores for user pairs based on the user model output. The kernel evaluation of scalar values is fast, and the whole can be highly paralleled by using multiple machines where each machine is in charge of multiple users. \n% After adopting $\\widetilde{n}$ ($\\widetilde{n} << n$) representative users instead to approximately model the fine-grained user correlations (Remark \\ref{remark_numerous_users}), we will then need $O(\\widetilde{n})$ time to calculate the output of user models, and $O(\\widetilde{n}^{2})$ time to calculate the exploitation/exploration scores for user pairs with kernel evaluations. In this way, the time complexity can be significantly reduced.\n\n\n% \\textbf{[Space complexity]}\n% After adopting $\\widetilde{n}$ representative users, our space complexity is no larger than $ 2T\\cdot \\widetilde{n}^{2}  + O(\\widetilde{n}\\cdot p\\cdot T) + O(n\\cdot p)$, and applying the approximation method in Remark \\ref{remark_avg_pool} can further significantly reduce the space complexity. Here, the first term $2T\\cdot \\widetilde{n}^{2}$ is the cost of storing past user graph adjacency matrices of selected arms, the second term is for storing past contexts and network gradients, and the third term is the cost of storing individual user models.\n% %\n% To model user behaviors, each user is assigned with a separate neural single-bandit model with no user collaborations \\cite{Neural-UCB,AGG-UCB_qi2022neural,neural_thompson-zhang2020neural}. For neural algorithms using the UCB \\cite{Neural-UCB,AGG-UCB_qi2022neural} and Thompson sampling \\cite{neural_thompson-zhang2020neural} to achieve exploration, they need the space complexity of $O(n\\cdot p^{2})$ ($p$ is the number of parameters in the neural model, which is a considerably large number) to store their gigantic gradient matrices, which demands large amount of running memory especially when the neural model is large. EE-Net \\cite{EE-Net_ban2021ee} has the space complexity of $O(n\\cdot p\\cdot T)$ while no user collaborations are involved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% \\newpage\n% \\section{User Networks Architecture.}   \\label{sec_appd_user_network_arch}\n% Here, we can choose different architectures for $f_{u}^{(1)}(\\cdot), f_{u}^{(2)}(\\cdot)$ to deal with various application scenarios (e.g., Convolutional Neural Networks [CNNs] for recommendation tasks of visual contents). In this paper, for the theoretical analysis and experiments, we apply separate $L$-layer fully-connected (FC) networks for user exploitation models and exploration models, as \n% \\begin{equation} \n% f_{u}(\\vect{\\chi}; \\matr{\\Theta}_{u} ) = \\matr{\\Theta}_{L} \\sigma ( \\matr{\\Theta}_{L-1}  \\sigma (\\matr{\\Theta}_{L-2} \\dots  \\sigma(\\matr{\\Theta}_{1} \\vect{\\chi}) ))\n% \\label{eq_user_model_structure}\n% \\end{equation}\n% with $\\matr{\\Theta}_{u} = [\\text{vec}(\\matr{\\Theta}_{1})^{\\intercal}, \\dots, \\text{vec}(\\matr{\\Theta}_{L})^{\\intercal}]^{\\intercal}$ being the trainable parameters, and $\\sigma$ being the ReLU activation. \n% Here, since $f_{u}^{(1)}(\\cdot), f_{u}^{(2)}(\\cdot)$ are both $L$-layer networks shown in \\textbf{Eq.}\\ref{eq_user_model_structure}, the input $\\vect{\\chi}$ can be either the arm $\\vect{x}$ or the network gradient $\\nabla_{\\matr{\\Theta}_{u}^{(1)}}f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$.\n\n% \\textbf{Initialization.}\n% The weight matrix of the input layer is different for two user networks where $\\matr{\\Theta}_{1}^{(1)} \\in \\mathbb{R}^{m\\times d}$ and $\\matr{\\Theta}_{1}^{(2)} \\in \\mathbb{R}^{m\\times p}$. The rest of the layers will be the same comparing the two kinds of user networks, which are $\\matr{\\Theta}_{l} \\in \\mathbb{R}^{m\\times m}, l\\in [2, \\cdots, L-1]$, and $\\matr{\\Theta}_{L} \\in \\mathbb{R}^{1\\times m}$. \n% For both kinds of user networks, the weight matrix entries for the first $L-1$ layers $\\{\\matr{\\Theta}_{1}, \\dots \\matr{\\Theta}_{L-1} \\}$ are drawn from the Gaussian distribution $N(0, 2 / m)$. The entries of the last layer $\\matr{\\Theta}_{L}$ are sampled from $N(0, 1 / m)$.\n\n\n\n\n\n\n% ===================================================================================== Algorithm 2\n\\clearpage\n"
                }
            },
            "section 9": {
                "name": "Pseudo-code for Training the \\name~Framework",
                "content": "   \\label{sec_appd_pseudo_code}\nIn this section, we present the pseudo-code for training \\name\\ with GD with the following \\textbf{Alg.} \\ref{algo_training}.\n% ===================================================================================== Algorithm 3\n% \\begin{algorithm}[h]\n% \\caption{User Model Training}\n% \\label{algo_user_training}\n% \\textbf{Input:} Initial parameter $\\vect{\\Theta_{0}}$, step size $\\eta_{1}, \\eta_{2}$, training steps $J_{1}, J_{2}$, network width $m$. Served user $u_{t}$. \\\\\n% \\textbf{Output:} Updated model parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$, $[\\vect{\\Theta}_{gnn}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t}$. \\\\\n\n% % ---------------------------------\n\n% \\BlankLine\n\n% $[\\matr{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]_{t}$ = User-Model-Training $\\big( u_{t}, [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}, [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0} \\big)$. \\\\\n% \\For{$\\forall u'\\in \\mathcal{U}$, $u' \\neq u_{t}$}{ \n%     $[\\matr{\\Theta}_{u'}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{u'}^{(1)}]_{t-1}$, \\quad \n%     $[\\matr{\\Theta}_{u'}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{u'}^{(2)}]_{t-1}$\\\\\n% } \n% $[\\matr{\\Theta}_{gnn}^{(1)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t}$ = GNN-Model-Training $\\big( [\\matr{\\Theta}_{gnn}^{(1)}]_{0}, [\\matr{\\Theta}_{gnn}^{(2)}]_{0} \\big)$. \\\\\n% Return $[\\matr{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(1)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t}$.\n\n\n% \\BlankLine\n% % ---------------------------------\n% $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}$, \\quad\n% $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0}$. \\\\\n\n% %\n\n% %\n% \\# Training of $f_{u}^{(1)}(\\cdot )$ \\\\\n% Let $\\mathcal{L}(\\Theta_{u_{t}}^{(1)}) := \\sum_{\\tau\\in \\mathcal{T}_{u_{t}, t}} \\abs{f_{u}^{(1)}(\\vect{x}_{\\tau}; \n% \\matr{\\Theta}_{u_{t}}^{(1)}) - r_{\\tau}}^{2}$ \\\\\n% %\n% \\For{$j = 1, 2, \\dots, J_{1}$}{ \n%     $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{j} = [\\matr{\\Theta}_{u_{t}}^{(1)}]^{j-1} - \\eta_{1} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{u_{t}}^{(1)}]^{j-1})$ \\\\\n% } \n\n% %\n\n% %\n% \\# Training of $f_{u}^{(2)}(\\cdot )$ \\\\\n% %\n% Let $\\mathcal{L}(\\Theta_{u_{t}}^{(2)}) := \\sum_{\\tau\\in \\mathcal{T}_{u_{t}, t}} \n% \\abs{f_{u}^{(2)}(\\nabla_{\\matr{\\Theta}_{u_{t}}^{(1)}} f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{\\tau-1}); \\matr{\\Theta}_{u_{t}}^{(2)}) - \n% \\big(r_{\\tau} - f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{\\tau-1})\\big)\n% }^{2}$ \\\\\n% %\n% \\For{$j = 1, 2, \\dots, J_{1}$}{ \n%     $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{j} = [\\matr{\\Theta}_{u_{t}}^{(2)}]^{j-1} - \\eta_{1}\\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{u_{t}}^{(2)}]^{j-1})$ \\\\\n% } \n% %\n% Let $[\\widehat{\\vect{\\Theta}}_{u_{t}}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{u_{t}}^{(1)}]^{J_{1}}$, \n% $[\\widehat{\\vect{\\Theta}}_{u_{t}}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{u_{t}}^{(2)}]^{J_{1}}$ \\\\\n% %\n% Sample and return new parameters $([\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}, [\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}) \\sim \\{ ([\\widehat{\\vect{\\Theta}}_{u_{t}}^{(1)}]_{\\tau}, [\\widehat{\\vect{\\Theta}}_{u_{t}}^{(2)}]_{\\tau}) \\}_{\\tau\\in [t]}$.\\\\\n% %\n% % Sample and Return new parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$.\n% % Return new parameters $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{J}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{J}$. \\\\\n\n\n% \\end{algorithm}\n\n\n% \\begin{algorithm}[h]\n% \\caption{GNN Training}\n% \\label{algo_GNN_training}\n% \\textbf{Input:} Initial parameter $\\vect{\\Theta_{0}}$, step size $\\eta_{1}, \\eta_{2}$, training steps $J_{1}, J_{2}$, network width $m$. Updated user graphs $\\mathcal{G}_{t}^{(1)}$, $\\mathcal{G}_{t}^{(2)}$. Served user $u_{t}$. \\\\\n% \\textbf{Output:} Updated model parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$. \\\\\n\n% % ---------------------------------\n\n\n\n% \\BlankLine\n% % ---------------------------------\n\n% % ---------------------------------\n% $[\\matr{\\Theta}_{gnn}^{(1)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{gnn}^{(1)}]_{0}$, \\quad\n% $[\\matr{\\Theta}_{gnn}^{(2)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{gnn}^{(2)}]_{0}$. \\\\\n\n% %\n\n% %\n% \\# Training of $f_{gnn}^{(1)}(\\cdot )$ \\\\\n% Let $\\mathcal{L}(\\Theta_{gnn}^{(1)}) := \\sum_{\\tau\\in [t]} \\abs{f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; \n% \\matr{\\Theta}_{gnn}^{(1)}) - r_{\\tau}}^{2}$ \\\\\n% %\n% \\For{$j = 1, 2, \\dots, J_{2}$}{ \n%     $[\\matr{\\Theta}_{gnn}^{(1)}]^{j} = [\\matr{\\Theta}_{gnn}^{(1)}]^{j-1} - \\eta_{2} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{gnn}^{(1)}]^{j-1})$ \\\\\n% } \n\n% %\n\n% %\n% \\# Training of $f_{gnn}^{(2)}(\\cdot )$ \\\\\n% Apply $f_{gnn}^{(1)}(\\vect{x}_{\\tau})$ to denote $f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})$. \\\\\n% %\n% Let $\\mathcal{L}(\\Theta_{gnn}^{(2)}) := \\sum_{\\tau\\in [t]} \n% \\abs{f_{gnn}^{(2)}(\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{\\tau}), \\mathcal{G}_{\\tau}^{(2)}; \\matr{\\Theta}_{gnn}^{(2)}) - \n% \\big(r_{\\tau} - f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)})\\big)\n% }^{2}$ \\\\\n% %\n% \\For{$j = 1, 2, \\dots, J_{2}$}{ \n%     $[\\matr{\\Theta}_{gnn}^{(2)}]^{j} = [\\matr{\\Theta}_{gnn}^{(2)}]^{j-1} - \\eta_{2} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{gnn}^{(2)}]^{j-1})$ \\\\\n% }\n% %\n% Let $[\\widehat{\\vect{\\Theta}}_{gnn}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{gnn}^{(1)}]^{J_{2}}$, \n% $[\\widehat{\\vect{\\Theta}}_{gnn}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{gnn}^{(2)}]^{J_{2}}$ \\\\\n% %\n% Sample and return new parameters $([\\vect{\\Theta}_{gnn}^{(1)}]_{t}, [\\vect{\\Theta}_{gnn}^{(2)}]_{t}) \\sim \\{ ([\\widehat{\\vect{\\Theta}}_{gnn}^{(1)}]_{\\tau}, [\\widehat{\\vect{\\Theta}}_{gnn}^{(2)}]_{\\tau}) \\}_{\\tau\\in [t]}$. \\\\\n% %\n% % Return new parameters $[\\vect{\\Theta}_{gnn}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t}$.\n% \\end{algorithm}\n\n\n\\begin{algorithm}[H]\n\\caption{Model Training}\n\\label{algo_training}\n\\textbf{Input:} Initial parameter $\\vect{\\Theta_{0}}$, step size $\\eta_{1}, \\eta_{2}$, training steps $J_{1}, J_{2}$, network width $m$. Updated user graphs $\\mathcal{G}_{t}^{(1)}$, $\\mathcal{G}_{t}^{(2)}$. Served user $u_{t}$. \\\\\n\\textbf{Output:} Updated model parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$, $[\\vect{\\Theta}_{gnn}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t}$. \\\\\n\n% ---------------------------------\n\n\\BlankLine\n\n$[\\matr{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]_{t}$ = User-Model-Training $\\big( u_{t}, [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}, [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0} \\big)$. \\\\\n\\For{$\\forall u'\\in \\mathcal{U}$, $u' \\neq u_{t}$}{ \n    $[\\matr{\\Theta}_{u'}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{u'}^{(1)}]_{t-1}$, \\quad \n    $[\\matr{\\Theta}_{u'}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{u'}^{(2)}]_{t-1}$\\\\\n} \n$[\\matr{\\Theta}_{gnn}^{(1)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t}$ = GNN-Model-Training $\\big( [\\matr{\\Theta}_{gnn}^{(1)}]_{0}, [\\matr{\\Theta}_{gnn}^{(2)}]_{0} \\big)$. \\\\\nReturn $[\\matr{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(1)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t}$.\n\n\n\\BlankLine\n% ---------------------------------\n\\SetKwProg{myproc}{Procedure}{}{end procedure}\n\\myproc{User-Model-Training $\\big( u_{t}, [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}, [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0} \\big)$}{\n    $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}$, \\quad\n    $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0}$. \\\\\n    \n    %\n    \n    %\n    \\# Training of $f_{u_{t}}^{(1)}(\\cdot )$ \\\\\n    Let $\\mathcal{L}_{u}^{(1)}(\\Theta_{u_{t}}^{(1)}) := \\sum_{\\tau\\in \\mathcal{T}_{u_{t}, t}} \\abs{f_{u_{t}}^{(1)}(\\vect{x}_{\\tau}; \n    \\matr{\\Theta}_{u_{t}}^{(1)}) - r_{\\tau}}^{2}$ \\\\\n    %\n    \\For{$j = 1, 2, \\dots, J_{1}$}{ \n        $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{j} = [\\matr{\\Theta}_{u_{t}}^{(1)}]^{j-1} - \\eta_{1} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}_{u}^{(1)}([\\matr{\\Theta}_{u_{t}}^{(1)}]^{j-1})$ \\\\\n    } \n    \n    %\n    \n    %\n    \\# Training of $f_{u_{t}}^{(2)}(\\cdot )$ \\\\\n    %\n    Let $\\mathcal{L}_{u}^{(2)}(\\Theta_{u_{t}}^{(2)}) := \\sum_{\\tau\\in \\mathcal{T}_{u_{t}, t}} \n    \\abs{f_{u_{t}}^{(2)}(\\nabla_{\\matr{\\Theta}_{u_{t}}^{(1)}} f_{u_{t}}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{\\tau-1}); \\matr{\\Theta}_{u_{t}}^{(2)}) - \n    \\big(r_{\\tau} - f_{u_{t}}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{\\tau-1})\\big)\n    }^{2}$ \\\\\n    %\n    \\For{$j = 1, 2, \\dots, J_{1}$}{ \n        $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{j} = [\\matr{\\Theta}_{u_{t}}^{(2)}]^{j-1} - \\eta_{1}\\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}_{u}^{(2)}([\\matr{\\Theta}_{u_{t}}^{(2)}]^{j-1})$ \\\\\n    } \n    %\n    Let $[\\widehat{\\vect{\\Theta}}_{u_{t}}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{u_{t}}^{(1)}]^{J_{1}}$, \n    $[\\widehat{\\vect{\\Theta}}_{u_{t}}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{u_{t}}^{(2)}]^{J_{1}}$ \\\\\n    %\n    Sample and return new parameters $([\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}, [\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}) \\sim \\{ ([\\widehat{\\vect{\\Theta}}_{u_{t}}^{(1)}]_{\\tau}, [\\widehat{\\vect{\\Theta}}_{u_{t}}^{(2)}]_{\\tau}) \\}_{\\tau\\in [t]}$.\\\\\n    %\n    % Sample and Return new parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$.\n    % Return new parameters $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{J}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{J}$. \\\\\n}\n%\n\n%\n\\BlankLine\n% ---------------------------------\n\\myproc{GNN-Model-Training $\\big( [\\vect{\\Theta}_{gnn}^{(1)}]_{0}$, $[\\vect{\\Theta}_{gnn}^{(2)}]_{0} \\big)$}{\n    $[\\matr{\\Theta}_{gnn}^{(1)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{gnn}^{(1)}]_{0}$, \\quad\n    $[\\matr{\\Theta}_{gnn}^{(2)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{gnn}^{(2)}]_{0}$. \\\\\n    \n    %\n    \n    %\n    \\# Training of $f_{gnn}^{(1)}(\\cdot )$ \\\\\n    Let $\\mathcal{L}_{gnn}^{(1)}(\\Theta_{gnn}^{(1)}) := \\sum_{\\tau\\in [t]} \\abs{f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; \n    \\matr{\\Theta}_{gnn}^{(1)}) - r_{\\tau}}^{2}$ \\\\\n    %\n    \\For{$j = 1, 2, \\dots, J_{2}$}{ \n        $[\\matr{\\Theta}_{gnn}^{(1)}]^{j} = [\\matr{\\Theta}_{gnn}^{(1)}]^{j-1} - \\eta_{2} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}_{gnn}^{(1)}([\\matr{\\Theta}_{gnn}^{(1)}]^{j-1})$ \\\\\n    } \n    \n    %\n    \n    %\n    \\# Training of $f_{gnn}^{(2)}(\\cdot )$ \\\\\n    Apply $f_{gnn}^{(1)}(\\vect{x}_{\\tau})$ to denote $f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})$. \\\\\n    %\n    Let $\\mathcal{L}_{gnn}^{(2)}(\\Theta_{gnn}^{(2)}) := \\sum_{\\tau\\in [t]} \n    \\abs{f_{gnn}^{(2)}(\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{\\tau}), \\mathcal{G}_{\\tau}^{(2)}; \\matr{\\Theta}_{gnn}^{(2)}) - \n    \\big(r_{\\tau} - f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})\\big)\n    }^{2}$ \\\\\n    %\n    \\For{$j = 1, 2, \\dots, J_{2}$}{ \n        $[\\matr{\\Theta}_{gnn}^{(2)}]^{j} = [\\matr{\\Theta}_{gnn}^{(2)}]^{j-1} - \\eta_{2} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}_{gnn}^{(2)}([\\matr{\\Theta}_{gnn}^{(2)}]^{j-1})$ \\\\\n    }\n    %\n    Let $[\\widehat{\\vect{\\Theta}}_{gnn}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{gnn}^{(1)}]^{J_{2}}$, \n    $[\\widehat{\\vect{\\Theta}}_{gnn}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{gnn}^{(2)}]^{J_{2}}$ \\\\\n    %\n    Sample and return new parameters $([\\vect{\\Theta}_{gnn}^{(1)}]_{t}, [\\vect{\\Theta}_{gnn}^{(2)}]_{t}) \\sim \\{ ([\\widehat{\\vect{\\Theta}}_{gnn}^{(1)}]_{\\tau}, [\\widehat{\\vect{\\Theta}}_{gnn}^{(2)}]_{\\tau}) \\}_{\\tau\\in [t]}$. \\\\\n    %\n    % Return new parameters $[\\vect{\\Theta}_{gnn}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t}$.\n}\n\\end{algorithm}\n\n\n\n\\vspace{-0.15cm}\n"
            },
            "section 10": {
                "name": "Intuition of Adaptive Exploration",
                "content": "\n\\label{sec_appd_adaptive_exp}\n\nRecall that GNB adopts a second GNN model ($f_{gnn}^{(2)}$) to adaptively learn the potential gain, which can be either positive or negative. \nThe intuition is that the exploitation model (i.e., $f_{gnn}^{(1)}(\\cdot)$) can \"provide the excessively high estimation of the reward\", and applying the UCB based exploration (e.g., \\cite{Neural-UCB}) can amplify the mistake as the UCB is non-negative. \nFor the simplicity of notation, let us denote the expected reward of an arm $x$ as $\\mathbb{E}[r] = h(x)$, where $h$ is the unknown reward mapping function. The reward estimation is denoted as $\\hat{r} = f_{1}(x)$ where $f_{1}(\\cdot)$ is the exploitation model. \nFor \\name,\nwhen the estimated reward is lower than the expected reward ($f_{1}(x) < h(x)$), we will apply the \"upward\" exploration (i.e., positive exploration score) to increase the chance of arm $x$ being explored. Otherwise, if the estimated reward is higher than the expected reward ($f_{1}(x) > h(x)$), we will apply the \"downward\" exploration  (i.e., negative exploration score)  instead to tackle the excessively high reward estimation. \nHere, we apply the exploration GNN $f_{gnn}^{(2)}$ to adaptively learn the relationship between the gradients of $f_{1}$ and the reward estimation residual $h(x) - f_{1}(x)$, based on the user exploration graph for a refined exploration strategy.\nReaders can also refer to \\cite{EE-Net_ban2021ee} for additional insights of neural adaptive exploration.\n\n% ------------------------------------------------------------------------------\n% ------------------------------------------------------------------------------\n% \\clearpage\n\\vspace{-0.2cm}\n"
            },
            "section 11": {
                "name": "theorem_regret_bound",
                "content": "    \\label{sec_appd_regre_bound_proof}\n\n% Due to the page limit, we include the proof sketch for \\textbf{Theorem} \\ref{theorem_regret_bound} in this section.\n% For the full proof of the regret bound, please refer to our arXiv version of the paper.\n%\nApart from two kinds of estimated user graphs $\\{\\mathcal{G}_{i, t}^{(1)}\\}_{i\\in [a]}$, $\\{\\mathcal{G}_{i, t}^{(2)}\\}_{i\\in [a]}$ at each time step $t$, we can also define true user exploitation graph $\\{\\mathcal{G}_{i, t}^{(1), *}\\}_{i\\in [a]}$ and true user exploration graph $\\{\\mathcal{G}_{i, t}^{(2), *}\\}_{i\\in [a]}$ based on \\textbf{Def.} \\ref{def_exploitation_simi} and \\textbf{Def.} \\ref{def_exploration_simi} respectively. Comparably, the true normalized adjacency matrices of $\\mathcal{G}_{i, t}^{(1), *}, i\\in [a]$ are represented as $\\vect{S}_{i, t}^{(1), *}$.\n%\nWith $r_{t}, r_{t}^{*}$ separately being the rewards for the selected arm $\\vect{x}_{t}\\in \\mathcal{X}_{t}$ and the optimal arm $\\vect{x}_{t}^{*}\\in \\mathcal{X}_{t}$, we formulate the pseudo-regret for a single round $t$, as\n$\n    R_{t} = \\mathbb{E} [r_{t}^{*} |  u_{t}, \\mathcal{X}_{t}] - \\mathbb{E} [r_{t} |  u_{t}, \\mathcal{X}_{t}]\n$\nw.r.t. the candidate arms $\\mathcal{X}_{t}$ and served user $u_{t}$. Here, with \\textbf{Algorithm} \\ref{algo_main}, we denote \n$\n    f_{gnn}(\\vect{x}_{t}) = \\ f_{gnn}^{(1)}(\\vect{x}_{t},~\\mathcal{G}_{t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) \n    + f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}_{t}) ,~\\mathcal{G}_{t}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}),\n$\nwith the arm $\\vect{x}_{t}$, gradients $\\nabla f_{t}^{(1)} (\\vect{x}_{t}) = \\frac{\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{t},~\\mathcal{G}_{t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})}{c_{g}L}$ ($c_{g} > 0$ is the normalization factor, such that $\\norm{\\nabla f_{t}^{(1)} (\\vect{x}_{t})}_{2} \\leq 1$), as well as the estimated user graphs $\\mathcal{G}_{t}^{(1)}$, $\\mathcal{G}_{t}^{(2)}$ related to chosen arm $\\vect{x}_{t}$. \nOn the other hand, with the true graph $\\mathcal{G}_{t}^{(1), *}$ of arm $\\vect{x}_{t}$,\nthe corresponding gradients will be $\\nabla f_{t}^{(1), *} (\\vect{x})$.\nAnalogously, we also have the estimated user graphs $\\mathcal{G}_{t, *}^{(1)}, \\mathcal{G}_{t, *}^{(2)}$ for the optimal arm $\\vect{x}_{t}^{*}$.\n%\nAfterwards, in round $t \\in [T]$, the single-round regret $R_{t}$ will be\n\\begin{displaymath}\n\\begin{split}\n    R_{t} & = \\mathbb{E} [r_{t}^{*} |  u_{t}, \\mathcal{X}_{t}] - \\mathbb{E} [r_{t} |  u_{t}, \\mathcal{X}_{t}] \\\\\n    & = \\mathbb{E} [r_{t}^{*} |  u_{t}, \\mathcal{X}_{t}]  - f_{gnn}(\\vect{x}_{t}) + f_{gnn}(\\vect{x}_{t}) - \\mathbb{E} [r_{t} |  u_{t}, \\mathcal{X}_{t}] \\\\\n    & \\underset{(\\text{i})}{\\leq} \\mathbb{E} [r_{t}^{*} |  u_{t}, \\mathcal{X}_{t}]  - f_{gnn}(\\vect{x}_{t}^{*}) + f_{gnn}(\\vect{x}_{t}) - \\mathbb{E} [r_{t} |  u_{t}, \\mathcal{X}_{t}] \\\\\n    & \\leq  \\mathbb{E} \\big[ \\abs{r_{t}^{*} - f_{gnn}(\\vect{x}_{t}^{*})} \\big|  u_{t}, \\mathcal{X}_{t}  \\big] + \n    \\mathbb{E} \\big[ \\abs{r_{t} - f_{gnn}(\\vect{x}_{t})} \\big|  u_{t}, \\mathcal{X}_{t}  \\big] \\\\\n    & = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}_{t}^{*}),~\\mathcal{G}_{t, *}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - (r_{t}^{*} -  f_{gnn}^{(1)}(\\vect{x}_{t}^{*},~\\mathcal{G}_{t, *}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] + \\\\\n    & \\quad \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}_{t}),~\\mathcal{G}_{t}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})  - (r_{t} -  f_{gnn}^{(1)}(\\vect{x}_{t},~\\mathcal{G}_{t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n    & = \\mathsf{CB}_{t}(\\vect{x}_{t}) + \\mathsf{CB}_{t}(\\vect{x}_{t}^{*})\n\\end{split}\n\\end{displaymath}\nwhere inequality $(\\text{i})$ is due to the arm pulling mechanism, i.e., $f_{gnn}(\\vect{x}_{t}) \\geq f_{gnn}(\\vect{x}_{t}^{*})$, and $\\mathsf{CB}_{t}(\\cdot)$ is the regret bound function in round $t$, formulated by the last equation. Then, given arm $\\vect{x} \\in \\mathcal{X}_{t}$ and its reward $r$, with the aforementioned notation, we have \n% \\begin{equation}\n\\vspace{-0.2cm}\n\\begin{equation}\n\\begin{split}\n     \\mathsf{CB}_{t}(\\vect{x}) & = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) \\\n    - (r -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n    & \\leq \\underbrace{\\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})  - (r -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))} \\ \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] }_{I_{1}}  \\\\\n    & + \\underbrace{\\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) - f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})}  \\bigg| u_{t}, \\mathcal{X}_{t}\\bigg]}_{I_{2}} \\\\\n    & + \\underbrace{\\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}), ~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) -   f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})} \\ \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] }_{I_{3}} \\\\\n    & + \\underbrace{ \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})    - f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})} \\ \\bigg| u_{t}, \\mathcal{X}_{t}  \\bigg] }_{I_{4}}.\n\\end{split}\n\\label{eq_single_CB}\n\\end{equation}\n\\vspace{-0.2cm}\n% \\label{eq_single_CB}\n\n\n% \\begin{equation}\n% \\begin{split}\n%      & \\mathsf{CB}_{t}(\\vect{x})  = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - \\\\\n%      &\\qquad\\qquad\\qquad\\qquad\\qquad (r -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n%     \\leq\\ & \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) \\\\\n%     &\\qquad\\qquad\\qquad - (r -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] + \\\\\n%     & \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) - f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] + \\\\\n%     & \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})  \\\\\n%     &\\qquad\\qquad\\qquad - f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})}  \\bigg| u_{t}, \\mathcal{X}_{t}\\bigg] + \\\\\n%     & \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})\\\\\n%     &\\qquad\\qquad\\qquad- f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})}  \\bigg| u_{t}, \\mathcal{X}_{t}\\bigg] \\\\\n%     &=\\ I_{1} + I_{2} + I_{3} + I_{4}\n% \\end{split}\n% \\label{eq_single_CB}\n% \\end{equation}\nHere, we have the term $I_{1}$ representing the estimation error induced by the GNN model parameters $\\{[\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}, [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}\\}$, the term $I_{2}$ denoting the error caused by the estimation of user exploitation graph. Then, error term $I_{3}$ is caused by the estimation of user exploration graph, and term $I_{4}$ is the output difference given input gradients $\\nabla f_{t}^{(1), *} (\\vect{x})$ and $ \\nabla f_{t}^{(1)} (\\vect{x})$, which are individually associated with the true user exploitation graph $\\mathcal{G}^{(1), *}$ and the estimation $\\mathcal{G}^{(1)}$. \n%\nThese four terms $I_{1}, I_{2}, I_{3}, I_{4}$ can be respectively bounded by \\blemma \\ref{lemma_term_I_1_selected_arm} (\\textbf{Corollary} \\ref{corollary_term_I_1_optimal_arm} and the bounds in Subsection \\ref{subsec_bounding_I_1}), \\blemma \\ref{lemma_I_2_bound}, \\blemma \\ref{lemma_I_3_bound}, and \\blemma \\ref{lemma_I_4_overall_bound} in the appendix. \nAfterwards, with the notation from \\btheorem \\ref{theorem_regret_bound}, we have the pseudo regret after $T$ rounds, i.e., $R(T)$, as\n\n\\begin{displaymath}\n\\begin{split}\n    %  R(t) & = \\sum_{t\\in [T]} R_{t} \\\\\n    % & \\leq  2\\cdot \\bigg( \n    % %\n    % \\sqrt{t} \\cdot \\big( \\sqrt{2\\xi_{2}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{2}) \\sqrt{2\\log(\\frac{Tn\\cdot a}{\\delta})}\\big) \\\\\n    % %\n    % & \\quad + \\big(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\big)\\cdot \\mathcal{O}(\\frac{t^{3}L}{\\rho\\sqrt{m}}\\log(m)) +\n    % \\mathcal{O}\\big( \\frac{t^{4}L^{2} \\log^{11/6} (m)}{\\rho^{4/3}m^{1/6}} \\big) \\\\\n    % %\n    % &\\quad + \\mathcal{O}(\\xi_{L}) \\cdot \\sqrt{8t} \\cdot \\big( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{Tn\\cdot a}{\\delta})}\\big) \n    % %\n    % +  \\mathcal{O} ( \\frac{\\xi_{L}tL^{7/2}  \\sqrt{\\log m}}{\\sqrt{mn}}) + 4\\Gamma_{t}\n    % \\bigg) \\implies  \\\\\n    %\n    R(T) = & \\sum_{t\\in [T]} R_{t} \\\\\n    & \\leq 2 \\cdot \\sqrt{T} \\big( \\sqrt{2\\xi_{2}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{2}) \\sqrt{2\\log(\\frac{Tn\\cdot a}{\\delta})}\\big)  +\n    \\sqrt{T}\\cdot \\mathcal{O}(\\xi_{L}) \\cdot \\big( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{Tn\\cdot a}{\\delta})}\\big) + \\mathcal{O}(1) \n    %\n    % & \\leq \\sqrt{T} \\cdot  \\bigg( \\mathcal{O}(\\sqrt{\\xi_{2}} + \\xi_{L}\\sqrt{\\xi_{1}} ) \n    %     + \\mathcal{O}(L\\xi_{L}) + \\mathcal{O}(\\xi_{L}) \\cdot \\sqrt{2\\log(\\frac{Tn\\cdot a}{\\delta})}\n    %      \\bigg) + \\\\\n    % & \\qquad  \\sqrt{T}\\cdot \\mathcal{O}(L) + \\mathcal{O}(\\sqrt{T}) + \\mathcal{O}(1)\n\\end{split}\n\\end{displaymath}\nwhere the inequality is because we have\nsufficient large network width $m \\geq \\Omega ( \\text{Poly}(T, L, a, 1 / \\rho) \\cdot \\log(1 / \\delta) ) $ as indicated in \\textbf{Theorem} \\ref{theorem_regret_bound}. Meanwhile, with sufficient $m \\geq \\Omega(\\text{Poly}(T, \\rho^{-1}))$, the terms $\\gamma_{1}, \\gamma_{2}$ can also be upper bounded by $\\mathcal{O}(1)$, which leads to\n\\begin{displaymath}\n\\begin{split}\n   & R(T)  \\leq \\sqrt{T} \\cdot  \\big( \\mathcal{O}(\\sqrt{\\xi_{2}} + \\xi_{L}\\sqrt{\\xi_{1}} ) \n        + \\mathcal{O}(L\\xi_{L}) + \\mathcal{O}(\\xi_{L}) \\cdot \\sqrt{2\\log(\\frac{Tn\\cdot a}{\\delta})}\n         \\big) + \\\\\n     & \\qquad\\qquad \\sqrt{T}\\cdot \\mathcal{O}(L) + \\mathcal{O}(\\xi_{L}) + \\mathcal{O}(1) \\\\\n    & \\leq  \\sqrt{T} \\cdot  \\big( \n        \\mathcal{O}(L\\xi_{L}) + \\mathcal{O}(\\xi_{L}) \\cdot \\sqrt{2\\log(\\frac{Tn\\cdot a}{\\delta})}\n         \\big) +  \\sqrt{T} \\mathcal{O}(L) + \\mathcal{O}(\\xi_{L}) + \\mathcal{O}(1)  \\\\\n\\end{split}\n\\end{displaymath}\nsince we have $\\xi_{1}, \\xi_{2} \\leq \\mathcal{O}(\\frac{1}{T})$.\nThis will complete the proof.\n\n% Apart from the two remarks in the main body (Remark \\ref{remark_bound_iid}, \\ref{remark_bound_sqrt_n}), we also want to mention another improvement over existing works with the \\textbf{Remark} \\ref{remark_bound_dimension} below.\n\n\n\n\n% % ===================================================================================== \n% % ===================================================================================== \n% % ===================================================================================== Algorithm 3\n% \\newpage\n% \\begin{algorithm}[ht]\n% \\caption{Model Training}\n% \\label{algo_training}\n% \\textbf{Input:} Initial parameter $\\vect{\\Theta_{0}}$, step size $\\eta_{1}, \\eta_{2}$, training steps $J_{1}, J_{2}$, network width $m$. Updated user graphs $\\mathcal{G}_{t}^{(1)}$, $\\mathcal{G}_{t}^{(2)}$. Served user $u_{t}$. \\\\\n% \\textbf{Output:} Updated model parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$, $[\\vect{\\Theta}_{gnn}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t}$. \\\\\n\n% % ---------------------------------\n\n% \\BlankLine\n\n% $[\\matr{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]_{t}$ = User-Model-Training $\\big( u_{t}, [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}, [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0} \\big)$. \\\\\n% \\For{$\\forall u'\\in \\mathcal{U}$, $u' \\neq u_{t}$}{ \n%     $[\\matr{\\Theta}_{u'}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{u'}^{(1)}]_{t-1}$, \\quad \n%     $[\\matr{\\Theta}_{u'}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{u'}^{(2)}]_{t-1}$\\\\\n% } \n% $[\\matr{\\Theta}_{gnn}^{(1)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t}$ = GNN-Model-Training $\\big( [\\matr{\\Theta}_{gnn}^{(1)}]_{0}, [\\matr{\\Theta}_{gnn}^{(2)}]_{0} \\big)$. \\\\\n% Return $[\\matr{\\Theta}_{u_{t}}^{(1)}]_{t}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(1)}]_{t}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t}$.\n\n% \\BlankLine\\BlankLine\n\n% % ---------------------------------\n% \\SetKwProg{myproc}{Procedure}{}{end procedure}\n% \\myproc{User-Model-Training $\\big( u_{t}, [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}, [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0} \\big)$}{\n%     $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{u_{t}}^{(1)}]_{0}$, \\quad\n%     $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{u_{t}}^{(2)}]_{0}$. \\\\\n    \n%     %\n%     \\BlankLine\\BlankLine\n%     %\n%     \\# Training of $f_{u}^{(1)}(\\cdot )$ \\\\\n%     Let $\\mathcal{L}(\\Theta_{u_{t}}^{(1)}) := \\sum_{\\tau\\in \\mathcal{X}_{u_{t}, t}} \\abs{f_{u}^{(1)}(\\vect{x}_{\\tau}; \n%     \\matr{\\Theta}_{u_{t}}^{(1)}) - r_{\\tau}}^{2}$ \\\\\n%     %\n%     \\For{$j = 1, 2, \\dots, J_{1}$}{ \n%         $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{j} = [\\matr{\\Theta}_{u_{t}}^{(1)}]^{j-1} - \\eta_{1} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{u_{t}}^{(1)}]^{j-1})$ \\\\\n%     } \n    \n%     %\n%     \\BlankLine\\BlankLine\n%     %\n%     \\# Training of $f_{u}^{(2)}(\\cdot )$ \\\\\n%     %\n%     Let $\\mathcal{L}(\\Theta_{u_{t}}^{(2)}) := \\sum_{\\tau\\in \\mathcal{X}_{u_{t}, t}} \n%     \\abs{f_{u}^{(2)}(\\nabla_{\\matr{\\Theta}_{u_{t}}^{(1)}} f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{\\tau-1}); \\matr{\\Theta}_{u_{t}}^{(2)}) - \n%     \\big(r_{\\tau} - f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u_{t}}^{(1)}]_{\\tau-1})\\big)\n%     }^{2}$ \\\\\n%     %\n%     \\For{$j = 1, 2, \\dots, J_{1}$}{ \n%         $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{j} = [\\matr{\\Theta}_{u_{t}}^{(2)}]^{j-1} - \\eta_{1}\\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{u_{t}}^{(2)}]^{j-1})$ \\\\\n%     } \n%     %\n%     Let $[\\widehat{\\vect{\\Theta}}_{u_{t}}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{u_{t}}^{(1)}]^{J}$, \n%     $[\\widehat{\\vect{\\Theta}}_{u_{t}}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{u_{t}}^{(2)}]^{J}$ \\\\\n%     %\n%     Return the new parameter $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t} \\sim \\{ [\\widehat{\\vect{\\Theta}}_{u_{t}}^{(1)}]_{\\tau} \\}_{\\tau\\in [t]}$ and $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t} \\sim \\{ [\\widehat{\\vect{\\Theta}}_{u_{t}}^{(2)}]_{\\tau} \\}_{\\tau\\in [t]}$. \\\\\n%     %\n%     % Return new parameters $[\\vect{\\Theta}_{u_{t}}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{u_{t}}^{(2)}]_{t}$.\n%     % Return new parameters $[\\matr{\\Theta}_{u_{t}}^{(1)}]^{J}$, $[\\matr{\\Theta}_{u_{t}}^{(2)}]^{J}$. \\\\\n% }\n% %\n% \\BlankLine\\BlankLine\\BlankLine\n% %\n\n% % ---------------------------------\n% \\myproc{GNN-Model-Training $\\big( [\\vect{\\Theta}_{gnn}^{(1)}]_{0}$, $[\\vect{\\Theta}_{gnn}^{(2)}]_{0} \\big)$}{\n%     $[\\matr{\\Theta}_{gnn}^{(1)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{gnn}^{(1)}]_{0}$, \\quad\n%     $[\\matr{\\Theta}_{gnn}^{(2)}]^{0} \\xleftarrow{} [\\matr{\\Theta}_{gnn}^{(2)}]_{0}$. \\\\\n    \n%     %\n%     \\BlankLine\n%     %\n%     \\# Training of $f_{gnn}^{(1)}(\\cdot )$ \\\\\n%     Let $\\mathcal{L}(\\Theta_{u_{t}}^{(1)}) := \\sum_{\\tau\\in \\mathcal{X}_{u_{t}, t}} \\abs{f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; \n%     \\matr{\\Theta}_{gnn}^{(1)}) - r_{\\tau}}^{2}$ \\\\\n%     %\n%     \\For{$j = 1, 2, \\dots, J_{2}$}{ \n%         $[\\matr{\\Theta}_{gnn}^{(1)}]^{j} = [\\matr{\\Theta}_{gnn}^{(1)}]^{j-1} - \\eta_{2} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{gnn}^{(1)}]^{j-1})$ \\\\\n%     } \n    \n%     %\n%     \\BlankLine\n%     %\n%     \\# Training of $f_{gnn}^{(2)}(\\cdot )$ \\\\\n%     $f_{gnn}^{(1)}(\\vect{x}_{\\tau}) \\leftarrow f_{gnn}^{(1)}(\\vect{x}_{\\tau}, \\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})$. \\\\\n%     %\n%     Let $\\mathcal{L}(\\Theta_{u_{t}}^{(2)}) := \\sum_{\\tau\\in \\mathcal{X}_{u_{t}, t}} \n%     \\abs{f_{gnn}^{(2)}(\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x}_{\\tau}), \\mathcal{G}_{\\tau}^{(2)}; \\matr{\\Theta}_{gnn}^{(2)}) - \n%     \\big(r_{\\tau} - f_{gnn}^{(1)}(\\vect{x}_{\\tau})\\big)\n%     }^{2}$ \\\\\n%     %\n%     \\For{$j = 1, 2, \\dots, J_{2}$}{ \n%         $[\\matr{\\Theta}_{gnn}^{(2)}]^{j} = [\\matr{\\Theta}_{gnn}^{(2)}]^{j-1} - \\eta_{2} \\cdot\\nabla_{\\vect{\\Theta}} \\mathcal{L}([\\matr{\\Theta}_{gnn}^{(2)}]^{j-1})$ \\\\\n%     }\n%     %\n%     Let $[\\widehat{\\vect{\\Theta}}_{gnn}^{(1)}]_{t} \\leftarrow [\\matr{\\Theta}_{gnn}^{(1)}]^{J}$, \n%     $[\\widehat{\\vect{\\Theta}}_{gnn}^{(2)}]_{t} \\leftarrow [\\matr{\\Theta}_{gnn}^{(2)}]^{J}$ \\\\\n%     %\n%     Return the new parameter $[\\vect{\\Theta}_{gnn}^{(1)}]_{t} \\sim \\{ [\\widehat{\\vect{\\Theta}}_{gnn}^{(1)}]_{\\tau} \\}_{\\tau\\in [t]}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t} \\sim \\{ [\\widehat{\\vect{\\Theta}}_{gnn}^{(2)}]_{\\tau} \\}_{\\tau\\in [t]}$. \\\\\n%     %\n%     % Return new parameters $[\\vect{\\Theta}_{gnn}^{(1)}]_{t}$ and $[\\vect{\\Theta}_{gnn}^{(2)}]_{t}$.\n% }\n% \\end{algorithm}\n\n\n"
            },
            "section 12": {
                "name": "Generalization of User Networks after GD",
                "content": "     \\label{sec_appd_analysis_user}\n\nIn this section, we present the generalization results of user networks $f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)}), f_{u}^{(2)}(\\cdot; \\matr{\\Theta}_{u}^{(2)}), u\\in \\mathcal{U}$. \n%\nUp to a certain time step $t$ and for a given user $u\\in \\mathcal{U}$, we have all its past arm-reward pairs $\\mathcal{P}_{u, t-1} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in \\mathcal{T}_{u, t}}$. \n%\nWithout loss of generality, following an analogous approach as in \\citep{conv_theory-allen2019convergence}, we let the last coordinate of each arm context $\\vect{x}$ to be a fixed small constant $c_{x} > 0$. \nThis can be achieved by simplify normalizing the input $\\vect{x}$ as $\\vect{x} \\leftarrow \\big( \\vect{x}\\cdot \\sqrt{1-c_{x}^{2}}, c_{x} \\big)$.\nMeanwhile, inspired by \\citep{conv_theory-allen2019convergence}, with two vectors $\\Tilde{\\vect{x}}, \\vect{x}$ such that $\\norm{\\Tilde{\\vect{x}}}_{2} \\leq 1, \\norm{\\vect{x}}_{2} = 1$, we first define the the following operator\n\\begin{equation}\n\\begin{split}\n    \\phi(\\Tilde{\\vect{x}}, \\vect{x}) = \n    (\\frac{\\Tilde{\\vect{x}}}{\\sqrt{2}}, \\frac{\\vect{x}}{2}, c)\n\\end{split}\n\\label{eq_concat_transformation}\n\\end{equation}\nas the concatenation of the two vectors $\\frac{\\Tilde{\\vect{x}}}{\\sqrt{2}}, \\frac{\\vect{x}}{2}$ and one constant $c$, where $c = \\sqrt{\\frac{3}{4} - (\\frac{\\norm{\\Tilde{\\vect{x}}}_{2}}{\\sqrt{2}})^{2}} \\geq \\frac{1}{2}$. And this operator makes the transformed vector $\\norm{\\phi(\\Tilde{\\vect{x}}, \\vect{x})}_{2} = 1$.\nThe idea of this operator is to make the gradients $\\nabla_{\\matr{\\Theta}_{u}^{(1)}} f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$ of the user exploitation model, which is the input of the user exploration model $f_{u}^{(2)}(\\cdot)$, comply with the normalization requirement and the separateness assumption (\\textbf{Assumption} \\ref{assumption_separateness}). For the sake of analysis, we will adopt this operation in the following proof. Note that this operator is just one possible solution, and our results could be easily generalized to other forms of input gradients under the unit-length and separateness assumption. Similar ideas are also applied in previous works \\citep{EE-Net_ban2021ee}.\n\n\n% ==================================================================\n",
                "subsection 12.1": {
                    "name": "User Exploitation Model",
                    "content": "\n\n% -------------------------------------\nWith the convergence result presented in \\blemma \\ref{lemma_convergence_user_f_1}, we could bound the output of the user exploitation model $f_{u}^{(1)}(\\cdot)$ after GD with the following lemma. \n\n\n% -------------------------------------\n\\begin{lemma} \\label{lemma_user_f_1_output_bound}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given user $u\\in\\mathcal{U}$ and its past records $\\mathcal{P}_{u, t-1}$ up to time step $t$, we suppose $m, \\eta_1, J_1$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}.\nThen, with probability at least $1 - \\delta$, given an arm-reward pair $(\\vect{x}, r)$, we have\n\\begin{displaymath}\n    \\abs{f_{u}^{(1)} (\\vect{x}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{t})} \\leq \\gamma_{1}\n\\end{displaymath}\nwhere \n\\begin{displaymath}\n    \\gamma_{1} = \n    2 + \\mathcal{O} \\left( \\frac{ t^3 L}{ n^{3} \\rho\\sqrt{m}} \\log m \\right)\n    + \\mathcal{O} \\left(  \\frac{ L^{2}t^4 }{ n^{4} \\rho^{4 / 3} m^{1 / 6}} \\log^{11 / 6} (m) \\right).\n\\end{displaymath}\n\\end{lemma}\n\n\n\n\\textbf{Proof.}\nFor brevity, we use $\\widehat{\\matr{\\Theta}}_{u}^{(1)}$ to denote $[\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{t}$.\nThe LHS of the inequality could be written as\n\\begin{displaymath}\n\\begin{split}\n    \\abs{f_{u}^{(1)} (\\vect{x}; \\widehat{\\matr{\\Theta}}_{u}^{(1)})} \\leq & \n    \\abs{f_{u}^{(1)} (\\vect{x}; \\widehat{\\matr{\\Theta}}_{u}^{(1)}) -  f_{u}^{(1)} (\\vect{x}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0}) - \\inp{\\nabla_{[\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0}} f_{u}^{(1)} (\\vect{x}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0})}{\\widehat{\\matr{\\Theta}}_{u}^{(1)} - [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0}}} \\\\\n    & + \\abs{f_{u}^{(1)} (\\vect{x}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0}) + \\inp{\\nabla_{[\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0}} f_{u}^{(1)} (\\vect{x}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0})}{\\widehat{\\matr{\\Theta}}_{u}^{(1)} - [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0}}}.\n\\end{split}\n\\end{displaymath}\nHere, we could bound the first term on the RHS with \\blemma \\ref{lemma_FC_network_inner_product}. Applying \\blemma \\ref{lemma_FC_network_output_and_gradient} on the second term, and recalling $\\norm{\\widehat{\\matr{\\Theta}}_{u}^{(1)} - [\\matr{\\Theta}_{u}^{(1)}]_{0}}_{2} \\leq \\omega$, would give\n\\begin{displaymath}\n\\begin{split}\n    \\abs{f_{u}^{(1)} (\\vect{x}; \\widehat{\\matr{\\Theta}}_{u}^{(1)})} &\\leq \n    2 +  \n    \\norm{\\nabla_{[\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0}} f_{u}^{(1)} (\\vect{x}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{0})}_{2}  \\norm{\\widehat{\\matr{\\Theta}}_{u}^{(1)} - [\\matr{\\Theta}_{u}^{(1)}]_{0}}_{2} + \\\\\n    &\\qquad \\mathcal{O}(\\omega^{1/3}L^{2}\\sqrt{m\\log(m)}) \\cdot \\norm{\\widehat{\\matr{\\Theta}}_{u}^{(1)} - [\\matr{\\Theta}_{u}^{(1)}]_{0}}_{2} \\\\\n    & \\leq 2 + \\mathcal{O}(L) \\cdot \\norm{\\widehat{\\matr{\\Theta}}_{u}^{(1)} - [\\matr{\\Theta}_{u}^{(1)}]_{0}}_{2}\n    + \\mathcal{O}(L^{2}\\sqrt{m\\log(m)}) (\\norm{\\widehat{\\matr{\\Theta}}_{u}^{(1)} - [\\matr{\\Theta}_{u}^{(1)}]_{0}}_{2})^{\\frac{4}{3}}.\n\\end{split}\n\\end{displaymath}\nThen, with $T_{u, t} = \\frac{t}{n}$, applying the conclusion of \\blemma \\ref{lemma_convergence_user_f_1} would lead to \n\\begin{displaymath}\n\\begin{split}\n    \\abs{f_{u}^{(1)} (\\vect{x}; \\widehat{\\matr{\\Theta}}_{u}^{(1)})} &\\leq \n    2 + \\mathcal{O}(L) \\cdot  \\mathcal{O} \\left( \\frac{ (T_{u, t})^3}{ \\rho \\sqrt{m}} \\log m \\right)\n    + \\mathcal{O}(L^{2}\\sqrt{m\\log(m)}) \\left( \\mathcal{O} ( \\frac{ (T_{u, t})^3}{ \\rho \\sqrt{m}} \\log m ) \\right)^{\\frac{4}{3}} \\\\\n    & = 2 + \\mathcal{O} \\left( \\frac{ t^3 L}{ n^{3} \\rho\\sqrt{m}} \\log m \\right)\n    + \\mathcal{O} \\left(  \\frac{ L^{2}t^4}{ n^{4} \\rho^{4 / 3} m^{1 / 6}} \\log^{11 / 6} (m) \\right) = \\gamma_{1}.\n\\end{split}\n\\end{displaymath}\n\\qed\n\n\n% -------------------------------------\nThen, under the assumption of arm separateness (\\textbf{Assumption} \\ref{assumption_separateness}), we proceed to bound the reward estimation error of the user exploitation network  $f_{u}^{(1)} (\\cdot; [\\matr{\\Theta}_{u}^{(1)}]_{t})$ in the current round $t$.\n\n\\begin{lemma} \\label{lemma_user_f_1_est_error}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given user $u\\in\\mathcal{U}$ and its past records $\\mathcal{P}_{u, t-1}$, we suppose $m, \\eta_1, J_1$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{u}^{(1)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$. Consider the past records $\\mathcal{P}_{u, t}$ up to round $t$ are generated by a fixed policy when witness the candidate arms $\\{\\mathcal{X}_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$.\nThen, with probability at least $1 - \\delta$ given an arm-reward pair $(\\vect{x}_{t}, r_{t})$, we have\n\\begin{displaymath}\n    \\sum_{\\tau\\in \\mathcal{T}_{u, t} \\cup \\{t\\}} \\mathbb{E} \\big[ \\abs{f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) - r_{\\tau}} ~\\big| \\mathcal{X}_{\\tau} \\big] \\leq \n    \\sqrt{\\frac{t}{n}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg)\n\\end{displaymath}\nwhere $r_{\\tau}$ is the corresponding reward generated by the reward mapping function given an arm $\\vect{x}_{\\tau}$.\n\\end{lemma}\n\n\\textbf{Proof.}\nWe proof this Lemma following a similar approach as in Lemma C.1 from \\citep{EE-Net_ban2021ee} and Lemma D.1 from \\citep{Meta-Ban}. First, for the LHS and with $\\tau \\in \\mathcal{T}_{u, t} \\cup \\{t\\}$, we have \n\\begin{displaymath}\n    \\abs{f_{u}^{(1)} (\\vect{x}_{t}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{t}} \\leq \n    \\abs{f_{u}^{(1)} (\\vect{x}_{t}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau})} + \\abs{r_{t}} \\leq 1 + \\gamma_{1}\n\\end{displaymath}\nbased on the conclusion from \\blemma \\ref{lemma_user_f_1_output_bound}.\nThen, for user $u$, we define the following martingale difference sequence with regard to the previous records $\\mathcal{P}_{u, \\tau}$ up to round $\\tau$ as\n\\begin{displaymath}\n    V_{\\tau}^{(1)} = \\mathbb{E}[ \\abs{f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{\\tau}} ] \n    - \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_\\tau}.\n\\end{displaymath}\nSince the records in set $\\mathcal{P}_{u, \\tau}$ are sharing the same reward mapping function, we have the expectation\n\\begin{displaymath}\n    \\mathbb{E}[V_{\\tau}^{(1)} \\big| F_{u, \\tau} ] = \\mathbb{E}\n    [ \\abs{f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{\\tau}} ] - \n    \\mathbb{E}[\\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_\\tau}  \\big| F_{u, \\tau} ] = 0.\n\\end{displaymath}\nwhere $F_{u, \\tau}$ denotes the filtration given the past records $\\mathcal{P}_{u, \\tau}$. And we have the mean value of $V_{\\tau}^{(1)}$ across different time steps as\n\\begin{displaymath}\n    \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} [V_{\\tau}^{(1)}] = \n    \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\mathbb{E}\n    [ \\abs{f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{\\tau}} ] \n    - \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_\\tau}.\n\\end{displaymath}\nwith the expectation of zero.\nThen, we proceed to bound the expected estimation error of the exploitation model with the estimation error from existing samples\nfollowing the Proposition 1 from \\citep{general_bound_online-cesa2004generalization}. Applying the Azuma-Hoeffding inequality, with a constant $\\delta \\in (0, 1)$, it leads to\n\\begin{displaymath}\n\\begin{split}\n    \\mathbb{P}\\bigg[ \n        \\frac{1}{T_{u, t}} &\\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\mathbb{E}\n        [ \\abs{f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{t}} ] - \n        \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_\\tau} \\\\\n        & \\geq (1+\\gamma_{1}) \\cdot \\sqrt{\\frac{2}{T_{u, t}}\\ln(\\frac{1}{\\delta})}\n    \\bigg] \\leq \\delta.\n\\end{split}  \n\\end{displaymath}\nAs we have the parameter $[\\matr{\\Theta}_{u}^{(1)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$, with the probability at least $1-\\delta$, the expected loss on $[\\matr{\\Theta}_{u}^{(1)}]_{t}$ could be bounded as\n\\begin{displaymath}\n\\begin{split}\n    \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\mathbb{E}\n        [ \\abs{f_{u}^{(1)}(\\vect{x}_{t}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{t}} ] \n    \\leq (1+\\gamma_{1}) \\cdot \\sqrt{\\frac{2}{T_{u, t}}\\ln(\\frac{1}{\\delta})} \n        + \\bigg(\\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_\\tau} \\bigg)\n\\end{split}  \n\\end{displaymath}\nwhere for the second term on the RHS, we have\n\\begin{displaymath}\n\\begin{split}\n    \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} & \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_\\tau} \\leq\n    \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{t}) - r_\\tau} + \n    \\frac{3L\\sqrt{2T_{u, t}}}{2} \\cdot \\frac{1}{T_{u, t}} \\\\\n    & \\leq \\frac{1}{T_{u, t}} \\sqrt{T_{u, t}\\cdot \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{t}) - r_\\tau}^{2}} + \n    \\frac{3L}{\\sqrt{2T_{u, t}}} \\\\\n    & \\leq \\sqrt{\\frac{\\xi_{1}}{T_{u, t}}} + \n    \\frac{3L}{\\sqrt{2T_{u, t}}} \n\\end{split}  \n\\end{displaymath}\nwhere the first inequality is the application of \\blemma \\ref{lemma_FC_network_bound_sampled_parameters}, and the last inequality is due to \\blemma \\ref{lemma_convergence_user_f_1}. Summing up all the components and applying the union bound for all $a$ arms, all $n$ users and $t$ time steps would complete the proof.\n\\qed\n\n\n\n% -------------------------------------\nThen, we also have the following Corollary for the rest of the candidate arms $\\vect{x}_{i, t} \\in \\big( \\mathcal{X}_{t} \\setminus \\{\\vect{x}_{t}\\} \\big)$.\n\n\\begin{corollary} \\label{corollary_user_f_1_est_error_other_arms}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given user $u\\in\\mathcal{U}$ and its past records $\\mathcal{P}_{u, t-1}$, we suppose $m, \\eta_1, J_1$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{u}^{(1)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$. For an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, consider its union set with the the collection of arms $\\widetilde{\\mathcal{P}}_{u, t} \\cup \\{\\vect{x}_{i, t}, r_{i, t}\\}$ are generated by a fixed policy when witness the candidate arms $\\{\\mathcal{X}_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$, with $\\widetilde{\\mathcal{P}}_{u, t} = \\{\\vect{x}_{i_{\\tau}, \\tau}, r_{i_{\\tau}, \\tau}\\}_{\\tau}$ being the collection of arms chosen by this policy.\nThen, with probability at least $1 - \\delta$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in \\mathcal{T}_{u, t}\\cup \\{t\\}} \\mathbb{E}  \\big[& \\abs{f_{u}^{(1)} (\\vect{x}_{i_{\\tau}, \\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{t}) - r_{i_{\\tau}, \\tau}} ~\\big| \\mathcal{X}_{\\tau} \\big] \\leq \n    \\sqrt{T_{u, t}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}\n\\end{split}\n\\end{displaymath}\nwhere $r_{i, \\tau}$ is the corresponding reward generated by the mapping function given an arm $\\vect{x}_{i, \\tau}$, and \n\\begin{displaymath}\n\\begin{split}\n    \\Gamma_{t} \\leq \n    \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(\\frac{t^{4}L}{\\rho\\sqrt{m}}\\log(m)) + \n    \\mathcal{O}\\bigg( \\frac{t^{5}L^{2} \\log^{11/6} (m)}{\\rho^{4/3}m^{1/6}} \\bigg).\n\\end{split}  \n\\end{displaymath}\n\\end{corollary}\n\n\\textbf{Proof.}\nThe proof of this Corollary follows an analogous approach as in Lemma \\ref{lemma_user_f_1_est_error}. \nFirst, suppose a shadow model $f_{u}^{(1)} (\\cdot; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{t})$, which is trained on the alternative trajectory $\\widetilde{\\mathcal{P}}_{u, t}$. Analogous to the proof of Lemma \\ref{lemma_user_f_1_est_error}, for user $u$, we can define the following martingale difference sequence with regard to the previous records $\\widetilde{\\mathcal{P}}_{u, \\tau}$ up to round $\\tau\\in [t]$ as\n\\begin{displaymath}\n    \\tilde{V}_{\\tau}^{(1)} = \\mathbb{E}[ \\abs{f_{u}^{(1)}(\\vect{x}_{i_{\\tau}, \\tau}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{\\tau}} ] \n    - \\abs{ f_{u}^{(1)}(\\vect{x}_{i_{\\tau}, \\tau}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{i_{\\tau}, \\tau}}.\n\\end{displaymath}\nSince the records in set $\\widetilde{\\mathcal{P}}_{u, \\tau}$ are sharing the same reward mapping function, we have the expectation\n\\begin{displaymath}\n    \\mathbb{E}[\\tilde{V}_{\\tau}^{(1)} \\big| \\tilde{F}_{u, \\tau} ] = \\mathbb{E}\n    [ \\abs{f_{u}^{(1)}(\\vect{x}_{i_{\\tau}, \\tau}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{i_{\\tau}, \\tau}} ] - \n    \\mathbb{E}[\\abs{ f_{u}^{(1)}(\\vect{x}_{i_{\\tau}, \\tau}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{i_{\\tau}, \\tau}}  \\big| \\tilde{F}_{u, \\tau} ] = 0.\n\\end{displaymath}\nwhere $\\tilde{F}_{u, \\tau}$ denotes the filtration given the past records $\\widetilde{\\mathcal{P}}_{u, \\tau}$. The mean value of $\\tilde{V}_{\\tau}^{(1)}$ across different time steps will be\n\\begin{displaymath}\n    \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} [\\tilde{V}_{\\tau}^{(1)}] = \n    \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\mathbb{E}\n    [ \\abs{f_{u}^{(1)}(\\vect{x}_{i_{\\tau}, \\tau}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{i_{\\tau}, \\tau}} ] \n    - \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{i_{\\tau}, \\tau}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{i_{\\tau}, \\tau}}.\n\\end{displaymath}\nwith the expectation of zero. Afterwards, applying the Azuma-Hoeffding inequality, with a constant $\\delta \\in (0, 1)$, it leads to\n\\begin{displaymath}\n\\begin{split}\n    \\mathbb{P}\\bigg[ \n        \\frac{1}{T_{u, t}} &\\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\mathbb{E}\n        [ \\abs{f_{u}^{(1)}(\\vect{x}_{i_{\\tau}, \\tau}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{i_{\\tau}, \\tau}} ] - \n        \\frac{1}{T_{u, t}} \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{i_{\\tau}, \\tau}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_{i_{\\tau}, \\tau}} \\\\\n        & \\geq (1+\\gamma_{1}) \\cdot \\sqrt{\\frac{2}{T_{u, t}}\\ln(\\frac{1}{\\delta})}\n    \\bigg] \\leq \\delta.\n\\end{split}  \n\\end{displaymath}\nTo bound the output difference between the shadow model $f_{u}^{(1)} (\\cdot; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{t})$ and the model we trained based on received records $f_{u}^{(1)} (\\cdot; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{t})$, we apply the conclusion from \\textbf{Lemma} \\ref{lemma_FC_network_different_parameters}, which leads to that given the same input $\\vect{x}$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\abs{f_{u}^{(1)} (\\vect{x}; [\\widetilde{\\matr{\\Theta}}_{u}^{(1)}]_{t}) & - f_{u}^{(1)} (\\vect{x}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{t})} \\leq \\\\\n    & \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(\\frac{t^{3}L}{\\rho\\sqrt{m}}\\log(m)) + \n    \\mathcal{O}\\bigg( \\frac{t^{4}L^{2} \\log^{11/6} (m)}{\\rho^{4/3}m^{1/6}} \\bigg).\n\\end{split}  \n\\end{displaymath}\nFinally, assembling all the components together will finish the proof.\n\n\\qed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% ==================================================================\n"
                },
                "subsection 12.2": {
                    "name": "User Exploration Model",
                    "content": "\n\nTo ensure the unit length of $f_{u}^{(2)}(\\cdot)$'s input, we normalize the gradient $\\frac{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{t}} f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{t})}{c'_{g}L}$ with \\blemma \\ref{lemma_FC_network_output_and_gradient}, \\blemma \\ref{lemma_FC_network_gradient_difference} and a normalization constant $c'_{g} > 0$. \nThen, to satisfy the separateness (\\textbf{Assumption} \\ref{assumption_separateness}) assumption, we adopt the operation mentioned in \\textbf{Eq.} \\ref{eq_concat_transformation} to derive the transformation \n$\\phi(\\frac{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{t}} f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{t})}{c'_{g}L}, \\vect{x})$ to make sure the transformed input gradient is of the norm of $1$, and the separateness of at least $\\frac{\\rho}{\\sqrt{2}}$.\n\n\n% --------------\nAnalogous to the user exploitation model, regarding the convergence result for FC networks in \\blemma \\ref{lemma_convergence_user_f_1}, we proceed to present the generalization result of the user exploration model $f_{u}^{(2)}(\\cdot)$ after GD with the following lemma.\n\n\\begin{lemma} \\label{lemma_user_f_2_est_error}\nFor the constants $c'_{g} > 0$, $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given user $u\\in\\mathcal{U}$ and its past records $\\mathcal{P}_{u, t-1}$, we suppose $m, \\eta_1, J_1$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{u}^{(2)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{u}^{(2)}]_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$.\nConsider the past records $\\mathcal{P}_{u, t}$ up to round $t$ are generated by a fixed policy when witness the candidate arms $\\{\\mathcal{X}_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$.\nThen, with probability at least $1 - \\delta$ given an arm-reward pair $(\\vect{x}_{t}, r_{t})$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in \\mathcal{T}_{u, t}\\cup \\{t\\}} \\mathbb{E} \n    \\bigg[ \n    \\abs{ f_{u}^{(2)}&\\bigg( \\phi(\\frac{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{\\tau}} f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau})}{c'_{g}L}, \\vect{x}_{\\tau}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) } \n    ~\\big| \\mathcal{X}_{\\tau} \\bigg] \\\\\n    & \\leq \n    \\sqrt{T_{u, t}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3 L}{\\sqrt{2}} + (1 + 2\\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg)\n\\end{split}\n\\end{displaymath}\n\\end{lemma}\n\n\n\\textbf{Proof.}\nThe proof of this lemma is inspired by Lemma C.1 from \\citep{EE-Net_ban2021ee}. Following the same procedure as in the proof of \\blemma \\ref{lemma_user_f_1_est_error}, we bound \n\\begin{displaymath}\n\\begin{split}\n    \\bigg| f_{u}^{(2)}\\bigg( & \\phi(\\frac{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{t}} f_{u}^{(1)} (\\vect{x}_{t}; [\\matr{\\Theta}_{u}^{(1)}]_{t})}{c'_{g}L}, \\vect{x}_{t}); [\\matr{\\Theta}_{u}^{(2)}]_{t} \\bigg)  - \\bigg(r_{t} - f_{u}^{(1)} (\\vect{x}_{t}; [\\matr{\\Theta}_{u}^{(1)}]_{t}) \\bigg) \\bigg| \\\\\n     &\\qquad \\leq  \\bigg| f_{u}^{(2)}\\bigg( \\phi(\\frac{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{t}} f_{u}^{(1)} (\\vect{x}_{t}; [\\matr{\\Theta}_{u}^{(1)}]_{t})}{c'_{g}L}, \\vect{x}_{t}); [\\matr{\\Theta}_{u}^{(2)}]_{t} \\bigg) \\bigg|\n     + \\bigg| f_{u}^{(1)} (\\vect{x}_{t}; [\\matr{\\Theta}_{u}^{(1)}]_{t})  \\bigg|  + 1 \\\\\n     &\\qquad \\leq  1 + 2\\gamma_{1}\n\\end{split}\n\\end{displaymath}\nby triangle inequality and applying the generalization result of FC networks (\\blemma \\ref{lemma_user_f_1_output_bound}) on $f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)}), f_{u}^{(2)}(\\cdot; \\matr{\\Theta}_{u}^{(2)})$.\n\nFor brevity, we use $\\nabla f_{u, \\tau}^{(1)} (\\vect{x}_{t})$ to denote $\\phi(\\frac{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{\\tau}} f_{u}^{(1)} (\\vect{x}_{t}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau})}{c'_{g}L}, \\vect{x}_{t})$ for the following proof.\n%\nDefine the difference sequence as\n\\begin{displaymath}\n\\begin{split}\n    V_{\\tau}^{(2)} = \\mathbb{E}\n    \\bigg[ \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)} (\\vect{x}_{\\tau})&; [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg| \\bigg] \\\\\n    & - \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)}(\\vect{x}_{\\tau}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg|.\n\\end{split}\n\\end{displaymath}\nSince the reward mapping is fixed given the specific user $u$, which means that the past rewards and the received arm-reward pairs $(\\vect{x}_{\\tau}, r_{\\tau})$ are generated by the same reward mapping function, we have the expectation\n\\begin{displaymath}\n\\begin{split}\n    \\mathbb{E}[V_{\\tau}^{(2)} \\big| F_{u, \\tau} ] = & \\mathbb{E}\n    \\bigg[ \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)} (\\vect{x}_{\\tau}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg| \\bigg] \\\\\n    & - \\mathbb{E}\\bigg[ \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)}(\\vect{x}_{\\tau}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg| \\big| F_{u, \\tau} \\bigg] = 0.\n\\end{split}\n\\end{displaymath}\nwhere $F_{u, \\tau}$ denotes the filtration given the past records $\\mathcal{P}_{u, \\tau}$, up to round $\\tau\\in [t]$. This also gives the fact that $V_{\\tau}^{(2)}$ is a martingale difference sequence.\nThen, after applying the martingale difference sequence over $\\mathcal{T}_{u, t}$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\frac{1}{T_{u, t}} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} V_{\\tau}^{(2)} = & \\frac{1}{T_{u, t}} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} \\mathbb{E}\n    \\bigg[ \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)} (\\vect{x}_{\\tau}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg| \\bigg] \\\\\n    & - \\frac{1}{T_{u, t}} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)}(\\vect{x}_{\\tau}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg|. \n\\end{split}\n\\end{displaymath}\nAnalogous to the proof of \\blemma \\ref{lemma_user_f_1_est_error}, by applying the Azuma-Hoeffding inequality, it leads to\n\\begin{displaymath}\n\\begin{split}\n    \\mathbb{P} \\bigg[ \n    \\frac{1}{T_{u, t}} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} V_{\\tau}^{(2)} - \\frac{1}{T_{u, t}}\\sum_{\\tau \\in \\mathcal{T}_{u, t}} \\mathbb{E} [V_{\\tau}^{(2)}] \\geq (1+2\\gamma_{1}) \\sqrt{\\frac{2\\log(1 / \\delta)}{T_{u, t}}}\n    \\bigg] \\leq \\delta\n\\end{split}\n\\end{displaymath}\nSince the expectation of $V_{\\tau}^{(2)}$ is zero, with the probability at least $1-\\delta$ and an existing set of parameters $\\widetilde{\\matr{\\Theta}}_{u}^{(2)}$ s.t. $\\norm{\\widetilde{\\matr{\\Theta}}_{u}^{(2)} - [\\matr{\\Theta}_{u}^{(2)}]_{\\tau}} \\leq \\mathcal{O} \\left( \\frac{ t^3}{  n^{3}\\rho \\sqrt{m}} \\log m \\right)$, the above inequality implies\n\\begin{displaymath}\n\\begin{split}\n    & \\frac{1}{T_{u, t}} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} V_{\\tau}^{(2)} \\leq (1+2\\gamma_{1}) \\sqrt{\\frac{2\\log(1 / \\delta)}{T_{u, t}}} \\implies \\\\\n    & \\frac{1}{T_{u, t}} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} \\mathbb{E}\n    \\bigg[ \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)} (\\vect{x}_{t}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{t} - f_{u}^{(1)} (\\vect{x}_{t}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg| \\bigg] \\\\\n    & \\leq \\frac{1}{T_{u, t}} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)}(\\vect{x}_{\\tau}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg| + (1+2\\gamma_{1}) \\sqrt{\\frac{2\\log(1 / \\delta)}{T_{u, t}}} \\\\\n    & \\underset{(\\text{i})}{\\leq} \\frac{1}{T_{u, t}} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)}(\\vect{x}_{\\tau}); \\widetilde{\\matr{\\Theta}}_{u}^{(2)} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg| + (1+2\\gamma_{1}) \\sqrt{\\frac{2\\log(1 / \\delta)}{T_{u, t}}} \\\\\n    & \\leq \\frac{1}{\\sqrt{T_{u, t}}} \\sqrt{ \\sum_{\\tau \\in \\mathcal{T}_{u, t}} \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)}(\\vect{x}_{\\tau}); \\widetilde{\\matr{\\Theta}}_{u}^{(2)} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg|^{2} } + (1+2\\gamma_{1}) \\sqrt{\\frac{2\\log(1 / \\delta)}{T_{u, t}}} \\\\\n    & \\underset{(\\text{ii})}{\\leq} \\sqrt{\\frac{2\\xi_{1}}{T_{u, t}}} + (1+2\\gamma_{1}) \\sqrt{\\frac{2\\log(1 / \\delta)}{T_{u, t}}}.\n\\end{split}\n\\end{displaymath}\nHere, the upper bound (i) is derived by applying the conclusions of \\blemma \\ref{lemma_convergence_user_f_1} and \\blemma \\ref{lemma_FC_network_bound_sampled_parameters}, and the inequality (ii) is derived by adopting \\blemma \\ref{lemma_convergence_user_f_1} while defining the empirical loss to be $\\frac{1}{2} \\sum_{\\tau \\in \\mathcal{T}_{u, t}} \\bigg| f_{u}^{(2)}\\bigg( \\nabla f_{u, \\tau}^{(1)}(\\vect{x}_{\\tau}); \\widetilde{\\matr{\\Theta}}_{u}^{(2)} \\bigg)  - \\bigg(r_{\\tau} - f_{u}^{(1)} (\\vect{x}_{\\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) \\bigg|^{2}$.\nFinally, applying the union bound would give the aforementioned results.\n\n\\qed\n\n\n% ==================================================================\n\\begin{corollary} \\label{corollary_user_f_2_est_error_other_arms}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given user $u\\in\\mathcal{U}$ and its past records $\\mathcal{P}_{u, t-1}$, we suppose $m, \\eta_1, J_1$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{u}^{(1)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$. For an arm $\\vect{x}_{i, t} \\in \\mathcal{X}_{t}$, consider its union set with the the collection of arms $\\widetilde{\\mathcal{P}}_{u, t} \\cup \\{\\vect{x}_{i, t}, r_{i, t}\\}$ are generated by a fixed policy when witness the candidate arms $\\{\\mathcal{X}_{\\tau}\\}_{\\tau\\in \\mathcal{T}_{u, t}}$, with $\\widetilde{\\mathcal{P}}_{u, t} = \\{\\vect{x}_{i_{\\tau}, \\tau}, r_{i_{\\tau}, \\tau}\\}_{\\tau}$ being the collection of arms chosen by this policy.\nThen, with probability at least $1 - \\delta$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in \\mathcal{T}_{u, t}\\cup \\{t\\}} \\mathbb{E}  \n    \\big[& \\abs{ f_{u}^{(2)}\\bigg( \\phi(\\frac{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{\\tau}} f_{u}^{(1)} (\\vect{x}_{i_{\\tau}, \\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau})}{c'_{g}L}, \\vect{x}_{i_{\\tau}, \\tau}); [\\matr{\\Theta}_{u}^{(2)}]_{\\tau} \\bigg)  - \\bigg(r_{i_{\\tau}, \\tau} - f_{u}^{(1)} (\\vect{x}_{i_{\\tau}, \\tau}; [\\matr{\\Theta}_{u}^{(1)}]_{\\tau}) \\bigg) } ~\\big| \\mathcal{X}_{\\tau} \\big] \\\\\n    & \\leq \n    \\sqrt{T_{u, t}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}\n\\end{split}\n\\end{displaymath}\nwhere $r_{i_{\\tau}, \\tau}$ is the corresponding reward generated by the mapping function given an arm $\\vect{x}_{i_{\\tau}, \\tau}$, and \n\\begin{displaymath}\n\\begin{split}\n    \\Gamma_{t} =\n    \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{n\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(\\frac{t^{4}L}{n^{4}\\rho\\sqrt{m}}\\log(m)) + \n    \\mathcal{O}\\bigg( \\frac{t^{5}L^{2} \\log^{11/6} (m)}{n^{5}\\rho^{4/3}m^{1/6}} \\bigg).\n\\end{split}  \n\\end{displaymath}\n\\end{corollary}\nThis corollary is the direct application of Lemma \\ref{lemma_user_f_2_est_error}, and the proof is analogous to that of Corollary \\ref{corollary_user_f_1_est_error_other_arms}.\n\n\n\n% ==================================================================\n"
                },
                "subsection 12.3": {
                    "name": "Lemmas for Over-parameterized User Networks",
                    "content": " \\label{subsec_user_over_param_nets}\n\nApplying $\\mathcal{P}_{u, t-1}$ as the training data, we have the following convergence result for the user exploitation network $f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$ after GD.\n\n\\begin{lemma} [Theorem 1 from \\citep{conv_theory-allen2019convergence}] \\label{lemma_convergence_user_f_1}\nFor any $ 0 < \\xi_1 \\leq 1$, $ 0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$. Given user $u\\in\\mathcal{U}$ and its past records $\\mathcal{P}_{u, t-1}$, suppose $m, \\eta_1, J_1$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, then with probability at least $1 - \\delta$, we could have\n\\begin{enumerate}\n    \\item $\\mathcal{L}(\\matr{\\Theta}_{u}^{(1)}) \\leq \\xi_{1}$ after $J_1$ iterations of GD.\n    \\item For any $j \\in [J_1]$, $\\norm{ [\\matr{\\Theta}_{u}^{(1)}]^{j}  -  [\\matr{\\Theta}_{u}^{(1)}]^{0}  } \\leq \\mathcal{O} \\left( \\frac{ (T_{u, t})^3}{ \\rho \\sqrt{m}} \\log m \\right)\n    = \\mathcal{O} \\left( \\frac{ t^3}{  n^{3}\\rho \\sqrt{m}} \\log m \\right)$.\n\\end{enumerate}\n\\end{lemma}\nIn particular, \\blemma \\ref{lemma_convergence_user_f_1} above provides the convergence guarantee for $f_{u}^{(1)}(\\cdot; \\matr{\\Theta}_{u}^{(1)})$ after certain rounds of GD training on the past records $\\mathcal{P}_{u, t-1}$.\n\n\n% -------------------\n\\begin{lemma} [Lemma 4.1 in \\citep{generalization_bound_cao2019generalization}] \\label{lemma_FC_network_inner_product}\nAssume a constant $\\omega$ such that $ \\mathcal{O}( m^{-3/2} L^{-3/2} [ \\log (TnL^2/\\delta) ]^{3/2}  )   \\leq \\omega \\leq  \\mathcal{O} (L^{-6}[\\log m]^{-3/2} )$ and $n$ training samples. With randomly initialized $[\\matr{\\Theta}_{u}^{(1)}]_{0}$, for parameters $\\matr{\\Theta}, \\matr{\\Theta}'$ satisfying $\\norm{\\matr{\\Theta} - [\\matr{\\Theta}_{u}^{(1)}]_{0}}, \\norm{\\matr{\\Theta} - [\\matr{\\Theta}_{u}^{(1)}]_{0}} \\leq \\omega$, we have\n\\begin{displaymath}\n    \\abs{f_{u}^{(1)} (\\vect{x}; \\matr{\\Theta}) -  f_{u}^{(1)} (\\vect{x}; \\matr{\\Theta}') - \\inp{\\nabla_{\\matr{\\Theta}'} f_{u}^{(1)} (\\vect{x}; \\matr{\\Theta}')}{\\matr{\\Theta} - \\matr{\\Theta}'}} \\leq \n    \\mathcal{O}(\\omega^{1/3}L^{2}\\sqrt{m\\log(m)}) \\norm{\\matr{\\Theta} - \\matr{\\Theta}'}\n\\end{displaymath}\nwith the probability at least $1-\\delta$.\n\\end{lemma}\n\n% -------------------\n\\begin{lemma} \\label{lemma_FC_network_output_and_gradient}\nAssume $m, \\eta_1, J_1$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound} and $[\\matr{\\Theta}_{u}^{(1)}]_{0}$ being randomly initialized.\nThen, with probability at least $1 - \\delta$ and given an arm $\\norm{\\vect{x}}_{2}=1$, we have\n\\begin{enumerate}\n    \\item $\\abs{ f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{0}) } \\leq 2$,\n    \\item $\\norm{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{0}} f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{0})}_{2} \\leq \\mathcal{O}(L) $.\n\\end{enumerate}\n\\end{lemma}\n\n\n\\textbf{Proof.}\nThe conclusion (1) is a direct application of Lemma 7.1 in \\citep{conv_theory-allen2019convergence}. For conclusion (2), applying Lemma 7.3 in \\citep{conv_theory-allen2019convergence}, for each layer $\\matr{\\Theta}_{l} \\in \\{\\matr{\\Theta}_{1}, \\dots, \\matr{\\Theta}_{L}\\}$, we have \n\\begin{displaymath}\n    \\norm{\\nabla_{\\matr{\\Theta}_{l}} f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{0})}_{2} = \\norm{ (\\matr{\\Theta}_{L} \\matr{D}_{L-1} \\cdots \\matr{D}_{l+1}\\matr{\\Theta}_{l+1})\\cdot (\\matr{D}_{l+1} \\matr{\\Theta}_{l+1} \\cdots \\matr{D}_{1}\\matr{\\Theta}_{1}) \\cdot \\vect{x}^{\\intercal} }_{2} = \\mathcal{O}(\\sqrt{L}).\n\\end{displaymath}\nThen, we could have the conclusion that \n\\begin{displaymath}\n    \\norm{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{0}} f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{0})}_{2} = \\sqrt{\\sum_{l\\in[L]} \\norm{\\nabla_{\\matr{\\Theta}_{l}} f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{0})}_{2}^\n    {2} } = \\mathcal{O}(L).\n\\end{displaymath}\n\n\\qed\n\n\n% -------------------\n\\begin{lemma} [Theorem 5 in \\citep{conv_theory-allen2019convergence}] \\label{lemma_FC_network_gradient_difference}\nAssume $m, \\eta_1, J_1$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound} and $[\\matr{\\Theta}_{u}^{(1)}]_{0}$ being randomly initialized.\nThen, with probability at least $1 - \\delta$, and for all parameter $\\matr{\\Theta}_{u}^{(1)}$ such that $\\norm{\\matr{\\Theta}_{u}^{(1)} - [\\matr{\\Theta}_{u}^{(1)}]_{0}}_{2} \\leq \\omega $, we have\n\\begin{displaymath}\n    \\norm{ \\nabla_{\\matr{\\Theta}_{u}^{(1)}} f_{u}^{(1)} (\\vect{x}; \\matr{\\Theta}_{u}^{(1)}) - \\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{0}} f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{0}) }_{2} \\leq\n    \\mathcal{O}(\\omega^{1/3}L^{3}\\sqrt{\\log(m)})\n\\end{displaymath}\n\\end{lemma}\n\n\n\n% -------------------\n\\begin{lemma} \\label{lemma_FC_network_bound_sampled_parameters}\n\nAssume $m, \\eta_{1}$ satisfy the condition in \\btheorem \\ref{theorem_regret_bound}. \nWith the probability at least $1-\\delta$, we have\n\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in \\mathcal{T}_{u, t}} & \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{\\tau}) - r_\\tau} \\leq\n    \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{u}^{(1)}]_{t}) - r_\\tau} + \n    \\frac{3L\\sqrt{2T_{u, t}}}{2} \n\\end{split}  \n\\end{displaymath}\n\n\\end{lemma}\n\n\\textbf{Proof.}\nWith the notation from Lemma 4.3 in \\citep{generalization_bound_cao2019generalization}, set $R=\\frac{T_{u, t}^{3}\\log(m)}{\\delta}$, $\\nu=R^{2}$, and $\\epsilon=\\frac{LR}{\\sqrt{2\\nu T_{u, t}}}$. Then, \nconsidering the loss function to be $\\mathcal{L}(\\matr{\\Theta}_{u}^{(1)}) := \\sum_{\\tau\\in \\mathcal{T}_{u, t}} \\abs{ f_{u}^{(1)}(\\vect{x}_{\\tau}; \\matr{\\Theta}_{u}^{(1)}) - r_\\tau}$ would complete the proof.\n\\qed\n"
                }
            },
            "section 13": {
                "name": "Proof of the Regret Bound",
                "content": "     \\label{sec_appd_analysis_GNN}\n\nIn this section, we present the generalization results of GNN models $f_{gnn}^{(1)}(\\cdot; \\matr{\\Theta}_{gnn}^{(1)}), f_{gnn}^{(2)}(\\cdot; \\matr{\\Theta}_{gnn}^{(2)})$. \nRecall that up to round $t$, we have all the past arm-reward pairs $\\mathcal{P}_{t} = \\{(\\vect{x}_{\\tau}, r_{\\tau})\\}_{\\tau\\in [t-1]}$ for the previous $t-1$ time steps. \nAnalogous to the generalization analysis of user models in Section \\ref{sec_appd_analysis_user}, we adopt the the operation in \\textbf{Eq.} \\ref{eq_concat_transformation} on the gradients $\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\cdot; \\matr{\\Theta}_{gnn}^{(1)})$ to comply with the assumptions of unit-length and separateness, and the transformed gradient input is denoted as $\\nabla f^{(1)} (\\vect{x})$ given the arm $\\vect{x}$.\n\n% --------------------\n\n",
                "subsection 13.1": {
                    "name": "Bounding the Parameter Estimation Error",
                    "content": "    \\label{subsec_bounding_I_1}\n\nRegarding \\textbf{Eq.}\\ref{eq_single_CB}, given an arbitrary candidate arm $\\vect{x} \\in \\mathcal{X}_{t}$ with its reward $r$, and its user graphs $\\mathcal{G}^{(1)}, \\mathcal{G}^{(2)}$, we have the bound for the estimation error as\n\\begin{displaymath}\n\\begin{split}\n    & \\mathsf{CB}_{t}(\\vect{x})  = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - (r_{t} -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n    & \\leq \\underbrace{\\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - (r_{t} -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg]}_{I_{1}} \\\\\n    & \\quad + I_{2} + I_{3} + I_{4}\n\\end{split}\n\\end{displaymath}\nwhere we have the term $I_{1}$ representing the estimation error induced by the GNN model parameters $\\{[\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}, [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}\\}$.  \nBased on our arm selected strategy given in \\textbf{Algorithm} \\ref{algo_main}, we have the selected arms and their rewards $\\{\\vect{x}_{\\tau}, r_{\\tau}\\}_{\\tau\\in [t-1]}$ up to round $t$. And we first proceed to bound term $I_{1}$ w.r.t. the selected arm $\\vect{x}_{t}$, i.e., $\\textsc{CB}_{t}(\\vect{x}_{t})$.\n\nAnalogous to the user-specific models, we also have bounded outputs for the GNN models shown in the following lemma.\n\n\\begin{lemma} \\label{lemma_GNN_f_1_output_bound}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{2} \\in (0, 1)$, the past records $\\mathcal{P}_{t}$ up to time step $t$, we suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}.\nThen, with probability at least $1 - \\delta$ and given an arm-reward pair $(\\vect{x}, r)$, we have\n\\begin{displaymath}\n    \\abs{f_{gnn}^{(1)} (\\vect{x}; [\\widehat{\\matr{\\Theta}}_{gnn}^{(1)}]_{t})} \\leq \\gamma_{2}\n\\end{displaymath}\nwhere \n\\begin{displaymath}\n    \\gamma_{2} = \n    2 + \\mathcal{O} \\left( \\frac{ t^3 L}{ \\rho\\sqrt{m}} \\log m \\right)\n    + \\mathcal{O} \\left(  \\frac{ L^{2}t^4 }{ \\rho^{4 / 3} m^{1 / 6}} \\log^{11 / 6} (m) \\right).\n\\end{displaymath}\n\\end{lemma}\n\n\\textbf{Proof.}\nThe proof of this lemma follows an analogous approach as in \\blemma \\ref{lemma_user_f_1_output_bound} where we have proved the conclusion for the FC networks. \n%\nGiven an arm $\\vect{x}$, we denote the adjacency matrix of its estimated user graph $\\mathcal{G}^{(1)}$ as $\\matr{A}^{(1)}$, and we have the normalized adjacency matrices as $\\matr{S}^{(1)} = \\matr{A}^{(1)} / n$.\nFor the received user $u_{t}\\in \\mathcal{U}$, we could deem the corresponding row of the matrix multiplication $\\matr{S} \\cdot \\matr{X}$, represented by $\\vect{h}_{u_{t}} = [\\matr{S} \\cdot \\matr{X}]_{i:}$, as the aggregated input for the network for the user-arm pair $(\\vect{x}, u_{t})$. Note that in this way, the rest of the network could be regarded as a $L+1$-layer FC network (one layer GNN + $L$-layer FC network), where the weight matrix of the first layer is $\\matr{\\Theta}_{agg}^{(1)}$. \nThen, to make sure each aggregated input has the norm of 1, we apply an additional transformation mentioned in \\textbf{Eq.} \\ref{eq_concat_transformation}\nas $\\Tilde{\\vect{h}}_{u_{t}} = \\phi(\\vect{h}_{u_{t}}, \\vect{x}) = \n    (\\frac{\\vect{h}_{u_{t}}}{\\sqrt{2}}, \\frac{\\vect{x}}{2}, c_{u_{t}})$ where $c_{u_{t}} = \\sqrt{\\frac{3}{4} - \\frac{1}{2}\\norm{\\vect{h}_{u_{t}}}_{2}^{2}}$. \nThis transformation ensures $\\norm{\\Tilde{\\vect{h}}_{u_{t}}}_{2} = 1$ while preserving the original information w.r.t. the user-arm pair $(\\vect{x}, u_{t})$, as it does not change the original aggregated hidden representation. Meantime, this transformation also ensures the separateness of the transformed contexts to be at least $\\frac{\\rho}{2}$, which would fit the original data separateness assumption (\\textbf{Assumption} \\ref{assumption_separateness}). Finally, following a similar approach as in the FC networks (\\blemma \\ref{lemma_user_f_1_output_bound}), on the transformed aggregated hidden representations would complete the proof. \n\n\\qed\n\n% ----------------------------------------------------------\n\nRegarding the definition for the true reward mapping function in Section \\ref{sec_problem_def_notation}, we have the following lemma for term $I_{1}$ given the arm-reward pair $(\\vect{x}_{t}, r_{t})$.\n\n\\begin{lemma} \\label{lemma_term_I_1_selected_arm}\n\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{2} \\in (0, 1)$, given user $u\\in\\mathcal{U}$ and its past records $\\mathcal{P}_{u, t}$, we suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameters $[\\matr{\\Theta}_{gnn}^{(1)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{gnn}^{(1)}]_{\\tau}\\}_{\\tau\\in [t]}$, $[\\matr{\\Theta}_{gnn}^{(2)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{gnn}^{(2)}]_{\\tau}\\}_{\\tau\\in [t]}$.\nThen, with probability at least $1 - \\delta$ given a sampled arm-reward pair $(\\vect{x}, r)$, we have\n\\begin{displaymath}\n\\begin{split}\n    & \\sum_{\\tau\\in [t]} \\mathbb{E} \n    \\bigg[ \\bigg| f_{gnn}^{(2)}\\bigg(\\nabla f_{t}^{(1), *} (\\vect{x}_{t}),~\\mathcal{G}_{t}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}\\bigg) - \\bigg(r_{t} -  f_{gnn}^{(1)}(\\vect{x}_{t},~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) \\bigg) \\bigg|  | u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n    & \\quad \\leq \n    \\sqrt{t} \\cdot \\bigg( \\sqrt{2\\xi_{2}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{2}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg)\n\\end{split}\n\\end{displaymath}\nwhere \n\\begin{displaymath}\n    \\gamma_{2} = \n    2 + \\mathcal{O} \\left( \\frac{ t^3 L}{  \\rho\\sqrt{m}} \\log m \\right)\n    + \\mathcal{O} \\left(  \\frac{ L^{2}t^4 }{ \\rho^{4 / 3} m^{1 / 6}} \\log^{11 / 6} (m) \\right).\n\\end{displaymath}\n\n\\end{lemma}\n\n\\textbf{Proof.}\nBased on the conclusion of \\blemma\\ref{lemma_GNN_f_1_output_bound}, we have the upper bound as\n\\begin{displaymath}\n\\begin{split}\n    & \\bigg| f_{gnn}^{(2)}\\bigg(\\nabla f_{t}^{(1), *} (\\vect{x}_{t}),~\\mathcal{G}_{t}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1} \\bigg) - \\bigg( r_{t} -  f_{gnn}^{(1)}(\\vect{x}_{t},~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) \\bigg) \\bigg| \\leq 1+2\\gamma_{2}\n\\end{split}\n\\end{displaymath}\nby simply using the triangular inequality. Then we proceed to define the sequence $V_{\\tau}, \\tau \\in [t]$ as\n\\begin{displaymath}\n\\begin{split}\n    V_{\\tau} = & \\mathbb{E}_{ \\mathcal{X}_{\\tau}}\n    \\bigg[ \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\bigg] \\\\\n    & - \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg|.\n\\end{split}\n\\end{displaymath}\nAnd since the candidate arms and the corresponding rewards are associated with the same reward mapping function $h(\\cdot)$, the sequence $V_{\\tau}$ is a martingale difference sequence with the expectation\n\\begin{displaymath}\n\\begin{split}\n    \\mathbb{E}[V_{\\tau} & \\big| F_{\\tau} ] = \\mathbb{E}_{\\mathcal{X}_{\\tau}}\n    \\bigg[ \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\bigg] \\\\\n    & - \\mathbb{E}_{\\mathcal{X}_{\\tau}} \\bigg[ \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\bigg] = 0.\n\\end{split}\n\\end{displaymath}\nwhere $F_{\\tau}$ denotes the filtration of all the past records $\\mathcal{P}_{\\tau}$ up to time step $\\tau$. \nThen, we will have the mean value for this sequence as\n\\begin{displaymath}\n\\begin{split}\n    & \\frac{1}{t}\\sum_{\\tau\\in [t]}V_{\\tau} = \\\\\n    & \\frac{1}{t}\\sum_{\\tau\\in [t]} \\mathbb{E}_{ \\mathcal{X}_{\\tau}}\n    \\bigg[ \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\bigg] \\\\\n    & - \n    \\frac{1}{t}\\sum_{\\tau\\in [t]} \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg|.\n\\end{split}\n\\end{displaymath}\nAs it has shown that the sequence is a martingale difference sequence, by directly applying the Azuma-Hoeffding inequality, we could bound the difference between the mean and its expectation as\n\\begin{displaymath}\n\\begin{split}\n    \\mathbb{P} \\bigg[ \n    \\frac{1}{t}\\sum_{\\tau\\in [t]}V_{\\tau} - \\frac{1}{t}\\sum_{\\tau\\in [t]} \\mathbb{E} [V_{\\tau}] \\geq (1+2\\gamma_{2}) \\sqrt{\\frac{2\\log(1 / \\delta)}{t}}\n    \\bigg] \\leq \\delta\n\\end{split}\n\\end{displaymath}\nwith the probability at least $1 - 2\\delta$. Since it has shown that the $V_{\\tau}$ is of zero expectation, we have the second term on the LHS of the inequality to be zero.\nThen, the inequality above is equivalent to \n\\begin{displaymath}\n\\begin{split}\n    & \\frac{1}{t}\\sum_{\\tau\\in [t]}V_{\\tau} \\leq (1+2\\gamma_{2}) \\sqrt{\\frac{2\\log(1 / \\delta)}{t}} \\implies \\\\\n    & \\frac{1}{t}\\sum_{\\tau\\in [t]} \\mathbb{E}_{ \\mathcal{X}_{\\tau}}\n    \\bigg[ \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau}  -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\bigg] \\\\\n    & \\leq \\frac{1}{t}\\sum_{\\tau\\in [t]} \n    \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\\\\n    & \\qquad + (1+2\\gamma_{2}) \\sqrt{\\frac{2\\log(1 / \\delta)}{t}}\n\\end{split}\n\\end{displaymath}\nwith the probability at least $1 - 2\\delta$. Then, for the RHS of the above inequality, by further applying \\blemma \\ref{lemma_convergence_GNN_f_1} and \\blemma \\ref{lemma_GNN_network_bound_sampled_parameters}, we have\n\\begin{displaymath}\n\\begin{split}\n    & \\frac{1}{t}\\sum_{\\tau\\in [t]} \n    \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\\\\n    & \\quad \\leq \n    \\frac{1}{t}\\sum_{\\tau\\in [t]} \n    \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; \\widetilde{\\matr{\\Theta}}_{agg}^{(1)}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\\\\n    & \\qquad + \\frac{3L\\sqrt{2t}}{2} \n\\end{split}\n\\end{displaymath}\nwith regard to the parameter $\\widetilde{\\matr{\\Theta}}_{agg}^{(1)}$ s.t. $\\norm{\\widetilde{\\matr{\\Theta}}_{agg}^{(1)} - [\\matr{\\Theta}_{gnn}^{(2)}]_{0}}_{2} \\leq \\mathcal{O} \\left( \\frac{ t^3}{ \\rho \\sqrt{m}} \\log m \\right)$. Therefore, by applying the conclusion from \\blemma \\ref{lemma_convergence_GNN_f_1}, we could bound the empirical loss w.r.t. $\\widetilde{\\matr{\\Theta}}_{agg}^{(1)}$ as\n\\begin{displaymath}\n\\begin{split}\n    & \\frac{1}{t}\\sum_{\\tau\\in [t]} \n    \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; \\widetilde{\\matr{\\Theta}}_{agg}^{(1)}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| \\\\\n    & \\leq \\frac{1}{\\sqrt{t}} \n    \\sqrt{\\sum_{\\tau\\in [t]} \n    \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}),~\\mathcal{G}_{\\tau}^{(2), *}; \\widetilde{\\matr{\\Theta}}_{agg}^{(1)}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})) \\bigg| ^{2}} \\\\\n    & \\leq \\sqrt{\\frac{2\\xi_{2}}{t}}.\n\\end{split}\n\\end{displaymath}\nFinally, assembling all the components and applying the union bound would complete the proof.\n\n\\qed\n\n\n% ------------------------------------------------------------------------------------------------------------------------\n% ------------------------------------------------------------------------------------------------------------------------\n\nAnalogous to the \\blemma \\ref{lemma_GNN_f_1_output_bound}, we could also have the following corollary of the generalization results for the optimal arms and their rewards $\\{\\vect{x}_{\\tau}^{*}, r_{\\tau}^{*}\\}_{\\tau\\in [t]}$ up to round $t$. Then, let $[\\widehat{\\matr{\\Theta}}_{gnn}^{(1), *}]_{t}$ be the parameter that is trained on $\\{\\vect{x}_{\\tau}^{*}, r_{\\tau}^{*}\\}_{\\tau\\in [t]}$, and denote $[\\widehat{\\matr{\\Theta}}_{gnn}^{(2), *}]_{t}$ as the parameter of $f_{gnn}^{(2)}(\\cdot)$ trained on corresponding gradients and residuals.\n\n\\begin{corollary} \\label{corollary_term_I_1_optimal_arm}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{2} \\in (0, 1)$, given user $u\\in\\mathcal{U}$ and its past records $\\mathcal{P}_{u, t}$, we suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{gnn}^{(1), *}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{gnn}^{(1), *}]_{\\tau}\\}_{\\tau\\in [t]}$, $[\\matr{\\Theta}_{gnn}^{(2), *}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{gnn}^{(2), *}]_{\\tau}\\}_{\\tau\\in [t]}$.\nThen, with probability at least $1 - \\delta$ given a sampled arm-reward pair $(\\vect{x}, r)$, we have\n\\begin{displaymath}\n\\begin{split}\n    & \\sum_{\\tau\\in [t]} \\mathbb{E} \n    \\bigg[ \\bigg| f_{gnn}^{(2)}\\bigg(\\nabla f_{t}^{(1), *} (\\vect{x}_{t}^{*}),~\\mathcal{G}_{t, *}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}\\bigg) - \\bigg(r_{t}^{*} -  f_{gnn}^{(1)}(\\vect{x}_{t},~\\mathcal{G}_{t, *}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) \\bigg) \\bigg|  | u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n    & \\quad \\leq \n    \\sqrt{t} \\cdot \\bigg( \\sqrt{2\\xi_{2}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{2}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}\n\\end{split}\n\\end{displaymath}\nwhere \n\\begin{displaymath}\n    \\gamma_{2} = \n    2 + \\mathcal{O} \\left( \\frac{ t^3 L}{  \\rho\\sqrt{m}} \\log m \\right)\n    + \\mathcal{O} \\left(  \\frac{ L^{2}t^4 }{ \\rho^{4 / 3} m^{1 / 6}} \\log^{11 / 6} (m) \\right).\n\\end{displaymath}\n\n\\end{corollary}\n\n\\textbf{Proof.}\nThe proof of this corollary is comparable to the proof of \\blemma \\ref{lemma_term_I_1_selected_arm}. At each time step $t$, regarding the definition of the optimal arm, we have $\\vect{x}_{t}^{*} = \\max_{\\vect{x}_{i, t}\\in \\mathcal{X}_{t}} \\mathbb{E}[r_{i, t} | u_{t}, \\vect{x}_{i, t}]$. Then, analogously, we could define the difference sequence as\n\\begin{displaymath}\n\\begin{split}\n    V_{\\tau}^{*} = & \\mathbb{E}_{ \\mathcal{X}_{\\tau}}\n    \\bigg[ \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}^{*}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2), *}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau}^{*},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1), *}]_{\\tau-1})) \\bigg| \\bigg] \\\\\n    & - \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}^{*}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2), *}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau}^{*},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1), *}]_{\\tau-1})) \\bigg|\n\\end{split}\n\\end{displaymath}\nwhere by reusing the notation, we denote $\\mathcal{G}_{\\tau}^{(1), *}, \\mathcal{G}_{\\tau}^{(2), *}$ to be the true user graphs w.r.t. the optimal arm $\\vect{x}_{\\tau}^{*}$ here. Then, similar to the proof of \\blemma \\ref{lemma_term_I_1_selected_arm}, we have the sequence to be the martingale difference sequence as \n\\begin{displaymath}\n\\begin{split}\n    \\mathbb{E}[V_{\\tau}^{*} & \\big| F_{\\tau}^{*} ] = \n    \\mathbb{E}_{ \\mathcal{X}_{\\tau}}\n    \\bigg[ \\bigg| f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}^{*}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2), *}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau}^{*},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1), *}]_{\\tau-1})) \\bigg| \\bigg] \\\\\n    & - \n    \\mathbb{E}_{ \\mathcal{X}_{\\tau}}\n    \\bigg[ \\bigg| f_{gnn}^{(2), *}(\\nabla f_{\\tau}^{(1), *} (\\vect{x}_{\\tau}^{*}),~\\mathcal{G}_{\\tau}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2), *}]_{\\tau-1}) - (r_{\\tau} -  f_{gnn}^{(1)}(\\vect{x}_{\\tau}^{*},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1), *}]_{\\tau-1})) \\bigg| \\bigg] = 0\n\\end{split}\n\\end{displaymath}\nwith $F_{\\tau}^{*}$ being the filtration of past optimal arms up to round $\\tau$. Then, we could also applying the Azuma-Hoeffding inequality to bound the difference between the mean $\\frac{1}{t} \\sum_{\\tau\\in [t]} V_{\\tau}^{*}$ and its expectation $\\frac{1}{t} \\sum_{\\tau\\in [t]} \\mathbb{E}[V_{\\tau}^{*}]$. Finally, like in the proof of \\blemma \\ref{lemma_term_I_1_selected_arm}, applying the conclusion from \\blemma \\ref{lemma_convergence_GNN_f_1} and \\blemma \\ref{lemma_GNN_network_bound_sampled_parameters} would complete the proof.\n\n\\qed\n\n\n% ---------------------------------------- \nThen, recall the definition of of the confidence bound function $\\textsf{CB}_{t}(\\vect{x}_{t}^{*})$ w.r.t. the optimal arm $\\vect{x}_{t}^{*}$, we \nthe corresponding term $I_{1}$ as\n\\begin{displaymath}\n\\begin{split}\n    I_{1} = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}_{t}^{*}),~\\mathcal{G}_{t}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - (r_{t} -  f_{gnn}^{(1)}(\\vect{x}_{t}^{*},~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg].\n\\end{split}\n\\end{displaymath}\nAnd it can be further decomposed as\n\\begin{displaymath}\n\\begin{split}\n    & \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}_{t}^{*}),~\\mathcal{G}_{t}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - (r_{t} -  f_{gnn}^{(1)}(\\vect{x}_{t}^{*},~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))} \\\\\n    & \\leq \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}_{t}^{*}),~\\mathcal{G}_{t}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2), *}]_{t-1}) - (r_{t} -  f_{gnn}^{(1)}(\\vect{x}_{t}^{*},~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1), *}]_{t-1}))} + \\\\\n    & \\qquad + \n    \\abs{f_{gnn}^{(1)}(\\vect{x}_{t}^{*},~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1), *}]_{t-1}) - f_{gnn}^{(1)}(\\vect{x}_{t}^{*},~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})} \\\\\n    & \\qquad +\n    \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}_{t}^{*}),~\\mathcal{G}_{t}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2), *}]_{t-1}) - f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}_{t}^{*}),~\\mathcal{G}_{t}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})} \n\\end{split}\n\\end{displaymath}\nwhere the first term on the RHS could be bounded by \\textbf{Corollary} \\ref{corollary_term_I_1_optimal_arm}. Then, for the second term, we first denote $\\vect{h}_{i}^{*} \\in \\mathbb{R}^{m}$ to be the aggregated hidden representation w.r.t. the user-arm pair $(u_{i}, \\vect{x}_{t}^{*})$ where $u_{i}$ is the $i$-th user. Here, $\\vect{h}_{t}^{*}$ is essentially the row in the aggregated representation matrix $\\matr{H}_{agg}$ corresponding to the user arm pair $(u_{t}, \\vect{x}_{t}^{*})$. Therefore, for the received user $u_{t}\\in \\mathcal{U}$, the reward estimation based on two samples regarding the two sets of parameters would have the same the input $\\vect{h}_{t}^{*}$.\nThen, for the second term, since the outputs w.r.t. two sets of parameters have the same input $\\vect{h}_{t}^{*}$, we could apply the conclusion from \\blemma \\ref{lemma_FC_network_different_parameters}, which will lead to\n\\begin{displaymath}\n\\begin{split}\n    \\abs{f_{gnn}^{(1)}(\\vect{x}_{t}^{*}, & ~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1), *}]_{t-1}) - f_{gnn}^{(1)}(\\vect{x}_{t}^{*},~\\mathcal{G}_{t}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})} \\\\\n    & \\leq \n    \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(\\frac{t^{3}L}{\\rho\\sqrt{m}}\\log(m)) + \n    \\mathcal{O}\\bigg( \\frac{t^{4}L^{2} \\log^{11/6} (m)}{\\rho^{4/3}m^{1/6}} \\bigg).\n\\end{split}\n\\end{displaymath}\nAnalogously, we could also have the same bound for the third term on the RHS. Summing up the bounds for three terms on the RHS would finish deriving the upper bound for term $I_{1}$.\n\n\n\n\n% ------------------------------------------------------------------------------------------------------------------------\n% ------------------------------------------------------------------------------------------------------------------------\n\n"
                },
                "subsection 13.2": {
                    "name": "Bounding the Exploitation Graph Estimation Error",
                    "content": "   \\label{subsec_bounding_I_2}\n\nThen, we proceed to bound the error induced by the estimation of user exploitation graph, i.e., the error term $I_{2}$. Recall that the confidence bound function $\\textsf{CB}_{t}(\\vect{x})$ for the given arm $\\vect{x}\\in \\mathcal{X}_{t}$ is\n\\begin{displaymath}\n\\begin{split}\n    & \\mathsf{CB}_{t}(\\vect{x})  = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - (r -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n    & \\leq \n    \\underbrace{\\mathbb{E} \\bigg[ \n    \\abs{f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) - f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg]}_{I_{2}} + I_{1} + I_{3} + I_{4}.\n\\end{split}\n\\end{displaymath}\ngiven an arbitrary arm $\\vect{x} \\in \\mathcal{X}_{t}$. For arm $\\vect{x}$, we use the following lemma to bound the error caused by the difference between the estimated exploitation graph $\\mathcal{G}^{(1)}$ and the true exploitation graph $\\mathcal{G}^{(1), *}$ associated with arm $\\vect{x}$.\n\n\nDenoting the adjacency matrix of the estimated graph $\\mathcal{G}^{(1)}$ as $\\matr{A}^{(1)}$, and the adjacency matrix for the true user exploitation graph $\\mathcal{G}^{(1), *}$ as $\\matr{A}^{(1), *}$, we have the normalized adjacency matrices as $\\matr{S}^{(1)} = \\matr{A}^{(1)} / n$ and $\\matr{S}^{(1), *} = \\matr{A}^{(1), *} / n$.\nFor the $i$-th user $u_{i}\\in \\mathcal{U}$, we could deem the $i$-th row of the matrix multiplication $\\matr{S} \\cdot \\matr{X}$, represented by $\\vect{h}_{0, i} = [\\matr{S} \\cdot \\matr{X}]_{i:}$, as the aggregated input for the network for the user-arm pair $(\\vect{x}, u_{i})$. Note that in this way, the rest of the network could be regarded as a $L+1$-layer FC network, where the weight matrix for the first layer is $\\matr{\\Theta}_{agg}^{(1)}$. \nThen, to make sure each aggregated input has the norm of 1, we apply an additional transformation mentioned in \\textbf{Eq.} \\ref{eq_concat_transformation}\nas $\\Tilde{\\vect{h}}_{0, i} = \\phi(\\vect{h}_{0, i}, \\vect{x}) = \n    (\\frac{\\vect{h}_{0, i}}{\\sqrt{2}}, \\frac{\\vect{x}}{2}, c_{0, i})$ where $c_{0, i} = \\sqrt{\\frac{3}{4} - \\frac{1}{2}\\norm{\\vect{h}_{0, i}}_{2}^{2}}$. \nAnd this transformation ensures $\\norm{\\Tilde{\\vect{h}}_{0, i}}_{2} = 1$ and $c_{0, i} \\geq \\frac{1}{2}$.\nSince this transformation does not alter the original aggregated representation $\\vect{h}_{0, i}$, it will not impair the original information w.r.t. the user-arm pair $(\\vect{x}, u_{i})$. Meantime, note that this transformation also ensures the separateness of the transformed contexts to be at least $\\frac{\\rho}{2}$.\n\n\n\n\n% --------------------------------\n\\begin{lemma}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given past records $\\mathcal{P}_{t-1}$, we suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{gnn}^{(1)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{gnn}^{(1)}]_{\\tau}\\}_{\\tau\\in [t]}$.\nThen, with probability at least $1 - \\delta$, given an arm $\\vect{x}\\in \\mathbb{R}^{d}$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in [t]} \\abs{f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1}) & - f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})} \\\\\n    & \\leq \\mathcal{O}(L) \\cdot \\sqrt{8t} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}.\n\\end{split}\n\\end{displaymath}\n\n\\label{lemma_I_2_bound}\n\\end{lemma}\n\n\n\\textbf{Proof.}\nBy the conclusion of \\blemma \\ref{lemma_user_f_1_est_error}, at time step $t$, the reward estimation error of the user exploitation model could be bounded as\n\\begin{displaymath}\n    \\sum_{\\tau\\in [t]}\\mathbb{E} \\big[ \\abs{f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{t}) - r} ~\\big| \\mathcal{X}_{t} \\big] \\leq \n    \\sqrt{\\frac{t}{n}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}.\n\\end{displaymath}\nwith the probability at least $1 - \\delta$.\nAnd given two users $u_{i}, u_{j} \\in \\mathcal{U}$ and an arbitrary arm $\\vect{x} \\in \\mathcal{X}_{t}$, we denote their individual reward as $r_{i}, r_{j}$ separately. We omit the expectation notation below for simplicity.\nThen, we could bound the absolute difference between the reward estimations as\n\\begin{displaymath}\n\\begin{split}\n     \\abs{ \\abs{r_{i} - r_{j}} & -\n    \\abs{f_{u}^{(1)} (\\vect{x}_{i}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) - f_{u}^{(1)} (\\vect{x}_{j}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})} } \n    \\leq \\abs{ (r_{i} - f_{u}^{(1)} (\\vect{x}_{i}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t})) - (r_{j} - f_{u}^{(1)} (\\vect{x}_{j}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})) } \\\\\n    & = \\abs{ (r_{i} - f_{u}^{(1)} (\\vect{x}_{i}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t})) - (r_{j} - f_{u}^{(1)} (\\vect{x}_{j}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})) } \\\\\n    & \\leq \\abs{ (r_{i} - f_{u}^{(1)} (\\vect{x}_{i}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t})) } + \n    \\abs{ (r_{j} - f_{u}^{(1)} (\\vect{x}_{j}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})) }.\n\\end{split}\n\\end{displaymath}\nBased on the definition of the mapping function $\\Psi_{1}$, it would naturally be Lipschitz continuous with the coefficient of 1, which is \n\\begin{displaymath}\n\\begin{split}\n     \\abs{ \\exp(-\\abs{r_{i} - r_{j}}) -\n    \\exp(-\\abs{f_{u}^{(1)} (\\vect{x}_{i}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) & - f_{u}^{(1)} (\\vect{x}_{j}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})}) } \\\\\n    &\\leq \\abs{ \\abs{r_{i} - r_{j}} -\n    \\abs{f_{u}^{(1)} (\\vect{x}_{i}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) - f_{u}^{(1)} (\\vect{x}_{j}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})} }.\n\\end{split}\n\\end{displaymath}\n\nwith the probability at least $1 - \\delta$. Finally, applying the union bound for all the $(n^{2} - n) / 2$ user pairs and re-scaling the $\\delta$ would give us the estimation error bound for the reward difference for each pair of users. \nTo achieve the upper bound, we apply the Corollary \\ref{corollary_user_f_1_est_error_other_arms} by considering the trajectory $\\tilde{\\mathcal{P}}_{u, t}$ consists of the past arm-reward pairs $\\{\\vect{x}_{i_{\\tau}, \\tau}, r_{i_{\\tau}, \\tau}\\}_{\\tau\\in [t]}$, where arm $\\vect{x}_{i_{\\tau}, \\tau}$ leads to the largest estimation error of the estimation model $f_{u_{\\tau}}^{(1)}(\\cdot)$ in each round $\\tau\\in [t]$.  \nThus, we have the bound for the edge weight difference, where the difference of an arbitrary $i$-th row could be bounded by\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in [t]}\\norm{ [\\matr{A}_{\\tau}^{(1)}]_{i:} - [\\matr{A}_{\\tau}^{(1), *}]_{i:} }_{2} \n    \\leq 2n \\sqrt{t} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t},\n\\end{split}\n\\end{displaymath}\nwhich implies\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in [t]}\\norm{ [\\matr{S}_{\\tau}^{(1)}]_{i:} - [\\matr{S}_{\\tau}^{(1), *}]_{i:} }_{2} \n    \\leq 2 \\sqrt{t} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}.\n\\end{split}\n\\end{displaymath}\nTherefore, applying the conclusions from \\blemma \\ref{lemma_user_f_1_est_error}, it leads to\n% \\begin{displaymath}\n% \\begin{split}\n%     & \\abs{r_{i} - r_{j}} = \n%     \\abs{r_{i} - f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) \n%     + f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) - \n%     f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})\n%     + f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t}) - r_{j}} \\\\\n%     & \\quad \\leq \\abs{r_{i} - f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) }\n%     + \\abs{ f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) - \n%     f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t}) }\n%     + \\abs{ f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t}) - r_{j} } \\\\\n%     & \\quad \\leq \n%     2\\sqrt{\\frac{1}{t}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \n%     \\abs{ f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) - \n%     f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t}) }.\n% \\end{split}\n% \\end{displaymath}\n% And this is equivalent to \n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in [t]} \\abs{ \\abs{r_{i, \\tau} - r_{j, \\tau}} -\n    \\abs{f_{u}^{(1)} (\\vect{x}_{i, \\tau}; &[\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}) -  f_{u}^{(1)} (\\vect{x}_{j, \\tau}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})} } \\\\ & \\leq \n    2 \\sqrt{\\frac{t}{n}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}\n\\end{split}\n\\end{displaymath}\nAfterwards, recalling the transformation at the beginning of this subsection, and given an user-arm pair $(u_{i}, \\vect{x})$ for the $i$-th user, we denote $\\vect{h} = [\\matr{S}^{(1)} \\cdot \\matr{X}]_{i:}$ and $\\vect{h}^{*} = [\\matr{S}^{(1), *} \\cdot \\matr{X}]_{i:}$. Based the aforementioned transformation in \\textbf{Eq.} \\ref{eq_concat_transformation}, their transformed form could naturally be $\\Tilde{\\vect{h}} = (\\frac{\\sqrt{2}}{2}\\vect{h}, \\frac{\\vect{x}}{2}, c)$ and $\\Tilde{\\vect{h}}^{*} = (\\frac{\\sqrt{2}}{2}\\vect{h}^{*}, \\frac{\\vect{x}}{2}, c^{*})$ with $\\norm{\\vect{x}}_{2} = 1$. Without the loss of generality, we let $c > c^{*}$.\nThen, we could have\n\n\\begin{displaymath}\n\\begin{split}\n    \\norm{ \\Tilde{\\vect{h}} - \\Tilde{\\vect{h}}^{*} }_{2} & = \n    \\sqrt{ \\norm{\\vect{h} - \\vect{h}^{*} }_{2}^{2} + (c - c^{*})^{2} }\n    \\underset{(\\text{i})}{\\leq} \\sqrt{ \\norm{\\vect{h} - \\vect{h}^{*} }_{2}^{2} + (c^{2} - (c^{*})^{2})^{2} } \\\\\n    & \\underset{(\\text{ii})}{=} \\sqrt{ \\norm{\\vect{h} - \\vect{h}^{*} }_{2}^{2} + \\frac{1}{4}(\\norm{\\vect{h}^{*}}_{2}^{2} - \\norm{\\vect{h}}_{2}^{2})^{2} } \\\\\n    & = \\sqrt{ \\norm{\\vect{h} - \\vect{h}^{*} }_{2}^{2} + \\frac{1}{4}( \\norm{\\vect{h}^{*} - \\vect{h}}_{2} \\cdot \\norm{\\vect{h}^{*} + \\vect{h}}_{2} )^{2} } \\\\\n    & \\underset{(\\text{iii})}{\\leq} \\sqrt{2} \\cdot \\norm{\\vect{h} - \\vect{h}^{*}}_{2}\n\\end{split}\n\\end{displaymath}\nHere, (i) is because $c, c^{*} \\geq \\frac{1}{2}$. (ii) is because of $c^{2} + \\frac{\\norm{\\vect{h}}_{2}^{2}}{2} = (c^{*})^{2} + \\frac{\\norm{\\vect{h}^{*}}_{2}^{2}}{2} = \\frac{3}{4}$, and (iii) is due to $\\norm{\\vect{h}}_{2}, \\norm{\\vect{h}^{*}}_{2} \\leq 1$.\n\n\n% \\begin{displaymath}\n% \\begin{split}\n%     \\norm{ \\Tilde{\\vect{h}} - \\Tilde{\\vect{h}}^{*} }_{2} & = \\sqrt{ \\norm{\\Tilde{\\vect{h}} - \\Tilde{\\vect{h}}^{*} }_{2}^{2} + (c - c^{*})^{2} }\n%     \\underset{(\\text{i})}{\\leq} \\sqrt{ \\norm{\\Tilde{\\vect{h}} - \\Tilde{\\vect{h}}^{*} }_{2}^{2} + c^{2} - (c^{*})^{2} } \\\\\n%     & \\underset{(\\text{ii})}{=} \\sqrt{ \\norm{\\Tilde{\\vect{h}} - \\Tilde{\\vect{h}}^{*} }_{2}^{2} + \\norm{\\Tilde{\\vect{h}}^{*}}_{2}^{2} - \\norm{\\Tilde{\\vect{h}}}_{2}^{2} } \\\\\n%     & \\underset{(\\text{iii})}{\\leq} \\sqrt{2\\cdot \\norm{\\vect{h} - \\vect{h}^{*}}_{2}}.\n% \\end{split}\n% \\end{displaymath}\n% Here, (i) is because $(a - b)^{2} \\leq a^{2} - b^{2}$ for any $a, b \\geq 0$ and $a \\geq b$. (ii) is because of $c^{2} + \\norm{\\Tilde{\\vect{h}}}_{2}^{2} = 1$, and (iii) is due to $\\norm{\\vect{h}}_{2}, \\norm{\\vect{h}^{*}}_{2} \\leq 1$.\n% Then, we proceed to bound $\\norm{\\vect{h} - \\vect{h}^{*}}_{2}$. Recall the definition from \\textbf{Eq.} \\ref{eq_new_embedding_matrix}, we have\n\n\nThen, we proceed to bound $\\norm{\\vect{h} - \\vect{h}^{*}}_{2}$. Recall the definition from \\textbf{Eq.} \\ref{eq_new_embedding_matrix}. Extending the above conclusion across different rounds $\\tau\\in [t]$, we will have\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in [t]}\\norm{\\vect{h}_{\\tau} - \\vect{h}_{\\tau}^{*}}_{2} \\leq \\norm{\\vect{x}_{\\tau}} \\cdot \\norm{ [\\matr{S}_{\\tau}^{(1)}]_{i:} - [\\matr{S}_{\\tau}^{(1), *}]_{i:} }_{2} \n    \\leq 2 \\sqrt{t} \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}.\n\\end{split}\n\\end{displaymath}\n% Therefore, we end up with the bound\n% \\begin{displaymath}\n% \\begin{split}\n%     \\norm{ \\Tilde{\\vect{h}} - \\Tilde{\\vect{h}}^{*} }_{2} & \\leq 2\\sqrt{2} \\cdot \\sqrt{\\frac{1}{t}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg).\n% \\end{split}\n% \\end{displaymath}\nFinally, combining the conclusion from \\blemma \\ref{lemma_FC_network_Lipschitz}, we finally have\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in [t]} \\abs{f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}_{\\tau}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1}) & - f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}_{\\tau}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{\\tau-1})} \\\\\n    & \\leq \\mathcal{O}(L) \\cdot \\sqrt{8t} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}\n\\end{split}\n\\end{displaymath}\nwhich concludes the proof.\n\n\\qed\n\n\n\n% --------------------\n\n"
                },
                "subsection 13.3": {
                    "name": "Bounding the Exploration Graph Estimation Error",
                    "content": "\nAgain, recall the definition of the confidence bound function $\\textsf{CB}_{t}(\\vect{x})$ which is \n\\begin{displaymath}\n\\begin{split}\n    & \\mathsf{CB}_{t}(\\vect{x})  = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - (r -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n    & \\leq \\underbrace{\\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})}  \\bigg| u_{t}, \\mathcal{X}_{t}\\bigg]}_{I_{3}} \\\\\n    & \\qquad + I_{1} + I_{2} + I_{4}.\n\\end{split}\n\\end{displaymath}\nAnalogous to the procedure for the user exploitation graph, we have the following lemma to bound the error induced by user exploitation graph estimation. \n\n\n\n% --------------------------------\n\\begin{lemma}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given past records $\\mathcal{P}_{t-1}$, we suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{gnn}^{(2)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{gnn}^{(2)}]_{\\tau}\\}_{\\tau\\in [t]}$.\nThen, with probability at least $1 - \\delta$, given an arm $\\vect{x}\\in \\mathbb{R}^{d}$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in [t]} \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} & (\\vect{x}),~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})  - f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})} \\\\\n    & \\leq \\mathcal{O}(\\xi_{L}) \\cdot \\sqrt{8t} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}.\n\\end{split}\n\\end{displaymath}\n\n\\label{lemma_I_3_bound}\n\\end{lemma}\n\n\\textbf{Proof.} The proof of this lemma could be derived based on a similar approach as in \\blemma \\ref{lemma_I_2_bound}. \nRecall that for the exploration GNN model $f_{gnn}^{(2)} (\\cdot)$, we have the gradients of the GNN exploitation model $\\nabla f_{u, t}^{(1)} (\\vect{x}) = \\frac{\\nabla_{[\\matr{\\Theta}_{u}^{(1)}]_{t}} f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u}^{(1)}]_{t})}{c'_{g}L}$ as the input given an arm $\\vect{x}$ and user $u\\in \\mathcal{U}$, whose norm $\\norm{\\nabla f_{u, t}^{(1)} (\\vect{x})}_{2} \\leq 1$.\n\nGiven two users $u_{i}, u_{j} \\in \\mathcal{U}$ and an arbitrary arm $\\vect{x} \\in \\mathcal{X}_{t}$, we denote their individual reward as $r_{i}, r_{j}$ separately. Then, we could bound the absolute difference between the potential gain estimations as\n\\begin{displaymath}\n\\begin{split}\n    & \\abs{ \\abs{ (r_{i} - f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t})) - (r_{j} - f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})) }  -\n    \\abs{ f_{u}^{(2)} (\\nabla f_{u_{i}, t}^{(1)} (\\vect{x}); [\\matr{\\Theta}_{u_{i}}^{(2)}]_{t}) - f_{u}^{(2)} (\\nabla f_{u_{j}, t}^{(1)} (\\vect{x}); [\\matr{\\Theta}_{u_{j}}^{(2)}]_{t}) } } \\\\\n    & \\quad \\leq \\abs{ f_{u}^{(2)} (\\nabla f_{u_{i}, t}^{(1)} (\\vect{x}); [\\matr{\\Theta}_{u_{i}}^{(2)}]_{t})) - (r_{i} - f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t}))  } \\\\\n    & \\qquad\\qquad\\qquad\n    +  \\abs{ f_{u}^{(2)} (\\nabla f_{u_{j}, t}^{(1)} (\\vect{x}); [\\matr{\\Theta}_{u_{j}}^{(2)}]_{t}) - (r_{j} - f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})) }.\n\\end{split}\n\\end{displaymath}\nAfterwards, applying the conclusion from \\blemma \\ref{lemma_user_f_2_est_error} would lead to the result that\n\\begin{displaymath}\n\\begin{split}\n    & \\sum_{\\tau\\in [t]} \\abs{ \\abs{ (r_{i} - f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{i}}^{(1)}]_{t})) - (r_{j} - f_{u}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{u_{j}}^{(1)}]_{t})) }  -\n    \\abs{ f_{u}^{(2)} (\\nabla f_{u_{i}, t}^{(1)} (\\vect{x}); [\\matr{\\Theta}_{u_{i}}^{(2)}]_{t}) - f_{u}^{(2)} (\\nabla f_{u_{j}, t}^{(1)} (\\vect{x}); [\\matr{\\Theta}_{u_{j}}^{(2)}]_{t}) } } \\\\\n    & \\qquad \\leq 2 \\sqrt{\\frac{t}{n}} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}.\n\\end{split}\n\\end{displaymath}\nFollowing a similar approach as in the proof of \\blemma \\ref{lemma_I_2_bound}, we proceed to consider the aggregated hidden representations for the input gradients. Since the entries between $\\matr{A}^{(2)} - \\matr{A}^{(2), *}$ (and also the distance between $\\matr{S}^{(2)} - \\matr{S}^{(2), *}$) are bounded, by adopting the aforementioned transformation in \\textbf{Eq.} \\ref{eq_concat_transformation} on the aggregated hidden representations for the input gradients and the initial arm contexts $\\vect{x}$, we would end up with the bound for the difference between transformed representations for input gradients. Finally, combining the conclusion from \\blemma \\ref{lemma_FC_network_Lipschitz} would give the proof.\n\n\\qed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% --------------------\n% \\clearpage\n"
                },
                "subsection 13.4": {
                    "name": "Bounding the Gradient Input Estimation Error",
                    "content": "\n\nFor the last term $I_{4}$ in the confidence bound function $\\textsf{CB}_{t}(\\vect{x})$, we have\n\\begin{displaymath}\n\\begin{split}\n    I_{4} = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) - f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})}  \\bigg| u_{t}, \\mathcal{X}_{t}\\bigg]\n\\end{split}\n\\end{displaymath}\nwhich represents the estimation error induced by the difference of input gradients. And we first bound the gradient difference with the following lemma.\n\n\\begin{lemma}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given past records $\\mathcal{P}_{t-1}$, we suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{gnn}^{(1)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{gnn}^{(1)}]_{\\tau}\\}_{\\tau\\in [t]}$.\nThen, with probability at least $1 - \\delta$, given an arm $\\vect{x}\\in \\mathbb{R}^{d}$, we have\n\\begin{displaymath}\n\\begin{split}\n     \\sum_{\\tau\\in [t]} &\\norm{ \\nabla f_{\\tau}^{(1)} (\\vect{x})  - \\nabla f_{\\tau}^{(1), *} (\\vect{x}) }_{2} \n     \\leq \\mathcal{O} ( \\frac{tL^{7/2}  \\sqrt{\\log m}}{\\sqrt{mn}})\n\\end{split}\n\\end{displaymath}\nwhere $\\nabla f_{t}^{(1)} (\\vect{x}) = \\frac{\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})}{c_{g}L}$, and $\\nabla f_{t}^{(1), *} (\\vect{x}) = \\frac{\\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})}{c_{g}L}$.\n\n\\label{lemma_I_4_gradient_difference_bound}\n\\end{lemma}\n\n\n\\textbf{Proof.}\n% Following the aggregation procedure and transformation procedure shown in section \\ref{subsec_bounding_I_2}, we have the transformed representations for given an user-arm pair $(u_{i}, \\vect{x})$ with the $i$-th user, which are $\\vect{h} = [\\matr{S}^{(1)} \\cdot \\matr{X}]_{i:}$ and $\\vect{h}^{*} = [\\matr{S}^{(1), *} \\cdot \\matr{X}]_{i:}$. And their transformed form could naturally be $\\Tilde{\\vect{h}} = (\\vect{h}, c)$ and $\\Tilde{\\vect{h}}^{*} = (\\vect{h}^{*}, c^{*})$. From the conclusion of \\blemma \\ref{lemma_I_2_bound}, we have\n% \\begin{displaymath}\n% \\begin{split}\n%     \\sum_{\\tau\\in [t]} \\norm{ \\Tilde{\\vect{h}}_{\\tau} - \\Tilde{\\vect{h}}_{\\tau}^{*} }_{2} \\leq \\sqrt{8t} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}.\n% \\end{split}\n% \\end{displaymath}\n% Then, applying the conclusion from \\blemma \\ref{lemma_FC_network_Lipschitz} would complete the proof.\n%\nSince the ReLU activation $\\sigma(\\cdot)$ is not Lipschitz smooth, we propose to bound the gradient difference from the perspective of parameter shift. Recall that in \\textbf{Eq.} \\ref{eq_GNN_aggegation}, with $k=1$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\matr{H}_{agg} = \\sigma\\big(\\matr{S}_{i, t}^{(1)} \\cdot (\\matr{X}_{i, t} \\matr{\\Theta}_{agg}^{(1)})\\big).\n\\end{split}\n\\end{displaymath}\nIt is known that the gradient shift can be directly measured by the model parameter shift \\cite{conv_theory-allen2019convergence,generalization_bound_cao2019generalization}.\nIn this case, since the model parameters $\\matr{\\Theta}_{gnn}^{(2)}$ stay the same for both $ \\nabla f_{\\tau}^{(1)} (\\vect{x}) $ and $ \\nabla f_{\\tau}^{(1), *} (\\vect{x})$, we can naturally integrate the effects of different adjacency matrices with the model parameters. Here, given the same input arm context $\\vect{x}$, let $\\widetilde{\\matr{\\Theta}}_{agg}^{(1)}, \\widetilde{\\matr{\\Theta}}_{agg}^{(1), *}$ separately represent the integrated parameters for the aggregation layer w.r.t. $ \\nabla f_{\\tau}^{(1)} (\\vect{x}) $ and $ \\nabla f_{\\tau}^{(1), *} (\\vect{x})$.\nWith all other layers stay the same, the parameter shift can be directly measure by the difference $\\norm{\\widetilde{\\matr{\\Theta}}_{agg}^{(1)} - \\widetilde{\\matr{\\Theta}}_{agg}^{(1), *}}$.\n\nThen, we proceed to measure the parameter shift $\\norm{\\widetilde{\\matr{\\Theta}}_{agg}^{(1)} - \\widetilde{\\matr{\\Theta}}_{agg}^{(1), *}}$.\nBased on the aggregation operation, and given an user-arm pair $(u_{i}, \\vect{x})$ for the $i$-th user, we denote $\\vect{h} = [\\matr{S}^{(1)} \\cdot \\matr{X}]_{i:}$ and $\\vect{h}^{*} = [\\matr{S}^{(1), *} \\cdot \\matr{X}]_{i:}$. \nBy definition, $\\vect{h}$ refers to the aggregated embedding for the user-arm pair $(u_{i}, \\vect{x})$ with the estimated user graph, while $\\vect{h}$ refers to the aggregated embedding with the true user graph.\nAlternatively, with a new matrix $\\widetilde{S}_{i}\\in \\mathbb{R}^{d\\times nd}$ for user $u_{i}$, we can consider each aggregated embedding to be generated by $\\vect{h} = \\vect{x}^{\\intercal}\\cdot \\widetilde{S}_{i}$, where\n\\begin{displaymath}\n\\begin{split}\n    \\widetilde{S}_{i} = \n\\left(\n    \\begin{array}{cccc}\n    w^{(1)}(u_{1}, u_{i}) & \\matr{0} & \\cdots & \\matr{0} \\\\\n    \\matr{0} & w^{(1)}(u_{1}, u_{i}) & \\cdots & \\matr{0} \\\\\n    \\vdots  &       & \\ddots   & \\vdots \\\\\n    \\matr{0} & \\matr{0} & \\cdots & w^{(1)}(u_{1}, u_{i}) \\\\\n    \\end{array} \\cdots\n    \\begin{array}{cccc}\n    w^{(1)}(u_{n}, u_{i}) & \\matr{0} & \\cdots & \\matr{0} \\\\\n    \\matr{0} & w^{(1)}(u_{n}, u_{i}) & \\cdots & \\matr{0} \\\\\n    \\vdots  &       & \\ddots   & \\vdots \\\\\n    \\matr{0} & \\matr{0} & \\cdots & w^{(1)}(u_{n}, u_{i}) \\\\\n    \\end{array}\n\\right) \\in \\mathbb{R}^{d\\times nd}.\n\\end{split}\n\\end{displaymath}\nAnalogously, we can also define the matrix $\\widetilde{S}^{*}_{i}\\in \\mathbb{R}^{d\\times nd}$ for the true user graph. Therefore, the Euclidean difference $\\norm{\\vect{h} - \\vect{h}^{*}}_{2} \\leq \\norm{\\vect{x}}_{2} \\cdot \\norm{\\widetilde{S}_{i} - \\widetilde{S}^{*}_{i}}_{2} \\leq \\norm{\\widetilde{S}_{i} - \\widetilde{S}^{*}_{i}}_{2}$. Let $\\Delta \\widetilde{S}_{i} = \\widetilde{S}_{i} - \\widetilde{S}^{*}_{i}$. Obviously, we can represent the $\\widetilde{\\matr{\\Theta}}_{agg}^{(1)} = \\Delta \\widetilde{S}_{i} \\cdot\\matr{\\Theta}_{agg}^{(1)}$. By definition, it is obvious that each element of the matrix subtraction $|[\\Delta \\widetilde{S}_{i}]_{p, q}| \\leq \\frac{1}{n}, 0\\geq p \\leq n, 0\\geq q\\leq nd$.\nHere, by the fact that $\\norm{\\Delta \\widetilde{S}_{i}}_{2}^{2} = \\max_{\\vect{z}, \\norm{\\vect{z}}=1} [\\vect{z}^{\\intercal} ((\\Delta \\widetilde{S}_{i})^{\\intercal}\\Delta \\widetilde{S}_{i}) \\vect{z}]$ and the Rayleigh-Ritz theorem, we can prove $\\norm{\\Delta \\widetilde{S}_{i}}_{2} = \\norm{(\\Delta \\widetilde{S}_{i})^{\\intercal}}_{2}$. \n\nThen, let us denote $\\Delta w_{j}^{(1)} = w^{(1)}(u_{j}, u_{i}) - w^{(1), *}(u_{j}, u_{i})$ for the same given user $u_{i}$, which corresponds to the elements in $\\Delta \\widetilde{S}_{i}$.\nFor $\\norm{(\\Delta \\widetilde{S}_{i})^{\\intercal}}_{2}$, given any $\\norm{\\vect{z}}_{2} = 1$, we see that $\\norm{(\\Delta \\widetilde{S}_{i})^{\\intercal}\\cdot \\vect{z}}_{2} = \\sqrt{\\sum_{j=1}^{n} \\Delta (w_{j}^{(1)})^{2} \\norm{x}_{2}^{2}} \\leq \\frac{1}{\\sqrt{n}}$, which means that $\\norm{\\Delta \\widetilde{S}_{i}} \\leq \\frac{1}{\\sqrt{n}}$.\nRecall that we aim to measure the parameter shift $\\norm{\\widetilde{\\matr{\\Theta}}_{agg}^{(1)} - \\widetilde{\\matr{\\Theta}}_{agg}^{(1), *}}$. Since the other parameter matrices stay the same for both $ \\nabla f_{\\tau}^{(1)} (\\vect{x}) $ and $ \\nabla f_{\\tau}^{(1), *} (\\vect{x})$, the parameter shift solely comes from the $\\Delta \\widetilde{S}$, which means that $\\norm{\\widetilde{\\matr{\\Theta}}_{agg}^{(1)} - \\widetilde{\\matr{\\Theta}}_{agg}^{(1), *}} \\leq \\mathcal{O}(\\frac{1}{\\sqrt{n}})$. \nWith $n \\geq \\mathcal{O}(\\text{Poly}(L, \\log(m)))$, based on \\textbf{Lemma} \\ref{lemma_theorem5_allenzhu}, we have \n$$\n\\norm{ \\nabla f_{\\tau}^{(1)} (\\vect{x})  - \\nabla f_{\\tau}^{(1), *} (\\vect{x}) }_{2}\n%\n\\leq \\mathcal{O} ( \\frac{\\omega L^3  \\sqrt{\\log m}}{\\sqrt{m}}) \\|   \\nabla_\\Theta f(\\vect{x}; \\Theta_0)    \\|_2 \n\\leq \\mathcal{O} ( \\frac{L^{7/2}  \\sqrt{\\log m}}{\\sqrt{mn}}).\n$$\n\n\n\n\n\n\n\n\n\\qed\n\n\n\n\n\n\n\n\n\n\n% -------------------------------------------------------------\n% -------------------------------------------------------------\n% -------------------------------------------------------------\n\n\nThen, we have the following lemma to bound the term $I_{4}$.\n\n\\begin{lemma}\nFor the constants $\\rho \\in (0, \\mathcal{O}(\\frac{1}{L}))$ and $\\xi_{1} \\in (0, 1)$, given past records $\\mathcal{P}_{t-1}$, we suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, and randomly draw the parameter $[\\matr{\\Theta}_{gnn}^{(1)}]_{t} \\sim \\{[\\widehat{\\matr{\\Theta}}_{gnn}^{(1)}]_{\\tau}\\}_{\\tau\\in [t]}$.\nThen, with probability at least $1 - \\delta$, given an arm $\\vect{x}\\in \\mathbb{R}^{d}$, we have\n\\begin{displaymath}\n\\begin{split}\n     \\sum_{\\tau\\in [t]} \\abs{f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1), *} (\\vect{x})  ,~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) &- f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1})}\n      \\leq \\\\\n     & \\mathcal{O} ( \\frac{\\xi_{L}tL^{7/2}  \\sqrt{\\log m}}{\\sqrt{mn}}) +\n     \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(\\frac{t^{3}L}{\\rho\\sqrt{m}}\\log(m)) + \n    \\mathcal{O}\\bigg( \\frac{t^{4}L^{2} \\log^{11/6} (m)}{\\rho^{4/3}m^{1/6}} \\bigg).\n\\end{split}\n\\end{displaymath}\n\n\\label{lemma_I_4_overall_bound}\n\\end{lemma}\n\n\\textbf{Proof.}\n% We again follow the aggregation procedure and transformation procedure presented in section \\ref{subsec_bounding_I_2}. Then, the aggregated and transformed input gradient could be denoted as\n% we have the transformed representations for given an user-arm pair $(u_{i}, \\vect{x})$ with the $i$-th user, which are $\\vect{g} = [\\matr{S}^{(2)} \\cdot \\matr{G}]_{i:}$ and $\\vect{g}^{*} = [\\matr{S}^{(2)} \\cdot \\matr{G}^{*}]_{i:}$, where $\\matr{G}$ denotes the gradient matrix embedded w.r.t. \\textbf{Eq.} \\ref{eq_new_embedding_matrix}. And their transformed form could be $\\Tilde{\\vect{g}} = (\\frac{\\sqrt{2}}{2}\\vect{g}, c)$ and $\\Tilde{\\vect{g}}^{*} = (\\frac{\\sqrt{2}}{2}\\vect{g}^{*}, c^{*})$.\n% Then, according to the definition of \\textbf{Eq.} \\ref{eq_new_embedding_matrix}, we could naturally have\n% \\begin{displaymath}\n% \\begin{split}\n%      \\norm{ \\Tilde{\\vect{g}} - \\Tilde{\\vect{g}}^{*} }_{2} \\leq \\norm{ [\\matr{S}^{(2)}]_{i:} }_{2} \\cdot \n%      \\norm{ \\nabla f_{t}^{(1), *} (\\vect{x}) - \\nabla f_{t}^{(1)} (\\vect{x})}_{2} \\leq \\norm{ \\nabla f_{t}^{(1), *} (\\vect{x}) - \\nabla f_{t}^{(1)} (\\vect{x})}_{2}\n% \\end{split}\n% \\end{displaymath}\n% since the normalization of the adjacency matrix ensures its arbitrary row has the norm smaller than 1.\n% Finally, applying the conclusions from \\blemma \\ref{lemma_I_4_gradient_difference_bound} and \\blemma \\ref{lemma_FC_network_Lipschitz}, it will leads to \n% \\begin{displaymath}\n% \\begin{split}\n%      \\sum_{\\tau\\in [t]} \\abs{f_{gnn}^{(2)}&(\\nabla f_{\\tau}^{(1), *} (\\vect{x})  ,~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1}) - f_{gnn}^{(2)}(\\nabla f_{\\tau}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{\\tau-1})} \\\\\n%      & \\leq \n%      \\mathcal{O}(\\frac{t^{2}L^{5}\\log^{5/6}(m)}{\\rho^{1/3}m^{1/6}}) + \n%      \\mathcal{O}(L^{2})\\cdot \\sqrt{8t} \\cdot \\bigg( \\sqrt{2\\xi_{1}} + \\frac{3L}{\\sqrt{2}} + (1 + \\gamma_{1}) \\sqrt{2\\log(\\frac{tn\\cdot a}{\\delta})}\\bigg) + \\Gamma_{t}.\n% \\end{split}\n% \\end{displaymath}\n%\nThe proof can be obtained by directly applying \\textbf{Lemma} \\ref{lemma_I_4_gradient_difference_bound} and \\textbf{Lemma} \\ref{lemma_FC_network_Lipschitz}.\n\n\\qed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% ==================================================================\n% \\clearpage\n"
                },
                "subsection 13.5": {
                    "name": "Lemmas for Over-parameterized Networks",
                    "content": "\n\nBased on the results from Subsection \\ref{subsec_user_over_param_nets}, with $\\mathcal{P}_{t-1}$ as the training data, we have the following convergence result for the exploitation GNN network $f_{gnn}^{(1)}(\\cdot; \\matr{\\Theta}_{gnn}^{(1)})$ after GD.\n\n\\begin{lemma} [Lemma \\ref{lemma_convergence_user_f_1} extended] \\label{lemma_convergence_GNN_f_1}\nFor any $ 0 < \\xi_{2} \\leq 1$, $ 0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$. Given past records $\\mathcal{P}_{t-1}$, suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}, then with probability at least $1 - \\delta$, we could have\n\\begin{enumerate}\n    \\item $\\mathcal{L}(\\matr{\\Theta}_{gnn}^{(1)}) \\leq \\xi_{2}$ after $J_{2}$ iterations of GD.\n    \\item For any $j \\in [J_{2}]$, $\\norm{ [\\matr{\\Theta}_{gnn}^{(1)}]^{j}  -  [\\matr{\\Theta}_{gnn}^{(1)}]^{0}  } \\leq \\mathcal{O} \\left( \\frac{ t^3}{ \\rho \\sqrt{m}} \\log m \\right)$.\n\\end{enumerate}\n\\end{lemma}\nIn particular, \\blemma \\ref{lemma_convergence_GNN_f_1} above provides the convergence guarantee for $f_{gnn}^{(1)}(\\cdot; \\matr{\\Theta}_{gnn}^{(1)})$ after certain rounds of GD training on the past records $\\mathcal{P}_{t-1}$.\n\n\n% -------------------\n\\begin{lemma} [Lemma \\ref{lemma_FC_network_inner_product} extended] \\label{lemma_GNN_network_inner_product}\nAssume a constant $\\omega$ such that $ \\mathcal{O}( m^{-3/2} L^{-3/2} [ \\log (TnL^2/\\delta) ]^{3/2}  )   \\leq \\omega \\leq  \\mathcal{O} (L^{-6}[\\log m]^{-3/2} )$ and $n$ training samples. With randomly initialized $[\\matr{\\Theta}_{gnn}^{(1)}]_{0}$, for parameters $\\matr{\\Theta}, \\matr{\\Theta}'$ satisfying $\\norm{\\matr{\\Theta} - [\\matr{\\Theta}_{gnn}^{(1)}]_{0}}, \\norm{\\matr{\\Theta} - [\\matr{\\Theta}_{gnn}^{(1)}]_{0}} \\leq \\omega$, we have\n\\begin{displaymath}\n    \\abs{f^{(1)} (\\vect{x}; \\matr{\\Theta}) -  f^{(1)} (\\vect{x}; \\matr{\\Theta}') - \\inp{\\nabla_{\\matr{\\Theta}'} f^{(1)} (\\vect{x}; \\matr{\\Theta}')}{\\matr{\\Theta} - \\matr{\\Theta}'}} \\leq \n    \\mathcal{O}(\\omega^{1/3}L^{2}\\sqrt{m\\log(m)}) \\norm{\\matr{\\Theta} - \\matr{\\Theta}'}\n\\end{displaymath}\nwith the probability at least $1-\\delta$.\n\\end{lemma}\n\n% -------------------\n\\begin{lemma} [Lemma \\ref{lemma_FC_network_output_and_gradient} extended] \\label{lemma_GNN_network_output_and_gradient}\nAssume $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound} and $[\\matr{\\Theta}_{gnn}^{(1)}]_{0}$ being randomly initialized.\nThen, with probability at least $1 - \\delta$ and given an arm $\\norm{\\vect{x}}_{2}=1$, we have\n\\begin{enumerate}\n    \\item $\\abs{ f_{gnn}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{gnn}^{(1)}]_{0}) } \\leq 2$,\n    \\item $ \\norm{\\nabla_{[\\matr{\\Theta}_{gnn}^{(1)}]_{0}} f_{gnn}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{gnn}^{(1)}]_{0})}_{2} \\leq \\mathcal{O}(L) $.\n\\end{enumerate}\n\\end{lemma}\n\n\n\\textbf{Proof.}\nThe conclusion (1) is a direct application of Lemma 7.1 in \\citep{conv_theory-allen2019convergence}. For conclusion (2), for each weight matrix $\\matr{\\Theta}_{l} \\in \\{\\matr{\\Theta}_{0}^{(1)}, \\matr{\\Theta}_{1}^{(1)}, \\dots, \\matr{\\Theta}_{L}^{(1)}\\}$ where $\\matr{\\Theta}_{0}^{(1)} = \\matr{\\Theta}_{agg}^{(1)}$, we have\n\\begin{displaymath}\n    \\norm{\\nabla_{\\matr{\\Theta}} f_{gnn}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{gnn}^{(1)}]_{0})}_{2} = \n    \\norm{ (\\matr{\\Theta}_{L} \\matr{D}_{L-1} \\cdots \\matr{D}_{l+1}\\matr{\\Theta}_{l+1})\\cdot (\\matr{D}_{l+1} \\matr{\\Theta}_{l+1} \\cdots \\matr{D}_{1}\\matr{\\Theta}_{1} \\matr{D}_{0}\\matr{\\Theta}_{0}) \\cdot \\vect{h}^{\\intercal} }_{2} \\leq \\mathcal{O}(\\sqrt{L})\n\\end{displaymath}\nby applying Lemma 7.3 in \\citep{conv_theory-allen2019convergence}, and $\\vect{h}$ denotes the aggregated hidden representation for each user-pair, namely the corresponding row in $\\matr{H}_{agg}$. Therefore, by combining the bounds for all the weight matrices, we could have\n\\begin{displaymath}\n\\begin{split}\n    \\norm{\\nabla_{[\\matr{\\Theta}_{gnn}^{(1)}]_{0}} f_{gnn}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{gnn}^{(1)}]_{0})}_{2} = \\sqrt{\\sum_{l\\in \\{0, \\dots, L\\}} \\norm{\\nabla_{\\matr{\\Theta}} f_{gnn}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{gnn}^{(1)}]_{0})}_{2}^{2}} = \\mathcal{O}(L).\n\\end{split}\n\\end{displaymath}\nwhich finishes the proof.\n\n\\qed\n\n\n% -------------------\n\\begin{lemma} [Lemma \\ref{lemma_FC_network_gradient_difference} extended] \\label{lemma_GNN_network_gradient_difference}\nAssume the training parameters $m, \\eta_{2}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound} and $[\\matr{\\Theta}_{gnn}^{(1)}]_{0}$ being randomly initialized.\nThen, with probability at least $1 - \\delta$, and for all parameter $\\matr{\\Theta}_{gnn}^{(1)}$ such that $\\norm{\\matr{\\Theta}_{gnn}^{(1)} - [\\matr{\\Theta}_{gnn}^{(1)}]_{0}}_{2} \\leq \\omega $, we have\n\\begin{displaymath}\n    \\norm{ \\nabla_{\\matr{\\Theta}_{gnn}^{(1)}} f_{gnn}^{(1)} (\\vect{x}; \\matr{\\Theta}_{gnn}^{(1)}) - \\nabla_{[\\matr{\\Theta}_{gnn}^{(1)}]_{0}} f_{gnn}^{(1)} (\\vect{x}; [\\matr{\\Theta}_{gnn}^{(1)}]_{0}) }_{2} \\leq\n    \\mathcal{O}(\\omega^{1/3}L^{3}\\sqrt{\\log(m)})\n\\end{displaymath}\n\\end{lemma}\n\n\n\n% -------------------\n\\begin{lemma} [Lemma \\ref{lemma_FC_network_bound_sampled_parameters} extended] \\label{lemma_GNN_network_bound_sampled_parameters}\n\nAssume $m, \\eta_{2}$ satisfy the condition in \\btheorem \\ref{theorem_regret_bound}. \nWith the probability at least $1-\\delta$, we have\n\n\\begin{displaymath}\n\\begin{split}\n    \\sum_{\\tau\\in [t]} & \\abs{ f(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{gnn}^{(1)}]_{\\tau}) - r_\\tau} \\leq\n    \\sum_{\\tau\\in [t]} \\abs{ f(\\vect{x}_{\\tau}; [\\widehat{\\matr{\\Theta}}_{gnn}^{(1)}]_{t}) - r_\\tau} + \n    \\frac{3L\\sqrt{2t}}{2} \n\\end{split}  \n\\end{displaymath}\n\n\\end{lemma}\n\n\\textbf{Proof.}\nWith the notation from Lemma 4.3 in \\citep{generalization_bound_cao2019generalization}, set $R=\\frac{t^{3}\\log(m)}{\\delta}$, $\\nu=R^{2}$, and $\\epsilon=\\frac{LR}{\\sqrt{2\\nu t}}$. Then, \nconsidering the loss function to be $\\mathcal{L}(\\matr{\\Theta}_{gnn}^{(1)}) := \\sum_{\\tau\\in [t]} \\abs{ f(\\vect{x}_{\\tau}; \\matr{\\Theta}_{gnn}^{(1)}) - r_\\tau}$ would complete the proof.\n\\qed\n\n\n\n\n% % --------------------------------------\n% \\begin{lemma} \\label{lemma_GNN_network_gradient_difference_bound}\n% For any $ 0 < \\xi_{2} \\leq 1$, $ 0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$. Given past records $\\mathcal{P}_{t-1}$, suppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions \\TODO{fill in}. Then, at time step $t$, with probability at least $1 - \\delta$, we could have\n% \\begin{displaymath}\n% \\begin{split}\n%     & \\norm{\\nabla_{\\matr{\\Theta}^{(1)}} f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})}_{2}, \n%     \\quad \\norm{\\nabla_{\\matr{\\Theta}^{(2)}} f_{gnn}^{(2)}(\\nabla f_{t}^{(1)}(\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})}_{2} \\\\\n%     & \\quad \\leq \n% \\end{split}  \n% \\end{displaymath}\n% given an arm $\\vect{x}$ and its associated arm graphs $\\mathcal{G}^{(1)}, \\mathcal{G}^{(2)}$.\n\n% \\end{lemma}\n\n\n% \\textbf{Proof.}\n\n\n\n% % --------------------------------------\n% \\begin{lemma} \\label{lemma_GNN_network_Lipschitz}\n% For any $ 0 < \\xi_{2} \\leq 1$, $ 0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$. Given past records $\\mathcal{P}_{t-1}$, suppose $m, \\eta_{2}, J_{2}$ satisfy the conditions \\TODO{fill in}. Then, at time step $t$, with probability at least $1 - \\delta$, we have\n% \\begin{displaymath}\n% \\begin{split}\n%     & \\norm{ f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) - \n%     f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) }_{2}\n% \\end{split}  \n% \\end{displaymath}\n% given an arm $\\vect{x}$ and its associated arm graphs $\\mathcal{G}^{(1)}, \\mathcal{G}^{(2)}$.\n\n% \\end{lemma}\n\n\n% \\textbf{Proof.}\n\n\n\n\n\n\\begin{lemma} \\label{lemma_theorem5_allenzhu}\nSuppose $m, \\eta_1, \\eta_2$ satisfy the conditions in Theorem \\ref{theorem_regret_bound}. \nWith probability at least $1- \\mathcal{O}(TkL^2)\\exp[-\\Omega(m \\omega^{2/3} L)] $ over randomness of $\\Theta_0$, for all $t \\in [T], i \\in [k]$, $\\Theta$ satisfying $ \\| \\Theta  - \\Theta_0 \\|_2 \\leq \\omega $ with $\\omega \\leq \\mathcal{O}(\\text{Poly}(L, \\log(m)))$, it holds uniformly:\n\\[\n\\| \\nabla_\\Theta f(\\vect{x}; \\Theta) -    \\nabla_\\Theta f(\\vect{x}; \\Theta_0) \\| \\leq \\mathcal{O} ( \\frac{\\omega L^3  \\sqrt{\\log m}}{\\sqrt{m}}) \\|   \\nabla_\\Theta f(\\vect{x}; \\Theta_0)    \\|_2 \n\\]\n\\end{lemma}\n\n\\textbf{Proof.}\nThis is an application of Theorem 5 \\cite{conv_theory-allen2019convergence}. It holds uniformly that\n\\[\n\\| \\sqrt{m} \\nabla_\\Theta f(\\vect{x}; \\Theta) -    \\sqrt{m} \\nabla_\\Theta f(\\vect{x}; \\Theta_0) \\| \\leq \\mathcal{O} (\\omega L^3  \\sqrt{\\log m}) \\|   \\nabla_\\Theta f(\\vect{x}; \\Theta_0)    \\|_2 \n\\]\nwhere the scale factor $\\sqrt{m}$ is because the last layer of neural network in \\cite{conv_theory-allen2019convergence} is initialized based on $N(0 ,1)$ while our last layer is initialized from $N(0 ,1/m)$.\n\n\\qed\n\n\n% --------------------------------------\n\\begin{lemma}   \\label{lemma_FC_network_Lipschitz}\nConsider a L-layer fully-connected network $f(\\cdot; \\matr{\\Theta}_{t})$ initialized w.r.t. Subsection \\ref{subsec_GNN_arch}. For any $ 0 < \\xi_{2} \\leq 1$, $ 0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$. Given the training data set with $t$ samples satisfying the unit-length and the $\\rho$-separateness assumption, suppose the training parameters $m, \\eta_{2}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}. Then, with probability at least $1 - \\delta$, we have\n\\begin{displaymath}\n\\begin{split}\n    & \\abs{ f(\\vect{x}; \\matr{\\Theta}_{t}) - f(\\vect{x}'; \\matr{\\Theta}_{t}) } \\leq \\mathcal{O}(\\xi_{L})\\cdot \\norm{ \\vect{x} - \\vect{x}' }_{2} + \n    \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(\\frac{t^{3}L}{\\rho\\sqrt{m}}\\log(m)) + \n    \\mathcal{O}\\bigg( \\frac{t^{4}L^{2} \\log^{11/6} (m)}{\\rho^{4/3}m^{1/6}} \\bigg) \\\\\n    % & \\norm{ \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t}) - \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}'; \\matr{\\Theta}_{t}) }_{2} \\leq\n    % \\mathcal{O}(\\frac{tL^{4}\\log^{5/6}(m)}{\\rho^{1/3}m^{1/6}}) + \n    % \\mathcal{O}(L)\\cdot \\norm{ \\vect{x} - \\vect{x}' }_{2}\n\\end{split}  \n\\end{displaymath}\nwhen given two new samples $\\vect{x}, \\vect{x}'$ and $c_{\\xi} > 0$, $\\xi_{L} = (c_{\\xi})^{L}$.\n\n\\end{lemma}\n\n\n\\textbf{Proof.}\n% Denoting $\\matr{D}_{l}$ to be the diagonal sign matrix of the $l$-th layer such that $\\matr{D}_{l}[i, i] = \\mathbb{I}[(\\matr{\\Theta}_{l} \\vect{h}_{l-1})_{i} \\geq 0], i\\in [m]$, we could have\n% \\begin{displaymath}\n% \\begin{split}\n%      \\abs{ f(\\vect{x}; \\matr{\\Theta}_{t}) - f(\\vect{x}'; \\matr{\\Theta}_{t}) } & =\n%     \\abs{(\\matr{\\Theta}_{L} \\matr{D}_{L-1} \\cdots \\matr{D}_{1}\\matr{\\Theta}_{1} ) \\cdot (\\vect{x} - \\vect{x}')^{\\intercal}} \\\\\n%     & \\leq \\norm{ \\matr{\\Theta}_{L} \\matr{D}_{L-1} \\cdots \\matr{D}_{1}\\matr{\\Theta}_{1} }_{2} \\cdot \\norm{\\vect{x} - \\vect{x}'}_{2}.\n% \\end{split}  \n% \\end{displaymath}\n% Based on Lemma 7.3 from \\citep{conv_theory-allen2019convergence} and Lemma C.4 from \\citep{EE-Net_ban2021ee}, we have we have $\\norm{ \\matr{\\Theta}_{L} \\matr{D}_{L-1} \\cdots \\matr{D}_{1}\\matr{\\Theta}_{1} }_{2} = \\mathcal{O}(L)$ for the initialized parameters $\\matr{\\Theta}_{0} = \\{ [\\matr{\\Theta}_{1}]_{0}, \\dots, [\\matr{\\Theta}_{L}]_{0} \\}$. Meantime, after training the network and ending up with trained parameters $\\matr{\\Theta}_{t} = \\{ [\\matr{\\Theta}_{1}]_{t}, \\dots, [\\matr{\\Theta}_{L}]_{t} \\}$, according to Lemma 8.6 from \\citep{conv_theory-allen2019convergence}, the bound $\\norm{ \\matr{\\Theta}_{L} \\matr{D}_{L-1} \\cdots \\matr{D}_{1}\\matr{\\Theta}_{1} }_{2} = \\mathcal{O}(L)$ still holds, which proves this statement.\n%\n% Then, for the bound on the gradients, we have\n% \\begin{displaymath}\n% \\begin{split}\n%     & \\norm{ \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t}) - \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}'; \\matr{\\Theta}_{t}) }_{2} \\\\\n%     & = \\norm{ \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t}) - \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) + \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) - \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}'; \\matr{\\Theta}_{0}) + \n%     \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}'; \\matr{\\Theta}_{0}) - \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}'; \\matr{\\Theta}_{t}) }_{2} \\\\\n%     & \\leq \\norm{ \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t}) - \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) }_{2}  + \\norm{ \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) - \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}'; \\matr{\\Theta}_{0}) }_{2}  + \\\\\n%     & \\qquad \\qquad\n%     \\norm{ \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}'; \\matr{\\Theta}_{0}) - \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}'; \\matr{\\Theta}_{t}) }_{2} .\n% \\end{split}  \n% \\end{displaymath}\n%\n% Firstly, we have \n% \\begin{displaymath}\n% \\begin{split}\n%     \\norm{ \\nabla_{[\\matr{\\Theta}_{l}]} f(\\vect{x}; \\matr{\\Theta}_{0}) }_{2} = \n%     \\norm{ (\\matr{\\Theta}_{L} \\matr{D}_{L-1} \\cdots \\matr{D}_{l+1}\\matr{\\Theta}_{l+1})\\cdot (\\matr{D}_{l+1} \\matr{\\Theta}_{l+1} \\cdots \\matr{D}_{1}\\matr{\\Theta}_{1}) \\cdot \\vect{x}^{\\intercal} }_{2} \\leq \\mathcal{O}(\\sqrt{L})\n% \\end{split}  \n% \\end{displaymath}\n% based on Lemma 7.3 from \\citep{conv_theory-allen2019convergence}, and this leads to $ \\norm{ \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) }_{2} \\leq \\mathcal{O}(L)$.\n% Analogously, we also derive\n% \\begin{displaymath}\n% \\begin{split}\n%     & \\norm{ \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) - \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}'; \\matr{\\Theta}_{0})}_{2} \\\\\n%     & \\quad = \\norm{ (\\matr{\\Theta}_{L} \\matr{D}_{L-1} \\cdots \\matr{D}_{l+1}\\matr{\\Theta}_{l+1})\\cdot (\\matr{D}_{l+1} \\matr{\\Theta}_{l+1} \\cdots \\matr{D}_{1}\\matr{\\Theta}_{1}) \\cdot (\\vect{x} - \\vect{x}')^{\\intercal} }_{2} \\leq \\mathcal{O}(L)\\cdot \\norm{\\vect{x} - \\vect{x}'}_{2}.\n% \\end{split}  \n% \\end{displaymath}\n% Then, according to Theorem 5 from \\citep{conv_theory-allen2019convergence} and with $\\norm{ \\matr{\\Theta}_{0} - \\matr{\\Theta}_{t} }_{2} \\leq \\omega$, we could have $\\norm{ \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) - \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t}) }_{2} \\leq \\mathcal{O}(\\omega^{1/3}L^{2} \\sqrt{\\log(m)}) \\cdot \\norm{ \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) }_{2}$. Substituting the $\\omega$ value with the conclusion from \\blemma \\ref{lemma_convergence_GNN_f_1}, we could have\n% \\begin{displaymath}\n% \\begin{split}\n%    \\norm{ \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) - \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t}) }_{2} & \\leq   \\mathcal{O}(\\omega^{1/3}L^{2} \\sqrt{\\log(m)}) \\cdot \\norm{ \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0}) }_{2}  \\\\ \n%    & = \\mathcal{O}(\\frac{tL^{4}\\log^{5/6}(m)}{\\rho^{1/3}m^{1/6}}).\n% \\end{split}  \n% \\end{displaymath}\n% Finally, assembling all parts together will lead to the conclusion.\n%\nThis lemma can be proved based on the fact that the ReLU activation is 1-Lipschitz continuous, and the weight matrix of each layer in the neural network has the norm of $\\mathcal{O}(1)$ at random initialization based on our Gaussian initialization of the weight matrices. Finally, combining the conclusion from \\textbf{Lemma} \\ref{lemma_FC_network_different_parameters} to bound $\\abs{ f(\\vect{x}; \\matr{\\Theta}_{t}) - f(\\vect{x}; \\matr{\\Theta}_{0}) }$ will complete the proof.\n\n\n\\qed\n\n\n\n% --------------------------------------\n\\begin{lemma}   \\label{lemma_FC_network_different_parameters}\nConsider a L-layer fully-connected network $f(\\cdot; \\matr{\\Theta}_{t})$ initialized w.r.t. Section \\ref{subsec_GNN_arch}. For any $ 0 < \\xi_{2} \\leq 1$, $ 0 < \\rho \\leq \\mathcal{O}(\\frac{1}{L})$. Let there be two sets of training samples $\\mathcal{P}_{t}, \\mathcal{P}'_{t}$ with the unit-length and the $\\rho$-separateness assumption, and let $\\matr{\\Theta}_{t}$ be the trained parameter on $\\mathcal{P}_{t}$ while $\\matr{\\Theta}'_{t}$ is the trained parameter on $\\mathcal{P}'_{t}$.\nSuppose $m, \\eta_{1}, \\eta_{2}, J_{1}, J_{2}$ satisfy the conditions in \\btheorem \\ref{theorem_regret_bound}. \nThen, with probability at least $1 - \\delta$, we have\n\\begin{displaymath}\n\\begin{split}\n    \\abs{ f(\\vect{x}; \\matr{\\Theta}_{t}) - & f(\\vect{x}; \\matr{\\Theta}'_{t}) } \\leq \\\\\n    & \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(\\frac{t^{3}L}{\\rho\\sqrt{m}}\\log(m)) + \n    \\mathcal{O}\\bigg( \\frac{t^{4}L^{2} \\log^{11/6} (m)}{\\rho^{4/3}m^{1/6}} \\bigg)\n    % & \\norm{ \\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t}) - \\nabla_{\\matr{\\Theta}'_{t}} f(\\vect{x}; \\matr{\\Theta}'_{t}) }_{2} \\leq\n\\end{split}  \n\\end{displaymath}\nwhen given a new sample $\\vect{x} \\in \\mathbb{R}^{d}$.\n\n\\end{lemma}\n\n\\textbf{Proof.}\nFirst, based on the conclusion from Theorem 1 from \\citep{conv_theory-allen2019convergence} and regarding the $t$ samples, the trained the parameters satisfy $\\norm{\\matr{\\Theta}_{t} - \\matr{\\Theta}_{0}}_{2}, \\norm{\\matr{\\Theta}'_{t} - \\matr{\\Theta}_{0}}_{2} \\leq \\mathcal{O}(\\frac{t^\n{3}}{\\rho\\sqrt{m}}\\log(m)) = \\omega$ where $\\matr{\\Theta}_{0}$ is the randomly initialized parameter. Then, we could have\n\\begin{displaymath}\n\\begin{split}\n    & \\norm{\\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t})}_{2} \\leq \n    \\norm{\\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0})}_{2} +\n    \\norm{\\nabla_{\\matr{\\Theta}_{t}} f(\\vect{x}; \\matr{\\Theta}_{t}) - \\nabla_{\\matr{\\Theta}_{0}} f(\\vect{x}; \\matr{\\Theta}_{0})}_{2} \\\\\n    & \\qquad \\leq \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(L)\n\\end{split}  \n\\end{displaymath}\nw.r.t. the conclusion from Theorem 1 and Theorem 5 of \\citep{conv_theory-allen2019convergence}. Then, regarding the Lemma 4.1 from \\citep{generalization_bound_cao2019generalization}, we would have\n\\begin{displaymath}\n\\begin{split}\n    & \\abs{ f(\\vect{x}; \\matr{\\Theta}_{t}) - f(\\vect{x}; \\matr{\\Theta}'_{t}) - \n    \\inp{\\nabla_{\\matr{\\Theta}'_{t}} f(\\vect{x}; \\matr{\\Theta}'_{t})}{\\matr{\\Theta}_{t} - \\matr{\\Theta}'_{t}} } \\leq \n    \\mathcal{O}(\\omega^{1/3} L^{2} \\sqrt{m\\log(m)}) \\cdot \\norm{\\matr{\\Theta}_{t} - \\matr{\\Theta}'_{t}}_{2}.\n\\end{split}  \n\\end{displaymath}\nTherefore, the our target could be reformed as\n\\begin{displaymath}\n\\begin{split}\n    & \\abs{ f(\\vect{x}; \\matr{\\Theta}_{t}) - f(\\vect{x}; \\matr{\\Theta}'_{t}) } \\leq \n    \\norm{ \\nabla_{\\matr{\\Theta}'_{t}} f(\\vect{x}; \\matr{\\Theta}'_{t}) }_{2}  \\norm{\\matr{\\Theta}_{t} - \\matr{\\Theta}'_{t}}_{2}\n    + \\mathcal{O}(\\omega^{1/3} L^{2} \\sqrt{m\\log(m)}) \\cdot \\norm{\\matr{\\Theta}_{t} - \\matr{\\Theta}'_{t}}_{2} \\\\\n    & \\qquad \\leq \n    \\bigg(1 + \\mathcal{O}(\\frac{tL^{3} \\log^{5/6} (m)}{\\rho^{1/3}m^{1/6}})\\bigg)\\cdot \\mathcal{O}(L) \\cdot \\omega + \\mathcal{O}(\\omega^{4/3} L^{2} \\sqrt{m\\log(m)})\n\\end{split}  \n\\end{displaymath}\nSubstituting the $\\omega$ with its value would complete the proof.\n\n\\qed\n\n\n\n\n\n\n\n\n% \\input{tex_Appd_tech_lemmas}\n\n% \\section{Computational Resources}\n\n% All the experiments are conducted on a Windows machine with an Intel Core i7 CPU, 64GB RAM, and two RTX 5000 GPUs.\n\n"
                }
            }
        },
        "tables": {
            "table_different_hops_100_users": "\\begin{table}[h]\n    \\centering\n    \\vspace{-0.3cm}\n    \\begin{tabular}{ |p{0.1cm}|p{1.8cm}p{1.8cm}p{1.8cm}p{1.8cm}|  }\n     \\hline\n     &\\multicolumn{4}{|c|}{ Bandwidth $\\gamma$ } \\\\\n     \\hline\n     \\textbf{$k$} & 0.1& 1& 2& 5 \\\\\n     \\hline\n     1    & 7276 [$1.6\\times 10^{-4}$]   &  7073 [$1.4\\times 10^{-3}$] &  7151 [$2.2\\times 10^{-3}$] &  7490 [$3.9\\times 10^{-3}$] \\\\\n     \\hline\n     2  &   6968 [$1.0\\times 10^{-4}$] & 6966 [$7.7\\times 10^{-4}$] & 7074 [$1.3\\times 10^{-3}$] &  7087 [$2.5\\times 10^{-3}$] \\\\\n     \\hline\n     3  &  7006 [$7.1\\times 10^{-5}$] & 7018 [$7.0\\times 10^{-4}$] &  6940 [$1.2\\times 10^{-3}$] &  7167 [$1.9\\times 10^{-3}$] \\\\\n     \\hline\n    \\end{tabular}\n    % \\vspace{+0.1in}\n    \\caption{ Cumulative regrets on MovieLens dataset with 100 users (different k / kernel bandwidth). The value in the brackets \"[]\" is the element standard deviation of the corresponding normalized adjacency matrix. }\n    \\vspace{-0.6cm}\n    \\label{table_different_hops_100_users}\n\\end{table}",
            "table_different_size_appx_neighborhood": "\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{ |p{2cm}|p{0.8cm}p{0.8cm}p{0.8cm}p{0.8cm}p{0.8cm}|  }\n     \\hline\n     &\\multicolumn{5}{|c|}{ Avg. regret per round at different $t$ } \\\\\n     \n     \\hline\n     \\textbf{Algorithm} & 2000  & 4000 & 6000 & 8000 & 10000 \\\\\n     \\hline\n      CLUB   & 0.7691 & 0.7513 & 0.7464 & 0.7468 & 0.7496\\\\\n     Neural-Ind  &   0.8901 &  0.8808 &  0.8790  &  0.8754 & 0.8741\\\\\n     Neural-Pool   & 0.7681 & 0.7526 & 0.7405 & 0.7362 & 0.7334\\\\\n     EE-Net  &   0.7886 & 0.7723 & 0.7642 & 0.7618 & 0.7582\\\\\n     Meta-Ban  &  0.7811 & 0.7761 & 0.7754 & 0.7729 & 0.7708\\\\\n     \\hline\n     \\name\\ ($\\widetilde{n}=50$)  &  0.7760 & 0.7245 & 0.7190 & 0.7265 & 0.7140\\\\\n     \\name\\ ($\\widetilde{n}=100$)   & 0.7406 & 0.7178 & 0.7172 & 0.7110 & 0.7104\\\\\n     \\name\\ ($\\widetilde{n}=150$)  &  0.7291 & 0.7228 & 0.7129 & 0.7105 & 0.7085\\\\\n     \\hline\n    \\end{tabular}\n    \\caption{ Running for 10000 rounds and with the number of users $n=500$ for the MovieLens data set, the comparison between \\name\\ and baselines on average regret per round. }\n    \\vspace{-0.5cm}\n    \\label{table_different_size_appx_neighborhood}\n\\end{table}",
            "table_exp_coef_regret_results": "\\begin{table}[h]\n    \\centering\n    \\vspace{-0.2cm}\n    \\begin{tabular}{ |p{1cm}|p{0.8cm}p{1.2cm}p{1.2cm}p{1.2cm}p{0.8cm}|  }\n     \\hline\n     &\\multicolumn{5}{|c|}{ Regret results with different $\\alpha$ values } \\\\\n     \n     \\hline\n     \\textbf{Dataset} & $\\alpha=0$  & $\\alpha=0.1$ & $\\alpha=0.3$ & $\\alpha=0.7$ & $\\alpha=1$ \\\\\n     \\hline\n      Yelp  & 7612 & 7444 & 7546 & 7509 & 7457\\\\\n     MNIST  &   2323 &  2110 &  2170  & 2151 & 2141\\\\\n     \\hline\n    \\end{tabular}\n    \\caption{ Results with different exploration coefficients $\\alpha$. }\n    \\vspace{-0.7cm}\n    \\label{table_exp_coef_regret_results}\n\\end{table}",
            "table_convergence_GNN": "\\begin{table}[h!]\n    \\centering\n    \\begin{tabular}{ |p{2.1cm}|p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}|  }\n     \\hline\n     &\\multicolumn{5}{|c|}{ Time step intervals } \\\\\n     \n     \\hline\n     \\textbf{Data (Algo.)} & 0-2k  & 2k-4k & 4k-6k & 6k-8k & 8k-10k  \\\\\n     \\hline\n     \\textbf{M (GNB)}   & 0.7050 &  0.6895 & 0.6795  &  0.6770 & 0.6685\\\\\n     \\textbf{Y  (GNB)}  &   0.7705 & 0.7685 & 0.7485 & 0.7450 & 0.7330\\\\\n     \\hline\n     M (EE-Net)    & 0.7320 & 0.7120 & 0.7072 & 0.6945 & 0.7070 \\\\\n     Y  (EE-Net)  &   0.8480 & 0.8420 & 0.7965 & 0.7950 & 0.7865\\\\\n     M (Neural-UCB)    & 0.8090 & 0.7115 & 0.7165 & 0.6945 & 0.6865\\\\\n      Y  (Neural-UCB)  &   0.8380 & 0.8115 & 0.8185 & 0.7940 & 0.8025\\\\\n     M (Meta-Ban)    & 0.7760 & 0.7245 & 0.7190 & 0.7265 & 0.7140 \\\\\n      Y  (Meta-Ban)  &   0.8525& 0.8035 & 0.7930 & 0.7600 & 0.7620\\\\\n     \\hline\n    \\end{tabular}\n    % \\vspace{+0.1in}\n    \\caption{ In different time step intervals, the average regrets per round on MovieLens (M) and Yelp (Y) data sets.}\n    % \\vspace{-0.7cm}\n    \\label{table_convergence_GNN}\n\\end{table}"
        },
        "figures": {
            "fig_model_arch": "\\begin{figure*}[t]\n  \\centering\n  \\vspace{-0.40cm}\n  \\includegraphics[width=0.8\\linewidth]{Figure/User_group.png}\n  \\vspace{-0.45cm}\n  \\caption{ Workflow of the proposed Graph Neural Bandits (\\name) framework. }\n  \\vspace{-0.45cm}\n  \\label{fig_model_arch}\n\\end{figure*}",
            "fig_experiment_regret_results": "\\begin{figure}[t!]\n  \\centering\n  \\vspace{-0.25cm}\n  \\includegraphics[width=\\linewidth]{Figure/cumu_regret_figure.pdf}\n  \\vspace{-0.8cm}\n  \\caption{Cumulative regrets on the recommendation and classification data sets.}\n  \\vspace{-0.5cm}\n  \\label{fig_experiment_regret_results}\n\\end{figure}",
            "fig_MORE_USERS_MovieLens": "\\begin{figure}[!h]\n  \\centering\n  \\vspace{-0.3cm}\n  \\includegraphics[width=\\linewidth]{Figure/cumu_regret_MORE_USERS_MovieLens.pdf}\n  \\vspace{-0.7cm}\n  \\caption{Cumulative regrets for different number of users with approximated user neighborhood (MovieLens data set).}\n  \\vspace{-0.6cm}\n  \\label{fig_MORE_USERS_MovieLens}\n\\end{figure}",
            "fig_running_time_main": "\\begin{figure}[ht]\n  \\centering\n  \\vspace{-0.10cm}\n  \\includegraphics[width=0.8\\linewidth]{Figure/scatter_running_time_main.png}\n  \\vspace{-0.3cm}\n  \\caption{Running time vs. performance with baselines.}\n  \\label{fig_running_time_main}\n  \\vspace{-0.3cm}\n\\end{figure}",
            "fig_rebuttal_group_size": "\\begin{figure}[th]\n    \\includegraphics[width=1\\columnwidth]{Figure/cumu_regret_rebuttal_movielens_datasets_figure.pdf}\\hfill \n    \\includegraphics[width=1\\columnwidth]{Figure/cumu_regret_rebuttal_yelp_datasets_figure.pdf}\\hfill\n    \\caption{ Cumulative regrets for different number of underlying user groups (on MovieLens and Yelp data sets)}\n       \\label{fig_rebuttal_group_size}\n\\end{figure}",
            "fig_running_time_appx": "\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.5\\linewidth]{Figure/scatter_running_time_appx.png}\n  \\caption{Running time \\& performance comparison.}\n  \\label{fig_running_time_appx}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\begin{split}\n    r_{i, t} =  h(\\vect{x}_{i, t}, u_{t}, \\mathcal{G}^{(1), *}_{i, t}) + \\epsilon_{i, t}\n\\end{split}\n\\label{eq_weighted_reward_func}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\begin{split}\n    \\big| \\mathbb{E}[r_{i,t} | u, \\vect{x}_{i, t}] - \\mathbb{E}[r_{i,t} | u', \\vect{x}_{i, t}] \\big| \\leq \\Psi \\big(\\mathcal{G}^{(1), *}_{i, t}[u:], \\mathcal{G}^{(1), *}_{i, t}[u':] \\big)\n\\end{split}   \n\\label{eq_assumption_correlation_vector}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\begin{split}\n    R(T) = \\mathbb{E}[\\sum_{t=1}^{T}(r_{t}^{*} - r_{t})] \n\\end{split}\n\\label{eq_pseudo_regret}\n\\end{equation}",
            "eq:4": "\\begin{equation} \n    w_{i, t}^{(1)}(u, u') = \\Psi^{(1)} \\big( f_{u}^{(1)}(\\vect{x}_{i, t}), f_{u'}^{(1)}(\\vect{x}_{i, t}) \\big)\n\\label{eq_est_exploit_correlation}\n\\end{equation}",
            "eq:5": "\\begin{equation} \n    w_{i, t}^{(2)}(u, u') = \\Psi^{(2)} \\bigg( f_{u}^{(2)}\\big( \\nabla f_{u}^{(1)}(\\vect{x}_{i, t}) ), f_{u'}^{(2)}\\big( \\nabla f_{u'}^{(1)}(\\vect{x}_{i, t}) \\big) \\bigg)\n\\label{eq_est_explore_correlation}\n\\end{equation}",
            "eq:6": "\\begin{equation} \nf_{u}(\\vect{\\chi}; \\matr{\\Theta}_{u} ) = \\matr{\\Theta}_{L} \\sigma ( \\matr{\\Theta}_{L-1}  \\sigma (\\matr{\\Theta}_{L-2} \\dots  \\sigma(\\matr{\\Theta}_{1} \\vect{\\chi}) )), ~~ \\sigma := \\text{ReLU}(\\cdot) \n\\label{eq_user_model_structure}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\begin{split}\n    \\matr{H}_{agg} = \\sigma\\big((\\matr{S}_{i, t}^{(1)})^{k} \\cdot (\\matr{X}_{i, t} \\matr{\\Theta}_{agg}^{(1)})\\big) \\in \\mathbb{R}^{n \\times m}\n\\end{split}\n\\label{eq_GNN_aggegation}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\matr{X}_{i, t} = \n\\left(\\begin{array}{cccc}\n\\vect{x}_{i, t}^{\\intercal} & \\matr{0} & \\cdots & \\matr{0} \\\\\n\\matr{0} & \\vect{x}_{i, t}^{\\intercal} & \\cdots & \\matr{0} \\\\\n\\vdots  &       & \\ddots   & \\vdots \\\\\n\\matr{0} & \\matr{0} & \\cdots & \\vect{x}_{i, t}^{\\intercal} \\\\\n\\end{array} \\right) \\in \\mathbb{R}^{n\\times nd}\n\\label{eq_new_embedding_matrix}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\begin{split}\n    &\\matr{H}_{l} = \\sigma(\\matr{H}_{l-1} \\cdot \\matr{\\Theta}_{l}^{(1)}) \\in \\mathbb{R}^{n\\times m} \n    ,~~ l \\in [L-1], \\\\\n    &\\widehat{\\vect{r}}_{all}(\\vect{x}_{i, t}) = \\matr{H}_{L-1} \\cdot  \\matr{\\Theta}_{L}^{(1)} \\in \\mathbb{R}^{n}\n\\end{split}\n\\label{eq_GNN_estimation}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n\\begin{split}\n    \\widehat{r}_{i, t} = f_{gnn}^{(1)}(\\vect{x}_{i, t},~\\mathcal{G}_{i, t}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) = [\\widehat{\\vect{r}}_{all}(\\vect{x}_{i, t})]_{u_{t}}\n\\end{split}\n\\label{eq_f_1_user_output}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n\\begin{split}\n    \\widehat{b}_{i, t} = f_{gnn}^{(2)}( \\nabla [f_{gnn}^{(1)}]_{i, t} , \\mathcal{G}_{i, t}^{(2)};  [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) \n    = [\\widehat{\\vect{b}}_{all}(\\vect{x}_{i, t})]_{u_{t}}.\n\\end{split}\n\\label{eq_f_2_explore_user_output}\n\\end{equation}",
            "eq:12": "\\begin{equation}\n\\begin{split}\n     \\mathsf{CB}_{t}(\\vect{x}) & = \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) \\\n    - (r -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))}  \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] \\\\\n    & \\leq \\underbrace{\\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})  - (r -  f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}))} \\ \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] }_{I_{1}}  \\\\\n    & + \\underbrace{\\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1), *}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1}) - f_{gnn}^{(1)}(\\vect{x},~\\mathcal{G}^{(1)}; [\\matr{\\Theta}_{gnn}^{(1)}]_{t-1})}  \\bigg| u_{t}, \\mathcal{X}_{t}\\bigg]}_{I_{2}} \\\\\n    & + \\underbrace{\\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}), ~\\mathcal{G}^{(2), *}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1}) -   f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})} \\ \\bigg| u_{t}, \\mathcal{X}_{t} \\bigg] }_{I_{3}} \\\\\n    & + \\underbrace{ \\mathbb{E} \\bigg[ \\abs{f_{gnn}^{(2)}(\\nabla f_{t}^{(1), *} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})    - f_{gnn}^{(2)}(\\nabla f_{t}^{(1)} (\\vect{x}),~\\mathcal{G}^{(2)}; [\\matr{\\Theta}_{gnn}^{(2)}]_{t-1})} \\ \\bigg| u_{t}, \\mathcal{X}_{t}  \\bigg] }_{I_{4}}.\n\\end{split}\n\\label{eq_single_CB}\n\\end{equation}",
            "eq:13": "\\begin{equation}\n\\begin{split}\n    \\phi(\\Tilde{\\vect{x}}, \\vect{x}) = \n    (\\frac{\\Tilde{\\vect{x}}}{\\sqrt{2}}, \\frac{\\vect{x}}{2}, c)\n\\end{split}\n\\label{eq_concat_transformation}\n\\end{equation}"
        }
    }
}