{
    "meta_info": {
        "title": "Toward Learning Robust and Invariant Representations with Alignment  Regularization and Data Augmentation",
        "abstract": "Data augmentation has been proven to be an effective technique for developing\nmachine learning models that are robust to known classes of distributional\nshifts (e.g., rotations of images), and alignment regularization is a technique\noften used together with data augmentation to further help the model learn\nrepresentations invariant to the shifts used to augment the data. In this\npaper, motivated by a proliferation of options of alignment regularizations, we\nseek to evaluate the performances of several popular design choices along the\ndimensions of robustness and invariance, for which we introduce a new test\nprocedure. Our synthetic experiment results speak to the benefits of squared l2\nnorm regularization. Further, we also formally analyze the behavior of\nalignment regularization to complement our empirical study under assumptions we\nconsider realistic. Finally, we test this simple technique we identify\n(worst-case data augmentation with squared l2 norm alignment regularization)\nand show that the benefits of this method outrun those of the specially\ndesigned methods. We also release a software package in both TensorFlow and\nPyTorch for users to use the method with a couple of lines at\nhttps://github.com/jyanln/AlignReg.",
        "author": "Haohan Wang, Zeyi Huang, Xindi Wu, Eric P. Xing",
        "link": "http://arxiv.org/abs/2206.01909v1",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "to appear at KDD 2022, the software package is at  https://github.com/jyanln/AlignReg. arXiv admin note: text overlap with  arXiv:2011.13052"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n%%\n%% data augmentation has been used a lot (for improving accuracy)\n%% AR, robustness, invariance\n%% our contributions\n%%\n\nData augmentation, \n\\textit{i.e.}, to increase the dataset size through \ngenerating new samples by \ntransforming the existing samples with some predefined functions, \nis probably one of the most often used techniques to improve \na machine learning model's performance. \nIt has helped machine learning models achieve high prediction accuracy \nover various benchmarks \n\\citep[\\textit{e.g.,}][]{LimKKKK19,HoLCSA19,Zhong0KL020,BuslaevIKPDK20,Ghiasi20,Kostrikov21}. \n\nIn addition to improving prediction accuracy, \nthe community has also leveraged data augmentation to help the models \nlearn more robust representations that can generalize \nto the datasets distributed differently \n\\citep[\\textit{e.g.,}][]{zheng2016improving,SunYOHX18,HuangWXH20,MinMDPL20}. \nTo improve robustness, the community usually designed the transformation functions used to augment the data in correspondence to the transformations we see in the real world \\citep{hernandez2020data}, \nsuch as the changes of image texture or contrast. \nThus, models trained with these augmented data \nare more likely to be invariant to these designed transformations, \nsuch as the texture or contrast variations of the input images. \n\nTo further help a model learn representations invariant to the transformations, \nwe can regularize the model so that the distance between representations learned by the model \nfrom a pair of data (the original one and the transformed counterpart) will be small.\nThis regularization has been used extensively recently to help models learn more robust and invariant representations \\citep[\\textit{e.g.,}][]{liang2018learning,kannan2018adversarial,ZhangYJXGJ19,Hendrycks2020augmix}. \nMotivated by this popularity, \nthis paper mainly studies the behaviors of this regularization, which we refer to as \n\\emph{alignment regularization} \n(AR).\nIn particular, we seek to answer the question:\n\\emph{how should we use alignment regularization to take advantage of the augmented data to the fullest extent to learn robust and invariant models?}\n\nTo answer this, \nwe first conduct a range of experiments \nover image classification benchmarks \nto evaluate how popular variants of AR\ncontribute to learning robust and invariant models. \nWe test for accuracy, robustness, and invariance, \nfor which we propose a new test procedure.\nOur empirical study favors \nthe squared \\ltwo{} norm. \n% Further, we also complement our empirical study \n% by showing that norm-based regularizations \n% can lead to bounded generalization error (under assumptions introduced later). \nOur contributions of this paper are as follows. \n\n\\begin{itemize}\n\\item With a new invariance test, we show \nthat \nalignment regularization is important\nto help the model \nlearn representations \ninvariant to the transformation function, \nand squared \\ltwo{} norm is considered  \nthe favorable choice as assessed by \na variety of empirical evaluations (Section~\\ref{sec:motivate}).  \n\\item We formalize a generalization error bound \nfor models trained with AR and augmented data\n(Section~\\ref{sec:method}). \n\\item We test the method we identified\n(squared \\ltwo{} norm as AR) \nin multiple scenarios.\n% We believe the fact that this generic approach \n% can compete with methods specially designed \n% for different scenarios\n% can endorse its empirical strength \nWe notice that this generic approach can compete with methods specially designed for different scenarios, which we believe endorses its empirical strength\n(Section~\\ref{sec:exp}). \n\\end{itemize}\n"
            },
            "section 2": {
                "name": "Related Work and Key Differences",
                "content": "\n\\label{sec:related}\n% Data augmentation has been used \n% effectively for years.\nTracing back to the earliest convolutional neural networks, \\citep{wang2017origin}, \nwe notice that even early models for the MNIST dataset \nhave been boosted by data augmentation \\citep{abu1990learning,simard1991tangent,lecun1998gradient}. \nLater, the rapidly growing machine learning community \nhas seen a proliferate development of data augmentation techniques \nthat have helped models climb the state-of-the-art ladder \\citep{shorten2019survey}. \nAmong the augmentation techniques, \nthe most relevant one to this paper is to\ngenerate the samples (with constraint) \nthat maximize the training loss along with training \\citep{FawziSTF16}.\n\nWhile the above paragraph mainly discusses \nhow to generate the augmented samples, \nwe mainly study\nhow to train the models with augmented samples. \nFor example, instead of directly mixing augmented samples \nwith the original samples, \none can consider regularizing the representations (or outputs)\nof original samples and augmented samples \nto be close under a distance metric (which we refer to as alignment regularization, AR). \nMany concrete ideas have been explored in different contexts. \nFor example, $\\ell_2$ distance and cosine similarities\nbetween internal representations in speech recognition\n\\citep{liang2018learning}, \nsquared $\\ell_2$ distance between logits \\citep{kannan2018adversarial},\nor KL divergence between softmax outputs \\citep{ZhangYJXGJ19} in adversarially robust vision models, \nJensenâ€“Shannon divergence (of three distributions) \nbetween embeddings for texture invariant image classification \\citep{Hendrycks2020augmix}. \nThese are but a few highlights of the concrete and successful implementations for different applications\nout of a vast collection (\\textit{e.g.}, \\citep{WuMWZGXX19,ZFY019, zhang2019regularizing, Shah_2019_CVPR, asai2020logicguided,sajjadi2016regularization,zheng2016improving,xie2015hyper}), \nand we can expect methods permuting these three elements (distance metrics, representation or outputs, and applications) \nto be discussed in the future. \nFurther, \ngiven the popularity of GAN \\citep{goodfellow2016nips} \nand domain adversarial neural network \\citep{ganin2016domain}, \nwe can also expect the distance metric generalizes \nto a specialized discriminator (\\textit{i.e.}, a classifier), \nwhich can be intuitively understood as a calculated\n(usually maximized) distance measure,\nand one example here is the \nWasserstein-1 metric \\citep{arjovsky2017wasserstein,gulrajani2017improved}. \n\n\\textbf{Key Differences:}\nWith this rich collection of regularizing choices, \nwhich one method should we consider in general? \nMore importantly, \ndo we need the regularization at all? \nThese questions are important \nfor multiple reasons,\nespecially since \nsometimes \nAR may worsen the results \\citep{jeong2019consistency}. \nIn this paper, we first conduct an empirical study to \nshow that \nAR \n(especially squared \\ltwo{} norm) \ncan help \nlearn robust and invariant models,\nwe then also derive generalization error bounds to \ncomplement our empirical conclusion. \n\n% In this paper, we answer the first question \n% with a proved upper bound \n% of the worst case generalization error, \n% and our upper bound explicitly describes what regularizations are needed. \n% For the second question, \n% we will show that regularizations can \n% help the model to learn the concept of invariance. \n\nThere are also several previous discussions \nregarding the detailed understandings of data augmentation \\citep{yang2019invariance, chen2019grouptheoretic,hernndezgarca2018data,rajput2019does,DaoGRSSR19,ghosh2021on,zhang2021how}, \namong which, \\citep{yang2019invariance} is probably the most relevant \nas it also defends the usage of the AR. \nIn addition to what is reported in \\citep{yang2019invariance}, our work also connects to invariance\nand shows that another advantage of AR is to learn invariant representations. \n% However, we believe our discussions are more comprehensive\n% and supported theoretically, \n% since our analysis directly suggests the ideal regularization.\n% Also, empirically, we design an invariance test \n% in addition to the worst-case accuracy used in the preceding work. \n\n\n\n% {\\color{red} This is the beginning of the new related work}\n% % 1. fancy data augmentation (auto-augmentation, etc)\n% % 2. alignment loss\n% % 3. theoretical understanding of data augmentation\n\n% \\textbf{Theoretical understanding of data augmentation}\n% Kernel aspect: \n% \\citep{burges1999geometory, scholkopf1996incorporating, muandet2012learning} studied data augmentation in the context of simple geometrical invariances with closed forms.\n% \\citep{dao2019kernel} proposed the explanation of data augmentation from the kernel perspective - it can improve generalization both by inducing invariance and by reducing model complexity. \n\n% Simplified linear setting: \n% \\citep{zhang2017mixup(-)} considered the simpler linear setting which is able to model linear transformations used in data augmentation. They showed that label-invariant transformations can add new information to the training data. \n% \\citep{zhang2017mixup} shows that the mixup data augmentation method has the effect of regularization by shrinking the weight of the training data relative to the l2 regularization term on the training data.\n\n\n"
            },
            "section 3": {
                "name": "Accuracy, Robustness, and Invariance",
                "content": "\n\\label{sec:background}\nThis section discusses the three major evaluation metrics we will use to test AR. \nWe will first recapitulate the background of accuracy and robustness. Then we will introduce our definition of invariance and our proposed evaluation.  \n\n\\textbf{Notations}\n$(\\X, \\Y)$ denotes the data,\nwhere $\\X \\in \\mathcal{R}^{n\\times p}$ and $\\Y \\in \\{0, 1\\}^{n\\times k}$\n(one-hot vectors for $k$ classes).  \n$(\\x, \\y)$ denotes a sample. \n$f(, \\theta)$ denotes the model,\nwhich takes in the data and outputs the softmax (probabilities of the prediction) \nand $\\theta$ denotes the corresponding parameters. \n$g()$ completes the prediction (\\textit{i.e.},\nmapping softmax to one-hot prediction). \n$a()$ denotes a function for data augmentation, \n\\textit{i.e.},\na transformation function. \n$a\\in \\ma$, \nwhich is the set of transformation functions of interest. \n$\\mP$ denotes the distribution of $(\\x, \\y)$.\nFor any sampled $(\\x, \\y)$, we can have $(a(\\x), \\y)$, \nand we use $\\mP_a$ to denote the distribution of these transformed samples. \nFurther, we use $\\mathbf{Q}_{a(\\x), \\wt}$ to \ndenote the distribution of $\\faxt$ for $(\\x, \\y)\\sim \\mP$. \n$D(\\cdot, \\cdot)$ is a distance measure over two distributions. \n$r(\\theta)$ denotes the risk of model $\\theta$. \n$\\widehat{\\cdot}$ denotes the estimation of the term $\\cdot$, \ne.g.,\n$\\wrp(\\wt)$ denotes the empirical risk of the estimated model.   \n\n",
                "subsection 3.1": {
                    "name": "Accuracy",
                    "content": "\n% Since one of the central goals of supervised machine learning study \n% is to improve the prediction accuracy (or to reduce the prediction error) of a model, \n% accuracy (or error) has widely accepted definitions. \n% For example, \nThe community studying the statistical property\nof the error bound usually focuses on the expected risk defined as\n\\begin{align}\n    \\rp(\\wt) =  \\mathbb{E}_{(\\x, \\y) \\sim \\mP} \\mI[g(\\fxt) \\neq \\y], \n    \\label{eq:accuracy}\n\\end{align}\nwhere $\\mI[\\mathbf{c}]$ is a function that returns $1$ if the condition $\\mathbf{c}$ holds. \n\nIn practice, the error is evaluated by replacing $\\mP$ with a hold-out test dataset, and the accuracy is $1 - \\rp(\\wt)$. \n\n"
                },
                "subsection 3.2": {
                    "name": "Robustness",
                    "content": "\n% Robustness has been widely studied in the fields of \n% cross-domain robustness \\citep[\\textit{e.g.},][]{ben2010theory,muandet2013domain}\n% or adversarial robustness\n% \\citep[\\textit{e.g.},][]{szegedy2013intriguing,goodfellow2014explaining}.\nWe define \nrobustness as \nthe worst-case expected risk\nwhen the test data is allowed to \nbe transformed  \nby functions in $\\ma$, following \\citep[\\textit{e.g.},][]{szegedy2013intriguing,goodfellow2014explaining}.\nFormally, we study the worst-case error as \n\\begin{align}\n    \\rpa(\\wt) =  \\mathbb{E}_{(\\x, \\y) \\sim \\mP} \\max_{a\\sim \\ma}\\mI[g(\\faxt) \\neq \\y],\n    \\label{eq:robustness}\n\\end{align}\nwhere we use $\\rpa(\\wt)$ to denote the robust error as it will depend on $\\ma$. \nIn practice, \nthe robust error is also evaluated \nby replacing $\\mP$ with a hold-out dataset. \n\n\n"
                },
                "subsection 3.3": {
                    "name": "Invariance",
                    "content": "\n\\label{sec:background:invariance}\n% While the robustness metric has \n% been fostering the development of \n% robust machine learning well, \nFurther, \nhigh robustness performances do not necessarily mean \nthe model is truly invariant to the transformation functions \\citep{hernandez2019learning}, \nand we continue to introduce a new test \nto evaluate the model's behavior in learning representations invariant to the transformations.\n% we notice that the metric alone \n% may not fully reveal how the models understand\n% the data. \n% For example, one incentive to use \n% data augmentation for robust models \n% is to dilute the undesired signals in the samples, \n% so that the models may focus more on the \n% remaining semantic patterns in the data. \n% While learning only semantic patterns \n% can lead to a model excelling the robustness evaluation metrics, \n% high robustness score does not necessarily \n% mean the model only learns semantic patterns \n% and discards undesired signals\n% (as we will show later). \n\n\\paragraph{Invariance}\nIf the model can learn a representation invariant to the transformation functions,\nit will map the samples of different transformations\nto the same representation. \nIntuitively, to measure how invariant a model is to the transformations in $\\ma$, \nwe can calculate the distances between each pair of the two transformed samples when a sample is transformed with functions in $\\ma$. \nThus, we define the following term to measure invariance:\n\\begin{align}\n    I_{\\mP, \\ma}(\\wt) = \\sup_{a_1, a_2\\in \\ma} D (\\mathbf{Q}_{a_1(\\x), \\wt}, \\mathbf{Q}_{a_2(\\x), \\wt}), \n    \\label{eq:invariance}\n\\end{align}\nWe suggest using\nWasserstein metric as $D(\\cdot, \\cdot)$,\nconsidering its favorable properties \n(\\textit{e.g.}, see practical examples in Figure 1 of\n\\citep{cuturi2014fast} or theoretical discussions in \\citep{villani2008optimal}).\n\nIn practice, we also need to replace $\\mP$ \nwith a hold-out dataset\nso that the evaluation can be performed. \nIn addition, we notice that\n$I_{\\mP, \\ma}(\\wt)$, \nalthough intuitive, \nis not convenient in practice because the evaluated values are not bounded. \nThus, we reformulate it into the following invariance test procedure, \nwhose final score will be bounded between 0 and 1 (the higher, the better). \nTherefore, the score can be conveniently discussed together with accuracy and robust accuracy, which are also bounded between 0 and 1.\n\n\\paragraph{Invariance test}\nGiven a family of transformation functions\nused in data augmentation $\\ma = \\{a_1(), a_2(), \\dots, a_t()\\}$ \nof $t$ elements, \nand a collection of samples (from the hold-out dataset) of the same label $i$, \ndenoted as $\\X^{(i)}$, \nthe evaluation procedure is as follows. \nWe first generate the transformed copies of $\\X^{(i)}$ with $\\ma$, \nresulting in $\\X^{(i)}_{a_1}, \\X^{(i)}_{a_2}, \\dots, \\X^{(i)}_{a_t}$. \nWe combined these copies into a dataset, denoted as $\\mathcal{X}^{(i)}$. \nFor every sample $\\x$ in $\\mathcal{X}^{(i)}$, \nwe retrieve its $t$ nearest neighbors of other samples in $\\mathcal{X}^{(i)}$, \nand calculate the overlap of the retrieved samples \nwith the transformed copies of $\\x$ by $\\ma$, \n\\textit{i.e.}, \n$\\{a_1(\\x), a_2(\\x), \\dots, a_t(\\x)\\}$. \nThe calculated overlap score will be in $[0, 1]$ in general,\nbut since the identity map is usually in $\\ma$, \nthis score will usually be in $[1/t, 1]$. \n\nDuring the retrieval of nearest neighbors, \nwe consider the distance function of the two samples (namely $\\x$ and $\\x'$)\nas \n%$c(\\x_1, \\x_2) = d(f(\\x_1;\\wt),f(\\x_2;\\wt))$, \n$$d(f(\\x;\\wt),f(\\x';\\wt))$$,\nwhere $\\wt$ is the model we are interested in examining. \nIn the empirical study later, \nwe consider $d(\\mathbf{u},\\mathbf{v})=\\Vert\\mathbf{u}-\\mathbf{v}\\Vert_1$, with $\\mathbf{u}$ and $\\mathbf{v}$ denoting two vectors.\nIf we use other distance functions, \nthe reported values may differ, but we notice that\nthe rank of the methods compared in terms of \nthis test barely changes. \n\nFinally, we iterate through label $i$ and report the averaged score for all the labels as the final score. \nA higher score \nindicates the model $\\wt$ is more invariant to \nthe transformation functions in $\\ma$. \n\nThis invariance test procedure is formally presented in Algorithm~\\ref{alg:invariance} below. \n\n\\begin{algorithm}\n\\scriptsize \n\\SetAlgoLined\n\\KwResult{$\\widehat{I}(\\wt)$}\n\\textbf{Input:} a family of transformation functions $\\ma = \\{a_1(), a_2(), \\dots, a_t()\\}$, \na hold-out dataset $(\\X,\\Y)$, the model of interest $\\wt$, and a distance metric $d()$\\;\n \\For {every label $i$ }{\n    identify all the samples from $(\\X,\\Y)$ with label $i$, name this set of samples $\\X^{(i)}$\\;\n    \\For {every $a() \\in \\ma$}{\n        generate $\\X_a^{(i)}$ by applying $a()$ to every $\\x \\in \\X^{(i)}$\\;\n    }\n    $\\mathcal{X}^{(i)} = \\X^{(i)}_{a_1} \\cup \\X^{(i)}_{a_2} \\cup, \\dots, \\cup \\X^{(i)}_{a_t}$\\;\n    \\For {every $\\x \\in \\X^{(i)}$}{\n        generate set $T = \\{a_1(\\x), a_2(\\x), \\dots, a_t(\\x)\\}$\\;\n        \\For {every $\\x' \\in \\mathcal{X}^{(i)}$}{\n            calculate the distance between $\\x$ and $\\x'$ with $d(f(\\x;\\wt),f(\\x';\\wt))$\\;\n        }\n        retrieve the $t$ nearest neighbors of $\\x$ out of $\\mathcal{X}^{(i)}$, and name this set of samples $K$\\;\n        calculate the score for $\\x$ with $\\vert T \\cap K\\vert/\\vert T\\vert$, where $\\vert T\\vert$ denotes the cardinality of the set $T$\\;\n    }\n    calculate the score for label $i$ as the average score across all $\\x \\in \\X^{(i)}$\\;\n }\n calculate the final score $\\widehat{I}(\\wt)$ as the average score across all the labels\\;\n \\caption{Invariance test}\n \\label{alg:invariance}\n\\end{algorithm} \n\n% \\paragraph{Invariance}\n% In addition to robustness, \n% we are also interested in whether the model \n% learns to be invariant to the undesired signals.  \n% Intuitively, \n% if data augmentation is used to help \n% dilute the undesired signals from data\n% by altering the undesired signals with $a() \\in \\ma$, \n% a successfully trained model \n% with augmented data \n% will map the raw data with various undesired signals \n% to the same embedding. \n% Thus, \n% we study the following metric \n% to quantify the model's ability in learning invariant representations:\n% \\begin{align}\n%     I(\\wt, \\mP) = \\sup_{a_1, a_2\\in \\ma} d_x (\\mP_{a_1, \\wt}, \\mP_{a_2, \\wt}), \n%     \\label{eq:invariance}\n% \\end{align}\n% where \n% $\\mP_{a, \\wt}$ to denote the distribution of $\\faxt$ for $(\\x, \\y)\\sim \\mP$. \n% $d_x()$ is a distance over two distributions, and we suggest to use \n% Wasserstein metric given its  favorable properties \n% (\\textit{e.g.}, see practical examples in Figure 1 of\n% \\citep{cuturi2014fast} or theoretical discussions in \\citep{villani2008optimal}). \n% Due to the difficulties in assessing $\\faxt$\n% (as it depends on $\\wt$), \n% we mainly study \\eqref{eq:invariance} empirically, \n% and argue that \n% models trained with explicit regularization \n% of the empirical counterpart\n% of \\eqref{eq:invariance} will have favorable invariance property. \n"
                }
            },
            "section 4": {
                "name": "Empirical Study",
                "content": "\n\\label{sec:motivate}\n% The invariance test conveniently \n% allows us to reexamine the value of \n% data augmentation for training models, \n% especially when the goal of training \n% is to develop a model that \n% focuses on the semantics of data. \nIn this section, \nwe conduct experiments to \nstudy the relationship between robustness and invariance, \nas well as how training with AR\ncan help improve the invariance score. \nIn short, our empirical study in this section\nwill lead us to the following three major conclusions:\n\\begin{itemize}\n    \\item High robust accuracy does not necessarily mean a high invariance score and vice versa. \n    \\item AR can help improve the invariance score. \n    \\item Squared \\ltwo{} norm over logits is considered the empirically most favorable AR option for learning robust and invariant representations. \n\\end{itemize}\n\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "Experiment Setup",
                    "content": "\n\\label{sec:exp:sync}\n\nOur empirical investigation is conducted over \ntwo benchmark datasets (MNIST dataset with LeNet architecture and CIFAR10 dataset with ResNet18 architecture) and \nthree sets of the transformations.\n\n% \\paragraph{Data and Model}\n% We study MNIST dataset with LeNet architecture, \n% and CIFAR10 dataset with ResNet18 architecture. \n% To examine the effects of the augmentation strategies, \n% we disable the heuristics that are frequently used \n% to boost the test accuracy of models, \n% such as the default augmentation many models trained for CIFAR10 adopted, \n% and the BatchNorm (also due to the recent arguments against \n% BatchNorm in learning robust features \\citep{galloway2019batch,wang2020high}),  \n% although forgoing these heuristics will result in \n% a lower overall performance than one usually expects. \n\n\\paragraph{Transformation Functions}\nWe consider three sets of transformation functions: \n\\begin{itemize}\n    \\item \\textbf{Texture}: we use Fourier transform to perturb the texture of the data by discarding the high-frequency components cut-off by a radius $r$, following \\citep{wang2020high}. The smaller $r$ is, the fewer high-frequency components the image has. We consider $$\\ma=\\{a_0(), a_{12}(), a_{10}(), a_{8}(), a_{6}()\\}$$, where the subscript denotes the radius $r$ except that $a_0()$ is the identity map. \n    We consider $\\ma$ during test time, but only $a_0()$ and $a_6()$ during training. \n    %Thus, vertexes are $a()$ and $a_6()$. \n    \\item \\textbf{Rotation}: we rotate the images clockwise $r$ degrees with $\\ma=\\{a_0(), a_{15}(), a_{30}(), $ $a_{45}(), a_{60}()\\}$, where the subscript denotes the degree of rotation and $a_0()$ is the identity map. \n    We consider $\\ma$ during test time, but only $a()_0$ and $a_{60}()$ during training. \n    %Thus, vertexes are $a()$ and $a_{60}()$.\n    \\item \\textbf{Contrast}: we create the images depicting the same visual information, but with different scales of the pixels, including the negative color representation. \n    Therefore, we have $\\ma = \\{a_0(\\x)=\\x, a_1(\\x) = \\x/2, a_2(\\x) = \\x/4, a_3(\\x) = 1-\\x, a_4(\\x) = (1-\\x)/2, a_5(\\x) = (1-\\x)/4$, where $\\x$ stands for the image whose pixel values have been set to be between 0 and 1. \n    We consider $\\ma$ during test time, but only $a_0()$ and $a_3()$ during training.\n    %We consider $a()$ and $a_3()$ as vertexes. \n\\end{itemize}\n\n\\paragraph{Alignment Regularizations}\nWe consider the following popular choices of AR (with $\\mathbf{u}$ and $\\mathbf{v}$ denoting two vector embeddings): %$\\mathbf{U}$ and $\\mathbf{V}$ denoting two batches of vectors)\n\\begin{itemize}\n    \\item [\\textsf{L}:] $\\ell_1$ norm of the vector differences, \\textit{i.e.}, $\\Vert \\mathbf{u} - \\mathbf{v} \\Vert_1$\n    \\item [\\textsf{S}:] squared $\\ell_2$ norm of the vector differences, \\textit{i.e.}, $\\Vert \\mathbf{u} - \\mathbf{v} \\Vert_2^2$\n    \\item [\\textsf{C}:] cosine similarity, \\textit{i.e.}, $\\mathbf{u}^T\\mathbf{v}/\\Vert\\mathbf{u}\\Vert\\cdot\\Vert\\mathbf{v}\\Vert$\n    \\item [\\textsf{K}:] KL divergence over a batch of paired embeddings; the second argument are augmented samples. \n    \\item [\\textsf{W}:] Wasserstein-1 metric over a batch of paired embeddings, with implementation following Wasserstein GAN \\citep{arjovsky2017wasserstein,gulrajani2017improved}\n    \\item [\\textsf{D}:] a vanilla GAN discriminator over a batch of paired embeddings, the one-layer discriminator is trained to classify samples vs. augmented samples. \n\\end{itemize}\nWe mainly discuss applying the AR to logits (embeddings prior to the final softmax function). We have also experimented with applying to the final softmax output and the embeddings one layer prior to logits. Both cases lead to substantially worse results, so we skip the discussion. \n\n\\paragraph{Hyperparameters} \nWe use standard data splits. \nWe first train the baseline models to get reasonably high performances\n(for MNIST, we train 100 epochs with learning rate set to be $10^{-4}$; \nfor CIFAR10, we train 200 epochs with learning rate initialized to be $10^{-1}$ \nand reduced one magnitude every 50 epochs; \nbatch sizes in both cases are set to be 128).  \nThen we train other augmented models with the same learning rate and batch size \\textit{etc}. \nThe regularization weight is searched with 8 choices evenly split in the logspace from $10^{-7}$ to $1$. \n% We start the search with $10^{-3}$, explore both directions, and terminate the search when the weight is too small and regularization has no observable effects in comparison to the baseline, as well as when the weight is too big and the model cannot predict above random chances. \nFor each method, the reported score is from the weight resulting in\nthe highest robust accuracy. \nWe test with three random seeds. \n\n\\paragraph{Evaluation Metrics:} We consider the three major evaluation metrics as we discussed in Section~\\ref{sec:background}: \n\\begin{itemize}\n    \\item \\textbf{Accuracy:} test accuracy on the original test data. \n    \\item \\textbf{Robustness:} the worst-case accuracy when each sample can be transformed with $a \\in \\ma$.  \n    \\item \\textbf{Invariance:} the metric to test whether the learned representations are invariant to the transformation functions, as introduced in Section~\\ref{sec:background:invariance}. \n\\end{itemize}\n\n"
                },
                "subsection 4.2": {
                    "name": "Results and Discussion",
                    "content": "\n\nTables~\\ref{tab:mnist:consistency} and~\\ref{tab:cifar10:consistency} show the empirical results across the three distribution shifts and the three evaluation metrics. \nFirst of all, \nno method can dominate across all these \nevaluations, \nprobably because of the tradeoff\nbetween accuracy and robustness \n\\citep{tsipras2018robustness,ZhangYJXGJ19,wang2020high}. \nSimilarly, the tradeoff \nbetween accuracy and invariance\ncan be expected from the role of the regularization weight: \nwhen the weight is small, \nAR has no effect, \nand the model is primarily optimized for improving accuracy;\nwhen the weight is considerable, \nthe model is pushed toward a trivial solution \nthat maps every sample to the same embedding, \nignoring other patterns of the data. \nThis is also the reason that results in \nTables~\\ref{tab:mnist:consistency} and~\\ref{tab:cifar10:consistency}\nare selected according to the robust accuracy. \n\nDue to the tradeoffs, it may not be strategic \nif we only focus on the highest number of each row. \nInstead, we suggest studying the three rows \nof each test case together and compare the tradeoffs, \nwhich is also a reason we reformulate the \ninvariance test in Section~\\ref{sec:background:invariance} \nso that we can have\nbounded invariance scores directly comparable to \naccuracy and robust accuracy. \n\n\nFor example, \nin the texture rows of Table~\\ref{tab:mnist:consistency}, \nwhile cosine similarity can outperform \nsquared \\ltwo{} norm in accuracy and robustness \nwith 0.3 and 0.1 margins, respectively, \nit is disadvantageous in invariance with a larger margin (1.0).\nSimilarly, for rotation rows of Table~\\ref{tab:mnist:consistency}, \nKL-divergence shows the overall highest scores, followed by $\\ell_1$ norm and squared $\\ell_2$ norm. \nFor contrast rows, \nboth $\\ell_1$ norm and squared $\\ell_2$ norm stand out. \nOverall, experiments in MNIST suggest the desired \nchoice to be $\\ell_1$ norm or squared $\\ell_2$ norm, \nand we believe squared $\\ell_2$ norm is marginally better. \n\nOn the other hand, the experiments in CIFAR10 in Table~\\ref{tab:cifar10:consistency}\nmostly favor $\\ell_1$ norm and squared $\\ell_2$ norm. \nGood performances can also be observed from \nWasserstein-1 metric for rotation. \nAlso, we notice that squared $\\ell_2$ norm, in general, outperforms \n$\\ell_1$ norm. \n\nThus, our empirical study recommends \nsquared $\\ell_2$ norm for AR to learn robust and invariant models, \nwith $\\ell_1$ norm as a runner-up. \n\n\n\n% Where the AR applies is also worth studying, and we consider the following three scenarios: \n% \\begin{itemize}\n%     \\item logits (embeddings prior to the final softmax function)\n%     \\item embeddings one layer prior to logits\n%     \\item final softmax output (However, results when AR applied here are seemingly substantially worse than when applied elsewhere, so the results are not reported here). \n% \\end{itemize}\n\n% We first train the baseline models to get reasonably high performance, and then train other augmented models with the same hyperparameters. \n% \\va{} and \\ra{} are augmented with vertexes, \n% while \\vwa{} and \\rwa{} are augmented with $\\ma$. \n% For methods with a regularizer, we run the experiments with \n% 9 hyperparameters evenly split in the logspace from $10^{-4}$ to $10^4$, and we report the methods with the best worst-case accuracy.\n\n\n"
                }
            },
            "section 5": {
                "name": "Analytical Support",
                "content": "\n\\label{sec:method}\n\nAccording to our discussions in Section~\\ref{sec:background:invariance}, \nWasserstein metric is supposed to be a favorable option \nas AR. \nHowever, its empirical performance does not \nstand out.\nWe believe this disparity is mainly due to \nthe difficulty in calculating the Wasserstein metric\nin practice. \n\nOn the other hand, \nnorm-based consistency regularizations stand out. \nWe are interested in studying the properties \nof these metrics. \nIn particular, this section aims to complement the \nempirical study by showing that\nnorm-based AR can lead to bounded robust generalization error\nunder certain assumptions.  \n\nAlso, our experiments only use two transformation functions \nfrom $\\ma$ during training but are tested with all the functions in $\\ma$. \nWe also discuss the properties of these two functions \nand argue that \ntraining with only these two functions can be a good strategy \nwhen certain assumptions are met. \n\n% Due to the limitation of space, \n% we have to list the details of the assumptions in the appendix. \n\n",
                "subsection 5.1": {
                    "name": "Overview of Analytical Results",
                    "content": "\n\nAll together, we need six assumptions (namely, A1-A6). \nOut of these assumptions, \nA1 is necessary for using data augmentation, \nand A4 is necessary for deriving machine learning generalization error bound. \nA2 and A3 are properties of data transformation functions, \nand A5 and A6 are technical assumptions. \nAlong with introducing these assumptions, we will also offer empirical evidence showing that these assumptions are likely to hold in practice.\n\nIn particular, \nwe will show that\n\\begin{itemize}\n    \\item With A2 holds, $\\ell_1$ norm can replace the empirical Wasserstein metric to regularize invariance (Proposition~\\ref{theory:proposition:l1}) and then we can derive a bounded robust error if all the functions in $\\ma$ are available (Theorem~\\ref{theory:theorem:main}). \n    \\item With the above result and A3, we can derive a bounded robust error if only two special functions in $\\ma$ (which we refer to as \\emph{vertices}) are available (Lemma~\\ref{theory:lemma:rv}).\n\\end{itemize}\n\n"
                },
                "subsection 5.2": {
                    "name": "Assumptions Setup",
                    "content": "\n\n\\label{sec:app:assumption}\n\n",
                    "subsubsection 5.2.1": {
                        "name": "Assumptions on Data Augmentation Functions",
                        "content": "\n\nOur first three assumptions are for the basic properties of the data transformation functions used. \nThese properties are formally introduced as assumptions below. \n\n\\begin{itemize}\n\\item [\\textbf{A1}:] \\textbf{Dependence-preservation:}\n    \\textit{\n    the transformation function will not alter the dependency regarding the label (\\textit{i.e.}, for any $a()\\in \\ma$, $a(\\x)$ will have the same label as $\\x$)\n    or the features (\\textit{i.e.}, $a_1(\\x_1)$ and $a_2(\\x_2)$ are independent if $\\x_1$ and $\\x_2$ are independent).}\n\\end{itemize}\nIntuitively, ``Dependence-preservation'' has two perspectives:  \nLabel-wise, the transformation cannot alter the label of the data, \nwhich is a central requirement of almost all the data augmentation functions in practice. \nFeature-wise, the transformation will not introduce new dependencies between the samples. \n            \nWe consider the label-wise half of this argument as a fundamental property of any data augmentations. It has to be always true for data augmentation to be a useful technique. On the other hand, the feature-wise half of this argument is a fundamental property required to derive the generalization error bounds. Intuitively, we believe this property holds for most data augmentation techniques in practice. \n\n\\begin{itemize}\n\\item [\\textbf{A2}:] \\textbf{Efficiency:} \\textit{ for $\\wt$ and \n    any $a()\\in \\ma$, $\\faxt$ is closer to $\\x$ than any other samples under a distance metric \n    $d_e(\\cdot, \\cdot)$, \\textit{i.e.}, \n    \\begin{align*}\n        d_e(\\faxt, \\fxt)\\leq\\min_{\\x'\\in\\X_{-\\x}}d_e(\\faxt, f(\\x';\\wt)).\n    \\end{align*}\n    We define $d_e(\\cdot, \\cdot)$ to be $\\ell_1$ norm. }\n\\end{itemize}\n\nIntuitively, the efficiency property means the augmentation should only generate new samples of the same label as minor perturbations of the original one.\nIf a transformation violates this property,\nthere should exist other simpler transformations\nthat can generate the same target sample. \n\n\\begin{itemize}\n\\item [\\textbf{A3}:] \\textbf{Vertices:} \\textit{\n    For a model $\\wt$ and a transformation $a()$, we use $\\mP_{a, \\wt}$ to denote the distribution of $\\faxt$ for $(\\x, \\y)\\sim \\mP$. \n    ``Vertices'' argues that exists two extreme elements in $\\ma$, namely $a^+$ and $a^-$, with certain metric $d_x(\\cdot, \\cdot)$, we have\n    \\begin{align*}\n        d_x (\\mP_{a^+, \\wt}, \\mP_{a^-, \\wt}) = \\sup_{a_1, a_2\\in \\ma} d_x (\\mP_{a_1, \\wt}, \\mP_{a_2, \\wt})\n    \\end{align*}\n    We define $d_x(\\cdot, \\cdot)$ to be Wasserstein-1 metric. }\n\\end{itemize}\n        \nIntuitively, ``Vertices'' suggests that there are extreme cases of the transformations. \nFor example, if one needs the model to be invariant to rotations from $0^{\\circ}$ to $60^{\\circ}$,\nwe consider the vertices to be $0^{\\circ}$ rotation function (thus identity map) and $60^{\\circ}$ rotation function. \nIn practice, one usually selects the transformation vertices with intuitions or domain knowledge. \n    \nNotice that we do not need to argue that \\textbf{A3} always holds. \nAll we need is that \\textbf{A3} can sometimes hold, \nand when it holds, we can directly train with the regularized vertex augmentation. Thus, anytime \\ra{} empirically performs well is a favorable argument for A3. \nTo show that \\ra{} can sometimes perform well, \nwe compare the \\ra{} with vanilla (non-regularized) worst-case data augmentation (\\vwa{}) method across our synthetic experiment setup. \nWe notice that \nout of six total scenarios (\\{texture, rotation, contrast\\} $\\times$ \\{MNIST, CIFAR10\\}), \\ra{} outperforms \\vwa{} frequently. This suggests that the domain-knowledge of vertices can help in most cases, although not guaranteed in every case. \n\n\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "Assumptions on Background and Generalization Error Bound",
                        "content": "\n\nIn an abstract manner, when the test data and train data are from the same distribution, \nseveral previous analyses on the generalization error can be sketched as (see examples in \\textbf{A4}):\n\\begin{align}\n    \\rp(\\wt) \\leq \\wrp(\\wt) + \\phi(|\\Theta|, n, \\delta)\n    \\label{eq:standard}\n\\end{align}\nwhich suggests that the expected risk $\\rp(\\wt)$ can be bounded by the empirical risk $\\wrp(\\wt)$\nand a function of hypothesis space $|\\Theta|$ and number of samples $n$; \n$\\delta$ accounts for the probability when the bound holds. \n$\\phi()$ is a function of these three terms. \nDependent on the details of different analyses, \ndifferent concrete examples of this generic term will need different assumptions. \n%For the convenience of further discussions, \nWe use a generic assumption \\textbf{A4} to denote \nthe assumptions required for each example. \n\nFollowing our main goal to study \nhow alignment regularization and data augmentation\nhelp in \naccuracy, robustness, and invariance, \nour strategy in theoretical analysis \nis to derive error bounds for accuracy and robustness,\nand the error bound directly contains terms to \nregularize the invariance. \nFurther, as robustness naturally bounds accuracy\n(\\textit{i.e.}, $\\rp(\\wt)\\leq\\rpa(\\wt)$ \nfollowing the definitions in \\eqref{eq:accuracy} and \\eqref{eq:robustness} respectively), \nwe only need to study the robust error. \n\nTo study the robust error, we need two additional technical assumptions. \n\\textbf{A5} connects the distribution of expected robust risk \nand the distribution of empirical robust risk, \nand \\textbf{A6} connects the 0-1 classification error and cross-entropy error. \n\n\\begin{itemize}\n\\item [\\textbf{A4}:] \\textit{We list two examples here: \n\\begin{itemize}\n    \\item when \\textbf{A4} is ``$\\Theta$ is finite, $l(\\cdot, \\cdot)$ is a zero-one loss, samples are \\textit{i.i.d}'',  $\\phi(|\\Theta|, n, \\delta)=\\sqrt{(\\log(|\\Theta|) + \\log(1/\\delta))/2n}$\n    \\item when \\textbf{A4} is ``samples are \\textit{i.i.d}'', $\\phi(|\\Theta|, n, \\delta) = 2\\mathcal{R}(\\mathcal{L}) + \\sqrt{(\\log{1/\\delta})/2n}$, where $\\mathcal{R}(\\mathcal{L})$ stands for Rademacher complexity and $\\mathcal{L} = \\{l_\\theta \\,|\\, \\theta \\in \\Theta \\}$, where $l_\\theta$ is the loss function corresponding to $\\theta$. \n\\end{itemize} }\n\\end{itemize}\nFor more information or more concrete examples of the generic term, \none can refer to relevant textbooks such as \\citep{bousquet2003introduction}. \n\nA4 stands for the fundamental assumptions used to derive standard generalization bounds. We rely on this assumption as how previous theoretical works rely on them. \n\n\\begin{itemize}\n\\item[\\textbf{A5}:] \\textit{the distribution for expected robust risk \nequals the distribution for \nempirical robust risk, \\textit{i.e.}, \n\\begin{align*}\n    \\argmax_{\\mathcal{P}'\\in T(\\mP, \\ma)} \\rpp(\\wt) \n    = \\argmax_{\\mathcal{P'}\\in T(\\mP, \\ma)}\\wrpp(\\wt) \n\\end{align*}\nwhere $T(\\mP, \\ma)$ is the collection of distributions created by elements in $\\ma$ over samples from $\\mP$. }\n\\end{itemize}\nEq. \\eqref{eq:robustness} can be written equivalently into the expected risk \nover a pseudo distribution $\\mP'$ (see Lemma 1 in \\citep{tu2019theoretical}), \nwhich is the distribution that can sample the data leading to the expected robust risk. \nThus, equivalently, we can consider \n$\\sup_{\\mathcal{P}'\\in T(\\mP, \\ma)} \\rpp(\\wt)$ as a surrogate of $\\rpa(\\wt)$, where $T(\\mP, \\ma)$ denotes the set of possible resulting distributions. \nFollowing the empirical strength of techniques such as adversarial training \\citep{MadryMSTV18}, \nwe introduce an assumption relating the distribution of expected robust risk \nand the distribution of empirical robust risk \n(namely, \\textbf{A5}, in  Appendix~\\ref{sec:app:assumption}).\n% we can analyze the generalization behavior with standard techniques. \nThus, \nthe bound of our interest (\\textit{i.e.}, $\\sup_{\\mP'\\in T(\\mP, \\ma)}\\rpp(\\wt)$) \ncan be analogously analyzed through $\\sup_{\\mP'\\in T(\\mP, \\ma)}\\wrpp(\\wt)$. \n    \n\\textbf{A5} is likely to hold in practice: Assumption \\textbf{A5} appears very strong, \nhowever, the successes of methods like adversarial training \\citep{MadryMSTV18} \nsuggest that, in practice, \n\\textbf{A5} might be much weaker \nthan it appears. \n\n\\begin{itemize}\n    \\item [\\textbf{A6}:] \\textit{ With $(\\x,\\y) \\in (\\X,\\Y)$, the sample maximizing cross-entropy loss and the sample maximizing classification error for model $\\wt$ follows: \n    \\begin{align}\n        \\forall \\x, \\quad \\dfrac{\\y^\\top \\fxt}{\\inf_{a\\in \\ma}\\y^\\top \\faxt} \\geq \\exp\\big(\\mathbb{I}(g(\\fxt)\\neq g(f(\\x';\\wt)))\\big)\n        \\label{eq:a2}\n    \\end{align}\n    where $\\x'$ stands for the sample maximizing classification error, \\textit{i.e.}, \n    \\begin{align*}\n        \\x' = \\argmin_\\x \\y^\\top g(\\fxt)\n    \\end{align*}\n    Also, \n    \\begin{align}\n        \\forall \\x, \\quad \\vert \\inf_{a\\in \\ma}\\y^\\top \\faxt \\vert \\geq 1\n    \\label{eq:assum:lipschitz}\n    \\end{align} }\n  \\end{itemize}  \nIntuitively, although Assumption \\textbf{A6} appears complicated, it describes the situations of two scenarios: \n\nIf $g(\\fxt)=g(f(\\x';\\wt))$, which means either the sample is misclassified by $\\wt$ or $\\ma$ is not rich enough for a transformation function to alter the prediction, the RHS of Eq.~\\ref{eq:a2} is 1, thus Eq.~\\ref{eq:a2} always holds (because $\\ma$ has the identity map as one of its elements). \n\nIf $g(\\fxt)\\neq g(f(\\x';\\wt))$, which means a transformation alters the prediction. In this case, \\textbf{A6} intuitively states that the $\\ma$ is reasonably rich and the transformation is reasonably powerful to create a gap of the probability for the correct class between the original sample and the transformed sample. The ratio is described as the ratio of the prediction confidence from the original sample over the prediction confidence from the transformed sample is greater than $e$.\n        \n% \\item Validation: We check Assumption \\textbf{A6} by directly \n% calculating the frequencies out of all the samples \n% when it holds. \n% Given a vanilla model (\\base{}), we notice that over 74\\% samples out of 50000 samples fit this assumption. \n% Thus, we consider it reasonable to make this assumption. \n    \n\n\n"
                    }
                },
                "subsection 5.3": {
                    "name": "Analytical Support",
                    "content": "\n\n\\paragraph{Regularized Worst-case Augmentation}\n% \\label{sec:method:rwa}\n\nTo have a model with a small invariance score, we should probably directly regularize the empirical counterpart of Eq.~\\eqref{eq:invariance}. \nHowever, Wasserstein distance is difficult to calculate \nin practice. \nFortunately, \nProposition~\\ref{theory:proposition:l1}\nconveniently allows us to use $\\ell_1$ norm to replace Wasserstein metric. \n%integrating the advantages of Wasserstein metric while avoiding the practical challenges of it.\nWith Proposition~\\ref{theory:proposition:l1}, now\nwe can offer our main technical result to study the robust error $\\rpa{\\wt}$ (as defined in Eq.~\\eqref{eq:robustness}).\n\\begin{theorem}\nWith Assumptions A1, A2, A4, A5, and A6, with probability at least $1-\\delta$, \n%the worst case generalization risk will be bounded as\nwe have\n\\begin{align*}\n    \\rpa{\\wt}  \\leq\n    \\wrp (\\wt) + \n    \\sum_{i}||f(\\x_i;\\wt) - f(\\x'_i;\\wt)||_1 + \\phi(|\\Theta|, n, \\delta)\n\\end{align*}\nand %$\\x'$ is the worst-case transformed sample, \\textit{i.e.}, \n$\\x' = a(\\x) $, where $ a = \\argmin_{a \\in \\ma} \\y^\\top \\faxt$.\n$\\phi(|\\Theta|, n, \\delta)$ is defined in A4. \n\\label{theory:theorem:main}\n\\end{theorem}\n\nThis technical result immediately inspires the method to guarantee worst case performance, \nas well as to explicitly enforce the concept of invariance. \nThe method \n$a = \\argmin_{a \\in \\ma} \\y^\\top \\faxt$ is selecting the transformation function \nmaximizing the cross-entropy loss \n(notice the sign difference between here and the cross-entropy loss), \nwhich we refer to as worst-case data augmentation.\nThis method is also closely connected to adversarial training \\citep[\\textit{e.g.},][]{MadryMSTV18}. \n\n\\paragraph{Regularized Vertex Augmentation}\n% \\label{sec:method:rv}\n\nAs $\\ma$ in practice usually has a large number of\n(and possibly infinite) elements, we may not always be able to identify \nthe worst-case transformation function with reasonable computational efforts. \n% This limitation also prevents us from effective estimating the generalization error\n% as the bound requires the identification of the worst case transformation. \nWe further leverage the vertex property (boundary cases of transformation functions, discussed as Assumption A3 in the appendix) of the transformation function \nto bound the worst-case generalization error:\n\\begin{lemma}\nWith Assumptions A1-A6, \n% and $d_e(\\cdot, \\cdot)$ in A2 chosen as $\\ell_1$ norm distance, \n% $d_x(\\cdot, \\cdot)$ in A3 chosen as Wasserstein-1 metric,\nassuming there is a $a'()\\in \\ma$ where $ \\widehat{r}_{\\mP_{a'}}(\\wt)=\\frac{1}{2}\\big(\\widehat{r}_{\\mP_{a^+}}(\\wt)+\\widehat{r}_{\\mP_{a^-}}(\\wt)\\big)$,\nwith probability at least $1-\\delta$, we have:\n\\begin{align*}\n    \\rpa(\\wt) \\leq &\n    \\dfrac{1}{2}\\big(\\widehat{r}_{\\mP_{a^+}}(\\wt) + \\widehat{r}_{\\mP_{a^-}}(\\wt)\\big) \\\\\n    &+ \\sum_{i}||f(a^+(\\x_i);\\wt) - f(a^-(\\x');\\wt)||_1 \n    + \\phi(|\\Theta|, n, \\delta), \n\\end{align*}\nwhere $a^+()$ and $a^-()$ are defined in A3, \nand $\\phi(|\\Theta|, n, \\delta)$ in A4.\n\\label{theory:lemma:rv}\n\\end{lemma}\n\nThis result corresponds to the method that \n%directly guarantees \n%the worst case generalization result and \ncan be optimized conveniently \nwithout searching for the worst-case transformations. \nHowever, the method requires good domain knowledge of the vertices \n(\\textit{i.e.}, boundary cases)\nof the transformation functions. \n\n\nThus, our theoretical discussions have complemented \nour empirical findings in Section~\\ref{sec:motivate}\nby showing that norm-based regularizations \ncan lead to bounded robust error. \nThere is a disparity that \nour analytical result is about \n$\\ell_1$ norm\nwhile our empirical study suggests \nsquared \\ltwo{} norm. \n% with squared \\ltwo{} norm being slightly advantageous. \nWe conjecture the disparity is mainly caused \nby the difficulty in passing the gradient \nof $\\ell_1$ norm in practice. \n\n% \\subsection{Engineering Specification of Relevant Methods}\n% Our theoretical analysis has lead to a line of methods, \n% however, not every method can be effectively implemented,\n% especially due to the difficulties of \n% passing gradient back for optimizations. \n% Therefore, to boost the influence of the loss function through backpropagation, \n% we recommend to adapt the methods with the following two changes: \n% 1) the regularization is enforced on logits instead of softmax; \n% 2) we use squared $\\ell_2$ norm instead of $\\ell_1$ norm \n% because $\\ell_1$ norm is not differentiable everywhere. \n% We discuss the effects of these compromises in ablation studies in  Appendix~\\ref{sec:app:more}. \n\n% Also, in the cases where we need to identify the worst case transformation functions,\n% we iterate through all the transformation functions and identify the function with the maximum loss. \n\n% Overall, our analysis leads to the following main training strategies: \n% \\begin{itemize}\n%     \\item \\va{} (vanilla augmentation): mix the augmented samples of a vertex function to the original ones for training (original samples are considered as from another vertex in following experiments). \n%     \\item \\vwa{} (vanilla worst-case augmentation): at each iteration, identify the worst-case transformation functions and train with samples generated by them (also known as adversarial training). \n%     \\item \\ra{} (regularized augmentation): regularizing the squared $\\ell_2$ distance over logits between the original samples and the augmented samples of a fixed vertex transformation function. \n%     \\item \\rwa{} (regularized worst-case augmentation): regularizing the squared $\\ell_2$ distance over logits between the original samples and the worst-case augmented samples identified at each iteration. \n%     % \\item \\rbwa{} (regularized best/worst-case augmentation): regularizing the norm distance between the best-case augmented samples and the worst-case augmented samples identified at each iteration, where the best-case are defined as the augmented copies leading to the minimum losses. {\\color{blue} we might need to delete this method to save some space.}\n% \\end{itemize}\n\n"
                }
            },
            "section 6": {
                "name": "Experiments with Advanced Methods",
                "content": "\n\\label{sec:exp}\n\nWe continue to test the methods we identified \nin comparison to more advanced methods. \nAlthough we argued for the value of invariance, \nfor a fair comparison, we will \ntest the performances evaluated by the metrics\nthe previous methods are designed for. \nOur method will use the same generic approach\nand the same transformation functions as in the previous\nempirical study, \nalthough these functions are not necessarily part of the  \ndistribution shift we test now. \nIn summary, \nour method can outperform (or be on par with) these SOTA techniques \nin the robustness metric they are designed for (Section~\\ref{sec:exp:robust}). \nIn addition, \nwe run a side test to show that \nour method can also \nimprove accuracy (Section~\\ref{sec:exp:accuracy}). \n\n",
                "subsection 6.1": {
                    "name": "Methods",
                    "content": "\n\nSection~\\ref{sec:motivate} and Section~\\ref{sec:method} lead us to test the following two methods: \n\\begin{itemize}\n    \\item \\ra{} (regularized vertex augmentation): using squared $\\ell_2$ norm as AR over logits between the original samples and the augmented samples of a fixed vertex transformation function (original samples are considered as from another vertex). \n    \\item \\rwa{} (regularized worst-case augmentation): using squared $\\ell_2$ norm as AR over logits between the original samples and the worst-case augmented samples identified at each iteration. Worst-case samples are generated by the function with the maximum loss when we iterate through all the transformation functions.\n\\end{itemize}\n\n\n"
                },
                "subsection 6.2": {
                    "name": "Robustness",
                    "content": "\n\\label{sec:exp:robust}\n\n\\paragraph{Rotation}\nWe compare our results with rotation-invariant models,\nmainly Spatial Transformer (\\textsf{ST}) \\citep{jaderberg2015spatial},\nGroup Convolution (\\textsf{GC}) \\citep{cohen2016group}, \nand Equivariant Transformer Network (\\textsf{ETN}) \\citep{tai2019equivariant}. \nWe also tried to run CGNet \\citep{kondor2018clebsch}, \nbut the method does not seem to scale to the CIFAR10 and ResNet level. \nAll these methods are tested with ResNet34 following popular settings in the community. \nThe results are in Table~\\ref{tab:real:rotation}. \nWe test the models every $15^{\\circ}$ rotation from $0^{\\circ}$  rotation to $345^{\\circ}$ rotation. \nAugmentation-related methods use the $\\ma$ of ``rotation''\nin synthetic experiments, \nso the testing scenario goes beyond what the augmentation methods have seen during training. \n\nWe report two summary results in Table~\\ref{tab:real:rotation}. \n``\\textsf{main}'' means the average prediction accuracy from images rotated from $300^{\\circ}$ to $60^{\\circ}$ (passing $0^{\\circ}$), \nwhen the resulting images are highly likely to preserve the class label. ``\\textsf{all}'' means the average accuracy of all rotations. \n\nOur results can be interpreted from two perspectives. \nFirst, by comparing all the columns in the first panel \nto the first column of the other three panels, \ndata augmentation and AR \ncan boost a vanilla model \nto outperform other advanced techniques. \nOn the other hand, \nby comparing the columns within each panel, \ndata augmentation and AR \ncan further improve the performances of these techniques. \n\nInterestingly, the baseline model \nwith our generic approach (\\rwa{} in the first panel)\ncan almost compete \nwith the advanced methods even when these methods also use augmentation \nand AR (\\rwa{} in \\textsf{GC} panel). \nWe believe this result strongly indicates \nthe potential of this simple augmentation and regularization method to match the advanced methods.\n%considering the margin improved by the method.  \n\nIn summary, \\rwa{} can boost\nthe vanilla model to outperform advanced methods. \nData augmentation and squared $\\ell_2$ AR can further improve the performances\nwhen plugged onto advanced methods. \n% The detailed results of each rotation are reported in Table~\\ref{tab:real:rotation:app} in Appendix.  \n\n\n\\paragraph{Texture \\& Contrast}\nWe follow \\citep{bahng2019learning} \nand compare the models for a nine super-class ImageNet classification \\citep{ilyas2019adversarial} with class-balanced strategies. \nAlso, we follow \\citep{bahng2019learning}\nto report standard accuracy (Acc.), \nweighted accuracy (WAcc.), a scenario where\nsamples with unusual texture are weighted more, \nand accuracy over ImageNet-A \\citep{hendrycks2019natural},\na collection of \nfailure cases for most ImageNet trained models. \nAdditionally, we also report \nthe performance over ImageNet-Sketch \\citep{wang2019learning}, \nan independently collected ImageNet test set\nwith only sketch images. \nAs \\citep{bahng2019learning} mainly aims to overcome \nthe texture bias, \nwe also use our texture-wise functions \nin Section~\\ref{sec:motivate} for augmentation. \nHowever, there are no direct connections between \nthese functions and the distribution shift\nof the test samples. \nAlso, we believe the distribution shifts here, \nespecially the one introduced by our newly added ImageNet-Sketch, \nare more than texture,\nand also correspond to the contrast case of our study. \n\nFollowing \\citep{bahng2019learning}, \nthe base network is ResNet, \nand we compare with the vanilla network (\\textsf{Base}), \nand several methods designed for this task:\nincluding \nStylisedIN (\\textsf{SIN}) \\citep{geirhos2018imagenettrained}, \nLearnedMixin (\\textsf{LM}) \\citep{clark2019don}, \nRUBi (\\textsf{RUBi}) \\citep{cadene2019rubi}\nand ReBias (\\textsf{RB}) \\citep{bahng2019learning}. \nResults are in Table~\\ref{tab:real:texture}. \n\nThe results favor our generic method in most cases. \n\\ra{} outperforms other methods in standard accuracy, weighted accuracy, and ImageNet-Sketch, and is shy from ReBias on ImageNet-A. \n\\rwa{} shows the same pattern as that of \\ra{} and further outperforms \\ra{}. \nOverall, these results validate the empirical strength of\ndata augmentation (even when the augmentation is not designed for the task) and squared $\\ell_2$ norm AR for learning robust models. \n\n"
                },
                "subsection 6.3": {
                    "name": "Accuracy",
                    "content": "\n\\label{sec:exp:accuracy}\n\nFurther, \nthese experiments help us notice that\nthe generic technique can also help improve the accuracy, \nalthough the technique is motivated by robustness and invariance.\nTherefore, \nwe follow the widely accepted \\href{https://github.com/weiaicunzai/pytorch-cifar100}{CIFAR100 test pipeline}\nand test the performances of different architectures\nof the ResNet family. \nThe results are reported in Table~\\ref{tab:accuracy}, \nwhere \\textsf{Base} stands for the baseline model with the default accuracy boosting configurations. \n\nFor both top-1 and top-5 accuracies and across the three ResNet architectures, \nour techniques can help improve the accuracy. \nIn addition, \nwe notice that \nour techniques can help bridge the gap of \ndifferent architectures within the ResNet family:\nfor example, \\rwa{} helps ResNet50 to outperform \nthe vanilla ResNet101. \n\n\n\n"
                }
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\n\\label{sec:con}\n% Data augmentation has benefited the \n% development of machine learning models substantially. \n% Given its widely usage, \n% in this paper, \n% we seek to answer that \n% how to train with augmented data so that\n% the assistance of augmentation can be taken to the fullest extent. \n% To answer this, \n% we first defined another dimension called invariance \n% and conducted a line of empirical study \n% to show that norm-based consistency loss \n% can help learn robust and invariant models. \n% Further, we complement our observations with formal derivations\n% with bounded generalization errors. \n% With progressively more specific assumptions, \n% we identified progressively simpler methods that can bound the worst case risk. We summarize the main take-home messages below:\n\n% \\begin{itemize}\n%     \\item Regularizing a norm distance between the logits of the originals samples and the logits of the augmented samples enjoys several merits:\n%     the trained model tend to have good worst case performance, and can learn the concept of invariance. \n%     Although our theory suggests $\\ell_1$ norm, but we recommend squared $\\ell_2$ norm in practice considering the difficulties of passing the gradient of $\\ell_1$ norm in backpropagation. \n%     \\item With the vertex assumption held (it usually requires domain knowledge to choose the vertex functions), one can use ``regularized training with vertices'' (\\ra{}) method and get good empirical performance in both accuracy and invariance, and the method is at the same complexity order of vanilla training without data augmentation. \n%     When we do not have the domain knowledge (thus are not confident in the vertex assumption), \n%     we recommend ``regularized worst-case augmentation'' (\\rwa{}), \n%     which has the best performance across most cases, but requires extra computations to identify the worst-case augmented samples at each iteration. \n% \\end{itemize}\n\nIn this paper, we seek to answer how to train with augmented data so that augmentation can be taken to the fullest extent. We first defined a new evaluation metric called invariance and conducted a line of empirical studies to show that norm-based alignment regularization can help learn robust and invariant models. \nFurther, we complement our observations with formal derivations\nof bounded generalization errors. \nWe notice that \nregularizing squared $\\ell_2$ norm between the logits of the originals samples and those of the augmented samples is favorable:\nthe trained model tends to have the most favorable performances in robust accuracy and invariance. \nIn general, the method we recommend is \n``regularized worst-case augmentation'' with squared $\\ell_2$ norm as the alignment regularization. \nOne can also consider ``regularized vertex augmentation'' when extra assumptions\non the vertex properties of the transformation functions are met. \nLastly, we would like to remind a potential limitation of alignment regularization: \nit may not always help improve the \\textit{i.i.d} accuracy due to the tradeoff between accuracy and robustness or invariance. \nIn addition, \nto simplify the procedure of users in leveraging our contribution, \nwe also release a software package \nin both \\textsf{TensorFlow} and \\textsf{PyTorch} \nfor users to use our identified methods \nwith a couple lines of code.\n\n% We summarize the main take-home messages below:\n% \\begin{itemize}\n%     \\item Regularizing squared $\\ell_2$ norm between the logits of the originals samples and the logits of the augmented samples enjoys several merits:\n%     the trained model tend to have favorable performances in robust accuracy and invariance. \n%     \\item With the vertex assumption held (it usually requires domain knowledge to choose the vertex functions), one can use ``regularized training with vertices'' (\\ra{}) method and get good empirical performance in both accuracy and invariance, and the method is at the same complexity order of vanilla training without data augmentation. \n%     When we do not have the domain knowledge (thus are not confident in the vertex assumption), \n%     we recommend ``regularized worst-case augmentation'' (\\rwa{}), \n%     which has the best performance across most cases, but requires extra computations to identify the worst-case augmented samples at each iteration. \n% \\end{itemize}\n\n\n% {\\color{red} todos: \n% to make sure the writing is consistent in semantics and desired signals\n% proofread the writings}\n\n\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\\begin{acks}\nThis work was supported by NIH R01GM114311, NIH P30DA035778, and NSF IIS1617583; NSF CAREER IIS-2150012 and IIS-2204808. \nThe authors would like to thank Hanru Yan for the implementation of the software package. \n\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{ref}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\n\\clearpage \n\\appendix\n\n% \\onecolumn\n\\newpage\n% \\appendix\n\n"
            },
            "section 8": {
                "name": "Appendices",
                "content": "\n\n% \\section{Validations of the Assumptions}\n% \\label{sec:app:assumption}\n\n% \\subsection{Assumptions on Data Augmentation Functions}\n\n% Validation of A2: We test the assumption with MNIST data and rotation experiment.  \n% A2 essentially states the distance $d_e(\\cdot, \\cdot)$ \n% is the smaller between a sample and its augmented copy (60$^{\\circ}$ rotation)\n% than the sample and the augmented copy from any other samples. \n% We take 1000 training examples and calculate the $\\ell_1$ pair-wise distances\n% between the samples and its augmented copies, \n% then we calculated the frequencies\n% when the A2 hold for one example. \n% We repeat this for three different models, \n% the vanilla model, \n% the model trained with augmented data, \n% and the model trained with regularized adversarial training. \n% The results are shown in the Table~\\ref{tab:assumption:a2} \n% and suggest that, \n% although the A2 does not hold in general, \n% it holds for regularized adversarial training case, \n% where A2 is used. \n% Further, we test the assumption in a more challenging case, \n% where half of the training samples \n% are 15$^{\\circ}$ rotations of the other half, \n% thus we may expect the A2 violated for every sample. \n% Finally, as A2 is essentially introduced \n% to replace the empirical Wasserstein distance \n% with $\\ell_1$ distances of the samples and the augmented copies, \n% we directly compare these metrics. \n% However, as the empirical Wasserstein distance is \n% forbiddingly hard to calculate (as it involves permutation statistics), we use a greedy heuristic to calculate\n% by iteratively picking the nearest neighbor of a sample and then remove the neighbor from the pool for the next sample. \n% Our inspection suggests that, \n% even in the challenging scenario, \n% the paired distance is a reasonably good representative of \n% Wasserstein distance for \n% regularized adversarial training method. \n\n% \\begin{table}[h]\n% \\small\n% \\centering \n% \\caption{Empirical results from synthetic data to validate Assumption A2.}\n% \\begin{tabular}{c|ccc|ccc}\n% \\hline\n%  & \\multicolumn{3}{c}{Standard Scenario} & \\multicolumn{3}{|c}{Challenging Scenario} \\\\\n%  & Vanilla & Augmented & Regularized & Vanilla & Augmented & Regularized \\\\ \\hline\n% Frequency when A2 holds & 0.005 & 0.152 & 0.999 & 0.001 & 0.021 & 0.711 \\\\ \\hline\n% Paired Distance & 217968.06 & 42236.75 & 1084.4 & 66058.4 & 28122.45 & 4287.31 \\\\\n% Wasserstein (greedy) & 152736.47 & 38117.77 & 1084.4 & 37156.5 & 20886.7 & 4218.53 \\\\\n% Paired/Wasserstein & 1.42 & 1.10 & 1 & 1.77 & 1.34 & 1.02 \\\\ \\hline\n% \\end{tabular}\n% \\label{tab:assumption:a2}\n% \\end{table}\n\n% \\begin{table}[h]\n% \\small \n% \\centering \n% \\caption{Results to show that \\textbf{A3} can sometimes hold (we do not need to show \\textbf{A3} always holds.)}\n% \\begin{tabular}{cc|ccc|ccc|ccc}\n% \\hline\n%  &  & \\multicolumn{3}{c|}{Texture} & \\multicolumn{3}{c|}{Rotation} & \\multicolumn{3}{c}{Invariance} \\\\\n%  &  & Acc. & Rob. & Inv. & Acc. & Rob. & Inv. & Acc. & Rob. & Inv. \\\\ \\hline\n% \\multirow{2}{*}{MNIST} & \\ra{} & 99.1 & 99 & 100 & 99.3 & 95.1 & 65.4 & 99.4 & 97.4 & 41.3 \\\\\n%  & \\vwa{} & 99.2 & 99.3 & 99.4 & 94.4 & 94.1 & 62.8 & 98.2 & 98.4 & 40.2 \\\\ \\hline\n% \\multirow{2}{*}{CIFAR10} & \\ra{} & 63.5 & 62.2 & 100 & 75.9 & 47.7 & 67.8 & 76.7 & 69.3 & 57.4 \\\\\n%  & \\vwa{} & 60.5 & 58.1 & 100 & 71.4 & 61.7 & 91.7 & 74.8 & 63.2 & 34.3 \\\\ \\hline\n% \\end{tabular}\n% \\label{tab:app:a3}\n% \\end{table}\n\n\n% Validation of A6: We check Assumption \\textbf{A6} by directly \n% calculating the frequencies out of all the samples \n% when it holds. \n% Given a vanilla model (\\base{}), we notice that over 74\\% samples out of 50000 samples fit this assumption. \n% Thus, we consider it reasonable to make this assumption. \n\n"
            },
            "section 9": {
                "name": "Proof of Theoretical Results",
                "content": "\n\n",
                "subsection 9.1": {
                    "name": "Lemma A.1 and its proof",
                    "content": "\n\\begin{lemma}\nWith Assumptions A1, A4, and A5, with probability at least $1-\\delta$, we have %the worst case generalization error as \n\\begin{align*}\n    \\rpa(\\wt)  \\leq \n    \\dfrac{1}{n}\\sum_{(\\x, \\y) \\sim \\mP}\\sup_{a\\in \\ma}\\mI(g(\\faxt) \\neq \\y)  + \\phi(|\\Theta|, n, \\delta)\n\\end{align*}\n\\end{lemma}\n\n\\begin{proof}\nWith Assumption A5, we have\n\\begin{align*}\n    \\argmax_{\\mP'\\in T(\\mP, \\ma)} \\rpp(\\wt) \n    = \\argmax_{\\mP'\\in T(\\mP, \\ma)}\\wrpp(\\wt) = \\mP_w\n\\end{align*}\nwe can analyze the expected risk following the standard classical techniques since both expected risk and empirical risk are studied over distribution $\\mP_w$. \n\nNow we only need to make sure the classical analyses (as discussed in A4) are still valid over distribution $\\mP_w$:\n\\begin{itemize}\n    \\item when \\textbf{A4} is ``$\\Theta$ is finite, $l(\\cdot, \\cdot)$ is a zero-one loss, samples are \\textit{i.i.d}'',  $\\phi(|\\Theta|, n, \\delta)=\\sqrt{\\dfrac{\\log(|\\Theta|) + \\log(1/\\delta)}{2n}}$. \n    The proof of this result uses Hoeffding's inequality, which only requires independence of random variables. One can refer to Section 3.6 in \\cite{liang2016cs229t} for the detailed proof. \n    \\item when \\textbf{A4} is ``samples are \\textit{i.i.d}'', $\\phi(|\\Theta|, n, \\delta) = 2\\mathcal{R}(\\mathcal{L}) + \\sqrt{\\dfrac{\\log{1/\\delta}}{2n}}$. \n    The proof of this result relies on McDiarmid's inequality, which also only requires independence of random variables. One can refer to Section 3.8 in \\cite{liang2016cs229t} for the detailed proof. \n\\end{itemize}\nAssumption \\textbf{A1} guarantees the samples from distribution $\\mP_w$ are still independent, thus the generic term holds for at least these two concrete examples, thus the claim is proved. \n\n\\end{proof}\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Proposition A.2 and Proof",
                    "content": "\n\\begin{proposition}\nWith A2, \n%and $d_e(\\cdot,\\cdot)$ in A2 chosen to be $\\ell_1$ norm, \nfor any $a\\in \\ma$, we have\n\\begin{align*}\n    W_1(\\widehat{\\mathbf{Q}}_{\\x, \\wt}, \\widehat{\\mathbf{Q}}_{a(\\x), \\wt})=\\sum_{i}^{|(\\X, \\Y)|} ||\\fxit-\\faxit||_1, \n\\end{align*}\nwhere $\\widehat{\\mathbf{Q}}_{\\x, \\wt}$ denotes the empirical \ndistribution of $\\fxt$ for $(\\x, \\y)\\in (\\X, \\Y)$. \n\\label{theory:proposition:l1}\n\\end{proposition}\n\\begin{proof}\nWe use the order statistics representation of Wasserstein metric over empirical distributions (\\textit{e.g.}, see Section 4 in \\cite{bobkov2019one})\n\\begin{align*}\n    W_1(\\widehat{\\mathbf{Q}}_{\\x, \\wt}, \\widehat{\\mathbf{Q}}_{a(\\x), \\wt})) = \\inf_{\\sigma}\\sum_{i}^{|(\\X, \\Y)|}||\\fxit - f(a(\\x_{\\sigma(i)}), \\wt)||_1\n\\end{align*}\nwhere $\\sigma$ stands for a permutation of the index, thus the infimum is taken over all possible permutations.\nWith Assumption A2, when $d_e(\\cdot,\\cdot)$ in A2 chosen to be $\\ell_1$ norm, we have: \n\\begin{align*}\n    ||\\fxit - \\faxit||_1 \\leq \\min_{j\\neq i} ||\\fxit - f(a(\\x_{j}), \\wt)||_1\n\\end{align*}\nThus, the infimum is taken when $\\sigma$ is the natural order of the samples, which leads to the claim. \n\\end{proof}\n\n"
                },
                "subsection 9.3": {
                    "name": "Proof of Theorem 5.1",
                    "content": "\n\\begin{theorem*}\nWith Assumptions A1, A2, A4, A5, and A6, with probability at least $1-\\delta$, we have\n\\begin{align}\n    \\rpa(\\wt) \\leq \n    \\wrp (\\wt) + \n    \\sum_{i}||f(\\x_i;\\wt) - f(\\x'_i;\\wt)||_1 + \n    \\phi(|\\Theta|, n, \\delta)\n\\end{align}\nand %$\\x'$ is the worst-case transformed sample, \\textit{i.e.}, \n$\\x' = a(\\x) $, where $ a = \\argmin_{a \\in \\ma} \\y^\\top \\faxt$.\n\\end{theorem*}\n\n\\begin{proof}\n% The key ingredient is to leverage the vertex property of the transformation function and bound \n% \\begin{align*}\n%     \\sup_{\\mP'\\in T(\\mP, \\ma)}\n%     \\wrpp(\\wt)\n% \\end{align*}\n\nFirst of all, in the context of multiclass classification, where $g(f(\\x, ;\\theta))$ predicts a label with one-hot representation, and $\\y$ is also represented with one-hot representation, we can have the empirical risk written as:\n\\begin{align*}\n    \\wrp(\\wt) = 1 -\n    \\dfrac{1}{n}\\sum_{(\\x, \\y) \\sim \\mP}\\y^\\top g(\\fxt)\n\\end{align*}\nThus, \n\\begin{align*}\n    \\sup_{\\mP'\\in T(\\mP, \\ma)}\n    \\wrpp(\\wt) \n    = & \\wrp(\\wt) + \n    \\sup_{\\mP'\\in T(\\mP, \\ma)}\n    \\wrpp(\\wt) -\n    \\wrp(\\wt) \\\\\n    = & \\wrp(\\wt) + \\dfrac{1}{n}\\sup_{\\mP'\\in T(\\mP, \\ma)}\\big(\\sum_{(\\x, \\y) \\sim \\mP} \\y^\\top g(\\fxt) \\\\\n     & - \\sum_{(\\x, \\y) \\sim \\mP'}\\y^\\top g(\\fxt)\\big)\n\\end{align*}\n\nWith A6, we can continue with: \n\\begin{align*}\n    \\sup_{\\mP'\\in T(\\mP, \\ma)}\n    \\wrpp(\\wt) \n    \\leq & \\wrp(\\wt)\n    + \\dfrac{1}{n}\\sup_{\\mP'\\in T(\\mP, \\ma)}\\big(\\sum_{(\\x, \\y) \\sim \\mP} \\y^\\top \\log(\\fxt) \\\\ &- \\sum_{(\\x, \\y) \\sim \\mP'}\\y^\\top \\log(\\fxt)\\big)\n\\end{align*}\nIf we use $e(\\cdot)=-\\y^\\top \\log(\\cdot)$ to replace the cross-entropy loss, we simply have:\n\\begin{align*}\n    \\sup_{\\mP'\\in T(\\mP, \\ma)}\n    \\wrpp(\\wt) \n    \\leq & \\wrp(\\wt)\n    + \\dfrac{1}{n}\\sup_{\\mP'\\in T(\\mP, \\ma)}\\big(\\sum_{(\\x, \\y) \\sim \\mP'} e(\\fxt) \\\\ &- \\sum_{(\\x, \\y) \\sim \\mP}e((\\fxt)\\big)\n\\end{align*}\nSince $e(\\cdot)$ is a Lipschitz function with constant $\\leq 1$ (because of A6, Eq.\\eqref{eq:assum:lipschitz}) and together with the dual representation of Wasserstein metric (See \\textit{e.g.}, \\cite{villani2003topics}), \nwe have\n\\begin{align*}\n    \\sup_{\\mP'\\in T(\\mP, \\ma)}\n    \\wrpp(\\wt) \n    & \\leq \\wrp(\\wt)\n    +  W_1(\\widehat{\\mathbf{Q}}_{\\x, \\wt}, \\widehat{\\mathbf{Q}}_{a(\\x), \\wt}))\n\\end{align*}\nwhere $\\x' = a(\\x) $, where $ a = \\argmin_{a \\in \\ma} \\y^\\top \\faxt$; $\\widehat{\\mathbf{Q}}_{\\x, \\wt}$ denotes the empirical \ndistribution of $\\faxt$ for $(\\x, \\y)\\in (\\X, \\Y)$.\nNote that $\\rpa(\\wt)$, by definition, is a shorthand notation for $$\\sup_{\\mP'\\in T(\\mP, \\ma)}\\rpp(\\wt)$$.\n\nFurther, we can use the help of Proposition B.2 to replace Wassertein metric with $\\ell_1$ distance. \nFinally, we can conclude the proof with Assumption A5 as how we did in the proof of Lemma B.1.    \n\\end{proof}\n\n\n\n\n\n\n"
                },
                "subsection 9.4": {
                    "name": "Proof of Lemma 5.2",
                    "content": "\n\\textbf{Lemma.}\n\\textit{\nWith Assumptions A1-A6, \nassuming there is a $a'()\\in \\ma$ where $ \\widehat{r}_{\\mP_{a'}}(\\wt)=\\frac{1}{2}\\big(\\widehat{r}_{\\mP_{a^+}}(\\wt)+\\widehat{r}_{\\mP_{a^-}}(\\wt)\\big)$,\nwith probability at least $1-\\delta$, we have:\n\\begin{align}\n    \\rpa(\\wt)  \\leq &\n    \\dfrac{1}{2}\\big(\\widehat{r}_{\\mP_{a^+}}(\\wt) \\\\\n    & + \\widehat{r}_{\\mP_{a^-}}(\\wt)\\big) + \n    \\sum_{i}||f(a^+(\\x_i);\\wt) - f(a^-(\\x');\\wt)||_1 + \n    \\phi(|\\Theta|, n, \\delta)\n\\end{align}\n}\n\\begin{proof}\nWe can continue with \n\\begin{align*}\n    \\sup_{\\mP'\\in T(\\mP, \\ma)}\n    \\wrpp(\\wt) \n    & \\leq \\wrp(\\wt)\n    +  W_1(\\widehat{\\mathbf{Q}}_{\\x, \\wt}, \\widehat{\\mathbf{Q}}_{a(\\x), \\wt})), \n\\end{align*}\nwhere $\\widehat{\\mathbf{Q}}_{\\x, \\wt}$ denotes the empirical \ndistribution of $\\faxt$ for $(\\x, \\y)\\in (\\X, \\Y)$.\nfrom the proof of Theorem 5.2. \nWith the help of Assumption A3, we have:\n\\begin{align*}\n    d_x (f(a^+(\\x), \\wt), f(a^-(\\x), \\wt)) \\geq d_x (f(\\x, \\wt), f(\\x', \\wt))\n\\end{align*}\nWhen $d_x(\\cdot, \\cdot)$ is chosen as Wasserstein-1 metric, we have: \n\\begin{align*}\n    \\sup_{\\mP'\\in T(\\mP, \\ma)}\n    \\wrpp(\\wt) \n    & \\leq \\wrp(\\wt)\n    +  W_1(\\widehat{\\mathbf{Q}}_{a^+(\\x), \\wt}, \\widehat{\\mathbf{Q}}_{a^-(\\x), \\wt}))\n\\end{align*}\nFurther, as the LHS is the robust risk generated by the transformation functions within $\\ma$, \nand $\\wrp(\\wt)$ is independent of the term $W_1(\\widehat{\\mathbf{Q}}_{a^+(\\x), \\wt}, \\widehat{\\mathbf{Q}}_{a^-(\\x), \\wt}))$, \nWLOG, we can replace $\\wrp(\\wt)$ with the risk of an arbitrary distribution generated by the transformation function in $\\ma$. \nIf we choose to use $ \\widehat{r}_{\\mP_{a'}}(\\wt)=\\frac{1}{2}\\big(\\widehat{r}_{\\mP_{a^+}}(\\wt)+\\widehat{r}_{\\mP_{a^-}}(\\wt)\\big)$, \nwe can conclude the proof with help from Proposition B.2 and Assumption A5 as how we did in the proof of Theorem 5.2.  \n\\end{proof}\n\n\n% \\section{Additional Details of Synthetic Experiments Setup}\n% \\label{sec:app:synthetic}\n% %Methods related to the worst case identify the worst case transformation through brute-force searching. \n\n% \\textbf{Results Discussion} Table~\\ref{tab:main:cifar} tells roughly the same story with Table~\\ref{tab:main:mnist}. \n% The invariance score of the worst case methods in Table~\\ref{tab:main:cifar}\n% behave lower than we expected, \n% we conjecture this is mainly because \n% some elements in $\\ma$ of ``contrast'' will transform the data \n% into samples inherently hard to predict \n% (\\textit{e.g.} $a(\\x)=\\x/4$ will squeeze the pixel values together, \n% so the images look blurry in general and hard to recognize), \n% the model repeatedly identifies these case as the worst case and ignores the others. \n% As a result, \\rwa{} effectively degrades to \n% \\ra{}\n% yet is inferior to \\ra{} because it does not have\n% the explicit vertex information. \n% To verify the conjecture, we count how often each augmented sample \n% to be considered as the worst case:\n% for ``texture'' and ``rotation'', each augmented sample generated by $\\ma$ \n% are picked up with an almost equal frequency, \n% while for ``constrast'', $\\x/2$ and $(1-\\x)/2$ \n% are identified only $10\\%$-$15\\%$ of the time $\\x/4$ \n% and $(1-\\x)/4$ are identified as the worst case. \n\n% \\newpage\n% \\section{More Synthetic Results}\n% \\label{sec:app:more}\n\n% \\begin{table}[]\n% \\centering \n% \\begin{tabular}{lcccccc}\n% \\hline\n%  & Worst & Clean & Vertex & All & Beyond & Invariance \\\\ \\hline\n% \\base{} & 0.9860 & \\multicolumn{2}{c}{0.9921} & 0.9911 & 0.9463 & 0.9236 \\\\\n% \\va{} & 0.9906 & \\textbf{0.9928} & \\textbf{0.9925} & \\textbf{0.9927} & 0.9650 & 0.9876 \\\\\n% \\ra{} & 0.9904 & 0.9909 & 0.9910 & 0.9909 & 0.9747 & 1 \\\\\n% \\vwa{} & 0.9903 & \\multicolumn{2}{c}{0.9922} & 0.9923 & 0.9696 & 0.9940 \\\\\n% \\rwa{} & \\textbf{0.9911} & \\multicolumn{2}{c}{0.9915} & 0.9915 & \\textbf{0.9773} & \\textbf{1} \\\\ \\hline\n% \\hline \n% \\ral{} & 0.9897 & 0.9904 & 0.9901 & 0.9903 & 0.9728 & \\textbf{1} \\\\\n% \\raw{} & 0.9858 & 0.9888 & 0.9902 & 0.9893 & 0.9433 & 0.6428 \\\\\n% \\rad{} & 0.9892 & 0.9921 & 0.9912 & 0.9919 & 0.9373 & 0.2588 \\\\\n% \\rak{} & 0.0980 & 0.0980 & 0.0980 & 0.0980 & 0.0980 & 0.2800 \\\\\n% \\ras{} & 0.9898 & 0.9917 & 0.9919 & 0.9920 & 0.9633 & 0.9928 \\\\\n% \\rasl{} & 0.9904 & 0.9925 & 0.9918 & 0.9925 & 0.9672 & 0.9960 \\\\ \\hline\n% \\end{tabular}\n% \\caption{More methods tested with more comprehensive metrics over MNIST on texture}\n% \\label{tab:more:mnist:texture}\n% \\end{table}\n\n\n% \\begin{table}[]\n% \\centering \n% \\begin{tabular}{lcccccc}\n% \\hline\n%  & Worst & Clean & Vertex & All & Beyond & Invariance \\\\ \\hline\n% \\base{} & 0.2960 & \\multicolumn{2}{c}{0.9921} & 0.7410 & 0.8914 & 0.2056 \\\\\n% \\va{} & 0.9336 & 0.9884 & 0.9886 & 0.9775 & 0.8711 & 0.5628 \\\\\n% \\ra{} & 0.9525 & 0.9930 & 0.9919 & 0.9829 & 0.9201 & 0.6044 \\\\\n% \\vwa{} & 0.9408 & \\multicolumn{2}{c}{0.9466} & 0.9827 & 0.5979 & 0.6284 \\\\\n% \\rwa{} & \\textbf{0.9882} & \\multicolumn{2}{c}{\\textbf{0.9934}} & \\textbf{0.9934} & \\textbf{0.9417} & \\textbf{0.8856} \\\\ \\hline \\hline \n% \\ral{} & 0.9532 & 0.9913 & 0.9916 & 0.9824 & 0.9145 & 0.5912 \\\\\n% \\raw{} & 0.9274 & 0.9882 & 0.9875 & 0.9757 & 0.8514 & 0.4600 \\\\\n% \\rad{} & 0.9368 & 0.9895 & 0.989 & 0.9782 & 0.8431 & 0.4132 \\\\\n% \\rak{} & 0.9424 & 0.9875 & 0.9872 & 0.9762 & 0.9194 & 0.6800 \\\\\n% \\ras{} & 0.9389 & 0.9900 & 0.9901 & 0.9792 & 0.8631 & 0.6060 \\\\\n% \\rasl{} & 0.9424 & 0.9913 & 0.9901 & 0.9804 & 0.8663 & 0.5864 \\\\ \\hline\n% \\end{tabular}\n% \\caption{More methods tested with more comprehensive metrics over MNIST on rotation. }\n% \\label{tab:more:mnist:rotation}\n% \\end{table}\n\n% \\begin{table}[]\n% \\centering \n% \\begin{tabular}{lcccccc}\n% \\hline\n%  & Worst & Clean & Vertex & All & Beyond & Invariance \\\\ \\hline\n% \\base{} & 0.2699 & \\multicolumn{2}{c}{0.9921} & 0.6377 & 0.2988 & 0.2003 \\\\\n% \\va{} & 0.9837 & 0.9922 & 0.9917 & 0.9913 & 0.6044 & 0.4153 \\\\\n% \\ra{} & 0.9823 & 0.9936 & 0.9930 & 0.9911 & 0.6512 & 0.4166 \\\\\n% \\vwa{} & 0.4470 & \\multicolumn{2}{c}{0.5360} & 0.7515 & 0.4649 & 0.2210 \\\\\n% \\rwa{} & \\textbf{0.9893} & \\multicolumn{2}{c}{\\textbf{0.9940}} & \\textbf{0.9930} & 0.4841 & \\textbf{0.8786} \\\\ \\hline \\hline \n% \\ral{} & 0.9776 & 0.9935 & 0.9932 & 0.9902 & 0.6251 & 0.4176 \\\\\n% \\raw{} & 0.7357 & 0.9867 & 0.9865 & 0.9361 & \\textbf{0.6547} & 0.2960 \\\\\n% \\rad{} & 0.9833 & 0.9913 & 0.9921 & 0.9909 & 0.6199 & 0.2000 \\\\\n% \\rak{} & 0.9105 & 0.9894 & 0.9882 & 0.9677 & 0.6001 & 0.4153 \\\\\n% \\ras{} & 0.9839 & 0.9916 & 0.9910 & 0.9906 & 0.6221 & 0.4273 \\\\\n% \\rasl{} & 0.9844 & 0.9920 & 0.9918 & 0.9909 & 0.5843 & 0.4236 \\\\ \\hline\n% \\end{tabular}\n% \\caption{More methods tested with more comprehensive metrics over MNIST on contrast. }\n% \\label{tab:more:mnist:contrast}\n% \\end{table}\n\n% \\subsection{Experiment Setup}\n% To understand these methods, \n% we introduce a more comprehensive test of these methods, \n% including the five methods discussed in the main paper, \n% and multiple ablation test methods, including \n% \\begin{itemize}[leftmargin=*]\n% \\item \\ral{}: when squared $\\ell_2$ norm of \\ra{} is replaced by $\\ell_1$ norm. \n% \\item \\raw{}: when the norm distance of \\ra{} is replaced by Wasserstein distance, enabled by the implementation of Wasserstein GAN \\cite{arjovsky2017wasserstein,gulrajani2017improved}. \n% \\item \\rad{}: when the norm distance of \\ra{} is replaced by a discriminator. Our implementation uses a one-layer neural network. \n% \\item \\rak{}: when the norm distance of \\ra{} is replaced by KL divergence. \n% \\item \\ras{}: when the regularization of \\ra{} is applied to softmax instead of logits. \n% \\item \\rasl{}: when the regularization of \\ra{} is applied to softmax instead of logits, and the squared $\\ell_2$ norm is replaced by $\\ell_1$ norm. This is the method suggested by pure theoretical discussion if we do not concern with the difficulties of passing gradient through backpropagation. \n% \\end{itemize}\n\n% And we test these methods in the three scenarios mentioned in the previous section: texture, rotation, and contrast. \n% The overall test follows the same regime as the one reported in the main manuscript, with additional tests:\n% \\begin{itemize}\n%     \\item Vertex: average test performance on the perturbed samples with the vertex function from $\\ma$. \n%     Models with worst case augmentation are not tested with vertex as these models do not have the specific concept of vertex. \n%     \\item All: average test performance on all the samples perturbed by all the elements in $\\ma$. \n%     \\item Beyond: To have some sense of how well the methods can perform in the setting that follows the same concept, but not considered in $\\ma$, and not (intuitively) limited by the verteics of $\\ma$, we also test the accuracy of the models with some transformations related to the elements in $\\ma$, but not in $\\ma$, To be specific: \n%     \\begin{itemize}\n%         \\item Texture: $\\ma_{\\textnormal{beyond}} = \\{a_5(), a_4()\\}$. \n%         \\item Rotation: $\\ma_{\\textnormal{beyond}} = \\{a_330(), a_345()\\}$.\n%         \\item Contrast: $\\ma_{\\textnormal{beyond}} = \\{ a(\\x)=\\x/2+0.5, a(\\x)=\\x/4+0.75, a(\\x)=(1-\\x)/2+0.5, a(\\x)=(1-\\x)/4+0.75 \\}$\n%     \\end{itemize}\n%     We report the average test accuracy of the samples tested all the elements in $\\ma_{\\textnormal{beyond}}$\n% \\end{itemize}\n\n\n% \\subsection{Results}\n\n% \\begin{table}[]\n% \\centering \n% \\begin{tabular}{lcccccc}\n% \\hline\n%  & Worst & Clean & Vertex & All & Beyond & Invariance \\\\ \\hline\n% \\base{} & 0.3219 & \\multicolumn{2}{c}{0.7013} & 0.5997 & 0.3084 & 0.7140 \\\\\n% \\va{} & 0.5949 & 0.6601 & 0.6394 & 0.6530 & 0.5583 & 0.9996 \\\\\n% \\ra{} & 0.6259 & 0.6571 & 0.6485 & 0.6553 & 0.5826 & \\textbf{1} \\\\\n% \\vwa{} & 0.5814 & \\multicolumn{2}{c}{0.6049} & 0.6024 & 0.5213 & \\textbf{1} \\\\\n% \\rwa{} & \\textbf{0.6358} & \\multicolumn{2}{c}{0.6630} & 0.6612 & \\textbf{0.5892} & \\textbf{1} \\\\ \\hline \\hline \n% \\ral{} & 0.6230 & 0.6609 & 0.6511 & 0.6578 & 0.5775 & \\textbf{1} \\\\\n% \\raw{} & 0.6140 & 0.6860 & 0.6578 & 0.6783 & 0.5801 & \\textbf{1} \\\\\n% \\rad{} & 0.5794 & \\textbf{0.7663} & \\textbf{0.6734} & \\textbf{0.7288} & 0.5632 & 0.3220 \\\\\n% \\rak{} & 0.5866 & 0.5873 & 0.5868 & 0.5870 & 0.5804 & \\textbf{1} \\\\\n% \\ras{} & 0.6197 & 0.6263 & 0.6268 & 0.6266 & 0.5831 & \\textbf{1} \\\\\n% \\rasl{} & 0.6319 & 0.653 & 0.6480 & 0.6516 & 0.5830 & \\textbf{1} \\\\ \\hline\n% \\end{tabular}\n% \\caption{More methods tested with more comprehensive metrics over CIFAR10 on texture}\n% \\label{tab:more:cifar:texture}\n% \\end{table}\n\n\n% \\begin{table}[]\n% \\centering \n% \\begin{tabular}{lcccccc}\n% \\hline\n%  & Worst & Clean & Vertex & All & Beyond & Invariance \\\\ \\hline\n% \\base{} & 0.0871 & \\multicolumn{2}{c}{0.7013} & 0.4061 & 0.4634 & 0.5016 \\\\\n% \\va{} & 0.4399 & 0.7378 & 0.7199 & 0.6835 & \\textbf{0.5096} & 0.6168 \\\\\n% \\ra{} & 0.5166 & 0.6815 & 0.6741 & 0.6452 & 0.4408 & 0.8520 \\\\\n% \\vwa{} & 0.6009 & \\multicolumn{2}{c}{0.7140} & 0.7406 & 0.4446 & 0.9172 \\\\\n% \\rwa{} & \\textbf{0.6486} & \\multicolumn{2}{c}{\\textbf{0.7606}} & \\textbf{0.7507} & 0.4614 & \\textbf{0.9244} \\\\ \\hline \\hline \n% \\ral{} & 0.4685 & 0.7505 & 0.7290 & 0.6852 & 0.4878 & 0.6248 \\\\\n% \\raw{} & 0.4228 & 0.7468 & 0.7287 & 0.6822 & 0.4753 & 0.6072 \\\\\n% \\rad{} & 0.4298 & \\textbf{0.7752} & 0.7456 & 0.6941 & 0.4662 & 0.2664 \\\\\n% \\rak{} & 0.5848 & 0.4241 & 0.4221 & 0.4211 & 0.3946 & 0.9200 \\\\\n% \\ras{} & 0.5143 & 0.7187 & 0.7175 & 0.6851 & 0.4694 & 0.8188 \\\\\n% \\rasl{} & 0.4779 & 0.7341 & 0.725 & 0.6911 & 0.4944 & 0.7288 \\\\ \\hline\n% \\end{tabular}\n% \\caption{More methods tested with more comprehensive metrics over CIFAR10 on rotation. }\n% \\label{tab:more:cifar:rotation}\n% \\end{table}\n\n% \\begin{table}[]\n% \\centering \n% \\begin{tabular}{lcccccc}\n% \\hline\n%  & Worst & Clean & Vertex & All & Beyond & Invariance \\\\ \\hline\n% \\base{} & 0.2079 & \\multicolumn{2}{c}{0.7013} & 0.4793 & 0.2605 & 0.3400 \\\\\n% \\va{} & 0.6372 & 0.7452 & 0.7243 & 0.7365 & 0.3733 & 0.4406 \\\\\n% \\ra{} & 0.6867 & \\textbf{0.7742} & \\textbf{0.7702} & \\textbf{0.7722} & 0.5527 & 0.5350 \\\\\n% \\vwa{} & 0.6708 & \\multicolumn{2}{c}{0.7387} & 0.7375 & 0.5539 & 0.4790 \\\\\n% \\rwa{} & 0.6326 & \\multicolumn{2}{c}{0.7489} & 0.7246 & 0.4789 & 0.3736 \\\\ \\hline \\hline \n% \\ral{} & \\textbf{0.7096} & 0.7688 & 0.7634 & 0.7666 & \\textbf{0.7330} & \\textbf{0.6260} \\\\\n% \\raw{} & 0.6325 & 0.7442 & 0.7303 & 0.7364 & 0.4994 & 0.4396 \\\\\n% \\rad{} & 0.6451 & 0.7515 & 0.7392 & 0.7479 & 0.4820 & 0.2393 \\\\\n% \\rak{} & 0.1137 & 0.4515 & 0.4517 & 0.3317 & 0.2648 & 0.5026 \\\\\n% \\ras{} & 0.6856 & 0.7618 & 0.7558 & 0.7609 & 0.6531 & 0.4833 \\\\\n% \\rasl{} & 0.6895 & 0.7585 & 0.7533 & 0.7581 & 0.7000 & 0.4946 \\\\ \\hline\n% \\end{tabular}\n% \\caption{More methods tested with more comprehensive metrics over CIFAR10 on contrast. }\n% \\label{tab:more:cifar:contrast}\n% \\end{table}\n\n% We report the results in Table~\\ref{tab:more:mnist:texture}-\\ref{tab:more:cifar:contrast}.\n\n% \\paragraph{Ablation Study} \n% First we consider the ablation study to validate our choice as the \n% squared $\\ell_2$ norm regularization, \n% particularly because our choice considers both the theoretical arguments and practical arguments regarding gradients. \n% In case of worst-case prediction, \n% we can see the other \\ra{} variants can barely outperform \\ra{}, \n% even not the one that our theoretical arguments directly suggest (\\rasl{} or \\raw{}). \n% We believe this is mostly due to the challenges of\n% passing the gradient with $\\ell_1$ norm and softmax, \n% or through a classifier. \n\n% We also test the performances of other regularizations that \n% are irrelevant to our theoretical studies, but are popular choices in general (\\rad{} and \\rak{}). \n% These methods in general perform badly, can barely match \\ra{} in terms of the worst-case performance. \n% Further, when some cases when \\rad{} and \\rak{} can outperform \\ra{} in other accuracy-wise testing, these methods tend to behave terribly in invariance test, which suggests these regularizations are not effective. \n% In the cases when \\rad{} and \\rak{} can match \\ra{} in invariance test, these methods can barely compete with \\ra{}. \n\n% \\paragraph{Broader Test}\n% We also test our methods in the broader test. \n% As we can see, \\rwa{} behaves the best in most of the cases. \n% In three out of these six test scenarios, \\rwa{} lost to three other different methods in the ``beyond'' case. \n% However, we believe, in general, this is still a strong evidence to show that \\rwa{} is a generally preferable method. \n\n% Also, comparing the methods of \\ra{} vs. \\va{}, and \\rwa{} vs. \\vwa{}, we can see that regularization helps mostly in the cases of ``beyond'' in addition to ``invariance'' test. \n% This result again suggests the importance of \n% regularizations, as in practice, \n% training phase is not always aware of all the transformation functions during test phase. \n\n\n% \\begin{table*}[]\n% \\centering \n% \\footnotesize \n% \\caption{Details of Rotation Experiment\n% }\n\n% \\begin{tabular}{c|ccc|ccc|ccc|ccc}\n% \\hline\n%  & \\multicolumn{3}{c|}{\\textsf{ResNet}} & \\multicolumn{3}{c|}{\\textsf{ResNet-GC}} & \\multicolumn{3}{c|}{\\textsf{ResNet-ST}} & \\multicolumn{3}{c}{\\textsf{ResNet-ETN}} \\\\ \n%  & \\base{} & \\ra{} & \\rwa{} & \\base{} & \\ra{} & \\rwa{} & \\base{} & \\ra{} & \\rwa{} & \\base{} & \\ra{} & \\rwa{} \\\\ \\hline\n% 0 & 0.836 & 0.8487 & 0.8708 & 0.7645 & 0.8537 & 0.8578 & 0.7805 & 0.7472 & 0.7787 & 0.8242 & 0.8537 & 0.8562 \\\\\n% 15 & 0.6938 & 0.7904 & 0.8871 & 0.5596 & 0.793 & 0.8538 & 0.7374 & 0.7361 & 0.7706 & 0.7151 & 0.8199 & 0.8538 \\\\\n% 30 & 0.4557 & 0.7455 & 0.8869 & 0.3558 & 0.7485 & 0.8275 & 0.6519 & 0.7362 & 0.7715 & 0.4407 & 0.8381 & 0.8467 \\\\\n% 45 & 0.3281 & 0.8005 & 0.887 & 0.246 & 0.7618 & 0.7482 & 0.4953 & 0.7375 & 0.7727 & 0.2839 & 0.8401 & 0.835 \\\\\n% 60 & 0.2578 & 0.8282 & 0.8818 & 0.1963 & 0.8251 & 0.8191 & 0.3974 & 0.7374 & 0.7663 & 0.2297 & 0.845 & 0.8395 \\\\\n% 75 & 0.2366 & 0.7236 & 0.8101 & 0.1878 & 0.7976 & 0.6715 & 0.3177 & 0.7342 & 0.7374 & 0.2484 & 0.8051 & 0.5985 \\\\\n% 90 & 0.2939 & 0.5615 & 0.5742 & 0.1966 & 0.6589 & 0.617 & 0.3044 & 0.7235 & 0.6721 & 0.2923 & 0.4452 & 0.3494 \\\\\n% 105 & 0.2027 & 0.3545 & 0.4242 & 0.1717 & 0.4483 & 0.5748 & 0.2698 & 0.7223 & 0.649 & 0.2225 & 0.4245 & 0.3059 \\\\\n% 120 & 0.1758 & 0.2992 & 0.3885 & 0.1651 & 0.4162 & 0.5996 & 0.2677 & 0.7169 & 0.6663 & 0.1877 & 0.3868 & 0.3017 \\\\\n% 135 & 0.1748 & 0.3115 & 0.3708 & 0.1683 & 0.3854 & 0.4152 & 0.2655 & 0.7145 & 0.6585 & 0.1907 & 0.4029 & 0.34 \\\\\n% 150 & 0.181 & 0.3347 & 0.3524 & 0.184 & 0.3777 & 0.3631 & 0.2813 & 0.7097 & 0.6266 & 0.2082 & 0.4012 & 0.2992 \\\\\n% 165 & 0.2283 & 0.325 & 0.3366 & 0.2091 & 0.3604 & 0.3174 & 0.3091 & 0.7054 & 0.5611 & 0.2585 & 0.3618 & 0.2953 \\\\\n% 180 & 0.3053 & 0.3485 & 0.3795 & 0.2673 & 0.3669 & 0.3183 & 0.3295 & 0.7176 & 0.534 & 0.3343 & 0.3796 & 0.3577 \\\\\n% 195 & 0.2607 & 0.3089 & 0.3781 & 0.2265 & 0.3221 & 0.3256 & 0.2985 & 0.705 & 0.5177 & 0.2663 & 0.3543 & 0.3445 \\\\\n% 210 & 0.2298 & 0.3109 & 0.3806 & 0.1963 & 0.3258 & 0.3468 & 0.2803 & 0.7003 & 0.5173 & 0.2225 & 0.362 & 0.3447 \\\\\n% 225 & 0.2218 & 0.3342 & 0.3723 & 0.1788 & 0.3316 & 0.3315 & 0.2687 & 0.6965 & 0.5151 & 0.1948 & 0.3622 & 0.3468 \\\\\n% 240 & 0.2042 & 0.3519 & 0.3729 & 0.1755 & 0.3613 & 0.3808 & 0.2558 & 0.7033 & 0.5235 & 0.1894 & 0.3597 & 0.3332 \\\\\n% 255 & 0.2023 & 0.3335 & 0.3631 & 0.194 & 0.36 & 0.3964 & 0.2663 & 0.7147 & 0.5597 & 0.2206 & 0.3706 & 0.2964 \\\\\n% 270 & 0.2683 & 0.3507 & 0.381 & 0.2297 & 0.4607 & 0.4411 & 0.3202 & 0.7318 & 0.6356 & 0.291 & 0.4945 & 0.3372 \\\\\n% 285 & 0.2275 & 0.3046 & 0.389 & 0.2056 & 0.5844 & 0.4521 & 0.335 & 0.7245 & 0.6339 & 0.2527 & 0.4255 & 0.3249 \\\\\n% 300 & 0.2196 & 0.3198 & 0.4012 & 0.2117 & 0.6469 & 0.4973 & 0.3686 & 0.7232 & 0.6443 & 0.2393 & 0.4292 & 0.3315 \\\\\n% 315 & 0.2573 & 0.3901 & 0.4251 & 0.2427 & 0.5596 & 0.4816 & 0.4211 & 0.7303 & 0.6412 & 0.2746 & 0.4588 & 0.3259 \\\\\n% 330 & 0.3873 & 0.5489 & 0.4852 & 0.3429 & 0.5755 & 0.7211 & 0.5552 & 0.7299 & 0.6592 & 0.4215 & 0.5057 & 0.3534 \\\\\n% 345 & 0.6502 & 0.717 & 0.6765 & 0.5463 & 0.74 & 0.8417 & 0.7193 & 0.7379 & 0.7215 & 0.7055 & 0.7023 & 0.6528 \\\\ \\hline\n% \\end{tabular}\n% \\label{tab:real:rotation:app}\n% \\end{table*}\n\n\n"
                }
            },
            "section 10": {
                "name": "Additional Results for Comparisons with Advanced Methods",
                "content": "\n\\label{sec:app:real}\n\n\nWe have also conducted two full ImageNet level experiments. However, due to the limitation of resources, we cannot tune the models substantially. \nOur current trial suggest that our techniques can improve the vanilla model to compete with SOTA models, limited by our resources, \nwe cannot do wide-range hyperparameters search to outperform them. \nAlso, considering the fact that many of these methods are significantly more complicated than us and also uses data augmentation specially designed for the tasks, we consider our experiments a success indication \nof the empirical strength of our methods. \n\n% \\textbf{Rotation-invariant Image Classification}\n% We compare our results with specifically designed rotation-invariant models,\n% mainly Spatial Transformer (\\textsf{ST}) \\citep{jaderberg2015spatial},\n% Group Convolution (\\textsf{GC}) \\citep{cohen2016group}, \n% and Equivariant Transformer Network (\\textsf{ETN}) \\citep{tai2019equivariant}. \n% We also attempted to run CGNet \\citep{kondor2018clebsch}, \n% but the procedure does not scale to the CIFAR10 and ResNet level. \n% The results are reported in Table~\\ref{tab:real:rotation:app}, \n% where most methods use the same architecture \n% (ResNet34 with most performance boosting heuristics enabled), \n% except that \\textsf{GC} uses ResNet18 because ResNet34 with \\textsf{GC} \n% runs 100 times slowly than others, thus not practical. \n% We test the models with nine different rotations including $0^{\\circ}$ degree rotation. \n% Augmentation related methods are using the $\\ma$ of ``rotation''\n% in synthetic experiments (Appendix~\\ref{sec:app:synthetic}), \n% so the testing scenario goes beyond what the augmentation methods have seen during training. \n% The results in Table~\\ref{tab:real:rotation:app} strongly endorses the \n% efficacy of augmentation-based methods. \n% Interestingly, regularized augmentation methods,\n% with the benefit of learning the concept of invariance, \n% tend to behave well in the transformations not considered during training. \n% As we can see, \\ra{} outperforms \\vwa{} on average. \n\n% \\begin{table}[]\n% \\small\n% \\centering \n% % \\begin{tabular}{ccccccccc}\n% % \\hline\n% %  & 330 & 345 & 0 (original) & 15 & 30 & 45 & 60 & Average \\\\ \\hline\n% % \\base{} & 0.3407 & 0.5393 & 0.7211 & 0.5548 & 0.3404 & 0.2525 & 0.2123 & 0.4230 \\\\\n% % \\textsf{ST} & 0.3751 & 0.5595 & 0.7155 & 0.6201 & 0.4014 & 0.2598 & 0.2265 & 0.4511 \\\\\n% % \\textsf{GC} & 0.246 & 0.3919 & 0.5859 & 0.4145 & 0.2534 & 0.1827 & 0.1501 & 0.3178 \\\\\n% % \\textsf{ETN} & 0.57\t& 0.6091 & 0.6617 &\t0.602&\t0.5519&\t0.5078&\t0.5838&\t0.5838 \\\\\n% % \\va{} & 0.3818 & 0.5118 & 0.6845 & 0.5727 & 0.5201 & 0.604 & 0.5118 & 0.5410 \\\\\n% % \\ra{} & \\textbf{0.469} & \\textbf{0.6242} & 0.7484 & 0.6841 & 0.6448 & 0.6925 & 0.7262 & 0.6556 \\\\\n% % \\vwa{} & 0.3483 & 0.4457 & 0.6545 & 0.7709 & 0.7822 & 0.7865 & 0.7683 & 0.6509 \\\\\n% % \\rwa{} & 0.433 & 0.6188 & \\textbf{0.7958} & \\textbf{0.8105} & \\textbf{0.8057} & \\textbf{0.8027} & \\textbf{0.7972} & \\textbf{0.7234} \\\\ \\hline\n% % \\end{tabular}\n% \\begin{tabular}{ccccccccccc}\n% \\hline\n%  & 300 & 315 & 330 & 345 & 0 & 15 & 30 & 45 & 60 & avg. \\\\ \\hline\n% \\base{} & 0.2196 & 0.2573 & 0.3873 & 0.6502 & 0.8360 & 0.6938 & 0.4557 & 0.3281 & 0.2578 & 0.4539 \\\\\n% \\textsf{ST} & 0.2391 & 0.2748 & 0.4214 & 0.7049 & 0.8251 & 0.7147 & 0.4398 & 0.2838 & 0.2300 & 0.4593 \\\\\n% \\textsf{GC} & 0.1540 & 0.1891 & 0.2460 & 0.3919 & 0.5859 & 0.4145 & 0.2534 & 0.1827 & 0.1507 & 0.2853 \\\\\n% \\textsf{ETN} & 0.3855 & \\textbf{0.4844} & \\textbf{0.6324} & \\textbf{0.7576} & 0.8276 & 0.7730 & 0.7324 & 0.6245 & 0.5060 & 0.6358 \\\\\n% \\va{} & 0.2233 & 0.2832 & 0.4318 & 0.6364 & 0.8124 & 0.6926 & 0.5973 & 0.7152 & 0.7923 & 0.5761 \\\\\n% \\ra{} & 0.3198 & 0.3901 & 0.5489 & 0.7170 & 0.8487 & 0.7904 & 0.7455 & 0.8005 & 0.8282 & 0.6655 \\\\\n% \\vwa{} & 0.3383 & 0.3484 & 0.3835 & 0.4569 & 0.7474 & 0.866 & 0.8776 & 0.8738 & 0.8629 & 0.6394 \\\\\n% \\rwa{} & \\textbf{0.4012} & 0.4251 & 0.4852 & 0.6765 & \\textbf{0.8708} & \\textbf{0.8871} & \\textbf{0.8869} & \\textbf{0.8870} & \\textbf{0.8818} & \\textbf{0.7113} \\\\ \\hline\n% \\end{tabular}\n% \\caption{Comparison to advanced rotation-invariant models. We report the test accuracy on the test sets clockwise rotated, $0^{\\circ}$-$60^{\\circ}$ and $300^{\\circ}$-$360^{\\circ}$.\n% Average accuracy is also reported. \n% Augmentation methods only consider $0^{\\circ}$-$60^{\\circ}$ clockwise rotations during training. }\n% \\label{tab:real:rotation:app}\n% \\end{table}\n\n% \\textbf{Texture-perturbed ImageNet classification}\n% We also test the performance on the image classification over multiple perturbations. \n% We train the model over standard ImageNet training set and test the model with ImageNet-C data \\cite{hendrycks2019robustness}, which is a perturbed version of ImageNet by corrupting the original ImageNet validation set with a collection of noises. Following the standard, the reported performance is mCE, which is the smaller the better. \n% We compare with several methods tested on this dataset, including \n% Patch Uniform (\\textsf{PU}) \\cite{lopes2019improving}, \n% AutoAugment (\\textsf{AA}) \\cite{cubuk2019autoaugment}, \n% MaxBlur pool (\\textsf{MBP}) \\cite{zhang2019making}, \n% Stylized ImageNet (\\textsf{SIN}) \\cite{hendrycks2019robustness}, \n% AugMix (\\textsf{AM}) \\cite{Hendrycks2020augmix}, \n% AugMix w. SIN (\\textsf{AMS}) \\cite{Hendrycks2020augmix}. \n% We use the performance reported in \\cite{Hendrycks2020augmix}. \n% % Our augmentation method only uses a simple augmentation strategy \n% % that uses the low-frequency representation of the data, which was previously shown to align with the human understanding of the images \\cite{wang2020high}. \n% Again, our augmention only uses the generic texture with perturbation (the $\\ma$ in our texture synthetic experiments with radius changed to $20, 25, 30, 35, 40$).\n% The results are reported in Table~\\ref{tab:real:c:app}, which shows that\n% our generic method outperform the current SOTA methods after a continued finetuning process with reducing learning rates. \n\n\\paragraph{Texture-perturbed ImageNet classification}\nWe also test the performance on the image classification over multiple perturbations. \nWe train the model over standard ImageNet training set and test the model with ImageNet-C data \\citep{hendrycks2019robustness}, which is a perturbed version of ImageNet by corrupting the original ImageNet validation set with a collection of noises. Following the standard, the reported performance is mCE, which is the smaller the better. \nWe compare with several methods tested on this dataset, including \nPatch Uniform (\\textsf{PU}) \\citep{lopes2019improving}, \nAutoAugment (\\textsf{AA}) \\citep{cubuk2019autoaugment}, \nMaxBlur pool (\\textsf{MBP}) \\citep{zhang2019making}, \nStylized ImageNet (\\textsf{SIN}) \\citep{hendrycks2019robustness}, \nAugMix (\\textsf{AM}) \\citep{Hendrycks2020augmix}, \nAugMix w. SIN (\\textsf{AMS}) \\citep{Hendrycks2020augmix}. \nWe use the performance reported in \\citep{Hendrycks2020augmix}. \n% Our augmentation method only uses a simple augmentation strategy \n% that uses the low-frequency representation of the data, which was previously shown to align with the human understanding of the images \\cite{wang2020high}. \nAgain, our augmention only uses the generic texture with perturbation (the $\\ma$ in our texture synthetic experiments with radius changed to $20, 25, 30, 35, 40$).\nThe results are reported in Table~\\ref{tab:real:c:app}, which shows that\nour generic method outperform the current SOTA methods after a continued finetuning process with reducing learning rates. \n\n\\paragraph{Cross-domain ImageNet-Sketch Classification}\nWe also compare to the methods used for cross-domain evaluation. \nWe follow the set-up advocated by \\citep{wang2019learning2} \nfor domain-agnostic cross-domain prediction, \nwhich is training the model on one or multiple domains \nwithout domain identifiers and test the model on an unseen domain. \nWe use the most challenging setup in this scenario: \ntrain the models with standard ImageNet training data, \nand test the model over ImageNet-Sketch data \\citep{wang2019learning}, which is a collection of sketches \nfollowing the structure ImageNet validation set. \nWe compare with previous methods with reported performance on this dataset, \nsuch as \\textsf{InfoDrop} \\citep{achille2018information}, \n\\textsf{HEX} \\citep{wang2019learning2}, \\textsf{PAR} \\citep{wang2019learning}, \n\\textsf{RSC} \\citep{HuangWXH20}\nand report the performances in Table~\\ref{tab:real:sketch:app}. \nNotice that, our data augmentation also follows the \nrequirement that the characteristics of the test domain cannot \nbe utilized during training. \nThus, we only augment the samples with a generic augmentation set\n($\\ma$ of ``contrast'' in synthetic experiments). \nThe results again support the usage of data augmentation and alignment regularization.  \n% \\newpage\n% \\section{Experiments Configurations}\n% The synthetic experiments are run with NVIDIA 1080 Ti, with configurations listed below:\n% \\VerbatimInput{configs/configure.txt}\n\n\n% The advanced image classification experiments are run with RTX 2080, with configurations listed below:\n% \\VerbatimInput{configs/configure2.txt}\n"
            }
        },
        "tables": {
            "tab:mnist:consistency": "\\begin{table*}[t]\n\\small \n\\centering \n\\caption{Test results on MNIST dataset for different ARs over three evaluation metrics and three distribution shifts. \n\\textsf{B} denotes Baseline, i.e., the model does not use any data augmentation; \n\\textsf{V} denotes vanilla augmentation, i.e., the model uses data augmentation but not AR; \n\\textsf{L} denotes $\\ell_1$ norm;\n\\textsf{S} denotes squared $\\ell_2$ norm;\n\\textsf{C} denotes cosine similarity;\n\\textsf{K} denotes KL divergence;\n\\textsf{W} denotes Wasserstein-1 metric;\n\\textsf{D} denotes GAN discriminator.\n}\n\\setlength\\tabcolsep{5pt}\n\\begin{tabular}{cccccccccc}\n\\hline\n &  & \\textsf{B} & \\textsf{V} & \\textsf{L} & \\textsf{S} & \\textsf{C} & \\textsf{K} & \\textsf{W} & \\textsf{D} \\\\ \\hline\n\\multirow{3}{*}{Texture} & \\textsf{Accuracy} & $99.2_{\\pm0.0}$ & $99.2_{\\pm0.0}$ & $99.0_{\\pm0.1}$ & $99.1_{\\pm0.0}$ & \\bm{$99.4_{\\pm0.0}$} & $68.7_{\\pm41}$ & $98.7_{\\pm0.1}$ & $99.1_{\\pm0.1}$ \\\\\n & \\textsf{Robustness} & $98.3_{\\pm0.3}$ & $99.0_{\\pm0.0}$ & $99.0_{\\pm0.0}$ & $99.0_{\\pm0.0}$ & \\bm{$99.1_{\\pm0.0}$} & $68.7_{\\pm41}$ & $98.4_{\\pm0.1}$ & $98.8_{\\pm0.1}$ \\\\\n & \\textsf{Invariance} & $92.4_{\\pm0.0}$ & $99.2_{\\pm0.0}$ & \\bm{$100_{\\pm0.0}$} & \\bm{$100_{\\pm0.0}$} & $99.0_{\\pm0.0}$ & $76.0_{\\pm34}$ & $60.7_{\\pm2.9}$ & $35.0_{\\pm6.7}$ \\\\ \\hline\n\\multirow{3}{*}{Rotation} & \\textsf{Accuracy} & $99.2_{\\pm0.0}$ & $99.0_{\\pm0.1}$ & \\bm{$99.3_{\\pm0.0}$} & \\bm{$99.3_{\\pm0.0}$} & $99.0_{\\pm0.0}$ & $98.8_{\\pm0.0}$ & $98.5_{\\pm0.4}$ & $98.9_{\\pm0.0}$ \\\\\n & \\textsf{Robustness} & $28.9_{\\pm0.6}$ & $93.6_{\\pm0.3}$ & \\bm{$95.2_{\\pm0.1}$} & $95.1_{\\pm0.1}$ & $93.5_{\\pm0.1}$ & $94.5_{\\pm0.2}$ & $92.3_{\\pm0.8}$ & $93.2_{\\pm0.7}$ \\\\\n & \\textsf{Invariance} & $20.6_{\\pm0.4}$ & $58.3_{\\pm2.2}$ & $66.0_{\\pm3.8}$ & $65.4_{\\pm3.5}$ & $29.1_{\\pm0.6}$ & \\bm{$71.9_{\\pm2.8}$} & $48.7_{\\pm1.9}$ & $39.3_{\\pm6.9}$ \\\\ \\hline\n\\multirow{3}{*}{Contrast} & \\textsf{Accuracy} & $99.2_{\\pm0.0}$ & $98.9_{\\pm0.3}$ & \\bm{$99.4_{\\pm0.0}$} & \\bm{$99.4_{\\pm0.0}$} & $99.2_{\\pm0.0}$ & $98.9_{\\pm0.0}$ & $98.7_{\\pm0.0}$ & $99.1_{\\pm0.0}$ \\\\\n & \\textsf{Robustness} & $26.0_{\\pm1.0}$ & $95.4_{\\pm2.6}$ & $96.8_{\\pm0.8}$ & $97.4_{\\pm0.6}$ & \\bm{$97.9_{\\pm0.4}$} & $88.4_{\\pm4.5}$ & $87.2_{\\pm9.6}$ & $97.7_{\\pm0.6}$ \\\\\n & \\textsf{Invariance} & $20.7_{\\pm1.1}$ & $37.5_{\\pm6.9}$ & \\bm{$41.4_{\\pm0.3}$} & $41.3_{\\pm0.4}$ & $26.3_{\\pm1.1}$ & $40.3_{\\pm0.9}$ & $28.4_{\\pm1.7}$ & $20.0_{\\pm0.1}$ \\\\ \\hline\n\\end{tabular}\n\\label{tab:mnist:consistency}\n\\end{table*}",
            "tab:cifar10:consistency": "\\begin{table*}[t]\n\\small \n\\centering \n\\caption{Test results on CIFAR10 dataset for different ARs over three evaluation metrics and three distribution shifts. \nNotations are the same as in Table~\\ref{tab:mnist:consistency}.\n}\n\\setlength\\tabcolsep{5pt}\n\\begin{tabular}{cccccccccc}\n\\hline\n &  & \\textsf{B} & \\textsf{V} & \\textsf{L} & \\textsf{S} & \\textsf{C} & \\textsf{K} & \\textsf{W} & \\textsf{D} \\\\ \\hline\n\\multirow{3}{*}{Texture} & \\textsf{Accuracy} & \\bm{$88.5_{\\pm1.7}$} & $86.3_{\\pm0.3}$ & $82.8_{\\pm0.4}$ & $82.0_{\\pm0.0}$ & $86.8_{\\pm0.1}$ & $84.6_{\\pm0.4}$ & $86.8_{\\pm0.1}$ & $86.5_{\\pm0.4}$ \\\\\n & \\textsf{Robustness} & $38.3_{\\pm0.7}$ & $76.5_{\\pm0.0}$ & $79.1_{\\pm0.2}$ & \\bm{$79.4_{\\pm0.1}$} & $76.8_{\\pm0.1}$ & $75.6_{\\pm0.1}$ & $76.8_{\\pm0.1}$ & $77.3_{\\pm0.2}$ \\\\\n & \\textsf{Invariance} & $44.7_{\\pm0.5}$ & $94.1_{\\pm0.6}$ & \\bm{$100_{\\pm0.0}$} & \\bm{$100_{\\pm0.0}$} & $94.2_{\\pm1.2}$ & $96.7_{\\pm1.0}$ & $93.4_{\\pm0.4}$ & $95.4_{\\pm0.8}$ \\\\ \\hline\n\\multirow{3}{*}{Rotation} & \\textsf{Accuracy} & \\bm{$88.5_{\\pm1.7}$} & $81.1_{\\pm2.3}$ & $80.3_{\\pm4.7}$ & $78.1_{\\pm2.2}$ & $80.7_{\\pm1.8}$ & $80.4_{\\pm6.8}$ & $87.5_{\\pm1.6}$ & $83.7_{\\pm4.0}$ \\\\\n & \\textsf{Robustness} & $15.3_{\\pm1.1}$ & $49.0_{\\pm0.4}$ & $50.7_{\\pm2.6}$ & \\bm{$51.1_{\\pm1.0}$} & $47.0_{\\pm0.6}$ & $46.6_{\\pm4.5}$ & $49.4_{\\pm0.9}$ & $47.7_{\\pm1.7}$ \\\\\n & \\textsf{Invariance} & $47.3_{\\pm0.6}$ & $54.6_{\\pm2.0}$ & $40.7_{\\pm1.4}$ & $55.2_{\\pm1.6}$ & \\bm{$55.7_{\\pm1.1}$} & $55.5_{\\pm1.0}$ & \\bm{$55.7_{\\pm1.3}$} & $54.5_{\\pm0.7}$ \\\\ \\hline\n\\multirow{3}{*}{Contrast} & \\textsf{Accuracy} & $88.5_{\\pm1.7}$ & $85.2_{\\pm4.5}$ & $88.8_{\\pm1.3}$ & $86.9_{\\pm2.0}$ & $89.6_{\\pm0.9}$ & $83.4_{\\pm3.7}$ & $87.2_{\\pm2.2}$ & \\bm{$89.7_{\\pm0.9}$} \\\\\n & \\textsf{Robustness} & $54.5_{\\pm0.8}$ & $77.7_{\\pm0.8}$ & $83.1_{\\pm1.3}$ & \\bm{$83.4_{\\pm1.1}$} & $80.7_{\\pm2.4}$ & $79.5_{\\pm2.9}$ & $82.8_{\\pm1.0}$ & $80.8_{\\pm5.6}$ \\\\\n & \\textsf{Invariance} & $53.1_{\\pm1.6}$ & $67.5_{\\pm0.1}$ & $69.8_{\\pm4.8}$ & \\bm{$71.3_{\\pm2.3}$} & $53.6_{\\pm4.7}$ & $73.0_{\\pm2.2}$ & $74.6_{\\pm0.8}$ & $66.6_{\\pm4.3}$ \\\\ \\hline\n\\end{tabular}\n\\label{tab:cifar10:consistency}\n\\end{table*}",
            "tab:real:c:app": "\\begin{table*}[]\n\\centering\n{\\tiny    \n\\begin{tabular}{cc|ccc|cccc|cccc|cccc|c}\n\\hline\n\\multirow{2}{*}{} & \\multirow{2}{*}{Clean} & \\multicolumn{3}{c|}{Noise} & \\multicolumn{4}{c|}{Blur} & \\multicolumn{4}{c|}{Weather} & \\multicolumn{4}{c|}{Digital} & \\multirow{2}{*}{mCE} \\\\ \\cline{3-17}\n &  & Gauss & Shot & Impulse & Defocus & Glass & Motion & Zoom & Snow & Frost & Fog & Bright & Contrast & Elastic & Pixel & JPEG &  \\\\ \\hline\n\\base{} & 23.9 & 79 & 80 & 82 & 82 & 90 & 84 & 80 & 86 & 81 & 75 & 65 & 79 & 91 & 77 & 80 & 80.6 \\\\\n%\\va{} & 23.7 & 79 & 80 & 79 & 75 & 87 & 80 & 79 & 78 & 76 & 69 & 58 &70 &86 &73 &75 & 76.3 \\\\\n\\ra{} & 23.6 & 78 & 78 & 79 & 74 & 87 & 79 & 76 & 78 & 75 & 69 & 58 & 68 & 85 & 75 & 75 & 75.6 \\\\\n%\\rwa{} & 23.1 & 76 & 77 & 78 & 71 & 86 & 76 & 75 & 75 & 73 & 66 & 55 & 68 & 83 & 76 & 73 & 73.9 \\\\\n\\rwa{} & 22.4 & 61 & 63 & 63 & 68 & 75 & 65 & 66 & 70 & 69 & 64 & 56 & 55 & 70 & 61 & 63 & 64.6 \\\\\n\\textsf{SU} & 24.5 & 67 & 68 & 70 & 74 & 83 & 81 & 77 & 80 & 74 & 75 & 62 & 77 & 84 & 71 & 71 & 74.3 \\\\\n\\textsf{AA} & 22.8 & 69 & 68 & 72 & 77 & 83 & 80 & 81 & 79 & 75 & 64 & 56 & 70 & 88 & 57 & 71 & 72.7 \\\\\n\\textsf{MBP} & 23 & 73 & 74 & 76 & 74 & 86 & 78 & 77 & 77 & 72 & 63 & 56 & 68 & 86 & 71 & 71 & 73.4 \\\\\n\\textsf{SIN} & 27.2 & 69 & 70 & 70 & 77 & 84 & 76 & 82 & 74 & 75 & 69 & 65 & 69 & 80 & 64 & 77 & 73.3 \\\\\n\\textsf{AM} & 22.4 & 65 & 66 & 67 & 70 & 80 & 66 & 66 & 75 & 72 & 67 & 58 & 58 & 79 & 69 & 69 & 68.4 \\\\\n\\textsf{AMS} & 25.2 & 61 & 62 & 61 & 69 & 77 & 63 & 72 & 66 & 68 & 63 & 59 & 52 & 74 & 60 & 67 & 64.9 \\\\ \\hline\n\\end{tabular}\n}\n\\caption{Comparison to advanced models over ImageNet-C data. Performance reported (mCE) follows the standard in ImageNet-C data: mCE is the smaller the better.}\n\\label{tab:real:c:app}\n\\end{table*}",
            "tab:real:sketch:app": "\\begin{table*}[]\n\\small\n\\centering \n\\begin{tabular}{cccccccccc}\n\\hline\n & \\base{} & \\textsf{InfoDrop} & \\textsf{HEX} & \\textsf{PAR} & \\va{} & \\ra{} & \\textsf{RSC} & \\vwa{} & \\rwa{}   \\\\ \\hline\nTop-1 & 0.1204 & 0.1224 & 0.1292 & 0.1306 & 0.1362& 0.1405&  0.1612 &0.1432 & 0.1486 \\\\\nTop-5 & 0.2408 & 0.256 & 0.2564 & 0.2627 &0.2715 &0.2793  &  0.3078& 0.2846 & 0.2933\\\\ \\hline\n\\end{tabular}\n\\caption{Comparison to advanced cross-domain image classification models, over ImageNet-Sketch dataset. We report top-1 and top-5 accuracy following standards on ImageNet related experiments. \n}\n\\label{tab:real:sketch:app}\n\\end{table*}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n    \\rp(\\wt) =  \\mathbb{E}_{(\\x, \\y) \\sim \\mP} \\mI[g(\\fxt) \\neq \\y], \n    \\label{eq:accuracy}\n\\end{align}",
            "eq:2": "\\begin{align}\n    \\rpa(\\wt) =  \\mathbb{E}_{(\\x, \\y) \\sim \\mP} \\max_{a\\sim \\ma}\\mI[g(\\faxt) \\neq \\y],\n    \\label{eq:robustness}\n\\end{align}",
            "eq:3": "\\begin{align}\n    I_{\\mP, \\ma}(\\wt) = \\sup_{a_1, a_2\\in \\ma} D (\\mathbf{Q}_{a_1(\\x), \\wt}, \\mathbf{Q}_{a_2(\\x), \\wt}), \n    \\label{eq:invariance}\n\\end{align}",
            "eq:4": "\\begin{align}\n    \\rp(\\wt) \\leq \\wrp(\\wt) + \\phi(|\\Theta|, n, \\delta)\n    \\label{eq:standard}\n\\end{align}"
        }
    }
}