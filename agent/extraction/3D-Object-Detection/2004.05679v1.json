{
    "meta_info": {
        "title": "MLCVNet: Multi-Level Context VoteNet for 3D Object Detection",
        "abstract": "In this paper, we address the 3D object detection task by capturing\nmulti-level contextual information with the self-attention mechanism and\nmulti-scale feature fusion. Most existing 3D object detection methods recognize\nobjects individually, without giving any consideration on contextual\ninformation between these objects. Comparatively, we propose Multi-Level\nContext VoteNet (MLCVNet) to recognize 3D objects correlatively, building on\nthe state-of-the-art VoteNet. We introduce three context modules into the\nvoting and classifying stages of VoteNet to encode contextual information at\ndifferent levels. Specifically, a Patch-to-Patch Context (PPC) module is\nemployed to capture contextual information between the point patches, before\nvoting for their corresponding object centroid points. Subsequently, an\nObject-to-Object Context (OOC) module is incorporated before the proposal and\nclassification stage, to capture the contextual information between object\ncandidates. Finally, a Global Scene Context (GSC) module is designed to learn\nthe global scene context. We demonstrate these by capturing contextual\ninformation at patch, object and scene levels. Our method is an effective way\nto promote detection accuracy, achieving new state-of-the-art detection\nperformance on challenging 3D object detection datasets, i.e., SUN RGBD and\nScanNet. We also release our code at https://github.com/NUAAXQ/MLCVNet.",
        "author": "Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, Jun Wang",
        "link": "http://arxiv.org/abs/2004.05679v1",
        "category": [
            "cs.CV",
            "cs.GR"
        ],
        "additionl_info": "To be presented at CVPR 2020"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n3D object detection is becoming an active research topic in both computer vision and computer graphics.\nCompared to 2D object detection in RGB images, predicting 3D bounding boxes in real world environments captured by point clouds is more essential for many tasks~\\cite{song2016deep} such as indoor robot navigation~\\cite{mccormac2018fusion++}, robot grasping~\\cite{wang2019densefusion}, etc.\nHowever, the unstructured data in point clouds makes the detection more challenging than in 2D. In particular, the popular convolutional neural networks (CNNs), which are highly successful in 2D object detection, are difficult to be applied to point clouds directly.\n\n\n\nGrowing interests have been attracted to tackle this challenge.\nWith the emergence of deep 3D points processing networks, such as~\\cite{qi2017pointnet,qi2017pointnet++}, several deep learning based 3D object detection works have been proposed recently to detect objects directly from 3D point clouds~\\cite{hou20193d,qi2019deep}.\nThe most recent work VoteNet~\\cite{qi2019deep} proposed an end-to-end 3D object detection network on the basis of Hough voting.\nVoteNet transfers the traditional Hough voting procedure into a regression problem implemented by a deep network, and samples\na number of seed points from the input point cloud to generate patches voting for potential object centers.\nThe voted centers are then used to estimate the 3D bounding boxes.\nThe voting strategy enables VoteNet to significantly reduce the searching space and achieve the state-of-the-art results in several benchmark datasets.\nHowever, treating every point patch and object individually, VoteNet lacks the consideration of the relationships between different objects and between objects and the scene they belong to, which limits its detection accuracy.\n\n\n\n\nAn example can be seen in Fig.~\\ref{fig:context_illu}. Point clouds, captured by e.g. depth cameras,\noften contain noisy and missing data. This together with indoor occlusions makes it difficult even for humans to recognize what and where an object is in Fig.~\\ref{fig:context_illu}(a). Nevertheless, considering the surrounding contextual information in Figs.~\\ref{fig:context_illu}(b-d), it is much easier to recognize it is a chair given the surrounding chairs and the table in the dining room scene.\nActually, the representation of a scanned point set could be ambiguous when it is presented individually,\ndue to lack of color appearance and data missing problems.\nTherefore, we argue that indoor depth scans are often so occluded that contexts could even play a more important role in recognizing objects than the point data itself.\nThis contextual information has been demonstrated to be helpful in a variety of computer vision tasks, including object detection~\\cite{hu2018relation,yu2016role}, image semantic segmentation~\\cite{zhang2019co,fu2019dual} and 3D scene understanding~\\cite{zhang2014panocontext,zhang2017deepcontext}.\nIn this paper, we show how to leverage the contextual information in 3D scenes to boost the performance of 3D object detection from point clouds.\n\n\nIn our view, contextual information for 3D object detection consists of multiple levels.\nAt the lowest is the patch level where the data missing problem is mitigated with a weighted sum over similar point patches to assist more accurate voting of object centers.\nAt the object level, coexistence of objects provides strong hints on detection of certain objects.\nFor example, as shown in Fig.~\\ref{fig:context_illu}(d), the detected table can give a tendency for chairs to be detected at surrounding points.\nAt the scene level, global scene clues can also prevent an object from being detected in an improper scene.\nFor example, we will not expect to detect a bed in a kitchen.\nThe contexts at different levels complement each other and are utilized together to assist the correct inference of objects in noisy and cluttered environments.\n\n\n\n\n\nWe thus propose a novel 3D object detection framework, called Multi-Level Context VoteNet (MLCVNet), to incorporate into VoteNet\nmulti-level contextual information for 3D object detection.\nSpecifically, we propose a unified network to model the multi-level contexts, from local point patches to global scenes.\nThe difference between VoteNet and the proposed network is highlighted in Fig.~\\ref{fig:votenet_vs_mlcvnet}.\nTo model the contextual information, three sub-modules are proposed in the framework, i.e., patch-to-patch context (PPC) module, object-to-object context (OOC) module and the global scene context (GSC) module.\nIn particular, similar to~\\cite{zhang2019pcan}, we use the self-attention mechanism to model the relationships between elements in both PPC and OOC modules.\nThese two sub-modules aim at adaptively encoding contextual information at the patch and object levels, respectively.\nFor the scene-level, we design a new branch as shown in Fig.~\\ref{fig:votenet_vs_mlcvnet}(c) to fuse multi-scale features to equip the network with the ability of learning global scene context.\nIn summary, the contributions of this paper include:\n\\begin{compactitem}\n\\item We propose the first 3D object detection network that exploits \\emph{multi-level} contextual information at patch, object and global scene levels.\n\\item We design three sub-modules, including two self-attention modules and a multi-scale feature fusion module, to capture the contextual information at multiple levels in 3D object detection. The new modules nicely fit in the state-of-the-art VoteNet framework. Ablation study demonstrates the effectiveness of these modules in improving detection accuracy.\n\\item Extensive experiments demonstrate the benefits of multi-level contextual information. The proposed network outperforms state-of-the-art methods on both SUN RGB-D and ScanNetV2 datasets.\n\\end{compactitem}\n\n\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{related_work}\n",
                "subsection 2.1": {
                    "name": "3D Object Detection From Point Clouds",
                    "content": "\nObject detection from 2D images has been studied for decades.\nSince the development of deep convolutional neural networks (DCNNs)~\\cite{krizhevsky2012imagenet}, both the accuracy and efficiency of 2D object detection have been significantly improved by deep learning techniques~\\cite{girshick2015fast,ren2015faster}.\nCompared to 2D, 3D object detection was dominated by non-deep learning based methods~\\cite{nan2012search,li2015database,wang2016cluttered} until the recent couple of years.\nWith the development of deep learning on 3D point clouds~\\cite{wang2017cnn,li2018pointcnn,atzmon2018point}, many deep learning based 3D object detection architectures have emerged~\\cite{chen2016monocular,chen2017multi,lahoud20172d}.\nHowever, most of these methods depend on using 2D detectors as an intermediate step, which restricts their generalization to situations where 2D detectors do not work well~\\cite{qi2018frustum}.\nTo address this issue, several deep learning based 3D detectors  which directly take raw point clouds as input have been proposed  recently~\\cite{zhou2018voxelnet,yang2019learning,hou20193d}.\nIn~\\cite{shi2019pointrcnn}, the authors introduced a two-stage 3D object detector, PointRCNN.\nTheir method first generates several 3D bounding box proposals, and then refines these proposals to obtain the final detection results.\nInstead of directly treating 3D object proposal generation as a bounding box regression problem, in~\\cite{yi2019gspn}, a novel 3D object proposal approach was proposed by taking an analysis-by-synthesis strategy and reconstructing 3D shapes from point clouds.\nInspired by the Hough voting strategy for 2D object detection in~\\cite{leibe2004combined}, the work in~\\cite{qi2019deep} presents an end-to-end trainable 3D object detection network, which directly deals with 3D point clouds, by virtue of the huge success in PointNet/PointNet++~\\cite{qi2017pointnet,qi2017pointnet++}.\nAlthough a lot of methods have been proposed recently, there is still large room for improvement especially for real-world challenging cases.\nPrevious works largely ignored contextual information, i.e., relationships within and between objects and scenes.\nIn this work, we show how to leverage the contextual information to improve the accuracy of 3D object detection.\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Contextual Information",
                    "content": "\nThe work in~\\cite{mottaghi2014role} has demonstrated that contextual information has significant positive effect on 2D semantic segmentation and object detection.\nSince then, contextual information has been successfully employed to improve performance on many tasks such as 2D object detection~\\cite{yu2016role,hu2018relation,liu2018structure}, 3D point matching~\\cite{deng2018ppfnet}, point cloud semantic segmentation~\\cite{engelmann2017exploring,ye20183d}, and 3D scene understanding~\\cite{zhang2014panocontext,zhang2017deepcontext}.\nThe work in~\\cite{hu2018semantic} achieves reasonable results on instance segmentation of 3D point clouds via analyzing point patch context.\nIn~\\cite{shi2019hierarchy}, a recursive auto-encoder based approach is proposed to predict 3D object detection via exploring hierarchical context priors in 3D object layout.\nInspired by the self-attention idea in natural language processing~\\cite{vaswani2017attention}, recent works connect the self-attention mechanism with contextual information mining to improve scene understanding tasks such as image recognition~\\cite{hu2018squeeze}, semantic segmentation~\\cite{fu2019dual} and point cloud recognition~\\cite{xie2018attentional}.\nAs to 3D point data processing, the work in~\\cite{zhang2019pcan} proposes to utilize the attention network to capture the contextual information in 3D points.\nSpecifically, it presents a point contextual attention network to encode local features into a global descriptor for point cloud based retrieval.\nIn~\\cite{paigwar2019attentional}, an attentional PointNet is proposed to search regions of interest instead of processing the whole input point cloud, when detecting 3D objects in large-scale point clouds.\nDifferent from previous works, we are interested in exploiting the combination of \\emph{multi-level} contextual information for 3D object detection from point clouds.\nIn particular, we integrate two self-attention modules and one multi-scale feature fusion module into a deep Hough voting network to learn multi-level contextual relationships between patches, objects and the global scene.\n\n\n%-------------------------------------------------------------------------\n"
                }
            },
            "section 3": {
                "name": "Approach",
                "content": "\n\\label{method}\nAs shown in Fig.~\\ref{fig:network}, our MLCVNet contains four main components: a fundamental 3D object detection framework based on VoteNet\nwhich follows the architecture in~\\cite{qi2019deep}, and three context encoding modules.\nThe PPC (patch-patch context) module\ncombines the point groups to encode the patch correlation information, which helps to vote for more accurate object centers.\nThe OOC (object-object context) module\nis for capturing the contextual information between object candidates.\nThis module helps to improve the results of 3D bounding box regression and classification.\nThe GSC (global scene context) module\nis to integrate the global scene contextual information.\nIn brief, the proposed three sub-modules are designed to capture complementary contextual information in 3D object detection at multiple levels, with the aim to improve the detection performance in 3D point clouds.\n\n",
                "subsection 3.1": {
                    "name": "VoteNet",
                    "content": "\n\\label{Vote}\nVoteNet~\\cite{qi2019deep} is the baseline of our work.\nAs illustrated in Fig.~\\ref{fig:votenet_vs_mlcvnet}, it is an end-to-end trainable 3D object detection network consisting of three main blocks: \\emph{point feature extraction}, \\emph{voting}, and \\emph{object proposal and classification}.\n\n\nTo extract point features, PointNet++ is used as the backbone network for seed sampling and extracting high dimensional features for the seed points from the raw input point cloud.\nThe features of each seed point contain information from its surrounding points within a radius as illustrated in Fig.~\\ref{fig:ppc}(a).\nAnalogous to regional patches in 2D, we thus call these seed points \\textit{point patches} in the remaining of this paper.\nThe voting block takes the point patches with extracted features as input and regresses object centers.\nThis center point prediction is performed by a multi-layer perceptron  (MLP) which simulates the Hough voting procedure.\nClusters are then generated by grouping the predicted centers, and form object candidates, from which the 3D bounding boxes are then proposed and classified through another MLP layer.\n\n\nNote that in VoteNet, both the point patches and the object candidates are processed independently, ignoring the surrounding patches or objects.\nHowever, we argue that relationships between these elements (i.e., point patches and object candidates) are useful information for object detection.\nThus, we introduce our MLCVNet\nto encode these relationships.\nOur detection network follows the general framework\nof VoteNet,\nbut integrates three new sub-modules to capture multi-level contextual information.\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "PPC Module",
                    "content": "\n\\label{PPC}\nWe consider relationships between point patches as the first level of context, i.e., patch-patch context (PPC), as shown in Fig.~\\ref{fig:ppc}(a).\nAt this level, contextual information between point patches,\non the one hand, helps relieve the data missing problem via gathering supplementary information from similar patches.\nOn the other hand, it considers inter-relationships between patches for voting~\\cite{wang2013learning} by aggregating voting information from both the current point patch and all the other patches.\nWe thus propose a sub-network, PPC module, to capture the relationships between point patches.\nFor each point patch, the basic idea is to employ a self-attention module to aggregate information from all the other patches before sending it to the voting stage.\n\n\n\n\n\n\n\n\nAs shown in Fig.~\\ref{fig:ppc}(a), after feature extraction using PointNet++, we get a feature map $\\mathbf{A} \\in \\mathbb{R}^{1024 \\times D}$, where $1024$ is the number of point patches sampled from the raw point cloud, and $D$ is the dimension of the feature vector.\nWe intend to generate a new feature map $\\mathbf{A'}$ that encodes the correlation between any two point patches,\nand it can be formulated as the non-local operation:\n\\begin{equation}\\label{NL_formula}\n    \\mathbf{A'}=f(\\theta(\\mathbf{A}), \\phi(\\mathbf{A})) g(\\mathbf{A})\n\\end{equation}\nwhere $\\theta(\\cdot), \\phi(\\cdot), g(\\cdot)$ are three different transform functions, and $f(\\cdot, \\cdot)$ encodes the similarities between any two positions of the input feature.\nMoreover, as shown in~\\cite{hu2018squeeze}, channel correlations in the feature map also contribute to the contextual information modeling in object detection tasks,\nwe thus make use of the compact generalized non-local network (CGNL)~\\cite{yue2018compact} as the attention module to explicitly model rich correlations between any pair of point patches and of any channels in the feature space.\nCGNL requires light computation and little additional parameters, making it more practically applicable.\nAfter the attention module, each row in the new feature map still corresponds to a point patch, but contains not only its own local features, but also the information associated with all the other point patches.\n\n\nThe effectiveness of the PPC module is visualized in Fig.~\\ref{fig:ppc}(b).\nAs shown, with the PPC module, the voted centers are more meaningful with more of them appearing on objects rather than on non-object regions.\nMoreover, the voted centers are more closely clustered compared to those without the module.\nThe results demonstrate that our self-attention based weighted fusion over local point patches can enhance the performance of voting for object centers.\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "OOC Module",
                    "content": "\n\\label{OOC}\nMost existing object detection frameworks detect each object individually.\nVoteNet is no exception, where each cluster is independently fed into the MLP layer to regress its object class and bounding box.\nHowever, combining features from other objects gives more information on the object relationships, which has been demonstrated to be helpful in image object detection~\\cite{chen2018context}.\nIntuitively, objects will get weighted messages from those highly correlated objects.\nIn such a way, the final predicted object result is not only determined by its own individual feature vector but also affected by object relationships.\nWe thus regard the relationships between objects as the second level contextual information, i.e., object-object context (OOC).\n\nWe get a set of vote clusters $\\mathbf{C}=\\left\\{\\mathcal{C}_{1}, \\mathcal{C}_{2}, \\dots, \\mathcal{C}_{K}\\right\\}$ after grouping the voted centers.\n$K$ is the number of generated clusters in this work.\nEach cluster $\\mathcal{C}=\\left\\{v_{1}, v_{2}, \\dots, v_{n}\\right\\}$ is fed into an MLP followed by a max pooling to form a single vector representing the cluster. Here $v_i$ represents the $i$-th vote in $\\mathcal{C}$, and $n$ is the number of votes in $\\mathcal{C}$.\nThen comes the difference from VoteNet. Instead of processing each cluster vector independently to generate a proposal and classification, we consider the relationships between objects.\nSpecifically, we introduce a self-attention module before the proposal and classification step, as shown in Fig.~\\ref{fig:network} (the blue module). Fig.~\\ref{fig:ooc}(a) shows the details inside the OOC module.\nSpecifically, after max pooling, the cluster vectors $\\textbf{{C}}\\in\\mathbb{R}^{K\\times D'}$\nare fed into the CGNL attention module to generate a new feature map to record the affinity between all clusters.\nThe encoding of object relationships can be summarized as:\n\\begin{equation}\\label{OOC_formula}\n  \\mathcal{C}_{OOC}=Attention(\\max _{i=1, \\ldots, n}\\left\\{MLP\\left(v_i\\right)\\right\\})\n\\end{equation}\nwhere $\\mathcal{C}_{OOC}$ is the enhanced feature vector in the new feature map $\\textbf{{C}}_{OOC}\\in \\mathbb{R}^{K\\times D'}$, and $Attention(\\cdot)$ is the CGNL attention mapping.\nBy doing so, the contextual relationships between these clusters (objects) are encoded into the new feature map.\n\nThe effectiveness of the OOC module is visualized in  Fig.~\\ref{fig:ooc}(b).\nAs shown, with the OOC module, there are fewer detected objects overlapping with each other, and the positions of the detected objects are more accurate.\n\n\n\n\n\n\n\n"
                },
                "subsection 3.4": {
                    "name": "GSC Module",
                    "content": "\n\\label{GSC}\nThe whole point cloud usually contains rich scene contextual information which can help enhance the object detection accuracy.\nFor example, it would be highly possible that a chair rather than a toilet is identified when the whole scene is a dining room rather than a bathroom.\nTherefore, we regard the information about the whole scene as the third level context, i.e., global scene context (GSC).\nInspired by the idea of scene context extraction in~\\cite{liu2018structure}, we propose the GSC module (the green module in Fig.~\\ref{fig:network}) to leverage the global scene context information to improve feature representation for 3D bounding box proposal and object classification, without explicit supervision of scenes.\n\nThe GSC module is designed to capture the global scene contextual information by introducing a global scene feature extraction branch.\nSpecifically, we create a new branch with the input from the patch and object levels, concatenating the features at layers before applying self attention in PPC and OOC.\nAs shown in Fig.~\\ref{fig:gsc}(a), at the two layers each row represents a point patch $\\mathcal{P} \\in \\textbf{{P}}=\\left\\{\\mathcal{P}_{1}, \\mathcal{P}_{2}, \\dots, \\mathcal{P}_{M}\\right\\}$ or an object candidate $\\mathcal{C} \\in \\mathcal{\\textbf{C}}=\\left\\{\\mathcal{C}_{1}, \\mathcal{C}_{2}, \\dots, \\mathcal{C}_{K}\\right\\}$, where\n$M$ and $K$ are the numbers of the sampled point patches and clusters, respectively.\nMax-pooling is first applied to get two vectors (i.e., the patch vector and the cluster vector), combining information from all the point patches and object candidates.\nFollowing the idea of multi-scale feature fusion in the contextual modeling strategy of 2D detectors, these two vectors are then concatenated to form a global feature vector.\nAn MLP layer is applied to further aggregate global information, and the output\nis subsequently expanded and combined with the output feature map of the OOC module.\nThis multi-scale feature fusion procedure can be summarized as:\n\\begin{equation}\\label{OOC_formula}\n  \\mathcal{\\textbf{C}}_{new}=MLP([\\max (\\mathcal{\\textbf{C}});\\max (\\mathcal{\\textbf{P}})]) + \\mathcal{\\textbf{C}}_{OOC}\n\\end{equation}\n\nIn this way, the inference of the final 3D bounding boxes and the object classes will consider the compatibility with the scene context, which makes the final prediction more reliable under the effect of global cues.\nAs shown in Fig.~\\ref{fig:gsc}(b), the GSC module effectively reduces false detection in the scene.\n%-------------------------------------------------------------------------\n"
                }
            },
            "section 4": {
                "name": "Results and Discussions",
                "content": "\n\\label{result}\n\n\n\n\n\\renewcommand\\arraystretch{1.2}\n\n\n\n\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "Dataset",
                    "content": "\n\\label{subsection:dataset}\nWe evaluate our approach on SUN RGB-D~\\cite{song2015sun} and ScanNet~\\cite{dai2017scannet} datasets.\nSUN RGB-D is a well-known public RGB-D image dataset of indoor scenes, consisting of 10,335 frames with 3D object bounding box annotations.\nOver 64,000 3D bounding boxes are given in the entire dataset.\nAs described in~\\cite{zhang2017deepcontext}, these scenes were mostly taken from household environments with strong context.\nThe occlusion problem is quite severe in SUN RGB-D dataset.\nSometimes, it is even difficult for humans to recognize the objects in the scene when merely a 3D point cloud is given without any color information.\nThus, it is a challenging dataset for 3D object detection.\n\n\nScanNet dataset contains 1513 scanned 3D indoor scenes with densely annotated meshes.\nThe ground-truth 3D bounding boxes of objects are also provided.\nThe completeness of scenes in ScanNet makes it an ideal dataset for training our network to learn the contextual information at multiple levels.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Training details",
                    "content": "\nOur network is trained end-to-end using an Adam optimizer and batch size 8.\nThe base learning rate is set to $0.01$ for ScanNet dataset and $0.001$ for SUN RGB-D dataset.\nThe network is trained for $220$ epochs on both datasets.\nThe learning rate decay steps are set to $\\{120, 160, 200\\}$ for ScanNet, $\\{100, 140, 180\\}$ for SUN RGB-D, and the decay rates are $\\{0.1, 0.1, 0.1\\}$.\nTraining the model until convergence on one RTX 2080 ti GPU takes around 4 hours on ScanNetV2 and 11 hours on SUN RGB-D.\nDuring training we found the mAP result fluctuates within a small range.\nThus, the mAP results reported in the paper are the mean results over three runs.\n\nFor parameter size, we check the file sizes of the stored PyTorch models for both our method and VoteNet..\nThe model size of our network is $13.9MB$, while VoteNet is $11.2MB$.\nFor training time, VoteNet takes around 40s for 1 epoch with batch size of 8, while ours is around 42s.\nFor inference time, we infer detection for 1 batch and measure the time.\nVoteNet takes around 0.13s, while ours is 0.14s.\nThe times reported here are all tested on ScanNet dataset.\nThese show that our method only slightly increases the complexity.\n\n"
                },
                "subsection 4.3": {
                    "name": "Comparisons with the State-of-the-art Methods",
                    "content": "\nWe first evaluate our method on SUN RGB-D dataset using the same 10 most common object categories as in~\\cite{qi2019deep}.\nTable~\\ref{tab:sunrgbd} gives a quantitative comparison of our method with deep sliding shapes (DSS)~\\cite{song2016deep}, cloud of gradients (COG)~\\cite{ren2016three}, 2D-driven~\\cite{lahoud20172d}, F-PointNet~\\cite{qi2018frustum} and VoteNet~\\cite{qi2019deep}.\n\n\n\n\nRemarkably, our method achieves better overall performance\nthan all the other methods on SUN RGB-D dataset.\nThe overall mAP (mean average precision) of MLCVNet reaches $59.8\\%$ on SUN RGB-D validation set, $2.1\\%$ higher than the current state-of-the-art, VoteNet.\nThe heavy occlusion presented in SUN RGB-D dataset is a challenge for methods (e.g., VoteNet) that consider point patches individually.\nHowever, the utilization of contextual information in MLCVNet helps with the detection of occluded objects with missing parts, which we believe is the reason for the improved detection accuracy.\n\nWe also evaluate our MLCVNet against several more competing approaches, MRCNN 2D-3D~\\cite{he2017mask}, GSPN~\\cite{yi2019gspn} and 3D-SIS~\\cite{hou20193d}, on ScanNet benchmark in Table~\\ref{tab:scannet_evaluate}.\nWe report the detection results on both mAP$@0.25$ and mAP$@0.5$.\nThe mAP$@0.25$ of MLCVNet on ScanNet validation set reaches $64.5\\%$ making $5.9$ absolute points improvement over the best competitor VoteNet, and the mAP$@0.50$ is even higher, making $7.9$ points improvement.\nThe significant improvements confirm the effectiveness of our integration of multi-level contextual information.\nTable~\\ref{tab:scannet_detail} shows the detailed results at mAP$@0.25$ for each object category in ScanNetV2 dataset.\nAs can be seen, for some specific categories, such as shower curtain and window, the improvements exceed 8 points.\nIt is found that plane-like objects, such as door, window, picture and shower curtain, usually get higher improvements.\nThe reason could be that these objects contain more similar point patches, which can be used by the attention module to complement each other to a great extent.\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study",
                    "content": "\nTo quantitatively evaluate the effectiveness of the proposed contextual sub-modules, we conduct experiments with different combinations of these modules.\nThe quantitative results are shown in Table~\\ref{tab:ablation}.\nThe baseline method is the VoteNet.\nWe then add the proposed sub-modules one by one into the baseline model.\nApplying the PPC module leads to improvements in mAP$@0.25$ of $0.8$ and $2.6$.\nThe combination of PPC and OOC modules further improves the evaluation scores to $59.1$ and $63.4$ respectively.\nAs expected, when equipped with all the three sub-modules, the mAP$@0.25$ of our MLCVNet is boosted up to the highest scores on both datasets.\nIt can be seen that contextual information captured by the designed sub-modules indeed brings notable improvements over the state-of-the-art method.\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Qualitative Results",
                    "content": "\nFig.~\\ref{fig:scannet_results} shows qualitative comparison of the results using MLCVNet and VoteNet for 3D bounding box prediction on ScanNetV2 validation set.\nIt is observed that the proposed MLCVNet detects more reasonable objects (red arrows), and predicts more precise boxes (blue arrows).\nThe pink box produced by VoteNet is classified as a window, which is improper to overlap with a door, while our method ensures the compatibility between objects and scenes.\nThe qualitative comparison results on SUN RGB-D are shown in Fig.~\\ref{fig:sun_results}.\nAs shown, our model is still able to produce high-quality boxes even though the scenes are much occluded and less informative.\nAs shown in the bedroom example in Fig.~\\ref{fig:sun_results}, there are overlaps and missing detection (red arrows) using VoteNet, while our model successfully detects all the objects with good precision compared to the ground-truth.\nFor the second scene in Fig.~\\ref{fig:sun_results}, VoteNet misclassifies the table, produces overlaps, and predicts inaccurate boxes (red arrows), while our model produces much cleaner and more accurate results.\nHowever, it is worth noting that our method may still fail in some predictions, such as the overlapped windows in the red square in Fig.~\\ref{fig:scannet_results}.\nTherefore, there is still room for improvements on 3D bounding box prediction when dealing with complicated scenes.\n\n"
                }
            },
            "section 5": {
                "name": "Conclusions",
                "content": "\n\\label{conclusion}\nIn this paper, we propose a novel network that integrates contextual information at multiple levels into 3D object detection.\nWe make use of self-attention mechanism and multi-scale feature fusion to model the multi-level contextual information, and propose three sub-modules. The PPC module encodes the relationships between point patches, the OOC module captures the contextual information of object candidates, and the GSC module aggregates the global scene context.\nAblation studies demonstrate the effectiveness of the proposed contextual sub-modules to improve the detection accuracy. Quantitative and qualitative experiments further demonstrate that our architecture successfully improves the performance of 3D object detection.\n\n\\textbf{Future work.}\nContextual information analysis in 3D object detection still offers huge space for exploration.\nFor example, to enhance the global scene context constraint, one possible way is to use the global feature in the GSC module to predict scene types as an auxiliary learning task, which can explicitly supervise the global feature representation.\nAnother direction would be a more effective mechanism to encode the contextual information as in~\\cite{hu2018relation}.\n%-------------------------------------------------------------------------\n%\\input{4-conclusion}\n%-------------------------------------------------------------------------\n\n"
            },
            "section 6": {
                "name": "Acknowledgment",
                "content": "\nThis work was supported in part by National Natural Science Foundation of China under Grant (61772267, 61572507, 61532003, 61622212), the Fundamental Research Funds for the Central Universities under Grant NE2016004, the National Key Research and Development Program of China (No. 2018AAA0102200) and the Natural Science Foundation of Jiangsu Province under Grant BK20190016.\n\n{\\small\n\\bibliographystyle{unsrt}\n\\bibliography{egbib}\n}\n\n"
            }
        },
        "equations": {
            "eq:NL_formula": "\\begin{equation}\\label{NL_formula}\n    \\mathbf{A'}=f(\\theta(\\mathbf{A}), \\phi(\\mathbf{A})) g(\\mathbf{A})\n\\end{equation}",
            "eq:OOC_formula": "\\begin{equation}\\label{OOC_formula}\n  \\mathcal{\\textbf{C}}_{new}=MLP([\\max (\\mathcal{\\textbf{C}});\\max (\\mathcal{\\textbf{P}})]) + \\mathcal{\\textbf{C}}_{OOC}\n\\end{equation}"
        },
        "git_link": "https://github.com/NUAAXQ/MLCVNet."
    }
}