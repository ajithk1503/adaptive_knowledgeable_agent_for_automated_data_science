{
    "meta_info": {
        "title": "Deep Isolation Forest for Anomaly Detection",
        "abstract": "Isolation forest (iForest) has been emerging as arguably the most popular\nanomaly detector in recent years due to its general effectiveness across\ndifferent benchmarks and strong scalability. Nevertheless, its linear\naxis-parallel isolation method often leads to (i) failure in detecting hard\nanomalies that are difficult to isolate in\nhigh-dimensional/non-linear-separable data space, and (ii) notorious\nalgorithmic bias that assigns unexpectedly lower anomaly scores to artefact\nregions. These issues contribute to high false negative errors. Several iForest\nextensions are introduced, but they essentially still employ shallow, linear\ndata partition, restricting their power in isolating true anomalies. Therefore,\nthis paper proposes deep isolation forest. We introduce a new representation\nscheme that utilises casually initialised neural networks to map original data\ninto random representation ensembles, where random axis-parallel cuts are\nsubsequently applied to perform the data partition. This representation scheme\nfacilitates high freedom of the partition in the original data space\n(equivalent to non-linear partition on subspaces of varying sizes), encouraging\na unique synergy between random representations and random partition-based\nisolation. Extensive experiments show that our model achieves significant\nimprovement over state-of-the-art isolation-based methods and deep detectors on\ntabular, graph and time series datasets; our model also inherits desired\nscalability from iForest.",
        "author": "Hongzuo Xu, Guansong Pang, Yijie Wang, Yongjun Wang",
        "link": "http://arxiv.org/abs/2206.06602v4",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "Accepted by IEEE Transactions on Knowledge and Data Engineering  (TKDE)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "\\IEEEraisesectionheading",
                "content": "{\\section{Introduction}\\label{sec:introduction}}\n% Computer Society journal (but not conference!) papers do something unusual\n% with the very first section heading (almost always called \"Introduction\").\n% They place it ABOVE the main text! IEEEtran.cls does not automatically do\n% this for you, but you can achieve this effect with the provided\n% \\IEEEraisesectionheading{} command. Note the need to keep any \\label that\n% is to refer to the section immediately after \\section in the above as\n% \\IEEEraisesectionheading puts \\section within a raised box.\n\n\n\n\n% The very first letter is a 2 line initial drop letter followed\n% by the rest of the first word in caps (small caps for compsoc).\n% \n% form to use if the first word consists of a single letter:\n% \\IEEEPARstart{A}{demo} file is ....\n% \n% form to use if you need the single drop letter followed by\n% normal text (unknown if ever used by the IEEE):\n% \\IEEEPARstart{A}{}demo file is ....\n% \n% Some journals put the first two words in caps:\n% \\IEEEPARstart{T}{his demo} file is ....\n\n\n\\IEEEPARstart{A}{nomaly} detection has broad applications in various domains, such as detection of insurance fraud and financial crime, surveillance of complex systems like data centres and spacecraft, and identification of attacks and potential threats in cyberspace. \nGiven these important applications, this task has been a popular research topic for decades, and numerous anomaly detection approaches have been introduced \\cite{aggarwal2017outlieranalysis,pang2021survey}. \n\n\n\n% In spite of a plethora of algorithms in the literature, a fifteen-year-old isolation-based method, Isolation Forest (iForest) \\cite{liu2008isolation}, is still competitive and practical in both academia and industry. \nIn recent years, isolation forest (iForest) \\cite{liu2008isolation} has been emerging as arguably the most popular anomaly detector due to its general effectiveness across different benchmarks and strong scalability. \nCompared to many existing methods such as distance/density-based methods, iForest better captures the key essence of anomalies, i.e., ``few and different''. It does not introduce extra assumptions of data characteristics, thus showing consistently effective performance across diverse datasets.\nAlso, iForest is with linear time complexity, which is often a very appealing advantage in many industrial applications when there are large-scale data and strict requirements of time efficiency. \nConcretely, iForest uses an ensemble of isolation trees (iTree), in which each iTree is grown by iteratively branching. Leaf nodes are built by using random cuts in the values of randomly selected features until the data objects are isolated. Data abnormality is estimated according to the average depth traversing from the root node to the isolated leaf node in these iTrees. \n\n\n% Besides, due to the ever growing availability of massive data in practical domains, advantages regarding efficiency, conceptual simplicity, and light computational overhead make iForest much appealing in many real-world applications. \n\n\nNevertheless, an explicit major issue is that it cannot handle hard anomalies (e.g., the anomalies that can be only isolated in higher-order subspaces by looking into the combination of multiple features) because it treats all features separately and considers only one feature per isolation operation. \nFig. \\ref{fig:hardanom} exemplifies this issue with a simple toy example. Anomalies (represented as red triangles) are surrounded by ring-shaped normal samples, which cannot be isolated by either x-axis or y-axis slicing cuts.\nAlthough these anomalies might be finally isolated by multiple cuts, it results in indistinguishable isolation depth in iTrees compared to normal data. \nThe failure of recalling these anomalies induces high false negative errors. \nGiven that anomalies often contain critical information related to potential accidents or faults, those false negatives may cause serious consequences. \nTherefore, this issue has been a major bottleneck hindering the performance of iForest, particularly on datasets with high-dimensional/non-linear-separable data spaces. \n\n\n\nAnother inherent imperfection of iForest is that it assigns unexpectedly low anomaly scores to artefacts introduced by the algorithm itself, which is revealed as the ``ghost region'' problem in \\cite{hariri2019eif}. \nTo clearly demonstrate this issue, we visualise the data distribution of three 2-D synthetic datasets used in \\cite{hariri2019eif} and anomaly score maps generated by iForest in the first two columns in Fig. \\ref{fig:teaser}, respectively. As can be seen from all three anomaly score maps, iForest assigns clearly lower anomaly scores to some artefact regions, i.e., the four rectangular areas centred around the presented data objects in the single-blob dataset (1st row), the upper right and bottom left clique areas in the two-blob dataset (2nd row), and the vertical rectangular areas along the sinusoid in the sinusoidal dataset (3rd row). However, these artefact regions are similar to other regions that contain no samples or have similar radial distances to the presented data objects.\nThe unusually lower anomaly scores are due to the intrinsic algorithmic bias of iForest, i.e., only axis-parallel partitions are admitted in iTree construction. \nInstead, an expected anomaly score map should smoothly approximate circular contour lines w.r.t. the density of the presented data. \nThis problem also leads to false negative errors, i.e., \niForest may fail to detect possible anomalies in these ``ghost regions'' such as the red triangles in Fig. \\ref{fig:teaser} as they are assigned similarly low anomaly scores as some of the presented normal objects and included in the normal areas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere have been a number of extensions of iForest introduced over the years \n\\cite{liu2010detecting,hariri2019eif,gopalan2019pid,lesouple2021generalized,cortes2021revisiting,tokovarov2022probabilistic}. \nThese extensions attempt to devise more advanced isolation methods by (i) using novel hierarchical criteria to selectively pick splitting thresholds and/or dimensions \\cite{gopalan2019pid,tokovarov2022probabilistic,cortes2021revisiting} or (ii) using optimal/random hyper-planes \\cite{liu2010detecting,hariri2019eif,lesouple2021generalized}.\nThese improvements successfully contribute to better detection performance, but their isolation methods are still confined by linear partition.\nInstead, data partition in the isolation process is expected to form non-linearly arbitrary shapes in the data space to handle hard anomalies in complicated datasets and eliminate the algorithmic bias in derived anomaly scores. \nWe show two state-of-the-art extensions (PID \\cite{gopalan2019pid} and EIF\\cite{hariri2019eif}) in Fig. \\ref{fig:teaser}. \nPID \\cite{gopalan2019pid} selectively chooses splitting dimensions and uses similar axis-parallel isolation operations. Thus, its score maps also encounter the same algorithmic bias problem. \nEIF \\cite{hariri2019eif} works better due to the use of hyper-planes with random slopes and intercepts as branching criteria, eliminating the limits of axis-parallel partitions to some extent. However, the partitions still operate linearly, which works ineffectively on challenging datasets where only non-linear partitions are useful in isolating anomalies from the other data. This problem is reflected by the rather wrinkled anomaly score contours in all three datasets in Fig. \\ref{fig:teaser}. \n\n\n\nTherefore, the key to these limitations becomes very clear now, i.e., a step further is required to liberate the isolation method from the underlying linear constraints.\nTo this end, this paper proposes a novel extension of iForest named \\textsc{Deep Isolation Forest} (DIF for short). \nThe key idea in DIF is to harness the strong representation power of neural networks to map the original data into a group of new data spaces, and non-linear isolation can be easily achieved by performing simple axis-parallel partitions upon these newly created data spaces (equivalent to non-linear partition on subspaces of varying sizes in the original data space).\nSpecifically, we propose a novel representation scheme -- random representation ensemble -- produced by optimisation-free deep neural networks. These networks are only casually initialised and do not involve any optimisation or training process. \nDIF then simply utilises random axis-parallel cuts upon these representations. \nThe randomness in our representation scheme allows high freedom of the partition in the original data space, encouraging a unique synergy effect between random representations and random partition-based isolation. \nThis facilitates effective isolation of hard anomalies and eliminates the algorithmic bias, thus significantly enhancing the detection performance.\nFig. \\ref{fig:hardanom} shows four random representations created by DIF, where those hard anomalies are possible to be exposed and easily isolated by using a few axis-parallel cuts. \nBesides, as shown in Fig. \\ref{fig:teaser}, DIF accurately assigns anomaly scores to the presented data objects and has a smooth estimation of the anomaly scores in the other regions, alleviating the aforementioned algorithmic bias problem. \n\n\n\nFurther, in our specific implementation of DIF, we propose the Computation-Efficient Representation Ensemble method (CERE) and the Deviation-Enhanced Anomaly Scoring function (DEAS).\nWith the help of CERE, DIF can efficiently produce the representation ensemble by taking full advantage of parallel accelerators in the mini-batch calculation, largely eliminating the computational overhead. \nDEAS leverages the hidden quantitative information enclosed in the mapped dense representations along with qualitative comparisons. This additional information enables a more accurate assessment of the isolation difficulty of data objects, leading to a better anomaly scoring function. \n\n\n\nOur main contributions are summarised as follows:\n\n\\begin{itemize}\n\t\\item \n\tWe propose the \\textsc{Deep Isolation Forest} (DIF) method. A new isolation method is introduced, which enables non-linear partition on subspaces of varying sizes, offering a more effective anomaly isolation solution than the current methods that are capable of linear isolation only.\n\tThe resulting approach can better handle hard anomalies in complex datasets with high-dimensional/non-linear-separable data spaces and eliminate the algorithmic bias.\n\tWe also show that DIF is a high-level generalisation of iForest and its very recent extension EIF. \n\t\n\t\\item We propose a novel representation scheme, the random representation ensemble, in which only casually initialised neural networks are required. This representation scheme facilitates high freedom of partitions in the original data space.\n\tThe unique synergy between random representations and random partition-based isolation brings excellent randomness and desired diversity into the overall ensemble-based abnormality estimation process. \n\n\t\n\t\\item We propose the representation ensemble method CERE to ensure that DIF can inherit good scalability from iForest. The anomaly scoring function DEAS is introduced to leverage additional quantitative information enclosed in mapped dense representations, enhancing the quality of anomaly scoring of DIF.\n\n\t\\item DIF offers a data-type-agnostic anomaly detection solution. We show that DIF is versatile to detect anomalies in different types of data by simply plugging in corresponding randomly initialised neural networks in the feature mapping. \n\t\n\\end{itemize}\n\n\n\nExtensive experiments on a large collection of real-world datasets, including not only tabular data but graph and time series data, show that: (i) DIF significantly outperforms iForest and its state-of-the-art extensions (Sec. \\ref{sec:effectiveness}); (ii) DIF also achieves remarkable improvement compared to the ensemble of advanced deep anomaly detectors (Sec. \\ref{sec:effectiveness}); (iii) DIF has desired scalability on high-dimensional, large-scale datasets (Sec. \\ref{sec:time}) \nand presents good robustness to anomaly contamination (Sec. \\ref{sec:robustness}); (iv) the significance of the synergy between random representations and random partition-based isolation is justified by comparing to several alternatives (Sec. \\ref{sec:significance}); \nand (v) the contribution of CERE and DEAS is separately verified in our ablation study (Sec. \\ref{sec:ablation}).\n\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Anomaly Detection",
                    "content": "\nAnomaly detection has been intensively studied in the last decades by using different data characteristics like distance, density, clustering membership, or probability \\cite{aggarwal2017outlieranalysis}. \nRecent studies \\cite{pang2018Learning,ruff2018dsvdd,bergman2019classification,wang2021rdp} devise deep anomaly detection models based on the representation learning, while some other methods use concepts like reconstruction \\cite{liu2021rca} or deviation \\cite{pang2019deep} in the deep learning framework as scoring function.\nSurveys and comparative studies can be found in \\cite{pang2021survey,ruff2021unifying,emmott2013systematic}. Also, some practical anomaly detection tools, e.g., \\cite{lai2021tods,alnegheimish2022sintel,zhao2019pyod,ibm}, are developed to facilitate the use of anomaly detection models in real-world applications. \n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Isolation Forest and Its Extensions",
                    "content": "\n% \\cite{emmott2013systematic}, \n\niForest \\cite{liu2008isolation} is a very popular anomaly detection method that identifies anomalies according to the isolating difficulty in the data space. \nAlthough iForest has shown effective performance on several benchmarks, a number of its extensions have been proposed, attempting to fix its notorious vulnerabilities and obtain better detection performance. \n\n\nThe mainstream of these extensions focuses on devising more effective isolation methods. \nSCIF \\cite{liu2010detecting} introduces a non-axis-parallel way as branching criteria, i.e., instead of selecting one feature during splitting, an optimal slicing hyper-plane is used. EIF \\cite{hariri2019eif} also uses hyper-planes but with random slopes and intercepts. \nAnother work \\cite{lesouple2021generalized} further fixes the empty branching problem of EIF \nby choosing splitting thresholds from the range of projected values onto the slope direction. \nPID \\cite{gopalan2019pid} \nselectively picks dimensions that have greater variance and selects split points according to the sparsity of the partition branches. The literature \\cite{tokovarov2022probabilistic} proposes a probability-based method to find better split values than random splitting. \nThese extensions normally achieve better performance by introducing non-axis-parallel and/or heuristic partition.\nHowever, the major problem is that they still rely on \\textit{linear isolation operations}, which means it is also hard to handle complicated data that require non-linear partitions as the isolation method. Also, they suffer from the aforementioned artefact problem due to the implicit algorithmic bias hidden in the isolation process. \nThere are also some extensions employing the nearest neighbour information into isolation process, e.g., LeSiNN \\cite{pang2015lesinn} and iNNE \\cite{bandaragoda2018nearest}. \nThe literature \\cite{zhang2017lshiforest} further uses locally-sensitive hashing to extend the isolation mechanism to any distance measures and data types.\nThese distance-based methods introduce an extra assumption, i.e., anomalies are far from other data objects. However, this assumption does not always hold since clustered anomalies are also very close to their adjacent neighbours \\cite{liu2010detecting}. The detection performance is also sensitive to the choice of distance metrics. \n\n\n\nAnother angle is to enhance the scoring method. The literature \\cite{mensi2021enhanced} introduces a path-weighted scoring method and a probability-based aggregation function. PID \\cite{gopalan2019pid} redefines the scoring function according to the sparsity rather than depth in the tree. Similarly, these extensions are still vulnerable due to their linear isolation methods. \n\n\n\n"
                },
                "subsection 2.3": {
                    "name": "Deep Ensembles",
                    "content": "\n\nDeep ensemble \\cite{lakshminarayanan2017de}, a simple framework that combines prediction results of a group of independently trained networks together, has garnered much interest.\nIt can improve prediction accuracy and provide uncertainty estimation in a simple framework without modifying the original working pipeline.\nSimilarly to other ensemble-based approaches, the quality of deep ensembles also largely hinges on the diversity of its members.\nBesides, albeit simple, deep ensembles still induce considerably larger computational costs. \nThus, many related studies attempt to address these two key limitations, e.g., \\cite{angelo2021repulsive} uses repulsive terms to ensure individual diversity, \n\\cite{rame2021dice} increases diversity \nby adversarially preventing features from being conditionally predictable from each other,\nand \\cite{nam2021diversity} proposes a distilled model that can absorb as much function diversity inside the ensemble as possible.\nIn addition, this framework inspires related studies on anomaly detection. The literature \\cite{chen2017outlier,kieu2019outlier} combines a group of autoencoders and uses the median of reconstruction errors as anomaly scores. To improve member diversity, these autoencoders are sparsely connected. \n\n\nOur work also involves the process of integrating neural networks. From the deep ensemble aspect, our work can effectively tackle the above two key issues. The diversity between ensemble members can be guaranteed and the calculation efficiency can be well maintained since only initialised networks are required.  \nOur work may also provides valuable insights into the deep ensemble research line. \n\n\n\n"
                }
            },
            "section 3": {
                "name": "Problem Statement and Notations",
                "content": "\n\nLet $\\mathcal{D}=\\{\\bm{o}_1, \\cdots, \\bm{o}_N \\}$ be a dataset with $N$ data objects, anomaly detection is to give a scoring function $f: \\mathcal{D} \\mapsto \\mathbb{R}^{N}$ that estimates the abnormality of each data object. Different from many existing studies that only focus on an individual data type, we do not restrict the type of data objects in this work, which means they can be multi-dimensional vectors, time series data, or graphs.\n% , or images. \nThroughout the paper, we use calligraphic fonts for sets, script typeface for functions, bold lowercase letters for\nvectors, bold uppercase letters to denote matrices. Table \\ref{tab:notation} summarises main notations.\n\n\n\n% Table generated by Excel2LaTeX from sheet 'data'\n%\n\n\n\n\n\n\n\n\n\n"
            },
            "section 4": {
                "name": "Preliminaries: Isolation Forest",
                "content": "\n\n\nFor the sake of clarity, we recall the basic procedure of isolation forest (iForest) \\cite{liu2008isolation}. \nA basic structure named isolation tree (iTree for short) is proposed. iTree $\\tau$ is essentially a binary tree, and each node in the tree corresponds to a pool of data objects. \nA subset containing $n$ data objects is used as the data pool of the root node, which is randomly subsampled from the whole dataset. \niTree $\\tau$ grows by recursively isolating data objects in the leaf node (i.e., a disjoint partition of data objects into two child nodes) in a top-down fashion until remaining one data object in the node or reaching the maximum depth limit. \niForest uses a simple isolation method that performs a comparison between the $j$-th dimension of the data object $\\bm{o}^{(j)}$ and a splitting value $\\eta$ as the branching criterion of each data object $\\bm{o}$, where $j$ and $\\eta$ respectively denote a randomly selected feature index and a split value within the range of available values of the $j$-th feature. \nEach data object $\\bm{o}$ has a traversing path $p(\\bm{o} | \\tau)$ in the iTree $\\tau$, and length of the traversed path, $ | p(\\bm{o} | \\tau) |$, can be naturally viewed as an indication of the abnormal degree of $\\bm{o}$ (anomalies are often easier to be isolated with shorter path length). iForest constructs a forest of $T$ iTrees $\\mathcal{T}$=$\\{\\tau_i\\}_{i=1}^{T}$. The anomaly score of data object $\\bm{o}$ is calculated based on its averaged path length $\\mathbb{E}_{\\tau_i \\in \\mathcal{T}}( |p(\\bm{o} | \\tau_i) |)$ over all of the iTrees in the forest $\\mathcal{T}$, i.e., $\\mathscr{F}_{\\text{iFoerst}} (\\bm{o} | \\mathcal{T})\\! =\\! 2^{-\\mathbb{E}_{\\tau_i \\in \\mathcal{T}} \\frac{ |p(\\bm{o} | \\tau_i)| }{C(T)}}$, where $C(T)$ is a normalising factor. \n\n\n\n\niForest uses a linear axis-parallel isolation method that only considers one dimension each time, and existing extension work introduces hyper-plane-based isolation that involves multiple dimensions, but similarly, only \\textit{linear partition is admitted}.\nThese current isolation methods are limited to effectively handle hard anomalies that cannot be isolated using linear partitions on individual features or simple combinations of multiple features. \nAdditionally, these existing methods generally suffer from the algorithmic bias brought by constraints hidden in their isolation strategies.  \n\n\n\n"
            },
            "section 5": {
                "name": "Deep Isolation Forest",
                "content": "\n\n\n\n\nWe require a new isolation method that is unleashed from the above constraints so that it can effectively isolate those hard anomalies and avoid algorithmic bias. \nTo this end, we introduce the \\textsc{Deep Isolation Forest} (DIF) method. \nIn a nutshell, DIF constructs an ensemble of representations derived from deep neural networks, and simple axis-parallel isolation is operated upon new data spaces to build iTrees in the forest. Instead of following the deep ensemble framework that combines independently trained neural networks, we use an ensemble of random representations produced by optimisation-free neural networks that only require simple casual initialisation.\nThis new representation scheme allows high freedom of the partition in the original data space. Each feature in the newly projected representation space is based on non-linear interactions among a number of original features, meaning that axis-parallel partitions of new space can form effective non-linear cuts in the original space. This way successfully liberates the slicing cuts from current linear constraints. Those hard anomalies that cannot be easily isolated in the original data space are possible to be exposed in representation spaces, and they can be isolated via fewer cuts and lead to clearer abnormality. \nMeanwhile, a unique synergy between random representations and random partition-based isolation can facilitate the overall ensemble-based abnormality estimation. \n\n\n\n\n\n\n\n\n\n",
                "subsection 5.1": {
                    "name": "Formulation of DIF",
                    "content": "\n\nDIF first produces the random representation ensemble via optimisation-free neural networks, which is defined as\n\\begin{equation}\\label{eqn:rep}\n\t\\mathscr{G}(\\mathcal{D}) = \\big\\{ \\mathcal{X}_u \\subset \\mathbb{R}^d \\big \\vert  \\mathcal{X}_u = \\phi_u(\\mathcal{D};\\theta_u)  \\big\\}_{u=1}^{r},\n\\end{equation}\nwhere $r$ is the ensemble size, $\\phi_u:\\mathcal{D}\\!\\mapsto \\!\\mathbb{R}^d$ is the network that maps original data into new $d$-dimensional spaces, and the network weights in $\\theta_u$ are randomly initialised.\n% and do not involve any optimisation or training process. \nEach representation is assigned with $t$ iTrees, and a forest $\\mathcal{T}$=$\\{ \\tau_i \\}_{i=1}^{T}$ containing $T$=$r$$\\times$$t$ iTrees is constructed. \niTree $\\tau_i$ of $\\mathcal{X}$ is initialised by a root node with a set of projected data $\\mathcal{P}_{1}$ $\\subset$ $\\mathcal{X}$.\nThe $k$-th node with the data pool $\\mathcal{P}_{k}$ is branched into two leaf nodes with disjoint subsets, i.e., $\\mathcal{P}_{2k} \\!=\\! \\{\\bm{x} | \\bm{x}^{(j_k)} \\leq \\eta_k,   \\bm{x} \\in \\mathcal{P}_{k}  \\}$ and $\\mathcal{P}_{2k+1} \\!=\\! \\{\\bm{x} | \\bm{x}^{(j_k)} \\!>\\! \\eta_k,  \\bm{x} \\in \\mathcal{P}_{k}   \\}$, \nwhere $j_k$ is selected uniformly at random among all the dimensions of the newly created data space $\\{1, \\cdots, d\\}$, $\\bm{x}^{(j_k)}$ is the $j_k$-th dimension of the projected data object, and $\\eta_k$ is a split value within the range \n% between the minimum and the maximum value of the set \nof $\\{ \\bm{x}^{(j_k)} \\vert  \\bm{x} \\in \\mathcal{P}_{k}  \\}$. \n\nAfter constructing $\\mathcal{T}$, the abnormality of a data object $\\bm{o}$ is evaluated by the isolation difficulty in each iTree of the forest $\\mathcal{T}$. The scoring function is defined as\n\\begin{equation}\\label{eqn:score_func_class}\n\t\\mathscr{F}(\\bm{o} | \\mathcal{T}) = \\Omega_{\\tau_i \\sim \\mathcal{T}} I(\\bm{o} | \\tau_i),\n\\end{equation}\nwhere $I(\\bm{o} | \\tau_i)$ denotes a function to measure the isolation difficulty in iTree $\\tau_i$, and $\\Omega$ denotes an integration function. \n\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Implementation of DIF",
                    "content": "\n\nThere are two main components in DIF, i.e., random representation ensemble function $\\mathscr{G}$ and isolation-based anomaly scoring function $\\mathscr{F}$. \nTo improve the time efficiency of representation function $\\mathscr{G}$, we propose Computation-Efficient deep Representation Ensemble method (CERE), in which all the ensemble members can be computed simultaneously in a given mini-batch.\nTo further improve the accuracy of anomaly scoring, we propose Deviation-Enhanced Anomaly Scoring function (DEAS) by leveraging the hidden quantitative information enclosed in the projected dense representations along with qualitative comparisons.\n\n\n",
                    "subsubsection 5.2.1": {
                        "name": "CERE: Computation-efficient Deep Representation Ensemble Method",
                        "content": "\n\nSuccessively feeding raw data into $r$ independent networks in Eq. (\\ref{eqn:rep}) can induce a considerably high memory and time overhead. To inherit outstanding scalability of the original iForest, we introduce CERE to efficiently implement the representation ensemble function $\\mathscr{G}(\\mathcal{D})$. \n\n\nLet $\\mathbf{W}\\in \\mathbb{R}^{m \\times n}$ be a weight matrix of a neural network layer, then following \\cite{wen2019batchensemble}, we use a tuple of small random vectors $\\bm{p}_i \\in \\mathbb{R}^m$ and $\\bm{q}_i \\in \\mathbb{R}^n$ to yield a rank-one matrix via multiplication, which is used to derive the full weight matrices of each ensemble member. \nFormally, based on a base weight matrix $\\mathbf{W}_0$, the weight matrix $\\mathbf{W}_i$ of the $i$-th ensemble member is generated as\n\\begin{equation}\n\t\\mathbf{W}_i = \\mathbf{W}_0 \\circ (\\bm{p}_i \\bm{q}_i^\\top ) ,\n\\end{equation}\nwhere $\\circ$ denotes the Hadamard product.\n\nThe mapping process of incoming neurons $\\bm{x}\\in \\mathbb{R}^{m}$ and the weight $\\mathbf{W}_i$ can be further derived as follows:\n\\begin{equation}\n\t\\begin{split}\n\t\t\\mathbf{W}_i^{\\top} \\bm{x}   &= (\\mathbf{W}_0 \\circ \\bm{p}_i \\bm{q}_i^\\top)^{\\top} \\bm{x} \\\\\n\t\t&= \\mathbf{W}_0^{\\top} (\\bm{x} \\circ \\bm{p}_i) \\circ \\bm{q}_i .\n\t\\end{split}\n\\end{equation}\n\n\nGiven $r$ tuples of weight vectors $\\{ \\langle \\bm{p}_i , \\bm{q}_i\\rangle\\}_{i=1}^{r}$ and a mini-batch of data $\\mathcal{X}\\in \\mathbb{R}^{b \\times m}$ with mini-batch size $b$ and dimension $m$, the ensemble of mapped results $\\{ \\mathcal{X}\\mathbf{W}_1, \\cdots, \\mathcal{X}\\mathbf{W}_r \\}$ can be calculated via\n\\begin{equation}\\label{eqn:batch}\n\t\\begin{split}\n\t\t\\begin{bmatrix}\n\t\t\t\\mathcal{X}\\mathbf{W}_1 \\\\\n\t\t\t\\mathcal{X}\\mathbf{W}_2 \\\\\n\t\t\t\\cdots \\\\\n\t\t\t\\mathcal{X} \\mathbf{W}_r \\\\\n\t\t\\end{bmatrix} \n\t\t=\n\t\t\\Bigg ( \\bigg ( \n\t\t\\begin{bmatrix}\n\t\t\t\\mathcal{X} \\\\\n\t\t\t\\mathcal{X} \\\\\n\t\t\t\\cdots \\\\\n\t\t\t\\mathcal{X} \\\\\n\t\t\\end{bmatrix} \n\t\t\\circ\n\t\t\\begin{bmatrix}\n\t\t\t\\mathbf{P}_1 \\\\\n\t\t\t\\mathbf{P}_2 \\\\\n\t\t\t\\cdots \\\\\n\t\t\t\\mathbf{P}_r \\\\\n\t\t\\end{bmatrix}\n\t\t\\bigg )\n\t\t\\mathbf{W}_0 \\Bigg ) \n\t\t\\circ\n\t\t\\begin{bmatrix}\n\t\t\t\\mathbf{Q}_1 \\\\\n\t\t\t\\mathbf{Q}_2 \\\\\n\t\t\t\\cdots \\\\\n\t\t\t\\mathbf{Q}_r \\\\\n\t\t\\end{bmatrix},\n\t\\end{split}\n\\end{equation}\nwhere each row in $\\mathbf{P}_i$ and $\\mathbf{Q}_i$ is duplicated $\\bm{p}_i$ and $\\bm{q}_i$. Let $\\mathbf{1}_b$ be a all-one vector with size $b$, $\\mathbf{P}_i$ and $\\mathbf{Q}_i$ are obtained via $\\mathbf{P}_i = \\mathbf{1}_b \\bm{p}_i^{\\top}$ and $\\mathbf{Q}_i = \\mathbf{1}_b \\bm{q}_i^{\\top}$. \n\n\nThe above vectorisation derivation allows the deep representation ensemble process in DIF to be efficiently calculated.\nWith the help of CERE, the time complexity of the ensemble process is similar to the feed-forward process of a single neural network since all the ensemble members can be computed simultaneously in a given mini-batch. \nEq. (\\ref{eqn:batch}) requires additional Hadamard product steps, but this operation is very cheap compared to matrix multiplication. \nOne possible limitation is the batch size in Eq. (\\ref{eqn:batch}). A mini-batch of $r \\times t$ objects is simultaneously computed, which is larger than conventional settings. \nHowever, as computation within a mini-batch is automatically parallelisable, increasing the batch size incurs almost no time overhead. \nAs for memory cost, this process is also feasible in a typical device because DIF does not involve optimisation, i.e., gradients are not calculated and saved. For example, a dataset with 10,000 features costs about 3GB of memory when using the recommended ensemble size $r$=$50$ and the batch size $b$=$64$. \n\n\nLet $\\Phi$ be the $L$-layer neural network using the newly-defined feed-forward step in Eq. (\\ref{eqn:batch}). The ensemble of representations can be directly generated as \n\\begin{equation}\n\t\\mathscr{G}_{\\text{CERE}}(\\mathcal{D}) = \\Phi \\big( \\mathcal{D} ; \\Theta \\big) = \\big\\{ \\mathcal{X}_i \\subset \\mathbb{R}^d \\big\\}_{i=1}^{r} ,\n\\end{equation}\nwhere $\\Theta\\! =\\! \\big\\{ \\mathbf{W}_l, \\{\\bm{p}_{(l,i)}\\}_{i=1}^{r}, \\{\\bm{q}_{(l,i)}\\}_{i=1}^{r} \\big \\}_{l=1}^{L}$. Note that other operations like activation or pooling and some layers that do not use a weight matrix are processed sequentially.\n\n\n\n\n\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "DEAS: Deviation-enhanced Anomaly Scoring Function",
                        "content": "\n\nWe further introduce a new anomaly scoring function DEAS. \nRecall that the standard anomaly scoring process in iForest only uses the length of the traversed path, i.e., all nodes are considered to have the same importance. The path length only provides limited information, which may not sufficiently delineate the isolation difficulty of data objects.\nExcept for qualitative comparison in each node, additional quantitative information is readily available to be leveraged, such as relation between the feature values of data objects and the branching threshold. \n\n\nMotivated by this, we utilise the deviation degree of the feature value to the branching threshold as additional weighting information to further improve the measurement of isolation difficulty. These deviation degrees are important indicators to the isolation difficulty because the feature values in newly created data spaces are typically densely distributed and these deviations reflect the local density in the projected space., e.g., a small deviation value indicates that the slicing cut is on a dense region and thus is hard to isolate the data object.\nSpecifically, let $\\bm{x}_u$ be the corresponding representation of a data object $\\bm{o}$ in an iTree $\\tau_i$. $p(\\bm{x}_u | \\tau_i) = \\{1, \\cdots, K\\}$ is its traversed node path. \nWe define the averaged deviation degree of $\\bm{x}_u$ in $\\tau_i$ as\n\n\\begin{equation}\\label{eqn:deviation}\n\tg(\\bm{x}_u | \\tau_i)  = \\frac{1}{ | p(\\bm{x}_u | \\tau_i) |} \\sum_{k \\in p(\\bm{x}_u | \\tau_i)} \\vert \\bm{x}_u^{(j_{k})} -  \\eta_{k} \\vert .\n\\end{equation}\n\nWe further combine the path length $|p(\\bm{x}_u | \\tau_i)|$ as in iForest and the deviation measure in Eq. (\\ref{eqn:deviation}) to specify the function in Eq. (\\ref{eqn:score_func_class}) by defining our deviation-enhanced isolation anomaly scoring function as:\n\\begin{equation}\\label{eqn:scoring}\n\t\\mathscr{F}_{\\text{DEAS}}(\\bm{o}|\\mathcal{T}) \n\t= 2^{\n\t\t-\\mathbb{E}_{\\tau_i \\in \\mathcal{T}} \\frac{ |p(\\bm{x}_u| \\tau_i)|}{C(T)}\n\t} \n\t\\times \n\t\\mathbb{E}_{\\tau_i \\in \\mathcal{T}}\\big(g(\\bm{x}_u | \\tau_i) \\big),\n\\end{equation}\nwhere the first term is the averaged depth used as anomaly scores in iForest and the second term is the deviation-based anomaly score we introduce.\n\n\n\n\n"
                    }
                },
                "subsection 5.3": {
                    "name": "Algorithm of DIF",
                    "content": "\nAlgorithm \\ref{alg1} presents the procedure of the construction of deep isolation trees $\\mathcal{T}$.  Step (2) prepares $r$ random representations, and $t$ isolation trees $\\{\\tau_i\\}_{i=1}^{t}$ are built upon each representation in Steps (4-15). \nFor each isolation tree $\\tau_i$, a subset of transformed data objects $\\mathcal{P}_{1}$ is first randomly subsampled in Step (5) to initialise the root node. Each leaf node $P_k$ is then iteratively split by using a comparison branching criteria based on a randomly selected representation dimension $j_k$ and a split point $\\eta_k$ in Steps (6-13). \n\n\nWe report the anomaly scoring procedure in Algorithm \\ref{alg2}. \nData object $\\bm{o}$ is transformed to vectorised representations $\\{\\bm{x}\\}_{u=1}^{r}$ in Step (1)\nAfter the initialisation in Step (4), the data object traverses each tree $\\tau_i$ by the criteria of each node and reaches the final node, during which the traverse path $p(\\bm{x}_u|\\tau_i)$ and the accumulated difference $\\beta$ are recorded in Steps (5-12). The path length $| p(\\bm{x}_u|\\tau_i) |$ and the deviation $g(\\bm{x}_u |\\tau_i)$ in iTree $\\tau_i$ are calculated in Step (13). The anomaly score of $\\bm{o}$ is calculated and returned in Steps (16-17).\n\n\n\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n\\begin{algorithm}\n\t\\caption{\\textit{Construction of Deep Isolation Trees}}\n\t\\label{alg1}\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE $\\mathcal{D}$ - input dataset\n\t\t\\ENSURE $\\mathcal{T}$ - forest of deep isolation trees\n\t\t\\STATE Initialise $\\mathcal{T}\\leftarrow \\varnothing $ \n\t\t\\STATE Generate representations $\\{\\mathcal{X}_{u}\\}_{u=1}^{r}$ via $\\mathscr{G}_{\\text{CERE}}$\n\t\t\\FOR{ $u=1$ to $r$ }\n\t\t\\FOR{ $i=1$ to $t$}\n\t\t\\STATE Initialise an isolation tree $\\tau_i$ by setting the root node using $\\mathcal{P}_{1} \\subseteq \\mathcal{X}_u$, $|\\mathcal{P}_{1}| = n$ \n\t\t\\WHILE{$\\mathcal{P}_{k}$ is a leaf node of tree $\\tau_i$}\n\t\t\\IF{$|\\mathcal{P}_{k}|>1$ and the depth is smaller than $J$}\n\t\t\\STATE Randomly select a dimension $j_k \\in \\{1, \\cdots, d\\}$\n\t\t\\STATE Randomly select a split point $\\eta_k$ between the \\textit{max} and \\textit{min} values of dimension $j_k$ in $\\mathcal{P}_{k}$\n\t\t\\STATE  $\\mathcal{P}_{2k} \\leftarrow \\{\\bm{x} | \\bm{x}^{(j_k)} \\leq \\eta_k,  \\bm{x} \\in \\mathcal{P}_{k}\\}$\n\t\t\\STATE  $\\mathcal{P}_{2k+1} \\leftarrow \\{\\bm{x} | \\bm{x}^{(j_k)} > \\eta_k, \\bm{x} \\in \\mathcal{P}_{k}\\} $\n\t\t\\ENDIF\n\t\t\\ENDWHILE\n\t\t\\STATE $\\mathcal{T} \\leftarrow \\mathcal{T} \\cup \\tau_i$\n\t\t\\ENDFOR\n\t\t\\ENDFOR\n\t\t\\RETURN $\\mathcal{T}$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\n\\begin{algorithm}\n\t\\caption{ \\textit{Deviation-enhanced Anomaly Scoring}\n\t}\n\t\\label{alg2}\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE $\\bm{o}$ - data object, $\\mathcal{T}$ - set of deep isolation trees\n\t\t\\ENSURE anomaly score $\\mathscr{F}_{\\text{DEAS}}(\\bm{o}|\\mathcal{T} )$\n\t\t\\STATE Generate representations $\\{\\bm{x}_u\\}_{u=i}^{r}$ via $\\mathscr{G}_{\\text{CERE}}$\n\t\t\\FOR{ $u=1$ to $r$ }\n\t\t\\FOR{ $i=1$ to $t$}\n\t\t\\STATE Initialise $k \\leftarrow 1$, $\\beta \\leftarrow 0$, $p(\\bm{x}_u|\\tau_i) \\leftarrow \\varnothing $\n\t\t\\WHILE{$|\\mathcal{P}_{k}| >1$ and not reaching $J$}\n\t\t\\IF{$\\bm{x}_u^{(j_k)} \\leq \\eta_k $ }\n\t\t\\STATE $k \\leftarrow 2k$\n\t\t\\ELSE\n\t\t\\STATE $k \\leftarrow 2k+1$\n\t\t\\ENDIF\n\t\t\\STATE $p(\\bm{x}_u|\\tau_i)\\leftarrow p(\\bm{x}_u|\\tau_i) \\cup k$, $\\beta \\leftarrow \\beta + |\\bm{x}_u^{(j_k)}- \\eta_k|$\n\t\t\\ENDWHILE\n\t\t%\t\t\\STATE  $h(\\bm{x}_u|\\tau_i) \\leftarrow |p(\\bm{x}_u|\\tau_i)|$,  $g(\\bm{x}_u|\\tau_i) \\leftarrow \\beta / |p(\\bm{x}_u|\\tau_i)|$\n\t\t\\STATE $g(\\bm{x}_u|\\tau_i) \\leftarrow \\beta / |p(\\bm{x}_u|\\tau_i)|$\n\t\t\\ENDFOR\n\t\t\\ENDFOR\n\t\t\\RETURN $\\mathscr{F}_{\\text{DEAS}}(\\bm{o}|\\mathcal{T})\\! \\leftarrow \\! 2^{ -\\mathbb{E}_{\\tau_i \\in \\mathcal{T}} \\frac{ |p(\\bm{x}_u | \\tau_i)| }{C(T)}\n\t\t}\\!  \\times\\! \\mathbb{E}_{\\tau \\in \\mathcal{T}}\\big(g(\\bm{x}_u | \\tau_i) \\big)$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Theoretical Analysis",
                    "content": "\n\n\n",
                    "subsubsection 5.4.1": {
                        "name": "Time Complexity Analysis",
                        "content": "\nWe first analyse the time complexity for the production of the random representation ensemble via CERE (i.e., Step 2 in Algorithm \\ref{alg1}). \nLet the input data $\\mathcal{D}$ be a tabular dataset with size $N \\times D$. Multi-layer perceptron network is used for $\\phi$. \nThe used network $\\phi$ comprises $L$ layers, and the $l$-th layer is with $d_l$ hidden units, the representation dimension is $d$. \nWe use CERE to implement the ensemble with $r$ members within each mini-batch, and thus the whole feed-forward computation induces $O(r\\times N \\times (Dd_1 + d_ld + \\sum_{l=1}^{L-1}d_ld_{l+1}))$. Only feed-forward steps are required in DIF, and the number of hidden units and representation dimension is generally small. \nThus, this process is linear w.r.t. both data size and dimensionality, which does not introduce much extra computational overhead than the original iForest. \nIn terms of the subsequent process of the iTree construction, given the depth limit $J$, we have the maximum $2^{J-1}$ splits (Steps (8-11)) during the growth of each iTree. For a node with $n$ samples, each split takes $O(n)$ complexity in determining the maximum and minimum value of the selected dimension and the assigning process. The overall process induces $O(2^{J-1}\\times n \\times r \\times t)$. $J$ and $n$ often use fixed small values (typically 8 and 256 respectively). Therefore, the overall complexity is linear w.r.t. the ensemble size $r\\times t$. As for the process in Algorithm \\ref{alg2}, the traversing process takes a similar computation process, which has linear time complexity w.r.t. the size of testing sets and the ensemble size. \nOverall, according to the above analysis, the time complexity of DIF is $O(ND(r \\times t))$. It has linear complexity w.r.t. data size, dimensionality, and ensemble size, which inherits desired scalability from iForest. \n\n\n\n"
                    },
                    "subsubsection 5.4.2": {
                        "name": "DIF as a Generalisation of iForest and EIF",
                        "content": "\\label{sec:generalisation_analsis}\n\nEIF \\cite{hariri2019eif} is a recent extension of iForest \\cite{liu2008isolation}, which has been shown to be a generalisation of iForest in \\cite{hariri2019eif}. We show that DIF can be viewed as a further higher-level generalisation of isolation methods used in both iForest and EIF, i.e., the branching criteria used in iForest and EIF can be transformed into the format of DIF. \n\n\t\n\t\n\tLet $\\bm{o} \\in \\mathbb{R}^{D}$ be a vectorised data object. Recall that the branching criterion in DIF is $\\phi(\\bm{o})^{(j)} \\leq \\eta$.\n\t% \t\\begin{equation}\n\t\t% \t\t\\phi(\\bm{o})^{(j)} \\leq \\eta.\n\t\t% \t\\end{equation}\n\tBoth iForest and EIF are special cases of DIF when the neural network $\\phi$ is with one linear layer parameterised by a weight matrix $\\mathbf{W}$, i.e., $\\phi(\\bm{o}) = \\mathbf{W}^{\\top}\\bm{o}$.\n\tiForest splits the node by using the criterion $\\bm{o}^{(j)} \\leq \\eta$, while DIF degrades to iForest if the weight matrix is set as an identity matrix, i.e., $\\mathbf{W} = I_D$. \n\tEIF uses a slicing hyper-plane for each node branching, and the slope of its hyper-plane is a normal vector $\\bm{k} \\in \\mathbb{R}^D$, and $\\bm{k}^{(i)} \\sim \\mathcal{N}(0, 1), \\forall i \\in \\{1, \\cdots D\\}$. The intercept of the hyper-plane $\\bm{p} \\in \\mathbb{R}^D$ is uniformly selected over the range of possible values at each branching point. The branching criterion is $(\\bm{o} - \\bm{p})\\cdot \\bm{k} \\leq 0$, which is equivalent to $\\bm{o}\\cdot\\bm{k} \\leq \\bm{p}\\cdot\\bm{k}$.\n\tWe can fulfil exactly the same operation in DIF when the weight matrix satisfies $\\mathbf{W} \\in\\mathbb{R}^{D\\times 1}$. \n\tThe elements in $\\mathbf{W}$ should also be initialised by a normal distribution $\\mathcal{N}(0,1)$ to satisfy $\\mathbf{W} = \\bm{k}$. Additionally, the splitting point $\\eta=\\bm{p} \\cdot \\bm{k}$ can be understood as a standard random vector $\\bm{p}$ sampled in possible values with a Gaussian noise $\\bm{k}$. \n\n\n\n"
                    }
                },
                "subsection 5.5": {
                    "name": "Discussions",
                    "content": "\n\nThe power of DIF mainly depends on (i) the strong representation ability of neural networks, (ii) the discard of optimised representations, and (iii) the synergy between random representations and random partition-based isolation, which are respectively discussed below. \n\n\n\n",
                    "subsubsection 5.5.1": {
                        "name": "Representation Ability of Neural Networks",
                        "content": " \nNeural networks have strong representation power, even for randomly initialised networks. As shown in Fig. \\ref{fig:proj} where the new data spaces are generated by random neural networks, the randomness in these initialised networks can create highly diversified data spaces, on which simple axis-parallel cuts can be equivalent to sophisticated slicing cuts in the original data space. Non-linear activation functions can effectively tweak and fold partition bounds to embed non-linearity into the isolation process, even though the networks are not optimised at all. \nOn the other hand, there have been different deep learning architectures developed for various data types, so DIF is empowered to handle diverse data types by plugging the data-specific network backbone (e.g., multi-perceptron networks, recurrent networks, or graph neural networks) to produce the representations (see Sec. \\ref{subsec: settings} for different neural networks used in DIF and Sec. \\ref{sec:effectiveness} for their performance in different data types).  \n\n\n\n\n"
                    },
                    "subsubsection 5.5.2": {
                        "name": "Optimised vs. Casually Initialised Representations",
                        "content": "\nIn general, we can use many representation learning networks specifically designed for anomaly detection, such as those in \\cite{pang2018Learning,ruff2018dsvdd,wang2021rdp,xu2021beyond}, to obtain well-optimised feature representations. \nThese representations are more expressive than randomly initialised representations if the optimisation objective well fits the input data.\nHowever, instead of using optimised representations, DIF uses the casually initialised representations due to the following two main reasons.\n(i) These loss functions are not versatile. It is difficult to devise one representation learning loss that can fit different data with diversified characteristics.\n(ii) The downstream data partition might be strongly controlled by the optimisation process.\nThis way weakens the randomness and diversity of the feature representations, which are required in isolation-based anomaly scoring methods. These two intuitions are empirically investigated in our experiments by showing the performance of DIF on optimised representations produced by recent normality feature learning algorithms and the quality of these optimised representations  (see Sec. \\ref{subsec:rep_scheme}) \n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 5.5.3": {
                        "name": "Synergy between Random Representations and Random Partition-based Isolation ",
                        "content": "\n\nDIF employs a novel representation scheme, i.e., the random representation ensemble produced via optimisation-free neural networks.\nThe parameters of these networks can be initialised by randomly sampling from widely-used initialisation distributions (e.g., normal or uniform distribution), easily yielding a set of feature representations with excellent randomness and diversity. Given a sufficiently large set of such random representations, we can largely boost the isolation power in the random data partition, making it possible to effectively isolate some really hard anomalies on some subsets of these representations. For example, as shown in Fig. \\ref{fig:hardanom}, among a large set of new representation spaces, there are some selective new spaces where hard anomalies become easy-to-isolate data objects. Recall that isolation methods, including DIF, are based on an average measure for anomaly scoring. Thus, the anomalies would stand out in the anomaly scores as long as they are effectively isolated in some of the isolation trees.\nDIF utilises this unique synergy between random representations and random partition-based isolation to largely improve the isolation process and subsequently the anomaly scoring function, \nresulting in significantly improved effectiveness of isolation-based anomaly detection.\nWe empirically investigate the significance of this synergy by respectively replacing random representations and random partition-based isolation with multiple alternatives (see Sec. \\ref{sec:significance}).  \n\n\n\n\n\n\n\n"
                    }
                }
            },
            "section 6": {
                "name": "Experiments",
                "content": "\nWe now introduce our experimental analysis. This section is organised as follows:\nIn Sec. \\ref{sec:setup}, we first start with the experimental setup including the used datasets, competing methods, parameter settings, and evaluation metrics;\nSec. \\ref{sec:effectiveness}, \\ref{sec:time}, and \\ref{sec:robustness} evaluate the performance of our method w.r.t. effectiveness, scalability, and robustness; and in Sec. \\ref{sec:significance} and \\ref{sec:ablation}, we empirically analyse our method by investigating the significance of the synergy between random representations and random partition-based isolation and the contribution of CERE and DEAS.\n\n\n",
                "subsection 6.1": {
                    "name": "Experimental Setup",
                    "content": "\\label{sec:setup}\n",
                    "subsubsection 6.1.1": {
                        "name": "Datasets",
                        "content": "\n\nWe employ a large collection of publicly available and commonly-used real-world datasets, including ten tabular datasets, four graph datasets, and four time-series datasets. Their basic information is shown in Table \\ref{tab:datainfo}.\n\n\n\\textbf{Tabular Data}.\n\\textit{Analysis}, \\textit{Backdoor}, \\textit{DoS}, and \\textit{Exploits} are taken from a popular intrusion detection benchmark UNSW\\_NB 15. Following \\cite{pang2021toward,pang2019deep}, we select different attacks as anomalies in these datasets against normal network traffic. \n\\textit{R8} is a highly-imbalanced text classification dataset, where the rare class are treated as anomalies by following \\cite{pang2018Learning,rayana2016less}.\n\\textit{Cover} is from the ecology domain. \\textit{Fraud} is for fraudulent credit card transaction detection. \n\\textit{Pageblocks} and \\textit{Shuttle} are provided by an anomaly benchmark study \\cite{campos2016evaluation}.\n\\textit{Thrombin} is to detect unusual molecular bio-activity for drug design, which is an ultrahigh-dimensional anomaly detection dataset used in \\cite{pang2018Learning}\n\n\n\n\\textbf{Graph Data}. We employ datasets from the popular graph benchmark Tox21, which is a project on toxicity evaluation of newly synthesised or used chemical compounds. These datasets are chosen since they are inherently imbalanced and contain real anomalies. The task is to detect abnormal graphs. \n\n\n\n\\textbf{Time-series Data}. Time series data are taken from the UCR time series anomaly archive \\cite{wu2021current}.\nWe employ data with natural anomalies. \n\\textit{Mars} is from NASA spacecraft. \n\\textit{Gait} is sensor data of a subject who has Huntington\u2019s disease (highly asymmetric gait), and anomalies are data from the weak leg.\n\\textit{ECG} is heartbeat data, where anomalies are ventricular beats, and \\textit{ECG-wandering} (\\textit{ECG-w} for short) is a long stretch of ECG with a wandering baseline. \n\n\n\n\n\n% Table generated by Excel2LaTeX from sheet 'data'\n%\n\n\n\n\n\n\n"
                    },
                    "subsubsection 6.1.2": {
                        "name": "Competing Methods",
                        "content": "\n\nTo have a comprehensive comparison, DIF is compared with the following two types of anomaly detection approaches.\n\n\\textbf{iForest and Its Extensions (IF-based Methods)}. Apart from the popular iForest algorithm \\cite{liu2008isolation}, its three advanced variants, i.e., EIF \\cite{hariri2019eif}, PID \\cite{gopalan2019pid}, and LeSiNN \\cite{pang2015lesinn}, are employed. \nEIF slices data by using hyperplanes with random slopes and intercepts. In PID, the choice of splits is optimised based on the variance in the sparsity, and the anomaly scoring is redefined according to the sparsity. LeSiNN employs the nearest neighbour distance-based isolation ensemble, which is also known as aNNE in \\cite{ting2017defying}. \n\n\n\\textbf{Ensemble of Deep Anomaly Detectors}.\n\\footnote{The ensemble performance of these deep models generally outperforms their solitary versions, and thus we focus on the comparison between ensemble-based results in the following experiments.}.\nDifferent state-of-the-art (SOTA) deep anomaly detection methods are employed as base models, and we use the deep ensemble framework \\cite{lakshminarayanan2017de} to construct a suite of ensemble-based deep contenders. \nFor tabular data, we utilise four state-of-the-art deep methods including RDP \\cite{wang2021rdp}, REPEN \\cite{pang2018Learning}, Deep SVDD \\cite{ruff2018dsvdd}, and a reconstruction-based Autoencoder baseline (RECON for short) \\cite{aggarwal2017outlieranalysis}. \nWe also employ the CERE ensemble method used in DIF for those methods to ensure their time efficiency and have a fair competition. \nAs for graph data, a deep graph-level anomaly detector GLocalKD \\cite{ma2022deep} is used. \nTranAD \\cite{tuli2022tranad} is employed for time-series data.\nTheir ensemble versions are denoted as eRDP, eREPEN, eDSVDD, eRECON, eGLocalKD, and eTranAD. \nAll these methods are specifically designed for anomaly detection on the corresponding data type. \n\n\n\n\n\n\n"
                    },
                    "subsubsection 6.1.3": {
                        "name": "Parameter Settings and Implementations",
                        "content": "\\label{subsec: settings}\n\n\nDIF uses 50 representations ($r$=$50$) and 6 isolation trees per representation ($t$=$6$), with 256 as subsampling size ($n$=$256$) for each iTree. DIF processes tabular data by using fully-connected multi-layer-perceptron networks. \nAll of the IF-based competing methods use 300 trees (the ensemble size is the same as DIF). The subsampling size is set as 256. We use the maximum extension level of EIF, i.e., the extension level is adaptively set as the dimensionality minus 1. For LeSiNN, the subsampling size is 8 by following \\cite{pang2015lesinn}. \nAs IF-based competitors cannot directly process non-tabular data, we employ the latest powerful unsupervised representation learning models to yield high-quality vectorised representations, and specifically, InfoGraph \\cite{sun2020infograph} and TS2Vec \\cite{yue2022ts2vec} are respectively utilised for graph data and time series. \nNote that InfoGraph uses GIN as its graph encoder architecture, and TS2Vec applies a dilated CNN module with residual blocks. They propose specific learning objectives to optimise generated representations.  \nFor the sake of fairness, DIF also respectively utilises the same GIN and CNN network structure to handle graph data and time series but without any optimisation steps. \nThe deep anomaly detectors are trained by 50 epochs and 30 steps per epoch. REPEN, DSVDD, and RECON take an Adam optimiser with a 1e-3 learning rate and use 64 objects per mini-batch, and we empirically found that RDP can work significantly better when using 1e-4. \nThe default/recommended settings are used for the other parameters of these competing methods.\n\n\n\n\n\n\nAll the anomaly detection algorithms in our experiments are implemented using Python, with iForest from \\texttt{scikit-learn} package, EIF from \\texttt{eif} package, and other methods from their authors' releases.\nThe implementation of our method is publicly available\\footnote{Source code of DIF can be downloaded from \\url{https://github.com/xuhongzuo/deep-iforest}}.\n\n\n\n\n%\n\n\n\n%\n\n\n\n"
                    },
                    "subsubsection 6.1.4": {
                        "name": "Evaluation Metrics and Computing Infrastructure",
                        "content": "\\label{sec:metric}\nFollowing the mainstream evaluation protocols of anomaly detection \\cite{pang2019deep,hariri2019eif,liu2008isolation}, the detection accuracy is evaluated by two complementary metrics including Area Under the ROC Curve (AUC-ROC) and Area Under the PR Curve (AUC-PR).\nROC curve indicates the true positives against false positives, while PR curve summarises precision and recall of the anomaly class only. \nThe paired \\textit{Wilcoxon} signed rank test is used to examine the statistical significance of the performance of DIF against each competing method.\n\n\nWe then introduce a new metric called Anomaly Isoability Index ($AII$) to measure the quality of representations.\nWe borrow the concept of triplet loss \\cite{schroff2015triplet} to count the percentage of effectively isolated anomalies among all true anomalies in each representation space, i.e.,\n\\begin{equation}\n\tAII = \n\t\\mathop{\\mathbf{P}}\\limits_{\\bm{a} \\sim \\mathcal{A}} \\Big( \\mathop{median}\\limits_{\\bm{n}_i \\in \\mathcal{N}} \\big \\{\\frac{1}{|\\mathcal{C}|} \\sum_{\\bm{n}_j \\in \\mathcal{C}} \\big( \\omega(\\bm{a} | \\bm{n}_i, \\bm{n}_j) \\big) \\big \\} >0 \\Big ),\n\\end{equation}\nwhere $\\omega(\\bm{a} | \\bm{n}_i, \\bm{n}_j)$ = $d(\\bm{a}, \\bm{n}_i)\\! -\\! d(\\bm{n}_j, \\bm{n}_j)$ denotes the difference between the Euclidean distances of the two pairs, $\\bm{a}\\!\\sim\\!\\mathcal{A}$ represents any anomaly drawn from the true anomaly set in the dataset, \n$\\mathcal{C}$ is a group of randomly sampled normal anchors, and $\\mathcal{N}$ is another set of random normal samples to delegate the whole normal distribution. All the above data objects are from the target representation space.\n$|\\mathcal{C}|$=$20$ and $|\\mathcal{N}|$=$1000$ are used as we found empirically that these two settings are sufficiently large to compute the $AII$ metric. \n\n\n\nThe computational time of all methods is based on a workstation with Intel Xeon Silver 4210R CPU, a single NVIDIA TITAN RTX GPU, and 64 GB RAM. \n\n\n\n\n\n%\n\n\n\n\n\n\n\n"
                    }
                },
                "subsection 6.2": {
                    "name": "Effectiveness in Reducing False Negatives",
                    "content": "\\label{sec:effectiveness}\n\n\n",
                    "subsubsection 6.2.1": {
                        "name": "Tabular Data",
                        "content": "\n\nTable \\ref{tab:tabular1} and \\ref{tab:tabular2} present the AUC-ROC and AUC-PR results of our method DIF and eight competing methods. \nOverall, DIF largely reduces the false negatives compared to iForest and its three extensions, resulting in substantial averaged performance improvement in detection precision and/or recall rates, and thus, DIF obtains superior AUC-PR and AUC-ROC performance. Particularly, in the average AUC-PR, DIF significantly outperforms EIF (61\\%), PID (186\\%), LeSiNN (56\\%), iForest (144\\%), eRDP (13\\%), eREPEN (77\\%), eDSVDD (19\\%), and eRECON (82\\%). DIF also obtains 4\\% - 11\\% AUC-ROC improvement across these methods.  \n\n\nAs shown in Table \\ref{tab:tabular1}, DIF significantly outperforms isolation-based methods at the 99\\% confidence level.\nDIF is the best isolation-based detector across all the ten datasets except AUC-PR on \\textit{Fraud}, on which the nearest neighbour-based anomaly measure LeSiNN is more effective. The features in \\textit{Fraud} are the results of PCA transformation due to the confidentiality issues, and thus the distance concept used in the nearest neighbour information can well reflect the proximity relationship.\n% of data objects. \nBy contrast, DIF does not rely on such prior information that is not always reliable in all the datasets. These results demonstrate superior isolation power of DIF, which can effectively isolate anomalies that may not be possible in existing IF-based methods due to the challenges like data sparsity and non-linearity. This is particularly true on challenging high-dimensional datasets like \\textit{R8}, \\textit{Analysis}, \\textit{Backdoor} and \\textit{DoS}. \n\n\nCompared to deep ensemble-based methods, DIF performs significantly better at the 99\\% confidence level according to the AUC-ROC performance in Table \\ref{tab:tabular2}.\nDIF achieves the best performance on seven out of ten datasets in terms of both AUC-ROC and AUC-PR, and it obtains very competitive results on the rest three datasets with less than 0.01 difference in AUC-ROC.\nNevertheless, the comparison results between DIF and its deep ensemble-based counterparts are very encouraging given the fact that DIF does not involve any optimisation while these deep methods need to be properly trained using pre-defined objective functions to better expose anomalies. \nThe superiority of DIF in this comparison owes to the representation diversity of each ensemble member and the unique synergy between random representations and random partition-based isolation in the downstream anomaly scoring process. \nMore importantly, DIF runs significantly faster than these deep ensemble-based contenders by around two orders of magnitude (see Sec. \\ref{sec:time}).\nIts superior computational efficiency endows DIF with stronger practicability in real-world applications.  \n\n\n\n\n\n"
                    },
                    "subsubsection 6.2.2": {
                        "name": "Graph Data and Time Series",
                        "content": "\n \n\nAs reported in Sec. \\ref{subsec: settings}, traditional isolation-based methods are performed upon vectorised representations learned by unsupervised representation learning method InfoGraph \\cite{sun2020infograph} and TS2Vec \\cite{yue2022ts2vec}.\nDIF respectively utilises the same network structure (GIN and dilated CNN) in InfoGraph and TS2Vec but without any optimisation steps. \nGLocalKD \\cite{ma2022deep} is a graph-level anomaly detector, and TranAD \\cite{tuli2022tranad} is a deep anomaly detection method specifically designed for time series. We also employ the ensemble version of these two SOTA methods as competitors. \n\n\nDetection performance on graph data and time series are shown in Table \\ref{tab:graphts}, in which EIF and LeSiNN are selected as contenders due to their preferable performance in tabular datasets over PID, with iForest also included as a baseline. \nDIF is the best performer on three out of four datasets in both graph-level and time series anomaly detection tasks. This performance of DIF is remarkable in that it outperforms not only the isolation-based methods that are empowered by the latest powerful representation learning models but the recent SOTA methods that are specifically designed to extensively learn data-type-specific characteristics (e.g., holistic graph structure or temporal dependence) for effectively detecting these graph/sequential anomalies. By contrast, DIF performs well in a unified framework by only replacing the network structure with a different randomly initialised network backbone, offering a significantly simpler yet data-type-agnostic effective solution. \n\n\nWe further show how DIF works by visualising its results on a time series dataset. Fig. \\ref{fig:case} visualises the ECG-w data (the id is \\texttt{sddb49} in the UCR benchmark) and the detection results of DIF and its competing methods. As illustrated in the introduction of the benchmark, this dataset is a challenging case that may confuse many algorithms due to its wandering shape. Albeit wandering baseline in testing data, we can also see this in the training set, and thus detection models are expected to tolerate these noisy regions. Our method DIF reduces the false negatives by successfully yielding significantly higher anomaly scores on anomalous heartbeats highlighted in golden yellow. In comparison, competing methods are misled by the fluctuating trends, showing much higher scores on noisy regions and omitting real anomalies.  \n\n\n\n\n\n\n\n\n\n\n"
                    }
                },
                "subsection 6.3": {
                    "name": "Scalability to High-dimensional, Large-scale Data",
                    "content": "\\label{sec:time}\n\nThis experiment examines the scalability of DIF and its contenders.\nThese anomaly detectors are performed on a group of synthetic tabular datasets with different sizes and dimensionalities to record their training times. Nine datasets are with 5,000 data objects, and their dimensions range from 16 to 4,096. The other nine datasets are with 32 features and varied sizes from a minimum of 1,000 up to 256,000.\nIF-based anomaly detectors only need CPU devices, while deep ensemble-based methods can leverage GPU acceleration. Therefore, we report the training time of DIF and its deep competitors on both GPU and CPU devices. \n\n\n\n\n\n\n\n\nFig. \\ref{fig:scal} (top) shows the scalability test results of DIF and its competing methods on a CPU device, and Fig. \\ref{fig:scal} (bottom) reports the comparison using GPU. \nDIF and all the other isolation-based methods present good scalability w.r.t. both dimensionality and data size compared to deep ensemble-based methods when using CPU computation. This is owed to the subsampling-based methods applied to both data samples and dimensions in isolation-based detectors. \nBesides, deep ensemble-based methods can greatly benefit from GPU acceleration when handling high-dimensional data. DIF shows outstanding time efficiency compared to its deep ensemble-based counterparts since it only requires one feed-forward step instead of a large number of training epochs. \nThese results demonstrate that DIF inherits excellent computational efficiency from the iForest. Note that DIF can obtain almost the same scalability as iForest w.r.t. the data dimensionality, as DIF performs isolation on newly projected spaces of much smaller dimensionality while iForest works on the original data space.\n\n\n\n\n\n\n\n\n"
                },
                "subsection 6.4": {
                    "name": "Robustness w.r.t. Anomaly Contamination",
                    "content": "\\label{sec:robustness}\nThis experiment examines the performance of DIF and its contenders when datasets containing different anomaly contamination ratios. \nTime-series datasets and graph datasets have pre-defined train-test split and their training sets do not contain anomalies, and thus we use tabular datasets as our bases here.\nFollowing \\cite{pang2021toward,pang2019deep}, we adjust the contamination ratios by injecting/removing anomalies, such that ratios range from 0\\% to 10\\%.\nThese detection approaches are trained on adjusted datasets with controlled contamination ratios and tested on the original version. \n\n\nThe AUC-ROC performance is reported in Fig. \\ref{fig:robustness}. Generally, the performance of all the anomaly detectors downgrades with the increasing contamination ratio.\nNevertheless, DIF has relatively clear superiority and stronger robustness in most of the datasets.\nThe success of deep ensemble on out-of-distribution robustness has been proved in many recent studies \\cite{liu2022nooverhead,angelo2021repulsive,nam2021diversity}, which partially explains why deep ensemble-based methods show better robustness w.r.t. anomaly contamination than IF-based methods. \nHowever, these competing methods still fail to provide consistently good robustness (e.g., eRDP, eREPEN, and eDSVDD on \\textit{R8} and eRECON on \\textit{Backdoor}). It is mainly due to two reasons: (i) their ensemble processes may suffer from the diversity problem, and (ii) their scoring strategies and training objectives rely on strong assumptions such as the distance concept that might not hold in some datasets. \nIt is interesting to note that eREPEN shows an uptrend in \\textit{Shuttle}. REPEN uses LeSiNN to estimate initial anomaly scores when generating triplet mini-batches. \nAs LeSiNN is with good robustness on \\textit{Shuttle}, this initial estimation can reliably obtain more anomaly examples as positive data when the contamination ratio increases, and thus the triplet learning process might benefit from the augmentation of the positive class. \n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 6.5": {
                    "name": "Significance of the Synergy between Random Representations and Random Partition-based Isolation",
                    "content": "\\label{sec:significance}\n\n\nWe respectively replace random representations and random isolation-based anomaly scoring in DIF to investigate their synergy effect. \n\n\n",
                    "subsubsection 6.5.1": {
                        "name": "Representation Scheme",
                        "content": "\\label{subsec:rep_scheme}\nThis experiment evaluates the effectiveness of our novel representation scheme by comparing it with representations produced by optimised neural networks, including RDP \\cite{wang2021rdp}, REPEN \\cite{pang2018Learning}, DSVDD \\cite{ruff2018dsvdd} and Reconstruction-based Autoencoder \\cite{aggarwal2017outlieranalysis}. \nThat is, we replace random representations with representations learned by one of these methods, with all the other components of DIF fixed. These four variants are denoted as RDP-DIF, REPEN-DIF, DSVDD-DIF and RECON-DIF. \nRDP, REPEN, and DSVDD are originally designed for learning a good representation for anomaly detection, and a dense representation can be implicitly derived from Autoencoders.\nThe representations are evaluated by the AUC-ROC performance and the individual representation quality as follows. \n\n\n\n\n\n\\textbf{AUC-ROC Results}. \nThe AUC-ROC results are shown in the upper half of Fig. \\ref{fig:random}. DIF outperforms four optimised representation ensemble-based methods on five datasets and shows very competitive performance to the best performer on the other five datasets. \nAveragely, the random representation ensemble contributes to 5\\%, 5\\%, 7\\%, and 15\\% AUC-ROC improvement than RDP-DIF, REPEN-DIF, DSVDD-DIF, and RECON-DIF, respectively. \nAlthough DIF may not be the best performer on all the datasets, it is very encouraging to see this performance achieved by the ensemble of random representations when compared to those optimised representations trained by state-of-the-art learning objectives. \n\n\n\n\n\n\n\n\\textbf{Quality per Representation}. To further analyse the mechanism behind the above results, we directly evaluate the quality of each representation produced by DIF and its variants. \nThe representation quality is measured by Anomaly Isoability Index ($AII$), as introduced in Sec. \\ref{sec:metric}.\nThe $AII$ results of each representation used in five anomaly detection methods are shown in Fig. \\ref{fig:random} (bottom), in which we use box plots to present the quality distribution of representations produced by 50 representations in the ensemble framework. \n\n\n\nBased on the above experiment results, the following three remarks can be made.\n\n\\begin{itemize}[leftmargin=*]\n\t\\item Our representation scheme achieves desired diversity and randomness,\n\t% across the base models, \n\twhile simultaneously maintaining \n\t% relatively \n\tstable expressiveness in each representation, enabling excellent synergy with the downstream isolation-based anomaly scoring mechanism. This is the main driving force behind the superior performance of DIF. \n\t\n\t\\item Optimised representations can be with consistently good quality on some datasets (e.g., near 80\\% true anomalies are well isolated by RECON-DIF on \\textit{Analysis}, \\textit{Backdoor}, and \\textit{DoS}), whereas the lack of diversity in representations downgrades the efficacy of this ensemble framework.\n\t\n\t\\item Optimisation may even lead to worse representations on some datasets compared to random representations (e.g., \\textit{R8}, \\textit{Cover}, \\textit{Pageblocks} and \\textit{Thrombin}). This may be due to the fact that the underlying assumption (e.g., one-class assumption) in their learning objectives may not hold in those datasets.\n\t\n\t\n\\end{itemize}\n\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 6.5.2": {
                        "name": "Scoring Strategy",
                        "content": "\n\n\nAs shown in the prior experiment, random representations are with good diversity and stable quality, fostering a unique excellent synergy effect in downstream ensemble-based anomaly scoring. This section further justifies this intuition by investigating the effectiveness of combining random representations with other anomaly scoring methods, including probability-based anomaly scoring method ECOD \\cite{li2022ecod}, distance-based method KNN \\cite{ramaswamy2000knn}, and density-based method LOF \\cite{breunig2000lof}. Each of these methods is used to replace the isolation-based scoring process, with all the other modules fixed. \nThese variants are denoted as DIF-ECOD, DIF-KNN, and DIF-LOF, respectively. Similarly, we evaluate their AUC-ROC performance and the individual scoring quality as follows. \n\n\n\n\n\n\n\n\\textbf{AUC-ROC Results}.\nThe AUC-ROC results are shown in the upper half of Fig. \\ref{fig:whyif}. \nDIF outperforms these competing variants on seven out of ten datasets. \nAveragely, DIF outperforms DIF-ECOD, DIF-KNN, and DIF-LOF by 3\\%, 13\\%, and 25\\%, respectively. \nThe superiority further justifies the synergy effect in DIF. \nNote that KNN and LOF are with very heavy computational overhead. They take around 60 hours to handle large-scale datasets \\textit{Cover} and \\textit{Fraud}. \n\n\\textbf{Quality per Anomaly Scoring Result.}\nWe also study the quality of individual anomaly scoring results produced by DIF and its variants on each random representation, as shown in Fig. \\ref{fig:whyif} (bottom). The quality is also estimated by AUC-ROC here. \n\n\n\nWe make the following three remarks.\n\n\n\\begin{itemize}[leftmargin=*]\n\\item These competing scoring methods can produce markedly better individual scoring results than our isolation-based scoring mechanism on \\textit{Exploits}, \\textit{R8}, and \\textit{Thrombin}. However, they only yield less effective or slightly better integrated results compared to DIF. \nThese scoring methods are unable to leverage the diversity embedded in our representation scheme.\n\n\\item By contrast, DIF combines data representation and anomaly scoring in a successful unified ensemble learning framework. DIF achieves better integrated performance by fully leveraging the diversity and randomness of representations, e.g., above 0.9 AUC-ROC on \\textit{R8} with the maximum individual value only achieving around 0.7. \n\n\\item DIF is inferior to its variants on \\textit{Shuttle}. It might be because anomalies in this dataset can be more easily identified by using the prior concepts used in these competing scoring methods (i.e., probability, distance, or density). However, these prior concepts may fail to work properly across all the datasets. \n\n\\end{itemize}\n\n\n\n\n\n\n\n"
                    }
                },
                "subsection 6.6": {
                    "name": "Ablation Study on CERE and DEAS",
                    "content": "\\label{sec:ablation}\n\nConsidering detection effectiveness and computational efficiency, we propose the Deviation-Enhanced Anomaly Scoring function (DEAS) and the Computation-Efficient deep Representation Ensemble method (CERE) in the specific implementation of DIF. This experiment is conducted to examine whether DIF can have better detection performance and use less training time with the help of DEAS and CERE. Two ablated variants are employed, i.e., \\textbf{w/o} CERE replaces $\\mathscr{G}_{\\text{CERE}}$ with the conventional sequential ensemble process, and \\textbf{w/o} DEAS uses the standard scoring function used in iForest to replace our scoring function $\\mathscr{F}_\\text{DEAS}$. \n\nThe AUC-ROC and AUC-PR results of DIF and \\textbf{w/o} DEAS are shown in Table \\ref{tab:ablation}. \nDIF significantly outperforms \\textbf{w/o} DEAS at the 90\\% confidence interval and achieves approximate 11\\% AUC-PR improvement. \n% These comparison results validate the substantial contribution of DEAS. \nBesides, DIF costs considerably less training time with the help of CERE. The total training time across all the ten datasets is only approximate one-tenth of the variant \\textbf{w/o} CERE. \nThe AUC-ROC/AUC-PR results of \\textbf{w/o} CERE are on par with DIF, which are omitted due to the space limitation. \nBased on the above comparison results, the contribution of the proposed DEAS and CERE is validated and quantitatively measured. \n\n\n\n\n\n%\n\n\n\n\n\n\n\n"
                }
            },
            "section 7": {
                "name": "Conclusions",
                "content": "\nThis paper introduces DIF, a novel extension of iForest. DIF takes the deep neural network-based random representation ensemble as a new representation scheme, enabling significantly versatile data partition in diverse random directions on subspaces of different sizes.\nThe anomaly scoring can be facilitated by the synergy between random representations and random partition-based isolation. \nThis enables DIF to fulfil (i) more effective isolation of anomalies, especially hard anomalies in data with intractable sparsity and non-linearity; \n(ii) liberation of isolation process from existing constraints to tackle the artefact problem;\n% (i.e., partitions can freely cross any part of the data space)\nand (iii) versatile ability to handle different data types.\n% by simply replacing the network backbones. \nExtensive experiments show that DIF significantly outperforms iForest and its existing extensions not only on tabular data but also on graph and time-series data. DIF also shows promising improvement compared to the ensemble of state-of-the-art deep anomaly detectors. \n\nIn our future work, we plan to devise new scoring strategies, together with relevant neural network backbones, to address other challenging yet important anomaly detection tasks, e.g., identify possible abnormal subsets \\cite{huang2021hybrid} and multi-view data \\cite{ji2019multi}.\n\n\n\n% use section* for acknowledgment\n\\ifCLASSOPTIONcompsoc\n% The Computer Society usually uses the plural form\n"
            },
            "section 8": {
                "name": "Acknowledgments",
                "content": "\n\\else\n% regular IEEE prefers the singular form\n"
            },
            "section 9": {
                "name": "Acknowledgment",
                "content": "\n\\fi\n\nHongzuo Xu, Yijie Wang, and Yongjun Wang are supported by the National Key R\\&D Program of China (No. 2022ZD0115302), the National Natural Science Foundation of China (No.61379052), the Science Foundation of Ministry of Education of China (No.2018A02002), the Postgraduate Scientific Research Innovation Project of Hunan Province (CX20210049), the Natural Science Foundation for Distinguished Young Scholars of Hunan Province (No.14JJ1026). Guansong Pang is supported in part by the Singapore Ministry of Education\n(MOE) Academic Research Fund (AcRF) Tier 1 grant (21SISSMU031).\n\n\nWe thank Eamonn Keogh and his students for creating the UCR  time series anomaly archive that has been used in our experiment. We also thank the referees for their comments, which helped improve this paper considerably.\"\n\n\n\n\n\n\n% Can use something like this to put references on a page\n% by themselves when using endfloat and the captionsoff option.\n\\ifCLASSOPTIONcaptionsoff\n\\newpage\n\\fi\n\n\n\n% trigger a \\newpage just before the given reference\n% number - used to balance the columns on the last page\n% adjust value as needed - may need to be readjusted if\n% the document is modified later\n%\\IEEEtriggeratref{8}\n% The \"triggered\" command can be changed if desired:\n%\\IEEEtriggercmd{\\enlargethispage{-5in}}\n\n\n\n% references section\n\\bibliographystyle{IEEEtran}\n\\bibliography{ref}\n\n\n\n% biography section\n% \n% If you have an EPS/PDF photo (graphicx package needed) extra braces are\n% needed around the contents of the optional argument to biography to prevent\n% the LaTeX parser from getting confused when it sees the complicated\n% \\includegraphics command within an optional argument. (You could create\n% your own custom macro containing the \\includegraphics command to make things\n% simpler here.)\n%\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}\n% or if you just want to reserve a space for a photo:\n\n\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biophoto/xu}}]{Hongzuo Xu}\nreceived the bachelor\u2019s and master\u2019s degree from the National University of Defense Technology, China, in 2017 and 2019, where he is currently pursuing the Ph.D. degree in computer science.\n% He has authored or coauthored over ten technical papers at prestigious international journals and conferences, including the \\textsc{IEEE Transactions on Knowledge and Data Engineering}, AAAI, IEEE International Conf. on Data Mining, ACM International World Wide Web Conference. \nHis research interests include anomaly detection, outlier interpretation, weakly-supervised learning\nand data mining. \n\\end{IEEEbiography}\n\n\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biophoto/Guansong.jpg}}]{Guansong Pang} is a tenure-track Assistant Professor of Computer Science in the School of Computing and Information Systems at Singapore Management University (SMU), Singapore. Before joining SMU, he was a Research Fellow with the Australian Institute for Machine Learning (AIML). He received a PhD degree from University of Technology Sydney in 2019. His research explores novel data mining and machine learning techniques and their applications, with a focus on detecting abnormal and unknown data.\n% for guarding the health, safety and security in both digital and physical worlds. \n\\end{IEEEbiography}\n\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biophoto/wang1}}]{Yijie Wang}\nreceived the PhD degree in computer science and technology from the National University of Defense Technology in 1998. \nShe was awarded the prize of National Excellent\nDoctoral Dissertation by Ministry\nof Education of PR China.\nShe is currently a Full Professor with the Science and Technology on Parallel and Distributed Processing Laboratory (PDL), National University of Defense Technology. Her research interests include\nbig data analysis, artificial intelligence and parallel and distributed processing.\n\\end{IEEEbiography}\n\n\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biophoto/wang}}]{Yongjun Wang}\nreceived the Ph.D. degree in computer architecture from the National University of Defense Technology, China, in 1998. He is currently a Full Professor with the College of Computer, National University of Defense Technology, Changsha, China. His research interests include network security and system security.\n\\end{IEEEbiography}\n\n\n\n\n\n\n\n\n\n% % if you will not have a photo at all:\n% \\begin{IEEEbiographynophoto}{John Doe}\n% Biography text here.\n% \\end{IEEEbiographynophoto}\n\n% % insert where needed to balance the two columns on the last page with\n% % biographies\n% %\\newpage\n\n% \\begin{IEEEbiographynophoto}{Jane Doe}\n% Biography text here.\n% \\end{IEEEbiographynophoto}\n\n% % You can push biographies down or up by placing\n% % a \\vfill before or after them. The appropriate\n% % use of \\vfill depends on what kind of text is\n% % on the last page and whether or not the columns\n% % are being equalized.\n\n% %\\vfill\n\n% % Can be used to pull up biographies so that the bottom of the last one\n% % is flush with the other column.\n% %\\enlargethispage{-5in}\n\n\n\n% that's all folks\n"
            }
        },
        "tables": {
            "tab:notation": "\\begin{table}[htbp]\n\t\\centering\n\t\\caption{The main notations used in the paper}\n\t\\scalebox{0.9}{\n\t\t\\begin{tabular}{p{2.6cm}p{6.4cm}}\n\t\t\t\\hline\n\t\t\t\\textbf{Format} & \\textbf{Notations} -- \\textbf{Descriptions}    \\\\\n\t\t\t\\hline\n\t\t\t\n\t\t\t\\makecell[l]{Calligraphic fonts}  & \n\t\t\t\\makecell[l]{$\\mathcal{D}$ -- datasets, $\\mathcal{X}$ -- representations, \\\\ $\\mathcal{T}$ -- the set of iTrees, $\\mathcal{P}$ -- data pools in iTree nodes} \\\\\n\t\t\t\n\t\t\tScript typeface & \n\t\t\t\\makecell[l]{ \n\t\t\t\t$\\mathscr{F}$ -- anomaly scoring function,\\\\ $\\mathscr{G}$ -- representation function} \\\\\n\t\t\t\n\t\t\t\\makecell[l]{Bold lowercase letters} & \n\t\t\t\\makecell[l]{\n\t\t\t\t$\\bm{o}$ -- original data objects, $\\bm{x}$ -- representation vectors, \\\\ $\\bm{p},\\bm{q}$ -- random vectors used in CERE} \\\\ \n\t\t\t\\makecell[l]{Bold uppercase letters} & \n\t\t\t\\makecell[l]{ $\\mathbf{W}$ -- weight matrices of neural networks, \\\\ $\\mathbf{P},\\mathbf{Q}$ -- matrices used in CERE } \\\\\n\t\t\t\\makecell[l]{Others} & \n\t\t\t\\makecell[l]{$r$ -- number of representations, \\\\ \n\t\t\t\t$t$ -- number of iTrees per representation, \\\\ \n\t\t\t\t$T$ -- total number of iTrees,  \\\\\n\t\t\t\t$p(\\cdot|\\cdot)$ -- traversed node path,\\\\ \n\t\t\t\t$g(\\cdot|\\cdot)$ -- averaged deviation degree\n\t\t\t} \\\\\n\t\t\t%\t\t$\\phi$ - neural network \\\\\n\t\t\t\\hline\n\t\\end{tabular}}%\n\t\\label{tab:notation}%\n\\end{table}",
            "tab:datainfo": "\\begin{table}[t]\n\t\\centering\n\t\\caption{Dataset information of the used tabular, graph, and time series (TS) datasets. $N$ indicates the number of data objects, with parenthesis denoting the pre-defined training/testing size of graph and time series datasets. $D$ denotes the number of dimensionality in tabular datasets, the average number of nodes per graph in graph datasets, and the number of sequences in each time series dataset. \\#Anom (ratio) is the number of anomalies and the corresponding ratios. }\n\t\\scalebox{0.9}{\n\t\t\\begin{tabular}{lllll}\n\t\t\t\\hline\n\t\t\t& \\textbf{Data} & \\textbf{$N$} & \\textbf{$D$} & \\textbf{\\#Anom (ratio)} \\\\\n\t\t\t\\hline\n\t\t\t% \\multicolumn{5}{c}{Tabular Data}  \\\\\n\t\t\t% \\hline\n\t\t\t\\multirow{10}[0]{*}{\\rotatebox{90}{Tabular}} \n\t\t\t& Analysis & 95,677 & 197   & 2,677 (2.80\\%) \\\\\n\t\t\t& Backdoor & 95,329 & 197   & 2,329 (2.44\\%) \\\\\n\t\t\t& DoS   & 96,000 & 197   & 3,000 (3.13\\%) \\\\\n\t\t\t& Exploits & 96,000 & 197   & 3,000 (3.13\\%) \\\\\n\t\t\t& R8    & 3,974 & 9,468 & 51 (1.28\\%) \\\\\n\t\t\t& Cover & 286,048 & 11    & 2,747 (0.96\\%) \\\\\n\t\t\t& Fraud & 284,807 & 30    & 492 (0.17\\%) \\\\\n\t\t\t& Pageblocks & 5,393 & 11    & 510 (9.46\\%) \\\\\n\t\t\t& Shuttle & 1,013 & 10    & 13 (1.28\\%) \\\\\n\t\t\t& Thrombin & 1,909 & 139,352 & 42 (2.20\\%) \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{4}[0]{*}{\\rotatebox{90}{Graph}} & HSE   & 8,417 (8,150/267) & 17    & 10 (3.75\\%) \\\\\n\t\t\t& MMP   & 7,558 (7,320/238) & 18    & 38 (0.50\\%) \\\\\n\t\t\t& p53   & 8,903 (8,634/269) & 18    & 28 (10.41\\%) \\\\\n\t\t\t& PPAR  & 8,451 (8,184/267) & 17    & 15 (5.62\\%) \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{4}[0]{*}{\\rotatebox{90}{TS}} & Mars  & 11,349 (3,500/7,849) & 5     & 64 (0.82\\%) \\\\\n\t\t\t& Gait  & 65,000 (22,167/42,833) & 3     & 401 (0.94\\%) \\\\\n\t\t\t& Heart & 55,374 (20,185/35,189) & 3     & 201 (0.57\\%) \\\\\n\t\t\t& Heart-w & 80,000 (20,000/60,000) & 1     & 251 (0.42\\%) \\\\\n\t\t\t\\hline\n\t\\end{tabular}}%\n\t\\label{tab:datainfo}%\n\\end{table}",
            "tab:tabular1": "\\begin{table*}[htbp]\n\t\\centering\n\t\\caption{AUC-ROC and AUC-PR performance (mean $\\pm$ standard deviation) of DIF and IF-based competing methods on ten real-world tabular datasets. PID and EIF runs out of memory (OOM) on the ultrahigh-dimensional dataset \\textit{Thrombin}. The best performer is boldfaced. \n\t}\n\t\\scalebox{0.9}{\n\t\t\\begin{tabular}{\n\t\t\t\tp{1.3cm} |\n\t\t\t\tp{1.4cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering} |\n\t\t\t\tp{1.4cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering} \n\t\t\t}\n\t\t\t\n\t\t\t\\hline\n\t\t\t\\multirow{2}[0]{*}{\\textbf{Data}} & \\multicolumn{5}{c|}{\\textbf{AUC-ROC}} & \\multicolumn{5}{c}{\\textbf{AUC-PR}} \\\\ \n\t\t\t\n\t\t\t\\cline{2-11}\n\t\t\t& \\textbf{DIF (ours)} & \\textbf{EIF} & \\textbf{PID} & \\textbf{LeSiNN} & \\textbf{IF} & \\textbf{DIF (ours)} & \\textbf{EIF} & \\textbf{PID} & \\textbf{LeSiNN} & \\textbf{IF} \\\\\n\t\t\t\\hline\n\t\t\t\n\t\t\tAnalysis & \\textbf{0.931$_{\\pm0.006}$} & 0.910$_{\\pm0.005}$ & 0.820$_{\\pm0.019}$ & 0.903$_{\\pm0.008}$ & 0.782$_{\\pm0.017}$ & \\textbf{0.404$_{\\pm0.051}$} & 0.198$_{\\pm0.022}$ & 0.075$_{\\pm0.007}$ & 0.183$_{\\pm0.028}$ & 0.063$_{\\pm0.006}$ \\\\\n\t\t\tBackdoor & \\textbf{0.918$_{\\pm0.002}$} & 0.902$_{\\pm0.005}$ & 0.808$_{\\pm0.016}$ & 0.894$_{\\pm0.006}$ & 0.731$_{\\pm0.021}$ & \\textbf{0.453$_{\\pm0.051}$} & 0.218$_{\\pm0.028}$ & 0.066$_{\\pm0.005}$ & 0.205$_{\\pm0.031}$ & 0.046$_{\\pm0.004}$ \\\\\n\t\t\tDoS   & \\textbf{0.932$_{\\pm0.003}$} & 0.918$_{\\pm0.004}$ & 0.802$_{\\pm0.013}$ & 0.896$_{\\pm0.009}$ & 0.747$_{\\pm0.020}$ & \\textbf{0.440$_{\\pm0.023}$} & 0.269$_{\\pm0.027}$ & 0.075$_{\\pm0.004}$ & 0.185$_{\\pm0.028}$ & 0.060$_{\\pm0.005}$ \\\\\n\t\t\tExploits & \\textbf{0.858$_{\\pm0.010}$} & 0.840$_{\\pm0.008}$ & 0.797$_{\\pm0.011}$ & 0.816$_{\\pm0.005}$ & 0.745$_{\\pm0.010}$ & \\textbf{0.273$_{\\pm0.020}$} & 0.167$_{\\pm0.011}$ & 0.077$_{\\pm0.003}$ & 0.120$_{\\pm0.013}$ & 0.062$_{\\pm0.003}$ \\\\\n\t\t\tR8    & \\textbf{0.930$_{\\pm0.008}$} & 0.854$_{\\pm0.006}$ & 0.881$_{\\pm0.018}$ & 0.859$_{\\pm0.001}$ & 0.853$_{\\pm0.016}$ & \\textbf{0.145$_{\\pm0.031}$} & 0.101$_{\\pm0.009}$ & 0.078$_{\\pm0.011}$ & 0.094$_{\\pm0.000}$ & 0.075$_{\\pm0.008}$ \\\\\n\t\t\tCover & \\textbf{0.972$_{\\pm0.010}$} & 0.872$_{\\pm0.017}$ & 0.939$_{\\pm0.007}$ & 0.885$_{\\pm0.008}$ & 0.888$_{\\pm0.017}$ & \\textbf{0.246$_{\\pm0.069}$} & 0.040$_{\\pm0.006}$ & 0.069$_{\\pm0.006}$ & 0.051$_{\\pm0.004}$ & 0.055$_{\\pm0.008}$ \\\\\n\t\t\tFraud & \\textbf{0.953$_{\\pm0.002}$} & 0.950$_{\\pm0.001}$ & 0.950$_{\\pm0.002}$ & 0.952$_{\\pm0.000}$ & 0.950$_{\\pm0.001}$ & 0.387$_{\\pm0.031}$ & 0.378$_{\\pm0.027}$ & 0.186$_{\\pm0.033}$ & \\textbf{0.401$_{\\pm0.001}$} & 0.155$_{\\pm0.015}$ \\\\\n\t\t\tPageblocks & \\textbf{0.903$_{\\pm0.006}$} & 0.902$_{\\pm0.001}$ & 0.851$_{\\pm0.003}$ & 0.887$_{\\pm0.002}$ & 0.900$_{\\pm0.005}$ & \\textbf{0.547$_{\\pm0.012}$} & 0.537$_{\\pm0.006}$ & 0.421$_{\\pm0.011}$ & 0.511$_{\\pm0.007}$ & 0.476$_{\\pm0.013}$ \\\\\n\t\t\tShuttle & \\textbf{0.941$_{\\pm0.006}$} & 0.843$_{\\pm0.009}$ & 0.864$_{\\pm0.017}$ & 0.805$_{\\pm0.005}$ & 0.862$_{\\pm0.019}$ & \\textbf{0.150$_{\\pm0.017}$} & 0.061$_{\\pm0.003}$ & 0.059$_{\\pm0.008}$ & 0.048$_{\\pm0.001}$ & 0.075$_{\\pm0.014}$ \\\\\n\t\t\tThrombin & \\textbf{0.913$_{\\pm0.003}$} & OOM   & OOM   & 0.912$_{\\pm0.000}$ & 0.905$_{\\pm0.002}$ & \\textbf{0.468$_{\\pm0.020}$} & OOM & OOM & 0.458$_{\\pm0.001}$ & 0.372$_{\\pm0.008}$ \\\\\n\t\t\t\\hline\n\t\t\t\\textit{Average}   & \\textbf{0.925$_{\\pm0.006}$} & 0.888$_{\\pm0.006}$ & 0.857$_{\\pm0.011}$ & 0.881$_{\\pm0.004}$ & 0.836$_{\\pm0.013}$ & \\textbf{0.351$_{\\pm0.033}$} & 0.219$_{\\pm0.015}$ & 0.123$_{\\pm0.010}$ & 0.226$_{\\pm0.011}$ & 0.144$_{\\pm0.008}$ \\\\\n\t\t\t\\textit{p-value} & - & 0.004 & 0.004 & 0.002 & 0.002 &   -   & 0.004 & 0.004 & 0.006 & 0.002 \\\\\n\t\t\t\\hline\n\t\t\t\n\t\\end{tabular}}%\n\t\\label{tab:tabular1}%\n\\end{table*}",
            "tab:tabular2": "\\begin{table*}[htbp]\n\t\\centering\n\t\\caption{AUC-ROC and AUC-PR performance of DIF and its deep ensemble-based competing methods. \n\t}\n\t\\scalebox{0.9}{\n\t\t\\begin{tabular}{\n\t\t\t\tp{1.3cm} |\n\t\t\t\tp{1.4cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering} |\n\t\t\t\tp{1.4cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering} \n\t\t\t}\n\t\t\t\n\t\t\t\\hline\n\t\t\t\\multirow{2}[0]{*}{\\textbf{Data}} & \\multicolumn{5}{c|}{\\textbf{AUC-ROC}} & \\multicolumn{5}{c}{\\textbf{AUC-PR}} \\\\ \n\t\t\t\n\t\t\t\\cline{2-11}\n\t\t\t& \\textbf{DIF (ours)} & \\textbf{eRDP} & \\textbf{eREPEN} & \\textbf{eDSVDD} & \\textbf{eRECON} &\n\t\t\t\\textbf{DIF (ours)} & \\textbf{eRDP} & \\textbf{eREPEN} & \\textbf{eDSVDD} & \\textbf{eRECON}   \\\\\n\t\t\t\\hline\n\t\t\t\n\t\t\tAnalysis & \\textbf{0.931$_{\\pm0.006}$} & 0.899$_{\\pm0.005}$ & 0.883$_{\\pm0.026}$ & 0.844$_{\\pm0.006}$ & 0.862$_{\\pm0.002}$ & \\textbf{0.404$_{\\pm0.051}$} & 0.400$_{\\pm0.019}$ & 0.168$_{\\pm0.051}$ & 0.374$_{\\pm0.028}$ & 0.093$_{\\pm0.002}$ \\\\\n\t\t\tBackdoor & \\textbf{0.918$_{\\pm0.002}$} & 0.900$_{\\pm0.006}$ & 0.863$_{\\pm0.022}$ & 0.916$_{\\pm0.004}$ & 0.840$_{\\pm0.003}$ & \\textbf{0.453$_{\\pm0.051}$} & 0.422$_{\\pm0.020}$ & 0.163$_{\\pm0.055}$ & 0.448$_{\\pm0.030}$ & 0.082$_{\\pm0.004}$ \\\\\n\t\t\tDoS   & \\textbf{0.932$_{\\pm0.003}$} & 0.905$_{\\pm0.004}$ & 0.861$_{\\pm0.025}$ & 0.900$_{\\pm0.004}$ & 0.856$_{\\pm0.004}$ & \\textbf{0.440$_{\\pm0.023}$} & 0.420$_{\\pm0.013}$ & 0.154$_{\\pm0.047}$ & 0.409$_{\\pm0.028}$ & 0.105$_{\\pm0.004}$ \\\\\n\t\t\tExploits & \\textbf{0.858$_{\\pm0.010}$} & 0.795$_{\\pm0.008}$ & 0.744$_{\\pm0.040}$ & 0.748$_{\\pm0.017}$ & 0.808$_{\\pm0.002}$ & \\textbf{0.273$_{\\pm0.020}$} & 0.250$_{\\pm0.008}$ & 0.080$_{\\pm0.019}$ & 0.249$_{\\pm0.013}$ & 0.086$_{\\pm0.001}$ \\\\\n\t\t\tR8    & \\textbf{0.930$_{\\pm0.008}$} & 0.864$_{\\pm0.020}$ & 0.904$_{\\pm0.004}$ & 0.883$_{\\pm0.014}$ & 0.779$_{\\pm0.000}$ & \\textbf{0.145$_{\\pm0.031}$} & 0.123$_{\\pm0.017}$ & 0.086$_{\\pm0.005}$ & 0.074$_{\\pm0.006}$ & 0.082$_{\\pm0.000}$ \\\\\n\t\t\tCover & 0.972$_{\\pm0.010}$ & 0.966$_{\\pm0.008}$ & 0.896$_{\\pm0.013}$ & \\textbf{0.981$_{\\pm0.005}$} & 0.922$_{\\pm0.004}$ & 0.246$_{\\pm0.069}$ & 0.192$_{\\pm0.049}$ & 0.044$_{\\pm0.006}$ & \\textbf{0.353$_{\\pm0.055}$} & 0.067$_{\\pm0.003}$ \\\\\n\t\t\tFraud & 0.953$_{\\pm0.002}$ & 0.947$_{\\pm0.002}$ & \\textbf{0.954$_{\\pm0.001}$} & 0.948$_{\\pm0.002}$ & 0.946$_{\\pm0.001}$ & 0.387$_{\\pm0.039}$ & 0.329$_{\\pm0.025}$ & 0.345$_{\\pm0.030}$ & \\textbf{0.480$_{\\pm0.026}$} & 0.372$_{\\pm0.002}$ \\\\\n\t\t\tPageblocks & \\textbf{0.903$_{\\pm0.010}$} & 0.850$_{\\pm0.005}$ & 0.896$_{\\pm0.005}$ & 0.882$_{\\pm0.005}$ & 0.829$_{\\pm0.012}$ & \\textbf{0.547$_{\\pm0.020}$} & 0.466$_{\\pm0.011}$ & 0.511$_{\\pm0.019}$ & 0.397$_{\\pm0.009}$ & 0.542$_{\\pm0.032}$ \\\\\n\t\t\tShuttle & \\textbf{0.941$_{\\pm0.006}$} & 0.900$_{\\pm0.009}$ & 0.780$_{\\pm0.025}$ & 0.837$_{\\pm0.037}$ & 0.779$_{\\pm0.014}$ & \\textbf{0.150$_{\\pm0.017}$} & 0.132$_{\\pm0.009}$ & 0.031$_{\\pm0.003}$ & 0.144$_{\\pm0.022}$ & 0.044$_{\\pm0.002}$ \\\\\n\t\t\tThrombin & 0.913$_{\\pm0.003}$ & 0.869$_{\\pm0.019}$ & \\textbf{0.914$_{\\pm0.004}$} & 0.366$_{\\pm0.019}$ & 0.911$_{\\pm0.000}$ & \\textbf{0.468$_{\\pm0.020}$} & 0.368$_{\\pm0.053}$ & 0.399$_{\\pm0.056}$ & 0.016$_{\\pm0.001}$ & 0.457$_{\\pm0.000}$ \\\\\n\t\t\t\\hline\n\t\t\t\\textit{Average}   & \\textbf{0.925$_{\\pm0.006}$} & 0.889$_{\\pm0.009}$ & 0.870$_{\\pm0.016}$ & 0.831$_{\\pm0.011}$ & 0.853$_{\\pm0.004}$ & \\textbf{0.351$_{\\pm0.034}$} & 0.310$_{\\pm0.022}$ & 0.198$_{\\pm0.029}$ & 0.294$_{\\pm0.022}$ & 0.193$_{\\pm0.005}$ \\\\\n\t\t\t\n\t\t\t\\textit{p-value} & - & 0.002 & 0.01  & 0.01  & 0.002 &    -   & 0.002 & 0.002 & 0.232 & 0.002 \\\\\n\t\t\t\n\t\t\t\\hline\n\t\\end{tabular}}%\n\t\\label{tab:tabular2}%\n\\end{table*}",
            "tab:graphts": "\\begin{table*}[htbp]\n\t\\centering\n\t\\caption{Results on detecting abnormal graphs and anomalies in time series.}\n\t\\scalebox{0.9}{\n\t\t\\begin{tabular}{\n\t\t\t\tp{0.3cm} p{1.2cm} |\n\t\t\t\tp{1.4cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering}|\n\t\t\t\tp{1.4cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering}p{1.3cm}<{\\centering}\n\t\t\t\tp{1.3cm}<{\\centering}}\n\t\t\t\n\t\t\t\\hline\n\t\t\t\n\t\t\t& \\multirow{2}[0]{*}{\\textbf{Data}}  & \\multicolumn{5}{c|}{\\textbf{AUC-ROC}}  & \\multicolumn{5}{c}{\\textbf{AUC-PR}} \\\\\n\t\t\t\\cline{3-12}\n\t\t\t& & \\textbf{DIF (ours)} & \\textbf{EIF} & \\textbf{LeSiNN} & \\textbf{iForest} & \\textbf{eGLocalKD} & \\textbf{DIF (ours)} & \\textbf{EIF} & \\textbf{LeSiNN} & \\textbf{iForest} & \\textbf{eGLocalKD} \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{4}[0]{*}{\\rotatebox{90}{Graph}} \n\t\t\t& HSE   & \\textbf{0.737$_{\\pm0.013}$} & 0.715$_{\\pm0.014}$ & 0.702$_{\\pm0.001}$ & 0.697$_{\\pm0.014}$ & 0.593$_{\\pm0.002}$ & \\textbf{0.094$_{\\pm0.005}$} & 0.088$_{\\pm0.004}$ & 0.084$_{\\pm0.000}$ & 0.082$_{\\pm0.004}$ & 0.054$_{\\pm0.000}$ \\\\\n\t\t\t& MMP   & \\textbf{0.715$_{\\pm0.006}$} & 0.663$_{\\pm0.012}$ & 0.666$_{\\pm0.000}$ & 0.667$_{\\pm0.018}$ & 0.675$_{\\pm0.001}$ & \\textbf{0.260$_{\\pm0.006}$} & 0.216$_{\\pm0.006}$ & 0.217$_{\\pm0.000}$ & 0.219$_{\\pm0.011}$ & 0.233$_{\\pm0.001}$ \\\\\n\t\t\t& p53   & \\textbf{0.680$_{\\pm0.008}$} & 0.597$_{\\pm0.017}$ & 0.606$_{\\pm0.000}$ & 0.619$_{\\pm0.013}$ & 0.640$_{\\pm0.001}$ & \\textbf{0.177$_{\\pm0.006}$} & 0.138$_{\\pm0.004}$ & 0.144$_{\\pm0.000}$ & 0.143$_{\\pm0.004}$ & 0.150$_{\\pm0.000}$ \\\\\n\t\t\t& PPAR  & 0.701$_{\\pm0.013}$ & 0.716$_{\\pm0.005}$ & 0.711$_{\\pm0.000}$ & \\textbf{0.733$_{\\pm0.009}$} & 0.643$_{\\pm0.001}$ & 0.127$_{\\pm0.008}$ & 0.173$_{\\pm0.006}$ & 0.165$_{\\pm0.001}$ & \\textbf{0.208$_{\\pm0.012}$} & 0.086$_{\\pm0.000}$ \\\\\n\t\t\t\n\t\t\t\\midrule\t\t\n\t\t\t&\t& \\textbf{DIF (ours)} & \\textbf{EIF} & \\textbf{LeSiNN} & \\textbf{iForest} & \\textbf{eTranAD} & \\textbf{DIF (ours)} & \\textbf{EIF} & \\textbf{LeSiNN} & \\textbf{iForest} & \\textbf{eTranAD} \\\\\n\t\t\t\\hline\n\t\t\t\n\t\t\t\\multirow{4}[0]{*}{\\rotatebox{90}{TS}} &\tMars  & 0.952$_{\\pm0.017}$ & \\textbf{0.980$_{\\pm0.006}$} & 0.942$_{\\pm0.014}$ & 0.947$_{\\pm0.015}$ & 0.947$_{\\pm0.016}$ & \\textbf{0.626$_{\\pm0.024}$} & 0.458$_{\\pm0.031}$ & 0.400$_{\\pm0.009}$ & 0.390$_{\\pm0.043}$ & 0.334$_{\\pm0.020}$ \\\\\n\t\t\t& Gait  & \\textbf{0.998$_{\\pm0.001}$} & 0.997$_{\\pm0.001}$ & 0.998$_{\\pm0.000}$ & 0.997$_{\\pm0.001}$ & 0.998$_{\\pm0.000}$ & \\textbf{0.835$_{\\pm0.064}$} & 0.772$_{\\pm0.048}$ & 0.829$_{\\pm0.010}$ & 0.741$_{\\pm0.073}$ & 0.806$_{\\pm0.010}$ \\\\\n\t\t\t& ECG & \\textbf{0.997$_{\\pm0.001}$} & 0.986$_{\\pm0.001}$ & 0.987$_{\\pm0.000}$ & 0.987$_{\\pm0.001}$ & 0.976$_{\\pm0.001}$ & \\textbf{0.809$_{\\pm0.031}$} & 0.705$_{\\pm0.004}$ & 0.710$_{\\pm0.000}$ & 0.711$_{\\pm0.002}$ & 0.692$_{\\pm0.001}$ \\\\\n\t\t\t& ECG-w & \\textbf{1.000$_{\\pm0.000}$} & 0.988$_{\\pm0.001}$ & 0.985$_{\\pm0.001}$ & 0.981$_{\\pm0.001}$ & 0.990$_{\\pm0.000}$ & \\textbf{1.000$_{\\pm0.000}$} & 0.255$_{\\pm0.011}$ & 0.219$_{\\pm0.008}$ & 0.181$_{\\pm0.005}$ & 0.297$_{\\pm0.001}$ \\\\\n\t\t\t\\hline\n\t\\end{tabular}}%\n\t\\label{tab:graphts}%\n\\end{table*}",
            "tab:ablation": "\\begin{table}[t]\n\\centering\n\\caption{AUC-ROC and AUC-PR results of DIF and \\textbf{w/o} DEAS, and training time (in seconds) of DIF and \\textbf{w/o} CERE. \\textbf{w/o} DEAS is an ablated version by replacing DEAS with the standard scoring function. \\textbf{w/o} CERE only uses the typical method to produce the representation ensemble.  }\n\\scalebox{0.86}{\n\\begin{tabular}{p{1.3cm} |\n\t\tp{0.5cm}<{\\centering}\n\t\tp{1.5cm}<{\\centering} |\n\t\tp{0.5cm}<{\\centering}\n\t\tp{1.5cm}<{\\centering} |\n\t\tp{0.5cm}<{\\centering}p{1.4cm}<{\\centering} }\n\t\\hline\n\t\\multirow{2}[0]{*}{\\textbf{Data}} & \\multicolumn{2}{c|}{\\textbf{AUC-ROC}} & \\multicolumn{2}{c|}{\\textbf{AUC-PR}} & \\multicolumn{2}{c}{\\textbf{Time (in seconds)}} \\\\\n\t\\cline{2-7}\n\t& \\textbf{DIF} & \\textbf{w/o} DEAS & \\textbf{DIF} & \\textbf{w/o} DEAS & \\textbf{DIF} & \\textbf{w/o} CERE \\\\\n\t\\hline\n\t\n\tAnalysis & \\textbf{0.931} & 0.922$_{\\pm0.007}$ & \\textbf{0.404} & 0.315$_{\\pm0.058}$ & 10.5  & 69.2  \\\\\n\tBackdoor & \\textbf{0.918} & 0.914$_{\\pm0.006}$ & \\textbf{0.453} & 0.381$_{\\pm0.067}$ & 10.5  & 66.2  \\\\\n\tDoS   & \\textbf{0.932} & 0.926$_{\\pm0.007}$ & \\textbf{0.440} & 0.388$_{\\pm0.074}$ & 10.4  & 65.6  \\\\\n\tExploits & \\textbf{0.858} & 0.854$_{\\pm0.008}$ & \\textbf{0.273} & 0.248$_{\\pm0.041}$ & 10.4  & 65.8  \\\\\n\tR8    & \\textbf{0.930} & 0.915$_{\\pm0.012}$ & \\textbf{0.145} & 0.123$_{\\pm0.034}$ & 5.2   & 19.2  \\\\\n\tCover & \\textbf{0.972} & 0.964$_{\\pm0.014}$ & \\textbf{0.246} & 0.210$_{\\pm0.084}$ & 33.0  & 154.3  \\\\\n\tFraud & \\textbf{0.953} & 0.952$_{\\pm0.002}$ & \\textbf{0.387} & \\textbf{0.387$_{\\pm0.042}$} & 33.6  & 169.1  \\\\\n\tPageblocks & 0.903 & \\textbf{0.912$_{\\pm0.007}$} & 0.547 & \\textbf{0.576$_{\\pm0.024}$} & 0.8   & 3.5  \\\\\n\tShuttle & \\textbf{0.941} & 0.923$_{\\pm0.012}$ & \\textbf{0.150} & 0.103$_{\\pm0.019}$ & 0.5   & 1.0  \\\\\n\tThrombin & 0.913 & \\textbf{0.916$_{\\pm0.002}$} & \\textbf{0.468} & 0.448$_{\\pm0.020}$ & 13.8  & 336.7  \\\\\n\t\\hline\n\t\\textit{Avg./Total} & \\textbf{0.925} & 0.920$_{\\pm0.008}$ & \\textbf{0.351} & 0.318$_{\\pm0.046}$ & 128.7  & 950.6  \\\\\n\t\n\t\\textit{p-value} &   -    & 0.07  &   -    & 0.03  &    -   &  - \\\\\n\t\n\t\\hline\n\\end{tabular}\n}%\n\\label{tab:ablation}%\n\\end{table}"
        },
        "figures": {
            "fig:hardanom": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.48\\textwidth]{fig2208/fig2208_hard_anom.pdf}\n\t\\caption{Illustration of the hard anomaly challenge. The left figure is a synthetic dataset, where red triangles are anomalies and blue points are normal samples. The following figures are projected spaces (i.e., random representations) created by our method. \n\t\tLinear data partition (in either vertical/horizontal or oblique forms) used in existing methods cannot effectively isolate these hard anomalies in the original data space. By contrast, these anomalies are possible to be exposed in newly created spaces. \n\t}\n\t\\label{fig:hardanom}\n\\end{figure}",
            "fig:teaser": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.48\\textwidth]{fig2208/fig2208_case.pdf}\n\t\\caption{Illustration of the algorithmic bias problem.\n\t\tThe three figures on the far left are three synthetic datasets, and the following panels of each dataset are the anomaly score maps produced by iForest and its two state-of-the-art extensions -- PID and EIF -- and our method DIF. Score maps indicate anomaly score distribution in the full data space (deeper colour indicates higher abnormality). \n\t\tBlack contour lines in each score map denote the 99th percentile of the anomaly scores of the original data, which can be seen as the boundary of data normality predicted by each anomaly detector. \n\t\tRed triangles represent possible anomalies.\n\t\tThere are some artefact regions in the score maps of iForest and its two extensions, and these methods may fail to identify the anomalies since these anomalies are assigned similarly low anomaly scores as the presented normal samples (they are included in the predicted normal areas). By contrast, DIF produces more accurate and smooth score maps, and DIF can successfully assign a high abnormal degree to anomalies compared to those original data samples.\n\t}\n\t\\label{fig:teaser}\n\\end{figure}",
            "fig:proj": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.49\\textwidth]{fig2208/fig2208_projection.pdf}\n\t\\caption{\n\t\tData space transformation produced by casually initialised neural networks (original data space vs. three transferred data spaces). \n\t}\n\t\\label{fig:proj}\n\\end{figure}",
            "fig:case": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.49\\textwidth]{fig2208/fig2303_showcase.pdf}\n\t\\caption{(\\textbf{Top to bottom}) Snippets of training/testing data of \\textit{ECG-w} with wandering baseline and detection results, i.e., anomaly scores, of DIF and its competitors. Anomalous heartbeats are highlighted in golden yellow. Red dashed lines indicate the reported highest anomaly scores in this anomalous duration. }\n\t\\label{fig:case}\n\\end{figure}",
            "fig:scal": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.49\\textwidth]{fig2208/fig2208_scalability.pdf}\n\t\\caption{Scalability test results. (\\textbf{Top}) The training time of all the anomaly detectors on a CPU device; and (\\textbf{Bottom}) The results of deep ensemble-based methods (including DIF) on a GPU device. }\n\t\\label{fig:scal}\n\\end{figure}",
            "fig:robustness": "\\begin{figure*}[htbp]\n\t\\centering \n\t% \t\\subfigbottomskip=-10pt\n\t\\subfigcapskip=-5pt %\n\t\\subfigure[DIF vs. IF-based Competing Methods]{\n\t\t\\includegraphics[width=0.49\\linewidth]{fig2208/fig2208_contamination1.pdf}} \n\t\\subfigure[DIF vs. Deep Ensemble-based Competing Methods]{\n\t\t\\includegraphics[width=0.49\\linewidth]{fig2208/fig2208_contamination2.pdf}}\n\t\\caption{AUC-ROC w.r.t. different contamination ratios $\\rho$ (the percentage of anomalies in the training set). \n\t}\n\t\\label{fig:robustness}\n\\end{figure*}",
            "fig:random": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.47\\textwidth]{fig2208/fig2208_whyrr.pdf}\n\t\\caption{(\\textbf{Top}) AUC-ROC of DIF and its variants that use optimised representations, and (\\textbf{Bottom}) the quality (measured by $AII$) distribution of randomised representations used in DIF and representations optimised by RDP, REPEN, DSVDD, and RECON.}\n\t\\label{fig:random}\n\\end{figure}",
            "fig:whyif": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.46\\textwidth]{fig2208/fig2208_whyif.pdf}\n\\caption{(\\textbf{Top}) AUC-ROC of DIF and its variants of using other anomaly scoring strategies upon random representations, and (\\textbf{Bottom}) their effectiveness (measured by AUC-ROC) on each ensemble member within the randomised representation groups. \n}\n\\label{fig:whyif}\n\\end{figure}"
        },
        "equations": {
            "eq:eqn:rep": "\\begin{equation}\\label{eqn:rep}\n\t\\mathscr{G}(\\mathcal{D}) = \\big\\{ \\mathcal{X}_u \\subset \\mathbb{R}^d \\big \\vert  \\mathcal{X}_u = \\phi_u(\\mathcal{D};\\theta_u)  \\big\\}_{u=1}^{r},\n\\end{equation}",
            "eq:eqn:score_func_class": "\\begin{equation}\\label{eqn:score_func_class}\n\t\\mathscr{F}(\\bm{o} | \\mathcal{T}) = \\Omega_{\\tau_i \\sim \\mathcal{T}} I(\\bm{o} | \\tau_i),\n\\end{equation}",
            "eq:1": "\\begin{equation}\n\t\\mathbf{W}_i = \\mathbf{W}_0 \\circ (\\bm{p}_i \\bm{q}_i^\\top ) ,\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\t\\begin{split}\n\t\t\\mathbf{W}_i^{\\top} \\bm{x}   &= (\\mathbf{W}_0 \\circ \\bm{p}_i \\bm{q}_i^\\top)^{\\top} \\bm{x} \\\\\n\t\t&= \\mathbf{W}_0^{\\top} (\\bm{x} \\circ \\bm{p}_i) \\circ \\bm{q}_i .\n\t\\end{split}\n\\end{equation}",
            "eq:eqn:batch": "\\begin{equation}\\label{eqn:batch}\n\t\\begin{split}\n\t\t\\begin{bmatrix}\n\t\t\t\\mathcal{X}\\mathbf{W}_1 \\\\\n\t\t\t\\mathcal{X}\\mathbf{W}_2 \\\\\n\t\t\t\\cdots \\\\\n\t\t\t\\mathcal{X} \\mathbf{W}_r \\\\\n\t\t\\end{bmatrix} \n\t\t=\n\t\t\\Bigg ( \\bigg ( \n\t\t\\begin{bmatrix}\n\t\t\t\\mathcal{X} \\\\\n\t\t\t\\mathcal{X} \\\\\n\t\t\t\\cdots \\\\\n\t\t\t\\mathcal{X} \\\\\n\t\t\\end{bmatrix} \n\t\t\\circ\n\t\t\\begin{bmatrix}\n\t\t\t\\mathbf{P}_1 \\\\\n\t\t\t\\mathbf{P}_2 \\\\\n\t\t\t\\cdots \\\\\n\t\t\t\\mathbf{P}_r \\\\\n\t\t\\end{bmatrix}\n\t\t\\bigg )\n\t\t\\mathbf{W}_0 \\Bigg ) \n\t\t\\circ\n\t\t\\begin{bmatrix}\n\t\t\t\\mathbf{Q}_1 \\\\\n\t\t\t\\mathbf{Q}_2 \\\\\n\t\t\t\\cdots \\\\\n\t\t\t\\mathbf{Q}_r \\\\\n\t\t\\end{bmatrix},\n\t\\end{split}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\t\\mathscr{G}_{\\text{CERE}}(\\mathcal{D}) = \\Phi \\big( \\mathcal{D} ; \\Theta \\big) = \\big\\{ \\mathcal{X}_i \\subset \\mathbb{R}^d \\big\\}_{i=1}^{r} ,\n\\end{equation}",
            "eq:eqn:deviation": "\\begin{equation}\\label{eqn:deviation}\n\tg(\\bm{x}_u | \\tau_i)  = \\frac{1}{ | p(\\bm{x}_u | \\tau_i) |} \\sum_{k \\in p(\\bm{x}_u | \\tau_i)} \\vert \\bm{x}_u^{(j_{k})} -  \\eta_{k} \\vert .\n\\end{equation}",
            "eq:eqn:scoring": "\\begin{equation}\\label{eqn:scoring}\n\t\\mathscr{F}_{\\text{DEAS}}(\\bm{o}|\\mathcal{T}) \n\t= 2^{\n\t\t-\\mathbb{E}_{\\tau_i \\in \\mathcal{T}} \\frac{ |p(\\bm{x}_u| \\tau_i)|}{C(T)}\n\t} \n\t\\times \n\t\\mathbb{E}_{\\tau_i \\in \\mathcal{T}}\\big(g(\\bm{x}_u | \\tau_i) \\big),\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\tAII = \n\t\\mathop{\\mathbf{P}}\\limits_{\\bm{a} \\sim \\mathcal{A}} \\Big( \\mathop{median}\\limits_{\\bm{n}_i \\in \\mathcal{N}} \\big \\{\\frac{1}{|\\mathcal{C}|} \\sum_{\\bm{n}_j \\in \\mathcal{C}} \\big( \\omega(\\bm{a} | \\bm{n}_i, \\bm{n}_j) \\big) \\big \\} >0 \\Big ),\n\\end{equation}"
        },
        "git_link": "https://github.com/xuhongzuo/deep-iforest"
    }
}