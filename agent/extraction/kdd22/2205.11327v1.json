{
    "meta_info": {
        "title": "HyperLogLogLog: Cardinality Estimation With One Log More",
        "abstract": "We present HyperLogLogLog, a practical compression of the HyperLogLog sketch\nthat compresses the sketch from $O(m\\log\\log n)$ bits down to $m\n\\log_2\\log_2\\log_2 m + O(m+\\log\\log n)$ bits for estimating the number of\ndistinct elements~$n$ using $m$~registers. The algorithm works as a drop-in\nreplacement that preserves all estimation properties of the HyperLogLog sketch,\nit is possible to convert back and forth between the compressed and\nuncompressed representations, and the compressed sketch maintains mergeability\nin the compressed domain. The compressed sketch can be updated in amortized\nconstant time, assuming $n$ is sufficiently larger than $m$. We provide a C++\nimplementation of the sketch, and show by experimental evaluation against\nwell-known implementations by Google and Apache that our implementation\nprovides small sketches while maintaining competitive update and merge times.\nConcretely, we observed approximately a 40% reduction in the sketch size.\nFurthermore, we obtain as a corollary a theoretical algorithm that compresses\nthe sketch down to $m\\log_2\\log_2\\log_2\\log_2 m+O(m\\log\\log\\log m/\\log\\log\nm+\\log\\log n)$ bits.",
        "author": "Matti Karppa, Rasmus Pagh",
        "link": "http://arxiv.org/abs/2205.11327v1",
        "category": [
            "cs.DS"
        ],
        "additionl_info": "10 pages, 7 figures, KDD '22"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n\nCounting the number of distinct elements, or the \\emph{cardinality},\nof a data stream is a basic operation that has applications in network\ntraffic analysis~\\cite{LiuCG:2016}, organization of large\ndatasets~\\cite{HalevyKNOPRW:2016}, genome\nanalysis~\\cite{BreitwieserBS:2018}, analysis of social\nnetworks~\\cite{BackstromBRUV:2012}, including large-scale industrial\napplications such as estimating the number of distinct Google search\nqueries~\\cite{HeuleNH:2013}. The na\\\"ive approach of storing all\nunique elements in the data stream quickly becomes prohibitive as the\nnumber of distinct elements in the data stream grows into the order of\nbillions, which calls for sketching approaches that maintain a sketch\nof the cardinality using a limited number of bits.\n\nApart from the sheer number of bits, other design considerations for\nthe sketches include idempotence of updates, that is, that repeated\nupdates with the same element never change the sketch, control on the\nestimation error, efficient updates, such that the sketch can be\nupdated even if the amount of computational resources is limited, and\nmergeability of sketches, such that the data stream can be processed\nby multiple computational units at once and the sketches can be\nmerged to produce output identical to as if a single machine had read\nall the input. The HyperLogLog sketch by Flajolet, Fusy, Gandouet, and\nMeunier~\\cite{FlajoletFGM:2007} is a practical example of such a\nsketch and has become a standard technique for cardinality\nestimation. The HyperLogLog sketch makes use of $m$ counters, or\n\\emph{registers}, of $O(\\log\\log n)$ bits to produce an estimate of the\ncardinality with a relative standard error of $1.04/\\sqrt{m}$.\n\n\\paragraph{Our contribution}\nWe show that if we store offsets from a base\nvalue in the registers and allow $O(m/\\log m)$ registers to overflow,\nstoring the overflowing registers sparsely using $O(\\log m)$ bits each, then,\nwith high probability,\n$m\\log_2\\log_2\\log_2 m+O(m+\\log\\log n)$ bits suffice\nto represent the HyperLogLog sketch. The sketch is a\ntrue compression of the HyperLogLog sketch, preserves all\nestimation properties of HyperLogLog, and is amenable to the tricks\nthat can be used to boost the estimation accuracy of HyperLogLog\nsketches. Furthermore, the sketch preserves mergeability in the\ncompressed domain without having to be decompressed for merging, and we\nshow that the update time is amortized constant for $n$ sufficiently\nlarger than $m$.\nAs a corollary, we obtain a sketch of\n$m\\log_2\\log_2\\log_2\\log_2 m + O(m\\log\\log\\log m /\\log\\log m+\\log\\log n)$ bits if a succinct\ndictionary is used for the sparse representation.  \n\nWe also provide a C++ implementation of the sketch and show\nempirically that it is applicable as a drop-in replacement for\nHyperLogLog in practice and can be implemented with a competitive\nperformance over a wide parameter range. \n\n\n\n\\paragraph{Related work}\nThe HyperLogLog branch of research was initiated by Flajolet and\nMartin in~\\cite{FlajoletM:1985} where the idea of their algorithm was\nto construct an $m\\times w$ bit matrix using a random hash function to\ndivide the data stream into $m$ independend streams, corresponding to\nthe rows of the matrix, where the high bits of the hash value select\nthe substream, and the location of the least significant one-bit in\nthe low-bits of the hash value determines the column; the value thus\ndetermined is set to 1. Each row then produces an estimate of the\ncardinality of the set, and an arithmetic mean is used to lower the\nvariance of the estimate. In~\\cite{DurandF:2003}, this was optimized\nby only storing the maximum of the \\emph{locations} of the first\none-bits, requiring $\\log \\log n$ bits, resulting in the LogLog\nalgorithm and its more practical variant, the\nSuperLogLog. Eventually, this lead to\nHyperLogLog~\\cite{FlajoletFGM:2007}, the key change of which \nwas the use of the harmonic mean instead of the arithmetic mean for lower variance.\n\nThe HyperLogLog sketch has seen a lot of practical applications and\nimprovements, including but not limited to the\nHyperLogLog++~\\cite{HeuleNH:2013} that includes practical engineering\nwork with numerous tricks for reducing memory usage and improving the\naccuracy of the estimates, and the HLL-Tailcut~\\cite{XiaoCZL:2020}\nwhere the registers store the offsets with respect to the minimum\nregister value, and overflowing values are discarded.\n\nOther approaches to counting distinct elements include Linear\nCounting~\\cite{WhangVT:1990}, and\nMinCount~\\cite{Bar-YossefJKST:2002,Giroire:2009} that was used by\nGoogle before being replaced with\nHyperLogLog++~\\cite{HeuleNH:2013}. There exist a number of\n\\emph{non-mergeable} methods for cardinality estimation in recent\nliterature that offer potentially better space--accuracy tradeoffs\nthan their mergeable counterparts, and also methods to convert\nmergeable sketches into more efficient sketches at the cost of\nnon-mergeability. See Pettie, Wang, and Yin~\\cite{PettieWY:2021} for\nan overview and analysis of such methods, along with their\n\\emph{Curtain} sketch.\n\n\nThe HyperLogLog sketch is known to be non-optimal in space\nusage. Both the\nFlajolet-Martin~\\cite{FlajoletM:1985} and the HyperLogLog\nsketch~\\cite{FlajoletFGM:2007} are known to have an entropy of $O(m)$\nbits~\\cite{PettieWY:2021}, suggesting the application of entropy\ncompression methods to achieve the lower bound, although at the\nexpense of losing constant time updates. In~\\cite{Lang:2017}, Lang\napplies entropy compression to the original Flajolet-Martin\nsketch~\\cite{FlajoletM:1985} to get a practical sketch below the\nentropy of the HyperLogLog sketch.  In~\\cite{KaneNW:2010}, Kane,\nNelson, and Woodruff present a theoretical algorithm that achieves the\noptimal $O(m)$ space bound with $O(1)$ updates. This was further\nimproved by B\\l{}asiok~\\cite{Blasiok:2020} by reducing the amount of\nspace required for parallel repetitions of the algorithm for reducing\nthe probability of failure. However, we are not aware of practical\nimplementations of these optimal algorithms, and regard them as mainly\nof theoretical interest.\\footnote{A particular difficulty in implementing these constructions is the use of \\emph{Variable-Bit-Length Arrays} to store an array of variable-length counters compactly. The construction of Blandford and Blelloch~\\cite{BlandfordB:2008} uses constant time per operation, but is complicated and a direct implementation appears to use space that is at least 3 times larger than the entropy of the counters.}\n\n\\paragraph{Organization}\nThe remainder of this paper is organized as follows. In\nSection~\\ref{sect:preliminaries}, we present the mathematical notation\nand preliminaries used in the remainder of the paper. In\nSection~\\ref{sect:hyperlogloglog}, we recap HyperLogLog and present\nour modifications that yield the HyperLogLogLog\nalgorithm, prove its space usage, \nand present changes that make the algorithm practical. In\nSection~\\ref{sect:implementation} we present an overview of our\nimplementation and engineering choices. Finally, in\nSection~\\ref{sect:experiments} we report on an empirical study where\nwe show that our algorithm is practical in terms of runtime and\nprovides space-efficient sketches that yield estimates equal to those\nof HyperLogLog, and compare the running times and sketch sizes to\nother well-known implementations.\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\label{sect:preliminaries}\n\nWe write $[n] = \\{ 1, 2, \\ldots, n \\}$. We say that $w$ is the\n\\emph{word size}, meaning that it matches the number of bits output by\nour hash functions and is somehow an efficient unit of data as\nprocessed by an underlying computer architecture. When the actual\nimplementations are concerned, we are implicitly assuming $w=64$\nunless noted otherwise. All logarithms are base 2 unless otherwise\nstated, and $\\ln$ is the natural logarithm.\n\nWe denote the universe over which we want to count the number of\ndistinct elements by $\\mathcal U$. In our experimental work,\n$\\mathcal U$ is either a subset of integers or ASCII strings. We\nassume we have access to a family of random hash functions\n$h : \\mathcal U \\to [2^w]$ that map the elements of the universe to\n$w$-bit integers uniformly at random.\n\nWhen we say that $M\\in U^m$ is an \\emph{array}, we mean that $M$\nconsists of $m$ elements from the set $U$, and denote the lookup of\nthe $j$th element by $M[j]$ and the assignment of element $u$ to the\n$j$th position by $M[j] \\gets u$.\n\nWhen we say that $S\\subseteq K\\times V$ is an \\emph{associative\n  array}, we mean that $S$ is a variable-sized data structure that\nstores tuples $(k,v)\\in K\\times V$, or equivalently keys $k\\in K$ that\nmap to values $v\\in V$. We assume $S$ supports the operations of\nmembership query $k\\in S$ to test whether the key $k$ has an\nassociated value in $S$, retrieval of values by key $S[k]$, assigning\nthe value $v$ to the key $k$ by $S[k] \\gets v$, and removing a\nkey-value pair $\\text{\\texttt{del }}S[k]$. We tacitly assume that\nthere is only ever at most one value associated with any particular\nkey, and that the assignment of a pre-existing key with a new value\nreplaces the old value, so we treat $S$ as a map.\n\nThe function $\\rho : [2^w]\\to [w]$ is defined to be the 1-based index\nof the first one-bit in the bit representation of the ($w$-bit)\ninteger. That is, for the bit sequence with $k-1$ initial zeros\n$\\rho(0^{k-1}1)=k$. We leave $\\rho(0)$ undefined.\n\nWe say that random variables $X_1,X_2,\\ldots,X_m$ are \\emph{negatively\n  dependent} if, for all $i\\neq j$, they satisfy\n$E[X_iX_j] \\leq E[X_i]E[X_j]$. We say that a random event happens \\emph{with high probability} if we can choose a constant $0<\\beta<1$ such that the event happens with probability $1-\\beta$. \nWe use the\nIverson bracket notation to denote an indicator variable\n$\\llbracket \\varphi \\rrbracket$ that receives 1 if and only if the\nexpression $\\varphi$ is true and 0 otherwise. We need the following form of the Chernoff bound.\n\\begin{lemma}[{Chernoff~\\cite[Equation~1.7]{DubhashiP:2009}}]\n  \\label{lem:chernoff}\n  Let $X=\\sum_{j\\in[m]} X_j$ be a sum of negatively dependent variables with all $X_j\\in[0,1]$. Then, for all $\\epsilon>0$,\n  \\[\n    \\Pr[X>(1+\\epsilon)E[X]]\\leq\\exp\\left(-\\frac{\\epsilon^2}{3}E[X]\\right)\\, .\n  \\]\n\\end{lemma}\n\n"
            },
            "section 3": {
                "name": "The HyperLogLogLog Algorithm",
                "content": "\n\\label{sect:hyperlogloglog}\n\n",
                "subsection 3.1": {
                    "name": "Problem statement",
                    "content": "\n\nWe formally state the problem as follows. Given a sequence of elements\n$(y_1,y_2,\\ldots,y_s)$ from a universe $\\mathcal U$, determine the\nnumber of distinct elements $n=|\\{ y_i : i\\in [s] \\}|$. \nUnless otherwise stated, we will assume that $n$ is the correct (but\nunknown) cardinality of the datastream in question.\n\n"
                },
                "subsection 3.2": {
                    "name": "HyperLogLog recap",
                    "content": "\n\nSince our algorithm is a modification on HyperLogLog, it is necessary\nto understand how HyperLogLog works, so we start by restating the\nHyperLogLog algorithm~\\cite{FlajoletFGM:2007} using our notation. The\nunderlying idea of the algorithm is that, if we hash the elements\nuniformly at random, we expect to see exactly one half of the elements\nbegin with a 1, exactly one fourth begin with the bitsequence 01, one\neighth begin with the bitsequence 001, and so on; so the largest\nposition~$r$ of the first one-bit witnessed over the sequence is a\n(weak) indication that the stream has a cardinality of approximately\n$2^r$.\n\nLet us assume that we work over a universe $\\mathcal U$. The\nHyperLogLog data structure consists of an array $M\\in[w]^m$ of $m$\nelements of $\\log w$ bits where $m$ is a power of two.\\footnote{This\n  requirement is not strictly necessary, but in practice the way the\n  algorithm implemented, this is a reasonable\n  assumption.} Furthermore, we fix two\\footnote{In the original\n  exposition~\\cite{FlajoletFGM:2007}, only one 32-bit hash function\n  was used and the most significant bits were used for register\n  selection and the least significant bits as input for the function\n  $\\rho$; this can be seen as a special case of our treatment.} random\nhash functions $h : \\mathcal U \\to [2^w]$ and $f : \\mathcal U \\to [m]$\nby drawing them from the family of random hash functions. Initially we\nset all elements of $M$ to be zeros. We say that the elements of $M$\nare \\emph{registers}. We make the observation that $w=\\Omega(\\log n)$\nby the pigeon hole principle, as otherwise hash collisions necessarily\nmask some of the distinct elements.\n\nThe data structure supports three operations: \\procedurecall{update}{$y$} that\nupdates the data structure with respect to the element $y\\in\\mathcal\nU$, \\procedurecall{estimate}{} that returns the present cardinality\nestimate, and \\procedurecall{merge}{$M_1,$ $M_2$} that takes two sketches as\ninput and returns a third sketch that corresponds to the sketch that\nwould have been obtained if the elements that were used to update the\nindividual sketches had been directly applied on the output\nsketch---assuming the two input sketches have been constructed using\nthe same hash functions.\n\nUpdates are performed by selecting the index of a register by\ncomputing $j=f(y)$, and then update the value of the register to be\nthe maximum of its present value and the value $\\rho(h(y))$. That is,\nthe invariant maintained by the algorithm is that the register $M[j]$\nholds the maximum $\\rho$-value over the elements assigned to the $j$th\nsubstream by the hash function $f$. Since the hash values are $w$ bits\nlong, we have $0 \\leq \\rho(h(y)) < w$, so the values can be\nrepresented with $\\log w = \\Theta(\\log \\log n)$ bits.\n\nTo construct an estimate from the register values, we compute the\nbias-corrected, normalized harmonic mean of the estimates for the\nsubstreams\n\\[\n  E=\\alpha_m m^2 \\cdot \\left( \\sum_{j=1}^m 2^{-M[j]} \\right)^{-1} \\, ,\n\\]\nwhere the bias-correction term $\\alpha_m$ satisfies $\\alpha_{16} =\n0.673$, $\\alpha_{32} = 0.697$, $\\alpha_{64}=0.709$, and $\\alpha_m =\n0.7213/(1+1.079/m)$ for $m\\geq 128$; see~\\cite{FlajoletFGM:2007} for\nthe details. Finally, the algorithm includes further bias-correction\nby applying Linear Counting for small cardinality ranges, and a\nsimilar correction for large ranges.\n\nMerging is very simple: simply construct a new array the elements of\nwhich are the elementwise maxima of the input sketches. These\nprocedures are presented in pseudocode in\nAlgorithm~\\ref{alg:hyperloglog}. The resulting estimate has low bias\n(but is not unbiased), and it can be shown that the relative standard\nerror of the estimate is approximately\n$1.04/\\sqrt{m}$~\\cite{FlajoletFGM:2007}.\n\n\\begin{algorithm}[t]\n  \\DontPrintSemicolon\n  \\SetKwProg{Proc}{Procedure}{}{{End Procedure}}\n  \\SetFuncSty{textsc}\n  \\SetKwFunction{update}{update}\n  \\SetKwFunction{estimate}{estimate}\n  \\SetKwFunction{merge}{merge}\n  \\caption{HyperLogLog.}\n  \\label{alg:hyperloglog}\n  Let $m$ be a power of two. Initialize array $M\\in[w]^m$ to be all zeros.\n  Let $h : \\mathcal U \\to [2^w]$ and $f : \\mathcal U \\to [m]$ be fixed\n  random hash function. \\;\n  \\Proc{\\update{$M,y$}}{\n    $x \\gets h(y)$\\;\n    $j \\gets f(y)$\\;\n    $M[j] \\gets \\max \\{ M[j], \\rho(x) \\}$\\;\n  }\n  \\Proc{\\estimate{$M$}}{\n    $E \\gets \\alpha_m m^2 \\cdot \\left( \\sum_{j=1}^m 2^{-M[j]}\n    \\right)^{-1}$\\;\n    $V \\gets |\\{ j : M[j] = 0\\}|$ \\;\n    \\Return\n    $\\left\\{\n      \\begin{array}{ll}\n        m\\ln\\frac mV & \\textrm{if } E \\leq \\frac{5}{2}m\n                       \\textrm{ and } V \\neq 0 \\\\\n        -2^{32} \\ln\\left(1-\\frac{E}{2^{32}}\\right) & \\textrm{if } E >\n                                                     \\frac{2^{32}}{30} \\\\\n        E & \\textrm{otherwise}\n      \\end{array}\\right.$\\;\n  }\n  \\Proc{\\merge{$M_1,M_2$}}{\n    Initialize $M\\in[w]^m$\\;\n    \\For{$j\\in[m]$}{\n      $M[j] \\gets \\max\\{M_1[j],M_2[j]\\}$\\;\n    }\n    \\Return $M$\n  }\n\\end{algorithm}\n\n"
                },
                "subsection 3.3": {
                    "name": "Asymptotic argument",
                    "content": "\nIt is well-known\\footnote{As mentioned, for example,\n  in~\\cite{PettieWY:2021}. Lang~\\cite{Lang:2017} suggests that the\n  constant is just below 2.832 which is supported by numerical\n  experiments with Lemma~\\ref{lem:prmjeqk}.} that $O(m)$ bits suffice\nfor an entropy-compressed version of HyperLogLog. However,\nentropy-compression is expensive and does not allow for efficient\nupdates. We will show that by using $O(\\log \\log n)$ bits to store a\n\\emph{base} value $B$ and an array of offsets $M$ as a dense array,\nplus a limited number of $(j,M[j])$ pairs as a \\emph{sparse}\nassociative array, $m\\log\\log\\log m$ + $O(m)$ bits suffice for the\nsketch with high probability.\n\nSuppose we feed $n$ distinct values to the\nHyperLogLog sketch. Then each of the registers $M[j]$ can be treated as a\nrandom variable, and the distribution of the register values is\ndetermined by the following lemma.\n\\begin{lemma}[{\\cite[Appendix~A]{XiaoCZL:2020}}]\n  \\label{lem:prmjeqk}\n  After updating the HyperLogLog sketch of $m$ registers with $n$\n  distinct values, the distribution of each $M[j]$ satisfies\n  \\[\n      \\Pr[M[j] = k] = \\left\\{\n      \\begin{array}{ll}\n        \\left(1 - \\frac 1m\\right)^n & \\mathrm{if} \\  k = 0\\, , \\\\\n        \\left(1-\\frac{1}{m2^k}\\right)^n - \\left(1-\\frac{1}{m2^{k-1}}\\right)^n &\n                                                          \\mathrm{otherwise}\n                                                          \\, .\n      \\end{array}\n    \\right.\n  \\]\n\\end{lemma}\n\\begin{corollary}\n  \\label{cor:prmjleqk}\n  The distribution of $M[j]$ satisfies\n  \\[\n    \\Pr[M[j] \\leq k] = \\left(1-\\frac{1}{m2^k}\\right)^n\n    \\, .\n  \\]\n\\end{corollary}\n\nNote that Lemma~\\ref{lem:prmjeqk} only applies to any particular\nregister $j$, but not to all as a whole, since the registers are\nnegatively dependent. Throughout this section, we are going to treat\nthe register values as real-valued, continuous random variables, as\nthe map $k\\mapsto 1-\\frac{1}{m2^k}$ is monotone, only rounding to\nintegers at the very end. We also need the following lemma.\n\\begin{lemma}\n  \\label{lem:registertails}\n  Let $B=\\log\\frac{n}{m}$. Then, for $\\Delta\\in(0,B)$ and each\n  $j\\in[m]$,\n  \\begin{itemize}\n  \\item $\\Pr[M[j] < B-\\Delta] < \\exp\\left(-2^\\Delta\\right) <\n    2^{-\\Delta}$, and\n  \\item $\\Pr[M[j] > B+\\Delta]$ $< 2^{-\\Delta}$.\n  \\end{itemize}\n\\end{lemma}\n\\begin{proof}\n  It is well-known that $(1-x)^n < \\exp(-nx)$ for all $0<x<1$. \n  By Corollary~\\ref{cor:prmjleqk} and treating $k$ as a real variable,\n  \\[\n    \\begin{split}\n      \\Pr[M[j] < B-\\Delta] & = \\left(1-\\frac{1}{m2^{B-\\Delta}}\\right)^n \\\\\n      & < \\exp\\left(-\\frac{n}{m2^{B-\\Delta}}\\right) = \\exp\\left(-2^\\Delta\\right) < 2^{-\\Delta} \\, .\n    \\end{split}\n  \\]\n  Likewise, by complement from Corollary~\\ref{cor:prmjleqk} and by approximating $(1-x)^n<1-nx$ for $0<x<1$,\n  \\[\n    \\begin{split}\n      \\Pr[M[j] > B+\\Delta] & = 1 - \\left(1-\\frac{1}{m2^{B+\\Delta}}\\right)^n \\\\\n      & < \\frac{n}{m2^{B+\\Delta}} = 2^{-\\Delta}\\,.\n    \\end{split}\n  \\]\n\\end{proof}\n\n\n\n\n\nWe will start by showing that, for sufficiently large $n$ and $m$, the\nsketch can be represented with $O(m\\log\\log m+\\log\\log n)$ bits, with high probability.\n\n\\begin{theorem}\n  \\label{thm:main:loglog}\n  For $\\beta\\in(0,1)$ and $n>2m^2/\\beta$, all register values can be represeted as offsets from the base value $B=\\log\\frac nm$ using at most $\\lceil \\log\\log \\frac{2m}{\\beta}\\rceil + 1 = O(\\log\\log m)$ bits, with probability at least $1-\\beta$.\n\\end{theorem}\n\\begin{proof}\n  Fix the constant $0<\\beta<1$. We will set\n  $\\Delta=\\log\\frac{2m}{\\beta}$. By our condition on $n$, $\\Delta<B$,\n  so we apply Lemma~\\ref{lem:registertails}, and get, for each\n  $j\\in[m]$,\n  $\\Pr[M[j]\\not\\in(B-\\Delta,B+\\Delta)] < 2\\cdot 2^{-\\Delta} =\n  \\frac{\\beta}{m}$. By the union bound over the $m$ registers, we get\n  that all registers are within this interval with probability at\n  least $1-\\beta$. Finally, by our choice of $\\Delta$, we can encode\n  an integer in the desired range of $(B-\\Delta,B+\\Delta)$ by using at\n  most $\\log(2\\Delta) = \\lceil \\log\\log\\frac{2m}{\\beta}\\rceil + 1$\n  bits.\n\\end{proof}\n\n  \n\n\n\n  \n\nTheorem~\\ref{thm:main:loglog} is similar to the HLL-Tailcut approach\nof~\\cite{XiaoCZL:2020}, and could indeed be used to show that the tailcut approach requires\nasymptotically only few bits.  However, we can do better than this and\nuse only $\\log\\log\\log m+O(1)$ bits per register by showing that\n$O(m)$ bits suffice for representing the overflowing registers\nsparsely as $(j,M[j])$ pairs.\n\n\\begin{theorem}\n  \\label{thm:main:logloglog}\n  Suppose $n>4m\\log m$. Then at least $m-\\frac{m}{\\log m}$ register values can be represented as offsets from the base value $B=\\log\\frac{n}{m}$ using at most $\\lceil \\log(2+\\log\\log m)\\rceil+1 = \\log\\log\\log m+O(1)$ bits, with probability at least $1-\\exp(-m/(6\\log m))$. \n\\end{theorem}\n\\begin{proof}\n  We will set $\\Delta = 2+\\log\\log m$. By our condition on $n$, $\\Delta<B$, so we can apply Lemma~\\ref{lem:registertails}. For each $j\\in[m]$, we define an indicator variable $X_j = \\llbracket M[j]\\not\\in(B-\\Delta,B+\\Delta)\\rrbracket$. We note that $X_j\\sim\\mathrm{Bernoulli}(p)$. We get from Lemma~\\ref{lem:registertails} that $p$ satisfies \n  \\[\n    p = \\Pr[X_j = 1] < 2\\cdot 2^{-\\Delta} = \\frac{1}{2\\log m} \\, .\n  \\]\n  We then define the sum variable $X=\\sum_{j\\in[m]}X_j$ and observe that, by linearity of expectation, $E[X] < m/(2\\log m)$. Since the variables $X_j$ are negatively dependent, we can apply the Chernoff bound of Lemma~\\ref{lem:chernoff} to bound the number of registers whose values do not fall in our desired interval\n  \\[\n    \\Pr[X>\\frac{m}{\\log m}] \\leq \\exp\\left(-\\frac{m}{6\\log m}\\right) \\, .\n  \\]\n  Finally, we note that by our choice of $\\Delta$, any integer in $(B-\\Delta,B+\\Delta)$ can be encoded using $\\lceil \\log(2\\Delta)\\rceil = \\lceil\\log(2+\\log\\log m)\\rceil + 1 = \\log\\log\\log m+O(1)$ bits, and the sparse representation requires $O\\left(\\frac{m}{\\log m}\\right)\\cdot O(\\log{m}) = O(m)$ bits by the fact that the register indices are in $[m]$ and Theorem~\\ref{thm:main:loglog}.\n\\end{proof}\n\n\n\n\nAs a corollary of Theorem~\\ref{thm:main:logloglog}, we obtain an\nalgorithm that we might call HyperLogLogLogLog that uses\n$m\\log\\log\\log\\log m+O(m\\log\\log\\log m / \\log\\log m+\\log\\log n)$ bits for the\nsketch if we use a succinct dictionary for the sparse\nrepresentation. We believe this algorithm is only a theoretical\ncuriosity, but present it for completeness.\n\n\\begin{corollary}\n  \\label{cor:main:loglogloglog}\n  Suppose $n>4m\\log\\log m$. Then at least $m-\\frac{m}{\\log\\log m}$ register values can be represented as offsets from the base value $B=\\log\\frac{n}{m}$ using at most\n  \\[\n    \\lceil \\log(2+\\log\\log\\log m)\\rceil + 1 = \\log\\log\\log\\log m + O(1)\n  \\]\n  bits, with probability at least $1-\\exp(-m/(6\\log\\log m))$. Using a\n  succinct dictionary for the sparse representation, the sketch has a\n  total size of $m\\log\\log\\log\\log m+O(m\\log\\log\\log m/\\log\\log\n  m+\\log\\log n)$ bits.\n\\end{corollary}\n\\begin{proof}\nThe proof is otherwise the same as in Theorem~\\ref{thm:main:logloglog}\nexcept we use $\\Delta = 2+\\log\\log\\log m$. For the sparse\nrepresentation, we note that it is known that succinct dictionaries\n(see, for example, \\cite{Pagh:2001}) require $O(s\\log\\frac{m}{s})$\nbits where $s$ is the number of distinct elements in the dictionary\nand $m$ is the size of the universe. In particular, the same $m$ as in\nour case works since the elements are pairs in $[m]\\times [\\log m]$ by\nTheorem~\\ref{thm:main:loglog}, with high probability, so the number of\nbits required in the standard representation for any element in the\nuniverse is $O(\\log m)$. Applying the bound on the succinct dictionary\nsize, together with $s=m/\\log\\log m$, by our choice of $\\Delta$, and\nthe number of bits required for the base value,\nwe get the bound of $m\\log\\log\\log\\log m + O(m\\log\\log\\log m /\n\\log\\log m+\\log\\log n)$ bits on the total size of the sketch.\n\\end{proof}\n\nFinally, we note that storing the base value $B$ requires $O(\\log\\log\nn)$ bits which we can afford since we are going to need $O(\\log n)$\nauxiliary space for storing the computed hash values anyway. This is\nalso reflected in the more precise bound of~\\cite{KaneNW:2010}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 3.4": {
                    "name": "Practical algorithm",
                    "content": "\n\nThe practical applicability of Theorems~\\ref{thm:main:logloglog} is\nsomewhat limited as the $\\log\\log\\log m$ is an extremely slowly\ngrowing function.  Indeed, it is commonly believed that there are\nroughly $10^{80}$ atoms in the observable universe, and\n$\\log\\log\\log 10^{80} < 3.01$ which is hardly a large\nnumber. Furthermore, the proofs only address the size of the resulting\nsketch and make use of $n$ which we cannot do in practice.\n\nHowever, numerical simulations reveal that using 3 bits per register\ntends to yield small sketches for practical values of $m$ and $n$, and\nso the intuition of the sparse/dense split seems well applicable in\npractice. We will introduce a separate parameter $\\kappa$ that\ncontrols the number of bits per dense register. For our\nimplementation, we fix $\\kappa=3$ (along with $w=64$). This is quite close to the entropy bound of $2.832$~\\cite{Lang:2017}, so we cannot even hope to do much better.\n\nLet $M'\\in[w]^m$ be the registers of the uncompressed HyperLogLog sketch. \nWe denote the\nfixed-size dense array of offsets by $M\\in[\\kappa]^m$, and the base\nvalue $B\\in[w]$. The dense part corresponds to the ``fat region'' of\nthe distribution of values around the expectation, and the registers\nin the fat region satisfy\n\\begin{equation}\n  \\label{eq:denseregister}\n  B\\leq M'[j] < B+2^\\kappa \\, ,\n\\end{equation}\nand we store $M[j] = M'[j] - B$ using $\\kappa$ bits.  The sparse part\ncorresponds to registers $j$ that do \\emph{not} satisfy\nEquation~\\eqref{eq:denseregister}, and they are stored in an\nassociative array $S\\subseteq [m]\\times[w]$ whose elements are $(j,r)$\npairs where $r=M'[j]$, or equivalently, $S$ maps the sparsely\nrepresented $j$ to their corresponding register values. Each element\nof $S$ takes $\\log m + \\log w$ bits.\n\nThus, the HyperLogLogLog data structure is a 3-tuple $(S,M,B)$, and\ninitially $S=\\emptyset$, $M$ is initialized to all-zeros, and\n$B=0$. The size of the data structure is variable, and is determined\nunambiguously by the number of elements stored in $S$, or\nequivalently, the choice of the base value $B$. The total size of the\ndata structure is\n\\[\n  m\\kappa+|S|\\cdot (\\log m+\\log w) \\, .\n\\]\nIf $B$ is\nchosen so as to minimize the number of bits required, we say that\n\\emph{the sparse-dense split is optimal}.\n\nAfter each update, if any of the register values was changed, we\nmaintain as an invariant that the split is optimal by running a\nsubroutine which we call \\procedurecall{compress}{}. The subroutine determines\nthe optimal $B'$, and if this differs from the present $B$, performs a\n\\procedurecall{rebase}{} operation whereby registers are reassigned into sparse\nor dense representations, depending on which side of the split determined by $B'$ they\nfall.\n\nThe \\procedurecall{update}{} procedure is shown as pseudocode in\nAlgorithm~\\ref{alg:hyperlogloglog}. The full runtimes of each\noperation depends on the actual choice of data structures, but it is\nimmediate that the compression routine is rather expensive: we need to\ntry up to $w$ different new base values, and evaluating the size of\nthe resulting data structure requires $\\Omega(m)$ operations for each\ndifferent proposed new base value, so the compression routine requires\nat least $\\Omega(mw)$ operations. However, it is easy to see that if\nthe $n$ is sufficiently larger than $m$,\nthen only very few\nelements actually trigger an update; thus, as the next lemma\nshows, the update times are actually amortized constant over a\nsufficiently large $n$. We note that Lemma~\\ref{lem:amortizedconstant}\nis not tight, as it makes some rather crude and pessimistic\napproximations.\n\n\\begin{lemma}\n  \\label{lem:amortizedconstant}\n  For sufficiently large $n$ satisfying $n/(\\log n)^2>m^2$,\n  the updates are amortized constant,\n  with high probability.\n\\end{lemma}\n\\begin{proof}\n  Fix a constant $\\gamma \\geq 1$.\n  From Corollary~\\ref{cor:prmjleqk} and by approximating $(1-x)^n \\geq 1-nx$, we get for each $j$ that\n  \\[\n    \\Pr[M[j] > k] \\leq \\frac{n}{m2^k} \\, .\n  \\]\n  Let $k$ be the largest value that we expect to see. Setting\n  $k=\\log(\\gamma n)$, we get by the union bound that all registers are\n  below this value at probability $1-1/\\gamma$.\n\n  Let us then consider the total amount of work for the\n  compressions. The compression is triggered at most $km$ times. Each\n  time the compression routine is triggered, the amount of work\n  required is $O(km)$ since we need to try at most $k$ base values and\n  perform $O(m)$ work for each base value candidate. The total amount of compression work is thus $O(m^2k^2) = O(m^2(\\log(\\gamma n))^2)$.\n  In addition to compressions, we also need $O(1)$ work for every update. Average work for $n$ distinct elements is thus$\\frac{O(n)+O(m^2(\\log n)^2)}{n} = O(1)$\n  for sufficiently large $n$ by our assumption that $n/(\\log n)^2>m^2$, thus an\n  amortized constant.\n\\end{proof}\n\n\\begin{algorithm}[t]\n  \\DontPrintSemicolon\n  \\SetKwProg{Proc}{Procedure}{}{{End Procedure}}\n  \\SetKwFunction{get}{get}\n  \\SetKwFunction{update}{update}\n  \\SetKwFunction{compress}{compress}\n  \\SetKwFunction{rebase}{rebase}\n  \\SetKwFunction{countsparse}{countsparse}\n  \\SetFuncSty{textsc}\n  \\caption{HyperLogLogLog update procedure.}\n  \\label{alg:hyperlogloglog}\n  Let $m$ be a power of two. Initialize array $M\\in[\\kappa]^m$ to be\n  all zeros, the associative array $S\\subseteq [m]\\times[w]$ to be\n  $S\\gets \\emptyset$, and the base $B\\gets 0$.\n  Let $h : \\mathcal U \\to [2^w]$ and $f : \\mathcal U \\to [m]$ be fixed\n  random hash function. \\;\n  \\Proc{\\get{$S,M,B,j$}}{\n    \\Return $\\left\\{\\begin{array}{ll}\n                      S[j] & \\textrm{if } j\\in S \\\\\n                      B+M[j] & \\textrm{otherwise}\n                    \\end{array}\\right.$\\;\n  }\n  \\Proc{\\update{$S,M,B,y$}}{\n    $j \\gets f(y)$\\;\n    $r \\gets \\rho(h(y))$\\;\n    $r_0 \\gets$\\get{$S$,$M$,$B$,$j$}\\;\n    \\If{$r>r_0$}{\n      \\uIf{$B\\leq r < B+2^\\kappa$}{\n        $M[j] \\gets r-B$\\;\n        \\If{$j\\in S$}{\n          $\\texttt{del }S[k]$\\;\n        }\n      }\n      \\Else{\n        $S[k]\\gets r$\\;\n      }\n      \\compress{$S,M,B$}\\;\n    }\n  }\n  \\Proc{\\compress{$S,M,B$}}{\n    $B'\\gets \\displaystyle\\argmax_{B''\\in[w]} |\\{j : B'' \\leq\n    \\get{S,M,B,j} < B'' + 2^\\kappa\\}|$\\;\n    \\If{$B'\\neq B$}{\n      \\rebase{$S,M,B,B'$}\\;\n    }\n  }\n  \\Proc{\\rebase{$S,M,B,B'$}}{\n    \\For{$j\\in[m]$}{\n      $r\\gets \\get{S,M,B,j}$\\;\n      \\uIf{$B'\\leq r<B'+2^\\kappa$}{\n        $M[j]\\gets r-B'$\\;\n        \\If{$j\\in S$}{\n          $\\texttt{del }S[j]$\\;\n        }\n      }\n      \\Else{\n        $S[j]\\gets r$\\;\n      }\n    }\n    $B\\gets B'$\\;\n  }\n\\end{algorithm}\n\nThe other supported operations are the regular\nHyperLogLog operations \\procedurecall{estimate}{}, and\n\\procedurecall{merge}{}. Estimation is performed exactly as in the\ncase of HyperLogLog, with the exception that instead of array access,\nwe need to use the auxiliary \\procedurecall{get}{} function to access\nthe elements. The \\procedurecall{merge}{} operation is performed in\nthe compressed domain, so at no point is there need to perform a full\ndecompression into the regular HyperLogLog representation. The\n\\procedurecall{merge}{} operation is presented as pseudocode in \nAlgorithm~\\ref{alg:hyperlogloglogmerge}.\n\n\\begin{algorithm}[t]\n  \\DontPrintSemicolon\n  \\SetKwProg{Proc}{Procedure}{}{{End Procedure}}\n  \\SetKwFunction{get}{get}\n  \\SetKwFunction{update}{update}\n  \\SetKwFunction{compress}{compress}\n  \\SetKwFunction{rebase}{rebase}\n  \\SetKwFunction{countsparse}{countsparse}\n  \\SetKwFunction{merge}{merge}\n  \\SetFuncSty{textsc}\n  \\caption{HyperLogLogLog merge procedure.}\n  \\label{alg:hyperlogloglogmerge}\n  \\Proc{\\merge{$S_1,M_1,B_1,S_2,M_2,B_2$}}{\n    Initialize $S\\gets \\emptyset$, $M\\in[2^\\kappa]^m$ as zeros, and $B\\gets\\max\\{B_1,B_2\\}$\\;\n    \\For{$j\\in[m]$}{\n      $r_1\\gets$\\get{$S_1,M_1,B_1$}\\;\n      $r_2 \\gets$\\get{$S_2,M_2,B_2$}\\;\n      $r\\gets\\max\\{r_1,r_2\\}$\\;\n      \\uIf{$B\\leq r<B+2^\\kappa$}{\n        $M[j] \\gets r-B$\\;\n      }\n      \\Else{\n        $S[j]\\gets r$\\;\n      }\n    }\n    \\compress{$S,M,B$}\\;\n    \\Return $(S,M,B)$\\;\n  }\n\\end{algorithm}\n\n"
                },
                "subsection 3.5": {
                    "name": "Performance improvements and heuristics",
                    "content": "\n\nThere are several tricks that can be done to improve the practical\nperformance of the HyperLogLogLog algorithm. Some of these tricks have\neffects on theoretical guarantees, some don't, and others are simply a\nmatter of implementation choices.\n\nImportantly, choosing suitable data structures enables cache-friendly\nlinear access through the entire HyperLogLogLog structure, and we need\nnot actually use a potentially expensive \\procedurecall{get}{}\nfunction in the \\procedurecall{compress}{}, \\procedurecall{rebase}{},\nor \\procedurecall{merge}{} subroutines.\n\nFurthermore, the updates are determined by pairs of random variables\n$(j,r)$ where $j$ is distributed uniformly over $[m]$ and $r$ follows\na geometric distribution. This means that in most cases, \\emph{no\n  update} takes place after seeing sufficiently many elements, as the\nvast majority of the $r$ values encountered are very small. If we use\n$\\log w$ bits to store the minimum register value, we can terminate\nthe \\procedurecall{update}{} process early without any effect on\ntheoretical guarantees, without having to even look up the actual\nregister value.\n\nAlso, it is obvious that any reasonable base value $B$ should be equal\nto an actual register value, so we need not try all $w$ options; also,\ntrying candidate base values in an ascending order enables us to\nmaintain a lower bound on the number of sparse elements, which leads\nto a possible early termination of the \\procedurecall{compress}{}\nroutine without loss of theoretical guarantees. We include these\noptimizations in our implementation of HyperLogLogLog.\n\nConsidering the behavior of the random $(j,r)$ pairs, we see that, in\ngeneral, the behavior is rather benign,\\footnote{This is assuming that\n  we are \\emph{not} dealing with an adversary who has access to the\n  hash function or can affect the coin tosses when selecting the hash\n  function; in this setting, a worst-case input can cause the\n  algorithm to fail catastrophically. This is, however, a well-known\n  property inherent to HyperLogLog itself, as the algorithm is not\n  robust against adversarial input~\\cite{PatersonR:2021}.} and trying\nall possible base values in the \\procedurecall{compress}{} routine is\nmostly useless, as even though it is possible to construct nasty\ncorner cases that require unexpected rebasing operations to maintain\noptimality, these seldom occur in practice with random\ndata. Therefore, we implement the following changes to a variant of\nthe algorithm we call HyperLogLogLog$^*$: we only call\n\\procedurecall{compress}{} if the size of the data structure needs to\nbe increased (that is, a new element needs to be added into $S$), and\nwe only try the next possible register value that is larger than $B$,\nand omit all other choices. Experiments show that these heuristics\nhave little effect on the actual size of the resulting data structure,\nbut they improve the actual runtimes.\n\nWe also provide as a baseline a variant which we call HyperLogLogLogB\nthat fixes $B$ to be the minimum register value, and maintains a\ncounter that records the number of minimum-valued\nregisters. Functionally, this variant behaves like\nHLL-Tailcut~\\cite{XiaoCZL:2020}, except instead of allowing for error\nwhen the register values overflow, it stores them sparsely. While\nthis is the fastest variant, as the compression becomes nearly\ntrivial, experiments show that the sketches are noticeably larger.\n\n"
                }
            },
            "section 4": {
                "name": "Implementation",
                "content": "\n\\label{sect:implementation}\n\n",
                "subsection 4.1": {
                    "name": "Overview",
                    "content": "\n\nWe provide a C++ implementation of the HyperLogLogLog,\nHyper\\-Log\\-Log\\-Log$^*$, and HyperLogLogLogB algorithms, along with a\ncomparable implementation of the vanilla HyperLogLog, and an\nentropy-compressed version of the HyperLogLog, compressed using the\nZstandard library.\\footnote{\\url{https://facebook.github.io/zstd/}} We\nalso provide our full experimental pipeline that can be built as a\nDocker container for reproducibility. The code is available\nonline\\footnote{\\url{https://github.com/mkarppa/hyperlogloglog}}\nunder the MIT license. \nFunctionally, our\nimplementation is a drop-in replacement for HyperLogLog and enables\nconversion between the compressed and uncompressed representations,\nwhilst producing the exact same estimates.\n\nThe guiding principles for constructing the implementation have been\nto minimize the memory usage and enable cache-friendly linear access\nthrough the data structure, at the cost of potentially losing\noptimization opportunities that would require auxiliary space. Our\nimplementation assumes a 64-bit environment, which is reflected in\nthe design choices for the data structures and hash functions.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Data structures",
                    "content": "\n\nWe implement both the array $M$ and the associative array $S$ as\nbit-packed arrays of 64-bit words. An array of $n$ elements of size\n$s$ bits will occupy $\\lceil\\frac{ns}{64}\\rceil\\cdot 64$ bits of\nmemory; importantly, we allow elements to cross word boundaries. For\nthe HyperLogLog implementation, the array $M$ consists of 6-bit\nelements, so the array size is $\\lceil \\frac{6m}{64} \\rceil \\cdot 64$\nbits, and for HyperLogLogLog, we use 3 bits per element, so the array\nsize is $\\lceil \\frac{3m}{64} \\rceil \\cdot 64$ bits.\n\nWe implement the associative array by concatenating the key-value\npairs into integers where the more significant bits are occupied by\nthe key and the less significant bits by the value. We maintain the\nbit-packed array sorted in ascending order by applying insertion sort\nafter each insertion. We perform lookups by binary search in\n$O(\\log|S|)$ time. For the HyperLogLogLog associative array $S$, the\nelements are of size $\\log m + 6$, since we use 64-bit hash\nfunctions. Maintaining the associative array $S$ sorted enables linear iteration over the entire HyperLogLogLog data structure.\n\n"
                },
                "subsection 4.3": {
                    "name": "Hash functions",
                    "content": "\n\nOur implementation accepts unsigned 64-bit integers and 8-bit byte\nstrings as input. We also accept raw $(j,r)\\in[m]\\times[w]$ integer\npairs as input for evaluating runtimes without hash function\nevaluations.\n\nFor the hash function $h:\\mathcal U \\to [2^w]$, we use Google's\nFarmhash.\\footnote{https://github.com/google/farmhash} Specifically\nwe use the function \\texttt{Fingerprint} for 64-bit integers, and\n\\texttt{Hash64} for strings.\n\nWe derive the hash function $f : \\mathcal U \\to [m]$ from $h$ by\napplying Fibonacci Hashing~\\cite[Section~6.4]{Knuth:1998} on the hash\nvalue $h(y)$, that is, we multiply the hash value by\n$9e3779b97f4a7c15_{16}$ modulo $2^{64}$ and take the $\\log m$ most\nsignificant bits of the result.\n\n"
                },
                "subsection 4.4": {
                    "name": "Entropy compression",
                    "content": "\n\nWe have implemented an entropy-compressed version of HyperLogLog as a\nbaseline. The implementation behaves like vanilla HyperLogLog, but\nthe array is compressed with Zstandard library after each update. The\nnumber of decompressions is limited by maintaining the information\nabout the minimum value in a separate variable, much like in the case\nof our HyperLogLogLog implementation, and decompression is only\ntriggered if the encountered $\\rho$-value is at least as large as the\nminimum value in the registers. The idea of using entropy compression\nis an obvious alternative, and we use Zstandard with compression level\nset to 1, as suggested in~\\cite{Lang:2017}.\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\\label{sect:experiments}\n\n\n",
                "subsection 5.1": {
                    "name": "Experimental setup",
                    "content": "\n\nWe evaluate a number of implementations by varying the number of\nregisters~$m$ as powers of two from $2^4$ to $2^{18}$, and generate random\ninput of~$n$ elements for $n=2^{\\frac{i}{2}}$ for $i=\\{4,5,\\ldots,60\\}$,\nthat is, $n$ is the power of a square root of 2 from $2^4$ to\n$2^{30}$, rounded to the nearest integer.\nThe data consists of random unsigned 64-bit integers,\n8-character-long alphanumeric random ASCII strings, and $(j,r)$-pairs\nwhere $j$ has been drawn from $[m]$ uniformly at random and\n$r\\sim\\mathrm{Geom}(0.5)$. The $(j,r)$ input is only supported by our\nown implementations. We also evaluate our hash functions separately,\nwithout performing a full update sequence.\n\nWe compare our implementations against Apache\nDataSketches\\footnote{\\url{https://datasketches.apache.org/}}\nimplementation of HyperLogLog, Apache DataSketches implementation of\nCompressed Probability Counting (CPC)~\\cite{Lang:2017}, and Google's\nZetaSketch\\footnote{\\url{https://github.com/google/zetasketch}}\nimplementation of HyperLogLog++~\\cite{HeuleNH:2013}. The\nimplementations are listed in Table~\\ref{tbl:implementations}.\n\nAll experiments\nare run in a Docker container that is managed by a Python\nscript. Data is generated by a separate C++ program, and the data is\nthen passed to a wrapper program that stores the data in RAM, so as to\navoid I/O issues.\n\nThere are two types of experiments: update experiments and merge\nexperiments. In the update case, the wrapper constructs a sketch by\nfeeding the data one element at a time. Once the sketch has been\nconstructed, the wrapper reports the time it took to construct the\nsketch, the cardinality estimate computed from the sketch, and bit\nsize of the sketch.\nFor the merge experiment, we first divide the\ndata into two\nequal-sized chunks and construct one sketch for each chunk.\nWe then report the time it takes to\nconstruct the merged sketch from the two sketches. In all cases, 10\nindependent repetitions were performed with different data, but the\nsame data supplied for each implementation at a given $m$, $n$, and\nrepetition.\n\nThe reported bit sizes for our implementations are $6m$ for\n\\texttt{HLL}, 8 times the number of bytes occupied by the Zstd\ncompressed register array for \\texttt{HLLZ}, and\n$|S|\\cdot(\\log m+6)+3m$ for \\texttt{HLLL}, \\texttt{HLLL*}, and\n\\texttt{HLLLB}. For \\texttt{Zeta}, following the documentation of the\nlibrary, we report $8m$. For Apache sketches, we use the bit size of the\noutput of the serialization function to upper bound the size of the sketch.\n\nThe experiments were run on a computer running two Intel Xeon\nE5-2690v4 CPUs at 2.60~GHz with a total of 28 cores, and a total of\n512~GiB of RAM, running Ubuntu 18.04.5~LTS. The experiments were run\nusing CPython~3.8.10 and Docker~20.10.7. The Docker environment ran\nUbuntu 20.04.3~LTS, and the C++ code was compiled with GCC\n10.3.0. ZetaSketch was compiled with Gradle 7.2 and\nOpenJDK~11.0.13. We ran up to 14 experiments in parallel.\n\n"
                },
                "subsection 5.2": {
                    "name": "Results",
                    "content": "\n\n\nThe scaling of update times is reported in\nFigure~\\ref{fig:scalingatm32768} at fixed $m=2^{15}$ registers. We\nreport the average time per distinct element for constructing the sketch. This\nshows the general behavior of the algorithm: \\texttt{HLL}\nis clearly the fastest, but \\texttt{HLLL*} catches up as the\nnumber of distinct elements grows. For another view,\nFigure~\\ref{fig:totaltime1073741824} records the total time for\nconstructing a sketch with $n=2^{30}$ distinct elements, as a\nfunction of $m$. \\texttt{HLLL} and \\texttt{HLLL*} remain\ncompetitive with vanilla HyperLogLog until $m=2^{12}$ and $m=2^{15}$,\nrespectively, and then the total time starts to grow.\n\n\n\nFigure~\\ref{fig:loadfactor} shows yet another view into this behavior\nin terms of \\emph{load factor} $n/m$. The figure shows the update time\nper distinct element with \\texttt{HLLL} as a function of the load\nfactor. Input data consists of $(j,r)$ pairs, so no hashing is\nperformed. Furthermore, the datapoints indicate the number of\nregisters used. We see that when the number of distinct elements is\nsufficiently large with respect to the number of registers,\nthe algorithm achieves amortized constant behavior. Indeed, this is to be expected, as once the registers are filled\nby distinct values, very few updates take place, as per\nLemma~\\ref{lem:amortizedconstant}. For \\texttt{HLLL},\nin this particular case,\nthis regime is reached when the load factor is approximately\n$2^{17}$; this is only achieved with $m\\leq 2^{13}$ due to fixed\n$n$. \\texttt{HLLL*} reaches the same effect at a lower load factor,\nbut the results are more noisy, owing to the heuristics, but we have\nomitted a separate figure for lack of space.\n\nAlthough not shown here for lack of space, in particular when the\nstream consists of strings, hashing can make a large portion of the\ntime. In fact, the vanilla HyperLogLog is so efficient in its updates,\nthat almost 100\\% of time is spent on hashing when performing the\nupdates with strings as input. This suggests that slower updates are\nnot necessarily a problem, since there are other bottlenecks that may\nbe unavoidable. In particular, I/O operations can be orders of\nmagnitude slower, which we have deliberately avoided in our\nexperiments by performing everything in RAM.\n\n\n\nFigure~\\ref{fig:merge} shows the time required for merging of two\nsketches that have been constructed using $n=2^{30}$ distinct\nelements, split equally in half among the sketches, as a function of\nthe number of registers $m$. We see that,\nfor \\texttt{HLLL},\nmerging becomes more\ncostly the larger the sketch size is, largely due to the application\nof the full compression routine after the merge. The same routine is\napplied also for \\texttt{HLLL*}, so there is no discernible difference\nin runtime from the \\texttt{HLLL}. Despite being slower, we are still\ntalking about less than one third of a second for merging two\n$m=2^{18}$ sketches. Perhaps surprisingly, \\texttt{HLLLB} performs\npoorly. This can be explained by suboptimal choice of the base value\nthat yields longer rebasing times, since the sparse representation is\noverused.\n\n\n\nFigure~\\ref{fig:relativesketchsize} shows the relative sketch size as\nbits / $m$ after $n=2^{30}$ distinct element updates. In particular,\nthis illustrates the efficiency of the heuristics we use for\n\\texttt{HLLL*} since there is no discernible difference in the sketch\nsize between \\texttt{HLLL} and \\texttt{HLLL*} when $m$ is not\nminuscule. This figure also shows that \\texttt{HLLLB} produces clearly\nlarger sketches than \\texttt{HLLL*}. Although the sketch sizes\ngiven for the Apache DataSketches algorithms are upper bounds, the\nbounds appear to be quite tight as $m$ grows. In concrete numbers, for\n$m=2^{18}$, \\texttt{HLLL} reduces the bit size of the sketch by\n37.3--37.7\\%. On average over all choices of $m$, at $n=2^{30}$, we see a reduction of over 41\\%.\n\n\n\n\n\nFinally, to quantify the tradeoffs among the different implementations, \nFigure~\\ref{fig:pareto} plots the sketch size vs. update time\nat $n=2^{30}$ and $m=2^{15}$ with unsigned\n64-bit integers as input. Algorithms to the left and towards the\nbottom would be preferred; this shows that \\texttt{HLLL*} performs\nquite well in this parameter regime. In fact, this is a sweet spot for\n\\texttt{HLLL*} where the sketch size is very minuscule while updates are\nstill within the amortized constant behavior zone. Entropy compression\nis required for achieving smaller sketch size, at the expense of\nconsiderably higher update times.\n\n\n\nFigure~\\ref{fig:pareto2} provides a multi-datapoint view into the\ntradeoff by fixing $n=2^{30}$ and plotting the relative sketch size\nvs. total update size at $m=2^{10},\\ldots,2^{18}$. In general, if a\ndatapoint (for fixed $m$, although\nthis cannot be seen in the figure) lies to the left and to the bottom\nof another datapoint, it should always be preferred. This shows that\nthere is indeed a regime where \\texttt{HLLL*} is preferred when $m$ is\nsufficiently small in relation to $n$, except when sketch minimality\nis desired, which would favor entropy compression. \\texttt{HLLL*} is at a\ndisadvantage when constant time updates are required at\nvery high accuracy, but at a relatively low $n$.\n\n\n\n\n\\begin{acks}\n  We thank Martin Aumller for constructive discussions.\n  This work was supported by VILLUM\n  Foundation grant 16582.\n\\end{acks}\n\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{refs.bib}\n\n\\appendix\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n"
                }
            }
        },
        "equations": {
            "eq:1": "\\begin{equation}\n  \\label{eq:denseregister}\n  B\\leq M'[j] < B+2^\\kappa \\, ,\n\\end{equation}"
        },
        "git_link": "https://github.com/mkarppa/hyperlogloglog"
    }
}