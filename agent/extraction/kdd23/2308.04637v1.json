{
    "meta_info": {
        "title": "Sparse Binary Transformers for Multivariate Time Series Modeling",
        "abstract": "Compressed Neural Networks have the potential to enable deep learning across\nnew applications and smaller computational environments. However, understanding\nthe range of learning tasks in which such models can succeed is not well\nstudied. In this work, we apply sparse and binary-weighted Transformers to\nmultivariate time series problems, showing that the lightweight models achieve\naccuracy comparable to that of dense floating-point Transformers of the same\nstructure. Our model achieves favorable results across three time series\nlearning tasks: classification, anomaly detection, and single-step forecasting.\nAdditionally, to reduce the computational complexity of the attention\nmechanism, we apply two modifications, which show little to no decline in model\nperformance: 1) in the classification task, we apply a fixed mask to the query,\nkey, and value activations, and 2) for forecasting and anomaly detection, which\nrely on predicting outputs at a single point in time, we propose an attention\nmask to allow computation only at the current time step. Together, each\ncompression technique and attention modification substantially reduces the\nnumber of non-zero operations necessary in the Transformer. We measure the\ncomputational savings of our approach over a range of metrics including\nparameter count, bit size, and floating point operation (FLOPs) count, showing\nup to a 53x reduction in storage size and up to 10.5x reduction in FLOPs.",
        "author": "Matt Gorbett, Hossein Shirazi, Indrakshi Ray",
        "link": "http://arxiv.org/abs/2308.04637v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "Published at KDD '23"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nThe success of deep learning can largely be attributed to the availability of massive computational resources \\cite{krizhevsky_imagenet_2012, simonyan_very_2014,he_deep_2016}.\nModels such as the Transformer \\cite{vaswani_attention_2017} have changed machine learning in fundamental ways, producing state-of-the-art results across fields such as \\gls{nlp}, computer vision \\cite{chen_pre-trained_2021,touvron_training_2021}, and time series learning \\cite{zerveas_transformer}.\nMuch effort has been aimed at scaling these models towards \\gls{nlp} efforts on large datasets \\cite{devlin_bert_2019,brown_language_2020}, however, such models cannot practically be deployed in resource-constrained machines due to their high memory requirements and power consumption. \n\n%this has come at the cost of a higher carbon footprint and a lack of democratization of such systems.  \n\n%Transformers have played a large role in the recent progress of deep learning, producing state-of-the-art results across fields such as \\gls{nlp}, computer vision, and time series learning.  \n%While much effort has been aimed at scaling these models towards generalized NLP efforts on large datasets, limited research has examined the effectiveness of Transformer-based architectures in resource constrained applications. \n\n\n\nParallel to the developments of the Transformer, the Lottery Ticket Hypothesis \\cite{frankle_lottery_2019} demonstrated that neural networks contain sparse subnetworks that achieve comparable accuracy to that of dense models.\nPruned deep learning models can substantially decrease computational cost, and enable a lower carbon footprint and the democratization of AI.  %with less memory and power consumption, and hence a lower carbon footprint.  Additionally, it enables the democratization of AI\nSubsequent work showed that we can find highly accurate subnetworks within randomly-initialized models without training them \\cite{ramanujan_whats_2020}, including binary-weighted neural networks \\cite{diffenderfer_multi-prize_2021}.\nSuch ``lottery-ticket'' style algorithms have mostly experimented with image classification using convolutional architectures, however, some work has shown success in pruning \\gls{nlp} Transformer models such as BERT \\cite{chen2020lottery,ganesh2021compressing,jiao-etal-2020-tinybert}. \n\nIn this work, we extend the Lottery Ticket Hypothesis to time series Transformers, showing that we can prune and binarize the weights of the model and still maintain an accuracy similar to that of a Dense Transformer of the same structure. \nTo achieve this, we employ the Biprop algorithm \\cite{diffenderfer_multi-prize_2021}, a state-of-the-art technique with proven success on complex datasets such as ImageNet \\cite{5206848}.\nThe combination of weight binarization and pruning is unique from previous efforts in Transformer compression.\nMoreover, each compression technique offers separate computational advantages: neural network pruning decreases the number of non-zero \\gls{flops}, while binarization reduces the storage size of the model.\nThe Biprop algorithm's two compression methods rely on each other during the training process to identify a high-performing subnetwork within a randomly weighted neural network.\nThe combination of pruning and weight binarization is depicted in Figure \\ref{teaser}a.\n\nWe apply our approach to multivariate time series modeling.\nResearch has shown that Transformers achieve strong results on time series tasks such as classification \\cite{zerveas_transformer}, anomaly detection \\cite{xu_anomaly_2022,tuli_tranad_2022}, and forecasting \\cite{liu2021pyraformer, zhou2021informer}.\nTime series data is evident in systems such as IoT devices \\cite{cook2019anomaly}, engines \\cite{malhotra2016lstm}, and spacecraft \\cite{su_robust_2019,baireddy2021spacecraft}, where new insights can be gleaned from the large amounts of unmonitored information.  \nMoreover, such systems often suffer from resource constraints, making regular deep learning models unrealistic -- for instance, in the Mars rover missions where battery-powered devices are searching for life \\cite{bhardwaj2021semi}.\nOther systems such as satellites contain thousands of telemetry channels that require granular monitoring.\nDeploying large deep learning models in each channel can be extremely inefficient.\nAs a result, lightweight Transformer models have the potential to enhance a wide variety of applications.  \n%such as Internet of Things (IoT) devices,  spacecraft \\cite{baireddy2021spacecraft,meng2019spacecraft,hundman2018detecting}, and smart phones.  \n\n\nIn addition to pruning and binarizing the Transformer architecture, we simplify the complexity of the attention mechanism by applying two modifications.\nFor anomaly detection and forecasting, which we model using overlapping sliding window inputs, we apply an attention mask to only consider attention at the current time step instead of considering attention for multiple previous time steps.\nFor classification tasks, we apply a static mask to the query, key, and value projections, showing that only a subset of activations is needed in the attention module to achieve the same accuracy as that obtained using all the activations.  \n\n\nFinally, we estimate the computational savings of the model in terms of parameters, storage cost, and non-zero \\gls{flops}, showing that pruned and binarized models achieve comparable accuracy to dense models with substantially lower computational costs.  \n\\newline\nOur contributions are as follows:\n\\begin{itemize}\n  \\item We show that sparse and binary-weighted Transformers achieve comparable accuracy to Dense Transformers on three time series learning tasks (classification, anomaly detection, forecasting).\n  To the best of our knowledge, this is the first research examining the efficacy of compressed neural networks on time series related learning.  \n  \\item We examine pruning and binarization jointly in Transformer-based models, showing the benefits of each approach across multiple computational metrics.\n  Weight binarization of Transformer based architectures has not been studied previously.   \n  %\\item We propose a general purpose framework for several time series learning tasks capable of pruning and binarizing the models weights.   With the \n\n\\end{itemize}\n\n%\\textbf{Contributions:} We show that sparse and binary weighted Transformers achieve comparable accuracy to that of Dense Transformers on three time series learning tasks (classification, anomaly detection, forecasting), highlighting the computational savings achieved by the model over a range of metrics.  \nThese findings provide new potential applications for the Transformer architecture, such as in resource-constrained environments that can benefit from time series related intelligence.\n%\\textbf{2)} We can substantially reduce the number of non-zero computations in Transformer models and still maintain high-accuracy in time series tasks.  We estimate the computational savings in terms of storage cost and FLOPs, showing that lightweight models achieve the power of the Dense Transformer at a fraction of the cost.  \n\n\n%time series, why time series are important in smaller compputee environments. \n\n\n%attention, complexity\n\n\n%contributions\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\\label{related}\nIn this section, we describe existing research related to Transformers in time series modeling, neural network pruning and compression, and finally efficient Transformer techniques. \n\n",
                "subsection 2.1": {
                    "name": "Transformers in Time Series",
                    "content": "\n%\\noindent\\textbf{Transformers in Time Series} $\\sbullet[.75]$\nVarious works have applied Transformers to time series learning tasks \\cite{wen_transformers_2022}.\nThe main advantage of the Transformer architecture is the attention mechanism, which learns the pairwise similarity of input patterns.\nMoreover, it can efficiently model long-range dependencies compared to other deep learning frameworks such as LSTM's \\cite{liu2021pyraformer}.\nZerveas et al. \\cite{zerveas_transformer} showed that we can use unsupervised pretrained Transformers for downstream time series learning tasks such as regression and classification.\nAdditional work in time series classification has proposed using a ``two tower\" attention approach with channel-wise and time-step-wise attention \\cite{liu2021gated}, while other work has highlighted the benefits of Transformers for satellite time series classification compared to both recurrent and convolutional neural networks  \\cite{russwurm2020self}.  \n\nFor anomaly detection tasks, Transformers have shown favorable results compared to traditional ML and deep learning techniques.\nNotably, Meng et al. \\cite{meng2019spacecraft} applied the model to NASA telemetry datasets and achieved strong accuracy (0.78 F1) in detecting anomalies.\nTranAD \\cite{tuli_tranad_2022} proposed an adversarial training procedure to exaggerate reconstruction errors in anomalies.  Xu et al. \\cite{xu_anomaly_2022} achieve state-of-the-art results in detecting anomalies in multivariate time series via association discrepancy.  Their key finding is that anomalies have high association with adjacent time points and low associations with the whole series, accentuating anomalies. %, who have high associations with the whole series of data.   \n\nFinally, Transformer variations have been proposed for time series forecasting to lower the attention complexity of long sequence time series \\cite{li2019enhancing,zhou2021informer,liu2021pyraformer,pmlr-v162-zhou22g}, add stochasticity \\cite{wu2020adversarial}, and incorporate traditional time series learning methods \\cite{pmlr-v162-zhou22g,wu2021autoformer}.  Li et al. \\cite{li2019enhancing} introduce LogSparse attention, which allows each cell to attend only to itself and its previous cells with an exponential step size.  The Informer method \\cite{zhou2021informer} selects dominant queries to use in the attention module based on a sparsity measurement.  Pyraformer \\cite{liu2021pyraformer} introduces a pyramidal attention mechanism for long-range time series, allowing for linear time and memory complexity.  Wu et al. \\cite{wu2020adversarial} use a Sparse Transformer as a generator in an encoder-decoder architecture for time series forecasting, using a discriminator to improve the prediction.  \n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Compressed Neural Networks",
                    "content": "\n%\\noindent\\textbf{Compressed Neural Networks} $\\sbullet[.75]$\nPruning unimportant weights from neural networks was first shown to be effective by Lecun et al. \\cite{lecun_optimal_1989}.  In recent years, deep learning has scaled the size and computational cost of neural networks. Naturally, research has been directed at decreasing size \\cite{han_learning_2015} and energy consumption \\cite{yang_designing_2017} of deep learning models. \n\nThe Lottery Ticket Hypothesis  \\cite{frankle_lottery_2019} showed that randomly initialized neural networks contain sparse subnetworks that, when trained in isolation, achieve comparable accuracy to a trained dense network of the same structure.\nThe implications of this finding are that over-parameterized neural networks are no longer necessary, and we can prune large models and still maintain the original accuracy.\n\nSubsequent work found that we do not need to train neural networks at all to find accurate sparse subnetworks; instead, we can find a high performance subnetwork using the randomly initialized weights \\cite{ramanujan_whats_2020,malach_proving_2020,chijiwa_pruning_2021,gorbett2023randomly}.\nEdge-Popup \\cite{ramanujan_whats_2020} applied a scoring parameter to learn the importance of each weight, using the straight-through estimator \\cite{bengio_estimating_2013} to find a high accuracy mask over randomly initialized models.  Diffenderfer and Kailkhuram \\cite{diffenderfer_multi-prize_2021} introduced the \\textit{Multi-Prize} Lottery Ticket Hypothesis, showing that 1) multiple accurate subnetworks exist within randomly initialized neural networks, and 2) these subnetworks are robust to quantization, such as binarization of weights.\nIn this work, we use the Biprop algorithm proposed in \\cite{diffenderfer_multi-prize_2021} to binarize the weights of Transformer models.  \n\n"
                },
                "subsection 2.3": {
                    "name": "Compressed and Efficient Transformers",
                    "content": "\n%\\noindent\\textbf{Compressed and Efficient Transformers} $\\sbullet[.75]$\nLarge-scale Transformers such as the BERT (110 million parameters) are a natural candidate for pruning and model compression \\cite{efficient_transformers,ganesh2021compressing}.\nChen et al. \\cite{chen_pre-trained_2021} first showed that the Lottery Ticket Hypothesis holds for BERT Networks, finding accurate subnetworks between 40\\% and 90\\% sparsity.  \nJaszczur et al. \\cite{jaszczur2021sparse} proposed scaling Transformers by using sparse variants for all layers in the Transformer.\nOther works have reported similar findings \\cite{lepikhin2020gshard,fedus2021switch}, showing that sparsity can help scale Transformer models to even larger levels.  \n \nOther works have proposed modifications for more efficient Transformers aside from pruning \\cite{efficient_transformers}.  Most research has focused on improving the $\\mathcal{O}(n^2)$ complexity of attention, via methods such as fixed patterns \\cite{qiu2019blockwise}, learnable patterns \\cite{kitaev2020reformer}, low rank/kernel methods \\cite{wang2020linformer,choromanski2020rethinking}, and downsampling \\cite{zhang2021poolingformer,beltagy2020longformer}.  Various other methods have been proposed for compressing BERT networks such as pruning via post-training mask searches \\cite{kwon2022fast}, block pruning \\cite{lagunas2021block}, and 8-bit quantization \\cite{zafrir2019q8bert}.  We refer readers to Tay et al. \\cite{efficient_transformers} for details. \n\n\nDespite the various works compressing Transformers, we were not able to find any research using both pruning and binarization.   Utilizing both methods allows for more efficient computation (measured using FLOPs) as well as a significant decrease in storage (due to binary weights).  Additionally, we find that our proposed model is still a fraction of the size of compressed NLP Transformers models when trained on time series tasks.  For instance, TinyBERT \\cite{jiao-etal-2020-tinybert}  contains 14.5 million parameters and 1.2 billion FLOPs, compared to our models  which contain less than 1.5 million \\textbf{binary} parameters and 38 million FLOPs. \n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Method",
                "content": "\\label{method}\n%Our base model consists of a transformer encoder \\cite{vaswani_attention_2017}. \n\n\n\nOur model consists of a Transformer encoder \\cite{vaswani_attention_2017} with several modifications.  \nWe base our model off of Zerveas et al. \\cite{zerveas_transformer}, who propose using a common Transformer framework for several time series modeling tasks.  \nTo begin, we describe the base architecture of the Transformer as applied to multivariate time series.  Subsequently, we describe the techniques used for pruning and binarization.  Finally, we describe the two changes applied to the attention mechanism.  \n\n\n",
                "subsection 3.1": {
                    "name": "Dense Transformer",
                    "content": " \n\nWe denote fully trained Transformers with no pruning and \\gls{fp32} weights as Dense Transformers.  %In this section we briefly describe the architecture as well as techniques common to our multivariate time series framework.  \nLet $\\mathbf{X_t} \\in\\mathbb{R}^{w\\times m}$ be a model input for time $t$ with window size $w$ and $m$ features.  Each input contains $w$ feature vectors  $\\mathbf{x} \\in\\mathbb{R}^{m}:\\mathbf{X_t} \\in\\mathbb{R}^{w\\times m}=[\\mathbf{x_{t-w}},\\mathbf{x_{t-w+1}},...,\\mathbf{x_{t}} ]$, ordered in time sequence of size $w$.  In classification datasets $w$ is predefined at the sample or dataset level. For anomaly detection and forecasting tasks, we fix $w$ to 50 or 200 and use an overlapping sliding window as inputs.  \n %Fixing $w$ allows us to control model parameterization, rather than scaling $w$ to the dataset size using padding.  \n \n The standard architecture (pre-binarization) projects $m$ features  onto a $d$-dimensional vector space using a linear module with learnable weights $\\mathbf{W_p}\\in\\mathbb{R}^{d \\times m}$\nand bias $\\mathbf{b_p}\\in\\mathbb{R}^{d}$.  \nWe use the standard positional encoder proposed by Vaswani et al.  \\cite{vaswani_attention_2017}, and we refer readers to the original work for details.  For the Dense Transformer classification models, we use learnable positional encoder  \\cite{zerveas_transformer}.  Zerveas et al. \\cite{zerveas_transformer} propose using batch normalization instead of layer normalization used in traditional Transformer NLP models.  They argue that batch normalization mitigates the effects of outliers in time series data.  We found that for classification tasks, batch normalization performed the best, while in forecasting tasks layer normalization worked better.  For anomaly detection tasks we found that neither normalization technique was needed.  %We provide results with various configurations in the Appendix.  \n\nEach Transformer encoder layer consists of a multi-head attention module followed by ReLU layers.   \nThe self-attention module takes input $\\mathbf{Z_t}\\in\\mathbb{R}^{w \\times d}$ and projects it onto a Query ($\\mathbf{Q}$), Key ($\\mathbf{K}$), and Value ($\\mathbf{V}$), each with learnable weights $\\mathbf{W}\\in\\mathbb{R}^{d \\times d}$\nand bias $\\mathbf{b}\\in\\mathbb{R}^{d}$.  Attention is defined as\n%which performs the scaled dot-product as \n%\\begin{equation}\n$\n    Attention(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) =\nSoftmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\intercal}{\\sqrt{d}} \\right) \\mathbf{V}\n$.  Queries, keys, and values are projected by the number of heads ($h$) to create multi-head attention. The resultant output $\\mathbf{Z_t}'$ undergoes a nonlinearity before being passed to the next encoder layer.  The Transformer consists of $N$ encoder layers followed by a final decoder layer.  For classification tasks, the decoder outputs $l$ classification labels: $\\mathbf{X'_t}\\in\\mathbb{R}^{w \\times l}$, which are averaged over $w$. For anomaly detection and forecasting, the decoder reconstructs the full input: $\\mathbf{X'_t}\\in\\mathbb{R}^{w \\times m}$.  \n%projected thr projection.  Each encoder layer passes the output through a non-linear layer with learnable weights $\\mathbf{W_{e_1}}\\in\\mathbb{R}^{r \\times d}$\n%and bias $\\mathbf{b_{e_1}}\\in\\mathbb{R}^{r}$ and a final layer with learnable weights $\\mathbf{W_{e_2}}\\in\\mathbb{R}^{d \\times r}$\n%and bias $\\mathbf{b_{e_2}}\\in\\mathbb{R}^{d}$.  \n%The Transformer consists of $N$ encoder layers followed by final decoder layer. \n\n%\\vspace{-.1cm}\n"
                },
                "subsection 3.2": {
                    "name": "Sparse Binary Transformer",
                    "content": "\n%\\vspace{-.1cm}\n\nCentral to our binarization architecture is the Biprop algorithm \\cite{diffenderfer_multi-prize_2021}, which uses randomly initialized floating point weights to find a binary mask over each layer.  \nGiven a neural network with weight matrix $\\mathbf{W}\\in\\mathbb{R}^{i \\times j}$ initialized with a standard method such as Kaiming Normal  \\cite{he2015delving}, \nwe can express a subnetwork over neural network $f(x;\\mathbf{W})$ as $f(x;\\mathbf{W}   \\odot \\mathbf{M})$, where $\\mathbf{M}\\in\\{0,1\\}$ is a binary mask and $\\odot$ is an elementwise multiplication. \n\n%For \\gls{bnns},  $\\mathbf{W}\\in\\{-1,1\\}^{i \\times j}$.  \n%The resulting network function becomes $f(x;\\mathcal{W} \\odot \\mathbf{M})$, with mask \\mathbf{M}  over binary weights.  \n\nTo find $\\mathbf{M}$, parameter $\\mathbf{S}\\in\\mathbb{R}^{i \\times j}$  is initialized for each corresponding $\\mathbf{W}\\in\\mathbb{R}^{i \\times j}$.  $\\mathbf{S}$ acts as a score assigned to each weight dictating the importance of the weights contribution to a successful subnetwork.  Using backpropagation as well as the straight-through estimator \\cite{bengio_estimating_2013}, the algorithm takes pruning rate hyperparameter $p \\in [0,1]$, and on the forward pass computes $\\mathbf{M}_k$ at layer $m$ as\n\n%\\vspace*{-\\abovedisplayskip}\n%\\vspace{-2pt}\n\\begin{equation}\n    \\mathbf{M}_k=   \\begin{cases}\n    1 & \\text{if $|\\mathrm{S}_k| \\in \\{\\tau(k)_{k=1}^{l_m}\\ge[l_m p]\\}$}\\\\%, {|\\mathrm{S}^j_{\\tau(i)}|} \\le |\\mathrm{S}^j_{\\tau(i+1)}|$ } \\\\\n    0 & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\n\nwhere $\\tau$ sorts indices $\\{k\\}^l_{k=1}\\in\\mathrm{S}$ such that $|S_{\\tau(k)}|\\le|S_{\\tau(k+1)}|$.\n%where $\\mathrm{S}^{top_k}$denotes top 100(1-p)\\% of $|\\mathrm{S}|$.  \nMasks are computed by taking the absolute value of scores for each layer, and setting the mask to 1 if the value falls above the top $p^{th}$ percentile.   \n%Biprop uses a customized linear module along with the straight-through estimator \\cite{bengio_estimating_2013} to \n\nTo convert each layer to binary weights Biprop introduces gain term $\\alpha\\in\\mathbb{R}$, which is common to \\gls{bnns} \\cite{qin2020binary}. The gain term utilizes floating-point weights \\textit{prior} to binarization during training.  During test-time, the alpha parameter scales the binarized weight vector.  The parameter rescales binary weights $\\mathbf{B} \\in\\{-1,1\\}$ to $\\{-\\alpha,\\alpha\\}$, and the network function becomes $f(x;\\alpha(\\mathbf{B} \\odot \\mathbf{M}))$.  $\\alpha$ is calculated as\n%$\n%||\\mathbf{M}\\odot\\mathbf{W}||_1/||\\mathbf{M}||_1\n%$.\n\\begin{equation}\n \\alpha=\\frac{||\\mathbf{M}\\odot\\mathbf{W}||_1}{||\\mathbf{M}||_1}\n\\end{equation}\n\nwith  $\\mathbf{M}$ being multiplied by $\\alpha$ for gradient descent (the straight-through estimator is still used for backpropagation). This calculation was originally derived by Rastegari et al.  \\cite{rastegari2016xnor}.  \n\n\n\n\n\nIn our approach we create sparse and binary modules for each linear and layer normalization layer.  Our model consists of two linear layers at the top most level: one for projecting the initial input (embedding in NLP models) and one used for the decoder output.  Additionally, each encoder layer consists of six linear layers: $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ projections, the multi-head attention output projection, and two additional layers to complement multi-head attention.  \n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Attention Modifications",
                    "content": "\\label{attentionmods}\n\nIn this section we describe two modifications made to the attention module\nto reduce its quadratic complexity.  Several previous works have proposed changes to attention in order to lessen this bottleneck, such as Sparse Transformers \\cite{child2019generating}, ProbSparse Attention \\cite{zhou2021informer}, and Pyramidal Attention \\cite{liu2021pyraformer}. While each of these works present quality enhancements to the memory bottleneck of attention, we instead seek to evaluate whether simple sparsification approaches can retain the accuracy of the model compared to canonical attention.  Our primary motivation for the following attention modifications are to test whether a compressed Transformer can retain the same accuracy as a Dense Transformer. % Since the attention module is the most critical aspect of the Transformer, applying such modifications becomes an important \n\n",
                    "subsubsection 3.3.1": {
                        "name": "Fixed Q,K, and V Projection Mask",
                        "content": "\nTo reduce the computational complexity of the matrix multiplications within the attention module, we apply random fixed masks to the  $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ projections.  We hypothesize that we can retain the accuracy of full attention by using this ``naive'' activation pruning approach, which requires no domain knowledge.  We argue that the success of this approach provides insight into the necessity of full attention computations. In other words, Transformers are expressive and powerful enough for certain tasks that we can prune the models in an unsophisticated way and maintain accuracy. Moreover, many time series datasets and datasets generated at the edge are often times simplistic enough that we can apply this unsophisticated pruning \\cite{gorbett2022local,gorbett2022wip}.\n\nTo apply this pruning, on model initialization we create random masks with prune rate $p_a \\in \\{0,1\\}$ for each attention module  and each projection Q,K, and V.  Attention heads within the same module inherit identical Q, K, or V masks.  The mask is applied to each projection during train and test. In each of our models we set the prune rate $p_a$ of the attention module equal to the prune rate of the linear modules ($p_a=p$).   %Specifically, prior to the attention computation, each projection is sorted based on its top absolute values.  The Top-P values are kept based on an attention module prune rate $p_a \\in \\{0,1\\}$, and all other values are set to zero.  The modification creates many zero computations in the attention mechanism, reducing overhead. \n\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Step-t Attention Mask",
                        "content": "\nFor anomaly detection and single-step forecasting tasks, the \\gls{sbt} algorithm relies on reconstructing or predicting outputs at the current time step $t$ for each feature $m$, despite $w$ time steps of data being provided to the model. Specifically, the \\gls{sbt} model is only interested in input vector  $\\mathbf{x_t}\\in\\mathbb{R}^{m}$. \nFor anomaly detection, the model reconstructs $\\mathbf{x_t}$ from the input, while in forecasting tasks the model masks $\\mathbf{x_t}=0$ prior to model input, reconstructs the actual values during training and inference.  \n\nIn both tasks, vector $\\mathbf{x_t}$ contains the only values necessary for the model to learn, and our loss function reflects this by only computing error for these values.\nAs a result, computing attention for each other time step  adds unnecessary computation.  As depicted in Figure \\ref{steptfig}, we pass a static mask to the attention module to compute attention only at step-T.  We additionally exclude attention computation at step-T with itself, forcing the variable to attend to historical time points for prediction.  Finally, we add diagonal ones to the attention mask at all past time points to add stability to training.  This masking method allows us to propagate the full input sample to multiple attention layers, helping us retain relevant historical information for downstream layers that would not be possible by changing the sizes of Q, K, and V to only model the $t$ time step.  \n\n\n%\\begin{comment}\n\n%\\end{comment}\n\n\n%Explain Transformer-attention, encoder, linear modules\n%Explain pruning, add algorithm\n%Explain masks in encoder\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n%Experimental Setup-keep brief, and describe datasets used. \n\n%Classification\n%Anomaly Detection\n%Time Series Forecasting\n\nIn this section we detail our experiments for time series classification, anomaly detection, and forecasting.  \nCommon to each learning task, we normalize each dataset prior to training such that each feature dimension $m$ has zero mean and unit variance.  We use the Transformer Encoder as described in Section \\ref{method}, training each learning task and dataset  using the Dense Transformer and the \\gls{sbt} to compare accuracy.  Finally, we run each experiment three times with a different weight seed, and present the average result.  For the \\gls{sbt} model, varying the weight seed shows evidence of the robustness to hyperparameters.   Specific modifications to the model are made for each learning task, which we describe in the following sections. Additional training and architecture details can be found in the Appendix.  \n\n\n",
                "subsection 4.1": {
                    "name": "Classification",
                    "content": "\n\nFor our first time series learning task we select several datasets from the UCR Time Series Classification Repository \\cite{bagnall16bakeoff}.  The datasets contain diverse characteristics including varying training set size (204-30,000), number of features (13-200), and window size (30-405).  We choose three datasets with the largest test set size (Insect Wingbeats, Spoken Arabic Digits, and Face Detection) as well as two smaller datasets (JapaneseVowels, Heartbeat).\nEach dataset contains a set window size except for Insect Wingbeats and Japanese Vowels, which contain a window size \\textit{up to} 30 and 29, respectively.  In these datasets, we pad samples with smaller windows to give them consistent window sizes. \nThe decoder in our classification architecture is a classification head, rather than a full reconstruction of the input as is used in anomaly detection and forecasting tasks.  %We use cross entropy to train the models.  \nThe \\gls{sbt} classification model is trained and tested using the fixed Q,K,V projection mask as described in Section \\ref{attentionmods}.  %The module takes the top $\\textbf{\\mathcal{Q}}$, $\\textbf{\\mathcal{K}}$, and $\\textbf{\\mathcal{V}}$ projections and sets lower values to zero.  The resulting projections $\\textbf{\\mathcal{Q}}'$, $\\textbf{\\mathcal{K}}'$, $\\textbf{\\mathcal{V}}'$ are passed through the standard attention computation, leading to significantly less non-zero computations.  Our hypothesis with Top-P attention is that we can replicate the results of fully connected attention with only a subset of the activations from $\\textbf{\\mathcal{Q}}$, $\\textbf{\\mathcal{K}}$, and $\\textbf{\\mathcal{V}}$%., even in a \"naive\" way in which $\\textbf{\\mathcal{Q}}'$, $\\textbf{\\mathcal{K}}'$, and $\\textbf{\\mathcal{V}}'$ are computed independently of each other.  \n\n\\noindent\\textbf{Results}\nIn Table \\ref{class_data}, we show that \\gls{sbt}s perform as well as, or similar to, the Dense Transformer for each dataset at $p=0.5$ and $p=0.75$.  Our models are averaged over three runs with different weight seeds.   When comparing our model to state-of-the-art approaches, we find that the \\gls{sbt} achieves strong results across each dataset, with the highest reported performance on three out of the five datasets.  Further, the \\gls{sbt} models perform consistently across datasets while models such as Rocket \\cite{dempster2020rocket} and Fran et al. \\cite{franceschi2019unsupervised} have lower performance on one or more datasets.   \n\nSurprisingly, the \\gls{sbt} model achieves stronger average accuracy than the Dense Transformer (80.2\\% versus 78.8\\%), indicating that the pruned and binarized Transformer achieves a robust performance across datasets.  Despite this, Insect Wingbeats and Japanese Vowels datasets achieved a slightly lower performance at $p=0.5$ with a more substantial dropoff at $p=0.75$, indicating the model may lose some of its power on certain tasks.  \n\n%Compared to state-of-the-art models on the five datasets (as documented in \\cite{zerveas_transformer}), \n\n%For Insect Wingbeats, which contained more overall data, our model performed on average 2\\% worse than the Dense transformer.  Despite this, it still attained a higher mean accuracy (63\\%)\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Anomaly Detection",
                    "content": "\n\nFor the anomaly detection task we test the \\gls{sbt} algorithm on established multivariate time series anomaly detection datasets used in previous literature: Soil Moisture Active Passive Satellite (SMAP) \\cite{hundman2018detecting}, Mars Science Labratory rover (MSL) \\cite{hundman2018detecting}, and the Server Machine Dataset (SMD) \\cite{su_robust_2019}.  SMAP and MSL contain telemetry data such as radiation and temperature, while SMD logs computer server data such as CPU load and memory usage.  The datasets contain benign samples in the training set, while the test set contains labeled anomalies (either sequences of anomalies or single point anomalies). \n\n%A key distinction in our work is that we model each dataset at the \\textit{entity} level, similar to Su et al. \\cite{su_robust_2019}, who propose entity level modeling in order to capture finer detail metrics.  In particular, we train our architecture on 28 SMD entities, 55 SMAP entities, and 27 MSL entities.  \n\nOur model takes sliding window data as input and reconstructs data at $\\mathbf{x_t}$ given previous time points.  We use MSE to reconstruct each feature in $\\mathbf{x_t}$. We use the step-T attention mask as described in Section \\ref{method}.  %The mask forces the model to attend only to previous time points for feature $m$ at $x^t$, while past time points attend only to themselves.  \nTo evaluate our results, we adopt an adjustment strategy similar to previous works \\cite{xu_anomaly_2022, su_robust_2019,shen2020timeseries,tuli_tranad_2022}: if \\textit{any} anomaly is detected within a successive abnormal segment of time, we consider all anomalies in this segment to have been detected.  The justification is that detecting any anomaly in a time segment will cause an alert in real-world applications.  \n\nTo flag anomalies, we retrieve  reconstruction loss $\\mathbf{x'_t}$ and threshold $\\tau$, and consider anomalies where \n$\\mathbf{x'_t}>\\tau$.  Since our model is trained with benign samples, anomalous samples in the test set should yield a higher $\\mathbf{x'_t}$. We compute $\\tau$ using two methods from previous works:  A manual threshold \\cite{xu_anomaly_2022} and the \\gls{pot} method \\cite{siffer2017anomaly}.  For the manual threshold, we consider proportion $r$ of the validation set as anomalous.  For SMD $r=0.5\\%$, and for MSL and SMAP $r=1\\%$.  For the \\gls{pot} method, similar to OmniAnomaly \\cite{su_robust_2019} and TranAd \\cite{tuli_tranad_2022}, we use the automatic threshold selector to find $\\tau$.  Specifically, given our training and validation set reconstruction losses, we use \\gls{pot} to fit the tail portion of a probability distribution using the generalized Pareto Distribution.  \\gls{pot} is advantageous when little information is known about a scenario, such as in datasets with an unknown number of anomalies. \n\n\n%Our threshold is selected using both a manual threshold \\cite{xu_anomaly_2022}, and additionally using the \\gls{pot} method \\cite{siffer2017anomaly}, similar to previous works \\cite{su_robust_2019, tuli_tranad_2022}.  For the manual Given our training and validation set reconstruction losses $x'_t$, we use \\gls{pot} to fit the tail portion of a probability distribution using the generalized Pareto Distribution.  The resulting threshold $\\tau$ detects anomalies where $x'_t>\\tau$.  \n\n\n\n\\noindent\\textbf{Results}\nIn Table \\ref{anomaly_data} we report the unique findings of our single-step anomaly detection method using Precision, Recall, and F1-scores.  Specifically, we find that when only considering inputs with fully benign examples in window $w$, both the \\gls{sbt} and the Dense Transformer achieve high accuracy on all three datasets (F1 between 90.6 and 100).  In other words, we find that our model performance is best when we filter examples that have an anomalous sequence or data point in $[\\mathbf{x_{t-w}}, \\mathbf{x_{t-w+1}},...,\\mathbf{x_{t-1}}]$. For SMD, $w=200$ and  for SMAP and MSL $w=50$.  This observation implies that the model needs time to stabilize after an anomalous period.  Intuitively, if an anomaly occurred recently, new benign observations will have a higher reconstruction loss as a result of their difference with the anomalous examples in their input window.  We argue that this validation metric is logical in real-world scenarios, where monitoring of a system after an anomalous period of time is necessary.  \n\nWe additionally report F1-scores compared to state-of-the-art time series anomaly detection models in Table \\ref{anomaly_sota}.  To accurately compare our model against existing methods, we use the full test set without filtering out benign inputs with anomalies in the near past.  \\gls{sbt} results are much more modest, with F1-scores between 70 and 88.  Despite this, our method still performs stronger than non-temporal algorithms such as the Isolation Forest, as well as other deep-learning based approaches such as Deep-SVDD and BeatGan.  \n\n%We note that we do not claim to attain state-of-the-art results on any task using our model, instead seeking to evaluate how our simple \\gls{sbt} framework performs on various tasks.\n%We leave sparsification and binarization of state-of-the-art models for future work.  \n\n% We report the results of our method using the F1-score In Tables \\ref{anomaly_data} and \\ref{anomaly_sota} as well as Precision and Recall in Table \\ref{anomaly_data}.  In Table \\ref{anomaly_data} we report the unique findings of our single-step anomaly detection method, finding that \n\n\n%We report Precision, Recall, and F1-Score in Table \\ref{anomaly_data}.  The \\gls{sbt} model achieves accuracy similar to the fully-trained Dense model in each case.  Further, we note Anomaly Transformer \\cite{xu_anomaly_2022} attained similar results to our baseline Transformer model when they only considered reconstruction loss: MSL attained an F1 of 76.64, SMD 79.72, and SMAP 73.74.  Their model \n\n\n\n\n\n%\\end{comment}\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Forecasting",
                    "content": "\n\nWe test our method on single-step forecasting using the Step-T attention mask. Specifically, using the framework outlined by Zerveas et al. \\cite{zerveas_transformer}, we train our model by masking the input at the forecasting time-step $t$.  For example, input\n$\\mathbf{X_t}$ containing $m$ features and $t$ time-steps\n$[\\mathbf{x_{t-w}},\\mathbf{x_{t-w+1}},...,\\mathbf{x_{t}} ]$ is passed through the network with \n $\\mathbf{x_{t}}=0$.  We then reconstruct this masked input with the Transformer model, using mean squared error between the masked inputs reconstruction and the actual value.  The masking method simulates unseen future data points during train time, making it compatible with the forecasting task during deployment.\n\nWe test our model on three datasets used in previous works:  ECL contains electricity consumption of 321 clients in Kwh.  The dataset is converted to hourly consumption values due to missing data. Weather contains data for twelve hourly climate features for 1,600 location in the U.S.  ETTm1 (Electricity Transformer Temperature) contains 15-minute interval data including oil temperature and six additional power load features.  Additional training details are available in the Appendix. \n\nWe compare our method against the Informer \\cite{zhou2021informer} and the Pyra-former \\cite{liu2021pyraformer}  trained with single-step forecasting.  Both are current state-of-the-art models that have shown robust results compared against a variety of forecasting techniques.  Importantly, each method is compatible with multivariate time series forecasting as opposed to some research.  We note that these models are built primarily for \\gls{lstf}, which we do not cover in this work.  %While each of these models is built primarily for \\gls{lstf}, we note that each model is compatible with single-step forecasting.  Further, each method is  %work has shown that several Transformer architectures are outperformed by a simple linear model on the \\gls{lstf} task \\cite{zeng2022transformers}.  \n\n\n\n\\noindent\\textbf{Results}\nWe evaluate results in Table \\ref{forecasting_results} using MSE and MAE on the test set of each dataset.  Results indicate that the \\gls{sbt} model achieves accuracy comparable to the Dense architecture in each dataset at $p=0.5$.  Interestingly, the Weather at ETTm1 \\gls{sbt} models achieved \\textit{better} accuracy than the dense model at $p=0.5$.  Both models additionally showed robustness to higher prune rates, with accuracy dropping off slowly.  ECL on the other hand showed some sensitivity to prune rate, with a slight drop off when increasing the prune rate.  We find that datasets with a higher dimensionality performed the worst: ECL contains 321 features, while Insect Wingbeats contains 200.  Increasing the dimensionality of the model ($d$) mitigated some of these effects, however it was at the cost of model size and complexity.  Despite this, we find that the \\gls{sbt} model is able to predict the general trend of complex patterns in data, as depicted in Figure \\ref{forecast_pic}.  \n\nCompared to state-of-the-art approaches such as the Pyraformer and Informer architectures, our general purpose forecasting approach performs comparably, or slightly worse, on the single-step forecasting task.  Metrics were not substantially different for any of the models except for the ECL dataset, where Pyraformer was easily the best model.  Comparing the architectures, we find that the \\gls{sbt} model achieves substantially lower computational cost than both the Informer and Pyraformer models.  For example, on the ECL dataset, Pyraformer contains 4.7 million parameters and the Informer 12.7 million parameters (both \\gls{fp32}, while the \\gls{sbt} model contains 1.5 million binary parameters.  \n\n\n\n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Architecture",
                    "content": "\nEach model in our framework consists of 2 encoder layers each with a multi-head attention module containing two heads.  The feedforward dimensionality for each model is 256 with ReLU used for nonlinearity.  Classification models had the best results using Batch Normalization layers, similar to \\cite{zerveas_transformer}, while forecasting models used Layer Normalization typical of other Transformer models.  For anomaly detection we did not use Batch or Layer Normalization.  For the output of our models, \nanomaly detection and forecasting rely on a single decoder linear layer which reconstructs the output to size ($m$, $w$), while classification outputs size ($d$, $num. classes$) and takes the mean of $d$ to formulate a final classification prediction.  Further details are included in the Appendix and the code repository.  \n\n\n"
                }
            },
            "section 5": {
                "name": "Computational Savings",
                "content": "\n\n\nIn this section we estimate the computational savings achieved by using  the \\gls{sbt} model.  \nWe will begin by introducing the metrics used to estimate computational savings, and will then summarize the results of these metrics for each model and task.  \n\n\n\n\nWe note that several works (highlighted in Section \\ref{related}) have proposed modifications to the Transformer in order to make attention more efficient.  In this section, we concentrate on the  enhancements achieved by 1) creating a sparsely connected Transformer with binary weights, and 2) simplifying the attention module for time series specific tasks such as single-step prediction and classification.  We argue that these enhancements are independent of the achievements made by previous works.  \n\n\n\n\n\n\n\n\n",
                "subsection 5.1": {
                    "name": "Metrics",
                    "content": "\n\n\\noindent \\textbf{FLOPs (Non-zero).} In the field of network pruning, \\gls{flops}, or the number of multiply-adds, is a commonly used metric to quantify the efficiency of a neural network \\cite{blalock2020state}.  The metric computes the number of floating point operations required for an input to pass through a neural network.  We use the ShrinkBench tool to calculate FLOPs, a framework \nproposed by Blalock et al. \\cite{blalock2020state} to perform standardized evaluation on pruned neural networks.  \n\n\n\n\nOur Transformer architecture contains \\gls{fp32} activations at each layer along with binary weights scaled to $\\{-\\alpha,\\alpha\\}$.  As a result, no binary operations are performed, and our total FLOPs count is a function of prune rate $p$.  For example, a linear module with a standard FLOPs count of $d \\times m$ has a new FLOPs count of $d \\times m \\times p$, where $p \\in [0,1]$.  Linear layers outside of attention do not need window size added to the matrix multiply because the inputs are permuted such that batch size is the second dimension of the layer input.  Each equation counts the number of \\textit{nonzero} multiply-adds necessary for the neural network.  \n\n\n\n\n\n\n\nFurthermore, we modify the FLOPs for the attention module to account for step-t attention mask and the fixed $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ mask, as summarized in Table \\ref{attention_equations}.  \nIn the standard attention module where $\\mathbf{Q},\\mathbf{K}$ and $\\mathbf{V}$ are equal sized projections,  matrix multiply operations ($\\mathbf{Q}\\mathbf{V}^\\intercal$, $\\mathbf{AV}$) for each head equate to $d'w^2$, where $d'=d/h$.  \n%The softmax operation equates to $w^2$.  Using the Step-T attention mask, we reformulate this equation to $2(w-1)d'$ for matrix multiplies and $2w$ for the softmax operation. \nFor step-t attention, we only require computation at the current time step (the last row in Figure \\ref{steptfig}), while each each of the identities for past time steps equates to one.  $\\mathbf{A} \\mathbf{V}$ requires double the computations because $\\mathbf{V}$ contains \\gls{fp32} activations multiplied by the diagonal in $\\mathbf{A}$.  For the fixed mask, \nsince $\\mathbf{Q}$ and $\\mathbf{K}$ are sparse projections, we only require $(wp_a)^2$ nonzero computations in the matrix multiply.  Since  $\\mathbf{A}$ is a dense matrix, we require $w^2$ FLOPs to multiply  sparse matrix $\\mathbf{V}$. \n\nA simplified equation for network FLOPs becomes $2L+N(2L+MHA)$, where $L$ is a linear layer, $N$ is the number of attention layers, and $MHA$ is the multihead attention FLOPs (details described in Table \\ref{attention_equations}).  Several FLOP counts are omitted from this equation, which we include in our code, including positional encoding, $Q$-scaling, and layer and batch norm. \n\n%we estimate both matrix multiplies and softmax operations as $(wp_a)^2$.  Top-P values vary across inputs in the datasets, with different projections being activated by different data samples.  As a result, we estimate the FLOPs for matrix multiplies as its highest theoretical value (where unpruned weights match column and row-wise in two multiplied matrices).  For Top-P attention, we  add $3wdlog(wd)$ for each attention module to account for the sorting required to attain the Top-P values of $\\mathbf{Q},\\mathbf{K}$ and $\\mathbf{V}$.  %More information on these equations is available in the Appendix. \n\n%N Encoder layers: 2 dense layers outside of attention, \n%$2L+N(2L+MHA)$\n\n\n\n\n\\noindent \\textbf{Storage Size.} We measure the size of each model in total bits.  Standard networks rely on weights optimized with the \\gls{fp32} data type (32 bits).  We consider each binarized module in our architecture to contain single bit weights with a single \\gls{fp32} $\\alpha$ parameter for each layer.  Anomaly detection and classification datasets contain 14 binarized modules, and forecasting contains 18 with the additional binarization of the layer normalization.   We note that the binarized quantities are only theoretical as a result of the PyTorch framework not supporting the binary data type.  Hardware limitations are also reported in other works \\cite{frankle_lottery_2019}.   \n\n\n"
                },
                "subsection 5.2": {
                    "name": "Model Size Selection",
                    "content": "\\label{size}\nImportant to our work is tuning the size of each model.  We analyze whether we can create a Dense Transformer with a smaller number of parameters and still retain a performance on par with a larger model.  Our motivation for model size selection is two-fold: 1)  Previous research has found that neural networks need to be sufficiently overparameterized to be pruned and retain the same accuracy of the dense model and 2) The time series datasets studied in this paper have a smaller number of dimensions than the vision datasets studied in most pruning and model compression papers.  The effect of model overparameterization is that we need a dense model with enough initial parameters in order to prune it and still retain high performance.  Theoretical estimates on the number of required parameters are proposed by the Strong Lottery Ticket Hypothesis \\cite{pensia_optimal_2020, orseau_logarithmic_2020} and are further explored in other pruning papers \\cite{diffenderfer_multi-prize_2021,chijiwa_pruning_2021}.  On the other hand, the limited features of some time series datasets (such as Weather with 7 features) leads us to wonder whether we could simply create a smaller model.  \n\nTo alter the model size, we vary the embedding dimension $d$ of the model.  To find the ideal size of the model, we start from a small embedding dimension (such as 8 or 16), and increase the value in the Dense Transformer until the model performance on the validation set stops increasing.  With this value of $d$, we test the \\gls{sbt} model.  \n\nOur results show that in each dataset, Dense Transformers with a smaller embedding dimension $d$ either a) perform worse than the \\gls{sbt} at the optimized size, b) contain more parameters (as measured in total bits), c) have more FLOPs, or d) some combination of the above.  In almost every dataset, the smaller Dense Transformer performs worse than the \\gls{sbt} while also requiring more size and FLOPs.  The exception to this was Spoken Arabic Digits, where the smaller Dense Transformers ($d=16$ and $d=32$) performed slightly better than the \\gls{sbt} with $d=64$.  Additionally, these models had a lower FLOPs count.  The advantage of the \\gls{sbt} model in this scenario was a substantially lower storage cost than both smaller Dense models.  Even if both Dense Transformer models were able to be quantized to 8-bit weights, the storage of the \\gls{sbt} would still be many times lower.  The ETTm1 dataset additionally had high performance Dense Transformers with a smaller size ($d=16, d=32)$.  However, both models were substantially more costly in terms of storage and additionally had a higher FLOPs count.  Detailed results are provided in the Appendix.  \n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Analysis",
                    "content": " \n\nResults in Table \\ref{table:flops_results} highlight the large computational savings achieved by \\gls{sbt}.   \nWe find that layer pruning reduces FLOPs count (due to the added nonzero computations), while binarization helps with the storage size. \n\nNotably, all models have a FLOPs count at least two times less than the original Dense model. FLOPs are dramatically reduced in the anomaly detection and forecasting datasets, largely due to the step-t masking. \nClassification datasets have a dense attention matrix, leading to a smaller FLOPs reduction due to the softmax operation and the $AV$ calculation (where $V$ is sparse).  We note that using a higher prune rate can reduce the FLOPs more, however we include results at 50\\% prune rate for classification since these models achieved slightly better accuracy.  \n\nWe highlight the storage savings of \\gls{sbt} models by measuring bit size and parameter count.  Table \\ref{table:flops_results} summarizes the substantial reduction in bit size for every model, with only two \\gls{sbt} models having a bit size greater than 1 million (Insect Wingbeats and ECL). \nThe two models with a larger size also had the highest dimensionality $m$, and consequently $d$.  %Further, the high dimensionality of these two datasets meant that they performed slightly weaker on their given tasks (classification and forecasting). \n  \nWe note that  \\gls{sbt} models contain a small number of \\gls{fp32} values due to the single $\\alpha$ parameter in each module.  Additionally, we forego a learnable encoding layer in \\gls{sbt} classification models, leading to a smaller overall count.  Finally, no bias term is added to the \\gls{sbt} modules, leading to a smaller number of overall parameters.  \n\nCompared to other efficient models, our model generally has a lower FLOPs count. For example, MobileV2 \\cite{sandler2018mobilenetv2} has 16.4 million FLOPs when modeling CIFAR10, while EfficientNetV2 \\cite{tan2021efficientnetv2} has 18.1 million parameters.  %All but two of our datasets achieve a lower FLOPs count than these two models.  \n\n%Classification datasets, where the Top-P attention mask is used, see the most improvement in model size as measured in total bits.  The reduction can be attributed to the sparse linear layers used within the attention module.  Anomaly detection and forecasting tasks, on the other hand, use fully connected linear layers within the attention module, employing a masking strategy on top on the linear projections to limit the necessary computation.  As a result the reduction in model size is only 3-4x, which still amounts to a substantial improvement.  \n\n%We also look at specific components of the architecture to analyze where  computational savings are being achieved.  Attention modules see the highest reduction in FLOPs in each model, with counts going from ~125k-42m in Dense architectures to between 8k and 240k in Sparse Binary Transformers.  Each range estimates the FLOPs for a single attention module. For linear modules, the FLOP count is reduced by multiplying the prune rate: linear layers with a FLOP count of 32k will be reduced to 16k with $p=0.5$.  Results are further detailed in the Appendix. \n\n%While the reduction in FLOPs from linear modules is less apparent than the computational savings achieved from the modifications to attention, we argue that sparse and binary linear modules are just as crucial to the computational savings because of the storage cost saved.  Binarized linear modules are able to be saved in bits rather than \\glspl{fp32}, creating a substantially smaller network in terms of storage.  To the best of our knowledge, no other work has examined the cost savings of weight binarization in attention-based architectures on time-series tasks.  \n\n\n\n%We compute the savings of using binary weights.  We additionally account for scale parameter $\\alpha$ in the computation.  \n%The PyTorch framework offers several options for quantizing dense \\gls{fp32} models, such as to 8-bit integer data types, however we considere this outside the scope of our work as \n\n%To begin, we introduce two metrics used in the field of Neural Network Pruning: \n%∂Blalock et al. \\cite{blalock2020state} introduced the ShrinkBench tool for standardizing the evaluation of pruning methods.  \n%From survey\n%\\textbf{Attention} Central to Transformer is the self-attention module. It can be\n%viewed as a fully connected layer with the weights that are\n%dynamically generated based on the pairwise similarity of input patterns. As a result, it shares the same maximum path\n%length as fully connected layers, but with much less number\n%of parameters, making it suitable for modeling long-term dependencies.\n%As we show in the previous section the self-attention module in vanilla Transformer has a time and memory complexity of O(L\n%2) (L is the input time series length), which becomes the computational bottleneck when dealing with long sequences.\n\n\n\n%\\section{Ablation Studies}\n%\\input{55-Ablation}\n\n%\\input{}\n"
                }
            },
            "section 6": {
                "name": "Discussion",
                "content": "\nWe show that Sparse Binary Transformers attain similar accuracy to the Dense Transformer across three multivariate time series learning tasks: anomaly detection, forecasting, and classification.  We estimate the computational savings of \\gls{sbt}'s by counting FLOPs as well as total size of the model.  %We argue that the retained accuracy of \\gls{sbt}'s coupled with the large reduction in computational cost provides avenues for the Transformer architectures to be deployed \n\n",
                "subsection 6.1": {
                    "name": "Applications",
                    "content": "\n\\gls{sbt}s retain high performance compared to dense models, coupled with a large reduction in computational cost. As a result, \n\\gls{sbt}s have the potential to impact a variety of new domains.  \nFor example, sensors and small embedded systems such as IoT devices could employ  \\gls{sbt}s for intelligent and data-driven decisions, such as detecting a malicious actor or forecasting a weather event.  \nSuch devices could be extended into new areas of research such as environmental monitoring.  Other small capacity applications include implantable devices, healthcare monitoring, and various industrial applications.  \n\nFinally, lightweight deep learning models can also benefit larger endeavors. For example, space and satellite applications, such as in the MSL and SMAP telemetry datasets, collect massive amounts of data that is difficult to monitor.  Employing effective and intelligent algorithms such as the Transformer could help in the processing and auditing of such systems. \n\n%to help with monitoring applications could enhance the Insect Wingbeat model in order to clasify \n%F%or example, several datasets chosen for this research are derived from areas where computational  \n%The high accuracy retained by \\gls{sbt}'s, coupled with the large reduction in computational cost provides new avenues for the Transformer architectures to be deployed.  Several datasets chosen for this research provide \n\n\n\n\n"
                },
                "subsection 6.2": {
                    "name": "Limitations and Future Work",
                    "content": "\nAlthough \\gls{sbt}s theoretically reduce computational costs, the method is not optimized for modern libraries and hardware.  Python libraries do not binarize weights to single bits, but 8-bit counts.  Special hardware in IoT devices and satellites could additionally make implementation a burden.  \nAdditionally, while our implementation shows that sparse binarized Transformers exist, the Biprop algorithm requires backpropagation over a dense network with randomly initialized \\gls{fp32} weights.  Hence, finding accurate binary subnetworks requires more computational power during training than it does during deployment.  This may be a key limitation in devices seeking autonomy.\nIn addition to addressing these limitations, a logical step for future work would be to implement \\gls{sbt}s in state-of-the-art Transformer models such as the Pyramformer for forecasting and the Anomaly Transformer for time series anomaly detection.    \n\n\\gls{sbt}s have the potential to enable widespread use of AI across new applications.  The Transformer stands as one of most powerful deep learning models in use today, and expanding this architecture into new domains provides promising directions for the future.   \n\n"
                }
            },
            "section 7": {
                "name": "Acknowledgements",
                "content": "\nThis work was supported in part by funding from NSF under Award\nNumbers ATD 2123761, CNS 1822118, NIST, ARL, Statnett, AMI,\nNewPush, and Cyber Risk Research.\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{sbt}\n\n%\\newpage\n%%\n%% If your work has an appendix, this is the place to put it.\n%\\clearpage\n\\appendix\n\\setcounter{figure}{0}\n\\setcounter{table}{0}\n\n%\\onecolumn\n%\\tableofcontents\n"
            },
            "section 8": {
                "name": "Supplemental Materials",
                "content": "\n%\\section{Research Methods}\n\n"
            },
            "section 9": {
                "name": "Ablation Studies",
                "content": "\nWe conduct two ablation studies testing the effects of removing the individual pruning mechanisms from the attention computation. We note that the attention pruning methods complement Biprop -- Biprop mainly reduces the model size, whereas attention pruning does a better job at  reducing the FLOPs. Each ablation experiment is averaged over three experimental runs with different seeds.  \n\nTable \\ref{table:ablation1} highlights the effects of removing random pruning from the time series classification models.  Notably, Biprop plus random pruning performs comparably to, or better than, Biprop on its own.  Adding random pruning even outperforms using only Biprop with the Japanese Vowels dataset.  \n\n\n\n\n\n\nTable \\ref{table:ablation2} highlights the results of attention variations for both anomaly detection and forecasting tasks. Specifically, we look at our proposed approach (Biprop+Step-T Mask), Biprop plus an identity matrix mask in the attention layers, and finally Biprop only.   We report results using mean squared error (MSE) loss averaged over three runs.  \n\nResults show that Biprop plus the Step-T mask performs comparably to using Biprop only.  For anomaly detection tasks, the MSE is even lower compared to just using Biprop.  Comparing both methods to the Biprop plus the identity matrix attention mask, we can see a significant difference in the results: the identity matrix attention mask attains a higher loss in each case.  \n\n\n\n\n"
            },
            "section 10": {
                "name": "Training Details",
                "content": "\n\\begin{comment}\n    \n\n\\begin{table*}[h]\n\\begin{center}\n\\setlength{\\tabcolsep}{0.27em}\n\\renewcommand{\\arraystretch}{1.15}%\n\\begin{tabular}{    c|cccc|ccc|ccc} \n\\toprule\n\\multicolumn{1}{c}{}&\\multicolumn{4}{c}{}&\\multicolumn{3}{c}{Dense Transformer}&\\multicolumn{3}{c}{Sparse Binary Transformer}\\\\\n%\\midrule\n\\multicolumn{1}{c}{Type}&\\multicolumn{1}{c}{Dataset}&$m$&$w$&\\multicolumn{1}{c}{$d$}&Epochs& Scheduler& \\multicolumn{1}{c}{Norm.}  & Epochs& Scheduler&\\multicolumn{1}{c}{Norm.} \\\\\n\\hline\n\n \\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\\footnotesize{Classification}}}}&Heartbeat\t&\t61\t&\t405\t&\t64\t&\t50&False&BN&\t250&False&BN\\\\\n&Insect W.B.\t&\t200\t&\t30\t&\t256\t&\t50&False&BN&\t250&False&BN\t\\\\\n&Arabic Dig.\t&\t13\t&\t93\t&\t64\t&\t50&False&BN&\t250&False&BN\\\\\n&Japan.Vowels\t&\t12\t&\t29\t&\t32\t& 50&False&BN&\t250&False&BN\\\\\n&FaceDetection\t&\t144\t&\t62\t&\t128\t&\t200&False&BN&\t200&False&BN\\\\\n\\hdashline\n \\parbox[t]{7.2mm}{\\multirow{3}{*}{\\rotatebox[origin=c]{90}{\\footnotesize{\\thead{Anomaly\\\\Detection}}}}}&MSL\t&\t55\t&\t50\t&\t110\t&50&True&None&50&True&None\\\\\n&SMAP\t&\t25\t&\t50\t&\t50\t&50&True&None&50&True&None\t\\\\\n&SMD\t&\t38\t&\t200\t&\t76\t&50&True&None&50&True&None\t\\\\\n\\hdashline\n \\parbox[t]{2mm}{\\multirow{3}{*}{\\rotatebox[origin=c]{90}{\\footnotesize{Forecast.}}}}&ECL\t&\t321\t&\t200\t&\t350\t& 100&True&LN&100&True&LN\\\\\n&Weather\t&\t7\t&\t200\t&\t100\t&50&True&LN&100&True&LN\\\\\n&ETTm1\t&\t12\t&\t200\t&\t64\t&50&True&LN&100&True&LN\\\\\n\\hline\n\\end{tabular}\n\\caption{Configurations for our various models.  Our scheduler uses a step learning rate with gamma=0.75.  We increment it if validation loss has not decreased. }\\label{train_details}\n \\vspace{-25pt}\n\\end{center}\n\\end{table*}\n\\end{comment}\n\nEach model is trained with Adam optimization with a learning rate of 1e-3 except for InsectWingbeats, where we use a learning rate of 1e-4.  For Dense Transformer classification models we use a learnable positional encoding, while in all other models we use a standard positional encoding.  \n\nWe found that \\gls{sbt} models sometimes take slightly longer to converge, hence we train the models for more epochs in the forecasting and classification tasks. These numbers are specified in the configuration files in the code repository. % We outline the number of epochs used for each task Table \\ref{train_details}. Additionally we denote whether we use a scheduler.  \nBatch Normalization is used for classification tasks, layer normalization is used for forecasting tasks, and no normalization is used for anomaly detection. \n\n\n\n%\\clearpage\n\n"
            },
            "section 11": {
                "name": "Analysis",
                "content": "\n",
                "subsection 11.1": {
                    "name": "Attention Magnitude Pruning versus Random Pruning",
                    "content": "\n\nAs apart of our attention pruning analysis, we also applied magnitude pruning to the attention layers.  However, this method requires extra computation as a result of the sorting required to take the top activation's for each input.  Below we compare the results of magnitude pruning versus random pruning, finding that random pruning achieves similar accuracy to magnitude pruning at a lower computational cost. \n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 11.2": {
                    "name": "Model size savings of Biprop versus Pruning",
                    "content": "\n\nIn Table \\ref{table:other2} we compare the model size savings of Biprop compared to 32-bit pruning as well as pruning plus quantization (8-bit).  We show that, even compared to pruning plus 8-bit quantization, Biprop achieves substantially lower model size.  \n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{comment}\n    \n\n\n\\subsection{Biprop with high prune rate versus Biprop with Random Pruning}\n\nIn Table \\ref{table:other3} we test whether we can achieve similar accuracy using a high Biprop prune rate compared to using Biprop with random pruning in the attention layer.  We find that, for Arabic Digits, accuracy goes down when using a higher rate.  For the Heartbeat dataset accuracy stays similar, however, the FLOPs are substantially higher. \n\n\n\n\\begin{table*}[h]\n\\large\n\\begin{center}\n\\setlength{\\tabcolsep}{0.75em}\n\\begin{tabular}{ ccccc }\n\\hline\n%Dataset\t&Biprop Only Accuracy (90\\% pr)\tBiprop only FLOPS (90\\% pr)\tBiprop+Random Pruning Accuracy (50\\% pr)\tBiprop+Random Pruning FLOPs (50\\% pr)\\\\\nDataset\t&\\thead{Biprop Only (90\\% pr)\\\\Accuracy}& \\thead{Biprop Only (90\\% pr)\\\\FLOPs}& \\thead{Biprop+Random Pruning (50\\% pr)\\\\Accuracy}& \\thead{Biprop+Random Pruning (50\\% pr)\\\\FLOPs}\\\\\t\n\\hline\nArabic Digits&\t87.8 $\\pm3.5$&\n1.3&\t98.2&\t1.3\\\\\nHeartbeat&\t75.8$\\pm2.2$ &\n43.7&\t77.2&\t21.6\\\\\n\n\\hline\n\\end{tabular}\n\\caption{Biprop with a high prune rate compared to using Biprop with random pruning and a lower prune rate.   }\\label{table:other3}\n\n\\end{center}\n\\end{table*}\n\n\\end{comment}\n\n"
                }
            },
            "section 12": {
                "name": "Dataset Details",
                "content": "\nWe report the details of datasets used for each task below.  For anomaly detection and forecasting tasks, we set the window size $w$ to a fixed value, while in classification, $w$ is predefined.  \n\n%\\subsection{Classification}\n\n%\\vspace{5px}\n\n\n\n\n\n\n\n\n%\\subsection{Single-Step Forecasting}\n\n\n\n\n\n"
            },
            "section 13": {
                "name": "Model Size Selection",
                "content": "\nWe measure model performance as well as computational cost at varying sizes for each model.  To vary the size, we increase the embedding dimension $d$ for each model and dataset combination.  Tables \\ref{table:classification_size} and \\ref{table:forecasting_size} show the results for each model size and dataset combination.  Overall, we find that the \\gls{sbt} generally performs better than the smaller Dense Transformer in terms of performance, except in a few cases.  In all scenarios, the \\gls{sbt} model has at least one computational advantage in terms of storage size or FLOPs count.  %We refer readers to Section \\ref{size} for more details.  \n\nAdditionally we find that, common with our intuition, datasets with a higher dimensionality $m$ need a higher embedding dimension, while simpler datasets are successful with a smaller embedding dimension.  For example, Insect Wingbeats ($m=200$), Face Detection ($m=144$), and ECL ($m=321$) require $d \\ge 128$ to achieve optimal performance.  \n%For example, \n\n\n\n\n\n\n\n\n\n\n\n%\\clearpage\n%\\newpage\n%\\mbox{~}\n%\\clearpage\n%\\newpage\n\n%\\section{Architecture}\n%\\input{appendix/arch}\n\n\n\n\n"
            }
        },
        "tables": {
            "attention_equations": "\\begin{table}[ht]\n\\begin{center}\n%\\setlength{\\tabcolsep}{0.4em}\n\\renewcommand{\\arraystretch}{1.4}%\n\\begin{tabular}{ lccc } \n\\toprule\nAttention Type & Q,K,V Proj.& $\\mathbf{Q}\\mathbf{V}^\\intercal$& $\\mathbf{AV}$   \\\\\n\\midrule\nCanonical&$d^2w$&$d'w^2$&$d'w^2$ \\\\ \nStep-T Mask&$(d^2w)p_a$&$(w-1)d'$&$2(w-1)d'$  \\\\\nQ,K,V Mask &$(d^2w)p_a$&$d'(wp_a)^2$&$(d'w^2)p_a$ \\\\\n\\hline\n\\end{tabular}\n\n\\end{center}\n\\caption{\\textbf{Non-zero FLOPs equations for various attention modules.}  These calculations assume $\\mathbf{Q},\\mathbf{K}$ and $\\mathbf{V}$ are equal sized projections in $\\mathbb{R}^{w \\times d}$, and $d'=d/h$.  $\\mathbf{Q}\\mathbf{V}^\\intercal$ and $\\mathbf{AV}$ are additionally multiplied by $h$.  $\\mathbf{Q}$-scaling and softmax FLOPs excluded from this table. }\\label{attention_equations}\n\\end{table}"
        },
        "figures": {
            "teaser": "\\begin{figure}\n\\includegraphics[width=8cm]{figures/teaser.png}\n\\caption{A sparse binary linear layer (left) and various attention modules (right). a) An example of a sparse and binary linear module, with binary weights $\\mathbf{B}$ scaled to $\\{-\\alpha, \\alpha\\}$.  b) A fully-connected attention module, where each point represents a time step ($w=6$). c) The Step-T attention module, where each past time point attends to itself and the latest time point $t$ attends to all past time points. d) An attention module with sparse Query (Q), Key (K), and Value (V) activations.  }\n\\label{teaser}\n\\end{figure}",
            "steptfig": "\\begin{figure}\n\\includegraphics[width=8cm]{figures/stept_mask.png}\n%\\includegraphics[width=8cm]{figures/stept2.png}\n\\caption{\\textbf{ Step-t Attention Mask}  Left: For the forecasting task we mask inputs during training in order to simulate unknown future time points. Right:  The Step-T attention mask used to calculate attention only at the current time-step versus past values.  Using this mask rather than setting our Query dimension to one enables us to pass time window vectors along multiple encoder layers.  }\\label{steptfig}\n\\end{figure}",
            "forecast_pic": "\\begin{figure}[t]\n%\\includegraphics[width=8cm]{figures/ts2.pdf}\n\\includegraphics[width=8.5cm]{figures/ts.pdf}\n\\caption{Time series predictions on the ETTm1 dataset for the Pyraformer (top) and Sparse Binary Transformer (bottom) .  We show 600 predictions across each model for two features (HULL, LUFL).    }\\label{forecast_pic}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\mathbf{M}_k=   \\begin{cases}\n    1 & \\text{if $|\\mathrm{S}_k| \\in \\{\\tau(k)_{k=1}^{l_m}\\ge[l_m p]\\}$}\\\\%, {|\\mathrm{S}^j_{\\tau(i)}|} \\le |\\mathrm{S}^j_{\\tau(i+1)}|$ } \\\\\n    0 & \\text{otherwise}\n  \\end{cases}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n \\alpha=\\frac{||\\mathbf{M}\\odot\\mathbf{W}||_1}{||\\mathbf{M}||_1}\n\\end{equation}"
        }
    }
}