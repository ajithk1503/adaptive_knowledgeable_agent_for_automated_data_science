{
    "meta_info": {
        "title": "Multiplex Heterogeneous Graph Convolutional Network",
        "abstract": "Heterogeneous graph convolutional networks have gained great popularity in\ntackling various network analytical tasks on heterogeneous network data,\nranging from link prediction to node classification. However, most existing\nworks ignore the relation heterogeneity with multiplex network between\nmulti-typed nodes and different importance of relations in meta-paths for node\nembedding, which can hardly capture the heterogeneous structure signals across\ndifferent relations. To tackle this challenge, this work proposes a Multiplex\nHeterogeneous Graph Convolutional Network (MHGCN) for heterogeneous network\nembedding. Our MHGCN can automatically learn the useful heterogeneous meta-path\ninteractions of different lengths in multiplex heterogeneous networks through\nmulti-layer convolution aggregation. Additionally, we effectively integrate\nboth multi-relation structural signals and attribute semantics into the learned\nnode embeddings with both unsupervised and semi-supervised learning paradigms.\nExtensive experiments on five real-world datasets with various network\nanalytical tasks demonstrate the significant superiority of MHGCN against\nstate-of-the-art embedding baselines in terms of all evaluation metrics.",
        "author": "Pengyang Yu, Chaofan Fu, Yanwei Yu, Chao Huang, Zhongying Zhao, Junyu Dong",
        "link": "http://arxiv.org/abs/2208.06129v1",
        "category": [
            "cs.SI",
            "cs.AI",
            "cs.LG"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nNetwork representation learning has emerged as a new learning paradigm to embed complex network into a low-dimensional vector space while preserving the proximities of nodes in both network topological structures and intrinsic properties. Effective network representation advances various network analytical tasks, ranging from link prediction~\\cite{DBLP:conf/kdd14Deepwalk,chen2018pme,liu2021motif}, node classification~\\cite{wu2019simplifying,liu2020fast,grover2016node2vec}, to recommendation~\\cite{shi2018heterogeneous,ji2021heterogeneous, 2021knowledge}. In recent years, Graph Convolutional Networks (GCNs)~\\cite{iclr17KipfGCN}, a class of neural networks designed to learn graph representation for complex networks with rich feature information, have been applied to many online services, such as E-commerce~\\cite{li2020hierarchical}, social media platforms~\\cite{wu2020graph} and advertising~\\cite{he2021click}.  \n\n% Early methods have made many efforts on representation learning for homogeneous networks with singular type of nodes~\\cite{tang2015line,grover2016node2vec,qiu2019netsmf,iclr17KipfGCN}. To capture the network heterogeneity properties, many subsequent researches propose to model heterogeneous graph structures based on predefined meta-paths, such as metapath2vec~\\cite{DongCS17metapath2vec}, HIN2Vec~\\cite{fu2017hin2vec}, and HERec~\\cite{shi2018heterogeneous}. To capture the rich neighborhood contextual signals, various Graph Neural Network (GNN) models have been proposed to aggregate feature information from neighbor nodes, such as graph attention networks (GAT)~\\cite{DBLP:conf/iclr18GAT}, relational graph convolutional networks (R-GCN)~\\cite{schlichtkrull2018modeling}, heterogeneous graph attention networks (HAN)~\\cite{www2019HAN}, and metapath aggregated graph neural network (MAGNN)~\\cite{fu2020magnn}. \n\nWhile many efforts have been made to study the representation learning over homogeneous graphs~\\cite{tang2015line,grover2016node2vec,qiu2019netsmf,iclr17KipfGCN}, the exploration of preserving network heterogeneous properties in graph representation paradigms has attracted much attention in recent studies, \\eg, metapath2vec~\\cite{dong2017metapath2vec} and HERec~\\cite{shi2018heterogeneous}. Inspired by the strength of Graph Neural Networks (GNNs) in aggregating contextual signals from neighboring nodes, various graph neural models have been introduced to tackle the challenge of heterogeneous graph learning, such as HAN~\\cite{www2019HAN}, MAGNN~\\cite{fu2020magnn} and HetGNN~\\cite{zhang2019heterogeneous}.\n\n\nAlbeit the effectiveness of existing heterogeneous network embedding methods~\\cite{ji2021heterogeneous,dong2017metapath2vec,lu2019relation,wang2020dynamic}, these works are generally designed for heterogeneous networks with a single view. In real-world scenarios, however, many networks are much more complex, comprising not only multi-typed nodes and diverse edges even between the same pair-wise nodes but also a rich set of attributes~\\cite{kdd19GATNE}. For example, in E-commerce networks, there are two types of nodes (\\ie, users and items), and multiple relations (\\eg, click, purchase, add-to-cart, or add-to-preference) between the same pairs of users and items~\\cite{2020multiplex}. The connections between multiple types of nodes in such networks are often heterogeneous with relation diversity, which yields networks with multiple different views. It is worth noting that the multiplicity of the network is fundamentally different from the heterogeneity of the network. \nTwo types of nodes, users and items, in a E-commerce network reflect the heterogeneity of the network. \n% Because user and item are two completely different nodes, each with specific attribute information. These two nodes cannot be effectively represented by the same processing. \nAt the same time, users may have several types of interactions (\\eg, click, purchse, review) with items~\\cite{wei2022contrastive}, which reflects the multiplex relationships of the network. Because different user-item interactions exhibit different views of user and item, and thus should be treated differently. We term this kind of networks with both multiplex network structures with multi-typed nodes and node attribute information as \\textbf{\\textit{attributed multiplex heterogeneous networks}} (AMHENs). \n\nPerforming representation learning on the AMHENs is of great importance to network mining tasks, yet it is very challenging due to such complicated network structures and node attributes. While some recent studies propose to solve the representation learning problem on multiplex heterogeneous network~\\cite{kdd19GATNE,nips19GTN,park2019DMGI,liu2020fast,xue2021multiplex}, several key limitations exist in those methods. i) The success of current representation learning models largely relies on the accurate design of meta-paths. How to design an automated learning framework to explore the complex meta-path-based dependencies over the multiplex heterogeneous graphs, remains a significant challenge. ii) Unlike the homogeneous node aggregation scheme, with the heterogeneous node types and multiplex node relationships, each meta-path can be regarded as relational information channel. An effective meta-path dependency encoder is a necessity to inject both the relation heterogeneity and multiplexity into the representations. iii) In real-world graph representation scenarios, efficiency is an important factor to handle the graph data with large number of heterogeneous nodes and multiplex edges. However, most current methods are limited to serve the large-scale network data, due to their high time complexity and memory consumption.\n\n\n% While a handful of studies attempt to learn node embeddings on multiplex heterogeneous network~\\cite{kdd19GATNE,nips19GTN,park2019DMGI,liu2020fast,xue2021multiplex}, these methods still have three key limitations. \n% \\textit{First}, most existing heterogeneous network embedding approaches require manual selection of meta-paths for different networks. Heterogeneous multi-typed nodes and relations raise the challenge of automatically capturing various useful meta-path interactions among nodes across multi-relations and high-order network structures in a unified embedding framework. \n\n% \\textit{Second}, all the previous meta-path based network embedding models ignore the importance of different relations in meta-paths for heterogeneous networks. It should be noted that different types of relations have different effects on node representations, which is more obviously in multiplex networks. \n% , the effects of different edges between the same node pairs are significantly different for different downstream network mining tasks.\n\n% However, this effect can not be achieved by defining several meta-paths and meta-path sampling. \n\n% \\textit{Last}, all existing learning methods have high time complexity and memory consumption, which are inefficient and thus can hardly serve the large-scale network data. \n\n% For example, GTN~\\cite{nips19GTN} needs explicit products of all candidate adjacency matrices of sub-networks and thus requires huge memory space and computing resources. Therefore, an efficient model with stronger joint representation learning capability of heterogeneous multiplex relations and contextual node features for large-scale network data, is urgently needed.  \n\n\n% It is worth noting that networks in real life are often presented in the form of heterogeneous networks, usually with a variety of objects and relationships. In particular, there are different relationships between the same objects, that is, multiplex heterogeneous networks. There are still many difficulties in the representation learning of multiplex heterogeneous networks: \n\n% Now there are some works that can solve multi-layer networks. However, most of them, such as PMNE~\\cite{liu2017PMNE}, DMGI~\\cite{park2019DMGI}, GATNE~\\cite{kdd19GATNE}, HGSL~\\cite{zhao2021heterogeneous}, need to specify meta paths artificially. In multiplex heterogeneous networks, meta paths will become more complex, and different tasks depend on meta path categories differently. Therefore, selecting the appropriate meta path is a complex problem. GTN~\\cite{nips19GTN} can automatically capture meta path information, but the calculation method of GTN determines that it needs a lot of resource overhead and is easy to fall into local optimization, which affects the embedding performance. In addition, we also note the complex relationships of objects in multi-layer heterogeneous networks. These relationships have different effects on embedding. For this, FAME~\\cite{liu2020fast} distinguishes the effects of different relationships by setting hyperparameters, but it can not dynamically adjust the weight of each relationship. \n\n\nTo address the aforementioned challenges, we propose a new \\underline{\\textbf{M}}ultiplex  \\underline{\\textbf{H}}eterogeneous  \\underline{\\textbf{G}}raph \\underline{\\textbf{C}}onvolutional  \\underline{\\textbf{N}}etwork, named \\textbf{\\system}, for AMHEN embedding. \nSpecifically, we first decouple the multiplex network into multiple homogeneous and bipartite sub-networks, and then re-aggregate the sub-networks with the exploration of their importance (\\ie, weights) in node representation learning. \n% In particular, we do not determine the weight in advance. We believe that the size of the weight should be dynamically adjusted according to the task, so we let the weight participate in the training together with the model. \nTo automatically capture meta-path information across multi-relations, we tactfully design a multilayer graph convolution module, which can effectively learn the useful heterogeneous meta-path interactions of different lengths in AMHENs through multilayer convolution aggregation in both unsupervised and semi-supervised learning paradigms. To improve the model efficiency, we endow our \\system with a simplified graph convolution for feature aggregation, in order to significantly reduce the model computational cost. \n% Additionally, to reduce the computation costs, we propose to endow our model with the efficient feature aggregation capability via simplifying graph convolution. \nOur evaluations are conducted on several real-world graph datasets to evaluate the model performance in both link prediction and node classification tasks. Experimental results show that our \\system framework can obtain the substantial performance improvement compared with state-of-the-art graph representation techniques. With the designed graph convolution module, our \\system achieves better model efficiency when competing with state-of-the-art GNN baselines for AMHENs by up to two orders of magnitudes (see efficiency analysis in the supplemental material). .\n\n% The resulting model significantly outperforms several strong baselines (5\\% F1 gain and 12\\% Macro-F1 gain average) on both link prediction and node classification tasks. \n\n% We also found that with the new designed graph convolution module, our \\system performs significantly faster than the state-of-the-art GNN baselines for AMHENs by up to two orders of magnitudes (see model efficiency analysis in the supplemental material). \n\n% We highlight the key contributions of this work as follows:\nWe summarize the contributions of this paper as follows:\n\\begin{itemize}\n\n\\item We propose an effective multiplex heterogeneous graph neural network, \\system, which can automatically capture the useful relation-aware topological structural signals between nodes for heterogeneous network embedding. \\\\\\vspace{-0.05in}\n\n\\item \\system integrates both network structures and node attribute features in node representations, and gains the capability to efficiently learn network representation with a simplified convolution-based message passing mechanism. \\\\\\vspace{-0.05in}\n\n\\item We conduct extensive experiments on five real-world datasets to verify the superiority of our proposed model in both link prediction and node classification when competing with state-of-the-art baselines.  \n\n\\end{itemize}\n\n\n\n\n\n% \\vspace{-1mm}\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n% \\subsection{Graph Neural Networks}\n\\textbf{Graph Neural Networks.} \nThe goal of a GNN is to learn a low-dimensional vector representation for each node, which can be used for many downstream network mining tasks. Kipf~\\etal~\\cite{iclr17KipfGCN} proposes to perform convolutional operations over graph neighboring node for information aggregation.\n% GCN~\\cite{iclr17KipfGCN} aggregates neighbors' features into the center node feature using convolutional operations.\nGraphSAGE~\\cite{William2017GraphSAGE} is an inductive GNN framework, which uses the general aggregating functions for efficient generation of node embeddings. %The aggregating function samples, extracts, and transforms a target node’s local neighborhood, and thus facilitates parallel training and generalization to unseen nodes in graphs.\nTo differentiate the influence of neighboring nodes, GAT~\\cite{DBLP:conf/iclr18GAT} has been proposed as an attentive message passing mechanism to learn the explicit weights of neighbor node embeddings. R-GCN~\\cite{schlichtkrull2018modeling} considers the influence of different edge types on nodes, and uses weight sharing and coefficient constraints to apply to multi-graphs with large numbers of relations. To simplify the design of graph convolutional network, LightGCN~\\cite{he2020lightgcn} omits the embedding projection with non-linearity during the message passing. Additionally, AM-GCN~\\cite{wang2020gcn} is proposed to adaptively learn deep correlation information between topological structures and node features. However, all algorithms mentioned above are developed for the homogeneous networks, and thus cannot effectively preserve the heterogeneous and multiplex graph characteristics for the network representation task. \\\\\\vspace{-0.1in}\n\n% GAT~\\cite{DBLP:conf/iclr18GAT} incorporates the attention mechanism into the aggregator function to capture the importance of neighbors to the center node more effectively. \n\n% R-GCN~\\cite{schlichtkrull2018modeling} considers the influence of different edge types on nodes, and uses weight sharing and coefficient constraints to apply to multi-graphs with large numbers of relations. \n% GGNN~\\cite{49081} adds a gated recurrent unit (GRU)~\\cite{Cho14Gru} to the aggregator function by treating the aggregated neighborhood information as the input to the GRU of the current time step. GaAN~\\cite{zhang2018gaan} combines GRU with the gated multi-head attention mechanism for dealing with spatiotemporal graphs. STAR-GCN~\\cite{zhang2019star} stacks multiple GCN encoder-decoders to boost the rating prediction performance.\n% SGC~\\cite{wu2019simplifying} is a simplified version of GCN, which removes nonlinear transformation and training parameters, that is, it only uses the product of high-order adjacency matrices and attribute matrix.\n\n% \\subsection{Heterogeneous Network Embedding}\n\\noindent \\textbf{Heterogeneous Graph Representation.}\nModeling the heterogeneous context of graphs has already received some attention~\\cite{dong2017metapath2vec,shi2018heterogeneous,zhang2019heterogeneous,2021recent,long2021social}. For example, some studies leverage random walks to construct meta-paths over the heterogeneous graph for node embeddings, including metapath2vec~\\cite{dong2017metapath2vec} and HERec~\\cite{shi2018heterogeneous}. As graph neural networks (GNNs) have become a popular choice for encoding graph structures, many heterogeneous graph neural network models are designed to enhance the GNN architecture with the capability of capturing the node and edge heterogeneous contextual signals. For example, HetGNN~\\cite{zhang2019heterogeneous} jointly encodes the graph topology and context heterogeneity for representation learning. \nHeGAN~\\cite{kdd19HeGAN} incorporates generative adversarial networks (GAN) for heterogeneous network embedding. NARS~\\cite{yu2020scalable} first generates relation subgraphs, learns node embeddings by 1D convolution on the subgraphs and then aggregates the learned embeddings. Fu~\\etal~\\cite{fu2020magnn} performs both the intra- and inter-metapath aggregation so as to distill the metapath-based relational context for learning node representations. However, most of those approaches rely on selecting the useful metapaths to guide the process of heterogeneous representation, which may need the external domain knowledge for constructing relevant metapaths.\n\nIn addition, there exist some recent studies attempting to relax the requirement of metapath construction for heterogeneous graph representations. In particular, HGT~\\cite{hu2020heterogeneous} proposes to incorporate the self-attention into the graph-based message passing mechanism for modeling the dynamic dependencies among heterogeneous nodes. HPN~\\cite{ji2021heterogeneous} eliminates semantic confusion by mapping nodes in meta-path to semantic space, and then aggregates the embeddings of nodes under different metapaths to obtain the final representation. \n% HGSL~\\cite{zhao2021heterogeneous} first obtains the node representation based on meta-paths, and then uses GNN to jointly train the heterogeneous graph, node representation and node attributes to obtain the final embedding. \nHowever, most of the above heterogeneous graph embedding models ignore the multiplex relational context of real-life graph data, in which multi-typed relationships exist among nodes. \n% However, all of these heterogeneous graph embedding methods have the limitations of either utilizing only some simple metapaths, ignoring the importance of relations, or failing to consider the multiplex relations. \n\n\n% Most heterogeneous network embedding focuses on preserving the meta-path based structural information. \n\n% metapath2vec~\\cite{dong2017metapath2vec} first uses meta-path based random walks to construct the heterogeneous neighborhood of nodes and then leverages a heterogeneous skip-gram model to learn node embeddings. \n\n% HERec~\\cite{shi2018heterogeneous} also uses meta-path based random walk sampling to generate meaningful node sequences to learn network embeddings. %, which are first transformed by a set of fusion functions and subsequently integrated into an extended matrix factorization (MF) model. \n\n% HetGNN~\\cite{zhang2019heterogeneous} and HeGAN~\\cite{kdd19HeGAN} incorporate bi-directional LSTM and attention mechanism, and generative adversarial networks (GAN) for heterogeneous network embedding, respectively. \n% HetGNN~\\cite{DBLP:conf/kdd19_HetGNN} also samples heterogeneous neighbors using random walk and uses bi-directional LSTM and attention mechanism for node embedding. HeGAN~\\cite{kdd19HeGAN} uses generative adversarial networks (GAN) to train the relation-aware discriminator and generator to fit the heterogeneous networks.\n\n% HAN~\\cite{www2019HAN} extends GAT to heterogeneous networks through meta-path based neighbor discovery strategy and hierarchical attention mechanism. \n\n% Recently, MAGNN~\\cite{fu2020magnn} applies node content transformation, intra-metapath aggregation, and inter-metapath aggregation to generate node embeddings for heterogeneous graphs.\n\n% These methods rely on domain knowledge to choose the valuable meta-paths, whereas there also exist several methods~\\cite{DBLP:conf/aaai19HANE_GCN,DBLP:conf/sdm/18AspEm} which do not require meta-path selection. \n% However, \\cite{DBLP:conf/sdm/18AspEm} still needs additional supervision to choice of meta-paths. \n% HANE~\\cite{DBLP:conf/aaai19HANE_GCN} transforms various types of nodes with different attributes into a uniform space, which cannot distinguish the diversity of edges between nodes.\n\n\n% \\subsection{Multiplex Heterogeneous Network Embedding.} \n\\noindent \\textbf{Multiplex Heterogeneous Network Embedding.}  \nReal-world graphs are often inherently multiplex, which involves various relations and interactions between two connected nodes. To tackle this challenge, many multiplex network embedding techniques are proposed to project diverse node edges into latent representations. For example, MNE~\\cite{zhang2018scalable} introduces a global transformation matrix for each layer of the network to align the embeddings with different dimensions for each relation type. GATNE~\\cite{kdd19GATNE} splits the node representation by learning base embedding, edge embedding as well as attribute embedding. The self-attention is utilized to fuse neighborhood information for generating edge representation. Motivated by the mutual information maximization scheme, DMGI~\\cite{park2019DMGI} is proposed as an unsupervised learning approach which aims to minimize the difference among relation-aware node representations. HGSL~\\cite{zhao2021heterogeneous} first obtains the node representation based on meta-paths, and then uses GNN to jointly train the heterogeneous graph, node representation and node attributes to obtain the final embedding. However, the generality of the above methods is limited by their manual construction of meta-paths. \n%FAME~\\cite{liu2020fast} proposes to utilize the random projection to \n% Multiplex networks are much richer than simple heterogeneous networks and often used to model complex interaction systems. \n% Most of the existing approaches usually only capture a single view of a heterogeneous network, whereas there are usually multiple types of relations between nodes (\\ie, multiplex heterogeneous networks), yielding networks with multiple views. \n% PMNE~\\cite{liu2017PMNE} proposes three aggregation models to obtain one overall node embedding for multiplex networks. \n% However, it cannot capture long-distance meta-path information between nodes and ignores the rich content information of the nodes. \n\n% GATNE~\\cite{kdd19GATNE} first learns base embedding, edge embedding and attribute embedding using random walk sampling on each edge type and aggregation of neighborhood, and then uses them to generate the overall node embeddings. \n% The base embedding and attribute embedding are shared among edges of different types, while the edge embedding is computed by aggregation of neighborhood information with the self-attention mechanism. \n% DMGI~\\cite{park2019DMGI} jointly integrates the node embedding from multiple graphs by maximizes the mutual information between local patches of each relation of the multiplex graph and the global representation of the relation. \n% DMGI~\\cite{park2019DMGI} learns a node encoder that maximizes the mutual information between local patches of each relation of the heterogeneous graph and the global representation of the relation. %\n% RHINE~\\cite{DBLP:conf/aaai19_RHINE} presents two structure-related measures that distinguish heterogeneous relations into affiliation relations and interaction relations. Then, it individually handles these two relations to learn network representation. \n% HGSL~\\cite{zhao2021heterogeneous} first obtains the node representation based on meta-paths, and then uses GNN to jointly train the heterogeneous graph, node representation and node attributes to obtain the final embedding. \n% However, these methods still need to specify the meta-path type manually. \n\nRecently, FAME~\\cite{liu2020fast} develops a spectral graph transformation component to aggregate information from sub-networks by preserving relation-aware node dependencies. However, this model is built on the random projection and sacrifices the adaptive parameter learning in exchange for fast embedding projection. Furthermore, to learn the node embeddings of multiplex bipartite graph, DualHGCN~\\cite{xue2021multiplex} firstly generates two sets of homogeneous hypergraphs and then perform the information propagation with the spectral hypergraph convolutions. In HDI~\\cite{jing2021hdmi}, Jing~\\etal~explores the high-order mutual information to construct the supervision signals for enhancing the node representations.\n\n% GTN~\\cite{nips19GTN} explicitly explores the weighted product of the adjacency matrices of all sub-networks to obtain useful meta-paths, and then learns node embeddings through a multi-layer graph convolution. \n\n% Recently, FAME~\\cite{liu2020fast} proposes using spectral graph transformation with a group of hyperparameters to capture meta-paths for AMHENs, and significantly improves efficiency through random projection. \n% GTN~\\cite{nips19GTN} computes the convex combinations of adjacency matrices of sub-networks with different weights to obtain candidate adjacency matrices to generate the useful meta-paths. \n% Nevertheless, GTN needs explicit products of all candidate adjacency matrices, thus the parameter search space of GTN is very large, which requires huge memory space and computing resources, and is easily trapped into local optimization. \n% More recently, DualHGCN~\\cite{xue2021multiplex} transforms the multiplex bipartite network into two sets of homogeneous hypergraphs, and uses dual hypergraph convolutional network to learn node embeddings. %for multiplex bipartite networks. \n\n\n\n\\vspace{-0mm}\n"
            },
            "section 3": {
                "name": "Problem Definition",
                "content": "\n\\label{sec.prob}\n% In this section, we first introduce key concepts used in this paper and then formally define the studied problem. \n\n% Let $\\mathcal{G}=\\{\\mathcal{V}, \\mathcal{E}\\}$ denote a network, where $\\mathcal{V}$ is the collection of nodes, and $\\mathcal{E}$ is the collection of edges between the nodes, each representing a relationship between two nodes.\n\nWe define graph $\\mathcal{G}=\\{\\mathcal{V}, \\mathcal{E}\\}$ with the set of nodes $\\mathcal{V}$ and edges $\\mathcal{E}$. Each edge in $\\mathcal{E}$ represents the connections among nodes.\n\n%Each edge $e_{ij} \\in \\mathcal{E}$ is an ordered pair $e_{ij}= (v_i, v_j)$ and is associated with a weight $w_{ij} > 0$, which indicates the strength of the relation. %If $\\mathcal{G}$ is undirected, we have $e_{ij}\\equiv e_{ji}$ and $w_{ij}\\equiv w_{ji}$; if $\\mathcal{G}$ is directed, we have $e_{ij}\\ne e_{ji}$ and $w_{ij}\\ne w_{ji}$. \n\n% \\begin{Def}[Attributed Network]\n% An attributed network is a network $\\mathcal{G}$ endowed with an attribute feature matrix, \\ie, $\\mathcal{G}=\\{\\mathcal{V}, \\mathcal{E}, \\mathbf{X}\\}$. $\\mathbf{X} \\in \\mathbb{R}^{n\\times m}$ is the matrix that consists of node attribute features for all nodes, where each row is the associated node feature vector of node $v_i$. Here, $n$ and $m$ denote the number of nodes and attributes, respectively. \n% \\end{Def} \n\n% \\begin{Def}[Heterogeneous Network, or HEN]\n% A heterogeneous network is a network $\\mathcal{G}=\\{\\mathcal{V}, \\mathcal{E}\\}$ associated with a node type mapping function $\\phi : \\mathcal{V} \\to \\mathcal{O}$ and an edge type mapping function $\\psi : \\mathcal{E} \\to \\mathcal{R}$, where $\\mathcal{O}$ and $\\mathcal{R}$ represent the set of all node types and the set of all edge types, respectively. Each node $v\\in \\mathcal{V}$ belongs to a particular node type, and each edge $e\\in \\mathcal{E}$ is categorized into a specific edge type.  \n% If $|\\mathcal{O}| + |\\mathcal{R}| > 2$, the network is \\textit{\\textbf{heterogeneous}}; otherwise \\textit{\\textbf{homogeneous}}.\n% \\end{Def} \n\n\\begin{Def}[Attributed Multiplex Heterogeneous Network, or AMHEN]\nGiven the defined graph $\\mathcal{G}$, we further associate all nodes in $\\mathcal{V}$ with the attribute feature vectors $\\mathbf{X} \\in \\mathbb{R}^{n\\times m}$. Here, the size of node set $\\mathcal{V}$ and attribute vector is represented by $n$ and $m$, respectively. With the consideration of node and edge heterogeneity, we define the node type and edge type mapping function as $\\phi : \\mathcal{V} \\to \\mathcal{O}$ and $\\psi : \\mathcal{E} \\to \\mathcal{R}$. Here, the set of node types and edge types is set with the size of $\\mathcal{O}$ and $\\mathcal{R}$, respectively. Each node $v\\in \\mathcal{V}$ and edge $e\\in \\mathcal{E}$ are associated with a specific type in $\\mathcal{O}$ and $\\mathcal{R}$, respectively. Additionally, with the consideration of edge multiplexity (\\ie, $|\\mathcal{O}| + |\\mathcal{R}| > 2$), the same pair of nodes can be connected through multi-typed edges.\n% An attributed heterogeneous network is a network $\\mathcal{G}=\\{\\mathcal{V}, \\mathcal{E}, \\mathbf{X}\\}$ associated with a node type mapping function $\\phi : \\mathcal{V} \\to \\mathcal{O}$ and an edge type mapping function $\\psi : \\mathcal{E} \\to \\mathcal{R}$, where $\\mathcal{O}$ and $\\mathcal{R}$ represent the set of all node types and the set of all edge types, respectively. Each node $v\\in \\mathcal{V}$ belongs to a particular node type, and each edge $e\\in \\mathcal{E}$ is categorized into a specific edge type. If $|\\mathcal{O}| + |\\mathcal{R}| > 2$, the network is \\textit{\\textbf{heterogeneous}}; If $|\\mathcal{O}| + |\\mathcal{R}| > 2$ and there exist different types of edges between same node pairs, the network is \\textit{\\textbf{attributed multiplex heterogeneous}}.\n\\end{Def} \n\n\\begin{Def}[Meta-path]\nA meta-path $\\mathcal{P}$ is defined as a path in the form of $O_1 \\xrightarrow{r_1} O_2 \\xrightarrow{r_2} \\cdots \\xrightarrow{r_{l-1}} O_l$ which describes a composite relation $R = r_1 \\circ r_2 \\cdots r_{l-1}$ between node types $O_1$ and $O_l$, where $\\circ$ denotes the composition operator on relations. \n\\end{Def} \n\n% \\begin{Def}[Meta-path] \n% \\textcolor{red}{A meta-path $\\mathcal{P}$ is defined as a path in the form of $\\mathbf{O}_1 \\xrightarrow{\\mathbf{R}_1} \\mathbf{O}_2 \\xrightarrow{\\mathbf{R}_2} \\cdots \\xrightarrow{\\mathbf{R}_{l-1}} \\mathbf{O}_l$ which describes a composite relation $\\mathbf{R} = \\mathbf{R}_1 \\circ \\mathbf{R}_2 \\cdots \\mathbf{R}_{l-1}$ between node types $\\mathbf{O}_1$ and $\\mathbf{O}_l$, where $\\circ$ denotes the composition operator on relations.}\n% \\end{Def} \n\n% \\begin{Def}[Attributed Multiplex Heterogeneous Network, or AMHEN]\n% An attributed heterogeneous network is a network $\\mathcal{G}=\\{\\mathcal{V}, \\mathcal{E}, \\mathbf{X}\\}=\\{\\mathcal{G}_1, \\mathcal{G}_2, \\dots, \\mathcal{G_{|R|}}\\}$, \n% where $\\mathcal{G}_r= \\{\\mathcal{V},  \\mathcal{E}_{r},\\mathbf{X}\\}$ is the graph that contains all edges with edge type $r \\in \\mathcal{R}$, $\\mathcal{E} = \\bigcup_{r\\in \\mathcal{R}} \\mathcal{E}_{r}$, and $\\mathcal{V} = \\bigcup_{o\\in \\mathcal{O}} \\mathcal{V}_{o}$ where $\\mathcal{V}_{o}$ is the collection of $o$-type nodes. \n% If $|\\mathcal{O}| + |\\mathcal{R}| > 2$ and there exist different types of edges between same node pairs, the network is \\textit{\\textbf{attributed multiplex heterogeneous}}.\n% \\end{Def} \n\n% More specifically, in a multiplex heterogeneous network, there may be multiple types of edges between node $v_i$ and $v_j$. Hence, an edge is denoted as $e_{ij}^{r}$ in AMHENs, where $r$ corresponds to a certain edge type. \nFor example, $U_1 \\xrightarrow{click} I_2 \\xrightarrow{buy} U_2$ is a meta-path sample of meta-path $User \\xrightarrow{click} Item \\xrightarrow{buy} User$. Based on the above definitions, we formally present the representation learning task over the multiplex heterogeneous graph as follows:\n% Given the above definitions, we next formally define our studied problem for representation learning on multiplex heterogeneous networks.\n\\begin{Problem}[Attributed Multiplex Heterogeneous Graph Representation]\nThe objective of our representation learning task over the attributed multiplex heterogeneous graph $\\mathcal{G}=\\{ \\mathcal{V}, \\mathcal{E}, \\mathbf{X}\\}$ is to learn low-dimensional latent embedding (with the hidden dimensionality of $d$ $d\\ll|\\mathcal{V}|$) for each node $v\\in \\mathcal{V}$, with the preservation of node and edge heterogeneity and multiplexity.\n% Given an attributed multiplex heterogeneous network $\\mathcal{G}=\\{ \\mathcal{V}, \\mathcal{E}, \\mathbf{X}\\}$, the problem of Attributed Multiplex Heterogeneous Network Embedding is to learn a $d$-dimensional vector representation for each node $v_i \\in \\mathcal{V}$ that reserves rich structural and attribute features involved in $\\mathcal{G}$, %\\ie, learn a mapping function $f: \\mathcal{V} \\rightarrow \\mathbb{R}^{d}$, \n% where $d\\ll|\\mathcal{V}|$.\n\\end{Problem}\n\nWe summarize the key notations of our technical solution in Table~\\ref{table_notations} presented in the supplementary material.\n% Key notations are  summarized in Table~\\ref{table_notations} in the supplement. \n\n% \\input{tables/Notations.tex}\n\n\n\n\n"
            },
            "section 4": {
                "name": "Methodology",
                "content": "\nThis section describe our framework \\system with the overall architecture shown in Figure~\\ref{fig:framework}. Particularly, our \\system contains two key learning modules: (i) \\textit{multiplex relation aggregation} and (ii) \\textit{multilayer graph convolution module}. \\textit{Multiplex relation aggregation} aims to aggregate the multi-relations among heterogeneous nodes in multiplex heterogeneous networks by differentiating each relation with importance. \n\\textit{Multilayer graph convolution module} can automatically capture the heterogeneous meta-paths of different lengths across multi-relations by aggregating neighboring nodes' characteristics to learn the low-dimensional representation of nodes.\n\n% In this section, we present the details of our \\system (as shown in Figure~\\ref{fig:framework}), consisting of two key components: (i) \\textit{multiplex relation aggregation} and (ii) \\textit{multilayer graph convolution module}. \\textit{Multiplex relation aggregation} aims to aggregate the multi-relations among heterogeneous nodes in multiplex heterogeneous networks by differentiating each relation with importance. \n% \\textit{Multilayer graph convolution module} can automatically capture the heterogeneous meta-paths of different lengths across multi-relations by aggregating neighboring nodes' characteristics to learn the low-dimensional representation of nodes. \n\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "Multiplex Relation Aggregation",
                    "content": "\n\nAs defined in Sec.~\\ref{sec.prob}, there exit different types of nodes and multiple types of edges between these nodes in AMHENs, and each type of edge has a different role and impact on node representation. Therefore, following~\\cite{liu2020fast}, we first generate multiple sub-graphs by differentiating the types of edge connections between nodes in the multiplex and heterogeneous graph. Afterwards, we aggregate the relation-aware graph contextual information with different importance weights.\n\n% we first decouple an AMHEN into multiple homogeneous and bipartite sub-networks to differentiate each relationship (\\ie, each type of edges) between nodes in the heterogeneous network. \n% Afterwards, we perform a weighted sum operation to aggregate multiplex relations with different importance. \n\n% Since AMHENs involve different types of nodes and relationships among these nodes (each relationship having a different role and impact on node representation), \n% we first decouple an AMHEN into multiple homogeneous and bipartite sub-networks (the latter involving two types of nodes) to differentiate each relationship between nodes in the network. \n% After that, we perform spectral graph transformation on these sub-networks to better capture the proximities between nodes. \n\nWe denote our generated sub-graph as $\\{\\mathcal{G}_r|r=1,2,\\dots,|\\mathcal{R}|\\}$ with the corresponding adjacent matrix $\\{\\mathbf{A}_r|r = 1,2,\\dots,|\\mathcal{R}|\\}$. Considering the scenario of multiplex user-item relations in online retailer (\\eg, click, purchase, review), the decomposed sub-graph corresponds to individual type of relationship between user and item.\n% Let $\\{\\mathcal{G}_r|r=1,2,\\dots,|\\mathcal{R}|\\}$ be the collection of obtained homogeneous and bipartite sub-networks and $\\{\\mathbf{A}_r|r = 1,2,\\dots,|\\mathcal{R}|\\}$ are the adjacency matrices corresponding to $\\{\\mathcal{G}_r\\}$. \n% Taking an E-commerce network as example (as shown in Figure~\\ref{fig:framework}), it contains four edge types (\\ie, click, buy, add-to-cart and add-to-preference) between two node types (\\ie, user and item). In this case, we decouple the E-commerce network into four bipartite sub-networks. \n% Notice that we extend the adjacency matrix $\\mathbf{A}_r$ of sub-network $\\mathcal{G}_r$ to include all nodes in the original network to fit the dimensions of all adjacency matrices. \nFor instance, for the graph representation learning in E-commerce platforms, different relationships (different edge types) between user and item nodes exhibit various dependency semantics. For example, the diverse behaviors of users (\\eg, click, add-to-favorite, purchase) reflect different preferences of users over items. Hence, multiplex user-item interactions with various relation semantics will have different impacts on the learning process of user representations. To capture such multi-typed node dependencies, our proposed \\system learns the relation-aware weights $\\beta_r$ to aggregate edge-type-specific sub-graph adjacent matrix as: $\\mathbb{A} = \\sum_{r=1}^{|\\mathcal{R}|} \\beta_r \\mathbf{A}_r$.\n% has different effect on node embedding, especially for different mining tasks. For example, to predict a user’s demand for a certain product, the behaviors of clicking, buying, add-to-preference, and add-to-cart all have an impact on this prediction, but their impact is definitely different. \n% Considering this factor, we propose to aggregate sub-networks with different weights to obtain a combined adjacency matrix:\n% \\begin{equation}\n% \\label{sum-up-adjacency_matrix}\n%     \\mathbb{A} = \\sum_{r=1}^{|\\mathcal{R}|} \\beta_r \\mathbf{A}_r,\n% \\end{equation}\n% where the weight $\\beta_r$ of the adjacency matrix $\\mathbf{A}_r$ indicates the importance of the corresponding edge type in the network.\nNotice that the set of weights $\\{\\beta_r|r=1,2,\\dots,|\\mathcal{R}|\\}$ should not be a set of hyperparameter, but should be dynamically changed according to different tasks, so we set them as trainable parameters to be learned in model training.\n\n"
                },
                "subsection 4.2": {
                    "name": "Multilayer Graph Convolution Module",
                    "content": "\nDifferent from homogeneous networks, heterogeneous networks contain different types of nodes and edges. The specified types of edges and nodes form a meta-path, which has an obvious effect on the representation learning of heterogeneous networks. Previous works require manually defined meta-paths and learn node representations on the sampled heterogeneous meta-paths. However, setting and sampling meta-paths artificially is a complex task. In a large-scale network, the number of meta-paths is very large. It takes a long time to sample such a large number of meta-paths. At the same time, aggregating meta-paths into meta-path graph also requires a lot of memory overhead. \nAdditionally, the type of meta-paths has an important impact on node representation, which almost determines the performance of network embedding in various downstream tasks. The number of types of heterogeneous meta-paths is also very large, involving different lengths and different relation interactions. Therefore, it is difficult to select the appropriate meta-path types for heterogeneous network embedding methods based on meta-path aggregation. \nOur \\system effectively solves the above problems. We now present our multilayer graph convolution module that automatically captures the the short and long meta-paths across multi-relations in AMHENs. \n\n\n% We first describe how our method automatically captures meta paths. As shown in the Figure~\\ref{fig:example}, we can learn the short and long meta-paths across multi-relations (including the high-order structure of the original sub network) through the spectral graph transformation. \n% Therefore, we propose a multi-layer fusion GCN to capture the short and long meta-paths across multi-relations: \n% \\begin{equation}\n% \\label{Multilayer fusion GCN}\n% \\begin{split}\n%     \\mathbf{H} &= \\frac{1}{l}\\sum_{i=1}^{l}\\mathbf{H}^{(i)}\\\\\n%     &=\\frac{1}{l} \\sum_{i=1}^{l}\\mathbb{A} \\cdot \\mathbf{H}^{(i-1)} \\cdot \\mathbf{W}^{(i)}.\n% \\end{split}\n% \\end{equation}\n% Where $\\mathbf{H}^{(i-1)}$ is the output of GCN layer \\emph{i}, $\\mathbf{H}^{(0)}$ is the node attribute matrix $\\mathbf{X}$, and $\\mathbf{W}$ is the weight matrix of GCN layer \\emph{i}.\n\nIt is worth noting that our model uses a multi-layer fusion GCN. As shown in Figure~\\ref{fig:framework}, our graph convolution module consists of multiple graph convolutional layers. Its purpose is to capture meta-path information of different lengths. \nNext, we take a two-layer GCN as an example to illustrate how our model capture meta-path information. For a single layer GCN:\n\\begin{equation}\n\\label{Single layer GCN}\n    \\mathbf{H}^{(1)} = \\mathbb{A} \\cdot \\mathbf{X} \\cdot \\mathbf{W}^{(1)},\n\\end{equation}\nwhere $\\mathbf{H}^{(1)}\\in \\mathbb{R}^{n\\times d}$ is the output of first layer (\\ie, hidden representation of network), $\\mathbf{X}\\in \\mathbb{R}^{n\\times m}$ is the node attribute matrix, and $\\mathbf{W}^{(1)}\\in \\mathbb{R}^{m\\times d}$ is the learnable weight matrix. \nNotice that our convolution adopts the idea of simplifying GCN~\\cite{wu2019simplifying}, that is, no non-linear activation function is used. \n% In the single-layer GCN, aggregated matrix $\\mathbb{A}$ can be regarded as a meta-path graph matrix generated by the 1-length meta-paths (\\ie, all linked node pairs across all edge types). \n\nFor the two-layer GCN, the message passing process can be represented as below:\n\\begin{equation}\n\\label{Double layer GCN}\n\\begin{split}\n    \\mathbf{H}^{(2)} &= \\mathbb{A} \\cdot \\mathbf{H}^{(1)} \\cdot \\mathbf{W}^{(2)}\\\\\n    &= \\mathbb{A} \\cdot (\\mathbb{A} \\cdot \\mathbf{X} \\cdot \\mathbf{W}^{(1)}) \\cdot \\mathbf{W}^{(2)}\\\\\n    &= \\mathbb{A}^{2} \\cdot \\mathbf{X} \\cdot \\mathbf{W}^{(1)}\\cdot \\mathbf{W}^{(2)},\n\\end{split}\n\\end{equation}\nwhere $\\mathbf{W}^{(2)}\\in \\mathbb{R}^{d\\times d}$ is the learnable weight matrix for second layer.  \n\n% A toy example of E-commerce network is illustrated in Figure~\\ref{fig:example}, we only consider two relations (\\ie, buy and click) between user and item nodes in this case. \n\nWe present an illustrated example with a graph generated from E-commerce data in Figure~\\ref{fig:example} based on two types of node relations, namely users' buy and click behaviors on items. As shown in Figure~\\ref{fig:example}, aggregated matrix $\\mathbb{A}$ can be regarded as a meta-path graph matrix generated by the 1-length meta-paths with importance (\\ie, all linked node pairs across all edge types with weights). For example, $\\mathbb{A}_{(1,3)}=1.5$ contains two 1-length meta-path samples with weights, \\ie, $U_1 \\xrightarrow{1*buy} I_1: 1$ and $U_1 \\xrightarrow{0.5*click} I_1: 0.5$. Therefore, the single-layer GCN can effectively learn the node representation that contains 1-length meta-path information. \nSimilarly, the second power of $\\mathbb{A}$ automatically captures the 2-length meta-path information with importance weights for all node pairs, including original sub-network high-order structures. For example, $\\mathbb{A}^2_{(1,1)}=2.5$ implies five 2-length meta-path samples across multi-relations with importance, \\ie, $U_1 \\xrightarrow{1*buy} I_1 \\xrightarrow{1*buy} U_1: 1$, $U_1 \\xrightarrow{1*buy} I_1 \\xrightarrow{0.5*click} U_1: 0.5$, $U_1 \\xrightarrow{0.5*click} I_1 \\xrightarrow{0.5*click} U_1: 0.25$, $U_1 \\xrightarrow{0.5*click} I_1 \\xrightarrow{1*buy} U_1: 0.5$, and $U_1 \\xrightarrow{0.5*click} I_2 \\xrightarrow{0.5*click} U_1: 0.25$. The sum of the importance of these five meta-path samples is 2.5. \n\nAt the same time, considering that the influence of meta-paths with different lengths on embedding should also be different, the learnable weight matrices $\\mathbf{W}^{(l)}$ in our multilayer graph convolution module can just play this role. \nEventually, we fuse the outputs of single-layer GCN and two-layer GCN:\n\\begin{equation}\n\\label{Double layer fusion GCN}\n    \\mathbf{H} = \\frac{1}{2}(\\mathbf{H}^{(1)} + \\mathbf{H}^{(2)}). \n\\end{equation}\nThe final embedding $\\mathbf{H}\\in \\mathbb{R}^{n\\times d}$ contains all 1-length and 2-length meta-path information. \n\nTo capture the more length heterogeneous meta-paths, we can extend it to $l$-layer: \n\\begin{equation}\n\\label{l-layer GCN}\n\\begin{split}\n    \\mathbf{H}^{(l)} &= \\mathbb{A} \\cdot \\mathbf{H}^{(l-1)} \\cdot \\mathbf{W}^{(l)}\\\\\n    &= \\mathbb{A} \\cdot (\\mathbb{A} \\cdot \\mathbf{H}^{(l-2)} \\cdot \\mathbf{W}^{(l-1)}) \\cdot \\mathbf{W}^{(l)}\\\\\n    &= \\underbrace{\\mathbb{A} \\cdots (\\mathbb{A}}_l \\cdot \\mathbf{X} \\cdot \\underbrace{\\mathbf{W}^{(1)}) \\cdots \\mathbf{W}^{(l)}}_l\\\\\n    &= \\mathbb{A}^{l} \\cdot \\mathbf{X} \\cdot \\underbrace{\\mathbf{W}^{(1)} \\cdots \\mathbf{W}^{(l)}}_l \n\\end{split}\n\\end{equation}\n\nTherefore, our multilayer graph convolution module fuses outputs of all layers to capture all meta-path information of different length across multi-relations: \n\\begin{equation}\n\\label{Multilayer fusion GCN}\n\\begin{split}\n    \\mathbf{H} &= \\frac{1}{l}\\sum_{i=1}^{l}\\mathbf{H}^{(i)}\\\\\n    &=\\frac{1}{l} \\sum_{i=1}^{l}\\mathbb{A} \\cdot \\mathbf{H}^{(i-1)} \\cdot \\mathbf{W}^{(i)},\n\\end{split}\n\\end{equation}\nwhere $\\mathbf{H}^{(0)}$ is the node attribute matrix $\\mathbf{X}$.\n\n\n% \\begin{algorithm}[t]\n% \\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n% \\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n% \\caption{The Learning Process of \\system}\n% \\label{alg.FastLANE}\n% \\begin{algorithmic}[1]\n% \\REQUIRE Input AMHEN $\\mathcal{G}$, node feature matrix $\\mathbf{X}$, embedding dimension $d$, the number of convolution layers $l$\n% \\ENSURE Embedding results $\\mathbf{H}$\n% \\STATE Decouple the attributed multiplex heterogeneous network into homogeneous networks and bipartite networks to obtain the adjacency matrices $\\{\\mathbf{A}_r|r=1,2,\\dots,|\\mathcal{R}|\\}$\n% \\STATE Calculate $\\mathbb{A} = \\sum_{r=1}^{|\\mathcal{R}|} \\beta_r \\mathbf{A}_r$\n% \\FOR{ $i=1$ to $l$}\n% \\STATE Calculate $\\mathbf{H}^{(i)} \\gets \\mathbb{A}\\cdot \\mathbf{H}^{(i-1)} \\cdot \\mathbf{W}^{(i)}$\n% \\ENDFOR\n% \\STATE $\\mathbf{H} = \\frac{1}{l}(\\mathbf{H}^{(1)} + \\dots + \\mathbf{H}^{(l)}) $\n% \\STATE Calculate $\\mathcal{L}$ using Eq.~\\eqref{Unsupervised Loss} or Eq.~\\eqref{Supervised Loss};\n% \\STATE Back propagation and update parameters in \\system\n% \\STATE Return $\\mathbf{H}$\n% \\end{algorithmic}\n% \\end{algorithm}\n\n\n\\vspace{-0mm}\n"
                },
                "subsection 4.3": {
                    "name": "Model Learning",
                    "content": "\n\n% After applying components introduced in the previous sections, we obtain the final node representations, which can then be used in different downstream tasks. Depending on the characteristics of different tasks and the availability of node labels, we can train \\system in two major learning paradigms, \\ie, semi-supervised learning and unsupervised learning. \n\nThis part presents the defined objective function to train our model to learn the final node representation. Depending on the requirements of different downstream tasks and the availability of node labels, we can train \\system in two major learning paradigms, \\ie, unsupervised learning and semi-supervised learning. \n\n\nFor unsupervised learning, we can optimize the model parameters by minimizing the following binary cross-entropy loss function through negative sampling: \n\\begin{equation}\n\\label{Unsupervised Loss}\n    \\begin{split}\n     \\mathcal{L} &=-\\sum_{(u, v) \\in \\Omega} \\log \\sigma(<\\mathbf{H}^\\mathsf{T}_u, \\mathbf{H}_v>) - \\sum_{(u', v') \\in \\Omega^{-}}\\log\\sigma(-<\\mathbf{H}^\\mathsf{T}_{u'}, \\mathbf{H}_{v'}>),\n    \\end{split}\n\\end{equation}\nwhere $\\mathbf{H}_v$ is the representation of node $v$, $\\mathsf{T}$ denotes matrix transposition,  $\\sigma(\\cdot)$ is the sigmoid function, $<,>$ can be any vector similarity measure function (\\eg, inner product), $\\Omega$ is the set of positive node pairs, $\\Omega^-$ is the set of negative node pairs sampled from all unobserved node pairs. \nThat is, we use the loss function to increase the similarities between the node representations in the positive samples and decrease the similarities between the node representations in the negative samples simultaneously. \n\nFor semi-supervised learning, we can optimize the model parameters by minimizing the cross entropy via backpropagation and gradient descent. %, and thereby learn meaningful node embeddings for multiplex heterogeneous networks. \nThe cross entropy loss over all labeled nodes between the ground-truth and the prediction is formulated as: \n\\begin{equation}\n\\label{Supervised Loss}\n     \\mathcal{L} = -\\sum_{i \\in \\mathcal{V}_{ids}} \\mathbf{Y}_i\\ \\mathrm{ln}(\\mathbf{C} \\cdot \\mathbf{H}_i), \n\\end{equation}\nwhere $\\mathcal{V}_{ids}$ is the set of node indices that have labels, $\\mathbf{Y}_i$ is the label of the $i$-th node, $\\mathbf{C}$ is the node classifier parameter, and $\\mathbf{H}_i$ is the representation of the $i$-th node.  \nWith the guide of a small fraction of labeled nodes, we can optimize the proposed model and then learn the embeddings of nodes for semi-supervised classification. \n\nNotice that $\\{\\mathbf{W}^{(i)}|i=1,2,\\dots, l\\}$ and  $\\{\\beta_r|r=1,2,\\dots,|\\mathcal{R}|\\}$ in our model can be learned during training phase. \n% Algorithm~\\ref{alg.FastLANE} shows the pseudo-code of our proposed \\system framework guided by the above objective functions. \nThe pseudo-code of our proposed \\system is shown in Algorithm~\\ref{alg.FastLANE} in the supplement. \n\n\\vspace{-0mm}\n"
                }
            },
            "section 5": {
                "name": "Experiment",
                "content": "\n\n \n% In this section, we study the effectiveness and efficiency of our proposed \\system in learning node embeddings on five heterogeneous networks compared to state-of-the-art baselines. \n\n% \\input{tables/methods.tex} \n\n",
                "subsection 5.1": {
                    "name": "Datasets",
                    "content": "\n\nIn our evaluation, five publicly available real-world datasets are used in experimental evaluation, \\ie, Alibaba\\footnote{\\url{https://tianchi.aliyun.com/competition/entrance/231719/information/}}, Amazon\\footnote{\\url{http://jmcauley.ucsd.edu/data/amazon/}}, AMiner\\footnote{\\url{https://github.com/librahu/}}, IMDB\\footnote{\\url{https://github.com/seongjunyun/Graph_Transformer_Networks}}, and DBLP\\footnote{\\url{https://www.dropbox.com/s/yh4grpeks87ugr2/DBLP_processed.zip?dl=0}}. \nDetailed dataset description can be found in the supplement. \n% Alibaba dataset\\footnote{\\url{https://tianchi.aliyun.com/competition/entrance/231719/information/}} includes four types of edges between user and item nodes. We use the category of item as the class label in node classification. \n% Amazon dataset\\footnote{\\url{http://jmcauley.ucsd.edu/data/amazon/}} includes one node type of products in Electronics category, and co-viewing and co-purchasing links between products. The product attributes contain the price, sales-rank, brand, category, etc.\n% AMiner dataset\\footnote{\\url{https://github.com/librahu/}} is a citation network, which contains three types of nodes: author, paper and conference. The domain of papers is considered as the class label. \n% IMDB dataset\\footnote{\\url{https://github.com/seongjunyun/Graph_Transformer_Networks}} contains three types of nodes, \\ie, movie, actor and director, and labels are genres of movies. Node features are given as bag-of-words representations of plots.\n% DBLP dataset\\footnote{\\url{https://www.dropbox.com/s/yh4grpeks87ugr2/DBLP_processed.zip?dl=0}} contains four types of nodes, \\ie, author, paper, term and venue. We use the authors' research field as a label for classification. \n% Since some of the baselines cannot scale to the whole Alibaba network, \nDue to the scalability limitation of applying some baselines in the whole Alibaba networ data, we evaluate all models on a sampled dataset from Alibaba. \nThe statistics of these five datasets are summarized in Table~\\ref{table_dataset}.  \n\n\n\n\n\\vspace{-0mm}\n"
                },
                "subsection 5.2": {
                    "name": "Baselines",
                    "content": "\nWe compare our \\system against the following eighteen graph learning baselines, which are divided into three categories.\n\nHomogeneous network embedding methods:\n\\begin{itemize}\n\n% \\item \\textbf{node2vec}~\\cite{grover2016node2vec} - node2vec is a network embedding method which samples short biased random walks. % with the balance between depth-first sampling and breadth-first sampling. \n\n\\item \\textbf{node2vec}~\\cite{grover2016node2vec} - node2vec is a representative method for graph representation by leveraging the random walk to generate node sequences over graphs.\n\n% \\item \\textbf{RandNE}~\\cite{icdm18billionNE} - RandNE is a network embedding approach based on Gaussian random projection, which preserves the high-order proximities between nodes.\n\n\\item \\textbf{RandNE}~\\cite{icdm18billionNE} - RandNE performs projection process in an iterative way to capture the high-order graph structures with the matrix factorization objective.\n\n% \\item \\textbf{FastRP}~\\cite{DBLP:conf/cikm19_fastRP} - FastRP is an extension of RandNE by using sparse random projection. % and normalizing node similarity matrix entries. \n\n\\item \\textbf{FastRP}~\\cite{DBLP:conf/cikm19_fastRP} - This method generates similarity matrix for modeling the transitive relations among nodes. Then, FastRP leverages sparse random projection to reduce dimension.\n\n% \\item \\textbf{SGC}~\\cite{wu2019simplifying} - SGC is a simplified version of GCN, which only uses the product of high-order adjacency matrices and attribute matrix, without nonlinear transformation. % and training parameters. %SGC, RandNE, FastRP and our method are all no-learning algorithms. \n\n\\item \\textbf{SGC}~\\cite{wu2019simplifying} - SGC proposes to simplify the graph convolutional networks by removing the non-linear projection during the information propagation between graph layers.\n\n\\item \\textbf{AM-GCN}~\\cite{wang2020gcn} - AM-GNN is a state-of-the-art graph convolutional network, which is an adaptive multi-channel graph convolutional networks for semi-supervised classification.\n\n\\end{itemize}\n\nHeterogeneous network embedding methods:\n\\begin{itemize}\n\\item \\textbf{R-GCN}~\\cite{schlichtkrull2018modeling} - R-GCN further considers the influence of different edge types on nodes, and uses weight sharing and coefficient constraints to apply to heterogeneous networks. \n\n\\item \\textbf{HAN}~\\cite{www2019HAN} - HAN applies graph attention network on multiplex network considering the inter- and intra-network interactions, which exploit manually selected meta-paths to learn node embedding. \n\n\\item \\textbf{NARS}~\\cite{yu2020scalable} NARS decouples heterogeneous networks according to the type of edge, and then aggregates neighbor features on the decoupled subgraph. \n\n% \\item \\textbf{AM-GCN}~\\cite{wang2020gcn} - AM-GNN is a state-of-the-art graph convolutional network, which is an adaptive multi-channel graph convolutional networks for semi-supervised classification. \n\n\\item \\textbf{MAGNN}~\\cite{fu2020magnn} - MAGNN is a metapath aggregated graph neural network for heterogeneous graphs. \n% It uses intra- and inter-metapath aggregations to incorporate intermediate semantic nodes and messages from multiple meta-paths.\n\n\\item \\textbf{HPN}~\\cite{ji2021heterogeneous} - HPN designs a semantic propagation mechanism to alleviate semantic confusion and a semantic fusion mechanism to integrate rich semantics. \n% \\item \\textbf{HPN}~\\cite{ji2021heterogeneous} - HPN expounds the concept of semantic confusion phenomenon, and designs a semantic propagation mechanism to alleviate semantic confusion and a semantic fusion mechanism to integrate rich semantics. \n\\end{itemize}\n\n\nMultiplex Heterogeneous network embedding methods:\n\\begin{itemize}\n\n\\item \\textbf{PMNE}~\\cite{liu2017PMNE} - PMNE contains three different models to merge the multiplex network to generate one overall embedding for each node, which are denoted as PMNE-n, PMNE-r, and PMNE-c, respectively. \n% We denote their network aggregation, result aggregation, and co-analysis model as PMNE-n, PMNE-r, and PMNE-c, respectively. \n\n% PMNE contains three different models to merge the multiplex network to generate one overall embedding for each node. We denote their network aggregation, result aggregation, and co-analysis model as PMNE-n, PMNE-r, and PMNE-c, respectively. \n\n\\item \\textbf{MNE}~\\cite{zhang2018scalable} - MNE obtains the final embedding by combining the high-dimensional common embedding and the low-dimensional hierarchical embedding. \n% \\textcolor{red}{MNE considers the unique embedding space of each layer of multiplex network, and obtains the final embedding by combining the high-dimensional common embedding  and the low-dimensional hierarchical embedding.}\n\n% \\item \\textbf{GATNE}~\\cite{kdd19GATNE} - GATNE includes two variants GATNE-T and GATNE-I. We use GATNE-I as our baseline method in experiments. %, which considers both the network structure and the node attributes, and then learns an inductive transformation function to obtain node embeddings. \n\n\\item \\textbf{GATNE}~\\cite{kdd19GATNE} - GATNE proposes to generate the overall node embeddings with the base embedding as well as the edge and attribute representations. The edge embedding is generated by fusing neighboring information using self-attention.\n\n% \\item \\textbf{HAN}~\\cite{www2019HAN} - HAN applies graph neural network and graph attention mechanism on multiplex network to learn node embedding. \n\n\n% \\item \\textbf{GTN}~\\cite{nips19GTN} - GTN transforms a heterogeneous graph into multiple meta-path graphs and then learns node embeddings via GCN on the meta-path graphs. \n\n\\item \\textbf{GTN}~\\cite{nips19GTN} - It studies the graph representation task by identifying effective meta-paths with high-order relations. \n\n\\item \\textbf{DMGI}~\\cite{park2019DMGI} - DMGI %learns a node encoder that maximizes the mutual information between local patches of each relation of the heterogeneous graph and the global representation of the relation. \ndevelops a consensus regularization scheme to consider the relations among type-specific node embeddings. Furthermore, each relational contextual signals are aggregated through attention mechanism.\n% integrates node embeddings from multiple graphs by introducing a consensus regularization framework and an universal discriminator. \n\n\n\\item \\textbf{FAME}~\\cite{liu2020fast} - FAME is a random projection-based network embedding for AMHENs, which uses spectral graph transformation to capture meta-paths, and significantly improves efficiency through random projection. \n\n\\item \\textbf{HGSL}~\\cite{zhao2021heterogeneous} - HGSL is a state-of-the-art heterogeneous GNN, which jointly performs heterogeneous graph structure learning and GNN parameter learning for classification.  \n% generates heterogeneous graph structures suitable for downstream tasks by mining feature similarity, interaction between features and structures, and high-order semantic structures in heterogeneous graphs, and jointly learns GNN parameters. \n\n\\item \\textbf{DualHGNN}~\\cite{xue2021multiplex} - DualHGCN uses dual hypergraph convolutional network to learn node embeddings for multiplex bipartite networks. \n\n\\end{itemize}\n\n% For homogeneous network embedding methods and heterogeneous network embedding methods to deal with multiplex networks, we feed separate graphs with a single-layer view into them to obtain different node embeddings, then perform mean pooling to generate final node embedding. \n% Since DualHGNN is designed only for multiplex bipartite networks, it can only work on Alibaba network. \nThe network types handled by the competitor methods are summarized in Table~\\ref{tab:method} in the supplemental material. \n\n\n\n\\vspace{-0mm}\n"
                },
                "subsection 5.3": {
                    "name": "Experimental Setting",
                    "content": "\n\n% For link prediction task, we treat the connected nodes in network as positive node pairs, and consider all unlinked nodes as negative node pairs. For each edge type, we divide the positive node pairs into training set, verification set and test set according to the proportion of 85\\%, 5\\% and 10\\%. At the same time, we randomly select the same number of negative node pairs to add into training set, validation set and test set. Notice that we predict each type of edge using all types of edges in datasets, and finally take the average of all edges as the final result.  \n% For node classification task, we first learn the representation of each node and then perform accuracy evaluation. \n% More specifically, we take 80\\% of the node embeddings as the training set, 10\\% as the validation set, and 10\\% as the test set. \n% In experiments, we train a logistic regression classifier for node classification. \n% Notice that we repeat each experiment 10 times to report average results.\n\n% For fair comparison, we uniformly set the number of training rounds to 500 for link prediction and 200 for node classification. \n\nFor baseline implementations, we either leverage OpenHGNN\\footnote{\\url{https://github.com/BUPT-GAMMA/OpenHGNN}} or use the released source code for evaluation. In our experiments, we keep $d$=200 for all compared methods. Others hyperparameter settings are considered according to their original papers. For our \\system, we set the number of convolution layers $l$ to 2. For fair comparison, we uniformly set the number of training rounds to 500 for link prediction and the number of training rounds to 200 for node classification. \nMore detailed experimental settings can be found in the supplementary material.\n\n% Following~\\cite{liu2020fast}, we set $d$ to 200 for all the methods. For baselines, we use the source code released by their authors or OpenHGNN\\footnote{\\url{https://github.com/BUPT-GAMMA/OpenHGNN}}, and\n\n% adopt the parameter settings recommended in their papers and fine-tune them to be optimal. \n\n% We set $p=2$ and $q=0.5$ for node2vec and set $\\alpha_r$ and $\\beta_r$ to 1 for every edge type $r$ on GATNE. \n% For the PMNE model, we use the hyperparameters given by the original paper. \n% For all deep learning methods (\\eg, GATNE, HAN, GTN), we tune learning rate in $\\{0.01, 0.05, 0.001, 0.005, 0.0001, 0.0005\\}$. \n% We set the regularization parameter to 0.001, the number of attention head is set as 8, and the dropout ratio of attention is 0.6. \n% For GTN, we use the sparse version of their released source code and set GT layers to 3 for all datasets. \n% For DMGI, we set the self-connection weight $w=3$ and tune $\\alpha, \\beta, \\gamma$ in $\\{0.0001, 0.001, 0.01, 0.1\\}$.  \n% For FAME, we perform optuna\\footnote{\\url{https://github.com/pfnet/optuna}} to tune the weights $\\alpha_1,\\dots,\\alpha_K, \\beta_1,\\dots,\\beta_{|\\mathcal{R}|}$ as described in the original paper. \n% For AM-GCN, we tune loss aggregation parameters $\\beta, \\gamma$ in $\\{0.0001, 0.001, 0.01, 0.1\\}$.\n% For MAGNN, we set the number of independent attention mechanisms $k=4$.\n% For HPN, we set iterations in semantic propagation $k=3$ and value of restart probability $\\alpha=0.1$.\n% For our \\system, we set the number of convolution layers $l$ is 2. \n% We evaluate the efficiency evaluation for all methods on a machine with Intel Xeon E5-2660 (2.2GHz) CPU, 80GB memory, and 2 $\\times$ GeForce RTX 2080 (8G). \n\n\n\\vspace{-0mm}\n"
                },
                "subsection 5.4": {
                    "name": "Link Prediction",
                    "content": "\nWe first evaluate the model performance by comparing our \\system with fifteen baselines on link prediction task in an unsupervised learning manner. \n% Because HAN, GTN and AM-GCN require node labels for training, thus they cannot be applied to the link prediction for nodes of different types. \n% DMGI does not conduct link prediction experiment and does not explain how to do link prediction between different types of nodes in their paper. Thus, we overlook the node type in the experiment.  \nThe results are shown in Table~\\ref{table_linkprediction}, where the best is shown in bold. The first seven baselines are homogeneous or heterogeneous network embedding methods, and the last eight are multiplex network embedding methods. \n\nWe can see that \\system significantly outperforms all baselines in terms of all evaluation metrics on five datasets. \nSpecifically, \\system achieves average gains of 5.68\\% F1 score in comparison to the best performed GNN baselines across all datasets (\\ie, FAME, MAGNN and HPN). \nOur \\system realizes a high accuracy of more than 96\\% on three datasets (Alibaba, Amazon, and IMDB), especially more than 99\\% prediction performance on Alibaba network. \nThis is because \\system automatically captures effective multi-relational topological structures through multiplex relation aggregation and multilayer graph convolution on the generated meta-paths across multiplex relations.  \nEspecially, compared with GATNE and MAGNN, our model has achieved better results, showing the ability of our model in automatically capturing meta-paths compared with manually setting meta-paths.\nFAME that use spectral graph transformation achieving the second best performance on most datasets also verifies the ability of multiplex relation aggregation to automatically capture useful heterogeneous meta-paths. \nHowever, \\system obtains better performance than FAME on all networks as \\system learns meaning node representations for AMHENs using multilayer graph convolution in a learning manner.  \nAdditionally, \\system also shows significant performance advantages on general heterogeneous networks (\\eg, IMDB and DBLP). \nThis may be because our \\system uses a weighted approach to differentiate the effects of different types of relations on node representation, which cannot be achieved by traditional meta-path sampling.\n\n% Especially, compared with GATNE and MAGNN, our model has achieved better results, showing the ability of our model in automatically capturing meta-paths compared with manually setting meta-paths.\n\n\n\n\n\n\n\n\n\n% \\begin{table*}[t]\n% \\begin{center}\n% % \\renewcommand{\\arraystretch}{0.9}\n% % \\caption{Node classification performance comparison of different methods on three datasets}\n% \\caption{Node classification performance comparison}\n% % \\vspace{-3mm}\n% \\label{tab:node_calssfication}\n% % \\setlength{\\tabcolsep}{1mm}{}\n% \\begin{threeparttable}\n% \\setlength{\\tabcolsep}{2.1mm}{}\n% \\scalebox{0.8}{\n% \\begin{tabular}{c|c|cc|cc|cc|cc}\n% \\toprule\n% \\multirow{2}{*}{Method} & \\multirow{2}{*}{Unsupervised} & \\multicolumn{2}{c|}{AMiner} & \\multicolumn{2}{c|}{Alibaba-S} & \\multicolumn{2}{c}{IMDB} & \\multicolumn{2}{c}{DBLP}\\\\\n%  &  & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 \\\\\n% \\midrule\n% % \\hline\n% RandNE & \\checkmark & 0.641 (0.0074) & 0.672 (0.0064) & 0.319 (0.0170) & 0.358 (0.0093) & 0.373 (0.0143) & 0.392 (0.0185) & 0.351(0.0153) & 0.372(0.0150) \\\\\n% FastRP & \\checkmark & 0.650 (0.0086) & 0.690 (0.0074) & 0.301 (0.0180) & 0.392 (0.0119) & 0.363 (0.0236) & 0.381(0.0140) & 0.343(0.0201) & 0.375(0.0199) \\\\\n% SGC & \\checkmark & 0.516 (0.0047) & 0.587 (0.0157) & 0.286 (0.0231) & 0.361 (0.0175) & 0.489 (0.0106) & 0.563 (0.0133) & 0.122(0.0009) & 0.323(0.0009)\\\\ \n% GATNE & \\checkmark & OOT & OOT & 0.291 (0.0086) & 0.390 (0.0014) & 0.169 (0.0132) & 0.333 (0.0005) & OOT & OOT \\\\\n% DMGI & \\checkmark & 0.473 (0.0155) & 0.626 (0.0093) & 0.220 (0.0214) & 0.392 (0.0026) & 0.548 (0.0190) & 0.544 (0.0189) &0.781(0.0303) & 0.787(0.0235)\\\\ \n% FAME & \\checkmark & 0.722 (0.0114) & 0.727 (0.0091) & 0.323 (0.0154) & 0.393 (0.0060) & 0.593 (0.0135) & 0.594 (0.0143) & 0.842(0.0183) & 0.868(0.0127)\\\\\\midrule\n% HAN & $\\times$ & 0.690 (0.0149) & 0.726 (0.0086) & 0.275 (0.0327) & 0.392 (0.0081) & 0.552 (0.0112) & 0.568 (0.0078) & 0.806(0.0078) & 0.813(0.0100)\\\\\n% GTN & $\\times$ & OOM & OOM & 0.255 (0.0420) & 0.392 (0.0071) & 0.615 (0.0108) & 0.616 (0.0093) & 0.852(0.0137) & 0.868(0.0125) \\\\\n% AM-GCN & $\\times$ & 0.7023 (0.0175) & 0.7125 (0.0223) & 0.307 (0.0232) & 0.399 (0.0156) & 0.610 (0.0021) & 0.640 (0.0013) & 0.867(0.0105) & 0.878(0.0112) \\\\ \n% MAGNN & $\\times$ &  0.755 (0.0105) & 0.757 (0.0133) & 0.348(0.0488) & 0.398 (0.0405) & 0.614 (0.0073) & 0.615 (0.0089) & 0.881(0.0284) & 0.895(0.0396) \\\\\n% HGSL & $\\times$ &  0.754 (0.0100) & 0.758 (0.0103) & 0.338(0.0121) & 0.398 (0.0238) & 0.620 (0.0048) & 0.638 (0.0030) & 0.893(0.0284) & 0.902(0.0396) \\\\\n% HPN & $\\times$ &  0.710 (0.0612) & 0.732 (0.0490) & 0.263(0.0346) & 0.392 (0.0405) & 0.578 (0.0023) & 0.584 (0.0021) & 0.822(0.0201) & 0.830(0.0201) \\\\\\hline\n% \\system & $\\times$ & \\textbf{0.868(0.0260)} & \\textbf{0.875(0.0300)} & \\textbf{0.351(0.0304)} & \\textbf{0.458(0.0160)} & \\textbf{0.733(0.0245)} & \\textbf{0.741(0.0238)} & \\textbf{0.945(0.082)} & \\textbf{0.952(0.082)} \\\\ \n% % \\hline\n% \\bottomrule\n% \\end{tabular}\n% }\n% \\begin{tablenotes}\n%         % \\footnotesize\n%         \\item OOM: Out Of Memory (80 GB), OOT: Out Of Time (36 hours). The standard deviations are reported in the parentheses. \n% \\end{tablenotes}\n% \\end{threeparttable}\n% \\end{center}\n% \\end{table*}\n\n\n\n\n% \\begin{figure*}[h]\n%     \\begin{center}\n%     \\vspace{-4mm}\n%     \\hspace{-1mm}\n%     \\subfigure[Macro-F1 score \\wrt. \\#layers]{\n%     \\label{fig:layer-Ma-F1}\n%     \\includegraphics[width=0.32\\textwidth]{figures/layerL-MaF1.pdf}\n%     }\n%     \\hspace{-1mm}\n%     \\subfigure[Macro-F1 score \\wrt. dimension $d$]{\n%     \\label{fig:dimension-Ma-F1}\n%     \\includegraphics[width=0.32\\textwidth]{figures/dimensionD-MaF1.pdf}\n%     }\n%     \\hspace{-1mm}\n%     \\subfigure[Macro-F1 score \\wrt. \\#rounds]{\n%     \\label{fig:round-Ma-F1}\n%     \\includegraphics[width=0.32\\textwidth]{figures/roundsR-MaF1.pdf}\n%     }\n%     \\vspace{-3mm}\n%     \\caption{Parameter sensitivity of proposed method \\wrt. \\#layers, dimension $d$, and \\#rounds.} \n%     \\label{fig:sensitive}\n%     \\vspace{-4mm}\n%     \\end{center}\n% \\end{figure*}\n\\vspace{-0mm}\n"
                },
                "subsection 5.5": {
                    "name": "Node Classification",
                    "content": "\nWe next evaluate the effectiveness of our model on the node classification task compared with state-of-the-art methods. \nThe results are shown in Table~\\ref{tab:node_calssfication}, where the best is shown in bold. \nThe first eight baselines are unsupervised embedding methods, and the rest are semi-supervised embedding methods. \n% Based on these results, we have the following observations: \n\nAs we see, \\system also achieves state-of-the-art performance on all tested networks. Specifically, our \\system achieves average 11.22\\% and 14.49\\% improvement over state-of-the-art GNN model HGSL across all datasets in terms of Macro-F1 and Micro-F1, respectively. \nConsidering that the performance gain in node classification task reported in some recent works~\\cite{fu2020magnn,zhao2021heterogeneous} is usually around 2-4\\%, this performance improvement achieved by our \\system is significant. \n% This experiment verifies the benefit of our method that models multi-relational structures and node attributes of an AMHEN using multiplex relation aggregation and multilayer graph convolution module guided with the labeled data. \n% This experiment verifies the benefit of our framework that models the multiplicity and the meta-path information of an AMHEN together with the node attributes in a single embedding framework. \n% (2) GTN cannot run on AMiner due to memory constraint as it needs to calculate the product of the adjacency matrices explicitly. Furthermore, GATNE cannot obtain the embeddings for AMiner network within 36 hours, and it cannot well learn the features that distinguish node categories. \n% (3) \\system that considers meta-path interactions performs better than the variation that only considers sub-network topological structure.  \n% (3) Meta-path information is more important in node classification for multi-type node networks \nFurthermore, we also observe that \\system performs much better than competitor methods on general heterogeneous network with multi-typed nodes (\\eg, IMDB and AMiner), achieving 23.23\\% and 22.19\\% improvement in Macro-F1 and Micro-F1 on IMDB network. \nThe possible reason is that our \\system effectively learns node representations for classification by exploring all meta-path interactions across multiple relations with different importance (\\ie, weights), which is ignored by the heterogeneous network embedding approaches based on manually setting meta-path sampling.\n\n\n\n\n\n"
                },
                "subsection 5.6": {
                    "name": "Ablation Study",
                    "content": "\n\nTo validate the effectiveness of each component of our model, we further conduct experiments on different \\system variations. Here \\varr does not consider the importance of different relations, that is, we set the weights $\\beta_r$ to 1; \\varl uses only a two-layer GCN to obtain the embedding, so it can only capture the 2-length meta-paths. We report the results of ablation study on four datasets for node classification in Figure~\\ref{fig: ablation}, where the performance on Alibaba refers to the right-ordinate axis. \n\nIt can be seen from the results that the two key components both contribute to performance improvement of our \\system. \nThe comparison between \\varr and \\system highlights the effectiveness of the importance of different relations. \nWe can observe that \\varr performs worse than \\system on all datasets in terms of both Macro-F1 and Micro-F1 metrics, reducing 9.68\\% performance in Macro-F1 score on Alibaba, which demonstrates the crucial role of our designed multiplex relation aggregation module in capturing the importance of different relations for node representation learning. \nThe comparison between \\varl and \\system reflects the importance of our multilayer graph convolution module. \nCompared with \\varl, \\system improves 2.97\\%, 18.98\\%, 4.09\\% and 1.51\\% over \\varl in terms of Macro-F1 on AMiner, Alibaba, IMDB, and DBLP, respectively.\nThis indicates that our proposed multilayer graph convolution module effectively captures useful meta-paths of different lengths across multiplex relations. \n\n\n\n\n\\vspace{-0mm}\n"
                },
                "subsection 5.7": {
                    "name": "Parameter Sensitivity",
                    "content": "\n\nWe conduct hyperparameter study in our new \\system in terms of key parameters, \\ie the number of graph layers $l$, the hidden dimension size of embeddings $d$, and the number of training rounds. We report Macro-F1 score on node classification task with different parameter settings on four datasets in Figure~\\ref{fig:sensitive}. Notice that the performance on Alibaba refers to the ordinate on the right.\n\n% We finally investigate the sensitivity of \\system with respect to the important parameters, including the number of layers $l$, embedding dimension $d$, and the number of training rounds. \n\nFrom the results shown in Figure~\\ref{fig:layer-Ma-F1}, we can observe that the best performance can be achieved with two layers. This observation indicates that considering the meta-path interactions with two-hops is sufficient to capture the node dependencies in the graph. Performing the message passing across more graph layers may involve noisy information for node representation. With the growth of GCN layers, the representation of nodes would be flattened after multiple convolutions, resulting in performance degradation. Additionally, we can notice that the increasing of embedding dimension first brings benefits for the performance improvement, and then leads to the performance degradation. The best prediction accuracy can be achieved with the setting of $d=128$. This is because the features of all nodes are compressed into a small embedding space when dimension $d$ is small, thus it is difficult to retain the characteristics proximities of all node pairs. \nConversely, a larger dimension would also flatten the distance between all node embeddings. Figure~\\ref{fig:round-Ma-F1} illustrates the performance of our \\system with respect to the number of training rounds in learning model weights. \nWe can find that our \\system can converge quickly and efficiently achieve stable performance within 80 rounds on all tested datasets.\n\n% As shown in Figure~\\ref{fig:layer-Ma-F1}, at first, the performance of \\system increases as $l$ increases, and then the performance begins to decline when $l\\ge 2$. This is mainly because 1-length and 2-length meta-path interactions already effectively capture the topological structures of network for node classification, while longer meta-paths would not lead to performance improvement. \n% Then we analyze the impact of embedding dimension on embedding performance. \n\n% From the results in Figure~\\ref{fig:dimension-Ma-F1}, we can see that the performance of \\system gradually rises and then decreases slightly as dimension $d$ increases, and achieves the best performance when embedding dimension $d=128$. \n\n% When the embedding dimension is very small, representation can not show strong performance. Because the features of each point are compressed into a small embedding space, it is difficult to retain the characteristics of each point. With the increase of dimension, the performance gradually improves and tends to be stable after a small fluctuation. This is because when the dimension increases enough to capture valid features, the dimension is no longer important. \n%, which reﬂects the efficiency advantages of our model. \n% In addition, the high efficiency of our \\system verified in the above experiment makes the parameter learning process more efficient, which greatly reﬂects the advantages of our method.\n% \\vspace{-1mm}\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\nIn this paper, we propose an embedding model \\system for attributed multiplex heterogeneous networks. Our model mainly includes two key components: multiplex relation aggregation and multilayer graph convolution module. Through multiplex relation aggregation, \\system can distinguish the importance of the relations between different nodes in multiplex heterogeneous networks. Through multilayer graph convolution module, \\system can automatically capture the short and long meta-path interactions across multi-relations, and learn meaning node embeddings with model parameter learning during training phase. Experiments results on five real-world heterogeneous networks show the superiority of the proposed \\system in different graph representation tasks. \n\n\n\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n% \\begin{acks}\n% To Robert, for the bagels and explaining CMYK and color spaces.\n% \\end{acks}\n\n\\begin{acks}\nThis work is partially supported by the National Natural Science Foundation of China under grant Nos. 62176243, 62072288, 61773331 and 41927805, and the National Key Research and Development Program of China under grant Nos. 2018AAA0100602 and 2019YFC1509100.\n\\end{acks}\n\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n% \\clearpage\n% \\newpage\n\\balance \n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{kdd2022}\n\n% \\balance\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\clearpage\n\\appendix\n\n% \\section{Supplemental Material}\n"
            },
            "section 7": {
                "name": "Supplemental Material",
                "content": "\n\n% In the supplement, we summarize the key notations, and present the pseudo-code of the proposed \\system framework. \n% For the reproducibility, we provide our detailed experimental settings and source code websites of all the baselines. \n% % The implementation details, including the detailed hyper-parameter values for all the experiments, are also provided. \n% The network types handled by the baselines are also summarized. \n% Finally, we show additional results of model efficiency to support the conclusions in our paper. \n\n",
                "subsection 7.1": {
                    "name": "Notations",
                    "content": "\n\nWe summarize the key notations used in the paper as well as their definitions in Table~\\ref{table_notations}. \n\n\n\n\n\n"
                },
                "subsection 7.2": {
                    "name": "Algorithm Pseudo-Code",
                    "content": "\n\nAlgorithm~\\ref{alg.FastLANE} shows the pseudo-code of our proposed \\system framework guided by the above objective functions (\\ie, Eq.~\\eqref{Unsupervised Loss} or Eq.~\\eqref{Supervised Loss}). \n\n\\begin{algorithm}[h]\n\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n\\caption{The Learning Procedure of our \\system Model}\n\\label{alg.FastLANE}\n\\begin{algorithmic}[1]\n\\REQUIRE The generated AMHEN $\\mathcal{G}$ and node feature matrix $\\mathbf{X}$.\n% \\REQUIRE Input AMHEN $\\mathcal{G}$, node feature matrix $\\mathbf{X}$, embedding dimension $d$, the number of convolution layers $l$\n%\\ENSURE Embedding results $\\mathbf{H}$\n\\ENSURE The node embeddings $\\mathbf{H}$ of graph $\\mathcal{G}$.\n% \\STATE Decouple the attributed multiplex heterogeneous network into homogeneous networks and bipartite networks to obtain the adjacency matrices $\\{\\mathbf{A}_r|r=1,2,\\dots,|\\mathcal{R}|\\}$\n\\STATE We generate the adjacency matrices $\\{\\mathbf{A}_r|r=1,2,\\dots,|\\mathcal{R}|\\}$ by decoupling the attributed multiplex heterogeneous network into homogeneous and bipartite graphs.\n\\STATE Calculate $\\mathbb{A} = \\sum_{r=1}^{|\\mathcal{R}|} \\beta_r \\mathbf{A}_r$\n\\FOR{ $i=1$ to $l$}\n\\STATE Calculate $\\mathbf{H}^{(i)} \\gets \\mathbb{A}\\cdot \\mathbf{H}^{(i-1)} \\cdot \\mathbf{W}^{(i)}$\n\\ENDFOR\n\\STATE $\\mathbf{H} = \\frac{1}{l}(\\mathbf{H}^{(1)} + \\dots + \\mathbf{H}^{(l)}) $\n\\STATE Calculate $\\mathcal{L}$ using Eq.~\\eqref{Unsupervised Loss} or Eq.~\\eqref{Supervised Loss};\n\\STATE Back propagation and update parameters in \\system\n\\STATE Return $\\mathbf{H}$\n\\end{algorithmic}\n\\end{algorithm}\n\n"
                },
                "subsection 7.3": {
                    "name": "Detailed Dataset Description",
                    "content": "\ni) For Alibaba dataset, four types of user-item interactions are regarded as the node-wise multiplex relationships. The item categories are considered as the ground truth labels for node classification. ii) For Amazon dataset, the multiplex edges are represented as the co-viewing and co-purchasing relations between different products. The node attributes include the external features of products, \\eg, category, sales-rank, brand and price information. iii) For the AMiner dataset, three types of nodes (\\ie, author, paper and conference) are included in the heterogeneous graph. The node labels are the paper domains. iv) For the IMDB dataset, movie, director and actor construct the heterogeneous nodes. We consider the genres of movies as the node labels. The bag-of-words representations are considered as the node attributed feature vectors.\n\nv) For the DBLP dataset, four types of nodes are involved in the heterogeneous graph, namely, author, paper, venue, and term. We regard the research field of authors as the node class labels.\n\n% Alibaba dataset includes four types of edges between user and item nodes. We use the category of item as the class label in node classification. \n\n% Amazon dataset includes one node type of products in Electronics category, and co-viewing and co-purchasing links between products. The product attributes contain the price, sales-rank, brand, category, etc.\n\n% AMiner dataset is a citation network, which contains three types of nodes: author, paper and conference. The domain of papers is considered as the class label. \n\n% IMDB dataset contains three types of nodes, \\ie, movie, actor and director, and labels are genres of movies. Node features are given as bag-of-words representations of plots.\n\n% DBLP dataset contains four types of nodes, \\ie, author, paper, term and venue. We use the authors' research field as a label for classification. \n\n\n\n"
                },
                "subsection 7.4": {
                    "name": "Detailed Experimental Settings",
                    "content": "\n\n\nFor link prediction task, we treat the connected nodes in network as positive node pairs, and consider all unlinked nodes as negative node pairs. For each edge type, we divide the positive node pairs into training set, verification set and test set according to the proportion of 85\\%, 5\\% and 10\\%. At the same time, we randomly select the same number of negative node pairs to add into training set, validation set and test set. Notice that we predict each type of edge using all types of edges in datasets, and finally take the average of all edges as the final result. In particular, the training, validation and test sets are generated with the ratio of 80\\%, 10\\% and 10\\%, respectively. In experiments, we train a logistic regression classifier for node classification. Notice that we repeat each experiment 10 times to report average results.\n\n% For node classification task, the model performance evaluation relies on the learned node embeddings for predicting the class of each node. \n\n\n% More specifically, we take 80\\% of the node embeddings as the training set, 10\\% as the validation set, and 10\\% as the test set.\n\n% For fair comparison, we uniformly set the number of training rounds to 500 for link prediction and 200 for node classification. \n\nFor fair comparison, we uniformly set the number of training rounds to 500 for link prediction and the number of training rounds to 200 for node classification. \n% We set $d$ to 200 for all the methods.\nFor node2vec method, the parameters $p$ and $q$ for random walk control are set as 2 and 0.5, respectively. For GATNE approach, the parameters $\\alpha_r$ and $\\beta_r$ are set as 1 for each edge type. For the MNE, we set the dimension of additional vectors to 10, set the length of walk as 10, set the number of walks as 20. For compared neural network-based models, the learning rate is searched from the range of $\\{0.0005, 0.0001, 0.005, 0.001, 0.05, 0.01,\\}$. We configure the multi-head attention with 8 head-specific representation spaces, and apply the dropout ratio of 0.6. For GTN method, the number of graph Transformer layers is set as 3. For DMGI baseline, the parameters $\\alpha, \\beta, \\gamma$ are chosen from the value range of $\\{0.0001, 0.001, 0.01, 0.1\\}$. The weight $w$ for self-connection is set as 3.\n\n% We set the regularization parameter to 0.001, the number of attention head is set as 8, and the dropout ratio of attention is 0.6. \n\n% For GTN, we use the sparse version of their released source code and set GT layers to 3 for all datasets.\n\n% For DMGI, we set the self-connection weight $w=3$ and tune $\\alpha, \\beta, \\gamma$ in $\\{0.0001, 0.001, 0.01, 0.1\\}$.\n\n%We set $p=2$ and $q=0.5$ for node2vec and\n\n% set $\\alpha_r$ and $\\beta_r$ to 1 for every edge type $r$ on GATNE. \n% For the PMNE model, we use the hyperparameters given by the original paper. \n% For all deep learning methods (\\eg, GATNE, HAN, GTN), we tune learning rate in $\\{0.01, 0.05, 0.001, 0.005, 0.0001, 0.0005\\}$. \n\nFor FAME, we use optuna\\footnote{\\url{https://github.com/pfnet/optuna}} to tune the parameters over $\\alpha_1,\\dots,\\alpha_K$ and $\\beta_1,\\dots,\\beta_{|\\mathcal{R}|}$ as described in the original paper. \nFor AM-GCN, we tune loss aggregation parameters $\\beta, \\gamma$ in $\\{0.0001, 0.001, 0.01, 0.1\\}$.\nFor MAGNN, we set the number of independent attention mechanisms $k=4$.\nFor HPN, we set iterations in semantic propagation $k=3$ and value of restart probability $\\alpha=0.1$. \nFor HGSL, we set the number of GNN layers to 2 and the hidden layer output dimension to 64. \nFor R-GCN, we set the batch size to 126, the number of GNN layers to 2, and the hidden layer dimension to 64.\nFor NARS, we set the number of hops to 2, and the number of feed-forward layers to 2.\nFor DualHGNN, we use the asymmetric operator and set $\\lambda$ as 0.5.\n\nFor our \\system, we set the number of convolution layers $l$ to 2, learning rate to 0.05, dropout to 0.5, and weight-decay to 0.0005. %, and training epochs to 10. \n\n% We evaluate the efficiency evaluation for all methods on a machine with Intel Xeon E5-2660 (2.2GHz) CPU, 80GB memory, and 2 $\\times$ GeForce RTX 2080 (8G). \n\nThe configurations of system platform for efficiency evaluation are as followed. CPU: Intel Xeon E5-2660 (2.2GHz), Memory: 80GB, 2 GPU units: GeForce RTX 2080 (8G).  \n\nThe source code of our model implementation is available at \\url{https://github.com/NSSSJSS/MHGCN}. \n\n\n"
                },
                "subsection 7.5": {
                    "name": "Baselines",
                    "content": "\n\nThe publicly source codes of baselines can be available at the following URLs: \n\\begin{itemize}\n    \\item \\textbf{node2vec} -- \\url{https://github.com/aditya-grover/node2vec}\n    \\item \\textbf{RandNE} -- \\url{https://github.com/ZW-ZHANG/RandNE} \n    \\item \\textbf{FastRP} -- \\url{https://github.com/GTmac/FastRP} \n    \\item \\textbf{SGC} -- \\url{https://github.com/Tiiiger/SGC}\n    \\item \\textbf{AM-GCN} -- \\url{https://github.com/zhumeiqiBUPT/AM-GCN}\n    \\item \\textbf{R-GCN} -- \\url{https://github.com/BUPT-GAMMA/OpenHGNN} \n    \\item \\textbf{HAN} -- \\url{https://github.com/Jhy1993/HAN} \n    \\item \\textbf{NARS} -- \\url{https://github.com/BUPT-GAMMA/OpenHGNN} \n    \\item \\textbf{MAGNN} -- \\url{https://github.com/cynricfu/MAGNN}\n    \\item \\textbf{HPN} -- \\url{https://github.com/BUPT-GAMMA/OpenHGNN}\n    \\item \\textbf{PMNE} -- The source code of PMNE used in this work is released by the authors of MNE at  \\url{https://github.com/HKUST-KnowComp/MNE}\n    \\item \\textbf{MNE} -- \\url{https://github.com/HKUST-KnowComp/MNE}\n    \\item \\textbf{GATNE} -- \\url{https://github.com/THUDM/GATNE}\n    \\item \\textbf{GTN} --  \\url{https://github.com/seongjunyun/Graph_Transformer_Networks}\n    \\item \\textbf{DMGI} -- \\url{https://github.com/pcy1302/DMGI}\n    \\item \\textbf{FAME} -- \\url{https://github.com/ZhijunLiu95/FAME}\n    \\item \\textbf{HGSL} -- \\url{https://github.com/Andy-Border/HGSL}\n    \\item \\textbf{DualHGNN} -- \\url{https://github.com/xuehansheng/DualHGCN}\n\\end{itemize}\n\nFor homogeneous network embedding methods and heterogeneous network embedding methods to deal with multiplex networks, we feed separate graphs with a single-layer view into them to obtain different node embeddings, then perform mean pooling to generate final node embedding. \nSince DualHGNN is designed only for multiplex bipartite networks, it can only work on Alibaba network. \n\nThe network types handled by the baseline methods are summarized in Table~\\ref{tab:method}. \n\n\n\n"
                },
                "subsection 7.6": {
                    "name": "Additional Experimental Results",
                    "content": "\n\n\n",
                    "subsubsection 7.6.1": {
                        "name": "Model Efficiency Analysis",
                        "content": "\n\nWe also compare the efficiency of our \\system with other GNN baselines for semi-supervised node classification. We report the experimental results on four datasets in Table~\\ref{time}. \n\nAs can be seen from Table~\\ref{time}, \nour \\system achieves the fourth-best performance after three heterogeneous network embedding methods (\\ie, R-GCN, NARS and HPN). However, from the above experimental results (Tables~\\ref{table_linkprediction} and \\ref{tab:node_calssfication}), \\system is significantly better than these three methods in both link prediction and node classification. \n\\system is significantly faster than the best performed GNN baseline in node classification task (\\ie, HGSL) on all datasets under the same number of training rounds. \nMore specifically, our \\system achieves up to 135$\\times$ speedup over state-of-the-art embedding method HAN. \n\\system is faster than state-of-the-art AMHEN embedding method GTN by 21.25 times on multiplex Alibaba network. \n\\system is even 2.33 times and 16.58 times faster than state-of-the-art heterogeneous GNN model MAGNN on Alibaba and AMiner, respectively. \nThe main reason is because our \\system adopts the idea of simplifying graph convolutional networks, that is, omitting non-linear activation function. Therefore, the training efficiency of \\system can be significantly improved. \nIn fact, according to the above experimental results in Figure~\\ref{fig:round-Ma-F1}, our model can converge quickly within 80 rounds for node classification on four tested datasets, that is, %our model can achieve faster efficiency without training 200 rounds. \nour model does not need to be trained for 200 rounds set in our experimental evaluation and thus can achieve faster efficiency.\n\n\n\n\n% The network types handled by the competitor methods are summarized in Table~\\ref{tab:method}. \n\n% \\input{tables/methods.tex} \n\n\n\n\n"
                    }
                }
            }
        },
        "tables": {
            "table_notations": "\\begin{table}[htbp]\n\\centering\n\\caption{Summary of key notations}\n\\label{table_notations}\n\\begin{tabular}{c|c}\n     \\hline\n     Notation & Definition \\\\\n     \\hline\n     $\\mathcal{G}$\t\t\t& The target graph\\\\\n     $\\mathcal{V}, \\mathcal{E}$ & the set of nodes and edges in $\\mathcal{G}$\\\\\n     $\\mathcal{O},\\mathcal{R}$\t& the set of node and edge types in $\\mathcal{G}$\\\\\n     $\\mathbf{X}$\t\t& the matrix of node attributes in $\\mathcal{G}$\\\\\n     $\\mathcal{G}_r$\t\t& the sub-network \\wrt. edge type $r$\\\\ \n     $\\mathbf{A}_r$\t\t& the adjacency matrix of $\\mathcal{G}_r$\\\\\n     $\\mathbb{A}$       & the aggregated adjacency matrix\\\\\n     $\\mathbf{H}$       & the node embeddings\\\\\n     $\\mathbf{H}^{(l)}$       & the hidden representation for the $l$-th layer\\\\\n     $d$\t\t\t  \t& the hidden dimensionality of embeddings\\\\\n     $n, m$\t\t\t  \t& the number of nodes and attributes\\\\\n     $\\beta_r$                & the learnable weight for edge type $r$\\\\\n     $\\mathbf{W}^{(l)}$\t\t\t    & the learnable weight matrix for the $l$-th layer\\\\\n     \\hline\n\\end{tabular}\n% \\vspace{-5mm}\n\\end{table}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\label{Single layer GCN}\n    \\mathbf{H}^{(1)} = \\mathbb{A} \\cdot \\mathbf{X} \\cdot \\mathbf{W}^{(1)},\n\\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{Double layer GCN}\n\\begin{split}\n    \\mathbf{H}^{(2)} &= \\mathbb{A} \\cdot \\mathbf{H}^{(1)} \\cdot \\mathbf{W}^{(2)}\\\\\n    &= \\mathbb{A} \\cdot (\\mathbb{A} \\cdot \\mathbf{X} \\cdot \\mathbf{W}^{(1)}) \\cdot \\mathbf{W}^{(2)}\\\\\n    &= \\mathbb{A}^{2} \\cdot \\mathbf{X} \\cdot \\mathbf{W}^{(1)}\\cdot \\mathbf{W}^{(2)},\n\\end{split}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{Double layer fusion GCN}\n    \\mathbf{H} = \\frac{1}{2}(\\mathbf{H}^{(1)} + \\mathbf{H}^{(2)}). \n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{l-layer GCN}\n\\begin{split}\n    \\mathbf{H}^{(l)} &= \\mathbb{A} \\cdot \\mathbf{H}^{(l-1)} \\cdot \\mathbf{W}^{(l)}\\\\\n    &= \\mathbb{A} \\cdot (\\mathbb{A} \\cdot \\mathbf{H}^{(l-2)} \\cdot \\mathbf{W}^{(l-1)}) \\cdot \\mathbf{W}^{(l)}\\\\\n    &= \\underbrace{\\mathbb{A} \\cdots (\\mathbb{A}}_l \\cdot \\mathbf{X} \\cdot \\underbrace{\\mathbf{W}^{(1)}) \\cdots \\mathbf{W}^{(l)}}_l\\\\\n    &= \\mathbb{A}^{l} \\cdot \\mathbf{X} \\cdot \\underbrace{\\mathbf{W}^{(1)} \\cdots \\mathbf{W}^{(l)}}_l \n\\end{split}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\label{Multilayer fusion GCN}\n\\begin{split}\n    \\mathbf{H} &= \\frac{1}{l}\\sum_{i=1}^{l}\\mathbf{H}^{(i)}\\\\\n    &=\\frac{1}{l} \\sum_{i=1}^{l}\\mathbb{A} \\cdot \\mathbf{H}^{(i-1)} \\cdot \\mathbf{W}^{(i)},\n\\end{split}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\label{Unsupervised Loss}\n    \\begin{split}\n     \\mathcal{L} &=-\\sum_{(u, v) \\in \\Omega} \\log \\sigma(<\\mathbf{H}^\\mathsf{T}_u, \\mathbf{H}_v>) - \\sum_{(u', v') \\in \\Omega^{-}}\\log\\sigma(-<\\mathbf{H}^\\mathsf{T}_{u'}, \\mathbf{H}_{v'}>),\n    \\end{split}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\label{Supervised Loss}\n     \\mathcal{L} = -\\sum_{i \\in \\mathcal{V}_{ids}} \\mathbf{Y}_i\\ \\mathrm{ln}(\\mathbf{C} \\cdot \\mathbf{H}_i), \n\\end{equation}"
        },
        "git_link": "https://github.com/NSSSJSS/MHGCN"
    }
}